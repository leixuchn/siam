INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "72"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 10:33:04.022692: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:33:04.022733: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:33:04.022739: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:33:04.022744: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:33:04.022748: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:33:04.676166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 8.75GiB
2017-12-05 10:33:04.676200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 10:33:04.676206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 10:33:04.676214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 10:33:11.297689: step 0, loss = 2.03, batch loss = 1.98 (1.8 examples/sec; 4.456 sec/batch; 411h:35m:16s remains)
2017-12-05 10:33:12.005101: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1926956 -4.1758146 -4.1542687 -4.1553249 -4.1564922 -4.1487708 -4.1460891 -4.1541123 -4.1592493 -4.1778564 -4.2153754 -4.2490287 -4.2575307 -4.2587919 -4.2604442][-4.209682 -4.19133 -4.1701713 -4.1739879 -4.1795378 -4.1759448 -4.177177 -4.1788917 -4.1775455 -4.196558 -4.237031 -4.2688208 -4.2739959 -4.271544 -4.2717338][-4.2137322 -4.1893868 -4.167779 -4.17461 -4.1898022 -4.1939321 -4.1948886 -4.1920171 -4.188417 -4.2067137 -4.2444906 -4.2700024 -4.269691 -4.2604632 -4.2588568][-4.2148952 -4.1846542 -4.1626229 -4.1722302 -4.1932774 -4.2021613 -4.2038312 -4.2024536 -4.2022738 -4.2196465 -4.2500014 -4.268127 -4.2620416 -4.2478447 -4.245841][-4.2176428 -4.1877813 -4.1687074 -4.1801457 -4.1982269 -4.2040358 -4.2055349 -4.2104268 -4.2204757 -4.2420363 -4.2661428 -4.27729 -4.2665563 -4.2490172 -4.2430229][-4.22565 -4.200119 -4.1867933 -4.1956491 -4.2031579 -4.2029791 -4.202961 -4.2128468 -4.2331557 -4.2637391 -4.2861962 -4.2908168 -4.2758436 -4.2584476 -4.2527122][-4.2308941 -4.2099547 -4.1996789 -4.1959138 -4.1894865 -4.18271 -4.179606 -4.19069 -4.2266541 -4.2716293 -4.2963943 -4.2979422 -4.2823882 -4.2702107 -4.2691007][-4.235642 -4.2162256 -4.2036967 -4.1871033 -4.1693478 -4.1528521 -4.1438537 -4.1583271 -4.2115784 -4.2666583 -4.2949257 -4.2977581 -4.2842259 -4.2767744 -4.2821426][-4.243443 -4.2271962 -4.2139463 -4.1913195 -4.1666112 -4.1433458 -4.1297584 -4.1486392 -4.2074409 -4.2596374 -4.2858438 -4.29036 -4.2769909 -4.271481 -4.2802606][-4.2510257 -4.2411771 -4.2322621 -4.2155786 -4.1959324 -4.1732879 -4.1615286 -4.1767817 -4.2188754 -4.2523527 -4.2706423 -4.2734108 -4.2587938 -4.2526703 -4.2610335][-4.254427 -4.2482791 -4.2455664 -4.2399149 -4.232677 -4.2215185 -4.2179556 -4.2281256 -4.2488356 -4.2622981 -4.2716031 -4.2677531 -4.2460561 -4.2292881 -4.2312608][-4.2537808 -4.2450237 -4.2445154 -4.2480626 -4.2532821 -4.2541804 -4.2591834 -4.2669911 -4.2734847 -4.2783427 -4.2815137 -4.2711873 -4.2463155 -4.223702 -4.2188473][-4.2557149 -4.2410388 -4.2355947 -4.24168 -4.2524729 -4.2601609 -4.270431 -4.2792749 -4.2807665 -4.2854986 -4.2866688 -4.2756276 -4.2511296 -4.2302189 -4.2241468][-4.2670107 -4.2440963 -4.2298913 -4.2340851 -4.2457404 -4.25526 -4.2641621 -4.2713919 -4.273562 -4.27998 -4.2814593 -4.2715459 -4.2525139 -4.2397771 -4.2381186][-4.2849431 -4.2579055 -4.237309 -4.2382112 -4.2464709 -4.2531295 -4.2574363 -4.2630839 -4.2701087 -4.2803645 -4.2834277 -4.274538 -4.2578154 -4.2497678 -4.25491]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 10:33:17.147431: step 10, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.476 sec/batch; 43h:59m:55s remains)
INFO - root - 2017-12-05 10:33:21.366756: step 20, loss = 2.05, batch loss = 1.99 (19.4 examples/sec; 0.413 sec/batch; 38h:10m:35s remains)
INFO - root - 2017-12-05 10:33:25.561978: step 30, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.451 sec/batch; 41h:41m:19s remains)
INFO - root - 2017-12-05 10:33:29.655900: step 40, loss = 2.08, batch loss = 2.02 (20.0 examples/sec; 0.400 sec/batch; 36h:55m:40s remains)
INFO - root - 2017-12-05 10:33:33.817666: step 50, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.435 sec/batch; 40h:09m:42s remains)
INFO - root - 2017-12-05 10:33:38.045101: step 60, loss = 2.09, batch loss = 2.03 (19.3 examples/sec; 0.414 sec/batch; 38h:11m:13s remains)
INFO - root - 2017-12-05 10:33:42.010306: step 70, loss = 2.06, batch loss = 2.00 (19.0 examples/sec; 0.420 sec/batch; 38h:47m:32s remains)
INFO - root - 2017-12-05 10:33:46.150037: step 80, loss = 2.05, batch loss = 2.00 (19.7 examples/sec; 0.407 sec/batch; 37h:33m:02s remains)
INFO - root - 2017-12-05 10:33:50.341897: step 90, loss = 2.08, batch loss = 2.02 (20.1 examples/sec; 0.399 sec/batch; 36h:49m:03s remains)
INFO - root - 2017-12-05 10:33:54.427318: step 100, loss = 2.06, batch loss = 2.01 (20.0 examples/sec; 0.400 sec/batch; 36h:54m:21s remains)
2017-12-05 10:33:54.842875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2065082 -4.1610227 -4.1204715 -4.10872 -4.1241183 -4.1463919 -4.1666927 -4.2003555 -4.2293816 -4.2348323 -4.2298894 -4.21927 -4.1895032 -4.1592484 -4.152914][-4.2031341 -4.1456895 -4.0898376 -4.0668688 -4.0786886 -4.1051097 -4.1308861 -4.1668339 -4.1992512 -4.2114873 -4.2151904 -4.2117133 -4.1900854 -4.166419 -4.1629629][-4.202704 -4.1388197 -4.0755448 -4.0465493 -4.0534468 -4.0773044 -4.1023765 -4.1388788 -4.1749291 -4.1924214 -4.2010274 -4.2026396 -4.1900225 -4.1729488 -4.1682415][-4.204143 -4.1399393 -4.07699 -4.047195 -4.0476265 -4.0650063 -4.0873036 -4.1238194 -4.1629539 -4.1851544 -4.1957259 -4.2001538 -4.1945572 -4.1809564 -4.170815][-4.2058477 -4.1455207 -4.0870414 -4.0559349 -4.0476289 -4.0575056 -4.0756993 -4.1088486 -4.1481304 -4.1759477 -4.1896019 -4.19755 -4.197669 -4.18898 -4.1753922][-4.2062373 -4.1486835 -4.0927877 -4.0574193 -4.0410814 -4.0449123 -4.0552206 -4.0786395 -4.114244 -4.1493583 -4.1693583 -4.1806073 -4.1865077 -4.1839142 -4.1722651][-4.2085514 -4.1542611 -4.0986829 -4.0579586 -4.0348077 -4.0318265 -4.0284944 -4.0354567 -4.0666618 -4.1094232 -4.1373458 -4.1526537 -4.1643233 -4.1696906 -4.1635957][-4.2094703 -4.1556997 -4.0993152 -4.0559635 -4.0295057 -4.0215731 -4.0101328 -4.0061288 -4.0357089 -4.081851 -4.1107674 -4.1264977 -4.1400294 -4.1477184 -4.143599][-4.2118764 -4.1591058 -4.1039772 -4.0633545 -4.039959 -4.0322776 -4.0213103 -4.0160952 -4.0427871 -4.0817757 -4.0999675 -4.1104383 -4.1234183 -4.1293974 -4.1228242][-4.2141967 -4.1620517 -4.1072354 -4.067883 -4.0459824 -4.0390296 -4.0343738 -4.037961 -4.0652332 -4.0923853 -4.0978537 -4.0989838 -4.1052847 -4.1068115 -4.100893][-4.2192373 -4.1673608 -4.1136684 -4.0754504 -4.0521932 -4.0439672 -4.0463743 -4.0606208 -4.0882506 -4.1062112 -4.1032991 -4.0953665 -4.09298 -4.0910659 -4.0898046][-4.2306538 -4.1809263 -4.1312156 -4.0981026 -4.0783458 -4.072823 -4.0812073 -4.0993586 -4.1224446 -4.1322756 -4.1260419 -4.115025 -4.1095695 -4.1056323 -4.1071143][-4.2476053 -4.2032356 -4.1585627 -4.1308656 -4.1168814 -4.1176653 -4.1322856 -4.1510611 -4.1675024 -4.1724606 -4.1677604 -4.1589193 -4.1528473 -4.1464677 -4.1472793][-4.2726169 -4.2381864 -4.2013388 -4.1763325 -4.1658978 -4.1726141 -4.1917109 -4.2097807 -4.2219768 -4.2249355 -4.2216492 -4.2128229 -4.2053957 -4.1988215 -4.2005963][-4.3000884 -4.2786751 -4.2544236 -4.2346296 -4.2262135 -4.2337141 -4.2510457 -4.2646208 -4.2715387 -4.2739091 -4.2713323 -4.263248 -4.25705 -4.2536511 -4.2564721]]...]
INFO - root - 2017-12-05 10:33:58.960309: step 110, loss = 2.08, batch loss = 2.02 (20.3 examples/sec; 0.394 sec/batch; 36h:23m:07s remains)
INFO - root - 2017-12-05 10:34:03.125320: step 120, loss = 2.05, batch loss = 1.99 (19.0 examples/sec; 0.422 sec/batch; 38h:56m:31s remains)
INFO - root - 2017-12-05 10:34:07.241762: step 130, loss = 2.06, batch loss = 2.00 (19.2 examples/sec; 0.417 sec/batch; 38h:31m:10s remains)
INFO - root - 2017-12-05 10:34:11.429779: step 140, loss = 2.06, batch loss = 2.00 (19.4 examples/sec; 0.412 sec/batch; 38h:00m:05s remains)
INFO - root - 2017-12-05 10:34:15.595692: step 150, loss = 2.09, batch loss = 2.03 (17.8 examples/sec; 0.449 sec/batch; 41h:26m:59s remains)
INFO - root - 2017-12-05 10:34:19.739822: step 160, loss = 2.05, batch loss = 1.99 (19.9 examples/sec; 0.403 sec/batch; 37h:10m:45s remains)
INFO - root - 2017-12-05 10:34:23.859604: step 170, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.421 sec/batch; 38h:54m:08s remains)
INFO - root - 2017-12-05 10:34:27.801560: step 180, loss = 2.05, batch loss = 1.99 (19.4 examples/sec; 0.412 sec/batch; 38h:01m:18s remains)
INFO - root - 2017-12-05 10:34:31.953119: step 190, loss = 2.07, batch loss = 2.01 (19.8 examples/sec; 0.404 sec/batch; 37h:17m:41s remains)
INFO - root - 2017-12-05 10:34:36.048185: step 200, loss = 2.05, batch loss = 2.00 (19.6 examples/sec; 0.408 sec/batch; 37h:38m:18s remains)
2017-12-05 10:34:36.476965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2447691 -4.2367063 -4.2240024 -4.2117057 -4.2066169 -4.20487 -4.2089992 -4.2122641 -4.214335 -4.2128887 -4.2168651 -4.2291503 -4.2373519 -4.2392225 -4.2370682][-4.232028 -4.2207727 -4.208262 -4.1969485 -4.1946878 -4.1947803 -4.1936693 -4.1895823 -4.1985774 -4.2067838 -4.21295 -4.2191939 -4.2235055 -4.2256985 -4.220974][-4.2020526 -4.1913071 -4.1885176 -4.181951 -4.1816978 -4.1816959 -4.1718297 -4.1622372 -4.1848512 -4.2084203 -4.21596 -4.2135863 -4.2130575 -4.2130365 -4.2062354][-4.1786785 -4.1688247 -4.1710234 -4.1718903 -4.1692724 -4.1601443 -4.1396456 -4.1259241 -4.1650128 -4.2033052 -4.212749 -4.209621 -4.2065716 -4.2012978 -4.19146][-4.1719894 -4.1604114 -4.159111 -4.1607542 -4.1511593 -4.1228442 -4.0784411 -4.0511341 -4.10569 -4.1683722 -4.1952677 -4.2000566 -4.1954494 -4.1867437 -4.1766891][-4.1733637 -4.1530313 -4.1429195 -4.1426258 -4.1239963 -4.0724411 -3.9847372 -3.9235618 -4.0007153 -4.10463 -4.1588974 -4.1757078 -4.1708317 -4.1586385 -4.153708][-4.17097 -4.141098 -4.1255045 -4.1295261 -4.10867 -4.0409622 -3.9096711 -3.802738 -3.8979115 -4.0413756 -4.1252184 -4.1539388 -4.1455231 -4.1294851 -4.1326604][-4.1631713 -4.1357756 -4.127779 -4.1448 -4.1320438 -4.0768456 -3.9617503 -3.863714 -3.9369061 -4.0624475 -4.1431513 -4.1788011 -4.1750164 -4.158452 -4.1572208][-4.1682382 -4.1490049 -4.1483264 -4.173521 -4.1725321 -4.1492577 -4.0827918 -4.0236387 -4.0582743 -4.1330953 -4.1936536 -4.2309103 -4.2339344 -4.2178159 -4.2052522][-4.1911669 -4.1801343 -4.1766758 -4.1957154 -4.1991973 -4.1987844 -4.1631451 -4.1255302 -4.1377616 -4.1804647 -4.2261944 -4.2610826 -4.26392 -4.2477565 -4.2312989][-4.2012286 -4.1903172 -4.183804 -4.19296 -4.1956525 -4.209043 -4.192235 -4.1705723 -4.1740503 -4.1964583 -4.22614 -4.2518773 -4.2562218 -4.24698 -4.23544][-4.1986728 -4.1877413 -4.1862035 -4.195528 -4.1990972 -4.218226 -4.2129116 -4.2000103 -4.2022605 -4.2124529 -4.2272043 -4.2450113 -4.250464 -4.2519116 -4.2519937][-4.2045174 -4.1960821 -4.2042027 -4.2157693 -4.2211723 -4.2400894 -4.2345624 -4.2238936 -4.22655 -4.2316966 -4.2391071 -4.2520385 -4.2595787 -4.2678995 -4.2770147][-4.2134857 -4.2113533 -4.2255325 -4.2392197 -4.2476082 -4.2641521 -4.2582173 -4.2484527 -4.2489634 -4.2492733 -4.2522335 -4.264967 -4.273839 -4.2796259 -4.2873311][-4.23698 -4.2368875 -4.2483673 -4.2577553 -4.2648363 -4.275352 -4.2735252 -4.269433 -4.2684784 -4.2659888 -4.266459 -4.2769823 -4.2841735 -4.2863426 -4.2898512]]...]
INFO - root - 2017-12-05 10:34:40.674289: step 210, loss = 2.06, batch loss = 2.00 (19.5 examples/sec; 0.410 sec/batch; 37h:51m:19s remains)
INFO - root - 2017-12-05 10:34:44.823076: step 220, loss = 2.08, batch loss = 2.02 (19.1 examples/sec; 0.419 sec/batch; 38h:41m:41s remains)
INFO - root - 2017-12-05 10:34:48.949509: step 230, loss = 2.05, batch loss = 1.99 (18.9 examples/sec; 0.423 sec/batch; 39h:00m:58s remains)
INFO - root - 2017-12-05 10:34:53.044341: step 240, loss = 2.04, batch loss = 1.98 (19.5 examples/sec; 0.410 sec/batch; 37h:52m:45s remains)
INFO - root - 2017-12-05 10:34:57.192695: step 250, loss = 2.06, batch loss = 2.00 (19.4 examples/sec; 0.412 sec/batch; 38h:03m:23s remains)
INFO - root - 2017-12-05 10:35:01.392139: step 260, loss = 2.06, batch loss = 2.01 (18.1 examples/sec; 0.441 sec/batch; 40h:44m:40s remains)
INFO - root - 2017-12-05 10:35:05.543209: step 270, loss = 2.08, batch loss = 2.03 (19.2 examples/sec; 0.416 sec/batch; 38h:25m:17s remains)
INFO - root - 2017-12-05 10:35:09.639963: step 280, loss = 2.04, batch loss = 1.98 (19.9 examples/sec; 0.402 sec/batch; 37h:03m:45s remains)
INFO - root - 2017-12-05 10:35:13.668126: step 290, loss = 2.08, batch loss = 2.02 (19.5 examples/sec; 0.411 sec/batch; 37h:55m:16s remains)
INFO - root - 2017-12-05 10:35:17.788912: step 300, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.420 sec/batch; 38h:46m:30s remains)
2017-12-05 10:35:18.236090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0812817 -4.0999088 -4.1277633 -4.1597753 -4.1875224 -4.2054033 -4.2047825 -4.1818862 -4.1520824 -4.1327729 -4.1378951 -4.150641 -4.1543508 -4.1572418 -4.1663375][-4.1549406 -4.1583838 -4.1622257 -4.176127 -4.2003484 -4.2195911 -4.222713 -4.210556 -4.1991549 -4.1930604 -4.19911 -4.201529 -4.1947517 -4.1834111 -4.1781464][-4.212831 -4.2063355 -4.1898761 -4.1846385 -4.20254 -4.2252707 -4.2360225 -4.2389159 -4.243165 -4.2445393 -4.2454319 -4.2361989 -4.2169285 -4.1921391 -4.1776695][-4.23236 -4.2201052 -4.1934347 -4.1726446 -4.1826634 -4.2070436 -4.2266212 -4.2426124 -4.2576842 -4.2631807 -4.2557244 -4.2346883 -4.2063737 -4.1769567 -4.1638322][-4.2145047 -4.2011418 -4.1720772 -4.1461091 -4.1495633 -4.1687841 -4.1928129 -4.2142711 -4.2297015 -4.2351112 -4.2226624 -4.1938014 -4.1577997 -4.1325526 -4.133019][-4.1844 -4.1714063 -4.1484928 -4.1277447 -4.1267419 -4.1352496 -4.1568165 -4.1777506 -4.1826673 -4.1777434 -4.1595111 -4.1292672 -4.0919328 -4.0747557 -4.0945144][-4.1676412 -4.1497626 -4.1291065 -4.1139278 -4.1099882 -4.1128922 -4.128109 -4.1448603 -4.1386242 -4.1232939 -4.0968289 -4.0616198 -4.0239105 -4.0184813 -4.0568562][-4.1626964 -4.1420484 -4.123014 -4.1102858 -4.1016164 -4.1014318 -4.1108651 -4.12286 -4.1111679 -4.0940709 -4.0667996 -4.0327268 -3.9981403 -4.0039191 -4.0534687][-4.1741061 -4.1613626 -4.15215 -4.1460795 -4.1385274 -4.1307468 -4.1326838 -4.1383352 -4.1242738 -4.1074443 -4.0863738 -4.0617437 -4.0384641 -4.0514874 -4.0974321][-4.1866183 -4.1873565 -4.1894832 -4.1925769 -4.1917496 -4.1801162 -4.1741548 -4.1765633 -4.1649833 -4.1520643 -4.1420879 -4.1304879 -4.1163321 -4.1275945 -4.1592989][-4.1908827 -4.2050028 -4.2161908 -4.2243886 -4.2273154 -4.2148681 -4.2041035 -4.2012057 -4.1876454 -4.1770511 -4.1787004 -4.1830273 -4.1804762 -4.1915503 -4.2112584][-4.1813226 -4.2080975 -4.2267947 -4.2428331 -4.2497368 -4.2371416 -4.22483 -4.21673 -4.1983252 -4.1874166 -4.1970258 -4.2120361 -4.2182827 -4.2312074 -4.2454286][-4.1742325 -4.2026172 -4.223928 -4.242382 -4.2512155 -4.2414479 -4.2285256 -4.2154908 -4.1952591 -4.1875963 -4.2036643 -4.2243786 -4.2337227 -4.2479076 -4.261241][-4.1873827 -4.2065196 -4.2241387 -4.2411971 -4.2506719 -4.2467752 -4.2378736 -4.2222223 -4.200635 -4.1928477 -4.2083364 -4.2279263 -4.2383285 -4.2508464 -4.2606053][-4.2147508 -4.2232819 -4.232964 -4.2437654 -4.2524762 -4.256052 -4.2542248 -4.2418671 -4.222127 -4.213841 -4.22081 -4.2314839 -4.2379575 -4.243741 -4.2458973]]...]
INFO - root - 2017-12-05 10:35:22.379924: step 310, loss = 2.07, batch loss = 2.01 (19.1 examples/sec; 0.418 sec/batch; 38h:33m:31s remains)
INFO - root - 2017-12-05 10:35:26.558155: step 320, loss = 2.04, batch loss = 1.99 (18.5 examples/sec; 0.433 sec/batch; 39h:59m:50s remains)
INFO - root - 2017-12-05 10:35:30.670335: step 330, loss = 2.07, batch loss = 2.01 (19.3 examples/sec; 0.415 sec/batch; 38h:16m:20s remains)
INFO - root - 2017-12-05 10:35:34.802777: step 340, loss = 2.07, batch loss = 2.01 (19.9 examples/sec; 0.401 sec/batch; 37h:02m:32s remains)
INFO - root - 2017-12-05 10:35:38.963251: step 350, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.454 sec/batch; 41h:53m:39s remains)
INFO - root - 2017-12-05 10:35:43.167210: step 360, loss = 2.07, batch loss = 2.01 (19.3 examples/sec; 0.414 sec/batch; 38h:13m:36s remains)
INFO - root - 2017-12-05 10:35:47.307023: step 370, loss = 2.07, batch loss = 2.01 (19.5 examples/sec; 0.410 sec/batch; 37h:51m:53s remains)
INFO - root - 2017-12-05 10:35:51.422362: step 380, loss = 2.06, batch loss = 2.00 (19.3 examples/sec; 0.414 sec/batch; 38h:09m:49s remains)
INFO - root - 2017-12-05 10:35:55.538171: step 390, loss = 2.06, batch loss = 2.00 (20.1 examples/sec; 0.398 sec/batch; 36h:41m:54s remains)
INFO - root - 2017-12-05 10:35:59.471086: step 400, loss = 2.05, batch loss = 1.99 (18.9 examples/sec; 0.423 sec/batch; 39h:03m:18s remains)
2017-12-05 10:35:59.896851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2314672 -4.22348 -4.2250967 -4.2304506 -4.2527795 -4.2705441 -4.2693515 -4.2642703 -4.2632394 -4.2587323 -4.2569561 -4.2564373 -4.2610512 -4.2752328 -4.2800264][-4.2241607 -4.2171955 -4.2201781 -4.2307734 -4.2567077 -4.2722378 -4.266376 -4.2567959 -4.2497072 -4.2438865 -4.2415357 -4.237381 -4.24054 -4.2539339 -4.2601438][-4.2250862 -4.2171073 -4.2202792 -4.2345839 -4.261899 -4.2763715 -4.268796 -4.2565346 -4.2441068 -4.2383351 -4.2351046 -4.2305903 -4.2335582 -4.2416 -4.2471933][-4.2202959 -4.2017221 -4.195282 -4.2090874 -4.2375269 -4.2532005 -4.2447023 -4.2329674 -4.219615 -4.2142906 -4.2122903 -4.2139783 -4.2217655 -4.2283764 -4.2347846][-4.2147818 -4.179038 -4.1546917 -4.1600852 -4.1842127 -4.2000284 -4.1967592 -4.1939754 -4.1874967 -4.1861763 -4.1873546 -4.1970515 -4.2091093 -4.2166181 -4.2253423][-4.2211757 -4.1795464 -4.1391644 -4.1241188 -4.1323504 -4.1408949 -4.1405511 -4.1454582 -4.1504931 -4.1580925 -4.1656642 -4.180584 -4.1960993 -4.2069249 -4.2201452][-4.2334189 -4.1988177 -4.1547327 -4.125402 -4.1221185 -4.122242 -4.1107812 -4.1046219 -4.1103115 -4.1249104 -4.1426835 -4.1655731 -4.1863203 -4.2032657 -4.2226405][-4.2305989 -4.2068672 -4.1736403 -4.15063 -4.1577268 -4.1676593 -4.1461716 -4.1203547 -4.113183 -4.1172 -4.1294193 -4.1527414 -4.1771183 -4.2010961 -4.2297649][-4.1936531 -4.1746206 -4.1551385 -4.1485333 -4.1787939 -4.2144475 -4.2029228 -4.1746707 -4.1579938 -4.1447163 -4.1397233 -4.1506081 -4.1717243 -4.1996937 -4.2356858][-4.1440392 -4.1253486 -4.1136284 -4.1198816 -4.1684384 -4.2233143 -4.2323618 -4.2208719 -4.2083025 -4.1885257 -4.1728234 -4.1717191 -4.184608 -4.2104797 -4.2473812][-4.1241503 -4.1004629 -4.0832005 -4.0899959 -4.1458144 -4.2087846 -4.2317462 -4.2351341 -4.2322268 -4.2186561 -4.2061486 -4.2019048 -4.2099242 -4.2298231 -4.2616339][-4.1522131 -4.1283326 -4.106029 -4.1086149 -4.1582246 -4.2125254 -4.2367015 -4.2455544 -4.245862 -4.2371941 -4.2308059 -4.23006 -4.237648 -4.2529736 -4.2759328][-4.2047639 -4.1870708 -4.1674747 -4.167511 -4.2033391 -4.2394552 -4.2534604 -4.258791 -4.2585788 -4.2521849 -4.2497768 -4.2524543 -4.2610965 -4.2735267 -4.2884111][-4.2507324 -4.24047 -4.2258 -4.2238274 -4.2470465 -4.2689228 -4.2742171 -4.2753048 -4.2738204 -4.2691035 -4.2665524 -4.2679348 -4.2738781 -4.2836342 -4.2944131][-4.2817559 -4.2756467 -4.2663765 -4.2654061 -4.278378 -4.28996 -4.2902961 -4.2902341 -4.2893806 -4.2869692 -4.2845883 -4.2833872 -4.28503 -4.2902994 -4.298058]]...]
INFO - root - 2017-12-05 10:36:03.977689: step 410, loss = 2.07, batch loss = 2.01 (19.3 examples/sec; 0.415 sec/batch; 38h:14m:14s remains)
INFO - root - 2017-12-05 10:36:08.125706: step 420, loss = 2.06, batch loss = 2.01 (19.7 examples/sec; 0.406 sec/batch; 37h:26m:27s remains)
INFO - root - 2017-12-05 10:36:12.281938: step 430, loss = 2.08, batch loss = 2.03 (19.0 examples/sec; 0.421 sec/batch; 38h:50m:41s remains)
INFO - root - 2017-12-05 10:36:16.445314: step 440, loss = 2.06, batch loss = 2.00 (19.3 examples/sec; 0.414 sec/batch; 38h:11m:05s remains)
INFO - root - 2017-12-05 10:36:20.568513: step 450, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.426 sec/batch; 39h:20m:04s remains)
INFO - root - 2017-12-05 10:36:24.714903: step 460, loss = 2.07, batch loss = 2.01 (19.1 examples/sec; 0.418 sec/batch; 38h:33m:19s remains)
INFO - root - 2017-12-05 10:36:28.842516: step 470, loss = 2.06, batch loss = 2.00 (20.3 examples/sec; 0.395 sec/batch; 36h:23m:52s remains)
INFO - root - 2017-12-05 10:36:33.004598: step 480, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 39h:40m:46s remains)
INFO - root - 2017-12-05 10:36:37.169131: step 490, loss = 2.08, batch loss = 2.02 (19.1 examples/sec; 0.419 sec/batch; 38h:39m:19s remains)
INFO - root - 2017-12-05 10:36:41.294423: step 500, loss = 2.07, batch loss = 2.02 (19.8 examples/sec; 0.403 sec/batch; 37h:10m:06s remains)
2017-12-05 10:36:41.702565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.188591 -4.2111468 -4.2437963 -4.268508 -4.2707844 -4.260294 -4.2465506 -4.233964 -4.2158542 -4.1921253 -4.1787343 -4.1824493 -4.1905813 -4.1937151 -4.1938763][-4.1783485 -4.2073417 -4.2447047 -4.2667289 -4.2545314 -4.2263231 -4.2122231 -4.2085567 -4.204998 -4.197094 -4.1877837 -4.18853 -4.1967592 -4.2022042 -4.1998763][-4.1748767 -4.2038746 -4.2374573 -4.2516589 -4.2289858 -4.183836 -4.1659102 -4.1751747 -4.1985269 -4.2170434 -4.2150135 -4.2114239 -4.2166486 -4.2201409 -4.2139039][-4.1768179 -4.1997313 -4.2266512 -4.2319236 -4.1965227 -4.1308646 -4.0948472 -4.1177206 -4.182869 -4.23277 -4.2416453 -4.2321091 -4.2356334 -4.2349858 -4.2234516][-4.1977248 -4.2139544 -4.2310433 -4.2212996 -4.1617641 -4.0621834 -3.9960418 -4.0294609 -4.1363521 -4.2204595 -4.2523527 -4.2490439 -4.2497792 -4.24492 -4.2297797][-4.2374949 -4.2471371 -4.2470722 -4.2148252 -4.1258984 -3.985167 -3.8762386 -3.916683 -4.0655317 -4.1912184 -4.2481627 -4.2582574 -4.2597613 -4.2529736 -4.2339172][-4.2703395 -4.2794003 -4.2657051 -4.2099066 -4.0981383 -3.9256968 -3.7671065 -3.8057117 -3.9967365 -4.1591711 -4.2378206 -4.2600675 -4.2642097 -4.2557068 -4.232296][-4.2903996 -4.2985525 -4.2819571 -4.2174134 -4.1003156 -3.9248443 -3.7530556 -3.7855272 -3.9880223 -4.1573625 -4.2417574 -4.2644377 -4.2620144 -4.2486348 -4.225203][-4.3024292 -4.308898 -4.30004 -4.247787 -4.1481919 -4.0138235 -3.886178 -3.9023993 -4.0596766 -4.1949682 -4.2618647 -4.2698078 -4.2518182 -4.2339234 -4.2178688][-4.3013539 -4.3058529 -4.3082709 -4.2774968 -4.2081037 -4.1253452 -4.0488558 -4.0492172 -4.1476636 -4.2415276 -4.2836671 -4.2702322 -4.2359738 -4.2165036 -4.2092962][-4.2881289 -4.2938356 -4.3054042 -4.2958255 -4.2562809 -4.2083063 -4.1605611 -4.1536818 -4.208468 -4.2698307 -4.2931204 -4.2636456 -4.2228141 -4.2057266 -4.207932][-4.269455 -4.2792244 -4.294632 -4.2989845 -4.2815709 -4.2521887 -4.2206769 -4.2162967 -4.2465134 -4.282043 -4.2897558 -4.2542334 -4.2139711 -4.2028255 -4.2131186][-4.2571807 -4.2684479 -4.2814422 -4.2906241 -4.2826762 -4.2602453 -4.2400594 -4.2427483 -4.2633424 -4.2858238 -4.284903 -4.2509742 -4.2145209 -4.2049961 -4.2176576][-4.2540593 -4.2618933 -4.2677855 -4.2744131 -4.2695804 -4.2515697 -4.2410707 -4.2504063 -4.2719569 -4.2924132 -4.2906575 -4.265532 -4.2358975 -4.2241521 -4.2346311][-4.2500429 -4.2507715 -4.2480683 -4.2509089 -4.2503052 -4.23834 -4.2354274 -4.2500768 -4.2749696 -4.2968111 -4.3018789 -4.2889085 -4.2684765 -4.2585287 -4.2650204]]...]
INFO - root - 2017-12-05 10:36:45.598000: step 510, loss = 2.08, batch loss = 2.02 (19.2 examples/sec; 0.416 sec/batch; 38h:22m:15s remains)
INFO - root - 2017-12-05 10:36:49.680499: step 520, loss = 2.07, batch loss = 2.01 (19.2 examples/sec; 0.417 sec/batch; 38h:26m:10s remains)
INFO - root - 2017-12-05 10:36:53.792531: step 530, loss = 2.05, batch loss = 1.99 (19.4 examples/sec; 0.413 sec/batch; 38h:05m:35s remains)
INFO - root - 2017-12-05 10:36:57.957507: step 540, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.430 sec/batch; 39h:37m:43s remains)
INFO - root - 2017-12-05 10:37:02.049586: step 550, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.435 sec/batch; 40h:04m:23s remains)
INFO - root - 2017-12-05 10:37:06.150404: step 560, loss = 2.06, batch loss = 2.00 (19.6 examples/sec; 0.407 sec/batch; 37h:34m:04s remains)
INFO - root - 2017-12-05 10:37:10.364859: step 570, loss = 2.06, batch loss = 2.00 (19.2 examples/sec; 0.416 sec/batch; 38h:20m:11s remains)
INFO - root - 2017-12-05 10:37:14.784468: step 580, loss = 2.07, batch loss = 2.02 (17.8 examples/sec; 0.448 sec/batch; 41h:20m:52s remains)
INFO - root - 2017-12-05 10:37:21.160060: step 590, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.615 sec/batch; 56h:40m:12s remains)
INFO - root - 2017-12-05 10:37:27.251675: step 600, loss = 2.05, batch loss = 1.99 (13.5 examples/sec; 0.595 sec/batch; 54h:48m:39s remains)
2017-12-05 10:37:27.805904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.204977 -4.1969452 -4.2001858 -4.2081575 -4.2177296 -4.2264681 -4.2484064 -4.2815418 -4.3091621 -4.322701 -4.3279881 -4.3139486 -4.2820959 -4.2601719 -4.2547255][-4.2122788 -4.2114305 -4.2168288 -4.22226 -4.2242384 -4.2255511 -4.243454 -4.2777576 -4.309082 -4.3239279 -4.3293471 -4.3218231 -4.3026619 -4.2934961 -4.292407][-4.220921 -4.2212439 -4.2269692 -4.2316127 -4.2306972 -4.2265587 -4.2389851 -4.2719903 -4.3037491 -4.3155751 -4.3184166 -4.3144636 -4.3067303 -4.3103337 -4.3174529][-4.2218165 -4.2169051 -4.2211351 -4.2271695 -4.2226295 -4.2088151 -4.2175345 -4.25097 -4.2822475 -4.2939844 -4.2980213 -4.300447 -4.2989235 -4.3086653 -4.3214669][-4.2145853 -4.2101822 -4.2166977 -4.2232776 -4.208087 -4.1791162 -4.1764431 -4.2034612 -4.2340741 -4.252821 -4.2659731 -4.2747746 -4.2749825 -4.2857928 -4.3004966][-4.1884823 -4.1890664 -4.1959538 -4.1945753 -4.1678705 -4.127039 -4.1170239 -4.1426859 -4.1803508 -4.2074909 -4.2263432 -4.2349992 -4.2325821 -4.2400146 -4.2508669][-4.1567097 -4.1507912 -4.1405568 -4.1165242 -4.0754137 -4.0286126 -4.0193849 -4.0582318 -4.1125526 -4.1497688 -4.1695495 -4.1672845 -4.1529 -4.1497984 -4.1606307][-4.1399183 -4.1253309 -4.0965514 -4.0499024 -3.9929435 -3.9432633 -3.9379289 -3.9868603 -4.0470872 -4.0812712 -4.0906005 -4.0751143 -4.0489717 -4.0400333 -4.0614324][-4.1398573 -4.1147423 -4.0784931 -4.0229445 -3.9627545 -3.9181147 -3.9135587 -3.949832 -3.9894633 -4.0028062 -3.9971437 -3.9731743 -3.9465253 -3.952734 -4.0006204][-4.1674619 -4.1433349 -4.1141272 -4.0717974 -4.0199509 -3.9816835 -3.9720583 -3.9845786 -3.995079 -3.9841578 -3.9628491 -3.9324079 -3.91039 -3.9289865 -3.9882131][-4.2025952 -4.1860824 -4.1671176 -4.1423774 -4.1063175 -4.0780697 -4.0682206 -4.067091 -4.062758 -4.046298 -4.023891 -3.9954855 -3.9793212 -3.9986331 -4.0477815][-4.2378893 -4.2258415 -4.2099628 -4.19594 -4.1751623 -4.1606016 -4.1575532 -4.155057 -4.1512752 -4.1408987 -4.1253934 -4.1059322 -4.0965986 -4.1117835 -4.1434541][-4.2550693 -4.2414403 -4.2270207 -4.2188687 -4.2096906 -4.2088084 -4.2130513 -4.2157207 -4.2175503 -4.2133589 -4.2038345 -4.1905503 -4.186202 -4.1966796 -4.2170882][-4.25746 -4.2408929 -4.2237897 -4.2180767 -4.21865 -4.2278976 -4.2371941 -4.2429347 -4.2464318 -4.2452154 -4.2398658 -4.2319021 -4.2308841 -4.2392507 -4.2528563][-4.2586713 -4.2440505 -4.2282233 -4.2254276 -4.2307072 -4.2415934 -4.247921 -4.2491727 -4.2463455 -4.2404509 -4.2326455 -4.2263112 -4.2287664 -4.2398295 -4.2533283]]...]
INFO - root - 2017-12-05 10:37:34.108702: step 610, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 60h:21m:20s remains)
INFO - root - 2017-12-05 10:37:40.366059: step 620, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 60h:19m:38s remains)
INFO - root - 2017-12-05 10:37:46.629048: step 630, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 57h:32m:26s remains)
INFO - root - 2017-12-05 10:37:52.891470: step 640, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.617 sec/batch; 56h:50m:33s remains)
INFO - root - 2017-12-05 10:37:59.187874: step 650, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 58h:37m:48s remains)
INFO - root - 2017-12-05 10:38:05.463358: step 660, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 61h:15m:25s remains)
INFO - root - 2017-12-05 10:38:11.750049: step 670, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 58h:37m:25s remains)
INFO - root - 2017-12-05 10:38:18.017565: step 680, loss = 2.08, batch loss = 2.03 (12.9 examples/sec; 0.619 sec/batch; 57h:04m:29s remains)
INFO - root - 2017-12-05 10:38:24.270379: step 690, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 57h:59m:07s remains)
INFO - root - 2017-12-05 10:38:30.576360: step 700, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.609 sec/batch; 56h:05m:33s remains)
2017-12-05 10:38:31.124916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1848788 -4.186461 -4.1883421 -4.192122 -4.195518 -4.196569 -4.1945124 -4.1917477 -4.191298 -4.1927524 -4.1926565 -4.1870413 -4.1746526 -4.1589489 -4.1457629][-4.1797357 -4.1778393 -4.1758695 -4.1759753 -4.1761756 -4.174274 -4.168231 -4.1609507 -4.1586628 -4.1609068 -4.1616588 -4.15564 -4.1424866 -4.12717 -4.1167135][-4.1991906 -4.1936278 -4.1871505 -4.1829457 -4.1804595 -4.1761394 -4.1668758 -4.155952 -4.1504545 -4.1504874 -4.1507559 -4.1460075 -4.1353145 -4.121479 -4.1123061][-4.2203751 -4.2131629 -4.2050271 -4.2008982 -4.2008214 -4.1986966 -4.1907496 -4.1807117 -4.1745267 -4.1721835 -4.1710358 -4.1682925 -4.1610641 -4.1484933 -4.1369448][-4.2171626 -4.2069449 -4.198308 -4.1976013 -4.2036748 -4.2077928 -4.2049351 -4.1993527 -4.1967735 -4.1966276 -4.197113 -4.1975622 -4.1937861 -4.1808858 -4.1655259][-4.1884279 -4.1711993 -4.1605778 -4.1635213 -4.1754894 -4.1834483 -4.183217 -4.1828384 -4.1903071 -4.200357 -4.2090893 -4.2185163 -4.2215753 -4.2084255 -4.1862254][-4.1537175 -4.1283188 -4.1135445 -4.1183419 -4.1323938 -4.1366625 -4.130321 -4.1302953 -4.148891 -4.1745858 -4.1990347 -4.2223778 -4.2352233 -4.2258964 -4.2014866][-4.138361 -4.1117229 -4.094214 -4.0967546 -4.1059031 -4.0976992 -4.075614 -4.0680823 -4.0903735 -4.1282825 -4.1688924 -4.206737 -4.2308908 -4.22881 -4.2070904][-4.1472473 -4.1282015 -4.1135907 -4.1128621 -4.1140137 -4.0955625 -4.063446 -4.0468612 -4.0648966 -4.1028852 -4.1473632 -4.1895127 -4.2178311 -4.2200384 -4.2019739][-4.1739635 -4.1639695 -4.1541333 -4.1514473 -4.1486654 -4.130053 -4.1018276 -4.0863557 -4.0971508 -4.1250796 -4.15889 -4.1909919 -4.211843 -4.21071 -4.1941257][-4.2099705 -4.2036009 -4.1944656 -4.1891985 -4.1830993 -4.1675305 -4.1487083 -4.1405821 -4.150187 -4.1712227 -4.1947427 -4.2134418 -4.2206511 -4.2095861 -4.1894422][-4.2405005 -4.231215 -4.2179527 -4.2082829 -4.1991777 -4.1861448 -4.1764197 -4.1760259 -4.1864066 -4.203774 -4.2220435 -4.2328963 -4.2300224 -4.2103496 -4.1866226][-4.2530479 -4.2425461 -4.228756 -4.2198625 -4.2122922 -4.2018023 -4.1956592 -4.1970315 -4.2039905 -4.2157822 -4.2293715 -4.236299 -4.230134 -4.2087865 -4.18729][-4.2483487 -4.2396374 -4.2285557 -4.2235775 -4.2202768 -4.2140293 -4.2100291 -4.2102752 -4.2127352 -4.2186012 -4.2277083 -4.2319288 -4.2245955 -4.204967 -4.1885395][-4.2351942 -4.2281122 -4.2195439 -4.2171421 -4.2171206 -4.2152987 -4.2152238 -4.2169294 -4.217823 -4.2190785 -4.2232018 -4.2244716 -4.2170711 -4.2008963 -4.18963]]...]
INFO - root - 2017-12-05 10:38:37.434276: step 710, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 58h:09m:08s remains)
INFO - root - 2017-12-05 10:38:43.732094: step 720, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 59h:31m:53s remains)
INFO - root - 2017-12-05 10:38:49.978589: step 730, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 58h:08m:27s remains)
INFO - root - 2017-12-05 10:38:56.164348: step 740, loss = 2.09, batch loss = 2.03 (13.0 examples/sec; 0.615 sec/batch; 56h:41m:39s remains)
INFO - root - 2017-12-05 10:39:02.447479: step 750, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 58h:18m:48s remains)
INFO - root - 2017-12-05 10:39:08.606885: step 760, loss = 2.09, batch loss = 2.03 (13.1 examples/sec; 0.613 sec/batch; 56h:29m:04s remains)
INFO - root - 2017-12-05 10:39:14.833208: step 770, loss = 2.05, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 57h:44m:39s remains)
INFO - root - 2017-12-05 10:39:21.085898: step 780, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.658 sec/batch; 60h:37m:36s remains)
INFO - root - 2017-12-05 10:39:27.212516: step 790, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.623 sec/batch; 57h:23m:51s remains)
INFO - root - 2017-12-05 10:39:33.399690: step 800, loss = 2.09, batch loss = 2.03 (13.0 examples/sec; 0.615 sec/batch; 56h:38m:19s remains)
2017-12-05 10:39:33.989436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.118217 -4.1386557 -4.158699 -4.1649885 -4.1649814 -4.1543031 -4.1355472 -4.1291332 -4.1459923 -4.1668406 -4.1727505 -4.1596489 -4.1480527 -4.1505308 -4.163578][-4.1494718 -4.1688523 -4.1827292 -4.182363 -4.1821122 -4.175952 -4.1524324 -4.1336942 -4.1405568 -4.1567349 -4.1588979 -4.1429706 -4.1299043 -4.1313295 -4.1517839][-4.1737771 -4.1861835 -4.1943855 -4.1965671 -4.2010942 -4.2016525 -4.1857433 -4.1642513 -4.1589236 -4.1678967 -4.1650453 -4.1447968 -4.1282859 -4.12716 -4.1476502][-4.1849365 -4.1909852 -4.1954308 -4.2009239 -4.2055612 -4.2066355 -4.2008724 -4.1796303 -4.1635113 -4.167613 -4.1647873 -4.1458755 -4.1284227 -4.1261039 -4.1460066][-4.1886544 -4.1886911 -4.1882224 -4.1899905 -4.1904435 -4.1859331 -4.1838212 -4.1572657 -4.1332746 -4.1411619 -4.145843 -4.1350908 -4.1213908 -4.122366 -4.1395864][-4.1771679 -4.1648126 -4.151216 -4.1434097 -4.1401272 -4.1327043 -4.1282983 -4.0869565 -4.05073 -4.072753 -4.0948734 -4.0986328 -4.0965276 -4.1034012 -4.1191092][-4.1434655 -4.1166072 -4.0874271 -4.0684991 -4.0634775 -4.0593514 -4.0516162 -3.9858155 -3.9311035 -3.9785597 -4.0213842 -4.0407333 -4.0544543 -4.0736008 -4.0965686][-4.112031 -4.0847745 -4.0526752 -4.0376506 -4.0362573 -4.0354404 -4.0269728 -3.956248 -3.90286 -3.9587679 -4.0001111 -4.0290036 -4.0540624 -4.0799584 -4.1076841][-4.1128926 -4.1046433 -4.093812 -4.0976615 -4.1021848 -4.1047015 -4.1039433 -4.0562363 -4.0184679 -4.0478492 -4.0629497 -4.0820351 -4.0999846 -4.1173716 -4.1365838][-4.1534467 -4.1637249 -4.173553 -4.1880012 -4.1966438 -4.2013049 -4.20144 -4.1728716 -4.146615 -4.1500058 -4.1454253 -4.1493225 -4.1522174 -4.1547852 -4.1597118][-4.1986694 -4.2152696 -4.22976 -4.2461033 -4.2592082 -4.2663307 -4.26239 -4.23772 -4.2166286 -4.2100534 -4.1982474 -4.1905732 -4.1777687 -4.1700664 -4.1692629][-4.2211814 -4.2397304 -4.2523 -4.26328 -4.2767534 -4.28616 -4.2781024 -4.2546673 -4.2377791 -4.2299027 -4.2202649 -4.2068558 -4.1846743 -4.1759777 -4.1778522][-4.2157512 -4.235764 -4.2416763 -4.2411518 -4.2528605 -4.2668457 -4.2610803 -4.2415986 -4.2308831 -4.2265973 -4.2219229 -4.2107239 -4.1892114 -4.1842141 -4.1885118][-4.2087913 -4.2272372 -4.2236753 -4.2109041 -4.218358 -4.2368908 -4.2381167 -4.2249269 -4.2181926 -4.218513 -4.2163854 -4.2079625 -4.1942925 -4.194366 -4.198132][-4.2307596 -4.2470522 -4.2355108 -4.2132888 -4.2108984 -4.2237444 -4.2251725 -4.2142382 -4.2081003 -4.2090883 -4.2095318 -4.2039604 -4.1981478 -4.202239 -4.2045469]]...]
INFO - root - 2017-12-05 10:39:40.369239: step 810, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 57h:10m:43s remains)
INFO - root - 2017-12-05 10:39:46.608459: step 820, loss = 2.04, batch loss = 1.98 (12.9 examples/sec; 0.622 sec/batch; 57h:15m:39s remains)
INFO - root - 2017-12-05 10:39:52.783983: step 830, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 58h:49m:36s remains)
INFO - root - 2017-12-05 10:39:58.979617: step 840, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.625 sec/batch; 57h:37m:06s remains)
INFO - root - 2017-12-05 10:40:05.198679: step 850, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 58h:17m:17s remains)
INFO - root - 2017-12-05 10:40:11.462928: step 860, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 58h:27m:54s remains)
INFO - root - 2017-12-05 10:40:17.654934: step 870, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 58h:33m:35s remains)
INFO - root - 2017-12-05 10:40:23.809360: step 880, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 58h:29m:24s remains)
INFO - root - 2017-12-05 10:40:29.970551: step 890, loss = 2.10, batch loss = 2.04 (13.1 examples/sec; 0.611 sec/batch; 56h:15m:05s remains)
INFO - root - 2017-12-05 10:40:36.241852: step 900, loss = 2.04, batch loss = 1.98 (12.9 examples/sec; 0.622 sec/batch; 57h:18m:35s remains)
2017-12-05 10:40:36.773885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0853028 -4.0790873 -4.0610991 -4.0544705 -4.049089 -4.038506 -4.0519195 -4.0807471 -4.1081085 -4.1213031 -4.1249371 -4.1223955 -4.1175971 -4.0876346 -4.0639849][-4.0223546 -4.0455627 -4.055552 -4.0662422 -4.0639029 -4.0502043 -4.0558367 -4.0848594 -4.1131415 -4.1268258 -4.1351857 -4.1307573 -4.118504 -4.0883651 -4.0699315][-3.9581442 -4.0245204 -4.0713468 -4.0982442 -4.0981064 -4.0805039 -4.0696859 -4.083137 -4.100914 -4.1103191 -4.1271968 -4.1361637 -4.1291714 -4.0994859 -4.0827131][-3.9898241 -4.0666752 -4.1232886 -4.1486735 -4.1402159 -4.1097407 -4.0764585 -4.0598836 -4.0608239 -4.0728331 -4.1011543 -4.1276441 -4.1310782 -4.1080465 -4.0914721][-4.082335 -4.1331563 -4.1670685 -4.178412 -4.1523037 -4.0997262 -4.0505953 -4.0148034 -4.0119486 -4.0380521 -4.0737638 -4.1086531 -4.1231313 -4.1021214 -4.088026][-4.1529169 -4.1682425 -4.1762333 -4.1654234 -4.12116 -4.05359 -3.9943264 -3.9424043 -3.9471149 -3.9958549 -4.0444226 -4.0838675 -4.0991597 -4.0755291 -4.0600433][-4.1946096 -4.1829734 -4.1595578 -4.1207032 -4.0629125 -3.9857914 -3.9185686 -3.8554695 -3.8779979 -3.9661877 -4.0318966 -4.0728803 -4.0847549 -4.0515761 -4.0276442][-4.202919 -4.1778436 -4.13485 -4.0818133 -4.026886 -3.9612348 -3.9137561 -3.8706903 -3.9100256 -4.01081 -4.07363 -4.1009274 -4.1034474 -4.0655322 -4.0277009][-4.1859956 -4.1606331 -4.1210074 -4.0756588 -4.0359921 -4.0047975 -3.9980857 -3.9909575 -4.0302954 -4.0978184 -4.12902 -4.1263371 -4.1230569 -4.0943179 -4.0623207][-4.1841946 -4.1715851 -4.1499662 -4.1212878 -4.0966468 -4.0873575 -4.0980973 -4.10577 -4.1324143 -4.1641121 -4.1680117 -4.1439166 -4.1331749 -4.1134629 -4.0977874][-4.1864147 -4.1875691 -4.1852722 -4.1761603 -4.1625233 -4.1602631 -4.1698337 -4.182096 -4.1952896 -4.1979856 -4.1817269 -4.1486921 -4.1333604 -4.1199865 -4.1106172][-4.19245 -4.1970472 -4.2002597 -4.1990933 -4.1941347 -4.1960311 -4.2052994 -4.2194467 -4.2256689 -4.2157078 -4.1886306 -4.1527576 -4.1372581 -4.1269016 -4.1148725][-4.2027764 -4.2047863 -4.2074604 -4.2107639 -4.2102141 -4.2147045 -4.2248816 -4.2355647 -4.2345281 -4.221765 -4.1967168 -4.1679769 -4.1589394 -4.1495614 -4.1335621][-4.2189603 -4.214366 -4.2126603 -4.2136045 -4.2154675 -4.2219143 -4.2307796 -4.2377157 -4.2373738 -4.2284169 -4.2075949 -4.1907582 -4.1857896 -4.173759 -4.1564765][-4.2322164 -4.2254543 -4.219327 -4.2174945 -4.2184324 -4.2223129 -4.2287192 -4.2342377 -4.2369146 -4.2268033 -4.2074051 -4.1945472 -4.1951108 -4.18699 -4.1746635]]...]
INFO - root - 2017-12-05 10:40:43.016952: step 910, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.622 sec/batch; 57h:20m:08s remains)
INFO - root - 2017-12-05 10:40:49.325607: step 920, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.625 sec/batch; 57h:33m:44s remains)
INFO - root - 2017-12-05 10:40:55.545069: step 930, loss = 2.08, batch loss = 2.02 (13.2 examples/sec; 0.605 sec/batch; 55h:40m:49s remains)
INFO - root - 2017-12-05 10:41:01.705127: step 940, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.628 sec/batch; 57h:51m:16s remains)
INFO - root - 2017-12-05 10:41:08.004612: step 950, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.621 sec/batch; 57h:10m:36s remains)
INFO - root - 2017-12-05 10:41:14.332326: step 960, loss = 2.04, batch loss = 1.98 (12.9 examples/sec; 0.620 sec/batch; 57h:05m:19s remains)
INFO - root - 2017-12-05 10:41:20.613173: step 970, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.624 sec/batch; 57h:25m:09s remains)
INFO - root - 2017-12-05 10:41:26.913153: step 980, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 58h:31m:17s remains)
INFO - root - 2017-12-05 10:41:33.137385: step 990, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.638 sec/batch; 58h:44m:16s remains)
INFO - root - 2017-12-05 10:41:39.406562: step 1000, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 59h:25m:00s remains)
2017-12-05 10:41:39.937129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1423941 -4.147193 -4.1481719 -4.1587086 -4.1737618 -4.1801133 -4.1660419 -4.1441636 -4.1336761 -4.1406384 -4.1608548 -4.187993 -4.2186 -4.2481623 -4.2770762][-4.12253 -4.1303892 -4.1363382 -4.1520891 -4.1695766 -4.1751294 -4.1565752 -4.1270423 -4.1136827 -4.1242194 -4.1509285 -4.1854124 -4.2233086 -4.2573638 -4.2867737][-4.114625 -4.1265273 -4.1424127 -4.1622076 -4.1777205 -4.1789722 -4.1544809 -4.1189876 -4.1054206 -4.1196594 -4.1493917 -4.1841207 -4.2232409 -4.2595878 -4.29036][-4.1258287 -4.142869 -4.1646948 -4.1827884 -4.1914172 -4.1835361 -4.1498146 -4.1085382 -4.0975351 -4.118022 -4.1515055 -4.1835151 -4.2186813 -4.2534647 -4.2858348][-4.1458449 -4.1650963 -4.1843228 -4.1943336 -4.1893778 -4.1659703 -4.1198192 -4.0734406 -4.0696435 -4.1045918 -4.1489916 -4.1834521 -4.2152786 -4.24914 -4.2828379][-4.1775875 -4.1932268 -4.20061 -4.1953416 -4.1728606 -4.13111 -4.0709252 -4.0164232 -4.0220394 -4.0806231 -4.14475 -4.1876807 -4.2198558 -4.2532425 -4.2865987][-4.2078619 -4.2179604 -4.2133856 -4.1959214 -4.1612983 -4.1058922 -4.0316339 -3.9630556 -3.9745805 -4.0580115 -4.1439075 -4.1982503 -4.2323966 -4.2641582 -4.294733][-4.2171245 -4.2192616 -4.2059011 -4.1842 -4.1483526 -4.092082 -4.0146556 -3.9394009 -3.9545543 -4.0505176 -4.1475987 -4.2106204 -4.245986 -4.2739253 -4.3009977][-4.2248068 -4.2211008 -4.2034307 -4.1831322 -4.156065 -4.1154685 -4.0575857 -4.0000334 -4.0137982 -4.0928349 -4.1757183 -4.2303462 -4.2587938 -4.2813621 -4.3058033][-4.2245932 -4.216414 -4.1971784 -4.1813431 -4.16851 -4.1522493 -4.1229787 -4.0865269 -4.0919933 -4.1415353 -4.2002664 -4.2407088 -4.261692 -4.2821259 -4.3064842][-4.2206106 -4.2071223 -4.1873574 -4.1767478 -4.1755629 -4.1773009 -4.1689258 -4.1472034 -4.1450853 -4.1733494 -4.2147679 -4.2442355 -4.2616358 -4.282979 -4.3084216][-4.2139168 -4.1989312 -4.18115 -4.1769629 -4.1844521 -4.1970625 -4.2003336 -4.1858053 -4.1810284 -4.1996627 -4.2301068 -4.2523804 -4.269362 -4.2919006 -4.3165774][-4.2168584 -4.2053089 -4.1915021 -4.1910996 -4.2016892 -4.2167139 -4.2225285 -4.2103624 -4.2049446 -4.2205844 -4.2458477 -4.2658472 -4.28521 -4.3078375 -4.3291521][-4.2309079 -4.2258282 -4.2171106 -4.2174735 -4.2261848 -4.2380471 -4.2408857 -4.2290344 -4.22575 -4.2419963 -4.2655969 -4.2859244 -4.3058238 -4.3250761 -4.3415742][-4.2553234 -4.2552576 -4.2510142 -4.2506361 -4.2552481 -4.2624512 -4.2616663 -4.2514906 -4.2517056 -4.2673554 -4.2886076 -4.3077464 -4.3245206 -4.3381472 -4.3497]]...]
INFO - root - 2017-12-05 10:41:46.325081: step 1010, loss = 2.05, batch loss = 2.00 (13.1 examples/sec; 0.609 sec/batch; 56h:06m:52s remains)
INFO - root - 2017-12-05 10:41:52.611282: step 1020, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.649 sec/batch; 59h:48m:00s remains)
INFO - root - 2017-12-05 10:41:58.857624: step 1030, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 58h:46m:51s remains)
INFO - root - 2017-12-05 10:42:05.164214: step 1040, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 57h:00m:25s remains)
INFO - root - 2017-12-05 10:42:11.334432: step 1050, loss = 2.10, batch loss = 2.04 (12.7 examples/sec; 0.632 sec/batch; 58h:08m:48s remains)
INFO - root - 2017-12-05 10:42:17.611854: step 1060, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 58h:22m:46s remains)
INFO - root - 2017-12-05 10:42:23.906249: step 1070, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 58h:05m:20s remains)
INFO - root - 2017-12-05 10:42:30.235811: step 1080, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 60h:10m:25s remains)
INFO - root - 2017-12-05 10:42:36.527356: step 1090, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 58h:16m:19s remains)
INFO - root - 2017-12-05 10:42:42.764986: step 1100, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.619 sec/batch; 56h:57m:40s remains)
2017-12-05 10:42:43.284398: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.188056 -4.1549954 -4.1183577 -4.10638 -4.1117 -4.1128731 -4.1398997 -4.1917014 -4.2318783 -4.2555437 -4.2611041 -4.2620673 -4.27992 -4.2937951 -4.2979531][-4.199295 -4.1724439 -4.1320438 -4.0997272 -4.0707612 -4.0442686 -4.0696635 -4.1363354 -4.2008123 -4.2429442 -4.2552385 -4.2574344 -4.2748952 -4.2922206 -4.3007779][-4.2039285 -4.1812749 -4.1420069 -4.0986347 -4.0444155 -3.998703 -4.023159 -4.092989 -4.1703138 -4.2277331 -4.250494 -4.2574048 -4.2754159 -4.2937193 -4.3035436][-4.204772 -4.187006 -4.1565208 -4.1189852 -4.0669327 -4.0190177 -4.0343456 -4.0919981 -4.1606836 -4.2186785 -4.2436771 -4.2586255 -4.2818036 -4.2987776 -4.3065853][-4.1924672 -4.17948 -4.1567349 -4.1298981 -4.091598 -4.0540972 -4.0583067 -4.0940056 -4.1417193 -4.1943679 -4.2253537 -4.2503223 -4.2818003 -4.30255 -4.3096142][-4.1718783 -4.1607776 -4.1391072 -4.1168327 -4.0878363 -4.0578451 -4.0479794 -4.0579424 -4.0908837 -4.1427617 -4.18577 -4.2265587 -4.2672305 -4.2959485 -4.308991][-4.1661968 -4.1562114 -4.1337109 -4.1126332 -4.0907192 -4.0598168 -4.0290918 -4.009964 -4.0293846 -4.0856438 -4.1393895 -4.1937351 -4.2456985 -4.284801 -4.3061914][-4.1881084 -4.1793709 -4.1571021 -4.136097 -4.1160583 -4.0867052 -4.04943 -4.0185175 -4.0269709 -4.0786171 -4.1336894 -4.1894441 -4.2428131 -4.2857065 -4.3109736][-4.2174935 -4.2114496 -4.1913471 -4.169467 -4.1465821 -4.118361 -4.0863943 -4.0628142 -4.0660415 -4.1021986 -4.1509509 -4.2041693 -4.2536988 -4.2943788 -4.3202453][-4.225347 -4.2216139 -4.2082677 -4.1903825 -4.1667771 -4.1372809 -4.1080647 -4.08932 -4.0867796 -4.1051288 -4.1472073 -4.2032533 -4.2554574 -4.2963443 -4.3244395][-4.2136889 -4.2112489 -4.2075844 -4.1989255 -4.1832643 -4.15979 -4.1330624 -4.1133103 -4.1015344 -4.1024208 -4.134469 -4.1886015 -4.2419477 -4.2866187 -4.3181648][-4.2170696 -4.2149925 -4.2171316 -4.2172232 -4.210762 -4.1948619 -4.1724687 -4.1518059 -4.1309776 -4.1153574 -4.13285 -4.1786561 -4.2284689 -4.2740388 -4.308022][-4.2410679 -4.2350888 -4.2339067 -4.2337623 -4.2315693 -4.2245131 -4.2124252 -4.1963716 -4.1707697 -4.1455522 -4.1510878 -4.1844449 -4.2265611 -4.26558 -4.2984824][-4.2596512 -4.2506537 -4.24294 -4.2392921 -4.2390828 -4.2382631 -4.236248 -4.225872 -4.1978621 -4.1680164 -4.1639843 -4.186563 -4.2233634 -4.255445 -4.285656][-4.2597914 -4.2500081 -4.2382784 -4.2309208 -4.2296143 -4.2320652 -4.2364054 -4.2296534 -4.2034593 -4.1754217 -4.1675768 -4.1847315 -4.2183228 -4.246963 -4.2739415]]...]
INFO - root - 2017-12-05 10:42:49.457394: step 1110, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.624 sec/batch; 57h:24m:42s remains)
INFO - root - 2017-12-05 10:42:55.660423: step 1120, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 57h:15m:39s remains)
INFO - root - 2017-12-05 10:43:01.802665: step 1130, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 58h:25m:02s remains)
INFO - root - 2017-12-05 10:43:08.050439: step 1140, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 59h:10m:44s remains)
INFO - root - 2017-12-05 10:43:14.340872: step 1150, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.604 sec/batch; 55h:37m:42s remains)
INFO - root - 2017-12-05 10:43:20.551892: step 1160, loss = 2.07, batch loss = 2.01 (13.2 examples/sec; 0.605 sec/batch; 55h:38m:49s remains)
INFO - root - 2017-12-05 10:43:26.806328: step 1170, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 57h:23m:32s remains)
INFO - root - 2017-12-05 10:43:33.093715: step 1180, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.651 sec/batch; 59h:52m:54s remains)
INFO - root - 2017-12-05 10:43:39.651610: step 1190, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.754 sec/batch; 69h:25m:08s remains)
INFO - root - 2017-12-05 10:43:48.220215: step 1200, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.816 sec/batch; 75h:04m:13s remains)
2017-12-05 10:43:48.868570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2324486 -4.2322168 -4.2365742 -4.2426848 -4.2480354 -4.2506557 -4.2537718 -4.2554793 -4.2568192 -4.2564735 -4.263299 -4.2749181 -4.2898736 -4.29612 -4.2938447][-4.2256241 -4.2210622 -4.2278748 -4.2400393 -4.2493019 -4.2533317 -4.25621 -4.2597 -4.2643871 -4.2651105 -4.27017 -4.2783594 -4.2927551 -4.2967076 -4.2951474][-4.1971369 -4.1901984 -4.1986113 -4.2163315 -4.2290082 -4.2374063 -4.2436419 -4.2505856 -4.2598519 -4.2608461 -4.2624145 -4.2650161 -4.2767491 -4.280066 -4.2772884][-4.1728649 -4.159318 -4.1663771 -4.192327 -4.2115307 -4.2205367 -4.2256684 -4.23229 -4.2430263 -4.2505474 -4.2538252 -4.2560511 -4.2676916 -4.2723656 -4.2656307][-4.1648183 -4.1503987 -4.1583056 -4.1901541 -4.2100725 -4.2106519 -4.2058692 -4.20894 -4.2173057 -4.2326689 -4.2473683 -4.2594819 -4.2722163 -4.2770991 -4.2697797][-4.1744752 -4.1645193 -4.1705351 -4.198462 -4.2087612 -4.1932883 -4.16841 -4.1626239 -4.174262 -4.2009754 -4.233438 -4.2568398 -4.2749057 -4.2818213 -4.2781987][-4.1727304 -4.1724792 -4.1801167 -4.1915345 -4.1847215 -4.1459708 -4.0877934 -4.0590239 -4.0846739 -4.1383924 -4.1923275 -4.2303367 -4.261342 -4.2780466 -4.2755461][-4.1661439 -4.1751342 -4.1858172 -4.1800151 -4.1467977 -4.075923 -3.97179 -3.9010596 -3.9562685 -4.0598159 -4.1428533 -4.1947808 -4.237658 -4.2648573 -4.2626452][-4.165864 -4.1734219 -4.1814585 -4.1686716 -4.122036 -4.0386367 -3.9188132 -3.8276088 -3.9014597 -4.0327229 -4.1235485 -4.1769462 -4.2176609 -4.2459965 -4.2474051][-4.1611724 -4.1633644 -4.1693521 -4.1609464 -4.1245861 -4.0649986 -3.9934273 -3.9465106 -3.9924195 -4.0842285 -4.150166 -4.1892686 -4.2179575 -4.2407675 -4.243907][-4.1737018 -4.174243 -4.1796756 -4.1782694 -4.1611433 -4.1365786 -4.1151681 -4.1076293 -4.1257577 -4.1682038 -4.2049913 -4.2236767 -4.2361407 -4.2506289 -4.2569013][-4.2047744 -4.2040496 -4.2103262 -4.2135124 -4.210156 -4.2086568 -4.2130604 -4.2242389 -4.229836 -4.2408261 -4.2515278 -4.2546377 -4.2558908 -4.2621241 -4.2676263][-4.2424383 -4.2393556 -4.2432404 -4.2456579 -4.2483768 -4.2565064 -4.2676349 -4.2786965 -4.2745404 -4.2653384 -4.2595139 -4.2563739 -4.2549386 -4.2553973 -4.2580018][-4.2667871 -4.2618761 -4.2617445 -4.2624993 -4.26583 -4.2696128 -4.2740841 -4.2770643 -4.2658415 -4.2487326 -4.2376418 -4.234273 -4.2371569 -4.2395325 -4.237772][-4.279798 -4.2726049 -4.2710433 -4.2722898 -4.2740579 -4.2717729 -4.2697716 -4.2684 -4.2562943 -4.240304 -4.2298179 -4.2249751 -4.2288508 -4.2374659 -4.2383914]]...]
INFO - root - 2017-12-05 10:43:57.030745: step 1210, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 75h:38m:31s remains)
INFO - root - 2017-12-05 10:44:05.258209: step 1220, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 76h:42m:48s remains)
INFO - root - 2017-12-05 10:44:13.465975: step 1230, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 75h:39m:42s remains)
INFO - root - 2017-12-05 10:44:21.955815: step 1240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 79h:14m:56s remains)
INFO - root - 2017-12-05 10:44:30.283976: step 1250, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 76h:28m:06s remains)
INFO - root - 2017-12-05 10:44:38.513245: step 1260, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.817 sec/batch; 75h:11m:28s remains)
INFO - root - 2017-12-05 10:44:46.778536: step 1270, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 77h:05m:11s remains)
INFO - root - 2017-12-05 10:44:55.089374: step 1280, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 77h:37m:22s remains)
INFO - root - 2017-12-05 10:45:03.396671: step 1290, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.737 sec/batch; 67h:46m:11s remains)
INFO - root - 2017-12-05 10:45:11.641139: step 1300, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 76h:57m:46s remains)
2017-12-05 10:45:12.300064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.174376 -4.1695795 -4.1507187 -4.1418848 -4.1361675 -4.1205473 -4.1257992 -4.1483326 -4.1724677 -4.19701 -4.2154522 -4.227622 -4.2341051 -4.2184329 -4.1932974][-4.165905 -4.1673045 -4.1562266 -4.1468821 -4.1366382 -4.1094646 -4.1028943 -4.1247149 -4.1534534 -4.1827846 -4.2091956 -4.2323456 -4.2434855 -4.2366424 -4.2165895][-4.1441154 -4.1471782 -4.1430736 -4.1364093 -4.1271214 -4.1007776 -4.085917 -4.1025391 -4.1300864 -4.1619687 -4.1954317 -4.2258115 -4.2417684 -4.2433467 -4.2289295][-4.1246858 -4.1237864 -4.1198568 -4.110363 -4.1005549 -4.0801506 -4.0618153 -4.0672 -4.0887232 -4.121634 -4.1585827 -4.1927462 -4.2105508 -4.2179904 -4.206439][-4.1027226 -4.0974264 -4.090663 -4.0754724 -4.0616097 -4.0450368 -4.022006 -4.0132403 -4.0360222 -4.0716338 -4.1057458 -4.1390672 -4.153666 -4.1591325 -4.1496086][-4.0542412 -4.0402317 -4.0281367 -4.0066023 -3.9905744 -3.9784827 -3.950115 -3.9242408 -3.9476378 -3.991775 -4.0310397 -4.0673656 -4.0865922 -4.0975914 -4.0920763][-3.9921312 -3.9638724 -3.9448521 -3.9251151 -3.9127989 -3.9002767 -3.8523269 -3.7998126 -3.8273563 -3.8946939 -3.944118 -3.9861121 -4.0125136 -4.0298119 -4.0287333][-3.9643703 -3.9266295 -3.9048069 -3.8930049 -3.8822851 -3.8619947 -3.7975073 -3.7274137 -3.7532327 -3.8318655 -3.8836107 -3.9195487 -3.9421432 -3.9621692 -3.9718266][-3.9794302 -3.9466844 -3.9333515 -3.931149 -3.9213033 -3.899662 -3.8466496 -3.7924886 -3.8072197 -3.8625269 -3.8934615 -3.9092221 -3.9211898 -3.9388452 -3.9504983][-4.0189137 -3.9987247 -3.9956677 -3.9978428 -3.9881532 -3.9713781 -3.9375551 -3.9051061 -3.9092689 -3.9317164 -3.9374914 -3.9371378 -3.9415474 -3.9533873 -3.9584243][-4.0712447 -4.0597725 -4.0610962 -4.0638113 -4.0587254 -4.0528011 -4.0350366 -4.0139041 -4.0053658 -4.0032072 -3.9913011 -3.981884 -3.9847949 -3.9945438 -3.9909754][-4.1152673 -4.1072187 -4.1064672 -4.1073575 -4.1052175 -4.1056771 -4.0968828 -4.0817156 -4.0696988 -4.058773 -4.0439248 -4.0327129 -4.036612 -4.0450659 -4.0373][-4.1462216 -4.1374831 -4.130661 -4.1266613 -4.124742 -4.12924 -4.1268487 -4.11815 -4.1096983 -4.1007681 -4.0907445 -4.0827174 -4.0844188 -4.088922 -4.0825758][-4.1716475 -4.1601334 -4.1464572 -4.136991 -4.1348777 -4.1417394 -4.1417823 -4.1351643 -4.129117 -4.1231856 -4.1180277 -4.1148658 -4.1166177 -4.1200743 -4.1182203][-4.192481 -4.18181 -4.1660671 -4.1545663 -4.1518984 -4.1579528 -4.1588879 -4.1537342 -4.14947 -4.1444869 -4.1391225 -4.1360154 -4.136857 -4.1387072 -4.1397433]]...]
INFO - root - 2017-12-05 10:45:20.530528: step 1310, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 76h:05m:11s remains)
INFO - root - 2017-12-05 10:45:28.896747: step 1320, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 77h:11m:58s remains)
INFO - root - 2017-12-05 10:45:37.222834: step 1330, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 75h:27m:17s remains)
INFO - root - 2017-12-05 10:45:45.624842: step 1340, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 76h:09m:38s remains)
INFO - root - 2017-12-05 10:45:53.890264: step 1350, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.829 sec/batch; 76h:15m:29s remains)
INFO - root - 2017-12-05 10:46:02.202582: step 1360, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 75h:59m:32s remains)
INFO - root - 2017-12-05 10:46:10.591058: step 1370, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 76h:59m:34s remains)
INFO - root - 2017-12-05 10:46:18.865138: step 1380, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.817 sec/batch; 75h:09m:31s remains)
INFO - root - 2017-12-05 10:46:27.146391: step 1390, loss = 2.04, batch loss = 1.98 (10.4 examples/sec; 0.770 sec/batch; 70h:49m:21s remains)
INFO - root - 2017-12-05 10:46:35.341097: step 1400, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.812 sec/batch; 74h:43m:27s remains)
2017-12-05 10:46:36.024322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.201973 -4.2094612 -4.2123904 -4.2128897 -4.2080455 -4.201632 -4.195776 -4.1888189 -4.1761322 -4.1653256 -4.1543913 -4.1521759 -4.1547394 -4.1656079 -4.184927][-4.1886783 -4.2004585 -4.2064509 -4.2084894 -4.2036119 -4.1970634 -4.1910429 -4.1831784 -4.1716061 -4.1657 -4.1597915 -4.1596556 -4.15994 -4.1658797 -4.1808138][-4.1794724 -4.1912236 -4.1949615 -4.19595 -4.18937 -4.1809955 -4.1753583 -4.1685882 -4.1625886 -4.164341 -4.1662874 -4.16975 -4.1681123 -4.1701517 -4.1808991][-4.166955 -4.17079 -4.167057 -4.1634812 -4.1550012 -4.1432662 -4.137033 -4.1368918 -4.143445 -4.1585045 -4.1690259 -4.1749272 -4.172462 -4.1704698 -4.1753483][-4.1539326 -4.1463337 -4.1329513 -4.1221347 -4.10673 -4.088737 -4.076561 -4.0803828 -4.0997739 -4.132288 -4.1541834 -4.1657271 -4.1681209 -4.1682668 -4.1708922][-4.1579189 -4.1394339 -4.1138082 -4.0872445 -4.053638 -4.020442 -3.9989147 -4.0030584 -4.0349689 -4.0916152 -4.1335359 -4.1564732 -4.1678662 -4.173245 -4.176136][-4.1892591 -4.1629453 -4.1249828 -4.0799055 -4.0222096 -3.9666548 -3.9280698 -3.9286308 -3.970114 -4.0515871 -4.1199179 -4.1631131 -4.1888833 -4.203074 -4.2078638][-4.2232666 -4.1920948 -4.148428 -4.0953031 -4.0303063 -3.9681034 -3.9249256 -3.9235573 -3.960994 -4.0483336 -4.1280327 -4.1821547 -4.2173095 -4.2395377 -4.2512045][-4.253294 -4.2204361 -4.1749043 -4.1218939 -4.061502 -4.0066137 -3.9747188 -3.9790998 -4.0097528 -4.08367 -4.1516471 -4.198432 -4.2311053 -4.2536268 -4.2683964][-4.2784357 -4.2509842 -4.211484 -4.1637568 -4.1118875 -4.0672259 -4.045815 -4.0548453 -4.0802689 -4.1353335 -4.1821594 -4.2135549 -4.2347751 -4.2486925 -4.2581787][-4.2897034 -4.2736225 -4.2449884 -4.210053 -4.1704593 -4.1348386 -4.1161642 -4.1203666 -4.1343637 -4.1664839 -4.1919608 -4.2116652 -4.2259188 -4.2340717 -4.2393713][-4.2851214 -4.2862082 -4.2707858 -4.2482443 -4.2220473 -4.196115 -4.1772919 -4.169395 -4.1656218 -4.1735806 -4.1823392 -4.1961513 -4.2124867 -4.2238574 -4.2310743][-4.2570605 -4.2749515 -4.277349 -4.2706008 -4.2575159 -4.2391396 -4.2209215 -4.2029753 -4.1861072 -4.1789517 -4.178093 -4.1895971 -4.2092519 -4.2262964 -4.2385821][-4.210146 -4.2375803 -4.25346 -4.2594266 -4.257875 -4.2497272 -4.23716 -4.2195449 -4.2013011 -4.1891561 -4.1845713 -4.1966562 -4.2169414 -4.2342796 -4.2474947][-4.1669636 -4.1945548 -4.2174478 -4.232574 -4.2390184 -4.2410312 -4.2367144 -4.2266388 -4.2146568 -4.203589 -4.1982837 -4.2084856 -4.226439 -4.2396059 -4.2496467]]...]
INFO - root - 2017-12-05 10:46:44.333328: step 1410, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 76h:00m:41s remains)
INFO - root - 2017-12-05 10:46:52.542758: step 1420, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.801 sec/batch; 73h:39m:44s remains)
INFO - root - 2017-12-05 10:47:00.784178: step 1430, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 76h:51m:33s remains)
INFO - root - 2017-12-05 10:47:09.028805: step 1440, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.813 sec/batch; 74h:44m:54s remains)
INFO - root - 2017-12-05 10:47:17.216140: step 1450, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 0.803 sec/batch; 73h:49m:02s remains)
INFO - root - 2017-12-05 10:47:25.506854: step 1460, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.822 sec/batch; 75h:37m:46s remains)
INFO - root - 2017-12-05 10:47:33.840835: step 1470, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 76h:18m:49s remains)
INFO - root - 2017-12-05 10:47:47.754579: step 1480, loss = 2.08, batch loss = 2.02 (1.3 examples/sec; 6.350 sec/batch; 583h:53m:09s remains)
INFO - root - 2017-12-05 10:47:55.984919: step 1490, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 76h:06m:27s remains)
INFO - root - 2017-12-05 10:48:04.127488: step 1500, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 0.802 sec/batch; 73h:43m:13s remains)
2017-12-05 10:48:04.774758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2204905 -4.2213836 -4.2269082 -4.2307315 -4.2251096 -4.2239089 -4.2345729 -4.2544785 -4.2623463 -4.2486057 -4.2320051 -4.2429509 -4.2677746 -4.2714529 -4.2627559][-4.1551442 -4.1607585 -4.1856475 -4.2066603 -4.2007279 -4.18902 -4.1947274 -4.2185388 -4.2281175 -4.2154059 -4.203958 -4.2236576 -4.2575011 -4.2672577 -4.2591233][-4.0896611 -4.10322 -4.1487818 -4.1818881 -4.1695724 -4.143559 -4.1475644 -4.1750941 -4.1877728 -4.1808105 -4.1796227 -4.209444 -4.25029 -4.2649469 -4.2584767][-4.06915 -4.0860887 -4.1399922 -4.1705289 -4.1454821 -4.1046867 -4.1036243 -4.12902 -4.1458488 -4.1517992 -4.1647453 -4.2040176 -4.2495947 -4.2691092 -4.2671962][-4.0855179 -4.0996766 -4.1467023 -4.1630635 -4.1238418 -4.0765605 -4.0749745 -4.1014981 -4.1224656 -4.1391439 -4.1612477 -4.2066236 -4.2543182 -4.2779031 -4.2822461][-4.1086025 -4.1193614 -4.1550174 -4.1584697 -4.1105404 -4.0600953 -4.0557356 -4.0853081 -4.1169562 -4.1390166 -4.1608577 -4.2062707 -4.2559404 -4.2843156 -4.29142][-4.1239948 -4.1340208 -4.15902 -4.1517777 -4.0985408 -4.0409832 -4.0293217 -4.0574527 -4.1009722 -4.1284328 -4.1452923 -4.1863136 -4.2358136 -4.2736068 -4.2875938][-4.1189508 -4.1310778 -4.146944 -4.1316581 -4.0771 -4.0112042 -3.9824257 -4.0021224 -4.0547056 -4.089046 -4.1016645 -4.1359277 -4.1887512 -4.2403736 -4.2658262][-4.105288 -4.1219654 -4.1364951 -4.1223593 -4.07374 -4.0015593 -3.9517097 -3.9586978 -4.0122128 -4.0472622 -4.0496259 -4.0762177 -4.1314111 -4.1940756 -4.2295403][-4.1136885 -4.1280651 -4.1384935 -4.1271496 -4.088047 -4.0214334 -3.9662738 -3.9614747 -4.0033293 -4.0340962 -4.029985 -4.0515056 -4.1035957 -4.167356 -4.2067847][-4.1163306 -4.1263547 -4.1355176 -4.1319189 -4.1120825 -4.0651488 -4.0178623 -4.0048189 -4.0279021 -4.048902 -4.0450392 -4.0625453 -4.1063523 -4.1613 -4.19486][-4.1115985 -4.1231384 -4.1357765 -4.1431646 -4.1442957 -4.1209631 -4.0861092 -4.0695987 -4.0767097 -4.0858731 -4.0828366 -4.0966573 -4.1321263 -4.1761909 -4.2010202][-4.1263766 -4.1383495 -4.1528897 -4.1651354 -4.1782389 -4.1724067 -4.151762 -4.1383572 -4.13484 -4.1360888 -4.1342525 -4.144701 -4.1709208 -4.2055306 -4.2259912][-4.1625752 -4.1732035 -4.1892424 -4.2034197 -4.2205253 -4.2245197 -4.2150779 -4.2050815 -4.1982017 -4.1949339 -4.193294 -4.1998911 -4.2154975 -4.2397566 -4.2587538][-4.2168226 -4.22735 -4.242209 -4.256351 -4.271709 -4.2775764 -4.2722507 -4.2651882 -4.2598372 -4.2582984 -4.2577615 -4.2609715 -4.2693129 -4.2847152 -4.2992191]]...]
INFO - root - 2017-12-05 10:48:13.012141: step 1510, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 75h:40m:08s remains)
INFO - root - 2017-12-05 10:48:21.152549: step 1520, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 75h:42m:21s remains)
INFO - root - 2017-12-05 10:48:29.455189: step 1530, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 77h:43m:51s remains)
INFO - root - 2017-12-05 10:48:37.719877: step 1540, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 76h:35m:28s remains)
INFO - root - 2017-12-05 10:48:46.063309: step 1550, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 76h:28m:28s remains)
INFO - root - 2017-12-05 10:48:54.579043: step 1560, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 75h:42m:37s remains)
INFO - root - 2017-12-05 10:49:02.905108: step 1570, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 77h:31m:34s remains)
INFO - root - 2017-12-05 10:49:11.159819: step 1580, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 77h:04m:36s remains)
INFO - root - 2017-12-05 10:49:19.485789: step 1590, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 75h:59m:52s remains)
INFO - root - 2017-12-05 10:49:27.929274: step 1600, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 80h:21m:08s remains)
2017-12-05 10:49:28.655919: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1926727 -4.1923637 -4.1873994 -4.1940279 -4.2015419 -4.2122874 -4.2244258 -4.2292504 -4.2202182 -4.1947951 -4.1655436 -4.1552396 -4.158329 -4.1585536 -4.1541452][-4.1884818 -4.1993284 -4.2010832 -4.2076778 -4.2120404 -4.2140751 -4.2214527 -4.226388 -4.2146721 -4.1825476 -4.1444645 -4.1272674 -4.1304855 -4.1342096 -4.1362176][-4.1831307 -4.1966619 -4.198297 -4.2011523 -4.198051 -4.1901693 -4.1874375 -4.1875176 -4.1775422 -4.1519279 -4.1188955 -4.1046367 -4.1098204 -4.1183057 -4.1303468][-4.1770463 -4.1848068 -4.1797752 -4.1734176 -4.1598268 -4.1413908 -4.1250491 -4.1153331 -4.1119728 -4.102829 -4.0837312 -4.076838 -4.0877771 -4.1031165 -4.122057][-4.1581221 -4.1613984 -4.1526561 -4.1398931 -4.1179252 -4.0822635 -4.044673 -4.0248752 -4.0383129 -4.0570726 -4.0558286 -4.0549173 -4.0664182 -4.082921 -4.1006722][-4.1302428 -4.1297321 -4.1213284 -4.1075139 -4.0801687 -4.0256124 -3.9544497 -3.9158406 -3.9533086 -4.0126724 -4.0397773 -4.0489783 -4.0575166 -4.0654588 -4.07348][-4.0981803 -4.090816 -4.0850329 -4.0775495 -4.0536089 -3.9897964 -3.8866787 -3.8159666 -3.8721454 -3.9685767 -4.0278025 -4.0565491 -4.0663729 -4.0649676 -4.0646787][-4.0828967 -4.0637236 -4.0570612 -4.0584517 -4.0486279 -4.0000815 -3.90329 -3.8226876 -3.8601627 -3.9532301 -4.025701 -4.0711155 -4.0874934 -4.0819225 -4.0723476][-4.0975609 -4.0675049 -4.0550237 -4.0612178 -4.0665836 -4.0434089 -3.9793425 -3.916955 -3.9197628 -3.9706912 -4.0290637 -4.0798545 -4.1035571 -4.1000819 -4.08377][-4.1260204 -4.0924554 -4.0761642 -4.0816703 -4.0897441 -4.0800676 -4.0448413 -4.0056744 -3.9919322 -4.0063205 -4.0384045 -4.0818591 -4.1076345 -4.1090865 -4.0958166][-4.1531076 -4.123332 -4.1103721 -4.1171155 -4.1249619 -4.1196947 -4.1006021 -4.077837 -4.0612164 -4.0547566 -4.062294 -4.0888944 -4.1112537 -4.121284 -4.1194658][-4.1778884 -4.1586204 -4.1514163 -4.1620207 -4.1734281 -4.1715207 -4.1608272 -4.1478949 -4.133069 -4.1159039 -4.1064978 -4.1178279 -4.13599 -4.1504703 -4.1534066][-4.194592 -4.1866322 -4.1844158 -4.1963506 -4.2111316 -4.2137341 -4.2089434 -4.2013927 -4.1893539 -4.17055 -4.1571641 -4.1608744 -4.173831 -4.1851568 -4.1858172][-4.1942806 -4.1930575 -4.1923733 -4.2037354 -4.2220664 -4.2297583 -4.2278376 -4.2238808 -4.2171183 -4.2037873 -4.1929331 -4.1923347 -4.1988387 -4.2045431 -4.2018991][-4.1749206 -4.1737709 -4.1693296 -4.1787057 -4.2013488 -4.2139721 -4.2136536 -4.2116041 -4.2089124 -4.2009573 -4.1928816 -4.1892433 -4.1887474 -4.18891 -4.1812382]]...]
INFO - root - 2017-12-05 10:49:37.231466: step 1610, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 79h:23m:43s remains)
INFO - root - 2017-12-05 10:49:45.720804: step 1620, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 76h:24m:22s remains)
INFO - root - 2017-12-05 10:49:54.239067: step 1630, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 82h:38m:33s remains)
INFO - root - 2017-12-05 10:50:02.646497: step 1640, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 76h:47m:14s remains)
INFO - root - 2017-12-05 10:50:11.259859: step 1650, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 82h:13m:42s remains)
INFO - root - 2017-12-05 10:50:19.837367: step 1660, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 76h:34m:48s remains)
INFO - root - 2017-12-05 10:50:28.282950: step 1670, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 79h:29m:35s remains)
INFO - root - 2017-12-05 10:50:36.906651: step 1680, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 78h:19m:16s remains)
INFO - root - 2017-12-05 10:50:45.576139: step 1690, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.881 sec/batch; 80h:58m:43s remains)
INFO - root - 2017-12-05 10:50:53.984935: step 1700, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 80h:04m:57s remains)
2017-12-05 10:50:54.716060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2000341 -4.1969666 -4.1808071 -4.1585178 -4.1383662 -4.1263394 -4.1227546 -4.1270795 -4.1394138 -4.1640177 -4.1821709 -4.1824646 -4.17822 -4.1715145 -4.1658258][-4.2255683 -4.2299886 -4.2175455 -4.1932263 -4.1701956 -4.1599979 -4.1577024 -4.1564789 -4.1630211 -4.1827788 -4.1957517 -4.1929445 -4.1891365 -4.1820755 -4.1828165][-4.2550616 -4.261301 -4.2502341 -4.2242789 -4.1997223 -4.1885371 -4.1855793 -4.1809931 -4.186512 -4.2047844 -4.2180219 -4.21934 -4.2194118 -4.2136836 -4.2152429][-4.2744727 -4.2782693 -4.2649021 -4.241806 -4.2173085 -4.2001767 -4.1890855 -4.1805749 -4.1916451 -4.2166257 -4.2345862 -4.2412653 -4.24245 -4.2349792 -4.2347846][-4.2784328 -4.2797265 -4.263032 -4.2391806 -4.2103338 -4.1794071 -4.1494341 -4.1348724 -4.1555448 -4.1978846 -4.2275209 -4.2419987 -4.2455282 -4.239778 -4.2400107][-4.2696223 -4.2691631 -4.2490788 -4.221148 -4.1813345 -4.1205406 -4.0473118 -4.01513 -4.062192 -4.1426063 -4.1973248 -4.2261205 -4.2376304 -4.2410564 -4.2460079][-4.2529616 -4.2451329 -4.220911 -4.1887522 -4.137116 -4.0431776 -3.9119973 -3.8430727 -3.9219599 -4.054389 -4.1504893 -4.2016921 -4.2229667 -4.2348614 -4.2459254][-4.2262526 -4.2112269 -4.1857328 -4.1576414 -4.1115818 -4.0148244 -3.8684783 -3.7796416 -3.8631458 -4.0094242 -4.1237979 -4.1864109 -4.2113094 -4.2225161 -4.2328734][-4.2000313 -4.1826882 -4.1671867 -4.1570992 -4.1393456 -4.08174 -3.9900725 -3.9349613 -3.9745491 -4.06354 -4.1440535 -4.1901436 -4.2067227 -4.2113724 -4.2160959][-4.1902208 -4.1795707 -4.1757174 -4.1795454 -4.1820736 -4.1611848 -4.1241074 -4.1034079 -4.1145706 -4.1487446 -4.1829247 -4.1993866 -4.2026677 -4.1994681 -4.2009954][-4.2009325 -4.1989827 -4.1953235 -4.1987262 -4.2048559 -4.2020054 -4.1942296 -4.1926975 -4.1945558 -4.1984591 -4.2031794 -4.2013617 -4.1966028 -4.186902 -4.1830177][-4.2202997 -4.2261834 -4.2221828 -4.2187524 -4.2206879 -4.2233019 -4.2244239 -4.2276525 -4.2223835 -4.2131872 -4.2083387 -4.2018356 -4.1931672 -4.1776524 -4.1648903][-4.2394161 -4.2515168 -4.2476068 -4.2411075 -4.2420855 -4.2444906 -4.244606 -4.2447548 -4.2349029 -4.2212367 -4.2124696 -4.2041554 -4.1943846 -4.176033 -4.1565351][-4.2510509 -4.2624269 -4.2573495 -4.2510672 -4.2511759 -4.2525396 -4.2524142 -4.249311 -4.2374005 -4.2209797 -4.2102714 -4.2039804 -4.2014456 -4.1922426 -4.176662][-4.2484241 -4.25453 -4.2489595 -4.2437277 -4.2451496 -4.2483406 -4.249373 -4.24613 -4.2326264 -4.2146125 -4.2041011 -4.2041678 -4.214448 -4.2172146 -4.2031159]]...]
INFO - root - 2017-12-05 10:51:03.189924: step 1710, loss = 2.11, batch loss = 2.05 (9.4 examples/sec; 0.854 sec/batch; 78h:30m:35s remains)
INFO - root - 2017-12-05 10:51:11.820403: step 1720, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 79h:50m:07s remains)
INFO - root - 2017-12-05 10:51:20.283488: step 1730, loss = 2.05, batch loss = 1.99 (10.2 examples/sec; 0.781 sec/batch; 71h:47m:28s remains)
INFO - root - 2017-12-05 10:51:28.914671: step 1740, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 80h:02m:58s remains)
INFO - root - 2017-12-05 10:51:37.482854: step 1750, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 76h:53m:13s remains)
INFO - root - 2017-12-05 10:51:45.979255: step 1760, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 76h:53m:08s remains)
INFO - root - 2017-12-05 10:51:54.535866: step 1770, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.874 sec/batch; 80h:19m:57s remains)
INFO - root - 2017-12-05 10:52:03.033559: step 1780, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 78h:05m:30s remains)
INFO - root - 2017-12-05 10:52:11.549871: step 1790, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.865 sec/batch; 79h:25m:11s remains)
INFO - root - 2017-12-05 10:52:20.045387: step 1800, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 78h:38m:16s remains)
2017-12-05 10:52:20.832451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1483612 -4.1501918 -4.1503954 -4.1422439 -4.1320548 -4.1385632 -4.1575942 -4.1867514 -4.2016191 -4.2077322 -4.2076683 -4.2056489 -4.2040477 -4.19851 -4.1850777][-4.1254163 -4.1269355 -4.1305389 -4.1211767 -4.1107192 -4.12551 -4.1487246 -4.1749029 -4.1906533 -4.19577 -4.1905408 -4.1858649 -4.1834493 -4.1794028 -4.1658182][-4.1199136 -4.111711 -4.10842 -4.0974054 -4.0894723 -4.1107135 -4.14242 -4.1736035 -4.1939383 -4.1950059 -4.1825881 -4.1730552 -4.1689177 -4.1685038 -4.1613636][-4.1196256 -4.10286 -4.0899658 -4.07495 -4.0681553 -4.0921178 -4.1332808 -4.1753416 -4.1980939 -4.1952877 -4.1763921 -4.1639256 -4.1567383 -4.1545463 -4.1553545][-4.1395583 -4.1261768 -4.1108003 -4.0800385 -4.0606265 -4.0721684 -4.1145945 -4.1684427 -4.2028313 -4.2021508 -4.1790895 -4.1608105 -4.1510444 -4.1430588 -4.1459041][-4.1647921 -4.1596956 -4.1490664 -4.1019311 -4.0552506 -4.0396929 -4.0731869 -4.1369205 -4.1886315 -4.1983671 -4.1768308 -4.1558213 -4.1344066 -4.1100917 -4.1081872][-4.1765728 -4.1822705 -4.1772189 -4.1268167 -4.0576172 -4.0136662 -4.0248241 -4.085815 -4.1506033 -4.1746063 -4.1593409 -4.1354384 -4.0979385 -4.0515351 -4.0390792][-4.1781039 -4.192894 -4.1905527 -4.14213 -4.0545282 -3.9744935 -3.9564161 -4.0067348 -4.0826283 -4.125937 -4.1259494 -4.1001639 -4.0489173 -3.9798956 -3.950809][-4.1592245 -4.1795492 -4.17622 -4.1271863 -4.0282073 -3.9068651 -3.8515067 -3.8984826 -3.999212 -4.0685644 -4.0904856 -4.0718217 -4.0209742 -3.9365072 -3.8866014][-4.1188231 -4.1428676 -4.1452713 -4.1016655 -4.0079017 -3.86615 -3.7687962 -3.8067484 -3.9338558 -4.026629 -4.0679812 -4.0673904 -4.0327439 -3.952621 -3.8961926][-4.0740652 -4.0990944 -4.110353 -4.0821443 -4.021039 -3.9117343 -3.8082819 -3.8136411 -3.9231212 -4.0210032 -4.0662832 -4.0782866 -4.0698495 -4.0224934 -3.9827054][-4.0567684 -4.0820403 -4.099884 -4.0912728 -4.067625 -4.0228143 -3.9647369 -3.9425228 -3.993289 -4.0571666 -4.0939369 -4.1079674 -4.1158981 -4.0983152 -4.0780621][-4.0790558 -4.0943666 -4.1122046 -4.1151748 -4.1146164 -4.1136217 -4.1042852 -4.0894022 -4.0994163 -4.120822 -4.142488 -4.1560688 -4.1678967 -4.1648717 -4.1560616][-4.1396713 -4.1392636 -4.1491323 -4.1534238 -4.1604142 -4.1745796 -4.1853857 -4.1859946 -4.1863236 -4.1891737 -4.1999493 -4.2133188 -4.2301331 -4.2369013 -4.2309055][-4.2136068 -4.2054834 -4.2055063 -4.2098145 -4.2224193 -4.2376189 -4.2480383 -4.2546592 -4.2585969 -4.2602477 -4.2665586 -4.2755013 -4.2890177 -4.2968121 -4.2893467]]...]
INFO - root - 2017-12-05 10:52:29.379104: step 1810, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 79h:23m:22s remains)
INFO - root - 2017-12-05 10:52:37.841417: step 1820, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 78h:08m:47s remains)
INFO - root - 2017-12-05 10:52:46.317710: step 1830, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 79h:32m:34s remains)
INFO - root - 2017-12-05 10:52:54.761086: step 1840, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 77h:51m:17s remains)
INFO - root - 2017-12-05 10:53:03.250830: step 1850, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 82h:31m:03s remains)
INFO - root - 2017-12-05 10:53:11.855031: step 1860, loss = 2.01, batch loss = 1.95 (9.2 examples/sec; 0.869 sec/batch; 79h:50m:27s remains)
INFO - root - 2017-12-05 10:53:20.367709: step 1870, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 77h:46m:01s remains)
INFO - root - 2017-12-05 10:53:28.903987: step 1880, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 78h:21m:47s remains)
INFO - root - 2017-12-05 10:53:37.398854: step 1890, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 78h:16m:26s remains)
INFO - root - 2017-12-05 10:53:45.832088: step 1900, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:49m:57s remains)
2017-12-05 10:53:46.552507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1768107 -4.159348 -4.1526012 -4.1709838 -4.1935968 -4.205204 -4.2036242 -4.2075577 -4.2202935 -4.2265477 -4.2220454 -4.2129669 -4.204452 -4.2127185 -4.2281642][-4.1455307 -4.1142211 -4.0971475 -4.1189246 -4.1517348 -4.1695805 -4.1672821 -4.1725216 -4.1903119 -4.1972885 -4.199759 -4.2042251 -4.2017145 -4.2048335 -4.2146411][-4.1322632 -4.0914373 -4.0583797 -4.0595479 -4.0896454 -4.1148391 -4.1232529 -4.1326442 -4.1535974 -4.1621385 -4.1710119 -4.1832743 -4.1870151 -4.1919966 -4.2004943][-4.1179991 -4.0789776 -4.0377831 -4.0173626 -4.0360107 -4.066083 -4.0891776 -4.1080623 -4.1309347 -4.1396317 -4.1512437 -4.1683731 -4.1793981 -4.1878366 -4.1990266][-4.0968113 -4.07202 -4.0343256 -3.9996855 -4.0008993 -4.0241442 -4.0527372 -4.0836134 -4.1135077 -4.1211944 -4.1298671 -4.1458611 -4.159061 -4.1722183 -4.189661][-4.0957813 -4.0827708 -4.0479345 -4.006453 -3.9877293 -3.99502 -4.0176296 -4.05007 -4.0845103 -4.0933285 -4.0979347 -4.1130309 -4.1300464 -4.1546087 -4.1830854][-4.1056423 -4.0971489 -4.069952 -4.0329857 -4.0030971 -3.9908009 -3.9957442 -4.0156264 -4.0503078 -4.0667377 -4.0724268 -4.0935264 -4.1190767 -4.1560106 -4.1923642][-4.1043863 -4.0990982 -4.0806589 -4.0572495 -4.0299411 -4.0044937 -3.9885638 -3.9918046 -4.02153 -4.0464158 -4.0591674 -4.087481 -4.1187692 -4.1610332 -4.1989627][-4.094974 -4.0876513 -4.0760541 -4.0630937 -4.0447927 -4.0170493 -3.9916527 -3.982512 -4.0039482 -4.0351524 -4.0586734 -4.0963159 -4.1335807 -4.1751122 -4.2059135][-4.1017761 -4.092206 -4.0799255 -4.0697641 -4.0582681 -4.0370193 -4.0155096 -4.0041084 -4.016129 -4.03895 -4.0624371 -4.105402 -4.1488519 -4.1851363 -4.2025747][-4.1374598 -4.1297836 -4.1174431 -4.1081696 -4.1009965 -4.0885816 -4.0765228 -4.0684953 -4.0682817 -4.0696683 -4.079226 -4.1135492 -4.1520405 -4.1815214 -4.1938634][-4.1840749 -4.1803637 -4.1711469 -4.16411 -4.1605473 -4.1547518 -4.14893 -4.1448755 -4.1415491 -4.1330352 -4.1286359 -4.145987 -4.1710553 -4.1916952 -4.2033815][-4.2353768 -4.2314062 -4.2250175 -4.2205873 -4.2189341 -4.2167768 -4.2141628 -4.2116575 -4.2085748 -4.1988244 -4.1894851 -4.19469 -4.2069354 -4.2171674 -4.2276621][-4.2789187 -4.2764678 -4.2729416 -4.2706714 -4.2695227 -4.2677369 -4.2655287 -4.2633877 -4.2608514 -4.2534909 -4.24595 -4.2445889 -4.2467518 -4.2484 -4.2558503][-4.294405 -4.2934103 -4.291255 -4.2891507 -4.2875309 -4.2857761 -4.2841153 -4.2825 -4.2805705 -4.2758636 -4.2712889 -4.2699618 -4.2701921 -4.2689214 -4.273284]]...]
INFO - root - 2017-12-05 10:53:55.175627: step 1910, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.836 sec/batch; 76h:46m:01s remains)
INFO - root - 2017-12-05 10:54:03.721161: step 1920, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 78h:48m:35s remains)
INFO - root - 2017-12-05 10:54:12.227999: step 1930, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 80h:24m:43s remains)
INFO - root - 2017-12-05 10:54:20.802471: step 1940, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 80h:52m:48s remains)
INFO - root - 2017-12-05 10:54:29.280210: step 1950, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 76h:09m:54s remains)
INFO - root - 2017-12-05 10:54:37.796518: step 1960, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 79h:20m:03s remains)
INFO - root - 2017-12-05 10:54:46.294821: step 1970, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.875 sec/batch; 80h:17m:40s remains)
INFO - root - 2017-12-05 10:54:54.803997: step 1980, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 77h:05m:12s remains)
INFO - root - 2017-12-05 10:55:03.264257: step 1990, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 76h:26m:13s remains)
INFO - root - 2017-12-05 10:55:11.788723: step 2000, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 77h:57m:43s remains)
2017-12-05 10:55:12.536386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3364377 -4.3354387 -4.3345971 -4.336165 -4.3377938 -4.3389564 -4.3403211 -4.3399911 -4.3363543 -4.3332696 -4.3329954 -4.3347511 -4.3384943 -4.3450012 -4.3488874][-4.32811 -4.3226161 -4.3172212 -4.3163791 -4.3166614 -4.317811 -4.3198233 -4.3173485 -4.3104668 -4.3055463 -4.3039894 -4.30646 -4.3139734 -4.3275247 -4.3391027][-4.3144755 -4.3021879 -4.2890196 -4.2828803 -4.2806978 -4.2823939 -4.2865829 -4.2825375 -4.272882 -4.264452 -4.2552376 -4.254477 -4.2654772 -4.2872152 -4.3117657][-4.3004174 -4.282722 -4.2640195 -4.2535253 -4.2488341 -4.2486782 -4.251091 -4.2420545 -4.2262797 -4.2065043 -4.1810846 -4.1720228 -4.1860261 -4.2186947 -4.26128][-4.2851458 -4.2632284 -4.2390661 -4.2235217 -4.2176895 -4.2177005 -4.2197137 -4.204217 -4.17823 -4.1438961 -4.1012516 -4.0837555 -4.1003294 -4.143961 -4.2046909][-4.2668605 -4.2372732 -4.2021537 -4.1768246 -4.1696877 -4.1711612 -4.1760926 -4.1556687 -4.1198788 -4.0727262 -4.0208268 -4.0034747 -4.0294042 -4.0913877 -4.1684737][-4.2415905 -4.1990705 -4.1498084 -4.1138725 -4.1018796 -4.1040983 -4.109272 -4.085906 -4.0491767 -4.0100107 -3.9731979 -3.9714694 -4.010407 -4.0845804 -4.1660028][-4.204895 -4.1455021 -4.0751314 -4.0190644 -3.9972713 -4.0057883 -4.0242696 -4.0071692 -3.9875255 -3.9791744 -3.9689121 -3.9830925 -4.0347562 -4.1123557 -4.1893291][-4.1611633 -4.0824738 -3.9831479 -3.9018483 -3.8776772 -3.9062166 -3.9531083 -3.9577773 -3.9671323 -3.990823 -4.0033293 -4.0314069 -4.0874996 -4.157454 -4.221446][-4.129076 -4.0365686 -3.9183369 -3.822541 -3.8100281 -3.8672924 -3.9465156 -3.9790545 -4.0123954 -4.0509543 -4.0721374 -4.1042175 -4.1527762 -4.205214 -4.2521768][-4.1395268 -4.0610042 -3.9692664 -3.8998675 -3.8971107 -3.9524894 -4.028379 -4.0729628 -4.1142306 -4.1502 -4.1665449 -4.1880403 -4.2203341 -4.2541189 -4.2844067][-4.2000136 -4.1472211 -4.0946169 -4.0601192 -4.0612435 -4.0973663 -4.1506257 -4.1883922 -4.2200017 -4.2420273 -4.2486362 -4.2589684 -4.2757044 -4.2943764 -4.3117723][-4.2707558 -4.2415848 -4.2166853 -4.2019677 -4.2041869 -4.2232761 -4.2513676 -4.2728248 -4.290174 -4.3009124 -4.3031697 -4.3078132 -4.3160424 -4.32563 -4.3337092][-4.3222256 -4.3096 -4.299962 -4.2925487 -4.2908115 -4.2977781 -4.3097563 -4.3190584 -4.3250613 -4.3287897 -4.3300686 -4.3334656 -4.3376951 -4.3417783 -4.3445649][-4.3448462 -4.3429227 -4.3406324 -4.3379064 -4.3351889 -4.3363757 -4.3386092 -4.3403316 -4.3405881 -4.3402929 -4.3402977 -4.3414464 -4.3434072 -4.3451676 -4.3461447]]...]
INFO - root - 2017-12-05 10:55:21.135305: step 2010, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 78h:42m:11s remains)
INFO - root - 2017-12-05 10:55:29.601364: step 2020, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 76h:13m:03s remains)
INFO - root - 2017-12-05 10:55:38.074975: step 2030, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 76h:19m:43s remains)
INFO - root - 2017-12-05 10:55:46.532219: step 2040, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 80h:06m:36s remains)
INFO - root - 2017-12-05 10:55:54.924691: step 2050, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.785 sec/batch; 72h:05m:41s remains)
INFO - root - 2017-12-05 10:56:03.442486: step 2060, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.813 sec/batch; 74h:36m:34s remains)
INFO - root - 2017-12-05 10:56:11.986232: step 2070, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 78h:41m:33s remains)
INFO - root - 2017-12-05 10:56:20.568070: step 2080, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 78h:28m:26s remains)
INFO - root - 2017-12-05 10:56:29.091695: step 2090, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 79h:26m:56s remains)
INFO - root - 2017-12-05 10:56:37.515798: step 2100, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 79h:13m:27s remains)
2017-12-05 10:56:38.294190: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1335645 -4.1168871 -4.1016636 -4.1040545 -4.120903 -4.127882 -4.133296 -4.145822 -4.1634321 -4.1825061 -4.2106514 -4.2362518 -4.248106 -4.2549434 -4.2565112][-4.1381493 -4.1302624 -4.1277995 -4.1365309 -4.153429 -4.1598196 -4.1625175 -4.1730452 -4.1953745 -4.2226791 -4.2513075 -4.2663317 -4.2686539 -4.2724071 -4.27589][-4.1597896 -4.1531487 -4.1545672 -4.1623936 -4.1752858 -4.1797242 -4.1774964 -4.1847415 -4.2082758 -4.2410622 -4.2699165 -4.2832737 -4.2869887 -4.28914 -4.2875834][-4.1830115 -4.1679759 -4.1622849 -4.1682248 -4.1755781 -4.1718144 -4.1622076 -4.1667557 -4.1891222 -4.2215037 -4.2532635 -4.2735796 -4.2861896 -4.2897253 -4.2884765][-4.1925483 -4.1698909 -4.1575708 -4.1608467 -4.1589956 -4.13672 -4.1108665 -4.1060405 -4.1285868 -4.1702642 -4.2159796 -4.24601 -4.2683907 -4.2798982 -4.2870078][-4.1982012 -4.1802735 -4.1649122 -4.1598015 -4.1421428 -4.0919533 -4.0342546 -4.0108356 -4.0365047 -4.1016974 -4.1724582 -4.2193274 -4.2513628 -4.2723541 -4.2905607][-4.2088046 -4.2024717 -4.1865053 -4.1727982 -4.1417251 -4.0711775 -3.9812331 -3.9268906 -3.9514487 -4.0382552 -4.1311393 -4.1976151 -4.2426982 -4.2738252 -4.2981505][-4.21673 -4.2230363 -4.2104692 -4.1938267 -4.1579094 -4.0841794 -3.9845176 -3.9151964 -3.9294591 -4.0144205 -4.10767 -4.18158 -4.2352633 -4.2696934 -4.291697][-4.216723 -4.23053 -4.2208586 -4.1993933 -4.1625686 -4.094995 -4.0025163 -3.9373226 -3.9494522 -4.0266542 -4.110898 -4.178956 -4.2290635 -4.26002 -4.2768812][-4.2139325 -4.2288432 -4.2201557 -4.195385 -4.1571326 -4.0998273 -4.0216002 -3.9679203 -3.9825811 -4.0499349 -4.121954 -4.1824174 -4.2238326 -4.2459965 -4.2551446][-4.2086725 -4.2213764 -4.2095761 -4.1852522 -4.1515069 -4.1105247 -4.0524654 -4.0085111 -4.021318 -4.0780616 -4.1362557 -4.1804476 -4.2065344 -4.2233171 -4.2309289][-4.2083521 -4.2170696 -4.2030182 -4.1801147 -4.1536536 -4.129518 -4.0942135 -4.0632734 -4.0690737 -4.1153455 -4.1612606 -4.18671 -4.1941128 -4.19939 -4.2066708][-4.217772 -4.2233458 -4.210587 -4.1905518 -4.1698666 -4.1542869 -4.1318989 -4.1136885 -4.1205254 -4.1563711 -4.1896896 -4.1982045 -4.1844525 -4.169621 -4.1719155][-4.2292123 -4.2387352 -4.2313 -4.2178125 -4.2019439 -4.1876454 -4.1706529 -4.1624079 -4.1699977 -4.1947546 -4.2145844 -4.207068 -4.1720705 -4.1320195 -4.1229367][-4.2259717 -4.235662 -4.2347021 -4.2298322 -4.2226906 -4.2145185 -4.2055759 -4.2025442 -4.2093692 -4.2239389 -4.2332358 -4.2166777 -4.171978 -4.1140132 -4.0855775]]...]
INFO - root - 2017-12-05 10:56:46.780592: step 2110, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 76h:06m:48s remains)
INFO - root - 2017-12-05 10:56:55.293487: step 2120, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 80h:09m:35s remains)
INFO - root - 2017-12-05 10:57:03.791964: step 2130, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 79h:44m:53s remains)
INFO - root - 2017-12-05 10:57:12.317891: step 2140, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 77h:59m:35s remains)
INFO - root - 2017-12-05 10:57:20.730102: step 2150, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 77h:18m:03s remains)
INFO - root - 2017-12-05 10:57:29.194980: step 2160, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 74h:46m:30s remains)
INFO - root - 2017-12-05 10:57:37.693244: step 2170, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 76h:38m:20s remains)
INFO - root - 2017-12-05 10:57:46.160103: step 2180, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 78h:23m:37s remains)
INFO - root - 2017-12-05 10:57:54.822610: step 2190, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 78h:38m:14s remains)
INFO - root - 2017-12-05 10:58:03.208196: step 2200, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 78h:06m:55s remains)
2017-12-05 10:58:03.955778: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2307482 -4.2394834 -4.2531281 -4.2522483 -4.2321057 -4.1931362 -4.1392684 -4.0983191 -4.1081634 -4.1478567 -4.1847429 -4.2021275 -4.2199559 -4.2369075 -4.2410522][-4.2362113 -4.2465034 -4.2632375 -4.2602119 -4.2317424 -4.1842279 -4.1194248 -4.0767512 -4.0916748 -4.1443877 -4.1940861 -4.2244253 -4.2496967 -4.2652235 -4.2608042][-4.2383308 -4.2513781 -4.2705145 -4.2607193 -4.21987 -4.15785 -4.084322 -4.0463886 -4.0727282 -4.1381464 -4.2003431 -4.2410374 -4.2708058 -4.2829843 -4.2693505][-4.2440219 -4.2572536 -4.2736206 -4.2562938 -4.2021122 -4.1211138 -4.0338755 -3.9981008 -4.038497 -4.1235585 -4.2037282 -4.2545223 -4.2851381 -4.2914071 -4.2677283][-4.2553425 -4.265821 -4.2741251 -4.2489767 -4.1807809 -4.079699 -3.9759955 -3.9421413 -4.0021887 -4.1113424 -4.2099705 -4.2680321 -4.2961855 -4.2954979 -4.2622056][-4.2669358 -4.2769623 -4.275568 -4.2394881 -4.15697 -4.0358658 -3.9183187 -3.8863745 -3.9700713 -4.10674 -4.2199149 -4.2802467 -4.3036709 -4.297411 -4.2569466][-4.2805266 -4.2902794 -4.278832 -4.2334118 -4.1418819 -4.0068765 -3.8723307 -3.832346 -3.934346 -4.0927305 -4.2198958 -4.2864914 -4.3083391 -4.2970161 -4.2508488][-4.2950611 -4.3028374 -4.2848577 -4.2379322 -4.14875 -4.0124536 -3.8688259 -3.8145645 -3.9151995 -4.078033 -4.2144442 -4.2895613 -4.3138285 -4.3006668 -4.2489338][-4.312716 -4.316678 -4.2955537 -4.2508616 -4.1711907 -4.0464168 -3.910615 -3.8520024 -3.9359221 -4.0838623 -4.2136364 -4.289094 -4.3139157 -4.3031526 -4.2551332][-4.3246627 -4.325541 -4.3047018 -4.2638454 -4.195868 -4.0948319 -3.9830186 -3.9286685 -3.986547 -4.1066747 -4.2176986 -4.284585 -4.3062921 -4.2990766 -4.2586565][-4.3234653 -4.3201828 -4.3026013 -4.2695012 -4.2174397 -4.1453991 -4.0650024 -4.0186391 -4.0506868 -4.1364689 -4.2213964 -4.2730827 -4.2901077 -4.286222 -4.2540283][-4.3183165 -4.3087993 -4.2939754 -4.2696929 -4.2351303 -4.1911116 -4.1418982 -4.1111126 -4.1281052 -4.1842756 -4.2427297 -4.2787209 -4.2896242 -4.2842569 -4.2574463][-4.3106842 -4.2954736 -4.2841859 -4.2660627 -4.2408547 -4.2128158 -4.1868949 -4.17687 -4.195035 -4.2349086 -4.2736535 -4.2936883 -4.2972007 -4.2907472 -4.2678924][-4.3046446 -4.2853541 -4.2756038 -4.2625613 -4.2437587 -4.2234392 -4.2094483 -4.2120194 -4.2296004 -4.2592387 -4.285357 -4.2961149 -4.2943912 -4.2896838 -4.2731595][-4.3047872 -4.2845011 -4.2735853 -4.2631121 -4.2507195 -4.236177 -4.228477 -4.2333364 -4.2449574 -4.264606 -4.2826915 -4.28984 -4.2874966 -4.2841058 -4.2722645]]...]
INFO - root - 2017-12-05 10:58:12.594983: step 2210, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 81h:48m:00s remains)
INFO - root - 2017-12-05 10:58:21.230566: step 2220, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 78h:36m:37s remains)
INFO - root - 2017-12-05 10:58:29.754423: step 2230, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 78h:10m:59s remains)
INFO - root - 2017-12-05 10:58:38.194470: step 2240, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.812 sec/batch; 74h:31m:31s remains)
INFO - root - 2017-12-05 10:58:46.736263: step 2250, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 78h:23m:55s remains)
INFO - root - 2017-12-05 10:58:55.190504: step 2260, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 76h:56m:04s remains)
INFO - root - 2017-12-05 10:59:03.690806: step 2270, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 79h:54m:44s remains)
INFO - root - 2017-12-05 10:59:12.213643: step 2280, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 78h:24m:34s remains)
INFO - root - 2017-12-05 10:59:20.809690: step 2290, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 79h:31m:03s remains)
INFO - root - 2017-12-05 10:59:29.244442: step 2300, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 75h:11m:13s remains)
2017-12-05 10:59:29.938394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2584233 -4.2499886 -4.2274842 -4.1971121 -4.1745739 -4.1556273 -4.1334133 -4.1202722 -4.1044903 -4.0930362 -4.0825505 -4.0632777 -4.0633831 -4.0976291 -4.1389475][-4.234838 -4.2203174 -4.1923766 -4.1618152 -4.142468 -4.1205764 -4.0892148 -4.0649486 -4.0308838 -4.0175104 -4.0368538 -4.0586681 -4.0885448 -4.1338558 -4.1745672][-4.2118783 -4.1890659 -4.1530838 -4.1215873 -4.106791 -4.0861635 -4.0539756 -4.0167503 -3.9735627 -3.969574 -4.01687 -4.067132 -4.1123471 -4.1648316 -4.2112947][-4.1979671 -4.1694889 -4.1323528 -4.1031313 -4.09143 -4.0687361 -4.031311 -3.9878159 -3.9609463 -3.9819174 -4.0431538 -4.1015754 -4.1501074 -4.1982555 -4.2391591][-4.2014031 -4.1775761 -4.1427565 -4.1071024 -4.08265 -4.0442386 -3.9911404 -3.9470592 -3.9562132 -4.01353 -4.086493 -4.1482038 -4.1936269 -4.2320251 -4.2608819][-4.1967273 -4.1822014 -4.1508207 -4.1076131 -4.0581527 -3.9881589 -3.9055932 -3.8585672 -3.917165 -4.0214314 -4.1124792 -4.1771421 -4.220037 -4.252677 -4.2700763][-4.1714964 -4.1708131 -4.1509581 -4.0991521 -4.0212069 -3.9164369 -3.8048882 -3.7734013 -3.8918381 -4.02921 -4.1244216 -4.1892834 -4.2333288 -4.2622843 -4.2715836][-4.1299033 -4.1337156 -4.1161156 -4.0672517 -3.9984357 -3.9178827 -3.8471012 -3.8540897 -3.97134 -4.082602 -4.1518269 -4.2043419 -4.240799 -4.2607784 -4.2669086][-4.0657535 -4.0737982 -4.0738087 -4.0600891 -4.0436249 -4.020865 -4.0046139 -4.0297632 -4.1003828 -4.1593 -4.1945143 -4.2261195 -4.2458181 -4.2570877 -4.2665319][-4.0265326 -4.0538368 -4.0784006 -4.0901017 -4.1008806 -4.1070337 -4.1123247 -4.1335573 -4.1725578 -4.2059684 -4.22674 -4.2458019 -4.2563319 -4.2648134 -4.278254][-4.0539074 -4.0872507 -4.1147447 -4.1334014 -4.1488571 -4.1575212 -4.1626592 -4.1776204 -4.1999187 -4.2213016 -4.2404561 -4.2591372 -4.2688894 -4.279705 -4.2944307][-4.1093512 -4.1373572 -4.1556425 -4.169055 -4.1774449 -4.1781297 -4.1792674 -4.1912727 -4.2066932 -4.2248487 -4.24543 -4.2650394 -4.2792377 -4.2939029 -4.3068743][-4.142715 -4.1635303 -4.1725941 -4.1817493 -4.1870842 -4.1878252 -4.1907053 -4.2025285 -4.2156782 -4.2326441 -4.2523203 -4.2737923 -4.2921519 -4.3064537 -4.3141146][-4.152545 -4.1673794 -4.1697836 -4.1781511 -4.18586 -4.1946807 -4.2080674 -4.2222471 -4.2367563 -4.2529674 -4.2705035 -4.2892547 -4.3052955 -4.3151207 -4.3177667][-4.1608748 -4.177516 -4.1776366 -4.1824489 -4.1925855 -4.2106905 -4.2320051 -4.2484446 -4.2637558 -4.2784553 -4.2931132 -4.3055348 -4.3141136 -4.3172216 -4.3152008]]...]
INFO - root - 2017-12-05 10:59:38.374707: step 2310, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.821 sec/batch; 75h:17m:03s remains)
INFO - root - 2017-12-05 10:59:46.927813: step 2320, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:41m:40s remains)
INFO - root - 2017-12-05 10:59:55.490130: step 2330, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 81h:47m:14s remains)
INFO - root - 2017-12-05 11:00:03.921521: step 2340, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.829 sec/batch; 76h:00m:46s remains)
INFO - root - 2017-12-05 11:00:12.389020: step 2350, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 79h:58m:28s remains)
INFO - root - 2017-12-05 11:00:20.828463: step 2360, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 75h:30m:31s remains)
INFO - root - 2017-12-05 11:00:29.280781: step 2370, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.829 sec/batch; 76h:01m:45s remains)
INFO - root - 2017-12-05 11:00:37.847722: step 2380, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.829 sec/batch; 76h:02m:49s remains)
INFO - root - 2017-12-05 11:00:46.386468: step 2390, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 77h:10m:23s remains)
INFO - root - 2017-12-05 11:00:54.813160: step 2400, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 79h:19m:33s remains)
2017-12-05 11:00:55.645042: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1011934 -4.1194444 -4.1435938 -4.1648927 -4.1818476 -4.1930318 -4.2014227 -4.206697 -4.2161217 -4.2252426 -4.2342887 -4.2389956 -4.23806 -4.2309752 -4.2221437][-4.0792933 -4.0990887 -4.1257 -4.1530929 -4.1776958 -4.19671 -4.2140369 -4.2256885 -4.2313886 -4.23434 -4.2371926 -4.2361364 -4.23339 -4.2282104 -4.2232451][-4.0962138 -4.1069384 -4.1252117 -4.1506438 -4.1782584 -4.2028732 -4.2265649 -4.2401714 -4.23991 -4.2358093 -4.2317491 -4.2250948 -4.2205043 -4.2148242 -4.2118707][-4.1186604 -4.1262512 -4.1399293 -4.163115 -4.1884303 -4.209476 -4.227685 -4.2333112 -4.22476 -4.2179136 -4.2121835 -4.2052355 -4.2012477 -4.1926575 -4.1876616][-4.1336427 -4.1416845 -4.1543622 -4.1724072 -4.1884408 -4.1968832 -4.2010975 -4.1938252 -4.175797 -4.1713428 -4.1735816 -4.1770029 -4.179904 -4.1735964 -4.1672382][-4.135313 -4.1406684 -4.1505275 -4.1582155 -4.1595931 -4.1519508 -4.1344528 -4.1081634 -4.0833492 -4.0920162 -4.1159062 -4.1416531 -4.1624789 -4.1669183 -4.1634703][-4.12096 -4.1200886 -4.1241965 -4.1219769 -4.1094422 -4.0809817 -4.0350671 -3.9846261 -3.9606152 -3.9960487 -4.0527921 -4.1077991 -4.1514845 -4.17155 -4.1735654][-4.1106572 -4.1037836 -4.1018214 -4.0946031 -4.0726619 -4.0271606 -3.9611979 -3.8956606 -3.8812523 -3.9387686 -4.0151577 -4.0864944 -4.1456871 -4.177031 -4.1840105][-4.1303077 -4.1195626 -4.1157484 -4.1116748 -4.0924773 -4.0472732 -3.9877348 -3.9371934 -3.9331918 -3.9767008 -4.0341258 -4.0934443 -4.147788 -4.1809645 -4.1913705][-4.1698704 -4.1548853 -4.1481977 -4.149168 -4.14094 -4.1135106 -4.0774393 -4.0515633 -4.0480771 -4.0637822 -4.0889888 -4.1241245 -4.16451 -4.1938062 -4.2038379][-4.2114034 -4.1934419 -4.1799345 -4.1787739 -4.1781321 -4.1687179 -4.1562715 -4.1494217 -4.14496 -4.1407585 -4.1416554 -4.1563687 -4.1823869 -4.2033062 -4.21193][-4.2326989 -4.2197747 -4.2038603 -4.1986008 -4.203136 -4.2061667 -4.2089367 -4.2086945 -4.1984482 -4.1810675 -4.1676788 -4.1698284 -4.1843576 -4.1968131 -4.2033496][-4.23194 -4.22921 -4.2191806 -4.2175288 -4.2252684 -4.2324347 -4.2371306 -4.2339778 -4.2175031 -4.1922469 -4.1658792 -4.1543446 -4.1571927 -4.1624155 -4.166893][-4.2103138 -4.2162819 -4.217875 -4.2241616 -4.2342987 -4.2412348 -4.2440381 -4.2367578 -4.2150526 -4.1842666 -4.146893 -4.1190414 -4.10873 -4.1069045 -4.1149559][-4.1734133 -4.1820617 -4.1938176 -4.2079649 -4.220633 -4.2284107 -4.2316732 -4.2230272 -4.199491 -4.1699486 -4.1334834 -4.101965 -4.0862026 -4.0819125 -4.0960274]]...]
INFO - root - 2017-12-05 11:01:04.074765: step 2410, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 81h:35m:43s remains)
INFO - root - 2017-12-05 11:01:12.626447: step 2420, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 76h:52m:25s remains)
INFO - root - 2017-12-05 11:01:21.095515: step 2430, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 77h:45m:24s remains)
INFO - root - 2017-12-05 11:01:29.564158: step 2440, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.838 sec/batch; 76h:49m:53s remains)
INFO - root - 2017-12-05 11:01:38.118950: step 2450, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.851 sec/batch; 78h:02m:31s remains)
INFO - root - 2017-12-05 11:01:46.570204: step 2460, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 79h:23m:11s remains)
INFO - root - 2017-12-05 11:01:55.041074: step 2470, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.798 sec/batch; 73h:10m:24s remains)
INFO - root - 2017-12-05 11:02:03.469876: step 2480, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 78h:25m:23s remains)
INFO - root - 2017-12-05 11:02:11.978055: step 2490, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 76h:51m:37s remains)
INFO - root - 2017-12-05 11:02:20.431082: step 2500, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.834 sec/batch; 76h:25m:07s remains)
2017-12-05 11:02:21.207456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2703047 -4.2713833 -4.27531 -4.2793317 -4.2800336 -4.2824969 -4.2896557 -4.3002315 -4.3111167 -4.3141327 -4.3052459 -4.2947454 -4.2907038 -4.2911797 -4.2959576][-4.2376485 -4.2349629 -4.238049 -4.2409439 -4.2381625 -4.2377605 -4.2488813 -4.269887 -4.2880735 -4.2981639 -4.2904077 -4.2760553 -4.2681761 -4.2678471 -4.2717628][-4.1918783 -4.1850681 -4.183877 -4.1867032 -4.187748 -4.1829853 -4.1944647 -4.2245946 -4.25548 -4.273665 -4.2683187 -4.253108 -4.2409625 -4.2384715 -4.2418814][-4.1374373 -4.1271315 -4.1285095 -4.1356707 -4.1355033 -4.1201673 -4.1240191 -4.1603913 -4.2042518 -4.2320385 -4.2326264 -4.2231574 -4.2110105 -4.2084022 -4.2123146][-4.1167092 -4.1004395 -4.0945683 -4.099503 -4.0948577 -4.0664835 -4.051887 -4.0856457 -4.1428142 -4.1858006 -4.1940165 -4.1904554 -4.187398 -4.1888666 -4.1948786][-4.096498 -4.0732288 -4.0574741 -4.051024 -4.0419993 -3.9984286 -3.9559171 -3.978981 -4.0541892 -4.1234417 -4.1482186 -4.1574769 -4.1669893 -4.1815906 -4.1957607][-4.0916409 -4.0657039 -4.0432839 -4.0220165 -3.9972396 -3.9351168 -3.8568695 -3.8560655 -3.9476888 -4.0498533 -4.1042938 -4.1326194 -4.1503639 -4.1716814 -4.1939306][-4.1132393 -4.0873938 -4.0597892 -4.0277567 -3.9941669 -3.9276025 -3.827605 -3.8071694 -3.8972156 -4.0071368 -4.0746593 -4.1140709 -4.1376338 -4.1649904 -4.1942358][-4.1302204 -4.1008105 -4.0723352 -4.0380492 -4.008749 -3.9579582 -3.8772979 -3.8646028 -3.9386013 -4.0196233 -4.06651 -4.0969191 -4.1208549 -4.1518631 -4.1881328][-4.1526408 -4.1265082 -4.1006417 -4.0725455 -4.0502739 -4.0147004 -3.9550118 -3.9393923 -3.9838943 -4.0283694 -4.0511646 -4.0747223 -4.0981073 -4.132709 -4.171598][-4.1367598 -4.1151037 -4.1002479 -4.0881925 -4.0790477 -4.0609217 -4.0253453 -4.00714 -4.0212169 -4.0318375 -4.0364242 -4.0570116 -4.0867968 -4.1266732 -4.1632886][-4.1336427 -4.112113 -4.1031089 -4.1019778 -4.10431 -4.1040316 -4.0905623 -4.0791693 -4.0810285 -4.0760937 -4.0671177 -4.0814905 -4.1120582 -4.1471353 -4.1778541][-4.1706204 -4.1505585 -4.142282 -4.1425452 -4.1501727 -4.1565604 -4.1523743 -4.1502647 -4.1541018 -4.143436 -4.1275463 -4.140286 -4.1680436 -4.194509 -4.2179055][-4.217483 -4.2054524 -4.2003021 -4.2011743 -4.2098656 -4.2179589 -4.21629 -4.2211261 -4.2280493 -4.2189746 -4.2054296 -4.2169733 -4.2358937 -4.2536292 -4.271143][-4.2645063 -4.2597747 -4.2577987 -4.259562 -4.2695041 -4.2787833 -4.2795539 -4.2856617 -4.292697 -4.287662 -4.2805066 -4.2880673 -4.2979484 -4.3066821 -4.3158088]]...]
INFO - root - 2017-12-05 11:02:29.612297: step 2510, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 77h:19m:15s remains)
INFO - root - 2017-12-05 11:02:38.131783: step 2520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 78h:47m:26s remains)
INFO - root - 2017-12-05 11:02:46.667911: step 2530, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 79h:16m:44s remains)
INFO - root - 2017-12-05 11:02:55.253828: step 2540, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 79h:36m:20s remains)
INFO - root - 2017-12-05 11:03:03.732748: step 2550, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 78h:19m:57s remains)
INFO - root - 2017-12-05 11:03:12.376598: step 2560, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 76h:44m:07s remains)
INFO - root - 2017-12-05 11:03:20.911832: step 2570, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 78h:27m:55s remains)
INFO - root - 2017-12-05 11:03:29.352001: step 2580, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 77h:24m:43s remains)
INFO - root - 2017-12-05 11:03:37.817598: step 2590, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 77h:12m:21s remains)
INFO - root - 2017-12-05 11:03:46.129307: step 2600, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 77h:05m:41s remains)
2017-12-05 11:03:46.875623: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1870418 -4.187768 -4.1871462 -4.1850386 -4.1862636 -4.1895633 -4.1945615 -4.1954622 -4.1968584 -4.19248 -4.18268 -4.1658211 -4.13708 -4.13134 -4.1488805][-4.1828842 -4.1744456 -4.167316 -4.1662483 -4.1742849 -4.1871867 -4.2018409 -4.2091842 -4.2103086 -4.2022967 -4.1866012 -4.1611328 -4.1299052 -4.1263075 -4.1446409][-4.1787477 -4.1690903 -4.1634531 -4.1699109 -4.183857 -4.1969934 -4.2107825 -4.2151246 -4.2102246 -4.1991839 -4.1807842 -4.1545572 -4.1268797 -4.120564 -4.1273732][-4.1769261 -4.1706452 -4.1717215 -4.18907 -4.20867 -4.2167082 -4.2207003 -4.2126107 -4.2020631 -4.1930118 -4.1788583 -4.1561322 -4.1287961 -4.1123438 -4.1029091][-4.1774559 -4.1801214 -4.1885648 -4.2124734 -4.2331433 -4.2353091 -4.22988 -4.2107863 -4.1933966 -4.1877537 -4.1832709 -4.1690078 -4.1410313 -4.1132035 -4.0905013][-4.1789026 -4.1891413 -4.2039895 -4.2304397 -4.2469168 -4.2443275 -4.2328138 -4.2057409 -4.1780195 -4.1709623 -4.1733937 -4.1663022 -4.14003 -4.1090689 -4.0832748][-4.1749077 -4.1908669 -4.2077818 -4.2297692 -4.239841 -4.2329631 -4.214047 -4.1789312 -4.1420221 -4.1309633 -4.138298 -4.1396179 -4.12189 -4.1021352 -4.0886588][-4.1548243 -4.1804428 -4.2027869 -4.2188754 -4.2232194 -4.2124534 -4.1870532 -4.1481824 -4.1092911 -4.0984769 -4.1090312 -4.1163197 -4.1130939 -4.110076 -4.1075068][-4.1334057 -4.1703911 -4.2010455 -4.2151136 -4.2164454 -4.2026768 -4.1740808 -4.1356859 -4.1019578 -4.0964346 -4.1049137 -4.1135163 -4.1234593 -4.1323118 -4.135345][-4.1230779 -4.1624146 -4.1971693 -4.2120991 -4.2130556 -4.1992674 -4.1720519 -4.13786 -4.111464 -4.111928 -4.1220036 -4.1322441 -4.1463995 -4.1565075 -4.1590357][-4.1258411 -4.1593542 -4.1893792 -4.2055516 -4.2107377 -4.1998596 -4.1752176 -4.1448879 -4.1279464 -4.1345468 -4.1474 -4.1573973 -4.1641421 -4.1647396 -4.1650219][-4.1445045 -4.1676254 -4.1880016 -4.2013626 -4.2079587 -4.1973 -4.1745472 -4.1497922 -4.1403685 -4.1523833 -4.1686878 -4.176733 -4.1755738 -4.1693587 -4.1683078][-4.1618519 -4.1789823 -4.1937895 -4.2032108 -4.209527 -4.2024455 -4.1859217 -4.1670079 -4.1627665 -4.1761861 -4.1921239 -4.1975455 -4.1906228 -4.1799784 -4.1783171][-4.189919 -4.2009363 -4.2124619 -4.2209053 -4.2277403 -4.22468 -4.2143106 -4.2020769 -4.2000742 -4.2100816 -4.2215376 -4.2238979 -4.2140121 -4.2023983 -4.2019682][-4.2268028 -4.2297521 -4.237658 -4.2449307 -4.2509236 -4.2513618 -4.246521 -4.2392941 -4.2370639 -4.2422843 -4.249166 -4.25031 -4.2428956 -4.2347617 -4.2360177]]...]
INFO - root - 2017-12-05 11:03:55.413528: step 2610, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 81h:22m:11s remains)
INFO - root - 2017-12-05 11:04:03.997069: step 2620, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 79h:29m:52s remains)
INFO - root - 2017-12-05 11:04:12.398759: step 2630, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 75h:28m:11s remains)
INFO - root - 2017-12-05 11:04:20.993337: step 2640, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 79h:01m:54s remains)
INFO - root - 2017-12-05 11:04:29.442440: step 2650, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.868 sec/batch; 79h:29m:33s remains)
INFO - root - 2017-12-05 11:04:37.883764: step 2660, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 77h:51m:37s remains)
INFO - root - 2017-12-05 11:04:46.331090: step 2670, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.850 sec/batch; 77h:50m:01s remains)
INFO - root - 2017-12-05 11:04:54.814997: step 2680, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 76h:54m:07s remains)
INFO - root - 2017-12-05 11:05:03.092685: step 2690, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.773 sec/batch; 70h:49m:33s remains)
INFO - root - 2017-12-05 11:05:11.527079: step 2700, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 77h:15m:51s remains)
2017-12-05 11:05:12.259033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.322412 -4.3172092 -4.3126345 -4.3083353 -4.3042088 -4.3001046 -4.2961979 -4.295033 -4.2981043 -4.3049254 -4.3119321 -4.31586 -4.3164048 -4.310843 -4.302146][-4.3122573 -4.3071566 -4.3036561 -4.3005066 -4.297719 -4.2933688 -4.2883892 -4.2883 -4.2961574 -4.3067789 -4.3177485 -4.3283892 -4.3350348 -4.3346272 -4.329989][-4.2994766 -4.2940788 -4.2912955 -4.2873497 -4.2798405 -4.2667041 -4.2524409 -4.2493944 -4.2596331 -4.2748771 -4.29199 -4.312068 -4.326386 -4.3318958 -4.3314948][-4.2920203 -4.2850833 -4.2781339 -4.2658267 -4.2462993 -4.2190247 -4.1941276 -4.1865935 -4.1976762 -4.2202744 -4.2487969 -4.281528 -4.3053155 -4.316143 -4.31787][-4.2809024 -4.26886 -4.2541738 -4.2303977 -4.19469 -4.1482611 -4.1065907 -4.0934367 -4.1108809 -4.1464996 -4.1896014 -4.2353163 -4.2691259 -4.2848797 -4.2892513][-4.2533274 -4.2338886 -4.2089663 -4.1687775 -4.1104035 -4.0370488 -3.9686532 -3.949326 -3.9874287 -4.0516872 -4.1191096 -4.1803551 -4.2236953 -4.2449627 -4.2539072][-4.1960154 -4.1654339 -4.1264095 -4.0676112 -3.9875686 -3.8867824 -3.78773 -3.7737286 -3.8566971 -3.9622965 -4.0536184 -4.1273136 -4.1757779 -4.2045593 -4.2221766][-4.1300554 -4.090344 -4.0403323 -3.96769 -3.8777509 -3.768203 -3.6639519 -3.6720834 -3.7880595 -3.9123695 -4.0089216 -4.0831766 -4.1308317 -4.1635928 -4.1910677][-4.0898485 -4.0497656 -3.9994719 -3.92978 -3.8616304 -3.7940121 -3.7357588 -3.7496898 -3.8341224 -3.9269476 -4.00535 -4.0693269 -4.11171 -4.1440129 -4.1756229][-4.094727 -4.0664921 -4.0345049 -3.9893682 -3.9558184 -3.9292462 -3.9051106 -3.9108675 -3.9526007 -4.0032763 -4.0562358 -4.1078429 -4.144599 -4.1713181 -4.1949358][-4.1453338 -4.131227 -4.1202722 -4.1012211 -4.0874782 -4.0783005 -4.0715261 -4.0752497 -4.0937238 -4.1174946 -4.1500249 -4.188221 -4.2165084 -4.2340994 -4.2450056][-4.2024364 -4.1949463 -4.1945186 -4.19214 -4.1917629 -4.1956224 -4.2033458 -4.2132616 -4.2250586 -4.2349515 -4.2509217 -4.272903 -4.2903337 -4.3004379 -4.3015332][-4.2416511 -4.236711 -4.2408729 -4.2489314 -4.2596731 -4.2732253 -4.2874727 -4.299387 -4.305912 -4.3059211 -4.3106561 -4.3228331 -4.3336816 -4.34038 -4.3380704][-4.2447915 -4.2430472 -4.2522621 -4.2677636 -4.2866592 -4.3051791 -4.3194904 -4.3272514 -4.3258104 -4.3186712 -4.317718 -4.3271384 -4.3395181 -4.3487349 -4.3483677][-4.2144861 -4.210875 -4.220561 -4.2396493 -4.2659779 -4.2911 -4.3080583 -4.3127356 -4.3039842 -4.2904186 -4.2864938 -4.2984724 -4.3180161 -4.3348131 -4.34192]]...]
INFO - root - 2017-12-05 11:05:20.660632: step 2710, loss = 2.10, batch loss = 2.04 (9.8 examples/sec; 0.818 sec/batch; 74h:55m:52s remains)
INFO - root - 2017-12-05 11:05:29.147324: step 2720, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.792 sec/batch; 72h:32m:43s remains)
INFO - root - 2017-12-05 11:05:37.533324: step 2730, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 77h:17m:07s remains)
INFO - root - 2017-12-05 11:05:45.943494: step 2740, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 78h:33m:10s remains)
INFO - root - 2017-12-05 11:05:54.468006: step 2750, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 80h:45m:56s remains)
INFO - root - 2017-12-05 11:06:02.895301: step 2760, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 77h:54m:02s remains)
INFO - root - 2017-12-05 11:06:11.358229: step 2770, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 77h:44m:27s remains)
INFO - root - 2017-12-05 11:06:19.738542: step 2780, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 79h:02m:26s remains)
INFO - root - 2017-12-05 11:06:28.266258: step 2790, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 76h:51m:33s remains)
INFO - root - 2017-12-05 11:06:36.485354: step 2800, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.803 sec/batch; 73h:34m:01s remains)
2017-12-05 11:06:37.198901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3169703 -4.3142762 -4.303503 -4.2963066 -4.2970815 -4.304955 -4.317142 -4.3295088 -4.3354397 -4.3357582 -4.3349557 -4.3320575 -4.3271575 -4.3202105 -4.3131971][-4.3226662 -4.3155513 -4.2968688 -4.2827311 -4.2802081 -4.2871423 -4.3017325 -4.3198428 -4.332366 -4.33951 -4.3428421 -4.3396726 -4.3319931 -4.3219228 -4.3131166][-4.3250923 -4.310637 -4.27808 -4.2512584 -4.23997 -4.2414069 -4.2554879 -4.2805753 -4.3061991 -4.328712 -4.3447089 -4.3485975 -4.3402038 -4.3276744 -4.3178945][-4.3208437 -4.2995262 -4.2507148 -4.2043276 -4.1762652 -4.1653366 -4.1748252 -4.2045979 -4.2476 -4.2952809 -4.3318357 -4.3476815 -4.3435326 -4.331645 -4.3225641][-4.3078351 -4.2815795 -4.2221231 -4.1548042 -4.0969911 -4.0573115 -4.0493956 -4.0821977 -4.1511879 -4.2335272 -4.2987385 -4.3312306 -4.3345828 -4.3261671 -4.3196778][-4.2935557 -4.2665215 -4.2076354 -4.1304665 -4.0461779 -3.9606323 -3.9031649 -3.9177396 -4.0171909 -4.1450548 -4.2439175 -4.2958832 -4.3110218 -4.3099613 -4.3080292][-4.2806334 -4.2558241 -4.2064624 -4.1369848 -4.0492411 -3.934428 -3.8158865 -3.7807369 -3.8874779 -4.0482917 -4.173758 -4.2441635 -4.2744861 -4.2853603 -4.2930312][-4.2750907 -4.2549324 -4.2196422 -4.1721573 -4.1083193 -4.0097861 -3.8863394 -3.8163218 -3.8769095 -4.0107088 -4.127418 -4.2010155 -4.2407756 -4.2609525 -4.277843][-4.2821426 -4.2675805 -4.2443242 -4.2180662 -4.1858768 -4.1283813 -4.047904 -3.9916971 -3.999871 -4.0580053 -4.1275015 -4.1830974 -4.2177248 -4.2382193 -4.25874][-4.2895508 -4.2793412 -4.2640338 -4.2509952 -4.2369576 -4.2095852 -4.1708312 -4.1412082 -4.1327586 -4.1417947 -4.1637774 -4.1921325 -4.211628 -4.2240648 -4.2409091][-4.2896776 -4.2839422 -4.2737732 -4.2644897 -4.2552891 -4.23961 -4.2240763 -4.2169189 -4.2143025 -4.2083216 -4.206841 -4.213459 -4.2174335 -4.2204146 -4.2298841][-4.2775655 -4.2732391 -4.2651286 -4.257988 -4.2483311 -4.2353449 -4.2326617 -4.2416492 -4.2489786 -4.24393 -4.2359757 -4.2297592 -4.2212791 -4.2166052 -4.2199659][-4.2552667 -4.2523956 -4.2459478 -4.2409892 -4.2318945 -4.2208366 -4.2222791 -4.23599 -4.2462869 -4.2436748 -4.2365685 -4.2264924 -4.2126822 -4.2027488 -4.2017393][-4.2244067 -4.2220149 -4.2149811 -4.2109542 -4.2033863 -4.1944389 -4.1939354 -4.2020531 -4.2080894 -4.2071018 -4.2039948 -4.1977367 -4.1880274 -4.177424 -4.1736608][-4.1939321 -4.1924725 -4.1858387 -4.184515 -4.1832895 -4.1791887 -4.176672 -4.1781425 -4.1777091 -4.1753583 -4.1741791 -4.1747241 -4.1728048 -4.1648245 -4.160934]]...]
INFO - root - 2017-12-05 11:06:45.770555: step 2810, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 79h:40m:39s remains)
INFO - root - 2017-12-05 11:06:54.264731: step 2820, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 77h:08m:12s remains)
INFO - root - 2017-12-05 11:07:02.797705: step 2830, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 76h:55m:53s remains)
INFO - root - 2017-12-05 11:07:11.380652: step 2840, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 78h:47m:37s remains)
INFO - root - 2017-12-05 11:07:19.937064: step 2850, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 75h:09m:48s remains)
INFO - root - 2017-12-05 11:07:28.353050: step 2860, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 77h:05m:41s remains)
INFO - root - 2017-12-05 11:07:36.956502: step 2870, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 77h:34m:21s remains)
INFO - root - 2017-12-05 11:07:45.414818: step 2880, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 77h:51m:46s remains)
INFO - root - 2017-12-05 11:07:53.983686: step 2890, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 79h:08m:33s remains)
INFO - root - 2017-12-05 11:08:02.426005: step 2900, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 78h:41m:27s remains)
2017-12-05 11:08:03.170080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2585382 -4.2751265 -4.2857857 -4.2894716 -4.2920289 -4.2817788 -4.2568827 -4.2263784 -4.2018671 -4.2048316 -4.2343607 -4.2643442 -4.2755146 -4.2639546 -4.2413116][-4.2649956 -4.277946 -4.2851777 -4.2845597 -4.28268 -4.2718 -4.2435579 -4.2091208 -4.1840796 -4.1935253 -4.2347522 -4.275249 -4.2927537 -4.2853069 -4.2648778][-4.2625093 -4.2688618 -4.2676911 -4.2619252 -4.2598724 -4.2564907 -4.2373977 -4.2105279 -4.189158 -4.1987562 -4.2419972 -4.2840028 -4.3013482 -4.2946367 -4.2780409][-4.26471 -4.2641759 -4.2519 -4.2406535 -4.2437487 -4.2532711 -4.2469082 -4.2272773 -4.206676 -4.210608 -4.2466221 -4.2819347 -4.2940145 -4.28605 -4.2734885][-4.2706861 -4.267056 -4.25068 -4.2390356 -4.2460651 -4.262466 -4.2618732 -4.2466803 -4.22831 -4.2269917 -4.2498527 -4.2703204 -4.2739825 -4.2656932 -4.2559557][-4.2701397 -4.2710676 -4.2600746 -4.2534623 -4.2614007 -4.2744346 -4.2731233 -4.2611027 -4.2461538 -4.2412858 -4.2488952 -4.2520075 -4.2456074 -4.2347331 -4.2280917][-4.2690053 -4.280457 -4.2803454 -4.2794852 -4.2846594 -4.2893686 -4.2837334 -4.2731981 -4.2620893 -4.2559342 -4.2539878 -4.2458353 -4.2326264 -4.2196383 -4.2142758][-4.2694454 -4.2900891 -4.2966609 -4.2966881 -4.296711 -4.2932878 -4.2837682 -4.2757206 -4.2698 -4.2677827 -4.2680607 -4.262032 -4.2506657 -4.2387104 -4.2319603][-4.266706 -4.2947531 -4.3051677 -4.3021736 -4.2938871 -4.2829781 -4.2711916 -4.2642112 -4.2612429 -4.2637243 -4.2702146 -4.271862 -4.2676473 -4.2633367 -4.2607341][-4.2549472 -4.2925673 -4.3074884 -4.3027411 -4.2874484 -4.2699022 -4.2556734 -4.2492456 -4.247776 -4.252996 -4.2622895 -4.2686338 -4.2700691 -4.2741785 -4.2797112][-4.2383938 -4.2847934 -4.3047476 -4.3014493 -4.2827716 -4.2591105 -4.2416406 -4.2353382 -4.234015 -4.2377844 -4.24449 -4.24939 -4.2527661 -4.2634006 -4.2784667][-4.232955 -4.2839208 -4.3063612 -4.3053508 -4.284883 -4.2554255 -4.2350445 -4.228363 -4.2242694 -4.2219877 -4.2206163 -4.2170539 -4.21579 -4.2289128 -4.2511816][-4.2477908 -4.2937841 -4.3121381 -4.309453 -4.288384 -4.2573032 -4.2362881 -4.2285671 -4.2222176 -4.2168341 -4.2071567 -4.1921263 -4.1819568 -4.1907187 -4.2120728][-4.2677536 -4.2971735 -4.3060484 -4.3016572 -4.2854114 -4.2598405 -4.24021 -4.2318873 -4.2287192 -4.2269063 -4.2146115 -4.1931992 -4.17681 -4.179359 -4.1879067][-4.2776184 -4.2908692 -4.2919273 -4.2894979 -4.283638 -4.2686753 -4.2511396 -4.241622 -4.2421346 -4.2456884 -4.2354441 -4.2141728 -4.1971583 -4.1930342 -4.1854305]]...]
INFO - root - 2017-12-05 11:08:11.551377: step 2910, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 78h:09m:23s remains)
INFO - root - 2017-12-05 11:08:19.970932: step 2920, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 77h:57m:56s remains)
INFO - root - 2017-12-05 11:08:28.528122: step 2930, loss = 2.10, batch loss = 2.05 (9.4 examples/sec; 0.853 sec/batch; 78h:05m:51s remains)
INFO - root - 2017-12-05 11:08:37.055037: step 2940, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 78h:27m:38s remains)
INFO - root - 2017-12-05 11:08:45.700361: step 2950, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 78h:10m:33s remains)
INFO - root - 2017-12-05 11:08:54.199552: step 2960, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.846 sec/batch; 77h:25m:36s remains)
INFO - root - 2017-12-05 11:09:02.757917: step 2970, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 77h:10m:30s remains)
INFO - root - 2017-12-05 11:09:11.289433: step 2980, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 76h:48m:35s remains)
INFO - root - 2017-12-05 11:09:19.863421: step 2990, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 75h:10m:29s remains)
INFO - root - 2017-12-05 11:09:28.377197: step 3000, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 77h:54m:37s remains)
2017-12-05 11:09:29.119988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1520615 -4.1713934 -4.198956 -4.22077 -4.2156467 -4.1783614 -4.1347961 -4.1066175 -4.1051707 -4.1238594 -4.1406116 -4.1752596 -4.2024441 -4.2064462 -4.2049642][-4.136374 -4.1578417 -4.1914124 -4.2212944 -4.2218113 -4.1850958 -4.1391411 -4.1057334 -4.0980959 -4.1122656 -4.1250277 -4.1587453 -4.1857767 -4.1917162 -4.1936188][-4.1180634 -4.1358614 -4.1754494 -4.216959 -4.2277818 -4.1994514 -4.1647129 -4.1395273 -4.1311126 -4.1384811 -4.1457911 -4.1735296 -4.1940093 -4.1993737 -4.2064266][-4.09276 -4.1023264 -4.1403217 -4.1866636 -4.2049608 -4.1895504 -4.1745877 -4.1694784 -4.1751733 -4.1885352 -4.2033772 -4.2304473 -4.2449446 -4.2461023 -4.2502389][-4.0701032 -4.0704618 -4.0952206 -4.1288247 -4.1417532 -4.1288452 -4.1215162 -4.129467 -4.1562958 -4.1851897 -4.2126474 -4.2401953 -4.2554169 -4.26625 -4.2739596][-4.03547 -4.0289121 -4.0382748 -4.0573711 -4.0610576 -4.0394669 -4.0180435 -4.0262566 -4.0765023 -4.1279669 -4.168962 -4.2013 -4.222158 -4.2458768 -4.2613239][-3.9994662 -3.9727089 -3.9568677 -3.9599595 -3.9510608 -3.9098337 -3.8577113 -3.8637218 -3.9438226 -4.023211 -4.0809121 -4.1175842 -4.147 -4.1829548 -4.2042332][-4.0122709 -3.9582663 -3.9077969 -3.8888025 -3.8621836 -3.7876675 -3.6848054 -3.6823168 -3.8008904 -3.9097269 -3.9816346 -4.0247846 -4.0627012 -4.1076136 -4.1346822][-4.0807261 -4.0228539 -3.9637604 -3.932059 -3.8974397 -3.8268952 -3.7362432 -3.7290289 -3.8240941 -3.9171476 -3.9766083 -4.0096917 -4.0354462 -4.0716352 -4.0905786][-4.1704421 -4.1188684 -4.0700936 -4.0398359 -4.0118113 -3.96637 -3.9157166 -3.9136415 -3.9629264 -4.0153761 -4.0510511 -4.0639558 -4.0694666 -4.0847688 -4.0889115][-4.24168 -4.2065845 -4.1748562 -4.1540475 -4.1383791 -4.1124744 -4.08426 -4.0757084 -4.0877242 -4.1021414 -4.1057878 -4.094408 -4.0837741 -4.07705 -4.0675755][-4.2806096 -4.2624164 -4.2468195 -4.2371783 -4.230926 -4.2163229 -4.1952667 -4.1789117 -4.1703606 -4.1601253 -4.1354313 -4.1001554 -4.0713916 -4.041554 -4.0100646][-4.2951951 -4.2858014 -4.2798805 -4.2780566 -4.275332 -4.2646232 -4.2462277 -4.226007 -4.20799 -4.182796 -4.1370373 -4.0838962 -4.0379024 -3.9919696 -3.9473023][-4.30327 -4.2967114 -4.293735 -4.2941208 -4.293108 -4.2848544 -4.2691031 -4.2478056 -4.2248373 -4.1918 -4.1371942 -4.0741143 -4.0201073 -3.9690125 -3.9214814][-4.3171248 -4.3109841 -4.3074584 -4.3059492 -4.3037534 -4.29771 -4.2850161 -4.2663813 -4.2445507 -4.214416 -4.1676855 -4.11239 -4.0616589 -4.0140772 -3.9713528]]...]
INFO - root - 2017-12-05 11:09:37.896703: step 3010, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 80h:00m:49s remains)
INFO - root - 2017-12-05 11:09:46.400033: step 3020, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 79h:36m:20s remains)
INFO - root - 2017-12-05 11:09:55.039601: step 3030, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 78h:20m:14s remains)
INFO - root - 2017-12-05 11:10:03.674295: step 3040, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 79h:20m:10s remains)
INFO - root - 2017-12-05 11:10:12.292555: step 3050, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 79h:40m:43s remains)
INFO - root - 2017-12-05 11:10:20.899200: step 3060, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.904 sec/batch; 82h:43m:17s remains)
INFO - root - 2017-12-05 11:10:29.613877: step 3070, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 78h:59m:30s remains)
INFO - root - 2017-12-05 11:10:38.172854: step 3080, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 79h:15m:31s remains)
INFO - root - 2017-12-05 11:10:46.846334: step 3090, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 80h:41m:58s remains)
INFO - root - 2017-12-05 11:10:55.310141: step 3100, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 75h:44m:22s remains)
2017-12-05 11:10:56.070981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2489133 -4.2597656 -4.2571836 -4.2465129 -4.2377782 -4.2391949 -4.2392769 -4.2163296 -4.1833773 -4.1792169 -4.2042413 -4.22728 -4.2269464 -4.201582 -4.1814561][-4.253047 -4.2589693 -4.2498093 -4.233799 -4.224865 -4.2275882 -4.2247276 -4.1967273 -4.1619773 -4.1586957 -4.1845727 -4.209816 -4.2133789 -4.194479 -4.1807561][-4.2182817 -4.2256594 -4.2204723 -4.2110786 -4.2107587 -4.2198515 -4.2193241 -4.1963649 -4.1703982 -4.1727743 -4.1979094 -4.2227812 -4.2319307 -4.2237144 -4.2152386][-4.1931243 -4.2064085 -4.2134361 -4.2209945 -4.23402 -4.2470617 -4.2466564 -4.2308073 -4.2144308 -4.2197146 -4.2386332 -4.2563152 -4.2653112 -4.2640338 -4.258019][-4.202775 -4.2178435 -4.234108 -4.2532544 -4.2712932 -4.2801747 -4.2735143 -4.2581992 -4.2465477 -4.2501988 -4.2609921 -4.2699928 -4.2763491 -4.2775979 -4.2736053][-4.2250462 -4.2322831 -4.2425652 -4.2572913 -4.2696238 -4.27004 -4.2519741 -4.2282395 -4.2185669 -4.2253065 -4.2364259 -4.2414565 -4.2451353 -4.2500725 -4.2519436][-4.2320557 -4.2246814 -4.2177773 -4.2184114 -4.2195263 -4.2065563 -4.1695781 -4.1320024 -4.12745 -4.1501017 -4.1730385 -4.1826353 -4.1908712 -4.2049832 -4.2144217][-4.2130642 -4.1893611 -4.1648645 -4.1530833 -4.1444716 -4.1195345 -4.0676794 -4.0215931 -4.0278912 -4.0733542 -4.1150413 -4.1392612 -4.1597924 -4.1864562 -4.2047558][-4.19863 -4.1713004 -4.144403 -4.1338639 -4.1263089 -4.1024547 -4.0548162 -4.0177603 -4.0308595 -4.0786772 -4.1229792 -4.1517358 -4.1787224 -4.2099981 -4.229888][-4.2015352 -4.1862144 -4.1726274 -4.17263 -4.1747351 -4.1620464 -4.1327591 -4.112103 -4.1233845 -4.1560307 -4.1866727 -4.2076883 -4.2265229 -4.2461119 -4.2557974][-4.2136922 -4.2104492 -4.2107153 -4.2189689 -4.2278056 -4.2247386 -4.2120304 -4.2035646 -4.210516 -4.2295785 -4.2474165 -4.2562065 -4.2616844 -4.2671924 -4.2646017][-4.2423544 -4.2432079 -4.2498679 -4.2589993 -4.2686386 -4.27154 -4.2694097 -4.2671762 -4.2694931 -4.2777448 -4.2863245 -4.2869353 -4.2831492 -4.278204 -4.2689862][-4.2756066 -4.2741165 -4.279108 -4.2818971 -4.2854395 -4.2890549 -4.2902651 -4.2874408 -4.2861748 -4.2901721 -4.2960982 -4.2951436 -4.28967 -4.2833676 -4.2768126][-4.2936034 -4.2881961 -4.2866812 -4.2788649 -4.2714868 -4.2712126 -4.271358 -4.2659168 -4.2605505 -4.2628503 -4.2728128 -4.2800241 -4.2793245 -4.276031 -4.2749][-4.2864537 -4.2786136 -4.2733083 -4.2573867 -4.2410731 -4.2383618 -4.2397079 -4.2366843 -4.230514 -4.2319131 -4.2461529 -4.2620211 -4.2655535 -4.2630615 -4.2644157]]...]
INFO - root - 2017-12-05 11:11:04.663069: step 3110, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.863 sec/batch; 78h:57m:35s remains)
INFO - root - 2017-12-05 11:11:13.360462: step 3120, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 80h:40m:25s remains)
INFO - root - 2017-12-05 11:11:21.914182: step 3130, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 80h:18m:14s remains)
INFO - root - 2017-12-05 11:11:30.551513: step 3140, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 78h:49m:16s remains)
INFO - root - 2017-12-05 11:11:39.150130: step 3150, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 79h:25m:35s remains)
INFO - root - 2017-12-05 11:11:47.666909: step 3160, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 78h:10m:43s remains)
INFO - root - 2017-12-05 11:11:56.218647: step 3170, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 77h:53m:24s remains)
INFO - root - 2017-12-05 11:12:04.770220: step 3180, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 80h:12m:20s remains)
INFO - root - 2017-12-05 11:12:13.401051: step 3190, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 77h:10m:18s remains)
INFO - root - 2017-12-05 11:12:21.925196: step 3200, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 80h:50m:54s remains)
2017-12-05 11:12:22.663661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.296412 -4.29571 -4.2965975 -4.2990704 -4.3050375 -4.3112988 -4.3113594 -4.3088732 -4.3058105 -4.3043909 -4.306273 -4.3066745 -4.3053989 -4.3099747 -4.3166904][-4.2648849 -4.2644982 -4.2650628 -4.26739 -4.2751036 -4.2854729 -4.286293 -4.2822247 -4.2766309 -4.2761307 -4.2821717 -4.2882824 -4.2870259 -4.2918854 -4.3011417][-4.2291207 -4.2280478 -4.2272248 -4.2273464 -4.2319493 -4.2419167 -4.24192 -4.2381821 -4.2351494 -4.2399311 -4.25344 -4.2677021 -4.2706351 -4.2796831 -4.2907915][-4.204329 -4.2023244 -4.1971741 -4.1886744 -4.1849442 -4.1890259 -4.1823196 -4.1763272 -4.1783719 -4.1872544 -4.2059307 -4.2306008 -4.243835 -4.2647352 -4.28236][-4.1776071 -4.167964 -4.1544857 -4.133306 -4.1194692 -4.1184907 -4.1106477 -4.1102705 -4.1195045 -4.1309195 -4.1544638 -4.1894894 -4.215539 -4.2484026 -4.2730751][-4.1558037 -4.1299291 -4.10438 -4.0707116 -4.0487833 -4.0443683 -4.0355535 -4.0363379 -4.0448833 -4.0529003 -4.0783677 -4.1232495 -4.1675024 -4.2242455 -4.2613959][-4.1197958 -4.0763273 -4.039566 -3.9978466 -3.9685359 -3.9596186 -3.942471 -3.9342372 -3.9357593 -3.9395895 -3.9647374 -4.0226192 -4.1015062 -4.1913671 -4.246541][-4.0790281 -4.0272913 -3.9859419 -3.9451265 -3.916137 -3.9028547 -3.874079 -3.8566182 -3.8597677 -3.8681488 -3.8995697 -3.970469 -4.0711207 -4.1784487 -4.2426214][-4.0620332 -4.0178394 -3.9854641 -3.9532309 -3.9291549 -3.9113598 -3.8804972 -3.8700819 -3.8867824 -3.9087186 -3.9482758 -4.01582 -4.1051421 -4.1994443 -4.2574224][-4.0681605 -4.0334253 -4.0114202 -3.9923375 -3.9783387 -3.9712145 -3.9547319 -3.9573588 -3.979059 -4.0033321 -4.0435061 -4.0991917 -4.1651988 -4.2361307 -4.2813511][-4.1081009 -4.0805893 -4.0694232 -4.0606179 -4.0549307 -4.057549 -4.0553207 -4.0672784 -4.087759 -4.1106958 -4.1451874 -4.1863203 -4.2287922 -4.2771969 -4.3099661][-4.1750212 -4.1558518 -4.1517572 -4.1506867 -4.1513376 -4.1577048 -4.1618466 -4.1750131 -4.1898637 -4.208602 -4.234179 -4.2616792 -4.2876449 -4.3177524 -4.3381338][-4.2414231 -4.2328048 -4.23519 -4.2408624 -4.2450085 -4.2506056 -4.2536392 -4.2634516 -4.2724152 -4.2833886 -4.3005662 -4.3185267 -4.3333669 -4.3481526 -4.3571339][-4.2969594 -4.2935362 -4.2989674 -4.3068457 -4.3108363 -4.31529 -4.3185143 -4.3242631 -4.3280568 -4.3326135 -4.3411922 -4.35053 -4.3579197 -4.3624115 -4.3631897][-4.3293328 -4.327456 -4.3320265 -4.3370495 -4.3379927 -4.3391123 -4.34105 -4.34351 -4.3444295 -4.3465657 -4.3514547 -4.3563819 -4.3599896 -4.3613458 -4.3603983]]...]
INFO - root - 2017-12-05 11:12:31.320952: step 3210, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 79h:05m:35s remains)
INFO - root - 2017-12-05 11:12:39.927351: step 3220, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 79h:56m:47s remains)
INFO - root - 2017-12-05 11:12:48.440771: step 3230, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 76h:23m:36s remains)
INFO - root - 2017-12-05 11:12:56.831100: step 3240, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 77h:47m:16s remains)
INFO - root - 2017-12-05 11:13:05.429924: step 3250, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 77h:31m:51s remains)
INFO - root - 2017-12-05 11:13:13.946685: step 3260, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 77h:09m:53s remains)
INFO - root - 2017-12-05 11:13:22.432916: step 3270, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 77h:15m:50s remains)
INFO - root - 2017-12-05 11:13:30.944028: step 3280, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 78h:23m:47s remains)
INFO - root - 2017-12-05 11:13:39.571943: step 3290, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 80h:34m:30s remains)
INFO - root - 2017-12-05 11:13:47.953858: step 3300, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 78h:44m:23s remains)
2017-12-05 11:13:48.670609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2349658 -4.2628441 -4.289597 -4.305829 -4.3084273 -4.3050385 -4.2945852 -4.2736979 -4.2526016 -4.2337179 -4.2106924 -4.1881185 -4.1909494 -4.2179627 -4.2384024][-4.2329431 -4.2678719 -4.299787 -4.3178787 -4.3218479 -4.3220816 -4.3180003 -4.3087687 -4.3008766 -4.2923164 -4.2801976 -4.2664075 -4.2622762 -4.2665944 -4.2625656][-4.2143154 -4.2515106 -4.2838593 -4.302321 -4.308794 -4.3132997 -4.3139391 -4.3134608 -4.3172174 -4.3199949 -4.3191223 -4.313859 -4.3062067 -4.2957907 -4.2745523][-4.2206774 -4.2473588 -4.2673879 -4.2758117 -4.2770977 -4.2781258 -4.2758765 -4.2768855 -4.2879682 -4.3008523 -4.3118773 -4.318 -4.316215 -4.3071661 -4.2859664][-4.24886 -4.256413 -4.2563863 -4.2475829 -4.2332478 -4.2175417 -4.1999726 -4.1939692 -4.2093344 -4.234345 -4.2620478 -4.2883253 -4.304327 -4.3086438 -4.2983184][-4.2700863 -4.262043 -4.2446055 -4.2185044 -4.1831703 -4.1394095 -4.0938654 -4.07613 -4.1004143 -4.14578 -4.1959329 -4.2455935 -4.28209 -4.2982769 -4.2950439][-4.2822518 -4.2657142 -4.2342744 -4.1905704 -4.1311903 -4.055336 -3.977272 -3.9485726 -3.9894707 -4.0634303 -4.1397047 -4.2104373 -4.2601852 -4.2810307 -4.2781491][-4.2944613 -4.2768345 -4.2373462 -4.1812248 -4.1019611 -3.9971476 -3.8871272 -3.8472345 -3.9048851 -4.0039115 -4.1021061 -4.1882405 -4.2439981 -4.26445 -4.2597008][-4.3123527 -4.296567 -4.2554097 -4.1954231 -4.1105561 -3.9964573 -3.8783343 -3.8379278 -3.9005213 -4.0042839 -4.1047149 -4.1915545 -4.2449079 -4.2628145 -4.25825][-4.3297663 -4.3139739 -4.2767272 -4.2233505 -4.1504569 -4.0551977 -3.9636433 -3.9403856 -3.9954038 -4.0805588 -4.16029 -4.2289376 -4.2700324 -4.283865 -4.2799997][-4.3410358 -4.325407 -4.2949009 -4.2540336 -4.2029014 -4.1407523 -4.0895052 -4.0858831 -4.1280131 -4.1844449 -4.232656 -4.2742229 -4.300036 -4.3109035 -4.3097844][-4.3421283 -4.325541 -4.3005309 -4.2730021 -4.244585 -4.2146783 -4.1975884 -4.2076983 -4.23593 -4.2654505 -4.2845492 -4.2993207 -4.3107672 -4.32105 -4.3246741][-4.329916 -4.3099055 -4.2888908 -4.2760019 -4.2684746 -4.2632012 -4.2681446 -4.2843237 -4.3004136 -4.3075995 -4.303359 -4.2959046 -4.2968278 -4.3094621 -4.320653][-4.3085203 -4.2868929 -4.2704058 -4.2694674 -4.2766938 -4.286242 -4.3025618 -4.3203707 -4.3284445 -4.3209267 -4.2990108 -4.2741451 -4.2655578 -4.2791038 -4.297379][-4.2913094 -4.2724929 -4.26086 -4.2663755 -4.2800083 -4.2952642 -4.3136678 -4.3288574 -4.3318624 -4.3188353 -4.2893362 -4.2553868 -4.2392812 -4.2495408 -4.2713709]]...]
INFO - root - 2017-12-05 11:13:57.043390: step 3310, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.822 sec/batch; 75h:12m:25s remains)
INFO - root - 2017-12-05 11:14:05.394289: step 3320, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 75h:58m:03s remains)
INFO - root - 2017-12-05 11:14:13.720013: step 3330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 79h:45m:31s remains)
INFO - root - 2017-12-05 11:14:22.175933: step 3340, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 78h:14m:45s remains)
INFO - root - 2017-12-05 11:14:30.705242: step 3350, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.874 sec/batch; 79h:56m:24s remains)
INFO - root - 2017-12-05 11:14:39.210565: step 3360, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 77h:39m:45s remains)
INFO - root - 2017-12-05 11:14:47.772522: step 3370, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 80h:11m:22s remains)
INFO - root - 2017-12-05 11:14:56.306593: step 3380, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 78h:05m:37s remains)
INFO - root - 2017-12-05 11:15:04.817377: step 3390, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 76h:25m:18s remains)
INFO - root - 2017-12-05 11:15:13.223386: step 3400, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 76h:31m:56s remains)
2017-12-05 11:15:14.003402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0780859 -4.1030321 -4.1270738 -4.1459718 -4.1528316 -4.1493521 -4.1512151 -4.1646209 -4.1852379 -4.1969132 -4.2078342 -4.2017989 -4.191267 -4.1874757 -4.1845326][-4.0888286 -4.1256571 -4.1545968 -4.1744785 -4.1849179 -4.1845717 -4.1825237 -4.1858807 -4.1954317 -4.2079425 -4.224194 -4.2240415 -4.2110419 -4.1950345 -4.1807337][-4.1054826 -4.1522756 -4.1843219 -4.2041321 -4.2171469 -4.2169466 -4.2057676 -4.1943841 -4.19513 -4.2079749 -4.2289686 -4.238162 -4.2259169 -4.1984348 -4.1747127][-4.1383295 -4.1808352 -4.2034039 -4.2171488 -4.2281351 -4.2222047 -4.2003775 -4.1767211 -4.1752787 -4.1924706 -4.2235155 -4.2419686 -4.2330394 -4.2027249 -4.17731][-4.1628127 -4.1953135 -4.2064323 -4.209774 -4.2129245 -4.1961884 -4.1611485 -4.1256728 -4.1258993 -4.1518197 -4.1917319 -4.2206211 -4.2184567 -4.1960335 -4.17935][-4.1714668 -4.19455 -4.1962447 -4.1886373 -4.1793365 -4.1461792 -4.0899348 -4.0367384 -4.0389996 -4.0869522 -4.142942 -4.1804924 -4.1892734 -4.1851573 -4.1835179][-4.1561189 -4.16606 -4.159658 -4.1419535 -4.1180286 -4.0671153 -3.9843698 -3.9122102 -3.9296174 -4.0176582 -4.1021562 -4.1560326 -4.1807513 -4.1902061 -4.19647][-4.1431208 -4.1349111 -4.1192284 -4.0970583 -4.0663028 -4.00608 -3.9134679 -3.8455353 -3.8947618 -4.0144463 -4.1146765 -4.1734958 -4.2029071 -4.2150221 -4.2159386][-4.147058 -4.13286 -4.1172075 -4.0972924 -4.0689054 -4.0261993 -3.970808 -3.9465446 -3.9982641 -4.0942206 -4.1695542 -4.2100577 -4.227272 -4.2313042 -4.2218485][-4.1550283 -4.1423459 -4.1350865 -4.1262603 -4.1101484 -4.0902228 -4.0667667 -4.0624948 -4.0980444 -4.1613722 -4.2069354 -4.2222257 -4.2202597 -4.2130351 -4.1959553][-4.1637206 -4.1481686 -4.1484375 -4.1488252 -4.1394982 -4.12801 -4.1163836 -4.1184044 -4.1453767 -4.1880512 -4.2147436 -4.2126513 -4.1937833 -4.1723857 -4.1504536][-4.1722012 -4.1518664 -4.1538963 -4.1584144 -4.1529408 -4.1451688 -4.1390533 -4.146389 -4.1710582 -4.2006431 -4.2135339 -4.19769 -4.1659346 -4.133111 -4.1062512][-4.1678424 -4.1450777 -4.1422453 -4.1450429 -4.1413293 -4.14055 -4.1416993 -4.1545091 -4.1809034 -4.20547 -4.2091866 -4.1827021 -4.1448731 -4.1119218 -4.0895052][-4.1581831 -4.13837 -4.1293993 -4.128531 -4.1220374 -4.1203556 -4.1197915 -4.133029 -4.1625476 -4.1862955 -4.1907992 -4.163537 -4.1287274 -4.1061883 -4.0926123][-4.1598153 -4.14822 -4.13715 -4.1268854 -4.1125078 -4.1001897 -4.0943861 -4.107923 -4.1351275 -4.1596289 -4.167902 -4.1486712 -4.1235571 -4.1136856 -4.1103978]]...]
INFO - root - 2017-12-05 11:15:22.598567: step 3410, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.821 sec/batch; 75h:00m:47s remains)
INFO - root - 2017-12-05 11:15:31.179664: step 3420, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 77h:08m:50s remains)
INFO - root - 2017-12-05 11:15:39.641094: step 3430, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 77h:07m:16s remains)
INFO - root - 2017-12-05 11:15:48.261135: step 3440, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 78h:37m:18s remains)
INFO - root - 2017-12-05 11:15:56.776500: step 3450, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 77h:26m:40s remains)
INFO - root - 2017-12-05 11:16:05.138448: step 3460, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 78h:42m:06s remains)
INFO - root - 2017-12-05 11:16:13.640312: step 3470, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 78h:46m:48s remains)
INFO - root - 2017-12-05 11:16:22.138516: step 3480, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 75h:16m:47s remains)
INFO - root - 2017-12-05 11:16:30.649046: step 3490, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 77h:12m:34s remains)
INFO - root - 2017-12-05 11:16:39.178788: step 3500, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 82h:30m:01s remains)
2017-12-05 11:16:39.960405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0962934 -4.1136069 -4.1276689 -4.1311312 -4.14063 -4.1549215 -4.1632433 -4.1636248 -4.1682844 -4.1732388 -4.1840458 -4.1900153 -4.1759057 -4.15567 -4.1357164][-4.0684123 -4.0840282 -4.0994039 -4.10458 -4.1168027 -4.1352396 -4.1404037 -4.1344728 -4.1389565 -4.1441526 -4.1579313 -4.1700196 -4.1588726 -4.1377354 -4.1130338][-4.0493593 -4.0651827 -4.0871835 -4.0982327 -4.1117392 -4.1285496 -4.1290536 -4.1147037 -4.1176133 -4.1224036 -4.1372132 -4.15211 -4.1443634 -4.12026 -4.0890512][-4.0697269 -4.0867672 -4.1108427 -4.1264529 -4.1379056 -4.1445742 -4.1337347 -4.1114235 -4.1114125 -4.1176777 -4.1328168 -4.1487808 -4.1459088 -4.1209593 -4.0866632][-4.1261573 -4.132699 -4.1459155 -4.1556854 -4.1623211 -4.160233 -4.1400261 -4.1134086 -4.1157041 -4.1281509 -4.148387 -4.1667118 -4.1685815 -4.1465316 -4.11056][-4.165731 -4.1550035 -4.1558743 -4.1607819 -4.1627746 -4.1519232 -4.1248579 -4.0995808 -4.10999 -4.1337075 -4.1606894 -4.183033 -4.193522 -4.1788239 -4.1410718][-4.1724467 -4.153821 -4.1497369 -4.1482725 -4.1453733 -4.1297383 -4.1017365 -4.08236 -4.0981169 -4.1264558 -4.1508346 -4.1713581 -4.1874828 -4.1781678 -4.1414003][-4.1754808 -4.1570358 -4.15303 -4.1509891 -4.1529026 -4.1426477 -4.1237931 -4.110755 -4.1217647 -4.1407437 -4.152719 -4.1613612 -4.1719532 -4.1613173 -4.1288157][-4.1691508 -4.1557331 -4.161623 -4.1726027 -4.18861 -4.1898069 -4.1831503 -4.175992 -4.1793246 -4.1849213 -4.1827321 -4.1782484 -4.1797948 -4.1708994 -4.1504683][-4.1727381 -4.1664586 -4.1822295 -4.2037077 -4.2256508 -4.2319875 -4.232985 -4.2308893 -4.2299004 -4.2280211 -4.2204623 -4.210743 -4.2059231 -4.198576 -4.189446][-4.171731 -4.1777115 -4.2026672 -4.2277317 -4.2491226 -4.2578506 -4.2636914 -4.2678528 -4.26857 -4.2657337 -4.2595811 -4.2535167 -4.2474856 -4.2371631 -4.2287378][-4.1695361 -4.1864319 -4.2140942 -4.2414131 -4.2644939 -4.2756414 -4.285059 -4.2922683 -4.2944746 -4.2947154 -4.2930193 -4.2892389 -4.2829227 -4.2741427 -4.2696238][-4.175437 -4.1982751 -4.2251391 -4.2486792 -4.2690754 -4.2809277 -4.291554 -4.29754 -4.300539 -4.3048596 -4.3084879 -4.3069825 -4.3021603 -4.297493 -4.2967782][-4.1731439 -4.1965346 -4.2224412 -4.2453384 -4.2654128 -4.27821 -4.2882071 -4.290503 -4.2916059 -4.2961078 -4.3014812 -4.3039865 -4.3050261 -4.3055553 -4.3045659][-4.1729631 -4.1913252 -4.2147307 -4.2391691 -4.2603083 -4.273756 -4.2826424 -4.2828884 -4.282269 -4.2842813 -4.2904921 -4.2961788 -4.3011079 -4.3067279 -4.3087935]]...]
INFO - root - 2017-12-05 11:16:48.463886: step 3510, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 80h:02m:04s remains)
INFO - root - 2017-12-05 11:16:57.012932: step 3520, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 77h:44m:20s remains)
INFO - root - 2017-12-05 11:17:05.481912: step 3530, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 79h:11m:56s remains)
INFO - root - 2017-12-05 11:17:14.020745: step 3540, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.834 sec/batch; 76h:13m:20s remains)
INFO - root - 2017-12-05 11:17:22.654952: step 3550, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 78h:54m:46s remains)
INFO - root - 2017-12-05 11:17:31.146886: step 3560, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 80h:43m:58s remains)
INFO - root - 2017-12-05 11:17:39.600841: step 3570, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 79h:33m:46s remains)
INFO - root - 2017-12-05 11:17:48.206223: step 3580, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 79h:43m:12s remains)
INFO - root - 2017-12-05 11:17:56.713053: step 3590, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 80h:36m:55s remains)
INFO - root - 2017-12-05 11:18:05.334371: step 3600, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 79h:45m:02s remains)
2017-12-05 11:18:06.091749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2556934 -4.2392807 -4.223073 -4.1909628 -4.1463952 -4.1176667 -4.1160531 -4.1225863 -4.1301255 -4.1495094 -4.1680574 -4.1687036 -4.1450453 -4.1178207 -4.1033239][-4.2399712 -4.2266674 -4.2184467 -4.1940336 -4.1491566 -4.1113353 -4.1035795 -4.1130052 -4.1292729 -4.1524734 -4.1740212 -4.1785316 -4.1573095 -4.1294918 -4.1105814][-4.2145743 -4.2140718 -4.2168884 -4.2041745 -4.1656036 -4.1239133 -4.1093106 -4.1208711 -4.1471434 -4.1732693 -4.1947746 -4.2015467 -4.18778 -4.165029 -4.1464076][-4.1918473 -4.205308 -4.2151022 -4.2125483 -4.1821213 -4.1409435 -4.1217332 -4.1334066 -4.1669936 -4.1959596 -4.2181888 -4.2285314 -4.2221079 -4.2071886 -4.1926718][-4.1646886 -4.186801 -4.2001114 -4.20487 -4.1830835 -4.1449876 -4.1236548 -4.1368361 -4.17391 -4.2061691 -4.2317095 -4.248229 -4.2493553 -4.240972 -4.2297568][-4.1360512 -4.1557236 -4.1687679 -4.1780148 -4.168282 -4.138638 -4.1192284 -4.1326256 -4.1674328 -4.1976466 -4.2244921 -4.2481527 -4.2578659 -4.2566938 -4.2492142][-4.1177096 -4.1231546 -4.1321163 -4.1446195 -4.1456094 -4.1280651 -4.1151671 -4.1278682 -4.1561708 -4.1802797 -4.2024007 -4.227807 -4.2455721 -4.2516022 -4.2496614][-4.129539 -4.1161261 -4.1167827 -4.1327715 -4.1445761 -4.1400328 -4.1347237 -4.1435161 -4.16026 -4.1722379 -4.1843371 -4.2070689 -4.2278843 -4.2408543 -4.2467523][-4.1691828 -4.1442595 -4.1352997 -4.1481824 -4.163054 -4.16637 -4.1680031 -4.174675 -4.1820688 -4.181118 -4.1844859 -4.202486 -4.2208538 -4.2359624 -4.248004][-4.2153454 -4.1896858 -4.1742506 -4.1801119 -4.190403 -4.1928554 -4.196095 -4.2000852 -4.2009692 -4.1933455 -4.195652 -4.2139773 -4.22763 -4.2385082 -4.2519865][-4.2457685 -4.2271862 -4.212276 -4.2132339 -4.21674 -4.2161145 -4.215097 -4.2141237 -4.2091513 -4.202424 -4.2089968 -4.2286892 -4.240098 -4.2485528 -4.2593822][-4.2496309 -4.2376204 -4.2257948 -4.2240019 -4.2234373 -4.2210665 -4.2168121 -4.2103004 -4.2021017 -4.1993852 -4.2098951 -4.228641 -4.2410159 -4.2497368 -4.2575178][-4.2448492 -4.237009 -4.228806 -4.2278028 -4.2283907 -4.2262158 -4.2196803 -4.2085385 -4.1983857 -4.1958046 -4.20673 -4.22503 -4.2381792 -4.2464633 -4.2523651][-4.245616 -4.2409344 -4.2364721 -4.2384324 -4.2429447 -4.2454448 -4.2417016 -4.2315049 -4.221395 -4.2168503 -4.2235494 -4.2381897 -4.2483888 -4.2534833 -4.2564645][-4.2517147 -4.2494965 -4.24661 -4.2493062 -4.2558088 -4.2616134 -4.2622085 -4.2565832 -4.2495012 -4.2441273 -4.2453566 -4.2535677 -4.2587738 -4.2603345 -4.2605057]]...]
INFO - root - 2017-12-05 11:18:14.710524: step 3610, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 77h:54m:57s remains)
INFO - root - 2017-12-05 11:18:23.340028: step 3620, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 78h:04m:31s remains)
INFO - root - 2017-12-05 11:18:31.934667: step 3630, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 78h:26m:23s remains)
INFO - root - 2017-12-05 11:18:40.520708: step 3640, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 79h:47m:22s remains)
INFO - root - 2017-12-05 11:18:49.127218: step 3650, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.821 sec/batch; 74h:59m:58s remains)
INFO - root - 2017-12-05 11:18:57.595582: step 3660, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 77h:09m:21s remains)
INFO - root - 2017-12-05 11:19:06.105726: step 3670, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 76h:17m:10s remains)
INFO - root - 2017-12-05 11:19:14.555542: step 3680, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.826 sec/batch; 75h:28m:21s remains)
INFO - root - 2017-12-05 11:19:23.209920: step 3690, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 79h:21m:52s remains)
INFO - root - 2017-12-05 11:19:31.672987: step 3700, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.812 sec/batch; 74h:10m:04s remains)
2017-12-05 11:19:32.430005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2418032 -4.2487521 -4.2410264 -4.2150841 -4.1865239 -4.1721268 -4.1691117 -4.1704946 -4.1735158 -4.1632152 -4.1648312 -4.1816959 -4.1881242 -4.1837039 -4.1874795][-4.2194662 -4.2304339 -4.2274666 -4.2051039 -4.1776738 -4.1673908 -4.1665854 -4.1643286 -4.1523285 -4.1268878 -4.1181617 -4.1365781 -4.1501212 -4.1533689 -4.161684][-4.1846504 -4.1941357 -4.1939111 -4.172224 -4.1494 -4.1470795 -4.1530209 -4.1513176 -4.1326885 -4.1007385 -4.0924473 -4.1191058 -4.1414924 -4.1533222 -4.1608496][-4.1422415 -4.1475482 -4.1467133 -4.1260066 -4.1106648 -4.1170359 -4.1321845 -4.1405244 -4.1346273 -4.1187434 -4.1238637 -4.1533809 -4.1715512 -4.1770144 -4.1720414][-4.0971417 -4.0991049 -4.1005759 -4.0834208 -4.0737195 -4.0828156 -4.099854 -4.1139913 -4.1238284 -4.1285024 -4.1450982 -4.1756821 -4.1887784 -4.1856623 -4.1671548][-4.0688639 -4.068686 -4.0664654 -4.0485263 -4.0355148 -4.037107 -4.0420671 -4.0492406 -4.0662708 -4.0864391 -4.1147079 -4.1492963 -4.1623893 -4.1594906 -4.1417737][-4.0200505 -4.0058265 -3.9826355 -3.9489455 -3.9260764 -3.9148946 -3.8996801 -3.8975499 -3.9271069 -3.9679053 -4.0172911 -4.0659671 -4.0890179 -4.0947371 -4.0929971][-3.9519923 -3.908247 -3.8503358 -3.7863088 -3.7490706 -3.73197 -3.7053423 -3.703423 -3.7559223 -3.8241115 -3.8956296 -3.9580076 -3.9924095 -4.0137439 -4.0383997][-3.979794 -3.9276547 -3.8639007 -3.7999601 -3.765903 -3.7557836 -3.7396264 -3.7421248 -3.7866776 -3.8400097 -3.8960848 -3.9451637 -3.9745312 -3.9964125 -4.0273447][-4.0874367 -4.0504518 -4.0066504 -3.9653673 -3.9464087 -3.944242 -3.9392433 -3.9418471 -3.9615202 -3.9832034 -4.00983 -4.0364795 -4.0515032 -4.0623984 -4.0806103][-4.1843257 -4.1608424 -4.132946 -4.1072345 -4.0955334 -4.0948758 -4.0949736 -4.0983014 -4.1058373 -4.1117005 -4.1231647 -4.13826 -4.1448236 -4.1452866 -4.151206][-4.2493572 -4.2320495 -4.2124534 -4.1951976 -4.1876879 -4.189651 -4.1943059 -4.1987863 -4.2013416 -4.2001123 -4.2025547 -4.2083292 -4.2080173 -4.2049227 -4.2068825][-4.2834296 -4.2692471 -4.2539759 -4.2402453 -4.2328453 -4.2328258 -4.2362494 -4.2405758 -4.2423682 -4.2411575 -4.2412963 -4.243331 -4.2430987 -4.2423234 -4.2445731][-4.2992373 -4.28859 -4.2763157 -4.2650051 -4.2573147 -4.2566018 -4.2592797 -4.2613335 -4.2620034 -4.2608409 -4.2603106 -4.2611761 -4.2624803 -4.2642355 -4.2671337][-4.3062234 -4.2993722 -4.2905784 -4.2822566 -4.2762513 -4.276834 -4.2806029 -4.2827282 -4.2821693 -4.2802105 -4.2787786 -4.2778754 -4.2778621 -4.27793 -4.2793183]]...]
INFO - root - 2017-12-05 11:19:41.005508: step 3710, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.856 sec/batch; 78h:08m:16s remains)
INFO - root - 2017-12-05 11:19:49.598537: step 3720, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 78h:13m:27s remains)
INFO - root - 2017-12-05 11:19:58.149158: step 3730, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 78h:28m:28s remains)
INFO - root - 2017-12-05 11:20:06.681637: step 3740, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 78h:18m:01s remains)
INFO - root - 2017-12-05 11:20:15.264896: step 3750, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 78h:16m:36s remains)
INFO - root - 2017-12-05 11:20:23.728924: step 3760, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 79h:19m:54s remains)
INFO - root - 2017-12-05 11:20:32.237895: step 3770, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 79h:36m:56s remains)
INFO - root - 2017-12-05 11:20:40.958329: step 3780, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 80h:18m:38s remains)
INFO - root - 2017-12-05 11:20:49.481585: step 3790, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 76h:28m:15s remains)
INFO - root - 2017-12-05 11:20:57.883686: step 3800, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.847 sec/batch; 77h:17m:27s remains)
2017-12-05 11:20:58.635856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3100262 -4.2983112 -4.2937331 -4.2964559 -4.3019338 -4.3049312 -4.3071494 -4.3088222 -4.306622 -4.3028755 -4.3011336 -4.3025727 -4.3079309 -4.3141804 -4.3186088][-4.2922726 -4.2755551 -4.267374 -4.2689037 -4.2766047 -4.2819285 -4.2869329 -4.2881484 -4.2837267 -4.2784748 -4.2776122 -4.2799473 -4.2867255 -4.29776 -4.3073468][-4.2638788 -4.23866 -4.2224059 -4.2207355 -4.23335 -4.2456059 -4.2553954 -4.2577262 -4.2554574 -4.2526064 -4.2529554 -4.2557244 -4.262733 -4.2774258 -4.2921152][-4.2345824 -4.1958079 -4.1659331 -4.1561537 -4.172298 -4.193027 -4.2079916 -4.2168403 -4.2224641 -4.2266088 -4.2304387 -4.2346115 -4.2437158 -4.2615075 -4.2799354][-4.2176867 -4.1649156 -4.1183434 -4.0914693 -4.0981636 -4.118639 -4.1357884 -4.1532855 -4.1782489 -4.2010632 -4.215373 -4.226964 -4.240562 -4.2583447 -4.277319][-4.2164011 -4.1506305 -4.0888944 -4.0412669 -4.0223546 -4.0218196 -4.0228434 -4.0381694 -4.0873637 -4.1402373 -4.17435 -4.197753 -4.2191672 -4.2427692 -4.2659864][-4.220356 -4.1511459 -4.0826268 -4.0186749 -3.9692006 -3.9305732 -3.881073 -3.8673902 -3.9430361 -4.0404172 -4.1047521 -4.144166 -4.1776361 -4.2094164 -4.2402091][-4.2293749 -4.1650877 -4.0986481 -4.0318065 -3.9726772 -3.9048958 -3.7939878 -3.7174366 -3.7973485 -3.9314516 -4.0282078 -4.0901895 -4.1404037 -4.1800671 -4.2202935][-4.2469406 -4.1982093 -4.1486869 -4.1000443 -4.0615263 -4.0063357 -3.9022679 -3.812953 -3.8425817 -3.9388068 -4.0284529 -4.0946403 -4.1479483 -4.1875925 -4.2265735][-4.2584772 -4.2209029 -4.1859593 -4.1598783 -4.148057 -4.1223431 -4.0642843 -4.0133681 -4.0142488 -4.0600228 -4.1153235 -4.1605535 -4.1971259 -4.2225337 -4.2463417][-4.2593589 -4.2285252 -4.2003164 -4.1855421 -4.1888752 -4.1848459 -4.1657419 -4.1516762 -4.1514449 -4.1725535 -4.2021208 -4.2286968 -4.251359 -4.2644286 -4.272872][-4.256999 -4.2353206 -4.2127266 -4.2015829 -4.2087946 -4.2136407 -4.2139826 -4.2239804 -4.2333832 -4.2478161 -4.2667661 -4.2832017 -4.2971807 -4.3031535 -4.3022246][-4.2644229 -4.2517085 -4.2364626 -4.2262187 -4.2310925 -4.2374477 -4.2455645 -4.2616038 -4.273663 -4.2880206 -4.3062415 -4.3192644 -4.3271637 -4.3287039 -4.3232427][-4.2909942 -4.287106 -4.2818193 -4.2758307 -4.2762065 -4.2798462 -4.2883677 -4.3005233 -4.3087339 -4.3170028 -4.3296442 -4.339417 -4.3425531 -4.3404679 -4.3336782][-4.3153405 -4.3148203 -4.3162546 -4.315824 -4.3153434 -4.3149147 -4.3187242 -4.3238344 -4.328052 -4.3327208 -4.3402691 -4.3467197 -4.3479986 -4.345305 -4.3405371]]...]
INFO - root - 2017-12-05 11:21:07.243499: step 3810, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 75h:36m:54s remains)
INFO - root - 2017-12-05 11:21:15.593430: step 3820, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 77h:32m:03s remains)
INFO - root - 2017-12-05 11:21:24.008360: step 3830, loss = 2.05, batch loss = 2.00 (9.9 examples/sec; 0.809 sec/batch; 73h:51m:37s remains)
INFO - root - 2017-12-05 11:21:32.506515: step 3840, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 77h:51m:39s remains)
INFO - root - 2017-12-05 11:21:41.075233: step 3850, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 77h:23m:36s remains)
INFO - root - 2017-12-05 11:21:49.565208: step 3860, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 78h:24m:41s remains)
INFO - root - 2017-12-05 11:21:57.917508: step 3870, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 75h:46m:38s remains)
INFO - root - 2017-12-05 11:22:06.534674: step 3880, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 78h:37m:55s remains)
INFO - root - 2017-12-05 11:22:15.061319: step 3890, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 79h:26m:19s remains)
INFO - root - 2017-12-05 11:22:23.472642: step 3900, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.818 sec/batch; 74h:39m:10s remains)
2017-12-05 11:22:24.195586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1948838 -4.1347404 -4.0798831 -4.0653038 -4.0890536 -4.1305332 -4.1766558 -4.2088242 -4.2109485 -4.1980143 -4.1821375 -4.1687188 -4.1547284 -4.1513257 -4.16313][-4.1866708 -4.1153336 -4.0497842 -4.0357966 -4.0755606 -4.131628 -4.1833119 -4.2112441 -4.2066445 -4.1888766 -4.1713686 -4.1601338 -4.1583772 -4.1604772 -4.1714258][-4.1790442 -4.0967741 -4.0185266 -4.0000753 -4.05483 -4.1310449 -4.1867881 -4.2124591 -4.2054276 -4.1855164 -4.1684771 -4.1574545 -4.1601624 -4.16312 -4.1721983][-4.1751695 -4.0895004 -4.0023732 -3.9769771 -4.0370131 -4.128202 -4.1888342 -4.2156672 -4.2094078 -4.1901317 -4.1748395 -4.1637435 -4.1639385 -4.1662211 -4.1765394][-4.1781487 -4.0941458 -4.0031238 -3.9677529 -4.0231419 -4.1201997 -4.1881175 -4.2192707 -4.2160993 -4.1999373 -4.1849575 -4.173172 -4.1718 -4.1765437 -4.190239][-4.1899881 -4.1114149 -4.0222864 -3.9761398 -4.0211678 -4.1172304 -4.1883011 -4.2208252 -4.2220006 -4.2090917 -4.1939025 -4.1835613 -4.1857862 -4.1917191 -4.2047677][-4.2075009 -4.1388683 -4.0575724 -4.0051174 -4.0349588 -4.1190147 -4.1840744 -4.21427 -4.2196355 -4.2078009 -4.1894479 -4.1791954 -4.1824827 -4.1905141 -4.2041411][-4.2171025 -4.1589088 -4.0887823 -4.0396919 -4.0593829 -4.1289353 -4.1861377 -4.2127166 -4.219244 -4.2041621 -4.1810083 -4.1672316 -4.1702805 -4.1823511 -4.1984205][-4.2207656 -4.1697006 -4.1120176 -4.0745516 -4.0939212 -4.1540422 -4.20397 -4.2223682 -4.2278481 -4.2136817 -4.192452 -4.1779675 -4.17638 -4.1857181 -4.1992559][-4.218441 -4.1726637 -4.1245704 -4.0971794 -4.1206722 -4.1751637 -4.2200685 -4.2332735 -4.2386866 -4.2268553 -4.2117472 -4.2017465 -4.1976719 -4.2025394 -4.2111626][-4.2122278 -4.1715937 -4.12924 -4.1092925 -4.1352453 -4.1854591 -4.2281013 -4.2403812 -4.24559 -4.2377157 -4.2280116 -4.219296 -4.2150917 -4.2184072 -4.224596][-4.2106185 -4.1769876 -4.1404891 -4.1242824 -4.1475868 -4.190496 -4.2284222 -4.24102 -4.247211 -4.2439446 -4.2406278 -4.2362065 -4.2341175 -4.2367277 -4.2418151][-4.2162423 -4.190052 -4.160778 -4.1475129 -4.1649723 -4.1992583 -4.2315373 -4.243175 -4.2494526 -4.2485242 -4.2478027 -4.2460117 -4.2474313 -4.2540646 -4.2629042][-4.225 -4.2064729 -4.1840224 -4.1732869 -4.1865664 -4.2128096 -4.2371049 -4.2451911 -4.2469087 -4.2443109 -4.2447362 -4.246376 -4.25161 -4.260036 -4.2713265][-4.2413721 -4.2283149 -4.2121539 -4.2038603 -4.2130361 -4.2320018 -4.2478518 -4.2512 -4.247252 -4.2405429 -4.2385283 -4.2421756 -4.2507257 -4.2621059 -4.2752256]]...]
INFO - root - 2017-12-05 11:22:32.882687: step 3910, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 78h:30m:58s remains)
INFO - root - 2017-12-05 11:22:41.368548: step 3920, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 77h:08m:36s remains)
INFO - root - 2017-12-05 11:22:49.831156: step 3930, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.877 sec/batch; 80h:04m:25s remains)
INFO - root - 2017-12-05 11:22:58.458834: step 3940, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 80h:52m:58s remains)
INFO - root - 2017-12-05 11:23:07.012860: step 3950, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 75h:52m:22s remains)
INFO - root - 2017-12-05 11:23:15.525665: step 3960, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 78h:20m:26s remains)
INFO - root - 2017-12-05 11:23:24.079822: step 3970, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 76h:48m:06s remains)
INFO - root - 2017-12-05 11:23:32.623451: step 3980, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 78h:55m:48s remains)
INFO - root - 2017-12-05 11:23:41.158337: step 3990, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.850 sec/batch; 77h:33m:41s remains)
INFO - root - 2017-12-05 11:23:49.741583: step 4000, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 80h:07m:20s remains)
2017-12-05 11:23:50.513870: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2086029 -4.21022 -4.2205195 -4.2346873 -4.2495756 -4.2741637 -4.2997961 -4.3143196 -4.3180819 -4.3154078 -4.3063793 -4.2953706 -4.2843523 -4.275126 -4.2798233][-4.20004 -4.2062736 -4.2187715 -4.2304144 -4.2386894 -4.2553296 -4.2783594 -4.2970362 -4.3076725 -4.3083482 -4.302146 -4.2939453 -4.2825313 -4.2699556 -4.2720165][-4.1889572 -4.1943727 -4.2061214 -4.2157555 -4.2189794 -4.2252903 -4.243691 -4.2667642 -4.2868333 -4.2931457 -4.2917066 -4.2876673 -4.2772017 -4.2599854 -4.256938][-4.173861 -4.1741948 -4.184423 -4.1951914 -4.1999712 -4.19977 -4.2114234 -4.2357469 -4.2624226 -4.2752652 -4.2779446 -4.2760053 -4.2658873 -4.2470064 -4.2378025][-4.1649451 -4.1617708 -4.1732488 -4.1859684 -4.1896343 -4.1810684 -4.1832151 -4.2057333 -4.2351074 -4.2530236 -4.260891 -4.2623253 -4.2559667 -4.2375159 -4.2236733][-4.167397 -4.1637182 -4.1748347 -4.1852589 -4.1815982 -4.160419 -4.1480894 -4.1670947 -4.2012897 -4.226428 -4.2435031 -4.2530155 -4.2537169 -4.2411728 -4.2269363][-4.1861849 -4.1863751 -4.1937327 -4.1956487 -4.1766462 -4.1331992 -4.0967178 -4.1089597 -4.1553698 -4.1977959 -4.23033 -4.25108 -4.258697 -4.2526627 -4.2403374][-4.2102041 -4.21527 -4.2166576 -4.20958 -4.17964 -4.1166234 -4.0532336 -4.0568542 -4.1186128 -4.180892 -4.2263808 -4.2535992 -4.2641478 -4.2601953 -4.2484035][-4.2269034 -4.2336435 -4.2297616 -4.2185564 -4.1879339 -4.1271348 -4.0582004 -4.0519328 -4.1125631 -4.1808176 -4.2301888 -4.257978 -4.2657523 -4.2610049 -4.2501192][-4.2388659 -4.243638 -4.2367125 -4.22468 -4.1970873 -4.1475391 -4.0917854 -4.0795746 -4.1257157 -4.1875734 -4.2351737 -4.2612743 -4.2673545 -4.2632747 -4.2542343][-4.2431293 -4.2473755 -4.2424436 -4.2345462 -4.2126465 -4.17489 -4.1310883 -4.1139097 -4.1456442 -4.1971517 -4.2396646 -4.264214 -4.2721167 -4.2694964 -4.26152][-4.2442045 -4.2473922 -4.2429767 -4.2378879 -4.2227707 -4.1973953 -4.16772 -4.1508546 -4.1716695 -4.2136788 -4.24865 -4.2704506 -4.2793818 -4.2755828 -4.2670941][-4.2487397 -4.2487526 -4.2399192 -4.2319765 -4.2210493 -4.2066112 -4.1910477 -4.1810021 -4.19704 -4.2316132 -4.2593694 -4.2755613 -4.2816634 -4.2746973 -4.2653627][-4.2586884 -4.2543535 -4.2395582 -4.2215891 -4.2069054 -4.1993041 -4.1947136 -4.1945882 -4.2084904 -4.2394719 -4.2625313 -4.2749419 -4.280262 -4.2731547 -4.2630506][-4.2680221 -4.2600708 -4.2386284 -4.2107148 -4.1900787 -4.1853065 -4.1889257 -4.1933179 -4.2060957 -4.2349157 -4.2595944 -4.2759771 -4.2847128 -4.27962 -4.2691836]]...]
INFO - root - 2017-12-05 11:23:59.061356: step 4010, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 78h:44m:53s remains)
INFO - root - 2017-12-05 11:24:07.654596: step 4020, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.835 sec/batch; 76h:09m:09s remains)
INFO - root - 2017-12-05 11:24:16.293386: step 4030, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.902 sec/batch; 82h:18m:17s remains)
INFO - root - 2017-12-05 11:24:24.734781: step 4040, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 77h:16m:57s remains)
INFO - root - 2017-12-05 11:24:33.363667: step 4050, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 79h:05m:22s remains)
INFO - root - 2017-12-05 11:24:41.999526: step 4060, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 77h:49m:46s remains)
INFO - root - 2017-12-05 11:24:50.579687: step 4070, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.827 sec/batch; 75h:27m:32s remains)
INFO - root - 2017-12-05 11:24:59.066195: step 4080, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 77h:52m:12s remains)
INFO - root - 2017-12-05 11:25:07.564508: step 4090, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 80h:46m:18s remains)
INFO - root - 2017-12-05 11:25:16.074242: step 4100, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 79h:17m:50s remains)
2017-12-05 11:25:16.814638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2934647 -4.2953095 -4.2985992 -4.3009405 -4.304894 -4.3101134 -4.314364 -4.3154383 -4.3134546 -4.3107061 -4.3103633 -4.3102565 -4.3092074 -4.3080006 -4.3062859][-4.2516761 -4.2568564 -4.2618451 -4.2642274 -4.267735 -4.2750535 -4.2839022 -4.2889566 -4.2877269 -4.2851377 -4.28617 -4.2872734 -4.2853689 -4.282258 -4.2793808][-4.2136574 -4.2192035 -4.2193327 -4.21593 -4.2160521 -4.2254138 -4.2428951 -4.2573061 -4.2625365 -4.2630339 -4.2668629 -4.2698035 -4.26739 -4.2612581 -4.254436][-4.1977129 -4.1980605 -4.1878662 -4.1735954 -4.1646147 -4.1694841 -4.1930714 -4.2196326 -4.2428794 -4.256609 -4.2653956 -4.27023 -4.2677617 -4.25869 -4.2453766][-4.1970954 -4.1883445 -4.1683807 -4.1424112 -4.1190434 -4.1063933 -4.1200337 -4.1549087 -4.2038045 -4.2398214 -4.2588253 -4.2699132 -4.2720766 -4.2626362 -4.246707][-4.1940532 -4.1764755 -4.1472573 -4.1113477 -4.0679574 -4.0207219 -3.998461 -4.0298152 -4.1069336 -4.1727142 -4.2099833 -4.2340207 -4.2459445 -4.2444534 -4.2347317][-4.1919308 -4.1669497 -4.1292672 -4.0856414 -4.0228462 -3.9351659 -3.8591361 -3.8681984 -3.9658074 -4.0563712 -4.1113887 -4.1495709 -4.1789966 -4.1966782 -4.2062058][-4.2015443 -4.1748543 -4.1356258 -4.0926886 -4.0270448 -3.9275751 -3.825942 -3.8064327 -3.8905973 -3.9756238 -4.0267076 -4.0636911 -4.1061082 -4.1460037 -4.1744347][-4.2287908 -4.2069097 -4.1727371 -4.138855 -4.0913777 -4.020453 -3.9495714 -3.9306993 -3.9728961 -4.0166125 -4.0388584 -4.0564861 -4.0916395 -4.131268 -4.1591368][-4.2606654 -4.2488222 -4.2232208 -4.1976867 -4.1683331 -4.1297078 -4.09334 -4.085042 -4.1000547 -4.1113868 -4.1087837 -4.1034741 -4.1192455 -4.1436391 -4.1631351][-4.2874718 -4.2877741 -4.2760611 -4.2622094 -4.2457132 -4.2253194 -4.2079792 -4.2032785 -4.2023563 -4.1967673 -4.1826229 -4.1641769 -4.1608195 -4.1681924 -4.1760769][-4.3006539 -4.3104033 -4.3091702 -4.3035793 -4.2969966 -4.2890553 -4.2810578 -4.274714 -4.2660813 -4.2534709 -4.2364016 -4.2131371 -4.1973085 -4.1913509 -4.1882982][-4.2976823 -4.3125134 -4.318037 -4.3171525 -4.3165274 -4.3140879 -4.3078418 -4.2972879 -4.2835917 -4.2693377 -4.2527285 -4.2307043 -4.2127857 -4.2012472 -4.19119][-4.2827129 -4.2990484 -4.3076253 -4.3087497 -4.3093209 -4.3056731 -4.296206 -4.2821302 -4.2664566 -4.2550488 -4.2411466 -4.2231436 -4.2086253 -4.1985054 -4.1849775][-4.2690115 -4.2846079 -4.2939892 -4.2940607 -4.2911091 -4.2814612 -4.2664213 -4.2494287 -4.2339478 -4.2253776 -4.2121077 -4.1981978 -4.1891203 -4.1807222 -4.1677365]]...]
INFO - root - 2017-12-05 11:25:25.428397: step 4110, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 78h:47m:57s remains)
INFO - root - 2017-12-05 11:25:33.845715: step 4120, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 77h:31m:35s remains)
INFO - root - 2017-12-05 11:25:42.390716: step 4130, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 76h:16m:44s remains)
INFO - root - 2017-12-05 11:25:50.927847: step 4140, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.857 sec/batch; 78h:12m:09s remains)
INFO - root - 2017-12-05 11:25:59.434932: step 4150, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 75h:49m:45s remains)
INFO - root - 2017-12-05 11:26:07.910616: step 4160, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 78h:12m:30s remains)
INFO - root - 2017-12-05 11:26:16.435254: step 4170, loss = 2.09, batch loss = 2.04 (9.8 examples/sec; 0.819 sec/batch; 74h:40m:09s remains)
INFO - root - 2017-12-05 11:26:25.001275: step 4180, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 77h:36m:41s remains)
INFO - root - 2017-12-05 11:26:33.402323: step 4190, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 76h:31m:22s remains)
INFO - root - 2017-12-05 11:26:41.921625: step 4200, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 77h:44m:18s remains)
2017-12-05 11:26:42.699291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.167098 -4.18688 -4.2030869 -4.2150607 -4.2098136 -4.1829906 -4.1438589 -4.1093426 -4.0970564 -4.1177287 -4.1335888 -4.13035 -4.1331015 -4.1632051 -4.2058053][-4.16606 -4.1943631 -4.2174687 -4.2325115 -4.221128 -4.18665 -4.1488743 -4.1233177 -4.1145411 -4.1229711 -4.1245604 -4.1131663 -4.1103477 -4.133831 -4.1772475][-4.1799483 -4.2145038 -4.2422209 -4.255528 -4.2371497 -4.1935058 -4.1517897 -4.1307416 -4.1254768 -4.132463 -4.1267686 -4.1103158 -4.1017175 -4.1159391 -4.1472912][-4.1988144 -4.2325969 -4.257688 -4.2628593 -4.23783 -4.1864614 -4.1308079 -4.1028271 -4.1066117 -4.1291776 -4.1369052 -4.1232853 -4.1149578 -4.1285114 -4.1501822][-4.2230344 -4.245717 -4.2579613 -4.2513528 -4.2183452 -4.1583591 -4.0864887 -4.044085 -4.052525 -4.0948577 -4.1265969 -4.1293683 -4.1321821 -4.1529064 -4.1736107][-4.2446327 -4.2517376 -4.2502809 -4.2361465 -4.1947494 -4.1222734 -4.032403 -3.9678183 -3.9704082 -4.0287275 -4.0908494 -4.1279979 -4.1536217 -4.1824179 -4.2050867][-4.2642274 -4.2615728 -4.253139 -4.2358055 -4.1871691 -4.1042519 -4.00343 -3.9223778 -3.9080758 -3.9642439 -4.047658 -4.1173582 -4.1659603 -4.2013454 -4.2253289][-4.2773948 -4.2730842 -4.2649889 -4.2466722 -4.1990457 -4.1195879 -4.0253611 -3.9447205 -3.9123154 -3.94459 -4.0211639 -4.1015134 -4.1641622 -4.2062283 -4.2345667][-4.2815876 -4.2773833 -4.2707872 -4.2522411 -4.2113104 -4.1501579 -4.078928 -4.0114641 -3.9731927 -3.9825726 -4.0340533 -4.1043777 -4.1689663 -4.214664 -4.244483][-4.2809367 -4.276114 -4.2710648 -4.2571607 -4.2301364 -4.1890955 -4.13668 -4.0813479 -4.0433278 -4.0408072 -4.0730281 -4.1290874 -4.187727 -4.2322373 -4.2573776][-4.2816339 -4.2757711 -4.2710857 -4.2632852 -4.2508926 -4.2285004 -4.1921725 -4.1475382 -4.11199 -4.1017051 -4.12317 -4.1667085 -4.2162962 -4.256074 -4.2766256][-4.2996397 -4.2932658 -4.2888222 -4.28427 -4.2782917 -4.2661033 -4.2427292 -4.212657 -4.1839867 -4.16886 -4.1786442 -4.2106309 -4.2515225 -4.2836537 -4.299768][-4.312973 -4.3062749 -4.3027649 -4.3001971 -4.2985072 -4.2942986 -4.28277 -4.2642107 -4.2434168 -4.2282887 -4.2303462 -4.2522788 -4.280057 -4.3020539 -4.3139629][-4.3183527 -4.3121133 -4.30871 -4.3074112 -4.3089924 -4.31035 -4.307488 -4.29607 -4.28044 -4.2675371 -4.2677326 -4.2826371 -4.2998796 -4.3123841 -4.3195314][-4.3231268 -4.31799 -4.3138256 -4.3116937 -4.3119845 -4.313612 -4.3135 -4.30929 -4.3005176 -4.2934322 -4.29418 -4.3046045 -4.3157654 -4.3217692 -4.324255]]...]
INFO - root - 2017-12-05 11:26:51.321449: step 4210, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 78h:03m:41s remains)
INFO - root - 2017-12-05 11:26:59.777384: step 4220, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.831 sec/batch; 75h:44m:08s remains)
INFO - root - 2017-12-05 11:27:08.257330: step 4230, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.834 sec/batch; 76h:05m:12s remains)
INFO - root - 2017-12-05 11:27:16.772880: step 4240, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 77h:16m:37s remains)
INFO - root - 2017-12-05 11:27:25.278804: step 4250, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 75h:49m:07s remains)
INFO - root - 2017-12-05 11:27:33.790991: step 4260, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 76h:39m:35s remains)
INFO - root - 2017-12-05 11:27:42.358000: step 4270, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 78h:17m:13s remains)
INFO - root - 2017-12-05 11:27:50.825845: step 4280, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 76h:33m:45s remains)
INFO - root - 2017-12-05 11:27:59.369942: step 4290, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 77h:32m:40s remains)
INFO - root - 2017-12-05 11:28:07.809008: step 4300, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 80h:49m:45s remains)
2017-12-05 11:28:08.662586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.365881 -4.3607831 -4.3584986 -4.3588533 -4.3607063 -4.3609977 -4.361834 -4.3644748 -4.3678317 -4.3703928 -4.3724904 -4.3751125 -4.3788619 -4.3819761 -4.38213][-4.3525972 -4.3422489 -4.3378944 -4.3385587 -4.3412228 -4.3400254 -4.3391252 -4.3433037 -4.3513746 -4.3576026 -4.3627424 -4.3683944 -4.3751221 -4.3809166 -4.3832049][-4.3282623 -4.3070512 -4.2965121 -4.2964382 -4.2961941 -4.285851 -4.2783604 -4.2829857 -4.2979379 -4.316267 -4.334002 -4.3479695 -4.3610992 -4.3719616 -4.37933][-4.3022251 -4.2674336 -4.2432504 -4.2322116 -4.2179861 -4.1906013 -4.1709743 -4.1744637 -4.198853 -4.2345762 -4.2726731 -4.3016505 -4.3272209 -4.3500323 -4.3671017][-4.272244 -4.2205119 -4.1742573 -4.1428394 -4.1094408 -4.0630531 -4.0284553 -4.0264926 -4.0574579 -4.1103978 -4.1714973 -4.2236185 -4.2739458 -4.3187351 -4.3497639][-4.2462106 -4.1794968 -4.1130881 -4.0602045 -4.0113044 -3.9464216 -3.8846126 -3.861047 -3.8899148 -3.9593239 -4.0471692 -4.1287436 -4.2116089 -4.2851434 -4.3339486][-4.2396431 -4.1690679 -4.0951562 -4.03111 -3.9683964 -3.8839188 -3.7965651 -3.7493482 -3.7718787 -3.851409 -3.9612703 -4.0685892 -4.1756163 -4.2666273 -4.3268223][-4.2523685 -4.1896935 -4.1240563 -4.0623751 -3.9966507 -3.9058487 -3.8151782 -3.7624326 -3.7850494 -3.8688776 -3.9830208 -4.0946817 -4.1993718 -4.2848015 -4.3390846][-4.2836194 -4.2348371 -4.1823788 -4.1321149 -4.0771885 -3.998862 -3.9189527 -3.8764689 -3.9024115 -3.97707 -4.0720224 -4.1651368 -4.2531905 -4.3223233 -4.363049][-4.3239679 -4.2908144 -4.2521334 -4.2152395 -4.1759343 -4.1238623 -4.0706592 -4.0438704 -4.0627995 -4.1120644 -4.1768608 -4.2432961 -4.3080635 -4.358583 -4.3853865][-4.3552809 -4.3361659 -4.3113642 -4.2885623 -4.2673821 -4.2410927 -4.2130094 -4.1961455 -4.2035136 -4.2287927 -4.2670608 -4.30979 -4.3513603 -4.3822861 -4.3954735][-4.3713622 -4.3612 -4.3462353 -4.3329263 -4.3221321 -4.3100491 -4.2981768 -4.2917972 -4.295064 -4.3083673 -4.3318563 -4.3566012 -4.37699 -4.3893175 -4.3922739][-4.3799896 -4.375701 -4.36792 -4.3598762 -4.3531075 -4.3459239 -4.3399014 -4.3381453 -4.341433 -4.3495774 -4.3623514 -4.3737035 -4.3814073 -4.3844371 -4.3833637][-4.3844967 -4.3826108 -4.3790965 -4.374969 -4.3708115 -4.3663387 -4.362545 -4.3619666 -4.3649149 -4.368959 -4.3732004 -4.3758664 -4.3774796 -4.3776412 -4.375587][-4.3861451 -4.3847413 -4.3830061 -4.3814259 -4.3796883 -4.3774123 -4.3748021 -4.374629 -4.3766146 -4.3780885 -4.3779945 -4.3773451 -4.377059 -4.375936 -4.3738923]]...]
INFO - root - 2017-12-05 11:28:17.238247: step 4310, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 78h:59m:33s remains)
INFO - root - 2017-12-05 11:28:25.824867: step 4320, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 82h:20m:19s remains)
INFO - root - 2017-12-05 11:28:34.253027: step 4330, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 79h:17m:49s remains)
INFO - root - 2017-12-05 11:28:43.043050: step 4340, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 78h:53m:07s remains)
INFO - root - 2017-12-05 11:28:51.548086: step 4350, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 76h:18m:45s remains)
INFO - root - 2017-12-05 11:28:59.965993: step 4360, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 78h:03m:59s remains)
INFO - root - 2017-12-05 11:29:08.669019: step 4370, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 76h:29m:16s remains)
INFO - root - 2017-12-05 11:29:17.176979: step 4380, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 77h:40m:19s remains)
INFO - root - 2017-12-05 11:29:25.677369: step 4390, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 77h:34m:27s remains)
INFO - root - 2017-12-05 11:29:34.158229: step 4400, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 79h:34m:11s remains)
2017-12-05 11:29:34.881223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16512 -4.1913939 -4.203217 -4.1954489 -4.186316 -4.1933374 -4.2015638 -4.1900153 -4.16435 -4.1485133 -4.1535015 -4.1730509 -4.197166 -4.2049408 -4.1904826][-4.1462803 -4.1686344 -4.1781068 -4.1719971 -4.1677389 -4.1788759 -4.1870437 -4.1730685 -4.1448579 -4.1295257 -4.1382971 -4.1555324 -4.1703029 -4.1685071 -4.1460404][-4.1005354 -4.1207395 -4.1318579 -4.1294842 -4.1265254 -4.13381 -4.1420178 -4.1345668 -4.1197834 -4.1118393 -4.1158571 -4.124053 -4.1231074 -4.1123838 -4.0933452][-4.0221586 -4.0404162 -4.0612469 -4.0665693 -4.0640616 -4.0716777 -4.0814657 -4.0867634 -4.0903935 -4.0924892 -4.0864477 -4.0797119 -4.067234 -4.0524735 -4.0417738][-3.9543877 -3.9736834 -4.0033445 -4.015276 -4.0118833 -4.0217872 -4.0340691 -4.0431089 -4.0515785 -4.0535855 -4.0378532 -4.0262976 -4.0155416 -4.0080967 -4.0053558][-3.9441442 -3.9626107 -3.9871721 -3.9937491 -3.9846048 -3.9887073 -3.9881818 -3.9795103 -3.983809 -3.9952712 -3.9839513 -3.9800067 -3.9758615 -3.9704602 -3.9734638][-3.9678178 -3.9791589 -3.9877965 -3.9728158 -3.9484594 -3.9320641 -3.8928831 -3.8437333 -3.85491 -3.9007931 -3.9089375 -3.9180264 -3.9286249 -3.9315324 -3.946631][-4.0196605 -4.016006 -4.002707 -3.9686944 -3.9279985 -3.8806159 -3.8044732 -3.7319546 -3.7642512 -3.8434877 -3.8696504 -3.8954651 -3.9267457 -3.9468687 -3.9807289][-4.0903616 -4.0726366 -4.0480871 -4.0109429 -3.9693809 -3.9178839 -3.8534937 -3.812571 -3.8591089 -3.9263792 -3.9454756 -3.97145 -4.0097427 -4.0349646 -4.0685015][-4.1419711 -4.1224785 -4.0987768 -4.069324 -4.0388288 -4.0059557 -3.9742825 -3.9635391 -4.0001545 -4.0409102 -4.0486846 -4.0659914 -4.0976453 -4.1166997 -4.1423573][-4.173039 -4.1619439 -4.1446843 -4.1246443 -4.1063027 -4.0925031 -4.0851564 -4.0874705 -4.1081276 -4.1293373 -4.1319671 -4.1411014 -4.16032 -4.1723375 -4.1905203][-4.2094064 -4.2010632 -4.1878953 -4.1751385 -4.1643548 -4.1601562 -4.1640387 -4.1686125 -4.1760163 -4.1856723 -4.18646 -4.18558 -4.1897678 -4.1920366 -4.2028022][-4.2517776 -4.2405152 -4.2283111 -4.2171617 -4.2098665 -4.2080903 -4.2111526 -4.2119269 -4.2100644 -4.2100639 -4.2061119 -4.1965351 -4.1874013 -4.1791954 -4.1833496][-4.2860136 -4.2755127 -4.26623 -4.2613831 -4.2602706 -4.2602282 -4.2605476 -4.2589483 -4.2539773 -4.2492604 -4.2406616 -4.2239671 -4.2044115 -4.1854043 -4.1773796][-4.3132691 -4.3061152 -4.3004627 -4.2979655 -4.2973781 -4.2969589 -4.2956476 -4.2935081 -4.2898631 -4.2856503 -4.2781553 -4.2639594 -4.2443151 -4.220191 -4.1991706]]...]
INFO - root - 2017-12-05 11:29:43.363743: step 4410, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 78h:02m:30s remains)
INFO - root - 2017-12-05 11:29:52.032562: step 4420, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 77h:14m:39s remains)
INFO - root - 2017-12-05 11:30:00.657209: step 4430, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 77h:28m:57s remains)
INFO - root - 2017-12-05 11:30:09.169348: step 4440, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 81h:04m:33s remains)
INFO - root - 2017-12-05 11:30:17.647043: step 4450, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 76h:56m:48s remains)
INFO - root - 2017-12-05 11:30:26.162045: step 4460, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 78h:32m:34s remains)
INFO - root - 2017-12-05 11:30:34.615968: step 4470, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 76h:56m:40s remains)
INFO - root - 2017-12-05 11:30:43.359383: step 4480, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 79h:57m:00s remains)
INFO - root - 2017-12-05 11:30:51.894111: step 4490, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 79h:29m:39s remains)
INFO - root - 2017-12-05 11:31:00.237442: step 4500, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 78h:37m:06s remains)
2017-12-05 11:31:00.972630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1976776 -4.2019534 -4.200243 -4.2043405 -4.21719 -4.2208872 -4.1945477 -4.1508608 -4.109776 -4.0913506 -4.1055384 -4.1468077 -4.2074447 -4.266046 -4.3069034][-4.1958404 -4.2018723 -4.1981807 -4.1956749 -4.2024837 -4.2046232 -4.1779232 -4.1282663 -4.0736403 -4.0481806 -4.0674114 -4.11799 -4.1912589 -4.2614741 -4.308465][-4.193675 -4.1973939 -4.1888676 -4.177352 -4.1794024 -4.1839142 -4.1635394 -4.1175079 -4.0562186 -4.0236645 -4.0435967 -4.099009 -4.1796708 -4.2562757 -4.3084893][-4.1814156 -4.17861 -4.1607962 -4.139257 -4.1381044 -4.14936 -4.1399097 -4.1017056 -4.0406675 -4.005228 -4.0275788 -4.0865431 -4.1702533 -4.249114 -4.3069983][-4.1834526 -4.1757703 -4.1488643 -4.1163235 -4.1051583 -4.1166577 -4.114646 -4.0855827 -4.0241885 -3.9882948 -4.0130677 -4.0751696 -4.15994 -4.2407703 -4.3031545][-4.1909294 -4.1873016 -4.1551523 -4.1099343 -4.079483 -4.0725584 -4.0635624 -4.03899 -3.9827321 -3.9542484 -3.9887967 -4.060133 -4.1495886 -4.2325277 -4.2976828][-4.1767206 -4.1907039 -4.1631088 -4.1107306 -4.0595093 -4.0315623 -4.0088539 -3.9846685 -3.9349732 -3.918612 -3.9640303 -4.0412321 -4.13595 -4.2236385 -4.2923021][-4.1486855 -4.1850944 -4.1755924 -4.1308651 -4.078496 -4.0406919 -4.0046053 -3.9674222 -3.911968 -3.8982499 -3.9487073 -4.0272236 -4.1240296 -4.2158604 -4.28634][-4.1234021 -4.1692586 -4.1808772 -4.15554 -4.1135807 -4.0771275 -4.033227 -3.9837534 -3.9213784 -3.8987424 -3.9428709 -4.0233555 -4.1208005 -4.2145042 -4.2847958][-4.11416 -4.1513991 -4.1758618 -4.172411 -4.14827 -4.1227064 -4.0827155 -4.0344934 -3.9705184 -3.9336288 -3.9619961 -4.0328588 -4.1257381 -4.218627 -4.2874718][-4.1334429 -4.153821 -4.1703987 -4.1721387 -4.1599474 -4.1481428 -4.1193972 -4.0789309 -4.0204253 -3.9813695 -4.0021014 -4.0628285 -4.14544 -4.230052 -4.2930775][-4.1632814 -4.1696997 -4.170712 -4.1681471 -4.161787 -4.1589036 -4.1423769 -4.1124477 -4.0634446 -4.0307775 -4.052495 -4.1072845 -4.1779628 -4.2499251 -4.3028345][-4.1847582 -4.184279 -4.1782293 -4.1716104 -4.1684294 -4.1718526 -4.1642408 -4.1431484 -4.10489 -4.0790558 -4.1009245 -4.1513743 -4.2120838 -4.2717013 -4.3134589][-4.2085905 -4.2041888 -4.19578 -4.1902647 -4.1915727 -4.1986694 -4.1962204 -4.1818924 -4.1541834 -4.1349831 -4.1544623 -4.1955895 -4.2445717 -4.2902732 -4.3207245][-4.2367725 -4.2336693 -4.2302403 -4.2314367 -4.2360559 -4.2422152 -4.242054 -4.232502 -4.2158413 -4.2052484 -4.2172532 -4.2436709 -4.2763371 -4.3060436 -4.3254576]]...]
INFO - root - 2017-12-05 11:31:09.450073: step 4510, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 80h:39m:21s remains)
INFO - root - 2017-12-05 11:31:17.915619: step 4520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 78h:04m:52s remains)
INFO - root - 2017-12-05 11:31:26.526501: step 4530, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 78h:06m:15s remains)
INFO - root - 2017-12-05 11:31:34.960445: step 4540, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 76h:34m:09s remains)
INFO - root - 2017-12-05 11:31:43.524450: step 4550, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 75h:51m:14s remains)
INFO - root - 2017-12-05 11:31:51.971695: step 4560, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 75h:53m:53s remains)
INFO - root - 2017-12-05 11:32:00.358115: step 4570, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 77h:03m:23s remains)
INFO - root - 2017-12-05 11:32:08.764894: step 4580, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 79h:11m:59s remains)
INFO - root - 2017-12-05 11:32:17.246750: step 4590, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 76h:18m:38s remains)
INFO - root - 2017-12-05 11:32:25.708757: step 4600, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 75h:45m:56s remains)
2017-12-05 11:32:26.501787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2310266 -4.2289019 -4.2281876 -4.2289228 -4.2296491 -4.2307348 -4.2317591 -4.2320862 -4.2313995 -4.2285571 -4.2292004 -4.2326822 -4.2335272 -4.2319961 -4.2312274][-4.2243018 -4.2217865 -4.2207847 -4.221839 -4.2224913 -4.2230978 -4.2239828 -4.2237277 -4.2223954 -4.2171364 -4.2148967 -4.2161312 -4.2169318 -4.216259 -4.2157927][-4.2263527 -4.2240338 -4.2234955 -4.2246737 -4.2255945 -4.2255745 -4.22576 -4.2251911 -4.2230735 -4.2168632 -4.21266 -4.2136574 -4.2177138 -4.2209735 -4.2238121][-4.2277412 -4.2225575 -4.2197819 -4.2190852 -4.2191181 -4.2192292 -4.2196536 -4.2214456 -4.2234216 -4.2219968 -4.2202311 -4.222929 -4.230957 -4.2364359 -4.2402925][-4.2300291 -4.2205338 -4.2126222 -4.2079477 -4.2041793 -4.2010059 -4.1993427 -4.2022243 -4.21027 -4.2188082 -4.2251863 -4.2348709 -4.2484655 -4.2569594 -4.2627091][-4.2028508 -4.1891117 -4.1752844 -4.1647673 -4.1577787 -4.15065 -4.1429529 -4.144485 -4.1605821 -4.1829343 -4.2025719 -4.2250819 -4.2476192 -4.2627192 -4.2732267][-4.1523 -4.1306176 -4.1051483 -4.0846796 -4.0686011 -4.0502629 -4.0288792 -4.0238786 -4.0482044 -4.0870247 -4.1244535 -4.164763 -4.2020078 -4.2301955 -4.2509556][-4.1213374 -4.0963326 -4.0655675 -4.0388989 -4.013556 -3.9806397 -3.9407935 -3.920289 -3.9403596 -3.9832416 -4.0309696 -4.0852022 -4.13589 -4.1763463 -4.2077103][-4.1428671 -4.1239161 -4.1018081 -4.0845766 -4.0665278 -4.0377188 -3.9982939 -3.9702625 -3.9719658 -3.9946177 -4.0274811 -4.0740781 -4.1240063 -4.165287 -4.1974139][-4.1936445 -4.1806326 -4.1658673 -4.1569324 -4.1483021 -4.132535 -4.1087136 -4.0880375 -4.0852637 -4.0967655 -4.1158075 -4.1477695 -4.1842957 -4.21416 -4.2358212][-4.2311759 -4.2235689 -4.2143674 -4.2103329 -4.2090287 -4.2039776 -4.192667 -4.1812382 -4.1805553 -4.1894126 -4.2043238 -4.2274442 -4.2532172 -4.2715487 -4.2805643][-4.25486 -4.2486591 -4.2441721 -4.2448149 -4.249404 -4.2526512 -4.2511926 -4.2489095 -4.2511697 -4.2563453 -4.2655883 -4.279171 -4.2938724 -4.3046374 -4.3061013][-4.26754 -4.2639742 -4.2614722 -4.2630286 -4.2690449 -4.275219 -4.2778459 -4.2809095 -4.2871065 -4.2872896 -4.28707 -4.2907143 -4.2951965 -4.300456 -4.2989173][-4.2570376 -4.2585778 -4.2592335 -4.2607069 -4.2674417 -4.2741241 -4.27484 -4.2783127 -4.2858615 -4.2811046 -4.2721314 -4.2680583 -4.2672448 -4.270802 -4.2720795][-4.24232 -4.2459135 -4.2466626 -4.2470946 -4.2518854 -4.255446 -4.2535076 -4.2551265 -4.2617035 -4.2542052 -4.2382793 -4.2286377 -4.2278872 -4.2351432 -4.2440753]]...]
INFO - root - 2017-12-05 11:32:35.015874: step 4610, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 78h:44m:56s remains)
INFO - root - 2017-12-05 11:32:43.442534: step 4620, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 77h:27m:58s remains)
INFO - root - 2017-12-05 11:32:51.978206: step 4630, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:07m:00s remains)
INFO - root - 2017-12-05 11:33:00.449938: step 4640, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 78h:15m:12s remains)
INFO - root - 2017-12-05 11:33:09.056848: step 4650, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 78h:38m:39s remains)
INFO - root - 2017-12-05 11:33:17.582486: step 4660, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 77h:15m:23s remains)
INFO - root - 2017-12-05 11:33:26.016552: step 4670, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 79h:15m:32s remains)
INFO - root - 2017-12-05 11:33:34.588470: step 4680, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 76h:15m:43s remains)
INFO - root - 2017-12-05 11:33:43.025619: step 4690, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 78h:26m:14s remains)
INFO - root - 2017-12-05 11:33:51.581133: step 4700, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 78h:39m:16s remains)
2017-12-05 11:33:52.382438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2606726 -4.267693 -4.2710762 -4.2750044 -4.2777214 -4.27829 -4.2785845 -4.2782221 -4.2780361 -4.2773604 -4.2772045 -4.2755337 -4.2628131 -4.2436213 -4.2276015][-4.3051109 -4.3142009 -4.316771 -4.3175588 -4.3151388 -4.3094511 -4.3038645 -4.2996655 -4.2948322 -4.2892919 -4.2834597 -4.2756972 -4.258512 -4.2357678 -4.2186232][-4.3273559 -4.3368335 -4.3399711 -4.3391938 -4.3311524 -4.3161931 -4.3028288 -4.2942638 -4.2856584 -4.2764373 -4.2659817 -4.2530174 -4.2342477 -4.2153482 -4.2076349][-4.3232369 -4.3327804 -4.3369308 -4.3351107 -4.3212061 -4.2977266 -4.2792234 -4.2680655 -4.2592392 -4.2501545 -4.2384348 -4.2208667 -4.1997695 -4.1848831 -4.1870332][-4.2934661 -4.3041954 -4.3096108 -4.3077397 -4.290298 -4.2620292 -4.2430849 -4.2338672 -4.2276039 -4.219173 -4.2061577 -4.1865439 -4.1670947 -4.1557727 -4.162662][-4.24947 -4.2577152 -4.260695 -4.2542496 -4.2282624 -4.1940489 -4.1818075 -4.1849041 -4.1887574 -4.1864214 -4.1756372 -4.1566591 -4.1401343 -4.132987 -4.1402059][-4.2020583 -4.2001014 -4.1924052 -4.1746154 -4.1329651 -4.0918646 -4.0952306 -4.1226683 -4.1453195 -4.1540146 -4.1502371 -4.1371937 -4.1292419 -4.1333747 -4.1441031][-4.1695633 -4.1529427 -4.1304522 -4.0956764 -4.0315571 -3.979423 -4.0014172 -4.0565815 -4.1002631 -4.1209936 -4.1232224 -4.116796 -4.1229157 -4.1465812 -4.1684508][-4.1683917 -4.1446371 -4.1149287 -4.0711737 -3.9942722 -3.9317915 -3.9571533 -4.0199666 -4.072288 -4.0989418 -4.1017413 -4.0957074 -4.1070457 -4.1446095 -4.181705][-4.1715307 -4.1530762 -4.1331677 -4.1071973 -4.0534225 -4.0079708 -4.0236878 -4.0671105 -4.1066709 -4.1285768 -4.1294942 -4.1198559 -4.1186032 -4.1457396 -4.1853762][-4.1780777 -4.1687422 -4.1627717 -4.1585245 -4.1335497 -4.1100979 -4.120245 -4.14542 -4.1666203 -4.1808324 -4.1799936 -4.1663589 -4.1513996 -4.1586485 -4.1871047][-4.1865191 -4.1858921 -4.1928449 -4.2049193 -4.1992564 -4.1917439 -4.2008038 -4.2128677 -4.2219687 -4.2297516 -4.224895 -4.20714 -4.1842031 -4.1741238 -4.1860085][-4.2022343 -4.2072353 -4.2230587 -4.243907 -4.2458072 -4.2394714 -4.2425671 -4.2454538 -4.2498503 -4.2566452 -4.2500877 -4.2333984 -4.21011 -4.1887364 -4.1835132][-4.2401586 -4.2420669 -4.2548628 -4.2683868 -4.262372 -4.2450304 -4.23699 -4.2375116 -4.2462459 -4.2574887 -4.2553387 -4.2437463 -4.2230959 -4.1974831 -4.1829338][-4.2803879 -4.2778292 -4.2815185 -4.280911 -4.2606735 -4.2294693 -4.2111425 -4.2096038 -4.2227516 -4.2415671 -4.2477202 -4.2416759 -4.2230611 -4.1992607 -4.1855035]]...]
INFO - root - 2017-12-05 11:34:00.927807: step 4710, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 80h:14m:26s remains)
INFO - root - 2017-12-05 11:34:09.328385: step 4720, loss = 2.02, batch loss = 1.96 (9.2 examples/sec; 0.866 sec/batch; 78h:49m:51s remains)
INFO - root - 2017-12-05 11:34:17.816417: step 4730, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 77h:21m:43s remains)
INFO - root - 2017-12-05 11:34:26.447417: step 4740, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 75h:32m:07s remains)
INFO - root - 2017-12-05 11:34:34.956868: step 4750, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 79h:14m:45s remains)
INFO - root - 2017-12-05 11:34:43.521676: step 4760, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 76h:05m:32s remains)
INFO - root - 2017-12-05 11:34:52.053797: step 4770, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 78h:56m:59s remains)
INFO - root - 2017-12-05 11:35:00.493180: step 4780, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.846 sec/batch; 77h:01m:12s remains)
INFO - root - 2017-12-05 11:35:09.138776: step 4790, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 79h:03m:53s remains)
INFO - root - 2017-12-05 11:35:17.585263: step 4800, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.876 sec/batch; 79h:43m:34s remains)
2017-12-05 11:35:18.330508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2804079 -4.2877946 -4.2883439 -4.2833314 -4.2789631 -4.2807484 -4.2906275 -4.30271 -4.3143511 -4.3244557 -4.326467 -4.3175368 -4.3052726 -4.2994447 -4.3024621][-4.2245507 -4.2427535 -4.2523093 -4.2544994 -4.255549 -4.2597313 -4.2713904 -4.2867165 -4.3042216 -4.3221979 -4.3307371 -4.3278623 -4.3177748 -4.310153 -4.3100834][-4.164362 -4.1895342 -4.2047668 -4.2120905 -4.2151332 -4.2197571 -4.2313609 -4.2475605 -4.2714691 -4.2991924 -4.3160686 -4.3203168 -4.3155169 -4.3095684 -4.308805][-4.1203461 -4.1494937 -4.1644459 -4.170846 -4.1721883 -4.172574 -4.1788812 -4.191649 -4.2177935 -4.2548046 -4.283637 -4.2994332 -4.3036146 -4.3032036 -4.3052411][-4.0949044 -4.1229944 -4.1346745 -4.1365738 -4.133173 -4.1267214 -4.1234155 -4.1288848 -4.1547089 -4.2003446 -4.2393351 -4.2671218 -4.281848 -4.2895179 -4.2955389][-4.0996094 -4.1184187 -4.1190276 -4.1101933 -4.0967655 -4.0784292 -4.0602274 -4.0525322 -4.0775418 -4.1346922 -4.1873317 -4.2248197 -4.2460632 -4.2595615 -4.27007][-4.1320848 -4.1389022 -4.1268177 -4.1009488 -4.0662737 -4.0274391 -3.9846647 -3.9538729 -3.9769764 -4.0553517 -4.1314993 -4.1794934 -4.2031789 -4.2172995 -4.2291031][-4.1745 -4.1765203 -4.157095 -4.1160145 -4.0598507 -3.9977496 -3.9283886 -3.8658712 -3.8739586 -3.9720898 -4.0766163 -4.1391377 -4.1665378 -4.1804428 -4.1926723][-4.2083788 -4.2148051 -4.1982803 -4.1523733 -4.0864949 -4.0107503 -3.9256659 -3.8464556 -3.8361425 -3.9289856 -4.0402594 -4.1098948 -4.1404095 -4.1559653 -4.16952][-4.2330441 -4.247797 -4.2370515 -4.1956029 -4.1346354 -4.0637279 -3.9836593 -3.9099386 -3.8899655 -3.953023 -4.042871 -4.1052856 -4.13355 -4.1498122 -4.1635685][-4.2459512 -4.2664232 -4.265275 -4.2391448 -4.1946807 -4.1392021 -4.0763822 -4.0192013 -3.9970949 -4.030129 -4.0881381 -4.1356707 -4.1593037 -4.1703858 -4.175447][-4.250587 -4.2707348 -4.2812376 -4.2734241 -4.2476845 -4.2079387 -4.1630197 -4.1250997 -4.10929 -4.1250863 -4.1576271 -4.1898136 -4.2072597 -4.2113361 -4.2033792][-4.2489629 -4.2669582 -4.2827353 -4.2856407 -4.2720189 -4.2471242 -4.2205954 -4.2024293 -4.2013917 -4.2132664 -4.2283726 -4.2456012 -4.2563329 -4.2573166 -4.2436953][-4.2380166 -4.2510624 -4.2649317 -4.2714558 -4.2672086 -4.256197 -4.246706 -4.2450023 -4.2568913 -4.2690759 -4.274817 -4.2811732 -4.2857504 -4.2877541 -4.27791][-4.2263818 -4.2335076 -4.2422361 -4.2485428 -4.2516823 -4.2519093 -4.2523966 -4.2601204 -4.2777276 -4.2905345 -4.2918396 -4.2922812 -4.2956314 -4.3017664 -4.2992873]]...]
INFO - root - 2017-12-05 11:35:26.852609: step 4810, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 78h:05m:36s remains)
INFO - root - 2017-12-05 11:35:35.486176: step 4820, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 77h:25m:52s remains)
INFO - root - 2017-12-05 11:35:44.013475: step 4830, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 79h:46m:48s remains)
INFO - root - 2017-12-05 11:35:52.532702: step 4840, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 78h:31m:47s remains)
INFO - root - 2017-12-05 11:36:00.908941: step 4850, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 78h:23m:03s remains)
INFO - root - 2017-12-05 11:36:09.454345: step 4860, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 77h:28m:35s remains)
INFO - root - 2017-12-05 11:36:18.001539: step 4870, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 74h:56m:39s remains)
INFO - root - 2017-12-05 11:36:26.579989: step 4880, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 76h:27m:46s remains)
INFO - root - 2017-12-05 11:36:35.067743: step 4890, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 78h:02m:53s remains)
INFO - root - 2017-12-05 11:36:43.582964: step 4900, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 79h:12m:40s remains)
2017-12-05 11:36:44.371058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2342434 -4.2231793 -4.2045956 -4.1947532 -4.2012744 -4.21451 -4.2291822 -4.2448339 -4.2613211 -4.2781038 -4.2896118 -4.2878642 -4.2763848 -4.2645068 -4.2574878][-4.2385864 -4.2293682 -4.2092485 -4.1995153 -4.2054567 -4.2155614 -4.222414 -4.2325768 -4.2506289 -4.2712874 -4.2866492 -4.288341 -4.2784414 -4.2654662 -4.2555752][-4.2332215 -4.2230134 -4.199008 -4.1853971 -4.1870661 -4.1915803 -4.1893573 -4.1909685 -4.2078042 -4.2296767 -4.247961 -4.2541814 -4.2508063 -4.2439079 -4.2373676][-4.218698 -4.2078009 -4.1813335 -4.1630211 -4.1606121 -4.1600218 -4.1487331 -4.1408148 -4.152514 -4.171916 -4.1898365 -4.1989269 -4.20369 -4.2088766 -4.2157578][-4.194272 -4.1814017 -4.1563044 -4.1395874 -4.1400385 -4.1406364 -4.1267333 -4.1147609 -4.1228719 -4.1358056 -4.1454191 -4.1507463 -4.1610432 -4.1781597 -4.1998944][-4.1747255 -4.1605072 -4.1406913 -4.1313424 -4.1376452 -4.1413584 -4.1280107 -4.1141958 -4.1198492 -4.1228542 -4.1169863 -4.1129823 -4.126483 -4.1526928 -4.1816597][-4.1761141 -4.158927 -4.1386786 -4.1298356 -4.1349454 -4.1377087 -4.1242313 -4.1084385 -4.1118274 -4.1062908 -4.0873656 -4.081048 -4.1025 -4.1363344 -4.1708775][-4.2061052 -4.1842237 -4.1555352 -4.1384435 -4.1372256 -4.1362028 -4.1229773 -4.1113691 -4.1180143 -4.1098228 -4.0881791 -4.0852981 -4.1090422 -4.1401291 -4.1729941][-4.2368383 -4.2157741 -4.1842222 -4.1654348 -4.1659226 -4.1706266 -4.1659474 -4.1630373 -4.1710076 -4.1624012 -4.14105 -4.1339731 -4.1492071 -4.1681495 -4.1909614][-4.2417879 -4.2277722 -4.2015862 -4.1882668 -4.1937742 -4.2043018 -4.2105374 -4.2207403 -4.2345257 -4.2294121 -4.2079215 -4.1949129 -4.198575 -4.2050681 -4.2172689][-4.2338319 -4.2241364 -4.2015939 -4.1894121 -4.1918526 -4.2039151 -4.2194257 -4.2412786 -4.2656717 -4.2724786 -4.2613983 -4.2496266 -4.2452178 -4.241147 -4.2427297][-4.2367878 -4.223537 -4.1949878 -4.1738286 -4.164104 -4.1699057 -4.1898842 -4.2209039 -4.256876 -4.2789884 -4.2822657 -4.278635 -4.2726736 -4.2625237 -4.2568984][-4.2407074 -4.2218781 -4.1892252 -4.1570239 -4.1296315 -4.1223726 -4.1374774 -4.1655755 -4.201786 -4.2318792 -4.2449255 -4.2511206 -4.2524056 -4.2471933 -4.2455263][-4.2434578 -4.2200875 -4.1844239 -4.1446853 -4.1049013 -4.0853992 -4.0893159 -4.1063452 -4.13364 -4.1628685 -4.1793575 -4.192205 -4.2021079 -4.2064543 -4.2144165][-4.260756 -4.2411923 -4.2104144 -4.172823 -4.1299939 -4.0996056 -4.0861697 -4.0870428 -4.10164 -4.12274 -4.13535 -4.1505613 -4.166975 -4.1785398 -4.1939569]]...]
INFO - root - 2017-12-05 11:36:52.857865: step 4910, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 77h:30m:46s remains)
INFO - root - 2017-12-05 11:37:01.422041: step 4920, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 78h:46m:25s remains)
INFO - root - 2017-12-05 11:37:09.997534: step 4930, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 80h:37m:13s remains)
INFO - root - 2017-12-05 11:37:18.444959: step 4940, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 77h:47m:42s remains)
INFO - root - 2017-12-05 11:37:27.112500: step 4950, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 81h:37m:31s remains)
INFO - root - 2017-12-05 11:37:35.592954: step 4960, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 77h:10m:10s remains)
INFO - root - 2017-12-05 11:37:44.176858: step 4970, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 75h:48m:49s remains)
INFO - root - 2017-12-05 11:37:52.685769: step 4980, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 77h:18m:48s remains)
INFO - root - 2017-12-05 11:38:01.212052: step 4990, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 78h:06m:50s remains)
INFO - root - 2017-12-05 11:38:09.603427: step 5000, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 74h:45m:10s remains)
2017-12-05 11:38:10.361562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3511305 -4.3556314 -4.3573575 -4.357708 -4.3571196 -4.3552089 -4.35134 -4.3490429 -4.3473287 -4.3462515 -4.3483987 -4.3554525 -4.3629079 -4.3694873 -4.3748097][-4.3246932 -4.3328843 -4.3359909 -4.3364954 -4.3339171 -4.3289013 -4.3235216 -4.3213782 -4.3175116 -4.3139086 -4.3156524 -4.3263421 -4.3393741 -4.3500648 -4.3582683][-4.2770553 -4.289463 -4.2952347 -4.2982969 -4.2979083 -4.2926011 -4.2866693 -4.2837591 -4.2741284 -4.2659373 -4.2666388 -4.27945 -4.2973886 -4.3114505 -4.3247447][-4.2023463 -4.2203832 -4.2271504 -4.2308965 -4.2319465 -4.223938 -4.2173085 -4.2129574 -4.1994748 -4.1907806 -4.1926451 -4.209527 -4.236361 -4.2544522 -4.2711315][-4.1169295 -4.1390533 -4.1455 -4.1471725 -4.1425285 -4.1254845 -4.1138206 -4.1068473 -4.0986166 -4.1005535 -4.1086674 -4.1278548 -4.160656 -4.183486 -4.2045541][-4.057261 -4.0728211 -4.0712614 -4.0612593 -4.0407419 -4.0061297 -3.9799867 -3.9667974 -3.9751854 -4.0066552 -4.0288954 -4.0513334 -4.0876589 -4.1170497 -4.1513443][-4.03687 -4.0410872 -4.02428 -3.9952517 -3.9476085 -3.8799629 -3.8168416 -3.7752917 -3.8074665 -3.8862491 -3.9420609 -3.9848127 -4.0280337 -4.0632191 -4.1143093][-4.0445967 -4.0428724 -4.0200248 -3.9805479 -3.9139829 -3.8183293 -3.7124081 -3.6403875 -3.6996489 -3.8228385 -3.9116986 -3.9764566 -4.0211625 -4.0529447 -4.1109586][-4.0864992 -4.0815735 -4.058074 -4.0217834 -3.9663522 -3.8939843 -3.8102479 -3.7545578 -3.8113723 -3.9138875 -3.9888427 -4.0458384 -4.077632 -4.0964336 -4.14568][-4.1437192 -4.1320376 -4.1078992 -4.0824928 -4.0584736 -4.0326948 -3.9991007 -3.9754524 -4.010983 -4.0653148 -4.1019745 -4.1322746 -4.1419616 -4.1470642 -4.1820388][-4.1817894 -4.1726346 -4.1566491 -4.1431923 -4.140017 -4.1427994 -4.1406403 -4.1328468 -4.149106 -4.1673007 -4.1739693 -4.1829567 -4.179522 -4.1756992 -4.2030649][-4.1852326 -4.186657 -4.1830254 -4.1765523 -4.1800027 -4.1895528 -4.1985292 -4.1981874 -4.2092719 -4.2118745 -4.2078433 -4.2085624 -4.2005558 -4.1964025 -4.2199526][-4.1779857 -4.1844778 -4.1812491 -4.172451 -4.173358 -4.1809883 -4.1924648 -4.1971016 -4.2080817 -4.2058325 -4.2031727 -4.204927 -4.2003036 -4.201705 -4.2236996][-4.1732697 -4.1818228 -4.1736226 -4.1594777 -4.153964 -4.1532617 -4.1570568 -4.1586075 -4.1676636 -4.1674318 -4.1718783 -4.1811395 -4.1848512 -4.1922979 -4.212894][-4.1586695 -4.1653223 -4.1530275 -4.1348534 -4.1284151 -4.123549 -4.11402 -4.1073017 -4.1208243 -4.130826 -4.1440859 -4.1612682 -4.1730328 -4.1845541 -4.2071185]]...]
INFO - root - 2017-12-05 11:38:18.851195: step 5010, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.829 sec/batch; 75h:25m:50s remains)
INFO - root - 2017-12-05 11:38:27.318641: step 5020, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 76h:59m:56s remains)
INFO - root - 2017-12-05 11:38:35.842249: step 5030, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 76h:53m:40s remains)
INFO - root - 2017-12-05 11:38:44.396825: step 5040, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 81h:07m:06s remains)
INFO - root - 2017-12-05 11:38:52.754531: step 5050, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 76h:46m:21s remains)
INFO - root - 2017-12-05 11:39:01.302636: step 5060, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 77h:45m:07s remains)
INFO - root - 2017-12-05 11:39:09.864601: step 5070, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 77h:58m:05s remains)
INFO - root - 2017-12-05 11:39:18.456697: step 5080, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 77h:56m:37s remains)
INFO - root - 2017-12-05 11:39:27.025324: step 5090, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 76h:10m:01s remains)
INFO - root - 2017-12-05 11:39:35.547182: step 5100, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 81h:36m:13s remains)
2017-12-05 11:39:36.279735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2393174 -4.2492185 -4.244596 -4.2349539 -4.2246127 -4.2140093 -4.2070074 -4.2103252 -4.2157817 -4.2128477 -4.2026935 -4.193397 -4.1860323 -4.187325 -4.19318][-4.2125854 -4.2203603 -4.2171478 -4.2085481 -4.1991987 -4.19266 -4.1934981 -4.2067661 -4.2184863 -4.2174673 -4.2060204 -4.1922536 -4.1817427 -4.1830788 -4.18773][-4.2043672 -4.208055 -4.2062192 -4.1984954 -4.1870961 -4.177557 -4.1796489 -4.1991081 -4.2187958 -4.2244148 -4.2162051 -4.2017422 -4.1888967 -4.1866012 -4.1850195][-4.215632 -4.2168417 -4.2145934 -4.205543 -4.1877818 -4.1672344 -4.162087 -4.1809115 -4.2073679 -4.2241516 -4.2270637 -4.2223411 -4.2139163 -4.209476 -4.2010832][-4.2352333 -4.2348385 -4.2294626 -4.2141886 -4.186069 -4.1517129 -4.1337843 -4.1450653 -4.1744504 -4.2036419 -4.2225256 -4.2326932 -4.2357292 -4.2382317 -4.2323852][-4.253242 -4.2508459 -4.24048 -4.217555 -4.1800151 -4.1358137 -4.108355 -4.1120005 -4.140358 -4.177814 -4.2084608 -4.231823 -4.2472873 -4.26027 -4.2634068][-4.2647767 -4.25945 -4.2427111 -4.212338 -4.1697512 -4.1258674 -4.0983629 -4.1010928 -4.12791 -4.1651597 -4.1971416 -4.2249055 -4.2474093 -4.2689147 -4.2821083][-4.2628818 -4.2513461 -4.2272139 -4.1877394 -4.1426687 -4.1069632 -4.0906811 -4.1043696 -4.1356544 -4.1685824 -4.19565 -4.2203994 -4.24281 -4.2676468 -4.2887683][-4.2507153 -4.2303572 -4.19581 -4.1451969 -4.0965891 -4.0692167 -4.0692382 -4.1037426 -4.148057 -4.1798267 -4.2033486 -4.2241096 -4.244349 -4.2675867 -4.2897935][-4.2395239 -4.2118106 -4.16947 -4.1113715 -4.0597839 -4.0364628 -4.0488973 -4.1020994 -4.1560616 -4.1875482 -4.2109818 -4.2289753 -4.2486181 -4.2692208 -4.287364][-4.2376461 -4.21028 -4.1694245 -4.1152439 -4.0672283 -4.0474043 -4.0635161 -4.1199441 -4.1733794 -4.2036591 -4.227397 -4.2429132 -4.259706 -4.2759547 -4.2878194][-4.245769 -4.2257028 -4.1962862 -4.1587534 -4.124198 -4.1094513 -4.1211357 -4.1669264 -4.2106333 -4.2342892 -4.2529273 -4.2640839 -4.2756848 -4.2857537 -4.290935][-4.2581067 -4.2477393 -4.2343884 -4.2168007 -4.199379 -4.1914964 -4.19819 -4.2279544 -4.25644 -4.2710967 -4.280879 -4.2847009 -4.2888494 -4.2918258 -4.2910666][-4.2737112 -4.2688546 -4.2658682 -4.263113 -4.2603092 -4.2600379 -4.2645173 -4.2812963 -4.2949271 -4.3004518 -4.3007956 -4.2971029 -4.29469 -4.2924466 -4.2875528][-4.2863894 -4.2845869 -4.2860632 -4.2902327 -4.2953196 -4.2991486 -4.3018231 -4.3097143 -4.3141994 -4.3137941 -4.3096881 -4.3032489 -4.2977629 -4.2932472 -4.2874374]]...]
INFO - root - 2017-12-05 11:39:44.737463: step 5110, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.816 sec/batch; 74h:14m:27s remains)
INFO - root - 2017-12-05 11:39:53.324499: step 5120, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 79h:09m:38s remains)
INFO - root - 2017-12-05 11:40:01.780558: step 5130, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 81h:19m:22s remains)
INFO - root - 2017-12-05 11:40:10.421951: step 5140, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 80h:35m:01s remains)
INFO - root - 2017-12-05 11:40:18.843560: step 5150, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 77h:03m:18s remains)
INFO - root - 2017-12-05 11:40:27.307911: step 5160, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.853 sec/batch; 77h:35m:42s remains)
INFO - root - 2017-12-05 11:40:35.825392: step 5170, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 74h:31m:52s remains)
INFO - root - 2017-12-05 11:40:44.412429: step 5180, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 77h:46m:30s remains)
INFO - root - 2017-12-05 11:40:52.943644: step 5190, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 75h:56m:02s remains)
INFO - root - 2017-12-05 11:41:01.540655: step 5200, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 78h:13m:49s remains)
2017-12-05 11:41:02.283594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3045883 -4.2995191 -4.2928095 -4.2863865 -4.2838631 -4.2852306 -4.2890859 -4.2926397 -4.295289 -4.2970595 -4.2990427 -4.3010445 -4.3012562 -4.2990632 -4.2961154][-4.3029728 -4.3006811 -4.2912335 -4.2787261 -4.2692285 -4.2684441 -4.276649 -4.2861319 -4.2936478 -4.2977724 -4.2999177 -4.3007579 -4.299952 -4.2975173 -4.2951317][-4.2897758 -4.2950225 -4.2842312 -4.2652755 -4.2464561 -4.2415338 -4.2545304 -4.2740722 -4.2916846 -4.3024497 -4.3069491 -4.3071518 -4.3041496 -4.3004122 -4.2975745][-4.2496662 -4.2700634 -4.2597704 -4.2310514 -4.2004371 -4.1888676 -4.2076945 -4.24367 -4.2804775 -4.305069 -4.3164358 -4.3176274 -4.3132515 -4.3068304 -4.3014665][-4.1886911 -4.2280436 -4.2193041 -4.1770382 -4.1292129 -4.1050186 -4.1253886 -4.1774812 -4.2387109 -4.2888703 -4.3164234 -4.3251033 -4.3221283 -4.3145914 -4.3065019][-4.1294317 -4.1892972 -4.1860051 -4.1304708 -4.0551767 -4.0040669 -4.0125341 -4.077693 -4.1642933 -4.2436581 -4.29726 -4.3210816 -4.3245649 -4.3182955 -4.3104548][-4.09264 -4.1670833 -4.1736045 -4.1164637 -4.0178537 -3.9233651 -3.8979323 -3.9637907 -4.0707974 -4.1764994 -4.2573032 -4.3020649 -4.3165164 -4.3148665 -4.3098478][-4.1132712 -4.1786504 -4.1894474 -4.1389351 -4.0423675 -3.9293351 -3.857595 -3.8908679 -3.993782 -4.111764 -4.2100368 -4.27315 -4.3014278 -4.307168 -4.3054895][-4.1863408 -4.2216129 -4.2240181 -4.1814909 -4.1068516 -4.0214972 -3.9574614 -3.9517651 -4.0083618 -4.1016855 -4.19265 -4.2578297 -4.2937775 -4.3045125 -4.3033233][-4.2599359 -4.2676792 -4.2534819 -4.217134 -4.1720481 -4.1281548 -4.0978155 -4.0902658 -4.1118731 -4.1617651 -4.22225 -4.2723627 -4.3033338 -4.3128443 -4.3093338][-4.2979383 -4.2860789 -4.2575631 -4.2271614 -4.2085166 -4.2017574 -4.2004385 -4.2017622 -4.2089972 -4.2294064 -4.2640309 -4.2983518 -4.3221121 -4.3285193 -4.3208432][-4.2977595 -4.2741508 -4.2383595 -4.2114019 -4.2082844 -4.2199049 -4.2304688 -4.2363048 -4.2355118 -4.2389736 -4.2623868 -4.2974796 -4.3272438 -4.337534 -4.3307443][-4.2717075 -4.2437468 -4.2095337 -4.1874924 -4.1896505 -4.2002263 -4.2019272 -4.1939859 -4.180007 -4.174181 -4.1990643 -4.2446346 -4.2915554 -4.3216472 -4.3279247][-4.2439466 -4.2176685 -4.1944275 -4.1848426 -4.1907763 -4.1900611 -4.1691227 -4.1249738 -4.0736012 -4.0468884 -4.0765257 -4.1434503 -4.2174211 -4.2767215 -4.307291][-4.2259197 -4.2055092 -4.1976871 -4.2042451 -4.2166896 -4.2120714 -4.1744809 -4.1020226 -4.0090342 -3.936245 -3.9462049 -4.0275126 -4.1269174 -4.2142954 -4.2718649]]...]
INFO - root - 2017-12-05 11:41:10.756814: step 5210, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 77h:04m:44s remains)
INFO - root - 2017-12-05 11:41:19.226678: step 5220, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 78h:50m:24s remains)
INFO - root - 2017-12-05 11:41:27.885274: step 5230, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 78h:58m:19s remains)
INFO - root - 2017-12-05 11:41:36.427763: step 5240, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 79h:06m:18s remains)
INFO - root - 2017-12-05 11:41:44.833913: step 5250, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.807 sec/batch; 73h:22m:03s remains)
INFO - root - 2017-12-05 11:41:53.373062: step 5260, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.862 sec/batch; 78h:19m:57s remains)
INFO - root - 2017-12-05 11:42:01.940756: step 5270, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 79h:49m:52s remains)
INFO - root - 2017-12-05 11:42:10.390873: step 5280, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 77h:12m:31s remains)
INFO - root - 2017-12-05 11:42:18.939717: step 5290, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 76h:33m:33s remains)
INFO - root - 2017-12-05 11:42:27.456138: step 5300, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 79h:43m:16s remains)
2017-12-05 11:42:28.162233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2816 -4.2893629 -4.2915931 -4.291203 -4.2835526 -4.27509 -4.2717619 -4.2736773 -4.2746062 -4.2726979 -4.2705383 -4.2676888 -4.2744284 -4.2844672 -4.2968593][-4.2504797 -4.2608323 -4.2653661 -4.2653131 -4.254653 -4.24306 -4.235816 -4.2366209 -4.2376637 -4.2366238 -4.233345 -4.2290816 -4.2381277 -4.2528443 -4.2722125][-4.2026186 -4.2173615 -4.2268348 -4.2284322 -4.215899 -4.2001219 -4.1866708 -4.1853142 -4.1914573 -4.195827 -4.1923103 -4.1844435 -4.1904383 -4.2057276 -4.2296281][-4.1564384 -4.1777587 -4.1938972 -4.1997266 -4.1848216 -4.1581593 -4.1313734 -4.1248679 -4.1379395 -4.154182 -4.1582918 -4.1514854 -4.1540236 -4.1679659 -4.1924143][-4.119462 -4.1463604 -4.1677184 -4.1776018 -4.1612663 -4.1238685 -4.0862641 -4.0736232 -4.0927439 -4.1227164 -4.1387105 -4.1398239 -4.1435895 -4.1568227 -4.1791487][-4.0919766 -4.1212096 -4.144877 -4.1534796 -4.1301279 -4.0787239 -4.029119 -4.0091991 -4.0347614 -4.0813503 -4.1161356 -4.1333823 -4.1430545 -4.156414 -4.1724381][-4.0956554 -4.1307793 -4.15532 -4.1561284 -4.1178203 -4.0478268 -3.9839716 -3.9582069 -3.9897151 -4.0506229 -4.1036596 -4.1366463 -4.1517487 -4.1650009 -4.1749563][-4.1134467 -4.1519423 -4.1751223 -4.1687031 -4.1205068 -4.0414228 -3.9734597 -3.9497137 -3.984915 -4.0501485 -4.1099825 -4.1462827 -4.1615958 -4.17433 -4.1851025][-4.121284 -4.1620455 -4.1858993 -4.1782656 -4.1312509 -4.0553083 -3.993396 -3.9761922 -4.0090446 -4.0655861 -4.1151247 -4.1414666 -4.1517649 -4.1634269 -4.1782794][-4.1237493 -4.1643767 -4.1891303 -4.1853313 -4.1462097 -4.0826311 -4.0309443 -4.0164747 -4.0400829 -4.0817461 -4.1152191 -4.1307354 -4.1379533 -4.1513262 -4.1708241][-4.1231084 -4.1573658 -4.1801171 -4.1797285 -4.1510677 -4.1043744 -4.0653114 -4.0538793 -4.0710111 -4.1018496 -4.1235752 -4.1324849 -4.1389723 -4.154294 -4.176713][-4.1209917 -4.1480327 -4.1674275 -4.1699834 -4.1523428 -4.1219187 -4.0975842 -4.0935411 -4.1080952 -4.130311 -4.1429396 -4.1485553 -4.1544046 -4.1705165 -4.193542][-4.1244793 -4.1465588 -4.1607413 -4.1621809 -4.1498051 -4.1277 -4.1139274 -4.1182933 -4.1347642 -4.1531487 -4.1608257 -4.1641784 -4.1692615 -4.1849484 -4.2061858][-4.1351757 -4.1510749 -4.1576042 -4.1564116 -4.147439 -4.1331415 -4.1280732 -4.1394658 -4.1592245 -4.1754394 -4.1788273 -4.1782637 -4.1806221 -4.1940732 -4.2134104][-4.1412554 -4.151535 -4.1534381 -4.1525245 -4.1485877 -4.1407547 -4.1411681 -4.1551671 -4.1749134 -4.1876435 -4.1874585 -4.1861272 -4.1874347 -4.198369 -4.2161326]]...]
INFO - root - 2017-12-05 11:42:36.594842: step 5310, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.861 sec/batch; 78h:15m:49s remains)
INFO - root - 2017-12-05 11:42:45.053096: step 5320, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 77h:27m:38s remains)
INFO - root - 2017-12-05 11:42:53.455563: step 5330, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 78h:20m:39s remains)
INFO - root - 2017-12-05 11:43:01.935987: step 5340, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 77h:40m:03s remains)
INFO - root - 2017-12-05 11:43:10.419679: step 5350, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 76h:18m:46s remains)
INFO - root - 2017-12-05 11:43:18.853256: step 5360, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 77h:54m:18s remains)
INFO - root - 2017-12-05 11:43:27.125305: step 5370, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 75h:56m:56s remains)
INFO - root - 2017-12-05 11:43:35.487164: step 5380, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 77h:45m:49s remains)
INFO - root - 2017-12-05 11:43:43.996518: step 5390, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.856 sec/batch; 77h:44m:29s remains)
INFO - root - 2017-12-05 11:43:52.557356: step 5400, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 76h:02m:58s remains)
2017-12-05 11:43:53.296563: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.288157 -4.2848029 -4.2864432 -4.2846971 -4.2817497 -4.2770309 -4.2788 -4.2822952 -4.2828279 -4.2750053 -4.2654638 -4.2627568 -4.2603164 -4.2629795 -4.2763052][-4.2776012 -4.2753077 -4.2792091 -4.2771707 -4.26909 -4.2590041 -4.2596583 -4.2644134 -4.2656965 -4.2563381 -4.2438083 -4.2382107 -4.2338147 -4.2358747 -4.25024][-4.2695026 -4.2668457 -4.2677979 -4.2628202 -4.2473974 -4.2310185 -4.2272387 -4.2325287 -4.2386093 -4.2345634 -4.2231603 -4.2175422 -4.2148161 -4.2184362 -4.2338538][-4.2575336 -4.2467422 -4.2359047 -4.220912 -4.19607 -4.1721821 -4.1609054 -4.167675 -4.1872582 -4.200964 -4.2017174 -4.2030864 -4.2068467 -4.2163453 -4.2368326][-4.2406921 -4.2210565 -4.195766 -4.1671271 -4.131556 -4.0957785 -4.0648074 -4.0609522 -4.0991988 -4.1429205 -4.1683016 -4.1851645 -4.1990042 -4.2168341 -4.243834][-4.2266192 -4.2007289 -4.1631966 -4.1221061 -4.0707893 -4.0131912 -3.9507296 -3.917603 -3.963347 -4.0403113 -4.0996423 -4.1436243 -4.1769142 -4.2081785 -4.2458167][-4.2225013 -4.1946807 -4.1532259 -4.1047597 -4.0410118 -3.9650624 -3.875627 -3.809571 -3.8415103 -3.935303 -4.021956 -4.0909405 -4.1461363 -4.19019 -4.2391291][-4.240674 -4.22498 -4.2026329 -4.1707296 -4.1196074 -4.0544868 -3.9750724 -3.9063058 -3.9076319 -3.9696813 -4.0375285 -4.0949526 -4.1449237 -4.1834149 -4.2290349][-4.2584071 -4.2575393 -4.2574472 -4.2512341 -4.2270188 -4.1868567 -4.1341648 -4.0784645 -4.0530043 -4.0654035 -4.0920334 -4.1191287 -4.1479383 -4.1726837 -4.2088194][-4.2589049 -4.2664404 -4.27908 -4.2860622 -4.2781658 -4.2590742 -4.2307239 -4.1936784 -4.1609397 -4.1420145 -4.13568 -4.1374321 -4.1435876 -4.155405 -4.185709][-4.2559233 -4.2630615 -4.2781148 -4.2897315 -4.2886543 -4.283936 -4.2760024 -4.2577343 -4.2326379 -4.2036295 -4.1794891 -4.1645732 -4.1591935 -4.1617222 -4.1872411][-4.2595634 -4.2587671 -4.2648392 -4.2709394 -4.2720437 -4.2787127 -4.286859 -4.285429 -4.2719226 -4.2487946 -4.2230582 -4.2040939 -4.1945291 -4.1945028 -4.2150927][-4.2671905 -4.2607179 -4.2587972 -4.259953 -4.2601995 -4.2698212 -4.2824821 -4.2888317 -4.2859707 -4.273623 -4.2563014 -4.2440033 -4.2363653 -4.2358332 -4.2499194][-4.27552 -4.2669263 -4.2626824 -4.2635202 -4.2630177 -4.2674928 -4.2748051 -4.278945 -4.2791066 -4.2755947 -4.2694712 -4.2636356 -4.2579303 -4.2579355 -4.2698107][-4.2824225 -4.2715225 -4.2651057 -4.2636504 -4.26169 -4.2622614 -4.2640018 -4.2639952 -4.2640867 -4.2660193 -4.2654657 -4.2630844 -4.2640777 -4.2713037 -4.2861238]]...]
INFO - root - 2017-12-05 11:44:01.784099: step 5410, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 77h:59m:04s remains)
INFO - root - 2017-12-05 11:44:10.255626: step 5420, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 76h:07m:17s remains)
INFO - root - 2017-12-05 11:44:18.650032: step 5430, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 75h:39m:57s remains)
INFO - root - 2017-12-05 11:44:27.165241: step 5440, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 79h:18m:32s remains)
INFO - root - 2017-12-05 11:44:35.736541: step 5450, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 75h:42m:17s remains)
INFO - root - 2017-12-05 11:44:44.204002: step 5460, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 74h:32m:07s remains)
INFO - root - 2017-12-05 11:44:52.610544: step 5470, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.777 sec/batch; 70h:37m:35s remains)
INFO - root - 2017-12-05 11:45:01.112858: step 5480, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.835 sec/batch; 75h:48m:31s remains)
INFO - root - 2017-12-05 11:45:09.594113: step 5490, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 79h:54m:24s remains)
INFO - root - 2017-12-05 11:45:18.050393: step 5500, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 76h:41m:54s remains)
2017-12-05 11:45:18.812368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1638737 -4.1706285 -4.16819 -4.1666355 -4.1659327 -4.1721315 -4.1891541 -4.2042665 -4.2220311 -4.2320619 -4.2107139 -4.1930809 -4.1864429 -4.1691055 -4.1760912][-4.1444416 -4.1564941 -4.1560559 -4.1569071 -4.1593156 -4.1664872 -4.1798568 -4.1930156 -4.2076921 -4.2188764 -4.1964946 -4.1750088 -4.1677914 -4.1570692 -4.1723561][-4.1268606 -4.1423445 -4.1422639 -4.1468267 -4.1528296 -4.1629243 -4.1830387 -4.1993065 -4.2111592 -4.2188458 -4.1919127 -4.1618252 -4.154798 -4.1534638 -4.1759853][-4.1177692 -4.1357069 -4.1364403 -4.1439991 -4.1516771 -4.1557817 -4.1715589 -4.1863728 -4.197084 -4.2055459 -4.1820273 -4.1478434 -4.1395082 -4.1445527 -4.172637][-4.1113167 -4.1303596 -4.1325474 -4.1377869 -4.1439452 -4.1373177 -4.1348171 -4.1441083 -4.1620116 -4.179575 -4.1657038 -4.1337771 -4.1219134 -4.1225615 -4.1498804][-4.0964413 -4.10816 -4.1019883 -4.1027393 -4.1039243 -4.0782061 -4.0499134 -4.0652604 -4.1082983 -4.142643 -4.1401854 -4.114573 -4.1038852 -4.100316 -4.12222][-4.0566449 -4.04889 -4.0216541 -4.0154128 -4.0067191 -3.9464874 -3.864419 -3.8845155 -3.9862113 -4.0651612 -4.0819774 -4.0675426 -4.0767679 -4.0870228 -4.111486][-4.0214758 -3.997685 -3.9576244 -3.9439 -3.93034 -3.8487241 -3.7132878 -3.7144687 -3.8610852 -3.9791074 -4.0166535 -4.0143008 -4.0486331 -4.0889235 -4.1212454][-4.0252171 -4.0140162 -3.9888582 -3.990448 -3.995506 -3.9446135 -3.8372376 -3.8187482 -3.9223125 -4.0151234 -4.0448418 -4.0438471 -4.0791721 -4.1210475 -4.150054][-4.0565338 -4.063879 -4.0594034 -4.0781622 -4.1048532 -4.0926781 -4.0329289 -4.0177522 -4.0696058 -4.1164832 -4.1199408 -4.1075988 -4.1324568 -4.1619864 -4.1811185][-4.0890093 -4.107182 -4.1165137 -4.1365652 -4.1643405 -4.1679387 -4.1411943 -4.1273055 -4.1413112 -4.154387 -4.1394448 -4.115551 -4.136785 -4.1694603 -4.1880665][-4.1175351 -4.1390209 -4.1589847 -4.1758833 -4.1951404 -4.2047691 -4.195303 -4.18177 -4.1724348 -4.1628981 -4.126946 -4.0828052 -4.0911584 -4.1297226 -4.1579227][-4.1214623 -4.150393 -4.1808057 -4.1980481 -4.2112441 -4.2199345 -4.2160459 -4.2044587 -4.1844215 -4.1603122 -4.1101418 -4.056416 -4.0609517 -4.1044393 -4.1402969][-4.1259508 -4.1558638 -4.1819162 -4.1958518 -4.2091265 -4.2199993 -4.2220759 -4.2128592 -4.1942391 -4.1703987 -4.1303639 -4.09394 -4.1043625 -4.1458383 -4.1772442][-4.1689186 -4.1912656 -4.205792 -4.2084923 -4.21512 -4.2231827 -4.2254786 -4.2172823 -4.2050834 -4.192028 -4.1709242 -4.1537566 -4.1673231 -4.1982794 -4.2231402]]...]
INFO - root - 2017-12-05 11:45:27.399103: step 5510, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 77h:14m:55s remains)
INFO - root - 2017-12-05 11:45:35.954950: step 5520, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.881 sec/batch; 80h:02m:06s remains)
INFO - root - 2017-12-05 11:45:44.551947: step 5530, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 76h:12m:16s remains)
INFO - root - 2017-12-05 11:45:53.075735: step 5540, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 77h:47m:17s remains)
INFO - root - 2017-12-05 11:46:01.621126: step 5550, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 79h:23m:22s remains)
INFO - root - 2017-12-05 11:46:10.182288: step 5560, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 77h:48m:13s remains)
INFO - root - 2017-12-05 11:46:18.733177: step 5570, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 77h:04m:53s remains)
INFO - root - 2017-12-05 11:46:27.166263: step 5580, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 78h:44m:26s remains)
INFO - root - 2017-12-05 11:46:35.649504: step 5590, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 78h:47m:16s remains)
INFO - root - 2017-12-05 11:46:44.097991: step 5600, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 78h:33m:47s remains)
2017-12-05 11:46:44.830245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1746178 -4.1400661 -4.1225004 -4.1196332 -4.1298537 -4.1465478 -4.1680431 -4.1769905 -4.1642609 -4.13962 -4.1293511 -4.1304541 -4.1298776 -4.1219754 -4.127315][-4.1402516 -4.1039162 -4.087182 -4.0888057 -4.1069469 -4.1294022 -4.1548514 -4.16862 -4.1635208 -4.1432371 -4.1308203 -4.1227732 -4.1174135 -4.1140709 -4.1248789][-4.1273775 -4.0924654 -4.07806 -4.0826855 -4.10355 -4.1242328 -4.1465054 -4.1606722 -4.1620426 -4.1518884 -4.1362381 -4.1175232 -4.1073318 -4.1120348 -4.1273785][-4.1220655 -4.0840483 -4.0661855 -4.0713019 -4.0898652 -4.1061649 -4.1242576 -4.1400747 -4.1496787 -4.1466966 -4.1258507 -4.099369 -4.0873117 -4.0975986 -4.1162796][-4.1071043 -4.0608063 -4.0392818 -4.0475669 -4.0648489 -4.0755215 -4.0847669 -4.0939274 -4.0985069 -4.0961633 -4.0776649 -4.0489268 -4.0314732 -4.0419803 -4.0633631][-4.0963235 -4.0439591 -4.0190778 -4.0271921 -4.040956 -4.0390749 -4.0282784 -4.0172763 -4.0109053 -4.0084462 -3.9995635 -3.9818306 -3.9687204 -3.9782653 -4.0032978][-4.0970273 -4.0543876 -4.0342717 -4.0340762 -4.0282416 -4.0021467 -3.9651227 -3.9342115 -3.9250758 -3.936655 -3.9543257 -3.9630888 -3.9601486 -3.9655039 -3.9871101][-4.1230397 -4.091608 -4.069839 -4.04809 -4.01685 -3.9750948 -3.9280853 -3.8959994 -3.8964379 -3.932107 -3.974205 -3.997695 -4.0015326 -4.0032654 -4.021029][-4.1570687 -4.1322708 -4.1107135 -4.0791025 -4.0411296 -4.006259 -3.9736929 -3.9555757 -3.9616349 -3.9985552 -4.0409532 -4.064374 -4.0716052 -4.0747957 -4.0928211][-4.1893563 -4.1715245 -4.1566157 -4.1338 -4.1094193 -4.09048 -4.071485 -4.0598984 -4.0605164 -4.0807185 -4.1117287 -4.1331487 -4.1441894 -4.1497121 -4.164187][-4.2173543 -4.2061572 -4.2002277 -4.191474 -4.18079 -4.1722274 -4.1588516 -4.1452794 -4.140501 -4.1505508 -4.1689796 -4.1861773 -4.1995025 -4.2072906 -4.2186232][-4.2437634 -4.2381811 -4.2394929 -4.2396207 -4.23717 -4.2359657 -4.2282305 -4.2173567 -4.2088633 -4.2097411 -4.2181568 -4.2278705 -4.2361512 -4.2418051 -4.2495322][-4.2695866 -4.2679925 -4.2714162 -4.2741303 -4.2751441 -4.2766504 -4.2740741 -4.2678924 -4.2608757 -4.2578144 -4.2582469 -4.2592306 -4.2597041 -4.2611423 -4.2660575][-4.2948427 -4.2936697 -4.2948627 -4.2960157 -4.2970848 -4.298934 -4.3003569 -4.2999258 -4.2981205 -4.2956829 -4.292788 -4.2882838 -4.2846384 -4.2844934 -4.2879124][-4.3175368 -4.3163905 -4.3149581 -4.314425 -4.3151622 -4.3173547 -4.3198481 -4.3212891 -4.3214025 -4.3200836 -4.3174119 -4.3140912 -4.312 -4.3121686 -4.3140535]]...]
INFO - root - 2017-12-05 11:46:53.408269: step 5610, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 76h:12m:42s remains)
INFO - root - 2017-12-05 11:47:01.941012: step 5620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 77h:40m:34s remains)
INFO - root - 2017-12-05 11:47:10.577225: step 5630, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 80h:06m:06s remains)
INFO - root - 2017-12-05 11:47:19.132180: step 5640, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 77h:29m:32s remains)
INFO - root - 2017-12-05 11:47:27.673567: step 5650, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 76h:49m:09s remains)
INFO - root - 2017-12-05 11:47:36.079178: step 5660, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 76h:41m:53s remains)
INFO - root - 2017-12-05 11:47:44.680072: step 5670, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 77h:25m:01s remains)
INFO - root - 2017-12-05 11:47:53.265749: step 5680, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 79h:21m:21s remains)
INFO - root - 2017-12-05 11:48:01.890896: step 5690, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 80h:00m:39s remains)
INFO - root - 2017-12-05 11:48:10.297884: step 5700, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.872 sec/batch; 79h:11m:55s remains)
2017-12-05 11:48:11.039495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2529464 -4.2339077 -4.202642 -4.1647449 -4.1288919 -4.1153126 -4.0914059 -4.0712867 -4.0659823 -4.0591784 -4.0519085 -4.052753 -4.0459495 -4.029388 -4.0341358][-4.2509847 -4.2268476 -4.1851554 -4.13461 -4.0901556 -4.0736184 -4.0526376 -4.0324044 -4.0240741 -4.0190244 -4.016583 -4.0177917 -4.0098176 -3.9973407 -4.0023832][-4.2492723 -4.2261491 -4.1839228 -4.1329689 -4.0920296 -4.075654 -4.060895 -4.0439811 -4.0363765 -4.0358195 -4.0338559 -4.0283217 -4.0168691 -4.0065575 -4.0065122][-4.2489672 -4.2336984 -4.2036557 -4.1666956 -4.1387935 -4.1273 -4.1227493 -4.115159 -4.1126823 -4.1156583 -4.109971 -4.093874 -4.0759234 -4.061933 -4.0558329][-4.2493854 -4.2424335 -4.2267976 -4.2072139 -4.1918106 -4.1878524 -4.1949577 -4.2004066 -4.2055316 -4.2088075 -4.1983414 -4.17667 -4.1559582 -4.1383886 -4.1301422][-4.2481093 -4.2457948 -4.2386241 -4.2282786 -4.2186632 -4.2197828 -4.2370019 -4.2542152 -4.2653804 -4.2690878 -4.2589388 -4.2409205 -4.2241292 -4.2081647 -4.2010937][-4.2459431 -4.243669 -4.2377276 -4.2278647 -4.2165504 -4.2191415 -4.2407689 -4.2622051 -4.275136 -4.279604 -4.2765245 -4.2703819 -4.2639427 -4.2555161 -4.2526894][-4.2440906 -4.2393203 -4.2293634 -4.2144704 -4.1990027 -4.2009091 -4.2198968 -4.2380705 -4.2482505 -4.2534308 -4.2592354 -4.2656288 -4.2710214 -4.2723441 -4.2752481][-4.239902 -4.2326927 -4.2181315 -4.1988263 -4.1795812 -4.17792 -4.1909246 -4.20236 -4.2086864 -4.2158022 -4.2286081 -4.2439079 -4.2585449 -4.267385 -4.2758732][-4.2374411 -4.229609 -4.2127037 -4.1916294 -4.1689682 -4.161521 -4.1682057 -4.1746149 -4.1819153 -4.1929545 -4.2090116 -4.2286687 -4.2478218 -4.2599783 -4.2706647][-4.23911 -4.2335386 -4.2179751 -4.1978865 -4.1733809 -4.161972 -4.1660156 -4.1722274 -4.182025 -4.1943331 -4.2101145 -4.2312841 -4.2517571 -4.2630854 -4.2716675][-4.2482605 -4.2458344 -4.2322059 -4.2135663 -4.1889143 -4.1764412 -4.1804075 -4.1881218 -4.1996551 -4.2114072 -4.2257285 -4.245163 -4.2631497 -4.2712359 -4.2757306][-4.2627082 -4.2626615 -4.2504025 -4.2336082 -4.2098713 -4.1975994 -4.2025628 -4.2130108 -4.2268796 -4.23821 -4.2485156 -4.2629704 -4.2763042 -4.2807088 -4.281157][-4.2755241 -4.2780352 -4.2676439 -4.2530026 -4.2315793 -4.2212796 -4.2266092 -4.2389913 -4.2544651 -4.2646928 -4.271246 -4.2810116 -4.2905855 -4.2919455 -4.2896457][-4.282515 -4.2869053 -4.278739 -4.2668872 -4.2486448 -4.2396345 -4.2441959 -4.257062 -4.2721128 -4.2811122 -4.2867079 -4.2947454 -4.3017259 -4.30087 -4.2970986]]...]
INFO - root - 2017-12-05 11:48:19.724055: step 5710, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 78h:11m:46s remains)
INFO - root - 2017-12-05 11:48:28.267897: step 5720, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 75h:08m:01s remains)
INFO - root - 2017-12-05 11:48:36.714116: step 5730, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 76h:32m:07s remains)
INFO - root - 2017-12-05 11:48:45.234270: step 5740, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 77h:02m:47s remains)
INFO - root - 2017-12-05 11:48:53.699328: step 5750, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 76h:28m:40s remains)
INFO - root - 2017-12-05 11:49:02.342653: step 5760, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.903 sec/batch; 81h:56m:46s remains)
INFO - root - 2017-12-05 11:49:10.819473: step 5770, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 77h:29m:38s remains)
INFO - root - 2017-12-05 11:49:19.363654: step 5780, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 76h:02m:51s remains)
INFO - root - 2017-12-05 11:49:27.745808: step 5790, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 78h:22m:53s remains)
INFO - root - 2017-12-05 11:49:36.266462: step 5800, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 76h:23m:23s remains)
2017-12-05 11:49:37.068501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1709738 -4.1550951 -4.150959 -4.1637816 -4.186235 -4.2136374 -4.2350464 -4.2402182 -4.2235765 -4.1903882 -4.1637177 -4.1487703 -4.1312518 -4.116488 -4.1083741][-4.1474705 -4.1440535 -4.1432772 -4.1557221 -4.1782765 -4.2072129 -4.2265525 -4.22991 -4.2177377 -4.1931911 -4.1703124 -4.1562362 -4.1436105 -4.1395698 -4.1417542][-4.1333323 -4.1480155 -4.1561027 -4.1677685 -4.1818104 -4.2033296 -4.2167416 -4.2135077 -4.2046657 -4.1943517 -4.1833963 -4.1764579 -4.1714678 -4.17232 -4.1778479][-4.1215162 -4.147428 -4.1603746 -4.1667461 -4.1647353 -4.1703615 -4.1822872 -4.1843944 -4.1878519 -4.1941404 -4.1967711 -4.1953125 -4.1919336 -4.1899447 -4.189744][-4.0895371 -4.1148906 -4.1275883 -4.1218076 -4.1021032 -4.1027994 -4.1212149 -4.1383719 -4.1543894 -4.1757965 -4.1879706 -4.1861868 -4.1773763 -4.161962 -4.1517925][-4.038281 -4.0522866 -4.0584955 -4.0344124 -3.9953938 -3.9888005 -4.0199933 -4.0586467 -4.0880709 -4.1228452 -4.1525464 -4.1592522 -4.148335 -4.1222572 -4.1029458][-4.0052414 -3.9872446 -3.9658422 -3.9132018 -3.8357935 -3.8019986 -3.8466754 -3.9274817 -3.9870799 -4.0396843 -4.0956807 -4.1300621 -4.1337414 -4.1133323 -4.0952005][-4.011353 -3.9662569 -3.9109318 -3.8229482 -3.7014742 -3.6134472 -3.647506 -3.7684507 -3.86663 -3.9398782 -4.025424 -4.0946636 -4.1230955 -4.1206975 -4.1147428][-4.0601068 -4.0070553 -3.9424243 -3.8594365 -3.7468147 -3.647603 -3.6484048 -3.7524166 -3.8433409 -3.9076896 -3.9918382 -4.0686517 -4.1088147 -4.12341 -4.1336312][-4.1434507 -4.0972309 -4.039299 -3.9761817 -3.9016709 -3.8346105 -3.8212459 -3.8749971 -3.930809 -3.9701421 -4.02422 -4.0774341 -4.1142521 -4.1357126 -4.1555753][-4.2171655 -4.1833291 -4.1379356 -4.0931358 -4.0478215 -4.0098128 -3.9979417 -4.0188127 -4.0513153 -4.0773449 -4.1050973 -4.137279 -4.1659927 -4.1835814 -4.1965566][-4.2657547 -4.244226 -4.2169862 -4.1909041 -4.1665483 -4.1492295 -4.1437092 -4.1505547 -4.1685286 -4.1859412 -4.2012687 -4.2196774 -4.2378235 -4.2462096 -4.2471485][-4.2875519 -4.2743778 -4.2621527 -4.2531805 -4.2458816 -4.2449503 -4.2457933 -4.2488494 -4.2583237 -4.2703385 -4.2788897 -4.2875295 -4.2937469 -4.2924314 -4.28491][-4.2969122 -4.2894406 -4.2849455 -4.2835307 -4.2829285 -4.2871728 -4.2922525 -4.2977157 -4.3048353 -4.3139262 -4.320312 -4.3242579 -4.3245978 -4.31875 -4.3101492][-4.2996945 -4.294302 -4.2929082 -4.2936325 -4.2947569 -4.2989364 -4.3053207 -4.3113 -4.3172531 -4.3248024 -4.3304067 -4.3323 -4.3296824 -4.3237371 -4.3180246]]...]
INFO - root - 2017-12-05 11:49:45.644292: step 5810, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 77h:53m:45s remains)
INFO - root - 2017-12-05 11:49:54.261988: step 5820, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 79h:16m:25s remains)
INFO - root - 2017-12-05 11:50:02.772838: step 5830, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 78h:00m:09s remains)
INFO - root - 2017-12-05 11:50:11.205833: step 5840, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 76h:42m:52s remains)
INFO - root - 2017-12-05 11:50:19.707211: step 5850, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 75h:47m:16s remains)
INFO - root - 2017-12-05 11:50:28.335676: step 5860, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 75h:46m:56s remains)
INFO - root - 2017-12-05 11:50:36.737532: step 5870, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 77h:46m:03s remains)
INFO - root - 2017-12-05 11:50:45.138621: step 5880, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 74h:56m:44s remains)
INFO - root - 2017-12-05 11:50:53.661452: step 5890, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 75h:55m:16s remains)
INFO - root - 2017-12-05 11:51:01.897849: step 5900, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 76h:40m:32s remains)
2017-12-05 11:51:02.721444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.148416 -4.1497622 -4.1537085 -4.165957 -4.1804581 -4.1906409 -4.1918535 -4.1982989 -4.2057323 -4.2098351 -4.2128444 -4.2156668 -4.2201343 -4.2205091 -4.217906][-4.1326342 -4.1339469 -4.1381 -4.1507034 -4.1634431 -4.1715436 -4.1713786 -4.1828728 -4.198658 -4.2098813 -4.2152042 -4.2180157 -4.2231226 -4.2219748 -4.2135754][-4.134726 -4.1377258 -4.1448784 -4.1588621 -4.1690221 -4.1716185 -4.1676555 -4.181963 -4.2042737 -4.2220907 -4.2316575 -4.23664 -4.2434673 -4.2406616 -4.2255578][-4.1364312 -4.1421924 -4.1556931 -4.1741309 -4.1829982 -4.1788993 -4.1681809 -4.1802616 -4.2039666 -4.2255149 -4.2397461 -4.2496409 -4.2599344 -4.2568874 -4.2365065][-4.1370778 -4.1455083 -4.1666231 -4.1891403 -4.1989765 -4.1904693 -4.1751595 -4.1837506 -4.2062187 -4.2298756 -4.2473354 -4.2610884 -4.2742462 -4.2704697 -4.2441959][-4.1288013 -4.1392426 -4.1674414 -4.1924353 -4.2017846 -4.1895485 -4.1739516 -4.1819 -4.2043352 -4.2297287 -4.2485471 -4.263525 -4.2769275 -4.2702661 -4.23917][-4.1186929 -4.1335797 -4.1684756 -4.1955585 -4.2032804 -4.187923 -4.1726575 -4.179379 -4.1993341 -4.2236433 -4.242805 -4.2592063 -4.2730389 -4.2654576 -4.2343693][-4.113019 -4.1330667 -4.1706109 -4.1969109 -4.2018209 -4.1846213 -4.1700807 -4.1756797 -4.1928482 -4.215435 -4.2364631 -4.2583194 -4.2755728 -4.2710838 -4.2436185][-4.1069007 -4.1302238 -4.1683569 -4.193265 -4.1955504 -4.1772251 -4.1619129 -4.165978 -4.1816154 -4.2048378 -4.22905 -4.2562447 -4.2761269 -4.2743521 -4.249671][-4.1135912 -4.1373062 -4.173986 -4.19896 -4.2000442 -4.1802335 -4.1651483 -4.1715612 -4.1920805 -4.2177505 -4.2402086 -4.2645073 -4.2814045 -4.278564 -4.2537823][-4.1376262 -4.158659 -4.1908269 -4.2131934 -4.2112765 -4.1893382 -4.1765561 -4.1878586 -4.2137032 -4.2402005 -4.2591424 -4.2780004 -4.2895141 -4.2857008 -4.2631412][-4.1642289 -4.1812472 -4.2075872 -4.2265196 -4.2226276 -4.2008543 -4.1913328 -4.2070312 -4.2351656 -4.2611465 -4.2754011 -4.2882233 -4.295382 -4.2924089 -4.2743254][-4.1851611 -4.196414 -4.2168474 -4.2332044 -4.2296143 -4.2119131 -4.20656 -4.2221003 -4.2468343 -4.26953 -4.2807484 -4.289649 -4.2940245 -4.2927389 -4.2799425][-4.2118506 -4.216465 -4.2297769 -4.2432203 -4.2411194 -4.2278066 -4.2252736 -4.2381415 -4.257925 -4.2765374 -4.2859812 -4.2924795 -4.2946424 -4.2936707 -4.2854376][-4.2462678 -4.2456217 -4.2526455 -4.2625861 -4.2618432 -4.2530603 -4.2516179 -4.2604036 -4.2745938 -4.2890768 -4.29647 -4.2997785 -4.2996979 -4.2986131 -4.2946124]]...]
INFO - root - 2017-12-05 11:51:11.223801: step 5910, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 77h:33m:34s remains)
INFO - root - 2017-12-05 11:51:19.774630: step 5920, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.830 sec/batch; 75h:17m:12s remains)
INFO - root - 2017-12-05 11:51:28.278580: step 5930, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.845 sec/batch; 76h:41m:08s remains)
INFO - root - 2017-12-05 11:51:36.768583: step 5940, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 77h:56m:13s remains)
INFO - root - 2017-12-05 11:51:45.250233: step 5950, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 77h:20m:10s remains)
INFO - root - 2017-12-05 11:51:53.789195: step 5960, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 78h:11m:02s remains)
INFO - root - 2017-12-05 11:52:02.354198: step 5970, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 77h:25m:23s remains)
INFO - root - 2017-12-05 11:52:10.861482: step 5980, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 79h:09m:01s remains)
INFO - root - 2017-12-05 11:52:19.374768: step 5990, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 76h:55m:26s remains)
INFO - root - 2017-12-05 11:52:27.795317: step 6000, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 76h:41m:29s remains)
2017-12-05 11:52:28.519406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1072631 -4.1242733 -4.1340895 -4.1360726 -4.1382375 -4.1429358 -4.1420803 -4.1400356 -4.14287 -4.1476784 -4.1535873 -4.1635461 -4.1728239 -4.1792727 -4.1919389][-4.1002755 -4.113461 -4.1191239 -4.1180348 -4.1161079 -4.118329 -4.1175289 -4.1100407 -4.1093116 -4.1091347 -4.1100521 -4.1174479 -4.1280131 -4.1397452 -4.1589661][-4.0846138 -4.0904651 -4.0916505 -4.0918436 -4.0966954 -4.0991182 -4.1009731 -4.0952177 -4.100019 -4.1048188 -4.1084294 -4.1170712 -4.1286173 -4.1434703 -4.1617012][-4.0619607 -4.0623646 -4.0625973 -4.0676332 -4.0817814 -4.0882487 -4.0909414 -4.08672 -4.0952787 -4.110899 -4.1244025 -4.1390934 -4.1541648 -4.1727505 -4.1871524][-4.0443964 -4.0416226 -4.0465569 -4.0502381 -4.0604739 -4.0605874 -4.0551748 -4.0409131 -4.0460505 -4.0745864 -4.1034822 -4.1303573 -4.1591525 -4.1899009 -4.2053256][-4.0460124 -4.0398784 -4.0418668 -4.0350266 -4.0277843 -4.0119023 -3.9788034 -3.9353759 -3.9281321 -3.9768836 -4.0311909 -4.0820851 -4.1287546 -4.1746111 -4.201973][-4.049859 -4.0405173 -4.0378804 -4.0203862 -3.9906831 -3.9454958 -3.8619769 -3.7530892 -3.7290916 -3.8245332 -3.9318807 -4.021657 -4.0908575 -4.1504154 -4.1899881][-4.0172324 -4.0147576 -4.0176783 -4.0004854 -3.9572215 -3.8845572 -3.741653 -3.553205 -3.5158532 -3.6797071 -3.8477378 -3.9737692 -4.0638103 -4.1352262 -4.1825485][-3.9855263 -3.993937 -4.0100889 -4.0024896 -3.9625075 -3.8865857 -3.7422943 -3.5547109 -3.520736 -3.6833305 -3.850589 -3.9713597 -4.06039 -4.1366482 -4.1833377][-3.9888935 -4.0085478 -4.0384507 -4.0414977 -4.017324 -3.9683912 -3.8808351 -3.770031 -3.7350967 -3.8213582 -3.9257381 -4.0017443 -4.0684128 -4.1364779 -4.1761484][-4.0136514 -4.0363936 -4.0682836 -4.0777369 -4.0689836 -4.0471935 -4.0125418 -3.9586878 -3.9194314 -3.9385509 -3.9890575 -4.0296345 -4.0771542 -4.1328807 -4.1633258][-4.0095639 -4.0268064 -4.0536184 -4.0747089 -4.0864987 -4.0858612 -4.0764875 -4.0446305 -4.0075531 -3.9973879 -4.0228391 -4.054318 -4.0899997 -4.1341214 -4.1566262][-3.9756811 -3.985244 -4.0032892 -4.0372682 -4.07127 -4.0895967 -4.0955291 -4.0753765 -4.0430512 -4.0276041 -4.0443377 -4.0742931 -4.1002398 -4.1309967 -4.1512952][-3.9637911 -3.9670203 -3.9781 -4.0191994 -4.0623055 -4.0852957 -4.1037188 -4.0952668 -4.0662665 -4.0489974 -4.0563116 -4.0734529 -4.0843725 -4.0966682 -4.113739][-4.0051808 -4.0045652 -4.0061364 -4.0345297 -4.0553846 -4.0645986 -4.0855355 -4.080946 -4.05343 -4.0328503 -4.0411649 -4.0510073 -4.0509114 -4.0477066 -4.0599866]]...]
INFO - root - 2017-12-05 11:52:36.937082: step 6010, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 77h:37m:49s remains)
INFO - root - 2017-12-05 11:52:45.461452: step 6020, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 77h:50m:25s remains)
INFO - root - 2017-12-05 11:52:53.970455: step 6030, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 77h:37m:32s remains)
INFO - root - 2017-12-05 11:53:02.381174: step 6040, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 74h:40m:51s remains)
INFO - root - 2017-12-05 11:53:10.858150: step 6050, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 78h:53m:57s remains)
INFO - root - 2017-12-05 11:53:19.360377: step 6060, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 0.804 sec/batch; 72h:52m:57s remains)
INFO - root - 2017-12-05 11:53:28.067562: step 6070, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 78h:26m:52s remains)
INFO - root - 2017-12-05 11:53:36.666388: step 6080, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.831 sec/batch; 75h:18m:22s remains)
INFO - root - 2017-12-05 11:53:45.211668: step 6090, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 74h:53m:24s remains)
INFO - root - 2017-12-05 11:53:53.714741: step 6100, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 88h:42m:49s remains)
2017-12-05 11:53:54.506864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3166108 -4.313056 -4.3126564 -4.3106875 -4.3070431 -4.3018537 -4.2958813 -4.2906957 -4.2889328 -4.2939906 -4.3017874 -4.3044147 -4.3034797 -4.3000031 -4.2979631][-4.3072591 -4.2994962 -4.296257 -4.2892947 -4.2819409 -4.2731948 -4.2631693 -4.2582803 -4.2566957 -4.2629175 -4.2734556 -4.2788687 -4.2780137 -4.2731161 -4.2691364][-4.2994189 -4.288558 -4.2821336 -4.2717819 -4.2611766 -4.2495275 -4.2338691 -4.2261348 -4.2266679 -4.2339931 -4.2455149 -4.2533393 -4.2576065 -4.2536535 -4.2456374][-4.2944317 -4.2818761 -4.274354 -4.2613816 -4.2482243 -4.2334132 -4.2129207 -4.2025108 -4.2056279 -4.2161732 -4.226408 -4.2344494 -4.2462454 -4.2443695 -4.2331147][-4.2894597 -4.2750978 -4.2664728 -4.253232 -4.2395949 -4.222538 -4.1993136 -4.1891804 -4.1964111 -4.2120924 -4.2202368 -4.2228284 -4.2351317 -4.2306414 -4.2163014][-4.2849293 -4.2679138 -4.2570143 -4.2431483 -4.2308908 -4.2142015 -4.1901884 -4.18285 -4.1926937 -4.2131248 -4.2224765 -4.2213783 -4.2265019 -4.2121835 -4.1923633][-4.2869053 -4.2668071 -4.2525587 -4.238678 -4.2289553 -4.2127466 -4.188652 -4.1803937 -4.1875334 -4.2100205 -4.2282996 -4.230823 -4.2267523 -4.2014208 -4.1758084][-4.2917695 -4.271131 -4.2561069 -4.243113 -4.2334819 -4.2153616 -4.1904778 -4.1804953 -4.1858063 -4.2105613 -4.2390666 -4.2494035 -4.2414508 -4.2127452 -4.1904941][-4.2977934 -4.2818208 -4.2715755 -4.2615528 -4.2473054 -4.2217178 -4.1911988 -4.1777554 -4.1803093 -4.2039967 -4.23872 -4.2579 -4.252769 -4.2322512 -4.2204876][-4.3034697 -4.2915974 -4.2872934 -4.2803555 -4.26364 -4.2344322 -4.1982884 -4.1794672 -4.1799803 -4.203341 -4.2384915 -4.2606359 -4.2573872 -4.2472014 -4.2453222][-4.3082905 -4.2989292 -4.29755 -4.29326 -4.278842 -4.2500596 -4.2123413 -4.1907544 -4.1913233 -4.2147021 -4.2443705 -4.2621961 -4.2612677 -4.260149 -4.264226][-4.3132782 -4.3053451 -4.30305 -4.2986903 -4.2864904 -4.2606883 -4.22941 -4.2123461 -4.2133541 -4.2327027 -4.2554235 -4.2676463 -4.2677841 -4.2692628 -4.2737966][-4.3161736 -4.3099346 -4.3060989 -4.300458 -4.2906985 -4.2723508 -4.253974 -4.2474036 -4.2503724 -4.2645659 -4.2799282 -4.2858286 -4.2836051 -4.2832036 -4.2856965][-4.3170967 -4.3132915 -4.3111773 -4.3077445 -4.3010073 -4.2901392 -4.2819057 -4.2829037 -4.2872648 -4.2967987 -4.3047237 -4.3048549 -4.3003306 -4.2983809 -4.2997885][-4.318285 -4.3166137 -4.3175445 -4.3179951 -4.3141165 -4.3076472 -4.3042593 -4.3083358 -4.3122973 -4.3157134 -4.3169346 -4.3150315 -4.3119869 -4.3107352 -4.3112855]]...]
INFO - root - 2017-12-05 11:54:03.002883: step 6110, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 80h:55m:25s remains)
INFO - root - 2017-12-05 11:54:11.617755: step 6120, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.871 sec/batch; 78h:56m:08s remains)
INFO - root - 2017-12-05 11:54:20.189730: step 6130, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 78h:32m:42s remains)
INFO - root - 2017-12-05 11:54:28.781371: step 6140, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.838 sec/batch; 75h:56m:26s remains)
INFO - root - 2017-12-05 11:54:37.335599: step 6150, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 79h:15m:02s remains)
INFO - root - 2017-12-05 11:54:45.844122: step 6160, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 76h:47m:22s remains)
INFO - root - 2017-12-05 11:54:54.487025: step 6170, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 78h:57m:38s remains)
INFO - root - 2017-12-05 11:55:03.054731: step 6180, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 77h:26m:10s remains)
INFO - root - 2017-12-05 11:55:11.556994: step 6190, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 77h:39m:43s remains)
INFO - root - 2017-12-05 11:55:20.026431: step 6200, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 79h:30m:39s remains)
2017-12-05 11:55:20.774512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2034135 -4.1941252 -4.180809 -4.1785474 -4.1811562 -4.1729717 -4.1612792 -4.1226525 -4.0631495 -4.0339265 -4.0649867 -4.1232185 -4.170845 -4.2040091 -4.2412839][-4.2099442 -4.201746 -4.1939521 -4.1988859 -4.2021337 -4.190073 -4.1759586 -4.1359472 -4.0826907 -4.0537243 -4.0811782 -4.1455154 -4.2004724 -4.2425537 -4.279767][-4.2170415 -4.2165241 -4.2155018 -4.223166 -4.2253551 -4.207294 -4.1829176 -4.1352577 -4.0863638 -4.0619836 -4.0918651 -4.1636004 -4.2242069 -4.2715716 -4.3031812][-4.2313814 -4.2375789 -4.2415519 -4.2475066 -4.2425108 -4.21198 -4.16626 -4.1037688 -4.0551982 -4.0439143 -4.0878258 -4.1701612 -4.2385354 -4.28728 -4.3110986][-4.2474203 -4.2559662 -4.2599783 -4.26003 -4.2395 -4.1892929 -4.118299 -4.0408187 -3.9987302 -4.0144076 -4.0788021 -4.1687784 -4.2423334 -4.291255 -4.3051472][-4.2616968 -4.2652173 -4.2631226 -4.2558012 -4.2191253 -4.1467905 -4.0448675 -3.9548819 -3.9413798 -4.0024176 -4.0911202 -4.1838608 -4.2539449 -4.293756 -4.291337][-4.2579165 -4.2546721 -4.2488661 -4.2351251 -4.1854372 -4.0907631 -3.9655013 -3.8789136 -3.9092047 -4.00951 -4.1121097 -4.2019715 -4.2639384 -4.2867064 -4.2663169][-4.2330685 -4.22496 -4.2180176 -4.20041 -4.1406889 -4.0373054 -3.9210408 -3.8736937 -3.9418881 -4.0490236 -4.1414251 -4.2148514 -4.2578526 -4.2596436 -4.2228966][-4.191402 -4.1871958 -4.1828489 -4.1645036 -4.1027489 -4.0081339 -3.9286325 -3.9295831 -4.0089045 -4.1027584 -4.1757407 -4.2276726 -4.2464471 -4.2279463 -4.1808252][-4.1486945 -4.1566753 -4.1582503 -4.1388812 -4.0756817 -3.9953234 -3.9525542 -3.9841528 -4.06308 -4.1434736 -4.2036037 -4.236001 -4.2339764 -4.2010036 -4.1476059][-4.1275716 -4.1426129 -4.1412182 -4.1164255 -4.0578947 -3.998806 -3.9905899 -4.0408587 -4.1147923 -4.1817803 -4.2308369 -4.2472816 -4.232348 -4.1919208 -4.1350465][-4.1307378 -4.142056 -4.1314235 -4.1040969 -4.0603943 -4.026351 -4.0403185 -4.095963 -4.1644096 -4.2207284 -4.2596674 -4.2651892 -4.2413487 -4.19246 -4.1326928][-4.1454844 -4.15086 -4.1340075 -4.1062417 -4.0780363 -4.0647259 -4.0892539 -4.142487 -4.2020197 -4.245595 -4.27364 -4.2739692 -4.2444015 -4.1880436 -4.1251802][-4.1642737 -4.1664152 -4.1471238 -4.1214452 -4.1058254 -4.1045938 -4.132021 -4.1775985 -4.222127 -4.2489862 -4.2667303 -4.2645087 -4.2347512 -4.1791086 -4.118165][-4.1835546 -4.1900024 -4.1780858 -4.1572952 -4.1438088 -4.1408954 -4.16013 -4.1919456 -4.2186675 -4.2304134 -4.2409058 -4.2377167 -4.2110333 -4.159729 -4.1050167]]...]
INFO - root - 2017-12-05 11:55:29.164393: step 6210, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 75h:40m:22s remains)
INFO - root - 2017-12-05 11:55:37.589354: step 6220, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 75h:31m:02s remains)
INFO - root - 2017-12-05 11:55:46.257920: step 6230, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 78h:32m:14s remains)
INFO - root - 2017-12-05 11:55:54.756815: step 6240, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 78h:50m:57s remains)
INFO - root - 2017-12-05 11:56:03.263782: step 6250, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 76h:15m:00s remains)
INFO - root - 2017-12-05 11:56:11.876234: step 6260, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 76h:46m:58s remains)
INFO - root - 2017-12-05 11:56:20.434733: step 6270, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 74h:19m:46s remains)
INFO - root - 2017-12-05 11:56:29.039470: step 6280, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 78h:12m:49s remains)
INFO - root - 2017-12-05 11:56:37.607750: step 6290, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 79h:23m:52s remains)
INFO - root - 2017-12-05 11:56:46.158047: step 6300, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 78h:01m:57s remains)
2017-12-05 11:56:46.931157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2903519 -4.2543426 -4.2004061 -4.120698 -4.0453053 -4.0117869 -4.0482922 -4.1099596 -4.1450233 -4.1614804 -4.1641374 -4.1609678 -4.1698627 -4.1900306 -4.2094684][-4.2835932 -4.2463565 -4.1923423 -4.1092806 -4.0214243 -3.9684398 -4.0049958 -4.090704 -4.1477127 -4.1709409 -4.1725154 -4.1682768 -4.1797795 -4.2057953 -4.2309871][-4.2813678 -4.2447171 -4.192275 -4.1100154 -4.0152116 -3.9407973 -3.965229 -4.0649014 -4.1402721 -4.1699615 -4.1715307 -4.165709 -4.17922 -4.2089438 -4.2379155][-4.28112 -4.2458019 -4.1967483 -4.120029 -4.0262156 -3.9357061 -3.9401791 -4.0431323 -4.1331029 -4.1697216 -4.1730475 -4.16634 -4.1795478 -4.2073522 -4.2344584][-4.2813306 -4.2466383 -4.2023072 -4.1354103 -4.0479665 -3.948946 -3.9307199 -4.0242033 -4.1224542 -4.1681428 -4.1771297 -4.1697769 -4.1792107 -4.19945 -4.2181134][-4.2808208 -4.2465491 -4.2072988 -4.1527348 -4.0749121 -3.9764814 -3.9370165 -4.0080686 -4.1058617 -4.1610518 -4.17903 -4.1718 -4.174757 -4.1840081 -4.1898355][-4.2809315 -4.2480621 -4.2157722 -4.1756935 -4.1116862 -4.0242653 -3.9730642 -4.0144339 -4.0985222 -4.1593819 -4.1891632 -4.1860018 -4.1835918 -4.1804342 -4.1698947][-4.2838778 -4.2531056 -4.2275405 -4.2018514 -4.1540561 -4.0854487 -4.033216 -4.0510855 -4.1143441 -4.1741309 -4.2127576 -4.218051 -4.2145081 -4.2037406 -4.1799645][-4.2897792 -4.2608747 -4.2408967 -4.2238164 -4.1913404 -4.1418028 -4.0962477 -4.0999069 -4.14433 -4.1963625 -4.2398996 -4.2514892 -4.2467303 -4.2333293 -4.2065988][-4.2948785 -4.2689576 -4.2543054 -4.2427464 -4.2220421 -4.189343 -4.1517086 -4.147727 -4.1782093 -4.2191782 -4.2643194 -4.2830935 -4.2805929 -4.2690325 -4.2436428][-4.30158 -4.27882 -4.2676806 -4.2605963 -4.24732 -4.2284856 -4.19705 -4.1864338 -4.2074943 -4.2412796 -4.28764 -4.3133168 -4.3147373 -4.307374 -4.2859612][-4.312315 -4.2932172 -4.2828035 -4.2771873 -4.2678542 -4.2579923 -4.233675 -4.2196131 -4.2351646 -4.2648325 -4.3083348 -4.336041 -4.3420863 -4.3383884 -4.3229742][-4.3216562 -4.3044491 -4.2914238 -4.284246 -4.2747335 -4.2679033 -4.2504292 -4.2371969 -4.2505074 -4.2775316 -4.3166351 -4.3429317 -4.3509684 -4.3486848 -4.3388405][-4.3236628 -4.3082151 -4.2931938 -4.2830873 -4.2724848 -4.2676363 -4.2562785 -4.2472758 -4.2585168 -4.2799644 -4.3118248 -4.3335171 -4.3414612 -4.3400888 -4.3337884][-4.3229022 -4.3096347 -4.295279 -4.2858286 -4.2773647 -4.2736616 -4.2663445 -4.2602353 -4.2679539 -4.2834277 -4.3051319 -4.3205781 -4.3263249 -4.3251524 -4.3212929]]...]
INFO - root - 2017-12-05 11:56:55.591108: step 6310, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 79h:40m:45s remains)
INFO - root - 2017-12-05 11:57:04.114308: step 6320, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 78h:14m:00s remains)
INFO - root - 2017-12-05 11:57:12.525412: step 6330, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.829 sec/batch; 75h:07m:24s remains)
INFO - root - 2017-12-05 11:57:21.142457: step 6340, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 78h:12m:09s remains)
INFO - root - 2017-12-05 11:57:29.699633: step 6350, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 77h:17m:11s remains)
INFO - root - 2017-12-05 11:57:38.239534: step 6360, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 75h:48m:07s remains)
INFO - root - 2017-12-05 11:57:46.713030: step 6370, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 75h:55m:21s remains)
INFO - root - 2017-12-05 11:57:55.380823: step 6380, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 78h:18m:50s remains)
INFO - root - 2017-12-05 11:58:03.967968: step 6390, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 78h:48m:49s remains)
INFO - root - 2017-12-05 11:58:12.484806: step 6400, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 77h:05m:34s remains)
2017-12-05 11:58:13.245949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2413969 -4.2496886 -4.247292 -4.2433562 -4.2340112 -4.2208133 -4.2008724 -4.1810575 -4.1803365 -4.1925664 -4.2066293 -4.2173867 -4.2105041 -4.1852894 -4.1624184][-4.25147 -4.2607222 -4.2568445 -4.2520051 -4.2394481 -4.2225828 -4.2044144 -4.1874566 -4.1887512 -4.1954465 -4.2022381 -4.2130914 -4.2088866 -4.1844769 -4.1646714][-4.2516084 -4.2576828 -4.2506289 -4.2446084 -4.2290826 -4.2104731 -4.1970339 -4.1897311 -4.199008 -4.2033095 -4.2057171 -4.21385 -4.2103362 -4.1874623 -4.1688046][-4.2424412 -4.2488551 -4.24135 -4.2329006 -4.2113376 -4.1830788 -4.1683626 -4.172802 -4.1946421 -4.2050176 -4.2057476 -4.2082019 -4.2027054 -4.1836662 -4.1670318][-4.2293167 -4.2374153 -4.2328734 -4.2209759 -4.190721 -4.1493311 -4.1280622 -4.1436477 -4.1753845 -4.1938772 -4.1913996 -4.1869087 -4.17971 -4.1675596 -4.1580782][-4.2155676 -4.2212505 -4.2187853 -4.20407 -4.1673326 -4.1173124 -4.0934486 -4.1186705 -4.1576214 -4.1775727 -4.1696253 -4.1559157 -4.1454444 -4.1376972 -4.1397891][-4.2034059 -4.211832 -4.2140875 -4.196455 -4.1545186 -4.0973105 -4.0688038 -4.0964341 -4.1357951 -4.1508436 -4.1371469 -4.1214781 -4.116982 -4.1199508 -4.1373277][-4.1978602 -4.2053661 -4.2117467 -4.1958685 -4.1539664 -4.0927868 -4.0575562 -4.0808458 -4.1181436 -4.1308455 -4.1221189 -4.1198621 -4.1290836 -4.1447783 -4.1717229][-4.1926827 -4.1929178 -4.1953821 -4.1794267 -4.139142 -4.0801029 -4.0413752 -4.0561476 -4.0926361 -4.1121621 -4.1174145 -4.1331863 -4.1579256 -4.1860385 -4.2145448][-4.1977773 -4.1910949 -4.1831017 -4.1639814 -4.1297565 -4.0806427 -4.0466666 -4.0521827 -4.0806313 -4.1036873 -4.1193371 -4.1474757 -4.1841636 -4.2181921 -4.2451878][-4.2104754 -4.1968164 -4.1806636 -4.1616268 -4.1389828 -4.1067696 -4.0817924 -4.0790682 -4.0925245 -4.1126766 -4.1327229 -4.1629167 -4.1967621 -4.2230692 -4.2377324][-4.2270288 -4.2100272 -4.189806 -4.1735272 -4.1582041 -4.1367121 -4.1183567 -4.1096754 -4.1129718 -4.1304064 -4.1519728 -4.1783743 -4.2012491 -4.2118354 -4.2093897][-4.2386255 -4.2261119 -4.2083988 -4.1950521 -4.185101 -4.1717892 -4.1599588 -4.1518288 -4.1554089 -4.1708579 -4.1881185 -4.1989341 -4.202662 -4.1944079 -4.1769209][-4.2430625 -4.2358055 -4.2215729 -4.2096925 -4.2022595 -4.1975527 -4.1926856 -4.1897688 -4.1967349 -4.2083249 -4.2142162 -4.2078204 -4.1966696 -4.1792283 -4.1559858][-4.2505693 -4.24579 -4.23403 -4.2224207 -4.21458 -4.2115736 -4.2097931 -4.2110615 -4.2194867 -4.2263432 -4.2244511 -4.2100821 -4.1933703 -4.1723461 -4.1464]]...]
INFO - root - 2017-12-05 11:58:21.882483: step 6410, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 77h:19m:33s remains)
INFO - root - 2017-12-05 11:58:30.478878: step 6420, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 78h:34m:19s remains)
INFO - root - 2017-12-05 11:58:38.888999: step 6430, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 77h:18m:04s remains)
INFO - root - 2017-12-05 11:58:47.505597: step 6440, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 78h:06m:40s remains)
INFO - root - 2017-12-05 11:58:55.995629: step 6450, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 78h:59m:54s remains)
INFO - root - 2017-12-05 11:59:04.681233: step 6460, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 80h:12m:59s remains)
INFO - root - 2017-12-05 11:59:13.240127: step 6470, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 76h:01m:33s remains)
INFO - root - 2017-12-05 11:59:21.673797: step 6480, loss = 2.03, batch loss = 1.97 (9.8 examples/sec; 0.818 sec/batch; 74h:04m:53s remains)
INFO - root - 2017-12-05 11:59:30.228741: step 6490, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 78h:45m:23s remains)
INFO - root - 2017-12-05 11:59:38.852345: step 6500, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 77h:54m:49s remains)
2017-12-05 11:59:39.632996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2872028 -4.2643828 -4.2313056 -4.189815 -4.1565781 -4.1313796 -4.1231651 -4.1442056 -4.1772513 -4.2081532 -4.2182369 -4.2155681 -4.2027373 -4.1912351 -4.1804504][-4.273777 -4.2467213 -4.204772 -4.1577477 -4.1197863 -4.0882411 -4.0787983 -4.1051083 -4.1447825 -4.1824126 -4.1958437 -4.1908946 -4.1739159 -4.1589675 -4.1460476][-4.2616634 -4.2296996 -4.1768827 -4.121469 -4.075902 -4.0406623 -4.0365381 -4.0729666 -4.1207776 -4.164258 -4.179472 -4.1699524 -4.150672 -4.1347952 -4.1189513][-4.2486258 -4.2120676 -4.155036 -4.0966597 -4.0455117 -4.0084858 -4.011857 -4.0575657 -4.110076 -4.1523848 -4.1631384 -4.1497779 -4.1305485 -4.1136665 -4.0947924][-4.2329116 -4.1913924 -4.1325588 -4.0746465 -4.0225377 -3.9857912 -3.9919405 -4.0392613 -4.0893292 -4.1251431 -4.1312833 -4.1208229 -4.1080461 -4.0898833 -4.066278][-4.2223353 -4.1736465 -4.1075826 -4.0456152 -3.9940679 -3.9568143 -3.9553328 -3.9899821 -4.0322266 -4.060699 -4.0715857 -4.0782151 -4.0824251 -4.0694003 -4.04168][-4.2205896 -4.1631255 -4.0846319 -4.0111094 -3.9498847 -3.9110045 -3.9028614 -3.9253745 -3.9629314 -3.9899526 -4.0112176 -4.036324 -4.0552969 -4.0478139 -4.0179863][-4.2212343 -4.1570754 -4.0705104 -3.9843686 -3.9128275 -3.8669157 -3.851764 -3.8682468 -3.9059699 -3.9368279 -3.9634316 -3.9979208 -4.0270057 -4.02319 -3.9935863][-4.2172079 -4.1501117 -4.0598583 -3.9688981 -3.8957109 -3.8440521 -3.8243358 -3.8427885 -3.8855875 -3.9170928 -3.9420152 -3.9765379 -4.0114532 -4.0140529 -3.9879007][-4.2120962 -4.1457725 -4.057744 -3.9730539 -3.9098902 -3.8626182 -3.8384085 -3.8569112 -3.9051058 -3.9394751 -3.96268 -3.9960535 -4.0324693 -4.0373077 -4.00966][-4.2115254 -4.1495347 -4.0689664 -3.9951708 -3.9432569 -3.9048779 -3.882308 -3.9055936 -3.957036 -3.9913404 -4.0135727 -4.044385 -4.0754709 -4.0791698 -4.0509148][-4.2156143 -4.1610837 -4.0950756 -4.0359087 -3.9927959 -3.9626069 -3.9474947 -3.9752328 -4.0187063 -4.0446491 -4.0626426 -4.0866413 -4.1105561 -4.1138272 -4.0924592][-4.2247763 -4.1797547 -4.1321688 -4.0891471 -4.0569777 -4.0352926 -4.0279589 -4.0558972 -4.088223 -4.1027088 -4.1138492 -4.1307387 -4.1477613 -4.1492925 -4.1356769][-4.2465835 -4.21401 -4.1820645 -4.1541538 -4.133822 -4.120502 -4.1187053 -4.1428404 -4.1655283 -4.1731648 -4.1758308 -4.1843276 -4.1937137 -4.1936135 -4.1846514][-4.27595 -4.2557716 -4.234324 -4.2155333 -4.2011871 -4.19086 -4.1903515 -4.20851 -4.22569 -4.23118 -4.2309613 -4.2346697 -4.2396269 -4.240097 -4.2360663]]...]
INFO - root - 2017-12-05 11:59:48.188978: step 6510, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.849 sec/batch; 76h:51m:28s remains)
INFO - root - 2017-12-05 11:59:56.784256: step 6520, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 76h:03m:06s remains)
INFO - root - 2017-12-05 12:00:05.389934: step 6530, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 78h:41m:20s remains)
INFO - root - 2017-12-05 12:00:13.845785: step 6540, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.872 sec/batch; 78h:56m:06s remains)
INFO - root - 2017-12-05 12:00:22.472237: step 6550, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 77h:32m:03s remains)
INFO - root - 2017-12-05 12:00:31.008800: step 6560, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 75h:10m:45s remains)
INFO - root - 2017-12-05 12:00:39.666861: step 6570, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 78h:13m:34s remains)
INFO - root - 2017-12-05 12:00:48.246407: step 6580, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 76h:06m:46s remains)
INFO - root - 2017-12-05 12:00:56.820354: step 6590, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 79h:10m:34s remains)
INFO - root - 2017-12-05 12:01:05.267579: step 6600, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 76h:15m:01s remains)
2017-12-05 12:01:06.044137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3005323 -4.2567115 -4.1999211 -4.1442947 -4.1052279 -4.0824704 -4.0753913 -4.0677643 -4.0562167 -4.060256 -4.0782275 -4.0924258 -4.0936966 -4.0944347 -4.093411][-4.2993422 -4.2550826 -4.1953478 -4.1366248 -4.0973682 -4.0784698 -4.0715647 -4.0661068 -4.0547776 -4.0505791 -4.0575089 -4.0735035 -4.0896869 -4.105022 -4.1019392][-4.2999096 -4.2560148 -4.197629 -4.1399808 -4.0984712 -4.0783129 -4.0722041 -4.0738635 -4.0739202 -4.0700722 -4.0724578 -4.0834703 -4.0978146 -4.1113358 -4.1050196][-4.3001251 -4.2525272 -4.1932 -4.1360974 -4.0961456 -4.0756836 -4.069406 -4.0759768 -4.0864291 -4.0962806 -4.1018295 -4.1035962 -4.1079693 -4.1163149 -4.1114879][-4.29929 -4.25026 -4.1889377 -4.1323457 -4.0934591 -4.0746775 -4.0637054 -4.0652847 -4.0841174 -4.1080451 -4.1169496 -4.1130219 -4.1128521 -4.1230454 -4.1233382][-4.2949033 -4.2433105 -4.1804671 -4.1212354 -4.0833225 -4.0604196 -4.0357742 -4.0259171 -4.0531521 -4.090724 -4.1060123 -4.1016436 -4.10003 -4.1101842 -4.1199656][-4.28818 -4.2339368 -4.1694913 -4.1084709 -4.0644355 -4.0283227 -3.981864 -3.9642944 -3.9970672 -4.0458751 -4.0753317 -4.0810032 -4.0817595 -4.093277 -4.1079235][-4.2850704 -4.2312474 -4.1716661 -4.115571 -4.0686526 -4.0232158 -3.9644411 -3.9380171 -3.9624176 -4.0114961 -4.0530534 -4.0704513 -4.0702314 -4.0750217 -4.0859447][-4.2876039 -4.2354169 -4.1842532 -4.1398368 -4.1004553 -4.0579982 -4.0026703 -3.9721749 -3.9844241 -4.0284996 -4.0705929 -4.0836768 -4.0713649 -4.0592833 -4.0584512][-4.2902417 -4.24215 -4.19662 -4.1572862 -4.1226945 -4.0906963 -4.0490112 -4.0247593 -4.0291262 -4.0622382 -4.0980525 -4.1011329 -4.0744314 -4.0473585 -4.0395718][-4.2908173 -4.2468781 -4.2033558 -4.1637545 -4.12865 -4.1056385 -4.0819392 -4.0692148 -4.0695362 -4.0874763 -4.1119623 -4.1119986 -4.0821009 -4.0476522 -4.0371423][-4.2939234 -4.2521944 -4.2055745 -4.1627779 -4.1295342 -4.1131186 -4.105855 -4.1058097 -4.1081362 -4.1147027 -4.1252003 -4.1190419 -4.0933161 -4.0613022 -4.0499635][-4.3060036 -4.2680073 -4.2209864 -4.1756806 -4.144165 -4.1341591 -4.1390076 -4.148056 -4.1522694 -4.1541352 -4.1555638 -4.1489611 -4.1266828 -4.0981178 -4.0885153][-4.3254161 -4.2963972 -4.2558374 -4.2166681 -4.1927147 -4.1902094 -4.2015066 -4.2129951 -4.2149696 -4.2111049 -4.2078886 -4.2048316 -4.191371 -4.1672297 -4.1512437][-4.346034 -4.3282452 -4.2999377 -4.2721682 -4.2578764 -4.2584314 -4.26773 -4.2734466 -4.2700605 -4.2648664 -4.2629042 -4.26466 -4.2590623 -4.2414908 -4.2235346]]...]
INFO - root - 2017-12-05 12:01:14.584924: step 6610, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 77h:25m:04s remains)
INFO - root - 2017-12-05 12:01:23.149994: step 6620, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 77h:20m:57s remains)
INFO - root - 2017-12-05 12:01:31.693794: step 6630, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 74h:34m:44s remains)
INFO - root - 2017-12-05 12:01:40.312298: step 6640, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.790 sec/batch; 71h:32m:41s remains)
INFO - root - 2017-12-05 12:01:48.827773: step 6650, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 78h:27m:39s remains)
INFO - root - 2017-12-05 12:01:57.410744: step 6660, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 78h:16m:15s remains)
INFO - root - 2017-12-05 12:02:06.051235: step 6670, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 77h:14m:05s remains)
INFO - root - 2017-12-05 12:02:14.580140: step 6680, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 77h:00m:45s remains)
INFO - root - 2017-12-05 12:02:23.099649: step 6690, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 77h:04m:54s remains)
INFO - root - 2017-12-05 12:02:31.505570: step 6700, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 76h:33m:07s remains)
2017-12-05 12:02:32.271658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2987714 -4.2869859 -4.2661777 -4.2443581 -4.2241797 -4.2109704 -4.2165923 -4.2334704 -4.2388325 -4.2270589 -4.2075839 -4.1883149 -4.1857872 -4.1996779 -4.2205949][-4.2967367 -4.2851543 -4.2648783 -4.2400761 -4.2173047 -4.2061095 -4.2150807 -4.2231064 -4.2161641 -4.1988921 -4.1812849 -4.1611123 -4.1578856 -4.1768427 -4.2029619][-4.2946711 -4.2822208 -4.2595029 -4.2320085 -4.2094131 -4.1982927 -4.2038679 -4.1942449 -4.1695189 -4.1482015 -4.1357636 -4.1221933 -4.126276 -4.1566811 -4.1906323][-4.2906804 -4.2748919 -4.2490396 -4.21934 -4.1950884 -4.1810074 -4.1715093 -4.133544 -4.0879154 -4.0695314 -4.0696349 -4.0691094 -4.0891738 -4.1350145 -4.1754742][-4.2844996 -4.2658544 -4.2367253 -4.2028852 -4.1752005 -4.1534314 -4.1186519 -4.0448537 -3.9792836 -3.9733543 -3.9978287 -4.0153661 -4.0558505 -4.1136003 -4.1615024][-4.2794466 -4.2586017 -4.2254052 -4.1871152 -4.1562414 -4.1173768 -4.0453367 -3.9297292 -3.8512392 -3.8690159 -3.9240494 -3.9687049 -4.0314136 -4.0962186 -4.1483288][-4.2760959 -4.2519612 -4.2129846 -4.1693873 -4.1304111 -4.06406 -3.9464016 -3.7984569 -3.732796 -3.7916503 -3.8771119 -3.9518795 -4.0340314 -4.0995255 -4.148283][-4.2711363 -4.2427082 -4.1983442 -4.1478672 -4.097754 -4.0070648 -3.8657079 -3.7270939 -3.7021842 -3.7949653 -3.8944571 -3.9770088 -4.0578032 -4.1133971 -4.158299][-4.2679687 -4.2389193 -4.1961684 -4.1474757 -4.0985065 -4.0084724 -3.8848267 -3.7906332 -3.7917268 -3.8767889 -3.9648049 -4.0348873 -4.1000757 -4.1428819 -4.1828628][-4.2679186 -4.24372 -4.2100353 -4.1710086 -4.1338687 -4.0633678 -3.973964 -3.9227767 -3.9339416 -3.9916272 -4.0549831 -4.1025991 -4.1491351 -4.1849818 -4.2175326][-4.2689538 -4.2496033 -4.2242603 -4.1958561 -4.1723208 -4.1271267 -4.0712175 -4.0469127 -4.0589 -4.0948625 -4.1387177 -4.1698251 -4.200325 -4.2286358 -4.255127][-4.2720037 -4.2539463 -4.2323928 -4.2119079 -4.1997323 -4.1766472 -4.1482925 -4.1388535 -4.1497383 -4.1751671 -4.2078066 -4.229166 -4.2509618 -4.2712193 -4.2925744][-4.2774763 -4.25874 -4.2373662 -4.2225142 -4.2188029 -4.2131109 -4.2059183 -4.2087359 -4.2240362 -4.2470922 -4.2721677 -4.285017 -4.2980046 -4.30955 -4.32454][-4.2868929 -4.268353 -4.24684 -4.2335739 -4.2332873 -4.2347264 -4.2393727 -4.2539907 -4.2759809 -4.3007703 -4.32007 -4.3256564 -4.3316889 -4.3357487 -4.3430758][-4.2948008 -4.2787385 -4.2555714 -4.2395816 -4.2363825 -4.2377839 -4.2474833 -4.2700753 -4.29872 -4.3253202 -4.3418431 -4.3463573 -4.3488131 -4.3482447 -4.3483477]]...]
INFO - root - 2017-12-05 12:02:40.850761: step 6710, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.838 sec/batch; 75h:47m:49s remains)
INFO - root - 2017-12-05 12:02:49.337816: step 6720, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 77h:56m:22s remains)
INFO - root - 2017-12-05 12:02:57.742224: step 6730, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 73h:55m:45s remains)
INFO - root - 2017-12-05 12:03:06.108445: step 6740, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 75h:27m:20s remains)
INFO - root - 2017-12-05 12:03:14.479444: step 6750, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.833 sec/batch; 75h:24m:07s remains)
INFO - root - 2017-12-05 12:03:22.879972: step 6760, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 75h:27m:53s remains)
INFO - root - 2017-12-05 12:03:31.395884: step 6770, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 76h:12m:25s remains)
INFO - root - 2017-12-05 12:03:39.998345: step 6780, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 78h:42m:19s remains)
INFO - root - 2017-12-05 12:03:48.480097: step 6790, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 76h:22m:08s remains)
INFO - root - 2017-12-05 12:03:56.863808: step 6800, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 77h:03m:48s remains)
2017-12-05 12:03:57.664327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.224824 -4.1946092 -4.1731973 -4.1606359 -4.1801605 -4.2228184 -4.255033 -4.2584233 -4.2550411 -4.2593775 -4.2643433 -4.2595024 -4.2507033 -4.2480993 -4.2279105][-4.2257633 -4.1906986 -4.1606131 -4.1406879 -4.1539106 -4.1925445 -4.2199888 -4.2246294 -4.2261047 -4.2358809 -4.2449837 -4.2418704 -4.2331638 -4.2312813 -4.20834][-4.2249074 -4.1881695 -4.1571693 -4.1372519 -4.1419878 -4.1664767 -4.1783123 -4.1787434 -4.1830735 -4.1989446 -4.214098 -4.2109823 -4.201129 -4.1981812 -4.1715565][-4.22545 -4.190382 -4.1629605 -4.1445222 -4.1401305 -4.1492634 -4.1471415 -4.1419148 -4.1422133 -4.1598716 -4.1801643 -4.1758032 -4.1622834 -4.1563377 -4.1231937][-4.2260709 -4.1910329 -4.1661005 -4.1492705 -4.1391568 -4.1385756 -4.13199 -4.1252818 -4.1209331 -4.1381445 -4.1610351 -4.1553483 -4.1417031 -4.1332545 -4.0996642][-4.2280488 -4.1940265 -4.1682796 -4.1493092 -4.1386447 -4.1428189 -4.1408195 -4.1337328 -4.126318 -4.140111 -4.1616445 -4.160953 -4.1574597 -4.149992 -4.1196423][-4.2287273 -4.1967583 -4.1660857 -4.14127 -4.1332784 -4.1439209 -4.14263 -4.1347804 -4.1250582 -4.1381612 -4.1623006 -4.1704073 -4.1747403 -4.1686287 -4.1418962][-4.2218909 -4.1832318 -4.1423974 -4.1108723 -4.1053524 -4.1129255 -4.1054459 -4.0951777 -4.08503 -4.1021605 -4.1312628 -4.150475 -4.1626029 -4.162035 -4.1468549][-4.209425 -4.1586056 -4.1060786 -4.0663714 -4.0591345 -4.0622644 -4.0518365 -4.0408716 -4.0293684 -4.0517478 -4.0874705 -4.1161742 -4.1373386 -4.148664 -4.1487527][-4.2024679 -4.143034 -4.0816164 -4.0345478 -4.0230346 -4.0248766 -4.0184832 -4.0063648 -3.994308 -4.0193357 -4.0600843 -4.0953741 -4.1246204 -4.146153 -4.1575408][-4.2099385 -4.153357 -4.0926504 -4.0414376 -4.0237012 -4.0266104 -4.0282488 -4.0240269 -4.0208855 -4.0432673 -4.0782642 -4.1105986 -4.1403179 -4.1668057 -4.1839671][-4.233459 -4.1861567 -4.1360884 -4.0906029 -4.0724235 -4.0730691 -4.0804048 -4.0856729 -4.0864611 -4.1004753 -4.124896 -4.1506228 -4.1772442 -4.2033076 -4.22202][-4.2658596 -4.2313132 -4.1949487 -4.1615295 -4.1467695 -4.1446748 -4.149992 -4.1560535 -4.1557484 -4.1618404 -4.1760178 -4.1939387 -4.2145991 -4.23573 -4.2513266][-4.2913055 -4.2687464 -4.2452087 -4.2242765 -4.2136416 -4.2095227 -4.210608 -4.21217 -4.2099595 -4.2108636 -4.2183847 -4.2292786 -4.2433758 -4.25903 -4.2721434][-4.3058743 -4.2925596 -4.2789984 -4.2679429 -4.26269 -4.2588959 -4.2579217 -4.25748 -4.2548122 -4.2534618 -4.255928 -4.2614064 -4.2693071 -4.2800746 -4.2893424]]...]
INFO - root - 2017-12-05 12:04:06.229085: step 6810, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 78h:32m:50s remains)
INFO - root - 2017-12-05 12:04:14.800529: step 6820, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 77h:03m:52s remains)
INFO - root - 2017-12-05 12:04:23.355964: step 6830, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 76h:42m:21s remains)
INFO - root - 2017-12-05 12:04:31.909175: step 6840, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 77h:41m:55s remains)
INFO - root - 2017-12-05 12:04:40.393604: step 6850, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 76h:57m:33s remains)
INFO - root - 2017-12-05 12:04:48.868620: step 6860, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 78h:31m:41s remains)
INFO - root - 2017-12-05 12:04:57.446214: step 6870, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 81h:14m:38s remains)
INFO - root - 2017-12-05 12:05:06.074592: step 6880, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 78h:36m:37s remains)
INFO - root - 2017-12-05 12:05:14.612736: step 6890, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 76h:17m:19s remains)
INFO - root - 2017-12-05 12:05:23.113521: step 6900, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 78h:56m:24s remains)
2017-12-05 12:05:23.887282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.230567 -4.2034688 -4.1917067 -4.1852036 -4.1711645 -4.1574535 -4.1545844 -4.1723585 -4.204596 -4.2248898 -4.24299 -4.2581778 -4.2595935 -4.2322669 -4.1966128][-4.2157149 -4.1842146 -4.1717758 -4.1671572 -4.149735 -4.1261492 -4.1173325 -4.140801 -4.1851 -4.2130742 -4.2351484 -4.2524419 -4.2492166 -4.2128782 -4.17071][-4.20737 -4.1729121 -4.1608381 -4.1573973 -4.1405592 -4.1131916 -4.099134 -4.1277003 -4.1858406 -4.2183113 -4.2377944 -4.252018 -4.2406321 -4.1942539 -4.1443677][-4.196919 -4.1562333 -4.1409092 -4.1394677 -4.13415 -4.1137137 -4.0985079 -4.1290216 -4.1911979 -4.2217908 -4.2382894 -4.2485156 -4.231379 -4.1811481 -4.1252708][-4.1937861 -4.1505404 -4.1318851 -4.1277261 -4.1271577 -4.1057162 -4.0758786 -4.0952 -4.1575012 -4.1971316 -4.222446 -4.23492 -4.2203846 -4.1782022 -4.1295428][-4.1977797 -4.1559248 -4.1346135 -4.1280322 -4.1224842 -4.0834618 -4.0231395 -4.0235167 -4.0970731 -4.1601448 -4.19989 -4.218606 -4.21254 -4.1855922 -4.1418562][-4.2011833 -4.1593161 -4.1314459 -4.1097946 -4.082015 -4.0085931 -3.9006376 -3.8855972 -3.9958758 -4.1020727 -4.168716 -4.1992779 -4.2092385 -4.202415 -4.1701694][-4.2088251 -4.1687036 -4.1311669 -4.0893965 -4.0280709 -3.9019461 -3.7297275 -3.7181726 -3.8903995 -4.04926 -4.1423197 -4.1891246 -4.217752 -4.22545 -4.2073736][-4.210732 -4.1721044 -4.1348004 -4.0954857 -4.0253263 -3.877193 -3.689965 -3.6950533 -3.8809066 -4.0444913 -4.1401572 -4.1968946 -4.2336974 -4.24288 -4.2285657][-4.20997 -4.1689453 -4.1372609 -4.1072845 -4.0553145 -3.9538968 -3.8522892 -3.8730874 -3.9939361 -4.0991707 -4.1622052 -4.2049031 -4.2331438 -4.2377839 -4.2248058][-4.2146897 -4.1754107 -4.1507392 -4.1270442 -4.0893822 -4.0257068 -3.9765334 -4.0007648 -4.0803261 -4.1436133 -4.1809072 -4.2079945 -4.2196784 -4.2108231 -4.195652][-4.229991 -4.1965861 -4.1775861 -4.1580849 -4.1255021 -4.0799074 -4.0418983 -4.0574646 -4.1088591 -4.1494527 -4.1749239 -4.1948032 -4.1998196 -4.1889443 -4.17478][-4.2457404 -4.2177415 -4.198822 -4.1808724 -4.1560163 -4.125874 -4.092123 -4.1032438 -4.136673 -4.1603956 -4.1781826 -4.1910248 -4.195312 -4.1915569 -4.1827216][-4.256238 -4.2300386 -4.2091231 -4.1911931 -4.1734295 -4.1533041 -4.1255426 -4.1360731 -4.1584167 -4.1735172 -4.1915193 -4.2049294 -4.214397 -4.2149434 -4.2058539][-4.2617788 -4.2391558 -4.2216167 -4.2060266 -4.1933141 -4.1775489 -4.1544123 -4.1648774 -4.1821537 -4.1959982 -4.2140226 -4.2293696 -4.2438259 -4.2442145 -4.2332149]]...]
INFO - root - 2017-12-05 12:05:32.468610: step 6910, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 77h:33m:43s remains)
INFO - root - 2017-12-05 12:05:40.998040: step 6920, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 77h:39m:08s remains)
INFO - root - 2017-12-05 12:05:49.656204: step 6930, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 76h:06m:15s remains)
INFO - root - 2017-12-05 12:05:58.151693: step 6940, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 77h:47m:15s remains)
INFO - root - 2017-12-05 12:06:06.765315: step 6950, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 76h:56m:07s remains)
INFO - root - 2017-12-05 12:06:15.442956: step 6960, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 75h:14m:02s remains)
INFO - root - 2017-12-05 12:06:23.808106: step 6970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 77h:54m:15s remains)
INFO - root - 2017-12-05 12:06:32.349082: step 6980, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 78h:57m:35s remains)
INFO - root - 2017-12-05 12:06:40.823181: step 6990, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 77h:41m:39s remains)
INFO - root - 2017-12-05 12:06:49.292086: step 7000, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.816 sec/batch; 73h:44m:49s remains)
2017-12-05 12:06:50.076506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1640186 -4.1758714 -4.1831465 -4.1898146 -4.196497 -4.2098579 -4.228591 -4.2308517 -4.2295737 -4.2320485 -4.2371345 -4.2355895 -4.2284575 -4.225421 -4.2214651][-4.150877 -4.161171 -4.1647677 -4.1673393 -4.1706223 -4.1840053 -4.206656 -4.2112207 -4.2110171 -4.2137709 -4.2196574 -4.2178755 -4.2100534 -4.2086396 -4.21168][-4.144423 -4.1502075 -4.151475 -4.1512265 -4.1508579 -4.1586089 -4.1757016 -4.1786685 -4.1751494 -4.1769476 -4.1843462 -4.1867456 -4.1839018 -4.1908355 -4.2068028][-4.1418748 -4.1394787 -4.1385188 -4.1413054 -4.1381741 -4.1342468 -4.1366391 -4.1326818 -4.1276283 -4.1344948 -4.1498332 -4.1556354 -4.1569657 -4.1738772 -4.2060137][-4.1317596 -4.1256661 -4.1281633 -4.1351571 -4.1313343 -4.1158118 -4.0977855 -4.0807285 -4.0832505 -4.1063147 -4.1326423 -4.1425204 -4.1439109 -4.1644158 -4.2046504][-4.1128254 -4.0982242 -4.09514 -4.0977488 -4.0871992 -4.0575976 -4.0117831 -3.9781039 -3.9970036 -4.0454087 -4.0885296 -4.1085353 -4.1160583 -4.1424856 -4.1869168][-4.093894 -4.0654078 -4.0477 -4.0394163 -4.0209188 -3.9813242 -3.9251311 -3.8920636 -3.9272707 -3.99297 -4.0471225 -4.0752544 -4.091279 -4.1207042 -4.1656113][-4.1052318 -4.0732775 -4.0482969 -4.0355835 -4.0188122 -3.9870241 -3.9493911 -3.9380274 -3.9776742 -4.028832 -4.0700321 -4.089942 -4.1049118 -4.1312485 -4.1653905][-4.1293736 -4.1051536 -4.0835528 -4.0728908 -4.0618982 -4.0417318 -4.0275893 -4.0358858 -4.0679655 -4.0927382 -4.1102147 -4.1171269 -4.1268029 -4.1502776 -4.1697178][-4.1399555 -4.1280422 -4.1148977 -4.1074309 -4.1006064 -4.08797 -4.0920467 -4.113059 -4.136879 -4.1444902 -4.1455569 -4.1427107 -4.1492472 -4.1672816 -4.171895][-4.1467743 -4.1445088 -4.140255 -4.1366491 -4.1340656 -4.1294947 -4.1412196 -4.1651344 -4.1833348 -4.18503 -4.1825962 -4.1802392 -4.1836152 -4.194252 -4.1891484][-4.1574264 -4.1612048 -4.1618209 -4.1620393 -4.163281 -4.1646214 -4.1797609 -4.1985416 -4.2078509 -4.2112966 -4.2134619 -4.2133031 -4.2172947 -4.2234521 -4.2189293][-4.1802549 -4.1876125 -4.1904492 -4.1948881 -4.2004514 -4.2052264 -4.2184157 -4.22722 -4.2282429 -4.2327437 -4.2379513 -4.2412138 -4.24822 -4.2542043 -4.2543225][-4.1976991 -4.2041521 -4.2070632 -4.2152104 -4.2262774 -4.2338562 -4.2450862 -4.2477536 -4.2458453 -4.2514596 -4.2577348 -4.2635031 -4.2727332 -4.2809582 -4.2875514][-4.213181 -4.2154036 -4.2190995 -4.2307696 -4.2440758 -4.2519712 -4.2590413 -4.2589855 -4.2572603 -4.2620406 -4.2676382 -4.2725959 -4.281692 -4.292304 -4.3035817]]...]
INFO - root - 2017-12-05 12:06:58.584354: step 7010, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 78h:18m:57s remains)
INFO - root - 2017-12-05 12:07:07.092834: step 7020, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 77h:27m:53s remains)
INFO - root - 2017-12-05 12:07:15.664234: step 7030, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 75h:59m:36s remains)
INFO - root - 2017-12-05 12:07:24.288048: step 7040, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 77h:26m:59s remains)
INFO - root - 2017-12-05 12:07:32.868564: step 7050, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 77h:04m:23s remains)
INFO - root - 2017-12-05 12:07:41.575845: step 7060, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 75h:14m:04s remains)
INFO - root - 2017-12-05 12:07:50.060175: step 7070, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 77h:33m:23s remains)
INFO - root - 2017-12-05 12:07:58.489927: step 7080, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 75h:32m:56s remains)
INFO - root - 2017-12-05 12:08:07.075017: step 7090, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 78h:33m:07s remains)
INFO - root - 2017-12-05 12:08:15.525305: step 7100, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.864 sec/batch; 78h:05m:07s remains)
2017-12-05 12:08:16.238790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1702394 -4.1616073 -4.1472383 -4.1314669 -4.1294503 -4.131588 -4.131588 -4.1337147 -4.1335745 -4.1412759 -4.1662512 -4.1955934 -4.1992311 -4.1862173 -4.18846][-4.1674118 -4.1650419 -4.1578383 -4.1422009 -4.1279016 -4.1212835 -4.1282949 -4.1421809 -4.1442866 -4.1457562 -4.16412 -4.1885886 -4.1910982 -4.1777306 -4.1799464][-4.1542411 -4.1585975 -4.1601787 -4.1488051 -4.1274624 -4.1123271 -4.1278429 -4.1587591 -4.1712441 -4.1713176 -4.1774225 -4.19331 -4.194 -4.1831679 -4.1868205][-4.1303606 -4.1408014 -4.1510429 -4.1448932 -4.11935 -4.0977335 -4.1194816 -4.1610575 -4.1825471 -4.1875062 -4.19196 -4.2050838 -4.2084947 -4.1996865 -4.2026963][-4.1056156 -4.1166148 -4.1330061 -4.1330528 -4.1076455 -4.0788474 -4.0906682 -4.1244164 -4.1486239 -4.1675963 -4.18307 -4.2019739 -4.2125044 -4.20864 -4.2096105][-4.1014 -4.1101017 -4.1289053 -4.1343164 -4.109529 -4.0674162 -4.0483608 -4.0478272 -4.0595074 -4.0986166 -4.1410007 -4.1749668 -4.1963658 -4.2064915 -4.2132664][-4.131763 -4.1343336 -4.1476307 -4.1534061 -4.1233044 -4.0637183 -4.0053248 -3.9524858 -3.9332063 -3.9911821 -4.0706062 -4.128015 -4.1607351 -4.1853995 -4.206686][-4.1695619 -4.1677041 -4.1710625 -4.1726184 -4.1419849 -4.0705271 -3.9819715 -3.8844073 -3.8283067 -3.8890023 -3.9961629 -4.0723305 -4.115561 -4.1538916 -4.1919384][-4.192225 -4.1921544 -4.1885295 -4.1836109 -4.163125 -4.1053495 -4.0243921 -3.9309816 -3.8700776 -3.8998933 -3.9745023 -4.0368876 -4.0811019 -4.1284094 -4.17669][-4.1702838 -4.1785455 -4.1803994 -4.179285 -4.1740737 -4.1492481 -4.1048903 -4.0505629 -4.0085135 -4.0038557 -4.0173926 -4.0333838 -4.0565987 -4.0971355 -4.1455379][-4.1223955 -4.1396556 -4.1543183 -4.1627421 -4.169745 -4.1705537 -4.1613159 -4.1431251 -4.122149 -4.1051064 -4.0813932 -4.0586491 -4.0533643 -4.0732889 -4.1096759][-4.091495 -4.105607 -4.1268992 -4.1389995 -4.1451726 -4.1535497 -4.1624503 -4.1664996 -4.1699533 -4.1646919 -4.1364183 -4.0993857 -4.0738039 -4.0719562 -4.0918422][-4.084362 -4.08705 -4.1095362 -4.1239729 -4.1215224 -4.1201887 -4.1291437 -4.1449842 -4.1693177 -4.1840148 -4.1728692 -4.1428647 -4.1137433 -4.1004667 -4.1099472][-4.0949807 -4.0951366 -4.1204729 -4.1399283 -4.13449 -4.1195211 -4.1124878 -4.1227655 -4.1516457 -4.179028 -4.1854148 -4.1697688 -4.1480236 -4.1341748 -4.1406231][-4.1225863 -4.1303883 -4.1552272 -4.1761737 -4.1755853 -4.1594543 -4.1394796 -4.131175 -4.1416173 -4.1584144 -4.1680193 -4.1651955 -4.1581063 -4.1549048 -4.1652904]]...]
INFO - root - 2017-12-05 12:08:24.731903: step 7110, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.836 sec/batch; 75h:33m:43s remains)
INFO - root - 2017-12-05 12:08:33.189547: step 7120, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 76h:36m:58s remains)
INFO - root - 2017-12-05 12:08:41.787483: step 7130, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 75h:20m:19s remains)
INFO - root - 2017-12-05 12:08:50.197091: step 7140, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 78h:39m:34s remains)
INFO - root - 2017-12-05 12:08:58.723219: step 7150, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 76h:22m:14s remains)
INFO - root - 2017-12-05 12:09:07.162204: step 7160, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 77h:59m:18s remains)
INFO - root - 2017-12-05 12:09:15.813284: step 7170, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 79h:20m:00s remains)
INFO - root - 2017-12-05 12:09:24.340833: step 7180, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 77h:38m:13s remains)
INFO - root - 2017-12-05 12:09:32.863962: step 7190, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 76h:52m:40s remains)
INFO - root - 2017-12-05 12:09:41.264087: step 7200, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 74h:29m:20s remains)
2017-12-05 12:09:41.991125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2218623 -4.2247119 -4.2257547 -4.2248359 -4.2276983 -4.2317438 -4.2273378 -4.2248712 -4.2264662 -4.2247143 -4.21537 -4.2046008 -4.20337 -4.2102957 -4.2139263][-4.2555332 -4.2548995 -4.2488403 -4.2417383 -4.2397079 -4.2433281 -4.2395616 -4.2366362 -4.2377963 -4.2376413 -4.2296438 -4.2202311 -4.2170215 -4.2153449 -4.2073345][-4.2773695 -4.2713871 -4.256568 -4.2393756 -4.23078 -4.2328448 -4.2267027 -4.22034 -4.2232523 -4.2333851 -4.2357063 -4.2311749 -4.2259688 -4.2142882 -4.1980944][-4.28142 -4.2748728 -4.2553129 -4.2302275 -4.2129731 -4.205204 -4.1931915 -4.1851277 -4.1952882 -4.2245994 -4.2453403 -4.2472529 -4.2407584 -4.2199059 -4.1957307][-4.2623806 -4.2614317 -4.2421618 -4.2081861 -4.1782217 -4.153275 -4.1239071 -4.1079483 -4.1295986 -4.1873341 -4.2375298 -4.2596669 -4.2613182 -4.2367535 -4.2087474][-4.2485485 -4.253016 -4.229301 -4.1775293 -4.1236515 -4.0692339 -4.0052338 -3.9681003 -4.0039315 -4.1020126 -4.1953998 -4.2571559 -4.28589 -4.2729673 -4.2496076][-4.2511196 -4.2555194 -4.2240257 -4.1525087 -4.068686 -3.9755228 -3.8635454 -3.7899616 -3.8419633 -3.9909053 -4.132834 -4.2354155 -4.2959275 -4.3017511 -4.2876463][-4.257339 -4.264256 -4.2311616 -4.1535187 -4.0545321 -3.9394834 -3.792963 -3.6807566 -3.734375 -3.9167268 -4.0855751 -4.207396 -4.2828584 -4.3005004 -4.2901473][-4.2634745 -4.2781639 -4.2504272 -4.1840124 -4.09799 -4.001657 -3.8754787 -3.7688453 -3.7969732 -3.946178 -4.0958781 -4.2046323 -4.2690229 -4.2825084 -4.2669616][-4.2778234 -4.2944765 -4.2720523 -4.222095 -4.1634583 -4.1007061 -4.0226135 -3.9538829 -3.9627736 -4.0531845 -4.1569238 -4.2282333 -4.2597761 -4.2533984 -4.2258139][-4.3087554 -4.320282 -4.3020868 -4.2661104 -4.2257662 -4.1866531 -4.1448 -4.1106372 -4.1167336 -4.168818 -4.2320814 -4.2673864 -4.2679868 -4.2410688 -4.1974335][-4.3266997 -4.3349061 -4.3234291 -4.3014703 -4.2712593 -4.241715 -4.2181873 -4.2041855 -4.2125049 -4.24694 -4.2867179 -4.2989979 -4.2811036 -4.2452273 -4.1941752][-4.3257751 -4.3321948 -4.3267379 -4.31797 -4.2995958 -4.277884 -4.2627411 -4.2591181 -4.2699046 -4.2937641 -4.3175616 -4.3189325 -4.2975087 -4.2618556 -4.2163024][-4.3209453 -4.3260961 -4.3215537 -4.3181767 -4.3118277 -4.3004704 -4.2912874 -4.2905993 -4.2977705 -4.3130751 -4.329546 -4.3330011 -4.3173914 -4.2889252 -4.2541718][-4.3135638 -4.3144026 -4.3073492 -4.3037295 -4.3019733 -4.2982082 -4.2966084 -4.2988219 -4.3025975 -4.311316 -4.3244181 -4.3345008 -4.3314743 -4.3161149 -4.295115]]...]
INFO - root - 2017-12-05 12:09:50.576164: step 7210, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 77h:42m:10s remains)
INFO - root - 2017-12-05 12:09:59.047105: step 7220, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.856 sec/batch; 77h:18m:24s remains)
INFO - root - 2017-12-05 12:10:07.521164: step 7230, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.819 sec/batch; 74h:02m:30s remains)
INFO - root - 2017-12-05 12:10:16.088549: step 7240, loss = 2.02, batch loss = 1.97 (9.2 examples/sec; 0.867 sec/batch; 78h:17m:25s remains)
INFO - root - 2017-12-05 12:10:24.620610: step 7250, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 77h:19m:09s remains)
INFO - root - 2017-12-05 12:10:33.144934: step 7260, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 77h:08m:15s remains)
INFO - root - 2017-12-05 12:10:41.656189: step 7270, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 79h:03m:18s remains)
INFO - root - 2017-12-05 12:10:50.118990: step 7280, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 75h:55m:45s remains)
INFO - root - 2017-12-05 12:10:58.622036: step 7290, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 75h:26m:56s remains)
INFO - root - 2017-12-05 12:11:07.019329: step 7300, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 77h:42m:49s remains)
2017-12-05 12:11:07.796201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.279624 -4.2685442 -4.2485662 -4.2283573 -4.2050748 -4.1592236 -4.1115589 -4.102026 -4.1004295 -4.1145639 -4.1478081 -4.18285 -4.2119694 -4.2236309 -4.2177758][-4.2807336 -4.2671585 -4.2423763 -4.2183247 -4.1933303 -4.1436272 -4.0923576 -4.0818949 -4.0868993 -4.1016059 -4.1233926 -4.1457949 -4.1709428 -4.1813707 -4.1674008][-4.285017 -4.270617 -4.2430377 -4.2187891 -4.1947002 -4.1440525 -4.0871248 -4.0693736 -4.0775275 -4.0934019 -4.1075964 -4.1196065 -4.1398468 -4.1506095 -4.1362491][-4.284399 -4.2705545 -4.2434072 -4.2185082 -4.1943817 -4.1471138 -4.0863442 -4.0610213 -4.07502 -4.0959153 -4.10275 -4.1089268 -4.1246352 -4.137434 -4.12729][-4.2806249 -4.2673154 -4.2426562 -4.2194524 -4.1944523 -4.148788 -4.0790353 -4.0412087 -4.0579357 -4.0858936 -4.0917044 -4.0900755 -4.0999203 -4.11463 -4.1097622][-4.2696562 -4.2580147 -4.2348433 -4.211257 -4.1835914 -4.1344433 -4.0505567 -4.0067897 -4.0408959 -4.08892 -4.1042366 -4.0971522 -4.0993967 -4.1104131 -4.1050062][-4.2585464 -4.2538166 -4.2314568 -4.1992478 -4.1592994 -4.0969372 -3.9925957 -3.9415159 -4.0039487 -4.0827675 -4.1150327 -4.1158261 -4.1161213 -4.1202149 -4.1121488][-4.25906 -4.2579031 -4.232039 -4.1866326 -4.1300511 -4.04413 -3.9076662 -3.8397014 -3.9291286 -4.0385671 -4.0897174 -4.1010923 -4.1100206 -4.1193905 -4.1176705][-4.2664127 -4.2651606 -4.2359662 -4.1850386 -4.1213331 -4.028851 -3.8891239 -3.8165042 -3.9052794 -4.0169277 -4.0724607 -4.0926223 -4.1099596 -4.1277113 -4.1335363][-4.271718 -4.2729573 -4.2471986 -4.2020817 -4.1532421 -4.0857944 -3.98186 -3.9204321 -3.9784548 -4.0610533 -4.1046715 -4.1198606 -4.1310019 -4.1442108 -4.14711][-4.2726212 -4.2757936 -4.2575626 -4.2231688 -4.19058 -4.1456103 -4.0749469 -4.025373 -4.0536237 -4.1090946 -4.1411939 -4.1525631 -4.1630597 -4.1766458 -4.1787825][-4.26945 -4.2752805 -4.2689514 -4.2501817 -4.2292213 -4.1980362 -4.150804 -4.1131082 -4.1208744 -4.1533432 -4.174324 -4.1821189 -4.1925597 -4.208694 -4.2126913][-4.2539959 -4.2643189 -4.2721734 -4.270021 -4.2587781 -4.2379661 -4.2080173 -4.1830916 -4.1848435 -4.2011571 -4.2097754 -4.2095146 -4.2153296 -4.2286 -4.2294388][-4.234076 -4.2476854 -4.2654538 -4.2721443 -4.2707109 -4.2612386 -4.2439566 -4.23027 -4.2319674 -4.238627 -4.235661 -4.2254858 -4.2233095 -4.2295904 -4.2278595][-4.2260547 -4.2367272 -4.255435 -4.26599 -4.2705588 -4.2657962 -4.2547436 -4.2465539 -4.2477331 -4.2513957 -4.2466149 -4.2355132 -4.2298636 -4.2266583 -4.2225571]]...]
INFO - root - 2017-12-05 12:11:16.222523: step 7310, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 76h:16m:43s remains)
INFO - root - 2017-12-05 12:11:24.830795: step 7320, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 78h:12m:58s remains)
INFO - root - 2017-12-05 12:11:33.394787: step 7330, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 76h:52m:25s remains)
INFO - root - 2017-12-05 12:11:42.042139: step 7340, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 79h:08m:03s remains)
INFO - root - 2017-12-05 12:11:50.565798: step 7350, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 79h:31m:22s remains)
INFO - root - 2017-12-05 12:11:59.187999: step 7360, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 77h:34m:31s remains)
INFO - root - 2017-12-05 12:12:07.766513: step 7370, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 78h:39m:59s remains)
INFO - root - 2017-12-05 12:12:16.377163: step 7380, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 77h:31m:50s remains)
INFO - root - 2017-12-05 12:12:24.872446: step 7390, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 78h:53m:56s remains)
INFO - root - 2017-12-05 12:12:33.221088: step 7400, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 0.793 sec/batch; 71h:37m:29s remains)
2017-12-05 12:12:34.068451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3193755 -4.3114905 -4.2917585 -4.2737103 -4.2608204 -4.2450013 -4.2278314 -4.2111907 -4.2220626 -4.2401962 -4.253819 -4.2544928 -4.2463522 -4.2391682 -4.235702][-4.31511 -4.3069363 -4.2856989 -4.2642765 -4.2486877 -4.2319126 -4.2085543 -4.1900039 -4.2034936 -4.2212868 -4.2359748 -4.241437 -4.2386804 -4.2365417 -4.2342753][-4.3103232 -4.3025045 -4.2828016 -4.2604256 -4.2456594 -4.2321982 -4.20688 -4.1879544 -4.199481 -4.2138634 -4.2243986 -4.2296882 -4.2282152 -4.2246485 -4.2193165][-4.305429 -4.2970357 -4.2759972 -4.2498226 -4.2339191 -4.2266612 -4.2065949 -4.1899242 -4.1941919 -4.2024937 -4.2113972 -4.2180533 -4.2191672 -4.2122769 -4.2031379][-4.3029871 -4.2930322 -4.2678227 -4.2369714 -4.2177348 -4.2152743 -4.20107 -4.183908 -4.1762104 -4.175827 -4.1857433 -4.1973281 -4.2063441 -4.2031083 -4.1948495][-4.3037186 -4.2924356 -4.2643781 -4.2286286 -4.2026711 -4.19764 -4.1824603 -4.1562138 -4.133657 -4.1271844 -4.1425314 -4.166256 -4.1880889 -4.1967969 -4.1947918][-4.3070183 -4.2966871 -4.2687778 -4.2280512 -4.1910582 -4.172473 -4.147738 -4.1091475 -4.0640264 -4.0441885 -4.0709248 -4.1152649 -4.1623516 -4.1926827 -4.2041063][-4.3095818 -4.2998538 -4.2722578 -4.2274508 -4.1832275 -4.1516676 -4.1229057 -4.0829058 -4.03098 -4.0099077 -4.0482559 -4.1064906 -4.1642351 -4.2021027 -4.2172341][-4.3075275 -4.2961936 -4.2681127 -4.2220631 -4.17657 -4.136343 -4.1114 -4.0882697 -4.0578356 -4.0566454 -4.0996208 -4.1499662 -4.196382 -4.2295909 -4.2406292][-4.3022404 -4.2897677 -4.2646751 -4.2251306 -4.1880946 -4.149827 -4.1318707 -4.1259918 -4.1237493 -4.1376338 -4.171205 -4.2050328 -4.2378531 -4.2628388 -4.2692142][-4.2962713 -4.2853894 -4.2678485 -4.2395682 -4.2162094 -4.1878953 -4.1746473 -4.1729822 -4.1834354 -4.2039557 -4.2295947 -4.2511611 -4.272481 -4.2869163 -4.2853031][-4.2944856 -4.2866707 -4.2754908 -4.2572074 -4.2445974 -4.2274609 -4.2181458 -4.2155581 -4.2298059 -4.2499814 -4.2697611 -4.2799115 -4.2883258 -4.2938743 -4.2858934][-4.2961731 -4.2911191 -4.2836375 -4.2712846 -4.2629676 -4.257298 -4.255105 -4.2538981 -4.267508 -4.2843661 -4.2953644 -4.294435 -4.2897406 -4.2856059 -4.2727413][-4.3010387 -4.2988243 -4.2926641 -4.2816691 -4.2724094 -4.2712345 -4.2707562 -4.2730055 -4.2878814 -4.3042974 -4.3109174 -4.3051653 -4.2926879 -4.2786794 -4.2582936][-4.3044667 -4.3052454 -4.3006425 -4.2876325 -4.2725658 -4.2663789 -4.2600994 -4.2630382 -4.28364 -4.3074894 -4.3175845 -4.3136444 -4.2993603 -4.279119 -4.251637]]...]
INFO - root - 2017-12-05 12:12:42.754152: step 7410, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 78h:27m:39s remains)
INFO - root - 2017-12-05 12:12:51.137591: step 7420, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 78h:04m:24s remains)
INFO - root - 2017-12-05 12:12:59.733321: step 7430, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 78h:06m:33s remains)
INFO - root - 2017-12-05 12:13:08.365142: step 7440, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 80h:34m:24s remains)
INFO - root - 2017-12-05 12:13:16.999530: step 7450, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 80h:28m:11s remains)
INFO - root - 2017-12-05 12:13:25.560504: step 7460, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 74h:16m:26s remains)
INFO - root - 2017-12-05 12:13:34.115601: step 7470, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 75h:52m:24s remains)
INFO - root - 2017-12-05 12:13:42.682624: step 7480, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 76h:51m:19s remains)
INFO - root - 2017-12-05 12:13:51.322398: step 7490, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 77h:04m:10s remains)
INFO - root - 2017-12-05 12:13:59.743429: step 7500, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 77h:18m:11s remains)
2017-12-05 12:14:00.492244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2824316 -4.2757511 -4.2694054 -4.2676597 -4.2598515 -4.2400107 -4.2237339 -4.2229509 -4.2305779 -4.2368736 -4.2396512 -4.2483139 -4.258215 -4.2585731 -4.2495518][-4.2706237 -4.2630739 -4.2544951 -4.2478218 -4.2278967 -4.19385 -4.1630683 -4.1610608 -4.1796565 -4.1954174 -4.2050958 -4.2226667 -4.2362318 -4.229939 -4.2120509][-4.2584791 -4.2498603 -4.2395096 -4.2269692 -4.1952295 -4.1456194 -4.1013718 -4.1023741 -4.1361656 -4.1698508 -4.1960564 -4.2216663 -4.2316546 -4.2148237 -4.1860528][-4.2486725 -4.2431636 -4.2318244 -4.213829 -4.1708689 -4.1061363 -4.0474091 -4.0494156 -4.1022635 -4.1566992 -4.1983757 -4.227891 -4.2350802 -4.2103066 -4.1699605][-4.2471881 -4.2439141 -4.2246017 -4.1914163 -4.1332035 -4.0535121 -3.9803579 -3.979531 -4.0534949 -4.1296282 -4.1836829 -4.2158308 -4.2212133 -4.1915827 -4.1451106][-4.2524462 -4.2450037 -4.2135143 -4.1635761 -4.0856276 -3.9878657 -3.8968444 -3.8842397 -3.9796448 -4.0832367 -4.1539764 -4.1948843 -4.2054663 -4.1753125 -4.1324153][-4.2548313 -4.2386031 -4.2001481 -4.1434932 -4.0549316 -3.9365277 -3.8142357 -3.7764788 -3.89304 -4.029882 -4.1194329 -4.1694784 -4.1825042 -4.1569848 -4.1248155][-4.2397943 -4.2160492 -4.1753292 -4.1217709 -4.0444751 -3.9279218 -3.7903297 -3.7302058 -3.847064 -3.994719 -4.0904012 -4.1365666 -4.1484604 -4.1274195 -4.1029611][-4.2121654 -4.1847339 -4.1470776 -4.1064582 -4.0605826 -3.984412 -3.8925512 -3.850812 -3.9193215 -4.0179329 -4.087925 -4.1177034 -4.1173081 -4.096148 -4.0820737][-4.191401 -4.1664457 -4.1338363 -4.1123996 -4.1002164 -4.0679965 -4.0196471 -3.9971364 -4.0301189 -4.0789337 -4.1179371 -4.1292105 -4.1163 -4.08829 -4.076086][-4.17132 -4.1492715 -4.1287036 -4.1215229 -4.12538 -4.1178885 -4.0965185 -4.086112 -4.1028428 -4.1263275 -4.1429667 -4.1427069 -4.1236696 -4.0884414 -4.0767341][-4.1483431 -4.1356359 -4.12744 -4.1298757 -4.1431441 -4.1501412 -4.1446953 -4.1418028 -4.1548886 -4.1693158 -4.1781869 -4.1758351 -4.15986 -4.1249113 -4.1112237][-4.146338 -4.1454077 -4.1451249 -4.1511469 -4.1662354 -4.1777754 -4.1790857 -4.1822162 -4.1983032 -4.2140117 -4.223217 -4.2237968 -4.2126932 -4.1851883 -4.1682291][-4.1637793 -4.1722765 -4.1796722 -4.1857548 -4.1988516 -4.2105722 -4.2142658 -4.2214646 -4.2396741 -4.2562866 -4.2682605 -4.2744732 -4.2667179 -4.2471504 -4.2280531][-4.2052 -4.2145209 -4.2241106 -4.2295618 -4.2365966 -4.2452788 -4.251328 -4.2589207 -4.2756548 -4.2911034 -4.3046231 -4.3136497 -4.309536 -4.2943025 -4.2779241]]...]
INFO - root - 2017-12-05 12:14:08.931070: step 7510, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 76h:55m:11s remains)
INFO - root - 2017-12-05 12:14:17.481999: step 7520, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 76h:06m:18s remains)
INFO - root - 2017-12-05 12:14:25.739051: step 7530, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 75h:07m:56s remains)
INFO - root - 2017-12-05 12:14:34.356850: step 7540, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 78h:34m:10s remains)
INFO - root - 2017-12-05 12:14:42.922812: step 7550, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 76h:18m:05s remains)
INFO - root - 2017-12-05 12:14:51.354668: step 7560, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 77h:22m:53s remains)
INFO - root - 2017-12-05 12:14:59.807871: step 7570, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 76h:28m:56s remains)
INFO - root - 2017-12-05 12:15:08.281282: step 7580, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 78h:00m:05s remains)
INFO - root - 2017-12-05 12:15:16.788372: step 7590, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 80h:50m:29s remains)
INFO - root - 2017-12-05 12:15:25.239924: step 7600, loss = 2.05, batch loss = 1.99 (10.4 examples/sec; 0.772 sec/batch; 69h:39m:30s remains)
2017-12-05 12:15:25.999239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.309536 -4.2637272 -4.2139163 -4.1790996 -4.1547828 -4.165854 -4.1996489 -4.2292581 -4.2502832 -4.26174 -4.2701092 -4.2714567 -4.2647038 -4.2486324 -4.2165461][-4.3025122 -4.24914 -4.1908312 -4.1532259 -4.1249919 -4.1346221 -4.1697989 -4.197361 -4.2163606 -4.2309623 -4.2482333 -4.258626 -4.2575603 -4.2426863 -4.2054257][-4.2942591 -4.23928 -4.1793046 -4.1360412 -4.0963206 -4.0963035 -4.1283016 -4.1495533 -4.1668282 -4.1896043 -4.2212443 -4.2435617 -4.2470393 -4.233201 -4.1930747][-4.2969093 -4.2515473 -4.2030935 -4.1553826 -4.09692 -4.0731544 -4.0931168 -4.1088758 -4.1223741 -4.1490126 -4.19045 -4.2242718 -4.2347069 -4.2248092 -4.1854405][-4.2992492 -4.2700658 -4.2378726 -4.1937475 -4.1248569 -4.0846233 -4.0933475 -4.1008797 -4.1057258 -4.1271129 -4.1713495 -4.2094569 -4.2254052 -4.2211652 -4.1873][-4.2958555 -4.2759566 -4.25357 -4.2104006 -4.1390781 -4.0974684 -4.1080647 -4.1197119 -4.1176882 -4.1229596 -4.1556468 -4.1907473 -4.2090278 -4.2154326 -4.1963611][-4.2852054 -4.2629437 -4.2390637 -4.1916361 -4.118207 -4.0830078 -4.1030383 -4.1200862 -4.1146021 -4.0967541 -4.1134863 -4.1535082 -4.1864591 -4.2112575 -4.2158408][-4.2760577 -4.2457857 -4.2131815 -4.1531267 -4.0676556 -4.0334096 -4.0609436 -4.0809813 -4.0677929 -4.026813 -4.0374589 -4.097291 -4.154551 -4.199883 -4.2261753][-4.277091 -4.2410512 -4.19819 -4.1226597 -4.0198574 -3.9766059 -4.0021224 -4.0224342 -4.0003886 -3.9477296 -3.9683819 -4.0509505 -4.1283045 -4.1873875 -4.2293625][-4.2800369 -4.2432551 -4.1962953 -4.1208577 -4.0194693 -3.975414 -4.0011029 -4.0291572 -4.0157361 -3.9668789 -3.9834633 -4.060091 -4.136003 -4.19605 -4.2401204][-4.285553 -4.2535043 -4.2103992 -4.1480913 -4.068984 -4.0358591 -4.0637183 -4.0980206 -4.0978923 -4.0652122 -4.0731363 -4.1232138 -4.1784997 -4.226387 -4.2615318][-4.2974181 -4.2719378 -4.234889 -4.189425 -4.135622 -4.1171436 -4.1460094 -4.1843319 -4.1966467 -4.1811042 -4.1830688 -4.2091341 -4.2428555 -4.2747474 -4.2972465][-4.3071823 -4.2864976 -4.2541404 -4.218245 -4.1821384 -4.1751928 -4.2066011 -4.2437716 -4.2623897 -4.257452 -4.2588263 -4.2728786 -4.2927294 -4.3106585 -4.3228517][-4.3118625 -4.2942572 -4.2641521 -4.2299695 -4.1987386 -4.1966314 -4.2294645 -4.2659941 -4.2872152 -4.2891388 -4.291297 -4.300879 -4.3151374 -4.3279667 -4.3349557][-4.3158164 -4.2975068 -4.2646356 -4.2283864 -4.1996703 -4.2043691 -4.2400618 -4.2765913 -4.2970815 -4.3009753 -4.303535 -4.3111887 -4.3215194 -4.3296337 -4.3325915]]...]
INFO - root - 2017-12-05 12:15:34.687206: step 7610, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 77h:11m:26s remains)
INFO - root - 2017-12-05 12:15:43.314514: step 7620, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 88h:12m:56s remains)
INFO - root - 2017-12-05 12:15:51.859073: step 7630, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 74h:44m:57s remains)
INFO - root - 2017-12-05 12:16:00.335091: step 7640, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 78h:11m:08s remains)
INFO - root - 2017-12-05 12:16:08.836034: step 7650, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 76h:01m:24s remains)
INFO - root - 2017-12-05 12:16:17.447243: step 7660, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 76h:46m:20s remains)
INFO - root - 2017-12-05 12:16:26.056690: step 7670, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.836 sec/batch; 75h:27m:35s remains)
INFO - root - 2017-12-05 12:16:34.780497: step 7680, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 77h:01m:17s remains)
INFO - root - 2017-12-05 12:16:43.325351: step 7690, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 78h:05m:32s remains)
INFO - root - 2017-12-05 12:16:51.861006: step 7700, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 79h:17m:34s remains)
2017-12-05 12:16:52.715377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1976643 -4.2145972 -4.2266579 -4.2188191 -4.2067451 -4.2021136 -4.2099991 -4.2293949 -4.2529154 -4.2771783 -4.2955127 -4.2970304 -4.2854719 -4.2669253 -4.2479267][-4.128787 -4.1569443 -4.1792622 -4.1748376 -4.1655016 -4.1634951 -4.1718726 -4.19302 -4.2176161 -4.2458138 -4.2654176 -4.2669334 -4.2558928 -4.2371874 -4.2146978][-4.0602288 -4.10302 -4.13906 -4.1435928 -4.1403027 -4.142045 -4.1490164 -4.1675634 -4.1932745 -4.2228751 -4.2421656 -4.2440619 -4.2368083 -4.2217007 -4.1980782][-4.0543127 -4.1035805 -4.1413331 -4.1490831 -4.1521044 -4.1517034 -4.1516671 -4.1570063 -4.1734319 -4.1981692 -4.21557 -4.2206755 -4.22283 -4.2171278 -4.2017131][-4.1210985 -4.1612482 -4.1848564 -4.1918921 -4.1974297 -4.1874909 -4.168107 -4.1499333 -4.1480961 -4.1694584 -4.1880889 -4.196981 -4.2069864 -4.215652 -4.2173095][-4.1824055 -4.209024 -4.2136106 -4.2083158 -4.2039523 -4.1802721 -4.1427107 -4.1096659 -4.1031604 -4.1237693 -4.1400237 -4.1477332 -4.1648426 -4.185967 -4.202106][-4.2070723 -4.2168579 -4.1987553 -4.1742859 -4.1492 -4.1077261 -4.0605307 -4.0264206 -4.028265 -4.05697 -4.0756459 -4.0839972 -4.1047993 -4.1256433 -4.1494565][-4.2194209 -4.2193832 -4.1882734 -4.1477022 -4.1030426 -4.0440545 -3.9967306 -3.97045 -3.9827766 -4.021697 -4.0428157 -4.0478373 -4.0613174 -4.0778146 -4.1105828][-4.2241926 -4.2209926 -4.1887188 -4.1397681 -4.0858226 -4.0222721 -3.9808772 -3.9680324 -3.9859157 -4.0236006 -4.0476317 -4.0536518 -4.0627542 -4.0789866 -4.1232533][-4.2269135 -4.2236094 -4.1930709 -4.1478214 -4.1007924 -4.0513492 -4.0204358 -4.0189624 -4.042913 -4.08261 -4.1132011 -4.1274571 -4.1359057 -4.1527276 -4.1928968][-4.2278361 -4.2252669 -4.2029324 -4.1727095 -4.1472225 -4.1238961 -4.1122794 -4.1236424 -4.153389 -4.1914458 -4.2200766 -4.2346573 -4.2384605 -4.2462516 -4.2676153][-4.2314005 -4.2335105 -4.2242675 -4.2147965 -4.2124715 -4.2123885 -4.2166052 -4.2310505 -4.2545176 -4.2834539 -4.3016343 -4.3069983 -4.3045473 -4.3047624 -4.3119774][-4.2208037 -4.2314692 -4.2395759 -4.2528496 -4.2683792 -4.2786813 -4.2875361 -4.2949185 -4.3058968 -4.3197694 -4.3259687 -4.3257489 -4.3237677 -4.3240695 -4.3258643][-4.1831865 -4.2012434 -4.2284613 -4.2624922 -4.2923346 -4.3072724 -4.3136721 -4.3147674 -4.3187327 -4.3238392 -4.324028 -4.3230863 -4.3225565 -4.3225489 -4.3214989][-4.1373925 -4.1561427 -4.1921482 -4.2388649 -4.2792935 -4.3000417 -4.3078938 -4.3091397 -4.3112831 -4.31363 -4.3135295 -4.3128371 -4.3124156 -4.3115282 -4.3102741]]...]
INFO - root - 2017-12-05 12:17:01.120591: step 7710, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 80h:40m:42s remains)
INFO - root - 2017-12-05 12:17:09.575901: step 7720, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 77h:30m:11s remains)
INFO - root - 2017-12-05 12:17:18.145106: step 7730, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 77h:40m:50s remains)
INFO - root - 2017-12-05 12:17:26.740664: step 7740, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 77h:57m:42s remains)
INFO - root - 2017-12-05 12:17:35.288404: step 7750, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 78h:57m:18s remains)
INFO - root - 2017-12-05 12:17:43.783054: step 7760, loss = 2.09, batch loss = 2.04 (9.7 examples/sec; 0.828 sec/batch; 74h:42m:30s remains)
INFO - root - 2017-12-05 12:17:52.167789: step 7770, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 77h:05m:55s remains)
INFO - root - 2017-12-05 12:18:00.727222: step 7780, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.826 sec/batch; 74h:29m:33s remains)
INFO - root - 2017-12-05 12:18:09.265008: step 7790, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.847 sec/batch; 76h:24m:46s remains)
INFO - root - 2017-12-05 12:18:17.687476: step 7800, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 77h:50m:43s remains)
2017-12-05 12:18:18.454248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2358685 -4.2294965 -4.2241216 -4.2185912 -4.2139153 -4.2167783 -4.2218618 -4.2224383 -4.2243037 -4.22672 -4.2293653 -4.2379656 -4.2519403 -4.260541 -4.2561111][-4.2059169 -4.1975427 -4.1965714 -4.1937776 -4.1875529 -4.1871028 -4.1906762 -4.1922293 -4.1959772 -4.1979818 -4.1996546 -4.206727 -4.2218523 -4.233942 -4.2343268][-4.1724324 -4.1666956 -4.1699309 -4.1667905 -4.1579514 -4.1516294 -4.1460547 -4.1456985 -4.1552558 -4.1622314 -4.1672778 -4.1762109 -4.19126 -4.2057052 -4.21226][-4.1595922 -4.1519041 -4.14888 -4.1416135 -4.1330662 -4.1197386 -4.1023107 -4.0945668 -4.1129622 -4.13285 -4.1474428 -4.1602368 -4.1786695 -4.197073 -4.2068405][-4.1570024 -4.143105 -4.1338949 -4.1280975 -4.1216955 -4.1010513 -4.0657415 -4.0444927 -4.0704784 -4.1049438 -4.1308484 -4.150919 -4.1754327 -4.1990862 -4.2127481][-4.1385803 -4.1199331 -4.1072235 -4.1018333 -4.0899053 -4.0559306 -3.9942169 -3.9537103 -3.9982486 -4.06276 -4.1044903 -4.1320572 -4.1583433 -4.1875792 -4.2101569][-4.1232982 -4.1045151 -4.0836287 -4.0658851 -4.0387259 -3.9809675 -3.8775387 -3.7948027 -3.8638725 -3.9778409 -4.0530014 -4.1002221 -4.1330891 -4.1691628 -4.1999106][-4.1437078 -4.1313324 -4.0994649 -4.061892 -4.0117574 -3.9365618 -3.8051841 -3.6815627 -3.7620444 -3.9120159 -4.0122614 -4.07163 -4.1125951 -4.1521707 -4.1858063][-4.1932697 -4.1884637 -4.1583486 -4.1213679 -4.0743971 -4.016469 -3.9235678 -3.8319221 -3.8761621 -3.9863148 -4.0675993 -4.1139994 -4.145227 -4.1713543 -4.1929455][-4.228847 -4.2279396 -4.2016554 -4.1720414 -4.1355519 -4.0928483 -4.0346813 -3.9795461 -4.0066218 -4.0809212 -4.1386628 -4.1713872 -4.1924996 -4.2064505 -4.2175221][-4.2512679 -4.25338 -4.2322226 -4.209681 -4.177959 -4.138505 -4.0936627 -4.0566349 -4.0740995 -4.12582 -4.1713567 -4.2010441 -4.2221751 -4.2346706 -4.2414937][-4.2634182 -4.2721128 -4.2596297 -4.2430935 -4.214396 -4.180316 -4.1444573 -4.1202536 -4.1320066 -4.1667552 -4.1984124 -4.2200637 -4.2366576 -4.2472887 -4.2515392][-4.2698159 -4.2874532 -4.2858658 -4.2709708 -4.2435942 -4.2179623 -4.1981878 -4.192596 -4.2059503 -4.2286763 -4.2465787 -4.2582822 -4.2685456 -4.2743 -4.2736373][-4.2706776 -4.2921371 -4.2988977 -4.2897649 -4.2694912 -4.2522955 -4.2440596 -4.2488465 -4.2606773 -4.273953 -4.2847161 -4.2931247 -4.2998104 -4.303575 -4.3034124][-4.2722917 -4.2953219 -4.30441 -4.2999825 -4.2879453 -4.2785573 -4.2770791 -4.2826829 -4.2887235 -4.294662 -4.3012681 -4.3076611 -4.3124909 -4.3160586 -4.3173513]]...]
INFO - root - 2017-12-05 12:18:27.081881: step 7810, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 75h:33m:58s remains)
INFO - root - 2017-12-05 12:18:35.594444: step 7820, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 78h:29m:13s remains)
INFO - root - 2017-12-05 12:18:44.062587: step 7830, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 75h:37m:05s remains)
INFO - root - 2017-12-05 12:18:52.609399: step 7840, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 78h:11m:24s remains)
INFO - root - 2017-12-05 12:19:01.167934: step 7850, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 79h:10m:56s remains)
INFO - root - 2017-12-05 12:19:09.582352: step 7860, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.837 sec/batch; 75h:26m:13s remains)
INFO - root - 2017-12-05 12:19:18.050677: step 7870, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 76h:46m:10s remains)
INFO - root - 2017-12-05 12:19:26.420532: step 7880, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 75h:42m:31s remains)
INFO - root - 2017-12-05 12:19:34.895157: step 7890, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 75h:03m:03s remains)
INFO - root - 2017-12-05 12:19:43.465965: step 7900, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 77h:38m:25s remains)
2017-12-05 12:19:44.185533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2145691 -4.2241254 -4.2420344 -4.2569389 -4.2622566 -4.2581291 -4.24846 -4.2431793 -4.2481523 -4.2615285 -4.2790313 -4.2911534 -4.2932487 -4.2898936 -4.2868752][-4.1626649 -4.1796479 -4.2072487 -4.226088 -4.2279782 -4.2164421 -4.2015553 -4.1970878 -4.207777 -4.2294226 -4.2535572 -4.2680387 -4.2697654 -4.2655678 -4.2620745][-4.1399646 -4.16301 -4.1959853 -4.215519 -4.2112021 -4.19197 -4.1720667 -4.1680546 -4.1824174 -4.2093816 -4.2384887 -4.2558146 -4.2577519 -4.2521372 -4.2487149][-4.1371379 -4.1620636 -4.1929421 -4.2081666 -4.198842 -4.174509 -4.1523461 -4.147501 -4.1626863 -4.1935334 -4.2274551 -4.2492018 -4.2534676 -4.2498279 -4.2473407][-4.1490092 -4.1740355 -4.1968417 -4.2035518 -4.1865864 -4.1579189 -4.1350245 -4.1274757 -4.1394024 -4.1694155 -4.2055826 -4.23177 -4.2432261 -4.2479787 -4.2519374][-4.1792469 -4.201787 -4.2142982 -4.2103262 -4.1883483 -4.1597452 -4.137702 -4.1293421 -4.1397963 -4.168931 -4.2023993 -4.2284184 -4.2452068 -4.2560549 -4.2657986][-4.2005754 -4.2182236 -4.2199869 -4.21012 -4.1883626 -4.1651106 -4.1493974 -4.1427536 -4.1529617 -4.182137 -4.2143221 -4.2388763 -4.255528 -4.2650752 -4.2704458][-4.2067966 -4.2211943 -4.2180662 -4.2091451 -4.1939406 -4.1799741 -4.1732802 -4.1676707 -4.17553 -4.2002811 -4.2268562 -4.2453442 -4.2575254 -4.2630692 -4.2610092][-4.1970329 -4.211854 -4.2117062 -4.2117825 -4.2077384 -4.2023158 -4.1998358 -4.1932359 -4.1940541 -4.2062635 -4.22057 -4.2285609 -4.2357459 -4.24139 -4.2386427][-4.1826315 -4.2007508 -4.2094774 -4.2210426 -4.22432 -4.2209067 -4.2182269 -4.211071 -4.2034826 -4.2005424 -4.1997948 -4.1935716 -4.1932735 -4.2002563 -4.2079329][-4.1712952 -4.192636 -4.2077889 -4.2255158 -4.231041 -4.2248793 -4.2193503 -4.2106133 -4.1975694 -4.1861544 -4.1769419 -4.1620111 -4.1564918 -4.166306 -4.1860843][-4.161757 -4.18534 -4.2047606 -4.2261128 -4.23259 -4.2233529 -4.2114916 -4.1990237 -4.18058 -4.1663013 -4.15961 -4.1480308 -4.1449261 -4.1560941 -4.181879][-4.1542916 -4.1802669 -4.2028403 -4.2261076 -4.2327428 -4.2199068 -4.2008309 -4.1811814 -4.1625819 -4.1532583 -4.1568308 -4.15724 -4.1572242 -4.1674852 -4.1945558][-4.1652007 -4.1916852 -4.2128057 -4.2317452 -4.2348537 -4.2203388 -4.1985326 -4.1743436 -4.1577139 -4.1558533 -4.1664062 -4.1749725 -4.1780424 -4.1875987 -4.2142339][-4.1973433 -4.2202911 -4.2354388 -4.2470927 -4.2463441 -4.2348714 -4.2163491 -4.1949859 -4.1832724 -4.1870966 -4.1990123 -4.2050147 -4.2064304 -4.2149806 -4.2382288]]...]
INFO - root - 2017-12-05 12:19:52.694422: step 7910, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 78h:28m:03s remains)
INFO - root - 2017-12-05 12:20:01.235268: step 7920, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.776 sec/batch; 69h:58m:42s remains)
INFO - root - 2017-12-05 12:20:09.892141: step 7930, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 77h:04m:58s remains)
INFO - root - 2017-12-05 12:20:18.427290: step 7940, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 75h:49m:19s remains)
INFO - root - 2017-12-05 12:20:26.880167: step 7950, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 75h:43m:14s remains)
INFO - root - 2017-12-05 12:20:35.429609: step 7960, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 76h:08m:23s remains)
INFO - root - 2017-12-05 12:20:43.870593: step 7970, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 74h:58m:33s remains)
INFO - root - 2017-12-05 12:20:52.369750: step 7980, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 74h:51m:50s remains)
INFO - root - 2017-12-05 12:21:00.940814: step 7990, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 77h:18m:50s remains)
INFO - root - 2017-12-05 12:21:09.364268: step 8000, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 75h:42m:52s remains)
2017-12-05 12:21:10.094157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0812564 -4.1054544 -4.1221409 -4.1005445 -4.0821629 -4.0632086 -4.0383897 -4.0245686 -4.0511012 -4.1011257 -4.1429448 -4.1959071 -4.2530308 -4.2884378 -4.3042426][-4.0746064 -4.0938773 -4.1057115 -4.091424 -4.0790539 -4.0618019 -4.0334888 -4.0209394 -4.0445495 -4.0956726 -4.1453342 -4.1989527 -4.2564383 -4.2946968 -4.3101072][-4.069387 -4.0892611 -4.0929303 -4.075233 -4.0605078 -4.0437117 -4.0111761 -4.0003681 -4.0297432 -4.079309 -4.1342878 -4.189301 -4.2492628 -4.2904425 -4.3079486][-4.0657344 -4.0859675 -4.0816455 -4.0532985 -4.0427828 -4.0316563 -3.9960558 -3.987325 -4.02643 -4.0777578 -4.1306024 -4.185976 -4.246079 -4.2861457 -4.3045726][-4.0599542 -4.0746708 -4.054574 -4.0135789 -4.0154386 -4.0091381 -3.958647 -3.9535031 -4.0117173 -4.0772934 -4.1312094 -4.1895356 -4.2490792 -4.2847815 -4.3023367][-4.0409193 -4.0366621 -4.0013089 -3.95421 -3.9682734 -3.9539444 -3.8643589 -3.8572013 -3.9549189 -4.0490875 -4.1163139 -4.1872296 -4.2489753 -4.2838526 -4.3016791][-4.0298643 -4.0145965 -3.9834318 -3.9405611 -3.9423475 -3.8903792 -3.7359133 -3.7149134 -3.8667536 -3.9916306 -4.0761566 -4.1690516 -4.2422194 -4.2822256 -4.3031459][-4.0620532 -4.0546956 -4.0529356 -4.0245161 -3.996464 -3.9143171 -3.7359164 -3.7008424 -3.855546 -3.9792094 -4.0595226 -4.1604671 -4.242383 -4.2845345 -4.3051162][-4.1097693 -4.1185255 -4.1431603 -4.1239247 -4.0895 -4.0216756 -3.8973496 -3.8681586 -3.9559526 -4.0401335 -4.0970473 -4.1824765 -4.2560449 -4.292294 -4.308753][-4.1361904 -4.1549473 -4.1908212 -4.1782465 -4.1532536 -4.1065965 -4.0316353 -4.0117679 -4.0511818 -4.1016273 -4.1417093 -4.2067194 -4.2656689 -4.29427 -4.3082027][-4.1508055 -4.1745963 -4.2110004 -4.2023973 -4.1833878 -4.1526041 -4.1042647 -4.0856466 -4.1051474 -4.1413116 -4.1752067 -4.2295437 -4.2763429 -4.2969713 -4.3073215][-4.1643896 -4.1888285 -4.2224011 -4.2235432 -4.2088861 -4.1838136 -4.1468153 -4.1330123 -4.14702 -4.1757178 -4.2058053 -4.250299 -4.2866569 -4.2988238 -4.3065505][-4.1833949 -4.2006874 -4.228034 -4.2344236 -4.2261152 -4.2075138 -4.1822147 -4.1739149 -4.1857495 -4.2087512 -4.2323251 -4.2664018 -4.2930384 -4.299521 -4.3062878][-4.2142859 -4.2222977 -4.23858 -4.240334 -4.2319183 -4.2170639 -4.2019153 -4.1982865 -4.2092857 -4.2287779 -4.2492061 -4.2756886 -4.2986865 -4.3045025 -4.3107796][-4.2586255 -4.2630229 -4.2689037 -4.2651386 -4.2541933 -4.23986 -4.2265372 -4.2216287 -4.2270246 -4.2388597 -4.2536726 -4.274374 -4.296937 -4.3071017 -4.3150921]]...]
INFO - root - 2017-12-05 12:21:18.715958: step 8010, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 77h:23m:16s remains)
INFO - root - 2017-12-05 12:21:27.287885: step 8020, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 77h:48m:28s remains)
INFO - root - 2017-12-05 12:21:35.649268: step 8030, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.814 sec/batch; 73h:23m:13s remains)
INFO - root - 2017-12-05 12:21:44.151181: step 8040, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 78h:56m:55s remains)
INFO - root - 2017-12-05 12:21:52.710823: step 8050, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 77h:27m:16s remains)
INFO - root - 2017-12-05 12:22:01.275983: step 8060, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 77h:14m:42s remains)
INFO - root - 2017-12-05 12:22:09.753390: step 8070, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 75h:44m:25s remains)
INFO - root - 2017-12-05 12:22:18.300894: step 8080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 77h:00m:30s remains)
INFO - root - 2017-12-05 12:22:26.877770: step 8090, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 77h:06m:50s remains)
INFO - root - 2017-12-05 12:22:35.365868: step 8100, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 77h:24m:05s remains)
2017-12-05 12:22:36.115115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1687827 -4.1784487 -4.17291 -4.1788793 -4.1849084 -4.180553 -4.1758718 -4.1691432 -4.1598678 -4.1477838 -4.1300888 -4.1080532 -4.0895076 -4.1023283 -4.1372724][-4.171423 -4.1889539 -4.1890016 -4.1950941 -4.1952658 -4.183599 -4.1759062 -4.1710086 -4.1646042 -4.1580362 -4.1405954 -4.1116357 -4.0877848 -4.0993757 -4.1367764][-4.1839952 -4.2063608 -4.2132063 -4.2161932 -4.2112103 -4.1949039 -4.1798835 -4.173697 -4.1752057 -4.1781812 -4.1697974 -4.1362514 -4.1078105 -4.1147552 -4.1459513][-4.1982059 -4.2203932 -4.2305965 -4.2329764 -4.2247448 -4.2071767 -4.1859035 -4.1800437 -4.1846972 -4.1923528 -4.1910305 -4.1613221 -4.1319013 -4.1314 -4.1500158][-4.204987 -4.2259607 -4.2384777 -4.2399187 -4.2271633 -4.20738 -4.182179 -4.1718364 -4.1743436 -4.1822844 -4.1817284 -4.160975 -4.143126 -4.1426263 -4.1552467][-4.2059684 -4.2274432 -4.2381864 -4.2348862 -4.2175946 -4.18797 -4.1551986 -4.1406403 -4.1409435 -4.15056 -4.1529665 -4.1423707 -4.141643 -4.1449432 -4.1579976][-4.2042093 -4.2230892 -4.2295814 -4.2214108 -4.1973805 -4.1576195 -4.118381 -4.1032948 -4.1027651 -4.1140585 -4.1215892 -4.1185884 -4.1299744 -4.1387358 -4.1576333][-4.1921988 -4.2069354 -4.2107234 -4.1982269 -4.168612 -4.1227436 -4.0809212 -4.0704117 -4.0750117 -4.089982 -4.1017208 -4.105186 -4.1170344 -4.1261706 -4.1461039][-4.1750722 -4.18862 -4.1910806 -4.175621 -4.1416025 -4.0905714 -4.051692 -4.0514889 -4.0707903 -4.094285 -4.1073503 -4.11228 -4.1120071 -4.1088128 -4.1199536][-4.1646948 -4.1826954 -4.1845016 -4.1663337 -4.1283236 -4.0772085 -4.044661 -4.0534053 -4.0816503 -4.1068506 -4.120142 -4.1192451 -4.1069765 -4.0916438 -4.0915885][-4.1616273 -4.1838379 -4.1886725 -4.1710582 -4.1297364 -4.0781755 -4.0478067 -4.0569649 -4.0876632 -4.1156754 -4.1363358 -4.1341944 -4.1163707 -4.0892053 -4.0742879][-4.1553988 -4.1788125 -4.1900706 -4.1776676 -4.1409268 -4.0901217 -4.0562406 -4.0621033 -4.0952907 -4.1317229 -4.1647458 -4.1731286 -4.1583748 -4.1243982 -4.0908756][-4.1534495 -4.1761174 -4.1920571 -4.1858196 -4.1563396 -4.1081753 -4.0718269 -4.071156 -4.1039453 -4.1425657 -4.1824107 -4.2003818 -4.1930466 -4.1592221 -4.1169786][-4.1584597 -4.1794615 -4.1973662 -4.1974568 -4.171411 -4.1237049 -4.083344 -4.0733266 -4.1011357 -4.1359482 -4.1724792 -4.1960673 -4.1953015 -4.1692982 -4.1277866][-4.1586552 -4.1823225 -4.2047615 -4.2136574 -4.196465 -4.1539874 -4.1119828 -4.0883641 -4.1019263 -4.1273575 -4.1590443 -4.1824584 -4.1822653 -4.1632175 -4.126833]]...]
INFO - root - 2017-12-05 12:22:44.681176: step 8110, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.833 sec/batch; 75h:04m:02s remains)
INFO - root - 2017-12-05 12:22:53.217482: step 8120, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 75h:50m:11s remains)
INFO - root - 2017-12-05 12:23:01.816397: step 8130, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 77h:43m:52s remains)
INFO - root - 2017-12-05 12:23:10.270735: step 8140, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.821 sec/batch; 73h:59m:21s remains)
INFO - root - 2017-12-05 12:23:18.783768: step 8150, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 77h:38m:08s remains)
INFO - root - 2017-12-05 12:23:27.295892: step 8160, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 74h:39m:43s remains)
INFO - root - 2017-12-05 12:23:35.712207: step 8170, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 74h:32m:03s remains)
INFO - root - 2017-12-05 12:23:44.198787: step 8180, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 77h:31m:03s remains)
INFO - root - 2017-12-05 12:23:52.659113: step 8190, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 76h:19m:52s remains)
INFO - root - 2017-12-05 12:24:00.974786: step 8200, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.815 sec/batch; 73h:24m:07s remains)
2017-12-05 12:24:01.767873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2766542 -4.2342629 -4.1724725 -4.1132088 -4.0846615 -4.098073 -4.1378493 -4.17358 -4.1914797 -4.206655 -4.2312131 -4.2638526 -4.2961874 -4.3150282 -4.3247833][-4.2589641 -4.2166548 -4.15559 -4.0953288 -4.06468 -4.0748982 -4.1111403 -4.1426272 -4.1572952 -4.1735544 -4.205255 -4.2445841 -4.2843814 -4.3105326 -4.324398][-4.237361 -4.204525 -4.1606164 -4.1133542 -4.0912061 -4.0991821 -4.124001 -4.1391416 -4.139462 -4.1518841 -4.1831713 -4.2221127 -4.264986 -4.2974129 -4.3174672][-4.208127 -4.1872764 -4.1627669 -4.1319761 -4.1193233 -4.1255608 -4.1350422 -4.1290827 -4.1150908 -4.1289554 -4.1643882 -4.2075434 -4.2549567 -4.2907834 -4.3126225][-4.1725907 -4.1587005 -4.1437945 -4.1259189 -4.1198645 -4.11965 -4.1129761 -4.0905547 -4.0728912 -4.0964727 -4.1377306 -4.1872826 -4.243638 -4.2874112 -4.3117409][-4.1422634 -4.1223249 -4.1121321 -4.1075482 -4.1079769 -4.0988903 -4.0827765 -4.05276 -4.0320053 -4.0522184 -4.0912466 -4.1465559 -4.2165971 -4.2737761 -4.3070273][-4.1084909 -4.0775213 -4.0593081 -4.0618792 -4.076 -4.0806322 -4.0700555 -4.0362082 -4.004777 -4.010438 -4.0465913 -4.1083736 -4.1856694 -4.254787 -4.3000293][-4.0737233 -4.0387468 -4.0136395 -4.0209856 -4.0531683 -4.0850549 -4.0828581 -4.0440149 -3.9988697 -3.9909477 -4.0221577 -4.0798159 -4.1533465 -4.2312489 -4.2893438][-4.0616684 -4.0311785 -4.0064106 -4.0156326 -4.0607944 -4.1114025 -4.1138549 -4.066782 -4.0033526 -3.9816658 -4.0101857 -4.0663805 -4.1359048 -4.2162123 -4.280232][-4.0929341 -4.0707207 -4.0452514 -4.0494947 -4.0933967 -4.1454535 -4.1446638 -4.0900922 -4.0154409 -3.9856136 -4.0162683 -4.0718317 -4.1365509 -4.2133436 -4.2751164][-4.1454926 -4.1356945 -4.1136718 -4.1086235 -4.1370239 -4.1756511 -4.1684508 -4.1172457 -4.050128 -4.0201292 -4.0438032 -4.0929275 -4.1479468 -4.2143416 -4.2707548][-4.1989131 -4.2003393 -4.1850328 -4.172442 -4.179378 -4.1960368 -4.1895332 -4.1555505 -4.1117821 -4.0868726 -4.0978165 -4.1341796 -4.1758342 -4.2253013 -4.2725024][-4.2475085 -4.2528992 -4.2502532 -4.2429843 -4.236537 -4.23441 -4.2299032 -4.2149558 -4.1921792 -4.1714106 -4.1684284 -4.1893964 -4.2164111 -4.2472868 -4.2798624][-4.281878 -4.28658 -4.2907596 -4.2903895 -4.2833238 -4.278326 -4.2777681 -4.27309 -4.2596607 -4.2399688 -4.2275791 -4.23451 -4.2480435 -4.2663841 -4.2888184][-4.2999835 -4.2991323 -4.3034859 -4.30764 -4.3074718 -4.3079677 -4.3103323 -4.3087897 -4.2979803 -4.2789841 -4.2644191 -4.26614 -4.2730389 -4.2834039 -4.2990046]]...]
INFO - root - 2017-12-05 12:24:10.327116: step 8210, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 77h:13m:59s remains)
INFO - root - 2017-12-05 12:24:18.846345: step 8220, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 76h:46m:25s remains)
INFO - root - 2017-12-05 12:24:27.436557: step 8230, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 75h:56m:29s remains)
INFO - root - 2017-12-05 12:24:35.918554: step 8240, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 0.793 sec/batch; 71h:24m:47s remains)
INFO - root - 2017-12-05 12:24:44.498941: step 8250, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 77h:40m:41s remains)
INFO - root - 2017-12-05 12:24:53.111740: step 8260, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 79h:01m:25s remains)
INFO - root - 2017-12-05 12:25:01.533093: step 8270, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 75h:42m:54s remains)
INFO - root - 2017-12-05 12:25:09.976563: step 8280, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 76h:30m:46s remains)
INFO - root - 2017-12-05 12:25:18.502494: step 8290, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 79h:51m:09s remains)
INFO - root - 2017-12-05 12:25:26.871490: step 8300, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 75h:23m:52s remains)
2017-12-05 12:25:27.613120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3357067 -4.3279405 -4.32317 -4.3210249 -4.3217454 -4.3230486 -4.3228784 -4.3216524 -4.32244 -4.3252068 -4.3290424 -4.333478 -4.3390589 -4.3464212 -4.3547258][-4.3210673 -4.3110514 -4.305357 -4.3028197 -4.3023896 -4.3018379 -4.3008037 -4.2996116 -4.3011484 -4.3041086 -4.3082576 -4.3134394 -4.3195577 -4.3286543 -4.3398309][-4.3035226 -4.2894263 -4.2804117 -4.2760892 -4.2720914 -4.2672367 -4.2634807 -4.2607989 -4.2622466 -4.2654991 -4.2694993 -4.2759728 -4.284658 -4.2967196 -4.3126078][-4.2832508 -4.2596793 -4.2426538 -4.2338085 -4.224997 -4.2134333 -4.2020197 -4.1935449 -4.1945314 -4.2011409 -4.2068911 -4.2161069 -4.2302351 -4.2477164 -4.2714648][-4.250905 -4.2134147 -4.1869826 -4.1735091 -4.1590719 -4.1384521 -4.113842 -4.0959034 -4.1030045 -4.1219149 -4.1327677 -4.1481338 -4.1699162 -4.1947432 -4.2287574][-4.2082429 -4.1604919 -4.1301289 -4.115541 -4.0919557 -4.0567894 -4.011879 -3.9823015 -4.0074878 -4.0501184 -4.0721822 -4.0940318 -4.1196961 -4.1488476 -4.1925483][-4.1776237 -4.1246452 -4.0914359 -4.0712953 -4.0297308 -3.9731646 -3.9055252 -3.8694181 -3.9263208 -4.000782 -4.0392814 -4.0597067 -4.0817437 -4.1149993 -4.1683445][-4.1701808 -4.1174626 -4.082346 -4.0529494 -3.9985337 -3.9331679 -3.8656943 -3.8437321 -3.9209604 -4.0120606 -4.0556178 -4.0692077 -4.0845079 -4.1175833 -4.1713123][-4.1915669 -4.1445537 -4.1101232 -4.0759816 -4.0233412 -3.9692869 -3.9264235 -3.9231277 -3.9909077 -4.07186 -4.1110015 -4.1227489 -4.135283 -4.1604037 -4.2004862][-4.2094197 -4.1676621 -4.1359711 -4.1070867 -4.0693831 -4.032186 -4.0100508 -4.0168786 -4.0647779 -4.1239867 -4.1578107 -4.1742554 -4.1862726 -4.201077 -4.2240305][-4.2246876 -4.1942143 -4.1731706 -4.1566463 -4.1367106 -4.1147766 -4.102994 -4.1077414 -4.1340966 -4.1723866 -4.2012258 -4.2184863 -4.2279949 -4.2356329 -4.2463951][-4.2549248 -4.2370872 -4.2272449 -4.2202568 -4.2114525 -4.1995835 -4.1923437 -4.1925578 -4.2038307 -4.2255177 -4.2461925 -4.2610984 -4.2709079 -4.2752862 -4.2799921][-4.2837677 -4.2740045 -4.2721496 -4.2729607 -4.274066 -4.27323 -4.2721429 -4.2718635 -4.2743497 -4.2811904 -4.28851 -4.2969837 -4.3044348 -4.3075147 -4.3100944][-4.3129134 -4.3085227 -4.3106251 -4.3144178 -4.3182058 -4.3224378 -4.3257375 -4.327198 -4.3276744 -4.3265114 -4.3244424 -4.3257866 -4.3301506 -4.333148 -4.3350611][-4.3408556 -4.3371396 -4.337997 -4.3397942 -4.3420858 -4.3459568 -4.3505116 -4.3541594 -4.3566046 -4.3549604 -4.3510494 -4.3489127 -4.3510957 -4.3548937 -4.355691]]...]
INFO - root - 2017-12-05 12:25:36.288655: step 8310, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 75h:59m:07s remains)
INFO - root - 2017-12-05 12:25:44.876807: step 8320, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 77h:22m:31s remains)
INFO - root - 2017-12-05 12:25:53.430843: step 8330, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 77h:41m:25s remains)
INFO - root - 2017-12-05 12:26:01.981077: step 8340, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 73h:52m:36s remains)
INFO - root - 2017-12-05 12:26:10.445967: step 8350, loss = 2.11, batch loss = 2.05 (9.2 examples/sec; 0.868 sec/batch; 78h:11m:14s remains)
INFO - root - 2017-12-05 12:26:18.959594: step 8360, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 75h:24m:33s remains)
INFO - root - 2017-12-05 12:26:27.460274: step 8370, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 76h:35m:40s remains)
INFO - root - 2017-12-05 12:26:35.858644: step 8380, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 76h:20m:57s remains)
INFO - root - 2017-12-05 12:26:44.352210: step 8390, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 75h:30m:02s remains)
INFO - root - 2017-12-05 12:26:52.751452: step 8400, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 0.798 sec/batch; 71h:51m:08s remains)
2017-12-05 12:26:53.491335: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3320265 -4.32719 -4.3197403 -4.3065286 -4.2807021 -4.2402329 -4.1940718 -4.16832 -4.1701012 -4.1955109 -4.2393222 -4.2831736 -4.3089442 -4.3116722 -4.3022933][-4.3414183 -4.3346248 -4.3293996 -4.325789 -4.3111186 -4.2776508 -4.2333426 -4.2008176 -4.1869307 -4.1943932 -4.2269769 -4.2705641 -4.3032718 -4.315639 -4.3116875][-4.3455596 -4.3385849 -4.336627 -4.3401856 -4.3336325 -4.308722 -4.2718706 -4.2369924 -4.2055216 -4.1887794 -4.20561 -4.2481074 -4.29123 -4.3177018 -4.3255243][-4.3466868 -4.3417721 -4.3421683 -4.3480263 -4.3449168 -4.324574 -4.2919526 -4.2523947 -4.2052917 -4.1676474 -4.1702533 -4.2140069 -4.2698593 -4.3123 -4.3357387][-4.3478322 -4.3462548 -4.347949 -4.351469 -4.345583 -4.3232918 -4.2886539 -4.239553 -4.178822 -4.1266403 -4.118176 -4.1672554 -4.2363415 -4.2938385 -4.3333654][-4.3486595 -4.3489513 -4.3495255 -4.3475871 -4.3345542 -4.3049378 -4.2638583 -4.204968 -4.1312523 -4.0625672 -4.0453238 -4.1069155 -4.1926241 -4.2647734 -4.319078][-4.3480196 -4.3475485 -4.3450403 -4.3352394 -4.3108373 -4.2722831 -4.2230644 -4.15537 -4.0660372 -3.9761944 -3.9517503 -4.0344763 -4.1421666 -4.2291942 -4.2958198][-4.3465724 -4.3440905 -4.3370161 -4.3172207 -4.2800474 -4.2318869 -4.1747737 -4.1001096 -3.9983337 -3.89162 -3.8677106 -3.9734302 -4.1002545 -4.1988716 -4.2742329][-4.3469186 -4.3414721 -4.3282604 -4.2991462 -4.2531004 -4.1990542 -4.1404724 -4.0686803 -3.9724727 -3.8772311 -3.86862 -3.9747889 -4.0965195 -4.1914291 -4.2642093][-4.3479276 -4.3398805 -4.322289 -4.2890282 -4.2409968 -4.1880364 -4.1345272 -4.075963 -4.0056605 -3.9473627 -3.9592838 -4.0441861 -4.1383433 -4.2142978 -4.2724619][-4.3480949 -4.3392038 -4.322032 -4.2927523 -4.252985 -4.2086906 -4.1652794 -4.1216865 -4.0782633 -4.0514097 -4.0719128 -4.1308551 -4.1970377 -4.2536063 -4.2949867][-4.3473291 -4.3394871 -4.3262072 -4.3068314 -4.2813797 -4.2507148 -4.2189016 -4.1886339 -4.1640992 -4.1554489 -4.1769066 -4.214952 -4.2590318 -4.2970724 -4.3214655][-4.3474021 -4.3423033 -4.33425 -4.3242226 -4.3111377 -4.2937737 -4.2742057 -4.255847 -4.2444019 -4.2470031 -4.266593 -4.2911105 -4.3158374 -4.3338752 -4.3402328][-4.3471193 -4.3439689 -4.3396668 -4.3355365 -4.330534 -4.3237367 -4.3149381 -4.3069758 -4.3045 -4.31143 -4.32529 -4.3371372 -4.3453503 -4.3456454 -4.3401608][-4.3445187 -4.3421807 -4.339951 -4.3387985 -4.3382916 -4.3386292 -4.3386164 -4.3384457 -4.3393393 -4.3430138 -4.3474841 -4.3476315 -4.343266 -4.3352113 -4.3277121]]...]
INFO - root - 2017-12-05 12:27:01.876336: step 8410, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 77h:34m:30s remains)
INFO - root - 2017-12-05 12:27:10.487040: step 8420, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 80h:15m:21s remains)
INFO - root - 2017-12-05 12:27:19.125565: step 8430, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 77h:45m:41s remains)
INFO - root - 2017-12-05 12:27:27.747796: step 8440, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 76h:19m:22s remains)
INFO - root - 2017-12-05 12:27:36.262133: step 8450, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.862 sec/batch; 77h:34m:41s remains)
INFO - root - 2017-12-05 12:27:44.618798: step 8460, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 76h:53m:05s remains)
INFO - root - 2017-12-05 12:27:53.119595: step 8470, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 75h:25m:33s remains)
INFO - root - 2017-12-05 12:28:01.601901: step 8480, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 77h:58m:06s remains)
INFO - root - 2017-12-05 12:28:10.155696: step 8490, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 74h:45m:54s remains)
INFO - root - 2017-12-05 12:28:18.530120: step 8500, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 75h:29m:16s remains)
2017-12-05 12:28:19.267873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1980658 -4.1931787 -4.18494 -4.1857119 -4.177937 -4.1645269 -4.1607876 -4.1707616 -4.1873717 -4.2027121 -4.2137671 -4.2119756 -4.20216 -4.19186 -4.181674][-4.1784964 -4.1771445 -4.1766033 -4.1818485 -4.1800489 -4.1730194 -4.1729107 -4.1813178 -4.1922545 -4.20076 -4.2025113 -4.1917276 -4.1764216 -4.1682892 -4.1626492][-4.1835055 -4.1820254 -4.1814947 -4.1838183 -4.1797557 -4.1711941 -4.1706419 -4.1788015 -4.1898575 -4.2005358 -4.2012115 -4.1851387 -4.1643639 -4.1527934 -4.1466041][-4.1889291 -4.1837111 -4.1774073 -4.1723466 -4.1620297 -4.147522 -4.1424727 -4.1505589 -4.1686139 -4.1863842 -4.1874785 -4.1684003 -4.1429253 -4.1252813 -4.1145215][-4.1873713 -4.1786575 -4.1616793 -4.1436 -4.1199741 -4.092597 -4.0798674 -4.0889072 -4.1171885 -4.1457682 -4.1559567 -4.1493664 -4.1322002 -4.1139541 -4.0964394][-4.18336 -4.1729331 -4.1442046 -4.1111584 -4.06911 -4.0225267 -3.9955721 -4.0004926 -4.034956 -4.0756307 -4.1054 -4.1225715 -4.1235852 -4.1125045 -4.0940828][-4.1878166 -4.1715274 -4.1289229 -4.0804434 -4.0255117 -3.9720685 -3.9397585 -3.9381111 -3.9730973 -4.0248632 -4.0718284 -4.1081448 -4.1248336 -4.1217279 -4.1053033][-4.1968074 -4.1728268 -4.1235538 -4.0701547 -4.0164456 -3.9746695 -3.9529839 -3.9505022 -3.982357 -4.03748 -4.0881848 -4.1235828 -4.1390076 -4.1326675 -4.1160283][-4.2046685 -4.1800871 -4.1361904 -4.09176 -4.0520587 -4.0297451 -4.0236039 -4.0230522 -4.0517392 -4.1014509 -4.1387076 -4.1574473 -4.1597028 -4.1451583 -4.12536][-4.2117672 -4.1961 -4.1668272 -4.1374846 -4.1125927 -4.1038961 -4.1030378 -4.0987735 -4.11969 -4.1565318 -4.1770344 -4.1734271 -4.155962 -4.1322684 -4.1116061][-4.2163281 -4.2130733 -4.1981487 -4.1775417 -4.1591911 -4.1555691 -4.1545906 -4.1455069 -4.16071 -4.1893411 -4.1971421 -4.1771317 -4.1427956 -4.1095538 -4.0896864][-4.218955 -4.2245693 -4.2173076 -4.1971989 -4.177496 -4.1698775 -4.164597 -4.1567411 -4.17132 -4.193687 -4.1915617 -4.1628389 -4.1274562 -4.0980482 -4.0868921][-4.2238665 -4.2328486 -4.2284822 -4.20484 -4.1758342 -4.1569076 -4.1455607 -4.1395745 -4.1533771 -4.1730733 -4.1678524 -4.142622 -4.1165257 -4.1018867 -4.1037922][-4.2263746 -4.2357163 -4.2328835 -4.2081852 -4.1697974 -4.1381092 -4.1180611 -4.111166 -4.1249013 -4.1497474 -4.1508851 -4.1333804 -4.1160846 -4.1140203 -4.1259365][-4.2227116 -4.2327652 -4.2329903 -4.2118039 -4.1699858 -4.1256223 -4.094512 -4.0877023 -4.1070628 -4.1401834 -4.152997 -4.145607 -4.1340103 -4.1366692 -4.15159]]...]
INFO - root - 2017-12-05 12:28:27.662176: step 8510, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.859 sec/batch; 77h:17m:09s remains)
INFO - root - 2017-12-05 12:28:36.091538: step 8520, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 76h:40m:30s remains)
INFO - root - 2017-12-05 12:28:44.621433: step 8530, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 77h:20m:20s remains)
INFO - root - 2017-12-05 12:28:53.136813: step 8540, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 76h:53m:36s remains)
INFO - root - 2017-12-05 12:29:01.658118: step 8550, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 74h:59m:49s remains)
INFO - root - 2017-12-05 12:29:10.047206: step 8560, loss = 2.05, batch loss = 2.00 (10.4 examples/sec; 0.771 sec/batch; 69h:24m:53s remains)
INFO - root - 2017-12-05 12:29:18.500592: step 8570, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 75h:29m:26s remains)
INFO - root - 2017-12-05 12:29:27.096185: step 8580, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 75h:37m:05s remains)
INFO - root - 2017-12-05 12:29:35.506010: step 8590, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.874 sec/batch; 78h:35m:58s remains)
INFO - root - 2017-12-05 12:29:43.873526: step 8600, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 77h:00m:43s remains)
2017-12-05 12:29:44.669782: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2845449 -4.2533107 -4.2166591 -4.1928983 -4.18369 -4.1932354 -4.225719 -4.2563596 -4.2711062 -4.2745466 -4.2693577 -4.2668476 -4.2695961 -4.2780962 -4.2825575][-4.2792888 -4.2456594 -4.2121143 -4.1940475 -4.1854639 -4.1953473 -4.2296081 -4.2614474 -4.2724781 -4.2708979 -4.2635479 -4.2626128 -4.2665834 -4.278585 -4.2854915][-4.2748504 -4.24124 -4.2112947 -4.1973777 -4.1872573 -4.1930761 -4.2246351 -4.2542672 -4.2635026 -4.2612762 -4.2569804 -4.2592411 -4.2659183 -4.280973 -4.2935176][-4.2692475 -4.2352371 -4.2060037 -4.1935129 -4.1823525 -4.1855369 -4.2135224 -4.2408376 -4.2524867 -4.2562141 -4.2536812 -4.2539091 -4.2595849 -4.2770944 -4.2984686][-4.2621555 -4.2233214 -4.1898565 -4.1725397 -4.15891 -4.1669555 -4.2002158 -4.2304473 -4.2457471 -4.2568016 -4.2536716 -4.2474771 -4.25317 -4.2757568 -4.3036547][-4.2520242 -4.2042785 -4.1610518 -4.13245 -4.1133361 -4.127142 -4.1760345 -4.2190113 -4.2441392 -4.2614684 -4.2558923 -4.2455049 -4.2503905 -4.2733965 -4.3006306][-4.2444706 -4.1872492 -4.1325774 -4.0888925 -4.0571623 -4.07146 -4.1404691 -4.2021527 -4.2408857 -4.2629271 -4.2566209 -4.2440004 -4.2468681 -4.2683315 -4.292707][-4.2476721 -4.1864972 -4.1251955 -4.0690236 -4.0218782 -4.0329084 -4.1152563 -4.1896634 -4.2361751 -4.261879 -4.2581921 -4.2445178 -4.2437878 -4.2640843 -4.2879553][-4.2585859 -4.1991787 -4.1376328 -4.077601 -4.0289316 -4.0433574 -4.12586 -4.1975241 -4.2372627 -4.2579489 -4.2523685 -4.23689 -4.23369 -4.2550106 -4.2806973][-4.2688961 -4.2140136 -4.1600475 -4.1080122 -4.0728526 -4.0919895 -4.1606274 -4.2168674 -4.2393141 -4.2447996 -4.2319837 -4.2169218 -4.2157516 -4.241178 -4.2692981][-4.2727947 -4.2255988 -4.1812015 -4.1400642 -4.1178675 -4.1411376 -4.1938267 -4.2328115 -4.2368469 -4.2233477 -4.2012682 -4.1858091 -4.1886678 -4.2224617 -4.2541871][-4.2707047 -4.2306266 -4.1930861 -4.16199 -4.1518016 -4.1777649 -4.2202621 -4.2485785 -4.2404394 -4.2100935 -4.1752763 -4.15473 -4.1601453 -4.2027183 -4.2371531][-4.2674017 -4.2312007 -4.1963573 -4.1712933 -4.1690421 -4.1963258 -4.2316265 -4.2542892 -4.2404175 -4.2001433 -4.158823 -4.131156 -4.137042 -4.1848893 -4.2225633][-4.2683082 -4.234396 -4.1998186 -4.1781321 -4.1804175 -4.2086749 -4.2367911 -4.2515225 -4.2347231 -4.1921563 -4.1510477 -4.1223893 -4.1299095 -4.1787896 -4.2181416][-4.2777715 -4.2477217 -4.21657 -4.1988826 -4.2040033 -4.2275791 -4.2451582 -4.2512126 -4.2343779 -4.1953969 -4.1608324 -4.1366711 -4.1466537 -4.1900954 -4.2259197]]...]
INFO - root - 2017-12-05 12:29:53.266370: step 8610, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 74h:51m:32s remains)
INFO - root - 2017-12-05 12:30:01.707014: step 8620, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.813 sec/batch; 73h:08m:02s remains)
INFO - root - 2017-12-05 12:30:10.152703: step 8630, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 75h:58m:33s remains)
INFO - root - 2017-12-05 12:30:18.625138: step 8640, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 77h:00m:56s remains)
INFO - root - 2017-12-05 12:30:27.093879: step 8650, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 76h:50m:13s remains)
INFO - root - 2017-12-05 12:30:35.564877: step 8660, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 75h:22m:15s remains)
INFO - root - 2017-12-05 12:30:44.173217: step 8670, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 79h:17m:36s remains)
INFO - root - 2017-12-05 12:30:52.791622: step 8680, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 76h:49m:24s remains)
INFO - root - 2017-12-05 12:31:01.363230: step 8690, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 75h:38m:05s remains)
INFO - root - 2017-12-05 12:31:09.821525: step 8700, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.857 sec/batch; 77h:03m:00s remains)
2017-12-05 12:31:10.590299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2539797 -4.2550507 -4.2538271 -4.2526526 -4.2522535 -4.2534823 -4.2569947 -4.2596769 -4.2613935 -4.2636175 -4.2653089 -4.2668672 -4.2687888 -4.2690849 -4.2675772][-4.2371149 -4.2365637 -4.23228 -4.2286525 -4.2285056 -4.2308869 -4.23578 -4.2394047 -4.243947 -4.2493057 -4.2532306 -4.256804 -4.2603722 -4.2615113 -4.2602863][-4.2130823 -4.2094769 -4.2025995 -4.1957388 -4.1940484 -4.1969461 -4.2033095 -4.2095914 -4.2176256 -4.2252841 -4.230886 -4.2377572 -4.2448454 -4.2494936 -4.2500696][-4.1943545 -4.1882305 -4.1775255 -4.1644754 -4.1590514 -4.1557474 -4.1558094 -4.1624861 -4.1754065 -4.1886239 -4.1996546 -4.2149386 -4.2302065 -4.239212 -4.2414355][-4.1817894 -4.1706886 -4.1522064 -4.1308837 -4.1185246 -4.1014957 -4.0876083 -4.0918903 -4.1107264 -4.1346188 -4.1592288 -4.188242 -4.2171059 -4.2344961 -4.2396784][-4.1674633 -4.146244 -4.1180511 -4.085268 -4.0601525 -4.0245347 -3.9953458 -3.9979551 -4.0227957 -4.056932 -4.0980945 -4.1446338 -4.1890273 -4.2171016 -4.2282314][-4.1546164 -4.11962 -4.0797906 -4.0360408 -3.9928691 -3.9295614 -3.882324 -3.8952594 -3.9365456 -3.9804502 -4.0363312 -4.0975127 -4.1522474 -4.1884055 -4.2078338][-4.1323786 -4.0864496 -4.0378432 -3.9877162 -3.9308941 -3.8464146 -3.7875588 -3.8172607 -3.8821492 -3.9376411 -3.9964879 -4.0558887 -4.1071463 -4.1456151 -4.1730046][-4.1270747 -4.0774775 -4.0273032 -3.9843252 -3.9390206 -3.8739531 -3.830153 -3.8538465 -3.9045637 -3.9451272 -3.984797 -4.0272884 -4.0663514 -4.0996814 -4.1284227][-4.1414776 -4.1016312 -4.0646491 -4.0378284 -4.0109611 -3.97481 -3.9534762 -3.9653971 -3.9846237 -3.9961212 -4.0115376 -4.0282054 -4.0481739 -4.0710287 -4.0973349][-4.1669779 -4.1413603 -4.1175275 -4.1040335 -4.0907111 -4.0703344 -4.0650496 -4.0725942 -4.0708318 -4.0543761 -4.0448627 -4.0400405 -4.0444837 -4.0543127 -4.0746489][-4.2004218 -4.1776757 -4.1557097 -4.1453285 -4.1365275 -4.1231971 -4.1222324 -4.1261559 -4.1171217 -4.0891776 -4.0625043 -4.0453329 -4.0435863 -4.0518832 -4.0704079][-4.2276673 -4.2054186 -4.1778188 -4.1580443 -4.144135 -4.1324081 -4.1311269 -4.13253 -4.1284509 -4.1086788 -4.0818653 -4.0617628 -4.0573387 -4.0667005 -4.0801835][-4.2373409 -4.2126641 -4.1762953 -4.1454096 -4.1255159 -4.112833 -4.1105719 -4.1092033 -4.1123219 -4.1129413 -4.1034269 -4.0889239 -4.0786839 -4.0867047 -4.092546][-4.2242184 -4.1969738 -4.15433 -4.1180553 -4.094861 -4.08363 -4.0797567 -4.073935 -4.0773582 -4.0894766 -4.0974355 -4.0955868 -4.0912738 -4.0993772 -4.0965667]]...]
INFO - root - 2017-12-05 12:31:19.245401: step 8710, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 76h:48m:57s remains)
INFO - root - 2017-12-05 12:31:27.841962: step 8720, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 76h:40m:26s remains)
INFO - root - 2017-12-05 12:31:36.433781: step 8730, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 76h:49m:35s remains)
INFO - root - 2017-12-05 12:31:44.886988: step 8740, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 77h:50m:47s remains)
INFO - root - 2017-12-05 12:31:53.343198: step 8750, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 74h:55m:09s remains)
INFO - root - 2017-12-05 12:32:01.893663: step 8760, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 77h:17m:36s remains)
INFO - root - 2017-12-05 12:32:10.470639: step 8770, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 81h:28m:43s remains)
INFO - root - 2017-12-05 12:32:18.935949: step 8780, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 78h:06m:02s remains)
INFO - root - 2017-12-05 12:32:27.520399: step 8790, loss = 2.02, batch loss = 1.96 (9.5 examples/sec; 0.846 sec/batch; 76h:05m:42s remains)
INFO - root - 2017-12-05 12:32:36.003801: step 8800, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 75h:45m:38s remains)
2017-12-05 12:32:36.824027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3237557 -4.3265018 -4.330822 -4.3357515 -4.3401303 -4.3409028 -4.340395 -4.339745 -4.33778 -4.3346968 -4.3333182 -4.3350267 -4.3397307 -4.3437095 -4.3484664][-4.2713022 -4.27636 -4.2852144 -4.2947249 -4.3014879 -4.3013616 -4.2977772 -4.2983584 -4.3013039 -4.2987967 -4.2945423 -4.2954159 -4.3024836 -4.3101063 -4.3195028][-4.1884322 -4.1935835 -4.2090979 -4.2237167 -4.2314873 -4.2265043 -4.2185664 -4.2217312 -4.2314262 -4.233315 -4.2295585 -4.2328553 -4.2427931 -4.2553134 -4.2739916][-4.0813618 -4.085609 -4.1031055 -4.1217589 -4.1325283 -4.1241493 -4.1094537 -4.1139235 -4.1319823 -4.1390676 -4.139575 -4.1494904 -4.1625767 -4.1801167 -4.2109747][-3.9993196 -4.0032139 -4.0112371 -4.0195675 -4.021513 -4.0042491 -3.9836159 -3.991832 -4.019743 -4.0380931 -4.0514684 -4.0722766 -4.0896039 -4.1113739 -4.151804][-3.9614558 -3.9674213 -3.9626875 -3.9510596 -3.936626 -3.9068203 -3.8790188 -3.8841705 -3.9147334 -3.9513824 -3.9897635 -4.023159 -4.042717 -4.0687318 -4.1172028][-3.955549 -3.9675438 -3.9591718 -3.9341068 -3.9099698 -3.8761818 -3.8479078 -3.8432884 -3.8599057 -3.9023733 -3.9651489 -4.0103512 -4.0296564 -4.0514789 -4.09598][-4.0018673 -4.0265718 -4.0288343 -4.00149 -3.9744663 -3.9368427 -3.9016211 -3.8838882 -3.8843603 -3.9180622 -3.9825408 -4.0358734 -4.0564885 -4.0690923 -4.0961676][-4.0650353 -4.1062317 -4.1270094 -4.1083765 -4.080277 -4.0425711 -4.0075722 -3.9862087 -3.9796591 -3.996434 -4.0369821 -4.0795093 -4.0954561 -4.0999866 -4.1127114][-4.1105433 -4.155838 -4.1912189 -4.1906204 -4.1688828 -4.1385918 -4.1132574 -4.1003361 -4.0989628 -4.1070652 -4.118536 -4.135087 -4.1359687 -4.1313472 -4.1328812][-4.1350985 -4.1792059 -4.2175493 -4.2293396 -4.2178311 -4.1963477 -4.1795282 -4.175581 -4.1813602 -4.1916981 -4.1962719 -4.1970596 -4.184062 -4.1673632 -4.1635094][-4.1304393 -4.1692796 -4.2027774 -4.2202854 -4.2222795 -4.2147827 -4.209168 -4.2117119 -4.2183876 -4.22615 -4.229866 -4.2281637 -4.2102137 -4.1887083 -4.1825981][-4.1229734 -4.1573548 -4.1843252 -4.2010975 -4.207871 -4.2109909 -4.2149177 -4.2221074 -4.2298489 -4.2359161 -4.2384109 -4.2341948 -4.214139 -4.1953993 -4.1929822][-4.1267085 -4.1596885 -4.1879249 -4.2060294 -4.212956 -4.2168741 -4.2226095 -4.2300482 -4.2392836 -4.2495351 -4.2519021 -4.2436962 -4.222384 -4.2041135 -4.2019458][-4.1589842 -4.1860986 -4.2123203 -4.233469 -4.2449827 -4.2493677 -4.2521977 -4.2579627 -4.2640805 -4.2720137 -4.2765565 -4.2713537 -4.2538004 -4.2383356 -4.235671]]...]
INFO - root - 2017-12-05 12:32:45.318338: step 8810, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 80h:32m:12s remains)
INFO - root - 2017-12-05 12:32:53.754763: step 8820, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 77h:03m:06s remains)
INFO - root - 2017-12-05 12:33:02.366230: step 8830, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 74h:48m:24s remains)
INFO - root - 2017-12-05 12:33:10.866556: step 8840, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 75h:26m:50s remains)
INFO - root - 2017-12-05 12:33:19.225995: step 8850, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 77h:51m:14s remains)
INFO - root - 2017-12-05 12:33:27.769801: step 8860, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 75h:49m:45s remains)
INFO - root - 2017-12-05 12:33:36.348885: step 8870, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 77h:56m:13s remains)
INFO - root - 2017-12-05 12:33:44.996770: step 8880, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 78h:46m:02s remains)
INFO - root - 2017-12-05 12:33:53.581819: step 8890, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 77h:59m:30s remains)
INFO - root - 2017-12-05 12:34:02.069942: step 8900, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 79h:29m:25s remains)
2017-12-05 12:34:02.850589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1649685 -4.1574364 -4.1562986 -4.1630993 -4.1724896 -4.1809292 -4.183485 -4.1703238 -4.1567483 -4.1585622 -4.1574426 -4.1573129 -4.15396 -4.1403594 -4.140029][-4.1119971 -4.1122155 -4.1253395 -4.1470933 -4.1717615 -4.1952252 -4.20246 -4.1904955 -4.179306 -4.1826363 -4.1781826 -4.1671534 -4.1431389 -4.1134262 -4.1055527][-4.0610433 -4.0748162 -4.1102471 -4.1526203 -4.1902823 -4.2205658 -4.2238126 -4.2073274 -4.1916237 -4.188591 -4.1783915 -4.1560893 -4.1146679 -4.0762854 -4.0662956][-3.9943185 -4.0301909 -4.094965 -4.1623626 -4.2059064 -4.2315063 -4.224628 -4.1969981 -4.1764894 -4.1711988 -4.1590719 -4.1340466 -4.0907855 -4.0574512 -4.0514674][-3.9604421 -4.0173416 -4.1009521 -4.1715879 -4.2042313 -4.2118964 -4.1799836 -4.1317286 -4.1093488 -4.1157589 -4.1137214 -4.1014638 -4.0788097 -4.0656157 -4.0725117][-4.0310335 -4.08906 -4.1587534 -4.1992536 -4.1990275 -4.1707039 -4.1096668 -4.038312 -4.02495 -4.0614161 -4.0842781 -4.0951133 -4.101244 -4.1085582 -4.1214967][-4.1099019 -4.1575055 -4.1980591 -4.2017736 -4.1609459 -4.0941544 -4.0010881 -3.9123731 -3.9263124 -4.0070391 -4.0667353 -4.1057386 -4.1352029 -4.1472497 -4.1532679][-4.1370435 -4.1634984 -4.1699572 -4.1368961 -4.0555072 -3.9590595 -3.845057 -3.7522321 -3.8153138 -3.9482884 -4.0419321 -4.101253 -4.1426449 -4.1478958 -4.1371179][-4.163548 -4.1660428 -4.1364813 -4.0711083 -3.9667859 -3.8638625 -3.7624226 -3.6941295 -3.7912891 -3.9481757 -4.0495987 -4.1093698 -4.1517534 -4.1480846 -4.1249952][-4.1748943 -4.1627703 -4.1156092 -4.0430431 -3.9499323 -3.8716261 -3.8176727 -3.7983456 -3.8830171 -4.01332 -4.0951085 -4.1411109 -4.1749096 -4.1671848 -4.1456623][-4.1685252 -4.1472993 -4.0955596 -4.0319448 -3.9614141 -3.9143658 -3.9016118 -3.9161918 -3.9823089 -4.07558 -4.1350937 -4.1657925 -4.1884942 -4.1806769 -4.1628919][-4.1780872 -4.15341 -4.1065683 -4.0581651 -4.0146337 -3.9948142 -4.0077848 -4.0398655 -4.0893764 -4.1471319 -4.1836982 -4.1971893 -4.2050838 -4.1975365 -4.1849508][-4.22054 -4.1969976 -4.1639094 -4.1365871 -4.1167803 -4.1121292 -4.1311827 -4.1614552 -4.1908078 -4.21816 -4.2363496 -4.2391052 -4.2365885 -4.2314911 -4.225667][-4.2723227 -4.2535424 -4.2318697 -4.21961 -4.2138977 -4.2173934 -4.2353306 -4.257719 -4.2730074 -4.2832522 -4.2909613 -4.289464 -4.2837572 -4.2801752 -4.278511][-4.3095622 -4.296247 -4.2837577 -4.2789288 -4.2793202 -4.2847919 -4.2974086 -4.3103123 -4.3182716 -4.3218784 -4.3238139 -4.32198 -4.317183 -4.314599 -4.3145194]]...]
INFO - root - 2017-12-05 12:34:11.385063: step 8910, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 75h:25m:40s remains)
INFO - root - 2017-12-05 12:34:19.936775: step 8920, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 75h:22m:49s remains)
INFO - root - 2017-12-05 12:34:28.484669: step 8930, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 76h:49m:32s remains)
INFO - root - 2017-12-05 12:34:37.083945: step 8940, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 77h:48m:40s remains)
INFO - root - 2017-12-05 12:34:45.670427: step 8950, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 77h:04m:17s remains)
INFO - root - 2017-12-05 12:34:54.214364: step 8960, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 76h:51m:06s remains)
INFO - root - 2017-12-05 12:35:02.802926: step 8970, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 77h:56m:30s remains)
INFO - root - 2017-12-05 12:35:11.355171: step 8980, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 76h:06m:05s remains)
INFO - root - 2017-12-05 12:35:19.837817: step 8990, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 77h:56m:19s remains)
INFO - root - 2017-12-05 12:35:28.333973: step 9000, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 76h:16m:23s remains)
2017-12-05 12:35:29.082596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1922741 -4.1864243 -4.1744394 -4.1622548 -4.1504726 -4.1480279 -4.1547976 -4.1624136 -4.1660671 -4.1730351 -4.1813359 -4.1811147 -4.1658182 -4.1374564 -4.109045][-4.1819715 -4.175281 -4.1640797 -4.15232 -4.1395597 -4.1310539 -4.1310549 -4.1354284 -4.1434383 -4.159327 -4.1756525 -4.1805558 -4.1689148 -4.1445255 -4.11762][-4.1612792 -4.1547766 -4.1443787 -4.1346116 -4.12358 -4.115068 -4.1138668 -4.1199784 -4.1344075 -4.1578088 -4.1789894 -4.1844339 -4.1744409 -4.1524725 -4.125854][-4.1370177 -4.1328025 -4.1235828 -4.1156106 -4.1058531 -4.0956945 -4.0924091 -4.1011443 -4.1236172 -4.1575603 -4.1858544 -4.1926842 -4.1845264 -4.1658845 -4.1434317][-4.113893 -4.11607 -4.1074266 -4.0947037 -4.0768194 -4.0557451 -4.0421009 -4.0502415 -4.0848312 -4.1350217 -4.1764235 -4.1897264 -4.1892691 -4.1769571 -4.1601291][-4.0782213 -4.0828657 -4.0714068 -4.0474877 -4.0133367 -3.9752095 -3.9462719 -3.9526961 -4.0034871 -4.0787873 -4.1436477 -4.17728 -4.192184 -4.1883636 -4.17699][-4.0726352 -4.0714107 -4.0543761 -4.0222688 -3.9776621 -3.9246769 -3.8741796 -3.8649278 -3.919178 -4.0070868 -4.0886536 -4.1394653 -4.1652083 -4.1650386 -4.1523671][-4.1138525 -4.1094718 -4.0949821 -4.0684385 -4.0328722 -3.9895232 -3.9423451 -3.9219182 -3.9514241 -4.0140553 -4.0795293 -4.1223617 -4.1409383 -4.1322579 -4.1088734][-4.1568618 -4.154099 -4.1416378 -4.1219769 -4.1013126 -4.0805788 -4.0550122 -4.0432072 -4.0560565 -4.0895448 -4.1285534 -4.1541605 -4.1610966 -4.14688 -4.1153574][-4.1801558 -4.1798167 -4.1703992 -4.15482 -4.1450539 -4.1409807 -4.134429 -4.1356483 -4.14602 -4.1635823 -4.1846209 -4.1987529 -4.2024364 -4.1924071 -4.1659703][-4.1938534 -4.1978507 -4.1923265 -4.1790767 -4.1719809 -4.1743045 -4.1799994 -4.1888022 -4.1981373 -4.2056184 -4.2158813 -4.221868 -4.2219572 -4.2149744 -4.198174][-4.2087321 -4.2128296 -4.2065973 -4.191546 -4.1814389 -4.1837215 -4.1930852 -4.2040625 -4.2110157 -4.2148185 -4.2214332 -4.2241583 -4.2220387 -4.2161641 -4.2069011][-4.2291784 -4.2308173 -4.2242889 -4.2084832 -4.194231 -4.1920881 -4.1985993 -4.2060966 -4.20831 -4.2079372 -4.2105064 -4.2101393 -4.2059169 -4.2000551 -4.1952929][-4.2402873 -4.2394404 -4.2374854 -4.2290945 -4.2184067 -4.2164884 -4.2204814 -4.2222247 -4.219728 -4.2156253 -4.2127905 -4.2083941 -4.2032323 -4.1984835 -4.1951275][-4.2381678 -4.2398534 -4.2423186 -4.2409697 -4.2364097 -4.2358222 -4.2384863 -4.2375522 -4.2344856 -4.2314882 -4.2275691 -4.2241244 -4.2226338 -4.2231679 -4.2230573]]...]
INFO - root - 2017-12-05 12:35:37.676884: step 9010, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 76h:55m:53s remains)
INFO - root - 2017-12-05 12:35:46.344942: step 9020, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 78h:28m:38s remains)
INFO - root - 2017-12-05 12:35:54.921686: step 9030, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 76h:36m:53s remains)
INFO - root - 2017-12-05 12:36:03.628926: step 9040, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 79h:17m:40s remains)
INFO - root - 2017-12-05 12:36:12.208942: step 9050, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 75h:29m:27s remains)
INFO - root - 2017-12-05 12:36:20.730292: step 9060, loss = 2.11, batch loss = 2.05 (9.7 examples/sec; 0.828 sec/batch; 74h:22m:58s remains)
INFO - root - 2017-12-05 12:36:29.167831: step 9070, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 77h:22m:07s remains)
INFO - root - 2017-12-05 12:36:37.737322: step 9080, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 75h:00m:38s remains)
INFO - root - 2017-12-05 12:36:46.362483: step 9090, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 76h:42m:08s remains)
INFO - root - 2017-12-05 12:36:54.858120: step 9100, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 78h:29m:16s remains)
2017-12-05 12:36:55.623430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3000441 -4.2918787 -4.2761011 -4.2515855 -4.2189231 -4.1884007 -4.1708422 -4.165875 -4.1686263 -4.1802397 -4.1967988 -4.2101736 -4.2118039 -4.208478 -4.2081227][-4.2979317 -4.2858152 -4.2642159 -4.2356958 -4.2028418 -4.1740718 -4.1640134 -4.1684356 -4.1748953 -4.1797757 -4.1873074 -4.1974907 -4.2082324 -4.2204733 -4.2260242][-4.2957611 -4.2791252 -4.251967 -4.2167997 -4.1792192 -4.15123 -4.1457515 -4.1595011 -4.1774316 -4.1870279 -4.1858363 -4.1855831 -4.1971903 -4.2196507 -4.2305808][-4.296442 -4.2765155 -4.2455568 -4.2030587 -4.1573434 -4.1281 -4.1243877 -4.1437297 -4.1734524 -4.1910472 -4.1886783 -4.1828041 -4.1864982 -4.2066274 -4.2189837][-4.2986655 -4.2768917 -4.2438941 -4.1958513 -4.141746 -4.1029949 -4.0928392 -4.1141186 -4.154376 -4.1851335 -4.1921573 -4.1904917 -4.1901622 -4.1990018 -4.2070584][-4.3029656 -4.2816162 -4.2470665 -4.1948042 -4.1303954 -4.0760384 -4.0490031 -4.0568609 -4.0994611 -4.1535959 -4.1883397 -4.199481 -4.2010279 -4.2033954 -4.2013354][-4.3100386 -4.29163 -4.2593122 -4.2059579 -4.1338749 -4.0643668 -4.0090604 -3.978539 -4.0054865 -4.08982 -4.1657743 -4.2007213 -4.2090926 -4.2119913 -4.2049556][-4.3156743 -4.3013821 -4.2738423 -4.2248964 -4.1516709 -4.074338 -3.9927206 -3.908855 -3.899291 -4.0089283 -4.1261892 -4.1878028 -4.2122426 -4.2229314 -4.215929][-4.3204093 -4.3106208 -4.2900147 -4.2496538 -4.1815367 -4.1088238 -4.0234804 -3.9248743 -3.8869476 -3.9776442 -4.0966911 -4.16816 -4.2073417 -4.2283983 -4.224494][-4.3253222 -4.3201742 -4.3063192 -4.2768979 -4.2190604 -4.1572075 -4.0906296 -4.021029 -3.9864535 -4.024096 -4.0971885 -4.1538486 -4.1981635 -4.2277155 -4.2313666][-4.3267736 -4.3239737 -4.3151245 -4.2925043 -4.243196 -4.1913342 -4.1432376 -4.1029134 -4.0827789 -4.0900054 -4.120616 -4.15472 -4.191288 -4.2238855 -4.2363849][-4.3250456 -4.3231931 -4.3144312 -4.2923079 -4.2478933 -4.2032428 -4.1674948 -4.1434264 -4.1328211 -4.1326714 -4.1473751 -4.1640639 -4.1864686 -4.2161 -4.2323337][-4.3230166 -4.3216391 -4.3120179 -4.28811 -4.243454 -4.2012758 -4.1712637 -4.153729 -4.1493735 -4.1560225 -4.171349 -4.183867 -4.1957393 -4.213975 -4.2226877][-4.3215485 -4.3215137 -4.3124809 -4.2861371 -4.2417822 -4.2018318 -4.1720867 -4.155735 -4.1574163 -4.17434 -4.1954823 -4.20832 -4.2172656 -4.2231364 -4.2176566][-4.3209591 -4.3217831 -4.3158865 -4.2947726 -4.258069 -4.224781 -4.198875 -4.1847777 -4.1882434 -4.2083821 -4.2279477 -4.2392616 -4.2460756 -4.2446375 -4.232851]]...]
INFO - root - 2017-12-05 12:37:04.120765: step 9110, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 77h:49m:26s remains)
INFO - root - 2017-12-05 12:37:12.760105: step 9120, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 78h:15m:58s remains)
INFO - root - 2017-12-05 12:37:21.393201: step 9130, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 79h:12m:27s remains)
INFO - root - 2017-12-05 12:37:29.884061: step 9140, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 75h:56m:33s remains)
INFO - root - 2017-12-05 12:37:38.467956: step 9150, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 77h:07m:32s remains)
INFO - root - 2017-12-05 12:37:46.979689: step 9160, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.861 sec/batch; 77h:20m:03s remains)
INFO - root - 2017-12-05 12:37:55.528650: step 9170, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 75h:45m:30s remains)
INFO - root - 2017-12-05 12:38:04.003765: step 9180, loss = 2.10, batch loss = 2.05 (9.6 examples/sec; 0.834 sec/batch; 74h:51m:53s remains)
INFO - root - 2017-12-05 12:38:12.521051: step 9190, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 77h:05m:48s remains)
INFO - root - 2017-12-05 12:38:20.874612: step 9200, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.797 sec/batch; 71h:35m:48s remains)
2017-12-05 12:38:21.617725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0645514 -4.0885773 -4.1166043 -4.1324639 -4.12421 -4.1077614 -4.1065221 -4.1144114 -4.1250997 -4.1483197 -4.1680827 -4.162087 -4.1511822 -4.156774 -4.1723156][-4.0486131 -4.0692668 -4.0888615 -4.1007028 -4.0946736 -4.0853863 -4.089499 -4.1013627 -4.1131978 -4.1404319 -4.166997 -4.1644316 -4.1583614 -4.1632986 -4.1771655][-4.0647573 -4.070209 -4.0796094 -4.0903392 -4.0852528 -4.0835776 -4.0883718 -4.0970025 -4.1080656 -4.1377945 -4.1753087 -4.1791821 -4.1755075 -4.1785712 -4.1895533][-4.1079731 -4.0969839 -4.0946736 -4.102097 -4.0956278 -4.0987339 -4.1002784 -4.0995979 -4.1046944 -4.1302748 -4.1732421 -4.1882715 -4.1860318 -4.1851707 -4.1911917][-4.1447716 -4.1312647 -4.1252513 -4.122026 -4.1059465 -4.1011953 -4.0840158 -4.0618157 -4.064992 -4.0955453 -4.1456723 -4.1709456 -4.1745563 -4.1783957 -4.1850367][-4.1558619 -4.1478305 -4.1444197 -4.1304936 -4.0990114 -4.0680723 -4.0095592 -3.9587557 -3.9715328 -4.0271769 -4.0954504 -4.1350875 -4.1516066 -4.1671033 -4.182148][-4.1360965 -4.1254458 -4.1174555 -4.0952239 -4.047153 -3.9792681 -3.8650236 -3.7756779 -3.8140247 -3.927392 -4.0365434 -4.1031771 -4.1414094 -4.1707234 -4.1923828][-4.1069431 -4.0932097 -4.0740561 -4.0426116 -3.9818747 -3.8906415 -3.7531655 -3.6541662 -3.727603 -3.8945675 -4.0350389 -4.1163111 -4.1636071 -4.19057 -4.2035518][-4.1132817 -4.0996542 -4.0767303 -4.0475125 -3.9924185 -3.919533 -3.8249354 -3.7761223 -3.8577437 -3.9958558 -4.1084981 -4.1676793 -4.1970863 -4.2091079 -4.2126274][-4.1248422 -4.1104908 -4.0940442 -4.0775061 -4.0436306 -4.0045071 -3.9638257 -3.9595783 -4.023602 -4.1075511 -4.1725473 -4.1956038 -4.2026482 -4.2039614 -4.2057772][-4.1385069 -4.1294742 -4.1248922 -4.1219039 -4.1034408 -4.085659 -4.0710936 -4.0804143 -4.1241813 -4.1728606 -4.2067113 -4.2053847 -4.1983457 -4.1962128 -4.1979866][-4.1404119 -4.1477137 -4.1615248 -4.172039 -4.1661806 -4.1554046 -4.1446733 -4.1525078 -4.1806049 -4.2102489 -4.2244134 -4.2099438 -4.19488 -4.1907163 -4.1979508][-4.1318531 -4.1494718 -4.1767988 -4.1945891 -4.1930475 -4.1848588 -4.1795259 -4.1842237 -4.1992803 -4.2148628 -4.2188039 -4.1973119 -4.1812496 -4.1851034 -4.2038746][-4.1195989 -4.140945 -4.1713557 -4.1915817 -4.1928678 -4.1871819 -4.1853085 -4.1875935 -4.1954193 -4.2027225 -4.2002063 -4.1807137 -4.1692185 -4.1816583 -4.2106409][-4.1432815 -4.165267 -4.1896448 -4.2053642 -4.2075362 -4.2055335 -4.2075129 -4.2109666 -4.2133169 -4.2126441 -4.207221 -4.195991 -4.1930928 -4.2056375 -4.2300434]]...]
INFO - root - 2017-12-05 12:38:30.110026: step 9210, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.836 sec/batch; 75h:06m:50s remains)
INFO - root - 2017-12-05 12:38:38.616365: step 9220, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 78h:51m:34s remains)
INFO - root - 2017-12-05 12:38:47.084305: step 9230, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 74h:41m:08s remains)
INFO - root - 2017-12-05 12:38:55.612566: step 9240, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 78h:03m:01s remains)
INFO - root - 2017-12-05 12:39:04.026531: step 9250, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 75h:12m:37s remains)
INFO - root - 2017-12-05 12:39:12.492065: step 9260, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 74h:49m:31s remains)
INFO - root - 2017-12-05 12:39:20.927598: step 9270, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 75h:54m:14s remains)
INFO - root - 2017-12-05 12:39:29.345431: step 9280, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 77h:15m:31s remains)
INFO - root - 2017-12-05 12:39:37.705882: step 9290, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 77h:02m:53s remains)
INFO - root - 2017-12-05 12:39:45.966862: step 9300, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 65h:48m:55s remains)
2017-12-05 12:39:46.753142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2253227 -4.2474556 -4.2547579 -4.2489238 -4.2412 -4.23123 -4.2075787 -4.1833663 -4.1708126 -4.1524839 -4.1296916 -4.1258173 -4.1568251 -4.1878262 -4.2071609][-4.2005291 -4.2229548 -4.2361131 -4.2349262 -4.226943 -4.2162657 -4.1968827 -4.1816154 -4.1816087 -4.1760082 -4.1571403 -4.148632 -4.1680307 -4.1899257 -4.2039981][-4.1856327 -4.201057 -4.2144794 -4.2138705 -4.201766 -4.1891084 -4.1762929 -4.1732507 -4.1895876 -4.198894 -4.1919088 -4.1836519 -4.1904578 -4.1994843 -4.2028294][-4.19098 -4.1974831 -4.2024965 -4.1989865 -4.1804328 -4.1609564 -4.1455374 -4.1451888 -4.1757579 -4.2035804 -4.213068 -4.2097731 -4.2052069 -4.1976018 -4.1909447][-4.2064075 -4.2089224 -4.2038536 -4.1935987 -4.1669383 -4.1278419 -4.0917625 -4.0834064 -4.1224589 -4.1722975 -4.202219 -4.208878 -4.2005553 -4.1850038 -4.1743731][-4.214767 -4.2144213 -4.2019143 -4.1823711 -4.1443152 -4.0867286 -4.0216537 -3.9944568 -4.0427074 -4.1206684 -4.1788864 -4.2030349 -4.20224 -4.1849661 -4.1702461][-4.2008772 -4.1985316 -4.1877203 -4.1641111 -4.1205325 -4.0557251 -3.9672177 -3.9111671 -3.9592981 -4.0611973 -4.1457458 -4.1877203 -4.1965017 -4.1853805 -4.1749682][-4.17929 -4.1811461 -4.1806364 -4.1636753 -4.1274233 -4.0771661 -3.9996324 -3.9377275 -3.9633102 -4.0493293 -4.1291142 -4.1696291 -4.1777225 -4.1712418 -4.169065][-4.1606092 -4.1618962 -4.1666369 -4.160284 -4.140173 -4.1172171 -4.0789428 -4.0398731 -4.0431867 -4.0872216 -4.1333094 -4.1533103 -4.1495171 -4.1412144 -4.1418157][-4.151032 -4.1446524 -4.1441517 -4.1402721 -4.13292 -4.1289229 -4.1222968 -4.10854 -4.1055613 -4.1176586 -4.1313519 -4.1299787 -4.1158185 -4.1061177 -4.1074381][-4.1431103 -4.1302762 -4.125288 -4.1243382 -4.1296616 -4.137301 -4.1478634 -4.1528578 -4.151279 -4.14686 -4.13989 -4.12555 -4.1080351 -4.0989933 -4.1011848][-4.1293731 -4.1155577 -4.1088414 -4.114264 -4.1335254 -4.1510363 -4.169435 -4.1809425 -4.1818142 -4.1744962 -4.1615171 -4.1438317 -4.128345 -4.120883 -4.1206236][-4.1290016 -4.1182618 -4.1117873 -4.1195197 -4.1423082 -4.1631961 -4.1830878 -4.1957264 -4.1983185 -4.1916437 -4.1807704 -4.16906 -4.1602607 -4.1561584 -4.1558967][-4.1591325 -4.155427 -4.1529756 -4.1593423 -4.1777935 -4.1959348 -4.2129269 -4.2242804 -4.2269812 -4.2217627 -4.2145872 -4.2091346 -4.2049522 -4.2032185 -4.2038317][-4.2022066 -4.2078271 -4.2121444 -4.2173772 -4.2273469 -4.2387576 -4.2493119 -4.2558551 -4.2569704 -4.2541723 -4.2507024 -4.2487588 -4.24733 -4.2463374 -4.2469735]]...]
INFO - root - 2017-12-05 12:39:55.146932: step 9310, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 78h:20m:24s remains)
INFO - root - 2017-12-05 12:40:03.610501: step 9320, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 76h:08m:15s remains)
INFO - root - 2017-12-05 12:40:12.020480: step 9330, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 75h:56m:56s remains)
INFO - root - 2017-12-05 12:40:20.517724: step 9340, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.850 sec/batch; 76h:19m:39s remains)
INFO - root - 2017-12-05 12:40:29.034666: step 9350, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 75h:26m:23s remains)
INFO - root - 2017-12-05 12:40:37.576902: step 9360, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 75h:37m:11s remains)
INFO - root - 2017-12-05 12:40:46.128718: step 9370, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.831 sec/batch; 74h:35m:18s remains)
INFO - root - 2017-12-05 12:40:54.716780: step 9380, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 75h:48m:35s remains)
INFO - root - 2017-12-05 12:41:03.331314: step 9390, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 76h:19m:43s remains)
INFO - root - 2017-12-05 12:41:11.761933: step 9400, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.763 sec/batch; 68h:26m:35s remains)
2017-12-05 12:41:12.542923: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2790194 -4.257988 -4.2443857 -4.2389879 -4.241776 -4.2508125 -4.2592092 -4.2569308 -4.2479997 -4.2484603 -4.25163 -4.2562675 -4.2662945 -4.2755203 -4.2880425][-4.2554369 -4.2254372 -4.208446 -4.2031093 -4.21085 -4.2214832 -4.2286706 -4.2244024 -4.2122145 -4.2110629 -4.2124977 -4.2173734 -4.2282424 -4.2345471 -4.2462044][-4.2329197 -4.1921911 -4.1681247 -4.1620913 -4.1734247 -4.1848812 -4.1903172 -4.1833787 -4.1677208 -4.1700134 -4.1731386 -4.1775832 -4.1867719 -4.1878672 -4.2001143][-4.2034469 -4.1551638 -4.1281943 -4.1254511 -4.1406832 -4.1514606 -4.1507773 -4.1335964 -4.1117883 -4.1206326 -4.1303678 -4.1368876 -4.1461272 -4.1423364 -4.1535077][-4.1677666 -4.1220489 -4.0998259 -4.1041574 -4.1189847 -4.1186943 -4.0981078 -4.057683 -4.0282335 -4.0492635 -4.07421 -4.0935173 -4.1083617 -4.1038246 -4.1144514][-4.1469336 -4.1066222 -4.0892439 -4.0980873 -4.1069303 -4.0866432 -4.0389438 -3.9732587 -3.9327407 -3.9722521 -4.02061 -4.0596161 -4.0793409 -4.0738049 -4.08554][-4.1573248 -4.1259093 -4.1132479 -4.1186824 -4.1109061 -4.0622635 -3.9809427 -3.8868461 -3.8377416 -3.9004648 -3.9780071 -4.0389781 -4.0631623 -4.0590696 -4.076674][-4.1944623 -4.17415 -4.1638241 -4.1535664 -4.1202617 -4.044436 -3.9381473 -3.8218381 -3.7719626 -3.85958 -3.9600818 -4.0316186 -4.0604105 -4.0627527 -4.08668][-4.2331519 -4.2178364 -4.2031627 -4.1780539 -4.1271911 -4.0436621 -3.9395807 -3.835691 -3.8015306 -3.8945591 -3.9921207 -4.0535951 -4.0811281 -4.0891895 -4.11706][-4.2535729 -4.2358446 -4.21496 -4.1830058 -4.1309061 -4.0592666 -3.9792945 -3.9119091 -3.9034741 -3.9796059 -4.0545149 -4.0985732 -4.1165395 -4.1205783 -4.1454725][-4.2635536 -4.2401886 -4.2146506 -4.1817794 -4.1377139 -4.0843844 -4.0335546 -3.9960146 -4.0002918 -4.0551882 -4.1106315 -4.143774 -4.1533351 -4.1501732 -4.1619654][-4.2686877 -4.2396822 -4.2125707 -4.1851654 -4.154016 -4.1192069 -4.0893779 -4.0617585 -4.0603294 -4.0955024 -4.1423292 -4.1757894 -4.1843429 -4.17455 -4.16937][-4.2696495 -4.2365179 -4.2085452 -4.19092 -4.176578 -4.1591258 -4.1435623 -4.1134553 -4.0958896 -4.1133409 -4.1519189 -4.1834283 -4.19302 -4.1830997 -4.1693206][-4.2605648 -4.2213912 -4.1925559 -4.18327 -4.1852684 -4.1841908 -4.1804256 -4.151401 -4.1236939 -4.129035 -4.1581535 -4.1864142 -4.2020802 -4.1985154 -4.1839557][-4.2412434 -4.1949291 -4.164155 -4.1603208 -4.174377 -4.1854925 -4.1939173 -4.1747732 -4.1497512 -4.152844 -4.1723685 -4.1957579 -4.2173853 -4.2236114 -4.2171874]]...]
INFO - root - 2017-12-05 12:41:20.960026: step 9410, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 75h:23m:07s remains)
INFO - root - 2017-12-05 12:41:29.435466: step 9420, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 78h:26m:36s remains)
INFO - root - 2017-12-05 12:41:37.942860: step 9430, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 76h:48m:44s remains)
INFO - root - 2017-12-05 12:41:46.526449: step 9440, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 76h:44m:20s remains)
INFO - root - 2017-12-05 12:41:55.122728: step 9450, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 78h:00m:27s remains)
INFO - root - 2017-12-05 12:42:03.669637: step 9460, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.838 sec/batch; 75h:10m:01s remains)
INFO - root - 2017-12-05 12:42:12.139866: step 9470, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 75h:48m:59s remains)
INFO - root - 2017-12-05 12:42:20.681564: step 9480, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 73h:58m:35s remains)
INFO - root - 2017-12-05 12:42:29.246952: step 9490, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 76h:31m:30s remains)
INFO - root - 2017-12-05 12:42:37.807098: step 9500, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.764 sec/batch; 68h:33m:03s remains)
2017-12-05 12:42:38.674035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1817803 -4.2050343 -4.2242718 -4.2165833 -4.1734247 -4.1150742 -4.0485506 -4.012743 -4.027997 -4.0928845 -4.1657977 -4.2059464 -4.2330375 -4.2469993 -4.2455058][-4.2059035 -4.2227015 -4.2342167 -4.2191849 -4.179266 -4.1273489 -4.06396 -4.02819 -4.0322261 -4.0884514 -4.1669984 -4.2156348 -4.2483439 -4.2671089 -4.2662263][-4.2164993 -4.2287359 -4.2300825 -4.2078795 -4.1742339 -4.1299653 -4.072269 -4.0267987 -4.0082994 -4.0515709 -4.1361027 -4.1972475 -4.2407589 -4.2713079 -4.2761679][-4.213243 -4.2215033 -4.216989 -4.1926336 -4.1625447 -4.1276422 -4.0826964 -4.0291615 -3.9942312 -4.0232024 -4.1022658 -4.1696224 -4.22105 -4.2574034 -4.2688446][-4.1941686 -4.1993179 -4.1952143 -4.16986 -4.1367874 -4.1049557 -4.0775075 -4.0314112 -3.994343 -4.0093074 -4.0774093 -4.1423955 -4.1969934 -4.2406144 -4.2536325][-4.1674266 -4.1684179 -4.1636443 -4.1359415 -4.0942421 -4.0597544 -4.0406752 -4.0035734 -3.9663076 -3.9677742 -4.0304403 -4.0996723 -4.1615491 -4.213655 -4.2289658][-4.1420913 -4.1418195 -4.1293173 -4.0852695 -4.0297775 -3.9960177 -3.9864247 -3.9604692 -3.9249516 -3.9191151 -3.9740963 -4.0502243 -4.12216 -4.1808071 -4.1982417][-4.1178937 -4.1248655 -4.1159024 -4.0718608 -4.0170913 -3.9887838 -3.9837947 -3.965672 -3.9358823 -3.9338145 -3.977684 -4.0470052 -4.1173892 -4.1748552 -4.1946669][-4.1272225 -4.1432729 -4.1439004 -4.11129 -4.0677004 -4.02956 -4.0178838 -4.0031376 -3.9833057 -3.9900126 -4.0246553 -4.0820384 -4.1390262 -4.1824722 -4.1987033][-4.1614127 -4.1772041 -4.1800227 -4.1594052 -4.1313438 -4.09378 -4.0754895 -4.0653472 -4.0525641 -4.0566554 -4.079834 -4.1232677 -4.1651893 -4.1959281 -4.2079415][-4.1964159 -4.2055073 -4.2132187 -4.203721 -4.1852288 -4.1605468 -4.1528411 -4.1529326 -4.140996 -4.1338058 -4.142148 -4.1687574 -4.2022796 -4.2240057 -4.230063][-4.2194233 -4.2219815 -4.2294469 -4.225687 -4.2151008 -4.2028103 -4.2064753 -4.2172532 -4.2133064 -4.2066951 -4.2043014 -4.2167912 -4.2396159 -4.2507143 -4.2483654][-4.2279067 -4.223351 -4.229516 -4.2305074 -4.2227478 -4.2174683 -4.22865 -4.2454824 -4.2468948 -4.240665 -4.2399373 -4.2470207 -4.25949 -4.2636256 -4.2612128][-4.2401609 -4.2304668 -4.2284369 -4.2282076 -4.2226181 -4.2177629 -4.2270932 -4.2412963 -4.2455292 -4.2458816 -4.2487273 -4.2544956 -4.2602429 -4.2639775 -4.2648869][-4.2572994 -4.2475133 -4.2401252 -4.2353735 -4.230257 -4.2253337 -4.2287993 -4.2372918 -4.2434998 -4.2486506 -4.2533717 -4.2584457 -4.262526 -4.266511 -4.2703667]]...]
INFO - root - 2017-12-05 12:42:47.090876: step 9510, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 76h:30m:24s remains)
INFO - root - 2017-12-05 12:42:55.602626: step 9520, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 77h:58m:13s remains)
INFO - root - 2017-12-05 12:43:04.124576: step 9530, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 76h:42m:18s remains)
INFO - root - 2017-12-05 12:43:12.588047: step 9540, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 75h:17m:18s remains)
INFO - root - 2017-12-05 12:43:21.032469: step 9550, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 79h:34m:24s remains)
INFO - root - 2017-12-05 12:43:29.709486: step 9560, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.863 sec/batch; 77h:23m:12s remains)
INFO - root - 2017-12-05 12:43:38.385640: step 9570, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 76h:17m:56s remains)
INFO - root - 2017-12-05 12:43:46.901600: step 9580, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 79h:38m:12s remains)
INFO - root - 2017-12-05 12:43:55.552368: step 9590, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 75h:48m:19s remains)
INFO - root - 2017-12-05 12:44:04.002555: step 9600, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 0.794 sec/batch; 71h:15m:12s remains)
2017-12-05 12:44:04.782723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2922521 -4.3010173 -4.3068075 -4.3084941 -4.3074737 -4.3072443 -4.3070683 -4.3053131 -4.3034816 -4.3039603 -4.3061666 -4.3080473 -4.3098807 -4.3121676 -4.31458][-4.2701964 -4.2850204 -4.29475 -4.2969666 -4.2947407 -4.2948375 -4.2956147 -4.2922544 -4.2871757 -4.285799 -4.2878566 -4.2895303 -4.2911205 -4.2932992 -4.2969522][-4.2538819 -4.2728171 -4.2843561 -4.2847867 -4.279851 -4.2791429 -4.2797856 -4.273953 -4.26539 -4.2633348 -4.2665133 -4.2688403 -4.270627 -4.2738361 -4.2796121][-4.2544284 -4.2737517 -4.2852664 -4.2823377 -4.2721753 -4.2701583 -4.270349 -4.2637081 -4.2556791 -4.2555261 -4.2601056 -4.2653322 -4.2703471 -4.2763891 -4.2837234][-4.2561283 -4.2706456 -4.2778172 -4.2684736 -4.2540259 -4.25288 -4.2559786 -4.253881 -4.2507472 -4.2533288 -4.2590413 -4.2682261 -4.2784147 -4.2886438 -4.2966642][-4.2541571 -4.2599511 -4.2574897 -4.2395191 -4.22368 -4.2253966 -4.235672 -4.24223 -4.245872 -4.2528048 -4.260798 -4.2729907 -4.286087 -4.2989912 -4.3069859][-4.2568355 -4.2531996 -4.2415805 -4.2166033 -4.1988111 -4.2037725 -4.2199554 -4.233295 -4.2438836 -4.2568207 -4.2662182 -4.2769995 -4.2874079 -4.2981119 -4.3032537][-4.2620468 -4.2505631 -4.2332621 -4.2050858 -4.1870909 -4.1925511 -4.2097526 -4.2262878 -4.2440739 -4.2613249 -4.2683268 -4.2724376 -4.2749043 -4.2791867 -4.2797871][-4.2616739 -4.2461972 -4.22619 -4.1977348 -4.1805596 -4.1862736 -4.2014766 -4.2154317 -4.2351537 -4.2538719 -4.25841 -4.2559891 -4.248333 -4.2451758 -4.24431][-4.2503452 -4.23278 -4.2122507 -4.1893988 -4.1787372 -4.1870413 -4.1984406 -4.2053528 -4.2261543 -4.2478867 -4.2512341 -4.2429352 -4.2260771 -4.2162423 -4.2151432][-4.2308106 -4.2122326 -4.1951962 -4.1829534 -4.1858988 -4.2029133 -4.2121162 -4.2132525 -4.2331858 -4.2555423 -4.2582912 -4.2433414 -4.2182107 -4.2027917 -4.200295][-4.2077394 -4.1928225 -4.1826582 -4.1830506 -4.2002516 -4.2246971 -4.236248 -4.2391305 -4.257247 -4.2763543 -4.2781115 -4.2594404 -4.22783 -4.2028518 -4.1951032][-4.1974092 -4.1898446 -4.1902914 -4.2040467 -4.2307057 -4.2552576 -4.2659307 -4.2707124 -4.2828794 -4.2946196 -4.2926841 -4.2728095 -4.2383804 -4.2042613 -4.1879897][-4.2098475 -4.20627 -4.213038 -4.23588 -4.2644081 -4.2827811 -4.2881436 -4.2905335 -4.2935925 -4.2932816 -4.2861719 -4.2660484 -4.2324514 -4.1953611 -4.1728458][-4.2300663 -4.226439 -4.2354622 -4.2596903 -4.2820449 -4.2922392 -4.292737 -4.2918472 -4.2877688 -4.2776942 -4.2624583 -4.2398243 -4.2091193 -4.1755962 -4.15314]]...]
INFO - root - 2017-12-05 12:44:13.412709: step 9610, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 77h:36m:45s remains)
INFO - root - 2017-12-05 12:44:21.906261: step 9620, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 77h:09m:32s remains)
INFO - root - 2017-12-05 12:44:30.341958: step 9630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 75h:58m:34s remains)
INFO - root - 2017-12-05 12:44:38.873098: step 9640, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.830 sec/batch; 74h:27m:41s remains)
INFO - root - 2017-12-05 12:44:47.412743: step 9650, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 85h:55m:12s remains)
INFO - root - 2017-12-05 12:44:56.016447: step 9660, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 79h:08m:02s remains)
INFO - root - 2017-12-05 12:45:04.612154: step 9670, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 76h:27m:10s remains)
INFO - root - 2017-12-05 12:45:13.151433: step 9680, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 76h:13m:15s remains)
INFO - root - 2017-12-05 12:45:21.704681: step 9690, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 77h:23m:01s remains)
INFO - root - 2017-12-05 12:45:30.292085: step 9700, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.807 sec/batch; 72h:20m:26s remains)
2017-12-05 12:45:31.131721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2041936 -4.1990433 -4.1925035 -4.1909018 -4.1953769 -4.19584 -4.1831326 -4.1877942 -4.2074471 -4.2280922 -4.251471 -4.2783575 -4.2957754 -4.3039823 -4.3087621][-4.1656241 -4.1539865 -4.1365337 -4.1277075 -4.13692 -4.1359024 -4.1099153 -4.111177 -4.1422729 -4.1772518 -4.2156005 -4.2539544 -4.2797704 -4.293457 -4.3016253][-4.147543 -4.1285253 -4.1012945 -4.0891261 -4.0992489 -4.0884018 -4.0390587 -4.0323043 -4.0778875 -4.1289005 -4.1813221 -4.2289753 -4.2616181 -4.28054 -4.2934251][-4.1301565 -4.1040754 -4.0718641 -4.0583344 -4.0636444 -4.0355506 -3.9556315 -3.936115 -3.9987051 -4.0699115 -4.1388855 -4.1987505 -4.2402663 -4.2655153 -4.2842064][-4.1256251 -4.0975771 -4.061553 -4.0447416 -4.0360889 -3.9830523 -3.8614588 -3.8272302 -3.9232206 -4.0267458 -4.1141505 -4.1821084 -4.2278452 -4.2578506 -4.2807918][-4.1490235 -4.1223116 -4.0858169 -4.0540538 -4.01967 -3.9251053 -3.742203 -3.7057285 -3.8627586 -4.0079341 -4.1082368 -4.1793532 -4.2271872 -4.2600837 -4.2831874][-4.1705451 -4.1517582 -4.1183672 -4.06999 -4.0033541 -3.8620198 -3.6237278 -3.6008604 -3.8172195 -3.99433 -4.1008925 -4.1734152 -4.2273059 -4.2642345 -4.2858224][-4.1925378 -4.1877503 -4.1639657 -4.1148171 -4.0451403 -3.9099934 -3.7019291 -3.6904831 -3.8771257 -4.03189 -4.121779 -4.1866326 -4.2408652 -4.2754931 -4.2933655][-4.1952906 -4.2047758 -4.1923242 -4.1538815 -4.0899744 -3.9763134 -3.8141315 -3.8104076 -3.9514844 -4.0676632 -4.1369529 -4.19585 -4.25051 -4.2854118 -4.2996163][-4.1879106 -4.2083511 -4.2065163 -4.1805978 -4.1264896 -4.0307508 -3.9050925 -3.904043 -4.0170522 -4.1082764 -4.1622162 -4.2147636 -4.265378 -4.2949929 -4.3035436][-4.1795988 -4.2100606 -4.2209649 -4.2085528 -4.1691666 -4.0956893 -4.0021634 -3.9996421 -4.0850329 -4.1542711 -4.19283 -4.2380557 -4.2813435 -4.3032336 -4.3077183][-4.1827092 -4.2192106 -4.2392783 -4.2406511 -4.2188039 -4.1681595 -4.105032 -4.0991173 -4.1532173 -4.2003155 -4.226079 -4.260396 -4.2914586 -4.3068104 -4.31113][-4.1935778 -4.2309031 -4.2539587 -4.2618647 -4.2516117 -4.2190914 -4.1766043 -4.1655664 -4.2000985 -4.2348633 -4.2526164 -4.2760296 -4.2966647 -4.3078732 -4.3136725][-4.2023349 -4.2398434 -4.2666292 -4.2798262 -4.2770996 -4.2553682 -4.2247953 -4.2134633 -4.2347474 -4.2586112 -4.2694345 -4.2807245 -4.2946329 -4.3050156 -4.313086][-4.240818 -4.2728157 -4.2955494 -4.3079743 -4.3081942 -4.2924829 -4.2688684 -4.2572742 -4.2657285 -4.2770038 -4.2790165 -4.2810493 -4.28719 -4.2955127 -4.3076744]]...]
INFO - root - 2017-12-05 12:45:39.570704: step 9710, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 78h:26m:35s remains)
INFO - root - 2017-12-05 12:45:48.136062: step 9720, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 75h:57m:44s remains)
INFO - root - 2017-12-05 12:45:56.578098: step 9730, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 76h:03m:45s remains)
INFO - root - 2017-12-05 12:46:05.049128: step 9740, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 75h:59m:12s remains)
INFO - root - 2017-12-05 12:46:13.685345: step 9750, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 76h:44m:07s remains)
INFO - root - 2017-12-05 12:46:22.171892: step 9760, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.870 sec/batch; 77h:58m:36s remains)
INFO - root - 2017-12-05 12:46:30.631205: step 9770, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 74h:50m:47s remains)
INFO - root - 2017-12-05 12:46:39.114630: step 9780, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.823 sec/batch; 73h:49m:02s remains)
INFO - root - 2017-12-05 12:46:47.796329: step 9790, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.878 sec/batch; 78h:42m:56s remains)
INFO - root - 2017-12-05 12:46:56.381428: step 9800, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 73h:25m:13s remains)
2017-12-05 12:46:57.104112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2940426 -4.2919288 -4.2858081 -4.2818165 -4.2784696 -4.2747951 -4.2669096 -4.2602386 -4.2634206 -4.28037 -4.3065944 -4.3285193 -4.3402381 -4.3455625 -4.3520617][-4.2800555 -4.2769675 -4.2675533 -4.2609596 -4.2564459 -4.2472091 -4.23403 -4.2221022 -4.221796 -4.2421541 -4.2765975 -4.3072739 -4.3246202 -4.3336935 -4.3456616][-4.2679558 -4.2624731 -4.2481394 -4.235713 -4.2266264 -4.2123685 -4.1954026 -4.1822019 -4.1812625 -4.2021317 -4.2398772 -4.27705 -4.3010421 -4.3182111 -4.336978][-4.238317 -4.2297716 -4.2122712 -4.1940436 -4.1782341 -4.1613989 -4.143374 -4.131464 -4.1343255 -4.1579285 -4.199295 -4.2435441 -4.2753196 -4.3014565 -4.3283029][-4.1896348 -4.1766233 -4.1590796 -4.1343703 -4.10734 -4.0856452 -4.0654473 -4.0532846 -4.0638666 -4.0996037 -4.1551285 -4.2138114 -4.2560868 -4.2900944 -4.3229108][-4.1385651 -4.1272058 -4.1182866 -4.0893278 -4.0457029 -4.0136218 -3.990907 -3.9802253 -3.9959304 -4.0459995 -4.1212626 -4.1988316 -4.2544146 -4.2928925 -4.3259177][-4.0992041 -4.0977368 -4.1004024 -4.0673 -4.0048161 -3.9584012 -3.9349244 -3.9329996 -3.9564505 -4.0178723 -4.1096516 -4.2018847 -4.2659683 -4.3040361 -4.3341446][-4.0840678 -4.0910168 -4.097373 -4.0631828 -3.9929986 -3.93896 -3.914916 -3.9215989 -3.9536657 -4.0166712 -4.1098223 -4.203928 -4.2696924 -4.3093171 -4.3391733][-4.0808249 -4.0916824 -4.1010222 -4.0780063 -4.0245686 -3.9767323 -3.948426 -3.9502435 -3.9796615 -4.0333905 -4.1121545 -4.19734 -4.261488 -4.3038392 -4.3364639][-4.0733786 -4.0843306 -4.0980611 -4.0956869 -4.0732083 -4.0462651 -4.0145974 -4.0040331 -4.0258846 -4.0648193 -4.120193 -4.1863661 -4.2443252 -4.2870641 -4.3248482][-4.0830188 -4.0953174 -4.113946 -4.12292 -4.1151872 -4.0963159 -4.0633454 -4.0470762 -4.0652528 -4.0955305 -4.1326332 -4.1798277 -4.2290978 -4.2696567 -4.3102984][-4.1124749 -4.1267257 -4.1454072 -4.1535621 -4.1420341 -4.1160407 -4.0769682 -4.0583835 -4.0759788 -4.109149 -4.1447678 -4.1846213 -4.226419 -4.2622581 -4.3021159][-4.1491466 -4.1595788 -4.1722403 -4.1772676 -4.1634483 -4.1336031 -4.095674 -4.076076 -4.0905728 -4.1253395 -4.1646886 -4.202508 -4.2392282 -4.27065 -4.3067694][-4.1973839 -4.2036548 -4.2100277 -4.21247 -4.1995096 -4.1721096 -4.1405268 -4.1234655 -4.1337972 -4.16555 -4.2026572 -4.2353096 -4.2674031 -4.2957692 -4.3246126][-4.2521152 -4.2553315 -4.2556953 -4.2561579 -4.2468071 -4.2282948 -4.2080574 -4.1961846 -4.2026515 -4.2267785 -4.2556362 -4.2799988 -4.3052125 -4.3283095 -4.3477445]]...]
INFO - root - 2017-12-05 12:47:05.692031: step 9810, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 75h:47m:19s remains)
INFO - root - 2017-12-05 12:47:14.347065: step 9820, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 76h:47m:36s remains)
INFO - root - 2017-12-05 12:47:22.871647: step 9830, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 75h:32m:40s remains)
INFO - root - 2017-12-05 12:47:31.286344: step 9840, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 76h:27m:57s remains)
INFO - root - 2017-12-05 12:47:39.870074: step 9850, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.865 sec/batch; 77h:29m:12s remains)
INFO - root - 2017-12-05 12:47:48.454527: step 9860, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.896 sec/batch; 80h:20m:42s remains)
INFO - root - 2017-12-05 12:47:57.076300: step 9870, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 77h:02m:03s remains)
INFO - root - 2017-12-05 12:48:05.601041: step 9880, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 76h:57m:31s remains)
INFO - root - 2017-12-05 12:48:14.111869: step 9890, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 75h:35m:14s remains)
INFO - root - 2017-12-05 12:48:22.664825: step 9900, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.792 sec/batch; 70h:59m:07s remains)
2017-12-05 12:48:23.372664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2663927 -4.2543626 -4.2501488 -4.2540522 -4.2634392 -4.2786336 -4.28747 -4.2869496 -4.2861862 -4.2904119 -4.2944326 -4.2969041 -4.3012214 -4.2996359 -4.2932405][-4.2493629 -4.2328796 -4.2233844 -4.2211428 -4.2254238 -4.2407665 -4.252378 -4.252275 -4.2535052 -4.2620625 -4.2702661 -4.2759461 -4.2822285 -4.2822781 -4.2769685][-4.2342839 -4.2146015 -4.1997423 -4.1877985 -4.1812153 -4.189817 -4.1972718 -4.1950555 -4.1978321 -4.2109137 -4.2252522 -4.2382479 -4.2539153 -4.26164 -4.2614217][-4.222909 -4.2012105 -4.1811976 -4.16128 -4.1477542 -4.1489172 -4.1433754 -4.1317039 -4.1347027 -4.1508274 -4.1718836 -4.1961842 -4.2257576 -4.2430606 -4.2468429][-4.2145844 -4.1876292 -4.1616716 -4.1361933 -4.1196241 -4.1122942 -4.0870943 -4.0584297 -4.062005 -4.0845456 -4.1169586 -4.159296 -4.2017684 -4.2249875 -4.2293181][-4.210567 -4.1772013 -4.1427231 -4.1092629 -4.0848837 -4.0608511 -4.0116158 -3.9652419 -3.9765599 -4.0173092 -4.0733042 -4.1393542 -4.1915588 -4.2166481 -4.2197552][-4.2162027 -4.1815219 -4.1408887 -4.0993023 -4.0648885 -4.0223103 -3.9471734 -3.8841238 -3.9116058 -3.9773448 -4.0559826 -4.1386762 -4.1967754 -4.2238212 -4.2255206][-4.2270417 -4.196425 -4.1604362 -4.1234374 -4.0893011 -4.0375981 -3.9466562 -3.8794324 -3.9174724 -3.9939182 -4.0727983 -4.1504626 -4.2054539 -4.2362638 -4.2430768][-4.2398577 -4.2149777 -4.1882319 -4.1608958 -4.1401081 -4.1017509 -4.0282822 -3.9773731 -4.00737 -4.0641661 -4.1176348 -4.1697607 -4.2120876 -4.2453885 -4.2591619][-4.2421908 -4.218677 -4.1940365 -4.1766338 -4.1746454 -4.1604958 -4.1178942 -4.0830793 -4.0968304 -4.1302261 -4.1597414 -4.1895752 -4.218986 -4.2489138 -4.264122][-4.24451 -4.219192 -4.1940866 -4.1815567 -4.1895061 -4.1904416 -4.171309 -4.14885 -4.1526327 -4.17137 -4.1906619 -4.2113833 -4.2329106 -4.2548552 -4.2674894][-4.2593074 -4.235714 -4.2124915 -4.2024279 -4.2087288 -4.2121983 -4.2046356 -4.1931505 -4.1921597 -4.1995931 -4.2116919 -4.2285581 -4.2461376 -4.2629738 -4.2743688][-4.2787 -4.2593775 -4.2405477 -4.2310829 -4.2307096 -4.2301764 -4.2265 -4.2219057 -4.2182021 -4.2160735 -4.2211976 -4.23853 -4.2584038 -4.273458 -4.2840347][-4.2917557 -4.2760839 -4.2624111 -4.2548666 -4.2527342 -4.25204 -4.2497931 -4.2460403 -4.2403383 -4.2319026 -4.2333083 -4.2516747 -4.2708454 -4.2833886 -4.2909703][-4.2954569 -4.283587 -4.2736387 -4.2701569 -4.2717605 -4.274786 -4.2767353 -4.2749681 -4.2687516 -4.2580786 -4.2569695 -4.2710576 -4.2855263 -4.2942739 -4.2983046]]...]
INFO - root - 2017-12-05 12:48:31.828183: step 9910, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 73h:57m:22s remains)
INFO - root - 2017-12-05 12:48:40.291851: step 9920, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 75h:12m:02s remains)
INFO - root - 2017-12-05 12:48:48.784929: step 9930, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 74h:11m:49s remains)
INFO - root - 2017-12-05 12:48:57.334672: step 9940, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 79h:32m:28s remains)
INFO - root - 2017-12-05 12:49:05.726225: step 9950, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 76h:42m:31s remains)
INFO - root - 2017-12-05 12:49:14.437857: step 9960, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 79h:09m:17s remains)
INFO - root - 2017-12-05 12:49:22.925815: step 9970, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 77h:33m:07s remains)
INFO - root - 2017-12-05 12:49:31.516393: step 9980, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 78h:45m:24s remains)
INFO - root - 2017-12-05 12:49:40.154438: step 9990, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 76h:46m:59s remains)
INFO - root - 2017-12-05 12:49:48.615603: step 10000, loss = 2.08, batch loss = 2.03 (10.4 examples/sec; 0.769 sec/batch; 68h:53m:45s remains)
2017-12-05 12:49:49.350850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.156394 -4.1639495 -4.1709752 -4.1758018 -4.180162 -4.1783986 -4.176527 -4.17391 -4.1660705 -4.1574826 -4.1489716 -4.1375117 -4.1330266 -4.1379938 -4.1524615][-4.1555495 -4.1675072 -4.1815104 -4.192071 -4.1983433 -4.19898 -4.1993065 -4.1993957 -4.194623 -4.1872935 -4.1822157 -4.1711779 -4.1611733 -4.15788 -4.169313][-4.1583486 -4.1765709 -4.1939812 -4.2023687 -4.2042756 -4.203805 -4.2040172 -4.2046771 -4.20192 -4.2009206 -4.2042823 -4.1959457 -4.1851063 -4.1817684 -4.1909161][-4.1622386 -4.1847653 -4.2033267 -4.2084475 -4.2023878 -4.1959043 -4.1925135 -4.1892462 -4.1845517 -4.18888 -4.2022333 -4.1995287 -4.1901588 -4.1897264 -4.2001843][-4.1597853 -4.1766305 -4.1864934 -4.1858821 -4.1694417 -4.1507134 -4.138792 -4.1237268 -4.1137447 -4.1264796 -4.1542554 -4.1693983 -4.1735921 -4.1828833 -4.199369][-4.15111 -4.15852 -4.1555724 -4.1411567 -4.1124034 -4.0832496 -4.05682 -4.0226183 -4.0049295 -4.032918 -4.0786605 -4.1135964 -4.13362 -4.1624317 -4.1951976][-4.1470571 -4.1380563 -4.1169791 -4.0841646 -4.0385308 -3.9933593 -3.9413075 -3.8718135 -3.8447075 -3.91073 -3.9911592 -4.04382 -4.0821333 -4.1330385 -4.1852984][-4.1524048 -4.1279445 -4.0863142 -4.0273323 -3.9578836 -3.8913274 -3.8073573 -3.6921813 -3.6654239 -3.7916567 -3.911505 -3.9775043 -4.02704 -4.0951061 -4.1621976][-4.1621161 -4.1369243 -4.0928659 -4.0275569 -3.9570079 -3.8903189 -3.8120358 -3.7117796 -3.7025998 -3.8160791 -3.9109802 -3.9518476 -3.9910252 -4.0611587 -4.134294][-4.1581144 -4.1437497 -4.1135087 -4.0709963 -4.0249753 -3.981992 -3.9392385 -3.89479 -3.896956 -3.9491956 -3.9818549 -3.977927 -3.9891369 -4.0425339 -4.1134281][-4.1454449 -4.1400466 -4.1271119 -4.1121969 -4.091907 -4.0689015 -4.0508695 -4.0382285 -4.0397878 -4.055614 -4.0539074 -4.0339022 -4.0372233 -4.0778451 -4.1357131][-4.1175652 -4.1332226 -4.1431456 -4.1463461 -4.1443415 -4.1350107 -4.1300349 -4.12876 -4.1239429 -4.1138391 -4.0935225 -4.077105 -4.0863423 -4.1264677 -4.1746135][-4.1024604 -4.1343627 -4.16151 -4.1744056 -4.1770797 -4.172771 -4.1657524 -4.1664891 -4.1585531 -4.1363811 -4.1106992 -4.1028657 -4.1187735 -4.155303 -4.19447][-4.1112309 -4.1410537 -4.1663451 -4.1788192 -4.1762705 -4.166471 -4.150671 -4.1459265 -4.1402621 -4.1282196 -4.1146488 -4.1210017 -4.1426744 -4.1717024 -4.1997743][-4.119287 -4.1418 -4.1616282 -4.1696076 -4.1659031 -4.1548839 -4.1343718 -4.1260653 -4.1254787 -4.1211834 -4.1128678 -4.1197753 -4.1370397 -4.1600885 -4.1872482]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 12:49:58.446237: step 10010, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 74h:55m:24s remains)
INFO - root - 2017-12-05 12:50:06.898767: step 10020, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 74h:06m:09s remains)
INFO - root - 2017-12-05 12:50:15.235947: step 10030, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 75h:07m:42s remains)
INFO - root - 2017-12-05 12:50:23.721772: step 10040, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 76h:26m:02s remains)
INFO - root - 2017-12-05 12:50:32.266340: step 10050, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 76h:06m:37s remains)
INFO - root - 2017-12-05 12:50:40.586026: step 10060, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 74h:52m:38s remains)
INFO - root - 2017-12-05 12:50:49.107960: step 10070, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 76h:47m:35s remains)
INFO - root - 2017-12-05 12:50:57.810261: step 10080, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 74h:17m:22s remains)
INFO - root - 2017-12-05 12:51:06.346019: step 10090, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 75h:18m:26s remains)
INFO - root - 2017-12-05 12:51:14.886711: step 10100, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.765 sec/batch; 68h:29m:49s remains)
2017-12-05 12:51:15.648045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2229414 -4.2226958 -4.232161 -4.2609477 -4.285419 -4.3046522 -4.3174124 -4.3154845 -4.3047838 -4.2890739 -4.2741947 -4.2695913 -4.2737875 -4.2810869 -4.2881002][-4.213706 -4.20335 -4.2013373 -4.2215858 -4.24566 -4.2708669 -4.29303 -4.3007278 -4.2946749 -4.2776952 -4.2589078 -4.2508249 -4.2507954 -4.2542176 -4.2642207][-4.2075744 -4.186492 -4.1718378 -4.18054 -4.1998558 -4.2246275 -4.2503939 -4.265543 -4.2673278 -4.254519 -4.2358122 -4.2234926 -4.2176619 -4.2192636 -4.2352591][-4.2027082 -4.1717572 -4.1480694 -4.1464319 -4.1554685 -4.1699882 -4.1878872 -4.2050657 -4.2167654 -4.2168837 -4.2053547 -4.1935811 -4.186645 -4.1898451 -4.208199][-4.1946163 -4.1545706 -4.1213813 -4.1063061 -4.0944271 -4.08479 -4.083766 -4.0998182 -4.131062 -4.1591825 -4.1680245 -4.1664424 -4.1684785 -4.1801462 -4.1982503][-4.19542 -4.1512084 -4.1090035 -4.076551 -4.0352111 -3.9865029 -3.9476156 -3.9529748 -4.0110779 -4.0809317 -4.1264915 -4.148756 -4.167942 -4.1890888 -4.2033753][-4.2040877 -4.163816 -4.117033 -4.0670695 -3.9940319 -3.9007115 -3.8124762 -3.7985435 -3.8834629 -3.9971564 -4.0856833 -4.1375432 -4.1762872 -4.2050481 -4.2179532][-4.2136073 -4.1830344 -4.1435752 -4.0931911 -4.0097027 -3.8940506 -3.7774708 -3.7441378 -3.8246748 -3.944391 -4.0484705 -4.1183238 -4.1715469 -4.2081661 -4.2277842][-4.207849 -4.18958 -4.1654725 -4.1349659 -4.07294 -3.9801798 -3.882853 -3.8433237 -3.88802 -3.9713087 -4.0553341 -4.1175556 -4.1652694 -4.200047 -4.221705][-4.180191 -4.1747313 -4.1700177 -4.1694946 -4.1408887 -4.0833979 -4.0184717 -3.9833391 -4.0007148 -4.0482259 -4.0998268 -4.1365609 -4.1625142 -4.1801844 -4.1920609][-4.1513939 -4.1534195 -4.1665626 -4.1972742 -4.2025523 -4.1805973 -4.1473951 -4.1226888 -4.1249356 -4.1441221 -4.160593 -4.1641564 -4.1634064 -4.1568937 -4.1489263][-4.1485367 -4.1543012 -4.1762619 -4.2222338 -4.2463965 -4.2458649 -4.2336845 -4.21855 -4.2132983 -4.2153068 -4.2080226 -4.1881251 -4.1673522 -4.1410065 -4.114666][-4.1777534 -4.1881094 -4.2115135 -4.2579513 -4.283812 -4.287653 -4.2788692 -4.2612863 -4.2470174 -4.234406 -4.2117949 -4.1832261 -4.1590819 -4.1336112 -4.1082821][-4.2303071 -4.24186 -4.262145 -4.298758 -4.3135633 -4.3081479 -4.2922292 -4.267725 -4.2422905 -4.2140369 -4.181128 -4.1525974 -4.1333232 -4.1214066 -4.1146479][-4.2790108 -4.2895308 -4.3047533 -4.3283691 -4.3298416 -4.3158145 -4.2956214 -4.2675362 -4.2326927 -4.1917758 -4.15401 -4.1294703 -4.1182685 -4.1210284 -4.1330271]]...]
INFO - root - 2017-12-05 12:51:24.174872: step 10110, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 77h:20m:19s remains)
INFO - root - 2017-12-05 12:51:32.805488: step 10120, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 75h:30m:40s remains)
INFO - root - 2017-12-05 12:51:41.293679: step 10130, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.816 sec/batch; 73h:01m:49s remains)
INFO - root - 2017-12-05 12:51:49.830128: step 10140, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 77h:43m:44s remains)
INFO - root - 2017-12-05 12:51:58.469955: step 10150, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 77h:34m:14s remains)
INFO - root - 2017-12-05 12:52:06.966439: step 10160, loss = 2.02, batch loss = 1.97 (10.2 examples/sec; 0.783 sec/batch; 70h:08m:16s remains)
INFO - root - 2017-12-05 12:52:15.457096: step 10170, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 75h:57m:54s remains)
INFO - root - 2017-12-05 12:52:23.953580: step 10180, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 74h:58m:55s remains)
INFO - root - 2017-12-05 12:52:32.518806: step 10190, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 77h:32m:15s remains)
INFO - root - 2017-12-05 12:52:40.987617: step 10200, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.751 sec/batch; 67h:14m:08s remains)
2017-12-05 12:52:41.723985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2898803 -4.2946486 -4.2911773 -4.2876773 -4.2894917 -4.2995811 -4.3114333 -4.3207994 -4.3242712 -4.3191319 -4.310966 -4.3060379 -4.3053708 -4.3087482 -4.3146262][-4.3109288 -4.3209176 -4.3183479 -4.311789 -4.3101335 -4.3145652 -4.3176885 -4.3172779 -4.312047 -4.3025289 -4.2956886 -4.2935629 -4.2958903 -4.3020706 -4.310071][-4.3132157 -4.32619 -4.324028 -4.3156819 -4.3093419 -4.3071642 -4.3024907 -4.292007 -4.2770314 -4.2671061 -4.2672677 -4.272049 -4.2810216 -4.2922812 -4.3016562][-4.2993841 -4.3150253 -4.3148341 -4.3070836 -4.2991996 -4.2925053 -4.281888 -4.2599249 -4.2330656 -4.2224412 -4.2338791 -4.2485013 -4.2634439 -4.2759914 -4.2818942][-4.2857013 -4.3040581 -4.3058119 -4.2979827 -4.2882581 -4.2774849 -4.2582521 -4.2215595 -4.1813149 -4.1720576 -4.198442 -4.2241783 -4.2427588 -4.2527113 -4.2531075][-4.2763681 -4.2982988 -4.3023815 -4.2918582 -4.2743149 -4.2507806 -4.21299 -4.1545911 -4.0999374 -4.1020713 -4.1535325 -4.2003078 -4.2302623 -4.2397022 -4.2345986][-4.2753716 -4.2982688 -4.3000879 -4.2810583 -4.248044 -4.2046795 -4.1425667 -4.0581126 -3.9918818 -4.015708 -4.1000323 -4.1750655 -4.2237725 -4.2374792 -4.2300496][-4.2794795 -4.3017778 -4.2977767 -4.2680016 -4.2216268 -4.1641374 -4.0864677 -3.9892018 -3.925535 -3.9741273 -4.0806479 -4.1730118 -4.2299843 -4.24397 -4.2318382][-4.2861347 -4.3078475 -4.299284 -4.2642841 -4.2134418 -4.1570683 -4.0883355 -4.0100961 -3.9725628 -4.0258956 -4.1228061 -4.2052116 -4.2511683 -4.2525349 -4.2299614][-4.3039513 -4.3212886 -4.3088775 -4.2737513 -4.2269969 -4.1817856 -4.1354418 -4.09138 -4.0784049 -4.1187963 -4.1870193 -4.2435842 -4.268878 -4.2552023 -4.2249584][-4.3285193 -4.3403091 -4.3269591 -4.2958817 -4.2570934 -4.224565 -4.1991434 -4.1805086 -4.1794577 -4.2044868 -4.2440324 -4.2759366 -4.2837338 -4.2638249 -4.2350016][-4.3453455 -4.3523631 -4.3420191 -4.3176403 -4.2872148 -4.2656426 -4.2539396 -4.2488203 -4.2506337 -4.2651339 -4.2862582 -4.3027391 -4.3033051 -4.2855535 -4.2653704][-4.3463373 -4.350801 -4.3443956 -4.3254108 -4.3026929 -4.2897787 -4.2862067 -4.2858586 -4.2888265 -4.2991643 -4.3126354 -4.323245 -4.3225713 -4.3086843 -4.2953744][-4.3363543 -4.33722 -4.3303514 -4.3129864 -4.2956886 -4.2889438 -4.2904849 -4.2925396 -4.2964253 -4.3053384 -4.3142371 -4.3232379 -4.3245606 -4.3159337 -4.3094187][-4.3183479 -4.3149071 -4.3033648 -4.2843609 -4.2681866 -4.261941 -4.2656264 -4.2686324 -4.27353 -4.2803273 -4.2857227 -4.296783 -4.3068438 -4.3079114 -4.3094273]]...]
INFO - root - 2017-12-05 12:52:50.324640: step 10210, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 76h:23m:43s remains)
INFO - root - 2017-12-05 12:52:58.892798: step 10220, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 74h:01m:08s remains)
INFO - root - 2017-12-05 12:53:07.438592: step 10230, loss = 2.11, batch loss = 2.05 (9.1 examples/sec; 0.877 sec/batch; 78h:32m:45s remains)
INFO - root - 2017-12-05 12:53:16.202101: step 10240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 77h:05m:59s remains)
INFO - root - 2017-12-05 12:53:24.730598: step 10250, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 76h:20m:15s remains)
INFO - root - 2017-12-05 12:53:33.244945: step 10260, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 73h:29m:24s remains)
INFO - root - 2017-12-05 12:53:41.758279: step 10270, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 77h:46m:31s remains)
INFO - root - 2017-12-05 12:53:50.303820: step 10280, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.853 sec/batch; 76h:21m:29s remains)
INFO - root - 2017-12-05 12:53:58.916237: step 10290, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 78h:31m:38s remains)
INFO - root - 2017-12-05 12:54:07.426169: step 10300, loss = 2.05, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 66h:41m:39s remains)
2017-12-05 12:54:08.137462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2239714 -4.2289195 -4.2316751 -4.2340527 -4.2355766 -4.2359271 -4.2364225 -4.2348795 -4.2296147 -4.2203035 -4.20783 -4.1974368 -4.1930213 -4.2032804 -4.2295408][-4.2228584 -4.2278523 -4.2315183 -4.2354584 -4.2372136 -4.235219 -4.2338247 -4.2340703 -4.2344227 -4.2309675 -4.2250776 -4.21772 -4.212472 -4.220325 -4.2417307][-4.2186775 -4.2251921 -4.2318983 -4.2387204 -4.2382441 -4.230823 -4.2245831 -4.2267036 -4.236722 -4.2435861 -4.2441254 -4.2390666 -4.23237 -4.2374582 -4.2555723][-4.188396 -4.197175 -4.207881 -4.2156539 -4.2103076 -4.1939673 -4.1778 -4.1813097 -4.2076421 -4.2334018 -4.245646 -4.2443309 -4.2342091 -4.2331033 -4.2474074][-4.1340537 -4.1483989 -4.1670189 -4.1779628 -4.1688809 -4.1356621 -4.0977535 -4.0937915 -4.1379638 -4.1913834 -4.2231708 -4.2282352 -4.21538 -4.2082591 -4.2218938][-4.0752096 -4.0991144 -4.126667 -4.1397958 -4.1214767 -4.0656881 -3.9975183 -3.9764619 -4.0347342 -4.1158595 -4.1723609 -4.1911645 -4.1835852 -4.1805024 -4.200995][-4.0190849 -4.0599775 -4.1087856 -4.1333332 -4.1111245 -4.0359268 -3.935585 -3.8856485 -3.9426677 -4.0354209 -4.1080837 -4.1456032 -4.1556149 -4.1676712 -4.199779][-3.9783647 -4.0307689 -4.1026783 -4.148994 -4.1411533 -4.0772696 -3.986084 -3.9265826 -3.9530239 -4.0174847 -4.0792823 -4.1252542 -4.1525784 -4.1845765 -4.227263][-3.9888787 -4.0409331 -4.1136808 -4.16651 -4.1727967 -4.1354561 -4.0798397 -4.0333967 -4.0372071 -4.066041 -4.1024251 -4.1393147 -4.1710544 -4.2148266 -4.2624412][-4.0425172 -4.0875163 -4.1471229 -4.1917048 -4.1992273 -4.1792474 -4.1521187 -4.1244612 -4.1211257 -4.1265674 -4.1373134 -4.1596427 -4.1887822 -4.234129 -4.2820978][-4.0876727 -4.1304631 -4.1853828 -4.2269669 -4.2358603 -4.2237177 -4.2083182 -4.1866808 -4.1747646 -4.1616268 -4.15426 -4.1664619 -4.1903243 -4.2329288 -4.2806625][-4.1054974 -4.1471715 -4.2035933 -4.248785 -4.2645469 -4.2610321 -4.2524405 -4.2327776 -4.2148423 -4.192224 -4.1765046 -4.1804481 -4.1966171 -4.2314939 -4.2735333][-4.1053557 -4.141891 -4.1981339 -4.2510219 -4.2816076 -4.2918234 -4.2925334 -4.2786903 -4.25869 -4.2321911 -4.2115955 -4.2071705 -4.2148442 -4.2374825 -4.269259][-4.0946932 -4.1208014 -4.17578 -4.2337103 -4.2775707 -4.30183 -4.3110294 -4.3000164 -4.2757487 -4.2481174 -4.227427 -4.2179909 -4.2204189 -4.2368588 -4.2624531][-4.0905871 -4.1070027 -4.1550794 -4.2116766 -4.2624593 -4.2957611 -4.3098783 -4.3012137 -4.2753162 -4.2466397 -4.2278037 -4.2178988 -4.216814 -4.2281666 -4.2512889]]...]
INFO - root - 2017-12-05 12:54:16.704198: step 10310, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 76h:07m:23s remains)
INFO - root - 2017-12-05 12:54:25.287440: step 10320, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 76h:31m:08s remains)
INFO - root - 2017-12-05 12:54:33.788045: step 10330, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 76h:13m:04s remains)
INFO - root - 2017-12-05 12:54:42.522177: step 10340, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 77h:01m:32s remains)
INFO - root - 2017-12-05 12:54:51.084956: step 10350, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 75h:48m:25s remains)
INFO - root - 2017-12-05 12:54:59.796735: step 10360, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 76h:46m:08s remains)
INFO - root - 2017-12-05 12:55:08.382710: step 10370, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 76h:14m:12s remains)
INFO - root - 2017-12-05 12:55:16.827870: step 10380, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.807 sec/batch; 72h:11m:10s remains)
INFO - root - 2017-12-05 12:55:25.280336: step 10390, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 75h:55m:52s remains)
INFO - root - 2017-12-05 12:55:33.780421: step 10400, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.794 sec/batch; 71h:00m:21s remains)
2017-12-05 12:55:34.523799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2460928 -4.2341366 -4.2225819 -4.21642 -4.2126274 -4.2064657 -4.1947312 -4.1905985 -4.2088113 -4.2302475 -4.2407875 -4.2367244 -4.224256 -4.2155304 -4.2200971][-4.2349758 -4.2209768 -4.2052751 -4.1933336 -4.1815553 -4.1670489 -4.1491094 -4.14588 -4.1725173 -4.2070017 -4.2242374 -4.2206945 -4.2043581 -4.1948504 -4.2015138][-4.2155442 -4.2032523 -4.1881223 -4.171874 -4.1521 -4.1266255 -4.1006413 -4.0975051 -4.1355348 -4.1857467 -4.2106934 -4.2115602 -4.1974564 -4.1877131 -4.1922774][-4.1950188 -4.1855178 -4.173605 -4.1558509 -4.1276355 -4.0885067 -4.0527658 -4.0508051 -4.1014309 -4.1641192 -4.1972475 -4.21082 -4.20972 -4.2047582 -4.2071395][-4.1847148 -4.1775532 -4.1665077 -4.1435094 -4.1032915 -4.0490584 -4.006669 -4.0106616 -4.0758233 -4.1460934 -4.1858354 -4.2113342 -4.2233152 -4.2271357 -4.2303267][-4.1806121 -4.1771822 -4.166451 -4.135232 -4.0793304 -4.0063586 -3.9499578 -3.9605637 -4.0466266 -4.1286454 -4.1752272 -4.2091374 -4.2284465 -4.2397823 -4.244432][-4.1758609 -4.1785254 -4.1682959 -4.1250629 -4.0489807 -3.9514394 -3.8676937 -3.8840346 -3.9942484 -4.0906873 -4.1506529 -4.1925292 -4.219728 -4.2392182 -4.2443819][-4.1657829 -4.181499 -4.1781716 -4.127564 -4.0368495 -3.9196048 -3.8100591 -3.8246579 -3.9407921 -4.0410748 -4.111784 -4.1620169 -4.1978912 -4.2231445 -4.2267518][-4.1569114 -4.1857948 -4.1897235 -4.1446347 -4.0636482 -3.956732 -3.8575826 -3.8570976 -3.9340274 -4.0099106 -4.0738444 -4.1236935 -4.1621184 -4.1868258 -4.1865559][-4.1530523 -4.1877923 -4.1970882 -4.1671863 -4.1136155 -4.0400095 -3.9716594 -3.9555132 -3.9774878 -4.0118628 -4.0518227 -4.0871744 -4.1201491 -4.1435132 -4.1407833][-4.1597552 -4.1890192 -4.2013831 -4.1899166 -4.1628513 -4.1204691 -4.0745211 -4.0488558 -4.0353088 -4.0359592 -4.043942 -4.0567617 -4.0794096 -4.1005073 -4.0993314][-4.1831026 -4.2044516 -4.2127457 -4.2103043 -4.1989155 -4.1766219 -4.146996 -4.1200519 -4.0936446 -4.07567 -4.0571947 -4.0450311 -4.0499616 -4.0622611 -4.0671477][-4.2100224 -4.2240672 -4.2236791 -4.219985 -4.2151966 -4.2052617 -4.1886539 -4.1692204 -4.1455278 -4.1192646 -4.0852308 -4.0542707 -4.0406151 -4.0431604 -4.0545487][-4.2315559 -4.2408895 -4.2322249 -4.2202611 -4.2160883 -4.2149639 -4.20873 -4.1976519 -4.1780281 -4.14688 -4.1027203 -4.0586629 -4.0350709 -4.0360708 -4.057023][-4.2348051 -4.2481155 -4.2397494 -4.2232504 -4.2184768 -4.2225776 -4.2250328 -4.2188129 -4.2001176 -4.1664009 -4.1165004 -4.06328 -4.0369148 -4.0422392 -4.0710077]]...]
INFO - root - 2017-12-05 12:55:42.964725: step 10410, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 76h:31m:09s remains)
INFO - root - 2017-12-05 12:55:51.499337: step 10420, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 74h:26m:35s remains)
INFO - root - 2017-12-05 12:56:00.082130: step 10430, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.881 sec/batch; 78h:51m:33s remains)
INFO - root - 2017-12-05 12:56:08.607744: step 10440, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 77h:40m:17s remains)
INFO - root - 2017-12-05 12:56:17.267670: step 10450, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.877 sec/batch; 78h:26m:08s remains)
INFO - root - 2017-12-05 12:56:25.798227: step 10460, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 76h:09m:00s remains)
INFO - root - 2017-12-05 12:56:34.382515: step 10470, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 75h:40m:34s remains)
INFO - root - 2017-12-05 12:56:42.991383: step 10480, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.796 sec/batch; 71h:14m:09s remains)
INFO - root - 2017-12-05 12:56:51.407681: step 10490, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 74h:14m:39s remains)
INFO - root - 2017-12-05 12:56:59.857053: step 10500, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.792 sec/batch; 70h:49m:04s remains)
2017-12-05 12:57:00.600552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2948942 -4.2928052 -4.2878213 -4.2788863 -4.2744589 -4.2796268 -4.2898431 -4.3012595 -4.3087182 -4.3122797 -4.3145351 -4.3154058 -4.3111062 -4.2980094 -4.2664342][-4.2750645 -4.2738662 -4.2714448 -4.267767 -4.2707553 -4.2818756 -4.2951651 -4.3076882 -4.3137879 -4.3144603 -4.3184357 -4.3210597 -4.312994 -4.2913918 -4.2515264][-4.2520189 -4.2527809 -4.2553878 -4.2599187 -4.2712746 -4.2861853 -4.2985582 -4.3090787 -4.3130846 -4.3136835 -4.3196549 -4.3210955 -4.3073654 -4.2767873 -4.2325492][-4.2232213 -4.2300868 -4.2409992 -4.2539673 -4.2718158 -4.2880049 -4.2978215 -4.3053041 -4.3076897 -4.3095889 -4.3163276 -4.3117361 -4.2893839 -4.2518578 -4.2114749][-4.1904125 -4.2074161 -4.2300787 -4.2518163 -4.2748528 -4.2902336 -4.2954926 -4.2957315 -4.2912488 -4.2908831 -4.2929826 -4.2799277 -4.2514286 -4.2148409 -4.1893821][-4.1634111 -4.1919985 -4.2260747 -4.2539039 -4.2760682 -4.2855139 -4.2798524 -4.2640996 -4.245882 -4.2419987 -4.2418165 -4.2286496 -4.206296 -4.1843066 -4.1815405][-4.1588788 -4.1917033 -4.2269974 -4.2512441 -4.2654476 -4.2628288 -4.2414637 -4.2044778 -4.175487 -4.1762538 -4.1834269 -4.1816521 -4.1775284 -4.1785231 -4.1954408][-4.1753759 -4.2010732 -4.2284479 -4.2445178 -4.2488885 -4.2345161 -4.1981072 -4.1413717 -4.1076636 -4.122756 -4.1480031 -4.1648216 -4.1810837 -4.1992812 -4.2249312][-4.20881 -4.2199697 -4.2356863 -4.2427044 -4.2380471 -4.2155743 -4.1694007 -4.1043162 -4.0783629 -4.1147127 -4.1580849 -4.1882324 -4.2144437 -4.2358832 -4.2573447][-4.2300525 -4.2328758 -4.2416239 -4.245594 -4.2356448 -4.2109952 -4.1680784 -4.1143169 -4.1088662 -4.1556821 -4.2011871 -4.23068 -4.2514 -4.2657743 -4.2795253][-4.2281489 -4.231503 -4.24191 -4.2474685 -4.2379766 -4.2186756 -4.1897273 -4.1584997 -4.1663146 -4.2052817 -4.2398982 -4.2605271 -4.2724671 -4.2804556 -4.2887778][-4.222888 -4.2278452 -4.2379513 -4.2450895 -4.2396111 -4.2269382 -4.2123809 -4.1978288 -4.2052984 -4.2271023 -4.2501235 -4.2639556 -4.2705097 -4.275208 -4.2798424][-4.2284307 -4.2304368 -4.2360277 -4.244019 -4.2430034 -4.232276 -4.2240138 -4.2153645 -4.2165937 -4.2259908 -4.2441025 -4.2523818 -4.253931 -4.2574625 -4.2608252][-4.2349176 -4.2313471 -4.2313957 -4.236927 -4.2374783 -4.2275629 -4.2229462 -4.2171559 -4.2156935 -4.2212505 -4.2356048 -4.2377033 -4.2347488 -4.2368722 -4.2394443][-4.2390442 -4.2319875 -4.22873 -4.23133 -4.2319374 -4.2243052 -4.22168 -4.216958 -4.2164364 -4.2217383 -4.23319 -4.2324023 -4.22639 -4.2251086 -4.2234583]]...]
INFO - root - 2017-12-05 12:57:09.217399: step 10510, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 76h:26m:13s remains)
INFO - root - 2017-12-05 12:57:17.687345: step 10520, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 75h:52m:37s remains)
INFO - root - 2017-12-05 12:57:26.313832: step 10530, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.882 sec/batch; 78h:53m:14s remains)
INFO - root - 2017-12-05 12:57:34.932712: step 10540, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 78h:13m:09s remains)
INFO - root - 2017-12-05 12:57:43.531852: step 10550, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 75h:46m:32s remains)
INFO - root - 2017-12-05 12:57:52.049793: step 10560, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 76h:50m:12s remains)
INFO - root - 2017-12-05 12:58:00.487407: step 10570, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 76h:01m:30s remains)
INFO - root - 2017-12-05 12:58:09.139348: step 10580, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 78h:50m:07s remains)
INFO - root - 2017-12-05 12:58:17.592759: step 10590, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 76h:52m:58s remains)
INFO - root - 2017-12-05 12:58:26.073688: step 10600, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.760 sec/batch; 67h:59m:25s remains)
2017-12-05 12:58:26.673942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3083544 -4.3033504 -4.2950535 -4.2850132 -4.2767391 -4.2722778 -4.2740622 -4.2791634 -4.2854571 -4.2893333 -4.2875786 -4.2826338 -4.2757206 -4.2698879 -4.270359][-4.3004918 -4.2923594 -4.2797413 -4.264318 -4.2501588 -4.2396188 -4.2380371 -4.2429924 -4.2509871 -4.255733 -4.2532969 -4.2484341 -4.241426 -4.2358823 -4.2402577][-4.2883744 -4.2754889 -4.2584772 -4.2385426 -4.2194242 -4.2023797 -4.19699 -4.2006264 -4.2108817 -4.21648 -4.2136621 -4.2087293 -4.201808 -4.1987896 -4.2096162][-4.2719193 -4.2529397 -4.2316427 -4.2089863 -4.1878839 -4.1662979 -4.1556349 -4.1558628 -4.1669812 -4.1716213 -4.1684885 -4.1625485 -4.1562877 -4.1577468 -4.1764178][-4.25381 -4.2274685 -4.2010584 -4.1757607 -4.1548204 -4.1307316 -4.113246 -4.107739 -4.1188722 -4.1249218 -4.1221261 -4.1141396 -4.1053848 -4.1117954 -4.1400118][-4.2397633 -4.2083983 -4.179009 -4.1524553 -4.1310606 -4.1044922 -4.078074 -4.0645537 -4.0759807 -4.0865197 -4.084177 -4.0721488 -4.0558276 -4.0631161 -4.1001592][-4.2368159 -4.2046695 -4.1741891 -4.1444626 -4.11965 -4.0853672 -4.0437684 -4.0183787 -4.034358 -4.0558453 -4.0567484 -4.0435495 -4.0255804 -4.0375586 -4.0839567][-4.2450771 -4.2178106 -4.1879787 -4.1509724 -4.112885 -4.061902 -3.996208 -3.9530339 -3.9759114 -4.0150223 -4.0272708 -4.0197968 -4.0078225 -4.0290756 -4.084034][-4.2533178 -4.2313061 -4.2017894 -4.1563482 -4.1046109 -4.0414343 -3.9570854 -3.8944757 -3.9167671 -3.9689155 -3.9937108 -3.9943986 -3.9916589 -4.0201178 -4.0801945][-4.2568355 -4.2387414 -4.21207 -4.1635017 -4.1084127 -4.047977 -3.9725032 -3.9157472 -3.9298878 -3.9739659 -3.9949465 -3.9938457 -3.9925807 -4.0188618 -4.069324][-4.252615 -4.2352376 -4.2091 -4.1601644 -4.1050277 -4.0487089 -3.9904509 -3.9503121 -3.9584394 -3.9879949 -4.0033865 -4.0048943 -4.0057321 -4.0264158 -4.0593915][-4.2463427 -4.2287221 -4.2016821 -4.1553903 -4.1014843 -4.0479097 -4.0045152 -3.9791689 -3.9863784 -4.0080662 -4.0232153 -4.0330644 -4.040822 -4.0566044 -4.0699859][-4.2464132 -4.2285409 -4.2029505 -4.1623158 -4.113698 -4.0644207 -4.0308967 -4.0181313 -4.0317678 -4.0510044 -4.0671124 -4.0812812 -4.0917635 -4.1008635 -4.1010494][-4.2576985 -4.2400761 -4.2166386 -4.1830606 -4.1426549 -4.1014743 -4.0740409 -4.0673747 -4.0813904 -4.0946431 -4.1063766 -4.1220536 -4.136261 -4.1427445 -4.1402154][-4.2786455 -4.2659707 -4.2487149 -4.2250147 -4.1955647 -4.1654944 -4.1446838 -4.13995 -4.1477094 -4.1504183 -4.1539779 -4.1678038 -4.1813459 -4.1841516 -4.1791806]]...]
INFO - root - 2017-12-05 12:58:35.296446: step 10610, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 75h:30m:14s remains)
INFO - root - 2017-12-05 12:58:43.897170: step 10620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 76h:11m:39s remains)
INFO - root - 2017-12-05 12:58:52.522812: step 10630, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 75h:30m:22s remains)
INFO - root - 2017-12-05 12:59:01.094275: step 10640, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.816 sec/batch; 72h:57m:42s remains)
INFO - root - 2017-12-05 12:59:09.716632: step 10650, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 76h:03m:25s remains)
INFO - root - 2017-12-05 12:59:18.372672: step 10660, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 79h:54m:07s remains)
INFO - root - 2017-12-05 12:59:26.999542: step 10670, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 76h:21m:28s remains)
INFO - root - 2017-12-05 12:59:35.528038: step 10680, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 75h:12m:39s remains)
INFO - root - 2017-12-05 12:59:44.123679: step 10690, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 76h:54m:45s remains)
INFO - root - 2017-12-05 12:59:52.662793: step 10700, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 77h:27m:38s remains)
2017-12-05 12:59:53.429223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1967158 -4.218646 -4.241838 -4.2609167 -4.2730331 -4.277915 -4.2771988 -4.2726169 -4.2663689 -4.2600765 -4.2537723 -4.2485695 -4.2450137 -4.242415 -4.240386][-4.223681 -4.2438488 -4.26069 -4.2726893 -4.2787051 -4.2791414 -4.275558 -4.2703457 -4.2641907 -4.2576866 -4.2499137 -4.2426438 -4.2378926 -4.2355771 -4.2346592][-4.2468958 -4.2638731 -4.27318 -4.2757797 -4.2737389 -4.2691107 -4.2632694 -4.2582817 -4.2531085 -4.2472749 -4.2392993 -4.2311339 -4.2258029 -4.2245231 -4.22518][-4.2547336 -4.2660208 -4.2663379 -4.2594919 -4.2505956 -4.2426305 -4.2362742 -4.2330675 -4.2310896 -4.2290487 -4.224411 -4.2186489 -4.2152162 -4.2160439 -4.2189732][-4.2531576 -4.2548289 -4.2451596 -4.2303624 -4.2161708 -4.2057624 -4.2003341 -4.2008591 -4.2049103 -4.2100034 -4.2123933 -4.2126083 -4.2132287 -4.2161126 -4.2203364][-4.2470822 -4.2405477 -4.2248006 -4.2063451 -4.1900153 -4.179822 -4.17697 -4.1821985 -4.1925879 -4.2046008 -4.213459 -4.2189193 -4.2227197 -4.2262955 -4.2296834][-4.2422128 -4.2335629 -4.2183681 -4.2013259 -4.1867285 -4.1787953 -4.1786704 -4.1867409 -4.1997437 -4.2133455 -4.2233014 -4.2296944 -4.2343206 -4.2379389 -4.2405529][-4.2413487 -4.2342114 -4.2224388 -4.2091603 -4.1982222 -4.1930976 -4.1947885 -4.2031827 -4.2144918 -4.2246184 -4.2312961 -4.2358336 -4.2398148 -4.2433815 -4.2461209][-4.2428284 -4.2380443 -4.2298012 -4.2205715 -4.2130609 -4.2103367 -4.2128263 -4.2197094 -4.2274289 -4.2325954 -4.2348714 -4.2365379 -4.238997 -4.2421718 -4.2451019][-4.2459612 -4.2428484 -4.2378154 -4.2320986 -4.2272983 -4.2259293 -4.2281666 -4.2325912 -4.2361827 -4.236896 -4.2358413 -4.2354507 -4.2367535 -4.2391911 -4.241859][-4.2492232 -4.2476487 -4.2450767 -4.2420092 -4.2391682 -4.2382421 -4.2393432 -4.2411938 -4.2418642 -4.2401996 -4.2378454 -4.2367883 -4.2374487 -4.2391968 -4.2413516][-4.2498922 -4.249527 -4.2484651 -4.2469411 -4.2452602 -4.2444663 -4.2448306 -4.2453475 -4.2451005 -4.243186 -4.2409387 -4.2398582 -4.2400417 -4.240984 -4.2425308][-4.2478294 -4.2478509 -4.2472491 -4.2464314 -4.2454109 -4.2446589 -4.2444525 -4.2442551 -4.2437921 -4.2423329 -4.2409687 -4.2404423 -4.2405467 -4.2412419 -4.2424316][-4.2442932 -4.24383 -4.2426891 -4.2415333 -4.2401724 -4.2387857 -4.2377992 -4.23702 -4.2363749 -4.2352285 -4.2344828 -4.234519 -4.23487 -4.23567 -4.23683][-4.2398677 -4.2387295 -4.2370977 -4.2356181 -4.2340198 -4.2322273 -4.2306495 -4.2291203 -4.2275405 -4.2255797 -4.2242393 -4.2239218 -4.2241859 -4.2249765 -4.2261343]]...]
INFO - root - 2017-12-05 13:00:01.920621: step 10710, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 0.785 sec/batch; 70h:08m:54s remains)
INFO - root - 2017-12-05 13:00:10.466555: step 10720, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 76h:29m:24s remains)
INFO - root - 2017-12-05 13:00:19.034211: step 10730, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 75h:53m:29s remains)
INFO - root - 2017-12-05 13:00:27.544837: step 10740, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.829 sec/batch; 74h:06m:10s remains)
INFO - root - 2017-12-05 13:00:36.061699: step 10750, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 74h:50m:46s remains)
INFO - root - 2017-12-05 13:00:44.544166: step 10760, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 78h:49m:11s remains)
INFO - root - 2017-12-05 13:00:53.116149: step 10770, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 77h:40m:43s remains)
INFO - root - 2017-12-05 13:01:01.687151: step 10780, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 76h:43m:05s remains)
INFO - root - 2017-12-05 13:01:10.168811: step 10790, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 76h:24m:01s remains)
INFO - root - 2017-12-05 13:01:18.567561: step 10800, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 75h:24m:37s remains)
2017-12-05 13:01:19.338393: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.311233 -4.3077869 -4.2985454 -4.2909513 -4.2813263 -4.2720919 -4.2609143 -4.2495856 -4.2406378 -4.2371283 -4.2365322 -4.224906 -4.2054477 -4.1846595 -4.1535077][-4.2918172 -4.2866311 -4.2713962 -4.25593 -4.2415266 -4.2323065 -4.2216554 -4.211905 -4.205451 -4.2064652 -4.2083149 -4.1958456 -4.172698 -4.1432204 -4.1062865][-4.2650433 -4.257092 -4.2351632 -4.2151828 -4.2039275 -4.1995869 -4.194757 -4.1911807 -4.1877408 -4.1946435 -4.1986217 -4.1835957 -4.1551452 -4.1196265 -4.0869741][-4.2360315 -4.2250962 -4.2009273 -4.181365 -4.1726832 -4.1687407 -4.1666117 -4.1663427 -4.168838 -4.1840181 -4.1989932 -4.1919961 -4.1703467 -4.1453743 -4.1260209][-4.210453 -4.2000375 -4.178772 -4.1590123 -4.1450005 -4.130115 -4.1205482 -4.1235137 -4.1356921 -4.1626205 -4.1939187 -4.1994972 -4.1935306 -4.184557 -4.1768389][-4.2033873 -4.1930332 -4.1719217 -4.1450467 -4.1117415 -4.0668831 -4.0314894 -4.0451436 -4.0784578 -4.1152072 -4.1556864 -4.1721549 -4.1844687 -4.1955051 -4.199297][-4.2148237 -4.1978211 -4.1657591 -4.12028 -4.0586967 -3.9726098 -3.9084082 -3.953156 -4.0207038 -4.0616546 -4.1009569 -4.1291976 -4.1633511 -4.1986318 -4.2169838][-4.2270031 -4.20366 -4.1543951 -4.0888066 -4.0099392 -3.910948 -3.8391552 -3.9159093 -4.0082808 -4.0423436 -4.0727978 -4.1097884 -4.1592231 -4.2099748 -4.2395868][-4.22036 -4.1954637 -4.145905 -4.0825005 -4.0193706 -3.960351 -3.923264 -3.9795675 -4.0443664 -4.0591311 -4.0810452 -4.1246328 -4.1788354 -4.2301965 -4.2615294][-4.1853547 -4.1770334 -4.1515307 -4.1139989 -4.0812087 -4.0632315 -4.0483894 -4.0691586 -4.1011844 -4.1056218 -4.1224961 -4.1631727 -4.209733 -4.2475524 -4.2715774][-4.1492047 -4.1673522 -4.1705322 -4.1594028 -4.1455483 -4.1445446 -4.1399927 -4.1441331 -4.1566415 -4.1570888 -4.1672716 -4.1957707 -4.2302737 -4.2537603 -4.2689743][-4.14349 -4.1807985 -4.1946888 -4.1932139 -4.1843829 -4.1853232 -4.1861925 -4.1875792 -4.1883788 -4.1809793 -4.1826963 -4.2002192 -4.2266006 -4.2446637 -4.2576971][-4.1612535 -4.1947455 -4.2016611 -4.1971583 -4.1861224 -4.1844654 -4.1902952 -4.1947908 -4.1936584 -4.1885939 -4.1905489 -4.2041936 -4.228878 -4.25019 -4.2675524][-4.1738954 -4.1930151 -4.1923218 -4.1821 -4.1667809 -4.1659117 -4.1776524 -4.189589 -4.1993446 -4.2079039 -4.2168708 -4.232367 -4.2576981 -4.2798424 -4.2960315][-4.1651192 -4.17495 -4.1731715 -4.168499 -4.1625824 -4.1699448 -4.1910243 -4.21138 -4.231688 -4.2503295 -4.2623625 -4.2747378 -4.2921319 -4.3059163 -4.31516]]...]
INFO - root - 2017-12-05 13:01:27.817539: step 10810, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 74h:20m:56s remains)
INFO - root - 2017-12-05 13:01:36.255331: step 10820, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.776 sec/batch; 69h:18m:04s remains)
INFO - root - 2017-12-05 13:01:44.731684: step 10830, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.818 sec/batch; 73h:05m:31s remains)
INFO - root - 2017-12-05 13:01:53.335547: step 10840, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 76h:42m:05s remains)
INFO - root - 2017-12-05 13:02:01.837477: step 10850, loss = 2.11, batch loss = 2.05 (9.2 examples/sec; 0.867 sec/batch; 77h:26m:31s remains)
INFO - root - 2017-12-05 13:02:10.325911: step 10860, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.874 sec/batch; 78h:07m:27s remains)
INFO - root - 2017-12-05 13:02:18.879021: step 10870, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 75h:21m:12s remains)
INFO - root - 2017-12-05 13:02:27.400231: step 10880, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 75h:23m:26s remains)
INFO - root - 2017-12-05 13:02:36.057082: step 10890, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.894 sec/batch; 79h:49m:22s remains)
INFO - root - 2017-12-05 13:02:44.623374: step 10900, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 75h:47m:56s remains)
2017-12-05 13:02:45.380660: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3135729 -4.3009858 -4.2908325 -4.2879777 -4.2919178 -4.2982168 -4.3055673 -4.3160033 -4.3278537 -4.3362803 -4.3408303 -4.3436937 -4.3463411 -4.3483734 -4.3485479][-4.2975821 -4.2812238 -4.2704272 -4.2668605 -4.2688794 -4.2716641 -4.2771869 -4.2907 -4.31101 -4.3252521 -4.3342056 -4.3359556 -4.3353753 -4.3377662 -4.3388119][-4.2804146 -4.2633052 -4.2520428 -4.2452779 -4.2394552 -4.2330494 -4.231142 -4.2433538 -4.2655215 -4.2858195 -4.3039694 -4.311348 -4.311502 -4.3154655 -4.3179765][-4.2599807 -4.2393751 -4.2227941 -4.2095757 -4.1925244 -4.1710253 -4.154736 -4.1646118 -4.1957946 -4.23181 -4.2668729 -4.2845063 -4.28701 -4.2910662 -4.2955804][-4.2384253 -4.212122 -4.1857796 -4.16536 -4.1383829 -4.0964928 -4.0544591 -4.0588193 -4.114923 -4.1833849 -4.2385173 -4.2683759 -4.2741308 -4.2751455 -4.27793][-4.2227278 -4.1842141 -4.1418624 -4.1058917 -4.0617695 -3.9909749 -3.9138365 -3.9078212 -3.9964342 -4.104867 -4.1829152 -4.2273178 -4.2424126 -4.2484527 -4.2556047][-4.211658 -4.1597352 -4.0986328 -4.0397487 -3.9759059 -3.8734481 -3.7627385 -3.733742 -3.845664 -3.9951952 -4.1024919 -4.1694145 -4.2059188 -4.2291536 -4.2444973][-4.2031169 -4.1389046 -4.0594516 -3.9775686 -3.9021349 -3.795552 -3.6849234 -3.6474032 -3.7615891 -3.9334157 -4.0599151 -4.137548 -4.1905088 -4.2283568 -4.247963][-4.2004566 -4.1278024 -4.0367808 -3.9403102 -3.8622646 -3.7754283 -3.7034421 -3.6883087 -3.7893655 -3.9436326 -4.0686126 -4.1454277 -4.1967068 -4.2357755 -4.2547603][-4.2106452 -4.1331668 -4.0361133 -3.9285674 -3.8351314 -3.7516842 -3.7066693 -3.7164626 -3.8131557 -3.950345 -4.0758581 -4.1551037 -4.2014437 -4.2366819 -4.2571917][-4.2302408 -4.1559167 -4.0595522 -3.9496188 -3.8439445 -3.7551248 -3.7191679 -3.7468884 -3.839685 -3.9599421 -4.0784454 -4.1599874 -4.2056446 -4.2417445 -4.2659039][-4.25524 -4.1942034 -4.1121078 -4.0194736 -3.9269 -3.8477614 -3.8149288 -3.8407981 -3.9180713 -4.0128465 -4.1156096 -4.1897025 -4.2330604 -4.26615 -4.2879286][-4.280396 -4.2374425 -4.180347 -4.1191378 -4.0584722 -4.0095291 -3.9908385 -4.0114751 -4.0645385 -4.1266894 -4.1991158 -4.2517014 -4.2815461 -4.3024621 -4.314229][-4.3021507 -4.2772241 -4.2447171 -4.2143855 -4.1854925 -4.16463 -4.1594329 -4.1750269 -4.2061734 -4.2414136 -4.2806749 -4.3094697 -4.3237371 -4.3311224 -4.3333488][-4.3155589 -4.3040185 -4.2876663 -4.2747135 -4.2632484 -4.2565432 -4.2591467 -4.2722731 -4.2902079 -4.3086162 -4.326992 -4.3385673 -4.3423991 -4.3422065 -4.3399534]]...]
INFO - root - 2017-12-05 13:02:53.833417: step 10910, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 76h:32m:32s remains)
INFO - root - 2017-12-05 13:03:02.265040: step 10920, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 76h:09m:28s remains)
INFO - root - 2017-12-05 13:03:10.654233: step 10930, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 74h:10m:15s remains)
INFO - root - 2017-12-05 13:03:19.159890: step 10940, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 75h:23m:18s remains)
INFO - root - 2017-12-05 13:03:27.702633: step 10950, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 75h:19m:24s remains)
INFO - root - 2017-12-05 13:03:36.251970: step 10960, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 74h:31m:08s remains)
INFO - root - 2017-12-05 13:03:44.903693: step 10970, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 77h:02m:35s remains)
INFO - root - 2017-12-05 13:03:53.301961: step 10980, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 76h:01m:14s remains)
INFO - root - 2017-12-05 13:04:01.771574: step 10990, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 73h:53m:34s remains)
INFO - root - 2017-12-05 13:04:10.401049: step 11000, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 78h:14m:45s remains)
2017-12-05 13:04:11.139595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2716422 -4.2719917 -4.2705512 -4.2749982 -4.2851167 -4.2983651 -4.3134642 -4.3206782 -4.3223672 -4.3176956 -4.3099213 -4.302319 -4.2972426 -4.2967758 -4.29795][-4.2333031 -4.234005 -4.2332878 -4.239522 -4.2500896 -4.2668056 -4.2864823 -4.295856 -4.2983623 -4.2940416 -4.286284 -4.2797265 -4.275105 -4.2739892 -4.274683][-4.1860213 -4.1848631 -4.1850939 -4.19616 -4.2094159 -4.2295408 -4.2486763 -4.2565422 -4.2613692 -4.2595873 -4.254283 -4.2492409 -4.2434306 -4.2414165 -4.2388539][-4.1413 -4.1426287 -4.1430573 -4.1577225 -4.1751671 -4.1948938 -4.2056866 -4.2074561 -4.2131786 -4.213151 -4.2089329 -4.2040758 -4.1972866 -4.195138 -4.1906452][-4.1049066 -4.1053047 -4.1018214 -4.1198606 -4.14085 -4.1561556 -4.1602206 -4.1542721 -4.1585951 -4.1602473 -4.1589313 -4.1608124 -4.1560874 -4.1546788 -4.1491456][-4.0789127 -4.0745654 -4.0643263 -4.079421 -4.0975547 -4.1065764 -4.0973673 -4.0801435 -4.0877929 -4.1023259 -4.1140637 -4.1262622 -4.1299405 -4.1313763 -4.1210027][-4.0630374 -4.05206 -4.0389709 -4.0477962 -4.0562248 -4.0554914 -4.0296 -3.999352 -4.0201616 -4.05771 -4.0808592 -4.0986066 -4.1114583 -4.1151528 -4.09811][-4.0718989 -4.0541363 -4.0431147 -4.0457525 -4.0411887 -4.0313015 -3.9944968 -3.9583538 -3.9867034 -4.0411925 -4.0686097 -4.0843058 -4.1007233 -4.108057 -4.0882726][-4.1047773 -4.0813489 -4.0725274 -4.0722971 -4.0597477 -4.0406737 -4.0079813 -3.9815831 -4.0069108 -4.0564804 -4.0753069 -4.0829444 -4.0992093 -4.1102738 -4.0974889][-4.1614528 -4.1326718 -4.1200938 -4.1160207 -4.1052394 -4.0878968 -4.0646052 -4.0483723 -4.0618505 -4.0899105 -4.0951595 -4.0964274 -4.1147227 -4.1344061 -4.1344][-4.2077584 -4.1821685 -4.16806 -4.1572247 -4.1446733 -4.1362853 -4.1265907 -4.1181097 -4.1205392 -4.1285887 -4.1256795 -4.1198173 -4.1374536 -4.166687 -4.1815805][-4.2335148 -4.214879 -4.2022147 -4.1896105 -4.1740656 -4.1699305 -4.166398 -4.1567264 -4.1487627 -4.1473455 -4.1424837 -4.1345997 -4.1464281 -4.1772413 -4.2016015][-4.2394304 -4.2280474 -4.2192435 -4.2071247 -4.1911993 -4.1842365 -4.1781292 -4.1666627 -4.1528487 -4.14476 -4.1392164 -4.1303511 -4.1409359 -4.1735878 -4.2032566][-4.23477 -4.2272253 -4.22478 -4.2156858 -4.1984782 -4.1856394 -4.1717086 -4.1521487 -4.1286879 -4.1135912 -4.1098361 -4.1073041 -4.1232958 -4.16197 -4.1958766][-4.2395325 -4.233448 -4.2312031 -4.2238111 -4.2078733 -4.1882877 -4.1626287 -4.129282 -4.09478 -4.075263 -4.0774632 -4.0852404 -4.1082668 -4.1466713 -4.178792]]...]
INFO - root - 2017-12-05 13:04:19.649121: step 11010, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 76h:59m:13s remains)
INFO - root - 2017-12-05 13:04:28.194207: step 11020, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 76h:41m:55s remains)
INFO - root - 2017-12-05 13:04:36.675409: step 11030, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 76h:31m:45s remains)
INFO - root - 2017-12-05 13:04:45.136451: step 11040, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 74h:30m:20s remains)
INFO - root - 2017-12-05 13:04:53.634347: step 11050, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.811 sec/batch; 72h:22m:49s remains)
INFO - root - 2017-12-05 13:05:02.149803: step 11060, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 0.811 sec/batch; 72h:23m:12s remains)
INFO - root - 2017-12-05 13:05:10.686434: step 11070, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 78h:33m:23s remains)
INFO - root - 2017-12-05 13:05:19.293493: step 11080, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 77h:58m:47s remains)
INFO - root - 2017-12-05 13:05:27.889186: step 11090, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.860 sec/batch; 76h:44m:44s remains)
INFO - root - 2017-12-05 13:05:36.438685: step 11100, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 75h:47m:34s remains)
2017-12-05 13:05:37.196933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2222691 -4.2196636 -4.2156849 -4.2120891 -4.20846 -4.2043004 -4.2012835 -4.1993508 -4.2001448 -4.203269 -4.2039208 -4.2011938 -4.2008896 -4.2068334 -4.2127571][-4.2153254 -4.2147651 -4.2141628 -4.2129812 -4.2086444 -4.2008033 -4.192894 -4.1863365 -4.1835022 -4.186657 -4.1914864 -4.19422 -4.1988797 -4.2085819 -4.2164984][-4.2136159 -4.2179441 -4.2214751 -4.2212558 -4.2122922 -4.1941452 -4.1754217 -4.1625638 -4.1574607 -4.1647077 -4.177176 -4.1902394 -4.2033362 -4.2179804 -4.227015][-4.2083626 -4.2136278 -4.2192183 -4.2186303 -4.2048512 -4.1766515 -4.1459017 -4.127831 -4.1266303 -4.1428127 -4.1661558 -4.1901569 -4.2103219 -4.2279253 -4.2384276][-4.216599 -4.2182183 -4.2180634 -4.2095385 -4.1838946 -4.1424723 -4.0959749 -4.0702648 -4.0764809 -4.1081071 -4.1487885 -4.1897287 -4.2176642 -4.2364531 -4.2442951][-4.2246804 -4.2233343 -4.2153239 -4.1965466 -4.1587195 -4.0978823 -4.0276194 -3.9853151 -3.9944649 -4.0465684 -4.112504 -4.177165 -4.2194376 -4.24227 -4.2446628][-4.2282805 -4.2250319 -4.2125525 -4.1861377 -4.1349893 -4.0548806 -3.9590237 -3.8934679 -3.8969378 -3.9620888 -4.0521135 -4.1399379 -4.1998258 -4.2305741 -4.2317023][-4.2343621 -4.2374425 -4.2306709 -4.2049809 -4.1472611 -4.0605755 -3.9539301 -3.8727536 -3.8628473 -3.9220669 -4.0127468 -4.1025438 -4.168129 -4.1983185 -4.1942449][-4.2541127 -4.2642756 -4.2651911 -4.2421021 -4.1858907 -4.1047349 -4.0129108 -3.9395797 -3.9203188 -3.9609046 -4.0353856 -4.1089606 -4.1599579 -4.1701503 -4.1522703][-4.27073 -4.2817717 -4.286025 -4.2659059 -4.21478 -4.1464796 -4.0840182 -4.0411267 -4.0280719 -4.0488486 -4.0975285 -4.1443129 -4.1678267 -4.1539369 -4.122056][-4.2734413 -4.2817421 -4.2857504 -4.2665191 -4.2186928 -4.1633291 -4.1303887 -4.1221995 -4.1266775 -4.1411037 -4.1646004 -4.1797657 -4.1737332 -4.1405191 -4.1037588][-4.2587042 -4.2608352 -4.2608652 -4.2415886 -4.1997929 -4.1590447 -4.146965 -4.1616397 -4.1828041 -4.2028894 -4.2137895 -4.207191 -4.1815157 -4.1407957 -4.1093583][-4.2516255 -4.2472715 -4.2411623 -4.2190504 -4.1785507 -4.1461105 -4.1464577 -4.1684985 -4.1989837 -4.2278571 -4.2381473 -4.225965 -4.1938028 -4.153759 -4.1303082][-4.2656627 -4.2592649 -4.2507625 -4.2258329 -4.1862235 -4.1595316 -4.1626425 -4.1815271 -4.2105947 -4.24228 -4.2524605 -4.2384911 -4.2046 -4.1681647 -4.1526814][-4.2830234 -4.2771616 -4.2667894 -4.2431779 -4.2106228 -4.1909761 -4.1956434 -4.2125483 -4.2381 -4.2644677 -4.268405 -4.2466807 -4.2096128 -4.17757 -4.171349]]...]
INFO - root - 2017-12-05 13:05:45.823780: step 11110, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.865 sec/batch; 77h:10m:43s remains)
INFO - root - 2017-12-05 13:05:54.170531: step 11120, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 74h:20m:59s remains)
INFO - root - 2017-12-05 13:06:02.655775: step 11130, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 76h:44m:09s remains)
INFO - root - 2017-12-05 13:06:11.251712: step 11140, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 77h:47m:54s remains)
INFO - root - 2017-12-05 13:06:19.663601: step 11150, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 75h:16m:16s remains)
INFO - root - 2017-12-05 13:06:28.216018: step 11160, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 77h:53m:15s remains)
INFO - root - 2017-12-05 13:06:36.754778: step 11170, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 76h:02m:32s remains)
INFO - root - 2017-12-05 13:06:45.195447: step 11180, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 75h:05m:17s remains)
INFO - root - 2017-12-05 13:06:53.757895: step 11190, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 76h:53m:04s remains)
INFO - root - 2017-12-05 13:07:02.378609: step 11200, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 78h:08m:30s remains)
2017-12-05 13:07:03.041598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2482662 -4.266109 -4.2713118 -4.2672591 -4.2635627 -4.2625933 -4.2666688 -4.2706509 -4.2683892 -4.2591629 -4.2451887 -4.2341685 -4.2312751 -4.2351537 -4.2416177][-4.2270174 -4.2445683 -4.2487187 -4.2425642 -4.2384553 -4.2398829 -4.2505269 -4.2619987 -4.2649231 -4.2572742 -4.2417889 -4.2258468 -4.2177286 -4.2205195 -4.2283034][-4.2084656 -4.2252626 -4.2260489 -4.2136369 -4.207727 -4.211803 -4.2309175 -4.2524638 -4.263948 -4.2610517 -4.2477345 -4.2331638 -4.2260666 -4.22868 -4.2328148][-4.2046933 -4.2142148 -4.2039828 -4.1785717 -4.1651292 -4.16688 -4.1941223 -4.2315011 -4.2584233 -4.2653975 -4.2609959 -4.2558069 -4.2554765 -4.2579422 -4.255662][-4.2119613 -4.21465 -4.189095 -4.1446552 -4.1159472 -4.1072688 -4.1347222 -4.1848612 -4.2324748 -4.2591915 -4.269784 -4.276825 -4.285583 -4.289845 -4.2850208][-4.2233543 -4.2222638 -4.1848254 -4.1204715 -4.0653262 -4.034214 -4.0470886 -4.1019812 -4.1743937 -4.2270794 -4.2558093 -4.2742481 -4.2908969 -4.3006487 -4.3000383][-4.2406254 -4.2404957 -4.1980743 -4.1169772 -4.0314584 -3.9626858 -3.9374506 -3.9821329 -4.0796714 -4.1635838 -4.2150469 -4.2483492 -4.276989 -4.2975245 -4.3065891][-4.2539158 -4.2593708 -4.2224822 -4.1380939 -4.0320463 -3.922447 -3.8412557 -3.8579652 -3.9749734 -4.08603 -4.1612182 -4.2107468 -4.2492862 -4.2793288 -4.3012452][-4.2631373 -4.2777109 -4.2544789 -4.1845703 -4.0832958 -3.9647756 -3.8617389 -3.8435323 -3.93897 -4.0462327 -4.124877 -4.1771173 -4.2121921 -4.2440581 -4.2758808][-4.2670856 -4.2905517 -4.2822933 -4.2327328 -4.15275 -4.0563741 -3.9733362 -3.9443202 -3.9908834 -4.0580153 -4.1109529 -4.1400313 -4.1512885 -4.1705594 -4.2145085][-4.2684164 -4.2987094 -4.3010678 -4.2674265 -4.2098675 -4.1409326 -4.0816689 -4.0555277 -4.070755 -4.0955682 -4.1044612 -4.084023 -4.044488 -4.0387626 -4.1038208][-4.2675362 -4.3016677 -4.3115 -4.2895069 -4.2496781 -4.2021461 -4.159358 -4.137311 -4.1358943 -4.1328945 -4.10496 -4.0341153 -3.932452 -3.8919587 -3.9788957][-4.2675366 -4.3043776 -4.3208027 -4.3100491 -4.2832136 -4.249845 -4.2180519 -4.1988091 -4.1901822 -4.1761565 -4.1329851 -4.0457811 -3.9262991 -3.8612609 -3.9307659][-4.26857 -4.3074684 -4.3296824 -4.3271737 -4.3104897 -4.2881136 -4.2647948 -4.2468505 -4.2335076 -4.218152 -4.1775341 -4.1007428 -4.0071583 -3.9509108 -3.9824708][-4.2692046 -4.3089643 -4.3341708 -4.3358297 -4.3259678 -4.3129287 -4.2981291 -4.283977 -4.2701316 -4.2528419 -4.2163591 -4.1588292 -4.0964961 -4.0572081 -4.0661798]]...]
INFO - root - 2017-12-05 13:07:11.730877: step 11210, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 77h:06m:37s remains)
INFO - root - 2017-12-05 13:07:20.329505: step 11220, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 76h:09m:16s remains)
INFO - root - 2017-12-05 13:07:28.671191: step 11230, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 76h:32m:28s remains)
INFO - root - 2017-12-05 13:07:37.220597: step 11240, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 76h:55m:36s remains)
INFO - root - 2017-12-05 13:07:45.803568: step 11250, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 77h:38m:52s remains)
INFO - root - 2017-12-05 13:07:54.274687: step 11260, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 75h:24m:22s remains)
INFO - root - 2017-12-05 13:08:02.781526: step 11270, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 76h:32m:33s remains)
INFO - root - 2017-12-05 13:08:11.396587: step 11280, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 76h:29m:43s remains)
INFO - root - 2017-12-05 13:08:19.981892: step 11290, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 76h:50m:21s remains)
INFO - root - 2017-12-05 13:08:28.523132: step 11300, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 78h:26m:23s remains)
2017-12-05 13:08:29.225002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1643171 -4.1973214 -4.2121005 -4.2130275 -4.2054534 -4.1947951 -4.1849937 -4.17623 -4.1724958 -4.1675735 -4.1343145 -4.0740166 -4.034595 -4.0548515 -4.0955129][-4.1614842 -4.1936293 -4.206161 -4.2084546 -4.2016039 -4.1886973 -4.1788774 -4.17345 -4.170857 -4.1613107 -4.12415 -4.0576086 -4.0158286 -4.0428972 -4.0871468][-4.1469722 -4.1761317 -4.1863346 -4.1868563 -4.180841 -4.1709809 -4.16612 -4.1682634 -4.1717043 -4.1635013 -4.1280069 -4.0618739 -4.021925 -4.0532618 -4.0980883][-4.1341867 -4.1599369 -4.1704249 -4.1727071 -4.1647978 -4.1533418 -4.1498594 -4.15506 -4.1587315 -4.1559458 -4.1312842 -4.0799551 -4.0514984 -4.0872 -4.1299148][-4.1197972 -4.1440306 -4.1565385 -4.1609488 -4.1504045 -4.1357708 -4.1286893 -4.1291056 -4.1283326 -4.1285987 -4.118844 -4.0884485 -4.0736761 -4.1118846 -4.1539774][-4.1008649 -4.126297 -4.1405525 -4.1460824 -4.1353393 -4.1160064 -4.1026778 -4.09847 -4.0941663 -4.0913782 -4.082058 -4.0560908 -4.0474224 -4.0930152 -4.1404748][-4.08606 -4.113019 -4.1322312 -4.1396508 -4.1266618 -4.0986161 -4.072896 -4.0604424 -4.05632 -4.0496354 -4.0321965 -3.996069 -3.9842024 -4.0418081 -4.1024351][-4.0835514 -4.1076641 -4.1275969 -4.1336102 -4.1176143 -4.0831504 -4.0523624 -4.0400662 -4.0395832 -4.0299463 -4.0077829 -3.9623418 -3.9428089 -4.0082283 -4.0765538][-4.0916276 -4.1091161 -4.1249409 -4.1274109 -4.1112857 -4.08214 -4.0573845 -4.0515838 -4.0595193 -4.0605392 -4.0487747 -4.0154757 -3.9986472 -4.0485296 -4.104588][-4.0980711 -4.1035109 -4.1141877 -4.1195836 -4.1113176 -4.0921988 -4.0771508 -4.0769992 -4.0902157 -4.0987349 -4.0965939 -4.0745683 -4.06513 -4.10306 -4.1489267][-4.0935817 -4.0927377 -4.1014166 -4.1112013 -4.1098394 -4.09819 -4.0924244 -4.0951443 -4.1060176 -4.1133862 -4.1114368 -4.0942812 -4.0890388 -4.1244745 -4.1700373][-4.074791 -4.0757461 -4.0856724 -4.1000848 -4.1065078 -4.1075954 -4.1096616 -4.1103692 -4.1127915 -4.1117735 -4.10292 -4.0854363 -4.087821 -4.1286297 -4.1797991][-4.0582476 -4.0594954 -4.071857 -4.0918608 -4.1061544 -4.1193447 -4.1303282 -4.1290283 -4.1218 -4.1164842 -4.1071572 -4.087235 -4.0916109 -4.13832 -4.1876206][-4.0481405 -4.0526147 -4.0698032 -4.0957556 -4.1206846 -4.1451306 -4.16403 -4.1602697 -4.1474886 -4.1440597 -4.1397252 -4.1197152 -4.116282 -4.1502013 -4.1857457][-4.0498486 -4.0570192 -4.079742 -4.11567 -4.148654 -4.1755438 -4.1961832 -4.1954889 -4.1857209 -4.1847248 -4.1856375 -4.1696591 -4.1575813 -4.1697168 -4.1876836]]...]
INFO - root - 2017-12-05 13:08:37.780477: step 11310, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 78h:04m:51s remains)
INFO - root - 2017-12-05 13:08:46.251008: step 11320, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 77h:39m:33s remains)
INFO - root - 2017-12-05 13:08:54.859040: step 11330, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 79h:08m:33s remains)
INFO - root - 2017-12-05 13:09:03.257603: step 11340, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 75h:35m:23s remains)
INFO - root - 2017-12-05 13:09:11.838857: step 11350, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 78h:55m:00s remains)
INFO - root - 2017-12-05 13:09:20.493935: step 11360, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 80h:25m:09s remains)
INFO - root - 2017-12-05 13:09:29.002756: step 11370, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 75h:54m:46s remains)
INFO - root - 2017-12-05 13:09:37.641922: step 11380, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 75h:35m:41s remains)
INFO - root - 2017-12-05 13:09:46.171699: step 11390, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 78h:14m:40s remains)
INFO - root - 2017-12-05 13:09:54.844254: step 11400, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 76h:01m:49s remains)
2017-12-05 13:09:55.634383: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2972369 -4.2928534 -4.2975678 -4.3007736 -4.3039923 -4.310627 -4.31307 -4.3059049 -4.2962556 -4.2913203 -4.2887688 -4.2972145 -4.3169136 -4.3372722 -4.34888][-4.260756 -4.2510729 -4.2526193 -4.2540407 -4.2590942 -4.2705274 -4.2774763 -4.2703228 -4.2566686 -4.25123 -4.2508364 -4.2619224 -4.2896581 -4.3220859 -4.3432803][-4.215138 -4.1980553 -4.1935034 -4.1935544 -4.1992459 -4.2150311 -4.2260985 -4.2188306 -4.202693 -4.1967459 -4.2001472 -4.21526 -4.2512503 -4.2972832 -4.3291283][-4.1763053 -4.1531534 -4.1378374 -4.1302309 -4.1273293 -4.1416125 -4.1535778 -4.141829 -4.127707 -4.1316071 -4.1478729 -4.1732345 -4.2154875 -4.2682986 -4.3058877][-4.1447792 -4.1174812 -4.0918531 -4.0693183 -4.0480556 -4.0453353 -4.0436444 -4.0208769 -4.0175176 -4.0500093 -4.0962315 -4.1407557 -4.1860714 -4.2378821 -4.2776952][-4.1288419 -4.1005216 -4.0695772 -4.032114 -3.9856331 -3.9496715 -3.9109926 -3.8560181 -3.8588009 -3.9377103 -4.0286098 -4.0981283 -4.1496572 -4.2039561 -4.2504234][-4.1278334 -4.0977335 -4.0657706 -4.0248089 -3.9648509 -3.8894396 -3.785424 -3.6606791 -3.6560171 -3.7963243 -3.9462037 -4.0493345 -4.1189485 -4.1881142 -4.2435956][-4.1244946 -4.0894 -4.0618768 -4.0314612 -3.974185 -3.8760402 -3.7194533 -3.5314627 -3.5182784 -3.7094388 -3.9031193 -4.0336723 -4.1184826 -4.1977777 -4.2552986][-4.1182585 -4.0835357 -4.0714378 -4.0614662 -4.0239105 -3.9385769 -3.7964613 -3.6313372 -3.6083767 -3.764864 -3.9391937 -4.063849 -4.1473475 -4.2243633 -4.2763128][-4.1095457 -4.0780249 -4.084888 -4.1031957 -4.098722 -4.046279 -3.9412785 -3.8161139 -3.7873459 -3.8876417 -4.0141706 -4.1124411 -4.1809096 -4.2464795 -4.2915564][-4.1129341 -4.0856924 -4.1001177 -4.1351881 -4.1580219 -4.1330175 -4.060091 -3.9720006 -3.9466267 -4.0055923 -4.0922956 -4.1647263 -4.2184949 -4.2701988 -4.3047733][-4.1299682 -4.1080885 -4.1200066 -4.1574831 -4.1960721 -4.1949167 -4.1564322 -4.1009145 -4.0817318 -4.1137662 -4.1706414 -4.2222028 -4.2626724 -4.2991982 -4.3220558][-4.1784782 -4.1650329 -4.1724892 -4.2009344 -4.2352295 -4.2410059 -4.226336 -4.1958742 -4.1844683 -4.2035756 -4.2407074 -4.2750187 -4.3029609 -4.3266859 -4.3410492][-4.25115 -4.2431335 -4.2450757 -4.2625217 -4.2855167 -4.29051 -4.2866116 -4.2709384 -4.2634735 -4.2741637 -4.2982292 -4.3188639 -4.3338909 -4.3475747 -4.3563542][-4.3116121 -4.3068852 -4.3060141 -4.3150015 -4.3265128 -4.3295302 -4.3304195 -4.3248782 -4.3206258 -4.3247905 -4.3387461 -4.3492732 -4.3554997 -4.3618889 -4.3647523]]...]
INFO - root - 2017-12-05 13:10:04.285385: step 11410, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 77h:14m:44s remains)
INFO - root - 2017-12-05 13:10:12.766257: step 11420, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 78h:54m:04s remains)
INFO - root - 2017-12-05 13:10:21.405006: step 11430, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 76h:49m:25s remains)
INFO - root - 2017-12-05 13:10:29.807299: step 11440, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 73h:45m:26s remains)
INFO - root - 2017-12-05 13:10:38.187154: step 11450, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.839 sec/batch; 74h:51m:57s remains)
INFO - root - 2017-12-05 13:10:46.653425: step 11460, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 76h:32m:45s remains)
INFO - root - 2017-12-05 13:10:55.095927: step 11470, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 75h:39m:12s remains)
INFO - root - 2017-12-05 13:11:03.542117: step 11480, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 77h:12m:13s remains)
INFO - root - 2017-12-05 13:11:11.986681: step 11490, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 76h:54m:01s remains)
INFO - root - 2017-12-05 13:11:20.518205: step 11500, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 75h:21m:13s remains)
2017-12-05 13:11:21.272267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3029737 -4.3009939 -4.2986832 -4.2937284 -4.2850428 -4.2800264 -4.2829795 -4.28786 -4.2929735 -4.2931991 -4.2888026 -4.2819409 -4.2829323 -4.2879267 -4.293004][-4.2966495 -4.2936106 -4.2897797 -4.2841134 -4.2748675 -4.2678118 -4.2681713 -4.2726603 -4.2808719 -4.28661 -4.2893491 -4.2901855 -4.2951255 -4.2982378 -4.2961416][-4.2885194 -4.2818842 -4.2762403 -4.2709117 -4.2613935 -4.2502213 -4.2453837 -4.2476549 -4.2582812 -4.2699304 -4.284184 -4.2977004 -4.3094034 -4.309742 -4.2986212][-4.280735 -4.2714038 -4.26557 -4.2602139 -4.2465205 -4.22332 -4.2081137 -4.2072163 -4.2211723 -4.2421689 -4.2725592 -4.3029914 -4.3217139 -4.3210292 -4.3037996][-4.2732182 -4.2636056 -4.2579122 -4.2506285 -4.2284255 -4.1904931 -4.1602073 -4.1530056 -4.1744208 -4.2107987 -4.2593012 -4.3031087 -4.3266559 -4.3265343 -4.3075428][-4.2633538 -4.2494931 -4.2357669 -4.2209849 -4.18675 -4.131815 -4.0790062 -4.0625539 -4.1020541 -4.1658268 -4.2350183 -4.289763 -4.3170204 -4.31883 -4.2997346][-4.2487226 -4.2241921 -4.1952353 -4.1643605 -4.1111073 -4.0315075 -3.9485135 -3.9124482 -3.9820814 -4.0878453 -4.1808739 -4.2480965 -4.286294 -4.2954235 -4.2791128][-4.224246 -4.1852617 -4.1387348 -4.0887842 -4.0178614 -3.9205587 -3.8125398 -3.7590277 -3.8612561 -4.003716 -4.1132069 -4.18869 -4.2403483 -4.2638893 -4.2569032][-4.2007675 -4.1528535 -4.0965452 -4.0384865 -3.9677455 -3.8809559 -3.7956948 -3.7663302 -3.8621955 -3.9898586 -4.0849915 -4.1533289 -4.2100258 -4.2464905 -4.2552929][-4.2033167 -4.1546512 -4.0986147 -4.046454 -3.9952412 -3.941973 -3.9020987 -3.9005384 -3.9629118 -4.043066 -4.1092381 -4.1652589 -4.2185469 -4.2584767 -4.2762194][-4.2298141 -4.1850863 -4.1336594 -4.0901341 -4.0569978 -4.029211 -4.0190611 -4.0318708 -4.0695343 -4.1142921 -4.1570148 -4.2031283 -4.2499995 -4.2855687 -4.3022232][-4.2564354 -4.2187567 -4.1744685 -4.136817 -4.1168356 -4.1079049 -4.1131682 -4.133038 -4.1593857 -4.1843934 -4.2091379 -4.2449603 -4.2831559 -4.3109808 -4.3233776][-4.2744751 -4.2448249 -4.2121449 -4.1867852 -4.1821041 -4.1872249 -4.2007532 -4.2220612 -4.2408791 -4.254447 -4.2671957 -4.2905164 -4.3147626 -4.330627 -4.33557][-4.2896643 -4.2732863 -4.2574692 -4.2467518 -4.2501163 -4.2602777 -4.273881 -4.2890759 -4.3000326 -4.3064289 -4.3113379 -4.3220282 -4.3317733 -4.3366566 -4.3356867][-4.3031816 -4.2979836 -4.2938323 -4.2904668 -4.2941189 -4.30048 -4.3084788 -4.316668 -4.321466 -4.3236942 -4.3245106 -4.3280582 -4.3318572 -4.3331819 -4.3312855]]...]
INFO - root - 2017-12-05 13:11:29.705310: step 11510, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 74h:31m:15s remains)
INFO - root - 2017-12-05 13:11:38.082604: step 11520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 76h:54m:25s remains)
INFO - root - 2017-12-05 13:11:46.527564: step 11530, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 77h:18m:03s remains)
INFO - root - 2017-12-05 13:11:54.918188: step 11540, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 77h:28m:10s remains)
INFO - root - 2017-12-05 13:12:03.325028: step 11550, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 74h:12m:34s remains)
INFO - root - 2017-12-05 13:12:11.727835: step 11560, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 74h:45m:00s remains)
INFO - root - 2017-12-05 13:12:20.268840: step 11570, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.866 sec/batch; 77h:13m:56s remains)
INFO - root - 2017-12-05 13:12:28.817909: step 11580, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 77h:16m:14s remains)
INFO - root - 2017-12-05 13:12:37.177934: step 11590, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 73h:40m:20s remains)
INFO - root - 2017-12-05 13:12:45.659874: step 11600, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.805 sec/batch; 71h:43m:16s remains)
2017-12-05 13:12:46.400195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2134032 -4.2236714 -4.2312684 -4.236393 -4.2398791 -4.2435331 -4.246563 -4.2484632 -4.2505136 -4.2499323 -4.2462344 -4.2405424 -4.2316132 -4.2173424 -4.2031589][-4.1856451 -4.190793 -4.1977539 -4.2062087 -4.2150207 -4.2260642 -4.2362146 -4.2451134 -4.2526226 -4.2555747 -4.253521 -4.250493 -4.2417359 -4.2277503 -4.2160268][-4.179183 -4.1706867 -4.1672692 -4.1726146 -4.1849003 -4.2011189 -4.2179356 -4.2351279 -4.2502284 -4.2578783 -4.2591352 -4.2599993 -4.2523289 -4.2380161 -4.2280359][-4.1935959 -4.1757855 -4.162539 -4.1605854 -4.1671996 -4.1769214 -4.19068 -4.2099848 -4.2307334 -4.244628 -4.2518816 -4.25729 -4.2509565 -4.2356791 -4.2255974][-4.1999688 -4.1805224 -4.1668344 -4.1646037 -4.1683569 -4.1683173 -4.1694975 -4.1801839 -4.1945624 -4.2039342 -4.2130961 -4.225512 -4.2246594 -4.211391 -4.2038584][-4.2014174 -4.1858335 -4.1744037 -4.1727862 -4.1774364 -4.1743731 -4.1660547 -4.16678 -4.1693254 -4.1639605 -4.1638632 -4.1725922 -4.1678152 -4.1520982 -4.1480455][-4.1982679 -4.1846204 -4.174315 -4.1749029 -4.1804008 -4.1756425 -4.1656408 -4.1639547 -4.1580458 -4.1419716 -4.131712 -4.1295285 -4.1119256 -4.0804296 -4.0713358][-4.1907439 -4.174974 -4.1619272 -4.1616559 -4.167058 -4.1619229 -4.1559906 -4.1627188 -4.1636558 -4.1511126 -4.1393337 -4.1309276 -4.10549 -4.0653596 -4.0480452][-4.1907563 -4.1753869 -4.1596661 -4.1538763 -4.152658 -4.1441112 -4.1401858 -4.1521692 -4.1623611 -4.1586833 -4.1529226 -4.1471744 -4.1271877 -4.0971937 -4.0835419][-4.2034731 -4.1928453 -4.179049 -4.1702304 -4.1653 -4.1541705 -4.1476488 -4.1563072 -4.165174 -4.1617875 -4.1582174 -4.1560616 -4.146307 -4.1319218 -4.13087][-4.2358823 -4.2279282 -4.2180867 -4.2094622 -4.2056694 -4.2011662 -4.1971068 -4.2002015 -4.2031302 -4.1952715 -4.1870356 -4.1802053 -4.1737881 -4.1713362 -4.1816721][-4.2622452 -4.2546878 -4.2505331 -4.2453661 -4.2452335 -4.2482314 -4.2486219 -4.2479968 -4.2456174 -4.2355242 -4.2229114 -4.2124739 -4.2089548 -4.2137389 -4.23005][-4.2797332 -4.2755961 -4.2744265 -4.2706385 -4.2707624 -4.2757759 -4.2785769 -4.2775955 -4.2753272 -4.2686467 -4.259737 -4.2526789 -4.2505193 -4.2542582 -4.2650843][-4.297688 -4.2957597 -4.2962747 -4.2948151 -4.294703 -4.297617 -4.29909 -4.2978864 -4.2965059 -4.2920485 -4.2871914 -4.2842593 -4.2843852 -4.2879777 -4.29432][-4.3159466 -4.3148284 -4.3154988 -4.3152862 -4.3153567 -4.3157024 -4.3148627 -4.3134265 -4.3126326 -4.3102169 -4.3076372 -4.3062544 -4.3066168 -4.3079357 -4.3105168]]...]
INFO - root - 2017-12-05 13:12:54.867474: step 11610, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 78h:43m:31s remains)
INFO - root - 2017-12-05 13:13:03.320658: step 11620, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 76h:30m:45s remains)
INFO - root - 2017-12-05 13:13:11.848433: step 11630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 75h:57m:06s remains)
INFO - root - 2017-12-05 13:13:20.358627: step 11640, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 75h:51m:32s remains)
INFO - root - 2017-12-05 13:13:28.927506: step 11650, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 75h:24m:04s remains)
INFO - root - 2017-12-05 13:13:37.422709: step 11660, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 76h:00m:14s remains)
INFO - root - 2017-12-05 13:13:45.951228: step 11670, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.862 sec/batch; 76h:50m:56s remains)
INFO - root - 2017-12-05 13:13:54.491082: step 11680, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 74h:49m:55s remains)
INFO - root - 2017-12-05 13:14:03.077562: step 11690, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 75h:04m:09s remains)
INFO - root - 2017-12-05 13:14:11.499469: step 11700, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 76h:34m:01s remains)
2017-12-05 13:14:12.195915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2811842 -4.269968 -4.2600551 -4.2511139 -4.2463593 -4.2435904 -4.2372036 -4.2330217 -4.2286777 -4.2141571 -4.2060127 -4.2077594 -4.2126975 -4.2188673 -4.2261076][-4.2819357 -4.2703862 -4.258348 -4.2481556 -4.2400589 -4.2320437 -4.2212343 -4.214026 -4.2124553 -4.1974468 -4.1885591 -4.1906428 -4.1957884 -4.1999087 -4.20357][-4.2838693 -4.2660675 -4.245398 -4.22941 -4.2196593 -4.2112441 -4.2012315 -4.1988916 -4.2066464 -4.1985183 -4.1905003 -4.1899371 -4.1934175 -4.1918259 -4.18791][-4.2780447 -4.2487688 -4.2155523 -4.1934066 -4.1851845 -4.1816878 -4.17708 -4.1806188 -4.1982884 -4.1969843 -4.1882639 -4.1841741 -4.1876097 -4.1851168 -4.1775174][-4.2672873 -4.2302203 -4.1899304 -4.1622381 -4.1538143 -4.1481652 -4.1415544 -4.1459174 -4.1704226 -4.1739464 -4.1684275 -4.1673679 -4.1738043 -4.1755404 -4.1723051][-4.2580309 -4.219532 -4.1780071 -4.1458092 -4.1288028 -4.1091194 -4.0861769 -4.0829735 -4.1103344 -4.121232 -4.124383 -4.1326537 -4.1459031 -4.155324 -4.160955][-4.2506204 -4.2114863 -4.1702828 -4.13192 -4.0971985 -4.0511212 -3.9972296 -3.9831786 -4.0217991 -4.0461988 -4.0623684 -4.0867319 -4.1095591 -4.129571 -4.1471229][-4.2411408 -4.1992927 -4.1542988 -4.1084414 -4.0593615 -3.9889672 -3.904881 -3.8838856 -3.9434338 -3.9907308 -4.0216784 -4.0610642 -4.0888963 -4.1159148 -4.1449885][-4.237793 -4.1914139 -4.1429491 -4.0961637 -4.0473175 -3.9798331 -3.8991985 -3.8827345 -3.9474187 -4.0031505 -4.0395136 -4.0782485 -4.0991497 -4.1207981 -4.1492982][-4.2427931 -4.1948657 -4.1480093 -4.1083212 -4.0743041 -4.035799 -3.9887948 -3.979387 -4.02208 -4.0616288 -4.0869007 -4.1144872 -4.1242976 -4.1313829 -4.14743][-4.25581 -4.2122235 -4.169836 -4.1365824 -4.1159282 -4.1002936 -4.0805244 -4.0730605 -4.0924025 -4.1149158 -4.1289177 -4.1462412 -4.1464825 -4.1415505 -4.1446934][-4.2696419 -4.2338691 -4.1981478 -4.17 -4.1554642 -4.1479211 -4.1396532 -4.130322 -4.1313715 -4.1399441 -4.1456838 -4.156198 -4.1531343 -4.1435227 -4.14204][-4.2748046 -4.2460756 -4.2207522 -4.20248 -4.19297 -4.1847849 -4.175168 -4.1579342 -4.142817 -4.1410718 -4.1434865 -4.1511993 -4.1521235 -4.1480203 -4.1510754][-4.276804 -4.2536755 -4.2378964 -4.2269731 -4.2176356 -4.2027636 -4.1848526 -4.1594663 -4.1356883 -4.1320605 -4.1382241 -4.1490922 -4.1574507 -4.159493 -4.1636734][-4.2819009 -4.2636781 -4.251801 -4.2419958 -4.2285275 -4.207943 -4.1855969 -4.1598577 -4.1380954 -4.1379538 -4.1506066 -4.1647377 -4.1773825 -4.184587 -4.1854286]]...]
INFO - root - 2017-12-05 13:14:20.681424: step 11710, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 76h:04m:33s remains)
INFO - root - 2017-12-05 13:14:29.238662: step 11720, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 76h:06m:10s remains)
INFO - root - 2017-12-05 13:14:37.828779: step 11730, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.874 sec/batch; 77h:55m:10s remains)
INFO - root - 2017-12-05 13:14:46.264722: step 11740, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 73h:09m:50s remains)
INFO - root - 2017-12-05 13:14:54.717399: step 11750, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 75h:43m:49s remains)
INFO - root - 2017-12-05 13:15:03.093480: step 11760, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 75h:13m:07s remains)
INFO - root - 2017-12-05 13:15:11.696384: step 11770, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 76h:35m:11s remains)
INFO - root - 2017-12-05 13:15:20.224361: step 11780, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 76h:57m:48s remains)
INFO - root - 2017-12-05 13:15:28.723003: step 11790, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 76h:10m:47s remains)
INFO - root - 2017-12-05 13:15:37.188246: step 11800, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 74h:40m:40s remains)
2017-12-05 13:15:37.942216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1908956 -4.1956568 -4.2132206 -4.21846 -4.2183452 -4.2149096 -4.212606 -4.2220097 -4.2352657 -4.2477269 -4.2614717 -4.2705288 -4.2724037 -4.2767558 -4.2888575][-4.1494994 -4.1606469 -4.1865277 -4.1987967 -4.2025318 -4.2035708 -4.2062583 -4.221457 -4.238307 -4.2492194 -4.2598991 -4.2693906 -4.2721019 -4.2745142 -4.2843804][-4.1112976 -4.1218839 -4.1479988 -4.1640677 -4.1749988 -4.185267 -4.1955791 -4.2212515 -4.24801 -4.2617674 -4.2703295 -4.2783737 -4.280232 -4.2787104 -4.2825422][-4.0847769 -4.0836415 -4.0974884 -4.1077971 -4.1216812 -4.1410313 -4.1591225 -4.195405 -4.2371335 -4.26144 -4.2743859 -4.2852058 -4.2877693 -4.2838774 -4.2843137][-4.0698252 -4.0613093 -4.0636396 -4.0617285 -4.0668664 -4.0832987 -4.10337 -4.1456447 -4.1982212 -4.233552 -4.2559381 -4.2773004 -4.2869606 -4.2875896 -4.2898369][-4.0722671 -4.0675673 -4.0661025 -4.0558019 -4.0516114 -4.0593824 -4.0724945 -4.1089406 -4.161788 -4.2028 -4.2276912 -4.2516079 -4.2682061 -4.2767119 -4.2851782][-4.0626664 -4.0630951 -4.0645332 -4.0498281 -4.0445247 -4.0481954 -4.0569186 -4.0828238 -4.127244 -4.1660948 -4.1897545 -4.2124472 -4.2320428 -4.2490616 -4.2680922][-4.0281367 -4.0290275 -4.0305548 -4.0133395 -4.0069685 -4.0095921 -4.0178828 -4.0431347 -4.0858636 -4.1248059 -4.1513023 -4.1733646 -4.1919107 -4.2137394 -4.2429943][-4.0023432 -4.0023074 -4.0044112 -3.9868038 -3.9745867 -3.9719727 -3.9812748 -4.0092559 -4.0507641 -4.0898924 -4.1202612 -4.1454983 -4.162796 -4.1862297 -4.2227783][-4.0253577 -4.0209284 -4.0250258 -4.0127859 -3.9983561 -3.9906261 -3.9946959 -4.0170383 -4.0504694 -4.0827847 -4.1102433 -4.1333117 -4.1490779 -4.173449 -4.214169][-4.0704741 -4.0634956 -4.0654793 -4.0568142 -4.0441532 -4.0369911 -4.0384917 -4.0556607 -4.0773478 -4.0981507 -4.1164932 -4.1332717 -4.1469908 -4.1727357 -4.21655][-4.1025782 -4.0969067 -4.0992422 -4.0933027 -4.0850506 -4.0819612 -4.0850325 -4.1002407 -4.1129346 -4.1228685 -4.1345668 -4.1469965 -4.1580133 -4.1827569 -4.2255464][-4.1556053 -4.151875 -4.1538534 -4.1504455 -4.1460147 -4.1449022 -4.1485686 -4.1587834 -4.1636314 -4.1670237 -4.174901 -4.1821594 -4.18791 -4.2061415 -4.2405486][-4.2211571 -4.2192159 -4.2205453 -4.2196612 -4.2181149 -4.2175322 -4.2190156 -4.2232885 -4.2235413 -4.2249341 -4.2296424 -4.2324347 -4.2332816 -4.2421875 -4.2637792][-4.2671366 -4.2672677 -4.2683372 -4.2686467 -4.2682095 -4.2676244 -4.2678223 -4.269309 -4.2701912 -4.2715092 -4.2741141 -4.2754679 -4.2756362 -4.2799997 -4.2916026]]...]
INFO - root - 2017-12-05 13:15:46.277488: step 11810, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 73h:54m:24s remains)
INFO - root - 2017-12-05 13:15:54.677767: step 11820, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.813 sec/batch; 72h:26m:09s remains)
INFO - root - 2017-12-05 13:16:03.020552: step 11830, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 74h:48m:40s remains)
INFO - root - 2017-12-05 13:16:11.464003: step 11840, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 75h:12m:04s remains)
INFO - root - 2017-12-05 13:16:19.920234: step 11850, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.819 sec/batch; 72h:57m:44s remains)
INFO - root - 2017-12-05 13:16:28.496682: step 11860, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 75h:57m:51s remains)
INFO - root - 2017-12-05 13:16:36.804290: step 11870, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.852 sec/batch; 75h:54m:47s remains)
INFO - root - 2017-12-05 13:16:45.321572: step 11880, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 76h:28m:52s remains)
INFO - root - 2017-12-05 13:16:53.821520: step 11890, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 75h:23m:04s remains)
INFO - root - 2017-12-05 13:17:02.290492: step 11900, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 74h:47m:13s remains)
2017-12-05 13:17:03.125840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2757993 -4.2627053 -4.25225 -4.245398 -4.2410927 -4.2320313 -4.2151012 -4.18989 -4.1654944 -4.1470995 -4.142652 -4.1640873 -4.2128453 -4.260056 -4.2914486][-4.2777457 -4.2636361 -4.2460594 -4.231576 -4.2240443 -4.2109632 -4.1898794 -4.1672716 -4.1463785 -4.1340408 -4.13501 -4.1600008 -4.2111754 -4.2622032 -4.2923121][-4.2645941 -4.246151 -4.2254534 -4.2114019 -4.2045689 -4.1887994 -4.1659279 -4.1390066 -4.1202893 -4.1193376 -4.129056 -4.1592789 -4.2119861 -4.25865 -4.2833285][-4.2402539 -4.2187452 -4.2007766 -4.1906896 -4.1861739 -4.1652074 -4.1366439 -4.1044283 -4.0904341 -4.1026917 -4.1235433 -4.1592836 -4.2092376 -4.2482605 -4.2652164][-4.2161093 -4.1918492 -4.1712613 -4.1549444 -4.1419373 -4.1068716 -4.0666513 -4.0347109 -4.040235 -4.0783677 -4.1144238 -4.1570396 -4.2009873 -4.2323418 -4.2432961][-4.1992335 -4.1755066 -4.1477804 -4.1159525 -4.081574 -4.0245986 -3.9610686 -3.9256601 -3.9660604 -4.0423193 -4.0962844 -4.1480632 -4.1921473 -4.2206306 -4.2316766][-4.192739 -4.1718946 -4.139802 -4.098012 -4.0463147 -3.9690046 -3.8774934 -3.8285575 -3.9004102 -4.0083137 -4.0760889 -4.1370616 -4.1842589 -4.2144628 -4.2306046][-4.1967554 -4.1762767 -4.1415062 -4.1004534 -4.0461349 -3.970439 -3.8725681 -3.81641 -3.8994246 -4.0078707 -4.0726047 -4.1350422 -4.1819406 -4.2135386 -4.2381759][-4.211297 -4.194767 -4.1665111 -4.1343703 -4.0883579 -4.0304074 -3.9469895 -3.8860273 -3.9528184 -4.0358067 -4.0854225 -4.1410165 -4.1847105 -4.2173691 -4.2496386][-4.2299342 -4.2176137 -4.1985912 -4.174624 -4.1380448 -4.1005845 -4.0359197 -3.9765661 -4.0199075 -4.0764675 -4.1140909 -4.1605234 -4.19967 -4.2322884 -4.26613][-4.2476254 -4.23516 -4.2201357 -4.202013 -4.17498 -4.1505156 -4.1053314 -4.0636845 -4.0964804 -4.1397772 -4.16825 -4.2029285 -4.2314506 -4.2565255 -4.2831945][-4.2654953 -4.2519517 -4.2374988 -4.2242861 -4.2074246 -4.1897254 -4.1586819 -4.1335306 -4.1604328 -4.1960182 -4.2180281 -4.2408376 -4.26008 -4.2743359 -4.2927938][-4.280231 -4.2623734 -4.2486739 -4.2406192 -4.2333217 -4.2209349 -4.1997375 -4.1864762 -4.2069736 -4.2313151 -4.2450347 -4.2611027 -4.27562 -4.2834897 -4.2967844][-4.2894268 -4.2720385 -4.2624006 -4.259676 -4.2578497 -4.2498932 -4.2362747 -4.2311406 -4.2447209 -4.2571936 -4.263104 -4.2729549 -4.2835069 -4.2872524 -4.2970638][-4.3041506 -4.291152 -4.2832165 -4.2807527 -4.2824764 -4.2781677 -4.2694535 -4.2655792 -4.2693009 -4.2671738 -4.2639856 -4.2683434 -4.278357 -4.2834053 -4.2944984]]...]
INFO - root - 2017-12-05 13:17:11.551826: step 11910, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 76h:20m:33s remains)
INFO - root - 2017-12-05 13:17:20.108104: step 11920, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 75h:55m:52s remains)
INFO - root - 2017-12-05 13:17:28.629598: step 11930, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 74h:46m:04s remains)
INFO - root - 2017-12-05 13:17:37.170829: step 11940, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 75h:02m:54s remains)
INFO - root - 2017-12-05 13:17:45.646094: step 11950, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.858 sec/batch; 76h:22m:11s remains)
INFO - root - 2017-12-05 13:17:54.056028: step 11960, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 75h:42m:54s remains)
INFO - root - 2017-12-05 13:18:02.595732: step 11970, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 75h:07m:48s remains)
INFO - root - 2017-12-05 13:18:10.995289: step 11980, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 76h:46m:08s remains)
INFO - root - 2017-12-05 13:18:19.571008: step 11990, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 74h:31m:44s remains)
INFO - root - 2017-12-05 13:18:28.130902: step 12000, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 76h:52m:25s remains)
2017-12-05 13:18:28.919751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2258658 -4.2224569 -4.2182441 -4.2164764 -4.2227788 -4.2244349 -4.2165012 -4.20694 -4.2049284 -4.2175941 -4.2294316 -4.2339597 -4.238831 -4.2302971 -4.2253838][-4.1981931 -4.1952963 -4.1916957 -4.1916409 -4.1941552 -4.1900344 -4.1737556 -4.156918 -4.1507664 -4.1646247 -4.1826982 -4.1890011 -4.1984539 -4.1884189 -4.1818485][-4.1709967 -4.1669555 -4.1629009 -4.167829 -4.1707106 -4.1620731 -4.1379204 -4.1134253 -4.1044283 -4.1176929 -4.1347065 -4.139431 -4.1534557 -4.1488233 -4.1438317][-4.1582766 -4.15321 -4.1454124 -4.1526475 -4.1580181 -4.1471114 -4.1216149 -4.0962319 -4.0845833 -4.0901113 -4.09416 -4.0960793 -4.1217942 -4.1291065 -4.1296253][-4.1627717 -4.1493087 -4.1313415 -4.1352792 -4.1424804 -4.1339989 -4.1137 -4.0877757 -4.0667095 -4.0598807 -4.049737 -4.0504546 -4.0869608 -4.1116104 -4.1259][-4.1704092 -4.1490188 -4.1219568 -4.1213493 -4.1281366 -4.1204653 -4.09937 -4.0726933 -4.0461621 -4.022222 -3.9943988 -3.9951127 -4.0454197 -4.0887604 -4.1212411][-4.1801996 -4.1610265 -4.1371431 -4.13103 -4.1309423 -4.1167355 -4.0886493 -4.0655184 -4.0422325 -4.0061121 -3.9628429 -3.965518 -4.0239325 -4.0779829 -4.1203518][-4.1842961 -4.1746721 -4.1603732 -4.1503839 -4.1420937 -4.1255155 -4.0976186 -4.0785785 -4.0638452 -4.0248022 -3.9838922 -3.9942269 -4.0433521 -4.0877509 -4.1276274][-4.1864676 -4.1806774 -4.1685019 -4.1534247 -4.1449418 -4.1365852 -4.1191778 -4.1076822 -4.0996594 -4.0696106 -4.0440745 -4.0574784 -4.0879927 -4.1095142 -4.1394424][-4.187994 -4.1737027 -4.1518517 -4.1294737 -4.1277318 -4.1407509 -4.1424994 -4.1436906 -4.1374235 -4.1168675 -4.1026554 -4.1081972 -4.1163545 -4.1179953 -4.1357665][-4.1924157 -4.1660628 -4.1286221 -4.0993328 -4.1016035 -4.1333041 -4.1538587 -4.1657171 -4.1616325 -4.147027 -4.1394863 -4.1365275 -4.1230626 -4.1141992 -4.1254549][-4.2006712 -4.1705308 -4.1250286 -4.0902348 -4.090579 -4.1286378 -4.1550584 -4.1720119 -4.1716061 -4.1602707 -4.1528726 -4.1408033 -4.1117887 -4.09729 -4.1107669][-4.2196331 -4.194066 -4.1510015 -4.1170378 -4.11325 -4.148108 -4.1667204 -4.1794229 -4.1816134 -4.1731911 -4.1624651 -4.1455712 -4.1149979 -4.1018982 -4.1178327][-4.240088 -4.2208562 -4.1858635 -4.15707 -4.1549459 -4.1811314 -4.1881218 -4.1893969 -4.1912761 -4.1874137 -4.1823044 -4.1738234 -4.1555438 -4.1451468 -4.1589022][-4.2628064 -4.2475591 -4.2193937 -4.1947513 -4.1905465 -4.1990032 -4.1968293 -4.19694 -4.2034521 -4.2089534 -4.2124357 -4.2151814 -4.2075639 -4.1969485 -4.2022839]]...]
INFO - root - 2017-12-05 13:18:37.306121: step 12010, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 74h:44m:09s remains)
INFO - root - 2017-12-05 13:18:45.837755: step 12020, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.829 sec/batch; 73h:47m:09s remains)
INFO - root - 2017-12-05 13:18:54.184963: step 12030, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 76h:00m:53s remains)
INFO - root - 2017-12-05 13:19:02.648036: step 12040, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.855 sec/batch; 76h:07m:50s remains)
INFO - root - 2017-12-05 13:19:11.063452: step 12050, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 74h:46m:43s remains)
INFO - root - 2017-12-05 13:19:19.629789: step 12060, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 75h:07m:17s remains)
INFO - root - 2017-12-05 13:19:28.130446: step 12070, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 74h:32m:08s remains)
INFO - root - 2017-12-05 13:19:36.809197: step 12080, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 0.791 sec/batch; 70h:26m:38s remains)
INFO - root - 2017-12-05 13:19:45.374304: step 12090, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 75h:42m:40s remains)
INFO - root - 2017-12-05 13:19:53.959860: step 12100, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 77h:23m:26s remains)
2017-12-05 13:19:54.726558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2043772 -4.2030892 -4.1927176 -4.1763258 -4.1622 -4.1675591 -4.1966128 -4.2333579 -4.2515106 -4.2584043 -4.2596531 -4.2556458 -4.2525144 -4.2602539 -4.2708774][-4.1977448 -4.1939607 -4.1794848 -4.1624141 -4.15389 -4.1663742 -4.2026944 -4.2433472 -4.2618446 -4.2660995 -4.2646675 -4.2552333 -4.246201 -4.247035 -4.2497349][-4.1922708 -4.1922913 -4.178885 -4.159924 -4.149766 -4.1639309 -4.20422 -4.247757 -4.2674689 -4.2719774 -4.2720304 -4.2628756 -4.2512374 -4.2445168 -4.2367463][-4.1964664 -4.2027907 -4.1958065 -4.1743464 -4.1531372 -4.1547642 -4.188201 -4.2310824 -4.2581573 -4.2704329 -4.2735415 -4.2670827 -4.2588148 -4.2524776 -4.2445703][-4.2133412 -4.2228112 -4.2188549 -4.194984 -4.1570668 -4.1316009 -4.1419096 -4.1830354 -4.2277327 -4.254981 -4.2652574 -4.2636261 -4.2599797 -4.2569356 -4.2547274][-4.2316041 -4.2389951 -4.2302761 -4.1943355 -4.134644 -4.0752892 -4.0517192 -4.092205 -4.1648116 -4.2182946 -4.2444692 -4.2531905 -4.2564149 -4.25592 -4.2574339][-4.2476664 -4.2517972 -4.2381878 -4.1935911 -4.1187072 -4.0266433 -3.9582546 -3.9835985 -4.0832758 -4.1689191 -4.2175393 -4.2404218 -4.2506342 -4.2533464 -4.2542977][-4.2545943 -4.2576485 -4.245986 -4.2069473 -4.1407604 -4.045866 -3.9470136 -3.9304326 -4.016675 -4.1115232 -4.1788864 -4.2213068 -4.2418427 -4.2472429 -4.2483206][-4.258667 -4.2613134 -4.254756 -4.2290421 -4.1844459 -4.1145592 -4.0319071 -3.9907227 -4.0210719 -4.0783639 -4.1343355 -4.1862845 -4.2181416 -4.2321439 -4.2403364][-4.2628155 -4.2663803 -4.2660012 -4.2541938 -4.2257032 -4.1804113 -4.1240754 -4.0872335 -4.0887542 -4.1060553 -4.1301627 -4.1642437 -4.1920786 -4.2116265 -4.2295151][-4.2607722 -4.2668376 -4.2715344 -4.2704043 -4.2554169 -4.2279639 -4.190022 -4.1657486 -4.1600928 -4.1623993 -4.1712303 -4.1856971 -4.1958842 -4.2046261 -4.2208443][-4.2655721 -4.2715988 -4.2765961 -4.2769918 -4.2701817 -4.2533674 -4.2242851 -4.2056122 -4.2003775 -4.2012258 -4.2109694 -4.2217617 -4.2225752 -4.220027 -4.2227859][-4.2811465 -4.2834973 -4.2804914 -4.2744308 -4.2673736 -4.2526646 -4.2261019 -4.2051315 -4.2001386 -4.2083921 -4.226778 -4.2427216 -4.2436972 -4.2399526 -4.2370973][-4.2947049 -4.2938271 -4.283124 -4.2707438 -4.2587767 -4.2406411 -4.2114806 -4.1856642 -4.1807795 -4.19519 -4.2202044 -4.2415333 -4.246583 -4.2476783 -4.2510295][-4.2996969 -4.2954373 -4.2806182 -4.2666788 -4.2536149 -4.234066 -4.2027907 -4.174706 -4.1729174 -4.1931381 -4.2204256 -4.2395105 -4.2408586 -4.2415404 -4.250967]]...]
INFO - root - 2017-12-05 13:20:03.257415: step 12110, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 78h:38m:02s remains)
INFO - root - 2017-12-05 13:20:11.838572: step 12120, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 77h:39m:44s remains)
INFO - root - 2017-12-05 13:20:20.391350: step 12130, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 75h:03m:21s remains)
INFO - root - 2017-12-05 13:20:28.852366: step 12140, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 74h:52m:58s remains)
INFO - root - 2017-12-05 13:20:37.445149: step 12150, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 75h:09m:07s remains)
INFO - root - 2017-12-05 13:20:46.013747: step 12160, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 76h:13m:38s remains)
INFO - root - 2017-12-05 13:20:54.530846: step 12170, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 76h:50m:10s remains)
INFO - root - 2017-12-05 13:21:03.057169: step 12180, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 73h:55m:35s remains)
INFO - root - 2017-12-05 13:21:11.660806: step 12190, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.873 sec/batch; 77h:43m:00s remains)
INFO - root - 2017-12-05 13:21:20.125152: step 12200, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 75h:12m:40s remains)
2017-12-05 13:21:20.853882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2924123 -4.2988076 -4.3028193 -4.3040738 -4.3050036 -4.3076334 -4.3132577 -4.3176842 -4.3187356 -4.3163886 -4.3118796 -4.30874 -4.3078856 -4.3086524 -4.3075051][-4.2811766 -4.2905359 -4.2980142 -4.3008523 -4.3008995 -4.3006148 -4.3048935 -4.311451 -4.3157268 -4.3151965 -4.31045 -4.3059492 -4.3062253 -4.3084793 -4.3069406][-4.2558346 -4.268332 -4.2790942 -4.2826815 -4.2786851 -4.272439 -4.2727246 -4.2826233 -4.29442 -4.3022404 -4.3039632 -4.3023448 -4.3048577 -4.3083391 -4.3056817][-4.2142992 -4.2265024 -4.2373228 -4.2375555 -4.226614 -4.2117243 -4.2083 -4.2236862 -4.2478714 -4.2690291 -4.283699 -4.2910566 -4.3000073 -4.3060303 -4.3024569][-4.1572394 -4.1660652 -4.175025 -4.1711793 -4.1508088 -4.124558 -4.1148658 -4.1351786 -4.1736183 -4.2099504 -4.2386093 -4.2596679 -4.2769036 -4.2844343 -4.27995][-4.0966597 -4.0986052 -4.1044645 -4.0952597 -4.0646906 -4.0225725 -3.9946535 -4.0097556 -4.0617371 -4.1192608 -4.1673684 -4.2042613 -4.2322726 -4.2436752 -4.2394543][-4.0501184 -4.0411139 -4.0394688 -4.0231519 -3.9810045 -3.9167261 -3.8583331 -3.8613298 -3.9338675 -4.0214205 -4.0939269 -4.1459637 -4.183517 -4.2005591 -4.2001963][-4.0214858 -3.9969075 -3.9811616 -3.9574146 -3.9133027 -3.8395123 -3.7568181 -3.74374 -3.8331351 -3.9447827 -4.0342932 -4.0946684 -4.1363211 -4.1600852 -4.1698394][-4.0150166 -3.9786646 -3.9512613 -3.9275026 -3.9000897 -3.8500171 -3.7873945 -3.7732606 -3.8400629 -3.9312861 -4.0084915 -4.0619593 -4.0961308 -4.1208644 -4.1440792][-4.0300713 -3.9913909 -3.9634438 -3.9495556 -3.9462447 -3.9315419 -3.9036558 -3.8941531 -3.9238286 -3.9724424 -4.0193033 -4.0545521 -4.0786395 -4.1026669 -4.13133][-4.0618267 -4.0285707 -4.0092154 -4.007761 -4.0186167 -4.0220366 -4.0120087 -4.0059361 -4.013813 -4.0317497 -4.0510683 -4.06987 -4.08923 -4.1116595 -4.1392221][-4.0965662 -4.071321 -4.0608654 -4.0661426 -4.0811806 -4.0916629 -4.0910773 -4.0878205 -4.0890622 -4.0941029 -4.1000738 -4.1109958 -4.1270075 -4.1459541 -4.16733][-4.1300926 -4.1121688 -4.1072927 -4.11468 -4.1285119 -4.1387596 -4.1412292 -4.1409688 -4.1412048 -4.14339 -4.1493454 -4.1608872 -4.1755939 -4.1906409 -4.206368][-4.1657023 -4.154098 -4.1529031 -4.1601171 -4.1706982 -4.1770144 -4.1793261 -4.1801829 -4.1814828 -4.1842909 -4.191668 -4.2047372 -4.2198834 -4.23298 -4.2437363][-4.2092009 -4.2036881 -4.204864 -4.2104564 -4.2172136 -4.2201519 -4.2205868 -4.2204103 -4.2209935 -4.2230272 -4.2287073 -4.23851 -4.2509537 -4.2620087 -4.2698503]]...]
INFO - root - 2017-12-05 13:21:29.337097: step 12210, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 76h:41m:12s remains)
INFO - root - 2017-12-05 13:21:37.781331: step 12220, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 75h:13m:25s remains)
INFO - root - 2017-12-05 13:21:46.272204: step 12230, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 73h:50m:27s remains)
INFO - root - 2017-12-05 13:21:54.766642: step 12240, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 75h:51m:53s remains)
INFO - root - 2017-12-05 13:22:03.098651: step 12250, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 75h:49m:42s remains)
INFO - root - 2017-12-05 13:22:11.597388: step 12260, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 75h:10m:07s remains)
INFO - root - 2017-12-05 13:22:20.073985: step 12270, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 74h:59m:21s remains)
INFO - root - 2017-12-05 13:22:28.592479: step 12280, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 73h:34m:04s remains)
INFO - root - 2017-12-05 13:22:37.309805: step 12290, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 75h:46m:28s remains)
INFO - root - 2017-12-05 13:22:45.713816: step 12300, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 75h:01m:03s remains)
2017-12-05 13:22:46.469863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.181921 -4.1851959 -4.1888385 -4.1892323 -4.1880379 -4.1841197 -4.1659851 -4.1528587 -4.1682777 -4.1967969 -4.2200379 -4.238884 -4.2516289 -4.2629662 -4.2746286][-4.1636496 -4.1778278 -4.1888843 -4.1939831 -4.1956444 -4.1944461 -4.1760783 -4.1579766 -4.1642823 -4.1890121 -4.2157693 -4.2320342 -4.239203 -4.2456856 -4.2565827][-4.1514254 -4.1690164 -4.1825266 -4.1903768 -4.1946096 -4.1896853 -4.1666369 -4.1459661 -4.1490335 -4.1719346 -4.1989145 -4.2129431 -4.2174563 -4.2206721 -4.2322383][-4.1310539 -4.1557112 -4.1738667 -4.1855421 -4.1840663 -4.1674957 -4.1376696 -4.1201344 -4.1276536 -4.1540885 -4.1789613 -4.1919208 -4.1949291 -4.1952085 -4.2027011][-4.1046362 -4.1389 -4.1648431 -4.1786394 -4.1668715 -4.1345782 -4.0944748 -4.071106 -4.0845532 -4.1216254 -4.152112 -4.167995 -4.171742 -4.1682673 -4.1662755][-4.0894895 -4.1293788 -4.1573071 -4.1646423 -4.1416674 -4.0951886 -4.0354586 -3.9898832 -4.0085683 -4.0707941 -4.1161051 -4.1290011 -4.126749 -4.1234646 -4.1195569][-4.0853233 -4.1233964 -4.14641 -4.1444554 -4.1172776 -4.0590615 -3.973913 -3.8999994 -3.9200413 -4.012979 -4.072341 -4.0729756 -4.0628819 -4.0703707 -4.0783644][-4.0819674 -4.1128893 -4.1272087 -4.1203341 -4.0975971 -4.0451908 -3.9548337 -3.8700025 -3.8917811 -3.9981005 -4.064734 -4.0642109 -4.0560813 -4.0716882 -4.0877585][-4.0924058 -4.1172361 -4.1276569 -4.1284032 -4.1213427 -4.092474 -4.0301642 -3.9679487 -3.9837058 -4.0611463 -4.1130295 -4.1139803 -4.1085224 -4.117167 -4.1249733][-4.150383 -4.1636305 -4.1686039 -4.1724691 -4.1793437 -4.1704707 -4.1352625 -4.0940228 -4.09595 -4.1333685 -4.15481 -4.1463213 -4.1371346 -4.1393733 -4.1468744][-4.2072067 -4.2110538 -4.207428 -4.2057061 -4.2103171 -4.2073336 -4.1876249 -4.1668415 -4.1665878 -4.1814351 -4.1789823 -4.1553617 -4.1406093 -4.1459055 -4.1620727][-4.2399487 -4.2412448 -4.2331948 -4.2235413 -4.2170162 -4.2109981 -4.2004166 -4.19306 -4.1990438 -4.2059989 -4.1889577 -4.1573524 -4.1430149 -4.1566749 -4.1789761][-4.2476144 -4.2483587 -4.2400384 -4.22722 -4.2176924 -4.2120924 -4.2063875 -4.2041788 -4.2118115 -4.213376 -4.191164 -4.1632867 -4.1572247 -4.1753826 -4.1953835][-4.2339458 -4.2325048 -4.2285767 -4.2236972 -4.2192163 -4.2145352 -4.2102852 -4.2093024 -4.215117 -4.2139859 -4.1938558 -4.1723032 -4.1716638 -4.18725 -4.2054052][-4.2204924 -4.2155614 -4.213047 -4.2145214 -4.2163291 -4.2144055 -4.2142062 -4.2173185 -4.221447 -4.2194061 -4.2052617 -4.1908011 -4.1916227 -4.2019749 -4.2112293]]...]
INFO - root - 2017-12-05 13:22:54.975198: step 12310, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 76h:56m:29s remains)
INFO - root - 2017-12-05 13:23:03.498707: step 12320, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 77h:02m:52s remains)
INFO - root - 2017-12-05 13:23:12.090577: step 12330, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 76h:20m:59s remains)
INFO - root - 2017-12-05 13:23:20.566775: step 12340, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 75h:26m:12s remains)
INFO - root - 2017-12-05 13:23:29.160267: step 12350, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 75h:27m:44s remains)
INFO - root - 2017-12-05 13:23:37.596870: step 12360, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 77h:34m:20s remains)
INFO - root - 2017-12-05 13:23:46.153889: step 12370, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 75h:14m:15s remains)
INFO - root - 2017-12-05 13:23:54.699693: step 12380, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 74h:35m:31s remains)
INFO - root - 2017-12-05 13:24:03.376702: step 12390, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 75h:52m:26s remains)
INFO - root - 2017-12-05 13:24:11.977237: step 12400, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 76h:46m:21s remains)
2017-12-05 13:24:12.703674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3146882 -4.3029056 -4.292222 -4.2885861 -4.2920356 -4.3007174 -4.310617 -4.3163004 -4.3185606 -4.3228517 -4.3282671 -4.3287487 -4.3246284 -4.3240595 -4.327199][-4.2969418 -4.279655 -4.2657647 -4.2615728 -4.2641845 -4.2750931 -4.2865171 -4.2918282 -4.292872 -4.2990112 -4.3080721 -4.3080578 -4.3003197 -4.3010616 -4.3096089][-4.2778 -4.2561345 -4.2392106 -4.2331429 -4.2332149 -4.2428064 -4.2512641 -4.2516823 -4.25029 -4.2621331 -4.2764959 -4.2698755 -4.2534885 -4.2560735 -4.2753172][-4.2628455 -4.23396 -4.2110524 -4.1989589 -4.197402 -4.2054906 -4.2052979 -4.1921864 -4.1850905 -4.2067857 -4.23398 -4.2295518 -4.2083817 -4.2122107 -4.2407651][-4.247714 -4.2094259 -4.1752253 -4.1519122 -4.15002 -4.1595964 -4.1439829 -4.1082191 -4.0983052 -4.1376357 -4.1820168 -4.18682 -4.1694155 -4.1783419 -4.2118998][-4.2350278 -4.18756 -4.1409817 -4.1012964 -4.0888009 -4.0873713 -4.0433059 -3.9692378 -3.9566338 -4.0329823 -4.109458 -4.1291 -4.122282 -4.1386914 -4.1746745][-4.228858 -4.1783538 -4.125227 -4.0728741 -4.0423017 -4.019979 -3.9449449 -3.8267698 -3.8093057 -3.9265749 -4.0376353 -4.0806227 -4.0921364 -4.1190791 -4.1546187][-4.2276473 -4.178412 -4.1264319 -4.0705719 -4.029654 -3.9957247 -3.912621 -3.7916915 -3.7722876 -3.8819528 -3.9937165 -4.0489736 -4.0696559 -4.1023097 -4.139061][-4.2303314 -4.1856027 -4.1387691 -4.0897837 -4.0513539 -4.0227566 -3.9591687 -3.8737772 -3.8608487 -3.9299598 -4.002943 -4.0412192 -4.0580206 -4.0920987 -4.1327782][-4.2364044 -4.2002707 -4.1639853 -4.1271596 -4.1010008 -4.0874424 -4.0497708 -4.0025005 -3.9954963 -4.0320888 -4.065711 -4.07383 -4.0756216 -4.1017332 -4.1388369][-4.2521634 -4.2246737 -4.1993613 -4.1733809 -4.1578894 -4.1588931 -4.144206 -4.1238723 -4.1212816 -4.1370473 -4.1451807 -4.1353064 -4.124794 -4.1390982 -4.1667442][-4.2771344 -4.2558355 -4.2376018 -4.2199426 -4.2115211 -4.2198763 -4.2207885 -4.2156496 -4.2187767 -4.2286248 -4.2300482 -4.21885 -4.2052059 -4.2069354 -4.2220349][-4.29548 -4.2768888 -4.2617793 -4.2498655 -4.2486429 -4.2609224 -4.2695255 -4.2713218 -4.2770791 -4.2862349 -4.289897 -4.2839813 -4.2747736 -4.2727513 -4.2782507][-4.307528 -4.2905416 -4.2768593 -4.2693472 -4.2731767 -4.2874846 -4.2988305 -4.3037448 -4.3102927 -4.3188443 -4.3229542 -4.3201642 -4.3154325 -4.3148789 -4.3156652][-4.3215008 -4.3086886 -4.2973642 -4.2912326 -4.2940016 -4.3040366 -4.3126736 -4.318821 -4.3250375 -4.3318634 -4.3362 -4.3357825 -4.3336115 -4.3337078 -4.3323216]]...]
INFO - root - 2017-12-05 13:24:21.099030: step 12410, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 76h:21m:51s remains)
INFO - root - 2017-12-05 13:24:29.704601: step 12420, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.860 sec/batch; 76h:27m:10s remains)
INFO - root - 2017-12-05 13:24:38.376564: step 12430, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 78h:36m:41s remains)
INFO - root - 2017-12-05 13:24:47.010074: step 12440, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 76h:48m:38s remains)
INFO - root - 2017-12-05 13:24:55.575919: step 12450, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 77h:25m:59s remains)
INFO - root - 2017-12-05 13:25:04.210578: step 12460, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 78h:21m:23s remains)
INFO - root - 2017-12-05 13:25:12.796139: step 12470, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 78h:15m:45s remains)
INFO - root - 2017-12-05 13:25:21.350292: step 12480, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 75h:46m:10s remains)
INFO - root - 2017-12-05 13:25:30.050514: step 12490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 76h:34m:14s remains)
INFO - root - 2017-12-05 13:25:38.691552: step 12500, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 74h:32m:23s remains)
2017-12-05 13:25:39.502974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2580481 -4.2493372 -4.2405477 -4.2346277 -4.2213449 -4.206151 -4.2001452 -4.1781 -4.1590042 -4.149394 -4.1578674 -4.1846538 -4.2302628 -4.2675767 -4.2937927][-4.2726316 -4.2628031 -4.2506094 -4.2404218 -4.2226024 -4.2007675 -4.1873183 -4.15685 -4.1320219 -4.1183681 -4.1273255 -4.1636758 -4.2214179 -4.2692533 -4.2988033][-4.2875762 -4.277338 -4.2652817 -4.2530394 -4.2352505 -4.2147336 -4.2028532 -4.1754375 -4.1508021 -4.1365767 -4.1461439 -4.1849208 -4.2397118 -4.2853847 -4.3097854][-4.2931967 -4.2844181 -4.2749949 -4.2631516 -4.2494483 -4.2353292 -4.2279334 -4.2097349 -4.1937041 -4.1851382 -4.1946917 -4.2278738 -4.2715116 -4.3079629 -4.326056][-4.2880745 -4.2798719 -4.2696753 -4.255147 -4.2424808 -4.232614 -4.2271509 -4.2152619 -4.2056274 -4.2047286 -4.2191133 -4.25115 -4.2889605 -4.3208103 -4.3347082][-4.2749319 -4.2649717 -4.2467175 -4.2206974 -4.1962156 -4.1752429 -4.161746 -4.1496305 -4.1472621 -4.1627259 -4.19319 -4.2369351 -4.2798929 -4.3148556 -4.330091][-4.2562876 -4.2425108 -4.2095394 -4.1629119 -4.1121173 -4.0630488 -4.0342908 -4.03317 -4.0510983 -4.0923305 -4.1459107 -4.2050838 -4.254684 -4.2965288 -4.3169622][-4.2433767 -4.2260108 -4.179275 -4.1140714 -4.0426717 -3.9776831 -3.9520125 -3.98056 -4.0249496 -4.083714 -4.1462307 -4.2062473 -4.2517371 -4.2917986 -4.3139811][-4.2443719 -4.228653 -4.1816864 -4.1219654 -4.0607653 -4.0187192 -4.0187407 -4.0639725 -4.1075368 -4.1540794 -4.2020874 -4.2486682 -4.2794113 -4.307477 -4.322298][-4.2495723 -4.23786 -4.2006311 -4.1605997 -4.1217923 -4.1058707 -4.1247535 -4.1646571 -4.1917024 -4.22194 -4.2529721 -4.2830033 -4.3009024 -4.3186007 -4.3273034][-4.2350845 -4.22759 -4.2018256 -4.178946 -4.1567268 -4.1564107 -4.1835842 -4.2155609 -4.2288809 -4.2460203 -4.2664413 -4.2834272 -4.2950306 -4.3076916 -4.3169055][-4.219274 -4.2235641 -4.210494 -4.1925783 -4.1716242 -4.1733384 -4.1982236 -4.2203712 -4.2230544 -4.2304473 -4.2464108 -4.258306 -4.2708364 -4.2843547 -4.3013282][-4.2096252 -4.2223611 -4.2147188 -4.1907973 -4.164247 -4.1636567 -4.1908212 -4.2115593 -4.2116923 -4.2156029 -4.2315073 -4.2431245 -4.2578344 -4.2732048 -4.2927375][-4.1989083 -4.215796 -4.2087502 -4.1773987 -4.149363 -4.1488433 -4.1807823 -4.2100081 -4.2191205 -4.2272253 -4.2467031 -4.2608037 -4.2753172 -4.2866278 -4.2978539][-4.1777387 -4.1930027 -4.1773386 -4.1382546 -4.1133695 -4.122272 -4.1613398 -4.2024984 -4.2259269 -4.2412281 -4.2644663 -4.2771311 -4.2894788 -4.2953472 -4.298183]]...]
INFO - root - 2017-12-05 13:25:47.869933: step 12510, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 74h:13m:00s remains)
INFO - root - 2017-12-05 13:25:56.480957: step 12520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 76h:38m:08s remains)
INFO - root - 2017-12-05 13:26:05.092562: step 12530, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 75h:05m:36s remains)
INFO - root - 2017-12-05 13:26:13.511389: step 12540, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 76h:16m:20s remains)
INFO - root - 2017-12-05 13:26:22.027530: step 12550, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 77h:28m:06s remains)
INFO - root - 2017-12-05 13:26:30.484715: step 12560, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 76h:27m:37s remains)
INFO - root - 2017-12-05 13:26:39.117239: step 12570, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.868 sec/batch; 77h:09m:47s remains)
INFO - root - 2017-12-05 13:26:47.512360: step 12580, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 73h:43m:28s remains)
INFO - root - 2017-12-05 13:26:56.087895: step 12590, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 75h:47m:34s remains)
INFO - root - 2017-12-05 13:27:04.669534: step 12600, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 78h:30m:19s remains)
2017-12-05 13:27:05.400004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0362253 -4.0334606 -4.0086217 -3.9910727 -3.99145 -3.9979706 -4.0032072 -4.0182366 -4.0312881 -4.0289888 -4.0380926 -4.0759778 -4.1207533 -4.1601286 -4.2048941][-4.0236921 -4.0141811 -3.9849329 -3.9659019 -3.9614584 -3.9630022 -3.9722795 -3.99116 -4.005506 -4.0053563 -4.0136437 -4.0511503 -4.0987563 -4.1464777 -4.2009768][-4.021853 -4.0063448 -3.9739738 -3.9523318 -3.9413545 -3.9353795 -3.9466262 -3.9693558 -3.9911015 -3.9993734 -4.017139 -4.048924 -4.0891194 -4.1345267 -4.189486][-4.0285139 -4.009376 -3.9807041 -3.9528203 -3.9287498 -3.9121323 -3.9213977 -3.9461555 -3.9753475 -3.9961276 -4.0262704 -4.0562606 -4.087739 -4.1280394 -4.1790953][-4.0480766 -4.0332251 -4.0096927 -3.9735808 -3.9311318 -3.8971536 -3.8948767 -3.915832 -3.9551635 -3.9943473 -4.0414624 -4.0777578 -4.1086988 -4.1425791 -4.1836543][-4.0736427 -4.0661759 -4.0439472 -3.9979985 -3.9352863 -3.8814831 -3.8695362 -3.8893719 -3.931706 -3.9825926 -4.0466857 -4.1018696 -4.1433511 -4.1733451 -4.2020068][-4.0958214 -4.0954123 -4.0726862 -4.0229297 -3.9585664 -3.9019337 -3.8880389 -3.9021504 -3.9326458 -3.9801257 -4.0494852 -4.1211972 -4.1726589 -4.2004852 -4.2192669][-4.1224341 -4.1285586 -4.1109304 -4.0659389 -4.0094843 -3.9625313 -3.9494925 -3.9511266 -3.9635575 -3.9988925 -4.0620847 -4.1390538 -4.1915135 -4.2141266 -4.2295976][-4.15206 -4.1631975 -4.1550708 -4.121139 -4.0734043 -4.0332103 -4.0208521 -4.0173244 -4.022 -4.0419145 -4.090858 -4.1570144 -4.2024097 -4.2185435 -4.2331271][-4.1745534 -4.1927352 -4.19295 -4.1705618 -4.1338587 -4.1017904 -4.08971 -4.0851398 -4.0870175 -4.0941067 -4.1244736 -4.1723242 -4.2079554 -4.2213387 -4.2367973][-4.1932898 -4.2131462 -4.2183485 -4.206418 -4.1832418 -4.1594238 -4.1475844 -4.1422806 -4.1404433 -4.1391406 -4.1536312 -4.1861773 -4.2143283 -4.2270455 -4.2448459][-4.2000608 -4.2174768 -4.2255864 -4.2220812 -4.2110372 -4.198019 -4.18811 -4.1832628 -4.1784172 -4.1737709 -4.1812272 -4.2051606 -4.2274647 -4.2404752 -4.2580605][-4.2176518 -4.2283726 -4.2333665 -4.23304 -4.229259 -4.2247219 -4.2196803 -4.2169676 -4.2129607 -4.2081828 -4.2130146 -4.2301431 -4.2471371 -4.2588196 -4.2733588][-4.241468 -4.2436848 -4.2442503 -4.2443848 -4.2430458 -4.2408266 -4.23863 -4.2381511 -4.2374463 -4.2341719 -4.2381725 -4.2513981 -4.264328 -4.27396 -4.2856231][-4.2588978 -4.2575464 -4.2563167 -4.255785 -4.2554622 -4.2544637 -4.2539649 -4.2554135 -4.2568541 -4.2559991 -4.2596426 -4.2694244 -4.2791467 -4.2873392 -4.2970085]]...]
INFO - root - 2017-12-05 13:27:13.869233: step 12610, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 77h:15m:53s remains)
INFO - root - 2017-12-05 13:27:22.309202: step 12620, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 74h:55m:04s remains)
INFO - root - 2017-12-05 13:27:30.956142: step 12630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 75h:17m:00s remains)
INFO - root - 2017-12-05 13:27:39.532230: step 12640, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 76h:08m:40s remains)
INFO - root - 2017-12-05 13:27:48.155065: step 12650, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 76h:10m:17s remains)
INFO - root - 2017-12-05 13:27:56.812499: step 12660, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.840 sec/batch; 74h:35m:50s remains)
INFO - root - 2017-12-05 13:28:05.399627: step 12670, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 75h:30m:08s remains)
INFO - root - 2017-12-05 13:28:14.092560: step 12680, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 76h:41m:41s remains)
INFO - root - 2017-12-05 13:28:22.487421: step 12690, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 76h:51m:32s remains)
INFO - root - 2017-12-05 13:28:30.994365: step 12700, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 75h:47m:42s remains)
2017-12-05 13:28:31.781419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1963506 -4.2076774 -4.2206774 -4.2270465 -4.2284627 -4.2319722 -4.2380033 -4.2488174 -4.2629638 -4.2784348 -4.2877493 -4.2824802 -4.2611585 -4.2320309 -4.2062016][-4.2350516 -4.2473836 -4.258357 -4.2624397 -4.26425 -4.2692866 -4.2765307 -4.2899842 -4.3031039 -4.3132257 -4.3163309 -4.30344 -4.2718616 -4.2356224 -4.2110381][-4.2515354 -4.2614341 -4.2661734 -4.2683873 -4.27073 -4.2756648 -4.2850852 -4.2975445 -4.3040733 -4.3088317 -4.3105974 -4.2991648 -4.2699327 -4.2401361 -4.2227287][-4.2560635 -4.2618804 -4.2621937 -4.2613487 -4.2602854 -4.26279 -4.2712359 -4.2787724 -4.2786717 -4.2825527 -4.287365 -4.281354 -4.2604346 -4.2468686 -4.242702][-4.2619324 -4.2615414 -4.2586813 -4.2526588 -4.2428656 -4.2380214 -4.23839 -4.2349143 -4.2265716 -4.2312236 -4.2365537 -4.2332506 -4.2243028 -4.2260709 -4.2346416][-4.258255 -4.2500815 -4.2436872 -4.2323375 -4.2117615 -4.1942568 -4.1794848 -4.1584082 -4.1418719 -4.1544075 -4.1683555 -4.1709661 -4.176827 -4.1884494 -4.1970286][-4.2354236 -4.2178817 -4.206357 -4.1843863 -4.1538453 -4.1281018 -4.098124 -4.0444818 -4.0086579 -4.0342994 -4.0646424 -4.0797186 -4.0999413 -4.1218982 -4.1272092][-4.20161 -4.1751304 -4.1525226 -4.1125956 -4.0705271 -4.0358505 -3.98231 -3.8810515 -3.8123097 -3.8731642 -3.9492445 -3.9946973 -4.0368023 -4.0701556 -4.073379][-4.1591244 -4.1200843 -4.0793071 -4.0229774 -3.9704034 -3.9251854 -3.8603978 -3.7502956 -3.6903961 -3.8096733 -3.9322431 -3.9987221 -4.0498009 -4.079463 -4.0747175][-4.1223269 -4.0753355 -4.0292482 -3.977726 -3.9362864 -3.8992457 -3.8682828 -3.8342834 -3.8382397 -3.937712 -4.0268354 -4.075491 -4.1174603 -4.1334686 -4.1213579][-4.1134453 -4.0749874 -4.0452118 -4.0196481 -4.0019493 -3.9853365 -3.9856293 -4.0011263 -4.0303416 -4.0895638 -4.1382947 -4.164885 -4.1945844 -4.2038651 -4.1952496][-4.1335554 -4.1214104 -4.1189609 -4.1198058 -4.1167212 -4.1113977 -4.1179447 -4.1339865 -4.1546416 -4.1854706 -4.2121696 -4.2260714 -4.2431412 -4.2521405 -4.2507691][-4.1428509 -4.1545167 -4.1702523 -4.1856122 -4.1901417 -4.1901169 -4.19671 -4.2054372 -4.2131319 -4.2234182 -4.2342014 -4.2409749 -4.249279 -4.256403 -4.2601361][-4.1253333 -4.1507177 -4.1793871 -4.2065139 -4.2165513 -4.2178612 -4.2202621 -4.2210031 -4.2200661 -4.2178078 -4.2175608 -4.2202339 -4.2249546 -4.2312312 -4.2396255][-4.0744452 -4.1057067 -4.1405797 -4.1713271 -4.1811829 -4.1818991 -4.1819239 -4.1773877 -4.1711178 -4.1638317 -4.1589665 -4.15773 -4.1571074 -4.1599975 -4.1699319]]...]
INFO - root - 2017-12-05 13:28:40.197040: step 12710, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 76h:15m:13s remains)
INFO - root - 2017-12-05 13:28:48.773186: step 12720, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 75h:59m:27s remains)
INFO - root - 2017-12-05 13:28:57.225481: step 12730, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 74h:50m:06s remains)
INFO - root - 2017-12-05 13:29:05.804168: step 12740, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 78h:07m:01s remains)
INFO - root - 2017-12-05 13:29:14.428896: step 12750, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 76h:01m:22s remains)
INFO - root - 2017-12-05 13:29:22.936966: step 12760, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.816 sec/batch; 72h:26m:26s remains)
INFO - root - 2017-12-05 13:29:31.425471: step 12770, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 74h:17m:53s remains)
INFO - root - 2017-12-05 13:29:39.851818: step 12780, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 75h:50m:29s remains)
INFO - root - 2017-12-05 13:29:48.425974: step 12790, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 75h:10m:57s remains)
INFO - root - 2017-12-05 13:29:56.899024: step 12800, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.894 sec/batch; 79h:21m:22s remains)
2017-12-05 13:29:57.653643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2553868 -4.2586641 -4.2567506 -4.2536292 -4.25067 -4.2470512 -4.244338 -4.2456055 -4.2483368 -4.2489362 -4.2487488 -4.2485442 -4.2493844 -4.2502422 -4.2486515][-4.2416334 -4.2452316 -4.2392287 -4.2310877 -4.2248063 -4.2196569 -4.2162142 -4.2194138 -4.225328 -4.2278719 -4.2282925 -4.227767 -4.2282352 -4.2294397 -4.2287383][-4.234302 -4.2359796 -4.2239785 -4.2052 -4.1898627 -4.1808996 -4.1762204 -4.1820607 -4.1962204 -4.2079139 -4.2160368 -4.2210412 -4.2244129 -4.2261429 -4.2254729][-4.2254434 -4.222558 -4.20207 -4.1706629 -4.1469431 -4.134613 -4.1275105 -4.135541 -4.1614971 -4.1878185 -4.2065015 -4.2195325 -4.2261686 -4.2284155 -4.22901][-4.2089968 -4.20307 -4.1754904 -4.1352582 -4.1056156 -4.086472 -4.0673327 -4.071445 -4.1087976 -4.150691 -4.1806822 -4.2037649 -4.2173843 -4.2235169 -4.2284346][-4.1939578 -4.1849275 -4.1542139 -4.1087055 -4.0739026 -4.041996 -4.001544 -3.9954898 -4.0466609 -4.1078687 -4.1523309 -4.188693 -4.2105145 -4.22096 -4.2312946][-4.1836624 -4.1714725 -4.1403522 -4.09178 -4.0515113 -4.006824 -3.9402053 -3.9184444 -3.9892774 -4.0765972 -4.1376863 -4.18275 -4.2073541 -4.2212644 -4.235465][-4.1587958 -4.1414614 -4.1085811 -4.0585122 -4.0128207 -3.9524655 -3.8491285 -3.8005333 -3.8996668 -4.0217819 -4.103498 -4.1593323 -4.188899 -4.2089353 -4.2278585][-4.1464605 -4.1230564 -4.0903769 -4.0419493 -3.9938929 -3.9244075 -3.7933483 -3.7098455 -3.8266706 -3.9690733 -4.0619373 -4.1262918 -4.1602325 -4.1858253 -4.2139359][-4.1519122 -4.1264639 -4.0979528 -4.0585065 -4.0204468 -3.9665666 -3.8553367 -3.7698557 -3.8559349 -3.9720287 -4.0447483 -4.0999174 -4.1309915 -4.1604171 -4.1970878][-4.1606903 -4.1363091 -4.1154461 -4.0889611 -4.0674167 -4.0355992 -3.9550278 -3.885443 -3.9430068 -4.0253654 -4.0680933 -4.1017294 -4.1252227 -4.1532617 -4.1902018][-4.1725726 -4.1531487 -4.141685 -4.1287537 -4.1191373 -4.0988669 -4.0367045 -3.9752736 -4.0151143 -4.0793076 -4.1059265 -4.1246295 -4.1385303 -4.1620345 -4.1924357][-4.1897259 -4.1776223 -4.1732154 -4.1707368 -4.1672454 -4.15212 -4.1020079 -4.0456123 -4.0689712 -4.120203 -4.1413302 -4.1510339 -4.159071 -4.1777015 -4.2004452][-4.2173257 -4.2111139 -4.2094049 -4.2091341 -4.2067084 -4.1938558 -4.1533217 -4.1069098 -4.1207619 -4.1648407 -4.1859941 -4.1929607 -4.196784 -4.2094297 -4.2236986][-4.2443075 -4.2432532 -4.243784 -4.2438483 -4.2411261 -4.2299585 -4.20079 -4.1672072 -4.1746559 -4.2094665 -4.2293959 -4.2338271 -4.232841 -4.2403831 -4.2476773]]...]
INFO - root - 2017-12-05 13:30:06.081040: step 12810, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 74h:52m:56s remains)
INFO - root - 2017-12-05 13:30:14.627389: step 12820, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 75h:49m:22s remains)
INFO - root - 2017-12-05 13:30:23.022712: step 12830, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 73h:19m:41s remains)
INFO - root - 2017-12-05 13:30:31.441690: step 12840, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 76h:26m:10s remains)
INFO - root - 2017-12-05 13:30:39.894335: step 12850, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 75h:33m:52s remains)
INFO - root - 2017-12-05 13:30:48.332322: step 12860, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 74h:52m:55s remains)
INFO - root - 2017-12-05 13:30:56.854693: step 12870, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 76h:53m:28s remains)
INFO - root - 2017-12-05 13:31:05.414598: step 12880, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 75h:44m:29s remains)
INFO - root - 2017-12-05 13:31:14.037648: step 12890, loss = 2.03, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 78h:04m:41s remains)
INFO - root - 2017-12-05 13:31:22.506532: step 12900, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 76h:01m:34s remains)
2017-12-05 13:31:23.232337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2159715 -4.2190886 -4.2259493 -4.2303424 -4.2285094 -4.2212276 -4.2082005 -4.191865 -4.1829329 -4.186625 -4.2004609 -4.21711 -4.2283144 -4.2359324 -4.2358322][-4.2126927 -4.2071376 -4.2056007 -4.2047243 -4.19711 -4.1853795 -4.1687927 -4.1517954 -4.1506214 -4.1632061 -4.181767 -4.2024221 -4.2133293 -4.2197938 -4.2240152][-4.1990538 -4.1931353 -4.1896987 -4.1847153 -4.1707239 -4.148519 -4.1250629 -4.1096 -4.1169424 -4.1379886 -4.1612916 -4.186204 -4.1983781 -4.2062135 -4.2108321][-4.1933589 -4.193759 -4.1894908 -4.1783147 -4.1495504 -4.1058903 -4.0694623 -4.0570345 -4.0761342 -4.1115217 -4.1455617 -4.175951 -4.19164 -4.1990438 -4.2007375][-4.185813 -4.1943846 -4.1941381 -4.1818619 -4.1382155 -4.0690131 -4.0104866 -3.9948914 -4.0297618 -4.0854287 -4.1318841 -4.1680884 -4.1857891 -4.1928358 -4.1912713][-4.1634269 -4.1748614 -4.1812725 -4.175735 -4.127635 -4.0342426 -3.9391296 -3.9102805 -3.9736121 -4.0577865 -4.117137 -4.1655192 -4.19076 -4.1977048 -4.1927166][-4.1421714 -4.157959 -4.1677747 -4.1633315 -4.1098914 -3.9902275 -3.8478756 -3.8117442 -3.9247875 -4.0468988 -4.1238842 -4.1819773 -4.2137985 -4.2192507 -4.2089024][-4.1209712 -4.1412435 -4.1517277 -4.1477289 -4.0927839 -3.963305 -3.8076644 -3.7970471 -3.9422197 -4.0714974 -4.1466227 -4.2007675 -4.2341022 -4.2409568 -4.2269149][-4.1072941 -4.1291046 -4.1452856 -4.1489234 -4.1060748 -4.0027008 -3.8950241 -3.9061182 -4.0190415 -4.1148667 -4.1737404 -4.2205086 -4.2504673 -4.2567668 -4.2428446][-4.1023636 -4.1207085 -4.1433911 -4.1548824 -4.1253495 -4.0572095 -4.0008745 -4.0197515 -4.0908737 -4.1566677 -4.2011909 -4.2379918 -4.259789 -4.2617974 -4.2496796][-4.1039205 -4.1153445 -4.1346207 -4.1443348 -4.1283245 -4.092092 -4.0653768 -4.08311 -4.1309037 -4.1812496 -4.2173681 -4.2436595 -4.2579937 -4.2564096 -4.2459106][-4.1143465 -4.1185465 -4.1301594 -4.137114 -4.1304092 -4.109386 -4.09489 -4.1121874 -4.1476579 -4.1869645 -4.2186651 -4.2385011 -4.24899 -4.2457089 -4.238441][-4.1339169 -4.1358709 -4.1380796 -4.1401267 -4.1279068 -4.101275 -4.092773 -4.1176205 -4.1497936 -4.1821141 -4.2087355 -4.2265425 -4.23447 -4.2314005 -4.2278566][-4.15151 -4.1534872 -4.1488242 -4.1448789 -4.1219916 -4.0876532 -4.08239 -4.1132021 -4.1466713 -4.176013 -4.202116 -4.2203751 -4.22725 -4.2255082 -4.2247262][-4.1672697 -4.1727586 -4.165844 -4.1576896 -4.1325183 -4.0976782 -4.088861 -4.1148133 -4.1439624 -4.1775064 -4.2045269 -4.2205648 -4.2298341 -4.2296724 -4.2288866]]...]
INFO - root - 2017-12-05 13:31:31.643202: step 12910, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 76h:23m:51s remains)
INFO - root - 2017-12-05 13:31:40.246647: step 12920, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 78h:59m:58s remains)
INFO - root - 2017-12-05 13:31:48.844861: step 12930, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 73h:52m:02s remains)
INFO - root - 2017-12-05 13:31:57.339669: step 12940, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.822 sec/batch; 72h:58m:25s remains)
INFO - root - 2017-12-05 13:32:05.888314: step 12950, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 76h:58m:03s remains)
INFO - root - 2017-12-05 13:32:14.503204: step 12960, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 76h:53m:39s remains)
INFO - root - 2017-12-05 13:32:23.072417: step 12970, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 72h:58m:02s remains)
INFO - root - 2017-12-05 13:32:31.614637: step 12980, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 76h:55m:30s remains)
INFO - root - 2017-12-05 13:32:40.241237: step 12990, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.824 sec/batch; 73h:05m:54s remains)
INFO - root - 2017-12-05 13:32:48.864730: step 13000, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 77h:04m:11s remains)
2017-12-05 13:32:49.597514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3128524 -4.3085647 -4.3015022 -4.2962828 -4.2939572 -4.2915254 -4.2880244 -4.2856259 -4.2855554 -4.2878213 -4.290153 -4.2926779 -4.2963634 -4.3010063 -4.3043838][-4.3093171 -4.3037705 -4.29637 -4.2907858 -4.2871814 -4.2830529 -4.2780542 -4.2739735 -4.2726235 -4.2750516 -4.279213 -4.28443 -4.2913523 -4.2991252 -4.3050075][-4.304143 -4.2970438 -4.2891107 -4.2823472 -4.2753716 -4.2671275 -4.258399 -4.2505927 -4.246541 -4.2492881 -4.2573175 -4.2670922 -4.2786121 -4.2906909 -4.3008246][-4.3006015 -4.2915378 -4.2806225 -4.2693982 -4.2546682 -4.2365217 -4.2184896 -4.2025747 -4.1935649 -4.1968942 -4.2112494 -4.2299333 -4.2505383 -4.2710114 -4.2887192][-4.2937131 -4.2838783 -4.2686319 -4.2485752 -4.2183461 -4.1800442 -4.1444612 -4.1167741 -4.1031957 -4.1103125 -4.1382093 -4.1742659 -4.2103968 -4.2428303 -4.2700629][-4.2725034 -4.2626653 -4.2436557 -4.2137494 -4.165164 -4.1032505 -4.0473986 -4.0106153 -3.9991102 -4.0160818 -4.0640373 -4.1226592 -4.1758103 -4.2198348 -4.2543559][-4.2398071 -4.2358665 -4.2205768 -4.1883731 -4.1303487 -4.0554571 -3.988013 -3.9478064 -3.9426234 -3.97257 -4.0406113 -4.1151743 -4.17508 -4.2201552 -4.2526855][-4.1991248 -4.2079215 -4.2097573 -4.19285 -4.1479964 -4.0860391 -4.0283227 -3.9945989 -3.9921877 -4.0208921 -4.0849247 -4.1539822 -4.2068162 -4.242835 -4.2646713][-4.1493907 -4.1729369 -4.1990519 -4.2111645 -4.1963124 -4.1625686 -4.1250181 -4.1006494 -4.0977588 -4.116941 -4.1633582 -4.2141004 -4.251821 -4.2744036 -4.2833757][-4.1018267 -4.1367526 -4.182117 -4.2202706 -4.2369952 -4.2336135 -4.2186861 -4.2043371 -4.2016759 -4.2118373 -4.2397761 -4.2705555 -4.2910223 -4.3000932 -4.2991018][-4.0806789 -4.1174507 -4.1682591 -4.2196078 -4.2575164 -4.278089 -4.2838974 -4.2799945 -4.2775779 -4.2807627 -4.2932916 -4.3070383 -4.3131275 -4.3113956 -4.3036728][-4.0714765 -4.1026835 -4.1518946 -4.2073574 -4.2573662 -4.2938452 -4.3164344 -4.3224578 -4.3186436 -4.3138943 -4.3128772 -4.3138261 -4.3106837 -4.3041282 -4.2950273][-4.0604019 -4.0837531 -4.13027 -4.1875677 -4.2426124 -4.2869697 -4.3157468 -4.3241153 -4.3137088 -4.2989421 -4.2887626 -4.2836356 -4.2793579 -4.2761736 -4.2722678][-4.0529728 -4.0680552 -4.110064 -4.1675811 -4.2254438 -4.2729325 -4.2992268 -4.2987537 -4.2738304 -4.2459974 -4.2299991 -4.2261343 -4.2278013 -4.2334452 -4.2368479][-4.0670285 -4.0782394 -4.1142673 -4.1663771 -4.2219453 -4.2668748 -4.2824388 -4.2633567 -4.2174082 -4.1729937 -4.1524839 -4.1535406 -4.1656713 -4.1827059 -4.1927772]]...]
INFO - root - 2017-12-05 13:32:58.125666: step 13010, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 76h:16m:10s remains)
INFO - root - 2017-12-05 13:33:06.568375: step 13020, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 73h:54m:11s remains)
INFO - root - 2017-12-05 13:33:15.112884: step 13030, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 78h:36m:37s remains)
INFO - root - 2017-12-05 13:33:23.660935: step 13040, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 76h:38m:30s remains)
INFO - root - 2017-12-05 13:33:32.288324: step 13050, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.886 sec/batch; 78h:38m:01s remains)
INFO - root - 2017-12-05 13:33:40.884260: step 13060, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 73h:52m:24s remains)
INFO - root - 2017-12-05 13:33:49.433367: step 13070, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 76h:02m:22s remains)
INFO - root - 2017-12-05 13:33:57.953756: step 13080, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 74h:39m:19s remains)
INFO - root - 2017-12-05 13:34:06.534278: step 13090, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 75h:37m:19s remains)
INFO - root - 2017-12-05 13:34:15.115964: step 13100, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 75h:40m:43s remains)
2017-12-05 13:34:15.915797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3003407 -4.283164 -4.2714677 -4.2672539 -4.2684107 -4.2678218 -4.26667 -4.2689986 -4.2729993 -4.277761 -4.286613 -4.2955308 -4.3054657 -4.3143034 -4.3164949][-4.2690582 -4.2413316 -4.2191572 -4.2078419 -4.2044468 -4.1992159 -4.1963515 -4.2065096 -4.2198319 -4.2274785 -4.2378635 -4.2490644 -4.2569971 -4.2641716 -4.266458][-4.230207 -4.188066 -4.1504741 -4.1272206 -4.1146889 -4.102354 -4.0972962 -4.1125612 -4.1352143 -4.1429195 -4.1538315 -4.1680546 -4.1753149 -4.1828127 -4.1930285][-4.1896286 -4.130249 -4.0758195 -4.0361018 -4.0081859 -3.9866323 -3.9787345 -3.9936092 -4.0214987 -4.0266752 -4.0375223 -4.0588489 -4.0715065 -4.0875077 -4.1138248][-4.1589332 -4.0844932 -4.0129318 -3.9569933 -3.9144297 -3.8814216 -3.864481 -3.8770559 -3.9047582 -3.9055061 -3.9239101 -3.9661961 -3.9932404 -4.0160818 -4.0542016][-4.139204 -4.0505047 -3.9621553 -3.8889244 -3.825578 -3.7661197 -3.7296913 -3.7485752 -3.7919183 -3.7989054 -3.83683 -3.9050725 -3.952522 -3.9841657 -4.0236459][-4.1284175 -4.023912 -3.9192245 -3.8337517 -3.7523661 -3.6705041 -3.6290457 -3.6782646 -3.7611058 -3.7872839 -3.835252 -3.9104865 -3.9682441 -4.0010662 -4.02705][-4.1227541 -4.0136833 -3.9111116 -3.8388398 -3.7792666 -3.7223856 -3.7039626 -3.7706251 -3.8657343 -3.8999279 -3.9412577 -3.9999464 -4.0430212 -4.0647063 -4.0751286][-4.1230621 -4.0279746 -3.9478869 -3.9051085 -3.8804064 -3.8574929 -3.8538127 -3.9022162 -3.9743702 -4.0056973 -4.0393291 -4.0850911 -4.119545 -4.1375608 -4.1434379][-4.1427827 -4.0673518 -4.0081654 -3.982492 -3.9741046 -3.9680245 -3.9685898 -4.002243 -4.0512652 -4.0770655 -4.1066947 -4.1430073 -4.1734295 -4.1891375 -4.1950474][-4.1827073 -4.1287994 -4.0855746 -4.0640287 -4.0580406 -4.0576291 -4.0585718 -4.0818481 -4.1185894 -4.1419678 -4.1642289 -4.1905384 -4.2142386 -4.22732 -4.2306662][-4.2195787 -4.1826463 -4.1533027 -4.1339455 -4.1270437 -4.1291957 -4.134593 -4.15829 -4.19296 -4.2170434 -4.2323 -4.245266 -4.2582235 -4.2624397 -4.2615185][-4.2460132 -4.2176695 -4.197196 -4.1837306 -4.181828 -4.1912589 -4.206058 -4.2330136 -4.26244 -4.2810411 -4.286902 -4.2880516 -4.2899995 -4.28763 -4.2856088][-4.2710409 -4.248064 -4.2324839 -4.2232742 -4.2252569 -4.239212 -4.2588139 -4.2838178 -4.3045759 -4.3155417 -4.3163047 -4.3128142 -4.3114214 -4.3082848 -4.3076668][-4.3001027 -4.2823524 -4.2699885 -4.2634811 -4.2646542 -4.2752781 -4.2911758 -4.3083324 -4.3212261 -4.3275609 -4.3280716 -4.3257475 -4.3246098 -4.3226161 -4.3219557]]...]
INFO - root - 2017-12-05 13:34:24.350632: step 13110, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 74h:07m:06s remains)
INFO - root - 2017-12-05 13:34:32.956863: step 13120, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 77h:03m:07s remains)
INFO - root - 2017-12-05 13:34:41.481225: step 13130, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 78h:39m:57s remains)
INFO - root - 2017-12-05 13:34:49.971897: step 13140, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 77h:51m:40s remains)
INFO - root - 2017-12-05 13:34:58.407206: step 13150, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 74h:59m:58s remains)
INFO - root - 2017-12-05 13:35:06.978243: step 13160, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 76h:22m:58s remains)
INFO - root - 2017-12-05 13:35:15.463819: step 13170, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 77h:02m:19s remains)
INFO - root - 2017-12-05 13:35:24.047809: step 13180, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 77h:53m:12s remains)
INFO - root - 2017-12-05 13:35:32.582859: step 13190, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.881 sec/batch; 78h:09m:49s remains)
INFO - root - 2017-12-05 13:35:41.166380: step 13200, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 75h:19m:02s remains)
2017-12-05 13:35:41.868798: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2422962 -4.2364469 -4.2324595 -4.2304649 -4.2349472 -4.245625 -4.2569752 -4.256813 -4.2527065 -4.2545934 -4.2590289 -4.2593203 -4.2593279 -4.2607589 -4.2674742][-4.2252479 -4.215354 -4.2040191 -4.196516 -4.2017817 -4.2180438 -4.2354283 -4.2331076 -4.22881 -4.2346449 -4.2430768 -4.2406645 -4.2353506 -4.233099 -4.2406507][-4.2053151 -4.1936727 -4.1745329 -4.159019 -4.15986 -4.1784563 -4.2047734 -4.200192 -4.193862 -4.2048335 -4.2181664 -4.2118807 -4.1992345 -4.1945682 -4.201417][-4.1900425 -4.1761675 -4.1502442 -4.1268091 -4.1170506 -4.1325035 -4.1664753 -4.1612916 -4.1583714 -4.1783695 -4.1997671 -4.1886716 -4.1627946 -4.1537085 -4.159009][-4.1726785 -4.1547837 -4.1246052 -4.090435 -4.0651789 -4.0745735 -4.116466 -4.1165318 -4.1137471 -4.1428938 -4.1750851 -4.1654196 -4.1290565 -4.1165328 -4.119173][-4.1583781 -4.1326575 -4.0927262 -4.0451512 -4.0026546 -4.0017118 -4.0456333 -4.0578532 -4.056221 -4.0870714 -4.1275005 -4.1165748 -4.070282 -4.0621576 -4.0733404][-4.1503782 -4.1183209 -4.0684495 -4.0113349 -3.9538157 -3.9329555 -3.9687247 -3.9913418 -3.9966574 -4.0221305 -4.0678992 -4.0589428 -4.0040584 -4.0019369 -4.0293813][-4.1475897 -4.1161323 -4.0669203 -4.0109234 -3.9491074 -3.9167218 -3.9346836 -3.9562793 -3.9677913 -3.9793258 -4.0223336 -4.0108833 -3.9494543 -3.9582591 -4.0018821][-4.1519737 -4.1269779 -4.0923829 -4.0520544 -4.0052772 -3.9765146 -3.981452 -3.9902427 -3.993758 -3.992826 -4.0174985 -3.9967849 -3.9404047 -3.9629691 -4.017488][-4.164403 -4.1483579 -4.1316648 -4.1113243 -4.0850391 -4.0575356 -4.0489559 -4.0433154 -4.0389986 -4.0342631 -4.0455494 -4.0258369 -3.9895349 -4.0176826 -4.0693054][-4.1804452 -4.1707191 -4.1590853 -4.1466827 -4.1323457 -4.1035028 -4.08327 -4.0720358 -4.0679684 -4.0682774 -4.0805764 -4.06653 -4.0416813 -4.0653291 -4.10559][-4.1907392 -4.1810865 -4.1675463 -4.1555824 -4.1491265 -4.1198912 -4.0934753 -4.0845585 -4.0883446 -4.0975571 -4.1144881 -4.10382 -4.0772762 -4.0851474 -4.1110172][-4.1979785 -4.1840343 -4.1705403 -4.1598725 -4.1583982 -4.1338668 -4.1142588 -4.1190958 -4.1297708 -4.1406474 -4.1564813 -4.1450925 -4.1163383 -4.11051 -4.1248231][-4.2130623 -4.2008843 -4.1903238 -4.1830063 -4.1833706 -4.1674819 -4.1614041 -4.1752572 -4.1862779 -4.1970906 -4.209671 -4.20062 -4.1766453 -4.1667533 -4.1746168][-4.2357059 -4.2311153 -4.2276931 -4.2263169 -4.2267103 -4.2171359 -4.2173877 -4.2310972 -4.2403555 -4.2485232 -4.2589555 -4.2564449 -4.2393918 -4.2282157 -4.2297826]]...]
INFO - root - 2017-12-05 13:35:50.447839: step 13210, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 76h:15m:06s remains)
INFO - root - 2017-12-05 13:35:59.003758: step 13220, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 75h:21m:59s remains)
INFO - root - 2017-12-05 13:36:07.614383: step 13230, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.829 sec/batch; 73h:33m:00s remains)
INFO - root - 2017-12-05 13:36:16.045925: step 13240, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 74h:35m:17s remains)
INFO - root - 2017-12-05 13:36:24.660716: step 13250, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 81h:18m:18s remains)
INFO - root - 2017-12-05 13:36:33.072906: step 13260, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 75h:54m:28s remains)
INFO - root - 2017-12-05 13:36:41.680809: step 13270, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 77h:38m:38s remains)
INFO - root - 2017-12-05 13:36:50.205881: step 13280, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 76h:52m:22s remains)
INFO - root - 2017-12-05 13:36:58.765555: step 13290, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 75h:47m:40s remains)
INFO - root - 2017-12-05 13:37:07.287279: step 13300, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 77h:54m:24s remains)
2017-12-05 13:37:08.062820: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1621947 -4.1675434 -4.16601 -4.1533408 -4.1307874 -4.1120672 -4.0930691 -4.0644412 -4.0270038 -3.9969673 -4.0012879 -4.0272503 -4.0504804 -4.0557632 -4.0378561][-4.1857204 -4.1955595 -4.1932507 -4.17866 -4.1558542 -4.1422286 -4.131269 -4.1079822 -4.0765948 -4.0496259 -4.0481582 -4.0574861 -4.0693951 -4.0714312 -4.063828][-4.2094293 -4.2198324 -4.2134228 -4.1914535 -4.1632266 -4.15158 -4.1467886 -4.1244636 -4.0950704 -4.073246 -4.0657797 -4.06298 -4.0635076 -4.0611529 -4.0667043][-4.2193022 -4.2260208 -4.2128687 -4.1801028 -4.1430831 -4.1295671 -4.1222053 -4.0966773 -4.0671263 -4.0491133 -4.0395365 -4.041563 -4.0487127 -4.0502014 -4.0639095][-4.2261825 -4.2246108 -4.202281 -4.1583848 -4.1139879 -4.0952091 -4.0838704 -4.0511007 -4.0191374 -4.0025349 -3.9948523 -4.0149226 -4.039876 -4.0503654 -4.06646][-4.2005816 -4.1890349 -4.1602864 -4.113308 -4.0633936 -4.0450406 -4.0406828 -4.0149045 -3.98287 -3.9596288 -3.949702 -3.9840598 -4.0264587 -4.0401568 -4.0578671][-4.1420264 -4.126925 -4.099997 -4.054121 -4.0084081 -4.0073433 -4.0215816 -4.005352 -3.9729645 -3.9397671 -3.922838 -3.9596066 -4.0063977 -4.0190268 -4.0382681][-4.0865779 -4.0753946 -4.0535078 -4.0125146 -3.9803724 -3.9958503 -4.0184979 -4.0074244 -3.9837091 -3.95357 -3.9393053 -3.9672151 -3.998318 -4.0072203 -4.0295644][-4.0602283 -4.0545449 -4.0371432 -4.0028486 -3.9898047 -4.0186095 -4.0430913 -4.041779 -4.0328236 -4.0126595 -4.0008941 -4.0119457 -4.0171685 -4.01332 -4.0288558][-4.0552692 -4.0525012 -4.0392461 -4.0150237 -4.0178123 -4.0536728 -4.0793481 -4.0837989 -4.0908537 -4.0873919 -4.0794005 -4.0757413 -4.0593767 -4.036551 -4.0385833][-4.0638084 -4.0654 -4.0529571 -4.0331 -4.0451736 -4.0774269 -4.0937538 -4.0948811 -4.1074252 -4.1221247 -4.1229234 -4.1140265 -4.0887909 -4.0576472 -4.0507374][-4.0945077 -4.0917072 -4.0720325 -4.0498238 -4.0587492 -4.0775094 -4.0790591 -4.0718603 -4.0861592 -4.112185 -4.12088 -4.1112843 -4.0892634 -4.0660381 -4.058774][-4.1263666 -4.1181521 -4.0962343 -4.0743332 -4.07341 -4.0770121 -4.0706906 -4.0574441 -4.0702291 -4.0955029 -4.0992746 -4.085887 -4.0713282 -4.0615635 -4.0614343][-4.1534662 -4.1422997 -4.1227684 -4.102457 -4.0894709 -4.0800314 -4.0700865 -4.0605755 -4.0738115 -4.0929003 -4.0893588 -4.0760169 -4.0678806 -4.0656796 -4.0696592][-4.17308 -4.1604071 -4.1462388 -4.1306486 -4.1120634 -4.0967836 -4.0884457 -4.0836058 -4.0947251 -4.1075873 -4.1023993 -4.09066 -4.0849571 -4.0832253 -4.0858431]]...]
INFO - root - 2017-12-05 13:37:16.515797: step 13310, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 73h:50m:52s remains)
INFO - root - 2017-12-05 13:37:25.133922: step 13320, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 77h:32m:39s remains)
INFO - root - 2017-12-05 13:37:33.715989: step 13330, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.881 sec/batch; 78h:07m:09s remains)
INFO - root - 2017-12-05 13:37:42.298790: step 13340, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 76h:22m:20s remains)
INFO - root - 2017-12-05 13:37:50.837243: step 13350, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 78h:09m:04s remains)
INFO - root - 2017-12-05 13:37:59.351778: step 13360, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 75h:06m:56s remains)
INFO - root - 2017-12-05 13:38:07.752497: step 13370, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 74h:28m:33s remains)
INFO - root - 2017-12-05 13:38:16.364884: step 13380, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 74h:06m:14s remains)
INFO - root - 2017-12-05 13:38:24.889504: step 13390, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 74h:58m:03s remains)
INFO - root - 2017-12-05 13:38:33.412838: step 13400, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 76h:56m:51s remains)
2017-12-05 13:38:34.217514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3286672 -4.3152246 -4.2946472 -4.2628751 -4.2335215 -4.2191935 -4.2125382 -4.19687 -4.1675878 -4.1315079 -4.0924368 -4.0760889 -4.1210961 -4.1962786 -4.26304][-4.3265505 -4.310174 -4.2853985 -4.2511621 -4.2225018 -4.2118182 -4.2035694 -4.1859565 -4.1558156 -4.121573 -4.0971537 -4.0993423 -4.1554918 -4.231245 -4.2917142][-4.3232307 -4.3027768 -4.2743387 -4.2404418 -4.2138095 -4.2059321 -4.1983056 -4.1787643 -4.1463933 -4.1163387 -4.1093564 -4.1307015 -4.1935468 -4.2663903 -4.3179][-4.3201804 -4.29698 -4.2665362 -4.234776 -4.21174 -4.2047729 -4.1975422 -4.1750889 -4.1388297 -4.1096253 -4.1132011 -4.1473465 -4.2130489 -4.2843409 -4.3307166][-4.3165913 -4.2936869 -4.2626295 -4.2289867 -4.2036386 -4.1893525 -4.1755257 -4.1458077 -4.1064782 -4.0849152 -4.1001158 -4.1450658 -4.2126989 -4.2851033 -4.33097][-4.3132873 -4.291203 -4.2597136 -4.2198 -4.1850872 -4.1548581 -4.1257591 -4.0826993 -4.0418067 -4.0329022 -4.0620012 -4.1219831 -4.1967568 -4.2722063 -4.3218808][-4.31126 -4.2878175 -4.254909 -4.2089329 -4.1560669 -4.0982304 -4.0428619 -3.9862185 -3.9541688 -3.9725311 -4.0286756 -4.1095133 -4.1927147 -4.2654748 -4.3147407][-4.31231 -4.285522 -4.2480965 -4.1914182 -4.1155691 -4.0269027 -3.945749 -3.8820503 -3.8752284 -3.93628 -4.0230064 -4.119278 -4.2065792 -4.2722273 -4.3157792][-4.3158789 -4.2875056 -4.245697 -4.1791997 -4.0883341 -3.9823337 -3.8889906 -3.8305442 -3.849453 -3.9409826 -4.046639 -4.144299 -4.2293544 -4.2872272 -4.3222241][-4.3205156 -4.2930546 -4.2545643 -4.1941175 -4.104301 -3.9999275 -3.9123452 -3.8628445 -3.8922596 -3.9894998 -4.0957546 -4.1868768 -4.2614536 -4.3066673 -4.3304429][-4.3240047 -4.2981753 -4.2654152 -4.21805 -4.14071 -4.051208 -3.9795337 -3.9389527 -3.9704318 -4.0590577 -4.1558046 -4.2344666 -4.2963853 -4.3278365 -4.3407331][-4.3302078 -4.30662 -4.2806368 -4.2457132 -4.1858764 -4.1153827 -4.06404 -4.0372248 -4.0651212 -4.1352105 -4.2147374 -4.2742538 -4.320828 -4.3446026 -4.3504353][-4.3394823 -4.3180375 -4.2962227 -4.2700586 -4.2232151 -4.1687365 -4.1361423 -4.1255412 -4.151 -4.2023716 -4.259737 -4.3008738 -4.3324924 -4.3506546 -4.3565083][-4.3457441 -4.3263392 -4.3053885 -4.282763 -4.2445908 -4.2039685 -4.184176 -4.1861334 -4.2105875 -4.24982 -4.2897944 -4.3184495 -4.339335 -4.3521051 -4.3586593][-4.3483915 -4.33233 -4.3139992 -4.2942405 -4.26391 -4.2320795 -4.2191448 -4.2279468 -4.2517104 -4.2829175 -4.3106208 -4.3315959 -4.3459215 -4.3538346 -4.3574152]]...]
INFO - root - 2017-12-05 13:38:42.698260: step 13410, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 74h:02m:55s remains)
INFO - root - 2017-12-05 13:38:51.235649: step 13420, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 76h:57m:45s remains)
INFO - root - 2017-12-05 13:38:59.821545: step 13430, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 76h:57m:17s remains)
INFO - root - 2017-12-05 13:39:08.375634: step 13440, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 74h:32m:18s remains)
INFO - root - 2017-12-05 13:39:16.789063: step 13450, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 73h:01m:38s remains)
INFO - root - 2017-12-05 13:39:25.271205: step 13460, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 73h:19m:57s remains)
INFO - root - 2017-12-05 13:39:33.742684: step 13470, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 76h:14m:41s remains)
INFO - root - 2017-12-05 13:39:42.262052: step 13480, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 74h:27m:16s remains)
INFO - root - 2017-12-05 13:39:50.820586: step 13490, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 75h:06m:49s remains)
INFO - root - 2017-12-05 13:39:59.365738: step 13500, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 75h:54m:05s remains)
2017-12-05 13:40:00.084649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2685676 -4.2558737 -4.2464623 -4.2409697 -4.2374229 -4.2373676 -4.239048 -4.2490745 -4.2689347 -4.2850313 -4.2937036 -4.306334 -4.3093357 -4.3023005 -4.2888455][-4.2730517 -4.2603426 -4.2531548 -4.2464919 -4.2393003 -4.2338862 -4.2264743 -4.228332 -4.2471447 -4.2700272 -4.2826228 -4.2942009 -4.297389 -4.2906036 -4.2753568][-4.2724814 -4.2653837 -4.2629724 -4.2535472 -4.2399979 -4.228857 -4.2131891 -4.2060833 -4.2207127 -4.248251 -4.2659817 -4.2786427 -4.2818427 -4.2738972 -4.2565665][-4.2699409 -4.2693906 -4.2688322 -4.2548018 -4.23929 -4.2254491 -4.2034645 -4.1849546 -4.1895514 -4.2203345 -4.2452879 -4.2633524 -4.2699056 -4.26409 -4.2465091][-4.264514 -4.2670178 -4.2630992 -4.241951 -4.2219543 -4.20535 -4.17862 -4.1517453 -4.1463251 -4.178597 -4.21738 -4.2504358 -4.2655158 -4.2658591 -4.2488651][-4.2559848 -4.2589736 -4.2537446 -4.22832 -4.1984358 -4.1694956 -4.1353574 -4.1065426 -4.0958366 -4.1265721 -4.1780353 -4.2273688 -4.2602515 -4.2713251 -4.2570915][-4.247726 -4.2485266 -4.2417626 -4.2115459 -4.170742 -4.1268396 -4.0820704 -4.0472908 -4.03157 -4.0587487 -4.1172953 -4.1829448 -4.2386155 -4.2642097 -4.2584066][-4.251358 -4.2503171 -4.2403741 -4.2048669 -4.1540475 -4.0942087 -4.0351892 -3.9882367 -3.9667478 -3.9885998 -4.0531068 -4.1348972 -4.2091727 -4.24766 -4.2520709][-4.2612271 -4.2577586 -4.24164 -4.2023125 -4.1471291 -4.080781 -4.0139403 -3.9645181 -3.944628 -3.961457 -4.0255032 -4.1111841 -4.1895452 -4.2341785 -4.249402][-4.2720366 -4.2680106 -4.2505474 -4.2115941 -4.1573086 -4.0905652 -4.0243225 -3.981684 -3.9738164 -3.9929845 -4.0503726 -4.1213427 -4.1866083 -4.2274766 -4.2459693][-4.2740407 -4.2716908 -4.2584691 -4.2251239 -4.1788988 -4.1210752 -4.0639834 -4.0303755 -4.0339508 -4.0606046 -4.1067085 -4.15269 -4.1924024 -4.2164164 -4.2255378][-4.2710867 -4.2704387 -4.2635903 -4.241785 -4.2099333 -4.1682806 -4.1289363 -4.1081066 -4.1175423 -4.1425657 -4.17302 -4.1930037 -4.1989551 -4.1946392 -4.1862383][-4.2478013 -4.2488031 -4.248385 -4.2392707 -4.2231135 -4.2015109 -4.1853504 -4.18427 -4.2007184 -4.2178035 -4.2287922 -4.2225857 -4.1989689 -4.167542 -4.1388106][-4.2094469 -4.2146034 -4.2191219 -4.2198534 -4.2149506 -4.2041941 -4.2043362 -4.2200522 -4.2456088 -4.2594337 -4.2571573 -4.23454 -4.1950293 -4.1488419 -4.1028175][-4.1734104 -4.179 -4.1800823 -4.1801639 -4.1769748 -4.1688085 -4.1767163 -4.2066545 -4.2455873 -4.2617016 -4.2535772 -4.2236509 -4.1761069 -4.1218457 -4.0681243]]...]
INFO - root - 2017-12-05 13:40:08.495705: step 13510, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 74h:49m:53s remains)
INFO - root - 2017-12-05 13:40:17.087729: step 13520, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 75h:09m:25s remains)
INFO - root - 2017-12-05 13:40:25.749959: step 13530, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 78h:16m:05s remains)
INFO - root - 2017-12-05 13:40:34.289917: step 13540, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 75h:38m:16s remains)
INFO - root - 2017-12-05 13:40:42.806060: step 13550, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 77h:05m:34s remains)
INFO - root - 2017-12-05 13:40:51.273477: step 13560, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 74h:08m:10s remains)
INFO - root - 2017-12-05 13:40:59.739482: step 13570, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.845 sec/batch; 74h:51m:39s remains)
INFO - root - 2017-12-05 13:41:08.207280: step 13580, loss = 2.04, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 73h:13m:10s remains)
INFO - root - 2017-12-05 13:41:16.726868: step 13590, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.809 sec/batch; 71h:37m:28s remains)
INFO - root - 2017-12-05 13:41:25.224724: step 13600, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 74h:32m:39s remains)
2017-12-05 13:41:25.951081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.169095 -4.1526804 -4.1545086 -4.13935 -4.1174908 -4.0946665 -4.0774369 -4.0604506 -4.0496655 -4.0247197 -3.9933419 -3.9852214 -4.0051541 -4.0371828 -4.0646076][-4.1624117 -4.1462936 -4.1445303 -4.1312613 -4.1072941 -4.0810828 -4.0664797 -4.0500922 -4.041532 -4.023694 -3.9976056 -3.9870024 -3.9952462 -4.0129724 -4.0276847][-4.1615014 -4.1465859 -4.1334834 -4.1173968 -4.092135 -4.0592308 -4.0411029 -4.0214014 -4.0166416 -4.0127673 -4.0051479 -4.002563 -4.0028391 -4.0023723 -4.004025][-4.1575317 -4.141315 -4.1195092 -4.0980725 -4.0703082 -4.0301514 -4.00086 -3.9778481 -3.9836979 -4.0048418 -4.0208025 -4.027081 -4.0218005 -4.0076675 -4.0028787][-4.1522532 -4.1402273 -4.1176524 -4.087173 -4.0543952 -4.0114636 -3.9712784 -3.9373446 -3.9515412 -4.0014477 -4.0384474 -4.0550194 -4.0474434 -4.023613 -4.0092874][-4.1498361 -4.1419187 -4.1187248 -4.08412 -4.0485415 -4.0019088 -3.9407206 -3.8826478 -3.9014449 -3.9808507 -4.0414176 -4.074904 -4.0758157 -4.054697 -4.0310259][-4.1448855 -4.1355095 -4.1154337 -4.0854692 -4.0456028 -3.9895742 -3.8997798 -3.8059964 -3.8242161 -3.9388106 -4.0308204 -4.0833144 -4.1010385 -4.0919704 -4.0690441][-4.1444678 -4.1354265 -4.1234751 -4.1031117 -4.0585618 -3.9914196 -3.8724174 -3.73135 -3.7451873 -3.8977823 -4.0152082 -4.0826273 -4.1121264 -4.1148925 -4.096067][-4.1463423 -4.1382594 -4.1320386 -4.1192055 -4.0776396 -4.0134916 -3.9014659 -3.7638006 -3.770566 -3.9059443 -4.0164123 -4.0811453 -4.1120224 -4.1254649 -4.1137357][-4.1460719 -4.1383152 -4.1343203 -4.1212168 -4.0877213 -4.046052 -3.9793801 -3.8975027 -3.8948705 -3.9655721 -4.037261 -4.0849319 -4.1088991 -4.1287169 -4.1270423][-4.1439652 -4.1361394 -4.1312056 -4.1151752 -4.092308 -4.0727239 -4.0444479 -4.0026827 -3.9902806 -4.0146565 -4.0540504 -4.0834723 -4.1006846 -4.1228113 -4.1286683][-4.133285 -4.1280608 -4.125505 -4.1116643 -4.096571 -4.0885611 -4.0750647 -4.0508642 -4.0334115 -4.0352812 -4.0546312 -4.072608 -4.0867705 -4.1073651 -4.1149039][-4.1045461 -4.1046891 -4.1094356 -4.098598 -4.0843911 -4.086113 -4.08755 -4.0730267 -4.0562429 -4.0551233 -4.06107 -4.0707922 -4.07865 -4.0887547 -4.09393][-4.062314 -4.0650554 -4.0775838 -4.0709496 -4.0596833 -4.0706334 -4.0856586 -4.082509 -4.0740094 -4.0762329 -4.0784774 -4.0821047 -4.0832195 -4.0819831 -4.0794435][-4.0344915 -4.0414748 -4.05647 -4.0498004 -4.039125 -4.0528393 -4.0703788 -4.0701313 -4.0672879 -4.0726686 -4.0836844 -4.0930996 -4.0967274 -4.0955067 -4.0868492]]...]
INFO - root - 2017-12-05 13:41:34.382259: step 13610, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 73h:48m:46s remains)
INFO - root - 2017-12-05 13:41:42.987041: step 13620, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 77h:39m:34s remains)
INFO - root - 2017-12-05 13:41:51.580227: step 13630, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 75h:52m:15s remains)
INFO - root - 2017-12-05 13:42:00.055941: step 13640, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 76h:27m:18s remains)
INFO - root - 2017-12-05 13:42:08.620784: step 13650, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 77h:45m:22s remains)
INFO - root - 2017-12-05 13:42:17.223267: step 13660, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 84h:26m:03s remains)
INFO - root - 2017-12-05 13:42:25.745363: step 13670, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 74h:05m:30s remains)
INFO - root - 2017-12-05 13:42:34.156189: step 13680, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.733 sec/batch; 64h:56m:19s remains)
INFO - root - 2017-12-05 13:42:42.668471: step 13690, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 75h:01m:15s remains)
INFO - root - 2017-12-05 13:42:51.147679: step 13700, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.843 sec/batch; 74h:39m:13s remains)
2017-12-05 13:42:51.928544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1410728 -4.144546 -4.1441832 -4.1586318 -4.1775188 -4.1856661 -4.1898537 -4.198997 -4.2047052 -4.1996541 -4.1939073 -4.1931024 -4.1889076 -4.1873012 -4.1896534][-4.1492276 -4.1655755 -4.1724815 -4.1881905 -4.2011528 -4.1971359 -4.1889577 -4.1901407 -4.195591 -4.1938887 -4.1859274 -4.180306 -4.17188 -4.1700568 -4.1728287][-4.1550064 -4.1817508 -4.1946273 -4.2101974 -4.2216682 -4.2160244 -4.1988397 -4.183702 -4.1809883 -4.1822267 -4.1759605 -4.168561 -4.1601672 -4.1578617 -4.1605043][-4.1536717 -4.1840696 -4.2020817 -4.21684 -4.2263765 -4.2214332 -4.1987863 -4.1709714 -4.1563959 -4.1535473 -4.1479306 -4.1427984 -4.1434736 -4.1458697 -4.1501656][-4.1469169 -4.1722584 -4.1931624 -4.2052593 -4.2067862 -4.1958756 -4.1576662 -4.1141644 -4.1016865 -4.1099095 -4.1056247 -4.10593 -4.11729 -4.1307898 -4.1450658][-4.13696 -4.1548958 -4.1745334 -4.1804304 -4.1711988 -4.140667 -4.0705228 -3.9960086 -3.9970047 -4.0462022 -4.0728269 -4.08818 -4.1056261 -4.1182733 -4.1334434][-4.119513 -4.127317 -4.14483 -4.1470461 -4.1284013 -4.0825734 -3.9852598 -3.8814392 -3.9060118 -4.0055356 -4.0691838 -4.0957241 -4.11003 -4.1121058 -4.1182175][-4.1155882 -4.1161351 -4.1289811 -4.1258407 -4.1014304 -4.0632925 -3.9928687 -3.9194465 -3.9515615 -4.0451078 -4.106174 -4.1254692 -4.1272788 -4.1192222 -4.1166053][-4.1335945 -4.1344995 -4.1434631 -4.1364737 -4.1126318 -4.0919352 -4.0688124 -4.047153 -4.0732036 -4.126091 -4.1588969 -4.1612644 -4.1477356 -4.1296844 -4.1213546][-4.1639533 -4.1690874 -4.1765518 -4.1698642 -4.151237 -4.1373682 -4.1341448 -4.1347466 -4.1524572 -4.1794119 -4.1928535 -4.1873446 -4.1677365 -4.1432295 -4.1323991][-4.1979256 -4.1977592 -4.1974435 -4.1919308 -4.1804981 -4.1699347 -4.1689606 -4.1715765 -4.1803489 -4.1915894 -4.1960578 -4.191884 -4.1769271 -4.1544738 -4.1402693][-4.215498 -4.2038803 -4.1953998 -4.1899066 -4.1823378 -4.1734767 -4.169591 -4.1699538 -4.1730647 -4.1793003 -4.1887145 -4.1959944 -4.1917691 -4.1741552 -4.1565976][-4.2142777 -4.1962681 -4.1829543 -4.1764011 -4.1711545 -4.1630707 -4.1588349 -4.1589651 -4.1595869 -4.1653357 -4.1806378 -4.1974792 -4.2015939 -4.1919942 -4.1766872][-4.2037659 -4.1847925 -4.1717367 -4.1633615 -4.1580229 -4.1551452 -4.1521621 -4.1524878 -4.1538939 -4.1601825 -4.1756654 -4.1950922 -4.2027516 -4.2005944 -4.1944909][-4.195435 -4.1806183 -4.1720972 -4.1650343 -4.1644678 -4.1646194 -4.1560965 -4.1513658 -4.1543818 -4.1599889 -4.170908 -4.1897182 -4.202878 -4.2085814 -4.2099829]]...]
INFO - root - 2017-12-05 13:43:00.353273: step 13710, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 75h:47m:42s remains)
INFO - root - 2017-12-05 13:43:08.863586: step 13720, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 77h:19m:41s remains)
INFO - root - 2017-12-05 13:43:17.366445: step 13730, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 74h:24m:01s remains)
INFO - root - 2017-12-05 13:43:25.878328: step 13740, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 75h:18m:40s remains)
INFO - root - 2017-12-05 13:43:34.463862: step 13750, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 76h:17m:34s remains)
INFO - root - 2017-12-05 13:43:43.012529: step 13760, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 76h:32m:00s remains)
INFO - root - 2017-12-05 13:43:51.498928: step 13770, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 74h:59m:48s remains)
INFO - root - 2017-12-05 13:44:00.109173: step 13780, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 76h:10m:56s remains)
INFO - root - 2017-12-05 13:44:08.568632: step 13790, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 76h:05m:06s remains)
INFO - root - 2017-12-05 13:44:17.152188: step 13800, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 75h:20m:45s remains)
2017-12-05 13:44:17.900264: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1780486 -4.1732645 -4.1895676 -4.1978607 -4.1997519 -4.195713 -4.18688 -4.1872759 -4.1992669 -4.2249746 -4.2518435 -4.2651787 -4.2653193 -4.2577548 -4.2466893][-4.1495333 -4.1386614 -4.1518908 -4.1623497 -4.1667829 -4.1590819 -4.14105 -4.1336112 -4.1474705 -4.1856704 -4.2237654 -4.2409043 -4.2437286 -4.2389703 -4.2317324][-4.1365547 -4.1186366 -4.1275878 -4.1373048 -4.143115 -4.1294112 -4.0996447 -4.0766535 -4.0911841 -4.1500697 -4.2058845 -4.228611 -4.2317 -4.2280807 -4.22287][-4.1296196 -4.1093545 -4.1123419 -4.1125736 -4.1138682 -4.092628 -4.0420351 -3.9937558 -4.01548 -4.1017957 -4.1762094 -4.2048979 -4.2098341 -4.2072148 -4.2044945][-4.1217475 -4.1007686 -4.0988793 -4.0924349 -4.08595 -4.0538716 -3.9718 -3.8843904 -3.9203002 -4.04762 -4.1436753 -4.1824117 -4.1891055 -4.1846232 -4.1829286][-4.1290455 -4.106606 -4.0985522 -4.0827246 -4.0690184 -4.02778 -3.9125848 -3.7837067 -3.8280635 -4.0018153 -4.1222906 -4.1696286 -4.1778636 -4.1708932 -4.1653166][-4.1522455 -4.1279526 -4.1091294 -4.0849886 -4.0644569 -4.021482 -3.8981991 -3.7634845 -3.7993391 -3.9808304 -4.1079445 -4.158884 -4.1683927 -4.16077 -4.148633][-4.1762166 -4.1519265 -4.123858 -4.0916543 -4.0716143 -4.0424395 -3.9522467 -3.8592639 -3.8811707 -4.0168848 -4.1154146 -4.1528916 -4.1555638 -4.1479683 -4.1344819][-4.1878223 -4.1658144 -4.1304827 -4.0888572 -4.0760059 -4.0723877 -4.029851 -3.9848747 -4.00146 -4.0825787 -4.1348734 -4.1488123 -4.143672 -4.1392365 -4.1327286][-4.2003174 -4.1809793 -4.1432567 -4.1010156 -4.0945945 -4.1082959 -4.1005588 -4.0866652 -4.1042347 -4.1433363 -4.156383 -4.1529651 -4.1396308 -4.138063 -4.1451359][-4.2194772 -4.2068424 -4.1803064 -4.1491966 -4.145596 -4.1574697 -4.1610684 -4.1551132 -4.1665192 -4.1769366 -4.1695795 -4.154222 -4.1347966 -4.1378593 -4.1652665][-4.2381015 -4.2267008 -4.2076516 -4.1914821 -4.1933641 -4.2054262 -4.2110357 -4.1986609 -4.19581 -4.1892304 -4.1717968 -4.1468925 -4.1297379 -4.1483107 -4.1927991][-4.2515755 -4.2381091 -4.2241116 -4.2208385 -4.231813 -4.2478185 -4.2559104 -4.2465763 -4.2383113 -4.2236862 -4.2028346 -4.1776204 -4.16955 -4.1983027 -4.2401137][-4.26309 -4.2486715 -4.2398791 -4.2441149 -4.2600021 -4.2779741 -4.2882423 -4.2845411 -4.2781839 -4.2647223 -4.2478266 -4.232482 -4.22931 -4.2537637 -4.2815371][-4.2784934 -4.2649612 -4.2605929 -4.2671843 -4.2836685 -4.3008294 -4.3092132 -4.3084564 -4.3050108 -4.2966461 -4.2852731 -4.2764649 -4.2752733 -4.2898293 -4.3066964]]...]
INFO - root - 2017-12-05 13:44:26.347477: step 13810, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 75h:55m:27s remains)
INFO - root - 2017-12-05 13:44:34.878295: step 13820, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 76h:05m:53s remains)
INFO - root - 2017-12-05 13:44:43.368705: step 13830, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 74h:14m:47s remains)
INFO - root - 2017-12-05 13:44:51.814385: step 13840, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 75h:11m:42s remains)
INFO - root - 2017-12-05 13:45:00.454407: step 13850, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 77h:49m:15s remains)
INFO - root - 2017-12-05 13:45:08.956511: step 13860, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 75h:02m:32s remains)
INFO - root - 2017-12-05 13:45:17.377244: step 13870, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 75h:45m:01s remains)
INFO - root - 2017-12-05 13:45:25.903628: step 13880, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 75h:45m:20s remains)
INFO - root - 2017-12-05 13:45:34.390104: step 13890, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 74h:00m:34s remains)
INFO - root - 2017-12-05 13:45:42.648544: step 13900, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 75h:00m:58s remains)
2017-12-05 13:45:43.436310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.229733 -4.23823 -4.2452736 -4.252851 -4.2697 -4.2853594 -4.2914944 -4.2883134 -4.2832284 -4.273428 -4.2636528 -4.2686067 -4.2832847 -4.2910476 -4.2885976][-4.2222462 -4.2304916 -4.238368 -4.244092 -4.2613392 -4.277009 -4.2817116 -4.2776732 -4.2741146 -4.2677817 -4.2639751 -4.2736425 -4.2906623 -4.300272 -4.2963748][-4.2219224 -4.2306194 -4.2369194 -4.2388544 -4.253921 -4.2712851 -4.27951 -4.280848 -4.2807322 -4.2760668 -4.2749186 -4.28337 -4.2964687 -4.3047023 -4.3015056][-4.2250223 -4.23086 -4.228631 -4.2200341 -4.2293839 -4.2502832 -4.2669759 -4.27727 -4.2846208 -4.2855296 -4.2875676 -4.2919931 -4.2974114 -4.3015714 -4.3014026][-4.2283831 -4.2246881 -4.204977 -4.1787729 -4.1778936 -4.1981783 -4.219717 -4.2393184 -4.2596927 -4.2743855 -4.2867103 -4.292726 -4.2930851 -4.294558 -4.2967935][-4.2304688 -4.2144284 -4.1748443 -4.1290526 -4.1092949 -4.1156869 -4.1309438 -4.1567712 -4.194665 -4.235003 -4.2700276 -4.2873006 -4.2898626 -4.2905626 -4.2920494][-4.2388411 -4.2143378 -4.1611304 -4.1001234 -4.0559497 -4.0337343 -4.0303106 -4.0519786 -4.1026654 -4.171227 -4.2346907 -4.2703681 -4.2826633 -4.2880721 -4.2871361][-4.2508264 -4.2292018 -4.177783 -4.1158648 -4.0555944 -4.0077767 -3.9801152 -3.9839337 -4.0309405 -4.109612 -4.1892376 -4.2404337 -4.2625566 -4.2738905 -4.2745061][-4.2606826 -4.252037 -4.2180667 -4.1729159 -4.1214905 -4.0706105 -4.0292473 -4.0126534 -4.0371356 -4.0977454 -4.1696677 -4.2222371 -4.2464628 -4.2593451 -4.2628832][-4.2639933 -4.2711754 -4.25838 -4.2370076 -4.210125 -4.1749783 -4.136775 -4.1089354 -4.1082034 -4.1383867 -4.1854539 -4.2233868 -4.2411842 -4.2535567 -4.26116][-4.2562413 -4.273541 -4.2753282 -4.2686472 -4.2608423 -4.2473755 -4.2248154 -4.2004218 -4.189404 -4.2006855 -4.2239037 -4.2407179 -4.2470846 -4.253355 -4.2593427][-4.2392259 -4.2555242 -4.2603426 -4.2569752 -4.2589145 -4.2633286 -4.258328 -4.2474403 -4.2421269 -4.2472181 -4.2549925 -4.2563839 -4.2547474 -4.2538266 -4.25276][-4.2228904 -4.2301984 -4.23145 -4.2280412 -4.2332778 -4.2457814 -4.251121 -4.2516932 -4.2533674 -4.2572851 -4.2596493 -4.2563319 -4.2500691 -4.2452435 -4.2399869][-4.2163582 -4.2151651 -4.2125487 -4.21013 -4.2162657 -4.2301774 -4.2371054 -4.2410059 -4.24454 -4.2483864 -4.2498856 -4.2457771 -4.2384586 -4.2322664 -4.2264261][-4.2360606 -4.2338877 -4.2304416 -4.2274246 -4.2302074 -4.2404361 -4.2453833 -4.2468963 -4.2487774 -4.252306 -4.2540975 -4.2516232 -4.2465444 -4.2420616 -4.2386994]]...]
INFO - root - 2017-12-05 13:45:51.791967: step 13910, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 74h:49m:20s remains)
INFO - root - 2017-12-05 13:46:00.341500: step 13920, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 75h:37m:39s remains)
INFO - root - 2017-12-05 13:46:08.919727: step 13930, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 74h:35m:10s remains)
INFO - root - 2017-12-05 13:46:17.412309: step 13940, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 76h:01m:09s remains)
INFO - root - 2017-12-05 13:46:25.879504: step 13950, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 75h:37m:03s remains)
INFO - root - 2017-12-05 13:46:34.434476: step 13960, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 75h:31m:16s remains)
INFO - root - 2017-12-05 13:46:43.059824: step 13970, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 76h:14m:34s remains)
INFO - root - 2017-12-05 13:46:51.590309: step 13980, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 73h:12m:29s remains)
INFO - root - 2017-12-05 13:47:00.011126: step 13990, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 75h:08m:04s remains)
INFO - root - 2017-12-05 13:47:08.447730: step 14000, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 64h:50m:28s remains)
2017-12-05 13:47:09.233652: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2309284 -4.216464 -4.2074976 -4.2097988 -4.2211986 -4.228991 -4.2420144 -4.2533727 -4.2585282 -4.2484417 -4.2159638 -4.1717296 -4.1473942 -4.1544566 -4.1767693][-4.2208776 -4.2076111 -4.201807 -4.204917 -4.2155213 -4.2218633 -4.2326884 -4.2391133 -4.2381392 -4.2189159 -4.1755209 -4.1206827 -4.0934634 -4.1081676 -4.1422405][-4.204926 -4.19248 -4.1889472 -4.1953969 -4.2071819 -4.215292 -4.2211995 -4.2206392 -4.2092557 -4.1747084 -4.1126184 -4.0509019 -4.0324378 -4.0606732 -4.1053228][-4.1948395 -4.18598 -4.1869855 -4.1993794 -4.2148647 -4.2243538 -4.2213836 -4.2042994 -4.1761661 -4.1231465 -4.0535059 -4.0071225 -4.0150352 -4.054709 -4.098115][-4.1962013 -4.1887689 -4.1908283 -4.2027011 -4.2137933 -4.2170095 -4.2025213 -4.1642895 -4.1171231 -4.0611277 -4.0148115 -4.0089469 -4.04459 -4.0876493 -4.1278234][-4.2069154 -4.1933594 -4.1879082 -4.1909046 -4.1898389 -4.1798196 -4.1425428 -4.0711312 -4.0018854 -3.9577296 -3.9600058 -4.0081677 -4.06809 -4.1175547 -4.1586924][-4.20702 -4.1877956 -4.1704621 -4.1572566 -4.1394176 -4.1053667 -4.0328069 -3.9209418 -3.8443391 -3.8411911 -3.9053028 -3.9937668 -4.0673018 -4.1180739 -4.1531329][-4.2088485 -4.1890659 -4.1620941 -4.1305447 -4.0987682 -4.0450125 -3.945308 -3.8227139 -3.7845614 -3.8415749 -3.9305205 -4.0177979 -4.0801911 -4.1191659 -4.139081][-4.2360463 -4.2202053 -4.1918168 -4.1519136 -4.1145434 -4.0573897 -3.9698853 -3.8900793 -3.8966367 -3.9652569 -4.0293717 -4.0905795 -4.133307 -4.1584167 -4.1658783][-4.2675114 -4.2562289 -4.2305465 -4.1901746 -4.1521492 -4.10617 -4.0524712 -4.0229092 -4.0489893 -4.0992723 -4.137929 -4.1745667 -4.1991668 -4.2145696 -4.21647][-4.2950706 -4.28764 -4.2675505 -4.23312 -4.1992416 -4.1664295 -4.1392879 -4.1405363 -4.1700311 -4.2052293 -4.2282562 -4.2449536 -4.2560043 -4.2665577 -4.2678251][-4.3161345 -4.3104019 -4.2954183 -4.2683129 -4.2407441 -4.2198973 -4.2108788 -4.2248549 -4.2484393 -4.2725282 -4.2852006 -4.28799 -4.2896032 -4.2954845 -4.2978487][-4.3280282 -4.3210926 -4.3081746 -4.2888155 -4.2685504 -4.256886 -4.2541566 -4.2674837 -4.2836347 -4.2986665 -4.30259 -4.2989211 -4.2973824 -4.3006606 -4.3030419][-4.3225694 -4.3133068 -4.3026786 -4.2891355 -4.2745128 -4.267436 -4.2667627 -4.2760887 -4.2862206 -4.2920208 -4.2893414 -4.2837481 -4.2821565 -4.282783 -4.2826214][-4.3095336 -4.2973285 -4.286447 -4.2753577 -4.2656569 -4.2608881 -4.2600775 -4.2636857 -4.2684402 -4.2712855 -4.2689338 -4.265892 -4.2660584 -4.2667742 -4.2665057]]...]
INFO - root - 2017-12-05 13:47:17.581228: step 14010, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.856 sec/batch; 75h:41m:41s remains)
INFO - root - 2017-12-05 13:47:26.097416: step 14020, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 75h:37m:24s remains)
INFO - root - 2017-12-05 13:47:34.654847: step 14030, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 76h:11m:54s remains)
INFO - root - 2017-12-05 13:47:43.195400: step 14040, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 72h:55m:48s remains)
INFO - root - 2017-12-05 13:47:51.659433: step 14050, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 76h:19m:59s remains)
INFO - root - 2017-12-05 13:48:00.080900: step 14060, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 75h:25m:55s remains)
INFO - root - 2017-12-05 13:48:08.604230: step 14070, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 75h:41m:22s remains)
INFO - root - 2017-12-05 13:48:17.025303: step 14080, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 75h:16m:08s remains)
INFO - root - 2017-12-05 13:48:25.592691: step 14090, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 76h:02m:46s remains)
INFO - root - 2017-12-05 13:48:34.112384: step 14100, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 78h:32m:23s remains)
2017-12-05 13:48:34.839003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9978492 -4.0092311 -4.0188093 -4.0200515 -4.0098252 -4.0173631 -4.0294476 -4.0543022 -4.0947 -4.1334686 -4.1496725 -4.1406565 -4.1256413 -4.1091757 -4.0904465][-3.9742436 -3.991816 -4.0022788 -4.0125771 -4.0095038 -4.012475 -4.0235391 -4.0464492 -4.0831323 -4.1253672 -4.1494508 -4.1475296 -4.1369815 -4.1190853 -4.101655][-3.9808393 -3.9962738 -4.002583 -4.0140848 -4.0112038 -4.0078073 -4.0132604 -4.0327649 -4.0658545 -4.1078572 -4.1394515 -4.1496477 -4.1527872 -4.142951 -4.1326571][-4.0061593 -4.0197349 -4.0201368 -4.0237942 -4.0131273 -3.9986968 -3.9950724 -4.0103426 -4.0376086 -4.0801063 -4.1225233 -4.1509314 -4.1712351 -4.1793766 -4.1773829][-4.0362515 -4.0477128 -4.0328426 -4.0218163 -4.0113096 -3.9904099 -3.9813497 -3.9937308 -4.0173025 -4.0631709 -4.1172905 -4.1616364 -4.193109 -4.2120233 -4.2128334][-4.0413394 -4.0431933 -4.0082626 -3.9794357 -3.9711614 -3.9598362 -3.9540296 -3.9716644 -4.0026855 -4.0556073 -4.1111455 -4.1559992 -4.1875243 -4.2108192 -4.2138138][-4.0290475 -4.0289364 -3.9854276 -3.9455454 -3.9309881 -3.9197226 -3.9218934 -3.9482086 -3.9853575 -4.0429664 -4.0945292 -4.127111 -4.1525264 -4.1775827 -4.1855407][-4.0127659 -4.0273848 -4.0020595 -3.9668853 -3.9411812 -3.9151139 -3.9081402 -3.9291763 -3.9631906 -4.0191417 -4.0714602 -4.0967007 -4.1218352 -4.1440606 -4.1502438][-4.003417 -4.0279055 -4.0182948 -3.994441 -3.9694743 -3.9393277 -3.9196653 -3.9310105 -3.9602752 -4.0123634 -4.0608468 -4.077837 -4.0974379 -4.11815 -4.1212239][-3.9947686 -4.0180283 -4.0117164 -3.9938037 -3.9779942 -3.9561641 -3.9354968 -3.9346886 -3.9547582 -3.9930656 -4.0292187 -4.0426044 -4.0647597 -4.0920148 -4.0995483][-4.0049567 -4.0237031 -4.0161862 -4.0012426 -3.998678 -3.9848983 -3.9568536 -3.9277067 -3.9276907 -3.9465537 -3.9653373 -3.9837372 -4.0229597 -4.0660057 -4.0796089][-4.0310068 -4.0500708 -4.0480309 -4.042007 -4.0460553 -4.0339637 -3.9960833 -3.9430707 -3.9167008 -3.9260361 -3.9403811 -3.960742 -4.0051003 -4.0538449 -4.071672][-4.0682449 -4.0851488 -4.0888457 -4.0868926 -4.0920253 -4.0810561 -4.0444918 -3.9856253 -3.9446912 -3.9451792 -3.9595556 -3.9770248 -4.011826 -4.0533361 -4.0718412][-4.126164 -4.1362581 -4.1394854 -4.1375756 -4.1420135 -4.1345453 -4.1060734 -4.0594811 -4.0192327 -4.0108666 -4.0205259 -4.03428 -4.0563269 -4.0857296 -4.1003213][-4.1784077 -4.1894484 -4.1945462 -4.1937623 -4.1952133 -4.1897221 -4.1718626 -4.1431341 -4.1169686 -4.1090784 -4.111393 -4.1170788 -4.1269846 -4.1400671 -4.1488819]]...]
INFO - root - 2017-12-05 13:48:43.059185: step 14110, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 74h:38m:01s remains)
INFO - root - 2017-12-05 13:48:51.556491: step 14120, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 73h:50m:38s remains)
INFO - root - 2017-12-05 13:49:00.149457: step 14130, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 76h:45m:57s remains)
INFO - root - 2017-12-05 13:49:08.774628: step 14140, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 78h:19m:05s remains)
INFO - root - 2017-12-05 13:49:17.412929: step 14150, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 73h:52m:33s remains)
INFO - root - 2017-12-05 13:49:25.973924: step 14160, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 75h:43m:01s remains)
INFO - root - 2017-12-05 13:49:34.524638: step 14170, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 75h:55m:42s remains)
INFO - root - 2017-12-05 13:49:43.089988: step 14180, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 76h:45m:18s remains)
INFO - root - 2017-12-05 13:49:51.529903: step 14190, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 74h:54m:30s remains)
INFO - root - 2017-12-05 13:50:00.049149: step 14200, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 74h:42m:11s remains)
2017-12-05 13:50:00.919192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3422489 -4.3372097 -4.3351278 -4.336102 -4.3377476 -4.336885 -4.3345742 -4.333715 -4.3335757 -4.3334985 -4.3333716 -4.3347325 -4.3362665 -4.3365459 -4.3378758][-4.3357277 -4.3269343 -4.3226523 -4.3230162 -4.324019 -4.3233271 -4.3220949 -4.3229818 -4.3246188 -4.326447 -4.3279729 -4.3294659 -4.3296113 -4.3289742 -4.3310342][-4.3199086 -4.307879 -4.3010941 -4.2997074 -4.2990909 -4.2989693 -4.2974224 -4.2968454 -4.2977805 -4.2991161 -4.3027215 -4.3070793 -4.3096414 -4.3118792 -4.3162971][-4.2963591 -4.2790837 -4.2696128 -4.2660546 -4.2644763 -4.2644444 -4.2610846 -4.2596188 -4.257844 -4.2547374 -4.2567949 -4.262845 -4.2719274 -4.2811322 -4.2901707][-4.2648454 -4.2375169 -4.2263069 -4.2221174 -4.2221236 -4.2235751 -4.2194977 -4.2175407 -4.2109618 -4.2012897 -4.2008305 -4.2103658 -4.2270107 -4.2432318 -4.2570796][-4.2312751 -4.1940374 -4.1815343 -4.1787972 -4.1792555 -4.1781392 -4.1687665 -4.1595092 -4.1435671 -4.1293058 -4.1360211 -4.1586714 -4.1857119 -4.2075715 -4.2223353][-4.2118759 -4.1725416 -4.1610475 -4.1564484 -4.14886 -4.1340132 -4.1101303 -4.0833755 -4.0535231 -4.0310688 -4.04743 -4.0900736 -4.1282744 -4.1571293 -4.1751914][-4.2124534 -4.1773248 -4.1673675 -4.1605511 -4.1432281 -4.1128883 -4.066793 -4.013144 -3.9616675 -3.9263458 -3.9460235 -4.00507 -4.0550585 -4.0930228 -4.12045][-4.2199516 -4.1922574 -4.18424 -4.1781693 -4.1621542 -4.1288567 -4.068408 -3.9956436 -3.9237845 -3.8733749 -3.8862326 -3.9473493 -4.0050197 -4.0501232 -4.0860906][-4.22332 -4.2021828 -4.198132 -4.1957436 -4.1895089 -4.1682625 -4.1181531 -4.0500793 -3.9798608 -3.9283686 -3.9289949 -3.9722896 -4.018261 -4.0601768 -4.0973768][-4.2191725 -4.2010665 -4.20001 -4.2020988 -4.2066736 -4.20282 -4.1765079 -4.130096 -4.0786643 -4.0374255 -4.0295887 -4.04944 -4.0782456 -4.1109715 -4.1381559][-4.2126822 -4.1927238 -4.190784 -4.1957722 -4.2089162 -4.2198548 -4.2139478 -4.1911497 -4.1602035 -4.1321297 -4.123158 -4.1321592 -4.1507754 -4.1692963 -4.179379][-4.2158871 -4.192678 -4.185164 -4.1889577 -4.2045021 -4.2203679 -4.2269711 -4.2183514 -4.2016091 -4.1856351 -4.1821427 -4.1899986 -4.2027059 -4.2077756 -4.2030373][-4.2244945 -4.1951632 -4.1800036 -4.1797376 -4.1920481 -4.2065735 -4.218977 -4.214735 -4.2021551 -4.1929917 -4.1954055 -4.2046213 -4.2153358 -4.2157145 -4.2066269][-4.2384214 -4.2035613 -4.1817055 -4.17499 -4.1802344 -4.1896968 -4.2012053 -4.1939483 -4.180254 -4.1733756 -4.1783891 -4.1879826 -4.1983986 -4.2009234 -4.1923304]]...]
INFO - root - 2017-12-05 13:50:09.301204: step 14210, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 73h:39m:29s remains)
INFO - root - 2017-12-05 13:50:17.626415: step 14220, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.830 sec/batch; 73h:25m:16s remains)
INFO - root - 2017-12-05 13:50:26.098009: step 14230, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 73h:48m:38s remains)
INFO - root - 2017-12-05 13:50:34.580450: step 14240, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 74h:50m:03s remains)
INFO - root - 2017-12-05 13:50:43.156813: step 14250, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 77h:21m:15s remains)
INFO - root - 2017-12-05 13:50:51.678281: step 14260, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 75h:05m:17s remains)
INFO - root - 2017-12-05 13:51:00.135574: step 14270, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 74h:29m:30s remains)
INFO - root - 2017-12-05 13:51:08.737201: step 14280, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 74h:32m:16s remains)
INFO - root - 2017-12-05 13:51:17.230664: step 14290, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 76h:05m:35s remains)
INFO - root - 2017-12-05 13:51:25.694199: step 14300, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.856 sec/batch; 75h:37m:12s remains)
2017-12-05 13:51:26.459405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1966791 -4.22534 -4.246882 -4.2484193 -4.2382836 -4.2202678 -4.2069016 -4.2193437 -4.2462549 -4.2679987 -4.27686 -4.2689581 -4.249197 -4.239718 -4.2270522][-4.1874862 -4.2179389 -4.243257 -4.2460032 -4.23151 -4.2042713 -4.1832714 -4.197063 -4.2320876 -4.2623539 -4.2777123 -4.2790875 -4.2705345 -4.2677441 -4.2606049][-4.1864734 -4.2102413 -4.2302237 -4.2272305 -4.2077336 -4.1803389 -4.1593852 -4.1688547 -4.2045379 -4.2439728 -4.2719688 -4.2852778 -4.2816839 -4.2789125 -4.276741][-4.1826849 -4.1973362 -4.2090549 -4.2066402 -4.1929436 -4.1714067 -4.1450577 -4.1365376 -4.1634369 -4.2101865 -4.2527595 -4.2733626 -4.2712383 -4.2651992 -4.2622948][-4.1701288 -4.17552 -4.1866713 -4.1935191 -4.1906905 -4.1681304 -4.1217566 -4.0844955 -4.1004605 -4.1557059 -4.2125692 -4.240418 -4.2409987 -4.2345839 -4.2312288][-4.1460676 -4.146071 -4.1587133 -4.1744032 -4.1745038 -4.1401825 -4.0658946 -3.9984643 -4.0088978 -4.0798616 -4.1528816 -4.1867061 -4.1923094 -4.1903868 -4.1900978][-4.1362181 -4.1379342 -4.1535659 -4.1685719 -4.1588454 -4.1032553 -3.9938107 -3.8887026 -3.9051132 -4.0027347 -4.0931311 -4.1312232 -4.13946 -4.143724 -4.1473389][-4.1474466 -4.1524487 -4.1656914 -4.172184 -4.1447511 -4.0664167 -3.9210792 -3.7805014 -3.811408 -3.9432013 -4.0505385 -4.0971661 -4.1142249 -4.1286268 -4.1394615][-4.183506 -4.1894341 -4.1951013 -4.1862249 -4.1401548 -4.0491748 -3.896121 -3.7542164 -3.7853065 -3.9179382 -4.0214095 -4.075707 -4.1096487 -4.1382985 -4.1586881][-4.2322974 -4.2375011 -4.2322383 -4.2091336 -4.1544895 -4.0737991 -3.9560373 -3.848443 -3.8576343 -3.9515316 -4.0291338 -4.0845942 -4.1352415 -4.1783304 -4.2062454][-4.26784 -4.2691503 -4.2577205 -4.2272258 -4.174067 -4.11258 -4.0383563 -3.9616363 -3.9478016 -4.0035515 -4.0579543 -4.1082916 -4.1661644 -4.217227 -4.2503181][-4.2848539 -4.28194 -4.2689977 -4.2399931 -4.1991944 -4.1600065 -4.1173725 -4.05992 -4.0355625 -4.0670829 -4.1044035 -4.14203 -4.1930561 -4.2423959 -4.2766142][-4.2879181 -4.277792 -4.2646942 -4.2432728 -4.2205176 -4.2023168 -4.1828752 -4.146337 -4.1280804 -4.1492157 -4.1734095 -4.1966252 -4.2338729 -4.2728081 -4.2990518][-4.2894115 -4.27532 -4.2628722 -4.2480516 -4.238255 -4.2335377 -4.2281752 -4.2095585 -4.1991539 -4.2132306 -4.2288051 -4.2414179 -4.2661419 -4.2942343 -4.3113623][-4.2879438 -4.2737336 -4.2639589 -4.2549624 -4.2506433 -4.252738 -4.2554383 -4.2492042 -4.242774 -4.2498503 -4.2595468 -4.2657962 -4.2791414 -4.2957006 -4.3049951]]...]
INFO - root - 2017-12-05 13:51:34.778886: step 14310, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 74h:39m:46s remains)
INFO - root - 2017-12-05 13:51:43.219333: step 14320, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 74h:49m:22s remains)
INFO - root - 2017-12-05 13:51:51.686579: step 14330, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 75h:15m:12s remains)
INFO - root - 2017-12-05 13:52:00.157737: step 14340, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.871 sec/batch; 77h:00m:02s remains)
INFO - root - 2017-12-05 13:52:08.623643: step 14350, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 74h:58m:46s remains)
INFO - root - 2017-12-05 13:52:17.114774: step 14360, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.816 sec/batch; 72h:06m:01s remains)
INFO - root - 2017-12-05 13:52:25.615773: step 14370, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 75h:21m:57s remains)
INFO - root - 2017-12-05 13:52:34.127654: step 14380, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 73h:52m:21s remains)
INFO - root - 2017-12-05 13:52:42.773580: step 14390, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.816 sec/batch; 72h:08m:21s remains)
INFO - root - 2017-12-05 13:52:51.157489: step 14400, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 74h:49m:01s remains)
2017-12-05 13:52:51.909711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2039428 -4.2049279 -4.2011847 -4.1892939 -4.1821551 -4.1946983 -4.21815 -4.2341704 -4.228467 -4.2034965 -4.17927 -4.1549606 -4.1363282 -4.1544576 -4.1941729][-4.1920004 -4.1909862 -4.1839156 -4.1676745 -4.1531787 -4.1598272 -4.1889725 -4.2124782 -4.2107449 -4.1832418 -4.1502662 -4.1187048 -4.1026182 -4.1338339 -4.1837053][-4.1938596 -4.1856756 -4.1729574 -4.1530252 -4.1279755 -4.1265407 -4.1616888 -4.1931996 -4.1900964 -4.1601815 -4.1264744 -4.0968056 -4.086041 -4.122829 -4.1788168][-4.2084765 -4.1882386 -4.1673069 -4.1413932 -4.1072431 -4.0979624 -4.1373687 -4.1727428 -4.1696606 -4.1486812 -4.12502 -4.099957 -4.0941505 -4.1296563 -4.1847587][-4.2309303 -4.1956377 -4.1594405 -4.1228843 -4.0831103 -4.0707793 -4.108674 -4.1421685 -4.1402907 -4.1376357 -4.1381769 -4.1284313 -4.1292119 -4.1615481 -4.2085938][-4.2420793 -4.1998405 -4.1503272 -4.0990348 -4.0499334 -4.0348511 -4.0681014 -4.0950952 -4.0961041 -4.1129665 -4.1381307 -4.1467905 -4.1596894 -4.1950445 -4.2361031][-4.2334409 -4.1974249 -4.1455512 -4.0860949 -4.0258389 -4.0023427 -4.0210238 -4.0385551 -4.037518 -4.0539656 -4.0889091 -4.1138668 -4.1476269 -4.197258 -4.2427149][-4.2292414 -4.2010651 -4.1571093 -4.1040835 -4.0439324 -4.0116863 -4.0092468 -4.006011 -3.9877205 -3.987618 -4.024076 -4.0707693 -4.1209216 -4.1799464 -4.2337842][-4.2324381 -4.2129803 -4.1817169 -4.1462822 -4.1035795 -4.0726681 -4.053812 -4.0332818 -3.9987359 -3.9839673 -4.0130105 -4.0657339 -4.1135945 -4.1670647 -4.2209206][-4.2322116 -4.219027 -4.1962423 -4.1782765 -4.15919 -4.1372271 -4.1149836 -4.0931773 -4.0665846 -4.0620604 -4.089076 -4.119298 -4.1401386 -4.1725192 -4.2189808][-4.2304049 -4.2243056 -4.2092361 -4.19741 -4.1886916 -4.1755886 -4.16014 -4.1456285 -4.1366482 -4.1525297 -4.1806526 -4.1906314 -4.1863055 -4.1984396 -4.2341313][-4.2350507 -4.2308049 -4.22147 -4.2145729 -4.2100406 -4.2017961 -4.1895814 -4.1811414 -4.186 -4.2116265 -4.2362404 -4.2403355 -4.2327018 -4.2372861 -4.2652693][-4.2432485 -4.2372584 -4.2335038 -4.2355714 -4.2395782 -4.2361221 -4.2229033 -4.2146983 -4.2219381 -4.2482104 -4.2700667 -4.276248 -4.2745285 -4.2793489 -4.2997832][-4.2556868 -4.2472563 -4.246592 -4.2548151 -4.2637072 -4.2648711 -4.2551227 -4.2475657 -4.2499738 -4.2691774 -4.2884827 -4.2993546 -4.3056431 -4.3125815 -4.3252459][-4.273911 -4.2660346 -4.2642365 -4.2695355 -4.276895 -4.2797232 -4.274714 -4.2706056 -4.2704277 -4.2813139 -4.2952461 -4.3074675 -4.3186183 -4.327035 -4.3341651]]...]
INFO - root - 2017-12-05 13:53:00.390811: step 14410, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 75h:55m:59s remains)
INFO - root - 2017-12-05 13:53:08.944511: step 14420, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 77h:40m:12s remains)
INFO - root - 2017-12-05 13:53:17.422089: step 14430, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 77h:27m:04s remains)
INFO - root - 2017-12-05 13:53:26.003442: step 14440, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 76h:29m:35s remains)
INFO - root - 2017-12-05 13:53:34.449486: step 14450, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 78h:51m:43s remains)
INFO - root - 2017-12-05 13:53:43.023210: step 14460, loss = 2.04, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 72h:42m:19s remains)
INFO - root - 2017-12-05 13:53:51.656905: step 14470, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 75h:49m:57s remains)
INFO - root - 2017-12-05 13:54:00.213403: step 14480, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 76h:44m:51s remains)
INFO - root - 2017-12-05 13:54:08.690068: step 14490, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 75h:00m:19s remains)
INFO - root - 2017-12-05 13:54:17.226858: step 14500, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 74h:23m:01s remains)
2017-12-05 13:54:18.013857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.200141 -4.2045794 -4.1926579 -4.1570888 -4.1116457 -4.0863948 -4.0845032 -4.1106391 -4.1444798 -4.1782427 -4.2014675 -4.2144656 -4.2062674 -4.1737661 -4.1392746][-4.2389932 -4.2350149 -4.2162857 -4.1748843 -4.1181369 -4.0772381 -4.0658388 -4.0848746 -4.1190038 -4.1522436 -4.175962 -4.1896281 -4.1839023 -4.1545472 -4.1237493][-4.2666874 -4.2557259 -4.2348013 -4.1966467 -4.1384959 -4.0815372 -4.0523019 -4.0588527 -4.0917797 -4.1201878 -4.1405454 -4.1583962 -4.1603289 -4.1409431 -4.1161847][-4.2769709 -4.2615547 -4.2404256 -4.2102613 -4.156095 -4.0857878 -4.0341263 -4.02739 -4.0621972 -4.0877929 -4.104948 -4.1307707 -4.1477008 -4.1403918 -4.1217961][-4.2738557 -4.2567596 -4.2343969 -4.2133293 -4.1653123 -4.0840807 -4.0100455 -3.9936187 -4.0347867 -4.0629072 -4.0790925 -4.1098661 -4.1389112 -4.1413321 -4.1316471][-4.2642865 -4.2473445 -4.2237768 -4.2074094 -4.1654859 -4.0750313 -3.9780252 -3.9561455 -4.0124073 -4.0471482 -4.0603328 -4.0903034 -4.1263518 -4.138145 -4.140842][-4.2430568 -4.2272267 -4.2018337 -4.1839485 -4.1477003 -4.054244 -3.9382663 -3.9146767 -3.992976 -4.0399356 -4.0439763 -4.0639887 -4.0991087 -4.117065 -4.1291013][-4.216177 -4.2004924 -4.1712728 -4.1496658 -4.1191473 -4.0329523 -3.9081969 -3.8829763 -3.9778824 -4.0405488 -4.0340209 -4.0355124 -4.059958 -4.078567 -4.0978518][-4.1961889 -4.1864691 -4.1610174 -4.1373138 -4.1111989 -4.0357175 -3.9049029 -3.8680334 -3.9614635 -4.0330081 -4.0224032 -4.0087805 -4.0231204 -4.0404186 -4.0685558][-4.1974726 -4.1971331 -4.1808887 -4.1569581 -4.1304622 -4.0650463 -3.9364479 -3.8834743 -3.9564981 -4.0222754 -4.0119238 -3.9946773 -4.0044971 -4.0216713 -4.0594287][-4.2097883 -4.2111073 -4.1997633 -4.1778531 -4.1513667 -4.0994205 -3.9907947 -3.9279008 -3.9710021 -4.0187283 -4.008687 -3.9958904 -4.0066056 -4.0259824 -4.0719948][-4.227128 -4.2229695 -4.2100339 -4.1902761 -4.1636238 -4.120729 -4.0370193 -3.9762678 -3.994482 -4.025054 -4.0144258 -4.0075803 -4.0207644 -4.0384521 -4.0876579][-4.2445827 -4.2290092 -4.2087469 -4.1880069 -4.1637321 -4.1273746 -4.0652475 -4.0162959 -4.0222173 -4.0397449 -4.0296121 -4.0280752 -4.0438275 -4.0575395 -4.1055112][-4.26236 -4.2368708 -4.2059193 -4.1810079 -4.1610293 -4.1352429 -4.0930281 -4.0570993 -4.0559616 -4.0615225 -4.0495081 -4.0486174 -4.0666461 -4.0811567 -4.1284828][-4.2803435 -4.2516522 -4.2145138 -4.18382 -4.1650276 -4.1487451 -4.1240687 -4.0981064 -4.0920243 -4.0881815 -4.0678396 -4.0615888 -4.0817723 -4.1023769 -4.1495576]]...]
INFO - root - 2017-12-05 13:54:26.435584: step 14510, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 75h:16m:05s remains)
INFO - root - 2017-12-05 13:54:35.114522: step 14520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 75h:45m:31s remains)
INFO - root - 2017-12-05 13:54:43.778922: step 14530, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 75h:51m:00s remains)
INFO - root - 2017-12-05 13:54:52.388325: step 14540, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 76h:41m:26s remains)
INFO - root - 2017-12-05 13:55:00.926883: step 14550, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 74h:32m:39s remains)
INFO - root - 2017-12-05 13:55:09.425939: step 14560, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 74h:34m:10s remains)
INFO - root - 2017-12-05 13:55:17.915574: step 14570, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 75h:51m:23s remains)
INFO - root - 2017-12-05 13:55:26.497315: step 14580, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 77h:58m:07s remains)
INFO - root - 2017-12-05 13:55:35.046670: step 14590, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 75h:04m:26s remains)
INFO - root - 2017-12-05 13:55:43.673511: step 14600, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 75h:57m:34s remains)
2017-12-05 13:55:44.443307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2028542 -4.2284522 -4.2271123 -4.1886468 -4.1318731 -4.0806932 -4.0541053 -4.072783 -4.1163378 -4.1524572 -4.1724663 -4.1766272 -4.1813025 -4.194973 -4.2181096][-4.2327647 -4.2543669 -4.2518668 -4.2156043 -4.162066 -4.1140456 -4.0802474 -4.0730362 -4.0887566 -4.1112056 -4.1229067 -4.1233053 -4.1377878 -4.16616 -4.2027688][-4.258019 -4.2708917 -4.2620912 -4.225771 -4.1722417 -4.1235662 -4.0861082 -4.0704107 -4.0725412 -4.0866408 -4.092608 -4.0919876 -4.1151681 -4.1525006 -4.1901693][-4.2720542 -4.279418 -4.2591567 -4.2119794 -4.1494088 -4.0923071 -4.0606761 -4.0631804 -4.0793185 -4.0974936 -4.1036997 -4.1047978 -4.1290722 -4.16213 -4.1863079][-4.2823558 -4.2829022 -4.247746 -4.1794467 -4.1002688 -4.0372252 -4.0150838 -4.0535712 -4.10651 -4.1365929 -4.1453948 -4.1524053 -4.171762 -4.1918993 -4.2011967][-4.2846131 -4.2759333 -4.2256312 -4.1351743 -4.0343318 -3.9582324 -3.9394107 -4.0129437 -4.1133189 -4.1677547 -4.1848607 -4.2019129 -4.2198596 -4.2284589 -4.2314429][-4.2841163 -4.2674632 -4.20049 -4.08333 -3.9531507 -3.8494008 -3.8116822 -3.9050419 -4.051342 -4.1408358 -4.1832824 -4.221972 -4.2505322 -4.2597384 -4.2657623][-4.278842 -4.2563391 -4.1798382 -4.0491576 -3.899713 -3.7688568 -3.7003145 -3.7882726 -3.9593091 -4.0773687 -4.1512585 -4.2167983 -4.2615 -4.2826157 -4.2970533][-4.2583547 -4.2435927 -4.1824613 -4.0753078 -3.9478602 -3.8314414 -3.7652268 -3.8261108 -3.9681449 -4.0747395 -4.1533337 -4.2243552 -4.2732544 -4.2993217 -4.3163247][-4.2231021 -4.2331548 -4.2033114 -4.1384706 -4.0568357 -3.9790585 -3.9336681 -3.9694631 -4.0614867 -4.13658 -4.1952424 -4.2495222 -4.2870951 -4.3067756 -4.3184128][-4.1748109 -4.218173 -4.225749 -4.2042732 -4.168262 -4.1278906 -4.1007705 -4.116611 -4.1683826 -4.2158656 -4.2505178 -4.280365 -4.30226 -4.3141384 -4.3190889][-4.1454725 -4.2170882 -4.2500663 -4.2581925 -4.2518787 -4.2347574 -4.2190318 -4.2222366 -4.2501068 -4.2797084 -4.29667 -4.30959 -4.3183045 -4.3214736 -4.3202386][-4.1490817 -4.2284622 -4.2700043 -4.2918081 -4.3023133 -4.2985721 -4.2888265 -4.28667 -4.3006473 -4.3180141 -4.3246694 -4.3287816 -4.3287473 -4.3257408 -4.320199][-4.1920633 -4.2563548 -4.292325 -4.3143625 -4.3278823 -4.3282895 -4.3216114 -4.3186789 -4.3244886 -4.3338141 -4.336885 -4.3374248 -4.3343372 -4.3293128 -4.322999][-4.2482853 -4.2896132 -4.3105631 -4.3225775 -4.3285966 -4.3269477 -4.3221807 -4.3203583 -4.3229761 -4.3280692 -4.3298717 -4.3295531 -4.32712 -4.3232341 -4.3182335]]...]
INFO - root - 2017-12-05 13:55:52.928220: step 14610, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 73h:39m:11s remains)
INFO - root - 2017-12-05 13:56:01.570787: step 14620, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 75h:14m:37s remains)
INFO - root - 2017-12-05 13:56:10.126045: step 14630, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 73h:06m:13s remains)
INFO - root - 2017-12-05 13:56:18.600713: step 14640, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 73h:51m:06s remains)
INFO - root - 2017-12-05 13:56:27.136330: step 14650, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 75h:31m:11s remains)
INFO - root - 2017-12-05 13:56:35.666220: step 14660, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 76h:46m:24s remains)
INFO - root - 2017-12-05 13:56:44.244410: step 14670, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 75h:09m:57s remains)
INFO - root - 2017-12-05 13:56:52.667188: step 14680, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 76h:15m:48s remains)
INFO - root - 2017-12-05 13:57:01.281068: step 14690, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 76h:56m:14s remains)
INFO - root - 2017-12-05 13:57:09.950383: step 14700, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 76h:10m:11s remains)
2017-12-05 13:57:10.702805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3052788 -4.2991738 -4.2970018 -4.3000212 -4.2965689 -4.2942934 -4.29623 -4.29624 -4.2910318 -4.282732 -4.2792373 -4.2773352 -4.2762718 -4.2788849 -4.284214][-4.249413 -4.242341 -4.2441859 -4.2536335 -4.2596016 -4.2620764 -4.2625194 -4.2579107 -4.2488651 -4.2380252 -4.2318983 -4.2270555 -4.2236781 -4.2249465 -4.2292833][-4.1757717 -4.161869 -4.159472 -4.17125 -4.1859913 -4.1935472 -4.1911755 -4.1811519 -4.1693759 -4.1559968 -4.1454072 -4.1338563 -4.1260252 -4.1248932 -4.1267524][-4.0897808 -4.0610619 -4.0456495 -4.0522671 -4.0690384 -4.0781212 -4.0755935 -4.0643864 -4.0531487 -4.038249 -4.0237727 -4.0066447 -3.9932709 -3.9886398 -3.9878068][-4.0051565 -3.9576359 -3.9264665 -3.9245772 -3.9381387 -3.9449255 -3.9410365 -3.9303958 -3.9204457 -3.9096136 -3.8968778 -3.8787093 -3.8604007 -3.8518932 -3.8483009][-3.9870398 -3.9348111 -3.8977008 -3.8876078 -3.8925266 -3.8919222 -3.8842168 -3.8761983 -3.8732135 -3.8742096 -3.87012 -3.8579674 -3.8449321 -3.8393242 -3.8369405][-4.0727282 -4.0311317 -3.9942 -3.9752619 -3.9699163 -3.9594855 -3.9459462 -3.940136 -3.9492667 -3.9695501 -3.983393 -3.9856007 -3.9832549 -3.9821968 -3.9802673][-4.1591425 -4.129673 -4.0999541 -4.0813508 -4.0699754 -4.0518308 -4.0299635 -4.0175686 -4.0292053 -4.0591631 -4.0865617 -4.1017313 -4.1089606 -4.1123834 -4.1105757][-4.2014527 -4.1791911 -4.1566882 -4.1431131 -4.1331534 -4.1164875 -4.0942311 -4.0786943 -4.0867858 -4.1144061 -4.1439085 -4.1637435 -4.17597 -4.1820793 -4.179728][-4.2087507 -4.1865063 -4.1658545 -4.155468 -4.149447 -4.1414609 -4.13012 -4.1209278 -4.12518 -4.1441393 -4.1675644 -4.1838431 -4.1928458 -4.1957688 -4.1917334][-4.2134428 -4.1903892 -4.1677704 -4.1576428 -4.1535611 -4.1517363 -4.1500206 -4.1476908 -4.1483507 -4.1564245 -4.1687202 -4.1774421 -4.1812296 -4.1800461 -4.1725855][-4.2267108 -4.2050843 -4.1802373 -4.1673789 -4.1608076 -4.1587558 -4.1584225 -4.1563182 -4.1506772 -4.148653 -4.1479826 -4.1490788 -4.1522355 -4.1529307 -4.1483269][-4.2407579 -4.2213478 -4.1958966 -4.17984 -4.1701493 -4.1636453 -4.158855 -4.1526256 -4.1428485 -4.1359978 -4.1281738 -4.1252131 -4.1301851 -4.1338968 -4.1331949][-4.2458925 -4.2268562 -4.2020473 -4.1862626 -4.1771889 -4.16771 -4.1602817 -4.1530361 -4.1460624 -4.1419744 -4.1356978 -4.133718 -4.1403842 -4.1439829 -4.1422195][-4.2434788 -4.2212095 -4.1978908 -4.1849718 -4.1781721 -4.1702909 -4.1642575 -4.1590939 -4.1567435 -4.1552343 -4.1493793 -4.1465225 -4.1527581 -4.1559029 -4.1561184]]...]
INFO - root - 2017-12-05 13:57:19.117749: step 14710, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 75h:08m:35s remains)
INFO - root - 2017-12-05 13:57:27.609686: step 14720, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 73h:50m:52s remains)
INFO - root - 2017-12-05 13:57:36.253845: step 14730, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 76h:41m:14s remains)
INFO - root - 2017-12-05 13:57:44.866499: step 14740, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 75h:03m:06s remains)
INFO - root - 2017-12-05 13:57:53.374493: step 14750, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 77h:41m:56s remains)
INFO - root - 2017-12-05 13:58:01.919327: step 14760, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 74h:44m:06s remains)
INFO - root - 2017-12-05 13:58:10.347913: step 14770, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 73h:48m:50s remains)
INFO - root - 2017-12-05 13:58:18.858463: step 14780, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 76h:11m:07s remains)
INFO - root - 2017-12-05 13:58:27.451397: step 14790, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 75h:35m:13s remains)
INFO - root - 2017-12-05 13:58:35.962268: step 14800, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 77h:00m:02s remains)
2017-12-05 13:58:36.695504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1714811 -4.2389164 -4.2777729 -4.29283 -4.2947474 -4.2975631 -4.3033457 -4.3100772 -4.3229003 -4.3317842 -4.3391752 -4.3472409 -4.3523207 -4.3529687 -4.351378][-4.1747789 -4.2401729 -4.2788134 -4.2933011 -4.2935987 -4.2955904 -4.3029728 -4.3153758 -4.3340988 -4.3470554 -4.3536315 -4.3582864 -4.3612823 -4.3600311 -4.3534613][-4.1984529 -4.2517738 -4.2821221 -4.2876759 -4.280592 -4.2797184 -4.2859163 -4.3017235 -4.3226514 -4.3364072 -4.34391 -4.3500776 -4.3557234 -4.3560481 -4.3483505][-4.2204151 -4.2501736 -4.2610598 -4.2518106 -4.2359843 -4.2320423 -4.2354989 -4.2507725 -4.2752004 -4.2940741 -4.3115888 -4.3312159 -4.3480248 -4.3531237 -4.3446007][-4.2014194 -4.2077165 -4.1946607 -4.1716418 -4.1467237 -4.1403751 -4.1407533 -4.1513762 -4.1827526 -4.2144809 -4.2501788 -4.2906237 -4.3242135 -4.3405676 -4.3377929][-4.1575346 -4.1483994 -4.11781 -4.0852156 -4.0478029 -4.0293813 -4.0198293 -4.0176163 -4.0535955 -4.1047654 -4.1652436 -4.2307849 -4.2845831 -4.3195786 -4.3322496][-4.1141143 -4.0971527 -4.0570083 -4.0193005 -3.9723992 -3.9321501 -3.8894768 -3.8579776 -3.8997064 -3.9786747 -4.0701857 -4.1595669 -4.2306442 -4.2853818 -4.3179455][-4.1086912 -4.0907993 -4.0469193 -4.0046859 -3.9519935 -3.8899808 -3.8114398 -3.745491 -3.7919221 -3.8900564 -3.9969795 -4.0936632 -4.16833 -4.2340469 -4.2839656][-4.1484346 -4.1389313 -4.1081905 -4.0717587 -4.0213647 -3.9573359 -3.877918 -3.8118427 -3.8402696 -3.9092236 -3.9865608 -4.0587921 -4.1201344 -4.1851082 -4.2416954][-4.2135477 -4.2134342 -4.1975646 -4.1705112 -4.1245294 -4.0666413 -4.0054574 -3.9575236 -3.9646361 -3.990799 -4.0213079 -4.0538321 -4.0891433 -4.1434941 -4.1995473][-4.29179 -4.2950506 -4.2800903 -4.2501197 -4.2027025 -4.1497173 -4.1059628 -4.0766263 -4.0757275 -4.0793118 -4.0824132 -4.0839114 -4.0913496 -4.1244063 -4.1672421][-4.3441105 -4.349719 -4.3342161 -4.3029346 -4.2599611 -4.2181444 -4.1897421 -4.1723986 -4.1732569 -4.173296 -4.171535 -4.1629081 -4.1510811 -4.1592112 -4.1730018][-4.3659821 -4.3736496 -4.3623581 -4.3355021 -4.2986665 -4.2700777 -4.25446 -4.2486186 -4.2592354 -4.2657933 -4.2699122 -4.2638907 -4.2470264 -4.2360449 -4.2224503][-4.3661709 -4.3740277 -4.3678823 -4.3482823 -4.3201637 -4.3028617 -4.2960324 -4.3000522 -4.3180623 -4.3330297 -4.345067 -4.3464661 -4.3339968 -4.3148727 -4.2874937][-4.357542 -4.3657732 -4.3643918 -4.3511596 -4.3278456 -4.3100629 -4.2989917 -4.3041382 -4.32706 -4.3523326 -4.3749967 -4.3864169 -4.3830419 -4.367383 -4.340548]]...]
INFO - root - 2017-12-05 13:58:45.208515: step 14810, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 72h:33m:42s remains)
INFO - root - 2017-12-05 13:58:53.700148: step 14820, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 74h:39m:12s remains)
INFO - root - 2017-12-05 13:59:02.105031: step 14830, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 73h:21m:56s remains)
INFO - root - 2017-12-05 13:59:10.664235: step 14840, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 76h:03m:22s remains)
INFO - root - 2017-12-05 13:59:19.164673: step 14850, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 75h:01m:24s remains)
INFO - root - 2017-12-05 13:59:27.553448: step 14860, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 75h:46m:47s remains)
INFO - root - 2017-12-05 13:59:36.172258: step 14870, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 73h:30m:05s remains)
INFO - root - 2017-12-05 13:59:44.729593: step 14880, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 74h:47m:41s remains)
INFO - root - 2017-12-05 13:59:53.200069: step 14890, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 76h:06m:39s remains)
INFO - root - 2017-12-05 14:00:01.847614: step 14900, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 77h:20m:23s remains)
2017-12-05 14:00:02.630300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1398787 -4.1374683 -4.1121988 -4.0919995 -4.0973959 -4.1213913 -4.143558 -4.1329679 -4.1039166 -4.0895095 -4.09426 -4.1073503 -4.1219215 -4.1172767 -4.0991287][-4.1487651 -4.1422639 -4.1108832 -4.0903144 -4.1008196 -4.1290059 -4.1474023 -4.1256723 -4.0827041 -4.0670385 -4.0829077 -4.1052222 -4.1194735 -4.102829 -4.072999][-4.1531949 -4.1479011 -4.1250811 -4.1173425 -4.1285505 -4.1452708 -4.1510906 -4.1230354 -4.0823622 -4.0770488 -4.09818 -4.1118507 -4.1123533 -4.0911222 -4.0698285][-4.1439662 -4.144578 -4.1378422 -4.1484861 -4.1590919 -4.1590724 -4.1582017 -4.1412525 -4.1232924 -4.1265016 -4.1425042 -4.1419911 -4.1324677 -4.1189246 -4.1130662][-4.1110291 -4.1219525 -4.1271615 -4.1452374 -4.1534739 -4.1432457 -4.14524 -4.14928 -4.1521516 -4.1594934 -4.1693335 -4.1646924 -4.1601849 -4.1605387 -4.16073][-4.0671029 -4.0800366 -4.0857754 -4.0939064 -4.0963769 -4.0883865 -4.0966005 -4.1077394 -4.1152864 -4.1219115 -4.1347227 -4.138804 -4.1465006 -4.1592278 -4.1604662][-4.0537496 -4.0468688 -4.0320792 -4.0178928 -4.0065141 -4.0014958 -4.0054526 -4.0095935 -4.0106978 -4.0097933 -4.0279045 -4.0558429 -4.0833421 -4.1061163 -4.1095171][-4.0770044 -4.04758 -4.004046 -3.9714892 -3.9553773 -3.9487419 -3.9403348 -3.9319351 -3.9200516 -3.907434 -3.9272163 -3.9710846 -4.0036197 -4.0155482 -4.0061169][-4.0852056 -4.060266 -4.0212789 -3.9999774 -3.9990804 -3.9948797 -3.9758208 -3.9570956 -3.9420524 -3.9273694 -3.9390504 -3.9688053 -3.9798074 -3.9633226 -3.9375782][-4.0779333 -4.0720377 -4.0591254 -4.0599465 -4.0731368 -4.0699253 -4.048038 -4.028018 -4.0186062 -4.0175343 -4.0279574 -4.0394721 -4.0297804 -4.0045323 -3.9850113][-4.0811262 -4.0898428 -4.094696 -4.107831 -4.1208286 -4.1127887 -4.0945187 -4.0844307 -4.0833926 -4.0906053 -4.1020622 -4.1005044 -4.0819526 -4.066328 -4.0643616][-4.1036344 -4.1152306 -4.123549 -4.1327138 -4.134409 -4.1239915 -4.1150646 -4.115695 -4.1168723 -4.1236248 -4.129086 -4.1200304 -4.1018114 -4.0989895 -4.1068916][-4.1238689 -4.1348877 -4.1358628 -4.1386666 -4.1330709 -4.1250682 -4.1233959 -4.1262674 -4.1228533 -4.1211557 -4.1257458 -4.1241879 -4.1142664 -4.1165805 -4.1223135][-4.1243253 -4.1270208 -4.1184583 -4.1183209 -4.113647 -4.1141829 -4.1202803 -4.1256 -4.1183081 -4.1107578 -4.12094 -4.1365829 -4.1392441 -4.143127 -4.1393652][-4.1039758 -4.1004548 -4.0833125 -4.07287 -4.0732765 -4.08424 -4.0933275 -4.10529 -4.1036634 -4.0988793 -4.1151838 -4.1449242 -4.1566453 -4.1617956 -4.1577244]]...]
INFO - root - 2017-12-05 14:00:11.014487: step 14910, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 77h:24m:28s remains)
INFO - root - 2017-12-05 14:00:19.558741: step 14920, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 76h:04m:11s remains)
INFO - root - 2017-12-05 14:00:28.144630: step 14930, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 74h:18m:52s remains)
INFO - root - 2017-12-05 14:00:36.768527: step 14940, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 75h:57m:02s remains)
INFO - root - 2017-12-05 14:00:45.316090: step 14950, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.856 sec/batch; 75h:27m:47s remains)
INFO - root - 2017-12-05 14:00:53.785764: step 14960, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 76h:59m:33s remains)
INFO - root - 2017-12-05 14:01:02.420273: step 14970, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 74h:30m:40s remains)
INFO - root - 2017-12-05 14:01:10.970827: step 14980, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 75h:47m:48s remains)
INFO - root - 2017-12-05 14:01:19.485148: step 14990, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 75h:29m:47s remains)
INFO - root - 2017-12-05 14:01:27.983759: step 15000, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 74h:45m:02s remains)
2017-12-05 14:01:28.720299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2448592 -4.2483106 -4.2486324 -4.2452712 -4.2372541 -4.2326736 -4.2327361 -4.2336769 -4.2376709 -4.2399535 -4.2251959 -4.2046189 -4.195045 -4.201509 -4.2127938][-4.2563524 -4.2613173 -4.2630267 -4.2622018 -4.2555623 -4.2509542 -4.2490954 -4.248704 -4.2507138 -4.2481284 -4.2270308 -4.2000113 -4.1879015 -4.1972303 -4.2127542][-4.2549267 -4.2618561 -4.2630095 -4.2629709 -4.25803 -4.2553859 -4.2555585 -4.2565451 -4.2584252 -4.2556691 -4.2353892 -4.2070746 -4.1915441 -4.1972837 -4.2099648][-4.23798 -4.2451639 -4.2450194 -4.2451563 -4.2409816 -4.2398891 -4.2426763 -4.2445951 -4.2480574 -4.2496634 -4.2381797 -4.2185893 -4.2082877 -4.2123146 -4.2211523][-4.2135849 -4.2176204 -4.2147923 -4.2106781 -4.1995974 -4.194356 -4.1978712 -4.2039347 -4.2164164 -4.2278767 -4.2284322 -4.2191353 -4.2176366 -4.2241368 -4.2332726][-4.19933 -4.197309 -4.1907659 -4.1796393 -4.1559734 -4.1325588 -4.1216598 -4.1295538 -4.1591921 -4.1878934 -4.2022686 -4.2034497 -4.20468 -4.2110262 -4.219964][-4.19721 -4.1884909 -4.1748943 -4.1517787 -4.10788 -4.0505633 -4.0036964 -4.0010881 -4.0506706 -4.1038413 -4.1397762 -4.1581125 -4.1645017 -4.1693683 -4.1811271][-4.1882248 -4.175714 -4.15527 -4.1208339 -4.0586271 -3.9677317 -3.8874068 -3.8744793 -3.94598 -4.024724 -4.0808835 -4.1131854 -4.1221814 -4.1252084 -4.1413417][-4.178328 -4.1658244 -4.1480322 -4.1206703 -4.0658221 -3.9816716 -3.907352 -3.8960958 -3.959147 -4.0316534 -4.0856895 -4.1134138 -4.1102233 -4.1058645 -4.1197433][-4.1857309 -4.1780934 -4.1732812 -4.1669106 -4.13868 -4.0871797 -4.0391617 -4.02578 -4.05623 -4.1014872 -4.14139 -4.1581621 -4.1453123 -4.1337595 -4.140008][-4.2055078 -4.1964116 -4.1972995 -4.2053409 -4.2019262 -4.1816821 -4.1559615 -4.1416488 -4.1511192 -4.1753216 -4.1999774 -4.2109251 -4.2000027 -4.1876464 -4.1877732][-4.2259007 -4.21585 -4.2189736 -4.2346363 -4.245718 -4.2440267 -4.233633 -4.2261305 -4.227572 -4.2370596 -4.2481041 -4.2546496 -4.2477231 -4.238009 -4.23549][-4.2350788 -4.2282753 -4.2332716 -4.2510071 -4.2664342 -4.271872 -4.2700806 -4.2682214 -4.2680912 -4.270226 -4.273438 -4.2769089 -4.2734966 -4.2670856 -4.2632809][-4.2188096 -4.2184653 -4.2281513 -4.2472749 -4.2633629 -4.27048 -4.2716355 -4.2707286 -4.269073 -4.26891 -4.2716928 -4.2773457 -4.2799067 -4.277669 -4.2733483][-4.18071 -4.1889329 -4.20638 -4.2294064 -4.2471032 -4.2554317 -4.257112 -4.255249 -4.2517438 -4.2511415 -4.2548356 -4.2632952 -4.2712669 -4.274529 -4.2736363]]...]
INFO - root - 2017-12-05 14:01:37.129730: step 15010, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.815 sec/batch; 71h:50m:33s remains)
INFO - root - 2017-12-05 14:01:45.647373: step 15020, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 77h:41m:06s remains)
INFO - root - 2017-12-05 14:01:54.243419: step 15030, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 72h:59m:22s remains)
INFO - root - 2017-12-05 14:02:02.777153: step 15040, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 74h:10m:52s remains)
INFO - root - 2017-12-05 14:02:11.340251: step 15050, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 75h:03m:01s remains)
INFO - root - 2017-12-05 14:02:19.930860: step 15060, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 74h:17m:46s remains)
INFO - root - 2017-12-05 14:02:28.437782: step 15070, loss = 2.12, batch loss = 2.06 (9.1 examples/sec; 0.882 sec/batch; 77h:45m:02s remains)
INFO - root - 2017-12-05 14:02:37.037882: step 15080, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 76h:00m:26s remains)
INFO - root - 2017-12-05 14:02:45.578706: step 15090, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.856 sec/batch; 75h:29m:43s remains)
INFO - root - 2017-12-05 14:02:54.210515: step 15100, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 76h:02m:05s remains)
2017-12-05 14:02:55.025145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3288417 -4.312305 -4.290319 -4.2657294 -4.2413712 -4.2203255 -4.2033181 -4.2000508 -4.21647 -4.2454967 -4.2685881 -4.2766347 -4.2797771 -4.2777386 -4.2684808][-4.3128805 -4.291749 -4.2665009 -4.2424421 -4.2166576 -4.1833086 -4.149292 -4.1348948 -4.1579804 -4.2040772 -4.2421389 -4.2566757 -4.2564535 -4.2495637 -4.236557][-4.2939157 -4.2699423 -4.2433238 -4.2209144 -4.195334 -4.1520605 -4.0960646 -4.0687175 -4.0998244 -4.1653557 -4.2172251 -4.2433438 -4.2459288 -4.2362204 -4.21989][-4.2712 -4.2441568 -4.2154922 -4.1884832 -4.1606517 -4.1114845 -4.0363336 -3.9876461 -4.0270224 -4.1123285 -4.17991 -4.22234 -4.2377343 -4.2345724 -4.2190456][-4.2473884 -4.2175474 -4.1844091 -4.1503572 -4.1190481 -4.0646396 -3.9765143 -3.9003735 -3.9324713 -4.0372925 -4.1278381 -4.1917644 -4.2233033 -4.233717 -4.2246714][-4.2299938 -4.1993756 -4.1650553 -4.1277184 -4.0900807 -4.0277863 -3.9269547 -3.8297391 -3.8435874 -3.9605596 -4.0753021 -4.1611257 -4.2065349 -4.2273264 -4.2258706][-4.2196994 -4.1905441 -4.1596389 -4.1231718 -4.0782242 -4.005054 -3.8942676 -3.7871211 -3.784621 -3.8958883 -4.0214596 -4.1210246 -4.1761723 -4.2056351 -4.2148366][-4.2131171 -4.1901608 -4.1682749 -4.1430655 -4.0988708 -4.0224185 -3.9159048 -3.8215787 -3.8127449 -3.8900898 -3.99766 -4.0911274 -4.1477642 -4.184876 -4.2036805][-4.2129765 -4.20043 -4.1900043 -4.1783772 -4.1437035 -4.0794854 -3.9941919 -3.9267731 -3.9177246 -3.9541879 -4.0293775 -4.1044645 -4.1564994 -4.190846 -4.2103353][-4.2179785 -4.2172542 -4.2144127 -4.2071238 -4.17907 -4.1260228 -4.0610228 -4.0189328 -4.01713 -4.0346646 -4.0890169 -4.1495728 -4.1926293 -4.2189384 -4.2354465][-4.2265759 -4.2330985 -4.2341385 -4.227387 -4.2073851 -4.1634622 -4.1093369 -4.07973 -4.0858488 -4.1021395 -4.1472063 -4.1985478 -4.2342954 -4.2547631 -4.2667613][-4.2380819 -4.24598 -4.2483048 -4.2464471 -4.2367477 -4.1979451 -4.148582 -4.1292386 -4.1440496 -4.1624675 -4.1938229 -4.23546 -4.2678847 -4.2886062 -4.2969341][-4.2450194 -4.2524848 -4.2547665 -4.2547431 -4.2493162 -4.2162523 -4.1761804 -4.1677642 -4.1878152 -4.2040324 -4.2232375 -4.2577419 -4.2898927 -4.3114982 -4.3164716][-4.2494011 -4.2537456 -4.2557483 -4.2559242 -4.2523704 -4.2278447 -4.2016497 -4.2015567 -4.2198825 -4.2324481 -4.2487988 -4.2755232 -4.3011456 -4.3174095 -4.3194056][-4.2655783 -4.2632465 -4.2630844 -4.26112 -4.2568545 -4.2427225 -4.2285991 -4.2292342 -4.2419724 -4.2547894 -4.2720394 -4.292522 -4.3073688 -4.3164697 -4.3165426]]...]
INFO - root - 2017-12-05 14:03:03.321874: step 15110, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 72h:51m:50s remains)
INFO - root - 2017-12-05 14:03:11.814015: step 15120, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 74h:33m:03s remains)
INFO - root - 2017-12-05 14:03:20.405967: step 15130, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 76h:26m:14s remains)
INFO - root - 2017-12-05 14:03:28.958583: step 15140, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 72h:39m:25s remains)
INFO - root - 2017-12-05 14:03:37.594352: step 15150, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 75h:43m:08s remains)
INFO - root - 2017-12-05 14:03:46.139706: step 15160, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 74h:57m:36s remains)
INFO - root - 2017-12-05 14:03:54.609676: step 15170, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 73h:48m:21s remains)
INFO - root - 2017-12-05 14:04:03.104243: step 15180, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.829 sec/batch; 73h:06m:56s remains)
INFO - root - 2017-12-05 14:04:11.594054: step 15190, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 75h:34m:21s remains)
INFO - root - 2017-12-05 14:04:20.148966: step 15200, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 75h:34m:12s remains)
2017-12-05 14:04:20.875357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2994409 -4.2816753 -4.2453971 -4.2131305 -4.2023921 -4.2025371 -4.2050719 -4.2031045 -4.1957645 -4.1941957 -4.2106676 -4.2319846 -4.253047 -4.2776823 -4.2996926][-4.2938757 -4.2779026 -4.244987 -4.2156467 -4.2051225 -4.2026567 -4.2030191 -4.1991348 -4.1945982 -4.197226 -4.2169433 -4.23855 -4.2583532 -4.2778583 -4.2977653][-4.2706356 -4.264977 -4.243494 -4.2172623 -4.2034354 -4.1981115 -4.1931567 -4.1842847 -4.1789994 -4.1843858 -4.2031641 -4.22325 -4.2448068 -4.2664957 -4.2892146][-4.2422705 -4.246984 -4.2363238 -4.2123251 -4.1942167 -4.1849155 -4.1725121 -4.1567655 -4.1463189 -4.1481981 -4.164093 -4.1861897 -4.2156725 -4.24941 -4.2791438][-4.2156515 -4.2246094 -4.2223768 -4.2017765 -4.1821404 -4.1689973 -4.1503081 -4.125988 -4.1031647 -4.0916052 -4.1016393 -4.1260624 -4.1737418 -4.2292075 -4.2726774][-4.1920061 -4.2021561 -4.2069163 -4.1938372 -4.1774549 -4.1628585 -4.1354651 -4.1000423 -4.0625458 -4.0356908 -4.0440788 -4.0784326 -4.1463218 -4.2216654 -4.2752023][-4.1681676 -4.1773233 -4.18707 -4.1814966 -4.1738911 -4.1645217 -4.1363616 -4.0927916 -4.041348 -4.0047183 -4.0172186 -4.0664277 -4.1503348 -4.2332072 -4.2858553][-4.1436272 -4.1579556 -4.1739864 -4.1751394 -4.173388 -4.168592 -4.1386352 -4.0869093 -4.0280027 -3.9920659 -4.0127606 -4.0796976 -4.1711497 -4.2512407 -4.2955985][-4.1278081 -4.1518059 -4.1745987 -4.1797504 -4.1766634 -4.1642661 -4.1259255 -4.0650187 -4.0088139 -3.9873779 -4.0262432 -4.1059504 -4.19446 -4.2654858 -4.3030429][-4.1457109 -4.1727157 -4.1949081 -4.1960597 -4.1820884 -4.1561966 -4.11266 -4.0578251 -4.019311 -4.02567 -4.0795937 -4.1543555 -4.2240071 -4.2780323 -4.3074465][-4.186379 -4.2109909 -4.2261953 -4.2157378 -4.1866097 -4.1506705 -4.1116009 -4.0734425 -4.0581379 -4.0844908 -4.1444964 -4.2074094 -4.2577114 -4.2952662 -4.3142533][-4.2283254 -4.2449522 -4.2499437 -4.2301712 -4.1972342 -4.1662636 -4.1381769 -4.1126094 -4.1082325 -4.1432362 -4.20206 -4.2542782 -4.2902484 -4.3168492 -4.3248954][-4.2764268 -4.2821364 -4.277236 -4.255414 -4.2311659 -4.2128696 -4.1958075 -4.179152 -4.1770663 -4.2086697 -4.2583079 -4.296711 -4.3220549 -4.3372455 -4.3356657][-4.3167963 -4.3124475 -4.2988834 -4.27707 -4.2594867 -4.2497134 -4.2435789 -4.239017 -4.2441883 -4.2732005 -4.3105087 -4.3347387 -4.3475842 -4.3503428 -4.3413863][-4.337338 -4.3275518 -4.3127975 -4.2952719 -4.2798743 -4.27345 -4.2744355 -4.2787514 -4.2919636 -4.3169417 -4.3401842 -4.3524671 -4.3557463 -4.3505559 -4.3391027]]...]
INFO - root - 2017-12-05 14:04:29.344124: step 15210, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 73h:53m:37s remains)
INFO - root - 2017-12-05 14:04:37.801178: step 15220, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 74h:48m:42s remains)
INFO - root - 2017-12-05 14:04:46.352414: step 15230, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 75h:30m:26s remains)
INFO - root - 2017-12-05 14:04:54.937949: step 15240, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 75h:53m:36s remains)
INFO - root - 2017-12-05 14:05:03.517257: step 15250, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 77h:07m:31s remains)
INFO - root - 2017-12-05 14:05:12.083805: step 15260, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 76h:05m:07s remains)
INFO - root - 2017-12-05 14:05:20.750731: step 15270, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 75h:55m:29s remains)
INFO - root - 2017-12-05 14:05:29.206347: step 15280, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 74h:54m:17s remains)
INFO - root - 2017-12-05 14:05:37.720127: step 15290, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 74h:10m:01s remains)
INFO - root - 2017-12-05 14:05:46.287356: step 15300, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 76h:57m:05s remains)
2017-12-05 14:05:47.046872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1744142 -4.1655836 -4.149951 -4.1407862 -4.159112 -4.1755309 -4.1729226 -4.1620669 -4.14526 -4.1127295 -4.0750327 -4.0574059 -4.0719204 -4.0798082 -4.0670285][-4.1637344 -4.1600747 -4.1452694 -4.1328864 -4.1500244 -4.1682577 -4.1674843 -4.1595659 -4.1470752 -4.1146502 -4.0764942 -4.0681448 -4.1031146 -4.1302304 -4.1364961][-4.1580958 -4.1565685 -4.1446886 -4.1317296 -4.1456866 -4.1626887 -4.1622353 -4.1524 -4.1408677 -4.1083331 -4.0725861 -4.0680866 -4.1079106 -4.1456504 -4.1698475][-4.1516681 -4.1483326 -4.1356735 -4.1194105 -4.1315117 -4.150733 -4.15422 -4.1439095 -4.1305194 -4.0993309 -4.0628715 -4.0561566 -4.0930634 -4.1345644 -4.1720972][-4.1214428 -4.1176739 -4.1026039 -4.0892797 -4.106935 -4.1294675 -4.1369381 -4.1280041 -4.1205454 -4.097918 -4.0660615 -4.0571136 -4.091629 -4.1378675 -4.1812272][-4.082181 -4.07756 -4.0599709 -4.0555224 -4.0775576 -4.096858 -4.0970769 -4.0893235 -4.0931311 -4.0881023 -4.0742445 -4.0728664 -4.10912 -4.16098 -4.2074122][-4.055655 -4.0547109 -4.0341339 -4.0338335 -4.0544643 -4.0591321 -4.0391707 -4.0236235 -4.0420084 -4.0660319 -4.0816197 -4.0929427 -4.1327477 -4.1870751 -4.2327256][-4.0330634 -4.0390921 -4.0171432 -4.0153275 -4.0347185 -4.0296912 -3.9913011 -3.9623959 -3.9892981 -4.0449052 -4.0918393 -4.1161847 -4.1518712 -4.2032084 -4.2448168][-4.0219722 -4.0306711 -4.0040064 -3.9960213 -4.0122762 -4.0070677 -3.9688566 -3.9381766 -3.9665732 -4.0365472 -4.098556 -4.1269121 -4.1536994 -4.1999841 -4.237535][-4.0267935 -4.03308 -4.005219 -3.996491 -4.0123496 -4.0086856 -3.98476 -3.9725235 -4.0046692 -4.0694141 -4.1250706 -4.1444855 -4.1608462 -4.1952715 -4.2214813][-4.0683517 -4.0705953 -4.042047 -4.0302167 -4.0448136 -4.0453076 -4.0384932 -4.0462122 -4.0802188 -4.1297765 -4.1683192 -4.1732121 -4.1759505 -4.1923161 -4.2014704][-4.1228433 -4.1237764 -4.1005745 -4.090703 -4.1047444 -4.1093554 -4.1131406 -4.1298285 -4.1599216 -4.1930842 -4.2127719 -4.2027431 -4.1938891 -4.1941376 -4.1906991][-4.1668739 -4.1705928 -4.1588035 -4.1560392 -4.1693 -4.1758013 -4.1827574 -4.2002635 -4.2237735 -4.2427969 -4.2487087 -4.2313523 -4.217351 -4.2093325 -4.201447][-4.18196 -4.1881924 -4.186141 -4.191051 -4.2041645 -4.2128515 -4.2223306 -4.241941 -4.2623873 -4.2749281 -4.2732453 -4.2566705 -4.2430544 -4.2340255 -4.2287741][-4.1792483 -4.1844807 -4.1901889 -4.20086 -4.2157559 -4.2286863 -4.2418551 -4.2607765 -4.279459 -4.289423 -4.2862973 -4.2754059 -4.26666 -4.2606621 -4.2580495]]...]
INFO - root - 2017-12-05 14:05:55.563548: step 15310, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 78h:12m:52s remains)
INFO - root - 2017-12-05 14:06:04.131641: step 15320, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 77h:23m:56s remains)
INFO - root - 2017-12-05 14:06:12.696317: step 15330, loss = 2.01, batch loss = 1.95 (9.2 examples/sec; 0.869 sec/batch; 76h:31m:14s remains)
INFO - root - 2017-12-05 14:06:21.364777: step 15340, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 75h:42m:00s remains)
INFO - root - 2017-12-05 14:06:29.867577: step 15350, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 74h:10m:35s remains)
INFO - root - 2017-12-05 14:06:38.346118: step 15360, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 74h:33m:44s remains)
INFO - root - 2017-12-05 14:06:47.039206: step 15370, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 73h:23m:59s remains)
INFO - root - 2017-12-05 14:06:55.622737: step 15380, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.894 sec/batch; 78h:42m:56s remains)
INFO - root - 2017-12-05 14:07:03.983674: step 15390, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.871 sec/batch; 76h:41m:54s remains)
INFO - root - 2017-12-05 14:07:12.572627: step 15400, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 74h:48m:02s remains)
2017-12-05 14:07:13.377941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3317142 -4.3286695 -4.3236012 -4.3164959 -4.3092732 -4.3039088 -4.2992368 -4.296566 -4.3003445 -4.3103976 -4.3230081 -4.3303542 -4.3281126 -4.3152986 -4.289063][-4.3281264 -4.3259683 -4.3216219 -4.3126683 -4.3050666 -4.2972794 -4.2873487 -4.2791348 -4.2803531 -4.290031 -4.3040509 -4.31364 -4.3145533 -4.3035235 -4.2680392][-4.3285055 -4.3283567 -4.3245 -4.3152947 -4.3065004 -4.2912507 -4.2695951 -4.2527752 -4.25309 -4.2651577 -4.2810807 -4.2933183 -4.2967248 -4.285861 -4.2371368][-4.3310862 -4.3300953 -4.3240247 -4.310533 -4.2967811 -4.2697625 -4.2315693 -4.2000823 -4.19917 -4.219264 -4.2460876 -4.2663751 -4.271575 -4.2552257 -4.1857038][-4.3303876 -4.3254504 -4.3153744 -4.3003311 -4.2824216 -4.2401547 -4.1794152 -4.123487 -4.1184173 -4.1592808 -4.2098541 -4.2419143 -4.2497506 -4.2249532 -4.1411762][-4.3242369 -4.3136826 -4.2995048 -4.2826452 -4.2583556 -4.196104 -4.1009674 -4.0050278 -3.99928 -4.0771985 -4.1635566 -4.2173667 -4.2357616 -4.209868 -4.1340046][-4.311964 -4.2960596 -4.2762594 -4.2546749 -4.2169743 -4.1268711 -3.986191 -3.8441875 -3.8510633 -3.9836681 -4.1151381 -4.1970716 -4.2310061 -4.2144318 -4.1605334][-4.3007188 -4.2803264 -4.2541885 -4.2241635 -4.1712084 -4.0646043 -3.9038498 -3.746552 -3.7673678 -3.9345989 -4.0975523 -4.1943088 -4.2326655 -4.2273154 -4.1980648][-4.2951059 -4.2727218 -4.2423882 -4.2057123 -4.1490965 -4.0588756 -3.9437249 -3.8504338 -3.8755951 -4.0042467 -4.136622 -4.2158961 -4.2476559 -4.249835 -4.2335005][-4.2958417 -4.2758875 -4.2513986 -4.220324 -4.1706047 -4.1079473 -4.0460148 -4.0125871 -4.0396247 -4.1148653 -4.1981025 -4.2539725 -4.2833633 -4.2861738 -4.263061][-4.3068461 -4.2938652 -4.2769508 -4.252975 -4.213717 -4.1728282 -4.1431284 -4.1400104 -4.1657481 -4.2089133 -4.2552266 -4.28897 -4.3087168 -4.2981949 -4.254961][-4.3195863 -4.3136344 -4.3011141 -4.2817788 -4.25415 -4.2288322 -4.2180676 -4.22821 -4.2501788 -4.2763596 -4.3020868 -4.3200846 -4.3217564 -4.2836857 -4.213058][-4.3297009 -4.3283916 -4.3213749 -4.3052564 -4.27923 -4.25571 -4.2499208 -4.2655778 -4.2884393 -4.3086123 -4.3218274 -4.3206277 -4.2951074 -4.220118 -4.1101232][-4.3250008 -4.326879 -4.3190308 -4.2961349 -4.258954 -4.224164 -4.2143979 -4.2346983 -4.2697635 -4.2970448 -4.3061209 -4.2972589 -4.2488713 -4.1354561 -3.9770973][-4.3095355 -4.3142519 -4.305459 -4.2700443 -4.2128024 -4.1523848 -4.1270609 -4.1585941 -4.2160463 -4.2583737 -4.275661 -4.2651105 -4.209435 -4.0869632 -3.9273055]]...]
INFO - root - 2017-12-05 14:07:21.775151: step 15410, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 75h:04m:30s remains)
INFO - root - 2017-12-05 14:07:30.350819: step 15420, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 74h:15m:37s remains)
INFO - root - 2017-12-05 14:07:38.876448: step 15430, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 76h:24m:47s remains)
INFO - root - 2017-12-05 14:07:47.287925: step 15440, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 73h:50m:04s remains)
INFO - root - 2017-12-05 14:07:55.903397: step 15450, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 77h:16m:21s remains)
INFO - root - 2017-12-05 14:08:04.448699: step 15460, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 73h:10m:48s remains)
INFO - root - 2017-12-05 14:08:13.020905: step 15470, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.874 sec/batch; 77h:00m:21s remains)
INFO - root - 2017-12-05 14:08:21.553933: step 15480, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 73h:57m:41s remains)
INFO - root - 2017-12-05 14:08:30.107952: step 15490, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 75h:43m:27s remains)
INFO - root - 2017-12-05 14:08:38.496697: step 15500, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 75h:03m:41s remains)
2017-12-05 14:08:39.251731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.10579 -4.1128297 -4.1148748 -4.1219559 -4.1267166 -4.1285629 -4.1502638 -4.1795087 -4.1862168 -4.1972804 -4.2217355 -4.2364984 -4.2320433 -4.2231331 -4.2211418][-4.0782237 -4.080873 -4.0907717 -4.1172266 -4.1394548 -4.1526589 -4.1776037 -4.2057714 -4.2085547 -4.2068152 -4.2208853 -4.2324057 -4.2255607 -4.2173052 -4.2161231][-4.0890737 -4.0951333 -4.1169105 -4.1583519 -4.1882105 -4.2003055 -4.2152719 -4.2278514 -4.2193065 -4.2066917 -4.2122936 -4.2211547 -4.2144351 -4.2152553 -4.2212191][-4.1340609 -4.1478415 -4.1749396 -4.20976 -4.2274027 -4.2269764 -4.218504 -4.2070303 -4.1917176 -4.1798444 -4.1865635 -4.1950412 -4.1876225 -4.1943936 -4.2109966][-4.1878624 -4.20165 -4.2229266 -4.2414651 -4.2405457 -4.2160478 -4.1712818 -4.1337805 -4.1247144 -4.1279168 -4.1429005 -4.1588097 -4.1581774 -4.1725645 -4.2018666][-4.2356982 -4.237947 -4.236783 -4.230505 -4.2046022 -4.1469865 -4.0538511 -3.9877441 -4.0061617 -4.0451217 -4.0740023 -4.1105576 -4.132338 -4.1626015 -4.2089992][-4.2548661 -4.2430949 -4.222805 -4.1930041 -4.1425476 -4.0508504 -3.9019074 -3.794558 -3.8490069 -3.9436424 -4.0009518 -4.0649796 -4.1170435 -4.1653538 -4.2278924][-4.257535 -4.2370529 -4.2023649 -4.1512384 -4.0791979 -3.9697974 -3.8038387 -3.6908154 -3.7735536 -3.9019175 -3.9862764 -4.0723882 -4.1453629 -4.2025151 -4.2614932][-4.2565489 -4.2296576 -4.1905804 -4.1287284 -4.0528712 -3.9658771 -3.8527963 -3.7872896 -3.8625662 -3.9680049 -4.0478077 -4.1376958 -4.2171211 -4.2697082 -4.3068895][-4.2622943 -4.2322989 -4.19502 -4.1363106 -4.07409 -4.02384 -3.9726107 -3.9561799 -4.02037 -4.0939541 -4.1565866 -4.2341542 -4.2960596 -4.327774 -4.3387561][-4.265769 -4.2269535 -4.1836071 -4.1348891 -4.0934896 -4.0766368 -4.0709558 -4.0917358 -4.153276 -4.2050657 -4.2430439 -4.2916646 -4.3270249 -4.3401895 -4.3364415][-4.245265 -4.18901 -4.1293211 -4.0827451 -4.0667686 -4.0845194 -4.1153479 -4.1643481 -4.2274733 -4.2630486 -4.2781367 -4.2962165 -4.3070207 -4.3070464 -4.3011312][-4.2035379 -4.1338081 -4.0592208 -4.0134811 -4.0288329 -4.0819707 -4.1410828 -4.2027164 -4.2559037 -4.2784052 -4.2731748 -4.2646923 -4.2532516 -4.2462916 -4.2430577][-4.170959 -4.0986953 -4.0224876 -3.9839945 -4.0180063 -4.092134 -4.1617732 -4.2197714 -4.2547569 -4.2592177 -4.237813 -4.2113957 -4.1836514 -4.17783 -4.1848512][-4.1839027 -4.1269321 -4.0636892 -4.0307932 -4.0615826 -4.1313152 -4.1937623 -4.2309885 -4.2408061 -4.2257514 -4.19104 -4.1568041 -4.124393 -4.1266818 -4.1481256]]...]
INFO - root - 2017-12-05 14:08:47.749827: step 15510, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.899 sec/batch; 79h:07m:56s remains)
INFO - root - 2017-12-05 14:08:56.532103: step 15520, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 74h:56m:17s remains)
INFO - root - 2017-12-05 14:09:05.289932: step 15530, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 76h:53m:11s remains)
INFO - root - 2017-12-05 14:09:13.906611: step 15540, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 76h:12m:26s remains)
INFO - root - 2017-12-05 14:09:22.372924: step 15550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 75h:34m:30s remains)
INFO - root - 2017-12-05 14:09:30.920960: step 15560, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 73h:36m:12s remains)
INFO - root - 2017-12-05 14:09:39.458595: step 15570, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 76h:03m:45s remains)
INFO - root - 2017-12-05 14:09:47.973881: step 15580, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.870 sec/batch; 76h:35m:20s remains)
INFO - root - 2017-12-05 14:09:56.566702: step 15590, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.872 sec/batch; 76h:46m:39s remains)
INFO - root - 2017-12-05 14:10:04.993278: step 15600, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.874 sec/batch; 76h:58m:21s remains)
2017-12-05 14:10:05.743115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2256026 -4.2454467 -4.2518096 -4.2297015 -4.2078481 -4.2187319 -4.2504215 -4.2741017 -4.268003 -4.2468061 -4.2238154 -4.2060466 -4.1992598 -4.2098856 -4.2407827][-4.2152724 -4.2366023 -4.2435637 -4.2206779 -4.1925693 -4.2002821 -4.2338867 -4.2598643 -4.2578259 -4.2387362 -4.2196536 -4.2062011 -4.2013011 -4.2129951 -4.2405548][-4.2194667 -4.2379308 -4.2395396 -4.2113609 -4.1787443 -4.1834135 -4.2181625 -4.245388 -4.2462168 -4.2325039 -4.2206845 -4.2137666 -4.2123656 -4.2250757 -4.2494717][-4.2323527 -4.2463403 -4.2388568 -4.2010841 -4.1599145 -4.1564713 -4.1895471 -4.2185259 -4.225862 -4.2207456 -4.2196493 -4.2216678 -4.2248163 -4.2383342 -4.2604475][-4.2461715 -4.2592492 -4.2461157 -4.2008324 -4.1505966 -4.1346588 -4.15522 -4.1756 -4.1841421 -4.1871963 -4.1971407 -4.210917 -4.2235823 -4.2432942 -4.266892][-4.2640853 -4.2790642 -4.2645559 -4.2177992 -4.1607456 -4.124712 -4.1178637 -4.1182752 -4.1226544 -4.132112 -4.15585 -4.1850214 -4.211453 -4.2411036 -4.269558][-4.2863026 -4.3016286 -4.2895694 -4.2480879 -4.1886282 -4.1283827 -4.0793982 -4.0467634 -4.0394344 -4.0552416 -4.0993524 -4.1490226 -4.1894794 -4.2267475 -4.2608557][-4.3071289 -4.3198905 -4.3103123 -4.2776737 -4.2246723 -4.1549478 -4.0752139 -4.0099831 -3.9858479 -4.0031443 -4.0588861 -4.1211071 -4.1690836 -4.2124906 -4.2518435][-4.3181744 -4.3296518 -4.3243508 -4.3018579 -4.2605605 -4.2021728 -4.1269588 -4.059422 -4.0261517 -4.032948 -4.0746918 -4.1281943 -4.1756983 -4.2224679 -4.2642727][-4.3173633 -4.3284926 -4.3271828 -4.313581 -4.2842965 -4.2415528 -4.1883006 -4.1414213 -4.1169238 -4.1130447 -4.1292768 -4.1582694 -4.1941133 -4.2377696 -4.278439][-4.3081894 -4.3178935 -4.3183756 -4.3111825 -4.2907686 -4.2605753 -4.2271695 -4.2034559 -4.1950884 -4.1912289 -4.1943889 -4.2047663 -4.223875 -4.2531009 -4.286274][-4.2953095 -4.3014817 -4.3011723 -4.294703 -4.2777638 -4.2542019 -4.2348003 -4.2290244 -4.2366562 -4.2394452 -4.2414279 -4.2455664 -4.25384 -4.2711949 -4.2952991][-4.2895975 -4.2888927 -4.28168 -4.2705555 -4.2524204 -4.2299232 -4.2168913 -4.2210274 -4.2412338 -4.2518039 -4.2578073 -4.2619 -4.2680378 -4.2811046 -4.2991562][-4.2946863 -4.2858062 -4.2673845 -4.246088 -4.2203312 -4.1925898 -4.1777735 -4.1867352 -4.2176871 -4.2375069 -4.2506061 -4.2583313 -4.2658887 -4.2779164 -4.2924204][-4.2993636 -4.2818017 -4.2499809 -4.2146282 -4.1771173 -4.144237 -4.1269288 -4.1374846 -4.17739 -4.2108631 -4.235343 -4.2488279 -4.2575741 -4.2681503 -4.2818136]]...]
INFO - root - 2017-12-05 14:10:14.377580: step 15610, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 76h:04m:13s remains)
INFO - root - 2017-12-05 14:10:22.980712: step 15620, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 76h:52m:53s remains)
INFO - root - 2017-12-05 14:10:31.482190: step 15630, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 74h:03m:46s remains)
INFO - root - 2017-12-05 14:10:39.985784: step 15640, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 76h:39m:46s remains)
INFO - root - 2017-12-05 14:10:48.550305: step 15650, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 74h:54m:32s remains)
INFO - root - 2017-12-05 14:10:56.972978: step 15660, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 75h:20m:03s remains)
INFO - root - 2017-12-05 14:11:05.573219: step 15670, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 76h:16m:59s remains)
INFO - root - 2017-12-05 14:11:14.036712: step 15680, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 75h:39m:25s remains)
INFO - root - 2017-12-05 14:11:22.579118: step 15690, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 75h:46m:15s remains)
INFO - root - 2017-12-05 14:11:31.169868: step 15700, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 75h:15m:22s remains)
2017-12-05 14:11:31.938147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3186536 -4.3125482 -4.3083639 -4.3063884 -4.3047442 -4.3039393 -4.303019 -4.300559 -4.2991085 -4.2995472 -4.2997894 -4.3006325 -4.3047819 -4.3120203 -4.3214183][-4.3110523 -4.3016777 -4.2953677 -4.2925186 -4.2902036 -4.2886767 -4.2860365 -4.2816486 -4.2795229 -4.2816257 -4.2803545 -4.2770576 -4.2799768 -4.2909636 -4.3057022][-4.3007545 -4.2862482 -4.2755423 -4.2686143 -4.2621765 -4.2563095 -4.2493324 -4.2434144 -4.2446685 -4.2528329 -4.2531385 -4.2477436 -4.2493634 -4.2622709 -4.2824507][-4.2872124 -4.2637358 -4.2443519 -4.2294865 -4.2163725 -4.203433 -4.1879096 -4.1755729 -4.1837606 -4.20884 -4.2195373 -4.216599 -4.2165155 -4.2295961 -4.2529793][-4.2739496 -4.24129 -4.2133079 -4.1901817 -4.1672616 -4.1380506 -4.1018262 -4.0745139 -4.0928297 -4.1489797 -4.1866994 -4.1947951 -4.195365 -4.2066164 -4.2286515][-4.2654648 -4.22828 -4.1929398 -4.1602435 -4.1208668 -4.0642538 -3.9873381 -3.9195631 -3.9390814 -4.0384364 -4.1189423 -4.1566429 -4.1760483 -4.1946111 -4.2149224][-4.2552538 -4.2158194 -4.1727676 -4.1303015 -4.0738239 -3.9859633 -3.8627639 -3.7340708 -3.7407959 -3.888855 -4.0186052 -4.0905294 -4.1369028 -4.1743279 -4.19827][-4.2454147 -4.203723 -4.1543732 -4.1062202 -4.048666 -3.9572551 -3.8209085 -3.6616979 -3.6450431 -3.8012757 -3.9466448 -4.0333867 -4.0982113 -4.1496959 -4.1818161][-4.2335806 -4.1903505 -4.137434 -4.0889874 -4.0504127 -3.9917941 -3.9023223 -3.7874832 -3.7582743 -3.8537326 -3.956233 -4.02525 -4.0867772 -4.1402617 -4.1783242][-4.2087417 -4.1631269 -4.1126909 -4.072587 -4.0575428 -4.0402112 -4.0052443 -3.948411 -3.9248853 -3.9678128 -4.021452 -4.0617805 -4.1068659 -4.1532035 -4.1922946][-4.1801534 -4.1329746 -4.0896521 -4.0642047 -4.0730839 -4.0902891 -4.0929527 -4.0722694 -4.0574479 -4.0679646 -4.089406 -4.109601 -4.1398196 -4.1763759 -4.2132444][-4.1713281 -4.1276188 -4.0916953 -4.0762277 -4.09769 -4.13108 -4.1514511 -4.1506281 -4.14086 -4.1404128 -4.1510544 -4.1673374 -4.1910377 -4.2193255 -4.2482677][-4.188437 -4.1544452 -4.1315393 -4.1289158 -4.1545849 -4.1876135 -4.2109308 -4.2166815 -4.2099409 -4.208849 -4.2195444 -4.2378335 -4.2577152 -4.2757688 -4.2912669][-4.2318349 -4.2075863 -4.1935935 -4.1986051 -4.2222157 -4.2489252 -4.2698917 -4.2791314 -4.2780843 -4.277669 -4.2839355 -4.2951565 -4.306963 -4.3168826 -4.3221607][-4.2753448 -4.2561512 -4.24652 -4.2532878 -4.2707262 -4.2890754 -4.3031421 -4.3124566 -4.3165927 -4.3183064 -4.32196 -4.327456 -4.3329129 -4.3373308 -4.3375869]]...]
INFO - root - 2017-12-05 14:11:40.275642: step 15710, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 75h:11m:41s remains)
INFO - root - 2017-12-05 14:11:48.771449: step 15720, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 74h:13m:44s remains)
INFO - root - 2017-12-05 14:11:57.213152: step 15730, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 73h:45m:02s remains)
INFO - root - 2017-12-05 14:12:05.682663: step 15740, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 73h:47m:31s remains)
INFO - root - 2017-12-05 14:12:14.104220: step 15750, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 76h:34m:46s remains)
INFO - root - 2017-12-05 14:12:22.541746: step 15760, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 72h:40m:26s remains)
INFO - root - 2017-12-05 14:12:31.091880: step 15770, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 75h:06m:24s remains)
INFO - root - 2017-12-05 14:12:39.667773: step 15780, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.838 sec/batch; 73h:41m:28s remains)
INFO - root - 2017-12-05 14:12:48.081832: step 15790, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 74h:01m:59s remains)
INFO - root - 2017-12-05 14:12:56.507546: step 15800, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 74h:35m:30s remains)
2017-12-05 14:12:57.364958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1260867 -4.1419778 -4.1660628 -4.1939158 -4.2219849 -4.2450266 -4.2563972 -4.2596655 -4.2548943 -4.2435217 -4.2205596 -4.1913748 -4.1807985 -4.1831746 -4.1901016][-4.1382904 -4.1434956 -4.1592231 -4.1836176 -4.2138748 -4.2375736 -4.246202 -4.2462516 -4.2411065 -4.2307906 -4.2103505 -4.1869521 -4.1852703 -4.1946597 -4.2057548][-4.1592345 -4.1549234 -4.1618066 -4.1799283 -4.2090449 -4.22874 -4.2308879 -4.2258859 -4.220932 -4.2164288 -4.2065935 -4.1956472 -4.1996441 -4.2110653 -4.2223735][-4.1880312 -4.1850181 -4.18675 -4.1939878 -4.2117248 -4.2169757 -4.2085838 -4.2012453 -4.20177 -4.2115359 -4.2198124 -4.2229996 -4.2332816 -4.240109 -4.2442932][-4.2111363 -4.2159729 -4.2160225 -4.211771 -4.2088819 -4.1933727 -4.1717014 -4.1628294 -4.1716113 -4.1993074 -4.2286983 -4.2487879 -4.2667327 -4.2699304 -4.263793][-4.2103415 -4.2218676 -4.222178 -4.2089987 -4.1877193 -4.1544371 -4.1164308 -4.1051741 -4.1292706 -4.1823525 -4.2355132 -4.2709827 -4.2920775 -4.2886872 -4.2712865][-4.1967759 -4.2073874 -4.2036362 -4.1825337 -4.1468215 -4.0992284 -4.0536685 -4.0529776 -4.1040039 -4.1817908 -4.2491989 -4.289999 -4.3082604 -4.2955985 -4.2675066][-4.1762471 -4.1748719 -4.1619134 -4.1337233 -4.0925612 -4.0494919 -4.0222635 -4.0488834 -4.1191034 -4.2017107 -4.2647672 -4.2990885 -4.3095255 -4.2915421 -4.2588725][-4.1556187 -4.135077 -4.1093965 -4.0777941 -4.0466452 -4.027173 -4.0306115 -4.0711064 -4.1365504 -4.2033567 -4.2501607 -4.2734542 -4.2767816 -4.2602158 -4.2330718][-4.1463723 -4.1106606 -4.0765843 -4.0467086 -4.0290918 -4.0278831 -4.0442338 -4.0796013 -4.128582 -4.1766253 -4.2115064 -4.2278728 -4.2268777 -4.21409 -4.1962595][-4.1536136 -4.1225247 -4.0952621 -4.073452 -4.0630493 -4.0662255 -4.0761752 -4.09573 -4.1273003 -4.1590691 -4.1796484 -4.186667 -4.1777644 -4.1624331 -4.14893][-4.1675391 -4.1542249 -4.1413536 -4.1297889 -4.1233482 -4.1259556 -4.1309314 -4.1400442 -4.1588545 -4.1753731 -4.1809354 -4.176847 -4.1588774 -4.1365814 -4.1175041][-4.1866717 -4.1896954 -4.1853065 -4.179595 -4.1749177 -4.1760745 -4.1802478 -4.1877432 -4.2025528 -4.2116966 -4.2092333 -4.1966228 -4.172749 -4.1450014 -4.11845][-4.2085028 -4.2144265 -4.20933 -4.2042894 -4.2013807 -4.2036514 -4.2104549 -4.2199903 -4.2329636 -4.2380233 -4.230844 -4.2141457 -4.1901264 -4.1654644 -4.1405377][-4.2281013 -4.2258363 -4.215518 -4.2085953 -4.2071738 -4.2114191 -4.2201724 -4.230464 -4.2400584 -4.2405705 -4.228755 -4.2103257 -4.192749 -4.17786 -4.1623688]]...]
INFO - root - 2017-12-05 14:13:05.788623: step 15810, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 75h:34m:09s remains)
INFO - root - 2017-12-05 14:13:14.144207: step 15820, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 74h:03m:40s remains)
INFO - root - 2017-12-05 14:13:22.589670: step 15830, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 75h:02m:16s remains)
INFO - root - 2017-12-05 14:13:31.127762: step 15840, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 73h:27m:49s remains)
INFO - root - 2017-12-05 14:13:39.739495: step 15850, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 74h:09m:54s remains)
INFO - root - 2017-12-05 14:13:48.300083: step 15860, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 73h:47m:04s remains)
INFO - root - 2017-12-05 14:13:56.791694: step 15870, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 75h:25m:39s remains)
INFO - root - 2017-12-05 14:14:05.218496: step 15880, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 75h:09m:14s remains)
INFO - root - 2017-12-05 14:14:13.639624: step 15890, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 74h:40m:01s remains)
INFO - root - 2017-12-05 14:14:22.060047: step 15900, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 75h:30m:45s remains)
2017-12-05 14:14:22.816267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2277203 -4.2057352 -4.179328 -4.1482892 -4.12814 -4.138093 -4.1611376 -4.1647348 -4.1624408 -4.1698914 -4.1732779 -4.1690059 -4.1591835 -4.1475773 -4.1412296][-4.2606649 -4.2482533 -4.2298136 -4.2052922 -4.1882119 -4.1928091 -4.2112856 -4.211791 -4.20448 -4.2069321 -4.2120996 -4.2183666 -4.2145367 -4.1997638 -4.1857176][-4.2867737 -4.2806339 -4.2686028 -4.2476377 -4.2306519 -4.23006 -4.2390261 -4.2355013 -4.2281327 -4.2274222 -4.2338672 -4.2457118 -4.2483687 -4.23606 -4.2194324][-4.2955666 -4.2906365 -4.2794838 -4.2592421 -4.2421122 -4.2380118 -4.2366142 -4.2280378 -4.2221575 -4.2225738 -4.2320552 -4.248558 -4.2565193 -4.2508407 -4.2376003][-4.2884707 -4.2812381 -4.2669373 -4.247283 -4.2300253 -4.21789 -4.1992407 -4.1793208 -4.1732063 -4.1812363 -4.20246 -4.2256837 -4.2353206 -4.2333751 -4.2232723][-4.2794571 -4.2672882 -4.2473536 -4.2235203 -4.1954274 -4.164783 -4.1200933 -4.0812883 -4.0736661 -4.0959888 -4.1336064 -4.1652169 -4.17696 -4.1782079 -4.1758761][-4.2683692 -4.2478156 -4.2177854 -4.1832671 -4.1408587 -4.0909314 -4.0221219 -3.9639027 -3.9581833 -3.9988132 -4.0532784 -4.0958447 -4.1204553 -4.1313543 -4.1370397][-4.2448931 -4.20993 -4.1634755 -4.11294 -4.0565791 -3.9989064 -3.9291935 -3.8796344 -3.8992579 -3.9663231 -4.0348544 -4.0850568 -4.1151977 -4.1233821 -4.1272583][-4.2119217 -4.1658716 -4.1111636 -4.0527396 -3.9945114 -3.9469283 -3.906682 -3.899354 -3.9462924 -4.01981 -4.0789175 -4.1147809 -4.1301827 -4.1245394 -4.11577][-4.1914244 -4.1413493 -4.0850167 -4.0283709 -3.9843369 -3.9661024 -3.9662087 -3.9896216 -4.0359554 -4.0885105 -4.1272206 -4.1468973 -4.1493239 -4.1366262 -4.1221232][-4.20368 -4.1525612 -4.0965996 -4.0491219 -4.0258188 -4.0356746 -4.0568733 -4.0864964 -4.1184011 -4.1488729 -4.1722317 -4.1854038 -4.18639 -4.1770678 -4.1676855][-4.2468486 -4.2005253 -4.1538696 -4.1215477 -4.113925 -4.1320734 -4.1541295 -4.1772165 -4.1991959 -4.2187719 -4.2353868 -4.2460575 -4.2474856 -4.2410345 -4.2338872][-4.290153 -4.2564712 -4.2268953 -4.2090631 -4.2078953 -4.2234669 -4.241065 -4.2561707 -4.2708211 -4.2826414 -4.291985 -4.2972527 -4.2971873 -4.291429 -4.2851086][-4.3148007 -4.2973037 -4.2850204 -4.2765207 -4.2771139 -4.2882338 -4.2974916 -4.3051782 -4.3121109 -4.317152 -4.3198118 -4.3208928 -4.3193984 -4.314291 -4.3095865][-4.3229108 -4.3138728 -4.3096261 -4.3068867 -4.3078794 -4.3127141 -4.3152852 -4.3165212 -4.3165808 -4.3159375 -4.3146534 -4.3135362 -4.3134766 -4.3131871 -4.3126]]...]
INFO - root - 2017-12-05 14:14:31.141683: step 15910, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 74h:59m:11s remains)
INFO - root - 2017-12-05 14:14:39.431081: step 15920, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 63h:57m:04s remains)
INFO - root - 2017-12-05 14:14:47.834112: step 15930, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.838 sec/batch; 73h:39m:10s remains)
INFO - root - 2017-12-05 14:14:56.272486: step 15940, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 73h:42m:36s remains)
INFO - root - 2017-12-05 14:15:04.792600: step 15950, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 75h:00m:30s remains)
INFO - root - 2017-12-05 14:15:13.346345: step 15960, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 76h:55m:06s remains)
INFO - root - 2017-12-05 14:15:21.989689: step 15970, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 78h:21m:50s remains)
INFO - root - 2017-12-05 14:15:30.551300: step 15980, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.809 sec/batch; 71h:06m:43s remains)
INFO - root - 2017-12-05 14:15:39.043236: step 15990, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 75h:19m:31s remains)
INFO - root - 2017-12-05 14:15:47.587498: step 16000, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 75h:27m:32s remains)
2017-12-05 14:15:48.383431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.304132 -4.309485 -4.3179359 -4.3260684 -4.3305292 -4.3260379 -4.3140969 -4.3005762 -4.2932129 -4.2963505 -4.3088408 -4.323813 -4.3387585 -4.3486753 -4.3533664][-4.2465782 -4.2530456 -4.2647753 -4.2796707 -4.2940726 -4.2924423 -4.2763944 -4.2583337 -4.2492838 -4.2554832 -4.2775159 -4.3011312 -4.3222237 -4.3367953 -4.3446059][-4.17787 -4.1817966 -4.1931658 -4.2142429 -4.2376714 -4.2348232 -4.21012 -4.1854615 -4.1768222 -4.1908855 -4.2300615 -4.2657905 -4.2935953 -4.3161221 -4.3303375][-4.1202731 -4.1189475 -4.1278067 -4.1498008 -4.1730628 -4.160758 -4.124311 -4.0893927 -4.0766683 -4.1023793 -4.1643777 -4.2181177 -4.2571383 -4.290884 -4.313642][-4.0892973 -4.0815668 -4.0836487 -4.1004744 -4.1150913 -4.0856037 -4.0311666 -3.9816933 -3.9641118 -4.0039029 -4.09437 -4.1753674 -4.228375 -4.2714324 -4.3010888][-4.0847917 -4.0683947 -4.0614338 -4.0745025 -4.0828261 -4.04398 -3.975414 -3.912637 -3.8840921 -3.9255247 -4.0339718 -4.1362829 -4.2027273 -4.2546449 -4.291595][-4.085156 -4.0633907 -4.0536389 -4.0643296 -4.0683036 -4.0343833 -3.9709463 -3.9169517 -3.8986864 -3.9361031 -4.0303612 -4.1247044 -4.1925268 -4.2484221 -4.291069][-4.0713129 -4.0463543 -4.0373468 -4.0464158 -4.0483947 -4.0296192 -3.9851165 -3.9554586 -3.9570343 -3.9898229 -4.0591545 -4.135272 -4.1958208 -4.2498655 -4.2947111][-4.0769105 -4.0571694 -4.0533638 -4.0617933 -4.0657 -4.059474 -4.0299029 -4.0156527 -4.0302615 -4.0553474 -4.0996265 -4.157958 -4.2089562 -4.2574816 -4.3007855][-4.1196079 -4.1097074 -4.1088071 -4.1148047 -4.1217618 -4.1254587 -4.109941 -4.1078868 -4.126298 -4.1386509 -4.1596847 -4.2004519 -4.2380548 -4.2750373 -4.3093638][-4.1716094 -4.1708741 -4.1687632 -4.167707 -4.1736779 -4.1779385 -4.1690903 -4.1760626 -4.1958365 -4.2013397 -4.211586 -4.2412724 -4.2681966 -4.2954764 -4.319654][-4.224834 -4.2299142 -4.2252326 -4.2157035 -4.2166886 -4.2200017 -4.2148681 -4.224689 -4.2412534 -4.245882 -4.2539463 -4.2754169 -4.2930565 -4.3133979 -4.3313713][-4.2765565 -4.2814293 -4.2741103 -4.2612205 -4.2605491 -4.2636328 -4.2625527 -4.2714305 -4.2829165 -4.2883649 -4.2949972 -4.3085704 -4.3188939 -4.331346 -4.3435421][-4.3207569 -4.3234768 -4.3159742 -4.3055754 -4.3057547 -4.3094273 -4.3118587 -4.3174291 -4.3233976 -4.3277826 -4.3321934 -4.338666 -4.3420506 -4.346859 -4.3530846][-4.3445578 -4.344892 -4.3405538 -4.3352966 -4.3360405 -4.339251 -4.3426232 -4.345468 -4.3483543 -4.3509021 -4.3529444 -4.3544331 -4.3543787 -4.3549662 -4.3571663]]...]
INFO - root - 2017-12-05 14:15:56.805212: step 16010, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 75h:20m:59s remains)
INFO - root - 2017-12-05 14:16:05.400666: step 16020, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 73h:55m:43s remains)
INFO - root - 2017-12-05 14:16:13.882097: step 16030, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 73h:45m:53s remains)
INFO - root - 2017-12-05 14:16:22.283865: step 16040, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 75h:04m:10s remains)
INFO - root - 2017-12-05 14:16:30.673078: step 16050, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 74h:43m:53s remains)
INFO - root - 2017-12-05 14:16:39.061189: step 16060, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 75h:02m:20s remains)
INFO - root - 2017-12-05 14:16:47.578731: step 16070, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 75h:10m:45s remains)
INFO - root - 2017-12-05 14:16:56.005941: step 16080, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 75h:07m:36s remains)
INFO - root - 2017-12-05 14:17:04.514781: step 16090, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 74h:54m:04s remains)
INFO - root - 2017-12-05 14:17:13.044618: step 16100, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 75h:54m:15s remains)
2017-12-05 14:17:13.993650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2844706 -4.2901316 -4.2960215 -4.2995024 -4.3021584 -4.3052754 -4.3092208 -4.3114233 -4.3097363 -4.3051248 -4.3010082 -4.2984939 -4.3002744 -4.3084855 -4.3207006][-4.2628007 -4.2692719 -4.2763352 -4.2806511 -4.28492 -4.2919331 -4.3000813 -4.3046474 -4.304503 -4.2995973 -4.2932515 -4.287488 -4.2866535 -4.2946415 -4.3092813][-4.2424407 -4.24313 -4.2444415 -4.2457938 -4.2504873 -4.2618303 -4.2734833 -4.2800741 -4.2838674 -4.2814674 -4.2748871 -4.2668004 -4.2636323 -4.2711616 -4.2900119][-4.2121806 -4.2018824 -4.1915989 -4.185595 -4.1872158 -4.1981506 -4.2071047 -4.2133322 -4.2273264 -4.2377858 -4.2384596 -4.2341828 -4.231986 -4.2412472 -4.2647605][-4.1719208 -4.1471095 -4.1210194 -4.1049838 -4.100172 -4.1022797 -4.096117 -4.0945263 -4.1229572 -4.1545811 -4.171443 -4.1804752 -4.1888332 -4.2054324 -4.2353268][-4.1377707 -4.0992122 -4.0573106 -4.0295124 -4.0143752 -3.9968181 -3.962199 -3.9461086 -3.9969029 -4.0583997 -4.0971141 -4.1240435 -4.1458697 -4.1715903 -4.2074838][-4.1436057 -4.0984139 -4.048595 -4.0074725 -3.9749954 -3.926923 -3.8564413 -3.8214681 -3.8934166 -3.9791825 -4.0343766 -4.078084 -4.1158714 -4.1510725 -4.1910391][-4.1844625 -4.1470413 -4.1064367 -4.0654192 -4.0220709 -3.9592185 -3.8787413 -3.8351216 -3.8925571 -3.9687338 -4.0212154 -4.0723338 -4.1232486 -4.1658688 -4.2054548][-4.2264009 -4.2072392 -4.1898546 -4.1652718 -4.1281495 -4.0734282 -4.0103374 -3.9744449 -3.9991875 -4.0390019 -4.0703869 -4.1105919 -4.1588397 -4.2002835 -4.2358561][-4.24519 -4.2490349 -4.2581468 -4.2554779 -4.2366118 -4.2025766 -4.162899 -4.1362157 -4.1354 -4.1416693 -4.1489348 -4.1701083 -4.2032418 -4.2344818 -4.2636623][-4.2233887 -4.2463756 -4.2759037 -4.2938828 -4.2990942 -4.2924428 -4.276886 -4.2611828 -4.2491984 -4.236074 -4.2242222 -4.2243896 -4.2384028 -4.2559242 -4.2772369][-4.1844764 -4.2169776 -4.2568159 -4.28762 -4.312665 -4.3272905 -4.3285193 -4.3230667 -4.3116293 -4.291656 -4.2693934 -4.2567215 -4.2561803 -4.2630367 -4.2781491][-4.1905789 -4.2151155 -4.2455163 -4.2730622 -4.3051138 -4.3299985 -4.33892 -4.3390331 -4.3323789 -4.3164482 -4.2951074 -4.279561 -4.27314 -4.272716 -4.2822471][-4.2405434 -4.2501278 -4.2644124 -4.2812257 -4.3074603 -4.3304915 -4.3404794 -4.3422136 -4.3390384 -4.3302035 -4.3169789 -4.3065028 -4.3004527 -4.2963138 -4.2999773][-4.2976432 -4.2973948 -4.3002863 -4.3077207 -4.3215294 -4.334466 -4.3400435 -4.3409624 -4.3409514 -4.3390431 -4.3344617 -4.3303728 -4.3273668 -4.3235812 -4.323513]]...]
INFO - root - 2017-12-05 14:17:22.364144: step 16110, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 76h:25m:28s remains)
INFO - root - 2017-12-05 14:17:30.848836: step 16120, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 73h:05m:37s remains)
INFO - root - 2017-12-05 14:17:39.524381: step 16130, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 76h:55m:35s remains)
INFO - root - 2017-12-05 14:17:47.960821: step 16140, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 75h:01m:09s remains)
INFO - root - 2017-12-05 14:17:56.553857: step 16150, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 75h:00m:17s remains)
INFO - root - 2017-12-05 14:18:05.212183: step 16160, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 75h:07m:52s remains)
INFO - root - 2017-12-05 14:18:13.722271: step 16170, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 75h:42m:04s remains)
INFO - root - 2017-12-05 14:18:22.344282: step 16180, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 74h:23m:24s remains)
INFO - root - 2017-12-05 14:18:30.832435: step 16190, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 75h:02m:46s remains)
INFO - root - 2017-12-05 14:18:39.393052: step 16200, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 75h:06m:04s remains)
2017-12-05 14:18:40.122831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3428721 -4.3393064 -4.3363132 -4.334506 -4.3339581 -4.3320985 -4.3262339 -4.3165941 -4.3066297 -4.3024306 -4.3053927 -4.31515 -4.3273492 -4.3352852 -4.3373632][-4.3403025 -4.3370695 -4.3335309 -4.3292332 -4.3228607 -4.3126445 -4.2975512 -4.27982 -4.2662582 -4.2667103 -4.2804546 -4.3029103 -4.325768 -4.3390284 -4.341342][-4.3378348 -4.3341875 -4.3281245 -4.3169208 -4.2989006 -4.2739072 -4.2419529 -4.208252 -4.1862435 -4.193038 -4.2245488 -4.2670517 -4.3077841 -4.33362 -4.3426547][-4.334291 -4.3273325 -4.3143387 -4.29177 -4.2578869 -4.2127314 -4.1568274 -4.0992656 -4.0637269 -4.07771 -4.1328497 -4.2012558 -4.2661409 -4.3121724 -4.3357148][-4.3269143 -4.3139243 -4.2906046 -4.2535038 -4.2022572 -4.1374159 -4.0595741 -3.9794369 -3.9297225 -3.9495134 -4.0269623 -4.1182857 -4.2061262 -4.2736316 -4.3146424][-4.312171 -4.292428 -4.2570548 -4.20425 -4.1378708 -4.0628657 -3.9775248 -3.8914371 -3.8396773 -3.8632517 -3.9516065 -4.051199 -4.1496592 -4.2304616 -4.2844806][-4.2915325 -4.265327 -4.2177482 -4.1507349 -4.0761147 -4.0042033 -3.9321516 -3.8655257 -3.8300147 -3.8537855 -3.9326911 -4.0209827 -4.1130404 -4.1935277 -4.2526188][-4.2745728 -4.2447166 -4.1899557 -4.1165252 -4.0441384 -3.9876029 -3.9414341 -3.9057903 -3.8916891 -3.913537 -3.9719696 -4.0382409 -4.1123481 -4.1799664 -4.2332697][-4.2730174 -4.2438717 -4.1900377 -4.1195688 -4.0565386 -4.0160894 -3.9930305 -3.9846425 -3.989223 -4.0109367 -4.052844 -4.0997334 -4.1547379 -4.2048516 -4.2448397][-4.2909436 -4.2659664 -4.2206616 -4.1628237 -4.1127396 -4.0842786 -4.0743914 -4.0800323 -4.0937958 -4.1157551 -4.1481123 -4.1831164 -4.2236271 -4.2581429 -4.2832007][-4.317421 -4.2980466 -4.2648239 -4.2238755 -4.1895695 -4.1725655 -4.1696196 -4.1787586 -4.192956 -4.2118239 -4.2361712 -4.2631054 -4.29352 -4.31729 -4.3307085][-4.3401809 -4.3269987 -4.3064785 -4.2822022 -4.2634792 -4.2564092 -4.2573795 -4.2645993 -4.2724776 -4.2820973 -4.2953711 -4.314178 -4.3376808 -4.3557692 -4.3632727][-4.3533344 -4.3467226 -4.3362894 -4.3245749 -4.3169971 -4.3165736 -4.3206234 -4.3261113 -4.3282709 -4.3285975 -4.3305688 -4.3401585 -4.3559165 -4.3692575 -4.3744116][-4.3554168 -4.3536491 -4.3486695 -4.3439555 -4.34232 -4.3452792 -4.3513865 -4.3564692 -4.3560934 -4.350574 -4.3444896 -4.3448148 -4.3523388 -4.3615794 -4.3665152][-4.3503494 -4.350369 -4.346653 -4.3437114 -4.343709 -4.3473148 -4.3532324 -4.3573551 -4.3566508 -4.350594 -4.3421903 -4.3372397 -4.3393874 -4.34606 -4.3521304]]...]
INFO - root - 2017-12-05 14:18:48.480572: step 16210, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 74h:15m:35s remains)
INFO - root - 2017-12-05 14:18:57.070130: step 16220, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 75h:27m:52s remains)
INFO - root - 2017-12-05 14:19:05.601237: step 16230, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 74h:45m:51s remains)
INFO - root - 2017-12-05 14:19:14.227034: step 16240, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 77h:19m:21s remains)
INFO - root - 2017-12-05 14:19:22.602467: step 16250, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 73h:49m:13s remains)
INFO - root - 2017-12-05 14:19:31.143071: step 16260, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 73h:04m:50s remains)
INFO - root - 2017-12-05 14:19:39.708461: step 16270, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 72h:16m:24s remains)
INFO - root - 2017-12-05 14:19:48.114249: step 16280, loss = 2.04, batch loss = 1.98 (10.0 examples/sec; 0.804 sec/batch; 70h:35m:27s remains)
INFO - root - 2017-12-05 14:19:56.681153: step 16290, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 76h:16m:16s remains)
INFO - root - 2017-12-05 14:20:05.187950: step 16300, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 77h:06m:21s remains)
2017-12-05 14:20:05.925103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2952652 -4.2942185 -4.2889948 -4.2828474 -4.2757015 -4.2658696 -4.2446666 -4.2022123 -4.1701694 -4.1786165 -4.2010393 -4.2244883 -4.2419314 -4.265841 -4.2845683][-4.2950516 -4.295064 -4.2881918 -4.2786231 -4.2678843 -4.2567725 -4.2370386 -4.1978683 -4.1744919 -4.1905341 -4.2158608 -4.2351847 -4.2485566 -4.2691956 -4.2862744][-4.2990236 -4.3000531 -4.2902155 -4.2767639 -4.2625828 -4.2507334 -4.2321386 -4.1981282 -4.1864648 -4.209362 -4.23298 -4.2461309 -4.2565169 -4.2752314 -4.2874432][-4.3034 -4.3064828 -4.2965059 -4.2818456 -4.2674661 -4.2544155 -4.2327447 -4.2031178 -4.2028546 -4.232151 -4.252955 -4.2594647 -4.2657094 -4.2804317 -4.2874784][-4.3071356 -4.3113875 -4.30274 -4.2874565 -4.2730961 -4.2588549 -4.232842 -4.2070522 -4.2198853 -4.2559018 -4.2753458 -4.2771077 -4.2772956 -4.282856 -4.2834716][-4.3118105 -4.3169942 -4.3090415 -4.2916322 -4.2690148 -4.2414579 -4.202435 -4.1794839 -4.2085443 -4.2604165 -4.28788 -4.2902074 -4.2867928 -4.2834806 -4.276854][-4.3182597 -4.3222075 -4.3112745 -4.2873392 -4.2484565 -4.1961532 -4.1316218 -4.0980487 -4.1433215 -4.2263985 -4.2751184 -4.2877936 -4.2857509 -4.2813349 -4.2719364][-4.325079 -4.3253384 -4.3066192 -4.2712383 -4.2123814 -4.1301055 -4.0281663 -3.9621403 -4.0190253 -4.1457419 -4.2293239 -4.2621675 -4.272851 -4.2757864 -4.2694769][-4.3296423 -4.3260055 -4.3016391 -4.2590971 -4.1943765 -4.1000142 -3.9728792 -3.8742449 -3.9240069 -4.07661 -4.1863613 -4.2377739 -4.2618027 -4.2745209 -4.2705][-4.3331575 -4.326201 -4.3014073 -4.2642918 -4.21375 -4.1420145 -4.0419903 -3.9620924 -3.9911706 -4.1060014 -4.1979284 -4.2446861 -4.2684531 -4.2816687 -4.2754893][-4.3364048 -4.3274188 -4.3065314 -4.2799644 -4.2490239 -4.2062836 -4.1456079 -4.0983977 -4.1130853 -4.1778679 -4.2371221 -4.2671719 -4.2823281 -4.2905469 -4.2791677][-4.3384004 -4.3279233 -4.3112392 -4.2943568 -4.276917 -4.2505722 -4.2139587 -4.184763 -4.1906295 -4.225769 -4.26103 -4.2785277 -4.28561 -4.2891603 -4.2752342][-4.3370843 -4.3262076 -4.31113 -4.2977195 -4.2855148 -4.2686806 -4.2457933 -4.2232909 -4.2215319 -4.2389393 -4.2601604 -4.2702041 -4.2742295 -4.2758269 -4.2658553][-4.3343372 -4.323277 -4.3101449 -4.2988358 -4.2880731 -4.2753706 -4.2551 -4.2301574 -4.225121 -4.2375789 -4.2540121 -4.2580676 -4.2594333 -4.2621975 -4.2557154][-4.3324132 -4.3220744 -4.3129225 -4.3068471 -4.2996798 -4.2888694 -4.2668891 -4.235579 -4.2257919 -4.2369447 -4.2506032 -4.2511892 -4.2509475 -4.2541351 -4.2485905]]...]
INFO - root - 2017-12-05 14:20:14.375001: step 16310, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 78h:19m:42s remains)
INFO - root - 2017-12-05 14:20:22.836109: step 16320, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 75h:00m:43s remains)
INFO - root - 2017-12-05 14:20:31.374001: step 16330, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.816 sec/batch; 71h:38m:40s remains)
INFO - root - 2017-12-05 14:20:39.997182: step 16340, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 78h:00m:51s remains)
INFO - root - 2017-12-05 14:20:48.406683: step 16350, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.821 sec/batch; 72h:05m:36s remains)
INFO - root - 2017-12-05 14:20:56.970424: step 16360, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.875 sec/batch; 76h:50m:16s remains)
INFO - root - 2017-12-05 14:21:05.596078: step 16370, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 77h:06m:28s remains)
INFO - root - 2017-12-05 14:21:14.235881: step 16380, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 77h:02m:32s remains)
INFO - root - 2017-12-05 14:21:22.815838: step 16390, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 75h:59m:07s remains)
INFO - root - 2017-12-05 14:21:31.300417: step 16400, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 74h:40m:20s remains)
2017-12-05 14:21:32.100907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2668076 -4.2663021 -4.265656 -4.2659912 -4.2668562 -4.2677073 -4.2683768 -4.2685213 -4.268373 -4.2681351 -4.2679105 -4.2679543 -4.2677393 -4.2671685 -4.2680073][-4.2586365 -4.259222 -4.2599225 -4.26125 -4.2622256 -4.2626643 -4.2626014 -4.2617545 -4.2605577 -4.2594829 -4.2586412 -4.2581787 -4.2574177 -4.2560616 -4.2561188][-4.2511616 -4.2559357 -4.2586622 -4.2601757 -4.2605829 -4.2592964 -4.2572818 -4.2550659 -4.2528214 -4.250874 -4.2491546 -4.248014 -4.2460451 -4.2427096 -4.2403989][-4.2543011 -4.2627363 -4.2661104 -4.2666984 -4.2656741 -4.2623014 -4.2584729 -4.2553339 -4.2526422 -4.250195 -4.2475262 -4.2454548 -4.2418966 -4.2363157 -4.2312751][-4.2658291 -4.273273 -4.2738004 -4.2721415 -4.2692957 -4.2641363 -4.2591276 -4.2560573 -4.2539029 -4.2519445 -4.2491565 -4.2473025 -4.2444286 -4.2401476 -4.2365861][-4.2783847 -4.2839503 -4.2813764 -4.2762136 -4.2692704 -4.2603145 -4.2523174 -4.247694 -4.2467055 -4.2468586 -4.2460384 -4.246532 -4.2464371 -4.245729 -4.245223][-4.278965 -4.281817 -4.2750764 -4.2634792 -4.2498026 -4.2346148 -4.2227068 -4.2163525 -4.2159348 -4.2174706 -4.2179656 -4.2206788 -4.2244153 -4.2288213 -4.2322526][-4.2601037 -4.2595873 -4.2469144 -4.2279196 -4.2070508 -4.1852741 -4.1683064 -4.15966 -4.1582689 -4.1591887 -4.1597695 -4.1640205 -4.1709752 -4.1796775 -4.1870565][-4.2201276 -4.2147059 -4.19814 -4.1753392 -4.1510429 -4.126862 -4.1081829 -4.098114 -4.0957212 -4.0965996 -4.0988173 -4.105288 -4.1153431 -4.1274104 -4.1381741][-4.1831098 -4.1751409 -4.1604457 -4.1414309 -4.1213989 -4.1014533 -4.0869017 -4.0798378 -4.0792851 -4.0823851 -4.0882382 -4.0978475 -4.109951 -4.1221991 -4.1330976][-4.1871905 -4.1831965 -4.1752534 -4.1633263 -4.1501112 -4.1374154 -4.1292725 -4.1271305 -4.1294327 -4.1349349 -4.1431026 -4.1533422 -4.1639786 -4.1728115 -4.1794434][-4.2214422 -4.2234635 -4.2222309 -4.216454 -4.2082906 -4.20054 -4.196682 -4.197156 -4.199645 -4.2044373 -4.2110839 -4.2182803 -4.2245178 -4.2286091 -4.2308264][-4.2400403 -4.2447004 -4.2476568 -4.2475495 -4.2437754 -4.2398682 -4.2381763 -4.2391195 -4.2403626 -4.2430062 -4.2462578 -4.2491269 -4.2509952 -4.2521152 -4.2530971][-4.2268486 -4.2294288 -4.2332377 -4.2360849 -4.2364554 -4.2370224 -4.2388487 -4.2413974 -4.2430243 -4.2449055 -4.2460241 -4.2464161 -4.2465863 -4.2471304 -4.248579][-4.211401 -4.2108474 -4.2127786 -4.21581 -4.2189369 -4.2239819 -4.230278 -4.2368112 -4.2416916 -4.2451854 -4.2461367 -4.2455888 -4.2445841 -4.2433214 -4.24238]]...]
INFO - root - 2017-12-05 14:21:40.524396: step 16410, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 73h:06m:22s remains)
INFO - root - 2017-12-05 14:21:49.106243: step 16420, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 0.807 sec/batch; 70h:48m:51s remains)
INFO - root - 2017-12-05 14:21:57.464282: step 16430, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.821 sec/batch; 72h:06m:55s remains)
INFO - root - 2017-12-05 14:22:06.000100: step 16440, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 75h:25m:15s remains)
INFO - root - 2017-12-05 14:22:14.689459: step 16450, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 79h:30m:04s remains)
INFO - root - 2017-12-05 14:22:23.146794: step 16460, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 73h:52m:49s remains)
INFO - root - 2017-12-05 14:22:31.736693: step 16470, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 73h:42m:24s remains)
INFO - root - 2017-12-05 14:22:40.301175: step 16480, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 75h:39m:41s remains)
INFO - root - 2017-12-05 14:22:48.745243: step 16490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 75h:54m:11s remains)
INFO - root - 2017-12-05 14:22:57.357748: step 16500, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.874 sec/batch; 76h:41m:58s remains)
2017-12-05 14:22:58.084457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2265444 -4.2275338 -4.2188911 -4.2092128 -4.2046885 -4.2084074 -4.2194152 -4.2339377 -4.2431593 -4.2482719 -4.2548985 -4.2601662 -4.2586346 -4.2516332 -4.2417493][-4.2171369 -4.2201662 -4.2075691 -4.1918325 -4.179647 -4.1738548 -4.1769218 -4.1873713 -4.1974335 -4.2082081 -4.224648 -4.242084 -4.249794 -4.2486367 -4.2406926][-4.1899657 -4.1940765 -4.1780019 -4.1591821 -4.1417656 -4.1271372 -4.1193357 -4.1208615 -4.1250129 -4.1361928 -4.1604791 -4.1906557 -4.2107058 -4.2178178 -4.2112217][-4.1426406 -4.1513653 -4.1391077 -4.1253614 -4.1120753 -4.0963411 -4.0817418 -4.07085 -4.0585437 -4.0568476 -4.0775394 -4.1121445 -4.1437416 -4.1602612 -4.1601682][-4.0989151 -4.1097236 -4.10307 -4.0968828 -4.0908079 -4.0780993 -4.0617437 -4.0402932 -4.0078835 -3.98862 -3.99905 -4.0277424 -4.0619698 -4.0873179 -4.1019506][-4.0739131 -4.0837874 -4.0810747 -4.0794291 -4.0753708 -4.0634193 -4.0449185 -4.0137925 -3.9653409 -3.9337888 -3.9321158 -3.9459815 -3.9757931 -4.005743 -4.0343485][-4.0611515 -4.0696273 -4.07084 -4.0727043 -4.0701532 -4.0630817 -4.0497274 -4.0173016 -3.9660919 -3.9316311 -3.9166427 -3.9102979 -3.9296448 -3.9606979 -3.9928238][-4.0570655 -4.0591903 -4.0601292 -4.0638485 -4.0606737 -4.0618539 -4.0615292 -4.0406594 -4.0024862 -3.9804392 -3.9688475 -3.961338 -3.9714978 -3.9850781 -3.9989202][-4.064322 -4.06045 -4.05894 -4.0585666 -4.0495076 -4.05106 -4.0585465 -4.0552931 -4.0360618 -4.0287266 -4.0282273 -4.0273724 -4.034759 -4.0281839 -4.0127888][-4.078711 -4.0689921 -4.0651946 -4.0611548 -4.0450187 -4.0406127 -4.0467243 -4.0529256 -4.0459118 -4.0462132 -4.0549912 -4.060699 -4.068151 -4.0513763 -4.0280328][-4.0943489 -4.083674 -4.0793886 -4.0752921 -4.0554361 -4.0423646 -4.0417023 -4.049325 -4.0463319 -4.0493989 -4.0619297 -4.0757637 -4.0856028 -4.0694752 -4.0591903][-4.1126752 -4.1100731 -4.1090016 -4.1045117 -4.0785122 -4.0564237 -4.049087 -4.0570421 -4.0566545 -4.0583854 -4.0715737 -4.0897865 -4.1049871 -4.1006227 -4.1110034][-4.1272125 -4.1311193 -4.1308069 -4.1256981 -4.0977426 -4.0725918 -4.0634317 -4.0694346 -4.069684 -4.0700574 -4.0836778 -4.1030293 -4.1201768 -4.1329064 -4.1647577][-4.1269016 -4.1292119 -4.1271534 -4.1266065 -4.1091137 -4.0889583 -4.0802937 -4.0852942 -4.0849185 -4.0836182 -4.09321 -4.1082816 -4.1216679 -4.1463957 -4.1911421][-4.1160851 -4.1134987 -4.1067047 -4.1112289 -4.1112318 -4.1020532 -4.097291 -4.1048713 -4.1072454 -4.1056404 -4.1086607 -4.1155195 -4.1219463 -4.1491709 -4.1938577]]...]
INFO - root - 2017-12-05 14:23:06.539892: step 16510, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 74h:30m:23s remains)
INFO - root - 2017-12-05 14:23:15.105245: step 16520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 75h:34m:55s remains)
INFO - root - 2017-12-05 14:23:23.522151: step 16530, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 0.787 sec/batch; 69h:03m:07s remains)
INFO - root - 2017-12-05 14:23:31.955615: step 16540, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 72h:46m:04s remains)
INFO - root - 2017-12-05 14:23:40.438405: step 16550, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 75h:50m:13s remains)
INFO - root - 2017-12-05 14:23:48.931098: step 16560, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 73h:22m:33s remains)
INFO - root - 2017-12-05 14:23:57.468433: step 16570, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 76h:16m:48s remains)
INFO - root - 2017-12-05 14:24:05.951174: step 16580, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 73h:10m:30s remains)
INFO - root - 2017-12-05 14:24:14.507146: step 16590, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 75h:00m:49s remains)
INFO - root - 2017-12-05 14:24:23.068960: step 16600, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 75h:13m:44s remains)
2017-12-05 14:24:23.833924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1181607 -4.1148558 -4.1016369 -4.0677118 -4.0790334 -4.1375995 -4.1584206 -4.1588864 -4.1694288 -4.1825438 -4.1779385 -4.1807432 -4.2042627 -4.2127509 -4.2049966][-4.132762 -4.1302714 -4.1154933 -4.078651 -4.0859442 -4.1434441 -4.1603756 -4.1595159 -4.1701851 -4.1848159 -4.1821332 -4.1852536 -4.2112427 -4.2187338 -4.20244][-4.1304951 -4.124526 -4.1104507 -4.0763268 -4.0844913 -4.1369653 -4.1476188 -4.1441493 -4.1532288 -4.1716905 -4.1806865 -4.19195 -4.2174482 -4.22464 -4.2072272][-4.1150517 -4.1040506 -4.0888147 -4.0560894 -4.0652819 -4.1111546 -4.1189504 -4.1207256 -4.1355076 -4.1573639 -4.1755128 -4.1936426 -4.212554 -4.21283 -4.19505][-4.0981603 -4.0758271 -4.05192 -4.0170636 -4.0290256 -4.0717411 -4.0684357 -4.0708151 -4.1004429 -4.1326752 -4.1577878 -4.1877642 -4.2119117 -4.2064261 -4.1860194][-4.0838456 -4.0446 -4.0023737 -3.9568355 -3.9626553 -3.9852643 -3.9460268 -3.9364238 -4.0022836 -4.0687656 -4.1096549 -4.155364 -4.1943207 -4.1974778 -4.1876526][-4.0908051 -4.0348058 -3.9736128 -3.9114964 -3.8948545 -3.8740783 -3.769655 -3.7374282 -3.8622861 -3.9846044 -4.048758 -4.1149836 -4.1732621 -4.1955066 -4.2011089][-4.1060462 -4.0507045 -3.9911509 -3.9313743 -3.9076595 -3.8718863 -3.7394905 -3.6790447 -3.8181396 -3.9559383 -4.022078 -4.0950165 -4.1651864 -4.1958656 -4.2059765][-4.1131506 -4.0662546 -4.019671 -3.9753408 -3.9635811 -3.947062 -3.8591504 -3.8113885 -3.9069712 -4.0087676 -4.0495567 -4.1119213 -4.1845403 -4.21499 -4.2211776][-4.1074886 -4.0778708 -4.0483589 -4.0115862 -4.0133195 -4.0184588 -3.9707279 -3.9432335 -4.0105858 -4.0819645 -4.0991445 -4.1440468 -4.2108974 -4.2385983 -4.2433586][-4.0838075 -4.0641928 -4.0484948 -4.02265 -4.0410914 -4.066597 -4.0446734 -4.0334983 -4.0872765 -4.1336923 -4.1358514 -4.1688113 -4.2238054 -4.2425447 -4.2396636][-4.0628772 -4.042953 -4.0357528 -4.0228906 -4.0577388 -4.1049356 -4.1052327 -4.1069479 -4.1531415 -4.1832867 -4.1719189 -4.1891384 -4.2274866 -4.2276115 -4.2106242][-4.0698967 -4.0467119 -4.0400395 -4.0321946 -4.0712681 -4.12988 -4.1472936 -4.1555915 -4.1860576 -4.1978335 -4.17812 -4.1828227 -4.2036271 -4.1950932 -4.1782112][-4.1010036 -4.0795903 -4.0702243 -4.0626478 -4.0941415 -4.1497464 -4.1721339 -4.1783042 -4.1990261 -4.2014313 -4.1815104 -4.1823158 -4.195899 -4.1885161 -4.1790562][-4.151649 -4.1379452 -4.1257815 -4.116828 -4.1376486 -4.181951 -4.1997795 -4.1990371 -4.2067075 -4.2026911 -4.1851668 -4.1849551 -4.1921825 -4.1861935 -4.1861787]]...]
INFO - root - 2017-12-05 14:24:32.220547: step 16610, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 0.797 sec/batch; 69h:56m:28s remains)
INFO - root - 2017-12-05 14:24:40.708917: step 16620, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 76h:49m:41s remains)
INFO - root - 2017-12-05 14:24:49.233411: step 16630, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 75h:21m:01s remains)
INFO - root - 2017-12-05 14:24:57.674304: step 16640, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 73h:09m:46s remains)
INFO - root - 2017-12-05 14:25:06.237243: step 16650, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 77h:00m:07s remains)
INFO - root - 2017-12-05 14:25:14.811996: step 16660, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 76h:48m:30s remains)
INFO - root - 2017-12-05 14:25:23.204814: step 16670, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 0.779 sec/batch; 68h:22m:25s remains)
INFO - root - 2017-12-05 14:25:31.540104: step 16680, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 74h:18m:26s remains)
INFO - root - 2017-12-05 14:25:39.863358: step 16690, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 71h:55m:59s remains)
INFO - root - 2017-12-05 14:25:48.240871: step 16700, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.803 sec/batch; 70h:25m:30s remains)
2017-12-05 14:25:49.003634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3369436 -4.3295412 -4.3236256 -4.3179426 -4.3151608 -4.3150578 -4.3177848 -4.3194566 -4.3179822 -4.3139334 -4.307991 -4.2976761 -4.2843409 -4.2738433 -4.2664013][-4.3306241 -4.3211775 -4.3159971 -4.3121233 -4.3107963 -4.31159 -4.3146415 -4.31673 -4.3160033 -4.3133888 -4.3093395 -4.3016915 -4.2912135 -4.2821517 -4.2739205][-4.3266306 -4.3170738 -4.3118625 -4.3079562 -4.3067889 -4.3072705 -4.3115454 -4.315403 -4.3161955 -4.3151164 -4.31379 -4.3097773 -4.30341 -4.2971191 -4.2893763][-4.3245769 -4.314218 -4.3061128 -4.2996497 -4.2975664 -4.2971544 -4.3020949 -4.3075972 -4.3095093 -4.30905 -4.3098435 -4.309957 -4.309166 -4.3059921 -4.2977724][-4.31771 -4.3013182 -4.286427 -4.2734294 -4.2651992 -4.2570586 -4.2575336 -4.2634873 -4.2681904 -4.26806 -4.2705731 -4.2747765 -4.2825594 -4.2864437 -4.2821665][-4.2925658 -4.2648664 -4.2375855 -4.2133665 -4.1903796 -4.163312 -4.1461406 -4.1531878 -4.1708016 -4.1784778 -4.1862922 -4.1983232 -4.2167759 -4.2304697 -4.23516][-4.2542806 -4.2174249 -4.1797605 -4.1388516 -4.0892315 -4.0240178 -3.9717648 -3.9938638 -4.0492573 -4.081512 -4.1037068 -4.1284781 -4.1567111 -4.1781468 -4.18975][-4.2199254 -4.1811204 -4.1435971 -4.0964575 -4.0299706 -3.9324579 -3.8542919 -3.8969705 -3.9870746 -4.04236 -4.078331 -4.1059761 -4.130105 -4.1489906 -4.1633844][-4.2138977 -4.1790843 -4.1496739 -4.112237 -4.0573435 -3.9770648 -3.9198489 -3.9580095 -4.0305014 -4.0739207 -4.0978394 -4.1121039 -4.1222382 -4.1329093 -4.1463561][-4.2171063 -4.1859808 -4.1636662 -4.13898 -4.1023369 -4.0491514 -4.0189657 -4.0447679 -4.0866919 -4.1115069 -4.1230683 -4.1289587 -4.133913 -4.1444182 -4.1593552][-4.2292557 -4.2026525 -4.1890521 -4.1778688 -4.1581211 -4.1263027 -4.1106915 -4.1225891 -4.1417422 -4.1539764 -4.1614084 -4.1692157 -4.1782475 -4.1899314 -4.2048192][-4.248426 -4.2276254 -4.2210727 -4.2172556 -4.208828 -4.1955996 -4.1903167 -4.1946454 -4.2055168 -4.2147493 -4.2236991 -4.2341757 -4.2456031 -4.2558737 -4.2651687][-4.2729816 -4.2579722 -4.257462 -4.2597952 -4.2602019 -4.2594566 -4.2624922 -4.26917 -4.2806396 -4.2905407 -4.2990074 -4.3070641 -4.3157406 -4.3215122 -4.32586][-4.2992363 -4.2918191 -4.2965479 -4.3052168 -4.3118963 -4.3159003 -4.3204355 -4.3254704 -4.3335757 -4.3409476 -4.3477712 -4.3542395 -4.360826 -4.3659668 -4.3690233][-4.3220129 -4.3175125 -4.3226004 -4.3322854 -4.3395882 -4.3421 -4.3439121 -4.3450804 -4.3474193 -4.3512492 -4.3570824 -4.3638964 -4.3704371 -4.3769789 -4.38098]]...]
INFO - root - 2017-12-05 14:25:57.371127: step 16710, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 74h:28m:26s remains)
INFO - root - 2017-12-05 14:26:05.875068: step 16720, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.829 sec/batch; 72h:41m:40s remains)
INFO - root - 2017-12-05 14:26:14.386597: step 16730, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 75h:50m:03s remains)
INFO - root - 2017-12-05 14:26:22.795507: step 16740, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 72h:59m:36s remains)
INFO - root - 2017-12-05 14:26:31.383308: step 16750, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 74h:24m:04s remains)
INFO - root - 2017-12-05 14:26:39.885011: step 16760, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 75h:38m:24s remains)
INFO - root - 2017-12-05 14:26:48.400323: step 16770, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 76h:07m:39s remains)
INFO - root - 2017-12-05 14:26:56.912337: step 16780, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 74h:22m:11s remains)
INFO - root - 2017-12-05 14:27:05.442422: step 16790, loss = 2.09, batch loss = 2.04 (9.8 examples/sec; 0.819 sec/batch; 71h:49m:24s remains)
INFO - root - 2017-12-05 14:27:14.049734: step 16800, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.969 sec/batch; 84h:56m:44s remains)
2017-12-05 14:27:14.875394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2885156 -4.2720609 -4.2600212 -4.2471189 -4.229373 -4.2116122 -4.2094412 -4.2116942 -4.2223158 -4.2411776 -4.250927 -4.2495418 -4.2487149 -4.2526937 -4.2543468][-4.2773247 -4.2594657 -4.2457638 -4.2289467 -4.205873 -4.1787181 -4.1660647 -4.1634126 -4.1765614 -4.2035408 -4.2212906 -4.222609 -4.2246342 -4.229712 -4.2324185][-4.2670937 -4.24975 -4.2367377 -4.2210674 -4.1982775 -4.1602583 -4.1371822 -4.1315637 -4.1475129 -4.1819458 -4.2090168 -4.2154374 -4.2202063 -4.2272558 -4.2289348][-4.2524447 -4.2353668 -4.223711 -4.2170882 -4.2054796 -4.1712604 -4.1430068 -4.1361384 -4.1548738 -4.1864772 -4.2136326 -4.22036 -4.2285552 -4.234169 -4.2336812][-4.2314777 -4.2119927 -4.19865 -4.2002845 -4.2035027 -4.1804538 -4.151577 -4.1428256 -4.1680183 -4.1989727 -4.2168083 -4.2150931 -4.2226987 -4.231668 -4.2334533][-4.205452 -4.1813517 -4.1634631 -4.1647687 -4.1700239 -4.1426291 -4.0998473 -4.0912027 -4.1375084 -4.1811161 -4.1975455 -4.1895266 -4.1952939 -4.2080083 -4.2149138][-4.1805525 -4.1512947 -4.1240225 -4.1170011 -4.1095591 -4.0516181 -3.9691296 -3.9406438 -4.0233951 -4.1094031 -4.1433244 -4.1462779 -4.1606379 -4.1778455 -4.1906152][-4.1757264 -4.1455007 -4.1131077 -4.0961189 -4.0667181 -3.9767542 -3.8473275 -3.7840164 -3.9004288 -4.0368838 -4.0971756 -4.1133852 -4.1383538 -4.1634007 -4.1855955][-4.2054234 -4.181385 -4.1511207 -4.1252675 -4.0843043 -3.9957137 -3.8751469 -3.8181748 -3.927119 -4.0590734 -4.119123 -4.1335735 -4.1562529 -4.1833429 -4.2107244][-4.2405849 -4.2200117 -4.1952486 -4.1707311 -4.1332254 -4.0727105 -3.9996717 -3.9758794 -4.0482922 -4.1402216 -4.1818376 -4.1906867 -4.2070189 -4.2311778 -4.2562118][-4.2576017 -4.2388992 -4.2230029 -4.2065144 -4.1864319 -4.1519332 -4.1104527 -4.1014576 -4.1469808 -4.2096925 -4.2432003 -4.250267 -4.2608256 -4.2792063 -4.2954068][-4.2635098 -4.2499619 -4.2419357 -4.2334485 -4.2249041 -4.2065983 -4.1822639 -4.1783004 -4.2102604 -4.2541966 -4.2806578 -4.2862945 -4.2911491 -4.30071 -4.31099][-4.2646651 -4.2531528 -4.2496996 -4.2460728 -4.2385 -4.2253013 -4.2102537 -4.2126303 -4.2389627 -4.2704067 -4.291225 -4.2975869 -4.2983294 -4.3010454 -4.3070793][-4.2667713 -4.2560205 -4.2555761 -4.2561059 -4.2535806 -4.2423725 -4.23006 -4.2329326 -4.253541 -4.276763 -4.2931113 -4.3004646 -4.3004417 -4.2995806 -4.3011432][-4.2755446 -4.2652578 -4.2640738 -4.2659445 -4.2663603 -4.2580729 -4.2485371 -4.2488818 -4.2626934 -4.28064 -4.29436 -4.301898 -4.3054814 -4.3052454 -4.303761]]...]
INFO - root - 2017-12-05 14:27:23.237035: step 16810, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 74h:42m:32s remains)
INFO - root - 2017-12-05 14:27:31.847154: step 16820, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.874 sec/batch; 76h:40m:44s remains)
INFO - root - 2017-12-05 14:27:40.380461: step 16830, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 76h:30m:05s remains)
INFO - root - 2017-12-05 14:27:48.932564: step 16840, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 75h:49m:24s remains)
INFO - root - 2017-12-05 14:27:57.442799: step 16850, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 74h:07m:40s remains)
INFO - root - 2017-12-05 14:28:05.956255: step 16860, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 73h:52m:05s remains)
INFO - root - 2017-12-05 14:28:14.521052: step 16870, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 75h:03m:21s remains)
INFO - root - 2017-12-05 14:28:22.991690: step 16880, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.862 sec/batch; 75h:31m:59s remains)
INFO - root - 2017-12-05 14:28:31.403276: step 16890, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.826 sec/batch; 72h:26m:31s remains)
INFO - root - 2017-12-05 14:28:40.028755: step 16900, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 76h:27m:49s remains)
2017-12-05 14:28:40.842308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1334262 -4.1412044 -4.1547809 -4.1664667 -4.168169 -4.1576128 -4.1538839 -4.1466045 -4.1134014 -4.0864606 -4.0847435 -4.1091261 -4.1400056 -4.1605287 -4.1697931][-4.1251669 -4.139524 -4.150064 -4.1588554 -4.1589842 -4.1462116 -4.143558 -4.1410184 -4.10528 -4.0748415 -4.0739832 -4.1083512 -4.1507955 -4.177875 -4.1818652][-4.1152334 -4.1326447 -4.1381407 -4.1414962 -4.1467018 -4.1389127 -4.1410513 -4.1471863 -4.1187005 -4.0838485 -4.0821404 -4.1206465 -4.1700168 -4.1979012 -4.1897349][-4.1034904 -4.1203294 -4.1240249 -4.1305804 -4.1440935 -4.1407752 -4.1422353 -4.156261 -4.1409802 -4.1113906 -4.1025195 -4.1266766 -4.1680789 -4.1927276 -4.1835513][-4.0797787 -4.096921 -4.100481 -4.1107755 -4.1257739 -4.1111169 -4.0951405 -4.1175313 -4.13017 -4.121676 -4.114325 -4.1250677 -4.14426 -4.1559143 -4.1451783][-4.0596657 -4.077004 -4.078577 -4.0813427 -4.0809784 -4.030098 -3.9677896 -3.9982457 -4.0659947 -4.0964956 -4.0998363 -4.1090708 -4.1114039 -4.1064124 -4.0877395][-4.0592132 -4.0736017 -4.0662212 -4.0479803 -4.0081224 -3.9008353 -3.7648792 -3.7859669 -3.9320834 -4.0310044 -4.0659156 -4.0803189 -4.083559 -4.0725036 -4.0491033][-4.0820665 -4.0927305 -4.0794749 -4.0516024 -4.0023317 -3.8923218 -3.7419362 -3.7220855 -3.8717625 -3.9976573 -4.0530958 -4.0725632 -4.0846844 -4.0818028 -4.060523][-4.1139207 -4.1294603 -4.1228132 -4.1040344 -4.0771151 -4.0175076 -3.9277177 -3.8973868 -3.976351 -4.0611172 -4.1009631 -4.1103606 -4.1169977 -4.1114674 -4.0797257][-4.1461754 -4.1599545 -4.1580358 -4.1507492 -4.1443033 -4.117733 -4.0801659 -4.0689325 -4.1069303 -4.1540737 -4.1698265 -4.1632161 -4.1556721 -4.1425958 -4.0984893][-4.1668687 -4.1718268 -4.1715617 -4.174397 -4.1789522 -4.170423 -4.1570773 -4.1628671 -4.1852365 -4.211164 -4.2139339 -4.1981897 -4.183322 -4.1734109 -4.1402893][-4.1765995 -4.1711478 -4.1663203 -4.1666789 -4.1713777 -4.1663418 -4.1627259 -4.1776028 -4.2012072 -4.2229953 -4.2210627 -4.2056336 -4.1933556 -4.1862435 -4.1663485][-4.1859426 -4.1709576 -4.1598763 -4.1520228 -4.1483016 -4.1393147 -4.1348233 -4.1489129 -4.1733723 -4.1910052 -4.1880236 -4.1756244 -4.1709275 -4.1731629 -4.1682754][-4.2196865 -4.1989784 -4.1801357 -4.1625185 -4.1482882 -4.134604 -4.1234469 -4.1252074 -4.1350136 -4.1395092 -4.1349311 -4.125773 -4.1257257 -4.13464 -4.1418991][-4.2688437 -4.2463307 -4.2216444 -4.1958294 -4.1750684 -4.1592317 -4.142828 -4.1295638 -4.1202841 -4.1074243 -4.0934234 -4.0814948 -4.0828381 -4.094656 -4.1090112]]...]
INFO - root - 2017-12-05 14:28:49.195473: step 16910, loss = 2.04, batch loss = 1.98 (10.0 examples/sec; 0.800 sec/batch; 70h:10m:05s remains)
INFO - root - 2017-12-05 14:28:57.681199: step 16920, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 74h:49m:55s remains)
INFO - root - 2017-12-05 14:29:06.323148: step 16930, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 76h:22m:48s remains)
INFO - root - 2017-12-05 14:29:14.954237: step 16940, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 76h:16m:33s remains)
INFO - root - 2017-12-05 14:29:23.509196: step 16950, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 76h:46m:26s remains)
INFO - root - 2017-12-05 14:29:32.019959: step 16960, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 73h:56m:51s remains)
INFO - root - 2017-12-05 14:29:40.511267: step 16970, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 74h:08m:01s remains)
INFO - root - 2017-12-05 14:29:48.921384: step 16980, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 74h:15m:26s remains)
INFO - root - 2017-12-05 14:29:57.319524: step 16990, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.815 sec/batch; 71h:24m:51s remains)
INFO - root - 2017-12-05 14:30:05.653414: step 17000, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 72h:30m:20s remains)
2017-12-05 14:30:06.402919: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1859431 -4.2158442 -4.2349472 -4.23506 -4.2176318 -4.2018185 -4.1899872 -4.1800437 -4.1749182 -4.1740303 -4.1849785 -4.2059031 -4.2238398 -4.2210317 -4.2142491][-4.2166495 -4.2445822 -4.2580156 -4.2493539 -4.2226577 -4.2020926 -4.1923432 -4.1897283 -4.1905427 -4.1916637 -4.203064 -4.2256446 -4.2442627 -4.2420354 -4.231328][-4.2071791 -4.2309337 -4.2427044 -4.2277923 -4.1975279 -4.1789083 -4.18211 -4.1970015 -4.2065654 -4.2101908 -4.2211475 -4.2396669 -4.2529869 -4.2511191 -4.2426333][-4.1538258 -4.1770511 -4.1924491 -4.1802316 -4.1532221 -4.1409545 -4.1593981 -4.1907072 -4.2062435 -4.2106047 -4.2202721 -4.232481 -4.2388477 -4.2384634 -4.2370868][-4.0912294 -4.1118274 -4.1265378 -4.1192226 -4.0980692 -4.0864186 -4.1073656 -4.1485195 -4.1735253 -4.1832552 -4.1971283 -4.2072415 -4.212081 -4.2150393 -4.2225695][-4.0536742 -4.0673618 -4.0726995 -4.0659928 -4.0494933 -4.0322213 -4.0403981 -4.0768828 -4.1075397 -4.1241665 -4.1502757 -4.1691914 -4.1794209 -4.1884789 -4.2031312][-4.0558338 -4.0586147 -4.0469542 -4.034318 -4.0187922 -3.9961383 -3.9865305 -4.0090303 -4.0359726 -4.0573435 -4.0991578 -4.1294584 -4.1435957 -4.1587806 -4.1844606][-4.0851941 -4.0803213 -4.0548773 -4.0321493 -4.0115056 -3.9889357 -3.974993 -3.986382 -4.0029316 -4.0239549 -4.0740247 -4.1098709 -4.1235905 -4.1418877 -4.1781178][-4.12508 -4.1172776 -4.0912466 -4.0677776 -4.0450091 -4.0250282 -4.0169768 -4.0241284 -4.02931 -4.0436664 -4.0902548 -4.1242313 -4.13614 -4.1545424 -4.1933608][-4.166368 -4.1547265 -4.1306562 -4.108068 -4.0857563 -4.0677514 -4.0655203 -4.0761614 -4.0778375 -4.0850949 -4.1215158 -4.1536665 -4.1673245 -4.1859045 -4.222168][-4.20809 -4.1944561 -4.1744165 -4.1575141 -4.1402831 -4.1237068 -4.124887 -4.138185 -4.1398621 -4.1419234 -4.1650534 -4.1900091 -4.2021279 -4.2178164 -4.2466254][-4.2377739 -4.2254143 -4.2122231 -4.2056093 -4.1976228 -4.1856179 -4.1891479 -4.2023311 -4.2041912 -4.2045441 -4.2164779 -4.22972 -4.2363987 -4.2471972 -4.2658634][-4.24672 -4.2371984 -4.2309737 -4.23368 -4.2347779 -4.2300816 -4.23683 -4.247087 -4.2482791 -4.2477069 -4.2517815 -4.2550125 -4.2573967 -4.2662344 -4.2774229][-4.2500944 -4.2417741 -4.2387328 -4.2451658 -4.2508235 -4.2522006 -4.260323 -4.2670226 -4.2661195 -4.2632093 -4.2623086 -4.2602291 -4.2619233 -4.2702055 -4.2773314][-4.2538404 -4.249342 -4.2483835 -4.253221 -4.2583714 -4.2621074 -4.2696056 -4.2728238 -4.2692232 -4.263689 -4.2592559 -4.25515 -4.2578716 -4.2660704 -4.2727208]]...]
INFO - root - 2017-12-05 14:30:14.712515: step 17010, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.829 sec/batch; 72h:39m:47s remains)
INFO - root - 2017-12-05 14:30:23.166132: step 17020, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 75h:22m:54s remains)
INFO - root - 2017-12-05 14:30:31.680733: step 17030, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 73h:17m:08s remains)
INFO - root - 2017-12-05 14:30:40.235779: step 17040, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 75h:14m:31s remains)
INFO - root - 2017-12-05 14:30:48.761067: step 17050, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 74h:26m:43s remains)
INFO - root - 2017-12-05 14:30:57.282799: step 17060, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 74h:19m:21s remains)
INFO - root - 2017-12-05 14:31:05.741554: step 17070, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 74h:44m:34s remains)
INFO - root - 2017-12-05 14:31:14.291956: step 17080, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 75h:23m:09s remains)
INFO - root - 2017-12-05 14:31:22.585646: step 17090, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.829 sec/batch; 72h:35m:56s remains)
INFO - root - 2017-12-05 14:31:31.061629: step 17100, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 75h:08m:24s remains)
2017-12-05 14:31:31.726986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2671819 -4.2628717 -4.2609715 -4.2581778 -4.258718 -4.2647882 -4.2732387 -4.2801981 -4.2837491 -4.2841892 -4.2805257 -4.2755017 -4.2732954 -4.2731614 -4.2715507][-4.2379384 -4.2236018 -4.2144351 -4.2061124 -4.2024 -4.2079048 -4.2222877 -4.2388959 -4.2520604 -4.2636561 -4.2681479 -4.2651272 -4.2628465 -4.2629719 -4.2611542][-4.1995854 -4.1787329 -4.160058 -4.1381745 -4.1249166 -4.1280279 -4.1467175 -4.175869 -4.2063975 -4.2336607 -4.2504086 -4.2521758 -4.2478452 -4.2443061 -4.241034][-4.177824 -4.1604733 -4.1318259 -4.0973988 -4.0678988 -4.0538435 -4.060533 -4.0976639 -4.150804 -4.1948524 -4.2247653 -4.2318645 -4.2245255 -4.2146049 -4.2090669][-4.1769552 -4.1663837 -4.1316333 -4.0865722 -4.037343 -3.9880497 -3.9608774 -3.9927888 -4.068387 -4.1334915 -4.17439 -4.1846628 -4.173017 -4.1582818 -4.1541047][-4.1700621 -4.1645026 -4.1278453 -4.0701141 -4.0021496 -3.9225979 -3.8559227 -3.87197 -3.9698391 -4.0596857 -4.1099763 -4.1222992 -4.111115 -4.1013861 -4.1066513][-4.1538391 -4.155447 -4.1252866 -4.0617156 -3.9854467 -3.8939857 -3.7955935 -3.7836757 -3.8992171 -4.0139909 -4.0730996 -4.0885906 -4.080296 -4.0792046 -4.0955424][-4.1480885 -4.1569118 -4.1360974 -4.0860338 -4.0224037 -3.9418085 -3.8293786 -3.7783031 -3.8864779 -4.0153255 -4.0786066 -4.0929756 -4.080646 -4.0742421 -4.0907335][-4.1635976 -4.1748137 -4.1641641 -4.1344104 -4.0913286 -4.0329604 -3.9336455 -3.8599856 -3.9344528 -4.0566969 -4.1140466 -4.1171217 -4.0918326 -4.0754027 -4.0820832][-4.1783357 -4.1896062 -4.1899743 -4.1773257 -4.151269 -4.1184607 -4.0523891 -3.9909279 -4.0305352 -4.1171517 -4.157373 -4.1480913 -4.1132154 -4.0841823 -4.0806732][-4.1783009 -4.190115 -4.1952128 -4.1931472 -4.1780062 -4.1658287 -4.13266 -4.0975552 -4.1211133 -4.1717882 -4.1901264 -4.1710529 -4.1313133 -4.0903735 -4.0770812][-4.1714168 -4.188962 -4.198247 -4.2013445 -4.1935797 -4.1904645 -4.1776004 -4.1661716 -4.1846838 -4.21294 -4.2164984 -4.1930132 -4.1556029 -4.1173592 -4.0989218][-4.1713853 -4.1910343 -4.204155 -4.2122808 -4.2132826 -4.2159877 -4.2161245 -4.2158184 -4.2274489 -4.2452645 -4.2475233 -4.2285013 -4.2025533 -4.1763597 -4.1635222][-4.1845965 -4.2008333 -4.2178836 -4.2303877 -4.2389364 -4.2450418 -4.2490973 -4.2490296 -4.2541375 -4.2672968 -4.2694416 -4.2546363 -4.2382259 -4.2248907 -4.2227221][-4.2060132 -4.2171679 -4.2321429 -4.2456946 -4.257906 -4.2661657 -4.2685771 -4.2674689 -4.2724891 -4.28286 -4.2835293 -4.2740788 -4.2681689 -4.2644215 -4.2681913]]...]
INFO - root - 2017-12-05 14:31:40.062193: step 17110, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 74h:36m:45s remains)
INFO - root - 2017-12-05 14:31:48.613861: step 17120, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 73h:16m:00s remains)
INFO - root - 2017-12-05 14:31:57.108730: step 17130, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 75h:44m:58s remains)
INFO - root - 2017-12-05 14:32:05.475802: step 17140, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 74h:11m:26s remains)
INFO - root - 2017-12-05 14:32:13.938861: step 17150, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 0.810 sec/batch; 70h:54m:58s remains)
INFO - root - 2017-12-05 14:32:22.317112: step 17160, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 73h:45m:41s remains)
INFO - root - 2017-12-05 14:32:30.775331: step 17170, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 73h:50m:08s remains)
INFO - root - 2017-12-05 14:32:39.234673: step 17180, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 77h:28m:50s remains)
INFO - root - 2017-12-05 14:32:47.725869: step 17190, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 73h:54m:45s remains)
INFO - root - 2017-12-05 14:32:56.061661: step 17200, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 74h:12m:32s remains)
2017-12-05 14:32:56.800109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2277751 -4.2265558 -4.2201953 -4.21176 -4.2006416 -4.1877513 -4.1783962 -4.1816354 -4.1956706 -4.2130332 -4.2268338 -4.2357659 -4.245759 -4.2567043 -4.2645416][-4.2290673 -4.2270555 -4.2187438 -4.2071223 -4.1902456 -4.1690273 -4.1533437 -4.1571445 -4.1796432 -4.2082386 -4.2315116 -4.24606 -4.2562141 -4.263998 -4.2682233][-4.2292314 -4.226212 -4.21655 -4.2027035 -4.180295 -4.1503582 -4.12637 -4.1268296 -4.1536493 -4.1923676 -4.227 -4.2498388 -4.2626209 -4.2690644 -4.2709589][-4.2291446 -4.2254381 -4.215673 -4.2014575 -4.176177 -4.1398163 -4.1074662 -4.0995736 -4.1230087 -4.1676517 -4.2135448 -4.2460332 -4.2639179 -4.2713089 -4.2725396][-4.2273612 -4.2245955 -4.216423 -4.2038021 -4.1787567 -4.1395445 -4.1005688 -4.0812407 -4.0945716 -4.1403122 -4.1961918 -4.2382641 -4.2623005 -4.2719646 -4.2733469][-4.2200418 -4.220592 -4.2159624 -4.2070088 -4.185276 -4.1464982 -4.1041574 -4.0747027 -4.0744481 -4.1163354 -4.1787925 -4.2294211 -4.2600608 -4.2723289 -4.2740793][-4.2115574 -4.2164392 -4.2152495 -4.209868 -4.1931667 -4.158308 -4.1169767 -4.0827608 -4.070045 -4.1024408 -4.1643577 -4.2205672 -4.257338 -4.2729406 -4.2753539][-4.2152338 -4.2209306 -4.2192545 -4.21342 -4.1997209 -4.170413 -4.1323652 -4.0966644 -4.076622 -4.0985336 -4.1555247 -4.21331 -4.2548914 -4.27397 -4.2774496][-4.2233963 -4.2293797 -4.2272096 -4.2205286 -4.208962 -4.1852326 -4.1512723 -4.1154571 -4.0923829 -4.1055064 -4.1527748 -4.2073269 -4.2510366 -4.2743435 -4.2798138][-4.2315784 -4.2391758 -4.2379551 -4.2320232 -4.2224574 -4.2034574 -4.1725435 -4.1372981 -4.1127334 -4.1163673 -4.1503968 -4.1985221 -4.2429996 -4.2705994 -4.2796125][-4.2392 -4.2478089 -4.2479887 -4.2444382 -4.2387362 -4.2257519 -4.200346 -4.1671219 -4.1393929 -4.1323862 -4.1523147 -4.1910973 -4.2341957 -4.26487 -4.2769785][-4.2423682 -4.2509017 -4.2522883 -4.2516422 -4.2504058 -4.2445259 -4.2268314 -4.1976976 -4.1682358 -4.1524944 -4.1606245 -4.1903315 -4.2301011 -4.2604642 -4.2734232][-4.2363586 -4.2455897 -4.2488236 -4.2511659 -4.2532797 -4.2537374 -4.2450242 -4.2233629 -4.1970086 -4.177546 -4.1770148 -4.1972327 -4.2312903 -4.258841 -4.2710352][-4.2289457 -4.2393894 -4.2438874 -4.2470632 -4.2501612 -4.2538376 -4.2530541 -4.2412114 -4.2227359 -4.2058954 -4.2010837 -4.2129855 -4.2389655 -4.2609043 -4.2710376][-4.230206 -4.2398057 -4.2433271 -4.24488 -4.2468772 -4.2506218 -4.253387 -4.2492485 -4.2396851 -4.2299571 -4.2268238 -4.2346916 -4.2526326 -4.267508 -4.2734556]]...]
INFO - root - 2017-12-05 14:33:05.166101: step 17210, loss = 2.04, batch loss = 1.98 (10.3 examples/sec; 0.775 sec/batch; 67h:51m:37s remains)
INFO - root - 2017-12-05 14:33:13.716952: step 17220, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 76h:15m:04s remains)
INFO - root - 2017-12-05 14:33:22.344549: step 17230, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 75h:54m:25s remains)
INFO - root - 2017-12-05 14:33:30.913349: step 17240, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.834 sec/batch; 73h:03m:18s remains)
INFO - root - 2017-12-05 14:33:39.359915: step 17250, loss = 2.10, batch loss = 2.04 (9.8 examples/sec; 0.818 sec/batch; 71h:35m:49s remains)
INFO - root - 2017-12-05 14:33:47.868756: step 17260, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.811 sec/batch; 70h:58m:52s remains)
INFO - root - 2017-12-05 14:33:56.316579: step 17270, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 71h:35m:00s remains)
INFO - root - 2017-12-05 14:34:04.921212: step 17280, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 74h:21m:46s remains)
INFO - root - 2017-12-05 14:34:13.567595: step 17290, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 75h:29m:22s remains)
INFO - root - 2017-12-05 14:34:22.043166: step 17300, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 74h:05m:13s remains)
2017-12-05 14:34:22.841771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2334223 -4.2535582 -4.2673831 -4.2666974 -4.2558718 -4.2595625 -4.2754583 -4.2850933 -4.284421 -4.2875438 -4.2881474 -4.2822113 -4.2825031 -4.2891364 -4.2852631][-4.2493939 -4.2633591 -4.2746305 -4.2765579 -4.2695522 -4.2749753 -4.290381 -4.2946844 -4.2832561 -4.2750678 -4.2666593 -4.2541666 -4.2514615 -4.2564149 -4.2516026][-4.2585721 -4.2671356 -4.2739215 -4.2756796 -4.2691665 -4.2716842 -4.28605 -4.2903852 -4.2790174 -4.2682419 -4.2539907 -4.2340956 -4.2267814 -4.2280836 -4.220963][-4.2523651 -4.2519307 -4.2557383 -4.25738 -4.2495041 -4.2462978 -4.2584095 -4.2680593 -4.2648544 -4.2608075 -4.2464576 -4.2244773 -4.21452 -4.2148094 -4.2097187][-4.2361116 -4.2297606 -4.2324061 -4.2354851 -4.230391 -4.2227397 -4.2290525 -4.2394843 -4.2442684 -4.249239 -4.2413044 -4.2234473 -4.2158012 -4.2165923 -4.2119594][-4.2122602 -4.1981025 -4.1931376 -4.1945963 -4.1922183 -4.1876874 -4.1929679 -4.2070484 -4.2214527 -4.2377787 -4.2416306 -4.2315049 -4.2268386 -4.2261271 -4.2190318][-4.1762376 -4.1514993 -4.1326013 -4.1239195 -4.1184511 -4.1208081 -4.1291838 -4.1445441 -4.167068 -4.1958303 -4.2116761 -4.2103176 -4.2126884 -4.2172947 -4.2148561][-4.1406541 -4.1048241 -4.0709391 -4.0485253 -4.0340567 -4.0391111 -4.0462747 -4.0564666 -4.0833783 -4.1250739 -4.1591687 -4.1731653 -4.189518 -4.2060084 -4.2151294][-4.11536 -4.0713716 -4.0279913 -3.9971561 -3.9767113 -3.9817319 -3.989012 -3.9950652 -4.01996 -4.0728745 -4.1269803 -4.1617732 -4.1942558 -4.2235055 -4.2415738][-4.1199708 -4.079143 -4.0384283 -4.0096927 -3.99209 -3.9929404 -4.0007305 -4.0076146 -4.0237136 -4.0684438 -4.1245613 -4.16959 -4.2086463 -4.2406774 -4.2600489][-4.1542859 -4.1257854 -4.0978045 -4.0797625 -4.07009 -4.0716381 -4.0814848 -4.0917916 -4.0993624 -4.1247864 -4.1648664 -4.2023349 -4.2332611 -4.2554741 -4.2698441][-4.186954 -4.1705041 -4.1538935 -4.1447706 -4.1465297 -4.1552548 -4.1709647 -4.1851993 -4.18799 -4.1969466 -4.2151666 -4.236155 -4.2493081 -4.2554073 -4.26179][-4.1917415 -4.1844769 -4.1764541 -4.1741538 -4.1899271 -4.2085757 -4.2327709 -4.2494154 -4.250021 -4.2460818 -4.2458582 -4.24992 -4.24744 -4.2403321 -4.2388144][-4.1622219 -4.1631236 -4.1641936 -4.1689916 -4.19822 -4.2264452 -4.2555861 -4.2732282 -4.2748833 -4.2656527 -4.2545433 -4.2485 -4.2365341 -4.2213264 -4.2130775][-4.1240144 -4.1329679 -4.13982 -4.1462049 -4.1725831 -4.2000747 -4.2273245 -4.2425003 -4.2467465 -4.2386546 -4.2256942 -4.2172031 -4.2039452 -4.1891289 -4.1797695]]...]
INFO - root - 2017-12-05 14:34:31.175651: step 17310, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 62h:48m:36s remains)
INFO - root - 2017-12-05 14:34:39.531250: step 17320, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 73h:36m:54s remains)
INFO - root - 2017-12-05 14:34:48.029177: step 17330, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 75h:41m:14s remains)
INFO - root - 2017-12-05 14:34:56.506572: step 17340, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.833 sec/batch; 72h:53m:55s remains)
INFO - root - 2017-12-05 14:35:04.948053: step 17350, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 75h:45m:06s remains)
INFO - root - 2017-12-05 14:35:13.313222: step 17360, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 77h:15m:33s remains)
INFO - root - 2017-12-05 14:35:21.719520: step 17370, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.828 sec/batch; 72h:27m:32s remains)
INFO - root - 2017-12-05 14:35:30.055898: step 17380, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 72h:22m:45s remains)
INFO - root - 2017-12-05 14:35:38.499201: step 17390, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 75h:05m:41s remains)
INFO - root - 2017-12-05 14:35:47.020908: step 17400, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 72h:49m:49s remains)
2017-12-05 14:35:47.763047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1490316 -4.14143 -4.125021 -4.1044946 -4.0908713 -4.1052389 -4.1519694 -4.1967378 -4.2240028 -4.2521758 -4.2664146 -4.2622805 -4.2557845 -4.2539539 -4.2554893][-4.157218 -4.1456356 -4.1230135 -4.1005726 -4.08579 -4.0981922 -4.145287 -4.1949863 -4.229001 -4.2573981 -4.27293 -4.2697349 -4.2633567 -4.2584357 -4.2544861][-4.1824269 -4.1706214 -4.1484957 -4.1244841 -4.1095109 -4.1144409 -4.1528215 -4.1965108 -4.2267647 -4.2499995 -4.2627397 -4.26144 -4.2565818 -4.2525411 -4.2496009][-4.2213974 -4.2068148 -4.18315 -4.1588273 -4.1464524 -4.145505 -4.1609554 -4.1870847 -4.2104268 -4.2282281 -4.2401452 -4.2413259 -4.2347507 -4.2314091 -4.2295742][-4.2388525 -4.219676 -4.1923046 -4.174468 -4.1652894 -4.1577497 -4.1509433 -4.1543593 -4.1658416 -4.1836791 -4.2023182 -4.2101812 -4.2036448 -4.1959105 -4.1946659][-4.2347951 -4.2120228 -4.1827188 -4.1697907 -4.1614943 -4.1403837 -4.1113081 -4.092989 -4.0919976 -4.1173167 -4.150435 -4.1676173 -4.1649504 -4.1535444 -4.1490808][-4.212152 -4.1801076 -4.1439486 -4.1284008 -4.1137748 -4.0762129 -4.0192232 -3.9676852 -3.9578316 -4.0196033 -4.0868058 -4.119339 -4.118917 -4.1004262 -4.0900764][-4.1875052 -4.1476979 -4.1022358 -4.0762663 -4.0508604 -3.998524 -3.9054091 -3.8015366 -3.7825041 -3.9082849 -4.0225916 -4.0720267 -4.0751944 -4.0555825 -4.0402379][-4.1746364 -4.1356764 -4.0883946 -4.055162 -4.0274248 -3.9731457 -3.8606944 -3.7116394 -3.6815915 -3.8516355 -3.9845836 -4.0396385 -4.0474467 -4.0384455 -4.0330815][-4.1774855 -4.147531 -4.1108437 -4.0835629 -4.0661726 -4.0286331 -3.9386613 -3.8177328 -3.8048265 -3.931139 -4.0256772 -4.061892 -4.057889 -4.0555825 -4.0639515][-4.1842842 -4.1630487 -4.1423588 -4.1299953 -4.1294322 -4.112999 -4.0618682 -3.9883265 -3.9839296 -4.049614 -4.0967126 -4.1099682 -4.091166 -4.0890069 -4.1077576][-4.1923051 -4.1732984 -4.1599994 -4.1598706 -4.1733584 -4.174561 -4.1524634 -4.1151438 -4.1122608 -4.1339889 -4.1499238 -4.1482854 -4.1254368 -4.1244292 -4.15056][-4.1990643 -4.1826773 -4.17307 -4.1766882 -4.194777 -4.2059326 -4.2033205 -4.1896863 -4.1857829 -4.1868882 -4.1892028 -4.1826048 -4.1641536 -4.1657653 -4.1883659][-4.2016058 -4.1913848 -4.1872921 -4.1896739 -4.201962 -4.215477 -4.2225833 -4.2202158 -4.2165589 -4.2146125 -4.2192221 -4.2180219 -4.2094388 -4.2130194 -4.2282944][-4.2202716 -4.2159691 -4.2143531 -4.215518 -4.2214231 -4.2312222 -4.2401366 -4.2440825 -4.2420449 -4.2385654 -4.243012 -4.2464104 -4.2471986 -4.2539926 -4.2603889]]...]
INFO - root - 2017-12-05 14:35:56.223988: step 17410, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 73h:53m:22s remains)
INFO - root - 2017-12-05 14:36:04.556893: step 17420, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 74h:53m:58s remains)
INFO - root - 2017-12-05 14:36:13.109211: step 17430, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 75h:39m:28s remains)
INFO - root - 2017-12-05 14:36:21.588321: step 17440, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 73h:56m:37s remains)
INFO - root - 2017-12-05 14:36:30.114079: step 17450, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 78h:21m:19s remains)
INFO - root - 2017-12-05 14:36:38.519553: step 17460, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 74h:57m:49s remains)
INFO - root - 2017-12-05 14:36:47.111903: step 17470, loss = 2.02, batch loss = 1.96 (9.2 examples/sec; 0.867 sec/batch; 75h:52m:20s remains)
INFO - root - 2017-12-05 14:36:55.587186: step 17480, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 75h:49m:49s remains)
INFO - root - 2017-12-05 14:37:04.170357: step 17490, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 76h:42m:35s remains)
INFO - root - 2017-12-05 14:37:12.651623: step 17500, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 77h:22m:44s remains)
2017-12-05 14:37:13.379957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2723193 -4.2664194 -4.2660623 -4.2756095 -4.2833948 -4.2908115 -4.3013778 -4.3093429 -4.3146434 -4.3177505 -4.3172255 -4.3147063 -4.3127065 -4.310647 -4.3082604][-4.2315073 -4.2238779 -4.2234311 -4.2357144 -4.245563 -4.2518253 -4.26478 -4.280592 -4.2950325 -4.3041177 -4.3041797 -4.2995529 -4.2965622 -4.2952595 -4.2909961][-4.176971 -4.1726012 -4.1724691 -4.1822066 -4.18836 -4.1889453 -4.2004013 -4.2236681 -4.2517028 -4.2748728 -4.2839708 -4.2818527 -4.2797127 -4.2777576 -4.2688618][-4.1263175 -4.13581 -4.1351476 -4.1400585 -4.1390734 -4.122303 -4.11521 -4.1348205 -4.1798253 -4.2243156 -4.2496319 -4.2540522 -4.2546563 -4.2514582 -4.2332363][-4.08014 -4.1088223 -4.1115561 -4.1120052 -4.1011744 -4.0578442 -4.0114231 -4.0143237 -4.083334 -4.1599059 -4.2050695 -4.2161689 -4.2163568 -4.2070823 -4.1700177][-4.0233908 -4.0751591 -4.0850143 -4.0834169 -4.0611153 -3.9829605 -3.87898 -3.8549721 -3.9618301 -4.0805068 -4.1507063 -4.1740174 -4.1734676 -4.152472 -4.0839872][-3.9835432 -4.0502305 -4.0646091 -4.0599675 -4.0248094 -3.906621 -3.7293968 -3.6599932 -3.8140531 -3.9885273 -4.0867553 -4.12317 -4.1230574 -4.0922861 -3.9912658][-3.9961541 -4.057663 -4.0677657 -4.0618935 -4.026639 -3.8974342 -3.6850975 -3.5659475 -3.7263436 -3.9275351 -4.0390162 -4.0846844 -4.0913458 -4.0588622 -3.9448771][-4.0552511 -4.1010213 -4.108 -4.1073442 -4.0840311 -3.9815571 -3.8117943 -3.6960626 -3.794405 -3.9578228 -4.051682 -4.0923014 -4.1050982 -4.0835261 -3.9889624][-4.117372 -4.1450396 -4.1513271 -4.1592546 -4.1503921 -4.0841846 -3.9776895 -3.8902135 -3.9311132 -4.0348029 -4.1000714 -4.1262169 -4.13738 -4.1299005 -4.0685315][-4.161952 -4.1711736 -4.1771331 -4.19193 -4.1970124 -4.1616197 -4.1036358 -4.0436425 -4.0519333 -4.1089549 -4.146193 -4.1623659 -4.1718936 -4.170979 -4.1397371][-4.1827359 -4.1798105 -4.1836643 -4.2035766 -4.2196369 -4.2074018 -4.1780062 -4.1387219 -4.133184 -4.1607347 -4.1778979 -4.1884165 -4.1965308 -4.1995416 -4.1871877][-4.2043986 -4.1950555 -4.1969943 -4.2143273 -4.2312074 -4.2312107 -4.2178383 -4.1944685 -4.1890168 -4.2016582 -4.2081914 -4.2127213 -4.2202711 -4.2255945 -4.2211828][-4.2487559 -4.2388206 -4.2383533 -4.2473121 -4.2587247 -4.2639742 -4.2618361 -4.2538576 -4.2540483 -4.2601566 -4.2612014 -4.2610364 -4.2650094 -4.26826 -4.2651839][-4.2937179 -4.2884173 -4.2865543 -4.2897053 -4.2953334 -4.2997556 -4.3017783 -4.3018203 -4.3050303 -4.3089185 -4.3095112 -4.3084707 -4.3087707 -4.3091049 -4.3069797]]...]
INFO - root - 2017-12-05 14:37:21.833844: step 17510, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 74h:01m:35s remains)
INFO - root - 2017-12-05 14:37:30.274877: step 17520, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 74h:49m:40s remains)
INFO - root - 2017-12-05 14:37:38.649146: step 17530, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 71h:31m:17s remains)
INFO - root - 2017-12-05 14:37:47.106214: step 17540, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 74h:00m:37s remains)
INFO - root - 2017-12-05 14:37:55.641746: step 17550, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 74h:11m:12s remains)
INFO - root - 2017-12-05 14:38:04.095510: step 17560, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.829 sec/batch; 72h:30m:48s remains)
INFO - root - 2017-12-05 14:38:12.639706: step 17570, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 74h:52m:18s remains)
INFO - root - 2017-12-05 14:38:21.127658: step 17580, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 75h:48m:40s remains)
INFO - root - 2017-12-05 14:38:29.649656: step 17590, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 73h:41m:07s remains)
INFO - root - 2017-12-05 14:38:38.173726: step 17600, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 73h:33m:46s remains)
2017-12-05 14:38:38.951018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2203922 -4.2310987 -4.2424927 -4.2506742 -4.2577267 -4.2599716 -4.2602844 -4.2600045 -4.2619452 -4.2672014 -4.2715673 -4.2747173 -4.2767024 -4.2769766 -4.2747478][-4.1924977 -4.2083921 -4.2256026 -4.2392449 -4.2499561 -4.255096 -4.2573686 -4.2589016 -4.2613573 -4.2635794 -4.2627711 -4.2600145 -4.2559581 -4.2555552 -4.2580976][-4.1815281 -4.1990609 -4.2170558 -4.2339396 -4.2489929 -4.2563252 -4.2595358 -4.2619815 -4.2643352 -4.2659926 -4.2620373 -4.2542753 -4.2436504 -4.2393827 -4.2419825][-4.1924577 -4.206521 -4.21813 -4.2307138 -4.2446957 -4.2509327 -4.2534904 -4.2565279 -4.2622137 -4.267931 -4.2689776 -4.2625771 -4.2485018 -4.2428284 -4.2438769][-4.2175765 -4.2244058 -4.2248235 -4.22784 -4.2320333 -4.2322249 -4.2275724 -4.2248688 -4.2311249 -4.243989 -4.2565317 -4.2597866 -4.2485676 -4.2446523 -4.247005][-4.2263179 -4.2206569 -4.2035837 -4.1871567 -4.1738176 -4.1613297 -4.1404972 -4.1255035 -4.1357217 -4.1630368 -4.1937504 -4.2157416 -4.2191629 -4.219871 -4.2215195][-4.1886125 -4.1627393 -4.1256337 -4.0878739 -4.0531645 -4.0199237 -3.9738612 -3.9402997 -3.9632773 -4.0180016 -4.0775275 -4.1246948 -4.1439557 -4.1498833 -4.1473827][-4.1162353 -4.0696278 -4.016583 -3.9595563 -3.8968546 -3.8347569 -3.7605603 -3.7079129 -3.7514384 -3.8382185 -3.9242544 -3.9918308 -4.0264053 -4.0386653 -4.041255][-4.06875 -4.0183668 -3.9681215 -3.912086 -3.8469386 -3.7874684 -3.7262671 -3.6872959 -3.7325525 -3.8077798 -3.8768358 -3.9318767 -3.9639621 -3.979476 -3.9917464][-4.0843506 -4.0529027 -4.0208268 -3.9820867 -3.9405978 -3.9133136 -3.8927822 -3.8809078 -3.9081042 -3.9451118 -3.977114 -4.005064 -4.02093 -4.0283804 -4.0404539][-4.1398993 -4.1298141 -4.1147513 -4.09289 -4.0722995 -4.0672226 -4.0696449 -4.0709252 -4.0857105 -4.1024256 -4.1175523 -4.1321936 -4.1385126 -4.139843 -4.1468349][-4.2102737 -4.2102361 -4.2058311 -4.1970987 -4.1904778 -4.1947184 -4.2036996 -4.20794 -4.2153959 -4.2225146 -4.2287755 -4.2365632 -4.2393727 -4.240952 -4.2460713][-4.2623038 -4.2660766 -4.2656503 -4.2639966 -4.2624035 -4.2665887 -4.2730985 -4.2771292 -4.2821975 -4.2861423 -4.2890229 -4.2931437 -4.2956009 -4.2969794 -4.2986746][-4.2624664 -4.2668533 -4.2677073 -4.2676849 -4.2656636 -4.2664089 -4.2694697 -4.2722917 -4.27559 -4.278213 -4.2800331 -4.2830191 -4.2855382 -4.2875404 -4.2880921][-4.2389221 -4.2413921 -4.2400913 -4.2381134 -4.2353783 -4.2340546 -4.2351246 -4.2365317 -4.2384539 -4.2409325 -4.242908 -4.2451491 -4.2466083 -4.2474422 -4.24742]]...]
INFO - root - 2017-12-05 14:38:47.516412: step 17610, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 74h:11m:39s remains)
INFO - root - 2017-12-05 14:38:55.928357: step 17620, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 72h:21m:02s remains)
INFO - root - 2017-12-05 14:39:04.489754: step 17630, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 76h:01m:11s remains)
INFO - root - 2017-12-05 14:39:12.800436: step 17640, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 74h:48m:47s remains)
INFO - root - 2017-12-05 14:39:21.224938: step 17650, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 73h:58m:48s remains)
INFO - root - 2017-12-05 14:39:29.777726: step 17660, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 73h:24m:05s remains)
INFO - root - 2017-12-05 14:39:38.289701: step 17670, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 72h:45m:48s remains)
INFO - root - 2017-12-05 14:39:46.833834: step 17680, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 75h:43m:58s remains)
INFO - root - 2017-12-05 14:39:55.350842: step 17690, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 73h:14m:13s remains)
INFO - root - 2017-12-05 14:40:03.798988: step 17700, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 76h:38m:48s remains)
2017-12-05 14:40:04.567199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1860065 -4.185297 -4.1760397 -4.16151 -4.1416516 -4.12664 -4.1085238 -4.0847116 -4.0836773 -4.12322 -4.1711955 -4.2128243 -4.2583089 -4.2848687 -4.2908759][-4.2008967 -4.2044506 -4.1955738 -4.181849 -4.1632638 -4.1459246 -4.1208863 -4.0911236 -4.0835457 -4.1132131 -4.1574578 -4.2043319 -4.2528439 -4.2826028 -4.2909088][-4.2056255 -4.2111998 -4.2009544 -4.1844 -4.1643329 -4.1479168 -4.1199079 -4.0899873 -4.0833406 -4.108542 -4.1497197 -4.1988192 -4.250083 -4.2826533 -4.2932439][-4.1968961 -4.2048411 -4.1938348 -4.1769295 -4.1587276 -4.1426392 -4.108777 -4.0741963 -4.0719914 -4.1031566 -4.1470003 -4.1959105 -4.24843 -4.2824373 -4.2944927][-4.1799603 -4.1887188 -4.1788759 -4.1660466 -4.1492767 -4.1279387 -4.0857573 -4.0410924 -4.039444 -4.0843267 -4.135519 -4.1864309 -4.24225 -4.2808738 -4.2955222][-4.1698565 -4.1689572 -4.1590257 -4.1489348 -4.1243038 -4.0877333 -4.0319819 -3.9731128 -3.973536 -4.0473204 -4.11753 -4.1763048 -4.239531 -4.2824206 -4.296525][-4.1748824 -4.1659861 -4.1569104 -4.1455965 -4.1051908 -4.0388 -3.9536085 -3.8689151 -3.8650384 -3.9722185 -4.07662 -4.1559677 -4.2319784 -4.2824678 -4.2962184][-4.1966753 -4.1895909 -4.1852608 -4.1752882 -4.1276331 -4.0412817 -3.9275861 -3.8020492 -3.7647092 -3.8790045 -4.0099611 -4.1128373 -4.20748 -4.2710428 -4.2913742][-4.2052512 -4.2133646 -4.2234039 -4.2232118 -4.1866736 -4.1130939 -4.0037231 -3.864186 -3.7932308 -3.8716846 -3.98592 -4.0833006 -4.1838307 -4.2571759 -4.2850471][-4.1803379 -4.2053986 -4.2330532 -4.2479444 -4.2335076 -4.1864381 -4.1042438 -3.9881968 -3.9193017 -3.9673941 -4.0472836 -4.1144595 -4.1941667 -4.2595544 -4.2874317][-4.1440239 -4.175386 -4.2160697 -4.2395148 -4.239769 -4.2164612 -4.1619959 -4.0772109 -4.0285606 -4.0672312 -4.1302757 -4.1777115 -4.2342958 -4.2835479 -4.3032436][-4.158637 -4.1837912 -4.2183161 -4.23707 -4.2386942 -4.2253132 -4.1855121 -4.1254406 -4.0961666 -4.1300888 -4.1842704 -4.2238445 -4.2662354 -4.30111 -4.3131618][-4.2112737 -4.2306204 -4.2543941 -4.26414 -4.26203 -4.2503018 -4.2180743 -4.1728005 -4.1533456 -4.1774507 -4.2187614 -4.2526717 -4.284605 -4.3088326 -4.31709][-4.2566829 -4.2722588 -4.2901525 -4.297863 -4.2965913 -4.2880964 -4.2646852 -4.2310405 -4.2140059 -4.2260852 -4.2524257 -4.2780895 -4.3020921 -4.3199282 -4.3242259][-4.2852597 -4.2967334 -4.308506 -4.3147674 -4.3149405 -4.309989 -4.296062 -4.2754979 -4.2626548 -4.2651825 -4.2799149 -4.2983809 -4.3156757 -4.3271 -4.3267727]]...]
INFO - root - 2017-12-05 14:40:13.041222: step 17710, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 0.812 sec/batch; 70h:59m:09s remains)
INFO - root - 2017-12-05 14:40:21.292412: step 17720, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 74h:21m:31s remains)
INFO - root - 2017-12-05 14:40:29.689691: step 17730, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 75h:23m:09s remains)
INFO - root - 2017-12-05 14:40:38.000252: step 17740, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.751 sec/batch; 65h:38m:33s remains)
INFO - root - 2017-12-05 14:40:46.261264: step 17750, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.847 sec/batch; 74h:00m:53s remains)
INFO - root - 2017-12-05 14:40:54.662804: step 17760, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 73h:05m:08s remains)
INFO - root - 2017-12-05 14:41:03.032427: step 17770, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 74h:09m:42s remains)
INFO - root - 2017-12-05 14:41:11.579372: step 17780, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 72h:18m:34s remains)
INFO - root - 2017-12-05 14:41:20.078378: step 17790, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 75h:02m:47s remains)
INFO - root - 2017-12-05 14:41:28.611750: step 17800, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 77h:36m:26s remains)
2017-12-05 14:41:29.345170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3109674 -4.2929 -4.2692266 -4.2483406 -4.2427616 -4.2510138 -4.2635307 -4.2745256 -4.281105 -4.2866864 -4.2922268 -4.297287 -4.2986 -4.2952151 -4.2905064][-4.3054218 -4.2794676 -4.2515779 -4.2329516 -4.23306 -4.244669 -4.2571769 -4.2679415 -4.2753286 -4.2811971 -4.2864151 -4.2896161 -4.2861462 -4.2754922 -4.2677417][-4.2935877 -4.2645721 -4.2392721 -4.2280116 -4.2335358 -4.2435508 -4.2496424 -4.2548814 -4.2587786 -4.2621679 -4.2661724 -4.2672615 -4.2598295 -4.2461705 -4.2398334][-4.2855434 -4.2618618 -4.2443442 -4.2380185 -4.2424111 -4.2433152 -4.2355022 -4.2278972 -4.2230549 -4.2255359 -4.23325 -4.237566 -4.2330165 -4.2247238 -4.2257986][-4.2863507 -4.2717657 -4.261157 -4.2546434 -4.2513952 -4.2399335 -4.2144489 -4.185791 -4.1688905 -4.1783285 -4.1994915 -4.21445 -4.22074 -4.2233214 -4.2328415][-4.2925425 -4.2847176 -4.2780566 -4.2687988 -4.2562852 -4.2315292 -4.1862073 -4.1320071 -4.1083198 -4.1360693 -4.1770062 -4.2048807 -4.2224984 -4.2358465 -4.2521782][-4.2903786 -4.2819963 -4.2751889 -4.2615557 -4.2391453 -4.2023239 -4.1414824 -4.0728235 -4.0613484 -4.1159177 -4.1714945 -4.2043858 -4.2245588 -4.2399073 -4.2581863][-4.2746172 -4.2629781 -4.2552843 -4.238503 -4.2100883 -4.1699448 -4.1133928 -4.0616927 -4.0746775 -4.134881 -4.1844897 -4.2096567 -4.2206335 -4.2288742 -4.2466908][-4.2651434 -4.2504845 -4.2394361 -4.2197537 -4.1902428 -4.1571846 -4.1245308 -4.1092243 -4.1325889 -4.1722679 -4.2013054 -4.2117639 -4.2074237 -4.2062926 -4.2266555][-4.2655015 -4.2491493 -4.2328019 -4.2098846 -4.181438 -4.1584568 -4.152256 -4.1613441 -4.1807365 -4.19894 -4.2117491 -4.2124743 -4.1971183 -4.18793 -4.2078357][-4.2702827 -4.2511868 -4.2293062 -4.2038279 -4.1787062 -4.1671748 -4.1793947 -4.1977329 -4.2110238 -4.2155418 -4.218513 -4.215744 -4.1974607 -4.1837592 -4.1991811][-4.2698298 -4.24759 -4.2205987 -4.1946697 -4.1735764 -4.1704431 -4.1899586 -4.2106714 -4.221756 -4.221034 -4.2199044 -4.2172041 -4.2023625 -4.1887994 -4.1985807][-4.2502646 -4.2272706 -4.198585 -4.1767082 -4.1605272 -4.1609449 -4.1783981 -4.1970015 -4.20979 -4.2115746 -4.2104988 -4.2098379 -4.2010241 -4.1920881 -4.1988778][-4.2216034 -4.2026262 -4.1763024 -4.1593785 -4.1491981 -4.1511517 -4.1632853 -4.1767058 -4.1898742 -4.1937575 -4.1934423 -4.1952825 -4.1914105 -4.1877985 -4.1929522][-4.1976075 -4.1831188 -4.1632342 -4.152472 -4.14958 -4.1544657 -4.1623416 -4.1701689 -4.1809239 -4.1838121 -4.182663 -4.1845169 -4.1833205 -4.18348 -4.1879473]]...]
INFO - root - 2017-12-05 14:41:37.813849: step 17810, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 74h:24m:02s remains)
INFO - root - 2017-12-05 14:41:46.201586: step 17820, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 76h:36m:11s remains)
INFO - root - 2017-12-05 14:41:54.675498: step 17830, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 76h:59m:54s remains)
INFO - root - 2017-12-05 14:42:03.093255: step 17840, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 73h:55m:09s remains)
INFO - root - 2017-12-05 14:42:11.285466: step 17850, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.809 sec/batch; 70h:40m:20s remains)
INFO - root - 2017-12-05 14:42:19.724179: step 17860, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.829 sec/batch; 72h:26m:50s remains)
INFO - root - 2017-12-05 14:42:28.161051: step 17870, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 73h:55m:49s remains)
INFO - root - 2017-12-05 14:42:36.561286: step 17880, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 73h:01m:43s remains)
INFO - root - 2017-12-05 14:42:44.959008: step 17890, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 74h:36m:14s remains)
INFO - root - 2017-12-05 14:42:53.397116: step 17900, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 74h:36m:34s remains)
2017-12-05 14:42:54.103137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1624494 -4.1609559 -4.182272 -4.211277 -4.2241149 -4.2244344 -4.2191548 -4.2151194 -4.2061081 -4.1865315 -4.146636 -4.1183863 -4.1265693 -4.1594453 -4.2013621][-4.1851196 -4.1757579 -4.1917 -4.2194672 -4.2331033 -4.2331505 -4.2272553 -4.2226391 -4.2134948 -4.1945348 -4.1598549 -4.1317806 -4.1385388 -4.1707444 -4.2110062][-4.219574 -4.2071095 -4.2171888 -4.2379565 -4.25042 -4.2518659 -4.247581 -4.244689 -4.2413716 -4.2288461 -4.2004313 -4.1693134 -4.1672359 -4.1915932 -4.2218938][-4.2568769 -4.24699 -4.2492828 -4.2581782 -4.265069 -4.2688327 -4.2669139 -4.2681017 -4.271039 -4.2675519 -4.24947 -4.2185764 -4.2096429 -4.2239027 -4.2427058][-4.2683783 -4.2660251 -4.2700872 -4.2761173 -4.2786841 -4.2821131 -4.283155 -4.2886295 -4.2962766 -4.3026681 -4.2967086 -4.2718749 -4.2599273 -4.2668557 -4.2765861][-4.2420754 -4.24389 -4.2507734 -4.2540355 -4.2552314 -4.2590871 -4.264976 -4.2756243 -4.2906785 -4.3047423 -4.3111873 -4.30401 -4.2991 -4.304203 -4.3075943][-4.1911449 -4.1969213 -4.2081976 -4.2053776 -4.201107 -4.2070322 -4.2215133 -4.2388563 -4.259181 -4.2783737 -4.2907157 -4.2987452 -4.3061242 -4.3114996 -4.3146653][-4.1374574 -4.148809 -4.1627574 -4.1526194 -4.1403413 -4.1433172 -4.1621933 -4.1864362 -4.2123308 -4.2343168 -4.2492003 -4.263556 -4.2782454 -4.2889962 -4.2987728][-4.1308703 -4.1408515 -4.1468377 -4.1269646 -4.1064892 -4.1005526 -4.1169858 -4.1393957 -4.1633658 -4.1842551 -4.1976719 -4.2148356 -4.2337651 -4.2524652 -4.27089][-4.1712594 -4.1763453 -4.1719923 -4.1464233 -4.1230006 -4.11123 -4.1174364 -4.1283636 -4.1429386 -4.1536984 -4.1569214 -4.1671462 -4.18473 -4.2065477 -4.2323675][-4.2210679 -4.2232242 -4.2165847 -4.1958971 -4.1780367 -4.1656165 -4.1629858 -4.1601372 -4.1616607 -4.1563425 -4.141573 -4.1339078 -4.1410112 -4.1642303 -4.1999826][-4.2205811 -4.2272005 -4.2256136 -4.2153597 -4.2087574 -4.2012711 -4.1959457 -4.1856475 -4.1758327 -4.1579685 -4.1265526 -4.0981913 -4.0936036 -4.118412 -4.1635823][-4.1882157 -4.2015195 -4.20698 -4.2057471 -4.2064519 -4.2031436 -4.1958132 -4.1814265 -4.1658726 -4.1417971 -4.1013756 -4.0599279 -4.0471854 -4.0731945 -4.1257925][-4.1911025 -4.2009478 -4.205163 -4.2047377 -4.2058973 -4.2045527 -4.1975222 -4.1848688 -4.1711674 -4.1482024 -4.1060519 -4.0635653 -4.0497627 -4.0741878 -4.1258879][-4.2114334 -4.2145357 -4.2157 -4.2158008 -4.2172251 -4.2166681 -4.2106628 -4.2027564 -4.1967096 -4.1802292 -4.1412024 -4.1014366 -4.0874496 -4.10834 -4.147995]]...]
INFO - root - 2017-12-05 14:43:02.608146: step 17910, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 75h:00m:34s remains)
INFO - root - 2017-12-05 14:43:11.035745: step 17920, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 74h:35m:30s remains)
INFO - root - 2017-12-05 14:43:19.619876: step 17930, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 74h:00m:34s remains)
INFO - root - 2017-12-05 14:43:28.139339: step 17940, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.810 sec/batch; 70h:48m:05s remains)
INFO - root - 2017-12-05 14:43:36.618167: step 17950, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 76h:30m:52s remains)
INFO - root - 2017-12-05 14:43:45.050882: step 17960, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 75h:03m:27s remains)
INFO - root - 2017-12-05 14:43:53.464305: step 17970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 74h:53m:18s remains)
INFO - root - 2017-12-05 14:44:02.057418: step 17980, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 75h:03m:10s remains)
INFO - root - 2017-12-05 14:44:10.566493: step 17990, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 73h:53m:00s remains)
INFO - root - 2017-12-05 14:44:18.978113: step 18000, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 74h:39m:59s remains)
2017-12-05 14:44:19.768503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3218989 -4.3039732 -4.2831526 -4.2613139 -4.2387362 -4.2218838 -4.218307 -4.2287965 -4.2492242 -4.2695189 -4.2835231 -4.2910075 -4.2938886 -4.2841063 -4.2633524][-4.3156586 -4.2931786 -4.2650042 -4.2372894 -4.2111416 -4.1946564 -4.1922369 -4.2040358 -4.2303443 -4.2547584 -4.2713785 -4.2775021 -4.277657 -4.263845 -4.2382855][-4.31043 -4.2855883 -4.2520914 -4.2184629 -4.1895347 -4.1740341 -4.1716266 -4.1837387 -4.21254 -4.2378759 -4.2578011 -4.2666445 -4.26587 -4.2502508 -4.2250953][-4.3063464 -4.2814908 -4.2468452 -4.2099524 -4.180799 -4.168232 -4.1666565 -4.1764493 -4.1995425 -4.2234421 -4.2469621 -4.2619276 -4.2604651 -4.2459674 -4.2242932][-4.301939 -4.2789164 -4.2478971 -4.2131438 -4.1851783 -4.1733823 -4.1696744 -4.1708431 -4.1812963 -4.2027464 -4.2323256 -4.2555923 -4.2564969 -4.2482934 -4.2348046][-4.2972918 -4.2756033 -4.24898 -4.2204514 -4.194169 -4.1794128 -4.1685128 -4.1565452 -4.1523795 -4.1698132 -4.2064819 -4.2446542 -4.2538753 -4.2548881 -4.2486405][-4.2930427 -4.2708545 -4.2450891 -4.2180152 -4.1906362 -4.1707196 -4.1504912 -4.1234913 -4.1091218 -4.128839 -4.1755347 -4.2311845 -4.2552481 -4.2630444 -4.2593546][-4.2907538 -4.2671208 -4.239347 -4.2083364 -4.1767979 -4.1475463 -4.1160259 -4.0792046 -4.0604668 -4.0894113 -4.1471004 -4.2147946 -4.2515507 -4.2644653 -4.2600379][-4.2922754 -4.2695441 -4.2410326 -4.2074876 -4.1684065 -4.127553 -4.0838242 -4.0406642 -4.0234003 -4.0617995 -4.13054 -4.2024684 -4.2425032 -4.2566957 -4.2510819][-4.2952194 -4.2754374 -4.2495651 -4.2150316 -4.1720076 -4.1257477 -4.0757885 -4.02922 -4.0133166 -4.0577369 -4.1340518 -4.2043214 -4.2404895 -4.251523 -4.2431946][-4.298089 -4.2784572 -4.2541871 -4.2221532 -4.1843004 -4.1457362 -4.1027055 -4.059351 -4.0464344 -4.0885668 -4.1604528 -4.2212057 -4.2522449 -4.2607589 -4.2506976][-4.2993655 -4.2766314 -4.2517409 -4.2216654 -4.1904917 -4.1645536 -4.1376081 -4.1078467 -4.101769 -4.1349525 -4.1899323 -4.2357335 -4.2620983 -4.2683883 -4.257822][-4.2998037 -4.2739329 -4.2448354 -4.2134428 -4.1876841 -4.16928 -4.1561251 -4.1453953 -4.1540036 -4.1826248 -4.2212062 -4.253376 -4.2735977 -4.2762084 -4.2659063][-4.3003392 -4.2712712 -4.237062 -4.2036467 -4.1820965 -4.1723404 -4.1711683 -4.1778398 -4.2002597 -4.2287569 -4.2572436 -4.2790794 -4.2888441 -4.2836103 -4.2697983][-4.3045897 -4.2736487 -4.2365618 -4.2043347 -4.1870842 -4.1862583 -4.1967554 -4.213829 -4.2394381 -4.2661867 -4.2863474 -4.2985568 -4.2993774 -4.287653 -4.2727747]]...]
INFO - root - 2017-12-05 14:44:28.189725: step 18010, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 72h:17m:34s remains)
INFO - root - 2017-12-05 14:44:36.624696: step 18020, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 74h:23m:32s remains)
INFO - root - 2017-12-05 14:44:45.178139: step 18030, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 76h:53m:44s remains)
INFO - root - 2017-12-05 14:44:53.727589: step 18040, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.859 sec/batch; 75h:03m:14s remains)
INFO - root - 2017-12-05 14:45:02.231663: step 18050, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 72h:59m:21s remains)
INFO - root - 2017-12-05 14:45:10.804725: step 18060, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 75h:56m:51s remains)
INFO - root - 2017-12-05 14:45:19.335776: step 18070, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 74h:48m:55s remains)
INFO - root - 2017-12-05 14:45:27.690818: step 18080, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 72h:33m:31s remains)
INFO - root - 2017-12-05 14:45:36.201902: step 18090, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.813 sec/batch; 71h:02m:30s remains)
INFO - root - 2017-12-05 14:45:44.724187: step 18100, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 75h:27m:43s remains)
2017-12-05 14:45:45.461199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3174505 -4.3090992 -4.3022451 -4.3002172 -4.2978239 -4.2940574 -4.2911158 -4.2910352 -4.2902241 -4.2822337 -4.2751069 -4.2746973 -4.2865319 -4.3088388 -4.3287292][-4.3039751 -4.29291 -4.2867703 -4.28528 -4.278111 -4.2707477 -4.2704592 -4.275579 -4.2745671 -4.262001 -4.2502236 -4.2456565 -4.2577658 -4.2893734 -4.3210564][-4.2796969 -4.2665591 -4.2624135 -4.2564278 -4.2342434 -4.2169137 -4.2197261 -4.2315354 -4.2306204 -4.2108755 -4.1959515 -4.1927605 -4.2094116 -4.2526059 -4.300849][-4.2583938 -4.2394991 -4.2324324 -4.2168941 -4.1725588 -4.13905 -4.1453142 -4.1650891 -4.1614747 -4.1289268 -4.1073966 -4.1101441 -4.1388135 -4.1968389 -4.26305][-4.2547832 -4.2297912 -4.2098131 -4.172308 -4.0979977 -4.0389323 -4.0440831 -4.0723963 -4.0636168 -4.0239787 -4.0101304 -4.0329094 -4.0836887 -4.1571417 -4.2354922][-4.2537456 -4.218657 -4.1799488 -4.1133952 -4.0043063 -3.9135869 -3.9139812 -3.9545705 -3.9595797 -3.94162 -3.9564383 -4.0063591 -4.0731874 -4.1501584 -4.2273164][-4.2521596 -4.20997 -4.1591439 -4.0756674 -3.9497724 -3.8443131 -3.837894 -3.8777232 -3.8921404 -3.8988893 -3.9385767 -4.0088472 -4.0858712 -4.160862 -4.2278619][-4.2626438 -4.2246647 -4.1804504 -4.1115308 -4.0124779 -3.9290857 -3.9171548 -3.9276848 -3.9226046 -3.9281411 -3.9711385 -4.0430846 -4.118319 -4.1850696 -4.2405205][-4.271265 -4.2411194 -4.2042851 -4.1528139 -4.0867195 -4.0333076 -4.0230894 -4.016746 -3.996244 -3.9959133 -4.0309668 -4.0926185 -4.1564393 -4.214148 -4.2615266][-4.2812233 -4.2574635 -4.2279716 -4.1894131 -4.1470222 -4.1134176 -4.1108446 -4.1076293 -4.0895424 -4.0833035 -4.1058226 -4.1507535 -4.2018361 -4.2496791 -4.2889452][-4.30318 -4.2866893 -4.2649221 -4.236434 -4.2074418 -4.183506 -4.1852951 -4.1883006 -4.1783862 -4.1714091 -4.1829004 -4.2127514 -4.250196 -4.2868562 -4.3148375][-4.3183818 -4.3077693 -4.2934866 -4.2748742 -4.2578096 -4.243794 -4.2484016 -4.2524753 -4.244163 -4.2350731 -4.2384052 -4.2549696 -4.2765217 -4.3012128 -4.3207917][-4.3243108 -4.3158941 -4.3070517 -4.2958479 -4.2855859 -4.279727 -4.286428 -4.2911553 -4.2859759 -4.2787857 -4.2776761 -4.2833085 -4.2919917 -4.30681 -4.3198733][-4.3228049 -4.3145375 -4.3059287 -4.2970777 -4.2897787 -4.2875004 -4.2945728 -4.301724 -4.3014989 -4.2974019 -4.2960296 -4.298624 -4.3037462 -4.3127327 -4.3206615][-4.3206072 -4.3122349 -4.3045464 -4.2979231 -4.2943678 -4.2946644 -4.3001237 -4.306397 -4.3083329 -4.3069782 -4.3056407 -4.3066692 -4.3104367 -4.3166451 -4.3220167]]...]
INFO - root - 2017-12-05 14:45:54.055721: step 18110, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 76h:37m:19s remains)
INFO - root - 2017-12-05 14:46:02.477668: step 18120, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 75h:30m:38s remains)
INFO - root - 2017-12-05 14:46:11.043366: step 18130, loss = 2.01, batch loss = 1.95 (9.6 examples/sec; 0.832 sec/batch; 72h:38m:07s remains)
INFO - root - 2017-12-05 14:46:19.575767: step 18140, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 74h:55m:42s remains)
INFO - root - 2017-12-05 14:46:28.234722: step 18150, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.814 sec/batch; 71h:03m:26s remains)
INFO - root - 2017-12-05 14:46:36.798934: step 18160, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 74h:43m:48s remains)
INFO - root - 2017-12-05 14:46:45.202722: step 18170, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 74h:01m:34s remains)
INFO - root - 2017-12-05 14:46:53.782574: step 18180, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.854 sec/batch; 74h:34m:00s remains)
INFO - root - 2017-12-05 14:47:02.352032: step 18190, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 77h:43m:44s remains)
INFO - root - 2017-12-05 14:47:10.985741: step 18200, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 75h:59m:53s remains)
2017-12-05 14:47:11.818907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.262177 -4.2524991 -4.2555518 -4.2613735 -4.2699962 -4.2796922 -4.2924905 -4.3014641 -4.3033552 -4.3060546 -4.3112049 -4.3123431 -4.3122697 -4.3082833 -4.3041654][-4.2267756 -4.2165871 -4.2200031 -4.2251177 -4.2345939 -4.2456822 -4.2633886 -4.2778478 -4.2826514 -4.2860565 -4.2930245 -4.2980719 -4.3034096 -4.2986259 -4.2914448][-4.1931705 -4.1854768 -4.18779 -4.1913042 -4.1965089 -4.2023983 -4.2236881 -4.2424989 -4.25339 -4.2625842 -4.271718 -4.2789764 -4.291111 -4.289875 -4.2820234][-4.1675858 -4.1585 -4.1538205 -4.1535769 -4.1550612 -4.15329 -4.171782 -4.1921654 -4.2120228 -4.2325082 -4.2463851 -4.25493 -4.2725258 -4.27778 -4.2723355][-4.1403618 -4.1239805 -4.1089568 -4.1029124 -4.1031 -4.0950003 -4.1024866 -4.1202054 -4.1496778 -4.1854916 -4.2054343 -4.22066 -4.2497964 -4.2664933 -4.2671728][-4.1207647 -4.0914135 -4.0580215 -4.0389414 -4.0392151 -4.0339661 -4.029943 -4.0386357 -4.0763206 -4.1245356 -4.1495891 -4.1730137 -4.2146297 -4.2446814 -4.2570319][-4.1150169 -4.0697103 -4.014236 -3.9822609 -3.9868536 -3.989095 -3.9755025 -3.9684474 -4.00644 -4.05707 -4.087997 -4.1207833 -4.1722927 -4.2107539 -4.2389984][-4.1281085 -4.0734191 -4.0058651 -3.9691889 -3.9715936 -3.9752026 -3.950314 -3.9183068 -3.9444439 -3.9945171 -4.0278692 -4.0652161 -4.1240582 -4.1736078 -4.2207665][-4.151433 -4.1031065 -4.0439496 -4.0064297 -3.9963896 -3.9915259 -3.95502 -3.9004238 -3.9066434 -3.9508255 -3.9823174 -4.021575 -4.0853066 -4.1428752 -4.2013626][-4.1730871 -4.1367464 -4.0931554 -4.0607481 -4.0438085 -4.0302558 -3.9936562 -3.9354525 -3.9207501 -3.949223 -3.9681509 -4.0004058 -4.0598521 -4.1200447 -4.18126][-4.1769528 -4.1486015 -4.1189051 -4.0993891 -4.0894856 -4.0809 -4.05615 -4.0162745 -3.9971521 -4.0024118 -3.9960394 -4.0082288 -4.0529552 -4.1070681 -4.1624][-4.168653 -4.1436934 -4.123641 -4.1164303 -4.114161 -4.11416 -4.1060991 -4.0915766 -4.0819693 -4.0736251 -4.0450892 -4.0334229 -4.0573182 -4.0973954 -4.1424627][-4.1573734 -4.1339045 -4.1207428 -4.1191096 -4.11901 -4.1244802 -4.1312728 -4.1343393 -4.137135 -4.1280107 -4.0922775 -4.0642533 -4.0693679 -4.0938058 -4.126555][-4.1598644 -4.1401997 -4.1308174 -4.1283889 -4.1296473 -4.1397309 -4.1549063 -4.1663055 -4.1741467 -4.1681643 -4.1388779 -4.1098008 -4.1013389 -4.1107054 -4.1354785][-4.1885719 -4.1739254 -4.1676879 -4.1662407 -4.1692986 -4.1809874 -4.196475 -4.2079773 -4.2161312 -4.21496 -4.1973543 -4.1761022 -4.1651134 -4.1672282 -4.182682]]...]
INFO - root - 2017-12-05 14:47:20.422073: step 18210, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 78h:14m:53s remains)
INFO - root - 2017-12-05 14:47:28.803765: step 18220, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 74h:10m:57s remains)
INFO - root - 2017-12-05 14:47:37.429119: step 18230, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.865 sec/batch; 75h:32m:53s remains)
INFO - root - 2017-12-05 14:47:45.870619: step 18240, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.836 sec/batch; 72h:57m:21s remains)
INFO - root - 2017-12-05 14:47:54.497938: step 18250, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 76h:09m:49s remains)
INFO - root - 2017-12-05 14:48:03.098111: step 18260, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 74h:54m:05s remains)
INFO - root - 2017-12-05 14:48:11.719579: step 18270, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 74h:32m:01s remains)
INFO - root - 2017-12-05 14:48:20.246827: step 18280, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 73h:57m:43s remains)
INFO - root - 2017-12-05 14:48:28.850178: step 18290, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 76h:54m:12s remains)
INFO - root - 2017-12-05 14:48:37.388233: step 18300, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 77h:26m:06s remains)
2017-12-05 14:48:38.100466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1990323 -4.2306256 -4.2433991 -4.249517 -4.2581491 -4.2579269 -4.2528243 -4.2489982 -4.2520666 -4.2630253 -4.2701445 -4.2730789 -4.2723594 -4.2641473 -4.245482][-4.2426624 -4.2659721 -4.2634044 -4.2497125 -4.2393112 -4.2236447 -4.2107539 -4.2108231 -4.2270269 -4.2519274 -4.268827 -4.2757688 -4.2755551 -4.2662673 -4.2449436][-4.2686954 -4.2840548 -4.2657228 -4.2310238 -4.1965303 -4.1584048 -4.1323981 -4.1362896 -4.1704168 -4.21609 -4.2525563 -4.2735677 -4.2806897 -4.2745018 -4.253294][-4.2719855 -4.2794919 -4.2485833 -4.1956291 -4.1379752 -4.07405 -4.0298061 -4.0373755 -4.0921273 -4.1611657 -4.2208576 -4.2617803 -4.2806282 -4.2802935 -4.2603312][-4.267437 -4.2682042 -4.2272129 -4.1608443 -4.0829873 -3.9937465 -3.9274933 -3.9367557 -4.0116796 -4.1044207 -4.1847324 -4.242866 -4.2729249 -4.2791576 -4.262897][-4.2572908 -4.2530518 -4.2054129 -4.1284966 -4.0326829 -3.9187174 -3.828172 -3.8383152 -3.9368052 -4.0553136 -4.1540022 -4.225276 -4.263361 -4.274085 -4.2607269][-4.24041 -4.2321811 -4.1796851 -4.0949922 -3.9834952 -3.8463781 -3.727983 -3.7339296 -3.8628955 -4.0114346 -4.1280422 -4.2102823 -4.2557516 -4.2719684 -4.2617388][-4.2224846 -4.2116914 -4.1582584 -4.072937 -3.9568822 -3.808358 -3.667702 -3.6609864 -3.8103385 -3.9807625 -4.1093946 -4.1987457 -4.2489533 -4.2702332 -4.26412][-4.2120552 -4.2042923 -4.1592016 -4.0861969 -3.9869552 -3.857091 -3.7303236 -3.7134972 -3.8392792 -3.9947436 -4.1125627 -4.1946182 -4.2417369 -4.2615376 -4.2585096][-4.2074375 -4.2081833 -4.1812963 -4.1340656 -4.0685749 -3.9808385 -3.8956082 -3.8796244 -3.9575744 -4.0633526 -4.1417131 -4.1982512 -4.235816 -4.2527966 -4.251852][-4.19208 -4.2011328 -4.193923 -4.1745987 -4.1462631 -4.1048789 -4.0620365 -4.0542474 -4.0951629 -4.1472354 -4.1804404 -4.2057118 -4.2281675 -4.2424464 -4.2428837][-4.1564093 -4.1725111 -4.1828208 -4.1921105 -4.1970181 -4.1904869 -4.1747046 -4.1711106 -4.185832 -4.1948709 -4.1894846 -4.1894789 -4.200748 -4.213418 -4.217659][-4.1114974 -4.1320219 -4.1564837 -4.1903591 -4.2219687 -4.2386794 -4.2398505 -4.2369161 -4.2316236 -4.2076464 -4.1712413 -4.1522055 -4.1573811 -4.1722708 -4.1821518][-4.0872617 -4.1103382 -4.1423812 -4.1890526 -4.2354608 -4.2647405 -4.2748475 -4.270462 -4.2502904 -4.2048154 -4.152761 -4.1242523 -4.1268082 -4.1412568 -4.1543884][-4.1063814 -4.1275082 -4.1550741 -4.1983709 -4.2461157 -4.2800779 -4.2953181 -4.2919245 -4.2666507 -4.2155814 -4.161869 -4.1336932 -4.134057 -4.1421227 -4.1494474]]...]
INFO - root - 2017-12-05 14:48:46.663617: step 18310, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 71h:53m:37s remains)
INFO - root - 2017-12-05 14:48:55.228114: step 18320, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 74h:28m:12s remains)
INFO - root - 2017-12-05 14:49:03.768077: step 18330, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 72h:57m:27s remains)
INFO - root - 2017-12-05 14:49:12.170683: step 18340, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 72h:01m:21s remains)
INFO - root - 2017-12-05 14:49:20.678364: step 18350, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 75h:07m:38s remains)
INFO - root - 2017-12-05 14:49:29.101970: step 18360, loss = 2.04, batch loss = 1.99 (9.7 examples/sec; 0.829 sec/batch; 72h:19m:53s remains)
INFO - root - 2017-12-05 14:49:37.629734: step 18370, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 76h:52m:07s remains)
INFO - root - 2017-12-05 14:49:46.197284: step 18380, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 73h:47m:04s remains)
INFO - root - 2017-12-05 14:49:54.668493: step 18390, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 74h:09m:37s remains)
INFO - root - 2017-12-05 14:50:03.308821: step 18400, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 74h:33m:51s remains)
2017-12-05 14:50:04.013429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.348331 -4.3472176 -4.3487172 -4.3500214 -4.351985 -4.3547525 -4.3576159 -4.3556232 -4.3487797 -4.3416147 -4.3366508 -4.3340082 -4.3366914 -4.345654 -4.3563237][-4.3298059 -4.3245835 -4.323689 -4.3246608 -4.3270283 -4.3316317 -4.3368006 -4.3324485 -4.3190317 -4.3071766 -4.3009858 -4.2982416 -4.3044381 -4.3217044 -4.3413773][-4.297564 -4.2852759 -4.2810373 -4.2810769 -4.2831669 -4.2886114 -4.2965727 -4.2886152 -4.2671623 -4.2529178 -4.2476745 -4.2487059 -4.2603989 -4.288651 -4.3198075][-4.2638693 -4.2478714 -4.2401433 -4.2369175 -4.2359705 -4.2395868 -4.2448125 -4.2298994 -4.2015505 -4.187696 -4.1868606 -4.1951494 -4.2158723 -4.254405 -4.2954454][-4.2397981 -4.2252269 -4.214325 -4.2047915 -4.1954684 -4.1954055 -4.1936169 -4.1634078 -4.1236076 -4.1132321 -4.1248107 -4.1485362 -4.1826191 -4.2317042 -4.2805209][-4.2168903 -4.1971216 -4.1768341 -4.15552 -4.1347046 -4.1240606 -4.1059675 -4.0539479 -4.0059309 -4.0087309 -4.0438809 -4.0924506 -4.1464877 -4.2087455 -4.2698188][-4.1872821 -4.1549959 -4.1212454 -4.0867467 -4.0588665 -4.0401173 -4.00155 -3.9287491 -3.8784356 -3.9043429 -3.9672363 -4.0390382 -4.1123185 -4.1878233 -4.2603474][-4.1793065 -4.1414595 -4.10281 -4.0685625 -4.049273 -4.0376434 -3.994411 -3.9198368 -3.8745914 -3.9111335 -3.9817629 -4.0528126 -4.12217 -4.1893306 -4.2569642][-4.1961536 -4.1657734 -4.1391196 -4.1215916 -4.12338 -4.1254196 -4.0963984 -4.0363879 -3.9897144 -4.0116653 -4.0667143 -4.11779 -4.1650887 -4.2133455 -4.2664709][-4.2265992 -4.2075076 -4.191875 -4.1850262 -4.1937985 -4.1992087 -4.1799431 -4.1327868 -4.0886736 -4.0958667 -4.1364708 -4.1748109 -4.2103391 -4.2459579 -4.2849336][-4.250515 -4.2397475 -4.2317429 -4.2294474 -4.238256 -4.2438092 -4.2311077 -4.1980762 -4.1636086 -4.1631403 -4.1898203 -4.2193527 -4.2477417 -4.2716646 -4.3000741][-4.2723675 -4.2650023 -4.2614665 -4.2603602 -4.2685809 -4.274332 -4.269433 -4.2497549 -4.2257657 -4.2262149 -4.2429643 -4.2609491 -4.2799006 -4.2955713 -4.3160043][-4.2867661 -4.2745976 -4.2695146 -4.2674689 -4.2754993 -4.281898 -4.2808857 -4.2668157 -4.2515526 -4.2568645 -4.270381 -4.284874 -4.3015924 -4.3145108 -4.3289514][-4.2951064 -4.2795529 -4.2735877 -4.2694311 -4.2710524 -4.2744827 -4.2755032 -4.2678223 -4.2608228 -4.2664237 -4.2779036 -4.2923136 -4.3101606 -4.3237467 -4.3362317][-4.312602 -4.2960505 -4.2893972 -4.2850475 -4.2847929 -4.286056 -4.286438 -4.2828922 -4.27978 -4.2841368 -4.2943211 -4.307436 -4.3222933 -4.3323522 -4.3429613]]...]
INFO - root - 2017-12-05 14:50:12.480390: step 18410, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 76h:00m:10s remains)
INFO - root - 2017-12-05 14:50:21.036100: step 18420, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 75h:36m:12s remains)
INFO - root - 2017-12-05 14:50:29.582029: step 18430, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 76h:32m:22s remains)
INFO - root - 2017-12-05 14:50:38.072877: step 18440, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.824 sec/batch; 71h:54m:44s remains)
INFO - root - 2017-12-05 14:50:46.700371: step 18450, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 74h:50m:48s remains)
INFO - root - 2017-12-05 14:50:55.256118: step 18460, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 75h:23m:57s remains)
INFO - root - 2017-12-05 14:51:03.800370: step 18470, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 75h:04m:33s remains)
INFO - root - 2017-12-05 14:51:12.212082: step 18480, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.825 sec/batch; 71h:55m:35s remains)
INFO - root - 2017-12-05 14:51:20.701667: step 18490, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 75h:23m:26s remains)
INFO - root - 2017-12-05 14:51:29.315540: step 18500, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 72h:38m:46s remains)
2017-12-05 14:51:30.048520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2609 -4.2709951 -4.2801266 -4.2874684 -4.2930965 -4.2987862 -4.3026347 -4.3027806 -4.2997527 -4.2951441 -4.2906685 -4.2829905 -4.2695632 -4.252419 -4.236434][-4.254086 -4.2670956 -4.2790146 -4.289444 -4.2982869 -4.3076568 -4.31587 -4.3189821 -4.3173018 -4.3120718 -4.3053408 -4.2950592 -4.2790833 -4.259532 -4.2411294][-4.2445388 -4.2552643 -4.2661052 -4.27812 -4.2914171 -4.3060632 -4.3210106 -4.3305426 -4.3329458 -4.3287945 -4.3202686 -4.307354 -4.28877 -4.2662892 -4.2449408][-4.2297854 -4.2301297 -4.2334552 -4.2441845 -4.2609086 -4.2801747 -4.30272 -4.3225555 -4.3341 -4.3351769 -4.3282471 -4.3152752 -4.2962627 -4.27205 -4.247963][-4.2081594 -4.1916161 -4.1805429 -4.1847186 -4.2008357 -4.2215853 -4.2499166 -4.2822442 -4.308672 -4.3204842 -4.3193665 -4.3099122 -4.2940326 -4.2712097 -4.246954][-4.1839528 -4.1499434 -4.1214905 -4.1129065 -4.1208611 -4.1349754 -4.1626186 -4.2051554 -4.2480583 -4.2743173 -4.2835121 -4.2827892 -4.2748938 -4.2587748 -4.2396173][-4.1589742 -4.1126037 -4.0690465 -4.0450697 -4.0372014 -4.0354071 -4.0522585 -4.1000247 -4.158895 -4.2023196 -4.2257385 -4.2378078 -4.2425146 -4.2372632 -4.2268653][-4.1386976 -4.0899515 -4.0394506 -4.0011106 -3.9730692 -3.9463949 -3.9432981 -3.9901819 -4.062861 -4.1246939 -4.1642475 -4.1896868 -4.2065058 -4.2130494 -4.2120662][-4.1219263 -4.0785027 -4.0301723 -3.9853973 -3.9417019 -3.8953791 -3.8727126 -3.91213 -3.9916034 -4.0670972 -4.120173 -4.1548877 -4.1793375 -4.1941209 -4.199131][-4.1106834 -4.0734544 -4.0323515 -3.9917314 -3.9474006 -3.8992188 -3.8712974 -3.8984759 -3.9712489 -4.048192 -4.1057744 -4.1423473 -4.1676321 -4.184691 -4.19229][-4.1174107 -4.0848012 -4.0515728 -4.0202603 -3.9868023 -3.9490917 -3.9261897 -3.9435909 -4.0015507 -4.0691295 -4.1199837 -4.1511054 -4.1715508 -4.1857071 -4.1927857][-4.1519337 -4.124393 -4.0988255 -4.0774789 -4.0561442 -4.0308843 -4.0140829 -4.024085 -4.0641489 -4.1141839 -4.1515708 -4.1727595 -4.1855917 -4.1933069 -4.1960692][-4.2111688 -4.1911197 -4.1726747 -4.1585712 -4.1453042 -4.12838 -4.1145835 -4.1166553 -4.1387377 -4.1689806 -4.1917367 -4.2023826 -4.2057652 -4.203383 -4.1974373][-4.2684827 -4.2604403 -4.2492414 -4.2380829 -4.2270184 -4.2132597 -4.2000284 -4.1955495 -4.2037625 -4.2175317 -4.2273917 -4.2290106 -4.2242646 -4.2108917 -4.1930814][-4.2914205 -4.2995076 -4.2983127 -4.2911639 -4.2812653 -4.2685828 -4.255363 -4.2473307 -4.2459397 -4.2474327 -4.2467403 -4.2407761 -4.2290626 -4.2052073 -4.1750703]]...]
INFO - root - 2017-12-05 14:51:38.689106: step 18510, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 75h:34m:04s remains)
INFO - root - 2017-12-05 14:51:46.698471: step 18520, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 73h:05m:05s remains)
INFO - root - 2017-12-05 14:51:55.161232: step 18530, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 75h:58m:11s remains)
INFO - root - 2017-12-05 14:52:03.615470: step 18540, loss = 2.04, batch loss = 1.99 (9.8 examples/sec; 0.817 sec/batch; 71h:17m:06s remains)
INFO - root - 2017-12-05 14:52:12.121754: step 18550, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.828 sec/batch; 72h:12m:58s remains)
INFO - root - 2017-12-05 14:52:20.548871: step 18560, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.827 sec/batch; 72h:09m:15s remains)
INFO - root - 2017-12-05 14:52:29.029853: step 18570, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 73h:56m:29s remains)
INFO - root - 2017-12-05 14:52:37.495615: step 18580, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 75h:42m:10s remains)
INFO - root - 2017-12-05 14:52:46.072595: step 18590, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 73h:47m:39s remains)
INFO - root - 2017-12-05 14:52:54.465137: step 18600, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 75h:08m:35s remains)
2017-12-05 14:52:55.227235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.203783 -4.2191253 -4.2364759 -4.240737 -4.2234907 -4.2041783 -4.18808 -4.1759849 -4.1630907 -4.1523476 -4.1440449 -4.15596 -4.1875896 -4.2255783 -4.2615271][-4.2090735 -4.2218528 -4.2354918 -4.2439947 -4.2346034 -4.2194624 -4.2001071 -4.183826 -4.1708446 -4.1603441 -4.156178 -4.167007 -4.1914935 -4.2234511 -4.2581458][-4.2169271 -4.22818 -4.2393303 -4.2513165 -4.24167 -4.2304044 -4.2175794 -4.1992774 -4.1903691 -4.1865773 -4.1804156 -4.1793942 -4.1904116 -4.2138205 -4.2478504][-4.2248192 -4.23723 -4.245894 -4.2561197 -4.2441406 -4.2334533 -4.2206249 -4.1978283 -4.1979852 -4.210403 -4.2081003 -4.1965652 -4.1953077 -4.2116895 -4.2458248][-4.2334452 -4.2451005 -4.24854 -4.2560105 -4.2400179 -4.2183189 -4.1869068 -4.1488466 -4.1670947 -4.2082176 -4.2228346 -4.2163672 -4.2122521 -4.2278471 -4.2590823][-4.2423153 -4.2509427 -4.2489123 -4.2528362 -4.2290249 -4.1841598 -4.1155252 -4.0466032 -4.0884194 -4.1670427 -4.2075191 -4.2189813 -4.2225227 -4.2409825 -4.2699738][-4.249804 -4.2527714 -4.2452345 -4.2444034 -4.2100325 -4.1382837 -4.0257945 -3.9188335 -3.9883935 -4.1030617 -4.1683545 -4.2021675 -4.2167931 -4.2425365 -4.271419][-4.2431259 -4.2383766 -4.22733 -4.2285686 -4.1907172 -4.1078491 -3.9782972 -3.8614564 -3.9435318 -4.0601 -4.1250277 -4.1684461 -4.1953163 -4.2276058 -4.2595739][-4.231708 -4.2242413 -4.2142849 -4.2202458 -4.1906896 -4.1244636 -4.0242558 -3.9481912 -4.0079856 -4.0811453 -4.118988 -4.1561203 -4.1841927 -4.2141171 -4.2417431][-4.2206874 -4.212914 -4.2068405 -4.2162895 -4.1991034 -4.1537991 -4.0888953 -4.0544586 -4.0952396 -4.1330023 -4.1433477 -4.1634274 -4.1801434 -4.2020311 -4.2283263][-4.2155986 -4.2069025 -4.200501 -4.210907 -4.203259 -4.1727753 -4.1296487 -4.1171746 -4.1475606 -4.1716304 -4.1732388 -4.1816888 -4.1878943 -4.2037935 -4.2308207][-4.22172 -4.2058444 -4.1973319 -4.209209 -4.2117643 -4.1887112 -4.1550512 -4.154376 -4.177103 -4.1925797 -4.1954288 -4.2035546 -4.20453 -4.216279 -4.2420139][-4.2428956 -4.2226243 -4.2144456 -4.2262006 -4.2308984 -4.2118707 -4.1822133 -4.1871037 -4.2040105 -4.213748 -4.2206588 -4.2338433 -4.237361 -4.2472134 -4.2653828][-4.2665167 -4.2491646 -4.2451515 -4.2544689 -4.2567825 -4.2403755 -4.2186913 -4.2251744 -4.2355185 -4.23884 -4.2482381 -4.2640595 -4.2714524 -4.2811427 -4.2933936][-4.2727547 -4.260541 -4.2604394 -4.2656236 -4.2654195 -4.2533712 -4.2413526 -4.2479587 -4.2529254 -4.2527089 -4.2596908 -4.2723584 -4.2811775 -4.2897058 -4.2987075]]...]
INFO - root - 2017-12-05 14:53:03.776269: step 18610, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 75h:09m:12s remains)
INFO - root - 2017-12-05 14:53:12.260821: step 18620, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 76h:29m:39s remains)
INFO - root - 2017-12-05 14:53:20.855051: step 18630, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 74h:26m:09s remains)
INFO - root - 2017-12-05 14:53:29.466584: step 18640, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 73h:59m:15s remains)
INFO - root - 2017-12-05 14:53:38.157533: step 18650, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 74h:04m:18s remains)
INFO - root - 2017-12-05 14:53:46.801236: step 18660, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 75h:02m:35s remains)
INFO - root - 2017-12-05 14:53:55.500812: step 18670, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 74h:49m:58s remains)
INFO - root - 2017-12-05 14:54:04.074195: step 18680, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 75h:07m:41s remains)
INFO - root - 2017-12-05 14:54:12.713399: step 18690, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 72h:06m:02s remains)
INFO - root - 2017-12-05 14:54:21.300580: step 18700, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 74h:38m:38s remains)
2017-12-05 14:54:22.071596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2030168 -4.1871958 -4.1499548 -4.1040177 -4.0978475 -4.1443472 -4.1940746 -4.2244964 -4.2342043 -4.214448 -4.1858945 -4.1624227 -4.1342349 -4.1219997 -4.1271715][-4.185801 -4.1710324 -4.1280737 -4.0734539 -4.04764 -4.085484 -4.1415238 -4.1842308 -4.2072616 -4.2060571 -4.1945391 -4.1776156 -4.1547084 -4.1443763 -4.1469755][-4.17673 -4.1639462 -4.1204691 -4.0609155 -4.0162935 -4.029808 -4.0774984 -4.1277852 -4.1632452 -4.1841488 -4.1959352 -4.1916313 -4.17471 -4.1651731 -4.1644735][-4.1883054 -4.1711621 -4.1265421 -4.0659881 -4.0061059 -3.9870906 -4.01444 -4.06934 -4.1197691 -4.159574 -4.1924891 -4.2012296 -4.1904368 -4.177505 -4.1704636][-4.2048397 -4.1814642 -4.1352139 -4.078846 -4.0131063 -3.9652953 -3.9671328 -4.0208149 -4.0855675 -4.13995 -4.1852403 -4.2041574 -4.1964283 -4.1810713 -4.1702261][-4.2138915 -4.18123 -4.1326561 -4.0791893 -4.0164676 -3.9641416 -3.95257 -3.997889 -4.0653472 -4.1227307 -4.1688476 -4.1926103 -4.1924767 -4.1858497 -4.1795087][-4.2162204 -4.1719871 -4.1162581 -4.0645285 -4.0107288 -3.9734464 -3.9692771 -4.00342 -4.0513382 -4.0874877 -4.1221304 -4.1513958 -4.1729422 -4.1886196 -4.1907396][-4.2100005 -4.16949 -4.1180606 -4.0740266 -4.0302067 -4.0044379 -3.999315 -4.0071697 -4.0173082 -4.0169992 -4.0282078 -4.0630345 -4.1173859 -4.164238 -4.1821527][-4.1985445 -4.1699409 -4.1387839 -4.1166158 -4.0933461 -4.0741467 -4.0532365 -4.0205832 -3.9788003 -3.9332535 -3.9169316 -3.952826 -4.0335703 -4.1076937 -4.1447811][-4.1955719 -4.1781354 -4.1709342 -4.1725287 -4.1701226 -4.1584549 -4.1246424 -4.0652962 -3.9874558 -3.916878 -3.8811781 -3.9078081 -3.9911149 -4.0712895 -4.1143909][-4.1997347 -4.196517 -4.2081017 -4.2266865 -4.2354178 -4.2259769 -4.1897612 -4.1278954 -4.0502529 -3.9852448 -3.9511228 -3.9685314 -4.0334897 -4.09533 -4.1271472][-4.219038 -4.2219896 -4.2360024 -4.2544508 -4.2632222 -4.2540541 -4.2242446 -4.1749949 -4.1148868 -4.0665617 -4.0446234 -4.0630126 -4.1122241 -4.1535854 -4.1707258][-4.2489791 -4.2519135 -4.2569942 -4.2663136 -4.271853 -4.2638807 -4.2420387 -4.2079086 -4.1694717 -4.1407785 -4.1332636 -4.154036 -4.1903324 -4.2153845 -4.2200003][-4.2754836 -4.2781892 -4.2775669 -4.2794447 -4.2820978 -4.2754607 -4.2620788 -4.2444167 -4.2236657 -4.2090921 -4.2071252 -4.2237968 -4.2462244 -4.2578487 -4.255928][-4.3008552 -4.3048344 -4.3031125 -4.3030729 -4.3050575 -4.302949 -4.2979617 -4.2905045 -4.2804456 -4.2725759 -4.2706938 -4.279489 -4.291173 -4.2948456 -4.2907853]]...]
INFO - root - 2017-12-05 14:54:30.673932: step 18710, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 76h:11m:20s remains)
INFO - root - 2017-12-05 14:54:39.195485: step 18720, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.856 sec/batch; 74h:34m:12s remains)
INFO - root - 2017-12-05 14:54:47.766407: step 18730, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 77h:14m:19s remains)
INFO - root - 2017-12-05 14:54:56.214319: step 18740, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 75h:36m:39s remains)
INFO - root - 2017-12-05 14:55:04.710741: step 18750, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 71h:21m:28s remains)
INFO - root - 2017-12-05 14:55:13.322655: step 18760, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 78h:12m:09s remains)
INFO - root - 2017-12-05 14:55:21.902910: step 18770, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 78h:44m:00s remains)
INFO - root - 2017-12-05 14:55:30.552711: step 18780, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 72h:08m:43s remains)
INFO - root - 2017-12-05 14:55:39.061511: step 18790, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 75h:14m:33s remains)
INFO - root - 2017-12-05 14:55:47.637295: step 18800, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 77h:54m:40s remains)
2017-12-05 14:55:48.454331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2282128 -4.2541494 -4.2659993 -4.262712 -4.241652 -4.2137289 -4.1906381 -4.1743255 -4.156991 -4.1336765 -4.1141391 -4.1044903 -4.1054258 -4.1228428 -4.1535416][-4.2068253 -4.231658 -4.2444668 -4.242722 -4.2241607 -4.199409 -4.1773148 -4.1584797 -4.1410403 -4.1204267 -4.1044273 -4.0957918 -4.0916748 -4.1020155 -4.1264458][-4.188067 -4.2083254 -4.2186346 -4.2160382 -4.19996 -4.181716 -4.16197 -4.1418781 -4.1235361 -4.1077905 -4.099071 -4.0951514 -4.0899153 -4.0908966 -4.1047335][-4.1630321 -4.1776776 -4.1840062 -4.1787915 -4.1637282 -4.1508594 -4.1331983 -4.1101055 -4.0935607 -4.0889091 -4.0907354 -4.0921507 -4.0898256 -4.08459 -4.0885186][-4.1405458 -4.1493568 -4.151063 -4.1403389 -4.1221714 -4.1076865 -4.0835385 -4.0533047 -4.0400033 -4.0532794 -4.0710278 -4.0826044 -4.0891666 -4.0872192 -4.087388][-4.1375093 -4.1447825 -4.1410751 -4.1196828 -4.0888815 -4.0593734 -4.0225396 -3.9787803 -3.9694331 -4.0035281 -4.0411239 -4.0660095 -4.0867152 -4.0983286 -4.1029334][-4.1488695 -4.1602073 -4.1540127 -4.1199589 -4.0679159 -4.0122428 -3.9487259 -3.8783491 -3.8729546 -3.9379215 -4.0025439 -4.0478325 -4.0832796 -4.1074309 -4.1202817][-4.1578336 -4.17324 -4.1658144 -4.124599 -4.056798 -3.9766786 -3.87725 -3.7727361 -3.7724559 -3.8749518 -3.9706044 -4.0335736 -4.081161 -4.1170297 -4.1409335][-4.165473 -4.1805892 -4.1691589 -4.1306057 -4.0680456 -3.9911633 -3.8909512 -3.7876675 -3.7888517 -3.8879032 -3.9783852 -4.0359006 -4.0854397 -4.1311722 -4.1644526][-4.1794648 -4.1935492 -4.1789613 -4.1425366 -4.0957885 -4.043314 -3.9792309 -3.9157419 -3.9129021 -3.9711645 -4.0273757 -4.0629416 -4.098567 -4.1406269 -4.174273][-4.1688933 -4.1873255 -4.175035 -4.14346 -4.1117911 -4.0826387 -4.0521722 -4.0214806 -4.0190372 -4.0468655 -4.0776734 -4.0962672 -4.1192536 -4.1554527 -4.1850109][-4.1531954 -4.1812358 -4.1784997 -4.15594 -4.1340914 -4.1173725 -4.1052513 -4.0907488 -4.0870471 -4.0986347 -4.1154995 -4.1261196 -4.142252 -4.1764212 -4.1985][-4.1604333 -4.1931605 -4.1992655 -4.1855397 -4.1694326 -4.1567073 -4.15036 -4.1417489 -4.135643 -4.1368361 -4.1434841 -4.1513014 -4.1643019 -4.1936855 -4.2060657][-4.1801138 -4.2150207 -4.2259865 -4.2192063 -4.2091122 -4.1991272 -4.1903868 -4.1788282 -4.1665254 -4.159308 -4.1583714 -4.1699176 -4.1913252 -4.2228875 -4.233973][-4.2048583 -4.2395787 -4.2519784 -4.2491 -4.2435236 -4.23749 -4.2292948 -4.2164569 -4.1999383 -4.1876593 -4.18634 -4.2013526 -4.2284775 -4.2577577 -4.2670755]]...]
INFO - root - 2017-12-05 14:55:56.960354: step 18810, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 74h:32m:15s remains)
INFO - root - 2017-12-05 14:56:05.396472: step 18820, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 75h:26m:47s remains)
INFO - root - 2017-12-05 14:56:13.996440: step 18830, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 76h:07m:57s remains)
INFO - root - 2017-12-05 14:56:22.596222: step 18840, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 78h:27m:47s remains)
INFO - root - 2017-12-05 14:56:31.202899: step 18850, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 73h:42m:50s remains)
INFO - root - 2017-12-05 14:56:39.857754: step 18860, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 71h:24m:21s remains)
INFO - root - 2017-12-05 14:56:48.464092: step 18870, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 74h:06m:11s remains)
INFO - root - 2017-12-05 14:56:57.056853: step 18880, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 73h:39m:20s remains)
INFO - root - 2017-12-05 14:57:05.532976: step 18890, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 76h:33m:52s remains)
INFO - root - 2017-12-05 14:57:14.126423: step 18900, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 73h:35m:04s remains)
2017-12-05 14:57:14.857205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2458954 -4.2497349 -4.254303 -4.2559676 -4.2477636 -4.2345052 -4.2267475 -4.2199349 -4.2088237 -4.20947 -4.217072 -4.2315702 -4.2408938 -4.2432513 -4.2456965][-4.2125759 -4.2142563 -4.2210693 -4.2279415 -4.2234149 -4.209547 -4.2012873 -4.1908984 -4.174696 -4.1743836 -4.1855431 -4.2067437 -4.2185349 -4.2201242 -4.2240186][-4.1724968 -4.1713848 -4.1777706 -4.1903224 -4.1930051 -4.183197 -4.1747246 -4.1580677 -4.133359 -4.1307068 -4.1442528 -4.1681032 -4.1850972 -4.1910129 -4.2045217][-4.1471024 -4.143044 -4.1443615 -4.1563144 -4.160006 -4.1484771 -4.1361971 -4.109808 -4.0800104 -4.0866032 -4.1104507 -4.14113 -4.1660132 -4.1804671 -4.2015467][-4.1378431 -4.1241174 -4.1223588 -4.1359806 -4.1392107 -4.1176157 -4.0873842 -4.0487537 -4.0293479 -4.0558739 -4.0909896 -4.1261544 -4.1635275 -4.1867857 -4.2114677][-4.1434908 -4.1121674 -4.0967121 -4.1057739 -4.1083064 -4.0782409 -4.0246878 -3.975121 -3.9841256 -4.0421038 -4.086937 -4.1254864 -4.1692858 -4.1969848 -4.2212148][-4.1579046 -4.1167369 -4.0957251 -4.0938139 -4.0799403 -4.0200281 -3.9199305 -3.8443992 -3.9025249 -4.0127544 -4.0812626 -4.1293874 -4.1744556 -4.2061663 -4.2255688][-4.1570859 -4.1242537 -4.1125774 -4.1133966 -4.0878692 -4.0060639 -3.8704174 -3.7704649 -3.8586566 -3.9976664 -4.080019 -4.1298676 -4.1713986 -4.205534 -4.2265959][-4.1393538 -4.1201577 -4.1171489 -4.1203828 -4.1038327 -4.0435281 -3.9440434 -3.8775225 -3.9335229 -4.0342221 -4.1028872 -4.142756 -4.1782446 -4.2106624 -4.2281704][-4.1223493 -4.1022849 -4.0972323 -4.1015048 -4.1011152 -4.0747757 -4.022428 -3.9898376 -4.0195441 -4.0781918 -4.1196513 -4.1468549 -4.1815643 -4.2118158 -4.2276554][-4.1092429 -4.0882888 -4.0816617 -4.0852108 -4.0918889 -4.08505 -4.0655241 -4.0602961 -4.0852971 -4.1196055 -4.1349149 -4.1519718 -4.1847053 -4.2125616 -4.2290306][-4.1130333 -4.0962224 -4.0945444 -4.1013026 -4.1081738 -4.1102581 -4.10833 -4.11399 -4.1349783 -4.1546559 -4.1525993 -4.1618385 -4.1902046 -4.21401 -4.2305822][-4.1340747 -4.1211205 -4.1224327 -4.1351342 -4.1511607 -4.1589508 -4.158978 -4.1623917 -4.1727343 -4.181294 -4.1760926 -4.1814733 -4.2008305 -4.2183666 -4.2365413][-4.1653032 -4.1523638 -4.1550984 -4.1722031 -4.191927 -4.2025685 -4.2038808 -4.202498 -4.2057209 -4.2088838 -4.2020698 -4.20012 -4.2110858 -4.2238584 -4.244019][-4.1906023 -4.1785531 -4.1799932 -4.1953068 -4.2143812 -4.2244649 -4.2255344 -4.2235618 -4.2268939 -4.2286453 -4.2206473 -4.2133336 -4.2169657 -4.2247567 -4.242466]]...]
INFO - root - 2017-12-05 14:57:23.345019: step 18910, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 72h:20m:52s remains)
INFO - root - 2017-12-05 14:57:31.606607: step 18920, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 74h:31m:49s remains)
INFO - root - 2017-12-05 14:57:40.002073: step 18930, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 74h:19m:32s remains)
INFO - root - 2017-12-05 14:57:48.455852: step 18940, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 72h:36m:03s remains)
INFO - root - 2017-12-05 14:57:56.886132: step 18950, loss = 2.02, batch loss = 1.96 (9.4 examples/sec; 0.853 sec/batch; 74h:19m:40s remains)
INFO - root - 2017-12-05 14:58:05.298566: step 18960, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 72h:29m:48s remains)
INFO - root - 2017-12-05 14:58:13.955282: step 18970, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 77h:32m:45s remains)
INFO - root - 2017-12-05 14:58:22.557752: step 18980, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 76h:19m:58s remains)
INFO - root - 2017-12-05 14:58:31.186834: step 18990, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 74h:40m:09s remains)
INFO - root - 2017-12-05 14:58:39.818986: step 19000, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 76h:23m:29s remains)
2017-12-05 14:58:40.570449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2881961 -4.3035254 -4.3116684 -4.3117576 -4.2999768 -4.2774076 -4.2471328 -4.221077 -4.2150521 -4.2309618 -4.2601938 -4.286561 -4.3103404 -4.32887 -4.341023][-4.2728963 -4.2898726 -4.2999229 -4.3026881 -4.2926497 -4.2713718 -4.2388506 -4.2098851 -4.2033057 -4.2199936 -4.2520552 -4.2810588 -4.3079448 -4.32697 -4.3392][-4.2632623 -4.2820363 -4.291925 -4.2948933 -4.2891741 -4.2710233 -4.237184 -4.2041926 -4.1942916 -4.2091579 -4.243609 -4.2779355 -4.3081765 -4.327353 -4.3397679][-4.2601595 -4.2790608 -4.2855492 -4.2859373 -4.282732 -4.2646837 -4.2261167 -4.1860557 -4.1733856 -4.1900883 -4.2296309 -4.2735858 -4.3096862 -4.3299289 -4.3423142][-4.2473049 -4.2614479 -4.26311 -4.2607946 -4.255146 -4.2327466 -4.186296 -4.1401849 -4.1293716 -4.1540165 -4.2039709 -4.2622209 -4.308322 -4.3327889 -4.3459291][-4.2353573 -4.245244 -4.2398691 -4.23017 -4.213026 -4.1763792 -4.12105 -4.0750771 -4.0725174 -4.1094 -4.1717253 -4.2454209 -4.3022823 -4.3338561 -4.3501587][-4.227951 -4.2395258 -4.2303233 -4.2106476 -4.1761112 -4.121089 -4.0536809 -4.0126457 -4.0241566 -4.0737576 -4.1462464 -4.22966 -4.2951689 -4.3341832 -4.3536706][-4.2267275 -4.24445 -4.2359509 -4.2074361 -4.1595225 -4.0950942 -4.0253754 -3.9922342 -4.0140023 -4.0698891 -4.1431193 -4.2250829 -4.2918143 -4.3337708 -4.3551517][-4.2312145 -4.2558427 -4.2509418 -4.2215881 -4.1746526 -4.11969 -4.0646763 -4.0386362 -4.058074 -4.1070437 -4.1665769 -4.2345076 -4.2946553 -4.3355594 -4.3556962][-4.2361631 -4.2615151 -4.2597423 -4.2383513 -4.2045174 -4.1682758 -4.13317 -4.1112776 -4.1190109 -4.1526628 -4.1956162 -4.2476721 -4.2982554 -4.3353343 -4.3545914][-4.2362967 -4.2599254 -4.2652225 -4.2571158 -4.2383819 -4.2176585 -4.1944456 -4.1692796 -4.1633067 -4.1848326 -4.2180829 -4.2594805 -4.3011951 -4.3322587 -4.3506455][-4.2343116 -4.2577043 -4.2717285 -4.2762451 -4.2679825 -4.25536 -4.2371697 -4.2093563 -4.1953712 -4.2103953 -4.2377491 -4.2702894 -4.3037815 -4.3292375 -4.3458242][-4.226903 -4.2514963 -4.2728062 -4.2864938 -4.2837658 -4.2755151 -4.2616878 -4.2339425 -4.2164083 -4.2260413 -4.2477164 -4.271399 -4.300076 -4.3240771 -4.34067][-4.2245979 -4.245697 -4.2675414 -4.2835021 -4.2852745 -4.2796612 -4.2666478 -4.237905 -4.2167335 -4.2191105 -4.2352171 -4.2564797 -4.2863359 -4.3148956 -4.3354564][-4.2231593 -4.2364049 -4.2526512 -4.2662911 -4.2704411 -4.2663136 -4.2521186 -4.2239337 -4.2035851 -4.2015676 -4.2167563 -4.2419024 -4.2760105 -4.3091989 -4.3325491]]...]
INFO - root - 2017-12-05 14:58:49.131662: step 19010, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 75h:16m:32s remains)
INFO - root - 2017-12-05 14:58:57.640571: step 19020, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 73h:30m:24s remains)
INFO - root - 2017-12-05 14:59:06.176796: step 19030, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 73h:46m:29s remains)
INFO - root - 2017-12-05 14:59:14.685862: step 19040, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 73h:02m:12s remains)
INFO - root - 2017-12-05 14:59:23.265907: step 19050, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 74h:58m:41s remains)
INFO - root - 2017-12-05 14:59:31.781659: step 19060, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 0.810 sec/batch; 70h:33m:10s remains)
INFO - root - 2017-12-05 14:59:40.170732: step 19070, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 74h:37m:17s remains)
INFO - root - 2017-12-05 14:59:48.781394: step 19080, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 74h:26m:10s remains)
INFO - root - 2017-12-05 14:59:57.330915: step 19090, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 75h:02m:25s remains)
INFO - root - 2017-12-05 15:00:05.811692: step 19100, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 74h:04m:39s remains)
2017-12-05 15:00:06.605199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2979107 -4.2968893 -4.2983432 -4.301785 -4.3070836 -4.3107672 -4.3125453 -4.313684 -4.3158422 -4.319839 -4.3250356 -4.3287034 -4.3302093 -4.3327055 -4.3371148][-4.2795086 -4.2741275 -4.2723532 -4.2760444 -4.284399 -4.2899861 -4.2907882 -4.2911005 -4.2951503 -4.3025575 -4.3123541 -4.32125 -4.3252535 -4.3293209 -4.335206][-4.2462955 -4.2351418 -4.230011 -4.2338171 -4.2419333 -4.2475076 -4.2474651 -4.246531 -4.2530789 -4.265038 -4.2817726 -4.2993722 -4.3098702 -4.3188334 -4.3282528][-4.2078795 -4.188622 -4.1773148 -4.1786709 -4.185823 -4.1874104 -4.1820741 -4.1776762 -4.1874275 -4.2039194 -4.2319427 -4.2625527 -4.2832651 -4.3006039 -4.3157067][-4.1703196 -4.1450171 -4.12638 -4.124578 -4.1302357 -4.12508 -4.1096983 -4.099968 -4.1125636 -4.1399689 -4.1831045 -4.2293987 -4.2615986 -4.2857904 -4.304359][-4.1422439 -4.1100321 -4.0851889 -4.08099 -4.0819592 -4.0678782 -4.0394516 -4.024425 -4.0456414 -4.0895567 -4.1477275 -4.2056336 -4.2442446 -4.2717447 -4.2921004][-4.1117864 -4.0680513 -4.0315289 -4.0219522 -4.0180187 -3.9962437 -3.9615772 -3.9542944 -3.9953279 -4.0551229 -4.1210446 -4.1835003 -4.2258449 -4.2575555 -4.2808847][-4.1068907 -4.0510678 -4.00068 -3.9809003 -3.9657848 -3.9350762 -3.9096098 -3.9268427 -3.9886181 -4.0561161 -4.1209111 -4.1845484 -4.2273932 -4.2612324 -4.2845712][-4.1304502 -4.07038 -4.0179081 -3.9971051 -3.9786992 -3.9500477 -3.9388833 -3.9672167 -4.0275488 -4.0887027 -4.1424484 -4.198534 -4.241837 -4.2763357 -4.2970066][-4.1510849 -4.0944047 -4.0490022 -4.035255 -4.0214047 -4.0006471 -4.0004878 -4.0258408 -4.07596 -4.1281619 -4.1703067 -4.2170777 -4.2582259 -4.2927666 -4.310214][-4.1693449 -4.1240759 -4.095674 -4.094408 -4.0866327 -4.07613 -4.083602 -4.1023822 -4.1396413 -4.1761413 -4.2030373 -4.2378187 -4.2722964 -4.3016539 -4.31593][-4.1946568 -4.1628904 -4.1507077 -4.15918 -4.159173 -4.1582041 -4.1690125 -4.1813278 -4.2040544 -4.2254567 -4.2389984 -4.2613621 -4.2849545 -4.3064923 -4.3181839][-4.230195 -4.2123628 -4.2104053 -4.2222972 -4.2301769 -4.24011 -4.2524714 -4.2577434 -4.2650685 -4.2732306 -4.2767973 -4.2876987 -4.3008313 -4.3132348 -4.3216944][-4.26851 -4.2599716 -4.2609129 -4.2714109 -4.2825351 -4.2955904 -4.3081236 -4.3103623 -4.3112435 -4.312747 -4.3113422 -4.3135505 -4.3177381 -4.3236094 -4.32875][-4.2961311 -4.2927308 -4.2946749 -4.3015447 -4.3107781 -4.3229332 -4.3338289 -4.3370094 -4.3365345 -4.3347826 -4.3322926 -4.3317952 -4.3320446 -4.3329282 -4.3342457]]...]
INFO - root - 2017-12-05 15:00:15.234855: step 19110, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 73h:00m:11s remains)
INFO - root - 2017-12-05 15:00:23.533435: step 19120, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 0.808 sec/batch; 70h:21m:39s remains)
INFO - root - 2017-12-05 15:00:32.057507: step 19130, loss = 2.05, batch loss = 2.00 (10.5 examples/sec; 0.765 sec/batch; 66h:33m:51s remains)
INFO - root - 2017-12-05 15:00:40.553658: step 19140, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 76h:04m:24s remains)
INFO - root - 2017-12-05 15:00:49.104671: step 19150, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 75h:13m:55s remains)
INFO - root - 2017-12-05 15:00:57.623755: step 19160, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 72h:35m:46s remains)
INFO - root - 2017-12-05 15:01:06.306994: step 19170, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 77h:21m:52s remains)
INFO - root - 2017-12-05 15:01:14.716368: step 19180, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.824 sec/batch; 71h:43m:58s remains)
INFO - root - 2017-12-05 15:01:23.161719: step 19190, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 73h:41m:43s remains)
INFO - root - 2017-12-05 15:01:31.648148: step 19200, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 73h:52m:46s remains)
2017-12-05 15:01:32.421169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1919708 -4.1939192 -4.1927042 -4.1858015 -4.1715565 -4.1539803 -4.1322703 -4.122911 -4.1356926 -4.1351576 -4.1297011 -4.1416535 -4.155756 -4.1439838 -4.1233878][-4.1793962 -4.1724415 -4.1616178 -4.1415534 -4.1248446 -4.1142068 -4.0996366 -4.1017294 -4.1275172 -4.1281281 -4.1252165 -4.1473789 -4.1636953 -4.1509242 -4.1293182][-4.1301212 -4.112114 -4.0900331 -4.055028 -4.0482726 -4.0655985 -4.0693765 -4.0885739 -4.1381135 -4.1525097 -4.1521893 -4.1750731 -4.1808805 -4.1623878 -4.1410728][-4.0400305 -4.0227728 -4.0007219 -3.9643879 -3.9813683 -4.0379667 -4.0640268 -4.0975971 -4.155818 -4.1773882 -4.1813993 -4.1963935 -4.1869745 -4.1594009 -4.141778][-3.9578819 -3.962569 -3.963913 -3.9351625 -3.9593251 -4.0088639 -4.0327 -4.0729542 -4.1383591 -4.1679626 -4.1786165 -4.1859231 -4.1687365 -4.1370115 -4.1225815][-3.9373772 -3.9707747 -3.9866827 -3.9621575 -3.9604783 -3.9639 -3.963336 -4.0013719 -4.0667372 -4.1063867 -4.1309094 -4.14304 -4.1286364 -4.1021795 -4.0972443][-3.9992914 -4.0167933 -4.013113 -3.9844027 -3.9625156 -3.9176242 -3.8660934 -3.8791716 -3.9355323 -3.9906077 -4.0368342 -4.0656619 -4.0746226 -4.0706925 -4.0850072][-4.049017 -4.04636 -4.0171785 -3.9772258 -3.9348843 -3.8402388 -3.7383904 -3.7263956 -3.7815897 -3.8738828 -3.9519286 -3.9980178 -4.0380163 -4.06379 -4.095716][-4.0758114 -4.0545878 -3.9982753 -3.9372907 -3.8902202 -3.7951782 -3.6960571 -3.6981611 -3.7608056 -3.8611746 -3.9468668 -3.9891162 -4.0397329 -4.0836043 -4.1248][-4.0803947 -4.0470071 -3.9941947 -3.9516416 -3.9355407 -3.882998 -3.8207972 -3.829463 -3.8718779 -3.9324234 -3.9922166 -4.0260572 -4.07255 -4.1124859 -4.1503892][-4.0994406 -4.0738406 -4.04541 -4.0245233 -4.019733 -3.9936991 -3.9576306 -3.9617245 -3.9847722 -4.0165882 -4.0470634 -4.0671158 -4.0956349 -4.1249018 -4.1542244][-4.1404877 -4.1170859 -4.0977387 -4.084733 -4.0819793 -4.0680003 -4.0511065 -4.0505939 -4.0584383 -4.07176 -4.0822272 -4.0902267 -4.1042752 -4.1250315 -4.148417][-4.1654181 -4.1381192 -4.1147261 -4.1030636 -4.1028261 -4.1016335 -4.0956511 -4.0963106 -4.096776 -4.0993948 -4.1042233 -4.1038713 -4.1070881 -4.1210604 -4.1391373][-4.1687636 -4.1401296 -4.1112213 -4.098 -4.1003928 -4.1090403 -4.1151538 -4.1150331 -4.108202 -4.1074648 -4.1151156 -4.1163292 -4.1094556 -4.1145458 -4.1280475][-4.1573291 -4.1433549 -4.1231365 -4.1160359 -4.1201291 -4.1292686 -4.1358051 -4.1319413 -4.1182303 -4.1174169 -4.1288643 -4.1340485 -4.121604 -4.1161027 -4.1199718]]...]
INFO - root - 2017-12-05 15:01:41.050164: step 19210, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 76h:11m:45s remains)
INFO - root - 2017-12-05 15:01:49.416306: step 19220, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 75h:04m:47s remains)
INFO - root - 2017-12-05 15:01:58.093245: step 19230, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 73h:18m:06s remains)
INFO - root - 2017-12-05 15:02:06.717949: step 19240, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 76h:53m:04s remains)
INFO - root - 2017-12-05 15:02:15.251235: step 19250, loss = 2.10, batch loss = 2.05 (9.2 examples/sec; 0.873 sec/batch; 75h:57m:09s remains)
INFO - root - 2017-12-05 15:02:23.831964: step 19260, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 76h:57m:07s remains)
INFO - root - 2017-12-05 15:02:32.545264: step 19270, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 75h:03m:45s remains)
INFO - root - 2017-12-05 15:02:41.365403: step 19280, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 76h:31m:13s remains)
INFO - root - 2017-12-05 15:02:49.789007: step 19290, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 72h:42m:53s remains)
INFO - root - 2017-12-05 15:02:58.367454: step 19300, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 75h:16m:59s remains)
2017-12-05 15:02:59.127002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2537541 -4.2742643 -4.2896032 -4.296464 -4.2957311 -4.2926283 -4.2811847 -4.2791572 -4.2946596 -4.2958813 -4.282618 -4.2724185 -4.2691379 -4.2637525 -4.2573161][-4.2154231 -4.2439756 -4.264936 -4.2749805 -4.2754927 -4.2748303 -4.2612834 -4.2584453 -4.2760448 -4.278553 -4.2621293 -4.2484851 -4.2437019 -4.2372642 -4.2229252][-4.1908255 -4.2212791 -4.2418861 -4.2518845 -4.254324 -4.2538905 -4.2353234 -4.2347422 -4.2569165 -4.2637744 -4.2500229 -4.2382679 -4.2332573 -4.2251396 -4.2059436][-4.1762915 -4.1997619 -4.2162471 -4.2214651 -4.2254596 -4.2243276 -4.2022657 -4.2083755 -4.2416019 -4.2593865 -4.2497087 -4.2374516 -4.2292352 -4.2200165 -4.2027936][-4.172688 -4.1847272 -4.1908402 -4.185348 -4.1838741 -4.1808348 -4.1556449 -4.1666479 -4.21541 -4.2458992 -4.2433434 -4.2380929 -4.2309847 -4.2224584 -4.2115602][-4.1767583 -4.1793261 -4.1742907 -4.1572318 -4.14528 -4.1382427 -4.1042547 -4.1151237 -4.1792622 -4.22027 -4.226541 -4.2341309 -4.2357688 -4.2349162 -4.2327523][-4.1848741 -4.1779737 -4.1671362 -4.1383233 -4.1074734 -4.0830131 -4.0264978 -4.0358262 -4.1207094 -4.1771073 -4.1974554 -4.2204747 -4.2335639 -4.2424641 -4.2508125][-4.1906905 -4.1794691 -4.1676526 -4.1332417 -4.0821257 -4.0254803 -3.9299233 -3.9303651 -4.0409155 -4.1158504 -4.1511536 -4.1878 -4.2136078 -4.234561 -4.2575445][-4.1985159 -4.1914344 -4.1874042 -4.1624975 -4.1138897 -4.0485611 -3.9426961 -3.9384525 -4.0340085 -4.0967064 -4.1338563 -4.1750264 -4.2050238 -4.2307477 -4.2621484][-4.1988077 -4.1973224 -4.1997328 -4.1891127 -4.1605425 -4.1148148 -4.0411668 -4.0386357 -4.0915394 -4.1186619 -4.1394291 -4.173595 -4.1993089 -4.2217751 -4.2507782][-4.1924047 -4.18941 -4.1920204 -4.1917152 -4.1801667 -4.1519575 -4.1065712 -4.1089597 -4.1393418 -4.1476169 -4.1571383 -4.178823 -4.1961865 -4.2125306 -4.2334266][-4.2013583 -4.1914625 -4.1882119 -4.1901979 -4.189527 -4.1734319 -4.1449456 -4.1518292 -4.169786 -4.1711712 -4.1758146 -4.1885972 -4.1978049 -4.20931 -4.2235107][-4.22951 -4.2151675 -4.2064419 -4.2038574 -4.2032919 -4.1921797 -4.1735821 -4.1792493 -4.1884918 -4.1865754 -4.189096 -4.1966996 -4.2034984 -4.2134075 -4.225112][-4.2547684 -4.24272 -4.2330532 -4.2282486 -4.2248712 -4.213686 -4.2004223 -4.2034841 -4.206737 -4.2042875 -4.2064652 -4.2100959 -4.2160172 -4.22427 -4.2342377][-4.2702518 -4.2644076 -4.2610645 -4.2596784 -4.255403 -4.2445917 -4.2348027 -4.2335143 -4.2322626 -4.2294846 -4.2311544 -4.233314 -4.2392693 -4.2474504 -4.2548509]]...]
INFO - root - 2017-12-05 15:03:07.629345: step 19310, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 74h:41m:58s remains)
INFO - root - 2017-12-05 15:03:16.089802: step 19320, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 75h:44m:20s remains)
INFO - root - 2017-12-05 15:03:24.611816: step 19330, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 76h:42m:45s remains)
INFO - root - 2017-12-05 15:03:33.278724: step 19340, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 72h:15m:28s remains)
INFO - root - 2017-12-05 15:03:41.720179: step 19350, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 72h:14m:31s remains)
INFO - root - 2017-12-05 15:03:50.246281: step 19360, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 74h:34m:37s remains)
INFO - root - 2017-12-05 15:03:58.699038: step 19370, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.816 sec/batch; 71h:00m:37s remains)
INFO - root - 2017-12-05 15:04:07.161778: step 19380, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 73h:36m:51s remains)
INFO - root - 2017-12-05 15:04:15.771008: step 19390, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 78h:55m:11s remains)
INFO - root - 2017-12-05 15:04:24.137597: step 19400, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.858 sec/batch; 74h:35m:16s remains)
2017-12-05 15:04:24.892706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2010617 -4.2162604 -4.2407513 -4.2498021 -4.2385669 -4.2249761 -4.2068768 -4.185019 -4.1807346 -4.1845503 -4.18391 -4.1943364 -4.2175903 -4.2372661 -4.246645][-4.188664 -4.2081575 -4.2339172 -4.2465696 -4.2363477 -4.22508 -4.2126045 -4.1962957 -4.1921148 -4.1915026 -4.1900287 -4.1995578 -4.2161322 -4.2291183 -4.2331095][-4.1762471 -4.2001629 -4.2264977 -4.2411885 -4.23115 -4.220108 -4.2084932 -4.1980343 -4.1967697 -4.1983256 -4.2021012 -4.2088375 -4.2150769 -4.2136822 -4.2036414][-4.1608658 -4.1888132 -4.221303 -4.2430563 -4.2369046 -4.2232914 -4.2051435 -4.1945882 -4.1961966 -4.20145 -4.21405 -4.2276244 -4.2335482 -4.2230372 -4.1997471][-4.1654143 -4.1868672 -4.214488 -4.2367039 -4.2318454 -4.2151031 -4.1948695 -4.1862717 -4.1857238 -4.1910987 -4.2143173 -4.2408772 -4.25659 -4.2489944 -4.2273936][-4.1894908 -4.1986446 -4.20846 -4.2193789 -4.2061214 -4.1810303 -4.1604967 -4.1569138 -4.16042 -4.1691103 -4.2009377 -4.2348146 -4.2597027 -4.2641306 -4.2540841][-4.2067819 -4.2081733 -4.2028589 -4.1987495 -4.1704688 -4.1315269 -4.1087022 -4.1097856 -4.1215553 -4.14103 -4.1783757 -4.2133546 -4.2424278 -4.2578325 -4.26065][-4.215 -4.2114205 -4.1984224 -4.1812315 -4.1399312 -4.0820427 -4.0441027 -4.0428457 -4.067018 -4.1041708 -4.1505442 -4.1878028 -4.2182279 -4.2385874 -4.2477016][-4.217545 -4.2109289 -4.19423 -4.1699209 -4.1226487 -4.0515757 -3.9932189 -3.9829481 -4.0163369 -4.0705914 -4.1294608 -4.174274 -4.2068653 -4.2280993 -4.2357793][-4.2299333 -4.2216182 -4.2085643 -4.1900063 -4.1513391 -4.090282 -4.036973 -4.0211854 -4.0482359 -4.0995474 -4.1576929 -4.2044024 -4.237627 -4.2567825 -4.25916][-4.256331 -4.2460389 -4.2403569 -4.2339911 -4.21093 -4.1704531 -4.1367059 -4.1282029 -4.1468124 -4.1818194 -4.2248125 -4.2640138 -4.2924752 -4.3067403 -4.3047018][-4.2859139 -4.2747478 -4.2726727 -4.2758322 -4.2655363 -4.2435942 -4.2248635 -4.2216873 -4.2347212 -4.2575111 -4.2875562 -4.31758 -4.338903 -4.3488703 -4.3461738][-4.3177505 -4.307621 -4.3048053 -4.3089738 -4.3051777 -4.2948194 -4.2865281 -4.2871046 -4.2958083 -4.310699 -4.3308625 -4.3515306 -4.3658867 -4.3725529 -4.3710618][-4.3402996 -4.332273 -4.3278909 -4.328053 -4.3253131 -4.3209429 -4.3191819 -4.3225102 -4.3296041 -4.34075 -4.3540154 -4.3670115 -4.3756747 -4.3795471 -4.3783803][-4.3497686 -4.3441877 -4.3401041 -4.3382616 -4.33577 -4.3339167 -4.3349161 -4.3385153 -4.3438735 -4.3519373 -4.3610034 -4.3687515 -4.3726673 -4.3736577 -4.3721304]]...]
INFO - root - 2017-12-05 15:04:33.370898: step 19410, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 74h:58m:23s remains)
INFO - root - 2017-12-05 15:04:41.712730: step 19420, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 74h:16m:45s remains)
INFO - root - 2017-12-05 15:04:50.204422: step 19430, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 74h:35m:53s remains)
INFO - root - 2017-12-05 15:04:58.663999: step 19440, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 73h:52m:45s remains)
INFO - root - 2017-12-05 15:05:07.188256: step 19450, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 75h:52m:32s remains)
INFO - root - 2017-12-05 15:05:15.724941: step 19460, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 74h:34m:18s remains)
INFO - root - 2017-12-05 15:05:24.321050: step 19470, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 74h:04m:13s remains)
INFO - root - 2017-12-05 15:05:32.897884: step 19480, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 75h:52m:14s remains)
INFO - root - 2017-12-05 15:05:41.486487: step 19490, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 73h:02m:52s remains)
INFO - root - 2017-12-05 15:05:50.058864: step 19500, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 73h:43m:51s remains)
2017-12-05 15:05:50.794985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3514013 -4.352303 -4.3518529 -4.3509717 -4.3500605 -4.3493052 -4.3490033 -4.3484678 -4.3483181 -4.3482065 -4.3471065 -4.3461328 -4.3453178 -4.3453865 -4.3459582][-4.3500857 -4.3490415 -4.3458877 -4.3428059 -4.3412123 -4.3423886 -4.3447127 -4.3471518 -4.3488665 -4.3485165 -4.3463588 -4.3444095 -4.3431005 -4.3432651 -4.3446417][-4.3452225 -4.3395343 -4.3307939 -4.3231144 -4.3191862 -4.3213882 -4.3276052 -4.336235 -4.3428407 -4.345922 -4.3458915 -4.3448205 -4.34377 -4.3441424 -4.3459525][-4.3314047 -4.3196235 -4.3032417 -4.2876868 -4.2775831 -4.2766705 -4.2840509 -4.2998776 -4.3156762 -4.3276339 -4.3347778 -4.3388438 -4.3413825 -4.3434029 -4.3458519][-4.3030124 -4.2873058 -4.2660708 -4.2437859 -4.2245994 -4.2158918 -4.2195754 -4.2375402 -4.261703 -4.2857413 -4.3058629 -4.3206234 -4.3297424 -4.3355503 -4.3397064][-4.2562141 -4.2393827 -4.2145181 -4.1856666 -4.1583271 -4.1432858 -4.1438646 -4.1615825 -4.1910496 -4.2227054 -4.252512 -4.2786269 -4.2982845 -4.312614 -4.322475][-4.1952267 -4.18132 -4.1529818 -4.1151395 -4.0765915 -4.0556335 -4.0559454 -4.0742793 -4.105967 -4.1422696 -4.1784616 -4.2131023 -4.2434607 -4.2699986 -4.2909884][-4.124754 -4.1188855 -4.0944643 -4.0547428 -4.0112367 -3.9852889 -3.9798608 -3.9895794 -4.0139437 -4.0479226 -4.0874925 -4.1290278 -4.1699853 -4.2101054 -4.2456026][-4.06112 -4.0619621 -4.0498176 -4.0271358 -4.0003009 -3.9768419 -3.9620109 -3.9595368 -3.97187 -3.9959645 -4.0310869 -4.0737753 -4.1209888 -4.1705408 -4.2160635][-4.0630383 -4.0628948 -4.0573545 -4.0495157 -4.0443082 -4.03989 -4.0362163 -4.0368409 -4.0441046 -4.0561938 -4.0785637 -4.1109457 -4.1502504 -4.1929436 -4.2329521][-4.142683 -4.1331177 -4.1236868 -4.1199718 -4.127738 -4.1410375 -4.1549335 -4.1702867 -4.1852241 -4.1957421 -4.207684 -4.2230864 -4.2423992 -4.264678 -4.2859259][-4.2464938 -4.2350483 -4.2242489 -4.2203445 -4.2303452 -4.2504249 -4.2727151 -4.29265 -4.3082094 -4.3162642 -4.3220229 -4.3266177 -4.3307028 -4.3352709 -4.3391433][-4.3238249 -4.3177075 -4.3117819 -4.3105879 -4.3191986 -4.3347106 -4.3512611 -4.3651996 -4.374475 -4.3783379 -4.3796263 -4.3788538 -4.3761897 -4.3723297 -4.3676972][-4.3625021 -4.361424 -4.3600745 -4.3610277 -4.3661385 -4.3733206 -4.3798461 -4.3846941 -4.3872867 -4.3877459 -4.3867469 -4.3841376 -4.3803182 -4.3756304 -4.3700218][-4.3669763 -4.3676996 -4.3686786 -4.3703761 -4.3729396 -4.37497 -4.3758368 -4.3756604 -4.3749008 -4.3739719 -4.3724055 -4.3697853 -4.3663735 -4.3625178 -4.3581486]]...]
INFO - root - 2017-12-05 15:05:59.248442: step 19510, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 73h:55m:32s remains)
INFO - root - 2017-12-05 15:06:07.755220: step 19520, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 73h:00m:26s remains)
INFO - root - 2017-12-05 15:06:16.414288: step 19530, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 77h:29m:34s remains)
INFO - root - 2017-12-05 15:06:24.954419: step 19540, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 73h:53m:25s remains)
INFO - root - 2017-12-05 15:06:33.511751: step 19550, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 73h:21m:51s remains)
INFO - root - 2017-12-05 15:06:42.108407: step 19560, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 74h:39m:46s remains)
INFO - root - 2017-12-05 15:06:50.735565: step 19570, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 75h:50m:01s remains)
INFO - root - 2017-12-05 15:06:59.207120: step 19580, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 72h:55m:23s remains)
INFO - root - 2017-12-05 15:07:07.752498: step 19590, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 75h:40m:55s remains)
INFO - root - 2017-12-05 15:07:16.383579: step 19600, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 77h:13m:07s remains)
2017-12-05 15:07:17.104198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2846255 -4.2838483 -4.2843275 -4.2831712 -4.281086 -4.276979 -4.2672615 -4.258049 -4.2604632 -4.2661147 -4.2704067 -4.2755957 -4.280076 -4.2855296 -4.291492][-4.2731361 -4.272 -4.2739887 -4.2751474 -4.2776251 -4.2774229 -4.2662587 -4.2523613 -4.2534323 -4.2578783 -4.2536316 -4.2476153 -4.2506423 -4.2629633 -4.2771974][-4.2568212 -4.2528682 -4.2557664 -4.2591152 -4.2668452 -4.2691407 -4.2543478 -4.2361937 -4.2391248 -4.2478065 -4.2367496 -4.2209244 -4.2225871 -4.2402816 -4.2609048][-4.2340746 -4.2250409 -4.227324 -4.2319107 -4.243001 -4.2469416 -4.2303863 -4.2088008 -4.2139382 -4.2277465 -4.2145844 -4.1930213 -4.1943455 -4.2173443 -4.2447505][-4.1927547 -4.1750531 -4.1753321 -4.1818485 -4.1970091 -4.2033887 -4.18679 -4.1652446 -4.1784892 -4.2022347 -4.1920424 -4.1666827 -4.1691413 -4.1945081 -4.2267213][-4.1261435 -4.094511 -4.090271 -4.1021495 -4.1216149 -4.1280127 -4.1054549 -4.0856786 -4.1224313 -4.1698327 -4.1708121 -4.1488771 -4.1538382 -4.1793604 -4.2108822][-4.054018 -3.9979365 -3.9777441 -3.986475 -4.0025425 -4.0063791 -3.9760747 -3.9670687 -4.0500388 -4.1351547 -4.1571956 -4.1495862 -4.1612921 -4.1819286 -4.2003269][-4.0045581 -3.9229774 -3.8739781 -3.8621371 -3.8563583 -3.8409972 -3.7971213 -3.8063979 -3.9424136 -4.074038 -4.1231503 -4.1360917 -4.1620049 -4.1859279 -4.1949596][-4.00102 -3.9114227 -3.8430271 -3.8028524 -3.7604933 -3.7042961 -3.6335568 -3.6476622 -3.8141925 -3.9759047 -4.0554719 -4.0945568 -4.1344748 -4.1658216 -4.1804934][-4.0495224 -3.9725907 -3.9044256 -3.8490446 -3.7816792 -3.6958313 -3.6075225 -3.6052411 -3.7390833 -3.8879609 -3.9808583 -4.0390892 -4.0936022 -4.1396284 -4.16958][-4.1294246 -4.0709844 -4.0153813 -3.9640176 -3.8956227 -3.8047755 -3.7124887 -3.6779406 -3.7453847 -3.8504877 -3.9348233 -3.9992244 -4.0632162 -4.1287112 -4.17833][-4.20575 -4.1671128 -4.1302347 -4.0936236 -4.0399795 -3.9639423 -3.8790104 -3.821816 -3.8360586 -3.8959348 -3.9583228 -4.0150785 -4.0815806 -4.1571488 -4.2144752][-4.2636862 -4.2404532 -4.2197618 -4.1986575 -4.1641722 -4.1129737 -4.0536613 -4.0063109 -4.0001721 -4.0280385 -4.0669575 -4.1099992 -4.1621518 -4.2200665 -4.2630224][-4.2996325 -4.2871733 -4.2766809 -4.26683 -4.24929 -4.22088 -4.18534 -4.1568203 -4.1503682 -4.1653786 -4.1900768 -4.2182264 -4.2490864 -4.2788267 -4.2994037][-4.3127527 -4.3067565 -4.302032 -4.2982335 -4.2905111 -4.2764292 -4.2585049 -4.2436972 -4.2416997 -4.251925 -4.2672458 -4.2825565 -4.2968931 -4.3082924 -4.3149657]]...]
INFO - root - 2017-12-05 15:07:25.812479: step 19610, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 74h:11m:06s remains)
INFO - root - 2017-12-05 15:07:34.315670: step 19620, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 77h:10m:16s remains)
INFO - root - 2017-12-05 15:07:42.816197: step 19630, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 76h:09m:42s remains)
INFO - root - 2017-12-05 15:07:51.355989: step 19640, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 72h:38m:04s remains)
INFO - root - 2017-12-05 15:07:59.816935: step 19650, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 71h:32m:24s remains)
INFO - root - 2017-12-05 15:08:08.489824: step 19660, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.888 sec/batch; 77h:08m:24s remains)
INFO - root - 2017-12-05 15:08:16.977704: step 19670, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 74h:22m:04s remains)
INFO - root - 2017-12-05 15:08:25.529373: step 19680, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 73h:52m:31s remains)
INFO - root - 2017-12-05 15:08:34.103215: step 19690, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 76h:10m:07s remains)
INFO - root - 2017-12-05 15:08:42.626985: step 19700, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 74h:03m:25s remains)
2017-12-05 15:08:43.390442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.254539 -4.2318134 -4.1960769 -4.1579418 -4.1335096 -4.1407628 -4.1696444 -4.1852775 -4.1795254 -4.1604848 -4.1542878 -4.1752453 -4.1831527 -4.1851764 -4.1976647][-4.2651682 -4.2479839 -4.21579 -4.182055 -4.1545811 -4.1497912 -4.1632209 -4.16345 -4.1437354 -4.1244946 -4.1295724 -4.1603441 -4.1757317 -4.1873469 -4.2098923][-4.2660475 -4.2558479 -4.2285643 -4.1987925 -4.1702557 -4.1529646 -4.1458936 -4.12607 -4.09224 -4.0715513 -4.0852594 -4.1242852 -4.1528106 -4.1831355 -4.217536][-4.2601542 -4.2583275 -4.2386327 -4.2112589 -4.1792855 -4.1490006 -4.1253228 -4.0946412 -4.0595937 -4.0392342 -4.0550823 -4.096806 -4.1392274 -4.1855083 -4.2294726][-4.2527785 -4.2592483 -4.2475758 -4.2214861 -4.1826096 -4.1409287 -4.1076131 -4.0812635 -4.0569577 -4.0448618 -4.0625863 -4.0985827 -4.1453905 -4.1986747 -4.2453227][-4.2445149 -4.2537303 -4.2450461 -4.2129612 -4.1626334 -4.1110039 -4.0769372 -4.0659742 -4.0634708 -4.0699062 -4.0936871 -4.1232414 -4.1669984 -4.217876 -4.2605505][-4.2297797 -4.2317019 -4.2155247 -4.1708612 -4.1096373 -4.0538054 -4.0306516 -4.0392385 -4.0579176 -4.0886054 -4.1255922 -4.1569886 -4.1965137 -4.2392626 -4.272788][-4.210516 -4.1970773 -4.1656113 -4.1091828 -4.0436869 -3.9938276 -3.9879634 -4.0156584 -4.0554872 -4.1057143 -4.1568394 -4.1937804 -4.2295756 -4.2608724 -4.2826023][-4.1912527 -4.1648297 -4.121314 -4.0615911 -4.0013881 -3.9640212 -3.9751167 -4.0215507 -4.075449 -4.1320987 -4.1875014 -4.2240295 -4.2534032 -4.2744722 -4.2871284][-4.1786547 -4.1490498 -4.1030951 -4.0469556 -3.9965751 -3.9717236 -3.9893904 -4.0414543 -4.1040587 -4.1646476 -4.2168632 -4.2474332 -4.2674341 -4.278048 -4.2835045][-4.1768503 -4.151144 -4.1102872 -4.0621724 -4.0203838 -4.0009713 -4.0145688 -4.0597258 -4.1246409 -4.1871686 -4.2364154 -4.2612939 -4.269968 -4.2697353 -4.270421][-4.1832461 -4.1657114 -4.1339378 -4.0953751 -4.0618086 -4.0416183 -4.0480309 -4.0825863 -4.1429253 -4.2007318 -4.2426362 -4.260725 -4.2631197 -4.2578878 -4.2556405][-4.1911125 -4.1814365 -4.1616158 -4.1363192 -4.112411 -4.0936575 -4.0923133 -4.1156325 -4.1619916 -4.2054486 -4.236937 -4.2502656 -4.2519488 -4.2465119 -4.2433224][-4.200942 -4.1954675 -4.1856408 -4.1732774 -4.1596003 -4.1468058 -4.1421795 -4.1535864 -4.1804976 -4.2052832 -4.2242475 -4.2351122 -4.2383161 -4.2338433 -4.2319446][-4.2105041 -4.2035546 -4.197228 -4.1936584 -4.1908765 -4.1868434 -4.1828775 -4.1846957 -4.1928625 -4.2012534 -4.2098613 -4.218266 -4.2237711 -4.2227163 -4.2228222]]...]
INFO - root - 2017-12-05 15:08:51.988808: step 19710, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 75h:21m:13s remains)
INFO - root - 2017-12-05 15:09:00.391900: step 19720, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 76h:05m:33s remains)
INFO - root - 2017-12-05 15:09:08.872268: step 19730, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.810 sec/batch; 70h:20m:05s remains)
INFO - root - 2017-12-05 15:09:17.514320: step 19740, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 75h:17m:35s remains)
INFO - root - 2017-12-05 15:09:26.013199: step 19750, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 74h:22m:30s remains)
INFO - root - 2017-12-05 15:09:34.550301: step 19760, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 72h:44m:04s remains)
INFO - root - 2017-12-05 15:09:42.991851: step 19770, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 72h:57m:41s remains)
INFO - root - 2017-12-05 15:09:51.448068: step 19780, loss = 2.12, batch loss = 2.06 (9.3 examples/sec; 0.863 sec/batch; 74h:59m:53s remains)
INFO - root - 2017-12-05 15:10:00.008010: step 19790, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 77h:11m:37s remains)
INFO - root - 2017-12-05 15:10:08.596013: step 19800, loss = 2.10, batch loss = 2.05 (9.3 examples/sec; 0.858 sec/batch; 74h:33m:00s remains)
2017-12-05 15:10:09.451901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.162291 -4.1541286 -4.1741338 -4.1943693 -4.2014518 -4.2045717 -4.2121263 -4.2243075 -4.2399235 -4.2521167 -4.2434092 -4.2115049 -4.1672311 -4.1300287 -4.119535][-4.1850762 -4.1745348 -4.1868372 -4.1999149 -4.2008357 -4.1974444 -4.1987886 -4.2056684 -4.2179832 -4.2319956 -4.22641 -4.1939325 -4.1381774 -4.0831466 -4.0618081][-4.2264848 -4.2174768 -4.2191253 -4.2193818 -4.2106171 -4.19675 -4.1875892 -4.1834474 -4.1861496 -4.1998529 -4.2062612 -4.1828361 -4.12088 -4.0512562 -4.024631][-4.2602897 -4.2503433 -4.2376575 -4.2238293 -4.2087317 -4.1904559 -4.1761875 -4.162437 -4.1525311 -4.163815 -4.1846561 -4.1787167 -4.1221852 -4.042778 -4.006433][-4.2805314 -4.26708 -4.2382588 -4.2090325 -4.1891103 -4.1714239 -4.155509 -4.1333089 -4.1115227 -4.1164694 -4.1496696 -4.1672707 -4.1275678 -4.0534172 -4.0071669][-4.2922692 -4.2743173 -4.2309079 -4.1848044 -4.1571975 -4.1366148 -4.1154637 -4.0858412 -4.0587659 -4.0616794 -4.1074333 -4.1493449 -4.132287 -4.0748124 -4.0219874][-4.2922425 -4.2739592 -4.2275929 -4.173192 -4.1356554 -4.1070995 -4.0797448 -4.0489497 -4.0310974 -4.0429268 -4.0941544 -4.1495271 -4.1493645 -4.1021147 -4.0420036][-4.284934 -4.2679114 -4.2254729 -4.1734209 -4.1292448 -4.0900655 -4.0514908 -4.0196676 -4.0155759 -4.0377049 -4.0886168 -4.1489024 -4.1656723 -4.1287217 -4.0645804][-4.2823324 -4.2655559 -4.229074 -4.1805825 -4.1314583 -4.08244 -4.0360727 -4.005034 -4.0096755 -4.0398531 -4.0915794 -4.1534328 -4.1849236 -4.158998 -4.0939126][-4.2775755 -4.2654595 -4.23632 -4.19718 -4.1477623 -4.0948081 -4.0453587 -4.0200877 -4.030282 -4.0664668 -4.1185169 -4.1760411 -4.2054124 -4.1774387 -4.11473][-4.2610016 -4.2584586 -4.2412038 -4.2151656 -4.17375 -4.1243696 -4.07679 -4.05395 -4.0637722 -4.1020164 -4.1545386 -4.2049065 -4.222899 -4.1877427 -4.1249962][-4.2350559 -4.240376 -4.2352352 -4.2217669 -4.193748 -4.15816 -4.1241961 -4.1087246 -4.1166792 -4.1471782 -4.1907516 -4.2323842 -4.2393203 -4.1949186 -4.1304545][-4.2158804 -4.2247272 -4.2292356 -4.2273955 -4.2131238 -4.1906672 -4.1710253 -4.1613016 -4.1644764 -4.1805124 -4.2082648 -4.2382145 -4.2388315 -4.187964 -4.1212244][-4.2011261 -4.21048 -4.2196136 -4.2250962 -4.2233729 -4.2139745 -4.2075891 -4.2054787 -4.2057562 -4.2104292 -4.2195296 -4.2335248 -4.2288623 -4.1777797 -4.1102753][-4.1971827 -4.2033887 -4.2121086 -4.2205162 -4.2267032 -4.2276468 -4.2295356 -4.2313967 -4.2270327 -4.2211823 -4.2168112 -4.2189431 -4.2119479 -4.1657968 -4.1026]]...]
INFO - root - 2017-12-05 15:10:18.030719: step 19810, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 74h:28m:01s remains)
INFO - root - 2017-12-05 15:10:26.487569: step 19820, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 73h:50m:39s remains)
INFO - root - 2017-12-05 15:10:35.077168: step 19830, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 75h:46m:26s remains)
INFO - root - 2017-12-05 15:10:43.582008: step 19840, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 75h:27m:17s remains)
INFO - root - 2017-12-05 15:10:52.183274: step 19850, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 76h:17m:03s remains)
INFO - root - 2017-12-05 15:11:00.861412: step 19860, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 74h:20m:43s remains)
INFO - root - 2017-12-05 15:11:09.345960: step 19870, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 73h:25m:49s remains)
INFO - root - 2017-12-05 15:11:17.886772: step 19880, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 72h:11m:45s remains)
INFO - root - 2017-12-05 15:11:26.351484: step 19890, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.846 sec/batch; 73h:25m:31s remains)
INFO - root - 2017-12-05 15:11:34.922382: step 19900, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.804 sec/batch; 69h:49m:47s remains)
2017-12-05 15:11:35.682805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.233809 -4.2313623 -4.2256465 -4.2126274 -4.1858482 -4.1610775 -4.1549697 -4.1891928 -4.2400942 -4.2634048 -4.2679896 -4.2686105 -4.2452531 -4.2001343 -4.1707053][-4.2287955 -4.2254291 -4.2202349 -4.205668 -4.1731281 -4.140584 -4.1314898 -4.17004 -4.2303834 -4.259449 -4.2668157 -4.2748346 -4.2620983 -4.2198462 -4.1933031][-4.2385545 -4.2347655 -4.2265735 -4.2095118 -4.1725297 -4.1344113 -4.1214132 -4.1610637 -4.2241764 -4.2562141 -4.2666135 -4.2794886 -4.2718363 -4.226088 -4.1974826][-4.2479892 -4.2443819 -4.2331529 -4.2114697 -4.1670513 -4.1189246 -4.1033463 -4.1465635 -4.2137275 -4.2468057 -4.2616739 -4.2773294 -4.2672477 -4.2153721 -4.18016][-4.2439108 -4.2397866 -4.2256613 -4.1964903 -4.1414642 -4.0822692 -4.0647097 -4.1144133 -4.1913261 -4.227016 -4.2463264 -4.2651668 -4.254766 -4.199863 -4.1582937][-4.2317171 -4.2259669 -4.2081151 -4.1702232 -4.1052322 -4.0378604 -4.0149646 -4.071362 -4.1585841 -4.1998067 -4.2219934 -4.244453 -4.2357388 -4.181706 -4.133934][-4.2184248 -4.2100883 -4.1888056 -4.1481552 -4.0842505 -4.0149426 -3.9823341 -4.0393019 -4.1301436 -4.1748972 -4.1978712 -4.226213 -4.22501 -4.1800795 -4.1298509][-4.2130365 -4.2009654 -4.1781173 -4.1408615 -4.085485 -4.0179658 -3.9748056 -4.0225892 -4.1072254 -4.1503792 -4.1727118 -4.2068844 -4.2140059 -4.1759887 -4.1236982][-4.2278657 -4.2127905 -4.1884389 -4.154037 -4.1062641 -4.0411787 -3.9883955 -4.0139771 -4.0814643 -4.1194706 -4.1422157 -4.1788054 -4.1904874 -4.1551633 -4.1032553][-4.2660508 -4.2499537 -4.2251749 -4.190414 -4.1421757 -4.07087 -4.0033984 -4.003006 -4.0504012 -4.0827694 -4.1119161 -4.1514797 -4.1652365 -4.1299577 -4.0768876][-4.3050251 -4.2909145 -4.2677035 -4.2334018 -4.1828537 -4.1035032 -4.0183277 -3.9935615 -4.0266738 -4.0592875 -4.0956917 -4.13653 -4.1504316 -4.1106305 -4.0499988][-4.3331914 -4.3237591 -4.3048978 -4.276011 -4.2315855 -4.1586976 -4.0726423 -4.0332766 -4.0524693 -4.0802026 -4.1113939 -4.142786 -4.1469316 -4.0962105 -4.0189886][-4.3483205 -4.3431153 -4.3302965 -4.311729 -4.282969 -4.2314353 -4.1649089 -4.1271181 -4.1314869 -4.146893 -4.163703 -4.1786604 -4.1669436 -4.10308 -4.0065646][-4.3529897 -4.3494973 -4.3413224 -4.3321352 -4.31893 -4.2913709 -4.2508326 -4.2238064 -4.2202363 -4.224854 -4.2288427 -4.2301693 -4.2089453 -4.1417375 -4.0353804][-4.3524208 -4.3495131 -4.3439975 -4.3401504 -4.33596 -4.3242636 -4.3044114 -4.290307 -4.2865367 -4.2855763 -4.2825928 -4.277771 -4.2576566 -4.200716 -4.102962]]...]
INFO - root - 2017-12-05 15:11:44.361534: step 19910, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 76h:40m:49s remains)
INFO - root - 2017-12-05 15:11:52.681515: step 19920, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.860 sec/batch; 74h:40m:52s remains)
INFO - root - 2017-12-05 15:12:01.117853: step 19930, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 73h:21m:55s remains)
INFO - root - 2017-12-05 15:12:09.590290: step 19940, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 72h:52m:38s remains)
INFO - root - 2017-12-05 15:12:17.920410: step 19950, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 72h:39m:15s remains)
INFO - root - 2017-12-05 15:12:26.409688: step 19960, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 74h:59m:58s remains)
INFO - root - 2017-12-05 15:12:34.916150: step 19970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 74h:54m:37s remains)
INFO - root - 2017-12-05 15:12:43.531978: step 19980, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 74h:43m:55s remains)
INFO - root - 2017-12-05 15:12:51.956026: step 19990, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 72h:37m:23s remains)
INFO - root - 2017-12-05 15:13:00.484726: step 20000, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 75h:10m:56s remains)
2017-12-05 15:13:01.278623: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2578359 -4.2564869 -4.2640548 -4.2762995 -4.2879615 -4.292861 -4.2868853 -4.2778659 -4.2702956 -4.2652206 -4.25733 -4.2403116 -4.2367105 -4.2524719 -4.2685428][-4.2147136 -4.2089806 -4.2124186 -4.2258716 -4.2437415 -4.2514968 -4.2422762 -4.22994 -4.2213469 -4.213563 -4.2019649 -4.1822238 -4.1795325 -4.2017746 -4.2266774][-4.1783438 -4.168304 -4.1683764 -4.1812372 -4.1963897 -4.2020736 -4.1859713 -4.1672735 -4.1576719 -4.1500626 -4.1413774 -4.1264696 -4.1279154 -4.1568756 -4.1911025][-4.1588717 -4.1459341 -4.1472383 -4.1582561 -4.1610947 -4.1574793 -4.1317654 -4.1006789 -4.0881214 -4.0856214 -4.0868654 -4.0821958 -4.094408 -4.1353364 -4.1781311][-4.1456537 -4.131629 -4.1353588 -4.1369219 -4.1202469 -4.1032829 -4.0635633 -4.0048227 -3.9810233 -3.9953957 -4.0157933 -4.0247316 -4.0604234 -4.1214046 -4.1735034][-4.1389294 -4.1171937 -4.1168051 -4.0997796 -4.0517373 -4.0201015 -3.9597363 -3.8532557 -3.8150811 -3.8681867 -3.9173555 -3.9446273 -4.0148973 -4.1012945 -4.1623731][-4.1575294 -4.1275225 -4.1146936 -4.066401 -3.9720149 -3.917146 -3.8206975 -3.6487522 -3.6043084 -3.7283463 -3.8146985 -3.8568301 -3.9597602 -4.07403 -4.1455956][-4.1814661 -4.1518397 -4.1288071 -4.0571556 -3.9344325 -3.8751383 -3.7700214 -3.5723014 -3.5425835 -3.7066669 -3.7963505 -3.8275921 -3.9345326 -4.0607495 -4.1344767][-4.2199807 -4.1934443 -4.1641951 -4.0947671 -3.9893816 -3.9487123 -3.8768072 -3.7285702 -3.7117829 -3.8292053 -3.8784051 -3.8857229 -3.9650388 -4.075016 -4.1429377][-4.2528644 -4.2306433 -4.2014012 -4.1527605 -4.0859861 -4.0650449 -4.0246592 -3.9328675 -3.9195645 -3.9826987 -4.0012794 -3.9946589 -4.0388265 -4.12173 -4.1787171][-4.278172 -4.2639651 -4.2405491 -4.2150559 -4.182426 -4.1762695 -4.1552091 -4.1019144 -4.0851192 -4.1095867 -4.1124034 -4.0999861 -4.1190243 -4.1744084 -4.2186913][-4.3071752 -4.3017521 -4.2886796 -4.2788754 -4.2667317 -4.2654748 -4.2571149 -4.2283173 -4.2077923 -4.2063284 -4.1997118 -4.1879191 -4.1940084 -4.2257628 -4.2557378][-4.3201056 -4.3171387 -4.312613 -4.3117628 -4.3107734 -4.3123288 -4.3110809 -4.2992544 -4.2827854 -4.2694278 -4.2581162 -4.2492347 -4.2498021 -4.2660146 -4.2850771][-4.3231249 -4.3181973 -4.3143549 -4.3122449 -4.3120146 -4.3134327 -4.3149447 -4.3152394 -4.3089457 -4.2981687 -4.28826 -4.2817221 -4.2818379 -4.2916012 -4.3048944][-4.3290567 -4.3227353 -4.3182588 -4.3141317 -4.3127766 -4.3133307 -4.31525 -4.3181276 -4.3184633 -4.3155761 -4.311799 -4.3082156 -4.3084388 -4.31536 -4.3245397]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 15:13:10.528486: step 20010, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 74h:48m:44s remains)
INFO - root - 2017-12-05 15:13:18.933977: step 20020, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.866 sec/batch; 75h:11m:51s remains)
INFO - root - 2017-12-05 15:13:27.517029: step 20030, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 73h:41m:37s remains)
INFO - root - 2017-12-05 15:13:36.041802: step 20040, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 73h:53m:52s remains)
INFO - root - 2017-12-05 15:13:44.694246: step 20050, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 74h:13m:36s remains)
INFO - root - 2017-12-05 15:13:53.190791: step 20060, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 75h:21m:04s remains)
INFO - root - 2017-12-05 15:14:01.839835: step 20070, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 73h:44m:41s remains)
INFO - root - 2017-12-05 15:14:10.429986: step 20080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 73h:34m:49s remains)
INFO - root - 2017-12-05 15:14:19.045237: step 20090, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 76h:17m:46s remains)
INFO - root - 2017-12-05 15:14:27.457318: step 20100, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 73h:09m:58s remains)
2017-12-05 15:14:28.201178: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1833072 -4.205647 -4.2208452 -4.2176104 -4.2021 -4.1776824 -4.1421833 -4.0959525 -4.0550609 -4.0454893 -4.0564713 -4.0879846 -4.1366286 -4.1753092 -4.1987953][-4.1868134 -4.2145281 -4.2311597 -4.2289009 -4.2132239 -4.1889086 -4.1495595 -4.0904765 -4.0343 -4.0152149 -4.0281615 -4.0596313 -4.1086516 -4.1451826 -4.1639628][-4.1974516 -4.2271752 -4.24091 -4.238461 -4.22245 -4.1950536 -4.1498237 -4.0817719 -4.0160446 -3.9941573 -4.0100904 -4.04238 -4.0891266 -4.1232476 -4.1379685][-4.2145452 -4.2415943 -4.248621 -4.2380462 -4.2166853 -4.1827497 -4.1314673 -4.060544 -4.0000672 -3.9898913 -4.022007 -4.059154 -4.0960236 -4.117012 -4.1214576][-4.2338533 -4.2568297 -4.2580733 -4.2351203 -4.2083879 -4.1737633 -4.12393 -4.0578 -4.0041094 -4.0072551 -4.0593047 -4.1072645 -4.132978 -4.138494 -4.1320744][-4.2418303 -4.2609363 -4.2598724 -4.2324862 -4.205914 -4.1783438 -4.13906 -4.0817533 -4.0312419 -4.0372753 -4.0990582 -4.1571121 -4.1849713 -4.1877389 -4.1746206][-4.2353358 -4.24901 -4.2473965 -4.2259269 -4.2052193 -4.1873264 -4.1626406 -4.1190348 -4.0752087 -4.0746007 -4.13013 -4.1904936 -4.229435 -4.2422853 -4.23164][-4.2318869 -4.2427874 -4.2385545 -4.222795 -4.2074161 -4.1964893 -4.1825833 -4.1534081 -4.1163549 -4.1050711 -4.1434741 -4.1956754 -4.2441015 -4.2672615 -4.2626534][-4.2434835 -4.2537389 -4.2466803 -4.2314496 -4.2155952 -4.2040215 -4.1943278 -4.1771879 -4.1494379 -4.1373467 -4.1586108 -4.1957827 -4.2419643 -4.2677011 -4.263546][-4.25691 -4.2674322 -4.2603722 -4.2458205 -4.2309384 -4.2177925 -4.2087297 -4.1970396 -4.1791253 -4.1745186 -4.1871819 -4.2090116 -4.2400751 -4.2573423 -4.2458577][-4.26786 -4.2771978 -4.2706842 -4.2587152 -4.2449174 -4.2336588 -4.2287421 -4.22479 -4.2159634 -4.2141171 -4.2188463 -4.2298293 -4.2489309 -4.2568464 -4.2380347][-4.2771716 -4.2818332 -4.2745652 -4.2662816 -4.2561159 -4.2464976 -4.2471957 -4.2536058 -4.2542892 -4.2524714 -4.2498627 -4.2549415 -4.2676163 -4.2725158 -4.2517581][-4.2839622 -4.2838264 -4.2736826 -4.2664089 -4.260848 -4.2548923 -4.2598181 -4.271647 -4.2760234 -4.2753549 -4.2741103 -4.2783651 -4.2874737 -4.2910762 -4.273355][-4.2928562 -4.2888923 -4.2768493 -4.2663713 -4.2597218 -4.2558346 -4.2636371 -4.2785892 -4.2862639 -4.2865324 -4.2875552 -4.2919884 -4.2972145 -4.3005695 -4.2897153][-4.3048172 -4.298296 -4.284018 -4.269197 -4.2582455 -4.2559505 -4.26638 -4.2824488 -4.2926016 -4.2924962 -4.291379 -4.2924347 -4.2936106 -4.2957497 -4.2904272]]...]
INFO - root - 2017-12-05 15:14:36.844681: step 20110, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 72h:46m:59s remains)
INFO - root - 2017-12-05 15:14:45.324376: step 20120, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 73h:48m:25s remains)
INFO - root - 2017-12-05 15:14:53.774774: step 20130, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 72h:04m:45s remains)
INFO - root - 2017-12-05 15:15:02.366426: step 20140, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 74h:29m:43s remains)
INFO - root - 2017-12-05 15:15:10.904175: step 20150, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 72h:37m:20s remains)
INFO - root - 2017-12-05 15:15:19.459612: step 20160, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 72h:36m:09s remains)
INFO - root - 2017-12-05 15:15:27.857080: step 20170, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 75h:12m:39s remains)
INFO - root - 2017-12-05 15:15:36.314105: step 20180, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 73h:18m:24s remains)
INFO - root - 2017-12-05 15:15:44.899930: step 20190, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 75h:55m:50s remains)
INFO - root - 2017-12-05 15:15:53.379638: step 20200, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 77h:09m:55s remains)
2017-12-05 15:15:54.157471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2209849 -4.2340493 -4.2450633 -4.2468462 -4.2354984 -4.219851 -4.2038674 -4.187912 -4.1807904 -4.1886044 -4.20779 -4.2245274 -4.2358723 -4.2368975 -4.2275686][-4.2455935 -4.252789 -4.2558951 -4.2483349 -4.2282782 -4.2046175 -4.1822028 -4.1652102 -4.16129 -4.1738052 -4.1960959 -4.2139163 -4.2228122 -4.2238269 -4.21606][-4.2668395 -4.2695923 -4.2677126 -4.2553272 -4.2324357 -4.2054195 -4.1794395 -4.1633239 -4.1641445 -4.1804023 -4.2007647 -4.2122164 -4.2133489 -4.2113523 -4.2054815][-4.2697105 -4.267755 -4.2640228 -4.25363 -4.2356148 -4.2128768 -4.1896667 -4.1774988 -4.184319 -4.2018938 -4.2183151 -4.2221618 -4.2147131 -4.2075529 -4.2012544][-4.2409949 -4.2358823 -4.231657 -4.2257352 -4.2169266 -4.2026992 -4.1862874 -4.179143 -4.1916671 -4.2104197 -4.2241526 -4.2235055 -4.2142916 -4.2090979 -4.2070079][-4.1927328 -4.1820517 -4.17469 -4.1699619 -4.1648669 -4.1558485 -4.1442671 -4.1409574 -4.1598706 -4.1889153 -4.2114162 -4.2147069 -4.2070727 -4.2057672 -4.2090087][-4.1538129 -4.135788 -4.1218185 -4.1083641 -4.0912232 -4.074338 -4.0579467 -4.0553322 -4.0868616 -4.1354408 -4.1727066 -4.1846933 -4.1822276 -4.1880784 -4.1985178][-4.1381769 -4.1145515 -4.09198 -4.0591679 -4.0142789 -3.9714432 -3.9354198 -3.9293413 -3.9807067 -4.0578394 -4.1141996 -4.1378307 -4.1457067 -4.1630554 -4.18212][-4.151381 -4.1247568 -4.0959563 -4.04786 -3.9796579 -3.9155264 -3.8585973 -3.8378091 -3.8919175 -3.9812973 -4.0462809 -4.0777631 -4.0959597 -4.1212549 -4.1441536][-4.1832952 -4.1625767 -4.1385288 -4.0926938 -4.0273876 -3.9709187 -3.919492 -3.8877356 -3.9049404 -3.9587178 -4.001482 -4.0239944 -4.04717 -4.0787916 -4.1037903][-4.2155423 -4.2050004 -4.1907067 -4.1563182 -4.1052008 -4.0628037 -4.0222321 -3.9878061 -3.978343 -3.99548 -4.0102167 -4.0181742 -4.0359626 -4.0670576 -4.0919805][-4.2520275 -4.2480993 -4.239059 -4.2143469 -4.1795063 -4.1505 -4.1191049 -4.0836368 -4.0594974 -4.0546465 -4.0538969 -4.05131 -4.059957 -4.081953 -4.1000519][-4.2859144 -4.283514 -4.276607 -4.2601204 -4.238471 -4.2227015 -4.2044964 -4.1779089 -4.1515245 -4.1355991 -4.1262617 -4.1174917 -4.1183853 -4.1292262 -4.1384273][-4.3043551 -4.3017988 -4.2962737 -4.2869496 -4.2760735 -4.2696528 -4.2622437 -4.2482858 -4.2297106 -4.2135768 -4.201685 -4.1912689 -4.1869488 -4.1884694 -4.1886458][-4.3143167 -4.3117919 -4.307189 -4.3009248 -4.2942319 -4.2908239 -4.28864 -4.2829089 -4.27396 -4.264452 -4.2563577 -4.24846 -4.2424045 -4.238112 -4.2319984]]...]
INFO - root - 2017-12-05 15:16:02.656202: step 20210, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 73h:59m:20s remains)
INFO - root - 2017-12-05 15:16:11.137717: step 20220, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 74h:06m:09s remains)
INFO - root - 2017-12-05 15:16:19.565360: step 20230, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 73h:26m:45s remains)
INFO - root - 2017-12-05 15:16:28.050808: step 20240, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 74h:50m:45s remains)
INFO - root - 2017-12-05 15:16:36.595187: step 20250, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 72h:52m:01s remains)
INFO - root - 2017-12-05 15:16:45.087577: step 20260, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.826 sec/batch; 71h:38m:33s remains)
INFO - root - 2017-12-05 15:16:53.588588: step 20270, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 71h:59m:47s remains)
INFO - root - 2017-12-05 15:17:02.129598: step 20280, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 74h:56m:28s remains)
INFO - root - 2017-12-05 15:17:10.691672: step 20290, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 73h:17m:53s remains)
INFO - root - 2017-12-05 15:17:19.219961: step 20300, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 75h:02m:29s remains)
2017-12-05 15:17:20.035620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1810417 -4.1954589 -4.2119303 -4.2217 -4.2271738 -4.2225809 -4.2163048 -4.2139473 -4.212585 -4.213007 -4.2161684 -4.2185769 -4.21871 -4.2158122 -4.2137451][-4.1627092 -4.1783719 -4.1964159 -4.2099614 -4.2223415 -4.2251468 -4.2245536 -4.2274408 -4.2307124 -4.2328405 -4.2381015 -4.242033 -4.2423015 -4.2373638 -4.2288842][-4.1514392 -4.1645436 -4.1778622 -4.188231 -4.2017303 -4.2105756 -4.214138 -4.2203922 -4.2267051 -4.232069 -4.2398028 -4.2465205 -4.2483468 -4.2432175 -4.233233][-4.14284 -4.1506605 -4.1559248 -4.15977 -4.169683 -4.1789002 -4.1827326 -4.1897345 -4.1960716 -4.2004504 -4.2095366 -4.2198572 -4.2278218 -4.2294312 -4.2251291][-4.1429472 -4.1450977 -4.142416 -4.1367745 -4.13632 -4.1379943 -4.1363297 -4.139617 -4.1427836 -4.1459875 -4.15799 -4.173234 -4.1898394 -4.2021718 -4.2065105][-4.1554518 -4.1490293 -4.1333852 -4.11043 -4.092484 -4.0810938 -4.0694628 -4.0640588 -4.0594234 -4.0566854 -4.074317 -4.1028938 -4.1339035 -4.1611719 -4.1783633][-4.1845379 -4.1691275 -4.1383162 -4.0982609 -4.0642052 -4.0420146 -4.0229573 -4.006309 -3.9859884 -3.9667978 -3.9806564 -4.0224915 -4.0706239 -4.1132035 -4.1451926][-4.2294307 -4.2113643 -4.1771045 -4.1336827 -4.096952 -4.0746059 -4.054739 -4.031631 -3.9994168 -3.9659626 -3.9660535 -4.0031228 -4.051033 -4.0951238 -4.1289234][-4.2782044 -4.2698188 -4.2436752 -4.2080812 -4.1798325 -4.1626763 -4.1480513 -4.1304646 -4.1020765 -4.0671115 -4.0508389 -4.0607347 -4.0821357 -4.1064506 -4.1270204][-4.2960424 -4.2979031 -4.2817473 -4.2558036 -4.2375455 -4.2262869 -4.2169061 -4.2075229 -4.1906009 -4.1636314 -4.1414309 -4.1308022 -4.1249161 -4.1259608 -4.1321568][-4.2779479 -4.2878466 -4.281527 -4.2685966 -4.2625394 -4.2567906 -4.2528758 -4.2523012 -4.2467518 -4.2316566 -4.215271 -4.199625 -4.1812425 -4.1678252 -4.1632409][-4.2396808 -4.2541308 -4.2558165 -4.2547054 -4.2608862 -4.2626386 -4.263957 -4.2688208 -4.2710862 -4.2662816 -4.2605968 -4.2526 -4.2376413 -4.2247996 -4.2177572][-4.212163 -4.2258587 -4.2304211 -4.2362781 -4.2506733 -4.2598953 -4.2664719 -4.2762189 -4.2843089 -4.2859197 -4.2880816 -4.2885494 -4.2824459 -4.2759857 -4.2706728][-4.210629 -4.2197771 -4.2232552 -4.2315474 -4.2490683 -4.2622061 -4.2727675 -4.2868524 -4.2975683 -4.3020287 -4.3077135 -4.3117967 -4.3116784 -4.3086777 -4.3039637][-4.22907 -4.2330894 -4.2346878 -4.2392607 -4.2517524 -4.2629457 -4.2747583 -4.2902 -4.30196 -4.3072963 -4.3130894 -4.3177409 -4.3196387 -4.3179493 -4.3139868]]...]
INFO - root - 2017-12-05 15:17:28.448627: step 20310, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 75h:45m:33s remains)
INFO - root - 2017-12-05 15:17:36.962991: step 20320, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 74h:54m:12s remains)
INFO - root - 2017-12-05 15:17:45.598089: step 20330, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 73h:15m:03s remains)
INFO - root - 2017-12-05 15:17:54.157239: step 20340, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 73h:29m:30s remains)
INFO - root - 2017-12-05 15:18:02.663057: step 20350, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 75h:08m:30s remains)
INFO - root - 2017-12-05 15:18:11.148315: step 20360, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.829 sec/batch; 71h:54m:53s remains)
INFO - root - 2017-12-05 15:18:19.769087: step 20370, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 74h:38m:37s remains)
INFO - root - 2017-12-05 15:18:28.424856: step 20380, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 74h:05m:12s remains)
INFO - root - 2017-12-05 15:18:36.823582: step 20390, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 74h:30m:31s remains)
INFO - root - 2017-12-05 15:18:45.327353: step 20400, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 74h:07m:13s remains)
2017-12-05 15:18:46.061034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.152431 -4.1670909 -4.1842704 -4.1912727 -4.1927276 -4.1943722 -4.1920424 -4.18665 -4.1810288 -4.1758146 -4.1767688 -4.1838217 -4.1982112 -4.2193584 -4.2435336][-4.117774 -4.1237216 -4.1391897 -4.1450272 -4.1454854 -4.1481915 -4.1480403 -4.1481943 -4.1464095 -4.1409249 -4.1352577 -4.1332259 -4.1433191 -4.1708217 -4.2117815][-4.09653 -4.0996971 -4.1180806 -4.1273174 -4.1308322 -4.136117 -4.1382127 -4.1413636 -4.1379313 -4.1267509 -4.1109557 -4.0996346 -4.1042538 -4.1324015 -4.1799135][-4.1141825 -4.1168823 -4.1348438 -4.1467857 -4.1560326 -4.1628246 -4.1606679 -4.1555238 -4.1387711 -4.1144433 -4.090425 -4.0805798 -4.09312 -4.1272306 -4.1775103][-4.1684155 -4.1718178 -4.1850729 -4.1944532 -4.2020454 -4.2020593 -4.1890621 -4.1673436 -4.1318965 -4.0925093 -4.0654182 -4.0697126 -4.10314 -4.1499748 -4.2023692][-4.2314949 -4.2351604 -4.2427673 -4.2474308 -4.2493582 -4.2395058 -4.2136779 -4.173532 -4.1225677 -4.0753908 -4.0539131 -4.0798311 -4.1358256 -4.1924505 -4.241365][-4.277844 -4.2795143 -4.2821307 -4.28231 -4.2781682 -4.26033 -4.2269897 -4.1809559 -4.1294866 -4.0879712 -4.0778642 -4.1175456 -4.18262 -4.2403646 -4.2813144][-4.2999187 -4.3010392 -4.3001785 -4.2961149 -4.2874894 -4.2675824 -4.2368951 -4.197753 -4.1596208 -4.1326957 -4.1327667 -4.1725225 -4.2294011 -4.2774963 -4.30597][-4.2887378 -4.2911873 -4.2916059 -4.2884941 -4.2809172 -4.2668591 -4.2477169 -4.2248764 -4.2056131 -4.1939211 -4.1984291 -4.2272081 -4.265501 -4.29576 -4.3096657][-4.2522278 -4.25886 -4.2649069 -4.267951 -4.2676926 -4.2638121 -4.2591615 -4.2552862 -4.2539434 -4.2532058 -4.2566032 -4.2703462 -4.2870607 -4.299459 -4.3026443][-4.2182164 -4.2303662 -4.2433281 -4.2538238 -4.2614264 -4.2669916 -4.2735095 -4.2821784 -4.2913494 -4.2958283 -4.2967448 -4.2990627 -4.2999792 -4.2990718 -4.2961183][-4.2135386 -4.2284451 -4.2445111 -4.2577162 -4.2669888 -4.2753863 -4.2852631 -4.2963619 -4.306294 -4.3107953 -4.3102269 -4.3083806 -4.3033667 -4.2963619 -4.2912483][-4.2314477 -4.2457509 -4.2607689 -4.27128 -4.2762756 -4.2804461 -4.285727 -4.2906823 -4.2944393 -4.294735 -4.2930369 -4.2918692 -4.2882318 -4.2831988 -4.2815347][-4.252583 -4.2633181 -4.2746577 -4.2809134 -4.2809367 -4.278677 -4.2763624 -4.2731094 -4.2692847 -4.2648168 -4.2621074 -4.2627821 -4.2631626 -4.2633786 -4.26728][-4.2720642 -4.2779536 -4.28427 -4.2865067 -4.2838306 -4.2785997 -4.2720652 -4.2633557 -4.2535152 -4.2449889 -4.2414279 -4.243547 -4.2466774 -4.2499237 -4.2570605]]...]
INFO - root - 2017-12-05 15:18:54.731756: step 20410, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 72h:49m:05s remains)
INFO - root - 2017-12-05 15:19:03.004924: step 20420, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.846 sec/batch; 73h:20m:34s remains)
INFO - root - 2017-12-05 15:19:11.558061: step 20430, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 72h:56m:16s remains)
INFO - root - 2017-12-05 15:19:20.123421: step 20440, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.858 sec/batch; 74h:23m:10s remains)
INFO - root - 2017-12-05 15:19:28.649709: step 20450, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.812 sec/batch; 70h:24m:58s remains)
INFO - root - 2017-12-05 15:19:37.162744: step 20460, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 72h:18m:51s remains)
INFO - root - 2017-12-05 15:19:45.827846: step 20470, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 76h:41m:54s remains)
INFO - root - 2017-12-05 15:19:54.399068: step 20480, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 74h:33m:39s remains)
INFO - root - 2017-12-05 15:20:03.023053: step 20490, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 73h:52m:51s remains)
INFO - root - 2017-12-05 15:20:11.431919: step 20500, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 76h:00m:28s remains)
2017-12-05 15:20:12.195530: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.162056 -4.15071 -4.163455 -4.1681852 -4.1651869 -4.1498518 -4.113966 -4.0972857 -4.1007142 -4.0991383 -4.0959148 -4.0985384 -4.0988216 -4.0766554 -4.0634408][-4.1691737 -4.1646171 -4.1793036 -4.1849694 -4.1789083 -4.1508665 -4.1065311 -4.0897188 -4.0958672 -4.0992656 -4.1084929 -4.1127391 -4.1059465 -4.0789552 -4.0632296][-4.1725664 -4.1730208 -4.1886692 -4.1935325 -4.1822009 -4.1503563 -4.1129417 -4.0993552 -4.0955305 -4.096734 -4.1197028 -4.1339135 -4.1272097 -4.0965538 -4.0774679][-4.18352 -4.188653 -4.2049904 -4.209054 -4.1944914 -4.1632257 -4.1296759 -4.1132755 -4.0986972 -4.0946331 -4.1211066 -4.1421351 -4.1403346 -4.1081114 -4.0887189][-4.1967716 -4.2084193 -4.2232866 -4.2260761 -4.2107978 -4.1824989 -4.1482415 -4.1295013 -4.1090927 -4.09476 -4.1083336 -4.1254873 -4.1286364 -4.098206 -4.0845623][-4.2116585 -4.225636 -4.2273269 -4.2205982 -4.200501 -4.1729589 -4.1347432 -4.1122766 -4.0876336 -4.0651574 -4.0736985 -4.0928774 -4.1021972 -4.0805049 -4.0772476][-4.2312903 -4.2384362 -4.224515 -4.19622 -4.1554666 -4.1124382 -4.0625281 -4.0335112 -4.0134544 -4.0002112 -4.0243926 -4.0622082 -4.0795956 -4.0677395 -4.0729713][-4.2362704 -4.2371125 -4.2092347 -4.1603651 -4.0968614 -4.0341058 -3.9756587 -3.95274 -3.9496572 -3.9593868 -4.00399 -4.0615005 -4.0844274 -4.0809631 -4.0863309][-4.2253227 -4.2279186 -4.1995897 -4.1522956 -4.0978327 -4.0474105 -4.0053363 -3.9931638 -3.994616 -4.0149221 -4.0603018 -4.106492 -4.1188092 -4.109973 -4.1064086][-4.210639 -4.214119 -4.19423 -4.1585336 -4.1277285 -4.1030793 -4.0792656 -4.0753489 -4.074574 -4.0907497 -4.1247206 -4.1475668 -4.1470327 -4.1344748 -4.131525][-4.1917171 -4.1922417 -4.1801353 -4.1551161 -4.13433 -4.1203561 -4.1053848 -4.1068845 -4.1051846 -4.1189089 -4.1486082 -4.1646371 -4.1644955 -4.1612372 -4.1644311][-4.1763921 -4.1746235 -4.1668005 -4.1455808 -4.1264691 -4.1194916 -4.1159096 -4.1196227 -4.1162477 -4.1305466 -4.1631451 -4.1777258 -4.1792269 -4.1824813 -4.1861949][-4.1669521 -4.1578541 -4.1467037 -4.1265588 -4.1112523 -4.1141605 -4.1256595 -4.1331258 -4.1316986 -4.1500058 -4.1873875 -4.1968946 -4.1913652 -4.1913176 -4.1925397][-4.1629286 -4.1449547 -4.1296716 -4.1101842 -4.0964856 -4.1021852 -4.1245503 -4.1390324 -4.1473036 -4.1703238 -4.2074122 -4.2070708 -4.1946368 -4.1916137 -4.1967573][-4.158524 -4.1347513 -4.1193528 -4.10459 -4.09754 -4.102201 -4.1288061 -4.147305 -4.1622391 -4.1827545 -4.2121754 -4.2066073 -4.191256 -4.1932039 -4.2028389]]...]
INFO - root - 2017-12-05 15:20:20.626772: step 20510, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 72h:42m:08s remains)
INFO - root - 2017-12-05 15:20:28.863761: step 20520, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 71h:14m:52s remains)
INFO - root - 2017-12-05 15:20:37.333424: step 20530, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 74h:04m:52s remains)
INFO - root - 2017-12-05 15:20:45.806989: step 20540, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 72h:00m:34s remains)
INFO - root - 2017-12-05 15:20:54.278946: step 20550, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.856 sec/batch; 74h:08m:18s remains)
INFO - root - 2017-12-05 15:21:02.884649: step 20560, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 76h:44m:53s remains)
INFO - root - 2017-12-05 15:21:11.608378: step 20570, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 73h:50m:01s remains)
INFO - root - 2017-12-05 15:21:20.157743: step 20580, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 73h:54m:58s remains)
INFO - root - 2017-12-05 15:21:28.679651: step 20590, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 72h:17m:21s remains)
INFO - root - 2017-12-05 15:21:37.203942: step 20600, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 71h:55m:26s remains)
2017-12-05 15:21:37.972296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2417026 -4.2402186 -4.2519407 -4.2660685 -4.2677941 -4.2636328 -4.2531018 -4.2481666 -4.263308 -4.2918711 -4.3094077 -4.3033476 -4.2771711 -4.2465386 -4.2369823][-4.2064958 -4.2031412 -4.2207909 -4.2490721 -4.2633991 -4.2585125 -4.2363653 -4.219408 -4.2325983 -4.2675886 -4.2945576 -4.2945786 -4.2721448 -4.2436585 -4.2321587][-4.1400666 -4.1322823 -4.1603184 -4.2119179 -4.2450624 -4.2419896 -4.2116537 -4.1860471 -4.1988988 -4.2421017 -4.2809567 -4.2897696 -4.2713332 -4.2457271 -4.2298331][-4.0492148 -4.0369592 -4.0811734 -4.1588383 -4.20746 -4.2040749 -4.1641526 -4.1290579 -4.1431832 -4.19946 -4.256258 -4.2779527 -4.26597 -4.2428031 -4.2221665][-3.9534433 -3.9410708 -4.0078154 -4.1079988 -4.1641521 -4.1532283 -4.0954361 -4.0431223 -4.0603547 -4.1353221 -4.2115579 -4.2482743 -4.2473021 -4.2316356 -4.2105141][-3.9197578 -3.9207692 -3.9981468 -4.0936928 -4.1333303 -4.0993834 -4.0172091 -3.9508104 -3.977562 -4.0728269 -4.1677485 -4.2196851 -4.2313986 -4.224051 -4.2051792][-3.9886017 -4.00242 -4.0563445 -4.1146269 -4.1221852 -4.0704722 -3.9822204 -3.9251218 -3.9643137 -4.0605197 -4.1511855 -4.202487 -4.2200079 -4.218286 -4.2025638][-4.0787015 -4.08942 -4.1146727 -4.1369762 -4.1270037 -4.0856862 -4.0280304 -4.0005641 -4.0347829 -4.1020837 -4.1647177 -4.2019596 -4.2190018 -4.2198682 -4.2061205][-4.1245861 -4.1289167 -4.1362596 -4.139101 -4.1281791 -4.1108241 -4.0900993 -4.084949 -4.1068559 -4.14593 -4.1846881 -4.2086539 -4.2205033 -4.2199836 -4.204494][-4.1453753 -4.1414156 -4.1364293 -4.1262155 -4.1130986 -4.1101632 -4.1103616 -4.1176057 -4.1361151 -4.1658335 -4.19559 -4.2133627 -4.219358 -4.2146297 -4.1958952][-4.1670489 -4.1530166 -4.1357942 -4.1147633 -4.0964613 -4.0949507 -4.1023178 -4.1155257 -4.1358652 -4.1671214 -4.1992383 -4.2171407 -4.2194843 -4.2105083 -4.1890712][-4.1869669 -4.1694131 -4.1499729 -4.1264219 -4.1045203 -4.0988426 -4.1019568 -4.1118469 -4.1303587 -4.1614218 -4.196002 -4.2161188 -4.2186003 -4.20893 -4.1851211][-4.2033153 -4.1906195 -4.1768618 -4.15656 -4.1347318 -4.1258655 -4.1247439 -4.1332393 -4.1491327 -4.1750937 -4.2061119 -4.2261858 -4.2301702 -4.2235079 -4.2004948][-4.2121344 -4.2044587 -4.19915 -4.1884623 -4.1755705 -4.1715226 -4.1720271 -4.1806083 -4.1914172 -4.207583 -4.2288394 -4.244585 -4.248641 -4.243084 -4.2213855][-4.2112703 -4.2069211 -4.2102718 -4.2122478 -4.2103777 -4.2094073 -4.2089949 -4.2157164 -4.2223344 -4.2304354 -4.2420034 -4.2508707 -4.2541032 -4.2513528 -4.23331]]...]
INFO - root - 2017-12-05 15:21:46.563753: step 20610, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.900 sec/batch; 77h:59m:19s remains)
INFO - root - 2017-12-05 15:21:54.728658: step 20620, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 53h:58m:17s remains)
INFO - root - 2017-12-05 15:22:03.063652: step 20630, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 73h:44m:18s remains)
INFO - root - 2017-12-05 15:22:11.693018: step 20640, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 76h:56m:44s remains)
INFO - root - 2017-12-05 15:22:20.208033: step 20650, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.808 sec/batch; 70h:02m:01s remains)
INFO - root - 2017-12-05 15:22:28.785753: step 20660, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 75h:51m:44s remains)
INFO - root - 2017-12-05 15:22:37.441242: step 20670, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 77h:09m:26s remains)
INFO - root - 2017-12-05 15:22:46.031552: step 20680, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 74h:28m:07s remains)
INFO - root - 2017-12-05 15:22:54.599540: step 20690, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 75h:53m:02s remains)
INFO - root - 2017-12-05 15:23:03.030037: step 20700, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 73h:12m:06s remains)
2017-12-05 15:23:03.875940: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2183142 -4.191381 -4.1680851 -4.1579285 -4.170814 -4.2050934 -4.2484703 -4.2830219 -4.304318 -4.3099127 -4.30359 -4.2989364 -4.3019428 -4.3104835 -4.3157954][-4.2364659 -4.2050185 -4.1771884 -4.164237 -4.1737213 -4.2047367 -4.2482152 -4.2846522 -4.3079448 -4.317647 -4.3172512 -4.3144 -4.3132176 -4.3161373 -4.3181767][-4.2477188 -4.2019215 -4.1612344 -4.1382079 -4.1398396 -4.1683455 -4.21567 -4.2559662 -4.2825723 -4.2967119 -4.3031521 -4.3041286 -4.30179 -4.3032179 -4.3043604][-4.2285385 -4.1621132 -4.1020126 -4.0628386 -4.0532393 -4.0785241 -4.1351867 -4.1888471 -4.2253304 -4.2485294 -4.2678657 -4.2771106 -4.2778282 -4.2800903 -4.2803903][-4.1888118 -4.1044035 -4.022922 -3.9631495 -3.9342206 -3.9507635 -4.0137339 -4.0820704 -4.1317797 -4.1688485 -4.2056551 -4.2324629 -4.2481179 -4.2589517 -4.26322][-4.1505446 -4.0560021 -3.9549434 -3.8685102 -3.8123934 -3.8126488 -3.8735149 -3.9508386 -4.0125132 -4.06713 -4.1277761 -4.1782022 -4.2152352 -4.2413039 -4.2540369][-4.144115 -4.0524297 -3.941355 -3.8317525 -3.7474597 -3.724977 -3.7714815 -3.8472285 -3.9176567 -3.9894781 -4.073122 -4.1447392 -4.2015047 -4.2402315 -4.2587852][-4.1725054 -4.0982375 -3.9979453 -3.89142 -3.8004179 -3.7534978 -3.765229 -3.8215389 -3.8888557 -3.9703138 -4.0648456 -4.1451907 -4.2078643 -4.2518396 -4.2732606][-4.2250581 -4.1743631 -4.0984097 -4.0155625 -3.9420991 -3.8860879 -3.861851 -3.8884537 -3.9404645 -4.011148 -4.0981703 -4.1739125 -4.2304363 -4.2683568 -4.2856884][-4.2747068 -4.2436843 -4.1898975 -4.1309028 -4.0764627 -4.02855 -3.9973242 -4.0040393 -4.0391359 -4.0907922 -4.1591191 -4.2205057 -4.2615261 -4.2841945 -4.2919912][-4.3054786 -4.2895665 -4.2531214 -4.2112432 -4.1712751 -4.1387925 -4.1203828 -4.1223278 -4.1412497 -4.1762986 -4.2228532 -4.2618103 -4.2813988 -4.2873306 -4.2858758][-4.3153739 -4.3061872 -4.2807894 -4.2499948 -4.2222714 -4.2065511 -4.2014027 -4.2073464 -4.2207766 -4.2439203 -4.2695146 -4.2855244 -4.2870846 -4.2794056 -4.2684722][-4.306973 -4.297791 -4.2819781 -4.2653766 -4.2537112 -4.2513618 -4.2559433 -4.2665133 -4.278686 -4.2899189 -4.2970152 -4.2965975 -4.2860088 -4.2692285 -4.2501268][-4.2878318 -4.2749887 -4.2663913 -4.2640781 -4.2674475 -4.276896 -4.2903562 -4.3054972 -4.3170962 -4.3191323 -4.3127961 -4.3000636 -4.2813892 -4.2590442 -4.2344961][-4.2656274 -4.2467155 -4.2429013 -4.2533236 -4.2692184 -4.287991 -4.3070455 -4.3252621 -4.3354788 -4.3314195 -4.3176908 -4.2985568 -4.2774715 -4.25286 -4.224298]]...]
INFO - root - 2017-12-05 15:23:12.289827: step 20710, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.818 sec/batch; 70h:51m:06s remains)
INFO - root - 2017-12-05 15:23:20.680946: step 20720, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 0.801 sec/batch; 69h:24m:14s remains)
INFO - root - 2017-12-05 15:23:29.270188: step 20730, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 72h:50m:09s remains)
INFO - root - 2017-12-05 15:23:37.641179: step 20740, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.834 sec/batch; 72h:13m:48s remains)
INFO - root - 2017-12-05 15:23:46.184689: step 20750, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 72h:37m:45s remains)
INFO - root - 2017-12-05 15:23:54.770852: step 20760, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.880 sec/batch; 76h:13m:36s remains)
INFO - root - 2017-12-05 15:24:03.236907: step 20770, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 74h:09m:19s remains)
INFO - root - 2017-12-05 15:24:11.780810: step 20780, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 72h:15m:42s remains)
INFO - root - 2017-12-05 15:24:20.184716: step 20790, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.825 sec/batch; 71h:25m:39s remains)
INFO - root - 2017-12-05 15:24:28.704729: step 20800, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 72h:48m:02s remains)
2017-12-05 15:24:29.465060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2709332 -4.2690225 -4.2698174 -4.2734785 -4.2766986 -4.2778368 -4.2786384 -4.2808824 -4.2843924 -4.2867727 -4.2875748 -4.28648 -4.2847314 -4.2840691 -4.2855778][-4.2737255 -4.275198 -4.2779717 -4.2802567 -4.27902 -4.2748222 -4.2724509 -4.2738943 -4.2797313 -4.2849569 -4.2869749 -4.2850986 -4.2813706 -4.2788186 -4.2795539][-4.2721963 -4.2760062 -4.278708 -4.2764606 -4.2671814 -4.2552285 -4.2487135 -4.25142 -4.2646255 -4.2792997 -4.288208 -4.288764 -4.2839389 -4.2782822 -4.2766595][-4.2566404 -4.2627797 -4.2638144 -4.2534032 -4.2315993 -4.2069016 -4.1942983 -4.2000918 -4.2264738 -4.2595844 -4.2829466 -4.2917156 -4.2887864 -4.2807837 -4.2754765][-4.223722 -4.2340994 -4.2344508 -4.216311 -4.1799245 -4.1380005 -4.1138396 -4.1213903 -4.1643333 -4.2220235 -4.2647476 -4.2849183 -4.2869997 -4.278367 -4.2696576][-4.1870909 -4.2026744 -4.2037735 -4.1790586 -4.1275048 -4.0655513 -4.0261168 -4.0324006 -4.0908184 -4.1738944 -4.2370968 -4.2694969 -4.278048 -4.270803 -4.2597055][-4.1765671 -4.1934047 -4.1922245 -4.159306 -4.0902863 -4.0075736 -3.9540586 -3.9572394 -4.0270419 -4.1309338 -4.2112389 -4.2534637 -4.2666936 -4.260426 -4.2477365][-4.2037625 -4.2133803 -4.2053461 -4.1662655 -4.0890336 -3.9986782 -3.9414411 -3.9430006 -4.013854 -4.1215577 -4.2061577 -4.2513061 -4.2666593 -4.2609677 -4.2478004][-4.2409158 -4.2418823 -4.2298441 -4.1946654 -4.1286545 -4.0530758 -4.0063787 -4.0074129 -4.0645843 -4.15334 -4.2243624 -4.2637129 -4.2783456 -4.2745252 -4.2636619][-4.2678986 -4.2652783 -4.2550378 -4.2328124 -4.1913457 -4.1437826 -4.1149712 -4.1159992 -4.1516304 -4.2075157 -4.2532668 -4.2790365 -4.2882223 -4.2850127 -4.2773256][-4.2786236 -4.2760706 -4.2707009 -4.2621589 -4.2451835 -4.2247753 -4.212863 -4.2141824 -4.2311234 -4.2576857 -4.2787356 -4.2897391 -4.2920771 -4.2885084 -4.2835732][-4.278049 -4.2771254 -4.2756763 -4.2748981 -4.2716265 -4.2665787 -4.2645736 -4.2672167 -4.2744484 -4.2834888 -4.2891116 -4.2912865 -4.2904673 -4.2878017 -4.285018][-4.275353 -4.2755308 -4.2764134 -4.2787094 -4.28038 -4.2804575 -4.2815232 -4.2838507 -4.2868958 -4.28911 -4.2896347 -4.2891779 -4.2884603 -4.2873707 -4.2863011][-4.2752981 -4.2751932 -4.276278 -4.2785568 -4.2810645 -4.2826324 -4.284554 -4.2868514 -4.2883992 -4.2888293 -4.2887688 -4.28832 -4.2884493 -4.2886543 -4.2888818][-4.277524 -4.2760959 -4.2760277 -4.2772737 -4.2791858 -4.2810626 -4.283299 -4.2854905 -4.2871008 -4.2875896 -4.2876835 -4.2874403 -4.2876778 -4.2880936 -4.2885804]]...]
INFO - root - 2017-12-05 15:24:37.918139: step 20810, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 75h:08m:53s remains)
INFO - root - 2017-12-05 15:24:46.412458: step 20820, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 73h:26m:00s remains)
INFO - root - 2017-12-05 15:24:54.812315: step 20830, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 75h:22m:09s remains)
INFO - root - 2017-12-05 15:25:03.336628: step 20840, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 76h:11m:18s remains)
INFO - root - 2017-12-05 15:25:11.857175: step 20850, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.859 sec/batch; 74h:19m:13s remains)
INFO - root - 2017-12-05 15:25:20.414605: step 20860, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 75h:57m:37s remains)
INFO - root - 2017-12-05 15:25:28.964678: step 20870, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 73h:29m:57s remains)
INFO - root - 2017-12-05 15:25:37.642638: step 20880, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 73h:50m:56s remains)
INFO - root - 2017-12-05 15:25:46.160685: step 20890, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 74h:39m:55s remains)
INFO - root - 2017-12-05 15:25:54.681211: step 20900, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 71h:36m:57s remains)
2017-12-05 15:25:55.495182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0944648 -4.1851845 -4.2502036 -4.2811184 -4.2846107 -4.2748518 -4.2610006 -4.2510276 -4.2502856 -4.253396 -4.2539096 -4.2511482 -4.2486768 -4.2483897 -4.2521939][-4.1182837 -4.2031617 -4.2604575 -4.2860489 -4.2863779 -4.2735243 -4.2576714 -4.2482758 -4.2497821 -4.2521877 -4.2479815 -4.2412572 -4.2326593 -4.2250042 -4.2263694][-4.188386 -4.2493334 -4.2828 -4.2913909 -4.2823734 -4.2642903 -4.2444468 -4.2357888 -4.2436781 -4.2521181 -4.2487569 -4.2403884 -4.2249913 -4.2072496 -4.2013669][-4.2482786 -4.2813511 -4.288271 -4.277554 -4.2572942 -4.2310057 -4.2098932 -4.2047405 -4.2237606 -4.2459726 -4.2526355 -4.251935 -4.2383442 -4.2156506 -4.203783][-4.2826867 -4.2865944 -4.2663813 -4.2347441 -4.2005587 -4.1670995 -4.1444492 -4.1441803 -4.1793385 -4.2251658 -4.2523265 -4.2684135 -4.2653031 -4.244647 -4.228066][-4.2843566 -4.2619181 -4.2121634 -4.1537905 -4.1006103 -4.0544686 -4.0218167 -4.0222511 -4.0811381 -4.1621704 -4.2200871 -4.2621846 -4.278933 -4.2697749 -4.2568007][-4.2726669 -4.230319 -4.1571183 -4.0738273 -3.9970465 -3.9268479 -3.8698902 -3.8628883 -3.9465353 -4.0659533 -4.1558766 -4.223208 -4.2654128 -4.2745709 -4.2699][-4.2701793 -4.2258592 -4.15203 -4.062829 -3.9723606 -3.8810711 -3.8004332 -3.7753525 -3.8607483 -3.996105 -4.1040416 -4.1870532 -4.2454143 -4.2672472 -4.2674489][-4.2876334 -4.2533545 -4.197042 -4.1240921 -4.0458703 -3.9642196 -3.88897 -3.8511512 -3.9048617 -4.0155039 -4.1147175 -4.1956348 -4.2522902 -4.2727904 -4.269681][-4.3131986 -4.2923937 -4.2545204 -4.2051511 -4.1502256 -4.0937991 -4.0386591 -4.0008121 -4.0248632 -4.0982184 -4.17535 -4.2402806 -4.2840447 -4.2954245 -4.2848544][-4.3290496 -4.320158 -4.2988167 -4.2689195 -4.2359133 -4.2009821 -4.1634245 -4.1324477 -4.1401944 -4.186739 -4.2437391 -4.2909908 -4.3197165 -4.3212562 -4.3058252][-4.330421 -4.3288956 -4.3216648 -4.3097014 -4.2943044 -4.275712 -4.2526803 -4.2314672 -4.2333331 -4.26183 -4.2987146 -4.3272171 -4.3421955 -4.3368592 -4.3217173][-4.3261743 -4.3263083 -4.3256469 -4.3238506 -4.3195133 -4.31249 -4.3011994 -4.2901692 -4.2912326 -4.3081689 -4.3282785 -4.3412733 -4.3459058 -4.3375297 -4.3242135][-4.3213086 -4.3212843 -4.32262 -4.3249159 -4.3261127 -4.3258648 -4.3222418 -4.3177028 -4.3199835 -4.3304124 -4.3397522 -4.34289 -4.3405948 -4.331986 -4.322649][-4.3193464 -4.3190475 -4.3207169 -4.324264 -4.327354 -4.3289638 -4.3286986 -4.3272672 -4.3296633 -4.3360648 -4.3406477 -4.3401322 -4.3352814 -4.3283277 -4.322648]]...]
INFO - root - 2017-12-05 15:26:04.110298: step 20910, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 73h:50m:05s remains)
INFO - root - 2017-12-05 15:26:12.782962: step 20920, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 74h:39m:44s remains)
INFO - root - 2017-12-05 15:26:21.208468: step 20930, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 76h:26m:39s remains)
INFO - root - 2017-12-05 15:26:29.733214: step 20940, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 74h:07m:40s remains)
INFO - root - 2017-12-05 15:26:38.206667: step 20950, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 71h:57m:33s remains)
INFO - root - 2017-12-05 15:26:46.816814: step 20960, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 74h:13m:51s remains)
INFO - root - 2017-12-05 15:26:55.302133: step 20970, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 74h:51m:51s remains)
INFO - root - 2017-12-05 15:27:03.900119: step 20980, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 73h:33m:25s remains)
INFO - root - 2017-12-05 15:27:12.498690: step 20990, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 76h:25m:47s remains)
INFO - root - 2017-12-05 15:27:21.157221: step 21000, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 74h:55m:25s remains)
2017-12-05 15:27:21.964386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2994061 -4.3013444 -4.3052487 -4.3053422 -4.2966237 -4.2812734 -4.2614565 -4.2443013 -4.2413621 -4.26098 -4.290235 -4.3064804 -4.3124771 -4.3113694 -4.3003488][-4.2984877 -4.2995834 -4.3040423 -4.306499 -4.2977891 -4.2765284 -4.246223 -4.2196178 -4.2150669 -4.2404671 -4.2767549 -4.2976084 -4.3059049 -4.3084192 -4.3028908][-4.2969503 -4.2955461 -4.2984095 -4.3011937 -4.2912536 -4.2614532 -4.2157955 -4.1797943 -4.1781187 -4.2153597 -4.2620983 -4.2859688 -4.2954893 -4.30062 -4.2982173][-4.2851572 -4.2848105 -4.2872095 -4.2921848 -4.2819295 -4.2423315 -4.1797638 -4.1348825 -4.1440678 -4.1963329 -4.2524676 -4.2768064 -4.2845078 -4.2895255 -4.2877789][-4.2609582 -4.2651172 -4.2702208 -4.277524 -4.264925 -4.2117248 -4.126277 -4.0718513 -4.0982442 -4.170011 -4.23616 -4.2637906 -4.2709146 -4.2755876 -4.2724686][-4.2309566 -4.2371073 -4.2422237 -4.2475176 -4.230134 -4.163837 -4.0505304 -3.9817679 -4.0329051 -4.1339407 -4.2152538 -4.2506804 -4.2591877 -4.2630615 -4.260015][-4.2025032 -4.2075934 -4.2104721 -4.208612 -4.1838312 -4.0993652 -3.9538758 -3.8616757 -3.9423738 -4.0795841 -4.18131 -4.2282138 -4.2412848 -4.2456913 -4.2450595][-4.1825314 -4.1829333 -4.1818728 -4.1772766 -4.1487818 -4.0480719 -3.872169 -3.7503505 -3.8533597 -4.0165362 -4.1345029 -4.1945724 -4.2157364 -4.2230816 -4.2274032][-4.1753054 -4.169807 -4.1676412 -4.1707273 -4.1505418 -4.0572371 -3.8910396 -3.7717943 -3.86251 -4.0082364 -4.1119647 -4.1711249 -4.1953812 -4.2042379 -4.2138066][-4.1788254 -4.17273 -4.1749668 -4.1893587 -4.1809349 -4.1162767 -4.0019355 -3.9233987 -3.9751573 -4.0698528 -4.1370206 -4.1785054 -4.1912022 -4.19336 -4.202456][-4.195837 -4.1910748 -4.1964526 -4.2144151 -4.2158961 -4.1792493 -4.1137624 -4.0699148 -4.0931139 -4.1482186 -4.1874809 -4.2110929 -4.2103066 -4.1989546 -4.1993475][-4.2168365 -4.2129388 -4.2156739 -4.2294188 -4.2315483 -4.2109036 -4.174376 -4.1505585 -4.1618953 -4.1962342 -4.2210484 -4.2385025 -4.2339778 -4.2139959 -4.2065043][-4.2365603 -4.2291174 -4.224772 -4.2302494 -4.2249846 -4.2046785 -4.1786361 -4.1633134 -4.1737666 -4.2001081 -4.2222767 -4.2444534 -4.2469072 -4.228003 -4.2162943][-4.2340245 -4.218493 -4.2074146 -4.2080789 -4.1949863 -4.168911 -4.1435146 -4.1298494 -4.1441946 -4.1753426 -4.2001939 -4.2299857 -4.2440114 -4.2336736 -4.2250638][-4.2245188 -4.2061343 -4.1966662 -4.1996145 -4.1808414 -4.1486659 -4.1223478 -4.1083322 -4.1245728 -4.1553717 -4.1790595 -4.2125926 -4.236105 -4.2345762 -4.2292113]]...]
INFO - root - 2017-12-05 15:27:30.461520: step 21010, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 72h:40m:30s remains)
INFO - root - 2017-12-05 15:27:38.970223: step 21020, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 74h:58m:36s remains)
INFO - root - 2017-12-05 15:27:47.351744: step 21030, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 72h:01m:57s remains)
INFO - root - 2017-12-05 15:27:55.825154: step 21040, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.856 sec/batch; 74h:01m:25s remains)
INFO - root - 2017-12-05 15:28:04.353714: step 21050, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 73h:53m:12s remains)
INFO - root - 2017-12-05 15:28:12.722581: step 21060, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 71h:57m:38s remains)
INFO - root - 2017-12-05 15:28:21.256398: step 21070, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 73h:31m:29s remains)
INFO - root - 2017-12-05 15:28:29.798791: step 21080, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 76h:24m:15s remains)
INFO - root - 2017-12-05 15:28:38.171022: step 21090, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 73h:18m:04s remains)
INFO - root - 2017-12-05 15:28:46.745088: step 21100, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 75h:05m:26s remains)
2017-12-05 15:28:47.523688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3172731 -4.3167067 -4.311049 -4.3025117 -4.2879391 -4.27263 -4.2619405 -4.2588444 -4.2638721 -4.2714324 -4.2755108 -4.2780094 -4.2794271 -4.2783165 -4.2711287][-4.29901 -4.2948651 -4.2798438 -4.2610855 -4.2368469 -4.2159066 -4.2050581 -4.2074189 -4.2241859 -4.2407007 -4.2464509 -4.2481089 -4.2493744 -4.2499142 -4.2432995][-4.2807341 -4.2733078 -4.2477164 -4.2156062 -4.1795774 -4.1519856 -4.1418271 -4.1533332 -4.1869226 -4.2135754 -4.2202234 -4.2197261 -4.2197318 -4.2223387 -4.2178135][-4.2647181 -4.2574735 -4.2234168 -4.1769857 -4.1268473 -4.0890851 -4.0771337 -4.099237 -4.1505919 -4.1851158 -4.1918006 -4.189713 -4.1892972 -4.1943407 -4.1950245][-4.2548704 -4.2508163 -4.20912 -4.1475577 -4.08081 -4.0266271 -4.0043902 -4.0342689 -4.1067972 -4.1515284 -4.1600862 -4.1585631 -4.1581259 -4.166626 -4.174283][-4.2514544 -4.249229 -4.1984854 -4.1183782 -4.0303578 -3.9507358 -3.9039049 -3.9383295 -4.0367713 -4.098804 -4.1132402 -4.1123605 -4.113585 -4.1318483 -4.152184][-4.2469716 -4.2404823 -4.1784291 -4.0785861 -3.9663823 -3.8527319 -3.7697864 -3.8102703 -3.9369979 -4.01813 -4.0410547 -4.0419779 -4.0499458 -4.0831528 -4.1194744][-4.247961 -4.2327409 -4.1610003 -4.048686 -3.9210281 -3.7868013 -3.6884956 -3.7394867 -3.8755472 -3.9604576 -3.9873939 -3.9893527 -4.0026526 -4.0437183 -4.0886722][-4.2599716 -4.2405419 -4.1701918 -4.0659728 -3.950182 -3.8324509 -3.7550011 -3.8064051 -3.9180272 -3.9832566 -4.003562 -4.0023685 -4.0145416 -4.0515585 -4.0910206][-4.2700725 -4.2550879 -4.2001095 -4.1199431 -4.0332046 -3.9474938 -3.8963263 -3.9362178 -4.0114465 -4.05195 -4.063468 -4.0621667 -4.0754371 -4.1067176 -4.1337638][-4.2729263 -4.2654657 -4.2301364 -4.17937 -4.1218781 -4.0645728 -4.0315318 -4.0560536 -4.098104 -4.1183839 -4.1240864 -4.1235833 -4.1365204 -4.1612411 -4.1776152][-4.2711949 -4.2703428 -4.251832 -4.2253766 -4.1916103 -4.1574211 -4.1406522 -4.1587181 -4.1812625 -4.1897297 -4.1913462 -4.1887231 -4.1968875 -4.2137909 -4.2211337][-4.25798 -4.2595205 -4.2519093 -4.2401686 -4.2243061 -4.2090354 -4.2066245 -4.2252607 -4.2407413 -4.2433314 -4.2398753 -4.2325616 -4.2332873 -4.242312 -4.2456965][-4.2408586 -4.2422733 -4.2411518 -4.2410192 -4.23837 -4.2361708 -4.2429724 -4.2602692 -4.27068 -4.2685781 -4.259079 -4.2464504 -4.2386408 -4.2390122 -4.2422018][-4.2343903 -4.2357364 -4.2391047 -4.2460961 -4.2521591 -4.2579217 -4.2670941 -4.2788539 -4.2818542 -4.2739253 -4.2585173 -4.2411971 -4.2284427 -4.2257209 -4.2293267]]...]
INFO - root - 2017-12-05 15:28:56.081076: step 21110, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 73h:43m:05s remains)
INFO - root - 2017-12-05 15:29:04.641190: step 21120, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 74h:18m:24s remains)
INFO - root - 2017-12-05 15:29:13.084051: step 21130, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 74h:56m:20s remains)
INFO - root - 2017-12-05 15:29:21.588329: step 21140, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 75h:29m:47s remains)
INFO - root - 2017-12-05 15:29:30.235954: step 21150, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.812 sec/batch; 70h:14m:53s remains)
INFO - root - 2017-12-05 15:29:38.654961: step 21160, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 74h:49m:37s remains)
INFO - root - 2017-12-05 15:29:47.122178: step 21170, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 72h:55m:24s remains)
INFO - root - 2017-12-05 15:29:55.639851: step 21180, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 76h:29m:25s remains)
INFO - root - 2017-12-05 15:30:04.137741: step 21190, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 72h:46m:25s remains)
INFO - root - 2017-12-05 15:30:12.575318: step 21200, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 72h:18m:57s remains)
2017-12-05 15:30:13.332762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1757121 -4.149756 -4.1232982 -4.1077833 -4.119915 -4.1499071 -4.1735435 -4.1842904 -4.1864567 -4.1784239 -4.1706209 -4.1889625 -4.2165847 -4.2294183 -4.230937][-4.165462 -4.1447682 -4.1267562 -4.1154404 -4.1236372 -4.1549444 -4.1791887 -4.1911936 -4.1989393 -4.1946115 -4.1883583 -4.20242 -4.2224355 -4.2277889 -4.2247734][-4.1551924 -4.1484108 -4.1451941 -4.1378121 -4.1372414 -4.1580753 -4.169148 -4.1733084 -4.1830854 -4.1864576 -4.1912394 -4.2054482 -4.2146964 -4.2093582 -4.202877][-4.1474857 -4.1588421 -4.1705351 -4.1662703 -4.1556768 -4.1575785 -4.1454377 -4.1309776 -4.1354685 -4.1539531 -4.1800609 -4.1990714 -4.2004561 -4.1860456 -4.1725216][-4.1584821 -4.1822248 -4.1990719 -4.1900496 -4.1630006 -4.138576 -4.0937233 -4.0516758 -4.0494862 -4.0897141 -4.1492119 -4.1811895 -4.1835909 -4.1682324 -4.1446314][-4.1989803 -4.2235355 -4.2351661 -4.2182622 -4.1687269 -4.1119995 -4.03184 -3.9600606 -3.9528739 -4.0164928 -4.1089721 -4.154737 -4.1562467 -4.1431074 -4.1151719][-4.2414713 -4.2619004 -4.2672234 -4.2447495 -4.1774249 -4.0879474 -3.9728515 -3.876615 -3.8693736 -3.9510014 -4.0619149 -4.1207304 -4.124651 -4.1123857 -4.0815239][-4.2709 -4.2844305 -4.2860751 -4.2649813 -4.1987391 -4.099865 -3.9797406 -3.8877194 -3.8832631 -3.9599514 -4.0625439 -4.1212788 -4.1233196 -4.1067076 -4.0722475][-4.284915 -4.2916675 -4.2914476 -4.2763762 -4.2279911 -4.1496272 -4.0636263 -4.0136056 -4.0219288 -4.0666647 -4.1258187 -4.1636019 -4.1606035 -4.1367135 -4.0962024][-4.28784 -4.2912254 -4.2900219 -4.2795558 -4.245337 -4.188458 -4.1367307 -4.1226735 -4.144453 -4.1636004 -4.1859145 -4.2005181 -4.1912689 -4.1629658 -4.1220465][-4.2826519 -4.2826281 -4.2780418 -4.2682252 -4.2415705 -4.1980624 -4.1638803 -4.1669455 -4.1948233 -4.2036834 -4.209373 -4.2131863 -4.201457 -4.1708574 -4.1276693][-4.2806048 -4.28173 -4.2749553 -4.2638612 -4.2409058 -4.206521 -4.1832461 -4.1933532 -4.2196226 -4.2230325 -4.2230749 -4.2216825 -4.2094846 -4.1769423 -4.13022][-4.2904887 -4.2956977 -4.2882833 -4.2736473 -4.25462 -4.2322469 -4.2212696 -4.2354198 -4.256114 -4.2527781 -4.24348 -4.2320204 -4.2140112 -4.1799893 -4.1363921][-4.3021445 -4.3084722 -4.298492 -4.2814493 -4.26612 -4.2527375 -4.2495794 -4.2664194 -4.2839689 -4.2788477 -4.2658877 -4.2473865 -4.22276 -4.1859179 -4.1431065][-4.2987041 -4.3062234 -4.2973466 -4.2816725 -4.2685494 -4.2598944 -4.2593288 -4.27542 -4.2944927 -4.2952495 -4.2833519 -4.2614055 -4.2286639 -4.184442 -4.1345053]]...]
INFO - root - 2017-12-05 15:30:21.819162: step 21210, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 72h:42m:27s remains)
INFO - root - 2017-12-05 15:30:30.375166: step 21220, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.863 sec/batch; 74h:35m:06s remains)
INFO - root - 2017-12-05 15:30:38.843946: step 21230, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 75h:19m:51s remains)
INFO - root - 2017-12-05 15:30:47.339540: step 21240, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 73h:22m:23s remains)
INFO - root - 2017-12-05 15:30:55.990770: step 21250, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 76h:46m:55s remains)
INFO - root - 2017-12-05 15:31:04.548384: step 21260, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 74h:50m:32s remains)
INFO - root - 2017-12-05 15:31:12.964178: step 21270, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 73h:38m:05s remains)
INFO - root - 2017-12-05 15:31:21.491247: step 21280, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.833 sec/batch; 72h:03m:09s remains)
INFO - root - 2017-12-05 15:31:30.057964: step 21290, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 72h:19m:09s remains)
INFO - root - 2017-12-05 15:31:38.596162: step 21300, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 73h:42m:20s remains)
2017-12-05 15:31:39.399640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1302886 -4.1658783 -4.1844625 -4.2026563 -4.2010055 -4.1762657 -4.1447067 -4.112083 -4.1150084 -4.1614609 -4.2140856 -4.2602887 -4.2909551 -4.3017278 -4.305656][-4.0922613 -4.1355486 -4.1593127 -4.1799951 -4.1784167 -4.1516519 -4.1196856 -4.0847392 -4.085578 -4.1410613 -4.2072415 -4.2598042 -4.2887049 -4.3010931 -4.3060451][-4.0610461 -4.111486 -4.1391268 -4.1614223 -4.1598864 -4.1283712 -4.0939488 -4.060154 -4.0644255 -4.1292791 -4.2056918 -4.2616425 -4.2899871 -4.302743 -4.3076243][-4.0504613 -4.1068048 -4.1308203 -4.1463151 -4.1462903 -4.1215658 -4.0928016 -4.0607862 -4.06301 -4.1246767 -4.2004619 -4.2600646 -4.2918177 -4.3043094 -4.3083653][-4.0753098 -4.1242809 -4.139185 -4.1391239 -4.1367579 -4.1303945 -4.1099648 -4.081408 -4.0740991 -4.1254659 -4.19873 -4.262537 -4.2967978 -4.3079896 -4.3098445][-4.1099472 -4.1395426 -4.1349034 -4.1184173 -4.11183 -4.1181941 -4.1129718 -4.097827 -4.0915036 -4.1373382 -4.2085752 -4.2695913 -4.30186 -4.3119259 -4.311605][-4.0990267 -4.1107097 -4.0915413 -4.0639291 -4.0563421 -4.07736 -4.0953975 -4.0949526 -4.0967865 -4.1397867 -4.209012 -4.267489 -4.3001814 -4.3108845 -4.3110418][-4.0621858 -4.0733829 -4.0555382 -4.0239925 -4.023509 -4.0580063 -4.0886145 -4.0927258 -4.0925689 -4.1305594 -4.1977649 -4.2585735 -4.2930746 -4.3053184 -4.3092051][-4.0230093 -4.0468922 -4.0496097 -4.0336313 -4.0492582 -4.090023 -4.1198044 -4.1265249 -4.1214161 -4.14532 -4.2004504 -4.2582865 -4.2914844 -4.3022542 -4.3075552][-4.0253754 -4.0563183 -4.0835423 -4.0855637 -4.1042671 -4.1425514 -4.164371 -4.1682549 -4.1613159 -4.1709728 -4.2084937 -4.2597275 -4.2929378 -4.3028884 -4.3069239][-4.0911064 -4.1169357 -4.1482449 -4.1566124 -4.1685314 -4.1927004 -4.205646 -4.2005491 -4.1907358 -4.1955895 -4.2237291 -4.2690239 -4.29964 -4.307024 -4.307693][-4.1634827 -4.1762981 -4.1958179 -4.2040482 -4.2107935 -4.2245374 -4.228292 -4.2188678 -4.2093048 -4.2147365 -4.2370968 -4.2801089 -4.3057389 -4.309814 -4.3081856][-4.2067637 -4.2147822 -4.2264395 -4.2277918 -4.2251191 -4.2260547 -4.2221718 -4.2157335 -4.2136889 -4.2195787 -4.2376657 -4.2778907 -4.3025513 -4.3080869 -4.3085318][-4.2272391 -4.2285995 -4.2336087 -4.2322736 -4.2224665 -4.209424 -4.1982622 -4.1979423 -4.2029552 -4.2118874 -4.2277741 -4.2657037 -4.292872 -4.3023643 -4.3070636][-4.2520962 -4.2514143 -4.2568789 -4.2520952 -4.2305455 -4.200727 -4.1821232 -4.1830559 -4.19058 -4.2059531 -4.2269826 -4.2638149 -4.2911429 -4.3012781 -4.3065276]]...]
INFO - root - 2017-12-05 15:31:47.870343: step 21310, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 73h:32m:31s remains)
INFO - root - 2017-12-05 15:31:56.276030: step 21320, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 0.811 sec/batch; 70h:05m:12s remains)
INFO - root - 2017-12-05 15:32:04.641884: step 21330, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 73h:43m:02s remains)
INFO - root - 2017-12-05 15:32:13.042461: step 21340, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 74h:55m:01s remains)
INFO - root - 2017-12-05 15:32:21.432622: step 21350, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 71h:52m:00s remains)
INFO - root - 2017-12-05 15:32:29.739989: step 21360, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 71h:19m:42s remains)
INFO - root - 2017-12-05 15:32:38.106571: step 21370, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 74h:45m:24s remains)
INFO - root - 2017-12-05 15:32:46.481808: step 21380, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 73h:09m:56s remains)
INFO - root - 2017-12-05 15:32:54.996329: step 21390, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 71h:42m:59s remains)
INFO - root - 2017-12-05 15:33:03.548694: step 21400, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 77h:29m:52s remains)
2017-12-05 15:33:04.270972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3112245 -4.3129606 -4.3154206 -4.3175216 -4.3178635 -4.3176141 -4.3179588 -4.3179479 -4.3219986 -4.319993 -4.311666 -4.3109422 -4.3160553 -4.3191123 -4.3199191][-4.288547 -4.2894864 -4.2959023 -4.2993436 -4.2987213 -4.2953844 -4.2957096 -4.2999377 -4.31003 -4.3101707 -4.2974858 -4.2887921 -4.2905655 -4.2969418 -4.2999249][-4.2651544 -4.2654543 -4.2754736 -4.2800808 -4.2767949 -4.2685027 -4.2648034 -4.2704463 -4.285955 -4.2909274 -4.2799191 -4.2643657 -4.2628889 -4.2715106 -4.2779412][-4.2474909 -4.2469444 -4.2593203 -4.2671442 -4.26057 -4.2459559 -4.2347441 -4.2380219 -4.2580395 -4.2729969 -4.2708526 -4.2544451 -4.2496581 -4.2561264 -4.2625785][-4.223022 -4.221446 -4.2354856 -4.2488704 -4.2422018 -4.221087 -4.19813 -4.1923809 -4.2168684 -4.246418 -4.2544417 -4.2416415 -4.2340412 -4.2357965 -4.2407832][-4.191021 -4.1912045 -4.2069764 -4.2237992 -4.2167335 -4.1825271 -4.1372461 -4.1147614 -4.1463833 -4.1979856 -4.2216372 -4.2163515 -4.209085 -4.2077203 -4.2143941][-4.1675577 -4.1699858 -4.1858296 -4.2012258 -4.18991 -4.1370282 -4.0583129 -4.009047 -4.0472088 -4.1253467 -4.1710558 -4.1814427 -4.1848392 -4.1876698 -4.1988077][-4.161252 -4.164392 -4.1792221 -4.1912518 -4.1762276 -4.1110144 -4.0041709 -3.9271262 -3.9643061 -4.061192 -4.125495 -4.1543274 -4.1780396 -4.1913671 -4.203002][-4.1701622 -4.1724219 -4.1826468 -4.1915517 -4.1760058 -4.1193857 -4.021296 -3.9498649 -3.9729009 -4.0520992 -4.1099486 -4.1446271 -4.1807027 -4.2034531 -4.2134857][-4.1702256 -4.1695342 -4.1778297 -4.1874008 -4.1812382 -4.150507 -4.091969 -4.0504155 -4.0584946 -4.0926962 -4.1197815 -4.1412492 -4.1725945 -4.1987238 -4.2111225][-4.1631012 -4.1569476 -4.1623826 -4.17376 -4.17774 -4.1732264 -4.1527429 -4.1318707 -4.1253371 -4.1301908 -4.1354446 -4.1407356 -4.159606 -4.1837287 -4.1995335][-4.1644979 -4.1543074 -4.1540442 -4.1613574 -4.1670027 -4.1771421 -4.1793914 -4.1699252 -4.1572328 -4.151371 -4.14688 -4.1415486 -4.1510468 -4.1720319 -4.1908045][-4.1640053 -4.1565008 -4.1585979 -4.1659274 -4.1745772 -4.1906276 -4.2019372 -4.1984706 -4.1881523 -4.1806054 -4.1694121 -4.1548991 -4.1548781 -4.16863 -4.1875758][-4.1839924 -4.1849461 -4.1963816 -4.2092576 -4.21949 -4.2323995 -4.2435093 -4.2421803 -4.2360892 -4.2288055 -4.2134495 -4.1932511 -4.18421 -4.1891408 -4.2054996][-4.2396078 -4.2478528 -4.2625518 -4.2738352 -4.2794366 -4.2851448 -4.2910643 -4.28851 -4.2837367 -4.2773042 -4.2647657 -4.2483711 -4.2393341 -4.2408209 -4.253747]]...]
INFO - root - 2017-12-05 15:33:12.805231: step 21410, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 72h:19m:56s remains)
INFO - root - 2017-12-05 15:33:21.381134: step 21420, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 74h:32m:33s remains)
INFO - root - 2017-12-05 15:33:29.825802: step 21430, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 74h:52m:16s remains)
INFO - root - 2017-12-05 15:33:38.444225: step 21440, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 75h:13m:21s remains)
INFO - root - 2017-12-05 15:33:46.976160: step 21450, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 71h:01m:37s remains)
INFO - root - 2017-12-05 15:33:55.502267: step 21460, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 75h:08m:23s remains)
INFO - root - 2017-12-05 15:34:04.053843: step 21470, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 72h:40m:15s remains)
INFO - root - 2017-12-05 15:34:12.534705: step 21480, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 73h:10m:44s remains)
INFO - root - 2017-12-05 15:34:20.777575: step 21490, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 71h:51m:24s remains)
INFO - root - 2017-12-05 15:34:29.237065: step 21500, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 72h:26m:17s remains)
2017-12-05 15:34:29.988628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1830611 -4.1896148 -4.1925936 -4.2032967 -4.2107739 -4.2064753 -4.2023773 -4.1962409 -4.1911721 -4.1937542 -4.2007189 -4.2072711 -4.21304 -4.2153659 -4.2124543][-4.1882367 -4.1951466 -4.2012777 -4.216671 -4.2276964 -4.2250686 -4.2203131 -4.2124405 -4.2047615 -4.202642 -4.2055068 -4.2095971 -4.2096739 -4.2064815 -4.2044435][-4.2014642 -4.2099614 -4.2183738 -4.2316532 -4.2397733 -4.2372313 -4.2332482 -4.2269344 -4.2169514 -4.2100387 -4.2063222 -4.2053065 -4.2019911 -4.1981282 -4.2003551][-4.2005472 -4.2123604 -4.2233453 -4.2338691 -4.2389107 -4.234479 -4.2263479 -4.214891 -4.2013574 -4.1905351 -4.1865849 -4.1879573 -4.1890969 -4.1896081 -4.195075][-4.2083368 -4.2171931 -4.2238765 -4.2287259 -4.22894 -4.2195067 -4.2039857 -4.1838789 -4.1647196 -4.1500812 -4.1462355 -4.1534963 -4.1638007 -4.1721654 -4.1793685][-4.2168155 -4.2164397 -4.2123394 -4.205883 -4.1963243 -4.177145 -4.1508765 -4.1210833 -4.0972152 -4.0851965 -4.0906219 -4.1134152 -4.1379843 -4.1552253 -4.1655416][-4.2097178 -4.1975589 -4.1812229 -4.1601357 -4.1402512 -4.1127925 -4.0776286 -4.0420475 -4.0222244 -4.0267935 -4.0484686 -4.0854621 -4.1203022 -4.1443572 -4.1590176][-4.1784382 -4.1566315 -4.1357951 -4.1094828 -4.0825357 -4.0416346 -3.9961736 -3.9584827 -3.956948 -3.9903791 -4.0314493 -4.0742927 -4.1109419 -4.1397786 -4.1548715][-4.1388593 -4.1152344 -4.0971417 -4.0682631 -4.0267739 -3.9672773 -3.9169068 -3.895242 -3.9269061 -3.9875379 -4.0349426 -4.0751686 -4.1114635 -4.1424937 -4.1567073][-4.1241913 -4.1027555 -4.0845251 -4.0455322 -3.9845543 -3.9158576 -3.885149 -3.8968554 -3.9496572 -4.0128222 -4.0545597 -4.0859313 -4.1176925 -4.1475892 -4.16001][-4.1281376 -4.1094527 -4.0910316 -4.0466075 -3.9843125 -3.9357142 -3.934325 -3.9594474 -4.0052185 -4.0532985 -4.0823822 -4.0998435 -4.1185508 -4.1409941 -4.1502304][-4.1430397 -4.1355095 -4.1254745 -4.0903587 -4.0459561 -4.0208216 -4.0258017 -4.0438142 -4.0759797 -4.1092992 -4.1242933 -4.1242948 -4.1256247 -4.1338277 -4.1373229][-4.1767774 -4.1829 -4.1812878 -4.1577921 -4.1287317 -4.1150193 -4.1174378 -4.1297369 -4.1543469 -4.1774211 -4.1792216 -4.1600404 -4.1424184 -4.1362777 -4.1369095][-4.2129579 -4.2263222 -4.2276092 -4.2097111 -4.1913176 -4.1850829 -4.1869106 -4.1977639 -4.2152042 -4.2288256 -4.2183089 -4.1871233 -4.1603842 -4.1501956 -4.1516366][-4.2365465 -4.2496014 -4.2520666 -4.2396049 -4.2292643 -4.2256656 -4.2265539 -4.2346783 -4.2457805 -4.2493119 -4.2318249 -4.1975527 -4.1722689 -4.1656184 -4.1683121]]...]
INFO - root - 2017-12-05 15:34:38.505818: step 21510, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 75h:57m:01s remains)
INFO - root - 2017-12-05 15:34:47.032144: step 21520, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 73h:47m:25s remains)
INFO - root - 2017-12-05 15:34:55.427430: step 21530, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 73h:46m:04s remains)
INFO - root - 2017-12-05 15:35:03.960552: step 21540, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 73h:41m:42s remains)
INFO - root - 2017-12-05 15:35:12.558702: step 21550, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 73h:50m:44s remains)
INFO - root - 2017-12-05 15:35:21.078611: step 21560, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 76h:41m:51s remains)
INFO - root - 2017-12-05 15:35:29.597889: step 21570, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 72h:14m:51s remains)
INFO - root - 2017-12-05 15:35:38.160982: step 21580, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 74h:47m:25s remains)
INFO - root - 2017-12-05 15:35:46.660118: step 21590, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 74h:40m:30s remains)
INFO - root - 2017-12-05 15:35:55.082608: step 21600, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 73h:46m:06s remains)
2017-12-05 15:35:55.823287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3524389 -4.358479 -4.36404 -4.3669252 -4.3669066 -4.367425 -4.3692107 -4.3727174 -4.3766561 -4.3793168 -4.3796439 -4.3781395 -4.3753405 -4.3713417 -4.3673234][-4.3266959 -4.3377562 -4.3491592 -4.357398 -4.3610106 -4.3631134 -4.3659463 -4.3704863 -4.3754945 -4.3786874 -4.3794518 -4.3785858 -4.37587 -4.37163 -4.3665318][-4.2910938 -4.310317 -4.3299575 -4.3450413 -4.3524742 -4.355402 -4.357511 -4.3603649 -4.3637662 -4.3657064 -4.3668618 -4.3672905 -4.3665748 -4.3643508 -4.3605652][-4.2418509 -4.2668066 -4.2925735 -4.3132634 -4.3242874 -4.3277292 -4.3280115 -4.3279686 -4.3295088 -4.3319507 -4.3356013 -4.3400693 -4.3442531 -4.3476353 -4.3491888][-4.1941819 -4.2129164 -4.235631 -4.2558889 -4.2663059 -4.2689486 -4.2673578 -4.2667017 -4.2687626 -4.2749939 -4.284296 -4.2949548 -4.3061037 -4.3150945 -4.3244038][-4.1601377 -4.1603713 -4.1696882 -4.1823096 -4.1888833 -4.1899858 -4.187398 -4.1860838 -4.1874766 -4.1948829 -4.2087135 -4.2251434 -4.2414403 -4.25629 -4.276845][-4.119318 -4.0988369 -4.090941 -4.0929155 -4.095253 -4.0949655 -4.0912933 -4.0885954 -4.0878744 -4.094419 -4.1091413 -4.1279631 -4.1474028 -4.1687059 -4.2036304][-4.0646009 -4.0337982 -4.0189943 -4.0156121 -4.0158844 -4.0178452 -4.0181575 -4.017005 -4.0176053 -4.02366 -4.0363474 -4.0545158 -4.0742154 -4.0993218 -4.1427369][-4.0433331 -4.0204725 -4.0153232 -4.0199771 -4.0254483 -4.0316119 -4.0353489 -4.0371566 -4.0421972 -4.0509105 -4.0636125 -4.0814109 -4.0996871 -4.120542 -4.1551728][-4.0919647 -4.0852571 -4.0957241 -4.1139774 -4.1294065 -4.141109 -4.1479044 -4.1517797 -4.1584473 -4.1663871 -4.1764336 -4.1898651 -4.2026148 -4.214375 -4.2324033][-4.1959586 -4.1975808 -4.2120938 -4.2329626 -4.2507772 -4.2637753 -4.2715087 -4.2761827 -4.2817574 -4.2869225 -4.2920032 -4.2974453 -4.3014264 -4.3038845 -4.308465][-4.2988796 -4.299799 -4.3081236 -4.3209391 -4.3321686 -4.3409896 -4.3464513 -4.3497553 -4.3534555 -4.3558521 -4.3570304 -4.3561649 -4.3546467 -4.3523474 -4.35079][-4.3547144 -4.3533149 -4.3558064 -4.36063 -4.3643126 -4.36778 -4.3701663 -4.3713832 -4.3726954 -4.3734717 -4.3733006 -4.3712716 -4.3686829 -4.3658862 -4.3633223][-4.3610272 -4.359324 -4.3594871 -4.36068 -4.361208 -4.3624563 -4.3637538 -4.3643932 -4.3653069 -4.366385 -4.3670316 -4.3660512 -4.3643265 -4.3625717 -4.3608394][-4.3481541 -4.3471746 -4.3470569 -4.3474646 -4.3483038 -4.3501043 -4.3520961 -4.3533416 -4.3553505 -4.3580952 -4.3598194 -4.3597779 -4.3582668 -4.3570437 -4.3557582]]...]
INFO - root - 2017-12-05 15:36:04.515788: step 21610, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 76h:47m:55s remains)
INFO - root - 2017-12-05 15:36:13.218091: step 21620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 73h:51m:24s remains)
INFO - root - 2017-12-05 15:36:21.583420: step 21630, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 73h:57m:53s remains)
INFO - root - 2017-12-05 15:36:30.164506: step 21640, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 75h:17m:48s remains)
INFO - root - 2017-12-05 15:36:38.766253: step 21650, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.851 sec/batch; 73h:31m:16s remains)
INFO - root - 2017-12-05 15:36:47.399882: step 21660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 74h:21m:46s remains)
INFO - root - 2017-12-05 15:36:55.893191: step 21670, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 73h:45m:35s remains)
INFO - root - 2017-12-05 15:37:04.434304: step 21680, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 73h:31m:05s remains)
INFO - root - 2017-12-05 15:37:12.922342: step 21690, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 72h:40m:52s remains)
INFO - root - 2017-12-05 15:37:21.347503: step 21700, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 73h:07m:44s remains)
2017-12-05 15:37:22.070082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2037153 -4.2034097 -4.1993079 -4.194262 -4.1859465 -4.1680818 -4.1421919 -4.1159611 -4.11516 -4.1316705 -4.1481042 -4.1580496 -4.1783814 -4.2060528 -4.2327681][-4.1736345 -4.1740408 -4.1668353 -4.157568 -4.1487327 -4.1281371 -4.0958395 -4.0615087 -4.0660734 -4.0861335 -4.1024361 -4.1160493 -4.1461449 -4.1839361 -4.212605][-4.158771 -4.1639605 -4.15365 -4.1395297 -4.1272082 -4.1030703 -4.0621595 -4.0234761 -4.0343938 -4.05789 -4.0761285 -4.098227 -4.1320553 -4.1732936 -4.2041206][-4.15137 -4.1589637 -4.1469965 -4.1267824 -4.1145988 -4.0894294 -4.0363512 -3.9923282 -4.0147552 -4.04966 -4.074038 -4.1022434 -4.1350274 -4.1714382 -4.2026711][-4.1327562 -4.1343622 -4.1213946 -4.0946064 -4.0763645 -4.0395136 -3.9703116 -3.926589 -3.9691572 -4.0232468 -4.05678 -4.0937548 -4.1296411 -4.1661024 -4.1997848][-4.1069374 -4.1039429 -4.0916181 -4.0535231 -4.0080442 -3.9387732 -3.839087 -3.7915261 -3.873642 -3.9651151 -4.0222993 -4.0747447 -4.1244369 -4.167366 -4.2036781][-4.0976052 -4.0901022 -4.0713444 -4.0185585 -3.9417138 -3.8283067 -3.6855562 -3.6403801 -3.7696266 -3.9044714 -3.9957144 -4.0696692 -4.1337867 -4.1820354 -4.2185631][-4.1235948 -4.1163793 -4.093399 -4.0413041 -3.9593723 -3.8329673 -3.6855314 -3.6486022 -3.7739818 -3.9150875 -4.0185661 -4.09971 -4.1664004 -4.2099924 -4.2393646][-4.1453304 -4.1428685 -4.1229553 -4.0831304 -4.0234118 -3.9313405 -3.8201523 -3.7821481 -3.8658133 -3.9824226 -4.0765009 -4.1488485 -4.2031112 -4.2354503 -4.2575364][-4.1721215 -4.17198 -4.1572595 -4.1245961 -4.0837293 -4.022512 -3.9350429 -3.8931355 -3.9434195 -4.0370193 -4.1200171 -4.1821756 -4.2265005 -4.250144 -4.2680717][-4.2104783 -4.2078447 -4.1935682 -4.1645846 -4.1277571 -4.075429 -3.9959471 -3.9536848 -3.9857488 -4.06375 -4.1392107 -4.1956325 -4.237175 -4.2574477 -4.273777][-4.2361245 -4.2251573 -4.2036862 -4.1741414 -4.139276 -4.0893903 -4.020762 -3.9884236 -4.0153346 -4.0838995 -4.1511092 -4.2053061 -4.2446775 -4.2651138 -4.2819023][-4.2346578 -4.2195911 -4.2000256 -4.1724496 -4.1389027 -4.0941954 -4.042757 -4.0267782 -4.0544858 -4.1137795 -4.1703706 -4.2161384 -4.2512188 -4.2716112 -4.2889895][-4.2315149 -4.219048 -4.2055478 -4.17946 -4.1458478 -4.1046157 -4.0708704 -4.0684142 -4.0966868 -4.144824 -4.1890984 -4.2250571 -4.254374 -4.2735863 -4.2903209][-4.217783 -4.211369 -4.2049766 -4.1833429 -4.1512871 -4.1170344 -4.0934997 -4.0925961 -4.1153426 -4.15293 -4.1899352 -4.2216372 -4.2479978 -4.2672858 -4.2859721]]...]
INFO - root - 2017-12-05 15:37:30.522906: step 21710, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 71h:57m:26s remains)
INFO - root - 2017-12-05 15:37:39.135798: step 21720, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 75h:16m:07s remains)
INFO - root - 2017-12-05 15:37:47.637966: step 21730, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.872 sec/batch; 75h:14m:43s remains)
INFO - root - 2017-12-05 15:37:56.262751: step 21740, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.867 sec/batch; 74h:48m:33s remains)
INFO - root - 2017-12-05 15:38:04.855167: step 21750, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 76h:28m:56s remains)
INFO - root - 2017-12-05 15:38:13.406124: step 21760, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 72h:02m:52s remains)
INFO - root - 2017-12-05 15:38:22.103536: step 21770, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 74h:30m:32s remains)
INFO - root - 2017-12-05 15:38:30.654681: step 21780, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 72h:44m:53s remains)
INFO - root - 2017-12-05 15:38:39.235962: step 21790, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 72h:05m:10s remains)
INFO - root - 2017-12-05 15:38:47.790652: step 21800, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 72h:57m:47s remains)
2017-12-05 15:38:48.534224: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3420115 -4.3401747 -4.3395543 -4.34008 -4.3378372 -4.3294339 -4.3129749 -4.2913823 -4.2773404 -4.2787166 -4.2866793 -4.2959952 -4.3124166 -4.3151932 -4.3151221][-4.3369417 -4.3327246 -4.3307343 -4.3303118 -4.32562 -4.3144135 -4.2986045 -4.2797165 -4.2681074 -4.2694459 -4.2775869 -4.2884068 -4.3091636 -4.3144341 -4.3135281][-4.3354893 -4.3288074 -4.3235087 -4.3187609 -4.30827 -4.2915664 -4.2765641 -4.2635379 -4.2580013 -4.2619743 -4.2690644 -4.2801943 -4.3060303 -4.3164258 -4.3153715][-4.3372812 -4.327517 -4.3169112 -4.305871 -4.2875829 -4.2623062 -4.2390656 -4.2277055 -4.2328172 -4.2455554 -4.2552137 -4.2684155 -4.3008208 -4.3175812 -4.3169122][-4.3346329 -4.3211451 -4.3064246 -4.2893624 -4.2645712 -4.2285309 -4.191864 -4.1768327 -4.1941004 -4.2198672 -4.2382379 -4.25741 -4.295825 -4.3180227 -4.3176408][-4.3231344 -4.305408 -4.2851076 -4.2607403 -4.2319345 -4.1878052 -4.1328626 -4.1075311 -4.1354017 -4.1792908 -4.2150078 -4.245153 -4.2905426 -4.3171763 -4.3177471][-4.309536 -4.289731 -4.263886 -4.231102 -4.1957107 -4.1405025 -4.0610561 -4.0137668 -4.0476913 -4.1208744 -4.1835628 -4.2288189 -4.2819571 -4.3133621 -4.3169217][-4.3028636 -4.2842407 -4.2562137 -4.2182164 -4.1737313 -4.1043715 -3.9970841 -3.9166811 -3.9485445 -4.0576057 -4.1530652 -4.2143431 -4.2729726 -4.3076391 -4.3154726][-4.3032813 -4.2868419 -4.2605562 -4.2227349 -4.1733532 -4.09466 -3.9776156 -3.8834209 -3.9128308 -4.0391374 -4.1478233 -4.2121844 -4.2690587 -4.3028059 -4.3132858][-4.3076806 -4.2932706 -4.2695355 -4.235105 -4.1903586 -4.1223378 -4.0328236 -3.9687073 -3.9935865 -4.0865912 -4.1706195 -4.2218056 -4.2698855 -4.3005691 -4.3110256][-4.3148279 -4.3040204 -4.2845335 -4.2565541 -4.2223191 -4.1792064 -4.13188 -4.0985346 -4.1099663 -4.1586022 -4.2059431 -4.2378354 -4.2758646 -4.3009925 -4.3096519][-4.3224669 -4.3199072 -4.3113813 -4.2940831 -4.2721286 -4.2475796 -4.2249875 -4.2031751 -4.198874 -4.2179785 -4.24043 -4.25911 -4.2864108 -4.305799 -4.3115873][-4.3237724 -4.3303118 -4.3336716 -4.3284512 -4.3194313 -4.308989 -4.2947769 -4.2734208 -4.259769 -4.2646885 -4.2740254 -4.2816668 -4.2989593 -4.3122063 -4.3155665][-4.316566 -4.3284454 -4.3383169 -4.3408456 -4.3415847 -4.342391 -4.33574 -4.3182611 -4.3022943 -4.2987251 -4.2988424 -4.2977128 -4.3075442 -4.3152556 -4.3177791][-4.2996006 -4.3114538 -4.3228745 -4.33031 -4.3384023 -4.3475909 -4.34673 -4.3332224 -4.3187938 -4.3129878 -4.3087058 -4.303823 -4.3106813 -4.3152018 -4.3172]]...]
INFO - root - 2017-12-05 15:38:57.024835: step 21810, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 72h:29m:16s remains)
INFO - root - 2017-12-05 15:39:05.616440: step 21820, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.884 sec/batch; 76h:16m:07s remains)
INFO - root - 2017-12-05 15:39:14.128788: step 21830, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 73h:38m:19s remains)
INFO - root - 2017-12-05 15:39:22.689064: step 21840, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 75h:31m:27s remains)
INFO - root - 2017-12-05 15:39:31.337241: step 21850, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 73h:28m:12s remains)
INFO - root - 2017-12-05 15:39:39.856658: step 21860, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 75h:04m:23s remains)
INFO - root - 2017-12-05 15:39:48.377768: step 21870, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 74h:12m:47s remains)
INFO - root - 2017-12-05 15:39:56.836435: step 21880, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.842 sec/batch; 72h:40m:38s remains)
INFO - root - 2017-12-05 15:40:05.247704: step 21890, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.812 sec/batch; 70h:04m:43s remains)
INFO - root - 2017-12-05 15:40:13.676326: step 21900, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 71h:48m:15s remains)
2017-12-05 15:40:14.461516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.265841 -4.2583346 -4.2638044 -4.2709394 -4.2682657 -4.258688 -4.2515259 -4.2423372 -4.2414308 -4.2469778 -4.2529664 -4.2533774 -4.2534971 -4.2627039 -4.2819586][-4.2102275 -4.2037048 -4.2187 -4.2372556 -4.2385306 -4.2274251 -4.2187939 -4.2063618 -4.1993284 -4.1988873 -4.2028375 -4.2090468 -4.21813 -4.2334132 -4.2583365][-4.1418395 -4.1306353 -4.1566486 -4.195416 -4.2098989 -4.2028327 -4.1925297 -4.1749172 -4.1609373 -4.1548419 -4.155602 -4.1659641 -4.1885085 -4.213737 -4.243475][-4.0917315 -4.0698843 -4.103405 -4.159523 -4.1843719 -4.1818628 -4.1719174 -4.1487155 -4.1287665 -4.1170774 -4.1186523 -4.134017 -4.1640816 -4.1968536 -4.2331309][-4.0907917 -4.058702 -4.0889807 -4.14346 -4.1639113 -4.1581874 -4.1456132 -4.1201944 -4.1021781 -4.0950475 -4.0995369 -4.1142073 -4.1446185 -4.1817827 -4.2257257][-4.099196 -4.0699005 -4.0965543 -4.1332636 -4.1334291 -4.1183605 -4.102541 -4.0772896 -4.0674019 -4.0804081 -4.1008711 -4.1232896 -4.1548395 -4.1920772 -4.2378831][-4.0849738 -4.0631719 -4.0810037 -4.0924277 -4.063272 -4.0299983 -3.9940765 -3.9574156 -3.9524271 -4.0023074 -4.06109 -4.1049943 -4.1515961 -4.1992421 -4.2489219][-4.0607882 -4.0377259 -4.0405312 -4.0290122 -3.9781535 -3.9199612 -3.8479447 -3.7737989 -3.7577009 -3.8530335 -3.9643741 -4.0446744 -4.1171269 -4.1838574 -4.244771][-4.0777073 -4.0587554 -4.0574746 -4.0459208 -4.0027461 -3.94094 -3.8513961 -3.7506986 -3.7099307 -3.7957435 -3.9076974 -3.9950047 -4.0758343 -4.1594534 -4.2348576][-4.1358438 -4.1228943 -4.1296482 -4.1354027 -4.1108747 -4.0650425 -4.0012741 -3.9278328 -3.8898478 -3.9228306 -3.9784081 -4.0339451 -4.09734 -4.1728373 -4.2443972][-4.1801963 -4.1723757 -4.1813622 -4.1950321 -4.1860461 -4.1616473 -4.1293807 -4.084619 -4.0509849 -4.0558767 -4.0804296 -4.1140609 -4.1565304 -4.2115278 -4.2682648][-4.2084084 -4.2072396 -4.2141933 -4.2259793 -4.221343 -4.21022 -4.1972575 -4.1736321 -4.1448464 -4.1375089 -4.1520467 -4.1770353 -4.2070189 -4.2498178 -4.2922359][-4.2273178 -4.2291279 -4.2346416 -4.24353 -4.2412267 -4.236342 -4.2322531 -4.2218862 -4.2036672 -4.1956463 -4.2046261 -4.2207241 -4.2428584 -4.27745 -4.30966][-4.2623281 -4.265759 -4.2710671 -4.2769308 -4.2742238 -4.26855 -4.2654595 -4.2610865 -4.251287 -4.2467313 -4.2533517 -4.2651725 -4.2841196 -4.309401 -4.3296242][-4.3071456 -4.3117905 -4.3172684 -4.3207073 -4.3182039 -4.3132677 -4.3107147 -4.309495 -4.3060822 -4.3053617 -4.3097062 -4.3168111 -4.3278823 -4.3396273 -4.3482537]]...]
INFO - root - 2017-12-05 15:40:22.914721: step 21910, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 73h:04m:37s remains)
INFO - root - 2017-12-05 15:40:31.316794: step 21920, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 72h:23m:24s remains)
INFO - root - 2017-12-05 15:40:39.663847: step 21930, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 72h:18m:28s remains)
INFO - root - 2017-12-05 15:40:48.208182: step 21940, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 74h:05m:07s remains)
INFO - root - 2017-12-05 15:40:56.804904: step 21950, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 74h:56m:11s remains)
INFO - root - 2017-12-05 15:41:05.434798: step 21960, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 73h:53m:45s remains)
INFO - root - 2017-12-05 15:41:13.922010: step 21970, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 74h:22m:40s remains)
INFO - root - 2017-12-05 15:41:22.525773: step 21980, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 75h:38m:36s remains)
INFO - root - 2017-12-05 15:41:30.989304: step 21990, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 74h:40m:09s remains)
INFO - root - 2017-12-05 15:41:39.577266: step 22000, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 72h:51m:48s remains)
2017-12-05 15:41:40.335083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2491541 -4.2032895 -4.1602211 -4.1415086 -4.1366367 -4.1083212 -4.0730305 -4.0659575 -4.0684657 -4.0630765 -4.048676 -4.0381575 -4.050426 -4.0690732 -4.0832253][-4.2620654 -4.2226453 -4.1872878 -4.1702738 -4.16152 -4.1262274 -4.080204 -4.0564442 -4.0465636 -4.041007 -4.041141 -4.0501142 -4.0654845 -4.0716133 -4.069922][-4.2704482 -4.236937 -4.2075334 -4.1908445 -4.1774659 -4.1368113 -4.0823975 -4.0428419 -4.0229926 -4.0152636 -4.0310555 -4.0594049 -4.0788279 -4.0775566 -4.0674987][-4.2729225 -4.2397437 -4.2084584 -4.18445 -4.1588092 -4.1100717 -4.0566707 -4.0231051 -4.0120845 -4.01297 -4.0407834 -4.0772214 -4.0892444 -4.0799375 -4.0723472][-4.26944 -4.2324367 -4.1919332 -4.153357 -4.1052351 -4.0370684 -3.9876132 -3.9826665 -4.0078387 -4.032321 -4.0699892 -4.103219 -4.1042 -4.088779 -4.0883584][-4.2599459 -4.2146645 -4.1618538 -4.1056547 -4.0253315 -3.9201744 -3.8704729 -3.912539 -3.9912386 -4.0450611 -4.0835738 -4.109839 -4.1081185 -4.0961113 -4.103663][-4.2453876 -4.1926584 -4.13062 -4.0590596 -3.9513326 -3.8098323 -3.7617357 -3.8503604 -3.968915 -4.0415454 -4.079545 -4.0960255 -4.0956688 -4.0918851 -4.1104198][-4.2323461 -4.1770263 -4.1127634 -4.0439515 -3.9506538 -3.8335433 -3.8001547 -3.8825779 -3.9842787 -4.04939 -4.0811963 -4.0854387 -4.0792866 -4.0785813 -4.1067648][-4.2307234 -4.1776175 -4.1196713 -4.0708346 -4.0230665 -3.9566276 -3.9335637 -3.9795463 -4.0426197 -4.0898385 -4.1155667 -4.1098576 -4.094861 -4.0910916 -4.1152186][-4.2365227 -4.1876669 -4.1388974 -4.1071882 -4.0878439 -4.0564818 -4.0435748 -4.0665874 -4.1020565 -4.1343989 -4.1584144 -4.15009 -4.1289539 -4.1187396 -4.1329675][-4.2506495 -4.2090821 -4.1702552 -4.1435738 -4.1290712 -4.1155949 -4.111506 -4.1225343 -4.14063 -4.1656775 -4.1939459 -4.1899118 -4.1717405 -4.1590257 -4.1609969][-4.2740626 -4.2396879 -4.2038546 -4.1739368 -4.15768 -4.1496782 -4.1481929 -4.1581764 -4.1774707 -4.2036033 -4.2346573 -4.2379179 -4.2256961 -4.2102375 -4.1980419][-4.296155 -4.2654104 -4.2324905 -4.2079625 -4.1961479 -4.1904392 -4.1899509 -4.200357 -4.221416 -4.2440982 -4.2663212 -4.2721572 -4.2638545 -4.2493062 -4.2321353][-4.3085003 -4.2835865 -4.2580051 -4.2437358 -4.2392125 -4.2395735 -4.2434087 -4.2518539 -4.2658672 -4.2783818 -4.2899027 -4.2931738 -4.2878451 -4.2780385 -4.2631521][-4.3123097 -4.2923832 -4.2763052 -4.270431 -4.2715831 -4.2755475 -4.2805033 -4.2848921 -4.2928071 -4.2999935 -4.3042741 -4.3043966 -4.301374 -4.2967138 -4.2887645]]...]
INFO - root - 2017-12-05 15:41:48.851718: step 22010, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 74h:22m:34s remains)
INFO - root - 2017-12-05 15:41:57.280297: step 22020, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 72h:22m:48s remains)
INFO - root - 2017-12-05 15:42:05.671337: step 22030, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 73h:40m:38s remains)
INFO - root - 2017-12-05 15:42:14.100931: step 22040, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 74h:30m:35s remains)
INFO - root - 2017-12-05 15:42:22.685680: step 22050, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 72h:54m:05s remains)
INFO - root - 2017-12-05 15:42:31.285391: step 22060, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 74h:57m:09s remains)
INFO - root - 2017-12-05 15:42:39.885417: step 22070, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 77h:22m:49s remains)
INFO - root - 2017-12-05 15:42:48.430785: step 22080, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 73h:43m:12s remains)
INFO - root - 2017-12-05 15:42:56.979493: step 22090, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 74h:11m:52s remains)
INFO - root - 2017-12-05 15:43:05.526969: step 22100, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.849 sec/batch; 73h:13m:26s remains)
2017-12-05 15:43:06.250713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2065787 -4.2001567 -4.1854987 -4.1733685 -4.1759124 -4.1840944 -4.1817985 -4.1626616 -4.144383 -4.1249471 -4.1364231 -4.166914 -4.2105184 -4.261631 -4.302011][-4.1662016 -4.1637993 -4.1534057 -4.1446629 -4.1472759 -4.1519332 -4.1508641 -4.1368737 -4.1244884 -4.1108646 -4.130619 -4.1675582 -4.2144184 -4.2641354 -4.303525][-4.1301203 -4.1323195 -4.1286392 -4.1225519 -4.1161351 -4.1109915 -4.1066556 -4.1005769 -4.1020555 -4.10463 -4.1369529 -4.1820941 -4.2276754 -4.2701159 -4.3030949][-4.1069946 -4.1187687 -4.1238203 -4.1179304 -4.10264 -4.0827541 -4.0650425 -4.0577517 -4.0707927 -4.0944366 -4.1444407 -4.1974664 -4.2424445 -4.2787 -4.3056769][-4.0844927 -4.1087561 -4.1250005 -4.1252227 -4.1073332 -4.0695119 -4.0315208 -4.0173445 -4.0385385 -4.0809245 -4.1459665 -4.2042279 -4.2487431 -4.2832956 -4.3069911][-4.0650167 -4.0977545 -4.1220455 -4.1265779 -4.106214 -4.0565834 -4.000546 -3.9796505 -4.0073237 -4.065414 -4.1432834 -4.2080021 -4.2543087 -4.2884135 -4.3097224][-4.0725284 -4.106647 -4.1341763 -4.1350336 -4.1121373 -4.0599089 -3.9944847 -3.9724405 -4.000916 -4.062396 -4.1437826 -4.211123 -4.2611084 -4.2927451 -4.31284][-4.1183929 -4.1396914 -4.1607332 -4.1589503 -4.1341066 -4.0856638 -4.0233335 -4.0002894 -4.0277672 -4.0830669 -4.1546488 -4.2166338 -4.2640772 -4.2957468 -4.3165288][-4.1642728 -4.1736474 -4.1846309 -4.1846209 -4.167479 -4.1344342 -4.0886831 -4.0699296 -4.0949464 -4.1381531 -4.1897783 -4.2370114 -4.2759128 -4.3043957 -4.3231454][-4.1927834 -4.1981459 -4.2064877 -4.2084894 -4.1963134 -4.1803184 -4.1532288 -4.144021 -4.1674628 -4.1975141 -4.2322507 -4.2663774 -4.2950854 -4.3174572 -4.3321657][-4.2212777 -4.2281256 -4.2335196 -4.2340326 -4.2238069 -4.2177448 -4.2011876 -4.19691 -4.2205734 -4.2451015 -4.2731638 -4.2998204 -4.3190436 -4.3330445 -4.3416924][-4.2615328 -4.2718992 -4.2775211 -4.2774434 -4.2683997 -4.2606058 -4.245378 -4.23882 -4.256835 -4.2773237 -4.3033595 -4.3251133 -4.3362131 -4.3429255 -4.3457861][-4.2886386 -4.3017063 -4.3096304 -4.3110123 -4.3029323 -4.2945213 -4.2819033 -4.2723064 -4.2818079 -4.2962041 -4.3162289 -4.3338451 -4.3403454 -4.3433003 -4.3441548][-4.3062186 -4.3194385 -4.3282666 -4.3311653 -4.3254523 -4.3186569 -4.3089614 -4.3003168 -4.302856 -4.3093929 -4.3228307 -4.3359995 -4.340322 -4.3421888 -4.3434758][-4.3230076 -4.3353395 -4.3426352 -4.3459215 -4.3421683 -4.3368635 -4.3293796 -4.324244 -4.3233042 -4.32398 -4.3322225 -4.340344 -4.3422074 -4.34327 -4.3446441]]...]
INFO - root - 2017-12-05 15:43:14.734118: step 22110, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 75h:34m:37s remains)
INFO - root - 2017-12-05 15:43:23.310372: step 22120, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 72h:38m:07s remains)
INFO - root - 2017-12-05 15:43:31.616641: step 22130, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.809 sec/batch; 69h:46m:54s remains)
INFO - root - 2017-12-05 15:43:40.200195: step 22140, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 74h:29m:50s remains)
INFO - root - 2017-12-05 15:43:48.664071: step 22150, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 77h:18m:29s remains)
INFO - root - 2017-12-05 15:43:57.259081: step 22160, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 72h:03m:24s remains)
INFO - root - 2017-12-05 15:44:05.770713: step 22170, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 72h:09m:12s remains)
INFO - root - 2017-12-05 15:44:14.378100: step 22180, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 74h:01m:57s remains)
INFO - root - 2017-12-05 15:44:23.052460: step 22190, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 74h:41m:08s remains)
INFO - root - 2017-12-05 15:44:31.667156: step 22200, loss = 2.01, batch loss = 1.96 (9.3 examples/sec; 0.862 sec/batch; 74h:15m:40s remains)
2017-12-05 15:44:32.429212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3029366 -4.2949615 -4.2924433 -4.2921066 -4.2939868 -4.2959728 -4.294982 -4.2914324 -4.2881804 -4.2870474 -4.2885666 -4.2942581 -4.30248 -4.3109479 -4.3185582][-4.2838116 -4.2689567 -4.2645259 -4.2664728 -4.27084 -4.2696218 -4.2659011 -4.2582664 -4.2525282 -4.2511597 -4.2575808 -4.2667584 -4.2776852 -4.291153 -4.3051476][-4.262887 -4.237515 -4.2262778 -4.227798 -4.235167 -4.2287407 -4.218513 -4.2069392 -4.2009382 -4.2030196 -4.2191219 -4.23123 -4.242023 -4.2605829 -4.2851529][-4.2431288 -4.2072926 -4.1883969 -4.188447 -4.1930194 -4.1768951 -4.1559792 -4.1395 -4.1349149 -4.1456037 -4.1748767 -4.1887765 -4.1959639 -4.2166 -4.2547021][-4.2296386 -4.1862836 -4.1619735 -4.1617 -4.1553483 -4.1234846 -4.0891333 -4.0700388 -4.0719714 -4.0989113 -4.1410351 -4.1519156 -4.1513247 -4.1690741 -4.2209692][-4.22277 -4.1727543 -4.1444807 -4.1408944 -4.1234226 -4.0779862 -4.0287004 -4.0029526 -4.0152874 -4.068532 -4.1220088 -4.1260748 -4.1150126 -4.1292233 -4.1970353][-4.2198386 -4.1645832 -4.1309204 -4.1221371 -4.1025848 -4.0528803 -3.9932137 -3.9573851 -3.9757466 -4.0497751 -4.1111164 -4.11244 -4.0964527 -4.110817 -4.192018][-4.2281661 -4.1748915 -4.1354513 -4.1237888 -4.1098294 -4.0706062 -4.013093 -3.9696033 -3.9788172 -4.0504503 -4.1113329 -4.1180267 -4.1054144 -4.12287 -4.2069736][-4.2528081 -4.2111998 -4.1743937 -4.1580906 -4.145237 -4.1198893 -4.0748219 -4.0326381 -4.0272722 -4.0777316 -4.1322784 -4.1462831 -4.14243 -4.1614294 -4.2341547][-4.282073 -4.2585788 -4.2291479 -4.2064595 -4.1902189 -4.1776094 -4.1449747 -4.1095376 -4.0976357 -4.1279836 -4.1718698 -4.1918321 -4.1972933 -4.2135143 -4.26572][-4.2993293 -4.287518 -4.2651625 -4.2395797 -4.2210884 -4.2211771 -4.2009354 -4.1734381 -4.1600189 -4.180088 -4.2136674 -4.2344542 -4.2449036 -4.2561555 -4.2893848][-4.3063264 -4.2988329 -4.2823782 -4.2597528 -4.2440567 -4.2504244 -4.2425933 -4.2248 -4.21248 -4.2277441 -4.2524066 -4.2677755 -4.2751727 -4.2821064 -4.302834][-4.3028774 -4.295805 -4.2825136 -4.2664747 -4.259068 -4.2714138 -4.2732925 -4.2621727 -4.2507992 -4.2611537 -4.2780137 -4.2858624 -4.28898 -4.29444 -4.3085418][-4.2951779 -4.2873173 -4.2765741 -4.2683954 -4.2694931 -4.2836533 -4.2876225 -4.2811518 -4.2731972 -4.2805429 -4.2917352 -4.2965713 -4.297174 -4.3013248 -4.3115931][-4.2926674 -4.2849345 -4.2762823 -4.2722521 -4.2756896 -4.28639 -4.2895918 -4.2874336 -4.2844944 -4.2924814 -4.3019414 -4.307725 -4.3084464 -4.3103085 -4.3156643]]...]
INFO - root - 2017-12-05 15:44:41.085438: step 22210, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 72h:02m:00s remains)
INFO - root - 2017-12-05 15:44:49.589502: step 22220, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 72h:36m:30s remains)
INFO - root - 2017-12-05 15:44:57.902267: step 22230, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 73h:14m:56s remains)
INFO - root - 2017-12-05 15:45:06.472148: step 22240, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 73h:50m:21s remains)
INFO - root - 2017-12-05 15:45:14.912823: step 22250, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 73h:48m:59s remains)
INFO - root - 2017-12-05 15:45:23.399785: step 22260, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 72h:51m:38s remains)
INFO - root - 2017-12-05 15:45:31.997466: step 22270, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 73h:05m:44s remains)
INFO - root - 2017-12-05 15:45:40.520628: step 22280, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 73h:34m:58s remains)
INFO - root - 2017-12-05 15:45:49.096862: step 22290, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 75h:30m:36s remains)
INFO - root - 2017-12-05 15:45:57.663492: step 22300, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 73h:27m:41s remains)
2017-12-05 15:45:58.422618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3221493 -4.3093691 -4.2976427 -4.2885828 -4.2737126 -4.2546058 -4.22855 -4.2011557 -4.1740918 -4.1618795 -4.1729612 -4.1880245 -4.2037721 -4.2192311 -4.2400208][-4.319828 -4.3015523 -4.2852468 -4.2720237 -4.251492 -4.2249041 -4.1874318 -4.1459236 -4.1088223 -4.0947518 -4.1106577 -4.1302404 -4.1514645 -4.1765008 -4.2099051][-4.3158879 -4.2913027 -4.2662516 -4.2452846 -4.2188668 -4.1842871 -4.1365232 -4.08581 -4.04751 -4.0440817 -4.0714564 -4.0958915 -4.1205735 -4.1488204 -4.1880226][-4.3064718 -4.2773647 -4.2453952 -4.2180157 -4.1881304 -4.1417742 -4.0790324 -4.0195308 -3.9835668 -4.0046568 -4.054595 -4.083003 -4.1045361 -4.1341705 -4.1749444][-4.2933764 -4.2630486 -4.2277079 -4.1958032 -4.1581087 -4.0904856 -4.0051003 -3.9239111 -3.8935025 -3.9567051 -4.0364385 -4.0695224 -4.0908451 -4.1233916 -4.1635346][-4.2837195 -4.25435 -4.2184443 -4.1827354 -4.1306782 -4.0354643 -3.917985 -3.8031666 -3.778187 -3.9048617 -4.0259342 -4.0710511 -4.0969043 -4.1295128 -4.1650219][-4.2827964 -4.2575111 -4.2254539 -4.185451 -4.1145782 -3.9886236 -3.8292811 -3.6663737 -3.6427035 -3.8428628 -4.0084925 -4.0716591 -4.1047149 -4.1396832 -4.1763167][-4.2857089 -4.2660904 -4.2408638 -4.19793 -4.113646 -3.9685249 -3.7826414 -3.5874004 -3.5674715 -3.8018579 -3.9863508 -4.0613284 -4.1021147 -4.14268 -4.1833782][-4.2882051 -4.2710962 -4.2520261 -4.212677 -4.1324434 -3.9953787 -3.8240957 -3.6569371 -3.6512258 -3.8486338 -4.0135779 -4.0882244 -4.12631 -4.1651783 -4.1990986][-4.2898388 -4.2708573 -4.2545009 -4.2225928 -4.1558504 -4.0457473 -3.9087439 -3.784121 -3.7801917 -3.9194407 -4.0492139 -4.1158047 -4.1530952 -4.1938624 -4.2221766][-4.2957 -4.2740593 -4.2575502 -4.2297416 -4.1768913 -4.0920029 -3.9865646 -3.8959556 -3.8863864 -3.9761994 -4.0743861 -4.1300273 -4.1671491 -4.2123337 -4.2372994][-4.3004804 -4.2763634 -4.2553668 -4.2294588 -4.1897526 -4.1284361 -4.0525618 -3.9871879 -3.9743705 -4.0333767 -4.1051626 -4.1457858 -4.1821146 -4.2302604 -4.2531791][-4.3012114 -4.2733026 -4.2456336 -4.2194161 -4.1936464 -4.15527 -4.103951 -4.05799 -4.0466156 -4.0895977 -4.139647 -4.1677475 -4.2016053 -4.249032 -4.2699747][-4.3008671 -4.2715092 -4.2406483 -4.215692 -4.1986957 -4.1783171 -4.1479049 -4.1176224 -4.1093907 -4.1415954 -4.179266 -4.1989622 -4.2236557 -4.2605758 -4.2788215][-4.2987156 -4.2715697 -4.2439718 -4.2247624 -4.2142706 -4.2041149 -4.1872625 -4.1657929 -4.1577115 -4.1793222 -4.2061057 -4.2190337 -4.2351189 -4.264214 -4.2833877]]...]
INFO - root - 2017-12-05 15:46:06.988123: step 22310, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 73h:14m:09s remains)
INFO - root - 2017-12-05 15:46:15.576870: step 22320, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 76h:34m:55s remains)
INFO - root - 2017-12-05 15:46:24.044896: step 22330, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.865 sec/batch; 74h:30m:11s remains)
INFO - root - 2017-12-05 15:46:32.517289: step 22340, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.876 sec/batch; 75h:29m:59s remains)
INFO - root - 2017-12-05 15:46:41.119027: step 22350, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 76h:25m:32s remains)
INFO - root - 2017-12-05 15:46:49.670129: step 22360, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 74h:02m:16s remains)
INFO - root - 2017-12-05 15:46:58.171121: step 22370, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 72h:16m:25s remains)
INFO - root - 2017-12-05 15:47:06.668405: step 22380, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 71h:41m:26s remains)
INFO - root - 2017-12-05 15:47:15.222180: step 22390, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 74h:43m:03s remains)
INFO - root - 2017-12-05 15:47:23.815634: step 22400, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 73h:26m:07s remains)
2017-12-05 15:47:24.608883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2566838 -4.2487831 -4.2410731 -4.2364526 -4.2351031 -4.235249 -4.2358804 -4.235218 -4.2312465 -4.2277174 -4.2292147 -4.2346725 -4.2361975 -4.2341919 -4.2293305][-4.23521 -4.2268424 -4.2205205 -4.2194691 -4.2203794 -4.2205338 -4.2170644 -4.2093716 -4.1990871 -4.1900258 -4.1905675 -4.1990485 -4.2024555 -4.2005668 -4.1946292][-4.20956 -4.2050447 -4.2032232 -4.2033424 -4.20012 -4.1920028 -4.1759415 -4.1539078 -4.1348438 -4.1191664 -4.1207271 -4.1357908 -4.1441936 -4.1491795 -4.1490355][-4.1913576 -4.1895151 -4.1872406 -4.1791697 -4.1624823 -4.1358352 -4.1000671 -4.0659251 -4.0435658 -4.0294328 -4.0383172 -4.0629783 -4.0828042 -4.1000218 -4.1107178][-4.1747394 -4.1674271 -4.1545544 -4.1280627 -4.0882149 -4.0412383 -3.9927781 -3.9632878 -3.9552176 -3.9604547 -3.9833279 -4.0175991 -4.046936 -4.0776696 -4.1004734][-4.1469464 -4.1252174 -4.0885291 -4.0356607 -3.9762225 -3.9164886 -3.8721938 -3.8702147 -3.8950346 -3.9288535 -3.965652 -4.004477 -4.0439053 -4.0824857 -4.1113396][-4.0933728 -4.0431023 -3.9720936 -3.8909876 -3.8277521 -3.782892 -3.7753048 -3.8192213 -3.8777862 -3.9362206 -3.9868755 -4.0275216 -4.0701094 -4.110127 -4.1390791][-4.0005145 -3.925271 -3.8326604 -3.7424231 -3.7076237 -3.7175021 -3.7684689 -3.8460889 -3.9207869 -3.9839113 -4.0383792 -4.0751505 -4.1138806 -4.1523552 -4.1778164][-3.8856122 -3.809206 -3.7376997 -3.6934779 -3.7121098 -3.7691705 -3.8464699 -3.9276705 -3.9975126 -4.0522113 -4.0973396 -4.1306295 -4.1673608 -4.1995435 -4.2159147][-3.7836735 -3.7398186 -3.7281897 -3.7486181 -3.804179 -3.8710568 -3.9407265 -4.0129924 -4.0709496 -4.1125226 -4.1487041 -4.1784081 -4.213727 -4.2352605 -4.2414632][-3.7429478 -3.7533152 -3.793757 -3.8474839 -3.9087386 -3.9587646 -4.0042224 -4.0663619 -4.1177192 -4.1485553 -4.1762304 -4.2051806 -4.2391491 -4.2587328 -4.2616515][-3.7940598 -3.837559 -3.8907027 -3.94613 -3.9896777 -4.0203843 -4.0494456 -4.1028767 -4.1489286 -4.172132 -4.1923318 -4.2228603 -4.2575197 -4.2768331 -4.2817936][-3.8848834 -3.9357593 -3.9798968 -4.0137262 -4.0430532 -4.0700669 -4.0964689 -4.1403627 -4.1782427 -4.1939025 -4.2077236 -4.2366548 -4.2664318 -4.2876325 -4.2976804][-3.9768758 -4.0217986 -4.046844 -4.06333 -4.0887647 -4.1172414 -4.1396508 -4.1703129 -4.2026887 -4.2196383 -4.2317777 -4.2541265 -4.2797265 -4.3006063 -4.3138471][-4.0458541 -4.0764327 -4.0902286 -4.1035838 -4.1308146 -4.1574254 -4.1740928 -4.1977038 -4.226923 -4.2452688 -4.2594161 -4.2789059 -4.2998271 -4.3149147 -4.3230042]]...]
INFO - root - 2017-12-05 15:47:33.147843: step 22410, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 74h:48m:35s remains)
INFO - root - 2017-12-05 15:47:41.665388: step 22420, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 72h:22m:05s remains)
INFO - root - 2017-12-05 15:47:50.077454: step 22430, loss = 2.02, batch loss = 1.96 (9.5 examples/sec; 0.845 sec/batch; 72h:45m:12s remains)
INFO - root - 2017-12-05 15:47:58.634215: step 22440, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 74h:58m:41s remains)
INFO - root - 2017-12-05 15:48:07.033519: step 22450, loss = 2.04, batch loss = 1.99 (9.9 examples/sec; 0.806 sec/batch; 69h:25m:51s remains)
INFO - root - 2017-12-05 15:48:15.728222: step 22460, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 74h:21m:44s remains)
INFO - root - 2017-12-05 15:48:24.223308: step 22470, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 75h:00m:19s remains)
INFO - root - 2017-12-05 15:48:32.744702: step 22480, loss = 2.12, batch loss = 2.06 (9.4 examples/sec; 0.849 sec/batch; 73h:04m:31s remains)
INFO - root - 2017-12-05 15:48:41.317818: step 22490, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 75h:30m:10s remains)
INFO - root - 2017-12-05 15:48:49.898761: step 22500, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.814 sec/batch; 70h:04m:56s remains)
2017-12-05 15:48:50.653691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2973437 -4.2885647 -4.2836242 -4.26942 -4.2428212 -4.2265968 -4.2351737 -4.2567897 -4.2789207 -4.2997093 -4.308805 -4.3098316 -4.3098555 -4.310668 -4.3104711][-4.2882447 -4.2700343 -4.2543263 -4.2242112 -4.1750464 -4.14262 -4.1526594 -4.1927075 -4.2360811 -4.272552 -4.2933531 -4.2999983 -4.3052864 -4.3082361 -4.3088617][-4.2779617 -4.24894 -4.2209744 -4.1735225 -4.1009111 -4.0517068 -4.0615492 -4.1197948 -4.1842141 -4.2373695 -4.2724066 -4.2879968 -4.2997923 -4.3048239 -4.3050928][-4.2724452 -4.2347975 -4.1937079 -4.1262412 -4.0296373 -3.9662797 -3.974416 -4.047462 -4.1309423 -4.2001348 -4.2505283 -4.2762156 -4.2940016 -4.3006554 -4.2997489][-4.2750559 -4.2345881 -4.1867208 -4.1037111 -3.9901564 -3.9162002 -3.9237051 -4.0067129 -4.1033063 -4.180553 -4.2383509 -4.2707062 -4.2917051 -4.2989407 -4.2966962][-4.2819424 -4.2441945 -4.1981826 -4.1161776 -4.0021548 -3.926302 -3.9332345 -4.0178032 -4.1146441 -4.1891184 -4.243958 -4.2760057 -4.2954426 -4.3010893 -4.2968822][-4.2884936 -4.2560415 -4.216145 -4.1475048 -4.0520525 -3.9854662 -3.9939559 -4.0720277 -4.1591425 -4.2216406 -4.265975 -4.2917328 -4.3067508 -4.3087888 -4.3018069][-4.2944894 -4.2676239 -4.2340345 -4.1799297 -4.1079783 -4.0556307 -4.0624504 -4.1267581 -4.2007084 -4.2523384 -4.2881474 -4.3070841 -4.3171253 -4.3165932 -4.3086557][-4.2999978 -4.2787542 -4.2519431 -4.2071977 -4.1447687 -4.0971422 -4.0984206 -4.1477489 -4.2122788 -4.26148 -4.2948761 -4.3106475 -4.3181505 -4.317584 -4.3108611][-4.3041482 -4.2880244 -4.2686706 -4.2315512 -4.168786 -4.1161427 -4.1074834 -4.1418161 -4.1978917 -4.2496376 -4.2857461 -4.3025723 -4.3111854 -4.313242 -4.3092194][-4.3059649 -4.2917085 -4.2764549 -4.2445993 -4.1816173 -4.1211123 -4.1009197 -4.1224957 -4.1734104 -4.229671 -4.2697077 -4.2904944 -4.3023481 -4.3081288 -4.3073111][-4.3048196 -4.2894812 -4.2726359 -4.2424622 -4.1813097 -4.1179085 -4.0915895 -4.1080761 -4.1584711 -4.2180862 -4.2602777 -4.2833467 -4.2967062 -4.3050847 -4.3068027][-4.3026528 -4.2861838 -4.2657971 -4.236033 -4.181077 -4.123477 -4.1000347 -4.1173935 -4.1659927 -4.2226219 -4.2623348 -4.2844963 -4.2968731 -4.3054552 -4.3079982][-4.3020082 -4.28648 -4.2656183 -4.2387605 -4.1956944 -4.1508131 -4.1354847 -4.153677 -4.1947818 -4.2416053 -4.2735758 -4.2908163 -4.3009691 -4.3081379 -4.3097196][-4.3035207 -4.2912292 -4.2745371 -4.2545838 -4.2269545 -4.1975627 -4.1891994 -4.2053862 -4.2356267 -4.2684879 -4.2892814 -4.2990389 -4.3055534 -4.3103819 -4.3106394]]...]
INFO - root - 2017-12-05 15:48:59.220537: step 22510, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 73h:10m:05s remains)
INFO - root - 2017-12-05 15:49:07.732683: step 22520, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 73h:04m:19s remains)
INFO - root - 2017-12-05 15:49:16.174657: step 22530, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 73h:43m:00s remains)
INFO - root - 2017-12-05 15:49:24.824340: step 22540, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 74h:49m:28s remains)
INFO - root - 2017-12-05 15:49:33.038412: step 22550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 74h:18m:13s remains)
INFO - root - 2017-12-05 15:49:41.566931: step 22560, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 73h:27m:57s remains)
INFO - root - 2017-12-05 15:49:50.085216: step 22570, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.848 sec/batch; 72h:58m:13s remains)
INFO - root - 2017-12-05 15:49:58.648991: step 22580, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 72h:44m:10s remains)
INFO - root - 2017-12-05 15:50:07.199208: step 22590, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 71h:50m:30s remains)
INFO - root - 2017-12-05 15:50:15.813540: step 22600, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.888 sec/batch; 76h:24m:50s remains)
2017-12-05 15:50:16.569162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2958817 -4.2925096 -4.2896495 -4.2912521 -4.2957683 -4.3020554 -4.3102169 -4.3185654 -4.3260589 -4.3292232 -4.330162 -4.3324227 -4.3342237 -4.3341088 -4.3318038][-4.2806549 -4.2713466 -4.2598176 -4.2548318 -4.2575517 -4.2663188 -4.2767429 -4.2907286 -4.3038311 -4.311379 -4.316205 -4.320713 -4.3217115 -4.318995 -4.3148141][-4.2603083 -4.2399278 -4.2133417 -4.1977239 -4.1934776 -4.1984611 -4.2076459 -4.2237406 -4.2480478 -4.263814 -4.2704592 -4.2779865 -4.2831936 -4.2825074 -4.2782793][-4.238369 -4.2054243 -4.1628623 -4.1372066 -4.1220751 -4.1218147 -4.1303368 -4.1531157 -4.1891522 -4.2142754 -4.219101 -4.2284789 -4.2405572 -4.2448139 -4.2440605][-4.2147818 -4.1703315 -4.1112194 -4.0754814 -4.0469742 -4.0333509 -4.0356097 -4.0621595 -4.11834 -4.1577506 -4.1629 -4.1756387 -4.19489 -4.210196 -4.214396][-4.1939368 -4.13517 -4.0591106 -4.0082908 -3.9573123 -3.9221241 -3.9012647 -3.9193695 -4.0126991 -4.0790677 -4.0872321 -4.10765 -4.1361761 -4.1641517 -4.1809597][-4.1686425 -4.0824652 -3.9813964 -3.9081986 -3.8277202 -3.7514343 -3.666795 -3.6683669 -3.8252621 -3.927017 -3.9384246 -3.9753048 -4.0344477 -4.083405 -4.1206946][-4.1469417 -4.031467 -3.9020107 -3.8040812 -3.6973355 -3.5754685 -3.4091444 -3.3940964 -3.6298118 -3.7740598 -3.798737 -3.8596351 -3.9529097 -4.017766 -4.0696507][-4.1479225 -4.0304055 -3.9042947 -3.8137512 -3.7247 -3.6150994 -3.4594355 -3.4651554 -3.6745441 -3.8039718 -3.8326578 -3.898649 -3.9833832 -4.0334682 -4.0784497][-4.1700439 -4.0750103 -3.9778886 -3.9153881 -3.8576643 -3.7882731 -3.7030122 -3.7274811 -3.851249 -3.9252372 -3.9403102 -3.9895794 -4.0492258 -4.0871539 -4.1253319][-4.1961803 -4.1231785 -4.0485034 -3.9936762 -3.9435563 -3.8900733 -3.8503308 -3.8837438 -3.9549606 -3.9909015 -4.00101 -4.0381312 -4.0831423 -4.1194496 -4.1550889][-4.2246175 -4.169394 -4.1087856 -4.0559883 -4.0068326 -3.9605422 -3.947938 -3.9826672 -4.0273056 -4.0449414 -4.0545554 -4.0861988 -4.1216707 -4.1566529 -4.1890993][-4.2560105 -4.218215 -4.1747875 -4.1371779 -4.1019897 -4.06949 -4.0732284 -4.1037211 -4.1290288 -4.1360464 -4.1427917 -4.1691723 -4.1966286 -4.2247553 -4.2500405][-4.2903666 -4.27042 -4.2485704 -4.2277384 -4.2082758 -4.1924706 -4.2018485 -4.2188616 -4.2288642 -4.23116 -4.2331491 -4.2511182 -4.2701488 -4.2879581 -4.3020878][-4.3109007 -4.3023081 -4.293623 -4.2825985 -4.2713809 -4.2651596 -4.2730365 -4.2810483 -4.2832303 -4.2841959 -4.2846279 -4.2949429 -4.3062291 -4.3159227 -4.3217516]]...]
INFO - root - 2017-12-05 15:50:25.082848: step 22610, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 75h:05m:53s remains)
INFO - root - 2017-12-05 15:50:33.625714: step 22620, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 74h:03m:06s remains)
INFO - root - 2017-12-05 15:50:41.994008: step 22630, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 73h:04m:33s remains)
INFO - root - 2017-12-05 15:50:50.569782: step 22640, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 71h:26m:16s remains)
INFO - root - 2017-12-05 15:50:59.227795: step 22650, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 73h:50m:25s remains)
INFO - root - 2017-12-05 15:51:07.513809: step 22660, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 72h:36m:08s remains)
INFO - root - 2017-12-05 15:51:16.054701: step 22670, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 74h:59m:06s remains)
INFO - root - 2017-12-05 15:51:24.632450: step 22680, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 76h:15m:47s remains)
INFO - root - 2017-12-05 15:51:33.247373: step 22690, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 73h:18m:32s remains)
INFO - root - 2017-12-05 15:51:41.659136: step 22700, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 74h:19m:50s remains)
2017-12-05 15:51:42.413650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2474489 -4.216835 -4.1890292 -4.1703048 -4.1650944 -4.165977 -4.16476 -4.1614013 -4.1593595 -4.1607985 -4.1682577 -4.1801295 -4.1928291 -4.2028894 -4.2120018][-4.2292662 -4.1924238 -4.1582522 -4.1348553 -4.1295776 -4.1304193 -4.1277246 -4.1213112 -4.1204057 -4.1263027 -4.1357374 -4.1524949 -4.1681929 -4.1747308 -4.1718674][-4.2228107 -4.1822591 -4.1424651 -4.1146946 -4.1050768 -4.097837 -4.0912871 -4.0892048 -4.0975189 -4.113369 -4.1247492 -4.1410556 -4.1573029 -4.15605 -4.1379862][-4.222033 -4.1771069 -4.1308532 -4.0922709 -4.0659366 -4.0427752 -4.0320606 -4.0450625 -4.0740542 -4.1076941 -4.124476 -4.14164 -4.1558533 -4.1472383 -4.1175661][-4.2200556 -4.1703238 -4.1133561 -4.0568304 -4.0060153 -3.9589581 -3.9383216 -3.9694541 -4.0288949 -4.0884833 -4.1189685 -4.140511 -4.1514125 -4.139791 -4.102963][-4.2178774 -4.1617312 -4.0950165 -4.0233474 -3.9506714 -3.8791938 -3.8324873 -3.8519056 -3.9362965 -4.0310445 -4.0838618 -4.1143036 -4.1263914 -4.1170173 -4.0847216][-4.213748 -4.1490231 -4.0729437 -3.9909706 -3.9053063 -3.8131826 -3.7186737 -3.686368 -3.7821841 -3.9179537 -3.99871 -4.0393515 -4.0590291 -4.0593653 -4.0429292][-4.2049313 -4.1341887 -4.0477109 -3.9584024 -3.874706 -3.7806909 -3.6619596 -3.5833511 -3.6700797 -3.8263204 -3.9195676 -3.9628513 -3.9850006 -3.9912038 -3.9926872][-4.2023854 -4.1353273 -4.0477824 -3.9616742 -3.892503 -3.8219221 -3.7364681 -3.6808791 -3.7298546 -3.8387222 -3.9086776 -3.9395113 -3.953562 -3.9567132 -3.96601][-4.2117672 -4.1537447 -4.0742464 -4.0022068 -3.9560652 -3.9138632 -3.8687046 -3.8458242 -3.8601735 -3.9023676 -3.9403126 -3.9623961 -3.9699347 -3.9683278 -3.9731629][-4.2352571 -4.1893044 -4.1241865 -4.06832 -4.0395603 -4.01912 -4.0005383 -3.9898751 -3.9831672 -3.9851558 -4.0059628 -4.0239711 -4.0287294 -4.0238414 -4.0201535][-4.2705913 -4.2377572 -4.1909065 -4.1509743 -4.1328721 -4.12494 -4.1216378 -4.117681 -4.1098919 -4.1012564 -4.1059742 -4.1126256 -4.1148353 -4.1096711 -4.1036367][-4.3014803 -4.2798519 -4.2506504 -4.2266607 -4.2184315 -4.218574 -4.2240562 -4.2259789 -4.223165 -4.2144055 -4.2076955 -4.2043815 -4.2064252 -4.204998 -4.2023988][-4.3212285 -4.3095322 -4.29525 -4.2849011 -4.2833238 -4.2850084 -4.2914023 -4.2951441 -4.2926211 -4.2857118 -4.2777596 -4.2726097 -4.27538 -4.2787104 -4.2804732][-4.3315692 -4.3257618 -4.3189497 -4.3153372 -4.3152 -4.3155093 -4.31909 -4.3209224 -4.3179626 -4.3140545 -4.3098154 -4.3069115 -4.3098392 -4.3132491 -4.31565]]...]
INFO - root - 2017-12-05 15:51:51.018316: step 22710, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.002 sec/batch; 86h:15m:00s remains)
INFO - root - 2017-12-05 15:51:59.510486: step 22720, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 72h:58m:36s remains)
INFO - root - 2017-12-05 15:52:08.040244: step 22730, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 74h:01m:40s remains)
INFO - root - 2017-12-05 15:52:16.417400: step 22740, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.833 sec/batch; 71h:41m:35s remains)
INFO - root - 2017-12-05 15:52:25.103059: step 22750, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 76h:11m:22s remains)
INFO - root - 2017-12-05 15:52:33.637382: step 22760, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.822 sec/batch; 70h:42m:32s remains)
INFO - root - 2017-12-05 15:52:42.200722: step 22770, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 73h:00m:31s remains)
INFO - root - 2017-12-05 15:52:50.820555: step 22780, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 76h:22m:12s remains)
INFO - root - 2017-12-05 15:52:59.237623: step 22790, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 72h:29m:07s remains)
INFO - root - 2017-12-05 15:53:07.726123: step 22800, loss = 2.03, batch loss = 1.98 (9.6 examples/sec; 0.831 sec/batch; 71h:31m:38s remains)
2017-12-05 15:53:08.514849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2621365 -4.2501774 -4.2397814 -4.2502542 -4.2789922 -4.3004103 -4.3059964 -4.301064 -4.2896767 -4.2803583 -4.2723584 -4.2702446 -4.2634239 -4.2584176 -4.27408][-4.2578568 -4.2501383 -4.2439351 -4.2548132 -4.2826943 -4.3014631 -4.3042784 -4.2987556 -4.2895703 -4.2855721 -4.2814865 -4.2785211 -4.2689438 -4.262866 -4.2766228][-4.2604642 -4.2548728 -4.2494254 -4.2577786 -4.2777357 -4.2906394 -4.2915888 -4.2872891 -4.2815571 -4.2845459 -4.2879939 -4.2875504 -4.2772417 -4.2677374 -4.27418][-4.2471247 -4.2428136 -4.2387214 -4.2446451 -4.257937 -4.2647314 -4.2641811 -4.2615266 -4.2601767 -4.2716637 -4.284976 -4.2911587 -4.2837434 -4.2741265 -4.2729173][-4.2200408 -4.2217135 -4.2210889 -4.2257724 -4.2321987 -4.2319508 -4.2278714 -4.2264714 -4.2295804 -4.2479196 -4.2688146 -4.2820921 -4.2819986 -4.2737646 -4.268548][-4.1938019 -4.2023983 -4.2031178 -4.2045665 -4.2022181 -4.19449 -4.184195 -4.18546 -4.1943722 -4.2159648 -4.237649 -4.253643 -4.2610855 -4.2576966 -4.2528753][-4.1588616 -4.1747 -4.1745481 -4.1693678 -4.1578197 -4.14144 -4.1246934 -4.1290288 -4.1432281 -4.1661777 -4.1872153 -4.2059121 -4.219008 -4.2201619 -4.2173505][-4.1104112 -4.1278391 -4.1264453 -4.1198535 -4.1057558 -4.0857449 -4.066288 -4.0683608 -4.0825543 -4.1056004 -4.1300273 -4.1531014 -4.1691303 -4.1747618 -4.175127][-4.0858436 -4.0960531 -4.0917029 -4.0893559 -4.083209 -4.0680842 -4.0480509 -4.044384 -4.0531344 -4.070262 -4.0938272 -4.1215253 -4.1402669 -4.1499243 -4.1544933][-4.1192994 -4.1182046 -4.10683 -4.1015258 -4.0959663 -4.0836058 -4.0679593 -4.0663323 -4.0733056 -4.0837069 -4.1017089 -4.1284914 -4.1494794 -4.1610923 -4.1683455][-4.1643095 -4.164257 -4.1553097 -4.1488147 -4.1375527 -4.1207485 -4.1048784 -4.1042113 -4.1132121 -4.1234369 -4.1397805 -4.1649723 -4.1842494 -4.1951909 -4.2006922][-4.184608 -4.1961803 -4.1994514 -4.1988091 -4.188406 -4.170805 -4.152451 -4.1469574 -4.1539254 -4.1635776 -4.1777563 -4.1970897 -4.2125115 -4.2206454 -4.2235928][-4.1840968 -4.2046018 -4.2192507 -4.2295647 -4.2274089 -4.21526 -4.1979136 -4.1871815 -4.1868963 -4.1915536 -4.2010541 -4.2126427 -4.2224216 -4.2287621 -4.2300792][-4.1798477 -4.2033782 -4.2270994 -4.248199 -4.2564049 -4.250639 -4.2361956 -4.222425 -4.2145395 -4.2100711 -4.2114239 -4.2174473 -4.2249784 -4.2324347 -4.2351103][-4.1766095 -4.201756 -4.2305064 -4.2572722 -4.2714591 -4.2704949 -4.2599993 -4.24859 -4.2384748 -4.2284846 -4.2225966 -4.2238607 -4.2277594 -4.2356205 -4.2413826]]...]
INFO - root - 2017-12-05 15:53:16.877006: step 22810, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 74h:44m:43s remains)
INFO - root - 2017-12-05 15:53:25.421433: step 22820, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 76h:46m:35s remains)
INFO - root - 2017-12-05 15:53:33.825670: step 22830, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.830 sec/batch; 71h:25m:01s remains)
INFO - root - 2017-12-05 15:53:42.393942: step 22840, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 73h:14m:12s remains)
INFO - root - 2017-12-05 15:53:50.966001: step 22850, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 73h:54m:46s remains)
INFO - root - 2017-12-05 15:53:59.498638: step 22860, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.860 sec/batch; 73h:57m:12s remains)
INFO - root - 2017-12-05 15:54:07.957165: step 22870, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.844 sec/batch; 72h:36m:07s remains)
INFO - root - 2017-12-05 15:54:16.543771: step 22880, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 74h:07m:17s remains)
INFO - root - 2017-12-05 15:54:25.143235: step 22890, loss = 2.10, batch loss = 2.05 (9.3 examples/sec; 0.863 sec/batch; 74h:12m:10s remains)
INFO - root - 2017-12-05 15:54:33.618435: step 22900, loss = 2.01, batch loss = 1.95 (9.3 examples/sec; 0.856 sec/batch; 73h:37m:04s remains)
2017-12-05 15:54:34.373414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1788454 -4.1995664 -4.1651635 -4.1202297 -4.1125417 -4.1469507 -4.1625862 -4.1413269 -4.1151061 -4.1246352 -4.1730127 -4.2081437 -4.2073808 -4.1872807 -4.1831269][-4.1846976 -4.2050457 -4.1776581 -4.1383104 -4.1327348 -4.1662092 -4.1813593 -4.1654372 -4.1458879 -4.157022 -4.1898675 -4.2084665 -4.1996446 -4.1791286 -4.1733809][-4.1870418 -4.209641 -4.1902027 -4.1561556 -4.1460719 -4.1734376 -4.1902251 -4.186419 -4.1848812 -4.1978822 -4.2088561 -4.2131848 -4.2000628 -4.1834631 -4.1729884][-4.1657462 -4.1900334 -4.1812406 -4.1564369 -4.1406503 -4.1533995 -4.1605854 -4.1667891 -4.1895103 -4.2127018 -4.212338 -4.2108374 -4.203351 -4.1928506 -4.1757021][-4.1412578 -4.165936 -4.156374 -4.1289043 -4.09966 -4.0870309 -4.0724158 -4.0836859 -4.1347847 -4.1802588 -4.1888046 -4.19217 -4.195015 -4.1914911 -4.1687326][-4.134367 -4.1440158 -4.1169667 -4.0723581 -4.0112014 -3.9567204 -3.9017038 -3.903918 -3.9886558 -4.0737963 -4.1199017 -4.14913 -4.1695433 -4.1758204 -4.1618476][-4.1420937 -4.1302319 -4.0827374 -4.022037 -3.9287748 -3.8193879 -3.7000518 -3.6775222 -3.7860789 -3.9175286 -4.0196571 -4.0872645 -4.1313639 -4.1541033 -4.1610279][-4.1681943 -4.1441956 -4.0922074 -4.0357518 -3.9429078 -3.8151069 -3.668684 -3.6150074 -3.7051368 -3.8381958 -3.9622869 -4.0490332 -4.1075592 -4.14455 -4.17069][-4.1984282 -4.1719894 -4.1254582 -4.0794659 -4.0100489 -3.9109402 -3.797823 -3.7460392 -3.793045 -3.8829703 -3.9819038 -4.058383 -4.1186376 -4.1643577 -4.2032504][-4.223927 -4.2021294 -4.1670308 -4.1309943 -4.0829177 -4.0160284 -3.9431586 -3.9048977 -3.9268739 -3.9811277 -4.0496187 -4.1056767 -4.1568131 -4.201457 -4.2426295][-4.2363605 -4.219151 -4.1976542 -4.17466 -4.1481414 -4.1098194 -4.069356 -4.0465097 -4.0545039 -4.0845342 -4.1253014 -4.1615257 -4.2002573 -4.241025 -4.2775044][-4.2486615 -4.2340794 -4.2229414 -4.2119803 -4.2020469 -4.1862559 -4.1695485 -4.1611552 -4.1611195 -4.168498 -4.1851778 -4.2050858 -4.2313247 -4.2652941 -4.29425][-4.2752476 -4.2628975 -4.2547536 -4.2494707 -4.2467332 -4.2435894 -4.2409754 -4.2402682 -4.2366261 -4.2317343 -4.233037 -4.2402549 -4.25475 -4.2805223 -4.3005877][-4.300972 -4.289403 -4.280654 -4.2739782 -4.2713146 -4.2725587 -4.27679 -4.2806692 -4.2791324 -4.2733145 -4.2712445 -4.2732425 -4.2808156 -4.29646 -4.3077736][-4.3133612 -4.3049307 -4.2970471 -4.2893825 -4.2860541 -4.2881804 -4.2940774 -4.2998486 -4.3015165 -4.2996488 -4.2988439 -4.2993441 -4.3017116 -4.3089776 -4.3143287]]...]
INFO - root - 2017-12-05 15:54:42.937645: step 22910, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 72h:51m:20s remains)
INFO - root - 2017-12-05 15:54:51.583494: step 22920, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 76h:08m:23s remains)
INFO - root - 2017-12-05 15:55:00.075597: step 22930, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 73h:56m:14s remains)
INFO - root - 2017-12-05 15:55:08.614120: step 22940, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 73h:16m:23s remains)
INFO - root - 2017-12-05 15:55:17.140061: step 22950, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 73h:20m:09s remains)
INFO - root - 2017-12-05 15:55:25.709519: step 22960, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 72h:43m:58s remains)
INFO - root - 2017-12-05 15:55:34.238191: step 22970, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 72h:35m:57s remains)
INFO - root - 2017-12-05 15:55:42.737917: step 22980, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 75h:12m:57s remains)
INFO - root - 2017-12-05 15:55:51.305419: step 22990, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 74h:21m:37s remains)
INFO - root - 2017-12-05 15:55:59.801893: step 23000, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 73h:17m:26s remains)
2017-12-05 15:56:00.568584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.165473 -4.1936884 -4.1812172 -4.1403246 -4.0996709 -4.0740075 -4.0729709 -4.0759234 -4.0794024 -4.0960531 -4.10949 -4.1042614 -4.110713 -4.1379471 -4.1571746][-4.1609788 -4.1859746 -4.1786685 -4.1469073 -4.1053824 -4.0723548 -4.0697937 -4.0871639 -4.1112227 -4.1330585 -4.1462913 -4.13635 -4.133831 -4.1468225 -4.1503162][-4.1646123 -4.1856384 -4.1857662 -4.1645851 -4.1288056 -4.092454 -4.0859537 -4.1088982 -4.1385546 -4.15966 -4.1745005 -4.171946 -4.16345 -4.1604285 -4.1476221][-4.15937 -4.1847162 -4.1957068 -4.18757 -4.1609449 -4.1265659 -4.1099286 -4.1241803 -4.1470938 -4.1669016 -4.189302 -4.2011709 -4.2007704 -4.1853375 -4.160253][-4.146162 -4.1745195 -4.1916423 -4.1932755 -4.17606 -4.1454039 -4.1168003 -4.1180515 -4.1353636 -4.1612182 -4.1912408 -4.2123113 -4.2180147 -4.2012329 -4.1727738][-4.1352053 -4.1591644 -4.1771665 -4.1818352 -4.1708431 -4.1456423 -4.1161523 -4.1055441 -4.1187935 -4.147512 -4.1779027 -4.1980166 -4.2096591 -4.2034192 -4.1830287][-4.1199365 -4.1339841 -4.1516757 -4.1608152 -4.1540141 -4.1341267 -4.1119304 -4.099474 -4.1039848 -4.1233754 -4.1435914 -4.1603308 -4.1802707 -4.1900077 -4.1861815][-4.1071997 -4.1116748 -4.1267681 -4.1360984 -4.1307111 -4.1149149 -4.1022673 -4.101676 -4.1012392 -4.1073027 -4.1125979 -4.1204329 -4.1424384 -4.1629786 -4.172936][-4.1073356 -4.1071954 -4.1174965 -4.1236377 -4.1183929 -4.1038175 -4.1041093 -4.1238074 -4.13075 -4.131237 -4.12663 -4.1279035 -4.1435409 -4.159441 -4.1678557][-4.1094618 -4.1073351 -4.1178188 -4.1209059 -4.1149569 -4.0983977 -4.10878 -4.1497331 -4.1770191 -4.1842408 -4.182385 -4.1866655 -4.19635 -4.2053857 -4.2101545][-4.1047144 -4.0996017 -4.1191154 -4.1342926 -4.1338444 -4.1154776 -4.1239605 -4.1676159 -4.2053876 -4.2200356 -4.2239943 -4.2333388 -4.2458491 -4.2551503 -4.2621746][-4.0892925 -4.0801039 -4.1130357 -4.1491823 -4.162096 -4.1493821 -4.147553 -4.1727419 -4.2008233 -4.2172008 -4.2327061 -4.252727 -4.2708421 -4.2845902 -4.2928767][-4.0672164 -4.0490341 -4.0804672 -4.130116 -4.163063 -4.1653633 -4.1606531 -4.173306 -4.1911163 -4.2044854 -4.2234883 -4.2491441 -4.2711606 -4.28455 -4.2905521][-4.0368938 -4.0151482 -4.0430408 -4.1004872 -4.1461654 -4.157795 -4.1518927 -4.1613469 -4.1769366 -4.188899 -4.2054615 -4.2299042 -4.2501507 -4.2594442 -4.2625446][-3.9976504 -3.980335 -4.0127769 -4.0820403 -4.1371531 -4.1506643 -4.1393828 -4.1404896 -4.1523762 -4.1632686 -4.1777344 -4.1986146 -4.2164679 -4.2254105 -4.2245193]]...]
INFO - root - 2017-12-05 15:56:09.067619: step 23010, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 73h:22m:23s remains)
INFO - root - 2017-12-05 15:56:17.567503: step 23020, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 73h:50m:12s remains)
INFO - root - 2017-12-05 15:56:26.016463: step 23030, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 75h:32m:29s remains)
INFO - root - 2017-12-05 15:56:34.674634: step 23040, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.897 sec/batch; 77h:08m:25s remains)
INFO - root - 2017-12-05 15:56:43.151030: step 23050, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.837 sec/batch; 71h:55m:21s remains)
INFO - root - 2017-12-05 15:56:51.619412: step 23060, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 71h:41m:29s remains)
INFO - root - 2017-12-05 15:57:00.146620: step 23070, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 73h:07m:49s remains)
INFO - root - 2017-12-05 15:57:08.670443: step 23080, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 77h:07m:56s remains)
INFO - root - 2017-12-05 15:57:17.153523: step 23090, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 74h:12m:39s remains)
INFO - root - 2017-12-05 15:57:25.790575: step 23100, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 73h:16m:36s remains)
2017-12-05 15:57:26.565932: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1911726 -4.1902337 -4.1832652 -4.1724029 -4.1528096 -4.1435065 -4.1550112 -4.1675482 -4.1913147 -4.2269759 -4.2552876 -4.2745285 -4.2837157 -4.28451 -4.2816806][-4.1621103 -4.1662254 -4.1642566 -4.1549077 -4.1312695 -4.1097436 -4.1114678 -4.1203914 -4.1490116 -4.1990738 -4.2437444 -4.2722263 -4.2832112 -4.2841825 -4.280076][-4.139533 -4.1485252 -4.1523838 -4.1485076 -4.1228919 -4.0926142 -4.0820861 -4.0837297 -4.1137972 -4.1739931 -4.229228 -4.2589493 -4.2684345 -4.2719927 -4.2712288][-4.1144223 -4.1296215 -4.1414042 -4.1437883 -4.1226177 -4.0918112 -4.0748825 -4.0731826 -4.102046 -4.1578875 -4.20661 -4.2269859 -4.2299743 -4.2350574 -4.2399964][-4.0828629 -4.0960512 -4.10936 -4.1242881 -4.1183133 -4.0944972 -4.0714455 -4.0679207 -4.0949392 -4.1368237 -4.1703854 -4.1795969 -4.17819 -4.1870675 -4.1998997][-4.0575905 -4.0607448 -4.0706587 -4.0975885 -4.1059127 -4.0798578 -4.03658 -4.021245 -4.0545058 -4.100203 -4.1344471 -4.1441574 -4.145916 -4.15697 -4.1732655][-4.0568604 -4.052207 -4.056324 -4.0769563 -4.0734034 -4.0141077 -3.9214795 -3.8845828 -3.950757 -4.0317974 -4.091332 -4.1193705 -4.1297913 -4.1390748 -4.1540728][-4.0710459 -4.063282 -4.0603733 -4.0601587 -4.0299158 -3.928509 -3.7704959 -3.7141845 -3.8362026 -3.9679005 -4.0574803 -4.1045833 -4.119267 -4.1261306 -4.1391892][-4.0945816 -4.0825438 -4.0753064 -4.0639634 -4.018013 -3.905333 -3.7345986 -3.68326 -3.8362792 -3.9872143 -4.07832 -4.1217427 -4.1331229 -4.1352549 -4.1430655][-4.1004019 -4.086525 -4.0871406 -4.0864115 -4.0555868 -3.980947 -3.8641813 -3.8195171 -3.9283214 -4.0504265 -4.1217895 -4.1538653 -4.1617975 -4.1593366 -4.1559253][-4.0838461 -4.0709057 -4.0793805 -4.0945516 -4.0935054 -4.0670905 -4.0025768 -3.9577863 -4.0140929 -4.1036673 -4.15861 -4.1796594 -4.183517 -4.1776214 -4.1650028][-4.0743108 -4.0568876 -4.0597739 -4.0797329 -4.1015372 -4.1089463 -4.0791125 -4.0379014 -4.062633 -4.1289225 -4.1718488 -4.1846991 -4.189456 -4.186471 -4.1692905][-4.0868435 -4.0578761 -4.0458617 -4.058682 -4.0891137 -4.1092167 -4.0990305 -4.0639825 -4.0738564 -4.1253786 -4.1612339 -4.1714873 -4.1817484 -4.1792922 -4.1603565][-4.1171331 -4.083488 -4.0548477 -4.05124 -4.0718379 -4.0872917 -4.0855455 -4.05659 -4.0611811 -4.10573 -4.1352835 -4.1439552 -4.1592975 -4.1591225 -4.1414933][-4.1413379 -4.1125884 -4.0786223 -4.0646563 -4.0748734 -4.0783739 -4.078475 -4.0553794 -4.0540695 -4.0875506 -4.1123672 -4.1237717 -4.1443782 -4.1496787 -4.1349506]]...]
INFO - root - 2017-12-05 15:57:35.115625: step 23110, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 75h:17m:20s remains)
INFO - root - 2017-12-05 15:57:43.702454: step 23120, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 75h:05m:24s remains)
INFO - root - 2017-12-05 15:57:52.063973: step 23130, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 76h:22m:22s remains)
INFO - root - 2017-12-05 15:58:00.557386: step 23140, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 73h:00m:22s remains)
INFO - root - 2017-12-05 15:58:09.232713: step 23150, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 76h:20m:24s remains)
INFO - root - 2017-12-05 15:58:17.755301: step 23160, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 73h:39m:27s remains)
INFO - root - 2017-12-05 15:58:26.405837: step 23170, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 74h:33m:44s remains)
INFO - root - 2017-12-05 15:58:34.861582: step 23180, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 73h:33m:26s remains)
INFO - root - 2017-12-05 15:58:43.367525: step 23190, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 76h:03m:01s remains)
INFO - root - 2017-12-05 15:58:51.981597: step 23200, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 73h:17m:47s remains)
2017-12-05 15:58:52.765869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3417096 -4.3368616 -4.3330646 -4.331748 -4.3317003 -4.3337584 -4.3356991 -4.3358903 -4.3355188 -4.3343725 -4.3317327 -4.3299379 -4.3294592 -4.329958 -4.3326807][-4.3414555 -4.3387303 -4.3361988 -4.3351297 -4.333847 -4.3337765 -4.3342724 -4.3349218 -4.3363161 -4.33637 -4.3346758 -4.3322554 -4.3302155 -4.3303008 -4.3336535][-4.340519 -4.3395257 -4.3384118 -4.3364596 -4.3325996 -4.3293419 -4.3285689 -4.33028 -4.3342457 -4.3370113 -4.3382883 -4.3369789 -4.3336205 -4.3318639 -4.3339872][-4.3390875 -4.3391056 -4.3373871 -4.3325706 -4.3249044 -4.3175688 -4.31305 -4.3136983 -4.3214755 -4.3289638 -4.3363028 -4.3391895 -4.3366365 -4.3330264 -4.3324122][-4.3335528 -4.3331819 -4.3282018 -4.3165264 -4.3004708 -4.2830405 -4.2691083 -4.2665896 -4.280488 -4.29892 -4.3191886 -4.3327775 -4.3360147 -4.3332286 -4.3298573][-4.32173 -4.3198328 -4.3102064 -4.2882071 -4.2542672 -4.2131348 -4.1798873 -4.1720924 -4.1973534 -4.2382016 -4.2827168 -4.3152971 -4.3300128 -4.330667 -4.3267312][-4.3079605 -4.3053021 -4.2898145 -4.2530818 -4.1907835 -4.1099081 -4.0428843 -4.027472 -4.0727658 -4.14771 -4.2264953 -4.2829452 -4.3142452 -4.3234696 -4.3233604][-4.297627 -4.2946172 -4.27218 -4.2192492 -4.1261306 -4.0006704 -3.8912492 -3.867218 -3.9408691 -4.0566034 -4.1698656 -4.250021 -4.2981033 -4.3170609 -4.3217788][-4.2979445 -4.2951474 -4.2685852 -4.207221 -4.0992503 -3.9522541 -3.8194196 -3.7932096 -3.8845243 -4.01986 -4.1451964 -4.2344322 -4.2905779 -4.3164129 -4.3241458][-4.3071551 -4.30676 -4.2836704 -4.2310057 -4.1406136 -4.01752 -3.9086268 -3.8879681 -3.9587591 -4.0697861 -4.1723552 -4.2471943 -4.296165 -4.3194122 -4.3267636][-4.3175879 -4.3216853 -4.3078675 -4.2756143 -4.2183232 -4.1399436 -4.0721216 -4.0590739 -4.0995684 -4.1677537 -4.2316413 -4.2798328 -4.3120656 -4.326324 -4.329524][-4.3238249 -4.3330946 -4.3287544 -4.3157234 -4.289279 -4.2510877 -4.218142 -4.2100291 -4.2272344 -4.2594218 -4.2896061 -4.3131862 -4.3281159 -4.3322849 -4.3310122][-4.3240194 -4.3364997 -4.3391366 -4.3367467 -4.3273258 -4.3112121 -4.2969513 -4.2906489 -4.295054 -4.30829 -4.3200812 -4.3290911 -4.3342853 -4.3340697 -4.3314943][-4.319972 -4.3336573 -4.3413458 -4.343895 -4.3387508 -4.3273754 -4.3151164 -4.3059521 -4.30325 -4.3110814 -4.3196812 -4.3267813 -4.3330841 -4.3344884 -4.3341136][-4.3107624 -4.3241525 -4.335103 -4.3402438 -4.3353829 -4.3211164 -4.3035989 -4.2883334 -4.2810359 -4.2893672 -4.3040733 -4.3186936 -4.3317952 -4.3376284 -4.3393788]]...]
INFO - root - 2017-12-05 15:59:01.394607: step 23210, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 72h:34m:54s remains)
INFO - root - 2017-12-05 15:59:09.927377: step 23220, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 73h:10m:12s remains)
INFO - root - 2017-12-05 15:59:18.439591: step 23230, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 71h:24m:08s remains)
INFO - root - 2017-12-05 15:59:26.901587: step 23240, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 71h:49m:00s remains)
INFO - root - 2017-12-05 15:59:35.368057: step 23250, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.865 sec/batch; 74h:19m:24s remains)
INFO - root - 2017-12-05 15:59:43.831499: step 23260, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 72h:32m:28s remains)
INFO - root - 2017-12-05 15:59:52.323008: step 23270, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 72h:44m:38s remains)
INFO - root - 2017-12-05 16:00:00.886829: step 23280, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 72h:21m:31s remains)
INFO - root - 2017-12-05 16:00:09.444859: step 23290, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 74h:01m:26s remains)
INFO - root - 2017-12-05 16:00:17.895418: step 23300, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 71h:52m:39s remains)
2017-12-05 16:00:18.606312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.121037 -4.0982323 -4.0768886 -4.0659938 -4.0676179 -4.0707 -4.0758882 -4.088697 -4.088953 -4.0803618 -4.0815244 -4.0814414 -4.0801463 -4.08556 -4.1044784][-4.1306872 -4.1112857 -4.0820589 -4.0636606 -4.0556536 -4.0513196 -4.0522795 -4.0694509 -4.0832648 -4.0883894 -4.0885119 -4.085217 -4.07897 -4.0796175 -4.0969858][-4.1245308 -4.1156182 -4.0886526 -4.0679889 -4.0553322 -4.0477748 -4.0426316 -4.0599508 -4.08638 -4.0994844 -4.100769 -4.0991945 -4.0905981 -4.0896864 -4.105864][-4.1144958 -4.1173487 -4.0968823 -4.0728436 -4.0522776 -4.0367022 -4.0305009 -4.0503716 -4.0843949 -4.1009645 -4.1011887 -4.10065 -4.0957222 -4.1023226 -4.1234331][-4.1042194 -4.1114073 -4.0951896 -4.0644689 -4.0316272 -4.0028419 -3.9972517 -4.0275283 -4.0670185 -4.0823288 -4.0762634 -4.0749598 -4.0718193 -4.0856576 -4.1127076][-4.1172175 -4.1215034 -4.10711 -4.0750856 -4.037508 -3.9981327 -3.9885581 -4.0156212 -4.0550232 -4.0727339 -4.070045 -4.0688157 -4.05921 -4.0610743 -4.0819407][-4.1263676 -4.1306734 -4.1262503 -4.1014266 -4.0708089 -4.038259 -4.0299382 -4.0521097 -4.0867057 -4.104013 -4.10685 -4.1064992 -4.0856404 -4.0660019 -4.06557][-4.123075 -4.1253314 -4.122848 -4.0999451 -4.0813837 -4.0726428 -4.0767074 -4.0957913 -4.1264606 -4.1423793 -4.1430254 -4.1431561 -4.1243529 -4.0966969 -4.0780253][-4.1282706 -4.1275077 -4.1193671 -4.0960541 -4.0833559 -4.0891 -4.1034985 -4.1202235 -4.1454988 -4.1662388 -4.1690388 -4.1651878 -4.150445 -4.124825 -4.1013508][-4.14215 -4.1434631 -4.1310167 -4.1098137 -4.1008587 -4.1106124 -4.12359 -4.1331253 -4.1498156 -4.1693048 -4.1775818 -4.1747437 -4.1596003 -4.1315222 -4.1077223][-4.1475487 -4.1564803 -4.1510315 -4.1369462 -4.1317024 -4.1364431 -4.1417589 -4.1470709 -4.1545935 -4.168292 -4.1765933 -4.1754937 -4.1597967 -4.1311502 -4.105782][-4.1350236 -4.1485853 -4.150908 -4.1485658 -4.1492043 -4.1520286 -4.1561069 -4.1621604 -4.1680684 -4.1795721 -4.1863527 -4.1849847 -4.172122 -4.1505566 -4.1256523][-4.1226487 -4.1342411 -4.1368155 -4.136713 -4.1386027 -4.143991 -4.1515808 -4.1596155 -4.165791 -4.1761837 -4.184176 -4.1895771 -4.1881337 -4.1780715 -4.158843][-4.1423664 -4.1479759 -4.1429787 -4.1348968 -4.1292682 -4.1294765 -4.1328363 -4.1356297 -4.1379228 -4.1466489 -4.1562419 -4.1639695 -4.1680088 -4.16937 -4.1610594][-4.1731238 -4.1790495 -4.1730843 -4.1611233 -4.1483245 -4.1410952 -4.1362166 -4.1314344 -4.1240134 -4.1202168 -4.1202631 -4.1216784 -4.1246519 -4.129097 -4.1260743]]...]
INFO - root - 2017-12-05 16:00:27.299544: step 23310, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 74h:02m:17s remains)
INFO - root - 2017-12-05 16:00:35.820135: step 23320, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 73h:06m:08s remains)
INFO - root - 2017-12-05 16:00:44.355540: step 23330, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 72h:00m:57s remains)
INFO - root - 2017-12-05 16:00:52.856198: step 23340, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 72h:38m:28s remains)
INFO - root - 2017-12-05 16:01:01.405252: step 23350, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 73h:41m:36s remains)
INFO - root - 2017-12-05 16:01:09.914241: step 23360, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 72h:55m:39s remains)
INFO - root - 2017-12-05 16:01:18.397933: step 23370, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 73h:11m:51s remains)
INFO - root - 2017-12-05 16:01:26.858624: step 23380, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.819 sec/batch; 70h:16m:58s remains)
INFO - root - 2017-12-05 16:01:35.474710: step 23390, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 73h:48m:56s remains)
INFO - root - 2017-12-05 16:01:44.236872: step 23400, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 74h:21m:58s remains)
2017-12-05 16:01:45.004178: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24864 -4.2384958 -4.2244716 -4.2036672 -4.1665039 -4.1169863 -4.07596 -4.0701809 -4.0984941 -4.1460967 -4.1981874 -4.2518764 -4.2922387 -4.3127623 -4.3160796][-4.2264333 -4.2181582 -4.2065563 -4.1803784 -4.1325049 -4.0669827 -4.013947 -4.0091228 -4.0430713 -4.0984378 -4.1581945 -4.2255206 -4.2833033 -4.3152695 -4.3255129][-4.2032638 -4.1971807 -4.1865263 -4.1567535 -4.0999808 -4.017827 -3.9527106 -3.9445083 -3.9846878 -4.0504708 -4.1202006 -4.1971507 -4.2684288 -4.3115058 -4.3298979][-4.1631 -4.1653981 -4.1630611 -4.1379256 -4.0763354 -3.9828148 -3.9057698 -3.8844838 -3.924036 -3.9990368 -4.0841022 -4.1715789 -4.2504668 -4.3015175 -4.3291874][-4.1303806 -4.1410246 -4.143374 -4.1191707 -4.0571289 -3.9625504 -3.8746397 -3.8410754 -3.875699 -3.9589977 -4.0556664 -4.1486411 -4.2352495 -4.2931967 -4.3262157][-4.1162834 -4.1288176 -4.1285233 -4.1043296 -4.0496264 -3.9666317 -3.8870387 -3.8558595 -3.8789818 -3.9556968 -4.0503454 -4.1404743 -4.22367 -4.2811651 -4.3181109][-4.1166096 -4.1243224 -4.1215725 -4.1032157 -4.0692034 -4.0142012 -3.955029 -3.9120789 -3.9012473 -3.953141 -4.0389619 -4.1256275 -4.2060432 -4.2667055 -4.3076215][-4.1380367 -4.1405544 -4.1339884 -4.1161847 -4.0958233 -4.0667892 -4.0186429 -3.959173 -3.9219635 -3.9477139 -4.0218153 -4.10597 -4.1826921 -4.248765 -4.2982993][-4.1779523 -4.1827674 -4.1742229 -4.1490965 -4.1305118 -4.1137714 -4.0715346 -4.0109463 -3.974005 -3.99081 -4.0510325 -4.1256285 -4.1895924 -4.2490029 -4.2966747][-4.2285933 -4.2351003 -4.2252388 -4.2003975 -4.1827407 -4.1684842 -4.1327453 -4.0790381 -4.0485406 -4.0596089 -4.1066408 -4.1696367 -4.2196817 -4.2667975 -4.30404][-4.2769303 -4.284493 -4.274652 -4.2546949 -4.239913 -4.2262831 -4.1942644 -4.1464543 -4.1226177 -4.1335731 -4.1707859 -4.2185197 -4.25427 -4.2884488 -4.3152008][-4.3109612 -4.3172708 -4.3111081 -4.2997608 -4.2885222 -4.2762861 -4.2532878 -4.217875 -4.2021203 -4.2152481 -4.2434998 -4.275353 -4.2967076 -4.3164816 -4.330945][-4.3338022 -4.3367033 -4.3341565 -4.3287725 -4.3226895 -4.3143759 -4.3029356 -4.2848282 -4.275991 -4.2827396 -4.2980137 -4.3155103 -4.3275681 -4.3374357 -4.3428049][-4.3479528 -4.3473091 -4.3458533 -4.3431611 -4.3399167 -4.3363585 -4.3331304 -4.326489 -4.3222313 -4.3233585 -4.327395 -4.3336058 -4.3393588 -4.3430128 -4.3427305][-4.347373 -4.344924 -4.3435111 -4.3418455 -4.3401604 -4.3384662 -4.3371773 -4.3351574 -4.3340936 -4.3341584 -4.3349323 -4.3358936 -4.3365612 -4.336472 -4.336164]]...]
INFO - root - 2017-12-05 16:01:53.400379: step 23410, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 72h:32m:33s remains)
INFO - root - 2017-12-05 16:02:01.993224: step 23420, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 71h:16m:43s remains)
INFO - root - 2017-12-05 16:02:10.302852: step 23430, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 74h:04m:01s remains)
INFO - root - 2017-12-05 16:02:18.824392: step 23440, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 72h:15m:57s remains)
INFO - root - 2017-12-05 16:02:27.178091: step 23450, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 70h:09m:19s remains)
INFO - root - 2017-12-05 16:02:35.537136: step 23460, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 73h:27m:56s remains)
INFO - root - 2017-12-05 16:02:43.877374: step 23470, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 70h:55m:16s remains)
INFO - root - 2017-12-05 16:02:52.454798: step 23480, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 71h:48m:23s remains)
INFO - root - 2017-12-05 16:03:00.952502: step 23490, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 73h:41m:16s remains)
INFO - root - 2017-12-05 16:03:09.535129: step 23500, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 73h:02m:12s remains)
2017-12-05 16:03:10.325442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1735048 -4.180757 -4.1799879 -4.1709094 -4.1407676 -4.0979552 -4.0690327 -4.0921707 -4.1613927 -4.2306056 -4.2898045 -4.3127003 -4.32061 -4.3267694 -4.3345785][-4.1862698 -4.1889973 -4.1845078 -4.1753798 -4.150372 -4.1052885 -4.0652475 -4.0818906 -4.1551819 -4.2291956 -4.2914257 -4.3160009 -4.3209968 -4.3258309 -4.3342113][-4.1945062 -4.1976204 -4.1876035 -4.1762795 -4.1490536 -4.0975709 -4.0555811 -4.0721264 -4.1481514 -4.2280684 -4.2929535 -4.318234 -4.3217163 -4.3255992 -4.33315][-4.1975517 -4.2051182 -4.1952267 -4.1792841 -4.1392245 -4.07532 -4.029017 -4.0470409 -4.1280031 -4.2170591 -4.2884531 -4.3183231 -4.3234415 -4.3269262 -4.3330569][-4.1828442 -4.1992579 -4.1973195 -4.17991 -4.1311512 -4.0536466 -3.996624 -4.0145416 -4.1023097 -4.201787 -4.2803698 -4.3159027 -4.3249884 -4.32936 -4.3345075][-4.169713 -4.1907043 -4.1975207 -4.1837206 -4.1364174 -4.05574 -3.9900148 -3.9989467 -4.0837493 -4.1892619 -4.2725277 -4.3151875 -4.32753 -4.3331842 -4.3365321][-4.1577969 -4.1769776 -4.1901073 -4.1804533 -4.1352 -4.0509415 -3.9777927 -3.9759321 -4.0571585 -4.1700807 -4.2626925 -4.31371 -4.3305426 -4.33638 -4.3379455][-4.1498284 -4.1611657 -4.1729431 -4.1643167 -4.1198473 -4.0344119 -3.9583442 -3.9497077 -4.0277052 -4.149116 -4.2520967 -4.310504 -4.3317947 -4.3372726 -4.338346][-4.1401453 -4.14514 -4.1590834 -4.1547241 -4.117075 -4.0453024 -3.974277 -3.9583251 -4.0232182 -4.1436687 -4.25017 -4.3133788 -4.3344121 -4.3381214 -4.33858][-4.1198893 -4.1205554 -4.1408496 -4.1521254 -4.1341853 -4.0882478 -4.036592 -4.0226836 -4.0722485 -4.1694832 -4.2624073 -4.3217692 -4.3396778 -4.3402023 -4.3389134][-4.0907469 -4.0909176 -4.1230574 -4.1530252 -4.1553793 -4.13249 -4.1014109 -4.0982261 -4.1401048 -4.21239 -4.2842231 -4.3327994 -4.3459673 -4.3420095 -4.3393354][-4.0735226 -4.0768547 -4.1131439 -4.1589222 -4.1767097 -4.1665673 -4.148592 -4.1520009 -4.1874652 -4.2452836 -4.3004932 -4.3380566 -4.3470516 -4.3427515 -4.3403678][-4.0841546 -4.0915737 -4.1314716 -4.185349 -4.2082896 -4.1996589 -4.1852236 -4.1877241 -4.2144275 -4.2618814 -4.3058934 -4.3332791 -4.3403368 -4.339201 -4.3391285][-4.122797 -4.1342249 -4.1769266 -4.2285986 -4.2463536 -4.2298903 -4.2118921 -4.21172 -4.2316756 -4.2707992 -4.3060045 -4.3268585 -4.3332257 -4.3334641 -4.335535][-4.17455 -4.1885152 -4.2272267 -4.2653208 -4.2681589 -4.2374134 -4.2130322 -4.2133827 -4.2312574 -4.2672939 -4.3007669 -4.3223963 -4.3299866 -4.3312163 -4.3334541]]...]
INFO - root - 2017-12-05 16:03:18.899897: step 23510, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 74h:28m:29s remains)
INFO - root - 2017-12-05 16:03:27.472621: step 23520, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 75h:39m:54s remains)
INFO - root - 2017-12-05 16:03:35.916056: step 23530, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 72h:52m:19s remains)
INFO - root - 2017-12-05 16:03:44.397823: step 23540, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 75h:49m:23s remains)
INFO - root - 2017-12-05 16:03:52.793082: step 23550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 73h:35m:32s remains)
INFO - root - 2017-12-05 16:04:01.224586: step 23560, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 72h:34m:15s remains)
INFO - root - 2017-12-05 16:04:09.685643: step 23570, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 74h:52m:30s remains)
INFO - root - 2017-12-05 16:04:18.081711: step 23580, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 71h:26m:54s remains)
INFO - root - 2017-12-05 16:04:26.620418: step 23590, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 0.816 sec/batch; 70h:02m:37s remains)
INFO - root - 2017-12-05 16:04:35.102508: step 23600, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 72h:52m:15s remains)
2017-12-05 16:04:35.800756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2218742 -4.2285767 -4.2335167 -4.2370024 -4.2381687 -4.2391367 -4.2365174 -4.2329769 -4.22926 -4.2231293 -4.2118011 -4.2048087 -4.2064581 -4.2092113 -4.2091117][-4.2335491 -4.2401543 -4.2434444 -4.24652 -4.2485285 -4.2489738 -4.2421522 -4.2357397 -4.2321019 -4.22639 -4.2184319 -4.2173166 -4.2229481 -4.2240052 -4.218225][-4.2412052 -4.2441568 -4.24354 -4.2430577 -4.2435775 -4.2415237 -4.2311926 -4.2238264 -4.2240124 -4.2251797 -4.2265325 -4.2333989 -4.24249 -4.2401338 -4.22867][-4.24315 -4.2393794 -4.2304473 -4.2207894 -4.2130136 -4.2040882 -4.1896138 -4.1839061 -4.1940813 -4.20787 -4.2219992 -4.2377968 -4.2511854 -4.24772 -4.23571][-4.2349806 -4.2252293 -4.20626 -4.1852846 -4.1665664 -4.146987 -4.1259089 -4.1231189 -4.1462269 -4.1745224 -4.2018056 -4.2280383 -4.2470164 -4.2463555 -4.2397265][-4.2177725 -4.2035971 -4.1784711 -4.1489697 -4.1179314 -4.0818038 -4.0458865 -4.045115 -4.0831537 -4.1273165 -4.1687574 -4.2056646 -4.2317104 -4.2373133 -4.2388144][-4.1975489 -4.1830277 -4.1562386 -4.12183 -4.078969 -4.023756 -3.9682398 -3.9671226 -4.0218873 -4.0813069 -4.132761 -4.1785293 -4.2112412 -4.2245436 -4.231894][-4.1971669 -4.1854348 -4.1618514 -4.1302848 -4.0882411 -4.0314403 -3.9723349 -3.9674585 -4.0201435 -4.0785346 -4.1261864 -4.168252 -4.2019558 -4.2192435 -4.2284484][-4.2115932 -4.2051849 -4.1910272 -4.1723738 -4.1454973 -4.1062851 -4.0619049 -4.0505538 -4.0795937 -4.1182222 -4.15147 -4.1841421 -4.213541 -4.2290297 -4.2343607][-4.2300496 -4.2303514 -4.2249947 -4.2158947 -4.2014785 -4.1787434 -4.1491275 -4.1339126 -4.1434655 -4.1648765 -4.1858592 -4.2097816 -4.2326007 -4.2448516 -4.2459168][-4.2447324 -4.2488122 -4.2486181 -4.2434549 -4.2358932 -4.2239056 -4.2047796 -4.1908045 -4.1930089 -4.2040753 -4.21501 -4.2303638 -4.245307 -4.2546854 -4.2544527][-4.2496085 -4.2554369 -4.257576 -4.2553844 -4.2514944 -4.2434998 -4.2293673 -4.2176533 -4.2177215 -4.2244539 -4.2310171 -4.2408314 -4.2497907 -4.2567739 -4.2569313][-4.2436748 -4.2495837 -4.2511683 -4.2507997 -4.2496734 -4.2440076 -4.2339454 -4.2264161 -4.2263961 -4.2324295 -4.2379775 -4.244956 -4.2495289 -4.2533655 -4.2540646][-4.2276282 -4.2310481 -4.2306561 -4.2305684 -4.232038 -4.2315154 -4.2279253 -4.2259674 -4.227674 -4.2343411 -4.2389526 -4.2436376 -4.2446146 -4.2452822 -4.2469311][-4.212379 -4.2129078 -4.2101722 -4.2091856 -4.2119603 -4.21558 -4.2175655 -4.2198906 -4.2235823 -4.2310781 -4.2364035 -4.2401071 -4.2399645 -4.2399907 -4.2433295]]...]
INFO - root - 2017-12-05 16:04:44.364882: step 23610, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 75h:10m:40s remains)
INFO - root - 2017-12-05 16:04:52.772893: step 23620, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 72h:52m:34s remains)
INFO - root - 2017-12-05 16:05:01.261180: step 23630, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 72h:24m:53s remains)
INFO - root - 2017-12-05 16:05:09.807146: step 23640, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 74h:22m:40s remains)
INFO - root - 2017-12-05 16:05:18.347913: step 23650, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 71h:41m:10s remains)
INFO - root - 2017-12-05 16:05:26.938324: step 23660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 73h:37m:28s remains)
INFO - root - 2017-12-05 16:05:35.370180: step 23670, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 0.820 sec/batch; 70h:19m:03s remains)
INFO - root - 2017-12-05 16:05:43.781203: step 23680, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.812 sec/batch; 69h:37m:06s remains)
INFO - root - 2017-12-05 16:05:52.145837: step 23690, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 71h:43m:35s remains)
INFO - root - 2017-12-05 16:06:00.764788: step 23700, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 75h:52m:50s remains)
2017-12-05 16:06:01.473459: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0427818 -4.0456948 -4.0391746 -4.0270834 -4.0281086 -4.0586257 -4.1029968 -4.1424575 -4.1713357 -4.1806307 -4.172967 -4.1509156 -4.1189651 -4.0898719 -4.0804629][-4.0534906 -4.0569391 -4.0482159 -4.0366154 -4.040689 -4.0703845 -4.1075416 -4.1389136 -4.1628189 -4.1718755 -4.1683607 -4.1540146 -4.134356 -4.1177735 -4.1167011][-4.0790577 -4.0895576 -4.0814114 -4.0685921 -4.0677013 -4.0862684 -4.1106434 -4.1339793 -4.1521978 -4.1605377 -4.1627655 -4.1588855 -4.1517272 -4.14759 -4.1538363][-4.1130095 -4.1270747 -4.1173024 -4.1007686 -4.0923104 -4.0992575 -4.10967 -4.1201887 -4.1285834 -4.134985 -4.144567 -4.151618 -4.1520805 -4.1532774 -4.1652255][-4.1467533 -4.1572733 -4.1387396 -4.1117496 -4.0902925 -4.0852036 -4.0824785 -4.0794539 -4.0805969 -4.0907688 -4.1093268 -4.1266971 -4.133081 -4.1386557 -4.1565309][-4.1641483 -4.1650953 -4.1333895 -4.0901275 -4.0545058 -4.039865 -4.0287366 -4.01886 -4.0202212 -4.0360031 -4.0644703 -4.0961728 -4.111166 -4.1201959 -4.143199][-4.1569986 -4.14942 -4.107964 -4.052793 -4.0043473 -3.9756885 -3.9524636 -3.936883 -3.9361269 -3.9522314 -3.9910576 -4.0415421 -4.0695882 -4.0849295 -4.1119404][-4.1415033 -4.1292682 -4.0844455 -4.0245371 -3.9674575 -3.9233537 -3.8858161 -3.8602495 -3.8458619 -3.85034 -3.9022808 -3.9715283 -4.0150971 -4.0403771 -4.0748687][-4.1320934 -4.1168056 -4.075655 -4.0206642 -3.9664307 -3.9208686 -3.8838594 -3.860827 -3.8491445 -3.8530087 -3.9013424 -3.9615016 -4.0004778 -4.025178 -4.0622039][-4.1301889 -4.1138139 -4.0792556 -4.0371685 -3.996016 -3.9612947 -3.934181 -3.9216993 -3.9191833 -3.9235513 -3.9507213 -3.9842663 -4.0096154 -4.0334959 -4.0737419][-4.1300654 -4.1188731 -4.0956788 -4.0692534 -4.0409422 -4.0136838 -3.9910102 -3.9828773 -3.9800513 -3.9811144 -3.9963033 -4.0140514 -4.02692 -4.0496054 -4.0892076][-4.1301513 -4.1320505 -4.1247654 -4.1137457 -4.0932846 -4.0680141 -4.0439706 -4.0314064 -4.0208735 -4.019784 -4.035852 -4.0451856 -4.0477738 -4.0658503 -4.099978][-4.1364 -4.1499186 -4.15609 -4.1581573 -4.142231 -4.1167564 -4.0875998 -4.0630722 -4.0397758 -4.0336647 -4.04843 -4.0572181 -4.0621037 -4.0842943 -4.1156034][-4.1487918 -4.1695986 -4.1854534 -4.19878 -4.1876421 -4.1586576 -4.1205411 -4.085422 -4.0536938 -4.0422454 -4.0538859 -4.0658913 -4.0797482 -4.10889 -4.1367455][-4.1630459 -4.18686 -4.207767 -4.225965 -4.2167816 -4.1869464 -4.1479726 -4.1132655 -4.0822349 -4.0667958 -4.0732393 -4.0883207 -4.109087 -4.139667 -4.159884]]...]
INFO - root - 2017-12-05 16:06:09.903471: step 23710, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 72h:02m:04s remains)
INFO - root - 2017-12-05 16:06:18.369442: step 23720, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 71h:53m:59s remains)
INFO - root - 2017-12-05 16:06:26.746425: step 23730, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 77h:11m:58s remains)
INFO - root - 2017-12-05 16:06:35.320078: step 23740, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 73h:09m:27s remains)
INFO - root - 2017-12-05 16:06:43.768554: step 23750, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 71h:38m:02s remains)
INFO - root - 2017-12-05 16:06:52.374147: step 23760, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 73h:42m:36s remains)
INFO - root - 2017-12-05 16:07:00.924129: step 23770, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 74h:07m:23s remains)
INFO - root - 2017-12-05 16:07:09.439814: step 23780, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.882 sec/batch; 75h:40m:12s remains)
INFO - root - 2017-12-05 16:07:17.920402: step 23790, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.771 sec/batch; 66h:04m:43s remains)
INFO - root - 2017-12-05 16:07:26.464075: step 23800, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 74h:31m:28s remains)
2017-12-05 16:07:27.285319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2070661 -4.2000575 -4.1935148 -4.1850066 -4.1776347 -4.1762781 -4.1803503 -4.1886153 -4.2007632 -4.2100854 -4.2154222 -4.2109575 -4.1973805 -4.1889133 -4.1884971][-4.2147927 -4.2031794 -4.1860819 -4.1645088 -4.1450162 -4.1371751 -4.1438537 -4.1626859 -4.18932 -4.2136407 -4.2332392 -4.2400622 -4.2351542 -4.2306647 -4.2299614][-4.2322531 -4.2186728 -4.192112 -4.1588354 -4.1262741 -4.1091623 -4.1152763 -4.1417365 -4.1828632 -4.22123 -4.2547827 -4.2735271 -4.2782168 -4.278049 -4.2767754][-4.235086 -4.2272658 -4.1971755 -4.1564851 -4.1154919 -4.090332 -4.0919595 -4.1198292 -4.1676545 -4.2157297 -4.2594914 -4.2897148 -4.303021 -4.3053918 -4.3000875][-4.2239103 -4.2249365 -4.1966696 -4.1517286 -4.1056428 -4.0726557 -4.0681705 -4.0938191 -4.1426015 -4.1947956 -4.2412605 -4.27403 -4.2906628 -4.2943296 -4.2845712][-4.1901221 -4.2042336 -4.1876774 -4.1455665 -4.0951872 -4.0530748 -4.040453 -4.0579576 -4.1010423 -4.151319 -4.196197 -4.2274156 -4.2413659 -4.2443752 -4.2352734][-4.1164331 -4.1505446 -4.1594052 -4.1339822 -4.0892959 -4.0436115 -4.0241537 -4.0310149 -4.0637054 -4.1059604 -4.1468015 -4.1776996 -4.1923981 -4.1989942 -4.2017007][-4.0431986 -4.0929451 -4.1245761 -4.1184335 -4.0862951 -4.0495014 -4.0314069 -4.0333705 -4.0578942 -4.0910993 -4.1275892 -4.1541448 -4.1666842 -4.17799 -4.195519][-4.0284891 -4.0799179 -4.1170588 -4.1230741 -4.1046076 -4.0827155 -4.0699158 -4.0711689 -4.0914116 -4.11689 -4.14731 -4.166883 -4.1734486 -4.183075 -4.2063828][-4.0891237 -4.1310058 -4.1595435 -4.1677914 -4.1579514 -4.144824 -4.1349978 -4.133615 -4.1482606 -4.1641684 -4.1844645 -4.1969533 -4.1975341 -4.2017074 -4.2201695][-4.1772461 -4.2042336 -4.219974 -4.2245631 -4.2174149 -4.2075057 -4.1971836 -4.191812 -4.2007718 -4.208931 -4.2173967 -4.2224188 -4.2196264 -4.2177949 -4.2253022][-4.2494974 -4.2640142 -4.2695079 -4.2700596 -4.2658944 -4.2602119 -4.2523656 -4.24683 -4.2515383 -4.2557683 -4.257092 -4.2559028 -4.2497854 -4.2436051 -4.2394948][-4.294302 -4.3045578 -4.3075676 -4.3082538 -4.3079429 -4.3057365 -4.3013535 -4.29737 -4.2994833 -4.300622 -4.2970271 -4.2894759 -4.2759929 -4.2626715 -4.2478013][-4.2898216 -4.3056417 -4.3164849 -4.3236737 -4.3287191 -4.3309097 -4.3296552 -4.3268352 -4.3245931 -4.321547 -4.3150811 -4.3049831 -4.2891922 -4.2732015 -4.254488][-4.2363009 -4.2639832 -4.2907166 -4.3111839 -4.323915 -4.3299246 -4.3301544 -4.3287792 -4.3245897 -4.3196435 -4.3127069 -4.3050542 -4.2930727 -4.27848 -4.2613463]]...]
INFO - root - 2017-12-05 16:07:35.800188: step 23810, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 72h:17m:44s remains)
INFO - root - 2017-12-05 16:07:44.341624: step 23820, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 71h:52m:17s remains)
INFO - root - 2017-12-05 16:07:52.776570: step 23830, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 74h:56m:12s remains)
INFO - root - 2017-12-05 16:08:01.196458: step 23840, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 73h:54m:45s remains)
INFO - root - 2017-12-05 16:08:09.717862: step 23850, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 75h:41m:36s remains)
INFO - root - 2017-12-05 16:08:18.325809: step 23860, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 73h:20m:23s remains)
INFO - root - 2017-12-05 16:08:26.829891: step 23870, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 73h:29m:38s remains)
INFO - root - 2017-12-05 16:08:35.460093: step 23880, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 74h:37m:14s remains)
INFO - root - 2017-12-05 16:08:43.992379: step 23890, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 70h:01m:41s remains)
INFO - root - 2017-12-05 16:08:52.504740: step 23900, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 71h:22m:05s remains)
2017-12-05 16:08:53.246607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.219059 -4.2228222 -4.2225156 -4.2282176 -4.2348962 -4.2398477 -4.23318 -4.2208591 -4.2246389 -4.2423959 -4.2585883 -4.2693343 -4.2756405 -4.2818823 -4.2897573][-4.175396 -4.1695809 -4.1625829 -4.1696897 -4.184176 -4.195796 -4.181808 -4.1501989 -4.1521711 -4.1865072 -4.2210402 -4.2456007 -4.2651935 -4.2808595 -4.291563][-4.1506114 -4.1336513 -4.1187873 -4.1232166 -4.1386185 -4.1470342 -4.1128168 -4.0480161 -4.0439415 -4.0966988 -4.1526484 -4.1946259 -4.2287669 -4.2579432 -4.2758093][-4.1494641 -4.1259918 -4.1039667 -4.1036563 -4.1179514 -4.1236153 -4.072444 -3.9748831 -3.9627743 -4.0319347 -4.1004667 -4.1509733 -4.1921663 -4.2324219 -4.2581897][-4.1333933 -4.101244 -4.0736923 -4.0716376 -4.0901432 -4.0983243 -4.0384054 -3.9185441 -3.8991697 -3.9911671 -4.0717235 -4.1239805 -4.1642656 -4.2121515 -4.2465596][-4.110405 -4.0690832 -4.0360756 -4.0298176 -4.0423775 -4.0391603 -3.9534683 -3.7949433 -3.7741489 -3.9154372 -4.0260768 -4.091558 -4.1407948 -4.1965647 -4.236825][-4.1068821 -4.0628076 -4.03081 -4.0181623 -4.0118179 -3.9752109 -3.8424304 -3.6338356 -3.620496 -3.8196931 -3.9625285 -4.0441785 -4.1080427 -4.1745892 -4.2212129][-4.1360478 -4.0997429 -4.0809293 -4.0722995 -4.0514388 -3.9943252 -3.8557699 -3.66744 -3.66638 -3.8526592 -3.9789639 -4.0489979 -4.1077266 -4.1700535 -4.2166657][-4.1561527 -4.1247387 -4.1170111 -4.119113 -4.1045527 -4.060545 -3.9581385 -3.8287895 -3.830147 -3.9641547 -4.05388 -4.1041875 -4.145268 -4.1919541 -4.2292852][-4.153255 -4.1250978 -4.1226697 -4.1320491 -4.1265311 -4.098042 -4.0253 -3.9305482 -3.9219589 -4.0214157 -4.0985727 -4.143415 -4.1780047 -4.2159114 -4.2454104][-4.1536636 -4.1284704 -4.1224775 -4.1308126 -4.1302419 -4.1126289 -4.0573826 -3.9764159 -3.9528766 -4.0285673 -4.1032786 -4.1502786 -4.1866031 -4.2229447 -4.2503505][-4.1775193 -4.1616783 -4.1562958 -4.1614981 -4.1602912 -4.1491861 -4.1091585 -4.04216 -4.011703 -4.0611038 -4.1196265 -4.1620064 -4.1964426 -4.2309842 -4.2599316][-4.2240772 -4.2168746 -4.2143168 -4.2139549 -4.2104511 -4.20216 -4.1735492 -4.1230659 -4.0974851 -4.1256137 -4.1634579 -4.1938076 -4.2197547 -4.2472982 -4.2736535][-4.2756438 -4.2740006 -4.27095 -4.2647448 -4.2586069 -4.2502255 -4.2276492 -4.1896095 -4.1687856 -4.1819663 -4.2031093 -4.2223711 -4.2408638 -4.2622309 -4.2831855][-4.3090024 -4.3090229 -4.3058739 -4.2983456 -4.29141 -4.2839532 -4.264328 -4.2306261 -4.2072082 -4.2105846 -4.222506 -4.2342052 -4.251348 -4.271719 -4.2897215]]...]
INFO - root - 2017-12-05 16:09:01.824903: step 23910, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 73h:41m:36s remains)
INFO - root - 2017-12-05 16:09:10.385202: step 23920, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 72h:40m:14s remains)
INFO - root - 2017-12-05 16:09:18.765148: step 23930, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.854 sec/batch; 73h:09m:43s remains)
INFO - root - 2017-12-05 16:09:27.315848: step 23940, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 74h:33m:48s remains)
INFO - root - 2017-12-05 16:09:35.911784: step 23950, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 73h:50m:52s remains)
INFO - root - 2017-12-05 16:09:44.605946: step 23960, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 70h:17m:53s remains)
INFO - root - 2017-12-05 16:09:53.258932: step 23970, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 73h:56m:37s remains)
INFO - root - 2017-12-05 16:10:01.827865: step 23980, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 72h:11m:06s remains)
INFO - root - 2017-12-05 16:10:10.408870: step 23990, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 73h:26m:20s remains)
INFO - root - 2017-12-05 16:10:18.973681: step 24000, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 73h:55m:49s remains)
2017-12-05 16:10:19.727168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2092462 -4.1952653 -4.1861658 -4.184145 -4.185019 -4.186235 -4.1918788 -4.2007632 -4.210124 -4.216032 -4.2324519 -4.2488465 -4.2622862 -4.2859755 -4.315474][-4.1948681 -4.1818781 -4.1744232 -4.1715846 -4.1753039 -4.1767068 -4.1769657 -4.1899467 -4.2096462 -4.2202711 -4.2373261 -4.2573104 -4.2718334 -4.2963719 -4.3252382][-4.1842637 -4.1761374 -4.1690297 -4.1674867 -4.1753674 -4.1781783 -4.1760769 -4.1889606 -4.2128677 -4.2271748 -4.24235 -4.2606206 -4.2745795 -4.3006086 -4.3271465][-4.186172 -4.1799359 -4.1715984 -4.1703629 -4.1756568 -4.1742663 -4.1680231 -4.1771197 -4.2050619 -4.2291226 -4.2517929 -4.2704592 -4.2832065 -4.3056188 -4.3256073][-4.1921334 -4.1845083 -4.1729269 -4.1668963 -4.160203 -4.1465626 -4.1275091 -4.1282811 -4.1604204 -4.1984043 -4.2358904 -4.2654214 -4.2865391 -4.3072982 -4.3211579][-4.18871 -4.18002 -4.1671114 -4.155973 -4.1376219 -4.1089234 -4.0748353 -4.0586762 -4.0855556 -4.1337104 -4.1891255 -4.2364483 -4.2722516 -4.2991614 -4.3139973][-4.1904655 -4.1832757 -4.1721358 -4.1611938 -4.1373577 -4.1013675 -4.0615959 -4.0302815 -4.0342059 -4.0721054 -4.1338181 -4.1915689 -4.2362261 -4.2736611 -4.2977681][-4.2055736 -4.202333 -4.1967826 -4.1889482 -4.1708713 -4.1420946 -4.1139011 -4.0890179 -4.078156 -4.0918465 -4.1318092 -4.1752992 -4.2135768 -4.2508788 -4.2815814][-4.2207022 -4.2223697 -4.2236533 -4.2205482 -4.2126141 -4.1969662 -4.1816149 -4.1663294 -4.1581903 -4.1620841 -4.1830621 -4.2110157 -4.235538 -4.2628303 -4.2881532][-4.2312512 -4.2371345 -4.2436085 -4.2457004 -4.2449369 -4.2389464 -4.2306395 -4.2227812 -4.2208271 -4.2232561 -4.2371387 -4.2597518 -4.2758117 -4.2927637 -4.3072853][-4.2393808 -4.2460861 -4.2524381 -4.2555408 -4.2573624 -4.257123 -4.2535148 -4.2508626 -4.2521067 -4.2578926 -4.27353 -4.29505 -4.3072386 -4.3185892 -4.3262916][-4.2457552 -4.2492042 -4.2531033 -4.2573762 -4.2610993 -4.2620397 -4.2585864 -4.2543769 -4.251699 -4.2586169 -4.2793884 -4.2996874 -4.3096657 -4.3209291 -4.3258619][-4.2472558 -4.2485962 -4.2508283 -4.2557292 -4.2600708 -4.2602205 -4.2535386 -4.2417946 -4.2319765 -4.2348018 -4.2535062 -4.2699351 -4.2828455 -4.2987432 -4.30701][-4.2477684 -4.2460904 -4.244966 -4.2462449 -4.2466941 -4.2426205 -4.2323723 -4.2133307 -4.1986117 -4.1988316 -4.2155991 -4.2324576 -4.249639 -4.2689223 -4.2833757][-4.2415738 -4.2414627 -4.2400756 -4.2392159 -4.2375336 -4.2330146 -4.2227111 -4.2021408 -4.18748 -4.1899533 -4.2078247 -4.2252674 -4.2409544 -4.2595186 -4.2760983]]...]
INFO - root - 2017-12-05 16:10:28.301765: step 24010, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 72h:24m:31s remains)
INFO - root - 2017-12-05 16:10:36.738926: step 24020, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.856 sec/batch; 73h:22m:19s remains)
INFO - root - 2017-12-05 16:10:45.104728: step 24030, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 73h:58m:40s remains)
INFO - root - 2017-12-05 16:10:53.705028: step 24040, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 75h:10m:55s remains)
INFO - root - 2017-12-05 16:11:02.316233: step 24050, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 71h:16m:00s remains)
INFO - root - 2017-12-05 16:11:10.900772: step 24060, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 72h:42m:18s remains)
INFO - root - 2017-12-05 16:11:19.460030: step 24070, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 72h:32m:17s remains)
INFO - root - 2017-12-05 16:11:28.084643: step 24080, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 71h:20m:13s remains)
INFO - root - 2017-12-05 16:11:36.516016: step 24090, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 72h:27m:05s remains)
INFO - root - 2017-12-05 16:11:45.070022: step 24100, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 74h:19m:34s remains)
2017-12-05 16:11:45.813162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28232 -4.28045 -4.2827363 -4.2853661 -4.2924933 -4.2996173 -4.2935786 -4.2755752 -4.2632251 -4.2640958 -4.2740459 -4.2895288 -4.3104992 -4.3268738 -4.3284159][-4.2955151 -4.2978249 -4.3008537 -4.3035178 -4.3091388 -4.3095303 -4.2889857 -4.2537336 -4.2270555 -4.2219729 -4.2333131 -4.2576566 -4.2913175 -4.3174644 -4.3251858][-4.3008127 -4.3096609 -4.3150706 -4.3180623 -4.3199911 -4.3115573 -4.2752438 -4.2220912 -4.1825762 -4.1742678 -4.1885657 -4.2217369 -4.2685056 -4.3051491 -4.3179555][-4.2976909 -4.3136768 -4.3210316 -4.3231182 -4.321578 -4.3046002 -4.253921 -4.1848783 -4.1355705 -4.1241593 -4.1400371 -4.1822162 -4.2447104 -4.2945557 -4.3130536][-4.2808466 -4.3067284 -4.3183961 -4.3200245 -4.3160043 -4.2928176 -4.2337441 -4.1572809 -4.104754 -4.0908322 -4.1041093 -4.1529965 -4.22583 -4.2836523 -4.3067169][-4.2591329 -4.2915416 -4.3062572 -4.3073373 -4.3015409 -4.27635 -4.2194109 -4.1484241 -4.1017008 -4.0862741 -4.0951285 -4.1420884 -4.2100787 -4.264677 -4.2902756][-4.2461948 -4.2778845 -4.2894692 -4.2883053 -4.2831106 -4.26371 -4.2193313 -4.1631312 -4.1270065 -4.1124039 -4.1172495 -4.1519394 -4.2000909 -4.2394452 -4.2608433][-4.2535095 -4.2803459 -4.2869329 -4.2828231 -4.2782221 -4.2656274 -4.2333913 -4.1903419 -4.1629539 -4.1508203 -4.1530833 -4.1733274 -4.1991858 -4.2209845 -4.2357907][-4.2759686 -4.2960205 -4.2969213 -4.2878108 -4.2815366 -4.2724228 -4.2482891 -4.2159166 -4.19436 -4.1859016 -4.1885042 -4.1988544 -4.2094779 -4.2194967 -4.2300181][-4.2944441 -4.3096514 -4.3077435 -4.29678 -4.2895713 -4.2812452 -4.2635469 -4.2405887 -4.2249389 -4.2214994 -4.2272735 -4.2352757 -4.2410212 -4.2466869 -4.2535][-4.305222 -4.3178892 -4.3152151 -4.3050537 -4.2984161 -4.291501 -4.2794533 -4.2640324 -4.2532191 -4.2527051 -4.258471 -4.265563 -4.2726827 -4.2797437 -4.2856793][-4.31408 -4.3252087 -4.32321 -4.3153572 -4.3108935 -4.3066521 -4.2996583 -4.2898889 -4.2817435 -4.2787385 -4.2801056 -4.284586 -4.2932873 -4.3029819 -4.3101459][-4.3220992 -4.3304386 -4.329073 -4.3245568 -4.3230848 -4.3215866 -4.3181553 -4.3124561 -4.3057747 -4.2996588 -4.2966762 -4.2988834 -4.3079596 -4.3184857 -4.3264084][-4.3250465 -4.33092 -4.3307753 -4.3291636 -4.3298612 -4.3301167 -4.32843 -4.3236437 -4.3162732 -4.3082056 -4.3034821 -4.304719 -4.3118739 -4.3216777 -4.330565][-4.321866 -4.3262691 -4.3274097 -4.3281188 -4.329782 -4.3307595 -4.328949 -4.3230867 -4.3148708 -4.3077445 -4.3043537 -4.3048029 -4.3091826 -4.3175259 -4.3271422]]...]
INFO - root - 2017-12-05 16:11:54.372586: step 24110, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 73h:47m:33s remains)
INFO - root - 2017-12-05 16:12:03.040375: step 24120, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 73h:19m:45s remains)
INFO - root - 2017-12-05 16:12:11.464726: step 24130, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 72h:00m:38s remains)
INFO - root - 2017-12-05 16:12:20.031831: step 24140, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.822 sec/batch; 70h:24m:24s remains)
INFO - root - 2017-12-05 16:12:28.478946: step 24150, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.756 sec/batch; 64h:47m:03s remains)
INFO - root - 2017-12-05 16:12:36.931114: step 24160, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 71h:47m:24s remains)
INFO - root - 2017-12-05 16:12:45.448547: step 24170, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 74h:35m:00s remains)
INFO - root - 2017-12-05 16:12:53.944707: step 24180, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 74h:44m:47s remains)
INFO - root - 2017-12-05 16:13:02.567507: step 24190, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 76h:21m:05s remains)
INFO - root - 2017-12-05 16:13:11.227882: step 24200, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 75h:25m:41s remains)
2017-12-05 16:13:11.992535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1882315 -4.1711159 -4.158596 -4.1555605 -4.1745129 -4.2024021 -4.2256665 -4.2274747 -4.2321286 -4.2406335 -4.2379732 -4.2223473 -4.2090816 -4.1911817 -4.17026][-4.1524491 -4.1270432 -4.1042476 -4.0972457 -4.1235914 -4.1583304 -4.1821146 -4.1885571 -4.2011747 -4.2184944 -4.2133493 -4.1925378 -4.1762867 -4.1472712 -4.1174521][-4.1285529 -4.0894132 -4.0549655 -4.0446019 -4.070786 -4.1049128 -4.1312971 -4.1491132 -4.1740532 -4.1970186 -4.195147 -4.1759524 -4.1588564 -4.1216717 -4.0838108][-4.1210871 -4.0677733 -4.0225148 -4.0080495 -4.0228491 -4.0408044 -4.0634713 -4.0931964 -4.1345587 -4.1658506 -4.1736097 -4.1605597 -4.1453075 -4.1104188 -4.0723324][-4.1323833 -4.0751467 -4.0249281 -4.0039363 -4.0036416 -4.0003071 -4.0011272 -4.025209 -4.08033 -4.1321044 -4.1568656 -4.1586657 -4.15337 -4.1268773 -4.095439][-4.1483803 -4.0904303 -4.0393829 -4.0131354 -4.0008636 -3.9740415 -3.9322526 -3.9266434 -3.9967105 -4.0876222 -4.1418476 -4.1623859 -4.1721849 -4.1588573 -4.1363835][-4.1538625 -4.1008191 -4.0551076 -4.0283957 -4.0050611 -3.9498019 -3.8419778 -3.7799139 -3.8734074 -4.0115924 -4.0928764 -4.1295953 -4.1602168 -4.1700792 -4.1594849][-4.1477284 -4.1105709 -4.0799189 -4.0560331 -4.0349116 -3.9649057 -3.810174 -3.7047045 -3.8023295 -3.9459531 -4.0270243 -4.0714154 -4.1283507 -4.1650143 -4.1715546][-4.1134071 -4.085041 -4.0735841 -4.0668755 -4.0635896 -4.0150504 -3.8909018 -3.801177 -3.8594375 -3.9433908 -3.9916701 -4.0332975 -4.1013179 -4.1506753 -4.1676445][-4.098875 -4.0725965 -4.0726924 -4.0817661 -4.0953126 -4.0839591 -4.0189638 -3.9593029 -3.9725742 -3.9971318 -4.0142336 -4.0456686 -4.1050539 -4.1466589 -4.1627693][-4.1217284 -4.097055 -4.1001954 -4.1124644 -4.129941 -4.1424384 -4.1252346 -4.0934629 -4.0827813 -4.0776443 -4.0797634 -4.1026144 -4.1465244 -4.1723442 -4.1855216][-4.173274 -4.1536784 -4.1574306 -4.168725 -4.18331 -4.2062039 -4.2172036 -4.208446 -4.1917686 -4.1754751 -4.1723533 -4.1854038 -4.2096786 -4.2251859 -4.2371917][-4.2399721 -4.22817 -4.2312136 -4.2381134 -4.2483821 -4.2698107 -4.2888551 -4.2922888 -4.2829676 -4.268971 -4.2632074 -4.2647505 -4.2722416 -4.2776089 -4.2866278][-4.2866225 -4.2821841 -4.2827649 -4.2865362 -4.2919936 -4.3041997 -4.3197508 -4.3269811 -4.3270907 -4.3207359 -4.3151808 -4.3104477 -4.3105197 -4.3111172 -4.3161154][-4.3110938 -4.3098211 -4.3081555 -4.3084044 -4.3104296 -4.317203 -4.3274045 -4.3309236 -4.3317537 -4.3311071 -4.3284855 -4.3259492 -4.3251987 -4.3262324 -4.3290725]]...]
INFO - root - 2017-12-05 16:13:20.476857: step 24210, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 70h:16m:53s remains)
INFO - root - 2017-12-05 16:13:29.061775: step 24220, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 75h:28m:38s remains)
INFO - root - 2017-12-05 16:13:37.524252: step 24230, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 74h:39m:55s remains)
INFO - root - 2017-12-05 16:13:46.074892: step 24240, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 72h:05m:03s remains)
INFO - root - 2017-12-05 16:13:54.519698: step 24250, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 72h:01m:44s remains)
INFO - root - 2017-12-05 16:14:02.998409: step 24260, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 72h:03m:18s remains)
INFO - root - 2017-12-05 16:14:11.647599: step 24270, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 75h:44m:39s remains)
INFO - root - 2017-12-05 16:14:20.203330: step 24280, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 74h:11m:50s remains)
INFO - root - 2017-12-05 16:14:28.735850: step 24290, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 73h:04m:01s remains)
INFO - root - 2017-12-05 16:14:37.309494: step 24300, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 72h:41m:44s remains)
2017-12-05 16:14:38.012545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2465224 -4.235518 -4.2177429 -4.188838 -4.1431141 -4.0936742 -4.062429 -4.0593143 -4.0850792 -4.1281705 -4.1659365 -4.1813254 -4.1777763 -4.1648531 -4.1473708][-4.2506371 -4.2411819 -4.2267108 -4.2017231 -4.15557 -4.111834 -4.0869827 -4.087357 -4.1156344 -4.1559963 -4.1893468 -4.2002473 -4.1954565 -4.1816025 -4.1647224][-4.2557616 -4.2483068 -4.2366447 -4.2181644 -4.1786604 -4.1436868 -4.1253281 -4.1280642 -4.1534629 -4.1822762 -4.2041249 -4.2073874 -4.2016025 -4.1924133 -4.1843133][-4.2506943 -4.2402759 -4.2254262 -4.2102318 -4.1787691 -4.1534147 -4.1441407 -4.1506515 -4.1730175 -4.1900644 -4.1993132 -4.1953073 -4.1867113 -4.1894827 -4.1996117][-4.2322865 -4.211525 -4.1903858 -4.173974 -4.1459942 -4.1275029 -4.1297369 -4.1435108 -4.1677208 -4.1825318 -4.178925 -4.1632962 -4.154119 -4.1677337 -4.1923661][-4.2161169 -4.1855221 -4.1543717 -4.1311831 -4.1014094 -4.0857782 -4.0957665 -4.1164589 -4.1443715 -4.1557388 -4.1411028 -4.120008 -4.1100974 -4.1194992 -4.1443281][-4.2098446 -4.1734428 -4.1389356 -4.1116776 -4.0775337 -4.058208 -4.0617614 -4.0779181 -4.1040182 -4.1128016 -4.0931549 -4.0768452 -4.0693388 -4.0665846 -4.0706773][-4.210722 -4.1744881 -4.1400647 -4.1097307 -4.0681338 -4.03485 -4.0191607 -4.0184522 -4.0385833 -4.05637 -4.0531278 -4.0559931 -4.0626531 -4.0575504 -4.0427356][-4.2145405 -4.1747422 -4.1335363 -4.0972433 -4.0515223 -4.0105672 -3.980571 -3.968622 -3.9835508 -4.0120511 -4.032959 -4.0587897 -4.0797238 -4.0818744 -4.0694141][-4.2210784 -4.1811814 -4.141088 -4.1089993 -4.0724468 -4.0368395 -4.0052404 -3.9885852 -3.9954495 -4.024406 -4.0552006 -4.0908051 -4.116971 -4.1263661 -4.1220846][-4.2320747 -4.2012787 -4.1732054 -4.1556549 -4.1330743 -4.1058569 -4.0814309 -4.0682106 -4.0749326 -4.099987 -4.1261406 -4.1536322 -4.1754832 -4.1892638 -4.1914415][-4.2382197 -4.2180238 -4.2036538 -4.2022991 -4.193327 -4.1753473 -4.1580877 -4.1505046 -4.1575594 -4.1787457 -4.1968355 -4.2105322 -4.2231989 -4.235424 -4.2396855][-4.2408094 -4.2265229 -4.221303 -4.2281175 -4.2299805 -4.2239656 -4.2153096 -4.2105241 -4.212955 -4.22767 -4.24063 -4.2487426 -4.25645 -4.2651434 -4.2706618][-4.2480817 -4.2362261 -4.2326779 -4.2399607 -4.2468395 -4.2477589 -4.2484894 -4.2481003 -4.2515664 -4.2638907 -4.2757335 -4.28261 -4.2868819 -4.2885876 -4.289804][-4.2588677 -4.2491574 -4.2457066 -4.2511015 -4.2566648 -4.2588143 -4.2617321 -4.264287 -4.2702341 -4.2819476 -4.2935977 -4.3000822 -4.3020406 -4.3004131 -4.2986841]]...]
INFO - root - 2017-12-05 16:14:46.534295: step 24310, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 74h:09m:14s remains)
INFO - root - 2017-12-05 16:14:55.167516: step 24320, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.847 sec/batch; 72h:28m:06s remains)
INFO - root - 2017-12-05 16:15:03.528808: step 24330, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 72h:35m:15s remains)
INFO - root - 2017-12-05 16:15:12.076270: step 24340, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 71h:03m:35s remains)
INFO - root - 2017-12-05 16:15:20.677140: step 24350, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 73h:59m:39s remains)
INFO - root - 2017-12-05 16:15:29.234298: step 24360, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 73h:10m:59s remains)
INFO - root - 2017-12-05 16:15:37.671295: step 24370, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 76h:42m:58s remains)
INFO - root - 2017-12-05 16:15:46.295583: step 24380, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 74h:28m:35s remains)
INFO - root - 2017-12-05 16:15:54.798268: step 24390, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 72h:58m:36s remains)
INFO - root - 2017-12-05 16:16:03.386382: step 24400, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 72h:28m:54s remains)
2017-12-05 16:16:04.245803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2991586 -4.2962513 -4.2945528 -4.2949352 -4.2979517 -4.30041 -4.2986221 -4.2963729 -4.2949243 -4.2942505 -4.2937369 -4.294076 -4.2944961 -4.2935305 -4.2900791][-4.3018413 -4.3025918 -4.3032703 -4.3046393 -4.3090057 -4.31336 -4.3134089 -4.3140635 -4.3144312 -4.3149195 -4.3144541 -4.3134675 -4.3119226 -4.3094454 -4.3040524][-4.2967076 -4.3012748 -4.3037457 -4.3045893 -4.30701 -4.3102655 -4.3102155 -4.3125443 -4.3132429 -4.3145041 -4.3161178 -4.3169394 -4.3167505 -4.3160572 -4.3119211][-4.2774682 -4.2830558 -4.2843733 -4.2818346 -4.2776923 -4.2734728 -4.2676406 -4.2726326 -4.2784624 -4.2861481 -4.2962317 -4.3068881 -4.3128357 -4.3160429 -4.3149433][-4.2406096 -4.2438111 -4.2413836 -4.2323828 -4.2173824 -4.201056 -4.1870413 -4.1939569 -4.2087808 -4.2279305 -4.2520742 -4.2752266 -4.2898841 -4.3001065 -4.3073735][-4.1884642 -4.1909132 -4.1847868 -4.1677022 -4.1389852 -4.1059036 -4.0753188 -4.0792813 -4.1083961 -4.1472521 -4.1858897 -4.2179284 -4.2372684 -4.2522283 -4.2664251][-4.1319218 -4.13615 -4.1289959 -4.1028461 -4.0567636 -3.9997296 -3.9433224 -3.9426906 -3.995481 -4.0604343 -4.1109028 -4.1440248 -4.1631365 -4.1809087 -4.1984458][-4.0818172 -4.0943131 -4.0868139 -4.0510826 -3.9889112 -3.909307 -3.8270814 -3.8165596 -3.8935654 -3.9835851 -4.0448418 -4.07999 -4.1005874 -4.1188426 -4.134552][-4.0293446 -4.06271 -4.07393 -4.0492964 -3.9913278 -3.9166884 -3.8392138 -3.819695 -3.88319 -3.963233 -4.01885 -4.0491405 -4.0681214 -4.0840993 -4.0966835][-3.9749253 -4.0308161 -4.0714126 -4.0730906 -4.0368185 -3.9940579 -3.9621446 -3.9586835 -3.9869208 -4.024426 -4.0522466 -4.0671959 -4.077415 -4.0853424 -4.0914497][-3.961585 -4.0229378 -4.072072 -4.0879569 -4.0722032 -4.0620937 -4.068542 -4.0831051 -4.0951877 -4.1093755 -4.1212845 -4.1265669 -4.1295943 -4.1284084 -4.1225767][-4.0124469 -4.0536466 -4.0888014 -4.1022754 -4.0969329 -4.1003022 -4.1170907 -4.1326442 -4.139009 -4.1519179 -4.1702394 -4.1832433 -4.1891165 -4.1850667 -4.172421][-4.1048613 -4.1179528 -4.1300864 -4.1342621 -4.1290007 -4.1247315 -4.1270752 -4.1297626 -4.1328316 -4.1533003 -4.1864686 -4.2121429 -4.2229595 -4.2210908 -4.2127814][-4.2063384 -4.2054477 -4.2019148 -4.1954303 -4.1841736 -4.1633916 -4.1414256 -4.1244254 -4.1234136 -4.1493359 -4.1891942 -4.2204432 -4.235817 -4.2426152 -4.2448177][-4.2742362 -4.2753048 -4.2676864 -4.2536941 -4.23547 -4.2057176 -4.1695371 -4.1383595 -4.1310186 -4.1546497 -4.1901875 -4.2200589 -4.2415881 -4.2576632 -4.2656636]]...]
INFO - root - 2017-12-05 16:16:12.867007: step 24410, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 74h:05m:31s remains)
INFO - root - 2017-12-05 16:16:21.321705: step 24420, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 71h:45m:11s remains)
INFO - root - 2017-12-05 16:16:29.895314: step 24430, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 76h:42m:23s remains)
INFO - root - 2017-12-05 16:16:38.452755: step 24440, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 71h:50m:49s remains)
INFO - root - 2017-12-05 16:16:46.835065: step 24450, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 73h:22m:56s remains)
INFO - root - 2017-12-05 16:16:55.383863: step 24460, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.822 sec/batch; 70h:20m:59s remains)
INFO - root - 2017-12-05 16:17:03.814256: step 24470, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 71h:33m:35s remains)
INFO - root - 2017-12-05 16:17:12.329510: step 24480, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 71h:01m:41s remains)
INFO - root - 2017-12-05 16:17:20.983879: step 24490, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 71h:19m:14s remains)
INFO - root - 2017-12-05 16:17:29.489982: step 24500, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 73h:21m:10s remains)
2017-12-05 16:17:30.286322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3017731 -4.2949347 -4.2874384 -4.28145 -4.2814512 -4.2870493 -4.298378 -4.3061485 -4.3056378 -4.3039303 -4.3022661 -4.3008146 -4.3017945 -4.3082418 -4.315949][-4.2798686 -4.2726212 -4.2667756 -4.2638431 -4.2685404 -4.278583 -4.2949944 -4.3033438 -4.296443 -4.2876821 -4.2793055 -4.2733026 -4.2747993 -4.2897468 -4.3062634][-4.2613182 -4.25385 -4.2488976 -4.2459641 -4.2496953 -4.2563367 -4.2696834 -4.27573 -4.2639885 -4.2509627 -4.2399745 -4.2340169 -4.2394447 -4.2678323 -4.2971931][-4.2498717 -4.2438984 -4.2391605 -4.2336063 -4.2306404 -4.2286453 -4.2325473 -4.2354584 -4.22096 -4.2042422 -4.1960821 -4.197968 -4.2125459 -4.251545 -4.2910748][-4.238133 -4.2298851 -4.2235107 -4.21277 -4.20068 -4.188674 -4.1798854 -4.174427 -4.1544447 -4.136445 -4.1363125 -4.1542687 -4.1826181 -4.2322817 -4.2802467][-4.2195644 -4.2065706 -4.1974573 -4.1809835 -4.156888 -4.1324444 -4.1115 -4.0961142 -4.070992 -4.0570097 -4.0720015 -4.1101203 -4.1543121 -4.2135677 -4.2676692][-4.2069921 -4.1882958 -4.1742 -4.1522722 -4.1140919 -4.0718637 -4.0353217 -4.0064697 -3.9756434 -3.9688678 -4.0042696 -4.067286 -4.1312542 -4.1998296 -4.2585239][-4.1943278 -4.1724672 -4.1573453 -4.1317997 -4.086205 -4.0344377 -3.9903624 -3.9570873 -3.9306405 -3.9322011 -3.9791598 -4.0549412 -4.1288209 -4.1985059 -4.2561769][-4.1980448 -4.1792026 -4.1689262 -4.1473374 -4.1065717 -4.0594029 -4.0243731 -4.0045524 -3.9950392 -4.00181 -4.0404611 -4.10247 -4.1649671 -4.2238684 -4.2721987][-4.223042 -4.2130685 -4.2091017 -4.1969061 -4.1689825 -4.1349411 -4.1109142 -4.1006689 -4.0978012 -4.1021261 -4.1275482 -4.1711869 -4.2157478 -4.2603602 -4.2969851][-4.2533212 -4.2501435 -4.2495127 -4.2409806 -4.2217216 -4.1980271 -4.1809649 -4.1761646 -4.1761813 -4.1795268 -4.1968012 -4.2290463 -4.2604761 -4.2924919 -4.3191543][-4.2815776 -4.2803493 -4.2798066 -4.2725983 -4.2589765 -4.2444167 -4.2342119 -4.2319016 -4.23176 -4.2310171 -4.2406774 -4.264502 -4.2890358 -4.3124318 -4.3318362][-4.3111291 -4.30887 -4.3054581 -4.2976356 -4.2880864 -4.2798357 -4.2753725 -4.2760081 -4.2759323 -4.2728252 -4.2767658 -4.2923846 -4.31028 -4.3272271 -4.3404365][-4.3334246 -4.3289933 -4.3236876 -4.3168154 -4.3110909 -4.3082561 -4.3087277 -4.3115253 -4.311522 -4.309217 -4.3101664 -4.3179708 -4.3282619 -4.3384337 -4.3462214][-4.3462687 -4.3418832 -4.3365741 -4.3312511 -4.3275776 -4.3267422 -4.3287406 -4.3321767 -4.3342905 -4.3343015 -4.3345313 -4.3369803 -4.3407965 -4.3447824 -4.3480821]]...]
INFO - root - 2017-12-05 16:17:38.798641: step 24510, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 76h:08m:29s remains)
INFO - root - 2017-12-05 16:17:47.329856: step 24520, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 72h:33m:18s remains)
INFO - root - 2017-12-05 16:17:55.611469: step 24530, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.834 sec/batch; 71h:22m:11s remains)
INFO - root - 2017-12-05 16:18:03.980995: step 24540, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 73h:03m:37s remains)
INFO - root - 2017-12-05 16:18:12.378682: step 24550, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 70h:33m:50s remains)
INFO - root - 2017-12-05 16:18:20.779325: step 24560, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 71h:34m:48s remains)
INFO - root - 2017-12-05 16:18:29.330935: step 24570, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 71h:38m:14s remains)
INFO - root - 2017-12-05 16:18:37.809153: step 24580, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 72h:36m:52s remains)
INFO - root - 2017-12-05 16:18:46.232041: step 24590, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 73h:25m:52s remains)
INFO - root - 2017-12-05 16:18:54.688275: step 24600, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 73h:02m:16s remains)
2017-12-05 16:18:55.449991: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2946315 -4.2953281 -4.2957721 -4.2934036 -4.2896109 -4.2882729 -4.2893677 -4.2855229 -4.2783675 -4.2717352 -4.2669868 -4.264472 -4.2636185 -4.2664886 -4.268136][-4.2694435 -4.273416 -4.2765346 -4.2748976 -4.2706366 -4.2683363 -4.2677312 -4.2630005 -4.2540417 -4.2457595 -4.2410903 -4.2403283 -4.2440352 -4.2528133 -4.2582216][-4.2261152 -4.2243032 -4.2253547 -4.2246027 -4.22257 -4.2203741 -4.2198329 -4.2164264 -4.2096076 -4.2060862 -4.2047877 -4.2069783 -4.2172136 -4.231555 -4.2419453][-4.17529 -4.1686497 -4.1713424 -4.1761289 -4.1804142 -4.1799607 -4.1776881 -4.1749172 -4.1723228 -4.1757951 -4.179637 -4.1858258 -4.198843 -4.2139831 -4.2243567][-4.1499033 -4.1383772 -4.1418066 -4.1492014 -4.1575804 -4.1576509 -4.1510739 -4.1443319 -4.1412396 -4.1482081 -4.1568985 -4.1689372 -4.1852946 -4.1982188 -4.2075982][-4.1324682 -4.1173577 -4.1191983 -4.124783 -4.1353383 -4.1372614 -4.1257968 -4.114459 -4.1111345 -4.1185932 -4.1317139 -4.1476412 -4.164134 -4.1732054 -4.18124][-4.1173487 -4.0955667 -4.091238 -4.0930705 -4.1007681 -4.0989833 -4.0807409 -4.0663381 -4.0673862 -4.0778995 -4.09303 -4.1066227 -4.1214867 -4.1301341 -4.1382861][-4.0967073 -4.068882 -4.0630755 -4.0654378 -4.0682864 -4.0559173 -4.0250578 -4.0130868 -4.0276184 -4.0447021 -4.0579257 -4.06387 -4.0738463 -4.0856438 -4.0957828][-4.0723948 -4.0412993 -4.0296016 -4.0282035 -4.0258245 -4.0099812 -3.9792714 -3.9793594 -4.0152254 -4.0446911 -4.0527024 -4.0509081 -4.0529742 -4.0633259 -4.0733733][-4.0544043 -4.0156469 -3.9930956 -3.9898229 -3.9905767 -3.9824245 -3.9634545 -3.9702325 -4.0184946 -4.0599957 -4.0761442 -4.072567 -4.0665879 -4.0699239 -4.0784922][-4.0472932 -4.0006609 -3.9748828 -3.9774318 -3.991024 -4.0004168 -3.9945722 -4.0000081 -4.0412951 -4.086935 -4.1121635 -4.11184 -4.1071234 -4.1063766 -4.1090608][-4.0698843 -4.0194545 -3.9874363 -3.9901862 -4.0107942 -4.0347314 -4.0427661 -4.0509152 -4.0824666 -4.1204824 -4.1437807 -4.1459846 -4.1493006 -4.1515732 -4.14905][-4.1083536 -4.0626397 -4.023221 -4.0196571 -4.041399 -4.0719624 -4.0863724 -4.095345 -4.1169086 -4.1431231 -4.1601791 -4.1678319 -4.1809807 -4.1903992 -4.1883144][-4.1612568 -4.1263738 -4.0879364 -4.0795541 -4.0997949 -4.13345 -4.1463771 -4.1497469 -4.1636605 -4.1822805 -4.1916265 -4.20079 -4.2180419 -4.2321692 -4.2342968][-4.2202978 -4.2005253 -4.1749668 -4.1675849 -4.182641 -4.2111483 -4.2206349 -4.2195973 -4.2255654 -4.2342954 -4.2344069 -4.2401891 -4.258472 -4.2736874 -4.2789598]]...]
INFO - root - 2017-12-05 16:19:03.960839: step 24610, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 74h:57m:24s remains)
INFO - root - 2017-12-05 16:19:12.542319: step 24620, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 74h:28m:14s remains)
INFO - root - 2017-12-05 16:19:20.945075: step 24630, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 71h:34m:56s remains)
INFO - root - 2017-12-05 16:19:29.433377: step 24640, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 72h:51m:55s remains)
INFO - root - 2017-12-05 16:19:37.935831: step 24650, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 71h:18m:44s remains)
INFO - root - 2017-12-05 16:19:46.467055: step 24660, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 71h:11m:22s remains)
INFO - root - 2017-12-05 16:19:54.917928: step 24670, loss = 2.02, batch loss = 1.96 (9.4 examples/sec; 0.850 sec/batch; 72h:39m:20s remains)
INFO - root - 2017-12-05 16:20:03.418973: step 24680, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 71h:24m:15s remains)
INFO - root - 2017-12-05 16:20:11.877949: step 24690, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 72h:10m:11s remains)
INFO - root - 2017-12-05 16:20:20.367604: step 24700, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 70h:35m:01s remains)
2017-12-05 16:20:21.131163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2911472 -4.295043 -4.2942934 -4.2932816 -4.2897725 -4.2856236 -4.2820005 -4.2799749 -4.2796745 -4.2817235 -4.2864575 -4.2924614 -4.2979488 -4.3011131 -4.3025818][-4.3119321 -4.3165312 -4.3168516 -4.3164744 -4.3132229 -4.309485 -4.306231 -4.303031 -4.3019266 -4.3030024 -4.3072042 -4.313107 -4.3185806 -4.3223944 -4.3245487][-4.3225622 -4.3252573 -4.3252935 -4.3258629 -4.3243756 -4.3218622 -4.3185883 -4.3154459 -4.3151116 -4.3153028 -4.3176341 -4.322206 -4.3268261 -4.3309312 -4.3348627][-4.3137269 -4.3122478 -4.3109941 -4.3124161 -4.3129358 -4.3102098 -4.3052025 -4.3004436 -4.3007288 -4.3013787 -4.3033133 -4.3077993 -4.3110247 -4.3148613 -4.3206549][-4.2955561 -4.2883353 -4.2817841 -4.279705 -4.2789259 -4.2738795 -4.2639947 -4.2583089 -4.2632933 -4.267292 -4.2736535 -4.2809687 -4.2823591 -4.2853136 -4.2890959][-4.2700672 -4.254117 -4.2368255 -4.2257524 -4.2199345 -4.208312 -4.1919656 -4.1894097 -4.2025161 -4.2128386 -4.229188 -4.2433329 -4.24269 -4.2425528 -4.241426][-4.2342706 -4.2053566 -4.1729612 -4.148694 -4.1356897 -4.1162605 -4.0979576 -4.1069722 -4.1344008 -4.1551661 -4.1837454 -4.2054338 -4.2017732 -4.1946559 -4.1844921][-4.1958566 -4.1470833 -4.0946331 -4.0540147 -4.02816 -3.9941077 -3.9677224 -3.9870112 -4.032671 -4.0655985 -4.1046572 -4.1351333 -4.1345344 -4.1219206 -4.1041484][-4.1648903 -4.0923333 -4.0168638 -3.9593797 -3.9223979 -3.8790984 -3.8510821 -3.884104 -3.95052 -3.9998031 -4.0523863 -4.0952158 -4.1057916 -4.0999632 -4.0838552][-4.1729407 -4.0987549 -4.0249286 -3.9731371 -3.9442935 -3.9144294 -3.8997207 -3.9352331 -4.0015073 -4.0515046 -4.1011534 -4.1439252 -4.1620765 -4.1643691 -4.155643][-4.2197933 -4.1636882 -4.1106071 -4.0754013 -4.05953 -4.0448465 -4.0416746 -4.0698867 -4.1200762 -4.1609373 -4.1972294 -4.227263 -4.2412286 -4.2440982 -4.2384777][-4.2760348 -4.2422261 -4.2103643 -4.190352 -4.1849465 -4.1815262 -4.1839929 -4.2021623 -4.2349505 -4.26269 -4.2836623 -4.29977 -4.3077693 -4.3082609 -4.3038244][-4.3276439 -4.3119216 -4.2964458 -4.2871013 -4.2855177 -4.2846212 -4.2867475 -4.2958651 -4.3134503 -4.3274326 -4.3367147 -4.3441272 -4.3473225 -4.3455877 -4.34193][-4.3558154 -4.3508463 -4.3456497 -4.3433752 -4.3433933 -4.3430514 -4.3445878 -4.3487859 -4.3558664 -4.359756 -4.3612065 -4.3611369 -4.3577828 -4.3515611 -4.3466587][-4.360075 -4.3566976 -4.3528242 -4.3507657 -4.3508244 -4.3522673 -4.3544497 -4.3570151 -4.3586326 -4.3567562 -4.3533554 -4.3500009 -4.3452916 -4.3397155 -4.3358474]]...]
INFO - root - 2017-12-05 16:20:29.730003: step 24710, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 73h:03m:30s remains)
INFO - root - 2017-12-05 16:20:38.263149: step 24720, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 75h:00m:12s remains)
INFO - root - 2017-12-05 16:20:46.658262: step 24730, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 71h:53m:59s remains)
INFO - root - 2017-12-05 16:20:55.296405: step 24740, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 73h:30m:49s remains)
INFO - root - 2017-12-05 16:21:03.780203: step 24750, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 73h:01m:58s remains)
INFO - root - 2017-12-05 16:21:12.250641: step 24760, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 73h:03m:27s remains)
INFO - root - 2017-12-05 16:21:20.775986: step 24770, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 73h:23m:15s remains)
INFO - root - 2017-12-05 16:21:29.265725: step 24780, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 73h:49m:58s remains)
INFO - root - 2017-12-05 16:21:37.779099: step 24790, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 73h:26m:54s remains)
INFO - root - 2017-12-05 16:21:46.213946: step 24800, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 73h:35m:56s remains)
2017-12-05 16:21:46.984495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2577972 -4.2507157 -4.2395082 -4.2330284 -4.2358317 -4.2371531 -4.2354259 -4.2365494 -4.2387776 -4.236269 -4.2321277 -4.231936 -4.233479 -4.23355 -4.2296619][-4.2320085 -4.22404 -4.2088056 -4.1995678 -4.2053523 -4.2104225 -4.210597 -4.2141547 -4.2207308 -4.2195539 -4.2139053 -4.2112083 -4.2127457 -4.2151585 -4.2119303][-4.2092729 -4.2037745 -4.1891603 -4.1763535 -4.1822996 -4.1874542 -4.1884041 -4.1952677 -4.2060361 -4.2066526 -4.2005949 -4.1937881 -4.1930156 -4.1951132 -4.192924][-4.1950169 -4.1967354 -4.1860504 -4.1692257 -4.1670141 -4.166688 -4.1654034 -4.1723871 -4.1848741 -4.1861954 -4.1823516 -4.1771708 -4.1745825 -4.1716723 -4.1673465][-4.1953983 -4.2006044 -4.1916862 -4.1697955 -4.1569185 -4.1519194 -4.1500773 -4.1552143 -4.1639667 -4.1666808 -4.1661062 -4.1620255 -4.1537085 -4.146246 -4.1413503][-4.1907845 -4.1944761 -4.1837626 -4.1612096 -4.1396842 -4.1303697 -4.1311307 -4.1383719 -4.1456909 -4.1487088 -4.1521454 -4.1489668 -4.1329737 -4.1226044 -4.1239586][-4.1834245 -4.1843052 -4.1743312 -4.1506643 -4.1175003 -4.094842 -4.0917392 -4.1038961 -4.1157861 -4.12631 -4.1330733 -4.1269541 -4.1053758 -4.0909662 -4.101418][-4.1844211 -4.18835 -4.1790342 -4.1498804 -4.1079497 -4.0702381 -4.0619011 -4.0762572 -4.0914874 -4.1085234 -4.1167936 -4.1072693 -4.0805178 -4.0630822 -4.079206][-4.1829677 -4.1938748 -4.1862421 -4.1484656 -4.1011968 -4.0575104 -4.0480494 -4.0653157 -4.08003 -4.0960813 -4.1026688 -4.0923285 -4.0673628 -4.055861 -4.0792494][-4.1803112 -4.1919837 -4.1816525 -4.1379991 -4.0942593 -4.0567126 -4.0507808 -4.0678606 -4.077611 -4.0929523 -4.1030827 -4.099256 -4.078938 -4.0740781 -4.1003003][-4.1805358 -4.1914997 -4.1813207 -4.1368456 -4.1010318 -4.0741348 -4.0731897 -4.0840211 -4.0825157 -4.0951347 -4.1127439 -4.1156354 -4.094717 -4.0872383 -4.1121387][-4.1667376 -4.1791234 -4.1714935 -4.1337671 -4.1052275 -4.0898776 -4.0961003 -4.1035457 -4.093246 -4.0970383 -4.116756 -4.125639 -4.1086988 -4.1030354 -4.1269274][-4.1407132 -4.1498785 -4.1478996 -4.1198444 -4.1008034 -4.0979371 -4.1139951 -4.1282988 -4.1156774 -4.1068177 -4.115696 -4.1258197 -4.1215668 -4.1258597 -4.1475396][-4.1363959 -4.1416144 -4.1428404 -4.1201015 -4.1062994 -4.110538 -4.1351004 -4.1567087 -4.1470337 -4.1278505 -4.1200104 -4.1280522 -4.1390076 -4.1555753 -4.1754766][-4.1581373 -4.1666327 -4.167438 -4.1419387 -4.12308 -4.1277785 -4.1547832 -4.1813912 -4.1767974 -4.1535521 -4.1340837 -4.1378832 -4.1560969 -4.181457 -4.2005925]]...]
INFO - root - 2017-12-05 16:21:55.509112: step 24810, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 71h:55m:21s remains)
INFO - root - 2017-12-05 16:22:04.102981: step 24820, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 72h:30m:31s remains)
INFO - root - 2017-12-05 16:22:12.622440: step 24830, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.875 sec/batch; 74h:47m:06s remains)
INFO - root - 2017-12-05 16:22:21.124076: step 24840, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 74h:56m:55s remains)
INFO - root - 2017-12-05 16:22:29.847057: step 24850, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 72h:23m:15s remains)
INFO - root - 2017-12-05 16:22:38.401415: step 24860, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 72h:20m:14s remains)
INFO - root - 2017-12-05 16:22:46.826142: step 24870, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 71h:28m:09s remains)
INFO - root - 2017-12-05 16:22:55.428729: step 24880, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 72h:18m:08s remains)
INFO - root - 2017-12-05 16:23:03.965049: step 24890, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 75h:49m:09s remains)
INFO - root - 2017-12-05 16:23:12.342834: step 24900, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 69h:50m:48s remains)
2017-12-05 16:23:13.212641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2682142 -4.2620807 -4.2580657 -4.2550488 -4.2523565 -4.2472954 -4.2445574 -4.2366834 -4.22523 -4.2267218 -4.2314038 -4.233531 -4.2212038 -4.2034106 -4.1904488][-4.2651892 -4.2576613 -4.252089 -4.247859 -4.2436666 -4.2401395 -4.2399564 -4.2322516 -4.2222137 -4.2272806 -4.2362552 -4.2373805 -4.2259321 -4.2153721 -4.2116289][-4.2637596 -4.2556915 -4.2493939 -4.2442713 -4.2386441 -4.2357168 -4.2341413 -4.223352 -4.2119293 -4.2220178 -4.2392311 -4.245728 -4.2392683 -4.2342815 -4.2358174][-4.2550006 -4.2446556 -4.2349133 -4.226573 -4.2202415 -4.2164245 -4.2091937 -4.1924195 -4.1839933 -4.2059703 -4.2362671 -4.252604 -4.2506151 -4.2468419 -4.250555][-4.2346191 -4.2196097 -4.2034497 -4.1873674 -4.1719866 -4.15638 -4.1342649 -4.1076684 -4.1090012 -4.1560063 -4.2081928 -4.2349334 -4.2377324 -4.238235 -4.2438064][-4.2086778 -4.1972752 -4.1794591 -4.1557288 -4.1244917 -4.0809069 -4.0226936 -3.9677343 -3.9860342 -4.0687089 -4.1456251 -4.1836858 -4.1952558 -4.2008557 -4.2081318][-4.1869173 -4.1860242 -4.1738343 -4.1467848 -4.1021538 -4.036376 -3.94837 -3.8631945 -3.8882833 -3.9923573 -4.08266 -4.127913 -4.1465149 -4.1583676 -4.1705537][-4.1877308 -4.1944637 -4.1891847 -4.163898 -4.1210146 -4.0614076 -3.9896007 -3.9271555 -3.9473984 -4.0204964 -4.0843306 -4.1169066 -4.1300521 -4.1418676 -4.1600008][-4.2131472 -4.2226624 -4.22041 -4.1999688 -4.1664724 -4.1227784 -4.0800471 -4.052515 -4.0714622 -4.1087179 -4.1382136 -4.1530485 -4.1579351 -4.1666617 -4.1852427][-4.2402539 -4.2518167 -4.2521634 -4.2375965 -4.2142115 -4.1869063 -4.1680169 -4.1657863 -4.1844177 -4.20067 -4.207243 -4.208868 -4.2074318 -4.2122774 -4.2256517][-4.2526116 -4.2644987 -4.2663655 -4.2574468 -4.2433791 -4.2313256 -4.228766 -4.2393093 -4.2570586 -4.2644329 -4.2609015 -4.2573261 -4.2528806 -4.2542982 -4.2622023][-4.253057 -4.2622337 -4.2629056 -4.2566752 -4.2495317 -4.2474518 -4.2541089 -4.2691193 -4.2835188 -4.2879992 -4.2835913 -4.2794127 -4.2751145 -4.2763891 -4.2817979][-4.2519755 -4.2559576 -4.254847 -4.2490873 -4.2452469 -4.2475696 -4.2571745 -4.2699118 -4.2802534 -4.2834797 -4.2818933 -4.2805204 -4.2788992 -4.2809305 -4.2856054][-4.2519832 -4.2505975 -4.2473426 -4.2412634 -4.2379651 -4.2421708 -4.2523971 -4.2622271 -4.2686586 -4.2706971 -4.2708697 -4.2716856 -4.2731266 -4.2779655 -4.2837443][-4.252068 -4.2484808 -4.2444549 -4.237113 -4.2318907 -4.2361326 -4.2473907 -4.2567205 -4.2603583 -4.2591333 -4.257061 -4.2571411 -4.2612772 -4.2699461 -4.2782812]]...]
INFO - root - 2017-12-05 16:23:21.696293: step 24910, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 70h:56m:17s remains)
INFO - root - 2017-12-05 16:23:30.298711: step 24920, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 73h:52m:42s remains)
INFO - root - 2017-12-05 16:23:38.680940: step 24930, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 71h:42m:20s remains)
INFO - root - 2017-12-05 16:23:47.168268: step 24940, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 71h:01m:37s remains)
INFO - root - 2017-12-05 16:23:55.819301: step 24950, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 72h:07m:47s remains)
INFO - root - 2017-12-05 16:24:04.370843: step 24960, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 72h:10m:16s remains)
INFO - root - 2017-12-05 16:24:12.813578: step 24970, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 72h:59m:43s remains)
INFO - root - 2017-12-05 16:24:21.511715: step 24980, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 78h:23m:55s remains)
INFO - root - 2017-12-05 16:24:30.030356: step 24990, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 72h:36m:33s remains)
INFO - root - 2017-12-05 16:24:38.466662: step 25000, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 72h:04m:15s remains)
2017-12-05 16:24:39.244262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2625012 -4.2643514 -4.2694569 -4.2714267 -4.2689424 -4.2722797 -4.2805023 -4.2856045 -4.2857342 -4.2808051 -4.2740211 -4.2554541 -4.2260151 -4.1803164 -4.12409][-4.2501163 -4.240221 -4.2366819 -4.2357 -4.2319231 -4.2371678 -4.2504725 -4.2580805 -4.2597241 -4.2603374 -4.2582746 -4.2415185 -4.2185769 -4.1749077 -4.1168623][-4.2376609 -4.2175384 -4.2035022 -4.1965275 -4.1884856 -4.1924143 -4.2040811 -4.2092257 -4.2172108 -4.2305675 -4.2375364 -4.2248173 -4.2077627 -4.1714487 -4.1198][-4.2344365 -4.2055941 -4.1793146 -4.1592917 -4.1409984 -4.137917 -4.1382332 -4.1338964 -4.1550794 -4.1916947 -4.2110853 -4.2086687 -4.1992674 -4.1711249 -4.1287189][-4.2230639 -4.1898055 -4.1562543 -4.1253567 -4.0985155 -4.0860214 -4.0652342 -4.0417547 -4.0781403 -4.143898 -4.1790257 -4.1879444 -4.1849303 -4.1590824 -4.1207438][-4.2087541 -4.1741743 -4.1361418 -4.096334 -4.0586195 -4.0297995 -3.9747627 -3.9175315 -3.971103 -4.0758281 -4.1261749 -4.14149 -4.1411018 -4.1124663 -4.0723643][-4.1940527 -4.1553783 -4.1106553 -4.0603142 -4.0033431 -3.9499345 -3.8483839 -3.7425771 -3.817039 -3.9645565 -4.0371552 -4.0638151 -4.0648632 -4.0374331 -4.0044594][-4.1702156 -4.1302786 -4.0842552 -4.037034 -3.9799967 -3.9213262 -3.8064506 -3.6785233 -3.7587788 -3.9165397 -3.9980197 -4.0332875 -4.0399885 -4.0245152 -4.0090332][-4.1436028 -4.1116834 -4.07787 -4.0513487 -4.0184851 -3.9893339 -3.920577 -3.8423457 -3.8985074 -4.0047455 -4.059464 -4.0820093 -4.081275 -4.0674734 -4.0630288][-4.1189532 -4.0986133 -4.085227 -4.0879679 -4.0798507 -4.0741587 -4.0436673 -4.0115027 -4.0476394 -4.1094465 -4.1389675 -4.1499104 -4.1437144 -4.1298661 -4.1324019][-4.1032472 -4.0934072 -4.10028 -4.1304388 -4.143105 -4.1487694 -4.1389709 -4.1325507 -4.1575246 -4.1949472 -4.2098594 -4.2112341 -4.2024593 -4.1916676 -4.1967716][-4.1032166 -4.09792 -4.1144557 -4.1604848 -4.184557 -4.1997771 -4.2081585 -4.2176657 -4.235765 -4.2600822 -4.2673144 -4.2609339 -4.2498255 -4.2386885 -4.2407951][-4.1476984 -4.1432195 -4.1563749 -4.2000093 -4.227294 -4.2475381 -4.2663503 -4.2814417 -4.2925673 -4.3058171 -4.3088579 -4.3031211 -4.2924418 -4.2799754 -4.2749205][-4.2069855 -4.2007847 -4.2060628 -4.2366557 -4.2593431 -4.2809634 -4.3018012 -4.3136606 -4.3174214 -4.321126 -4.3199921 -4.3186884 -4.3130369 -4.3025227 -4.29652][-4.2566772 -4.2513905 -4.2543039 -4.2751489 -4.2913632 -4.3093967 -4.3260822 -4.3316884 -4.32874 -4.3257537 -4.323122 -4.3228045 -4.3198195 -4.3133297 -4.3105884]]...]
INFO - root - 2017-12-05 16:24:47.635461: step 25010, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 73h:19m:45s remains)
INFO - root - 2017-12-05 16:24:56.069847: step 25020, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 78h:18m:39s remains)
INFO - root - 2017-12-05 16:25:04.569865: step 25030, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 74h:07m:32s remains)
INFO - root - 2017-12-05 16:25:13.085168: step 25040, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 72h:25m:46s remains)
INFO - root - 2017-12-05 16:25:21.570445: step 25050, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 70h:56m:13s remains)
INFO - root - 2017-12-05 16:25:30.012969: step 25060, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.874 sec/batch; 74h:40m:33s remains)
INFO - root - 2017-12-05 16:25:38.478473: step 25070, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 71h:30m:41s remains)
INFO - root - 2017-12-05 16:25:47.027446: step 25080, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 72h:29m:35s remains)
INFO - root - 2017-12-05 16:25:55.452257: step 25090, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 72h:30m:19s remains)
INFO - root - 2017-12-05 16:26:03.864766: step 25100, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 71h:18m:44s remains)
2017-12-05 16:26:04.637809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9953771 -4.00597 -4.0171747 -4.0236087 -4.0421124 -4.08229 -4.0970616 -4.0721893 -4.0377107 -4.04582 -4.0899372 -4.1525912 -4.2169194 -4.2551632 -4.2798653][-4.0829048 -4.0828319 -4.089746 -4.0931225 -4.1054058 -4.1357632 -4.1485133 -4.1397691 -4.1214566 -4.1320376 -4.1730461 -4.2284389 -4.2767735 -4.2970104 -4.3054481][-4.1748309 -4.1694546 -4.1692266 -4.1658936 -4.1658626 -4.1766953 -4.17665 -4.1707296 -4.1655245 -4.1866331 -4.2303758 -4.2791243 -4.3122659 -4.32065 -4.3191142][-4.2192631 -4.2152262 -4.2087545 -4.1921864 -4.1726408 -4.1625471 -4.1454406 -4.1272435 -4.1267138 -4.163815 -4.2163911 -4.2731686 -4.3104081 -4.321825 -4.3196712][-4.2115107 -4.21212 -4.2022853 -4.1757741 -4.1475668 -4.12755 -4.0977311 -4.0545936 -4.038682 -4.0849066 -4.1563172 -4.2390275 -4.2961249 -4.3176193 -4.3188405][-4.1796703 -4.1893706 -4.1822991 -4.1598253 -4.133564 -4.111742 -4.0649061 -3.9827034 -3.9318128 -3.9779251 -4.0787888 -4.1908054 -4.2734942 -4.3078613 -4.3142185][-4.1537085 -4.1765785 -4.1673174 -4.1420259 -4.1135383 -4.0887403 -4.0246754 -3.9017062 -3.811224 -3.8550894 -3.9830408 -4.1220593 -4.2305789 -4.2846427 -4.302608][-4.1022515 -4.1414189 -4.1402211 -4.1122432 -4.0789413 -4.0378981 -3.9476626 -3.7856097 -3.6682148 -3.7244027 -3.8874807 -4.0514121 -4.1858163 -4.25839 -4.2900324][-4.040906 -4.0917459 -4.1042781 -4.0853257 -4.0552363 -4.0005436 -3.8782306 -3.681771 -3.5484591 -3.6124752 -3.7882874 -3.9671679 -4.1229734 -4.2173529 -4.2669897][-4.0455728 -4.0959454 -4.1152229 -4.1102128 -4.0971947 -4.0503902 -3.9255838 -3.7347231 -3.607744 -3.6502337 -3.7908812 -3.9454019 -4.0912457 -4.1866579 -4.2450976][-4.1098957 -4.1477871 -4.1695285 -4.1726928 -4.168251 -4.1362991 -4.0450163 -3.9092855 -3.8184397 -3.8418941 -3.9327698 -4.0399504 -4.1475258 -4.2142925 -4.2556176][-4.1882005 -4.2106175 -4.225327 -4.2291708 -4.2279038 -4.20943 -4.1548357 -4.0753021 -4.0223269 -4.0357876 -4.091114 -4.159759 -4.2319107 -4.2704687 -4.2893724][-4.2319355 -4.2430906 -4.2503996 -4.2499466 -4.2441926 -4.23416 -4.2103548 -4.1737561 -4.1526856 -4.1686592 -4.2074142 -4.2504821 -4.2921352 -4.3083754 -4.315455][-4.2378235 -4.2444615 -4.2466826 -4.2416534 -4.2299514 -4.2227159 -4.2168565 -4.2078214 -4.2045631 -4.2220869 -4.2551394 -4.2853484 -4.3066516 -4.3144212 -4.3219786][-4.2185912 -4.2179222 -4.2132807 -4.2037168 -4.1916318 -4.1854057 -4.185986 -4.1848421 -4.18577 -4.2012362 -4.2311254 -4.258831 -4.2777052 -4.2906833 -4.3077993]]...]
INFO - root - 2017-12-05 16:26:13.093060: step 25110, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 0.803 sec/batch; 68h:35m:33s remains)
INFO - root - 2017-12-05 16:26:21.553922: step 25120, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.894 sec/batch; 76h:20m:01s remains)
INFO - root - 2017-12-05 16:26:29.962056: step 25130, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 72h:52m:42s remains)
INFO - root - 2017-12-05 16:26:38.402901: step 25140, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 71h:40m:28s remains)
INFO - root - 2017-12-05 16:26:46.779578: step 25150, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 72h:38m:19s remains)
INFO - root - 2017-12-05 16:26:55.122061: step 25160, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 72h:03m:34s remains)
INFO - root - 2017-12-05 16:27:03.608602: step 25170, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 73h:57m:37s remains)
INFO - root - 2017-12-05 16:27:12.046918: step 25180, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 71h:21m:20s remains)
INFO - root - 2017-12-05 16:27:20.636628: step 25190, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 72h:29m:55s remains)
INFO - root - 2017-12-05 16:27:29.248852: step 25200, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 75h:11m:53s remains)
2017-12-05 16:27:30.008192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.316752 -4.3353157 -4.3412037 -4.342433 -4.3411007 -4.3385177 -4.3361182 -4.3352437 -4.3357768 -4.3365631 -4.3372979 -4.338541 -4.339767 -4.3405347 -4.3406577][-4.2823706 -4.3180962 -4.3353782 -4.3414493 -4.3395929 -4.3350267 -4.3315935 -4.3316908 -4.3345866 -4.337327 -4.3387451 -4.3396392 -4.3403816 -4.3408771 -4.340889][-4.2399249 -4.2951341 -4.3255682 -4.33673 -4.3350987 -4.3284626 -4.3240128 -4.3240366 -4.3287215 -4.3345504 -4.3387032 -4.3415084 -4.3429074 -4.34312 -4.3425479][-4.2053723 -4.2743564 -4.3117976 -4.3221817 -4.31663 -4.3053179 -4.2969255 -4.2947145 -4.3016934 -4.3146343 -4.3268223 -4.337718 -4.3437638 -4.3457518 -4.3449292][-4.2182446 -4.2777872 -4.3040605 -4.3032508 -4.2868176 -4.2656894 -4.2469091 -4.2360005 -4.2431207 -4.264802 -4.2915797 -4.3177476 -4.3358512 -4.3453245 -4.3470659][-4.2620974 -4.2951083 -4.2954388 -4.2746615 -4.2385516 -4.2001672 -4.1662765 -4.142107 -4.1487603 -4.183413 -4.2324209 -4.2811861 -4.3174939 -4.3389344 -4.3471327][-4.2964988 -4.3055873 -4.2792087 -4.234036 -4.172677 -4.1097217 -4.0475416 -4.0009518 -4.0122075 -4.0703197 -4.1508856 -4.2275 -4.2863674 -4.3240662 -4.341785][-4.3091669 -4.3027749 -4.25603 -4.1867161 -4.0978394 -4.0056124 -3.9073544 -3.8310924 -3.8530145 -3.9463871 -4.0624866 -4.166832 -4.2482448 -4.3033257 -4.3322954][-4.3096762 -4.2961216 -4.241281 -4.1628275 -4.0642653 -3.9644053 -3.8589666 -3.7769537 -3.8061287 -3.9137764 -4.0434957 -4.1564226 -4.2438641 -4.3032026 -4.33348][-4.3074841 -4.3006673 -4.2561522 -4.192462 -4.1144495 -4.039876 -3.9662297 -3.915319 -3.9418824 -4.0249767 -4.1297174 -4.2197514 -4.2869191 -4.3291821 -4.3471632][-4.3109841 -4.3159237 -4.2862964 -4.2432952 -4.1920629 -4.1407914 -4.0888305 -4.0607128 -4.0889068 -4.1524811 -4.2303495 -4.2925973 -4.3350525 -4.3563662 -4.3600788][-4.3167682 -4.3293333 -4.3104396 -4.2799006 -4.2446623 -4.2069869 -4.1652317 -4.1495619 -4.182816 -4.2361245 -4.2949576 -4.3374681 -4.362679 -4.3696876 -4.3638][-4.3198938 -4.3364649 -4.3249331 -4.3020983 -4.2745314 -4.2435374 -4.2064819 -4.1959186 -4.2310576 -4.2788525 -4.3262229 -4.3555589 -4.3696809 -4.368196 -4.3586426][-4.3192225 -4.3370676 -4.3300085 -4.3113937 -4.2863545 -4.2571812 -4.2216482 -4.2147441 -4.2513871 -4.296371 -4.3368669 -4.3577747 -4.3655825 -4.3608265 -4.3510752][-4.3162284 -4.3348341 -4.3308206 -4.3144555 -4.2902102 -4.2625527 -4.2312613 -4.2285032 -4.2638421 -4.3054705 -4.3404026 -4.3555861 -4.35955 -4.3537498 -4.3457575]]...]
INFO - root - 2017-12-05 16:27:38.560991: step 25210, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.854 sec/batch; 72h:55m:13s remains)
INFO - root - 2017-12-05 16:27:46.904622: step 25220, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 73h:26m:45s remains)
INFO - root - 2017-12-05 16:27:55.375449: step 25230, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 74h:46m:15s remains)
INFO - root - 2017-12-05 16:28:04.022850: step 25240, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 73h:46m:16s remains)
INFO - root - 2017-12-05 16:28:12.599430: step 25250, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 72h:24m:28s remains)
INFO - root - 2017-12-05 16:28:21.083258: step 25260, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 72h:52m:23s remains)
INFO - root - 2017-12-05 16:28:29.579825: step 25270, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 70h:59m:28s remains)
INFO - root - 2017-12-05 16:28:38.095388: step 25280, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 74h:04m:52s remains)
INFO - root - 2017-12-05 16:28:46.788579: step 25290, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 75h:06m:35s remains)
INFO - root - 2017-12-05 16:28:55.275358: step 25300, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 72h:03m:41s remains)
2017-12-05 16:28:56.025587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2797651 -4.2855377 -4.2916923 -4.2839823 -4.2603712 -4.2268634 -4.1892786 -4.151866 -4.1144428 -4.0814052 -4.0559468 -4.0519104 -4.0766892 -4.1107554 -4.1225376][-4.2932343 -4.2964334 -4.2947025 -4.2753482 -4.239666 -4.1932936 -4.1456 -4.1013513 -4.0681648 -4.0486207 -4.03479 -4.0388465 -4.0698128 -4.1109581 -4.1370745][-4.2987547 -4.2974753 -4.2876334 -4.2575169 -4.2061615 -4.1425915 -4.0850449 -4.0416875 -4.0297213 -4.0404472 -4.0483508 -4.0624027 -4.0907111 -4.1233416 -4.1490936][-4.3004494 -4.2933474 -4.2734571 -4.2296457 -4.1571913 -4.0688744 -3.9990482 -3.9623797 -3.9890189 -4.0473909 -4.0882559 -4.1146121 -4.1406856 -4.1594419 -4.1721411][-4.2942076 -4.2818475 -4.2529945 -4.1949587 -4.0992603 -3.9859953 -3.89888 -3.8653157 -3.9334111 -4.0410261 -4.1158628 -4.1596861 -4.1950049 -4.2095585 -4.2105618][-4.2859354 -4.2699862 -4.2346916 -4.1635246 -4.0461969 -3.90204 -3.7798476 -3.7388225 -3.8491983 -4.0085135 -4.1171036 -4.1830816 -4.232286 -4.2511153 -4.2511759][-4.2771811 -4.2593403 -4.2222805 -4.1472297 -4.0168223 -3.8384941 -3.667408 -3.6204462 -3.7724216 -3.9701309 -4.1030612 -4.1849456 -4.2452221 -4.2681236 -4.2725205][-4.270112 -4.2508936 -4.2164531 -4.1464663 -4.0143957 -3.8243866 -3.6313071 -3.5903966 -3.7625017 -3.9708915 -4.1152563 -4.2028117 -4.2650948 -4.2857471 -4.2898583][-4.2636456 -4.2430854 -4.2123547 -4.1529675 -4.0412912 -3.8824592 -3.7256019 -3.6953435 -3.8393683 -4.0217276 -4.1528425 -4.2360716 -4.2931166 -4.3079886 -4.3087959][-4.2550135 -4.2293229 -4.2006125 -4.15293 -4.0711174 -3.9624097 -3.8630257 -3.851748 -3.9625194 -4.1039758 -4.2083068 -4.2745495 -4.3182425 -4.3258796 -4.3229942][-4.2419891 -4.20751 -4.1764584 -4.1362772 -4.0793633 -4.013247 -3.9655662 -3.9805133 -4.0706019 -4.1753793 -4.2517629 -4.3029089 -4.3338976 -4.336669 -4.3303118][-4.2248807 -4.1772337 -4.1404209 -4.1093445 -4.0763292 -4.04464 -4.0379748 -4.0738807 -4.1491017 -4.2270613 -4.2839327 -4.3218269 -4.341701 -4.342123 -4.333827][-4.214128 -4.1578765 -4.1187158 -4.0973792 -4.0819588 -4.0744691 -4.0971918 -4.1481204 -4.2131634 -4.2698879 -4.3089709 -4.3352747 -4.3468642 -4.3443661 -4.3339338][-4.2153993 -4.15821 -4.1199512 -4.1020994 -4.0995789 -4.1141257 -4.1574349 -4.2151852 -4.2700248 -4.310533 -4.3354692 -4.3506989 -4.3527527 -4.3433514 -4.3302183][-4.2204165 -4.1699991 -4.1354284 -4.1192484 -4.129364 -4.161973 -4.2131715 -4.263329 -4.3060317 -4.3352942 -4.351274 -4.3578968 -4.3527708 -4.3389225 -4.3242307]]...]
INFO - root - 2017-12-05 16:29:04.604125: step 25310, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 74h:40m:11s remains)
INFO - root - 2017-12-05 16:29:13.163058: step 25320, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 73h:22m:55s remains)
INFO - root - 2017-12-05 16:29:21.324443: step 25330, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 74h:02m:43s remains)
INFO - root - 2017-12-05 16:29:29.742975: step 25340, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 73h:08m:33s remains)
INFO - root - 2017-12-05 16:29:38.310142: step 25350, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 71h:47m:16s remains)
INFO - root - 2017-12-05 16:29:46.740429: step 25360, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 71h:21m:32s remains)
INFO - root - 2017-12-05 16:29:55.372107: step 25370, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 72h:09m:31s remains)
INFO - root - 2017-12-05 16:30:03.990744: step 25380, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 76h:58m:19s remains)
INFO - root - 2017-12-05 16:30:12.602889: step 25390, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 72h:36m:27s remains)
INFO - root - 2017-12-05 16:30:21.100512: step 25400, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 72h:35m:12s remains)
2017-12-05 16:30:21.877580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2803578 -4.2612324 -4.2438669 -4.2372246 -4.2388544 -4.2350345 -4.2255945 -4.2198272 -4.2223558 -4.2307663 -4.2512035 -4.280736 -4.308845 -4.3295588 -4.3466153][-4.2494893 -4.2256765 -4.2040129 -4.1971741 -4.1997085 -4.1938868 -4.17405 -4.15574 -4.1549911 -4.1702967 -4.2011003 -4.2423906 -4.2821088 -4.3134594 -4.3386388][-4.2211127 -4.1895742 -4.1602826 -4.1504126 -4.1546226 -4.1506629 -4.117444 -4.0823073 -4.0760565 -4.0989437 -4.1442308 -4.1999164 -4.2533684 -4.2962704 -4.3295155][-4.1892862 -4.1552777 -4.1243787 -4.1102614 -4.1169639 -4.1188827 -4.073606 -4.0181284 -4.0055923 -4.0404429 -4.0990038 -4.1636491 -4.2290168 -4.2821341 -4.3217163][-4.1507258 -4.1198888 -4.0944977 -4.0803871 -4.0818372 -4.0794368 -4.0283155 -3.9551327 -3.9356587 -3.9876461 -4.0646052 -4.1358223 -4.2086148 -4.2706723 -4.3166542][-4.1156588 -4.0877323 -4.0685172 -4.0536094 -4.0492558 -4.033535 -3.9757652 -3.8899755 -3.8600125 -3.935132 -4.0387535 -4.1206365 -4.1986346 -4.2654276 -4.3144617][-4.0819635 -4.0553112 -4.0354171 -4.0215893 -4.014822 -3.98881 -3.9276986 -3.8258893 -3.7779648 -3.8828464 -4.0163994 -4.1126738 -4.1951671 -4.2640405 -4.3138461][-4.0557847 -4.0361867 -4.0138278 -3.9975562 -3.9967005 -3.9658968 -3.8975744 -3.7719326 -3.7010689 -3.8355381 -3.99451 -4.102375 -4.1910281 -4.2617702 -4.3122449][-4.0446496 -4.0380588 -4.0233903 -4.0114551 -4.0242805 -4.0042863 -3.9348028 -3.797998 -3.7193222 -3.8491836 -4.0036387 -4.1100059 -4.1973162 -4.26329 -4.31082][-4.07074 -4.0867405 -4.0847349 -4.0826526 -4.105022 -4.0960855 -4.0279179 -3.9025257 -3.8376164 -3.9299176 -4.0462961 -4.1383414 -4.2157869 -4.2694325 -4.3102283][-4.1377158 -4.1621637 -4.1651449 -4.1671948 -4.1832743 -4.1725206 -4.108779 -4.0030122 -3.9510574 -4.0136847 -4.0998774 -4.1775293 -4.2410617 -4.2806821 -4.3128037][-4.2001472 -4.215847 -4.219193 -4.2238097 -4.2318211 -4.217865 -4.1637912 -4.0757542 -4.0353842 -4.0813279 -4.149199 -4.2129011 -4.2636151 -4.293817 -4.3180456][-4.2591596 -4.2651534 -4.26688 -4.2719369 -4.2724185 -4.2554145 -4.211257 -4.143322 -4.1099858 -4.1445265 -4.197145 -4.2455058 -4.2845507 -4.3078446 -4.3262281][-4.3000274 -4.3013759 -4.3042188 -4.3109107 -4.3110957 -4.296946 -4.2657557 -4.2185526 -4.1901984 -4.2115474 -4.2470837 -4.2803874 -4.3098154 -4.3266034 -4.3392868][-4.3331532 -4.3341703 -4.3360372 -4.3412004 -4.3418751 -4.3351669 -4.316937 -4.2864084 -4.2640004 -4.273128 -4.2936573 -4.3138618 -4.3332357 -4.3443141 -4.3519254]]...]
INFO - root - 2017-12-05 16:30:30.407431: step 25410, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 73h:19m:49s remains)
INFO - root - 2017-12-05 16:30:38.948602: step 25420, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.828 sec/batch; 70h:37m:29s remains)
INFO - root - 2017-12-05 16:30:47.391559: step 25430, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 74h:33m:04s remains)
INFO - root - 2017-12-05 16:30:55.724095: step 25440, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 71h:12m:06s remains)
INFO - root - 2017-12-05 16:31:04.236170: step 25450, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.846 sec/batch; 72h:10m:34s remains)
INFO - root - 2017-12-05 16:31:12.860584: step 25460, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 70h:21m:29s remains)
INFO - root - 2017-12-05 16:31:21.407513: step 25470, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 74h:51m:18s remains)
INFO - root - 2017-12-05 16:31:29.972847: step 25480, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 72h:41m:16s remains)
INFO - root - 2017-12-05 16:31:38.546108: step 25490, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 73h:40m:20s remains)
INFO - root - 2017-12-05 16:31:47.020177: step 25500, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 74h:17m:51s remains)
2017-12-05 16:31:47.819900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.282721 -4.2672281 -4.2331762 -4.1844792 -4.1301994 -4.0791197 -4.0364656 -4.0323105 -4.073596 -4.1289744 -4.165791 -4.2055707 -4.2330871 -4.2264776 -4.1883087][-4.2819362 -4.2662573 -4.2334094 -4.1810718 -4.1232843 -4.0663786 -4.0182815 -4.01167 -4.04935 -4.1134367 -4.1605139 -4.198195 -4.2203636 -4.2164025 -4.1830692][-4.2797303 -4.2642713 -4.2329769 -4.1797638 -4.1172137 -4.05479 -4.00449 -3.9939647 -4.0271287 -4.1004124 -4.1606178 -4.1966505 -4.2092004 -4.2024355 -4.1759973][-4.2748623 -4.2580104 -4.2304854 -4.1806006 -4.1163669 -4.0517726 -4.0043473 -3.9945734 -4.0284653 -4.1037917 -4.170207 -4.2008963 -4.2022924 -4.1905065 -4.1692615][-4.2699671 -4.251029 -4.2266469 -4.1836634 -4.1204505 -4.0563955 -4.0146155 -4.0130849 -4.0561113 -4.129385 -4.1903195 -4.2098007 -4.198473 -4.1801753 -4.1633406][-4.2648134 -4.2427368 -4.219667 -4.1814084 -4.1223478 -4.0616617 -4.0230579 -4.0309763 -4.0839024 -4.1506705 -4.199883 -4.208415 -4.1901951 -4.1676288 -4.1505432][-4.2601132 -4.2371416 -4.2145915 -4.1786776 -4.1257272 -4.067174 -4.0255938 -4.03497 -4.0869122 -4.1430278 -4.1868806 -4.1906672 -4.1676164 -4.1427755 -4.1307769][-4.2561593 -4.2346911 -4.2110872 -4.1725259 -4.119525 -4.0586486 -4.0090208 -4.015964 -4.06414 -4.1169596 -4.1628556 -4.1698203 -4.1450133 -4.1212511 -4.1160812][-4.2535768 -4.2331705 -4.2056861 -4.1627622 -4.1086755 -4.0478196 -3.9890704 -3.9895074 -4.0390134 -4.0937896 -4.1446433 -4.1585135 -4.1380258 -4.1172256 -4.1120586][-4.2524447 -4.2343435 -4.206759 -4.1637011 -4.1161404 -4.0630412 -4.0056639 -4.0035267 -4.0505672 -4.1016116 -4.1454172 -4.1599493 -4.144804 -4.1255736 -4.1163044][-4.2526841 -4.2369752 -4.2102833 -4.1702862 -4.1306462 -4.0899029 -4.0479536 -4.0482125 -4.0887027 -4.1311088 -4.1650648 -4.175457 -4.162075 -4.1433697 -4.1297274][-4.253861 -4.2404757 -4.2158146 -4.1810684 -4.1450334 -4.1142335 -4.0806122 -4.082552 -4.1190195 -4.1599784 -4.1886663 -4.1958308 -4.183825 -4.1658382 -4.1468997][-4.2539139 -4.2393737 -4.2158232 -4.1882324 -4.1585941 -4.1317496 -4.09773 -4.0919294 -4.1192989 -4.1635571 -4.1978731 -4.2123761 -4.2066855 -4.1920133 -4.1705422][-4.2519021 -4.2345972 -4.2099562 -4.1829109 -4.1561055 -4.1295328 -4.0940275 -4.081203 -4.1005378 -4.1488643 -4.1915808 -4.213315 -4.2172947 -4.2088933 -4.1882257][-4.2484636 -4.2281861 -4.2030363 -4.1777182 -4.1522331 -4.123548 -4.0958095 -4.0858765 -4.0979972 -4.1426969 -4.1860771 -4.2093797 -4.219049 -4.2147264 -4.1959696]]...]
INFO - root - 2017-12-05 16:31:56.475407: step 25510, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 72h:22m:52s remains)
INFO - root - 2017-12-05 16:32:04.927006: step 25520, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 72h:18m:02s remains)
INFO - root - 2017-12-05 16:32:13.293627: step 25530, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 73h:10m:06s remains)
INFO - root - 2017-12-05 16:32:21.736157: step 25540, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 71h:54m:54s remains)
INFO - root - 2017-12-05 16:32:30.179215: step 25550, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 73h:37m:24s remains)
INFO - root - 2017-12-05 16:32:38.842076: step 25560, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 73h:35m:34s remains)
INFO - root - 2017-12-05 16:32:47.317580: step 25570, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 72h:05m:49s remains)
INFO - root - 2017-12-05 16:32:55.814591: step 25580, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 70h:58m:37s remains)
INFO - root - 2017-12-05 16:33:04.294821: step 25590, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 71h:23m:01s remains)
INFO - root - 2017-12-05 16:33:12.883393: step 25600, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 73h:05m:21s remains)
2017-12-05 16:33:13.620606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2423482 -4.2246222 -4.2174911 -4.2120848 -4.187336 -4.1457152 -4.097754 -4.0673909 -4.0961781 -4.1686945 -4.2382174 -4.282083 -4.3111925 -4.3234286 -4.3215318][-4.2439709 -4.2161312 -4.2005606 -4.1925316 -4.1710958 -4.1357789 -4.0872841 -4.0566916 -4.0909524 -4.167069 -4.2360015 -4.2810283 -4.3108964 -4.324595 -4.3228493][-4.2467933 -4.2116671 -4.19015 -4.1766858 -4.1542211 -4.1199322 -4.0715852 -4.04299 -4.082 -4.1624551 -4.2334504 -4.2806888 -4.3103576 -4.3244848 -4.322906][-4.2551689 -4.2173109 -4.1923532 -4.1716986 -4.1429534 -4.1012607 -4.0453162 -4.0179882 -4.0654197 -4.154026 -4.2312751 -4.2821689 -4.3123088 -4.3251014 -4.3226609][-4.26902 -4.234345 -4.2067451 -4.1766448 -4.1368537 -4.0829334 -4.0159512 -3.9895778 -4.0464416 -4.1444564 -4.2283626 -4.2839537 -4.3153739 -4.3266859 -4.3230128][-4.2892885 -4.2628121 -4.2347841 -4.1966429 -4.1464844 -4.0807161 -4.0034747 -3.9763682 -4.0362697 -4.13829 -4.2244296 -4.28408 -4.3177571 -4.3290133 -4.3242755][-4.3093553 -4.293294 -4.2701426 -4.23108 -4.1773028 -4.1042256 -4.0188837 -3.983465 -4.0372677 -4.1370082 -4.2213907 -4.283143 -4.3182068 -4.3305 -4.3257027][-4.3206758 -4.3149514 -4.2986832 -4.2677188 -4.219583 -4.1467743 -4.0561004 -4.002954 -4.0423474 -4.1360607 -4.2177582 -4.2809253 -4.3174424 -4.331141 -4.3268905][-4.32238 -4.3249049 -4.315764 -4.2954888 -4.2590332 -4.1933546 -4.0988259 -4.0250292 -4.0456772 -4.1314635 -4.2113104 -4.2773294 -4.3157058 -4.33084 -4.3276758][-4.3185544 -4.3250618 -4.3208823 -4.3096347 -4.2853932 -4.2300458 -4.137392 -4.0495524 -4.0525408 -4.1271396 -4.2040739 -4.2721376 -4.3126235 -4.329843 -4.3278937][-4.3143625 -4.3222327 -4.3200932 -4.3136249 -4.2981215 -4.2539997 -4.16762 -4.0737672 -4.061986 -4.1252556 -4.1985536 -4.2679162 -4.3098941 -4.3287764 -4.3276954][-4.3128686 -4.3204427 -4.3186426 -4.3140078 -4.3023262 -4.2667069 -4.1892037 -4.0952206 -4.0691905 -4.1225443 -4.1944404 -4.26534 -4.3090324 -4.3280611 -4.3267179][-4.3116117 -4.318305 -4.3169909 -4.3145123 -4.3058953 -4.2781954 -4.2126322 -4.1261659 -4.0913877 -4.1323581 -4.1988897 -4.2671924 -4.3100777 -4.3270569 -4.3251247][-4.3093982 -4.3138752 -4.3131804 -4.31432 -4.3096266 -4.288085 -4.2335224 -4.1610951 -4.1264467 -4.1542845 -4.2098455 -4.2703495 -4.31007 -4.325016 -4.3231683][-4.3060994 -4.3081236 -4.3090305 -4.3137379 -4.3113155 -4.2931285 -4.2477145 -4.1904087 -4.1623588 -4.180676 -4.2240105 -4.2744331 -4.309299 -4.32285 -4.3213077]]...]
INFO - root - 2017-12-05 16:33:22.096820: step 25610, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 70h:27m:15s remains)
INFO - root - 2017-12-05 16:33:30.674755: step 25620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 72h:23m:12s remains)
INFO - root - 2017-12-05 16:33:39.200918: step 25630, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 72h:37m:21s remains)
INFO - root - 2017-12-05 16:33:47.608934: step 25640, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 70h:31m:56s remains)
INFO - root - 2017-12-05 16:33:56.047577: step 25650, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 75h:04m:32s remains)
INFO - root - 2017-12-05 16:34:04.536862: step 25660, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 72h:54m:21s remains)
INFO - root - 2017-12-05 16:34:12.997039: step 25670, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.862 sec/batch; 73h:28m:23s remains)
INFO - root - 2017-12-05 16:34:21.535066: step 25680, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 76h:06m:55s remains)
INFO - root - 2017-12-05 16:34:30.110277: step 25690, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 74h:01m:51s remains)
INFO - root - 2017-12-05 16:34:38.647735: step 25700, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 72h:20m:15s remains)
2017-12-05 16:34:39.439300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3052554 -4.3127713 -4.304821 -4.2872052 -4.2815647 -4.2955694 -4.3135681 -4.3138161 -4.2987962 -4.2791009 -4.2509007 -4.22045 -4.1917844 -4.1765251 -4.1804371][-4.3106313 -4.318038 -4.3093448 -4.294188 -4.2924695 -4.3071051 -4.3238239 -4.3267751 -4.318326 -4.3012323 -4.2678509 -4.2233262 -4.1802807 -4.1605487 -4.1707697][-4.2927761 -4.2974682 -4.2837734 -4.2663746 -4.26455 -4.278636 -4.2925639 -4.3010011 -4.3052959 -4.3015523 -4.2791018 -4.2390261 -4.1924453 -4.1666822 -4.1700497][-4.2630653 -4.2624087 -4.241354 -4.2140503 -4.2043767 -4.2149277 -4.2266016 -4.2432318 -4.2664571 -4.283329 -4.2824111 -4.2591538 -4.2222261 -4.1936665 -4.1813188][-4.2338114 -4.2262206 -4.1953549 -4.1565323 -4.1353006 -4.1364694 -4.1405973 -4.1637173 -4.206686 -4.2465115 -4.2669353 -4.2646842 -4.2442784 -4.2193227 -4.194345][-4.1974726 -4.1815696 -4.14449 -4.1014867 -4.0740509 -4.0612764 -4.0515862 -4.0777626 -4.1402559 -4.2043538 -4.2447896 -4.2579837 -4.2479048 -4.224658 -4.1910357][-4.1693277 -4.1408539 -4.0974369 -4.0569568 -4.0334287 -4.0112138 -3.9838514 -3.9979324 -4.0685234 -4.1539273 -4.2135696 -4.2406859 -4.2400541 -4.222846 -4.188879][-4.1640511 -4.1215987 -4.0730906 -4.037065 -4.0227232 -4.0010118 -3.9585721 -3.9485726 -4.0070796 -4.098855 -4.1714725 -4.2133312 -4.2284713 -4.2268429 -4.2037759][-4.1855187 -4.1368084 -4.0890255 -4.0594931 -4.0543747 -4.0391951 -3.9929507 -3.9639802 -3.9982653 -4.0761919 -4.1486459 -4.200901 -4.2320213 -4.2441516 -4.232954][-4.2290468 -4.1864834 -4.1460018 -4.121748 -4.1222711 -4.1132216 -4.0746441 -4.0413723 -4.0552454 -4.1088705 -4.1665874 -4.2169247 -4.2531624 -4.2714772 -4.2672873][-4.2654576 -4.236352 -4.2059774 -4.1882882 -4.1928825 -4.1918383 -4.1659179 -4.13661 -4.1372766 -4.1689224 -4.2083859 -4.2478523 -4.2774506 -4.2946944 -4.2934546][-4.2777562 -4.2625089 -4.243577 -4.2344632 -4.2440486 -4.2492828 -4.2346158 -4.2132483 -4.2078643 -4.2250981 -4.2484751 -4.2724133 -4.2885046 -4.2984691 -4.2970872][-4.258882 -4.2520556 -4.2425342 -4.2443929 -4.261054 -4.2718792 -4.2642512 -4.2487049 -4.2419872 -4.2508755 -4.261517 -4.2698655 -4.2734694 -4.2765131 -4.2745032][-4.2207918 -4.2144365 -4.2115245 -4.2232237 -4.2444925 -4.257617 -4.2515254 -4.2371936 -4.2301507 -4.235393 -4.2391105 -4.2370214 -4.2333231 -4.2349343 -4.2359943][-4.2019372 -4.1945066 -4.1933837 -4.2060761 -4.2226324 -4.2310061 -4.2208042 -4.2035074 -4.194273 -4.1985593 -4.201592 -4.1971259 -4.1922188 -4.1964841 -4.2032447]]...]
INFO - root - 2017-12-05 16:34:47.941207: step 25710, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 74h:13m:20s remains)
INFO - root - 2017-12-05 16:34:56.410152: step 25720, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 72h:49m:04s remains)
INFO - root - 2017-12-05 16:35:04.873823: step 25730, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 73h:45m:10s remains)
INFO - root - 2017-12-05 16:35:13.493951: step 25740, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 72h:46m:19s remains)
INFO - root - 2017-12-05 16:35:21.936700: step 25750, loss = 2.03, batch loss = 1.97 (10.2 examples/sec; 0.783 sec/batch; 66h:44m:43s remains)
INFO - root - 2017-12-05 16:35:30.455099: step 25760, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.838 sec/batch; 71h:23m:21s remains)
INFO - root - 2017-12-05 16:35:39.059675: step 25770, loss = 2.03, batch loss = 1.97 (8.0 examples/sec; 1.000 sec/batch; 85h:12m:55s remains)
INFO - root - 2017-12-05 16:35:47.604126: step 25780, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 73h:30m:31s remains)
INFO - root - 2017-12-05 16:35:56.164422: step 25790, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 74h:05m:33s remains)
INFO - root - 2017-12-05 16:36:04.693099: step 25800, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 71h:38m:21s remains)
2017-12-05 16:36:05.441607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1634035 -4.1466646 -4.1298771 -4.1263337 -4.1372056 -4.1634965 -4.1949816 -4.2137604 -4.2122126 -4.1975126 -4.18241 -4.1688628 -4.1513691 -4.1346951 -4.1251473][-4.16259 -4.1403275 -4.1183004 -4.1065049 -4.1121793 -4.1404424 -4.1730785 -4.1884165 -4.1827 -4.1692472 -4.1596289 -4.1502 -4.1333375 -4.1122251 -4.10017][-4.171515 -4.142633 -4.111433 -4.0906992 -4.0975065 -4.1289306 -4.1574006 -4.1649523 -4.1563358 -4.15009 -4.1514835 -4.1485353 -4.1294065 -4.0979466 -4.0755925][-4.157733 -4.1214104 -4.0859442 -4.0704222 -4.08953 -4.1241097 -4.1436687 -4.1395802 -4.12683 -4.1295261 -4.1457644 -4.1500173 -4.1225605 -4.0748158 -4.0391269][-4.1249833 -4.0934868 -4.07058 -4.0712366 -4.0991116 -4.1263776 -4.1281834 -4.1004586 -4.0729322 -4.0808363 -4.1156139 -4.131043 -4.0970044 -4.0385942 -4.0036745][-4.1046968 -4.0917788 -4.0892043 -4.0991735 -4.1177444 -4.1240139 -4.0961113 -4.0319815 -3.9809241 -3.9999847 -4.0562725 -4.0866408 -4.0602655 -4.0145187 -4.0004988][-4.1029048 -4.1092482 -4.1178451 -4.1228628 -4.120348 -4.0979042 -4.0348129 -3.9265108 -3.8614998 -3.9199595 -4.0115318 -4.0645871 -4.0621967 -4.041934 -4.0440164][-4.0961337 -4.1135397 -4.1259289 -4.1257315 -4.1105747 -4.0670457 -3.982419 -3.8621283 -3.8256435 -3.9252982 -4.0353942 -4.0978017 -4.1072569 -4.0936608 -4.0910244][-4.093225 -4.1182117 -4.1320996 -4.1298838 -4.1106033 -4.0665007 -3.9971566 -3.9183612 -3.9220214 -4.009316 -4.0947175 -4.1414657 -4.1467128 -4.1299076 -4.1158795][-4.11614 -4.142303 -4.1519694 -4.1414428 -4.1194048 -4.08214 -4.0342307 -3.9957759 -4.0207529 -4.0844464 -4.1400051 -4.1656828 -4.1646338 -4.1443276 -4.1174855][-4.1491041 -4.1634369 -4.1573172 -4.1363554 -4.1166162 -4.088129 -4.0601091 -4.0529208 -4.0905876 -4.1393862 -4.1720767 -4.1827803 -4.1758509 -4.1507773 -4.1143947][-4.1697087 -4.1703205 -4.1513672 -4.1268845 -4.111299 -4.0933695 -4.0868354 -4.10699 -4.1501069 -4.1903324 -4.2108874 -4.2133913 -4.203104 -4.1750708 -4.1387649][-4.1879778 -4.1801367 -4.1577387 -4.137969 -4.1269493 -4.1196032 -4.1290455 -4.16435 -4.2037473 -4.2325954 -4.2442307 -4.2450309 -4.2359033 -4.2095447 -4.1791725][-4.2178812 -4.208499 -4.1870875 -4.1725111 -4.1669292 -4.16531 -4.1779938 -4.2122035 -4.2429876 -4.2623053 -4.2678752 -4.266016 -4.2581987 -4.2365923 -4.2121449][-4.2562976 -4.24964 -4.2324362 -4.2208805 -4.2203693 -4.2246337 -4.2361264 -4.2590179 -4.2768755 -4.2858715 -4.2865071 -4.2819815 -4.2721148 -4.2539058 -4.2350788]]...]
INFO - root - 2017-12-05 16:36:14.047041: step 25810, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.843 sec/batch; 71h:51m:32s remains)
INFO - root - 2017-12-05 16:36:22.649839: step 25820, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 72h:16m:38s remains)
INFO - root - 2017-12-05 16:36:31.094730: step 25830, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 71h:12m:30s remains)
INFO - root - 2017-12-05 16:36:39.636219: step 25840, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 70h:47m:04s remains)
INFO - root - 2017-12-05 16:36:48.316545: step 25850, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 74h:37m:13s remains)
INFO - root - 2017-12-05 16:36:56.692792: step 25860, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 72h:07m:21s remains)
INFO - root - 2017-12-05 16:37:05.291266: step 25870, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 74h:31m:56s remains)
INFO - root - 2017-12-05 16:37:13.785163: step 25880, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 72h:55m:02s remains)
INFO - root - 2017-12-05 16:37:22.367427: step 25890, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 74h:36m:13s remains)
INFO - root - 2017-12-05 16:37:31.132674: step 25900, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 71h:55m:05s remains)
2017-12-05 16:37:31.890476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3468995 -4.3376431 -4.3286552 -4.3219156 -4.3186092 -4.3162575 -4.31447 -4.3149643 -4.3199325 -4.321516 -4.3232965 -4.3323331 -4.3363051 -4.3415432 -4.3505006][-4.327579 -4.3108006 -4.2935181 -4.2831917 -4.2749786 -4.2667618 -4.2632236 -4.2662568 -4.2731347 -4.2756324 -4.2817836 -4.2978525 -4.3016605 -4.3085093 -4.321393][-4.31429 -4.2894793 -4.2648935 -4.2482395 -4.2307916 -4.2155266 -4.2113247 -4.2153349 -4.2219887 -4.2265759 -4.2354851 -4.2561722 -4.2594304 -4.2697172 -4.28846][-4.3087115 -4.2783351 -4.250248 -4.2270808 -4.1991706 -4.1785278 -4.1751184 -4.1795835 -4.185071 -4.184896 -4.1872015 -4.2054448 -4.2085528 -4.2228942 -4.248455][-4.3097425 -4.2799149 -4.2531142 -4.2258315 -4.1928596 -4.1691155 -4.1578121 -4.151041 -4.1467061 -4.1320925 -4.1239371 -4.1386433 -4.1429377 -4.1644039 -4.1995597][-4.3124576 -4.2858911 -4.2616906 -4.2330613 -4.1979012 -4.1664414 -4.13578 -4.1076388 -4.0868049 -4.0551882 -4.0399184 -4.0561361 -4.0641327 -4.0995345 -4.1488948][-4.3057818 -4.2778511 -4.2512593 -4.2170773 -4.1728945 -4.1275496 -4.074574 -4.02521 -3.9920835 -3.9503093 -3.9333603 -3.9570405 -3.9740727 -4.0315351 -4.1019316][-4.2781177 -4.2374272 -4.1995921 -4.1532331 -4.0953112 -4.0347223 -3.966795 -3.9071913 -3.8706622 -3.8349009 -3.8277159 -3.8649883 -3.8997719 -3.9833972 -4.0754061][-4.24092 -4.1827617 -4.131444 -4.0748105 -4.0071316 -3.9386072 -3.8690686 -3.8199489 -3.79898 -3.7890587 -3.8057261 -3.8605618 -3.9130528 -4.0091925 -4.1044855][-4.237031 -4.1760516 -4.1250229 -4.0750618 -4.0194516 -3.9682405 -3.9229007 -3.9008987 -3.8994 -3.9082046 -3.9365108 -3.9903769 -4.0423975 -4.1214247 -4.1939311][-4.2783709 -4.2355714 -4.2043972 -4.177186 -4.147079 -4.1226144 -4.1036797 -4.0980992 -4.102572 -4.1128907 -4.1337996 -4.1687407 -4.2043567 -4.25273 -4.2939224][-4.3296747 -4.3098106 -4.299798 -4.2935915 -4.2844858 -4.2768989 -4.2706237 -4.2689891 -4.2722845 -4.2788334 -4.2894011 -4.3059745 -4.3239365 -4.3443537 -4.3595176][-4.3595657 -4.3517737 -4.3497415 -4.3507648 -4.3507171 -4.3510766 -4.3513336 -4.3529582 -4.3565269 -4.3596096 -4.3637772 -4.3697352 -4.3761635 -4.3814197 -4.3842187][-4.3709507 -4.3669004 -4.3660913 -4.3678517 -4.3693337 -4.371223 -4.3723326 -4.3736482 -4.3763809 -4.3779445 -4.3800964 -4.3826194 -4.3850436 -4.3863993 -4.3866396][-4.3762794 -4.3739543 -4.3738942 -4.3748107 -4.3753152 -4.3763022 -4.3768997 -4.3777075 -4.3787241 -4.3793902 -4.3800979 -4.38066 -4.3810182 -4.3810859 -4.3812909]]...]
INFO - root - 2017-12-05 16:37:40.522005: step 25910, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 73h:03m:27s remains)
INFO - root - 2017-12-05 16:37:48.955592: step 25920, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 72h:22m:56s remains)
INFO - root - 2017-12-05 16:37:57.411131: step 25930, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 73h:44m:29s remains)
INFO - root - 2017-12-05 16:38:05.958432: step 25940, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 75h:06m:26s remains)
INFO - root - 2017-12-05 16:38:14.535660: step 25950, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 74h:58m:58s remains)
INFO - root - 2017-12-05 16:38:23.137342: step 25960, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 71h:07m:05s remains)
INFO - root - 2017-12-05 16:38:31.607170: step 25970, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 70h:06m:33s remains)
INFO - root - 2017-12-05 16:38:40.112970: step 25980, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 70h:50m:22s remains)
INFO - root - 2017-12-05 16:38:48.545485: step 25990, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 72h:21m:01s remains)
INFO - root - 2017-12-05 16:38:57.226497: step 26000, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 73h:35m:31s remains)
2017-12-05 16:38:57.991591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1704245 -4.1844811 -4.1776023 -4.1658568 -4.1538906 -4.1482186 -4.149056 -4.1502557 -4.1532855 -4.160687 -4.16656 -4.15668 -4.1352611 -4.1192355 -4.1248894][-4.2314405 -4.241477 -4.237555 -4.2317333 -4.2237811 -4.2183719 -4.2154365 -4.20878 -4.2037592 -4.2077036 -4.2138033 -4.2059956 -4.1885562 -4.174819 -4.1761231][-4.280714 -4.2840958 -4.2804151 -4.2780867 -4.276093 -4.2753968 -4.2717419 -4.2636695 -4.2567477 -4.2590156 -4.2659216 -4.2627439 -4.25075 -4.2418885 -4.2416873][-4.3029943 -4.2962227 -4.2876015 -4.2837777 -4.2842145 -4.2858377 -4.283865 -4.2793431 -4.2760868 -4.2817783 -4.2936277 -4.2974396 -4.2918181 -4.2881985 -4.2875943][-4.2914124 -4.2694755 -4.2485189 -4.235693 -4.2278919 -4.2249913 -4.2253151 -4.2293205 -4.2387848 -4.2578487 -4.2814689 -4.29504 -4.2962503 -4.2944942 -4.2907443][-4.2675238 -4.2324624 -4.1969204 -4.1668029 -4.1370115 -4.1153383 -4.1098146 -4.1224508 -4.1517944 -4.1924787 -4.2322006 -4.258553 -4.2704921 -4.2738714 -4.2694654][-4.2434616 -4.200263 -4.1543756 -4.1080561 -4.0544157 -4.0078716 -3.9884973 -4.0031476 -4.0479 -4.1055145 -4.1587687 -4.2022133 -4.2294703 -4.2415819 -4.2425][-4.2352571 -4.1886926 -4.1421671 -4.0968747 -4.0380735 -3.9792247 -3.9487784 -3.9548037 -3.9936192 -4.0524817 -4.1073685 -4.157536 -4.19443 -4.2138762 -4.2254224][-4.2550097 -4.2154503 -4.1799879 -4.149909 -4.1090436 -4.0655551 -4.0431557 -4.040658 -4.0541282 -4.0898643 -4.1234007 -4.1576533 -4.1841645 -4.2023206 -4.2207146][-4.2870116 -4.2603927 -4.237556 -4.221077 -4.1978755 -4.1772609 -4.1686754 -4.1656494 -4.164495 -4.1775823 -4.1874824 -4.1952581 -4.1996617 -4.2076054 -4.2287693][-4.3080788 -4.2951593 -4.2832584 -4.2752028 -4.2629819 -4.2546644 -4.2510624 -4.2486191 -4.2447052 -4.2497525 -4.2457762 -4.233295 -4.2201972 -4.218626 -4.2405658][-4.3182421 -4.3163157 -4.3122983 -4.3073092 -4.3001685 -4.2953033 -4.2938519 -4.2920446 -4.2867537 -4.2906895 -4.2815251 -4.2587166 -4.2372365 -4.2327695 -4.2542052][-4.3191319 -4.3251305 -4.3268619 -4.325882 -4.3235216 -4.3213682 -4.3218565 -4.3202472 -4.3163323 -4.3177986 -4.3038373 -4.2758636 -4.2523804 -4.2482085 -4.2685337][-4.2986889 -4.3090115 -4.315567 -4.3216252 -4.3269696 -4.3294158 -4.3285022 -4.3236928 -4.3188186 -4.3158488 -4.3009729 -4.2755823 -4.2576332 -4.2582965 -4.2778249][-4.2659736 -4.2765269 -4.2858925 -4.2936773 -4.3011045 -4.3055491 -4.3054872 -4.2996893 -4.2937946 -4.2912908 -4.282196 -4.265027 -4.2544827 -4.2588277 -4.2762713]]...]
INFO - root - 2017-12-05 16:39:06.621783: step 26010, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 73h:12m:04s remains)
INFO - root - 2017-12-05 16:39:15.111198: step 26020, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 72h:27m:28s remains)
INFO - root - 2017-12-05 16:39:23.589174: step 26030, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.860 sec/batch; 73h:14m:55s remains)
INFO - root - 2017-12-05 16:39:32.412622: step 26040, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 73h:48m:04s remains)
INFO - root - 2017-12-05 16:39:41.069071: step 26050, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 72h:24m:20s remains)
INFO - root - 2017-12-05 16:39:49.696242: step 26060, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 73h:01m:24s remains)
INFO - root - 2017-12-05 16:39:58.279151: step 26070, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 0.777 sec/batch; 66h:09m:18s remains)
INFO - root - 2017-12-05 16:40:06.766764: step 26080, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.836 sec/batch; 71h:10m:40s remains)
INFO - root - 2017-12-05 16:40:15.396280: step 26090, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 70h:39m:03s remains)
INFO - root - 2017-12-05 16:40:23.849090: step 26100, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 75h:28m:11s remains)
2017-12-05 16:40:24.621512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2712297 -4.2678795 -4.2657542 -4.2732978 -4.2741566 -4.2578869 -4.231492 -4.2117481 -4.2002983 -4.2002087 -4.2116842 -4.227078 -4.2334189 -4.2173772 -4.1853366][-4.2769427 -4.2716112 -4.2636156 -4.2694306 -4.26888 -4.2469382 -4.2166553 -4.1940126 -4.1846652 -4.1933956 -4.2154293 -4.2377172 -4.2463264 -4.2279425 -4.1962934][-4.2726488 -4.2654252 -4.2563515 -4.2621789 -4.2586541 -4.2320971 -4.2008781 -4.1817346 -4.1828895 -4.2032819 -4.2306933 -4.2538042 -4.2612996 -4.242044 -4.2123594][-4.2653027 -4.25376 -4.2430396 -4.2463675 -4.2354836 -4.2018266 -4.1671796 -4.1573224 -4.1785836 -4.2157111 -4.2465525 -4.2660952 -4.2749996 -4.2577009 -4.225913][-4.2481661 -4.2339926 -4.2221613 -4.224401 -4.2046032 -4.1602817 -4.1163321 -4.1127315 -4.1602759 -4.2131553 -4.2462311 -4.2647448 -4.2786264 -4.2652311 -4.2305546][-4.2160416 -4.2002392 -4.192028 -4.1951203 -4.1688938 -4.1127996 -4.054245 -4.0551853 -4.1287937 -4.1962185 -4.2306352 -4.2480474 -4.2634034 -4.2508936 -4.2147946][-4.187768 -4.1738567 -4.1649632 -4.160358 -4.1260052 -4.0576367 -3.9854574 -4.0021868 -4.0994573 -4.1779089 -4.2147417 -4.2334723 -4.2445765 -4.2284336 -4.190135][-4.1878233 -4.1734667 -4.1566844 -4.1375203 -4.0932517 -4.013926 -3.933713 -3.9682267 -4.0772715 -4.1608381 -4.203464 -4.2247195 -4.2314019 -4.2092257 -4.1672478][-4.2025061 -4.1835117 -4.1614037 -4.1324458 -4.0819435 -4.0041056 -3.9346452 -3.9818118 -4.0828958 -4.1593094 -4.2052579 -4.2278628 -4.2342157 -4.2078609 -4.1630049][-4.2217031 -4.2020583 -4.1832819 -4.1554356 -4.1074553 -4.0406551 -3.9933734 -4.0452375 -4.1214428 -4.1799431 -4.2177043 -4.2384176 -4.2409487 -4.2155209 -4.1710782][-4.2496567 -4.2327433 -4.2173409 -4.1918235 -4.1525006 -4.0988083 -4.0688128 -4.1114044 -4.1655211 -4.2113523 -4.2396021 -4.2565336 -4.2538633 -4.2254786 -4.1772728][-4.2721753 -4.2587533 -4.2450814 -4.2216268 -4.1848154 -4.1370363 -4.1143632 -4.1514249 -4.1993604 -4.2388811 -4.2565203 -4.2671919 -4.2620797 -4.2327614 -4.1833844][-4.2814412 -4.2688122 -4.2568893 -4.2367492 -4.1995678 -4.1516547 -4.1337543 -4.1733561 -4.2251725 -4.2606368 -4.2699876 -4.2727423 -4.2645235 -4.236486 -4.1919026][-4.28591 -4.270968 -4.2576065 -4.2403278 -4.2068372 -4.1628313 -4.1508584 -4.1881571 -4.2389355 -4.2711964 -4.2757354 -4.2732134 -4.2621508 -4.2355576 -4.1936522][-4.2886982 -4.2736416 -4.2585387 -4.2421455 -4.2135458 -4.17852 -4.1699033 -4.2005816 -4.2449484 -4.2702641 -4.2715564 -4.2661915 -4.2548041 -4.2293158 -4.1875577]]...]
INFO - root - 2017-12-05 16:40:33.091937: step 26110, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 73h:31m:27s remains)
INFO - root - 2017-12-05 16:40:41.703517: step 26120, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 72h:55m:12s remains)
INFO - root - 2017-12-05 16:40:50.207756: step 26130, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 73h:15m:38s remains)
INFO - root - 2017-12-05 16:40:58.678860: step 26140, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 71h:28m:52s remains)
INFO - root - 2017-12-05 16:41:07.119463: step 26150, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 73h:48m:09s remains)
INFO - root - 2017-12-05 16:41:15.647997: step 26160, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 75h:06m:09s remains)
INFO - root - 2017-12-05 16:41:24.072807: step 26170, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 74h:53m:18s remains)
INFO - root - 2017-12-05 16:41:32.586855: step 26180, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 72h:47m:24s remains)
INFO - root - 2017-12-05 16:41:41.144098: step 26190, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 71h:50m:20s remains)
INFO - root - 2017-12-05 16:41:49.610006: step 26200, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 0.808 sec/batch; 68h:46m:42s remains)
2017-12-05 16:41:50.367410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.250062 -4.2697206 -4.2766027 -4.2702808 -4.2623835 -4.2594719 -4.2643881 -4.2722735 -4.2750468 -4.2762628 -4.2759657 -4.273057 -4.2669125 -4.2609625 -4.2579088][-4.2361383 -4.259644 -4.2637215 -4.2494187 -4.2359247 -4.2303305 -4.2364559 -4.246666 -4.2495065 -4.2515993 -4.2539506 -4.2510934 -4.2458739 -4.2389054 -4.2344508][-4.220839 -4.2454524 -4.2482338 -4.229465 -4.2102423 -4.1973634 -4.1984472 -4.2080874 -4.2128124 -4.2181082 -4.2254848 -4.2258773 -4.2248387 -4.2176929 -4.2123723][-4.1941137 -4.221314 -4.2240229 -4.2029867 -4.17919 -4.1579494 -4.1503797 -4.1541338 -4.1632161 -4.1794157 -4.1961784 -4.2036743 -4.2073054 -4.1982679 -4.1870513][-4.1570392 -4.1848288 -4.1895742 -4.171217 -4.1434407 -4.1095734 -4.0848031 -4.082746 -4.1065559 -4.1429086 -4.1725597 -4.1887875 -4.1966734 -4.1861339 -4.1653428][-4.12109 -4.1384854 -4.1381497 -4.1180997 -4.0796313 -4.019475 -3.9638779 -3.9652929 -4.025032 -4.0919924 -4.1383419 -4.1644449 -4.1791019 -4.16827 -4.1343293][-4.1104527 -4.1099977 -4.0957522 -4.0653806 -4.0063591 -3.910466 -3.8121481 -3.8248811 -3.9332819 -4.0302367 -4.0873737 -4.1180611 -4.1404762 -4.131547 -4.0826864][-4.1263943 -4.1195469 -4.0976548 -4.0636353 -3.9969347 -3.8920367 -3.789135 -3.8089783 -3.9245348 -4.0174809 -4.0651684 -4.088717 -4.1094422 -4.09623 -4.0333104][-4.1439533 -4.1417069 -4.1233778 -4.0976858 -4.05245 -3.9830587 -3.9237652 -3.937875 -4.0080981 -4.0643249 -4.0907531 -4.1008983 -4.1124725 -4.0922022 -4.0252461][-4.1409311 -4.1413884 -4.1297793 -4.1183176 -4.1027288 -4.073113 -4.0533829 -4.0644703 -4.0998254 -4.1255732 -4.1350017 -4.1346974 -4.1393938 -4.1218295 -4.0710688][-4.1240835 -4.1239882 -4.11672 -4.1226377 -4.1327558 -4.1275663 -4.1276379 -4.1390505 -4.1652579 -4.1835127 -4.1874318 -4.1803374 -4.1802311 -4.1680737 -4.1371255][-4.1150522 -4.1162672 -4.1098137 -4.1241775 -4.1470037 -4.1572623 -4.1679134 -4.1803188 -4.2060809 -4.2271056 -4.2310338 -4.2238984 -4.2191477 -4.2104096 -4.1932068][-4.12012 -4.119019 -4.1074834 -4.1176963 -4.1437559 -4.167645 -4.1882148 -4.2053838 -4.2362695 -4.2606897 -4.26336 -4.25679 -4.2500639 -4.2422433 -4.2329621][-4.1417122 -4.13646 -4.1208396 -4.1232677 -4.1440921 -4.1713963 -4.1983767 -4.2243743 -4.2604141 -4.2887893 -4.2924547 -4.2863674 -4.2794185 -4.2704196 -4.2632565][-4.1608019 -4.15489 -4.1433935 -4.1462207 -4.1606903 -4.1868978 -4.2180791 -4.2491951 -4.2888179 -4.318922 -4.3242688 -4.317503 -4.3086452 -4.3007913 -4.2967429]]...]
INFO - root - 2017-12-05 16:41:58.874963: step 26210, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.858 sec/batch; 72h:58m:34s remains)
INFO - root - 2017-12-05 16:42:07.480519: step 26220, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 71h:33m:20s remains)
INFO - root - 2017-12-05 16:42:15.972593: step 26230, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.856 sec/batch; 72h:47m:11s remains)
INFO - root - 2017-12-05 16:42:24.568309: step 26240, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 73h:49m:14s remains)
INFO - root - 2017-12-05 16:42:33.120852: step 26250, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 72h:44m:38s remains)
INFO - root - 2017-12-05 16:42:41.785900: step 26260, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.837 sec/batch; 71h:13m:59s remains)
INFO - root - 2017-12-05 16:42:50.462906: step 26270, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 72h:08m:22s remains)
INFO - root - 2017-12-05 16:42:59.135193: step 26280, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 70h:00m:22s remains)
INFO - root - 2017-12-05 16:43:07.553214: step 26290, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 73h:10m:27s remains)
INFO - root - 2017-12-05 16:43:16.123375: step 26300, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 75h:07m:08s remains)
2017-12-05 16:43:16.885499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1092138 -4.1431122 -4.1699176 -4.1733303 -4.164258 -4.1469603 -4.1196804 -4.1014838 -4.1086144 -4.134304 -4.1625223 -4.1788244 -4.1750436 -4.1624618 -4.1388535][-4.0979939 -4.1309605 -4.1584234 -4.164556 -4.1593595 -4.14759 -4.1268706 -4.1097713 -4.1171818 -4.1468115 -4.1811581 -4.2000465 -4.1948581 -4.1843462 -4.1652994][-4.099051 -4.1311407 -4.1606894 -4.16715 -4.1600218 -4.1466551 -4.1258039 -4.1089907 -4.1167841 -4.1491137 -4.1909256 -4.2177982 -4.2161527 -4.2097449 -4.19726][-4.1263885 -4.1617079 -4.1907287 -4.1918287 -4.17594 -4.1524577 -4.1227093 -4.1051183 -4.1099467 -4.1406684 -4.1915655 -4.2280006 -4.2336168 -4.2354665 -4.2342434][-4.1836071 -4.2142038 -4.2302666 -4.2146754 -4.1840358 -4.1476603 -4.1026783 -4.0697622 -4.0655832 -4.101687 -4.1610279 -4.211607 -4.2306838 -4.2468653 -4.2570524][-4.2401938 -4.2588754 -4.2552357 -4.2181954 -4.16895 -4.112587 -4.0435591 -3.9811542 -3.9628177 -4.0131693 -4.0883303 -4.1529589 -4.18518 -4.2164626 -4.2392011][-4.2581606 -4.2685761 -4.2517657 -4.2046304 -4.1427183 -4.0735435 -3.9855146 -3.8942978 -3.8604331 -3.9175062 -4.0035963 -4.0777707 -4.1225181 -4.1623459 -4.1930089][-4.250463 -4.2580838 -4.2378068 -4.193543 -4.1355338 -4.0654812 -3.9782445 -3.8766241 -3.8294525 -3.8849142 -3.9682779 -4.0346794 -4.0827589 -4.1219363 -4.1515203][-4.2385764 -4.2440028 -4.2215753 -4.1806879 -4.1256604 -4.0648661 -3.9941645 -3.9034922 -3.8626163 -3.9258676 -4.0044508 -4.0521779 -4.09504 -4.1200709 -4.1398368][-4.2186193 -4.2227817 -4.2013712 -4.166182 -4.1169333 -4.0642567 -4.0127959 -3.9472256 -3.9257343 -3.9932473 -4.0564766 -4.0834303 -4.1122713 -4.1230359 -4.1361847][-4.1969881 -4.2038441 -4.1889229 -4.1626029 -4.1245108 -4.0810242 -4.0484881 -4.0103378 -4.0020404 -4.0554137 -4.092855 -4.1012764 -4.1189041 -4.123363 -4.1361756][-4.1899567 -4.198926 -4.192575 -4.1788011 -4.1543121 -4.1190877 -4.0957193 -4.0755816 -4.0729418 -4.1010838 -4.1148763 -4.1127129 -4.1277237 -4.13871 -4.1563525][-4.1957073 -4.2065992 -4.2067018 -4.2034082 -4.1883287 -4.1589828 -4.1393356 -4.1273637 -4.1211214 -4.1257 -4.1241951 -4.1251259 -4.1443892 -4.1655579 -4.1865983][-4.2040596 -4.2112918 -4.2126536 -4.2104807 -4.1957989 -4.1690321 -4.1508951 -4.1443214 -4.1365228 -4.1270871 -4.1220541 -4.1368475 -4.1638932 -4.1886554 -4.2106824][-4.2223907 -4.2194433 -4.2129779 -4.2080245 -4.191999 -4.1647444 -4.1403794 -4.132556 -4.1295867 -4.129355 -4.1406407 -4.1702223 -4.1973372 -4.2153292 -4.2296433]]...]
INFO - root - 2017-12-05 16:43:25.456417: step 26310, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.878 sec/batch; 74h:38m:22s remains)
INFO - root - 2017-12-05 16:43:33.940965: step 26320, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 73h:46m:03s remains)
INFO - root - 2017-12-05 16:43:42.508351: step 26330, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 71h:31m:06s remains)
INFO - root - 2017-12-05 16:43:51.010070: step 26340, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 73h:28m:42s remains)
INFO - root - 2017-12-05 16:43:59.437113: step 26350, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.814 sec/batch; 69h:15m:16s remains)
INFO - root - 2017-12-05 16:44:08.092140: step 26360, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 72h:20m:43s remains)
INFO - root - 2017-12-05 16:44:16.608282: step 26370, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 70h:45m:29s remains)
INFO - root - 2017-12-05 16:44:25.169113: step 26380, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 71h:27m:11s remains)
INFO - root - 2017-12-05 16:44:33.580533: step 26390, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.781 sec/batch; 66h:22m:24s remains)
INFO - root - 2017-12-05 16:44:42.027647: step 26400, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 69h:43m:14s remains)
2017-12-05 16:44:42.732745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31801 -4.3139825 -4.3073821 -4.3014235 -4.3023324 -4.3137274 -4.3291373 -4.3397317 -4.3451815 -4.3465075 -4.341054 -4.3321323 -4.3228812 -4.3165469 -4.3151808][-4.3108006 -4.2969584 -4.2815042 -4.2712903 -4.2732325 -4.2899208 -4.3143644 -4.3334484 -4.3449864 -4.3508487 -4.3477807 -4.3400273 -4.3320284 -4.3261089 -4.3249083][-4.3001146 -4.2715597 -4.2409973 -4.2211862 -4.2215762 -4.2428713 -4.2767129 -4.3067341 -4.3295994 -4.3449779 -4.3484769 -4.3445735 -4.3386493 -4.3333421 -4.3318238][-4.2928929 -4.2500467 -4.2002015 -4.1645451 -4.1569881 -4.1783562 -4.2152958 -4.2544379 -4.2937632 -4.3252606 -4.3423624 -4.3483038 -4.3481584 -4.3450208 -4.3421941][-4.2925353 -4.2425838 -4.1776066 -4.1229177 -4.0984616 -4.1074862 -4.1355743 -4.1780372 -4.2351856 -4.2870884 -4.321661 -4.3421035 -4.3528996 -4.3561831 -4.3542571][-4.2970037 -4.249989 -4.1808929 -4.1113176 -4.0635939 -4.0437846 -4.0462174 -4.0840726 -4.1603684 -4.2352729 -4.2876525 -4.3220983 -4.3452806 -4.3576889 -4.3600373][-4.3039293 -4.2684803 -4.2077618 -4.1341448 -4.0647287 -4.0062304 -3.9660251 -3.9865906 -4.0760574 -4.1720018 -4.2406759 -4.2893949 -4.3253927 -4.3478017 -4.3568492][-4.3114963 -4.2904186 -4.2447982 -4.1794806 -4.101738 -4.0144048 -3.9333577 -3.9246941 -4.0081811 -4.1092639 -4.18618 -4.2476668 -4.2968774 -4.3299985 -4.3470607][-4.3167028 -4.30807 -4.2792182 -4.2311835 -4.1620665 -4.069489 -3.9731927 -3.937341 -3.9887 -4.0696144 -4.1397538 -4.2055345 -4.263494 -4.3066621 -4.3327589][-4.3174958 -4.316987 -4.3030224 -4.2743974 -4.2250171 -4.1480823 -4.0603585 -4.0106659 -4.0242038 -4.0710635 -4.1213431 -4.1785464 -4.2347584 -4.2826157 -4.3160858][-4.3167396 -4.31982 -4.31524 -4.30235 -4.27383 -4.2218394 -4.1552711 -4.10531 -4.0917325 -4.1055927 -4.1312952 -4.1702681 -4.216116 -4.2620854 -4.2983465][-4.3160543 -4.32018 -4.3205552 -4.317492 -4.3059788 -4.2782707 -4.2366686 -4.1969242 -4.1721625 -4.1649356 -4.1687841 -4.1852541 -4.2135944 -4.2511168 -4.2845755][-4.31868 -4.3233242 -4.3237267 -4.3231487 -4.321404 -4.3123097 -4.2933927 -4.2700834 -4.2490907 -4.2351885 -4.2264643 -4.2241793 -4.2343874 -4.2580953 -4.2826838][-4.3230391 -4.3299069 -4.3291636 -4.3264022 -4.3265224 -4.3260365 -4.3213444 -4.3128271 -4.302845 -4.293499 -4.2831674 -4.2727675 -4.2714667 -4.2817764 -4.294591][-4.3219204 -4.331449 -4.330349 -4.3258429 -4.3245492 -4.3249822 -4.3259192 -4.3261576 -4.3252034 -4.3228421 -4.3174191 -4.3094034 -4.3056707 -4.3079648 -4.3115859]]...]
INFO - root - 2017-12-05 16:44:51.329191: step 26410, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 70h:28m:29s remains)
INFO - root - 2017-12-05 16:44:59.870500: step 26420, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 71h:05m:09s remains)
INFO - root - 2017-12-05 16:45:08.258833: step 26430, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 73h:40m:52s remains)
INFO - root - 2017-12-05 16:45:16.707120: step 26440, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 74h:05m:09s remains)
INFO - root - 2017-12-05 16:45:25.239446: step 26450, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 72h:35m:00s remains)
INFO - root - 2017-12-05 16:45:33.800902: step 26460, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 74h:30m:57s remains)
INFO - root - 2017-12-05 16:45:42.445548: step 26470, loss = 2.03, batch loss = 1.98 (9.1 examples/sec; 0.881 sec/batch; 74h:51m:16s remains)
INFO - root - 2017-12-05 16:45:51.032642: step 26480, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 74h:18m:46s remains)
INFO - root - 2017-12-05 16:45:59.603698: step 26490, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 73h:02m:36s remains)
INFO - root - 2017-12-05 16:46:07.863746: step 26500, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 72h:58m:32s remains)
2017-12-05 16:46:08.671841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2799788 -4.2750869 -4.25977 -4.2288756 -4.2039938 -4.1893077 -4.1755672 -4.1587625 -4.1573949 -4.1814122 -4.2258468 -4.2704091 -4.2964225 -4.29856 -4.2805014][-4.2960124 -4.2880754 -4.2627163 -4.2223458 -4.1881738 -4.1737061 -4.1687241 -4.1653113 -4.1784325 -4.2054515 -4.245265 -4.2869043 -4.309608 -4.30805 -4.2819462][-4.2970147 -4.2820864 -4.2452893 -4.1985192 -4.1635103 -4.1590443 -4.1666274 -4.1733365 -4.1840444 -4.1986403 -4.2339964 -4.2764816 -4.2984986 -4.2936211 -4.2604146][-4.2837567 -4.260006 -4.2117443 -4.163619 -4.1422935 -4.1493831 -4.1572113 -4.1544433 -4.1445189 -4.1482582 -4.1906228 -4.241353 -4.268435 -4.259017 -4.2175441][-4.2648921 -4.2393804 -4.1878562 -4.1462026 -4.1384196 -4.139833 -4.1242385 -4.0853133 -4.0432467 -4.0499883 -4.1214566 -4.1874256 -4.2150369 -4.1976156 -4.1538129][-4.2488441 -4.2281761 -4.1868639 -4.1516128 -4.1368365 -4.1089096 -4.0437713 -3.9411945 -3.8689809 -3.9194293 -4.0309324 -4.1061697 -4.1299868 -4.114212 -4.086534][-4.2246852 -4.2112069 -4.1807175 -4.1456661 -4.1121516 -4.0472951 -3.9319222 -3.7883446 -3.7451267 -3.8701787 -4.0102487 -4.0941453 -4.1267624 -4.1299477 -4.1279306][-4.1918635 -4.1837316 -4.167788 -4.1476126 -4.115252 -4.0535197 -3.9580507 -3.8706574 -3.8922002 -4.010993 -4.1227512 -4.1847253 -4.2049575 -4.2053595 -4.203095][-4.18887 -4.1872244 -4.1911063 -4.1895843 -4.1688862 -4.1274848 -4.0757565 -4.049644 -4.0837312 -4.1552243 -4.2165494 -4.2492561 -4.2594476 -4.2572207 -4.2492266][-4.20275 -4.207653 -4.221035 -4.2257547 -4.2168837 -4.1943536 -4.1668315 -4.1569142 -4.1758895 -4.2109776 -4.2425275 -4.2626457 -4.2744403 -4.2786245 -4.272264][-4.206089 -4.2148366 -4.2313218 -4.23698 -4.2310786 -4.2145758 -4.1919484 -4.1811595 -4.1918888 -4.2143912 -4.23373 -4.2480383 -4.263155 -4.2763996 -4.2786832][-4.1964455 -4.207613 -4.2238197 -4.2279029 -4.2225351 -4.2013431 -4.1756821 -4.1712461 -4.1858058 -4.2079778 -4.2229891 -4.2375984 -4.2568989 -4.2760568 -4.2823715][-4.2004743 -4.2077608 -4.2195024 -4.2211504 -4.2103176 -4.1840286 -4.16434 -4.1674852 -4.1850247 -4.2053943 -4.2168036 -4.2288814 -4.2482972 -4.2696853 -4.2779918][-4.2235832 -4.2217722 -4.2241049 -4.2188234 -4.2033219 -4.1789913 -4.1683216 -4.179512 -4.19689 -4.2124543 -4.2191691 -4.2273235 -4.2430544 -4.2608194 -4.2681508][-4.2443991 -4.2347908 -4.2293911 -4.2211056 -4.2114892 -4.1958466 -4.1922269 -4.2051044 -4.2158322 -4.2185087 -4.2148223 -4.2144179 -4.2247558 -4.238976 -4.2456574]]...]
INFO - root - 2017-12-05 16:46:17.282129: step 26510, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 73h:20m:38s remains)
INFO - root - 2017-12-05 16:46:25.795051: step 26520, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 74h:11m:59s remains)
INFO - root - 2017-12-05 16:46:34.301554: step 26530, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 72h:09m:23s remains)
INFO - root - 2017-12-05 16:46:42.891981: step 26540, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 72h:36m:34s remains)
INFO - root - 2017-12-05 16:46:51.357854: step 26550, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 72h:35m:47s remains)
INFO - root - 2017-12-05 16:46:59.923461: step 26560, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 71h:56m:06s remains)
INFO - root - 2017-12-05 16:47:08.322391: step 26570, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 69h:28m:14s remains)
INFO - root - 2017-12-05 16:47:16.885193: step 26580, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 72h:10m:59s remains)
INFO - root - 2017-12-05 16:47:25.516255: step 26590, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 72h:46m:58s remains)
INFO - root - 2017-12-05 16:47:34.043695: step 26600, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 72h:50m:03s remains)
2017-12-05 16:47:34.789412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2419624 -4.2320652 -4.2185946 -4.2002273 -4.1841965 -4.1802769 -4.1909604 -4.2142744 -4.228 -4.2254004 -4.2198362 -4.2146683 -4.2096624 -4.2077827 -4.2060676][-4.1984215 -4.1815243 -4.1593947 -4.1281109 -4.1057267 -4.107924 -4.129848 -4.1641655 -4.1815653 -4.179255 -4.1771779 -4.1729479 -4.1623139 -4.1585231 -4.1638927][-4.142673 -4.1189542 -4.0925641 -4.0533528 -4.0290108 -4.0365796 -4.0670137 -4.1050673 -4.1187487 -4.1154952 -4.1178188 -4.1159062 -4.1046023 -4.10638 -4.1246066][-4.0932808 -4.063539 -4.036232 -3.9974387 -3.9717035 -3.976613 -4.0040574 -4.0363522 -4.039453 -4.0356812 -4.0468426 -4.0506907 -4.0456786 -4.0612149 -4.0952215][-4.0789723 -4.0467496 -4.0207987 -3.9840105 -3.9497571 -3.9376454 -3.9445395 -3.9588037 -3.9462962 -3.940351 -3.9612496 -3.9826467 -3.9971826 -4.0335922 -4.0751867][-4.0935378 -4.0622916 -4.0331049 -3.9930987 -3.9465935 -3.9124038 -3.8936937 -3.8748193 -3.842648 -3.8452704 -3.8911259 -3.9359827 -3.970758 -4.0172758 -4.0520549][-4.1119428 -4.0849681 -4.0566583 -4.0151558 -3.9592876 -3.9115915 -3.8667321 -3.8069711 -3.7599871 -3.7892005 -3.8612909 -3.9212041 -3.9615488 -4.0015759 -4.0241237][-4.1260462 -4.1045527 -4.0790844 -4.0348635 -3.98126 -3.930279 -3.8720524 -3.7977045 -3.7604654 -3.8139324 -3.8914604 -3.9478092 -3.9803135 -4.001987 -4.0117712][-4.131146 -4.1133575 -4.0867524 -4.0443025 -3.9993629 -3.9522831 -3.9002066 -3.8602765 -3.8568835 -3.9040337 -3.9510159 -3.9908206 -4.0190086 -4.0334945 -4.0362678][-4.1311631 -4.1099176 -4.0823445 -4.0445275 -4.0037794 -3.9647226 -3.9397266 -3.9471328 -3.9671872 -3.9954324 -4.0119038 -4.032846 -4.0605712 -4.0779676 -4.0785027][-4.1303124 -4.0990853 -4.0653496 -4.0318384 -4.0007639 -3.9745622 -3.9776998 -4.013319 -4.0446048 -4.0600739 -4.0604773 -4.0745344 -4.1003652 -4.1142945 -4.1136694][-4.1295619 -4.0918121 -4.055964 -4.0236325 -3.9973612 -3.9813378 -3.9975603 -4.0395994 -4.0727229 -4.0864544 -4.0890484 -4.1033816 -4.1276679 -4.1395235 -4.1381755][-4.1431456 -4.1073632 -4.0749569 -4.0446472 -4.01669 -4.0010476 -4.01755 -4.0556231 -4.086607 -4.1028991 -4.1153355 -4.1346092 -4.1566572 -4.1626983 -4.1574354][-4.1831594 -4.1550517 -4.1285758 -4.1008673 -4.0734549 -4.0585551 -4.070159 -4.1006002 -4.1278195 -4.1455584 -4.15792 -4.1740751 -4.1925445 -4.1952448 -4.1876359][-4.2387776 -4.2196379 -4.2014451 -4.1821675 -4.1648054 -4.1557713 -4.1619329 -4.1821871 -4.2003851 -4.2110558 -4.2175388 -4.2273126 -4.2394638 -4.24076 -4.2343774]]...]
INFO - root - 2017-12-05 16:47:43.252969: step 26610, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 74h:10m:32s remains)
INFO - root - 2017-12-05 16:47:51.787270: step 26620, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 73h:56m:22s remains)
INFO - root - 2017-12-05 16:48:00.256767: step 26630, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 72h:24m:42s remains)
INFO - root - 2017-12-05 16:48:08.721332: step 26640, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 73h:36m:57s remains)
INFO - root - 2017-12-05 16:48:17.236034: step 26650, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 73h:03m:06s remains)
INFO - root - 2017-12-05 16:48:25.811264: step 26660, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 72h:54m:57s remains)
INFO - root - 2017-12-05 16:48:34.312852: step 26670, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.884 sec/batch; 75h:07m:21s remains)
INFO - root - 2017-12-05 16:48:42.966697: step 26680, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 73h:06m:55s remains)
INFO - root - 2017-12-05 16:48:51.504204: step 26690, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 71h:12m:15s remains)
INFO - root - 2017-12-05 16:49:00.003414: step 26700, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 72h:16m:30s remains)
2017-12-05 16:49:00.767613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2729344 -4.2752438 -4.2732377 -4.2692842 -4.2696095 -4.2745152 -4.2753487 -4.2703071 -4.2683616 -4.2709689 -4.2731023 -4.2717586 -4.2657871 -4.2539949 -4.2473955][-4.2543902 -4.259346 -4.2576284 -4.2513528 -4.2498779 -4.254117 -4.2512465 -4.2378664 -4.22914 -4.2329826 -4.2413611 -4.2430558 -4.2365136 -4.2240977 -4.22182][-4.2298069 -4.2379251 -4.2383361 -4.2302585 -4.2239447 -4.2247 -4.2123461 -4.1857996 -4.1695843 -4.1781788 -4.1982985 -4.2059469 -4.2022734 -4.1964149 -4.2024231][-4.2065253 -4.21535 -4.2121921 -4.2012796 -4.1914225 -4.1846266 -4.1611366 -4.1228762 -4.1033225 -4.1215377 -4.1567941 -4.1744437 -4.1744885 -4.1786017 -4.1918154][-4.184576 -4.1896272 -4.1809874 -4.1680541 -4.15285 -4.1324453 -4.0908451 -4.0417938 -4.0305271 -4.073246 -4.1262231 -4.1498547 -4.1531157 -4.1648121 -4.1813545][-4.1669927 -4.1681938 -4.1539035 -4.1381931 -4.1096163 -4.0654659 -3.994314 -3.9307158 -3.9487813 -4.0342665 -4.1053071 -4.1344132 -4.1407113 -4.1522832 -4.1656771][-4.1407356 -4.1402779 -4.1237268 -4.1055007 -4.0698528 -4.0003009 -3.8884146 -3.8032644 -3.8699608 -4.0068545 -4.0965662 -4.1338582 -4.1430931 -4.1497192 -4.1612639][-4.113471 -4.1118817 -4.096384 -4.0876503 -4.0579557 -3.9791963 -3.8435113 -3.7495232 -3.8580191 -4.0153227 -4.1047411 -4.1388178 -4.143435 -4.1407723 -4.1459875][-4.1044092 -4.10185 -4.0919495 -4.0972886 -4.0824351 -4.0187197 -3.9093654 -3.8560085 -3.9564514 -4.0802913 -4.1439042 -4.1633844 -4.15804 -4.142014 -4.1366334][-4.1223383 -4.1187506 -4.1096711 -4.1201196 -4.1137209 -4.0669737 -3.9973865 -3.9829717 -4.0625215 -4.1474648 -4.1870484 -4.1922665 -4.1784806 -4.1550312 -4.1387882][-4.1498694 -4.1470089 -4.1384482 -4.1510954 -4.1466923 -4.1078959 -4.0619273 -4.0674906 -4.1328611 -4.1924152 -4.2192 -4.2208004 -4.2064486 -4.1795325 -4.1586828][-4.1643324 -4.1689653 -4.169805 -4.1825638 -4.1779084 -4.1426296 -4.1078482 -4.1204977 -4.1725278 -4.2155714 -4.2370076 -4.2421312 -4.23201 -4.2044878 -4.182096][-4.1809807 -4.1958046 -4.2065296 -4.2166619 -4.2092552 -4.1804185 -4.1570959 -4.168323 -4.1999822 -4.2248669 -4.2431211 -4.2519717 -4.246737 -4.2218809 -4.2040739][-4.2057171 -4.2251792 -4.2370558 -4.2428617 -4.2317061 -4.2109079 -4.2014117 -4.20652 -4.2157254 -4.2270384 -4.2428894 -4.2534728 -4.2511606 -4.23315 -4.2239323][-4.2377596 -4.2529244 -4.262732 -4.2659125 -4.2548985 -4.2406573 -4.2381887 -4.2390561 -4.2395587 -4.2449384 -4.2586422 -4.2675915 -4.2674408 -4.2573066 -4.2524767]]...]
INFO - root - 2017-12-05 16:49:09.197478: step 26710, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 73h:07m:48s remains)
INFO - root - 2017-12-05 16:49:17.633868: step 26720, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 70h:24m:40s remains)
INFO - root - 2017-12-05 16:49:26.030476: step 26730, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 72h:54m:12s remains)
INFO - root - 2017-12-05 16:49:34.544189: step 26740, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 74h:10m:29s remains)
INFO - root - 2017-12-05 16:49:42.900865: step 26750, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 71h:58m:56s remains)
INFO - root - 2017-12-05 16:49:51.427339: step 26760, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 71h:01m:22s remains)
INFO - root - 2017-12-05 16:49:59.972175: step 26770, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 69h:45m:46s remains)
INFO - root - 2017-12-05 16:50:08.368541: step 26780, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 71h:29m:51s remains)
INFO - root - 2017-12-05 16:50:16.911958: step 26790, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 72h:05m:27s remains)
INFO - root - 2017-12-05 16:50:25.426283: step 26800, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 73h:57m:41s remains)
2017-12-05 16:50:26.224246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2008376 -4.2141333 -4.2195392 -4.2197533 -4.214366 -4.2052579 -4.2010789 -4.2140388 -4.227005 -4.2269135 -4.2116179 -4.1808767 -4.1529846 -4.1355495 -4.1313658][-4.1319547 -4.1500459 -4.158854 -4.1601429 -4.1551509 -4.1453614 -4.1400189 -4.1554236 -4.173871 -4.1792574 -4.1731009 -4.1520643 -4.1281695 -4.1130319 -4.11621][-4.1011839 -4.1214328 -4.131073 -4.1327176 -4.1289654 -4.1218886 -4.1177773 -4.1324272 -4.1509938 -4.1551447 -4.1453443 -4.1227484 -4.098392 -4.080996 -4.0862951][-4.1155515 -4.1363626 -4.1476412 -4.1509194 -4.1486487 -4.1447644 -4.1411285 -4.1484728 -4.1582866 -4.1549788 -4.1322856 -4.0972066 -4.0631809 -4.0394278 -4.0417][-4.1269245 -4.1460166 -4.1594448 -4.1655312 -4.1667614 -4.1678791 -4.1621141 -4.1583424 -4.1575785 -4.1467252 -4.1158166 -4.0734987 -4.0333047 -4.0061765 -4.0053039][-4.1312051 -4.1397438 -4.1484795 -4.1542325 -4.1548605 -4.1607904 -4.1524057 -4.1387858 -4.1331387 -4.1275482 -4.108819 -4.0726991 -4.03188 -3.9991114 -3.9907191][-4.1117663 -4.1052265 -4.1057949 -4.1087008 -4.1110554 -4.120441 -4.1102433 -4.0920095 -4.087801 -4.0963621 -4.0954032 -4.0710106 -4.0383887 -4.0110397 -4.0029988][-4.0523825 -4.0316553 -4.02526 -4.0280056 -4.0321646 -4.0467138 -4.0417695 -4.0255365 -4.0294662 -4.0553603 -4.0768571 -4.0721006 -4.0622678 -4.0549688 -4.057128][-4.0011549 -3.9662528 -3.951839 -3.9551561 -3.9637766 -3.9842887 -3.9862218 -3.9738157 -3.9837189 -4.0237918 -4.0670362 -4.0875525 -4.1040077 -4.1162629 -4.1253576][-4.0167027 -3.977345 -3.9653587 -3.9794507 -3.9974532 -4.01851 -4.0169792 -4.00234 -4.0124707 -4.0581455 -4.1112943 -4.1432066 -4.1684637 -4.1856542 -4.1934323][-4.1077275 -4.0789995 -4.07802 -4.0986876 -4.1170344 -4.1297364 -4.1204662 -4.1010079 -4.1037607 -4.1350832 -4.1748157 -4.1976695 -4.2131095 -4.2216511 -4.2243481][-4.2085824 -4.1989708 -4.20462 -4.2213655 -4.231926 -4.235496 -4.2230215 -4.2036219 -4.1981263 -4.2084818 -4.2211094 -4.2240839 -4.2243981 -4.2234187 -4.2199559][-4.2864952 -4.2876072 -4.291986 -4.2990656 -4.3035975 -4.3052845 -4.2954378 -4.2792339 -4.2686281 -4.2637272 -4.2568903 -4.24389 -4.2297554 -4.21466 -4.1999826][-4.320426 -4.3249941 -4.3278184 -4.3294053 -4.3323755 -4.3351588 -4.3283477 -4.3141518 -4.3018417 -4.2903504 -4.2753568 -4.2541161 -4.2308307 -4.2060051 -4.1824322][-4.3273296 -4.3346744 -4.3382163 -4.3391805 -4.3416538 -4.3435278 -4.3381062 -4.3256 -4.3120413 -4.2960262 -4.2789879 -4.2564287 -4.2315159 -4.2050977 -4.1803174]]...]
INFO - root - 2017-12-05 16:50:34.763432: step 26810, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.867 sec/batch; 73h:37m:32s remains)
INFO - root - 2017-12-05 16:50:43.313523: step 26820, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 73h:21m:53s remains)
INFO - root - 2017-12-05 16:50:51.752148: step 26830, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 71h:35m:10s remains)
INFO - root - 2017-12-05 16:51:00.292219: step 26840, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 71h:44m:05s remains)
INFO - root - 2017-12-05 16:51:08.873383: step 26850, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 79h:18m:08s remains)
INFO - root - 2017-12-05 16:51:17.349697: step 26860, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 72h:31m:21s remains)
INFO - root - 2017-12-05 16:51:25.890955: step 26870, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.838 sec/batch; 71h:06m:26s remains)
INFO - root - 2017-12-05 16:51:34.516738: step 26880, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 76h:41m:15s remains)
INFO - root - 2017-12-05 16:51:42.964719: step 26890, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 72h:18m:12s remains)
INFO - root - 2017-12-05 16:51:51.540059: step 26900, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 72h:07m:37s remains)
2017-12-05 16:51:52.365868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2682328 -4.2737117 -4.2823391 -4.2921185 -4.3005109 -4.3051424 -4.3063254 -4.3048635 -4.3010015 -4.2984829 -4.2987261 -4.3017149 -4.3051648 -4.3068976 -4.30592][-4.2591987 -4.2652478 -4.274611 -4.284606 -4.2931108 -4.2996044 -4.3015337 -4.2986469 -4.2937813 -4.2915878 -4.2919393 -4.2955623 -4.3001847 -4.3023224 -4.3006592][-4.2393885 -4.2387695 -4.2428193 -4.2491903 -4.2584796 -4.2708235 -4.277874 -4.2764516 -4.2739582 -4.2758284 -4.2784805 -4.2835417 -4.291038 -4.2948017 -4.2937326][-4.213644 -4.2005167 -4.196178 -4.1962948 -4.2066488 -4.2256327 -4.2397118 -4.2433591 -4.2498336 -4.2597818 -4.26621 -4.269948 -4.2786293 -4.2842293 -4.28622][-4.1811357 -4.1537032 -4.1415491 -4.1418667 -4.1538358 -4.1715708 -4.1884928 -4.2040348 -4.2254043 -4.2460418 -4.2546678 -4.2548623 -4.2600756 -4.2628932 -4.2653856][-4.1372571 -4.0985661 -4.0818129 -4.0827923 -4.0869665 -4.0867009 -4.0967298 -4.1312733 -4.1757011 -4.2127852 -4.2265286 -4.2258177 -4.2308226 -4.2344646 -4.2360182][-4.0972834 -4.047276 -4.0144582 -4.0022831 -3.9846334 -3.9455721 -3.9261155 -3.9832194 -4.0683174 -4.1338096 -4.1597214 -4.1667247 -4.1829677 -4.1967511 -4.2026687][-4.0843854 -4.03051 -3.9830971 -3.9507928 -3.900682 -3.8039031 -3.7219369 -3.7888823 -3.9236915 -4.0281487 -4.077333 -4.1019835 -4.1353369 -4.1670122 -4.1843872][-4.11826 -4.0767269 -4.0346112 -4.0010138 -3.9404905 -3.8222079 -3.7075253 -3.7527385 -3.8969476 -4.0155673 -4.0741887 -4.1060572 -4.1427026 -4.1808181 -4.2026505][-4.1905494 -4.1638746 -4.1391225 -4.1227751 -4.0830622 -4.0010033 -3.9182158 -3.925838 -4.0143943 -4.1017914 -4.1481261 -4.1718416 -4.1943178 -4.2212939 -4.2317491][-4.2435613 -4.2265053 -4.211947 -4.208396 -4.1894011 -4.1404271 -4.08821 -4.0789967 -4.1238866 -4.175745 -4.2071691 -4.2202854 -4.2303586 -4.2424912 -4.2374353][-4.256669 -4.2481232 -4.2409844 -4.2448778 -4.235867 -4.1984229 -4.1604733 -4.1474056 -4.1712747 -4.2031159 -4.2219677 -4.2279048 -4.2323174 -4.2368169 -4.2243881][-4.2398524 -4.2409229 -4.2397842 -4.2439 -4.2355332 -4.2043309 -4.1703358 -4.1509104 -4.1601381 -4.1815324 -4.1967049 -4.2069635 -4.2183805 -4.2313251 -4.2288818][-4.1937485 -4.2077661 -4.2125597 -4.2159066 -4.2050471 -4.1750364 -4.1421342 -4.1201973 -4.1223435 -4.1411791 -4.1626663 -4.1867714 -4.2124472 -4.2368808 -4.2453308][-4.1478639 -4.171895 -4.180944 -4.1826911 -4.1704969 -4.1420555 -4.1120539 -4.0920777 -4.0977077 -4.120501 -4.1487393 -4.181293 -4.21417 -4.2404456 -4.2508025]]...]
INFO - root - 2017-12-05 16:52:00.869516: step 26910, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 72h:56m:22s remains)
INFO - root - 2017-12-05 16:52:09.393470: step 26920, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 72h:26m:49s remains)
INFO - root - 2017-12-05 16:52:17.842555: step 26930, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 74h:01m:13s remains)
INFO - root - 2017-12-05 16:52:26.532083: step 26940, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 72h:11m:59s remains)
INFO - root - 2017-12-05 16:52:35.151989: step 26950, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 72h:23m:46s remains)
INFO - root - 2017-12-05 16:52:43.708532: step 26960, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 73h:17m:20s remains)
INFO - root - 2017-12-05 16:52:52.077117: step 26970, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 72h:28m:33s remains)
INFO - root - 2017-12-05 16:53:00.686693: step 26980, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 73h:11m:54s remains)
INFO - root - 2017-12-05 16:53:09.189150: step 26990, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 71h:08m:45s remains)
INFO - root - 2017-12-05 16:53:17.855169: step 27000, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 74h:29m:34s remains)
2017-12-05 16:53:18.600889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1261835 -4.15593 -4.165833 -4.1637959 -4.1499434 -4.1341209 -4.1233497 -4.1308656 -4.1477752 -4.1801705 -4.2099195 -4.228982 -4.2472115 -4.2734189 -4.30266][-4.1460385 -4.1697206 -4.1685133 -4.1550431 -4.1369276 -4.1160512 -4.0988612 -4.0970535 -4.1009483 -4.1268125 -4.1584873 -4.1807795 -4.209034 -4.2438807 -4.2828965][-4.1474457 -4.1718888 -4.1690564 -4.1528344 -4.1351638 -4.1134095 -4.0881352 -4.0768261 -4.0723276 -4.0929909 -4.1292582 -4.1578331 -4.1929212 -4.2295675 -4.26868][-4.1313415 -4.1620393 -4.166008 -4.15597 -4.1450491 -4.1312823 -4.1084094 -4.0996418 -4.0969939 -4.1158886 -4.1575484 -4.191812 -4.2260165 -4.2536473 -4.2784972][-4.1193027 -4.1554761 -4.1643147 -4.1575508 -4.1460943 -4.1327353 -4.1126013 -4.1129704 -4.1217756 -4.1472459 -4.1971774 -4.2348752 -4.2647562 -4.2863135 -4.2984653][-4.1215158 -4.1558022 -4.1568484 -4.1372976 -4.1072721 -4.0789385 -4.0545907 -4.0609717 -4.0897059 -4.1342263 -4.1913147 -4.234046 -4.2659779 -4.2917447 -4.3022442][-4.1179767 -4.1345572 -4.1111941 -4.0614147 -3.9995539 -3.9369435 -3.8876877 -3.9012501 -3.9691606 -4.0540509 -4.1325212 -4.1905665 -4.2336307 -4.2708597 -4.2901316][-4.0978308 -4.0911293 -4.03956 -3.9587836 -3.8626184 -3.758173 -3.6712756 -3.6996231 -3.8254342 -3.9601004 -4.0676374 -4.1447668 -4.2008557 -4.245759 -4.2735395][-4.0885563 -4.067759 -4.006093 -3.9169426 -3.8193684 -3.7148786 -3.6274641 -3.6643848 -3.8027835 -3.9453716 -4.0580435 -4.1383262 -4.1952763 -4.2375393 -4.2656689][-4.1082287 -4.0880671 -4.0322852 -3.9628921 -3.8985627 -3.8349473 -3.7898402 -3.824435 -3.9232509 -4.0290909 -4.1170869 -4.1800113 -4.2232313 -4.253747 -4.2762895][-4.153832 -4.1370854 -4.0933037 -4.0478253 -4.0148354 -3.9863811 -3.973767 -4.0017824 -4.0604954 -4.1284451 -4.188365 -4.2304511 -4.2581077 -4.2780914 -4.2939487][-4.2156262 -4.2003593 -4.168983 -4.1394038 -4.121695 -4.1077008 -4.1050239 -4.1218872 -4.1531439 -4.1924939 -4.2287602 -4.256042 -4.2752943 -4.2928886 -4.3050337][-4.2702665 -4.2588992 -4.2387385 -4.2215438 -4.210629 -4.2015562 -4.1984982 -4.2058496 -4.2172656 -4.2334509 -4.2503996 -4.2655172 -4.2797804 -4.2957053 -4.3056679][-4.294023 -4.2866807 -4.2764812 -4.2697721 -4.2652378 -4.2606626 -4.2605033 -4.2644205 -4.2644057 -4.2644873 -4.2638106 -4.265162 -4.2732725 -4.2877378 -4.29668][-4.2928028 -4.2869105 -4.2823153 -4.2813697 -4.2813482 -4.2803631 -4.2829323 -4.2859693 -4.2808228 -4.272923 -4.262753 -4.2560239 -4.2607713 -4.2748461 -4.2849269]]...]
INFO - root - 2017-12-05 16:53:27.262364: step 27010, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 70h:39m:10s remains)
INFO - root - 2017-12-05 16:53:35.840838: step 27020, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 76h:26m:30s remains)
INFO - root - 2017-12-05 16:53:44.141989: step 27030, loss = 2.04, batch loss = 1.98 (10.5 examples/sec; 0.760 sec/batch; 64h:29m:08s remains)
INFO - root - 2017-12-05 16:53:52.480012: step 27040, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.832 sec/batch; 70h:37m:58s remains)
INFO - root - 2017-12-05 16:54:00.948744: step 27050, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 74h:04m:37s remains)
INFO - root - 2017-12-05 16:54:09.495568: step 27060, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 71h:55m:55s remains)
INFO - root - 2017-12-05 16:54:18.022484: step 27070, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 74h:32m:03s remains)
INFO - root - 2017-12-05 16:54:26.560433: step 27080, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 73h:29m:42s remains)
INFO - root - 2017-12-05 16:54:35.117498: step 27090, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 71h:58m:49s remains)
INFO - root - 2017-12-05 16:54:43.639533: step 27100, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 70h:47m:46s remains)
2017-12-05 16:54:44.430861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0780859 -4.0816045 -4.0906973 -4.1008935 -4.1043768 -4.1045675 -4.1100693 -4.1265674 -4.1486187 -4.1781058 -4.1993146 -4.201499 -4.1958466 -4.19737 -4.1959434][-4.1077504 -4.1200318 -4.1353049 -4.1397748 -4.1304564 -4.1258383 -4.1286845 -4.1305637 -4.1399026 -4.1619473 -4.1817722 -4.1848006 -4.1770306 -4.1760917 -4.1782813][-4.112783 -4.1254497 -4.1458559 -4.155529 -4.15238 -4.1561337 -4.1647606 -4.1661596 -4.1667848 -4.1733775 -4.1806216 -4.1750503 -4.1635723 -4.1595311 -4.1644306][-4.0971761 -4.1023703 -4.113502 -4.123054 -4.1296358 -4.1455097 -4.1684084 -4.1849508 -4.1935515 -4.1949563 -4.1895556 -4.1689472 -4.1473489 -4.1376181 -4.1416044][-4.0769434 -4.0709138 -4.0703883 -4.0768566 -4.0825558 -4.0898876 -4.1127677 -4.1523333 -4.189127 -4.2053804 -4.1964083 -4.16024 -4.1252718 -4.10813 -4.1103024][-4.0725579 -4.0553284 -4.0446968 -4.0442677 -4.03691 -4.0074897 -3.9934433 -4.045917 -4.1215129 -4.1650639 -4.1681643 -4.13897 -4.1059 -4.0860524 -4.08364][-4.098979 -4.0751505 -4.0511255 -4.0383582 -4.0103712 -3.9314616 -3.8480351 -3.8913405 -4.0091724 -4.0886378 -4.1194854 -4.119246 -4.1068163 -4.091495 -4.0837574][-4.144412 -4.1157503 -4.0862551 -4.0676112 -4.036757 -3.9460204 -3.8271637 -3.8362498 -3.9500017 -4.0338578 -4.0829692 -4.1104693 -4.1180644 -4.11412 -4.1084962][-4.1941381 -4.1695561 -4.1443582 -4.1294913 -4.108849 -4.0500388 -3.9694927 -3.9502795 -3.9994595 -4.0404773 -4.0802145 -4.1130571 -4.1251149 -4.1321683 -4.1348944][-4.2294745 -4.215435 -4.2005038 -4.1870403 -4.1710706 -4.1416397 -4.1016388 -4.0789123 -4.0827661 -4.0859137 -4.1025639 -4.1222606 -4.133019 -4.1448765 -4.15497][-4.2376356 -4.2341104 -4.2265587 -4.2173343 -4.2070475 -4.1972847 -4.1874871 -4.1752739 -4.1603332 -4.1412005 -4.1364417 -4.1407948 -4.1484218 -4.1625786 -4.1744518][-4.2298422 -4.2248979 -4.2174845 -4.2159877 -4.2148814 -4.2177076 -4.2255192 -4.2227964 -4.206728 -4.185492 -4.1722164 -4.1708488 -4.1791334 -4.1907811 -4.1987991][-4.2244039 -4.214241 -4.20111 -4.200974 -4.2066016 -4.2207627 -4.2376108 -4.2434154 -4.2353039 -4.2192721 -4.2050357 -4.2050886 -4.2170391 -4.2278242 -4.2331767][-4.2337842 -4.2245808 -4.2118726 -4.2070246 -4.2086611 -4.2191076 -4.2330079 -4.2425117 -4.2478509 -4.2431407 -4.2327228 -4.2358613 -4.2499046 -4.2605247 -4.267169][-4.2638087 -4.2542911 -4.2388558 -4.2269788 -4.2208819 -4.221755 -4.2274733 -4.2355752 -4.2513123 -4.2577772 -4.2547231 -4.2591219 -4.2681708 -4.2747331 -4.2829394]]...]
INFO - root - 2017-12-05 16:54:52.977716: step 27110, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.853 sec/batch; 72h:22m:44s remains)
INFO - root - 2017-12-05 16:55:01.444192: step 27120, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.859 sec/batch; 72h:51m:42s remains)
INFO - root - 2017-12-05 16:55:10.023758: step 27130, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 75h:11m:19s remains)
INFO - root - 2017-12-05 16:55:18.461962: step 27140, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.827 sec/batch; 70h:10m:41s remains)
INFO - root - 2017-12-05 16:55:27.094666: step 27150, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 72h:57m:50s remains)
INFO - root - 2017-12-05 16:55:35.578661: step 27160, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 71h:56m:28s remains)
INFO - root - 2017-12-05 16:55:44.064360: step 27170, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 70h:58m:51s remains)
INFO - root - 2017-12-05 16:55:52.633406: step 27180, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.878 sec/batch; 74h:26m:37s remains)
INFO - root - 2017-12-05 16:56:01.190209: step 27190, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 74h:47m:59s remains)
INFO - root - 2017-12-05 16:56:09.860532: step 27200, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 71h:37m:30s remains)
2017-12-05 16:56:10.680260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2667489 -4.2827659 -4.2932787 -4.3006592 -4.3084946 -4.316546 -4.3235135 -4.3293757 -4.3319521 -4.3286419 -4.3239646 -4.3229289 -4.3226118 -4.3190455 -4.3135786][-4.2842989 -4.2948151 -4.3060656 -4.3147635 -4.3189673 -4.3211627 -4.3235455 -4.3261433 -4.32788 -4.324615 -4.3192339 -4.3177695 -4.3187475 -4.3161817 -4.3097248][-4.2930603 -4.2996569 -4.3110485 -4.3214765 -4.3248186 -4.3223433 -4.3191314 -4.3162584 -4.3140068 -4.3096352 -4.304431 -4.3053231 -4.3099895 -4.3094811 -4.3031249][-4.2802358 -4.2873158 -4.3008308 -4.3147111 -4.3189077 -4.3129458 -4.3041062 -4.2945914 -4.2873392 -4.2804866 -4.2762241 -4.2822485 -4.293664 -4.2983994 -4.2942886][-4.2428951 -4.2525177 -4.2687249 -4.2853918 -4.2902365 -4.2815118 -4.267601 -4.2521806 -4.2405167 -4.2317071 -4.2284946 -4.2413149 -4.2626829 -4.2771382 -4.2806239][-4.1952939 -4.2049155 -4.2202935 -4.2375183 -4.2426543 -4.2337728 -4.2183256 -4.1985312 -4.1820922 -4.171989 -4.1720924 -4.1929727 -4.2258506 -4.2520614 -4.2658563][-4.1644831 -4.17079 -4.1815753 -4.19589 -4.2012539 -4.1942105 -4.1795931 -4.1567259 -4.1352758 -4.1247716 -4.1311917 -4.1612678 -4.203546 -4.2392259 -4.2607527][-4.1667824 -4.17146 -4.1777163 -4.1887989 -4.1928487 -4.1858821 -4.1696286 -4.1437979 -4.1176219 -4.1079392 -4.1209989 -4.1562181 -4.2020426 -4.2404532 -4.2643328][-4.1956697 -4.2023239 -4.2068596 -4.2152705 -4.2170348 -4.2083263 -4.1911345 -4.1642523 -4.135798 -4.1257639 -4.1387014 -4.1697974 -4.2110915 -4.2470622 -4.2694945][-4.232254 -4.240591 -4.2460332 -4.2533827 -4.2539191 -4.2447042 -4.228107 -4.2024617 -4.1744161 -4.1627994 -4.1717796 -4.1952515 -4.2280445 -4.2582712 -4.27772][-4.2651415 -4.2727942 -4.2798343 -4.2874975 -4.2883148 -4.2795749 -4.2644153 -4.2429867 -4.2184496 -4.2065134 -4.2112436 -4.2264123 -4.2479749 -4.2680159 -4.2820749][-4.2903461 -4.2952194 -4.3006368 -4.3073344 -4.3091431 -4.3023815 -4.2906871 -4.275372 -4.2570453 -4.2462721 -4.2466106 -4.2540774 -4.2639217 -4.2732177 -4.2827377][-4.3067856 -4.3092971 -4.3126388 -4.3178177 -4.3206029 -4.31671 -4.3085122 -4.2982926 -4.2864838 -4.2779336 -4.2741942 -4.2741981 -4.2751856 -4.2779279 -4.2849345][-4.3183789 -4.3186922 -4.3204365 -4.3242788 -4.3278871 -4.3270836 -4.3227997 -4.3169756 -4.3102665 -4.3048887 -4.3003216 -4.2970142 -4.2946115 -4.2951 -4.3006077][-4.3258276 -4.3239212 -4.3244848 -4.3278165 -4.3321128 -4.3339448 -4.3333859 -4.3313751 -4.3281059 -4.3245955 -4.3206058 -4.3172441 -4.3149495 -4.3151188 -4.3196588]]...]
INFO - root - 2017-12-05 16:56:19.203634: step 27210, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 72h:51m:21s remains)
INFO - root - 2017-12-05 16:56:27.782916: step 27220, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 71h:31m:47s remains)
INFO - root - 2017-12-05 16:56:36.201012: step 27230, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 72h:11m:28s remains)
INFO - root - 2017-12-05 16:56:44.686013: step 27240, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.833 sec/batch; 70h:39m:59s remains)
INFO - root - 2017-12-05 16:56:53.212506: step 27250, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.834 sec/batch; 70h:42m:27s remains)
INFO - root - 2017-12-05 16:57:01.796820: step 27260, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 72h:30m:52s remains)
INFO - root - 2017-12-05 16:57:10.475578: step 27270, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 75h:09m:10s remains)
INFO - root - 2017-12-05 16:57:19.031429: step 27280, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 72h:34m:59s remains)
INFO - root - 2017-12-05 16:57:27.572412: step 27290, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 72h:14m:21s remains)
INFO - root - 2017-12-05 16:57:36.126933: step 27300, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 71h:48m:34s remains)
2017-12-05 16:57:36.893931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2893085 -4.2904644 -4.2906418 -4.2868061 -4.2807245 -4.2763686 -4.2743034 -4.2727256 -4.272892 -4.278038 -4.2816467 -4.2823095 -4.2846389 -4.2763505 -4.2629867][-4.3014545 -4.310286 -4.3162975 -4.3165555 -4.3129783 -4.3079696 -4.3048453 -4.3033042 -4.3036766 -4.3084307 -4.3078218 -4.3041792 -4.3051028 -4.2973695 -4.2807212][-4.2969565 -4.3089089 -4.3166552 -4.3195729 -4.3185282 -4.3144851 -4.3130455 -4.310575 -4.3092237 -4.3135567 -4.3129215 -4.3063312 -4.3046908 -4.2942729 -4.2696395][-4.2504053 -4.2632127 -4.2713084 -4.2752838 -4.2762446 -4.2752986 -4.2761559 -4.2684531 -4.2620277 -4.2690144 -4.2745671 -4.2709403 -4.2688532 -4.2589169 -4.2298322][-4.1756158 -4.186079 -4.1951952 -4.1964955 -4.1916194 -4.1861491 -4.1788449 -4.153327 -4.1415873 -4.1653314 -4.1889968 -4.1942434 -4.196527 -4.1927414 -4.1671133][-4.1127348 -4.1151152 -4.1137137 -4.0988793 -4.0767112 -4.0555353 -4.0230627 -3.9566133 -3.9342504 -3.9959357 -4.0579929 -4.0886836 -4.1102777 -4.12384 -4.1127858][-4.1241097 -4.1255522 -4.111649 -4.0795474 -4.0413809 -4.0050497 -3.9600127 -3.8789368 -3.8517635 -3.9331658 -4.0113225 -4.0527439 -4.0795245 -4.098031 -4.0966806][-4.1765528 -4.182404 -4.17454 -4.1524444 -4.1232696 -4.0958247 -4.0722136 -4.0262113 -4.0031013 -4.0451374 -4.0885854 -4.108737 -4.1160522 -4.1224308 -4.12359][-4.21352 -4.2207589 -4.2225623 -4.2171674 -4.2062345 -4.1972466 -4.1915326 -4.167676 -4.1468372 -4.1577635 -4.1738124 -4.179997 -4.1787529 -4.1797609 -4.1777496][-4.2149129 -4.2237792 -4.22972 -4.2344723 -4.2386904 -4.2431216 -4.2456274 -4.2290044 -4.2083697 -4.203711 -4.2079315 -4.2120905 -4.2138309 -4.21906 -4.2150178][-4.1677432 -4.1786609 -4.1909943 -4.2089114 -4.2308331 -4.2491264 -4.2582378 -4.2501936 -4.2344022 -4.2278423 -4.2289171 -4.2341089 -4.2384343 -4.2442861 -4.2398868][-4.119648 -4.1260471 -4.14408 -4.1756229 -4.2108469 -4.238606 -4.2545652 -4.2580276 -4.2530508 -4.2490282 -4.248312 -4.2526383 -4.2562666 -4.2599459 -4.2556744][-4.1278086 -4.1242075 -4.1403074 -4.1696224 -4.2017469 -4.2294631 -4.2514448 -4.262826 -4.2675929 -4.2647858 -4.2612896 -4.2622752 -4.2625151 -4.2635469 -4.2583604][-4.1671047 -4.1537929 -4.1626043 -4.1865292 -4.2118454 -4.2332368 -4.2544403 -4.2695265 -4.2765417 -4.2728124 -4.2670608 -4.26523 -4.2635627 -4.2635422 -4.2596669][-4.206295 -4.1924772 -4.1949763 -4.2122903 -4.2308016 -4.2480993 -4.2662258 -4.278904 -4.2844238 -4.2780476 -4.27085 -4.2675848 -4.2641177 -4.2606578 -4.2511263]]...]
INFO - root - 2017-12-05 16:57:45.424412: step 27310, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 72h:15m:27s remains)
INFO - root - 2017-12-05 16:57:54.042885: step 27320, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 75h:05m:17s remains)
INFO - root - 2017-12-05 16:58:02.534761: step 27330, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 71h:22m:40s remains)
INFO - root - 2017-12-05 16:58:11.098036: step 27340, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.891 sec/batch; 75h:33m:23s remains)
INFO - root - 2017-12-05 16:58:19.702790: step 27350, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.806 sec/batch; 68h:20m:30s remains)
INFO - root - 2017-12-05 16:58:28.337461: step 27360, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 73h:49m:54s remains)
INFO - root - 2017-12-05 16:58:36.861997: step 27370, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 71h:05m:15s remains)
INFO - root - 2017-12-05 16:58:45.512909: step 27380, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.871 sec/batch; 73h:50m:12s remains)
INFO - root - 2017-12-05 16:58:54.030989: step 27390, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 70h:32m:44s remains)
INFO - root - 2017-12-05 16:59:02.626716: step 27400, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 71h:33m:32s remains)
2017-12-05 16:59:03.341926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2386861 -4.2290535 -4.2225709 -4.2116022 -4.210413 -4.2268033 -4.2513728 -4.2761407 -4.2882223 -4.2902803 -4.2827005 -4.2625918 -4.2341137 -4.2032962 -4.1903272][-4.2120323 -4.2012062 -4.1937933 -4.1825738 -4.186985 -4.2064328 -4.23292 -4.2556019 -4.2690587 -4.274929 -4.2715607 -4.249824 -4.2112455 -4.1643434 -4.1415467][-4.209466 -4.1980057 -4.1878095 -4.176198 -4.1787376 -4.1880722 -4.2041306 -4.2199073 -4.234344 -4.2516308 -4.2650261 -4.2501841 -4.2122169 -4.1643972 -4.1310406][-4.2246037 -4.2112784 -4.1928096 -4.1764946 -4.1696644 -4.1594024 -4.1596413 -4.1702809 -4.189219 -4.2189083 -4.24934 -4.2524276 -4.2317586 -4.1974869 -4.1637492][-4.2429605 -4.2315187 -4.208746 -4.1845813 -4.1654024 -4.1402225 -4.126924 -4.1272163 -4.1413593 -4.1787367 -4.22451 -4.2497749 -4.2554808 -4.2411757 -4.2208705][-4.2523451 -4.24072 -4.2075076 -4.1701279 -4.1359043 -4.1035051 -4.0804758 -4.0636535 -4.0769153 -4.123888 -4.1824408 -4.2310481 -4.2607956 -4.2688069 -4.2654929][-4.2338538 -4.2101955 -4.1668334 -4.1210008 -4.0774951 -4.0336523 -3.9832926 -3.9434524 -3.9665096 -4.0364504 -4.1179333 -4.1844 -4.2288613 -4.2522483 -4.2637558][-4.1942334 -4.1513638 -4.0975237 -4.0496902 -4.0075941 -3.9542863 -3.869097 -3.8047032 -3.8388085 -3.9374576 -4.0429726 -4.1230145 -4.1683264 -4.1953053 -4.2178345][-4.1594267 -4.1004739 -4.0383539 -3.9926152 -3.9590883 -3.9042735 -3.8054638 -3.7324903 -3.7711473 -3.8781481 -3.98603 -4.0605168 -4.0998921 -4.1250367 -4.1531825][-4.1496644 -4.0946345 -4.0418081 -4.0033126 -3.9731596 -3.926971 -3.8555436 -3.817811 -3.8540041 -3.9283726 -4.0011559 -4.0478387 -4.07818 -4.1006508 -4.1236362][-4.1780896 -4.1465688 -4.1168814 -4.0886784 -4.0553007 -4.0118723 -3.9696887 -3.965414 -3.9990482 -4.0409565 -4.0764904 -4.0972161 -4.1157923 -4.13211 -4.142355][-4.2279558 -4.217042 -4.2082195 -4.1941628 -4.1659231 -4.1253 -4.0956903 -4.1036477 -4.1378417 -4.165309 -4.1828046 -4.1869125 -4.1876383 -4.1943254 -4.1963825][-4.2795644 -4.2807865 -4.28264 -4.2769933 -4.2609711 -4.2321248 -4.2105532 -4.2213621 -4.2497125 -4.2684455 -4.277545 -4.2752094 -4.2675171 -4.2661142 -4.26609][-4.3152175 -4.320394 -4.3254118 -4.3243504 -4.3186588 -4.3053575 -4.29319 -4.2986135 -4.3152747 -4.3271704 -4.3322926 -4.3310471 -4.3252897 -4.323195 -4.3216434][-4.3344793 -4.3357973 -4.3375483 -4.3373985 -4.3361087 -4.3322177 -4.3276134 -4.3298421 -4.3378711 -4.3445892 -4.3477635 -4.34727 -4.3454413 -4.3440318 -4.3429379]]...]
INFO - root - 2017-12-05 16:59:11.835687: step 27410, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.875 sec/batch; 74h:11m:16s remains)
INFO - root - 2017-12-05 16:59:20.493609: step 27420, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 75h:16m:19s remains)
INFO - root - 2017-12-05 16:59:28.949719: step 27430, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 72h:53m:41s remains)
INFO - root - 2017-12-05 16:59:37.548329: step 27440, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 71h:10m:55s remains)
INFO - root - 2017-12-05 16:59:46.165925: step 27450, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.856 sec/batch; 72h:32m:18s remains)
INFO - root - 2017-12-05 16:59:54.674933: step 27460, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.865 sec/batch; 73h:15m:49s remains)
INFO - root - 2017-12-05 17:00:03.152702: step 27470, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.856 sec/batch; 72h:29m:20s remains)
INFO - root - 2017-12-05 17:00:11.828135: step 27480, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 74h:52m:16s remains)
INFO - root - 2017-12-05 17:00:20.383779: step 27490, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 71h:42m:51s remains)
INFO - root - 2017-12-05 17:00:28.827307: step 27500, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 71h:49m:41s remains)
2017-12-05 17:00:29.589711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1677103 -4.1554027 -4.1494923 -4.1609058 -4.1888165 -4.2163715 -4.229713 -4.2372613 -4.2415628 -4.2421494 -4.237885 -4.2204957 -4.1990671 -4.1832304 -4.1831455][-4.1810908 -4.16289 -4.147397 -4.1504655 -4.1725869 -4.1988945 -4.2138133 -4.2228203 -4.2295513 -4.2295127 -4.2252612 -4.2105608 -4.1941776 -4.1841197 -4.1843667][-4.1940956 -4.1752429 -4.1539621 -4.1467342 -4.1548185 -4.1729679 -4.1866188 -4.19705 -4.2050104 -4.2059107 -4.1995339 -4.18956 -4.186574 -4.1869287 -4.1882472][-4.20091 -4.1829309 -4.1586018 -4.1433253 -4.135211 -4.1378374 -4.1501508 -4.1655526 -4.177423 -4.1800771 -4.1758723 -4.1743565 -4.1857252 -4.1951413 -4.1982579][-4.2132163 -4.1975374 -4.1760082 -4.1534767 -4.1259904 -4.1042418 -4.1046343 -4.1222692 -4.1437669 -4.1580443 -4.1650672 -4.1749268 -4.1988573 -4.2147655 -4.2213659][-4.2361674 -4.2240806 -4.209033 -4.1823859 -4.1409016 -4.0954094 -4.0706873 -4.0772862 -4.106123 -4.14086 -4.1676474 -4.189949 -4.2173367 -4.2361555 -4.2424312][-4.25673 -4.2465048 -4.2381272 -4.2143927 -4.1684546 -4.1084185 -4.0551572 -4.0338383 -4.06298 -4.1229248 -4.1751308 -4.2094822 -4.2360888 -4.251533 -4.2524114][-4.2694616 -4.2584372 -4.2556014 -4.2409883 -4.2033124 -4.1446586 -4.0787492 -4.0349956 -4.0622168 -4.14045 -4.2085757 -4.2459965 -4.2627463 -4.26609 -4.2592325][-4.288589 -4.2801242 -4.2832346 -4.2813616 -4.258112 -4.2124753 -4.1548986 -4.1105337 -4.1259365 -4.1963296 -4.2614465 -4.2933116 -4.2939892 -4.2799897 -4.2654314][-4.311543 -4.308639 -4.3167562 -4.3246984 -4.3141956 -4.2851825 -4.2446542 -4.20749 -4.207787 -4.2529492 -4.3043232 -4.3271585 -4.3146539 -4.2890062 -4.2691655][-4.3242216 -4.3247023 -4.3345289 -4.3475189 -4.3460526 -4.3297806 -4.3044896 -4.274662 -4.2643218 -4.2878661 -4.3254304 -4.3401418 -4.3208847 -4.2929354 -4.2733793][-4.3225121 -4.3192816 -4.3270769 -4.3435116 -4.34921 -4.3419652 -4.3256412 -4.30171 -4.2877593 -4.298892 -4.3250451 -4.3360295 -4.3207541 -4.2996521 -4.2851062][-4.3125472 -4.3038769 -4.3086734 -4.3257728 -4.3376856 -4.3375711 -4.3274617 -4.3086624 -4.2954807 -4.2980032 -4.3141584 -4.3248253 -4.3187647 -4.3076406 -4.2992277][-4.3043189 -4.29512 -4.298121 -4.3118567 -4.3222141 -4.3236551 -4.316402 -4.3007808 -4.2888217 -4.2876668 -4.2978654 -4.3085847 -4.3094635 -4.305728 -4.3020535][-4.2903233 -4.2825952 -4.2837267 -4.2934918 -4.3013854 -4.3035293 -4.2995167 -4.2889705 -4.2801185 -4.2770205 -4.282196 -4.2902865 -4.2939034 -4.294136 -4.2928495]]...]
INFO - root - 2017-12-05 17:00:38.085872: step 27510, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 70h:03m:22s remains)
INFO - root - 2017-12-05 17:00:46.564390: step 27520, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 71h:41m:11s remains)
INFO - root - 2017-12-05 17:00:54.976097: step 27530, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 72h:14m:08s remains)
INFO - root - 2017-12-05 17:01:03.521359: step 27540, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 73h:22m:53s remains)
INFO - root - 2017-12-05 17:01:12.013481: step 27550, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 70h:20m:53s remains)
INFO - root - 2017-12-05 17:01:20.402777: step 27560, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.813 sec/batch; 68h:53m:20s remains)
INFO - root - 2017-12-05 17:01:28.872594: step 27570, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 70h:30m:54s remains)
INFO - root - 2017-12-05 17:01:37.345694: step 27580, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 70h:30m:08s remains)
INFO - root - 2017-12-05 17:01:45.820844: step 27590, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 72h:09m:04s remains)
INFO - root - 2017-12-05 17:01:54.299908: step 27600, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 72h:57m:17s remains)
2017-12-05 17:01:55.103491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2149048 -4.2104712 -4.2133932 -4.2214856 -4.2166371 -4.208766 -4.2061515 -4.2150965 -4.2131486 -4.1921062 -4.1665673 -4.1441188 -4.1386485 -4.1554236 -4.1889272][-4.2081962 -4.2081943 -4.2187819 -4.23359 -4.2282338 -4.2107673 -4.1984735 -4.2045588 -4.2048721 -4.1869283 -4.1620307 -4.1399355 -4.1373873 -4.1552515 -4.1856709][-4.1959267 -4.2061877 -4.2262678 -4.24851 -4.241848 -4.2135887 -4.1881804 -4.188735 -4.1947131 -4.1839652 -4.16574 -4.1470108 -4.1482205 -4.1636429 -4.1876454][-4.1813717 -4.2007728 -4.2287993 -4.2555547 -4.2425132 -4.1995311 -4.15582 -4.1481657 -4.1658726 -4.172061 -4.1664939 -4.1517243 -4.1528516 -4.16346 -4.1823893][-4.1682367 -4.1889477 -4.2194877 -4.2484231 -4.2275243 -4.1675372 -4.1001496 -4.0814662 -4.1165175 -4.1517835 -4.1631227 -4.1528597 -4.1481175 -4.1511278 -4.1659493][-4.1541452 -4.1744552 -4.2051415 -4.2295666 -4.2022815 -4.1237211 -4.0275049 -3.9895329 -4.0451822 -4.1196041 -4.157958 -4.1533918 -4.1408372 -4.1374903 -4.1527486][-4.1306996 -4.1570578 -4.1897316 -4.2117863 -4.1758394 -4.0759277 -3.9386353 -3.8648129 -3.9431787 -4.0663457 -4.1379347 -4.1464052 -4.1357737 -4.1325965 -4.1503386][-4.103344 -4.1317568 -4.1708865 -4.1978021 -4.1666017 -4.0595479 -3.8932972 -3.7761097 -3.8636737 -4.0249147 -4.1246133 -4.1493292 -4.147006 -4.1473975 -4.1647439][-4.1049919 -4.1297822 -4.16401 -4.18935 -4.1745596 -4.0977964 -3.9637756 -3.8577642 -3.9114769 -4.0466 -4.14067 -4.1704941 -4.1751275 -4.1775751 -4.1912842][-4.141644 -4.1557727 -4.1768913 -4.1929965 -4.1891189 -4.1516423 -4.0732803 -4.01049 -4.0292811 -4.10377 -4.1638441 -4.1877966 -4.1967144 -4.2015724 -4.2138052][-4.1921759 -4.1946964 -4.1997323 -4.2043896 -4.2046595 -4.1923923 -4.1540623 -4.1233988 -4.1254163 -4.1539969 -4.1817436 -4.1963215 -4.2056623 -4.2141986 -4.2264986][-4.2307162 -4.2280674 -4.2241344 -4.2195387 -4.2189603 -4.2164626 -4.2036772 -4.1922436 -4.1872559 -4.1896234 -4.1936989 -4.2018762 -4.21265 -4.225616 -4.2411][-4.2571363 -4.2567391 -4.253727 -4.2464247 -4.2434053 -4.2465963 -4.2490635 -4.2460008 -4.2317705 -4.2165165 -4.201694 -4.2004929 -4.2145391 -4.2379947 -4.2606893][-4.2595458 -4.2675276 -4.2715507 -4.2674284 -4.2649784 -4.2713842 -4.277657 -4.2748151 -4.25393 -4.2280731 -4.2036152 -4.1957579 -4.2120571 -4.2444692 -4.2755146][-4.2521915 -4.2645068 -4.2675052 -4.26093 -4.258913 -4.26857 -4.27963 -4.277812 -4.2561941 -4.2299886 -4.207849 -4.2007656 -4.215939 -4.2483716 -4.2820907]]...]
INFO - root - 2017-12-05 17:02:03.605925: step 27610, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 72h:45m:09s remains)
INFO - root - 2017-12-05 17:02:12.139019: step 27620, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.838 sec/batch; 70h:56m:02s remains)
INFO - root - 2017-12-05 17:02:20.482877: step 27630, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 62h:00m:27s remains)
INFO - root - 2017-12-05 17:02:29.005065: step 27640, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 71h:42m:11s remains)
INFO - root - 2017-12-05 17:02:37.478438: step 27650, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.832 sec/batch; 70h:25m:20s remains)
INFO - root - 2017-12-05 17:02:46.007049: step 27660, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 73h:25m:45s remains)
INFO - root - 2017-12-05 17:02:54.519078: step 27670, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.815 sec/batch; 69h:01m:03s remains)
INFO - root - 2017-12-05 17:03:03.048127: step 27680, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 70h:22m:14s remains)
INFO - root - 2017-12-05 17:03:11.644460: step 27690, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.852 sec/batch; 72h:08m:18s remains)
INFO - root - 2017-12-05 17:03:20.133273: step 27700, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 72h:59m:46s remains)
2017-12-05 17:03:20.871844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2346482 -4.2057447 -4.1943579 -4.2001948 -4.1915717 -4.1549516 -4.1279569 -4.1302443 -4.1538677 -4.1945071 -4.221827 -4.2425737 -4.2602925 -4.2644095 -4.2499452][-4.2381158 -4.2109404 -4.1993661 -4.2045431 -4.1953993 -4.1640854 -4.1487274 -4.1593537 -4.1832933 -4.2221694 -4.2501049 -4.2681656 -4.281497 -4.2763624 -4.2497211][-4.2424865 -4.2157459 -4.2012086 -4.200881 -4.1905088 -4.1684256 -4.1653047 -4.177783 -4.1947556 -4.2290835 -4.2598948 -4.2821527 -4.2979813 -4.2913942 -4.2583947][-4.2447147 -4.2186756 -4.1995387 -4.188467 -4.1733618 -4.1566253 -4.1578274 -4.1638317 -4.1721573 -4.2052069 -4.243978 -4.276247 -4.3015537 -4.3021588 -4.2716832][-4.2426934 -4.2175508 -4.1926188 -4.169467 -4.144084 -4.120008 -4.1093531 -4.1004944 -4.10672 -4.1485786 -4.2017593 -4.2480612 -4.2862892 -4.295639 -4.2725406][-4.235023 -4.2087574 -4.1794977 -4.146584 -4.1088576 -4.0688443 -4.0283995 -3.9906409 -3.9962773 -4.0544949 -4.1273575 -4.191256 -4.2459931 -4.2714672 -4.2584138][-4.2245307 -4.1916823 -4.1542811 -4.1106806 -4.0590935 -3.9931283 -3.9070916 -3.8290443 -3.8398321 -3.9295919 -4.0308414 -4.114305 -4.1874027 -4.2304482 -4.2267327][-4.21485 -4.1723032 -4.1221418 -4.0648694 -3.9968405 -3.9064028 -3.7848113 -3.6756802 -3.7002642 -3.8318 -3.9635868 -4.0602169 -4.1377821 -4.1834464 -4.1850042][-4.2088747 -4.1550379 -4.0881767 -4.0184507 -3.9447591 -3.8611782 -3.7632406 -3.690392 -3.732888 -3.8622904 -3.9843898 -4.0683751 -4.1288853 -4.1624274 -4.1644845][-4.2091041 -4.1518631 -4.0789804 -4.0111232 -3.9512842 -3.9005184 -3.8549032 -3.8391585 -3.8942804 -3.9911375 -4.0753679 -4.1278582 -4.1635942 -4.1837311 -4.1847105][-4.2190671 -4.1690488 -4.1046891 -4.0507708 -4.0136294 -3.9936848 -3.9882507 -4.0066366 -4.0602856 -4.126935 -4.1787424 -4.2071691 -4.2281036 -4.2409663 -4.2417941][-4.236186 -4.1981044 -4.1506367 -4.1157451 -4.0988035 -4.0983291 -4.1151624 -4.1488795 -4.1949797 -4.2363935 -4.2654877 -4.2808719 -4.2930975 -4.2999015 -4.2979369][-4.2552814 -4.2267261 -4.1934762 -4.1755261 -4.1757889 -4.1881819 -4.2138224 -4.2485518 -4.2815371 -4.3038268 -4.3176212 -4.3240232 -4.3306723 -4.3339753 -4.3307843][-4.2702727 -4.24873 -4.223752 -4.214262 -4.2233047 -4.2433543 -4.2727504 -4.3021116 -4.3220496 -4.332016 -4.3375316 -4.3379669 -4.3386607 -4.3395295 -4.33775][-4.2755923 -4.2604547 -4.24181 -4.2367725 -4.2482777 -4.270124 -4.2997494 -4.3222971 -4.3311939 -4.3318524 -4.3315477 -4.3285923 -4.3268642 -4.3255544 -4.3234344]]...]
INFO - root - 2017-12-05 17:03:29.442909: step 27710, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 71h:18m:29s remains)
INFO - root - 2017-12-05 17:03:37.936676: step 27720, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 71h:53m:03s remains)
INFO - root - 2017-12-05 17:03:46.356288: step 27730, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.756 sec/batch; 63h:57m:45s remains)
INFO - root - 2017-12-05 17:03:54.859934: step 27740, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 71h:19m:55s remains)
INFO - root - 2017-12-05 17:04:03.450272: step 27750, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 71h:42m:21s remains)
INFO - root - 2017-12-05 17:04:11.914351: step 27760, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.823 sec/batch; 69h:38m:11s remains)
INFO - root - 2017-12-05 17:04:20.483312: step 27770, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 73h:38m:56s remains)
INFO - root - 2017-12-05 17:04:28.894674: step 27780, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.836 sec/batch; 70h:45m:49s remains)
INFO - root - 2017-12-05 17:04:37.348074: step 27790, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 70h:11m:37s remains)
INFO - root - 2017-12-05 17:04:45.887470: step 27800, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 72h:54m:08s remains)
2017-12-05 17:04:46.641812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2845521 -4.2871928 -4.2785268 -4.2649889 -4.2553053 -4.2535248 -4.2541213 -4.249341 -4.2386928 -4.2291169 -4.2279696 -4.2302933 -4.225843 -4.2135539 -4.214962][-4.2790928 -4.2827897 -4.2754159 -4.2620125 -4.2502027 -4.2479806 -4.2525845 -4.2543368 -4.248723 -4.2393823 -4.2338405 -4.2279763 -4.215044 -4.1950517 -4.1926737][-4.2663527 -4.2687764 -4.2637806 -4.2525191 -4.2382278 -4.2318368 -4.2398758 -4.25359 -4.2586994 -4.2507319 -4.238996 -4.22244 -4.1992135 -4.1728792 -4.1695166][-4.2464485 -4.247757 -4.2472129 -4.2401791 -4.2225118 -4.205163 -4.2063494 -4.2287993 -4.2500243 -4.2513261 -4.2390513 -4.2163181 -4.1875091 -4.1591825 -4.1572351][-4.224566 -4.2274294 -4.2334146 -4.2323871 -4.2105427 -4.1746612 -4.154748 -4.1764421 -4.2178478 -4.2381434 -4.2342224 -4.2121835 -4.1830106 -4.15779 -4.1602125][-4.2073207 -4.2118607 -4.2229557 -4.2273617 -4.2018824 -4.1470566 -4.0972195 -4.1080961 -4.1719074 -4.2201219 -4.2340326 -4.2182775 -4.1909356 -4.1689882 -4.1748285][-4.1959906 -4.1976819 -4.2095294 -4.2163653 -4.1879377 -4.1161351 -4.0300937 -4.0162034 -4.0998592 -4.1839643 -4.2245369 -4.2232256 -4.1997142 -4.1803102 -4.186636][-4.191051 -4.1863947 -4.1949897 -4.2011485 -4.1725116 -4.0928836 -3.9807065 -3.9344585 -4.0231276 -4.1322541 -4.1975551 -4.2147937 -4.2015467 -4.1863065 -4.1895251][-4.1939769 -4.1860967 -4.192337 -4.1997824 -4.1796231 -4.1147194 -4.0139847 -3.9554644 -4.0149388 -4.11329 -4.1807008 -4.206696 -4.2035441 -4.1901679 -4.1875625][-4.1994948 -4.1879992 -4.1912642 -4.1998444 -4.1908083 -4.1512103 -4.0830026 -4.0338006 -4.0574574 -4.1215096 -4.1726561 -4.1986451 -4.2005882 -4.1878209 -4.1783967][-4.2065926 -4.1903443 -4.1875725 -4.1925225 -4.1895213 -4.1714468 -4.1331573 -4.0967064 -4.0952454 -4.1240268 -4.1572981 -4.1844082 -4.1937881 -4.183506 -4.1679897][-4.2144232 -4.19698 -4.1915379 -4.1929708 -4.1923614 -4.1843791 -4.1644287 -4.1371322 -4.1226211 -4.1272416 -4.143517 -4.16722 -4.1826863 -4.1770658 -4.1593881][-4.2209945 -4.2082138 -4.2072315 -4.211185 -4.21028 -4.2007709 -4.1826239 -4.1571283 -4.1367292 -4.1285653 -4.1354079 -4.1550603 -4.1703186 -4.1673665 -4.1492419][-4.22953 -4.2171206 -4.2180219 -4.2262359 -4.2282596 -4.2173591 -4.1964326 -4.1702061 -4.1475887 -4.1350307 -4.1384044 -4.1557961 -4.1706944 -4.1710682 -4.1579571][-4.2421179 -4.2264657 -4.2225223 -4.2315979 -4.2385383 -4.2303991 -4.211009 -4.1879845 -4.1689672 -4.1596074 -4.1632948 -4.1798282 -4.1954541 -4.2010975 -4.1958709]]...]
INFO - root - 2017-12-05 17:04:55.180612: step 27810, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 71h:03m:34s remains)
INFO - root - 2017-12-05 17:05:03.701348: step 27820, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.818 sec/batch; 69h:14m:27s remains)
INFO - root - 2017-12-05 17:05:12.074704: step 27830, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 0.775 sec/batch; 65h:33m:31s remains)
INFO - root - 2017-12-05 17:05:20.540829: step 27840, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 71h:12m:01s remains)
INFO - root - 2017-12-05 17:05:28.967268: step 27850, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 70h:01m:44s remains)
INFO - root - 2017-12-05 17:05:37.402145: step 27860, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 71h:18m:23s remains)
INFO - root - 2017-12-05 17:05:45.885747: step 27870, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 72h:06m:05s remains)
INFO - root - 2017-12-05 17:05:54.308459: step 27880, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 70h:38m:06s remains)
INFO - root - 2017-12-05 17:06:02.691786: step 27890, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 71h:15m:32s remains)
INFO - root - 2017-12-05 17:06:11.156813: step 27900, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 74h:02m:58s remains)
2017-12-05 17:06:11.877855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1248422 -4.1424351 -4.1509957 -4.1450348 -4.1333675 -4.1369867 -4.1487651 -4.1706347 -4.1974092 -4.2088828 -4.1983743 -4.17391 -4.1475739 -4.12307 -4.0903554][-4.1233516 -4.1469331 -4.1670938 -4.1741757 -4.1702042 -4.1697307 -4.1714849 -4.1844697 -4.2107267 -4.2326436 -4.2321434 -4.2119989 -4.1849971 -4.159194 -4.1252441][-4.1025505 -4.1258197 -4.1542163 -4.1761312 -4.1846337 -4.1827192 -4.1744375 -4.1756682 -4.1965337 -4.2253451 -4.2390738 -4.2319264 -4.2114458 -4.1867619 -4.1523657][-4.0770092 -4.0919552 -4.1232662 -4.1587234 -4.1768222 -4.1710339 -4.1502657 -4.1377974 -4.1507959 -4.1852169 -4.2141738 -4.2245207 -4.2179618 -4.2014141 -4.1729593][-4.0675015 -4.0734911 -4.0996337 -4.136014 -4.1513729 -4.1355915 -4.0996532 -4.0708618 -4.0755067 -4.118444 -4.1630664 -4.1922531 -4.2035394 -4.2006965 -4.1837459][-4.068789 -4.0708385 -4.0898304 -4.120327 -4.1250591 -4.0951872 -4.0395994 -3.9900284 -3.9859011 -4.0418682 -4.10478 -4.1494451 -4.1720204 -4.1751289 -4.1671619][-4.0651822 -4.0667 -4.080142 -4.1030693 -4.0989537 -4.0584617 -3.99091 -3.9285872 -3.9212866 -3.9884272 -4.0643187 -4.1142893 -4.1379704 -4.140492 -4.1412349][-4.0601811 -4.0634475 -4.0718093 -4.0887413 -4.0842886 -4.0525007 -4.0000858 -3.9511735 -3.9427366 -3.9940381 -4.0545282 -4.0914817 -4.102705 -4.09976 -4.1090779][-4.0622907 -4.0649767 -4.0654726 -4.0721946 -4.0723853 -4.0605946 -4.0326462 -4.0002356 -3.9876053 -4.0145669 -4.0549955 -4.0796795 -4.0780611 -4.0664129 -4.0795569][-4.0726681 -4.0724282 -4.0637403 -4.0583258 -4.0587378 -4.0610528 -4.051764 -4.031178 -4.0161576 -4.0216179 -4.0444889 -4.0619383 -4.0581579 -4.0492926 -4.0665975][-4.0797658 -4.0747638 -4.0591969 -4.0461907 -4.0485988 -4.0632362 -4.0720787 -4.0628781 -4.046351 -4.0366511 -4.0430822 -4.0516958 -4.0496297 -4.0465884 -4.0648623][-4.0886374 -4.0811682 -4.0639958 -4.0506234 -4.0551057 -4.0782914 -4.0993438 -4.1005044 -4.0870023 -4.070096 -4.0680151 -4.0720882 -4.0704913 -4.0655513 -4.0791712][-4.1054826 -4.0956783 -4.0771461 -4.0622382 -4.0679531 -4.0941892 -4.1194463 -4.129756 -4.1213269 -4.101284 -4.0933871 -4.0963283 -4.0992112 -4.0985394 -4.1083283][-4.1210303 -4.1075854 -4.0851097 -4.0667763 -4.0710444 -4.1000643 -4.1323605 -4.152421 -4.1512847 -4.1361556 -4.1248651 -4.1227813 -4.1259861 -4.1290832 -4.1372509][-4.1294036 -4.11244 -4.0826545 -4.0536728 -4.05213 -4.0864916 -4.1296148 -4.1590147 -4.1688738 -4.1672273 -4.1613278 -4.157856 -4.15985 -4.1608381 -4.1674719]]...]
INFO - root - 2017-12-05 17:06:20.358923: step 27910, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 74h:23m:57s remains)
INFO - root - 2017-12-05 17:06:29.014574: step 27920, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 72h:47m:07s remains)
INFO - root - 2017-12-05 17:06:37.681113: step 27930, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 76h:05m:09s remains)
INFO - root - 2017-12-05 17:06:46.203230: step 27940, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 71h:20m:20s remains)
INFO - root - 2017-12-05 17:06:54.650678: step 27950, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 73h:10m:27s remains)
INFO - root - 2017-12-05 17:07:02.958348: step 27960, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.824 sec/batch; 69h:43m:18s remains)
INFO - root - 2017-12-05 17:07:11.488204: step 27970, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 72h:21m:41s remains)
INFO - root - 2017-12-05 17:07:20.056724: step 27980, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 72h:52m:24s remains)
INFO - root - 2017-12-05 17:07:28.566542: step 27990, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.829 sec/batch; 70h:07m:56s remains)
INFO - root - 2017-12-05 17:07:36.966614: step 28000, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 71h:01m:07s remains)
2017-12-05 17:07:37.711505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3106837 -4.3069482 -4.3064651 -4.3110065 -4.3165989 -4.3153744 -4.3067789 -4.297276 -4.2951469 -4.2977734 -4.2988858 -4.3009386 -4.308475 -4.3184266 -4.3213563][-4.3094349 -4.3057575 -4.304409 -4.3055043 -4.3058705 -4.2994819 -4.2855186 -4.2716112 -4.2686071 -4.2732797 -4.2784371 -4.2866759 -4.2988324 -4.3103366 -4.3134522][-4.2980204 -4.2946429 -4.2916265 -4.2888756 -4.2848806 -4.2745085 -4.2569203 -4.2425046 -4.2438273 -4.2548056 -4.2646904 -4.2779593 -4.2929311 -4.3034391 -4.3065848][-4.2786951 -4.2772913 -4.2733445 -4.2656288 -4.2548785 -4.24016 -4.2184944 -4.2059183 -4.2166715 -4.2382755 -4.2538176 -4.2708807 -4.287631 -4.29749 -4.3007216][-4.25166 -4.2518077 -4.2443705 -4.2298641 -4.2117691 -4.19224 -4.1651607 -4.1518607 -4.1720405 -4.20835 -4.2334719 -4.25401 -4.2742329 -4.2870083 -4.2942095][-4.2188911 -4.2172518 -4.2037907 -4.18053 -4.1528077 -4.1213069 -4.0810804 -4.0626535 -4.0959272 -4.1529417 -4.1929488 -4.2193246 -4.2451382 -4.2672176 -4.2838545][-4.1784487 -4.1707854 -4.1475229 -4.1120396 -4.071384 -4.0215158 -3.9588168 -3.9304051 -3.9847796 -4.0732131 -4.1328449 -4.1680589 -4.2002439 -4.2343979 -4.26564][-4.1335392 -4.1265917 -4.103374 -4.0660925 -4.0198503 -3.9564557 -3.8707912 -3.8259654 -3.8965034 -4.0111375 -4.0870309 -4.1315565 -4.1709714 -4.2137384 -4.25333][-4.1223316 -4.1276016 -4.1221833 -4.1066394 -4.0812669 -4.0388803 -3.9741149 -3.9338515 -3.9817798 -4.0707421 -4.130806 -4.1653666 -4.1950994 -4.2290621 -4.2618918][-4.1590233 -4.1680121 -4.1714416 -4.1696444 -4.1656079 -4.149765 -4.1157045 -4.0921092 -4.1180472 -4.17274 -4.2149105 -4.2377028 -4.2531486 -4.2699919 -4.2870574][-4.2157626 -4.2167726 -4.2144856 -4.2151928 -4.2194109 -4.2153897 -4.1981463 -4.1868715 -4.2044349 -4.2403975 -4.2704544 -4.2870111 -4.2954359 -4.3016405 -4.3064733][-4.2638807 -4.2557549 -4.2463431 -4.2455597 -4.2499809 -4.2448535 -4.2331905 -4.2286253 -4.241888 -4.2668481 -4.2877712 -4.3032227 -4.3119316 -4.3141813 -4.3119874][-4.2999754 -4.2913551 -4.2805758 -4.2773232 -4.2761784 -4.2650504 -4.2522545 -4.248745 -4.2595448 -4.2783966 -4.2938428 -4.3076892 -4.3171811 -4.3184233 -4.3134637][-4.3177938 -4.3113394 -4.3015451 -4.2974596 -4.2918539 -4.2766452 -4.2641854 -4.2634468 -4.2751055 -4.2901435 -4.3008437 -4.3120947 -4.3196292 -4.3182864 -4.3111186][-4.3170018 -4.3105206 -4.30112 -4.295712 -4.286273 -4.26851 -4.2566071 -4.2597327 -4.2750282 -4.2898731 -4.2996273 -4.3118639 -4.318336 -4.314743 -4.3063803]]...]
INFO - root - 2017-12-05 17:07:46.126107: step 28010, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 73h:32m:37s remains)
INFO - root - 2017-12-05 17:07:54.668995: step 28020, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 70h:31m:13s remains)
INFO - root - 2017-12-05 17:08:03.206644: step 28030, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 72h:07m:31s remains)
INFO - root - 2017-12-05 17:08:11.726531: step 28040, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.828 sec/batch; 69h:59m:04s remains)
INFO - root - 2017-12-05 17:08:20.225049: step 28050, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 72h:28m:09s remains)
INFO - root - 2017-12-05 17:08:28.936069: step 28060, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.882 sec/batch; 74h:35m:52s remains)
INFO - root - 2017-12-05 17:08:37.441819: step 28070, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 71h:55m:54s remains)
INFO - root - 2017-12-05 17:08:46.040360: step 28080, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 73h:37m:26s remains)
INFO - root - 2017-12-05 17:08:54.530798: step 28090, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 73h:21m:11s remains)
INFO - root - 2017-12-05 17:09:02.925459: step 28100, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 71h:51m:29s remains)
2017-12-05 17:09:03.720166: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2258496 -4.2005024 -4.1808753 -4.1566916 -4.1450486 -4.1583519 -4.1921573 -4.2351637 -4.27263 -4.2872829 -4.2914739 -4.2798209 -4.254446 -4.2239194 -4.1965656][-4.2156024 -4.1766191 -4.1429758 -4.1141958 -4.1074929 -4.138608 -4.189826 -4.2442555 -4.2891026 -4.3094597 -4.3131275 -4.2996321 -4.2747488 -4.2464905 -4.2191515][-4.2067924 -4.1572204 -4.1085162 -4.0731063 -4.0723572 -4.1213303 -4.18746 -4.2503781 -4.3000259 -4.3267355 -4.3342271 -4.32432 -4.3059249 -4.2840066 -4.2605977][-4.2066941 -4.1554513 -4.0961185 -4.0499511 -4.0484552 -4.1046443 -4.1762524 -4.2421651 -4.2955723 -4.3309927 -4.3484254 -4.3487415 -4.3371964 -4.3200531 -4.2981811][-4.2127485 -4.1673212 -4.1061382 -4.0528593 -4.040585 -4.08657 -4.1497674 -4.2091413 -4.2628713 -4.3071747 -4.3397274 -4.3545914 -4.3513708 -4.3387938 -4.3211322][-4.2210116 -4.1881905 -4.1370835 -4.0839877 -4.0551987 -4.0712132 -4.1057491 -4.1486044 -4.2005658 -4.2532053 -4.3009715 -4.332562 -4.3403397 -4.3335867 -4.3229356][-4.2282305 -4.2086139 -4.1705031 -4.1211667 -4.0767641 -4.0566916 -4.0527105 -4.071579 -4.1158104 -4.1712346 -4.2315955 -4.2794256 -4.3011155 -4.3028212 -4.29968][-4.2378159 -4.2267823 -4.1990142 -4.154953 -4.1047311 -4.063509 -4.02994 -4.0241418 -4.0504642 -4.0956893 -4.1550484 -4.2086763 -4.2400842 -4.2516165 -4.2564716][-4.2466359 -4.2416506 -4.22252 -4.1874156 -4.1426973 -4.0987377 -4.0546246 -4.0327396 -4.0394292 -4.0670409 -4.113029 -4.1582432 -4.1894221 -4.20525 -4.2147188][-4.2567267 -4.2580447 -4.2472711 -4.2234774 -4.1914382 -4.1569767 -4.1157737 -4.0889335 -4.0828962 -4.0939789 -4.1195526 -4.1476312 -4.1717939 -4.1873932 -4.1989665][-4.262846 -4.2699785 -4.2690945 -4.2578359 -4.2399187 -4.2176285 -4.186358 -4.1610026 -4.1475997 -4.1466041 -4.1540155 -4.1644826 -4.1799679 -4.193439 -4.206789][-4.2659783 -4.2766933 -4.2823782 -4.2805896 -4.2732863 -4.261003 -4.2401681 -4.2196865 -4.204031 -4.19742 -4.1961713 -4.1972985 -4.2063479 -4.2166915 -4.2295041][-4.2734432 -4.2843952 -4.2927313 -4.2953033 -4.2929983 -4.286871 -4.2763009 -4.2646322 -4.2534447 -4.247457 -4.2438917 -4.2412286 -4.2452736 -4.2510042 -4.259141][-4.2852225 -4.2936773 -4.3016291 -4.3053384 -4.3054085 -4.3023987 -4.2979016 -4.2923632 -4.2863746 -4.2821884 -4.2781672 -4.2746749 -4.2762046 -4.2790675 -4.2835207][-4.2953486 -4.2998838 -4.3059711 -4.3103695 -4.3121934 -4.310905 -4.3089166 -4.3060322 -4.30344 -4.3015785 -4.2985959 -4.2955594 -4.2947817 -4.2952671 -4.2981997]]...]
INFO - root - 2017-12-05 17:09:12.240059: step 28110, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 72h:23m:18s remains)
INFO - root - 2017-12-05 17:09:20.792619: step 28120, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 71h:07m:47s remains)
INFO - root - 2017-12-05 17:09:29.232933: step 28130, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 74h:18m:03s remains)
INFO - root - 2017-12-05 17:09:37.642545: step 28140, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 71h:14m:24s remains)
INFO - root - 2017-12-05 17:09:46.266112: step 28150, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 73h:57m:19s remains)
INFO - root - 2017-12-05 17:09:54.804955: step 28160, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 74h:59m:09s remains)
INFO - root - 2017-12-05 17:10:03.381647: step 28170, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 71h:55m:38s remains)
INFO - root - 2017-12-05 17:10:11.859329: step 28180, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 70h:24m:44s remains)
INFO - root - 2017-12-05 17:10:20.410465: step 28190, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 72h:17m:41s remains)
INFO - root - 2017-12-05 17:10:29.025310: step 28200, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 72h:44m:49s remains)
2017-12-05 17:10:29.797787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.288116 -4.2831359 -4.2758808 -4.2629495 -4.2510004 -4.2428565 -4.2379584 -4.2306132 -4.2246051 -4.2285004 -4.2387938 -4.2481256 -4.2464137 -4.2482982 -4.2509661][-4.2497873 -4.244288 -4.2444181 -4.2373242 -4.2209587 -4.205184 -4.1962328 -4.1879168 -4.1913891 -4.20824 -4.2235608 -4.2325792 -4.2269664 -4.2296853 -4.2368393][-4.2022333 -4.1956906 -4.2042294 -4.2036166 -4.1893516 -4.1734204 -4.158813 -4.1422949 -4.1490178 -4.1763482 -4.1963611 -4.2074394 -4.2017174 -4.2074366 -4.2234278][-4.159585 -4.1559339 -4.1657653 -4.1623883 -4.1500015 -4.1387639 -4.1223688 -4.0983438 -4.0965743 -4.1246696 -4.1473293 -4.1615596 -4.1617804 -4.1733413 -4.1954889][-4.1362348 -4.1346784 -4.143353 -4.1341896 -4.1130614 -4.0958333 -4.0780573 -4.05963 -4.0496993 -4.06665 -4.0883131 -4.1081672 -4.122973 -4.1455917 -4.1712093][-4.1258492 -4.124022 -4.1339736 -4.1256042 -4.0929446 -4.0526142 -4.015101 -4.0016117 -4.00868 -4.0299644 -4.047452 -4.0688872 -4.096262 -4.1278863 -4.1557012][-4.1219006 -4.1224461 -4.1333256 -4.1350923 -4.0998344 -4.0282536 -3.9496372 -3.925782 -3.9697123 -4.030663 -4.0632167 -4.0816936 -4.1045866 -4.1352139 -4.1630516][-4.1150107 -4.1165051 -4.132267 -4.1429071 -4.1140738 -4.0347061 -3.9237297 -3.8657031 -3.9304285 -4.0291405 -4.0822325 -4.1020432 -4.1151013 -4.13447 -4.1632838][-4.1085396 -4.1103411 -4.1247334 -4.1348152 -4.1132116 -4.0516777 -3.962059 -3.9077325 -3.9498894 -4.0300064 -4.078269 -4.1004739 -4.1144118 -4.1301823 -4.1629181][-4.1006985 -4.1064863 -4.1178565 -4.1191969 -4.096704 -4.0566297 -4.0113354 -3.9951577 -4.0253086 -4.0692892 -4.0971308 -4.1178169 -4.1337967 -4.148232 -4.1755829][-4.0947909 -4.1093049 -4.1281242 -4.1301064 -4.1102004 -4.0818963 -4.0632277 -4.07394 -4.1058655 -4.1327238 -4.1472611 -4.164186 -4.1787443 -4.1896358 -4.2082195][-4.1103821 -4.1271372 -4.1536579 -4.166904 -4.161159 -4.1420808 -4.1285367 -4.14406 -4.1761432 -4.1993351 -4.2104654 -4.2223554 -4.2313385 -4.2378945 -4.2504277][-4.1631737 -4.1770329 -4.2026243 -4.2205877 -4.2252631 -4.2162375 -4.2056141 -4.2139072 -4.2381248 -4.2591825 -4.2700896 -4.2780523 -4.2830267 -4.2883782 -4.2977228][-4.2424636 -4.25059 -4.2676134 -4.281024 -4.2860756 -4.2824559 -4.2770925 -4.2812314 -4.2934871 -4.3058615 -4.3127728 -4.3186727 -4.3242979 -4.3316283 -4.3395419][-4.3112054 -4.3158603 -4.3241982 -4.331111 -4.3339329 -4.3310037 -4.328073 -4.3306513 -4.3363433 -4.342123 -4.3456426 -4.3486285 -4.3521385 -4.3562627 -4.3604279]]...]
INFO - root - 2017-12-05 17:10:38.281371: step 28210, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 71h:15m:52s remains)
INFO - root - 2017-12-05 17:10:46.826109: step 28220, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 70h:17m:29s remains)
INFO - root - 2017-12-05 17:10:55.467817: step 28230, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 72h:53m:08s remains)
INFO - root - 2017-12-05 17:11:04.043909: step 28240, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 70h:17m:46s remains)
INFO - root - 2017-12-05 17:11:12.572114: step 28250, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 72h:26m:37s remains)
INFO - root - 2017-12-05 17:11:21.119178: step 28260, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 73h:25m:03s remains)
INFO - root - 2017-12-05 17:11:29.633572: step 28270, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.832 sec/batch; 70h:16m:16s remains)
INFO - root - 2017-12-05 17:11:38.130106: step 28280, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 69h:52m:07s remains)
INFO - root - 2017-12-05 17:11:46.479432: step 28290, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 71h:47m:48s remains)
INFO - root - 2017-12-05 17:11:55.119955: step 28300, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 72h:06m:12s remains)
2017-12-05 17:11:55.951322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2990584 -4.3010111 -4.3012218 -4.293849 -4.2734642 -4.2427621 -4.22105 -4.2101665 -4.2203341 -4.2533183 -4.2914729 -4.3140659 -4.3103571 -4.2903872 -4.2735486][-4.3013725 -4.3056436 -4.306531 -4.2981248 -4.2735424 -4.2358279 -4.207499 -4.1901879 -4.1975894 -4.2311382 -4.2738371 -4.3057189 -4.3092504 -4.2921114 -4.2743211][-4.296854 -4.3019772 -4.3050079 -4.2967734 -4.2695756 -4.2249126 -4.1899362 -4.1767335 -4.1880364 -4.222158 -4.2621675 -4.2963676 -4.3040218 -4.2947588 -4.2848945][-4.2881312 -4.2926707 -4.2937369 -4.2850885 -4.2566905 -4.2100081 -4.167161 -4.155746 -4.1773872 -4.2147603 -4.2529783 -4.2832332 -4.289145 -4.2836142 -4.2823377][-4.2779808 -4.282752 -4.2812681 -4.2706552 -4.2385082 -4.1841254 -4.12663 -4.1137171 -4.1526217 -4.20088 -4.2376151 -4.2633328 -4.2672887 -4.262908 -4.2644558][-4.2729092 -4.2751641 -4.2681012 -4.2485847 -4.2046003 -4.1349759 -4.0541854 -4.0359616 -4.0999246 -4.169662 -4.2106032 -4.2307234 -4.2329159 -4.229928 -4.2342381][-4.2736139 -4.2654843 -4.2448277 -4.2081666 -4.1449728 -4.0521932 -3.9363613 -3.9002938 -3.9935029 -4.0943961 -4.1537886 -4.1799431 -4.1873055 -4.1870031 -4.1960793][-4.2639327 -4.244596 -4.2105718 -4.1606054 -4.0873704 -3.9874384 -3.8539567 -3.7937291 -3.8971534 -4.0156164 -4.086937 -4.1215711 -4.1353655 -4.1368055 -4.1499906][-4.2645788 -4.2414179 -4.2029443 -4.1520562 -4.0912924 -4.0206256 -3.9276459 -3.8790445 -3.9420724 -4.0278544 -4.0823269 -4.1096878 -4.1208696 -4.1175709 -4.12712][-4.2790904 -4.2589588 -4.224194 -4.1797976 -4.1351209 -4.0957756 -4.0509529 -4.0260296 -4.0554156 -4.0994473 -4.1265454 -4.1412554 -4.1467347 -4.1369023 -4.1432877][-4.2921729 -4.2795434 -4.2524142 -4.2155061 -4.1791735 -4.1560416 -4.1382918 -4.1290765 -4.1440921 -4.165926 -4.1785536 -4.1863575 -4.1892543 -4.1762028 -4.1788397][-4.3011513 -4.2957439 -4.2779393 -4.2515788 -4.2226171 -4.2049522 -4.194623 -4.1907134 -4.2020082 -4.2178674 -4.2266021 -4.2321324 -4.2351141 -4.2249622 -4.2266922][-4.3072548 -4.3053656 -4.2936296 -4.2775211 -4.257885 -4.2452621 -4.2369924 -4.232945 -4.2431078 -4.256248 -4.2607083 -4.259706 -4.2595029 -4.2540278 -4.2567029][-4.3183861 -4.3159218 -4.3070116 -4.2969379 -4.2839432 -4.2746644 -4.2686272 -4.2661042 -4.2731743 -4.2836065 -4.2861228 -4.2821803 -4.2794027 -4.2746673 -4.2732725][-4.3252826 -4.3222876 -4.3156428 -4.3082156 -4.2996473 -4.2929282 -4.2883177 -4.2875419 -4.2926869 -4.2992783 -4.3018484 -4.2981362 -4.2945871 -4.2893734 -4.2841015]]...]
INFO - root - 2017-12-05 17:12:04.484754: step 28310, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 74h:42m:12s remains)
INFO - root - 2017-12-05 17:12:13.050605: step 28320, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 71h:15m:04s remains)
INFO - root - 2017-12-05 17:12:21.592400: step 28330, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 73h:44m:31s remains)
INFO - root - 2017-12-05 17:12:29.998579: step 28340, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 70h:51m:17s remains)
INFO - root - 2017-12-05 17:12:38.530885: step 28350, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 69h:56m:02s remains)
INFO - root - 2017-12-05 17:12:47.108785: step 28360, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 73h:47m:06s remains)
INFO - root - 2017-12-05 17:12:55.672614: step 28370, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 71h:02m:19s remains)
INFO - root - 2017-12-05 17:13:04.109438: step 28380, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 72h:46m:15s remains)
INFO - root - 2017-12-05 17:13:12.725519: step 28390, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 73h:48m:26s remains)
INFO - root - 2017-12-05 17:13:21.200372: step 28400, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.811 sec/batch; 68h:28m:14s remains)
2017-12-05 17:13:21.957254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.15234 -4.1612635 -4.1680098 -4.1664915 -4.1635828 -4.1632652 -4.1637211 -4.1626892 -4.1667681 -4.1788955 -4.1877356 -4.1915555 -4.1737232 -4.1488562 -4.1397538][-4.1440659 -4.1418042 -4.1451321 -4.1440229 -4.1435432 -4.1466365 -4.1492319 -4.1521354 -4.1602807 -4.1716194 -4.180759 -4.1899533 -4.17903 -4.1568723 -4.1423693][-4.1544781 -4.1398888 -4.1361036 -4.1297035 -4.1314988 -4.1383405 -4.1422405 -4.1501584 -4.1650333 -4.1780286 -4.1837764 -4.19048 -4.1847754 -4.1690116 -4.1553969][-4.1763992 -4.1494579 -4.1343017 -4.1172194 -4.10973 -4.1083918 -4.1068416 -4.1197515 -4.1515179 -4.1790152 -4.1853218 -4.1859136 -4.175633 -4.1627932 -4.1529865][-4.1912384 -4.1577954 -4.1339865 -4.1046133 -4.0811043 -4.061286 -4.0419273 -4.0554485 -4.1091185 -4.1618867 -4.1789994 -4.1769161 -4.158668 -4.1397161 -4.1305966][-4.1858611 -4.1533356 -4.1286135 -4.0884466 -4.043951 -3.9945612 -3.9471939 -3.9540555 -4.0268288 -4.105248 -4.1462193 -4.1614518 -4.1462173 -4.1228333 -4.1133428][-4.1636844 -4.1342974 -4.1162467 -4.0749559 -4.0237679 -3.9549978 -3.8837926 -3.8750877 -3.9498336 -4.0410981 -4.1056757 -4.143013 -4.1420341 -4.12082 -4.1105361][-4.1570315 -4.1383996 -4.1342 -4.1057806 -4.0711856 -4.0211411 -3.9615533 -3.944751 -3.9936533 -4.0626707 -4.1153221 -4.1479154 -4.1494794 -4.1303768 -4.1168733][-4.1836662 -4.1757712 -4.1795917 -4.1610832 -4.1428275 -4.1225996 -4.094707 -4.0844126 -4.101862 -4.1302481 -4.1490254 -4.1592603 -4.1542425 -4.13506 -4.123004][-4.229197 -4.228066 -4.2312446 -4.2154202 -4.2045918 -4.1982875 -4.1900539 -4.1829739 -4.182085 -4.1825047 -4.1785984 -4.1742907 -4.1619935 -4.1433196 -4.1319537][-4.2643118 -4.2643886 -4.2666035 -4.2547088 -4.2499533 -4.2451367 -4.2400751 -4.2328267 -4.22293 -4.2127657 -4.20357 -4.1933579 -4.1755705 -4.1553378 -4.1446533][-4.2763557 -4.2742653 -4.2774553 -4.2729797 -4.2727985 -4.2688594 -4.2630367 -4.2592993 -4.2505355 -4.2408314 -4.232461 -4.2178 -4.1901507 -4.1623669 -4.1525121][-4.2750149 -4.2717962 -4.277256 -4.2772145 -4.2793674 -4.2762017 -4.2716484 -4.2736049 -4.2697477 -4.2618351 -4.2551031 -4.2404094 -4.2122884 -4.1827588 -4.1730909][-4.2828441 -4.2774162 -4.2830033 -4.2830358 -4.2837448 -4.2812424 -4.27944 -4.28163 -4.279284 -4.2724419 -4.266789 -4.2549219 -4.2324553 -4.2112756 -4.2053423][-4.2991858 -4.2949767 -4.3019261 -4.3028684 -4.3003049 -4.2964096 -4.294477 -4.2934437 -4.2880087 -4.2798533 -4.2709703 -4.2584224 -4.2414751 -4.2301369 -4.2284794]]...]
INFO - root - 2017-12-05 17:13:30.450907: step 28410, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 74h:21m:15s remains)
INFO - root - 2017-12-05 17:13:38.847443: step 28420, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 73h:14m:00s remains)
INFO - root - 2017-12-05 17:13:47.454758: step 28430, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 70h:04m:30s remains)
INFO - root - 2017-12-05 17:13:55.900105: step 28440, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 72h:55m:21s remains)
INFO - root - 2017-12-05 17:14:04.571458: step 28450, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 71h:12m:43s remains)
INFO - root - 2017-12-05 17:14:13.283609: step 28460, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 71h:47m:47s remains)
INFO - root - 2017-12-05 17:14:21.803774: step 28470, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 72h:42m:05s remains)
INFO - root - 2017-12-05 17:14:30.317157: step 28480, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 70h:54m:07s remains)
INFO - root - 2017-12-05 17:14:38.840043: step 28490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 72h:21m:48s remains)
INFO - root - 2017-12-05 17:14:47.312907: step 28500, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 68h:59m:17s remains)
2017-12-05 17:14:48.038160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2577734 -4.2497268 -4.2467232 -4.2539668 -4.2670684 -4.2749906 -4.2755451 -4.2735553 -4.2664385 -4.2542877 -4.249301 -4.2534852 -4.2559147 -4.2656722 -4.284718][-4.2544312 -4.2470083 -4.2432733 -4.2504554 -4.2650781 -4.2704973 -4.2665391 -4.2589536 -4.24183 -4.2178974 -4.2112689 -4.2217808 -4.2313833 -4.2473621 -4.2707076][-4.2452159 -4.2357106 -4.2297754 -4.2343025 -4.2488551 -4.2506618 -4.2420073 -4.2309837 -4.2062664 -4.1769981 -4.1751981 -4.1964111 -4.2170396 -4.2395868 -4.2651882][-4.2295814 -4.2138619 -4.2031574 -4.2036257 -4.2157588 -4.2129869 -4.1990261 -4.1867661 -4.1634493 -4.1395597 -4.1459923 -4.1756425 -4.2056723 -4.2334409 -4.2622366][-4.2012777 -4.1748352 -4.1608744 -4.1616869 -4.1721172 -4.1612883 -4.1331105 -4.1184959 -4.1058569 -4.092988 -4.1025243 -4.1309071 -4.1700339 -4.2102284 -4.2481561][-4.1731224 -4.1366358 -4.1169004 -4.1155005 -4.1165619 -4.0836835 -4.029448 -4.0165639 -4.0228286 -4.0255103 -4.0386028 -4.0711336 -4.1262064 -4.1843543 -4.2331181][-4.1399117 -4.0958557 -4.0678205 -4.0518489 -4.0275216 -3.9596469 -3.8770859 -3.8667858 -3.8956745 -3.9275866 -3.9678867 -4.02954 -4.1063032 -4.1803026 -4.2350059][-4.1175208 -4.0663557 -4.0274863 -3.9886332 -3.9352281 -3.8399739 -3.7501426 -3.761452 -3.8281469 -3.9008887 -3.9710789 -4.0491819 -4.1315727 -4.2035871 -4.2561116][-4.13103 -4.0753264 -4.0297632 -3.9815588 -3.9268842 -3.8488488 -3.7844019 -3.8134346 -3.8952842 -3.9805431 -4.0515876 -4.1187382 -4.1874261 -4.2473054 -4.290205][-4.1668959 -4.1144419 -4.0742145 -4.0364528 -3.9998829 -3.9572549 -3.9284968 -3.9585464 -4.0213442 -4.0916719 -4.150537 -4.2027869 -4.2549858 -4.2977247 -4.3229537][-4.2030234 -4.1640882 -4.1410985 -4.1221209 -4.0982637 -4.0766783 -4.0703921 -4.1007576 -4.1442852 -4.1914163 -4.2321625 -4.2704449 -4.3070326 -4.3321362 -4.34388][-4.2359724 -4.20809 -4.1987457 -4.1954665 -4.1838908 -4.1768212 -4.184236 -4.2123313 -4.2409182 -4.2665939 -4.2904158 -4.3146076 -4.3354287 -4.3487868 -4.3513236][-4.263062 -4.2440705 -4.2392683 -4.2429547 -4.2425742 -4.2468696 -4.261529 -4.2835937 -4.2996039 -4.3121247 -4.3240376 -4.3352237 -4.3465543 -4.3539023 -4.35209][-4.2879672 -4.2796144 -4.2795811 -4.2858729 -4.2912574 -4.2983356 -4.3090644 -4.3203931 -4.3276238 -4.3319511 -4.3362532 -4.34131 -4.3464756 -4.3489137 -4.3449802][-4.3070393 -4.3035579 -4.3027897 -4.3065481 -4.3113265 -4.3160696 -4.3209214 -4.325747 -4.3281212 -4.3278866 -4.3286467 -4.3305755 -4.332377 -4.3335471 -4.3324785]]...]
INFO - root - 2017-12-05 17:14:56.513345: step 28510, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 71h:45m:47s remains)
INFO - root - 2017-12-05 17:15:04.985916: step 28520, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 71h:38m:10s remains)
INFO - root - 2017-12-05 17:15:13.474138: step 28530, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 71h:27m:24s remains)
INFO - root - 2017-12-05 17:15:21.954606: step 28540, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 72h:51m:13s remains)
INFO - root - 2017-12-05 17:15:30.664570: step 28550, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 75h:17m:28s remains)
INFO - root - 2017-12-05 17:15:39.226784: step 28560, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 70h:54m:27s remains)
INFO - root - 2017-12-05 17:15:47.788327: step 28570, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 71h:27m:56s remains)
INFO - root - 2017-12-05 17:15:56.298739: step 28580, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 71h:13m:44s remains)
INFO - root - 2017-12-05 17:16:04.735290: step 28590, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 70h:03m:15s remains)
INFO - root - 2017-12-05 17:16:13.206218: step 28600, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 73h:24m:36s remains)
2017-12-05 17:16:13.931452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1289921 -4.1126246 -4.090517 -4.0816817 -4.0871148 -4.0977607 -4.1001058 -4.0979576 -4.0995779 -4.0988736 -4.0930238 -4.0817556 -4.0820031 -4.0862126 -4.0838261][-4.09855 -4.0803943 -4.0524082 -4.0413828 -4.0469589 -4.0573263 -4.0614018 -4.0614357 -4.0572696 -4.0537782 -4.0573683 -4.059814 -4.0649109 -4.0664887 -4.0593877][-4.0900621 -4.0691528 -4.040525 -4.034389 -4.0464525 -4.0626545 -4.0690241 -4.07156 -4.0595994 -4.0478325 -4.0566659 -4.0645208 -4.0669489 -4.0607495 -4.0517058][-4.1085024 -4.0932784 -4.0710669 -4.0674376 -4.0743709 -4.0807781 -4.0834889 -4.0860462 -4.07275 -4.063365 -4.0776505 -4.0914092 -4.0949173 -4.0822654 -4.0693111][-4.1255918 -4.1187248 -4.1021657 -4.1009374 -4.1017714 -4.093133 -4.0782728 -4.0629811 -4.0426626 -4.0398641 -4.0684295 -4.10068 -4.1149125 -4.107049 -4.0943933][-4.1263747 -4.1300616 -4.1218429 -4.1280403 -4.1278358 -4.1086664 -4.07434 -4.0280786 -3.9811416 -3.9788597 -4.0338678 -4.0931916 -4.1242337 -4.1263909 -4.1184611][-4.1149378 -4.1254482 -4.1204367 -4.124475 -4.1250362 -4.1037087 -4.0559316 -3.9823408 -3.9044721 -3.9001188 -3.9899931 -4.0747514 -4.118207 -4.1325846 -4.13308][-4.1068816 -4.1206722 -4.1206336 -4.1251354 -4.1311231 -4.1206708 -4.083138 -4.0148058 -3.935483 -3.9260204 -4.0109773 -4.0900469 -4.13043 -4.1428447 -4.14672][-4.1134014 -4.1339345 -4.1391778 -4.144321 -4.15172 -4.1484995 -4.1283159 -4.0915031 -4.0442243 -4.0440984 -4.0932078 -4.1369267 -4.1604061 -4.1654348 -4.1685781][-4.1196222 -4.1457853 -4.1597562 -4.1650753 -4.1706309 -4.1702118 -4.153903 -4.1335273 -4.1083579 -4.1079235 -4.132997 -4.1555562 -4.1687565 -4.1707683 -4.1774316][-4.118186 -4.1448531 -4.1655116 -4.1703711 -4.176137 -4.1788015 -4.1653361 -4.149847 -4.1314826 -4.1254516 -4.1368737 -4.1534123 -4.1658425 -4.166163 -4.1712518][-4.1121516 -4.1327944 -4.156075 -4.1654663 -4.1729684 -4.1778026 -4.1695633 -4.1544881 -4.1367116 -4.1234665 -4.1290622 -4.146276 -4.1625533 -4.1626477 -4.1609578][-4.0970716 -4.1140623 -4.1385989 -4.1559114 -4.1671042 -4.1742144 -4.1706572 -4.1515183 -4.127121 -4.1060252 -4.1060162 -4.1271176 -4.1550884 -4.1590486 -4.1521435][-4.1000214 -4.1058407 -4.1246672 -4.1419072 -4.1572938 -4.1700926 -4.1729293 -4.1502461 -4.1177197 -4.0913825 -4.0894289 -4.1120644 -4.1478314 -4.1643682 -4.1629314][-4.0967317 -4.0926723 -4.1087623 -4.1286421 -4.1499128 -4.1733093 -4.1822748 -4.1581068 -4.1205411 -4.0924759 -4.0852551 -4.1017451 -4.1363463 -4.1614628 -4.1640778]]...]
INFO - root - 2017-12-05 17:16:22.449495: step 28610, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 73h:18m:50s remains)
INFO - root - 2017-12-05 17:16:30.967757: step 28620, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 72h:07m:20s remains)
INFO - root - 2017-12-05 17:16:39.589423: step 28630, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 74h:30m:00s remains)
INFO - root - 2017-12-05 17:16:47.700836: step 28640, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 72h:29m:44s remains)
INFO - root - 2017-12-05 17:16:56.120541: step 28650, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 73h:18m:38s remains)
INFO - root - 2017-12-05 17:17:04.556295: step 28660, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 71h:56m:06s remains)
INFO - root - 2017-12-05 17:17:13.020693: step 28670, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 70h:25m:56s remains)
INFO - root - 2017-12-05 17:17:21.386462: step 28680, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 71h:46m:50s remains)
INFO - root - 2017-12-05 17:17:29.868283: step 28690, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.815 sec/batch; 68h:44m:57s remains)
INFO - root - 2017-12-05 17:17:38.399887: step 28700, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 75h:02m:50s remains)
2017-12-05 17:17:39.145812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3211112 -4.3225269 -4.3206105 -4.3153749 -4.3084126 -4.3028045 -4.3013535 -4.3049612 -4.3128834 -4.3181853 -4.320693 -4.3204789 -4.320364 -4.3202863 -4.3200941][-4.31719 -4.3133221 -4.30249 -4.2864242 -4.2712684 -4.2637143 -4.2664757 -4.2765274 -4.2948246 -4.3089089 -4.3163314 -4.316968 -4.3158383 -4.3149471 -4.3135839][-4.3074589 -4.2960644 -4.2730289 -4.2423191 -4.2173166 -4.2071133 -4.2091269 -4.2197394 -4.2448187 -4.2704606 -4.2863293 -4.2900681 -4.2914386 -4.2938123 -4.2956142][-4.2935047 -4.2742662 -4.2401543 -4.1966066 -4.1664972 -4.1568394 -4.1554165 -4.1549621 -4.174469 -4.20771 -4.2306571 -4.23703 -4.24305 -4.2558727 -4.2668247][-4.277451 -4.2507057 -4.2068162 -4.1539612 -4.1227121 -4.1157422 -4.1057138 -4.0866561 -4.0972862 -4.1387439 -4.1716089 -4.1761775 -4.1833739 -4.2094531 -4.2325797][-4.262363 -4.2313967 -4.1815171 -4.12222 -4.0899153 -4.0816627 -4.0524549 -4.0036373 -4.0058413 -4.0664854 -4.1158824 -4.1221743 -4.1307526 -4.1698055 -4.2060366][-4.2526774 -4.2207952 -4.172195 -4.1097407 -4.0698571 -4.0496879 -3.9915814 -3.8950844 -3.8845491 -3.9809549 -4.0584369 -4.0752039 -4.0958056 -4.1497889 -4.197854][-4.2464104 -4.2161112 -4.1742411 -4.1132383 -4.0654817 -4.0314693 -3.9469686 -3.8009219 -3.7712483 -3.9064965 -4.0160642 -4.0525684 -4.0926809 -4.1551013 -4.2075415][-4.24942 -4.2183995 -4.1825891 -4.1276383 -4.0794983 -4.0425096 -3.9592378 -3.8112545 -3.7656417 -3.9037633 -4.0249238 -4.0803633 -4.1318679 -4.1908603 -4.2347074][-4.2641678 -4.2328682 -4.1967525 -4.1447306 -4.0986719 -4.0706067 -4.0158958 -3.9187598 -3.8827033 -3.9807785 -4.0835714 -4.1461945 -4.19677 -4.2414112 -4.2690787][-4.28239 -4.253479 -4.2148519 -4.160706 -4.1146383 -4.0956197 -4.0699005 -4.0274038 -4.0163193 -4.0786519 -4.1537156 -4.21158 -4.2542105 -4.2827792 -4.2955527][-4.2980576 -4.276494 -4.2402825 -4.1877751 -4.1453409 -4.1332874 -4.1238565 -4.1141691 -4.1217279 -4.1644917 -4.2131672 -4.2573681 -4.289588 -4.3049269 -4.3097711][-4.3090887 -4.2973576 -4.2715168 -4.2307253 -4.1995807 -4.1938086 -4.1928911 -4.19343 -4.2045493 -4.2297335 -4.2545886 -4.2798824 -4.2998176 -4.30844 -4.3115478][-4.3156643 -4.312685 -4.3020225 -4.2784939 -4.26099 -4.260344 -4.2638063 -4.2666645 -4.2712269 -4.2794485 -4.2861438 -4.2948232 -4.3037167 -4.3075995 -4.3103576][-4.3184042 -4.3203335 -4.3196182 -4.3113613 -4.3033695 -4.3046565 -4.30812 -4.3096557 -4.3093476 -4.3087344 -4.3067079 -4.3057609 -4.3072786 -4.3086414 -4.3111634]]...]
INFO - root - 2017-12-05 17:17:47.699825: step 28710, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 73h:27m:20s remains)
INFO - root - 2017-12-05 17:17:56.257534: step 28720, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 71h:50m:14s remains)
INFO - root - 2017-12-05 17:18:04.695424: step 28730, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 71h:53m:12s remains)
INFO - root - 2017-12-05 17:18:12.947289: step 28740, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.812 sec/batch; 68h:29m:31s remains)
INFO - root - 2017-12-05 17:18:21.532444: step 28750, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 70h:20m:11s remains)
INFO - root - 2017-12-05 17:18:30.074747: step 28760, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 74h:48m:09s remains)
INFO - root - 2017-12-05 17:18:38.566065: step 28770, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 71h:54m:31s remains)
INFO - root - 2017-12-05 17:18:47.015977: step 28780, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 71h:07m:12s remains)
INFO - root - 2017-12-05 17:18:55.577463: step 28790, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 71h:54m:00s remains)
INFO - root - 2017-12-05 17:19:04.127654: step 28800, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 70h:53m:47s remains)
2017-12-05 17:19:04.837420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.228478 -4.2302737 -4.2278323 -4.2249708 -4.2266603 -4.2301059 -4.2300458 -4.2199006 -4.2048278 -4.1874862 -4.178628 -4.1905122 -4.2024555 -4.1932125 -4.1857495][-4.2368774 -4.2381544 -4.2406549 -4.2438521 -4.2469296 -4.24567 -4.239275 -4.2265267 -4.2131329 -4.1997781 -4.1960158 -4.2087317 -4.216938 -4.2000647 -4.1823807][-4.2423797 -4.2450547 -4.2566757 -4.2681384 -4.2712517 -4.2643037 -4.2491412 -4.2341762 -4.2249494 -4.2181969 -4.2210288 -4.235745 -4.2415905 -4.2148261 -4.1827831][-4.2437587 -4.2494378 -4.2671771 -4.2799459 -4.2797403 -4.2633648 -4.2363253 -4.21573 -4.2098174 -4.2109489 -4.2227731 -4.246664 -4.2575974 -4.2304397 -4.1881175][-4.229105 -4.2357144 -4.2524161 -4.2619872 -4.2588077 -4.2358813 -4.2020092 -4.1749806 -4.1688638 -4.1745791 -4.1982045 -4.2349834 -4.2608471 -4.2482238 -4.2078137][-4.2170906 -4.223629 -4.2340989 -4.2338181 -4.2223535 -4.1932411 -4.1533995 -4.1208539 -4.1146159 -4.124126 -4.1588941 -4.2085524 -4.2499595 -4.2548804 -4.2243137][-4.2114348 -4.2199855 -4.2292624 -4.2222085 -4.2021146 -4.1659679 -4.1170988 -4.0793505 -4.0713243 -4.0802946 -4.1170082 -4.1782093 -4.2345166 -4.2542152 -4.2340512][-4.2013493 -4.2124057 -4.2283525 -4.2254968 -4.2090769 -4.1750216 -4.1236606 -4.081892 -4.0701675 -4.073308 -4.1034079 -4.162683 -4.2226892 -4.2476916 -4.2315116][-4.1897993 -4.201262 -4.2240367 -4.2333045 -4.2279878 -4.2044387 -4.1625395 -4.1254354 -4.1118565 -4.1104484 -4.130146 -4.1738734 -4.2231016 -4.2402215 -4.2188587][-4.1896796 -4.1972852 -4.2239432 -4.2411151 -4.2432485 -4.2291679 -4.201561 -4.17822 -4.1690893 -4.1663623 -4.1760058 -4.20239 -4.2327824 -4.2334123 -4.2028375][-4.1977873 -4.2000165 -4.2222986 -4.2393951 -4.243011 -4.2348366 -4.2173004 -4.2048864 -4.2011104 -4.1986623 -4.2049441 -4.225121 -4.2448058 -4.2359381 -4.2011352][-4.2031946 -4.1979856 -4.2088938 -4.2239451 -4.2274694 -4.2194152 -4.2070494 -4.1997104 -4.1983237 -4.1955142 -4.2024107 -4.2250462 -4.2439823 -4.2360907 -4.2080784][-4.2201085 -4.2136946 -4.2141533 -4.2195315 -4.2186069 -4.2070408 -4.1960659 -4.191607 -4.1937666 -4.1931314 -4.2014575 -4.22437 -4.2405367 -4.2329955 -4.2153988][-4.2509689 -4.24985 -4.2477169 -4.2429314 -4.2358809 -4.2226448 -4.21387 -4.2125378 -4.2176056 -4.2174745 -4.2249913 -4.2439332 -4.2545366 -4.2438893 -4.2324419][-4.287333 -4.2926316 -4.2929764 -4.2859149 -4.276505 -4.262394 -4.2550273 -4.2548666 -4.2596674 -4.261776 -4.2682838 -4.2830625 -4.2860003 -4.2708898 -4.2580504]]...]
INFO - root - 2017-12-05 17:19:13.412201: step 28810, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.865 sec/batch; 72h:56m:02s remains)
INFO - root - 2017-12-05 17:19:21.909898: step 28820, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 70h:52m:21s remains)
INFO - root - 2017-12-05 17:19:30.321086: step 28830, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.819 sec/batch; 69h:04m:20s remains)
INFO - root - 2017-12-05 17:19:38.564706: step 28840, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 71h:05m:49s remains)
INFO - root - 2017-12-05 17:19:46.923569: step 28850, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 70h:47m:14s remains)
INFO - root - 2017-12-05 17:19:55.309909: step 28860, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 68h:54m:57s remains)
INFO - root - 2017-12-05 17:20:03.791904: step 28870, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.847 sec/batch; 71h:23m:45s remains)
INFO - root - 2017-12-05 17:20:12.285359: step 28880, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 71h:18m:30s remains)
INFO - root - 2017-12-05 17:20:20.673616: step 28890, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.813 sec/batch; 68h:32m:56s remains)
INFO - root - 2017-12-05 17:20:29.115494: step 28900, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 70h:26m:32s remains)
2017-12-05 17:20:29.938903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3600359 -4.3600888 -4.3589048 -4.35735 -4.35592 -4.35551 -4.35546 -4.3547826 -4.3547068 -4.3560166 -4.3580608 -4.35989 -4.3614798 -4.3624768 -4.3624711][-4.3562841 -4.3549433 -4.3523164 -4.3495255 -4.3471594 -4.346046 -4.3458605 -4.3454313 -4.3458214 -4.3484578 -4.3522639 -4.3556013 -4.3585796 -4.3612294 -4.3625269][-4.3512254 -4.3485689 -4.3448582 -4.3409286 -4.336318 -4.3309889 -4.3256645 -4.320364 -4.318965 -4.324544 -4.3341312 -4.344111 -4.3525424 -4.35891 -4.3618131][-4.345675 -4.3411365 -4.3340206 -4.3242779 -4.3117604 -4.2956567 -4.2786374 -4.2661543 -4.2649288 -4.2787204 -4.3027821 -4.328042 -4.3474326 -4.359395 -4.3645911][-4.3398395 -4.3326497 -4.3176522 -4.2941504 -4.2636504 -4.228487 -4.1939468 -4.1717024 -4.1738753 -4.2041144 -4.2522755 -4.3003411 -4.3357677 -4.3563156 -4.3650546][-4.3315334 -4.3176079 -4.2889533 -4.2433949 -4.1860547 -4.1218791 -4.0622144 -4.027761 -4.0373373 -4.0915122 -4.1705394 -4.2487788 -4.3064761 -4.340734 -4.3578167][-4.324964 -4.302 -4.2555332 -4.1838489 -4.0957904 -4.0035329 -3.9233811 -3.8824987 -3.903616 -3.9850614 -4.0951071 -4.199254 -4.2754054 -4.3231673 -4.3496189][-4.324553 -4.2986765 -4.2452741 -4.1603842 -4.0562043 -3.9524035 -3.8697917 -3.8342166 -3.8651309 -3.9632163 -4.0897756 -4.201673 -4.2787104 -4.3258843 -4.351409][-4.32912 -4.3111548 -4.2692814 -4.1996484 -4.1123633 -4.0268254 -3.9634812 -3.9405775 -3.9699397 -4.056078 -4.16576 -4.2580242 -4.3154159 -4.3467469 -4.3609986][-4.328836 -4.3220935 -4.3002086 -4.2612877 -4.2105327 -4.1602421 -4.1242809 -4.1150651 -4.1394253 -4.1974607 -4.2670946 -4.3218336 -4.3521614 -4.3656459 -4.3683825][-4.3170905 -4.3210659 -4.3170505 -4.3051929 -4.2875504 -4.2689548 -4.2577195 -4.2589846 -4.2754812 -4.3072829 -4.3406186 -4.3629313 -4.3716965 -4.3723707 -4.3685775][-4.2859721 -4.3017774 -4.3130565 -4.3202224 -4.32385 -4.3245382 -4.32619 -4.3316545 -4.3413305 -4.354496 -4.3656878 -4.3712678 -4.3704457 -4.3668189 -4.3620358][-4.2379956 -4.2627678 -4.2844663 -4.3028831 -4.3183 -4.3286843 -4.3359418 -4.3420053 -4.3473616 -4.3518381 -4.3545828 -4.3551755 -4.3536415 -4.3512745 -4.3492637][-4.1874313 -4.2130547 -4.2375236 -4.2605543 -4.2814388 -4.2957854 -4.3076453 -4.3162947 -4.322207 -4.3259468 -4.3276525 -4.3282857 -4.3292766 -4.331254 -4.3337822][-4.1780806 -4.1898227 -4.20383 -4.2226477 -4.2434421 -4.2594709 -4.2750473 -4.2870708 -4.2951493 -4.2992387 -4.3005171 -4.3005075 -4.302917 -4.3084989 -4.315413]]...]
INFO - root - 2017-12-05 17:20:38.521716: step 28910, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 72h:25m:13s remains)
INFO - root - 2017-12-05 17:20:46.929592: step 28920, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 71h:14m:08s remains)
INFO - root - 2017-12-05 17:20:55.369111: step 28930, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 72h:16m:26s remains)
INFO - root - 2017-12-05 17:21:03.761290: step 28940, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 70h:25m:52s remains)
INFO - root - 2017-12-05 17:21:12.216420: step 28950, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 71h:12m:24s remains)
INFO - root - 2017-12-05 17:21:20.582654: step 28960, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 71h:32m:54s remains)
INFO - root - 2017-12-05 17:21:29.042899: step 28970, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 70h:07m:52s remains)
INFO - root - 2017-12-05 17:21:37.617527: step 28980, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 72h:02m:42s remains)
INFO - root - 2017-12-05 17:21:46.276222: step 28990, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.857 sec/batch; 72h:13m:12s remains)
INFO - root - 2017-12-05 17:21:54.924795: step 29000, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 71h:02m:03s remains)
2017-12-05 17:21:55.635182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3080559 -4.3138256 -4.317996 -4.3189158 -4.3179922 -4.3165812 -4.3141384 -4.3121548 -4.3133454 -4.3163357 -4.3156676 -4.3037772 -4.276051 -4.2363696 -4.2005734][-4.3039117 -4.308053 -4.3106709 -4.3103313 -4.3089366 -4.3071065 -4.3033175 -4.3004675 -4.3007679 -4.3031597 -4.2978792 -4.27369 -4.2261515 -4.1631851 -4.1066604][-4.3052545 -4.3058152 -4.306047 -4.3049788 -4.303875 -4.301343 -4.295156 -4.2876096 -4.2810249 -4.27746 -4.2625737 -4.2257981 -4.1596165 -4.0706992 -3.9830947][-4.3113561 -4.3099709 -4.309783 -4.3085794 -4.305469 -4.2997737 -4.2899141 -4.2764869 -4.260591 -4.2481327 -4.2257533 -4.1841702 -4.1158614 -4.0278382 -3.9447634][-4.3166256 -4.3149447 -4.3144679 -4.3103056 -4.3012967 -4.2871232 -4.2714248 -4.251555 -4.23039 -4.2195091 -4.2069726 -4.184948 -4.1433783 -4.0885019 -4.0427155][-4.31757 -4.3129749 -4.3065038 -4.2938743 -4.2748713 -4.2491145 -4.2258296 -4.1962595 -4.1702614 -4.1689553 -4.178462 -4.1882162 -4.182478 -4.1656866 -4.155437][-4.3048835 -4.2932668 -4.2760148 -4.2505875 -4.2185421 -4.1777506 -4.1451693 -4.104424 -4.0723171 -4.0867519 -4.124135 -4.1686749 -4.2028418 -4.2231355 -4.2388368][-4.2802691 -4.2569389 -4.2252746 -4.1854496 -4.1375346 -4.0806704 -4.0339565 -3.9794912 -3.9507215 -3.990484 -4.0560584 -4.1290622 -4.1975918 -4.2459941 -4.27689][-4.2525153 -4.2158637 -4.1705146 -4.119277 -4.0624042 -4.0017848 -3.9531095 -3.9062071 -3.9002 -3.9622593 -4.0388055 -4.1202168 -4.1994972 -4.2503304 -4.2740049][-4.2362461 -4.2003627 -4.1605034 -4.1192026 -4.078414 -4.0385122 -4.0134335 -3.9951687 -4.0040932 -4.0540504 -4.1102586 -4.1673412 -4.2212625 -4.2479625 -4.2466836][-4.2271609 -4.2018204 -4.1824183 -4.1649957 -4.1475878 -4.1333179 -4.1321425 -4.1346979 -4.146503 -4.1752343 -4.2022333 -4.2233963 -4.2400675 -4.2372408 -4.2107644][-4.2267547 -4.2125664 -4.2107558 -4.2118959 -4.2132158 -4.2190361 -4.2325792 -4.2419219 -4.2473168 -4.2516236 -4.2491593 -4.2414813 -4.2324395 -4.2196412 -4.1959758][-4.2464881 -4.2416134 -4.2472658 -4.2543902 -4.26226 -4.2751822 -4.2890759 -4.2924395 -4.284472 -4.2650113 -4.23967 -4.2081442 -4.1823459 -4.1774206 -4.1772676][-4.2744112 -4.2748513 -4.278399 -4.2796912 -4.2816834 -4.2881517 -4.2895226 -4.2771854 -4.2505093 -4.2066936 -4.1606779 -4.1059422 -4.0697322 -4.0886912 -4.1228542][-4.2919836 -4.2892394 -4.2849007 -4.2758369 -4.266108 -4.2591043 -4.2423539 -4.2110825 -4.1654449 -4.0997014 -4.0319018 -3.9578757 -3.9273629 -3.9779725 -4.0451183]]...]
INFO - root - 2017-12-05 17:22:04.182840: step 29010, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 71h:23m:17s remains)
INFO - root - 2017-12-05 17:22:12.671675: step 29020, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 73h:04m:32s remains)
INFO - root - 2017-12-05 17:22:21.236410: step 29030, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 70h:06m:06s remains)
INFO - root - 2017-12-05 17:22:29.682187: step 29040, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 70h:09m:17s remains)
INFO - root - 2017-12-05 17:22:38.163665: step 29050, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 71h:40m:27s remains)
INFO - root - 2017-12-05 17:22:46.731753: step 29060, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 71h:58m:43s remains)
INFO - root - 2017-12-05 17:22:55.135699: step 29070, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 74h:46m:33s remains)
INFO - root - 2017-12-05 17:23:03.655002: step 29080, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 70h:21m:15s remains)
INFO - root - 2017-12-05 17:23:12.172667: step 29090, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 69h:47m:47s remains)
INFO - root - 2017-12-05 17:23:20.642551: step 29100, loss = 2.02, batch loss = 1.96 (9.6 examples/sec; 0.837 sec/batch; 70h:30m:26s remains)
2017-12-05 17:23:21.449526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0952077 -4.0057545 -3.9379549 -3.969918 -4.065968 -4.1569672 -4.2259307 -4.268064 -4.2866454 -4.2874079 -4.2786541 -4.251255 -4.1989393 -4.1242037 -4.0386863][-4.1467614 -4.0959048 -4.0574541 -4.0715942 -4.1227379 -4.171195 -4.2070141 -4.233788 -4.2559371 -4.2734518 -4.2810445 -4.2647333 -4.2206006 -4.1484208 -4.0516896][-4.209167 -4.18896 -4.1666694 -4.1657305 -4.1784678 -4.1835251 -4.18105 -4.1859655 -4.2087288 -4.2451539 -4.27399 -4.27661 -4.2488637 -4.1856437 -4.0812511][-4.2620354 -4.2606487 -4.2462969 -4.2350383 -4.2201376 -4.1889963 -4.1515951 -4.1276956 -4.1398373 -4.1913633 -4.2470484 -4.27787 -4.2748728 -4.2286663 -4.1314421][-4.2958088 -4.302793 -4.2938604 -4.2770429 -4.2423248 -4.1847 -4.1168785 -4.058825 -4.0501056 -4.1159954 -4.2019162 -4.2644944 -4.2919383 -4.272409 -4.1998258][-4.3090672 -4.31616 -4.3083749 -4.2868662 -4.243145 -4.1712995 -4.0810528 -3.9866076 -3.9466541 -4.0277944 -4.1467156 -4.2388306 -4.2922158 -4.3002095 -4.25935][-4.3081346 -4.3113942 -4.3016806 -4.2798638 -4.2374735 -4.1624 -4.0569577 -3.9316995 -3.8510809 -3.9421585 -4.0927038 -4.210475 -4.28314 -4.31289 -4.2994695][-4.2967668 -4.2940521 -4.2839441 -4.2679687 -4.2341027 -4.1670408 -4.0631332 -3.9339612 -3.8392224 -3.92014 -4.0686398 -4.1887984 -4.2658057 -4.3059926 -4.3124166][-4.2784648 -4.2733097 -4.2647114 -4.2545719 -4.2322893 -4.1841168 -4.1059589 -4.008348 -3.938103 -3.981076 -4.0825853 -4.1751051 -4.2359095 -4.2705917 -4.2900939][-4.2555842 -4.2502489 -4.2468243 -4.2434287 -4.2359123 -4.2129536 -4.16863 -4.1071472 -4.0612721 -4.074276 -4.1228261 -4.1753235 -4.2048192 -4.2191916 -4.2408957][-4.2329741 -4.2302752 -4.2348647 -4.2420821 -4.2509327 -4.2509217 -4.2345529 -4.20126 -4.17103 -4.1682053 -4.1775303 -4.18727 -4.1786261 -4.1613054 -4.1709938][-4.226923 -4.2334833 -4.2467113 -4.2612705 -4.2763677 -4.2858582 -4.2847672 -4.26902 -4.2492552 -4.2390356 -4.2243876 -4.2009706 -4.1558022 -4.1030722 -4.0835471][-4.2395811 -4.2561107 -4.2736397 -4.2873492 -4.2997613 -4.3088369 -4.3122578 -4.3050261 -4.2920179 -4.2773371 -4.2501636 -4.207325 -4.1356015 -4.0484948 -3.9877152][-4.2632732 -4.2838511 -4.2981963 -4.3055592 -4.3109989 -4.31423 -4.3168139 -4.3137736 -4.3026819 -4.2848334 -4.255548 -4.2087226 -4.1241364 -4.0118175 -3.9076114][-4.2862725 -4.301713 -4.3080864 -4.3083525 -4.3078465 -4.3061171 -4.3053064 -4.3028188 -4.2925067 -4.2765441 -4.251492 -4.2104831 -4.1273632 -4.0085068 -3.8825281]]...]
INFO - root - 2017-12-05 17:23:29.988801: step 29110, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 72h:43m:55s remains)
INFO - root - 2017-12-05 17:23:38.482378: step 29120, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 71h:32m:11s remains)
INFO - root - 2017-12-05 17:23:47.077218: step 29130, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 71h:19m:51s remains)
INFO - root - 2017-12-05 17:23:55.449005: step 29140, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.831 sec/batch; 70h:00m:25s remains)
INFO - root - 2017-12-05 17:24:03.854541: step 29150, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 69h:47m:35s remains)
INFO - root - 2017-12-05 17:24:12.435125: step 29160, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 73h:40m:04s remains)
INFO - root - 2017-12-05 17:24:20.862899: step 29170, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 74h:13m:30s remains)
INFO - root - 2017-12-05 17:24:29.522170: step 29180, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 69h:44m:01s remains)
INFO - root - 2017-12-05 17:24:38.059218: step 29190, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 70h:55m:01s remains)
INFO - root - 2017-12-05 17:24:46.531112: step 29200, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 70h:55m:42s remains)
2017-12-05 17:24:47.291246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2967434 -4.2770987 -4.253293 -4.2294278 -4.2071748 -4.1854935 -4.1537008 -4.1381383 -4.1619563 -4.1811519 -4.173594 -4.1739116 -4.1765571 -4.176414 -4.1785355][-4.2732148 -4.2456331 -4.2099996 -4.1705475 -4.1351719 -4.106318 -4.0649328 -4.0402846 -4.0739055 -4.1060038 -4.096662 -4.0952425 -4.1029429 -4.1047173 -4.1018453][-4.2548451 -4.2186294 -4.1698337 -4.1222744 -4.0812426 -4.0493526 -4.0040135 -3.970475 -4.0110049 -4.0509157 -4.0375676 -4.0394354 -4.0572877 -4.060678 -4.0473938][-4.2414284 -4.1970057 -4.1406755 -4.0944571 -4.0597444 -4.0312996 -3.9783423 -3.9326077 -3.9748206 -4.0165691 -3.9954166 -3.9959991 -4.017777 -4.0216327 -4.0029087][-4.2247429 -4.1709223 -4.1120353 -4.074935 -4.0528269 -4.0244484 -3.9601707 -3.9059463 -3.9559078 -4.00764 -3.987735 -3.9848466 -3.9996331 -4.0043111 -3.9952419][-4.2092657 -4.146306 -4.0823803 -4.0462775 -4.02808 -3.9932151 -3.9204769 -3.8578472 -3.9178779 -3.9880862 -3.9870229 -3.9875846 -3.9991434 -4.011961 -4.0247812][-4.1982589 -4.12615 -4.0469041 -3.997962 -3.9667878 -3.9132671 -3.8156872 -3.7323575 -3.8100832 -3.9087348 -3.9325426 -3.9499743 -3.9757636 -4.0089264 -4.0497775][-4.1853771 -4.1022325 -4.00496 -3.9348495 -3.8723297 -3.7709475 -3.6101611 -3.5003729 -3.6397533 -3.8029633 -3.8655543 -3.9033427 -3.9452276 -4.0028148 -4.0571828][-4.1696286 -4.0834703 -3.9848371 -3.9060743 -3.8145828 -3.6700046 -3.4645443 -3.3490453 -3.5390666 -3.7464037 -3.8288667 -3.8715427 -3.9242139 -3.996634 -4.0567875][-4.1618228 -4.0818443 -3.9983056 -3.9372313 -3.8642352 -3.7566102 -3.6136115 -3.5403454 -3.6715238 -3.8173537 -3.8674397 -3.896883 -3.9469826 -4.0162659 -4.068964][-4.1761494 -4.1055093 -4.0331345 -3.9865279 -3.9419026 -3.8811245 -3.8036118 -3.7646358 -3.8451633 -3.9343846 -3.9587719 -3.970964 -4.0107927 -4.0610104 -4.1001987][-4.209024 -4.1451912 -4.0818028 -4.0436754 -4.0199914 -3.9887247 -3.9460142 -3.9270577 -3.9772649 -4.0316992 -4.0445595 -4.0463991 -4.0700889 -4.1031327 -4.1355686][-4.24739 -4.1895547 -4.1342149 -4.1015978 -4.0878353 -4.0740385 -4.0487819 -4.041151 -4.0769248 -4.1124606 -4.1163945 -4.1131511 -4.1252093 -4.1446433 -4.1681352][-4.2873759 -4.2420077 -4.1969032 -4.1694512 -4.1585336 -4.1490688 -4.1355271 -4.1355753 -4.1603203 -4.1796136 -4.1792688 -4.1787076 -4.1880093 -4.1983123 -4.2122808][-4.3166986 -4.2844253 -4.251905 -4.2317419 -4.2245 -4.2209182 -4.2156019 -4.2184224 -4.2303352 -4.2379103 -4.23708 -4.2389684 -4.2460561 -4.2523136 -4.2594786]]...]
INFO - root - 2017-12-05 17:24:55.816306: step 29210, loss = 2.01, batch loss = 1.95 (9.3 examples/sec; 0.862 sec/batch; 72h:39m:30s remains)
INFO - root - 2017-12-05 17:25:04.378400: step 29220, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 73h:57m:09s remains)
INFO - root - 2017-12-05 17:25:12.840218: step 29230, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 68h:49m:29s remains)
INFO - root - 2017-12-05 17:25:21.229758: step 29240, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 70h:42m:26s remains)
INFO - root - 2017-12-05 17:25:29.660737: step 29250, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 69h:34m:47s remains)
INFO - root - 2017-12-05 17:25:38.168857: step 29260, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 69h:06m:36s remains)
INFO - root - 2017-12-05 17:25:46.657136: step 29270, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 72h:54m:58s remains)
INFO - root - 2017-12-05 17:25:55.109599: step 29280, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 72h:32m:14s remains)
INFO - root - 2017-12-05 17:26:03.614748: step 29290, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 70h:00m:29s remains)
INFO - root - 2017-12-05 17:26:12.412128: step 29300, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 74h:10m:37s remains)
2017-12-05 17:26:13.203144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2894788 -4.2887444 -4.2896223 -4.2891769 -4.2861385 -4.2822204 -4.2727852 -4.262177 -4.2568822 -4.2613392 -4.2712169 -4.2808313 -4.289144 -4.2938538 -4.2989793][-4.2540789 -4.2545514 -4.2561784 -4.2561293 -4.2539515 -4.2493305 -4.2363949 -4.2198863 -4.2115154 -4.2211528 -4.2371588 -4.2513065 -4.2647285 -4.2719994 -4.2795281][-4.2158365 -4.2185669 -4.2211709 -4.2202239 -4.2186933 -4.2135777 -4.1967974 -4.173378 -4.1619062 -4.1792521 -4.202704 -4.223084 -4.240664 -4.2503738 -4.2626472][-4.1840086 -4.1889439 -4.1919241 -4.1874604 -4.1825476 -4.1740885 -4.1514516 -4.118279 -4.1078639 -4.137392 -4.1703749 -4.198308 -4.2197461 -4.2338548 -4.2537308][-4.1683278 -4.175674 -4.1776686 -4.16794 -4.1554556 -4.136272 -4.0976887 -4.0472994 -4.0413179 -4.0877709 -4.1312408 -4.1676936 -4.1944294 -4.2162051 -4.2465191][-4.1633506 -4.172667 -4.1727571 -4.1576743 -4.1337037 -4.0975142 -4.0366082 -3.9642491 -3.9635525 -4.0285964 -4.0886312 -4.1377525 -4.169076 -4.1986942 -4.2406788][-4.1718554 -4.1838622 -4.1815515 -4.1580529 -4.1208181 -4.0690985 -3.9890263 -3.9018705 -3.9093208 -3.9896059 -4.0647411 -4.1231575 -4.1582217 -4.1935387 -4.2436285][-4.189168 -4.2050991 -4.200551 -4.1663594 -4.1155009 -4.0559511 -3.9756453 -3.9027064 -3.9200823 -3.9977443 -4.0711427 -4.1288481 -4.1673651 -4.205452 -4.257185][-4.2190275 -4.2376747 -4.2298937 -4.1861892 -4.124567 -4.0641923 -4.004796 -3.9687853 -3.9984312 -4.061677 -4.1220837 -4.1706343 -4.2035017 -4.2357411 -4.2779517][-4.2471466 -4.2674756 -4.2582994 -4.2140732 -4.1512847 -4.0993338 -4.0640731 -4.0598869 -4.095468 -4.1451554 -4.1914105 -4.2272587 -4.2496839 -4.2707877 -4.3001032][-4.2565446 -4.2763329 -4.2674923 -4.2285705 -4.17284 -4.1342564 -4.1193767 -4.1329136 -4.1674843 -4.2051468 -4.2383189 -4.2625775 -4.2756052 -4.2884154 -4.309525][-4.2576447 -4.2758636 -4.2676034 -4.2340717 -4.1907544 -4.1689825 -4.1707625 -4.1948953 -4.2263927 -4.2500567 -4.2675505 -4.2813926 -4.2866716 -4.2934589 -4.31071][-4.2580156 -4.2736292 -4.2655406 -4.2365394 -4.204947 -4.201951 -4.2211442 -4.2493315 -4.2728181 -4.2825723 -4.2872653 -4.2931767 -4.2931828 -4.296464 -4.3116875][-4.259922 -4.2733541 -4.2674675 -4.2441344 -4.2238183 -4.2335691 -4.2608552 -4.287375 -4.3027496 -4.3040795 -4.3019204 -4.3024106 -4.2993059 -4.300631 -4.3137894][-4.2741208 -4.2876511 -4.283566 -4.2652435 -4.2534041 -4.2654982 -4.2892761 -4.3099818 -4.3194876 -4.3164926 -4.3121877 -4.3094673 -4.3054113 -4.3064733 -4.3172755]]...]
INFO - root - 2017-12-05 17:26:21.709256: step 29310, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 72h:28m:22s remains)
INFO - root - 2017-12-05 17:26:30.422432: step 29320, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 72h:21m:55s remains)
INFO - root - 2017-12-05 17:26:38.948098: step 29330, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 71h:22m:00s remains)
INFO - root - 2017-12-05 17:26:47.349850: step 29340, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 70h:09m:10s remains)
INFO - root - 2017-12-05 17:26:55.805250: step 29350, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 72h:27m:47s remains)
INFO - root - 2017-12-05 17:27:04.372562: step 29360, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.823 sec/batch; 69h:16m:57s remains)
INFO - root - 2017-12-05 17:27:12.838612: step 29370, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 71h:10m:16s remains)
INFO - root - 2017-12-05 17:27:21.350801: step 29380, loss = 2.05, batch loss = 1.99 (10.2 examples/sec; 0.783 sec/batch; 65h:53m:12s remains)
INFO - root - 2017-12-05 17:27:29.778899: step 29390, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 74h:16m:08s remains)
INFO - root - 2017-12-05 17:27:38.310217: step 29400, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 72h:56m:21s remains)
2017-12-05 17:27:39.063521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2272277 -4.2461796 -4.2620559 -4.2666769 -4.2635345 -4.2552514 -4.2535753 -4.2544117 -4.2561407 -4.2641869 -4.2755466 -4.2907352 -4.3093824 -4.322506 -4.3350568][-4.1879554 -4.2203565 -4.2425113 -4.2453427 -4.2345133 -4.2169 -4.2114124 -4.2144632 -4.2169776 -4.2282252 -4.2443776 -4.2663188 -4.290421 -4.3058128 -4.3222923][-4.1619482 -4.2004128 -4.2209263 -4.2172594 -4.1983366 -4.1760545 -4.1663947 -4.1726494 -4.1760445 -4.1902804 -4.2128563 -4.2474828 -4.2789035 -4.2957535 -4.3125935][-4.156251 -4.1926794 -4.2057581 -4.1901655 -4.1607285 -4.1346407 -4.124753 -4.1332507 -4.1408792 -4.160006 -4.1883779 -4.2328415 -4.272367 -4.2919755 -4.3103857][-4.1571755 -4.1891031 -4.1918755 -4.1638913 -4.1217308 -4.0906687 -4.08582 -4.1016808 -4.119112 -4.1439424 -4.1738286 -4.217926 -4.2624865 -4.2897315 -4.3126464][-4.1484785 -4.1757011 -4.17154 -4.13039 -4.0742292 -4.0384107 -4.0431433 -4.0706577 -4.1023946 -4.1396432 -4.1702642 -4.2101312 -4.2575836 -4.28906 -4.3146377][-4.1156688 -4.1448469 -4.1385078 -4.0896111 -4.0169272 -3.9735038 -3.9844272 -4.0208569 -4.0651865 -4.1206331 -4.1628838 -4.2065182 -4.2577467 -4.2878623 -4.3135533][-4.0583148 -4.08735 -4.0757413 -4.0196409 -3.9349632 -3.8900924 -3.9068296 -3.9494193 -4.0096312 -4.0872459 -4.1514997 -4.2061176 -4.2601123 -4.2866735 -4.310246][-4.0181351 -4.0367808 -4.0126433 -3.9431603 -3.8581085 -3.8255765 -3.8576396 -3.908519 -3.9830885 -4.0765491 -4.1530495 -4.207891 -4.2581015 -4.2827516 -4.3057823][-4.0246778 -4.0256705 -3.9847326 -3.9095685 -3.8349586 -3.8195627 -3.8676066 -3.9280064 -4.0074654 -4.0997458 -4.1698861 -4.2149439 -4.2590327 -4.2844977 -4.3061547][-4.0686016 -4.0537705 -4.0049133 -3.9360008 -3.8780324 -3.8754711 -3.9287596 -3.9914677 -4.0687795 -4.1521316 -4.2082129 -4.242558 -4.2781243 -4.299849 -4.3163996][-4.1348829 -4.1154675 -4.0659513 -4.0092268 -3.9664879 -3.9702182 -4.0194616 -4.0770063 -4.1475425 -4.2164288 -4.2586555 -4.280447 -4.3053613 -4.3204226 -4.3293309][-4.1961164 -4.1774812 -4.1361785 -4.0927691 -4.0629525 -4.0697722 -4.1112261 -4.1606946 -4.2180915 -4.2695642 -4.29895 -4.3119469 -4.3259358 -4.3333125 -4.3370786][-4.2442274 -4.2296343 -4.2009106 -4.1715961 -4.1546855 -4.1614566 -4.1912217 -4.2263775 -4.2677979 -4.3029208 -4.3208923 -4.3258195 -4.3311949 -4.3352523 -4.3378425][-4.27967 -4.2694554 -4.2527218 -4.2358871 -4.2271686 -4.2316651 -4.2508664 -4.2730012 -4.296648 -4.31697 -4.3260241 -4.3265119 -4.3283114 -4.3315253 -4.3347211]]...]
INFO - root - 2017-12-05 17:27:47.629871: step 29410, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.839 sec/batch; 70h:36m:12s remains)
INFO - root - 2017-12-05 17:27:56.124418: step 29420, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 72h:10m:26s remains)
INFO - root - 2017-12-05 17:28:04.660548: step 29430, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 70h:43m:02s remains)
INFO - root - 2017-12-05 17:28:13.079315: step 29440, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 71h:10m:22s remains)
INFO - root - 2017-12-05 17:28:21.563479: step 29450, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 70h:38m:44s remains)
INFO - root - 2017-12-05 17:28:30.107620: step 29460, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 70h:04m:51s remains)
INFO - root - 2017-12-05 17:28:38.633220: step 29470, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 72h:25m:29s remains)
INFO - root - 2017-12-05 17:28:47.048650: step 29480, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 71h:00m:12s remains)
INFO - root - 2017-12-05 17:28:55.307630: step 29490, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.829 sec/batch; 69h:46m:25s remains)
INFO - root - 2017-12-05 17:29:03.521153: step 29500, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 69h:57m:43s remains)
2017-12-05 17:29:04.310891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1850209 -4.1567922 -4.1524673 -4.1502323 -4.1389585 -4.1292305 -4.1247659 -4.12304 -4.1235838 -4.1276278 -4.1288404 -4.1210675 -4.1142778 -4.1132131 -4.1183968][-4.1573215 -4.1195555 -4.1080947 -4.1101265 -4.1091042 -4.1046181 -4.1024661 -4.1016226 -4.1025605 -4.1090884 -4.1099205 -4.0972085 -4.0882254 -4.0901146 -4.0984483][-4.156323 -4.1147404 -4.0973349 -4.1022892 -4.1070409 -4.1039677 -4.1011553 -4.1001592 -4.10276 -4.1123419 -4.1161785 -4.1019096 -4.0925107 -4.0961232 -4.1077094][-4.1722493 -4.13523 -4.1139822 -4.1133108 -4.116179 -4.1109571 -4.1044493 -4.0979033 -4.0967007 -4.1105042 -4.1217942 -4.1120634 -4.103766 -4.1071453 -4.1204419][-4.1801238 -4.1473575 -4.1209588 -4.1084881 -4.0982556 -4.0793872 -4.0593867 -4.0443754 -4.038991 -4.0613379 -4.0865078 -4.0866528 -4.0881081 -4.0983744 -4.1208997][-4.16137 -4.13069 -4.1010985 -4.0804453 -4.05896 -4.0239019 -3.9918952 -3.9771445 -3.9809625 -4.0159888 -4.0540066 -4.065094 -4.0808697 -4.1037784 -4.1344337][-4.1397767 -4.1073918 -4.0776072 -4.0553389 -4.0318136 -3.9911344 -3.9586535 -3.958349 -3.9791324 -4.0238433 -4.0694604 -4.0949197 -4.1222992 -4.1541066 -4.183948][-4.1468372 -4.1182442 -4.0901728 -4.0666947 -4.0412025 -4.0026522 -3.9777815 -3.9911633 -4.0220814 -4.0630574 -4.1034384 -4.1326265 -4.1586156 -4.184536 -4.2065735][-4.1672249 -4.1398492 -4.1135745 -4.0957174 -4.0757084 -4.0447416 -4.0309033 -4.0490513 -4.0804029 -4.112884 -4.1444917 -4.1642551 -4.174665 -4.1858974 -4.1988225][-4.1871929 -4.1600351 -4.1347342 -4.1215305 -4.1048989 -4.0793295 -4.0723848 -4.0929842 -4.1232605 -4.149509 -4.1686549 -4.1780519 -4.1763859 -4.1763248 -4.1822453][-4.2056723 -4.1784716 -4.1565619 -4.1500907 -4.1410294 -4.1232939 -4.1211548 -4.1420774 -4.1686268 -4.1842046 -4.189764 -4.1907897 -4.1823268 -4.174159 -4.1733494][-4.2216105 -4.1966076 -4.1818347 -4.1832833 -4.1823888 -4.1754875 -4.1795192 -4.1956716 -4.2109604 -4.2133136 -4.2092171 -4.206079 -4.1974058 -4.1862822 -4.1804252][-4.2461333 -4.2277293 -4.218667 -4.2224779 -4.2233696 -4.221745 -4.2290239 -4.2399082 -4.2437859 -4.2365241 -4.2279325 -4.223691 -4.2176704 -4.2067456 -4.19852][-4.2844806 -4.2741756 -4.2676659 -4.2675958 -4.2645407 -4.2603455 -4.2640495 -4.2698131 -4.2686305 -4.2607622 -4.255465 -4.2557931 -4.2555633 -4.2492642 -4.2420659][-4.3219671 -4.31677 -4.3122883 -4.3091106 -4.3034315 -4.2971458 -4.2954397 -4.29692 -4.2972822 -4.2946806 -4.2937007 -4.2959366 -4.2982144 -4.2973113 -4.2939205]]...]
INFO - root - 2017-12-05 17:29:12.792972: step 29510, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 72h:13m:37s remains)
INFO - root - 2017-12-05 17:29:21.148608: step 29520, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 71h:28m:53s remains)
INFO - root - 2017-12-05 17:29:29.684905: step 29530, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 69h:16m:29s remains)
INFO - root - 2017-12-05 17:29:38.147927: step 29540, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 72h:59m:02s remains)
INFO - root - 2017-12-05 17:29:46.572606: step 29550, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 70h:46m:10s remains)
INFO - root - 2017-12-05 17:29:55.124915: step 29560, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 73h:51m:18s remains)
INFO - root - 2017-12-05 17:30:03.674371: step 29570, loss = 2.11, batch loss = 2.05 (9.1 examples/sec; 0.883 sec/batch; 74h:17m:46s remains)
INFO - root - 2017-12-05 17:30:12.102637: step 29580, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 70h:08m:59s remains)
INFO - root - 2017-12-05 17:30:20.657028: step 29590, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 71h:32m:49s remains)
INFO - root - 2017-12-05 17:30:29.128742: step 29600, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 72h:29m:36s remains)
2017-12-05 17:30:29.834062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1794271 -4.1928153 -4.1940613 -4.1999679 -4.2103491 -4.2159457 -4.2205243 -4.2275515 -4.2368841 -4.2385187 -4.2275033 -4.2103519 -4.1926117 -4.1753006 -4.169929][-4.1877561 -4.2050309 -4.21027 -4.2165813 -4.2216616 -4.2201633 -4.221911 -4.2314911 -4.2423449 -4.2440181 -4.2327342 -4.2148848 -4.1922474 -4.17116 -4.1669765][-4.2225485 -4.2372417 -4.2398295 -4.2428665 -4.2394161 -4.2279143 -4.2199912 -4.2227855 -4.2321124 -4.2371049 -4.2352 -4.2249575 -4.2041812 -4.18502 -4.1867523][-4.2530422 -4.264977 -4.2634459 -4.25765 -4.2419729 -4.2170005 -4.2004485 -4.1988907 -4.211884 -4.22957 -4.2470989 -4.2565827 -4.2482948 -4.2324357 -4.2310524][-4.2528787 -4.2601852 -4.2546344 -4.2363706 -4.2039585 -4.1647468 -4.1410694 -4.1410136 -4.16995 -4.215992 -4.263052 -4.2958121 -4.3013225 -4.2901492 -4.281795][-4.222106 -4.22614 -4.2150397 -4.17911 -4.1229639 -4.0613508 -4.0161252 -4.0119328 -4.066041 -4.1503239 -4.2309542 -4.2854943 -4.3069372 -4.3073597 -4.2994847][-4.1680722 -4.1637917 -4.1387119 -4.0814753 -3.9923859 -3.8842561 -3.7914667 -3.7791011 -3.8773432 -4.0178461 -4.1413765 -4.2284536 -4.27523 -4.2931919 -4.2934475][-4.1374774 -4.1161771 -4.0610418 -3.9672661 -3.8323207 -3.6636977 -3.5016794 -3.4676652 -3.6256137 -3.8368509 -4.0132828 -4.1427741 -4.2200174 -4.256393 -4.2640748][-4.1542344 -4.1246424 -4.0586486 -3.962877 -3.8306725 -3.6646862 -3.5032411 -3.4612541 -3.6050978 -3.7995784 -3.9639728 -4.0946641 -4.1765528 -4.2127452 -4.2242932][-4.2091541 -4.1796632 -4.1242867 -4.0517688 -3.9562595 -3.8520396 -3.7626948 -3.7453423 -3.8319831 -3.9482734 -4.0459266 -4.1325574 -4.1878881 -4.2064338 -4.2104993][-4.2623243 -4.236939 -4.1957026 -4.146534 -4.0792708 -4.0147524 -3.9752617 -3.982125 -4.0398011 -4.1077094 -4.1571736 -4.2009635 -4.2253995 -4.2224364 -4.2145281][-4.2700968 -4.2471828 -4.2213936 -4.1944838 -4.15134 -4.1118183 -4.1004391 -4.117579 -4.1589279 -4.1992493 -4.2160392 -4.2326765 -4.2389913 -4.2263007 -4.2097845][-4.2549105 -4.234416 -4.2174177 -4.200902 -4.1714964 -4.1507497 -4.1584496 -4.1833735 -4.21521 -4.2402596 -4.2404628 -4.2408895 -4.2377934 -4.2245259 -4.2049532][-4.2460365 -4.2299609 -4.2223163 -4.2115393 -4.188292 -4.1774774 -4.1960273 -4.2249346 -4.2525635 -4.27055 -4.2660155 -4.2618327 -4.257772 -4.2463951 -4.2261724][-4.2475014 -4.2361016 -4.2320642 -4.2232881 -4.2015 -4.1905537 -4.2056642 -4.2336016 -4.2622256 -4.2846813 -4.2866659 -4.2849059 -4.2824292 -4.269794 -4.2488675]]...]
INFO - root - 2017-12-05 17:30:38.273530: step 29610, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.826 sec/batch; 69h:30m:45s remains)
INFO - root - 2017-12-05 17:30:46.810117: step 29620, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 72h:35m:30s remains)
INFO - root - 2017-12-05 17:30:55.244556: step 29630, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 71h:14m:16s remains)
INFO - root - 2017-12-05 17:31:03.547857: step 29640, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.840 sec/batch; 70h:39m:38s remains)
INFO - root - 2017-12-05 17:31:12.090495: step 29650, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 73h:12m:51s remains)
INFO - root - 2017-12-05 17:31:20.618101: step 29660, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.826 sec/batch; 69h:27m:13s remains)
INFO - root - 2017-12-05 17:31:29.327853: step 29670, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 73h:34m:36s remains)
INFO - root - 2017-12-05 17:31:37.989497: step 29680, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 71h:43m:55s remains)
INFO - root - 2017-12-05 17:31:46.611382: step 29690, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 72h:12m:49s remains)
INFO - root - 2017-12-05 17:31:55.235741: step 29700, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 72h:30m:14s remains)
2017-12-05 17:31:55.973794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3433862 -4.3291287 -4.3160739 -4.2931395 -4.2553453 -4.198101 -4.1323514 -4.0962796 -4.0899186 -4.1079149 -4.126194 -4.1263 -4.1308289 -4.1506076 -4.1897736][-4.3420572 -4.3249288 -4.3113093 -4.2918043 -4.2583818 -4.1984673 -4.1212883 -4.0772305 -4.0736918 -4.1029186 -4.1338468 -4.1363235 -4.1339746 -4.1519008 -4.1957021][-4.3405147 -4.3205051 -4.3071084 -4.2923508 -4.264986 -4.20512 -4.114614 -4.0589175 -4.0597239 -4.1048808 -4.1525373 -4.1587906 -4.1507082 -4.164506 -4.2085757][-4.3395042 -4.3162575 -4.3023643 -4.2916946 -4.2687316 -4.2096152 -4.1083035 -4.0375385 -4.043642 -4.1104078 -4.175374 -4.1884165 -4.1796465 -4.1917048 -4.2305226][-4.33878 -4.3121471 -4.296361 -4.2889457 -4.2692556 -4.2096596 -4.1003628 -4.0185008 -4.030899 -4.1145282 -4.1972022 -4.2214994 -4.2140565 -4.2225928 -4.2513261][-4.3388381 -4.310699 -4.292963 -4.2869639 -4.269567 -4.2086496 -4.0914016 -4.0001512 -4.0157595 -4.1142745 -4.2151079 -4.2537637 -4.2498178 -4.2537565 -4.2733169][-4.3395061 -4.3118176 -4.2928843 -4.2847757 -4.2680779 -4.2063737 -4.0815163 -3.978102 -3.9945002 -4.1100039 -4.226047 -4.2774005 -4.2794704 -4.2795224 -4.2895317][-4.340457 -4.3141403 -4.2945228 -4.2825174 -4.2654037 -4.2046556 -4.0795078 -3.9662991 -3.9701016 -4.0919275 -4.2182288 -4.2784348 -4.2917295 -4.2922149 -4.2935104][-4.3410192 -4.3160791 -4.2958083 -4.2799673 -4.2631593 -4.2083879 -4.0947351 -3.9816327 -3.9581254 -4.0682898 -4.1971226 -4.2646842 -4.2883911 -4.2925444 -4.2893019][-4.3406658 -4.3161688 -4.2960234 -4.2781839 -4.262382 -4.2160082 -4.1224227 -4.0240417 -3.9799047 -4.0571122 -4.1713963 -4.2436376 -4.277894 -4.2864075 -4.2832851][-4.339695 -4.3144116 -4.2947588 -4.2779617 -4.2630935 -4.2257261 -4.1555839 -4.08479 -4.037643 -4.0764203 -4.1569014 -4.2186837 -4.26173 -4.2774124 -4.276845][-4.3384118 -4.3115563 -4.2919416 -4.2776265 -4.2645383 -4.2356606 -4.187459 -4.1455541 -4.1085892 -4.1212592 -4.1631832 -4.2005324 -4.2434688 -4.2650051 -4.2710261][-4.3380728 -4.3091116 -4.2876077 -4.2734346 -4.2626023 -4.2424922 -4.212079 -4.1938391 -4.1722746 -4.1740227 -4.1883907 -4.2001882 -4.2330756 -4.2539921 -4.2649288][-4.3385839 -4.3076639 -4.2819562 -4.2663217 -4.2556396 -4.2421403 -4.2233467 -4.2193074 -4.2132 -4.2177434 -4.2209449 -4.2139 -4.2303267 -4.2478747 -4.2605343][-4.3397088 -4.3082433 -4.2778282 -4.2589741 -4.2460456 -4.2348709 -4.2226324 -4.2264628 -4.231967 -4.2429924 -4.2469716 -4.2340307 -4.2360897 -4.2452412 -4.2553382]]...]
INFO - root - 2017-12-05 17:32:04.554276: step 29710, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 72h:22m:35s remains)
INFO - root - 2017-12-05 17:32:13.002263: step 29720, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 71h:56m:35s remains)
INFO - root - 2017-12-05 17:32:21.522631: step 29730, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 72h:44m:40s remains)
INFO - root - 2017-12-05 17:32:30.086327: step 29740, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 73h:07m:57s remains)
INFO - root - 2017-12-05 17:32:38.676474: step 29750, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 74h:23m:38s remains)
INFO - root - 2017-12-05 17:32:47.256145: step 29760, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 71h:59m:14s remains)
INFO - root - 2017-12-05 17:32:55.802164: step 29770, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 70h:34m:00s remains)
INFO - root - 2017-12-05 17:33:04.379256: step 29780, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 72h:45m:45s remains)
INFO - root - 2017-12-05 17:33:12.900347: step 29790, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 70h:07m:23s remains)
INFO - root - 2017-12-05 17:33:21.433462: step 29800, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 70h:16m:28s remains)
2017-12-05 17:33:22.239655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3054967 -4.3003688 -4.2975726 -4.29842 -4.3024583 -4.3082147 -4.3129911 -4.3153653 -4.3124466 -4.3045855 -4.2948871 -4.2843671 -4.2755179 -4.2695513 -4.2668724][-4.2780809 -4.2759366 -4.2753792 -4.2771792 -4.2815561 -4.28894 -4.2973137 -4.3035851 -4.3014517 -4.2923965 -4.2821445 -4.2692447 -4.2559962 -4.244534 -4.2376456][-4.2473392 -4.2458272 -4.2471142 -4.2514019 -4.2573419 -4.2646656 -4.271183 -4.2765956 -4.275774 -4.2691197 -4.2629075 -4.2518392 -4.2380161 -4.2244182 -4.2152786][-4.2262988 -4.2201171 -4.2186618 -4.2226171 -4.2270141 -4.228466 -4.22785 -4.2293134 -4.2366271 -4.2430873 -4.2512469 -4.2496405 -4.2407589 -4.2294879 -4.2206373][-4.2207603 -4.2024546 -4.18574 -4.1767406 -4.1707191 -4.1581616 -4.1435146 -4.1434913 -4.1701488 -4.203094 -4.2339091 -4.2469258 -4.2514319 -4.2513165 -4.2454648][-4.2212553 -4.1837888 -4.143806 -4.108397 -4.0787616 -4.0375867 -4.0022902 -4.0104132 -4.0718026 -4.1421022 -4.1971536 -4.2274508 -4.2495689 -4.2634215 -4.2648377][-4.2175083 -4.1653676 -4.101078 -4.0293956 -3.9617341 -3.8855574 -3.8374429 -3.871989 -3.9692757 -4.0652471 -4.135057 -4.18035 -4.2209234 -4.2483873 -4.2605295][-4.2212105 -4.1695781 -4.0966616 -4.0005484 -3.9025249 -3.8108983 -3.7740541 -3.8325074 -3.9372749 -4.0262337 -4.0906014 -4.1448774 -4.1991296 -4.2381463 -4.259378][-4.2358165 -4.2065935 -4.1501346 -4.0623646 -3.9683867 -3.8952246 -3.8811126 -3.9308746 -4.0026655 -4.060101 -4.1059713 -4.155889 -4.2077231 -4.2456512 -4.2683792][-4.2387867 -4.2381783 -4.2083435 -4.1516695 -4.0916123 -4.0486183 -4.0445275 -4.0735178 -4.108891 -4.1403985 -4.173749 -4.212635 -4.2509 -4.2774787 -4.2938571][-4.224566 -4.2464957 -4.2440591 -4.2223415 -4.1977253 -4.1795774 -4.1800513 -4.1948476 -4.2136917 -4.2361631 -4.2609043 -4.2875347 -4.3093586 -4.3210058 -4.3286643][-4.229825 -4.2567563 -4.2690697 -4.2704425 -4.2678494 -4.26591 -4.2683315 -4.2769265 -4.2896719 -4.3081145 -4.3287525 -4.3447447 -4.3540769 -4.3537827 -4.3540511][-4.2667642 -4.286303 -4.3002739 -4.3114123 -4.3179564 -4.321074 -4.3214307 -4.3248277 -4.3345213 -4.3485656 -4.36194 -4.3668165 -4.3669171 -4.364624 -4.3634491][-4.3133817 -4.3267269 -4.33512 -4.3448987 -4.3516231 -4.3525686 -4.3492818 -4.3484778 -4.3534565 -4.361517 -4.3658018 -4.3632503 -4.3604422 -4.3592229 -4.358531][-4.3507204 -4.3565321 -4.3584232 -4.3629613 -4.3659744 -4.3648396 -4.3603053 -4.3567028 -4.3562427 -4.3574643 -4.3573046 -4.3529 -4.3493023 -4.3476744 -4.346837]]...]
INFO - root - 2017-12-05 17:33:30.750264: step 29810, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 74h:36m:15s remains)
INFO - root - 2017-12-05 17:33:39.250916: step 29820, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 71h:50m:23s remains)
INFO - root - 2017-12-05 17:33:47.877756: step 29830, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 76h:42m:26s remains)
INFO - root - 2017-12-05 17:33:56.372795: step 29840, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 72h:51m:28s remains)
INFO - root - 2017-12-05 17:34:05.081716: step 29850, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 71h:40m:19s remains)
INFO - root - 2017-12-05 17:34:13.584455: step 29860, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 72h:40m:36s remains)
INFO - root - 2017-12-05 17:34:22.016981: step 29870, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 69h:46m:19s remains)
INFO - root - 2017-12-05 17:34:30.566241: step 29880, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.876 sec/batch; 73h:38m:34s remains)
INFO - root - 2017-12-05 17:34:38.964415: step 29890, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 69h:30m:25s remains)
INFO - root - 2017-12-05 17:34:47.655515: step 29900, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 74h:25m:08s remains)
2017-12-05 17:34:48.426183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1987333 -4.2086353 -4.2392054 -4.2482891 -4.2240944 -4.1871071 -4.1527238 -4.1583967 -4.197587 -4.2371483 -4.2480788 -4.2184906 -4.1565518 -4.1066341 -4.1217852][-4.2181835 -4.2207928 -4.2382841 -4.23841 -4.2069149 -4.1611285 -4.1171346 -4.1247363 -4.1777725 -4.2292023 -4.2446823 -4.2179432 -4.1577873 -4.1049109 -4.1184735][-4.2269344 -4.2264233 -4.2320766 -4.2228642 -4.184145 -4.1298704 -4.0782552 -4.0870824 -4.1538153 -4.219274 -4.2425609 -4.22443 -4.1693916 -4.111577 -4.1184654][-4.2246776 -4.2265577 -4.2260346 -4.2094431 -4.159266 -4.0953293 -4.03495 -4.0452018 -4.1330214 -4.2158732 -4.2473845 -4.2357631 -4.1847439 -4.1238403 -4.121655][-4.2102127 -4.2168541 -4.2201681 -4.2029471 -4.1436281 -4.066133 -3.9934425 -4.0056477 -4.1159344 -4.2141881 -4.2532821 -4.2441392 -4.1964493 -4.1349053 -4.126214][-4.1919422 -4.2073035 -4.2157741 -4.1983171 -4.1320043 -4.0373573 -3.9389124 -3.9536145 -4.0896316 -4.2086415 -4.2584028 -4.253335 -4.2107992 -4.1495447 -4.1338716][-4.1751184 -4.1986265 -4.214118 -4.2019753 -4.1359158 -4.02337 -3.8907676 -3.8984909 -4.0579772 -4.1959372 -4.2561679 -4.2594528 -4.2255764 -4.164144 -4.1403918][-4.1571794 -4.1820211 -4.2064443 -4.2070928 -4.155232 -4.0487747 -3.9097083 -3.8941717 -4.0405064 -4.1790152 -4.2448535 -4.2584815 -4.2333341 -4.1738114 -4.1428423][-4.14456 -4.1613774 -4.1910763 -4.2056518 -4.1779051 -4.0984831 -3.9889727 -3.9613664 -4.0610847 -4.172111 -4.2365165 -4.2560964 -4.2364559 -4.1770067 -4.1380286][-4.1546855 -4.154283 -4.1762028 -4.1949549 -4.1840634 -4.1331096 -4.0568085 -4.0302372 -4.0932469 -4.1773076 -4.2356238 -4.2568045 -4.2391796 -4.1784029 -4.1332765][-4.1848693 -4.1710234 -4.1763163 -4.18548 -4.17907 -4.1513367 -4.105001 -4.0820785 -4.1188931 -4.1824007 -4.2336817 -4.2565656 -4.2435341 -4.1853747 -4.1333442][-4.2127843 -4.1939669 -4.1866741 -4.1830072 -4.1737828 -4.1579118 -4.1338816 -4.1171417 -4.1340928 -4.1789384 -4.2247715 -4.2505965 -4.247107 -4.1961837 -4.1367512][-4.2285008 -4.2069569 -4.1910157 -4.180634 -4.1734056 -4.1667676 -4.1588888 -4.1465731 -4.1469011 -4.1729469 -4.2107043 -4.2418289 -4.24932 -4.2058787 -4.1398435][-4.2335796 -4.2116227 -4.1879 -4.1767 -4.1747894 -4.1788206 -4.1825128 -4.170167 -4.1576791 -4.166347 -4.194098 -4.2296524 -4.2460866 -4.209528 -4.1421924][-4.2253785 -4.2030859 -4.1803823 -4.1737022 -4.1785817 -4.1905937 -4.2026539 -4.1909208 -4.170435 -4.1667314 -4.18071 -4.2142096 -4.235671 -4.2103596 -4.1543751]]...]
INFO - root - 2017-12-05 17:34:56.894916: step 29910, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.844 sec/batch; 70h:58m:49s remains)
INFO - root - 2017-12-05 17:35:05.441047: step 29920, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 72h:39m:59s remains)
INFO - root - 2017-12-05 17:35:13.877608: step 29930, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 72h:22m:00s remains)
INFO - root - 2017-12-05 17:35:22.285631: step 29940, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 72h:54m:27s remains)
INFO - root - 2017-12-05 17:35:31.008851: step 29950, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 71h:55m:33s remains)
INFO - root - 2017-12-05 17:35:39.841336: step 29960, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 72h:38m:33s remains)
INFO - root - 2017-12-05 17:35:48.405338: step 29970, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 71h:41m:16s remains)
INFO - root - 2017-12-05 17:35:57.034472: step 29980, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 72h:40m:23s remains)
INFO - root - 2017-12-05 17:36:05.641184: step 29990, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 71h:05m:25s remains)
INFO - root - 2017-12-05 17:36:14.148922: step 30000, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 71h:37m:00s remains)
2017-12-05 17:36:14.968737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1341166 -4.1470146 -4.1516318 -4.1541338 -4.1623039 -4.1759486 -4.1885486 -4.19567 -4.19189 -4.1841569 -4.1927528 -4.214807 -4.2346759 -4.2443585 -4.2358508][-4.1467552 -4.1680403 -4.1854372 -4.1973124 -4.2148256 -4.2363319 -4.2538509 -4.2651706 -4.2647119 -4.258873 -4.2661533 -4.2854929 -4.3006034 -4.3039608 -4.2919927][-4.1466932 -4.1741424 -4.2027135 -4.2237182 -4.2425385 -4.2627234 -4.2785325 -4.2905283 -4.2985291 -4.3032184 -4.3134332 -4.3265791 -4.3303728 -4.3211575 -4.3029232][-4.1529384 -4.1753745 -4.20258 -4.2238479 -4.2368817 -4.2469263 -4.252666 -4.2624583 -4.2787356 -4.2972412 -4.3138313 -4.3217778 -4.312057 -4.289474 -4.2641821][-4.1792917 -4.1910949 -4.201798 -4.2060809 -4.2010708 -4.1923223 -4.1881409 -4.1939907 -4.2107425 -4.2341776 -4.2555742 -4.2661366 -4.2581882 -4.237762 -4.2138577][-4.2139111 -4.2086172 -4.1944242 -4.1741419 -4.1470652 -4.1210012 -4.1086907 -4.1060052 -4.1165195 -4.1422958 -4.1697693 -4.1899171 -4.1950331 -4.1872015 -4.1718659][-4.2404566 -4.2241435 -4.1930342 -4.1536694 -4.1028104 -4.0522304 -4.0202193 -4.0002823 -4.0067859 -4.0393977 -4.0800066 -4.1157327 -4.1388226 -4.1484394 -4.1452389][-4.2526689 -4.2322474 -4.1992106 -4.1547909 -4.085834 -4.0106897 -3.9540486 -3.9215362 -3.9262142 -3.9646447 -4.0136957 -4.061821 -4.0997176 -4.1236506 -4.1311822][-4.2510338 -4.232553 -4.2045317 -4.1616 -4.086082 -3.9990993 -3.9355681 -3.9081213 -3.9192827 -3.9609261 -4.0114703 -4.055553 -4.0872207 -4.1066456 -4.1138058][-4.2399769 -4.2180166 -4.1845217 -4.142076 -4.0718284 -3.9958625 -3.9501662 -3.9437852 -3.9731035 -4.0186253 -4.0581217 -4.0728135 -4.0679755 -4.0619826 -4.06527][-4.2052345 -4.1733861 -4.1303024 -4.0950966 -4.0498214 -4.0096965 -3.9973946 -4.0098233 -4.0421205 -4.0769954 -4.0898023 -4.0652242 -4.0260158 -4.0154815 -4.0325546][-4.1554456 -4.1319318 -4.1022654 -4.0882397 -4.0708 -4.0578122 -4.0637112 -4.0831518 -4.1098008 -4.1274214 -4.1166244 -4.0700359 -4.0265965 -4.0318627 -4.0658445][-4.1306963 -4.1428046 -4.140574 -4.1403241 -4.13334 -4.1260991 -4.1339235 -4.154881 -4.1767216 -4.1838531 -4.1657128 -4.1217475 -4.0888281 -4.098527 -4.1279268][-4.1405716 -4.1792789 -4.1950288 -4.2007847 -4.1963296 -4.1882715 -4.1909604 -4.2060871 -4.2226362 -4.2254276 -4.2071795 -4.1743608 -4.1526833 -4.1540012 -4.1667571][-4.1399193 -4.1907859 -4.2125444 -4.2190533 -4.2170615 -4.2101283 -4.2109361 -4.2206049 -4.2315288 -4.2319603 -4.2191377 -4.1978359 -4.1822395 -4.1758704 -4.1759963]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 17:36:24.098519: step 30010, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 71h:30m:26s remains)
INFO - root - 2017-12-05 17:36:32.686749: step 30020, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 68h:46m:51s remains)
INFO - root - 2017-12-05 17:36:41.281097: step 30030, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.867 sec/batch; 72h:51m:45s remains)
INFO - root - 2017-12-05 17:36:49.733982: step 30040, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 72h:42m:06s remains)
INFO - root - 2017-12-05 17:36:58.264611: step 30050, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 68h:37m:16s remains)
INFO - root - 2017-12-05 17:37:06.799178: step 30060, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.850 sec/batch; 71h:22m:05s remains)
INFO - root - 2017-12-05 17:37:15.485738: step 30070, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 74h:48m:39s remains)
INFO - root - 2017-12-05 17:37:23.983920: step 30080, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 70h:33m:21s remains)
INFO - root - 2017-12-05 17:37:32.541449: step 30090, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 75h:32m:06s remains)
INFO - root - 2017-12-05 17:37:41.141885: step 30100, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 72h:52m:52s remains)
2017-12-05 17:37:41.963153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2077489 -4.2091532 -4.2120161 -4.2212372 -4.2382827 -4.257061 -4.279274 -4.2997479 -4.3020096 -4.29088 -4.269392 -4.2442045 -4.22879 -4.2285185 -4.2323222][-4.1912942 -4.1960568 -4.2072592 -4.2229872 -4.2462516 -4.2678356 -4.290349 -4.3112483 -4.313694 -4.3028884 -4.2836342 -4.2586412 -4.2382712 -4.2347894 -4.2396832][-4.1772213 -4.1800261 -4.1930366 -4.2093062 -4.2349362 -4.2620492 -4.2883372 -4.3095078 -4.3114071 -4.3004255 -4.2819867 -4.2584858 -4.2341323 -4.2249575 -4.2274714][-4.1568193 -4.1435976 -4.1429281 -4.1509857 -4.1784739 -4.2152362 -4.2559619 -4.2862315 -4.2947712 -4.2906308 -4.2777543 -4.2579451 -4.23038 -4.2123294 -4.2038851][-4.1489639 -4.1123476 -4.0848112 -4.0753889 -4.0993843 -4.1457014 -4.2015324 -4.2448153 -4.2633548 -4.2707539 -4.2683892 -4.2560849 -4.230814 -4.2059693 -4.1802983][-4.1680555 -4.1220069 -4.0741229 -4.038238 -4.0396285 -4.075521 -4.133604 -4.1868267 -4.2166705 -4.2350893 -4.2424464 -4.237298 -4.2159877 -4.1871924 -4.1490059][-4.1962357 -4.1586261 -4.1131129 -4.0686603 -4.0485048 -4.0533962 -4.0840874 -4.1224976 -4.1536937 -4.1801581 -4.1982064 -4.2018495 -4.1855421 -4.1539845 -4.1097426][-4.2202988 -4.1988058 -4.1692553 -4.1356425 -4.112421 -4.0975242 -4.0970659 -4.1022739 -4.1125474 -4.1303616 -4.1565781 -4.1758804 -4.1694088 -4.138432 -4.0912285][-4.2290754 -4.2203093 -4.20854 -4.1931682 -4.1787834 -4.1604352 -4.14661 -4.1326427 -4.1209612 -4.1263394 -4.1559792 -4.1804633 -4.1807342 -4.1592965 -4.118618][-4.2254357 -4.2181945 -4.2147741 -4.2148085 -4.2146716 -4.2065992 -4.1992664 -4.1828141 -4.16 -4.158134 -4.1906118 -4.2177758 -4.2190804 -4.2039294 -4.1716042][-4.2211552 -4.2133474 -4.2104568 -4.2135696 -4.2224407 -4.2266994 -4.2305808 -4.2234445 -4.2039461 -4.2000337 -4.22695 -4.250257 -4.2505555 -4.2379637 -4.2159448][-4.210772 -4.1992726 -4.192802 -4.1958265 -4.2096729 -4.2237558 -4.2374144 -4.2390103 -4.2264075 -4.2242355 -4.2430148 -4.2570171 -4.2561941 -4.2498312 -4.2405276][-4.2012262 -4.180707 -4.1642056 -4.1603208 -4.1721854 -4.1907134 -4.2149835 -4.2288704 -4.224319 -4.2286925 -4.2430768 -4.2524896 -4.2491302 -4.2489314 -4.24865][-4.2068424 -4.1764755 -4.1464424 -4.130681 -4.1336656 -4.1512675 -4.1829062 -4.2066073 -4.2150874 -4.2337623 -4.2530913 -4.2613158 -4.2548838 -4.2526278 -4.2524667][-4.2278113 -4.193861 -4.15872 -4.137814 -4.13235 -4.1460085 -4.1813946 -4.2105823 -4.226553 -4.2496443 -4.2719665 -4.2827864 -4.27549 -4.2665563 -4.2602892]]...]
INFO - root - 2017-12-05 17:37:50.448920: step 30110, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 70h:33m:23s remains)
INFO - root - 2017-12-05 17:37:58.859495: step 30120, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 70h:38m:03s remains)
INFO - root - 2017-12-05 17:38:07.394858: step 30130, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 71h:58m:17s remains)
INFO - root - 2017-12-05 17:38:15.943840: step 30140, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 74h:43m:55s remains)
INFO - root - 2017-12-05 17:38:24.546048: step 30150, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 73h:04m:52s remains)
INFO - root - 2017-12-05 17:38:33.065873: step 30160, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 72h:59m:24s remains)
INFO - root - 2017-12-05 17:38:41.494077: step 30170, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 68h:53m:54s remains)
INFO - root - 2017-12-05 17:38:49.958167: step 30180, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.833 sec/batch; 69h:56m:29s remains)
INFO - root - 2017-12-05 17:38:58.458946: step 30190, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 70h:44m:45s remains)
INFO - root - 2017-12-05 17:39:06.953925: step 30200, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 71h:59m:08s remains)
2017-12-05 17:39:07.693911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2475438 -4.2106738 -4.1769557 -4.1508365 -4.1412611 -4.1335325 -4.1453457 -4.1660018 -4.1648288 -4.1549621 -4.1515384 -4.1573243 -4.1537385 -4.1409869 -4.14528][-4.2522292 -4.2155862 -4.1818428 -4.1582093 -4.1463461 -4.1331105 -4.1400824 -4.1632595 -4.1670985 -4.1629086 -4.16734 -4.1806746 -4.1828132 -4.172483 -4.179657][-4.2544107 -4.2216053 -4.1886206 -4.1668997 -4.1519041 -4.129003 -4.1288829 -4.153791 -4.1644526 -4.1648979 -4.1704826 -4.1865764 -4.1910224 -4.1821647 -4.1946855][-4.2628813 -4.2355328 -4.2065654 -4.1872988 -4.1680679 -4.1323009 -4.1174374 -4.1405206 -4.1581573 -4.1673646 -4.1775975 -4.1969995 -4.2040262 -4.1911869 -4.2021413][-4.2717834 -4.2485471 -4.2226286 -4.2044392 -4.1786327 -4.1215572 -4.0799742 -4.101079 -4.1326013 -4.1583667 -4.185977 -4.2191148 -4.2289963 -4.2108626 -4.21152][-4.2745142 -4.2539477 -4.2280846 -4.2073383 -4.1648269 -4.072947 -3.9834657 -4.00578 -4.0748677 -4.1289864 -4.1756473 -4.2219381 -4.2381349 -4.2159514 -4.2091255][-4.2690487 -4.2525263 -4.226481 -4.196383 -4.13262 -4.0035367 -3.8645079 -3.8902588 -4.0094447 -4.0958982 -4.1515841 -4.1968231 -4.2053995 -4.1803041 -4.1738691][-4.2583923 -4.245626 -4.22325 -4.1880636 -4.1157975 -3.9887226 -3.8579168 -3.8881783 -4.0131121 -4.100266 -4.14272 -4.1724505 -4.1659031 -4.1394615 -4.1384988][-4.2532248 -4.2420921 -4.2243042 -4.1915388 -4.1279902 -4.0324726 -3.9559894 -3.9940038 -4.0841203 -4.14196 -4.1645269 -4.1705732 -4.1484766 -4.1196485 -4.1220741][-4.2587514 -4.2470312 -4.2314358 -4.2018962 -4.1551142 -4.096951 -4.0625119 -4.0981445 -4.1576052 -4.1928434 -4.2086515 -4.2075744 -4.1830778 -4.1555181 -4.1554103][-4.2718182 -4.260386 -4.247396 -4.2233868 -4.1933579 -4.1632094 -4.150362 -4.1731577 -4.2059579 -4.2259336 -4.240262 -4.241097 -4.2302933 -4.2164073 -4.2192655][-4.2869124 -4.2770629 -4.2681403 -4.2519331 -4.2333403 -4.2211328 -4.217586 -4.2295575 -4.246798 -4.2590671 -4.271102 -4.27538 -4.2778487 -4.2780819 -4.2824149][-4.3045406 -4.2982974 -4.2945 -4.2866845 -4.2756491 -4.2707787 -4.2702289 -4.2769346 -4.2886319 -4.3004575 -4.3112397 -4.3146882 -4.3203015 -4.3240929 -4.3270082][-4.3225756 -4.3190885 -4.3165879 -4.3136611 -4.3080983 -4.3053112 -4.3049178 -4.3079338 -4.3145418 -4.3247805 -4.3335738 -4.3351626 -4.3382568 -4.340497 -4.3418922][-4.337553 -4.3347745 -4.3320041 -4.3297958 -4.3285108 -4.3284554 -4.3286881 -4.3289127 -4.3313217 -4.3359337 -4.3400316 -4.3415418 -4.3429828 -4.3434453 -4.3435478]]...]
INFO - root - 2017-12-05 17:39:16.293734: step 30210, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 73h:10m:35s remains)
INFO - root - 2017-12-05 17:39:24.840994: step 30220, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 70h:41m:58s remains)
INFO - root - 2017-12-05 17:39:33.359244: step 30230, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 69h:49m:55s remains)
INFO - root - 2017-12-05 17:39:41.733459: step 30240, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 70h:00m:52s remains)
INFO - root - 2017-12-05 17:39:50.340693: step 30250, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 74h:24m:54s remains)
INFO - root - 2017-12-05 17:39:59.014839: step 30260, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 70h:07m:07s remains)
INFO - root - 2017-12-05 17:40:07.545432: step 30270, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 74h:18m:00s remains)
INFO - root - 2017-12-05 17:40:16.033417: step 30280, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 73h:04m:39s remains)
INFO - root - 2017-12-05 17:40:24.563907: step 30290, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 71h:07m:10s remains)
INFO - root - 2017-12-05 17:40:33.116985: step 30300, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 71h:42m:01s remains)
2017-12-05 17:40:33.845312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3493404 -4.3488636 -4.3468904 -4.3453436 -4.3438492 -4.3438196 -4.3453679 -4.345767 -4.3441119 -4.3413482 -4.3401 -4.34073 -4.3459096 -4.3549972 -4.36295][-4.3264222 -4.3247805 -4.3208528 -4.3176446 -4.3165054 -4.3172584 -4.32002 -4.3207951 -4.3173113 -4.3112659 -4.3072519 -4.3077459 -4.3156738 -4.3327794 -4.3507657][-4.3023348 -4.2993579 -4.2928391 -4.2864208 -4.2821026 -4.2801943 -4.2819824 -4.2800775 -4.2717671 -4.2622514 -4.2607045 -4.2668362 -4.2775097 -4.30134 -4.3301339][-4.2866564 -4.2821083 -4.2724857 -4.2590795 -4.2475457 -4.2379551 -4.2311354 -4.2184 -4.2009273 -4.1912527 -4.1997995 -4.2199054 -4.23947 -4.2673168 -4.3030286][-4.2789812 -4.2733746 -4.2587466 -4.23539 -4.2118 -4.1879129 -4.1649804 -4.1362724 -4.1108761 -4.1076217 -4.1354256 -4.1777053 -4.2116022 -4.242094 -4.279089][-4.2770362 -4.2703013 -4.2477665 -4.2099209 -4.1689568 -4.12327 -4.0819807 -4.0474434 -4.0251608 -4.0355854 -4.0851812 -4.1480093 -4.1955209 -4.2287583 -4.2629585][-4.2761803 -4.2664661 -4.2329578 -4.1797347 -4.1187029 -4.0492435 -3.9986594 -3.977931 -3.9754636 -4.0056992 -4.0690565 -4.1385565 -4.1894026 -4.22258 -4.2534904][-4.2719545 -4.2580786 -4.2166386 -4.1513753 -4.0730758 -3.9830616 -3.9289875 -3.9331412 -3.9644234 -4.0151749 -4.08613 -4.1522174 -4.1942739 -4.2229762 -4.2502913][-4.260293 -4.2452531 -4.2045193 -4.140027 -4.0578966 -3.9607117 -3.8987107 -3.9158113 -3.9768889 -4.043849 -4.1125464 -4.1724973 -4.2036986 -4.2279625 -4.2536154][-4.2455692 -4.2350473 -4.203444 -4.1514378 -4.0827546 -3.9973783 -3.9347708 -3.9450059 -4.0096922 -4.0775084 -4.1366057 -4.188231 -4.2136407 -4.2360592 -4.2615662][-4.2305565 -4.2289457 -4.2087631 -4.1714025 -4.1231971 -4.0615778 -4.01213 -4.0092912 -4.0582309 -4.1136031 -4.1576066 -4.1992893 -4.221571 -4.2434998 -4.270071][-4.2209678 -4.2277703 -4.21407 -4.187561 -4.1576242 -4.120441 -4.0852294 -4.0709167 -4.0971813 -4.1370096 -4.1707077 -4.2027669 -4.2226596 -4.2457438 -4.2768016][-4.2275791 -4.2393045 -4.2307487 -4.2153134 -4.197804 -4.1749949 -4.1485448 -4.1252661 -4.13024 -4.15513 -4.1799278 -4.2058005 -4.22789 -4.2570477 -4.2924919][-4.251893 -4.2654095 -4.2609038 -4.2519889 -4.240747 -4.2221484 -4.1989112 -4.1725783 -4.1667213 -4.1814089 -4.2005024 -4.22426 -4.2508945 -4.2823768 -4.3142118][-4.2795277 -4.2932558 -4.29208 -4.2870612 -4.2788129 -4.2645531 -4.2475166 -4.2271776 -4.2209787 -4.2284007 -4.2425866 -4.2636137 -4.2896118 -4.3159513 -4.3381038]]...]
INFO - root - 2017-12-05 17:40:42.463151: step 30310, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 73h:56m:48s remains)
INFO - root - 2017-12-05 17:40:51.026724: step 30320, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 71h:05m:45s remains)
INFO - root - 2017-12-05 17:40:59.511890: step 30330, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.808 sec/batch; 67h:47m:20s remains)
INFO - root - 2017-12-05 17:41:07.802573: step 30340, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.738 sec/batch; 61h:59m:02s remains)
INFO - root - 2017-12-05 17:41:16.481178: step 30350, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 72h:33m:52s remains)
INFO - root - 2017-12-05 17:41:25.038232: step 30360, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 71h:04m:49s remains)
INFO - root - 2017-12-05 17:41:33.663841: step 30370, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 71h:55m:59s remains)
INFO - root - 2017-12-05 17:41:42.159839: step 30380, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 70h:30m:45s remains)
INFO - root - 2017-12-05 17:41:50.681261: step 30390, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 73h:11m:53s remains)
INFO - root - 2017-12-05 17:41:59.226180: step 30400, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 71h:36m:07s remains)
2017-12-05 17:41:59.995535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2597117 -4.2388358 -4.2257972 -4.21899 -4.2247248 -4.249136 -4.2765861 -4.2910366 -4.2972775 -4.2991223 -4.2972469 -4.2922535 -4.2865691 -4.282577 -4.2815852][-4.2342019 -4.2003317 -4.1755414 -4.1617689 -4.16517 -4.1952119 -4.2308135 -4.2544827 -4.27032 -4.2808533 -4.2870188 -4.2868352 -4.2805734 -4.2732749 -4.2704215][-4.2254953 -4.183013 -4.1453238 -4.1174588 -4.110857 -4.13981 -4.1801887 -4.2115016 -4.2366219 -4.2607622 -4.2821555 -4.2929211 -4.290781 -4.2829304 -4.2784123][-4.2318554 -4.1859984 -4.1400323 -4.0981622 -4.0741758 -4.0912724 -4.1292734 -4.1637464 -4.1946793 -4.2346783 -4.2742672 -4.2952209 -4.2983088 -4.2957029 -4.2930408][-4.2425375 -4.1944938 -4.1447277 -4.0959077 -4.0563 -4.0532193 -4.0769773 -4.1110597 -4.1461411 -4.1978049 -4.2506084 -4.2775283 -4.2837114 -4.2876897 -4.2900505][-4.2514052 -4.2035775 -4.1526227 -4.1001654 -4.0495443 -4.0207305 -4.0137525 -4.0431242 -4.0853262 -4.1422887 -4.2024679 -4.2366076 -4.2448459 -4.2502694 -4.2572875][-4.2580471 -4.2105927 -4.1592674 -4.1040773 -4.0500712 -4.0080295 -3.9761305 -3.98632 -4.0257478 -4.0830846 -4.1456442 -4.1880732 -4.2009964 -4.202981 -4.2100759][-4.2631431 -4.2169909 -4.1686335 -4.1146502 -4.0620875 -4.0224628 -3.98606 -3.9784105 -3.9957316 -4.0382814 -4.0991735 -4.1439652 -4.1607032 -4.1639633 -4.1701179][-4.2674437 -4.2234135 -4.1787434 -4.1283741 -4.0754385 -4.04047 -4.0059085 -3.9888823 -3.9871991 -4.0054679 -4.0572228 -4.1030416 -4.1206679 -4.1245728 -4.1292057][-4.2741823 -4.2351913 -4.1944342 -4.1484327 -4.0949211 -4.0593605 -4.0249958 -3.9997199 -3.9902484 -3.9983587 -4.0339375 -4.069519 -4.0829496 -4.0881147 -4.098732][-4.2810645 -4.2495418 -4.2159204 -4.1758819 -4.1271129 -4.0898261 -4.0556965 -4.0279188 -4.018127 -4.0243711 -4.04296 -4.0565386 -4.0562429 -4.0593276 -4.0786843][-4.2859359 -4.2627392 -4.2397618 -4.21062 -4.1731977 -4.1432395 -4.1099615 -4.0782638 -4.0669532 -4.0702109 -4.0786633 -4.071527 -4.0567436 -4.0506773 -4.0720096][-4.2906017 -4.2753758 -4.2646146 -4.2507672 -4.2291617 -4.2113485 -4.1828384 -4.1490459 -4.127605 -4.1148214 -4.1101193 -4.0937304 -4.0700855 -4.0588803 -4.07322][-4.2926903 -4.2818332 -4.2798352 -4.2806091 -4.2743673 -4.2652326 -4.2440972 -4.2089028 -4.1749911 -4.1472149 -4.1349826 -4.1165943 -4.0933738 -4.077054 -4.091342][-4.2912612 -4.2820067 -4.2842879 -4.29144 -4.2925973 -4.2884264 -4.2740021 -4.241653 -4.2008228 -4.1639438 -4.153235 -4.1393232 -4.120698 -4.1045661 -4.115684]]...]
INFO - root - 2017-12-05 17:42:08.460117: step 30410, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.829 sec/batch; 69h:36m:09s remains)
INFO - root - 2017-12-05 17:42:17.009353: step 30420, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 70h:47m:30s remains)
INFO - root - 2017-12-05 17:42:25.662513: step 30430, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.810 sec/batch; 68h:00m:07s remains)
INFO - root - 2017-12-05 17:42:34.061333: step 30440, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 72h:48m:46s remains)
INFO - root - 2017-12-05 17:42:42.572366: step 30450, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 70h:47m:07s remains)
INFO - root - 2017-12-05 17:42:51.303411: step 30460, loss = 2.04, batch loss = 1.98 (7.5 examples/sec; 1.064 sec/batch; 89h:17m:30s remains)
INFO - root - 2017-12-05 17:42:59.869338: step 30470, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 72h:02m:20s remains)
INFO - root - 2017-12-05 17:43:08.473292: step 30480, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 69h:16m:18s remains)
INFO - root - 2017-12-05 17:43:16.928215: step 30490, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.815 sec/batch; 68h:21m:15s remains)
INFO - root - 2017-12-05 17:43:25.547496: step 30500, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 69h:51m:23s remains)
2017-12-05 17:43:26.364538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3204131 -4.3218803 -4.3234692 -4.324182 -4.3223362 -4.3193927 -4.3182559 -4.3200455 -4.3240142 -4.32784 -4.3292642 -4.3280492 -4.3246655 -4.3208489 -4.3186445][-4.3079329 -4.3083949 -4.3088245 -4.3076777 -4.3024731 -4.2955122 -4.2912922 -4.2936735 -4.3025403 -4.3127937 -4.3191986 -4.3204088 -4.316834 -4.3115888 -4.3079157][-4.2976389 -4.2954769 -4.2930579 -4.2890921 -4.2801113 -4.2675281 -4.2559156 -4.2532191 -4.2661886 -4.2877665 -4.3064041 -4.3163624 -4.31616 -4.3106327 -4.3046341][-4.2965088 -4.2887659 -4.2792988 -4.2683573 -4.2528415 -4.2308717 -4.2029428 -4.1868229 -4.2030783 -4.24126 -4.2811995 -4.3097339 -4.3218651 -4.3212428 -4.3150859][-4.2980504 -4.282865 -4.2621121 -4.2370558 -4.2063894 -4.1635618 -4.1071167 -4.06907 -4.0918188 -4.1577096 -4.230988 -4.2893758 -4.3223376 -4.3319969 -4.3289533][-4.29798 -4.2759647 -4.2428136 -4.1992712 -4.1444693 -4.0674443 -3.9682243 -3.8968198 -3.9281106 -4.0320711 -4.1455212 -4.2418981 -4.30346 -4.3297849 -4.3338432][-4.2990179 -4.2745752 -4.2344117 -4.1775355 -4.1011419 -3.9932752 -3.8488023 -3.7340891 -3.7660122 -3.9038119 -4.0525246 -4.1825523 -4.2695622 -4.3127909 -4.3275175][-4.3046455 -4.2820115 -4.2444167 -4.1887317 -4.1086812 -3.9964938 -3.8435619 -3.7136841 -3.7302303 -3.86218 -4.0122261 -4.1513109 -4.2470689 -4.2981915 -4.3189383][-4.3179507 -4.298574 -4.2670522 -4.2228618 -4.1578994 -4.0678077 -3.9501464 -3.85281 -3.8602326 -3.9519808 -4.0653958 -4.1793981 -4.2593889 -4.3032866 -4.3221145][-4.3320932 -4.3178062 -4.2954049 -4.2664862 -4.2238021 -4.1638646 -4.0891957 -4.0297709 -4.035615 -4.091073 -4.1636367 -4.2430468 -4.297009 -4.3248858 -4.3340569][-4.3427429 -4.335444 -4.324502 -4.3112354 -4.2894659 -4.256165 -4.2173223 -4.1880522 -4.1933761 -4.2244143 -4.2655587 -4.3116012 -4.3397727 -4.3491521 -4.3464518][-4.3487496 -4.3483667 -4.3471708 -4.3458452 -4.3394332 -4.3253365 -4.3087678 -4.2968812 -4.301393 -4.3170533 -4.3376541 -4.359436 -4.368751 -4.365263 -4.3546004][-4.3473334 -4.3533692 -4.35957 -4.3640256 -4.36496 -4.3611155 -4.3557134 -4.3519778 -4.3550072 -4.3618956 -4.3699007 -4.3765006 -4.3753896 -4.3666878 -4.3541794][-4.333734 -4.343904 -4.3544211 -4.3611488 -4.3639293 -4.3636603 -4.3626466 -4.3624344 -4.3645988 -4.3674383 -4.3693538 -4.3687787 -4.36445 -4.3568339 -4.34793][-4.3146563 -4.3261571 -4.33792 -4.345809 -4.3492522 -4.3502564 -4.3503075 -4.3501115 -4.35106 -4.3520741 -4.3521876 -4.3508706 -4.34804 -4.3442373 -4.3402457]]...]
INFO - root - 2017-12-05 17:43:35.023854: step 30510, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 72h:49m:55s remains)
INFO - root - 2017-12-05 17:43:43.520095: step 30520, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 70h:26m:56s remains)
INFO - root - 2017-12-05 17:43:52.031517: step 30530, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 71h:42m:05s remains)
INFO - root - 2017-12-05 17:44:00.606376: step 30540, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 70h:10m:17s remains)
INFO - root - 2017-12-05 17:44:09.072510: step 30550, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 72h:41m:05s remains)
INFO - root - 2017-12-05 17:44:17.728755: step 30560, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 72h:11m:55s remains)
INFO - root - 2017-12-05 17:44:26.221727: step 30570, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.819 sec/batch; 68h:40m:01s remains)
INFO - root - 2017-12-05 17:44:34.812389: step 30580, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 70h:03m:49s remains)
INFO - root - 2017-12-05 17:44:43.300293: step 30590, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 72h:35m:28s remains)
INFO - root - 2017-12-05 17:44:51.885613: step 30600, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 72h:52m:50s remains)
2017-12-05 17:44:52.687327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2611237 -4.2648969 -4.2646923 -4.2658525 -4.2691793 -4.2728925 -4.2776279 -4.2847762 -4.2890739 -4.28877 -4.2864466 -4.2871532 -4.29142 -4.2946005 -4.2956367][-4.2596707 -4.26108 -4.259161 -4.2561221 -4.2547894 -4.2563682 -4.2618985 -4.2740507 -4.2851048 -4.2893085 -4.28781 -4.2851667 -4.2845378 -4.284667 -4.2841368][-4.2656074 -4.2608848 -4.250843 -4.2364855 -4.2257042 -4.2213874 -4.225009 -4.2428379 -4.263999 -4.2757173 -4.2774749 -4.2745123 -4.2701426 -4.2659712 -4.2624078][-4.27015 -4.2548337 -4.2326274 -4.2061033 -4.1852193 -4.1723256 -4.1703267 -4.1935391 -4.2275548 -4.2512131 -4.2590747 -4.2575245 -4.2513752 -4.2425318 -4.2375364][-4.2573552 -4.2279129 -4.1877561 -4.1433134 -4.1082311 -4.0818553 -4.0736275 -4.1075287 -4.1632948 -4.2055335 -4.2250695 -4.2271581 -4.2188663 -4.202549 -4.1954203][-4.2324929 -4.188765 -4.1312189 -4.0676293 -4.0134516 -3.9699988 -3.9511962 -3.9956882 -4.080049 -4.1477218 -4.1831641 -4.1924481 -4.1858768 -4.1671815 -4.1590505][-4.2124572 -4.1622357 -4.095808 -4.020267 -3.9495707 -3.8845758 -3.8381462 -3.8795781 -3.9864566 -4.081707 -4.1400547 -4.1612554 -4.1622634 -4.1516562 -4.1478515][-4.2235985 -4.1745415 -4.1055627 -4.0254064 -3.9487681 -3.8650174 -3.7834835 -3.8023155 -3.9109075 -4.0262532 -4.1117649 -4.1522856 -4.1697721 -4.174212 -4.1750946][-4.2601376 -4.2194629 -4.1580882 -4.0873694 -4.02129 -3.94298 -3.859354 -3.8538144 -3.9319317 -4.037034 -4.1314363 -4.1835122 -4.21214 -4.2297678 -4.2387342][-4.3029051 -4.2753162 -4.23019 -4.177382 -4.1277943 -4.071455 -4.0097313 -3.9922745 -4.0341234 -4.1109118 -4.1925788 -4.2405276 -4.268043 -4.2890935 -4.3026519][-4.3330021 -4.3182778 -4.2872686 -4.2483807 -4.2115889 -4.1745496 -4.1325674 -4.1136918 -4.1345024 -4.18704 -4.247364 -4.2853527 -4.3085241 -4.3264518 -4.3344169][-4.3481107 -4.3441696 -4.3264227 -4.2997556 -4.2731638 -4.2470756 -4.2195663 -4.204649 -4.2148542 -4.2456665 -4.2845044 -4.310853 -4.3261423 -4.3359857 -4.33566][-4.347548 -4.3517 -4.3480639 -4.3347559 -4.3189425 -4.3000135 -4.2814937 -4.2706966 -4.2746868 -4.2898474 -4.3085237 -4.32332 -4.3296008 -4.3294425 -4.3206182][-4.3385758 -4.3467131 -4.3519545 -4.3494081 -4.3430762 -4.33375 -4.3211236 -4.3122821 -4.31036 -4.3142767 -4.3190165 -4.3229771 -4.3211432 -4.314362 -4.3014736][-4.3353548 -4.34218 -4.3483019 -4.3487682 -4.3477154 -4.3458042 -4.34101 -4.3347287 -4.32806 -4.3230362 -4.3172426 -4.312253 -4.3073797 -4.298842 -4.2861156]]...]
INFO - root - 2017-12-05 17:45:01.243047: step 30610, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 71h:31m:39s remains)
INFO - root - 2017-12-05 17:45:09.817612: step 30620, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 71h:46m:39s remains)
INFO - root - 2017-12-05 17:45:18.470754: step 30630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 71h:27m:18s remains)
INFO - root - 2017-12-05 17:45:26.727844: step 30640, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 73h:13m:15s remains)
INFO - root - 2017-12-05 17:45:35.274035: step 30650, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 72h:15m:41s remains)
INFO - root - 2017-12-05 17:45:43.719830: step 30660, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.814 sec/batch; 68h:14m:33s remains)
INFO - root - 2017-12-05 17:45:52.302957: step 30670, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 71h:17m:40s remains)
INFO - root - 2017-12-05 17:46:00.822935: step 30680, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 72h:11m:16s remains)
INFO - root - 2017-12-05 17:46:09.247829: step 30690, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 71h:39m:27s remains)
INFO - root - 2017-12-05 17:46:17.884440: step 30700, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 70h:54m:08s remains)
2017-12-05 17:46:18.692959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2850294 -4.28161 -4.2871 -4.289156 -4.2835226 -4.2809539 -4.2805533 -4.2765617 -4.2692032 -4.2669177 -4.2714939 -4.2794776 -4.2873759 -4.2932868 -4.2986221][-4.2641964 -4.2591424 -4.26323 -4.2631321 -4.2527504 -4.2480478 -4.2487369 -4.2474284 -4.2427278 -4.2445974 -4.2516732 -4.2603092 -4.2659054 -4.2701678 -4.2773943][-4.2447157 -4.2381845 -4.2396274 -4.2348843 -4.2178736 -4.210743 -4.2141938 -4.218256 -4.219677 -4.2271252 -4.2368383 -4.2458091 -4.2497363 -4.2522459 -4.2627673][-4.2211032 -4.2134533 -4.2137833 -4.2073259 -4.1866431 -4.1789103 -4.1828346 -4.1905732 -4.1999516 -4.2159128 -4.2295976 -4.23775 -4.2430768 -4.2457185 -4.259099][-4.1894369 -4.1838751 -4.1864314 -4.1837978 -4.1687412 -4.1614923 -4.1577454 -4.1581125 -4.1698895 -4.1903539 -4.2027559 -4.2078028 -4.2134824 -4.218379 -4.2391577][-4.1725645 -4.1738367 -4.1823039 -4.1873703 -4.1754417 -4.1524153 -4.1193347 -4.1010866 -4.1155152 -4.1394405 -4.1520991 -4.1566095 -4.1693392 -4.18764 -4.224864][-4.1827474 -4.1847062 -4.1889915 -4.1856837 -4.161334 -4.111311 -4.039711 -4.0002489 -4.0359297 -4.0864229 -4.115562 -4.1256466 -4.1460481 -4.1783276 -4.22633][-4.1844249 -4.17754 -4.1693811 -4.1549263 -4.125391 -4.067668 -3.9882331 -3.958472 -4.0259695 -4.0963087 -4.1297884 -4.1339521 -4.1457829 -4.1765265 -4.2205534][-4.1771894 -4.1610894 -4.1490884 -4.1414528 -4.1308603 -4.1008506 -4.0586948 -4.0542793 -4.1129436 -4.1662278 -4.1823034 -4.1713166 -4.1709728 -4.1902542 -4.2238355][-4.1709142 -4.1576285 -4.1599188 -4.16672 -4.16782 -4.1550021 -4.1344509 -4.1348295 -4.1716523 -4.2040863 -4.206284 -4.1883516 -4.184792 -4.1991463 -4.2256637][-4.1765933 -4.1736474 -4.1872568 -4.199913 -4.2003679 -4.1899137 -4.1761436 -4.1724563 -4.1943278 -4.2183151 -4.2220488 -4.2090206 -4.2026258 -4.2113471 -4.2342691][-4.1991858 -4.1977491 -4.212101 -4.2260852 -4.2288046 -4.2202096 -4.2089925 -4.2043953 -4.2186928 -4.2387319 -4.24718 -4.2408891 -4.23219 -4.2330151 -4.2500138][-4.2277575 -4.2235255 -4.2341776 -4.2474251 -4.2520547 -4.2481327 -4.2423096 -4.2397289 -4.2477674 -4.260663 -4.2696958 -4.2693877 -4.261972 -4.2602367 -4.2705564][-4.2583847 -4.2550607 -4.2634039 -4.2747664 -4.2801309 -4.2799616 -4.2778025 -4.2754688 -4.2788033 -4.2851119 -4.2902155 -4.2904029 -4.2855272 -4.2834044 -4.2894645][-4.2895541 -4.2870493 -4.2939572 -4.302093 -4.3059874 -4.3067336 -4.305768 -4.305604 -4.3090839 -4.3126769 -4.3145466 -4.3134418 -4.3087025 -4.3055887 -4.3089662]]...]
INFO - root - 2017-12-05 17:46:27.155781: step 30710, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 72h:34m:55s remains)
INFO - root - 2017-12-05 17:46:35.690742: step 30720, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 71h:06m:30s remains)
INFO - root - 2017-12-05 17:46:44.169352: step 30730, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 73h:19m:45s remains)
INFO - root - 2017-12-05 17:46:52.541524: step 30740, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.884 sec/batch; 74h:05m:52s remains)
INFO - root - 2017-12-05 17:47:01.103381: step 30750, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 72h:44m:34s remains)
INFO - root - 2017-12-05 17:47:09.586792: step 30760, loss = 2.11, batch loss = 2.05 (9.4 examples/sec; 0.847 sec/batch; 70h:57m:47s remains)
INFO - root - 2017-12-05 17:47:18.037348: step 30770, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 72h:17m:01s remains)
INFO - root - 2017-12-05 17:47:26.602173: step 30780, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 72h:10m:15s remains)
INFO - root - 2017-12-05 17:47:35.175325: step 30790, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 72h:21m:10s remains)
INFO - root - 2017-12-05 17:47:43.609828: step 30800, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 72h:44m:13s remains)
2017-12-05 17:47:44.398045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.329113 -4.3217292 -4.3131094 -4.3098545 -4.3076916 -4.3033371 -4.2968636 -4.2932658 -4.2930212 -4.2938471 -4.2920489 -4.2853231 -4.2787437 -4.2793217 -4.2893238][-4.3317857 -4.3225656 -4.311789 -4.3075004 -4.3030472 -4.2958665 -4.2869606 -4.2801876 -4.2803574 -4.2835231 -4.282228 -4.2734551 -4.2647767 -4.2643323 -4.2752433][-4.3329391 -4.3220983 -4.3073535 -4.2952394 -4.28253 -4.2735815 -4.2633619 -4.2545052 -4.2565308 -4.2627597 -4.2599645 -4.2484012 -4.2400684 -4.2429404 -4.2584066][-4.3253522 -4.310112 -4.288897 -4.2674484 -4.2459817 -4.2343106 -4.2220759 -4.2118464 -4.2208657 -4.2340679 -4.2306762 -4.2161732 -4.2104812 -4.2208204 -4.2446189][-4.3067112 -4.2832851 -4.2521276 -4.220643 -4.1934433 -4.1758065 -4.1561956 -4.1421418 -4.1655569 -4.1959677 -4.1971045 -4.1827664 -4.1813116 -4.2010179 -4.2346764][-4.2889524 -4.2568688 -4.2165046 -4.1767716 -4.1421738 -4.112504 -4.073926 -4.0501814 -4.0943518 -4.1509295 -4.1629705 -4.1522307 -4.1581964 -4.1873412 -4.2290554][-4.2742915 -4.231554 -4.1816406 -4.1332731 -4.0880294 -4.0371432 -3.9671464 -3.92036 -3.9915042 -4.0885615 -4.1218295 -4.1224179 -4.1391258 -4.1777081 -4.2251639][-4.2706261 -4.2245712 -4.1706481 -4.1178761 -4.0689363 -4.0077724 -3.9175906 -3.8518322 -3.9385786 -4.0585938 -4.1068206 -4.1168137 -4.1412706 -4.1814613 -4.22635][-4.2857618 -4.2479558 -4.203433 -4.162499 -4.1269937 -4.0849886 -4.0197468 -3.9714088 -4.0310912 -4.1197529 -4.1562176 -4.1657329 -4.1850514 -4.2110229 -4.240953][-4.3036661 -4.2741685 -4.2394562 -4.212081 -4.19154 -4.1703906 -4.1393046 -4.11547 -4.1478209 -4.1974068 -4.2190242 -4.2223482 -4.2309289 -4.2430425 -4.25939][-4.3111234 -4.2887912 -4.2602282 -4.2420707 -4.232193 -4.2213712 -4.209053 -4.1975579 -4.2130432 -4.2409258 -4.2565913 -4.2572837 -4.2597585 -4.2654829 -4.2758055][-4.3098512 -4.2928023 -4.2692103 -4.2557244 -4.2502723 -4.2441378 -4.2374172 -4.230207 -4.2368731 -4.256042 -4.2703347 -4.2726049 -4.274436 -4.2795658 -4.2896404][-4.3028421 -4.2891517 -4.2701678 -4.2576461 -4.2528672 -4.2503095 -4.248208 -4.2463784 -4.2508206 -4.2634439 -4.2756152 -4.2798662 -4.2816439 -4.287178 -4.2985392][-4.3050308 -4.2931652 -4.2756958 -4.2627544 -4.2569814 -4.2558932 -4.25748 -4.2614732 -4.2672009 -4.2730637 -4.2801051 -4.2856994 -4.2899432 -4.2963696 -4.3076353][-4.3160005 -4.3058581 -4.2899246 -4.2771363 -4.2706904 -4.2709 -4.2745595 -4.2810221 -4.2861624 -4.2873654 -4.2897863 -4.2955914 -4.3020477 -4.3089476 -4.3191137]]...]
INFO - root - 2017-12-05 17:47:52.939047: step 30810, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 72h:03m:04s remains)
INFO - root - 2017-12-05 17:48:01.436868: step 30820, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 69h:48m:09s remains)
INFO - root - 2017-12-05 17:48:10.035686: step 30830, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 71h:24m:56s remains)
INFO - root - 2017-12-05 17:48:18.429596: step 30840, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 70h:51m:23s remains)
INFO - root - 2017-12-05 17:48:27.025585: step 30850, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 72h:54m:25s remains)
INFO - root - 2017-12-05 17:48:35.579644: step 30860, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 71h:45m:25s remains)
INFO - root - 2017-12-05 17:48:44.177491: step 30870, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 74h:36m:34s remains)
INFO - root - 2017-12-05 17:48:52.577089: step 30880, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 71h:45m:07s remains)
INFO - root - 2017-12-05 17:49:01.017207: step 30890, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 71h:31m:06s remains)
INFO - root - 2017-12-05 17:49:09.610012: step 30900, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 71h:58m:56s remains)
2017-12-05 17:49:10.368753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.204143 -4.1923513 -4.1798444 -4.1769676 -4.1796627 -4.1896276 -4.2018504 -4.2150922 -4.2139416 -4.2039628 -4.1904788 -4.1826382 -4.178555 -4.1621823 -4.1391611][-4.1704478 -4.1594863 -4.1566992 -4.1626177 -4.1685405 -4.1799483 -4.1942215 -4.2085876 -4.2112546 -4.2095518 -4.2030816 -4.1970243 -4.1938195 -4.1792665 -4.1559649][-4.1475391 -4.1463995 -4.1626825 -4.1820073 -4.1932631 -4.20192 -4.2088666 -4.21635 -4.216887 -4.2156477 -4.2111325 -4.2050767 -4.201189 -4.1855316 -4.1605406][-4.1534829 -4.15836 -4.1854625 -4.2114792 -4.2234297 -4.223969 -4.2201977 -4.2210011 -4.2204704 -4.2214928 -4.2225804 -4.2211132 -4.2143106 -4.19246 -4.1633205][-4.1898665 -4.1945467 -4.2186241 -4.2348371 -4.2357726 -4.2234883 -4.2083912 -4.2018237 -4.2020626 -4.2088275 -4.2217932 -4.23399 -4.23274 -4.2078309 -4.1739807][-4.2388792 -4.2449307 -4.255609 -4.2519674 -4.2329316 -4.2010894 -4.1729975 -4.1588407 -4.1592107 -4.1714396 -4.1971865 -4.2261372 -4.2353668 -4.211844 -4.1758156][-4.2680054 -4.2739892 -4.2739196 -4.2594781 -4.2277241 -4.1805148 -4.1370173 -4.1092968 -4.1044631 -4.1195869 -4.153533 -4.1890521 -4.201395 -4.1811385 -4.1485558][-4.2623382 -4.2671337 -4.2614417 -4.2494617 -4.2212114 -4.1728039 -4.115715 -4.0702991 -4.0579329 -4.0732651 -4.11251 -4.1479344 -4.1577225 -4.1417942 -4.1160975][-4.2333388 -4.2360358 -4.2284822 -4.2220888 -4.2040291 -4.1657739 -4.10758 -4.0513716 -4.0340028 -4.053009 -4.0913353 -4.123426 -4.129014 -4.1173582 -4.1003613][-4.1993785 -4.2054086 -4.2002 -4.19734 -4.1889267 -4.1647606 -4.1184244 -4.06844 -4.0527816 -4.0724154 -4.1041231 -4.1286974 -4.1315346 -4.121603 -4.1088638][-4.1921611 -4.1985674 -4.1937022 -4.1911259 -4.1851792 -4.1712437 -4.1422429 -4.108501 -4.1001916 -4.114224 -4.1373587 -4.1536293 -4.1537366 -4.1407137 -4.1240273][-4.2214823 -4.22789 -4.2222877 -4.213901 -4.1982851 -4.1805186 -4.1587844 -4.138515 -4.1381292 -4.1519356 -4.1710486 -4.1825638 -4.1806822 -4.1646223 -4.14017][-4.2587414 -4.2622976 -4.2574158 -4.2467189 -4.2241173 -4.1974921 -4.1770844 -4.1643152 -4.1678791 -4.1795506 -4.194149 -4.20105 -4.1983008 -4.1814241 -4.1515079][-4.2782993 -4.2763319 -4.2712169 -4.2639952 -4.2447634 -4.2168474 -4.1958561 -4.1854148 -4.1877995 -4.1937032 -4.2019143 -4.2038727 -4.1972952 -4.1775956 -4.1433816][-4.27705 -4.2706671 -4.2642779 -4.2576494 -4.2417607 -4.2153392 -4.1942925 -4.1847854 -4.1848931 -4.1864066 -4.1903009 -4.1915207 -4.18509 -4.1672106 -4.1333575]]...]
INFO - root - 2017-12-05 17:49:18.920115: step 30910, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 74h:07m:29s remains)
INFO - root - 2017-12-05 17:49:27.462478: step 30920, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 69h:55m:53s remains)
INFO - root - 2017-12-05 17:49:35.870943: step 30930, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 69h:54m:30s remains)
INFO - root - 2017-12-05 17:49:44.349701: step 30940, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 70h:15m:29s remains)
INFO - root - 2017-12-05 17:49:52.903494: step 30950, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 72h:35m:05s remains)
INFO - root - 2017-12-05 17:50:01.394888: step 30960, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 69h:28m:53s remains)
INFO - root - 2017-12-05 17:50:09.928352: step 30970, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 72h:27m:27s remains)
INFO - root - 2017-12-05 17:50:18.407284: step 30980, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.733 sec/batch; 61h:24m:36s remains)
INFO - root - 2017-12-05 17:50:26.866465: step 30990, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.835 sec/batch; 69h:55m:45s remains)
INFO - root - 2017-12-05 17:50:35.359248: step 31000, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 68h:45m:23s remains)
2017-12-05 17:50:36.077998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3194442 -4.3218307 -4.3210063 -4.3204074 -4.3197355 -4.3198619 -4.3214211 -4.3226733 -4.3225589 -4.31982 -4.3160162 -4.3127952 -4.3105793 -4.3088408 -4.3072939][-4.3275237 -4.3271828 -4.3250189 -4.3232031 -4.3214893 -4.3217936 -4.3241291 -4.3270326 -4.3265538 -4.3216338 -4.3144231 -4.3078642 -4.3046203 -4.3034692 -4.3029633][-4.3156838 -4.3135448 -4.3114758 -4.3078918 -4.3039918 -4.3012652 -4.3011322 -4.3037357 -4.3026309 -4.296144 -4.2866583 -4.2795177 -4.2799878 -4.2852798 -4.290514][-4.2793083 -4.281497 -4.2835093 -4.2786193 -4.2683082 -4.2554293 -4.2440996 -4.2376857 -4.237299 -4.2353897 -4.2296491 -4.2266345 -4.2355413 -4.2525582 -4.2692151][-4.2210541 -4.2387018 -4.2487559 -4.2419658 -4.218915 -4.1856279 -4.1487532 -4.1249094 -4.1299767 -4.1438 -4.1516767 -4.1606536 -4.1842737 -4.2170658 -4.2473941][-4.1477122 -4.1892476 -4.2105036 -4.2018552 -4.1626377 -4.098042 -4.019537 -3.9704521 -3.9923546 -4.0398736 -4.0741611 -4.1066241 -4.1527023 -4.1997766 -4.2377949][-4.0722666 -4.1393795 -4.1762733 -4.1687541 -4.1147895 -4.0169921 -3.8953078 -3.8252378 -3.8751664 -3.9664183 -4.0356021 -4.0973415 -4.1627312 -4.2129989 -4.2461257][-4.0393419 -4.1211905 -4.16844 -4.1643844 -4.1093616 -4.0066214 -3.8819876 -3.8187294 -3.8788474 -3.9836929 -4.0682878 -4.1421494 -4.2083454 -4.2493544 -4.2692785][-4.0800042 -4.1541843 -4.1973648 -4.197084 -4.1521058 -4.0700173 -3.9817357 -3.9456356 -3.9927349 -4.0719237 -4.1414428 -4.2040033 -4.2543449 -4.280035 -4.2870684][-4.1514807 -4.2074556 -4.2402749 -4.2421813 -4.2091808 -4.1521807 -4.1024141 -4.09037 -4.1196938 -4.1653991 -4.2093768 -4.2511663 -4.2814746 -4.2933283 -4.2923574][-4.2175717 -4.2555833 -4.2777109 -4.2782989 -4.2541428 -4.2179255 -4.1946559 -4.1955938 -4.2109303 -4.2309194 -4.251513 -4.274652 -4.2902684 -4.29398 -4.2901897][-4.2611556 -4.2844982 -4.2971048 -4.2945023 -4.2769432 -4.2560153 -4.2483525 -4.2534194 -4.2597942 -4.2663908 -4.2749677 -4.2865887 -4.2938786 -4.293673 -4.2891674][-4.2871866 -4.2986536 -4.3022547 -4.2976608 -4.2866049 -4.2768836 -4.2763095 -4.2812567 -4.2834792 -4.2853413 -4.2886853 -4.2933517 -4.2949562 -4.2923746 -4.2879972][-4.2996054 -4.3033481 -4.3014994 -4.296433 -4.290103 -4.286377 -4.2875438 -4.2904778 -4.2902231 -4.289753 -4.2904959 -4.2915483 -4.2907095 -4.2877831 -4.28425][-4.299386 -4.29918 -4.2946963 -4.2899427 -4.2862287 -4.2847342 -4.2856421 -4.2867403 -4.2858224 -4.284874 -4.2848287 -4.2852144 -4.2842307 -4.2822413 -4.2801652]]...]
INFO - root - 2017-12-05 17:50:44.566206: step 31010, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 69h:58m:10s remains)
INFO - root - 2017-12-05 17:50:53.088963: step 31020, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 72h:57m:38s remains)
INFO - root - 2017-12-05 17:51:01.594773: step 31030, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 73h:22m:44s remains)
INFO - root - 2017-12-05 17:51:10.056169: step 31040, loss = 2.03, batch loss = 1.98 (9.0 examples/sec; 0.893 sec/batch; 74h:47m:51s remains)
INFO - root - 2017-12-05 17:51:18.600594: step 31050, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 0.807 sec/batch; 67h:35m:56s remains)
INFO - root - 2017-12-05 17:51:27.116173: step 31060, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 72h:43m:46s remains)
INFO - root - 2017-12-05 17:51:35.686393: step 31070, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 72h:10m:05s remains)
INFO - root - 2017-12-05 17:51:44.326630: step 31080, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 71h:30m:57s remains)
INFO - root - 2017-12-05 17:51:52.711784: step 31090, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 71h:19m:37s remains)
INFO - root - 2017-12-05 17:52:01.191549: step 31100, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 69h:00m:23s remains)
2017-12-05 17:52:01.907907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1265445 -4.1348805 -4.1244931 -4.1012568 -4.0838327 -4.1024384 -4.1629286 -4.2059231 -4.2164226 -4.2042894 -4.186686 -4.1719909 -4.1664672 -4.1897454 -4.2235093][-4.1319089 -4.1323457 -4.11469 -4.0917134 -4.0777359 -4.0956426 -4.1473861 -4.1842785 -4.202858 -4.2049794 -4.2000308 -4.1891637 -4.1778073 -4.1858263 -4.2076674][-4.1500568 -4.1429334 -4.1183195 -4.097352 -4.0890636 -4.1018152 -4.129777 -4.1505814 -4.1743541 -4.1911726 -4.2014103 -4.1988807 -4.1840224 -4.17909 -4.1930571][-4.1674848 -4.1535673 -4.1214981 -4.1047211 -4.1028852 -4.1043878 -4.1058331 -4.109344 -4.1375971 -4.1684074 -4.1925163 -4.1989989 -4.1885777 -4.1784387 -4.1889877][-4.1707268 -4.1522903 -4.1188488 -4.1052113 -4.1044621 -4.095315 -4.0750337 -4.0668669 -4.1037068 -4.15067 -4.1838346 -4.1952825 -4.191967 -4.1829262 -4.1954126][-4.1740031 -4.1562538 -4.1258554 -4.110918 -4.1025519 -4.0792451 -4.0395446 -4.027761 -4.0802226 -4.1443067 -4.1812344 -4.1899 -4.1862373 -4.1811509 -4.2029815][-4.1831779 -4.1702642 -4.1451516 -4.1294179 -4.1015396 -4.0545 -4.0008965 -4.000772 -4.0774212 -4.1566672 -4.1930285 -4.1905503 -4.1791682 -4.1776624 -4.2083073][-4.1908188 -4.1853456 -4.1690812 -4.1502819 -4.0952621 -4.0197411 -3.9652934 -3.9943411 -4.0962157 -4.1825151 -4.212873 -4.1989331 -4.1816535 -4.1840186 -4.2179103][-4.1973019 -4.2022181 -4.1952405 -4.1726842 -4.0956073 -3.9987292 -3.9541383 -4.0136604 -4.1258821 -4.2056813 -4.2256455 -4.2053661 -4.1928072 -4.20436 -4.2391696][-4.2040896 -4.2168274 -4.2151752 -4.18989 -4.1093774 -4.0108895 -3.9848089 -4.0582075 -4.1592712 -4.2202234 -4.2254338 -4.2025251 -4.202076 -4.2267475 -4.263031][-4.2077484 -4.2244244 -4.2237911 -4.2013669 -4.1300235 -4.0495868 -4.0434489 -4.1138239 -4.1920338 -4.2294469 -4.2202306 -4.1961708 -4.205915 -4.238317 -4.2723913][-4.2132893 -4.2283072 -4.2277751 -4.2114277 -4.1580267 -4.102766 -4.1117563 -4.1699347 -4.2237759 -4.238688 -4.2189975 -4.1976557 -4.2126379 -4.2444124 -4.2722831][-4.2113681 -4.2208471 -4.2191839 -4.2133789 -4.1820793 -4.1534481 -4.1715307 -4.212646 -4.243207 -4.2411323 -4.2185245 -4.2041059 -4.2222419 -4.2491994 -4.2683849][-4.2124004 -4.215416 -4.2121658 -4.2134666 -4.1999192 -4.19064 -4.209877 -4.2345715 -4.2492156 -4.2420073 -4.2246666 -4.2183833 -4.2342691 -4.2527595 -4.2630711][-4.2269421 -4.2259407 -4.2227306 -4.2306285 -4.2279596 -4.2257314 -4.2401633 -4.2531118 -4.2587113 -4.2520027 -4.2405634 -4.2397423 -4.2507658 -4.2610278 -4.2648563]]...]
INFO - root - 2017-12-05 17:52:10.518674: step 31110, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 69h:56m:37s remains)
INFO - root - 2017-12-05 17:52:19.114723: step 31120, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 71h:29m:34s remains)
INFO - root - 2017-12-05 17:52:27.652929: step 31130, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 71h:23m:40s remains)
INFO - root - 2017-12-05 17:52:35.933757: step 31140, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 72h:16m:17s remains)
INFO - root - 2017-12-05 17:52:44.437916: step 31150, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 71h:05m:54s remains)
INFO - root - 2017-12-05 17:52:52.939853: step 31160, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 70h:46m:48s remains)
INFO - root - 2017-12-05 17:53:01.392385: step 31170, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 72h:12m:48s remains)
INFO - root - 2017-12-05 17:53:09.939772: step 31180, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 74h:11m:57s remains)
INFO - root - 2017-12-05 17:53:18.439574: step 31190, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 74h:14m:43s remains)
INFO - root - 2017-12-05 17:53:27.148615: step 31200, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 81h:18m:35s remains)
2017-12-05 17:53:27.948343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1675038 -4.1493416 -4.1387506 -4.1321034 -4.1371975 -4.1453762 -4.1548185 -4.1675053 -4.1902504 -4.22146 -4.2511554 -4.2714219 -4.273654 -4.2659597 -4.2501936][-4.1606317 -4.1435885 -4.13639 -4.1268516 -4.1259847 -4.1310878 -4.1420636 -4.1580057 -4.1840057 -4.2187414 -4.2517242 -4.2738109 -4.2719693 -4.2535462 -4.2261958][-4.1551108 -4.1439505 -4.1441059 -4.1396852 -4.1347919 -4.1333208 -4.1360779 -4.1468377 -4.1717277 -4.2080679 -4.2431521 -4.2671585 -4.2640419 -4.2388682 -4.2044234][-4.1492844 -4.1457319 -4.1545248 -4.1555939 -4.146966 -4.1362791 -4.1279969 -4.1289697 -4.1443858 -4.180881 -4.2177558 -4.2440128 -4.2484488 -4.2274041 -4.1954317][-4.1519361 -4.1565409 -4.1704144 -4.1715508 -4.1521654 -4.1249423 -4.0935097 -4.0707064 -4.071898 -4.1185069 -4.16636 -4.1998138 -4.2180405 -4.2126274 -4.1943846][-4.1534095 -4.1630712 -4.1744561 -4.1625767 -4.1229959 -4.0741496 -4.0094514 -3.9417863 -3.9196272 -3.9932706 -4.0744128 -4.1270561 -4.1605806 -4.1733313 -4.1712394][-4.1536903 -4.1649594 -4.1657038 -4.133287 -4.0638304 -3.9802704 -3.87698 -3.7578511 -3.7052457 -3.8233776 -3.9605522 -4.0410628 -4.0848117 -4.1094747 -4.1194286][-4.175416 -4.1828227 -4.1673346 -4.1142931 -4.0175228 -3.8998361 -3.7645979 -3.6156185 -3.5565231 -3.7195005 -3.8996108 -4.0020375 -4.0514245 -4.0842161 -4.1010256][-4.2122865 -4.21072 -4.1867275 -4.1265779 -4.02493 -3.8996713 -3.7689891 -3.6592441 -3.6413467 -3.7924151 -3.9482167 -4.0401616 -4.083003 -4.1106 -4.1258435][-4.2453032 -4.237339 -4.2132788 -4.1607256 -4.0791521 -3.9814425 -3.886507 -3.8299727 -3.8432472 -3.9505231 -4.0546308 -4.1191587 -4.1482434 -4.1644392 -4.1744442][-4.2650166 -4.2543626 -4.2367659 -4.2055373 -4.156775 -4.0970507 -4.040822 -4.0171595 -4.0375628 -4.1010661 -4.1588097 -4.1929131 -4.2059779 -4.2119679 -4.2150092][-4.2701612 -4.2668896 -4.2634196 -4.2590919 -4.2419963 -4.2111344 -4.1816745 -4.1704259 -4.1775193 -4.2012992 -4.2220244 -4.2341256 -4.2370305 -4.2364068 -4.2339888][-4.26664 -4.276165 -4.2882438 -4.301465 -4.3022542 -4.28967 -4.2756152 -4.2659621 -4.2595387 -4.2551694 -4.2497511 -4.245533 -4.2409625 -4.2356567 -4.2296224][-4.2473164 -4.2680717 -4.2953677 -4.3190355 -4.3308115 -4.3311343 -4.3256526 -4.3133993 -4.2960362 -4.273375 -4.2528567 -4.2392182 -4.2318535 -4.225637 -4.2176218][-4.21057 -4.2365937 -4.2754154 -4.3070483 -4.3261323 -4.3328085 -4.3290257 -4.3150496 -4.2929711 -4.2658358 -4.2423654 -4.2266831 -4.2174077 -4.2107773 -4.2002416]]...]
INFO - root - 2017-12-05 17:53:36.375729: step 31210, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 70h:16m:37s remains)
INFO - root - 2017-12-05 17:53:44.832171: step 31220, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 69h:31m:16s remains)
INFO - root - 2017-12-05 17:53:53.328961: step 31230, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 70h:43m:03s remains)
INFO - root - 2017-12-05 17:54:01.718418: step 31240, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.851 sec/batch; 71h:13m:51s remains)
INFO - root - 2017-12-05 17:54:10.110301: step 31250, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 69h:33m:04s remains)
INFO - root - 2017-12-05 17:54:18.713248: step 31260, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 72h:10m:17s remains)
INFO - root - 2017-12-05 17:54:27.161051: step 31270, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 71h:22m:57s remains)
INFO - root - 2017-12-05 17:54:35.649806: step 31280, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 71h:19m:38s remains)
INFO - root - 2017-12-05 17:54:44.177787: step 31290, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.856 sec/batch; 71h:34m:49s remains)
INFO - root - 2017-12-05 17:54:52.574056: step 31300, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 0.782 sec/batch; 65h:25m:41s remains)
2017-12-05 17:54:53.323929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1467886 -4.1318645 -4.1177144 -4.1105642 -4.1275706 -4.16238 -4.191853 -4.208375 -4.2132559 -4.2062769 -4.1902666 -4.1693144 -4.1484523 -4.1311646 -4.125308][-4.165946 -4.1511855 -4.1420321 -4.1435218 -4.1660957 -4.1999598 -4.2246008 -4.2366247 -4.2369208 -4.2266064 -4.2082524 -4.1829929 -4.1575923 -4.1396484 -4.1353331][-4.1786985 -4.1738124 -4.1736908 -4.1800508 -4.20014 -4.2253461 -4.2429543 -4.25184 -4.2523222 -4.2450056 -4.2313724 -4.2092605 -4.1854696 -4.170609 -4.1694789][-4.178308 -4.1843328 -4.19121 -4.1974878 -4.2098289 -4.224431 -4.2347822 -4.2418022 -4.2453289 -4.245615 -4.2416506 -4.2293258 -4.2147856 -4.2075257 -4.2112327][-4.1722245 -4.1809063 -4.1873255 -4.1875668 -4.1883259 -4.1905 -4.1926994 -4.1998587 -4.2110395 -4.222826 -4.2328072 -4.2360067 -4.2371597 -4.24127 -4.2509742][-4.17455 -4.1734457 -4.1709547 -4.1617103 -4.1497808 -4.1392708 -4.1326613 -4.1397176 -4.1592183 -4.1829743 -4.2070551 -4.2246895 -4.242322 -4.2607088 -4.2764158][-4.1869969 -4.1704855 -4.1539903 -4.1308479 -4.1035891 -4.0801687 -4.0665164 -4.0741043 -4.1009588 -4.1334515 -4.1675916 -4.196094 -4.2268362 -4.2565331 -4.2765632][-4.2081213 -4.1825962 -4.156033 -4.1213484 -4.0830641 -4.0528059 -4.0393124 -4.0489011 -4.07273 -4.1011167 -4.1332517 -4.16345 -4.199779 -4.23436 -4.2567778][-4.2358484 -4.2076879 -4.1760068 -4.1374941 -4.0991092 -4.07272 -4.0655336 -4.07419 -4.0848813 -4.0980196 -4.1183009 -4.1420503 -4.1755037 -4.2077751 -4.2287955][-4.2612948 -4.2346783 -4.2030835 -4.1675477 -4.1345768 -4.1141849 -4.1105123 -4.1139512 -4.1116362 -4.1104527 -4.118145 -4.1333303 -4.1594253 -4.1832223 -4.1984324][-4.2769418 -4.2545609 -4.2281322 -4.1998758 -4.1752205 -4.1615763 -4.1592469 -4.1560793 -4.1440105 -4.1333756 -4.1300764 -4.1356721 -4.1531234 -4.1673551 -4.1761518][-4.2835774 -4.2672119 -4.2488909 -4.2312269 -4.2176895 -4.2124867 -4.2125807 -4.2075109 -4.1933002 -4.179801 -4.1689653 -4.1638951 -4.16706 -4.1680021 -4.167901][-4.2814045 -4.2724943 -4.2640085 -4.2580295 -4.2560277 -4.2589626 -4.2624335 -4.2591977 -4.2471461 -4.2337284 -4.217628 -4.2021728 -4.1889148 -4.1760259 -4.1658239][-4.2747726 -4.2717481 -4.27198 -4.2755675 -4.2818766 -4.29044 -4.2967811 -4.2963715 -4.2862577 -4.2706008 -4.2473049 -4.2194419 -4.191946 -4.1708155 -4.1556749][-4.2692089 -4.2670293 -4.2700496 -4.2764187 -4.2842646 -4.2925344 -4.2980256 -4.2995963 -4.2914662 -4.2747149 -4.2484651 -4.2155461 -4.1834335 -4.159852 -4.1424255]]...]
INFO - root - 2017-12-05 17:55:01.930155: step 31310, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.885 sec/batch; 74h:02m:15s remains)
INFO - root - 2017-12-05 17:55:10.349534: step 31320, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 0.798 sec/batch; 66h:46m:57s remains)
INFO - root - 2017-12-05 17:55:18.807926: step 31330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 72h:30m:36s remains)
INFO - root - 2017-12-05 17:55:27.483504: step 31340, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 72h:01m:52s remains)
INFO - root - 2017-12-05 17:55:36.017821: step 31350, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 70h:19m:58s remains)
INFO - root - 2017-12-05 17:55:44.516332: step 31360, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.818 sec/batch; 68h:26m:31s remains)
INFO - root - 2017-12-05 17:55:53.216082: step 31370, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 72h:45m:26s remains)
INFO - root - 2017-12-05 17:56:01.814851: step 31380, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 72h:09m:08s remains)
INFO - root - 2017-12-05 17:56:10.370182: step 31390, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 71h:45m:46s remains)
INFO - root - 2017-12-05 17:56:18.913598: step 31400, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.831 sec/batch; 69h:32m:10s remains)
2017-12-05 17:56:19.668019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3083873 -4.2944236 -4.2888107 -4.2814708 -4.2669382 -4.2537723 -4.2460575 -4.2476873 -4.247942 -4.2487016 -4.24956 -4.2481446 -4.2460246 -4.24 -4.2304039][-4.30421 -4.2900939 -4.2827363 -4.2735658 -4.2596383 -4.24863 -4.2415862 -4.2432013 -4.2435493 -4.24568 -4.2505612 -4.2564321 -4.2604508 -4.2607055 -4.2512159][-4.3037043 -4.2903819 -4.2804732 -4.2693167 -4.2547154 -4.2416167 -4.2307982 -4.2313128 -4.235045 -4.240551 -4.2477579 -4.2565165 -4.2624788 -4.2618313 -4.2484407][-4.3025103 -4.2877245 -4.2753191 -4.2618151 -4.245225 -4.2274075 -4.2100358 -4.2072253 -4.2155471 -4.229455 -4.2427716 -4.257113 -4.2628384 -4.2563415 -4.2363105][-4.296771 -4.2777939 -4.2600842 -4.243576 -4.2261138 -4.2046475 -4.1821637 -4.1737604 -4.1863074 -4.2107096 -4.2333159 -4.2521935 -4.2564669 -4.2445912 -4.2214751][-4.2888665 -4.26414 -4.2386994 -4.2162695 -4.1921024 -4.1607294 -4.126565 -4.109941 -4.1286983 -4.1700959 -4.2090497 -4.2382035 -4.2491493 -4.2392287 -4.2200284][-4.2803774 -4.2505474 -4.2163534 -4.1838355 -4.1473875 -4.1009364 -4.05349 -4.0312963 -4.062942 -4.1282439 -4.1876669 -4.2289243 -4.2441134 -4.23371 -4.2153811][-4.2719736 -4.2393684 -4.2017813 -4.1632566 -4.1150131 -4.0579004 -4.0042386 -3.9816396 -4.0221238 -4.1040797 -4.1737666 -4.2211919 -4.2375402 -4.22246 -4.1988621][-4.2667794 -4.234786 -4.2011957 -4.1687307 -4.1230626 -4.0674233 -4.0219769 -4.0098572 -4.0499959 -4.1271324 -4.1861186 -4.2227635 -4.2293406 -4.2047844 -4.1690216][-4.2628937 -4.2295566 -4.2012115 -4.17849 -4.1423249 -4.0950875 -4.0632715 -4.0665841 -4.1070366 -4.1695924 -4.2065353 -4.2193365 -4.2050576 -4.168673 -4.1255794][-4.2596025 -4.2237253 -4.1950083 -4.1771665 -4.147265 -4.1075158 -4.0813046 -4.0900879 -4.1282463 -4.1780519 -4.1994247 -4.193871 -4.1615572 -4.1152968 -4.0727181][-4.2565565 -4.2187276 -4.1888161 -4.1709824 -4.1473317 -4.1167879 -4.0960569 -4.1026535 -4.1338272 -4.1721497 -4.1810017 -4.1694136 -4.1334486 -4.0909085 -4.0554385][-4.2612209 -4.2245145 -4.1964245 -4.1787944 -4.159966 -4.1361237 -4.1202588 -4.123189 -4.1472478 -4.1733294 -4.1741562 -4.1598268 -4.129632 -4.0982122 -4.0719113][-4.2704411 -4.236465 -4.2124658 -4.1973686 -4.1825175 -4.1643381 -4.1533937 -4.153954 -4.1684303 -4.18458 -4.1832104 -4.1733489 -4.1552267 -4.1405272 -4.124445][-4.2819095 -4.2500153 -4.2288785 -4.2137313 -4.1991668 -4.1848631 -4.1807666 -4.1832047 -4.1907992 -4.197803 -4.1949911 -4.1893196 -4.1849108 -4.1849384 -4.1804223]]...]
INFO - root - 2017-12-05 17:56:28.014986: step 31410, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 72h:50m:13s remains)
INFO - root - 2017-12-05 17:56:36.516023: step 31420, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 69h:36m:11s remains)
INFO - root - 2017-12-05 17:56:45.015596: step 31430, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 70h:45m:27s remains)
INFO - root - 2017-12-05 17:56:53.378794: step 31440, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 69h:45m:38s remains)
INFO - root - 2017-12-05 17:57:02.008644: step 31450, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 73h:22m:56s remains)
INFO - root - 2017-12-05 17:57:10.666134: step 31460, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 71h:21m:50s remains)
INFO - root - 2017-12-05 17:57:19.176248: step 31470, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 71h:39m:55s remains)
INFO - root - 2017-12-05 17:57:27.804307: step 31480, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 71h:09m:19s remains)
INFO - root - 2017-12-05 17:57:36.380291: step 31490, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 74h:02m:55s remains)
INFO - root - 2017-12-05 17:57:44.887746: step 31500, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 69h:24m:08s remains)
2017-12-05 17:57:45.613739: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2199574 -4.2308226 -4.2309361 -4.2216768 -4.2032809 -4.1739931 -4.1426544 -4.1350188 -4.1652427 -4.2161369 -4.2695975 -4.307991 -4.3240843 -4.3230524 -4.3181262][-4.26231 -4.2623425 -4.2514887 -4.22784 -4.1906242 -4.14235 -4.0933208 -4.0761776 -4.112565 -4.1846633 -4.264657 -4.3244414 -4.3520679 -4.3580461 -4.3574224][-4.3008914 -4.2894764 -4.265398 -4.2277431 -4.1774516 -4.1130366 -4.0402131 -4.0005884 -4.0298944 -4.1128359 -4.2192082 -4.3026571 -4.3458848 -4.3612895 -4.3650331][-4.3206611 -4.3004041 -4.26883 -4.2244925 -4.1673851 -4.0922465 -4.0003066 -3.9352846 -3.9499605 -4.0352693 -4.1565051 -4.2576022 -4.3180103 -4.3446074 -4.3514833][-4.3249912 -4.3011336 -4.2666931 -4.2179232 -4.1546659 -4.0709691 -3.9664555 -3.8849223 -3.8886228 -3.9732225 -4.1011195 -4.2110257 -4.2844186 -4.3219209 -4.3335323][-4.3203769 -4.2958736 -4.2614727 -4.2107892 -4.1436043 -4.053452 -3.9438291 -3.8515496 -3.8457088 -3.9321785 -4.065876 -4.1828361 -4.2644277 -4.3095608 -4.3246331][-4.314528 -4.2904534 -4.2570686 -4.2084966 -4.14145 -4.0487647 -3.9366393 -3.8384027 -3.8237662 -3.9073176 -4.0452876 -4.1727123 -4.2627678 -4.3119454 -4.3291116][-4.3063812 -4.2832942 -4.2493653 -4.2028508 -4.138845 -4.0512638 -3.9478104 -3.8564904 -3.8376832 -3.9131508 -4.0449233 -4.1753969 -4.2709031 -4.3192096 -4.3372169][-4.2972717 -4.2764649 -4.2427092 -4.197679 -4.1396914 -4.0636635 -3.9777286 -3.9040003 -3.8869243 -3.953017 -4.0692458 -4.18887 -4.281024 -4.3272285 -4.3452682][-4.29382 -4.2784691 -4.2465549 -4.2035966 -4.1519909 -4.0888786 -4.0221577 -3.9678707 -3.9553232 -4.0135522 -4.1135416 -4.2167935 -4.2967672 -4.3378506 -4.3524013][-4.2994075 -4.2894139 -4.262876 -4.2255797 -4.1821609 -4.1339655 -4.09057 -4.0610991 -4.0578151 -4.1057539 -4.1824636 -4.2622266 -4.322053 -4.3507791 -4.3590403][-4.3101716 -4.305254 -4.2882547 -4.2614026 -4.2304249 -4.1989937 -4.174561 -4.1627879 -4.1671987 -4.2018023 -4.2532516 -4.3067932 -4.3453164 -4.3611422 -4.3626137][-4.3223934 -4.3218017 -4.3151708 -4.301 -4.2836986 -4.268477 -4.2592731 -4.256875 -4.2649589 -4.2880993 -4.3166304 -4.3454695 -4.3644586 -4.3679318 -4.3637261][-4.3340449 -4.3355289 -4.3348217 -4.3301625 -4.3237844 -4.3200378 -4.3201466 -4.3222351 -4.33148 -4.3451843 -4.3586683 -4.3699632 -4.37508 -4.3705974 -4.3633928][-4.3432407 -4.3445024 -4.3454671 -4.3460627 -4.3464317 -4.348608 -4.3521948 -4.3564191 -4.3635492 -4.3707223 -4.3747263 -4.3760891 -4.3740439 -4.3669372 -4.3606014]]...]
INFO - root - 2017-12-05 17:57:54.141319: step 31510, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 72h:30m:27s remains)
INFO - root - 2017-12-05 17:58:02.513411: step 31520, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 69h:23m:49s remains)
INFO - root - 2017-12-05 17:58:10.955157: step 31530, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 70h:47m:49s remains)
INFO - root - 2017-12-05 17:58:19.331225: step 31540, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 70h:44m:20s remains)
INFO - root - 2017-12-05 17:58:27.870108: step 31550, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 69h:40m:34s remains)
INFO - root - 2017-12-05 17:58:36.419863: step 31560, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 71h:55m:32s remains)
INFO - root - 2017-12-05 17:58:45.038768: step 31570, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 70h:07m:03s remains)
INFO - root - 2017-12-05 17:58:53.445176: step 31580, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 71h:07m:29s remains)
INFO - root - 2017-12-05 17:59:01.932706: step 31590, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 72h:07m:05s remains)
INFO - root - 2017-12-05 17:59:10.480420: step 31600, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 69h:59m:25s remains)
2017-12-05 17:59:11.230284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1677513 -4.1821561 -4.1952043 -4.2106843 -4.2101684 -4.2168469 -4.2331328 -4.2469864 -4.2552166 -4.24724 -4.228951 -4.2121263 -4.217422 -4.249557 -4.2875962][-4.175065 -4.1929393 -4.2048736 -4.2188835 -4.2178841 -4.22412 -4.2427874 -4.2634974 -4.2760129 -4.2641964 -4.2356243 -4.2059846 -4.2038193 -4.2390189 -4.2817297][-4.1900597 -4.203742 -4.2040248 -4.2069507 -4.2014432 -4.2063241 -4.2298517 -4.2578788 -4.274405 -4.2603621 -4.226717 -4.1895308 -4.1812391 -4.2180595 -4.2616544][-4.1988072 -4.2028804 -4.1895819 -4.1770811 -4.160882 -4.1594543 -4.1859303 -4.2219028 -4.2464848 -4.2391191 -4.2116971 -4.1771827 -4.1685739 -4.2007017 -4.2367435][-4.2095256 -4.2027445 -4.1760616 -4.14291 -4.109457 -4.0947309 -4.120162 -4.1622238 -4.195219 -4.2028861 -4.1974349 -4.179378 -4.175386 -4.195323 -4.2168403][-4.2080007 -4.1928358 -4.1574144 -4.1053829 -4.0512252 -4.0221019 -4.0436239 -4.0843997 -4.1170297 -4.1391616 -4.1629939 -4.1720963 -4.1740413 -4.179987 -4.1863923][-4.2048283 -4.1926608 -4.159183 -4.0955019 -4.0212717 -3.975672 -3.9851615 -4.0081391 -4.0236497 -4.05149 -4.1054492 -4.1447835 -4.1585107 -4.1581359 -4.1555767][-4.2125964 -4.2090788 -4.1871176 -4.1273031 -4.0503926 -3.9930193 -3.9799271 -3.9691439 -3.9518766 -3.974227 -4.0554671 -4.1265545 -4.1575284 -4.1587992 -4.153513][-4.2300806 -4.2370124 -4.2323685 -4.1921015 -4.1333752 -4.0834351 -4.0538464 -4.0151458 -3.9714241 -3.9784484 -4.061285 -4.1407313 -4.1788244 -4.1822691 -4.1749759][-4.2422581 -4.2551656 -4.26126 -4.2383552 -4.2016864 -4.1688857 -4.1404271 -4.099164 -4.057724 -4.0594192 -4.1225924 -4.1852994 -4.2139983 -4.2112203 -4.2008915][-4.2522264 -4.2623773 -4.2705173 -4.2580919 -4.2378945 -4.2230129 -4.206233 -4.1809635 -4.1580181 -4.1624093 -4.2021322 -4.2413459 -4.2557306 -4.2465863 -4.2328129][-4.2577548 -4.2635708 -4.2702084 -4.2653556 -4.2540975 -4.2478042 -4.2410588 -4.2294903 -4.2210779 -4.2271557 -4.253624 -4.2792253 -4.285388 -4.27285 -4.257184][-4.2706838 -4.2724872 -4.2751751 -4.2725658 -4.2674108 -4.2646551 -4.2622318 -4.2578831 -4.2563648 -4.2633324 -4.2819595 -4.2996798 -4.3040977 -4.2952886 -4.2829876][-4.2972302 -4.2985678 -4.2986956 -4.2957363 -4.2932243 -4.2921629 -4.2918553 -4.2906423 -4.2908206 -4.2939706 -4.3032885 -4.3134189 -4.3174167 -4.3143258 -4.3085117][-4.3218 -4.3237019 -4.3232384 -4.3210492 -4.3196144 -4.318728 -4.3180184 -4.3176332 -4.317946 -4.3188376 -4.3225207 -4.32658 -4.3288345 -4.3284955 -4.3263969]]...]
INFO - root - 2017-12-05 17:59:19.682538: step 31610, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 70h:37m:33s remains)
INFO - root - 2017-12-05 17:59:28.048603: step 31620, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 70h:18m:23s remains)
INFO - root - 2017-12-05 17:59:36.650156: step 31630, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 70h:53m:25s remains)
INFO - root - 2017-12-05 17:59:45.085611: step 31640, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 69h:31m:28s remains)
INFO - root - 2017-12-05 17:59:53.528876: step 31650, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.842 sec/batch; 70h:20m:10s remains)
INFO - root - 2017-12-05 18:00:01.965594: step 31660, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 71h:33m:30s remains)
INFO - root - 2017-12-05 18:00:10.552778: step 31670, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 71h:01m:20s remains)
INFO - root - 2017-12-05 18:00:19.107968: step 31680, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 72h:32m:38s remains)
INFO - root - 2017-12-05 18:00:27.539453: step 31690, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 69h:11m:24s remains)
INFO - root - 2017-12-05 18:00:36.002092: step 31700, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 73h:25m:00s remains)
2017-12-05 18:00:36.733903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1270304 -4.1259303 -4.1214423 -4.1218081 -4.1293464 -4.1282406 -4.1219287 -4.11758 -4.116715 -4.119987 -4.1248779 -4.1286888 -4.1263819 -4.1325979 -4.1534319][-4.1013293 -4.1024504 -4.1063867 -4.1212587 -4.1361227 -4.1386604 -4.1289725 -4.123857 -4.1227722 -4.1215949 -4.1174259 -4.1120467 -4.1106744 -4.1245036 -4.1515961][-4.0863972 -4.079411 -4.0849371 -4.107923 -4.1233206 -4.1270308 -4.1189337 -4.1148863 -4.1165686 -4.1203227 -4.1150622 -4.1014256 -4.0999489 -4.1219764 -4.1543732][-4.0991731 -4.0839539 -4.0859985 -4.1094966 -4.1203337 -4.1163678 -4.10623 -4.1040463 -4.1111703 -4.1274395 -4.1317854 -4.1170864 -4.1123023 -4.1301308 -4.1622305][-4.1115403 -4.0976052 -4.1001263 -4.1219344 -4.1278958 -4.1068482 -4.0770373 -4.0720911 -4.094316 -4.1263862 -4.1503897 -4.1476369 -4.1384072 -4.1472106 -4.1727047][-4.0891709 -4.0842819 -4.0982623 -4.1213603 -4.1243367 -4.0903645 -4.0361104 -4.0122728 -4.0458431 -4.0999956 -4.147819 -4.1610684 -4.1550412 -4.1592469 -4.1782193][-4.0519962 -4.0566325 -4.0817595 -4.1066461 -4.107141 -4.0611606 -3.9789939 -3.9277411 -3.9674928 -4.0525255 -4.127162 -4.1557961 -4.1548338 -4.1600242 -4.17885][-4.0503631 -4.0545182 -4.0791903 -4.1011972 -4.0878396 -4.0234594 -3.9209027 -3.8623843 -3.9141243 -4.0238271 -4.1087556 -4.144165 -4.1467843 -4.1594596 -4.1836953][-4.0826507 -4.0842061 -4.104207 -4.1176319 -4.0961866 -4.0338511 -3.9505103 -3.9125197 -3.9648366 -4.0608106 -4.1236196 -4.1500835 -4.1531343 -4.1697464 -4.1936793][-4.1237054 -4.121985 -4.1373115 -4.1498394 -4.1357541 -4.0965986 -4.0497346 -4.0313621 -4.0683441 -4.1284156 -4.1521373 -4.1587214 -4.1642256 -4.1855488 -4.2095046][-4.1463289 -4.1473794 -4.1643562 -4.176332 -4.1757631 -4.1671453 -4.1481886 -4.1371675 -4.1575446 -4.1872625 -4.18262 -4.1694927 -4.1713929 -4.197803 -4.228272][-4.1793566 -4.1800313 -4.198607 -4.2078786 -4.2118926 -4.2130117 -4.207583 -4.1991339 -4.2108746 -4.22358 -4.204915 -4.1794729 -4.1788535 -4.2080317 -4.2464581][-4.2339449 -4.2353787 -4.2510676 -4.2601027 -4.2637186 -4.261548 -4.2540331 -4.2443328 -4.2474055 -4.247366 -4.2247319 -4.2014709 -4.2004337 -4.2297144 -4.267508][-4.28318 -4.287806 -4.2968326 -4.3033943 -4.3084178 -4.304491 -4.2906613 -4.2800903 -4.2772474 -4.2683058 -4.248354 -4.2334504 -4.2335768 -4.2557645 -4.2844629][-4.297194 -4.3007755 -4.3061342 -4.3113322 -4.3157134 -4.3127847 -4.3010044 -4.2941985 -4.2906127 -4.2809706 -4.26735 -4.2608809 -4.2609668 -4.2750473 -4.2970586]]...]
INFO - root - 2017-12-05 18:00:45.293615: step 31710, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 72h:26m:00s remains)
INFO - root - 2017-12-05 18:00:53.823784: step 31720, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 69h:54m:18s remains)
INFO - root - 2017-12-05 18:01:02.330467: step 31730, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 72h:42m:01s remains)
INFO - root - 2017-12-05 18:01:10.705070: step 31740, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 70h:45m:41s remains)
INFO - root - 2017-12-05 18:01:19.302714: step 31750, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 71h:29m:35s remains)
INFO - root - 2017-12-05 18:01:27.782832: step 31760, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 68h:14m:57s remains)
INFO - root - 2017-12-05 18:01:36.270201: step 31770, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 72h:47m:06s remains)
INFO - root - 2017-12-05 18:01:44.796750: step 31780, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 69h:24m:44s remains)
INFO - root - 2017-12-05 18:01:53.280980: step 31790, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.868 sec/batch; 72h:31m:03s remains)
INFO - root - 2017-12-05 18:02:01.665804: step 31800, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 68h:44m:19s remains)
2017-12-05 18:02:02.419803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2781549 -4.2640562 -4.2510018 -4.2400618 -4.2251472 -4.2169476 -4.2197967 -4.2344975 -4.244925 -4.2458162 -4.244204 -4.2498131 -4.2406988 -4.2162919 -4.209157][-4.2790408 -4.2723608 -4.2651706 -4.2543478 -4.2389674 -4.2266006 -4.2205725 -4.2294583 -4.2429347 -4.2530789 -4.2584357 -4.256969 -4.234127 -4.1993032 -4.1884308][-4.2740283 -4.2720876 -4.2673669 -4.2569804 -4.2397561 -4.2204733 -4.2002525 -4.2010136 -4.224689 -4.2480407 -4.2598357 -4.2508812 -4.2155924 -4.1670413 -4.1497474][-4.2672763 -4.2687325 -4.2657552 -4.2556782 -4.2319775 -4.1965952 -4.1526728 -4.1451573 -4.1864414 -4.2265773 -4.2487516 -4.241384 -4.2018814 -4.1451745 -4.1214643][-4.2591357 -4.2625494 -4.2601323 -4.2499018 -4.2205958 -4.168848 -4.105032 -4.0900245 -4.1470714 -4.20654 -4.2438035 -4.2478638 -4.2154665 -4.1621032 -4.1332378][-4.2565832 -4.2565174 -4.2495179 -4.2361016 -4.2042475 -4.1390824 -4.0535803 -4.0304327 -4.1021352 -4.1812253 -4.2384667 -4.2587376 -4.2412829 -4.1988397 -4.1686721][-4.2510409 -4.2468619 -4.2316914 -4.2065382 -4.1603432 -4.0773964 -3.9689729 -3.9357035 -4.0207095 -4.1258764 -4.21124 -4.2555876 -4.2576509 -4.2300963 -4.2018247][-4.2230067 -4.2196856 -4.2004309 -4.17068 -4.1192989 -4.0241556 -3.903441 -3.8640311 -3.9493108 -4.06315 -4.1694927 -4.2305841 -4.2459612 -4.2313051 -4.2077231][-4.1928415 -4.1983275 -4.1871943 -4.1676955 -4.1293025 -4.0483 -3.9380274 -3.8993306 -3.9659977 -4.0599303 -4.1514597 -4.2079387 -4.2270241 -4.2194204 -4.2011042][-4.1682682 -4.1882319 -4.1914682 -4.1823888 -4.1581893 -4.1021137 -4.0157852 -3.9919028 -4.0388193 -4.096735 -4.155632 -4.2006435 -4.2223258 -4.2212672 -4.2061234][-4.1439137 -4.1774554 -4.1958704 -4.2014384 -4.1898026 -4.1508417 -4.0854855 -4.0730138 -4.1036153 -4.1306806 -4.1611881 -4.1973176 -4.2230358 -4.2275267 -4.2166052][-4.140605 -4.1782384 -4.2050562 -4.2175455 -4.2109323 -4.17969 -4.1276064 -4.116117 -4.1339021 -4.1455 -4.1629119 -4.1950622 -4.2248077 -4.2372279 -4.2328768][-4.167768 -4.1943884 -4.2171259 -4.2294235 -4.2260537 -4.2061343 -4.1702738 -4.1620517 -4.1716809 -4.1764216 -4.1854744 -4.2087197 -4.2323585 -4.2479095 -4.2509289][-4.2075043 -4.2161236 -4.2248764 -4.2306929 -4.2291994 -4.2208261 -4.2050314 -4.2051153 -4.2115812 -4.2132487 -4.2170196 -4.2292132 -4.2423143 -4.2521377 -4.2549338][-4.2369108 -4.2352619 -4.233521 -4.2339664 -4.2339377 -4.233242 -4.229959 -4.2340631 -4.2381978 -4.2393761 -4.2411518 -4.2450571 -4.2480111 -4.2499094 -4.248682]]...]
INFO - root - 2017-12-05 18:02:10.818968: step 31810, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.835 sec/batch; 69h:43m:39s remains)
INFO - root - 2017-12-05 18:02:19.258789: step 31820, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.818 sec/batch; 68h:18m:04s remains)
INFO - root - 2017-12-05 18:02:27.663492: step 31830, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 72h:32m:25s remains)
INFO - root - 2017-12-05 18:02:35.916650: step 31840, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 70h:05m:17s remains)
INFO - root - 2017-12-05 18:02:44.259521: step 31850, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 71h:11m:34s remains)
INFO - root - 2017-12-05 18:02:52.814782: step 31860, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.844 sec/batch; 70h:26m:56s remains)
INFO - root - 2017-12-05 18:03:01.313083: step 31870, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.822 sec/batch; 68h:37m:52s remains)
INFO - root - 2017-12-05 18:03:09.909694: step 31880, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 71h:30m:24s remains)
INFO - root - 2017-12-05 18:03:18.437951: step 31890, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 71h:30m:05s remains)
INFO - root - 2017-12-05 18:03:27.009500: step 31900, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 73h:11m:02s remains)
2017-12-05 18:03:27.756039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2441397 -4.2057481 -4.1634536 -4.1382809 -4.1394711 -4.1674747 -4.1943417 -4.1931629 -4.186223 -4.1767979 -4.1646667 -4.1580834 -4.165781 -4.1932373 -4.2265482][-4.2536221 -4.2202311 -4.1718688 -4.1369081 -4.1342649 -4.1597018 -4.1770658 -4.1684389 -4.1632128 -4.1665115 -4.167531 -4.1694193 -4.1811967 -4.2106957 -4.2445626][-4.2657142 -4.2366314 -4.1829495 -4.1373539 -4.1282253 -4.1434245 -4.1438336 -4.1246376 -4.1255651 -4.1461687 -4.1649256 -4.1803641 -4.2020659 -4.2361236 -4.2681737][-4.2734437 -4.2478051 -4.1909947 -4.1357226 -4.1171193 -4.1167855 -4.0985336 -4.0678959 -4.0810728 -4.1255159 -4.1642132 -4.1922522 -4.2249303 -4.2637649 -4.293119][-4.2698169 -4.24785 -4.1879463 -4.1230059 -4.0920367 -4.0731525 -4.0346107 -3.9909489 -4.0197387 -4.0925717 -4.15267 -4.1928425 -4.2358065 -4.279511 -4.3057165][-4.25773 -4.2352486 -4.1699476 -4.0945106 -4.0543828 -4.0172296 -3.9568479 -3.8957002 -3.9435537 -4.0465269 -4.1244841 -4.1748438 -4.2273974 -4.2753906 -4.2985973][-4.2290754 -4.2064142 -4.1403313 -4.0640745 -4.0210185 -3.9692612 -3.8915143 -3.8154018 -3.8825536 -4.0052352 -4.09133 -4.14725 -4.2075224 -4.2585549 -4.2810392][-4.2003174 -4.1787567 -4.1142988 -4.0415726 -3.9965906 -3.9355023 -3.8428738 -3.7526035 -3.8224959 -3.9498644 -4.0405488 -4.103641 -4.174931 -4.2370238 -4.2666388][-4.1916881 -4.1707449 -4.1129332 -4.0511189 -4.0089841 -3.9491754 -3.8634729 -3.7863393 -3.8388736 -3.9430394 -4.0230174 -4.0859814 -4.16223 -4.2305613 -4.26798][-4.1901851 -4.1769295 -4.1374931 -4.0977364 -4.0655856 -4.0154724 -3.9526982 -3.90423 -3.935276 -4.0002775 -4.0579505 -4.1101518 -4.1767054 -4.2347856 -4.2694883][-4.2007546 -4.1941638 -4.1721945 -4.1513929 -4.1286035 -4.0890088 -4.04731 -4.0168228 -4.029357 -4.0642862 -4.1029954 -4.1429286 -4.1924868 -4.2342162 -4.2598433][-4.2036438 -4.2038665 -4.1962347 -4.1900129 -4.1804237 -4.1572528 -4.1333551 -4.1163225 -4.1191297 -4.1371617 -4.1628518 -4.189723 -4.2214117 -4.2481685 -4.2637663][-4.2209277 -4.2265186 -4.2278461 -4.2295446 -4.2298369 -4.221868 -4.211412 -4.2008772 -4.1971765 -4.2047834 -4.2198734 -4.2348108 -4.2486115 -4.2603011 -4.2665715][-4.2589273 -4.2662435 -4.268846 -4.2710733 -4.2754722 -4.2756481 -4.2720594 -4.2644081 -4.2571168 -4.2565956 -4.26108 -4.2643166 -4.2654219 -4.2656717 -4.2640758][-4.287672 -4.2937446 -4.2951794 -4.2967534 -4.300344 -4.3014159 -4.2996407 -4.2936935 -4.2859912 -4.2816739 -4.2807145 -4.2782903 -4.2728529 -4.2655864 -4.2582288]]...]
INFO - root - 2017-12-05 18:03:36.208277: step 31910, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 71h:10m:52s remains)
INFO - root - 2017-12-05 18:03:44.738991: step 31920, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 70h:28m:01s remains)
INFO - root - 2017-12-05 18:03:53.201589: step 31930, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 71h:08m:28s remains)
INFO - root - 2017-12-05 18:04:01.541017: step 31940, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 72h:33m:06s remains)
INFO - root - 2017-12-05 18:04:10.072158: step 31950, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 71h:27m:08s remains)
INFO - root - 2017-12-05 18:04:18.618396: step 31960, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 70h:00m:41s remains)
INFO - root - 2017-12-05 18:04:27.202422: step 31970, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 71h:34m:35s remains)
INFO - root - 2017-12-05 18:04:35.663533: step 31980, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 72h:32m:58s remains)
INFO - root - 2017-12-05 18:04:44.255579: step 31990, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 72h:09m:46s remains)
INFO - root - 2017-12-05 18:04:52.707721: step 32000, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 69h:39m:47s remains)
2017-12-05 18:04:53.607766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3156819 -4.3015394 -4.29208 -4.2851768 -4.2742844 -4.2602482 -4.2494192 -4.240428 -4.2314138 -4.227283 -4.2257056 -4.227457 -4.2327642 -4.2435746 -4.2550297][-4.3129611 -4.2989264 -4.2906761 -4.2818432 -4.2637043 -4.2450485 -4.2371278 -4.2299294 -4.2163138 -4.2088666 -4.2066741 -4.2093506 -4.2173896 -4.2296729 -4.2409234][-4.3088975 -4.2951293 -4.2859483 -4.2711487 -4.2455778 -4.2230763 -4.2165785 -4.2138419 -4.2024646 -4.195117 -4.191648 -4.1934886 -4.204391 -4.2206674 -4.2325234][-4.3012528 -4.28675 -4.2760563 -4.2573786 -4.2286057 -4.2025676 -4.19527 -4.1968718 -4.1857934 -4.1729846 -4.1676817 -4.1735497 -4.1864433 -4.2058716 -4.2192559][-4.2934422 -4.2721806 -4.2566051 -4.2329354 -4.2000389 -4.1690178 -4.1613889 -4.1683593 -4.1543803 -4.1359582 -4.132555 -4.1461854 -4.1651063 -4.1865916 -4.2053919][-4.2885823 -4.2582793 -4.2338276 -4.2036123 -4.1689425 -4.1368108 -4.129488 -4.1381831 -4.1214638 -4.0995479 -4.1031208 -4.127984 -4.1555676 -4.1838155 -4.2099328][-4.2918634 -4.2567449 -4.22373 -4.1855483 -4.1483831 -4.119112 -4.1128917 -4.1170154 -4.0982113 -4.0812435 -4.0918679 -4.12635 -4.1629848 -4.1992793 -4.227716][-4.2999196 -4.2656784 -4.2314305 -4.192853 -4.1531568 -4.1223264 -4.1128097 -4.109252 -4.092823 -4.0811496 -4.0915442 -4.12645 -4.1695156 -4.2105083 -4.2352338][-4.3043103 -4.2738466 -4.2454567 -4.2171397 -4.1867585 -4.1567459 -4.1346822 -4.1173639 -4.1007895 -4.0899534 -4.0962443 -4.1285419 -4.1728239 -4.2134576 -4.2314544][-4.3023958 -4.2738233 -4.2510748 -4.2349634 -4.2156415 -4.1861649 -4.1545391 -4.1214404 -4.0968456 -4.0871892 -4.0956 -4.1232681 -4.1644392 -4.2022953 -4.2186522][-4.2933025 -4.2625585 -4.2396188 -4.2286286 -4.2133913 -4.1809459 -4.1407743 -4.1017575 -4.0832505 -4.0838175 -4.0973191 -4.1236706 -4.1561852 -4.1874142 -4.206286][-4.2838798 -4.2472625 -4.2177792 -4.2028236 -4.1868386 -4.1546149 -4.1167383 -4.087533 -4.0920978 -4.104156 -4.1178842 -4.1330404 -4.1459789 -4.1679006 -4.18673][-4.2821589 -4.2412248 -4.20302 -4.1816878 -4.1660776 -4.1424742 -4.1187925 -4.1065459 -4.1252193 -4.1402297 -4.1477046 -4.1451807 -4.1361208 -4.1433759 -4.1632557][-4.289 -4.2460985 -4.1999493 -4.17269 -4.1626749 -4.1530395 -4.1465158 -4.1491928 -4.1707473 -4.1839089 -4.1831775 -4.169435 -4.1488147 -4.1451564 -4.16363][-4.2962866 -4.2519135 -4.1990056 -4.1650763 -4.1601868 -4.1577225 -4.1562781 -4.1668863 -4.1927953 -4.2063279 -4.2030144 -4.189724 -4.1689715 -4.162972 -4.1775541]]...]
INFO - root - 2017-12-05 18:05:02.037032: step 32010, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 71h:22m:31s remains)
INFO - root - 2017-12-05 18:05:10.473085: step 32020, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 73h:06m:33s remains)
INFO - root - 2017-12-05 18:05:18.841104: step 32030, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 70h:39m:02s remains)
INFO - root - 2017-12-05 18:05:27.462382: step 32040, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 72h:00m:50s remains)
INFO - root - 2017-12-05 18:05:35.917864: step 32050, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 72h:22m:28s remains)
INFO - root - 2017-12-05 18:05:44.456461: step 32060, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 72h:30m:41s remains)
INFO - root - 2017-12-05 18:05:52.945638: step 32070, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.822 sec/batch; 68h:35m:38s remains)
INFO - root - 2017-12-05 18:06:01.550668: step 32080, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 73h:56m:25s remains)
INFO - root - 2017-12-05 18:06:10.018371: step 32090, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 74h:07m:11s remains)
INFO - root - 2017-12-05 18:06:18.563904: step 32100, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 71h:47m:32s remains)
2017-12-05 18:06:19.391433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2267685 -4.2211142 -4.2034483 -4.1905608 -4.1955204 -4.208508 -4.217566 -4.2218504 -4.2242856 -4.2260218 -4.2326717 -4.2370768 -4.2378902 -4.2406116 -4.2443657][-4.2244039 -4.2125716 -4.18938 -4.17577 -4.1821156 -4.1965222 -4.2110958 -4.2231436 -4.2268968 -4.2258 -4.2289877 -4.2290988 -4.2234879 -4.2205997 -4.2221642][-4.2213111 -4.2094293 -4.1856623 -4.1679025 -4.1670294 -4.1759219 -4.1921725 -4.2127972 -4.2220712 -4.2235723 -4.2253714 -4.2241521 -4.2139325 -4.2040944 -4.2025166][-4.2127256 -4.2063122 -4.1843362 -4.1610379 -4.150209 -4.1529112 -4.167439 -4.1889672 -4.2040262 -4.2117257 -4.2137623 -4.2116013 -4.2016139 -4.1910987 -4.1885643][-4.18612 -4.1871 -4.1710305 -4.1478887 -4.1312256 -4.1305757 -4.1473179 -4.1691875 -4.1851811 -4.1943116 -4.1954551 -4.190321 -4.1817608 -4.174336 -4.1734428][-4.1403337 -4.1522284 -4.1452055 -4.1273041 -4.1114597 -4.1116438 -4.1305547 -4.1522355 -4.1651697 -4.172358 -4.1748409 -4.1702447 -4.1632557 -4.1588798 -4.1591539][-4.0933542 -4.1075182 -4.1093273 -4.1017275 -4.0921459 -4.0929885 -4.1099439 -4.1281543 -4.139432 -4.1490026 -4.156991 -4.1574163 -4.1549478 -4.1533957 -4.1546168][-4.0710306 -4.0815349 -4.0845165 -4.0832167 -4.079565 -4.0764642 -4.0818276 -4.090188 -4.1013145 -4.1215091 -4.1446562 -4.1569648 -4.161756 -4.164535 -4.1677189][-4.0847797 -4.0915952 -4.092916 -4.092422 -4.087009 -4.0697284 -4.0542536 -4.0475144 -4.0542521 -4.0818348 -4.1221886 -4.1513629 -4.1688704 -4.1807723 -4.1885538][-4.1177292 -4.1235008 -4.1242952 -4.1229753 -4.1147647 -4.0878849 -4.0559444 -4.0329523 -4.02672 -4.0473948 -4.0881906 -4.1246719 -4.1516953 -4.1732011 -4.1920419][-4.143189 -4.1533685 -4.1561689 -4.1553907 -4.1486297 -4.1255083 -4.0920525 -4.0613422 -4.0410438 -4.0419693 -4.0628991 -4.0868626 -4.10914 -4.1347685 -4.1653829][-4.1455159 -4.1608171 -4.1709929 -4.17666 -4.1760869 -4.1624613 -4.138144 -4.1125221 -4.088728 -4.0744658 -4.0679784 -4.0638809 -4.0682249 -4.0878997 -4.1198649][-4.1251407 -4.1410522 -4.158946 -4.1757402 -4.1853766 -4.1851678 -4.1754894 -4.1617527 -4.143837 -4.1276865 -4.1081538 -4.0833783 -4.0673642 -4.0707159 -4.0883713][-4.0976319 -4.111042 -4.1323662 -4.1548991 -4.1712751 -4.1817369 -4.187613 -4.1909642 -4.1873841 -4.1787076 -4.1571221 -4.1239481 -4.0977454 -4.0909848 -4.0943661][-4.0888424 -4.0967512 -4.1146021 -4.1346974 -4.1487212 -4.159904 -4.1741929 -4.1902089 -4.2007246 -4.2016606 -4.1856484 -4.1566815 -4.1322641 -4.1229248 -4.1189985]]...]
INFO - root - 2017-12-05 18:06:27.845427: step 32110, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 70h:28m:28s remains)
INFO - root - 2017-12-05 18:06:36.361427: step 32120, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 70h:34m:56s remains)
INFO - root - 2017-12-05 18:06:44.778304: step 32130, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.802 sec/batch; 66h:55m:33s remains)
INFO - root - 2017-12-05 18:06:53.421415: step 32140, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 71h:26m:13s remains)
INFO - root - 2017-12-05 18:07:01.984071: step 32150, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 71h:52m:45s remains)
INFO - root - 2017-12-05 18:07:10.396099: step 32160, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 72h:45m:06s remains)
INFO - root - 2017-12-05 18:07:18.996600: step 32170, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 72h:15m:45s remains)
INFO - root - 2017-12-05 18:07:27.419717: step 32180, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.831 sec/batch; 69h:18m:28s remains)
INFO - root - 2017-12-05 18:07:35.926110: step 32190, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 72h:37m:21s remains)
INFO - root - 2017-12-05 18:07:44.422953: step 32200, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 71h:46m:09s remains)
2017-12-05 18:07:45.277746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1312342 -4.15281 -4.1673822 -4.1808743 -4.1844735 -4.1764736 -4.1630411 -4.1539979 -4.1596441 -4.1742678 -4.18353 -4.1993361 -4.2220721 -4.2282257 -4.2427588][-4.1213226 -4.135159 -4.143754 -4.1516061 -4.1538782 -4.1529441 -4.1523061 -4.1469622 -4.1462421 -4.152626 -4.1542225 -4.16392 -4.1882887 -4.2059135 -4.2327342][-4.097105 -4.106111 -4.117836 -4.1308265 -4.1400738 -4.150033 -4.1562805 -4.1479268 -4.1393366 -4.1410151 -4.1382055 -4.1419091 -4.1638384 -4.1903696 -4.2271886][-4.0812793 -4.0920639 -4.1109581 -4.1278915 -4.1370821 -4.14568 -4.1491346 -4.1381884 -4.1347342 -4.1419444 -4.1427975 -4.1446238 -4.1639228 -4.1921968 -4.2310872][-4.07758 -4.09172 -4.1114912 -4.1222272 -4.1229577 -4.1228237 -4.1212025 -4.1119061 -4.1123347 -4.1255264 -4.1340957 -4.1388488 -4.1580043 -4.189508 -4.2303934][-4.0918169 -4.1047196 -4.1150656 -4.1130743 -4.104022 -4.0968294 -4.091629 -4.0804758 -4.072031 -4.0741591 -4.0801549 -4.0934134 -4.1237683 -4.1665998 -4.2152152][-4.1119089 -4.1183639 -4.1181483 -4.1079984 -4.0938549 -4.0885558 -4.0817928 -4.06409 -4.0358024 -4.0137129 -4.0084987 -4.0312366 -4.0779738 -4.1327677 -4.1913757][-4.1198344 -4.1244726 -4.1141129 -4.0994406 -4.0898113 -4.0871043 -4.0822725 -4.0647874 -4.0245986 -3.9888995 -3.9778991 -4.0049286 -4.0576272 -4.1127496 -4.174819][-4.1076336 -4.1153975 -4.1057067 -4.0930243 -4.0887704 -4.0893645 -4.0887833 -4.0733337 -4.0366397 -4.0015821 -3.9931746 -4.0222783 -4.0694485 -4.115427 -4.1690912][-4.0965962 -4.1140127 -4.1123524 -4.10819 -4.1108775 -4.1127839 -4.1117344 -4.0984492 -4.0763268 -4.0494971 -4.0402942 -4.0626044 -4.098732 -4.1334434 -4.1752071][-4.1021471 -4.1294575 -4.1391659 -4.1461544 -4.1519742 -4.1475735 -4.1411533 -4.1266632 -4.1147332 -4.0929637 -4.0838842 -4.0983405 -4.1253982 -4.1536293 -4.1885591][-4.1311111 -4.15654 -4.1743488 -4.1873121 -4.1981478 -4.1954737 -4.1817975 -4.1621094 -4.1516833 -4.1287742 -4.1189184 -4.1319242 -4.1547732 -4.1759729 -4.2056746][-4.1538458 -4.17516 -4.1924672 -4.2025013 -4.2133284 -4.2163672 -4.2037845 -4.1849375 -4.1760106 -4.1541262 -4.148016 -4.16148 -4.1831713 -4.198884 -4.2227187][-4.1614785 -4.1783094 -4.1872106 -4.19027 -4.1969562 -4.2081866 -4.2047877 -4.1950221 -4.18814 -4.1710348 -4.1722345 -4.1892676 -4.2097459 -4.2222471 -4.2421231][-4.1605449 -4.1667385 -4.1651344 -4.160533 -4.1674447 -4.1869297 -4.1963997 -4.19662 -4.1966333 -4.1933246 -4.2023659 -4.2202115 -4.2355223 -4.2457747 -4.2640204]]...]
INFO - root - 2017-12-05 18:07:53.740405: step 32210, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 73h:47m:36s remains)
INFO - root - 2017-12-05 18:08:02.264874: step 32220, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 71h:15m:55s remains)
INFO - root - 2017-12-05 18:08:10.712522: step 32230, loss = 2.09, batch loss = 2.04 (9.7 examples/sec; 0.829 sec/batch; 69h:08m:06s remains)
INFO - root - 2017-12-05 18:08:19.002776: step 32240, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 70h:11m:26s remains)
INFO - root - 2017-12-05 18:08:27.393733: step 32250, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 69h:12m:22s remains)
INFO - root - 2017-12-05 18:08:35.847333: step 32260, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.805 sec/batch; 67h:08m:33s remains)
INFO - root - 2017-12-05 18:08:44.277824: step 32270, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 71h:07m:35s remains)
INFO - root - 2017-12-05 18:08:52.650902: step 32280, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.821 sec/batch; 68h:29m:42s remains)
INFO - root - 2017-12-05 18:09:01.020808: step 32290, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 69h:46m:13s remains)
INFO - root - 2017-12-05 18:09:09.490539: step 32300, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 68h:05m:22s remains)
2017-12-05 18:09:10.314201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2343469 -4.2051253 -4.1725392 -4.1519051 -4.1517653 -4.1659474 -4.1711497 -4.1707377 -4.1831036 -4.2075276 -4.2283092 -4.218091 -4.1833482 -4.1473594 -4.1277571][-4.228466 -4.2033463 -4.1760273 -4.1662178 -4.1713119 -4.1788292 -4.17312 -4.1681447 -4.1789021 -4.2015257 -4.2280774 -4.2327871 -4.207438 -4.1671047 -4.1336036][-4.2200117 -4.2059426 -4.1899939 -4.1905141 -4.1931391 -4.1867547 -4.1708336 -4.1618524 -4.16645 -4.1877131 -4.2220688 -4.2413449 -4.2272692 -4.18366 -4.1399][-4.21347 -4.2083387 -4.2027516 -4.2080383 -4.211791 -4.1963191 -4.1678352 -4.14428 -4.1403208 -4.1653061 -4.2101007 -4.241128 -4.2382936 -4.2013702 -4.156601][-4.206727 -4.2023973 -4.2012863 -4.2098193 -4.2142043 -4.1921778 -4.1437349 -4.0998154 -4.0958791 -4.1367226 -4.1912484 -4.2295046 -4.2379 -4.2131071 -4.177083][-4.2063274 -4.19561 -4.19004 -4.1975641 -4.2016978 -4.1656489 -4.0850596 -4.0187445 -4.0347056 -4.104001 -4.1695004 -4.2115254 -4.2318678 -4.2222633 -4.1993566][-4.2093253 -4.1932745 -4.18208 -4.1873789 -4.1877966 -4.1364427 -4.01729 -3.9170165 -3.9601698 -4.0675735 -4.1510892 -4.1985416 -4.2233963 -4.2267237 -4.2162714][-4.2014985 -4.1897144 -4.1800241 -4.1886544 -4.1864414 -4.129385 -3.9949613 -3.8696034 -3.9171405 -4.0464664 -4.1442 -4.1943502 -4.2170663 -4.2259932 -4.22402][-4.17868 -4.1799574 -4.1787424 -4.19064 -4.1922865 -4.1475348 -4.0415111 -3.9376554 -3.9575148 -4.0620461 -4.150332 -4.1955743 -4.2119436 -4.2190142 -4.2201147][-4.1572695 -4.168107 -4.1726851 -4.1807008 -4.1847329 -4.1596251 -4.0956297 -4.0299644 -4.0319443 -4.0967679 -4.1643996 -4.2021756 -4.2115793 -4.2085943 -4.2055798][-4.1489482 -4.1593966 -4.1628513 -4.1655869 -4.1729794 -4.1662707 -4.1306949 -4.0918889 -4.0875521 -4.1212568 -4.1730785 -4.2064457 -4.2115984 -4.1987271 -4.1882415][-4.1526814 -4.1593 -4.1595788 -4.1527205 -4.1602979 -4.16893 -4.1516552 -4.1270089 -4.1188254 -4.1345177 -4.1731749 -4.2059755 -4.2129221 -4.1977329 -4.17947][-4.1674447 -4.1663408 -4.1616821 -4.146769 -4.1508231 -4.1653476 -4.1612473 -4.1422014 -4.1326351 -4.1399922 -4.1691012 -4.2007561 -4.2118039 -4.2021914 -4.1820884][-4.1931391 -4.1871052 -4.17585 -4.155879 -4.15177 -4.1613164 -4.161653 -4.1403375 -4.1309185 -4.1406727 -4.1670346 -4.1973491 -4.2096534 -4.2055793 -4.189713][-4.2180295 -4.2130475 -4.1982636 -4.1820569 -4.1726747 -4.1704087 -4.1641016 -4.1389804 -4.1302123 -4.1457024 -4.1721621 -4.199285 -4.2119246 -4.2085328 -4.1956878]]...]
INFO - root - 2017-12-05 18:09:18.711528: step 32310, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 0.800 sec/batch; 66h:41m:56s remains)
INFO - root - 2017-12-05 18:09:27.104291: step 32320, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 70h:01m:09s remains)
INFO - root - 2017-12-05 18:09:35.518906: step 32330, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.827 sec/batch; 68h:57m:55s remains)
INFO - root - 2017-12-05 18:09:43.904192: step 32340, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 69h:43m:46s remains)
INFO - root - 2017-12-05 18:09:52.309378: step 32350, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 73h:22m:30s remains)
INFO - root - 2017-12-05 18:10:00.732701: step 32360, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 71h:45m:07s remains)
INFO - root - 2017-12-05 18:10:09.167513: step 32370, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 71h:06m:40s remains)
INFO - root - 2017-12-05 18:10:17.664354: step 32380, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 73h:29m:54s remains)
INFO - root - 2017-12-05 18:10:26.157481: step 32390, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 69h:55m:13s remains)
INFO - root - 2017-12-05 18:10:34.645251: step 32400, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 69h:35m:43s remains)
2017-12-05 18:10:35.420662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1916022 -4.2260771 -4.2468214 -4.2552447 -4.2543616 -4.2452693 -4.2407002 -4.24228 -4.246388 -4.2469425 -4.2408953 -4.2322006 -4.2188616 -4.2038908 -4.1910734][-4.1787519 -4.2121964 -4.2319069 -4.2376347 -4.2340193 -4.226213 -4.2247968 -4.2315383 -4.2401276 -4.2443528 -4.241683 -4.2298517 -4.2116404 -4.1951056 -4.181088][-4.1559176 -4.1886005 -4.207871 -4.2107368 -4.2041993 -4.1953607 -4.1958237 -4.2066994 -4.2197485 -4.2271719 -4.2296381 -4.2204084 -4.2017717 -4.1851568 -4.16523][-4.11818 -4.1501737 -4.1732006 -4.1787691 -4.1723027 -4.1624203 -4.1619539 -4.1732984 -4.1878281 -4.19567 -4.2020345 -4.1968822 -4.1792135 -4.1590338 -4.1264172][-4.0778637 -4.1086817 -4.1340685 -4.1398497 -4.1314664 -4.1177559 -4.1124339 -4.1221628 -4.1382856 -4.1489367 -4.1551104 -4.1496811 -4.1321821 -4.1100779 -4.0675883][-4.0463223 -4.07201 -4.0924535 -4.0914412 -4.0745077 -4.0536075 -4.0427308 -4.0517764 -4.0745883 -4.0954146 -4.096364 -4.0808897 -4.0534949 -4.0208726 -3.9691417][-4.0305238 -4.0383711 -4.0408087 -4.0226636 -3.9895866 -3.9578474 -3.9476395 -3.9696438 -4.0161295 -4.0552878 -4.0558124 -4.0312481 -3.9967532 -3.9566088 -3.9059632][-4.0205326 -4.01004 -3.9907982 -3.9491313 -3.8991098 -3.864778 -3.8691192 -3.9171963 -3.9905083 -4.0471153 -4.0533838 -4.0273781 -3.9901624 -3.9458487 -3.8991971][-4.0438819 -4.0224218 -3.986063 -3.9270949 -3.8739817 -3.85078 -3.8703589 -3.9298177 -4.00914 -4.0673108 -4.0768771 -4.052742 -4.0140266 -3.9686987 -3.9293871][-4.0540657 -4.036119 -4.0033827 -3.95222 -3.9116046 -3.9038863 -3.9309821 -3.987427 -4.0572305 -4.1067805 -4.1181841 -4.1002274 -4.0690346 -4.03283 -4.0055609][-4.0722775 -4.0610557 -4.0383558 -4.0017462 -3.9757712 -3.9793921 -4.0117593 -4.06358 -4.1191263 -4.156095 -4.1654277 -4.152792 -4.1287169 -4.1035018 -4.0860844][-4.1063976 -4.1013284 -4.0879869 -4.0646291 -4.0495729 -4.0565839 -4.0833 -4.12107 -4.158247 -4.1808453 -4.1871843 -4.1799107 -4.1638651 -4.1476564 -4.1369877][-4.1367068 -4.1383266 -4.1372409 -4.1289248 -4.1234708 -4.1293273 -4.1464338 -4.1694341 -4.1905069 -4.2013974 -4.2050834 -4.2018476 -4.1935654 -4.1848488 -4.1797514][-4.1758356 -4.180131 -4.1867065 -4.18844 -4.1897488 -4.1948209 -4.2047715 -4.2156272 -4.2240868 -4.2282147 -4.2296071 -4.2287 -4.2256641 -4.2227182 -4.221664][-4.2143736 -4.2174592 -4.2243619 -4.2293105 -4.2329307 -4.2366557 -4.2420859 -4.2475863 -4.2516222 -4.2527323 -4.2513213 -4.2492895 -4.2479482 -4.2481203 -4.2502136]]...]
INFO - root - 2017-12-05 18:10:43.897821: step 32410, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 74h:30m:37s remains)
INFO - root - 2017-12-05 18:10:52.367342: step 32420, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 69h:57m:48s remains)
INFO - root - 2017-12-05 18:11:00.957175: step 32430, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 70h:21m:27s remains)
INFO - root - 2017-12-05 18:11:09.356845: step 32440, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 70h:45m:06s remains)
INFO - root - 2017-12-05 18:11:18.024049: step 32450, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 72h:44m:04s remains)
INFO - root - 2017-12-05 18:11:26.606086: step 32460, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 70h:30m:41s remains)
INFO - root - 2017-12-05 18:11:35.053753: step 32470, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 71h:26m:28s remains)
INFO - root - 2017-12-05 18:11:43.424092: step 32480, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 70h:34m:44s remains)
INFO - root - 2017-12-05 18:11:51.922198: step 32490, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 73h:14m:43s remains)
INFO - root - 2017-12-05 18:12:00.531234: step 32500, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 69h:20m:30s remains)
2017-12-05 18:12:01.371026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2821727 -4.2227473 -4.1401906 -4.0666556 -4.0159822 -4.0061088 -4.0328078 -4.0559888 -4.0567651 -4.0460491 -4.0337334 -4.0058842 -4.0018964 -4.0260592 -4.0752697][-4.2943716 -4.2439022 -4.1722984 -4.1089969 -4.0620666 -4.0457497 -4.0634966 -4.0738111 -4.0640535 -4.0495315 -4.0381775 -4.0180378 -4.0136838 -4.0333772 -4.080451][-4.30568 -4.2634945 -4.2041631 -4.1549368 -4.1220827 -4.1100583 -4.1218224 -4.1264877 -4.1099339 -4.0805817 -4.0589437 -4.0444837 -4.0394855 -4.0529318 -4.0877666][-4.3136606 -4.2761226 -4.2246742 -4.1851249 -4.1602116 -4.1473751 -4.1506987 -4.1562796 -4.1442647 -4.113677 -4.0933771 -4.0888734 -4.0819316 -4.0851746 -4.1027465][-4.3167791 -4.27938 -4.2299752 -4.1902561 -4.1593323 -4.138792 -4.1376033 -4.1428 -4.1361346 -4.1141973 -4.1019568 -4.1045327 -4.0994229 -4.0970988 -4.1059685][-4.3158512 -4.2746768 -4.2181935 -4.1679583 -4.1240439 -4.0956526 -4.0909314 -4.0913587 -4.0917444 -4.0875835 -4.0940628 -4.1050844 -4.0963225 -4.09425 -4.1092348][-4.3101287 -4.263629 -4.1947927 -4.121892 -4.0537004 -4.0153942 -4.0088983 -4.0140266 -4.0324221 -4.0530443 -4.0819168 -4.0986729 -4.0878067 -4.0878839 -4.1055846][-4.299624 -4.2432775 -4.1530132 -4.0452313 -3.9448962 -3.9030926 -3.9003019 -3.915767 -3.9489176 -3.9751861 -4.0101151 -4.0329933 -4.0296659 -4.038969 -4.0612288][-4.2908125 -4.2270231 -4.1208105 -3.9890602 -3.8807764 -3.8495181 -3.8569303 -3.8886027 -3.9339108 -3.955147 -3.9825442 -4.000771 -4.0008841 -4.0107279 -4.0322566][-4.2940035 -4.2359066 -4.1405883 -4.035604 -3.9624507 -3.9449511 -3.9527121 -3.9860141 -4.0284939 -4.0470357 -4.0598183 -4.0589142 -4.047451 -4.046083 -4.0597563][-4.301496 -4.252337 -4.1734505 -4.0946712 -4.0480995 -4.0386238 -4.0445166 -4.0751834 -4.1118445 -4.1261473 -4.1283522 -4.1195579 -4.104569 -4.0961466 -4.1012435][-4.3042812 -4.2630672 -4.1929197 -4.124054 -4.085566 -4.0697575 -4.0744295 -4.1063118 -4.1320586 -4.1344776 -4.1287403 -4.1220069 -4.1061635 -4.0931444 -4.09791][-4.3012161 -4.2625303 -4.1929111 -4.1279554 -4.0872221 -4.0587835 -4.0539665 -4.084868 -4.1049619 -4.1016312 -4.0955243 -4.09254 -4.0759888 -4.0649538 -4.07133][-4.2898483 -4.244298 -4.1662354 -4.0982556 -4.0544081 -4.018445 -4.0087161 -4.0389667 -4.0536618 -4.053298 -4.0544004 -4.05738 -4.0515575 -4.0515695 -4.0637231][-4.28173 -4.2289262 -4.1443543 -4.0733142 -4.0238352 -3.9824479 -3.9738119 -4.0044169 -4.0241442 -4.0295839 -4.0282426 -4.0300074 -4.0274072 -4.0339084 -4.0512948]]...]
INFO - root - 2017-12-05 18:12:09.877568: step 32510, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 71h:29m:08s remains)
INFO - root - 2017-12-05 18:12:18.388099: step 32520, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.874 sec/batch; 72h:51m:51s remains)
INFO - root - 2017-12-05 18:12:26.953833: step 32530, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 72h:12m:02s remains)
INFO - root - 2017-12-05 18:12:35.408946: step 32540, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.830 sec/batch; 69h:11m:49s remains)
INFO - root - 2017-12-05 18:12:43.913760: step 32550, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 73h:43m:30s remains)
INFO - root - 2017-12-05 18:12:52.457439: step 32560, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 69h:40m:11s remains)
INFO - root - 2017-12-05 18:13:00.870145: step 32570, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 70h:58m:31s remains)
INFO - root - 2017-12-05 18:13:09.378003: step 32580, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 72h:01m:42s remains)
INFO - root - 2017-12-05 18:13:17.738300: step 32590, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 70h:48m:28s remains)
INFO - root - 2017-12-05 18:13:26.385527: step 32600, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 72h:18m:10s remains)
2017-12-05 18:13:27.124403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1636496 -4.1812763 -4.1976752 -4.2155013 -4.228951 -4.2214355 -4.19342 -4.171679 -4.1863112 -4.1959867 -4.2006207 -4.2160406 -4.2268219 -4.2423797 -4.264791][-4.1565681 -4.1763654 -4.1951952 -4.211863 -4.2253685 -4.2178521 -4.191021 -4.1652689 -4.1703062 -4.16487 -4.1648817 -4.1857448 -4.2031174 -4.2273016 -4.2541957][-4.1402812 -4.1590004 -4.1792107 -4.1992731 -4.2158246 -4.2165117 -4.20139 -4.1764073 -4.1654348 -4.1446013 -4.1468363 -4.171073 -4.190403 -4.2169013 -4.2435355][-4.1187048 -4.1341362 -4.1546035 -4.1807771 -4.2017403 -4.2068663 -4.2001443 -4.1785731 -4.151279 -4.1151357 -4.1210594 -4.1533704 -4.1768556 -4.2101684 -4.2410288][-4.0958428 -4.1087189 -4.1305051 -4.1627421 -4.1874475 -4.1891575 -4.183589 -4.16422 -4.1310363 -4.0866179 -4.0970373 -4.1439342 -4.1810184 -4.2193151 -4.2492251][-4.0869217 -4.0954 -4.1138706 -4.1416421 -4.1620893 -4.160903 -4.1591787 -4.1486011 -4.121675 -4.0897994 -4.1071091 -4.1554356 -4.1925688 -4.2273722 -4.2489657][-4.088356 -4.0843678 -4.0904174 -4.1069269 -4.1173844 -4.1117969 -4.1109324 -4.1116161 -4.0975547 -4.085588 -4.1071329 -4.148437 -4.1785259 -4.2116871 -4.2343731][-4.1037903 -4.0875468 -4.0769057 -4.0769219 -4.068296 -4.0496039 -4.0469437 -4.0621934 -4.0655251 -4.0755424 -4.1005845 -4.1335711 -4.1590753 -4.1884031 -4.2122431][-4.1090527 -4.0896082 -4.0758958 -4.0646214 -4.0374 -4.0096979 -4.0065689 -4.034606 -4.0546083 -4.080276 -4.1109457 -4.1394782 -4.1575584 -4.174881 -4.1925888][-4.100081 -4.0895882 -4.0833526 -4.0677042 -4.0345392 -4.0070419 -4.0073528 -4.0444851 -4.0779924 -4.106986 -4.1365376 -4.1567144 -4.1624217 -4.1671925 -4.1785827][-4.1170769 -4.120717 -4.1189952 -4.0991106 -4.069037 -4.049624 -4.0529027 -4.078445 -4.1083775 -4.1370897 -4.1656818 -4.179626 -4.1740613 -4.1675911 -4.1692333][-4.1601324 -4.1685762 -4.1666517 -4.1497464 -4.131959 -4.1252966 -4.127223 -4.1345463 -4.1511059 -4.1717486 -4.1917744 -4.1955318 -4.1793795 -4.164279 -4.1597147][-4.1931663 -4.1951346 -4.1869349 -4.1717424 -4.1662803 -4.1722188 -4.1765037 -4.1739779 -4.1756368 -4.1825061 -4.187758 -4.1869664 -4.17624 -4.1674337 -4.1634421][-4.2004089 -4.2011209 -4.1909852 -4.1768765 -4.1808615 -4.19576 -4.2015567 -4.1973648 -4.1920533 -4.1882114 -4.183609 -4.1796093 -4.1733503 -4.1723504 -4.1734343][-4.1909842 -4.1968403 -4.1951795 -4.1879339 -4.1942677 -4.2059665 -4.2096477 -4.2084069 -4.2034211 -4.1941271 -4.1879826 -4.181993 -4.1763158 -4.1803908 -4.1860204]]...]
INFO - root - 2017-12-05 18:13:35.664043: step 32610, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 70h:58m:54s remains)
INFO - root - 2017-12-05 18:13:44.139966: step 32620, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.816 sec/batch; 67h:56m:24s remains)
INFO - root - 2017-12-05 18:13:52.688956: step 32630, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 70h:11m:44s remains)
INFO - root - 2017-12-05 18:14:01.177039: step 32640, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 71h:42m:31s remains)
INFO - root - 2017-12-05 18:14:09.602011: step 32650, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.822 sec/batch; 68h:29m:31s remains)
INFO - root - 2017-12-05 18:14:18.112812: step 32660, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 71h:03m:19s remains)
INFO - root - 2017-12-05 18:14:26.663008: step 32670, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 73h:00m:06s remains)
INFO - root - 2017-12-05 18:14:35.152770: step 32680, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 70h:46m:40s remains)
INFO - root - 2017-12-05 18:14:43.721138: step 32690, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 74h:15m:23s remains)
INFO - root - 2017-12-05 18:14:52.226171: step 32700, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 72h:28m:44s remains)
2017-12-05 18:14:52.966339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2499275 -4.241 -4.2381616 -4.2447348 -4.245605 -4.2368493 -4.2344723 -4.2359171 -4.2358503 -4.2441711 -4.2477522 -4.2488737 -4.2399693 -4.2386246 -4.24234][-4.2242012 -4.2078443 -4.2033906 -4.2117939 -4.216846 -4.2104607 -4.2100072 -4.21069 -4.20411 -4.2066116 -4.2050858 -4.20624 -4.1988215 -4.1956396 -4.1992779][-4.2067575 -4.1883993 -4.1851077 -4.1949954 -4.2034307 -4.201602 -4.198688 -4.1853213 -4.167964 -4.1661391 -4.1631713 -4.166667 -4.1655345 -4.1609125 -4.1627789][-4.21098 -4.1933193 -4.1885471 -4.1933193 -4.1991973 -4.1961627 -4.1809292 -4.1509948 -4.1214437 -4.1178012 -4.118166 -4.1303539 -4.1407127 -4.1346707 -4.1323347][-4.2342181 -4.2164593 -4.2059278 -4.2001004 -4.1916013 -4.1714182 -4.1409907 -4.100008 -4.0691237 -4.07179 -4.0835881 -4.1038404 -4.1165733 -4.1154137 -4.1166778][-4.2593708 -4.2413898 -4.2250142 -4.2061839 -4.1738787 -4.1224318 -4.0603495 -4.0040064 -3.9855182 -4.0161486 -4.0498972 -4.0829868 -4.106144 -4.1199079 -4.1295061][-4.2749114 -4.25585 -4.2376113 -4.2077465 -4.1531129 -4.0673509 -3.9616022 -3.8774345 -3.8799882 -3.9548807 -4.0219655 -4.0743508 -4.1153269 -4.1478009 -4.1668582][-4.2758751 -4.2566147 -4.2393079 -4.2071843 -4.1432385 -4.0349226 -3.8941307 -3.7866745 -3.8157122 -3.9302957 -4.0238676 -4.0979552 -4.1569271 -4.197443 -4.215734][-4.2695155 -4.24938 -4.2310729 -4.1990643 -4.1376948 -4.0333276 -3.9040668 -3.8190126 -3.860065 -3.9693806 -4.0571241 -4.1316366 -4.1945395 -4.2307715 -4.2387128][-4.2654243 -4.24224 -4.219233 -4.1876712 -4.1327982 -4.0485563 -3.956399 -3.9133873 -3.9605429 -4.0518103 -4.120728 -4.1826329 -4.2352457 -4.2602692 -4.2581263][-4.26404 -4.2386837 -4.211648 -4.1798639 -4.128058 -4.0645494 -4.0111003 -4.0043221 -4.0533848 -4.1297565 -4.1874952 -4.2345295 -4.2636504 -4.276433 -4.270226][-4.2630634 -4.2358556 -4.2078266 -4.1765985 -4.1277738 -4.0792708 -4.05356 -4.0694065 -4.1210284 -4.1878557 -4.2365427 -4.2700477 -4.2789006 -4.280776 -4.2722573][-4.2616296 -4.232162 -4.2051725 -4.179038 -4.1366568 -4.1006131 -4.0919967 -4.1222115 -4.1722827 -4.2270522 -4.2644606 -4.2856426 -4.2827673 -4.2797503 -4.2724938][-4.2606063 -4.2291827 -4.2062097 -4.1864347 -4.1519117 -4.1254959 -4.1293497 -4.1656218 -4.2088947 -4.2478185 -4.2712893 -4.2810678 -4.2751551 -4.2747593 -4.2712159][-4.2616715 -4.2305098 -4.2117062 -4.1975513 -4.1702967 -4.1536322 -4.1659632 -4.2000046 -4.2305312 -4.2501945 -4.2569704 -4.25642 -4.2543 -4.2607274 -4.2651167]]...]
INFO - root - 2017-12-05 18:15:01.524902: step 32710, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 70h:11m:06s remains)
INFO - root - 2017-12-05 18:15:10.052413: step 32720, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 73h:03m:36s remains)
INFO - root - 2017-12-05 18:15:18.540712: step 32730, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 69h:27m:13s remains)
INFO - root - 2017-12-05 18:15:26.937923: step 32740, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 73h:42m:05s remains)
INFO - root - 2017-12-05 18:15:35.528208: step 32750, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 71h:54m:08s remains)
INFO - root - 2017-12-05 18:15:44.030582: step 32760, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 71h:35m:12s remains)
INFO - root - 2017-12-05 18:15:52.450522: step 32770, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 72h:39m:28s remains)
INFO - root - 2017-12-05 18:16:00.999539: step 32780, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 70h:38m:17s remains)
INFO - root - 2017-12-05 18:16:09.436417: step 32790, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 71h:03m:17s remains)
INFO - root - 2017-12-05 18:16:17.740870: step 32800, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 68h:42m:03s remains)
2017-12-05 18:16:18.486405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2590084 -4.2549696 -4.2566366 -4.2599883 -4.2666059 -4.2775211 -4.2876635 -4.2968345 -4.2981377 -4.2932482 -4.2905459 -4.2948985 -4.3063331 -4.3139811 -4.3190045][-4.2301455 -4.2226057 -4.2212682 -4.220264 -4.2264223 -4.242806 -4.2600827 -4.2783427 -4.2863541 -4.2857013 -4.2836447 -4.2891641 -4.3036695 -4.311161 -4.3161869][-4.2076097 -4.19542 -4.1858563 -4.1774354 -4.1831279 -4.2037425 -4.2277923 -4.2521129 -4.2633314 -4.269371 -4.2727327 -4.2828784 -4.3015857 -4.3087096 -4.3143964][-4.193821 -4.174026 -4.152513 -4.136035 -4.1407194 -4.1630106 -4.1862278 -4.2151775 -4.2289219 -4.2408562 -4.2574 -4.2739773 -4.2970128 -4.3044305 -4.3139367][-4.1720967 -4.1418452 -4.1083355 -4.0862718 -4.0913911 -4.1111526 -4.1299253 -4.1635222 -4.1873136 -4.2080178 -4.2400565 -4.264576 -4.2918673 -4.3022456 -4.312902][-4.1363993 -4.0952597 -4.0556946 -4.029418 -4.02888 -4.0312657 -4.03241 -4.0685544 -4.11561 -4.1562905 -4.2090163 -4.2454762 -4.2786007 -4.2949381 -4.3050804][-4.0870328 -4.0361323 -3.9977429 -3.9649856 -3.9503107 -3.9246628 -3.8957748 -3.9390712 -4.0196066 -4.0854988 -4.1555347 -4.2021217 -4.2402835 -4.2688961 -4.2858806][-4.040328 -3.9850738 -3.950686 -3.9098947 -3.87268 -3.8161781 -3.7565529 -3.8014903 -3.9025428 -3.9812725 -4.0610065 -4.1190772 -4.1657224 -4.2153492 -4.252593][-4.0386572 -3.9946053 -3.9719987 -3.9300146 -3.8782125 -3.7999394 -3.7221959 -3.74408 -3.8222713 -3.8853307 -3.9599295 -4.02594 -4.0888486 -4.1580477 -4.2152033][-4.07451 -4.0459771 -4.0342159 -4.0033488 -3.9547272 -3.8805857 -3.8091483 -3.8046937 -3.8409791 -3.8767815 -3.937654 -4.0061088 -4.0765872 -4.1483035 -4.2072754][-4.1509714 -4.1332383 -4.1265392 -4.1045628 -4.064311 -4.0044961 -3.9517417 -3.9326291 -3.9372458 -3.9535038 -3.9964509 -4.0590897 -4.1239219 -4.185277 -4.2328706][-4.2390051 -4.2281737 -4.2230787 -4.2036295 -4.1704865 -4.1239882 -4.0900416 -4.0732021 -4.0656924 -4.0684071 -4.0960417 -4.1444616 -4.1956596 -4.2407017 -4.2740507][-4.3070526 -4.2979593 -4.2916784 -4.2777205 -4.2566338 -4.2303729 -4.21007 -4.1959848 -4.1867566 -4.1851759 -4.1996841 -4.2265382 -4.2546945 -4.2837529 -4.3070292][-4.3396459 -4.3333178 -4.330996 -4.3265243 -4.3195295 -4.3108096 -4.300693 -4.2885785 -4.2789025 -4.2750597 -4.2798738 -4.2896595 -4.3010678 -4.3164968 -4.3315487][-4.3432264 -4.3395691 -4.340014 -4.3412347 -4.3422947 -4.3411136 -4.3377633 -4.3316607 -4.3260417 -4.3238082 -4.3233919 -4.3260055 -4.3312979 -4.339632 -4.3484364]]...]
INFO - root - 2017-12-05 18:16:26.896801: step 32810, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 68h:15m:01s remains)
INFO - root - 2017-12-05 18:16:35.227706: step 32820, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 71h:06m:18s remains)
INFO - root - 2017-12-05 18:16:43.655352: step 32830, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 69h:52m:41s remains)
INFO - root - 2017-12-05 18:16:51.950902: step 32840, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.831 sec/batch; 69h:12m:14s remains)
INFO - root - 2017-12-05 18:17:00.489883: step 32850, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 69h:43m:55s remains)
INFO - root - 2017-12-05 18:17:08.941463: step 32860, loss = 2.04, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 68h:45m:07s remains)
INFO - root - 2017-12-05 18:17:17.484590: step 32870, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 71h:01m:17s remains)
INFO - root - 2017-12-05 18:17:26.056462: step 32880, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 74h:11m:52s remains)
INFO - root - 2017-12-05 18:17:34.400018: step 32890, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.823 sec/batch; 68h:29m:52s remains)
INFO - root - 2017-12-05 18:17:42.859798: step 32900, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 71h:11m:36s remains)
2017-12-05 18:17:43.713103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2603908 -4.2586656 -4.2559004 -4.2509623 -4.2454629 -4.2415056 -4.2367344 -4.2289214 -4.2180257 -4.2083063 -4.2009091 -4.2043228 -4.220715 -4.2398477 -4.2456717][-4.2611442 -4.2611032 -4.2594938 -4.2557697 -4.2526879 -4.2520084 -4.24965 -4.2405066 -4.2260923 -4.2141533 -4.2064152 -4.2108035 -4.2273703 -4.2441373 -4.2490525][-4.2629247 -4.2645059 -4.2653356 -4.2656083 -4.2665744 -4.26783 -4.2651072 -4.2531466 -4.237165 -4.2239871 -4.2145524 -4.2161589 -4.22747 -4.2374444 -4.2357054][-4.2646818 -4.2651939 -4.2652922 -4.266149 -4.26773 -4.2658024 -4.2576623 -4.2397022 -4.2198033 -4.2066789 -4.1981397 -4.1993561 -4.2087541 -4.2182312 -4.21252][-4.2533813 -4.2514906 -4.2503877 -4.2520142 -4.251729 -4.2409606 -4.2189918 -4.1903257 -4.1678848 -4.1576166 -4.154151 -4.161108 -4.1769571 -4.1915579 -4.1828036][-4.2269049 -4.2206082 -4.2193484 -4.223362 -4.2197585 -4.1950555 -4.1522889 -4.1065354 -4.0814834 -4.0827203 -4.094161 -4.1119642 -4.1345921 -4.1537867 -4.1465735][-4.1873784 -4.1749 -4.1708245 -4.1751919 -4.1654572 -4.1213531 -4.0477037 -3.9782813 -3.9564478 -3.9830136 -4.0235634 -4.0609326 -4.0911984 -4.1129222 -4.11288][-4.135675 -4.1127715 -4.1039839 -4.1044641 -4.0861144 -4.0224137 -3.917625 -3.8262398 -3.8191195 -3.887187 -3.9685404 -4.0316014 -4.0711288 -4.097703 -4.1078606][-4.095221 -4.0660973 -4.0556388 -4.0523834 -4.0274606 -3.9585736 -3.8548379 -3.7776239 -3.7981877 -3.8907645 -3.9876773 -4.0559573 -4.0947366 -4.1192427 -4.1324167][-4.08828 -4.071372 -4.0683627 -4.065475 -4.0439186 -3.9907641 -3.92084 -3.8821502 -3.9112148 -3.9819546 -4.0537004 -4.1029387 -4.1330376 -4.153523 -4.1651192][-4.0968394 -4.1025782 -4.1082177 -4.1067176 -4.0876646 -4.0524621 -4.0116229 -3.9932761 -4.0096073 -4.0475054 -4.0880542 -4.123404 -4.1543279 -4.1792917 -4.1910028][-4.0939097 -4.1165638 -4.124033 -4.1189528 -4.1023712 -4.0788321 -4.0552607 -4.0435429 -4.04382 -4.0566926 -4.0783467 -4.1078415 -4.1456189 -4.1767488 -4.1940479][-4.0979543 -4.1226997 -4.1209526 -4.1080689 -4.095336 -4.0821533 -4.0686746 -4.0625563 -4.0580573 -4.0552239 -4.0609426 -4.0828381 -4.1235046 -4.1605062 -4.1866226][-4.1244349 -4.1459665 -4.1397161 -4.1235752 -4.1136818 -4.10978 -4.0997152 -4.0927668 -4.0831714 -4.0715523 -4.0665455 -4.0780244 -4.1136 -4.1505356 -4.1808896][-4.1507511 -4.1705074 -4.16666 -4.1561837 -4.15346 -4.1571221 -4.147964 -4.1329522 -4.1170397 -4.1032143 -4.0932641 -4.0927243 -4.1145735 -4.1445093 -4.1754956]]...]
INFO - root - 2017-12-05 18:17:52.272675: step 32910, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 69h:36m:47s remains)
INFO - root - 2017-12-05 18:18:00.864932: step 32920, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 70h:39m:36s remains)
INFO - root - 2017-12-05 18:18:09.461616: step 32930, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 70h:08m:23s remains)
INFO - root - 2017-12-05 18:18:17.910785: step 32940, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 69h:54m:18s remains)
INFO - root - 2017-12-05 18:18:26.561225: step 32950, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 74h:09m:50s remains)
INFO - root - 2017-12-05 18:18:35.188667: step 32960, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 71h:23m:45s remains)
INFO - root - 2017-12-05 18:18:43.734045: step 32970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 71h:34m:46s remains)
INFO - root - 2017-12-05 18:18:52.195780: step 32980, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 70h:59m:28s remains)
INFO - root - 2017-12-05 18:19:00.775987: step 32990, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 72h:43m:18s remains)
INFO - root - 2017-12-05 18:19:09.332109: step 33000, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 70h:17m:06s remains)
2017-12-05 18:19:10.099298: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.17172 -4.134274 -4.12621 -4.1392555 -4.1520677 -4.1628661 -4.1607385 -4.1482716 -4.1308537 -4.1350822 -4.1686726 -4.1974721 -4.2050128 -4.2100043 -4.2217188][-4.1497269 -4.1061687 -4.0902052 -4.0976787 -4.1028147 -4.1076827 -4.1009049 -4.075223 -4.0422759 -4.0546517 -4.1132927 -4.1624355 -4.1793213 -4.1903372 -4.2067308][-4.1296825 -4.0848923 -4.0660028 -4.0630603 -4.0575142 -4.0604768 -4.0499458 -4.0048738 -3.9580755 -3.986335 -4.0724797 -4.1380339 -4.1649895 -4.1803064 -4.1984634][-4.1136565 -4.0757227 -4.0622754 -4.0531712 -4.0414405 -4.0383983 -4.0061555 -3.9295948 -3.8741124 -3.9254823 -4.0340266 -4.113368 -4.1501751 -4.1738186 -4.1922956][-4.119864 -4.0919757 -4.084374 -4.0691743 -4.0511818 -4.025569 -3.9354658 -3.8022511 -3.7469354 -3.8436453 -3.981787 -4.0804024 -4.1362066 -4.1708522 -4.1897531][-4.1459832 -4.1243372 -4.1111975 -4.0839663 -4.05505 -3.9946079 -3.8324332 -3.6246991 -3.5816512 -3.7543235 -3.9392982 -4.0631413 -4.1368866 -4.1762261 -4.1914597][-4.176312 -4.1521196 -4.1230831 -4.0751128 -4.0268593 -3.9333181 -3.7085278 -3.4290829 -3.4257412 -3.6910987 -3.9251173 -4.0678253 -4.1441741 -4.1794848 -4.1897607][-4.2116432 -4.1851521 -4.1437039 -4.0808854 -4.0165033 -3.9143229 -3.6943262 -3.4391711 -3.4687846 -3.7370508 -3.9605169 -4.0858507 -4.1426163 -4.1695619 -4.178][-4.2478609 -4.224566 -4.182229 -4.1247263 -4.0619717 -3.9751143 -3.8121395 -3.6400902 -3.6742167 -3.8689437 -4.0250163 -4.1082282 -4.1394105 -4.1563988 -4.1671662][-4.2754364 -4.2567492 -4.2232022 -4.1779881 -4.122632 -4.0486865 -3.9304457 -3.8226705 -3.8597462 -3.987617 -4.07936 -4.1233015 -4.136313 -4.1484122 -4.1640096][-4.2865672 -4.2702432 -4.2377491 -4.1956949 -4.1460667 -4.0830126 -3.9859338 -3.9177299 -3.9583845 -4.053689 -4.1163921 -4.143075 -4.1464996 -4.1567631 -4.1731029][-4.2891469 -4.2705479 -4.2339134 -4.1896963 -4.1469235 -4.099494 -4.0262079 -3.987299 -4.0328212 -4.1070013 -4.1509457 -4.1683354 -4.1672173 -4.175128 -4.1896529][-4.2836151 -4.2610068 -4.2259846 -4.190176 -4.1580009 -4.12669 -4.0826511 -4.0679235 -4.1121283 -4.16585 -4.1930075 -4.1982965 -4.1904087 -4.1949291 -4.2075205][-4.27927 -4.2542462 -4.2264609 -4.2037292 -4.1857796 -4.17161 -4.1536527 -4.15275 -4.1805634 -4.2103162 -4.2240815 -4.2224493 -4.214334 -4.2182288 -4.2306724][-4.2947764 -4.2740993 -4.2548738 -4.242187 -4.2323546 -4.2261992 -4.2204142 -4.2222385 -4.2344923 -4.2494774 -4.2569695 -4.2534232 -4.2486672 -4.255743 -4.2689595]]...]
INFO - root - 2017-12-05 18:19:18.524196: step 33010, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 71h:43m:30s remains)
INFO - root - 2017-12-05 18:19:27.069234: step 33020, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 70h:42m:44s remains)
INFO - root - 2017-12-05 18:19:35.692697: step 33030, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 70h:54m:05s remains)
INFO - root - 2017-12-05 18:19:44.162042: step 33040, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 70h:32m:53s remains)
INFO - root - 2017-12-05 18:19:52.797352: step 33050, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 70h:33m:36s remains)
INFO - root - 2017-12-05 18:20:01.486833: step 33060, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 72h:25m:04s remains)
INFO - root - 2017-12-05 18:20:10.075819: step 33070, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 71h:39m:05s remains)
INFO - root - 2017-12-05 18:20:18.629910: step 33080, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 72h:00m:11s remains)
INFO - root - 2017-12-05 18:20:27.298391: step 33090, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 70h:33m:58s remains)
INFO - root - 2017-12-05 18:20:35.832430: step 33100, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 74h:03m:20s remains)
2017-12-05 18:20:36.599794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2438188 -4.2302856 -4.232245 -4.2277651 -4.2241287 -4.2248464 -4.2163582 -4.200654 -4.1904263 -4.1982369 -4.2264671 -4.2459922 -4.2518439 -4.2385983 -4.2177677][-4.1964545 -4.1796484 -4.187798 -4.1912704 -4.1943479 -4.19377 -4.1833682 -4.1678782 -4.1623726 -4.1761093 -4.2037377 -4.2221055 -4.2244487 -4.2054329 -4.18174][-4.1551056 -4.1375484 -4.15256 -4.1689272 -4.1781797 -4.17303 -4.1520152 -4.1318231 -4.1347756 -4.1593914 -4.187664 -4.2025065 -4.2024069 -4.1788869 -4.1563993][-4.1292868 -4.1177874 -4.1420441 -4.1663785 -4.1750908 -4.1606417 -4.1243949 -4.1023231 -4.1196489 -4.1590366 -4.190259 -4.2009125 -4.1968446 -4.1740084 -4.1551609][-4.130496 -4.1207442 -4.1471314 -4.1700392 -4.1694536 -4.1425714 -4.09996 -4.0883532 -4.120358 -4.1650839 -4.1930337 -4.1916633 -4.1799364 -4.1616416 -4.156467][-4.1411438 -4.1285658 -4.1544085 -4.1725969 -4.1623192 -4.1304655 -4.090426 -4.0906129 -4.1296096 -4.1706891 -4.1851997 -4.1743679 -4.156487 -4.1361284 -4.1413441][-4.1478868 -4.1364717 -4.1622128 -4.1771908 -4.166595 -4.1350145 -4.0979629 -4.0971208 -4.1298504 -4.1644664 -4.1725574 -4.1636128 -4.1471939 -4.129498 -4.1363363][-4.1453152 -4.1402144 -4.1684132 -4.1839223 -4.1786895 -4.1509614 -4.1117935 -4.1025953 -4.1244221 -4.1527724 -4.1598644 -4.1564555 -4.1489911 -4.1385007 -4.1397972][-4.1385021 -4.1412535 -4.1767583 -4.1987767 -4.2019615 -4.1767859 -4.1386118 -4.1221886 -4.1340933 -4.1532254 -4.1581683 -4.1611714 -4.1620703 -4.1521645 -4.1369109][-4.1148224 -4.1157656 -4.1599989 -4.1948142 -4.2081132 -4.1911793 -4.1566696 -4.1425309 -4.1490436 -4.1576986 -4.160872 -4.1681161 -4.1686234 -4.1495442 -4.1167021][-4.0827065 -4.0724397 -4.1092176 -4.1488 -4.1738286 -4.1691742 -4.1514955 -4.1517415 -4.163506 -4.171804 -4.1723 -4.1754036 -4.1694708 -4.1384435 -4.0923471][-4.0682268 -4.0409679 -4.0555115 -4.0782151 -4.1028528 -4.1169438 -4.1240683 -4.1466589 -4.1787047 -4.1973691 -4.1975064 -4.1962876 -4.182344 -4.1408005 -4.089612][-4.0949154 -4.0717325 -4.062521 -4.0461493 -4.04758 -4.0698595 -4.0980129 -4.13567 -4.1809936 -4.2087779 -4.2161636 -4.2222137 -4.21111 -4.1716518 -4.1249971][-4.1440077 -4.133399 -4.1155133 -4.0779524 -4.0622797 -4.0803156 -4.1163726 -4.1513424 -4.1888 -4.2131977 -4.2220345 -4.2327619 -4.2296462 -4.1949978 -4.1554518][-4.1825161 -4.1785936 -4.161284 -4.1268311 -4.1105957 -4.1223335 -4.1513352 -4.1792908 -4.208631 -4.2296138 -4.2332997 -4.2376976 -4.230866 -4.1969342 -4.1607971]]...]
INFO - root - 2017-12-05 18:20:45.131532: step 33110, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 71h:42m:24s remains)
INFO - root - 2017-12-05 18:20:53.533942: step 33120, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 72h:08m:31s remains)
INFO - root - 2017-12-05 18:21:01.926810: step 33130, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 70h:14m:46s remains)
INFO - root - 2017-12-05 18:21:10.495724: step 33140, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 71h:52m:40s remains)
INFO - root - 2017-12-05 18:21:19.048815: step 33150, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 72h:07m:40s remains)
INFO - root - 2017-12-05 18:21:27.721684: step 33160, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 73h:34m:55s remains)
INFO - root - 2017-12-05 18:21:36.481262: step 33170, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 74h:58m:49s remains)
INFO - root - 2017-12-05 18:21:45.152635: step 33180, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 73h:21m:43s remains)
INFO - root - 2017-12-05 18:21:53.654359: step 33190, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 70h:09m:49s remains)
INFO - root - 2017-12-05 18:22:02.174303: step 33200, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 72h:17m:12s remains)
2017-12-05 18:22:03.079049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2485356 -4.2460179 -4.2267227 -4.1878119 -4.1437931 -4.097425 -4.0598326 -4.0462108 -4.053565 -4.0826902 -4.1227202 -4.1560583 -4.1703081 -4.1661725 -4.1891866][-4.238987 -4.2351441 -4.2137284 -4.1702209 -4.1142416 -4.0575228 -4.0189123 -4.0178308 -4.0403152 -4.07344 -4.1122785 -4.1521149 -4.1767015 -4.1777978 -4.1951175][-4.2234683 -4.215569 -4.1905265 -4.1473737 -4.0823259 -4.0181079 -3.9859407 -4.0039005 -4.0384183 -4.0728307 -4.1064529 -4.146101 -4.1783605 -4.1868834 -4.202517][-4.1995525 -4.1909189 -4.1650715 -4.1284266 -4.0597286 -3.9861927 -3.9565859 -3.9962313 -4.0421143 -4.075654 -4.1035056 -4.138638 -4.1749306 -4.1894817 -4.2060723][-4.1750026 -4.170917 -4.1464891 -4.1123719 -4.03848 -3.9461188 -3.9052837 -3.9646959 -4.0324559 -4.0761371 -4.1014557 -4.1295366 -4.1667671 -4.1871338 -4.210567][-4.1531682 -4.1560154 -4.1337619 -4.0932746 -4.0055323 -3.8753874 -3.8010576 -3.8872125 -3.9943693 -4.061573 -4.0963888 -4.1230664 -4.158906 -4.1886339 -4.22328][-4.1154661 -4.126842 -4.1082959 -4.0602956 -3.9574809 -3.7925508 -3.684124 -3.8050013 -3.9497259 -4.0396276 -4.0914326 -4.1240635 -4.15587 -4.1920457 -4.2365084][-4.069478 -4.0906806 -4.0865264 -4.0431247 -3.9488249 -3.8027444 -3.7118344 -3.8243027 -3.9604862 -4.0444636 -4.1006584 -4.1360822 -4.1597152 -4.1932435 -4.2396226][-4.08195 -4.1034155 -4.1055717 -4.0740991 -4.00808 -3.9162869 -3.8652573 -3.9338574 -4.0211425 -4.0714097 -4.1102433 -4.1384516 -4.1566305 -4.185523 -4.229331][-4.1315436 -4.1421423 -4.1373105 -4.1150789 -4.0730734 -4.0233097 -3.9962311 -4.0302854 -4.0798459 -4.1074305 -4.1345019 -4.1558781 -4.1711063 -4.1936936 -4.2254577][-4.1651592 -4.1703796 -4.1660976 -4.1508422 -4.1216478 -4.09327 -4.0808663 -4.1008654 -4.1290965 -4.1476793 -4.170258 -4.18777 -4.2031193 -4.2208843 -4.2397404][-4.1941257 -4.1961532 -4.1941419 -4.1837654 -4.159678 -4.1415987 -4.1405973 -4.1570139 -4.1752715 -4.1905055 -4.2059212 -4.2134142 -4.2274652 -4.2462893 -4.2603188][-4.2179055 -4.2183962 -4.2155461 -4.2079577 -4.1860089 -4.1734738 -4.1777511 -4.191112 -4.2055407 -4.2178082 -4.2275529 -4.2303853 -4.2430506 -4.26072 -4.2743778][-4.2314672 -4.2322283 -4.2316642 -4.2271237 -4.21247 -4.2061443 -4.2109079 -4.2195487 -4.227005 -4.2335062 -4.2404842 -4.2448592 -4.2573228 -4.2737994 -4.28683][-4.2465763 -4.2500329 -4.2565956 -4.258708 -4.253397 -4.2514038 -4.2560091 -4.2576714 -4.256794 -4.2567453 -4.2593107 -4.2618208 -4.2727728 -4.2894616 -4.3021536]]...]
INFO - root - 2017-12-05 18:22:11.610809: step 33210, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 71h:05m:13s remains)
INFO - root - 2017-12-05 18:22:20.172915: step 33220, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 68h:46m:05s remains)
INFO - root - 2017-12-05 18:22:28.411454: step 33230, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 72h:47m:26s remains)
INFO - root - 2017-12-05 18:22:36.844422: step 33240, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 68h:47m:15s remains)
INFO - root - 2017-12-05 18:22:45.521388: step 33250, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 71h:27m:52s remains)
INFO - root - 2017-12-05 18:22:54.100051: step 33260, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 70h:51m:52s remains)
INFO - root - 2017-12-05 18:23:02.686156: step 33270, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 73h:50m:20s remains)
INFO - root - 2017-12-05 18:23:11.264057: step 33280, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 74h:36m:55s remains)
INFO - root - 2017-12-05 18:23:19.771168: step 33290, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 70h:19m:14s remains)
INFO - root - 2017-12-05 18:23:28.287212: step 33300, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 70h:53m:18s remains)
2017-12-05 18:23:29.034312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1550469 -4.1653461 -4.1696 -4.1722713 -4.1648264 -4.1501923 -4.1593747 -4.1748409 -4.1744304 -4.1756744 -4.181088 -4.1752157 -4.1669345 -4.1689177 -4.1678357][-4.17496 -4.1892381 -4.1926823 -4.1942492 -4.1848364 -4.1664863 -4.1705232 -4.1888642 -4.1957121 -4.1964145 -4.1955233 -4.18292 -4.1682839 -4.1664715 -4.1657128][-4.1916995 -4.2012634 -4.200397 -4.2022605 -4.1918883 -4.170486 -4.1709585 -4.191751 -4.2044864 -4.2100339 -4.2102103 -4.1960359 -4.1801019 -4.175508 -4.1727324][-4.2031713 -4.2052684 -4.197906 -4.1986175 -4.1893363 -4.1678767 -4.1685705 -4.1908665 -4.204998 -4.213603 -4.2167883 -4.2068381 -4.1974049 -4.1926694 -4.1853256][-4.2004871 -4.1958957 -4.1860986 -4.188004 -4.1845284 -4.1724424 -4.1764293 -4.1933441 -4.1981311 -4.2024269 -4.20902 -4.2070937 -4.2062364 -4.2039938 -4.1936011][-4.1918874 -4.1851192 -4.1787915 -4.184885 -4.1847844 -4.1806145 -4.1864777 -4.195971 -4.1896667 -4.185822 -4.1943245 -4.1987925 -4.2025919 -4.2020073 -4.1917071][-4.1961842 -4.1907663 -4.1881275 -4.1952147 -4.1914759 -4.1875319 -4.1893125 -4.1925564 -4.1827822 -4.1763859 -4.1838517 -4.187716 -4.1904693 -4.190197 -4.1815009][-4.2017632 -4.1979675 -4.1990848 -4.2027574 -4.190598 -4.1813207 -4.1814618 -4.1884532 -4.1849723 -4.1844611 -4.189064 -4.1858172 -4.1780114 -4.1706514 -4.1685262][-4.1977983 -4.1994672 -4.2075758 -4.2066088 -4.1807694 -4.1595969 -4.1608925 -4.1780586 -4.1855488 -4.1917062 -4.1963439 -4.187624 -4.1672249 -4.1509957 -4.1547561][-4.18413 -4.1966515 -4.2091026 -4.2052121 -4.1723309 -4.1436853 -4.1476727 -4.1734352 -4.1883373 -4.1962576 -4.2000823 -4.1919413 -4.1725841 -4.1583891 -4.1639895][-4.1616416 -4.1891112 -4.2071853 -4.2079058 -4.181704 -4.1533952 -4.153738 -4.1750665 -4.1862831 -4.1904798 -4.1962104 -4.1951203 -4.1848006 -4.1805906 -4.1879063][-4.1486692 -4.1835279 -4.2070632 -4.2195573 -4.2063656 -4.1769338 -4.1643486 -4.1748319 -4.1820593 -4.1854253 -4.1926293 -4.1991191 -4.1990004 -4.1992149 -4.2048454][-4.1480141 -4.17903 -4.2030773 -4.2252479 -4.2234893 -4.1946564 -4.1718259 -4.1769924 -4.1872115 -4.1962643 -4.2023497 -4.2070875 -4.2080169 -4.2061973 -4.2087021][-4.1529956 -4.1773653 -4.1977768 -4.2232332 -4.2304206 -4.2080441 -4.1876626 -4.1950746 -4.2077713 -4.2223411 -4.2263441 -4.2252307 -4.2196679 -4.2100272 -4.2051086][-4.1647534 -4.1797547 -4.1954274 -4.2205954 -4.2337246 -4.22343 -4.2092304 -4.2170792 -4.22839 -4.241981 -4.2452645 -4.2421689 -4.2319818 -4.2151384 -4.2039013]]...]
INFO - root - 2017-12-05 18:23:37.721634: step 33310, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 72h:38m:28s remains)
INFO - root - 2017-12-05 18:23:46.327444: step 33320, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 71h:09m:39s remains)
INFO - root - 2017-12-05 18:23:54.715504: step 33330, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.818 sec/batch; 67h:59m:19s remains)
INFO - root - 2017-12-05 18:24:02.931901: step 33340, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 68h:10m:17s remains)
INFO - root - 2017-12-05 18:24:11.496878: step 33350, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 70h:55m:46s remains)
INFO - root - 2017-12-05 18:24:20.014476: step 33360, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 69h:54m:55s remains)
INFO - root - 2017-12-05 18:24:28.634913: step 33370, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 73h:02m:36s remains)
INFO - root - 2017-12-05 18:24:37.305441: step 33380, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 73h:16m:48s remains)
INFO - root - 2017-12-05 18:24:45.862001: step 33390, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 73h:03m:32s remains)
INFO - root - 2017-12-05 18:24:54.308515: step 33400, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 69h:41m:48s remains)
2017-12-05 18:24:55.035908: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2249093 -4.2123866 -4.19317 -4.1785269 -4.1775947 -4.1882472 -4.2080078 -4.2241755 -4.2289667 -4.2257128 -4.2212362 -4.2276196 -4.2411218 -4.2548704 -4.2649274][-4.2055273 -4.19495 -4.1776862 -4.1637664 -4.1614103 -4.1667361 -4.1790051 -4.1888442 -4.1907921 -4.1883163 -4.18618 -4.19411 -4.2081642 -4.221745 -4.2326717][-4.1942644 -4.1861925 -4.17473 -4.1672025 -4.1655807 -4.1627822 -4.1595035 -4.1556172 -4.1511307 -4.1485176 -4.1512156 -4.162128 -4.1771016 -4.1910014 -4.2039652][-4.18465 -4.1776991 -4.1741419 -4.1756773 -4.1772728 -4.167573 -4.1480355 -4.1278481 -4.1194124 -4.1232815 -4.1331644 -4.143136 -4.15371 -4.1650343 -4.1795278][-4.1694989 -4.1615624 -4.1643085 -4.1740761 -4.181 -4.1709557 -4.1422834 -4.108274 -4.0975623 -4.1116657 -4.129746 -4.1414924 -4.1480217 -4.1534944 -4.1661825][-4.152307 -4.1438746 -4.1471 -4.156497 -4.162724 -4.150218 -4.1113157 -4.0665984 -4.0594349 -4.0910335 -4.1239104 -4.145761 -4.1530004 -4.156426 -4.1680489][-4.1386905 -4.1344585 -4.1358724 -4.1374617 -4.1349735 -4.1095691 -4.0502591 -3.9854217 -3.9794254 -4.0355124 -4.0975375 -4.1423974 -4.1617002 -4.1694145 -4.1837907][-4.1179104 -4.12359 -4.1297646 -4.1253734 -4.1120391 -4.0696664 -3.9874642 -3.89544 -3.8796427 -3.9620066 -4.0610242 -4.1343217 -4.1694136 -4.1803489 -4.1927242][-4.0985012 -4.1113667 -4.1251807 -4.1238537 -4.1105156 -4.0696678 -3.9959621 -3.9103665 -3.8862677 -3.958673 -4.0584221 -4.1389251 -4.1801019 -4.1877422 -4.1892896][-4.0903978 -4.1009903 -4.1182551 -4.1265435 -4.1257005 -4.1034842 -4.0579782 -4.0019035 -3.9787686 -4.0190177 -4.0874748 -4.1466265 -4.1801724 -4.1816921 -4.1735191][-4.0876431 -4.0840797 -4.0988603 -4.1176853 -4.13327 -4.128952 -4.1055355 -4.0741339 -4.0594392 -4.0794096 -4.1192603 -4.1544733 -4.1773872 -4.1747994 -4.1602831][-4.0862937 -4.0640116 -4.072403 -4.1010833 -4.1325169 -4.1419353 -4.1270661 -4.106472 -4.0984879 -4.1114931 -4.1407633 -4.1651154 -4.1802244 -4.1753712 -4.1574955][-4.0835972 -4.0521994 -4.0557036 -4.0869937 -4.1261573 -4.1445756 -4.13234 -4.1138692 -4.1096339 -4.1225324 -4.1493211 -4.1716413 -4.1847491 -4.1813378 -4.1663456][-4.0865517 -4.0633368 -4.0650787 -4.09317 -4.1310444 -4.1540718 -4.1453404 -4.1289968 -4.1258268 -4.1374621 -4.1627364 -4.1841855 -4.1956725 -4.1932173 -4.1829114][-4.109776 -4.0995121 -4.1023135 -4.1214237 -4.151978 -4.1758108 -4.1740804 -4.1646485 -4.1650734 -4.1752248 -4.1969142 -4.2160897 -4.2240939 -4.2203984 -4.2127409]]...]
INFO - root - 2017-12-05 18:25:03.503206: step 33410, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 71h:06m:55s remains)
INFO - root - 2017-12-05 18:25:11.859921: step 33420, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 71h:24m:56s remains)
INFO - root - 2017-12-05 18:25:20.378438: step 33430, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 69h:52m:27s remains)
INFO - root - 2017-12-05 18:25:28.939761: step 33440, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 74h:23m:49s remains)
INFO - root - 2017-12-05 18:25:37.483348: step 33450, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 72h:29m:19s remains)
INFO - root - 2017-12-05 18:25:46.056635: step 33460, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 68h:56m:31s remains)
INFO - root - 2017-12-05 18:25:54.647729: step 33470, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 71h:14m:10s remains)
INFO - root - 2017-12-05 18:26:03.222656: step 33480, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 71h:22m:20s remains)
INFO - root - 2017-12-05 18:26:11.787735: step 33490, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 72h:33m:20s remains)
INFO - root - 2017-12-05 18:26:20.398288: step 33500, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 70h:06m:20s remains)
2017-12-05 18:26:21.131928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1862459 -4.1704035 -4.151557 -4.1364608 -4.1402879 -4.1458254 -4.1493587 -4.1549644 -4.1488338 -4.1307211 -4.1000662 -4.075738 -4.0999355 -4.1553335 -4.1993866][-4.1824641 -4.1630325 -4.1338925 -4.109324 -4.1056719 -4.1042581 -4.1111207 -4.1343403 -4.1411219 -4.1212931 -4.0869155 -4.0596232 -4.0772352 -4.1303387 -4.1843405][-4.183589 -4.1544747 -4.1151605 -4.0877285 -4.0728226 -4.055481 -4.0616536 -4.1002903 -4.1221228 -4.1090946 -4.079031 -4.0546293 -4.0633645 -4.109879 -4.1679611][-4.1794806 -4.1406264 -4.0962906 -4.0662708 -4.0364547 -4.0000043 -4.0084896 -4.0632529 -4.1044135 -4.1041541 -4.0885539 -4.0738583 -4.0713429 -4.1013207 -4.1539087][-4.1759896 -4.1323543 -4.082057 -4.0448 -4.0028625 -3.9526908 -3.9583 -4.0273604 -4.0875316 -4.1051726 -4.11136 -4.1127324 -4.1030955 -4.1108041 -4.1434584][-4.1874757 -4.139204 -4.0833459 -4.0341263 -3.9800138 -3.9119787 -3.901695 -3.9751589 -4.0545473 -4.0946031 -4.1198287 -4.1354523 -4.1285186 -4.121819 -4.1347885][-4.1983938 -4.15075 -4.0954466 -4.0403752 -3.9743118 -3.880892 -3.8382492 -3.9059105 -4.0025768 -4.0637889 -4.1027751 -4.12642 -4.1300154 -4.1231017 -4.1261258][-4.1934428 -4.1571655 -4.1100965 -4.0612812 -3.9898024 -3.8753519 -3.7929392 -3.8498037 -3.9601929 -4.0400271 -4.0836811 -4.109138 -4.121789 -4.124516 -4.1258316][-4.1809683 -4.15763 -4.1268458 -4.096777 -4.0368195 -3.9289021 -3.8333888 -3.867039 -3.9613373 -4.0404973 -4.0797424 -4.1009722 -4.1214242 -4.135107 -4.1387525][-4.168344 -4.1555672 -4.1374779 -4.1236339 -4.0838704 -4.0054655 -3.9333789 -3.9449406 -4.0017757 -4.0565209 -4.0823436 -4.1001072 -4.1271129 -4.1501474 -4.160212][-4.1575985 -4.1532278 -4.1419497 -4.1342578 -4.1090207 -4.0550594 -4.0135374 -4.0164571 -4.0417008 -4.0633211 -4.0708346 -4.0869102 -4.1196012 -4.152226 -4.1733427][-4.1482987 -4.1548476 -4.147872 -4.1388936 -4.1236649 -4.0869865 -4.0635028 -4.0609984 -4.0651364 -4.0619526 -4.0582523 -4.0737143 -4.110261 -4.1516628 -4.1840243][-4.1424432 -4.1611366 -4.1635971 -4.1576867 -4.1508379 -4.12536 -4.1080923 -4.098105 -4.0907578 -4.0769072 -4.0643644 -4.0726509 -4.1037765 -4.148272 -4.1885405][-4.1538458 -4.1792574 -4.1898031 -4.1917095 -4.1916227 -4.1743312 -4.16107 -4.1446848 -4.1308489 -4.1110229 -4.0876637 -4.08135 -4.1010976 -4.1455288 -4.192728][-4.1778603 -4.2036 -4.2185636 -4.2265387 -4.2309341 -4.2193847 -4.2072177 -4.1853919 -4.1677647 -4.1465993 -4.1176195 -4.1025429 -4.1148629 -4.1546621 -4.2008643]]...]
INFO - root - 2017-12-05 18:26:29.730664: step 33510, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 73h:35m:38s remains)
INFO - root - 2017-12-05 18:26:38.261910: step 33520, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.881 sec/batch; 73h:10m:27s remains)
INFO - root - 2017-12-05 18:26:46.974763: step 33530, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 70h:04m:02s remains)
INFO - root - 2017-12-05 18:26:55.444871: step 33540, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 71h:38m:57s remains)
INFO - root - 2017-12-05 18:27:03.910112: step 33550, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 71h:11m:51s remains)
INFO - root - 2017-12-05 18:27:12.497577: step 33560, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 70h:57m:43s remains)
INFO - root - 2017-12-05 18:27:21.056846: step 33570, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 72h:15m:42s remains)
INFO - root - 2017-12-05 18:27:29.534702: step 33580, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 72h:55m:08s remains)
INFO - root - 2017-12-05 18:27:38.122782: step 33590, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 73h:16m:52s remains)
INFO - root - 2017-12-05 18:27:46.623873: step 33600, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 70h:17m:20s remains)
2017-12-05 18:27:47.343372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3077903 -4.2956362 -4.2833424 -4.2734337 -4.2679386 -4.2623563 -4.256669 -4.25172 -4.255137 -4.2668877 -4.279532 -4.2905912 -4.3005033 -4.3062911 -4.3115835][-4.2868829 -4.2717171 -4.2581348 -4.2476106 -4.242239 -4.23443 -4.2236447 -4.2165384 -4.22202 -4.2399578 -4.2542176 -4.2664576 -4.2785048 -4.2846494 -4.2898793][-4.26155 -4.244473 -4.2337928 -4.2258172 -4.2197094 -4.2069421 -4.1913667 -4.183476 -4.1942821 -4.2174716 -4.2304955 -4.2408261 -4.2539077 -4.2598352 -4.2642207][-4.2298827 -4.2163181 -4.2107081 -4.2044282 -4.1972237 -4.1800508 -4.1563811 -4.14575 -4.1621995 -4.1909714 -4.2045431 -4.2148342 -4.2280841 -4.2353177 -4.2398][-4.1979818 -4.1920094 -4.1910758 -4.1822085 -4.1708455 -4.1452441 -4.10392 -4.0810857 -4.1057229 -4.1509414 -4.1731887 -4.187798 -4.2025166 -4.2114487 -4.2195363][-4.1678147 -4.171319 -4.1740565 -4.1627836 -4.1424408 -4.104382 -4.0386305 -3.9939876 -4.0307589 -4.1076427 -4.1476893 -4.1693554 -4.1820951 -4.1932311 -4.2056675][-4.1475334 -4.1560144 -4.1593561 -4.1454816 -4.1187854 -4.0680776 -3.971998 -3.8932765 -3.9503362 -4.0656004 -4.1251674 -4.1544333 -4.1663389 -4.1797156 -4.1951733][-4.1393309 -4.1493907 -4.1505532 -4.1366882 -4.108911 -4.0525093 -3.9301026 -3.8132572 -3.8845119 -4.0301828 -4.1055684 -4.1427951 -4.1570387 -4.1693377 -4.1844745][-4.1402373 -4.1501617 -4.1524839 -4.1463232 -4.1281672 -4.0853815 -3.9778864 -3.8685575 -3.9266489 -4.0520048 -4.120317 -4.152926 -4.1658363 -4.1788416 -4.1900668][-4.1464772 -4.155407 -4.1638126 -4.1654153 -4.1589766 -4.1348691 -4.063674 -3.9950495 -4.0302243 -4.1050353 -4.146328 -4.1675291 -4.1810832 -4.1971292 -4.2039738][-4.1507244 -4.1579709 -4.1735229 -4.1821947 -4.1838131 -4.1692958 -4.1234274 -4.0850439 -4.1089182 -4.1475554 -4.1643949 -4.1747856 -4.1868596 -4.2021694 -4.20598][-4.1654582 -4.1708574 -4.1925092 -4.2053237 -4.2096844 -4.2005596 -4.168963 -4.1449213 -4.16224 -4.1828918 -4.18521 -4.1858654 -4.1943259 -4.2050395 -4.2064772][-4.1850076 -4.18482 -4.2048259 -4.2168336 -4.22086 -4.2118287 -4.18384 -4.1638083 -4.17705 -4.19402 -4.1984458 -4.2027664 -4.2119017 -4.2210455 -4.2233624][-4.2108183 -4.2083359 -4.22301 -4.2299008 -4.2295866 -4.2183108 -4.1905622 -4.1716566 -4.1832223 -4.200913 -4.2145677 -4.2296042 -4.242907 -4.2537808 -4.2560625][-4.2489662 -4.2424383 -4.248239 -4.2492237 -4.2463784 -4.2362237 -4.2154741 -4.2052298 -4.2179966 -4.2363229 -4.2547693 -4.2711253 -4.2832341 -4.2921977 -4.29276]]...]
INFO - root - 2017-12-05 18:27:55.892792: step 33610, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 69h:58m:40s remains)
INFO - root - 2017-12-05 18:28:04.440980: step 33620, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.863 sec/batch; 71h:38m:24s remains)
INFO - root - 2017-12-05 18:28:13.016247: step 33630, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 69h:58m:25s remains)
INFO - root - 2017-12-05 18:28:21.476500: step 33640, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 68h:38m:53s remains)
INFO - root - 2017-12-05 18:28:29.919246: step 33650, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.768 sec/batch; 63h:46m:29s remains)
INFO - root - 2017-12-05 18:28:38.712121: step 33660, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 70h:53m:43s remains)
INFO - root - 2017-12-05 18:28:47.186304: step 33670, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 72h:43m:09s remains)
INFO - root - 2017-12-05 18:28:55.695195: step 33680, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 68h:52m:52s remains)
INFO - root - 2017-12-05 18:29:04.250563: step 33690, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 72h:03m:58s remains)
INFO - root - 2017-12-05 18:29:12.744117: step 33700, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 69h:42m:55s remains)
2017-12-05 18:29:13.487389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23204 -4.2626605 -4.2875209 -4.3009753 -4.2973332 -4.2903371 -4.2924232 -4.2901397 -4.2750311 -4.2636456 -4.2732453 -4.2912421 -4.3090425 -4.3224058 -4.334341][-4.1606894 -4.2070408 -4.2477384 -4.2691073 -4.2631912 -4.2532907 -4.2561617 -4.2514033 -4.2319584 -4.2197728 -4.2367468 -4.2619848 -4.2835855 -4.3021841 -4.3214841][-4.0865431 -4.1501632 -4.2075505 -4.2319126 -4.2208533 -4.2099104 -4.2153707 -4.2083316 -4.1850495 -4.1738067 -4.197753 -4.2304106 -4.2562141 -4.28083 -4.3076344][-4.0374308 -4.1099906 -4.1758785 -4.1971316 -4.1776 -4.1626067 -4.1679168 -4.159729 -4.1331573 -4.1248817 -4.15627 -4.2025704 -4.237052 -4.2688584 -4.299376][-4.0281243 -4.0959721 -4.1544075 -4.1635742 -4.1255784 -4.0995131 -4.1039357 -4.09927 -4.0787954 -4.0795789 -4.122282 -4.1826038 -4.2246113 -4.2611694 -4.2908611][-4.0534296 -4.1008725 -4.1338682 -4.1158438 -4.0512066 -4.0062947 -4.0131831 -4.0256863 -4.0164027 -4.0260224 -4.07717 -4.1487226 -4.2029839 -4.249207 -4.2779627][-4.0956311 -4.1166396 -4.1176262 -4.0660782 -3.9687369 -3.8962548 -3.9050717 -3.9419427 -3.9449801 -3.9617665 -4.0204029 -4.0981617 -4.1617155 -4.221849 -4.2596812][-4.13932 -4.1358986 -4.1112432 -4.0388088 -3.9255133 -3.8356988 -3.8360353 -3.872112 -3.862484 -3.8793025 -3.9506335 -4.0347781 -4.0995731 -4.1697254 -4.2245016][-4.1750603 -4.1579638 -4.1267762 -4.058785 -3.9522476 -3.8646951 -3.85063 -3.8627563 -3.8335023 -3.8430481 -3.9148526 -3.9954669 -4.0537663 -4.123498 -4.1903486][-4.2113733 -4.1919436 -4.1651206 -4.116456 -4.0368085 -3.9714813 -3.9521406 -3.94575 -3.9079413 -3.9018455 -3.9507849 -4.010087 -4.05294 -4.1140022 -4.1848297][-4.2531385 -4.23914 -4.2211981 -4.1928754 -4.1439481 -4.1013818 -4.081203 -4.0631962 -4.025949 -4.0077634 -4.0299664 -4.0631881 -4.0994778 -4.15718 -4.2201695][-4.292387 -4.2846222 -4.2758102 -4.2617126 -4.2359686 -4.2123823 -4.19828 -4.1813617 -4.1520333 -4.1294861 -4.1333508 -4.1465693 -4.1778979 -4.2275729 -4.2759671][-4.3166556 -4.3147283 -4.314158 -4.3103986 -4.2975922 -4.287353 -4.2839913 -4.2774982 -4.2582874 -4.2339272 -4.2257438 -4.2295084 -4.2534347 -4.2911739 -4.324131][-4.3280864 -4.3276963 -4.3300633 -4.3315597 -4.3273315 -4.3252187 -4.3286438 -4.3293462 -4.3196917 -4.3007793 -4.2875876 -4.2890987 -4.3072639 -4.3318758 -4.3510838][-4.3331609 -4.3319592 -4.3332043 -4.3347111 -4.3348212 -4.3360491 -4.3412232 -4.344974 -4.3424683 -4.3323436 -4.3223548 -4.32384 -4.3359032 -4.3502135 -4.358314]]...]
INFO - root - 2017-12-05 18:29:22.143586: step 33710, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.914 sec/batch; 75h:53m:48s remains)
INFO - root - 2017-12-05 18:29:30.687655: step 33720, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 71h:52m:53s remains)
INFO - root - 2017-12-05 18:29:39.177451: step 33730, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 70h:40m:54s remains)
INFO - root - 2017-12-05 18:29:47.626856: step 33740, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 69h:09m:36s remains)
INFO - root - 2017-12-05 18:29:56.076224: step 33750, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 70h:56m:29s remains)
INFO - root - 2017-12-05 18:30:04.451629: step 33760, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 70h:44m:36s remains)
INFO - root - 2017-12-05 18:30:12.903609: step 33770, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 69h:28m:58s remains)
INFO - root - 2017-12-05 18:30:21.369680: step 33780, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 70h:48m:07s remains)
INFO - root - 2017-12-05 18:30:29.912051: step 33790, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 71h:01m:25s remains)
INFO - root - 2017-12-05 18:30:38.467820: step 33800, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 71h:47m:56s remains)
2017-12-05 18:30:39.207799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2237258 -4.231029 -4.2095642 -4.1742249 -4.119556 -4.0443788 -3.9681704 -3.97653 -4.0438356 -4.1069822 -4.158392 -4.2187476 -4.2591228 -4.2758112 -4.2679415][-4.2399225 -4.2399592 -4.2139616 -4.1828394 -4.1420212 -4.076704 -3.9951696 -3.9823012 -4.0306387 -4.08781 -4.1324382 -4.1928973 -4.2472262 -4.2764072 -4.2737517][-4.255549 -4.2490664 -4.2194972 -4.1885257 -4.1536336 -4.0975471 -4.0191522 -3.9936144 -4.0318871 -4.0841341 -4.1235552 -4.1810222 -4.2409444 -4.2759781 -4.2756453][-4.2620978 -4.2514687 -4.225153 -4.1963539 -4.1607928 -4.1021619 -4.0139728 -3.9778693 -4.0227675 -4.0831628 -4.126915 -4.1811461 -4.2401986 -4.2754273 -4.2762909][-4.2643137 -4.245635 -4.2152138 -4.1815038 -4.1382012 -4.06889 -3.9659729 -3.9170213 -3.9744267 -4.05508 -4.1180744 -4.1804304 -4.2415748 -4.2777138 -4.2790637][-4.2649636 -4.2363553 -4.1966267 -4.1531205 -4.099247 -4.0138431 -3.8927948 -3.8357852 -3.9068949 -4.0072432 -4.0958595 -4.1743212 -4.2403188 -4.2781711 -4.2808][-4.2650967 -4.2294984 -4.1814232 -4.1257839 -4.0601931 -3.9672408 -3.8532345 -3.8186493 -3.8986127 -3.9965785 -4.089004 -4.1731625 -4.2386446 -4.2764525 -4.2805228][-4.2598319 -4.2223663 -4.1720238 -4.1096177 -4.0383759 -3.9478328 -3.8521857 -3.8410785 -3.9161596 -4.0028467 -4.0874233 -4.1730318 -4.2373509 -4.2770004 -4.2828593][-4.2565846 -4.2216392 -4.1779633 -4.1206512 -4.0513239 -3.9573267 -3.8599205 -3.8492711 -3.910301 -3.9894443 -4.0727396 -4.1654592 -4.2359457 -4.2788911 -4.2872815][-4.2616191 -4.2305312 -4.1960216 -4.1508532 -4.0923586 -3.9946182 -3.8815212 -3.8536506 -3.9084163 -3.9940772 -4.0805964 -4.1727176 -4.2433696 -4.2852683 -4.2959008][-4.2715869 -4.2441349 -4.2141094 -4.1765404 -4.127316 -4.0348206 -3.9196243 -3.8825533 -3.9364846 -4.028688 -4.1185427 -4.2030811 -4.2645721 -4.2955036 -4.3020368][-4.2832632 -4.2621794 -4.2363582 -4.2047062 -4.1622806 -4.0856366 -3.9871981 -3.9442868 -3.9774461 -4.0582657 -4.1467032 -4.2256131 -4.2761884 -4.2931776 -4.2914562][-4.2922139 -4.2763615 -4.2569718 -4.2343574 -4.200407 -4.1418705 -4.067225 -4.0178261 -4.0201492 -4.0816941 -4.1647196 -4.2365761 -4.2770844 -4.2851572 -4.2789669][-4.2953768 -4.2786822 -4.2651386 -4.2526546 -4.229907 -4.1858349 -4.1278133 -4.0800519 -4.0697145 -4.1159935 -4.1857319 -4.2453642 -4.2767315 -4.2791066 -4.2709222][-4.2915754 -4.2731886 -4.2657633 -4.260829 -4.2438955 -4.2078028 -4.1599994 -4.1223526 -4.1180139 -4.1552181 -4.20803 -4.2517939 -4.2738624 -4.2712603 -4.2604179]]...]
INFO - root - 2017-12-05 18:30:47.729605: step 33810, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 70h:51m:49s remains)
INFO - root - 2017-12-05 18:30:56.160444: step 33820, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 69h:42m:39s remains)
INFO - root - 2017-12-05 18:31:04.756205: step 33830, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 72h:49m:25s remains)
INFO - root - 2017-12-05 18:31:13.156199: step 33840, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 70h:05m:56s remains)
INFO - root - 2017-12-05 18:31:21.648652: step 33850, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 68h:49m:34s remains)
INFO - root - 2017-12-05 18:31:30.072179: step 33860, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 68h:11m:05s remains)
INFO - root - 2017-12-05 18:31:38.497718: step 33870, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 71h:59m:25s remains)
INFO - root - 2017-12-05 18:31:47.057884: step 33880, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 73h:09m:58s remains)
INFO - root - 2017-12-05 18:31:55.514793: step 33890, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.824 sec/batch; 68h:19m:21s remains)
INFO - root - 2017-12-05 18:32:03.999348: step 33900, loss = 2.10, batch loss = 2.05 (9.4 examples/sec; 0.847 sec/batch; 70h:14m:02s remains)
2017-12-05 18:32:04.757411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2383604 -4.234817 -4.2410951 -4.2458415 -4.251719 -4.2545638 -4.2584834 -4.2565312 -4.2467184 -4.2464848 -4.2570162 -4.2687964 -4.2740655 -4.2806969 -4.2936506][-4.1994524 -4.1937613 -4.1987696 -4.2072878 -4.2164335 -4.2240214 -4.2274542 -4.2186189 -4.2098665 -4.2119622 -4.2228808 -4.2330394 -4.2364359 -4.24422 -4.2649212][-4.1572046 -4.1456313 -4.1495662 -4.1625113 -4.1761913 -4.18656 -4.1914744 -4.1810303 -4.1739712 -4.1790552 -4.1934419 -4.2087212 -4.2166004 -4.2250147 -4.2476659][-4.1281304 -4.11126 -4.1106882 -4.1240425 -4.1357737 -4.1417832 -4.1441164 -4.1308732 -4.1230741 -4.1325755 -4.1516833 -4.1715021 -4.1873875 -4.2043285 -4.233294][-4.0940018 -4.0709071 -4.0633192 -4.06388 -4.0622997 -4.0646768 -4.0747929 -4.0688648 -4.0661159 -4.0812411 -4.1036353 -4.12825 -4.1543894 -4.1808362 -4.2145047][-4.0762672 -4.0438991 -4.0203004 -3.9967561 -3.9728198 -3.9724565 -3.9939547 -4.0009308 -4.0059872 -4.0319533 -4.0629821 -4.0950065 -4.1292834 -4.1591649 -4.197155][-4.0910721 -4.050251 -4.0179029 -3.9789183 -3.9390645 -3.9373732 -3.9599514 -3.9724579 -3.9830468 -4.0151153 -4.0498471 -4.0883141 -4.1241593 -4.1541004 -4.19423][-4.1111722 -4.0716963 -4.047565 -4.0138392 -3.973779 -3.9667296 -3.9796944 -3.9867096 -3.9947679 -4.018856 -4.048748 -4.0903907 -4.1257286 -4.1549907 -4.1946669][-4.1478362 -4.1190243 -4.107892 -4.0901794 -4.0595016 -4.0461264 -4.0454826 -4.0411019 -4.0386739 -4.0489292 -4.0701976 -4.1055236 -4.1366172 -4.1632743 -4.1986923][-4.19067 -4.1735678 -4.1706247 -4.1646056 -4.1475224 -4.1365485 -4.1323285 -4.1247787 -4.1125851 -4.10788 -4.1157217 -4.138288 -4.1638947 -4.1889358 -4.2188272][-4.2326169 -4.2256012 -4.2281742 -4.227828 -4.2195334 -4.2130914 -4.2101464 -4.2067885 -4.1940651 -4.1828246 -4.1824675 -4.1934767 -4.2105789 -4.2277188 -4.2477174][-4.2620292 -4.2612724 -4.265872 -4.2668452 -4.2632289 -4.2593827 -4.2596388 -4.2604389 -4.2518578 -4.2437286 -4.2439795 -4.2521935 -4.263279 -4.2710037 -4.2804465][-4.2724409 -4.2741966 -4.2806168 -4.2827344 -4.2817655 -4.2806125 -4.2828841 -4.2856216 -4.2812896 -4.2770438 -4.2800155 -4.2894063 -4.2986436 -4.3030052 -4.30573][-4.2737765 -4.2750072 -4.2811556 -4.2839565 -4.2845688 -4.2849064 -4.2877812 -4.2907548 -4.2890491 -4.2878885 -4.2927742 -4.3020921 -4.3097038 -4.3136706 -4.3157344][-4.2848458 -4.2856822 -4.2895823 -4.2906303 -4.2906194 -4.2914367 -4.2933178 -4.2950983 -4.2941861 -4.2940369 -4.2982688 -4.3061795 -4.3137922 -4.3186736 -4.3207893]]...]
INFO - root - 2017-12-05 18:32:13.365931: step 33910, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.876 sec/batch; 72h:41m:20s remains)
INFO - root - 2017-12-05 18:32:21.842713: step 33920, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 72h:15m:23s remains)
INFO - root - 2017-12-05 18:32:30.473578: step 33930, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.825 sec/batch; 68h:25m:32s remains)
INFO - root - 2017-12-05 18:32:38.966414: step 33940, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 73h:09m:12s remains)
INFO - root - 2017-12-05 18:32:47.605205: step 33950, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 69h:38m:27s remains)
INFO - root - 2017-12-05 18:32:56.226635: step 33960, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 70h:49m:54s remains)
INFO - root - 2017-12-05 18:33:04.790963: step 33970, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 69h:14m:23s remains)
INFO - root - 2017-12-05 18:33:13.284034: step 33980, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 72h:23m:21s remains)
INFO - root - 2017-12-05 18:33:21.786779: step 33990, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.841 sec/batch; 69h:42m:27s remains)
INFO - root - 2017-12-05 18:33:30.330350: step 34000, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 72h:01m:21s remains)
2017-12-05 18:33:31.208752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1210079 -4.1230197 -4.13043 -4.1363726 -4.1420074 -4.132596 -4.1108027 -4.0845709 -4.0690575 -4.0934815 -4.124835 -4.1314135 -4.1373563 -4.1584425 -4.2007375][-4.1256948 -4.1263313 -4.130424 -4.1361027 -4.1351814 -4.1299267 -4.1124582 -4.0876846 -4.0728931 -4.0964804 -4.1276975 -4.1341047 -4.1385975 -4.1589727 -4.1970572][-4.1211162 -4.1154022 -4.1130557 -4.116827 -4.1168752 -4.1140323 -4.097713 -4.0757394 -4.0633426 -4.0875282 -4.1172647 -4.1242466 -4.1289506 -4.1529083 -4.1922846][-4.1221061 -4.1049867 -4.0934734 -4.0942822 -4.0948005 -4.0917873 -4.0779819 -4.0619373 -4.0529518 -4.0755553 -4.1017323 -4.108315 -4.1138911 -4.1402912 -4.1832371][-4.1311045 -4.1026816 -4.0771813 -4.0647888 -4.0554762 -4.0461612 -4.04081 -4.0417161 -4.0423489 -4.06857 -4.0947185 -4.1037445 -4.1100807 -4.1332912 -4.1773381][-4.1306748 -4.1037869 -4.0700979 -4.0378108 -3.9975219 -3.9719293 -3.9774368 -4.0064883 -4.0307174 -4.0672207 -4.0969396 -4.1106567 -4.119617 -4.14418 -4.1861715][-4.131628 -4.1031523 -4.0563631 -3.9967647 -3.9247589 -3.8910089 -3.9154251 -3.9680848 -4.0123253 -4.0572886 -4.0979338 -4.1186604 -4.1347175 -4.1622133 -4.2028241][-4.133872 -4.1092081 -4.056304 -3.9824729 -3.8954768 -3.8676057 -3.9065382 -3.9558794 -3.9919009 -4.0366144 -4.0894747 -4.1215296 -4.1441736 -4.1736579 -4.2123866][-4.1396604 -4.1255455 -4.08455 -4.021708 -3.9572423 -3.9451785 -3.9697096 -3.9858854 -3.99637 -4.03493 -4.0891294 -4.1265688 -4.1528354 -4.1826282 -4.2151294][-4.147491 -4.1467223 -4.1238842 -4.0876765 -4.051671 -4.0406446 -4.0436025 -4.0418715 -4.0348186 -4.0605116 -4.106811 -4.1375594 -4.1591535 -4.1849627 -4.2137923][-4.1686182 -4.1750145 -4.1652689 -4.1470003 -4.1243343 -4.1080995 -4.1065192 -4.101954 -4.0852823 -4.09602 -4.1258621 -4.1435509 -4.1602221 -4.1813707 -4.2104058][-4.1922574 -4.2001224 -4.1964293 -4.1885777 -4.1755557 -4.1599331 -4.1548724 -4.1487136 -4.1266317 -4.1245532 -4.140439 -4.1500287 -4.1630287 -4.1817684 -4.2124228][-4.2112908 -4.2176123 -4.2178564 -4.213479 -4.2004337 -4.186069 -4.1807413 -4.1707468 -4.1464796 -4.1389346 -4.1476846 -4.1550612 -4.1687717 -4.1886482 -4.2206726][-4.2259426 -4.2271051 -4.225616 -4.2191677 -4.2027044 -4.1898489 -4.1855955 -4.1726604 -4.1505046 -4.1434083 -4.1518812 -4.1616139 -4.1770697 -4.1995778 -4.2341824][-4.2348213 -4.2333727 -4.2276855 -4.2154641 -4.1952252 -4.1832352 -4.180582 -4.1682987 -4.1476669 -4.1461244 -4.1579094 -4.1708007 -4.1859355 -4.2086468 -4.246325]]...]
INFO - root - 2017-12-05 18:33:39.938332: step 34010, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 74h:34m:47s remains)
INFO - root - 2017-12-05 18:33:48.491315: step 34020, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 70h:39m:45s remains)
INFO - root - 2017-12-05 18:33:57.047211: step 34030, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 68h:49m:06s remains)
INFO - root - 2017-12-05 18:34:05.510228: step 34040, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 71h:25m:40s remains)
INFO - root - 2017-12-05 18:34:14.236346: step 34050, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 70h:36m:17s remains)
INFO - root - 2017-12-05 18:34:22.774838: step 34060, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 71h:21m:41s remains)
INFO - root - 2017-12-05 18:34:31.404617: step 34070, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 72h:46m:06s remains)
INFO - root - 2017-12-05 18:34:39.830551: step 34080, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 72h:08m:15s remains)
INFO - root - 2017-12-05 18:34:48.495618: step 34090, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 71h:32m:53s remains)
INFO - root - 2017-12-05 18:34:57.064000: step 34100, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 70h:09m:38s remains)
2017-12-05 18:34:57.906336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.334374 -4.3349576 -4.3357215 -4.3335085 -4.3264694 -4.3158092 -4.3068976 -4.3045535 -4.3077 -4.3151541 -4.3227091 -4.3261685 -4.3256078 -4.3226862 -4.32005][-4.3396888 -4.3395934 -4.3415275 -4.3425016 -4.3407364 -4.336771 -4.333559 -4.3331966 -4.33527 -4.3398657 -4.3434091 -4.3432908 -4.339344 -4.3335309 -4.3290577][-4.34018 -4.3398967 -4.3423996 -4.3445177 -4.3454428 -4.3460536 -4.3466182 -4.3470564 -4.3477755 -4.3506336 -4.3524942 -4.3513427 -4.3452468 -4.3365932 -4.3289685][-4.3355737 -4.3351741 -4.3363109 -4.3361263 -4.336657 -4.3385911 -4.3381424 -4.3357272 -4.3345313 -4.33828 -4.3430405 -4.3457522 -4.3415236 -4.3313289 -4.3199072][-4.3251963 -4.3231864 -4.3194747 -4.3110018 -4.30374 -4.3001256 -4.2933879 -4.284719 -4.2829094 -4.2922611 -4.3073244 -4.3219833 -4.3264618 -4.3183742 -4.3043184][-4.3096786 -4.3034382 -4.2897816 -4.2652931 -4.2385421 -4.2178164 -4.1970034 -4.1785913 -4.1794682 -4.2044482 -4.2421312 -4.2776785 -4.2967954 -4.2963257 -4.2822881][-4.2946973 -4.2808661 -4.2523847 -4.20389 -4.1478491 -4.0974784 -4.050827 -4.0201921 -4.0325732 -4.0859447 -4.1552606 -4.2127533 -4.2477789 -4.2597351 -4.2510295][-4.284111 -4.2629962 -4.2209392 -4.1511593 -4.0653367 -3.9803009 -3.9008112 -3.8526402 -3.8743513 -3.9553442 -4.0554047 -4.1346717 -4.1888003 -4.2174549 -4.2225018][-4.2726994 -4.2511511 -4.2090721 -4.1393437 -4.0495076 -3.9560869 -3.868372 -3.818831 -3.8442569 -3.9295645 -4.0332923 -4.1157527 -4.1757035 -4.2108793 -4.2234082][-4.2598677 -4.24632 -4.2197104 -4.1729846 -4.1084962 -4.0422626 -3.9845281 -3.9603782 -3.9858878 -4.0493469 -4.1253104 -4.185533 -4.2265639 -4.2484226 -4.2514057][-4.2573776 -4.2570777 -4.2496 -4.2279005 -4.1935015 -4.1580157 -4.1304865 -4.1258855 -4.1489143 -4.19176 -4.2405624 -4.2762218 -4.2942653 -4.2957735 -4.2812634][-4.2706456 -4.2776132 -4.2803817 -4.2743549 -4.2620721 -4.2507753 -4.2433186 -4.2477651 -4.2657218 -4.2917652 -4.3181124 -4.3357334 -4.3392868 -4.329546 -4.3072371][-4.2935352 -4.3009276 -4.3062382 -4.3069439 -4.3065248 -4.3077931 -4.3096461 -4.3146243 -4.3247342 -4.337101 -4.3486481 -4.3555532 -4.3539944 -4.3443069 -4.32675][-4.3167925 -4.3229542 -4.3273087 -4.3293571 -4.3330421 -4.3370519 -4.3385143 -4.3390856 -4.3421874 -4.3464479 -4.3498859 -4.35065 -4.3483486 -4.3433046 -4.3345084][-4.3324175 -4.3370008 -4.3401532 -4.3417835 -4.3447313 -4.3468723 -4.3457623 -4.34324 -4.341095 -4.3395147 -4.3384953 -4.3367195 -4.3357077 -4.3348789 -4.3336186]]...]
INFO - root - 2017-12-05 18:35:06.324943: step 34110, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 68h:21m:08s remains)
INFO - root - 2017-12-05 18:35:14.844065: step 34120, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 70h:08m:16s remains)
INFO - root - 2017-12-05 18:35:23.306283: step 34130, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 71h:59m:46s remains)
INFO - root - 2017-12-05 18:35:31.868226: step 34140, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 72h:35m:10s remains)
INFO - root - 2017-12-05 18:35:40.485467: step 34150, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 70h:40m:53s remains)
INFO - root - 2017-12-05 18:35:49.045496: step 34160, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 70h:27m:34s remains)
INFO - root - 2017-12-05 18:35:57.687016: step 34170, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 70h:50m:45s remains)
INFO - root - 2017-12-05 18:36:06.122691: step 34180, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 67h:56m:49s remains)
INFO - root - 2017-12-05 18:36:14.699601: step 34190, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 71h:26m:05s remains)
INFO - root - 2017-12-05 18:36:23.253193: step 34200, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 69h:12m:57s remains)
2017-12-05 18:36:23.976858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.17083 -4.1967926 -4.2285228 -4.2424488 -4.245676 -4.2441077 -4.2330022 -4.2211733 -4.2221379 -4.222332 -4.2153416 -4.2109952 -4.2098641 -4.2037029 -4.1960216][-4.1585245 -4.1867056 -4.2141109 -4.2223296 -4.2246165 -4.2294278 -4.2237496 -4.216001 -4.2172575 -4.2206106 -4.214674 -4.21146 -4.2102308 -4.2011647 -4.1929665][-4.1425934 -4.1685772 -4.1889944 -4.1929588 -4.1942549 -4.2024307 -4.2030306 -4.1977186 -4.1962523 -4.1964335 -4.194066 -4.1951432 -4.1995931 -4.1933804 -4.184886][-4.1308656 -4.1582594 -4.1750469 -4.175684 -4.1715212 -4.1745791 -4.1744957 -4.1705394 -4.1664515 -4.1612854 -4.1610036 -4.1674771 -4.1807051 -4.1818876 -4.1777706][-4.1129651 -4.1422873 -4.1586366 -4.1564732 -4.1436634 -4.1322427 -4.1201868 -4.1129951 -4.112886 -4.1152368 -4.1244397 -4.1405492 -4.1631689 -4.1727896 -4.1724548][-4.0793591 -4.1031842 -4.1179152 -4.1129165 -4.0908508 -4.0604005 -4.0295997 -4.019825 -4.0318279 -4.0559549 -4.0863371 -4.1159582 -4.1478581 -4.1609488 -4.1613483][-4.0506296 -4.0653272 -4.0761757 -4.0624552 -4.031508 -3.9890332 -3.9443853 -3.931005 -3.9541326 -4.0002184 -4.0521979 -4.0972414 -4.1348872 -4.1499562 -4.148828][-4.0578322 -4.0602961 -4.0631156 -4.0417271 -4.0066061 -3.9595456 -3.9013619 -3.8770721 -3.9036562 -3.9668939 -4.0368719 -4.0904517 -4.1258717 -4.1411328 -4.1409411][-4.1004519 -4.0884261 -4.0849204 -4.063611 -4.0329432 -3.9930594 -3.9369354 -3.9103348 -3.9297848 -3.9883742 -4.0529051 -4.0976377 -4.1207891 -4.1316648 -4.1331735][-4.1444693 -4.1277 -4.127152 -4.1153688 -4.0948868 -4.0689273 -4.0290279 -4.007411 -4.0157633 -4.0537567 -4.0963192 -4.1232219 -4.1297545 -4.1282024 -4.1286693][-4.1631761 -4.1539745 -4.1631656 -4.1641703 -4.1539083 -4.1398015 -4.1151967 -4.0981631 -4.0990105 -4.1207991 -4.1481476 -4.160213 -4.1527867 -4.137702 -4.1346893][-4.160449 -4.1569719 -4.1726265 -4.1829476 -4.1826777 -4.1786923 -4.16508 -4.1507878 -4.1489897 -4.1639457 -4.1858091 -4.1927409 -4.1787295 -4.1573534 -4.15252][-4.1549568 -4.1513691 -4.1635513 -4.1743774 -4.1813364 -4.1896024 -4.1892939 -4.1812224 -4.1781049 -4.1903224 -4.2061491 -4.2073441 -4.1907306 -4.1708803 -4.1680222][-4.1620679 -4.1559176 -4.1597848 -4.1645403 -4.1721349 -4.1867595 -4.1991997 -4.2026229 -4.2014117 -4.2071576 -4.21142 -4.2006721 -4.1805363 -4.1640234 -4.1651893][-4.1806216 -4.170958 -4.167798 -4.1702323 -4.1778512 -4.1934133 -4.2114129 -4.2228041 -4.2231951 -4.2196145 -4.2087073 -4.1879845 -4.1616626 -4.1453876 -4.145277]]...]
INFO - root - 2017-12-05 18:36:32.629146: step 34210, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 70h:14m:27s remains)
INFO - root - 2017-12-05 18:36:41.149920: step 34220, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 69h:10m:26s remains)
INFO - root - 2017-12-05 18:36:49.789723: step 34230, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 73h:30m:34s remains)
INFO - root - 2017-12-05 18:36:58.234811: step 34240, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 68h:13m:39s remains)
INFO - root - 2017-12-05 18:37:06.851712: step 34250, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 70h:50m:06s remains)
INFO - root - 2017-12-05 18:37:15.499898: step 34260, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 71h:05m:56s remains)
INFO - root - 2017-12-05 18:37:24.013318: step 34270, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 70h:05m:03s remains)
INFO - root - 2017-12-05 18:37:32.602176: step 34280, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 70h:51m:44s remains)
INFO - root - 2017-12-05 18:37:41.180673: step 34290, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 70h:35m:48s remains)
INFO - root - 2017-12-05 18:37:49.716226: step 34300, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 73h:02m:10s remains)
2017-12-05 18:37:50.534768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2847004 -4.2796197 -4.2632256 -4.2430687 -4.2328248 -4.2406893 -4.2637792 -4.2907357 -4.3096681 -4.3140631 -4.3020964 -4.2896495 -4.2875023 -4.291862 -4.2963953][-4.274086 -4.2556539 -4.2215195 -4.188148 -4.17202 -4.1795564 -4.2100868 -4.2500324 -4.282671 -4.3007469 -4.2945552 -4.2824941 -4.278872 -4.2833719 -4.2914114][-4.24292 -4.2121286 -4.1643734 -4.1186609 -4.0959253 -4.1023726 -4.1376433 -4.1877441 -4.2342982 -4.26982 -4.2756205 -4.269156 -4.269392 -4.2759528 -4.2873826][-4.2038779 -4.1720524 -4.1256647 -4.0803246 -4.06035 -4.0676904 -4.1008191 -4.1457767 -4.1913271 -4.2331743 -4.2498655 -4.2526479 -4.260324 -4.2701397 -4.2823429][-4.176692 -4.15416 -4.1224318 -4.0893197 -4.0780478 -4.0840006 -4.1017756 -4.1247978 -4.1547246 -4.1935511 -4.2186313 -4.2324805 -4.2495842 -4.2651396 -4.278038][-4.1698828 -4.1530538 -4.1328182 -4.1074958 -4.0956798 -4.0950856 -4.0963488 -4.0974483 -4.1128745 -4.147779 -4.1778464 -4.2014475 -4.22843 -4.2519693 -4.268116][-4.1867404 -4.1648312 -4.1445956 -4.1218386 -4.1065392 -4.0953226 -4.0827651 -4.0707922 -4.0815368 -4.1102481 -4.1372576 -4.1640267 -4.196866 -4.2277951 -4.2467346][-4.218996 -4.1876173 -4.1619558 -4.1403146 -4.1223917 -4.1034269 -4.0794935 -4.0599275 -4.0678358 -4.0872846 -4.1044827 -4.1282372 -4.1586986 -4.1868467 -4.2058744][-4.2514882 -4.2112837 -4.1812987 -4.1629486 -4.1453953 -4.1279984 -4.1044488 -4.0810857 -4.0766268 -4.0826488 -4.08668 -4.1000242 -4.11513 -4.1263542 -4.1390691][-4.2702184 -4.2252836 -4.1930819 -4.179667 -4.1685948 -4.1622834 -4.1487007 -4.1244936 -4.1061692 -4.0941925 -4.0774875 -4.0676956 -4.0576982 -4.0537362 -4.0659628][-4.2762942 -4.23105 -4.2000332 -4.1925611 -4.1886039 -4.1922712 -4.1909747 -4.1720719 -4.1470032 -4.1188378 -4.0826197 -4.0486703 -4.0198503 -4.0140777 -4.0348577][-4.2811947 -4.2416663 -4.2154 -4.210928 -4.2121019 -4.2221332 -4.2275052 -4.2131071 -4.1861534 -4.1512375 -4.108892 -4.0692477 -4.0398779 -4.040451 -4.0700321][-4.2905126 -4.2602172 -4.2415128 -4.2400885 -4.2427812 -4.2511487 -4.2549863 -4.2430797 -4.2218819 -4.192708 -4.1565657 -4.1207142 -4.095078 -4.1019592 -4.1369667][-4.2954717 -4.2739582 -4.2615423 -4.2616854 -4.2631183 -4.2654953 -4.2641854 -4.2554355 -4.244884 -4.2307124 -4.2097168 -4.1859784 -4.1698985 -4.1820345 -4.2152233][-4.2892423 -4.2735348 -4.2644148 -4.26351 -4.2614875 -4.2558918 -4.2483959 -4.243959 -4.2456503 -4.2503967 -4.2518606 -4.2487097 -4.2496037 -4.265295 -4.292181]]...]
INFO - root - 2017-12-05 18:37:58.926170: step 34310, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.826 sec/batch; 68h:27m:16s remains)
INFO - root - 2017-12-05 18:38:07.520985: step 34320, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 71h:09m:08s remains)
INFO - root - 2017-12-05 18:38:15.866290: step 34330, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 71h:41m:32s remains)
INFO - root - 2017-12-05 18:38:24.213313: step 34340, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 70h:02m:25s remains)
INFO - root - 2017-12-05 18:38:32.650619: step 34350, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 73h:42m:11s remains)
INFO - root - 2017-12-05 18:38:41.108733: step 34360, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 69h:56m:04s remains)
INFO - root - 2017-12-05 18:38:49.697568: step 34370, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 69h:16m:10s remains)
INFO - root - 2017-12-05 18:38:58.326051: step 34380, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 71h:53m:14s remains)
INFO - root - 2017-12-05 18:39:06.753558: step 34390, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 70h:01m:37s remains)
INFO - root - 2017-12-05 18:39:15.107035: step 34400, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 69h:55m:01s remains)
2017-12-05 18:39:15.904578: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3156385 -4.3205872 -4.3258724 -4.3278909 -4.3259878 -4.3231955 -4.320262 -4.3189278 -4.3201709 -4.3243117 -4.3273129 -4.3279338 -4.325881 -4.3250661 -4.3303337][-4.2923031 -4.297945 -4.3056879 -4.3101783 -4.307404 -4.3044038 -4.2995915 -4.2954121 -4.2947822 -4.2992153 -4.302793 -4.3045516 -4.3047638 -4.3058925 -4.3122106][-4.2568436 -4.261786 -4.2740717 -4.2813158 -4.2758417 -4.2724195 -4.2651334 -4.2554955 -4.2523642 -4.2570353 -4.2623887 -4.2669663 -4.2723837 -4.2775407 -4.2865415][-4.2137971 -4.2169075 -4.2329922 -4.242291 -4.2339091 -4.2286768 -4.2177896 -4.1997175 -4.1941886 -4.2044187 -4.2163582 -4.2301321 -4.2437325 -4.2536473 -4.2654905][-4.1767559 -4.1747308 -4.1875176 -4.1934586 -4.1808429 -4.174264 -4.1562796 -4.1256447 -4.1225119 -4.1483197 -4.1734848 -4.1986842 -4.2215133 -4.2380624 -4.2524419][-4.1356764 -4.1165009 -4.1176844 -4.1196451 -4.1055317 -4.0961781 -4.0642748 -4.0117097 -4.01389 -4.0683689 -4.115881 -4.1560121 -4.1891532 -4.2140722 -4.2364378][-4.0891156 -4.0496192 -4.033534 -4.0287452 -4.0102081 -3.9957213 -3.942878 -3.8563423 -3.8683662 -3.9658964 -4.0487461 -4.1114988 -4.1603713 -4.1921744 -4.2207088][-4.0612469 -4.0099988 -3.9871719 -3.98141 -3.963706 -3.9441791 -3.8720398 -3.7571573 -3.7754319 -3.9012871 -4.0091405 -4.0900545 -4.1533771 -4.1886721 -4.2170763][-4.0513597 -4.00849 -3.9962366 -4.0030131 -4.0005031 -3.9965196 -3.9437881 -3.8512435 -3.8623338 -3.9586141 -4.0451694 -4.1158862 -4.1729932 -4.2007294 -4.2242494][-4.0744944 -4.0461969 -4.0460019 -4.0606575 -4.0690188 -4.079196 -4.0545974 -3.9996712 -4.0040851 -4.0609107 -4.1155968 -4.1636796 -4.2012868 -4.2174277 -4.236608][-4.123127 -4.1087551 -4.1152015 -4.1303334 -4.1410565 -4.1546836 -4.1482577 -4.1215863 -4.1240873 -4.1555023 -4.18571 -4.2116246 -4.2321396 -4.2397175 -4.2545209][-4.1971431 -4.1935844 -4.200655 -4.2095823 -4.2152495 -4.2235203 -4.2216549 -4.2096105 -4.2139883 -4.2336287 -4.2505426 -4.2614665 -4.2724161 -4.2760878 -4.2842231][-4.2636876 -4.2635269 -4.2677245 -4.2722712 -4.2734985 -4.2755275 -4.2735777 -4.269268 -4.2762713 -4.2908354 -4.3015461 -4.306356 -4.313827 -4.3167667 -4.3188829][-4.3042336 -4.3067293 -4.3114262 -4.31557 -4.31667 -4.3166413 -4.3156209 -4.315661 -4.322382 -4.3324175 -4.338757 -4.3407145 -4.345891 -4.3474221 -4.3452735][-4.329917 -4.3326178 -4.3362656 -4.3388953 -4.3394823 -4.339148 -4.3383689 -4.3387232 -4.343658 -4.3499856 -4.3543496 -4.3561711 -4.3600969 -4.361105 -4.3579974]]...]
INFO - root - 2017-12-05 18:39:24.375459: step 34410, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 69h:55m:29s remains)
INFO - root - 2017-12-05 18:39:32.901548: step 34420, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.813 sec/batch; 67h:20m:20s remains)
INFO - root - 2017-12-05 18:39:41.323680: step 34430, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 69h:46m:03s remains)
INFO - root - 2017-12-05 18:39:49.659771: step 34440, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.814 sec/batch; 67h:22m:32s remains)
INFO - root - 2017-12-05 18:39:58.167860: step 34450, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 69h:39m:45s remains)
INFO - root - 2017-12-05 18:40:06.614333: step 34460, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 70h:26m:50s remains)
INFO - root - 2017-12-05 18:40:15.023502: step 34470, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 0.807 sec/batch; 66h:49m:23s remains)
INFO - root - 2017-12-05 18:40:23.468570: step 34480, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 69h:39m:08s remains)
INFO - root - 2017-12-05 18:40:31.864538: step 34490, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 72h:06m:17s remains)
INFO - root - 2017-12-05 18:40:40.361899: step 34500, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 68h:13m:55s remains)
2017-12-05 18:40:41.155637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1818128 -4.1852045 -4.185205 -4.187789 -4.1916342 -4.1940045 -4.1997123 -4.209816 -4.2213845 -4.218153 -4.1953659 -4.1578479 -4.1247783 -4.1085515 -4.1249204][-4.2027049 -4.2047191 -4.2047071 -4.2060084 -4.2072988 -4.2077427 -4.21275 -4.2208238 -4.2275705 -4.2203312 -4.1946487 -4.1546326 -4.1207194 -4.1105623 -4.1369061][-4.230722 -4.2301803 -4.2287579 -4.2284074 -4.2287817 -4.2293134 -4.23377 -4.2394876 -4.2426796 -4.2361774 -4.21522 -4.1827922 -4.1576433 -4.1550097 -4.1820259][-4.2551413 -4.2508888 -4.2478876 -4.2473688 -4.2482114 -4.2492743 -4.2520404 -4.2554479 -4.2574167 -4.2550044 -4.2445431 -4.2281723 -4.2177653 -4.2198482 -4.2383828][-4.2681236 -4.2626266 -4.2605348 -4.2604423 -4.260354 -4.2579927 -4.2544088 -4.2506075 -4.2491732 -4.2523308 -4.2562747 -4.2599254 -4.2648277 -4.270927 -4.2806997][-4.2699232 -4.2653308 -4.2641768 -4.2629294 -4.2583542 -4.2462788 -4.2289953 -4.2105594 -4.2020473 -4.2117286 -4.2331314 -4.2593279 -4.2795596 -4.290132 -4.2941546][-4.2590017 -4.2542229 -4.2511315 -4.2456412 -4.2318983 -4.2039661 -4.1658978 -4.1273756 -4.1121354 -4.132936 -4.1779537 -4.2283945 -4.2640629 -4.2803583 -4.2823853][-4.2410712 -4.2337832 -4.2251911 -4.2114844 -4.1866875 -4.1425791 -4.0845671 -4.0298924 -4.0131564 -4.0523324 -4.1247277 -4.1960282 -4.2426105 -4.26277 -4.264545][-4.2174311 -4.206984 -4.1925888 -4.1715188 -4.1401739 -4.0887303 -4.0220842 -3.962116 -3.9523807 -4.0120816 -4.10322 -4.1841655 -4.2342606 -4.253839 -4.2536902][-4.1909308 -4.1808376 -4.1668038 -4.146914 -4.1216421 -4.0800891 -4.025394 -3.9774392 -3.97676 -4.0392752 -4.1253929 -4.1983495 -4.2410607 -4.2546906 -4.2502518][-4.1721582 -4.1690073 -4.1635695 -4.1553364 -4.1451221 -4.1227746 -4.0886641 -4.0597095 -4.0646124 -4.1149054 -4.1797762 -4.2320294 -4.2597394 -4.263082 -4.2529535][-4.1613741 -4.166091 -4.1705875 -4.1739511 -4.1772504 -4.1698756 -4.154685 -4.1450677 -4.1564879 -4.1950779 -4.2382751 -4.2682624 -4.2778678 -4.2699213 -4.2551627][-4.1710029 -4.1797314 -4.1883912 -4.1964488 -4.2052484 -4.207098 -4.2077532 -4.2143664 -4.2304435 -4.258069 -4.2822895 -4.2913036 -4.2843523 -4.2683277 -4.2525368][-4.1996551 -4.2059336 -4.2115169 -4.2169442 -4.2253819 -4.2325158 -4.2447758 -4.2615609 -4.2784615 -4.2950034 -4.3029509 -4.2972469 -4.2813931 -4.2631087 -4.2493706][-4.231595 -4.2318993 -4.2313371 -4.2316918 -4.2364745 -4.2437234 -4.2590041 -4.2773147 -4.2914848 -4.2998366 -4.2991004 -4.289207 -4.2744746 -4.25989 -4.2501955]]...]
INFO - root - 2017-12-05 18:40:49.368236: step 34510, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 69h:22m:47s remains)
INFO - root - 2017-12-05 18:40:57.721751: step 34520, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.830 sec/batch; 68h:42m:52s remains)
INFO - root - 2017-12-05 18:41:06.281447: step 34530, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 70h:14m:34s remains)
INFO - root - 2017-12-05 18:41:14.717382: step 34540, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 68h:18m:55s remains)
INFO - root - 2017-12-05 18:41:23.021817: step 34550, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 69h:54m:12s remains)
INFO - root - 2017-12-05 18:41:31.606157: step 34560, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 70h:32m:10s remains)
INFO - root - 2017-12-05 18:41:40.092147: step 34570, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 68h:52m:29s remains)
INFO - root - 2017-12-05 18:41:48.605934: step 34580, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 71h:36m:30s remains)
INFO - root - 2017-12-05 18:41:57.104917: step 34590, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 72h:22m:40s remains)
INFO - root - 2017-12-05 18:42:05.731826: step 34600, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 68h:03m:58s remains)
2017-12-05 18:42:06.512373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2499323 -4.2580104 -4.2644396 -4.2635889 -4.2584696 -4.2497516 -4.2442193 -4.2479444 -4.2593913 -4.2714796 -4.2768459 -4.2716842 -4.2615147 -4.2515011 -4.2444434][-4.2605658 -4.27064 -4.2740974 -4.2675171 -4.2576962 -4.245698 -4.239552 -4.2463918 -4.2596011 -4.2730684 -4.2799048 -4.2806911 -4.2800655 -4.2705646 -4.2547235][-4.2797718 -4.2880087 -4.2822218 -4.2709632 -4.2608986 -4.2463808 -4.2349544 -4.2371745 -4.2499609 -4.266005 -4.2765803 -4.2835007 -4.286479 -4.2719746 -4.2439904][-4.298686 -4.3027539 -4.2924433 -4.280787 -4.2689209 -4.2476563 -4.2255054 -4.2163095 -4.2262583 -4.2428765 -4.2540379 -4.2656217 -4.2707219 -4.248261 -4.2030196][-4.3110681 -4.3130159 -4.2997561 -4.2831869 -4.261899 -4.2259274 -4.1845489 -4.16038 -4.1679645 -4.1896172 -4.2062974 -4.227037 -4.2402649 -4.2128468 -4.1524696][-4.3184004 -4.3139977 -4.2922564 -4.2628365 -4.2239408 -4.1645732 -4.0958881 -4.0529027 -4.06396 -4.1066904 -4.1446519 -4.1868234 -4.2164574 -4.1969709 -4.1462765][-4.3178744 -4.3079 -4.275846 -4.233295 -4.1725955 -4.0798731 -3.9745085 -3.9123471 -3.9436009 -4.0290356 -4.1041017 -4.1667066 -4.2090011 -4.2061281 -4.179728][-4.2998409 -4.2886147 -4.2527537 -4.204391 -4.1305008 -4.0163069 -3.8929379 -3.8313603 -3.893487 -4.0171142 -4.1123819 -4.1736875 -4.2127819 -4.2182159 -4.21276][-4.2664962 -4.2598524 -4.23549 -4.1963906 -4.1332636 -4.0356193 -3.939369 -3.8976185 -3.9591625 -4.0673065 -4.1454086 -4.1898565 -4.216733 -4.220696 -4.2238951][-4.215385 -4.2166586 -4.2119217 -4.19708 -4.1643996 -4.1049252 -4.048799 -4.0226927 -4.0580893 -4.1289549 -4.1824012 -4.2115836 -4.2239833 -4.2195258 -4.2219892][-4.1703916 -4.1754317 -4.1843739 -4.1893139 -4.1844664 -4.1603084 -4.1321568 -4.1168075 -4.133328 -4.1754169 -4.2123022 -4.2312288 -4.2334204 -4.2276888 -4.2306929][-4.1533337 -4.1487784 -4.1564054 -4.1712208 -4.18627 -4.1901507 -4.1837554 -4.1776838 -4.1855307 -4.2100234 -4.2377887 -4.2521782 -4.2546635 -4.2547832 -4.2605915][-4.1564007 -4.1344838 -4.1343389 -4.1511397 -4.179348 -4.2058926 -4.2232509 -4.229599 -4.235671 -4.2512488 -4.2692375 -4.2789559 -4.2806492 -4.2828217 -4.2872205][-4.184474 -4.1579623 -4.1531115 -4.1673441 -4.1963086 -4.2308826 -4.257143 -4.2688484 -4.2754006 -4.283515 -4.2946181 -4.301991 -4.3007588 -4.2983346 -4.2988329][-4.2300673 -4.2129354 -4.2085586 -4.2153382 -4.2330041 -4.2617097 -4.2872543 -4.3011537 -4.3065853 -4.3070993 -4.3110504 -4.3153973 -4.3126621 -4.306881 -4.3039727]]...]
INFO - root - 2017-12-05 18:42:15.055515: step 34610, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 70h:56m:30s remains)
INFO - root - 2017-12-05 18:42:23.663922: step 34620, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 71h:11m:09s remains)
INFO - root - 2017-12-05 18:42:32.243967: step 34630, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 71h:49m:39s remains)
INFO - root - 2017-12-05 18:42:40.639467: step 34640, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 70h:33m:11s remains)
INFO - root - 2017-12-05 18:42:49.287411: step 34650, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 71h:30m:15s remains)
INFO - root - 2017-12-05 18:42:57.865930: step 34660, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 72h:04m:25s remains)
INFO - root - 2017-12-05 18:43:06.295892: step 34670, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.798 sec/batch; 66h:03m:25s remains)
INFO - root - 2017-12-05 18:43:14.976261: step 34680, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 71h:34m:08s remains)
INFO - root - 2017-12-05 18:43:23.460287: step 34690, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 70h:54m:58s remains)
INFO - root - 2017-12-05 18:43:32.057879: step 34700, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 72h:23m:32s remains)
2017-12-05 18:43:32.797203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.318749 -4.316565 -4.3150344 -4.3148131 -4.3156567 -4.3169885 -4.3191919 -4.3218532 -4.3232884 -4.3223968 -4.3206768 -4.3197656 -4.320682 -4.3227515 -4.3252444][-4.318 -4.3144221 -4.3119655 -4.3115945 -4.3116789 -4.3115778 -4.312952 -4.3155317 -4.3175268 -4.3169336 -4.3145275 -4.3131833 -4.3143244 -4.3180432 -4.3225121][-4.3118429 -4.3051853 -4.3005123 -4.2998543 -4.2999487 -4.2988305 -4.2999678 -4.302876 -4.3049784 -4.3035655 -4.2995553 -4.2979126 -4.2997212 -4.3057 -4.3127561][-4.2991624 -4.2871218 -4.27764 -4.2751651 -4.2747622 -4.2714729 -4.2700157 -4.271481 -4.2741542 -4.2734766 -4.2694979 -4.2682204 -4.2721987 -4.2832651 -4.29581][-4.2752867 -4.2558761 -4.2390623 -4.2313342 -4.2256465 -4.2128363 -4.2015357 -4.2008781 -4.2108908 -4.2183132 -4.218286 -4.2194147 -4.227757 -4.247015 -4.2679563][-4.24274 -4.2169538 -4.1940384 -4.1806831 -4.1641498 -4.1330075 -4.0998335 -4.0921936 -4.1138754 -4.1357059 -4.1452603 -4.152297 -4.1666222 -4.1947808 -4.2267394][-4.2106504 -4.1816349 -4.1567678 -4.1390481 -4.1108413 -4.0593219 -4.0003347 -3.9806442 -4.0144563 -4.0562048 -4.0818 -4.0997806 -4.1213713 -4.1547213 -4.1939631][-4.2013917 -4.1731377 -4.1484127 -4.127902 -4.0949941 -4.0378366 -3.9731939 -3.9473135 -3.9818456 -4.0321746 -4.0714512 -4.1007743 -4.1269479 -4.1582193 -4.1925159][-4.2153277 -4.1954479 -4.1755295 -4.1587667 -4.1327147 -4.0911894 -4.0482903 -4.0309339 -4.0520844 -4.0890775 -4.1242962 -4.1526175 -4.1741266 -4.1958461 -4.216588][-4.2323551 -4.2253742 -4.2171578 -4.211031 -4.1991272 -4.1779518 -4.1586494 -4.1529832 -4.1643229 -4.1828365 -4.20243 -4.215632 -4.2219052 -4.2287674 -4.2352366][-4.235384 -4.2368226 -4.2374148 -4.238595 -4.2379293 -4.2341013 -4.2330403 -4.2332244 -4.2352314 -4.2379951 -4.2422719 -4.2417536 -4.2349443 -4.2292337 -4.2273712][-4.2167773 -4.221436 -4.2240963 -4.225204 -4.2258334 -4.2309046 -4.24017 -4.2414684 -4.2374396 -4.234282 -4.2347145 -4.2321167 -4.2239785 -4.2171288 -4.2145529][-4.20208 -4.2064042 -4.2061086 -4.2007828 -4.1962967 -4.2016144 -4.2148161 -4.2165504 -4.2122869 -4.2118316 -4.2149878 -4.2160215 -4.2139597 -4.2134867 -4.2145576][-4.2126608 -4.2166195 -4.2122583 -4.2008314 -4.1931005 -4.1967611 -4.2071686 -4.2052884 -4.2003474 -4.2019305 -4.2078052 -4.213294 -4.2189112 -4.2258716 -4.2309432][-4.2341871 -4.2372689 -4.2297506 -4.2148719 -4.2068472 -4.2102242 -4.21814 -4.2144585 -4.2094398 -4.21079 -4.2185168 -4.2291217 -4.2419181 -4.2543888 -4.260448]]...]
INFO - root - 2017-12-05 18:43:41.272956: step 34710, loss = 2.09, batch loss = 2.04 (9.9 examples/sec; 0.808 sec/batch; 66h:47m:49s remains)
INFO - root - 2017-12-05 18:43:49.674481: step 34720, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 71h:24m:50s remains)
INFO - root - 2017-12-05 18:43:58.224156: step 34730, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 70h:32m:18s remains)
INFO - root - 2017-12-05 18:44:06.624627: step 34740, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 70h:48m:52s remains)
INFO - root - 2017-12-05 18:44:15.112483: step 34750, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 70h:22m:54s remains)
INFO - root - 2017-12-05 18:44:23.617994: step 34760, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 69h:03m:15s remains)
INFO - root - 2017-12-05 18:44:32.028265: step 34770, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 71h:22m:33s remains)
INFO - root - 2017-12-05 18:44:40.579224: step 34780, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 70h:55m:38s remains)
INFO - root - 2017-12-05 18:44:49.193928: step 34790, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 71h:49m:18s remains)
INFO - root - 2017-12-05 18:44:57.748972: step 34800, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 73h:35m:32s remains)
2017-12-05 18:44:58.544150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0991325 -4.1070933 -4.0922174 -4.0705338 -4.0630007 -4.0703082 -4.0920591 -4.1076927 -4.1024184 -4.1084008 -4.1242647 -4.1455307 -4.1592231 -4.1714239 -4.1654596][-4.0982442 -4.1048203 -4.0914936 -4.0723233 -4.0633645 -4.0617933 -4.0768275 -4.0961018 -4.1021061 -4.1187406 -4.1376424 -4.151072 -4.1543865 -4.1590652 -4.151011][-4.1322427 -4.1347704 -4.1217046 -4.1064529 -4.093574 -4.0763931 -4.0757203 -4.0843558 -4.0907116 -4.1155748 -4.1454253 -4.1627359 -4.1636968 -4.1652303 -4.1604714][-4.1618347 -4.1661115 -4.1557631 -4.1407657 -4.1194248 -4.0888071 -4.0717854 -4.0671029 -4.0725036 -4.1033745 -4.1432891 -4.1681933 -4.171968 -4.176847 -4.17386][-4.1614223 -4.1693258 -4.1615782 -4.143466 -4.1163068 -4.078527 -4.0449567 -4.0294085 -4.0466228 -4.0930138 -4.1378407 -4.1634212 -4.1699719 -4.1787367 -4.1758246][-4.1429749 -4.1447997 -4.134789 -4.1151638 -4.0812197 -4.0264797 -3.9657872 -3.9424171 -3.9908695 -4.0672393 -4.1217823 -4.1519589 -4.1674318 -4.1835208 -4.1814151][-4.1262007 -4.1125693 -4.0942988 -4.0728602 -4.0278535 -3.9419632 -3.846261 -3.8323154 -3.9296153 -4.03464 -4.0969896 -4.1366181 -4.16504 -4.1894789 -4.191999][-4.1281691 -4.0991964 -4.0757923 -4.0609426 -4.0164266 -3.9250083 -3.8323681 -3.8404131 -3.9432602 -4.0384841 -4.0943613 -4.1373377 -4.1717958 -4.1985412 -4.2055869][-4.1420674 -4.1047053 -4.0867805 -4.0854349 -4.0560589 -3.993746 -3.9419956 -3.9547524 -4.0151772 -4.07432 -4.1177039 -4.1575212 -4.1917377 -4.2159681 -4.2196341][-4.1401734 -4.10022 -4.0903215 -4.100709 -4.089148 -4.0573177 -4.0348949 -4.0401187 -4.0635 -4.097199 -4.1355667 -4.1752305 -4.2081151 -4.2267737 -4.22609][-4.1447458 -4.1050963 -4.0988412 -4.1126232 -4.1106067 -4.0960484 -4.0869088 -4.0853391 -4.0899363 -4.1113858 -4.1477423 -4.1844468 -4.2126284 -4.2255483 -4.2232494][-4.1664839 -4.1287894 -4.1160388 -4.1211586 -4.1236582 -4.1194906 -4.1176419 -4.1150784 -4.1153359 -4.1319256 -4.1619081 -4.19207 -4.2118669 -4.2187452 -4.2203808][-4.1940064 -4.1595283 -4.13866 -4.1315036 -4.1329041 -4.1356544 -4.1407609 -4.1437216 -4.1465921 -4.1597209 -4.1805015 -4.2016692 -4.2136569 -4.2183867 -4.2237215][-4.2218547 -4.1952167 -4.1737294 -4.1595016 -4.1572018 -4.1631737 -4.1725559 -4.1783719 -4.1822958 -4.191534 -4.2043357 -4.2159352 -4.2229371 -4.2273889 -4.2325883][-4.2403507 -4.2229671 -4.2076483 -4.1961932 -4.1939178 -4.1999044 -4.2073889 -4.2100787 -4.2113943 -4.2151208 -4.2213631 -4.2276659 -4.2334642 -4.2385054 -4.2431273]]...]
INFO - root - 2017-12-05 18:45:07.066392: step 34810, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 71h:50m:32s remains)
INFO - root - 2017-12-05 18:45:15.646263: step 34820, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 71h:23m:56s remains)
INFO - root - 2017-12-05 18:45:24.101858: step 34830, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 71h:41m:11s remains)
INFO - root - 2017-12-05 18:45:32.578965: step 34840, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.854 sec/batch; 70h:35m:47s remains)
INFO - root - 2017-12-05 18:45:41.217000: step 34850, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 70h:14m:03s remains)
INFO - root - 2017-12-05 18:45:49.888486: step 34860, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 70h:55m:54s remains)
INFO - root - 2017-12-05 18:45:58.383485: step 34870, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 71h:01m:08s remains)
INFO - root - 2017-12-05 18:46:06.912511: step 34880, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 70h:30m:25s remains)
INFO - root - 2017-12-05 18:46:15.513788: step 34890, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 71h:11m:41s remains)
INFO - root - 2017-12-05 18:46:24.092607: step 34900, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 70h:26m:24s remains)
2017-12-05 18:46:24.843270: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1448545 -4.125432 -4.1096315 -4.1239843 -4.1741457 -4.2113695 -4.2181716 -4.19139 -4.1335936 -4.0940142 -4.10968 -4.1457362 -4.1781731 -4.2066393 -4.2311745][-4.1664081 -4.1470394 -4.1296315 -4.1430359 -4.1890659 -4.2222342 -4.2245879 -4.1912146 -4.1182036 -4.0618362 -4.0747848 -4.1173472 -4.1601071 -4.1991234 -4.2309928][-4.1829395 -4.164278 -4.1505728 -4.1651478 -4.2046647 -4.2315426 -4.2327552 -4.2004566 -4.1299372 -4.06819 -4.0745912 -4.1177411 -4.1686082 -4.2145395 -4.2490029][-4.1887822 -4.1679454 -4.1575484 -4.1691132 -4.1996493 -4.2231026 -4.2286639 -4.2081404 -4.156827 -4.1028872 -4.0985961 -4.1371403 -4.191453 -4.2370625 -4.2670856][-4.1851997 -4.1580291 -4.1443186 -4.1482115 -4.1694407 -4.1957569 -4.215754 -4.2165442 -4.1887589 -4.1474538 -4.1338873 -4.1606069 -4.2078915 -4.24688 -4.27249][-4.1821284 -4.142242 -4.1142321 -4.10589 -4.1199117 -4.153594 -4.1902037 -4.2114549 -4.2041955 -4.1751628 -4.1567912 -4.1667461 -4.1989818 -4.23223 -4.259573][-4.1789484 -4.1278543 -4.0837445 -4.0570717 -4.0639019 -4.105176 -4.1550465 -4.1929188 -4.2012448 -4.1819763 -4.1585197 -4.1525335 -4.1667972 -4.1947317 -4.2282376][-4.1835237 -4.1349988 -4.0844326 -4.0411072 -4.0356255 -4.0750127 -4.1278629 -4.1732473 -4.1892743 -4.1719656 -4.1406555 -4.1213684 -4.1182146 -4.1428671 -4.1890817][-4.1887879 -4.1521831 -4.1076307 -4.0649695 -4.0512114 -4.073566 -4.1133995 -4.1542869 -4.1705761 -4.1546054 -4.1205039 -4.093389 -4.0772614 -4.0996132 -4.1566329][-4.1877213 -4.1621909 -4.1332612 -4.1046934 -4.0922847 -4.0949349 -4.1110663 -4.1399374 -4.1556177 -4.1415434 -4.1114817 -4.0849252 -4.0651217 -4.0881457 -4.1465816][-4.1767311 -4.162086 -4.1526675 -4.1458735 -4.142838 -4.1323757 -4.1287184 -4.144875 -4.1572118 -4.1474214 -4.1274061 -4.107461 -4.0909138 -4.1099157 -4.1565213][-4.1607308 -4.158299 -4.1652174 -4.1780291 -4.1864891 -4.1695194 -4.1574292 -4.1683 -4.17947 -4.1774487 -4.1707892 -4.1592164 -4.1443624 -4.1555223 -4.1801043][-4.1490912 -4.1581473 -4.1756749 -4.2006178 -4.2153249 -4.1987338 -4.1850648 -4.1950073 -4.2079921 -4.2131567 -4.2152061 -4.2078915 -4.1940422 -4.1994209 -4.2069144][-4.1429038 -4.1647367 -4.1897087 -4.2171512 -4.2283864 -4.2099361 -4.1971684 -4.2077532 -4.2250757 -4.2380466 -4.2454767 -4.2416458 -4.2297139 -4.2298894 -4.2273431][-4.1534548 -4.1781688 -4.20541 -4.2300782 -4.2346559 -4.2152367 -4.2030258 -4.2129903 -4.2327781 -4.2486672 -4.2557521 -4.2485895 -4.235383 -4.2305937 -4.2233496]]...]
INFO - root - 2017-12-05 18:46:33.405071: step 34910, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 70h:58m:46s remains)
INFO - root - 2017-12-05 18:46:42.002043: step 34920, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 72h:23m:28s remains)
INFO - root - 2017-12-05 18:46:50.692300: step 34930, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 70h:12m:16s remains)
INFO - root - 2017-12-05 18:46:59.081700: step 34940, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 70h:52m:59s remains)
INFO - root - 2017-12-05 18:47:07.597487: step 34950, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 70h:12m:54s remains)
INFO - root - 2017-12-05 18:47:16.200652: step 34960, loss = 2.03, batch loss = 1.98 (9.1 examples/sec; 0.878 sec/batch; 72h:32m:51s remains)
INFO - root - 2017-12-05 18:47:24.850338: step 34970, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 69h:32m:02s remains)
INFO - root - 2017-12-05 18:47:33.441759: step 34980, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 70h:17m:06s remains)
INFO - root - 2017-12-05 18:47:41.846727: step 34990, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 70h:55m:31s remains)
INFO - root - 2017-12-05 18:47:50.392952: step 35000, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 71h:29m:04s remains)
2017-12-05 18:47:51.096265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.09332 -4.1002321 -4.0956783 -4.0948257 -4.09049 -4.0979733 -4.105967 -4.1114383 -4.1221981 -4.1398458 -4.1392717 -4.1223221 -4.1317625 -4.1505275 -4.1568747][-4.0910554 -4.1033454 -4.0938983 -4.0811529 -4.0619445 -4.05681 -4.06289 -4.0717449 -4.0946488 -4.127192 -4.1356225 -4.12596 -4.1363673 -4.1549993 -4.1675506][-4.0657787 -4.0944643 -4.0909004 -4.0720353 -4.0408845 -4.0216985 -4.0215168 -4.0334511 -4.063467 -4.106297 -4.1223645 -4.1239166 -4.1404262 -4.1565938 -4.1704559][-4.010982 -4.0577679 -4.0689907 -4.0551682 -4.0223694 -3.9960408 -3.9929025 -4.0067863 -4.0371776 -4.0754552 -4.093791 -4.1054778 -4.1369205 -4.15937 -4.1740694][-3.9459038 -3.9858246 -3.9978385 -3.9852343 -3.952698 -3.9236593 -3.9223332 -3.9446433 -3.9746745 -4.0096912 -4.028398 -4.0512748 -4.1009812 -4.1335049 -4.153275][-3.9270735 -3.9373295 -3.9251447 -3.8979006 -3.8409035 -3.7771719 -3.7623105 -3.7985635 -3.8466694 -3.8951716 -3.9216032 -3.9558382 -4.022902 -4.0742154 -4.1028543][-3.9343615 -3.9144149 -3.87602 -3.8330476 -3.748596 -3.6365263 -3.5911834 -3.6349754 -3.7094879 -3.7821417 -3.8224063 -3.8650727 -3.9439077 -4.0113878 -4.0505567][-3.9608214 -3.9275882 -3.8895659 -3.8568692 -3.7807193 -3.6738629 -3.6233284 -3.6336484 -3.68531 -3.7525425 -3.7967639 -3.8502431 -3.9390564 -4.0122051 -4.05117][-4.0029364 -3.9857361 -3.9638689 -3.9498477 -3.908958 -3.846616 -3.8148785 -3.7960083 -3.8085122 -3.8472662 -3.87319 -3.9128942 -3.9937611 -4.0619965 -4.0877905][-4.0262666 -4.0249844 -4.018959 -4.0202875 -4.0054116 -3.9724517 -3.95644 -3.9380224 -3.9370568 -3.9504981 -3.9472086 -3.958689 -4.0206475 -4.0707941 -4.0781536][-4.0312967 -4.0485725 -4.0578036 -4.0653095 -4.0606747 -4.0389581 -4.0285354 -4.0209923 -4.020637 -4.0205817 -4.0005703 -3.9908264 -4.028163 -4.04923 -4.0388093][-4.0222507 -4.0546618 -4.0739479 -4.08433 -4.0860662 -4.0763268 -4.0725126 -4.0726738 -4.0744462 -4.072381 -4.0485086 -4.02894 -4.04748 -4.0496273 -4.0283933][-4.060132 -4.0961738 -4.1140261 -4.1201744 -4.1220622 -4.118453 -4.1183591 -4.1197457 -4.12158 -4.1215191 -4.1094227 -4.0947633 -4.1024132 -4.0998282 -4.0780439][-4.1525221 -4.1805139 -4.1887064 -4.1861758 -4.1861892 -4.18449 -4.1848097 -4.1857162 -4.1870594 -4.1873503 -4.1830955 -4.1770668 -4.1823196 -4.1840677 -4.1711879][-4.235971 -4.2524281 -4.2522016 -4.2467308 -4.2466645 -4.2460313 -4.2456975 -4.245491 -4.2460923 -4.2465067 -4.2453308 -4.2432466 -4.2488546 -4.2556272 -4.2535334]]...]
INFO - root - 2017-12-05 18:47:59.688374: step 35010, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 71h:57m:07s remains)
INFO - root - 2017-12-05 18:48:08.332576: step 35020, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 70h:12m:04s remains)
INFO - root - 2017-12-05 18:48:16.961078: step 35030, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 70h:46m:09s remains)
INFO - root - 2017-12-05 18:48:25.253093: step 35040, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 68h:32m:23s remains)
INFO - root - 2017-12-05 18:48:33.859618: step 35050, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 72h:31m:18s remains)
INFO - root - 2017-12-05 18:48:42.523652: step 35060, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 69h:57m:36s remains)
INFO - root - 2017-12-05 18:48:50.994704: step 35070, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.843 sec/batch; 69h:39m:01s remains)
INFO - root - 2017-12-05 18:48:59.596481: step 35080, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 72h:19m:04s remains)
INFO - root - 2017-12-05 18:49:08.111532: step 35090, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 70h:38m:20s remains)
INFO - root - 2017-12-05 18:49:16.766945: step 35100, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 69h:46m:08s remains)
2017-12-05 18:49:17.515993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24763 -4.2488775 -4.2565346 -4.2654972 -4.274991 -4.2800045 -4.2783151 -4.2751288 -4.2771993 -4.2818766 -4.2897115 -4.2972527 -4.3051009 -4.3063059 -4.2989707][-4.2152352 -4.2231188 -4.2436547 -4.264616 -4.2794089 -4.2852521 -4.2817259 -4.2752209 -4.2728209 -4.2748466 -4.2820926 -4.289598 -4.3006225 -4.3049221 -4.3004613][-4.1749439 -4.1888981 -4.2230735 -4.259665 -4.2803545 -4.2866116 -4.2821741 -4.2735281 -4.2655005 -4.2637148 -4.27215 -4.2819066 -4.292757 -4.2953143 -4.2922525][-4.1365161 -4.1524086 -4.1970267 -4.2477169 -4.2772384 -4.2869172 -4.2828994 -4.2708559 -4.2559829 -4.2524 -4.2654748 -4.2773361 -4.2837725 -4.2821236 -4.278986][-4.0944963 -4.1101618 -4.1605635 -4.2158127 -4.2509832 -4.2631855 -4.2551885 -4.2375154 -4.2142158 -4.2143312 -4.2377548 -4.2570086 -4.2635489 -4.2660565 -4.2702203][-4.055799 -4.0667605 -4.1219831 -4.181746 -4.2200089 -4.22909 -4.2047796 -4.1663609 -4.129817 -4.1353197 -4.1821432 -4.2220426 -4.2398081 -4.2560735 -4.2693024][-4.03686 -4.0258026 -4.0764332 -4.1410003 -4.1858277 -4.1924872 -4.1446609 -4.06276 -3.9988494 -4.0175042 -4.0985565 -4.1699634 -4.2093487 -4.2438197 -4.263175][-4.052094 -4.0109115 -4.0406032 -4.1003776 -4.1511745 -4.1660519 -4.1067543 -3.9818542 -3.882988 -3.91039 -4.0195885 -4.1146636 -4.1740761 -4.2201657 -4.2444472][-4.0946712 -4.0365791 -4.0395932 -4.0844827 -4.133244 -4.15518 -4.1065459 -3.9809377 -3.8690178 -3.8872595 -3.9915538 -4.0860672 -4.1507392 -4.2034006 -4.2347412][-4.1254349 -4.06907 -4.0573349 -4.0912156 -4.1379576 -4.1675324 -4.1405144 -4.0514231 -3.9588785 -3.9522111 -4.0235662 -4.1006026 -4.1565881 -4.2035985 -4.2356224][-4.1184607 -4.0749707 -4.0698628 -4.1053381 -4.151175 -4.1857939 -4.1801667 -4.1296735 -4.0659537 -4.0389261 -4.0745907 -4.1351371 -4.1810665 -4.214045 -4.2388196][-4.0967116 -4.063652 -4.0679216 -4.1055388 -4.150095 -4.1863422 -4.1992731 -4.1774707 -4.1357908 -4.0983257 -4.1098413 -4.1555252 -4.1936016 -4.2184558 -4.2398343][-4.0869479 -4.0594234 -4.0682216 -4.1041551 -4.1457758 -4.177834 -4.1943374 -4.1870866 -4.1577315 -4.1203513 -4.1174273 -4.1494322 -4.1837077 -4.2111092 -4.2363892][-4.1098385 -4.085712 -4.0944424 -4.1231451 -4.1579738 -4.1856546 -4.1965241 -4.1905565 -4.1696863 -4.140625 -4.1315584 -4.1487336 -4.1780744 -4.2100086 -4.2361813][-4.155798 -4.1308303 -4.1334729 -4.1510825 -4.1749153 -4.1949492 -4.2016163 -4.1964259 -4.1846662 -4.1700993 -4.1649966 -4.1731043 -4.1939335 -4.2206707 -4.240068]]...]
INFO - root - 2017-12-05 18:49:26.199433: step 35110, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 71h:51m:36s remains)
INFO - root - 2017-12-05 18:49:34.772164: step 35120, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 71h:14m:21s remains)
INFO - root - 2017-12-05 18:49:43.344110: step 35130, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 70h:35m:43s remains)
INFO - root - 2017-12-05 18:49:51.803392: step 35140, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 67h:40m:11s remains)
INFO - root - 2017-12-05 18:50:00.232746: step 35150, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 69h:55m:04s remains)
INFO - root - 2017-12-05 18:50:08.775458: step 35160, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 71h:00m:30s remains)
INFO - root - 2017-12-05 18:50:17.370580: step 35170, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 73h:19m:42s remains)
INFO - root - 2017-12-05 18:50:25.964290: step 35180, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 68h:36m:06s remains)
INFO - root - 2017-12-05 18:50:34.552860: step 35190, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 73h:37m:23s remains)
INFO - root - 2017-12-05 18:50:43.022148: step 35200, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 72h:03m:37s remains)
2017-12-05 18:50:43.768375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.224936 -4.2282991 -4.2259517 -4.2363443 -4.2343307 -4.205143 -4.1796079 -4.1617074 -4.1458492 -4.1400309 -4.1409507 -4.1492772 -4.163013 -4.1760654 -4.189033][-4.2308736 -4.2323608 -4.2300491 -4.2387691 -4.2335792 -4.1995668 -4.1683888 -4.1446986 -4.1259794 -4.1222568 -4.124372 -4.1274753 -4.1332154 -4.1432619 -4.1561337][-4.2384615 -4.2384233 -4.2347107 -4.2403755 -4.23099 -4.1931357 -4.1592579 -4.1355238 -4.1216431 -4.1254573 -4.1315389 -4.1302037 -4.1221018 -4.1207247 -4.1296115][-4.2453113 -4.2466216 -4.2435832 -4.2471905 -4.2340527 -4.1923418 -4.156775 -4.138176 -4.1337476 -4.14305 -4.1500931 -4.1413693 -4.1197071 -4.1063766 -4.1134334][-4.251502 -4.2549334 -4.253355 -4.2554126 -4.2376924 -4.1863151 -4.140502 -4.1216974 -4.1254587 -4.1404767 -4.1495104 -4.1384373 -4.1120348 -4.0954161 -4.1070056][-4.2518086 -4.2544227 -4.2506032 -4.2472496 -4.2205977 -4.1536875 -4.0894918 -4.06284 -4.0771503 -4.1073818 -4.1271868 -4.1234865 -4.1019969 -4.0879555 -4.1028104][-4.2439909 -4.2417789 -4.2311187 -4.2186275 -4.1807561 -4.0989528 -4.0149918 -3.9748785 -4.00304 -4.0581717 -4.0971723 -4.1075344 -4.0968843 -4.0884156 -4.1030655][-4.2294264 -4.2202663 -4.2004519 -4.1791339 -4.1333771 -4.0470619 -3.9602244 -3.9157615 -3.9534209 -4.0286841 -4.0870543 -4.1120725 -4.1141167 -4.1143408 -4.1268315][-4.2178149 -4.2033539 -4.1770039 -4.1531982 -4.11153 -4.0393839 -3.9745307 -3.9424086 -3.973268 -4.04255 -4.1030636 -4.1362019 -4.1505032 -4.1610084 -4.1734209][-4.2139192 -4.1995592 -4.173758 -4.1543651 -4.1234388 -4.0711646 -4.030808 -4.01121 -4.0301781 -4.0789771 -4.1291385 -4.1654892 -4.1883779 -4.2050872 -4.2138906][-4.2113466 -4.2002935 -4.1803961 -4.1674538 -4.1454492 -4.1075172 -4.0835338 -4.0744166 -4.0853972 -4.1143432 -4.1481972 -4.1804385 -4.2049212 -4.2228446 -4.227078][-4.2093744 -4.2021461 -4.1875892 -4.1792431 -4.1623974 -4.1331787 -4.1198788 -4.1194034 -4.1281748 -4.1430254 -4.1594586 -4.1803722 -4.1982913 -4.2111654 -4.2103972][-4.2063417 -4.2001781 -4.1853485 -4.1769872 -4.1607642 -4.1369743 -4.133472 -4.1407585 -4.1495795 -4.1567063 -4.1632891 -4.1745524 -4.184813 -4.1892829 -4.1836214][-4.1940293 -4.1844835 -4.1659551 -4.1572266 -4.1443348 -4.127852 -4.1319413 -4.1427708 -4.1501679 -4.1526833 -4.1564016 -4.1658125 -4.17395 -4.1732764 -4.1662478][-4.1753492 -4.1617002 -4.1410012 -4.1339841 -4.1254749 -4.1156583 -4.1230049 -4.1329746 -4.1380539 -4.1390424 -4.1430125 -4.1521616 -4.1601434 -4.1587873 -4.1532526]]...]
INFO - root - 2017-12-05 18:50:52.330653: step 35210, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 71h:02m:53s remains)
INFO - root - 2017-12-05 18:51:00.991711: step 35220, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 70h:52m:57s remains)
INFO - root - 2017-12-05 18:51:09.482045: step 35230, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 69h:49m:16s remains)
INFO - root - 2017-12-05 18:51:17.934764: step 35240, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 69h:53m:11s remains)
INFO - root - 2017-12-05 18:51:26.482404: step 35250, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 71h:28m:51s remains)
INFO - root - 2017-12-05 18:51:34.900245: step 35260, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 70h:31m:34s remains)
INFO - root - 2017-12-05 18:51:43.465932: step 35270, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.841 sec/batch; 69h:26m:14s remains)
INFO - root - 2017-12-05 18:51:51.895837: step 35280, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 71h:10m:17s remains)
INFO - root - 2017-12-05 18:52:00.451665: step 35290, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 71h:47m:58s remains)
INFO - root - 2017-12-05 18:52:08.914575: step 35300, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 70h:01m:40s remains)
2017-12-05 18:52:09.663901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2511954 -4.2518263 -4.2488451 -4.2408881 -4.2353077 -4.2288704 -4.2130728 -4.1783924 -4.1541247 -4.1712189 -4.2057261 -4.2436481 -4.2926116 -4.3361583 -4.3597965][-4.2512093 -4.2452354 -4.2434797 -4.2399192 -4.2337489 -4.21867 -4.1925817 -4.1526585 -4.1320648 -4.1631546 -4.2125959 -4.2569561 -4.3031778 -4.3425984 -4.3640213][-4.2475209 -4.2329645 -4.22701 -4.2230854 -4.2167549 -4.1982527 -4.1657085 -4.1176991 -4.0958071 -4.138484 -4.2027087 -4.2556643 -4.302948 -4.3409681 -4.3603249][-4.2391815 -4.2183647 -4.2077775 -4.20515 -4.2024684 -4.18787 -4.1548643 -4.1000481 -4.0694261 -4.1117978 -4.183857 -4.2436843 -4.2912636 -4.3304081 -4.3504329][-4.2187705 -4.196137 -4.1821189 -4.1817265 -4.1866865 -4.1839275 -4.157867 -4.1028156 -4.0626335 -4.0955672 -4.1672087 -4.2277436 -4.2725887 -4.3125458 -4.3363547][-4.1918492 -4.1675348 -4.1458592 -4.1405878 -4.1479082 -4.1556864 -4.1400032 -4.0909424 -4.0455317 -4.0686731 -4.1369662 -4.1988783 -4.2457585 -4.2924037 -4.3227162][-4.1728821 -4.1429195 -4.1073155 -4.0893092 -4.0895562 -4.1000142 -4.0938673 -4.0523076 -4.0043859 -4.0222235 -4.092155 -4.162055 -4.2164645 -4.27315 -4.3109035][-4.1644859 -4.1238132 -4.0682826 -4.0294728 -4.0163555 -4.0250134 -4.0261 -3.9953449 -3.953306 -3.972204 -4.045403 -4.1246095 -4.1889281 -4.2545872 -4.2992854][-4.1743474 -4.1266603 -4.0578561 -4.0027838 -3.9782083 -3.985492 -3.9948053 -3.9814751 -3.9537041 -3.9736738 -4.0414271 -4.1189132 -4.1846375 -4.2496185 -4.2942238][-4.1919365 -4.1504288 -4.0907865 -4.0445933 -4.0259953 -4.0360265 -4.0497379 -4.0510559 -4.0378675 -4.0531306 -4.1043897 -4.165709 -4.2198195 -4.2714081 -4.3058453][-4.2029743 -4.1788654 -4.1420631 -4.116991 -4.1106868 -4.1224771 -4.1363606 -4.14175 -4.1338305 -4.1423893 -4.1791306 -4.2243185 -4.2669492 -4.3043113 -4.3248515][-4.2072287 -4.2004385 -4.1843638 -4.1763544 -4.1782117 -4.1885085 -4.196279 -4.1948791 -4.1838737 -4.1868124 -4.2142568 -4.2523575 -4.2891054 -4.3187995 -4.3331809][-4.1993628 -4.210216 -4.2120538 -4.2144747 -4.2172904 -4.2212214 -4.21938 -4.2080259 -4.1922879 -4.1925087 -4.2168889 -4.2515793 -4.2860937 -4.3141627 -4.3289781][-4.1834655 -4.2076879 -4.2196054 -4.2264376 -4.2285709 -4.2278514 -4.2194219 -4.202374 -4.1844497 -4.1838303 -4.2067652 -4.2386637 -4.270237 -4.2978897 -4.3155274][-4.1728086 -4.1986175 -4.2140946 -4.2236891 -4.2250571 -4.2209449 -4.2089829 -4.1900306 -4.1714897 -4.170578 -4.1918039 -4.2193804 -4.2480044 -4.2767305 -4.2979989]]...]
INFO - root - 2017-12-05 18:52:18.047641: step 35310, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.753 sec/batch; 62h:07m:55s remains)
INFO - root - 2017-12-05 18:52:26.781303: step 35320, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 71h:26m:50s remains)
INFO - root - 2017-12-05 18:52:35.404149: step 35330, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 71h:50m:15s remains)
INFO - root - 2017-12-05 18:52:43.932953: step 35340, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 72h:11m:32s remains)
INFO - root - 2017-12-05 18:52:52.403349: step 35350, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 71h:16m:42s remains)
INFO - root - 2017-12-05 18:53:00.803366: step 35360, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.757 sec/batch; 62h:26m:40s remains)
INFO - root - 2017-12-05 18:53:09.226503: step 35370, loss = 2.02, batch loss = 1.96 (9.5 examples/sec; 0.844 sec/batch; 69h:41m:57s remains)
INFO - root - 2017-12-05 18:53:17.679730: step 35380, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 72h:10m:53s remains)
INFO - root - 2017-12-05 18:53:26.267757: step 35390, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 72h:36m:52s remains)
INFO - root - 2017-12-05 18:53:34.769786: step 35400, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.798 sec/batch; 65h:51m:10s remains)
2017-12-05 18:53:35.572607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28647 -4.2828474 -4.2689576 -4.2492571 -4.2300434 -4.2138777 -4.2044539 -4.2081237 -4.2147079 -4.2120118 -4.2056918 -4.2018485 -4.2047348 -4.2133155 -4.231318][-4.291266 -4.2844152 -4.2604671 -4.22998 -4.2022572 -4.1797276 -4.1690588 -4.1765127 -4.185955 -4.18416 -4.17786 -4.1731114 -4.1760311 -4.1860003 -4.2073779][-4.2937264 -4.2823691 -4.253705 -4.2172384 -4.1825323 -4.1533804 -4.13954 -4.1477489 -4.16109 -4.1600204 -4.1546707 -4.1520476 -4.1594806 -4.171649 -4.1911006][-4.2952552 -4.2833128 -4.2560196 -4.2190604 -4.1808534 -4.147471 -4.1280708 -4.1300168 -4.1501293 -4.1550093 -4.1530762 -4.1560249 -4.1661663 -4.1754618 -4.1892495][-4.2917504 -4.2839265 -4.2632494 -4.2279491 -4.1873407 -4.1454697 -4.1086612 -4.1010132 -4.1306028 -4.1509223 -4.1606483 -4.174685 -4.1874843 -4.1922011 -4.1971722][-4.2861776 -4.2823491 -4.2604122 -4.2181916 -4.1671605 -4.1080275 -4.0423894 -4.0256929 -4.0741515 -4.1240506 -4.1562238 -4.1845818 -4.20237 -4.2044024 -4.1994047][-4.2832108 -4.2762961 -4.2457762 -4.1902566 -4.1202393 -4.0303817 -3.9281049 -3.9178674 -4.0088763 -4.09046 -4.1397915 -4.1748219 -4.1943703 -4.1968517 -4.1894813][-4.2790709 -4.2641354 -4.2245765 -4.1575637 -4.07401 -3.963928 -3.8619006 -3.8929191 -4.0123482 -4.0925393 -4.1331849 -4.1589446 -4.1705604 -4.1728792 -4.1704221][-4.2721515 -4.2492895 -4.2067018 -4.1448922 -4.0748267 -3.9988477 -3.9531825 -3.995729 -4.07961 -4.1257091 -4.1413822 -4.1513195 -4.1534605 -4.1561718 -4.1630669][-4.2642531 -4.2408681 -4.2070041 -4.1628509 -4.1191974 -4.0826912 -4.0720811 -4.1028962 -4.1446466 -4.1594381 -4.1590075 -4.1586809 -4.1571622 -4.1650162 -4.1857653][-4.2656479 -4.2459221 -4.223207 -4.1929369 -4.1667457 -4.1509957 -4.15305 -4.1710129 -4.1860962 -4.1854911 -4.1794176 -4.1748419 -4.1752086 -4.1927652 -4.2191434][-4.2779856 -4.2643409 -4.2498536 -4.2285333 -4.2106647 -4.2017961 -4.2049241 -4.2155051 -4.221231 -4.2134752 -4.2057824 -4.200686 -4.2025986 -4.2216711 -4.24456][-4.2907581 -4.2839208 -4.2725534 -4.2542338 -4.2380414 -4.2302732 -4.2335596 -4.2457132 -4.252851 -4.2425461 -4.2333164 -4.2262864 -4.2257619 -4.2389693 -4.2521405][-4.30102 -4.2968483 -4.2852521 -4.2659359 -4.2502713 -4.2448654 -4.249547 -4.2626405 -4.2699466 -4.2593937 -4.2479525 -4.2382269 -4.2339864 -4.2410345 -4.2475295][-4.3066578 -4.3008909 -4.2866812 -4.2673163 -4.2508 -4.2451644 -4.248579 -4.2598643 -4.2679758 -4.2573433 -4.2445269 -4.2360396 -4.2345896 -4.2411208 -4.2442675]]...]
INFO - root - 2017-12-05 18:53:44.054497: step 35410, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 71h:23m:29s remains)
INFO - root - 2017-12-05 18:53:52.492470: step 35420, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.800 sec/batch; 65h:58m:52s remains)
INFO - root - 2017-12-05 18:54:01.018015: step 35430, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 69h:46m:43s remains)
INFO - root - 2017-12-05 18:54:09.435602: step 35440, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 69h:22m:55s remains)
INFO - root - 2017-12-05 18:54:18.046979: step 35450, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 68h:51m:47s remains)
INFO - root - 2017-12-05 18:54:26.537379: step 35460, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 69h:37m:42s remains)
INFO - root - 2017-12-05 18:54:35.016504: step 35470, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 71h:29m:18s remains)
INFO - root - 2017-12-05 18:54:43.665548: step 35480, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 70h:39m:59s remains)
INFO - root - 2017-12-05 18:54:52.219676: step 35490, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 70h:24m:53s remains)
INFO - root - 2017-12-05 18:55:00.939098: step 35500, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.838 sec/batch; 69h:05m:38s remains)
2017-12-05 18:55:01.702144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3165612 -4.3007121 -4.2800112 -4.2612715 -4.23967 -4.2144938 -4.1933436 -4.1899939 -4.2152743 -4.2373614 -4.2499137 -4.2547374 -4.2567124 -4.26161 -4.2698669][-4.3056006 -4.2802749 -4.2469888 -4.214921 -4.1793122 -4.1443529 -4.11605 -4.1074591 -4.1408882 -4.173346 -4.1906157 -4.1967769 -4.1979828 -4.2064404 -4.2229733][-4.2973251 -4.2663627 -4.2266412 -4.18445 -4.1397347 -4.0992522 -4.0657721 -4.0491357 -4.0866628 -4.1227341 -4.1371989 -4.1395583 -4.138072 -4.151268 -4.1769128][-4.2944756 -4.2556343 -4.2069407 -4.1553888 -4.1011763 -4.0561886 -4.0248513 -4.0082889 -4.0461512 -4.0734172 -4.0758505 -4.0738106 -4.0700841 -4.0869155 -4.121047][-4.28671 -4.2381272 -4.1753016 -4.1129613 -4.0523968 -4.0096993 -3.9833765 -3.9643559 -3.9966455 -4.0077529 -3.993695 -3.9920652 -3.9977102 -4.0283689 -4.0749087][-4.27548 -4.2209091 -4.1529484 -4.0900145 -4.0341625 -3.9955528 -3.9592962 -3.9162564 -3.9292295 -3.9337435 -3.9144917 -3.9191184 -3.9389682 -3.9832482 -4.0378985][-4.264307 -4.2054858 -4.1357331 -4.0777736 -4.0255489 -3.9785666 -3.9189067 -3.8448167 -3.8447332 -3.8658867 -3.8668604 -3.8829777 -3.9095495 -3.951427 -4.0057025][-4.2604256 -4.1993146 -4.1328306 -4.0773053 -4.0177488 -3.9553158 -3.8825078 -3.8055692 -3.8166685 -3.8599377 -3.8756125 -3.8930213 -3.9155478 -3.9440975 -3.99077][-4.2676125 -4.2112737 -4.1502538 -4.0995417 -4.0428953 -3.9892449 -3.9403496 -3.8988793 -3.9185097 -3.9554648 -3.9610448 -3.9672661 -3.9799497 -3.9972231 -4.0340815][-4.2851014 -4.2390227 -4.1860442 -4.1418128 -4.0973792 -4.0620461 -4.0374036 -4.0146012 -4.0270839 -4.0514569 -4.0519 -4.0580173 -4.0720396 -4.084394 -4.1104445][-4.2998409 -4.2628894 -4.2196717 -4.1833887 -4.1522837 -4.130899 -4.11678 -4.1000457 -4.1051383 -4.1207848 -4.1224828 -4.1358786 -4.1589746 -4.1712313 -4.1850243][-4.3082309 -4.2818651 -4.2529731 -4.2303915 -4.2124467 -4.1983085 -4.1874256 -4.1718521 -4.1696362 -4.1797795 -4.18557 -4.2018352 -4.2254596 -4.2335577 -4.2402654][-4.3169479 -4.2997007 -4.2823858 -4.269063 -4.2587204 -4.2500458 -4.2406816 -4.2290955 -4.2304306 -4.2441592 -4.2541389 -4.2681952 -4.2843037 -4.2855048 -4.2883735][-4.3279114 -4.318037 -4.3076844 -4.2986884 -4.2904582 -4.2846737 -4.2769628 -4.2700686 -4.2752037 -4.2898712 -4.2987051 -4.3067083 -4.3159795 -4.3155322 -4.3168449][-4.3378344 -4.3315377 -4.324264 -4.3159814 -4.3076892 -4.3008127 -4.29351 -4.2893548 -4.2960205 -4.3068695 -4.3130236 -4.3172526 -4.3217268 -4.3217854 -4.3237309]]...]
INFO - root - 2017-12-05 18:55:10.224216: step 35510, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 72h:37m:29s remains)
INFO - root - 2017-12-05 18:55:18.780007: step 35520, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 69h:05m:05s remains)
INFO - root - 2017-12-05 18:55:27.245194: step 35530, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 0.788 sec/batch; 65h:02m:08s remains)
INFO - root - 2017-12-05 18:55:35.733451: step 35540, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 71h:44m:53s remains)
INFO - root - 2017-12-05 18:55:44.324686: step 35550, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 68h:38m:07s remains)
INFO - root - 2017-12-05 18:55:52.870173: step 35560, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 68h:29m:17s remains)
INFO - root - 2017-12-05 18:56:01.545806: step 35570, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.841 sec/batch; 69h:22m:20s remains)
INFO - root - 2017-12-05 18:56:09.965361: step 35580, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 71h:25m:24s remains)
INFO - root - 2017-12-05 18:56:18.511320: step 35590, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 69h:39m:56s remains)
INFO - root - 2017-12-05 18:56:27.215971: step 35600, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 71h:48m:59s remains)
2017-12-05 18:56:27.978191: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2727914 -4.237144 -4.1946936 -4.1413293 -4.0889559 -4.05695 -4.0767217 -4.1320362 -4.1790433 -4.2202954 -4.2661748 -4.2994537 -4.3211808 -4.3278046 -4.3250113][-4.2681179 -4.2304211 -4.18478 -4.1259594 -4.0634608 -4.0186567 -4.0370884 -4.1011658 -4.1580949 -4.2095466 -4.2656355 -4.3053069 -4.327373 -4.3311644 -4.3251781][-4.2673974 -4.2268353 -4.1749396 -4.1088438 -4.0358362 -3.9781246 -3.9962635 -4.0744934 -4.1459942 -4.2071548 -4.2707906 -4.3139224 -4.3354106 -4.3352056 -4.3249745][-4.2653236 -4.2236819 -4.1668768 -4.0945878 -4.0109844 -3.940526 -3.9526031 -4.0449495 -4.1341896 -4.2074027 -4.2763424 -4.3210282 -4.3406992 -4.3373213 -4.3225613][-4.2610793 -4.2226605 -4.1660657 -4.0886726 -3.9994226 -3.9219315 -3.9195862 -4.0192609 -4.1233063 -4.2052345 -4.27855 -4.3253016 -4.3428764 -4.3367081 -4.317678][-4.2572417 -4.2240939 -4.1682162 -4.08527 -3.9887986 -3.9014587 -3.8805368 -3.983779 -4.1028814 -4.1930413 -4.270987 -4.3197761 -4.3367248 -4.3299761 -4.3093996][-4.2520037 -4.2225342 -4.1652637 -4.0752215 -3.9645271 -3.8605914 -3.8226449 -3.9300687 -4.0664024 -4.1695 -4.2541008 -4.3047118 -4.3232803 -4.3172383 -4.2956405][-4.2487531 -4.2231841 -4.1672716 -4.0744495 -3.9501042 -3.8283708 -3.7729042 -3.8721271 -4.0199347 -4.1380672 -4.2316403 -4.2862368 -4.3075857 -4.302609 -4.27963][-4.2495036 -4.2289176 -4.1823545 -4.0992694 -3.9806128 -3.8646393 -3.7979326 -3.8684721 -4.0059114 -4.1249814 -4.2189741 -4.2741714 -4.2963347 -4.2922068 -4.26894][-4.2499986 -4.2339392 -4.199338 -4.1331573 -4.0386071 -3.9452789 -3.8787837 -3.912585 -4.022841 -4.1277766 -4.2111659 -4.2613916 -4.2836843 -4.28019 -4.2563581][-4.2528644 -4.2372575 -4.2099404 -4.158103 -4.0863867 -4.0180678 -3.966186 -3.9805174 -4.0639114 -4.1501961 -4.2173114 -4.2581768 -4.2771778 -4.274363 -4.25258][-4.2564716 -4.2415614 -4.2206907 -4.182148 -4.1302657 -4.0827284 -4.0500159 -4.0607233 -4.1221952 -4.1902056 -4.2409759 -4.2695765 -4.2797804 -4.2740011 -4.2524343][-4.2591238 -4.2469587 -4.2344041 -4.2100291 -4.1758995 -4.1453538 -4.1291833 -4.1397958 -4.184721 -4.2349653 -4.2700419 -4.285418 -4.285058 -4.2746921 -4.25264][-4.2641025 -4.2546043 -4.2483163 -4.2348046 -4.215198 -4.1983104 -4.1917276 -4.1999593 -4.2310371 -4.2662973 -4.2902088 -4.2970576 -4.290926 -4.2794342 -4.2606955][-4.2709203 -4.2646184 -4.2633319 -4.2576847 -4.2479734 -4.2391915 -4.2382674 -4.2456374 -4.2649612 -4.2858272 -4.299139 -4.3015022 -4.2944503 -4.2838731 -4.2700119]]...]
INFO - root - 2017-12-05 18:56:36.488635: step 35610, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 70h:48m:52s remains)
INFO - root - 2017-12-05 18:56:45.198796: step 35620, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 72h:51m:56s remains)
INFO - root - 2017-12-05 18:56:53.782723: step 35630, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 72h:00m:34s remains)
INFO - root - 2017-12-05 18:57:02.158771: step 35640, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 73h:21m:17s remains)
INFO - root - 2017-12-05 18:57:10.875100: step 35650, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 70h:51m:58s remains)
INFO - root - 2017-12-05 18:57:19.581211: step 35660, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 72h:06m:33s remains)
INFO - root - 2017-12-05 18:57:28.150515: step 35670, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 70h:41m:45s remains)
INFO - root - 2017-12-05 18:57:36.682730: step 35680, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 69h:28m:48s remains)
INFO - root - 2017-12-05 18:57:45.130329: step 35690, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 72h:41m:21s remains)
INFO - root - 2017-12-05 18:57:53.658352: step 35700, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 70h:13m:03s remains)
2017-12-05 18:57:54.473760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2651772 -4.2475047 -4.2278824 -4.2126675 -4.1993513 -4.2072039 -4.231843 -4.2507906 -4.2556605 -4.2535768 -4.25496 -4.254549 -4.25522 -4.2619696 -4.2628579][-4.2620282 -4.24273 -4.2227788 -4.20726 -4.1907811 -4.192893 -4.212616 -4.2278852 -4.2337823 -4.2357597 -4.2397633 -4.2425008 -4.2473369 -4.2529697 -4.2458973][-4.259233 -4.2386041 -4.2206349 -4.2084026 -4.1931243 -4.1882243 -4.2017946 -4.2097344 -4.2118254 -4.2160158 -4.2225962 -4.2268324 -4.2342157 -4.242177 -4.2329636][-4.2530951 -4.2268667 -4.2077603 -4.1967926 -4.1804233 -4.1692758 -4.1774983 -4.1804271 -4.1782084 -4.1822724 -4.1929407 -4.1999559 -4.2116323 -4.2255816 -4.2187805][-4.2499609 -4.2189178 -4.1953473 -4.1795521 -4.1571183 -4.1369929 -4.1332035 -4.1293612 -4.128108 -4.13843 -4.160635 -4.1727018 -4.1856723 -4.2023354 -4.199614][-4.2518268 -4.2201624 -4.1932507 -4.1680765 -4.1347094 -4.1024375 -4.0863552 -4.0786786 -4.0844631 -4.1071086 -4.1424317 -4.1617765 -4.1701012 -4.18656 -4.1887684][-4.2552161 -4.2249837 -4.1941481 -4.1623745 -4.1206737 -4.0797014 -4.0553155 -4.0507326 -4.0709486 -4.1036072 -4.1427341 -4.167345 -4.172626 -4.1871328 -4.1929955][-4.2584419 -4.2335134 -4.2045264 -4.1724648 -4.1256604 -4.0796814 -4.0516405 -4.0483146 -4.0746431 -4.1065054 -4.1431613 -4.1722169 -4.1790051 -4.18864 -4.19395][-4.2614875 -4.241785 -4.215445 -4.1854649 -4.1410747 -4.0993185 -4.0775332 -4.0779614 -4.098382 -4.1189513 -4.146647 -4.1723671 -4.1788206 -4.1813264 -4.1830864][-4.2648582 -4.246408 -4.2200484 -4.190505 -4.1539397 -4.1251926 -4.1187062 -4.13021 -4.14531 -4.1528707 -4.168509 -4.1831603 -4.1846018 -4.1797824 -4.1738658][-4.2710543 -4.252284 -4.2224073 -4.1916838 -4.1650476 -4.1502304 -4.1581411 -4.17831 -4.1923218 -4.1928334 -4.1972346 -4.2031584 -4.1955738 -4.18136 -4.1643953][-4.2773147 -4.257133 -4.2253518 -4.195734 -4.1788216 -4.17527 -4.1861134 -4.204309 -4.2181282 -4.2188268 -4.2204208 -4.2158122 -4.1991267 -4.1769156 -4.153954][-4.2811742 -4.26225 -4.234612 -4.2097397 -4.1992526 -4.2001362 -4.2082834 -4.222343 -4.2323074 -4.2300825 -4.225708 -4.2118168 -4.191143 -4.1697984 -4.1470804][-4.2802706 -4.2610135 -4.2415948 -4.2259746 -4.2177134 -4.21978 -4.2265329 -4.238349 -4.2456918 -4.2429543 -4.2345529 -4.2152877 -4.1920753 -4.1723471 -4.1540561][-4.2759423 -4.2579451 -4.2475858 -4.2405429 -4.2341356 -4.2363539 -4.2434306 -4.2544141 -4.2614923 -4.2606406 -4.2520757 -4.234005 -4.2093692 -4.18748 -4.1712046]]...]
INFO - root - 2017-12-05 18:58:02.994112: step 35710, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 70h:25m:10s remains)
INFO - root - 2017-12-05 18:58:11.716965: step 35720, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 70h:39m:01s remains)
INFO - root - 2017-12-05 18:58:20.220943: step 35730, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 70h:43m:45s remains)
INFO - root - 2017-12-05 18:58:28.646791: step 35740, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 68h:45m:35s remains)
INFO - root - 2017-12-05 18:58:37.171808: step 35750, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.801 sec/batch; 66h:00m:49s remains)
INFO - root - 2017-12-05 18:58:45.698925: step 35760, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 70h:53m:31s remains)
INFO - root - 2017-12-05 18:58:54.353450: step 35770, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 71h:25m:39s remains)
INFO - root - 2017-12-05 18:59:02.822480: step 35780, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.832 sec/batch; 68h:36m:55s remains)
INFO - root - 2017-12-05 18:59:11.410336: step 35790, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 72h:19m:58s remains)
INFO - root - 2017-12-05 18:59:20.026743: step 35800, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.874 sec/batch; 72h:03m:38s remains)
2017-12-05 18:59:20.819669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0155816 -4.0897236 -4.1497569 -4.1924958 -4.2203317 -4.2372174 -4.236021 -4.19346 -4.1091824 -4.0141287 -3.9708798 -4.0027332 -4.0770717 -4.1514425 -4.2150702][-4.0888157 -4.155652 -4.1986485 -4.2237244 -4.2392354 -4.2542367 -4.2573662 -4.2334666 -4.1941071 -4.1488843 -4.134388 -4.1551414 -4.188683 -4.2275929 -4.2639823][-4.1470976 -4.2050233 -4.2349372 -4.2452326 -4.2487979 -4.2524033 -4.2477388 -4.2342572 -4.2292976 -4.2228723 -4.2308359 -4.2534475 -4.2723074 -4.2943435 -4.3127327][-4.1844616 -4.2303271 -4.2506542 -4.2540278 -4.2444954 -4.2263741 -4.1955523 -4.1798892 -4.2075024 -4.2427049 -4.2808595 -4.316258 -4.3339343 -4.3444343 -4.3452978][-4.2218304 -4.2467184 -4.2492518 -4.2383218 -4.208468 -4.1558833 -4.0831509 -4.0452976 -4.0942683 -4.1789274 -4.2599163 -4.3229489 -4.3571086 -4.36926 -4.3589163][-4.2584143 -4.2541094 -4.2334967 -4.1912107 -4.1192694 -4.0136342 -3.8790128 -3.8012357 -3.8803463 -4.0346746 -4.1717515 -4.2710924 -4.32863 -4.3571925 -4.3554606][-4.2835345 -4.2600784 -4.2174921 -4.1395359 -4.0144906 -3.8421774 -3.6245317 -3.4793921 -3.5883715 -3.8334761 -4.0447483 -4.1849179 -4.2666254 -4.3163457 -4.3317866][-4.2953053 -4.2645836 -4.2120552 -4.1235142 -3.988307 -3.7933152 -3.5199451 -3.2964315 -3.3809078 -3.6635339 -3.924063 -4.0930271 -4.1965752 -4.265543 -4.29645][-4.2871294 -4.2548246 -4.2105474 -4.1447287 -4.050334 -3.9063544 -3.679806 -3.4754002 -3.4928386 -3.6869886 -3.9045777 -4.0524106 -4.1542249 -4.2256985 -4.2653141][-4.2729239 -4.2464876 -4.2119594 -4.1698861 -4.1189985 -4.03396 -3.8873153 -3.7542663 -3.7473385 -3.8428211 -3.9800107 -4.0785189 -4.1570911 -4.2161546 -4.2527018][-4.2637997 -4.2502966 -4.2290163 -4.2030354 -4.1737089 -4.123548 -4.0367007 -3.9587936 -3.9396296 -3.9702308 -4.0515523 -4.1221671 -4.1837549 -4.2328157 -4.2595224][-4.2539725 -4.2567015 -4.2539148 -4.2443528 -4.2260885 -4.1958637 -4.1434712 -4.0918975 -4.0587182 -4.0504389 -4.0871229 -4.1371641 -4.1919632 -4.2408695 -4.2688189][-4.2451096 -4.2609038 -4.273603 -4.2761493 -4.2703381 -4.2561035 -4.223588 -4.18068 -4.1265135 -4.0800605 -4.074234 -4.1037092 -4.1537967 -4.2124028 -4.2563038][-4.2452946 -4.2637596 -4.2807527 -4.2870727 -4.2891097 -4.2885118 -4.2711973 -4.2382631 -4.1790004 -4.1034055 -4.0538545 -4.0505295 -4.0863037 -4.153255 -4.2182217][-4.253345 -4.267312 -4.2779222 -4.2803435 -4.2814445 -4.2832632 -4.2746549 -4.2513142 -4.2016077 -4.1262441 -4.0554137 -4.0224543 -4.0388756 -4.1055694 -4.1856756]]...]
INFO - root - 2017-12-05 18:59:29.295108: step 35810, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.828 sec/batch; 68h:11m:51s remains)
INFO - root - 2017-12-05 18:59:37.836688: step 35820, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 70h:38m:40s remains)
INFO - root - 2017-12-05 18:59:46.464423: step 35830, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 71h:13m:21s remains)
INFO - root - 2017-12-05 18:59:54.923658: step 35840, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 70h:41m:40s remains)
INFO - root - 2017-12-05 19:00:03.551633: step 35850, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 70h:45m:12s remains)
INFO - root - 2017-12-05 19:00:12.151162: step 35860, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 0.798 sec/batch; 65h:46m:38s remains)
INFO - root - 2017-12-05 19:00:20.764875: step 35870, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 71h:03m:00s remains)
INFO - root - 2017-12-05 19:00:29.353784: step 35880, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 71h:15m:50s remains)
INFO - root - 2017-12-05 19:00:37.830329: step 35890, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 69h:41m:36s remains)
INFO - root - 2017-12-05 19:00:46.334397: step 35900, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 73h:06m:28s remains)
2017-12-05 19:00:47.143231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1665258 -4.1801572 -4.1928296 -4.19719 -4.2000346 -4.2017155 -4.1944461 -4.1804581 -4.165648 -4.1671081 -4.195981 -4.2279997 -4.2438364 -4.2511568 -4.2395043][-4.1653876 -4.1748061 -4.1858644 -4.1878638 -4.1852007 -4.1821933 -4.17565 -4.165719 -4.1523027 -4.15626 -4.1904173 -4.2359266 -4.2636003 -4.2720222 -4.2501006][-4.179882 -4.1779332 -4.1798682 -4.1778398 -4.1668448 -4.15071 -4.1389461 -4.1406717 -4.1422963 -4.1570168 -4.1953235 -4.2513165 -4.2928505 -4.30475 -4.2776389][-4.2086806 -4.1971545 -4.1849785 -4.1715717 -4.150897 -4.1193066 -4.0987797 -4.1117721 -4.1308541 -4.1548905 -4.1928263 -4.2526665 -4.3031559 -4.3224049 -4.3017693][-4.2373543 -4.2194571 -4.1910648 -4.16235 -4.1326022 -4.0939722 -4.0706964 -4.0813894 -4.1027083 -4.1303267 -4.1723709 -4.2348275 -4.2890234 -4.3156052 -4.3100848][-4.2497096 -4.2307281 -4.1948848 -4.1582861 -4.125751 -4.08461 -4.0508432 -4.0368571 -4.0376606 -4.0707293 -4.1378112 -4.2125087 -4.2713003 -4.3052282 -4.311245][-4.2553349 -4.2433057 -4.2117329 -4.1758666 -4.1358986 -4.084682 -4.0266619 -3.9701078 -3.9371057 -3.9872169 -4.095294 -4.1878972 -4.2511272 -4.2869925 -4.2995253][-4.2547035 -4.2534094 -4.2295074 -4.1962657 -4.1493449 -4.0855365 -4.0117941 -3.9300704 -3.8827953 -3.9466059 -4.0715241 -4.1639285 -4.2267213 -4.2608647 -4.2715969][-4.2317996 -4.2430139 -4.2301669 -4.2039289 -4.1580129 -4.0938482 -4.0274572 -3.9645224 -3.9301891 -3.9737732 -4.0663052 -4.1418047 -4.203074 -4.23339 -4.2430129][-4.1937017 -4.2117934 -4.2114062 -4.1981416 -4.1652732 -4.119813 -4.0744972 -4.0354891 -4.0092678 -4.0144038 -4.0617266 -4.119319 -4.1722126 -4.1990309 -4.2136555][-4.1647754 -4.1803474 -4.186337 -4.1808348 -4.166532 -4.1499915 -4.1247015 -4.0990739 -4.0676351 -4.0393238 -4.0539622 -4.1021576 -4.1481495 -4.1724916 -4.1909494][-4.1545649 -4.1627173 -4.1649375 -4.1600237 -4.1549196 -4.1535964 -4.1448836 -4.1329641 -4.1034985 -4.0597324 -4.0645142 -4.1105347 -4.1435642 -4.1555758 -4.1615505][-4.1522903 -4.1523848 -4.1464906 -4.1387129 -4.1334877 -4.1381178 -4.1418381 -4.1453238 -4.1309013 -4.0975266 -4.1032538 -4.1310458 -4.1362929 -4.1257257 -4.1191258][-4.1633396 -4.1564016 -4.1467929 -4.1357346 -4.1243782 -4.131103 -4.146163 -4.1643453 -4.1683664 -4.1530991 -4.1507621 -4.148407 -4.1263022 -4.1033707 -4.0946994][-4.1779628 -4.1694522 -4.1606874 -4.1458368 -4.1301522 -4.1410708 -4.1656861 -4.1898417 -4.1995473 -4.1883168 -4.1767507 -4.155602 -4.1218529 -4.0988946 -4.0943441]]...]
INFO - root - 2017-12-05 19:00:55.623197: step 35910, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.829 sec/batch; 68h:17m:27s remains)
INFO - root - 2017-12-05 19:01:04.077234: step 35920, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 70h:46m:39s remains)
INFO - root - 2017-12-05 19:01:12.685668: step 35930, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 70h:11m:48s remains)
INFO - root - 2017-12-05 19:01:21.038763: step 35940, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 69h:13m:43s remains)
INFO - root - 2017-12-05 19:01:29.667704: step 35950, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 72h:31m:03s remains)
INFO - root - 2017-12-05 19:01:38.275608: step 35960, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 71h:46m:37s remains)
INFO - root - 2017-12-05 19:01:46.789951: step 35970, loss = 2.04, batch loss = 1.98 (10.2 examples/sec; 0.784 sec/batch; 64h:36m:41s remains)
INFO - root - 2017-12-05 19:01:55.251953: step 35980, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 70h:54m:07s remains)
INFO - root - 2017-12-05 19:02:03.897895: step 35990, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 70h:48m:45s remains)
INFO - root - 2017-12-05 19:02:12.453036: step 36000, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 69h:06m:19s remains)
2017-12-05 19:02:13.314452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2328243 -4.2076025 -4.18559 -4.1769085 -4.1716127 -4.1633859 -4.1608996 -4.1751738 -4.1916904 -4.2047367 -4.23533 -4.2670307 -4.2838221 -4.2797961 -4.2654619][-4.2277069 -4.2030072 -4.169981 -4.1416864 -4.11816 -4.101378 -4.1040211 -4.1377778 -4.1795959 -4.2095351 -4.2460327 -4.2740283 -4.2849016 -4.2774439 -4.261044][-4.2267847 -4.2118297 -4.1733522 -4.1264997 -4.0819607 -4.0474019 -4.0405712 -4.0847874 -4.1502571 -4.1984639 -4.2428341 -4.2724919 -4.2804189 -4.2719851 -4.2552285][-4.2238779 -4.2190409 -4.1779318 -4.1180882 -4.0478811 -3.9841163 -3.955337 -4.0045972 -4.0969214 -4.170042 -4.2278261 -4.263248 -4.2714691 -4.262588 -4.2458315][-4.2086487 -4.217391 -4.177146 -4.1055942 -4.0062103 -3.9011054 -3.8389003 -3.9000793 -4.0286026 -4.1299634 -4.2027941 -4.2465219 -4.2595682 -4.2541127 -4.2373757][-4.1817617 -4.2036715 -4.1718855 -4.1015158 -3.9805648 -3.8218207 -3.7081182 -3.7903018 -3.9595938 -4.0823669 -4.1669965 -4.2242007 -4.2486968 -4.2485 -4.2319522][-4.1648927 -4.1982985 -4.1775336 -4.1182237 -4.0010762 -3.8228438 -3.6779251 -3.7586827 -3.9240599 -4.0393128 -4.124835 -4.1942763 -4.2351913 -4.2441359 -4.2314439][-4.1708589 -4.2065425 -4.1951036 -4.1536007 -4.0730715 -3.9443216 -3.8282051 -3.8592179 -3.9595964 -4.0346103 -4.10362 -4.1731677 -4.224761 -4.2416978 -4.2336717][-4.1809282 -4.2069993 -4.201654 -4.1800342 -4.1404104 -4.0736866 -3.994565 -3.9833288 -4.0209556 -4.0554419 -4.0977664 -4.1534381 -4.2075672 -4.234489 -4.2343836][-4.1744666 -4.1904149 -4.183939 -4.17478 -4.16819 -4.1426592 -4.0850911 -4.0470819 -4.0401816 -4.0454793 -4.0692706 -4.1148844 -4.1722808 -4.2113218 -4.2234707][-4.1625633 -4.1722455 -4.1657028 -4.164135 -4.1785316 -4.1757493 -4.128273 -4.0729189 -4.0376329 -4.0231662 -4.03616 -4.0795789 -4.1400485 -4.1881161 -4.2102275][-4.1588097 -4.1676316 -4.1628909 -4.1659989 -4.1901336 -4.1981764 -4.1647439 -4.1108489 -4.0622506 -4.0323629 -4.0387135 -4.07911 -4.13025 -4.1758289 -4.2049942][-4.1628776 -4.1743288 -4.1763783 -4.1893716 -4.2175894 -4.2310915 -4.2104754 -4.1700487 -4.1202888 -4.0863619 -4.0887828 -4.1156588 -4.1496611 -4.1860428 -4.2179976][-4.1807828 -4.1979628 -4.2070661 -4.2255731 -4.2523332 -4.2648635 -4.24957 -4.2193336 -4.1814995 -4.1546445 -4.1568131 -4.1732106 -4.1941643 -4.2208128 -4.2476311][-4.1931629 -4.2175627 -4.2341022 -4.2513452 -4.2730193 -4.2836242 -4.2715731 -4.2474008 -4.2214141 -4.2043161 -4.2082829 -4.2193446 -4.2370076 -4.2610927 -4.2820268]]...]
INFO - root - 2017-12-05 19:02:21.878020: step 36010, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 71h:32m:18s remains)
INFO - root - 2017-12-05 19:02:30.425200: step 36020, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 70h:41m:44s remains)
INFO - root - 2017-12-05 19:02:38.981131: step 36030, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 71h:57m:26s remains)
INFO - root - 2017-12-05 19:02:47.514505: step 36040, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 67h:46m:52s remains)
INFO - root - 2017-12-05 19:02:56.123211: step 36050, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 70h:24m:08s remains)
INFO - root - 2017-12-05 19:03:04.718841: step 36060, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 70h:41m:02s remains)
INFO - root - 2017-12-05 19:03:13.336181: step 36070, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 71h:39m:52s remains)
INFO - root - 2017-12-05 19:03:21.928846: step 36080, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 0.783 sec/batch; 64h:27m:11s remains)
INFO - root - 2017-12-05 19:03:30.542196: step 36090, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.873 sec/batch; 71h:52m:02s remains)
INFO - root - 2017-12-05 19:03:39.125971: step 36100, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 71h:19m:27s remains)
2017-12-05 19:03:39.824271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2221 -4.2321162 -4.2433271 -4.2495914 -4.2495241 -4.2427511 -4.2383022 -4.2422652 -4.2512746 -4.2587571 -4.2646914 -4.2731843 -4.2760572 -4.2667851 -4.2542057][-4.1639161 -4.1792488 -4.1994777 -4.2128773 -4.2126231 -4.2028823 -4.1948056 -4.2005577 -4.21563 -4.2269363 -4.2348409 -4.2472572 -4.2514176 -4.2410564 -4.2272053][-4.1349497 -4.1490269 -4.1698647 -4.1862082 -4.1859174 -4.1739545 -4.161644 -4.1666236 -4.1880383 -4.2076144 -4.2194743 -4.2319894 -4.2348275 -4.2246242 -4.2112012][-4.1392508 -4.1478572 -4.1632156 -4.1781464 -4.1758718 -4.1615443 -4.1431413 -4.1439114 -4.169054 -4.1979675 -4.2162666 -4.2291675 -4.2305565 -4.2224035 -4.2112784][-4.1539316 -4.1587539 -4.1680765 -4.1771483 -4.1706047 -4.1536889 -4.131494 -4.1276526 -4.1496391 -4.1841493 -4.2107821 -4.2279387 -4.2304749 -4.2254419 -4.2182651][-4.1869812 -4.1903195 -4.1936712 -4.192893 -4.1817818 -4.1608677 -4.1320019 -4.12171 -4.1409483 -4.1783662 -4.2126803 -4.2348933 -4.2377295 -4.2327924 -4.2277651][-4.23298 -4.2304115 -4.2247639 -4.2132339 -4.1973519 -4.1719837 -4.1325655 -4.1113005 -4.1270905 -4.1696854 -4.2131777 -4.2399654 -4.2422314 -4.2348223 -4.2305851][-4.272131 -4.2613378 -4.2472677 -4.228693 -4.2093158 -4.1837931 -4.1378889 -4.10069 -4.1069422 -4.1527181 -4.2060733 -4.2425113 -4.24743 -4.2371111 -4.2315917][-4.2904196 -4.2745843 -4.2570906 -4.2392921 -4.2193828 -4.1948638 -4.1468592 -4.0980415 -4.0945272 -4.137722 -4.199748 -4.2449994 -4.2527857 -4.2417579 -4.2335615][-4.2908821 -4.2723608 -4.252985 -4.2387652 -4.2221293 -4.2004681 -4.1587796 -4.1108541 -4.1002612 -4.1382432 -4.1969051 -4.2406378 -4.2478857 -4.23782 -4.2298617][-4.2835321 -4.2640085 -4.2430549 -4.2305527 -4.2220626 -4.2084966 -4.1804714 -4.1453376 -4.1326809 -4.1579523 -4.1994982 -4.2323694 -4.235889 -4.2286086 -4.2215877][-4.2642689 -4.246377 -4.2282147 -4.2209692 -4.2224064 -4.2174678 -4.2036967 -4.1834855 -4.1683869 -4.1785407 -4.2028937 -4.2242422 -4.2240543 -4.2173486 -4.2116785][-4.2400017 -4.2245092 -4.2112327 -4.2101078 -4.2156677 -4.2127228 -4.20779 -4.1934967 -4.1753669 -4.1817532 -4.1982145 -4.212914 -4.2139573 -4.2089729 -4.2046232][-4.221539 -4.2096295 -4.2031379 -4.2050605 -4.2073293 -4.2009754 -4.2004042 -4.1892743 -4.1721516 -4.1777234 -4.1908817 -4.2038741 -4.2085567 -4.2083774 -4.2049947][-4.2146282 -4.2088337 -4.2111487 -4.215219 -4.2085376 -4.1937504 -4.1936617 -4.1827712 -4.1694603 -4.1754942 -4.1905684 -4.2052431 -4.2146363 -4.2177281 -4.2152467]]...]
INFO - root - 2017-12-05 19:03:48.167821: step 36110, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 71h:33m:46s remains)
INFO - root - 2017-12-05 19:03:56.659095: step 36120, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 70h:23m:40s remains)
INFO - root - 2017-12-05 19:04:05.159833: step 36130, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 70h:11m:45s remains)
INFO - root - 2017-12-05 19:04:13.567874: step 36140, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.838 sec/batch; 68h:57m:54s remains)
INFO - root - 2017-12-05 19:04:22.016757: step 36150, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 68h:54m:14s remains)
INFO - root - 2017-12-05 19:04:30.475576: step 36160, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 69h:52m:31s remains)
INFO - root - 2017-12-05 19:04:39.033239: step 36170, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 71h:03m:39s remains)
INFO - root - 2017-12-05 19:04:47.458790: step 36180, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 69h:09m:35s remains)
INFO - root - 2017-12-05 19:04:55.976621: step 36190, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 70h:26m:04s remains)
INFO - root - 2017-12-05 19:05:04.618661: step 36200, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 72h:12m:06s remains)
2017-12-05 19:05:05.357887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2435732 -4.2500544 -4.2355862 -4.1990438 -4.1190228 -3.9959495 -3.8615191 -3.8226273 -3.9246283 -4.0615687 -4.1816378 -4.2715111 -4.3181882 -4.3357925 -4.3358746][-4.2415733 -4.2408724 -4.2248607 -4.191812 -4.1182528 -4.0032606 -3.8724034 -3.8351192 -3.932744 -4.0659342 -4.1848731 -4.2742772 -4.3200712 -4.3384008 -4.3379683][-4.2317452 -4.2221189 -4.20669 -4.1829152 -4.1250453 -4.0288615 -3.9177318 -3.8910613 -3.97946 -4.0981617 -4.203239 -4.2818961 -4.3232727 -4.3401389 -4.3396211][-4.2295837 -4.2132869 -4.1964164 -4.1781311 -4.1321497 -4.0508895 -3.9595025 -3.9438863 -4.0275645 -4.132422 -4.2229352 -4.2906489 -4.3282318 -4.3416476 -4.3402405][-4.237083 -4.219605 -4.2011042 -4.1802697 -4.1336622 -4.0560808 -3.9739738 -3.9655397 -4.0494947 -4.1491203 -4.2337627 -4.295938 -4.3313003 -4.3421397 -4.3397226][-4.2438865 -4.2287703 -4.2100806 -4.1850224 -4.1311569 -4.0470891 -3.9609964 -3.95373 -4.0405369 -4.1432662 -4.230341 -4.2935376 -4.3297253 -4.3411651 -4.3387027][-4.2397304 -4.2298026 -4.2164364 -4.1916385 -4.13151 -4.036797 -3.9394274 -3.9284317 -4.0183086 -4.1269088 -4.2187881 -4.2867365 -4.3255267 -4.3392768 -4.3379903][-4.2267189 -4.2225323 -4.2157545 -4.19491 -4.1340303 -4.0323386 -3.9231696 -3.9051175 -3.9962704 -4.1089697 -4.2051277 -4.2781687 -4.3204212 -4.3372078 -4.3375626][-4.2133842 -4.2139893 -4.2124128 -4.1962333 -4.1379952 -4.0367622 -3.9253464 -3.9035263 -3.9923463 -4.1029024 -4.1987138 -4.273387 -4.3179183 -4.3363528 -4.3372474][-4.2109427 -4.2126265 -4.2124557 -4.1992393 -4.1465335 -4.0537639 -3.9510589 -3.9325228 -4.0146456 -4.1154594 -4.2045059 -4.2758255 -4.319376 -4.3369069 -4.3369837][-4.2168579 -4.2176323 -4.2160273 -4.2042427 -4.1603003 -4.079731 -3.9889655 -3.9754753 -4.050817 -4.1392303 -4.2187047 -4.2845931 -4.3245091 -4.338881 -4.3371735][-4.2204909 -4.2211952 -4.2183084 -4.2077494 -4.1721816 -4.1021762 -4.0216331 -4.0116477 -4.0823259 -4.1609445 -4.2334547 -4.2941508 -4.3299155 -4.3409429 -4.3377428][-4.2116337 -4.2125235 -4.2090483 -4.2006931 -4.1689048 -4.1030288 -4.0283308 -4.0221815 -4.0937719 -4.1707582 -4.2428665 -4.3007731 -4.333313 -4.342206 -4.3381915][-4.2006674 -4.2009339 -4.1960855 -4.1891823 -4.15816 -4.0922184 -4.0196314 -4.0185823 -4.0948734 -4.1748013 -4.2462597 -4.30206 -4.3328323 -4.3412418 -4.3378386][-4.1978369 -4.197073 -4.1916952 -4.1860962 -4.1533203 -4.0863781 -4.0180678 -4.0241308 -4.1043873 -4.1839371 -4.2500029 -4.30191 -4.3304925 -4.3389907 -4.3371339]]...]
INFO - root - 2017-12-05 19:05:13.974862: step 36210, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.869 sec/batch; 71h:30m:32s remains)
INFO - root - 2017-12-05 19:05:22.492144: step 36220, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 70h:48m:02s remains)
INFO - root - 2017-12-05 19:05:31.062532: step 36230, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 70h:11m:22s remains)
INFO - root - 2017-12-05 19:05:39.462119: step 36240, loss = 2.11, batch loss = 2.05 (10.7 examples/sec; 0.747 sec/batch; 61h:26m:14s remains)
INFO - root - 2017-12-05 19:05:47.989234: step 36250, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 69h:31m:18s remains)
INFO - root - 2017-12-05 19:05:56.483067: step 36260, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 70h:20m:44s remains)
INFO - root - 2017-12-05 19:06:04.989907: step 36270, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 69h:29m:04s remains)
INFO - root - 2017-12-05 19:06:13.556678: step 36280, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 70h:38m:11s remains)
INFO - root - 2017-12-05 19:06:22.037037: step 36290, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 70h:36m:53s remains)
INFO - root - 2017-12-05 19:06:30.561580: step 36300, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 68h:58m:47s remains)
2017-12-05 19:06:31.344407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3216152 -4.3115 -4.3043652 -4.3024421 -4.302628 -4.3037925 -4.3038244 -4.30256 -4.2975335 -4.2860346 -4.2756295 -4.2766185 -4.2861109 -4.298821 -4.3181872][-4.3090644 -4.2916245 -4.27678 -4.2702432 -4.267796 -4.2689867 -4.2669005 -4.2607479 -4.2499886 -4.2307763 -4.2159743 -4.21811 -4.2340407 -4.2543964 -4.2820826][-4.2927423 -4.2663145 -4.242116 -4.2296157 -4.2243843 -4.2248349 -4.2198076 -4.2069426 -4.1887765 -4.1680942 -4.1602535 -4.1707187 -4.1932654 -4.2211132 -4.2525015][-4.2734118 -4.2396069 -4.2108159 -4.1992154 -4.19592 -4.196734 -4.1864486 -4.1629796 -4.1445737 -4.1356626 -4.1408257 -4.1546135 -4.1741419 -4.2019329 -4.235755][-4.2561159 -4.21783 -4.1889329 -4.1813951 -4.1809082 -4.1805716 -4.1590633 -4.1184444 -4.1020589 -4.1110587 -4.124824 -4.1401148 -4.1593285 -4.1917114 -4.2299762][-4.2409315 -4.1999435 -4.1711273 -4.1653113 -4.161921 -4.153254 -4.1156478 -4.05312 -4.0366483 -4.0702682 -4.1019392 -4.1205597 -4.1409144 -4.1789374 -4.2223697][-4.2309361 -4.1859336 -4.1543493 -4.1475406 -4.1402559 -4.1178465 -4.0542741 -3.95773 -3.9360578 -4.0063815 -4.0681944 -4.0978303 -4.1196089 -4.1610246 -4.2085757][-4.2249017 -4.175559 -4.1409473 -4.1345215 -4.1279974 -4.1003695 -4.0206251 -3.9014244 -3.8778007 -3.9764965 -4.062501 -4.098321 -4.1201162 -4.1619735 -4.2104096][-4.2225304 -4.1734529 -4.1396203 -4.1332655 -4.1298709 -4.1112466 -4.0507236 -3.9603081 -3.9430077 -4.0221386 -4.0974779 -4.1292739 -4.1496539 -4.1887021 -4.2326303][-4.2312341 -4.1901793 -4.163249 -4.1560407 -4.1503267 -4.1421571 -4.1140685 -4.0693307 -4.0616488 -4.1113892 -4.1637354 -4.1860204 -4.2035003 -4.2372103 -4.2740436][-4.2454967 -4.2153263 -4.1936255 -4.1827083 -4.1715455 -4.1699448 -4.1666875 -4.1525607 -4.1514878 -4.1813831 -4.2155337 -4.23131 -4.2493825 -4.27979 -4.3086696][-4.2643709 -4.2446671 -4.2272577 -4.2124419 -4.1994834 -4.2039814 -4.2139215 -4.2125392 -4.2110176 -4.2243476 -4.2445812 -4.257535 -4.2777748 -4.3071527 -4.329905][-4.2890668 -4.2781329 -4.2661629 -4.2536287 -4.2431431 -4.2483363 -4.2610264 -4.2630363 -4.25709 -4.2582459 -4.2679462 -4.2800946 -4.3026309 -4.3284583 -4.3451476][-4.3120418 -4.3063941 -4.2999496 -4.292676 -4.284502 -4.2852058 -4.2924623 -4.2931504 -4.2854433 -4.2811885 -4.2833595 -4.2919359 -4.3098106 -4.3290644 -4.3412576][-4.3280878 -4.3241186 -4.3201613 -4.3158312 -4.3099189 -4.3071551 -4.3078861 -4.3063335 -4.3007121 -4.2968926 -4.2987881 -4.3052955 -4.3172679 -4.3292179 -4.3363404]]...]
INFO - root - 2017-12-05 19:06:40.052889: step 36310, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 70h:50m:56s remains)
INFO - root - 2017-12-05 19:06:48.699863: step 36320, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 70h:15m:40s remains)
INFO - root - 2017-12-05 19:06:57.232899: step 36330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 71h:46m:54s remains)
INFO - root - 2017-12-05 19:07:05.652654: step 36340, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.780 sec/batch; 64h:09m:56s remains)
INFO - root - 2017-12-05 19:07:14.255994: step 36350, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.863 sec/batch; 70h:58m:30s remains)
INFO - root - 2017-12-05 19:07:22.881195: step 36360, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 70h:47m:23s remains)
INFO - root - 2017-12-05 19:07:31.406211: step 36370, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 71h:27m:40s remains)
INFO - root - 2017-12-05 19:07:39.947323: step 36380, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 68h:50m:10s remains)
INFO - root - 2017-12-05 19:07:48.495897: step 36390, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.828 sec/batch; 68h:07m:41s remains)
INFO - root - 2017-12-05 19:07:57.012504: step 36400, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 70h:59m:55s remains)
2017-12-05 19:07:57.772812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1864805 -4.1551466 -4.1505804 -4.1822338 -4.2118893 -4.2312603 -4.23372 -4.2152705 -4.1802216 -4.1441712 -4.1332827 -4.1506524 -4.1740742 -4.2053328 -4.2350454][-4.1897578 -4.1550217 -4.1480727 -4.1774216 -4.207252 -4.2261467 -4.2311916 -4.2201128 -4.1949935 -4.1625423 -4.1402469 -4.1400847 -4.1565313 -4.1895428 -4.221498][-4.2003632 -4.1699033 -4.1656408 -4.1898227 -4.2125859 -4.2243056 -4.23115 -4.2307267 -4.2213683 -4.2004828 -4.1745248 -4.1589208 -4.1618457 -4.1836953 -4.2070827][-4.2149887 -4.1914887 -4.1897936 -4.2054863 -4.2144761 -4.2163229 -4.224381 -4.2404594 -4.2555342 -4.2560015 -4.2422142 -4.2284184 -4.2217731 -4.2251453 -4.23109][-4.2256002 -4.2104244 -4.2087255 -4.21386 -4.2055354 -4.1949124 -4.2002926 -4.2276464 -4.2610922 -4.277245 -4.2776079 -4.2746806 -4.2708559 -4.2693272 -4.2693491][-4.2336321 -4.2265353 -4.2245255 -4.2189112 -4.192132 -4.16078 -4.1486778 -4.1717057 -4.2143621 -4.2429037 -4.2530866 -4.2599974 -4.2633228 -4.2682753 -4.2751637][-4.2401195 -4.2374663 -4.2350426 -4.2214308 -4.1806087 -4.1251211 -4.0832677 -4.0838184 -4.1235905 -4.1576848 -4.1716905 -4.1856122 -4.1969972 -4.2161946 -4.2448034][-4.2413249 -4.2397161 -4.2390528 -4.2246594 -4.1796474 -4.1109581 -4.0415635 -4.0076251 -4.0293174 -4.0567784 -4.0661588 -4.0751185 -4.0843368 -4.1153069 -4.1703897][-4.2397118 -4.2350879 -4.2357512 -4.2269444 -4.1914887 -4.1323876 -4.0641379 -4.0150881 -4.0128412 -4.0269284 -4.0293956 -4.0276771 -4.02708 -4.0536065 -4.1103711][-4.2510195 -4.2439632 -4.2454724 -4.2466264 -4.2290554 -4.19409 -4.1503439 -4.1119304 -4.100244 -4.1046228 -4.1066103 -4.1048284 -4.1016135 -4.116581 -4.1488767][-4.27345 -4.2639718 -4.2633853 -4.2674365 -4.260674 -4.2419333 -4.2188063 -4.195075 -4.1828952 -4.184505 -4.1902323 -4.1960049 -4.1996827 -4.2100821 -4.2226062][-4.2734857 -4.2615442 -4.2601337 -4.2672033 -4.2665977 -4.2591352 -4.247468 -4.2331381 -4.223628 -4.2241282 -4.2344971 -4.2472987 -4.256979 -4.263001 -4.2645388][-4.2529674 -4.240869 -4.2420454 -4.2549343 -4.2619696 -4.260963 -4.2499509 -4.2343745 -4.2210026 -4.2196064 -4.2334647 -4.2492714 -4.2605228 -4.2633481 -4.2608614][-4.225153 -4.2092853 -4.2104588 -4.2269216 -4.24133 -4.2471852 -4.2362747 -4.2167068 -4.1970677 -4.1922565 -4.2053342 -4.22276 -4.2369442 -4.2412019 -4.2392306][-4.2086883 -4.1868067 -4.1839275 -4.1991763 -4.2144384 -4.2250819 -4.2191405 -4.2024088 -4.1828446 -4.172863 -4.1778212 -4.1915522 -4.2060084 -4.2111983 -4.2099237]]...]
INFO - root - 2017-12-05 19:08:06.265886: step 36410, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.811 sec/batch; 66h:43m:01s remains)
INFO - root - 2017-12-05 19:08:14.738813: step 36420, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 68h:07m:19s remains)
INFO - root - 2017-12-05 19:08:23.222883: step 36430, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 72h:12m:53s remains)
INFO - root - 2017-12-05 19:08:31.786311: step 36440, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 60h:29m:31s remains)
INFO - root - 2017-12-05 19:08:40.326765: step 36450, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 68h:53m:45s remains)
INFO - root - 2017-12-05 19:08:48.858810: step 36460, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 68h:04m:01s remains)
INFO - root - 2017-12-05 19:08:57.276191: step 36470, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 69h:59m:38s remains)
INFO - root - 2017-12-05 19:09:05.851282: step 36480, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 72h:48m:52s remains)
INFO - root - 2017-12-05 19:09:14.381896: step 36490, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.833 sec/batch; 68h:28m:50s remains)
INFO - root - 2017-12-05 19:09:23.150908: step 36500, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 69h:57m:20s remains)
2017-12-05 19:09:23.997168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1939697 -4.2152481 -4.228169 -4.25708 -4.2797356 -4.2821331 -4.280098 -4.2785935 -4.2814178 -4.2828088 -4.2822533 -4.2827811 -4.2760735 -4.251657 -4.2141838][-4.1660223 -4.1872387 -4.1956782 -4.2233114 -4.2494383 -4.2523317 -4.2474585 -4.2448959 -4.2510266 -4.2561417 -4.2571654 -4.2588377 -4.2544079 -4.2303805 -4.1944957][-4.1562738 -4.1721926 -4.1726151 -4.1894026 -4.2070928 -4.205636 -4.1954 -4.1898704 -4.1990967 -4.2138681 -4.2245283 -4.2319112 -4.2338376 -4.2163062 -4.1868939][-4.1659379 -4.170392 -4.1599293 -4.1623869 -4.1667848 -4.15854 -4.13931 -4.1254344 -4.137804 -4.1712184 -4.1972084 -4.2120304 -4.2215824 -4.21378 -4.1937308][-4.167861 -4.16327 -4.14709 -4.135458 -4.1230283 -4.1022868 -4.0662928 -4.0367 -4.0556927 -4.1158404 -4.1631842 -4.1876307 -4.2039914 -4.204 -4.1909881][-4.1527777 -4.148931 -4.1358061 -4.1138835 -4.0795393 -4.0362172 -3.9719546 -3.9137914 -3.9388788 -4.0369816 -4.1163673 -4.1573777 -4.1839404 -4.1888247 -4.1782484][-4.1319637 -4.1379037 -4.1340733 -4.1075311 -4.05187 -3.9824746 -3.882807 -3.7820239 -3.8047028 -3.9440107 -4.0588436 -4.1204009 -4.16029 -4.1734486 -4.1690817][-4.1425405 -4.1625781 -4.1703668 -4.1511693 -4.0951929 -4.0195723 -3.909672 -3.7957942 -3.8079724 -3.9470217 -4.0578628 -4.116776 -4.1569242 -4.1757269 -4.1798][-4.16964 -4.1976066 -4.2150722 -4.2121458 -4.1753483 -4.1199975 -4.0345144 -3.9476523 -3.9484663 -4.0419426 -4.1155128 -4.1518288 -4.1810279 -4.2004356 -4.2068958][-4.1819892 -4.2087398 -4.2299838 -4.2429962 -4.2279525 -4.1964412 -4.1396985 -4.0799117 -4.0736637 -4.1268034 -4.1655326 -4.180614 -4.19635 -4.21272 -4.219893][-4.1902494 -4.21181 -4.2305603 -4.2543268 -4.2541723 -4.2378106 -4.2024426 -4.1636906 -4.1569433 -4.186152 -4.2029624 -4.2033205 -4.205965 -4.2165103 -4.221271][-4.1924057 -4.209341 -4.2269006 -4.2564855 -4.2648993 -4.2576027 -4.2353806 -4.2081523 -4.2017055 -4.2158108 -4.2227645 -4.2201028 -4.2173438 -4.2214193 -4.2193618][-4.1837583 -4.202282 -4.2204075 -4.2534161 -4.2645059 -4.2605991 -4.2464905 -4.2266874 -4.2180829 -4.2230043 -4.2253942 -4.2244034 -4.2210793 -4.2182193 -4.2054243][-4.1576791 -4.1820793 -4.2026343 -4.2384844 -4.2518125 -4.2474456 -4.2376242 -4.2232776 -4.2156987 -4.2173238 -4.218677 -4.2201972 -4.2191648 -4.2109895 -4.1902256][-4.1334324 -4.1659017 -4.1924858 -4.2318807 -4.249414 -4.2457957 -4.2401705 -4.2307205 -4.2240381 -4.2231865 -4.2234159 -4.2246871 -4.2230725 -4.2095289 -4.185164]]...]
INFO - root - 2017-12-05 19:09:32.445858: step 36510, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 70h:17m:56s remains)
INFO - root - 2017-12-05 19:09:40.959913: step 36520, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 71h:29m:21s remains)
INFO - root - 2017-12-05 19:09:49.300683: step 36530, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 71h:08m:56s remains)
INFO - root - 2017-12-05 19:09:57.723958: step 36540, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.804 sec/batch; 66h:07m:59s remains)
INFO - root - 2017-12-05 19:10:06.327386: step 36550, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 68h:18m:20s remains)
INFO - root - 2017-12-05 19:10:14.920052: step 36560, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 71h:50m:58s remains)
INFO - root - 2017-12-05 19:10:23.418717: step 36570, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 71h:31m:39s remains)
INFO - root - 2017-12-05 19:10:32.002432: step 36580, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 68h:44m:59s remains)
INFO - root - 2017-12-05 19:10:40.566825: step 36590, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 70h:26m:48s remains)
INFO - root - 2017-12-05 19:10:49.167074: step 36600, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 70h:49m:32s remains)
2017-12-05 19:10:49.911942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2719021 -4.2634416 -4.2643809 -4.2721815 -4.2805481 -4.284667 -4.2863083 -4.2908936 -4.2971559 -4.3040261 -4.3076992 -4.3083067 -4.3063927 -4.3041687 -4.3037467][-4.2371168 -4.2256179 -4.2280512 -4.2395391 -4.2514715 -4.2569733 -4.2574878 -4.2630048 -4.2749305 -4.2899718 -4.3009028 -4.3056188 -4.3045015 -4.3007483 -4.2980127][-4.2015285 -4.1869693 -4.1899881 -4.2042589 -4.2211084 -4.2252789 -4.2175207 -4.2182641 -4.2341518 -4.2595139 -4.2807851 -4.2944989 -4.298944 -4.2976422 -4.2935371][-4.1822586 -4.1643124 -4.1632128 -4.1733441 -4.1871715 -4.1825042 -4.1623955 -4.157414 -4.1775408 -4.2123842 -4.2458463 -4.2719245 -4.2870293 -4.2925639 -4.2903423][-4.1851 -4.1626306 -4.1508646 -4.1490011 -4.1504927 -4.1280885 -4.0898886 -4.0787172 -4.1059566 -4.1524868 -4.1987076 -4.2359214 -4.2617445 -4.273747 -4.273881][-4.205296 -4.1790195 -4.1556654 -4.1359973 -4.1136894 -4.0628786 -3.9951794 -3.9704659 -4.0098453 -4.07622 -4.137361 -4.1853595 -4.2216454 -4.2421188 -4.2484245][-4.22219 -4.197382 -4.1727824 -4.1431594 -4.1010318 -4.0227709 -3.9179695 -3.8667376 -3.9165833 -4.0036011 -4.0749803 -4.1316838 -4.178071 -4.2044473 -4.2205877][-4.22917 -4.2084441 -4.1925864 -4.1685281 -4.1252604 -4.0427809 -3.9321406 -3.8761814 -3.9221902 -3.9985518 -4.0510225 -4.0972261 -4.1448855 -4.1738362 -4.1949492][-4.2328734 -4.2201333 -4.2164249 -4.2026234 -4.1688108 -4.1024046 -4.0259118 -3.9935358 -4.0258765 -4.0696683 -4.0876646 -4.1062942 -4.140274 -4.1668787 -4.1863456][-4.2425733 -4.236589 -4.2392793 -4.2299032 -4.206327 -4.1617694 -4.1198549 -4.1099334 -4.1334243 -4.1546793 -4.1490803 -4.1424074 -4.1579151 -4.1778178 -4.1939263][-4.2653985 -4.2619543 -4.264616 -4.2546272 -4.2340813 -4.200295 -4.1764989 -4.1796303 -4.2003875 -4.2117696 -4.1995173 -4.1835647 -4.1886082 -4.2012696 -4.2124119][-4.2908897 -4.2886882 -4.2879381 -4.2753515 -4.2543349 -4.2274475 -4.2141042 -4.2211175 -4.2367678 -4.2409668 -4.2307491 -4.219305 -4.2213912 -4.2293859 -4.235456][-4.3127604 -4.311554 -4.3095655 -4.2980762 -4.280509 -4.262742 -4.2549367 -4.2562976 -4.2595081 -4.2567053 -4.2499971 -4.2452688 -4.2498255 -4.2574162 -4.26147][-4.3271046 -4.3271389 -4.3267851 -4.3204918 -4.3099003 -4.30041 -4.2953916 -4.2931981 -4.2890778 -4.2833357 -4.2782674 -4.2770963 -4.2812533 -4.2854342 -4.2885041][-4.3337321 -4.3349156 -4.336587 -4.3349476 -4.3301888 -4.3254991 -4.322258 -4.3196955 -4.3155317 -4.3112226 -4.3085923 -4.3081884 -4.310091 -4.3125963 -4.313838]]...]
INFO - root - 2017-12-05 19:10:58.452901: step 36610, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 70h:35m:25s remains)
INFO - root - 2017-12-05 19:11:07.048431: step 36620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 70h:09m:31s remains)
INFO - root - 2017-12-05 19:11:15.578381: step 36630, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.823 sec/batch; 67h:38m:15s remains)
INFO - root - 2017-12-05 19:11:24.030512: step 36640, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.811 sec/batch; 66h:39m:26s remains)
INFO - root - 2017-12-05 19:11:32.500199: step 36650, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 71h:50m:40s remains)
INFO - root - 2017-12-05 19:11:41.021402: step 36660, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 69h:32m:55s remains)
INFO - root - 2017-12-05 19:11:49.545011: step 36670, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 72h:13m:35s remains)
INFO - root - 2017-12-05 19:11:58.147654: step 36680, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.847 sec/batch; 69h:33m:47s remains)
INFO - root - 2017-12-05 19:12:06.739377: step 36690, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 72h:33m:06s remains)
INFO - root - 2017-12-05 19:12:15.268113: step 36700, loss = 2.02, batch loss = 1.96 (9.3 examples/sec; 0.863 sec/batch; 70h:55m:53s remains)
2017-12-05 19:12:16.078238: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2220678 -4.2249413 -4.2350636 -4.2410789 -4.2440863 -4.2480068 -4.256752 -4.2648416 -4.2682843 -4.2728105 -4.2767539 -4.2749648 -4.268702 -4.2592111 -4.2537766][-4.1739187 -4.1759343 -4.1865969 -4.194242 -4.1976633 -4.201467 -4.2134018 -4.2256808 -4.2339826 -4.2436385 -4.2529907 -4.2553234 -4.2502041 -4.2428379 -4.238019][-4.1264906 -4.1277523 -4.1393085 -4.1509142 -4.1576719 -4.1573496 -4.1619334 -4.1747665 -4.1872988 -4.202961 -4.2199469 -4.2278624 -4.2247643 -4.2213163 -4.219728][-4.0820622 -4.0811334 -4.0949831 -4.1084223 -4.1092877 -4.09737 -4.091928 -4.1067152 -4.12611 -4.1513724 -4.1792183 -4.1974726 -4.1984 -4.1952829 -4.1940088][-4.056941 -4.0587573 -4.0748649 -4.0766163 -4.0565457 -4.0257397 -4.0069828 -4.0223575 -4.052855 -4.0924606 -4.1304793 -4.1546569 -4.1620445 -4.164156 -4.1670609][-4.0614281 -4.0639124 -4.0670247 -4.0433822 -3.9977624 -3.9495211 -3.9220853 -3.9432926 -3.9931946 -4.0476604 -4.0909228 -4.1158581 -4.1265235 -4.134655 -4.1419129][-4.0944881 -4.0821576 -4.0544357 -3.9935377 -3.9203506 -3.866642 -3.8512497 -3.8825636 -3.9492013 -4.0152454 -4.0649457 -4.0934157 -4.1047683 -4.1135788 -4.1193843][-4.1372867 -4.1076837 -4.0483313 -3.9600139 -3.8750639 -3.8268189 -3.8309956 -3.8789322 -3.9534037 -4.020256 -4.0714865 -4.1019812 -4.112587 -4.1218619 -4.129982][-4.1856809 -4.147501 -4.0790448 -3.9892166 -3.9114666 -3.8763874 -3.9052911 -3.9697108 -4.0394273 -4.0895023 -4.1234756 -4.1460004 -4.1566257 -4.1707287 -4.1834297][-4.2268953 -4.1921506 -4.1328378 -4.0562067 -3.991658 -3.9714921 -4.0144038 -4.0859833 -4.1417212 -4.1735682 -4.1899595 -4.2034802 -4.2130928 -4.2274928 -4.2408047][-4.254427 -4.2239847 -4.1775455 -4.1173987 -4.0690231 -4.0588789 -4.0997596 -4.1591558 -4.200748 -4.222508 -4.22979 -4.2402816 -4.2514343 -4.2643251 -4.272646][-4.2747736 -4.2474265 -4.2116361 -4.1690216 -4.1367373 -4.133266 -4.1677613 -4.2140055 -4.242907 -4.2547007 -4.2527456 -4.2551384 -4.2611847 -4.2681484 -4.2712507][-4.2785544 -4.2563963 -4.2326522 -4.2071786 -4.1910939 -4.1943178 -4.2213817 -4.2539992 -4.2682376 -4.2709775 -4.2639313 -4.2586875 -4.2555094 -4.2560654 -4.2564692][-4.2688513 -4.253686 -4.2421441 -4.2310653 -4.2273254 -4.2335577 -4.2518559 -4.2715721 -4.2778215 -4.2763844 -4.2693691 -4.2617126 -4.2541575 -4.2494745 -4.2473016][-4.2673092 -4.2592382 -4.2549152 -4.25325 -4.2549815 -4.2595978 -4.2724996 -4.2850952 -4.2890487 -4.287837 -4.2830791 -4.2753258 -4.2663264 -4.2589684 -4.2550983]]...]
INFO - root - 2017-12-05 19:12:24.685800: step 36710, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 73h:11m:04s remains)
INFO - root - 2017-12-05 19:12:33.272600: step 36720, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.843 sec/batch; 69h:17m:15s remains)
INFO - root - 2017-12-05 19:12:41.806103: step 36730, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 70h:08m:38s remains)
INFO - root - 2017-12-05 19:12:50.331191: step 36740, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.754 sec/batch; 61h:57m:06s remains)
INFO - root - 2017-12-05 19:12:58.866516: step 36750, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 75h:40m:13s remains)
INFO - root - 2017-12-05 19:13:07.422344: step 36760, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 71h:26m:00s remains)
INFO - root - 2017-12-05 19:13:16.081343: step 36770, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 71h:03m:08s remains)
INFO - root - 2017-12-05 19:13:24.671427: step 36780, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 69h:33m:54s remains)
INFO - root - 2017-12-05 19:13:33.297352: step 36790, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 71h:43m:44s remains)
INFO - root - 2017-12-05 19:13:41.797331: step 36800, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 70h:21m:04s remains)
2017-12-05 19:13:42.581735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2065859 -4.1805038 -4.152576 -4.1410971 -4.1385231 -4.13388 -4.1375403 -4.1451521 -4.1543808 -4.1727953 -4.1850648 -4.17972 -4.1580315 -4.1401124 -4.1319432][-4.2068253 -4.16177 -4.1215134 -4.1080008 -4.1044483 -4.0943303 -4.0965557 -4.112864 -4.1309624 -4.1555648 -4.1724133 -4.1643071 -4.133039 -4.1088924 -4.0996909][-4.2117805 -4.1631837 -4.114933 -4.0891662 -4.071033 -4.0387816 -4.028759 -4.0487685 -4.0810242 -4.1227803 -4.1569958 -4.1591306 -4.1268239 -4.0998268 -4.0887346][-4.2107768 -4.1709547 -4.1282878 -4.09816 -4.0682425 -4.0095592 -3.9715481 -3.9818227 -4.0243211 -4.0866404 -4.1458645 -4.1706219 -4.1488047 -4.1231914 -4.1090555][-4.2104959 -4.1813703 -4.153728 -4.1326108 -4.0995917 -4.0298176 -3.9654405 -3.9423776 -3.9670982 -4.0352759 -4.1171288 -4.1671576 -4.16349 -4.1480126 -4.1317983][-4.1954088 -4.1780539 -4.1675444 -4.1611547 -4.1366491 -4.0782309 -4.0158792 -3.9770837 -3.9739547 -4.0185351 -4.0969439 -4.1568527 -4.1653371 -4.163084 -4.1501985][-4.1686692 -4.1655483 -4.1741548 -4.1867442 -4.1760669 -4.1380262 -4.093184 -4.0627718 -4.0515461 -4.0721064 -4.1264386 -4.1730952 -4.1824713 -4.1833496 -4.1741695][-4.1563525 -4.1629329 -4.1804776 -4.2023497 -4.201755 -4.1805992 -4.1486673 -4.1298351 -4.1264358 -4.1391983 -4.171916 -4.199769 -4.2063289 -4.2042418 -4.1888542][-4.1427178 -4.152966 -4.1707368 -4.1901922 -4.1940379 -4.1839175 -4.1694202 -4.1612015 -4.1649675 -4.1801076 -4.200696 -4.2148237 -4.2189345 -4.2145467 -4.1965652][-4.1368256 -4.1385431 -4.1472964 -4.1612382 -4.16343 -4.157877 -4.155396 -4.1573358 -4.1663637 -4.1852846 -4.2051263 -4.2150621 -4.2207808 -4.21751 -4.2009277][-4.1385818 -4.1321449 -4.1310339 -4.1371059 -4.13318 -4.12723 -4.1275034 -4.1364188 -4.1505432 -4.1712623 -4.19385 -4.2069182 -4.2134967 -4.2123632 -4.198164][-4.1490092 -4.1386089 -4.1342816 -4.1351585 -4.1295166 -4.1246862 -4.1265783 -4.1342573 -4.1459937 -4.1640944 -4.1841159 -4.1965952 -4.2032533 -4.2063828 -4.1972246][-4.1687856 -4.1602864 -4.1545267 -4.1570783 -4.1586204 -4.157186 -4.15951 -4.16522 -4.1722426 -4.1816936 -4.1874037 -4.1876721 -4.1928425 -4.2007823 -4.2002816][-4.1874342 -4.1857696 -4.1821909 -4.1851306 -4.1895795 -4.1927538 -4.193047 -4.195435 -4.1986351 -4.1982584 -4.1923718 -4.1858497 -4.1855645 -4.1902409 -4.1953154][-4.1963906 -4.2014937 -4.2005739 -4.2051253 -4.2096987 -4.2114172 -4.2052031 -4.1988807 -4.1985035 -4.1954255 -4.186862 -4.1794305 -4.1761432 -4.1751347 -4.17834]]...]
INFO - root - 2017-12-05 19:13:51.186686: step 36810, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 69h:36m:30s remains)
INFO - root - 2017-12-05 19:13:59.727610: step 36820, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 71h:32m:29s remains)
INFO - root - 2017-12-05 19:14:08.223252: step 36830, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 70h:56m:58s remains)
INFO - root - 2017-12-05 19:14:16.573066: step 36840, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 0.785 sec/batch; 64h:27m:07s remains)
INFO - root - 2017-12-05 19:14:24.953859: step 36850, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.798 sec/batch; 65h:31m:22s remains)
INFO - root - 2017-12-05 19:14:33.365138: step 36860, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 69h:31m:34s remains)
INFO - root - 2017-12-05 19:14:41.941632: step 36870, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 71h:18m:51s remains)
INFO - root - 2017-12-05 19:14:50.562967: step 36880, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.876 sec/batch; 71h:55m:15s remains)
INFO - root - 2017-12-05 19:14:59.097587: step 36890, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.831 sec/batch; 68h:13m:41s remains)
INFO - root - 2017-12-05 19:15:07.696500: step 36900, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 71h:10m:04s remains)
2017-12-05 19:15:08.553557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3111715 -4.307899 -4.2915707 -4.2748108 -4.2652845 -4.2612429 -4.2697639 -4.2810831 -4.2745037 -4.2509604 -4.2122164 -4.1740456 -4.1558943 -4.16772 -4.1921654][-4.3078151 -4.3155055 -4.3122773 -4.3007145 -4.2880011 -4.2780147 -4.2786436 -4.2826252 -4.2763844 -4.2613831 -4.2354388 -4.2119536 -4.2033553 -4.2138119 -4.2261596][-4.2778878 -4.2897139 -4.2932014 -4.2890081 -4.2814822 -4.2760334 -4.2746735 -4.2745328 -4.2707067 -4.2627945 -4.2479734 -4.2379112 -4.240962 -4.2523484 -4.257164][-4.24799 -4.2598467 -4.2636061 -4.2591043 -4.2542114 -4.2529597 -4.2526326 -4.2517991 -4.2540464 -4.256156 -4.2483697 -4.2478371 -4.2588367 -4.2714052 -4.2728667][-4.2177715 -4.2298293 -4.2311192 -4.2242785 -4.2185416 -4.2187567 -4.2207584 -4.220644 -4.2259045 -4.2365403 -4.2350388 -4.2391543 -4.2512026 -4.260169 -4.2583218][-4.1734962 -4.1795015 -4.1781583 -4.1713018 -4.164443 -4.1622372 -4.1612306 -4.1600413 -4.1694579 -4.1872487 -4.1903954 -4.197351 -4.2130604 -4.2200336 -4.2149282][-4.1386871 -4.1295033 -4.1188436 -4.10584 -4.0934567 -4.0867352 -4.0839477 -4.0808358 -4.095479 -4.1193643 -4.1318631 -4.1476746 -4.1715569 -4.1806355 -4.1749134][-4.1515903 -4.1140304 -4.0803828 -4.0491443 -4.0226331 -4.0126481 -4.0099788 -4.0068121 -4.0315056 -4.0712848 -4.1039448 -4.1347475 -4.1642928 -4.1726494 -4.1652308][-4.1947613 -4.1416311 -4.0869155 -4.0383635 -4.0058627 -3.9987416 -4.0005841 -4.0033665 -4.0387392 -4.0929451 -4.1400533 -4.1729703 -4.1951327 -4.1958294 -4.1837335][-4.2353606 -4.1848359 -4.1253195 -4.0713882 -4.0417247 -4.0434494 -4.0562167 -4.0695972 -4.1097279 -4.1650629 -4.2087097 -4.2264786 -4.2303572 -4.2225018 -4.2097287][-4.2537608 -4.2156315 -4.1611528 -4.1055851 -4.0720406 -4.0724 -4.08969 -4.1103606 -4.1504917 -4.1977925 -4.2258124 -4.226831 -4.2177391 -4.2068553 -4.2029653][-4.2630296 -4.2343416 -4.1851349 -4.1257191 -4.0808725 -4.0680814 -4.0782518 -4.0984411 -4.1326628 -4.1727619 -4.1914182 -4.1827793 -4.1652479 -4.15495 -4.1609688][-4.2784433 -4.2586646 -4.2115393 -4.148622 -4.0929527 -4.0646749 -4.0611877 -4.0734692 -4.0962658 -4.1299963 -4.1463122 -4.1339536 -4.111516 -4.1011095 -4.1130114][-4.2862682 -4.2789912 -4.24082 -4.1830487 -4.1284356 -4.0926065 -4.0820937 -4.0881233 -4.1014681 -4.1247888 -4.1390347 -4.1273503 -4.1044235 -4.0923681 -4.101325][-4.2786016 -4.2861919 -4.2651234 -4.2238603 -4.1840129 -4.1553187 -4.1458573 -4.1502037 -4.1558523 -4.1689558 -4.1813879 -4.1755681 -4.1613216 -4.1527104 -4.1550283]]...]
INFO - root - 2017-12-05 19:15:17.261812: step 36910, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 74h:54m:24s remains)
INFO - root - 2017-12-05 19:15:25.791358: step 36920, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.818 sec/batch; 67h:11m:21s remains)
INFO - root - 2017-12-05 19:15:34.342351: step 36930, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 71h:46m:07s remains)
INFO - root - 2017-12-05 19:15:42.935179: step 36940, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 70h:25m:50s remains)
INFO - root - 2017-12-05 19:15:51.393011: step 36950, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 68h:42m:12s remains)
INFO - root - 2017-12-05 19:15:59.875728: step 36960, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 0.776 sec/batch; 63h:43m:20s remains)
INFO - root - 2017-12-05 19:16:08.339795: step 36970, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 69h:59m:25s remains)
INFO - root - 2017-12-05 19:16:16.906522: step 36980, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 69h:46m:27s remains)
INFO - root - 2017-12-05 19:16:25.567477: step 36990, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 70h:25m:08s remains)
INFO - root - 2017-12-05 19:16:34.242614: step 37000, loss = 2.02, batch loss = 1.97 (9.3 examples/sec; 0.860 sec/batch; 70h:34m:24s remains)
2017-12-05 19:16:35.066546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.167922 -4.1144714 -4.0792785 -4.06358 -4.0671258 -4.0927806 -4.1269221 -4.1493616 -4.1639009 -4.1755342 -4.1938448 -4.216579 -4.2328 -4.2277026 -4.1965923][-4.1404533 -4.0961452 -4.0796089 -4.0858684 -4.0989943 -4.1157584 -4.1417613 -4.1536708 -4.1509156 -4.1465282 -4.1588206 -4.1824222 -4.2021642 -4.2011752 -4.1854143][-4.1223845 -4.0890765 -4.0888691 -4.1052322 -4.1210585 -4.1338778 -4.1535487 -4.1534252 -4.1332 -4.1137848 -4.115459 -4.1357741 -4.1608782 -4.1701894 -4.1714282][-4.1214809 -4.0957518 -4.1032977 -4.1262231 -4.1446776 -4.154211 -4.1657834 -4.1548538 -4.1226187 -4.0889211 -4.0703053 -4.07734 -4.1099811 -4.140027 -4.1617384][-4.1388273 -4.1095738 -4.1097655 -4.1261711 -4.1429214 -4.1523285 -4.15739 -4.1417551 -4.1031504 -4.0526047 -4.0093493 -4.0063357 -4.0579157 -4.1163321 -4.1576366][-4.178381 -4.1350174 -4.1108084 -4.1053572 -4.1052647 -4.1044049 -4.1021457 -4.0894938 -4.0593104 -4.0067139 -3.9540584 -3.9477482 -4.0137062 -4.0965872 -4.1541562][-4.215414 -4.1531644 -4.0998993 -4.0671077 -4.0432892 -4.026494 -4.023665 -4.0286589 -4.0217614 -3.986588 -3.9361327 -3.9242735 -3.9928191 -4.0883026 -4.1545548][-4.2255969 -4.1527429 -4.08264 -4.0354071 -4.0042005 -3.9893622 -4.0004878 -4.0273614 -4.0406804 -4.0220594 -3.9791608 -3.9634097 -4.0178819 -4.0994296 -4.1588717][-4.2176762 -4.1438947 -4.0795527 -4.0411015 -4.0252781 -4.0275688 -4.0547876 -4.0922165 -4.1162248 -4.1116281 -4.0770297 -4.0564251 -4.0873075 -4.1422439 -4.1828432][-4.2028117 -4.1438351 -4.0997953 -4.0787778 -4.07568 -4.087008 -4.1152415 -4.1506586 -4.1773891 -4.1785893 -4.1531744 -4.1299086 -4.1411815 -4.1727443 -4.1987896][-4.1929173 -4.1543822 -4.1317048 -4.1234884 -4.1271205 -4.1378436 -4.1574631 -4.1816764 -4.2033973 -4.209342 -4.1956487 -4.1761928 -4.1744108 -4.1903234 -4.208827][-4.1923976 -4.1703353 -4.1643062 -4.170568 -4.1818986 -4.190979 -4.2024875 -4.2166462 -4.231616 -4.2425423 -4.2391357 -4.2271624 -4.2236142 -4.2310104 -4.2419429][-4.2119932 -4.2019396 -4.2063093 -4.2211194 -4.2361426 -4.2462273 -4.2528176 -4.2596779 -4.2701397 -4.2817068 -4.2838292 -4.2774091 -4.2742105 -4.2766728 -4.2819281][-4.2427068 -4.2418194 -4.2523766 -4.2686892 -4.2827945 -4.2903967 -4.2928734 -4.2946215 -4.300611 -4.3090076 -4.312634 -4.3105364 -4.3091636 -4.3092422 -4.3108907][-4.283257 -4.2878895 -4.3001008 -4.3132706 -4.3219409 -4.3257985 -4.3260107 -4.325459 -4.327177 -4.3308816 -4.3326292 -4.3320422 -4.3307905 -4.3286796 -4.3269262]]...]
INFO - root - 2017-12-05 19:16:43.577191: step 37010, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 69h:49m:14s remains)
INFO - root - 2017-12-05 19:16:52.186641: step 37020, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 68h:27m:11s remains)
INFO - root - 2017-12-05 19:17:00.758471: step 37030, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 70h:25m:10s remains)
INFO - root - 2017-12-05 19:17:09.310284: step 37040, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 69h:02m:16s remains)
INFO - root - 2017-12-05 19:17:17.755179: step 37050, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 69h:50m:42s remains)
INFO - root - 2017-12-05 19:17:26.273181: step 37060, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 68h:02m:12s remains)
INFO - root - 2017-12-05 19:17:34.629648: step 37070, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.812 sec/batch; 66h:38m:23s remains)
INFO - root - 2017-12-05 19:17:43.252913: step 37080, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 70h:31m:36s remains)
INFO - root - 2017-12-05 19:17:51.713651: step 37090, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 69h:39m:54s remains)
INFO - root - 2017-12-05 19:18:00.349587: step 37100, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 69h:52m:07s remains)
2017-12-05 19:18:01.069216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2297616 -4.2103071 -4.1967168 -4.1913462 -4.1919494 -4.1913881 -4.1609144 -4.1072574 -4.0690589 -4.0841427 -4.1257029 -4.1601076 -4.18162 -4.190794 -4.1975369][-4.2615252 -4.2456346 -4.2387547 -4.2325063 -4.2236958 -4.2067466 -4.1645322 -4.1034646 -4.0602303 -4.0720272 -4.1247087 -4.1774478 -4.2151546 -4.2326126 -4.2395039][-4.27294 -4.261261 -4.26346 -4.2664795 -4.2629051 -4.2436566 -4.1989412 -4.1390543 -4.0927248 -4.0932426 -4.1412821 -4.1984215 -4.2427793 -4.2652588 -4.2726779][-4.2659121 -4.2598534 -4.2680969 -4.2809858 -4.2884383 -4.2774639 -4.2422204 -4.1931152 -4.1514578 -4.1445932 -4.1783409 -4.2272124 -4.2661271 -4.2866 -4.2947669][-4.2347484 -4.2222867 -4.2252049 -4.2412033 -4.2669034 -4.280807 -4.271162 -4.2425818 -4.2118993 -4.20567 -4.2268877 -4.2636414 -4.2907419 -4.3025236 -4.3069158][-4.1793337 -4.1398997 -4.1223888 -4.1384053 -4.1876597 -4.239749 -4.267468 -4.2663755 -4.2534785 -4.2538424 -4.2692208 -4.2956944 -4.3126197 -4.3178716 -4.315515][-4.1102304 -4.035336 -3.9861548 -3.9923439 -4.058506 -4.1429396 -4.2037883 -4.235199 -4.2514052 -4.270072 -4.2883124 -4.3100357 -4.3228421 -4.327517 -4.3221822][-4.0495372 -3.9486775 -3.868978 -3.859623 -3.9336774 -4.0330696 -4.1103263 -4.1641312 -4.2079916 -4.2513905 -4.28415 -4.3113508 -4.325325 -4.3305793 -4.3263922][-4.0275359 -3.9079783 -3.7983639 -3.7627432 -3.8347487 -3.9449069 -4.0364876 -4.1032844 -4.16275 -4.21885 -4.2656221 -4.2999988 -4.3164935 -4.3213496 -4.3185039][-4.0640097 -3.9434969 -3.815681 -3.7567339 -3.8110816 -3.9149718 -4.0104861 -4.0775466 -4.1349669 -4.1926579 -4.2445021 -4.2824516 -4.3002563 -4.3060255 -4.3049154][-4.1378326 -4.0420809 -3.9356396 -3.883935 -3.91304 -3.98286 -4.0506811 -4.0954704 -4.1386461 -4.1879287 -4.2340584 -4.2665472 -4.2839575 -4.2912021 -4.2915573][-4.2093844 -4.1486454 -4.0892763 -4.0679288 -4.0822363 -4.11474 -4.1418853 -4.1549463 -4.1755471 -4.2100906 -4.2466617 -4.2696624 -4.2815595 -4.2839618 -4.2814503][-4.26165 -4.23184 -4.2097588 -4.2074194 -4.2138062 -4.2277808 -4.235 -4.2336478 -4.2388973 -4.25618 -4.2790031 -4.2917171 -4.2949142 -4.2893019 -4.2814889][-4.29252 -4.2829523 -4.2789216 -4.2814856 -4.2826085 -4.2884135 -4.2922344 -4.2908797 -4.2902584 -4.2952075 -4.3028893 -4.3052626 -4.3015771 -4.2918191 -4.2825255][-4.30456 -4.3026452 -4.3049841 -4.3091469 -4.3100834 -4.3123531 -4.3156486 -4.3170581 -4.3167052 -4.31583 -4.3130589 -4.3092966 -4.3037496 -4.2962756 -4.2910485]]...]
INFO - root - 2017-12-05 19:18:09.518662: step 37110, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 68h:31m:18s remains)
INFO - root - 2017-12-05 19:18:18.040374: step 37120, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.830 sec/batch; 68h:06m:45s remains)
INFO - root - 2017-12-05 19:18:26.687743: step 37130, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 68h:48m:04s remains)
INFO - root - 2017-12-05 19:18:35.300571: step 37140, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 69h:30m:44s remains)
INFO - root - 2017-12-05 19:18:43.684571: step 37150, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 69h:55m:04s remains)
INFO - root - 2017-12-05 19:18:52.156424: step 37160, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 68h:37m:10s remains)
INFO - root - 2017-12-05 19:19:00.677590: step 37170, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 67h:55m:46s remains)
INFO - root - 2017-12-05 19:19:08.971023: step 37180, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 68h:15m:19s remains)
INFO - root - 2017-12-05 19:19:17.496620: step 37190, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 69h:11m:11s remains)
INFO - root - 2017-12-05 19:19:26.145439: step 37200, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 71h:25m:11s remains)
2017-12-05 19:19:26.878397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2886333 -4.2919712 -4.2944217 -4.3009348 -4.3129921 -4.32144 -4.3236566 -4.3221421 -4.3193746 -4.3167562 -4.3149538 -4.3124104 -4.3082628 -4.3026667 -4.2998419][-4.2384558 -4.237421 -4.2370715 -4.2462306 -4.2671933 -4.2813087 -4.2861605 -4.2855163 -4.2824717 -4.2776461 -4.2752891 -4.2737188 -4.2695274 -4.2638788 -4.2637239][-4.1823335 -4.1767797 -4.1742582 -4.1866908 -4.2172589 -4.2380819 -4.2460246 -4.2467222 -4.2447033 -4.2396522 -4.2381592 -4.2382088 -4.2333484 -4.2266126 -4.2279725][-4.145556 -4.1337419 -4.1316772 -4.14814 -4.1853566 -4.2089977 -4.2171535 -4.2173667 -4.2168684 -4.2141528 -4.2178578 -4.2228112 -4.2178807 -4.2082629 -4.2075944][-4.1501679 -4.1350412 -4.1323166 -4.14722 -4.177249 -4.1939769 -4.1960025 -4.193161 -4.1944804 -4.1988544 -4.21183 -4.224009 -4.2225704 -4.2140274 -4.2115617][-4.1832004 -4.1704626 -4.1671133 -4.1734538 -4.1870346 -4.1906781 -4.1867275 -4.1808343 -4.1823835 -4.1928587 -4.212831 -4.2301021 -4.2328544 -4.2286453 -4.22753][-4.2075238 -4.2008243 -4.2001619 -4.2022038 -4.2035131 -4.1961579 -4.18779 -4.1803384 -4.1805444 -4.1925073 -4.2123823 -4.2282639 -4.2342095 -4.2352471 -4.2384372][-4.2051463 -4.2039876 -4.2060494 -4.208715 -4.207377 -4.1961961 -4.18416 -4.1765518 -4.1773934 -4.1901188 -4.2058449 -4.2169738 -4.2245803 -4.2315326 -4.2393813][-4.1785574 -4.1798186 -4.1844788 -4.1920977 -4.1977386 -4.1893988 -4.1740065 -4.1645465 -4.1687541 -4.1833739 -4.1950965 -4.2023611 -4.2103057 -4.2206855 -4.230906][-4.1538367 -4.1553707 -4.1636338 -4.1804 -4.1956606 -4.1908212 -4.1728354 -4.15926 -4.1617637 -4.1752419 -4.1824455 -4.1891232 -4.1977725 -4.2079344 -4.2192874][-4.1562247 -4.1596003 -4.17108 -4.1940904 -4.2170548 -4.2170939 -4.2011313 -4.1849766 -4.1801925 -4.1852951 -4.1868272 -4.1925578 -4.202435 -4.2109704 -4.2215104][-4.189208 -4.1967936 -4.2105885 -4.2332544 -4.2573724 -4.2625847 -4.250587 -4.2332048 -4.2210746 -4.217834 -4.2143378 -4.2179527 -4.2262845 -4.2302217 -4.2391353][-4.2409506 -4.2499537 -4.2627106 -4.2800813 -4.2989573 -4.30687 -4.3006291 -4.2851696 -4.2696562 -4.2606788 -4.2546277 -4.2560277 -4.2619905 -4.2627187 -4.2696819][-4.2926054 -4.3005271 -4.3089666 -4.3200536 -4.3317604 -4.3389091 -4.3379769 -4.329679 -4.3173013 -4.3078856 -4.3014383 -4.3014064 -4.3041286 -4.3035326 -4.3084126][-4.3308754 -4.3365407 -4.3406658 -4.3458142 -4.3512239 -4.3557062 -4.3582397 -4.35716 -4.350934 -4.3445449 -4.3404427 -4.3394775 -4.3391256 -4.3366613 -4.3373094]]...]
INFO - root - 2017-12-05 19:19:35.340384: step 37210, loss = 2.02, batch loss = 1.97 (9.6 examples/sec; 0.837 sec/batch; 68h:37m:28s remains)
INFO - root - 2017-12-05 19:19:43.763171: step 37220, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 68h:30m:14s remains)
INFO - root - 2017-12-05 19:19:52.420381: step 37230, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 71h:15m:04s remains)
INFO - root - 2017-12-05 19:20:00.947456: step 37240, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 69h:19m:37s remains)
INFO - root - 2017-12-05 19:20:09.305345: step 37250, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 69h:37m:00s remains)
INFO - root - 2017-12-05 19:20:17.809440: step 37260, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 70h:22m:15s remains)
INFO - root - 2017-12-05 19:20:26.253587: step 37270, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 68h:53m:40s remains)
INFO - root - 2017-12-05 19:20:34.640953: step 37280, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 69h:23m:56s remains)
INFO - root - 2017-12-05 19:20:42.846962: step 37290, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.744 sec/batch; 60h:59m:23s remains)
INFO - root - 2017-12-05 19:20:51.411784: step 37300, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 68h:48m:08s remains)
2017-12-05 19:20:52.331295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3059278 -4.3059449 -4.3010111 -4.2885818 -4.2716303 -4.256042 -4.2493887 -4.2546744 -4.2645907 -4.2806163 -4.3012462 -4.3129182 -4.3142133 -4.3145976 -4.3177142][-4.2956133 -4.2989697 -4.2951751 -4.2767544 -4.2492266 -4.224648 -4.21242 -4.2118034 -4.2148409 -4.2272449 -4.2504463 -4.2691827 -4.2780876 -4.2835274 -4.2921963][-4.2835689 -4.2877727 -4.2833915 -4.2633061 -4.2348566 -4.2090726 -4.1922021 -4.1813149 -4.1709509 -4.1792812 -4.204174 -4.2251177 -4.2398162 -4.2485719 -4.2561889][-4.2758613 -4.2729874 -4.2624331 -4.2422857 -4.2180138 -4.1941094 -4.1740408 -4.1540112 -4.1323218 -4.1352792 -4.1588287 -4.1793795 -4.2021127 -4.2172017 -4.222002][-4.2684793 -4.2561774 -4.23892 -4.2178307 -4.1971755 -4.1770287 -4.15931 -4.1362333 -4.1067119 -4.0964651 -4.1079116 -4.1245952 -4.1565003 -4.1824417 -4.1922569][-4.2583432 -4.2433829 -4.2195616 -4.1943264 -4.1782842 -4.1717305 -4.16553 -4.1440406 -4.1095247 -4.0833883 -4.0766687 -4.0832529 -4.1137238 -4.147016 -4.1649451][-4.2431021 -4.2299385 -4.2019043 -4.1712785 -4.1582384 -4.164381 -4.1766763 -4.1673126 -4.1359758 -4.0996184 -4.0756364 -4.0697093 -4.090229 -4.1231894 -4.1484938][-4.22672 -4.2185206 -4.1916423 -4.1547394 -4.1378422 -4.1472106 -4.1691604 -4.1773868 -4.1620603 -4.1301451 -4.0980158 -4.0803018 -4.087646 -4.1164269 -4.1493621][-4.2031684 -4.21266 -4.1994214 -4.1717296 -4.152729 -4.157383 -4.1754022 -4.1885862 -4.1851645 -4.1646442 -4.1369476 -4.11403 -4.1051593 -4.120873 -4.1533289][-4.164434 -4.1925178 -4.199585 -4.1931143 -4.1850758 -4.1865726 -4.1953907 -4.2037272 -4.204844 -4.1933064 -4.1725149 -4.1476393 -4.1252322 -4.1261735 -4.1515713][-4.1305671 -4.1699109 -4.1909561 -4.1985159 -4.1991358 -4.2024908 -4.2102313 -4.2180834 -4.2201252 -4.2161937 -4.2023706 -4.1799765 -4.1550546 -4.1450243 -4.1553149][-4.1211958 -4.1541982 -4.17672 -4.191731 -4.2005038 -4.2073188 -4.2153106 -4.2246633 -4.2295895 -4.2318077 -4.2237515 -4.2052908 -4.1844182 -4.1735587 -4.175858][-4.1294303 -4.1426516 -4.1547909 -4.1662288 -4.1775241 -4.1912456 -4.2042446 -4.2178211 -4.2286415 -4.2386546 -4.2395515 -4.2280922 -4.2121172 -4.2009664 -4.1974168][-4.1324272 -4.1361475 -4.1404824 -4.1417623 -4.1459675 -4.1585274 -4.1740985 -4.1930046 -4.2099934 -4.226707 -4.2384963 -4.2404227 -4.2327228 -4.2219453 -4.211381][-4.1201196 -4.1198316 -4.1188889 -4.1140604 -4.1143589 -4.1244397 -4.1409268 -4.1626697 -4.1824346 -4.2009678 -4.217494 -4.2286696 -4.2309594 -4.2256861 -4.2177186]]...]
INFO - root - 2017-12-05 19:21:00.786662: step 37310, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 70h:26m:10s remains)
INFO - root - 2017-12-05 19:21:09.296225: step 37320, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 73h:12m:08s remains)
INFO - root - 2017-12-05 19:21:17.762318: step 37330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 71h:23m:00s remains)
INFO - root - 2017-12-05 19:21:26.376685: step 37340, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 72h:07m:19s remains)
INFO - root - 2017-12-05 19:21:34.765107: step 37350, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 67h:32m:46s remains)
INFO - root - 2017-12-05 19:21:43.327785: step 37360, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 70h:57m:44s remains)
INFO - root - 2017-12-05 19:21:51.858505: step 37370, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 70h:30m:19s remains)
INFO - root - 2017-12-05 19:22:00.452252: step 37380, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 70h:19m:27s remains)
INFO - root - 2017-12-05 19:22:08.829102: step 37390, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 0.783 sec/batch; 64h:10m:56s remains)
INFO - root - 2017-12-05 19:22:17.304222: step 37400, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.780 sec/batch; 63h:58m:05s remains)
2017-12-05 19:22:18.076937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2103615 -4.1992021 -4.1833172 -4.1877518 -4.2164559 -4.2405968 -4.2467375 -4.2508035 -4.2587256 -4.2676787 -4.2682533 -4.2562289 -4.2490497 -4.2530847 -4.2669282][-4.1447916 -4.1277294 -4.1025333 -4.1053786 -4.1474857 -4.1888919 -4.2050495 -4.2101088 -4.2180514 -4.2280984 -4.2359066 -4.2213941 -4.2089972 -4.2156687 -4.2361469][-4.063168 -4.0399327 -4.0191035 -4.0320082 -4.0899 -4.1410389 -4.1700358 -4.1763649 -4.1804514 -4.1910172 -4.1934333 -4.1704793 -4.1532426 -4.1609697 -4.189652][-3.983398 -3.9571705 -3.937788 -3.9647272 -4.0302629 -4.0820036 -4.1144 -4.1231117 -4.1218805 -4.1299939 -4.1293869 -4.0992017 -4.08142 -4.0923738 -4.1304979][-3.9264119 -3.89265 -3.8690243 -3.9063575 -3.97801 -4.0220838 -4.0436921 -4.0425844 -4.0325351 -4.0438571 -4.0526371 -4.0320592 -4.0234981 -4.04162 -4.089952][-3.9177072 -3.8738265 -3.8343384 -3.8689613 -3.9293745 -3.9447229 -3.9265082 -3.8933125 -3.8860869 -3.9297922 -3.9682119 -3.9765351 -3.9944193 -4.0261283 -4.0780239][-3.9663258 -3.9174647 -3.853828 -3.8518298 -3.8658514 -3.8286352 -3.7566233 -3.6855745 -3.7049036 -3.8116348 -3.8973722 -3.9446003 -3.9921782 -4.0420918 -4.0968018][-4.0345731 -3.9821897 -3.9057593 -3.8518889 -3.8080959 -3.7226694 -3.6066821 -3.5121408 -3.5697589 -3.7338765 -3.8665137 -3.9506316 -4.0227265 -4.0921292 -4.14877][-4.0769854 -4.0249033 -3.947715 -3.8746896 -3.8072946 -3.7135849 -3.6040504 -3.5444326 -3.6338265 -3.7971952 -3.9239554 -4.011837 -4.0920963 -4.1657734 -4.2130246][-4.1075368 -4.0642424 -3.9987297 -3.9354253 -3.8784451 -3.8075783 -3.7336843 -3.7141309 -3.7998695 -3.9190388 -4.0047493 -4.0704789 -4.1371503 -4.2028661 -4.2431564][-4.1128197 -4.0765228 -4.0308585 -3.9917541 -3.962661 -3.9224224 -3.8798528 -3.8784163 -3.9405155 -4.0149794 -4.0609875 -4.0955639 -4.1421781 -4.1995797 -4.2405829][-4.0992069 -4.0729904 -4.0470295 -4.0282855 -4.0213194 -4.00859 -3.996587 -4.0135341 -4.0578766 -4.097291 -4.1140165 -4.1196513 -4.1428318 -4.1889153 -4.227479][-4.1039538 -4.0922079 -4.0813303 -4.0780015 -4.0829206 -4.0858812 -4.0931287 -4.11974 -4.1522412 -4.1718521 -4.1719818 -4.15977 -4.1655865 -4.1993518 -4.2285237][-4.1586637 -4.1585984 -4.1556959 -4.1564207 -4.1633945 -4.1690559 -4.1789708 -4.2028861 -4.2267027 -4.2373552 -4.2323685 -4.2206473 -4.2239294 -4.2428985 -4.256237][-4.229907 -4.2339525 -4.2346587 -4.2368422 -4.2420368 -4.2464261 -4.2517543 -4.2635775 -4.2756066 -4.280313 -4.27408 -4.2654848 -4.2692266 -4.2796755 -4.2844391]]...]
INFO - root - 2017-12-05 19:22:26.609383: step 37410, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 70h:38m:11s remains)
INFO - root - 2017-12-05 19:22:35.121093: step 37420, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 70h:43m:03s remains)
INFO - root - 2017-12-05 19:22:43.646873: step 37430, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 69h:25m:15s remains)
INFO - root - 2017-12-05 19:22:52.097520: step 37440, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 70h:22m:20s remains)
INFO - root - 2017-12-05 19:23:00.683595: step 37450, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 71h:20m:52s remains)
INFO - root - 2017-12-05 19:23:09.278500: step 37460, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 70h:27m:41s remains)
INFO - root - 2017-12-05 19:23:17.752065: step 37470, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 68h:53m:03s remains)
INFO - root - 2017-12-05 19:23:26.302404: step 37480, loss = 2.02, batch loss = 1.97 (9.4 examples/sec; 0.854 sec/batch; 70h:01m:21s remains)
INFO - root - 2017-12-05 19:23:34.741305: step 37490, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 68h:21m:32s remains)
INFO - root - 2017-12-05 19:23:43.092288: step 37500, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.825 sec/batch; 67h:37m:10s remains)
2017-12-05 19:23:43.938707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2387114 -4.2425718 -4.248374 -4.2509928 -4.2545848 -4.2556167 -4.2496037 -4.2390838 -4.22703 -4.2086306 -4.1809044 -4.1462636 -4.1268091 -4.1354575 -4.1535492][-4.2238345 -4.2231569 -4.2263579 -4.2291112 -4.2319818 -4.2289643 -4.2173257 -4.2034273 -4.1868157 -4.1584911 -4.1119623 -4.0559769 -4.0282273 -4.0471697 -4.0814228][-4.2162585 -4.2062263 -4.2044244 -4.2076707 -4.2087216 -4.2007971 -4.182169 -4.1664162 -4.1481233 -4.1169176 -4.0593982 -3.991818 -3.9646673 -3.9970243 -4.0466337][-4.2171345 -4.1961918 -4.1849508 -4.1837277 -4.17746 -4.15943 -4.1355176 -4.124753 -4.1143265 -4.0861945 -4.0351267 -3.981504 -3.9667933 -4.0045867 -4.0569959][-4.2290468 -4.1930633 -4.1714282 -4.1628594 -4.1485028 -4.1227932 -4.093739 -4.087533 -4.0873718 -4.0712724 -4.0370135 -4.0031633 -4.001914 -4.0415635 -4.0903077][-4.2291579 -4.177278 -4.1481042 -4.1398611 -4.1285071 -4.1046076 -4.0697417 -4.0584183 -4.0616455 -4.05712 -4.0380611 -4.0188923 -4.0367393 -4.0891066 -4.1358719][-4.19295 -4.1294656 -4.1022029 -4.1116581 -4.1211181 -4.1105762 -4.0768514 -4.0575366 -4.0545497 -4.050374 -4.0455465 -4.0432296 -4.0758095 -4.1324353 -4.1776104][-4.1316614 -4.0646439 -4.0477829 -4.0843725 -4.1172876 -4.1239104 -4.099247 -4.0765395 -4.0637126 -4.0564537 -4.0627642 -4.0726695 -4.1068969 -4.1573458 -4.1973891][-4.0844421 -4.0211034 -4.0129943 -4.0656915 -4.1060123 -4.1191921 -4.1046643 -4.0860863 -4.0767117 -4.0751328 -4.0889435 -4.1059842 -4.1339841 -4.172667 -4.20344][-4.0852003 -4.0397162 -4.0360823 -4.0765262 -4.1025553 -4.1081448 -4.0984759 -4.0885153 -4.086113 -4.0943837 -4.114912 -4.13708 -4.1609769 -4.1868596 -4.2093477][-4.1287351 -4.1047831 -4.0998583 -4.1163607 -4.1205277 -4.1091518 -4.0945754 -4.0849757 -4.0873542 -4.104722 -4.1340008 -4.1660476 -4.189723 -4.2087655 -4.2274714][-4.1716285 -4.1595387 -4.1502404 -4.1513987 -4.140729 -4.1183338 -4.0933514 -4.0804076 -4.0891013 -4.1186633 -4.1582069 -4.1944389 -4.2164669 -4.2316308 -4.2490125][-4.1812944 -4.1640382 -4.1477146 -4.1415215 -4.1327181 -4.1145163 -4.08863 -4.0800591 -4.1013269 -4.1430254 -4.187757 -4.221446 -4.23877 -4.2456279 -4.2558165][-4.1318016 -4.0948653 -4.0664959 -4.0631704 -4.0699034 -4.0750957 -4.0704861 -4.0837135 -4.1202397 -4.1663814 -4.2071223 -4.233767 -4.24565 -4.2438841 -4.2450466][-4.0609465 -4.0016704 -3.9573438 -3.9573276 -3.9852593 -4.0258985 -4.0606227 -4.1019425 -4.1486034 -4.1930504 -4.2259221 -4.2430534 -4.2466044 -4.2392483 -4.2321134]]...]
INFO - root - 2017-12-05 19:23:52.464368: step 37510, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 69h:58m:19s remains)
INFO - root - 2017-12-05 19:24:01.160615: step 37520, loss = 2.02, batch loss = 1.97 (9.4 examples/sec; 0.852 sec/batch; 69h:49m:20s remains)
INFO - root - 2017-12-05 19:24:09.672868: step 37530, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 71h:40m:50s remains)
INFO - root - 2017-12-05 19:24:18.179231: step 37540, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 67h:29m:14s remains)
INFO - root - 2017-12-05 19:24:26.585396: step 37550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 70h:45m:40s remains)
INFO - root - 2017-12-05 19:24:35.172806: step 37560, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 69h:44m:13s remains)
INFO - root - 2017-12-05 19:24:43.794446: step 37570, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 72h:04m:51s remains)
INFO - root - 2017-12-05 19:24:52.294154: step 37580, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 68h:48m:10s remains)
INFO - root - 2017-12-05 19:25:00.783890: step 37590, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 69h:24m:05s remains)
INFO - root - 2017-12-05 19:25:09.370794: step 37600, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 69h:31m:50s remains)
2017-12-05 19:25:10.125149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1859717 -4.2056437 -4.2004042 -4.1805224 -4.1544843 -4.1238132 -4.120615 -4.1477275 -4.1988354 -4.262228 -4.3030491 -4.3168979 -4.31615 -4.3207235 -4.3308339][-4.1798863 -4.1851554 -4.1705804 -4.1450362 -4.1223059 -4.0982652 -4.1010294 -4.1337824 -4.1899691 -4.2528715 -4.2928662 -4.3092027 -4.3123193 -4.3199654 -4.3312936][-4.2067971 -4.193644 -4.1660762 -4.1334267 -4.1058187 -4.0774679 -4.0701709 -4.1004753 -4.1583166 -4.2224808 -4.2686915 -4.2935967 -4.3042388 -4.318182 -4.3318491][-4.2471175 -4.22579 -4.1931148 -4.1531048 -4.1124954 -4.0618291 -4.0270038 -4.0469346 -4.1090431 -4.181365 -4.2396817 -4.2780657 -4.2991724 -4.3177609 -4.3335633][-4.2773824 -4.2563925 -4.2255583 -4.1832271 -4.1309037 -4.0538363 -3.9911523 -4.0017867 -4.0685081 -4.1520262 -4.2235742 -4.2730527 -4.3001366 -4.3199797 -4.3362885][-4.2912507 -4.274559 -4.2488928 -4.21187 -4.1605525 -4.0749316 -3.9982116 -4.000494 -4.0661497 -4.1543183 -4.2313538 -4.2830191 -4.3082833 -4.324297 -4.3390093][-4.2844563 -4.269495 -4.2473412 -4.2209816 -4.1765394 -4.0954857 -4.0234036 -4.0244207 -4.0846152 -4.1692376 -4.2464056 -4.296535 -4.3156357 -4.3270807 -4.3400316][-4.2600164 -4.2451992 -4.2284465 -4.2135968 -4.1780457 -4.1107144 -4.0565925 -4.062109 -4.1142073 -4.1912756 -4.2637067 -4.3084106 -4.3207021 -4.3275204 -4.3396378][-4.2259841 -4.2082109 -4.1957397 -4.1888041 -4.1630015 -4.112246 -4.0811739 -4.093852 -4.1436863 -4.2147145 -4.2792673 -4.3169723 -4.3238521 -4.3274794 -4.3394022][-4.186883 -4.1635728 -4.1495466 -4.1426649 -4.1215353 -4.0838671 -4.0744262 -4.1010051 -4.1571913 -4.229022 -4.2897396 -4.3236384 -4.32702 -4.3278065 -4.3394246][-4.1478243 -4.1168952 -4.09319 -4.07508 -4.0511351 -4.0283628 -4.0452142 -4.0911417 -4.1590743 -4.2347322 -4.2953134 -4.3275123 -4.3290071 -4.3277359 -4.3391032][-4.11989 -4.0831904 -4.0483727 -4.0182686 -3.9903076 -3.9843628 -4.0283694 -4.0940094 -4.1720128 -4.24704 -4.3046708 -4.3317604 -4.3296161 -4.3261929 -4.3373737][-4.1103883 -4.0780015 -4.0414672 -4.0064511 -3.9769845 -3.9871995 -4.0473037 -4.123414 -4.2020774 -4.2693863 -4.3164511 -4.3349524 -4.3285151 -4.3234596 -4.3347449][-4.1262875 -4.1083407 -4.0806441 -4.0501628 -4.0202565 -4.0298295 -4.0852356 -4.1569085 -4.2276325 -4.28377 -4.31966 -4.332736 -4.3254089 -4.3218861 -4.3338013][-4.1497507 -4.1473455 -4.133966 -4.1132541 -4.0874019 -4.0846972 -4.121768 -4.178041 -4.2365742 -4.2840185 -4.3140941 -4.3263035 -4.32158 -4.3214822 -4.3337193]]...]
INFO - root - 2017-12-05 19:25:18.503585: step 37610, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 68h:38m:14s remains)
INFO - root - 2017-12-05 19:25:26.907324: step 37620, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 68h:35m:18s remains)
INFO - root - 2017-12-05 19:25:35.359270: step 37630, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 69h:36m:34s remains)
INFO - root - 2017-12-05 19:25:43.932722: step 37640, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 69h:56m:30s remains)
INFO - root - 2017-12-05 19:25:52.358550: step 37650, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 70h:35m:05s remains)
INFO - root - 2017-12-05 19:26:00.819391: step 37660, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 69h:01m:27s remains)
INFO - root - 2017-12-05 19:26:09.341974: step 37670, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 69h:22m:29s remains)
INFO - root - 2017-12-05 19:26:17.808413: step 37680, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 70h:04m:20s remains)
INFO - root - 2017-12-05 19:26:26.271440: step 37690, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 68h:57m:22s remains)
INFO - root - 2017-12-05 19:26:34.827434: step 37700, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 70h:01m:32s remains)
2017-12-05 19:26:35.580492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2870088 -4.2600479 -4.2338333 -4.2172422 -4.2159863 -4.2335491 -4.2603526 -4.2817836 -4.2864017 -4.2753282 -4.2517319 -4.2277317 -4.209878 -4.1972671 -4.1898961][-4.2843962 -4.2571478 -4.2306342 -4.2104459 -4.20494 -4.2210155 -4.2531996 -4.2812371 -4.2906079 -4.2787895 -4.2492738 -4.2191138 -4.1963868 -4.1813741 -4.174984][-4.2877793 -4.2636423 -4.2388487 -4.2166128 -4.2081003 -4.2230873 -4.2575865 -4.2898498 -4.3021884 -4.2900662 -4.2598581 -4.2312393 -4.2089481 -4.1924224 -4.1840625][-4.2924643 -4.271729 -4.24963 -4.2275624 -4.2164154 -4.227767 -4.2582064 -4.2893786 -4.3032064 -4.2960372 -4.27403 -4.2550597 -4.2394338 -4.2251964 -4.216321][-4.29328 -4.2730312 -4.2516179 -4.2282572 -4.2119207 -4.2183623 -4.2434797 -4.2732081 -4.2905889 -4.2925959 -4.2835922 -4.2764773 -4.26967 -4.2603407 -4.253613][-4.2897325 -4.2661743 -4.2411823 -4.2116904 -4.1864004 -4.1868753 -4.2119765 -4.2461853 -4.2705612 -4.2833109 -4.2872939 -4.2903519 -4.2907796 -4.2852693 -4.2809319][-4.285048 -4.2570319 -4.2254581 -4.1877565 -4.1558022 -4.1547918 -4.1860375 -4.2301793 -4.2611489 -4.2795115 -4.2882414 -4.2951369 -4.2975993 -4.2922378 -4.2884927][-4.2836246 -4.25593 -4.2222967 -4.182354 -4.1519728 -4.1550932 -4.1918654 -4.2388825 -4.2683992 -4.2836838 -4.2891312 -4.2940826 -4.2947173 -4.2874241 -4.2834029][-4.2843323 -4.2612495 -4.2315283 -4.1976442 -4.176003 -4.182961 -4.2137513 -4.2526479 -4.2749977 -4.2847772 -4.2858343 -4.2875276 -4.2866459 -4.2798309 -4.277194][-4.2846994 -4.2654548 -4.2396889 -4.2103181 -4.1946049 -4.20192 -4.2254448 -4.2555728 -4.2730627 -4.2803082 -4.2801809 -4.2766538 -4.2751393 -4.2726731 -4.2746277][-4.2810087 -4.2642288 -4.23812 -4.2069349 -4.1922956 -4.1967311 -4.2174749 -4.2423267 -4.2584205 -4.2666712 -4.2674694 -4.2622623 -4.2630186 -4.2672114 -4.2753258][-4.2728748 -4.2566762 -4.2285895 -4.1935072 -4.1753588 -4.17459 -4.193428 -4.2149286 -4.2301927 -4.2400675 -4.2422004 -4.2425518 -4.2511864 -4.2637053 -4.2772][-4.2674875 -4.2533641 -4.2281637 -4.1956282 -4.1769133 -4.1695967 -4.1797318 -4.192296 -4.2007933 -4.2083349 -4.2127719 -4.2237148 -4.2438293 -4.2644296 -4.2813354][-4.2665048 -4.2534876 -4.2304459 -4.2034974 -4.1871853 -4.1747003 -4.1730051 -4.1738648 -4.1742907 -4.1776381 -4.1882472 -4.2118382 -4.2433257 -4.269331 -4.2868285][-4.2645068 -4.2493596 -4.2254987 -4.2016878 -4.1866627 -4.1714444 -4.162251 -4.1574659 -4.1557708 -4.161562 -4.1801953 -4.2133694 -4.2498074 -4.2753544 -4.2900085]]...]
INFO - root - 2017-12-05 19:26:44.014769: step 37710, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 69h:48m:18s remains)
INFO - root - 2017-12-05 19:26:52.524852: step 37720, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.829 sec/batch; 67h:54m:26s remains)
INFO - root - 2017-12-05 19:27:00.980971: step 37730, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 70h:58m:41s remains)
INFO - root - 2017-12-05 19:27:09.691321: step 37740, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 69h:03m:59s remains)
INFO - root - 2017-12-05 19:27:18.002815: step 37750, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 69h:23m:30s remains)
INFO - root - 2017-12-05 19:27:26.531034: step 37760, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.848 sec/batch; 69h:24m:27s remains)
INFO - root - 2017-12-05 19:27:34.984056: step 37770, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 71h:31m:34s remains)
INFO - root - 2017-12-05 19:27:43.548755: step 37780, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 70h:42m:37s remains)
INFO - root - 2017-12-05 19:27:52.127121: step 37790, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 69h:16m:31s remains)
INFO - root - 2017-12-05 19:28:00.574365: step 37800, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 68h:51m:19s remains)
2017-12-05 19:28:01.334797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2086191 -4.1851039 -4.1812811 -4.1906686 -4.2065988 -4.2308607 -4.246284 -4.2490964 -4.2450347 -4.2419348 -4.2425656 -4.2458305 -4.2514472 -4.2524142 -4.2506166][-4.1579089 -4.1184897 -4.11374 -4.1308165 -4.1572857 -4.1810265 -4.1906238 -4.1901064 -4.1855025 -4.1835561 -4.190537 -4.2024446 -4.2180147 -4.2248712 -4.224113][-4.107183 -4.0515847 -4.0464349 -4.0757771 -4.11765 -4.1416035 -4.1352081 -4.1170793 -4.10721 -4.1042376 -4.117703 -4.1435895 -4.1774788 -4.1987209 -4.2061009][-4.0727067 -4.0117335 -4.01263 -4.0506077 -4.1002569 -4.1205106 -4.0835233 -4.0279474 -4.00628 -4.007019 -4.0316458 -4.076932 -4.13284 -4.1728873 -4.1956611][-4.0770626 -4.0201259 -4.0248613 -4.0620689 -4.1010575 -4.0938392 -4.011682 -3.9085324 -3.8665042 -3.8825488 -3.9362304 -4.014009 -4.0910664 -4.1430316 -4.1763458][-4.124825 -4.072084 -4.0599971 -4.0752921 -4.0833077 -4.0358319 -3.9061971 -3.757478 -3.6938961 -3.7418342 -3.8479445 -3.9706674 -4.0664091 -4.1234717 -4.1557751][-4.1762896 -4.1233349 -4.0895581 -4.078269 -4.0553946 -3.9769778 -3.8200953 -3.6489697 -3.5731478 -3.6452265 -3.7937489 -3.9498184 -4.05668 -4.1098943 -4.1355624][-4.209691 -4.1602082 -4.1229496 -4.1050377 -4.0719037 -3.9801803 -3.8304205 -3.679388 -3.6105702 -3.6806722 -3.8233349 -3.9686136 -4.0588284 -4.08792 -4.0952983][-4.2434025 -4.2067313 -4.1767964 -4.1575656 -4.12345 -4.0367851 -3.9159417 -3.8104591 -3.770052 -3.8239071 -3.9171481 -4.0038891 -4.043962 -4.03266 -4.0253057][-4.2758021 -4.2499228 -4.2205353 -4.198276 -4.1671529 -4.1061435 -4.023694 -3.9549375 -3.9359953 -3.9763565 -4.0249524 -4.0477338 -4.0300789 -3.98527 -3.9734373][-4.3100853 -4.290689 -4.2610373 -4.2341022 -4.2087064 -4.1777115 -4.130641 -4.089355 -4.0812182 -4.1018186 -4.1134143 -4.0928826 -4.037683 -3.975703 -3.9627953][-4.3343291 -4.3209314 -4.2916832 -4.2664256 -4.2517028 -4.2374482 -4.2063355 -4.180604 -4.1782918 -4.1868987 -4.1746864 -4.1364264 -4.0785279 -4.0177889 -4.0041056][-4.3303061 -4.3220716 -4.3005066 -4.2816672 -4.2739277 -4.265542 -4.2428265 -4.2265539 -4.2287941 -4.2330036 -4.217865 -4.1865788 -4.1440225 -4.0971875 -4.081203][-4.3134828 -4.308044 -4.2939572 -4.2821021 -4.2763696 -4.2703571 -4.2600284 -4.2547722 -4.2604585 -4.2652817 -4.2555456 -4.235 -4.2088571 -4.177474 -4.1597648][-4.2987914 -4.294271 -4.2872071 -4.2826772 -4.2788982 -4.2757969 -4.2742138 -4.2753458 -4.2806387 -4.2835193 -4.278985 -4.2684178 -4.2541366 -4.2360497 -4.22099]]...]
INFO - root - 2017-12-05 19:28:09.764306: step 37810, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 67h:51m:01s remains)
INFO - root - 2017-12-05 19:28:18.130061: step 37820, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 71h:03m:12s remains)
INFO - root - 2017-12-05 19:28:26.584608: step 37830, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 69h:49m:58s remains)
INFO - root - 2017-12-05 19:28:35.062169: step 37840, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.829 sec/batch; 67h:50m:50s remains)
INFO - root - 2017-12-05 19:28:43.486882: step 37850, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 70h:04m:28s remains)
INFO - root - 2017-12-05 19:28:51.979772: step 37860, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 70h:11m:23s remains)
INFO - root - 2017-12-05 19:29:00.582586: step 37870, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 72h:47m:54s remains)
INFO - root - 2017-12-05 19:29:09.077958: step 37880, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 69h:33m:03s remains)
INFO - root - 2017-12-05 19:29:17.600361: step 37890, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 69h:12m:52s remains)
INFO - root - 2017-12-05 19:29:26.086083: step 37900, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 68h:51m:08s remains)
2017-12-05 19:29:26.904956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.208631 -4.2112889 -4.2075562 -4.1970015 -4.179738 -4.1669278 -4.15778 -4.145987 -4.1412678 -4.159831 -4.1920938 -4.2213364 -4.2524104 -4.2868252 -4.313797][-4.2215495 -4.2234139 -4.2186704 -4.2012272 -4.1718717 -4.1491547 -4.1401534 -4.1329188 -4.1380129 -4.1669459 -4.2045717 -4.2338934 -4.263741 -4.2948227 -4.3178415][-4.237793 -4.2366762 -4.2287111 -4.2060089 -4.1706762 -4.1479368 -4.1352134 -4.1219912 -4.1262193 -4.1566758 -4.1995792 -4.233623 -4.2681417 -4.2993717 -4.3183479][-4.2285738 -4.2201529 -4.2117252 -4.1958747 -4.1693859 -4.1525497 -4.1374373 -4.111948 -4.1088076 -4.1450834 -4.1967697 -4.2382193 -4.2759023 -4.3061118 -4.3193507][-4.1866283 -4.1742916 -4.1731339 -4.1726871 -4.1585827 -4.1380973 -4.1056185 -4.05646 -4.0501962 -4.1032805 -4.1684675 -4.2205267 -4.2696629 -4.3060961 -4.318315][-4.1440911 -4.1305671 -4.1355309 -4.1441717 -4.12864 -4.0928364 -4.0215111 -3.9262094 -3.9237876 -4.0195889 -4.116426 -4.1885471 -4.2556982 -4.3029976 -4.3187332][-4.1498675 -4.1348763 -4.1275568 -4.122869 -4.0938392 -4.0328484 -3.904345 -3.7358234 -3.741838 -3.9065869 -4.048315 -4.1435056 -4.2286725 -4.2872334 -4.3089442][-4.1723347 -4.1525426 -4.1325488 -4.1145439 -4.0807395 -4.0133271 -3.8657961 -3.678791 -3.6889241 -3.8696804 -4.0212812 -4.1205754 -4.2076483 -4.2669878 -4.2932153][-4.1864929 -4.16922 -4.1531549 -4.139699 -4.1203089 -4.07314 -3.9579508 -3.821558 -3.828774 -3.950917 -4.0594091 -4.1413603 -4.2172737 -4.2659874 -4.2881775][-4.1838303 -4.1746469 -4.1699743 -4.1686859 -4.1637106 -4.135098 -4.0518994 -3.9532928 -3.9527988 -4.0292087 -4.1007156 -4.1640763 -4.2306018 -4.2758293 -4.2972789][-4.1918516 -4.1913891 -4.1906176 -4.1895709 -4.1903205 -4.1723804 -4.112422 -4.0392632 -4.031858 -4.0771027 -4.132432 -4.1879473 -4.2488542 -4.293107 -4.3136744][-4.2187676 -4.2174573 -4.2143 -4.2078853 -4.204545 -4.1909857 -4.1442108 -4.088325 -4.0760217 -4.104918 -4.1560726 -4.207901 -4.259851 -4.2999892 -4.3221512][-4.2381921 -4.2353387 -4.2297 -4.2217021 -4.2162433 -4.2047954 -4.1709657 -4.1337643 -4.1233435 -4.1438475 -4.1830115 -4.2219272 -4.261405 -4.2967954 -4.3213434][-4.2470484 -4.2388649 -4.2310195 -4.2238131 -4.2228055 -4.2185078 -4.1981711 -4.1774373 -4.1759667 -4.193357 -4.219399 -4.2441454 -4.2705765 -4.29858 -4.3223186][-4.2639513 -4.2465076 -4.2314615 -4.2215066 -4.2225022 -4.2241859 -4.2160563 -4.2087708 -4.2142806 -4.2308512 -4.2488985 -4.265492 -4.2850156 -4.3073921 -4.3284822]]...]
INFO - root - 2017-12-05 19:29:35.394254: step 37910, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 72h:33m:41s remains)
INFO - root - 2017-12-05 19:29:43.959382: step 37920, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 70h:29m:57s remains)
INFO - root - 2017-12-05 19:29:52.215499: step 37930, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 70h:09m:41s remains)
INFO - root - 2017-12-05 19:30:00.736241: step 37940, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 67h:50m:40s remains)
INFO - root - 2017-12-05 19:30:09.084412: step 37950, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 70h:09m:18s remains)
INFO - root - 2017-12-05 19:30:17.486239: step 37960, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 69h:36m:20s remains)
INFO - root - 2017-12-05 19:30:26.060027: step 37970, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 70h:32m:33s remains)
INFO - root - 2017-12-05 19:30:34.575317: step 37980, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 70h:08m:39s remains)
INFO - root - 2017-12-05 19:30:43.057232: step 37990, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 66h:48m:43s remains)
INFO - root - 2017-12-05 19:30:51.526673: step 38000, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 68h:26m:28s remains)
2017-12-05 19:30:52.317382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2859921 -4.2598524 -4.2250376 -4.1971278 -4.1815128 -4.1801748 -4.1796288 -4.1785965 -4.1863961 -4.2014174 -4.2132621 -4.2132149 -4.2150159 -4.2231674 -4.22635][-4.2634888 -4.2348351 -4.1964893 -4.1686521 -4.1534338 -4.1553082 -4.1599431 -4.1607914 -4.1694155 -4.1860552 -4.2013516 -4.2048335 -4.209712 -4.224772 -4.2339344][-4.2367086 -4.2106719 -4.17826 -4.158411 -4.1505575 -4.1589322 -4.1665611 -4.1646938 -4.1668038 -4.1780548 -4.1943865 -4.20618 -4.21762 -4.2362857 -4.2503891][-4.1995616 -4.1767812 -4.1560826 -4.1530638 -4.1619682 -4.1798577 -4.1893206 -4.1816254 -4.1735711 -4.1764731 -4.1917391 -4.2126455 -4.2306247 -4.2518096 -4.2704139][-4.1555581 -4.1324658 -4.1203809 -4.1339874 -4.1609654 -4.1888704 -4.204021 -4.1966748 -4.1855664 -4.1861892 -4.2026134 -4.2257853 -4.2434096 -4.2619863 -4.2773643][-4.1063843 -4.0781369 -4.0727806 -4.1021624 -4.1413226 -4.1765351 -4.1983304 -4.2008691 -4.2002425 -4.2062416 -4.2206678 -4.2386813 -4.2515173 -4.2610316 -4.269083][-4.0762558 -4.04286 -4.0421362 -4.0761218 -4.1148291 -4.1513743 -4.1785107 -4.1950688 -4.2079091 -4.2175188 -4.229094 -4.2427526 -4.2522869 -4.255724 -4.2584071][-4.0714 -4.0401468 -4.0393267 -4.061563 -4.0860229 -4.1160259 -4.14326 -4.1689863 -4.1912894 -4.2043653 -4.2174926 -4.2280626 -4.2350674 -4.2393894 -4.2414665][-4.085609 -4.0599632 -4.0556512 -4.061944 -4.0739121 -4.098031 -4.1231046 -4.1483665 -4.1705008 -4.1867676 -4.2020831 -4.209547 -4.2148108 -4.2209449 -4.2234192][-4.102066 -4.0883512 -4.0873919 -4.0872397 -4.0907183 -4.101872 -4.1211638 -4.1448474 -4.1641784 -4.177825 -4.1905951 -4.1936064 -4.1962767 -4.2049708 -4.2107129][-4.1197276 -4.1168556 -4.120265 -4.1194215 -4.1166506 -4.1183805 -4.1323328 -4.1513848 -4.1675253 -4.18015 -4.1914206 -4.1907783 -4.1860614 -4.1937051 -4.1997652][-4.1357341 -4.1350155 -4.1368408 -4.1363626 -4.1324387 -4.1332426 -4.1467991 -4.1618571 -4.1757603 -4.1894717 -4.1982608 -4.1959977 -4.1868496 -4.1892066 -4.1928596][-4.1421995 -4.1402869 -4.1350331 -4.1321888 -4.1301389 -4.1336646 -4.1468444 -4.1588497 -4.1709547 -4.1837692 -4.1897821 -4.1897511 -4.1854954 -4.1858754 -4.187398][-4.1437778 -4.1417303 -4.1321855 -4.124577 -4.121614 -4.1288762 -4.1424389 -4.1530991 -4.1614156 -4.1706185 -4.1781759 -4.1844144 -4.1865249 -4.18607 -4.1847143][-4.1467505 -4.1457577 -4.1375442 -4.1284246 -4.1286221 -4.1392879 -4.1515303 -4.1581941 -4.1608462 -4.16392 -4.1689477 -4.1778316 -4.1828 -4.1808925 -4.1776714]]...]
INFO - root - 2017-12-05 19:31:00.687107: step 38010, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 69h:10m:56s remains)
INFO - root - 2017-12-05 19:31:09.140299: step 38020, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.829 sec/batch; 67h:50m:03s remains)
INFO - root - 2017-12-05 19:31:17.532994: step 38030, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.781 sec/batch; 63h:50m:36s remains)
INFO - root - 2017-12-05 19:31:26.093733: step 38040, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.844 sec/batch; 69h:02m:40s remains)
INFO - root - 2017-12-05 19:31:34.593207: step 38050, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 70h:22m:45s remains)
INFO - root - 2017-12-05 19:31:43.100882: step 38060, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 71h:46m:16s remains)
INFO - root - 2017-12-05 19:31:51.625494: step 38070, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 71h:37m:16s remains)
INFO - root - 2017-12-05 19:32:00.312972: step 38080, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 71h:43m:44s remains)
INFO - root - 2017-12-05 19:32:08.890554: step 38090, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 71h:00m:59s remains)
INFO - root - 2017-12-05 19:32:17.494499: step 38100, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 70h:24m:43s remains)
2017-12-05 19:32:18.240157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1314034 -4.115284 -4.13008 -4.1608529 -4.2100377 -4.2349844 -4.2385006 -4.2289991 -4.2110906 -4.1826839 -4.1444974 -4.1282296 -4.1328287 -4.1440082 -4.1689353][-4.0752892 -4.0600224 -4.0846982 -4.1321468 -4.1932364 -4.2202487 -4.2206039 -4.2060332 -4.180582 -4.1427565 -4.0940609 -4.0676622 -4.072391 -4.0934796 -4.1279607][-4.0655866 -4.060267 -4.0883775 -4.1404748 -4.1970916 -4.2182841 -4.2117038 -4.1881604 -4.1577773 -4.1174035 -4.0644212 -4.0313754 -4.0402813 -4.0766897 -4.1249213][-4.0957136 -4.0992842 -4.1198525 -4.1583891 -4.2012148 -4.2100253 -4.1956134 -4.1640491 -4.1317329 -4.0975418 -4.0492349 -4.0148172 -4.028131 -4.0790715 -4.1425405][-4.1477113 -4.1558347 -4.1621909 -4.1773787 -4.1941309 -4.1829319 -4.154644 -4.1188211 -4.0972643 -4.0849066 -4.057416 -4.03309 -4.052196 -4.1076417 -4.174221][-4.1987848 -4.2031841 -4.1975174 -4.1874695 -4.1719689 -4.1346173 -4.0879803 -4.0484347 -4.0479345 -4.0730462 -4.0785689 -4.0784907 -4.1092954 -4.1588531 -4.2159667][-4.2301016 -4.2259059 -4.2061982 -4.1717443 -4.1247134 -4.0561867 -3.9847906 -3.9415257 -3.9692347 -4.0405231 -4.082305 -4.1102414 -4.1567841 -4.2069163 -4.2544723][-4.2393589 -4.2239122 -4.1876268 -4.1331134 -4.0626936 -3.9668658 -3.863137 -3.8082232 -3.8607583 -3.9751792 -4.0507464 -4.1057014 -4.1694779 -4.2276645 -4.2728457][-4.2305765 -4.20958 -4.169076 -4.11276 -4.0398159 -3.9389236 -3.8171484 -3.7455065 -3.7998219 -3.9288177 -4.0203819 -4.093154 -4.1675463 -4.2338033 -4.2799578][-4.2243481 -4.2011929 -4.1658506 -4.1201081 -4.0621181 -3.9812891 -3.8836889 -3.8306103 -3.8647261 -3.9608395 -4.0384531 -4.1100249 -4.1818414 -4.2422509 -4.2841549][-4.2320457 -4.2068558 -4.1784673 -4.1459479 -4.1053205 -4.0465703 -3.984931 -3.960362 -3.9837339 -4.0505457 -4.1089268 -4.1652555 -4.2191491 -4.2601619 -4.2893806][-4.2578821 -4.2336617 -4.2088957 -4.1834679 -4.1543694 -4.1115303 -4.0771294 -4.0768743 -4.1021156 -4.1514921 -4.1949382 -4.2351174 -4.2679844 -4.2881689 -4.3037996][-4.2935181 -4.2776909 -4.2597365 -4.2387505 -4.2170424 -4.1886892 -4.1690989 -4.1752992 -4.1979375 -4.2347503 -4.2671514 -4.2928834 -4.3100624 -4.3167191 -4.3226194][-4.32202 -4.3166122 -4.3074408 -4.2933583 -4.28047 -4.2629037 -4.2479072 -4.2499876 -4.2635822 -4.2868242 -4.3089375 -4.3246603 -4.333395 -4.3343725 -4.3356538][-4.3366156 -4.3367276 -4.3338037 -4.326592 -4.3193212 -4.3100824 -4.3002663 -4.2988038 -4.3035631 -4.3147821 -4.3283911 -4.3376379 -4.342145 -4.3418736 -4.3416886]]...]
INFO - root - 2017-12-05 19:32:26.896152: step 38110, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 72h:01m:18s remains)
INFO - root - 2017-12-05 19:32:35.558750: step 38120, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 70h:05m:13s remains)
INFO - root - 2017-12-05 19:32:44.164038: step 38130, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 71h:13m:57s remains)
INFO - root - 2017-12-05 19:32:52.629718: step 38140, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 69h:48m:29s remains)
INFO - root - 2017-12-05 19:33:01.081287: step 38150, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.879 sec/batch; 71h:53m:13s remains)
INFO - root - 2017-12-05 19:33:09.635314: step 38160, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 68h:51m:14s remains)
INFO - root - 2017-12-05 19:33:18.156555: step 38170, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 69h:16m:13s remains)
INFO - root - 2017-12-05 19:33:26.677717: step 38180, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 69h:51m:08s remains)
INFO - root - 2017-12-05 19:33:35.241263: step 38190, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 72h:23m:56s remains)
INFO - root - 2017-12-05 19:33:43.822387: step 38200, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 69h:57m:32s remains)
2017-12-05 19:33:44.681093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2591624 -4.2558975 -4.2634249 -4.2669673 -4.268734 -4.2734818 -4.2758102 -4.2780924 -4.2811475 -4.2817969 -4.28281 -4.2835274 -4.2833219 -4.2868466 -4.2895389][-4.2293243 -4.2201881 -4.2258019 -4.2265921 -4.22683 -4.2297907 -4.2304273 -4.2326131 -4.2375908 -4.23617 -4.2379208 -4.2451925 -4.2497678 -4.2572746 -4.2678161][-4.1936936 -4.1733789 -4.1747379 -4.1769252 -4.1785669 -4.1783447 -4.1735373 -4.1737447 -4.1793818 -4.1807451 -4.1899567 -4.203651 -4.2122521 -4.2211142 -4.243103][-4.1611738 -4.1280713 -4.119637 -4.1200008 -4.1238356 -4.1222687 -4.1074739 -4.0979853 -4.1035748 -4.116055 -4.1316309 -4.1431127 -4.1547542 -4.1678057 -4.2015619][-4.1389656 -4.0896964 -4.0611362 -4.0541821 -4.0562754 -4.0524144 -4.0257368 -3.9957206 -3.99589 -4.0253105 -4.056416 -4.0658321 -4.0793767 -4.1066713 -4.1526122][-4.1150923 -4.0435553 -3.9871879 -3.9606447 -3.9563642 -3.952291 -3.9168556 -3.8533936 -3.825361 -3.8827741 -3.9406977 -3.9508989 -3.9756584 -4.0232677 -4.0905175][-4.077785 -3.9742672 -3.8780673 -3.8194485 -3.8059156 -3.8009949 -3.7463851 -3.6169312 -3.5121236 -3.6299562 -3.7577734 -3.795083 -3.842391 -3.9219885 -4.0188832][-4.0409784 -3.9029598 -3.7649302 -3.6722996 -3.6530211 -3.6428332 -3.5660744 -3.3710032 -3.19741 -3.4063518 -3.6198061 -3.6936102 -3.7584653 -3.856189 -3.9648271][-4.0265331 -3.878474 -3.7311878 -3.6390884 -3.6392648 -3.6377089 -3.5806937 -3.4249408 -3.2985563 -3.486407 -3.6663649 -3.7269406 -3.7865496 -3.8704126 -3.9567304][-4.0490932 -3.9279099 -3.8145165 -3.7566347 -3.7708964 -3.7713943 -3.7436566 -3.6755095 -3.6270807 -3.729413 -3.8183064 -3.8462715 -3.8966546 -3.9600756 -4.020155][-4.091423 -4.0085387 -3.9360266 -3.9078636 -3.91637 -3.90668 -3.8965902 -3.8839805 -3.8794122 -3.9303856 -3.9613762 -3.9752574 -4.0172052 -4.0597577 -4.1021485][-4.1527019 -4.0981841 -4.0519466 -4.036365 -4.0447531 -4.0368166 -4.0355029 -4.0392003 -4.0542822 -4.0904918 -4.1066647 -4.1194406 -4.1436262 -4.1624227 -4.1854][-4.2230625 -4.1883869 -4.1625056 -4.158299 -4.1704817 -4.1713529 -4.1760712 -4.1758142 -4.1886568 -4.2130108 -4.2259645 -4.2396779 -4.252213 -4.25861 -4.2697854][-4.283709 -4.2661281 -4.2542744 -4.2550883 -4.2650328 -4.2703395 -4.2766848 -4.2755623 -4.2808409 -4.2928324 -4.2994533 -4.3060851 -4.311451 -4.3148942 -4.3207922][-4.3115883 -4.3045716 -4.3003078 -4.3015532 -4.3048458 -4.3085933 -4.3130226 -4.3134217 -4.3158813 -4.3225222 -4.3264532 -4.3286924 -4.3295784 -4.3296394 -4.3318253]]...]
INFO - root - 2017-12-05 19:33:53.248369: step 38210, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 69h:08m:35s remains)
INFO - root - 2017-12-05 19:34:01.646258: step 38220, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 70h:39m:00s remains)
INFO - root - 2017-12-05 19:34:10.199207: step 38230, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 71h:49m:46s remains)
INFO - root - 2017-12-05 19:34:18.708856: step 38240, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.828 sec/batch; 67h:42m:37s remains)
INFO - root - 2017-12-05 19:34:26.937225: step 38250, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 69h:30m:53s remains)
INFO - root - 2017-12-05 19:34:35.411435: step 38260, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 69h:44m:57s remains)
INFO - root - 2017-12-05 19:34:44.142312: step 38270, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 70h:42m:03s remains)
INFO - root - 2017-12-05 19:34:52.589181: step 38280, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 68h:16m:26s remains)
INFO - root - 2017-12-05 19:35:01.172676: step 38290, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 69h:11m:23s remains)
INFO - root - 2017-12-05 19:35:09.660766: step 38300, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 71h:51m:58s remains)
2017-12-05 19:35:10.447231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3007069 -4.2997861 -4.2840424 -4.2631631 -4.2475615 -4.2383003 -4.2279515 -4.2171421 -4.2078786 -4.1800117 -4.13122 -4.0897026 -4.0730762 -4.0744648 -4.0860033][-4.3002248 -4.2961245 -4.2765913 -4.2533965 -4.2397027 -4.2346711 -4.2285638 -4.2234364 -4.2203016 -4.1988254 -4.1504636 -4.1094141 -4.0965495 -4.0979185 -4.1034937][-4.28647 -4.2772937 -4.2572141 -4.2345805 -4.2239962 -4.2221189 -4.2184153 -4.2167563 -4.2178226 -4.2019792 -4.1560793 -4.1126108 -4.0981412 -4.102529 -4.1120224][-4.2657075 -4.2521472 -4.2320123 -4.2127681 -4.2067356 -4.2058315 -4.2030349 -4.1980309 -4.195209 -4.1838737 -4.1448054 -4.1049414 -4.0893788 -4.0953035 -4.1116438][-4.2373762 -4.2238374 -4.2065444 -4.1921911 -4.1875706 -4.1805162 -4.1708918 -4.1570859 -4.1431112 -4.133008 -4.1044922 -4.0788994 -4.0774446 -4.0934629 -4.11501][-4.210207 -4.199615 -4.1822362 -4.1631122 -4.1463771 -4.1251764 -4.106523 -4.0849028 -4.0601158 -4.0502586 -4.0386071 -4.0397272 -4.0647244 -4.0954628 -4.1254435][-4.1631422 -4.1496325 -4.1279278 -4.1027718 -4.0749254 -4.0403018 -4.0095816 -3.9723926 -3.9414575 -3.9514391 -3.9765503 -4.014442 -4.0652485 -4.1070242 -4.1407752][-4.0917883 -4.068954 -4.0464177 -4.0219712 -3.9882891 -3.9440057 -3.9011643 -3.8519192 -3.8317776 -3.8865108 -3.9552023 -4.0233884 -4.0871387 -4.1316552 -4.1622267][-4.0208 -4.0016603 -3.9897895 -3.9811733 -3.9598556 -3.9264319 -3.8987875 -3.869195 -3.8723018 -3.9368069 -4.0021882 -4.0611892 -4.1141748 -4.1510167 -4.174211][-3.9920924 -3.993156 -4.0029783 -4.0147233 -4.010344 -3.9960713 -3.9904122 -3.985939 -3.997231 -4.0367155 -4.0702004 -4.0975227 -4.1224327 -4.1450133 -4.1633167][-4.0198541 -4.0359035 -4.0523558 -4.0687351 -4.0729017 -4.0682096 -4.0731511 -4.0836959 -4.0968013 -4.1143103 -4.1218843 -4.120028 -4.1164703 -4.1207519 -4.13538][-4.0515385 -4.0705295 -4.08661 -4.1021552 -4.1139851 -4.1169653 -4.1223936 -4.1324048 -4.1427412 -4.1449447 -4.1301284 -4.1072412 -4.087584 -4.0846529 -4.1020451][-4.0607014 -4.0770679 -4.0986042 -4.1192088 -4.1350584 -4.1394367 -4.137125 -4.1361909 -4.1359997 -4.125483 -4.100276 -4.0734644 -4.0598655 -4.0699883 -4.0997887][-4.0706563 -4.0739937 -4.09129 -4.1119385 -4.1291161 -4.1335716 -4.1285367 -4.1240568 -4.1198378 -4.1090326 -4.0910578 -4.0760231 -4.0788965 -4.1002316 -4.1302624][-4.0781622 -4.07215 -4.0847163 -4.1082029 -4.129106 -4.1365447 -4.1328535 -4.1276383 -4.1246414 -4.1183248 -4.1085243 -4.1018844 -4.1060729 -4.12217 -4.1403828]]...]
INFO - root - 2017-12-05 19:35:19.046858: step 38310, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 70h:08m:23s remains)
INFO - root - 2017-12-05 19:35:27.635103: step 38320, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 69h:00m:38s remains)
INFO - root - 2017-12-05 19:35:36.156653: step 38330, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 68h:33m:47s remains)
INFO - root - 2017-12-05 19:35:44.716330: step 38340, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 73h:41m:43s remains)
INFO - root - 2017-12-05 19:35:53.115241: step 38350, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.721 sec/batch; 58h:53m:25s remains)
INFO - root - 2017-12-05 19:36:01.625979: step 38360, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 70h:33m:47s remains)
INFO - root - 2017-12-05 19:36:10.222960: step 38370, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 69h:13m:15s remains)
INFO - root - 2017-12-05 19:36:18.771468: step 38380, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 69h:34m:19s remains)
INFO - root - 2017-12-05 19:36:27.235034: step 38390, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 70h:01m:23s remains)
INFO - root - 2017-12-05 19:36:35.809509: step 38400, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 69h:43m:18s remains)
2017-12-05 19:36:36.554624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1830277 -4.1827397 -4.1812506 -4.1814079 -4.1838155 -4.1860762 -4.19156 -4.2005992 -4.2105508 -4.2177258 -4.2049794 -4.1739883 -4.1363511 -4.1035604 -4.0608797][-4.1667871 -4.1682229 -4.1654162 -4.1618028 -4.1620111 -4.1689239 -4.1874022 -4.2106304 -4.2282786 -4.2387891 -4.2273068 -4.2009554 -4.1669397 -4.1307259 -4.087532][-4.1519022 -4.148078 -4.1369529 -4.1267819 -4.123178 -4.1301446 -4.157043 -4.1928291 -4.2229924 -4.2434082 -4.2437334 -4.2298908 -4.2084947 -4.1823626 -4.1532078][-4.1690574 -4.1604829 -4.1458941 -4.1333828 -4.1258917 -4.123693 -4.1415005 -4.1776857 -4.2128129 -4.2363482 -4.2436976 -4.24089 -4.2295136 -4.2127094 -4.1958952][-4.2108512 -4.1984692 -4.1812258 -4.1683517 -4.1599946 -4.1550474 -4.1637316 -4.1897812 -4.2198458 -4.2394266 -4.2471313 -4.2506952 -4.2490263 -4.2404857 -4.2259808][-4.2418933 -4.2304726 -4.2116804 -4.1963716 -4.1860647 -4.1755071 -4.1735311 -4.1905403 -4.21789 -4.2395048 -4.2549782 -4.2653933 -4.268846 -4.2638288 -4.251627][-4.2457533 -4.2357254 -4.2164407 -4.1968384 -4.1779046 -4.1557736 -4.1398149 -4.1509056 -4.1816635 -4.2146912 -4.2457662 -4.26968 -4.2853341 -4.2886114 -4.2814078][-4.2400408 -4.226665 -4.2008591 -4.1699367 -4.137527 -4.0967736 -4.0609555 -4.0642829 -4.1077724 -4.1637306 -4.2184868 -4.2623777 -4.2954946 -4.3107347 -4.3113275][-4.2530112 -4.23152 -4.1962748 -4.155448 -4.1065912 -4.041635 -3.9813373 -3.9670622 -4.0168133 -4.0928135 -4.1714678 -4.236351 -4.28774 -4.3159814 -4.32379][-4.275207 -4.2498021 -4.210021 -4.168087 -4.1156783 -4.0454354 -3.972908 -3.939121 -3.9759285 -4.05327 -4.1368251 -4.2094803 -4.2702212 -4.3076024 -4.321209][-4.29291 -4.2706676 -4.2347279 -4.1993384 -4.1594181 -4.1076283 -4.049818 -4.0153294 -4.0327754 -4.0887008 -4.1553354 -4.2141571 -4.263835 -4.2962356 -4.3099837][-4.3001437 -4.284164 -4.2578917 -4.232017 -4.2071185 -4.1788788 -4.1475811 -4.1288834 -4.1366053 -4.1672149 -4.2072496 -4.244483 -4.2773538 -4.2994342 -4.308795][-4.3049369 -4.2939644 -4.2781053 -4.2622495 -4.24836 -4.2348537 -4.2216907 -4.2144189 -4.2178993 -4.232986 -4.25605 -4.278821 -4.2989321 -4.3126683 -4.3187504][-4.3140349 -4.303535 -4.2912245 -4.2793393 -4.2694964 -4.261838 -4.2573514 -4.2552209 -4.2564526 -4.2644143 -4.2783608 -4.293787 -4.3078189 -4.3175011 -4.323144][-4.3245478 -4.3154283 -4.3055148 -4.2954617 -4.285953 -4.2784629 -4.2743955 -4.2728438 -4.2725258 -4.27576 -4.2840948 -4.2954025 -4.30662 -4.315999 -4.3221121]]...]
INFO - root - 2017-12-05 19:36:45.264281: step 38410, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 68h:54m:05s remains)
INFO - root - 2017-12-05 19:36:53.730307: step 38420, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 67h:45m:45s remains)
INFO - root - 2017-12-05 19:37:02.349732: step 38430, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 68h:41m:28s remains)
INFO - root - 2017-12-05 19:37:10.917146: step 38440, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 68h:40m:58s remains)
INFO - root - 2017-12-05 19:37:19.270542: step 38450, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 70h:12m:03s remains)
INFO - root - 2017-12-05 19:37:27.672036: step 38460, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.848 sec/batch; 69h:16m:27s remains)
INFO - root - 2017-12-05 19:37:36.149352: step 38470, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 69h:05m:59s remains)
INFO - root - 2017-12-05 19:37:44.837786: step 38480, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 71h:50m:35s remains)
INFO - root - 2017-12-05 19:37:53.490209: step 38490, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 72h:17m:17s remains)
INFO - root - 2017-12-05 19:38:01.917340: step 38500, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 72h:01m:35s remains)
2017-12-05 19:38:02.720269: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1790342 -4.1591148 -4.1383495 -4.1248488 -4.1188965 -4.1201587 -4.12302 -4.1306286 -4.1371131 -4.1515279 -4.173244 -4.1988568 -4.2220349 -4.2339568 -4.2383337][-4.1834269 -4.168467 -4.1542354 -4.1431003 -4.1378465 -4.1413937 -4.1423883 -4.144413 -4.1454821 -4.1532154 -4.1669273 -4.1863174 -4.20132 -4.2054138 -4.2077808][-4.18007 -4.1716051 -4.1618986 -4.15181 -4.1512313 -4.1623721 -4.1698818 -4.1721907 -4.1696138 -4.1648507 -4.1617985 -4.1632237 -4.166338 -4.165493 -4.1707807][-4.1637869 -4.160954 -4.1519556 -4.141561 -4.1418657 -4.1554065 -4.1681957 -4.1759553 -4.1776862 -4.17148 -4.1572018 -4.1400232 -4.1322088 -4.1318622 -4.1376162][-4.1442566 -4.1494164 -4.143559 -4.1337094 -4.1255379 -4.1313233 -4.1430154 -4.1534367 -4.1587696 -4.1579881 -4.1473627 -4.1274085 -4.1146822 -4.1126933 -4.1152267][-4.1262169 -4.1365905 -4.1295772 -4.1146421 -4.0968728 -4.0928779 -4.0987692 -4.1085157 -4.1215219 -4.132587 -4.1416769 -4.13419 -4.1246424 -4.119822 -4.1120067][-4.0918369 -4.1003165 -4.0890756 -4.0632377 -4.0327067 -4.0132308 -4.0074267 -4.021626 -4.0519242 -4.0887947 -4.1250381 -4.1405435 -4.1374149 -4.1295857 -4.1164532][-4.0629478 -4.05858 -4.0352068 -4.0027232 -3.9713013 -3.9490554 -3.9346805 -3.9528809 -3.9964254 -4.0502868 -4.1008649 -4.1291895 -4.1342111 -4.1288266 -4.1228604][-4.0693316 -4.0467019 -4.0095387 -3.9763641 -3.9541631 -3.9432585 -3.9266753 -3.9369705 -3.9758954 -4.0264473 -4.0731583 -4.0990725 -4.108572 -4.1088061 -4.1162491][-4.0933666 -4.0608325 -4.01756 -3.9867964 -3.9699457 -3.9623284 -3.947119 -3.944787 -3.9666646 -4.0036716 -4.0350685 -4.0531611 -4.0661526 -4.0726094 -4.093029][-4.1283736 -4.0985074 -4.06392 -4.0393405 -4.0275583 -4.01827 -3.9995275 -3.9881248 -3.9950902 -4.0148849 -4.0277967 -4.0413427 -4.0584574 -4.0699635 -4.0899014][-4.1691608 -4.15045 -4.1326771 -4.1177959 -4.1109028 -4.1047025 -4.0863118 -4.07065 -4.0708542 -4.0737343 -4.0724573 -4.0806351 -4.1000314 -4.1137524 -4.1268568][-4.2102485 -4.2026529 -4.197969 -4.1936088 -4.1931648 -4.1923485 -4.1817675 -4.1684623 -4.1654739 -4.1648564 -4.1603241 -4.162766 -4.1749535 -4.1859784 -4.1878219][-4.2361059 -4.2335582 -4.2330422 -4.2335644 -4.2360172 -4.2380476 -4.2356157 -4.2312703 -4.2302985 -4.2318754 -4.230135 -4.2304649 -4.2373848 -4.241694 -4.2354794][-4.2282095 -4.2285242 -4.2292185 -4.2296047 -4.2310386 -4.2332058 -4.2337818 -4.2327185 -4.23444 -4.2386971 -4.2407522 -4.2430644 -4.2493339 -4.2522364 -4.2427921]]...]
INFO - root - 2017-12-05 19:38:11.321509: step 38510, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 70h:33m:30s remains)
INFO - root - 2017-12-05 19:38:19.809772: step 38520, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 69h:19m:18s remains)
INFO - root - 2017-12-05 19:38:28.329092: step 38530, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 68h:34m:49s remains)
INFO - root - 2017-12-05 19:38:36.866631: step 38540, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 68h:43m:56s remains)
INFO - root - 2017-12-05 19:38:45.210057: step 38550, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 69h:11m:35s remains)
INFO - root - 2017-12-05 19:38:53.681360: step 38560, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 68h:25m:57s remains)
INFO - root - 2017-12-05 19:39:02.018387: step 38570, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 68h:08m:07s remains)
INFO - root - 2017-12-05 19:39:10.559714: step 38580, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 73h:06m:40s remains)
INFO - root - 2017-12-05 19:39:19.177640: step 38590, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 68h:04m:30s remains)
INFO - root - 2017-12-05 19:39:27.658594: step 38600, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.829 sec/batch; 67h:41m:36s remains)
2017-12-05 19:39:28.511164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2428679 -4.2488303 -4.2532387 -4.258945 -4.2648783 -4.2702813 -4.2692184 -4.2595282 -4.2456226 -4.2329631 -4.2302485 -4.240057 -4.2566676 -4.2777176 -4.2998438][-4.2111921 -4.2208843 -4.228631 -4.2373981 -4.2447381 -4.24627 -4.2359152 -4.2173705 -4.1984038 -4.185091 -4.1861973 -4.2026415 -4.2257171 -4.2534418 -4.2820578][-4.2025514 -4.2126493 -4.21893 -4.2270141 -4.2300611 -4.225256 -4.2066603 -4.18023 -4.1584849 -4.147059 -4.1532912 -4.1747642 -4.2024832 -4.2355146 -4.2712092][-4.2061133 -4.2120638 -4.2133622 -4.2162986 -4.2123871 -4.2010436 -4.176722 -4.1445613 -4.1186137 -4.1085925 -4.1238413 -4.1539059 -4.1866646 -4.2251906 -4.2679963][-4.2084169 -4.2077851 -4.2024722 -4.19626 -4.1824188 -4.159586 -4.1234317 -4.07656 -4.0415783 -4.0429111 -4.0793595 -4.1262693 -4.1708717 -4.2207737 -4.2705255][-4.2077403 -4.2004957 -4.188025 -4.1713262 -4.1436763 -4.1025982 -4.045804 -3.9757178 -3.9374776 -3.9713531 -4.0453172 -4.1113467 -4.1663003 -4.2246561 -4.2774396][-4.2016549 -4.18976 -4.1704068 -4.1420622 -4.0984473 -4.0400081 -3.9657681 -3.8754845 -3.8520072 -3.9326091 -4.0390067 -4.1128268 -4.1728697 -4.2351933 -4.287931][-4.1838646 -4.175405 -4.1558161 -4.1200066 -4.0687194 -4.0050278 -3.9367378 -3.8696427 -3.8765283 -3.9656093 -4.0588832 -4.1237569 -4.1834869 -4.2452593 -4.2960863][-4.1570268 -4.1574121 -4.1410913 -4.10625 -4.0603881 -4.0122051 -3.9714115 -3.9347382 -3.9524364 -4.0212674 -4.0868216 -4.1416039 -4.1997614 -4.2567387 -4.3021379][-4.1194406 -4.1358914 -4.1288671 -4.1042075 -4.0754318 -4.0512009 -4.0302744 -4.00692 -4.0224733 -4.0715022 -4.1163549 -4.163269 -4.2142138 -4.2634869 -4.3057814][-4.09632 -4.1286058 -4.1341157 -4.1197162 -4.1045485 -4.0962257 -4.0801563 -4.0578866 -4.07205 -4.1090374 -4.1402245 -4.1791925 -4.2244678 -4.271513 -4.3125024][-4.0997362 -4.1352353 -4.1473446 -4.139637 -4.1317334 -4.1323414 -4.1175723 -4.0983057 -4.1091027 -4.1338272 -4.1588335 -4.1938434 -4.236958 -4.2801309 -4.3183804][-4.1227384 -4.1461759 -4.1587186 -4.1590514 -4.156364 -4.1587033 -4.1464381 -4.1327538 -4.1392131 -4.1572719 -4.1809587 -4.2136889 -4.2497649 -4.285018 -4.3179421][-4.1609073 -4.1694336 -4.17927 -4.1848645 -4.1856294 -4.1893091 -4.1825223 -4.1782765 -4.1849418 -4.1955056 -4.210526 -4.2362604 -4.2641983 -4.2910342 -4.3188105][-4.2024207 -4.2059073 -4.2151961 -4.2253513 -4.2296333 -4.233624 -4.2281809 -4.22549 -4.2307324 -4.2350698 -4.2425871 -4.2617149 -4.2832689 -4.3038821 -4.3252177]]...]
INFO - root - 2017-12-05 19:39:36.917994: step 38610, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 68h:56m:51s remains)
INFO - root - 2017-12-05 19:39:45.360762: step 38620, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 70h:20m:06s remains)
INFO - root - 2017-12-05 19:39:53.799995: step 38630, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.833 sec/batch; 68h:02m:12s remains)
INFO - root - 2017-12-05 19:40:02.287805: step 38640, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 69h:45m:31s remains)
INFO - root - 2017-12-05 19:40:10.669050: step 38650, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 67h:50m:19s remains)
INFO - root - 2017-12-05 19:40:19.165121: step 38660, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 67h:57m:20s remains)
INFO - root - 2017-12-05 19:40:27.760984: step 38670, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 68h:02m:23s remains)
INFO - root - 2017-12-05 19:40:36.235712: step 38680, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 71h:16m:17s remains)
INFO - root - 2017-12-05 19:40:44.807793: step 38690, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 72h:09m:59s remains)
INFO - root - 2017-12-05 19:40:53.347491: step 38700, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 70h:16m:10s remains)
2017-12-05 19:40:54.094302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1827912 -4.179347 -4.1669354 -4.1613812 -4.173079 -4.1978807 -4.2146168 -4.2039289 -4.1832623 -4.1658263 -4.1497231 -4.1521254 -4.1822338 -4.2229457 -4.2557483][-4.1909723 -4.1874251 -4.1755719 -4.1750774 -4.1875405 -4.2030663 -4.2068186 -4.185564 -4.1614776 -4.1532059 -4.1487303 -4.1581 -4.19126 -4.2327271 -4.2646165][-4.1971912 -4.1921277 -4.1827979 -4.1868052 -4.19618 -4.2038641 -4.1992331 -4.1719432 -4.1435461 -4.1368017 -4.1429968 -4.1607647 -4.2003913 -4.243073 -4.2747641][-4.2166562 -4.2131 -4.2052636 -4.2066326 -4.2061181 -4.2036653 -4.1891055 -4.1579971 -4.1290817 -4.1212397 -4.1351342 -4.1642003 -4.2128797 -4.2586889 -4.2912912][-4.2368836 -4.2350564 -4.2259436 -4.2216768 -4.21408 -4.19821 -4.16829 -4.1308908 -4.1063309 -4.1026082 -4.1233187 -4.1679654 -4.2250071 -4.2754259 -4.3096652][-4.2476044 -4.2448921 -4.2317228 -4.2218924 -4.2092562 -4.1834965 -4.1367111 -4.09155 -4.0739927 -4.083087 -4.1176357 -4.1752281 -4.237494 -4.2907782 -4.3269691][-4.243875 -4.235702 -4.2191572 -4.2055483 -4.1900811 -4.1597204 -4.1030498 -4.0546494 -4.0438576 -4.0696316 -4.1247296 -4.19171 -4.2544627 -4.3068547 -4.3418612][-4.2306833 -4.2154036 -4.1960149 -4.1780953 -4.15841 -4.1281662 -4.0795684 -4.0388613 -4.0363994 -4.075202 -4.1437712 -4.2137294 -4.272687 -4.3209491 -4.3524613][-4.2228026 -4.2052345 -4.1858845 -4.1627221 -4.1366997 -4.1057472 -4.0655503 -4.0355816 -4.0409327 -4.0846925 -4.1558847 -4.2261572 -4.2837253 -4.3295116 -4.3563948][-4.229033 -4.2127271 -4.1929526 -4.1653128 -4.1331277 -4.0998764 -4.0644197 -4.0420594 -4.0526643 -4.09663 -4.1633573 -4.2298656 -4.2888093 -4.3333859 -4.3563633][-4.2477756 -4.2309289 -4.2098169 -4.1801395 -4.1461244 -4.1130781 -4.0829639 -4.0657377 -4.0769486 -4.1166792 -4.1743479 -4.235064 -4.2931676 -4.3341789 -4.352622][-4.2659874 -4.252739 -4.233727 -4.2074757 -4.176517 -4.1460638 -4.1189427 -4.103025 -4.1130686 -4.1476555 -4.1956234 -4.2483473 -4.2997932 -4.3341188 -4.3477359][-4.2769217 -4.2705941 -4.2589064 -4.2415891 -4.2173948 -4.1906195 -4.16576 -4.1496668 -4.1574793 -4.1861043 -4.2254453 -4.2681961 -4.308217 -4.3341665 -4.3443928][-4.2906384 -4.2906103 -4.2867756 -4.2777467 -4.2604971 -4.2377605 -4.2142782 -4.1994576 -4.2049022 -4.2271895 -4.2574563 -4.2888007 -4.3166203 -4.3346572 -4.3424625][-4.3080692 -4.3097153 -4.3097973 -4.3062968 -4.2967997 -4.28097 -4.2631822 -4.2512684 -4.2534537 -4.2678246 -4.2886024 -4.3098154 -4.3271165 -4.338522 -4.3438377]]...]
INFO - root - 2017-12-05 19:41:02.547701: step 38710, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 67h:24m:51s remains)
INFO - root - 2017-12-05 19:41:11.067022: step 38720, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.846 sec/batch; 69h:01m:00s remains)
INFO - root - 2017-12-05 19:41:19.703404: step 38730, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.871 sec/batch; 71h:04m:23s remains)
INFO - root - 2017-12-05 19:41:28.274359: step 38740, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 68h:43m:57s remains)
INFO - root - 2017-12-05 19:41:36.655306: step 38750, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 70h:24m:10s remains)
INFO - root - 2017-12-05 19:41:45.239419: step 38760, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 71h:44m:13s remains)
INFO - root - 2017-12-05 19:41:53.724200: step 38770, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 69h:55m:15s remains)
INFO - root - 2017-12-05 19:42:02.235451: step 38780, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 68h:48m:48s remains)
INFO - root - 2017-12-05 19:42:10.759030: step 38790, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 71h:31m:53s remains)
INFO - root - 2017-12-05 19:42:19.246938: step 38800, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 69h:02m:50s remains)
2017-12-05 19:42:20.023275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2320361 -4.2428894 -4.2471161 -4.2437882 -4.2397718 -4.2380037 -4.2377877 -4.2418871 -4.24636 -4.2469211 -4.24733 -4.2470765 -4.2457771 -4.2416081 -4.2301407][-4.2398281 -4.250021 -4.2507572 -4.2429466 -4.2364197 -4.2360263 -4.23883 -4.2451177 -4.2516317 -4.2549562 -4.2566748 -4.2558489 -4.2529478 -4.2462192 -4.2332025][-4.2448015 -4.2532868 -4.2509885 -4.2412043 -4.233912 -4.2344275 -4.2387714 -4.24606 -4.2538724 -4.2605047 -4.2649646 -4.2651277 -4.2624717 -4.2563848 -4.2463932][-4.2427764 -4.2493472 -4.2446733 -4.2353039 -4.2276964 -4.2269917 -4.2298036 -4.2371426 -4.2471008 -4.2589445 -4.2695136 -4.274075 -4.2741318 -4.2704892 -4.2647338][-4.2259703 -4.229887 -4.2248554 -4.2181616 -4.2129545 -4.2108054 -4.209209 -4.2135334 -4.2236114 -4.2400556 -4.2567921 -4.2672591 -4.2718687 -4.2720413 -4.2697821][-4.1989551 -4.1993518 -4.1934643 -4.1861467 -4.1792455 -4.1698432 -4.1577463 -4.1547546 -4.1641388 -4.1861825 -4.2097659 -4.2261329 -4.2370725 -4.2432151 -4.2448473][-4.1732 -4.17198 -4.1638856 -4.1519203 -4.1353559 -4.1090322 -4.07842 -4.0658827 -4.075285 -4.1017718 -4.1295714 -4.1502218 -4.1664238 -4.1792812 -4.1877408][-4.1661663 -4.1674762 -4.1601825 -4.1451063 -4.1172071 -4.0725188 -4.0238581 -4.0034728 -4.0136762 -4.0424604 -4.0715017 -4.0936737 -4.1123023 -4.1289864 -4.142477][-4.180872 -4.187356 -4.1854029 -4.1739764 -4.1447663 -4.0964355 -4.0457158 -4.0265274 -4.0383468 -4.0645275 -4.0894327 -4.1086259 -4.1259646 -4.1419597 -4.1541724][-4.1989903 -4.2082772 -4.2118249 -4.2065864 -4.1844068 -4.1456227 -4.1065421 -4.0949025 -4.1088538 -4.1318789 -4.1522098 -4.1666365 -4.1789894 -4.188633 -4.192028][-4.2076778 -4.2174392 -4.2233281 -4.22144 -4.2060342 -4.1778364 -4.1511111 -4.1456432 -4.1590433 -4.1775322 -4.1920414 -4.2003503 -4.2063994 -4.2080221 -4.201746][-4.2135267 -4.2219944 -4.2269835 -4.2250261 -4.2138519 -4.1951141 -4.1778874 -4.1732311 -4.18015 -4.1898913 -4.1962819 -4.1993814 -4.2006645 -4.19696 -4.1842842][-4.2260394 -4.2329636 -4.2354617 -4.2319603 -4.2232571 -4.213141 -4.2033234 -4.1970587 -4.1963677 -4.1975284 -4.1979456 -4.1980586 -4.1978474 -4.1926975 -4.1778593][-4.2415075 -4.2477207 -4.2474089 -4.2426887 -4.2358794 -4.2308035 -4.2255392 -4.2199945 -4.2173262 -4.2152348 -4.2136078 -4.21313 -4.2131529 -4.2099023 -4.1980739][-4.2506747 -4.2559996 -4.2543526 -4.2504077 -4.245739 -4.2429233 -4.2396584 -4.2368827 -4.2355552 -4.2339535 -4.2330322 -4.2332325 -4.2338934 -4.2332554 -4.227654]]...]
INFO - root - 2017-12-05 19:42:28.566182: step 38810, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 70h:50m:38s remains)
INFO - root - 2017-12-05 19:42:37.161635: step 38820, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 69h:55m:03s remains)
INFO - root - 2017-12-05 19:42:45.548736: step 38830, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.803 sec/batch; 65h:31m:20s remains)
INFO - root - 2017-12-05 19:42:54.048810: step 38840, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 68h:36m:11s remains)
INFO - root - 2017-12-05 19:43:02.404303: step 38850, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 67h:02m:55s remains)
INFO - root - 2017-12-05 19:43:10.947656: step 38860, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 69h:05m:13s remains)
INFO - root - 2017-12-05 19:43:19.448175: step 38870, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 68h:54m:01s remains)
INFO - root - 2017-12-05 19:43:27.984885: step 38880, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 71h:44m:47s remains)
INFO - root - 2017-12-05 19:43:36.327874: step 38890, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 67h:28m:11s remains)
INFO - root - 2017-12-05 19:43:44.817302: step 38900, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 70h:24m:06s remains)
2017-12-05 19:43:45.585870: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1536665 -4.1299472 -4.096746 -4.0830274 -4.0872436 -4.0943646 -4.0903864 -4.0866404 -4.1021223 -4.1399035 -4.1699767 -4.181694 -4.171349 -4.1438651 -4.1233678][-4.1716762 -4.1462607 -4.1108904 -4.0918164 -4.0977483 -4.117475 -4.1241555 -4.1245832 -4.1389785 -4.171885 -4.1975546 -4.2113614 -4.21174 -4.1979694 -4.1854486][-4.19387 -4.1626315 -4.1250196 -4.1067309 -4.1197877 -4.1517344 -4.1640515 -4.1612396 -4.1724033 -4.2033238 -4.2304673 -4.2528591 -4.2646227 -4.2596149 -4.2502003][-4.1975636 -4.1670809 -4.1334934 -4.1180916 -4.1346488 -4.1669536 -4.1766224 -4.1639628 -4.168714 -4.2017956 -4.2383265 -4.2714396 -4.2934551 -4.2934623 -4.282443][-4.180903 -4.1614828 -4.1407061 -4.1307316 -4.1418443 -4.1574607 -4.1485023 -4.1169653 -4.1203294 -4.1658583 -4.2185631 -4.2617435 -4.2874165 -4.28694 -4.2723289][-4.1671324 -4.157939 -4.1486106 -4.1402016 -4.1358824 -4.1204853 -4.075151 -4.0200377 -4.0301309 -4.0997477 -4.1755142 -4.2283783 -4.2557983 -4.2527547 -4.2347221][-4.1622624 -4.1570997 -4.1518269 -4.1405892 -4.121357 -4.0755954 -3.9873371 -3.9048731 -3.9327216 -4.0374026 -4.1352358 -4.1926074 -4.2156534 -4.205699 -4.18362][-4.165659 -4.1604109 -4.1563416 -4.1454258 -4.1215706 -4.066422 -3.96403 -3.8802371 -3.9227161 -4.0347133 -4.1258817 -4.1695542 -4.1754251 -4.153204 -4.1330032][-4.1737156 -4.1651688 -4.1614275 -4.1561069 -4.1424141 -4.1047258 -4.0288639 -3.971436 -4.0008125 -4.0755982 -4.1326342 -4.1494288 -4.1349344 -4.1083241 -4.1012321][-4.1770205 -4.16845 -4.1681046 -4.1727457 -4.1721296 -4.1537638 -4.10556 -4.067256 -4.0794692 -4.1179833 -4.1406937 -4.1316485 -4.10091 -4.0785317 -4.0840697][-4.1767092 -4.1695919 -4.1731257 -4.1883378 -4.1955681 -4.1850042 -4.1545186 -4.1314211 -4.1382513 -4.1597795 -4.1652403 -4.1442833 -4.1069074 -4.0867457 -4.0963578][-4.1759663 -4.1784539 -4.1846623 -4.1990914 -4.2081041 -4.2055178 -4.1923652 -4.1849723 -4.1937943 -4.2052026 -4.1992755 -4.1783819 -4.1469145 -4.1288991 -4.1346664][-4.177763 -4.1917696 -4.2052851 -4.2201414 -4.2275305 -4.2316456 -4.2314305 -4.2322674 -4.2402859 -4.2449102 -4.2376852 -4.2254672 -4.2050948 -4.18616 -4.1851521][-4.1905389 -4.2121687 -4.2312465 -4.2458758 -4.2514715 -4.2562475 -4.2596049 -4.2632818 -4.2693582 -4.2724905 -4.2706785 -4.26701 -4.2548556 -4.2377 -4.23083][-4.2232294 -4.242692 -4.2567081 -4.2668195 -4.2704396 -4.2713184 -4.2735767 -4.2777624 -4.2834773 -4.2876372 -4.2892237 -4.2903261 -4.2835069 -4.2688928 -4.2603707]]...]
INFO - root - 2017-12-05 19:43:54.074362: step 38910, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 69h:51m:36s remains)
INFO - root - 2017-12-05 19:44:02.502372: step 38920, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 72h:53m:43s remains)
INFO - root - 2017-12-05 19:44:10.969201: step 38930, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 69h:45m:28s remains)
INFO - root - 2017-12-05 19:44:19.339510: step 38940, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 59h:50m:53s remains)
INFO - root - 2017-12-05 19:44:27.640807: step 38950, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.843 sec/batch; 68h:43m:47s remains)
INFO - root - 2017-12-05 19:44:36.089417: step 38960, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 69h:48m:09s remains)
INFO - root - 2017-12-05 19:44:44.672027: step 38970, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 69h:44m:10s remains)
INFO - root - 2017-12-05 19:44:53.250497: step 38980, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 69h:39m:20s remains)
INFO - root - 2017-12-05 19:45:01.884575: step 38990, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 68h:50m:22s remains)
INFO - root - 2017-12-05 19:45:10.365371: step 39000, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 68h:46m:26s remains)
2017-12-05 19:45:11.136379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3248458 -4.3205957 -4.3140416 -4.306375 -4.3003221 -4.2970757 -4.2980747 -4.303288 -4.3114715 -4.3201714 -4.326932 -4.3312058 -4.3344455 -4.336113 -4.3356476][-4.3159151 -4.3143339 -4.3068781 -4.2966828 -4.2875972 -4.2801571 -4.2764182 -4.2790537 -4.28762 -4.2987857 -4.3098249 -4.3181672 -4.3252335 -4.3302069 -4.3324857][-4.3017898 -4.3039603 -4.2953763 -4.281219 -4.2666144 -4.2508917 -4.2377534 -4.236124 -4.2479382 -4.264946 -4.2814646 -4.2952895 -4.308207 -4.3192482 -4.3264885][-4.27506 -4.2820168 -4.2718029 -4.253654 -4.2318048 -4.2049394 -4.1787624 -4.1696138 -4.1837449 -4.2083168 -4.2326169 -4.2549796 -4.2780561 -4.29963 -4.3164821][-4.2306275 -4.2507749 -4.2457633 -4.2264662 -4.1965203 -4.1565228 -4.1146603 -4.08955 -4.0932679 -4.1139703 -4.143446 -4.179338 -4.2225242 -4.2653112 -4.2989058][-4.167078 -4.2083344 -4.222713 -4.2175517 -4.1948442 -4.1520147 -4.0974326 -4.0443945 -4.0067711 -3.9881992 -4.0041056 -4.0591846 -4.1368523 -4.2127142 -4.2702446][-4.0919566 -4.1430531 -4.1808395 -4.2005496 -4.200501 -4.175745 -4.128902 -4.057641 -3.9741883 -3.8971965 -3.8718677 -3.9309332 -4.0382986 -4.1450243 -4.2286162][-4.06856 -4.0951185 -4.1310005 -4.1625667 -4.180625 -4.1743913 -4.1419382 -4.0737481 -3.9841475 -3.8911386 -3.8422933 -3.8845673 -3.9825292 -4.0889187 -4.1836667][-4.1099181 -4.1073337 -4.1205015 -4.1453323 -4.1641941 -4.1598754 -4.1268086 -4.0634265 -3.9962444 -3.9383545 -3.9115732 -3.941658 -4.0067248 -4.0823345 -4.1645451][-4.1626925 -4.1448121 -4.1405296 -4.1517425 -4.1611104 -4.154829 -4.1218524 -4.0673938 -4.026639 -4.0039234 -3.9990764 -4.0244336 -4.0687985 -4.1174126 -4.1745367][-4.1923652 -4.1631165 -4.1448936 -4.1383591 -4.136888 -4.1348772 -4.1217489 -4.0986838 -4.0882282 -4.0860562 -4.0873008 -4.1033549 -4.13171 -4.1606789 -4.1971006][-4.2165108 -4.1811247 -4.1459551 -4.1140661 -4.0948 -4.0989728 -4.1175823 -4.1305776 -4.1453958 -4.1589236 -4.16637 -4.1791668 -4.1963305 -4.2090497 -4.2276893][-4.2426863 -4.2085009 -4.1654997 -4.1157041 -4.0796766 -4.0835309 -4.1187634 -4.150238 -4.1753864 -4.1959248 -4.2099257 -4.2259588 -4.2392683 -4.2436233 -4.2517815][-4.2462749 -4.2243509 -4.1942472 -4.1548033 -4.1190166 -4.1115103 -4.1312366 -4.156702 -4.1807156 -4.1990552 -4.2137427 -4.231914 -4.24912 -4.2562275 -4.2639446][-4.2271819 -4.2227144 -4.2147803 -4.1983147 -4.175703 -4.1562653 -4.1443028 -4.138382 -4.1398649 -4.1518011 -4.1698203 -4.1978569 -4.2274446 -4.2469888 -4.262372]]...]
INFO - root - 2017-12-05 19:45:19.670425: step 39010, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 70h:53m:28s remains)
INFO - root - 2017-12-05 19:45:28.225678: step 39020, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 70h:42m:12s remains)
INFO - root - 2017-12-05 19:45:36.834789: step 39030, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 72h:31m:26s remains)
INFO - root - 2017-12-05 19:45:45.306870: step 39040, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 67h:37m:37s remains)
INFO - root - 2017-12-05 19:45:53.688630: step 39050, loss = 2.03, batch loss = 1.98 (10.2 examples/sec; 0.784 sec/batch; 63h:54m:36s remains)
INFO - root - 2017-12-05 19:46:02.267676: step 39060, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 73h:33m:46s remains)
INFO - root - 2017-12-05 19:46:10.777545: step 39070, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 69h:09m:00s remains)
INFO - root - 2017-12-05 19:46:19.298882: step 39080, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 70h:53m:59s remains)
INFO - root - 2017-12-05 19:46:27.762162: step 39090, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.820 sec/batch; 66h:50m:48s remains)
INFO - root - 2017-12-05 19:46:36.293711: step 39100, loss = 2.10, batch loss = 2.05 (9.6 examples/sec; 0.830 sec/batch; 67h:36m:50s remains)
2017-12-05 19:46:37.109593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2491908 -4.2409258 -4.2344 -4.2253656 -4.2158809 -4.1982923 -4.1634779 -4.1661186 -4.2259746 -4.228497 -4.2155142 -4.2036781 -4.1877589 -4.1958475 -4.2036829][-4.2497959 -4.2358651 -4.2223887 -4.2104616 -4.2010045 -4.1860938 -4.1557345 -4.1590571 -4.2204232 -4.2254438 -4.2158141 -4.2087731 -4.1946883 -4.2011418 -4.211957][-4.2483687 -4.2273946 -4.2080765 -4.196815 -4.1896372 -4.1776319 -4.1491027 -4.1458468 -4.2034554 -4.2163982 -4.2131515 -4.2113175 -4.2008944 -4.2086544 -4.2211804][-4.25625 -4.2374005 -4.2209396 -4.2133484 -4.2057004 -4.1915793 -4.1617608 -4.1509571 -4.2015581 -4.2265773 -4.2351851 -4.2365203 -4.22543 -4.2271051 -4.2328215][-4.267447 -4.2483249 -4.2335753 -4.2263994 -4.2186031 -4.2042766 -4.1728425 -4.154758 -4.1999593 -4.2337909 -4.2502613 -4.2525325 -4.2373862 -4.2302923 -4.2288923][-4.2696438 -4.2460046 -4.2251778 -4.2083955 -4.1903572 -4.1634421 -4.1209817 -4.0970025 -4.1419163 -4.1854496 -4.2093854 -4.2151136 -4.1986146 -4.1882033 -4.1907873][-4.2527637 -4.2259097 -4.1969151 -4.1683817 -4.1373196 -4.0940175 -4.0310173 -3.9906325 -4.0246477 -4.0750589 -4.1170063 -4.1414242 -4.1361895 -4.1349273 -4.1508112][-4.2377973 -4.2163196 -4.1887636 -4.1542559 -4.1106 -4.0496988 -3.9623713 -3.8963606 -3.9025595 -3.9504681 -4.0223265 -4.0797567 -4.0987096 -4.1093197 -4.1317163][-4.252934 -4.2457762 -4.2292876 -4.2007928 -4.1557336 -4.0910454 -4.0051312 -3.9419134 -3.9366932 -3.9673548 -4.0356069 -4.0990982 -4.1287084 -4.1421294 -4.1587291][-4.2780404 -4.2786789 -4.2745156 -4.2588038 -4.2238588 -4.172833 -4.1083722 -4.0720983 -4.0786519 -4.094244 -4.1343589 -4.1787238 -4.2024655 -4.21038 -4.2180891][-4.290761 -4.2903595 -4.2897139 -4.2830443 -4.2601342 -4.2218728 -4.1791229 -4.1655169 -4.1827755 -4.1892948 -4.2115941 -4.2408409 -4.2546277 -4.2551627 -4.25257][-4.2876697 -4.2844267 -4.2830305 -4.2807169 -4.2676091 -4.242424 -4.2155786 -4.2163072 -4.2378645 -4.2388916 -4.2458611 -4.2578034 -4.2612886 -4.2584639 -4.2492566][-4.2816067 -4.2763038 -4.2738762 -4.2736006 -4.267828 -4.2534971 -4.2394071 -4.2455235 -4.2622261 -4.2578936 -4.2542014 -4.2523689 -4.246563 -4.2393055 -4.2295637][-4.2735434 -4.2620544 -4.2546568 -4.25482 -4.2539825 -4.246531 -4.2389436 -4.243793 -4.2543488 -4.2491469 -4.2420692 -4.2346716 -4.2269015 -4.21863 -4.2140603][-4.2673411 -4.2497668 -4.2362127 -4.2342954 -4.2319961 -4.2262287 -4.2179852 -4.217947 -4.225656 -4.22492 -4.2218575 -4.2183604 -4.2147303 -4.2090154 -4.2092948]]...]
INFO - root - 2017-12-05 19:46:45.607294: step 39110, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 70h:06m:17s remains)
INFO - root - 2017-12-05 19:46:54.099738: step 39120, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 69h:19m:26s remains)
INFO - root - 2017-12-05 19:47:02.651542: step 39130, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 69h:28m:26s remains)
INFO - root - 2017-12-05 19:47:11.194236: step 39140, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 69h:11m:47s remains)
INFO - root - 2017-12-05 19:47:19.677488: step 39150, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 68h:05m:43s remains)
INFO - root - 2017-12-05 19:47:28.181421: step 39160, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 0.798 sec/batch; 65h:03m:44s remains)
INFO - root - 2017-12-05 19:47:36.524368: step 39170, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.826 sec/batch; 67h:15m:54s remains)
INFO - root - 2017-12-05 19:47:44.869671: step 39180, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 67h:04m:24s remains)
INFO - root - 2017-12-05 19:47:53.298631: step 39190, loss = 2.11, batch loss = 2.05 (9.5 examples/sec; 0.839 sec/batch; 68h:23m:26s remains)
INFO - root - 2017-12-05 19:48:01.761493: step 39200, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 69h:15m:11s remains)
2017-12-05 19:48:02.522523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3249383 -4.323451 -4.3231788 -4.3232141 -4.3235559 -4.3238268 -4.3242369 -4.3245878 -4.3242922 -4.3232832 -4.3222475 -4.3212018 -4.319922 -4.3186212 -4.3175406][-4.3065238 -4.3058662 -4.3053885 -4.304266 -4.3029804 -4.3020425 -4.3013892 -4.30058 -4.2989883 -4.2970085 -4.2956886 -4.2951612 -4.2950387 -4.2949166 -4.2945433][-4.2984452 -4.3011031 -4.3001056 -4.2953458 -4.2901244 -4.2861958 -4.2826118 -4.2793574 -4.2761116 -4.2731662 -4.2719359 -4.272522 -4.2744823 -4.2764864 -4.277298][-4.2955503 -4.3001633 -4.2981277 -4.291007 -4.2834635 -4.2793813 -4.2760158 -4.2734466 -4.2708549 -4.2674661 -4.2661657 -4.2664728 -4.2685356 -4.2709379 -4.2713013][-4.2889256 -4.2922893 -4.2868 -4.2745976 -4.2639027 -4.2617984 -4.2651129 -4.270751 -4.2739325 -4.2717605 -4.2702312 -4.2693133 -4.2698517 -4.2711477 -4.2704496][-4.2509303 -4.2449679 -4.2303491 -4.2110553 -4.2012148 -4.2086477 -4.2257981 -4.2486339 -4.2656775 -4.2684445 -4.2674313 -4.2628818 -4.2583122 -4.2558117 -4.2533178][-4.1725678 -4.1502776 -4.1221557 -4.0930586 -4.0834284 -4.1045976 -4.1425476 -4.1918521 -4.23091 -4.2433028 -4.2437744 -4.2348657 -4.2237692 -4.2163682 -4.2112775][-4.1088119 -4.074717 -4.0332394 -3.9914474 -3.9750533 -3.9969797 -4.0468183 -4.1194549 -4.1822453 -4.2101326 -4.2191567 -4.2131796 -4.2017064 -4.1927385 -4.186151][-4.1144371 -4.0836086 -4.043427 -4.0022168 -3.9816067 -3.9910917 -4.0285954 -4.098269 -4.166913 -4.204947 -4.2226534 -4.2235131 -4.2169185 -4.2110634 -4.2049661][-4.1754742 -4.1594896 -4.1332097 -4.1050968 -4.08877 -4.0899754 -4.1083112 -4.152626 -4.2018881 -4.2348294 -4.2530007 -4.2568383 -4.2537503 -4.2508888 -4.246572][-4.2453279 -4.2427282 -4.231102 -4.2142086 -4.2005243 -4.1952143 -4.19926 -4.2202077 -4.2469511 -4.2688708 -4.2811184 -4.2838459 -4.28204 -4.2817135 -4.2806997][-4.2828565 -4.2884269 -4.2873979 -4.2785287 -4.2669358 -4.2575397 -4.2537522 -4.260498 -4.2753096 -4.2896972 -4.2976427 -4.2969432 -4.2925448 -4.2898641 -4.28806][-4.2852578 -4.2942171 -4.2963605 -4.289669 -4.2783976 -4.2674 -4.2582207 -4.2568011 -4.2684579 -4.2812953 -4.2847123 -4.2768583 -4.2663136 -4.258636 -4.2526169][-4.2700243 -4.2772903 -4.27581 -4.2643132 -4.2479377 -4.2335129 -4.2203374 -4.2158151 -4.2252483 -4.2348256 -4.2299151 -4.2084608 -4.1852956 -4.1690688 -4.160253][-4.2579713 -4.2610197 -4.2546687 -4.2379708 -4.2150369 -4.1956844 -4.1802721 -4.1739926 -4.1809039 -4.1874313 -4.17542 -4.1431074 -4.1040864 -4.0771623 -4.0640674]]...]
INFO - root - 2017-12-05 19:48:10.955239: step 39210, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 67h:35m:17s remains)
INFO - root - 2017-12-05 19:48:19.442759: step 39220, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 70h:01m:48s remains)
INFO - root - 2017-12-05 19:48:27.876123: step 39230, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 68h:16m:25s remains)
INFO - root - 2017-12-05 19:48:36.188224: step 39240, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.809 sec/batch; 65h:54m:57s remains)
INFO - root - 2017-12-05 19:48:44.653053: step 39250, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 68h:36m:21s remains)
INFO - root - 2017-12-05 19:48:53.066176: step 39260, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 68h:45m:54s remains)
INFO - root - 2017-12-05 19:49:01.566549: step 39270, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 70h:14m:02s remains)
INFO - root - 2017-12-05 19:49:09.995250: step 39280, loss = 2.03, batch loss = 1.98 (9.8 examples/sec; 0.818 sec/batch; 66h:39m:33s remains)
INFO - root - 2017-12-05 19:49:18.494570: step 39290, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 69h:47m:08s remains)
INFO - root - 2017-12-05 19:49:26.915580: step 39300, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 69h:26m:04s remains)
2017-12-05 19:49:27.725657: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.323669 -4.3059354 -4.2721562 -4.2337036 -4.2088814 -4.1944032 -4.1607232 -4.1208215 -4.087832 -4.0742106 -4.1006241 -4.1418128 -4.1802683 -4.2162638 -4.2508693][-4.3235841 -4.3051548 -4.2673383 -4.2197838 -4.1813107 -4.1529746 -4.1159687 -4.0909338 -4.0719252 -4.0591164 -4.0774927 -4.1140656 -4.1599703 -4.20769 -4.2522421][-4.3227029 -4.3006449 -4.2537103 -4.187108 -4.1217937 -4.07556 -4.046988 -4.0531034 -4.0645018 -4.0642538 -4.0775232 -4.1056132 -4.1496248 -4.1969543 -4.2424355][-4.3206873 -4.2945566 -4.2365751 -4.1469779 -4.0541964 -3.9954388 -3.9810472 -4.0184083 -4.0646038 -4.0857034 -4.1055908 -4.12966 -4.1633615 -4.1980815 -4.233407][-4.3177953 -4.2952118 -4.2364383 -4.1397705 -4.0388789 -3.9759927 -3.964653 -4.0132256 -4.0729265 -4.1124668 -4.1462255 -4.1741734 -4.1986213 -4.2177205 -4.2386837][-4.3161564 -4.2999697 -4.2475514 -4.1599298 -4.0724926 -4.0149775 -3.9968443 -4.0373778 -4.0932059 -4.1411 -4.1853042 -4.2158713 -4.2313557 -4.2354689 -4.2434955][-4.3158979 -4.3037009 -4.2592793 -4.1826138 -4.1098762 -4.0587668 -4.0328012 -4.058969 -4.1068983 -4.1584506 -4.2085595 -4.2391138 -4.2498875 -4.2412477 -4.2380176][-4.3165684 -4.3046303 -4.2629118 -4.1905384 -4.1250148 -4.0775847 -4.0504012 -4.0664268 -4.1071315 -4.1631732 -4.2225475 -4.2524924 -4.2561183 -4.2376833 -4.2220378][-4.3158154 -4.3000703 -4.2551389 -4.1852069 -4.1243744 -4.0771084 -4.0515833 -4.0590386 -4.0958877 -4.1570315 -4.2275791 -4.2589784 -4.2576461 -4.2322078 -4.2042923][-4.3142362 -4.29449 -4.2513647 -4.191658 -4.137373 -4.0893912 -4.058424 -4.0596495 -4.0925488 -4.1514707 -4.2256031 -4.262712 -4.2641406 -4.2401252 -4.2054052][-4.3142133 -4.294117 -4.2587008 -4.2129264 -4.1675472 -4.1203356 -4.0833478 -4.0796733 -4.1080279 -4.1557078 -4.2188854 -4.2639937 -4.2769656 -4.2623615 -4.2281842][-4.3207989 -4.3034019 -4.2751746 -4.2384863 -4.2016339 -4.159234 -4.1204963 -4.112165 -4.1388044 -4.1746187 -4.2173247 -4.2601504 -4.2851791 -4.2827044 -4.2585583][-4.3285189 -4.310441 -4.2822361 -4.2482562 -4.217732 -4.1835523 -4.1495347 -4.1381578 -4.1633806 -4.192523 -4.2190351 -4.2524891 -4.2822275 -4.289854 -4.2767315][-4.3354559 -4.3140936 -4.2809157 -4.24159 -4.21075 -4.1842303 -4.1588187 -4.1504655 -4.17095 -4.19423 -4.2114949 -4.2355466 -4.2633824 -4.2764416 -4.2728415][-4.3398728 -4.3160443 -4.2812886 -4.2378321 -4.2015672 -4.1744633 -4.1547866 -4.151175 -4.167037 -4.183712 -4.1980166 -4.2160568 -4.2387242 -4.2531271 -4.2568688]]...]
INFO - root - 2017-12-05 19:49:36.298568: step 39310, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 69h:40m:00s remains)
INFO - root - 2017-12-05 19:49:44.673536: step 39320, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 69h:30m:57s remains)
INFO - root - 2017-12-05 19:49:53.223644: step 39330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 70h:28m:39s remains)
INFO - root - 2017-12-05 19:50:01.741641: step 39340, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 68h:57m:24s remains)
INFO - root - 2017-12-05 19:50:10.200306: step 39350, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 70h:51m:28s remains)
INFO - root - 2017-12-05 19:50:18.669488: step 39360, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 69h:30m:41s remains)
INFO - root - 2017-12-05 19:50:27.180050: step 39370, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 69h:13m:52s remains)
INFO - root - 2017-12-05 19:50:35.544483: step 39380, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 70h:39m:17s remains)
INFO - root - 2017-12-05 19:50:44.139371: step 39390, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 69h:28m:50s remains)
INFO - root - 2017-12-05 19:50:52.681940: step 39400, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 68h:23m:31s remains)
2017-12-05 19:50:53.493576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2368679 -4.2474613 -4.2474003 -4.2398415 -4.2225251 -4.2072349 -4.2002525 -4.2038646 -4.2114744 -4.2131486 -4.2123523 -4.21696 -4.2351117 -4.2665076 -4.2942705][-4.22847 -4.2377048 -4.2382507 -4.2293386 -4.2063189 -4.1852107 -4.1755309 -4.1851673 -4.2001696 -4.2041626 -4.2044597 -4.2071109 -4.2206936 -4.2520113 -4.2830877][-4.2219377 -4.2307096 -4.23318 -4.2253752 -4.1979046 -4.1681337 -4.1476474 -4.1577907 -4.1834645 -4.1937313 -4.1983585 -4.2015462 -4.2093368 -4.2394705 -4.2723951][-4.2153277 -4.2249551 -4.2310004 -4.2250452 -4.1928811 -4.1456475 -4.1085725 -4.1189461 -4.1583986 -4.17715 -4.186326 -4.1919708 -4.1983242 -4.2306867 -4.2665925][-4.2096381 -4.2180109 -4.2235889 -4.2186508 -4.1804314 -4.1096129 -4.0472121 -4.0576005 -4.1160955 -4.1545238 -4.1743579 -4.1880326 -4.1987867 -4.2328997 -4.2690549][-4.2106109 -4.2170258 -4.2200594 -4.2171116 -4.175375 -4.0805197 -3.9783127 -3.9794006 -4.0594678 -4.1247349 -4.1606603 -4.1849027 -4.20651 -4.2414083 -4.2742186][-4.2039251 -4.2078524 -4.2119713 -4.2094755 -4.16856 -4.0563669 -3.9209924 -3.9135005 -4.0141177 -4.1008844 -4.1488504 -4.1825104 -4.2145829 -4.2501802 -4.2782521][-4.1825619 -4.1852813 -4.1924376 -4.1958256 -4.1658678 -4.06 -3.9269259 -3.9167366 -4.0147963 -4.1040115 -4.1536155 -4.1876383 -4.2233567 -4.2579117 -4.2820168][-4.1679997 -4.17182 -4.1817503 -4.1930194 -4.1807251 -4.1068358 -4.0098205 -3.9910216 -4.0567832 -4.1285477 -4.1718855 -4.2003369 -4.2340765 -4.2677531 -4.2883058][-4.1769047 -4.1743808 -4.1818256 -4.1991549 -4.2023382 -4.16376 -4.1086497 -4.0866795 -4.11927 -4.1692638 -4.2017188 -4.2205758 -4.2481594 -4.2773733 -4.2941785][-4.2039757 -4.1957583 -4.1957693 -4.2100344 -4.2202258 -4.2094116 -4.1868806 -4.1685286 -4.1819787 -4.2149239 -4.2369146 -4.2450271 -4.2634282 -4.2847753 -4.29733][-4.230135 -4.2203226 -4.2148876 -4.222528 -4.2338157 -4.2387886 -4.2352304 -4.2191782 -4.220438 -4.2404695 -4.2552328 -4.2576408 -4.2695131 -4.2885571 -4.2995243][-4.2474866 -4.2357407 -4.2274613 -4.2303658 -4.2394509 -4.2505808 -4.2562008 -4.2411475 -4.23388 -4.2485719 -4.2623577 -4.2640944 -4.2744288 -4.2950649 -4.3059421][-4.252389 -4.2410269 -4.2309775 -4.2334828 -4.2395177 -4.251636 -4.25919 -4.2439923 -4.2319164 -4.2451587 -4.2599988 -4.2655678 -4.2792392 -4.3022485 -4.3136954][-4.2498274 -4.2435293 -4.2365551 -4.2394657 -4.2436657 -4.2514725 -4.2561378 -4.2418904 -4.22929 -4.238646 -4.254138 -4.2659082 -4.2841806 -4.30785 -4.3200927]]...]
INFO - root - 2017-12-05 19:51:01.991001: step 39410, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 67h:21m:06s remains)
INFO - root - 2017-12-05 19:51:10.384239: step 39420, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.754 sec/batch; 61h:21m:32s remains)
INFO - root - 2017-12-05 19:51:19.022540: step 39430, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 68h:19m:40s remains)
INFO - root - 2017-12-05 19:51:27.684709: step 39440, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 69h:00m:09s remains)
INFO - root - 2017-12-05 19:51:36.081257: step 39450, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 68h:22m:10s remains)
INFO - root - 2017-12-05 19:51:44.653958: step 39460, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 68h:37m:36s remains)
INFO - root - 2017-12-05 19:51:53.272795: step 39470, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 71h:35m:31s remains)
INFO - root - 2017-12-05 19:52:01.783726: step 39480, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 68h:40m:27s remains)
INFO - root - 2017-12-05 19:52:10.324923: step 39490, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 73h:04m:35s remains)
INFO - root - 2017-12-05 19:52:18.830754: step 39500, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 70h:14m:09s remains)
2017-12-05 19:52:19.584330: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1816235 -4.1841412 -4.1861591 -4.1845126 -4.1860056 -4.1921439 -4.2009525 -4.2026114 -4.2046852 -4.2129879 -4.2121158 -4.2004771 -4.1880155 -4.1712379 -4.1605749][-4.1422849 -4.149929 -4.1690159 -4.1798277 -4.190217 -4.207222 -4.2172813 -4.2166357 -4.2185307 -4.2251272 -4.2216735 -4.2035232 -4.1750822 -4.1442094 -4.1225939][-4.0732193 -4.092608 -4.1425557 -4.1813498 -4.2034712 -4.2218451 -4.2249546 -4.2149515 -4.2120256 -4.2156324 -4.2088633 -4.188386 -4.1517625 -4.1142673 -4.0907278][-4.014482 -4.0471563 -4.1241984 -4.1838365 -4.210237 -4.2178993 -4.2024684 -4.1721973 -4.1626258 -4.1689725 -4.166769 -4.1572609 -4.133007 -4.1078806 -4.0944443][-4.0461822 -4.0842066 -4.1589417 -4.2084508 -4.2158904 -4.1980953 -4.1592517 -4.1091576 -4.0946465 -4.1101055 -4.1227775 -4.1330009 -4.1358895 -4.1348333 -4.1362476][-4.1222939 -4.1562333 -4.2044225 -4.2228723 -4.1999497 -4.1505685 -4.0857668 -4.0184407 -4.0133066 -4.0554605 -4.0955591 -4.1286249 -4.157197 -4.169991 -4.1775208][-4.1570821 -4.1784048 -4.1962509 -4.1830454 -4.1250644 -4.0451207 -3.9598203 -3.8812695 -3.9034688 -3.9917309 -4.0677748 -4.12478 -4.1663208 -4.1788249 -4.1802645][-4.1778784 -4.1825466 -4.1683655 -4.1252651 -4.0397682 -3.9454148 -3.8590326 -3.7837617 -3.8358591 -3.9597106 -4.0534973 -4.1170096 -4.1613235 -4.1697474 -4.1621652][-4.19507 -4.1872106 -4.1502442 -4.0956626 -4.0162153 -3.9401412 -3.8868108 -3.8489971 -3.8990297 -4.0087156 -4.0882912 -4.1377535 -4.174706 -4.1822438 -4.1731095][-4.1925788 -4.1782403 -4.134696 -4.0817103 -4.0229139 -3.9745245 -3.9547691 -3.9522743 -3.9924884 -4.0723553 -4.1311817 -4.1650953 -4.1913624 -4.1992078 -4.1924148][-4.1854796 -4.1669569 -4.1262312 -4.0855217 -4.0492959 -4.0228353 -4.0238214 -4.0439425 -4.0778227 -4.1287637 -4.1689081 -4.18833 -4.20077 -4.205451 -4.1988182][-4.2101622 -4.1908016 -4.1602836 -4.1386743 -4.1240163 -4.1147566 -4.1246252 -4.1478815 -4.1716394 -4.1971178 -4.2189226 -4.2263794 -4.2274232 -4.229434 -4.2237716][-4.2594671 -4.2422705 -4.2239313 -4.2173052 -4.2163687 -4.2172737 -4.2282643 -4.244854 -4.257977 -4.267139 -4.277863 -4.2816849 -4.27954 -4.2787247 -4.2730207][-4.3071594 -4.2932787 -4.2845111 -4.2853312 -4.2898226 -4.293396 -4.3023286 -4.3124933 -4.31883 -4.3217649 -4.3256807 -4.3269315 -4.3246078 -4.3232594 -4.3181782][-4.3323197 -4.3218279 -4.3179784 -4.3201785 -4.3245134 -4.3272724 -4.3317451 -4.3369823 -4.3404684 -4.3423724 -4.3441334 -4.343945 -4.3416748 -4.3409009 -4.3383451]]...]
INFO - root - 2017-12-05 19:52:28.078834: step 39510, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 68h:51m:50s remains)
INFO - root - 2017-12-05 19:52:36.633823: step 39520, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 70h:52m:08s remains)
INFO - root - 2017-12-05 19:52:45.028658: step 39530, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 71h:28m:02s remains)
INFO - root - 2017-12-05 19:52:53.474164: step 39540, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 69h:07m:09s remains)
INFO - root - 2017-12-05 19:53:01.977353: step 39550, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 67h:20m:37s remains)
INFO - root - 2017-12-05 19:53:10.582274: step 39560, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 70h:21m:41s remains)
INFO - root - 2017-12-05 19:53:19.161709: step 39570, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 70h:35m:01s remains)
INFO - root - 2017-12-05 19:53:27.701315: step 39580, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 66h:47m:24s remains)
INFO - root - 2017-12-05 19:53:36.114785: step 39590, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 67h:43m:03s remains)
INFO - root - 2017-12-05 19:53:44.717048: step 39600, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 69h:52m:56s remains)
2017-12-05 19:53:45.540192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.171504 -4.1968884 -4.2314734 -4.246551 -4.2333517 -4.2139163 -4.2076349 -4.2226214 -4.2531223 -4.2747021 -4.2801328 -4.2712765 -4.2622142 -4.2595296 -4.258677][-4.1507759 -4.1805596 -4.2150817 -4.2269115 -4.21165 -4.1897831 -4.1794829 -4.1930032 -4.2238379 -4.2467279 -4.250586 -4.241796 -4.2351022 -4.2340097 -4.2330022][-4.1384449 -4.1696811 -4.2021036 -4.2096233 -4.193543 -4.1714373 -4.1599092 -4.1689658 -4.1969123 -4.2195272 -4.2251878 -4.2196121 -4.21512 -4.214056 -4.2101474][-4.1316938 -4.1592374 -4.1871939 -4.1907196 -4.1705966 -4.1427817 -4.1287289 -4.1377168 -4.1680274 -4.1912594 -4.1962996 -4.1909976 -4.18735 -4.1832962 -4.1752596][-4.1419716 -4.1652341 -4.1889577 -4.1879025 -4.1552019 -4.1087718 -4.0814037 -4.0921788 -4.1320171 -4.1624374 -4.1690645 -4.1612229 -4.1533079 -4.1427984 -4.1297741][-4.1576457 -4.1774907 -4.1992221 -4.1950965 -4.1494431 -4.0758924 -4.0215707 -4.0335803 -4.0931029 -4.1434026 -4.1606603 -4.1536264 -4.1410341 -4.126205 -4.1102104][-4.1623721 -4.1803541 -4.2013922 -4.1960516 -4.1405683 -4.0389414 -3.9486427 -3.9574695 -4.0438948 -4.1229138 -4.159946 -4.1637244 -4.15373 -4.1377439 -4.1198726][-4.1534014 -4.1693935 -4.19377 -4.1935592 -4.1371441 -4.0190272 -3.9010305 -3.9049335 -4.0112143 -4.1088576 -4.1598916 -4.1749029 -4.1722994 -4.161685 -4.1488924][-4.1375213 -4.1540575 -4.183115 -4.191361 -4.1465025 -4.0429878 -3.9356587 -3.9349618 -4.0253062 -4.1093459 -4.1576376 -4.1787992 -4.184783 -4.1827068 -4.1774988][-4.1164875 -4.1332703 -4.1671333 -4.1855211 -4.1606121 -4.090528 -4.0155983 -4.0081725 -4.0633874 -4.1185341 -4.15506 -4.1755219 -4.1822925 -4.1875348 -4.1892443][-4.0953293 -4.1124411 -4.1525888 -4.18415 -4.1780424 -4.1342783 -4.0800462 -4.0657454 -4.0960374 -4.131772 -4.1567369 -4.1693335 -4.1710558 -4.1789904 -4.1887174][-4.0840254 -4.0963964 -4.1382809 -4.1788373 -4.1862626 -4.1588626 -4.118154 -4.1038675 -4.1253853 -4.15335 -4.1693368 -4.1737432 -4.1694393 -4.1749005 -4.1877694][-4.0996237 -4.1076303 -4.1423512 -4.1800241 -4.191113 -4.1716962 -4.1418061 -4.1306934 -4.1472497 -4.1678085 -4.1779838 -4.1777458 -4.1723046 -4.1750321 -4.1854081][-4.1354513 -4.1454406 -4.1714392 -4.1992688 -4.2060881 -4.1907263 -4.1690445 -4.1591272 -4.1693206 -4.1830893 -4.189126 -4.1877451 -4.1860161 -4.1906939 -4.1989169][-4.172668 -4.1845546 -4.2035956 -4.22351 -4.2256188 -4.2126122 -4.1965361 -4.1846051 -4.1885128 -4.1982412 -4.2006016 -4.2015471 -4.2073913 -4.2161169 -4.2242956]]...]
INFO - root - 2017-12-05 19:53:54.007032: step 39610, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 69h:30m:44s remains)
INFO - root - 2017-12-05 19:54:02.459131: step 39620, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 68h:39m:35s remains)
INFO - root - 2017-12-05 19:54:11.013754: step 39630, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 70h:48m:40s remains)
INFO - root - 2017-12-05 19:54:19.520934: step 39640, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 69h:07m:59s remains)
INFO - root - 2017-12-05 19:54:27.902348: step 39650, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.837 sec/batch; 68h:07m:34s remains)
INFO - root - 2017-12-05 19:54:36.494594: step 39660, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 68h:45m:15s remains)
INFO - root - 2017-12-05 19:54:45.073474: step 39670, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.821 sec/batch; 66h:49m:01s remains)
INFO - root - 2017-12-05 19:54:53.686027: step 39680, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 70h:54m:08s remains)
INFO - root - 2017-12-05 19:55:02.297145: step 39690, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 68h:14m:24s remains)
INFO - root - 2017-12-05 19:55:10.826759: step 39700, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 70h:06m:50s remains)
2017-12-05 19:55:11.528767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1458569 -4.2027545 -4.2516165 -4.2829514 -4.29619 -4.2941337 -4.27166 -4.2263122 -4.1710005 -4.1169796 -4.0715995 -4.0455246 -4.0390086 -4.0461249 -4.0686145][-4.1524987 -4.2082267 -4.2502103 -4.2801533 -4.2945023 -4.2939086 -4.2712765 -4.2294126 -4.1866913 -4.1514778 -4.1252542 -4.1134796 -4.1152554 -4.1280828 -4.1425772][-4.2039251 -4.2431164 -4.2644515 -4.2823839 -4.2890496 -4.2801771 -4.2531486 -4.2154136 -4.1871691 -4.1692004 -4.1599503 -4.1649904 -4.1794066 -4.19341 -4.1966295][-4.2608352 -4.2794828 -4.2833953 -4.2838039 -4.2742577 -4.2474422 -4.2071261 -4.1649017 -4.1447735 -4.1432891 -4.1483383 -4.1689329 -4.200356 -4.2217689 -4.2271829][-4.2941017 -4.3015208 -4.2924075 -4.2716675 -4.2371726 -4.1831188 -4.1167741 -4.0595493 -4.04529 -4.0651131 -4.0889258 -4.1271749 -4.177691 -4.2161608 -4.2385454][-4.2916336 -4.2950597 -4.2734432 -4.229815 -4.1651058 -4.0768547 -3.9730058 -3.8972521 -3.9076684 -3.9657378 -4.0184 -4.0775814 -4.1477809 -4.2087107 -4.2506814][-4.2678666 -4.2646322 -4.2291579 -4.165288 -4.0786142 -3.9641082 -3.8290915 -3.7499106 -3.8159258 -3.9252563 -4.0077696 -4.0787539 -4.1516662 -4.2149644 -4.2604918][-4.2375298 -4.2309504 -4.1939321 -4.1277413 -4.0431714 -3.9407148 -3.8340323 -3.8002152 -3.8932726 -4.0090184 -4.0932169 -4.1549115 -4.2036333 -4.24243 -4.2716775][-4.2267861 -4.2227612 -4.1976085 -4.1495371 -4.0859723 -4.0185781 -3.9683571 -3.9744093 -4.0454516 -4.1258988 -4.1905413 -4.2364264 -4.2647181 -4.2804022 -4.2872906][-4.2350197 -4.235322 -4.2215953 -4.192081 -4.1510253 -4.1164184 -4.1054454 -4.1263013 -4.1707745 -4.2170343 -4.2588391 -4.2905908 -4.3095465 -4.3128328 -4.3035588][-4.2398911 -4.2433925 -4.2394824 -4.2265444 -4.2069116 -4.2011862 -4.2137437 -4.2355814 -4.2593207 -4.2816539 -4.3010478 -4.3155622 -4.3220878 -4.3161206 -4.301445][-4.2221293 -4.2309642 -4.24016 -4.2449441 -4.2443166 -4.2579641 -4.2814064 -4.3004651 -4.3120217 -4.3164377 -4.3160434 -4.31367 -4.3077474 -4.2931852 -4.2766628][-4.1902928 -4.1993976 -4.217309 -4.229722 -4.2386122 -4.2589812 -4.281816 -4.2978916 -4.30316 -4.2983532 -4.2868977 -4.2741184 -4.2608395 -4.2433381 -4.2312012][-4.1545167 -4.1537485 -4.1727037 -4.189755 -4.2022696 -4.2175121 -4.2308903 -4.2405996 -4.2398453 -4.2322855 -4.2210073 -4.2089157 -4.1992879 -4.1913218 -4.1941752][-4.1397285 -4.12789 -4.1425352 -4.1645021 -4.1769028 -4.1821265 -4.1844821 -4.1893206 -4.1833344 -4.176044 -4.1738782 -4.1713881 -4.1714683 -4.1766067 -4.1961622]]...]
INFO - root - 2017-12-05 19:55:20.100564: step 39710, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 70h:28m:20s remains)
INFO - root - 2017-12-05 19:55:28.558808: step 39720, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 69h:38m:08s remains)
INFO - root - 2017-12-05 19:55:37.089134: step 39730, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 67h:45m:07s remains)
INFO - root - 2017-12-05 19:55:45.522022: step 39740, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 0.762 sec/batch; 61h:57m:11s remains)
INFO - root - 2017-12-05 19:55:53.976938: step 39750, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 68h:25m:29s remains)
INFO - root - 2017-12-05 19:56:02.466338: step 39760, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 66h:16m:37s remains)
INFO - root - 2017-12-05 19:56:11.130839: step 39770, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.866 sec/batch; 70h:24m:56s remains)
INFO - root - 2017-12-05 19:56:19.698785: step 39780, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.816 sec/batch; 66h:22m:53s remains)
INFO - root - 2017-12-05 19:56:28.307943: step 39790, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 71h:42m:51s remains)
INFO - root - 2017-12-05 19:56:36.846128: step 39800, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 68h:18m:51s remains)
2017-12-05 19:56:37.622356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2561307 -4.2469249 -4.247148 -4.2603836 -4.2765455 -4.2849722 -4.2764168 -4.2530928 -4.2157965 -4.190002 -4.1800785 -4.1815786 -4.2012148 -4.2260995 -4.2438574][-4.263413 -4.2584729 -4.2604036 -4.2724829 -4.2862577 -4.2897477 -4.2724471 -4.2363958 -4.1901813 -4.1681809 -4.1659307 -4.1763034 -4.2014303 -4.2287035 -4.250741][-4.2810383 -4.2813263 -4.2846184 -4.2906609 -4.2945676 -4.2853818 -4.2536011 -4.2055359 -4.1560879 -4.146544 -4.1603265 -4.1813412 -4.209528 -4.2371006 -4.2600431][-4.3011069 -4.3032641 -4.3032184 -4.3000455 -4.2904081 -4.2646179 -4.213953 -4.150043 -4.1001649 -4.1110435 -4.1483068 -4.1848707 -4.2198534 -4.2474294 -4.2666736][-4.3171358 -4.3193512 -4.3131146 -4.2998009 -4.2772422 -4.2349043 -4.1629553 -4.0734186 -4.0166903 -4.0583334 -4.1334438 -4.1970677 -4.2415714 -4.2647042 -4.2761388][-4.3172317 -4.3164473 -4.3034849 -4.2813215 -4.250483 -4.1956387 -4.0998778 -3.9802287 -3.9160559 -3.9975283 -4.117682 -4.2078972 -4.2594128 -4.2781243 -4.2836089][-4.299871 -4.2932119 -4.2745323 -4.2490559 -4.2162318 -4.1570072 -4.0425048 -3.8967364 -3.8408065 -3.9637632 -4.1171889 -4.2212148 -4.27312 -4.288332 -4.2893257][-4.2793679 -4.2711658 -4.2530465 -4.2308736 -4.2004576 -4.1379871 -4.0134296 -3.8617234 -3.8372703 -3.9905107 -4.1485434 -4.2452455 -4.2861714 -4.2949247 -4.2907906][-4.2648344 -4.2584057 -4.2436695 -4.2282891 -4.2014656 -4.1392164 -4.02544 -3.9022522 -3.9177155 -4.067256 -4.196876 -4.2709455 -4.29897 -4.3018923 -4.2930889][-4.2522206 -4.2475867 -4.2338057 -4.2223287 -4.201911 -4.1492333 -4.0654807 -3.992481 -4.0348225 -4.1545229 -4.2425094 -4.2900639 -4.3066187 -4.30485 -4.2940321][-4.2356148 -4.2331738 -4.2199407 -4.2127 -4.2024956 -4.1679373 -4.1164055 -4.0792832 -4.1192713 -4.2025661 -4.2614069 -4.2921286 -4.2983513 -4.2912178 -4.2809262][-4.2211137 -4.2212353 -4.2116756 -4.2071333 -4.2084465 -4.1972594 -4.170156 -4.1489806 -4.1715665 -4.2213259 -4.2606368 -4.2822185 -4.2825155 -4.2720089 -4.2621779][-4.2180486 -4.2171979 -4.2063527 -4.2007971 -4.2117271 -4.2212954 -4.2150345 -4.2053566 -4.2172666 -4.2455716 -4.2690635 -4.2848477 -4.2847128 -4.2735782 -4.2613206][-4.2388391 -4.2314744 -4.2114983 -4.199307 -4.2148008 -4.2391281 -4.2513242 -4.2533946 -4.2649884 -4.2832232 -4.2958145 -4.3062344 -4.3068309 -4.2965155 -4.2816229][-4.2594862 -4.244947 -4.2148557 -4.1956797 -4.2136383 -4.2480679 -4.2726359 -4.283473 -4.298048 -4.3109236 -4.317121 -4.3248 -4.3256907 -4.3190355 -4.3062453]]...]
INFO - root - 2017-12-05 19:56:46.093187: step 39810, loss = 2.02, batch loss = 1.96 (9.5 examples/sec; 0.841 sec/batch; 68h:21m:34s remains)
INFO - root - 2017-12-05 19:56:54.666868: step 39820, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 70h:27m:43s remains)
INFO - root - 2017-12-05 19:57:03.109521: step 39830, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.847 sec/batch; 68h:49m:12s remains)
INFO - root - 2017-12-05 19:57:11.729615: step 39840, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 69h:58m:08s remains)
INFO - root - 2017-12-05 19:57:20.130889: step 39850, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 67h:32m:34s remains)
INFO - root - 2017-12-05 19:57:28.685842: step 39860, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 67h:47m:22s remains)
INFO - root - 2017-12-05 19:57:37.189463: step 39870, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.838 sec/batch; 68h:05m:21s remains)
INFO - root - 2017-12-05 19:57:45.800503: step 39880, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 70h:21m:03s remains)
INFO - root - 2017-12-05 19:57:54.283435: step 39890, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 69h:49m:14s remains)
INFO - root - 2017-12-05 19:58:02.855783: step 39900, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 69h:48m:35s remains)
2017-12-05 19:58:03.662334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1117296 -4.0756583 -4.0519524 -4.0607595 -4.0977964 -4.13915 -4.167151 -4.1716695 -4.1557636 -4.1319532 -4.1101952 -4.0889139 -4.0723672 -4.0708508 -4.0920095][-4.1058259 -4.0695934 -4.0474305 -4.0574641 -4.0923405 -4.133997 -4.166379 -4.1785192 -4.1707344 -4.14983 -4.1240129 -4.0981555 -4.07867 -4.0791516 -4.1062574][-4.0972276 -4.0612788 -4.0352211 -4.0398316 -4.0703073 -4.1133823 -4.1538234 -4.1774988 -4.1782827 -4.1598291 -4.1344786 -4.1130805 -4.0968471 -4.1063185 -4.1453366][-4.1040773 -4.0697703 -4.0385079 -4.0315151 -4.0542717 -4.0970664 -4.1443391 -4.1720057 -4.1758647 -4.1589832 -4.1349535 -4.1206193 -4.1121745 -4.1347003 -4.1780171][-4.1213503 -4.0995212 -4.0691657 -4.04741 -4.0574441 -4.09788 -4.1479225 -4.1721864 -4.1724882 -4.1523166 -4.1294761 -4.1231384 -4.1248493 -4.1562934 -4.1923122][-4.1383548 -4.1265941 -4.0933018 -4.0557404 -4.0558596 -4.1017208 -4.1565304 -4.176302 -4.1710286 -4.1443138 -4.1219988 -4.1221042 -4.1328688 -4.1628389 -4.1875334][-4.1514688 -4.13527 -4.0956025 -4.0559473 -4.0626216 -4.116281 -4.168406 -4.1787391 -4.1691432 -4.1363888 -4.113584 -4.115078 -4.12693 -4.1493835 -4.1602459][-4.1382842 -4.1134396 -4.0700507 -4.0414758 -4.0604677 -4.1170607 -4.1603274 -4.1625772 -4.1480703 -4.1109881 -4.0930533 -4.1013017 -4.1156926 -4.1285977 -4.1280107][-4.1017714 -4.0700855 -4.0307894 -4.0154538 -4.0408592 -4.0965939 -4.1359744 -4.1328888 -4.1088548 -4.0664434 -4.0632925 -4.0925136 -4.1171889 -4.1244726 -4.1161141][-4.1014533 -4.0688691 -4.0318866 -4.017983 -4.0353861 -4.07643 -4.1084437 -4.105154 -4.0686622 -4.0237904 -4.03709 -4.0868564 -4.1224008 -4.1272058 -4.11442][-4.1254435 -4.102109 -4.0745573 -4.0592341 -4.06135 -4.0796537 -4.0971732 -4.0931044 -4.0534849 -4.0168619 -4.0420785 -4.094605 -4.1296773 -4.1321111 -4.1213255][-4.1584558 -4.1480436 -4.1315613 -4.1153026 -4.1047964 -4.1068459 -4.1130457 -4.1071196 -4.0770826 -4.0583448 -4.0854425 -4.1252389 -4.1502948 -4.1532331 -4.1494284][-4.1896658 -4.19112 -4.1829238 -4.1698489 -4.157824 -4.1552749 -4.158011 -4.153677 -4.13518 -4.1286521 -4.1478214 -4.1715789 -4.1862803 -4.1903739 -4.1872678][-4.2178087 -4.2262621 -4.2238016 -4.2168784 -4.2100739 -4.207983 -4.2116122 -4.2104578 -4.2016091 -4.2009659 -4.2126188 -4.225492 -4.2319021 -4.2289667 -4.2194614][-4.2526889 -4.2580471 -4.2536254 -4.2501163 -4.2472582 -4.2471018 -4.2508731 -4.2544093 -4.2540026 -4.2564335 -4.2629476 -4.269629 -4.2711015 -4.2633414 -4.2487569]]...]
INFO - root - 2017-12-05 19:58:12.165338: step 39910, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 69h:52m:37s remains)
INFO - root - 2017-12-05 19:58:20.685125: step 39920, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 66h:36m:36s remains)
INFO - root - 2017-12-05 19:58:29.222168: step 39930, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.875 sec/batch; 71h:09m:02s remains)
INFO - root - 2017-12-05 19:58:37.886807: step 39940, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 71h:30m:21s remains)
INFO - root - 2017-12-05 19:58:46.403793: step 39950, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 69h:06m:59s remains)
INFO - root - 2017-12-05 19:58:54.763251: step 39960, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 69h:57m:01s remains)
INFO - root - 2017-12-05 19:59:03.239535: step 39970, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 68h:41m:19s remains)
INFO - root - 2017-12-05 19:59:11.831584: step 39980, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 68h:46m:18s remains)
INFO - root - 2017-12-05 19:59:20.309696: step 39990, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 69h:48m:15s remains)
INFO - root - 2017-12-05 19:59:28.812593: step 40000, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 68h:48m:01s remains)
2017-12-05 19:59:29.561395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2584925 -4.2486424 -4.218256 -4.1888556 -4.1946945 -4.2126145 -4.2361021 -4.260047 -4.2678671 -4.2718477 -4.2736773 -4.2714868 -4.2649765 -4.2577958 -4.2565379][-4.257267 -4.2517042 -4.2278476 -4.2076211 -4.2158566 -4.2288628 -4.2485318 -4.272357 -4.2858658 -4.2917023 -4.2933407 -4.2876558 -4.2791748 -4.2721491 -4.2707019][-4.2517819 -4.2406487 -4.219902 -4.2071357 -4.2161455 -4.2209597 -4.2292957 -4.246429 -4.2629967 -4.2729864 -4.2781296 -4.2769 -4.269053 -4.2616782 -4.2631474][-4.252018 -4.2312794 -4.2086196 -4.1933284 -4.1951871 -4.1935573 -4.1874042 -4.1920328 -4.213532 -4.2329478 -4.2490759 -4.2566137 -4.24948 -4.2402282 -4.2442036][-4.2529125 -4.2256145 -4.1990161 -4.1734662 -4.163497 -4.1475134 -4.1169195 -4.105701 -4.1400595 -4.1788526 -4.2118392 -4.2286706 -4.221199 -4.2073436 -4.2101636][-4.2521238 -4.222137 -4.1926003 -4.1591835 -4.1321869 -4.0911951 -4.0169454 -3.9796634 -4.0400586 -4.1151853 -4.16883 -4.1894631 -4.173562 -4.1476073 -4.1477942][-4.2498245 -4.2189 -4.1873856 -4.1519938 -4.111886 -4.0371661 -3.9018073 -3.8245769 -3.9311242 -4.059453 -4.132359 -4.1514916 -4.121563 -4.0799561 -4.0812531][-4.2414412 -4.2069159 -4.1770296 -4.1490827 -4.1093383 -4.0164633 -3.8510318 -3.7582355 -3.9009938 -4.0541492 -4.1298862 -4.1422606 -4.0977297 -4.0420585 -4.0491796][-4.2406082 -4.2057505 -4.1808734 -4.1658907 -4.1450205 -4.0709271 -3.9394555 -3.8765972 -3.9886198 -4.1052094 -4.1590266 -4.1574268 -4.1045775 -4.0490241 -4.0652308][-4.2419662 -4.2061753 -4.1856332 -4.1838064 -4.1863809 -4.1516995 -4.0756764 -4.0356746 -4.0978308 -4.1648703 -4.1931839 -4.1878109 -4.1472034 -4.1053958 -4.1217546][-4.2458792 -4.2082939 -4.189146 -4.1955643 -4.2137294 -4.2111297 -4.179132 -4.1537476 -4.1782203 -4.20767 -4.2221541 -4.2250996 -4.204886 -4.1820679 -4.1917467][-4.2624931 -4.2268443 -4.2090797 -4.21421 -4.236414 -4.2513261 -4.2442708 -4.2289219 -4.2318845 -4.2394323 -4.2494965 -4.2578363 -4.2514815 -4.2423353 -4.2476277][-4.2741313 -4.2415037 -4.2222066 -4.222949 -4.2422314 -4.263329 -4.2710185 -4.2626081 -4.2564025 -4.2570825 -4.2648921 -4.2742577 -4.2742767 -4.2736006 -4.2787981][-4.2845116 -4.2569904 -4.2388072 -4.2351913 -4.2522392 -4.2748618 -4.2895765 -4.2892213 -4.2814507 -4.2782035 -4.2807441 -4.2846718 -4.2870879 -4.2905536 -4.2956219][-4.2894521 -4.2702746 -4.2566614 -4.2470627 -4.258708 -4.2779965 -4.2946868 -4.3017206 -4.2975655 -4.2951403 -4.2948813 -4.2931318 -4.2929587 -4.2982969 -4.3038211]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 19:59:38.601058: step 40010, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 67h:49m:50s remains)
INFO - root - 2017-12-05 19:59:47.188570: step 40020, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 69h:39m:49s remains)
INFO - root - 2017-12-05 19:59:55.677537: step 40030, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.848 sec/batch; 68h:54m:24s remains)
INFO - root - 2017-12-05 20:00:04.119924: step 40040, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 0.793 sec/batch; 64h:26m:49s remains)
INFO - root - 2017-12-05 20:00:12.603250: step 40050, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 70h:11m:07s remains)
INFO - root - 2017-12-05 20:00:21.040438: step 40060, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 69h:39m:15s remains)
INFO - root - 2017-12-05 20:00:29.566733: step 40070, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 69h:36m:12s remains)
INFO - root - 2017-12-05 20:00:38.029011: step 40080, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 68h:11m:49s remains)
INFO - root - 2017-12-05 20:00:46.530264: step 40090, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 68h:22m:18s remains)
INFO - root - 2017-12-05 20:00:55.030157: step 40100, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 69h:09m:02s remains)
2017-12-05 20:00:55.768256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9183748 -3.9694309 -4.0246005 -4.0761905 -4.0779252 -4.0429163 -4.0233707 -4.0480485 -4.0882654 -4.1133084 -4.1273608 -4.138001 -4.1505389 -4.1529961 -4.1320944][-3.8862889 -3.9426451 -4.0016093 -4.0577083 -4.0733171 -4.0500441 -4.0290771 -4.0463948 -4.0761008 -4.0934892 -4.1056638 -4.1165695 -4.1315536 -4.1391964 -4.1181211][-3.9188759 -3.9686146 -4.0230131 -4.0746117 -4.0925188 -4.0794196 -4.0614929 -4.0723858 -4.0934248 -4.102747 -4.1084385 -4.1130795 -4.1275973 -4.1329188 -4.1093802][-4.0008531 -4.0305371 -4.0656319 -4.0933 -4.0974417 -4.082283 -4.0673237 -4.0764189 -4.0955672 -4.0988526 -4.1006384 -4.1048779 -4.1262493 -4.1293235 -4.0989065][-4.085793 -4.0990057 -4.1121731 -4.1147261 -4.09512 -4.0618954 -4.02799 -4.024384 -4.0512319 -4.0656562 -4.0789156 -4.099905 -4.1314793 -4.1343808 -4.1015129][-4.1266017 -4.1362638 -4.138133 -4.1172462 -4.0759764 -4.0149956 -3.9442053 -3.9101763 -3.9538603 -4.0062122 -4.0493088 -4.0889997 -4.1271596 -4.1362128 -4.1075153][-4.1310697 -4.1387477 -4.1310611 -4.0950136 -4.0425935 -3.9540992 -3.8284163 -3.737381 -3.8059032 -3.9174211 -4.0054169 -4.068697 -4.1151867 -4.1333361 -4.110919][-4.1255417 -4.1291451 -4.1202483 -4.0885124 -4.0413623 -3.943671 -3.7662549 -3.5954089 -3.6703289 -3.8334184 -3.9598553 -4.0410223 -4.0913072 -4.1099186 -4.0907278][-4.1222086 -4.1290135 -4.1294079 -4.1126246 -4.0808554 -3.9996982 -3.8377278 -3.6629097 -3.6967654 -3.8365362 -3.9612577 -4.0417771 -4.0847969 -4.094017 -4.0782785][-4.129612 -4.1370077 -4.140461 -4.1335268 -4.1168008 -4.0679593 -3.9619384 -3.849972 -3.8568919 -3.9371712 -4.0243812 -4.0818267 -4.1013546 -4.0954118 -4.0823951][-4.1393285 -4.1421175 -4.142149 -4.1359406 -4.1282883 -4.0977311 -4.0334468 -3.9722197 -3.9802852 -4.029274 -4.0876722 -4.1231132 -4.123055 -4.1009073 -4.0811205][-4.1483054 -4.1487579 -4.1405125 -4.1318545 -4.1247883 -4.0984974 -4.0505061 -4.014699 -4.0342937 -4.0788283 -4.1224165 -4.1402063 -4.1246729 -4.0882764 -4.0614753][-4.1743832 -4.172195 -4.1563735 -4.1395307 -4.1294613 -4.1032543 -4.0569606 -4.0304775 -4.0609083 -4.1097884 -4.1509752 -4.1593332 -4.1308317 -4.0846834 -4.0523939][-4.1963444 -4.1926231 -4.1765871 -4.1622524 -4.1565728 -4.13232 -4.0877209 -4.0642262 -4.0957069 -4.1428719 -4.1795082 -4.1875238 -4.155591 -4.1016784 -4.0665627][-4.2034092 -4.1991014 -4.1905217 -4.1868415 -4.1885209 -4.1762309 -4.1397276 -4.1148262 -4.1337776 -4.1684861 -4.1995111 -4.2110887 -4.1852784 -4.1310892 -4.0953493]]...]
INFO - root - 2017-12-05 20:01:04.236271: step 40110, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 69h:52m:44s remains)
INFO - root - 2017-12-05 20:01:12.761557: step 40120, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 69h:16m:56s remains)
INFO - root - 2017-12-05 20:01:21.239657: step 40130, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 70h:04m:39s remains)
INFO - root - 2017-12-05 20:01:29.927489: step 40140, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 71h:26m:20s remains)
INFO - root - 2017-12-05 20:01:38.338025: step 40150, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 69h:09m:27s remains)
INFO - root - 2017-12-05 20:01:46.813918: step 40160, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 67h:44m:24s remains)
INFO - root - 2017-12-05 20:01:55.181017: step 40170, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 68h:00m:38s remains)
INFO - root - 2017-12-05 20:02:03.686194: step 40180, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.854 sec/batch; 69h:18m:48s remains)
INFO - root - 2017-12-05 20:02:12.248582: step 40190, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 68h:03m:55s remains)
INFO - root - 2017-12-05 20:02:20.821596: step 40200, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 70h:12m:04s remains)
2017-12-05 20:02:21.555907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2999358 -4.3017817 -4.3074379 -4.3154211 -4.3229923 -4.3265543 -4.3261504 -4.3241329 -4.3200145 -4.3151846 -4.3155475 -4.3194447 -4.3264732 -4.3358645 -4.3443861][-4.2682395 -4.2702885 -4.2790227 -4.2919245 -4.302855 -4.3078737 -4.3060741 -4.2976208 -4.2860603 -4.2761612 -4.2769561 -4.2856956 -4.3004074 -4.3181553 -4.334816][-4.2319756 -4.2323713 -4.2441363 -4.2622461 -4.2749887 -4.2795258 -4.2760134 -4.2620773 -4.2457309 -4.23316 -4.2364831 -4.2514949 -4.273922 -4.2982078 -4.3202071][-4.195096 -4.1926432 -4.2057133 -4.2259526 -4.2394609 -4.2410884 -4.2339659 -4.2182565 -4.20351 -4.1959186 -4.2044873 -4.2243171 -4.2531319 -4.2824388 -4.3065262][-4.1606064 -4.1535892 -4.1643758 -4.1802306 -4.1908145 -4.1910362 -4.1783919 -4.1568537 -4.14011 -4.1374531 -4.1533294 -4.1806216 -4.2189884 -4.2578511 -4.2881484][-4.1211085 -4.1071196 -4.1122465 -4.1217504 -4.1283646 -4.1280913 -4.1105218 -4.0785336 -4.0498295 -4.0455747 -4.0697765 -4.1137195 -4.1708488 -4.2268486 -4.2699938][-4.0686769 -4.0434837 -4.036181 -4.0355949 -4.0387588 -4.0389729 -4.0154037 -3.9669249 -3.9161835 -3.9091449 -3.9510098 -4.0241284 -4.10898 -4.1886888 -4.2506547][-4.0271044 -3.9900179 -3.9705648 -3.9615369 -3.9676924 -3.9759216 -3.9536891 -3.8963945 -3.8353815 -3.8354814 -3.8975582 -3.9900959 -4.0846639 -4.1724138 -4.2424564][-4.0273442 -3.9888349 -3.9681416 -3.95899 -3.9740193 -3.9914172 -3.976403 -3.9327126 -3.8886971 -3.903075 -3.9680011 -4.046422 -4.1223016 -4.1971884 -4.2587109][-4.0690145 -4.0404606 -4.0311089 -4.0296297 -4.0462918 -4.0616407 -4.0500331 -4.0228906 -3.9986815 -4.0135417 -4.0602484 -4.115468 -4.1735225 -4.2347183 -4.284214][-4.1328335 -4.1163864 -4.1156321 -4.1205668 -4.1366558 -4.1480479 -4.1366482 -4.1185369 -4.1012988 -4.107842 -4.1345444 -4.1708584 -4.2142339 -4.2628546 -4.3027344][-4.18572 -4.1771 -4.1811996 -4.1881428 -4.2017164 -4.2100353 -4.2013564 -4.1843996 -4.1678004 -4.1680875 -4.1842208 -4.2110205 -4.2436781 -4.2814746 -4.3139834][-4.2258487 -4.2223587 -4.2293868 -4.237596 -4.24712 -4.2536311 -4.2500191 -4.2369633 -4.222158 -4.2189403 -4.2293577 -4.2503948 -4.2749248 -4.3010731 -4.3238521][-4.2661347 -4.266027 -4.2717919 -4.2779541 -4.2828622 -4.2876177 -4.2891588 -4.2829719 -4.2747259 -4.272676 -4.2803659 -4.2943988 -4.3098688 -4.3231506 -4.3349667][-4.2999835 -4.301198 -4.3035836 -4.3060575 -4.3077712 -4.3099251 -4.3117218 -4.3107533 -4.3091908 -4.3098259 -4.3151269 -4.3231568 -4.3313317 -4.3377423 -4.3437796]]...]
INFO - root - 2017-12-05 20:02:30.086449: step 40210, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 67h:45m:26s remains)
INFO - root - 2017-12-05 20:02:38.531067: step 40220, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 69h:22m:09s remains)
INFO - root - 2017-12-05 20:02:47.050774: step 40230, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.834 sec/batch; 67h:42m:05s remains)
INFO - root - 2017-12-05 20:02:55.565631: step 40240, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 71h:03m:07s remains)
INFO - root - 2017-12-05 20:03:03.926719: step 40250, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 68h:55m:01s remains)
INFO - root - 2017-12-05 20:03:12.557431: step 40260, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 69h:26m:50s remains)
INFO - root - 2017-12-05 20:03:21.002312: step 40270, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 69h:13m:54s remains)
INFO - root - 2017-12-05 20:03:29.377723: step 40280, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 70h:05m:00s remains)
INFO - root - 2017-12-05 20:03:37.891329: step 40290, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.869 sec/batch; 70h:31m:30s remains)
INFO - root - 2017-12-05 20:03:46.487648: step 40300, loss = 2.03, batch loss = 1.98 (9.6 examples/sec; 0.833 sec/batch; 67h:38m:52s remains)
2017-12-05 20:03:47.356159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2627511 -4.2499552 -4.2199674 -4.1786237 -4.1497931 -4.1533117 -4.1761832 -4.2047429 -4.2387 -4.2729025 -4.2950931 -4.3020473 -4.3009853 -4.297029 -4.2865038][-4.2725325 -4.2523561 -4.2069526 -4.1435189 -4.0975351 -4.10662 -4.1494513 -4.193531 -4.2389021 -4.2844443 -4.3154817 -4.3249083 -4.3206334 -4.3131418 -4.2991667][-4.2807488 -4.2585397 -4.2033858 -4.118628 -4.0490546 -4.0600624 -4.1210113 -4.1779327 -4.2321124 -4.28703 -4.327189 -4.3418832 -4.3394179 -4.3328938 -4.32055][-4.2884946 -4.268743 -4.2098384 -4.1104856 -4.0181746 -4.021801 -4.0902257 -4.1531358 -4.2113943 -4.2713857 -4.3185158 -4.3409657 -4.3464737 -4.346168 -4.3397775][-4.2923989 -4.2764993 -4.2203593 -4.1203985 -4.0142965 -3.9942942 -4.0500765 -4.1091657 -4.1670017 -4.2322192 -4.2871518 -4.3208771 -4.3400097 -4.3496137 -4.3505826][-4.2901216 -4.2779846 -4.2288027 -4.1382661 -4.0235677 -3.9630637 -3.98648 -4.03789 -4.1025376 -4.1818261 -4.2485642 -4.2934136 -4.3256955 -4.3447924 -4.3522391][-4.2918668 -4.282999 -4.2405214 -4.1593351 -4.03559 -3.9256356 -3.9015832 -3.9463854 -4.0298719 -4.1298137 -4.2097435 -4.2661042 -4.3103476 -4.3364854 -4.3470159][-4.2997618 -4.2938046 -4.2580996 -4.1878843 -4.0665388 -3.9257295 -3.8579032 -3.884917 -3.9761891 -4.0906129 -4.1818933 -4.2463255 -4.2975612 -4.3291154 -4.3423638][-4.3095789 -4.308866 -4.2832689 -4.2300262 -4.130043 -3.9950206 -3.8986425 -3.888886 -3.9624248 -4.07431 -4.1672907 -4.23088 -4.2828298 -4.3189263 -4.3354783][-4.320693 -4.3245144 -4.3105073 -4.2773647 -4.2068839 -4.0950904 -3.9834421 -3.9328659 -3.9732225 -4.0689578 -4.1551213 -4.2161584 -4.2672877 -4.3064146 -4.32801][-4.3317823 -4.3371725 -4.3317919 -4.3139219 -4.269012 -4.184032 -4.0751157 -3.9969053 -3.9975185 -4.0664668 -4.1417031 -4.2013769 -4.2531815 -4.2974443 -4.3254151][-4.3393517 -4.3420095 -4.3385124 -4.3299065 -4.3033533 -4.245625 -4.1551971 -4.0651393 -4.0254779 -4.0591316 -4.1211247 -4.17757 -4.2326803 -4.2841406 -4.3190737][-4.3403382 -4.3390865 -4.3337874 -4.3298969 -4.3164287 -4.279469 -4.2106051 -4.1215668 -4.0525265 -4.0531645 -4.101192 -4.1536651 -4.21037 -4.268528 -4.3085456][-4.3333631 -4.3291712 -4.3207908 -4.317852 -4.3141155 -4.2934031 -4.246881 -4.1735983 -4.0973806 -4.0709772 -4.0998182 -4.1475945 -4.2045846 -4.2631164 -4.3006563][-4.3171029 -4.3135676 -4.3029575 -4.29957 -4.30213 -4.2954559 -4.2710347 -4.2236228 -4.1610465 -4.12274 -4.1318212 -4.1696563 -4.2184682 -4.2655015 -4.2910843]]...]
INFO - root - 2017-12-05 20:03:55.831456: step 40310, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 69h:28m:42s remains)
INFO - root - 2017-12-05 20:04:04.342542: step 40320, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 68h:34m:25s remains)
INFO - root - 2017-12-05 20:04:12.841380: step 40330, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 67h:24m:23s remains)
INFO - root - 2017-12-05 20:04:21.274802: step 40340, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 68h:22m:37s remains)
INFO - root - 2017-12-05 20:04:29.727965: step 40350, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 69h:02m:03s remains)
INFO - root - 2017-12-05 20:04:38.304657: step 40360, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 69h:26m:48s remains)
INFO - root - 2017-12-05 20:04:46.762486: step 40370, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.823 sec/batch; 66h:49m:00s remains)
INFO - root - 2017-12-05 20:04:55.097244: step 40380, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 69h:10m:29s remains)
INFO - root - 2017-12-05 20:05:03.617676: step 40390, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 69h:24m:56s remains)
INFO - root - 2017-12-05 20:05:12.216561: step 40400, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 69h:11m:58s remains)
2017-12-05 20:05:12.926109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2863426 -4.2755532 -4.2682538 -4.2598453 -4.2344475 -4.198153 -4.18727 -4.2054682 -4.2349896 -4.2617884 -4.268466 -4.272871 -4.2766643 -4.2799954 -4.2823663][-4.2809205 -4.27045 -4.2649632 -4.2610717 -4.2421465 -4.2094 -4.1913424 -4.20129 -4.22939 -4.2573261 -4.269486 -4.2783713 -4.28416 -4.2880449 -4.2911577][-4.2744842 -4.2608585 -4.2518969 -4.2466054 -4.2274895 -4.1925721 -4.1695995 -4.1745858 -4.2021427 -4.2319336 -4.2508225 -4.265687 -4.2769022 -4.285851 -4.2925096][-4.2798438 -4.2633066 -4.2468839 -4.2306247 -4.1990042 -4.1564236 -4.1281338 -4.1285224 -4.1554933 -4.1876993 -4.2132182 -4.2368846 -4.2569389 -4.2755961 -4.2889595][-4.2929111 -4.2728472 -4.2471375 -4.2147703 -4.1634364 -4.1075568 -4.0759811 -4.0721507 -4.0938983 -4.1199222 -4.1496849 -4.1881633 -4.2249293 -4.2603083 -4.285234][-4.3017459 -4.2813983 -4.2482796 -4.201427 -4.1336775 -4.07185 -4.0404148 -4.02904 -4.0337982 -4.0412736 -4.0639982 -4.1166453 -4.1775608 -4.2373977 -4.2767787][-4.3078651 -4.2857027 -4.2468877 -4.1921177 -4.1167374 -4.0530968 -4.0231533 -4.007144 -3.9952526 -3.9775274 -3.98387 -4.0464129 -4.1320734 -4.2160592 -4.2684355][-4.3113189 -4.2891335 -4.250525 -4.1944089 -4.113039 -4.0469637 -4.02131 -4.0086956 -3.9880781 -3.9532163 -3.9458759 -4.0111208 -4.1070676 -4.2031126 -4.2640119][-4.3090892 -4.2886152 -4.2563739 -4.2041874 -4.1252546 -4.061492 -4.0367632 -4.0295835 -4.0093741 -3.9709299 -3.9596746 -4.0195646 -4.1115346 -4.205606 -4.2650895][-4.3065591 -4.290422 -4.2661223 -4.2253509 -4.1621203 -4.1056986 -4.0821519 -4.0775642 -4.06306 -4.0345345 -4.0273676 -4.0766006 -4.1510525 -4.227931 -4.277142][-4.3036551 -4.2899003 -4.2714276 -4.241724 -4.1972828 -4.1540833 -4.13665 -4.141192 -4.138711 -4.1231909 -4.1169 -4.1492367 -4.1989069 -4.2515268 -4.2882876][-4.2969151 -4.2821546 -4.26693 -4.2475662 -4.22172 -4.1952081 -4.187583 -4.2010894 -4.2086678 -4.20476 -4.2004638 -4.2174788 -4.2452335 -4.2779827 -4.3041167][-4.2933702 -4.2758093 -4.2619462 -4.251586 -4.2406168 -4.2292056 -4.2324786 -4.2504053 -4.2644167 -4.2698097 -4.2689738 -4.2756596 -4.2878833 -4.3059278 -4.3232589][-4.3005242 -4.2815752 -4.2687197 -4.2593679 -4.2519355 -4.2493172 -4.2590928 -4.2777724 -4.2940726 -4.3052111 -4.3072414 -4.3093038 -4.3133945 -4.3220968 -4.3317103][-4.3129992 -4.2960052 -4.284492 -4.2719221 -4.2571406 -4.2524991 -4.2658119 -4.2856736 -4.3030705 -4.3163786 -4.3184185 -4.3192873 -4.3204494 -4.323802 -4.3279]]...]
INFO - root - 2017-12-05 20:05:21.429750: step 40410, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 70h:31m:28s remains)
INFO - root - 2017-12-05 20:05:30.011182: step 40420, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 70h:06m:03s remains)
INFO - root - 2017-12-05 20:05:38.559086: step 40430, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 68h:16m:54s remains)
INFO - root - 2017-12-05 20:05:47.067241: step 40440, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 69h:12m:22s remains)
INFO - root - 2017-12-05 20:05:55.683262: step 40450, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 70h:01m:10s remains)
INFO - root - 2017-12-05 20:06:04.226823: step 40460, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 68h:34m:37s remains)
INFO - root - 2017-12-05 20:06:12.761981: step 40470, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 68h:09m:33s remains)
INFO - root - 2017-12-05 20:06:20.903140: step 40480, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 70h:17m:31s remains)
INFO - root - 2017-12-05 20:06:29.362661: step 40490, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.847 sec/batch; 68h:40m:03s remains)
INFO - root - 2017-12-05 20:06:37.850313: step 40500, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 69h:34m:19s remains)
2017-12-05 20:06:38.647035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1666107 -4.12341 -4.1223183 -4.1772065 -4.2333694 -4.2601748 -4.2681513 -4.2697158 -4.2659025 -4.2534366 -4.2333832 -4.2099848 -4.1997552 -4.2098022 -4.2257972][-4.1539927 -4.1046753 -4.0916848 -4.1456103 -4.2059741 -4.2322526 -4.2393341 -4.2409973 -4.2410135 -4.2360439 -4.2255421 -4.2074537 -4.1995831 -4.2096763 -4.2224269][-4.1579633 -4.1040573 -4.0822678 -4.1291838 -4.1844583 -4.2034678 -4.2043571 -4.2062345 -4.212326 -4.2175975 -4.2224436 -4.2131419 -4.205698 -4.2123604 -4.2183027][-4.174623 -4.1289921 -4.101675 -4.132894 -4.1726775 -4.1789513 -4.1712556 -4.171423 -4.1816297 -4.197598 -4.21606 -4.2147617 -4.2062917 -4.204669 -4.2037778][-4.1866488 -4.16359 -4.1417685 -4.1541805 -4.1720109 -4.1621518 -4.1405368 -4.1299572 -4.139298 -4.164484 -4.1919255 -4.1986203 -4.1898103 -4.1817322 -4.1782703][-4.1973872 -4.1977611 -4.1871481 -4.186945 -4.183826 -4.1551781 -4.1138043 -4.0856156 -4.0903058 -4.1221137 -4.154943 -4.1705937 -4.1638546 -4.1530385 -4.1481261][-4.2122817 -4.2288389 -4.2282987 -4.2200832 -4.2009196 -4.1547313 -4.092999 -4.0470181 -4.0455432 -4.0825996 -4.1219306 -4.1500978 -4.1487017 -4.1359663 -4.1278968][-4.22836 -4.2550673 -4.2617087 -4.2470975 -4.2138247 -4.1558404 -4.0818825 -4.0267658 -4.0205965 -4.0616026 -4.1076941 -4.1437316 -4.1444287 -4.1252413 -4.1104064][-4.2486687 -4.27761 -4.2856045 -4.2641373 -4.2176552 -4.14863 -4.0667562 -4.0086203 -4.0012541 -4.0462046 -4.0992088 -4.1428146 -4.144258 -4.1160083 -4.0935698][-4.2678423 -4.292048 -4.2973847 -4.2708592 -4.2158866 -4.137361 -4.0504327 -3.992311 -3.9853611 -4.0315323 -4.091826 -4.1411562 -4.143312 -4.1106439 -4.0894604][-4.2844505 -4.2976151 -4.2967362 -4.26541 -4.2058811 -4.1241083 -4.0387521 -3.9864156 -3.9836497 -4.0283318 -4.0895782 -4.1369967 -4.134491 -4.1001596 -4.0909781][-4.292871 -4.2924714 -4.2829485 -4.2494345 -4.19115 -4.1155562 -4.0407343 -4.0009694 -4.0029387 -4.0409989 -4.09149 -4.1229839 -4.1068115 -4.0692496 -4.0766168][-4.2896938 -4.28138 -4.2693253 -4.240943 -4.19279 -4.1326427 -4.0769067 -4.0518084 -4.0523171 -4.0716629 -4.0947766 -4.0957503 -4.0574727 -4.0120196 -4.0317822][-4.2829113 -4.2717643 -4.2601938 -4.2392225 -4.204957 -4.1642342 -4.1271882 -4.1107659 -4.10534 -4.1014419 -4.09218 -4.0625811 -4.003365 -3.9512239 -3.9792283][-4.280385 -4.2686319 -4.2579837 -4.2420807 -4.2193747 -4.1949859 -4.1703854 -4.156096 -4.1435571 -4.1215529 -4.0864353 -4.0360274 -3.9681339 -3.9175596 -3.9482198]]...]
INFO - root - 2017-12-05 20:06:47.213410: step 40510, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 68h:51m:38s remains)
INFO - root - 2017-12-05 20:06:55.697934: step 40520, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 67h:43m:00s remains)
INFO - root - 2017-12-05 20:07:04.053260: step 40530, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 67h:06m:03s remains)
INFO - root - 2017-12-05 20:07:12.717257: step 40540, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 69h:19m:18s remains)
INFO - root - 2017-12-05 20:07:21.085889: step 40550, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 69h:21m:17s remains)
INFO - root - 2017-12-05 20:07:29.722225: step 40560, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 67h:41m:04s remains)
INFO - root - 2017-12-05 20:07:38.220189: step 40570, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 72h:00m:01s remains)
INFO - root - 2017-12-05 20:07:46.648328: step 40580, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.829 sec/batch; 67h:15m:30s remains)
INFO - root - 2017-12-05 20:07:55.060852: step 40590, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 69h:16m:32s remains)
INFO - root - 2017-12-05 20:08:03.546961: step 40600, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 71h:52m:38s remains)
2017-12-05 20:08:04.312211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3234429 -4.3217244 -4.31405 -4.3077421 -4.305 -4.3036294 -4.3067021 -4.3113275 -4.3068833 -4.2939935 -4.2815371 -4.2705827 -4.2560945 -4.2460656 -4.2456474][-4.3159342 -4.3102031 -4.2997346 -4.2945056 -4.2953219 -4.297071 -4.3032532 -4.3110332 -4.307857 -4.291955 -4.2720766 -4.2521353 -4.2340741 -4.2296162 -4.2396727][-4.3053 -4.2948713 -4.2801332 -4.2730865 -4.2730556 -4.2739077 -4.2807131 -4.2913818 -4.2946434 -4.2856984 -4.2689548 -4.2500372 -4.2354093 -4.2374892 -4.2548218][-4.2931175 -4.2781477 -4.2585473 -4.2452817 -4.236443 -4.2281237 -4.2303019 -4.2448134 -4.2621808 -4.2723646 -4.2727261 -4.2666225 -4.2600403 -4.2633371 -4.2796707][-4.2804585 -4.2634616 -4.2390575 -4.214735 -4.1871371 -4.15852 -4.1480036 -4.1649342 -4.2001886 -4.2372627 -4.2651448 -4.2797413 -4.2823424 -4.2836881 -4.2943554][-4.2683969 -4.2508097 -4.2227859 -4.1882229 -4.1376009 -4.0805025 -4.0468907 -4.06077 -4.1138196 -4.1794434 -4.2378631 -4.2754316 -4.2882018 -4.2864394 -4.2903833][-4.2570033 -4.2397442 -4.2111483 -4.1707563 -4.1029496 -4.0157995 -3.95191 -3.9590886 -4.0291343 -4.118742 -4.2011242 -4.2581358 -4.2789073 -4.272687 -4.2700191][-4.2496934 -4.2328238 -4.2077875 -4.1704683 -4.0960407 -3.9885178 -3.898216 -3.8985202 -3.9750719 -4.0725918 -4.1662073 -4.234901 -4.2618523 -4.2525339 -4.2450814][-4.2500663 -4.2346411 -4.2159586 -4.1894326 -4.123281 -4.0187478 -3.9249129 -3.9198654 -3.9842925 -4.0682535 -4.1552114 -4.2253547 -4.2550821 -4.2452025 -4.2372231][-4.2658806 -4.2593641 -4.2501163 -4.2340913 -4.1837964 -4.0984855 -4.0178475 -4.0055027 -4.05082 -4.11095 -4.1770849 -4.2376871 -4.2672105 -4.2633467 -4.2610421][-4.281889 -4.286799 -4.2865562 -4.2799993 -4.2449574 -4.1768003 -4.1064777 -4.0869761 -4.114253 -4.1547394 -4.2022376 -4.2527885 -4.28434 -4.2917533 -4.2984815][-4.2801018 -4.2914419 -4.2973938 -4.29926 -4.2753758 -4.2180891 -4.1546283 -4.1307836 -4.1480522 -4.178668 -4.2164874 -4.2592111 -4.29028 -4.3044033 -4.3164868][-4.2664318 -4.2767692 -4.2830925 -4.2900362 -4.274663 -4.22894 -4.1790519 -4.1600523 -4.1756248 -4.2001772 -4.2290335 -4.2617207 -4.2858458 -4.2979126 -4.3073926][-4.2641444 -4.2667 -4.2683454 -4.2773738 -4.2714868 -4.2441654 -4.2187881 -4.2138777 -4.2268229 -4.2399883 -4.2520742 -4.2646332 -4.271915 -4.2740045 -4.2762237][-4.2719254 -4.2641144 -4.2599635 -4.2725878 -4.2805929 -4.2763853 -4.2769752 -4.2849989 -4.291213 -4.2885485 -4.2802548 -4.2703738 -4.2610254 -4.2552471 -4.2521906]]...]
INFO - root - 2017-12-05 20:08:12.858279: step 40610, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 70h:01m:01s remains)
INFO - root - 2017-12-05 20:08:21.311073: step 40620, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 68h:49m:37s remains)
INFO - root - 2017-12-05 20:08:29.939902: step 40630, loss = 2.10, batch loss = 2.05 (9.1 examples/sec; 0.880 sec/batch; 71h:18m:31s remains)
INFO - root - 2017-12-05 20:08:38.511084: step 40640, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 69h:27m:54s remains)
INFO - root - 2017-12-05 20:08:46.747644: step 40650, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 70h:46m:00s remains)
INFO - root - 2017-12-05 20:08:55.288117: step 40660, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 70h:59m:07s remains)
INFO - root - 2017-12-05 20:09:03.828433: step 40670, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 71h:52m:56s remains)
INFO - root - 2017-12-05 20:09:12.464595: step 40680, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 67h:58m:26s remains)
INFO - root - 2017-12-05 20:09:21.001630: step 40690, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 69h:02m:15s remains)
INFO - root - 2017-12-05 20:09:29.268814: step 40700, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 67h:30m:10s remains)
2017-12-05 20:09:30.004053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.124218 -4.173131 -4.1982856 -4.2092538 -4.2059808 -4.1969223 -4.1814418 -4.1737585 -4.1716995 -4.177115 -4.1803856 -4.1781974 -4.1924076 -4.2187 -4.2469316][-4.1306138 -4.1765795 -4.1911778 -4.1884651 -4.1788859 -4.173686 -4.16566 -4.1615882 -4.1599808 -4.1664119 -4.1743655 -4.1764646 -4.1918354 -4.2143135 -4.2453341][-4.1516333 -4.1855936 -4.1870556 -4.1754527 -4.1671381 -4.1674252 -4.1650596 -4.1656628 -4.1641226 -4.1662245 -4.1706986 -4.1732802 -4.1886258 -4.2080822 -4.2387352][-4.1755271 -4.1948442 -4.1838956 -4.1652164 -4.1613836 -4.1656523 -4.171597 -4.1812687 -4.18802 -4.1927285 -4.1938858 -4.1948442 -4.2065849 -4.2192092 -4.2453012][-4.1941667 -4.2018337 -4.1791878 -4.1542487 -4.1481805 -4.1494536 -4.1563168 -4.1761804 -4.2000914 -4.2169895 -4.2218003 -4.2248468 -4.2312503 -4.2406092 -4.2611785][-4.1917996 -4.1906834 -4.1611385 -4.126864 -4.1078496 -4.093729 -4.0932846 -4.123939 -4.1683807 -4.2002921 -4.212007 -4.2195792 -4.2275548 -4.2438259 -4.2667322][-4.1788073 -4.17806 -4.1470432 -4.0990348 -4.0500059 -4.0015454 -3.9859664 -4.0260329 -4.0955925 -4.1458731 -4.17076 -4.1889987 -4.205791 -4.2307982 -4.2568765][-4.1672463 -4.1709776 -4.1403618 -4.0783887 -3.9933572 -3.9015014 -3.8633363 -3.9082375 -4.0010881 -4.0743723 -4.1177649 -4.1550031 -4.1826091 -4.2125292 -4.239346][-4.1752028 -4.1907725 -4.1712718 -4.1171513 -4.0305085 -3.9309125 -3.8820662 -3.9105115 -3.98316 -4.0446196 -4.0867558 -4.1334481 -4.1729732 -4.2074881 -4.2331238][-4.1925797 -4.2156277 -4.2072296 -4.17497 -4.114397 -4.0428257 -4.0102358 -4.0235176 -4.0576177 -4.0834274 -4.1053824 -4.1438627 -4.1838017 -4.2190394 -4.241539][-4.2064767 -4.2235355 -4.2188659 -4.1999717 -4.1574345 -4.1091352 -4.0961037 -4.1088691 -4.1224914 -4.1297646 -4.13988 -4.1691284 -4.2068276 -4.2434745 -4.2622476][-4.2068963 -4.205615 -4.1994748 -4.1874409 -4.1566386 -4.1213489 -4.1205144 -4.1316323 -4.1299238 -4.1215105 -4.12777 -4.1591334 -4.2016835 -4.2443175 -4.2648377][-4.1846986 -4.1697426 -4.164402 -4.156569 -4.1342955 -4.110724 -4.1175537 -4.1257544 -4.1099296 -4.086163 -4.0889311 -4.1257839 -4.1741333 -4.2212024 -4.2454572][-4.152524 -4.1399517 -4.1430721 -4.1431479 -4.1326618 -4.1218781 -4.1314535 -4.1335268 -4.1095891 -4.0855002 -4.09481 -4.132154 -4.1693568 -4.2017465 -4.2246566][-4.1518326 -4.1453781 -4.1528482 -4.1560464 -4.15429 -4.1555824 -4.1639791 -4.1598034 -4.1351538 -4.1217065 -4.1400118 -4.1716213 -4.1961017 -4.2124496 -4.2268343]]...]
INFO - root - 2017-12-05 20:09:38.439639: step 40710, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 67h:19m:03s remains)
INFO - root - 2017-12-05 20:09:47.010327: step 40720, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 69h:17m:53s remains)
INFO - root - 2017-12-05 20:09:55.388086: step 40730, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 67h:28m:58s remains)
INFO - root - 2017-12-05 20:10:03.794205: step 40740, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.825 sec/batch; 66h:49m:47s remains)
INFO - root - 2017-12-05 20:10:12.186352: step 40750, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 68h:22m:47s remains)
INFO - root - 2017-12-05 20:10:20.536488: step 40760, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 69h:11m:02s remains)
INFO - root - 2017-12-05 20:10:28.970134: step 40770, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 67h:07m:08s remains)
INFO - root - 2017-12-05 20:10:37.418935: step 40780, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 70h:00m:33s remains)
INFO - root - 2017-12-05 20:10:45.917236: step 40790, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 66h:53m:55s remains)
INFO - root - 2017-12-05 20:10:54.330193: step 40800, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 67h:10m:39s remains)
2017-12-05 20:10:55.039654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0279346 -4.0607853 -4.0694661 -4.0625792 -4.0624204 -4.0538549 -4.0388184 -4.0334525 -4.038166 -4.0595918 -4.0779 -4.0633283 -4.0432734 -4.0227547 -3.9951992][-4.0262747 -4.0517855 -4.0530314 -4.044477 -4.0446272 -4.0434341 -4.0382557 -4.0398107 -4.0423341 -4.0624018 -4.0769911 -4.0576563 -4.038343 -4.0267267 -4.0019331][-4.0322304 -4.0467105 -4.0424819 -4.0352659 -4.0408287 -4.0504346 -4.0586567 -4.064332 -4.0653296 -4.0825047 -4.089365 -4.0667038 -4.0549994 -4.0509257 -4.033917][-4.0344977 -4.0452108 -4.0452585 -4.045733 -4.0591621 -4.07703 -4.0938091 -4.0941854 -4.0937772 -4.1026115 -4.1009994 -4.0767832 -4.0658374 -4.0617862 -4.0567794][-4.0320048 -4.0412602 -4.0520906 -4.0651603 -4.0807867 -4.09689 -4.1123977 -4.1097503 -4.1087656 -4.1078181 -4.0969057 -4.0744839 -4.0644054 -4.0620193 -4.0603395][-4.011867 -4.0166874 -4.0303741 -4.04564 -4.0558214 -4.0688925 -4.0839133 -4.0809736 -4.0772033 -4.0717859 -4.0605874 -4.046195 -4.0465941 -4.0558395 -4.0571961][-3.9850669 -3.9825637 -3.9860067 -3.9911392 -3.9923286 -4.004025 -4.02521 -4.0261722 -4.0237656 -4.0238209 -4.0190959 -4.0113139 -4.0245366 -4.0513053 -4.0573716][-3.9887857 -3.9740293 -3.9583364 -3.9490697 -3.9359827 -3.9431605 -3.9732723 -3.9924767 -4.0044971 -4.0151243 -4.0152249 -4.00974 -4.0278153 -4.0560265 -4.0556436][-4.0093179 -3.9875169 -3.9642773 -3.9457769 -3.920701 -3.9185622 -3.9475951 -3.9760933 -3.9978998 -4.0179267 -4.0246983 -4.0201941 -4.0364509 -4.058732 -4.0572596][-4.0446916 -4.0216885 -4.0068283 -3.9949405 -3.9747128 -3.9677517 -3.9848571 -4.0072689 -4.0221009 -4.0380034 -4.0431719 -4.0366592 -4.0484381 -4.0688872 -4.0783892][-4.0889287 -4.0713282 -4.0656781 -4.0605779 -4.0456185 -4.0328264 -4.0387011 -4.0492749 -4.0535216 -4.0685349 -4.0815415 -4.0784106 -4.0865765 -4.108716 -4.1310949][-4.1053042 -4.089798 -4.0857682 -4.081368 -4.0649385 -4.0475583 -4.0423441 -4.0422926 -4.0456543 -4.06445 -4.0862708 -4.0945907 -4.1121 -4.14226 -4.1716866][-4.1065273 -4.09143 -4.0827556 -4.0708575 -4.0483422 -4.0287051 -4.0169477 -4.0110435 -4.0173945 -4.0382423 -4.0570226 -4.0657153 -4.0915713 -4.1341529 -4.167305][-4.1241341 -4.110909 -4.0970359 -4.0787024 -4.0530176 -4.0311341 -4.0171189 -4.0101414 -4.0156474 -4.0350108 -4.0452108 -4.0432034 -4.0606623 -4.1047049 -4.1422296][-4.1563888 -4.1450315 -4.1254864 -4.1029148 -4.0747151 -4.0537095 -4.042285 -4.0317092 -4.0296831 -4.040772 -4.0481205 -4.0462923 -4.0564384 -4.0943103 -4.1301785]]...]
INFO - root - 2017-12-05 20:11:03.596428: step 40810, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 69h:31m:00s remains)
INFO - root - 2017-12-05 20:11:12.172202: step 40820, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.838 sec/batch; 67h:51m:54s remains)
INFO - root - 2017-12-05 20:11:20.703210: step 40830, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.862 sec/batch; 69h:50m:09s remains)
INFO - root - 2017-12-05 20:11:29.248742: step 40840, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 69h:57m:28s remains)
INFO - root - 2017-12-05 20:11:37.678750: step 40850, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 67h:47m:40s remains)
INFO - root - 2017-12-05 20:11:46.238975: step 40860, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 66h:54m:58s remains)
INFO - root - 2017-12-05 20:11:54.771579: step 40870, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 70h:37m:36s remains)
INFO - root - 2017-12-05 20:12:03.273193: step 40880, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 68h:08m:02s remains)
INFO - root - 2017-12-05 20:12:11.681505: step 40890, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.817 sec/batch; 66h:08m:47s remains)
INFO - root - 2017-12-05 20:12:20.302550: step 40900, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 69h:17m:13s remains)
2017-12-05 20:12:21.078215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3142529 -4.3108244 -4.3082795 -4.3052239 -4.2974167 -4.2901807 -4.2888923 -4.2895942 -4.2805738 -4.2600856 -4.2423344 -4.230824 -4.2290235 -4.2458477 -4.2767282][-4.3091421 -4.3051729 -4.3030095 -4.2975812 -4.2868423 -4.2793221 -4.2793326 -4.2775717 -4.2640157 -4.241116 -4.2183204 -4.2034917 -4.2049007 -4.2312784 -4.2705417][-4.3040509 -4.2962122 -4.2920804 -4.2854366 -4.2773967 -4.2763367 -4.2761946 -4.2677984 -4.2491827 -4.2234545 -4.1961575 -4.1787071 -4.1849461 -4.2175417 -4.2624121][-4.3015227 -4.2889037 -4.281951 -4.2765813 -4.2736068 -4.2753844 -4.26839 -4.2499738 -4.2261963 -4.1983943 -4.1695695 -4.154345 -4.1657004 -4.20253 -4.2522216][-4.3022947 -4.2895789 -4.2795782 -4.2700744 -4.2682528 -4.26645 -4.2419205 -4.2119203 -4.1919804 -4.1695032 -4.1491876 -4.1424584 -4.1639462 -4.2015285 -4.2511415][-4.2992735 -4.2845411 -4.2674613 -4.2502737 -4.2420678 -4.227551 -4.1792984 -4.1362739 -4.130681 -4.1319389 -4.1280508 -4.1355462 -4.1666951 -4.2033677 -4.2452979][-4.2849588 -4.2664084 -4.241117 -4.2164593 -4.1976662 -4.1612048 -4.0828977 -4.0156059 -4.0336432 -4.0835772 -4.1143713 -4.1379862 -4.1727033 -4.2000823 -4.2268791][-4.2513847 -4.2273388 -4.1972976 -4.1677837 -4.1381354 -4.079947 -3.9638944 -3.8488426 -3.8844633 -3.99552 -4.0750532 -4.1191678 -4.1567221 -4.1732931 -4.1884136][-4.2195492 -4.1890168 -4.1538334 -4.1217942 -4.0914969 -4.0356297 -3.913352 -3.7819891 -3.82409 -3.9601307 -4.0550609 -4.1038609 -4.1339931 -4.1400347 -4.1439776][-4.2095189 -4.178628 -4.1466274 -4.1217432 -4.1081223 -4.07813 -3.9925234 -3.8990452 -3.9260817 -4.0268989 -4.0958524 -4.1285434 -4.1435452 -4.139708 -4.1290612][-4.2062626 -4.1787562 -4.1540356 -4.1385984 -4.1378036 -4.1292439 -4.0799484 -4.0235143 -4.0343385 -4.0951037 -4.1370945 -4.155582 -4.1629181 -4.1557493 -4.1400914][-4.1963229 -4.1718512 -4.156733 -4.1513176 -4.1577072 -4.1620245 -4.1384115 -4.1064472 -4.1089482 -4.1460142 -4.1750422 -4.1874843 -4.1904535 -4.1823258 -4.1693859][-4.1961031 -4.1724296 -4.1568131 -4.1500573 -4.1563339 -4.1673303 -4.159873 -4.1441097 -4.1460829 -4.1718693 -4.19537 -4.2076893 -4.2072763 -4.2003088 -4.1917667][-4.2210484 -4.1943936 -4.1706953 -4.156373 -4.1597338 -4.172091 -4.1767216 -4.1738825 -4.1748915 -4.1897306 -4.2077456 -4.2189665 -4.2199759 -4.2175465 -4.2138586][-4.2687531 -4.2427349 -4.2161317 -4.2008963 -4.2029042 -4.2128582 -4.2220917 -4.2271581 -4.22763 -4.2328506 -4.241457 -4.247086 -4.2500167 -4.2553205 -4.2581153]]...]
INFO - root - 2017-12-05 20:12:29.479255: step 40910, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 68h:36m:05s remains)
INFO - root - 2017-12-05 20:12:37.799842: step 40920, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 68h:34m:41s remains)
INFO - root - 2017-12-05 20:12:46.284791: step 40930, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 70h:58m:22s remains)
INFO - root - 2017-12-05 20:12:54.795204: step 40940, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 72h:32m:53s remains)
INFO - root - 2017-12-05 20:13:03.269836: step 40950, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 68h:31m:32s remains)
INFO - root - 2017-12-05 20:13:11.875606: step 40960, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 72h:08m:16s remains)
INFO - root - 2017-12-05 20:13:20.345841: step 40970, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 68h:38m:45s remains)
INFO - root - 2017-12-05 20:13:28.939945: step 40980, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 67h:51m:30s remains)
INFO - root - 2017-12-05 20:13:37.463698: step 40990, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 69h:06m:13s remains)
INFO - root - 2017-12-05 20:13:46.046459: step 41000, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 71h:26m:39s remains)
2017-12-05 20:13:46.812067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1651096 -4.161087 -4.1824927 -4.2093773 -4.2145333 -4.1949034 -4.1492577 -4.0895519 -4.0787349 -4.1284032 -4.1994114 -4.2555432 -4.292182 -4.3218365 -4.3411784][-4.1721258 -4.1520343 -4.1598539 -4.1840734 -4.1881804 -4.1630092 -4.1156635 -4.0604835 -4.0624151 -4.1243958 -4.1998334 -4.2557011 -4.2918487 -4.322011 -4.3415561][-4.1654367 -4.1427517 -4.144268 -4.157198 -4.1517706 -4.1238756 -4.0782194 -4.0320687 -4.0464425 -4.1182451 -4.1956468 -4.2519813 -4.2915535 -4.324481 -4.3423276][-4.174201 -4.1577129 -4.1542721 -4.1506367 -4.1299644 -4.0928726 -4.038847 -3.9906387 -4.0133581 -4.0971041 -4.1851273 -4.2484927 -4.2948003 -4.3287187 -4.3435211][-4.2038765 -4.1924281 -4.1824951 -4.1603847 -4.12424 -4.0728083 -4.0003939 -3.9479818 -3.9818063 -4.0834823 -4.1794424 -4.2464409 -4.2972674 -4.330235 -4.3439484][-4.2327647 -4.2232976 -4.2047691 -4.1695862 -4.1274219 -4.0658097 -3.9803128 -3.9280014 -3.9735584 -4.0886364 -4.1825008 -4.248394 -4.30163 -4.3319907 -4.3435535][-4.2456789 -4.238348 -4.2154775 -4.172183 -4.1260152 -4.0577173 -3.9639339 -3.9102068 -3.9614997 -4.0860953 -4.1840119 -4.2539573 -4.3091512 -4.3359923 -4.3425388][-4.2507434 -4.2419372 -4.2158356 -4.1667771 -4.1183944 -4.052659 -3.9625692 -3.9127321 -3.960516 -4.083075 -4.1841674 -4.2607689 -4.3152018 -4.3379297 -4.3409481][-4.25487 -4.2403941 -4.2110753 -4.1616588 -4.1083484 -4.04628 -3.9734011 -3.9471054 -3.9968181 -4.0999584 -4.1919794 -4.2664552 -4.3171678 -4.3351436 -4.3384585][-4.2574973 -4.2396879 -4.2095823 -4.1632719 -4.1080608 -4.0482526 -3.9893475 -3.9840269 -4.03867 -4.1224117 -4.1981068 -4.26239 -4.3108039 -4.3284154 -4.33483][-4.2577262 -4.2393584 -4.2129221 -4.1752629 -4.1252823 -4.0667353 -4.0153675 -4.0136218 -4.0648785 -4.1406178 -4.2072053 -4.2620826 -4.305634 -4.3231554 -4.3316593][-4.2572131 -4.2408123 -4.2187319 -4.1892848 -4.1457071 -4.0904312 -4.0416174 -4.0327091 -4.075666 -4.1497006 -4.2164149 -4.2669826 -4.3029137 -4.3193579 -4.3309331][-4.2579851 -4.246685 -4.2299018 -4.20452 -4.163569 -4.1113935 -4.0623975 -4.0408292 -4.0744386 -4.1478662 -4.2180724 -4.268559 -4.2990651 -4.314949 -4.3306417][-4.2651858 -4.2656827 -4.2557411 -4.2338305 -4.194766 -4.141335 -4.0881066 -4.0515747 -4.072051 -4.143703 -4.2183332 -4.2708039 -4.2992954 -4.3148484 -4.3320589][-4.2765317 -4.2894659 -4.2868857 -4.2707644 -4.2363534 -4.1822643 -4.1253171 -4.0797038 -4.0885782 -4.1532598 -4.2238479 -4.27563 -4.3035531 -4.31928 -4.3355222]]...]
INFO - root - 2017-12-05 20:13:55.374906: step 41010, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 69h:36m:13s remains)
INFO - root - 2017-12-05 20:14:03.750925: step 41020, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 70h:04m:33s remains)
INFO - root - 2017-12-05 20:14:12.260123: step 41030, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 68h:19m:11s remains)
INFO - root - 2017-12-05 20:14:20.787914: step 41040, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 68h:47m:07s remains)
INFO - root - 2017-12-05 20:14:28.991101: step 41050, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 67h:40m:51s remains)
INFO - root - 2017-12-05 20:14:37.417775: step 41060, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 68h:33m:16s remains)
INFO - root - 2017-12-05 20:14:46.154769: step 41070, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 70h:32m:11s remains)
INFO - root - 2017-12-05 20:14:54.619253: step 41080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 69h:01m:46s remains)
INFO - root - 2017-12-05 20:15:03.230423: step 41090, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 70h:01m:25s remains)
INFO - root - 2017-12-05 20:15:11.686001: step 41100, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 67h:37m:13s remains)
2017-12-05 20:15:12.535233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1618042 -4.1497927 -4.1686134 -4.1928558 -4.2044425 -4.2053061 -4.1985731 -4.189991 -4.1866622 -4.1845622 -4.1804395 -4.1853213 -4.2112923 -4.244669 -4.2707396][-4.1492457 -4.1381249 -4.1544847 -4.1728005 -4.1778 -4.1739373 -4.1683264 -4.16375 -4.1655269 -4.1681294 -4.1669436 -4.1759963 -4.2053766 -4.2409105 -4.272542][-4.162365 -4.1505013 -4.1552324 -4.1580505 -4.150229 -4.1345921 -4.1202383 -4.1152058 -4.1256518 -4.1442986 -4.1637597 -4.186429 -4.2166662 -4.2493095 -4.2791677][-4.1979551 -4.1829686 -4.1703892 -4.1502624 -4.1185689 -4.07577 -4.0409474 -4.037117 -4.0702629 -4.119082 -4.168776 -4.2078919 -4.2383661 -4.2673407 -4.2923474][-4.2398467 -4.2227616 -4.1942587 -4.1478152 -4.0795159 -3.9911563 -3.9253693 -3.9361446 -4.0099368 -4.0973911 -4.1751118 -4.2292647 -4.2617826 -4.2878032 -4.3061433][-4.2670693 -4.2492337 -4.2103596 -4.1402864 -4.0270662 -3.8856463 -3.7906 -3.8376071 -3.9690802 -4.0917349 -4.1821976 -4.2426591 -4.2797632 -4.30402 -4.3148665][-4.2741961 -4.2606921 -4.2166343 -4.129673 -3.988548 -3.82735 -3.7387886 -3.8233109 -3.9839938 -4.1160693 -4.2025952 -4.2617822 -4.2982016 -4.3144264 -4.3129811][-4.2682858 -4.2604494 -4.2143488 -4.1245136 -3.9900455 -3.8649886 -3.831847 -3.9277153 -4.0623651 -4.1692505 -4.2387075 -4.2863131 -4.3128438 -4.3107567 -4.2912431][-4.2572503 -4.2580705 -4.2189746 -4.1403928 -4.0350828 -3.9641111 -3.9760382 -4.0581994 -4.15146 -4.2264457 -4.2741952 -4.30309 -4.3100829 -4.2886081 -4.25714][-4.2482519 -4.2626123 -4.2395425 -4.1841455 -4.1174655 -4.0859914 -4.1109152 -4.1665373 -4.2255535 -4.2741728 -4.2995567 -4.3043981 -4.2937579 -4.26415 -4.23428][-4.2545991 -4.2775187 -4.2677321 -4.2342877 -4.1995473 -4.190115 -4.2124782 -4.2449193 -4.2754974 -4.29812 -4.3020868 -4.289619 -4.2729473 -4.2479906 -4.2269688][-4.2709718 -4.2907758 -4.2856827 -4.2661672 -4.2500086 -4.2488871 -4.2653604 -4.2840934 -4.2966471 -4.2951226 -4.28447 -4.2682967 -4.255528 -4.2386041 -4.2250915][-4.2842431 -4.2919149 -4.2843051 -4.2699256 -4.2618618 -4.2616186 -4.27194 -4.2812848 -4.2822604 -4.2707582 -4.2589726 -4.24862 -4.240108 -4.2274175 -4.2172894][-4.2928152 -4.2892866 -4.2757931 -4.2589955 -4.2503 -4.2451067 -4.2452412 -4.2440267 -4.2397203 -4.2299786 -4.2237968 -4.2228303 -4.2174211 -4.2051411 -4.197053][-4.2993021 -4.29182 -4.2744274 -4.2545209 -4.2394171 -4.2235818 -4.2114062 -4.2001863 -4.1930304 -4.1856208 -4.1829329 -4.1851664 -4.1821995 -4.1737728 -4.170548]]...]
INFO - root - 2017-12-05 20:15:21.070137: step 41110, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.878 sec/batch; 71h:05m:30s remains)
INFO - root - 2017-12-05 20:15:29.651545: step 41120, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 69h:01m:53s remains)
INFO - root - 2017-12-05 20:15:38.085543: step 41130, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 67h:59m:07s remains)
INFO - root - 2017-12-05 20:15:46.652180: step 41140, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 69h:17m:27s remains)
INFO - root - 2017-12-05 20:15:55.109625: step 41150, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 70h:36m:13s remains)
INFO - root - 2017-12-05 20:16:03.674336: step 41160, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.841 sec/batch; 68h:05m:35s remains)
INFO - root - 2017-12-05 20:16:12.115697: step 41170, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 68h:48m:46s remains)
INFO - root - 2017-12-05 20:16:20.716688: step 41180, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 72h:11m:33s remains)
INFO - root - 2017-12-05 20:16:29.281107: step 41190, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 70h:28m:11s remains)
INFO - root - 2017-12-05 20:16:37.783253: step 41200, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 67h:51m:12s remains)
2017-12-05 20:16:38.560858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.270771 -4.2581391 -4.2519045 -4.251339 -4.2554007 -4.2626085 -4.2727041 -4.28504 -4.2963386 -4.3066278 -4.3166928 -4.3245616 -4.3290248 -4.3308368 -4.3310218][-4.3037782 -4.291153 -4.2831445 -4.2791138 -4.2788725 -4.2814994 -4.2871885 -4.2954078 -4.3030205 -4.3105659 -4.3187642 -4.3254685 -4.3295197 -4.3313389 -4.3315816][-4.3308368 -4.3197308 -4.3108897 -4.3040175 -4.2991853 -4.2962689 -4.2965446 -4.30047 -4.3050938 -4.3105679 -4.3176451 -4.3241615 -4.3286719 -4.3312006 -4.3319826][-4.3356891 -4.3242331 -4.3127713 -4.3014197 -4.2908645 -4.2821708 -4.278553 -4.2807984 -4.2865286 -4.2944622 -4.3043075 -4.31379 -4.3214231 -4.3267407 -4.3297315][-4.3114238 -4.2976336 -4.2812958 -4.262579 -4.2434797 -4.22847 -4.2220097 -4.224546 -4.2341847 -4.2498584 -4.2680144 -4.2855363 -4.3006339 -4.3124723 -4.3207035][-4.2619553 -4.2463059 -4.2245431 -4.1963396 -4.1656466 -4.1410966 -4.1302986 -4.1336956 -4.1489711 -4.1754551 -4.2069831 -4.2372813 -4.2635269 -4.2851911 -4.3014641][-4.2063556 -4.1963511 -4.1761928 -4.1434536 -4.1030235 -4.06673 -4.0464458 -4.04463 -4.0599594 -4.092237 -4.1343374 -4.1770153 -4.2148347 -4.2471261 -4.27203][-4.1613569 -4.1672664 -4.1614475 -4.1382637 -4.10163 -4.0621157 -4.0330648 -4.0207276 -4.0259 -4.0498395 -4.0882759 -4.1322789 -4.1739936 -4.2108316 -4.2401376][-4.1451497 -4.1671772 -4.1793857 -4.1732674 -4.15217 -4.1236339 -4.0964589 -4.0796041 -4.0736079 -4.0802932 -4.0995684 -4.1283488 -4.1603932 -4.1913342 -4.2177854][-4.1698437 -4.1980209 -4.2198691 -4.22694 -4.2221551 -4.2096915 -4.192368 -4.1780005 -4.1668277 -4.1594806 -4.157917 -4.164463 -4.1777158 -4.19456 -4.2124305][-4.2198434 -4.2453623 -4.2689147 -4.283947 -4.29203 -4.2935586 -4.2876387 -4.279109 -4.267818 -4.2536736 -4.2392111 -4.2289104 -4.22484 -4.2264452 -4.2326503][-4.2678657 -4.2873921 -4.3084755 -4.3266807 -4.3419056 -4.3520203 -4.3537855 -4.349997 -4.3411274 -4.3275518 -4.3106947 -4.2948623 -4.2829394 -4.275537 -4.2727304][-4.3018746 -4.3134146 -4.3287778 -4.3447232 -4.359694 -4.3708148 -4.375865 -4.3763161 -4.3719611 -4.3633533 -4.3514996 -4.3394 -4.3283591 -4.3195176 -4.3138313][-4.3164244 -4.3205462 -4.3286066 -4.3380075 -4.3483825 -4.3575096 -4.36403 -4.3685837 -4.3702993 -4.3685637 -4.3640289 -4.3585577 -4.3524728 -4.3462267 -4.3414516][-4.3216357 -4.32138 -4.3249588 -4.3300538 -4.3366132 -4.3434305 -4.3495922 -4.3551159 -4.3592529 -4.3611321 -4.361155 -4.3601341 -4.3577189 -4.3542705 -4.3514442]]...]
INFO - root - 2017-12-05 20:16:47.118182: step 41210, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 69h:12m:52s remains)
INFO - root - 2017-12-05 20:16:55.660248: step 41220, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 67h:51m:26s remains)
INFO - root - 2017-12-05 20:17:04.096769: step 41230, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 68h:26m:33s remains)
INFO - root - 2017-12-05 20:17:12.443974: step 41240, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 68h:31m:04s remains)
INFO - root - 2017-12-05 20:17:20.720242: step 41250, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.829 sec/batch; 67h:03m:06s remains)
INFO - root - 2017-12-05 20:17:29.235813: step 41260, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 68h:42m:41s remains)
INFO - root - 2017-12-05 20:17:37.628253: step 41270, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 67h:00m:52s remains)
INFO - root - 2017-12-05 20:17:46.059034: step 41280, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 68h:07m:22s remains)
INFO - root - 2017-12-05 20:17:54.492563: step 41290, loss = 2.05, batch loss = 2.00 (10.3 examples/sec; 0.774 sec/batch; 62h:35m:05s remains)
INFO - root - 2017-12-05 20:18:02.923382: step 41300, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 67h:22m:01s remains)
2017-12-05 20:18:03.815041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1067772 -4.0881643 -4.0903735 -4.0918493 -4.0995297 -4.125834 -4.1579132 -4.175705 -4.1819363 -4.178607 -4.1846986 -4.2028084 -4.2141891 -4.201889 -4.1885104][-4.1043577 -4.0778604 -4.0686488 -4.0561237 -4.053966 -4.074791 -4.1058297 -4.1313748 -4.1500163 -4.1634364 -4.1830864 -4.2076612 -4.2208409 -4.2082191 -4.1934981][-4.1384435 -4.1122522 -4.0867424 -4.0550976 -4.0339355 -4.0354481 -4.0506439 -4.0720224 -4.099133 -4.1283474 -4.1661172 -4.2025948 -4.223846 -4.2165856 -4.2002587][-4.1972017 -4.1794567 -4.1438956 -4.0952907 -4.0536366 -4.0248032 -4.0136046 -4.0221262 -4.0509067 -4.0931458 -4.1486712 -4.2028346 -4.2358904 -4.2366667 -4.2189393][-4.2471719 -4.2386417 -4.2020516 -4.1457481 -4.088244 -4.0280466 -3.9806688 -3.9599223 -3.9815953 -4.03983 -4.1174679 -4.1936641 -4.2399039 -4.2492871 -4.234159][-4.2745323 -4.2740331 -4.2444177 -4.1952462 -4.1353092 -4.0582595 -3.9770367 -3.9086199 -3.9022799 -3.9739833 -4.0792961 -4.1745272 -4.2289634 -4.2434483 -4.2327623][-4.2776041 -4.2798929 -4.2571316 -4.2217188 -4.1719775 -4.0933785 -3.9904594 -3.8711832 -3.8176007 -3.8955989 -4.0313787 -4.1484556 -4.211473 -4.2277422 -4.2192469][-4.2583108 -4.26338 -4.2474852 -4.2255025 -4.1899867 -4.1224785 -4.0218368 -3.8850574 -3.8001428 -3.8611653 -4.0010533 -4.125433 -4.194068 -4.2094173 -4.197371][-4.239646 -4.2466936 -4.2382736 -4.2261972 -4.202661 -4.1490498 -4.0673909 -3.9505906 -3.8709726 -3.9077456 -4.0190291 -4.1282854 -4.1920018 -4.2041483 -4.1866155][-4.2337394 -4.2420068 -4.2352929 -4.2261248 -4.2077947 -4.1678953 -4.1110215 -4.0253673 -3.96754 -3.9981837 -4.0805206 -4.1643739 -4.2098913 -4.2092633 -4.1832409][-4.2476335 -4.2539587 -4.2444849 -4.229538 -4.2077651 -4.1761036 -4.1367393 -4.0797324 -4.0463905 -4.0773606 -4.1413121 -4.2057281 -4.2319565 -4.2157507 -4.183835][-4.2657347 -4.2699957 -4.2577586 -4.23758 -4.21235 -4.1864519 -4.1602206 -4.1272836 -4.115169 -4.1442671 -4.1934423 -4.241271 -4.25285 -4.2272215 -4.1944065][-4.2781634 -4.2844667 -4.2737141 -4.2547231 -4.2317367 -4.2121634 -4.195363 -4.1784773 -4.1807365 -4.2062535 -4.2407546 -4.2707438 -4.2691188 -4.2403874 -4.2075272][-4.30001 -4.3069172 -4.2958293 -4.2796779 -4.2638154 -4.2531447 -4.2452836 -4.2365875 -4.24337 -4.262198 -4.2840657 -4.2989769 -4.2875051 -4.2589283 -4.226584][-4.3127418 -4.3179469 -4.3081388 -4.2978125 -4.2916636 -4.2899246 -4.2874093 -4.2826858 -4.2883916 -4.2997484 -4.3113165 -4.3136635 -4.2951007 -4.2654853 -4.23558]]...]
INFO - root - 2017-12-05 20:18:12.135330: step 41310, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 69h:13m:06s remains)
INFO - root - 2017-12-05 20:18:20.521972: step 41320, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.844 sec/batch; 68h:15m:14s remains)
INFO - root - 2017-12-05 20:18:29.001501: step 41330, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 68h:52m:06s remains)
INFO - root - 2017-12-05 20:18:37.335041: step 41340, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.759 sec/batch; 61h:23m:23s remains)
INFO - root - 2017-12-05 20:18:45.969581: step 41350, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 68h:48m:10s remains)
INFO - root - 2017-12-05 20:18:54.383917: step 41360, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 68h:39m:18s remains)
INFO - root - 2017-12-05 20:19:02.800298: step 41370, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 66h:54m:45s remains)
INFO - root - 2017-12-05 20:19:11.430967: step 41380, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 68h:38m:16s remains)
INFO - root - 2017-12-05 20:19:19.963914: step 41390, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.828 sec/batch; 66h:54m:59s remains)
INFO - root - 2017-12-05 20:19:28.415584: step 41400, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 68h:38m:36s remains)
2017-12-05 20:19:29.130562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2929573 -4.2788639 -4.2656775 -4.2636909 -4.2666764 -4.2693968 -4.2753682 -4.2796431 -4.2851925 -4.2908664 -4.2928734 -4.2834215 -4.2682424 -4.2554822 -4.2501254][-4.2822776 -4.2695189 -4.2598758 -4.25843 -4.2581758 -4.2562494 -4.26007 -4.2661467 -4.2753072 -4.2827697 -4.2849417 -4.2739735 -4.2517581 -4.2309279 -4.2236257][-4.265274 -4.25599 -4.2500405 -4.248198 -4.2405133 -4.2268248 -4.2251344 -4.234653 -4.2516222 -4.2656918 -4.2715158 -4.261651 -4.2340293 -4.2066922 -4.2001595][-4.2506166 -4.244288 -4.2394185 -4.2328715 -4.2127056 -4.1832952 -4.173172 -4.1863742 -4.2148547 -4.2422552 -4.2567344 -4.2517257 -4.2255936 -4.1994629 -4.1958146][-4.2360482 -4.2312012 -4.2259436 -4.2160215 -4.1862416 -4.14261 -4.1209645 -4.1337824 -4.1739364 -4.2187281 -4.24692 -4.2507591 -4.2310033 -4.2139788 -4.2169065][-4.2207212 -4.2191858 -4.2139554 -4.1993914 -4.1604948 -4.1011586 -4.0636506 -4.0684452 -4.1191254 -4.1886024 -4.2358809 -4.2477465 -4.2351055 -4.228219 -4.2380147][-4.2094517 -4.2115717 -4.2057352 -4.1833019 -4.1295547 -4.0497851 -3.9864368 -3.97531 -4.0368853 -4.1322222 -4.201344 -4.2258358 -4.220561 -4.2231817 -4.2373915][-4.2028184 -4.2072649 -4.2010555 -4.1690183 -4.1009703 -4.0047979 -3.9171691 -3.8865643 -3.9528792 -4.0643282 -4.1491818 -4.1857176 -4.1870394 -4.1939058 -4.2127738][-4.2059288 -4.2210402 -4.2211714 -4.1872907 -4.1200194 -4.0239406 -3.9293458 -3.8883014 -3.9434764 -4.046545 -4.1313438 -4.1705008 -4.1714 -4.1754932 -4.194407][-4.2114773 -4.2389612 -4.2477231 -4.2197661 -4.1649113 -4.0856667 -4.0053372 -3.966423 -4.0045943 -4.0865974 -4.1560583 -4.1887302 -4.1857953 -4.1851482 -4.2023387][-4.2208915 -4.2525916 -4.2658348 -4.2486329 -4.208765 -4.1497922 -4.0889459 -4.0582085 -4.0834303 -4.144001 -4.1959929 -4.221499 -4.2215033 -4.2218018 -4.2358685][-4.2400908 -4.2654014 -4.2724934 -4.260036 -4.2314563 -4.1907678 -4.1504622 -4.1319404 -4.1503944 -4.1945662 -4.2319269 -4.252244 -4.2573271 -4.263823 -4.2772937][-4.25965 -4.2698622 -4.2637272 -4.2518029 -4.2320738 -4.20479 -4.1820507 -4.1772 -4.1947479 -4.2285757 -4.2573824 -4.2744861 -4.2829704 -4.2921171 -4.3028393][-4.27652 -4.2727566 -4.2578254 -4.245903 -4.2312307 -4.2130775 -4.2011342 -4.2030845 -4.2194724 -4.2426224 -4.2613611 -4.2716026 -4.2804093 -4.291873 -4.302228][-4.2906256 -4.2802534 -4.2609777 -4.2458916 -4.2310815 -4.2164388 -4.2075481 -4.21002 -4.2225952 -4.2367239 -4.2470217 -4.2512393 -4.2559686 -4.2650495 -4.2742305]]...]
INFO - root - 2017-12-05 20:19:37.530388: step 41410, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.814 sec/batch; 65h:48m:28s remains)
INFO - root - 2017-12-05 20:19:46.041107: step 41420, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 68h:52m:24s remains)
INFO - root - 2017-12-05 20:19:54.644653: step 41430, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 69h:08m:42s remains)
INFO - root - 2017-12-05 20:20:03.120918: step 41440, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 69h:03m:42s remains)
INFO - root - 2017-12-05 20:20:11.342064: step 41450, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 66h:58m:31s remains)
INFO - root - 2017-12-05 20:20:19.830730: step 41460, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 69h:43m:00s remains)
INFO - root - 2017-12-05 20:20:28.119453: step 41470, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 68h:03m:07s remains)
INFO - root - 2017-12-05 20:20:36.442475: step 41480, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 67h:04m:51s remains)
INFO - root - 2017-12-05 20:20:44.821529: step 41490, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 71h:37m:24s remains)
INFO - root - 2017-12-05 20:20:53.394335: step 41500, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.829 sec/batch; 67h:02m:00s remains)
2017-12-05 20:20:54.183868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1402674 -4.1461806 -4.1635976 -4.1941848 -4.2300243 -4.2561116 -4.2539134 -4.2287984 -4.20057 -4.1895289 -4.1889615 -4.2002807 -4.2109318 -4.2119465 -4.2029548][-4.1411986 -4.1520181 -4.1737013 -4.2066936 -4.2444634 -4.2721605 -4.272089 -4.2429967 -4.2104979 -4.1977887 -4.1980834 -4.2109981 -4.2214003 -4.2237372 -4.2164054][-4.1339331 -4.1531959 -4.1871915 -4.2276611 -4.2653856 -4.2886415 -4.2856126 -4.2523189 -4.2129827 -4.1948037 -4.1924944 -4.2042127 -4.2170877 -4.2239375 -4.2229404][-4.1286683 -4.1656146 -4.2136359 -4.2557807 -4.2859349 -4.2980065 -4.283556 -4.2437787 -4.1994438 -4.1764297 -4.17609 -4.1906629 -4.2097325 -4.2221508 -4.2295728][-4.1404762 -4.191967 -4.2452869 -4.2768016 -4.28961 -4.2819185 -4.2504253 -4.1984119 -4.148829 -4.1331086 -4.1483526 -4.1802721 -4.213378 -4.2357116 -4.2501087][-4.1627545 -4.2135086 -4.2591772 -4.277504 -4.2704225 -4.2467303 -4.1981177 -4.1287255 -4.0771732 -4.086184 -4.131845 -4.1866913 -4.2303472 -4.2550678 -4.2710166][-4.1901588 -4.2267342 -4.2550282 -4.257544 -4.2332606 -4.1949859 -4.1223559 -4.0263376 -3.9836974 -4.0365462 -4.1168032 -4.1886668 -4.2328348 -4.2537079 -4.2693706][-4.2036471 -4.2191992 -4.228251 -4.2224469 -4.1889973 -4.1364837 -4.0365596 -3.9102187 -3.8947992 -4.0006981 -4.1086617 -4.1917224 -4.2332268 -4.2475772 -4.2617855][-4.1939621 -4.18827 -4.1856589 -4.1799822 -4.1516228 -4.0979142 -3.9980056 -3.8832819 -3.9043617 -4.0253353 -4.1301231 -4.2049756 -4.2373986 -4.2452188 -4.2551203][-4.1608963 -4.1400037 -4.1368952 -4.1382704 -4.1254745 -4.0898848 -4.0244827 -3.9672444 -4.0092673 -4.1051879 -4.1830807 -4.235898 -4.2556705 -4.2577925 -4.2625666][-4.1278415 -4.1019168 -4.1036391 -4.1107693 -4.1129718 -4.1014166 -4.0753222 -4.0651674 -4.1121874 -4.1822615 -4.2346487 -4.271 -4.2848945 -4.28453 -4.2855015][-4.112422 -4.0925736 -4.1005697 -4.1129866 -4.1243167 -4.1342549 -4.1349993 -4.1421251 -4.1826572 -4.2343831 -4.2740817 -4.3014793 -4.3127708 -4.3128228 -4.31247][-4.105145 -4.0974522 -4.1122828 -4.1313782 -4.1495523 -4.1703243 -4.1821861 -4.1931643 -4.2242575 -4.2672791 -4.3012824 -4.3228221 -4.329915 -4.3264384 -4.3261089][-4.1310072 -4.1343322 -4.1501017 -4.165659 -4.1823516 -4.2031693 -4.2177553 -4.2279291 -4.2529011 -4.2880077 -4.3138256 -4.3269391 -4.3274512 -4.319643 -4.3188391][-4.1943312 -4.2022614 -4.2111506 -4.2146988 -4.2180891 -4.2270961 -4.2379336 -4.2433043 -4.2624016 -4.2892666 -4.306313 -4.3110566 -4.3044181 -4.29297 -4.2924833]]...]
INFO - root - 2017-12-05 20:21:02.751222: step 41510, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 69h:40m:20s remains)
INFO - root - 2017-12-05 20:21:11.319575: step 41520, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 70h:03m:11s remains)
INFO - root - 2017-12-05 20:21:19.873936: step 41530, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 69h:55m:39s remains)
INFO - root - 2017-12-05 20:21:28.410592: step 41540, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 69h:57m:44s remains)
INFO - root - 2017-12-05 20:21:36.810651: step 41550, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 69h:55m:18s remains)
INFO - root - 2017-12-05 20:21:45.188244: step 41560, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 68h:54m:18s remains)
INFO - root - 2017-12-05 20:21:53.719573: step 41570, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 68h:42m:22s remains)
INFO - root - 2017-12-05 20:22:02.235394: step 41580, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 68h:56m:41s remains)
INFO - root - 2017-12-05 20:22:10.795926: step 41590, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 69h:01m:18s remains)
INFO - root - 2017-12-05 20:22:19.309912: step 41600, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 69h:54m:25s remains)
2017-12-05 20:22:20.120170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1916833 -4.1779094 -4.1564665 -4.1414871 -4.1459265 -4.1629376 -4.1752114 -4.1806908 -4.1853161 -4.1957788 -4.208477 -4.2125683 -4.1999526 -4.1778846 -4.1560912][-4.2032933 -4.1829429 -4.1580715 -4.1421943 -4.1475987 -4.1606126 -4.1692777 -4.1742558 -4.1743288 -4.172173 -4.1683426 -4.1605525 -4.1534967 -4.1437006 -4.1330585][-4.2048159 -4.1812835 -4.1524644 -4.1298971 -4.1299644 -4.1350379 -4.1410627 -4.1553464 -4.1623139 -4.1544719 -4.1310234 -4.1059637 -4.09201 -4.0870333 -4.0907369][-4.1980028 -4.1772318 -4.1433315 -4.1096396 -4.095809 -4.0858369 -4.0870681 -4.1159062 -4.1398811 -4.135829 -4.1063852 -4.0715508 -4.0495892 -4.0472922 -4.0567584][-4.1813874 -4.1628361 -4.1245589 -4.0807476 -4.0495996 -4.0209446 -4.0128856 -4.0553522 -4.1038041 -4.1187253 -4.102901 -4.0774822 -4.0524483 -4.0411048 -4.0400705][-4.1597872 -4.1461859 -4.1081467 -4.0617123 -4.0178361 -3.9683332 -3.9351983 -3.9794118 -4.0527945 -4.0945516 -4.10262 -4.0931592 -4.0684853 -4.0439982 -4.0288515][-4.1529784 -4.143086 -4.1032453 -4.0488634 -3.9859693 -3.9077489 -3.8381038 -3.870945 -3.9766591 -4.055903 -4.0935349 -4.0991588 -4.0799627 -4.0523109 -4.0313134][-4.166378 -4.1526432 -4.11138 -4.0538697 -3.9861829 -3.9028418 -3.826005 -3.8453231 -3.9609661 -4.0592394 -4.1120453 -4.1208873 -4.1029363 -4.074666 -4.0534625][-4.182404 -4.166707 -4.1321068 -4.0887771 -4.0419235 -3.985651 -3.9336579 -3.93602 -4.0099516 -4.0838747 -4.127542 -4.1384039 -4.121882 -4.0915065 -4.075778][-4.1961088 -4.1786733 -4.1515203 -4.1239967 -4.0957913 -4.0582595 -4.0183482 -3.9977865 -4.0243058 -4.0683908 -4.1066093 -4.1230497 -4.1135383 -4.0895634 -4.082983][-4.2053232 -4.1822238 -4.1547031 -4.1362786 -4.1237059 -4.1019168 -4.0676804 -4.0315862 -4.02467 -4.0453424 -4.0777664 -4.0954018 -4.0949512 -4.0836048 -4.0875349][-4.2096477 -4.1876245 -4.1628251 -4.1526175 -4.152102 -4.1392107 -4.1046443 -4.06113 -4.0377431 -4.0472012 -4.0672321 -4.0738626 -4.077332 -4.0815578 -4.0920358][-4.2213192 -4.2015319 -4.1770568 -4.1666327 -4.1692872 -4.1555014 -4.116786 -4.0723896 -4.0508976 -4.0601187 -4.0687776 -4.0629826 -4.0676837 -4.0830512 -4.0981722][-4.2188888 -4.1937284 -4.1673641 -4.1507592 -4.14995 -4.134325 -4.0971913 -4.0592451 -4.0505042 -4.0670047 -4.0654969 -4.0483036 -4.0556178 -4.0817294 -4.1050911][-4.2170539 -4.192059 -4.1678467 -4.1488094 -4.1359515 -4.1074786 -4.0626554 -4.0244617 -4.0240207 -4.0522122 -4.0469928 -4.0161319 -4.0232129 -4.0598688 -4.0939984]]...]
INFO - root - 2017-12-05 20:22:28.597262: step 41610, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 69h:24m:21s remains)
INFO - root - 2017-12-05 20:22:37.018937: step 41620, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 67h:57m:57s remains)
INFO - root - 2017-12-05 20:22:45.588428: step 41630, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 66h:27m:17s remains)
INFO - root - 2017-12-05 20:22:54.208488: step 41640, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 69h:46m:39s remains)
INFO - root - 2017-12-05 20:23:02.652926: step 41650, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.847 sec/batch; 68h:23m:41s remains)
INFO - root - 2017-12-05 20:23:11.253084: step 41660, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 70h:20m:32s remains)
INFO - root - 2017-12-05 20:23:19.693706: step 41670, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 69h:13m:29s remains)
INFO - root - 2017-12-05 20:23:28.353546: step 41680, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 69h:44m:43s remains)
INFO - root - 2017-12-05 20:23:36.835148: step 41690, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 68h:52m:40s remains)
INFO - root - 2017-12-05 20:23:45.405688: step 41700, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 71h:08m:26s remains)
2017-12-05 20:23:46.152632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33197 -4.3284397 -4.3254504 -4.3236089 -4.32236 -4.3121295 -4.2911987 -4.2660618 -4.2541704 -4.2539692 -4.2556705 -4.2595906 -4.2699819 -4.2811971 -4.2809162][-4.3275037 -4.3222113 -4.3163176 -4.3120389 -4.3073096 -4.2944465 -4.2745147 -4.2545981 -4.2455244 -4.2457652 -4.2467484 -4.2511883 -4.2649908 -4.2766862 -4.2770605][-4.3289423 -4.3220682 -4.3116579 -4.3022213 -4.2937884 -4.2771244 -4.2567873 -4.240036 -4.2332754 -4.2349658 -4.237339 -4.2435966 -4.2624521 -4.2780075 -4.2783074][-4.3302569 -4.3198881 -4.3032384 -4.2881432 -4.2784448 -4.2615886 -4.2350893 -4.2115035 -4.2070575 -4.2156177 -4.2231035 -4.2327485 -4.2586455 -4.2821274 -4.2828827][-4.32474 -4.3097291 -4.2880187 -4.268 -4.2558465 -4.2385206 -4.2001605 -4.1617618 -4.1575146 -4.1776447 -4.1981988 -4.2179604 -4.2526965 -4.283915 -4.288754][-4.3132396 -4.2929239 -4.2672358 -4.2386155 -4.2176275 -4.1930447 -4.140142 -4.0814104 -4.0699368 -4.10519 -4.1505432 -4.1902032 -4.2375765 -4.2778296 -4.29136][-4.3009868 -4.2781243 -4.2501731 -4.2116613 -4.1776633 -4.1411963 -4.0689011 -3.9800897 -3.9486904 -4.0064406 -4.090116 -4.1576 -4.2195754 -4.2681313 -4.2906704][-4.2946105 -4.2731695 -4.2461681 -4.2056255 -4.1638718 -4.1162534 -4.0214334 -3.8947344 -3.8302343 -3.9057868 -4.031167 -4.1258759 -4.2011671 -4.2559485 -4.2860146][-4.290411 -4.2702127 -4.2438192 -4.2060537 -4.1641788 -4.1101041 -4.0145926 -3.8914902 -3.8276346 -3.9029162 -4.031496 -4.1261864 -4.1976762 -4.2516851 -4.283401][-4.291194 -4.2718744 -4.2465558 -4.2117047 -4.1752377 -4.1316266 -4.070641 -3.9963841 -3.9637494 -4.0117149 -4.0901594 -4.148561 -4.2032332 -4.251986 -4.28181][-4.2936592 -4.2797537 -4.2625384 -4.2368574 -4.2096305 -4.1821904 -4.1528397 -4.1144791 -4.0931687 -4.1125727 -4.1455569 -4.1705809 -4.2125564 -4.2572289 -4.2810259][-4.2983909 -4.2949605 -4.2907472 -4.2790489 -4.2638469 -4.2494493 -4.2309008 -4.2008972 -4.178637 -4.1788459 -4.1870189 -4.19586 -4.2286105 -4.2671275 -4.2838745][-4.301702 -4.30679 -4.3116055 -4.3092203 -4.304564 -4.3014512 -4.2899394 -4.2639894 -4.2419686 -4.2316585 -4.2257013 -4.2240119 -4.2459731 -4.2771659 -4.2882257][-4.2993555 -4.3087111 -4.3168669 -4.3186212 -4.3183389 -4.3197589 -4.3134947 -4.2957239 -4.2812304 -4.2707744 -4.2590117 -4.2494473 -4.2625909 -4.2874274 -4.2928267][-4.288177 -4.298398 -4.3085971 -4.313417 -4.3165555 -4.3200431 -4.3176022 -4.3035269 -4.2912693 -4.2828684 -4.2721896 -4.2627158 -4.2726078 -4.2921643 -4.2936807]]...]
INFO - root - 2017-12-05 20:23:54.662223: step 41710, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 70h:02m:09s remains)
INFO - root - 2017-12-05 20:24:03.126080: step 41720, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 68h:40m:44s remains)
INFO - root - 2017-12-05 20:24:11.748761: step 41730, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 68h:44m:57s remains)
INFO - root - 2017-12-05 20:24:20.151405: step 41740, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.814 sec/batch; 65h:43m:03s remains)
INFO - root - 2017-12-05 20:24:28.608496: step 41750, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.838 sec/batch; 67h:38m:45s remains)
INFO - root - 2017-12-05 20:24:37.148419: step 41760, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 70h:05m:02s remains)
INFO - root - 2017-12-05 20:24:45.645454: step 41770, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.848 sec/batch; 68h:27m:07s remains)
INFO - root - 2017-12-05 20:24:54.109264: step 41780, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 69h:26m:41s remains)
INFO - root - 2017-12-05 20:25:02.620643: step 41790, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 70h:34m:55s remains)
INFO - root - 2017-12-05 20:25:10.869860: step 41800, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.824 sec/batch; 66h:34m:04s remains)
2017-12-05 20:25:11.587098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2331204 -4.217422 -4.1863861 -4.16583 -4.1799178 -4.20143 -4.213419 -4.21827 -4.2236786 -4.2266941 -4.2211132 -4.2222137 -4.2317753 -4.2374525 -4.2326717][-4.2033749 -4.18275 -4.1462493 -4.1260138 -4.1490636 -4.1782894 -4.1925116 -4.1902671 -4.1923561 -4.2040162 -4.2060733 -4.2120047 -4.22698 -4.2310357 -4.2144423][-4.181263 -4.1550217 -4.118474 -4.0988593 -4.1164975 -4.138402 -4.1408925 -4.1250806 -4.12609 -4.1510406 -4.166029 -4.18007 -4.2025595 -4.2026196 -4.1734228][-4.1753483 -4.1458011 -4.1103139 -4.0877295 -4.08873 -4.0826569 -4.0595865 -4.0354209 -4.046432 -4.0913739 -4.1261845 -4.1505914 -4.1775856 -4.176074 -4.1387091][-4.1878691 -4.158196 -4.1238832 -4.0970125 -4.0768251 -4.0332942 -3.9780216 -3.949513 -3.9815116 -4.0499105 -4.1019564 -4.1369681 -4.1684542 -4.1683884 -4.1321106][-4.1981006 -4.1664829 -4.1300182 -4.0981064 -4.0642347 -3.9991472 -3.9242415 -3.896728 -3.9516809 -4.0409412 -4.1021738 -4.1406569 -4.1716166 -4.172821 -4.1411381][-4.2016673 -4.1692085 -4.130312 -4.0955257 -4.0578465 -3.9916873 -3.9226098 -3.9065442 -3.9699907 -4.0601177 -4.119195 -4.1512017 -4.1736727 -4.175046 -4.1552186][-4.2038245 -4.1668572 -4.1213174 -4.0841093 -4.0520864 -3.9997442 -3.9501832 -3.9461038 -4.0056 -4.0847206 -4.1334109 -4.1523981 -4.1610217 -4.1633043 -4.1530514][-4.2011547 -4.1607151 -4.1100883 -4.0735159 -4.0487285 -4.0132155 -3.9845085 -3.9917948 -4.0418067 -4.1062751 -4.1440287 -4.149786 -4.1451383 -4.1470203 -4.1436405][-4.1931467 -4.1488371 -4.0956674 -4.0615039 -4.0485039 -4.0295806 -4.023478 -4.0411787 -4.0790124 -4.12899 -4.15431 -4.1498904 -4.1342115 -4.1334057 -4.1347103][-4.1798792 -4.1358871 -4.0842919 -4.0569248 -4.06195 -4.0618553 -4.0721259 -4.0914078 -4.1140423 -4.1438565 -4.1562037 -4.1482224 -4.1316881 -4.1285019 -4.1300163][-4.1664762 -4.1299462 -4.0894117 -4.0725121 -4.0923443 -4.10717 -4.1271267 -4.1410012 -4.1483455 -4.1578546 -4.1597338 -4.1525922 -4.1439486 -4.1424403 -4.1451559][-4.1658444 -4.1368484 -4.1032887 -4.09218 -4.1153221 -4.1391478 -4.1661577 -4.1770291 -4.1765194 -4.1740842 -4.1694446 -4.1643152 -4.1633439 -4.1668777 -4.1711073][-4.1750126 -4.1488175 -4.1186495 -4.1083059 -4.1301713 -4.1583509 -4.1887 -4.1951284 -4.1920195 -4.18673 -4.1837897 -4.1849222 -4.189466 -4.1969247 -4.199914][-4.1976876 -4.1693239 -4.1395955 -4.1259203 -4.136291 -4.1608491 -4.1887274 -4.197166 -4.1952705 -4.1954122 -4.2007914 -4.2075791 -4.2125907 -4.2177191 -4.2192597]]...]
INFO - root - 2017-12-05 20:25:20.140534: step 41810, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 68h:05m:03s remains)
INFO - root - 2017-12-05 20:25:28.474146: step 41820, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.824 sec/batch; 66h:31m:03s remains)
INFO - root - 2017-12-05 20:25:36.865215: step 41830, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 67h:57m:20s remains)
INFO - root - 2017-12-05 20:25:45.238453: step 41840, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.819 sec/batch; 66h:08m:23s remains)
INFO - root - 2017-12-05 20:25:53.719260: step 41850, loss = 2.07, batch loss = 2.02 (10.4 examples/sec; 0.771 sec/batch; 62h:13m:38s remains)
INFO - root - 2017-12-05 20:26:02.300803: step 41860, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 70h:31m:17s remains)
INFO - root - 2017-12-05 20:26:10.887898: step 41870, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 69h:08m:05s remains)
INFO - root - 2017-12-05 20:26:19.430100: step 41880, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 71h:44m:47s remains)
INFO - root - 2017-12-05 20:26:27.970353: step 41890, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 68h:20m:45s remains)
INFO - root - 2017-12-05 20:26:36.522087: step 41900, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 69h:54m:46s remains)
2017-12-05 20:26:37.341531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2279181 -4.2409115 -4.2381334 -4.2269483 -4.2071729 -4.1808095 -4.1544576 -4.1418767 -4.1288304 -4.1228104 -4.1330652 -4.1701093 -4.2058949 -4.2276616 -4.2402353][-4.222383 -4.2353392 -4.2284226 -4.2073011 -4.1801229 -4.14502 -4.1123962 -4.0979109 -4.0872602 -4.0908184 -4.1141448 -4.1539946 -4.1894131 -4.2148361 -4.2330928][-4.2108407 -4.2251143 -4.2138247 -4.1819754 -4.1446948 -4.1005783 -4.0640378 -4.0479445 -4.0555525 -4.0813241 -4.1135712 -4.1459122 -4.1764722 -4.206408 -4.2275419][-4.1964192 -4.2150149 -4.20293 -4.1628428 -4.1144586 -4.0629163 -4.0269122 -4.0177531 -4.0445547 -4.0878296 -4.1199059 -4.1402535 -4.1642957 -4.1920729 -4.2128057][-4.1858888 -4.2039413 -4.18514 -4.1325121 -4.07075 -4.0163784 -3.9871781 -3.9914877 -4.0310459 -4.0804024 -4.1145678 -4.1289449 -4.1448784 -4.1684933 -4.1850619][-4.1866646 -4.1914096 -4.1556683 -4.0904555 -4.0162783 -3.9627874 -3.937423 -3.9553318 -4.0093846 -4.0612493 -4.1005316 -4.1105561 -4.116673 -4.135406 -4.1447086][-4.1892452 -4.1740966 -4.1310105 -4.06617 -3.9953079 -3.9479437 -3.9226696 -3.9416411 -4.0004582 -4.04494 -4.0791397 -4.0858212 -4.0928884 -4.1089797 -4.1076655][-4.1869922 -4.1616364 -4.1208634 -4.0682907 -4.0076752 -3.9696608 -3.9515927 -3.9721508 -4.0275407 -4.0564146 -4.0727549 -4.078135 -4.0893912 -4.0977168 -4.0810623][-4.1945624 -4.1663837 -4.1318746 -4.0890689 -4.0442386 -4.0181713 -4.0131135 -4.0388851 -4.0857773 -4.0897169 -4.0842996 -4.0866494 -4.0883131 -4.0803056 -4.0491958][-4.215714 -4.1860981 -4.1500835 -4.1159716 -4.0886073 -4.0769415 -4.086381 -4.1157913 -4.1482382 -4.134891 -4.117023 -4.1122966 -4.0921469 -4.06691 -4.0321879][-4.2451019 -4.2230392 -4.1915832 -4.1644659 -4.1456337 -4.1387835 -4.1504068 -4.174027 -4.1934085 -4.1740956 -4.1556726 -4.14845 -4.1203537 -4.0917387 -4.058661][-4.2616253 -4.2521305 -4.2329621 -4.215332 -4.2024646 -4.1966214 -4.2051926 -4.2188258 -4.2212515 -4.2043347 -4.1880889 -4.176405 -4.1474242 -4.1209631 -4.0920386][-4.2623181 -4.2618103 -4.2563457 -4.2527027 -4.2482076 -4.2451782 -4.247467 -4.2501483 -4.2437468 -4.2289839 -4.2140937 -4.19875 -4.16791 -4.1426024 -4.1197114][-4.25535 -4.2594428 -4.261579 -4.2672505 -4.2711639 -4.273984 -4.2750659 -4.2723408 -4.2643256 -4.2512259 -4.2358928 -4.2201967 -4.1918468 -4.1653767 -4.1448216][-4.2447762 -4.2527566 -4.2589188 -4.2659736 -4.27183 -4.2771564 -4.28053 -4.2803478 -4.2756381 -4.2673993 -4.2547269 -4.2423167 -4.2203741 -4.1982274 -4.1801047]]...]
INFO - root - 2017-12-05 20:26:45.806030: step 41910, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 67h:37m:22s remains)
INFO - root - 2017-12-05 20:26:54.290422: step 41920, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 66h:22m:22s remains)
INFO - root - 2017-12-05 20:27:02.958475: step 41930, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 70h:17m:17s remains)
INFO - root - 2017-12-05 20:27:11.571201: step 41940, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 71h:47m:42s remains)
INFO - root - 2017-12-05 20:27:20.118991: step 41950, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.780 sec/batch; 62h:58m:37s remains)
INFO - root - 2017-12-05 20:27:28.839995: step 41960, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 71h:02m:10s remains)
INFO - root - 2017-12-05 20:27:37.350652: step 41970, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 68h:50m:20s remains)
INFO - root - 2017-12-05 20:27:45.880076: step 41980, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 68h:53m:45s remains)
INFO - root - 2017-12-05 20:27:54.327771: step 41990, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 71h:33m:21s remains)
INFO - root - 2017-12-05 20:28:02.983421: step 42000, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 68h:20m:05s remains)
2017-12-05 20:28:03.742240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.242403 -4.2453938 -4.2430186 -4.2346129 -4.2222724 -4.2085409 -4.1886978 -4.158288 -4.127574 -4.1013722 -4.0833559 -4.0933046 -4.1234937 -4.1500731 -4.1642385][-4.2337642 -4.2315073 -4.2231412 -4.21516 -4.2099934 -4.20464 -4.1876283 -4.1594224 -4.1371145 -4.1206994 -4.1074815 -4.1100936 -4.126749 -4.1338797 -4.1325955][-4.2205982 -4.2104983 -4.1979637 -4.1944203 -4.1944714 -4.1926947 -4.1782379 -4.1537571 -4.1428752 -4.1377316 -4.1311064 -4.1267462 -4.1243992 -4.1089854 -4.0890636][-4.1950674 -4.1759477 -4.1626067 -4.164464 -4.1695085 -4.1735964 -4.1622062 -4.1385937 -4.138803 -4.1512313 -4.1524558 -4.1420426 -4.1201506 -4.0857773 -4.052906][-4.1710191 -4.1417532 -4.1262589 -4.1327453 -4.1363311 -4.133956 -4.1051073 -4.0649791 -4.07804 -4.1182857 -4.1402984 -4.1344347 -4.1022105 -4.0650187 -4.0393982][-4.1448255 -4.105463 -4.0769367 -4.07504 -4.060967 -4.0316725 -3.9565043 -3.8789093 -3.9219143 -4.0203986 -4.0869231 -4.104681 -4.0836406 -4.0644865 -4.0624485][-4.1172643 -4.0655518 -4.0165715 -3.9879911 -3.9373841 -3.8607163 -3.7215486 -3.5877731 -3.6857584 -3.8772798 -4.0061946 -4.06174 -4.0634761 -4.0696392 -4.0941739][-4.1160994 -4.060585 -4.0044003 -3.9546576 -3.8803976 -3.7761605 -3.612268 -3.4663429 -3.5998371 -3.82201 -3.9623582 -4.0241418 -4.0350208 -4.058435 -4.10205][-4.1229224 -4.08011 -4.0375109 -3.9956627 -3.939085 -3.8652058 -3.7635736 -3.6867423 -3.7813542 -3.9214525 -4.0017085 -4.0335617 -4.0339708 -4.0625339 -4.1110926][-4.1356611 -4.1112151 -4.0910263 -4.0692935 -4.0432653 -4.0095725 -3.9629459 -3.9285307 -3.9802675 -4.0456009 -4.074688 -4.081903 -4.0769415 -4.0969906 -4.1299734][-4.1735849 -4.1611934 -4.1537023 -4.14451 -4.1376433 -4.1244583 -4.100461 -4.0806789 -4.0995197 -4.1259007 -4.1333127 -4.1330738 -4.1304893 -4.1358266 -4.14618][-4.2229195 -4.21431 -4.2119074 -4.2109046 -4.2106943 -4.2027984 -4.1884956 -4.174582 -4.1762128 -4.1862926 -4.1897845 -4.1883154 -4.182611 -4.173924 -4.1688476][-4.2689595 -4.2645941 -4.2642903 -4.2670808 -4.2652092 -4.2561336 -4.2457881 -4.2376733 -4.238966 -4.2461309 -4.2488871 -4.2451668 -4.2355561 -4.2180862 -4.2054725][-4.3040786 -4.3039703 -4.3066545 -4.3117461 -4.3096581 -4.3010864 -4.2970295 -4.295434 -4.2988515 -4.30619 -4.3085647 -4.3038058 -4.2941542 -4.2763653 -4.2634206][-4.331079 -4.331912 -4.3366766 -4.3424935 -4.3423867 -4.338727 -4.3386397 -4.3388638 -4.340003 -4.3439708 -4.3455453 -4.3429 -4.3381796 -4.3282094 -4.3184204]]...]
INFO - root - 2017-12-05 20:28:12.394078: step 42010, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 68h:46m:43s remains)
INFO - root - 2017-12-05 20:28:20.873556: step 42020, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 67h:56m:38s remains)
INFO - root - 2017-12-05 20:28:29.591333: step 42030, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 72h:15m:15s remains)
INFO - root - 2017-12-05 20:28:38.085188: step 42040, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 66h:42m:22s remains)
INFO - root - 2017-12-05 20:28:46.555505: step 42050, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.779 sec/batch; 62h:50m:54s remains)
INFO - root - 2017-12-05 20:28:55.212198: step 42060, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 70h:46m:06s remains)
INFO - root - 2017-12-05 20:29:03.859066: step 42070, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.874 sec/batch; 70h:32m:33s remains)
INFO - root - 2017-12-05 20:29:12.417513: step 42080, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 70h:01m:29s remains)
INFO - root - 2017-12-05 20:29:20.994383: step 42090, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 71h:33m:13s remains)
INFO - root - 2017-12-05 20:29:29.651426: step 42100, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 68h:55m:10s remains)
2017-12-05 20:29:30.423565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1443057 -4.1633916 -4.1891556 -4.2100649 -4.2101474 -4.1844597 -4.1501102 -4.1265426 -4.1197863 -4.1264143 -4.1471686 -4.175756 -4.1949196 -4.1894832 -4.1843][-4.1159239 -4.1407447 -4.1777415 -4.2005334 -4.1963916 -4.1727476 -4.1436281 -4.11972 -4.1131945 -4.1241055 -4.1481986 -4.1759777 -4.1908679 -4.1803188 -4.1725664][-4.10439 -4.1292357 -4.16676 -4.1867609 -4.1768546 -4.1579452 -4.1403651 -4.1205778 -4.1137829 -4.1280713 -4.1548643 -4.1783986 -4.190814 -4.1760383 -4.1583538][-4.0982966 -4.1186714 -4.1466374 -4.1656828 -4.158978 -4.1476555 -4.1406484 -4.1234155 -4.1177721 -4.134984 -4.1635857 -4.1861763 -4.1962633 -4.17928 -4.1538568][-4.07899 -4.0959291 -4.1154871 -4.1329217 -4.1293588 -4.1202278 -4.1160822 -4.104898 -4.1094694 -4.1400056 -4.1765633 -4.2025714 -4.2089562 -4.1919346 -4.1647658][-4.072742 -4.0827456 -4.0869436 -4.0903339 -4.0781822 -4.0674648 -4.0644064 -4.0601206 -4.0777755 -4.1273456 -4.17525 -4.2037106 -4.2081 -4.1922669 -4.1701565][-4.0834904 -4.0884581 -4.0769215 -4.0557113 -4.0207868 -3.9936521 -3.9802094 -3.9691672 -3.9922028 -4.0689478 -4.1388206 -4.1789227 -4.181531 -4.1616993 -4.147716][-4.0864406 -4.0933032 -4.081356 -4.0431256 -3.9910457 -3.9486036 -3.9105973 -3.8738751 -3.8927951 -3.9939885 -4.084384 -4.1323433 -4.1309109 -4.1072545 -4.1052785][-4.1039677 -4.1154246 -4.1092935 -4.0720463 -4.0314617 -3.9976573 -3.9603469 -3.9230542 -3.929493 -4.0038629 -4.0709906 -4.099895 -4.0833921 -4.05617 -4.0652933][-4.1291924 -4.148273 -4.1506147 -4.1249256 -4.0992656 -4.0811834 -4.0602579 -4.0423145 -4.041934 -4.0797558 -4.1125555 -4.1167212 -4.0920372 -4.0623646 -4.0679121][-4.1468859 -4.1716757 -4.1792297 -4.1649647 -4.1494694 -4.1423016 -4.1300716 -4.119833 -4.115737 -4.1346831 -4.1517611 -4.15061 -4.1323304 -4.10855 -4.1083813][-4.1365485 -4.1624513 -4.1777096 -4.1765079 -4.1726112 -4.1720552 -4.1670885 -4.1657782 -4.1625967 -4.1692767 -4.1764421 -4.1732869 -4.1609783 -4.145268 -4.147459][-4.1108127 -4.1375127 -4.1600952 -4.1697121 -4.1756105 -4.1821985 -4.1853247 -4.1902843 -4.1907287 -4.1919384 -4.1912932 -4.1882129 -4.1835408 -4.1756344 -4.1793089][-4.1086559 -4.1290851 -4.1518364 -4.1662874 -4.17725 -4.1874733 -4.1947932 -4.2008505 -4.20495 -4.2052441 -4.201292 -4.2023549 -4.2044697 -4.1996417 -4.1979923][-4.121007 -4.1359711 -4.1561089 -4.1715379 -4.1836715 -4.1929932 -4.19901 -4.2036786 -4.2078695 -4.2065659 -4.2042732 -4.2096853 -4.2162757 -4.2144904 -4.2110019]]...]
INFO - root - 2017-12-05 20:29:38.884919: step 42110, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 69h:35m:18s remains)
INFO - root - 2017-12-05 20:29:47.335834: step 42120, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 68h:41m:30s remains)
INFO - root - 2017-12-05 20:29:55.889587: step 42130, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 70h:18m:54s remains)
INFO - root - 2017-12-05 20:30:04.345358: step 42140, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 69h:06m:26s remains)
INFO - root - 2017-12-05 20:30:12.906598: step 42150, loss = 2.04, batch loss = 1.99 (10.4 examples/sec; 0.767 sec/batch; 61h:52m:10s remains)
INFO - root - 2017-12-05 20:30:21.552752: step 42160, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 68h:27m:00s remains)
INFO - root - 2017-12-05 20:30:30.102611: step 42170, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 68h:32m:08s remains)
INFO - root - 2017-12-05 20:30:38.657608: step 42180, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 67h:55m:34s remains)
INFO - root - 2017-12-05 20:30:47.230705: step 42190, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 71h:08m:05s remains)
INFO - root - 2017-12-05 20:30:55.613815: step 42200, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 67h:04m:39s remains)
2017-12-05 20:30:56.381047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3046341 -4.30715 -4.3093791 -4.3109193 -4.3113265 -4.3096061 -4.3046055 -4.3028088 -4.306489 -4.3102527 -4.3163128 -4.3211074 -4.323771 -4.3264432 -4.3299518][-4.2969084 -4.2998033 -4.3041029 -4.309485 -4.3118148 -4.3082776 -4.2969589 -4.2935586 -4.3032541 -4.3113146 -4.3200531 -4.3245707 -4.3239112 -4.3230681 -4.325757][-4.2780628 -4.2785807 -4.283534 -4.2915597 -4.2956567 -4.2905951 -4.2710142 -4.2668805 -4.2858334 -4.2992821 -4.3091903 -4.3129187 -4.3082714 -4.3058338 -4.3088665][-4.2486706 -4.2420735 -4.2437787 -4.2536478 -4.2568278 -4.2458181 -4.214397 -4.2089462 -4.2390294 -4.2606649 -4.2757244 -4.2839937 -4.2802553 -4.27737 -4.2820354][-4.2126794 -4.1928906 -4.1851993 -4.1918154 -4.1903429 -4.1700826 -4.1281447 -4.1210332 -4.1644135 -4.2020712 -4.2321463 -4.2515922 -4.2556095 -4.2524185 -4.2597737][-4.1807141 -4.1484861 -4.1285586 -4.1247659 -4.1124454 -4.0799375 -4.0282412 -4.0210824 -4.0775714 -4.1322494 -4.1797843 -4.2114658 -4.2246685 -4.2236595 -4.237617][-4.1413851 -4.0975056 -4.0649858 -4.0425844 -4.0081716 -3.9611964 -3.902359 -3.8970482 -3.9656947 -4.033504 -4.0943689 -4.134829 -4.1568546 -4.1623044 -4.1888657][-4.1116085 -4.0515714 -3.9988635 -3.9458125 -3.8759315 -3.8149087 -3.7625964 -3.7737799 -3.8653667 -3.9514859 -4.0255771 -4.0704136 -4.097765 -4.111197 -4.149745][-4.122602 -4.0670638 -4.0170012 -3.9535697 -3.8612106 -3.7866082 -3.73728 -3.7525373 -3.8434753 -3.9224076 -3.9883444 -4.0292735 -4.0584359 -4.0819197 -4.1308241][-4.1428795 -4.1054382 -4.0793066 -4.0399079 -3.9748013 -3.9121056 -3.8595412 -3.8522351 -3.9075074 -3.9577243 -4.0018172 -4.0311828 -4.0533724 -4.0779443 -4.1297574][-4.141705 -4.1100893 -4.0984635 -4.0821986 -4.0484185 -4.0070314 -3.9660118 -3.9530134 -3.9843032 -4.0159569 -4.0450525 -4.06726 -4.0854759 -4.1068273 -4.1509252][-4.1466432 -4.1142006 -4.1034327 -4.0925069 -4.0736027 -4.0457969 -4.0173111 -4.007309 -4.0270848 -4.0504036 -4.0765443 -4.1000752 -4.1199117 -4.1410017 -4.1789327][-4.1874256 -4.1606312 -4.1489086 -4.1385756 -4.1229329 -4.0990629 -4.0758429 -4.0678253 -4.0807152 -4.0984859 -4.1224971 -4.145566 -4.16533 -4.1848078 -4.2147098][-4.2471757 -4.2328296 -4.2266574 -4.2203522 -4.2083063 -4.1888328 -4.1718416 -4.1671453 -4.1732874 -4.1856194 -4.2045979 -4.2238822 -4.2377548 -4.24881 -4.2647538][-4.2897081 -4.28459 -4.2848535 -4.284039 -4.2797656 -4.2722425 -4.2666588 -4.2655268 -4.2673364 -4.2728481 -4.2827792 -4.2929339 -4.2990475 -4.3018312 -4.3063731]]...]
INFO - root - 2017-12-05 20:31:04.908316: step 42210, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 69h:38m:56s remains)
INFO - root - 2017-12-05 20:31:13.575385: step 42220, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 69h:53m:28s remains)
INFO - root - 2017-12-05 20:31:22.053355: step 42230, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 68h:01m:31s remains)
INFO - root - 2017-12-05 20:31:30.503438: step 42240, loss = 2.02, batch loss = 1.96 (9.5 examples/sec; 0.845 sec/batch; 68h:09m:01s remains)
INFO - root - 2017-12-05 20:31:38.900846: step 42250, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.754 sec/batch; 60h:48m:58s remains)
INFO - root - 2017-12-05 20:31:47.460218: step 42260, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 70h:39m:37s remains)
INFO - root - 2017-12-05 20:31:55.993159: step 42270, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 71h:49m:45s remains)
INFO - root - 2017-12-05 20:32:04.403469: step 42280, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 69h:11m:51s remains)
INFO - root - 2017-12-05 20:32:12.779427: step 42290, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.865 sec/batch; 69h:42m:32s remains)
INFO - root - 2017-12-05 20:32:21.290156: step 42300, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 69h:06m:23s remains)
2017-12-05 20:32:22.074978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2518148 -4.2506189 -4.2574525 -4.2707 -4.2787228 -4.2767425 -4.2711573 -4.2639217 -4.2629585 -4.2629204 -4.2666616 -4.2813535 -4.2983618 -4.3061471 -4.3163419][-4.23978 -4.237421 -4.2402649 -4.2488527 -4.2537603 -4.2470508 -4.2362709 -4.2282457 -4.2324829 -4.2390203 -4.2449636 -4.2614021 -4.2805028 -4.2898645 -4.2999425][-4.2265029 -4.229198 -4.2284312 -4.2322497 -4.2328959 -4.2231388 -4.2085414 -4.2020197 -4.2144666 -4.2311358 -4.242949 -4.25786 -4.2738762 -4.2831888 -4.2927632][-4.1965189 -4.2055988 -4.2089272 -4.2122951 -4.2138443 -4.2029324 -4.185359 -4.1764584 -4.1919384 -4.2168994 -4.2356048 -4.2521482 -4.2683105 -4.28171 -4.2963786][-4.170332 -4.1805668 -4.1875963 -4.1921406 -4.1963668 -4.1884274 -4.1739593 -4.1658435 -4.1771045 -4.2015176 -4.2252493 -4.2450824 -4.2643023 -4.2845778 -4.3063273][-4.1668839 -4.1701055 -4.1744852 -4.1743035 -4.1727586 -4.1638036 -4.1536665 -4.1510892 -4.1654439 -4.1939883 -4.2238135 -4.2499676 -4.2740278 -4.297637 -4.3186135][-4.175508 -4.168242 -4.1606135 -4.1450453 -4.1212764 -4.0955706 -4.0835714 -4.0907965 -4.1168184 -4.1603279 -4.2039757 -4.2403331 -4.27304 -4.2984977 -4.3150067][-4.1976252 -4.17519 -4.153399 -4.12568 -4.0863609 -4.0490446 -4.03419 -4.0443373 -4.0766063 -4.1281672 -4.176733 -4.2173305 -4.2549658 -4.2826304 -4.299274][-4.210165 -4.1849756 -4.162437 -4.1440105 -4.1162581 -4.0974259 -4.0955338 -4.1037378 -4.1282549 -4.1695666 -4.2027845 -4.2333293 -4.2630453 -4.2808633 -4.291595][-4.2071891 -4.1901827 -4.1787581 -4.1750836 -4.1703143 -4.1753497 -4.1872482 -4.1899643 -4.2029605 -4.2332592 -4.24926 -4.2658486 -4.2841115 -4.289454 -4.2919645][-4.1958456 -4.194025 -4.1932235 -4.1992483 -4.2077985 -4.2216949 -4.2369585 -4.2355285 -4.2403207 -4.2603354 -4.2694106 -4.2809505 -4.2934976 -4.2921906 -4.2925525][-4.2054543 -4.2159715 -4.22183 -4.2298293 -4.2391953 -4.2526417 -4.2649517 -4.2612262 -4.261837 -4.2747874 -4.2805629 -4.2908416 -4.30071 -4.2945838 -4.2910328][-4.2468677 -4.2601929 -4.2666183 -4.272418 -4.2745557 -4.279346 -4.2847018 -4.2786393 -4.276423 -4.2856321 -4.2916918 -4.3018789 -4.3101983 -4.303298 -4.2967467][-4.291141 -4.2999797 -4.3011904 -4.3007345 -4.2963867 -4.291173 -4.2877207 -4.2799745 -4.2772436 -4.2826095 -4.2905984 -4.3030124 -4.3123174 -4.309947 -4.3038554][-4.3202085 -4.3221469 -4.3163905 -4.3086061 -4.3003459 -4.2935705 -4.288414 -4.2827096 -4.2824817 -4.2857842 -4.2916427 -4.2999234 -4.3082271 -4.3103352 -4.306931]]...]
INFO - root - 2017-12-05 20:32:30.479786: step 42310, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 67h:52m:20s remains)
INFO - root - 2017-12-05 20:32:38.912867: step 42320, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.829 sec/batch; 66h:50m:09s remains)
INFO - root - 2017-12-05 20:32:47.376988: step 42330, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 67h:36m:42s remains)
INFO - root - 2017-12-05 20:32:55.981225: step 42340, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 69h:21m:45s remains)
INFO - root - 2017-12-05 20:33:04.465214: step 42350, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 69h:42m:52s remains)
INFO - root - 2017-12-05 20:33:12.855665: step 42360, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 69h:11m:37s remains)
INFO - root - 2017-12-05 20:33:21.427656: step 42370, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 66h:53m:50s remains)
INFO - root - 2017-12-05 20:33:30.011835: step 42380, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 68h:45m:59s remains)
INFO - root - 2017-12-05 20:33:38.406835: step 42390, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 66h:58m:56s remains)
INFO - root - 2017-12-05 20:33:46.860436: step 42400, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 67h:39m:28s remains)
2017-12-05 20:33:47.680764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2884917 -4.2946677 -4.2979374 -4.2993979 -4.3017368 -4.305943 -4.3121443 -4.3154984 -4.3167152 -4.3171515 -4.3178344 -4.3182487 -4.31876 -4.3201675 -4.3208146][-4.2473893 -4.2491937 -4.2512174 -4.2540531 -4.2593122 -4.2683005 -4.2789569 -4.2868214 -4.2891173 -4.2895169 -4.292738 -4.2989712 -4.3040891 -4.3083248 -4.3108134][-4.2062392 -4.2035656 -4.2033982 -4.2033548 -4.2057233 -4.2123303 -4.2227039 -4.2348108 -4.2433372 -4.2468567 -4.2548637 -4.2690158 -4.2799315 -4.28781 -4.2924504][-4.1804752 -4.1700273 -4.1612539 -4.1504035 -4.1432867 -4.1431766 -4.1532488 -4.1745834 -4.1946611 -4.2085142 -4.2263651 -4.2496114 -4.2630572 -4.2721963 -4.27658][-4.1613026 -4.1389422 -4.1177535 -4.0925741 -4.07607 -4.072402 -4.0879769 -4.118844 -4.1482334 -4.1714749 -4.1978054 -4.2297935 -4.246882 -4.2561316 -4.2611818][-4.1409297 -4.1061268 -4.0723653 -4.0322642 -4.00734 -4.0033278 -4.0153923 -4.0439906 -4.0773873 -4.1103172 -4.1495743 -4.1981225 -4.2272849 -4.2434816 -4.2522693][-4.1138563 -4.0701408 -4.0251713 -3.9756353 -3.949192 -3.9483955 -3.9567349 -3.9852424 -4.0244861 -4.0645175 -4.1155586 -4.1777315 -4.215683 -4.2367587 -4.2487159][-4.0921855 -4.0390134 -3.9866312 -3.9305089 -3.9025149 -3.9068012 -3.9126391 -3.9404612 -3.9837961 -4.024272 -4.0818152 -4.154151 -4.1976867 -4.2222686 -4.2372646][-4.08489 -4.0369077 -3.9874279 -3.9317484 -3.8992095 -3.8987484 -3.8963349 -3.913547 -3.9488533 -3.9819691 -4.0418491 -4.1197147 -4.1672039 -4.1976066 -4.2194362][-4.1025944 -4.0680366 -4.0341091 -3.9901226 -3.9607921 -3.9561338 -3.9508054 -3.9610007 -3.9855537 -4.0114951 -4.0633888 -4.1297126 -4.1692691 -4.1972 -4.2173281][-4.1383591 -4.106657 -4.0790963 -4.0444436 -4.0245228 -4.0219865 -4.0217896 -4.0317903 -4.0509305 -4.0750566 -4.1218071 -4.1741796 -4.2046847 -4.2263 -4.2386489][-4.18649 -4.1522803 -4.1295991 -4.1071191 -4.0970788 -4.1009274 -4.1095414 -4.1233125 -4.1407905 -4.1629472 -4.1986041 -4.2323356 -4.2496114 -4.2618213 -4.2674212][-4.2399249 -4.2097478 -4.1949644 -4.1861687 -4.1837234 -4.1921697 -4.207499 -4.2217321 -4.2368155 -4.253984 -4.276103 -4.2923222 -4.2977829 -4.3020091 -4.3039889][-4.2905955 -4.2724447 -4.26504 -4.2616024 -4.2631083 -4.2734394 -4.2882428 -4.2978644 -4.3052793 -4.3150787 -4.3257542 -4.3323708 -4.3341165 -4.3349895 -4.3352046][-4.3220086 -4.3167162 -4.3148513 -4.313189 -4.3129506 -4.3175373 -4.3240771 -4.3279262 -4.3305092 -4.3351169 -4.3404579 -4.3447833 -4.3465533 -4.347178 -4.3467493]]...]
INFO - root - 2017-12-05 20:33:56.155491: step 42410, loss = 2.05, batch loss = 1.99 (10.4 examples/sec; 0.771 sec/batch; 62h:07m:19s remains)
INFO - root - 2017-12-05 20:34:04.695297: step 42420, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.849 sec/batch; 68h:25m:56s remains)
INFO - root - 2017-12-05 20:34:13.259202: step 42430, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 68h:39m:19s remains)
INFO - root - 2017-12-05 20:34:21.756257: step 42440, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.834 sec/batch; 67h:11m:51s remains)
INFO - root - 2017-12-05 20:34:30.238845: step 42450, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 67h:09m:50s remains)
INFO - root - 2017-12-05 20:34:38.510653: step 42460, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 67h:48m:12s remains)
INFO - root - 2017-12-05 20:34:47.041526: step 42470, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 68h:18m:07s remains)
INFO - root - 2017-12-05 20:34:55.414941: step 42480, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 66h:23m:42s remains)
INFO - root - 2017-12-05 20:35:03.845908: step 42490, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 68h:17m:24s remains)
INFO - root - 2017-12-05 20:35:12.406137: step 42500, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 66h:55m:58s remains)
2017-12-05 20:35:13.133133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1659417 -4.1666923 -4.1655979 -4.1571212 -4.1492648 -4.141952 -4.1388612 -4.1427479 -4.1425138 -4.131619 -4.1209917 -4.1213708 -4.1294231 -4.1407003 -4.1567268][-4.1558075 -4.1556544 -4.152256 -4.1409883 -4.1344905 -4.1314936 -4.1354222 -4.1471119 -4.15131 -4.1397133 -4.1231279 -4.1129975 -4.109622 -4.11391 -4.1285014][-4.1603942 -4.15643 -4.1508274 -4.1426773 -4.1416368 -4.1446075 -4.152998 -4.1668563 -4.1704926 -4.159585 -4.1426272 -4.1291313 -4.1177711 -4.1091642 -4.1140447][-4.1728477 -4.1654944 -4.15911 -4.1561208 -4.1611953 -4.1675549 -4.179708 -4.1957116 -4.1974363 -4.184587 -4.1666141 -4.1531029 -4.1377897 -4.1183138 -4.1106358][-4.1808925 -4.1711674 -4.1625347 -4.1597333 -4.1633615 -4.1689506 -4.1824756 -4.2028146 -4.2102089 -4.1991243 -4.17732 -4.16199 -4.1468325 -4.1248469 -4.1107178][-4.1822805 -4.16798 -4.1552105 -4.1481028 -4.1462717 -4.147006 -4.157505 -4.1780634 -4.1877251 -4.1766486 -4.1516833 -4.1380496 -4.1319785 -4.1218495 -4.1125159][-4.1903591 -4.1708131 -4.151782 -4.140995 -4.1363554 -4.13129 -4.1310134 -4.14257 -4.1460352 -4.1330466 -4.1100206 -4.1035748 -4.1141138 -4.125257 -4.1300817][-4.211453 -4.1960621 -4.1781631 -4.1686573 -4.1634893 -4.1535192 -4.1414 -4.1378722 -4.1317344 -4.1169453 -4.0998287 -4.1017594 -4.1231585 -4.1468487 -4.1600237][-4.23992 -4.2339473 -4.2235012 -4.2162123 -4.208405 -4.197011 -4.1819372 -4.1696906 -4.1562886 -4.1400185 -4.1279778 -4.1348505 -4.1565886 -4.1795859 -4.1904507][-4.2654433 -4.2697864 -4.2645645 -4.2577238 -4.2500386 -4.24288 -4.23168 -4.2187152 -4.203681 -4.18741 -4.1790023 -4.1870742 -4.2025905 -4.2185125 -4.2242408][-4.2875681 -4.298173 -4.2959385 -4.2888703 -4.2790909 -4.2711024 -4.2615633 -4.2511625 -4.2401867 -4.2298989 -4.2277584 -4.2364125 -4.2442975 -4.2507224 -4.2497764][-4.2920885 -4.3052368 -4.3046489 -4.2986093 -4.2888117 -4.2819514 -4.2753339 -4.2689595 -4.2628927 -4.25801 -4.259634 -4.2665238 -4.2675171 -4.2654724 -4.2588439][-4.2782154 -4.2895374 -4.2877216 -4.2836175 -4.2786021 -4.2753749 -4.2717838 -4.2689457 -4.2662935 -4.2654982 -4.270339 -4.2772393 -4.2764559 -4.2695661 -4.2596498][-4.2597003 -4.2672305 -4.2638764 -4.261219 -4.26046 -4.2604427 -4.2576327 -4.2544861 -4.2532048 -4.2546406 -4.2609367 -4.2683849 -4.2714882 -4.2675228 -4.2602735][-4.2488422 -4.2529554 -4.2490497 -4.2462835 -4.2472196 -4.2490463 -4.2478065 -4.2449775 -4.2442784 -4.2455645 -4.2504663 -4.2574778 -4.2626295 -4.2630734 -4.2607164]]...]
INFO - root - 2017-12-05 20:35:21.630547: step 42510, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 69h:04m:30s remains)
INFO - root - 2017-12-05 20:35:30.042877: step 42520, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 69h:57m:40s remains)
INFO - root - 2017-12-05 20:35:38.504973: step 42530, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 66h:31m:49s remains)
INFO - root - 2017-12-05 20:35:47.141448: step 42540, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 70h:17m:38s remains)
INFO - root - 2017-12-05 20:35:55.630838: step 42550, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 68h:39m:02s remains)
INFO - root - 2017-12-05 20:36:04.031546: step 42560, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.825 sec/batch; 66h:26m:46s remains)
INFO - root - 2017-12-05 20:36:12.457176: step 42570, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 71h:17m:13s remains)
INFO - root - 2017-12-05 20:36:20.930273: step 42580, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 68h:19m:03s remains)
INFO - root - 2017-12-05 20:36:29.536247: step 42590, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 70h:19m:45s remains)
INFO - root - 2017-12-05 20:36:38.186513: step 42600, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 70h:16m:00s remains)
2017-12-05 20:36:38.935726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3174195 -4.3001966 -4.2876949 -4.2801843 -4.2731752 -4.2683825 -4.265986 -4.2691922 -4.2725487 -4.2737541 -4.2765436 -4.2739716 -4.2735171 -4.2734828 -4.266644][-4.3041182 -4.28324 -4.2676554 -4.255661 -4.2429929 -4.2332554 -4.2267828 -4.2252641 -4.2237821 -4.2249107 -4.2284727 -4.2243614 -4.2239256 -4.2239561 -4.2188811][-4.2954092 -4.2710347 -4.2505989 -4.235301 -4.2214494 -4.210773 -4.1986346 -4.1871281 -4.1810446 -4.1864519 -4.1960211 -4.1955771 -4.1951761 -4.19645 -4.1967721][-4.2914081 -4.2601771 -4.2339044 -4.2177954 -4.20698 -4.1974244 -4.179523 -4.1609831 -4.1538134 -4.1591644 -4.1756477 -4.183845 -4.1855783 -4.18653 -4.1913905][-4.288908 -4.2498984 -4.2193747 -4.2056003 -4.2002459 -4.191504 -4.1708417 -4.1492653 -4.1375642 -4.1367517 -4.1558633 -4.1712956 -4.1758881 -4.1765776 -4.1833987][-4.28699 -4.2437587 -4.2130132 -4.1992126 -4.1925464 -4.1807489 -4.1588297 -4.1418047 -4.1311169 -4.1300273 -4.1528258 -4.1753654 -4.1842475 -4.1849394 -4.1907511][-4.2901707 -4.2477736 -4.2170715 -4.19946 -4.1857281 -4.1685166 -4.1472478 -4.1375437 -4.1336718 -4.1426225 -4.1722808 -4.1975121 -4.2094908 -4.2130079 -4.2173471][-4.2973304 -4.2602506 -4.2310476 -4.20963 -4.1888218 -4.1685386 -4.1471896 -4.1370959 -4.133388 -4.1441617 -4.1772819 -4.2066264 -4.2230868 -4.2313161 -4.2365265][-4.3045788 -4.273025 -4.2438331 -4.2186785 -4.1917939 -4.1689534 -4.1491456 -4.1389432 -4.1323323 -4.1373272 -4.1708713 -4.2076168 -4.2300029 -4.2424808 -4.2498012][-4.3095126 -4.2811828 -4.252316 -4.2243 -4.194694 -4.1742635 -4.1599579 -4.1485443 -4.1376534 -4.136827 -4.1682954 -4.2068009 -4.2319145 -4.2481685 -4.2596889][-4.3135471 -4.2872696 -4.2583938 -4.2298322 -4.2007937 -4.183074 -4.173696 -4.1623693 -4.1472611 -4.1425304 -4.1683354 -4.2004695 -4.2270136 -4.2480807 -4.263279][-4.3204703 -4.2977109 -4.2708278 -4.2439017 -4.2199039 -4.2072778 -4.2001352 -4.1884708 -4.1735048 -4.1686935 -4.1879826 -4.2141519 -4.23843 -4.2587357 -4.27042][-4.3297572 -4.3111248 -4.2885022 -4.2653809 -4.2461333 -4.2355151 -4.22672 -4.2150464 -4.201931 -4.1961241 -4.2071371 -4.2259669 -4.2467232 -4.2626886 -4.269115][-4.3389411 -4.3244038 -4.3059587 -4.2873158 -4.2736049 -4.2670646 -4.2620316 -4.2527275 -4.2411647 -4.2336874 -4.2363787 -4.2480884 -4.2643609 -4.2760143 -4.2781181][-4.3494525 -4.3405323 -4.3285074 -4.3175378 -4.3108559 -4.309135 -4.3067608 -4.2989883 -4.2895079 -4.2831159 -4.2835975 -4.2919621 -4.3026896 -4.3087912 -4.30748]]...]
INFO - root - 2017-12-05 20:36:47.754303: step 42610, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 68h:08m:37s remains)
INFO - root - 2017-12-05 20:36:56.281524: step 42620, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 67h:10m:36s remains)
INFO - root - 2017-12-05 20:37:04.631424: step 42630, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 69h:24m:23s remains)
INFO - root - 2017-12-05 20:37:13.189068: step 42640, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 69h:59m:18s remains)
INFO - root - 2017-12-05 20:37:21.923917: step 42650, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 68h:36m:23s remains)
INFO - root - 2017-12-05 20:37:30.458022: step 42660, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 68h:28m:06s remains)
INFO - root - 2017-12-05 20:37:38.966269: step 42670, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 68h:40m:35s remains)
INFO - root - 2017-12-05 20:37:47.456777: step 42680, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 71h:11m:58s remains)
INFO - root - 2017-12-05 20:37:55.901764: step 42690, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 69h:27m:56s remains)
INFO - root - 2017-12-05 20:38:04.579622: step 42700, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 71h:55m:29s remains)
2017-12-05 20:38:05.361265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2228785 -4.2138844 -4.1738586 -4.1436787 -4.1421366 -4.1701379 -4.1976881 -4.2111 -4.2145004 -4.2115521 -4.2054405 -4.1954646 -4.1819615 -4.1610279 -4.1588364][-4.2215881 -4.2077537 -4.16336 -4.135222 -4.1401777 -4.1746125 -4.2004695 -4.2122183 -4.2151918 -4.2082996 -4.1944551 -4.1825547 -4.1764793 -4.1685095 -4.1756535][-4.2120686 -4.1963549 -4.1572385 -4.1389122 -4.1450667 -4.1729059 -4.1936717 -4.2039604 -4.2063804 -4.1959176 -4.18203 -4.1755347 -4.1793151 -4.1850014 -4.1922488][-4.20292 -4.1853266 -4.1598349 -4.1529431 -4.1570988 -4.1762671 -4.1865358 -4.1914277 -4.1894135 -4.1743412 -4.16622 -4.1718287 -4.1840835 -4.1931329 -4.1922541][-4.1953268 -4.1708426 -4.1534395 -4.15297 -4.1532841 -4.161674 -4.1587477 -4.1497345 -4.14019 -4.123961 -4.1288986 -4.1546597 -4.1774807 -4.1835093 -4.1743817][-4.18816 -4.1622229 -4.14702 -4.1462345 -4.1402717 -4.1344757 -4.1199389 -4.0975323 -4.08021 -4.0729232 -4.0964961 -4.1406922 -4.1729612 -4.1728506 -4.1571717][-4.1799293 -4.1626363 -4.1537309 -4.1541095 -4.1461554 -4.1327624 -4.1132579 -4.0897908 -4.0816517 -4.0824022 -4.1098847 -4.1501589 -4.1742272 -4.1622405 -4.1430883][-4.1800537 -4.1721015 -4.169199 -4.1716433 -4.16659 -4.1550813 -4.1370573 -4.1227269 -4.1253443 -4.1309981 -4.1477208 -4.16529 -4.1646433 -4.1390867 -4.1188025][-4.1893106 -4.1835122 -4.1817012 -4.1851082 -4.1842566 -4.1778021 -4.1659102 -4.1573615 -4.1634531 -4.168407 -4.1670241 -4.1563497 -4.135838 -4.1071482 -4.0960326][-4.2087646 -4.20406 -4.2037172 -4.2063365 -4.207366 -4.2056251 -4.1989007 -4.1910768 -4.1883521 -4.1817503 -4.1642718 -4.1369033 -4.1099586 -4.0925484 -4.0960779][-4.2251544 -4.2239203 -4.2272611 -4.2311435 -4.23174 -4.2315588 -4.2268782 -4.2158771 -4.2020149 -4.1827841 -4.1551 -4.1260405 -4.1065674 -4.104744 -4.1146445][-4.2315025 -4.232923 -4.2396755 -4.2456474 -4.2473087 -4.2477803 -4.2411408 -4.22414 -4.2006564 -4.1748762 -4.1464167 -4.1202617 -4.1103139 -4.1155796 -4.124897][-4.2368989 -4.2393818 -4.2474985 -4.2535291 -4.2530231 -4.249475 -4.236464 -4.21535 -4.1909246 -4.1677151 -4.1464195 -4.1267686 -4.119566 -4.1251407 -4.1312304][-4.2358751 -4.24239 -4.2516532 -4.2559834 -4.2510324 -4.2421994 -4.2259874 -4.2063146 -4.1891222 -4.1789594 -4.1719332 -4.1621065 -4.1557193 -4.1566606 -4.1548114][-4.2263856 -4.2396889 -4.2540941 -4.2608762 -4.2562351 -4.2455454 -4.2273693 -4.209672 -4.2043862 -4.2070527 -4.2101555 -4.2065659 -4.2003589 -4.196847 -4.188201]]...]
INFO - root - 2017-12-05 20:38:13.906842: step 42710, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 67h:20m:11s remains)
INFO - root - 2017-12-05 20:38:22.334647: step 42720, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 69h:17m:48s remains)
INFO - root - 2017-12-05 20:38:30.800030: step 42730, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.885 sec/batch; 71h:15m:12s remains)
INFO - root - 2017-12-05 20:38:39.175748: step 42740, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 69h:17m:07s remains)
INFO - root - 2017-12-05 20:38:47.760196: step 42750, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 67h:41m:39s remains)
INFO - root - 2017-12-05 20:38:56.361906: step 42760, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 68h:13m:30s remains)
INFO - root - 2017-12-05 20:39:04.886940: step 42770, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 66h:55m:17s remains)
INFO - root - 2017-12-05 20:39:13.570086: step 42780, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 66h:48m:50s remains)
INFO - root - 2017-12-05 20:39:21.982364: step 42790, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 69h:25m:44s remains)
INFO - root - 2017-12-05 20:39:30.493999: step 42800, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 68h:55m:11s remains)
2017-12-05 20:39:31.259485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1710696 -4.1794744 -4.1833777 -4.1839023 -4.1779561 -4.1708522 -4.163415 -4.1658111 -4.1776772 -4.1857328 -4.1874795 -4.1826024 -4.1775751 -4.179131 -4.1901503][-4.1991935 -4.207078 -4.2137666 -4.2152529 -4.2118578 -4.2056127 -4.1905618 -4.1748061 -4.1703353 -4.1675019 -4.1624308 -4.1565914 -4.1570258 -4.1641765 -4.1798549][-4.2246094 -4.2314692 -4.2342463 -4.2332706 -4.2282248 -4.2140689 -4.1855063 -4.1523337 -4.133162 -4.1201935 -4.1120844 -4.1145806 -4.1290793 -4.1434631 -4.1626248][-4.2283463 -4.2367754 -4.2377963 -4.231101 -4.2179151 -4.189301 -4.1451492 -4.0972667 -4.0722423 -4.0666971 -4.0709209 -4.0941825 -4.1269069 -4.1428318 -4.1550627][-4.2185092 -4.2272744 -4.2261696 -4.2117028 -4.1927438 -4.1596961 -4.1139808 -4.0665045 -4.049005 -4.056663 -4.0687084 -4.0985589 -4.1343193 -4.1469374 -4.1526151][-4.2102618 -4.2143521 -4.2107482 -4.1937084 -4.1750174 -4.150754 -4.118659 -4.0855069 -4.0752358 -4.0821095 -4.0908637 -4.1153522 -4.1452851 -4.1520095 -4.1531467][-4.2146158 -4.2076583 -4.1987696 -4.180871 -4.1642094 -4.1538229 -4.1424818 -4.1272974 -4.1210728 -4.122591 -4.1285257 -4.1471987 -4.166677 -4.1668015 -4.16558][-4.2289691 -4.2112322 -4.1963949 -4.1793752 -4.1731205 -4.1776628 -4.18018 -4.1773205 -4.1741409 -4.1705823 -4.1744885 -4.1859756 -4.1953082 -4.1914191 -4.1882453][-4.2403421 -4.2249436 -4.2134266 -4.2038403 -4.2060475 -4.2172151 -4.2240648 -4.2226906 -4.2195621 -4.2173185 -4.2177038 -4.223124 -4.2275038 -4.224215 -4.2178326][-4.2431474 -4.2370834 -4.2312 -4.2271676 -4.2305093 -4.2405162 -4.2509604 -4.2541485 -4.25232 -4.2508845 -4.2512012 -4.2544084 -4.255291 -4.2526073 -4.246244][-4.236989 -4.243113 -4.2427425 -4.2443128 -4.2531748 -4.2683678 -4.2824059 -4.2878237 -4.2829194 -4.2758594 -4.2733889 -4.2753029 -4.2762642 -4.2760067 -4.2721796][-4.2381291 -4.25317 -4.2575779 -4.2659836 -4.2816048 -4.3001494 -4.3128467 -4.3149109 -4.3048167 -4.2922873 -4.2853584 -4.2872872 -4.2901139 -4.2923355 -4.2906847][-4.2524552 -4.2708039 -4.2782621 -4.2883139 -4.3041744 -4.3198881 -4.3272538 -4.3251219 -4.3118572 -4.2960348 -4.2879124 -4.2913337 -4.2984376 -4.3039441 -4.30559][-4.2640905 -4.2817612 -4.2907634 -4.3003979 -4.3129454 -4.3234344 -4.3264866 -4.3232193 -4.3120885 -4.2975707 -4.290503 -4.29568 -4.3061643 -4.3150077 -4.3192039][-4.2723966 -4.2823434 -4.2905717 -4.2997141 -4.3091044 -4.3179212 -4.3218665 -4.3210449 -4.3146319 -4.3040004 -4.29951 -4.3050127 -4.3153639 -4.325305 -4.3302865]]...]
INFO - root - 2017-12-05 20:39:39.849964: step 42810, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 71h:21m:36s remains)
INFO - root - 2017-12-05 20:39:48.441967: step 42820, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 69h:46m:17s remains)
INFO - root - 2017-12-05 20:39:57.194832: step 42830, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 69h:49m:16s remains)
INFO - root - 2017-12-05 20:40:05.610547: step 42840, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 69h:05m:02s remains)
INFO - root - 2017-12-05 20:40:14.131863: step 42850, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 69h:25m:21s remains)
INFO - root - 2017-12-05 20:40:22.788733: step 42860, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 70h:00m:04s remains)
INFO - root - 2017-12-05 20:40:31.195250: step 42870, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 68h:34m:38s remains)
INFO - root - 2017-12-05 20:40:39.727304: step 42880, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 69h:12m:56s remains)
INFO - root - 2017-12-05 20:40:48.264308: step 42890, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 68h:18m:02s remains)
INFO - root - 2017-12-05 20:40:56.703238: step 42900, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 68h:18m:08s remains)
2017-12-05 20:40:57.567334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3120737 -4.2986298 -4.2924638 -4.2996316 -4.3080668 -4.3172116 -4.3272214 -4.3294573 -4.3236418 -4.3160434 -4.3091264 -4.30508 -4.3054037 -4.310317 -4.3161368][-4.2872458 -4.2691326 -4.2598329 -4.2666845 -4.2823758 -4.3005266 -4.3142676 -4.3158679 -4.3070455 -4.2925119 -4.2811275 -4.278914 -4.2847185 -4.2953176 -4.3086805][-4.2600164 -4.2416935 -4.2309532 -4.23786 -4.2586765 -4.2828827 -4.2943392 -4.29034 -4.2762203 -4.2572684 -4.2425585 -4.2403164 -4.2513909 -4.2710228 -4.2941947][-4.2262511 -4.2100725 -4.2057109 -4.21963 -4.2462473 -4.2690225 -4.2680726 -4.2507839 -4.2346354 -4.2201328 -4.2076006 -4.2072964 -4.2218208 -4.2456503 -4.2703104][-4.1843424 -4.1788964 -4.1866179 -4.2089505 -4.2337785 -4.2416239 -4.2189279 -4.1895981 -4.1819715 -4.1814408 -4.1809573 -4.191329 -4.2128272 -4.2357688 -4.2537017][-4.1256585 -4.1284819 -4.1454735 -4.1732802 -4.1895428 -4.1737218 -4.12516 -4.092658 -4.1058884 -4.1279731 -4.1481552 -4.1744838 -4.205729 -4.2304664 -4.2448139][-4.0705881 -4.0732555 -4.0914335 -4.1187835 -4.12322 -4.077343 -3.9956846 -3.9612699 -4.010664 -4.0709467 -4.1167831 -4.1554966 -4.1918759 -4.2197256 -4.2326059][-4.0516882 -4.0428095 -4.0493078 -4.0585046 -4.0440493 -3.9766285 -3.8820143 -3.8585744 -3.9369466 -4.0251245 -4.0873671 -4.1318889 -4.1657519 -4.1926494 -4.2083292][-4.0959959 -4.0742488 -4.0605917 -4.0484753 -4.0288725 -3.984118 -3.9304235 -3.9190273 -3.9698157 -4.0329828 -4.083653 -4.122901 -4.1509819 -4.1771975 -4.2029233][-4.1503277 -4.127851 -4.1123719 -4.1034126 -4.1025305 -4.0955977 -4.0807471 -4.0684443 -4.0767331 -4.0983706 -4.1251984 -4.1528349 -4.1780319 -4.2028565 -4.2282934][-4.1763577 -4.1623554 -4.1597919 -4.1671586 -4.186132 -4.1989808 -4.1989169 -4.1867666 -4.1754789 -4.1713753 -4.1793108 -4.2017465 -4.22543 -4.2472124 -4.2654705][-4.1900725 -4.1887393 -4.2015338 -4.2206578 -4.2453957 -4.2615557 -4.2662854 -4.2561517 -4.2408762 -4.2284646 -4.2298689 -4.2496924 -4.2696061 -4.2870154 -4.2971654][-4.2020264 -4.2092009 -4.229743 -4.253551 -4.27282 -4.2827473 -4.2876387 -4.2826476 -4.2715569 -4.2596035 -4.2598515 -4.2777009 -4.2924371 -4.3033624 -4.3068457][-4.2202272 -4.2307758 -4.2494087 -4.2701263 -4.2816672 -4.284411 -4.2854733 -4.2803478 -4.27099 -4.2621031 -4.2656541 -4.2804971 -4.29004 -4.2953105 -4.2962723][-4.2468452 -4.2528062 -4.2616158 -4.2727981 -4.2797213 -4.2793703 -4.2773232 -4.2715926 -4.2639012 -4.2616777 -4.2689161 -4.2770925 -4.27863 -4.2798543 -4.2809911]]...]
INFO - root - 2017-12-05 20:41:06.063025: step 42910, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 68h:12m:40s remains)
INFO - root - 2017-12-05 20:41:14.661261: step 42920, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 69h:09m:41s remains)
INFO - root - 2017-12-05 20:41:23.171549: step 42930, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 68h:44m:29s remains)
INFO - root - 2017-12-05 20:41:31.665700: step 42940, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 68h:39m:26s remains)
INFO - root - 2017-12-05 20:41:40.055093: step 42950, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 70h:03m:08s remains)
INFO - root - 2017-12-05 20:41:48.588227: step 42960, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 69h:06m:03s remains)
INFO - root - 2017-12-05 20:41:57.216888: step 42970, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 68h:17m:39s remains)
INFO - root - 2017-12-05 20:42:05.731437: step 42980, loss = 2.03, batch loss = 1.98 (9.7 examples/sec; 0.829 sec/batch; 66h:39m:59s remains)
INFO - root - 2017-12-05 20:42:14.382156: step 42990, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 70h:47m:35s remains)
INFO - root - 2017-12-05 20:42:23.046879: step 43000, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 66h:56m:38s remains)
2017-12-05 20:42:23.828273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3159285 -4.3078079 -4.2965059 -4.2823486 -4.2751 -4.2773838 -4.2867942 -4.296247 -4.2954612 -4.2892866 -4.2774181 -4.2601471 -4.2366142 -4.2171369 -4.2209907][-4.3053021 -4.2938852 -4.2780571 -4.2593055 -4.2460475 -4.242425 -4.2499194 -4.2639179 -4.26646 -4.25964 -4.24078 -4.212007 -4.175858 -4.1474161 -4.1530986][-4.29438 -4.2816539 -4.2643118 -4.2429709 -4.2231407 -4.212173 -4.2148423 -4.2301168 -4.2370272 -4.2305098 -4.2095103 -4.17282 -4.1269097 -4.0932794 -4.1038218][-4.2715607 -4.2618566 -4.2529063 -4.2361817 -4.21308 -4.1955118 -4.189857 -4.1976857 -4.2041206 -4.2006254 -4.1859989 -4.1526031 -4.1084065 -4.0798469 -4.0992827][-4.2487335 -4.2457089 -4.2408872 -4.2243333 -4.1976743 -4.1725526 -4.1529202 -4.1451864 -4.1526856 -4.1631384 -4.1652288 -4.1457734 -4.1138282 -4.10008 -4.1330872][-4.2332211 -4.2329206 -4.2243347 -4.2022285 -4.16847 -4.1304264 -4.0883121 -4.0602093 -4.07377 -4.1100039 -4.1348906 -4.1366835 -4.1282015 -4.1339788 -4.1769094][-4.2187247 -4.2186413 -4.2081914 -4.1843796 -4.1448956 -4.0890441 -4.0095124 -3.9445913 -3.9564073 -4.0240846 -4.0803185 -4.1101446 -4.1291647 -4.1579533 -4.2097578][-4.2038112 -4.2048006 -4.1958971 -4.1750684 -4.1359787 -4.0659561 -3.9505544 -3.83674 -3.8355105 -3.93052 -4.0192227 -4.0789671 -4.1239972 -4.1718178 -4.228651][-4.2048478 -4.2030349 -4.1947355 -4.1791172 -4.1486692 -4.0811539 -3.9675896 -3.8523715 -3.8436017 -3.9285941 -4.0163159 -4.0832314 -4.1393285 -4.1920533 -4.2436261][-4.2113948 -4.2022567 -4.1917057 -4.1793437 -4.1598768 -4.1146646 -4.0381317 -3.9666984 -3.9634101 -4.0131917 -4.0727715 -4.1240945 -4.1715136 -4.2163873 -4.2598367][-4.2035532 -4.1900597 -4.180007 -4.1731119 -4.162612 -4.1390553 -4.0963473 -4.0682817 -4.0789227 -4.11086 -4.1477551 -4.1813293 -4.2154164 -4.2491059 -4.2825513][-4.1814661 -4.1687984 -4.1639891 -4.1657362 -4.1637568 -4.1559358 -4.1390409 -4.13814 -4.1587954 -4.18565 -4.2131276 -4.2373953 -4.2611618 -4.2836466 -4.306][-4.1582575 -4.148355 -4.1480393 -4.1565762 -4.1628585 -4.1691418 -4.1711345 -4.1838183 -4.207346 -4.2339664 -4.2581887 -4.2769008 -4.2939072 -4.3088675 -4.3233352][-4.1519742 -4.1452127 -4.1448836 -4.1566958 -4.1733003 -4.1910758 -4.2063551 -4.225626 -4.2473912 -4.270143 -4.2898226 -4.30455 -4.3162456 -4.3257346 -4.33524][-4.1815319 -4.1801181 -4.1828237 -4.195487 -4.2134628 -4.2331376 -4.2513509 -4.2689528 -4.2840095 -4.299294 -4.3128071 -4.3226991 -4.3302679 -4.33624 -4.3424172]]...]
INFO - root - 2017-12-05 20:42:32.239993: step 43010, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 68h:36m:14s remains)
INFO - root - 2017-12-05 20:42:40.801469: step 43020, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 70h:21m:48s remains)
INFO - root - 2017-12-05 20:42:49.435371: step 43030, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 68h:06m:29s remains)
INFO - root - 2017-12-05 20:42:57.951052: step 43040, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 70h:34m:14s remains)
INFO - root - 2017-12-05 20:43:06.579437: step 43050, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 67h:08m:31s remains)
INFO - root - 2017-12-05 20:43:15.106946: step 43060, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.844 sec/batch; 67h:49m:31s remains)
INFO - root - 2017-12-05 20:43:23.619725: step 43070, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 68h:51m:45s remains)
INFO - root - 2017-12-05 20:43:32.222427: step 43080, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 68h:55m:46s remains)
INFO - root - 2017-12-05 20:43:40.836149: step 43090, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 68h:38m:26s remains)
INFO - root - 2017-12-05 20:43:49.419444: step 43100, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 69h:02m:02s remains)
2017-12-05 20:43:50.189328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2074118 -4.1788778 -4.1472535 -4.1272473 -4.12951 -4.1380172 -4.1499453 -4.1669965 -4.1905437 -4.2097149 -4.2216039 -4.222971 -4.2164445 -4.210876 -4.2135315][-4.1889124 -4.1622925 -4.1321774 -4.1088538 -4.1086082 -4.1210947 -4.1421418 -4.1673322 -4.1923018 -4.2075262 -4.2151871 -4.2139058 -4.2072859 -4.206388 -4.2114105][-4.1913843 -4.17192 -4.1484966 -4.1283655 -4.1302428 -4.145802 -4.1699219 -4.1945381 -4.2119608 -4.219183 -4.2211308 -4.215694 -4.2097778 -4.2131305 -4.2173653][-4.2020931 -4.190887 -4.1759534 -4.1628995 -4.1690521 -4.1830354 -4.1997504 -4.2146535 -4.222363 -4.2252326 -4.2255177 -4.2165141 -4.2104492 -4.2149563 -4.2162294][-4.2089424 -4.1989989 -4.1855736 -4.1790032 -4.1865468 -4.1918049 -4.1877394 -4.1850004 -4.1882882 -4.1945758 -4.1973495 -4.1896939 -4.1874695 -4.1935577 -4.1945844][-4.206152 -4.1890626 -4.1677279 -4.1581807 -4.1557655 -4.142561 -4.1114388 -4.0957837 -4.1101494 -4.1362824 -4.154356 -4.1504269 -4.1480784 -4.1490846 -4.1478577][-4.1787763 -4.1518679 -4.1207857 -4.1002531 -4.0803723 -4.0408916 -3.9812088 -3.9560137 -3.9950411 -4.0544429 -4.0954456 -4.0990639 -4.0918546 -4.081533 -4.0743513][-4.1388378 -4.1049261 -4.0713363 -4.042479 -4.0099673 -3.9519191 -3.870132 -3.8353028 -3.8961957 -3.9766748 -4.0292492 -4.0359406 -4.0277948 -4.0185561 -4.0152116][-4.1091514 -4.074029 -4.0425282 -4.0115991 -3.9791477 -3.925545 -3.8535755 -3.8247733 -3.880645 -3.9472969 -3.988219 -3.9921455 -3.9852371 -3.9836278 -3.9925389][-4.1086063 -4.0783596 -4.0535245 -4.0294051 -4.0093446 -3.9807692 -3.9432328 -3.9333315 -3.9662232 -3.9969652 -4.0109692 -4.0067573 -3.9952474 -3.9910712 -4.0049372][-4.1600094 -4.1388936 -4.1233993 -4.1084819 -4.102952 -4.096282 -4.0864291 -4.0911365 -4.1070046 -4.1107726 -4.1035271 -4.0903583 -4.0722508 -4.0612187 -4.0694866][-4.2349029 -4.2228713 -4.2132454 -4.2034678 -4.204031 -4.2088294 -4.2131228 -4.2228422 -4.230618 -4.2257605 -4.2123294 -4.1961355 -4.178196 -4.1672182 -4.1707282][-4.2948966 -4.2890458 -4.2834826 -4.2758093 -4.2751579 -4.280149 -4.2851725 -4.2925544 -4.2957249 -4.2903433 -4.2788196 -4.2663589 -4.2565904 -4.2528152 -4.2577929][-4.3318133 -4.3285751 -4.3244734 -4.31887 -4.3167448 -4.3186922 -4.3207884 -4.3230333 -4.3213577 -4.3154669 -4.3070507 -4.3004069 -4.2977209 -4.2991252 -4.3065605][-4.3494396 -4.3472548 -4.3439355 -4.3405027 -4.338758 -4.3388677 -4.3385468 -4.3376255 -4.3344736 -4.3300991 -4.3256454 -4.3232341 -4.3237586 -4.3264418 -4.3307786]]...]
INFO - root - 2017-12-05 20:43:58.681586: step 43110, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 69h:41m:50s remains)
INFO - root - 2017-12-05 20:44:07.167819: step 43120, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 68h:34m:16s remains)
INFO - root - 2017-12-05 20:44:15.691074: step 43130, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 69h:20m:06s remains)
INFO - root - 2017-12-05 20:44:24.204014: step 43140, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 67h:16m:40s remains)
INFO - root - 2017-12-05 20:44:32.750998: step 43150, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 69h:02m:00s remains)
INFO - root - 2017-12-05 20:44:41.108340: step 43160, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 68h:31m:42s remains)
INFO - root - 2017-12-05 20:44:49.657229: step 43170, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 68h:38m:10s remains)
INFO - root - 2017-12-05 20:44:58.202320: step 43180, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 68h:16m:37s remains)
INFO - root - 2017-12-05 20:45:06.811721: step 43190, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 71h:13m:59s remains)
INFO - root - 2017-12-05 20:45:15.408361: step 43200, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 69h:34m:03s remains)
2017-12-05 20:45:16.196817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1458197 -4.1252065 -4.1157541 -4.1171217 -4.1236753 -4.1340995 -4.1497655 -4.1715512 -4.1895523 -4.1822462 -4.1644163 -4.1497126 -4.140873 -4.1449814 -4.1445179][-4.1400762 -4.1229043 -4.1207361 -4.1302977 -4.145649 -4.1582575 -4.1674676 -4.1753869 -4.1821547 -4.1733575 -4.1540179 -4.1387033 -4.1292706 -4.13414 -4.1430597][-4.1447678 -4.13743 -4.1400814 -4.1474967 -4.1582808 -4.160852 -4.1567059 -4.1495142 -4.1530666 -4.1511064 -4.1389408 -4.1288128 -4.1168652 -4.113914 -4.1254511][-4.1542377 -4.1598115 -4.1646748 -4.1652589 -4.1632586 -4.1468954 -4.1252208 -4.1031842 -4.1059136 -4.1121612 -4.11491 -4.1214285 -4.1173511 -4.1098008 -4.1204553][-4.156899 -4.1766109 -4.1879377 -4.1834717 -4.1665225 -4.1276007 -4.0853553 -4.0513444 -4.0535283 -4.068603 -4.0915089 -4.1221128 -4.1331134 -4.1304049 -4.1398172][-4.1597772 -4.1947327 -4.2166533 -4.2126365 -4.18001 -4.1174817 -4.0541396 -4.0091658 -4.0100274 -4.0329714 -4.0768595 -4.1311417 -4.1579313 -4.1647272 -4.1731997][-4.1681232 -4.21503 -4.2423229 -4.2398639 -4.1961989 -4.116632 -4.0358362 -3.9802969 -3.9765189 -4.0035124 -4.0649948 -4.1365981 -4.1768045 -4.1941586 -4.2047467][-4.1778679 -4.2282591 -4.2579165 -4.2550898 -4.2026439 -4.1141591 -4.0272503 -3.9685171 -3.9625876 -3.9927955 -4.06347 -4.1401396 -4.1877804 -4.2131476 -4.2256675][-4.1922784 -4.2400203 -4.2684131 -4.2614279 -4.2021294 -4.1128716 -4.0295672 -3.9783454 -3.9766567 -4.0104666 -4.0808196 -4.1526313 -4.2000108 -4.2281 -4.2396307][-4.209805 -4.2499623 -4.2742171 -4.2648711 -4.2093997 -4.131094 -4.0589991 -4.0180902 -4.019598 -4.0523567 -4.1152048 -4.1767492 -4.2177691 -4.2422748 -4.2506876][-4.2331119 -4.2624083 -4.2804451 -4.2730727 -4.2308936 -4.1720963 -4.117588 -4.0884595 -4.0903654 -4.1156387 -4.16336 -4.2107096 -4.2431774 -4.26277 -4.2700996][-4.2614932 -4.2784619 -4.2876725 -4.2824364 -4.254746 -4.2164855 -4.1819077 -4.1649532 -4.1658182 -4.1802287 -4.2085071 -4.2404542 -4.2639093 -4.2784548 -4.2847886][-4.284142 -4.2898779 -4.2913847 -4.2875271 -4.2723064 -4.2526722 -4.237246 -4.2321849 -4.2338414 -4.2396517 -4.250495 -4.2666159 -4.278564 -4.2859797 -4.2887187][-4.2960324 -4.2939873 -4.289062 -4.284431 -4.2765679 -4.26971 -4.2677112 -4.2712879 -4.2772484 -4.2821546 -4.2849045 -4.2904305 -4.2924743 -4.2917733 -4.2891979][-4.3003526 -4.2925081 -4.2795315 -4.27005 -4.26354 -4.2640152 -4.2693272 -4.2780414 -4.289403 -4.2973342 -4.2988639 -4.3022881 -4.3009109 -4.29604 -4.289135]]...]
INFO - root - 2017-12-05 20:45:24.762990: step 43210, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 68h:30m:37s remains)
INFO - root - 2017-12-05 20:45:33.220045: step 43220, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.834 sec/batch; 67h:01m:48s remains)
INFO - root - 2017-12-05 20:45:41.609310: step 43230, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 70h:32m:51s remains)
INFO - root - 2017-12-05 20:45:50.091655: step 43240, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.877 sec/batch; 70h:30m:09s remains)
INFO - root - 2017-12-05 20:45:58.654608: step 43250, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 67h:37m:49s remains)
INFO - root - 2017-12-05 20:46:07.032533: step 43260, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 66h:30m:18s remains)
INFO - root - 2017-12-05 20:46:15.595683: step 43270, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 68h:31m:28s remains)
INFO - root - 2017-12-05 20:46:24.095671: step 43280, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 67h:50m:00s remains)
INFO - root - 2017-12-05 20:46:32.657882: step 43290, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 68h:42m:39s remains)
INFO - root - 2017-12-05 20:46:41.234528: step 43300, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 69h:08m:25s remains)
2017-12-05 20:46:41.978798: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21358 -4.2238894 -4.2297578 -4.2351518 -4.233098 -4.2294722 -4.2322416 -4.240015 -4.23895 -4.23476 -4.2290988 -4.2125592 -4.1877351 -4.1841774 -4.2143865][-4.2105808 -4.2175 -4.2238431 -4.224371 -4.2175817 -4.2163749 -4.2282267 -4.2474794 -4.2525868 -4.2470517 -4.2384868 -4.2238135 -4.1994791 -4.1943154 -4.22191][-4.2134571 -4.2131152 -4.2140493 -4.2080975 -4.1984468 -4.19652 -4.2117686 -4.2403603 -4.2586584 -4.2601891 -4.2582707 -4.2541895 -4.2394576 -4.2295074 -4.2435584][-4.2246981 -4.2179341 -4.2111354 -4.2021327 -4.19073 -4.178648 -4.1829581 -4.2115388 -4.2426872 -4.2570295 -4.2673597 -4.2768421 -4.2716932 -4.2527246 -4.2474465][-4.2389407 -4.2338576 -4.2211246 -4.203023 -4.182878 -4.1523952 -4.1314678 -4.1487274 -4.19385 -4.2290893 -4.2543221 -4.2758989 -4.2788076 -4.2572055 -4.2358308][-4.2528133 -4.2537031 -4.2377067 -4.2074418 -4.1740575 -4.121428 -4.0640073 -4.0523071 -4.1113133 -4.1805429 -4.2317886 -4.26567 -4.2757664 -4.2563853 -4.2251711][-4.2575231 -4.2676883 -4.2506695 -4.211904 -4.1632419 -4.0888119 -3.9909995 -3.9367046 -4.0068274 -4.121264 -4.2050319 -4.2537494 -4.2708063 -4.2557492 -4.2228904][-4.2520695 -4.2733593 -4.2620373 -4.2242475 -4.1722493 -4.0928307 -3.9731092 -3.8788662 -3.9435096 -4.0855722 -4.1912222 -4.2478161 -4.2713203 -4.2643089 -4.236167][-4.2448339 -4.2732019 -4.2742014 -4.2501144 -4.2125616 -4.1532841 -4.0557108 -3.9677844 -4.0000019 -4.1141529 -4.2087884 -4.262291 -4.2856326 -4.2808871 -4.252192][-4.23975 -4.2678084 -4.2794938 -4.2751174 -4.2580848 -4.2250314 -4.1630464 -4.1036181 -4.1094913 -4.1737905 -4.2376628 -4.2762914 -4.2924137 -4.2854624 -4.2544174][-4.2317638 -4.2550344 -4.2707157 -4.2796249 -4.2777715 -4.2631111 -4.2287216 -4.1942806 -4.1863303 -4.2085457 -4.2379656 -4.2583342 -4.2643547 -4.2539296 -4.2262659][-4.2045546 -4.2184243 -4.2312117 -4.2434597 -4.2485766 -4.2428503 -4.2251153 -4.208364 -4.1979213 -4.1929369 -4.1912575 -4.1904745 -4.1886239 -4.1829886 -4.1692386][-4.1713467 -4.1721969 -4.1756182 -4.1817217 -4.185257 -4.1793947 -4.1696854 -4.1670928 -4.1620193 -4.1480389 -4.1263742 -4.105299 -4.09691 -4.1001835 -4.1032677][-4.1526513 -4.1412783 -4.1337829 -4.1301126 -4.1279755 -4.123477 -4.1220083 -4.1310883 -4.133914 -4.1229711 -4.0952258 -4.0629978 -4.0512762 -4.0602202 -4.0742106][-4.1652026 -4.14915 -4.1349325 -4.123786 -4.1150866 -4.1098952 -4.1133685 -4.1293516 -4.139153 -4.1356797 -4.1157141 -4.0885792 -4.0818367 -4.0948925 -4.1128664]]...]
INFO - root - 2017-12-05 20:46:50.596856: step 43310, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.887 sec/batch; 71h:17m:27s remains)
INFO - root - 2017-12-05 20:46:59.222644: step 43320, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 65h:39m:34s remains)
INFO - root - 2017-12-05 20:47:07.722395: step 43330, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 67h:00m:27s remains)
INFO - root - 2017-12-05 20:47:16.216634: step 43340, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 68h:19m:23s remains)
INFO - root - 2017-12-05 20:47:24.720621: step 43350, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 66h:58m:10s remains)
INFO - root - 2017-12-05 20:47:33.208061: step 43360, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.871 sec/batch; 69h:55m:59s remains)
INFO - root - 2017-12-05 20:47:41.814472: step 43370, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.873 sec/batch; 70h:07m:48s remains)
INFO - root - 2017-12-05 20:47:50.179102: step 43380, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 69h:52m:19s remains)
INFO - root - 2017-12-05 20:47:58.741296: step 43390, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 70h:35m:13s remains)
INFO - root - 2017-12-05 20:48:07.422742: step 43400, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 69h:18m:14s remains)
2017-12-05 20:48:08.142437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1544852 -4.1557312 -4.1714745 -4.1965923 -4.2342825 -4.2697349 -4.2950239 -4.3118725 -4.31463 -4.3053451 -4.2960496 -4.290102 -4.2852788 -4.2744894 -4.2526274][-4.1481981 -4.1408424 -4.1519451 -4.176991 -4.2197285 -4.2621679 -4.2904835 -4.3104663 -4.3147879 -4.3062167 -4.2959361 -4.288301 -4.28278 -4.2757578 -4.2589741][-4.1509218 -4.1301656 -4.1301289 -4.15142 -4.1964741 -4.2458167 -4.283411 -4.3101821 -4.3171153 -4.3103271 -4.3011527 -4.2944846 -4.2907653 -4.2875195 -4.2744288][-4.1628504 -4.1263819 -4.1113625 -4.1223841 -4.163919 -4.2194629 -4.2705956 -4.3060956 -4.3169827 -4.314652 -4.3080897 -4.3021131 -4.2986794 -4.2974062 -4.2872829][-4.1731453 -4.1223788 -4.0910363 -4.086729 -4.1209011 -4.1830711 -4.24729 -4.2917252 -4.3077564 -4.3118114 -4.3090167 -4.3047714 -4.3009977 -4.2982917 -4.2900791][-4.1707945 -4.111887 -4.0636339 -4.0413179 -4.0682988 -4.13956 -4.2164335 -4.2673416 -4.2897034 -4.3009396 -4.3027973 -4.2999763 -4.2929292 -4.2848215 -4.2750196][-4.1542549 -4.093626 -4.0331144 -3.9956214 -4.0127521 -4.0900869 -4.1777921 -4.2370148 -4.268476 -4.2859497 -4.2913947 -4.2881513 -4.276834 -4.2620282 -4.2479405][-4.1172605 -4.0627742 -4.0000591 -3.9528623 -3.9571424 -4.0295057 -4.121964 -4.1893296 -4.229136 -4.2557049 -4.268621 -4.2682676 -4.2557817 -4.2396979 -4.2248621][-4.0777922 -4.0344276 -3.9797347 -3.9283283 -3.9155264 -3.9711936 -4.0623665 -4.1354737 -4.1809711 -4.2161565 -4.2378464 -4.2420211 -4.2324147 -4.22111 -4.2092938][-4.06829 -4.0352545 -3.9884427 -3.9352164 -3.9045181 -3.9389369 -4.0203557 -4.0917807 -4.1384649 -4.175107 -4.2014608 -4.2109485 -4.2084484 -4.2058034 -4.2005219][-4.1021137 -4.0749593 -4.0308251 -3.9747636 -3.9311113 -3.9426677 -4.003089 -4.063561 -4.1065507 -4.1413503 -4.1683655 -4.1800447 -4.1838145 -4.1865649 -4.1845927][-4.1638703 -4.1401706 -4.0975819 -4.0431962 -3.9962752 -3.9901128 -4.0262046 -4.0704403 -4.1053324 -4.1342492 -4.1547756 -4.1618004 -4.1623821 -4.1631556 -4.1629133][-4.2211752 -4.2023082 -4.1672478 -4.1218548 -4.0817933 -4.06794 -4.08368 -4.110301 -4.1342621 -4.1559105 -4.1688795 -4.1662154 -4.1556849 -4.1450491 -4.1372528][-4.2627974 -4.2486334 -4.223753 -4.1911755 -4.161902 -4.1475821 -4.1504064 -4.1628366 -4.1761832 -4.1906333 -4.1977558 -4.1892428 -4.166501 -4.1372743 -4.1128902][-4.291563 -4.2818866 -4.2664595 -4.2464042 -4.2265739 -4.2135181 -4.2097578 -4.2121472 -4.2162027 -4.2240033 -4.2280197 -4.2180352 -4.18868 -4.1431913 -4.0996466]]...]
INFO - root - 2017-12-05 20:48:16.709250: step 43410, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 69h:24m:14s remains)
INFO - root - 2017-12-05 20:48:25.235164: step 43420, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 67h:50m:02s remains)
INFO - root - 2017-12-05 20:48:33.767449: step 43430, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 67h:50m:27s remains)
INFO - root - 2017-12-05 20:48:42.448416: step 43440, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.888 sec/batch; 71h:17m:49s remains)
INFO - root - 2017-12-05 20:48:50.954626: step 43450, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 67h:22m:18s remains)
INFO - root - 2017-12-05 20:48:59.403279: step 43460, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 69h:06m:16s remains)
INFO - root - 2017-12-05 20:49:07.902656: step 43470, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 67h:41m:25s remains)
INFO - root - 2017-12-05 20:49:16.549530: step 43480, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 69h:31m:33s remains)
INFO - root - 2017-12-05 20:49:24.985437: step 43490, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 67h:31m:24s remains)
INFO - root - 2017-12-05 20:49:33.648525: step 43500, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 69h:08m:41s remains)
2017-12-05 20:49:34.387145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2855783 -4.2629066 -4.2482767 -4.2370043 -4.232337 -4.2389231 -4.2450037 -4.2413168 -4.2365432 -4.2417793 -4.2574682 -4.2760048 -4.2959433 -4.3170781 -4.3343287][-4.2481036 -4.220036 -4.2058578 -4.1908 -4.180613 -4.1870518 -4.1971207 -4.1958346 -4.1915236 -4.1992455 -4.221302 -4.2498837 -4.2824192 -4.3118744 -4.3329239][-4.209157 -4.1747479 -4.1607318 -4.1434107 -4.1311288 -4.1386547 -4.147922 -4.1408176 -4.1375289 -4.1545048 -4.1880512 -4.2272725 -4.2717276 -4.3089442 -4.3334112][-4.1773267 -4.1396227 -4.1247063 -4.1081867 -4.0990987 -4.1038885 -4.0991397 -4.0777173 -4.074482 -4.1031137 -4.1529016 -4.2066293 -4.262166 -4.3082204 -4.3356528][-4.1437941 -4.1047425 -4.0901117 -4.0828075 -4.0797272 -4.0778689 -4.0509577 -4.0049467 -3.9891253 -4.0318084 -4.10897 -4.1836448 -4.2503524 -4.302453 -4.3325338][-4.1014347 -4.0618162 -4.0494432 -4.0603342 -4.07094 -4.0616989 -4.0014272 -3.9112339 -3.8710608 -3.9410589 -4.0604773 -4.1605749 -4.2382874 -4.2954822 -4.3302383][-4.0692768 -4.0295539 -4.0220122 -4.0465722 -4.062459 -4.0362372 -3.9277236 -3.7684853 -3.7010114 -3.8248017 -4.0016847 -4.129539 -4.217721 -4.2833052 -4.3263245][-4.0523863 -4.0162191 -4.0098 -4.0314784 -4.0402207 -4.0036292 -3.8571115 -3.6344519 -3.5591307 -3.7442665 -3.9606736 -4.1049304 -4.1956768 -4.265759 -4.3161888][-4.065155 -4.0397496 -4.0372744 -4.0551658 -4.0588565 -4.0270405 -3.8973243 -3.709415 -3.6641316 -3.815928 -3.9964309 -4.1203175 -4.2012343 -4.2655015 -4.3098612][-4.1205664 -4.1066732 -4.1043448 -4.1166282 -4.1167474 -4.0977736 -4.0154066 -3.9082785 -3.8849683 -3.9623 -4.0742831 -4.1668224 -4.2355895 -4.287293 -4.3174877][-4.1983666 -4.1928763 -4.1857038 -4.185523 -4.1781921 -4.1646152 -4.1195855 -4.0648789 -4.0455723 -4.0737486 -4.1418757 -4.2167621 -4.2741961 -4.3141174 -4.3318119][-4.2719736 -4.2698693 -4.2584496 -4.2482438 -4.234055 -4.2244797 -4.2027249 -4.171998 -4.1526628 -4.1620507 -4.2093272 -4.2707276 -4.3146615 -4.3399224 -4.345871][-4.3216043 -4.3186979 -4.3080878 -4.296392 -4.2831059 -4.277998 -4.2719975 -4.2547717 -4.23685 -4.2370038 -4.2693892 -4.3155456 -4.3469729 -4.360199 -4.3586807][-4.3435264 -4.3405185 -4.3343158 -4.3270907 -4.3177247 -4.3145418 -4.316381 -4.3080025 -4.2956128 -4.2930541 -4.3118887 -4.3405695 -4.3605905 -4.3701959 -4.3677883][-4.348279 -4.3435149 -4.3410034 -4.3390813 -4.3343873 -4.3307328 -4.3302937 -4.3248963 -4.3192992 -4.3182516 -4.3275313 -4.3437638 -4.3569989 -4.366147 -4.3667431]]...]
INFO - root - 2017-12-05 20:49:42.920700: step 43510, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 68h:33m:17s remains)
INFO - root - 2017-12-05 20:49:51.551444: step 43520, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 68h:52m:22s remains)
INFO - root - 2017-12-05 20:50:00.181738: step 43530, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 68h:36m:00s remains)
INFO - root - 2017-12-05 20:50:08.694534: step 43540, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 69h:56m:14s remains)
INFO - root - 2017-12-05 20:50:17.200754: step 43550, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 67h:44m:02s remains)
INFO - root - 2017-12-05 20:50:25.567081: step 43560, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 69h:16m:55s remains)
INFO - root - 2017-12-05 20:50:34.115177: step 43570, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 68h:50m:44s remains)
INFO - root - 2017-12-05 20:50:42.759167: step 43580, loss = 2.02, batch loss = 1.97 (9.2 examples/sec; 0.870 sec/batch; 69h:50m:34s remains)
INFO - root - 2017-12-05 20:50:51.024376: step 43590, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.871 sec/batch; 69h:53m:06s remains)
INFO - root - 2017-12-05 20:50:59.545740: step 43600, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 70h:01m:10s remains)
2017-12-05 20:51:00.330403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1948557 -4.1952782 -4.18604 -4.1701584 -4.1603193 -4.1591091 -4.1684256 -4.1947913 -4.2289739 -4.2468953 -4.2438211 -4.2296252 -4.2096739 -4.1922965 -4.1676526][-4.1974058 -4.2029672 -4.1961632 -4.1805687 -4.1722507 -4.1719747 -4.1790452 -4.1995144 -4.2284608 -4.2478194 -4.2521758 -4.2451468 -4.2307911 -4.217936 -4.19883][-4.2070832 -4.2185807 -4.2154903 -4.2010617 -4.192914 -4.1898375 -4.1913624 -4.2005463 -4.2206044 -4.2391419 -4.24533 -4.2405186 -4.2284713 -4.2190628 -4.2043786][-4.2176523 -4.2345943 -4.2295103 -4.2101622 -4.1981916 -4.1884632 -4.1811271 -4.17416 -4.1855288 -4.205821 -4.21573 -4.2167988 -4.2119541 -4.2094126 -4.2026691][-4.2232928 -4.240253 -4.227447 -4.19753 -4.1748805 -4.15216 -4.1295981 -4.1048627 -4.1155157 -4.151454 -4.1752596 -4.1906571 -4.20041 -4.2087502 -4.21418][-4.2219882 -4.2339149 -4.2131357 -4.1709528 -4.1305027 -4.0893908 -4.0521374 -4.0166321 -4.0345449 -4.093842 -4.1385083 -4.17173 -4.1966906 -4.2170782 -4.2330956][-4.220542 -4.2280974 -4.2037377 -4.156456 -4.1070957 -4.0576782 -4.0183983 -3.9841189 -4.0081649 -4.0748315 -4.1258659 -4.1660309 -4.1952152 -4.220664 -4.2397656][-4.2220106 -4.2274995 -4.20361 -4.1640482 -4.1235342 -4.0818906 -4.0531535 -4.0278616 -4.0431328 -4.0876489 -4.1251397 -4.1572843 -4.1800413 -4.204587 -4.2235551][-4.227715 -4.2320862 -4.2112269 -4.1856627 -4.1608438 -4.1298242 -4.1095023 -4.0920582 -4.0918417 -4.1094766 -4.1314692 -4.1512737 -4.1666322 -4.1879873 -4.2049971][-4.2333493 -4.236372 -4.2193589 -4.2044282 -4.1873422 -4.159441 -4.139854 -4.1263905 -4.1210494 -4.1299038 -4.1483407 -4.1632261 -4.1773291 -4.1954675 -4.2082753][-4.2197132 -4.2221785 -4.2122097 -4.208385 -4.1985383 -4.1749439 -4.1545067 -4.1411443 -4.1358833 -4.1461973 -4.1674757 -4.1836452 -4.1997805 -4.213635 -4.2194266][-4.1895452 -4.190218 -4.188592 -4.1929793 -4.1922693 -4.1780124 -4.1621218 -4.1514311 -4.1509523 -4.1647744 -4.1847572 -4.1994195 -4.2128253 -4.2203941 -4.221489][-4.1472497 -4.1455441 -4.1517692 -4.1655192 -4.1744633 -4.1719713 -4.1656594 -4.1608748 -4.1626678 -4.1734686 -4.1850595 -4.1927009 -4.1989956 -4.2024088 -4.2020373][-4.10715 -4.1099076 -4.1271729 -4.1512408 -4.1675744 -4.1729674 -4.17324 -4.171289 -4.1709275 -4.1752133 -4.1783495 -4.180294 -4.1825356 -4.185462 -4.1866541][-4.0946097 -4.1053724 -4.1323552 -4.1618123 -4.1781487 -4.1826735 -4.1832952 -4.1811676 -4.1790991 -4.1799245 -4.1801205 -4.1816049 -4.1840115 -4.1876965 -4.1889982]]...]
INFO - root - 2017-12-05 20:51:08.884234: step 43610, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 68h:59m:53s remains)
INFO - root - 2017-12-05 20:51:17.400609: step 43620, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 69h:26m:19s remains)
INFO - root - 2017-12-05 20:51:25.919592: step 43630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 68h:24m:46s remains)
INFO - root - 2017-12-05 20:51:34.506645: step 43640, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 69h:39m:00s remains)
INFO - root - 2017-12-05 20:51:43.021328: step 43650, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 67h:36m:14s remains)
INFO - root - 2017-12-05 20:51:51.575037: step 43660, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 71h:09m:46s remains)
INFO - root - 2017-12-05 20:52:00.036644: step 43670, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 67h:37m:56s remains)
INFO - root - 2017-12-05 20:52:08.609465: step 43680, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 70h:41m:43s remains)
INFO - root - 2017-12-05 20:52:17.251365: step 43690, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 67h:31m:16s remains)
INFO - root - 2017-12-05 20:52:25.709102: step 43700, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 68h:29m:55s remains)
2017-12-05 20:52:26.554976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1256361 -4.1144328 -4.1080475 -4.1271248 -4.1772294 -4.2353592 -4.2783389 -4.3041253 -4.320045 -4.3260908 -4.3272562 -4.3258429 -4.3246117 -4.3234344 -4.3218856][-4.0514007 -4.0468311 -4.0499344 -4.0814886 -4.1461043 -4.2123337 -4.2568231 -4.2827077 -4.3008556 -4.3060412 -4.3055477 -4.3047194 -4.3050961 -4.3061309 -4.3059106][-4.001874 -4.0150213 -4.0311208 -4.066267 -4.1274233 -4.1870871 -4.226768 -4.2463188 -4.2560396 -4.2534809 -4.2528577 -4.2595434 -4.2698331 -4.2777433 -4.2813649][-4.0006928 -4.0361176 -4.06261 -4.0853076 -4.1231351 -4.1623793 -4.1877651 -4.1871414 -4.1763477 -4.1667442 -4.1758385 -4.1955075 -4.2211246 -4.2396116 -4.24662][-4.0493236 -4.096117 -4.1192069 -4.1237917 -4.1328835 -4.1419048 -4.1380339 -4.1030021 -4.0686121 -4.055027 -4.0753522 -4.10674 -4.1499405 -4.1826949 -4.1936789][-4.1252623 -4.1616368 -4.1668959 -4.1477213 -4.1290469 -4.0952196 -4.0427012 -3.9733486 -3.9346495 -3.9323595 -3.9677658 -4.0132308 -4.0782156 -4.1296158 -4.1516542][-4.2057581 -4.21894 -4.2022228 -4.1638179 -4.1201591 -4.0463657 -3.9496362 -3.8635149 -3.8469269 -3.8749342 -3.9347777 -4.0019836 -4.0870323 -4.1474023 -4.1718435][-4.264924 -4.261415 -4.2291865 -4.1820126 -4.128108 -4.0454926 -3.9486835 -3.8834558 -3.8915255 -3.938205 -4.0107112 -4.0870171 -4.1650925 -4.212925 -4.2267585][-4.30278 -4.2994881 -4.2678947 -4.2247028 -4.1797085 -4.121223 -4.0623651 -4.0257683 -4.0330057 -4.0698981 -4.1285338 -4.1912947 -4.246963 -4.2759361 -4.2792664][-4.3260603 -4.332233 -4.313251 -4.2854371 -4.2540865 -4.216743 -4.1852822 -4.1665878 -4.1657581 -4.1850743 -4.2222767 -4.2651186 -4.2958469 -4.3093419 -4.3120322][-4.3329716 -4.3482165 -4.3437862 -4.3295836 -4.3084378 -4.2808185 -4.2602086 -4.250453 -4.250217 -4.2617331 -4.2817187 -4.3048334 -4.3176808 -4.3210244 -4.3256688][-4.3304234 -4.3472795 -4.35027 -4.3434963 -4.3286257 -4.3069458 -4.2880621 -4.2780375 -4.278142 -4.2869182 -4.2983036 -4.3104897 -4.3162107 -4.3179092 -4.3246827][-4.3200521 -4.3342128 -4.3401003 -4.3387547 -4.3285894 -4.3089995 -4.2876229 -4.2756553 -4.2740574 -4.2801614 -4.2887897 -4.2970262 -4.3001475 -4.2981539 -4.3022375][-4.2880015 -4.2977605 -4.3045664 -4.3074889 -4.3012977 -4.2828007 -4.2604113 -4.2472324 -4.2432261 -4.2472262 -4.2576165 -4.2667317 -4.2688403 -4.2661781 -4.2682023][-4.2560043 -4.2630715 -4.2702785 -4.2769938 -4.2764516 -4.2653937 -4.2473516 -4.2326732 -4.22327 -4.2235093 -4.2333417 -4.2438 -4.2506514 -4.254848 -4.2571363]]...]
INFO - root - 2017-12-05 20:52:35.037258: step 43710, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.816 sec/batch; 65h:25m:25s remains)
INFO - root - 2017-12-05 20:52:43.517014: step 43720, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 67h:12m:15s remains)
INFO - root - 2017-12-05 20:52:51.939098: step 43730, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.836 sec/batch; 67h:02m:31s remains)
INFO - root - 2017-12-05 20:53:00.418919: step 43740, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 69h:44m:13s remains)
INFO - root - 2017-12-05 20:53:08.942997: step 43750, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 69h:42m:04s remains)
INFO - root - 2017-12-05 20:53:17.472771: step 43760, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.860 sec/batch; 68h:56m:28s remains)
INFO - root - 2017-12-05 20:53:26.009647: step 43770, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.876 sec/batch; 70h:13m:57s remains)
INFO - root - 2017-12-05 20:53:34.576441: step 43780, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.818 sec/batch; 65h:35m:52s remains)
INFO - root - 2017-12-05 20:53:43.289312: step 43790, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 69h:25m:45s remains)
INFO - root - 2017-12-05 20:53:51.693897: step 43800, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 69h:17m:15s remains)
2017-12-05 20:53:52.483463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3109169 -4.3141108 -4.3150625 -4.3165092 -4.3185825 -4.3203697 -4.3209434 -4.319273 -4.3134751 -4.300528 -4.28201 -4.2641811 -4.2459307 -4.228898 -4.2174263][-4.3174033 -4.3202662 -4.3225517 -4.3252416 -4.3275647 -4.3298216 -4.3300552 -4.32668 -4.3191023 -4.3037524 -4.2831359 -4.262362 -4.2387404 -4.2101731 -4.1841474][-4.3192663 -4.3210411 -4.3222632 -4.3244596 -4.3264527 -4.3281765 -4.3289723 -4.3244104 -4.3146958 -4.29906 -4.2788043 -4.2587771 -4.2358766 -4.2047558 -4.17189][-4.3114758 -4.3124776 -4.3128343 -4.3138742 -4.3135958 -4.3129334 -4.3134608 -4.3071775 -4.2942271 -4.2805133 -4.26646 -4.2529655 -4.2394013 -4.2186365 -4.1890039][-4.29472 -4.2946839 -4.2925911 -4.2899504 -4.2828207 -4.278 -4.2761483 -4.2673707 -4.251924 -4.2438526 -4.2417603 -4.2417822 -4.2460365 -4.2391729 -4.2174187][-4.2771997 -4.2731256 -4.2657723 -4.2574782 -4.2442427 -4.2339621 -4.2268348 -4.212913 -4.1914783 -4.19027 -4.1993384 -4.2105937 -4.2302117 -4.2351933 -4.2196131][-4.2499404 -4.2387342 -4.2238722 -4.2091236 -4.188242 -4.16748 -4.1528563 -4.1359816 -4.1123548 -4.1174421 -4.1441383 -4.1678205 -4.1952319 -4.2075825 -4.1924357][-4.2069325 -4.1841683 -4.1577759 -4.1297035 -4.0884891 -4.0479865 -4.0247049 -4.0118294 -3.9993682 -4.0175023 -4.0649624 -4.110724 -4.1463485 -4.164629 -4.1481323][-4.1661181 -4.1319318 -4.0928688 -4.0488663 -3.980952 -3.915627 -3.8845553 -3.8842762 -3.8949602 -3.9378877 -4.0099626 -4.0731106 -4.1156955 -4.1319065 -4.1111889][-4.146173 -4.1067286 -4.0614777 -4.0103097 -3.9355319 -3.8723059 -3.8541212 -3.8760121 -3.9092255 -3.9647913 -4.0434093 -4.1046753 -4.14014 -4.1483436 -4.1275458][-4.15956 -4.1262803 -4.0885344 -4.0447059 -3.9889011 -3.9535956 -3.957309 -3.98768 -4.0227642 -4.0709066 -4.1310205 -4.1766949 -4.2004957 -4.203948 -4.1856771][-4.19015 -4.1706543 -4.1438828 -4.1154737 -4.0853467 -4.0772791 -4.0935969 -4.124012 -4.1542954 -4.1906881 -4.2319093 -4.2594366 -4.2701254 -4.2678 -4.2523341][-4.2230525 -4.2196527 -4.2050943 -4.1924367 -4.1833558 -4.1882315 -4.2052827 -4.2288079 -4.2492223 -4.2722874 -4.2966151 -4.3116083 -4.315176 -4.3123803 -4.3045053][-4.2544694 -4.266777 -4.2637744 -4.2590532 -4.2592268 -4.2649527 -4.2730927 -4.2845321 -4.294106 -4.3071451 -4.3193293 -4.3243542 -4.3230863 -4.3205462 -4.3175364][-4.2792416 -4.2950535 -4.2954531 -4.2938132 -4.2956934 -4.2979198 -4.2974815 -4.300015 -4.3057232 -4.3148 -4.3225012 -4.3251123 -4.3236389 -4.3213797 -4.3204842]]...]
INFO - root - 2017-12-05 20:54:01.051426: step 43810, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 67h:53m:51s remains)
INFO - root - 2017-12-05 20:54:09.579161: step 43820, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 69h:57m:46s remains)
INFO - root - 2017-12-05 20:54:18.029061: step 43830, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 67h:43m:04s remains)
INFO - root - 2017-12-05 20:54:26.601331: step 43840, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 68h:14m:56s remains)
INFO - root - 2017-12-05 20:54:35.066848: step 43850, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.812 sec/batch; 65h:08m:16s remains)
INFO - root - 2017-12-05 20:54:43.434670: step 43860, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 67h:06m:13s remains)
INFO - root - 2017-12-05 20:54:51.933504: step 43870, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 67h:18m:49s remains)
INFO - root - 2017-12-05 20:55:00.364576: step 43880, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 67h:47m:15s remains)
INFO - root - 2017-12-05 20:55:08.897595: step 43890, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 68h:31m:27s remains)
INFO - root - 2017-12-05 20:55:17.343380: step 43900, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 68h:25m:37s remains)
2017-12-05 20:55:18.076433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2109718 -4.2359185 -4.2500668 -4.2559905 -4.2474012 -4.2258382 -4.2041774 -4.1970677 -4.2131095 -4.2368226 -4.2581239 -4.27131 -4.2832603 -4.2898779 -4.2794704][-4.20228 -4.2296953 -4.247643 -4.2584243 -4.2517524 -4.2284384 -4.2018337 -4.1902661 -4.2059011 -4.2317405 -4.2567949 -4.2745495 -4.2899456 -4.2964025 -4.2832341][-4.1867094 -4.2179747 -4.2430582 -4.261683 -4.2599316 -4.236239 -4.2046418 -4.1863508 -4.1980872 -4.2235751 -4.2516322 -4.2746134 -4.2933869 -4.2990294 -4.28308][-4.1682673 -4.203918 -4.2375908 -4.265914 -4.27084 -4.248178 -4.2110825 -4.1839638 -4.1894584 -4.2133231 -4.2446246 -4.2734542 -4.2959805 -4.3016539 -4.2837725][-4.1479287 -4.187685 -4.2299981 -4.2673788 -4.2806091 -4.2607193 -4.2180772 -4.181015 -4.1787076 -4.2007937 -4.235395 -4.2698264 -4.2959013 -4.3025575 -4.2843833][-4.1325369 -4.1744843 -4.2233682 -4.2673922 -4.2876921 -4.2709961 -4.2246647 -4.17927 -4.169755 -4.1893549 -4.2261381 -4.264894 -4.2936745 -4.3009129 -4.2829347][-4.1242552 -4.1651921 -4.2170997 -4.2650166 -4.2900157 -4.2769303 -4.2299237 -4.1792846 -4.1634512 -4.1795926 -4.2170658 -4.2587214 -4.2891288 -4.2963614 -4.2789621][-4.1265893 -4.1631727 -4.212894 -4.2597508 -4.2863169 -4.2769814 -4.2324648 -4.1794872 -4.1585841 -4.1714029 -4.2086844 -4.2517357 -4.2831717 -4.2908192 -4.2752228][-4.1294637 -4.1600127 -4.205101 -4.2496691 -4.2775335 -4.2727604 -4.23254 -4.18023 -4.1562686 -4.1664343 -4.2024755 -4.2451506 -4.2770743 -4.286272 -4.2743011][-4.1331048 -4.1572537 -4.1975365 -4.2398396 -4.26793 -4.2668405 -4.2317305 -4.18301 -4.158555 -4.1669841 -4.20056 -4.2410908 -4.2722239 -4.2833452 -4.2752819][-4.1407857 -4.1593971 -4.1948142 -4.2342935 -4.2614827 -4.2623386 -4.2320018 -4.1887479 -4.1666069 -4.1746635 -4.2055211 -4.2426238 -4.2715354 -4.2836056 -4.2786288][-4.1523609 -4.1658249 -4.1959767 -4.2319012 -4.2573919 -4.2597079 -4.2338996 -4.1973104 -4.1790166 -4.1870217 -4.2144766 -4.246964 -4.2728505 -4.2850652 -4.282114][-4.1660442 -4.1749983 -4.1993847 -4.2309775 -4.2546763 -4.2582436 -4.2365108 -4.2061963 -4.1919804 -4.1999965 -4.2242374 -4.2524276 -4.2754078 -4.2872877 -4.2863836][-4.1772695 -4.1823082 -4.2010345 -4.2284441 -4.2506666 -4.2554169 -4.2374206 -4.2128534 -4.2023745 -4.2104025 -4.2322531 -4.2575541 -4.2784429 -4.2899203 -4.2906523][-4.1841416 -4.1854825 -4.1990032 -4.2230506 -4.2447748 -4.2509747 -4.2373762 -4.218329 -4.211247 -4.2193193 -4.2392473 -4.2621932 -4.2814054 -4.2922025 -4.2935491]]...]
INFO - root - 2017-12-05 20:55:26.309711: step 43910, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 67h:56m:06s remains)
INFO - root - 2017-12-05 20:55:34.979314: step 43920, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 67h:14m:39s remains)
INFO - root - 2017-12-05 20:55:43.577349: step 43930, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.879 sec/batch; 70h:29m:56s remains)
INFO - root - 2017-12-05 20:55:52.037746: step 43940, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 68h:37m:41s remains)
INFO - root - 2017-12-05 20:56:00.500217: step 43950, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 67h:07m:12s remains)
INFO - root - 2017-12-05 20:56:08.955097: step 43960, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 70h:08m:33s remains)
INFO - root - 2017-12-05 20:56:17.392944: step 43970, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 66h:52m:27s remains)
INFO - root - 2017-12-05 20:56:25.984543: step 43980, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 67h:37m:23s remains)
INFO - root - 2017-12-05 20:56:34.473318: step 43990, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.823 sec/batch; 65h:59m:06s remains)
INFO - root - 2017-12-05 20:56:42.791491: step 44000, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.841 sec/batch; 67h:22m:09s remains)
2017-12-05 20:56:43.537191: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1191912 -4.0876465 -4.0627 -4.0432076 -4.0428109 -4.06075 -4.08265 -4.0969543 -4.1282635 -4.1852593 -4.2440119 -4.2898955 -4.3185825 -4.3340526 -4.3500905][-4.1011457 -4.0566874 -4.0184689 -3.9953225 -4.0007582 -4.0243516 -4.044723 -4.0547347 -4.086956 -4.1523962 -4.2224073 -4.2767806 -4.310194 -4.3283324 -4.3465524][-4.098886 -4.0497961 -4.0064988 -3.982028 -3.9890981 -4.0166445 -4.0396667 -4.0504627 -4.0819755 -4.1486874 -4.2200842 -4.27406 -4.3055315 -4.3210921 -4.3385839][-4.1079521 -4.0688782 -4.0332832 -4.0145683 -4.026103 -4.0553608 -4.080389 -4.0912671 -4.1143217 -4.1699591 -4.2336721 -4.279881 -4.3059859 -4.3159413 -4.3302703][-4.1200528 -4.0893564 -4.0639324 -4.0596743 -4.0789242 -4.1063251 -4.129108 -4.136158 -4.1436744 -4.1846642 -4.2407918 -4.2828636 -4.3062582 -4.314733 -4.3261418][-4.1378188 -4.1069303 -4.0868411 -4.0934472 -4.1136479 -4.1301684 -4.1413689 -4.1362457 -4.1278772 -4.1600842 -4.2167416 -4.2654486 -4.2955585 -4.3099637 -4.3253069][-4.1591024 -4.1248579 -4.107295 -4.1159086 -4.1241755 -4.121408 -4.1089978 -4.0843215 -4.0606661 -4.0869079 -4.1531639 -4.2196612 -4.2668362 -4.2939186 -4.3193307][-4.1717033 -4.1349282 -4.1160288 -4.1174946 -4.107563 -4.0847535 -4.0539484 -4.0107379 -3.97356 -3.9994264 -4.0782676 -4.1656928 -4.2314653 -4.274189 -4.3101053][-4.16871 -4.1294193 -4.1098757 -4.0999608 -4.0730753 -4.04303 -4.012363 -3.9667165 -3.9285216 -3.9603555 -4.04814 -4.14878 -4.2208328 -4.2694206 -4.3070378][-4.1545339 -4.117732 -4.0980139 -4.0799546 -4.0458884 -4.0190468 -4.00153 -3.9727712 -3.9557042 -3.9981771 -4.0867858 -4.1837249 -4.250082 -4.2930856 -4.3215251][-4.1418166 -4.1087494 -4.0887542 -4.06754 -4.0388374 -4.0189881 -4.0137858 -4.0081215 -4.0171504 -4.0676761 -4.1470566 -4.2323661 -4.2885933 -4.3203969 -4.3376241][-4.157701 -4.1256237 -4.1024051 -4.0795369 -4.06004 -4.0514984 -4.0539193 -4.0624175 -4.086144 -4.1354656 -4.2005625 -4.2695384 -4.3135562 -4.3359046 -4.3473997][-4.1954484 -4.1663308 -4.1377392 -4.1085691 -4.0910907 -4.0902829 -4.0965185 -4.1110697 -4.1411948 -4.18352 -4.2341919 -4.2874331 -4.3200097 -4.3366461 -4.3493414][-4.2185173 -4.1970496 -4.1684418 -4.1358862 -4.1166382 -4.118731 -4.1274886 -4.1458735 -4.1786165 -4.2150879 -4.2527289 -4.2893882 -4.3126173 -4.3296947 -4.3473964][-4.2142806 -4.2008743 -4.1784835 -4.1506662 -4.133482 -4.1350284 -4.1414595 -4.160388 -4.1937447 -4.2277832 -4.25589 -4.2799459 -4.297152 -4.3166566 -4.3395119]]...]
INFO - root - 2017-12-05 20:56:51.907002: step 44010, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 66h:20m:42s remains)
INFO - root - 2017-12-05 20:57:00.178103: step 44020, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 68h:52m:44s remains)
INFO - root - 2017-12-05 20:57:08.655505: step 44030, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.865 sec/batch; 69h:20m:50s remains)
INFO - root - 2017-12-05 20:57:17.188453: step 44040, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 67h:50m:36s remains)
INFO - root - 2017-12-05 20:57:25.751768: step 44050, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 67h:22m:32s remains)
INFO - root - 2017-12-05 20:57:34.203611: step 44060, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 67h:42m:25s remains)
INFO - root - 2017-12-05 20:57:42.651183: step 44070, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 67h:20m:13s remains)
INFO - root - 2017-12-05 20:57:51.082621: step 44080, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 68h:43m:18s remains)
INFO - root - 2017-12-05 20:57:59.636937: step 44090, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 68h:10m:42s remains)
INFO - root - 2017-12-05 20:58:08.073991: step 44100, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 67h:47m:36s remains)
2017-12-05 20:58:09.009754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2179265 -4.2054286 -4.1945133 -4.2058568 -4.2116323 -4.197751 -4.1722207 -4.1445179 -4.1345015 -4.1333885 -4.1451311 -4.1637197 -4.1706476 -4.1770773 -4.1911106][-4.1975908 -4.1863213 -4.1661634 -4.1638918 -4.1624837 -4.1606617 -4.1513038 -4.1352777 -4.1287518 -4.1415172 -4.1662626 -4.1901464 -4.1975312 -4.2031579 -4.2166476][-4.188693 -4.175168 -4.1522179 -4.1366735 -4.1362123 -4.1556134 -4.1645737 -4.1556759 -4.1516366 -4.1711669 -4.1972542 -4.2152739 -4.2155819 -4.2148418 -4.2244487][-4.1876473 -4.174952 -4.1566014 -4.1488705 -4.1582723 -4.1860442 -4.2011876 -4.1857452 -4.1758242 -4.1931581 -4.2201872 -4.235158 -4.2291179 -4.2189879 -4.2226949][-4.1793365 -4.1692328 -4.1623039 -4.1666865 -4.1798539 -4.2009974 -4.2038021 -4.1702642 -4.15797 -4.1794429 -4.2085791 -4.219367 -4.2063761 -4.1925611 -4.1924095][-4.1822972 -4.1773772 -4.1754608 -4.176321 -4.1822448 -4.1833186 -4.1551061 -4.1001182 -4.0933652 -4.1258616 -4.1545968 -4.155632 -4.1334572 -4.1178555 -4.1252418][-4.1757207 -4.1708312 -4.1644621 -4.1502867 -4.1376548 -4.1090865 -4.0445929 -3.9748917 -3.9943202 -4.0548997 -4.0937028 -4.09636 -4.0755591 -4.0615387 -4.0715003][-4.1235147 -4.1176515 -4.1064668 -4.08072 -4.0558033 -4.0049143 -3.9154239 -3.8438191 -3.8936355 -3.9806116 -4.0349655 -4.04541 -4.0419388 -4.043952 -4.0646296][-4.089396 -4.0864587 -4.076611 -4.0535216 -4.0290961 -3.9825344 -3.9100256 -3.8542445 -3.9041758 -3.9882393 -4.041081 -4.0515924 -4.0493512 -4.057848 -4.0893373][-4.1137943 -4.1075439 -4.0951924 -4.07939 -4.0638647 -4.0421195 -4.0047603 -3.9733953 -4.0058885 -4.062079 -4.0998936 -4.1064696 -4.0985947 -4.09702 -4.1207652][-4.1607313 -4.1530848 -4.1341538 -4.1214538 -4.11281 -4.1053839 -4.0882273 -4.0688982 -4.0878983 -4.1194515 -4.1447663 -4.1559291 -4.1507864 -4.1442566 -4.1558127][-4.2014422 -4.1894813 -4.1690645 -4.1613917 -4.1617713 -4.1617603 -4.1532545 -4.14293 -4.1566381 -4.1759191 -4.19225 -4.2047248 -4.2078652 -4.2044497 -4.2067065][-4.2181988 -4.2036371 -4.1888847 -4.1928563 -4.1998048 -4.2004504 -4.1936274 -4.1878295 -4.1970825 -4.212049 -4.2259407 -4.2387767 -4.2450213 -4.2443204 -4.2445545][-4.2149639 -4.2052159 -4.1982789 -4.20791 -4.2196994 -4.2236867 -4.2211723 -4.2169051 -4.219521 -4.2275906 -4.2370381 -4.24454 -4.2476134 -4.2485027 -4.251616][-4.2280273 -4.2231464 -4.2183771 -4.2236333 -4.23368 -4.2409873 -4.2435303 -4.2416654 -4.239234 -4.2404294 -4.2426472 -4.2427998 -4.2398815 -4.2389708 -4.2430468]]...]
INFO - root - 2017-12-05 20:58:17.500670: step 44110, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 68h:08m:06s remains)
INFO - root - 2017-12-05 20:58:25.812738: step 44120, loss = 2.04, batch loss = 1.98 (10.3 examples/sec; 0.777 sec/batch; 62h:15m:40s remains)
INFO - root - 2017-12-05 20:58:34.301412: step 44130, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 69h:06m:22s remains)
INFO - root - 2017-12-05 20:58:42.784126: step 44140, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 67h:40m:11s remains)
INFO - root - 2017-12-05 20:58:51.306247: step 44150, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 68h:21m:45s remains)
INFO - root - 2017-12-05 20:58:59.675994: step 44160, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.824 sec/batch; 66h:02m:07s remains)
INFO - root - 2017-12-05 20:59:08.156681: step 44170, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 69h:11m:56s remains)
INFO - root - 2017-12-05 20:59:16.709461: step 44180, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 67h:33m:03s remains)
INFO - root - 2017-12-05 20:59:25.258550: step 44190, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 67h:04m:06s remains)
INFO - root - 2017-12-05 20:59:33.830599: step 44200, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 69h:04m:12s remains)
2017-12-05 20:59:34.558795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2083292 -4.2050118 -4.2028522 -4.2003245 -4.201375 -4.2006855 -4.2055168 -4.2166691 -4.2276583 -4.23734 -4.243433 -4.2474027 -4.2448349 -4.2316527 -4.2082067][-4.1932931 -4.1864777 -4.1816521 -4.1803246 -4.1829371 -4.1765833 -4.1730347 -4.1829214 -4.2014451 -4.2239871 -4.2399621 -4.2488461 -4.2503586 -4.2399483 -4.2203579][-4.2033262 -4.1922221 -4.182775 -4.1798987 -4.1810102 -4.1684246 -4.1507916 -4.151752 -4.1756706 -4.2093725 -4.2366848 -4.2542024 -4.26004 -4.2523956 -4.2343726][-4.2075534 -4.1928806 -4.1822548 -4.1795049 -4.1799293 -4.1568556 -4.11913 -4.1041956 -4.1307039 -4.1748753 -4.2168584 -4.2516804 -4.2632289 -4.2516847 -4.2316675][-4.2041421 -4.1916065 -4.186552 -4.1881003 -4.1817527 -4.1413059 -4.0735641 -4.0297503 -4.054389 -4.1173477 -4.1839252 -4.2379289 -4.2561145 -4.2419047 -4.2185225][-4.2130013 -4.2049346 -4.201962 -4.2017832 -4.1824765 -4.1149387 -4.0057297 -3.9197626 -3.9449339 -4.0449572 -4.1476808 -4.2210894 -4.2479224 -4.2349005 -4.2108135][-4.2230544 -4.212399 -4.2052817 -4.1976686 -4.1602173 -4.0656505 -3.9147487 -3.7857022 -3.8198588 -3.9680576 -4.1084623 -4.2030334 -4.2429695 -4.2382135 -4.2181711][-4.2214684 -4.2067661 -4.1943321 -4.1803675 -4.1356792 -4.0336533 -3.8716393 -3.7348583 -3.7789578 -3.9426932 -4.0929341 -4.1975994 -4.2493296 -4.2508492 -4.2308383][-4.214036 -4.2013931 -4.188159 -4.1752329 -4.1388421 -4.0575352 -3.9361653 -3.8420842 -3.8789153 -4.0043826 -4.1213284 -4.2099309 -4.2628136 -4.2676806 -4.2441316][-4.2043972 -4.1958904 -4.1863976 -4.1774716 -4.154058 -4.1000023 -4.0211077 -3.962631 -3.9846175 -4.0657368 -4.1483603 -4.214673 -4.2646832 -4.2761173 -4.2546778][-4.1861463 -4.1856761 -4.1855121 -4.1877518 -4.18075 -4.1483331 -4.0919657 -4.0475807 -4.0549626 -4.1059728 -4.1673155 -4.2178669 -4.2622056 -4.2787724 -4.264503][-4.1630945 -4.1745143 -4.1852417 -4.1962752 -4.1977916 -4.1759257 -4.1341152 -4.10112 -4.1021156 -4.1354575 -4.1858039 -4.2315526 -4.2678442 -4.2832193 -4.2751794][-4.1594133 -4.1796188 -4.1953592 -4.2099371 -4.2150111 -4.2020564 -4.1776938 -4.1592264 -4.1579981 -4.176898 -4.2126236 -4.2499909 -4.2778826 -4.2882891 -4.2794356][-4.1931739 -4.2138405 -4.2245049 -4.2333241 -4.2384481 -4.2323337 -4.2217169 -4.21425 -4.2127829 -4.2198191 -4.2387562 -4.2629628 -4.2811527 -4.2865324 -4.2791305][-4.2279496 -4.2436752 -4.2481413 -4.2509432 -4.2524681 -4.247417 -4.2421837 -4.2400923 -4.2390809 -4.2417946 -4.251564 -4.2660089 -4.2773991 -4.2828174 -4.2808275]]...]
INFO - root - 2017-12-05 20:59:43.013859: step 44210, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.793 sec/batch; 63h:28m:12s remains)
INFO - root - 2017-12-05 20:59:51.442705: step 44220, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 68h:22m:03s remains)
INFO - root - 2017-12-05 20:59:59.895704: step 44230, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 68h:17m:31s remains)
INFO - root - 2017-12-05 21:00:08.405214: step 44240, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.780 sec/batch; 62h:28m:30s remains)
INFO - root - 2017-12-05 21:00:17.003346: step 44250, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 68h:51m:59s remains)
INFO - root - 2017-12-05 21:00:25.386719: step 44260, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 69h:11m:53s remains)
INFO - root - 2017-12-05 21:00:33.892179: step 44270, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 67h:54m:49s remains)
INFO - root - 2017-12-05 21:00:42.519495: step 44280, loss = 2.11, batch loss = 2.05 (9.6 examples/sec; 0.830 sec/batch; 66h:26m:10s remains)
INFO - root - 2017-12-05 21:00:51.105528: step 44290, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 69h:40m:19s remains)
INFO - root - 2017-12-05 21:00:59.505274: step 44300, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 66h:50m:31s remains)
2017-12-05 21:01:00.292582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2624683 -4.2655153 -4.2804685 -4.2957816 -4.3019977 -4.3031378 -4.2991185 -4.2889786 -4.2718358 -4.2599893 -4.2661605 -4.279387 -4.2837811 -4.2740293 -4.2532969][-4.2407551 -4.2457352 -4.2608151 -4.2755351 -4.2814608 -4.2787838 -4.2675834 -4.2515273 -4.2286458 -4.2113962 -4.2159529 -4.229207 -4.2319851 -4.2168312 -4.1918497][-4.2149215 -4.2220974 -4.2353663 -4.2473497 -4.2519236 -4.2428479 -4.2261143 -4.2119656 -4.1928973 -4.1739006 -4.1694221 -4.1705675 -4.1665545 -4.1464276 -4.122839][-4.188489 -4.1968112 -4.2046013 -4.2096639 -4.21084 -4.1963768 -4.1769614 -4.1692929 -4.1638856 -4.1529479 -4.1427155 -4.1317482 -4.1174512 -4.0911603 -4.0710735][-4.1669006 -4.1715035 -4.170927 -4.1700568 -4.1667824 -4.1455851 -4.1224771 -4.1166039 -4.1278772 -4.1406593 -4.1418219 -4.1296468 -4.1116824 -4.0802779 -4.0635872][-4.14727 -4.1463003 -4.1386113 -4.1317525 -4.1220469 -4.0937548 -4.06059 -4.0444531 -4.0703611 -4.1193256 -4.1461482 -4.143445 -4.1258039 -4.0924964 -4.0775127][-4.144753 -4.1413722 -4.1274157 -4.1128154 -4.0932355 -4.0557642 -4.0058522 -3.9681442 -4.0042095 -4.0935922 -4.1484742 -4.1577735 -4.1447182 -4.1156178 -4.1000929][-4.1463566 -4.1438346 -4.1310797 -4.1129889 -4.0855827 -4.041048 -3.9748604 -3.9199181 -3.9668193 -4.0750551 -4.1455073 -4.1673036 -4.1610494 -4.1378989 -4.1191521][-4.1463771 -4.1503611 -4.1459394 -4.1311412 -4.1033835 -4.0589948 -3.9902065 -3.9383626 -3.9846449 -4.0818686 -4.149951 -4.1783695 -4.176177 -4.1571274 -4.1368837][-4.1464214 -4.1622066 -4.1705203 -4.1661253 -4.1500177 -4.1148853 -4.0586348 -4.0219078 -4.0512033 -4.1146536 -4.1686668 -4.1949186 -4.1934986 -4.17665 -4.1581631][-4.1468477 -4.172811 -4.1952457 -4.2040248 -4.2028575 -4.1848006 -4.1501884 -4.1312871 -4.1381865 -4.1638818 -4.1962023 -4.2172689 -4.2167497 -4.2025247 -4.1886334][-4.1615367 -4.1902986 -4.2212815 -4.2401071 -4.2508245 -4.2489896 -4.2345538 -4.2277422 -4.2208414 -4.2197809 -4.2331796 -4.2464294 -4.2449274 -4.2331071 -4.222682][-4.1987519 -4.2234859 -4.2529988 -4.2736931 -4.2905746 -4.2993631 -4.2973161 -4.2960563 -4.285069 -4.2721143 -4.273334 -4.2799945 -4.2774043 -4.2667322 -4.2561975][-4.259872 -4.2761536 -4.2953339 -4.3104377 -4.32517 -4.3345413 -4.3355875 -4.3342533 -4.323719 -4.3103485 -4.3069639 -4.3093786 -4.3072467 -4.2985611 -4.2894731][-4.3070254 -4.3151231 -4.3241119 -4.3318233 -4.3411779 -4.3465891 -4.34758 -4.3464508 -4.34068 -4.3332043 -4.3291392 -4.3289056 -4.3276386 -4.3220749 -4.3163676]]...]
INFO - root - 2017-12-05 21:01:08.787177: step 44310, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 67h:53m:07s remains)
INFO - root - 2017-12-05 21:01:17.186100: step 44320, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.818 sec/batch; 65h:29m:52s remains)
INFO - root - 2017-12-05 21:01:25.792599: step 44330, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 68h:13m:18s remains)
INFO - root - 2017-12-05 21:01:34.093804: step 44340, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 67h:52m:08s remains)
INFO - root - 2017-12-05 21:01:42.616051: step 44350, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 68h:46m:43s remains)
INFO - root - 2017-12-05 21:01:51.071411: step 44360, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 66h:53m:12s remains)
INFO - root - 2017-12-05 21:01:59.663127: step 44370, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 70h:19m:42s remains)
INFO - root - 2017-12-05 21:02:08.126618: step 44380, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 66h:52m:54s remains)
INFO - root - 2017-12-05 21:02:16.754314: step 44390, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 71h:12m:03s remains)
INFO - root - 2017-12-05 21:02:25.291369: step 44400, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 68h:07m:39s remains)
2017-12-05 21:02:26.064871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2467003 -4.2359505 -4.2235074 -4.2116284 -4.2033134 -4.1964793 -4.1924586 -4.1907783 -4.1918936 -4.1963487 -4.2048182 -4.212286 -4.21802 -4.2225952 -4.2230682][-4.2405825 -4.2271314 -4.2116685 -4.1996484 -4.1942391 -4.1894546 -4.1846876 -4.1806273 -4.1789975 -4.1812477 -4.18857 -4.1964636 -4.2040095 -4.2113881 -4.2142167][-4.2350225 -4.2186432 -4.1990395 -4.18562 -4.1828389 -4.1826758 -4.1814861 -4.1799707 -4.1794343 -4.1788254 -4.1797833 -4.1826544 -4.1884475 -4.198051 -4.2046986][-4.2335844 -4.2152014 -4.1922078 -4.1758103 -4.1733751 -4.1772046 -4.1819539 -4.1871948 -4.1914086 -4.1898012 -4.1830521 -4.1783323 -4.1796031 -4.1887627 -4.1977472][-4.2362466 -4.2181616 -4.194026 -4.1745424 -4.1686373 -4.1726646 -4.1811337 -4.1929469 -4.2032576 -4.2035937 -4.1934996 -4.1836729 -4.1804123 -4.1855311 -4.1924453][-4.2441235 -4.2303519 -4.2085862 -4.1862135 -4.1725755 -4.1689124 -4.1716824 -4.1828814 -4.1978884 -4.2045851 -4.1988521 -4.1904845 -4.1853709 -4.1853833 -4.1872144][-4.2496729 -4.2418461 -4.2251678 -4.2025833 -4.1819839 -4.1665936 -4.1564026 -4.1599126 -4.1763773 -4.19009 -4.1924186 -4.1899519 -4.1852231 -4.1817527 -4.1788878][-4.2491322 -4.2440238 -4.2316384 -4.212687 -4.1912351 -4.1703367 -4.1518474 -4.1481156 -4.1614342 -4.1760626 -4.1807756 -4.1802216 -4.1760292 -4.1730762 -4.171052][-4.2428765 -4.2365251 -4.2266011 -4.2133245 -4.1987109 -4.1842222 -4.1716261 -4.1679792 -4.1736088 -4.1796165 -4.1774907 -4.1723518 -4.167007 -4.1653738 -4.1674204][-4.2326832 -4.222095 -4.2123489 -4.2053514 -4.2021241 -4.2004976 -4.2002439 -4.2003226 -4.1990733 -4.1945562 -4.1822472 -4.1693707 -4.1591196 -4.1552405 -4.15864][-4.2239437 -4.206924 -4.19417 -4.1902971 -4.195446 -4.2035351 -4.2121983 -4.2161822 -4.2121053 -4.2034569 -4.1883893 -4.1724572 -4.1585746 -4.1510415 -4.1517305][-4.2241759 -4.2007318 -4.1815662 -4.1750746 -4.1804414 -4.189702 -4.2006459 -4.2070327 -4.2049675 -4.2001061 -4.1915703 -4.1810169 -4.1683927 -4.157599 -4.1528797][-4.23149 -4.2037125 -4.178875 -4.1672964 -4.1675549 -4.1717353 -4.1810141 -4.189146 -4.1900935 -4.1890726 -4.1878266 -4.1860609 -4.1804547 -4.1717329 -4.1632519][-4.2418232 -4.2142448 -4.1888208 -4.1747289 -4.170054 -4.1676521 -4.1719937 -4.1782947 -4.1779442 -4.1764717 -4.1789465 -4.1858544 -4.1922946 -4.1931 -4.1867094][-4.2493868 -4.2248063 -4.202003 -4.1893106 -4.183897 -4.1788721 -4.1795979 -4.1820917 -4.1788397 -4.1754885 -4.1781611 -4.1895685 -4.2051139 -4.2134652 -4.2105927]]...]
INFO - root - 2017-12-05 21:02:34.628077: step 44410, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 68h:28m:30s remains)
INFO - root - 2017-12-05 21:02:43.118380: step 44420, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 68h:08m:20s remains)
INFO - root - 2017-12-05 21:02:51.501237: step 44430, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 0.768 sec/batch; 61h:29m:08s remains)
INFO - root - 2017-12-05 21:03:00.083698: step 44440, loss = 2.09, batch loss = 2.04 (10.7 examples/sec; 0.748 sec/batch; 59h:53m:29s remains)
INFO - root - 2017-12-05 21:03:08.651470: step 44450, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 67h:26m:10s remains)
INFO - root - 2017-12-05 21:03:17.066278: step 44460, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 68h:14m:50s remains)
INFO - root - 2017-12-05 21:03:25.532559: step 44470, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 69h:25m:31s remains)
INFO - root - 2017-12-05 21:03:34.071063: step 44480, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 68h:32m:42s remains)
INFO - root - 2017-12-05 21:03:42.564919: step 44490, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 69h:16m:38s remains)
INFO - root - 2017-12-05 21:03:51.029146: step 44500, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 65h:35m:49s remains)
2017-12-05 21:03:51.799729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2416477 -4.2435055 -4.2370229 -4.2271886 -4.2154236 -4.2061448 -4.2032833 -4.2124753 -4.2283196 -4.2411056 -4.2344565 -4.2043328 -4.1686049 -4.1442795 -4.1272287][-4.2154131 -4.21496 -4.2015443 -4.1802425 -4.1577148 -4.1443505 -4.1434264 -4.1582084 -4.1893435 -4.2202454 -4.2246242 -4.1955962 -4.1560087 -4.1304631 -4.1118116][-4.2003489 -4.1942806 -4.1725979 -4.139132 -4.1042113 -4.08433 -4.083518 -4.1039572 -4.151588 -4.20479 -4.2259207 -4.201602 -4.1643133 -4.1393771 -4.1177835][-4.2025867 -4.1929331 -4.1636848 -4.1192374 -4.0720868 -4.0429425 -4.0365238 -4.0535755 -4.1143084 -4.1918983 -4.2363567 -4.2250357 -4.191165 -4.162673 -4.136178][-4.2173963 -4.2087483 -4.1761312 -4.1233492 -4.0614119 -4.0091777 -3.9776864 -3.9796548 -4.0549731 -4.1640215 -4.2403088 -4.2498226 -4.2264085 -4.1986656 -4.167][-4.2299914 -4.2256 -4.1938853 -4.1328635 -4.0496449 -3.9586091 -3.8780322 -3.8579137 -3.9587946 -4.1113253 -4.2232018 -4.2594724 -4.2520118 -4.2309594 -4.1957622][-4.2363405 -4.2357306 -4.2056823 -4.1348147 -4.0263538 -3.8878675 -3.7401271 -3.6916919 -3.8316324 -4.0357027 -4.181716 -4.2445412 -4.2584348 -4.2473435 -4.2087541][-4.2405577 -4.2445974 -4.2190328 -4.141973 -4.0135279 -3.8349447 -3.6260428 -3.5414824 -3.7192492 -3.9712842 -4.1477137 -4.2336183 -4.2655258 -4.2600389 -4.2155809][-4.2424726 -4.25545 -4.2400455 -4.1705513 -4.0476184 -3.8695402 -3.6514444 -3.5468576 -3.7195389 -3.972012 -4.1512575 -4.2424731 -4.2781882 -4.2699752 -4.2155967][-4.2419386 -4.2656579 -4.2629018 -4.211916 -4.1191258 -3.9816494 -3.8119376 -3.7292151 -3.8503137 -4.0460725 -4.1897173 -4.26149 -4.2883019 -4.2710829 -4.2073369][-4.2355261 -4.2673745 -4.2739067 -4.2420011 -4.1806579 -4.0908628 -3.9817441 -3.9285667 -4.0037146 -4.1349216 -4.2305117 -4.2724953 -4.2831287 -4.25532 -4.1881866][-4.2293658 -4.263731 -4.2751026 -4.2555852 -4.21712 -4.1569719 -4.0893941 -4.0609059 -4.1088562 -4.1943374 -4.2566915 -4.2798166 -4.278666 -4.2426524 -4.1778235][-4.2333417 -4.2656579 -4.27672 -4.2642822 -4.2437439 -4.2050638 -4.1603279 -4.145751 -4.1810832 -4.2407928 -4.2845426 -4.2988153 -4.2914152 -4.25179 -4.194519][-4.2492332 -4.276125 -4.2846217 -4.279871 -4.274291 -4.2536964 -4.2233162 -4.2134256 -4.2402616 -4.2816119 -4.3099942 -4.3185105 -4.3105216 -4.2770982 -4.2317276][-4.2741818 -4.2932668 -4.2989755 -4.3006268 -4.3058834 -4.2998042 -4.2825856 -4.2745748 -4.2907829 -4.3156123 -4.3314133 -4.3347111 -4.3264332 -4.3038387 -4.274426]]...]
INFO - root - 2017-12-05 21:04:00.335914: step 44510, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 67h:17m:54s remains)
INFO - root - 2017-12-05 21:04:08.888853: step 44520, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 69h:25m:03s remains)
INFO - root - 2017-12-05 21:04:17.383110: step 44530, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 70h:59m:25s remains)
INFO - root - 2017-12-05 21:04:25.973339: step 44540, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 0.790 sec/batch; 63h:13m:04s remains)
INFO - root - 2017-12-05 21:04:34.468176: step 44550, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 70h:20m:37s remains)
INFO - root - 2017-12-05 21:04:42.832071: step 44560, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 71h:17m:05s remains)
INFO - root - 2017-12-05 21:04:51.403702: step 44570, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 68h:39m:54s remains)
INFO - root - 2017-12-05 21:05:00.007957: step 44580, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 68h:28m:26s remains)
INFO - root - 2017-12-05 21:05:08.451574: step 44590, loss = 2.03, batch loss = 1.98 (9.7 examples/sec; 0.824 sec/batch; 65h:55m:02s remains)
INFO - root - 2017-12-05 21:05:17.049971: step 44600, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 72h:46m:06s remains)
2017-12-05 21:05:17.825834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2981772 -4.2974987 -4.2975664 -4.2979603 -4.3013005 -4.3070159 -4.31203 -4.3148422 -4.314868 -4.3142304 -4.3151493 -4.3179874 -4.3224349 -4.3277044 -4.3305082][-4.2631531 -4.2603369 -4.2577829 -4.2570524 -4.2613206 -4.2701831 -4.2770944 -4.2803826 -4.2813425 -4.2835078 -4.2864184 -4.2902179 -4.2979636 -4.3077569 -4.3133874][-4.2243676 -4.2131233 -4.2029762 -4.2004242 -4.2066131 -4.2169523 -4.2228937 -4.2251191 -4.2296872 -4.2385163 -4.2456455 -4.25167 -4.26503 -4.2804537 -4.2906818][-4.1914263 -4.1668577 -4.1452069 -4.1396089 -4.146791 -4.1542473 -4.1537256 -4.1523046 -4.1646676 -4.1856942 -4.1993546 -4.209311 -4.2295594 -4.2493496 -4.2645488][-4.16608 -4.1307855 -4.0993423 -4.0908823 -4.0943503 -4.0932918 -4.0792451 -4.0707154 -4.0921092 -4.1291595 -4.1500258 -4.163455 -4.1911068 -4.2173839 -4.2404923][-4.1511931 -4.1122026 -4.0760918 -4.0627055 -4.0547428 -4.0358148 -3.9983277 -3.9805796 -4.0142903 -4.0728807 -4.1068573 -4.1253428 -4.1560354 -4.1889172 -4.2218709][-4.1471243 -4.1096687 -4.0737009 -4.05813 -4.0372944 -3.9952841 -3.9313593 -3.9083385 -3.9599493 -4.0389962 -4.0836105 -4.100369 -4.1240225 -4.15819 -4.1987948][-4.151567 -4.117466 -4.0844378 -4.0703616 -4.0496292 -3.9990063 -3.9231744 -3.9054017 -3.9681964 -4.046659 -4.0812469 -4.0801935 -4.0892053 -4.121594 -4.1662073][-4.1712222 -4.1401343 -4.1101327 -4.0971146 -4.0849571 -4.04577 -3.9849057 -3.9787769 -4.0331707 -4.0873022 -4.0969853 -4.074995 -4.0707779 -4.0970535 -4.1366143][-4.1933746 -4.1632771 -4.1351652 -4.1206684 -4.1167135 -4.0967813 -4.0621195 -4.0657635 -4.1007547 -4.1278806 -4.1159086 -4.0827866 -4.0725155 -4.0882649 -4.1137729][-4.2243237 -4.1962447 -4.1716061 -4.157578 -4.1579881 -4.1482739 -4.131969 -4.1388597 -4.1569619 -4.1660233 -4.1478472 -4.1211352 -4.1144691 -4.1201587 -4.12754][-4.2701044 -4.2448373 -4.2237058 -4.2123213 -4.2143264 -4.2096386 -4.2042203 -4.2106924 -4.2201529 -4.2233949 -4.2115774 -4.199379 -4.1987925 -4.1981206 -4.1914144][-4.3141541 -4.2956014 -4.278235 -4.2699118 -4.2723966 -4.2714629 -4.2737904 -4.2804232 -4.2888441 -4.2932806 -4.2884026 -4.2853894 -4.287127 -4.2833285 -4.2718196][-4.3424864 -4.3339572 -4.3253613 -4.3224216 -4.3253546 -4.3269911 -4.3312192 -4.3365526 -4.3426085 -4.3467145 -4.3455524 -4.3459792 -4.3480034 -4.343544 -4.3325167][-4.3532925 -4.351419 -4.3501725 -4.35123 -4.3539424 -4.3558507 -4.359045 -4.3615227 -4.3644843 -4.3669167 -4.36701 -4.3673811 -4.3682208 -4.3649325 -4.3577423]]...]
INFO - root - 2017-12-05 21:05:26.306409: step 44610, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 66h:35m:51s remains)
INFO - root - 2017-12-05 21:05:34.775858: step 44620, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 67h:31m:27s remains)
INFO - root - 2017-12-05 21:05:43.328710: step 44630, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 69h:56m:44s remains)
INFO - root - 2017-12-05 21:05:51.763713: step 44640, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 66h:28m:44s remains)
INFO - root - 2017-12-05 21:06:00.159410: step 44650, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 65h:48m:53s remains)
INFO - root - 2017-12-05 21:06:08.465595: step 44660, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 67h:06m:17s remains)
INFO - root - 2017-12-05 21:06:17.049147: step 44670, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 67h:11m:48s remains)
INFO - root - 2017-12-05 21:06:25.651790: step 44680, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 70h:36m:35s remains)
INFO - root - 2017-12-05 21:06:34.171741: step 44690, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 67h:23m:33s remains)
INFO - root - 2017-12-05 21:06:42.820000: step 44700, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 70h:13m:03s remains)
2017-12-05 21:06:43.550763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2946849 -4.3010459 -4.3011289 -4.2993646 -4.2979403 -4.2932034 -4.2864165 -4.2803669 -4.2746825 -4.2701364 -4.2694521 -4.2696085 -4.271843 -4.2786374 -4.2905169][-4.2627659 -4.2689071 -4.2676644 -4.2668514 -4.2665968 -4.2614145 -4.2508106 -4.2375107 -4.2236204 -4.2165031 -4.2197728 -4.2249589 -4.2292495 -4.2367492 -4.2525091][-4.2357631 -4.2393188 -4.2352881 -4.2340412 -4.2351694 -4.2312737 -4.216188 -4.1918435 -4.1670465 -4.1613464 -4.1728888 -4.1853666 -4.1927776 -4.2030959 -4.2244315][-4.2142787 -4.21645 -4.209641 -4.2065864 -4.2067795 -4.201519 -4.1776557 -4.1420746 -4.1159797 -4.1185026 -4.1344767 -4.1489754 -4.1610093 -4.1807575 -4.2106023][-4.1772027 -4.1842122 -4.1788177 -4.1760693 -4.1758904 -4.1665659 -4.1303673 -4.0874352 -4.0729485 -4.0893264 -4.1007133 -4.1059227 -4.1207833 -4.1568518 -4.2004881][-4.11928 -4.1352987 -4.1327724 -4.1272578 -4.1200547 -4.1016269 -4.0512152 -4.0101123 -4.0183764 -4.0510478 -4.0577784 -4.0501962 -4.0674186 -4.1194234 -4.1776209][-4.0731235 -4.0923157 -4.0858388 -4.0698838 -4.0508509 -4.0168962 -3.9492555 -3.9087987 -3.9354353 -3.9812634 -3.9887848 -3.9761505 -3.9981833 -4.0639386 -4.1355][-4.0555649 -4.0715823 -4.0565519 -4.0299463 -4.0023069 -3.9560688 -3.8727703 -3.8251286 -3.8585644 -3.9146729 -3.9234157 -3.9036853 -3.9300628 -4.0098944 -4.0903611][-4.0651622 -4.0740128 -4.0525908 -4.0298481 -4.0156016 -3.975266 -3.8953147 -3.8454177 -3.8679316 -3.9167938 -3.915349 -3.8844414 -3.90889 -3.9875844 -4.0594258][-4.0787354 -4.0864716 -4.0721917 -4.0692358 -4.0807762 -4.0682287 -4.0176644 -3.9832811 -3.9959953 -4.026391 -4.0086932 -3.9632988 -3.9700177 -4.0259314 -4.07315][-4.0882049 -4.0983462 -4.0995488 -4.1161633 -4.1481209 -4.1605697 -4.1423531 -4.1324611 -4.1473374 -4.1572266 -4.1214733 -4.066606 -4.0530567 -4.0852089 -4.1139164][-4.0894346 -4.1041565 -4.1225967 -4.1552453 -4.1955605 -4.2181625 -4.2182732 -4.2236843 -4.2374506 -4.2295547 -4.1791439 -4.1181993 -4.0968757 -4.11981 -4.1435995][-4.0894 -4.1059175 -4.13593 -4.1802034 -4.2226791 -4.2449093 -4.2522464 -4.2644939 -4.2783065 -4.2622514 -4.2061968 -4.1425776 -4.1185117 -4.1388845 -4.1601033][-4.1138597 -4.1279545 -4.1576042 -4.1988025 -4.2389889 -4.2596407 -4.2688065 -4.2838631 -4.2981062 -4.2846088 -4.2337794 -4.1725969 -4.1485949 -4.1630678 -4.1824064][-4.169683 -4.18279 -4.2058454 -4.233366 -4.262682 -4.2832832 -4.2945051 -4.3099174 -4.3234777 -4.3164392 -4.2792473 -4.2315812 -4.2111545 -4.2178063 -4.232429]]...]
INFO - root - 2017-12-05 21:06:51.957048: step 44710, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 67h:06m:34s remains)
INFO - root - 2017-12-05 21:07:00.532296: step 44720, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 68h:02m:12s remains)
INFO - root - 2017-12-05 21:07:08.980554: step 44730, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 67h:58m:41s remains)
INFO - root - 2017-12-05 21:07:17.440169: step 44740, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 69h:23m:26s remains)
INFO - root - 2017-12-05 21:07:25.855108: step 44750, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 67h:42m:31s remains)
INFO - root - 2017-12-05 21:07:34.255293: step 44760, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.788 sec/batch; 62h:57m:32s remains)
INFO - root - 2017-12-05 21:07:42.656419: step 44770, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 67h:23m:40s remains)
INFO - root - 2017-12-05 21:07:51.100248: step 44780, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 69h:54m:29s remains)
INFO - root - 2017-12-05 21:07:59.611316: step 44790, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.819 sec/batch; 65h:27m:42s remains)
INFO - root - 2017-12-05 21:08:08.263058: step 44800, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 70h:31m:45s remains)
2017-12-05 21:08:09.059724: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2633982 -4.2438588 -4.2420416 -4.2478328 -4.2508597 -4.2513824 -4.2494845 -4.2442813 -4.231185 -4.2017055 -4.1589947 -4.1179748 -4.0976629 -4.111692 -4.1625338][-4.2504444 -4.2300253 -4.2331548 -4.2474284 -4.2615337 -4.26935 -4.2686391 -4.2582412 -4.2366834 -4.1926045 -4.1355791 -4.0843768 -4.0666285 -4.0985184 -4.1714025][-4.2452097 -4.2254486 -4.2308025 -4.2476211 -4.2633419 -4.2699471 -4.2636709 -4.2483463 -4.2248034 -4.1798091 -4.1214371 -4.0750127 -4.0724478 -4.12381 -4.2023988][-4.2476211 -4.2298946 -4.2323866 -4.2432761 -4.2507625 -4.2495623 -4.2359614 -4.217452 -4.1990485 -4.1644959 -4.1214471 -4.0958495 -4.1118393 -4.1674027 -4.2344041][-4.2485437 -4.2282572 -4.2205243 -4.2172265 -4.2114639 -4.2001996 -4.1807742 -4.1609044 -4.1504412 -4.1349564 -4.11742 -4.1144476 -4.1416283 -4.1903486 -4.2422032][-4.24267 -4.2160649 -4.1975641 -4.18172 -4.1615191 -4.1340666 -4.1022515 -4.0784078 -4.0763211 -4.0783 -4.0858088 -4.1060042 -4.1417055 -4.1829419 -4.2197137][-4.2311463 -4.1962805 -4.1688542 -4.1434569 -4.1099687 -4.06098 -4.0107355 -3.9829471 -3.9882791 -4.0077319 -4.0437303 -4.0862894 -4.1246185 -4.1532726 -4.1752453][-4.2155967 -4.1756158 -4.1458931 -4.1178122 -4.0768752 -4.0191636 -3.9699218 -3.9561684 -3.9737806 -4.0023069 -4.0469422 -4.0886765 -4.1130939 -4.1234984 -4.1283603][-4.2031112 -4.1671648 -4.1435161 -4.119997 -4.0861683 -4.0486259 -4.0253129 -4.0293846 -4.051178 -4.0746989 -4.1041393 -4.1249452 -4.1276379 -4.1195631 -4.111043][-4.2109637 -4.1871729 -4.1756592 -4.1629186 -4.1441207 -4.13322 -4.1343966 -4.1434145 -4.1582661 -4.1694388 -4.1800766 -4.1849737 -4.1770992 -4.1603312 -4.1490703][-4.2424073 -4.2287636 -4.2251449 -4.2220826 -4.2176886 -4.2217836 -4.2317529 -4.241075 -4.24955 -4.252789 -4.2529025 -4.2515497 -4.2454 -4.2329593 -4.2266583][-4.2817707 -4.2722383 -4.2706633 -4.273654 -4.2794371 -4.2892179 -4.3004355 -4.3084793 -4.3138065 -4.3132477 -4.3093634 -4.3077984 -4.3070908 -4.3040595 -4.3018446][-4.3165631 -4.3101621 -4.3092132 -4.3139815 -4.3208723 -4.3290868 -4.337203 -4.3410363 -4.3424668 -4.34257 -4.3425951 -4.3445797 -4.347517 -4.3500109 -4.35095][-4.339004 -4.3358808 -4.3353119 -4.3380051 -4.3426633 -4.3480787 -4.3516731 -4.3521552 -4.3518453 -4.3525424 -4.3549819 -4.3582354 -4.3610249 -4.3637133 -4.3655558][-4.35141 -4.3501425 -4.3502684 -4.3523655 -4.3550997 -4.3575659 -4.3586226 -4.357954 -4.3570981 -4.3571706 -4.3582263 -4.3598871 -4.3609772 -4.362092 -4.3635988]]...]
INFO - root - 2017-12-05 21:08:17.723039: step 44810, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 69h:29m:47s remains)
INFO - root - 2017-12-05 21:08:26.309293: step 44820, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 68h:41m:34s remains)
INFO - root - 2017-12-05 21:08:34.861467: step 44830, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 67h:13m:39s remains)
INFO - root - 2017-12-05 21:08:43.446598: step 44840, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 68h:49m:29s remains)
INFO - root - 2017-12-05 21:08:52.034995: step 44850, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 68h:41m:30s remains)
INFO - root - 2017-12-05 21:09:00.392422: step 44860, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 67h:28m:17s remains)
INFO - root - 2017-12-05 21:09:08.805877: step 44870, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.795 sec/batch; 63h:28m:50s remains)
INFO - root - 2017-12-05 21:09:17.463909: step 44880, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 69h:41m:53s remains)
INFO - root - 2017-12-05 21:09:25.968983: step 44890, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 66h:49m:22s remains)
INFO - root - 2017-12-05 21:09:34.529690: step 44900, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 67h:53m:46s remains)
2017-12-05 21:09:35.366765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2740111 -4.2853537 -4.2824059 -4.2654033 -4.2513824 -4.2465315 -4.2458653 -4.2345386 -4.2242088 -4.22603 -4.2384062 -4.2561646 -4.2681007 -4.2701178 -4.2675796][-4.2584038 -4.268795 -4.2617826 -4.2422647 -4.2306466 -4.2298555 -4.228559 -4.209868 -4.1948967 -4.1980305 -4.2155113 -4.2408276 -4.2592111 -4.2622852 -4.26][-4.2367411 -4.2487135 -4.241889 -4.222456 -4.2130165 -4.2098508 -4.1977234 -4.1700616 -4.1551576 -4.1607656 -4.1791615 -4.2097893 -4.2367959 -4.2441087 -4.2441134][-4.21337 -4.2279887 -4.2231631 -4.2046375 -4.1953568 -4.1868968 -4.1610093 -4.1267457 -4.1125326 -4.1191883 -4.1372004 -4.1668963 -4.2012677 -4.2147727 -4.2175994][-4.1860237 -4.1992164 -4.2032642 -4.1905446 -4.1791229 -4.1600542 -4.1185684 -4.0731449 -4.0631232 -4.0800223 -4.1090536 -4.1439371 -4.1852679 -4.2040591 -4.2086306][-4.1509442 -4.1632261 -4.1768131 -4.1688628 -4.1506271 -4.1201305 -4.0554619 -3.9863429 -3.9756043 -4.0134573 -4.0631938 -4.1141424 -4.1688561 -4.1939883 -4.2031436][-4.1041794 -4.1178012 -4.1354346 -4.133172 -4.1126132 -4.0652547 -3.9596784 -3.8408928 -3.8292134 -3.9102986 -3.9953425 -4.06883 -4.1430779 -4.1822138 -4.194797][-4.0599394 -4.0758419 -4.0937514 -4.0931635 -4.0668683 -3.9967914 -3.8433862 -3.6618423 -3.6627519 -3.809485 -3.9331145 -4.0239377 -4.1150827 -4.16923 -4.18418][-4.058135 -4.076438 -4.0911708 -4.0822077 -4.043828 -3.9621382 -3.796021 -3.6021004 -3.6230564 -3.8023531 -3.9368742 -4.0280843 -4.1165347 -4.171504 -4.1812887][-4.0989165 -4.1149058 -4.1234584 -4.1074448 -4.0680881 -3.9999373 -3.8866639 -3.7730656 -3.7977376 -3.9173412 -4.0135617 -4.0817432 -4.147089 -4.1899881 -4.1911874][-4.1501675 -4.1604676 -4.1607366 -4.1411757 -4.10696 -4.0561771 -3.9966688 -3.9544394 -3.9775305 -4.0322437 -4.0934396 -4.1464968 -4.1908607 -4.21703 -4.21241][-4.1912937 -4.201642 -4.1971464 -4.1668115 -4.1305461 -4.0918703 -4.0602112 -4.0533853 -4.0770264 -4.1095033 -4.1546474 -4.2000575 -4.2300234 -4.2441711 -4.2392769][-4.2245603 -4.2344718 -4.2271032 -4.1898727 -4.1514897 -4.1206427 -4.0993128 -4.100523 -4.1251373 -4.1605787 -4.2027526 -4.2430954 -4.2654443 -4.2711792 -4.2672367][-4.238205 -4.2482443 -4.244463 -4.2144504 -4.1799622 -4.1573839 -4.1412549 -4.1366229 -4.157557 -4.1970148 -4.2371531 -4.2729187 -4.2915769 -4.2928462 -4.2894816][-4.2325158 -4.2456245 -4.2445064 -4.2262359 -4.20146 -4.1863317 -4.174108 -4.1669593 -4.1834173 -4.219697 -4.2550097 -4.28412 -4.2998943 -4.3005004 -4.2983871]]...]
INFO - root - 2017-12-05 21:09:44.122288: step 44910, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 71h:42m:21s remains)
INFO - root - 2017-12-05 21:09:52.749699: step 44920, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 70h:17m:59s remains)
INFO - root - 2017-12-05 21:10:01.375818: step 44930, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 72h:07m:43s remains)
INFO - root - 2017-12-05 21:10:09.928008: step 44940, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 68h:40m:07s remains)
INFO - root - 2017-12-05 21:10:18.492069: step 44950, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 67h:56m:47s remains)
INFO - root - 2017-12-05 21:10:26.911715: step 44960, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 70h:01m:51s remains)
INFO - root - 2017-12-05 21:10:35.607669: step 44970, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.965 sec/batch; 77h:06m:41s remains)
INFO - root - 2017-12-05 21:10:44.074484: step 44980, loss = 2.05, batch loss = 1.99 (10.2 examples/sec; 0.782 sec/batch; 62h:26m:09s remains)
INFO - root - 2017-12-05 21:10:52.656914: step 44990, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 66h:54m:15s remains)
INFO - root - 2017-12-05 21:11:01.208246: step 45000, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 67h:36m:30s remains)
2017-12-05 21:11:02.056734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1885605 -4.1726437 -4.1729445 -4.1929255 -4.209692 -4.22618 -4.2424459 -4.2441907 -4.2360196 -4.2225838 -4.2139726 -4.2132425 -4.2202063 -4.2356849 -4.2507][-4.1679344 -4.1541438 -4.1580563 -4.1755495 -4.1846294 -4.1969023 -4.2121243 -4.214416 -4.2076774 -4.1974154 -4.1871538 -4.1866546 -4.197361 -4.2117877 -4.2240539][-4.15223 -4.1359091 -4.1366162 -4.1441126 -4.1441031 -4.15168 -4.169148 -4.1779013 -4.1777954 -4.1717839 -4.1626019 -4.1608515 -4.169313 -4.1834435 -4.1971936][-4.1590567 -4.1415339 -4.1394467 -4.1355624 -4.122591 -4.1215396 -4.1374846 -4.1496935 -4.1545167 -4.1503186 -4.1398325 -4.1357551 -4.1404538 -4.1548071 -4.1729383][-4.1801405 -4.1656561 -4.1610627 -4.1453667 -4.1212583 -4.1118417 -4.1226974 -4.1358356 -4.1442065 -4.1377563 -4.1179733 -4.105495 -4.1108422 -4.1334629 -4.1559711][-4.1913381 -4.1778069 -4.1603141 -4.1235471 -4.0869284 -4.0699768 -4.082756 -4.1042566 -4.1151967 -4.1070056 -4.0832043 -4.0707865 -4.0825233 -4.1157413 -4.1387911][-4.189837 -4.1741209 -4.1433678 -4.089972 -4.0434651 -4.0227156 -4.0390768 -4.0728822 -4.0917 -4.0877271 -4.0676951 -4.0622482 -4.0762 -4.1074681 -4.1229324][-4.1860576 -4.1756458 -4.1484389 -4.0988026 -4.0548697 -4.0290561 -4.0414124 -4.0783997 -4.1030312 -4.1068711 -4.0938373 -4.0867143 -4.0943127 -4.1165514 -4.128274][-4.2011852 -4.2072434 -4.1992307 -4.1641254 -4.1237192 -4.0891757 -4.0897694 -4.1179237 -4.1356506 -4.1397285 -4.1288295 -4.1183238 -4.1227007 -4.1445856 -4.1544585][-4.2240276 -4.2444754 -4.2540617 -4.2349777 -4.1956229 -4.1521945 -4.1393762 -4.1519265 -4.1627011 -4.1673317 -4.1600046 -4.1491838 -4.1513824 -4.1736231 -4.1843367][-4.2307973 -4.2533121 -4.269052 -4.261147 -4.2286963 -4.1897063 -4.1723394 -4.1755285 -4.1846371 -4.1887 -4.1810107 -4.1719856 -4.1771765 -4.1989074 -4.2091789][-4.2185578 -4.2383728 -4.25446 -4.2523875 -4.2343359 -4.2090006 -4.1947803 -4.1951504 -4.1973495 -4.1947737 -4.1882567 -4.1857896 -4.1929359 -4.2093883 -4.2183495][-4.20898 -4.2204342 -4.2287245 -4.2283554 -4.2231417 -4.2132478 -4.2063055 -4.205586 -4.1983638 -4.1865468 -4.1795235 -4.1766715 -4.1810989 -4.1952653 -4.2089763][-4.210597 -4.2152977 -4.2181621 -4.2170997 -4.2166514 -4.216157 -4.2141042 -4.2140841 -4.2028952 -4.1841192 -4.1727805 -4.1679459 -4.1690044 -4.1803679 -4.1966505][-4.2341 -4.2352262 -4.2357497 -4.2326651 -4.2289038 -4.2275114 -4.2254739 -4.2276187 -4.2224607 -4.20587 -4.1926136 -4.1867685 -4.1868153 -4.1944647 -4.2090149]]...]
INFO - root - 2017-12-05 21:11:10.536934: step 45010, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 69h:16m:36s remains)
INFO - root - 2017-12-05 21:11:19.031422: step 45020, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 67h:45m:13s remains)
INFO - root - 2017-12-05 21:11:27.597230: step 45030, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.846 sec/batch; 67h:35m:39s remains)
INFO - root - 2017-12-05 21:11:36.100451: step 45040, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 69h:29m:53s remains)
INFO - root - 2017-12-05 21:11:44.639907: step 45050, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 66h:07m:42s remains)
INFO - root - 2017-12-05 21:11:53.117347: step 45060, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 67h:11m:23s remains)
INFO - root - 2017-12-05 21:12:01.860461: step 45070, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 68h:47m:32s remains)
INFO - root - 2017-12-05 21:12:10.365132: step 45080, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 66h:44m:21s remains)
INFO - root - 2017-12-05 21:12:18.743805: step 45090, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 0.781 sec/batch; 62h:21m:09s remains)
INFO - root - 2017-12-05 21:12:27.229078: step 45100, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 68h:44m:28s remains)
2017-12-05 21:12:28.011065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0966616 -4.1095295 -4.1219254 -4.125144 -4.1388006 -4.175096 -4.1929078 -4.1822991 -4.162158 -4.148591 -4.1436987 -4.1460924 -4.1679268 -4.1948781 -4.1995435][-3.9686947 -3.9946284 -4.0204053 -4.0284209 -4.0614662 -4.1335826 -4.1875482 -4.2011042 -4.1901693 -4.1733742 -4.1520638 -4.1293993 -4.12833 -4.1390076 -4.1330113][-3.9030848 -3.9338923 -3.9653015 -3.983758 -4.0314374 -4.1175089 -4.1802912 -4.1973138 -4.1882339 -4.1643744 -4.119422 -4.06954 -4.04714 -4.0424976 -4.0285845][-3.9589396 -3.9862776 -4.0139971 -4.0336051 -4.0745721 -4.1442938 -4.1953921 -4.2053609 -4.191031 -4.1580296 -4.0978651 -4.0336566 -3.9977252 -3.9780352 -3.967154][-4.0695639 -4.0844131 -4.0993237 -4.1112604 -4.1344643 -4.1785035 -4.2117896 -4.2155075 -4.2016945 -4.1746125 -4.1293793 -4.0818582 -4.0509129 -4.0273833 -4.018878][-4.1656632 -4.1669779 -4.1626444 -4.1546164 -4.1515183 -4.1669731 -4.1896811 -4.1945043 -4.1900883 -4.184525 -4.1751151 -4.1599741 -4.1412749 -4.1206961 -4.114192][-4.2285275 -4.218348 -4.1912532 -4.1529984 -4.1134348 -4.1005635 -4.1157212 -4.1251154 -4.1322455 -4.1540265 -4.1825833 -4.1954365 -4.1878257 -4.1755662 -4.1794887][-4.2618194 -4.23668 -4.1803341 -4.1032133 -4.0239258 -3.9892211 -3.9985526 -4.0141916 -4.0399976 -4.092844 -4.1535821 -4.1862917 -4.1920681 -4.1975288 -4.2171988][-4.275362 -4.2377009 -4.161633 -4.0593495 -3.9597969 -3.9138455 -3.9241736 -3.9535823 -3.9978611 -4.0661955 -4.1356339 -4.17526 -4.1952486 -4.2185907 -4.2483091][-4.2800131 -4.2435222 -4.1723046 -4.0790553 -3.9947696 -3.9603827 -3.973424 -4.0035663 -4.042902 -4.10053 -4.1606288 -4.2004652 -4.225934 -4.2543836 -4.282577][-4.2831278 -4.2590332 -4.2105742 -4.1515245 -4.0979714 -4.0749407 -4.0808568 -4.1014504 -4.1305532 -4.1726174 -4.2166758 -4.2480512 -4.2692022 -4.2915907 -4.3113632][-4.2884569 -4.2771139 -4.2507854 -4.2215061 -4.1941204 -4.1807284 -4.1823063 -4.1954136 -4.2154427 -4.2445641 -4.2723112 -4.2929292 -4.3073683 -4.3202024 -4.3283343][-4.2958369 -4.2962918 -4.2870903 -4.276979 -4.2658372 -4.2624893 -4.2647457 -4.2738461 -4.2865224 -4.3010497 -4.3128076 -4.3211021 -4.3263578 -4.330658 -4.330193][-4.3051658 -4.3160291 -4.3170676 -4.3165007 -4.3162684 -4.3191161 -4.3219285 -4.3253312 -4.329021 -4.3328524 -4.3347459 -4.3344741 -4.3319464 -4.3286166 -4.3213234][-4.3183465 -4.3361592 -4.3419766 -4.3444991 -4.3468294 -4.34934 -4.3500853 -4.3483381 -4.3440995 -4.3408971 -4.3381481 -4.3351026 -4.3316245 -4.3261318 -4.3168879]]...]
INFO - root - 2017-12-05 21:12:36.589585: step 45110, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.862 sec/batch; 68h:47m:31s remains)
INFO - root - 2017-12-05 21:12:45.262576: step 45120, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 67h:46m:02s remains)
INFO - root - 2017-12-05 21:12:53.784989: step 45130, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 67h:39m:07s remains)
INFO - root - 2017-12-05 21:13:02.220849: step 45140, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 67h:35m:22s remains)
INFO - root - 2017-12-05 21:13:10.795491: step 45150, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 67h:07m:41s remains)
INFO - root - 2017-12-05 21:13:19.249189: step 45160, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 68h:21m:54s remains)
INFO - root - 2017-12-05 21:13:28.065275: step 45170, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 70h:41m:27s remains)
INFO - root - 2017-12-05 21:13:36.664850: step 45180, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.884 sec/batch; 70h:31m:11s remains)
INFO - root - 2017-12-05 21:13:45.180716: step 45190, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 70h:25m:03s remains)
INFO - root - 2017-12-05 21:13:53.675546: step 45200, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.816 sec/batch; 65h:08m:29s remains)
2017-12-05 21:13:54.464338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3081894 -4.3150377 -4.3113542 -4.3020887 -4.296155 -4.2934165 -4.29183 -4.2921162 -4.2886133 -4.2845325 -4.2787046 -4.2682076 -4.25631 -4.252594 -4.2566562][-4.30242 -4.3125463 -4.3131676 -4.3100896 -4.308145 -4.3052931 -4.298738 -4.2932115 -4.2871718 -4.2800517 -4.2708073 -4.2587781 -4.2491522 -4.24791 -4.2517438][-4.2874742 -4.2998252 -4.3032742 -4.303628 -4.3034968 -4.2989588 -4.2882609 -4.279695 -4.2739339 -4.2686296 -4.2623062 -4.2544966 -4.2494545 -4.2499352 -4.253366][-4.2705526 -4.2838306 -4.2881031 -4.2884364 -4.2851825 -4.2750297 -4.2572279 -4.2406716 -4.2338605 -4.2352052 -4.2393727 -4.2421908 -4.2458167 -4.2507195 -4.2568388][-4.2635212 -4.2770662 -4.2798934 -4.2740664 -4.2589269 -4.2335825 -4.1990275 -4.1664782 -4.1572766 -4.1727762 -4.1965594 -4.2146678 -4.2310462 -4.24659 -4.2591748][-4.2672997 -4.2781243 -4.272943 -4.25488 -4.222671 -4.1740117 -4.1143837 -4.0589452 -4.0449934 -4.0785489 -4.1270704 -4.1646566 -4.1943474 -4.2247334 -4.2486463][-4.2709584 -4.2745781 -4.25842 -4.2273908 -4.1772757 -4.1055913 -4.0154614 -3.9282866 -3.9017823 -3.956116 -4.03364 -4.0930424 -4.1369252 -4.1799593 -4.2166672][-4.2690787 -4.2640109 -4.2381077 -4.1954584 -4.1314049 -4.0423455 -3.9337754 -3.8291535 -3.800041 -3.8716052 -3.9623139 -4.0304637 -4.0856495 -4.1406326 -4.1892843][-4.2675452 -4.2558131 -4.2250366 -4.1802397 -4.1181593 -4.0394087 -3.9528358 -3.873524 -3.85631 -3.9103842 -3.9767771 -4.0281382 -4.0755749 -4.1320186 -4.1873446][-4.2731328 -4.2631383 -4.2390847 -4.2051864 -4.1601715 -4.1116757 -4.0649056 -4.0213003 -4.0144167 -4.0422325 -4.0730076 -4.0952845 -4.1196728 -4.1616278 -4.2101374][-4.2885795 -4.2856555 -4.2740269 -4.2565012 -4.2345772 -4.2122097 -4.1931372 -4.1719346 -4.1682682 -4.1784372 -4.1845388 -4.1854191 -4.1881022 -4.2073727 -4.2392464][-4.2977138 -4.3018107 -4.2999654 -4.296144 -4.2888803 -4.2806654 -4.275579 -4.2692065 -4.2701254 -4.2725139 -4.2695003 -4.26049 -4.2494493 -4.2486033 -4.2619781][-4.2766032 -4.2872415 -4.2952209 -4.3016639 -4.3034983 -4.3028831 -4.3053818 -4.3070145 -4.3103223 -4.310214 -4.3041515 -4.2921209 -4.2748113 -4.261323 -4.2598748][-4.2333384 -4.2506561 -4.2656779 -4.2780766 -4.2848463 -4.2881808 -4.2935629 -4.29896 -4.3037686 -4.3018417 -4.2926288 -4.2780647 -4.2600965 -4.244844 -4.2377443][-4.1996346 -4.216455 -4.2308183 -4.2440467 -4.2532325 -4.2598057 -4.2675543 -4.2751374 -4.2810564 -4.2785287 -4.2678237 -4.2523956 -4.2373881 -4.22748 -4.2233534]]...]
INFO - root - 2017-12-05 21:14:03.140558: step 45210, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 65h:25m:25s remains)
INFO - root - 2017-12-05 21:14:11.728631: step 45220, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 68h:33m:18s remains)
INFO - root - 2017-12-05 21:14:20.357997: step 45230, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.864 sec/batch; 68h:58m:02s remains)
INFO - root - 2017-12-05 21:14:28.917468: step 45240, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 67h:47m:18s remains)
INFO - root - 2017-12-05 21:14:37.405412: step 45250, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 66h:38m:21s remains)
INFO - root - 2017-12-05 21:14:45.810768: step 45260, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 68h:08m:46s remains)
INFO - root - 2017-12-05 21:14:54.380755: step 45270, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 68h:29m:17s remains)
INFO - root - 2017-12-05 21:15:02.913013: step 45280, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 66h:12m:47s remains)
INFO - root - 2017-12-05 21:15:11.415857: step 45290, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 70h:16m:11s remains)
INFO - root - 2017-12-05 21:15:19.947182: step 45300, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 67h:13m:20s remains)
2017-12-05 21:15:20.741907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1681504 -4.1568384 -4.14132 -4.145443 -4.1690359 -4.1874743 -4.1914654 -4.1898508 -4.1909852 -4.1979885 -4.216382 -4.2380195 -4.2573538 -4.2803049 -4.3032613][-4.1700144 -4.1576557 -4.1474862 -4.1545668 -4.1733575 -4.185658 -4.1839547 -4.1813979 -4.18623 -4.1973968 -4.21737 -4.23806 -4.2582979 -4.2823353 -4.3059177][-4.15627 -4.144958 -4.1420197 -4.1516714 -4.1632895 -4.1661777 -4.1595836 -4.1574669 -4.1676707 -4.1840887 -4.2066402 -4.2265806 -4.2473516 -4.27352 -4.3001008][-4.1360173 -4.1282444 -4.1332273 -4.144712 -4.1503711 -4.1463947 -4.139051 -4.1386304 -4.1517043 -4.1726494 -4.1989121 -4.2199454 -4.2408147 -4.2681341 -4.297102][-4.1170416 -4.1134853 -4.1221914 -4.132401 -4.1328216 -4.126946 -4.1230245 -4.12605 -4.140305 -4.1641788 -4.192524 -4.2139821 -4.2339687 -4.2617497 -4.2933288][-4.10185 -4.0993366 -4.1061306 -4.1104703 -4.1067762 -4.1040063 -4.1079249 -4.116354 -4.1296735 -4.1536379 -4.1823497 -4.2031918 -4.22218 -4.2512965 -4.2858925][-4.090497 -4.0901484 -4.0912867 -4.0842338 -4.0761714 -4.0809708 -4.0973492 -4.1146989 -4.1291237 -4.1520953 -4.1791568 -4.1976376 -4.2142558 -4.2437377 -4.2803125][-4.076117 -4.0810819 -4.0814767 -4.0686626 -4.058166 -4.0685086 -4.0943494 -4.1184945 -4.1338282 -4.1551785 -4.1812081 -4.1982312 -4.2144365 -4.2447181 -4.2816153][-4.0789614 -4.0878897 -4.0913095 -4.0818911 -4.0772223 -4.0930495 -4.1240792 -4.1516747 -4.1665092 -4.1833811 -4.204145 -4.2168064 -4.230598 -4.25789 -4.2911973][-4.1082921 -4.1122189 -4.1124892 -4.1047912 -4.10515 -4.1232557 -4.154511 -4.1817017 -4.1953692 -4.2099595 -4.2298846 -4.2431521 -4.2554913 -4.2772822 -4.3041587][-4.1319313 -4.1317167 -4.1272321 -4.1191864 -4.11984 -4.1358838 -4.1610875 -4.1818919 -4.1924586 -4.2088509 -4.2334943 -4.2531061 -4.2677922 -4.2871127 -4.3106346][-4.1377559 -4.136518 -4.129395 -4.1231508 -4.1249275 -4.136971 -4.153616 -4.1630745 -4.1666555 -4.1824203 -4.2112937 -4.2381611 -4.2581549 -4.2801261 -4.3054581][-4.1308813 -4.1274509 -4.11718 -4.110981 -4.1160288 -4.1299148 -4.14448 -4.1463742 -4.1409283 -4.1513886 -4.1797118 -4.2093682 -4.2337108 -4.2614279 -4.2932487][-4.1375427 -4.1332245 -4.1220956 -4.1153293 -4.1217132 -4.1367097 -4.1512113 -4.152133 -4.1430883 -4.1467705 -4.1674843 -4.1903119 -4.210969 -4.2406979 -4.2785692][-4.1589494 -4.1582346 -4.152081 -4.1498408 -4.1577592 -4.1709957 -4.1837296 -4.1858439 -4.178153 -4.1783733 -4.1896853 -4.2001667 -4.2099838 -4.2335224 -4.2705283]]...]
INFO - root - 2017-12-05 21:15:29.187262: step 45310, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 0.789 sec/batch; 62h:56m:40s remains)
INFO - root - 2017-12-05 21:15:37.698010: step 45320, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.858 sec/batch; 68h:26m:11s remains)
INFO - root - 2017-12-05 21:15:46.310848: step 45330, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 67h:13m:09s remains)
INFO - root - 2017-12-05 21:15:54.981774: step 45340, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 67h:30m:53s remains)
INFO - root - 2017-12-05 21:16:03.579602: step 45350, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 68h:10m:47s remains)
INFO - root - 2017-12-05 21:16:12.003104: step 45360, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 70h:07m:55s remains)
INFO - root - 2017-12-05 21:16:20.501469: step 45370, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 68h:20m:54s remains)
INFO - root - 2017-12-05 21:16:29.162564: step 45380, loss = 2.11, batch loss = 2.06 (9.3 examples/sec; 0.856 sec/batch; 68h:17m:17s remains)
INFO - root - 2017-12-05 21:16:37.713329: step 45390, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 67h:13m:47s remains)
INFO - root - 2017-12-05 21:16:46.448316: step 45400, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 67h:15m:50s remains)
2017-12-05 21:16:47.242397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2637539 -4.2618442 -4.2605758 -4.2604704 -4.2621717 -4.265667 -4.2687378 -4.2699838 -4.2697253 -4.2723308 -4.277451 -4.2748313 -4.2605929 -4.2309527 -4.2098174][-4.2580247 -4.2553926 -4.2535558 -4.2529655 -4.2551584 -4.2605443 -4.2657709 -4.2678633 -4.2678108 -4.2704716 -4.2755446 -4.2735329 -4.2609544 -4.2386079 -4.2274752][-4.2379322 -4.2334857 -4.23051 -4.22862 -4.2322068 -4.2412457 -4.2502742 -4.2546935 -4.2563162 -4.2605314 -4.2658386 -4.2665339 -4.2586703 -4.2471876 -4.2467222][-4.2093883 -4.1999722 -4.1932583 -4.189846 -4.1970358 -4.2128096 -4.2276096 -4.2348685 -4.2388797 -4.2451148 -4.2521334 -4.2568555 -4.2540874 -4.2518 -4.2600756][-4.1771884 -4.1614704 -4.1492329 -4.1440377 -4.1562719 -4.1785851 -4.1966567 -4.2062516 -4.2137413 -4.2234507 -4.2347097 -4.2420316 -4.2421026 -4.2441769 -4.2572412][-4.1448069 -4.1219816 -4.1016269 -4.0916438 -4.1055942 -4.1297503 -4.1458392 -4.1546803 -4.1659293 -4.181891 -4.2013531 -4.2140741 -4.2159963 -4.2191672 -4.236763][-4.1244631 -4.09461 -4.0641432 -4.0458217 -4.0596323 -4.0836759 -4.0964546 -4.1021471 -4.1141396 -4.1363807 -4.1645794 -4.1852045 -4.191051 -4.1940985 -4.216104][-4.1154327 -4.0854945 -4.0504832 -4.0256186 -4.0356493 -4.057785 -4.068676 -4.072763 -4.0837765 -4.1105328 -4.1466012 -4.1742764 -4.1829438 -4.1820431 -4.2015004][-4.1235595 -4.1051464 -4.0779843 -4.0551224 -4.0609779 -4.077549 -4.0875087 -4.0916595 -4.1007767 -4.1267672 -4.1639628 -4.1920056 -4.1977458 -4.1912303 -4.2029963][-4.1451025 -4.1413116 -4.1301003 -4.11532 -4.116384 -4.1263947 -4.1353383 -4.1393194 -4.146328 -4.1686621 -4.1993375 -4.2231145 -4.2260842 -4.2169876 -4.2207422][-4.1847787 -4.18964 -4.1898932 -4.18282 -4.1791945 -4.18528 -4.1946964 -4.1999702 -4.2056684 -4.2209578 -4.23911 -4.2526922 -4.2522902 -4.2415009 -4.2399578][-4.2332158 -4.2412181 -4.2443738 -4.2418776 -4.2385182 -4.2434373 -4.2513447 -4.2574492 -4.2637672 -4.2725906 -4.2796369 -4.2822771 -4.2742972 -4.2600331 -4.2550497][-4.26684 -4.2758574 -4.2775264 -4.2760968 -4.2759132 -4.28048 -4.2859926 -4.2921014 -4.2987971 -4.3052006 -4.3070393 -4.2998919 -4.2809849 -4.25984 -4.2525811][-4.2713013 -4.2777233 -4.2767572 -4.2759843 -4.2774858 -4.2799897 -4.2831206 -4.2871237 -4.2913084 -4.2959127 -4.2971449 -4.2865591 -4.2610297 -4.2379971 -4.2327118][-4.2629423 -4.2666569 -4.264565 -4.2643518 -4.2663841 -4.2689352 -4.2707944 -4.2736006 -4.2758646 -4.279861 -4.2814026 -4.2718611 -4.2455058 -4.2215247 -4.2151537]]...]
INFO - root - 2017-12-05 21:16:55.774684: step 45410, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 67h:37m:13s remains)
INFO - root - 2017-12-05 21:17:04.314213: step 45420, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 0.794 sec/batch; 63h:20m:31s remains)
INFO - root - 2017-12-05 21:17:12.913935: step 45430, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.887 sec/batch; 70h:41m:55s remains)
INFO - root - 2017-12-05 21:17:21.541654: step 45440, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 68h:11m:16s remains)
INFO - root - 2017-12-05 21:17:30.131559: step 45450, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 66h:55m:23s remains)
INFO - root - 2017-12-05 21:17:38.598300: step 45460, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 70h:05m:15s remains)
INFO - root - 2017-12-05 21:17:47.294930: step 45470, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 67h:09m:30s remains)
INFO - root - 2017-12-05 21:17:55.959569: step 45480, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 71h:22m:48s remains)
INFO - root - 2017-12-05 21:18:04.449702: step 45490, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 65h:31m:02s remains)
INFO - root - 2017-12-05 21:18:13.090258: step 45500, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.905 sec/batch; 72h:07m:14s remains)
2017-12-05 21:18:13.918038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.123035 -4.1428895 -4.1498833 -4.1374822 -4.1306267 -4.1155653 -4.083889 -4.0675826 -4.0865679 -4.109158 -4.117228 -4.116838 -4.1191864 -4.1179223 -4.1195035][-4.0909934 -4.11147 -4.1270146 -4.116724 -4.1051426 -4.0911551 -4.066226 -4.0546584 -4.0701609 -4.088459 -4.0909405 -4.0908318 -4.0928073 -4.0864239 -4.0825567][-4.0688276 -4.0908022 -4.1143346 -4.1035647 -4.08612 -4.0758095 -4.0640178 -4.0574932 -4.0683923 -4.0808845 -4.0826478 -4.0801525 -4.0733719 -4.06025 -4.0522594][-4.0617919 -4.0837049 -4.1108713 -4.1026635 -4.0894737 -4.0873265 -4.0884943 -4.0850015 -4.089262 -4.0966506 -4.0950084 -4.082983 -4.0668793 -4.0524664 -4.0447025][-4.0570407 -4.0726204 -4.0999856 -4.1012135 -4.0955367 -4.0915508 -4.0893059 -4.08421 -4.0827022 -4.0897646 -4.0892711 -4.0702724 -4.0511203 -4.048913 -4.055398][-4.0510716 -4.0573163 -4.0773339 -4.0834689 -4.0820756 -4.0629025 -4.0378008 -4.0194736 -4.0188665 -4.0442286 -4.0575352 -4.043829 -4.0361814 -4.0555224 -4.0819106][-4.0508704 -4.0630064 -4.0833182 -4.0908985 -4.08864 -4.0544596 -4.0012078 -3.9607809 -3.9596534 -4.001543 -4.0252757 -4.0222516 -4.0304317 -4.0632281 -4.1005616][-4.0439692 -4.068018 -4.0885892 -4.093997 -4.0861259 -4.04769 -3.9840906 -3.934762 -3.9358666 -3.9830604 -4.0120845 -4.0233517 -4.0480175 -4.0849404 -4.1180019][-4.0260015 -4.0577855 -4.0758162 -4.0735197 -4.0600233 -4.0259423 -3.9771442 -3.9501207 -3.9672894 -4.0156446 -4.0466146 -4.0673103 -4.0956473 -4.1195507 -4.1366887][-4.0181546 -4.050487 -4.0681233 -4.0681791 -4.0565271 -4.0329852 -4.0068903 -4.004828 -4.0326357 -4.0699439 -4.0957813 -4.1142917 -4.1285167 -4.1256537 -4.1201773][-4.033844 -4.0612707 -4.08207 -4.0951185 -4.0950117 -4.0806346 -4.06232 -4.0671787 -4.0928364 -4.1192908 -4.1334071 -4.136694 -4.1306133 -4.1061378 -4.0855908][-4.0779419 -4.0932703 -4.1032977 -4.1207509 -4.1326427 -4.12462 -4.1039386 -4.1043167 -4.1238346 -4.1387744 -4.1375065 -4.1278152 -4.1117964 -4.0797873 -4.0608544][-4.118495 -4.1218915 -4.1102619 -4.1189551 -4.1405654 -4.13937 -4.1183581 -4.1190548 -4.1384721 -4.1436696 -4.1264644 -4.1097512 -4.09439 -4.0731487 -4.0661869][-4.1537395 -4.1456766 -4.1186867 -4.1168084 -4.1392226 -4.1460466 -4.1275635 -4.1230345 -4.1392994 -4.1439166 -4.1267719 -4.1061597 -4.0946603 -4.0897508 -4.0956407][-4.1782255 -4.1655273 -4.1415973 -4.1414161 -4.1616387 -4.1784434 -4.16676 -4.1500297 -4.1608906 -4.170516 -4.1595764 -4.141017 -4.1316738 -4.1390238 -4.1516304]]...]
INFO - root - 2017-12-05 21:18:22.327851: step 45510, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 69h:31m:03s remains)
INFO - root - 2017-12-05 21:18:31.014316: step 45520, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 67h:53m:09s remains)
INFO - root - 2017-12-05 21:18:39.512071: step 45530, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 0.783 sec/batch; 62h:25m:26s remains)
INFO - root - 2017-12-05 21:18:48.059259: step 45540, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 67h:11m:12s remains)
INFO - root - 2017-12-05 21:18:56.481811: step 45550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 68h:24m:26s remains)
INFO - root - 2017-12-05 21:19:04.842170: step 45560, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 68h:08m:29s remains)
INFO - root - 2017-12-05 21:19:13.335237: step 45570, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 68h:38m:05s remains)
INFO - root - 2017-12-05 21:19:21.872113: step 45580, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 68h:02m:13s remains)
INFO - root - 2017-12-05 21:19:30.463420: step 45590, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 68h:38m:47s remains)
INFO - root - 2017-12-05 21:19:39.048573: step 45600, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 67h:20m:04s remains)
2017-12-05 21:19:39.875492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2729187 -4.2852902 -4.2878222 -4.286046 -4.2901926 -4.2926784 -4.293148 -4.2916651 -4.2898707 -4.2909203 -4.2948503 -4.2992396 -4.3015904 -4.3031569 -4.2974353][-4.2430897 -4.2468786 -4.2474113 -4.25159 -4.2662392 -4.2769656 -4.2818747 -4.2822013 -4.2785344 -4.2777 -4.2820549 -4.2873178 -4.2902927 -4.2922182 -4.2866149][-4.2058735 -4.1928978 -4.1880579 -4.1998444 -4.2299809 -4.2527928 -4.2668557 -4.2743754 -4.274168 -4.2744765 -4.2796831 -4.2842216 -4.2848387 -4.2855816 -4.2786613][-4.1825981 -4.1497765 -4.1323972 -4.143014 -4.1804814 -4.2133856 -4.2352867 -4.2520213 -4.2631364 -4.273962 -4.2854695 -4.2913966 -4.2906795 -4.290113 -4.283391][-4.1880727 -4.1428661 -4.1088953 -4.1026058 -4.1243372 -4.1500831 -4.1712136 -4.1950192 -4.2253461 -4.2547922 -4.2779641 -4.2896881 -4.2943845 -4.2969627 -4.2946134][-4.1988463 -4.1591148 -4.1177621 -4.0862322 -4.0672812 -4.053225 -4.040801 -4.0575385 -4.1108155 -4.1726451 -4.2208819 -4.2495055 -4.2684469 -4.2810411 -4.2875462][-4.2043314 -4.1837459 -4.150281 -4.1004739 -4.03539 -3.9571416 -3.8706081 -3.8472087 -3.9287069 -4.0384073 -4.1227751 -4.1736631 -4.2069359 -4.2296805 -4.2468696][-4.2034039 -4.2141461 -4.2045841 -4.1572223 -4.0720234 -3.9507327 -3.7920406 -3.7060058 -3.7996662 -3.9360201 -4.0394921 -4.0957074 -4.1274443 -4.1480942 -4.1685128][-4.1790795 -4.2215691 -4.2440405 -4.2253838 -4.160666 -4.0564671 -3.9126806 -3.8187635 -3.8600974 -3.9536867 -4.029542 -4.0622458 -4.0739536 -4.0787058 -4.0874195][-4.1287904 -4.1910043 -4.2408338 -4.2600164 -4.2369061 -4.1763139 -4.0816517 -4.0056996 -4.0005236 -4.0396729 -4.0754185 -4.079988 -4.0676804 -4.0529823 -4.0421462][-4.07373 -4.1377783 -4.2054582 -4.2560658 -4.2673955 -4.2420206 -4.187901 -4.1302791 -4.1073446 -4.1167331 -4.1293473 -4.1212296 -4.0998664 -4.0740552 -4.0435481][-4.048542 -4.093019 -4.1580753 -4.2178392 -4.2488866 -4.2494392 -4.2278495 -4.1888924 -4.1626582 -4.1576567 -4.1633549 -4.1571417 -4.1416631 -4.1222191 -4.0890274][-4.0716076 -4.0900555 -4.1318083 -4.1745534 -4.2039785 -4.2209544 -4.2308364 -4.2188907 -4.1970253 -4.1799321 -4.1769853 -4.1708875 -4.1681557 -4.1711531 -4.1554403][-4.1454048 -4.1509032 -4.1664462 -4.18041 -4.1906104 -4.2075033 -4.2344875 -4.2422152 -4.2262659 -4.20417 -4.1945672 -4.1916924 -4.2012911 -4.2207108 -4.2208343][-4.242794 -4.2499657 -4.2518511 -4.2479053 -4.2443905 -4.2500381 -4.2700577 -4.2805209 -4.2699761 -4.251318 -4.2445321 -4.2480187 -4.2608151 -4.2792344 -4.2846532]]...]
INFO - root - 2017-12-05 21:19:48.293884: step 45610, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 68h:22m:30s remains)
INFO - root - 2017-12-05 21:19:56.766930: step 45620, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 66h:44m:56s remains)
INFO - root - 2017-12-05 21:20:05.264421: step 45630, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 68h:11m:12s remains)
INFO - root - 2017-12-05 21:20:13.827682: step 45640, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 67h:39m:26s remains)
INFO - root - 2017-12-05 21:20:22.352923: step 45650, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 67h:58m:45s remains)
INFO - root - 2017-12-05 21:20:30.783201: step 45660, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 70h:13m:16s remains)
INFO - root - 2017-12-05 21:20:39.323201: step 45670, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 67h:30m:30s remains)
INFO - root - 2017-12-05 21:20:47.929863: step 45680, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 67h:46m:52s remains)
INFO - root - 2017-12-05 21:20:56.542193: step 45690, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 68h:53m:50s remains)
INFO - root - 2017-12-05 21:21:05.055186: step 45700, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 70h:07m:36s remains)
2017-12-05 21:21:05.846819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3362255 -4.3320613 -4.327733 -4.322978 -4.3187852 -4.3155384 -4.3145547 -4.3167639 -4.3210049 -4.3226538 -4.3218493 -4.3218665 -4.3222194 -4.321909 -4.32153][-4.319418 -4.3106771 -4.3010573 -4.292274 -4.2867293 -4.2818203 -4.2795324 -4.2821155 -4.2888303 -4.2915049 -4.2897205 -4.2899327 -4.2907243 -4.2906041 -4.2907639][-4.2941518 -4.2779269 -4.2594318 -4.2440963 -4.2387457 -4.2356057 -4.2343988 -4.2380505 -4.2479596 -4.2523828 -4.249289 -4.2486615 -4.2495532 -4.2498922 -4.2515721][-4.2640476 -4.2376251 -4.2080126 -4.1854558 -4.1805673 -4.1811028 -4.1823688 -4.1884222 -4.2009277 -4.2058358 -4.2017794 -4.2012172 -4.2012115 -4.2018042 -4.2061157][-4.238133 -4.2015352 -4.1615219 -4.1319871 -4.1288748 -4.132318 -4.1360273 -4.1437683 -4.15917 -4.166347 -4.1645918 -4.1652784 -4.1651464 -4.1669283 -4.1738658][-4.221024 -4.1744657 -4.1227932 -4.0831146 -4.0801177 -4.0846133 -4.08694 -4.0930476 -4.1076307 -4.1170607 -4.1188293 -4.1215725 -4.11915 -4.1186314 -4.1236424][-4.2135277 -4.1568022 -4.0877786 -4.0323014 -4.0254941 -4.0276957 -4.0225596 -4.021359 -4.0341997 -4.0434728 -4.0481386 -4.0513396 -4.0447469 -4.0379252 -4.0378284][-4.2164207 -4.1550322 -4.0763168 -4.01133 -4.0011387 -3.99963 -3.9851108 -3.9750161 -3.9853601 -3.9951444 -4.0004468 -4.0024858 -3.9927216 -3.9828265 -3.9805803][-4.22877 -4.1746559 -4.111516 -4.0625944 -4.0515523 -4.0409212 -4.0188379 -4.0015674 -4.0065866 -4.0168529 -4.0218673 -4.0196333 -4.0071006 -3.996264 -3.9918413][-4.2421465 -4.1983714 -4.153872 -4.1245642 -4.1155658 -4.0990658 -4.0752749 -4.0574851 -4.0614672 -4.0744691 -4.0793605 -4.0730624 -4.055594 -4.0347548 -4.017899][-4.2518086 -4.2136741 -4.1808882 -4.1634235 -4.1544676 -4.1359777 -4.1147137 -4.100956 -4.109138 -4.1309409 -4.1390481 -4.1313095 -4.1092663 -4.0822721 -4.0566397][-4.2630544 -4.229413 -4.2026024 -4.1896319 -4.1796126 -4.1624126 -4.1451187 -4.1356583 -4.1467247 -4.1728764 -4.1821876 -4.173655 -4.1533828 -4.1301351 -4.108881][-4.2760434 -4.2472057 -4.2239542 -4.2116194 -4.2005925 -4.1838331 -4.1663408 -4.1580405 -4.1706815 -4.1968579 -4.2073135 -4.20333 -4.1941662 -4.1842265 -4.1756597][-4.2922106 -4.2695727 -4.2506466 -4.2390084 -4.2296028 -4.2145452 -4.1966071 -4.1901054 -4.2046118 -4.2262411 -4.2340527 -4.2339044 -4.2339473 -4.2354264 -4.2381296][-4.3105183 -4.2946677 -4.2794809 -4.2697587 -4.2631106 -4.2503271 -4.2337828 -4.2315688 -4.2478013 -4.2651067 -4.2706223 -4.2729387 -4.2758789 -4.2812352 -4.2864423]]...]
INFO - root - 2017-12-05 21:21:14.477349: step 45710, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 67h:43m:56s remains)
INFO - root - 2017-12-05 21:21:22.951304: step 45720, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 66h:43m:15s remains)
INFO - root - 2017-12-05 21:21:31.340456: step 45730, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 67h:54m:25s remains)
INFO - root - 2017-12-05 21:21:39.799114: step 45740, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 67h:56m:19s remains)
INFO - root - 2017-12-05 21:21:48.326231: step 45750, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.838 sec/batch; 66h:43m:04s remains)
INFO - root - 2017-12-05 21:21:56.706621: step 45760, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 68h:05m:05s remains)
INFO - root - 2017-12-05 21:22:05.147401: step 45770, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 66h:35m:48s remains)
INFO - root - 2017-12-05 21:22:13.671111: step 45780, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 65h:36m:45s remains)
INFO - root - 2017-12-05 21:22:22.138088: step 45790, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 67h:05m:31s remains)
INFO - root - 2017-12-05 21:22:30.714055: step 45800, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 68h:22m:27s remains)
2017-12-05 21:22:31.500745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.213614 -4.2175021 -4.2182822 -4.2210064 -4.2170434 -4.2093077 -4.1995807 -4.1927905 -4.1830506 -4.1723762 -4.1687193 -4.1699114 -4.169261 -4.16571 -4.1577888][-4.2112689 -4.2175555 -4.2212543 -4.225276 -4.2226634 -4.2165194 -4.2089996 -4.1998043 -4.1853676 -4.1729932 -4.1695356 -4.1722846 -4.171411 -4.1669736 -4.1635652][-4.2077637 -4.2108369 -4.2135282 -4.2150578 -4.2112513 -4.2058306 -4.1982656 -4.1826534 -4.1593142 -4.1461143 -4.1500278 -4.1630135 -4.1697145 -4.1718841 -4.1751642][-4.2019458 -4.1961 -4.1930552 -4.191288 -4.1868334 -4.1823926 -4.1757183 -4.1509895 -4.1109028 -4.0904155 -4.0990338 -4.121943 -4.1405911 -4.155303 -4.169354][-4.1884017 -4.1705675 -4.1591687 -4.15292 -4.1479278 -4.1432152 -4.1341124 -4.0954614 -4.0357966 -4.003633 -4.0161872 -4.0518146 -4.0825324 -4.1065316 -4.1248493][-4.1663375 -4.1348228 -4.1143885 -4.1043272 -4.0969071 -4.0909796 -4.0770159 -4.0231619 -3.9403355 -3.8892357 -3.9020591 -3.9500992 -3.9998987 -4.0376468 -4.0631146][-4.1598339 -4.1203628 -4.0921712 -4.0744767 -4.0587263 -4.0458159 -4.0249104 -3.9640257 -3.8676581 -3.7950816 -3.801872 -3.858402 -3.9210842 -3.9674382 -3.9926825][-4.1745062 -4.133234 -4.1009932 -4.0755768 -4.05233 -4.0294423 -4.0042343 -3.9496748 -3.8612964 -3.7855787 -3.7883105 -3.8397281 -3.89212 -3.9238961 -3.9292095][-4.2010536 -4.16594 -4.1339436 -4.1029911 -4.073791 -4.048151 -4.0244188 -3.9897516 -3.9307177 -3.8790867 -3.8786945 -3.905529 -3.9285445 -3.9261229 -3.9014306][-4.2249479 -4.1999936 -4.1730223 -4.1421003 -4.1103253 -4.0815535 -4.0562696 -4.0408096 -4.0177169 -3.9941816 -3.9879441 -3.9923058 -3.9917977 -3.9643183 -3.9258032][-4.229269 -4.2124968 -4.19367 -4.1700368 -4.141068 -4.1106505 -4.0829258 -4.0741458 -4.0738444 -4.0682425 -4.0583644 -4.0537663 -4.0455346 -4.017601 -3.9806688][-4.2080946 -4.1969752 -4.1876464 -4.1720209 -4.1468182 -4.1174655 -4.0916767 -4.0879297 -4.0995083 -4.1036057 -4.0964594 -4.09049 -4.0839005 -4.0628409 -4.0305715][-4.1742249 -4.1633224 -4.1590805 -4.153718 -4.140285 -4.1175332 -4.0955343 -4.094059 -4.1090746 -4.1148915 -4.1094203 -4.103199 -4.0985241 -4.0807753 -4.0556874][-4.1442313 -4.1332173 -4.1312032 -4.1325741 -4.1287155 -4.1133151 -4.0953107 -4.09584 -4.1143265 -4.1257386 -4.1287107 -4.12397 -4.1152787 -4.0961776 -4.077704][-4.1378531 -4.1283383 -4.125134 -4.1279373 -4.1293812 -4.1217575 -4.1106038 -4.1129932 -4.1312165 -4.1502118 -4.1611395 -4.1575985 -4.1463761 -4.1281133 -4.1147466]]...]
INFO - root - 2017-12-05 21:22:40.003225: step 45810, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 68h:30m:00s remains)
INFO - root - 2017-12-05 21:22:48.445131: step 45820, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.843 sec/batch; 67h:06m:53s remains)
INFO - root - 2017-12-05 21:22:56.770942: step 45830, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 66h:58m:57s remains)
INFO - root - 2017-12-05 21:23:05.286436: step 45840, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 69h:27m:56s remains)
INFO - root - 2017-12-05 21:23:13.735921: step 45850, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 66h:10m:47s remains)
INFO - root - 2017-12-05 21:23:22.103656: step 45860, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.816 sec/batch; 64h:59m:35s remains)
INFO - root - 2017-12-05 21:23:30.602504: step 45870, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 66h:57m:12s remains)
INFO - root - 2017-12-05 21:23:39.169952: step 45880, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 68h:39m:32s remains)
INFO - root - 2017-12-05 21:23:47.623677: step 45890, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 69h:55m:52s remains)
INFO - root - 2017-12-05 21:23:56.100694: step 45900, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 65h:25m:12s remains)
2017-12-05 21:23:56.863976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2013917 -4.1922612 -4.1939254 -4.2055469 -4.2185979 -4.22207 -4.2144 -4.2034931 -4.2048192 -4.2152977 -4.2160797 -4.2001843 -4.1891885 -4.1708527 -4.1452765][-4.1952591 -4.1887965 -4.1891031 -4.1929393 -4.1977448 -4.1964278 -4.1898308 -4.1847739 -4.1952343 -4.2121868 -4.2174082 -4.2043371 -4.18906 -4.1649628 -4.1371293][-4.1875834 -4.189239 -4.1882787 -4.18467 -4.1819015 -4.1742668 -4.1645355 -4.1640673 -4.1798229 -4.1979475 -4.204597 -4.1984143 -4.1878448 -4.1676307 -4.1457567][-4.166224 -4.1767983 -4.1829233 -4.1804562 -4.172132 -4.1590853 -4.1434093 -4.1405931 -4.1543608 -4.1718345 -4.181972 -4.1838484 -4.1828041 -4.1697521 -4.1558046][-4.14228 -4.1540346 -4.1628637 -4.1605787 -4.146266 -4.1214633 -4.0874181 -4.0690074 -4.0867362 -4.1224446 -4.149416 -4.1649137 -4.1739454 -4.1683416 -4.1591315][-4.1430912 -4.1450586 -4.1385641 -4.1223855 -4.0947866 -4.0461078 -3.9765978 -3.9249959 -3.9571714 -4.041338 -4.1114111 -4.1548572 -4.1784792 -4.182765 -4.1780415][-4.1571879 -4.1487713 -4.1308837 -4.1069078 -4.0712662 -4.006351 -3.9100351 -3.8230371 -3.8595061 -3.9847004 -4.0904703 -4.1590538 -4.2004147 -4.2165103 -4.2152829][-4.1661911 -4.1561327 -4.1441011 -4.1359015 -4.1172013 -4.0692282 -3.9920902 -3.9139605 -3.931921 -4.0284305 -4.1129007 -4.1724467 -4.2139997 -4.2321777 -4.2299871][-4.1779342 -4.1740656 -4.1735244 -4.1794987 -4.1771736 -4.1520624 -4.1035457 -4.0483141 -4.0531983 -4.108078 -4.1587577 -4.1969371 -4.2250657 -4.235795 -4.2282114][-4.2057843 -4.207902 -4.2092938 -4.214232 -4.212234 -4.1962104 -4.1652975 -4.1276402 -4.1245732 -4.15205 -4.18515 -4.2144566 -4.2348061 -4.2396231 -4.2262034][-4.237802 -4.2398081 -4.236227 -4.2326384 -4.22612 -4.2139788 -4.1939716 -4.1678147 -4.1560912 -4.167479 -4.1965637 -4.2248826 -4.2427969 -4.2448177 -4.2298546][-4.256989 -4.2578306 -4.2515111 -4.243588 -4.2352548 -4.2260966 -4.2130432 -4.1961837 -4.183969 -4.1871505 -4.2147121 -4.2407851 -4.2553735 -4.2592683 -4.2486429][-4.264946 -4.266048 -4.2619567 -4.2570634 -4.2515912 -4.2451105 -4.2373114 -4.2279372 -4.2203612 -4.221858 -4.243319 -4.2622776 -4.2711544 -4.2744665 -4.2665286][-4.269855 -4.2713575 -4.269742 -4.2677855 -4.2672687 -4.2662706 -4.2649198 -4.2631903 -4.26118 -4.2615581 -4.2718062 -4.2820826 -4.2855563 -4.2848768 -4.2770004][-4.2755671 -4.274509 -4.2713518 -4.2685995 -4.2681985 -4.26907 -4.2709756 -4.2729964 -4.2742214 -4.275362 -4.2813754 -4.2879257 -4.2902131 -4.2900844 -4.2860918]]...]
INFO - root - 2017-12-05 21:24:05.348286: step 45910, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 66h:16m:01s remains)
INFO - root - 2017-12-05 21:24:13.872917: step 45920, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 70h:22m:57s remains)
INFO - root - 2017-12-05 21:24:22.357193: step 45930, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 65h:37m:34s remains)
INFO - root - 2017-12-05 21:24:30.979184: step 45940, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 69h:47m:28s remains)
INFO - root - 2017-12-05 21:24:39.462779: step 45950, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.851 sec/batch; 67h:46m:23s remains)
INFO - root - 2017-12-05 21:24:47.987899: step 45960, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 65h:50m:26s remains)
INFO - root - 2017-12-05 21:24:56.465552: step 45970, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 65h:00m:43s remains)
INFO - root - 2017-12-05 21:25:04.798832: step 45980, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 67h:27m:52s remains)
INFO - root - 2017-12-05 21:25:13.367009: step 45990, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 65h:57m:14s remains)
INFO - root - 2017-12-05 21:25:21.901979: step 46000, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 68h:50m:36s remains)
2017-12-05 21:25:22.678265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1673565 -4.1565289 -4.1450138 -4.1385651 -4.1370344 -4.1398878 -4.1552978 -4.1749892 -4.1893277 -4.199995 -4.2053576 -4.2052407 -4.2092657 -4.2198086 -4.2287178][-4.1700459 -4.1562691 -4.1435351 -4.1325951 -4.1216226 -4.1180248 -4.1329021 -4.1596613 -4.1791668 -4.1892362 -4.1904058 -4.185823 -4.1885977 -4.2042265 -4.220252][-4.1930404 -4.1797938 -4.1619949 -4.1376648 -4.1104155 -4.0962329 -4.108335 -4.1369944 -4.1626835 -4.1765594 -4.175159 -4.1674047 -4.1681466 -4.184876 -4.2073855][-4.2274103 -4.2179084 -4.1947651 -4.1555343 -4.1129827 -4.0856433 -4.0890617 -4.1154156 -4.1510754 -4.17798 -4.18174 -4.1758947 -4.1793451 -4.1945577 -4.2130494][-4.2424374 -4.2359838 -4.212821 -4.1638184 -4.1050596 -4.0587964 -4.04716 -4.0656638 -4.1171746 -4.1692019 -4.1868467 -4.1868048 -4.1955276 -4.2075839 -4.2123485][-4.2449083 -4.2395949 -4.2178974 -4.1693082 -4.0992336 -4.0256042 -3.9828472 -3.9815416 -4.0508804 -4.131803 -4.1706719 -4.1857014 -4.1972871 -4.2025204 -4.1922016][-4.2419672 -4.2384892 -4.2220097 -4.1800809 -4.106369 -4.0093126 -3.9188743 -3.8792357 -3.9530959 -4.0591331 -4.1243286 -4.1557193 -4.1670861 -4.1660185 -4.1482735][-4.2395983 -4.2410827 -4.2352133 -4.2117591 -4.149931 -4.0517635 -3.9375861 -3.8630781 -3.9180405 -4.0301347 -4.1104174 -4.146955 -4.1524887 -4.1446023 -4.1263962][-4.2304263 -4.2394586 -4.2485147 -4.2487936 -4.2127385 -4.1397872 -4.0423656 -3.9690638 -3.9995649 -4.081584 -4.1503649 -4.1813984 -4.1788039 -4.1649523 -4.1520057][-4.2056894 -4.2191086 -4.23996 -4.2613831 -4.2525167 -4.2139087 -4.1512632 -4.0975575 -4.1110659 -4.1567745 -4.2009821 -4.219346 -4.20681 -4.1847191 -4.174201][-4.1612082 -4.1799717 -4.2095642 -4.2421846 -4.2541 -4.24873 -4.2199569 -4.1855884 -4.1858149 -4.2066183 -4.2299204 -4.2347732 -4.208322 -4.1744747 -4.1612077][-4.1058612 -4.126595 -4.1653571 -4.2071776 -4.235476 -4.2532864 -4.2487326 -4.2308435 -4.2231612 -4.226737 -4.2311625 -4.2182136 -4.1738491 -4.1278911 -4.1138377][-4.0711775 -4.0877838 -4.127583 -4.1718979 -4.2120066 -4.2461786 -4.2602105 -4.2576237 -4.2494268 -4.2411084 -4.2307739 -4.2034025 -4.1506786 -4.1004133 -4.088882][-4.1041689 -4.1162014 -4.1471248 -4.1785011 -4.2132692 -4.2491255 -4.2709727 -4.2788587 -4.27526 -4.2616858 -4.2424679 -4.2121229 -4.1656651 -4.122427 -4.1128564][-4.191483 -4.199894 -4.2152481 -4.2257338 -4.2401 -4.2606931 -4.2801657 -4.2926545 -4.2931795 -4.279119 -4.2593465 -4.2378159 -4.2080827 -4.1801481 -4.1734147]]...]
INFO - root - 2017-12-05 21:25:31.317992: step 46010, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 69h:17m:24s remains)
INFO - root - 2017-12-05 21:25:39.853017: step 46020, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 67h:14m:28s remains)
INFO - root - 2017-12-05 21:25:48.418208: step 46030, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 68h:51m:12s remains)
INFO - root - 2017-12-05 21:25:56.873917: step 46040, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 0.785 sec/batch; 62h:29m:32s remains)
INFO - root - 2017-12-05 21:26:05.436422: step 46050, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 67h:54m:51s remains)
INFO - root - 2017-12-05 21:26:13.879751: step 46060, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 67h:41m:34s remains)
INFO - root - 2017-12-05 21:26:22.503580: step 46070, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 68h:45m:15s remains)
INFO - root - 2017-12-05 21:26:31.133731: step 46080, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 66h:59m:26s remains)
INFO - root - 2017-12-05 21:26:39.601903: step 46090, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 68h:30m:59s remains)
INFO - root - 2017-12-05 21:26:48.273430: step 46100, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 67h:21m:18s remains)
2017-12-05 21:26:49.027290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2667503 -4.2571516 -4.2423048 -4.2245 -4.2076859 -4.1902003 -4.1900239 -4.2078514 -4.2369719 -4.2615871 -4.2806797 -4.2865381 -4.28023 -4.2739859 -4.2660789][-4.2669449 -4.2603173 -4.2412853 -4.2145262 -4.1919155 -4.1741529 -4.1670313 -4.1816382 -4.2179189 -4.2560372 -4.2886071 -4.3025765 -4.2983031 -4.2927527 -4.2864966][-4.2606831 -4.2527971 -4.2313914 -4.1976495 -4.16774 -4.1461382 -4.131321 -4.142067 -4.18742 -4.2360983 -4.2761879 -4.2965574 -4.29766 -4.2937078 -4.2890453][-4.259232 -4.2502089 -4.2271695 -4.1910586 -4.1546092 -4.1223421 -4.0927472 -4.0932717 -4.1423135 -4.1995845 -4.2453513 -4.2679338 -4.2727332 -4.2707143 -4.2707124][-4.263247 -4.2539072 -4.2304554 -4.1954508 -4.1549582 -4.1100154 -4.0533781 -4.0325851 -4.079855 -4.1463604 -4.196703 -4.2228756 -4.2357621 -4.2443404 -4.253365][-4.2657175 -4.2548089 -4.2289023 -4.1951847 -4.1578732 -4.1000624 -4.00265 -3.9478631 -3.9987936 -4.0752454 -4.1242528 -4.152267 -4.17691 -4.2076354 -4.2346053][-4.2601366 -4.2434821 -4.2119088 -4.1712523 -4.1262116 -4.0433559 -3.8912759 -3.7946849 -3.8758492 -3.9866116 -4.0468721 -4.0791941 -4.1170082 -4.1742096 -4.2211089][-4.2504168 -4.2234406 -4.1797662 -4.1291618 -4.0724473 -3.9715354 -3.7952778 -3.689625 -3.806669 -3.9455132 -4.0221677 -4.0705442 -4.1231451 -4.189456 -4.2365627][-4.2388897 -4.2023258 -4.145288 -4.0916595 -4.0379276 -3.9495733 -3.821455 -3.7667398 -3.8789272 -3.9980838 -4.0715675 -4.1222048 -4.1765165 -4.2331386 -4.267036][-4.2276993 -4.1890969 -4.1349387 -4.0949759 -4.0562711 -3.9945107 -3.9314489 -3.9349382 -4.0288687 -4.10819 -4.159339 -4.1957841 -4.2343221 -4.2726464 -4.2930589][-4.2148566 -4.1776295 -4.1361661 -4.1141038 -4.0949364 -4.0556412 -4.0324368 -4.0645776 -4.1437244 -4.1954665 -4.2264957 -4.2474461 -4.2679868 -4.2855406 -4.2921882][-4.1982636 -4.1615677 -4.130024 -4.1157002 -4.1050634 -4.0783844 -4.0734649 -4.116251 -4.1829424 -4.2181158 -4.2320156 -4.23823 -4.2444472 -4.2486582 -4.2467332][-4.1881304 -4.1527209 -4.1278954 -4.1124115 -4.099843 -4.0773606 -4.07982 -4.1222129 -4.1752262 -4.1966262 -4.1966028 -4.1935921 -4.1914473 -4.189229 -4.186018][-4.1927471 -4.16125 -4.1395092 -4.1238956 -4.1108146 -4.0948286 -4.1007481 -4.1352849 -4.171813 -4.182704 -4.178812 -4.1743054 -4.172524 -4.1711559 -4.1698084][-4.2152953 -4.1965504 -4.184494 -4.1765981 -4.1663947 -4.1536913 -4.1599312 -4.1834836 -4.2046976 -4.2119637 -4.2098589 -4.2079358 -4.2081032 -4.2083068 -4.2082052]]...]
INFO - root - 2017-12-05 21:26:57.588855: step 46110, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.830 sec/batch; 66h:02m:00s remains)
INFO - root - 2017-12-05 21:27:06.030140: step 46120, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 66h:47m:44s remains)
INFO - root - 2017-12-05 21:27:14.640173: step 46130, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 67h:52m:11s remains)
INFO - root - 2017-12-05 21:27:23.261941: step 46140, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 68h:44m:55s remains)
INFO - root - 2017-12-05 21:27:31.863894: step 46150, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 68h:23m:45s remains)
INFO - root - 2017-12-05 21:27:40.451254: step 46160, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 65h:48m:09s remains)
INFO - root - 2017-12-05 21:27:49.076416: step 46170, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 71h:40m:53s remains)
INFO - root - 2017-12-05 21:27:57.730795: step 46180, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.876 sec/batch; 69h:39m:30s remains)
INFO - root - 2017-12-05 21:28:06.380368: step 46190, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 69h:19m:08s remains)
INFO - root - 2017-12-05 21:28:14.882067: step 46200, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 68h:33m:37s remains)
2017-12-05 21:28:15.732366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.247529 -4.2425909 -4.2436562 -4.2541919 -4.2637405 -4.2577038 -4.2433925 -4.2282062 -4.220901 -4.2238779 -4.2251348 -4.217051 -4.20351 -4.1776361 -4.153789][-4.2454925 -4.2415352 -4.2410851 -4.24972 -4.2580495 -4.2500076 -4.2347426 -4.2219629 -4.2213135 -4.2318559 -4.2340751 -4.2184854 -4.1946344 -4.1644912 -4.1450472][-4.2408853 -4.2431321 -4.2436085 -4.2483435 -4.2494621 -4.2341428 -4.2149558 -4.20318 -4.2085061 -4.2274165 -4.233572 -4.2165408 -4.1879654 -4.158217 -4.146915][-4.22609 -4.2355146 -4.2381554 -4.2364826 -4.2237864 -4.1974821 -4.1745977 -4.1641684 -4.1807461 -4.2152495 -4.2312255 -4.2189865 -4.1907673 -4.1651964 -4.1570482][-4.2026625 -4.2163715 -4.2234011 -4.2181492 -4.1909056 -4.1499424 -4.1156092 -4.1041718 -4.1376991 -4.1905403 -4.2198906 -4.2176037 -4.1960111 -4.1761389 -4.1690011][-4.1703296 -4.1869988 -4.2006564 -4.1931834 -4.1537085 -4.0966449 -4.0368223 -4.0146327 -4.0753331 -4.1539445 -4.1994743 -4.2088566 -4.1948476 -4.1821208 -4.1796384][-4.1426582 -4.1576681 -4.170505 -4.15999 -4.1083746 -4.0251265 -3.9227581 -3.8916342 -4.001832 -4.1167727 -4.180757 -4.1998539 -4.1944313 -4.1909943 -4.1955404][-4.1235247 -4.1375256 -4.1495876 -4.1366811 -4.0778093 -3.9729848 -3.8333771 -3.8069425 -3.9697556 -4.1057158 -4.1771574 -4.2005434 -4.1998653 -4.202415 -4.2097754][-4.1250958 -4.1402931 -4.1527162 -4.1415644 -4.0945725 -4.0079126 -3.9015245 -3.9030943 -4.0346365 -4.1360674 -4.1899238 -4.2070489 -4.2060304 -4.2042055 -4.2066507][-4.1373916 -4.1532969 -4.167098 -4.1627088 -4.13281 -4.0815773 -4.0319228 -4.0499568 -4.1247206 -4.1783366 -4.2122264 -4.2219725 -4.2215261 -4.2139344 -4.2030735][-4.1597533 -4.1725163 -4.1827993 -4.1839743 -4.1667695 -4.1385589 -4.1209617 -4.1414323 -4.1776981 -4.2037296 -4.2261496 -4.2340117 -4.2334824 -4.2219954 -4.1999693][-4.1807752 -4.1890163 -4.1965442 -4.1995921 -4.1852064 -4.1676979 -4.1622314 -4.1766748 -4.1959057 -4.2126374 -4.2297549 -4.2369161 -4.2407789 -4.2253795 -4.193459][-4.1901245 -4.1997132 -4.2099133 -4.2111835 -4.1973596 -4.1808176 -4.1748185 -4.18661 -4.199626 -4.2105074 -4.2224469 -4.2304754 -4.2364726 -4.2230124 -4.1901007][-4.1928525 -4.2087026 -4.2232718 -4.2258229 -4.2127337 -4.195581 -4.1816859 -4.1829262 -4.1867995 -4.1874657 -4.1967325 -4.2110429 -4.2204723 -4.215353 -4.1957436][-4.1999388 -4.2243733 -4.2409654 -4.2447634 -4.2392659 -4.2248049 -4.1983175 -4.183023 -4.1747489 -4.168098 -4.1743445 -4.186213 -4.1972504 -4.207037 -4.208652]]...]
INFO - root - 2017-12-05 21:28:24.278761: step 46210, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 67h:05m:45s remains)
INFO - root - 2017-12-05 21:28:32.850109: step 46220, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 66h:28m:42s remains)
INFO - root - 2017-12-05 21:28:41.405013: step 46230, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.883 sec/batch; 70h:12m:15s remains)
INFO - root - 2017-12-05 21:28:50.052599: step 46240, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 68h:25m:27s remains)
INFO - root - 2017-12-05 21:28:58.677740: step 46250, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 67h:45m:46s remains)
INFO - root - 2017-12-05 21:29:07.091117: step 46260, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 68h:44m:35s remains)
INFO - root - 2017-12-05 21:29:15.597618: step 46270, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.843 sec/batch; 66h:59m:18s remains)
INFO - root - 2017-12-05 21:29:24.204638: step 46280, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 68h:15m:48s remains)
INFO - root - 2017-12-05 21:29:32.826925: step 46290, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 69h:20m:37s remains)
INFO - root - 2017-12-05 21:29:41.309766: step 46300, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.802 sec/batch; 63h:47m:22s remains)
2017-12-05 21:29:42.071092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1966186 -4.2151513 -4.225 -4.2254477 -4.2364125 -4.2450256 -4.2403617 -4.2251387 -4.2144752 -4.2196689 -4.2414255 -4.27108 -4.2880836 -4.28377 -4.2832575][-4.1959362 -4.2143435 -4.2216935 -4.2227411 -4.2378345 -4.2476587 -4.2412848 -4.2250667 -4.2148485 -4.2206593 -4.244647 -4.277101 -4.2967329 -4.2931247 -4.2903194][-4.1943035 -4.2094579 -4.2139254 -4.2165446 -4.2313948 -4.2381334 -4.2313418 -4.2203693 -4.2155542 -4.2247148 -4.2521996 -4.2872286 -4.3085389 -4.3065429 -4.3021164][-4.1844773 -4.1985974 -4.2015815 -4.2054629 -4.2187071 -4.2213058 -4.2182279 -4.2156787 -4.2176971 -4.2293382 -4.2599206 -4.29741 -4.3182521 -4.3174558 -4.3134418][-4.1765895 -4.1892424 -4.1889443 -4.1887579 -4.1947193 -4.1943493 -4.1974235 -4.2032557 -4.2113991 -4.2266479 -4.2605004 -4.2974887 -4.3142276 -4.3133478 -4.3106823][-4.1747947 -4.1868634 -4.1822963 -4.1760411 -4.1733389 -4.1727877 -4.1798158 -4.1890454 -4.2008061 -4.219697 -4.2545295 -4.2889743 -4.3049712 -4.3050089 -4.3026452][-4.166348 -4.1813083 -4.1749973 -4.1618009 -4.15225 -4.15346 -4.1611624 -4.1707897 -4.1842742 -4.2050095 -4.2362905 -4.26924 -4.2875137 -4.2910504 -4.2931523][-4.1526151 -4.1660461 -4.1548729 -4.1329222 -4.1145163 -4.112257 -4.1125212 -4.1141729 -4.1255546 -4.1481605 -4.1787338 -4.2186255 -4.2490306 -4.2647877 -4.2764392][-4.170754 -4.1728234 -4.1533532 -4.1227884 -4.0968142 -4.090436 -4.0828404 -4.0687804 -4.0721374 -4.0924497 -4.121284 -4.1649904 -4.2069082 -4.2377796 -4.2623587][-4.207654 -4.2027011 -4.180913 -4.1507645 -4.12645 -4.1197662 -4.1098633 -4.0890627 -4.0886025 -4.1076136 -4.13399 -4.1725788 -4.21115 -4.2425833 -4.2699642][-4.2191858 -4.2226005 -4.2142348 -4.1968694 -4.1776342 -4.1720419 -4.1622615 -4.1401615 -4.1416407 -4.1626577 -4.1871929 -4.2189727 -4.2490206 -4.2712 -4.2914896][-4.2125888 -4.2248497 -4.2274351 -4.2191849 -4.2058988 -4.2045064 -4.2007785 -4.1869841 -4.1944957 -4.2183347 -4.2408328 -4.268137 -4.2923746 -4.3058243 -4.3154216][-4.2178383 -4.23172 -4.2377915 -4.2323351 -4.2213569 -4.2222819 -4.2221427 -4.2135963 -4.2245641 -4.2495222 -4.2731595 -4.30352 -4.3261571 -4.3322387 -4.3339348][-4.2301979 -4.2410312 -4.247972 -4.2473702 -4.2458158 -4.25374 -4.256228 -4.2496953 -4.2578654 -4.2780561 -4.2993636 -4.3279176 -4.3460855 -4.34632 -4.3440447][-4.222147 -4.2269006 -4.2318544 -4.2376766 -4.2498817 -4.2684112 -4.2747641 -4.272295 -4.2796435 -4.2976656 -4.3154469 -4.3363423 -4.347867 -4.3466644 -4.3444691]]...]
INFO - root - 2017-12-05 21:29:50.554573: step 46310, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 68h:31m:48s remains)
INFO - root - 2017-12-05 21:29:59.275277: step 46320, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 69h:21m:33s remains)
INFO - root - 2017-12-05 21:30:07.813384: step 46330, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 67h:27m:26s remains)
INFO - root - 2017-12-05 21:30:16.354990: step 46340, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 67h:59m:32s remains)
INFO - root - 2017-12-05 21:30:24.944416: step 46350, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 70h:50m:11s remains)
INFO - root - 2017-12-05 21:30:33.495005: step 46360, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.801 sec/batch; 63h:39m:44s remains)
INFO - root - 2017-12-05 21:30:41.994897: step 46370, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 67h:00m:06s remains)
INFO - root - 2017-12-05 21:30:50.688711: step 46380, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 68h:00m:37s remains)
INFO - root - 2017-12-05 21:30:59.139332: step 46390, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.847 sec/batch; 67h:18m:53s remains)
INFO - root - 2017-12-05 21:31:07.732443: step 46400, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 68h:31m:42s remains)
2017-12-05 21:31:08.515260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2761049 -4.2734876 -4.2802567 -4.2823863 -4.2770014 -4.2698092 -4.2625189 -4.2544217 -4.2460814 -4.2517147 -4.2729721 -4.2896457 -4.29551 -4.2911143 -4.2790012][-4.2567754 -4.25337 -4.265132 -4.2715445 -4.2655563 -4.2584195 -4.2462893 -4.2252345 -4.2017746 -4.20426 -4.2342882 -4.2571092 -4.2644825 -4.2581983 -4.2403941][-4.2396307 -4.233427 -4.24952 -4.2608528 -4.255475 -4.2425151 -4.2176571 -4.18085 -4.1400919 -4.1431527 -4.1854639 -4.2104769 -4.2148685 -4.2068014 -4.1873646][-4.2247882 -4.2124186 -4.2306314 -4.2475471 -4.2414827 -4.2198944 -4.1841326 -4.1282291 -4.0722289 -4.0847793 -4.1460419 -4.175272 -4.17452 -4.1606345 -4.1376419][-4.2102084 -4.1887755 -4.2060957 -4.2270517 -4.2188377 -4.18949 -4.1408257 -4.0639243 -3.9975262 -4.0230608 -4.1062388 -4.1467004 -4.1437631 -4.1233444 -4.09565][-4.2000623 -4.173913 -4.1903858 -4.2118669 -4.2002487 -4.1625028 -4.0959468 -3.9986739 -3.9241824 -3.9645231 -4.0648985 -4.1120028 -4.1082468 -4.0861664 -4.0623894][-4.1973162 -4.1709361 -4.1849971 -4.2052526 -4.1945591 -4.1480203 -4.061028 -3.9489346 -3.8809209 -3.9450943 -4.0500474 -4.0916057 -4.0824308 -4.0648489 -4.057435][-4.20252 -4.1737804 -4.1844625 -4.2066493 -4.2022872 -4.1511784 -4.0543957 -3.9464593 -3.893317 -3.9570689 -4.049325 -4.0888133 -4.0830803 -4.0790367 -4.0947042][-4.2134604 -4.1831126 -4.1932707 -4.2224827 -4.2278285 -4.1869245 -4.1011744 -4.0101871 -3.9695945 -4.0217185 -4.0954094 -4.1318707 -4.1365924 -4.1437416 -4.1632533][-4.2305565 -4.1989789 -4.2073164 -4.236495 -4.2455149 -4.2184105 -4.1520796 -4.0780811 -4.0457425 -4.0906305 -4.1550603 -4.1907091 -4.1995153 -4.2072945 -4.216979][-4.2487745 -4.215857 -4.2150764 -4.235981 -4.2454634 -4.2293024 -4.1782703 -4.1206036 -4.0984979 -4.1412883 -4.2006497 -4.2304416 -4.2392673 -4.2433214 -4.2430058][-4.2606707 -4.2260184 -4.2134724 -4.2243748 -4.2340446 -4.2259731 -4.1907067 -4.1535416 -4.1456614 -4.1837239 -4.2304778 -4.2485075 -4.2540135 -4.2555194 -4.2512312][-4.2701135 -4.234973 -4.2162085 -4.21723 -4.2241015 -4.2203751 -4.2004662 -4.1851664 -4.1892071 -4.2168756 -4.2468495 -4.2561226 -4.2571244 -4.2557425 -4.2503][-4.2838049 -4.2513819 -4.2314363 -4.2277136 -4.2326741 -4.2312369 -4.2230229 -4.2227626 -4.2340946 -4.2532897 -4.2694893 -4.2709494 -4.2680335 -4.2642989 -4.2599487][-4.3019738 -4.2793965 -4.2647696 -4.2617397 -4.26608 -4.266829 -4.2656436 -4.2707262 -4.2818437 -4.2935433 -4.2993364 -4.2972307 -4.2926474 -4.2884736 -4.285852]]...]
INFO - root - 2017-12-05 21:31:16.962688: step 46410, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 65h:58m:13s remains)
INFO - root - 2017-12-05 21:31:25.618955: step 46420, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 68h:57m:14s remains)
INFO - root - 2017-12-05 21:31:34.189860: step 46430, loss = 2.10, batch loss = 2.05 (9.8 examples/sec; 0.815 sec/batch; 64h:47m:40s remains)
INFO - root - 2017-12-05 21:31:42.821837: step 46440, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 66h:48m:11s remains)
INFO - root - 2017-12-05 21:31:51.329519: step 46450, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 67h:31m:17s remains)
INFO - root - 2017-12-05 21:31:59.912177: step 46460, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 68h:16m:23s remains)
INFO - root - 2017-12-05 21:32:08.471291: step 46470, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 68h:06m:39s remains)
INFO - root - 2017-12-05 21:32:16.960730: step 46480, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 67h:21m:15s remains)
INFO - root - 2017-12-05 21:32:25.639879: step 46490, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 71h:19m:57s remains)
INFO - root - 2017-12-05 21:32:34.261786: step 46500, loss = 2.04, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 65h:39m:33s remains)
2017-12-05 21:32:35.083163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2742147 -4.2698565 -4.2626877 -4.2550068 -4.2495446 -4.2463174 -4.2466006 -4.2473269 -4.246532 -4.2535453 -4.2630339 -4.2659431 -4.2608938 -4.2612162 -4.2696586][-4.2549286 -4.2550654 -4.2538004 -4.2478404 -4.239284 -4.2287726 -4.2225003 -4.2179737 -4.2158861 -4.2282205 -4.245728 -4.250308 -4.2448988 -4.2464848 -4.2553787][-4.2296624 -4.2375274 -4.2466197 -4.2424803 -4.2297282 -4.2135277 -4.1991248 -4.1823821 -4.1736732 -4.1962204 -4.2313309 -4.2406173 -4.2344208 -4.2362075 -4.2446542][-4.2096024 -4.2232108 -4.2388325 -4.2359362 -4.2202191 -4.1968021 -4.1684747 -4.1305604 -4.1115236 -4.1521 -4.2096691 -4.2276621 -4.2254148 -4.2264347 -4.2325139][-4.1907411 -4.2063613 -4.2248249 -4.2237811 -4.2078586 -4.1788278 -4.1318145 -4.06715 -4.0451937 -4.1138186 -4.1905913 -4.2177348 -4.2159705 -4.2135172 -4.213903][-4.1655707 -4.1864586 -4.2107034 -4.2136068 -4.2000875 -4.1651516 -4.0939908 -4.0001373 -3.984237 -4.0856223 -4.1787634 -4.2114 -4.20964 -4.2006865 -4.1926589][-4.1516991 -4.1793609 -4.207603 -4.21465 -4.1991897 -4.1497512 -4.0483856 -3.9266646 -3.9195585 -4.0472293 -4.1571903 -4.199286 -4.2008615 -4.1845417 -4.1671624][-4.1576705 -4.1937923 -4.2228513 -4.228653 -4.206378 -4.1396217 -4.0106187 -3.8668184 -3.8672457 -4.0173059 -4.1409812 -4.1898036 -4.1909671 -4.1658659 -4.1363697][-4.1721268 -4.2140927 -4.2419949 -4.2450671 -4.215075 -4.1372352 -3.9983993 -3.8558221 -3.8656409 -4.0190315 -4.1395407 -4.1847425 -4.1765928 -4.1392689 -4.1023092][-4.177453 -4.2200665 -4.2498655 -4.2520165 -4.2191567 -4.1417713 -4.0192604 -3.9069827 -3.9239323 -4.0506487 -4.1492987 -4.1822872 -4.1592846 -4.1080542 -4.0753708][-4.1772017 -4.2194605 -4.2491727 -4.2503858 -4.2217979 -4.1566715 -4.0609107 -3.98296 -4.0043468 -4.09717 -4.1649976 -4.1773486 -4.1371431 -4.0812469 -4.0665741][-4.1775513 -4.2152023 -4.2420125 -4.2448831 -4.2245979 -4.1756516 -4.1078982 -4.0569468 -4.0749121 -4.1356506 -4.1747541 -4.166842 -4.116518 -4.0697227 -4.0773182][-4.1722488 -4.199235 -4.2234559 -4.2304907 -4.2184281 -4.18618 -4.1419892 -4.1142645 -4.129324 -4.1639709 -4.1806498 -4.1602831 -4.1094561 -4.0778184 -4.099411][-4.1737528 -4.189 -4.2079906 -4.2171 -4.2114153 -4.1918421 -4.1663356 -4.1552358 -4.1670723 -4.1841621 -4.1872907 -4.1631737 -4.1193938 -4.1015205 -4.1277223][-4.1967716 -4.2022061 -4.214438 -4.2220464 -4.218523 -4.207829 -4.1982126 -4.1967869 -4.2042985 -4.2093372 -4.204133 -4.1825614 -4.1545691 -4.1504059 -4.1712508]]...]
INFO - root - 2017-12-05 21:32:43.573411: step 46510, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 67h:27m:14s remains)
INFO - root - 2017-12-05 21:32:52.015857: step 46520, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 68h:48m:59s remains)
INFO - root - 2017-12-05 21:33:00.581275: step 46530, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.813 sec/batch; 64h:36m:37s remains)
INFO - root - 2017-12-05 21:33:09.194387: step 46540, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 67h:49m:34s remains)
INFO - root - 2017-12-05 21:33:17.817645: step 46550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 68h:11m:56s remains)
INFO - root - 2017-12-05 21:33:26.360350: step 46560, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 68h:15m:52s remains)
INFO - root - 2017-12-05 21:33:34.923080: step 46570, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 68h:47m:52s remains)
INFO - root - 2017-12-05 21:33:43.368259: step 46580, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 71h:22m:30s remains)
INFO - root - 2017-12-05 21:33:51.971771: step 46590, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 67h:38m:24s remains)
INFO - root - 2017-12-05 21:34:00.582120: step 46600, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 68h:02m:46s remains)
2017-12-05 21:34:01.434773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.279088 -4.26139 -4.2400265 -4.2129312 -4.1934047 -4.1762953 -4.1602831 -4.1693673 -4.1985245 -4.2289066 -4.2501168 -4.2641945 -4.2715168 -4.2717595 -4.2673817][-4.2710671 -4.2560682 -4.2411909 -4.2232742 -4.2129364 -4.2007132 -4.1847978 -4.1876884 -4.2107534 -4.2346396 -4.2491918 -4.2562418 -4.2636313 -4.25985 -4.249104][-4.25292 -4.2385325 -4.228539 -4.2222381 -4.22107 -4.2130351 -4.1987872 -4.1995044 -4.21687 -4.2357755 -4.2453361 -4.2438393 -4.2447572 -4.2353196 -4.2210431][-4.2291427 -4.21177 -4.2044048 -4.2057185 -4.2090144 -4.2004051 -4.185595 -4.1872206 -4.2017822 -4.2200017 -4.2336559 -4.2272692 -4.2157922 -4.2016754 -4.1883864][-4.1989541 -4.1735535 -4.1641569 -4.1635189 -4.1615853 -4.1498361 -4.134315 -4.137444 -4.1555252 -4.179255 -4.2019596 -4.1955714 -4.1681986 -4.1490192 -4.14057][-4.1679463 -4.1323118 -4.1128578 -4.1023097 -4.0844707 -4.0587883 -4.0312691 -4.0309854 -4.064209 -4.1094027 -4.1484528 -4.1404629 -4.0955906 -4.0705385 -4.06537][-4.1384654 -4.09139 -4.0600157 -4.0338578 -3.9927094 -3.9379115 -3.873261 -3.8665533 -3.9346013 -4.0206342 -4.0865078 -4.0810833 -4.0198421 -3.9852238 -3.9744627][-4.12088 -4.068573 -4.0276718 -3.986541 -3.9313626 -3.8471923 -3.7318914 -3.7079458 -3.8071127 -3.934638 -4.0349016 -4.0471907 -3.9836533 -3.9427528 -3.9269555][-4.1316013 -4.084517 -4.0435476 -4.0008936 -3.9544618 -3.8767855 -3.7600093 -3.7191343 -3.7927432 -3.920485 -4.0326271 -4.0648212 -4.0146484 -3.9806268 -3.9701626][-4.1586986 -4.1215968 -4.0898972 -4.0559926 -4.03197 -3.9840684 -3.901623 -3.8670609 -3.8971415 -3.9872665 -4.0809965 -4.1182809 -4.08908 -4.0666471 -4.0635672][-4.1888561 -4.1588087 -4.1346164 -4.1150231 -4.110055 -4.0855708 -4.0338836 -4.0127344 -4.0237 -4.0840163 -4.1530194 -4.188026 -4.1747718 -4.1593747 -4.157928][-4.2281632 -4.2026916 -4.184155 -4.1757059 -4.1807046 -4.1695833 -4.1358151 -4.1220932 -4.1273503 -4.1678319 -4.2183619 -4.2520208 -4.2461815 -4.23442 -4.2301617][-4.2643361 -4.2401071 -4.2208714 -4.215939 -4.224966 -4.2253518 -4.205636 -4.1956472 -4.1964617 -4.2216463 -4.2591929 -4.286201 -4.2823982 -4.2748055 -4.2707119][-4.2900038 -4.2678647 -4.248385 -4.2400174 -4.2487183 -4.2548027 -4.2466297 -4.2402544 -4.2399583 -4.2577462 -4.2828245 -4.2980337 -4.2954721 -4.2907715 -4.2876492][-4.2975187 -4.2767348 -4.2584853 -4.2479024 -4.2541618 -4.2590842 -4.2576909 -4.2548156 -4.253027 -4.2665744 -4.2839894 -4.293129 -4.2927108 -4.28935 -4.2864304]]...]
INFO - root - 2017-12-05 21:34:09.951655: step 46610, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 69h:45m:44s remains)
INFO - root - 2017-12-05 21:34:18.441543: step 46620, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 68h:13m:00s remains)
INFO - root - 2017-12-05 21:34:26.845398: step 46630, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 66h:36m:58s remains)
INFO - root - 2017-12-05 21:34:35.419882: step 46640, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 68h:34m:26s remains)
INFO - root - 2017-12-05 21:34:43.989218: step 46650, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 67h:20m:44s remains)
INFO - root - 2017-12-05 21:34:52.453005: step 46660, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 69h:07m:27s remains)
INFO - root - 2017-12-05 21:35:01.012412: step 46670, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 69h:29m:42s remains)
INFO - root - 2017-12-05 21:35:09.566389: step 46680, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 65h:40m:52s remains)
INFO - root - 2017-12-05 21:35:18.119444: step 46690, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 67h:34m:40s remains)
INFO - root - 2017-12-05 21:35:26.688509: step 46700, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 68h:46m:08s remains)
2017-12-05 21:35:27.452408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.15334 -4.12114 -4.0891142 -4.0780129 -4.0964961 -4.1297894 -4.1525607 -4.166903 -4.1701927 -4.1663489 -4.1696234 -4.177825 -4.1819792 -4.187295 -4.184073][-4.168067 -4.1364303 -4.0992813 -4.0750527 -4.0783567 -4.1014595 -4.11904 -4.1400232 -4.160079 -4.1705179 -4.1857038 -4.2041411 -4.2117176 -4.218823 -4.2152352][-4.18575 -4.1599445 -4.1246629 -4.0876522 -4.0675321 -4.0689259 -4.0772357 -4.1119184 -4.1537051 -4.1835675 -4.2113886 -4.234756 -4.2456384 -4.2539988 -4.2473216][-4.2160358 -4.1966848 -4.1669364 -4.1208658 -4.0725131 -4.0386143 -4.0297241 -4.0755811 -4.1382456 -4.1858158 -4.223959 -4.2495275 -4.2625875 -4.2716813 -4.2624593][-4.2310748 -4.2232828 -4.2044964 -4.1597176 -4.09175 -4.0212307 -3.9833648 -4.0261431 -4.1038995 -4.1665044 -4.2153559 -4.2475266 -4.2655935 -4.2734666 -4.2600465][-4.2421894 -4.2450013 -4.2317176 -4.1864548 -4.1085019 -4.0080743 -3.9391482 -3.9713378 -4.0622563 -4.14306 -4.207593 -4.2492056 -4.2700214 -4.27433 -4.2581549][-4.2558508 -4.2558861 -4.23568 -4.1870914 -4.1078062 -3.999239 -3.9194093 -3.9468429 -4.0460978 -4.144383 -4.2201042 -4.2641368 -4.2805614 -4.2797332 -4.2623653][-4.2611213 -4.2487516 -4.220263 -4.1741076 -4.1085634 -4.0222178 -3.9619985 -3.9901986 -4.07793 -4.1712341 -4.2421155 -4.2770119 -4.2849822 -4.2764 -4.2562447][-4.2535715 -4.2295775 -4.1918883 -4.1455441 -4.1001372 -4.0503478 -4.0257277 -4.0589137 -4.1294107 -4.2064686 -4.2620611 -4.2823768 -4.2793818 -4.2591224 -4.232162][-4.2444663 -4.2102633 -4.1612954 -4.1058636 -4.0709014 -4.0506449 -4.0591636 -4.1036906 -4.1677103 -4.2345662 -4.2766538 -4.2847528 -4.2691488 -4.2333231 -4.1994357][-4.2341132 -4.1957579 -4.1428504 -4.08021 -4.0426626 -4.0354943 -4.0665984 -4.1255794 -4.1905618 -4.2491364 -4.279716 -4.276669 -4.2470708 -4.1979694 -4.1586175][-4.2370825 -4.2002459 -4.1470194 -4.0829811 -4.038064 -4.0326967 -4.0756726 -4.141881 -4.2065668 -4.2586403 -4.2832623 -4.2763319 -4.238893 -4.1803889 -4.1343937][-4.2407718 -4.2124276 -4.1687436 -4.1169744 -4.0752425 -4.074594 -4.1203995 -4.1809492 -4.2354436 -4.278966 -4.2968578 -4.2874889 -4.250948 -4.1933823 -4.1452518][-4.2351208 -4.2182908 -4.1897707 -4.1560907 -4.1266279 -4.1329923 -4.1725774 -4.2190742 -4.25945 -4.2931905 -4.3073964 -4.2994795 -4.272356 -4.2274523 -4.1833038][-4.2223792 -4.2158055 -4.1944413 -4.1693993 -4.1515379 -4.16357 -4.1969275 -4.2346573 -4.2659569 -4.2928109 -4.3042951 -4.2962346 -4.2783151 -4.2514081 -4.2201004]]...]
INFO - root - 2017-12-05 21:35:35.954240: step 46710, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 66h:02m:48s remains)
INFO - root - 2017-12-05 21:35:44.506611: step 46720, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 68h:06m:27s remains)
INFO - root - 2017-12-05 21:35:53.066715: step 46730, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 67h:00m:29s remains)
INFO - root - 2017-12-05 21:36:01.564120: step 46740, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 65h:51m:49s remains)
INFO - root - 2017-12-05 21:36:10.148180: step 46750, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 65h:06m:03s remains)
INFO - root - 2017-12-05 21:36:18.643048: step 46760, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 69h:12m:47s remains)
INFO - root - 2017-12-05 21:36:27.348910: step 46770, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 69h:25m:37s remains)
INFO - root - 2017-12-05 21:36:35.917563: step 46780, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 67h:51m:00s remains)
INFO - root - 2017-12-05 21:36:44.455454: step 46790, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 68h:00m:14s remains)
INFO - root - 2017-12-05 21:36:52.970716: step 46800, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 66h:14m:58s remains)
2017-12-05 21:36:53.760899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3053718 -4.30071 -4.2979107 -4.3033681 -4.3060637 -4.3074579 -4.3053226 -4.3024211 -4.3023448 -4.30498 -4.3055859 -4.3021808 -4.2964964 -4.2971897 -4.3034587][-4.2642436 -4.2603931 -4.2585659 -4.269805 -4.2809954 -4.2879953 -4.2875853 -4.2842312 -4.28286 -4.2828064 -4.2789092 -4.2716274 -4.2620945 -4.2623734 -4.2695484][-4.2123957 -4.2101235 -4.21433 -4.23207 -4.2517633 -4.2640433 -4.2653017 -4.2637215 -4.2616372 -4.2570305 -4.2456489 -4.2323723 -4.2201128 -4.2203269 -4.22806][-4.1667619 -4.1640954 -4.17729 -4.2001595 -4.2206345 -4.2305565 -4.2294674 -4.2288737 -4.2273955 -4.2198 -4.2006807 -4.18274 -4.1726327 -4.1773348 -4.1902752][-4.1439672 -4.1404896 -4.1590824 -4.1794806 -4.1897283 -4.1883216 -4.1777334 -4.1760297 -4.1797752 -4.1739717 -4.1535344 -4.1372142 -4.1315422 -4.1423841 -4.1645083][-4.1385589 -4.1331787 -4.1454773 -4.156179 -4.1514869 -4.1328993 -4.1072526 -4.1020966 -4.1184669 -4.1236811 -4.1108174 -4.1010785 -4.1042418 -4.1240706 -4.1545262][-4.1483936 -4.1386771 -4.1363215 -4.1340766 -4.1165996 -4.0774193 -4.0284624 -4.0136247 -4.047051 -4.0746617 -4.0784636 -4.0790954 -4.0954118 -4.1298227 -4.1680045][-4.169229 -4.1523862 -4.1349721 -4.1217422 -4.0977674 -4.0483079 -3.9833422 -3.9613044 -4.0128078 -4.0631595 -4.0845342 -4.0950184 -4.1178656 -4.156014 -4.1926246][-4.189187 -4.1712213 -4.1509695 -4.1376381 -4.124567 -4.0925646 -4.0401545 -4.0155177 -4.0590353 -4.108285 -4.1333117 -4.1471858 -4.1652617 -4.1928172 -4.2189231][-4.1987009 -4.1784678 -4.1571422 -4.148231 -4.1541004 -4.15526 -4.1328206 -4.1110563 -4.1303182 -4.1583114 -4.1757832 -4.1879964 -4.19903 -4.216217 -4.2299819][-4.2053528 -4.1769052 -4.1463284 -4.1334915 -4.1514578 -4.1783905 -4.181191 -4.1684117 -4.1711454 -4.1815586 -4.1913853 -4.2026749 -4.2130766 -4.224247 -4.2277751][-4.2008424 -4.1662006 -4.1270738 -4.1055508 -4.1248875 -4.1652989 -4.1835866 -4.17761 -4.1733546 -4.1755409 -4.1815023 -4.19274 -4.2035494 -4.2107978 -4.2091503][-4.1819382 -4.1438327 -4.101377 -4.0769472 -4.092617 -4.134923 -4.158381 -4.157001 -4.1513238 -4.1479826 -4.1492219 -4.1606593 -4.1751175 -4.1833649 -4.1834059][-4.1531219 -4.1165013 -4.0780725 -4.0584526 -4.0712752 -4.1057835 -4.12349 -4.1222939 -4.1190281 -4.1150136 -4.1149592 -4.1261978 -4.1407785 -4.1518235 -4.159543][-4.1520219 -4.1265764 -4.0989671 -4.0857911 -4.0955024 -4.1184049 -4.1285925 -4.1274061 -4.1270933 -4.1260152 -4.1261373 -4.135705 -4.1484809 -4.1593428 -4.1718531]]...]
INFO - root - 2017-12-05 21:37:02.364911: step 46810, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.866 sec/batch; 68h:41m:55s remains)
INFO - root - 2017-12-05 21:37:10.934144: step 46820, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 65h:39m:01s remains)
INFO - root - 2017-12-05 21:37:19.567272: step 46830, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 67h:28m:07s remains)
INFO - root - 2017-12-05 21:37:28.056132: step 46840, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 67h:52m:11s remains)
INFO - root - 2017-12-05 21:37:36.535950: step 46850, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.829 sec/batch; 65h:46m:03s remains)
INFO - root - 2017-12-05 21:37:45.049197: step 46860, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 69h:13m:23s remains)
INFO - root - 2017-12-05 21:37:53.546963: step 46870, loss = 2.04, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 65h:42m:34s remains)
INFO - root - 2017-12-05 21:38:02.048340: step 46880, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 67h:49m:27s remains)
INFO - root - 2017-12-05 21:38:10.588581: step 46890, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 68h:22m:53s remains)
INFO - root - 2017-12-05 21:38:19.017869: step 46900, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 67h:46m:41s remains)
2017-12-05 21:38:19.761375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.188509 -4.185638 -4.1791124 -4.1782069 -4.1835351 -4.1866236 -4.1718473 -4.1527185 -4.1385922 -4.1360149 -4.1331739 -4.1144452 -4.1069627 -4.13042 -4.1696987][-4.2175884 -4.21593 -4.2076311 -4.2031403 -4.2038045 -4.2009315 -4.1855807 -4.1749358 -4.1681004 -4.172473 -4.1714835 -4.1499944 -4.1358418 -4.1519241 -4.1839776][-4.2106905 -4.2065077 -4.1991725 -4.2011075 -4.2092934 -4.208066 -4.1927452 -4.1842279 -4.1835189 -4.1996493 -4.2060575 -4.1857915 -4.162725 -4.1637688 -4.1850028][-4.1670384 -4.1607 -4.1586809 -4.1728096 -4.1919465 -4.1912656 -4.16479 -4.1457024 -4.1547632 -4.1909633 -4.2140303 -4.2012157 -4.178062 -4.1725874 -4.1814933][-4.1017723 -4.0942612 -4.1021709 -4.1328306 -4.1639347 -4.1619654 -4.1196027 -4.0828142 -4.095612 -4.1498075 -4.1902695 -4.1911807 -4.1752229 -4.1684914 -4.1685658][-4.0378475 -4.0274882 -4.0368643 -4.0723748 -4.109385 -4.1035466 -4.0443287 -3.9893117 -4.006227 -4.0826454 -4.1414185 -4.1539969 -4.1424403 -4.130372 -4.1254182][-4.0421147 -4.0234709 -4.016026 -4.0325513 -4.0539451 -4.0243974 -3.9311132 -3.8504021 -3.8764839 -3.9829719 -4.0657578 -4.0986047 -4.0990744 -4.0880709 -4.08339][-4.1146908 -4.1002789 -4.0877595 -4.0878348 -4.0830688 -4.0260224 -3.9104731 -3.819952 -3.8468328 -3.9506941 -4.0323987 -4.0700188 -4.0784249 -4.0750957 -4.0773435][-4.1987123 -4.1939173 -4.1845231 -4.1756129 -4.1575184 -4.102169 -4.0135126 -3.9506745 -3.9607573 -4.0184588 -4.0694981 -4.0931673 -4.0966535 -4.0943933 -4.0990291][-4.253233 -4.2572064 -4.2513571 -4.2388134 -4.2160912 -4.1682463 -4.1017385 -4.0544705 -4.050621 -4.0777793 -4.1063752 -4.121192 -4.1231785 -4.1237617 -4.1287389][-4.2876287 -4.2931042 -4.2886782 -4.2782197 -4.2575784 -4.2180753 -4.1662874 -4.1301861 -4.1235776 -4.1397152 -4.1560993 -4.1641374 -4.1654239 -4.1662917 -4.164722][-4.3014932 -4.3074517 -4.3070536 -4.304162 -4.2911477 -4.259964 -4.2189112 -4.1936717 -4.19151 -4.2038193 -4.2133818 -4.2144876 -4.2094488 -4.2042742 -4.1971049][-4.2764235 -4.2887006 -4.2953796 -4.3023419 -4.2992806 -4.27982 -4.2491217 -4.2287951 -4.2279563 -4.2357311 -4.2403736 -4.2393322 -4.2332973 -4.2250185 -4.2162127][-4.2270508 -4.242022 -4.2515678 -4.2656584 -4.2716708 -4.2655373 -4.2501717 -4.23756 -4.2384219 -4.2443237 -4.2487617 -4.249722 -4.2490463 -4.2454624 -4.2379375][-4.1898575 -4.2002063 -4.2048454 -4.2171946 -4.2257733 -4.2258234 -4.2212281 -4.216918 -4.2204819 -4.22714 -4.2350974 -4.242156 -4.2472134 -4.2496691 -4.246109]]...]
INFO - root - 2017-12-05 21:38:28.359767: step 46910, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 68h:39m:56s remains)
INFO - root - 2017-12-05 21:38:36.989346: step 46920, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 72h:06m:08s remains)
INFO - root - 2017-12-05 21:38:45.627196: step 46930, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 69h:05m:56s remains)
INFO - root - 2017-12-05 21:38:54.220914: step 46940, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 68h:01m:19s remains)
INFO - root - 2017-12-05 21:39:02.719007: step 46950, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 66h:27m:54s remains)
INFO - root - 2017-12-05 21:39:11.164988: step 46960, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 70h:50m:25s remains)
INFO - root - 2017-12-05 21:39:19.719349: step 46970, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 68h:34m:08s remains)
INFO - root - 2017-12-05 21:39:28.388905: step 46980, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 68h:49m:08s remains)
INFO - root - 2017-12-05 21:39:36.885394: step 46990, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 67h:20m:19s remains)
INFO - root - 2017-12-05 21:39:45.412449: step 47000, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 68h:22m:47s remains)
2017-12-05 21:39:46.227085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2375684 -4.2409921 -4.2496839 -4.259109 -4.260437 -4.2586169 -4.2618022 -4.2719293 -4.2789912 -4.2810493 -4.2846136 -4.2786365 -4.2695503 -4.2630243 -4.2586513][-4.2429757 -4.2497563 -4.2530584 -4.2551031 -4.2546654 -4.2568107 -4.2684021 -4.2856708 -4.2984309 -4.3020034 -4.3016934 -4.2923279 -4.2798929 -4.2681541 -4.2609153][-4.2509022 -4.2558851 -4.2550659 -4.2495193 -4.2459679 -4.2515492 -4.2695017 -4.2871509 -4.2997069 -4.3020291 -4.2981234 -4.2888517 -4.277185 -4.2624817 -4.2545571][-4.2648249 -4.2646217 -4.2590284 -4.2499027 -4.244287 -4.2470112 -4.2635345 -4.2770576 -4.2873378 -4.285665 -4.2769003 -4.2679844 -4.2582169 -4.243607 -4.2387562][-4.2662787 -4.2646809 -4.25716 -4.2463832 -4.2368445 -4.2317257 -4.238235 -4.24279 -4.2469916 -4.24214 -4.2327347 -4.231894 -4.2330589 -4.2232184 -4.2196059][-4.2495451 -4.2456412 -4.2361741 -4.2218213 -4.2054076 -4.1899228 -4.185039 -4.1791873 -4.1783462 -4.1760216 -4.1756468 -4.1910167 -4.2086096 -4.2072644 -4.2045503][-4.2232704 -4.2189097 -4.2091594 -4.1887064 -4.1614165 -4.1303921 -4.10704 -4.0927992 -4.0940533 -4.0984893 -4.1113749 -4.1422496 -4.1768866 -4.190618 -4.1935906][-4.1831274 -4.1830549 -4.178833 -4.1566687 -4.1220551 -4.0747976 -4.0274343 -4.0092549 -4.0310864 -4.0580115 -4.0840144 -4.1185222 -4.1602364 -4.1875315 -4.2004595][-4.1398678 -4.1424961 -4.1440229 -4.12694 -4.0995235 -4.0499892 -3.9911575 -3.9811809 -4.0336943 -4.0848427 -4.1173038 -4.1437368 -4.1756506 -4.2031846 -4.2171125][-4.1158996 -4.1206455 -4.129602 -4.12495 -4.1156974 -4.0849671 -4.0443869 -4.0479417 -4.1034408 -4.1483555 -4.1697645 -4.18306 -4.1992536 -4.2150397 -4.2198911][-4.1171556 -4.1276941 -4.1455617 -4.1527486 -4.1593547 -4.1532307 -4.1369863 -4.1456871 -4.1830149 -4.2084603 -4.2163453 -4.2164164 -4.2191596 -4.2222824 -4.2186346][-4.1293778 -4.145885 -4.1682525 -4.1833034 -4.1984706 -4.2066145 -4.2024183 -4.2034321 -4.219604 -4.2324529 -4.2339315 -4.2294278 -4.2290525 -4.2277036 -4.2210088][-4.1390958 -4.1607666 -4.1871519 -4.2052212 -4.2220707 -4.2336321 -4.2295222 -4.221489 -4.2250724 -4.2322326 -4.2330093 -4.2309895 -4.2327 -4.2294822 -4.2235041][-4.138803 -4.164669 -4.1928458 -4.2110453 -4.2236457 -4.2302823 -4.2233267 -4.2164149 -4.2192416 -4.2264166 -4.2295265 -4.2309651 -4.230123 -4.2206678 -4.2141461][-4.1317873 -4.162776 -4.1894193 -4.2071815 -4.2200427 -4.2251811 -4.2194986 -4.2190175 -4.2238913 -4.2297215 -4.2308931 -4.2294207 -4.2239337 -4.2115254 -4.20811]]...]
INFO - root - 2017-12-05 21:39:54.790117: step 47010, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 69h:10m:25s remains)
INFO - root - 2017-12-05 21:40:03.251008: step 47020, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 68h:06m:37s remains)
INFO - root - 2017-12-05 21:40:11.705615: step 47030, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 67h:20m:46s remains)
INFO - root - 2017-12-05 21:40:20.224034: step 47040, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 67h:46m:55s remains)
INFO - root - 2017-12-05 21:40:28.704636: step 47050, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 66h:35m:05s remains)
INFO - root - 2017-12-05 21:40:37.174863: step 47060, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 67h:17m:34s remains)
INFO - root - 2017-12-05 21:40:45.668249: step 47070, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 68h:29m:03s remains)
INFO - root - 2017-12-05 21:40:54.219228: step 47080, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 66h:46m:32s remains)
INFO - root - 2017-12-05 21:41:02.896753: step 47090, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 68h:46m:24s remains)
INFO - root - 2017-12-05 21:41:11.403808: step 47100, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.837 sec/batch; 66h:23m:36s remains)
2017-12-05 21:41:12.161864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2998343 -4.298708 -4.2980614 -4.2984385 -4.2999396 -4.3016515 -4.3028126 -4.3030853 -4.3022561 -4.3005705 -4.2985873 -4.2974935 -4.2973671 -4.2977047 -4.2980723][-4.3043122 -4.3028893 -4.3016577 -4.3021431 -4.3048306 -4.3083224 -4.3114338 -4.3135514 -4.3136868 -4.3119831 -4.3092265 -4.3072295 -4.3061171 -4.3049741 -4.3033643][-4.3083758 -4.3056078 -4.3020606 -4.3012209 -4.3043857 -4.3095732 -4.3148484 -4.3199215 -4.3223929 -4.3218722 -4.3194356 -4.3178849 -4.3172083 -4.3155961 -4.3128014][-4.3095675 -4.3020611 -4.2920022 -4.2852297 -4.2852764 -4.28906 -4.2942662 -4.3019981 -4.3095269 -4.3149958 -4.3181238 -4.3217044 -4.3258181 -4.3264141 -4.3234043][-4.3056474 -4.2866273 -4.2616005 -4.2415562 -4.2330856 -4.2318444 -4.2328062 -4.2402315 -4.2540884 -4.2724929 -4.290854 -4.3093414 -4.3266621 -4.3352304 -4.3336439][-4.2913618 -4.2564955 -4.2112331 -4.1730351 -4.1510086 -4.1370764 -4.1259861 -4.1287189 -4.1530709 -4.1931815 -4.2363229 -4.2781849 -4.3150182 -4.3347545 -4.3362594][-4.2600679 -4.2129478 -4.1529937 -4.100009 -4.06431 -4.0342007 -4.002212 -3.9934692 -4.0299263 -4.096107 -4.1684132 -4.2354803 -4.2926316 -4.3243766 -4.3300929][-4.2146611 -4.1677055 -4.1040788 -4.0418215 -3.9903908 -3.9353821 -3.8739016 -3.8509648 -3.9013035 -3.9966958 -4.0980668 -4.1898756 -4.266192 -4.30991 -4.3210754][-4.1674275 -4.13534 -4.082623 -4.0234528 -3.9650877 -3.8909068 -3.8033471 -3.7662742 -3.8233249 -3.9341476 -4.051477 -4.1581221 -4.24618 -4.2976561 -4.3137317][-4.1402988 -4.1340094 -4.1061091 -4.0658312 -4.0167246 -3.9440975 -3.85516 -3.8135679 -3.8605249 -3.9597554 -4.0686212 -4.1697969 -4.2513475 -4.2977943 -4.3129621][-4.1595054 -4.1725249 -4.1681027 -4.1508141 -4.1214767 -4.0707068 -4.0058856 -3.9742556 -4.0039296 -4.0711975 -4.1493607 -4.2247744 -4.2832322 -4.3136187 -4.3205013][-4.2202964 -4.238781 -4.2433047 -4.2387681 -4.2267375 -4.2018056 -4.1684775 -4.152535 -4.1687651 -4.2056212 -4.2490177 -4.291431 -4.3203282 -4.3291397 -4.3234963][-4.2780275 -4.2906942 -4.2907305 -4.2861652 -4.2818828 -4.275342 -4.2656121 -4.2622557 -4.2711616 -4.2882328 -4.3064651 -4.3215623 -4.3259969 -4.319375 -4.3071513][-4.3009634 -4.3048706 -4.2973156 -4.2879829 -4.2852554 -4.2876263 -4.2907052 -4.2946367 -4.3010936 -4.3085008 -4.312705 -4.3107858 -4.3028607 -4.2925267 -4.2815552][-4.2939081 -4.2905173 -4.2782125 -4.266757 -4.2643323 -4.2701054 -4.278285 -4.2854252 -4.2906919 -4.2926054 -4.2894325 -4.2807546 -4.2699656 -4.2616229 -4.255662]]...]
INFO - root - 2017-12-05 21:41:20.627642: step 47110, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 67h:21m:01s remains)
INFO - root - 2017-12-05 21:41:29.068166: step 47120, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 67h:35m:13s remains)
INFO - root - 2017-12-05 21:41:37.582052: step 47130, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 68h:01m:47s remains)
INFO - root - 2017-12-05 21:41:46.139394: step 47140, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 67h:34m:45s remains)
INFO - root - 2017-12-05 21:41:54.651533: step 47150, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 67h:06m:53s remains)
INFO - root - 2017-12-05 21:42:03.175815: step 47160, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 66h:27m:41s remains)
INFO - root - 2017-12-05 21:42:11.698401: step 47170, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.850 sec/batch; 67h:21m:11s remains)
INFO - root - 2017-12-05 21:42:20.116095: step 47180, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.891 sec/batch; 70h:38m:18s remains)
INFO - root - 2017-12-05 21:42:28.645072: step 47190, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 69h:31m:05s remains)
INFO - root - 2017-12-05 21:42:37.118651: step 47200, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 67h:32m:49s remains)
2017-12-05 21:42:37.881487: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2078238 -4.1956034 -4.1867466 -4.1771326 -4.1721158 -4.1754918 -4.1773844 -4.1796 -4.1838136 -4.1904273 -4.1990786 -4.2074752 -4.2165871 -4.2270465 -4.2385497][-4.1898823 -4.1758356 -4.167891 -4.157496 -4.1519561 -4.1564083 -4.1593685 -4.1592546 -4.1592641 -4.1679516 -4.18077 -4.191103 -4.2027316 -4.2169046 -4.2322154][-4.1664453 -4.1540709 -4.1467748 -4.1340671 -4.1257644 -4.1318035 -4.1367893 -4.1351976 -4.1323118 -4.1415658 -4.1563687 -4.1674261 -4.183301 -4.2036815 -4.2240381][-4.151258 -4.142879 -4.1364841 -4.11961 -4.1053185 -4.107945 -4.1161404 -4.1186857 -4.1129661 -4.1185665 -4.1293058 -4.1385098 -4.158114 -4.1840615 -4.2091093][-4.1270208 -4.1192451 -4.1065726 -4.0790792 -4.0518332 -4.0500402 -4.0718737 -4.0880656 -4.0872254 -4.0935645 -4.1045527 -4.1157446 -4.13538 -4.1625853 -4.1877489][-4.1018691 -4.093492 -4.0728931 -4.0297527 -3.9778078 -3.9608059 -3.9966354 -4.0283146 -4.0360918 -4.0476079 -4.0672493 -4.0886526 -4.1145692 -4.1446447 -4.1689277][-4.1053548 -4.098599 -4.0802813 -4.0370383 -3.9786265 -3.9493909 -3.9788084 -4.0049076 -4.00681 -4.0162368 -4.0388937 -4.0663896 -4.0978389 -4.1328745 -4.1604476][-4.1401587 -4.1354561 -4.1207008 -4.0865955 -4.0404449 -4.0127516 -4.0229192 -4.0291853 -4.0213528 -4.0253768 -4.0464654 -4.0752974 -4.106492 -4.1416068 -4.1704865][-4.1813812 -4.1804209 -4.1701064 -4.1489167 -4.119307 -4.0976367 -4.0997176 -4.094779 -4.0804005 -4.07882 -4.0942745 -4.1165872 -4.1402435 -4.1672292 -4.1910758][-4.2302108 -4.2306442 -4.2221365 -4.2062087 -4.185513 -4.169631 -4.1710663 -4.1637573 -4.1483374 -4.1405387 -4.1506968 -4.1676822 -4.1867719 -4.2091608 -4.2282219][-4.2663078 -4.263814 -4.2541442 -4.2393417 -4.2231007 -4.2118487 -4.210989 -4.2061791 -4.1971192 -4.1895552 -4.1972632 -4.2129622 -4.23332 -4.2541604 -4.2682328][-4.2802539 -4.2755265 -4.2644358 -4.2531385 -4.2457609 -4.2425523 -4.2452383 -4.2470961 -4.2466483 -4.2411609 -4.2438741 -4.2539663 -4.2705331 -4.2855582 -4.29152][-4.2615876 -4.2564368 -4.2466407 -4.2396483 -4.2401109 -4.2454705 -4.2566438 -4.2675071 -4.2747312 -4.273334 -4.2747593 -4.2798004 -4.2873917 -4.2940516 -4.2932343][-4.2498484 -4.2443728 -4.2345886 -4.229033 -4.2334857 -4.242579 -4.2562647 -4.2684155 -4.2787552 -4.2834954 -4.2869153 -4.2896233 -4.2915254 -4.2916193 -4.2869635][-4.2617908 -4.2554865 -4.2449889 -4.2389078 -4.2422366 -4.2494564 -4.2590008 -4.2679243 -4.2785487 -4.2856359 -4.289453 -4.2905326 -4.2907267 -4.2901731 -4.2860932]]...]
INFO - root - 2017-12-05 21:42:46.409746: step 47210, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 68h:01m:12s remains)
INFO - root - 2017-12-05 21:42:54.976964: step 47220, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 67h:54m:59s remains)
INFO - root - 2017-12-05 21:43:03.465248: step 47230, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 66h:42m:43s remains)
INFO - root - 2017-12-05 21:43:12.125037: step 47240, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 66h:35m:56s remains)
INFO - root - 2017-12-05 21:43:20.695999: step 47250, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 69h:39m:27s remains)
INFO - root - 2017-12-05 21:43:29.129566: step 47260, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 66h:11m:31s remains)
INFO - root - 2017-12-05 21:43:37.622527: step 47270, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 67h:41m:24s remains)
INFO - root - 2017-12-05 21:43:46.131446: step 47280, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 67h:21m:33s remains)
INFO - root - 2017-12-05 21:43:54.571647: step 47290, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 69h:32m:05s remains)
INFO - root - 2017-12-05 21:44:03.110639: step 47300, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 68h:31m:22s remains)
2017-12-05 21:44:03.902355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3199553 -4.3124461 -4.3096857 -4.31195 -4.31912 -4.3280487 -4.3345819 -4.3380876 -4.3372316 -4.3320351 -4.3262558 -4.32765 -4.3347921 -4.3406296 -4.3425236][-4.2985883 -4.288094 -4.285018 -4.2892451 -4.2991405 -4.3130059 -4.3246026 -4.3337178 -4.3327436 -4.321701 -4.3092666 -4.3084779 -4.3183432 -4.3281384 -4.3338776][-4.2734518 -4.2618074 -4.2585845 -4.2627578 -4.2722683 -4.2889223 -4.3063927 -4.3246579 -4.3315077 -4.320684 -4.3043694 -4.2993031 -4.3069224 -4.3175039 -4.325563][-4.2492561 -4.2373762 -4.230248 -4.232038 -4.2353334 -4.2447438 -4.2626343 -4.2926702 -4.3173504 -4.3212781 -4.3147812 -4.3093605 -4.3105164 -4.3163123 -4.3234711][-4.2293167 -4.2163482 -4.2019205 -4.1926966 -4.1825547 -4.1733317 -4.1785169 -4.2154765 -4.26542 -4.2978067 -4.313643 -4.3210273 -4.3224154 -4.3254118 -4.3304958][-4.212184 -4.1944304 -4.1692181 -4.144073 -4.1151361 -4.0765395 -4.0505981 -4.0787387 -4.1516709 -4.2230949 -4.2730632 -4.3072939 -4.3271294 -4.3371067 -4.3438778][-4.1858158 -4.1609869 -4.1270919 -4.0899062 -4.043879 -3.9740119 -3.9055 -3.9060566 -3.9891808 -4.09648 -4.1827912 -4.2475371 -4.2947826 -4.3266387 -4.3435769][-4.1589041 -4.1309738 -4.0960617 -4.0554962 -4.0028844 -3.9101517 -3.7976031 -3.7490246 -3.8202081 -3.94459 -4.0520477 -4.1404171 -4.2151437 -4.2761221 -4.3128996][-4.1379213 -4.1089768 -4.0786314 -4.0432687 -3.995645 -3.9055896 -3.7892957 -3.7165184 -3.7406249 -3.8257809 -3.9172642 -4.0130653 -4.1088481 -4.1976042 -4.2603498][-4.1462369 -4.1220946 -4.0977216 -4.0722709 -4.0400405 -3.9749408 -3.8902977 -3.82855 -3.8109765 -3.8278458 -3.8704028 -3.9472113 -4.0420394 -4.1379776 -4.2134938][-4.19837 -4.1834493 -4.1678548 -4.1553307 -4.1429887 -4.1094742 -4.0618811 -4.02003 -3.9835267 -3.9578464 -3.9621842 -4.0087404 -4.0775428 -4.1524911 -4.2170386][-4.2655425 -4.2612205 -4.2516265 -4.2473531 -4.2486773 -4.2385135 -4.219172 -4.1952038 -4.1630363 -4.1304164 -4.1187239 -4.1372252 -4.1724286 -4.2160969 -4.2581058][-4.3204169 -4.3230996 -4.3202138 -4.3210897 -4.32679 -4.3257866 -4.318646 -4.306334 -4.288269 -4.2664595 -4.2529168 -4.2546887 -4.2669053 -4.2855153 -4.305625][-4.35083 -4.3538218 -4.3541059 -4.357029 -4.3616161 -4.361856 -4.3592238 -4.3550115 -4.3489432 -4.3400154 -4.331039 -4.3273525 -4.3290648 -4.3334432 -4.3389187][-4.3566394 -4.3568912 -4.3571835 -4.3588128 -4.36051 -4.3603392 -4.3592286 -4.35848 -4.358819 -4.358355 -4.3551636 -4.3527546 -4.3521566 -4.351831 -4.3528514]]...]
INFO - root - 2017-12-05 21:44:12.418519: step 47310, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 66h:28m:08s remains)
INFO - root - 2017-12-05 21:44:20.819149: step 47320, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 65h:52m:24s remains)
INFO - root - 2017-12-05 21:44:29.289464: step 47330, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 67h:21m:06s remains)
INFO - root - 2017-12-05 21:44:37.904935: step 47340, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 69h:06m:49s remains)
INFO - root - 2017-12-05 21:44:46.535949: step 47350, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 70h:29m:45s remains)
INFO - root - 2017-12-05 21:44:55.021581: step 47360, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 68h:44m:05s remains)
INFO - root - 2017-12-05 21:45:03.485280: step 47370, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 64h:41m:35s remains)
INFO - root - 2017-12-05 21:45:12.010473: step 47380, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 67h:32m:02s remains)
INFO - root - 2017-12-05 21:45:20.479659: step 47390, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 66h:53m:00s remains)
INFO - root - 2017-12-05 21:45:28.988266: step 47400, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 68h:52m:33s remains)
2017-12-05 21:45:29.769157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1925693 -4.1969829 -4.1877127 -4.1777325 -4.1670833 -4.1687331 -4.1802168 -4.1898537 -4.1902823 -4.1918588 -4.1996832 -4.2000847 -4.1841431 -4.1620784 -4.1402154][-4.1760063 -4.1656303 -4.1474137 -4.1342096 -4.1236396 -4.1226892 -4.1309071 -4.1360621 -4.123775 -4.1128993 -4.121819 -4.13294 -4.1252108 -4.1080379 -4.0853782][-4.1329288 -4.11415 -4.0932131 -4.079453 -4.0689621 -4.0667729 -4.0829282 -4.0947762 -4.0780969 -4.0660868 -4.0815954 -4.0957346 -4.0924134 -4.0806127 -4.0523129][-4.0825591 -4.0730462 -4.0628929 -4.0537572 -4.0413313 -4.0327425 -4.047483 -4.0613956 -4.044848 -4.0380282 -4.0612464 -4.0766582 -4.0775013 -4.0776305 -4.0583177][-4.060698 -4.0669532 -4.072248 -4.072072 -4.0580053 -4.0406871 -4.0431962 -4.0446634 -4.0217957 -4.0150056 -4.0423703 -4.0642457 -4.0735083 -4.0865922 -4.0828667][-4.0873661 -4.1048794 -4.1138539 -4.1117339 -4.0834146 -4.0423889 -4.0192924 -3.9972558 -3.9740965 -3.9783549 -4.015892 -4.0564976 -4.0818205 -4.1054759 -4.1153388][-4.1430836 -4.1525006 -4.1475544 -4.1346216 -4.0873294 -4.01917 -3.9745169 -3.9397395 -3.9315231 -3.9580214 -4.00577 -4.0617023 -4.1033821 -4.1381578 -4.1616611][-4.187398 -4.182148 -4.1668992 -4.14546 -4.0891037 -4.0219398 -3.9764283 -3.9379611 -3.9517891 -3.9983544 -4.0491719 -4.1060443 -4.1521554 -4.1894078 -4.2111082][-4.2070818 -4.1955142 -4.181942 -4.1651769 -4.1178131 -4.0765162 -4.0447879 -4.0141282 -4.0451026 -4.0990996 -4.1435513 -4.1844621 -4.21561 -4.2397785 -4.2503314][-4.2007093 -4.1907792 -4.1922522 -4.1901712 -4.1602364 -4.1458926 -4.1348972 -4.121582 -4.1558628 -4.2028437 -4.2323704 -4.255486 -4.2689233 -4.2770286 -4.2789555][-4.1911454 -4.1848965 -4.1970224 -4.203445 -4.1905189 -4.1928024 -4.1981435 -4.20127 -4.2312751 -4.2637143 -4.2826915 -4.2954049 -4.300487 -4.3008242 -4.297533][-4.1961055 -4.1893811 -4.1972642 -4.2052407 -4.20243 -4.2134504 -4.2274532 -4.2377644 -4.2585106 -4.2804127 -4.2970986 -4.3099594 -4.3132296 -4.3079958 -4.2992396][-4.2122684 -4.2086973 -4.2104535 -4.213716 -4.2129855 -4.218802 -4.2278771 -4.2335291 -4.2440376 -4.260457 -4.2792611 -4.29665 -4.3017259 -4.2941689 -4.2820163][-4.2359662 -4.2329168 -4.2287307 -4.2267733 -4.22084 -4.2177272 -4.219686 -4.2209749 -4.2297316 -4.247138 -4.2658806 -4.2824597 -4.2867441 -4.2768006 -4.2621384][-4.2522483 -4.255867 -4.2491531 -4.2401724 -4.227653 -4.2208438 -4.2218432 -4.2235026 -4.2383766 -4.2597518 -4.2760692 -4.2861323 -4.2840223 -4.2680774 -4.2474151]]...]
INFO - root - 2017-12-05 21:45:38.389029: step 47410, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 68h:30m:30s remains)
INFO - root - 2017-12-05 21:45:47.004441: step 47420, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 66h:37m:29s remains)
INFO - root - 2017-12-05 21:45:55.427638: step 47430, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 66h:27m:27s remains)
INFO - root - 2017-12-05 21:46:04.037375: step 47440, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 69h:09m:28s remains)
INFO - root - 2017-12-05 21:46:12.609763: step 47450, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 66h:59m:47s remains)
INFO - root - 2017-12-05 21:46:21.076386: step 47460, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 66h:39m:13s remains)
INFO - root - 2017-12-05 21:46:29.633944: step 47470, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 70h:32m:43s remains)
INFO - root - 2017-12-05 21:46:38.207971: step 47480, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.892 sec/batch; 70h:35m:48s remains)
INFO - root - 2017-12-05 21:46:46.748662: step 47490, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 67h:57m:49s remains)
INFO - root - 2017-12-05 21:46:55.396945: step 47500, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 70h:15m:07s remains)
2017-12-05 21:46:56.134360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1826687 -4.1739335 -4.1408486 -4.1215086 -4.1392884 -4.1791468 -4.2155337 -4.242928 -4.2655478 -4.279541 -4.2769461 -4.2621207 -4.2568231 -4.260901 -4.2640882][-4.2057071 -4.1969295 -4.1702023 -4.1527276 -4.1645651 -4.1951332 -4.2265191 -4.2521372 -4.2676115 -4.2777076 -4.2780681 -4.2677088 -4.2667551 -4.2693896 -4.2712183][-4.2433877 -4.2296824 -4.2088885 -4.1906204 -4.1855497 -4.1976604 -4.2189913 -4.2399845 -4.2531309 -4.2653508 -4.267695 -4.2607026 -4.2608905 -4.2629642 -4.2662115][-4.2577114 -4.240449 -4.2252278 -4.2063603 -4.1853685 -4.1713338 -4.1754484 -4.1956429 -4.2175479 -4.2351756 -4.2419639 -4.2384729 -4.233696 -4.2359667 -4.2449164][-4.244833 -4.2251983 -4.2117214 -4.1901841 -4.154377 -4.1154485 -4.0972219 -4.1274018 -4.17674 -4.206471 -4.2174544 -4.21629 -4.2031522 -4.200501 -4.2123413][-4.2061009 -4.1822314 -4.1672416 -4.1378846 -4.0814028 -4.00781 -3.9673789 -4.0240254 -4.1163645 -4.1663876 -4.1839161 -4.1831489 -4.1603308 -4.1514478 -4.1662154][-4.1794868 -4.1477776 -4.1196666 -4.0730357 -3.9835672 -3.8533688 -3.7766166 -3.8875339 -4.0394835 -4.1116867 -4.1360922 -4.1276941 -4.09339 -4.0838046 -4.1024189][-4.1763096 -4.1375918 -4.0959148 -4.0330863 -3.9185896 -3.732681 -3.6106339 -3.7756271 -3.9741521 -4.0613389 -4.0896053 -4.0750275 -4.0372705 -4.0315609 -4.0656314][-4.1897969 -4.1529684 -4.1106138 -4.05102 -3.9438853 -3.7674539 -3.6559324 -3.8155091 -3.9978685 -4.0772424 -4.0992494 -4.0753646 -4.0424042 -4.0449481 -4.0881457][-4.2032847 -4.1770916 -4.1486845 -4.1060781 -4.0297556 -3.9129744 -3.8531957 -3.9637341 -4.0804143 -4.1324549 -4.1382408 -4.1040211 -4.0781579 -4.094945 -4.1405087][-4.2120323 -4.195478 -4.1861539 -4.1639194 -4.1206112 -4.0575223 -4.035615 -4.0940137 -4.1509285 -4.1776867 -4.1683235 -4.1299648 -4.1107788 -4.1352196 -4.1747847][-4.2141247 -4.2059669 -4.2111974 -4.2086558 -4.1919518 -4.1627927 -4.1545234 -4.1766348 -4.1987209 -4.2095556 -4.1953783 -4.1680593 -4.1610088 -4.1801777 -4.2014713][-4.2225857 -4.2200003 -4.2293773 -4.2365837 -4.2343855 -4.2248926 -4.222074 -4.225255 -4.2309027 -4.23541 -4.2234755 -4.2118177 -4.21347 -4.2251029 -4.2350821][-4.2368793 -4.2371264 -4.2434211 -4.2491207 -4.2530651 -4.2556119 -4.25806 -4.2538843 -4.2525687 -4.2585282 -4.2522836 -4.2471676 -4.2509389 -4.2587032 -4.2642636][-4.2568116 -4.2569609 -4.2616024 -4.2669592 -4.2709188 -4.2760954 -4.2791924 -4.2728267 -4.2686486 -4.2767534 -4.2787881 -4.277277 -4.2770252 -4.2801108 -4.2868881]]...]
INFO - root - 2017-12-05 21:47:04.647176: step 47510, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.844 sec/batch; 66h:50m:43s remains)
INFO - root - 2017-12-05 21:47:13.206100: step 47520, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 67h:35m:27s remains)
INFO - root - 2017-12-05 21:47:21.935473: step 47530, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 66h:30m:50s remains)
INFO - root - 2017-12-05 21:47:30.445820: step 47540, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 67h:22m:23s remains)
INFO - root - 2017-12-05 21:47:39.004664: step 47550, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 66h:58m:42s remains)
INFO - root - 2017-12-05 21:47:47.448314: step 47560, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 66h:48m:44s remains)
INFO - root - 2017-12-05 21:47:55.978462: step 47570, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 67h:57m:34s remains)
INFO - root - 2017-12-05 21:48:04.581442: step 47580, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.871 sec/batch; 68h:54m:59s remains)
INFO - root - 2017-12-05 21:48:13.073322: step 47590, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 68h:02m:40s remains)
INFO - root - 2017-12-05 21:48:21.598920: step 47600, loss = 2.03, batch loss = 1.97 (9.8 examples/sec; 0.814 sec/batch; 64h:24m:06s remains)
2017-12-05 21:48:22.386033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2144995 -4.2194476 -4.2269344 -4.2375283 -4.2550955 -4.2642565 -4.2663174 -4.2762833 -4.2940149 -4.2985325 -4.2875443 -4.2771358 -4.270659 -4.2729559 -4.2835727][-4.1785383 -4.1850042 -4.1984668 -4.2195239 -4.2455487 -4.2588835 -4.2608652 -4.270515 -4.2893491 -4.2905321 -4.2675853 -4.245616 -4.2314529 -4.232348 -4.2559881][-4.1230106 -4.1440697 -4.1713824 -4.1986284 -4.2264714 -4.2406807 -4.2383785 -4.2459488 -4.2651529 -4.26511 -4.2308435 -4.1994557 -4.1831017 -4.1845164 -4.2211485][-4.0950661 -4.1356292 -4.1750855 -4.20194 -4.2158961 -4.2151847 -4.1997647 -4.2014756 -4.22656 -4.2365475 -4.2077055 -4.1780987 -4.1636882 -4.1670337 -4.206243][-4.1222348 -4.1695514 -4.2025375 -4.2159953 -4.2052951 -4.1782012 -4.1398635 -4.1332088 -4.1690373 -4.2005949 -4.1972594 -4.1867776 -4.1803994 -4.181982 -4.2064428][-4.1712384 -4.2031775 -4.2106385 -4.1978707 -4.1601429 -4.1010585 -4.032311 -4.0166245 -4.0693741 -4.128406 -4.1603184 -4.1787357 -4.1876369 -4.1940117 -4.2029657][-4.1931863 -4.1979623 -4.1755843 -4.1380162 -4.075429 -3.9831223 -3.8802152 -3.8528216 -3.9303594 -4.0222416 -4.0848308 -4.1293116 -4.1564908 -4.1688251 -4.1676745][-4.1806631 -4.1630273 -4.121201 -4.0718255 -3.9988446 -3.889647 -3.7689176 -3.7347994 -3.8263817 -3.935987 -4.0079989 -4.0631862 -4.0973067 -4.1051555 -4.0936103][-4.1615682 -4.1334972 -4.08676 -4.0432868 -3.9814 -3.8906169 -3.802629 -3.7843485 -3.8500504 -3.9328136 -3.9878857 -4.0341706 -4.0611882 -4.0579138 -4.0344071][-4.1661329 -4.1347313 -4.0921879 -4.0645518 -4.0243206 -3.967217 -3.9295526 -3.9340811 -3.9657738 -4.0062609 -4.033618 -4.0595784 -4.0715675 -4.0558424 -4.021893][-4.2008367 -4.1698108 -4.1350074 -4.1219931 -4.1038246 -4.0725965 -4.0652866 -4.0823584 -4.0960603 -4.1081319 -4.1172705 -4.1285 -4.1285663 -4.1046772 -4.0674033][-4.2440119 -4.21818 -4.191638 -4.1858439 -4.1798081 -4.1652327 -4.170898 -4.1881952 -4.1929746 -4.1966767 -4.20241 -4.2079597 -4.206039 -4.1850147 -4.1540537][-4.2743692 -4.254847 -4.2324033 -4.2247419 -4.2192063 -4.2120485 -4.221519 -4.2349806 -4.2392035 -4.2471223 -4.2566705 -4.2630663 -4.2644548 -4.2533197 -4.234858][-4.2844181 -4.2674384 -4.2467895 -4.23357 -4.2255583 -4.2184296 -4.2234621 -4.2329473 -4.2404294 -4.25693 -4.2748137 -4.2847247 -4.289721 -4.2866492 -4.2777123][-4.2814031 -4.2664676 -4.2490783 -4.2341375 -4.224185 -4.2149067 -4.2141504 -4.219492 -4.2296023 -4.25124 -4.2727747 -4.2804909 -4.2830234 -4.283494 -4.2789068]]...]
INFO - root - 2017-12-05 21:48:30.947573: step 47610, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 67h:18m:29s remains)
INFO - root - 2017-12-05 21:48:39.400051: step 47620, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 67h:32m:33s remains)
INFO - root - 2017-12-05 21:48:48.018564: step 47630, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 67h:48m:36s remains)
INFO - root - 2017-12-05 21:48:56.449652: step 47640, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 67h:53m:08s remains)
INFO - root - 2017-12-05 21:49:04.992149: step 47650, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 66h:05m:34s remains)
INFO - root - 2017-12-05 21:49:13.467208: step 47660, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 67h:51m:34s remains)
INFO - root - 2017-12-05 21:49:21.921295: step 47670, loss = 2.10, batch loss = 2.05 (9.4 examples/sec; 0.847 sec/batch; 67h:02m:15s remains)
INFO - root - 2017-12-05 21:49:30.387374: step 47680, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.839 sec/batch; 66h:21m:23s remains)
INFO - root - 2017-12-05 21:49:38.951655: step 47690, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 69h:14m:57s remains)
INFO - root - 2017-12-05 21:49:47.496192: step 47700, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 68h:49m:16s remains)
2017-12-05 21:49:48.226986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2922325 -4.2842007 -4.2845883 -4.2856517 -4.2819457 -4.2834191 -4.289083 -4.2941327 -4.3020821 -4.3082943 -4.3125911 -4.3196783 -4.3288879 -4.3347616 -4.3369036][-4.2817388 -4.2692194 -4.2649126 -4.2614555 -4.2541752 -4.2544141 -4.2639089 -4.2747464 -4.2885613 -4.2990832 -4.3051753 -4.3138418 -4.3254638 -4.3337431 -4.3370814][-4.259625 -4.2395716 -4.2210941 -4.2045255 -4.1873922 -4.1840777 -4.1992745 -4.222331 -4.2446036 -4.2599216 -4.2689419 -4.2779408 -4.2944241 -4.309587 -4.3186297][-4.2292204 -4.1986408 -4.1632886 -4.1242423 -4.0877914 -4.0757809 -4.0974293 -4.1340237 -4.169714 -4.1948924 -4.2121506 -4.2270269 -4.2517958 -4.2765923 -4.291533][-4.1963267 -4.1571708 -4.1103945 -4.0498662 -3.9851654 -3.9482777 -3.9629314 -4.0180902 -4.0797558 -4.1237717 -4.154604 -4.1818528 -4.2209277 -4.2562313 -4.2753806][-4.16531 -4.1168609 -4.0612636 -3.9810925 -3.8817821 -3.8023882 -3.7849693 -3.8535612 -3.9539897 -4.03238 -4.0866122 -4.13135 -4.1850252 -4.2324953 -4.2565818][-4.1487641 -4.0906515 -4.0266619 -3.9353211 -3.8132522 -3.6961629 -3.6427159 -3.7108903 -3.840574 -3.948698 -4.0236068 -4.0806961 -4.1436396 -4.2036562 -4.2352872][-4.1679173 -4.1085763 -4.0491724 -3.9708109 -3.8700166 -3.7739582 -3.7346601 -3.7867517 -3.8863182 -3.9766276 -4.0435143 -4.0958633 -4.1545296 -4.2146282 -4.2446208][-4.20416 -4.1537924 -4.1073074 -4.0513868 -3.9848661 -3.9299161 -3.9178863 -3.9509768 -4.0048089 -4.0596542 -4.1094208 -4.1553111 -4.2043471 -4.2539148 -4.2740307][-4.2259493 -4.1853137 -4.1496439 -4.1100821 -4.06555 -4.0360336 -4.0383892 -4.063004 -4.0920568 -4.1231351 -4.1618972 -4.2028751 -4.2443609 -4.2831106 -4.2961531][-4.2319984 -4.2020564 -4.175879 -4.1453919 -4.1113734 -4.0951967 -4.1070595 -4.1286941 -4.145802 -4.1647339 -4.1959243 -4.2296281 -4.2636623 -4.2939243 -4.3030005][-4.2369962 -4.2138839 -4.1921396 -4.1681414 -4.14259 -4.1362877 -4.1544561 -4.1766233 -4.193934 -4.2104568 -4.2332439 -4.2558231 -4.2807436 -4.3018861 -4.3072963][-4.2510724 -4.2297497 -4.2108846 -4.1931214 -4.1769371 -4.1763072 -4.1947379 -4.2127528 -4.2276878 -4.2453084 -4.2647252 -4.28049 -4.2972856 -4.3092289 -4.3102827][-4.2746921 -4.2559004 -4.238688 -4.2245035 -4.2151055 -4.2170739 -4.2278919 -4.2346735 -4.2439394 -4.2616687 -4.2803259 -4.293602 -4.3027053 -4.3066058 -4.3070431][-4.3037076 -4.2903128 -4.276011 -4.2646523 -4.2577577 -4.2569909 -4.2593131 -4.2591171 -4.2648268 -4.2803373 -4.2960567 -4.3063192 -4.3119092 -4.3135614 -4.3135834]]...]
INFO - root - 2017-12-05 21:49:56.780123: step 47710, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 67h:28m:03s remains)
INFO - root - 2017-12-05 21:50:05.376896: step 47720, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 68h:20m:02s remains)
INFO - root - 2017-12-05 21:50:13.819940: step 47730, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 66h:41m:50s remains)
INFO - root - 2017-12-05 21:50:22.434282: step 47740, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 66h:05m:32s remains)
INFO - root - 2017-12-05 21:50:30.812805: step 47750, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 67h:18m:22s remains)
INFO - root - 2017-12-05 21:50:39.206537: step 47760, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 66h:22m:15s remains)
INFO - root - 2017-12-05 21:50:47.751631: step 47770, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 69h:49m:29s remains)
INFO - root - 2017-12-05 21:50:56.286114: step 47780, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 66h:46m:36s remains)
INFO - root - 2017-12-05 21:51:04.747853: step 47790, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 66h:35m:43s remains)
INFO - root - 2017-12-05 21:51:13.300614: step 47800, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 66h:54m:42s remains)
2017-12-05 21:51:14.107964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2840772 -4.2812314 -4.268724 -4.261333 -4.2552996 -4.2425017 -4.2413793 -4.2632656 -4.2781343 -4.285655 -4.2870874 -4.2901783 -4.2968569 -4.2996678 -4.2940774][-4.2782016 -4.2736125 -4.2648792 -4.2638221 -4.2611403 -4.2460485 -4.2402649 -4.2630239 -4.2798381 -4.288487 -4.2932072 -4.299252 -4.3030252 -4.3012486 -4.294004][-4.2719393 -4.2648215 -4.257699 -4.2598009 -4.25963 -4.2443357 -4.2372422 -4.2610335 -4.2801528 -4.2905736 -4.2991643 -4.3071065 -4.3076949 -4.3012967 -4.294157][-4.2687144 -4.2596288 -4.2509656 -4.2516465 -4.2521043 -4.2364936 -4.2291718 -4.2535381 -4.2718897 -4.2815714 -4.293632 -4.3043022 -4.3051815 -4.2990723 -4.294414][-4.266398 -4.2568688 -4.2459383 -4.2431684 -4.2414265 -4.2251606 -4.2200923 -4.2439971 -4.2603092 -4.2699761 -4.2859726 -4.2983241 -4.2996063 -4.2956958 -4.2941394][-4.2629619 -4.25116 -4.23752 -4.2319269 -4.2287579 -4.2139225 -4.213212 -4.2363133 -4.2493544 -4.2582412 -4.2754016 -4.2862597 -4.2866411 -4.2842064 -4.2840815][-4.2587733 -4.2440281 -4.226963 -4.2199907 -4.2176905 -4.2048569 -4.2069983 -4.2279205 -4.2373538 -4.2433796 -4.2567139 -4.261416 -4.2565694 -4.2533422 -4.2556138][-4.2542439 -4.2368121 -4.2162771 -4.2070527 -4.2036324 -4.191493 -4.195549 -4.2167959 -4.2239633 -4.2270322 -4.2365394 -4.2369118 -4.2276583 -4.223958 -4.2297373][-4.2480464 -4.2285275 -4.2054543 -4.1927047 -4.1860027 -4.1711097 -4.1769481 -4.2041631 -4.2129054 -4.2124362 -4.2192421 -4.2180963 -4.2081428 -4.2051182 -4.2143021][-4.2424822 -4.2210507 -4.1957073 -4.178359 -4.1662717 -4.1487479 -4.1570129 -4.1907063 -4.2028646 -4.1996193 -4.2019691 -4.2000475 -4.1903076 -4.1857681 -4.1973238][-4.2390828 -4.2154808 -4.187654 -4.1663566 -4.1508732 -4.1325917 -4.1419215 -4.1778693 -4.1898274 -4.1828814 -4.1808147 -4.1766963 -4.1664009 -4.162066 -4.181447][-4.236948 -4.2102509 -4.1782861 -4.1535568 -4.1379113 -4.1227369 -4.1332374 -4.1686249 -4.1797853 -4.1707282 -4.1652055 -4.1595645 -4.1507955 -4.1494374 -4.175457][-4.2364774 -4.2072587 -4.1718278 -4.14482 -4.1297827 -4.1192374 -4.13156 -4.1655893 -4.1751995 -4.1652293 -4.1580791 -4.1524348 -4.1467829 -4.14902 -4.177774][-4.2369418 -4.2052507 -4.16754 -4.138773 -4.1239166 -4.1168785 -4.13141 -4.1633883 -4.1719184 -4.1625123 -4.1540217 -4.1476517 -4.1438904 -4.1482453 -4.1741967][-4.2397714 -4.2064848 -4.1674256 -4.1364646 -4.1197305 -4.1141038 -4.130055 -4.1591916 -4.1684093 -4.1625681 -4.1558361 -4.15254 -4.1536121 -4.1605611 -4.1781139]]...]
INFO - root - 2017-12-05 21:51:22.566915: step 47810, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 65h:53m:24s remains)
INFO - root - 2017-12-05 21:51:31.108220: step 47820, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 66h:13m:10s remains)
INFO - root - 2017-12-05 21:51:39.618542: step 47830, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 67h:52m:54s remains)
INFO - root - 2017-12-05 21:51:48.188305: step 47840, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 66h:48m:38s remains)
INFO - root - 2017-12-05 21:51:56.601605: step 47850, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.825 sec/batch; 65h:13m:20s remains)
INFO - root - 2017-12-05 21:52:04.964130: step 47860, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.830 sec/batch; 65h:36m:18s remains)
INFO - root - 2017-12-05 21:52:13.407794: step 47870, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 65h:54m:58s remains)
INFO - root - 2017-12-05 21:52:21.919675: step 47880, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 65h:50m:13s remains)
INFO - root - 2017-12-05 21:52:30.360231: step 47890, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 67h:54m:28s remains)
INFO - root - 2017-12-05 21:52:38.714014: step 47900, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 68h:13m:29s remains)
2017-12-05 21:52:39.504347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2237563 -4.2386532 -4.2504992 -4.2586093 -4.2624879 -4.2612686 -4.2576509 -4.2545214 -4.253191 -4.2567625 -4.2610536 -4.2594738 -4.2563953 -4.2495785 -4.2392249][-4.1883535 -4.2103329 -4.22701 -4.2371011 -4.2422252 -4.2464213 -4.2558541 -4.2693973 -4.28082 -4.2870388 -4.290843 -4.2844353 -4.2713146 -4.2524991 -4.2317524][-4.1731071 -4.1967378 -4.2135868 -4.2209954 -4.2204866 -4.2244363 -4.239758 -4.2639384 -4.2845688 -4.2943492 -4.2982388 -4.28768 -4.263402 -4.2353139 -4.2135477][-4.1808047 -4.19738 -4.2032027 -4.1994643 -4.1879754 -4.1849537 -4.1982417 -4.2281671 -4.25914 -4.2765632 -4.2843256 -4.27389 -4.2453256 -4.215692 -4.2006245][-4.2042727 -4.2039804 -4.1907735 -4.1701522 -4.144165 -4.1284752 -4.1310034 -4.1589489 -4.2015643 -4.2315812 -4.2497363 -4.2479253 -4.2249246 -4.2016068 -4.1966386][-4.2320771 -4.2114563 -4.1739755 -4.131897 -4.0865068 -4.0517564 -4.0349407 -4.0554986 -4.112433 -4.16169 -4.1988859 -4.2169728 -4.2105823 -4.198699 -4.2002273][-4.2591052 -4.2206059 -4.1563792 -4.0871058 -4.0153575 -3.9560208 -3.9095764 -3.9154508 -3.9881964 -4.0622425 -4.1289525 -4.1764569 -4.1968193 -4.2027154 -4.2090607][-4.2722635 -4.2270379 -4.1492443 -4.0630112 -3.972822 -3.8974214 -3.8269465 -3.8169353 -3.892612 -3.981338 -4.0678296 -4.1415963 -4.1921258 -4.2213764 -4.2362385][-4.2685757 -4.22958 -4.1568146 -4.0754724 -3.99775 -3.9403851 -3.8854749 -3.8750913 -3.930208 -4.0013528 -4.08033 -4.1562023 -4.2189093 -4.2607737 -4.2817559][-4.265193 -4.2409368 -4.1842918 -4.1188822 -4.063756 -4.0336437 -4.0070353 -4.0059757 -4.0439353 -4.0921006 -4.14922 -4.2097297 -4.2644525 -4.3005648 -4.31777][-4.2739911 -4.2648821 -4.22801 -4.1833458 -4.1499515 -4.1407294 -4.1353269 -4.140955 -4.1673861 -4.1966028 -4.2308521 -4.2690763 -4.3051181 -4.3267717 -4.3357697][-4.29488 -4.2980533 -4.280673 -4.2576756 -4.2407064 -4.2393241 -4.2393556 -4.2447376 -4.2606487 -4.276545 -4.2938061 -4.3123155 -4.3289504 -4.3366404 -4.3376713][-4.3160448 -4.3258524 -4.3229847 -4.3151593 -4.3068767 -4.3053079 -4.3031688 -4.3049822 -4.313314 -4.3222823 -4.3309808 -4.3373556 -4.3409648 -4.3397994 -4.3357515][-4.3307333 -4.3431911 -4.3475342 -4.3479939 -4.3431144 -4.3384805 -4.3326259 -4.3302755 -4.3337293 -4.3393145 -4.3442707 -4.3465066 -4.3454766 -4.3412437 -4.335403][-4.3320546 -4.3432045 -4.350543 -4.3549042 -4.354023 -4.3492131 -4.3426538 -4.3382421 -4.3384895 -4.3417492 -4.3454151 -4.3470116 -4.3452005 -4.3404236 -4.3347669]]...]
INFO - root - 2017-12-05 21:52:48.002664: step 47910, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 67h:39m:38s remains)
INFO - root - 2017-12-05 21:52:56.537303: step 47920, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.829 sec/batch; 65h:30m:58s remains)
INFO - root - 2017-12-05 21:53:05.126368: step 47930, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 68h:12m:07s remains)
INFO - root - 2017-12-05 21:53:13.666288: step 47940, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.863 sec/batch; 68h:14m:56s remains)
INFO - root - 2017-12-05 21:53:22.160184: step 47950, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.838 sec/batch; 66h:11m:54s remains)
INFO - root - 2017-12-05 21:53:30.466795: step 47960, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 67h:47m:43s remains)
INFO - root - 2017-12-05 21:53:39.017335: step 47970, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 69h:52m:09s remains)
INFO - root - 2017-12-05 21:53:47.553758: step 47980, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 65h:13m:57s remains)
INFO - root - 2017-12-05 21:53:56.007222: step 47990, loss = 2.03, batch loss = 1.98 (9.1 examples/sec; 0.876 sec/batch; 69h:11m:47s remains)
INFO - root - 2017-12-05 21:54:04.545953: step 48000, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 66h:00m:20s remains)
2017-12-05 21:54:05.279126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1154189 -4.1244574 -4.1314445 -4.1353478 -4.1399622 -4.1355519 -4.1159449 -4.1074395 -4.1119847 -4.1132107 -4.1070156 -4.0964689 -4.0917559 -4.0923233 -4.0950313][-4.108386 -4.1223216 -4.1309543 -4.1372261 -4.1439595 -4.1467366 -4.1321993 -4.119616 -4.1171732 -4.1159782 -4.1089554 -4.095777 -4.0864434 -4.0900221 -4.1001883][-4.1101208 -4.1272016 -4.1335969 -4.1320252 -4.1378789 -4.1452851 -4.1370082 -4.1241236 -4.1147408 -4.1102715 -4.1004539 -4.0857129 -4.0756817 -4.0814495 -4.095283][-4.1071033 -4.1226864 -4.1265774 -4.1213388 -4.1258845 -4.1295481 -4.1224771 -4.1195555 -4.1157279 -4.1123724 -4.1012173 -4.0843005 -4.0720806 -4.07502 -4.0856657][-4.0863986 -4.0982237 -4.1032481 -4.1009622 -4.100821 -4.0928922 -4.0783181 -4.0825076 -4.0955729 -4.1004643 -4.0870833 -4.0655665 -4.0540776 -4.0606995 -4.0722523][-4.0599508 -4.0722485 -4.0754566 -4.0692539 -4.0580006 -4.0348382 -4.009851 -4.0154366 -4.0407224 -4.0511332 -4.03802 -4.0150747 -4.0111165 -4.0274343 -4.0422564][-4.0335331 -4.0465841 -4.04741 -4.0330863 -4.0125213 -3.9805732 -3.9529645 -3.9599147 -3.9866912 -4.0003424 -3.9970531 -3.9851489 -3.9888496 -4.0034986 -4.0099549][-4.0118527 -4.0225167 -4.0260625 -4.0121155 -3.9880474 -3.9641752 -3.9496026 -3.9598732 -3.9824185 -3.9949582 -4.0010138 -4.0044427 -4.0128689 -4.0139627 -4.0012908][-4.0092316 -4.0169587 -4.0276165 -4.0249271 -4.0131278 -4.0055976 -4.0074739 -4.0160055 -4.0282493 -4.0337415 -4.040802 -4.0481353 -4.0572357 -4.0554342 -4.0383873][-4.0131378 -4.0207257 -4.0371542 -4.0557942 -4.0636783 -4.0667405 -4.0743556 -4.083982 -4.0918379 -4.0926447 -4.0961981 -4.1039233 -4.1140366 -4.1190486 -4.107976][-4.0214748 -4.027945 -4.0518074 -4.0857277 -4.1082573 -4.11764 -4.1277523 -4.1397181 -4.1499629 -4.1499548 -4.1495628 -4.1566644 -4.1651821 -4.170547 -4.1622615][-4.0463114 -4.0480256 -4.0698996 -4.1046691 -4.1301427 -4.1431885 -4.1571555 -4.1710315 -4.181087 -4.1817408 -4.1773939 -4.1801491 -4.182981 -4.1862464 -4.18412][-4.0776868 -4.0752096 -4.0896368 -4.1122751 -4.1290684 -4.1391249 -4.1515784 -4.1574717 -4.1625423 -4.165997 -4.1624393 -4.15913 -4.1600266 -4.1667018 -4.1735396][-4.0985703 -4.0929055 -4.0996747 -4.1061225 -4.1079555 -4.1100683 -4.11349 -4.1154909 -4.1205406 -4.127141 -4.1261115 -4.1174498 -4.1126986 -4.1193628 -4.1291189][-4.0980525 -4.084023 -4.0825691 -4.0800719 -4.0733781 -4.0700989 -4.0684028 -4.0700808 -4.0798664 -4.0952635 -4.0964108 -4.0797038 -4.0722923 -4.078898 -4.0831327]]...]
INFO - root - 2017-12-05 21:54:13.987114: step 48010, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 66h:36m:34s remains)
INFO - root - 2017-12-05 21:54:22.513052: step 48020, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 66h:44m:23s remains)
INFO - root - 2017-12-05 21:54:31.101887: step 48030, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 67h:30m:03s remains)
INFO - root - 2017-12-05 21:54:39.721564: step 48040, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 66h:29m:15s remains)
INFO - root - 2017-12-05 21:54:48.460000: step 48050, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 69h:10m:02s remains)
INFO - root - 2017-12-05 21:54:56.769425: step 48060, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 68h:33m:16s remains)
INFO - root - 2017-12-05 21:55:05.249565: step 48070, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 67h:12m:57s remains)
INFO - root - 2017-12-05 21:55:13.805807: step 48080, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.835 sec/batch; 65h:57m:02s remains)
INFO - root - 2017-12-05 21:55:22.380140: step 48090, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 68h:27m:41s remains)
INFO - root - 2017-12-05 21:55:30.938197: step 48100, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 66h:04m:06s remains)
2017-12-05 21:55:31.693921: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2181029 -4.2307725 -4.2432022 -4.2542973 -4.2587762 -4.2571993 -4.2513351 -4.2422309 -4.2400007 -4.2442532 -4.2472816 -4.2478504 -4.2485986 -4.2471209 -4.2436271][-4.2063365 -4.2064056 -4.2145681 -4.2252827 -4.2304053 -4.227293 -4.2234573 -4.2189088 -4.2237067 -4.2333684 -4.2393751 -4.2393317 -4.2371564 -4.2339363 -4.2337203][-4.177011 -4.1630068 -4.163991 -4.1732335 -4.17992 -4.1783504 -4.1839705 -4.1909513 -4.2040396 -4.2185664 -4.2273321 -4.2289286 -4.2276597 -4.2290854 -4.2337275][-4.143714 -4.122385 -4.1163931 -4.1222105 -4.133739 -4.1431336 -4.1630745 -4.185616 -4.2056108 -4.2220988 -4.2305789 -4.2305961 -4.2281032 -4.2313709 -4.2383313][-4.131237 -4.1132264 -4.1007376 -4.0985975 -4.106667 -4.1169534 -4.137692 -4.1653571 -4.1906176 -4.2123847 -4.2231379 -4.2247863 -4.2251787 -4.2314553 -4.2399292][-4.1415563 -4.1333084 -4.1157918 -4.1026258 -4.0927577 -4.0847726 -4.08988 -4.1115026 -4.1390972 -4.16638 -4.1863713 -4.19836 -4.21088 -4.2292528 -4.2425532][-4.1540065 -4.1541538 -4.1297441 -4.1006956 -4.066884 -4.0330777 -4.0231719 -4.0361433 -4.0626669 -4.09441 -4.1272988 -4.1586285 -4.1929245 -4.2323122 -4.2559977][-4.1430058 -4.138576 -4.1029024 -4.0565543 -4.0099287 -3.9706106 -3.9649353 -3.9797935 -4.0088854 -4.0510821 -4.0974541 -4.1445718 -4.1934304 -4.2448788 -4.2729354][-4.1132283 -4.09687 -4.0551119 -4.0094128 -3.9695048 -3.9511039 -3.9668639 -3.9924722 -4.0238485 -4.0693374 -4.114789 -4.159843 -4.2055216 -4.2500515 -4.2722497][-4.1053243 -4.0796442 -4.0389867 -4.0047636 -3.9829886 -3.9896898 -4.0179572 -4.0461226 -4.0721989 -4.1098957 -4.1456442 -4.1805453 -4.2142787 -4.2419896 -4.2546015][-4.1233826 -4.0888367 -4.049552 -4.0253334 -4.0196962 -4.0412869 -4.0726275 -4.0999155 -4.1183848 -4.1411304 -4.1674614 -4.1970291 -4.2174687 -4.2301035 -4.2357016][-4.1413383 -4.1075544 -4.0743179 -4.05713 -4.0555706 -4.0780039 -4.1098943 -4.134656 -4.1458373 -4.1573496 -4.1761932 -4.199162 -4.207479 -4.2129226 -4.2186432][-4.1493149 -4.1214833 -4.0970273 -4.0846639 -4.0858335 -4.1078014 -4.1356292 -4.1537523 -4.1591597 -4.1615977 -4.1670585 -4.1807122 -4.1877928 -4.1940918 -4.2037811][-4.1460586 -4.1189952 -4.1020536 -4.09373 -4.0952878 -4.1139965 -4.140274 -4.1569223 -4.1583896 -4.1532321 -4.1459947 -4.1516666 -4.161314 -4.1738062 -4.1866288][-4.1461325 -4.1152678 -4.0998044 -4.0917239 -4.089828 -4.1020641 -4.1241341 -4.1388741 -4.1427884 -4.138742 -4.1281042 -4.1272478 -4.1349373 -4.1534052 -4.1707735]]...]
INFO - root - 2017-12-05 21:55:40.243812: step 48110, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 66h:29m:02s remains)
INFO - root - 2017-12-05 21:55:48.839995: step 48120, loss = 2.02, batch loss = 1.96 (9.9 examples/sec; 0.805 sec/batch; 63h:33m:21s remains)
INFO - root - 2017-12-05 21:55:57.412948: step 48130, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 70h:32m:52s remains)
INFO - root - 2017-12-05 21:56:05.987584: step 48140, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 66h:48m:02s remains)
INFO - root - 2017-12-05 21:56:14.648715: step 48150, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 68h:00m:35s remains)
INFO - root - 2017-12-05 21:56:23.033807: step 48160, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 66h:56m:45s remains)
INFO - root - 2017-12-05 21:56:31.472732: step 48170, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 67h:20m:12s remains)
INFO - root - 2017-12-05 21:56:39.849727: step 48180, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 68h:42m:53s remains)
INFO - root - 2017-12-05 21:56:48.443917: step 48190, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 68h:58m:31s remains)
INFO - root - 2017-12-05 21:56:56.954653: step 48200, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 67h:45m:31s remains)
2017-12-05 21:56:57.708911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2496467 -4.23099 -4.23122 -4.2419548 -4.2566109 -4.2736025 -4.2796869 -4.2781234 -4.2718177 -4.2690687 -4.2605028 -4.2377734 -4.2180419 -4.2146015 -4.2270584][-4.2181525 -4.1928535 -4.1899266 -4.1997056 -4.2175937 -4.2396841 -4.2456264 -4.2403741 -4.2310977 -4.2296662 -4.2211475 -4.1913209 -4.1683521 -4.1745934 -4.1998014][-4.1856956 -4.15483 -4.1486688 -4.1530943 -4.1727395 -4.2030287 -4.2079358 -4.1917996 -4.1788445 -4.182128 -4.1772346 -4.141377 -4.1193366 -4.140173 -4.1803055][-4.1514225 -4.11601 -4.1089525 -4.1109757 -4.1338816 -4.1683936 -4.1664534 -4.1303468 -4.1108613 -4.1264553 -4.1287913 -4.0910587 -4.07197 -4.1034408 -4.1547108][-4.1308312 -4.1023507 -4.1018257 -4.10335 -4.1212473 -4.1468458 -4.1212673 -4.0475612 -4.0200624 -4.0568986 -4.0810761 -4.0606971 -4.0552192 -4.0928659 -4.1472878][-4.1315641 -4.1149611 -4.1164303 -4.1097546 -4.108706 -4.1083727 -4.0408483 -3.9125731 -3.8821383 -3.9644413 -4.026288 -4.0369658 -4.051846 -4.1003881 -4.1561217][-4.1382775 -4.1282959 -4.1232586 -4.1021843 -4.0790415 -4.052917 -3.9432728 -3.7624836 -3.7298021 -3.8601449 -3.9628642 -4.0063467 -4.0409179 -4.0988431 -4.1517963][-4.1519036 -4.1414533 -4.1293116 -4.1006866 -4.0743656 -4.0518703 -3.9591308 -3.8029451 -3.7637863 -3.8806479 -3.9806538 -4.0250411 -4.0537953 -4.101438 -4.1461549][-4.1674004 -4.1496129 -4.1337013 -4.1080608 -4.0914469 -4.0874205 -4.0414019 -3.9497232 -3.9155326 -3.987365 -4.0561128 -4.0826383 -4.092063 -4.1174083 -4.1534719][-4.1953111 -4.1709003 -4.153904 -4.1338558 -4.1238718 -4.129818 -4.116333 -4.0768037 -4.0593295 -4.0996656 -4.142827 -4.1558323 -4.1501102 -4.1555023 -4.178813][-4.235836 -4.2122784 -4.1943312 -4.1794209 -4.1753807 -4.1840734 -4.1892371 -4.1821923 -4.176445 -4.1933408 -4.2154021 -4.2203755 -4.20472 -4.2018108 -4.2170515][-4.2769103 -4.2584548 -4.239985 -4.2277956 -4.2263041 -4.2330637 -4.2430191 -4.2502313 -4.2538362 -4.2608538 -4.2691631 -4.2659574 -4.2463 -4.2420239 -4.2552295][-4.2980061 -4.2840724 -4.268856 -4.2607055 -4.2603016 -4.264842 -4.2740078 -4.2854686 -4.2924442 -4.2938724 -4.2956061 -4.2900352 -4.2711997 -4.2653236 -4.2792606][-4.3046508 -4.2917523 -4.2797308 -4.27612 -4.2781 -4.2821116 -4.290451 -4.2987733 -4.3034658 -4.3031511 -4.3037462 -4.2987809 -4.2832651 -4.2770324 -4.2900696][-4.3143435 -4.3041906 -4.2966652 -4.2958865 -4.2983465 -4.3012719 -4.3061714 -4.3095341 -4.3108578 -4.309876 -4.3098788 -4.3061833 -4.2972589 -4.294004 -4.3035731]]...]
INFO - root - 2017-12-05 21:57:06.237840: step 48210, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 65h:58m:18s remains)
INFO - root - 2017-12-05 21:57:14.827556: step 48220, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 66h:31m:06s remains)
INFO - root - 2017-12-05 21:57:23.230467: step 48230, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 67h:24m:10s remains)
INFO - root - 2017-12-05 21:57:31.801505: step 48240, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 66h:22m:41s remains)
INFO - root - 2017-12-05 21:57:40.331046: step 48250, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.811 sec/batch; 63h:59m:59s remains)
INFO - root - 2017-12-05 21:57:48.871163: step 48260, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 70h:08m:45s remains)
INFO - root - 2017-12-05 21:57:57.435731: step 48270, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 67h:58m:25s remains)
INFO - root - 2017-12-05 21:58:05.830494: step 48280, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 68h:27m:28s remains)
INFO - root - 2017-12-05 21:58:14.372805: step 48290, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 67h:15m:06s remains)
INFO - root - 2017-12-05 21:58:22.958111: step 48300, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 69h:17m:33s remains)
2017-12-05 21:58:23.813031: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1809578 -4.18326 -4.1713152 -4.1529517 -4.1482644 -4.1386871 -4.1308513 -4.1413 -4.1405149 -4.1505127 -4.1679125 -4.1854787 -4.2091074 -4.2260518 -4.2412748][-4.1863089 -4.1851811 -4.1755881 -4.1621971 -4.167788 -4.1635361 -4.1570449 -4.1612124 -4.1575565 -4.16678 -4.1691728 -4.1735873 -4.1957836 -4.2125821 -4.231132][-4.1767187 -4.1761265 -4.1706223 -4.1645684 -4.1744695 -4.1718483 -4.1673751 -4.1645904 -4.1554794 -4.1669679 -4.1608047 -4.1546364 -4.1736417 -4.1918378 -4.2188277][-4.1645904 -4.1658535 -4.1605148 -4.157443 -4.1668739 -4.1649928 -4.1621003 -4.1488633 -4.1331334 -4.1456923 -4.1380715 -4.128479 -4.1482153 -4.1735182 -4.2089682][-4.1523433 -4.1591086 -4.1611013 -4.1604252 -4.1666522 -4.1606021 -4.1516047 -4.1228738 -4.0978951 -4.1169887 -4.1202 -4.1164641 -4.1406841 -4.1724858 -4.2111225][-4.1343765 -4.1407666 -4.1436439 -4.1439309 -4.1487222 -4.1364484 -4.11869 -4.0708447 -4.0370593 -4.0725951 -4.0967841 -4.1059971 -4.1397343 -4.1774216 -4.2194242][-4.1165638 -4.110774 -4.1011033 -4.0932021 -4.088501 -4.0663085 -4.0333791 -3.9587727 -3.9099998 -3.9745867 -4.0300393 -4.0607843 -4.1082511 -4.1581645 -4.21154][-4.1239524 -4.10724 -4.08622 -4.0631852 -4.0411768 -4.0085254 -3.9631553 -3.8635502 -3.7966523 -3.8819048 -3.9639134 -4.0166068 -4.0737853 -4.1335459 -4.19823][-4.14964 -4.1326213 -4.1121407 -4.0858655 -4.057632 -4.0345521 -4.0087695 -3.9349771 -3.880795 -3.9397445 -4.0026822 -4.0485339 -4.0961094 -4.1487422 -4.2097631][-4.165132 -4.1550155 -4.1448555 -4.1244135 -4.09724 -4.088943 -4.0843563 -4.0422616 -4.0074191 -4.043509 -4.0857759 -4.1152225 -4.1477451 -4.1861382 -4.2341347][-4.15672 -4.1549392 -4.1534505 -4.1407857 -4.1191554 -4.1173887 -4.1242867 -4.101944 -4.0778823 -4.1003885 -4.12747 -4.14702 -4.1720724 -4.2052751 -4.2503586][-4.1284714 -4.135571 -4.1400561 -4.1377821 -4.1274071 -4.131669 -4.1455026 -4.1358328 -4.1158476 -4.12716 -4.1422486 -4.1537075 -4.175735 -4.2086482 -4.255949][-4.1044216 -4.114574 -4.1148047 -4.1153326 -4.1162753 -4.1282015 -4.1497612 -4.14829 -4.1287737 -4.133275 -4.1414819 -4.1513367 -4.1747313 -4.2102289 -4.26027][-4.1049027 -4.1103191 -4.1010442 -4.0966964 -4.1010485 -4.1141777 -4.1375103 -4.1360059 -4.1150293 -4.119679 -4.1307135 -4.1478162 -4.1772232 -4.214385 -4.2642727][-4.1261945 -4.1301632 -4.1185784 -4.1132426 -4.1169586 -4.1259933 -4.1411376 -4.1280608 -4.1014524 -4.1036749 -4.1150174 -4.1407866 -4.1767716 -4.2122493 -4.2611318]]...]
INFO - root - 2017-12-05 21:58:32.320051: step 48310, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 68h:05m:39s remains)
INFO - root - 2017-12-05 21:58:40.915473: step 48320, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 68h:03m:21s remains)
INFO - root - 2017-12-05 21:58:49.519317: step 48330, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 65h:41m:22s remains)
INFO - root - 2017-12-05 21:58:58.150846: step 48340, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 67h:49m:50s remains)
INFO - root - 2017-12-05 21:59:06.634257: step 48350, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 67h:30m:45s remains)
INFO - root - 2017-12-05 21:59:15.037835: step 48360, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 68h:53m:59s remains)
INFO - root - 2017-12-05 21:59:23.614486: step 48370, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 67h:42m:54s remains)
INFO - root - 2017-12-05 21:59:32.146728: step 48380, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 69h:31m:00s remains)
INFO - root - 2017-12-05 21:59:40.484373: step 48390, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 66h:50m:45s remains)
INFO - root - 2017-12-05 21:59:49.036833: step 48400, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 66h:00m:39s remains)
2017-12-05 21:59:49.795557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.170723 -4.1663017 -4.159554 -4.15466 -4.15227 -4.1543622 -4.160593 -4.1669087 -4.1669679 -4.1616054 -4.1566 -4.1546812 -4.1602964 -4.1667032 -4.1700616][-4.1545763 -4.1571875 -4.1561422 -4.1550856 -4.1571908 -4.1630259 -4.1705995 -4.17717 -4.1748466 -4.1661053 -4.157177 -4.147306 -4.1420293 -4.1429262 -4.1451025][-4.1389909 -4.1499114 -4.1557183 -4.1569209 -4.1591654 -4.1651578 -4.1712 -4.178278 -4.1759377 -4.1661897 -4.1509876 -4.1275673 -4.1077228 -4.1048136 -4.1103163][-4.1189618 -4.1281786 -4.1312919 -4.1284122 -4.1284795 -4.1339884 -4.1449323 -4.1610365 -4.1653423 -4.1603913 -4.1453724 -4.1166787 -4.0863271 -4.0774531 -4.0811481][-4.0953422 -4.0929146 -4.0837989 -4.0715666 -4.06568 -4.0699925 -4.0851946 -4.1118011 -4.1258421 -4.1306653 -4.1254091 -4.1054544 -4.0755172 -4.0598598 -4.0576181][-4.0675917 -4.0531235 -4.0300379 -4.0041766 -3.9833379 -3.9756856 -3.9872518 -4.0240054 -4.0567455 -4.08479 -4.100203 -4.0968909 -4.0727825 -4.0503435 -4.0469651][-4.0679326 -4.0476408 -4.0082488 -3.9619565 -3.9171762 -3.8867974 -3.882648 -3.9142857 -3.9605896 -4.0156913 -4.0578113 -4.0758247 -4.0657144 -4.0413609 -4.0337129][-4.0880737 -4.0674005 -4.01656 -3.9555984 -3.8957059 -3.8499784 -3.8295224 -3.8528142 -3.9017963 -3.9647613 -4.0199809 -4.0485387 -4.0495882 -4.0282121 -4.0142078][-4.1217813 -4.1060467 -4.057858 -3.9989471 -3.9413238 -3.8958035 -3.8759646 -3.8974595 -3.9365485 -3.980818 -4.0207238 -4.0422916 -4.0439024 -4.0247436 -4.0032406][-4.1648207 -4.1586475 -4.1194906 -4.0699949 -4.0220332 -3.9849031 -3.9705076 -3.9883516 -4.00986 -4.0278368 -4.0403457 -4.0451255 -4.0410848 -4.0209632 -3.9948516][-4.1945963 -4.1959095 -4.168786 -4.1340671 -4.1004686 -4.0719781 -4.0588984 -4.0699334 -4.07692 -4.0700507 -4.0571756 -4.0461197 -4.0361776 -4.0144296 -3.9950395][-4.208056 -4.2120519 -4.1977758 -4.1764 -4.15436 -4.1311388 -4.1164093 -4.1186533 -4.1163759 -4.0954514 -4.0729527 -4.0584054 -4.0472531 -4.0246277 -4.0127182][-4.1993 -4.2046928 -4.1992431 -4.1858921 -4.16881 -4.1479611 -4.1335664 -4.1313949 -4.1296673 -4.1148248 -4.1001973 -4.0920091 -4.0828624 -4.061749 -4.0514994][-4.1776705 -4.1802979 -4.1789594 -4.170783 -4.1571493 -4.1410947 -4.1327615 -4.1324968 -4.1362591 -4.1360435 -4.1340389 -4.132267 -4.1277895 -4.1123238 -4.0985975][-4.1448541 -4.1416616 -4.14199 -4.1414456 -4.1347084 -4.1240859 -4.1206126 -4.1243496 -4.1324205 -4.144886 -4.1540909 -4.15902 -4.1584148 -4.1503716 -4.1402106]]...]
INFO - root - 2017-12-05 21:59:58.275883: step 48410, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 67h:02m:51s remains)
INFO - root - 2017-12-05 22:00:06.811776: step 48420, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 66h:13m:43s remains)
INFO - root - 2017-12-05 22:00:15.465034: step 48430, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 70h:14m:10s remains)
INFO - root - 2017-12-05 22:00:24.058552: step 48440, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 68h:02m:02s remains)
INFO - root - 2017-12-05 22:00:32.727096: step 48450, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 66h:11m:52s remains)
INFO - root - 2017-12-05 22:00:41.159729: step 48460, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 69h:07m:32s remains)
INFO - root - 2017-12-05 22:00:49.734568: step 48470, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 67h:21m:35s remains)
INFO - root - 2017-12-05 22:00:58.273673: step 48480, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 67h:42m:56s remains)
INFO - root - 2017-12-05 22:01:06.804508: step 48490, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 67h:38m:28s remains)
INFO - root - 2017-12-05 22:01:15.112976: step 48500, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 67h:10m:22s remains)
2017-12-05 22:01:15.911677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0808129 -4.0928097 -4.0792832 -4.0737891 -4.082572 -4.0879889 -4.090662 -4.1003919 -4.1214252 -4.147244 -4.1633244 -4.1664052 -4.1511278 -4.1193247 -4.0876627][-4.0976281 -4.0984273 -4.0895467 -4.0923066 -4.1020856 -4.1076775 -4.1063094 -4.1120725 -4.1262164 -4.1461806 -4.1631269 -4.1664019 -4.150074 -4.1103554 -4.070334][-4.130281 -4.1211748 -4.1110806 -4.111382 -4.1081538 -4.1080389 -4.1012964 -4.1025038 -4.1143861 -4.1356182 -4.1581159 -4.1686621 -4.1580915 -4.117938 -4.0748849][-4.1680708 -4.1492772 -4.1268139 -4.1080341 -4.0842366 -4.0693297 -4.0533547 -4.0579944 -4.0782557 -4.108707 -4.1428237 -4.1694431 -4.1722455 -4.1419954 -4.1001825][-4.1896048 -4.1647577 -4.129519 -4.0833969 -4.0317788 -3.9889059 -3.9632583 -3.9824352 -4.0287056 -4.082706 -4.1297054 -4.1641073 -4.1819644 -4.1662016 -4.130796][-4.1988029 -4.1698089 -4.120429 -4.0403204 -3.9508553 -3.8733091 -3.8398237 -3.8782291 -3.958832 -4.0461073 -4.11443 -4.1623497 -4.1927295 -4.1882367 -4.1624031][-4.2070193 -4.1700764 -4.0954266 -3.9777308 -3.8450766 -3.7376099 -3.7091515 -3.7804022 -3.90079 -4.0167203 -4.0985808 -4.1578779 -4.2004342 -4.2065778 -4.1885934][-4.20722 -4.1672769 -4.0813389 -3.9534295 -3.8053536 -3.6831927 -3.6703784 -3.7807817 -3.9101315 -4.0191431 -4.0907874 -4.1510358 -4.1976967 -4.2108703 -4.1982336][-4.1983595 -4.1640377 -4.0920758 -3.9929919 -3.8629894 -3.7434225 -3.7366281 -3.8573124 -3.9674537 -4.0486026 -4.0980673 -4.152473 -4.1963482 -4.2081895 -4.1959562][-4.18095 -4.1618891 -4.1165476 -4.0553527 -3.9644403 -3.868371 -3.8547337 -3.9504364 -4.0294566 -4.07851 -4.1082525 -4.1521654 -4.1896119 -4.1963649 -4.1875291][-4.1803236 -4.1759892 -4.1562843 -4.1203847 -4.0626774 -3.9953322 -3.9811294 -4.0407109 -4.0785284 -4.0965786 -4.1123042 -4.14489 -4.1750069 -4.1794872 -4.1761484][-4.1776829 -4.1823797 -4.1744323 -4.1503029 -4.1119308 -4.0637889 -4.0473833 -4.0793095 -4.0997205 -4.10752 -4.1183195 -4.1379519 -4.1549354 -4.1582575 -4.1614][-4.1708393 -4.1856093 -4.1788363 -4.1576109 -4.1317196 -4.0972362 -4.0776176 -4.0926313 -4.1173368 -4.1352277 -4.1416426 -4.1409774 -4.1437788 -4.1493955 -4.1630869][-4.1478834 -4.1728015 -4.1715436 -4.1531963 -4.1376858 -4.1207433 -4.1040912 -4.1058426 -4.133738 -4.158771 -4.1613288 -4.1464596 -4.14239 -4.1483259 -4.1617203][-4.1055417 -4.139524 -4.1481962 -4.1426239 -4.1414237 -4.1400852 -4.1327367 -4.128993 -4.1452389 -4.165513 -4.1609869 -4.1366296 -4.1279969 -4.1381974 -4.1483307]]...]
INFO - root - 2017-12-05 22:01:24.580122: step 48510, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 69h:49m:57s remains)
INFO - root - 2017-12-05 22:01:33.111309: step 48520, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 67h:00m:02s remains)
INFO - root - 2017-12-05 22:01:41.730854: step 48530, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 67h:54m:56s remains)
INFO - root - 2017-12-05 22:01:50.186092: step 48540, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 66h:48m:17s remains)
INFO - root - 2017-12-05 22:01:58.756427: step 48550, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 67h:14m:01s remains)
INFO - root - 2017-12-05 22:02:07.251601: step 48560, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 69h:37m:32s remains)
INFO - root - 2017-12-05 22:02:15.772509: step 48570, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.892 sec/batch; 70h:21m:35s remains)
INFO - root - 2017-12-05 22:02:24.280026: step 48580, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 69h:15m:16s remains)
INFO - root - 2017-12-05 22:02:32.824244: step 48590, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 66h:04m:47s remains)
INFO - root - 2017-12-05 22:02:41.354481: step 48600, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 64h:46m:21s remains)
2017-12-05 22:02:42.139913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3551254 -4.3583374 -4.357162 -4.3570032 -4.3573613 -4.3581161 -4.360055 -4.3628821 -4.3631678 -4.3618417 -4.3607259 -4.3607697 -4.3618703 -4.3620658 -4.3630724][-4.3469043 -4.3464928 -4.3409457 -4.3391566 -4.3387523 -4.3401837 -4.3450584 -4.3522558 -4.3551846 -4.3551474 -4.3546696 -4.353344 -4.3524389 -4.3513684 -4.3526211][-4.3270626 -4.3200779 -4.3088856 -4.3035488 -4.3018041 -4.3057466 -4.3154144 -4.3288479 -4.33567 -4.3368936 -4.3362837 -4.3332477 -4.3298249 -4.3281536 -4.3323784][-4.2983708 -4.2838278 -4.2672877 -4.2591109 -4.2561278 -4.2596707 -4.27025 -4.2852297 -4.2929111 -4.2967119 -4.2980676 -4.2952294 -4.2901649 -4.2886534 -4.2976661][-4.2719932 -4.2527051 -4.2330194 -4.2243085 -4.2189765 -4.2169991 -4.2216892 -4.2292204 -4.2346058 -4.2401509 -4.2456894 -4.2440758 -4.2372465 -4.2367167 -4.2511067][-4.2420673 -4.2161736 -4.1918011 -4.1790533 -4.1706753 -4.1596241 -4.1518631 -4.1481223 -4.1499863 -4.1604881 -4.173027 -4.1761575 -4.1715183 -4.1750965 -4.1987424][-4.2014737 -4.1691947 -4.1447 -4.1285477 -4.1139803 -4.0923772 -4.0678883 -4.0469608 -4.04293 -4.0585842 -4.0822296 -4.0980797 -4.1047363 -4.1208081 -4.1585026][-4.1741982 -4.1460757 -4.1267395 -4.1086922 -4.0820689 -4.0432906 -3.9940829 -3.9512808 -3.9455817 -3.9725885 -4.0141091 -4.0490117 -4.0724258 -4.1048813 -4.1552715][-4.1814647 -4.1665535 -4.1529217 -4.130343 -4.0888906 -4.0329318 -3.9673245 -3.9206452 -3.9270091 -3.9692674 -4.0250978 -4.0698938 -4.1027575 -4.1405687 -4.1925993][-4.2117338 -4.2122908 -4.2037573 -4.1808119 -4.1393723 -4.081358 -4.0216994 -3.9917133 -4.0095849 -4.0521755 -4.1040449 -4.145431 -4.173203 -4.2026258 -4.2452331][-4.231658 -4.2439404 -4.2379932 -4.2158384 -4.185039 -4.1452446 -4.1078606 -4.0953684 -4.1185384 -4.154109 -4.1936259 -4.220396 -4.2341104 -4.24906 -4.2790079][-4.2391481 -4.2481456 -4.2403336 -4.2218676 -4.2043242 -4.1847539 -4.1632972 -4.1609159 -4.188344 -4.2215915 -4.2493167 -4.2571239 -4.252553 -4.2545538 -4.2766619][-4.230711 -4.2348366 -4.2193661 -4.2015796 -4.1925774 -4.1840658 -4.1707029 -4.1745906 -4.2061625 -4.23935 -4.2579536 -4.2534852 -4.235271 -4.2270966 -4.2468739][-4.2024589 -4.2065659 -4.1837788 -4.163156 -4.1592989 -4.1573744 -4.149313 -4.1647124 -4.202507 -4.2301712 -4.2406154 -4.2330089 -4.2092457 -4.1967254 -4.2168622][-4.1614137 -4.16018 -4.1319165 -4.1103554 -4.1076746 -4.1120329 -4.1151843 -4.1488152 -4.1966286 -4.2225146 -4.2304735 -4.2236791 -4.1977315 -4.1853476 -4.2073617]]...]
INFO - root - 2017-12-05 22:02:50.651834: step 48610, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 67h:14m:02s remains)
INFO - root - 2017-12-05 22:02:59.163446: step 48620, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 68h:26m:12s remains)
INFO - root - 2017-12-05 22:03:07.664324: step 48630, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 66h:01m:59s remains)
INFO - root - 2017-12-05 22:03:16.249107: step 48640, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.876 sec/batch; 69h:06m:07s remains)
INFO - root - 2017-12-05 22:03:24.731291: step 48650, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.828 sec/batch; 65h:16m:27s remains)
INFO - root - 2017-12-05 22:03:33.185615: step 48660, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 65h:27m:10s remains)
INFO - root - 2017-12-05 22:03:41.738775: step 48670, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 68h:29m:16s remains)
INFO - root - 2017-12-05 22:03:50.201659: step 48680, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.805 sec/batch; 63h:26m:50s remains)
INFO - root - 2017-12-05 22:03:58.738932: step 48690, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.841 sec/batch; 66h:20m:06s remains)
INFO - root - 2017-12-05 22:04:07.111955: step 48700, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 64h:52m:48s remains)
2017-12-05 22:04:07.935000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1501327 -4.1686072 -4.1938143 -4.2015262 -4.1846476 -4.1696672 -4.1771455 -4.1993823 -4.2119842 -4.2158942 -4.2187209 -4.2177916 -4.2135563 -4.2034173 -4.1883631][-4.1616311 -4.1752319 -4.2009168 -4.2085152 -4.1828461 -4.1569757 -4.1558552 -4.1759691 -4.1918783 -4.201766 -4.2090039 -4.209228 -4.2072487 -4.2004595 -4.1930976][-4.1509786 -4.1583552 -4.1816392 -4.1961155 -4.1782832 -4.1507797 -4.1409569 -4.1567559 -4.1753225 -4.1870427 -4.1910248 -4.1926527 -4.1960597 -4.1980076 -4.2019396][-4.1280165 -4.1316929 -4.153893 -4.1727476 -4.1671238 -4.1417127 -4.1262565 -4.1382866 -4.1617446 -4.17665 -4.1786709 -4.1828017 -4.1930885 -4.1988082 -4.2075739][-4.1140947 -4.1192837 -4.1365843 -4.150506 -4.14833 -4.1271548 -4.1123147 -4.1239805 -4.151525 -4.1692486 -4.1725063 -4.1804724 -4.1944275 -4.1990061 -4.2022853][-4.0973754 -4.1102238 -4.1271191 -4.1365843 -4.1340504 -4.1150293 -4.099164 -4.1090126 -4.1373773 -4.1566219 -4.1659307 -4.1777391 -4.1905041 -4.189661 -4.1859107][-4.0636768 -4.0973806 -4.1210103 -4.1312466 -4.1288753 -4.1080852 -4.0826869 -4.0758967 -4.0957365 -4.1221318 -4.147964 -4.1729512 -4.1853957 -4.1813769 -4.17698][-4.0360055 -4.0876946 -4.1188555 -4.1316257 -4.1273417 -4.10144 -4.06133 -4.0295763 -4.0387578 -4.0752282 -4.1231956 -4.1622958 -4.1769128 -4.171659 -4.1716714][-4.0394411 -4.0959806 -4.1287794 -4.1403341 -4.1355314 -4.1091251 -4.0626225 -4.0147624 -4.0144897 -4.0555305 -4.11369 -4.1560478 -4.1638432 -4.15401 -4.1554246][-4.0732837 -4.1236258 -4.1538482 -4.1645231 -4.1623564 -4.1404366 -4.0974817 -4.0477419 -4.0392547 -4.0769587 -4.1290665 -4.1637187 -4.1619692 -4.1470623 -4.1499228][-4.10863 -4.1507487 -4.1785221 -4.1903324 -4.1896663 -4.1702981 -4.1310616 -4.0855093 -4.0754981 -4.1096578 -4.1547961 -4.1808791 -4.1767058 -4.1652722 -4.1751161][-4.1346993 -4.1650467 -4.1853523 -4.1969953 -4.1996613 -4.1840463 -4.1503386 -4.1112266 -4.1023026 -4.133265 -4.1748943 -4.1982307 -4.1970558 -4.1921625 -4.2057362][-4.1631017 -4.1825409 -4.1975965 -4.2092886 -4.2133245 -4.2027721 -4.1767497 -4.1429453 -4.1319256 -4.1556325 -4.1917586 -4.21295 -4.211689 -4.2097092 -4.2209139][-4.1980252 -4.205719 -4.2148957 -4.2247171 -4.2298436 -4.2231941 -4.2053709 -4.179234 -4.1654191 -4.17898 -4.2041245 -4.2190762 -4.2154555 -4.2140555 -4.2184796][-4.2119746 -4.21206 -4.2162628 -4.2246337 -4.2316809 -4.229661 -4.2164283 -4.1920404 -4.1746826 -4.1785855 -4.1960192 -4.2102556 -4.2123575 -4.2126832 -4.2089128]]...]
INFO - root - 2017-12-05 22:04:16.369175: step 48710, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 69h:00m:18s remains)
INFO - root - 2017-12-05 22:04:24.787985: step 48720, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 66h:19m:57s remains)
INFO - root - 2017-12-05 22:04:33.260812: step 48730, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 68h:31m:24s remains)
INFO - root - 2017-12-05 22:04:41.789018: step 48740, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 66h:46m:23s remains)
INFO - root - 2017-12-05 22:04:50.247825: step 48750, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 66h:03m:43s remains)
INFO - root - 2017-12-05 22:04:58.628484: step 48760, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 66h:39m:25s remains)
INFO - root - 2017-12-05 22:05:07.095573: step 48770, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.825 sec/batch; 65h:03m:33s remains)
INFO - root - 2017-12-05 22:05:15.644826: step 48780, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 66h:36m:48s remains)
INFO - root - 2017-12-05 22:05:24.170013: step 48790, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 68h:45m:56s remains)
INFO - root - 2017-12-05 22:05:32.725837: step 48800, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 67h:11m:37s remains)
2017-12-05 22:05:33.505928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.308023 -4.3178515 -4.3205395 -4.3151951 -4.3092775 -4.2999163 -4.2802076 -4.2339592 -4.1570096 -4.0671749 -4.0229254 -4.0530138 -4.1254969 -4.2131982 -4.2842298][-4.2927332 -4.3016648 -4.3046427 -4.303175 -4.3006291 -4.2984614 -4.2933292 -4.2718835 -4.2313142 -4.1835322 -4.1578426 -4.1753807 -4.2212749 -4.2776556 -4.32071][-4.2835026 -4.2892113 -4.2885003 -4.2861633 -4.2835126 -4.2838039 -4.2857156 -4.280591 -4.2652879 -4.2456851 -4.2381372 -4.2564454 -4.2895141 -4.3240571 -4.344914][-4.2861137 -4.285562 -4.2775292 -4.2657118 -4.2552562 -4.2470803 -4.2408237 -4.2381344 -4.2400026 -4.2483864 -4.264317 -4.2951937 -4.3261404 -4.3507843 -4.3573747][-4.2933073 -4.2824783 -4.262434 -4.2365923 -4.2103529 -4.1816144 -4.1512136 -4.1404448 -4.1596475 -4.2029161 -4.24764 -4.2957191 -4.3344021 -4.3572178 -4.3591][-4.3004346 -4.2792749 -4.244637 -4.1992164 -4.1497974 -4.0858107 -4.015903 -3.9868727 -4.0244308 -4.1084304 -4.18996 -4.2638006 -4.3174677 -4.3454571 -4.3493648][-4.3030005 -4.2767248 -4.2329116 -4.170104 -4.0933046 -3.9862969 -3.8676562 -3.8135383 -3.8716342 -3.9983928 -4.1225739 -4.2260585 -4.2964497 -4.33044 -4.3360348][-4.2979388 -4.2733574 -4.229507 -4.1612883 -4.06544 -3.9328625 -3.7891912 -3.7228982 -3.797127 -3.9459977 -4.0950227 -4.213901 -4.2906141 -4.3239865 -4.3290253][-4.2932534 -4.2745781 -4.237782 -4.1768241 -4.0833025 -3.9574375 -3.8327289 -3.7843616 -3.8526425 -3.9855635 -4.1235785 -4.2333817 -4.3030748 -4.3309908 -4.3318276][-4.2936759 -4.284833 -4.2597876 -4.2129731 -4.1364474 -4.0344996 -3.9429293 -3.912313 -3.96369 -4.0660415 -4.1778531 -4.2659736 -4.3230124 -4.3418183 -4.3366523][-4.2981014 -4.2980614 -4.2847233 -4.2556062 -4.2007532 -4.1266456 -4.064611 -4.0457783 -4.0805287 -4.152627 -4.2330971 -4.2958755 -4.3354173 -4.3442035 -4.3357439][-4.307219 -4.3127384 -4.3074136 -4.2933564 -4.2606578 -4.2142434 -4.1768756 -4.1676497 -4.1893492 -4.2344356 -4.2841377 -4.3190022 -4.33797 -4.3372817 -4.3283162][-4.3170447 -4.3220038 -4.3210678 -4.3170567 -4.3037567 -4.2827616 -4.266098 -4.263289 -4.2754769 -4.2977734 -4.3209791 -4.3335648 -4.3353634 -4.3270931 -4.3190479][-4.3260431 -4.3260627 -4.3241611 -4.3242536 -4.3226728 -4.3193889 -4.3178744 -4.3203607 -4.3271809 -4.3347044 -4.3405819 -4.3396339 -4.3315606 -4.3197918 -4.312624][-4.3308592 -4.3271265 -4.3233585 -4.3236523 -4.3255706 -4.3287525 -4.332489 -4.3357258 -4.3390918 -4.3400264 -4.3395329 -4.3345203 -4.3252511 -4.3156114 -4.3107209]]...]
INFO - root - 2017-12-05 22:05:42.044677: step 48810, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 68h:01m:23s remains)
INFO - root - 2017-12-05 22:05:50.484330: step 48820, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 67h:21m:54s remains)
INFO - root - 2017-12-05 22:05:58.933440: step 48830, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 65h:27m:33s remains)
INFO - root - 2017-12-05 22:06:07.404327: step 48840, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 66h:37m:28s remains)
INFO - root - 2017-12-05 22:06:15.851473: step 48850, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 67h:26m:57s remains)
INFO - root - 2017-12-05 22:06:24.267110: step 48860, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 68h:30m:34s remains)
INFO - root - 2017-12-05 22:06:32.731280: step 48870, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 66h:47m:07s remains)
INFO - root - 2017-12-05 22:06:41.213779: step 48880, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 67h:51m:18s remains)
INFO - root - 2017-12-05 22:06:49.833005: step 48890, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 68h:30m:41s remains)
INFO - root - 2017-12-05 22:06:58.267106: step 48900, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 65h:28m:33s remains)
2017-12-05 22:06:59.023212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.169692 -4.1773472 -4.2172103 -4.2614045 -4.2809744 -4.28691 -4.28192 -4.2657819 -4.2381954 -4.2090411 -4.185554 -4.1640468 -4.1466665 -4.1621485 -4.192605][-4.1800852 -4.1897726 -4.2273636 -4.2684541 -4.2886968 -4.2939916 -4.2931566 -4.2868643 -4.2727256 -4.2580423 -4.244525 -4.2295537 -4.2167678 -4.2234983 -4.2334523][-4.1679373 -4.1818466 -4.2172422 -4.2542639 -4.273685 -4.2791805 -4.2830596 -4.2861772 -4.2873158 -4.2892361 -4.2884941 -4.283814 -4.2749276 -4.2708774 -4.2612138][-4.1684794 -4.1824961 -4.210556 -4.23502 -4.2466207 -4.24675 -4.2491636 -4.255156 -4.2667575 -4.2839994 -4.296926 -4.3018651 -4.2965288 -4.2871647 -4.2674789][-4.1966033 -4.2024426 -4.2138729 -4.2206459 -4.2161212 -4.2025595 -4.1924562 -4.18873 -4.1998615 -4.2283854 -4.2567706 -4.2758474 -4.2829728 -4.2823372 -4.2692246][-4.2221651 -4.2194505 -4.2135282 -4.202764 -4.1797848 -4.1452227 -4.1099844 -4.08408 -4.0890226 -4.1334381 -4.1854897 -4.2246742 -4.2506313 -4.2631626 -4.2586565][-4.2370124 -4.2290769 -4.208457 -4.1787186 -4.135745 -4.0777164 -4.0124 -3.9591935 -3.9576747 -4.02588 -4.1076875 -4.1696835 -4.2129354 -4.2331753 -4.2323208][-4.2511215 -4.2449622 -4.2161431 -4.1689148 -4.1085348 -4.0295591 -3.9341192 -3.8508294 -3.8414223 -3.9305258 -4.0401468 -4.1249504 -4.1857057 -4.2125936 -4.2149816][-4.2667174 -4.265348 -4.2348566 -4.1822782 -4.1193237 -4.0352812 -3.9253955 -3.8260417 -3.8122761 -3.9049368 -4.0218806 -4.1155767 -4.182755 -4.211381 -4.2115169][-4.285902 -4.2826905 -4.2523761 -4.2039022 -4.1497269 -4.0785832 -3.9820855 -3.9005506 -3.8946123 -3.9713101 -4.069128 -4.1502318 -4.2081156 -4.2316327 -4.2289467][-4.2974916 -4.2863398 -4.2551928 -4.2134666 -4.1725092 -4.122016 -4.055234 -4.0090389 -4.0181303 -4.0777531 -4.1485114 -4.2064829 -4.248898 -4.2671866 -4.2653694][-4.3025017 -4.2823439 -4.2504573 -4.2178226 -4.1937408 -4.1651955 -4.1284904 -4.1145053 -4.1326761 -4.17392 -4.2164536 -4.2483082 -4.2726259 -4.2852731 -4.2876358][-4.29315 -4.2640862 -4.2273126 -4.2000394 -4.192883 -4.1873322 -4.176939 -4.1862578 -4.2097697 -4.2369561 -4.2536354 -4.2598491 -4.2649317 -4.2726207 -4.2798944][-4.2739882 -4.2412171 -4.2014594 -4.1782932 -4.182682 -4.1939268 -4.2032351 -4.2270341 -4.2520165 -4.2667885 -4.2611442 -4.2437606 -4.2308903 -4.2336335 -4.2459388][-4.2579217 -4.233376 -4.2014909 -4.1843462 -4.1941113 -4.2107658 -4.2261281 -4.2501788 -4.2694774 -4.2732806 -4.25307 -4.2197809 -4.1953607 -4.1951 -4.20919]]...]
INFO - root - 2017-12-05 22:07:07.473526: step 48910, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 65h:32m:21s remains)
INFO - root - 2017-12-05 22:07:15.865473: step 48920, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.859 sec/batch; 67h:37m:57s remains)
INFO - root - 2017-12-05 22:07:24.294336: step 48930, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.807 sec/batch; 63h:33m:26s remains)
INFO - root - 2017-12-05 22:07:32.772118: step 48940, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 68h:29m:20s remains)
INFO - root - 2017-12-05 22:07:41.296677: step 48950, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 69h:47m:54s remains)
INFO - root - 2017-12-05 22:07:49.708804: step 48960, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 68h:11m:33s remains)
INFO - root - 2017-12-05 22:07:58.168360: step 48970, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 66h:17m:42s remains)
INFO - root - 2017-12-05 22:08:06.641457: step 48980, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 67h:27m:24s remains)
INFO - root - 2017-12-05 22:08:15.125310: step 48990, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.811 sec/batch; 63h:51m:20s remains)
INFO - root - 2017-12-05 22:08:23.551897: step 49000, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 67h:10m:57s remains)
2017-12-05 22:08:24.422896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2127833 -4.2073908 -4.1986604 -4.1810584 -4.1586494 -4.1496811 -4.1685171 -4.1916618 -4.2076097 -4.2121325 -4.2057877 -4.2043781 -4.2199574 -4.2322297 -4.2320757][-4.2135272 -4.2037692 -4.1951714 -4.1817188 -4.1585073 -4.1410456 -4.1496444 -4.1749454 -4.1992126 -4.2118468 -4.2117972 -4.2102737 -4.2232165 -4.2319231 -4.2301507][-4.2078171 -4.1975918 -4.1920428 -4.1857309 -4.1664891 -4.1403666 -4.1345534 -4.1608405 -4.1931009 -4.2116437 -4.2138014 -4.2081423 -4.2119236 -4.2141414 -4.2127213][-4.1992388 -4.187984 -4.1833611 -4.1870842 -4.1791654 -4.1505833 -4.12861 -4.149529 -4.1844597 -4.2038293 -4.20601 -4.2006941 -4.20039 -4.2025032 -4.2015052][-4.1855311 -4.1691895 -4.1631083 -4.1737237 -4.1743741 -4.1449246 -4.111784 -4.1256528 -4.1656146 -4.1926956 -4.204278 -4.2046995 -4.2036428 -4.2039261 -4.2040811][-4.1731167 -4.1532316 -4.1415434 -4.14899 -4.151813 -4.1218767 -4.0770187 -4.083652 -4.13641 -4.183116 -4.2084346 -4.2140145 -4.2080369 -4.2025018 -4.2025008][-4.174437 -4.1548667 -4.1392212 -4.1359539 -4.1301432 -4.088192 -4.0163713 -3.9977953 -4.0711217 -4.1498108 -4.1941671 -4.2097597 -4.2056236 -4.2015796 -4.2050676][-4.1877747 -4.176384 -4.1605115 -4.144053 -4.1203771 -4.0577264 -3.9517503 -3.8918049 -3.9882591 -4.1057868 -4.1775417 -4.2089634 -4.2148991 -4.219336 -4.2208266][-4.2046084 -4.2027135 -4.1879621 -4.16734 -4.1400986 -4.0794816 -3.9784255 -3.90843 -3.9954076 -4.111526 -4.18606 -4.2199159 -4.2303324 -4.2343903 -4.2248306][-4.2176747 -4.2234445 -4.21596 -4.19863 -4.1760716 -4.134995 -4.0674329 -4.0155916 -4.0636582 -4.1415339 -4.194818 -4.2194338 -4.231082 -4.2328997 -4.2185121][-4.2403221 -4.2464757 -4.2391305 -4.2171507 -4.19404 -4.1693373 -4.1354332 -4.1020079 -4.1209531 -4.1604004 -4.1961 -4.2177796 -4.2304649 -4.2385836 -4.2315822][-4.274878 -4.2728748 -4.2548018 -4.2223854 -4.1966162 -4.1849656 -4.1817889 -4.1697159 -4.1760349 -4.1899586 -4.2115006 -4.2317162 -4.2447329 -4.2555709 -4.252984][-4.303339 -4.2936788 -4.2705235 -4.2359443 -4.20959 -4.2093463 -4.2244744 -4.2301736 -4.2332673 -4.233973 -4.2440391 -4.2527308 -4.259315 -4.2689309 -4.2682824][-4.313982 -4.3043108 -4.2860193 -4.2562389 -4.2357979 -4.2409611 -4.2580113 -4.2695942 -4.2715278 -4.265554 -4.2666521 -4.2683024 -4.2703533 -4.2779207 -4.2813864][-4.3122392 -4.306181 -4.2931361 -4.2724795 -4.2608609 -4.2671003 -4.2793269 -4.287921 -4.2869625 -4.2786694 -4.2786236 -4.2802014 -4.2831235 -4.2905045 -4.2961664]]...]
INFO - root - 2017-12-05 22:08:32.910587: step 49010, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 67h:00m:17s remains)
INFO - root - 2017-12-05 22:08:41.358098: step 49020, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 67h:38m:09s remains)
INFO - root - 2017-12-05 22:08:49.804486: step 49030, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 68h:23m:25s remains)
INFO - root - 2017-12-05 22:08:58.319892: step 49040, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 67h:30m:17s remains)
INFO - root - 2017-12-05 22:09:06.816211: step 49050, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 67h:22m:12s remains)
INFO - root - 2017-12-05 22:09:15.273449: step 49060, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 68h:01m:17s remains)
INFO - root - 2017-12-05 22:09:23.743358: step 49070, loss = 2.03, batch loss = 1.98 (9.8 examples/sec; 0.815 sec/batch; 64h:11m:53s remains)
INFO - root - 2017-12-05 22:09:32.167707: step 49080, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 66h:33m:54s remains)
INFO - root - 2017-12-05 22:09:40.725063: step 49090, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 67h:37m:25s remains)
INFO - root - 2017-12-05 22:09:49.197533: step 49100, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 67h:06m:31s remains)
2017-12-05 22:09:49.981667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3146381 -4.2899542 -4.2425094 -4.1776528 -4.1051192 -4.0496049 -4.0428987 -4.06712 -4.0884957 -4.10854 -4.1191015 -4.1482859 -4.1809936 -4.2047849 -4.2299891][-4.3135 -4.2874656 -4.2353916 -4.1625404 -4.0786505 -4.0103784 -3.9986608 -4.0261683 -4.0561786 -4.0784988 -4.0897326 -4.1185188 -4.1499071 -4.1710935 -4.2005534][-4.3121777 -4.2841234 -4.2255206 -4.1427751 -4.04761 -3.9755161 -3.9648027 -3.9987378 -4.0357294 -4.0590229 -4.0654383 -4.0806823 -4.1004648 -4.1161728 -4.1490874][-4.3091574 -4.2773752 -4.2136765 -4.123158 -4.025526 -3.9659393 -3.9696853 -4.0099087 -4.0482807 -4.0645027 -4.0588484 -4.0451775 -4.034955 -4.0355539 -4.0805321][-4.3064876 -4.2694359 -4.20276 -4.1085663 -4.0122671 -3.9654837 -3.9859371 -4.0337706 -4.0693 -4.0789018 -4.0620389 -4.0182076 -3.9727592 -3.9582944 -4.0264473][-4.3077135 -4.266408 -4.1962848 -4.0894074 -3.9805658 -3.9342866 -3.9687331 -4.030911 -4.0709085 -4.0867624 -4.0700455 -4.0154729 -3.9552133 -3.9350877 -4.0173745][-4.3115697 -4.2664547 -4.1887751 -4.0628433 -3.9312143 -3.8781939 -3.9252555 -4.0011458 -4.051815 -4.0845952 -4.08134 -4.0363293 -3.9845648 -3.9682221 -4.0429139][-4.31622 -4.2714348 -4.1907263 -4.0581422 -3.91974 -3.8579907 -3.9006596 -3.9711635 -4.0221391 -4.0673537 -4.0856428 -4.0618315 -4.0291972 -4.0235796 -4.0866871][-4.3198371 -4.2785492 -4.2048125 -4.0901918 -3.9740241 -3.9134369 -3.9376667 -3.9871929 -4.0296197 -4.0778975 -4.1138086 -4.1135154 -4.1007605 -4.1054821 -4.1522646][-4.3183403 -4.2800541 -4.2134552 -4.1214614 -4.0382729 -3.9938838 -4.0093861 -4.0461187 -4.087893 -4.1359482 -4.1731811 -4.181046 -4.1807127 -4.1897526 -4.2179494][-4.3162847 -4.2819343 -4.2238874 -4.1499529 -4.0919375 -4.0644903 -4.0800419 -4.1120214 -4.1532679 -4.1962705 -4.2248449 -4.233717 -4.241879 -4.2505279 -4.2650046][-4.3133273 -4.2828007 -4.2312989 -4.169198 -4.1226048 -4.0988874 -4.1143627 -4.1496296 -4.1917934 -4.2301321 -4.2525454 -4.2611079 -4.2721558 -4.2808518 -4.2883658][-4.3089929 -4.2804041 -4.2350688 -4.1823158 -4.1387916 -4.1115589 -4.1264529 -4.1695619 -4.2123408 -4.2451596 -4.2654276 -4.2733502 -4.2816396 -4.2880497 -4.2925806][-4.3032737 -4.2739544 -4.2324691 -4.1839657 -4.137969 -4.1095791 -4.1289039 -4.1794229 -4.2222586 -4.2505126 -4.269074 -4.2749224 -4.2786512 -4.279387 -4.2791734][-4.3013148 -4.2718458 -4.2289114 -4.1807547 -4.1300311 -4.0963964 -4.1202159 -4.1764345 -4.218257 -4.2429748 -4.2595773 -4.2632051 -4.2657065 -4.2654634 -4.2639413]]...]
INFO - root - 2017-12-05 22:09:58.499960: step 49110, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 67h:34m:58s remains)
INFO - root - 2017-12-05 22:10:07.072710: step 49120, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 68h:06m:05s remains)
INFO - root - 2017-12-05 22:10:15.640350: step 49130, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 67h:44m:37s remains)
INFO - root - 2017-12-05 22:10:24.099843: step 49140, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 67h:08m:32s remains)
INFO - root - 2017-12-05 22:10:32.591576: step 49150, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 68h:35m:40s remains)
INFO - root - 2017-12-05 22:10:40.877137: step 49160, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.845 sec/batch; 66h:31m:30s remains)
INFO - root - 2017-12-05 22:10:49.407510: step 49170, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 64h:56m:11s remains)
INFO - root - 2017-12-05 22:10:57.856937: step 49180, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 67h:47m:30s remains)
INFO - root - 2017-12-05 22:11:06.358609: step 49190, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.865 sec/batch; 68h:05m:36s remains)
INFO - root - 2017-12-05 22:11:14.879913: step 49200, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.852 sec/batch; 67h:03m:33s remains)
2017-12-05 22:11:15.639449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2765841 -4.2776465 -4.2747941 -4.2668991 -4.2548623 -4.2446475 -4.2434421 -4.2524934 -4.2641993 -4.2751937 -4.2824488 -4.2830935 -4.2803292 -4.2769938 -4.2731047][-4.2950091 -4.2925715 -4.2845731 -4.2691231 -4.2491064 -4.2323794 -4.229353 -4.2436266 -4.26445 -4.2850461 -4.2998295 -4.3035593 -4.3020759 -4.2992086 -4.2953768][-4.296196 -4.2898197 -4.2745972 -4.248373 -4.2174845 -4.1926708 -4.1836004 -4.199502 -4.2301779 -4.2629395 -4.2858043 -4.2957115 -4.2986865 -4.2993565 -4.2992039][-4.2949476 -4.2847281 -4.2610588 -4.2229352 -4.1815724 -4.1493278 -4.1313248 -4.1441722 -4.1809392 -4.2212763 -4.249126 -4.2668772 -4.2810121 -4.2911859 -4.2992082][-4.2931056 -4.2807198 -4.2506275 -4.2030063 -4.1527562 -4.1118054 -4.0794597 -4.0792251 -4.1168861 -4.1626806 -4.1928997 -4.2196903 -4.2518492 -4.27754 -4.2967067][-4.2922816 -4.2798738 -4.2475863 -4.1943541 -4.1349592 -4.0815272 -4.0249643 -3.9974773 -4.0306234 -4.0873137 -4.1243315 -4.1620679 -4.2145762 -4.2584696 -4.2900858][-4.2923751 -4.2805781 -4.2489157 -4.1952648 -4.1294112 -4.0605874 -3.975517 -3.9086595 -3.9289556 -4.0032573 -4.0609159 -4.1167569 -4.1881008 -4.2464175 -4.286345][-4.2915931 -4.2797475 -4.2504025 -4.1997519 -4.1271296 -4.0402565 -3.9287729 -3.8215878 -3.8260922 -3.9281342 -4.020287 -4.0991459 -4.1853395 -4.2511783 -4.2913427][-4.2905149 -4.2786593 -4.2532072 -4.2090983 -4.1343989 -4.0402322 -3.9272485 -3.8180249 -3.8120942 -3.9219656 -4.0331149 -4.1236644 -4.2098551 -4.2711487 -4.30294][-4.291677 -4.2793293 -4.2567844 -4.2199254 -4.1553731 -4.074235 -3.9848237 -3.9049051 -3.8979685 -3.991703 -4.1000514 -4.186996 -4.2564735 -4.2997422 -4.3164191][-4.2970958 -4.2868247 -4.2675629 -4.2362428 -4.185267 -4.1232309 -4.0616007 -4.0106735 -4.0095758 -4.0853815 -4.1809254 -4.2558784 -4.3032384 -4.3241453 -4.3243594][-4.3005981 -4.2918038 -4.2752934 -4.2480707 -4.2098484 -4.1673827 -4.13533 -4.1150355 -4.1219597 -4.1783447 -4.2512941 -4.3054056 -4.3325768 -4.3354244 -4.3240418][-4.2974272 -4.2872014 -4.2716169 -4.248714 -4.22181 -4.2003741 -4.1931009 -4.1956034 -4.2083945 -4.2443352 -4.2929921 -4.3287296 -4.342164 -4.3348055 -4.3181953][-4.2877502 -4.2724395 -4.2523065 -4.230916 -4.2128229 -4.2086296 -4.2241912 -4.2434287 -4.2607927 -4.2824183 -4.3115363 -4.3335071 -4.3396349 -4.3290734 -4.3124695][-4.2768846 -4.2519569 -4.2235293 -4.1987991 -4.1798668 -4.18182 -4.2128963 -4.2453623 -4.2687745 -4.2861085 -4.3079176 -4.3251953 -4.3313894 -4.32339 -4.3087635]]...]
INFO - root - 2017-12-05 22:11:24.196458: step 49210, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 66h:05m:24s remains)
INFO - root - 2017-12-05 22:11:32.783719: step 49220, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.848 sec/batch; 66h:43m:50s remains)
INFO - root - 2017-12-05 22:11:41.163787: step 49230, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.829 sec/batch; 65h:15m:09s remains)
INFO - root - 2017-12-05 22:11:49.595171: step 49240, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 68h:54m:01s remains)
INFO - root - 2017-12-05 22:11:57.988921: step 49250, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.829 sec/batch; 65h:13m:26s remains)
INFO - root - 2017-12-05 22:12:06.444362: step 49260, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 65h:50m:27s remains)
INFO - root - 2017-12-05 22:12:14.877965: step 49270, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 65h:50m:07s remains)
INFO - root - 2017-12-05 22:12:23.372156: step 49280, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.893 sec/batch; 70h:13m:14s remains)
INFO - root - 2017-12-05 22:12:31.926139: step 49290, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.874 sec/batch; 68h:47m:20s remains)
INFO - root - 2017-12-05 22:12:40.481361: step 49300, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 66h:12m:22s remains)
2017-12-05 22:12:41.237544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2827983 -4.2722187 -4.2670345 -4.2584085 -4.24823 -4.2361531 -4.2306633 -4.2390637 -4.2406244 -4.2344923 -4.2365928 -4.2443566 -4.2466869 -4.2482276 -4.2525959][-4.2596211 -4.2430005 -4.2348461 -4.2198544 -4.201726 -4.1836338 -4.1778584 -4.1911116 -4.196353 -4.1937561 -4.2031264 -4.2145762 -4.2164063 -4.2181163 -4.2227492][-4.2467003 -4.2293968 -4.2198677 -4.1999311 -4.1762233 -4.1534686 -4.1446996 -4.1568666 -4.1659722 -4.1709414 -4.1864157 -4.198019 -4.1973557 -4.1991315 -4.2071166][-4.2474155 -4.2315831 -4.2193193 -4.1934457 -4.1615515 -4.1293583 -4.11286 -4.1220732 -4.1322775 -4.14472 -4.1683955 -4.1840086 -4.1860285 -4.1901283 -4.203373][-4.249898 -4.2290635 -4.2063847 -4.1676574 -4.1265492 -4.0876794 -4.064126 -4.0736365 -4.0879607 -4.10879 -4.1450834 -4.1699362 -4.1809645 -4.1908641 -4.210505][-4.2483468 -4.2217479 -4.1878409 -4.1344967 -4.0792065 -4.0271692 -3.9971385 -4.0147371 -4.044683 -4.0848641 -4.1323748 -4.1637425 -4.181736 -4.1980486 -4.22279][-4.2291107 -4.1943 -4.1506371 -4.084496 -4.0142107 -3.9552677 -3.9333372 -3.9731534 -4.0307546 -4.0867882 -4.135819 -4.16743 -4.188251 -4.2088051 -4.2373724][-4.1994982 -4.1571198 -4.1124215 -4.0476446 -3.9791083 -3.9347286 -3.9376454 -3.9975429 -4.0640254 -4.1160631 -4.1527047 -4.1773319 -4.1983042 -4.2221236 -4.2498336][-4.189579 -4.1507797 -4.1136727 -4.062521 -4.012044 -3.9912238 -4.0148382 -4.076088 -4.1309028 -4.1663952 -4.1868973 -4.2027512 -4.2215424 -4.2442603 -4.2686434][-4.2048965 -4.1699724 -4.1395121 -4.1078973 -4.0811148 -4.0808868 -4.1143909 -4.1667233 -4.2029481 -4.2214427 -4.2320237 -4.2415257 -4.2535038 -4.2691073 -4.2874103][-4.2342939 -4.20435 -4.1791735 -4.1635389 -4.1541748 -4.16566 -4.1998587 -4.2383018 -4.2589149 -4.2665887 -4.2715821 -4.2751007 -4.2788877 -4.287755 -4.3025217][-4.2637472 -4.2401724 -4.2182689 -4.2130418 -4.2154884 -4.2302709 -4.2562737 -4.279829 -4.2895627 -4.2896147 -4.2907014 -4.2908487 -4.2918382 -4.29863 -4.3112173][-4.2775154 -4.2578659 -4.2416477 -4.2412682 -4.2485704 -4.2623405 -4.2799134 -4.2921009 -4.296936 -4.2959242 -4.2961836 -4.2962632 -4.2972374 -4.3032413 -4.313446][-4.2812581 -4.2641358 -4.2528515 -4.253552 -4.2611885 -4.27366 -4.2850862 -4.2902551 -4.2931576 -4.293611 -4.2946892 -4.2952614 -4.2958903 -4.3014011 -4.3116732][-4.2886195 -4.2757568 -4.2672896 -4.2659235 -4.2711358 -4.2808132 -4.2883248 -4.2910571 -4.2930946 -4.29383 -4.2957067 -4.2959142 -4.2966919 -4.30133 -4.30942]]...]
INFO - root - 2017-12-05 22:12:49.793591: step 49310, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 67h:17m:17s remains)
INFO - root - 2017-12-05 22:12:58.351821: step 49320, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 69h:44m:09s remains)
INFO - root - 2017-12-05 22:13:06.817994: step 49330, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 67h:44m:38s remains)
INFO - root - 2017-12-05 22:13:15.389391: step 49340, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 66h:53m:45s remains)
INFO - root - 2017-12-05 22:13:23.880364: step 49350, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 67h:30m:38s remains)
INFO - root - 2017-12-05 22:13:32.326907: step 49360, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 65h:58m:35s remains)
INFO - root - 2017-12-05 22:13:40.762065: step 49370, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 68h:04m:27s remains)
INFO - root - 2017-12-05 22:13:49.190230: step 49380, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 66h:45m:26s remains)
INFO - root - 2017-12-05 22:13:57.753721: step 49390, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.878 sec/batch; 69h:01m:15s remains)
INFO - root - 2017-12-05 22:14:06.290448: step 49400, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 66h:00m:01s remains)
2017-12-05 22:14:07.088205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3236809 -4.3183904 -4.3153591 -4.317595 -4.3216949 -4.3243141 -4.3245115 -4.3226986 -4.3241897 -4.3260803 -4.3269062 -4.3279767 -4.3294368 -4.3322191 -4.3338838][-4.3082337 -4.3018603 -4.2987094 -4.3022118 -4.3068357 -4.3104291 -4.3111653 -4.3080826 -4.3100333 -4.313293 -4.31441 -4.3141508 -4.3149843 -4.31905 -4.3226762][-4.2909794 -4.2806821 -4.273438 -4.274538 -4.2775331 -4.2812753 -4.28229 -4.2763824 -4.2757421 -4.2794323 -4.2830009 -4.2853866 -4.2890568 -4.2968268 -4.3052888][-4.28089 -4.2653637 -4.250875 -4.2457476 -4.2433929 -4.2443867 -4.2451696 -4.2376533 -4.2362146 -4.24178 -4.2448716 -4.2494974 -4.2581711 -4.27358 -4.2884955][-4.2710519 -4.2476225 -4.2245479 -4.2121997 -4.2063532 -4.2049718 -4.2047639 -4.1944456 -4.1938739 -4.2034025 -4.2064772 -4.2146921 -4.2295952 -4.2548442 -4.2800484][-4.253212 -4.2230692 -4.1945949 -4.1733236 -4.1576047 -4.1464009 -4.1375489 -4.1217079 -4.1232171 -4.1431441 -4.1542797 -4.1729107 -4.1972251 -4.2331233 -4.2669673][-4.2353711 -4.1971407 -4.1587677 -4.1200275 -4.0830989 -4.0494432 -4.0239615 -4.0028496 -4.0095778 -4.0455494 -4.0728307 -4.1087403 -4.1490288 -4.1974449 -4.2385449][-4.2265854 -4.1812491 -4.129467 -4.0697041 -4.0084438 -3.9508162 -3.9066017 -3.8843579 -3.9044871 -3.9628034 -4.0116944 -4.0634594 -4.1178737 -4.1768289 -4.2212873][-4.2312431 -4.1843381 -4.1298075 -4.0624633 -3.9961252 -3.944068 -3.9107766 -3.8986738 -3.9274955 -3.9849665 -4.02974 -4.0752258 -4.1275592 -4.1855021 -4.2265978][-4.2379556 -4.1906304 -4.1381884 -4.0749459 -4.0192232 -3.99098 -3.9861443 -3.9913836 -4.0186062 -4.0563436 -4.0814881 -4.1120467 -4.1528654 -4.2025609 -4.238337][-4.2391315 -4.1924467 -4.1455579 -4.0980039 -4.0618196 -4.055367 -4.0711408 -4.0857987 -4.104929 -4.1227641 -4.1309619 -4.1490831 -4.1793113 -4.2203522 -4.2538342][-4.238925 -4.1984611 -4.1638465 -4.13828 -4.1237216 -4.1299238 -4.1524429 -4.1709743 -4.1863441 -4.1944985 -4.1963172 -4.2077489 -4.2292786 -4.257566 -4.2822528][-4.249692 -4.2220459 -4.2036076 -4.1974564 -4.1976871 -4.2111411 -4.2322741 -4.2469678 -4.2579579 -4.2630439 -4.2637334 -4.2701616 -4.2830458 -4.2995214 -4.3133578][-4.2735906 -4.2591906 -4.2523861 -4.2550478 -4.2625737 -4.27582 -4.2899652 -4.2981715 -4.3044124 -4.3075843 -4.3081408 -4.3095365 -4.3134975 -4.3212638 -4.3280053][-4.2966 -4.2878504 -4.2855234 -4.2895188 -4.2977996 -4.3069081 -4.3148746 -4.3180885 -4.3212981 -4.3242636 -4.3257394 -4.3258672 -4.3268061 -4.3309708 -4.3344483]]...]
INFO - root - 2017-12-05 22:14:15.770626: step 49410, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 66h:23m:06s remains)
INFO - root - 2017-12-05 22:14:24.278228: step 49420, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 66h:50m:23s remains)
INFO - root - 2017-12-05 22:14:32.831846: step 49430, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 67h:46m:47s remains)
INFO - root - 2017-12-05 22:14:41.338802: step 49440, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 66h:03m:48s remains)
INFO - root - 2017-12-05 22:14:49.872296: step 49450, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 65h:50m:10s remains)
INFO - root - 2017-12-05 22:14:58.234247: step 49460, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 69h:02m:38s remains)
INFO - root - 2017-12-05 22:15:06.862828: step 49470, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 67h:24m:20s remains)
INFO - root - 2017-12-05 22:15:15.354720: step 49480, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 67h:00m:11s remains)
INFO - root - 2017-12-05 22:15:23.852229: step 49490, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 66h:20m:23s remains)
INFO - root - 2017-12-05 22:15:32.352723: step 49500, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 68h:32m:01s remains)
2017-12-05 22:15:33.115489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9737442 -3.9158323 -3.8842862 -3.8914249 -3.973897 -4.08571 -4.1814303 -4.2499423 -4.2892036 -4.3101459 -4.3206749 -4.3273134 -4.3299036 -4.3281946 -4.3260441][-4.0742249 -4.027957 -4.00148 -4.0019913 -4.0544171 -4.1303411 -4.1946654 -4.2441711 -4.278327 -4.2996092 -4.3132296 -4.324029 -4.32957 -4.3283772 -4.3248692][-4.1832261 -4.146759 -4.121511 -4.1122923 -4.134439 -4.1734681 -4.2054186 -4.2331824 -4.2609792 -4.284656 -4.305037 -4.3203058 -4.3289332 -4.32866 -4.324698][-4.2727895 -4.2464709 -4.22012 -4.1983523 -4.1925712 -4.197546 -4.1983409 -4.2072716 -4.230648 -4.2581868 -4.2861376 -4.3095646 -4.3234987 -4.3254147 -4.3219719][-4.3190241 -4.2983146 -4.2727857 -4.2429638 -4.2179027 -4.1937928 -4.1654029 -4.1562171 -4.1746287 -4.2066588 -4.2446918 -4.2794857 -4.3041668 -4.3134956 -4.31432][-4.328619 -4.3100243 -4.2803369 -4.2424498 -4.2042403 -4.1574731 -4.1052566 -4.0769315 -4.0868273 -4.1223311 -4.176342 -4.2317977 -4.2753024 -4.2978945 -4.3068662][-4.319478 -4.305768 -4.2749286 -4.2258768 -4.1640105 -4.0897574 -4.0123186 -3.9604731 -3.9584317 -4.0007229 -4.0836887 -4.1710796 -4.2395997 -4.2784686 -4.2994][-4.3081927 -4.3031049 -4.2733903 -4.2173829 -4.1352186 -4.0322752 -3.9316788 -3.8604558 -3.8396156 -3.8868752 -4.0010195 -4.117445 -4.2070637 -4.2638297 -4.2950683][-4.2976665 -4.2988615 -4.2776446 -4.2317562 -4.1566873 -4.0545969 -3.9567089 -3.8870258 -3.8539548 -3.8878615 -3.9965961 -4.1076827 -4.1921778 -4.2525487 -4.2875566][-4.2844305 -4.2940488 -4.2846971 -4.2603908 -4.2116971 -4.1351871 -4.0592012 -4.0021024 -3.9707704 -3.9859116 -4.0565376 -4.1297827 -4.186861 -4.2348585 -4.2696095][-4.26546 -4.2868619 -4.2965927 -4.2953315 -4.2736797 -4.2261014 -4.1739993 -4.1251669 -4.0917506 -4.0886168 -4.1225324 -4.1555877 -4.1804934 -4.2117209 -4.24275][-4.234333 -4.2687016 -4.2977333 -4.3191223 -4.3223109 -4.2986064 -4.2636614 -4.2219958 -4.1848683 -4.1625118 -4.1632762 -4.1586637 -4.1550503 -4.1731229 -4.2033525][-4.1749711 -4.2163591 -4.2619176 -4.30416 -4.3302522 -4.3294373 -4.3085337 -4.2735348 -4.235395 -4.1987238 -4.1729927 -4.1356664 -4.10748 -4.1141124 -4.14084][-4.0716786 -4.1137114 -4.1773424 -4.2473197 -4.3003926 -4.3241463 -4.3164611 -4.2910719 -4.2506075 -4.1994457 -4.1486683 -4.0805779 -4.023911 -4.012917 -4.0318675][-3.935832 -3.9759102 -4.0639076 -4.168231 -4.2533073 -4.3020124 -4.3100595 -4.2955956 -4.2573338 -4.1931243 -4.1127243 -4.0100832 -3.9234796 -3.8862393 -3.8910446]]...]
INFO - root - 2017-12-05 22:15:41.674402: step 49510, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 67h:03m:50s remains)
INFO - root - 2017-12-05 22:15:50.373361: step 49520, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 68h:15m:24s remains)
INFO - root - 2017-12-05 22:15:58.954996: step 49530, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 67h:41m:04s remains)
INFO - root - 2017-12-05 22:16:07.448306: step 49540, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 67h:52m:30s remains)
INFO - root - 2017-12-05 22:16:15.912746: step 49550, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 66h:23m:38s remains)
INFO - root - 2017-12-05 22:16:24.172503: step 49560, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 65h:37m:24s remains)
INFO - root - 2017-12-05 22:16:32.695367: step 49570, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 65h:41m:15s remains)
INFO - root - 2017-12-05 22:16:41.320249: step 49580, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 66h:53m:48s remains)
INFO - root - 2017-12-05 22:16:49.903293: step 49590, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.821 sec/batch; 64h:32m:45s remains)
INFO - root - 2017-12-05 22:16:58.397895: step 49600, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 68h:21m:55s remains)
2017-12-05 22:16:59.217941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2224331 -4.225883 -4.2290258 -4.24219 -4.2524238 -4.2473087 -4.2251115 -4.2006512 -4.1881509 -4.168673 -4.146687 -4.1513448 -4.1627879 -4.1705513 -4.1894073][-4.2006474 -4.2038622 -4.2161622 -4.2326269 -4.2455711 -4.2377787 -4.2083406 -4.1784725 -4.1617236 -4.1430817 -4.1281867 -4.1302609 -4.1403356 -4.1554189 -4.1824632][-4.1580558 -4.1642885 -4.18854 -4.20541 -4.2194929 -4.2097187 -4.1760721 -4.1438103 -4.1308708 -4.1222606 -4.1174231 -4.11657 -4.1271558 -4.1490059 -4.1818452][-4.1236553 -4.137249 -4.1655374 -4.1787405 -4.190958 -4.1756549 -4.1347237 -4.0979428 -4.0878429 -4.0946541 -4.1101856 -4.1171579 -4.13214 -4.1572742 -4.1873236][-4.1044126 -4.1264958 -4.152741 -4.1547422 -4.1588106 -4.127212 -4.0704632 -4.0238643 -4.0171213 -4.0456138 -4.0883765 -4.1140394 -4.1380906 -4.165153 -4.1898165][-4.106709 -4.1319332 -4.1535234 -4.1344452 -4.1176705 -4.0624342 -3.9768555 -3.9086289 -3.9089854 -3.9706519 -4.0469952 -4.0926771 -4.1250157 -4.1555409 -4.1798859][-4.1456723 -4.1670275 -4.1741729 -4.130065 -4.0875874 -4.0041523 -3.8851271 -3.7906723 -3.7971528 -3.8966107 -4.00606 -4.0680776 -4.1040659 -4.1363788 -4.1613903][-4.1985703 -4.2097163 -4.1972537 -4.1369443 -4.076324 -3.9822869 -3.8571091 -3.7666969 -3.7894905 -3.9029553 -4.0164371 -4.0738649 -4.1031537 -4.1331806 -4.1522541][-4.2353826 -4.2340021 -4.2078576 -4.1421 -4.0779524 -3.99183 -3.8948321 -3.8412797 -3.8819051 -3.9790769 -4.0662136 -4.105576 -4.1246085 -4.1489596 -4.1623507][-4.243289 -4.23522 -4.2059526 -4.1405034 -4.0796151 -4.0079317 -3.9505358 -3.9413486 -3.9914498 -4.0655746 -4.1238775 -4.1469631 -4.1584363 -4.1771321 -4.1901817][-4.2323694 -4.2200646 -4.199585 -4.1400509 -4.0762439 -4.0127745 -3.9855182 -4.011261 -4.0687389 -4.124898 -4.1642852 -4.1798973 -4.1893744 -4.2069979 -4.2219429][-4.2231 -4.2106209 -4.1929955 -4.1386204 -4.0704923 -4.0076027 -3.9967957 -4.0447736 -4.1112051 -4.1606035 -4.1900969 -4.2027092 -4.2132006 -4.2301526 -4.2464666][-4.2178717 -4.2023993 -4.1777673 -4.1199865 -4.0472322 -3.9860477 -3.9907167 -4.055119 -4.130971 -4.1807156 -4.2059774 -4.2166924 -4.2256584 -4.2405291 -4.2559571][-4.2172074 -4.1960559 -4.1614518 -4.0965056 -4.0251117 -3.9760184 -3.9938428 -4.0662518 -4.1429338 -4.1917577 -4.2135148 -4.2216139 -4.2284517 -4.2409115 -4.2540011][-4.2189374 -4.1960096 -4.156105 -4.0877185 -4.02502 -3.9957523 -4.0293202 -4.0989623 -4.1641469 -4.2071638 -4.2234564 -4.2293811 -4.235323 -4.2458787 -4.2537017]]...]
INFO - root - 2017-12-05 22:17:07.673374: step 49610, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 66h:19m:28s remains)
INFO - root - 2017-12-05 22:17:16.206413: step 49620, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 68h:08m:41s remains)
INFO - root - 2017-12-05 22:17:24.748557: step 49630, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.876 sec/batch; 68h:49m:48s remains)
INFO - root - 2017-12-05 22:17:33.230604: step 49640, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 67h:31m:06s remains)
INFO - root - 2017-12-05 22:17:41.997147: step 49650, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 67h:10m:31s remains)
INFO - root - 2017-12-05 22:17:50.321554: step 49660, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 66h:06m:57s remains)
INFO - root - 2017-12-05 22:17:58.618237: step 49670, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 66h:27m:24s remains)
INFO - root - 2017-12-05 22:18:07.100244: step 49680, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 69h:36m:08s remains)
INFO - root - 2017-12-05 22:18:15.463669: step 49690, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 65h:37m:57s remains)
INFO - root - 2017-12-05 22:18:23.833366: step 49700, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 67h:41m:46s remains)
2017-12-05 22:18:24.585310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1588354 -4.1886806 -4.209372 -4.2083712 -4.2129335 -4.2241192 -4.2303934 -4.23142 -4.2288857 -4.2268724 -4.2268629 -4.2200975 -4.2014155 -4.1817255 -4.1612906][-4.1668954 -4.1896529 -4.1993275 -4.1898904 -4.1889791 -4.1947947 -4.1978221 -4.1980796 -4.1957808 -4.198813 -4.2055993 -4.2014909 -4.1866388 -4.1731215 -4.1577744][-4.1763778 -4.1942568 -4.1984429 -4.1856065 -4.1766014 -4.1708441 -4.163784 -4.1593041 -4.1581345 -4.1690917 -4.1811085 -4.1791787 -4.1698494 -4.1664977 -4.1607523][-4.1785169 -4.1931319 -4.1962032 -4.1831932 -4.1710963 -4.1590896 -4.141161 -4.1224728 -4.1196413 -4.1414323 -4.1627483 -4.1669745 -4.1668038 -4.1693969 -4.1648912][-4.1720314 -4.1863327 -4.1905885 -4.176331 -4.1583753 -4.1370749 -4.104599 -4.0693989 -4.06895 -4.1107478 -4.151269 -4.1667657 -4.1691036 -4.1701803 -4.1625266][-4.1734095 -4.1898007 -4.19322 -4.1723213 -4.1410227 -4.0978837 -4.0360489 -3.973284 -3.9772289 -4.0490832 -4.1169395 -4.1546288 -4.1677065 -4.1708503 -4.1642613][-4.1772776 -4.2021585 -4.2107453 -4.1852474 -4.1321816 -4.0521097 -3.9459336 -3.8399987 -3.8525856 -3.9674423 -4.0770073 -4.1476192 -4.1810431 -4.1892905 -4.1827927][-4.1788249 -4.2143755 -4.2317338 -4.2166667 -4.1637383 -4.0721722 -3.9514394 -3.824146 -3.8355725 -3.9579654 -4.0775328 -4.1570048 -4.1969385 -4.2066031 -4.2006812][-4.1760015 -4.2207971 -4.2454066 -4.2437158 -4.2081251 -4.1405697 -4.0542955 -3.9664528 -3.9650979 -4.0407991 -4.12047 -4.1755714 -4.2010422 -4.2055845 -4.2030282][-4.1759548 -4.2180591 -4.2434454 -4.2507319 -4.2328877 -4.1925688 -4.1440077 -4.1012115 -4.0992222 -4.1367822 -4.1749291 -4.2003136 -4.2065611 -4.2027764 -4.2018089][-4.1781235 -4.2081232 -4.2273574 -4.2374125 -4.2340317 -4.2167969 -4.1951547 -4.1792831 -4.182827 -4.2031279 -4.2161765 -4.2213788 -4.2150393 -4.2080932 -4.207324][-4.1808577 -4.1973982 -4.2112217 -4.2211585 -4.2251959 -4.2251358 -4.2233582 -4.2217174 -4.2273431 -4.24025 -4.24517 -4.2425237 -4.2291303 -4.2159328 -4.2117395][-4.1913381 -4.191782 -4.1938252 -4.1995573 -4.2063074 -4.21616 -4.23101 -4.2416782 -4.2505288 -4.2622137 -4.2657819 -4.25995 -4.2421727 -4.2210851 -4.2091651][-4.2107496 -4.1997342 -4.1912503 -4.1893115 -4.1897936 -4.1978178 -4.2167959 -4.2350049 -4.2490082 -4.2607093 -4.265161 -4.2590318 -4.2394166 -4.2141123 -4.2000732][-4.2407703 -4.22888 -4.2210603 -4.2157159 -4.2090545 -4.2091093 -4.2237744 -4.2407932 -4.2527237 -4.2618661 -4.2673693 -4.2643886 -4.2501345 -4.227757 -4.2128015]]...]
INFO - root - 2017-12-05 22:18:33.008497: step 49710, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 66h:37m:07s remains)
INFO - root - 2017-12-05 22:18:41.543592: step 49720, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.874 sec/batch; 68h:41m:30s remains)
INFO - root - 2017-12-05 22:18:50.160788: step 49730, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 67h:47m:09s remains)
INFO - root - 2017-12-05 22:18:58.686193: step 49740, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 67h:55m:02s remains)
INFO - root - 2017-12-05 22:19:07.249109: step 49750, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 66h:25m:54s remains)
INFO - root - 2017-12-05 22:19:15.639521: step 49760, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 66h:24m:00s remains)
INFO - root - 2017-12-05 22:19:24.148199: step 49770, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 67h:23m:10s remains)
INFO - root - 2017-12-05 22:19:32.448034: step 49780, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.823 sec/batch; 64h:36m:13s remains)
INFO - root - 2017-12-05 22:19:40.908872: step 49790, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 67h:19m:01s remains)
INFO - root - 2017-12-05 22:19:49.398812: step 49800, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 64h:46m:18s remains)
2017-12-05 22:19:50.248971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2789187 -4.2865009 -4.2928762 -4.2981262 -4.3008718 -4.3016338 -4.2964463 -4.2846932 -4.275105 -4.2703815 -4.2730494 -4.2795916 -4.2869754 -4.29132 -4.2918673][-4.2956681 -4.3032956 -4.3077331 -4.3096514 -4.3061352 -4.2981443 -4.2844515 -4.2679248 -4.2570696 -4.2537308 -4.2609715 -4.275959 -4.2929125 -4.3059163 -4.3106942][-4.3074327 -4.3152676 -4.3168917 -4.3124385 -4.2969089 -4.2731485 -4.246347 -4.2224255 -4.2062392 -4.2003841 -4.2129722 -4.2410479 -4.2699723 -4.2935047 -4.3047562][-4.2987409 -4.3067861 -4.3072948 -4.2983465 -4.2734442 -4.2374716 -4.198935 -4.1639686 -4.1367154 -4.123044 -4.137444 -4.1759071 -4.215066 -4.2471142 -4.2650852][-4.276031 -4.2838092 -4.284472 -4.273356 -4.244916 -4.2044926 -4.1583166 -4.11202 -4.0722971 -4.0504947 -4.0653253 -4.1099639 -4.1539059 -4.1896048 -4.2128558][-4.2498326 -4.2577271 -4.2593217 -4.24894 -4.2224383 -4.1834216 -4.1344709 -4.0842628 -4.0419416 -4.0205059 -4.0366993 -4.077065 -4.1159935 -4.1460705 -4.1652665][-4.2389088 -4.2465978 -4.250474 -4.24363 -4.2231188 -4.1899605 -4.1450872 -4.0999188 -4.0640736 -4.0501313 -4.0660753 -4.094491 -4.1201019 -4.1363144 -4.1415825][-4.2491288 -4.254581 -4.2589474 -4.2567048 -4.2450128 -4.2226439 -4.1905918 -4.1583228 -4.1337366 -4.1273689 -4.1397233 -4.1552429 -4.1670876 -4.1691551 -4.1596379][-4.2661147 -4.2684917 -4.2716756 -4.2725549 -4.2685714 -4.2572513 -4.2392192 -4.2203236 -4.2065849 -4.2056828 -4.2145896 -4.2213893 -4.2237549 -4.2161179 -4.1986403][-4.2859459 -4.28631 -4.28762 -4.2894745 -4.2886562 -4.2826843 -4.2725511 -4.2612562 -4.2547579 -4.2579637 -4.2665777 -4.2712431 -4.2692819 -4.2578559 -4.2395973][-4.3047128 -4.3044462 -4.3040142 -4.3046908 -4.3032994 -4.2976437 -4.2888923 -4.2791076 -4.2746263 -4.2792263 -4.2887588 -4.2944546 -4.2926955 -4.283916 -4.2716479][-4.3141012 -4.313961 -4.3131475 -4.3136549 -4.3124003 -4.3066635 -4.2979717 -4.2880287 -4.2828317 -4.2874131 -4.2977104 -4.3044124 -4.3038259 -4.298419 -4.2921638][-4.3183932 -4.319252 -4.3193412 -4.320806 -4.3210244 -4.3154206 -4.3056865 -4.2944245 -4.2876077 -4.2907796 -4.3001018 -4.3064051 -4.3064919 -4.3036594 -4.3022194][-4.3236752 -4.3258767 -4.3272648 -4.3298845 -4.3317118 -4.3271904 -4.3172059 -4.30552 -4.2976766 -4.2983036 -4.3039961 -4.3078384 -4.3072333 -4.3057919 -4.3072681][-4.331193 -4.332931 -4.3344097 -4.3377957 -4.3414125 -4.3389387 -4.3301759 -4.3191524 -4.3106627 -4.3077807 -4.3078189 -4.3075852 -4.3054333 -4.3045263 -4.307529]]...]
INFO - root - 2017-12-05 22:19:58.636602: step 49810, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.840 sec/batch; 65h:59m:34s remains)
INFO - root - 2017-12-05 22:20:07.091029: step 49820, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.816 sec/batch; 64h:02m:42s remains)
INFO - root - 2017-12-05 22:20:15.619219: step 49830, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 68h:50m:35s remains)
INFO - root - 2017-12-05 22:20:24.240414: step 49840, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 68h:00m:28s remains)
INFO - root - 2017-12-05 22:20:32.841126: step 49850, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 66h:41m:19s remains)
INFO - root - 2017-12-05 22:20:41.319916: step 49860, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 72h:36m:19s remains)
INFO - root - 2017-12-05 22:20:49.891302: step 49870, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 66h:29m:17s remains)
INFO - root - 2017-12-05 22:20:58.218646: step 49880, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 65h:14m:06s remains)
INFO - root - 2017-12-05 22:21:06.671761: step 49890, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.863 sec/batch; 67h:46m:12s remains)
INFO - root - 2017-12-05 22:21:15.229540: step 49900, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 68h:18m:55s remains)
2017-12-05 22:21:16.018141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1649241 -4.1614494 -4.170013 -4.1834965 -4.1953349 -4.1942511 -4.1762605 -4.1428509 -4.1280332 -4.1268539 -4.1194005 -4.1114211 -4.1073551 -4.1156011 -4.1406589][-4.1685448 -4.1609697 -4.16351 -4.1741657 -4.18554 -4.1839828 -4.1626925 -4.1176405 -4.0954189 -4.0980372 -4.1012774 -4.1034985 -4.10247 -4.1071582 -4.1252995][-4.1651225 -4.1536536 -4.1507263 -4.1577787 -4.1687479 -4.1724906 -4.1603122 -4.1158924 -4.0907373 -4.0951824 -4.1072426 -4.1168036 -4.114861 -4.109427 -4.1135635][-4.1569309 -4.1423597 -4.1342516 -4.13871 -4.1523094 -4.1660857 -4.1675968 -4.1339512 -4.11209 -4.1172152 -4.1353021 -4.1466966 -4.1397796 -4.1257443 -4.1145658][-4.1467538 -4.1269073 -4.1088161 -4.1077108 -4.1233225 -4.1433077 -4.1552682 -4.1365137 -4.1257558 -4.1380968 -4.1655498 -4.178196 -4.1690044 -4.1527047 -4.1326389][-4.1436615 -4.1149755 -4.0804005 -4.0682015 -4.0811367 -4.098495 -4.1127357 -4.1065426 -4.1104484 -4.135457 -4.173521 -4.1916342 -4.1908584 -4.18324 -4.1660604][-4.1621342 -4.1266804 -4.0791616 -4.0522795 -4.0510459 -4.0546012 -4.0622854 -4.0569806 -4.060194 -4.0855055 -4.1256852 -4.1560369 -4.1729417 -4.1811781 -4.1760807][-4.1885567 -4.1550269 -4.1057611 -4.0676742 -4.0444069 -4.0292764 -4.0230627 -4.0046697 -3.9859271 -3.992475 -4.0282345 -4.0740771 -4.1143613 -4.1466966 -4.1607361][-4.2008367 -4.1728077 -4.1284766 -4.0851693 -4.0489864 -4.0251174 -4.0120845 -3.9848297 -3.948751 -3.935967 -3.962569 -4.0157309 -4.0728126 -4.1228 -4.1525183][-4.1974335 -4.173142 -4.1363544 -4.095499 -4.0608606 -4.0441494 -4.0437422 -4.0322075 -4.0046034 -3.986649 -4.0005279 -4.039876 -4.0898085 -4.1376615 -4.1686592][-4.183074 -4.1591482 -4.1287742 -4.0922441 -4.0651727 -4.0617647 -4.07868 -4.0932927 -4.0885439 -4.0807285 -4.0871153 -4.1042471 -4.1309571 -4.1630383 -4.1834197][-4.1721582 -4.1448369 -4.1155448 -4.0802231 -4.0576963 -4.0628672 -4.0902553 -4.1213913 -4.131619 -4.1349669 -4.1389165 -4.1398096 -4.1458144 -4.1613603 -4.1700377][-4.1624122 -4.1309857 -4.0985961 -4.0602846 -4.0395379 -4.0488286 -4.0798926 -4.1113443 -4.1217256 -4.1292706 -4.1318092 -4.1233015 -4.1175194 -4.122818 -4.1230588][-4.1547008 -4.1224761 -4.0883408 -4.047451 -4.0298052 -4.0424066 -4.0717807 -4.0943632 -4.0965405 -4.1008496 -4.1031656 -4.0946774 -4.0879836 -4.0936093 -4.0918059][-4.1566882 -4.1280746 -4.0977697 -4.0609927 -4.0504255 -4.0708952 -4.1059332 -4.1267796 -4.1264024 -4.1268759 -4.1271367 -4.120306 -4.1150513 -4.1224871 -4.1187396]]...]
INFO - root - 2017-12-05 22:21:24.535243: step 49910, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 66h:45m:10s remains)
INFO - root - 2017-12-05 22:21:33.039063: step 49920, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 65h:09m:38s remains)
INFO - root - 2017-12-05 22:21:41.406503: step 49930, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 66h:02m:59s remains)
INFO - root - 2017-12-05 22:21:49.873455: step 49940, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 66h:35m:14s remains)
INFO - root - 2017-12-05 22:21:58.324238: step 49950, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 66h:02m:13s remains)
INFO - root - 2017-12-05 22:22:06.705819: step 49960, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 67h:43m:04s remains)
INFO - root - 2017-12-05 22:22:15.162570: step 49970, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 67h:17m:18s remains)
INFO - root - 2017-12-05 22:22:23.774071: step 49980, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 67h:41m:29s remains)
INFO - root - 2017-12-05 22:22:32.206809: step 49990, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 67h:45m:25s remains)
INFO - root - 2017-12-05 22:22:40.755010: step 50000, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 65h:24m:43s remains)
2017-12-05 22:22:41.562602: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3372455 -4.3266768 -4.3126216 -4.2977791 -4.282835 -4.2672729 -4.2553248 -4.2553477 -4.2649026 -4.2739978 -4.280313 -4.2927284 -4.3032584 -4.3071833 -4.3124022][-4.3307829 -4.31715 -4.2990966 -4.2806234 -4.2623096 -4.2395492 -4.2184453 -4.2135077 -4.2267017 -4.2383394 -4.2442884 -4.2566938 -4.2689285 -4.2723312 -4.2780437][-4.3252354 -4.308322 -4.2838869 -4.2590337 -4.2346458 -4.2031403 -4.1701956 -4.1531119 -4.1667037 -4.1820993 -4.1941366 -4.2109423 -4.2275844 -4.2344604 -4.2444453][-4.3204932 -4.3016949 -4.2730145 -4.2433662 -4.2117748 -4.1710467 -4.1222644 -4.08901 -4.0986333 -4.1185551 -4.1372623 -4.1610537 -4.1853328 -4.2020493 -4.2185955][-4.3132887 -4.2924552 -4.2580566 -4.2174892 -4.1757836 -4.1242576 -4.0624251 -4.0207806 -4.0278692 -4.0524921 -4.0763631 -4.1113219 -4.151526 -4.1834197 -4.206552][-4.3038778 -4.2779512 -4.2297096 -4.1725435 -4.1190987 -4.0595288 -3.9920585 -3.9429855 -3.9467645 -3.988595 -4.0251346 -4.0788527 -4.1415658 -4.1847644 -4.2089782][-4.2954903 -4.2595816 -4.1950626 -4.1244369 -4.0577049 -3.9858744 -3.9005046 -3.8208547 -3.8233416 -3.9083564 -3.9888449 -4.0734754 -4.1475325 -4.1894794 -4.2126231][-4.2874427 -4.2372885 -4.1523972 -4.06069 -3.9678328 -3.8581569 -3.7280986 -3.5971 -3.6168668 -3.7881198 -3.9397831 -4.0543127 -4.1300879 -4.1681385 -4.1943512][-4.2795219 -4.2227588 -4.1307306 -4.0228348 -3.9046662 -3.7637219 -3.6107302 -3.4572723 -3.5052536 -3.7255096 -3.895345 -4.0060287 -4.0750093 -4.1191568 -4.1599941][-4.2794642 -4.2258782 -4.1416564 -4.0367541 -3.9281435 -3.8189232 -3.72982 -3.6549115 -3.7006214 -3.8282967 -3.9214451 -3.9899354 -4.0447536 -4.0977335 -4.1501222][-4.2883596 -4.2419095 -4.1676021 -4.0749469 -3.9917264 -3.9280562 -3.8949451 -3.8717892 -3.8919246 -3.9416404 -3.9764338 -4.0170746 -4.062397 -4.11554 -4.1673312][-4.3058133 -4.2681513 -4.2085938 -4.141119 -4.0850005 -4.0483551 -4.03061 -4.0209389 -4.0228934 -4.0452981 -4.0646076 -4.0913091 -4.1234474 -4.16309 -4.1998873][-4.3228674 -4.2966924 -4.2556229 -4.2130332 -4.1766834 -4.1524048 -4.1394024 -4.1339636 -4.1377277 -4.1574063 -4.1732707 -4.1861248 -4.2015195 -4.21446 -4.2297735][-4.3389163 -4.3236766 -4.2970982 -4.2684712 -4.2452965 -4.2317867 -4.2276764 -4.2297115 -4.2350683 -4.2498169 -4.2623491 -4.2668247 -4.2671866 -4.2631826 -4.26603][-4.3514624 -4.3446765 -4.3303151 -4.3136716 -4.3012514 -4.2955556 -4.2988839 -4.3050661 -4.3105226 -4.3173361 -4.3191872 -4.31683 -4.3107667 -4.3035955 -4.3044038]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 22:22:50.745683: step 50010, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 65h:19m:06s remains)
INFO - root - 2017-12-05 22:22:59.194218: step 50020, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 65h:21m:13s remains)
INFO - root - 2017-12-05 22:23:07.675037: step 50030, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 65h:58m:45s remains)
INFO - root - 2017-12-05 22:23:16.250216: step 50040, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 70h:44m:37s remains)
INFO - root - 2017-12-05 22:23:24.799775: step 50050, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 69h:21m:02s remains)
INFO - root - 2017-12-05 22:23:33.185042: step 50060, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 67h:15m:21s remains)
INFO - root - 2017-12-05 22:23:41.700097: step 50070, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 65h:24m:10s remains)
INFO - root - 2017-12-05 22:23:50.122781: step 50080, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 65h:21m:15s remains)
INFO - root - 2017-12-05 22:23:58.642742: step 50090, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 0.791 sec/batch; 62h:03m:19s remains)
INFO - root - 2017-12-05 22:24:07.170451: step 50100, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 69h:16m:17s remains)
2017-12-05 22:24:07.948153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1283507 -4.1393046 -4.1771393 -4.2107739 -4.2138538 -4.1691103 -4.1054358 -4.0661278 -4.0724077 -4.1102552 -4.1450987 -4.1676784 -4.1693168 -4.1643891 -4.1665664][-4.130651 -4.1554308 -4.1918588 -4.2169108 -4.21425 -4.1679082 -4.11171 -4.0829964 -4.092968 -4.1206436 -4.1418338 -4.1605983 -4.1640019 -4.1669359 -4.1758347][-4.1286964 -4.1629682 -4.1964564 -4.2105007 -4.1978393 -4.1529474 -4.1063581 -4.0878129 -4.1009469 -4.1222229 -4.1363096 -4.1538749 -4.1588521 -4.1630363 -4.1712103][-4.1318569 -4.169209 -4.1994309 -4.2076144 -4.1844096 -4.1351504 -4.0953641 -4.0828724 -4.1013713 -4.1226959 -4.1375232 -4.1573954 -4.1681118 -4.173202 -4.1789317][-4.1338387 -4.1708159 -4.1967912 -4.2006993 -4.1749082 -4.1269183 -4.08944 -4.0799851 -4.0998297 -4.1174593 -4.1303577 -4.1561885 -4.1735582 -4.1843762 -4.1925354][-4.1162682 -4.1514754 -4.1798048 -4.1843991 -4.1615505 -4.1210032 -4.0853748 -4.083199 -4.1099434 -4.1280808 -4.14342 -4.1705613 -4.1883554 -4.2012625 -4.2101111][-4.0882158 -4.1188579 -4.1541824 -4.1615367 -4.14184 -4.1031594 -4.0663915 -4.0713925 -4.1089516 -4.13743 -4.1621013 -4.1930122 -4.2083812 -4.2160659 -4.2218184][-4.0514622 -4.0815229 -4.1262283 -4.1385145 -4.1203136 -4.0767365 -4.0315247 -4.0395393 -4.0907125 -4.1330018 -4.1667023 -4.199985 -4.2143006 -4.2182741 -4.222425][-4.0269628 -4.058702 -4.1099486 -4.1228561 -4.1028118 -4.056941 -4.0114784 -4.0290427 -4.0939379 -4.1468296 -4.1862311 -4.2160826 -4.227509 -4.2269454 -4.2210307][-4.0598087 -4.0864635 -4.12434 -4.1340308 -4.1130524 -4.0683651 -4.0292759 -4.0505695 -4.1163564 -4.1733294 -4.2140656 -4.2384114 -4.2491436 -4.2469573 -4.2356124][-4.1147532 -4.1394472 -4.1590433 -4.1703644 -4.1600103 -4.1250563 -4.095222 -4.1127191 -4.1642723 -4.2116 -4.2471447 -4.2681408 -4.2760119 -4.2718353 -4.2609][-4.1609325 -4.1837306 -4.1901264 -4.2067642 -4.2110543 -4.1942568 -4.1700263 -4.1773729 -4.2086639 -4.2378983 -4.2654371 -4.2857428 -4.2929535 -4.2855983 -4.2746816][-4.211585 -4.2292395 -4.2229061 -4.2332778 -4.2454591 -4.2397494 -4.2183323 -4.2185287 -4.2368808 -4.2486625 -4.2673731 -4.2903218 -4.3024755 -4.2980561 -4.2907405][-4.2426844 -4.2549262 -4.2398973 -4.238194 -4.2523036 -4.2532916 -4.2338166 -4.2296929 -4.2414656 -4.2459912 -4.2618504 -4.2863474 -4.3025193 -4.3051891 -4.3049817][-4.2457871 -4.2511187 -4.2308078 -4.2239027 -4.2411609 -4.2456713 -4.2312908 -4.2260962 -4.2343364 -4.2385888 -4.2554007 -4.2776566 -4.2924943 -4.3007727 -4.3061419]]...]
INFO - root - 2017-12-05 22:24:16.455443: step 50110, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 66h:50m:38s remains)
INFO - root - 2017-12-05 22:24:25.129989: step 50120, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 67h:29m:57s remains)
INFO - root - 2017-12-05 22:24:33.720940: step 50130, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 67h:26m:56s remains)
INFO - root - 2017-12-05 22:24:42.340274: step 50140, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 67h:18m:57s remains)
INFO - root - 2017-12-05 22:24:50.913241: step 50150, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 68h:57m:13s remains)
INFO - root - 2017-12-05 22:24:59.505478: step 50160, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 67h:20m:51s remains)
INFO - root - 2017-12-05 22:25:07.955496: step 50170, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 66h:09m:05s remains)
INFO - root - 2017-12-05 22:25:16.540829: step 50180, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 67h:44m:16s remains)
INFO - root - 2017-12-05 22:25:25.035332: step 50190, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 64h:55m:51s remains)
INFO - root - 2017-12-05 22:25:33.463125: step 50200, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 69h:44m:15s remains)
2017-12-05 22:25:34.286347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3107109 -4.3159995 -4.326057 -4.3333297 -4.3351994 -4.3355756 -4.3356028 -4.3345442 -4.3325644 -4.3299379 -4.3230982 -4.3162885 -4.311861 -4.3064618 -4.3012781][-4.2760983 -4.2825527 -4.3023539 -4.3184152 -4.3220043 -4.3214254 -4.3205028 -4.3184967 -4.3147645 -4.30942 -4.2976904 -4.284657 -4.2758765 -4.26998 -4.2671561][-4.2337108 -4.2348971 -4.2619929 -4.2853031 -4.2891269 -4.2865677 -4.2838 -4.278904 -4.274847 -4.271677 -4.2610574 -4.2459912 -4.2386646 -4.2376752 -4.2399092][-4.1936307 -4.1857696 -4.2132139 -4.238399 -4.2432685 -4.237978 -4.2281604 -4.2168055 -4.2162166 -4.2222881 -4.2152061 -4.2029781 -4.2002592 -4.2036872 -4.2099962][-4.151535 -4.1365848 -4.1626954 -4.1867671 -4.1875973 -4.171113 -4.1412773 -4.1220708 -4.1359692 -4.1609311 -4.1641073 -4.1586003 -4.1606317 -4.1649389 -4.1770606][-4.1153216 -4.0917439 -4.1087542 -4.1237988 -4.1157265 -4.0766792 -4.0132937 -3.9869432 -4.0329819 -4.0868559 -4.1057997 -4.1128383 -4.1231947 -4.138999 -4.1653509][-4.0936375 -4.0582247 -4.060699 -4.0590925 -4.0321965 -3.9601862 -3.8528028 -3.8245935 -3.9148715 -4.0057359 -4.044241 -4.06852 -4.0939312 -4.1277676 -4.16966][-4.080627 -4.0382667 -4.0303082 -4.0214915 -3.9811573 -3.8788466 -3.7410843 -3.7238591 -3.8535247 -3.9653549 -4.0167418 -4.0555568 -4.0958591 -4.1414986 -4.1862593][-4.0857859 -4.0515847 -4.0558839 -4.0614123 -4.0307817 -3.9401488 -3.8400602 -3.846684 -3.9485116 -4.0252476 -4.0599723 -4.0945053 -4.1353035 -4.1788697 -4.2150097][-4.1141524 -4.0912423 -4.1052918 -4.1213212 -4.1066341 -4.0485315 -3.9979653 -4.0147681 -4.0719156 -4.1086993 -4.1214485 -4.1415763 -4.1769052 -4.2174554 -4.2485814][-4.142323 -4.1266327 -4.1464605 -4.1698117 -4.1703534 -4.14095 -4.1203732 -4.1352835 -4.1676025 -4.1835051 -4.1805239 -4.1865373 -4.2108727 -4.2428746 -4.2671857][-4.1692729 -4.1586061 -4.1827126 -4.2104139 -4.2147908 -4.1975822 -4.1919079 -4.2050328 -4.225316 -4.2292047 -4.2174907 -4.217051 -4.2343163 -4.2585979 -4.2746258][-4.1982965 -4.1912065 -4.2169619 -4.2422194 -4.2430243 -4.2306075 -4.230649 -4.2450075 -4.2587261 -4.2528591 -4.2353954 -4.2357764 -4.2519703 -4.2706947 -4.2826862][-4.2321005 -4.2274189 -4.2495785 -4.2674241 -4.2649326 -4.2557464 -4.2580781 -4.2707095 -4.2767615 -4.2656732 -4.2494173 -4.254097 -4.2711706 -4.2859783 -4.2953763][-4.2543211 -4.2502966 -4.2654276 -4.2761593 -4.273623 -4.2681355 -4.2711506 -4.2815952 -4.2848263 -4.2753906 -4.2633605 -4.2681041 -4.2812047 -4.2942147 -4.3046508]]...]
INFO - root - 2017-12-05 22:25:42.759342: step 50210, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 66h:17m:24s remains)
INFO - root - 2017-12-05 22:25:51.291096: step 50220, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 68h:16m:03s remains)
INFO - root - 2017-12-05 22:25:59.807479: step 50230, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.828 sec/batch; 64h:55m:53s remains)
INFO - root - 2017-12-05 22:26:08.461389: step 50240, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 65h:38m:52s remains)
INFO - root - 2017-12-05 22:26:17.093536: step 50250, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 67h:32m:49s remains)
INFO - root - 2017-12-05 22:26:25.654891: step 50260, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 68h:44m:35s remains)
INFO - root - 2017-12-05 22:26:34.234812: step 50270, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 66h:32m:11s remains)
INFO - root - 2017-12-05 22:26:42.853170: step 50280, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.885 sec/batch; 69h:24m:00s remains)
INFO - root - 2017-12-05 22:26:51.382802: step 50290, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 67h:40m:01s remains)
INFO - root - 2017-12-05 22:26:59.919968: step 50300, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 67h:54m:32s remains)
2017-12-05 22:27:00.706839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2882404 -4.2668204 -4.2466006 -4.2323575 -4.2346048 -4.2593212 -4.2858133 -4.2970934 -4.2992816 -4.2952075 -4.2814569 -4.2686543 -4.259697 -4.2496786 -4.2457395][-4.2921371 -4.2802196 -4.2693477 -4.2587028 -4.25873 -4.2777176 -4.2959805 -4.2997541 -4.2980938 -4.2961493 -4.2911692 -4.2907848 -4.2894592 -4.2810769 -4.2757149][-4.2949839 -4.2915897 -4.2903109 -4.285912 -4.2843437 -4.2936425 -4.2979746 -4.2871628 -4.2785339 -4.2731414 -4.2704353 -4.2775445 -4.277226 -4.2655525 -4.2541213][-4.2920823 -4.2917538 -4.29471 -4.2950873 -4.2929587 -4.2926121 -4.2811422 -4.253345 -4.2323866 -4.223567 -4.2237854 -4.23466 -4.2332506 -4.2177281 -4.1997423][-4.2820253 -4.2800879 -4.2814312 -4.2803822 -4.2777648 -4.2707057 -4.2415714 -4.1959782 -4.1644187 -4.153511 -4.1609259 -4.1785364 -4.1793723 -4.1617346 -4.1373787][-4.2691388 -4.2650738 -4.2599468 -4.2540283 -4.2504449 -4.2389917 -4.1997328 -4.1453195 -4.109715 -4.1000338 -4.1156797 -4.1392694 -4.1432209 -4.1264563 -4.0973258][-4.2593651 -4.2520986 -4.2385612 -4.2275529 -4.2253118 -4.21562 -4.1780748 -4.1242042 -4.0920267 -4.0884485 -4.1095967 -4.1346946 -4.1403713 -4.1278224 -4.098948][-4.2484689 -4.2371707 -4.2190833 -4.2077041 -4.2093744 -4.2044535 -4.1732235 -4.1273408 -4.1011176 -4.10184 -4.1241751 -4.1487989 -4.1566262 -4.1543536 -4.134726][-4.2380867 -4.2220173 -4.2040586 -4.1976948 -4.2042341 -4.2045622 -4.1825466 -4.1459584 -4.1263752 -4.1315217 -4.1513247 -4.1699991 -4.1777072 -4.183507 -4.1761651][-4.2259169 -4.2080178 -4.1956592 -4.1966429 -4.2058153 -4.2117352 -4.1994123 -4.1756716 -4.1640887 -4.1685491 -4.1832719 -4.1991944 -4.2101355 -4.2179437 -4.2138696][-4.226625 -4.2121096 -4.2045593 -4.2083664 -4.2167206 -4.2238684 -4.2176948 -4.2045116 -4.1989107 -4.2019691 -4.210464 -4.223949 -4.2387729 -4.2483425 -4.24496][-4.2495174 -4.240169 -4.2366767 -4.2408533 -4.2472367 -4.2492852 -4.2429481 -4.2333517 -4.2317796 -4.2337723 -4.2363434 -4.2480569 -4.2668695 -4.2787223 -4.2748294][-4.2771678 -4.2732134 -4.27075 -4.2710509 -4.2740264 -4.2743487 -4.2692466 -4.2583623 -4.2518167 -4.2491031 -4.2493963 -4.2618608 -4.2823462 -4.2979641 -4.2982807][-4.2907414 -4.2880383 -4.2840061 -4.2786312 -4.2801733 -4.2827039 -4.2821245 -4.2722697 -4.2601018 -4.2529416 -4.2562461 -4.2724795 -4.2926512 -4.3079653 -4.3114128][-4.2859006 -4.2795587 -4.2712445 -4.2594075 -4.2587252 -4.2667074 -4.2751594 -4.2699304 -4.2550983 -4.2464919 -4.25022 -4.2663426 -4.2839165 -4.2960258 -4.3004527]]...]
INFO - root - 2017-12-05 22:27:09.230709: step 50310, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 65h:40m:42s remains)
INFO - root - 2017-12-05 22:27:17.882604: step 50320, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.849 sec/batch; 66h:34m:40s remains)
INFO - root - 2017-12-05 22:27:26.442248: step 50330, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 65h:24m:47s remains)
INFO - root - 2017-12-05 22:27:34.933798: step 50340, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 65h:35m:43s remains)
INFO - root - 2017-12-05 22:27:43.447607: step 50350, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 66h:19m:18s remains)
INFO - root - 2017-12-05 22:27:51.869472: step 50360, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 65h:56m:57s remains)
INFO - root - 2017-12-05 22:28:00.478276: step 50370, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 69h:01m:14s remains)
INFO - root - 2017-12-05 22:28:09.123196: step 50380, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 66h:39m:22s remains)
INFO - root - 2017-12-05 22:28:17.609735: step 50390, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 66h:59m:16s remains)
INFO - root - 2017-12-05 22:28:26.173700: step 50400, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 68h:30m:40s remains)
2017-12-05 22:28:26.925251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0902152 -4.0926714 -4.0925546 -4.0972495 -4.1047444 -4.1095505 -4.1123142 -4.1186986 -4.1287861 -4.1421347 -4.1527038 -4.1517687 -4.1409874 -4.1242323 -4.107913][-4.1171656 -4.1217403 -4.1247625 -4.135561 -4.1478214 -4.1533942 -4.1544948 -4.1624146 -4.1737227 -4.18362 -4.1902122 -4.1894503 -4.1784849 -4.1596365 -4.140914][-4.1425457 -4.1516714 -4.1585913 -4.1761761 -4.1968603 -4.2064662 -4.2078195 -4.2135344 -4.22321 -4.2314229 -4.2364807 -4.2352862 -4.2227407 -4.2009978 -4.1789527][-4.149467 -4.1586614 -4.1665583 -4.1873636 -4.2107754 -4.2169285 -4.2121081 -4.2132072 -4.2240882 -4.2393384 -4.2540364 -4.2592373 -4.2488155 -4.2264719 -4.2043066][-4.1484475 -4.1571617 -4.1626687 -4.1745338 -4.1831059 -4.1709504 -4.1496568 -4.1422276 -4.1612091 -4.1968613 -4.2305646 -4.2506995 -4.2528424 -4.2377019 -4.2179494][-4.1636505 -4.1686339 -4.1608691 -4.1489286 -4.1256456 -4.0804219 -4.0316191 -4.0113511 -4.044868 -4.1127353 -4.1736441 -4.2136726 -4.230372 -4.2282004 -4.2172346][-4.1896219 -4.1880889 -4.16463 -4.1218166 -4.0620847 -3.9794962 -3.8854949 -3.8334959 -3.8868055 -4.0007257 -4.09956 -4.1627827 -4.1931396 -4.2039509 -4.2027359][-4.204515 -4.2036371 -4.176209 -4.1201057 -4.0393972 -3.932518 -3.8020277 -3.7107596 -3.7728093 -3.9189904 -4.0378971 -4.11672 -4.1617694 -4.1842723 -4.1930442][-4.2004819 -4.20165 -4.1861982 -4.1466255 -4.0813794 -3.9907036 -3.8856487 -3.8127739 -3.8465991 -3.9503398 -4.0454245 -4.1145988 -4.1605778 -4.1883411 -4.2039318][-4.1895909 -4.1922894 -4.1891575 -4.1721182 -4.1353922 -4.0786347 -4.0136108 -3.9636979 -3.9671645 -4.0180478 -4.0768733 -4.1303468 -4.16945 -4.1948395 -4.2087975][-4.1802821 -4.1817646 -4.1855493 -4.1877179 -4.175561 -4.1499791 -4.1164136 -4.0802331 -4.0612068 -4.0749192 -4.1078644 -4.148541 -4.1773534 -4.1952209 -4.2012439][-4.1626978 -4.1671734 -4.1789064 -4.1916871 -4.1973014 -4.1931863 -4.1831141 -4.1596289 -4.131824 -4.1230521 -4.1372294 -4.1633148 -4.1809335 -4.1896105 -4.1894073][-4.1463075 -4.1602507 -4.1806622 -4.197093 -4.2071257 -4.2101355 -4.2073331 -4.19443 -4.1702704 -4.150023 -4.1476951 -4.1613684 -4.1735535 -4.1774306 -4.1694155][-4.1385789 -4.1646991 -4.190659 -4.2031574 -4.208992 -4.2102757 -4.2079859 -4.1990619 -4.17863 -4.1532283 -4.1380205 -4.1418991 -4.1567874 -4.1638637 -4.1532507][-4.1350942 -4.1694613 -4.1961069 -4.2036529 -4.2027245 -4.198041 -4.193696 -4.1894374 -4.1722236 -4.1454272 -4.1253862 -4.1224875 -4.1343164 -4.1413617 -4.1367168]]...]
INFO - root - 2017-12-05 22:28:35.302280: step 50410, loss = 2.04, batch loss = 1.98 (10.1 examples/sec; 0.794 sec/batch; 62h:14m:37s remains)
INFO - root - 2017-12-05 22:28:43.862758: step 50420, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 66h:50m:10s remains)
INFO - root - 2017-12-05 22:28:52.323055: step 50430, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 66h:49m:57s remains)
INFO - root - 2017-12-05 22:29:00.873531: step 50440, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 65h:19m:50s remains)
INFO - root - 2017-12-05 22:29:09.457291: step 50450, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 67h:41m:57s remains)
INFO - root - 2017-12-05 22:29:17.984821: step 50460, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 68h:54m:15s remains)
INFO - root - 2017-12-05 22:29:26.637143: step 50470, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 65h:11m:23s remains)
INFO - root - 2017-12-05 22:29:35.097469: step 50480, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 68h:05m:03s remains)
INFO - root - 2017-12-05 22:29:43.670649: step 50490, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 68h:24m:00s remains)
INFO - root - 2017-12-05 22:29:52.315508: step 50500, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 66h:52m:54s remains)
2017-12-05 22:29:53.063793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3198304 -4.3147144 -4.3109994 -4.3099794 -4.3115668 -4.3148317 -4.3192768 -4.3241296 -4.3279867 -4.330379 -4.3318052 -4.3329124 -4.3334241 -4.333086 -4.3337646][-4.3096652 -4.2994394 -4.2915826 -4.2853427 -4.2831326 -4.2858639 -4.2924247 -4.301374 -4.3096676 -4.3144388 -4.31738 -4.319622 -4.320117 -4.3190212 -4.3195853][-4.2932959 -4.2767429 -4.2643213 -4.2506247 -4.240324 -4.2372866 -4.2439532 -4.2622976 -4.2793164 -4.2871504 -4.293005 -4.2987485 -4.3011389 -4.2997336 -4.2987523][-4.2641573 -4.2359495 -4.2158165 -4.1945739 -4.1756091 -4.16976 -4.1802926 -4.2108297 -4.2360582 -4.2463636 -4.2572227 -4.2704139 -4.2763643 -4.27721 -4.2777476][-4.2278719 -4.1794276 -4.1428022 -4.1084037 -4.0793252 -4.07894 -4.1059995 -4.1505461 -4.1772518 -4.1826897 -4.2023726 -4.2308636 -4.2464223 -4.2530274 -4.2593513][-4.202117 -4.1343341 -4.0756178 -4.0173726 -3.9722574 -3.9769011 -4.0253625 -4.0839863 -4.1061664 -4.1013961 -4.1284957 -4.1755128 -4.2057309 -4.2235332 -4.2407484][-4.1966691 -4.1180687 -4.0441918 -3.9713087 -3.915566 -3.9148905 -3.9717021 -4.0310559 -4.0388989 -4.0227647 -4.0562639 -4.1190076 -4.1613836 -4.1904097 -4.2206945][-4.2115602 -4.1330895 -4.0565739 -3.9872723 -3.9396279 -3.9383759 -3.9839468 -4.0207267 -4.0009618 -3.9689121 -4.0052071 -4.0791721 -4.13067 -4.1672039 -4.2032866][-4.2358775 -4.1676216 -4.0987878 -4.0356727 -3.9971135 -3.9934483 -4.0218143 -4.0296855 -3.9854045 -3.942122 -3.9799092 -4.0615196 -4.1236496 -4.1633534 -4.1991844][-4.2506876 -4.1955209 -4.13925 -4.0903215 -4.06246 -4.0576229 -4.0668364 -4.0519209 -3.9924941 -3.948138 -3.9808953 -4.06142 -4.1322484 -4.1752377 -4.2118154][-4.2652755 -4.2260804 -4.1851368 -4.1505814 -4.1329923 -4.1266127 -4.11513 -4.0825257 -4.0215373 -3.9817426 -4.0057559 -4.0758538 -4.1453276 -4.1889338 -4.22879][-4.2835526 -4.2568388 -4.2270584 -4.2030206 -4.1910028 -4.1796989 -4.1522088 -4.1061988 -4.0467734 -4.015831 -4.0374222 -4.0960321 -4.1571984 -4.2004647 -4.2411036][-4.3025479 -4.2807274 -4.2544041 -4.2308702 -4.2157025 -4.1978 -4.1594934 -4.107213 -4.0512133 -4.0299249 -4.0566225 -4.1111255 -4.169075 -4.2081394 -4.2412367][-4.322535 -4.3055911 -4.2809262 -4.2540393 -4.233851 -4.2138834 -4.1780496 -4.1333966 -4.087225 -4.0726266 -4.0998511 -4.1477647 -4.1984377 -4.2294741 -4.2517323][-4.3363924 -4.3240867 -4.3035874 -4.27738 -4.2576156 -4.2420688 -4.2156224 -4.1845942 -4.1535053 -4.147666 -4.1731567 -4.2107034 -4.2469854 -4.26581 -4.2776337]]...]
INFO - root - 2017-12-05 22:30:01.535912: step 50510, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 65h:30m:37s remains)
INFO - root - 2017-12-05 22:30:09.926426: step 50520, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 64h:21m:06s remains)
INFO - root - 2017-12-05 22:30:18.569429: step 50530, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 65h:54m:59s remains)
INFO - root - 2017-12-05 22:30:27.168549: step 50540, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 67h:12m:26s remains)
INFO - root - 2017-12-05 22:30:35.717775: step 50550, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 66h:08m:33s remains)
INFO - root - 2017-12-05 22:30:44.178819: step 50560, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 68h:02m:55s remains)
INFO - root - 2017-12-05 22:30:52.702127: step 50570, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 64h:46m:46s remains)
INFO - root - 2017-12-05 22:31:01.268092: step 50580, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 68h:14m:49s remains)
INFO - root - 2017-12-05 22:31:09.747533: step 50590, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 67h:57m:47s remains)
INFO - root - 2017-12-05 22:31:18.397453: step 50600, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 69h:16m:28s remains)
2017-12-05 22:31:19.153353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2059393 -4.2192979 -4.228879 -4.2335052 -4.2385063 -4.2272587 -4.2003527 -4.1809411 -4.184588 -4.2163329 -4.2497096 -4.2649093 -4.2656155 -4.2643514 -4.2605338][-4.2228723 -4.2355042 -4.2447057 -4.2534294 -4.2609134 -4.2452068 -4.2056651 -4.1666603 -4.1567264 -4.1893425 -4.2283173 -4.2497973 -4.2641454 -4.2764773 -4.2818589][-4.2440391 -4.252367 -4.2590451 -4.2663317 -4.2695227 -4.2476363 -4.1916881 -4.1323214 -4.114686 -4.1482825 -4.1909437 -4.2230868 -4.2544312 -4.2810307 -4.2958813][-4.2499442 -4.2510662 -4.2524161 -4.25515 -4.2535939 -4.2260032 -4.1520686 -4.0728178 -4.0540314 -4.0926595 -4.1464725 -4.1969075 -4.24123 -4.2787933 -4.3013525][-4.23824 -4.2338753 -4.2291303 -4.2266245 -4.2160163 -4.1792378 -4.0872183 -3.9890008 -3.9709036 -4.02847 -4.1038485 -4.1740246 -4.2293458 -4.2720337 -4.2966671][-4.2267828 -4.2171397 -4.207942 -4.1992497 -4.1748939 -4.1203675 -4.0028334 -3.8739202 -3.8558412 -3.9511516 -4.0618153 -4.1543322 -4.2177982 -4.2617021 -4.2843256][-4.2217364 -4.2108812 -4.2000365 -4.1806612 -4.1384897 -4.0614233 -3.9069853 -3.7286589 -3.7189209 -3.8800142 -4.0307274 -4.1382475 -4.2080507 -4.2531176 -4.2775822][-4.220346 -4.212162 -4.198585 -4.1672077 -4.1115193 -4.022295 -3.8509729 -3.651001 -3.6629426 -3.8722353 -4.0364218 -4.1416955 -4.211 -4.256299 -4.2827821][-4.22268 -4.2173586 -4.2002859 -4.1606722 -4.1057825 -4.0265 -3.8850591 -3.7453971 -3.7824898 -3.9552588 -4.0833058 -4.1627836 -4.2255592 -4.2719221 -4.2975764][-4.2286563 -4.2257328 -4.2038417 -4.1621704 -4.1188536 -4.06365 -3.9730048 -3.9036896 -3.9451363 -4.0476317 -4.1241751 -4.1795373 -4.237505 -4.2842765 -4.3097615][-4.2425814 -4.2354126 -4.2040415 -4.1665096 -4.1423512 -4.1163526 -4.0716438 -4.0447369 -4.069644 -4.117589 -4.15739 -4.1978445 -4.2477388 -4.2933497 -4.3196721][-4.2585297 -4.24384 -4.2031522 -4.1717806 -4.1700506 -4.1718278 -4.1603804 -4.1542821 -4.1601229 -4.1762924 -4.1961856 -4.2263479 -4.2628407 -4.3002639 -4.3246346][-4.2670627 -4.2493515 -4.205143 -4.1749887 -4.1852226 -4.2040367 -4.2132745 -4.2202926 -4.2237549 -4.2259464 -4.2322178 -4.252872 -4.2760429 -4.3014879 -4.322051][-4.2774944 -4.2597427 -4.2195306 -4.1911664 -4.2007961 -4.2220993 -4.2403464 -4.2566061 -4.2653866 -4.2623496 -4.2599988 -4.2698383 -4.2803025 -4.2945943 -4.3076305][-4.2803154 -4.2632704 -4.2352324 -4.2144465 -4.219059 -4.2390585 -4.2613192 -4.2798553 -4.2862206 -4.2794633 -4.2714863 -4.2721367 -4.2748966 -4.2798505 -4.2856216]]...]
INFO - root - 2017-12-05 22:31:27.724964: step 50610, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 69h:10m:37s remains)
INFO - root - 2017-12-05 22:31:36.283485: step 50620, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 67h:26m:12s remains)
INFO - root - 2017-12-05 22:31:44.664527: step 50630, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.858 sec/batch; 67h:11m:50s remains)
INFO - root - 2017-12-05 22:31:53.228236: step 50640, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 67h:13m:29s remains)
INFO - root - 2017-12-05 22:32:01.791635: step 50650, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 67h:16m:34s remains)
INFO - root - 2017-12-05 22:32:09.981804: step 50660, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 65h:43m:57s remains)
INFO - root - 2017-12-05 22:32:18.431458: step 50670, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 66h:41m:30s remains)
INFO - root - 2017-12-05 22:32:26.998385: step 50680, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 67h:30m:01s remains)
INFO - root - 2017-12-05 22:32:35.530128: step 50690, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 68h:15m:37s remains)
INFO - root - 2017-12-05 22:32:44.097665: step 50700, loss = 2.02, batch loss = 1.96 (9.3 examples/sec; 0.857 sec/batch; 67h:07m:18s remains)
2017-12-05 22:32:44.915238: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1867085 -4.2207937 -4.2474141 -4.2580681 -4.2506056 -4.2310123 -4.212709 -4.2067428 -4.2178617 -4.2376413 -4.2517505 -4.2446132 -4.2045784 -4.1577606 -4.1337614][-4.1888232 -4.2197618 -4.241086 -4.244432 -4.2287774 -4.2045741 -4.1896892 -4.1887965 -4.2077451 -4.2348313 -4.2519059 -4.24444 -4.2006812 -4.1466179 -4.120791][-4.1963077 -4.2246618 -4.242311 -4.2356963 -4.206274 -4.1773229 -4.1717772 -4.1784954 -4.2044654 -4.2342386 -4.2522569 -4.2458692 -4.2072234 -4.1564827 -4.1299591][-4.2038479 -4.2308416 -4.2465487 -4.2317104 -4.1901722 -4.1585321 -4.16033 -4.17312 -4.2042985 -4.234849 -4.2525959 -4.2481742 -4.2191253 -4.1780205 -4.1517115][-4.212081 -4.2371888 -4.2499228 -4.2287011 -4.1777267 -4.1408386 -4.1417913 -4.15789 -4.1961722 -4.2328053 -4.2549443 -4.253551 -4.2311673 -4.1990414 -4.1742907][-4.2140884 -4.2380905 -4.2503624 -4.2262139 -4.1669445 -4.1181746 -4.1106572 -4.1297679 -4.1784377 -4.2253876 -4.2553596 -4.2568541 -4.2352533 -4.2057104 -4.1834111][-4.2079587 -4.2329469 -4.2473788 -4.2265005 -4.163836 -4.10205 -4.0822144 -4.1005721 -4.1563139 -4.212153 -4.2493696 -4.254014 -4.2308187 -4.2011805 -4.1823983][-4.1854444 -4.2126684 -4.2328753 -4.2193685 -4.1631184 -4.0978856 -4.0694728 -4.0826674 -4.1362567 -4.1933503 -4.2341638 -4.2424946 -4.2211285 -4.1923838 -4.1762624][-4.1515074 -4.1745472 -4.1978421 -4.1947846 -4.1561995 -4.1044683 -4.0801988 -4.0898075 -4.131134 -4.179739 -4.21685 -4.2265587 -4.2104115 -4.1863084 -4.1731482][-4.1353989 -4.1431212 -4.157012 -4.1557145 -4.1308107 -4.0978365 -4.0893111 -4.1065288 -4.1390738 -4.1775436 -4.2085867 -4.2185416 -4.20671 -4.1858335 -4.1734152][-4.1596832 -4.1467824 -4.1405358 -4.12756 -4.105938 -4.0879259 -4.0940223 -4.1183295 -4.1461158 -4.1759377 -4.2016082 -4.2110362 -4.20106 -4.1812968 -4.1693149][-4.2116714 -4.1919627 -4.1690488 -4.1415129 -4.1169114 -4.1061964 -4.11526 -4.1340261 -4.1497765 -4.1666274 -4.1826596 -4.190732 -4.1842861 -4.167232 -4.1579366][-4.2536845 -4.2467132 -4.2259393 -4.1952634 -4.1700435 -4.15914 -4.1589026 -4.1618414 -4.1597681 -4.1595173 -4.165349 -4.1697273 -4.1639695 -4.1483665 -4.1393414][-4.2522459 -4.266839 -4.2652607 -4.2516875 -4.2376189 -4.228344 -4.2181215 -4.2053533 -4.1877384 -4.1730475 -4.1686306 -4.1635785 -4.1514511 -4.1314311 -4.1167431][-4.2149348 -4.2448092 -4.2635055 -4.2714419 -4.2757459 -4.2753897 -4.2660384 -4.2496505 -4.2275453 -4.2089658 -4.2011795 -4.1880918 -4.1624207 -4.1291509 -4.1037364]]...]
INFO - root - 2017-12-05 22:32:53.398184: step 50710, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 68h:27m:44s remains)
INFO - root - 2017-12-05 22:33:01.996705: step 50720, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 66h:26m:37s remains)
INFO - root - 2017-12-05 22:33:10.511106: step 50730, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 65h:22m:58s remains)
INFO - root - 2017-12-05 22:33:19.014652: step 50740, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 69h:57m:14s remains)
INFO - root - 2017-12-05 22:33:27.658094: step 50750, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 67h:02m:47s remains)
INFO - root - 2017-12-05 22:33:36.113965: step 50760, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 65h:29m:22s remains)
INFO - root - 2017-12-05 22:33:44.621685: step 50770, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 66h:11m:03s remains)
INFO - root - 2017-12-05 22:33:53.165113: step 50780, loss = 2.03, batch loss = 1.98 (9.7 examples/sec; 0.823 sec/batch; 64h:24m:56s remains)
INFO - root - 2017-12-05 22:34:01.691720: step 50790, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 68h:08m:35s remains)
INFO - root - 2017-12-05 22:34:10.233227: step 50800, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 67h:55m:26s remains)
2017-12-05 22:34:11.009092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1397324 -4.1542797 -4.1703491 -4.1685 -4.1548243 -4.1443534 -4.142756 -4.1535821 -4.1719642 -4.1858482 -4.1952353 -4.2172079 -4.2385097 -4.2477479 -4.2557817][-4.0901785 -4.0977764 -4.104948 -4.0958085 -4.076757 -4.0677257 -4.0733194 -4.0925941 -4.1235342 -4.1488886 -4.1655531 -4.1926179 -4.2204375 -4.2367697 -4.2545886][-4.0589476 -4.0597897 -4.0628462 -4.0502768 -4.0304475 -4.0228481 -4.0329304 -4.0559673 -4.0951428 -4.132062 -4.1600637 -4.1918063 -4.2187357 -4.2341404 -4.25719][-4.0738811 -4.075356 -4.081953 -4.073885 -4.0573282 -4.0479894 -4.0505252 -4.0619144 -4.0919728 -4.1265364 -4.1601834 -4.1939554 -4.2164845 -4.2305689 -4.2532697][-4.1007433 -4.1076436 -4.1195955 -4.1202378 -4.1128154 -4.1035061 -4.0953541 -4.0906858 -4.10386 -4.1261134 -4.1592669 -4.1944075 -4.2160745 -4.2316136 -4.2512746][-4.1077685 -4.1233993 -4.1401353 -4.144331 -4.1386085 -4.1251597 -4.1043224 -4.0887456 -4.0921764 -4.1098752 -4.1441383 -4.1786237 -4.2016325 -4.2236619 -4.2455363][-4.0802193 -4.0982919 -4.1176796 -4.1168876 -4.1007605 -4.0744319 -4.0463834 -4.0324616 -4.0366545 -4.0585313 -4.1062231 -4.1502476 -4.1772261 -4.2053576 -4.234252][-4.0443516 -4.0471954 -4.0596943 -4.0476546 -4.0153847 -3.9733582 -3.9427063 -3.9408062 -3.9601774 -4.0001278 -4.0653648 -4.1225948 -4.1576529 -4.1934848 -4.2281795][-4.02539 -4.0159097 -4.0244832 -4.0066242 -3.9676142 -3.9249268 -3.9019275 -3.9173942 -3.9489894 -3.9956129 -4.056365 -4.1119132 -4.1496744 -4.1916122 -4.2324409][-4.029706 -4.015871 -4.0244927 -4.0105805 -3.9771285 -3.9484789 -3.9453981 -3.9718661 -4.0008378 -4.0340552 -4.0757518 -4.1177473 -4.1523714 -4.1965523 -4.2397804][-4.0500336 -4.0386834 -4.0468946 -4.0395594 -4.0184431 -4.0025468 -4.007812 -4.0314341 -4.0508089 -4.0701137 -4.0984068 -4.1295962 -4.1602259 -4.20323 -4.2460933][-4.0986013 -4.098516 -4.1106062 -4.1123781 -4.1041851 -4.0988369 -4.1033945 -4.1169367 -4.1267376 -4.1371021 -4.1551528 -4.1767378 -4.199337 -4.2304564 -4.2627392][-4.1572371 -4.1645756 -4.1784849 -4.1835837 -4.1827908 -4.1824083 -4.185514 -4.1913066 -4.1960387 -4.202323 -4.2136583 -4.2275515 -4.2427154 -4.2648845 -4.2873926][-4.22213 -4.2291803 -4.2386594 -4.2415438 -4.2414432 -4.2408166 -4.241684 -4.2434573 -4.2455244 -4.2497406 -4.2587042 -4.2693844 -4.2800117 -4.2953072 -4.3108978][-4.2696867 -4.2732625 -4.2770061 -4.2774944 -4.2769909 -4.2763438 -4.2761168 -4.276175 -4.2770195 -4.2807503 -4.2882838 -4.296771 -4.3045468 -4.314568 -4.3255005]]...]
INFO - root - 2017-12-05 22:34:19.441931: step 50810, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 65h:36m:47s remains)
INFO - root - 2017-12-05 22:34:27.983263: step 50820, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 65h:55m:46s remains)
INFO - root - 2017-12-05 22:34:36.454742: step 50830, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 66h:32m:59s remains)
INFO - root - 2017-12-05 22:34:44.862167: step 50840, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 66h:46m:15s remains)
INFO - root - 2017-12-05 22:34:53.260077: step 50850, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 65h:48m:27s remains)
INFO - root - 2017-12-05 22:35:01.729654: step 50860, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 66h:56m:59s remains)
INFO - root - 2017-12-05 22:35:10.195130: step 50870, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 65h:32m:44s remains)
INFO - root - 2017-12-05 22:35:18.711574: step 50880, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 65h:21m:54s remains)
INFO - root - 2017-12-05 22:35:27.150548: step 50890, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 65h:20m:44s remains)
INFO - root - 2017-12-05 22:35:35.634098: step 50900, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.819 sec/batch; 64h:05m:36s remains)
2017-12-05 22:35:36.412917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2514668 -4.2398252 -4.2352405 -4.2426724 -4.2461243 -4.2429824 -4.23152 -4.2068481 -4.1748767 -4.1636724 -4.1685848 -4.1712365 -4.1825762 -4.2055917 -4.2233582][-4.2430506 -4.2293739 -4.2265759 -4.2381353 -4.2444763 -4.2398238 -4.2303724 -4.2055812 -4.1718378 -4.1543756 -4.1523232 -4.1535969 -4.1617236 -4.1859593 -4.2109542][-4.2378249 -4.2159033 -4.2120938 -4.2229137 -4.2294388 -4.2234931 -4.2124405 -4.1793666 -4.1429725 -4.1265478 -4.1261225 -4.1299696 -4.1403809 -4.1705074 -4.2045751][-4.2244544 -4.1993279 -4.2000575 -4.2144017 -4.2232089 -4.2116928 -4.186728 -4.1358485 -4.0941854 -4.0880623 -4.1005192 -4.1160364 -4.1333404 -4.1660533 -4.2026567][-4.2031016 -4.1777339 -4.1792474 -4.1963396 -4.2084231 -4.1872692 -4.1325955 -4.0595994 -4.0219097 -4.0384135 -4.0819139 -4.1191292 -4.1428609 -4.167954 -4.198947][-4.1925664 -4.1740155 -4.1760216 -4.18133 -4.1799593 -4.1334238 -4.0378342 -3.9461405 -3.9243188 -3.9791405 -4.0683904 -4.1281071 -4.1501126 -4.1643944 -4.1839809][-4.1991377 -4.1930723 -4.1904507 -4.1725655 -4.1386905 -4.0542483 -3.920269 -3.8311522 -3.849525 -3.9505422 -4.0693507 -4.1359625 -4.1487069 -4.1523814 -4.1592965][-4.22091 -4.2240319 -4.2131085 -4.1658525 -4.0887876 -3.9766366 -3.8426638 -3.7861638 -3.8477139 -3.9809222 -4.0966964 -4.1497555 -4.1538763 -4.146759 -4.1424341][-4.2453842 -4.24527 -4.21757 -4.1418104 -4.0387573 -3.9317789 -3.8528147 -3.8504677 -3.9330916 -4.0603771 -4.1486192 -4.182312 -4.1782022 -4.1658187 -4.1551414][-4.2550364 -4.2408633 -4.1868715 -4.0913429 -3.9891357 -3.9226923 -3.9129388 -3.9571486 -4.0396662 -4.1356182 -4.1942158 -4.2125354 -4.2054577 -4.1969829 -4.1923504][-4.2451882 -4.2127385 -4.1379604 -4.0345116 -3.9523304 -3.9365704 -3.9789054 -4.0464287 -4.1173849 -4.179368 -4.21362 -4.221447 -4.2167606 -4.219491 -4.228169][-4.208385 -4.1607685 -4.0748606 -3.9798532 -3.9340086 -3.9650371 -4.0347533 -4.1045423 -4.1599474 -4.1980925 -4.2140746 -4.2152867 -4.2154412 -4.2282963 -4.2467804][-4.1653996 -4.1081419 -4.0230122 -3.952719 -3.9439387 -3.9956715 -4.0692596 -4.1301146 -4.1756382 -4.204114 -4.2121682 -4.2119756 -4.2154408 -4.2318325 -4.2528658][-4.1327405 -4.08042 -4.011055 -3.9657328 -3.9752231 -4.0283408 -4.0964427 -4.1487856 -4.1924191 -4.2199025 -4.2251759 -4.2247496 -4.2286425 -4.2447453 -4.2634892][-4.1215682 -4.0816851 -4.0336785 -4.0042663 -4.0185051 -4.070653 -4.134089 -4.1804953 -4.220469 -4.2429128 -4.24613 -4.2470407 -4.2515488 -4.2642169 -4.277422]]...]
INFO - root - 2017-12-05 22:35:44.818960: step 50910, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 65h:08m:43s remains)
INFO - root - 2017-12-05 22:35:53.066051: step 50920, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.816 sec/batch; 63h:49m:59s remains)
INFO - root - 2017-12-05 22:36:01.678991: step 50930, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 66h:09m:51s remains)
INFO - root - 2017-12-05 22:36:10.258712: step 50940, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 67h:51m:37s remains)
INFO - root - 2017-12-05 22:36:18.701669: step 50950, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 66h:33m:00s remains)
INFO - root - 2017-12-05 22:36:27.087206: step 50960, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 65h:56m:23s remains)
INFO - root - 2017-12-05 22:36:35.555009: step 50970, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 66h:52m:00s remains)
INFO - root - 2017-12-05 22:36:44.101789: step 50980, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 66h:36m:21s remains)
INFO - root - 2017-12-05 22:36:52.634205: step 50990, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 68h:19m:12s remains)
INFO - root - 2017-12-05 22:37:01.217334: step 51000, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.868 sec/batch; 67h:52m:32s remains)
2017-12-05 22:37:02.021124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3178964 -4.3210912 -4.3204274 -4.3204937 -4.3222175 -4.3231144 -4.3222914 -4.3217854 -4.3224764 -4.326972 -4.3342938 -4.3408518 -4.34663 -4.3505082 -4.3514423][-4.3024797 -4.3043427 -4.301198 -4.3015733 -4.3044953 -4.3014693 -4.2932396 -4.2900648 -4.2946811 -4.3036776 -4.3152184 -4.3252234 -4.3326011 -4.33671 -4.3402271][-4.2874451 -4.2854772 -4.2785087 -4.2794676 -4.2814546 -4.270062 -4.2477293 -4.2363529 -4.2431011 -4.2594714 -4.277926 -4.2952003 -4.310338 -4.3191113 -4.3275075][-4.2681217 -4.2636547 -4.2546144 -4.2544146 -4.254436 -4.2307215 -4.19018 -4.1678433 -4.1815505 -4.2120328 -4.2398038 -4.2657104 -4.2923055 -4.3096328 -4.3214345][-4.2423038 -4.2342715 -4.2207475 -4.2148976 -4.2098269 -4.1769118 -4.1182976 -4.0857463 -4.1105881 -4.1598897 -4.1989875 -4.2313886 -4.2680631 -4.295763 -4.3132687][-4.2021151 -4.1856742 -4.1663613 -4.1532569 -4.1417155 -4.0969586 -4.0168686 -3.9765003 -4.022501 -4.090095 -4.1408629 -4.1841345 -4.2337761 -4.271842 -4.2984524][-4.1492367 -4.1233125 -4.098598 -4.0813532 -4.0677781 -4.0112615 -3.8960345 -3.8337345 -3.909631 -4.0100932 -4.0828452 -4.1449656 -4.2087736 -4.25746 -4.2894816][-4.1139088 -4.08797 -4.0624256 -4.0443597 -4.0367494 -3.9904037 -3.8701725 -3.7921004 -3.8742967 -3.9909482 -4.0733356 -4.1415486 -4.207715 -4.2571716 -4.2884297][-4.109694 -4.091507 -4.0757546 -4.0662217 -4.073832 -4.0560827 -3.9823461 -3.9268222 -3.9758909 -4.0641804 -4.1252303 -4.1746492 -4.2256932 -4.2659187 -4.2912297][-4.1261349 -4.1118908 -4.1054387 -4.1070971 -4.1233263 -4.1183424 -4.0791559 -4.0487223 -4.0778203 -4.1330233 -4.1690712 -4.2006817 -4.2364054 -4.2690024 -4.2944412][-4.1612349 -4.1421113 -4.1351137 -4.1380343 -4.1489 -4.1441212 -4.1165409 -4.0988221 -4.1204824 -4.1565285 -4.1801705 -4.2059536 -4.238606 -4.2718334 -4.3020997][-4.2147255 -4.1902618 -4.1742043 -4.1703033 -4.175086 -4.1704555 -4.1520796 -4.1413469 -4.1537042 -4.1745157 -4.1892333 -4.2119904 -4.2467031 -4.2818375 -4.3126078][-4.26758 -4.2451372 -4.2257919 -4.2160449 -4.2163792 -4.2153869 -4.2050924 -4.20111 -4.2068453 -4.2163043 -4.2232356 -4.2389922 -4.267684 -4.2976522 -4.3235097][-4.3077283 -4.2916512 -4.2762532 -4.2648196 -4.262454 -4.2637758 -4.2601852 -4.259428 -4.262898 -4.268435 -4.2721062 -4.2815804 -4.3003469 -4.3200927 -4.3362727][-4.3297729 -4.3202062 -4.3109016 -4.3023648 -4.297709 -4.297256 -4.2967587 -4.2986865 -4.30278 -4.3072352 -4.310359 -4.3162 -4.3268538 -4.3378572 -4.3452754]]...]
INFO - root - 2017-12-05 22:37:10.589543: step 51010, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 67h:04m:47s remains)
INFO - root - 2017-12-05 22:37:19.108934: step 51020, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 68h:04m:47s remains)
INFO - root - 2017-12-05 22:37:27.597946: step 51030, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 65h:42m:22s remains)
INFO - root - 2017-12-05 22:37:36.079502: step 51040, loss = 2.02, batch loss = 1.96 (9.4 examples/sec; 0.847 sec/batch; 66h:12m:20s remains)
INFO - root - 2017-12-05 22:37:44.443733: step 51050, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.748 sec/batch; 58h:26m:33s remains)
INFO - root - 2017-12-05 22:37:52.907721: step 51060, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 67h:26m:34s remains)
INFO - root - 2017-12-05 22:38:01.432434: step 51070, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 66h:51m:32s remains)
INFO - root - 2017-12-05 22:38:10.034460: step 51080, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 66h:56m:31s remains)
INFO - root - 2017-12-05 22:38:18.567959: step 51090, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 67h:45m:53s remains)
INFO - root - 2017-12-05 22:38:27.082302: step 51100, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 67h:37m:52s remains)
2017-12-05 22:38:27.926535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2238159 -4.1847639 -4.1362524 -4.0955229 -4.0940876 -4.1283054 -4.1619186 -4.1717429 -4.1618938 -4.1457229 -4.1462564 -4.1660814 -4.18689 -4.1992083 -4.1866508][-4.219851 -4.1904807 -4.1595144 -4.1359596 -4.1378393 -4.1578794 -4.1756096 -4.1802368 -4.1699839 -4.1583548 -4.1643229 -4.1825542 -4.1999211 -4.2092714 -4.1932955][-4.1979127 -4.1765728 -4.1618652 -4.1487708 -4.1499586 -4.1583357 -4.1647415 -4.1670165 -4.16203 -4.1604242 -4.174623 -4.195066 -4.2133904 -4.2218871 -4.2055006][-4.1552563 -4.1344118 -4.133173 -4.1354356 -4.14001 -4.1404285 -4.1318297 -4.1244044 -4.1232028 -4.1393757 -4.1696587 -4.1963305 -4.2159386 -4.2265711 -4.2160692][-4.1133423 -4.0936856 -4.1036592 -4.1192493 -4.1257987 -4.1092381 -4.074172 -4.0499349 -4.0542574 -4.092144 -4.1441607 -4.1830153 -4.2057962 -4.2183313 -4.2170005][-4.1211424 -4.1110845 -4.1191792 -4.12726 -4.1187711 -4.0717854 -3.9970324 -3.9457297 -3.9547732 -4.0183635 -4.0970707 -4.1507077 -4.1784329 -4.1917953 -4.1963096][-4.1870642 -4.1828957 -4.1704984 -4.1541338 -4.1235762 -4.0467887 -3.9319448 -3.8509784 -3.8694665 -3.958684 -4.0544004 -4.1145048 -4.143126 -4.1549878 -4.160574][-4.2541575 -4.2563868 -4.2317867 -4.1963835 -4.1509352 -4.0674205 -3.9516261 -3.8725748 -3.8885083 -3.9674213 -4.0470443 -4.095191 -4.1174545 -4.1264873 -4.1294994][-4.3016133 -4.3129454 -4.2892766 -4.251 -4.204917 -4.1348691 -4.0489459 -3.9947696 -3.9984283 -4.0360293 -4.0684061 -4.0916243 -4.1077104 -4.1163368 -4.1164303][-4.3210807 -4.3374958 -4.3185077 -4.2832432 -4.2419462 -4.19026 -4.13274 -4.097703 -4.098063 -4.1089511 -4.1075578 -4.1093545 -4.1166148 -4.1251421 -4.1221128][-4.3198576 -4.3361378 -4.3214679 -4.287816 -4.24867 -4.2057667 -4.1639428 -4.1394677 -4.1413922 -4.1534677 -4.1550064 -4.1517954 -4.1501431 -4.1538258 -4.145936][-4.3064108 -4.3208046 -4.3093982 -4.2823429 -4.2519155 -4.2183361 -4.1883988 -4.1729465 -4.1792784 -4.200387 -4.2144604 -4.2120314 -4.2028704 -4.1973581 -4.1784315][-4.2880435 -4.2989006 -4.2942133 -4.2812867 -4.2655544 -4.24376 -4.2235479 -4.2169943 -4.2277684 -4.2490697 -4.2657242 -4.2604551 -4.2450795 -4.2312036 -4.2025661][-4.2752023 -4.2822371 -4.2831879 -4.2820592 -4.2785625 -4.2695851 -4.2606316 -4.2605333 -4.2700987 -4.2838011 -4.2944713 -4.286087 -4.2672133 -4.2469525 -4.21452][-4.2954969 -4.2990136 -4.3005371 -4.3024883 -4.3028669 -4.3018069 -4.3014646 -4.3042674 -4.3104253 -4.3169994 -4.320611 -4.3130269 -4.2973228 -4.2770848 -4.2464452]]...]
INFO - root - 2017-12-05 22:38:36.519350: step 51110, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 65h:35m:06s remains)
INFO - root - 2017-12-05 22:38:45.111219: step 51120, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.835 sec/batch; 65h:15m:39s remains)
INFO - root - 2017-12-05 22:38:53.673725: step 51130, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 65h:41m:38s remains)
INFO - root - 2017-12-05 22:39:02.220072: step 51140, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 70h:54m:08s remains)
INFO - root - 2017-12-05 22:39:10.801287: step 51150, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 67h:07m:32s remains)
INFO - root - 2017-12-05 22:39:19.182443: step 51160, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 68h:11m:34s remains)
INFO - root - 2017-12-05 22:39:27.708738: step 51170, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 64h:15m:27s remains)
INFO - root - 2017-12-05 22:39:36.331419: step 51180, loss = 2.12, batch loss = 2.06 (9.6 examples/sec; 0.833 sec/batch; 65h:03m:34s remains)
INFO - root - 2017-12-05 22:39:44.922733: step 51190, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 68h:42m:02s remains)
INFO - root - 2017-12-05 22:39:53.462369: step 51200, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 68h:35m:46s remains)
2017-12-05 22:39:54.308467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2935791 -4.2915816 -4.2893438 -4.2864604 -4.2823138 -4.2787127 -4.2774105 -4.2786903 -4.282042 -4.2870021 -4.2920032 -4.2946315 -4.2938366 -4.2921543 -4.2912216][-4.2730508 -4.2717648 -4.2701783 -4.2664447 -4.2587857 -4.2509055 -4.2476277 -4.2498198 -4.2566547 -4.2656264 -4.2742844 -4.2798405 -4.2805061 -4.2800007 -4.2802067][-4.2570653 -4.2569275 -4.255136 -4.2479029 -4.2342243 -4.2197647 -4.2125978 -4.2151923 -4.227756 -4.2429361 -4.256022 -4.2641687 -4.2649341 -4.2634897 -4.2625337][-4.2469616 -4.2453089 -4.2389507 -4.2237396 -4.200346 -4.1772957 -4.1657686 -4.1707597 -4.19362 -4.219532 -4.2396255 -4.250226 -4.2489529 -4.2434435 -4.2370458][-4.2206173 -4.217195 -4.2048283 -4.1798425 -4.1447186 -4.1101665 -4.090445 -4.0979037 -4.1339922 -4.1758924 -4.2084661 -4.2238131 -4.2205358 -4.2081203 -4.1927371][-4.1841145 -4.1814542 -4.1638713 -4.1298528 -4.0835409 -4.0348825 -3.9985197 -4.0022244 -4.0524664 -4.1143575 -4.1649647 -4.1907959 -4.1915722 -4.1756 -4.1506562][-4.1615977 -4.1599693 -4.1394072 -4.1006923 -4.0457039 -3.9833612 -3.9315326 -3.9296613 -3.9914341 -4.0688691 -4.1345329 -4.1712027 -4.1793232 -4.1656375 -4.1323996][-4.1808753 -4.1796861 -4.1628566 -4.1280646 -4.0744748 -4.0132017 -3.9648712 -3.9647324 -4.0230365 -4.0962071 -4.1605535 -4.1973758 -4.2069325 -4.1922116 -4.1503582][-4.1994624 -4.2039423 -4.198442 -4.1736631 -4.1281972 -4.0765114 -4.0422854 -4.050374 -4.0976515 -4.1515718 -4.1995816 -4.2281556 -4.234839 -4.2176385 -4.17318][-4.1921296 -4.205183 -4.2134285 -4.2011676 -4.1662569 -4.1241388 -4.0990977 -4.1112871 -4.1499352 -4.1876855 -4.2201834 -4.2425623 -4.2498751 -4.2387195 -4.2032471][-4.1629381 -4.1879835 -4.2113447 -4.2106204 -4.1831408 -4.14688 -4.1225348 -4.1316476 -4.16508 -4.1983418 -4.2293916 -4.2564836 -4.270721 -4.2696462 -4.2444148][-4.1291437 -4.1581945 -4.1891046 -4.1982141 -4.1797709 -4.1509075 -4.1250434 -4.1261725 -4.1566644 -4.192719 -4.2283077 -4.2605238 -4.2801595 -4.2868915 -4.2714643][-4.1039433 -4.1323786 -4.1681275 -4.1875706 -4.1837168 -4.1699047 -4.1457834 -4.1366081 -4.1575208 -4.1875582 -4.221313 -4.2546287 -4.2781725 -4.2894912 -4.282989][-4.1104059 -4.1365857 -4.1691203 -4.1887641 -4.1977372 -4.2027087 -4.1876535 -4.1726441 -4.1805086 -4.1961365 -4.2198925 -4.247489 -4.2712297 -4.2823696 -4.2842255][-4.1380153 -4.1531243 -4.1722488 -4.18301 -4.2001476 -4.2234106 -4.2229996 -4.2120309 -4.209486 -4.2101727 -4.2214255 -4.2427545 -4.264585 -4.2762356 -4.2847052]]...]
INFO - root - 2017-12-05 22:40:02.780233: step 51210, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 65h:51m:09s remains)
INFO - root - 2017-12-05 22:40:11.306605: step 51220, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 68h:10m:24s remains)
INFO - root - 2017-12-05 22:40:19.912229: step 51230, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 65h:08m:07s remains)
INFO - root - 2017-12-05 22:40:28.411520: step 51240, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 66h:31m:06s remains)
INFO - root - 2017-12-05 22:40:36.891079: step 51250, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.857 sec/batch; 66h:55m:47s remains)
INFO - root - 2017-12-05 22:40:45.270692: step 51260, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 67h:24m:26s remains)
INFO - root - 2017-12-05 22:40:53.564413: step 51270, loss = 2.02, batch loss = 1.96 (9.5 examples/sec; 0.841 sec/batch; 65h:41m:22s remains)
INFO - root - 2017-12-05 22:41:02.016770: step 51280, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 64h:52m:15s remains)
INFO - root - 2017-12-05 22:41:10.505040: step 51290, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 65h:20m:38s remains)
INFO - root - 2017-12-05 22:41:18.920363: step 51300, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.802 sec/batch; 62h:36m:26s remains)
2017-12-05 22:41:19.693499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3233328 -4.3255625 -4.3273573 -4.3296623 -4.3308415 -4.3318572 -4.331965 -4.33214 -4.3313656 -4.3306131 -4.3289204 -4.32755 -4.3275275 -4.3276315 -4.3265543][-4.3232965 -4.3258171 -4.3287954 -4.3307662 -4.329535 -4.3290486 -4.3274341 -4.3260837 -4.3238239 -4.3220396 -4.3176079 -4.3158336 -4.3173738 -4.3190613 -4.317143][-4.30051 -4.3026662 -4.3084588 -4.3135486 -4.3146024 -4.315906 -4.3116355 -4.3074365 -4.3030381 -4.2999196 -4.2918177 -4.2897229 -4.2931972 -4.2981782 -4.2977219][-4.2584929 -4.2598782 -4.26752 -4.2726088 -4.2773733 -4.2842612 -4.2794013 -4.272378 -4.26979 -4.2681789 -4.258976 -4.2592554 -4.2674956 -4.27298 -4.2708983][-4.2089391 -4.2115355 -4.2196207 -4.220552 -4.2241945 -4.2310715 -4.2211742 -4.2111835 -4.2131114 -4.2175078 -4.2129235 -4.2201881 -4.2367644 -4.241292 -4.2338123][-4.1657825 -4.1719995 -4.178061 -4.1713715 -4.1729302 -4.1766725 -4.1639957 -4.1549516 -4.1608853 -4.1671042 -4.16594 -4.1781244 -4.1979156 -4.1987977 -4.1910963][-4.119782 -4.1263723 -4.1295137 -4.1178665 -4.1191044 -4.1244378 -4.11216 -4.1056385 -4.1135454 -4.116662 -4.1171088 -4.1353431 -4.1502461 -4.144906 -4.1399813][-4.0951076 -4.1017609 -4.1044135 -4.0929222 -4.0933795 -4.0954828 -4.0845671 -4.0795455 -4.0877762 -4.0876608 -4.08794 -4.1007838 -4.1042433 -4.0992632 -4.1009946][-4.1323295 -4.1406236 -4.1450686 -4.1416626 -4.1414294 -4.1406174 -4.1296597 -4.1173086 -4.1154208 -4.1069217 -4.1060576 -4.1111269 -4.1090059 -4.1139445 -4.1228414][-4.1858258 -4.1989708 -4.2061725 -4.2062893 -4.2028842 -4.1960731 -4.1816788 -4.1622014 -4.1471519 -4.1345029 -4.1352262 -4.1354322 -4.1309719 -4.145092 -4.1653838][-4.2184987 -4.2372136 -4.2497015 -4.2522326 -4.2446837 -4.2291589 -4.2088094 -4.187429 -4.1658621 -4.1481476 -4.1455154 -4.1448708 -4.1432233 -4.161984 -4.1873975][-4.23695 -4.2489238 -4.2574449 -4.2610903 -4.2509031 -4.2353868 -4.2177253 -4.2006674 -4.1836052 -4.1710615 -4.1666222 -4.1697459 -4.1746726 -4.1872463 -4.2020597][-4.2396441 -4.2415929 -4.2429762 -4.2440004 -4.2329159 -4.2204714 -4.2066917 -4.1905103 -4.175745 -4.1719222 -4.176775 -4.1914406 -4.2056608 -4.2137976 -4.2173467][-4.2250657 -4.2217021 -4.218761 -4.2200317 -4.2159562 -4.20777 -4.1943588 -4.1739244 -4.1607475 -4.1656432 -4.1854525 -4.2119856 -4.2359004 -4.243278 -4.2412033][-4.2011962 -4.1925607 -4.1840987 -4.18952 -4.1961832 -4.1943908 -4.1847377 -4.1666155 -4.1574473 -4.1701083 -4.1989369 -4.2308106 -4.2563419 -4.2640219 -4.260004]]...]
INFO - root - 2017-12-05 22:41:28.173766: step 51310, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 66h:00m:05s remains)
INFO - root - 2017-12-05 22:41:36.811136: step 51320, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 67h:19m:01s remains)
INFO - root - 2017-12-05 22:41:45.370265: step 51330, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 66h:13m:36s remains)
INFO - root - 2017-12-05 22:41:53.989627: step 51340, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 69h:58m:27s remains)
INFO - root - 2017-12-05 22:42:02.589883: step 51350, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 67h:42m:26s remains)
INFO - root - 2017-12-05 22:42:10.931315: step 51360, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 65h:19m:38s remains)
INFO - root - 2017-12-05 22:42:19.434059: step 51370, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 65h:10m:46s remains)
INFO - root - 2017-12-05 22:42:28.055281: step 51380, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 67h:33m:13s remains)
INFO - root - 2017-12-05 22:42:36.599722: step 51390, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.821 sec/batch; 64h:04m:54s remains)
INFO - root - 2017-12-05 22:42:45.095253: step 51400, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 66h:17m:46s remains)
2017-12-05 22:42:45.857568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3074317 -4.3003387 -4.2954721 -4.2933583 -4.2924809 -4.2939072 -4.2948079 -4.2948074 -4.2949619 -4.2968645 -4.3007808 -4.3033876 -4.306632 -4.3102808 -4.3140016][-4.2907395 -4.2819762 -4.2774448 -4.2769532 -4.2768607 -4.2772894 -4.2750387 -4.2718077 -4.2714577 -4.276114 -4.2848711 -4.2912755 -4.29789 -4.3040957 -4.3095522][-4.2587714 -4.2465754 -4.2423253 -4.2436829 -4.2439308 -4.2409472 -4.232028 -4.2228756 -4.2216897 -4.2330785 -4.2533665 -4.2682309 -4.2806296 -4.2927365 -4.3028893][-4.2144175 -4.1952419 -4.1878958 -4.187912 -4.1874251 -4.1806669 -4.1662712 -4.152699 -4.1525927 -4.1732917 -4.2068319 -4.2316966 -4.2507229 -4.2701306 -4.2884011][-4.1702542 -4.1447825 -4.1339879 -4.1299667 -4.1265426 -4.1155319 -4.0949626 -4.0772839 -4.0816879 -4.1161189 -4.1640944 -4.1975775 -4.2213411 -4.24677 -4.2732716][-4.1460609 -4.116075 -4.1007318 -4.0898046 -4.0792041 -4.0614524 -4.0314932 -4.0106716 -4.024344 -4.0739884 -4.1333771 -4.1749682 -4.2025251 -4.2312903 -4.2631021][-4.1531959 -4.1227179 -4.1067886 -4.0947175 -4.0788274 -4.0542722 -4.0186472 -3.9964716 -4.0129609 -4.0673566 -4.128798 -4.172544 -4.2004609 -4.2284102 -4.2594795][-4.176146 -4.1502781 -4.1410742 -4.1358929 -4.1242275 -4.1010737 -4.0638824 -4.0386105 -4.0495567 -4.0968308 -4.1503344 -4.1890831 -4.213954 -4.2373753 -4.262836][-4.1845508 -4.1643124 -4.1633091 -4.1671624 -4.1648397 -4.1476278 -4.1130195 -4.088747 -4.0980644 -4.1374097 -4.180469 -4.2127976 -4.2336588 -4.2513633 -4.2713227][-4.1884036 -4.1746969 -4.1791148 -4.1884212 -4.1912723 -4.1785822 -4.1510262 -4.1361871 -4.1490674 -4.1819158 -4.2169704 -4.2440629 -4.25935 -4.2705588 -4.2849264][-4.2040792 -4.1964164 -4.2032328 -4.21273 -4.2175894 -4.2081633 -4.1889749 -4.1845603 -4.2018752 -4.2314954 -4.2609906 -4.2827554 -4.29232 -4.2956605 -4.3015161][-4.2400084 -4.2377892 -4.2453279 -4.2554922 -4.2610331 -4.2545571 -4.2419243 -4.2428374 -4.2598124 -4.2836294 -4.3059168 -4.3207541 -4.3253627 -4.321629 -4.3188477][-4.2871079 -4.28901 -4.2968292 -4.3055668 -4.3099904 -4.3061972 -4.2985406 -4.3007164 -4.3140736 -4.3301229 -4.3433151 -4.3499694 -4.3486214 -4.3400812 -4.3319759][-4.3219051 -4.325891 -4.3322616 -4.3377514 -4.3409977 -4.33963 -4.3356314 -4.3367729 -4.3441157 -4.3523445 -4.3579435 -4.3586206 -4.3534136 -4.3443322 -4.3360538][-4.3355989 -4.3386869 -4.3422089 -4.3441982 -4.3449163 -4.3442507 -4.3422351 -4.3418217 -4.3442006 -4.34679 -4.348 -4.34678 -4.3425903 -4.3371387 -4.332305]]...]
INFO - root - 2017-12-05 22:42:54.421127: step 51410, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 67h:06m:02s remains)
INFO - root - 2017-12-05 22:43:02.912157: step 51420, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 66h:48m:47s remains)
INFO - root - 2017-12-05 22:43:11.523237: step 51430, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 67h:04m:29s remains)
INFO - root - 2017-12-05 22:43:20.023926: step 51440, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 67h:04m:23s remains)
INFO - root - 2017-12-05 22:43:28.580455: step 51450, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 66h:26m:44s remains)
INFO - root - 2017-12-05 22:43:36.981910: step 51460, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.829 sec/batch; 64h:44m:52s remains)
INFO - root - 2017-12-05 22:43:45.286185: step 51470, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 67h:51m:31s remains)
INFO - root - 2017-12-05 22:43:53.780000: step 51480, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 66h:39m:47s remains)
INFO - root - 2017-12-05 22:44:02.295005: step 51490, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 65h:49m:17s remains)
INFO - root - 2017-12-05 22:44:10.847757: step 51500, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 65h:37m:35s remains)
2017-12-05 22:44:11.615263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2642055 -4.2701397 -4.2673106 -4.2643609 -4.2670913 -4.27033 -4.268693 -4.266768 -4.2669234 -4.2683716 -4.2687607 -4.2689166 -4.2689261 -4.2682481 -4.263855][-4.2505088 -4.2548757 -4.2488642 -4.2418127 -4.2459702 -4.2539835 -4.2544093 -4.2541056 -4.2591 -4.2656465 -4.2665353 -4.2648463 -4.2622976 -4.2567163 -4.2449775][-4.2199564 -4.2221241 -4.2171807 -4.2091346 -4.2111316 -4.2203488 -4.2205029 -4.220324 -4.2299237 -4.2422051 -4.2457123 -4.2419353 -4.2345867 -4.2193108 -4.1928749][-4.1962485 -4.1974998 -4.1960835 -4.1903338 -4.1907392 -4.1974573 -4.1911364 -4.1853805 -4.2019076 -4.2252622 -4.2361832 -4.231596 -4.2176971 -4.1880069 -4.1388078][-4.1682067 -4.1729789 -4.1761303 -4.1685677 -4.1582556 -4.1510038 -4.1324787 -4.1169872 -4.1467819 -4.1928391 -4.2217093 -4.2266612 -4.2153831 -4.1791105 -4.1112123][-4.1393337 -4.1486549 -4.1493616 -4.1349859 -4.10904 -4.0777068 -4.0239429 -3.9784765 -4.0244718 -4.1060119 -4.1640487 -4.1932483 -4.1993151 -4.1720729 -4.1048841][-4.1208487 -4.1281385 -4.122797 -4.0973392 -4.0472088 -3.9749863 -3.8538208 -3.7576489 -3.8360884 -3.9732292 -4.0692587 -4.1287851 -4.1648779 -4.1597037 -4.1117945][-4.1424484 -4.1508784 -4.1506786 -4.1305776 -4.0758681 -3.9865682 -3.8403795 -3.7257791 -3.8110991 -3.9589703 -4.0573015 -4.1235638 -4.1767635 -4.191103 -4.1674337][-4.18518 -4.1974039 -4.2076578 -4.2036 -4.1639977 -4.092247 -3.9827909 -3.900378 -3.9531274 -4.0493164 -4.1078377 -4.1552291 -4.2043681 -4.2248721 -4.2177815][-4.1963921 -4.2161245 -4.2419291 -4.25279 -4.2326822 -4.1849551 -4.1128902 -4.0570221 -4.0771546 -4.1232352 -4.15192 -4.1856556 -4.2280445 -4.247654 -4.2454329][-4.190433 -4.2169852 -4.2515035 -4.2707205 -4.2656031 -4.241745 -4.1970992 -4.158824 -4.1586175 -4.1748228 -4.1847844 -4.2089338 -4.2443242 -4.2637415 -4.2613091][-4.1867132 -4.214715 -4.2500472 -4.2726712 -4.2771235 -4.2676849 -4.2449117 -4.2267246 -4.2241731 -4.2259336 -4.222424 -4.2340646 -4.2557745 -4.2633276 -4.2545137][-4.1944442 -4.2143679 -4.2400331 -4.259481 -4.27048 -4.2733 -4.2686477 -4.2679014 -4.2713118 -4.2713232 -4.2620482 -4.260922 -4.2655554 -4.2556186 -4.2381549][-4.2086635 -4.2189345 -4.2321138 -4.2449722 -4.2594533 -4.2697082 -4.2770939 -4.2880464 -4.2960625 -4.2976122 -4.2875791 -4.2803545 -4.2734685 -4.255394 -4.2359571][-4.2421231 -4.2423673 -4.238318 -4.2364049 -4.2447176 -4.254025 -4.2663817 -4.2838831 -4.3000779 -4.308238 -4.3041105 -4.2939262 -4.2804155 -4.2607803 -4.2393918]]...]
INFO - root - 2017-12-05 22:44:20.030199: step 51510, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.805 sec/batch; 62h:49m:56s remains)
INFO - root - 2017-12-05 22:44:28.491837: step 51520, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.837 sec/batch; 65h:20m:19s remains)
INFO - root - 2017-12-05 22:44:37.038238: step 51530, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 64h:45m:26s remains)
INFO - root - 2017-12-05 22:44:45.530800: step 51540, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 64h:36m:20s remains)
INFO - root - 2017-12-05 22:44:54.083303: step 51550, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 66h:45m:33s remains)
INFO - root - 2017-12-05 22:45:02.496757: step 51560, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 69h:09m:40s remains)
INFO - root - 2017-12-05 22:45:10.992334: step 51570, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 67h:02m:48s remains)
INFO - root - 2017-12-05 22:45:19.536342: step 51580, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 67h:49m:20s remains)
INFO - root - 2017-12-05 22:45:27.971701: step 51590, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 67h:25m:21s remains)
INFO - root - 2017-12-05 22:45:36.468741: step 51600, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.880 sec/batch; 68h:41m:56s remains)
2017-12-05 22:45:37.242670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2485523 -4.2399116 -4.2217579 -4.2013822 -4.1814713 -4.1692815 -4.1706586 -4.1833467 -4.1996326 -4.2161765 -4.2285557 -4.2389264 -4.2456322 -4.2501903 -4.2551107][-4.2573018 -4.252707 -4.240561 -4.2287569 -4.21568 -4.2056284 -4.2070022 -4.2179041 -4.2284918 -4.2354622 -4.2373891 -4.2367373 -4.2333918 -4.2324018 -4.2362771][-4.2571268 -4.2571683 -4.249507 -4.2391348 -4.2266212 -4.2158175 -4.2143087 -4.2235212 -4.2338738 -4.2389412 -4.2380338 -4.2320175 -4.2231865 -4.2186508 -4.2185836][-4.2382216 -4.2450185 -4.2411709 -4.22926 -4.2135081 -4.199502 -4.1969695 -4.2075872 -4.2220082 -4.2314682 -4.2345705 -4.2305655 -4.2222729 -4.216527 -4.2127829][-4.2002683 -4.2146349 -4.2138777 -4.197063 -4.175879 -4.162972 -4.1664124 -4.1830564 -4.2024016 -4.2150412 -4.2206969 -4.2173567 -4.2108822 -4.2090058 -4.2088294][-4.1652522 -4.1780372 -4.173048 -4.149025 -4.1251698 -4.1173706 -4.12947 -4.1551518 -4.1820312 -4.1992245 -4.203773 -4.1976857 -4.1907649 -4.1892252 -4.1918344][-4.1420646 -4.1479511 -4.1336756 -4.1016741 -4.0746207 -4.0691609 -4.0846286 -4.1159472 -4.1541181 -4.1826458 -4.1923151 -4.1852822 -4.1760435 -4.1736856 -4.1747274][-4.127758 -4.1307726 -4.1141186 -4.0789118 -4.046627 -4.0306668 -4.029654 -4.0570316 -4.1075368 -4.1546316 -4.1774087 -4.1734343 -4.1634359 -4.1629667 -4.1652513][-4.1283922 -4.1332488 -4.1215224 -4.0894451 -4.0494432 -4.0107875 -3.9759283 -3.9879889 -4.052238 -4.1156693 -4.14803 -4.1506872 -4.1423578 -4.1429319 -4.1497407][-4.1338925 -4.1411667 -4.1332703 -4.1072345 -4.0654855 -4.0066772 -3.9366584 -3.9252448 -3.9987338 -4.0741181 -4.11562 -4.1308408 -4.1285419 -4.123126 -4.1251411][-4.1226025 -4.134079 -4.1304307 -4.1127367 -4.0805254 -4.0240664 -3.9466128 -3.9112747 -3.9637949 -4.033927 -4.0839524 -4.1169338 -4.1261892 -4.11437 -4.1046772][-4.0974646 -4.1195316 -4.1248446 -4.1187534 -4.1007886 -4.0629215 -4.0006042 -3.949785 -3.9596324 -4.0059071 -4.0583024 -4.1042075 -4.1253266 -4.1146016 -4.0968351][-4.0712457 -4.1037173 -4.1198635 -4.12543 -4.1164713 -4.0909019 -4.0457435 -3.9956388 -3.9759865 -3.9972615 -4.0428925 -4.0946078 -4.1297274 -4.1288948 -4.1110129][-4.0552998 -4.083457 -4.1036758 -4.1202707 -4.1209526 -4.0982232 -4.0662527 -4.0311422 -4.0037374 -4.00627 -4.0386944 -4.0910497 -4.1377873 -4.1500587 -4.1351228][-4.04575 -4.060513 -4.0821581 -4.10859 -4.1189246 -4.1037846 -4.0847464 -4.0661182 -4.0434165 -4.0371628 -4.0512133 -4.0919771 -4.1380935 -4.1575785 -4.1440048]]...]
INFO - root - 2017-12-05 22:45:45.866725: step 51610, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 67h:18m:26s remains)
INFO - root - 2017-12-05 22:45:54.400377: step 51620, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 67h:35m:52s remains)
INFO - root - 2017-12-05 22:46:02.936761: step 51630, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.830 sec/batch; 64h:45m:29s remains)
INFO - root - 2017-12-05 22:46:11.514700: step 51640, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 66h:18m:48s remains)
INFO - root - 2017-12-05 22:46:20.009019: step 51650, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 67h:44m:07s remains)
INFO - root - 2017-12-05 22:46:28.403755: step 51660, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 65h:45m:46s remains)
INFO - root - 2017-12-05 22:46:36.900356: step 51670, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 66h:49m:25s remains)
INFO - root - 2017-12-05 22:46:45.480359: step 51680, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 66h:47m:26s remains)
INFO - root - 2017-12-05 22:46:53.822194: step 51690, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 67h:57m:33s remains)
INFO - root - 2017-12-05 22:47:02.313812: step 51700, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 66h:56m:42s remains)
2017-12-05 22:47:03.167315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1891985 -4.1840777 -4.1904159 -4.2131538 -4.2345729 -4.2573066 -4.2775316 -4.2916746 -4.299047 -4.29799 -4.2939587 -4.2911973 -4.2893672 -4.2882051 -4.2845888][-4.1699009 -4.1627889 -4.1668658 -4.1893234 -4.2132068 -4.2406497 -4.2655582 -4.2865891 -4.2975903 -4.2997789 -4.29903 -4.2984195 -4.2968073 -4.2942362 -4.2891889][-4.1819215 -4.1748104 -4.1781421 -4.1953812 -4.2138095 -4.2345762 -4.2546191 -4.2744555 -4.2879472 -4.2975292 -4.3028331 -4.3070083 -4.3066635 -4.302938 -4.2962613][-4.2060103 -4.1999769 -4.1993308 -4.206861 -4.2130742 -4.218843 -4.2274842 -4.2413673 -4.2537222 -4.2685771 -4.2814107 -4.2959991 -4.3051744 -4.307714 -4.3043976][-4.2158437 -4.2069178 -4.1972289 -4.1923094 -4.1873579 -4.1791148 -4.1738105 -4.178092 -4.1897135 -4.2126331 -4.2354984 -4.2627687 -4.2864332 -4.3021 -4.3086686][-4.1977339 -4.1827669 -4.1586633 -4.1390476 -4.1188297 -4.0918016 -4.0699191 -4.0670962 -4.0858421 -4.1254926 -4.1615791 -4.199636 -4.23976 -4.2711549 -4.2915325][-4.1592164 -4.1302996 -4.0843 -4.0418844 -3.9967456 -3.9385695 -3.883363 -3.865912 -3.9007645 -3.9671433 -4.0217419 -4.0811348 -4.1480708 -4.2041283 -4.246562][-4.1075926 -4.0673571 -4.0059509 -3.9467268 -3.8837128 -3.8003128 -3.702342 -3.6566398 -3.7043042 -3.7911801 -3.8612258 -3.9391205 -4.0302157 -4.1151948 -4.1853218][-4.0804615 -4.0422745 -3.991456 -3.9433095 -3.8909745 -3.8209956 -3.7321947 -3.6704788 -3.6814384 -3.7358985 -3.7859321 -3.8547554 -3.9500468 -4.0482621 -4.132833][-4.1122317 -4.091105 -4.0664625 -4.0483775 -4.0301762 -3.9995451 -3.9474194 -3.8890874 -3.8543277 -3.8488367 -3.850317 -3.8780379 -3.9465375 -4.027843 -4.10123][-4.1846204 -4.180059 -4.1763334 -4.1795349 -4.1842065 -4.1790776 -4.1559095 -4.11814 -4.0832577 -4.0537658 -4.0241375 -4.01178 -4.0338187 -4.0742197 -4.118454][-4.2508488 -4.2510362 -4.2532845 -4.2620087 -4.2733531 -4.2792816 -4.2740073 -4.2601981 -4.2458086 -4.2227893 -4.191937 -4.1663141 -4.1586056 -4.1654367 -4.1801796][-4.2882118 -4.286727 -4.2860031 -4.2912674 -4.2986403 -4.3031783 -4.3006845 -4.2991548 -4.302557 -4.300868 -4.2897081 -4.2729621 -4.2574968 -4.2467079 -4.2422523][-4.3019361 -4.2927713 -4.282558 -4.2766485 -4.2734261 -4.2718863 -4.2671738 -4.2704983 -4.2878881 -4.3075991 -4.3207068 -4.321795 -4.3118229 -4.2950006 -4.278409][-4.293354 -4.2695127 -4.24509 -4.2255583 -4.212141 -4.2047153 -4.1957607 -4.1991587 -4.2264361 -4.2637224 -4.2986522 -4.3214903 -4.3263006 -4.3115659 -4.2865171]]...]
INFO - root - 2017-12-05 22:47:11.656993: step 51710, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 65h:04m:51s remains)
INFO - root - 2017-12-05 22:47:20.130402: step 51720, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.828 sec/batch; 64h:33m:29s remains)
INFO - root - 2017-12-05 22:47:28.588736: step 51730, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 66h:24m:40s remains)
INFO - root - 2017-12-05 22:47:37.142267: step 51740, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 65h:40m:19s remains)
INFO - root - 2017-12-05 22:47:45.711412: step 51750, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 65h:50m:53s remains)
INFO - root - 2017-12-05 22:47:54.142777: step 51760, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 66h:00m:05s remains)
INFO - root - 2017-12-05 22:48:02.575187: step 51770, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 64h:17m:44s remains)
INFO - root - 2017-12-05 22:48:11.332662: step 51780, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 67h:34m:34s remains)
INFO - root - 2017-12-05 22:48:19.834386: step 51790, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 69h:32m:30s remains)
INFO - root - 2017-12-05 22:48:28.162251: step 51800, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 65h:24m:46s remains)
2017-12-05 22:48:28.917235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.314836 -4.3122134 -4.3057885 -4.2939491 -4.278934 -4.2603226 -4.2392015 -4.2329764 -4.2424126 -4.2579226 -4.272089 -4.2688274 -4.2530994 -4.2461176 -4.2420888][-4.2978606 -4.2941537 -4.2862849 -4.2693815 -4.24662 -4.2196069 -4.1907387 -4.1834249 -4.1952915 -4.2128763 -4.2284408 -4.2314224 -4.2285061 -4.2376342 -4.2510967][-4.2739134 -4.2736263 -4.2694783 -4.2528362 -4.2250967 -4.1886492 -4.1495843 -4.13746 -4.1468 -4.16021 -4.1721969 -4.1789284 -4.1889172 -4.2142181 -4.242816][-4.2332821 -4.2309427 -4.2301097 -4.2232366 -4.2040806 -4.1667047 -4.12197 -4.104991 -4.1084638 -4.1157427 -4.1209383 -4.1255603 -4.1407146 -4.1724172 -4.209188][-4.1921563 -4.1792741 -4.1734591 -4.1726117 -4.1675735 -4.1435905 -4.1109781 -4.0948443 -4.0932703 -4.0918112 -4.0823541 -4.0754423 -4.0865183 -4.1211786 -4.1654229][-4.1778593 -4.1555166 -4.13613 -4.1282554 -4.1320958 -4.1270618 -4.1150918 -4.1057982 -4.0989294 -4.0895743 -4.0714779 -4.0534172 -4.051775 -4.0803957 -4.1271629][-4.1857071 -4.1653428 -4.1367822 -4.1178718 -4.1162052 -4.1221862 -4.1298862 -4.1276469 -4.1197195 -4.1081982 -4.0887184 -4.0660391 -4.0567112 -4.0816207 -4.1275039][-4.1877866 -4.1792331 -4.154562 -4.1300149 -4.1167326 -4.1182961 -4.1321163 -4.1378903 -4.1353273 -4.1273212 -4.1086912 -4.0818934 -4.0707922 -4.09776 -4.1450329][-4.1838512 -4.1946259 -4.1892953 -4.1751547 -4.158926 -4.1483688 -4.1509376 -4.157465 -4.1604176 -4.1580372 -4.1410084 -4.1096754 -4.0912313 -4.1120176 -4.1525159][-4.1685433 -4.1939549 -4.2053146 -4.206954 -4.2018428 -4.1915054 -4.1847992 -4.1841965 -4.1898952 -4.1951365 -4.1832838 -4.1509681 -4.1246562 -4.1338367 -4.1616993][-4.1493497 -4.1776304 -4.1940222 -4.2006812 -4.2060146 -4.2067227 -4.2021036 -4.2022829 -4.2120695 -4.2254553 -4.2219048 -4.1941743 -4.1663337 -4.1666656 -4.1808181][-4.1514926 -4.1689048 -4.17854 -4.1848235 -4.196147 -4.2048192 -4.2032418 -4.2024159 -4.2151771 -4.234005 -4.2400694 -4.2244549 -4.2061253 -4.205945 -4.2129169][-4.1647391 -4.1742182 -4.1753426 -4.1770391 -4.1848006 -4.1916342 -4.1913729 -4.1927791 -4.207109 -4.2293911 -4.2427797 -4.2389951 -4.228415 -4.2321315 -4.24181][-4.1720362 -4.1811895 -4.1783438 -4.1738148 -4.1720243 -4.1706252 -4.1726327 -4.1816411 -4.1990137 -4.2191553 -4.2324986 -4.2365751 -4.2337532 -4.2425628 -4.2563043][-4.1703892 -4.1798482 -4.1750331 -4.1641273 -4.1536751 -4.1447506 -4.1496887 -4.1680427 -4.1898179 -4.2040448 -4.2119923 -4.2127643 -4.2114758 -4.2289653 -4.2525082]]...]
INFO - root - 2017-12-05 22:48:37.615275: step 51810, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 66h:48m:09s remains)
INFO - root - 2017-12-05 22:48:46.096000: step 51820, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 66h:03m:58s remains)
INFO - root - 2017-12-05 22:48:54.690788: step 51830, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 66h:59m:25s remains)
INFO - root - 2017-12-05 22:49:03.127047: step 51840, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 66h:44m:47s remains)
INFO - root - 2017-12-05 22:49:11.593732: step 51850, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 65h:14m:50s remains)
INFO - root - 2017-12-05 22:49:20.059507: step 51860, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 67h:45m:55s remains)
INFO - root - 2017-12-05 22:49:28.582265: step 51870, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 67h:29m:44s remains)
INFO - root - 2017-12-05 22:49:37.122106: step 51880, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 66h:01m:08s remains)
INFO - root - 2017-12-05 22:49:45.603800: step 51890, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 66h:36m:25s remains)
INFO - root - 2017-12-05 22:49:54.058641: step 51900, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.822 sec/batch; 64h:03m:33s remains)
2017-12-05 22:49:54.926717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1675534 -4.1774635 -4.1873131 -4.2019925 -4.220417 -4.2374258 -4.2401032 -4.2341571 -4.2381668 -4.2471886 -4.2513413 -4.2532463 -4.2548532 -4.2691693 -4.2766328][-4.1451631 -4.1538358 -4.1632113 -4.1767144 -4.192924 -4.2096262 -4.2146997 -4.2117653 -4.2195787 -4.2358775 -4.2497997 -4.2577376 -4.2588654 -4.2746077 -4.2809362][-4.14771 -4.1510353 -4.1553955 -4.1624522 -4.170557 -4.1797886 -4.1841235 -4.1873064 -4.2011361 -4.2263312 -4.2496352 -4.2663956 -4.2705431 -4.285501 -4.289557][-4.1575594 -4.1532574 -4.1516495 -4.1529679 -4.1526952 -4.1554551 -4.1555052 -4.1623683 -4.1876168 -4.22051 -4.2507706 -4.2715087 -4.2763157 -4.2891273 -4.2927403][-4.1430764 -4.1353917 -4.1280422 -4.1217542 -4.11739 -4.1172466 -4.10969 -4.1144505 -4.1491842 -4.190763 -4.2279758 -4.2519417 -4.2626915 -4.2779961 -4.2856503][-4.0766878 -4.0692739 -4.0624313 -4.0515141 -4.0405431 -4.02999 -4.0117846 -4.0136304 -4.0552649 -4.1139317 -4.1657939 -4.2024107 -4.2292266 -4.2565641 -4.2742219][-3.9625592 -3.954809 -3.9548695 -3.9478171 -3.9350884 -3.9145298 -3.8833981 -3.8815441 -3.9400909 -4.0244665 -4.1024289 -4.1617112 -4.207335 -4.2475991 -4.2714481][-3.8960679 -3.8861942 -3.8901646 -3.8855424 -3.8685398 -3.8342967 -3.7899861 -3.784883 -3.8585892 -3.9590504 -4.0579643 -4.1350374 -4.1948824 -4.2463875 -4.2745619][-3.9743719 -3.9605827 -3.9516153 -3.9362214 -3.9073234 -3.8606753 -3.807884 -3.7934103 -3.8588226 -3.9454153 -4.0370712 -4.1102352 -4.1695037 -4.2276959 -4.260591][-4.0889039 -4.078805 -4.0628748 -4.0452337 -4.0220947 -3.9805667 -3.9354103 -3.9186673 -3.9589221 -4.0136375 -4.0738549 -4.1236324 -4.1638765 -4.2132874 -4.2447824][-4.1765571 -4.1710458 -4.1555095 -4.1411228 -4.1229935 -4.0863814 -4.0530939 -4.0397415 -4.0637779 -4.0980654 -4.1343946 -4.162219 -4.1866627 -4.2212706 -4.24294][-4.2276039 -4.22564 -4.2151089 -4.2068257 -4.1945076 -4.1691356 -4.1466794 -4.1380062 -4.1505294 -4.1710353 -4.191278 -4.2040391 -4.2153416 -4.2388816 -4.2492337][-4.255343 -4.2570634 -4.2530217 -4.2502656 -4.2434816 -4.2283521 -4.21425 -4.2094288 -4.2167797 -4.2313957 -4.2442756 -4.2496738 -4.2520752 -4.2674069 -4.2684326][-4.2681341 -4.2699533 -4.2703338 -4.269125 -4.2667785 -4.2606564 -4.2539034 -4.2540412 -4.2641811 -4.2782955 -4.2927604 -4.2988524 -4.2981763 -4.3049283 -4.2985015][-4.2595654 -4.2592473 -4.260941 -4.2609887 -4.2622633 -4.2617126 -4.2588587 -4.2642937 -4.2797508 -4.295444 -4.3100142 -4.3181233 -4.3197269 -4.3241563 -4.3174167]]...]
INFO - root - 2017-12-05 22:50:03.242565: step 51910, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 66h:52m:25s remains)
INFO - root - 2017-12-05 22:50:11.604411: step 51920, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 65h:50m:52s remains)
INFO - root - 2017-12-05 22:50:20.145790: step 51930, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 65h:45m:12s remains)
INFO - root - 2017-12-05 22:50:28.685006: step 51940, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 67h:11m:36s remains)
INFO - root - 2017-12-05 22:50:37.210800: step 51950, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 67h:33m:02s remains)
INFO - root - 2017-12-05 22:50:45.660700: step 51960, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 67h:23m:33s remains)
INFO - root - 2017-12-05 22:50:54.155779: step 51970, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 69h:10m:26s remains)
INFO - root - 2017-12-05 22:51:02.709833: step 51980, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 65h:49m:12s remains)
INFO - root - 2017-12-05 22:51:11.328710: step 51990, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 67h:17m:27s remains)
INFO - root - 2017-12-05 22:51:19.796604: step 52000, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 66h:28m:48s remains)
2017-12-05 22:51:20.646065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3252597 -4.327271 -4.3314424 -4.33707 -4.3396344 -4.3343525 -4.3180318 -4.2900581 -4.2571559 -4.234025 -4.2348905 -4.2628589 -4.2966986 -4.3148561 -4.3142219][-4.3106217 -4.3124943 -4.3164086 -4.322125 -4.3242488 -4.3179812 -4.3004603 -4.2688036 -4.229651 -4.2019787 -4.2054386 -4.2442837 -4.2916226 -4.3188429 -4.3204622][-4.2957473 -4.2977324 -4.2996626 -4.3015194 -4.2977304 -4.2861753 -4.2627192 -4.2245169 -4.1772656 -4.1440578 -4.1529636 -4.2070746 -4.272769 -4.3127723 -4.320374][-4.28783 -4.2890844 -4.2869496 -4.2809763 -4.2685218 -4.247612 -4.2135839 -4.1607337 -4.0968 -4.056416 -4.0764565 -4.1532936 -4.2426581 -4.299418 -4.3175464][-4.2886786 -4.2883024 -4.2813978 -4.2652507 -4.2399559 -4.2049541 -4.1551166 -4.0816689 -3.9961545 -3.9528794 -3.9948676 -4.1020265 -4.2131491 -4.2851529 -4.3127894][-4.2952342 -4.29328 -4.2822027 -4.2574019 -4.2200603 -4.1691241 -4.1037984 -4.0134258 -3.9132333 -3.875767 -3.9448569 -4.077476 -4.2016025 -4.2798314 -4.3120251][-4.3033719 -4.3019671 -4.29003 -4.2621064 -4.2184758 -4.1578546 -4.0850425 -3.9874625 -3.8849514 -3.8604548 -3.945909 -4.0847731 -4.2097292 -4.2861633 -4.3198428][-4.3073516 -4.3072424 -4.2952161 -4.2658625 -4.2207661 -4.1604605 -4.0899997 -3.9979105 -3.9069955 -3.8979192 -3.9833317 -4.1113448 -4.2267694 -4.2963605 -4.3301978][-4.3066487 -4.30738 -4.29532 -4.2653661 -4.221415 -4.16676 -4.1066647 -4.0326581 -3.9678452 -3.9740109 -4.0494261 -4.1570492 -4.253181 -4.3109274 -4.3399777][-4.3016276 -4.3037286 -4.2936115 -4.2664175 -4.2279806 -4.1815786 -4.1367221 -4.0895529 -4.0561972 -4.0738487 -4.1335697 -4.2140603 -4.2852936 -4.32734 -4.34666][-4.2915053 -4.2931566 -4.2846713 -4.2613435 -4.2300081 -4.1965218 -4.1724052 -4.1541338 -4.1474528 -4.169652 -4.2116675 -4.2639813 -4.3108983 -4.3365655 -4.3447771][-4.28037 -4.2793956 -4.2712746 -4.2525268 -4.229198 -4.21173 -4.20984 -4.21609 -4.2270117 -4.2474012 -4.2725377 -4.3015747 -4.32778 -4.3377867 -4.3355975][-4.2676253 -4.2631903 -4.2529564 -4.2357011 -4.2194819 -4.2194781 -4.2401686 -4.2657323 -4.2867484 -4.302042 -4.3128171 -4.3230562 -4.3305783 -4.3278193 -4.3193922][-4.2543178 -4.2457805 -4.2314839 -4.2124515 -4.2018342 -4.2171593 -4.2549138 -4.2933164 -4.3188496 -4.3290534 -4.32941 -4.3260179 -4.3202105 -4.3090148 -4.2979355][-4.2423549 -4.2319441 -4.2162614 -4.1980357 -4.1926527 -4.2153187 -4.2572207 -4.29747 -4.3211274 -4.3277073 -4.3228183 -4.31274 -4.3004589 -4.2863269 -4.2763767]]...]
INFO - root - 2017-12-05 22:51:29.001559: step 52010, loss = 2.05, batch loss = 2.00 (10.0 examples/sec; 0.799 sec/batch; 62h:14m:40s remains)
INFO - root - 2017-12-05 22:51:37.520776: step 52020, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 65h:57m:02s remains)
INFO - root - 2017-12-05 22:51:46.175586: step 52030, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.878 sec/batch; 68h:24m:00s remains)
INFO - root - 2017-12-05 22:51:54.690489: step 52040, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 67h:30m:26s remains)
INFO - root - 2017-12-05 22:52:03.206308: step 52050, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.859 sec/batch; 66h:57m:12s remains)
INFO - root - 2017-12-05 22:52:11.618522: step 52060, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 67h:19m:46s remains)
INFO - root - 2017-12-05 22:52:20.197406: step 52070, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 66h:14m:04s remains)
INFO - root - 2017-12-05 22:52:28.600800: step 52080, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 64h:50m:45s remains)
INFO - root - 2017-12-05 22:52:37.152965: step 52090, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 66h:32m:46s remains)
INFO - root - 2017-12-05 22:52:45.655719: step 52100, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 67h:44m:11s remains)
2017-12-05 22:52:46.386040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1660986 -4.2069969 -4.2342448 -4.2505932 -4.2514391 -4.2426677 -4.2324681 -4.234118 -4.2563124 -4.2828989 -4.2948446 -4.2910995 -4.2787023 -4.2718515 -4.2732086][-4.1125097 -4.1678753 -4.2063313 -4.2329493 -4.230567 -4.2100596 -4.1869316 -4.1881957 -4.2206025 -4.2585669 -4.2746053 -4.2704887 -4.2528849 -4.2463851 -4.2564182][-4.0738182 -4.1413908 -4.1878872 -4.2195215 -4.2114873 -4.1731219 -4.128417 -4.121222 -4.1628366 -4.217401 -4.2437763 -4.2383919 -4.2135677 -4.2073255 -4.22895][-4.0505533 -4.12366 -4.175271 -4.2049589 -4.1889038 -4.1328249 -4.0651779 -4.0426459 -4.096004 -4.1732016 -4.2143393 -4.2039251 -4.1680937 -4.1589003 -4.1933293][-4.0627608 -4.1325164 -4.1807108 -4.1984968 -4.1704965 -4.0929632 -3.9949427 -3.942739 -4.0075026 -4.1192513 -4.184238 -4.1744175 -4.1307087 -4.120172 -4.1652503][-4.1307263 -4.184042 -4.2158422 -4.2132974 -4.1708159 -4.0656295 -3.9238608 -3.8240819 -3.8973751 -4.0553422 -4.1527419 -4.1533766 -4.11324 -4.1029959 -4.1506505][-4.2078795 -4.2446823 -4.2648044 -4.2516465 -4.1980257 -4.0717058 -3.88819 -3.7279642 -3.7873583 -3.982713 -4.1150942 -4.14147 -4.1198044 -4.1124492 -4.1547871][-4.252758 -4.2771621 -4.293859 -4.2855325 -4.2351742 -4.110065 -3.9150867 -3.7224917 -3.7448626 -3.9390104 -4.0891075 -4.1440177 -4.150876 -4.1531496 -4.1845064][-4.2445517 -4.2675004 -4.288764 -4.2934031 -4.2593503 -4.1643009 -4.00495 -3.8384516 -3.82478 -3.9707937 -4.1056328 -4.1725655 -4.2012453 -4.2131667 -4.2334194][-4.2115297 -4.235642 -4.2628603 -4.2804012 -4.2652059 -4.2026858 -4.0905113 -3.9663224 -3.9381907 -4.0393238 -4.1503167 -4.2178621 -4.2546659 -4.27339 -4.2849946][-4.1944551 -4.2162805 -4.2441821 -4.26901 -4.2700434 -4.23598 -4.1648927 -4.08045 -4.0548811 -4.1227255 -4.2041373 -4.2611141 -4.2973356 -4.3185673 -4.3260031][-4.199666 -4.2186832 -4.2435312 -4.2696571 -4.2820654 -4.2738934 -4.2390375 -4.1871729 -4.1656656 -4.2040319 -4.2578187 -4.298635 -4.3266988 -4.34402 -4.3472142][-4.2239385 -4.2424355 -4.2631435 -4.2852516 -4.3008027 -4.3048081 -4.2910738 -4.2607822 -4.2431421 -4.2586432 -4.2900562 -4.3184781 -4.3388543 -4.3502817 -4.350595][-4.2538738 -4.274673 -4.291882 -4.3071685 -4.3194594 -4.3237534 -4.3176775 -4.3016949 -4.29019 -4.2954717 -4.3110027 -4.3264027 -4.3382835 -4.3448739 -4.3451357][-4.2847409 -4.3018069 -4.3131795 -4.3213019 -4.3297415 -4.3329587 -4.3300858 -4.3231158 -4.317749 -4.3195376 -4.3257222 -4.3316612 -4.3364773 -4.3395166 -4.3398333]]...]
INFO - root - 2017-12-05 22:52:54.882443: step 52110, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 67h:20m:36s remains)
INFO - root - 2017-12-05 22:53:03.054393: step 52120, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 53h:22m:26s remains)
INFO - root - 2017-12-05 22:53:11.553949: step 52130, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 66h:16m:52s remains)
INFO - root - 2017-12-05 22:53:20.026391: step 52140, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 65h:18m:41s remains)
INFO - root - 2017-12-05 22:53:28.562544: step 52150, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 65h:27m:19s remains)
INFO - root - 2017-12-05 22:53:37.007178: step 52160, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 68h:15m:53s remains)
INFO - root - 2017-12-05 22:53:45.452541: step 52170, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.819 sec/batch; 63h:47m:13s remains)
INFO - root - 2017-12-05 22:53:53.923946: step 52180, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.824 sec/batch; 64h:08m:22s remains)
INFO - root - 2017-12-05 22:54:02.377342: step 52190, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 67h:05m:53s remains)
INFO - root - 2017-12-05 22:54:10.872931: step 52200, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 67h:30m:56s remains)
2017-12-05 22:54:11.744124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25518 -4.2542319 -4.249855 -4.2495141 -4.2506061 -4.2510681 -4.2533731 -4.2572732 -4.2667837 -4.2691493 -4.2632852 -4.2563448 -4.2521596 -4.2465076 -4.24938][-4.2288871 -4.22745 -4.2233286 -4.2269506 -4.229136 -4.2251344 -4.2224388 -4.2192245 -4.2271748 -4.235064 -4.2403069 -4.2459421 -4.2501922 -4.2454863 -4.2442474][-4.1990805 -4.2051296 -4.2056785 -4.2114406 -4.215188 -4.2060676 -4.1952186 -4.1754055 -4.1767583 -4.1940002 -4.2161579 -4.2369537 -4.2480135 -4.2430539 -4.2385864][-4.1786795 -4.192615 -4.1973839 -4.2000709 -4.20192 -4.1904964 -4.1667585 -4.1318793 -4.1317163 -4.1598816 -4.1943755 -4.2194653 -4.2344604 -4.232986 -4.2288628][-4.1623912 -4.1797066 -4.189672 -4.1898656 -4.1883774 -4.1763439 -4.1408753 -4.0949025 -4.1023078 -4.1508861 -4.1913834 -4.2106609 -4.2242665 -4.2231054 -4.2191381][-4.1566176 -4.1807089 -4.1951137 -4.1901245 -4.1790686 -4.1521916 -4.0962286 -4.0348563 -4.0531931 -4.1302629 -4.1857915 -4.2042561 -4.2127771 -4.2127123 -4.2084117][-4.1546855 -4.1936479 -4.2133365 -4.2044134 -4.179481 -4.1337409 -4.0398817 -3.9393673 -3.9673879 -4.0821476 -4.1610689 -4.1850657 -4.1968884 -4.2015576 -4.1952085][-4.1583667 -4.2115684 -4.233057 -4.2188878 -4.1837025 -4.1201181 -3.9894264 -3.8387742 -3.8697815 -4.0262127 -4.1255684 -4.1562395 -4.1724448 -4.1845274 -4.1791019][-4.1739321 -4.2273545 -4.2445588 -4.2280111 -4.1878324 -4.116467 -3.9826422 -3.8360834 -3.855206 -4.0101018 -4.11093 -4.13788 -4.1509852 -4.1633692 -4.1594977][-4.1895418 -4.2328763 -4.2418728 -4.2210903 -4.1844015 -4.1229839 -4.0206795 -3.9220283 -3.9340303 -4.0447907 -4.1282887 -4.1492977 -4.1551566 -4.1611571 -4.1575675][-4.2042789 -4.2324381 -4.2316952 -4.2088165 -4.1765308 -4.1314497 -4.0637 -4.0081463 -4.0194054 -4.08954 -4.1515288 -4.16822 -4.1688232 -4.1689248 -4.1626678][-4.2177162 -4.2299418 -4.2211738 -4.200942 -4.1754408 -4.1453695 -4.1023555 -4.0746026 -4.0862317 -4.1315045 -4.1723413 -4.1843905 -4.1829534 -4.1826291 -4.1759448][-4.2290378 -4.229208 -4.2180614 -4.2021241 -4.1873431 -4.1685247 -4.1429787 -4.1298866 -4.1385155 -4.1656594 -4.1891146 -4.1948214 -4.1932192 -4.1935744 -4.1922665][-4.2453308 -4.23778 -4.2270188 -4.2156835 -4.2066188 -4.1963043 -4.1830769 -4.1764607 -4.1820292 -4.1971831 -4.212544 -4.2151661 -4.2130022 -4.2149096 -4.2203493][-4.2648411 -4.2568588 -4.2503395 -4.2442951 -4.2409282 -4.2361817 -4.2294602 -4.2254896 -4.2278528 -4.2347879 -4.2441049 -4.2464647 -4.2460508 -4.2489958 -4.2563114]]...]
INFO - root - 2017-12-05 22:54:20.198694: step 52210, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 65h:22m:35s remains)
INFO - root - 2017-12-05 22:54:28.766855: step 52220, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 65h:07m:39s remains)
INFO - root - 2017-12-05 22:54:37.149430: step 52230, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.810 sec/batch; 63h:01m:55s remains)
INFO - root - 2017-12-05 22:54:45.605086: step 52240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 66h:39m:42s remains)
INFO - root - 2017-12-05 22:54:54.006498: step 52250, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 64h:19m:30s remains)
INFO - root - 2017-12-05 22:55:02.376855: step 52260, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 63h:44m:38s remains)
INFO - root - 2017-12-05 22:55:10.959760: step 52270, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 65h:36m:48s remains)
INFO - root - 2017-12-05 22:55:19.481932: step 52280, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 65h:50m:33s remains)
INFO - root - 2017-12-05 22:55:28.020726: step 52290, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 67h:59m:08s remains)
INFO - root - 2017-12-05 22:55:36.558238: step 52300, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 67h:19m:25s remains)
2017-12-05 22:55:37.354600: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2695155 -4.2746973 -4.2729611 -4.266551 -4.2633343 -4.258203 -4.2471013 -4.2401738 -4.236403 -4.2419853 -4.2500882 -4.2555923 -4.259768 -4.2536263 -4.2504826][-4.2460828 -4.25306 -4.25301 -4.2450566 -4.2342782 -4.2163768 -4.1963363 -4.1866393 -4.1823406 -4.1943612 -4.2069478 -4.2158403 -4.2253141 -4.2210793 -4.2171912][-4.2278171 -4.2338438 -4.2317553 -4.2190409 -4.1987638 -4.1685505 -4.1441331 -4.1329246 -4.1255345 -4.1400418 -4.1603179 -4.1777296 -4.1944046 -4.1970744 -4.193161][-4.214191 -4.2163877 -4.2110567 -4.1900063 -4.1586237 -4.1184483 -4.0926304 -4.08209 -4.0695224 -4.0773563 -4.0988121 -4.1243386 -4.1477795 -4.1556077 -4.1541071][-4.2015538 -4.1981592 -4.1856508 -4.156219 -4.1129503 -4.0684943 -4.0427275 -4.0350184 -4.0255938 -4.0312791 -4.0522985 -4.0832086 -4.111412 -4.1217756 -4.1211176][-4.179162 -4.1721053 -4.1465573 -4.1080003 -4.0610456 -4.018333 -3.9943202 -3.9911733 -3.9988773 -4.0132551 -4.0412588 -4.0774469 -4.1100969 -4.1186619 -4.114356][-4.1450858 -4.1398106 -4.1007819 -4.0524049 -4.0106225 -3.980396 -3.9524934 -3.9488559 -3.977381 -4.0058951 -4.0430326 -4.0880203 -4.1206188 -4.1279154 -4.1225824][-4.1190124 -4.1106815 -4.0610509 -4.0046644 -3.9632528 -3.9328356 -3.8860502 -3.8758264 -3.931814 -3.9848847 -4.0311046 -4.0842423 -4.1214895 -4.13522 -4.1308537][-4.1031094 -4.0893531 -4.0409193 -3.9914167 -3.9537368 -3.9198875 -3.8666575 -3.8552425 -3.9221103 -3.9899554 -4.0418324 -4.0915594 -4.1311088 -4.1460619 -4.1422706][-4.1132884 -4.0963426 -4.0561881 -4.0225286 -4.0005789 -3.9734695 -3.937171 -3.9370003 -3.9883652 -4.0471191 -4.089098 -4.1171317 -4.1434984 -4.149241 -4.1438713][-4.1539755 -4.1346803 -4.10246 -4.0772271 -4.0588675 -4.0355849 -4.012476 -4.0188971 -4.0558481 -4.1037865 -4.1368942 -4.1538157 -4.1720943 -4.1702523 -4.1602025][-4.2025285 -4.1850739 -4.1589971 -4.1322508 -4.1111526 -4.0929837 -4.0801473 -4.087461 -4.1156492 -4.1551671 -4.1817617 -4.1957288 -4.2119527 -4.2116847 -4.2006488][-4.2458215 -4.2310038 -4.2097921 -4.185729 -4.165905 -4.1540108 -4.1512213 -4.1566443 -4.1766191 -4.2098885 -4.2313032 -4.2419672 -4.2552805 -4.2561049 -4.2446637][-4.2692657 -4.2614145 -4.2477083 -4.2325726 -4.2222123 -4.2170353 -4.2191596 -4.2212515 -4.2333283 -4.2569637 -4.272882 -4.2815785 -4.2923169 -4.2912788 -4.2780833][-4.2829933 -4.2779503 -4.2683449 -4.2593627 -4.253695 -4.2529798 -4.2565393 -4.2575507 -4.2644453 -4.279367 -4.291543 -4.3005338 -4.3062882 -4.3045893 -4.2937183]]...]
INFO - root - 2017-12-05 22:55:45.994276: step 52310, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 65h:59m:47s remains)
INFO - root - 2017-12-05 22:55:54.587317: step 52320, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 66h:37m:50s remains)
INFO - root - 2017-12-05 22:56:03.035464: step 52330, loss = 2.05, batch loss = 1.99 (10.4 examples/sec; 0.767 sec/batch; 59h:42m:35s remains)
INFO - root - 2017-12-05 22:56:11.404167: step 52340, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 56h:46m:03s remains)
INFO - root - 2017-12-05 22:56:20.078776: step 52350, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 67h:20m:32s remains)
INFO - root - 2017-12-05 22:56:28.429740: step 52360, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 66h:50m:12s remains)
INFO - root - 2017-12-05 22:56:37.010482: step 52370, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 67h:16m:45s remains)
INFO - root - 2017-12-05 22:56:45.632261: step 52380, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 69h:19m:43s remains)
INFO - root - 2017-12-05 22:56:54.218717: step 52390, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 67h:41m:11s remains)
INFO - root - 2017-12-05 22:57:02.763905: step 52400, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 65h:49m:48s remains)
2017-12-05 22:57:03.620637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2981696 -4.3021755 -4.3038621 -4.3045597 -4.3033047 -4.2942719 -4.2775021 -4.2604165 -4.2478948 -4.2351823 -4.2122736 -4.1707354 -4.1239791 -4.0843606 -4.0657477][-4.281877 -4.2848773 -4.2866688 -4.2877064 -4.2855473 -4.275033 -4.2592049 -4.24372 -4.2334194 -4.2252874 -4.2077665 -4.1743727 -4.1369987 -4.1048617 -4.0911736][-4.2701831 -4.2679048 -4.2649131 -4.2604051 -4.2507014 -4.2349606 -4.2222624 -4.2130685 -4.2094445 -4.2080054 -4.2003522 -4.1806479 -4.1549926 -4.132946 -4.1284823][-4.2554655 -4.2442417 -4.2325654 -4.2183371 -4.1964278 -4.17448 -4.1679716 -4.17027 -4.1790376 -4.1867647 -4.1839604 -4.1674085 -4.1490021 -4.1409893 -4.1481285][-4.2384133 -4.221025 -4.2012057 -4.1738529 -4.1374564 -4.1072569 -4.1020513 -4.1212826 -4.1514497 -4.1734743 -4.1692615 -4.1391368 -4.1191926 -4.122232 -4.143918][-4.222456 -4.2059884 -4.18203 -4.1409221 -4.0858579 -4.0317063 -4.0121913 -4.0535164 -4.11921 -4.1646414 -4.1618934 -4.1158237 -4.0836992 -4.0889282 -4.1183953][-4.2071075 -4.19258 -4.1657982 -4.1127152 -4.0321488 -3.9357705 -3.8891294 -3.956537 -4.0704408 -4.1453185 -4.1476707 -4.0969105 -4.0572014 -4.0586433 -4.0877066][-4.1869855 -4.1765313 -4.1497378 -4.0903554 -3.9911344 -3.8547163 -3.7704196 -3.862258 -4.0146985 -4.1073117 -4.1172161 -4.0771151 -4.0447273 -4.0476861 -4.0730963][-4.17171 -4.1643538 -4.1402559 -4.087687 -3.9980311 -3.8661833 -3.78377 -3.8646481 -4.0011191 -4.0873666 -4.1010685 -4.079515 -4.0623488 -4.070858 -4.0988441][-4.1750979 -4.1695232 -4.1485434 -4.1106863 -4.0547047 -3.9744966 -3.9317565 -3.9763582 -4.0545745 -4.1099319 -4.119524 -4.1105332 -4.1054473 -4.12069 -4.152082][-4.2055421 -4.2028294 -4.18513 -4.1596618 -4.1336961 -4.1019735 -4.0888891 -4.1056867 -4.135581 -4.1587877 -4.1632762 -4.1602054 -4.1600513 -4.1762633 -4.2036767][-4.2484317 -4.2506337 -4.2386804 -4.223793 -4.2133589 -4.2039504 -4.2027206 -4.2069335 -4.2138181 -4.2203178 -4.2218304 -4.2212214 -4.2211823 -4.2340922 -4.2540064][-4.2854867 -4.2912784 -4.2863827 -4.2768307 -4.269618 -4.2670875 -4.2713728 -4.2757306 -4.2798805 -4.2824507 -4.28249 -4.2810826 -4.2790689 -4.2869425 -4.2995577][-4.3157945 -4.3227153 -4.3210087 -4.3095303 -4.2990785 -4.2977333 -4.3035269 -4.3107753 -4.3162103 -4.319417 -4.31834 -4.3155994 -4.3142686 -4.3188939 -4.3279586][-4.3339195 -4.342196 -4.3392553 -4.3225775 -4.3045793 -4.2979794 -4.298 -4.2996664 -4.3002234 -4.3015628 -4.300653 -4.3014379 -4.3042979 -4.3104477 -4.3184085]]...]
INFO - root - 2017-12-05 22:57:12.073262: step 52410, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 65h:48m:16s remains)
INFO - root - 2017-12-05 22:57:20.585570: step 52420, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 66h:23m:54s remains)
INFO - root - 2017-12-05 22:57:29.250510: step 52430, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 67h:37m:23s remains)
INFO - root - 2017-12-05 22:57:37.679961: step 52440, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 66h:34m:38s remains)
INFO - root - 2017-12-05 22:57:46.153222: step 52450, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.792 sec/batch; 61h:34m:26s remains)
INFO - root - 2017-12-05 22:57:54.668666: step 52460, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 69h:25m:54s remains)
INFO - root - 2017-12-05 22:58:03.286599: step 52470, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 71h:00m:42s remains)
INFO - root - 2017-12-05 22:58:11.783942: step 52480, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 65h:18m:30s remains)
INFO - root - 2017-12-05 22:58:20.265320: step 52490, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 65h:18m:46s remains)
INFO - root - 2017-12-05 22:58:28.902630: step 52500, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 68h:44m:07s remains)
2017-12-05 22:58:29.676841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.303154 -4.2967076 -4.290997 -4.286499 -4.2865844 -4.2895575 -4.2870469 -4.2792311 -4.2710404 -4.2604961 -4.2514 -4.2441587 -4.2422743 -4.2534986 -4.264709][-4.3074489 -4.30087 -4.29363 -4.2847915 -4.2792349 -4.2729464 -4.2623997 -4.2515392 -4.2443867 -4.2367334 -4.2295871 -4.2239866 -4.2219234 -4.22971 -4.2360411][-4.313797 -4.3096738 -4.3033009 -4.291842 -4.2788739 -4.2612472 -4.2407241 -4.2251258 -4.2169657 -4.2110758 -4.2085552 -4.2079644 -4.2094574 -4.2126918 -4.2128248][-4.3169436 -4.3177891 -4.3126822 -4.2982197 -4.2767773 -4.2489748 -4.2205343 -4.1999869 -4.1881032 -4.1829805 -4.1859317 -4.1940961 -4.20207 -4.2019711 -4.1999664][-4.3116074 -4.3177352 -4.3143559 -4.2990561 -4.2736664 -4.2416587 -4.2097263 -4.1863551 -4.174027 -4.1717677 -4.1812935 -4.2001958 -4.2149758 -4.2155643 -4.2164927][-4.2966981 -4.3070335 -4.3061109 -4.2919421 -4.2666793 -4.2359419 -4.2065015 -4.1871195 -4.1787643 -4.1810684 -4.1982827 -4.2268262 -4.2482257 -4.2513652 -4.2540298][-4.276988 -4.287807 -4.2888331 -4.2743936 -4.2508173 -4.2238011 -4.1974831 -4.1816373 -4.1754827 -4.1790242 -4.1979852 -4.2306781 -4.2547784 -4.2598543 -4.2646294][-4.26397 -4.2701283 -4.2681689 -4.2510567 -4.2266846 -4.2017417 -4.1748166 -4.1570697 -4.1489282 -4.1479816 -4.1635642 -4.1986971 -4.223577 -4.2304106 -4.2403836][-4.2651424 -4.2657118 -4.2570062 -4.2334771 -4.203064 -4.1751766 -4.1457095 -4.1217432 -4.1097369 -4.1040106 -4.1170421 -4.1544461 -4.17791 -4.1852169 -4.1997161][-4.2749729 -4.2737584 -4.2622132 -4.23356 -4.1979723 -4.1675644 -4.1376405 -4.1123624 -4.098402 -4.09297 -4.1100636 -4.1472588 -4.1655159 -4.1718893 -4.1869216][-4.2857108 -4.2864332 -4.2754927 -4.2472277 -4.2131343 -4.1813774 -4.1524386 -4.1298251 -4.1189756 -4.118372 -4.1412163 -4.1746521 -4.1893735 -4.1959949 -4.2107558][-4.2925663 -4.2940927 -4.2849994 -4.2610483 -4.2320518 -4.2035975 -4.1770864 -4.1608496 -4.1555748 -4.159081 -4.1816998 -4.2096534 -4.2216296 -4.2278295 -4.2404203][-4.2962165 -4.2970471 -4.2896938 -4.27104 -4.2492824 -4.2311077 -4.214397 -4.2077017 -4.209259 -4.2170568 -4.2362304 -4.25548 -4.2613325 -4.2658453 -4.2732191][-4.2898717 -4.2920938 -4.2889409 -4.2784324 -4.2651196 -4.2567525 -4.2523537 -4.2541451 -4.2618394 -4.2731271 -4.2890663 -4.301332 -4.3033829 -4.3061719 -4.3079567][-4.2647705 -4.2677407 -4.2712727 -4.2734246 -4.2691441 -4.2688355 -4.2736669 -4.2813244 -4.2901793 -4.3021278 -4.3155136 -4.3232937 -4.3231821 -4.3214855 -4.3154926]]...]
INFO - root - 2017-12-05 22:58:38.335189: step 52510, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 66h:32m:55s remains)
INFO - root - 2017-12-05 22:58:46.896335: step 52520, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 65h:27m:02s remains)
INFO - root - 2017-12-05 22:58:55.456540: step 52530, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 65h:33m:54s remains)
INFO - root - 2017-12-05 22:59:03.992063: step 52540, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.830 sec/batch; 64h:33m:35s remains)
INFO - root - 2017-12-05 22:59:12.427132: step 52550, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 64h:44m:37s remains)
INFO - root - 2017-12-05 22:59:20.646515: step 52560, loss = 2.04, batch loss = 1.98 (10.7 examples/sec; 0.744 sec/batch; 57h:52m:38s remains)
INFO - root - 2017-12-05 22:59:29.152551: step 52570, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 65h:17m:15s remains)
INFO - root - 2017-12-05 22:59:37.594235: step 52580, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 65h:47m:25s remains)
INFO - root - 2017-12-05 22:59:46.067633: step 52590, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.835 sec/batch; 64h:55m:06s remains)
INFO - root - 2017-12-05 22:59:54.573658: step 52600, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 67h:06m:33s remains)
2017-12-05 22:59:55.358343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2598343 -4.2853613 -4.2983031 -4.2957783 -4.2817974 -4.2676678 -4.2633181 -4.2704439 -4.277349 -4.2783971 -4.2787066 -4.2799668 -4.2790494 -4.2820196 -4.2853251][-4.2424159 -4.2721195 -4.2863851 -4.2875886 -4.2784567 -4.2708869 -4.2721682 -4.2826929 -4.2919879 -4.2966175 -4.300128 -4.3031397 -4.30269 -4.3010683 -4.2967281][-4.2327857 -4.2636237 -4.2806191 -4.2863541 -4.2842689 -4.2841129 -4.2880874 -4.296227 -4.303741 -4.3083134 -4.3122797 -4.3181763 -4.3217773 -4.3204575 -4.3135071][-4.2394819 -4.2652044 -4.2810168 -4.28815 -4.2882924 -4.2882833 -4.289041 -4.291018 -4.2946644 -4.2980709 -4.3018103 -4.3119864 -4.322567 -4.3251805 -4.3192425][-4.2449546 -4.2652941 -4.2794495 -4.2863188 -4.2859855 -4.2811565 -4.2743421 -4.2672205 -4.2624416 -4.26213 -4.2661004 -4.2809668 -4.299139 -4.3071957 -4.3058419][-4.2248216 -4.2427769 -4.2552152 -4.2598138 -4.2571206 -4.2484241 -4.2339759 -4.2164721 -4.2006197 -4.1959105 -4.2038341 -4.2252645 -4.2524109 -4.2680135 -4.2735481][-4.1770043 -4.1936269 -4.2039266 -4.2034578 -4.194479 -4.1776237 -4.1540861 -4.1257763 -4.0998049 -4.0970922 -4.1163039 -4.1491575 -4.1869678 -4.2107716 -4.2227135][-4.1363873 -4.1450796 -4.1474729 -4.1365576 -4.1116037 -4.07811 -4.0448165 -4.0141249 -3.9939632 -4.0089188 -4.0479169 -4.0923452 -4.1329927 -4.15953 -4.1747322][-4.1329026 -4.1300473 -4.11835 -4.0886474 -4.040802 -3.9878635 -3.9511602 -3.9369178 -3.945055 -3.9869902 -4.0409188 -4.0884833 -4.123837 -4.1446209 -4.1554646][-4.1762209 -4.1652236 -4.1407261 -4.0935955 -4.0237951 -3.9529448 -3.9172549 -3.9256275 -3.9644108 -4.0269618 -4.0895939 -4.1372566 -4.1652136 -4.1773615 -4.1802745][-4.2410774 -4.2284827 -4.1991544 -4.1449618 -4.0679021 -3.9946613 -3.9633319 -3.982147 -4.0339541 -4.10338 -4.1663184 -4.2094388 -4.2297297 -4.2350087 -4.2322135][-4.286768 -4.2775168 -4.2526803 -4.2048545 -4.1383538 -4.078474 -4.0531664 -4.0694523 -4.1147804 -4.1750493 -4.2290235 -4.2651114 -4.2819204 -4.2851133 -4.2816777][-4.3096991 -4.3039665 -4.2871628 -4.2523422 -4.2037816 -4.1618133 -4.1444426 -4.1561975 -4.1891565 -4.2324438 -4.2702761 -4.2952247 -4.30668 -4.3081636 -4.3052979][-4.32206 -4.3201828 -4.3109093 -4.2880349 -4.2562451 -4.2303967 -4.2209635 -4.2287908 -4.2485423 -4.2732553 -4.2939858 -4.3072095 -4.3134627 -4.3145041 -4.3133636][-4.3296247 -4.3289161 -4.3238778 -4.310287 -4.2921276 -4.2793794 -4.2766762 -4.2822819 -4.2918949 -4.3025222 -4.3109422 -4.3165889 -4.3197269 -4.3207297 -4.3206]]...]
INFO - root - 2017-12-05 23:00:03.913314: step 52610, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 65h:57m:28s remains)
INFO - root - 2017-12-05 23:00:12.485618: step 52620, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 66h:44m:48s remains)
INFO - root - 2017-12-05 23:00:20.950604: step 52630, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 65h:52m:16s remains)
INFO - root - 2017-12-05 23:00:29.559648: step 52640, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 68h:58m:07s remains)
INFO - root - 2017-12-05 23:00:38.084115: step 52650, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 0.796 sec/batch; 61h:53m:15s remains)
INFO - root - 2017-12-05 23:00:46.517933: step 52660, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.798 sec/batch; 61h:59m:49s remains)
INFO - root - 2017-12-05 23:00:54.951270: step 52670, loss = 2.04, batch loss = 1.98 (10.3 examples/sec; 0.775 sec/batch; 60h:13m:07s remains)
INFO - root - 2017-12-05 23:01:03.561227: step 52680, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 66h:55m:31s remains)
INFO - root - 2017-12-05 23:01:12.123179: step 52690, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 68h:02m:47s remains)
INFO - root - 2017-12-05 23:01:20.626986: step 52700, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 66h:32m:49s remains)
2017-12-05 23:01:21.342414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1841321 -4.1798406 -4.1733656 -4.1881628 -4.2037139 -4.2058368 -4.2077708 -4.2160048 -4.2224793 -4.22053 -4.2217941 -4.2225361 -4.2276468 -4.2391858 -4.2670302][-4.160831 -4.1472735 -4.1325035 -4.1514359 -4.1683707 -4.1698003 -4.1753397 -4.1985159 -4.2177377 -4.224946 -4.2298794 -4.2297473 -4.2347741 -4.2456613 -4.2709775][-4.1107168 -4.0926218 -4.0779362 -4.1016526 -4.1243429 -4.1308646 -4.1423168 -4.17575 -4.1986041 -4.2110415 -4.2242751 -4.2329178 -4.2388673 -4.2519994 -4.2778487][-4.0438709 -4.0276837 -4.0207844 -4.0579023 -4.0922894 -4.1044936 -4.1194215 -4.1593208 -4.1823373 -4.2019839 -4.2211232 -4.233717 -4.2408032 -4.2566352 -4.2839365][-3.9905093 -3.9809914 -3.9896276 -4.0359511 -4.0687485 -4.0726418 -4.0796485 -4.1210761 -4.1431723 -4.16513 -4.1881061 -4.2051535 -4.2168531 -4.2448678 -4.2795115][-3.9678743 -3.9630389 -3.9746747 -4.0087318 -4.0219378 -4.0072465 -4.0024943 -4.0432506 -4.0724087 -4.103281 -4.1378345 -4.1715093 -4.19299 -4.2298131 -4.2700987][-3.9657471 -3.9517143 -3.9471874 -3.954246 -3.9450512 -3.9074583 -3.8844168 -3.9311657 -3.9841609 -4.0396729 -4.0980396 -4.1504273 -4.1793232 -4.2183332 -4.2587981][-3.9949529 -3.9638419 -3.940331 -3.9211695 -3.8876891 -3.8296936 -3.7921848 -3.8459687 -3.9222045 -3.9978881 -4.0691509 -4.1282897 -4.1633739 -4.2042165 -4.2470417][-4.0647449 -4.0259995 -3.9879198 -3.9509068 -3.9039679 -3.846034 -3.8127649 -3.8587403 -3.9241383 -3.988775 -4.0556755 -4.11128 -4.1517773 -4.1977291 -4.2415662][-4.1414533 -4.10979 -4.0708938 -4.0293722 -3.982084 -3.9355886 -3.9124029 -3.9401283 -3.97979 -4.023253 -4.0763645 -4.1256294 -4.167047 -4.2132187 -4.2512922][-4.209094 -4.18859 -4.1581669 -4.122088 -4.0844545 -4.0512395 -4.0362177 -4.0512962 -4.0700316 -4.0920153 -4.1327944 -4.1738305 -4.2099395 -4.2467446 -4.2729306][-4.2678909 -4.2570543 -4.2375045 -4.2111459 -4.1856751 -4.1614242 -4.149941 -4.1568165 -4.1612105 -4.167356 -4.1946769 -4.2242928 -4.251586 -4.2782125 -4.2984686][-4.3060627 -4.2981129 -4.2858768 -4.2684264 -4.2512107 -4.2332821 -4.2229648 -4.2234874 -4.2208667 -4.2217908 -4.23794 -4.2585297 -4.2791572 -4.2982059 -4.3146591][-4.3229427 -4.3160892 -4.3069153 -4.2941523 -4.2820148 -4.2713361 -4.2645612 -4.2621307 -4.257483 -4.2561126 -4.2644091 -4.2781043 -4.2930512 -4.3073168 -4.3185925][-4.3323426 -4.3281164 -4.322711 -4.3150482 -4.3082933 -4.302444 -4.2969584 -4.2914495 -4.2854667 -4.2835064 -4.28679 -4.2940779 -4.3030925 -4.3135476 -4.3217421]]...]
INFO - root - 2017-12-05 23:01:29.835318: step 52710, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 67h:48m:53s remains)
INFO - root - 2017-12-05 23:01:38.341379: step 52720, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 67h:13m:40s remains)
INFO - root - 2017-12-05 23:01:46.898618: step 52730, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 65h:41m:27s remains)
INFO - root - 2017-12-05 23:01:55.595813: step 52740, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 67h:25m:54s remains)
INFO - root - 2017-12-05 23:02:04.226829: step 52750, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.877 sec/batch; 68h:10m:15s remains)
INFO - root - 2017-12-05 23:02:12.695797: step 52760, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.775 sec/batch; 60h:14m:29s remains)
INFO - root - 2017-12-05 23:02:21.233817: step 52770, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 66h:05m:24s remains)
INFO - root - 2017-12-05 23:02:29.622928: step 52780, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 63h:42m:57s remains)
INFO - root - 2017-12-05 23:02:38.115115: step 52790, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 64h:35m:22s remains)
INFO - root - 2017-12-05 23:02:46.624450: step 52800, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 68h:33m:04s remains)
2017-12-05 23:02:47.439035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2333422 -4.2401743 -4.2465167 -4.2512631 -4.257978 -4.2620935 -4.2619362 -4.262022 -4.2642193 -4.2713194 -4.27929 -4.2855043 -4.2916951 -4.2971759 -4.2974515][-4.2505455 -4.2509484 -4.2548842 -4.2599087 -4.2663751 -4.2717619 -4.2745214 -4.277812 -4.282424 -4.2896376 -4.2945356 -4.2968988 -4.3011818 -4.3066268 -4.3076463][-4.2589221 -4.2528267 -4.2546358 -4.2600574 -4.2657895 -4.2713885 -4.2777038 -4.2844443 -4.2909937 -4.2970276 -4.2989874 -4.2975273 -4.2994862 -4.3047247 -4.3084087][-4.2291241 -4.2179971 -4.2190838 -4.2254739 -4.2283106 -4.230711 -4.2393804 -4.2521129 -4.2638454 -4.2699194 -4.2676315 -4.2619162 -4.2620153 -4.2700686 -4.2789884][-4.1579885 -4.1450925 -4.1461954 -4.1489758 -4.1442971 -4.1403189 -4.1506052 -4.1768446 -4.2037964 -4.2173219 -4.214191 -4.2044911 -4.2024221 -4.21143 -4.2228675][-4.0906844 -4.0853124 -4.0857921 -4.076376 -4.04986 -4.0202174 -4.0188851 -4.0593033 -4.1127377 -4.1475954 -4.15583 -4.1511474 -4.1495109 -4.157352 -4.1664233][-4.05833 -4.0703754 -4.0802617 -4.0676394 -4.0248747 -3.9630551 -3.9241076 -3.9529347 -4.0186739 -4.0757179 -4.1030731 -4.1112108 -4.1139212 -4.11952 -4.1229448][-4.0737443 -4.0981059 -4.1191087 -4.1171021 -4.0862164 -4.0294547 -3.9736197 -3.9658008 -4.0053883 -4.0557308 -4.0870495 -4.099793 -4.1026034 -4.1026521 -4.0981345][-4.1165137 -4.1436296 -4.1680431 -4.1741214 -4.1616969 -4.1287155 -4.0858974 -4.0662737 -4.0777321 -4.1020565 -4.1196194 -4.1283455 -4.1312246 -4.1292949 -4.1207547][-4.143847 -4.1722741 -4.1935358 -4.2017646 -4.2012696 -4.191216 -4.1747389 -4.1700249 -4.1772866 -4.1866961 -4.1880746 -4.1859355 -4.1858759 -4.1856318 -4.1786103][-4.1587772 -4.1821909 -4.1964869 -4.1981006 -4.1995015 -4.2028413 -4.2120652 -4.2308741 -4.2503242 -4.2604995 -4.2563386 -4.2445946 -4.2383013 -4.23833 -4.2363677][-4.1704092 -4.1898494 -4.1966829 -4.1864238 -4.1778321 -4.1822748 -4.2031822 -4.2393446 -4.2740746 -4.2944942 -4.2957196 -4.2833991 -4.2713842 -4.2681794 -4.2716103][-4.1793008 -4.1995006 -4.2059994 -4.1905193 -4.1691103 -4.1627083 -4.1783714 -4.2134113 -4.2528954 -4.2811856 -4.2908063 -4.283987 -4.2716413 -4.2664866 -4.2727013][-4.1907988 -4.2122712 -4.2235441 -4.2082572 -4.1802406 -4.1613841 -4.1633582 -4.1865535 -4.2191811 -4.2463684 -4.2612185 -4.2623067 -4.2528877 -4.2459869 -4.25315][-4.2046294 -4.2223148 -4.2374568 -4.2243414 -4.1933303 -4.1658545 -4.1570635 -4.1720338 -4.1980939 -4.2215014 -4.23738 -4.2426233 -4.2375259 -4.2300382 -4.2349868]]...]
INFO - root - 2017-12-05 23:02:55.878728: step 52810, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 65h:45m:37s remains)
INFO - root - 2017-12-05 23:03:04.414904: step 52820, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 65h:20m:03s remains)
INFO - root - 2017-12-05 23:03:12.923064: step 52830, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.838 sec/batch; 65h:04m:13s remains)
INFO - root - 2017-12-05 23:03:21.376792: step 52840, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 64h:42m:36s remains)
INFO - root - 2017-12-05 23:03:29.954519: step 52850, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 68h:58m:31s remains)
INFO - root - 2017-12-05 23:03:38.413270: step 52860, loss = 2.03, batch loss = 1.97 (10.5 examples/sec; 0.761 sec/batch; 59h:06m:33s remains)
INFO - root - 2017-12-05 23:03:46.987904: step 52870, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 65h:11m:40s remains)
INFO - root - 2017-12-05 23:03:55.622296: step 52880, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 67h:04m:26s remains)
INFO - root - 2017-12-05 23:04:03.991923: step 52890, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 0.787 sec/batch; 61h:08m:32s remains)
INFO - root - 2017-12-05 23:04:12.519099: step 52900, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.856 sec/batch; 66h:26m:39s remains)
2017-12-05 23:04:13.294061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2925091 -4.2907276 -4.2770844 -4.2582283 -4.2328534 -4.1942372 -4.160501 -4.1436238 -4.152441 -4.1861458 -4.2174158 -4.2324228 -4.2296896 -4.231298 -4.2372713][-4.2837615 -4.2797837 -4.2661457 -4.2451844 -4.2177496 -4.1772265 -4.1405253 -4.1123953 -4.1066251 -4.1381521 -4.1746583 -4.1945496 -4.1941886 -4.1976848 -4.2052913][-4.2787013 -4.2718816 -4.2556458 -4.2303557 -4.1990881 -4.1557741 -4.1163526 -4.0785904 -4.0559492 -4.0757842 -4.1113544 -4.1382375 -4.1431856 -4.1504617 -4.1586914][-4.2791262 -4.2725859 -4.2553039 -4.2263775 -4.1877055 -4.1392269 -4.0913224 -4.0399446 -4.0023403 -4.0165105 -4.0537853 -4.0888615 -4.0994925 -4.1119661 -4.1279664][-4.2796717 -4.275188 -4.25839 -4.2256308 -4.1790566 -4.1187763 -4.0545754 -3.9860339 -3.9379478 -3.9671571 -4.0255532 -4.0730233 -4.0890341 -4.1053872 -4.1287541][-4.2788281 -4.2737465 -4.2570181 -4.2223234 -4.1691365 -4.0912361 -4.0057511 -3.9206631 -3.8727255 -3.9342694 -4.0252457 -4.0899811 -4.1074305 -4.1238494 -4.1493797][-4.2784061 -4.2709308 -4.2539153 -4.2175803 -4.1582875 -4.0636945 -3.9608378 -3.8732717 -3.8464422 -3.9517887 -4.0649619 -4.1367 -4.1514058 -4.1596837 -4.1764283][-4.2785783 -4.2674718 -4.2478571 -4.2098007 -4.1477695 -4.0501041 -3.9448044 -3.869947 -3.8822823 -4.0132875 -4.12799 -4.1945019 -4.2006035 -4.193254 -4.1897259][-4.2785392 -4.2643905 -4.2427034 -4.2047377 -4.1434 -4.0546083 -3.9656763 -3.9154584 -3.9572802 -4.0814734 -4.1799335 -4.2319188 -4.2277865 -4.204248 -4.178072][-4.2804151 -4.2646976 -4.2414932 -4.2031569 -4.1439414 -4.0704994 -4.0046644 -3.9793367 -4.0290551 -4.1274476 -4.2045841 -4.2419147 -4.227788 -4.1899567 -4.1433325][-4.285759 -4.270628 -4.2493458 -4.2113142 -4.1532211 -4.0907683 -4.0416861 -4.0325437 -4.0836053 -4.1566339 -4.2117181 -4.231678 -4.2057528 -4.1587462 -4.1026468][-4.2909746 -4.2776461 -4.2609525 -4.2233653 -4.1677771 -4.1137547 -4.0759721 -4.0721006 -4.1173062 -4.1733084 -4.2125912 -4.2171159 -4.1877861 -4.1428714 -4.09493][-4.2970896 -4.2846017 -4.2700324 -4.2322922 -4.1790404 -4.132781 -4.1023417 -4.0970225 -4.129189 -4.172821 -4.2034016 -4.19995 -4.173461 -4.142725 -4.1114078][-4.3044205 -4.2921696 -4.2760139 -4.2382474 -4.1850052 -4.1418972 -4.1097627 -4.0959721 -4.11411 -4.1476288 -4.1755166 -4.1772795 -4.1593218 -4.13727 -4.1142864][-4.3094192 -4.3012323 -4.2855039 -4.2493253 -4.1966038 -4.1462884 -4.1001773 -4.0726919 -4.0743613 -4.0998297 -4.1348982 -4.1538043 -4.1468859 -4.1251822 -4.1046381]]...]
INFO - root - 2017-12-05 23:04:21.923713: step 52910, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 64h:07m:10s remains)
INFO - root - 2017-12-05 23:04:30.362994: step 52920, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 66h:55m:49s remains)
INFO - root - 2017-12-05 23:04:38.942912: step 52930, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 67h:45m:07s remains)
INFO - root - 2017-12-05 23:04:47.529506: step 52940, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 66h:27m:38s remains)
INFO - root - 2017-12-05 23:04:56.097281: step 52950, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 66h:23m:02s remains)
INFO - root - 2017-12-05 23:05:04.717781: step 52960, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.788 sec/batch; 61h:10m:28s remains)
INFO - root - 2017-12-05 23:05:13.249476: step 52970, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.809 sec/batch; 62h:47m:18s remains)
INFO - root - 2017-12-05 23:05:21.725876: step 52980, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 65h:44m:45s remains)
INFO - root - 2017-12-05 23:05:30.295594: step 52990, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 67h:58m:58s remains)
INFO - root - 2017-12-05 23:05:38.744019: step 53000, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.776 sec/batch; 60h:15m:51s remains)
2017-12-05 23:05:39.496389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2935266 -4.2977867 -4.2997437 -4.3001313 -4.295054 -4.2906709 -4.2875 -4.2825351 -4.2792497 -4.2847214 -4.2979918 -4.3094268 -4.3208389 -4.3310061 -4.34263][-4.2779889 -4.283021 -4.2855268 -4.2880058 -4.2819266 -4.2762413 -4.2745075 -4.2685781 -4.2604303 -4.2644415 -4.2773309 -4.2879004 -4.2993288 -4.3146453 -4.3338847][-4.2624249 -4.2670712 -4.2702332 -4.2758675 -4.2689543 -4.2615738 -4.2606993 -4.2542014 -4.2416096 -4.2432284 -4.2548842 -4.2620726 -4.2725606 -4.2945776 -4.3227215][-4.2485008 -4.25262 -4.2567463 -4.2646041 -4.2576256 -4.2499313 -4.2488928 -4.2408352 -4.2259712 -4.2240152 -4.2318416 -4.2327886 -4.2418528 -4.2730455 -4.3102655][-4.2285523 -4.2294893 -4.231204 -4.2407007 -4.237092 -4.2313738 -4.2293968 -4.2173004 -4.1974621 -4.19003 -4.196795 -4.1989226 -4.2116294 -4.2514796 -4.2965612][-4.2098203 -4.2036276 -4.2006774 -4.21043 -4.2090793 -4.2052264 -4.2008786 -4.1827869 -4.1555333 -4.140749 -4.1489677 -4.1593218 -4.1800985 -4.2281651 -4.2806773][-4.1962562 -4.1817756 -4.1747193 -4.1842871 -4.1870017 -4.1887741 -4.1862841 -4.1685486 -4.1414142 -4.1202788 -4.1280155 -4.1411777 -4.1655312 -4.2159657 -4.2718091][-4.2030191 -4.1866145 -4.1768026 -4.1838293 -4.19026 -4.1968675 -4.19676 -4.1841073 -4.1594868 -4.1334829 -4.1346636 -4.14302 -4.1668615 -4.2166409 -4.2711015][-4.2239666 -4.2123113 -4.1996388 -4.2049923 -4.2153487 -4.2268157 -4.2262015 -4.2150173 -4.193141 -4.1664567 -4.161509 -4.1639776 -4.1826668 -4.2256527 -4.2759805][-4.2592211 -4.25103 -4.2359939 -4.2369504 -4.2470722 -4.26053 -4.259419 -4.2497196 -4.2307696 -4.2079973 -4.1996627 -4.1988435 -4.2104049 -4.2413206 -4.2851677][-4.2874422 -4.2791271 -4.2617426 -4.25718 -4.2669625 -4.2806497 -4.2800541 -4.2724652 -4.2561784 -4.236867 -4.2259092 -4.2216 -4.2286983 -4.2525415 -4.2918725][-4.3008618 -4.2916079 -4.2755084 -4.2699418 -4.277421 -4.2867317 -4.285192 -4.280025 -4.2668514 -4.2500639 -4.2389979 -4.2347445 -4.2406969 -4.2609792 -4.2961192][-4.3040423 -4.2951341 -4.2847676 -4.2842312 -4.2904797 -4.2947006 -4.2917571 -4.2861867 -4.2751393 -4.2609282 -4.2518311 -4.2501011 -4.2553368 -4.2734537 -4.3034592][-4.304081 -4.2979922 -4.2932553 -4.2941647 -4.2959747 -4.2955532 -4.2936754 -4.2906022 -4.2835293 -4.2740068 -4.2685542 -4.2687311 -4.2752757 -4.2916989 -4.3156495][-4.3067918 -4.3034616 -4.3003411 -4.2995729 -4.2977123 -4.2958231 -4.295444 -4.295 -4.2924232 -4.2890687 -4.2881165 -4.2908726 -4.2989621 -4.3128948 -4.3299074]]...]
INFO - root - 2017-12-05 23:05:47.991936: step 53010, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 66h:57m:44s remains)
INFO - root - 2017-12-05 23:05:56.586317: step 53020, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 64h:13m:04s remains)
INFO - root - 2017-12-05 23:06:05.205667: step 53030, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 64h:37m:44s remains)
INFO - root - 2017-12-05 23:06:13.838301: step 53040, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 65h:17m:23s remains)
INFO - root - 2017-12-05 23:06:22.399184: step 53050, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 66h:54m:06s remains)
INFO - root - 2017-12-05 23:06:30.871566: step 53060, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.746 sec/batch; 57h:53m:14s remains)
INFO - root - 2017-12-05 23:06:39.479044: step 53070, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 68h:45m:29s remains)
INFO - root - 2017-12-05 23:06:48.026434: step 53080, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 68h:43m:22s remains)
INFO - root - 2017-12-05 23:06:56.703454: step 53090, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 65h:57m:20s remains)
INFO - root - 2017-12-05 23:07:05.442960: step 53100, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 67h:16m:54s remains)
2017-12-05 23:07:06.277082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.250783 -4.2398896 -4.2317352 -4.2325592 -4.2402105 -4.2497845 -4.2609019 -4.2707229 -4.2807236 -4.2882385 -4.2885947 -4.2874861 -4.2784762 -4.2668519 -4.2708931][-4.2317595 -4.221364 -4.2091408 -4.2068634 -4.2097049 -4.2167058 -4.2256966 -4.2380252 -4.2567148 -4.2751465 -4.2813907 -4.2829165 -4.2761922 -4.2642441 -4.2661843][-4.2120748 -4.2015042 -4.1885962 -4.1853523 -4.1832075 -4.1853328 -4.18976 -4.2025161 -4.2255917 -4.2520709 -4.2626076 -4.2684479 -4.270308 -4.2647915 -4.2694411][-4.2085776 -4.1938839 -4.1778564 -4.1654744 -4.1553755 -4.1548262 -4.1591725 -4.1738429 -4.2029786 -4.2388067 -4.2502022 -4.2557664 -4.26336 -4.2623682 -4.2697458][-4.197401 -4.183991 -4.16893 -4.1487031 -4.1287518 -4.1182694 -4.1154685 -4.1248946 -4.163518 -4.213037 -4.2316737 -4.2426639 -4.2547359 -4.2602129 -4.2710719][-4.1601329 -4.154408 -4.1441092 -4.1207023 -4.0882173 -4.0610633 -4.0396566 -4.0375762 -4.0883188 -4.15555 -4.1858406 -4.205256 -4.2240038 -4.2369661 -4.2560949][-4.0975804 -4.0955672 -4.0864415 -4.0609202 -4.0190196 -3.9707484 -3.9204578 -3.9031096 -3.9679432 -4.056128 -4.1015434 -4.1324592 -4.1624036 -4.1881728 -4.2209129][-4.0214009 -4.0214205 -4.0118103 -3.9840236 -3.9378533 -3.8733804 -3.7921638 -3.753603 -3.8299448 -3.9358327 -3.9952545 -4.0367584 -4.0777955 -4.1177044 -4.1688466][-4.0005679 -4.0065603 -3.99914 -3.9719274 -3.9317758 -3.8765061 -3.8004005 -3.752274 -3.8031456 -3.8878179 -3.9392872 -3.9771326 -4.0163074 -4.0640235 -4.1278644][-4.0405235 -4.0515356 -4.0489383 -4.0276728 -4.0015793 -3.9720674 -3.9269867 -3.8907738 -3.9119809 -3.95935 -3.9914427 -4.0140142 -4.0407128 -4.0831227 -4.139823][-4.0957808 -4.1094131 -4.1103411 -4.095264 -4.0807228 -4.0698953 -4.0497656 -4.0295305 -4.0371485 -4.0617814 -4.0799541 -4.0924439 -4.1108332 -4.143456 -4.1860867][-4.1504216 -4.1626334 -4.1663327 -4.1578226 -4.150888 -4.1506743 -4.1464071 -4.1366911 -4.13746 -4.1477561 -4.1566329 -4.1631107 -4.1774664 -4.1998091 -4.2290659][-4.1948266 -4.2034616 -4.2082758 -4.2064271 -4.2049837 -4.207736 -4.2080669 -4.2047448 -4.2029181 -4.2053185 -4.2095418 -4.2144046 -4.2234912 -4.23766 -4.2582111][-4.2448258 -4.2488661 -4.2521458 -4.25251 -4.2526879 -4.2555032 -4.2575526 -4.2570825 -4.2542892 -4.2534547 -4.2550879 -4.2579341 -4.2635794 -4.2726307 -4.2867141][-4.2999005 -4.3015332 -4.3037043 -4.3043833 -4.3045521 -4.3057814 -4.307024 -4.307199 -4.3054709 -4.3037038 -4.3035069 -4.3042912 -4.3067093 -4.311481 -4.3195691]]...]
INFO - root - 2017-12-05 23:07:14.844195: step 53110, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 66h:45m:46s remains)
INFO - root - 2017-12-05 23:07:23.297341: step 53120, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.858 sec/batch; 66h:35m:41s remains)
INFO - root - 2017-12-05 23:07:31.828509: step 53130, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 67h:25m:51s remains)
INFO - root - 2017-12-05 23:07:40.366407: step 53140, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 66h:16m:28s remains)
INFO - root - 2017-12-05 23:07:48.932666: step 53150, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.819 sec/batch; 63h:30m:51s remains)
INFO - root - 2017-12-05 23:07:57.488718: step 53160, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 57h:08m:08s remains)
INFO - root - 2017-12-05 23:08:06.176937: step 53170, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 67h:16m:05s remains)
INFO - root - 2017-12-05 23:08:14.890972: step 53180, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 67h:41m:48s remains)
INFO - root - 2017-12-05 23:08:23.431297: step 53190, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 69h:09m:18s remains)
INFO - root - 2017-12-05 23:08:32.040710: step 53200, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 68h:50m:30s remains)
2017-12-05 23:08:32.809390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9589279 -3.9716253 -4.0241556 -4.0871267 -4.1356821 -4.166142 -4.1886234 -4.2301044 -4.2845349 -4.326335 -4.3474264 -4.3508129 -4.3348393 -4.3027463 -4.2653947][-3.9524012 -3.9327254 -3.9605765 -4.0207005 -4.0771294 -4.1062217 -4.1178885 -4.1563272 -4.2196932 -4.2746983 -4.3094463 -4.3231783 -4.31401 -4.2904572 -4.2618904][-3.9826779 -3.940335 -3.9458287 -4.000031 -4.0568895 -4.0727963 -4.0618911 -4.0861745 -4.1516747 -4.2185879 -4.2683234 -4.2898426 -4.2824368 -4.2639847 -4.2373061][-4.0433345 -3.999954 -3.9955218 -4.0375128 -4.0775943 -4.0643773 -4.0222979 -4.0247183 -4.088376 -4.1725769 -4.2401457 -4.2678227 -4.26004 -4.2434645 -4.2138968][-4.1128693 -4.0810308 -4.0762272 -4.0991035 -4.1076026 -4.0582232 -3.9856734 -3.96473 -4.0295787 -4.139534 -4.2288117 -4.2665405 -4.2641625 -4.2544317 -4.2246189][-4.1637115 -4.1485634 -4.1502991 -4.15394 -4.1320429 -4.0578079 -3.9673672 -3.9284282 -3.9933288 -4.1227341 -4.227159 -4.2758756 -4.2836442 -4.2835765 -4.2608314][-4.1913433 -4.1836071 -4.1851692 -4.1734591 -4.1333752 -4.0516338 -3.9576995 -3.9123359 -3.9758382 -4.1106563 -4.2240038 -4.2839088 -4.3064861 -4.3169417 -4.3022895][-4.2098818 -4.1991019 -4.1924915 -4.1659718 -4.1151867 -4.0354462 -3.947474 -3.9016423 -3.9586895 -4.0864468 -4.2072868 -4.2802806 -4.3181014 -4.3397045 -4.3355637][-4.2367697 -4.2220955 -4.2051444 -4.1715317 -4.119329 -4.0474839 -3.9670599 -3.9174273 -3.9546924 -4.0570068 -4.1713428 -4.2511487 -4.2957129 -4.3250003 -4.3339939][-4.2669721 -4.2525148 -4.2347059 -4.204371 -4.1638379 -4.1100626 -4.0422163 -3.9826734 -3.9844949 -4.0472231 -4.1400146 -4.212224 -4.2528782 -4.2799735 -4.2957063][-4.2898755 -4.2791758 -4.2650185 -4.2462153 -4.2234406 -4.189712 -4.13412 -4.0703077 -4.0429745 -4.0706992 -4.1379004 -4.19283 -4.2181458 -4.2274609 -4.2335854][-4.2972269 -4.2911129 -4.2802753 -4.2730041 -4.2661142 -4.2495723 -4.2093945 -4.1540852 -4.1159554 -4.1192741 -4.1615911 -4.1989427 -4.2081604 -4.1963682 -4.1827312][-4.2696714 -4.266151 -4.256238 -4.2599592 -4.2706003 -4.274076 -4.2577195 -4.2228069 -4.1892228 -4.1791592 -4.1979766 -4.219481 -4.2187304 -4.1925359 -4.1633244][-4.2190614 -4.2142715 -4.2040877 -4.215097 -4.2411218 -4.2649484 -4.2767544 -4.2679963 -4.2490973 -4.2384872 -4.2446842 -4.2555585 -4.2478609 -4.213788 -4.1725144][-4.1824918 -4.1730132 -4.1614966 -4.1727762 -4.2020521 -4.2403226 -4.2759438 -4.2905073 -4.2893729 -4.2892814 -4.29525 -4.3023772 -4.2890167 -4.2463546 -4.1923823]]...]
INFO - root - 2017-12-05 23:08:41.444573: step 53210, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 67h:36m:30s remains)
INFO - root - 2017-12-05 23:08:49.950304: step 53220, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 66h:17m:02s remains)
INFO - root - 2017-12-05 23:08:58.451100: step 53230, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 65h:48m:06s remains)
INFO - root - 2017-12-05 23:09:07.004385: step 53240, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 66h:35m:54s remains)
INFO - root - 2017-12-05 23:09:15.547571: step 53250, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 66h:36m:17s remains)
INFO - root - 2017-12-05 23:09:24.051885: step 53260, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 0.799 sec/batch; 61h:57m:44s remains)
INFO - root - 2017-12-05 23:09:32.661497: step 53270, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 66h:44m:00s remains)
INFO - root - 2017-12-05 23:09:41.315138: step 53280, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 66h:35m:23s remains)
INFO - root - 2017-12-05 23:09:49.793487: step 53290, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.801 sec/batch; 62h:08m:50s remains)
INFO - root - 2017-12-05 23:09:58.338316: step 53300, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 66h:16m:25s remains)
2017-12-05 23:09:59.148885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1621933 -4.1726379 -4.1860747 -4.2011614 -4.1990018 -4.1690016 -4.132688 -4.1305 -4.1611814 -4.2003951 -4.2470322 -4.2865114 -4.3142581 -4.3242478 -4.329793][-4.1745048 -4.1840081 -4.2001705 -4.2130103 -4.2018647 -4.1635904 -4.1228371 -4.1119413 -4.1339412 -4.1709585 -4.2229176 -4.2721329 -4.3086195 -4.3244128 -4.3298192][-4.1866355 -4.1909819 -4.2013702 -4.2090387 -4.1963949 -4.1620178 -4.1287413 -4.1176243 -4.1280642 -4.1537271 -4.200542 -4.2552252 -4.2992859 -4.3186865 -4.3255796][-4.2025533 -4.2058353 -4.2084746 -4.2075043 -4.1969171 -4.1744447 -4.1537786 -4.1501055 -4.1572714 -4.1725988 -4.208457 -4.25915 -4.3018556 -4.3190818 -4.3258009][-4.218852 -4.2236767 -4.2224393 -4.2159548 -4.2049465 -4.18894 -4.1748281 -4.17629 -4.1861219 -4.2018514 -4.2303052 -4.2751808 -4.3130589 -4.3262725 -4.3303623][-4.2280169 -4.2358942 -4.2314787 -4.2176991 -4.1980448 -4.1754241 -4.1560006 -4.160387 -4.18262 -4.2109165 -4.2437253 -4.2885208 -4.3231373 -4.3337154 -4.33636][-4.2297053 -4.2394381 -4.23148 -4.2035146 -4.1620588 -4.11306 -4.0735512 -4.0787983 -4.1250272 -4.179842 -4.2319503 -4.285749 -4.3212996 -4.332458 -4.3373251][-4.228754 -4.2349963 -4.2218885 -4.1785078 -4.1071839 -4.0191894 -3.9435561 -3.9404511 -4.0154715 -4.1103721 -4.1921644 -4.2616324 -4.3035603 -4.3199134 -4.3308821][-4.2352161 -4.2402406 -4.2277079 -4.1868505 -4.1136208 -4.0180063 -3.9285669 -3.9108429 -3.9854369 -4.0931106 -4.1849494 -4.2602344 -4.304163 -4.3212581 -4.3313241][-4.2435389 -4.2461805 -4.2374868 -4.2107949 -4.1628418 -4.0988646 -4.0387073 -4.0253072 -4.0733581 -4.1534677 -4.2255883 -4.2861481 -4.3194671 -4.3300538 -4.3355317][-4.2500467 -4.2488818 -4.2428718 -4.230226 -4.2078371 -4.1735754 -4.1403546 -4.134326 -4.1654372 -4.2172742 -4.2640605 -4.3075476 -4.3335509 -4.3397312 -4.3406391][-4.2525649 -4.2488284 -4.2448025 -4.2420597 -4.2337246 -4.2126904 -4.1863046 -4.1805239 -4.204411 -4.2419896 -4.2741871 -4.3083506 -4.3357806 -4.3456488 -4.3465862][-4.244945 -4.2408195 -4.2380815 -4.2385454 -4.231276 -4.205555 -4.1706986 -4.1586032 -4.1807876 -4.2180195 -4.2506042 -4.2860589 -4.3183188 -4.336019 -4.3434048][-4.2231789 -4.2178221 -4.2140975 -4.2128282 -4.2018108 -4.1657639 -4.1176224 -4.1008916 -4.1282892 -4.172616 -4.212348 -4.2534738 -4.2946129 -4.3209538 -4.3342733][-4.1908646 -4.1835608 -4.1776333 -4.1741815 -4.1603112 -4.1180787 -4.0655837 -4.0484228 -4.0801086 -4.1285114 -4.1728706 -4.2188463 -4.2657332 -4.298645 -4.3180127]]...]
INFO - root - 2017-12-05 23:10:07.716576: step 53310, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 68h:07m:55s remains)
INFO - root - 2017-12-05 23:10:16.278013: step 53320, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.863 sec/batch; 66h:54m:47s remains)
INFO - root - 2017-12-05 23:10:24.719045: step 53330, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 67h:39m:03s remains)
INFO - root - 2017-12-05 23:10:33.259393: step 53340, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.874 sec/batch; 67h:48m:16s remains)
INFO - root - 2017-12-05 23:10:41.795861: step 53350, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 66h:43m:23s remains)
INFO - root - 2017-12-05 23:10:50.384857: step 53360, loss = 2.09, batch loss = 2.03 (10.6 examples/sec; 0.754 sec/batch; 58h:29m:38s remains)
INFO - root - 2017-12-05 23:10:58.908058: step 53370, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.844 sec/batch; 65h:24m:22s remains)
INFO - root - 2017-12-05 23:11:07.457262: step 53380, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 66h:34m:52s remains)
INFO - root - 2017-12-05 23:11:16.099985: step 53390, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 66h:06m:40s remains)
INFO - root - 2017-12-05 23:11:24.567197: step 53400, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 68h:37m:31s remains)
2017-12-05 23:11:25.425343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1101727 -4.1161985 -4.139987 -4.1726241 -4.1950459 -4.1980891 -4.1794186 -4.1467643 -4.1199589 -4.1086144 -4.1247363 -4.1526365 -4.1638894 -4.1590848 -4.1600728][-4.1188507 -4.1226578 -4.14219 -4.171495 -4.1867046 -4.1832285 -4.161447 -4.1287427 -4.105278 -4.0956697 -4.1110163 -4.1352148 -4.1418638 -4.1376786 -4.1466551][-4.11424 -4.1121678 -4.1287146 -4.1545625 -4.1638842 -4.1534867 -4.1291475 -4.0983305 -4.0833321 -4.0789962 -4.0917444 -4.1082544 -4.1053562 -4.0918856 -4.1019387][-4.0812807 -4.0732222 -4.0870619 -4.1095791 -4.1178222 -4.10369 -4.0758214 -4.0541797 -4.0546312 -4.0622325 -4.0760183 -4.0851855 -4.0727563 -4.0511255 -4.0603418][-4.0276046 -4.0206337 -4.0331359 -4.0508208 -4.0529165 -4.0338912 -4.0092216 -4.0036926 -4.02392 -4.0447598 -4.0611267 -4.0733018 -4.0644035 -4.0485754 -4.0620351][-3.9893751 -3.9861646 -3.994329 -4.00621 -3.9983158 -3.9759798 -3.9599447 -3.9661574 -3.9954197 -4.0203924 -4.0398889 -4.0630827 -4.0704136 -4.0676193 -4.0857325][-4.0187025 -4.021431 -4.0329542 -4.0384521 -4.0259981 -3.9975061 -3.9700308 -3.9626677 -3.9798467 -4.00156 -4.0210567 -4.0515008 -4.0704069 -4.0751233 -4.0928903][-4.0915713 -4.0980749 -4.1176896 -4.12489 -4.1107521 -4.0725141 -4.0293403 -4.0007277 -3.996809 -4.0113273 -4.0297222 -4.0584712 -4.075613 -4.07388 -4.0742435][-4.1617236 -4.1673975 -4.1850867 -4.1944742 -4.1828623 -4.1448474 -4.0988927 -4.0621881 -4.0482435 -4.0591044 -4.0764151 -4.0929656 -4.0952148 -4.077755 -4.0552731][-4.195188 -4.193944 -4.2030683 -4.2121339 -4.2019567 -4.1720839 -4.1371541 -4.1059952 -4.0974307 -4.1157489 -4.1363378 -4.1468968 -4.1400256 -4.1098871 -4.06929][-4.1624656 -4.1563487 -4.1691165 -4.1874948 -4.1798139 -4.1528306 -4.1280489 -4.1097765 -4.1108418 -4.1397266 -4.1713076 -4.1896062 -4.1880469 -4.1622095 -4.1191664][-4.0845971 -4.084444 -4.113852 -4.1460061 -4.1450424 -4.1145391 -4.088603 -4.0785232 -4.0867233 -4.1197333 -4.164607 -4.2017646 -4.2168097 -4.2007351 -4.1711593][-4.025589 -4.0380139 -4.0794625 -4.116889 -4.1200886 -4.0887237 -4.0555911 -4.0432067 -4.0463266 -4.0728717 -4.1233544 -4.1762538 -4.2117414 -4.2049665 -4.18588][-4.0222921 -4.0422931 -4.0858517 -4.1219091 -4.1241684 -4.0941792 -4.0541358 -4.031415 -4.0198259 -4.0315008 -4.0720854 -4.1232872 -4.1684256 -4.1741753 -4.1668692][-4.0657439 -4.0805569 -4.1156292 -4.1458521 -4.1459823 -4.1189957 -4.0789561 -4.0496845 -4.0307736 -4.0282788 -4.044817 -4.078867 -4.1230268 -4.1402984 -4.1423383]]...]
INFO - root - 2017-12-05 23:11:33.888917: step 53410, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 65h:50m:11s remains)
INFO - root - 2017-12-05 23:11:42.454134: step 53420, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 66h:43m:06s remains)
INFO - root - 2017-12-05 23:11:50.994079: step 53430, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 66h:13m:05s remains)
INFO - root - 2017-12-05 23:11:59.569747: step 53440, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 66h:11m:27s remains)
INFO - root - 2017-12-05 23:12:08.162676: step 53450, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 66h:24m:55s remains)
INFO - root - 2017-12-05 23:12:16.681967: step 53460, loss = 2.05, batch loss = 1.99 (10.5 examples/sec; 0.759 sec/batch; 58h:47m:46s remains)
INFO - root - 2017-12-05 23:12:25.286886: step 53470, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.814 sec/batch; 63h:05m:05s remains)
INFO - root - 2017-12-05 23:12:33.734381: step 53480, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.828 sec/batch; 64h:11m:58s remains)
INFO - root - 2017-12-05 23:12:42.298137: step 53490, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 65h:46m:08s remains)
INFO - root - 2017-12-05 23:12:50.781746: step 53500, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 64h:58m:51s remains)
2017-12-05 23:12:51.569647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2263842 -4.2198415 -4.218739 -4.2255969 -4.2422013 -4.2587047 -4.2620959 -4.2484355 -4.2253013 -4.2085495 -4.198123 -4.1880188 -4.1833005 -4.193398 -4.2155929][-4.2188368 -4.2118049 -4.211987 -4.2177925 -4.2326512 -4.2451196 -4.245616 -4.2317309 -4.2104578 -4.1974421 -4.1935868 -4.1872172 -4.1819673 -4.1860685 -4.2011709][-4.2105293 -4.2045488 -4.2056189 -4.2111688 -4.2235279 -4.2337832 -4.2342811 -4.2235522 -4.2086582 -4.2030468 -4.2079811 -4.2098293 -4.20873 -4.2101035 -4.2161217][-4.2022924 -4.1967039 -4.1965318 -4.2005997 -4.2117743 -4.2224679 -4.2250113 -4.2206793 -4.2134237 -4.2126884 -4.2228656 -4.2300606 -4.2317495 -4.2316365 -4.2291536][-4.1986508 -4.1915989 -4.1871548 -4.1877704 -4.1980858 -4.2093349 -4.2135634 -4.2114997 -4.2067223 -4.2097836 -4.2233405 -4.2352395 -4.2418489 -4.2421355 -4.23389][-4.2000508 -4.194437 -4.1887221 -4.1883788 -4.1962376 -4.2011137 -4.1951985 -4.1825476 -4.1706958 -4.1762166 -4.1974487 -4.218298 -4.2346416 -4.2401037 -4.2309608][-4.2002544 -4.1996894 -4.1953645 -4.1947012 -4.1971822 -4.1891379 -4.1651006 -4.1298718 -4.1001639 -4.1085763 -4.1445532 -4.1807094 -4.2115531 -4.2270031 -4.221384][-4.1838379 -4.1864681 -4.179904 -4.1723065 -4.1644993 -4.1415215 -4.0968828 -4.0342293 -3.9864159 -4.0103092 -4.0766044 -4.1389761 -4.1911683 -4.2186966 -4.21865][-4.16678 -4.1688557 -4.1577125 -4.1407194 -4.1200347 -4.0835156 -4.0227313 -3.938292 -3.876966 -3.9203095 -4.0183616 -4.102849 -4.1664968 -4.20102 -4.2066226][-4.1669865 -4.1673617 -4.1535721 -4.1343546 -4.1129446 -4.0835924 -4.0365691 -3.9683552 -3.9198954 -3.9594355 -4.0465021 -4.116744 -4.1622977 -4.185091 -4.1912236][-4.1862035 -4.1845922 -4.1706071 -4.1574807 -4.1463995 -4.1333413 -4.1095924 -4.0698309 -4.0406036 -4.0637531 -4.1181951 -4.1552749 -4.1700454 -4.1728959 -4.1763086][-4.211473 -4.2120123 -4.2011585 -4.1945558 -4.1916337 -4.1865082 -4.1733327 -4.1492677 -4.1284637 -4.1346178 -4.1601291 -4.171226 -4.1628995 -4.1521568 -4.1555119][-4.2177463 -4.2195745 -4.2131147 -4.2116575 -4.2114434 -4.2074451 -4.1992788 -4.1852779 -4.1701388 -4.1660676 -4.1736217 -4.1705117 -4.15017 -4.1323891 -4.1367235][-4.2033477 -4.2030792 -4.1985335 -4.1978755 -4.1974134 -4.1948285 -4.1902924 -4.1818457 -4.1702452 -4.1635132 -4.1646857 -4.1601205 -4.1409531 -4.1246796 -4.1286569][-4.171257 -4.1669645 -4.1647172 -4.1673546 -4.1699824 -4.1708364 -4.1707058 -4.1677189 -4.1613827 -4.1577229 -4.1592479 -4.1585436 -4.1474662 -4.13789 -4.140048]]...]
INFO - root - 2017-12-05 23:13:00.025315: step 53510, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 66h:31m:23s remains)
INFO - root - 2017-12-05 23:13:08.589354: step 53520, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 64h:06m:02s remains)
INFO - root - 2017-12-05 23:13:17.262272: step 53530, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 64h:50m:45s remains)
INFO - root - 2017-12-05 23:13:25.861802: step 53540, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 67h:24m:16s remains)
INFO - root - 2017-12-05 23:13:34.394238: step 53550, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 68h:53m:30s remains)
INFO - root - 2017-12-05 23:13:42.927270: step 53560, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.775 sec/batch; 60h:03m:10s remains)
INFO - root - 2017-12-05 23:13:51.466697: step 53570, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 66h:32m:21s remains)
INFO - root - 2017-12-05 23:14:00.103536: step 53580, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 66h:36m:42s remains)
INFO - root - 2017-12-05 23:14:08.638308: step 53590, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 68h:04m:34s remains)
INFO - root - 2017-12-05 23:14:17.253469: step 53600, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 66h:51m:22s remains)
2017-12-05 23:14:18.016308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2630224 -4.2707806 -4.2747931 -4.28082 -4.2888479 -4.2946911 -4.294672 -4.2952075 -4.2945089 -4.2877097 -4.2747326 -4.2593613 -4.2445326 -4.23682 -4.2349615][-4.2354178 -4.2446461 -4.24862 -4.2565012 -4.2679462 -4.2775931 -4.2783833 -4.2814422 -4.2825804 -4.2760625 -4.2662277 -4.2518821 -4.2353759 -4.2248745 -4.2212396][-4.2031732 -4.2122149 -4.2170544 -4.2266903 -4.2415724 -4.2548628 -4.2582312 -4.2641478 -4.2678256 -4.2649922 -4.2622466 -4.2520275 -4.2351027 -4.220468 -4.2113261][-4.1603327 -4.169704 -4.1774092 -4.1916265 -4.2093749 -4.2245412 -4.2285395 -4.2368479 -4.2455773 -4.2500019 -4.2569609 -4.2552075 -4.2382865 -4.2147336 -4.1946926][-4.1135149 -4.1206937 -4.1274695 -4.1439581 -4.1632037 -4.1755314 -4.1768956 -4.188581 -4.2064009 -4.21911 -4.2377391 -4.2458177 -4.2301626 -4.19817 -4.1643786][-4.07839 -4.0831766 -4.087182 -4.0961471 -4.1076441 -4.1073852 -4.0943217 -4.1075597 -4.139657 -4.164124 -4.1945844 -4.2129788 -4.2032614 -4.1731997 -4.1391706][-4.0713964 -4.064 -4.0532207 -4.0404506 -4.0292768 -4.0027337 -3.9607112 -3.9726672 -4.0308 -4.0757 -4.1204171 -4.1559749 -4.16417 -4.1538429 -4.1361327][-4.1085329 -4.0937357 -4.0710268 -4.0369387 -4.0033636 -3.9512153 -3.8821557 -3.8856554 -3.9586735 -4.018466 -4.0698118 -4.1171236 -4.144484 -4.1578736 -4.1636033][-4.1805682 -4.174561 -4.1606083 -4.1293416 -4.0970526 -4.0523252 -4.00022 -3.9967315 -4.0431504 -4.0864615 -4.1253552 -4.165153 -4.1930447 -4.2114944 -4.2219238][-4.22449 -4.22807 -4.2250376 -4.2095594 -4.1918883 -4.1670575 -4.1404843 -4.1364737 -4.1594224 -4.1858344 -4.2136426 -4.2455788 -4.2678618 -4.2806706 -4.284174][-4.2277126 -4.2339492 -4.2406273 -4.2398415 -4.2386909 -4.2312136 -4.2229753 -4.2249303 -4.241868 -4.2645555 -4.2875476 -4.308238 -4.3212552 -4.3259182 -4.3231697][-4.2324038 -4.2427764 -4.2565875 -4.2642436 -4.2705483 -4.2720723 -4.2757082 -4.2848873 -4.3016677 -4.3192444 -4.3331037 -4.3413363 -4.3454137 -4.343411 -4.3369985][-4.2493467 -4.2662816 -4.2835555 -4.2939587 -4.3001442 -4.3027477 -4.30889 -4.3211823 -4.3370209 -4.3499312 -4.355617 -4.3546348 -4.3519969 -4.3472996 -4.3399849][-4.2603049 -4.2812977 -4.3002458 -4.3093238 -4.3122759 -4.3141518 -4.3212595 -4.3340855 -4.3474126 -4.3559375 -4.3576074 -4.3524771 -4.3467097 -4.3420815 -4.33588][-4.2700324 -4.2917056 -4.3070831 -4.3085432 -4.3072863 -4.3076849 -4.3149819 -4.3270807 -4.3387232 -4.3459272 -4.3469386 -4.3412819 -4.3341665 -4.3299317 -4.3260617]]...]
INFO - root - 2017-12-05 23:14:26.578545: step 53610, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 65h:18m:42s remains)
INFO - root - 2017-12-05 23:14:35.155392: step 53620, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 66h:15m:36s remains)
INFO - root - 2017-12-05 23:14:43.661236: step 53630, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 65h:08m:49s remains)
INFO - root - 2017-12-05 23:14:52.211126: step 53640, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 65h:15m:10s remains)
INFO - root - 2017-12-05 23:15:00.791792: step 53650, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 65h:45m:34s remains)
INFO - root - 2017-12-05 23:15:09.174052: step 53660, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.796 sec/batch; 61h:37m:58s remains)
INFO - root - 2017-12-05 23:15:17.682671: step 53670, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 66h:07m:03s remains)
INFO - root - 2017-12-05 23:15:26.350243: step 53680, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 66h:31m:57s remains)
INFO - root - 2017-12-05 23:15:34.804559: step 53690, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 67h:28m:38s remains)
INFO - root - 2017-12-05 23:15:43.426113: step 53700, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 66h:24m:22s remains)
2017-12-05 23:15:44.217702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1593952 -4.1324797 -4.1100607 -4.0808592 -4.0487914 -4.0421543 -4.0718613 -4.1312904 -4.1712551 -4.1695514 -4.1469355 -4.1101189 -4.0862317 -4.0830016 -4.1019168][-4.1274624 -4.1025953 -4.0799155 -4.0549965 -4.029634 -4.0418363 -4.0878224 -4.1455193 -4.1738558 -4.1616297 -4.1318059 -4.0990267 -4.0836835 -4.0865431 -4.1040263][-4.1047006 -4.0912352 -4.0773106 -4.0606556 -4.0455751 -4.0638428 -4.1063447 -4.1494269 -4.164165 -4.1482816 -4.120214 -4.0962629 -4.0923076 -4.0994263 -4.1103225][-4.0903664 -4.090085 -4.0896521 -4.0856042 -4.0828609 -4.0974107 -4.1273193 -4.151618 -4.1520982 -4.1351895 -4.1117725 -4.0966654 -4.101 -4.1120005 -4.1214][-4.0981712 -4.1069984 -4.1136956 -4.1168737 -4.11809 -4.1252589 -4.1428947 -4.154839 -4.1509128 -4.1360812 -4.117403 -4.109508 -4.1199522 -4.1320949 -4.1365471][-4.114584 -4.1271729 -4.1391344 -4.1467171 -4.1464438 -4.1452894 -4.1513381 -4.1556845 -4.1523142 -4.1451507 -4.138268 -4.1396093 -4.1543131 -4.1628962 -4.1527171][-4.1179104 -4.1343546 -4.1534424 -4.1723475 -4.1794934 -4.1764112 -4.16856 -4.1608973 -4.1553144 -4.1544681 -4.1567307 -4.168961 -4.1840754 -4.1868758 -4.1688561][-4.1200638 -4.1435204 -4.1699 -4.1970668 -4.207725 -4.2013006 -4.1886625 -4.1760583 -4.1680532 -4.1685667 -4.1745596 -4.19095 -4.20452 -4.1971469 -4.1729856][-4.1104875 -4.1411858 -4.1728358 -4.2017593 -4.2155757 -4.2140031 -4.2064838 -4.1956367 -4.1867213 -4.1836472 -4.1845922 -4.1957397 -4.2073145 -4.1975417 -4.1761813][-4.1105266 -4.1456532 -4.1774721 -4.2056904 -4.22124 -4.2207775 -4.2151785 -4.2089152 -4.2020478 -4.1962409 -4.1898375 -4.1924949 -4.2035542 -4.2021427 -4.1874456][-4.1244617 -4.1562181 -4.1774545 -4.1976814 -4.2135029 -4.2179747 -4.215085 -4.212326 -4.2077174 -4.1995678 -4.1865578 -4.1830425 -4.1937408 -4.1988354 -4.1951423][-4.1590123 -4.1807866 -4.187861 -4.1969972 -4.2119169 -4.2229681 -4.2264347 -4.2253938 -4.2198749 -4.208221 -4.1918664 -4.18068 -4.1843204 -4.187315 -4.1913815][-4.1966391 -4.2074556 -4.2019119 -4.1998634 -4.2117705 -4.2276607 -4.2380176 -4.2418432 -4.23834 -4.2261958 -4.20786 -4.1922917 -4.1904087 -4.1908455 -4.1942139][-4.2045827 -4.2086425 -4.1993127 -4.1926913 -4.2012367 -4.2185569 -4.234973 -4.2467384 -4.2499247 -4.2385249 -4.2202358 -4.2049875 -4.2001357 -4.1996412 -4.2024441][-4.1992874 -4.2014394 -4.1956682 -4.1894479 -4.193819 -4.2081552 -4.225111 -4.238903 -4.2445784 -4.2325196 -4.2134013 -4.1989818 -4.192903 -4.1914206 -4.1953793]]...]
INFO - root - 2017-12-05 23:15:52.827754: step 53710, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 67h:11m:09s remains)
INFO - root - 2017-12-05 23:16:01.327603: step 53720, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 67h:26m:49s remains)
INFO - root - 2017-12-05 23:16:09.849579: step 53730, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 67h:11m:53s remains)
INFO - root - 2017-12-05 23:16:18.339993: step 53740, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.810 sec/batch; 62h:45m:04s remains)
INFO - root - 2017-12-05 23:16:26.932481: step 53750, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 66h:30m:15s remains)
INFO - root - 2017-12-05 23:16:35.366135: step 53760, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.750 sec/batch; 58h:06m:18s remains)
INFO - root - 2017-12-05 23:16:43.824934: step 53770, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 65h:22m:32s remains)
INFO - root - 2017-12-05 23:16:52.398143: step 53780, loss = 2.10, batch loss = 2.05 (9.4 examples/sec; 0.851 sec/batch; 65h:51m:33s remains)
INFO - root - 2017-12-05 23:17:00.884540: step 53790, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 67h:36m:39s remains)
INFO - root - 2017-12-05 23:17:09.429145: step 53800, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 66h:55m:55s remains)
2017-12-05 23:17:10.180071: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1741109 -4.1816649 -4.1980662 -4.2083893 -4.204196 -4.1933675 -4.19054 -4.1957135 -4.19491 -4.1948514 -4.1789937 -4.1401625 -4.0819135 -4.0418944 -4.0537009][-4.1594238 -4.1704941 -4.1897335 -4.2017441 -4.2027373 -4.1965766 -4.19535 -4.2039142 -4.2084241 -4.2113748 -4.1951895 -4.1555319 -4.0976233 -4.0510149 -4.0502682][-4.1691346 -4.1822758 -4.1985321 -4.2037234 -4.1977797 -4.1835823 -4.1763797 -4.1825442 -4.1942763 -4.2066932 -4.2044754 -4.1882291 -4.1562252 -4.1253819 -4.1213784][-4.1929407 -4.1982908 -4.2048578 -4.1962857 -4.170166 -4.1394057 -4.1169839 -4.1154504 -4.13108 -4.1535025 -4.1697984 -4.1823 -4.1821055 -4.1788254 -4.1866903][-4.2171183 -4.204258 -4.1938543 -4.1668019 -4.1133938 -4.0513477 -4.004734 -3.9853036 -3.9998856 -4.0375972 -4.0700459 -4.1031094 -4.1327467 -4.1583285 -4.1894207][-4.229846 -4.20434 -4.1787724 -4.133553 -4.0520353 -3.953469 -3.8707457 -3.8172998 -3.8245013 -3.878911 -3.9224482 -3.9708424 -4.021903 -4.0702496 -4.1316628][-4.2318997 -4.2065406 -4.1702166 -4.1115632 -4.0164962 -3.9003439 -3.7907562 -3.710021 -3.7065272 -3.7644804 -3.7979484 -3.8462431 -3.9145308 -3.9802027 -4.0596037][-4.2278566 -4.2169962 -4.1900697 -4.1400652 -4.0619164 -3.9678895 -3.8779566 -3.8124242 -3.8069553 -3.8342328 -3.8247504 -3.8379147 -3.8951435 -3.9527659 -4.0248566][-4.2229228 -4.2247982 -4.2146292 -4.1912689 -4.1474109 -4.0884748 -4.03009 -3.9858289 -3.975378 -3.9739614 -3.9368968 -3.9168262 -3.9483106 -3.9846992 -4.0364766][-4.2237048 -4.2338142 -4.2371273 -4.2333736 -4.2175364 -4.186018 -4.1481733 -4.1174288 -4.1018357 -4.0865421 -4.0467997 -4.0181041 -4.0276484 -4.0422368 -4.0755119][-4.23594 -4.2516866 -4.2617574 -4.2674985 -4.2652507 -4.2522469 -4.2282629 -4.2057943 -4.1898804 -4.1762042 -4.1511045 -4.1284103 -4.1234188 -4.1210227 -4.1367159][-4.2633724 -4.2769451 -4.2835131 -4.2890592 -4.2886238 -4.28377 -4.2703443 -4.2574849 -4.2500753 -4.2471528 -4.238657 -4.2278357 -4.2201166 -4.2097144 -4.2099552][-4.2885957 -4.2993584 -4.3011265 -4.3017473 -4.2994885 -4.2940121 -4.2844682 -4.2782021 -4.2801914 -4.2864561 -4.2892876 -4.2900081 -4.2864952 -4.2762074 -4.2700267][-4.2978988 -4.3036747 -4.3022151 -4.3016119 -4.3007655 -4.2944717 -4.2878032 -4.2839608 -4.29097 -4.2998433 -4.3060737 -4.3118749 -4.3123369 -4.3056889 -4.3002][-4.2934113 -4.2946754 -4.288784 -4.28678 -4.2866659 -4.2832932 -4.2820244 -4.2830667 -4.2927928 -4.3022037 -4.3089337 -4.3137059 -4.3150835 -4.3105917 -4.3048172]]...]
INFO - root - 2017-12-05 23:17:18.771977: step 53810, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 68h:16m:33s remains)
INFO - root - 2017-12-05 23:17:27.346181: step 53820, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 66h:53m:47s remains)
INFO - root - 2017-12-05 23:17:35.878762: step 53830, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 67h:11m:29s remains)
INFO - root - 2017-12-05 23:17:44.474625: step 53840, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 68h:11m:36s remains)
INFO - root - 2017-12-05 23:17:53.114958: step 53850, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 67h:37m:43s remains)
INFO - root - 2017-12-05 23:18:01.666418: step 53860, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.761 sec/batch; 58h:53m:06s remains)
INFO - root - 2017-12-05 23:18:10.126409: step 53870, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 65h:10m:56s remains)
INFO - root - 2017-12-05 23:18:18.542281: step 53880, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 67h:34m:26s remains)
INFO - root - 2017-12-05 23:18:27.067387: step 53890, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 0.815 sec/batch; 63h:05m:19s remains)
INFO - root - 2017-12-05 23:18:35.588329: step 53900, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 66h:03m:05s remains)
2017-12-05 23:18:36.327336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.288404 -4.2889166 -4.2876158 -4.2862096 -4.2863045 -4.2879596 -4.2906585 -4.2936969 -4.2955709 -4.2963638 -4.2965117 -4.2956305 -4.2943211 -4.2930942 -4.2926297][-4.2742071 -4.2758946 -4.2750912 -4.2735453 -4.2735844 -4.2757406 -4.27902 -4.283318 -4.2870073 -4.2895045 -4.2901549 -4.2884407 -4.2858825 -4.2831268 -4.2813597][-4.2547741 -4.2594175 -4.2601538 -4.2589917 -4.2583141 -4.2596693 -4.2625141 -4.2676415 -4.272841 -4.2770195 -4.2786813 -4.276546 -4.271431 -4.2649627 -4.2599168][-4.2457309 -4.2516937 -4.2515635 -4.2471137 -4.2412295 -4.2370205 -4.2361031 -4.2414231 -4.2502956 -4.2598276 -4.266099 -4.2668781 -4.2613916 -4.2517905 -4.2428193][-4.2409921 -4.2447257 -4.2421236 -4.2325082 -4.2190237 -4.2064476 -4.1991172 -4.2013388 -4.2113004 -4.22682 -4.2414289 -4.2500238 -4.2509079 -4.245121 -4.2370424][-4.21863 -4.2199435 -4.213851 -4.1997757 -4.181283 -4.1624327 -4.1478715 -4.14372 -4.1514988 -4.167932 -4.1878991 -4.2059913 -4.2199149 -4.225883 -4.2262206][-4.17982 -4.1790075 -4.1704636 -4.1525865 -4.131249 -4.1076179 -4.0843625 -4.0711908 -4.0712609 -4.0834951 -4.1042724 -4.1297607 -4.15831 -4.1793 -4.1925364][-4.1506481 -4.149159 -4.1399069 -4.1242275 -4.1058221 -4.0814495 -4.0516605 -4.0316834 -4.02569 -4.0289564 -4.0406079 -4.0615182 -4.0950146 -4.1250372 -4.1490541][-4.1471877 -4.1477795 -4.141819 -4.1323209 -4.1201811 -4.0989 -4.0703487 -4.0493841 -4.0414028 -4.0401058 -4.0436773 -4.0551548 -4.0802975 -4.1050062 -4.1272097][-4.1611729 -4.1649394 -4.1670618 -4.167233 -4.16416 -4.1506062 -4.1279049 -4.1088452 -4.099628 -4.0949874 -4.0940204 -4.0989089 -4.1132579 -4.1252809 -4.1361375][-4.18641 -4.19046 -4.1988168 -4.20897 -4.2168922 -4.2134461 -4.2014909 -4.1915116 -4.1866817 -4.1796775 -4.1715436 -4.1672015 -4.1690412 -4.1674223 -4.1657548][-4.2043657 -4.2004862 -4.2057219 -4.2197866 -4.2353759 -4.2417011 -4.2416315 -4.2447691 -4.2493172 -4.2441764 -4.2337632 -4.2259812 -4.2223935 -4.2126265 -4.201057][-4.2060084 -4.1967187 -4.1958637 -4.2056956 -4.2227578 -4.2359457 -4.2439404 -4.2556429 -4.2665806 -4.2657456 -4.2561631 -4.248095 -4.24469 -4.2335758 -4.218102][-4.2109089 -4.2024841 -4.196032 -4.1989317 -4.2123322 -4.2255306 -4.2353721 -4.2497492 -4.2631259 -4.2643957 -4.2548518 -4.24645 -4.2457662 -4.2409172 -4.2319565][-4.2231927 -4.2192931 -4.2104354 -4.2054524 -4.2121325 -4.2207022 -4.2263908 -4.2387471 -4.2522817 -4.2571192 -4.2511892 -4.2456379 -4.2482271 -4.250216 -4.2496872]]...]
INFO - root - 2017-12-05 23:18:44.859816: step 53910, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 66h:45m:07s remains)
INFO - root - 2017-12-05 23:18:53.380302: step 53920, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 64h:03m:07s remains)
INFO - root - 2017-12-05 23:19:01.861218: step 53930, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.779 sec/batch; 60h:14m:46s remains)
INFO - root - 2017-12-05 23:19:10.414580: step 53940, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 66h:38m:35s remains)
INFO - root - 2017-12-05 23:19:18.863161: step 53950, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 66h:25m:48s remains)
INFO - root - 2017-12-05 23:19:27.281432: step 53960, loss = 2.05, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 56h:57m:53s remains)
INFO - root - 2017-12-05 23:19:35.823127: step 53970, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 67h:12m:32s remains)
INFO - root - 2017-12-05 23:19:44.468571: step 53980, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 68h:10m:32s remains)
INFO - root - 2017-12-05 23:19:52.922305: step 53990, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 67h:11m:02s remains)
INFO - root - 2017-12-05 23:20:01.460895: step 54000, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 66h:17m:51s remains)
2017-12-05 23:20:02.323945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2306542 -4.2210879 -4.2136531 -4.2087054 -4.2096968 -4.2226453 -4.2372584 -4.2434888 -4.2362833 -4.2171912 -4.2001657 -4.2075353 -4.2144504 -4.19634 -4.1864886][-4.1977019 -4.1910615 -4.1825557 -4.1728849 -4.1700869 -4.1844149 -4.2074265 -4.2229195 -4.2235084 -4.210381 -4.1954346 -4.2022309 -4.2038679 -4.1830316 -4.1720791][-4.16411 -4.1639237 -4.155025 -4.139914 -4.1307154 -4.139255 -4.1636825 -4.1877465 -4.2070394 -4.2110829 -4.2056642 -4.2062974 -4.193676 -4.1651564 -4.1507268][-4.148725 -4.1520839 -4.1385202 -4.121026 -4.1034875 -4.0916629 -4.0985732 -4.1259875 -4.1693125 -4.1969266 -4.2005725 -4.1946158 -4.1671653 -4.1281366 -4.107532][-4.1532106 -4.1546841 -4.1342006 -4.1123171 -4.0843062 -4.0432963 -4.0151424 -4.0383787 -4.1127448 -4.1729522 -4.1923494 -4.191884 -4.1569924 -4.1076913 -4.072423][-4.1677194 -4.1638107 -4.1383529 -4.1027389 -4.0590525 -3.989342 -3.9093552 -3.9170871 -4.0304213 -4.1375608 -4.1832337 -4.1970773 -4.173358 -4.1210313 -4.0701036][-4.1856561 -4.1778421 -4.1443157 -4.0982075 -4.0409889 -3.9448371 -3.7983985 -3.76017 -3.9109905 -4.0765591 -4.1597824 -4.1893439 -4.1824608 -4.1395812 -4.0884762][-4.1961479 -4.1850452 -4.1476679 -4.1006675 -4.0468149 -3.9441035 -3.7544184 -3.6601238 -3.8265605 -4.0272222 -4.1400881 -4.1810603 -4.1776953 -4.1431284 -4.0942817][-4.1945148 -4.1836438 -4.1526856 -4.1172328 -4.0766907 -3.9996536 -3.8453116 -3.754663 -3.8780718 -4.0502729 -4.1546454 -4.1906633 -4.1727982 -4.1270428 -4.0793209][-4.1797686 -4.176589 -4.1673951 -4.1519985 -4.1199913 -4.0668297 -3.9727309 -3.9148016 -3.9903586 -4.1105666 -4.1863112 -4.2107816 -4.1763062 -4.1143751 -4.0663819][-4.1665034 -4.1775713 -4.188417 -4.1883454 -4.1654043 -4.1275563 -4.0744586 -4.0410094 -4.0819736 -4.1533751 -4.202219 -4.2172251 -4.1758833 -4.1023741 -4.0566244][-4.1619687 -4.1934605 -4.2106724 -4.2118173 -4.2004132 -4.1759691 -4.14541 -4.1232538 -4.1387243 -4.173779 -4.2038918 -4.2199082 -4.1837025 -4.1167073 -4.0827723][-4.1629229 -4.2033834 -4.2254558 -4.2256222 -4.2160506 -4.1997023 -4.1836591 -4.1728287 -4.1749377 -4.1858182 -4.2070088 -4.2244253 -4.1951442 -4.1457329 -4.12597][-4.1677389 -4.2091489 -4.233613 -4.2326288 -4.2199807 -4.2044005 -4.1976171 -4.200562 -4.2033024 -4.2036967 -4.2179866 -4.22878 -4.2076254 -4.17717 -4.1702967][-4.1917167 -4.2264862 -4.2442789 -4.2397227 -4.2254014 -4.2091231 -4.20353 -4.2135777 -4.2208333 -4.22007 -4.2281704 -4.2367249 -4.2290916 -4.2169719 -4.2202282]]...]
INFO - root - 2017-12-05 23:20:10.856036: step 54010, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 64h:43m:26s remains)
INFO - root - 2017-12-05 23:20:19.417894: step 54020, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 65h:49m:31s remains)
INFO - root - 2017-12-05 23:20:27.995359: step 54030, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 67h:12m:00s remains)
INFO - root - 2017-12-05 23:20:36.343078: step 54040, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 65h:00m:19s remains)
INFO - root - 2017-12-05 23:20:44.929940: step 54050, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.821 sec/batch; 63h:29m:09s remains)
INFO - root - 2017-12-05 23:20:53.343796: step 54060, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 51h:33m:15s remains)
INFO - root - 2017-12-05 23:21:01.828297: step 54070, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 66h:27m:34s remains)
INFO - root - 2017-12-05 23:21:10.346638: step 54080, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 66h:40m:47s remains)
INFO - root - 2017-12-05 23:21:18.996362: step 54090, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 68h:31m:49s remains)
INFO - root - 2017-12-05 23:21:27.568573: step 54100, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 66h:03m:12s remains)
2017-12-05 23:21:28.304814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24291 -4.2397995 -4.2289581 -4.2261062 -4.2310896 -4.2371411 -4.2398434 -4.2421026 -4.2440434 -4.2485251 -4.2555041 -4.26329 -4.2695827 -4.2711325 -4.27219][-4.243186 -4.2388773 -4.2260842 -4.2202921 -4.2232761 -4.2270126 -4.2259984 -4.2242503 -4.2236505 -4.227313 -4.2352405 -4.2445865 -4.2496891 -4.2474747 -4.2455297][-4.2298608 -4.227747 -4.219666 -4.2173395 -4.2178555 -4.2173548 -4.2142034 -4.2124124 -4.2111955 -4.215992 -4.2273049 -4.238338 -4.2416148 -4.2332187 -4.2271934][-4.2020464 -4.2037921 -4.2044697 -4.2067552 -4.2041068 -4.1992774 -4.1962948 -4.1975031 -4.1979537 -4.203558 -4.2160659 -4.2280908 -4.2308264 -4.2179613 -4.2089715][-4.1750607 -4.1737843 -4.1779938 -4.1855211 -4.1842427 -4.1796708 -4.179163 -4.1822777 -4.1841831 -4.18643 -4.1930523 -4.2023478 -4.2053781 -4.1932907 -4.1845613][-4.1589437 -4.1427522 -4.14111 -4.1538706 -4.1592078 -4.1605577 -4.165041 -4.1693411 -4.1645803 -4.1549063 -4.152245 -4.1615458 -4.1681237 -4.1597972 -4.1545758][-4.157331 -4.1246471 -4.1120224 -4.125814 -4.1402044 -4.1531329 -4.1627913 -4.1580319 -4.13511 -4.1121964 -4.1081934 -4.1260071 -4.14279 -4.1406341 -4.1358404][-4.1785159 -4.1382365 -4.1148224 -4.1212082 -4.138 -4.159668 -4.1733584 -4.1577621 -4.1183681 -4.0883923 -4.091146 -4.1184483 -4.1418939 -4.1461692 -4.1421757][-4.2087603 -4.1753225 -4.1495304 -4.1461382 -4.1531487 -4.1694751 -4.1822114 -4.164062 -4.1220212 -4.0952134 -4.1054482 -4.137291 -4.1624703 -4.1705055 -4.1697989][-4.2299619 -4.2063093 -4.1815062 -4.1692667 -4.1633482 -4.1712193 -4.1808791 -4.1672058 -4.1346607 -4.1153612 -4.129127 -4.1587152 -4.1819768 -4.1949849 -4.1995287][-4.2390122 -4.2219858 -4.1987529 -4.1817732 -4.1695161 -4.1701145 -4.1761951 -4.1693773 -4.1499887 -4.139071 -4.1514874 -4.1742587 -4.1952553 -4.2111397 -4.2201772][-4.2325916 -4.2220573 -4.20453 -4.18952 -4.1780429 -4.1784258 -4.1844568 -4.1831894 -4.1756659 -4.1737452 -4.1870461 -4.2072778 -4.225338 -4.2396579 -4.2477818][-4.2228308 -4.2216926 -4.2156 -4.2086687 -4.2022614 -4.2034931 -4.2109685 -4.2166414 -4.2188163 -4.2237906 -4.2357345 -4.2512627 -4.2649417 -4.27419 -4.2788773][-4.2288523 -4.2336454 -4.2340631 -4.2315516 -4.2271852 -4.2266517 -4.2311478 -4.2377663 -4.2447772 -4.252377 -4.2628694 -4.276794 -4.2901449 -4.3003035 -4.3059473][-4.2391562 -4.243731 -4.2453246 -4.2433777 -4.2393823 -4.2392554 -4.2430191 -4.2506046 -4.2600856 -4.2690897 -4.279922 -4.29313 -4.3058987 -4.3172889 -4.3255773]]...]
INFO - root - 2017-12-05 23:21:36.771945: step 54110, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 65h:58m:52s remains)
INFO - root - 2017-12-05 23:21:45.343789: step 54120, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 65h:46m:48s remains)
INFO - root - 2017-12-05 23:21:53.919246: step 54130, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 67h:57m:28s remains)
INFO - root - 2017-12-05 23:22:02.511781: step 54140, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 64h:21m:56s remains)
INFO - root - 2017-12-05 23:22:11.180217: step 54150, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 68h:08m:27s remains)
INFO - root - 2017-12-05 23:22:19.712792: step 54160, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 0.782 sec/batch; 60h:28m:51s remains)
INFO - root - 2017-12-05 23:22:28.248009: step 54170, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 68h:59m:28s remains)
INFO - root - 2017-12-05 23:22:36.867588: step 54180, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 67h:39m:55s remains)
INFO - root - 2017-12-05 23:22:45.367283: step 54190, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 65h:15m:33s remains)
INFO - root - 2017-12-05 23:22:53.991562: step 54200, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 65h:59m:37s remains)
2017-12-05 23:22:54.779141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1561747 -4.1071472 -4.0763874 -4.0446377 -4.0074039 -3.9871287 -3.9549861 -3.9084787 -3.9211993 -3.9786723 -4.03098 -4.0510397 -4.063303 -4.0759478 -4.0785241][-4.1516676 -4.0971532 -4.0606718 -4.0178137 -3.9760652 -3.9677868 -3.9535508 -3.9209342 -3.9375377 -3.995589 -4.0495152 -4.0692921 -4.078476 -4.0890217 -4.0848713][-4.1506233 -4.0963655 -4.0613737 -4.0184855 -3.9776278 -3.9797306 -3.9781199 -3.9476349 -3.9535377 -4.00525 -4.0673285 -4.0950847 -4.0982051 -4.1010289 -4.0966][-4.1584897 -4.1059003 -4.07247 -4.0342321 -3.9945376 -3.9968305 -3.9922738 -3.946909 -3.9314237 -3.9832644 -4.0643387 -4.1126814 -4.1226835 -4.1226192 -4.1160278][-4.1681595 -4.1153736 -4.0807376 -4.0461488 -4.0079045 -4.0020256 -3.981745 -3.9125788 -3.8788755 -3.9420285 -4.047121 -4.10789 -4.1278434 -4.1395912 -4.1401515][-4.1767864 -4.124445 -4.0897417 -4.0622783 -4.0298386 -4.0084977 -3.9560602 -3.8481457 -3.7960835 -3.8826051 -4.01088 -4.08525 -4.1194663 -4.1511717 -4.1679435][-4.1857285 -4.1354303 -4.1018395 -4.08339 -4.0567536 -4.0204597 -3.9326305 -3.7901318 -3.7391753 -3.8522382 -3.9884636 -4.0607738 -4.1000094 -4.1442175 -4.17492][-4.1933465 -4.1443696 -4.1144514 -4.1035013 -4.0852704 -4.0529408 -3.9691532 -3.8460274 -3.8195546 -3.9184775 -4.0203166 -4.065033 -4.089642 -4.1304193 -4.1638117][-4.2001629 -4.1534853 -4.1273823 -4.12599 -4.12316 -4.1086507 -4.0550671 -3.9734142 -3.9647431 -4.0356793 -4.0970426 -4.1132426 -4.1174335 -4.1409812 -4.1639371][-4.2082872 -4.1636992 -4.1405878 -4.1476407 -4.156146 -4.1554279 -4.1241808 -4.0744224 -4.0781283 -4.1289058 -4.1637769 -4.1641431 -4.1555753 -4.1579537 -4.1615181][-4.2174234 -4.1762276 -4.1565161 -4.1685181 -4.1828427 -4.188221 -4.1702404 -4.1398225 -4.1468754 -4.1808252 -4.1948586 -4.182065 -4.1662292 -4.1572452 -4.1507559][-4.2248893 -4.1877337 -4.1710925 -4.1815314 -4.1939816 -4.2030625 -4.1943746 -4.1690049 -4.1684389 -4.1896019 -4.1922159 -4.1738076 -4.1556072 -4.1454759 -4.1393228][-4.2321224 -4.1970339 -4.1816607 -4.1883106 -4.19621 -4.2077346 -4.2039967 -4.1753483 -4.1684885 -4.1862197 -4.1876268 -4.1714597 -4.1554103 -4.1497493 -4.1480207][-4.2378063 -4.203753 -4.1879807 -4.1901612 -4.1933045 -4.2037854 -4.199749 -4.1715484 -4.1691618 -4.193687 -4.2018375 -4.191968 -4.1807594 -4.1781635 -4.1787004][-4.2427568 -4.2096248 -4.1933689 -4.1920819 -4.1921334 -4.20116 -4.1960859 -4.1705742 -4.17428 -4.206089 -4.2217431 -4.218915 -4.2126169 -4.21303 -4.2160058]]...]
INFO - root - 2017-12-05 23:23:03.219988: step 54210, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 63h:46m:02s remains)
INFO - root - 2017-12-05 23:23:11.876404: step 54220, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 66h:54m:11s remains)
INFO - root - 2017-12-05 23:23:20.473299: step 54230, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 64h:21m:02s remains)
INFO - root - 2017-12-05 23:23:29.091053: step 54240, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 65h:19m:51s remains)
INFO - root - 2017-12-05 23:23:37.566681: step 54250, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 65h:15m:44s remains)
INFO - root - 2017-12-05 23:23:46.099496: step 54260, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.819 sec/batch; 63h:15m:54s remains)
INFO - root - 2017-12-05 23:23:54.696310: step 54270, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 67h:07m:42s remains)
INFO - root - 2017-12-05 23:24:03.341459: step 54280, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 66h:08m:15s remains)
INFO - root - 2017-12-05 23:24:11.900270: step 54290, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 64h:04m:57s remains)
INFO - root - 2017-12-05 23:24:20.478574: step 54300, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 65h:34m:18s remains)
2017-12-05 23:24:21.219021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.236578 -4.2225204 -4.1949115 -4.1560664 -4.1213069 -4.0743279 -4.0333161 -4.037118 -4.0802469 -4.13754 -4.1570473 -4.127378 -4.0792451 -4.0748215 -4.108119][-4.2376308 -4.2238445 -4.1957288 -4.1564131 -4.1185136 -4.0652952 -4.0198927 -4.0294437 -4.0765195 -4.1345677 -4.1658187 -4.1616335 -4.1312027 -4.1196752 -4.1284223][-4.240212 -4.225503 -4.1987734 -4.1578794 -4.1180992 -4.05238 -3.9880366 -3.9920106 -4.0447946 -4.1088347 -4.146493 -4.1596746 -4.1451044 -4.1360579 -4.137969][-4.2430825 -4.2245884 -4.1932063 -4.1471133 -4.1024933 -4.0310707 -3.9513936 -3.944768 -3.9992008 -4.0711727 -4.1183667 -4.1423178 -4.1481886 -4.1500731 -4.1471467][-4.2449083 -4.2251596 -4.1896992 -4.1400032 -4.0879173 -4.0056853 -3.9104135 -3.8922288 -3.9540765 -4.0340147 -4.0937047 -4.13319 -4.1566515 -4.1674414 -4.1549625][-4.2444391 -4.2259636 -4.18908 -4.137126 -4.0775447 -3.9772761 -3.8548608 -3.82399 -3.9036081 -4.0000758 -4.0765214 -4.1330881 -4.1648879 -4.1778069 -4.156539][-4.2423267 -4.221662 -4.1822028 -4.1282873 -4.060451 -3.9364238 -3.7856426 -3.7539828 -3.8650429 -3.9910507 -4.0869532 -4.1532507 -4.190289 -4.197165 -4.1636295][-4.2394271 -4.2160978 -4.1706862 -4.1116195 -4.0362239 -3.8990369 -3.741035 -3.7306092 -3.8660095 -4.0102816 -4.1157422 -4.1850786 -4.2213984 -4.2186208 -4.1686382][-4.2362318 -4.2114792 -4.1661568 -4.1090474 -4.0366073 -3.9060056 -3.7728229 -3.7906833 -3.9249213 -4.0584192 -4.1568193 -4.2165785 -4.2422109 -4.2225494 -4.1534786][-4.2344246 -4.2087536 -4.1705265 -4.1246672 -4.0628619 -3.9525979 -3.8554242 -3.8912725 -4.0069757 -4.1140685 -4.1944022 -4.2379136 -4.2472863 -4.2097487 -4.1321082][-4.2349706 -4.2119026 -4.1819773 -4.1485934 -4.0988235 -4.0140276 -3.9509838 -3.9908285 -4.0823793 -4.1628561 -4.2238073 -4.2513204 -4.2469149 -4.2005515 -4.13256][-4.2370062 -4.2189741 -4.198575 -4.1734118 -4.132988 -4.0730886 -4.0396495 -4.0785127 -4.1503983 -4.2110314 -4.254571 -4.2686906 -4.254818 -4.205749 -4.1515584][-4.24102 -4.2273607 -4.2148747 -4.1975775 -4.1680574 -4.1319389 -4.122045 -4.1573782 -4.2125893 -4.2584262 -4.2849059 -4.2870674 -4.267611 -4.2215009 -4.1790247][-4.2457752 -4.2359333 -4.2313075 -4.2230759 -4.2042069 -4.1890793 -4.1953192 -4.2250886 -4.2657228 -4.2958231 -4.3052492 -4.2966404 -4.2746396 -4.23916 -4.20995][-4.2497511 -4.2424107 -4.2438216 -4.2406678 -4.2286277 -4.2263246 -4.2395926 -4.2647858 -4.29341 -4.3084679 -4.3066134 -4.294014 -4.2764478 -4.2563276 -4.2404413]]...]
INFO - root - 2017-12-05 23:24:29.801213: step 54310, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 65h:51m:09s remains)
INFO - root - 2017-12-05 23:24:38.410861: step 54320, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 66h:33m:26s remains)
INFO - root - 2017-12-05 23:24:47.044236: step 54330, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 67h:37m:58s remains)
INFO - root - 2017-12-05 23:24:55.528193: step 54340, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 65h:40m:00s remains)
INFO - root - 2017-12-05 23:25:04.049982: step 54350, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 66h:48m:56s remains)
INFO - root - 2017-12-05 23:25:12.501777: step 54360, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.778 sec/batch; 60h:07m:46s remains)
INFO - root - 2017-12-05 23:25:21.051472: step 54370, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 67h:52m:38s remains)
INFO - root - 2017-12-05 23:25:29.692077: step 54380, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 68h:05m:02s remains)
INFO - root - 2017-12-05 23:25:38.251905: step 54390, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 66h:09m:47s remains)
INFO - root - 2017-12-05 23:25:46.816617: step 54400, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 65h:05m:17s remains)
2017-12-05 23:25:47.586690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3154068 -4.3013082 -4.2800217 -4.2466784 -4.2027283 -4.1633511 -4.144412 -4.159934 -4.1993508 -4.2423129 -4.2810721 -4.2844682 -4.2734962 -4.2530603 -4.2160387][-4.312839 -4.2958851 -4.2721453 -4.2362595 -4.1875124 -4.1421628 -4.1191311 -4.140039 -4.1834817 -4.2241311 -4.2621341 -4.2720504 -4.2725124 -4.260211 -4.2294087][-4.3097262 -4.292614 -4.2681556 -4.2329016 -4.1846428 -4.1368828 -4.1109447 -4.134726 -4.1754704 -4.2089047 -4.2381177 -4.2474313 -4.2563233 -4.255167 -4.2379675][-4.3081193 -4.2907624 -4.2642784 -4.2287621 -4.180131 -4.1306977 -4.1018653 -4.1275053 -4.1675415 -4.1968422 -4.2142115 -4.2185178 -4.2293963 -4.236371 -4.2322583][-4.3083625 -4.2887621 -4.255372 -4.2107067 -4.1533279 -4.1000948 -4.06782 -4.0983396 -4.1499543 -4.1799927 -4.1870933 -4.1845217 -4.1970844 -4.2103195 -4.2168479][-4.3079352 -4.2857242 -4.2454429 -4.1867452 -4.115046 -4.0484715 -4.0066352 -4.041811 -4.1128526 -4.1507869 -4.154439 -4.1478996 -4.1654072 -4.1863694 -4.2008748][-4.3085113 -4.2870655 -4.2451415 -4.1758776 -4.0840235 -3.9930487 -3.9246159 -3.9480615 -4.0378842 -4.0959005 -4.1131387 -4.1159511 -4.1409903 -4.1667929 -4.1869893][-4.3100438 -4.2901459 -4.2498469 -4.1778932 -4.0722237 -3.9600847 -3.8596296 -3.8581312 -3.9603662 -4.0436606 -4.0793853 -4.0959558 -4.1228466 -4.1472874 -4.1677322][-4.3099561 -4.2903647 -4.2515569 -4.1823421 -4.0787506 -3.9713562 -3.8720632 -3.8671658 -3.9695756 -4.0541992 -4.0951266 -4.1144462 -4.1307135 -4.141767 -4.1502986][-4.3078809 -4.2889438 -4.250289 -4.1877446 -4.1019816 -4.020647 -3.9473376 -3.9499409 -4.0380092 -4.1063876 -4.1368828 -4.1492624 -4.1510477 -4.1476922 -4.1411495][-4.3027329 -4.2815132 -4.24139 -4.1846638 -4.118216 -4.063446 -4.0164704 -4.026154 -4.1030107 -4.1608186 -4.1859407 -4.193521 -4.1862726 -4.1747479 -4.1585155][-4.2946625 -4.2695756 -4.2292681 -4.1802273 -4.1297541 -4.092876 -4.0679646 -4.0865469 -4.1584744 -4.2134905 -4.2380924 -4.2425609 -4.2298489 -4.2130966 -4.1906548][-4.2874327 -4.2586403 -4.219593 -4.178834 -4.14039 -4.1167493 -4.1062303 -4.1291213 -4.1913714 -4.2377472 -4.2615604 -4.2693439 -4.2621431 -4.249815 -4.231307][-4.285378 -4.2559366 -4.2191091 -4.1848197 -4.1549907 -4.1402993 -4.13685 -4.1573386 -4.2065487 -4.2457833 -4.2697296 -4.2830224 -4.2872543 -4.2864647 -4.2780504][-4.2902093 -4.2641668 -4.2313628 -4.2021337 -4.1792536 -4.171536 -4.1715069 -4.1905742 -4.2288094 -4.2609143 -4.2821727 -4.29599 -4.3048363 -4.3107543 -4.309587]]...]
INFO - root - 2017-12-05 23:25:56.251652: step 54410, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 66h:41m:48s remains)
INFO - root - 2017-12-05 23:26:04.842757: step 54420, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 65h:57m:05s remains)
INFO - root - 2017-12-05 23:26:13.261172: step 54430, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 64h:09m:48s remains)
INFO - root - 2017-12-05 23:26:21.938490: step 54440, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.826 sec/batch; 63h:45m:51s remains)
INFO - root - 2017-12-05 23:26:30.421994: step 54450, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 65h:44m:16s remains)
INFO - root - 2017-12-05 23:26:38.947658: step 54460, loss = 2.10, batch loss = 2.04 (10.5 examples/sec; 0.763 sec/batch; 58h:57m:53s remains)
INFO - root - 2017-12-05 23:26:47.534565: step 54470, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 64h:54m:30s remains)
INFO - root - 2017-12-05 23:26:55.999626: step 54480, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 69h:21m:23s remains)
INFO - root - 2017-12-05 23:27:04.570612: step 54490, loss = 2.10, batch loss = 2.04 (9.8 examples/sec; 0.820 sec/batch; 63h:19m:19s remains)
INFO - root - 2017-12-05 23:27:13.067331: step 54500, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 64h:54m:08s remains)
2017-12-05 23:27:13.959286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.124383 -4.1260114 -4.1275945 -4.1306186 -4.1391287 -4.1578326 -4.1659422 -4.1799803 -4.1974478 -4.2058487 -4.2089958 -4.2025318 -4.176434 -4.1456609 -4.1322274][-4.1187248 -4.1237559 -4.12433 -4.1256933 -4.1325545 -4.1536751 -4.1651568 -4.1733632 -4.1815448 -4.18582 -4.1917691 -4.1900029 -4.1663575 -4.1387048 -4.1252942][-4.1229014 -4.1288466 -4.1304789 -4.1319633 -4.139369 -4.1572556 -4.1621208 -4.1613994 -4.1628408 -4.1674752 -4.1785107 -4.182981 -4.1680026 -4.1450539 -4.128334][-4.1330895 -4.1360993 -4.1359649 -4.1359043 -4.1371212 -4.1413031 -4.1312847 -4.1228452 -4.1243262 -4.1331425 -4.1503863 -4.1603012 -4.156867 -4.1436033 -4.1291227][-4.1475339 -4.1449118 -4.140893 -4.1346245 -4.1230822 -4.1033177 -4.0696235 -4.0521164 -4.0623407 -4.0849323 -4.1121345 -4.1314363 -4.145256 -4.1496582 -4.1432953][-4.1546783 -4.1470947 -4.1354837 -4.1187153 -4.0896888 -4.0406728 -3.9784243 -3.9612744 -3.9970446 -4.04697 -4.0937757 -4.1259027 -4.1531186 -4.1708846 -4.1749406][-4.1652021 -4.1556897 -4.137908 -4.1114521 -4.0645733 -3.9839189 -3.8894072 -3.879585 -3.9534736 -4.0393734 -4.1124105 -4.1571808 -4.1857319 -4.2067518 -4.2152257][-4.1756124 -4.1643367 -4.146544 -4.1212883 -4.07132 -3.9852958 -3.890528 -3.8896871 -3.9733169 -4.06714 -4.1465573 -4.1967344 -4.2244711 -4.2442646 -4.2548933][-4.185596 -4.1717434 -4.1587353 -4.1412044 -4.0985155 -4.0328641 -3.97124 -3.9742599 -4.036994 -4.1112914 -4.1761093 -4.221364 -4.2490568 -4.2692795 -4.2816625][-4.1926651 -4.1794381 -4.1679106 -4.1549559 -4.1185632 -4.0752625 -4.0491962 -4.0603037 -4.1058178 -4.1608644 -4.2062335 -4.2411327 -4.264266 -4.2809119 -4.2946591][-4.1909151 -4.1798596 -4.167417 -4.1532197 -4.1164293 -4.0876217 -4.0903997 -4.1167226 -4.1553159 -4.1963387 -4.2290311 -4.2546329 -4.2712617 -4.2845449 -4.2947774][-4.1907988 -4.1812344 -4.1663651 -4.1481333 -4.1139317 -4.0962882 -4.1185012 -4.1548643 -4.1884136 -4.2183914 -4.2407317 -4.2575388 -4.2682376 -4.278295 -4.2849712][-4.1968141 -4.1879311 -4.1709456 -4.1521139 -4.1298776 -4.1239171 -4.150908 -4.183229 -4.2094908 -4.2300105 -4.2445045 -4.2543449 -4.2595963 -4.2629156 -4.2634945][-4.2067957 -4.19912 -4.1832032 -4.168201 -4.1562119 -4.156847 -4.1794605 -4.203651 -4.221581 -4.2363663 -4.2454453 -4.2473369 -4.2468553 -4.242826 -4.2385845][-4.2210369 -4.2114043 -4.1971054 -4.1859603 -4.1770077 -4.17662 -4.1924648 -4.2125063 -4.2280903 -4.2390003 -4.2438941 -4.2436953 -4.2422061 -4.2357669 -4.2306576]]...]
INFO - root - 2017-12-05 23:27:22.547654: step 54510, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 65h:00m:26s remains)
INFO - root - 2017-12-05 23:27:31.006509: step 54520, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 66h:03m:07s remains)
INFO - root - 2017-12-05 23:27:39.656578: step 54530, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 67h:42m:14s remains)
INFO - root - 2017-12-05 23:27:48.186510: step 54540, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 66h:11m:57s remains)
INFO - root - 2017-12-05 23:27:56.687804: step 54550, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 65h:18m:55s remains)
INFO - root - 2017-12-05 23:28:05.317069: step 54560, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.785 sec/batch; 60h:36m:14s remains)
INFO - root - 2017-12-05 23:28:13.937819: step 54570, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 66h:53m:58s remains)
INFO - root - 2017-12-05 23:28:22.320003: step 54580, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 65h:53m:38s remains)
INFO - root - 2017-12-05 23:28:30.863716: step 54590, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 63h:35m:18s remains)
INFO - root - 2017-12-05 23:28:39.397450: step 54600, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 66h:37m:10s remains)
2017-12-05 23:28:40.195921: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2946334 -4.2830215 -4.27376 -4.2693758 -4.2618093 -4.2596946 -4.2607746 -4.2661963 -4.2847276 -4.305016 -4.320868 -4.3332682 -4.3425159 -4.3447447 -4.34412][-4.256722 -4.2405343 -4.2277446 -4.2191377 -4.2058811 -4.198071 -4.1977863 -4.2036953 -4.2312627 -4.2651477 -4.291677 -4.3092651 -4.3244486 -4.3315568 -4.333642][-4.2163224 -4.1956959 -4.1795287 -4.1702332 -4.1569362 -4.14519 -4.1401157 -4.1462293 -4.1811666 -4.2278433 -4.2641444 -4.2865763 -4.3073936 -4.3180676 -4.3227315][-4.18284 -4.1616216 -4.1455874 -4.1390905 -4.1301341 -4.1141467 -4.1018085 -4.1074791 -4.1496034 -4.2061787 -4.2477431 -4.2734661 -4.2987008 -4.3109007 -4.3177028][-4.1701527 -4.148293 -4.13356 -4.1293821 -4.1229038 -4.0996852 -4.0830793 -4.0959287 -4.1491065 -4.208437 -4.2488675 -4.2772794 -4.3037267 -4.3142905 -4.3200731][-4.1745291 -4.1484709 -4.1294417 -4.1214485 -4.1148028 -4.0909777 -4.0851707 -4.115231 -4.1743231 -4.2313838 -4.2650981 -4.2903152 -4.3154674 -4.3247056 -4.3264122][-4.165339 -4.1235504 -4.0864706 -4.069561 -4.0616608 -4.0457382 -4.05633 -4.1075025 -4.1793447 -4.2412195 -4.2732863 -4.2981157 -4.3240719 -4.3316402 -4.3295827][-4.1360106 -4.0757132 -4.0237579 -3.9982264 -3.9829948 -3.9673555 -3.9889266 -4.0591826 -4.1485796 -4.2213373 -4.2597713 -4.2876468 -4.3173022 -4.3274474 -4.32703][-4.1008759 -4.0362391 -3.9792273 -3.9423988 -3.9116874 -3.8880041 -3.9130735 -3.9956827 -4.1016741 -4.1859856 -4.2322831 -4.2636585 -4.2984142 -4.3160868 -4.3204904][-4.0741529 -4.0147638 -3.9598088 -3.9139581 -3.8756924 -3.85174 -3.8779941 -3.9617903 -4.0726666 -4.1627021 -4.2124071 -4.2472568 -4.2882161 -4.3110008 -4.3163772][-4.0715375 -4.0158873 -3.9655862 -3.9201765 -3.8830271 -3.8650074 -3.8944087 -3.9730492 -4.0745215 -4.1571021 -4.207128 -4.2482486 -4.2943649 -4.3159285 -4.3182645][-4.1044164 -4.0554109 -4.0118361 -3.9736989 -3.9409225 -3.9298253 -3.9620709 -4.0330496 -4.1171803 -4.1853008 -4.2309251 -4.2711477 -4.3110175 -4.3247185 -4.3234606][-4.166501 -4.1288033 -4.0976472 -4.069242 -4.042418 -4.0353785 -4.0622044 -4.1174879 -4.1800051 -4.2322593 -4.2679281 -4.2994242 -4.3255935 -4.3310003 -4.32901][-4.232152 -4.2064829 -4.1882739 -4.1693521 -4.1494 -4.1451573 -4.1647658 -4.2031021 -4.2435317 -4.2779565 -4.3025546 -4.3225455 -4.33517 -4.3339448 -4.3329635][-4.2838011 -4.26969 -4.2624073 -4.2521839 -4.2388515 -4.2361078 -4.2475247 -4.2699409 -4.2939444 -4.314405 -4.3289814 -4.3392954 -4.3426971 -4.3386626 -4.3378177]]...]
INFO - root - 2017-12-05 23:28:48.843103: step 54610, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 67h:58m:03s remains)
INFO - root - 2017-12-05 23:28:57.402840: step 54620, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 65h:58m:52s remains)
INFO - root - 2017-12-05 23:29:05.989857: step 54630, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 67h:55m:38s remains)
INFO - root - 2017-12-05 23:29:14.625686: step 54640, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 67h:54m:29s remains)
INFO - root - 2017-12-05 23:29:23.070873: step 54650, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 66h:18m:22s remains)
INFO - root - 2017-12-05 23:29:31.484497: step 54660, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.779 sec/batch; 60h:05m:31s remains)
INFO - root - 2017-12-05 23:29:40.073964: step 54670, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 64h:08m:18s remains)
INFO - root - 2017-12-05 23:29:48.668993: step 54680, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 67h:22m:45s remains)
INFO - root - 2017-12-05 23:29:57.102007: step 54690, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 64h:49m:57s remains)
INFO - root - 2017-12-05 23:30:05.735022: step 54700, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 66h:58m:08s remains)
2017-12-05 23:30:06.553198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2932358 -4.276278 -4.252058 -4.2235527 -4.1990004 -4.1803484 -4.1755667 -4.1906581 -4.2206941 -4.2388415 -4.2397032 -4.2341361 -4.2265549 -4.2224941 -4.225152][-4.2872834 -4.2659769 -4.2336645 -4.1937528 -4.1577892 -4.1328158 -4.1289458 -4.1493568 -4.1912618 -4.2236018 -4.2316918 -4.2306404 -4.2242007 -4.2168417 -4.2139139][-4.2800069 -4.2540197 -4.2180691 -4.1716189 -4.1258497 -4.0957007 -4.0903339 -4.1089678 -4.1551371 -4.1982617 -4.2182751 -4.2274656 -4.2277613 -4.2223463 -4.2188435][-4.2746449 -4.2459068 -4.2101579 -4.1624455 -4.1152582 -4.0843797 -4.0749483 -4.0844078 -4.1262 -4.171761 -4.2001243 -4.2190609 -4.2280636 -4.2304573 -4.2336078][-4.2754507 -4.2507586 -4.2206311 -4.1793852 -4.1361203 -4.1052036 -4.0910368 -4.084219 -4.1091704 -4.1471629 -4.17786 -4.2026916 -4.2202787 -4.2361526 -4.250793][-4.2753663 -4.2540083 -4.227087 -4.1948366 -4.161036 -4.1316285 -4.108561 -4.0830846 -4.085454 -4.1088152 -4.13508 -4.1647453 -4.1924052 -4.2231612 -4.25227][-4.2732654 -4.249372 -4.2202315 -4.1909618 -4.163157 -4.1350579 -4.1083088 -4.0762267 -4.062614 -4.0653434 -4.0766096 -4.1024365 -4.1349959 -4.1795387 -4.2254086][-4.2668538 -4.2335067 -4.1952248 -4.1635261 -4.1381979 -4.1118841 -4.0891881 -4.0678563 -4.0593691 -4.0511842 -4.0445943 -4.049932 -4.073288 -4.1242495 -4.1852055][-4.2581573 -4.2128162 -4.1639729 -4.131115 -4.1075082 -4.0831022 -4.0652261 -4.0628982 -4.0727839 -4.0662723 -4.0500402 -4.0371065 -4.0453167 -4.0935321 -4.1588459][-4.2580695 -4.2071414 -4.1536341 -4.1189284 -4.0904083 -4.0617762 -4.0482235 -4.064024 -4.0902014 -4.0913906 -4.0772672 -4.0621243 -4.0645714 -4.1053433 -4.1639915][-4.2731152 -4.22931 -4.1825733 -4.1453938 -4.1063967 -4.0715704 -4.05969 -4.0825305 -4.1154428 -4.1269555 -4.125752 -4.1222534 -4.1267638 -4.1526666 -4.1921606][-4.2987251 -4.2687283 -4.2342896 -4.2011108 -4.1633172 -4.1303358 -4.1183944 -4.1335549 -4.1597371 -4.1773882 -4.1873708 -4.1929593 -4.198709 -4.211494 -4.2322307][-4.3246984 -4.3057318 -4.2827334 -4.2603974 -4.2360768 -4.2147284 -4.2024946 -4.2035923 -4.2153888 -4.2298446 -4.2411623 -4.2481108 -4.2527323 -4.2587891 -4.2694755][-4.3418374 -4.3311391 -4.3172641 -4.3046322 -4.2912331 -4.2786965 -4.2669578 -4.2610359 -4.2626095 -4.2698693 -4.2775583 -4.284409 -4.2886224 -4.2907934 -4.294909][-4.3496742 -4.3440185 -4.3368173 -4.3305569 -4.3229628 -4.3156428 -4.3090854 -4.3041253 -4.3018413 -4.3035631 -4.3066697 -4.3099489 -4.3120551 -4.3129582 -4.3144207]]...]
INFO - root - 2017-12-05 23:30:15.105742: step 54710, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 65h:28m:02s remains)
INFO - root - 2017-12-05 23:30:23.703155: step 54720, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 65h:58m:31s remains)
INFO - root - 2017-12-05 23:30:32.312196: step 54730, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 65h:44m:16s remains)
INFO - root - 2017-12-05 23:30:40.863835: step 54740, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 65h:28m:48s remains)
INFO - root - 2017-12-05 23:30:49.365685: step 54750, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 64h:40m:03s remains)
INFO - root - 2017-12-05 23:30:57.755444: step 54760, loss = 2.05, batch loss = 1.99 (10.4 examples/sec; 0.771 sec/batch; 59h:28m:24s remains)
INFO - root - 2017-12-05 23:31:06.330663: step 54770, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 67h:55m:30s remains)
INFO - root - 2017-12-05 23:31:14.882615: step 54780, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 64h:21m:54s remains)
INFO - root - 2017-12-05 23:31:23.260158: step 54790, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 65h:13m:16s remains)
INFO - root - 2017-12-05 23:31:31.847525: step 54800, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 66h:03m:09s remains)
2017-12-05 23:31:32.671762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2640381 -4.2515507 -4.2292128 -4.2090821 -4.2163625 -4.2440858 -4.2662725 -4.2696805 -4.26035 -4.2429 -4.2362971 -4.2506428 -4.25978 -4.2675343 -4.2803841][-4.2517028 -4.2430558 -4.2191863 -4.1926455 -4.1941094 -4.2225194 -4.2509918 -4.2631531 -4.257257 -4.2387571 -4.2308669 -4.2396183 -4.24276 -4.2489462 -4.2706118][-4.2436981 -4.2287641 -4.2008166 -4.1702147 -4.1647434 -4.1914077 -4.2287149 -4.2506027 -4.2495294 -4.2373157 -4.231668 -4.235518 -4.2380171 -4.2467217 -4.2725577][-4.2340264 -4.2054634 -4.169435 -4.1358581 -4.128849 -4.1599565 -4.2052565 -4.2336707 -4.234581 -4.2267351 -4.216814 -4.211566 -4.2123528 -4.22126 -4.2437959][-4.2226634 -4.1838841 -4.1425495 -4.1014066 -4.0905538 -4.1298361 -4.1866546 -4.2238317 -4.22866 -4.2224817 -4.2065172 -4.18712 -4.17469 -4.1759095 -4.1909032][-4.2134957 -4.1715279 -4.1254983 -4.0692377 -4.0396042 -4.0757976 -4.1449428 -4.1932182 -4.2036281 -4.2000546 -4.1822219 -4.1481829 -4.119925 -4.1147728 -4.1281242][-4.203568 -4.1597242 -4.1054683 -4.0299916 -3.9655781 -3.9756868 -4.0507431 -4.1106558 -4.1354051 -4.14059 -4.1291828 -4.0953431 -4.0693417 -4.0697374 -4.0850754][-4.205339 -4.1659732 -4.114275 -4.0388412 -3.9614561 -3.9431336 -4.0011959 -4.0613122 -4.1048889 -4.1237521 -4.1205072 -4.0954351 -4.0755858 -4.0784836 -4.090169][-4.214292 -4.1842227 -4.1451116 -4.0925384 -4.0398192 -4.015419 -4.0439296 -4.0819311 -4.1216407 -4.1473026 -4.1531906 -4.1351743 -4.1171217 -4.1158094 -4.1208644][-4.2217355 -4.1997347 -4.1731358 -4.1403046 -4.1097064 -4.09183 -4.0993791 -4.1145988 -4.1433072 -4.1714468 -4.1829891 -4.1701241 -4.1603575 -4.1601992 -4.1590824][-4.225852 -4.2084637 -4.1913376 -4.1755629 -4.1668062 -4.1605992 -4.1564035 -4.1552873 -4.1742268 -4.1979084 -4.2087398 -4.2050695 -4.2063828 -4.211978 -4.2081976][-4.2360945 -4.2205043 -4.2103109 -4.2093253 -4.2189426 -4.224678 -4.2218275 -4.2165761 -4.2282252 -4.241188 -4.2462678 -4.2463169 -4.2528963 -4.2599173 -4.257688][-4.2494335 -4.2366247 -4.2319469 -4.2387719 -4.2555652 -4.2670727 -4.2654643 -4.2584076 -4.2619247 -4.2672319 -4.2668962 -4.2639976 -4.2698059 -4.2783475 -4.2801585][-4.2609439 -4.2491851 -4.2486582 -4.2583513 -4.2758169 -4.2876825 -4.2876215 -4.28371 -4.2839956 -4.2837934 -4.2791233 -4.2746077 -4.2800465 -4.2896729 -4.2932391][-4.2702789 -4.2576823 -4.2566662 -4.2639065 -4.2777777 -4.2898879 -4.2954855 -4.2978492 -4.2987018 -4.2958765 -4.2898226 -4.2859993 -4.2911487 -4.3003922 -4.3035908]]...]
INFO - root - 2017-12-05 23:31:41.182535: step 54810, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 65h:46m:23s remains)
INFO - root - 2017-12-05 23:31:49.726217: step 54820, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 67h:11m:22s remains)
INFO - root - 2017-12-05 23:31:58.531098: step 54830, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 64h:29m:17s remains)
INFO - root - 2017-12-05 23:32:07.086552: step 54840, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 65h:49m:53s remains)
INFO - root - 2017-12-05 23:32:15.562571: step 54850, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 64h:21m:38s remains)
INFO - root - 2017-12-05 23:32:23.978797: step 54860, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.789 sec/batch; 60h:49m:19s remains)
INFO - root - 2017-12-05 23:32:32.520181: step 54870, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 63h:41m:42s remains)
INFO - root - 2017-12-05 23:32:41.165072: step 54880, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 65h:45m:48s remains)
INFO - root - 2017-12-05 23:32:49.668115: step 54890, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 67h:43m:55s remains)
INFO - root - 2017-12-05 23:32:58.166753: step 54900, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 66h:01m:53s remains)
2017-12-05 23:32:58.927476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1959648 -4.2030029 -4.2044654 -4.2113795 -4.22606 -4.2363033 -4.2291908 -4.20379 -4.1858387 -4.1689467 -4.1390162 -4.1259794 -4.126986 -4.12174 -4.1222878][-4.2177267 -4.2310538 -4.2283049 -4.2282333 -4.2365012 -4.2316475 -4.211112 -4.17956 -4.1552639 -4.1366138 -4.1138911 -4.1100063 -4.1230431 -4.12537 -4.1275063][-4.2285004 -4.2469025 -4.2449007 -4.2408094 -4.2435532 -4.2296185 -4.2042 -4.1747189 -4.1499643 -4.1340652 -4.1226878 -4.1249189 -4.1427584 -4.1492867 -4.148057][-4.2163363 -4.2356248 -4.2345481 -4.2291822 -4.2289786 -4.2113528 -4.1842809 -4.1595407 -4.1429796 -4.1297731 -4.1183743 -4.1198149 -4.1367044 -4.1467128 -4.1436019][-4.1872058 -4.2055783 -4.2030559 -4.1930194 -4.1801472 -4.1486259 -4.1058626 -4.0846872 -4.09138 -4.0908508 -4.0817761 -4.0875688 -4.1032691 -4.109148 -4.10159][-4.1471519 -4.1590366 -4.1554089 -4.1357255 -4.1039934 -4.040946 -3.9692769 -3.961225 -4.0120211 -4.041749 -4.0438428 -4.060544 -4.0818543 -4.0836186 -4.0717564][-4.1181135 -4.1197758 -4.1164637 -4.0961976 -4.0513339 -3.9660132 -3.8872223 -3.9076355 -3.99364 -4.0449262 -4.0577226 -4.08025 -4.102366 -4.1015258 -4.0866671][-4.1185994 -4.1172218 -4.1168256 -4.1081343 -4.0783696 -4.0213785 -3.975574 -4.0060835 -4.07498 -4.1164069 -4.1291275 -4.1466188 -4.1594129 -4.1528835 -4.1365886][-4.1408534 -4.143075 -4.14396 -4.1501522 -4.14889 -4.1282334 -4.111392 -4.1355767 -4.1780515 -4.2011576 -4.2049994 -4.2126212 -4.21753 -4.2106953 -4.195612][-4.1527176 -4.1595306 -4.1637926 -4.179565 -4.1983719 -4.2048841 -4.2077317 -4.2277908 -4.2565722 -4.2704511 -4.2698741 -4.2704577 -4.2670894 -4.2588253 -4.2465472][-4.1536098 -4.166997 -4.1737037 -4.192039 -4.221683 -4.2426581 -4.2592764 -4.2839217 -4.3069534 -4.3151975 -4.3131108 -4.3097534 -4.3018675 -4.2925653 -4.2826519][-4.1577115 -4.1768017 -4.1908059 -4.2110229 -4.2430172 -4.26797 -4.2889395 -4.3132248 -4.330616 -4.3338633 -4.3312483 -4.3277907 -4.3208456 -4.3123407 -4.3046193][-4.1806207 -4.203084 -4.223423 -4.2448664 -4.273809 -4.2967353 -4.3144445 -4.3333635 -4.3448553 -4.3436723 -4.3380761 -4.3337169 -4.3251882 -4.3143821 -4.3058853][-4.2293329 -4.2554655 -4.276216 -4.2936277 -4.311976 -4.32521 -4.3348269 -4.3450341 -4.3488874 -4.3436093 -4.3365211 -4.3309155 -4.3216758 -4.3111067 -4.303587][-4.2763605 -4.304512 -4.3222041 -4.3353276 -4.343019 -4.3464203 -4.3483872 -4.3489842 -4.3440957 -4.3357859 -4.3294139 -4.3254638 -4.3194151 -4.3123183 -4.3062978]]...]
INFO - root - 2017-12-05 23:33:07.602290: step 54910, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 69h:30m:13s remains)
INFO - root - 2017-12-05 23:33:16.189731: step 54920, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 67h:04m:30s remains)
INFO - root - 2017-12-05 23:33:24.645461: step 54930, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 64h:23m:44s remains)
INFO - root - 2017-12-05 23:33:33.282726: step 54940, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 66h:38m:24s remains)
INFO - root - 2017-12-05 23:33:41.875913: step 54950, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 64h:45m:22s remains)
INFO - root - 2017-12-05 23:33:50.176017: step 54960, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.743 sec/batch; 57h:17m:04s remains)
INFO - root - 2017-12-05 23:33:58.676289: step 54970, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 65h:06m:22s remains)
INFO - root - 2017-12-05 23:34:06.982552: step 54980, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 66h:43m:38s remains)
INFO - root - 2017-12-05 23:34:15.459343: step 54990, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 64h:10m:18s remains)
INFO - root - 2017-12-05 23:34:23.780541: step 55000, loss = 2.05, batch loss = 1.99 (10.4 examples/sec; 0.770 sec/batch; 59h:22m:20s remains)
2017-12-05 23:34:24.568979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1126027 -4.1787791 -4.2186193 -4.2121587 -4.1808982 -4.1423426 -4.0962629 -4.0556207 -4.03571 -4.0531235 -4.1064587 -4.1621928 -4.1980457 -4.2238541 -4.2340856][-4.0841918 -4.1641645 -4.2173204 -4.2190623 -4.1856556 -4.1396012 -4.0790944 -4.0232658 -3.9917047 -4.0038538 -4.0642462 -4.129849 -4.1741972 -4.2052059 -4.2219996][-4.056303 -4.1423235 -4.2028832 -4.208 -4.1778712 -4.1308484 -4.0635686 -4.0042315 -3.9724388 -3.9798357 -4.0318623 -4.0983496 -4.1445913 -4.176909 -4.2010918][-4.0543971 -4.1353555 -4.192194 -4.1944876 -4.1613264 -4.11265 -4.0504403 -4.0014467 -3.9844437 -3.9976254 -4.0398021 -4.0939817 -4.1336017 -4.1644788 -4.1936035][-4.094192 -4.153688 -4.1890159 -4.1839986 -4.1412897 -4.0884056 -4.0350113 -3.9977078 -3.9989028 -4.0306253 -4.0778751 -4.1219397 -4.1517253 -4.174931 -4.197679][-4.1561756 -4.1838326 -4.1935673 -4.1758623 -4.1210918 -4.0589261 -4.0090756 -3.9846895 -4.0048585 -4.059258 -4.1179733 -4.1594677 -4.1822286 -4.1954966 -4.2050447][-4.2070789 -4.2096362 -4.1974716 -4.166554 -4.1070571 -4.0393243 -3.9907532 -3.9749646 -4.0060973 -4.0722265 -4.1422362 -4.19114 -4.2166314 -4.2247472 -4.2217164][-4.2351904 -4.2258234 -4.1989226 -4.1557479 -4.0972862 -4.0343089 -3.9905822 -3.9832315 -4.01973 -4.087009 -4.1516175 -4.1985445 -4.22515 -4.2331314 -4.2259173][-4.2513084 -4.2384496 -4.2027106 -4.1509819 -4.0942769 -4.0387936 -4.00077 -4.0028296 -4.0481844 -4.1135006 -4.1608739 -4.1862493 -4.2047372 -4.2148418 -4.2143826][-4.261806 -4.2457094 -4.2077308 -4.15616 -4.1052551 -4.0567775 -4.0277257 -4.0419426 -4.0911751 -4.1482506 -4.1742749 -4.1736474 -4.1731439 -4.1809044 -4.191246][-4.2655025 -4.2439218 -4.2036977 -4.1636167 -4.132607 -4.099267 -4.0767093 -4.0913925 -4.1336255 -4.1745572 -4.1838951 -4.1681852 -4.1507549 -4.1531243 -4.1706219][-4.2608 -4.2333393 -4.1927309 -4.1638613 -4.1538954 -4.1359158 -4.1217356 -4.1338391 -4.1653514 -4.1864829 -4.17962 -4.1604366 -4.1392784 -4.1399035 -4.1635518][-4.2562847 -4.2300186 -4.1906366 -4.1625195 -4.1615744 -4.1585569 -4.1542473 -4.1659112 -4.1864862 -4.1901908 -4.1699581 -4.1478567 -4.1294055 -4.1339631 -4.16665][-4.2522798 -4.2340093 -4.1987524 -4.1706271 -4.1676006 -4.1755447 -4.1841373 -4.1954045 -4.2007265 -4.1867404 -4.156321 -4.1291757 -4.1127362 -4.123498 -4.1678395][-4.2553277 -4.244791 -4.2174711 -4.1938152 -4.185854 -4.1939945 -4.2104611 -4.2196865 -4.2117786 -4.1864724 -4.1513858 -4.1188917 -4.1040707 -4.1205869 -4.1724105]]...]
INFO - root - 2017-12-05 23:34:33.001771: step 55010, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 65h:15m:35s remains)
INFO - root - 2017-12-05 23:34:41.553712: step 55020, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.897 sec/batch; 69h:06m:04s remains)
INFO - root - 2017-12-05 23:34:50.055007: step 55030, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 65h:29m:38s remains)
INFO - root - 2017-12-05 23:34:58.766845: step 55040, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 76h:24m:18s remains)
INFO - root - 2017-12-05 23:35:07.260359: step 55050, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 66h:46m:50s remains)
INFO - root - 2017-12-05 23:35:15.795544: step 55060, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 66h:23m:59s remains)
INFO - root - 2017-12-05 23:35:24.325734: step 55070, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 65h:25m:25s remains)
INFO - root - 2017-12-05 23:35:32.925948: step 55080, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 69h:31m:48s remains)
INFO - root - 2017-12-05 23:35:41.531038: step 55090, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 66h:15m:24s remains)
INFO - root - 2017-12-05 23:35:50.007481: step 55100, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 64h:12m:42s remains)
2017-12-05 23:35:50.740866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2734504 -4.2650161 -4.2623963 -4.262146 -4.2658577 -4.2709279 -4.2692757 -4.2664618 -4.2731619 -4.2802219 -4.2864594 -4.29595 -4.3037987 -4.3105664 -4.3155451][-4.25377 -4.240119 -4.2337313 -4.22947 -4.2329955 -4.2427726 -4.2419734 -4.2383919 -4.2541094 -4.2681551 -4.2757106 -4.2872882 -4.2971416 -4.3076773 -4.3155656][-4.2248983 -4.2057714 -4.1953297 -4.1877422 -4.1888628 -4.2016511 -4.20153 -4.1952782 -4.2169952 -4.2409868 -4.2534552 -4.2681146 -4.2807016 -4.2946658 -4.3062325][-4.1966453 -4.1711817 -4.1570325 -4.1479931 -4.1422324 -4.152688 -4.1527071 -4.1427813 -4.1660109 -4.1987591 -4.2198806 -4.242259 -4.2585998 -4.2724504 -4.2880683][-4.1768279 -4.142396 -4.1204114 -4.1036472 -4.086617 -4.0910711 -4.0954361 -4.08786 -4.1139207 -4.1573396 -4.18737 -4.2176642 -4.2392521 -4.2530804 -4.2717018][-4.1658 -4.1235719 -4.0904174 -4.060708 -4.0304747 -4.0277333 -4.0218163 -3.9999027 -4.0265794 -4.088644 -4.1339617 -4.1768069 -4.2080941 -4.2288318 -4.2556505][-4.1629972 -4.1123829 -4.0688815 -4.0309544 -3.9912896 -3.9709544 -3.93394 -3.8750658 -3.9032733 -3.9978161 -4.0674605 -4.1297917 -4.1739664 -4.2033443 -4.2393756][-4.1668196 -4.1055832 -4.0519447 -4.0053039 -3.954397 -3.9121552 -3.8446307 -3.7673836 -3.8135462 -3.9373121 -4.02448 -4.0989141 -4.15157 -4.1841373 -4.2248049][-4.1656313 -4.0968633 -4.0368576 -3.9806945 -3.9229097 -3.8731265 -3.807241 -3.7498589 -3.8171759 -3.9420571 -4.0234017 -4.0962543 -4.1501427 -4.1829772 -4.2226896][-4.1709337 -4.1061792 -4.0468407 -3.9916835 -3.942641 -3.9048603 -3.8564169 -3.8136981 -3.876359 -3.9861984 -4.0578375 -4.1211233 -4.1705275 -4.20164 -4.2359796][-4.1886344 -4.1336174 -4.0862546 -4.0471373 -4.0167556 -3.9974272 -3.9664598 -3.9341531 -3.9838848 -4.0699458 -4.1246338 -4.173708 -4.2156382 -4.2394814 -4.2611165][-4.2023196 -4.1561313 -4.1204481 -4.0964184 -4.0824027 -4.0771947 -4.0605326 -4.0436773 -4.0841327 -4.1428871 -4.1785965 -4.21821 -4.2516494 -4.2690072 -4.2819448][-4.220171 -4.1871324 -4.1660028 -4.1534424 -4.1494975 -4.1531057 -4.1461329 -4.1378889 -4.1667418 -4.200366 -4.2221622 -4.2490592 -4.2707348 -4.2827978 -4.2914929][-4.2451329 -4.2260122 -4.2172008 -4.2148738 -4.2158847 -4.2202287 -4.215693 -4.209208 -4.2254968 -4.2444305 -4.255507 -4.2698436 -4.2818589 -4.2888803 -4.2928209][-4.2697783 -4.2602272 -4.256279 -4.2555447 -4.2548637 -4.2558446 -4.2534757 -4.2506213 -4.2624364 -4.2748151 -4.2811933 -4.2865868 -4.2916055 -4.2922997 -4.292171]]...]
INFO - root - 2017-12-05 23:35:59.125852: step 55110, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 64h:44m:00s remains)
INFO - root - 2017-12-05 23:36:07.643492: step 55120, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 64h:18m:08s remains)
INFO - root - 2017-12-05 23:36:16.256183: step 55130, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 66h:50m:47s remains)
INFO - root - 2017-12-05 23:36:24.787661: step 55140, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 66h:58m:36s remains)
INFO - root - 2017-12-05 23:36:33.380242: step 55150, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 75h:43m:51s remains)
INFO - root - 2017-12-05 23:36:42.013906: step 55160, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 66h:25m:31s remains)
INFO - root - 2017-12-05 23:36:50.498565: step 55170, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 66h:21m:33s remains)
INFO - root - 2017-12-05 23:36:59.059286: step 55180, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 65h:41m:29s remains)
INFO - root - 2017-12-05 23:37:07.679369: step 55190, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 64h:50m:52s remains)
INFO - root - 2017-12-05 23:37:16.130299: step 55200, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 64h:32m:46s remains)
2017-12-05 23:37:16.973380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.311481 -4.2883744 -4.2715144 -4.2514009 -4.235261 -4.2448025 -4.2642365 -4.2821622 -4.292726 -4.2909956 -4.2864065 -4.2851591 -4.2888889 -4.2958026 -4.299643][-4.2804589 -4.245122 -4.221673 -4.1964369 -4.1788468 -4.1969724 -4.2230768 -4.2460012 -4.2610478 -4.2581739 -4.2519708 -4.2505703 -4.2551503 -4.2634478 -4.2681909][-4.2515039 -4.2073159 -4.1806817 -4.1550727 -4.1390691 -4.1592712 -4.1792049 -4.1988339 -4.21664 -4.2166867 -4.2130709 -4.2157688 -4.2221704 -4.231905 -4.2365675][-4.2388005 -4.1971054 -4.1758142 -4.1550946 -4.1377869 -4.1444016 -4.1435733 -4.1438627 -4.1572065 -4.1618786 -4.1676564 -4.1824789 -4.1967783 -4.2103062 -4.2147889][-4.2406111 -4.207376 -4.1914349 -4.1716824 -4.1445165 -4.1251826 -4.0893764 -4.05703 -4.0654593 -4.084096 -4.1087255 -4.1440992 -4.1702895 -4.1887674 -4.194015][-4.2506504 -4.22657 -4.2104197 -4.1852779 -4.1397424 -4.0826588 -4.0020823 -3.9372902 -3.9537258 -4.00191 -4.054554 -4.1114688 -4.1445904 -4.1629286 -4.1670566][-4.2609911 -4.2382345 -4.2165885 -4.1788139 -4.109457 -4.0152264 -3.8975172 -3.8091094 -3.8456492 -3.932102 -4.014101 -4.0874405 -4.12352 -4.1406717 -4.1440606][-4.2709389 -4.24113 -4.2078581 -4.1578121 -4.0715613 -3.9619045 -3.8404303 -3.7591321 -3.8159838 -3.9261258 -4.0215125 -4.0929 -4.1263247 -4.14351 -4.1479053][-4.2679219 -4.2249789 -4.1768856 -4.1198058 -4.0410404 -3.9607334 -3.8883171 -3.8509517 -3.9098105 -4.0068288 -4.0868492 -4.1373749 -4.1626506 -4.1787891 -4.1833138][-4.2618794 -4.2078371 -4.1495457 -4.0947094 -4.0411773 -4.0097456 -3.9888911 -3.9874327 -4.0386395 -4.1114182 -4.1665773 -4.19573 -4.2158427 -4.2299314 -4.2306743][-4.2654719 -4.2111058 -4.1551561 -4.1151042 -4.0905371 -4.0933547 -4.1011119 -4.1148667 -4.1554575 -4.2061834 -4.239109 -4.2529359 -4.2682042 -4.2774286 -4.2719407][-4.2791228 -4.2357936 -4.1946454 -4.1719675 -4.1646409 -4.1804271 -4.1974173 -4.2132978 -4.2410827 -4.274497 -4.2915096 -4.2945967 -4.3023109 -4.3059492 -4.298255][-4.2985811 -4.2737193 -4.2505512 -4.2397103 -4.2383137 -4.2525816 -4.2682056 -4.2805781 -4.2966814 -4.3157792 -4.3208818 -4.3164539 -4.3160748 -4.313623 -4.3060379][-4.3100982 -4.2975335 -4.287601 -4.2826557 -4.281095 -4.291028 -4.30436 -4.3146663 -4.32281 -4.3299804 -4.3278337 -4.3187971 -4.3127422 -4.307013 -4.3007703][-4.31776 -4.31136 -4.3077478 -4.3062944 -4.3066545 -4.3133845 -4.3225908 -4.3282022 -4.32966 -4.330236 -4.3255172 -4.3163629 -4.3103085 -4.3059268 -4.3021288]]...]
INFO - root - 2017-12-05 23:37:25.471668: step 55210, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 68h:44m:21s remains)
INFO - root - 2017-12-05 23:37:33.946773: step 55220, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 66h:44m:08s remains)
INFO - root - 2017-12-05 23:37:42.523800: step 55230, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 66h:39m:54s remains)
INFO - root - 2017-12-05 23:37:51.064703: step 55240, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 67h:50m:10s remains)
INFO - root - 2017-12-05 23:37:59.648020: step 55250, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 65h:37m:32s remains)
INFO - root - 2017-12-05 23:38:08.189516: step 55260, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.856 sec/batch; 65h:53m:26s remains)
INFO - root - 2017-12-05 23:38:16.686444: step 55270, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 66h:09m:53s remains)
INFO - root - 2017-12-05 23:38:25.343027: step 55280, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 67h:42m:19s remains)
INFO - root - 2017-12-05 23:38:33.892617: step 55290, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.826 sec/batch; 63h:35m:10s remains)
INFO - root - 2017-12-05 23:38:42.451455: step 55300, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 67h:22m:08s remains)
2017-12-05 23:38:43.214768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2454057 -4.2555928 -4.26886 -4.2759085 -4.2675819 -4.2448926 -4.2269974 -4.2284503 -4.2441621 -4.2573652 -4.2600794 -4.2499504 -4.2370625 -4.2352338 -4.251133][-4.230864 -4.2456827 -4.2620034 -4.2684927 -4.2563062 -4.2270536 -4.2039509 -4.2041726 -4.2228751 -4.2383223 -4.2403941 -4.2273674 -4.2130842 -4.2129827 -4.2354937][-4.2098813 -4.2278295 -4.2454491 -4.2508025 -4.2369576 -4.2035203 -4.1771512 -4.1748886 -4.1947236 -4.2137933 -4.2162828 -4.2012105 -4.1856055 -4.187048 -4.2150903][-4.1835165 -4.2033076 -4.2207584 -4.225265 -4.2114062 -4.1758652 -4.1477532 -4.143723 -4.1660857 -4.1920118 -4.1970525 -4.1789861 -4.1602249 -4.1615305 -4.1933022][-4.1601343 -4.1849551 -4.2040854 -4.2086921 -4.1927934 -4.1538534 -4.123024 -4.1164889 -4.1439495 -4.1791334 -4.189085 -4.1685185 -4.1454024 -4.1444445 -4.1776819][-4.135839 -4.1664104 -4.1895027 -4.1952462 -4.1767607 -4.1345234 -4.1012588 -4.0937886 -4.1258116 -4.1667767 -4.1805048 -4.160573 -4.136188 -4.1341319 -4.1685166][-4.1127472 -4.1463947 -4.1733847 -4.1819639 -4.1604815 -4.1133242 -4.0739021 -4.06659 -4.1027069 -4.146739 -4.1649904 -4.1505675 -4.1295547 -4.1296806 -4.166688][-4.1010261 -4.1376867 -4.1685305 -4.1777725 -4.1507587 -4.0910964 -4.0383816 -4.0270534 -4.0659328 -4.1157746 -4.1440892 -4.1409159 -4.127224 -4.1320853 -4.1731849][-4.1014032 -4.1414828 -4.1764541 -4.1826525 -4.1462646 -4.070724 -3.9990249 -3.9797552 -4.0222712 -4.0809917 -4.1216464 -4.1323481 -4.1264896 -4.1353178 -4.1797376][-4.0982733 -4.1421375 -4.1811976 -4.1864777 -4.1463771 -4.0640039 -3.9836571 -3.9586797 -4.0001817 -4.0637603 -4.1113505 -4.1284266 -4.1262989 -4.1387553 -4.1843967][-4.0904622 -4.1349092 -4.1763868 -4.1831708 -4.1488104 -4.0755072 -4.0031376 -3.9781592 -4.0121746 -4.0695953 -4.111887 -4.1248193 -4.1206713 -4.1365094 -4.183506][-4.0851455 -4.1240792 -4.1632986 -4.1742129 -4.1510777 -4.0987029 -4.0458417 -4.0247793 -4.0475097 -4.091629 -4.1220541 -4.125927 -4.1177278 -4.1345043 -4.1808429][-4.0820761 -4.1133265 -4.1462317 -4.1593127 -4.1464157 -4.1170244 -4.0866737 -4.0736227 -4.0898824 -4.1222897 -4.14074 -4.1348376 -4.1211448 -4.1350231 -4.1777911][-4.0853772 -4.1094232 -4.1339159 -4.1453156 -4.1376467 -4.1247225 -4.1125712 -4.1088543 -4.1232224 -4.1475086 -4.1569877 -4.1437654 -4.1250167 -4.1333709 -4.1722932][-4.096117 -4.1152506 -4.1316667 -4.138618 -4.1332211 -4.1307006 -4.1320109 -4.1366372 -4.1513681 -4.170104 -4.17149 -4.1505527 -4.1272321 -4.1311736 -4.1672077]]...]
INFO - root - 2017-12-05 23:38:51.601175: step 55310, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 63h:31m:43s remains)
INFO - root - 2017-12-05 23:39:00.048586: step 55320, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 65h:53m:22s remains)
INFO - root - 2017-12-05 23:39:08.581662: step 55330, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 66h:35m:30s remains)
INFO - root - 2017-12-05 23:39:17.203581: step 55340, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 69h:28m:26s remains)
INFO - root - 2017-12-05 23:39:25.688983: step 55350, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 65h:45m:07s remains)
INFO - root - 2017-12-05 23:39:34.261251: step 55360, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 66h:37m:53s remains)
INFO - root - 2017-12-05 23:39:42.776234: step 55370, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 64h:02m:33s remains)
INFO - root - 2017-12-05 23:39:51.234484: step 55380, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 64h:44m:30s remains)
INFO - root - 2017-12-05 23:39:59.678013: step 55390, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 63h:09m:03s remains)
INFO - root - 2017-12-05 23:40:08.202912: step 55400, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 65h:12m:42s remains)
2017-12-05 23:40:08.947901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1875463 -4.1688108 -4.15355 -4.1400738 -4.1350379 -4.1412392 -4.1640096 -4.1947007 -4.212893 -4.2110281 -4.2056918 -4.20739 -4.2164507 -4.2291374 -4.2430263][-4.1433558 -4.1265125 -4.1130953 -4.1052494 -4.1105189 -4.1278896 -4.155704 -4.1867666 -4.2040706 -4.2016892 -4.1968207 -4.1956983 -4.1976089 -4.2025957 -4.2107587][-4.1374741 -4.1312246 -4.1238723 -4.1187539 -4.1302223 -4.1532726 -4.17731 -4.2006407 -4.2150583 -4.2161551 -4.2176685 -4.2149425 -4.2071962 -4.19967 -4.1963258][-4.1606131 -4.1664772 -4.1677241 -4.1676049 -4.1787353 -4.196094 -4.2081995 -4.217227 -4.2288656 -4.2398481 -4.2539363 -4.2565112 -4.2459784 -4.2274113 -4.2088885][-4.2079558 -4.2260351 -4.2337346 -4.2331529 -4.2333241 -4.2331319 -4.2216363 -4.2112474 -4.2222157 -4.2500792 -4.2782865 -4.291544 -4.2876229 -4.2698746 -4.2438135][-4.2549162 -4.2747464 -4.2801485 -4.2710848 -4.2509432 -4.2203379 -4.1736894 -4.138483 -4.154047 -4.2056432 -4.2586913 -4.2938261 -4.3061943 -4.300149 -4.2782583][-4.2736788 -4.2923293 -4.2937737 -4.2767615 -4.2402806 -4.1766558 -4.085546 -4.0159283 -4.0388703 -4.11983 -4.2002292 -4.2593541 -4.289362 -4.3003678 -4.2901826][-4.2627058 -4.2824774 -4.2810831 -4.254796 -4.2060022 -4.1174788 -3.9916947 -3.8918741 -3.9243855 -4.0307465 -4.1302352 -4.2055545 -4.2487397 -4.2731481 -4.2742219][-4.240458 -4.2604165 -4.2575321 -4.2290816 -4.1857886 -4.1067166 -3.9934802 -3.8985987 -3.9294226 -4.0225091 -4.1057935 -4.1729107 -4.2166805 -4.245738 -4.2499223][-4.2277513 -4.2446632 -4.2416544 -4.2229962 -4.1991882 -4.1551003 -4.0893927 -4.0322542 -4.0514278 -4.1024027 -4.146636 -4.1904006 -4.2224402 -4.242559 -4.2418575][-4.2400846 -4.2502055 -4.2427611 -4.2302284 -4.21903 -4.2008123 -4.1748385 -4.1535397 -4.1696448 -4.1966333 -4.2175961 -4.2411046 -4.2569814 -4.2613544 -4.2504745][-4.2560439 -4.2569766 -4.2472153 -4.2404966 -4.2429371 -4.2425261 -4.2376676 -4.2339716 -4.2477465 -4.2643323 -4.2753687 -4.2876086 -4.2930202 -4.2881031 -4.272016][-4.2679262 -4.2649078 -4.2552171 -4.2529645 -4.2630315 -4.2725277 -4.278717 -4.282558 -4.2937064 -4.3045373 -4.3101492 -4.3146586 -4.3152895 -4.3094029 -4.2946873][-4.2833385 -4.2770848 -4.2645874 -4.2595348 -4.2684679 -4.2797379 -4.2903657 -4.29884 -4.3068943 -4.3125811 -4.3137484 -4.3152528 -4.3163314 -4.3127789 -4.3008018][-4.2924786 -4.2843189 -4.2705617 -4.2629085 -4.2663188 -4.272831 -4.2814026 -4.2900634 -4.2939482 -4.2940249 -4.2917957 -4.2933583 -4.2987304 -4.3009334 -4.2930665]]...]
INFO - root - 2017-12-05 23:40:17.362018: step 55410, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 65h:50m:31s remains)
INFO - root - 2017-12-05 23:40:25.757062: step 55420, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 65h:01m:27s remains)
INFO - root - 2017-12-05 23:40:34.198974: step 55430, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.865 sec/batch; 66h:36m:39s remains)
INFO - root - 2017-12-05 23:40:42.754298: step 55440, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 63h:06m:10s remains)
INFO - root - 2017-12-05 23:40:51.270852: step 55450, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 65h:33m:13s remains)
INFO - root - 2017-12-05 23:40:59.736037: step 55460, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 66h:20m:50s remains)
INFO - root - 2017-12-05 23:41:08.130312: step 55470, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 66h:35m:14s remains)
INFO - root - 2017-12-05 23:41:16.634027: step 55480, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 65h:31m:48s remains)
INFO - root - 2017-12-05 23:41:25.185748: step 55490, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 66h:15m:27s remains)
INFO - root - 2017-12-05 23:41:33.686261: step 55500, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 65h:57m:27s remains)
2017-12-05 23:41:34.556776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0722051 -4.0957613 -4.1399388 -4.1740322 -4.1533737 -4.0936751 -4.0781746 -4.1191154 -4.166193 -4.2205114 -4.2646194 -4.3022995 -4.324254 -4.3290429 -4.3272123][-4.0975528 -4.1292763 -4.1677904 -4.1884384 -4.1650043 -4.1128535 -4.0924225 -4.1119404 -4.1447406 -4.1968112 -4.2501092 -4.2999129 -4.3297567 -4.3374643 -4.3343611][-4.1392465 -4.1705537 -4.1987495 -4.2085943 -4.1836567 -4.1361976 -4.1026058 -4.0941691 -4.1152906 -4.1726427 -4.2396312 -4.296299 -4.3285446 -4.3372488 -4.3313622][-4.1787767 -4.2028623 -4.2198234 -4.2219334 -4.19945 -4.1550269 -4.1107626 -4.0825763 -4.1015482 -4.1625061 -4.2312865 -4.2890148 -4.3236632 -4.334837 -4.3278513][-4.2166348 -4.2308335 -4.2376828 -4.2351575 -4.2153268 -4.1749239 -4.1239929 -4.083941 -4.1059761 -4.1694961 -4.2370648 -4.2915831 -4.3233242 -4.3340549 -4.3265562][-4.2479877 -4.2540216 -4.2492523 -4.2388573 -4.2170978 -4.1724524 -4.1045027 -4.0548096 -4.0965357 -4.1766214 -4.2526321 -4.3061385 -4.3307261 -4.3368363 -4.3283658][-4.258781 -4.2609572 -4.2524638 -4.2361355 -4.2025242 -4.1345377 -4.030448 -3.9752173 -4.0593991 -4.1759095 -4.2653122 -4.3175917 -4.3363218 -4.3387313 -4.3289003][-4.2433619 -4.2408457 -4.2322745 -4.2137632 -4.1659212 -4.0653167 -3.9244132 -3.8786883 -4.0146041 -4.1634688 -4.2607989 -4.3109279 -4.3257122 -4.3289971 -4.3199615][-4.2104416 -4.2071791 -4.2041354 -4.19423 -4.1452703 -4.0356951 -3.8969262 -3.8814344 -4.0287156 -4.1694522 -4.2548394 -4.2978406 -4.3108721 -4.3149877 -4.3075452][-4.2149353 -4.2134113 -4.2150464 -4.2122803 -4.174551 -4.0876274 -3.9896939 -3.9991105 -4.112289 -4.2106771 -4.2695956 -4.3024116 -4.3131585 -4.315937 -4.3070216][-4.2458882 -4.2416039 -4.2409859 -4.2375684 -4.2104969 -4.14853 -4.0888085 -4.1102071 -4.1912713 -4.2579913 -4.2954941 -4.3153162 -4.3205972 -4.3186979 -4.3071151][-4.2629914 -4.2548561 -4.2522278 -4.2462792 -4.2276049 -4.1885486 -4.1599636 -4.1861362 -4.2457442 -4.2912612 -4.3116627 -4.3202453 -4.3216982 -4.31751 -4.3051186][-4.2545815 -4.2520037 -4.2521973 -4.246614 -4.2352405 -4.2156429 -4.2084641 -4.2328253 -4.2732625 -4.300034 -4.3074226 -4.3106551 -4.3118639 -4.3062968 -4.2929506][-4.2444568 -4.2470503 -4.2488451 -4.2447004 -4.238595 -4.2327619 -4.234623 -4.2529826 -4.2786808 -4.2914782 -4.2918782 -4.2947426 -4.2964964 -4.2898655 -4.2769504][-4.2510738 -4.2526431 -4.2539148 -4.2512493 -4.2484937 -4.2485561 -4.2508044 -4.2631073 -4.2786765 -4.2839203 -4.2835011 -4.2886457 -4.29158 -4.2851748 -4.2744246]]...]
INFO - root - 2017-12-05 23:41:43.025716: step 55510, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 65h:02m:22s remains)
INFO - root - 2017-12-05 23:41:51.603751: step 55520, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 67h:14m:00s remains)
INFO - root - 2017-12-05 23:41:59.988723: step 55530, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 65h:29m:14s remains)
INFO - root - 2017-12-05 23:42:08.381177: step 55540, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 66h:13m:28s remains)
INFO - root - 2017-12-05 23:42:16.925861: step 55550, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 66h:02m:33s remains)
INFO - root - 2017-12-05 23:42:25.534434: step 55560, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 65h:12m:36s remains)
INFO - root - 2017-12-05 23:42:33.932421: step 55570, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 65h:04m:05s remains)
INFO - root - 2017-12-05 23:42:42.595442: step 55580, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 67h:38m:16s remains)
INFO - root - 2017-12-05 23:42:51.097022: step 55590, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 63h:59m:12s remains)
INFO - root - 2017-12-05 23:42:59.700361: step 55600, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 64h:15m:50s remains)
2017-12-05 23:43:00.518187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1882815 -4.21031 -4.2445889 -4.2709656 -4.27337 -4.2548933 -4.2340994 -4.2212734 -4.2199612 -4.2289443 -4.2443733 -4.2577882 -4.2546663 -4.2510533 -4.2570543][-4.1928024 -4.2149482 -4.2474494 -4.2731953 -4.271626 -4.2455153 -4.2202315 -4.2112155 -4.2190747 -4.242846 -4.265542 -4.2795382 -4.27697 -4.2729764 -4.2716842][-4.1708307 -4.1933994 -4.2315178 -4.26402 -4.2644353 -4.2365994 -4.2107358 -4.2098594 -4.22734 -4.2576451 -4.2792315 -4.286654 -4.2812843 -4.271719 -4.2607818][-4.1453609 -4.1730494 -4.2199106 -4.25537 -4.2605414 -4.2363939 -4.211133 -4.2077785 -4.2257614 -4.2534413 -4.2686977 -4.2664204 -4.2540274 -4.2357078 -4.2162266][-4.1376138 -4.1692553 -4.2191772 -4.2500196 -4.252461 -4.228981 -4.1943722 -4.181406 -4.1965561 -4.2208061 -4.2321749 -4.2222714 -4.2059174 -4.182694 -4.1617465][-4.1382861 -4.1707258 -4.2182603 -4.2415743 -4.234735 -4.202136 -4.1495442 -4.1182208 -4.1322379 -4.164207 -4.1812563 -4.17376 -4.162499 -4.1410861 -4.1223984][-4.1293507 -4.1636577 -4.2060127 -4.2229242 -4.2090664 -4.1670704 -4.1001158 -4.0482483 -4.0601721 -4.1002536 -4.126657 -4.1263394 -4.1250529 -4.114851 -4.1037297][-4.1074977 -4.14777 -4.1923008 -4.2099862 -4.1950388 -4.1516771 -4.0823789 -4.02415 -4.0304995 -4.0702519 -4.1014347 -4.1077743 -4.1138282 -4.1122217 -4.1064277][-4.1019988 -4.1469483 -4.1961231 -4.2160888 -4.2010016 -4.1597042 -4.0943251 -4.0404525 -4.04378 -4.0763488 -4.1115251 -4.1247025 -4.1319609 -4.133605 -4.1299095][-4.1369815 -4.1789289 -4.2214346 -4.2389183 -4.2222595 -4.1824493 -4.1234341 -4.0822387 -4.0858645 -4.11559 -4.1492047 -4.1641531 -4.1727381 -4.1761131 -4.1703577][-4.1734672 -4.2078695 -4.2423134 -4.2547293 -4.2398143 -4.2094126 -4.1590881 -4.1301112 -4.1369729 -4.1662846 -4.1931086 -4.2057991 -4.214613 -4.2191119 -4.2143993][-4.1902108 -4.2216086 -4.2460303 -4.2486567 -4.2363014 -4.2197189 -4.1851716 -4.166286 -4.1771989 -4.2027988 -4.2215176 -4.2306042 -4.2384467 -4.2444062 -4.2441578][-4.1864085 -4.2115 -4.222312 -4.2115593 -4.1995358 -4.1999092 -4.1865277 -4.1830821 -4.1964149 -4.2147441 -4.227561 -4.2352052 -4.2430444 -4.2525654 -4.2577753][-4.1673541 -4.183639 -4.181097 -4.1591797 -4.1441522 -4.1586657 -4.169054 -4.1839461 -4.2054095 -4.2213249 -4.2339683 -4.24237 -4.2506294 -4.2636318 -4.2744966][-4.1630845 -4.16818 -4.1584225 -4.1334028 -4.1150589 -4.1361966 -4.165812 -4.1961122 -4.2244267 -4.2413893 -4.2554145 -4.2652316 -4.2744608 -4.2875738 -4.2985516]]...]
INFO - root - 2017-12-05 23:43:08.941031: step 55610, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 64h:11m:04s remains)
INFO - root - 2017-12-05 23:43:17.571215: step 55620, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 66h:49m:01s remains)
INFO - root - 2017-12-05 23:43:26.195656: step 55630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 65h:45m:06s remains)
INFO - root - 2017-12-05 23:43:34.408861: step 55640, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.806 sec/batch; 61h:57m:04s remains)
INFO - root - 2017-12-05 23:43:42.897537: step 55650, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 64h:56m:27s remains)
INFO - root - 2017-12-05 23:43:51.456509: step 55660, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 66h:24m:54s remains)
INFO - root - 2017-12-05 23:43:59.930548: step 55670, loss = 2.02, batch loss = 1.96 (9.3 examples/sec; 0.859 sec/batch; 66h:02m:52s remains)
INFO - root - 2017-12-05 23:44:08.580119: step 55680, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 63h:59m:38s remains)
INFO - root - 2017-12-05 23:44:17.203755: step 55690, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 65h:07m:55s remains)
INFO - root - 2017-12-05 23:44:25.901281: step 55700, loss = 2.02, batch loss = 1.96 (9.1 examples/sec; 0.881 sec/batch; 67h:43m:01s remains)
2017-12-05 23:44:26.652829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3061309 -4.3008327 -4.2940817 -4.28599 -4.2716141 -4.2484708 -4.2221384 -4.2119846 -4.2457662 -4.2794518 -4.2919536 -4.3019557 -4.3054004 -4.2965503 -4.2879763][-4.3068838 -4.3040433 -4.2993684 -4.2962804 -4.2855148 -4.2639451 -4.2381573 -4.2274179 -4.2553287 -4.2847672 -4.2975025 -4.3104205 -4.3152671 -4.3086691 -4.3018036][-4.3069258 -4.3051147 -4.3028193 -4.3052 -4.29484 -4.2687306 -4.2362132 -4.215601 -4.2351542 -4.2649612 -4.2838745 -4.3030286 -4.3128552 -4.3113976 -4.3078957][-4.3045893 -4.302568 -4.3023262 -4.3059673 -4.289206 -4.2503519 -4.2006421 -4.1599655 -4.1749163 -4.2156091 -4.24846 -4.2781773 -4.2972317 -4.302474 -4.3011422][-4.3026 -4.2980494 -4.2945466 -4.2911773 -4.263783 -4.2062292 -4.1259217 -4.0557308 -4.0655789 -4.1239347 -4.1795859 -4.228024 -4.263917 -4.2794442 -4.2775078][-4.30023 -4.2913918 -4.2816973 -4.2692208 -4.2271433 -4.1409283 -4.0206475 -3.9097638 -3.9135189 -4.0000663 -4.0863571 -4.1598663 -4.2157803 -4.2404943 -4.2343197][-4.2961011 -4.2828755 -4.2661428 -4.2397966 -4.1785712 -4.0677285 -3.9152603 -3.7691052 -3.7686179 -3.8883662 -4.0092587 -4.1030169 -4.1715307 -4.2006664 -4.1913185][-4.2893596 -4.2703223 -4.2474937 -4.2116661 -4.14222 -4.029923 -3.8792109 -3.7396386 -3.7547028 -3.8895745 -4.0125747 -4.1008773 -4.1542115 -4.1681404 -4.1498289][-4.2822337 -4.2598257 -4.2356148 -4.1984186 -4.1372452 -4.0501308 -3.9415317 -3.8546467 -3.8986118 -4.0107288 -4.0886297 -4.1412683 -4.1640415 -4.1505175 -4.1171236][-4.2747555 -4.2484193 -4.2207866 -4.183321 -4.1301465 -4.0675974 -4.0028386 -3.966047 -4.0260782 -4.1119246 -4.1503267 -4.1684341 -4.1625619 -4.1283092 -4.0909882][-4.2738805 -4.2440252 -4.213953 -4.1764359 -4.1261954 -4.0765424 -4.0388861 -4.0271721 -4.0905662 -4.1570683 -4.1734147 -4.1700854 -4.1479774 -4.112566 -4.0886126][-4.2880569 -4.2613816 -4.2351069 -4.2007327 -4.1544189 -4.1090994 -4.0777249 -4.071084 -4.1240282 -4.1741571 -4.1819468 -4.1733246 -4.1543045 -4.133698 -4.1280475][-4.3126993 -4.2951283 -4.278017 -4.2537894 -4.2161827 -4.1754322 -4.1474562 -4.139791 -4.176784 -4.2115316 -4.215055 -4.2094736 -4.1988268 -4.1902971 -4.1954017][-4.338347 -4.3298669 -4.3210845 -4.3070865 -4.2843533 -4.2577891 -4.2415986 -4.2377453 -4.2601194 -4.2802997 -4.2786427 -4.2721033 -4.2635808 -4.2590494 -4.264936][-4.3546028 -4.3529062 -4.350482 -4.3437839 -4.3323407 -4.3198004 -4.3148513 -4.3176165 -4.3300443 -4.3379731 -4.332674 -4.3256321 -4.3185821 -4.3161416 -4.3201327]]...]
INFO - root - 2017-12-05 23:44:35.195351: step 55710, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 64h:41m:54s remains)
INFO - root - 2017-12-05 23:44:43.695162: step 55720, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 65h:03m:09s remains)
INFO - root - 2017-12-05 23:44:52.396802: step 55730, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 65h:21m:06s remains)
INFO - root - 2017-12-05 23:45:00.885954: step 55740, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 65h:10m:28s remains)
INFO - root - 2017-12-05 23:45:09.190964: step 55750, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 64h:38m:19s remains)
INFO - root - 2017-12-05 23:45:17.742576: step 55760, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 66h:05m:21s remains)
INFO - root - 2017-12-05 23:45:26.266413: step 55770, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 63h:17m:02s remains)
INFO - root - 2017-12-05 23:45:34.780228: step 55780, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 66h:15m:38s remains)
INFO - root - 2017-12-05 23:45:43.433885: step 55790, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 69h:13m:26s remains)
INFO - root - 2017-12-05 23:45:51.830268: step 55800, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 63h:58m:16s remains)
2017-12-05 23:45:52.768945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2368412 -4.2411695 -4.2369957 -4.2333937 -4.230319 -4.2357817 -4.2435 -4.2432737 -4.2390876 -4.2392249 -4.2418914 -4.2453866 -4.2497344 -4.2540016 -4.255733][-4.2062535 -4.2096705 -4.2078671 -4.2103381 -4.2143478 -4.225677 -4.2368226 -4.2386613 -4.234004 -4.231503 -4.2302318 -4.2300982 -4.2332597 -4.2375221 -4.2384491][-4.1904907 -4.19639 -4.1965194 -4.2013731 -4.2101364 -4.2230577 -4.233819 -4.2360573 -4.2318559 -4.2286754 -4.2259111 -4.22184 -4.2178178 -4.2140574 -4.2064724][-4.1968322 -4.2096682 -4.2146134 -4.2228909 -4.2315674 -4.239985 -4.2463694 -4.2495432 -4.2493606 -4.2489948 -4.2484813 -4.2436934 -4.2323165 -4.2164049 -4.1944618][-4.1993351 -4.2204289 -4.2327166 -4.2438684 -4.2492833 -4.2500691 -4.2496982 -4.2516632 -4.2570243 -4.26389 -4.27305 -4.2755742 -4.2640419 -4.2448411 -4.2192059][-4.1753697 -4.2085409 -4.2314596 -4.2444158 -4.2396755 -4.2270513 -4.2148466 -4.2097836 -4.2176895 -4.2331605 -4.2561841 -4.27142 -4.2676582 -4.25587 -4.2423677][-4.1340866 -4.1701827 -4.1952591 -4.2021084 -4.1824675 -4.1522064 -4.1194057 -4.0975752 -4.1090164 -4.1403646 -4.1816525 -4.2126489 -4.2219157 -4.2274938 -4.2368736][-4.0990386 -4.1232991 -4.1378431 -4.1284785 -4.0912971 -4.04967 -3.9970353 -3.9537981 -3.9679978 -4.0141816 -4.0684037 -4.1131778 -4.13489 -4.1562862 -4.187067][-4.1258492 -4.1340609 -4.1348176 -4.1124411 -4.0687943 -4.031476 -3.9829226 -3.9396021 -3.9492402 -3.9832339 -4.0230079 -4.060689 -4.0814924 -4.1069841 -4.1457644][-4.2111473 -4.2158074 -4.2122536 -4.189693 -4.1564345 -4.1321225 -4.1030111 -4.0774312 -4.0804687 -4.0933986 -4.109189 -4.1297641 -4.1416564 -4.1590533 -4.188055][-4.28435 -4.2899704 -4.2867808 -4.2706256 -4.2497759 -4.23509 -4.2217007 -4.2094369 -4.20897 -4.2151241 -4.2219977 -4.2344522 -4.2416158 -4.2501583 -4.2650142][-4.3205452 -4.3239317 -4.3222337 -4.313828 -4.3029628 -4.2959175 -4.2900476 -4.2852831 -4.28509 -4.289628 -4.2928343 -4.2984352 -4.2981458 -4.2951584 -4.2983775][-4.3279152 -4.3256612 -4.324224 -4.3214173 -4.3176346 -4.313601 -4.3090782 -4.3057528 -4.3053131 -4.3067183 -4.3048863 -4.2998834 -4.2893081 -4.2788043 -4.2775993][-4.3211279 -4.3137374 -4.310873 -4.309917 -4.3081217 -4.3025575 -4.2944117 -4.2881861 -4.2885113 -4.2916465 -4.2855377 -4.2706723 -4.2516003 -4.2366452 -4.2331066][-4.30926 -4.298986 -4.29256 -4.2893171 -4.283689 -4.2708616 -4.255342 -4.2463126 -4.2505765 -4.2599826 -4.2543273 -4.232738 -4.2051282 -4.1816306 -4.1714535]]...]
INFO - root - 2017-12-05 23:46:01.244491: step 55810, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 64h:28m:29s remains)
INFO - root - 2017-12-05 23:46:09.768432: step 55820, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 66h:23m:34s remains)
INFO - root - 2017-12-05 23:46:18.223865: step 55830, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 65h:12m:42s remains)
INFO - root - 2017-12-05 23:46:26.821954: step 55840, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 67h:02m:31s remains)
INFO - root - 2017-12-05 23:46:35.319676: step 55850, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 65h:04m:13s remains)
INFO - root - 2017-12-05 23:46:43.722780: step 55860, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 64h:32m:12s remains)
INFO - root - 2017-12-05 23:46:52.145837: step 55870, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 65h:39m:30s remains)
INFO - root - 2017-12-05 23:47:00.761318: step 55880, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 66h:12m:34s remains)
INFO - root - 2017-12-05 23:47:09.347944: step 55890, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 66h:41m:02s remains)
INFO - root - 2017-12-05 23:47:18.054242: step 55900, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 69h:20m:30s remains)
2017-12-05 23:47:18.849086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.03392 -4.1345558 -4.21323 -4.2681484 -4.3018994 -4.3186011 -4.3216767 -4.3175178 -4.3102269 -4.301919 -4.2909727 -4.27764 -4.2590604 -4.2350159 -4.2129273][-4.1347189 -4.2029104 -4.2553105 -4.2890663 -4.3067856 -4.3155317 -4.3171363 -4.3148265 -4.3102088 -4.3071923 -4.3038712 -4.2988405 -4.2895613 -4.2747498 -4.258585][-4.2333908 -4.2706151 -4.2963834 -4.31005 -4.3140163 -4.3143253 -4.313108 -4.3108706 -4.3073425 -4.3042512 -4.3020029 -4.2997746 -4.2960925 -4.2913446 -4.2864766][-4.2977786 -4.3106313 -4.314539 -4.3141108 -4.3091507 -4.3037906 -4.3037281 -4.3076358 -4.30828 -4.30588 -4.3026237 -4.2975821 -4.2913408 -4.2877407 -4.28799][-4.3191338 -4.3128581 -4.30032 -4.2864342 -4.273068 -4.2651353 -4.2718377 -4.2880864 -4.299294 -4.3043351 -4.306098 -4.3023787 -4.2936616 -4.2852778 -4.2816944][-4.3117 -4.2919021 -4.2638969 -4.2351174 -4.2124572 -4.2014112 -4.2136855 -4.2416067 -4.2702322 -4.2918205 -4.3075995 -4.31347 -4.3079619 -4.2986751 -4.2901816][-4.290329 -4.2576189 -4.2146888 -4.1701055 -4.1354394 -4.112432 -4.11459 -4.1516886 -4.2068415 -4.2575 -4.296546 -4.3176208 -4.3206935 -4.3151417 -4.306067][-4.2747774 -4.2311335 -4.1718607 -4.1069179 -4.0481429 -3.9938047 -3.9653845 -4.0068421 -4.1001167 -4.1912231 -4.2591758 -4.2987752 -4.3151212 -4.3178959 -4.3127203][-4.2783504 -4.2340364 -4.1695786 -4.0887113 -3.9987521 -3.9002228 -3.8222141 -3.8553531 -3.980823 -4.1066628 -4.1972308 -4.2520747 -4.2809596 -4.2928972 -4.2955174][-4.2989478 -4.2649531 -4.2103448 -4.1331 -4.03655 -3.9201856 -3.8118806 -3.8109479 -3.9232988 -4.0468545 -4.1388311 -4.1962051 -4.2280121 -4.2424192 -4.2505589][-4.3193216 -4.2991133 -4.26196 -4.2048092 -4.127193 -4.0308113 -3.9390132 -3.9190536 -3.9847693 -4.0686917 -4.134367 -4.1722651 -4.1884789 -4.1902056 -4.19569][-4.3302674 -4.3209171 -4.3007822 -4.2665267 -4.2169247 -4.1546912 -4.0926619 -4.069654 -4.1005192 -4.1499615 -4.187325 -4.1998882 -4.1929274 -4.175837 -4.1695991][-4.3321972 -4.3319035 -4.3248849 -4.3097816 -4.2860394 -4.255095 -4.2208424 -4.2029963 -4.2141161 -4.2381954 -4.2541671 -4.2497296 -4.2304688 -4.2046561 -4.1885576][-4.3299279 -4.3340554 -4.3351474 -4.3316569 -4.3238235 -4.3128428 -4.298986 -4.2908964 -4.2953057 -4.3032913 -4.3032804 -4.2901397 -4.2676945 -4.2441406 -4.2299075][-4.3269944 -4.3318853 -4.3359222 -4.3384972 -4.3380103 -4.335937 -4.3319993 -4.328753 -4.3280993 -4.3248158 -4.3155384 -4.2998867 -4.2799211 -4.263566 -4.2584534]]...]
INFO - root - 2017-12-05 23:47:27.441080: step 55910, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 64h:30m:05s remains)
INFO - root - 2017-12-05 23:47:36.130883: step 55920, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 71h:31m:58s remains)
INFO - root - 2017-12-05 23:47:44.800851: step 55930, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 67h:04m:39s remains)
INFO - root - 2017-12-05 23:47:53.285860: step 55940, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 65h:29m:42s remains)
INFO - root - 2017-12-05 23:48:01.853430: step 55950, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 67h:18m:16s remains)
INFO - root - 2017-12-05 23:48:10.341329: step 55960, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 62h:44m:46s remains)
INFO - root - 2017-12-05 23:48:18.622622: step 55970, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 63h:53m:41s remains)
INFO - root - 2017-12-05 23:48:27.156303: step 55980, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 63h:49m:20s remains)
INFO - root - 2017-12-05 23:48:35.691687: step 55990, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 63h:00m:31s remains)
INFO - root - 2017-12-05 23:48:44.289314: step 56000, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 65h:46m:22s remains)
2017-12-05 23:48:45.063908: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.284112 -4.2659869 -4.2466545 -4.2370405 -4.2405534 -4.239203 -4.2323642 -4.2264428 -4.2205834 -4.2134662 -4.2134409 -4.2305694 -4.2450452 -4.2465911 -4.2270203][-4.27236 -4.252131 -4.2296767 -4.218976 -4.2254052 -4.2314515 -4.2329559 -4.2339935 -4.2321372 -4.2236176 -4.2163744 -4.2322755 -4.255496 -4.2649164 -4.2447877][-4.2654619 -4.244339 -4.2183084 -4.2018309 -4.2035336 -4.2087135 -4.21482 -4.2209206 -4.2219586 -4.2110796 -4.1990719 -4.21794 -4.2520976 -4.2689319 -4.25354][-4.2661843 -4.2458563 -4.2156444 -4.1933722 -4.1866183 -4.184876 -4.1887293 -4.195539 -4.1988907 -4.1857824 -4.173543 -4.1974964 -4.2402363 -4.2600012 -4.2521553][-4.2734432 -4.253612 -4.2207127 -4.1945567 -4.177763 -4.1639748 -4.1577358 -4.1583323 -4.1616817 -4.1511827 -4.1454439 -4.17313 -4.2168856 -4.238719 -4.2397842][-4.2798223 -4.262466 -4.229569 -4.1979914 -4.1687264 -4.1396952 -4.1224747 -4.1178775 -4.1237626 -4.1254458 -4.1303658 -4.1610761 -4.2018309 -4.2243128 -4.2331305][-4.2884855 -4.2749724 -4.2408648 -4.1987085 -4.1538949 -4.1117382 -4.0905414 -4.0913486 -4.1101203 -4.1296639 -4.1442294 -4.1721816 -4.2043691 -4.2214284 -4.2317953][-4.2980685 -4.2866426 -4.2503777 -4.1982489 -4.1411715 -4.0902863 -4.0707812 -4.0810318 -4.112813 -4.1461649 -4.1678 -4.1917467 -4.2169642 -4.2287621 -4.2383146][-4.3021221 -4.2895803 -4.2488036 -4.1887121 -4.1232548 -4.0683408 -4.0563164 -4.0799866 -4.1242042 -4.16514 -4.1873007 -4.203033 -4.219327 -4.2267265 -4.2325826][-4.3035846 -4.2892323 -4.246995 -4.1818066 -4.11348 -4.0628333 -4.0594249 -4.08954 -4.1357183 -4.1763597 -4.1955695 -4.2010388 -4.2089591 -4.2156472 -4.2219958][-4.3046813 -4.290647 -4.2503881 -4.1860647 -4.121665 -4.0800028 -4.0806146 -4.1087952 -4.1496477 -4.1828585 -4.1969562 -4.1955314 -4.1986275 -4.2058854 -4.2159677][-4.3062673 -4.2932849 -4.2580767 -4.2004209 -4.1440482 -4.1099448 -4.1103439 -4.13333 -4.1679397 -4.1926327 -4.1986227 -4.1930943 -4.19463 -4.2016358 -4.213882][-4.3131723 -4.3038144 -4.2784877 -4.2338233 -4.1891603 -4.1629028 -4.1609759 -4.1784377 -4.20537 -4.2227387 -4.2216845 -4.2138486 -4.2138472 -4.219141 -4.2293892][-4.3205743 -4.3158741 -4.300703 -4.2702742 -4.2404194 -4.2259011 -4.2260756 -4.2391233 -4.2579265 -4.26728 -4.2621217 -4.2528529 -4.2523689 -4.2582603 -4.2654233][-4.3284149 -4.3279228 -4.3214469 -4.3038459 -4.2882681 -4.2839379 -4.2869978 -4.2963991 -4.309135 -4.3114772 -4.3041086 -4.2940793 -4.2946177 -4.3025327 -4.3082132]]...]
INFO - root - 2017-12-05 23:48:53.561155: step 56010, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 66h:45m:20s remains)
INFO - root - 2017-12-05 23:49:02.272201: step 56020, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 67h:17m:08s remains)
INFO - root - 2017-12-05 23:49:10.923084: step 56030, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 65h:49m:07s remains)
INFO - root - 2017-12-05 23:49:19.402147: step 56040, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 63h:15m:38s remains)
INFO - root - 2017-12-05 23:49:28.149980: step 56050, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 69h:00m:19s remains)
INFO - root - 2017-12-05 23:49:36.745849: step 56060, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 65h:15m:17s remains)
INFO - root - 2017-12-05 23:49:45.077209: step 56070, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 65h:08m:55s remains)
INFO - root - 2017-12-05 23:49:53.539698: step 56080, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.826 sec/batch; 63h:24m:49s remains)
INFO - root - 2017-12-05 23:50:02.085588: step 56090, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 65h:29m:17s remains)
INFO - root - 2017-12-05 23:50:10.762182: step 56100, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 66h:34m:53s remains)
2017-12-05 23:50:11.519063: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1515265 -4.1353168 -4.1427565 -4.1595254 -4.1699085 -4.1657991 -4.1625662 -4.1716924 -4.1740994 -4.1696043 -4.1605678 -4.1688013 -4.1753364 -4.1765704 -4.1745024][-4.1830773 -4.1834841 -4.1972337 -4.2152677 -4.221107 -4.2092834 -4.1992073 -4.2059855 -4.2124157 -4.2141562 -4.211401 -4.22198 -4.2327137 -4.2377663 -4.2361789][-4.2128005 -4.216743 -4.2267518 -4.2385197 -4.238802 -4.226644 -4.2156715 -4.2231612 -4.2368326 -4.2487755 -4.25653 -4.2719278 -4.2851329 -4.2905221 -4.28634][-4.2466273 -4.2440281 -4.24399 -4.243063 -4.2379408 -4.2246432 -4.2119436 -4.2186518 -4.2398133 -4.2645245 -4.2847381 -4.306118 -4.3217783 -4.3261657 -4.31669][-4.2763629 -4.2634554 -4.2507124 -4.2354746 -4.2181721 -4.1912951 -4.1638708 -4.1621351 -4.1902895 -4.2295895 -4.2630405 -4.2931066 -4.3137937 -4.3191147 -4.3052244][-4.2836576 -4.2585545 -4.22746 -4.1945243 -4.1613593 -4.1135392 -4.0575781 -4.0378389 -4.0748715 -4.1356359 -4.18766 -4.2312021 -4.2609544 -4.2715993 -4.2592497][-4.2611427 -4.2291121 -4.1838331 -4.1331291 -4.0779691 -4.0034161 -3.911413 -3.8678041 -3.9170287 -4.0045695 -4.0840688 -4.1523166 -4.2022133 -4.2285957 -4.2291679][-4.2254848 -4.1961389 -4.1530371 -4.0991688 -4.0339122 -3.9439018 -3.8282311 -3.762064 -3.812968 -3.9147613 -4.0144238 -4.1038208 -4.171917 -4.2157726 -4.23383][-4.2123427 -4.19398 -4.1657729 -4.1248927 -4.07303 -3.9985757 -3.9027295 -3.8350847 -3.8640318 -3.9476717 -4.0429854 -4.1309972 -4.19622 -4.2396879 -4.2606592][-4.242383 -4.230844 -4.2133055 -4.1854191 -4.152513 -4.1020155 -4.0372796 -3.9830976 -3.992161 -4.0482712 -4.122961 -4.1928353 -4.2451243 -4.2799387 -4.2920222][-4.2824764 -4.27204 -4.2594495 -4.2405834 -4.22013 -4.1877022 -4.1454816 -4.1069436 -4.1051464 -4.1415458 -4.1975107 -4.2489 -4.2869239 -4.3115988 -4.314899][-4.2994976 -4.2870541 -4.2761183 -4.2643356 -4.2522807 -4.2369881 -4.2133126 -4.1875153 -4.17931 -4.2005715 -4.2418561 -4.2807465 -4.3092685 -4.3256569 -4.32264][-4.2867465 -4.2722116 -4.2622137 -4.2559366 -4.2494693 -4.2435617 -4.2315702 -4.2141061 -4.206656 -4.2226887 -4.2585278 -4.2914267 -4.3131852 -4.3233652 -4.3198142][-4.2605529 -4.245039 -4.2338138 -4.2279878 -4.2236757 -4.2235112 -4.2210374 -4.2157087 -4.216527 -4.234798 -4.2678618 -4.2962441 -4.3117328 -4.3171339 -4.3131618][-4.2417603 -4.2285733 -4.2182555 -4.21442 -4.2144804 -4.2192459 -4.2230387 -4.2245808 -4.2302265 -4.2465291 -4.2731528 -4.29485 -4.3043761 -4.3067522 -4.3030353]]...]
INFO - root - 2017-12-05 23:50:20.077649: step 56110, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 65h:18m:22s remains)
INFO - root - 2017-12-05 23:50:28.693913: step 56120, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 67h:30m:32s remains)
INFO - root - 2017-12-05 23:50:37.190037: step 56130, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 64h:46m:39s remains)
INFO - root - 2017-12-05 23:50:45.766841: step 56140, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 63h:56m:27s remains)
INFO - root - 2017-12-05 23:50:54.406808: step 56150, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 68h:33m:57s remains)
INFO - root - 2017-12-05 23:51:03.003911: step 56160, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 65h:12m:44s remains)
INFO - root - 2017-12-05 23:51:11.495427: step 56170, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 67h:11m:32s remains)
INFO - root - 2017-12-05 23:51:20.117275: step 56180, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 65h:27m:40s remains)
INFO - root - 2017-12-05 23:51:28.500272: step 56190, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 66h:11m:13s remains)
INFO - root - 2017-12-05 23:51:37.024121: step 56200, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.826 sec/batch; 63h:23m:17s remains)
2017-12-05 23:51:37.851632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2591629 -4.2603731 -4.2560034 -4.2475319 -4.2363372 -4.2283125 -4.2255759 -4.2284579 -4.2282023 -4.2223849 -4.2171431 -4.2220707 -4.2331166 -4.244194 -4.2496486][-4.2599058 -4.261611 -4.2569027 -4.2471633 -4.233511 -4.2226562 -4.2183166 -4.2215023 -4.21939 -4.2083597 -4.1977754 -4.2029948 -4.2183647 -4.2350988 -4.2486][-4.2504549 -4.2528706 -4.2494121 -4.241621 -4.2274928 -4.2134886 -4.2083454 -4.2118745 -4.2098451 -4.1984096 -4.1848235 -4.1892567 -4.2070332 -4.2265363 -4.2465711][-4.2348404 -4.2377043 -4.2376833 -4.2345891 -4.2188721 -4.2029691 -4.1978827 -4.197279 -4.1960316 -4.1927266 -4.181994 -4.1835175 -4.1993828 -4.2189636 -4.2432003][-4.212965 -4.2161136 -4.2192216 -4.2200737 -4.2012715 -4.1778989 -4.1656413 -4.1619143 -4.1706281 -4.1884356 -4.1930709 -4.1966648 -4.2068958 -4.2225232 -4.244997][-4.2037277 -4.203619 -4.2006493 -4.1965795 -4.1687469 -4.1267653 -4.0941024 -4.0843639 -4.1148138 -4.1639791 -4.1946554 -4.2041583 -4.2077713 -4.2170572 -4.2336411][-4.2049503 -4.1956925 -4.1791782 -4.1646862 -4.1246567 -4.0545697 -3.9828331 -3.9532311 -4.0098042 -4.09752 -4.1588426 -4.1810594 -4.1821351 -4.1883769 -4.2045135][-4.2007046 -4.1819687 -4.1529803 -4.1279025 -4.0781212 -3.9856715 -3.8764136 -3.8230543 -3.9003091 -4.020472 -4.1066823 -4.1415443 -4.1421485 -4.1485486 -4.1732235][-4.2035704 -4.187252 -4.1597333 -4.1363091 -4.0959749 -4.0169477 -3.9226599 -3.8761318 -3.9372056 -4.0399456 -4.1122994 -4.1361747 -4.12845 -4.1301441 -4.1557522][-4.2257357 -4.21719 -4.1996427 -4.1854987 -4.1623406 -4.1137791 -4.0611925 -4.0327029 -4.0618014 -4.1230688 -4.1649141 -4.1680708 -4.1499367 -4.1451464 -4.1637149][-4.2384262 -4.2343559 -4.2254548 -4.2216306 -4.2159204 -4.1940465 -4.1710024 -4.1553707 -4.1650214 -4.1949897 -4.2153106 -4.2113481 -4.1953564 -4.1915169 -4.2041678][-4.2306089 -4.2327762 -4.2329803 -4.2393174 -4.2471914 -4.2453866 -4.2419958 -4.2394938 -4.2426028 -4.253202 -4.2594471 -4.2541585 -4.2437873 -4.24108 -4.2490358][-4.2010036 -4.2100697 -4.21941 -4.2335639 -4.2483888 -4.2563238 -4.2638226 -4.2696967 -4.272099 -4.2739344 -4.2747416 -4.2723894 -4.2682395 -4.2664294 -4.2689948][-4.1601934 -4.1774049 -4.1965981 -4.217597 -4.2374177 -4.2490821 -4.2563682 -4.2619371 -4.2648048 -4.2658029 -4.2700009 -4.2735543 -4.2735496 -4.2721062 -4.2699037][-4.1354942 -4.1570058 -4.1835871 -4.2097931 -4.2337127 -4.2476935 -4.2506595 -4.2508917 -4.2513709 -4.2533712 -4.2608223 -4.2693443 -4.2732291 -4.2732797 -4.2694373]]...]
INFO - root - 2017-12-05 23:51:46.401318: step 56210, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 64h:43m:43s remains)
INFO - root - 2017-12-05 23:51:54.877380: step 56220, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 66h:00m:50s remains)
INFO - root - 2017-12-05 23:52:03.511576: step 56230, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 65h:05m:52s remains)
INFO - root - 2017-12-05 23:52:11.978781: step 56240, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.808 sec/batch; 61h:58m:46s remains)
INFO - root - 2017-12-05 23:52:20.496588: step 56250, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 64h:34m:15s remains)
INFO - root - 2017-12-05 23:52:29.100313: step 56260, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 64h:46m:44s remains)
INFO - root - 2017-12-05 23:52:37.539999: step 56270, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 64h:12m:22s remains)
INFO - root - 2017-12-05 23:52:46.064998: step 56280, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 65h:41m:15s remains)
INFO - root - 2017-12-05 23:52:54.704044: step 56290, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 66h:03m:15s remains)
INFO - root - 2017-12-05 23:53:03.031468: step 56300, loss = 2.04, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 63h:27m:36s remains)
2017-12-05 23:53:03.913341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2168236 -4.2599373 -4.2896123 -4.2923503 -4.28749 -4.2871184 -4.2817612 -4.257483 -4.2322636 -4.2201438 -4.2222862 -4.2403851 -4.2658715 -4.2844877 -4.287499][-4.2202935 -4.2676539 -4.2986045 -4.301908 -4.2971888 -4.2911043 -4.2745371 -4.2424879 -4.2154336 -4.210238 -4.2242484 -4.2480264 -4.2738295 -4.2890849 -4.2864375][-4.2235069 -4.2716532 -4.3003011 -4.3017459 -4.2909188 -4.2726049 -4.2404771 -4.2042761 -4.1914859 -4.2128258 -4.2434855 -4.2681003 -4.2895269 -4.2978563 -4.2889361][-4.2340612 -4.2828794 -4.3086257 -4.3041697 -4.2768354 -4.2363257 -4.1819949 -4.1443944 -4.1558061 -4.2102394 -4.2580085 -4.2828212 -4.3005652 -4.3057342 -4.2967572][-4.2456841 -4.2901883 -4.3094735 -4.2932682 -4.245132 -4.1751394 -4.09534 -4.063427 -4.1119318 -4.1986008 -4.2610378 -4.2882986 -4.3008227 -4.3015594 -4.2960167][-4.2589459 -4.2994475 -4.3065858 -4.2732172 -4.2017403 -4.101337 -3.9999852 -3.9908075 -4.0866909 -4.1962953 -4.2651148 -4.2945886 -4.3028312 -4.2990484 -4.2951965][-4.269381 -4.3041553 -4.2994494 -4.2509193 -4.1576056 -4.029428 -3.9126687 -3.9432251 -4.086411 -4.206697 -4.2724028 -4.298161 -4.3007936 -4.295393 -4.29178][-4.2763715 -4.3008814 -4.2883396 -4.2321157 -4.124784 -3.9820919 -3.866653 -3.9387481 -4.1066828 -4.2207031 -4.2768221 -4.2941003 -4.2941184 -4.2945065 -4.2917433][-4.2729154 -4.2910485 -4.2787762 -4.2226458 -4.120276 -3.9902306 -3.9016941 -3.9930892 -4.1497297 -4.2438388 -4.28383 -4.2908649 -4.2926126 -4.3033361 -4.3021522][-4.2721705 -4.2815766 -4.2702041 -4.2190528 -4.1312103 -4.0290804 -3.9777765 -4.0689344 -4.1970277 -4.2684908 -4.2917919 -4.2906103 -4.2965369 -4.3107486 -4.3066664][-4.2682629 -4.2720366 -4.2623258 -4.2171063 -4.1456947 -4.0741611 -4.0563378 -4.1390462 -4.2391758 -4.2858882 -4.2923608 -4.2862544 -4.2928977 -4.3068194 -4.3041077][-4.2671666 -4.264143 -4.253602 -4.2131944 -4.1595616 -4.1217179 -4.1332273 -4.2059336 -4.2798729 -4.3025994 -4.2969127 -4.2928786 -4.2987771 -4.3083234 -4.3019867][-4.2699037 -4.2614903 -4.2461319 -4.2102647 -4.176774 -4.16592 -4.1933861 -4.2536831 -4.3029017 -4.3096819 -4.3052096 -4.3072395 -4.3128943 -4.3169641 -4.2997508][-4.2625756 -4.256855 -4.2472315 -4.2235041 -4.2055616 -4.207818 -4.2370763 -4.2834764 -4.3119392 -4.309267 -4.30732 -4.313159 -4.3228889 -4.3278475 -4.3043432][-4.2490697 -4.2543826 -4.2504683 -4.2386289 -4.2301726 -4.2377357 -4.2645812 -4.2941 -4.303906 -4.2952147 -4.2985883 -4.3112626 -4.3266306 -4.3330445 -4.309094]]...]
INFO - root - 2017-12-05 23:53:12.460992: step 56310, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 64h:22m:54s remains)
INFO - root - 2017-12-05 23:53:21.119944: step 56320, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 65h:51m:25s remains)
INFO - root - 2017-12-05 23:53:29.688058: step 56330, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.806 sec/batch; 61h:49m:05s remains)
INFO - root - 2017-12-05 23:53:38.228214: step 56340, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 64h:37m:43s remains)
INFO - root - 2017-12-05 23:53:46.704398: step 56350, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 64h:22m:14s remains)
INFO - root - 2017-12-05 23:53:55.315755: step 56360, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.860 sec/batch; 65h:59m:46s remains)
INFO - root - 2017-12-05 23:54:03.640910: step 56370, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 64h:09m:01s remains)
INFO - root - 2017-12-05 23:54:12.197610: step 56380, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 66h:37m:50s remains)
INFO - root - 2017-12-05 23:54:20.580869: step 56390, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 63h:15m:31s remains)
INFO - root - 2017-12-05 23:54:29.131567: step 56400, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 66h:02m:01s remains)
2017-12-05 23:54:29.942157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2681007 -4.2737856 -4.28635 -4.2967157 -4.2977138 -4.2906885 -4.2772694 -4.26202 -4.246326 -4.2321768 -4.2179003 -4.2054334 -4.2102218 -4.2418175 -4.2769513][-4.2895813 -4.2916441 -4.3000751 -4.3079762 -4.3076568 -4.3003216 -4.2856007 -4.2665172 -4.2485194 -4.2338638 -4.2205529 -4.2125216 -4.2230968 -4.2576818 -4.2919378][-4.3058639 -4.3030467 -4.3039207 -4.3059683 -4.3026333 -4.2956767 -4.2847457 -4.2687287 -4.2533541 -4.2424545 -4.2366648 -4.2373738 -4.2513537 -4.2844043 -4.3140903][-4.29397 -4.28785 -4.2821903 -4.2766252 -4.269383 -4.2645097 -4.2604179 -4.2514396 -4.2420425 -4.2385807 -4.2432995 -4.2537041 -4.2719936 -4.3047686 -4.3324094][-4.261302 -4.25054 -4.237577 -4.2256241 -4.216238 -4.2169003 -4.2233391 -4.2251172 -4.2234912 -4.2242517 -4.2330756 -4.2489147 -4.2715011 -4.3069434 -4.3358603][-4.2289534 -4.2126603 -4.1922679 -4.1712179 -4.1565943 -4.161232 -4.1784816 -4.1920819 -4.1971731 -4.1968007 -4.201879 -4.2199187 -4.2504435 -4.2925229 -4.3255224][-4.1979108 -4.1759853 -4.1477213 -4.1162395 -4.0937567 -4.097096 -4.1189413 -4.1390066 -4.1478024 -4.1461167 -4.149559 -4.1747379 -4.2193995 -4.2741423 -4.3147879][-4.1732388 -4.1524663 -4.1230931 -4.0849543 -4.0561795 -4.0541592 -4.0709405 -4.0876217 -4.0966425 -4.0971022 -4.1060152 -4.1412668 -4.19709 -4.2611094 -4.3078775][-4.171948 -4.1600957 -4.1354394 -4.0972352 -4.0638876 -4.0517926 -4.0560808 -4.0643315 -4.0742426 -4.0835967 -4.1027751 -4.1435318 -4.2012119 -4.2658086 -4.3121438][-4.1895046 -4.18818 -4.1709938 -4.1389174 -4.1080441 -4.090374 -4.0857573 -4.0903711 -4.1042142 -4.122838 -4.1502433 -4.1892138 -4.2368422 -4.289782 -4.3265986][-4.2053256 -4.213716 -4.2054734 -4.1858206 -4.1632977 -4.1450787 -4.1352491 -4.137979 -4.1518259 -4.1746888 -4.2045445 -4.2384505 -4.2740645 -4.312511 -4.3397441][-4.2014179 -4.215147 -4.216692 -4.209878 -4.1980515 -4.1853309 -4.1775589 -4.1830769 -4.2009487 -4.225605 -4.2538476 -4.2797947 -4.3031697 -4.32865 -4.3470535][-4.1862555 -4.2002363 -4.2110782 -4.2175012 -4.2185364 -4.2167234 -4.2159514 -4.2243285 -4.2438469 -4.2662172 -4.2896466 -4.3088031 -4.3238583 -4.3394051 -4.3499651][-4.17467 -4.1854014 -4.20094 -4.2165008 -4.2284217 -4.2365255 -4.2432175 -4.2543006 -4.271471 -4.2878747 -4.3072171 -4.3245735 -4.3370042 -4.3472238 -4.3524785][-4.1775885 -4.1860595 -4.199842 -4.219902 -4.2370424 -4.2478914 -4.2564387 -4.2651911 -4.2766037 -4.2873287 -4.3054161 -4.3250623 -4.3398733 -4.3495936 -4.3526163]]...]
INFO - root - 2017-12-05 23:54:38.349040: step 56410, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 67h:11m:03s remains)
INFO - root - 2017-12-05 23:54:46.941275: step 56420, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.885 sec/batch; 67h:51m:45s remains)
INFO - root - 2017-12-05 23:54:55.453020: step 56430, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 65h:53m:36s remains)
INFO - root - 2017-12-05 23:55:03.934750: step 56440, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 66h:37m:04s remains)
INFO - root - 2017-12-05 23:55:12.555445: step 56450, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 65h:56m:15s remains)
INFO - root - 2017-12-05 23:55:21.162245: step 56460, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 65h:35m:37s remains)
INFO - root - 2017-12-05 23:55:29.563806: step 56470, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 66h:19m:04s remains)
INFO - root - 2017-12-05 23:55:38.228557: step 56480, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 65h:03m:36s remains)
INFO - root - 2017-12-05 23:55:46.647425: step 56490, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 66h:28m:37s remains)
INFO - root - 2017-12-05 23:55:55.191634: step 56500, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 64h:07m:37s remains)
2017-12-05 23:55:55.979064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2522397 -4.2563291 -4.26099 -4.2656131 -4.2692428 -4.2723694 -4.276957 -4.2839642 -4.2895412 -4.2899246 -4.2858882 -4.2806411 -4.274405 -4.2688127 -4.2651796][-4.2376823 -4.2407994 -4.2459126 -4.2504187 -4.2539368 -4.2564826 -4.2602043 -4.2671313 -4.2747278 -4.27711 -4.2747564 -4.2703209 -4.2637677 -4.2573528 -4.2525373][-4.2342157 -4.2350211 -4.2375317 -4.23926 -4.2414279 -4.2424812 -4.2441974 -4.2501612 -4.2589993 -4.2635093 -4.2635226 -4.2609911 -4.2553711 -4.2497239 -4.2449727][-4.23633 -4.233212 -4.230968 -4.2266641 -4.2235603 -4.21981 -4.2161946 -4.2185736 -4.2270575 -4.2349 -4.2408152 -4.2444525 -4.2441449 -4.2431388 -4.241528][-4.25129 -4.2459188 -4.2376833 -4.2249103 -4.2144718 -4.2061219 -4.1979094 -4.1970196 -4.2047172 -4.2153149 -4.2267284 -4.2378335 -4.2445273 -4.2485924 -4.2503395][-4.2691078 -4.2599993 -4.2433524 -4.2198582 -4.2004924 -4.1873689 -4.1777148 -4.1788449 -4.1926703 -4.2095828 -4.2250524 -4.2401824 -4.2516904 -4.2594228 -4.2636542][-4.27736 -4.2622509 -4.2357473 -4.2011032 -4.1719685 -4.1530013 -4.1430669 -4.1503859 -4.1760159 -4.2047896 -4.2277579 -4.2463646 -4.2605329 -4.2705183 -4.2763505][-4.2742944 -4.2551804 -4.2221222 -4.1783843 -4.1391625 -4.1122346 -4.1009398 -4.1162968 -4.1554441 -4.1971869 -4.2283392 -4.2503891 -4.265451 -4.2764854 -4.2836108][-4.2691793 -4.2516336 -4.2178574 -4.1709991 -4.1249528 -4.0897427 -4.0751991 -4.0938044 -4.1393385 -4.1863375 -4.2212877 -4.2467804 -4.265285 -4.27826 -4.286787][-4.2750707 -4.2644272 -4.2379966 -4.1979637 -4.1538925 -4.1138506 -4.0932417 -4.1042314 -4.1410732 -4.1804709 -4.211072 -4.2364907 -4.2574768 -4.2731156 -4.284667][-4.2837496 -4.2819915 -4.2662225 -4.2393241 -4.2064338 -4.1706038 -4.1457052 -4.1413097 -4.1565332 -4.1772571 -4.1980729 -4.22095 -4.2430649 -4.2616491 -4.2764249][-4.2799764 -4.28512 -4.2771382 -4.2602539 -4.2386889 -4.2120714 -4.188611 -4.1741729 -4.1717515 -4.1767592 -4.1883707 -4.2067695 -4.2275095 -4.246387 -4.2618752][-4.2511525 -4.2616959 -4.2597322 -4.2507291 -4.2378855 -4.2203588 -4.2029715 -4.1877007 -4.1778512 -4.1740084 -4.17814 -4.1899295 -4.2059479 -4.221673 -4.2346153][-4.2001247 -4.215672 -4.2205548 -4.2196274 -4.2149587 -4.2069459 -4.19844 -4.1888695 -4.1787987 -4.1711521 -4.16967 -4.17485 -4.184968 -4.1958075 -4.2040648][-4.1690941 -4.1861024 -4.1948643 -4.199172 -4.2007928 -4.2003193 -4.1990285 -4.1953487 -4.18868 -4.1817064 -4.1781826 -4.1794348 -4.184237 -4.1900239 -4.1937394]]...]
INFO - root - 2017-12-05 23:56:04.534346: step 56510, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 65h:08m:02s remains)
INFO - root - 2017-12-05 23:56:13.019038: step 56520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 65h:57m:35s remains)
INFO - root - 2017-12-05 23:56:21.515088: step 56530, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 63h:54m:54s remains)
INFO - root - 2017-12-05 23:56:29.964678: step 56540, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 66h:21m:09s remains)
INFO - root - 2017-12-05 23:56:38.494694: step 56550, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 64h:14m:36s remains)
INFO - root - 2017-12-05 23:56:46.973763: step 56560, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 63h:50m:10s remains)
INFO - root - 2017-12-05 23:56:55.469196: step 56570, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 65h:34m:12s remains)
INFO - root - 2017-12-05 23:57:04.011553: step 56580, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 64h:12m:22s remains)
INFO - root - 2017-12-05 23:57:12.581679: step 56590, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 68h:24m:59s remains)
INFO - root - 2017-12-05 23:57:21.059336: step 56600, loss = 2.10, batch loss = 2.04 (10.4 examples/sec; 0.773 sec/batch; 59h:13m:33s remains)
2017-12-05 23:57:21.885521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2578182 -4.2575436 -4.2496133 -4.2357664 -4.2282772 -4.2223573 -4.21062 -4.202199 -4.2097373 -4.2303061 -4.2481422 -4.2635131 -4.2728896 -4.2768421 -4.2752476][-4.2421551 -4.2452917 -4.2377472 -4.2228708 -4.2138281 -4.20064 -4.1770539 -4.1615934 -4.1752729 -4.2070642 -4.2339845 -4.2584481 -4.2722735 -4.2796822 -4.2814984][-4.2340226 -4.2397294 -4.2338734 -4.2180276 -4.2031889 -4.1751204 -4.1308317 -4.1018476 -4.1204648 -4.1658115 -4.2045512 -4.2435541 -4.2654285 -4.2779779 -4.2834492][-4.2351503 -4.2399235 -4.2338142 -4.2174125 -4.19553 -4.1489358 -4.0790305 -4.0266709 -4.0491376 -4.1139245 -4.166441 -4.2198496 -4.2494426 -4.2655697 -4.2744308][-4.2360282 -4.2380805 -4.2319422 -4.2153382 -4.18894 -4.1257415 -4.025187 -3.9383068 -3.9653804 -4.0578632 -4.1279359 -4.1958184 -4.2324271 -4.2530432 -4.266911][-4.237473 -4.2365084 -4.2299056 -4.2156162 -4.1856794 -4.1082382 -3.9709864 -3.8374991 -3.8711951 -4.0021291 -4.096417 -4.179244 -4.2219639 -4.2474079 -4.2663889][-4.242507 -4.2409306 -4.2331071 -4.2167654 -4.1779394 -4.0882688 -3.9152422 -3.7266502 -3.76295 -3.9405828 -4.0668907 -4.1668167 -4.2130766 -4.2445612 -4.2694254][-4.2529316 -4.2482929 -4.2338576 -4.2095442 -4.1639757 -4.0717564 -3.8824909 -3.6585236 -3.6911838 -3.9032884 -4.0552573 -4.165144 -4.2119875 -4.2453766 -4.2727394][-4.2637382 -4.2490234 -4.2270031 -4.2000527 -4.1604857 -4.0880256 -3.932405 -3.7450187 -3.7657175 -3.9506624 -4.0863638 -4.1806846 -4.2224584 -4.2519016 -4.2770729][-4.2686596 -4.2452564 -4.2212558 -4.1995091 -4.1723433 -4.1269 -4.0214205 -3.8954334 -3.9054184 -4.0346346 -4.135385 -4.20325 -4.2375164 -4.261024 -4.2825894][-4.265759 -4.2409716 -4.2196274 -4.2018509 -4.1825109 -4.1542916 -4.0878282 -4.0060544 -4.0118647 -4.1001563 -4.174747 -4.2229338 -4.2494035 -4.2691069 -4.2880564][-4.26089 -4.2377081 -4.2179389 -4.2006121 -4.1881595 -4.170289 -4.134058 -4.0881214 -4.0999017 -4.1641908 -4.2175646 -4.2484732 -4.2647514 -4.2804508 -4.2958255][-4.2554641 -4.2330265 -4.2148876 -4.1988888 -4.1953282 -4.1880031 -4.1713114 -4.1544671 -4.1739197 -4.2213926 -4.2541456 -4.2698021 -4.281045 -4.2947688 -4.3033452][-4.25774 -4.2365589 -4.2188115 -4.20591 -4.2079768 -4.210916 -4.2076421 -4.2071781 -4.2296638 -4.2614713 -4.2784553 -4.2866206 -4.2974339 -4.306601 -4.3085284][-4.268682 -4.2507229 -4.2363663 -4.2263904 -4.2277818 -4.2372541 -4.24358 -4.2502909 -4.2686405 -4.2862582 -4.2943864 -4.3008351 -4.3093224 -4.31229 -4.3090453]]...]
INFO - root - 2017-12-05 23:57:30.319261: step 56610, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 65h:27m:57s remains)
INFO - root - 2017-12-05 23:57:38.841626: step 56620, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 66h:18m:54s remains)
INFO - root - 2017-12-05 23:57:47.275762: step 56630, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 64h:52m:35s remains)
INFO - root - 2017-12-05 23:57:55.784042: step 56640, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 64h:22m:57s remains)
INFO - root - 2017-12-05 23:58:04.458375: step 56650, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 68h:14m:18s remains)
INFO - root - 2017-12-05 23:58:13.030730: step 56660, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 65h:03m:36s remains)
INFO - root - 2017-12-05 23:58:21.529762: step 56670, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 68h:32m:40s remains)
INFO - root - 2017-12-05 23:58:30.010010: step 56680, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.865 sec/batch; 66h:15m:36s remains)
INFO - root - 2017-12-05 23:58:38.522025: step 56690, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 65h:20m:32s remains)
INFO - root - 2017-12-05 23:58:47.040818: step 56700, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 66h:44m:11s remains)
2017-12-05 23:58:47.883591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2555861 -4.2176814 -4.1931868 -4.1847329 -4.1963286 -4.2018714 -4.1962681 -4.1953111 -4.1816592 -4.1770787 -4.1910319 -4.2038016 -4.2167149 -4.2392917 -4.2476845][-4.2442331 -4.2064791 -4.1863351 -4.1805182 -4.192235 -4.19501 -4.1882143 -4.1870112 -4.1777239 -4.1704807 -4.1868434 -4.2122359 -4.2435021 -4.2720942 -4.2770987][-4.2437367 -4.2138824 -4.1989532 -4.1931276 -4.2040935 -4.2069273 -4.1949463 -4.1896443 -4.1847334 -4.1852622 -4.2023153 -4.2295656 -4.2604971 -4.284698 -4.2854323][-4.2548995 -4.2310643 -4.2175226 -4.2102318 -4.2195177 -4.2246337 -4.2150416 -4.2102075 -4.2042484 -4.20235 -4.2130036 -4.2328973 -4.2576952 -4.2740145 -4.2746439][-4.2666593 -4.24537 -4.2323408 -4.2246051 -4.2318754 -4.2365084 -4.2262988 -4.2205453 -4.208756 -4.1968188 -4.1987247 -4.2148743 -4.2378273 -4.2560825 -4.2637267][-4.27304 -4.2499781 -4.234551 -4.2236972 -4.2281327 -4.23065 -4.2196174 -4.2109346 -4.1975026 -4.1800609 -4.1834288 -4.2075615 -4.2354093 -4.25795 -4.2724929][-4.2710013 -4.2436781 -4.2226849 -4.2064266 -4.2056837 -4.2059269 -4.1929078 -4.1786218 -4.1644626 -4.1483355 -4.16062 -4.19311 -4.225523 -4.2510343 -4.269949][-4.2639318 -4.2299919 -4.2024884 -4.1792617 -4.1745362 -4.1724024 -4.1563873 -4.1384625 -4.1339602 -4.1313353 -4.1515355 -4.187449 -4.2203856 -4.2470107 -4.2641087][-4.2516012 -4.2073874 -4.1664391 -4.13526 -4.1268182 -4.1242146 -4.1136136 -4.110764 -4.1336832 -4.1568189 -4.1858397 -4.2188354 -4.2461181 -4.2689581 -4.2841516][-4.2449946 -4.1926193 -4.1407175 -4.1031132 -4.0954823 -4.104126 -4.1141696 -4.1370215 -4.1794624 -4.2123532 -4.2405739 -4.2650471 -4.2815051 -4.2973943 -4.3084035][-4.2584782 -4.2140594 -4.1691794 -4.1388378 -4.1368346 -4.1549468 -4.1762152 -4.2046714 -4.2412038 -4.2642951 -4.2812676 -4.2938247 -4.2976742 -4.3056602 -4.3109083][-4.2793174 -4.2524791 -4.22617 -4.2084641 -4.2089267 -4.2281818 -4.2473946 -4.2638006 -4.2791433 -4.2831583 -4.2865391 -4.2880611 -4.2844839 -4.2892613 -4.2944694][-4.294817 -4.281343 -4.2672043 -4.2567158 -4.2584443 -4.2776208 -4.291502 -4.292038 -4.288177 -4.2793078 -4.2734241 -4.2679052 -4.264266 -4.2709503 -4.27743][-4.3011866 -4.2908359 -4.2784734 -4.2706246 -4.2799153 -4.3050661 -4.314816 -4.3040109 -4.2914972 -4.2761221 -4.2645278 -4.2545104 -4.2495174 -4.2544713 -4.2579594][-4.301475 -4.2908859 -4.2795591 -4.2726765 -4.2866616 -4.3122973 -4.3185844 -4.3033118 -4.2876472 -4.2679834 -4.2508039 -4.2380958 -4.2339444 -4.2374434 -4.2393351]]...]
INFO - root - 2017-12-05 23:58:56.212902: step 56710, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 64h:55m:21s remains)
INFO - root - 2017-12-05 23:59:04.662920: step 56720, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 65h:42m:50s remains)
INFO - root - 2017-12-05 23:59:13.117988: step 56730, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 64h:35m:31s remains)
INFO - root - 2017-12-05 23:59:21.536789: step 56740, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 65h:16m:33s remains)
INFO - root - 2017-12-05 23:59:30.039400: step 56750, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 64h:57m:14s remains)
INFO - root - 2017-12-05 23:59:38.577405: step 56760, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 64h:05m:42s remains)
INFO - root - 2017-12-05 23:59:47.027801: step 56770, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 66h:23m:12s remains)
INFO - root - 2017-12-05 23:59:55.566292: step 56780, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 65h:07m:54s remains)
INFO - root - 2017-12-06 00:00:04.035093: step 56790, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 65h:11m:43s remains)
INFO - root - 2017-12-06 00:00:12.770601: step 56800, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 63h:50m:19s remains)
2017-12-06 00:00:13.518207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1333261 -4.134902 -4.1476746 -4.1618729 -4.1802516 -4.1993437 -4.2179809 -4.2353568 -4.24248 -4.2402048 -4.2352057 -4.2364087 -4.23063 -4.2193456 -4.21341][-4.1725373 -4.1649513 -4.1658468 -4.1727161 -4.1884465 -4.2108197 -4.2319188 -4.2499595 -4.2581329 -4.253932 -4.2448564 -4.2407703 -4.2262616 -4.2064776 -4.1957889][-4.2054157 -4.1861343 -4.1734834 -4.1700215 -4.1792631 -4.1969485 -4.2180185 -4.2374439 -4.2514067 -4.253027 -4.2443128 -4.2383604 -4.2181993 -4.1863308 -4.1597729][-4.2255015 -4.1932936 -4.1648769 -4.1438618 -4.1389785 -4.1466346 -4.167325 -4.1925993 -4.2151251 -4.2296724 -4.2353358 -4.2356296 -4.2147245 -4.1736374 -4.1294112][-4.233758 -4.1958013 -4.1545444 -4.1146054 -4.0886741 -4.0813012 -4.09726 -4.1296978 -4.1577921 -4.1858592 -4.2152853 -4.23263 -4.2176657 -4.1738324 -4.1199675][-4.2395453 -4.19765 -4.1475034 -4.0953922 -4.0530787 -4.0289869 -4.026247 -4.0422907 -4.0541182 -4.0949492 -4.158154 -4.2094264 -4.2198877 -4.190383 -4.1430526][-4.2427673 -4.2014856 -4.150013 -4.0958881 -4.0466275 -4.0039229 -3.9699128 -3.9434714 -3.9158731 -3.9594698 -4.0620327 -4.1552238 -4.205337 -4.2111149 -4.1886868][-4.2479558 -4.2139177 -4.1744857 -4.1327562 -4.0926085 -4.0490923 -3.9927614 -3.9210885 -3.8412571 -3.858973 -3.9759297 -4.0948653 -4.1760883 -4.2142944 -4.2206178][-4.2515044 -4.2257986 -4.20242 -4.1794791 -4.1595125 -4.130713 -4.079133 -4.0085473 -3.9281754 -3.9157112 -3.9879308 -4.0813022 -4.1569643 -4.2068191 -4.2312808][-4.2503223 -4.2317495 -4.2203655 -4.21326 -4.2091842 -4.1968856 -4.1630993 -4.1142874 -4.0608468 -4.04523 -4.0769539 -4.1308784 -4.1811204 -4.21783 -4.2419906][-4.2453408 -4.2320056 -4.2274261 -4.2296543 -4.2356558 -4.2391191 -4.2257447 -4.200398 -4.17265 -4.161561 -4.1720943 -4.2001414 -4.2305245 -4.2530704 -4.2676821][-4.2430382 -4.2331705 -4.2308769 -4.2353139 -4.2486768 -4.2637205 -4.2658219 -4.2615538 -4.2518072 -4.2457428 -4.2484889 -4.2640066 -4.2803807 -4.2907619 -4.2960706][-4.2424712 -4.2362108 -4.2347569 -4.2402477 -4.2577386 -4.2783585 -4.2880278 -4.2973437 -4.3026223 -4.303216 -4.301661 -4.3074975 -4.31322 -4.3117456 -4.3092537][-4.2427769 -4.2386775 -4.2375154 -4.2434621 -4.2622452 -4.284234 -4.296217 -4.3079572 -4.3185844 -4.3244305 -4.3210521 -4.3198247 -4.3175755 -4.3110275 -4.3060427][-4.2450633 -4.2412219 -4.2397032 -4.2447591 -4.2629905 -4.2839484 -4.2951584 -4.3052669 -4.3143606 -4.319417 -4.3137393 -4.3094797 -4.3035221 -4.2963939 -4.2924366]]...]
INFO - root - 2017-12-06 00:00:22.012833: step 56810, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 66h:20m:01s remains)
INFO - root - 2017-12-06 00:00:30.476763: step 56820, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 65h:58m:54s remains)
INFO - root - 2017-12-06 00:00:39.056355: step 56830, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 65h:58m:47s remains)
INFO - root - 2017-12-06 00:00:47.548942: step 56840, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 64h:51m:38s remains)
INFO - root - 2017-12-06 00:00:55.989496: step 56850, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 65h:03m:48s remains)
INFO - root - 2017-12-06 00:01:04.475815: step 56860, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 67h:02m:13s remains)
INFO - root - 2017-12-06 00:01:12.832420: step 56870, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 66h:44m:42s remains)
INFO - root - 2017-12-06 00:01:21.384989: step 56880, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 65h:24m:38s remains)
INFO - root - 2017-12-06 00:01:29.875986: step 56890, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 67h:27m:09s remains)
INFO - root - 2017-12-06 00:01:38.319643: step 56900, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.881 sec/batch; 67h:27m:07s remains)
2017-12-06 00:01:39.080879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.092474 -4.1010356 -4.1196079 -4.1469893 -4.1806726 -4.2152815 -4.2465868 -4.2682757 -4.2893877 -4.3132529 -4.3331771 -4.3414874 -4.3302174 -4.3001785 -4.2536][-4.1250768 -4.1024814 -4.0993962 -4.1093454 -4.1270342 -4.1564918 -4.1954517 -4.2306185 -4.2655115 -4.3013234 -4.33099 -4.3473659 -4.3446307 -4.3211703 -4.2797794][-4.1865878 -4.1267767 -4.0954576 -4.0877595 -4.0904889 -4.104897 -4.1389661 -4.183548 -4.235487 -4.2867017 -4.3286657 -4.3549304 -4.3606982 -4.3455434 -4.3140593][-4.2579651 -4.1853271 -4.1200948 -4.0853677 -4.0703745 -4.0724669 -4.094842 -4.1370492 -4.2009878 -4.2679596 -4.3231058 -4.3588653 -4.3728504 -4.3653431 -4.343081][-4.3070531 -4.2544928 -4.1798167 -4.1098919 -4.0613046 -4.0430093 -4.0520725 -4.0870581 -4.1605425 -4.2432966 -4.3094893 -4.3538613 -4.3770952 -4.379272 -4.3639932][-4.3249984 -4.2931528 -4.2336907 -4.1549287 -4.069304 -4.0053134 -3.98024 -4.0025864 -4.0928698 -4.2025013 -4.2822967 -4.33231 -4.3632689 -4.3766661 -4.3715272][-4.3175249 -4.2963991 -4.2500787 -4.1809092 -4.0828905 -3.9626763 -3.8653893 -3.8532486 -3.9654326 -4.1145082 -4.2185564 -4.2797918 -4.3190765 -4.3452787 -4.3568254][-4.2971172 -4.2823625 -4.2466812 -4.1921196 -4.10171 -3.9564469 -3.787765 -3.698606 -3.8060579 -3.9855196 -4.116971 -4.19226 -4.2370105 -4.2750363 -4.3061562][-4.2847238 -4.2802558 -4.2593961 -4.2230763 -4.1557035 -4.0306234 -3.8652763 -3.7398856 -3.7854934 -3.9294872 -4.0494752 -4.1195717 -4.1568661 -4.1900225 -4.2292485][-4.2816472 -4.2917285 -4.2863722 -4.2693777 -4.22821 -4.1390371 -4.0126462 -3.90381 -3.9026213 -3.979337 -4.0498462 -4.0911865 -4.1064215 -4.1196852 -4.1474476][-4.2719483 -4.297267 -4.3085647 -4.3089767 -4.288784 -4.2275915 -4.1335812 -4.0461011 -4.0228763 -4.0512409 -4.0803919 -4.0893841 -4.080049 -4.0709038 -4.0768046][-4.2505021 -4.2904129 -4.3164668 -4.3310575 -4.3274345 -4.2898507 -4.2223072 -4.1503258 -4.1143937 -4.1122479 -4.116838 -4.1063309 -4.0755563 -4.0450525 -4.0317183][-4.214766 -4.2676334 -4.3093042 -4.3350258 -4.3458467 -4.3300242 -4.287673 -4.2313743 -4.1870332 -4.1665506 -4.1577144 -4.1428132 -4.103745 -4.051446 -4.01261][-4.1713305 -4.2336912 -4.2918983 -4.3296947 -4.3487921 -4.3490353 -4.3285494 -4.2907248 -4.2487597 -4.2171431 -4.1990576 -4.1836271 -4.1537471 -4.0996933 -4.0367055][-4.1351776 -4.19808 -4.2658563 -4.3147078 -4.3410797 -4.3519654 -4.3481197 -4.3298087 -4.29928 -4.2645841 -4.2388391 -4.2202 -4.2001109 -4.1660042 -4.1074982]]...]
INFO - root - 2017-12-06 00:01:47.518159: step 56910, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 0.810 sec/batch; 62h:01m:13s remains)
INFO - root - 2017-12-06 00:01:55.905485: step 56920, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.819 sec/batch; 62h:39m:59s remains)
INFO - root - 2017-12-06 00:02:04.376604: step 56930, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 64h:33m:32s remains)
INFO - root - 2017-12-06 00:02:12.819912: step 56940, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 63h:10m:08s remains)
INFO - root - 2017-12-06 00:02:21.329481: step 56950, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 66h:18m:46s remains)
INFO - root - 2017-12-06 00:02:29.773501: step 56960, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 66h:52m:24s remains)
INFO - root - 2017-12-06 00:02:38.006055: step 56970, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 64h:14m:55s remains)
INFO - root - 2017-12-06 00:02:46.590697: step 56980, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 64h:43m:09s remains)
INFO - root - 2017-12-06 00:02:55.150028: step 56990, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 67h:14m:55s remains)
INFO - root - 2017-12-06 00:03:03.642440: step 57000, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 65h:45m:42s remains)
2017-12-06 00:03:04.402720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2435822 -4.2194734 -4.1900678 -4.1742282 -4.189827 -4.2171669 -4.2369194 -4.2342067 -4.2216477 -4.2071075 -4.1977558 -4.1893568 -4.1920109 -4.2038174 -4.2215886][-4.2169719 -4.1847119 -4.1470828 -4.1273956 -4.1498308 -4.1861386 -4.2144556 -4.2135324 -4.2004185 -4.1796632 -4.161952 -4.1452794 -4.1410131 -4.1541843 -4.1804972][-4.2005434 -4.162816 -4.116868 -4.0939083 -4.1211491 -4.1647511 -4.2012744 -4.2052145 -4.1986232 -4.1790371 -4.1572666 -4.13414 -4.1216016 -4.1299462 -4.1556339][-4.1884217 -4.1466384 -4.0926085 -4.064826 -4.0949841 -4.1436872 -4.1843486 -4.1879272 -4.1799192 -4.1637168 -4.1463242 -4.1245184 -4.1072655 -4.1134644 -4.1373577][-4.1797223 -4.1324205 -4.0699353 -4.0334644 -4.0582089 -4.1089096 -4.1520061 -4.1544185 -4.1415296 -4.1263213 -4.1135969 -4.0945249 -4.0803537 -4.0894594 -4.1123319][-4.1746063 -4.117538 -4.0408144 -3.9853671 -3.9984074 -4.0458989 -4.0924525 -4.0963669 -4.0902414 -4.0929112 -4.0938005 -4.082118 -4.0745521 -4.0838466 -4.1025243][-4.1673665 -4.0972562 -4.0067077 -3.9310966 -3.9231331 -3.9510951 -3.9859173 -3.9866309 -3.9894168 -4.0224957 -4.060133 -4.0772529 -4.084744 -4.0979881 -4.1159611][-4.1829338 -4.1080933 -4.0159569 -3.9353211 -3.912674 -3.9136279 -3.9225802 -3.9119289 -3.9135897 -3.9624984 -4.0282421 -4.0749521 -4.099575 -4.122323 -4.1415205][-4.2121282 -4.1482272 -4.0705161 -4.00278 -3.97877 -3.9658904 -3.9609447 -3.9485841 -3.949719 -3.9910195 -4.0542951 -4.1062474 -4.133779 -4.1559415 -4.1727586][-4.2430615 -4.1999526 -4.1470428 -4.0985284 -4.0775375 -4.060894 -4.054234 -4.0466294 -4.0509858 -4.0769653 -4.1218243 -4.1616964 -4.1830511 -4.2005734 -4.2146177][-4.2761626 -4.2502079 -4.2159805 -4.1819296 -4.1660581 -4.1569781 -4.156157 -4.1589789 -4.1702428 -4.1881227 -4.2174988 -4.2422824 -4.2531834 -4.2593713 -4.2653608][-4.2937112 -4.2745147 -4.2502637 -4.2306128 -4.2259965 -4.2281065 -4.234921 -4.24167 -4.2527442 -4.2638054 -4.2815347 -4.29493 -4.3002691 -4.3014235 -4.3022828][-4.3009949 -4.2840514 -4.2627416 -4.2493968 -4.2522254 -4.2632947 -4.27594 -4.2860165 -4.2955232 -4.3020387 -4.3097577 -4.3134236 -4.3125896 -4.3118706 -4.3124537][-4.3065233 -4.2910962 -4.2736073 -4.2638836 -4.2679367 -4.2779603 -4.2872796 -4.2955441 -4.3038521 -4.3102126 -4.31269 -4.3106737 -4.3074822 -4.30703 -4.3096852][-4.3155556 -4.3052154 -4.2928562 -4.2857037 -4.287818 -4.2943034 -4.2998419 -4.3058529 -4.3116322 -4.3154497 -4.315136 -4.3109107 -4.3070736 -4.3068423 -4.3095794]]...]
INFO - root - 2017-12-06 00:03:13.072116: step 57010, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 66h:41m:49s remains)
INFO - root - 2017-12-06 00:03:21.544189: step 57020, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 65h:27m:41s remains)
INFO - root - 2017-12-06 00:03:29.926264: step 57030, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.852 sec/batch; 65h:13m:55s remains)
INFO - root - 2017-12-06 00:03:38.395319: step 57040, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 64h:45m:54s remains)
INFO - root - 2017-12-06 00:03:46.793207: step 57050, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 63h:49m:41s remains)
INFO - root - 2017-12-06 00:03:55.306936: step 57060, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.815 sec/batch; 62h:20m:46s remains)
INFO - root - 2017-12-06 00:04:03.581405: step 57070, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 65h:10m:33s remains)
INFO - root - 2017-12-06 00:04:12.088519: step 57080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 65h:11m:50s remains)
INFO - root - 2017-12-06 00:04:20.695458: step 57090, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 67h:02m:52s remains)
INFO - root - 2017-12-06 00:04:29.262323: step 57100, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 65h:14m:15s remains)
2017-12-06 00:04:30.030415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0967159 -4.1032457 -4.1042213 -4.1002393 -4.1020308 -4.1007223 -4.0762434 -4.0433393 -4.0456805 -4.0846963 -4.131187 -4.1600761 -4.1868315 -4.2173 -4.247015][-4.0854435 -4.0968871 -4.0996995 -4.0977035 -4.0995421 -4.0955286 -4.0584841 -4.0151377 -4.019455 -4.0646434 -4.1169114 -4.1501393 -4.1823359 -4.21381 -4.2423391][-4.08896 -4.1038742 -4.1044192 -4.0940123 -4.0927243 -4.0832081 -4.0340919 -3.9766765 -3.9799931 -4.0316458 -4.0936418 -4.1379442 -4.1777511 -4.2114582 -4.2383208][-4.0928941 -4.1089091 -4.1123238 -4.1010189 -4.0906391 -4.0647016 -4.0043082 -3.9458632 -3.9583747 -4.0227275 -4.0898581 -4.1391296 -4.1777911 -4.2128654 -4.2387657][-4.1058593 -4.1209431 -4.1220994 -4.1110115 -4.0904779 -4.0449266 -3.9747336 -3.9222026 -3.9494064 -4.0242815 -4.0937834 -4.1492715 -4.1873178 -4.2182331 -4.2404556][-4.1239052 -4.1349621 -4.1305418 -4.1160555 -4.0859222 -4.0302224 -3.9554586 -3.9028876 -3.9463954 -4.0292025 -4.1034164 -4.1616287 -4.1994414 -4.2272716 -4.242898][-4.12199 -4.1328692 -4.12247 -4.09792 -4.0495858 -3.9802163 -3.8853943 -3.8219147 -3.9012792 -4.0165181 -4.1028361 -4.16738 -4.20766 -4.2332649 -4.2457108][-4.0889044 -4.1001353 -4.0912876 -4.0649443 -4.0018468 -3.9013028 -3.7430477 -3.6344895 -3.7803895 -3.9576328 -4.0773373 -4.1610346 -4.2065458 -4.2321191 -4.246027][-4.0610938 -4.071084 -4.0640583 -4.0453529 -4.0006957 -3.9051011 -3.7346032 -3.6175163 -3.7688403 -3.955744 -4.0832081 -4.170217 -4.2100081 -4.232574 -4.24721][-4.0609703 -4.0687394 -4.0561633 -4.0422611 -4.0273857 -3.9786539 -3.8790991 -3.8183768 -3.9133313 -4.0387955 -4.1316981 -4.1946597 -4.221734 -4.2383227 -4.2507691][-4.0520887 -4.061595 -4.049859 -4.0435877 -4.0464716 -4.0191979 -3.9588075 -3.9304438 -3.9986722 -4.0896039 -4.1550188 -4.1976457 -4.2201996 -4.2356892 -4.2476][-4.0264292 -4.0395451 -4.0383725 -4.0429678 -4.05625 -4.03173 -3.9764707 -3.9575183 -4.0100665 -4.0885425 -4.1434622 -4.1803393 -4.2076273 -4.2282119 -4.2443018][-4.0013161 -4.0211892 -4.0300431 -4.0452123 -4.0635777 -4.0421033 -3.9952474 -3.9811864 -4.0222445 -4.0832443 -4.1290355 -4.1674232 -4.2001753 -4.2251883 -4.2444782][-4.0232897 -4.0454679 -4.0558438 -4.0715375 -4.0834069 -4.0670924 -4.0266151 -4.013175 -4.0409966 -4.0868053 -4.1287088 -4.1694217 -4.20389 -4.2275562 -4.2454424][-4.0559325 -4.0780373 -4.0856037 -4.0975289 -4.1063585 -4.094501 -4.0603113 -4.0430341 -4.0634723 -4.0998859 -4.1400166 -4.1791062 -4.20981 -4.2316446 -4.2465839]]...]
INFO - root - 2017-12-06 00:04:38.470912: step 57110, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 62h:45m:34s remains)
INFO - root - 2017-12-06 00:04:46.977699: step 57120, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.815 sec/batch; 62h:21m:42s remains)
INFO - root - 2017-12-06 00:04:55.453176: step 57130, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 67h:55m:06s remains)
INFO - root - 2017-12-06 00:05:03.892188: step 57140, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 64h:55m:53s remains)
INFO - root - 2017-12-06 00:05:12.275360: step 57150, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 64h:48m:39s remains)
INFO - root - 2017-12-06 00:05:20.812843: step 57160, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 65h:38m:42s remains)
INFO - root - 2017-12-06 00:05:29.294545: step 57170, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 64h:18m:12s remains)
INFO - root - 2017-12-06 00:05:37.664109: step 57180, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 65h:37m:10s remains)
INFO - root - 2017-12-06 00:05:46.258322: step 57190, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 65h:22m:12s remains)
INFO - root - 2017-12-06 00:05:54.728944: step 57200, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.865 sec/batch; 66h:08m:34s remains)
2017-12-06 00:05:55.537591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3047066 -4.3126707 -4.3156829 -4.3136606 -4.3118668 -4.3146081 -4.3210731 -4.3288193 -4.3345995 -4.3381624 -4.3379688 -4.3382554 -4.3413954 -4.3468604 -4.352294][-4.28101 -4.2897305 -4.2934508 -4.2914047 -4.2899456 -4.2959094 -4.3065128 -4.3151464 -4.3193145 -4.3220358 -4.3228049 -4.3244162 -4.3300781 -4.3398433 -4.3518953][-4.2521782 -4.2605648 -4.2673907 -4.265728 -4.261879 -4.2724895 -4.2863436 -4.2932487 -4.2941918 -4.2963195 -4.3010163 -4.3061914 -4.3150554 -4.3286619 -4.347415][-4.2216535 -4.23037 -4.2384372 -4.2330933 -4.2233524 -4.2392211 -4.2562356 -4.261466 -4.25875 -4.2622266 -4.2715092 -4.283905 -4.2978034 -4.3173227 -4.3427763][-4.192441 -4.2063274 -4.2147121 -4.2015548 -4.1852546 -4.2015357 -4.217135 -4.2181311 -4.2136669 -4.2209725 -4.2378354 -4.2618594 -4.2843351 -4.3104277 -4.3410258][-4.169241 -4.1847129 -4.1902261 -4.16975 -4.1462455 -4.1582289 -4.1695275 -4.1644764 -4.1596904 -4.177166 -4.2078156 -4.245976 -4.2764287 -4.3069096 -4.3422446][-4.1465321 -4.158989 -4.1606274 -4.1362205 -4.1068487 -4.1090784 -4.1133442 -4.1073604 -4.1129518 -4.1479168 -4.1939 -4.2390904 -4.2753358 -4.3084335 -4.3444633][-4.13098 -4.1424732 -4.1458769 -4.1225777 -4.0884843 -4.0758162 -4.0746021 -4.0738382 -4.0929313 -4.1386051 -4.1890616 -4.2357707 -4.2752337 -4.3080978 -4.3407011][-4.1353812 -4.1458659 -4.1515765 -4.1335864 -4.096715 -4.0725689 -4.0672832 -4.0717082 -4.09091 -4.1295729 -4.1742983 -4.2220044 -4.2644444 -4.2980947 -4.3284507][-4.177526 -4.1833606 -4.1833138 -4.1658525 -4.1306739 -4.102571 -4.0927978 -4.0948548 -4.102602 -4.1236773 -4.155798 -4.2024469 -4.2478371 -4.2843642 -4.3130541][-4.2169967 -4.2167387 -4.2126966 -4.1934013 -4.1622667 -4.1366053 -4.1262712 -4.1274419 -4.1272883 -4.1333847 -4.1537447 -4.1944752 -4.2392807 -4.2749209 -4.3029275][-4.2260451 -4.2175903 -4.2126322 -4.1969304 -4.1728172 -4.1580467 -4.1548128 -4.1617184 -4.1642675 -4.1637383 -4.1748276 -4.2048168 -4.2427826 -4.2716808 -4.2970815][-4.2228847 -4.198225 -4.1869779 -4.1801577 -4.1715856 -4.1736922 -4.1823936 -4.1992459 -4.2086091 -4.2060695 -4.207623 -4.2236567 -4.2504225 -4.269917 -4.2912674][-4.2252297 -4.1843419 -4.1627541 -4.159781 -4.1653233 -4.1857076 -4.2077165 -4.2316141 -4.2445288 -4.2370434 -4.2286234 -4.2327576 -4.250452 -4.2632346 -4.282084][-4.2422905 -4.194468 -4.15964 -4.1518316 -4.1602082 -4.1906595 -4.2226887 -4.2508588 -4.2616324 -4.2484903 -4.2351441 -4.2332339 -4.2444334 -4.2531877 -4.271668]]...]
INFO - root - 2017-12-06 00:06:03.983630: step 57210, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 65h:05m:22s remains)
INFO - root - 2017-12-06 00:06:12.549179: step 57220, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 64h:08m:01s remains)
INFO - root - 2017-12-06 00:06:20.986297: step 57230, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 63h:43m:09s remains)
INFO - root - 2017-12-06 00:06:29.358119: step 57240, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 0.775 sec/batch; 59h:17m:13s remains)
INFO - root - 2017-12-06 00:06:37.885051: step 57250, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 64h:47m:15s remains)
INFO - root - 2017-12-06 00:06:46.520215: step 57260, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 62h:26m:28s remains)
INFO - root - 2017-12-06 00:06:54.884022: step 57270, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 63h:55m:44s remains)
INFO - root - 2017-12-06 00:07:03.502418: step 57280, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 66h:08m:34s remains)
INFO - root - 2017-12-06 00:07:11.901536: step 57290, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 66h:05m:08s remains)
INFO - root - 2017-12-06 00:07:20.348071: step 57300, loss = 2.03, batch loss = 1.98 (9.9 examples/sec; 0.809 sec/batch; 61h:52m:16s remains)
2017-12-06 00:07:21.116129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2402358 -4.2389035 -4.2425656 -4.238359 -4.23089 -4.2291427 -4.2374954 -4.2431355 -4.2474031 -4.2453675 -4.2375369 -4.2315249 -4.232944 -4.2387052 -4.250484][-4.2216959 -4.2225432 -4.2249184 -4.2159157 -4.2022462 -4.1969991 -4.20405 -4.2065468 -4.2143397 -4.2173586 -4.210331 -4.1994228 -4.1965342 -4.2030749 -4.2185984][-4.2000589 -4.2034383 -4.1986775 -4.1790738 -4.1547713 -4.1453018 -4.1552415 -4.1636739 -4.1768064 -4.1867476 -4.1829314 -4.1659818 -4.1572719 -4.1617727 -4.1777949][-4.1643844 -4.1733284 -4.1620421 -4.1374016 -4.1053376 -4.093473 -4.1054688 -4.1230254 -4.1465349 -4.1574354 -4.1510906 -4.1309571 -4.1193223 -4.1190653 -4.1344976][-4.1386213 -4.1478634 -4.1310825 -4.1091456 -4.0762925 -4.0579419 -4.05983 -4.0790648 -4.1141138 -4.1269994 -4.1170321 -4.0932088 -4.0730181 -4.062439 -4.0791826][-4.1191521 -4.1221175 -4.0994725 -4.0778356 -4.0369744 -3.9891169 -3.9663877 -3.9968965 -4.0506244 -4.0809355 -4.0844274 -4.0687151 -4.0445018 -4.0287523 -4.0441933][-4.1131134 -4.1059151 -4.0722823 -4.0382814 -3.9792604 -3.8728671 -3.7906158 -3.8554339 -3.9563072 -4.0223613 -4.0463161 -4.043674 -4.0255008 -4.00829 -4.0230756][-4.12832 -4.1132908 -4.0676804 -4.0090661 -3.9360645 -3.8038163 -3.6888783 -3.7714994 -3.9027791 -3.9897082 -4.0266714 -4.0286579 -4.0096774 -3.98332 -3.9982471][-4.1494994 -4.1356874 -4.0931244 -4.0377092 -3.9911067 -3.9180739 -3.854259 -3.889343 -3.9692445 -4.0320983 -4.0611415 -4.0521641 -4.0212774 -3.9918098 -4.0110831][-4.1464276 -4.1439977 -4.1137815 -4.0732245 -4.0552087 -4.0367079 -4.012979 -4.0164256 -4.0456152 -4.08083 -4.103375 -4.0978522 -4.07568 -4.0633802 -4.0850263][-4.1394653 -4.1474447 -4.136004 -4.1106806 -4.1013751 -4.0973597 -4.091949 -4.0852966 -4.0882649 -4.1049895 -4.1191382 -4.1212296 -4.1180153 -4.1225171 -4.1469121][-4.1305051 -4.1489019 -4.1538434 -4.141149 -4.1321034 -4.1312218 -4.1376052 -4.1363206 -4.1243181 -4.122375 -4.1273651 -4.1227264 -4.1223469 -4.1337953 -4.1671224][-4.1457191 -4.1668997 -4.1771502 -4.1716824 -4.1641893 -4.1615577 -4.170537 -4.1714544 -4.1573577 -4.1488905 -4.1459212 -4.1389546 -4.1400528 -4.153892 -4.1892891][-4.1829762 -4.199954 -4.2112432 -4.2091351 -4.205102 -4.2047544 -4.2091947 -4.2068386 -4.1943984 -4.186296 -4.1812887 -4.17442 -4.1792789 -4.1943116 -4.2228913][-4.230804 -4.2409854 -4.250103 -4.2511268 -4.2530117 -4.251513 -4.2471461 -4.242269 -4.2372785 -4.2329226 -4.2283134 -4.2242727 -4.2309566 -4.2412391 -4.2562227]]...]
INFO - root - 2017-12-06 00:07:29.669500: step 57310, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 65h:00m:51s remains)
INFO - root - 2017-12-06 00:07:38.175200: step 57320, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 65h:34m:25s remains)
INFO - root - 2017-12-06 00:07:46.730167: step 57330, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 66h:36m:22s remains)
INFO - root - 2017-12-06 00:07:55.256199: step 57340, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 66h:37m:55s remains)
INFO - root - 2017-12-06 00:08:03.714831: step 57350, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.824 sec/batch; 62h:56m:56s remains)
INFO - root - 2017-12-06 00:08:12.326433: step 57360, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 63h:53m:54s remains)
INFO - root - 2017-12-06 00:08:20.767507: step 57370, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 63h:07m:49s remains)
INFO - root - 2017-12-06 00:08:29.348596: step 57380, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 65h:06m:27s remains)
INFO - root - 2017-12-06 00:08:37.852679: step 57390, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 67h:07m:31s remains)
INFO - root - 2017-12-06 00:08:46.266999: step 57400, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 64h:23m:03s remains)
2017-12-06 00:08:47.007985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3389525 -4.3377433 -4.3382978 -4.3396125 -4.3412852 -4.3431606 -4.3450475 -4.3469625 -4.3486862 -4.3482089 -4.3441725 -4.3356037 -4.318572 -4.2865772 -4.2431784][-4.3365922 -4.3371782 -4.3393879 -4.3411951 -4.3425355 -4.3432527 -4.3437624 -4.3441133 -4.345283 -4.3462658 -4.3463225 -4.3451591 -4.3403158 -4.3283653 -4.31129][-4.3397183 -4.3394523 -4.340229 -4.3395152 -4.337122 -4.3317571 -4.3260517 -4.3218966 -4.3223977 -4.3266296 -4.3331389 -4.3411179 -4.3468766 -4.3483162 -4.3458881][-4.3419504 -4.3392587 -4.3356 -4.328433 -4.3161592 -4.2964644 -4.2749705 -4.2616715 -4.263011 -4.2770634 -4.2989731 -4.3220882 -4.3399143 -4.350121 -4.3537774][-4.3393846 -4.3312798 -4.3175879 -4.2956877 -4.2610292 -4.2125692 -4.16418 -4.1408267 -4.1516914 -4.1885014 -4.2386208 -4.2863879 -4.3203259 -4.3392639 -4.3464971][-4.3315158 -4.3146076 -4.2860045 -4.2399015 -4.16883 -4.0801682 -4.0001092 -3.9724405 -4.005476 -4.0766811 -4.1640577 -4.24125 -4.2930593 -4.3220968 -4.3347931][-4.320796 -4.2929077 -4.2456245 -4.1684589 -4.0559187 -3.9293091 -3.8278151 -3.8116941 -3.8822656 -3.9928861 -4.1129384 -4.2105374 -4.2738748 -4.31017 -4.3267722][-4.3137527 -4.2788224 -4.2201319 -4.1259432 -3.9954915 -3.8623834 -3.7693763 -3.7775226 -3.8753734 -4.0004177 -4.1256289 -4.2212553 -4.2814236 -4.3151 -4.3292108][-4.3149319 -4.2819223 -4.2285953 -4.1466084 -4.0408449 -3.944768 -3.8891811 -3.9116642 -3.9998481 -4.1028781 -4.2018709 -4.27197 -4.3132749 -4.3341112 -4.3398762][-4.3226161 -4.2991037 -4.2631011 -4.2113953 -4.1490855 -4.1002154 -4.0802116 -4.1057677 -4.1682138 -4.2347431 -4.2945595 -4.3312726 -4.3480229 -4.3531523 -4.3499026][-4.3337893 -4.3223481 -4.3059664 -4.2841077 -4.2592769 -4.2448716 -4.245647 -4.2662191 -4.3023624 -4.3360486 -4.36143 -4.3704782 -4.3684731 -4.3622079 -4.3536162][-4.3421478 -4.3397522 -4.3368549 -4.3330779 -4.3283992 -4.3296547 -4.3366504 -4.349545 -4.3667884 -4.3785872 -4.3823681 -4.3776684 -4.3686438 -4.3592496 -4.350307][-4.3450065 -4.3460288 -4.3477206 -4.3499455 -4.3514462 -4.354856 -4.3600712 -4.36595 -4.3711472 -4.37203 -4.3686762 -4.3626356 -4.3558373 -4.3496823 -4.344429][-4.3432574 -4.3436728 -4.3445964 -4.3458629 -4.3466411 -4.3479896 -4.3496118 -4.3506575 -4.3508878 -4.349812 -4.3478293 -4.3460083 -4.3441324 -4.3428006 -4.3416386][-4.3403826 -4.3390903 -4.3382878 -4.3379025 -4.3375812 -4.3375573 -4.33802 -4.3388138 -4.3398018 -4.3403573 -4.3406796 -4.3409553 -4.3409863 -4.3413639 -4.3415556]]...]
INFO - root - 2017-12-06 00:08:55.770375: step 57410, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 64h:37m:21s remains)
INFO - root - 2017-12-06 00:09:04.222211: step 57420, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 65h:41m:33s remains)
INFO - root - 2017-12-06 00:09:12.743541: step 57430, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 65h:10m:13s remains)
INFO - root - 2017-12-06 00:09:21.235228: step 57440, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 64h:41m:40s remains)
INFO - root - 2017-12-06 00:09:29.678568: step 57450, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 64h:10m:02s remains)
INFO - root - 2017-12-06 00:09:38.281220: step 57460, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 67h:43m:16s remains)
INFO - root - 2017-12-06 00:09:46.868710: step 57470, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 65h:41m:17s remains)
INFO - root - 2017-12-06 00:09:55.337242: step 57480, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 62h:51m:56s remains)
INFO - root - 2017-12-06 00:10:03.702061: step 57490, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 62h:45m:30s remains)
INFO - root - 2017-12-06 00:10:12.192291: step 57500, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 64h:23m:31s remains)
2017-12-06 00:10:12.956627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.317102 -4.3135047 -4.3013983 -4.281075 -4.2477746 -4.1949205 -4.1348238 -4.1054811 -4.1147323 -4.1640224 -4.234488 -4.2878914 -4.3229175 -4.3452473 -4.3476863][-4.310626 -4.3072104 -4.29913 -4.2791615 -4.2439051 -4.1854215 -4.1234689 -4.0945315 -4.1046925 -4.15832 -4.2303996 -4.2820535 -4.31319 -4.3352652 -4.3411946][-4.3024278 -4.2979064 -4.2891035 -4.2671676 -4.22896 -4.1655469 -4.1072812 -4.0840955 -4.0999966 -4.1517277 -4.2182789 -4.2662463 -4.2962985 -4.3215022 -4.3326283][-4.2960629 -4.2855992 -4.269248 -4.2409863 -4.1958733 -4.12502 -4.0665541 -4.0464106 -4.0683818 -4.1228065 -4.1839247 -4.2326736 -4.2724118 -4.3052835 -4.3219409][-4.2874513 -4.2674394 -4.23815 -4.1998553 -4.1448936 -4.0685372 -4.009553 -3.98534 -4.0099435 -4.0717077 -4.1355524 -4.1921372 -4.2462063 -4.2873721 -4.3071613][-4.2706633 -4.2407889 -4.2001028 -4.1521611 -4.0887713 -4.01091 -3.9507735 -3.9184403 -3.9447203 -4.0139961 -4.0830092 -4.1507506 -4.2155719 -4.2600574 -4.2813797][-4.2536335 -4.2146721 -4.1639209 -4.1094732 -4.044229 -3.9701624 -3.9132736 -3.8805199 -3.90738 -3.9745255 -4.0433135 -4.1205916 -4.1851163 -4.2273622 -4.24554][-4.2493668 -4.2090583 -4.1601286 -4.108088 -4.0469384 -3.9823961 -3.9380186 -3.9141338 -3.9327548 -3.9830704 -4.0425062 -4.1113143 -4.161871 -4.1901336 -4.19641][-4.2649693 -4.2364798 -4.2010245 -4.1600208 -4.1135902 -4.0666761 -4.0413504 -4.0272627 -4.0284033 -4.0524621 -4.0937247 -4.1345568 -4.1534085 -4.1536784 -4.1366091][-4.2895465 -4.2807217 -4.264554 -4.2397108 -4.2120275 -4.1807947 -4.16493 -4.1515107 -4.1344213 -4.1324592 -4.1511316 -4.1611 -4.1512051 -4.1251974 -4.0883908][-4.3006582 -4.3081861 -4.30577 -4.2908778 -4.2725081 -4.2521911 -4.2389255 -4.2228718 -4.1960821 -4.18361 -4.1940117 -4.1902308 -4.1672578 -4.1221852 -4.0679584][-4.2777886 -4.2962823 -4.3031993 -4.2917871 -4.2773013 -4.2649608 -4.2573557 -4.2420864 -4.222055 -4.2147026 -4.2247629 -4.2183213 -4.1963725 -4.1468339 -4.0813751][-4.2400522 -4.262115 -4.2735362 -4.2577028 -4.2388716 -4.2285981 -4.2261829 -4.2170105 -4.2132525 -4.2262888 -4.2472377 -4.24932 -4.2309027 -4.1897259 -4.1310053][-4.1959691 -4.2268634 -4.2407923 -4.2190456 -4.1917896 -4.1761508 -4.16785 -4.1620927 -4.171978 -4.2021847 -4.2381945 -4.2584872 -4.251874 -4.2275891 -4.1871452][-4.1447215 -4.1783104 -4.1962657 -4.1786857 -4.1500416 -4.1305084 -4.1162815 -4.1128354 -4.1341486 -4.1725926 -4.2117848 -4.2377367 -4.2419991 -4.2327003 -4.2010393]]...]
INFO - root - 2017-12-06 00:10:21.404884: step 57510, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 67h:18m:30s remains)
INFO - root - 2017-12-06 00:10:29.789975: step 57520, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 65h:36m:23s remains)
INFO - root - 2017-12-06 00:10:38.320052: step 57530, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 65h:35m:23s remains)
INFO - root - 2017-12-06 00:10:46.784722: step 57540, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 63h:35m:56s remains)
INFO - root - 2017-12-06 00:10:55.318600: step 57550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 65h:33m:40s remains)
INFO - root - 2017-12-06 00:11:03.666349: step 57560, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.763 sec/batch; 58h:15m:33s remains)
INFO - root - 2017-12-06 00:11:12.027276: step 57570, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 63h:02m:37s remains)
INFO - root - 2017-12-06 00:11:20.432131: step 57580, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 65h:36m:13s remains)
INFO - root - 2017-12-06 00:11:29.049430: step 57590, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 66h:51m:06s remains)
INFO - root - 2017-12-06 00:11:37.435955: step 57600, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 65h:16m:05s remains)
2017-12-06 00:11:38.186444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2186375 -4.214705 -4.2082653 -4.184937 -4.1589828 -4.150342 -4.1434088 -4.1291971 -4.1260924 -4.142519 -4.1767497 -4.2093554 -4.2240505 -4.2130952 -4.1889305][-4.2443829 -4.2422361 -4.2287631 -4.197535 -4.1641879 -4.1461153 -4.1336541 -4.1140022 -4.1107955 -4.1328616 -4.1698875 -4.2004437 -4.2120571 -4.1976748 -4.1676655][-4.2620749 -4.2614045 -4.2413464 -4.2023997 -4.1628723 -4.1338353 -4.11185 -4.0878983 -4.0876808 -4.117795 -4.1573339 -4.1880956 -4.2005258 -4.1856503 -4.1532078][-4.2679777 -4.2645454 -4.2388921 -4.1951332 -4.1494923 -4.1121554 -4.0821095 -4.0534482 -4.0553551 -4.0931635 -4.1374121 -4.1736164 -4.1930337 -4.1812067 -4.15011][-4.260447 -4.252399 -4.2220664 -4.1770034 -4.1320467 -4.0925736 -4.058394 -4.0289578 -4.0318031 -4.0764155 -4.1242352 -4.1647387 -4.1909604 -4.1852918 -4.1599436][-4.2466097 -4.23501 -4.2041855 -4.1638675 -4.1227236 -4.0848885 -4.0503964 -4.0202065 -4.02604 -4.0781069 -4.1293907 -4.1704121 -4.1979489 -4.1954823 -4.1749535][-4.2357745 -4.223732 -4.1935019 -4.1573119 -4.1191344 -4.081459 -4.0447431 -4.0134611 -4.0261393 -4.0883312 -4.1448383 -4.1860685 -4.2116566 -4.2078495 -4.1886096][-4.2339525 -4.220408 -4.1904182 -4.1561995 -4.1190066 -4.0784292 -4.0333872 -3.9975166 -4.0186458 -4.0939307 -4.1576371 -4.2026834 -4.2263279 -4.2211323 -4.203033][-4.2354455 -4.2185063 -4.1866031 -4.154007 -4.11964 -4.0742054 -4.0171561 -3.9764032 -4.0073047 -4.0910134 -4.1592746 -4.2082782 -4.2336884 -4.2324972 -4.219152][-4.238986 -4.2182465 -4.1828213 -4.15135 -4.1192126 -4.06963 -4.00258 -3.9631076 -4.0054049 -4.0896573 -4.1542106 -4.2047615 -4.2331152 -4.237206 -4.2270575][-4.2447224 -4.2204094 -4.1805954 -4.1465254 -4.1142278 -4.0645862 -3.9982708 -3.9726408 -4.0265436 -4.102273 -4.1561179 -4.202354 -4.2310033 -4.2371883 -4.2268615][-4.249619 -4.2281651 -4.1890559 -4.1543741 -4.1233664 -4.0772028 -4.0208063 -4.012187 -4.064621 -4.1223912 -4.1597309 -4.1985378 -4.2269616 -4.2350149 -4.2236252][-4.2527528 -4.2407656 -4.210433 -4.1790209 -4.148788 -4.1041093 -4.0574908 -4.059567 -4.1023731 -4.1390009 -4.1577072 -4.1900668 -4.2204795 -4.2303152 -4.2204208][-4.2458715 -4.2451997 -4.2255197 -4.19832 -4.1673341 -4.1260581 -4.0922718 -4.1006045 -4.1294141 -4.1427832 -4.1439042 -4.1726265 -4.2079172 -4.2221513 -4.216548][-4.2270131 -4.2321806 -4.2203941 -4.1976647 -4.1690321 -4.1394124 -4.1245365 -4.1379957 -4.1521087 -4.146666 -4.1347313 -4.1590085 -4.1976337 -4.2167411 -4.2164817]]...]
INFO - root - 2017-12-06 00:11:46.596626: step 57610, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.796 sec/batch; 60h:48m:00s remains)
INFO - root - 2017-12-06 00:11:55.100488: step 57620, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 64h:13m:18s remains)
INFO - root - 2017-12-06 00:12:03.646416: step 57630, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 64h:51m:26s remains)
INFO - root - 2017-12-06 00:12:12.118547: step 57640, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 67h:00m:14s remains)
INFO - root - 2017-12-06 00:12:20.566885: step 57650, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 64h:40m:28s remains)
INFO - root - 2017-12-06 00:12:29.033684: step 57660, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 65h:29m:47s remains)
INFO - root - 2017-12-06 00:12:37.433584: step 57670, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 66h:36m:04s remains)
INFO - root - 2017-12-06 00:12:45.861883: step 57680, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 64h:40m:12s remains)
INFO - root - 2017-12-06 00:12:54.317246: step 57690, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.876 sec/batch; 66h:51m:02s remains)
INFO - root - 2017-12-06 00:13:02.848283: step 57700, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.813 sec/batch; 62h:02m:39s remains)
2017-12-06 00:13:03.609138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25721 -4.2422824 -4.2263222 -4.2019811 -4.164906 -4.1174192 -4.067163 -4.0274329 -4.0143909 -4.0318007 -4.060647 -4.0861936 -4.0994234 -4.096622 -4.0927739][-4.2485352 -4.2312961 -4.2115593 -4.1778917 -4.1281533 -4.0676737 -4.0047784 -3.95623 -3.9461033 -3.9749851 -4.0183635 -4.0594196 -4.0771866 -4.0687366 -4.0599475][-4.2340078 -4.2143145 -4.1901569 -4.1510038 -4.0938854 -4.0248237 -3.9556577 -3.9085631 -3.9130936 -3.9591177 -4.0155792 -4.0642514 -4.0767126 -4.0598345 -4.042943][-4.2099018 -4.1919713 -4.1718545 -4.1379428 -4.0821929 -4.0072517 -3.938463 -3.9018452 -3.9268951 -3.987612 -4.0464649 -4.0862575 -4.0882459 -4.063602 -4.0413327][-4.1827121 -4.1691704 -4.16036 -4.1391382 -4.0895214 -4.0152478 -3.9485512 -3.9212685 -3.9593215 -4.0234842 -4.0759206 -4.1027603 -4.0935011 -4.0590062 -4.0307121][-4.152966 -4.1431684 -4.1461587 -4.1383781 -4.0971146 -4.0300236 -3.9665384 -3.9436879 -3.990031 -4.0592241 -4.1061263 -4.1177316 -4.0940075 -4.0498419 -4.023356][-4.1337924 -4.1297421 -4.1415081 -4.14011 -4.10284 -4.035984 -3.9708273 -3.9513817 -4.0114508 -4.0931082 -4.136694 -4.1355219 -4.0980058 -4.049006 -4.0321846][-4.1252933 -4.135715 -4.1563535 -4.1565175 -4.1198978 -4.0483065 -3.9781618 -3.9613061 -4.0337563 -4.1246486 -4.162993 -4.1536026 -4.1126161 -4.0633783 -4.053288][-4.1332216 -4.1595755 -4.1889629 -4.1922255 -4.1565552 -4.0819173 -4.0098362 -4.0014157 -4.0787182 -4.1661129 -4.1977067 -4.1830187 -4.1386013 -4.0894723 -4.0785227][-4.1542211 -4.1898251 -4.2230573 -4.2305794 -4.2019281 -4.1338758 -4.0693312 -4.0674815 -4.1352663 -4.208487 -4.2310562 -4.2121449 -4.1677523 -4.1222959 -4.1087103][-4.1839867 -4.2176228 -4.248889 -4.2611756 -4.2433228 -4.1890264 -4.1367035 -4.1364613 -4.1885257 -4.24598 -4.2614813 -4.2439895 -4.2061186 -4.1670041 -4.1511445][-4.2251253 -4.248455 -4.27156 -4.28432 -4.2752228 -4.2347288 -4.1936712 -4.192935 -4.2314553 -4.2773204 -4.2907434 -4.2797704 -4.250567 -4.2172837 -4.2012262][-4.2617774 -4.2750554 -4.2918024 -4.303298 -4.2993088 -4.2707734 -4.240274 -4.2374516 -4.2654166 -4.3013449 -4.3143992 -4.3098073 -4.2890263 -4.2625813 -4.2487755][-4.2819171 -4.291883 -4.3061481 -4.3188491 -4.3191304 -4.2996373 -4.2748652 -4.2651105 -4.2808428 -4.3069978 -4.3203659 -4.3195868 -4.305655 -4.2865872 -4.2770576][-4.2828069 -4.2926345 -4.3064919 -4.3206167 -4.3256164 -4.313159 -4.2913041 -4.2727075 -4.2759748 -4.2923889 -4.305172 -4.3060083 -4.2972751 -4.2860126 -4.2816033]]...]
INFO - root - 2017-12-06 00:13:12.264567: step 57710, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 66h:00m:20s remains)
INFO - root - 2017-12-06 00:13:20.797397: step 57720, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.767 sec/batch; 58h:34m:42s remains)
INFO - root - 2017-12-06 00:13:29.315257: step 57730, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 66h:54m:09s remains)
INFO - root - 2017-12-06 00:13:37.768766: step 57740, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 65h:06m:51s remains)
INFO - root - 2017-12-06 00:13:46.437982: step 57750, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 65h:41m:50s remains)
INFO - root - 2017-12-06 00:13:54.922950: step 57760, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.865 sec/batch; 65h:58m:47s remains)
INFO - root - 2017-12-06 00:14:03.520279: step 57770, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 66h:12m:59s remains)
INFO - root - 2017-12-06 00:14:11.958423: step 57780, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.832 sec/batch; 63h:29m:28s remains)
INFO - root - 2017-12-06 00:14:20.463631: step 57790, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 65h:23m:12s remains)
INFO - root - 2017-12-06 00:14:29.037161: step 57800, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 64h:14m:02s remains)
2017-12-06 00:14:29.811443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1733255 -4.2074838 -4.2290096 -4.2458615 -4.2526121 -4.256659 -4.2487211 -4.2430887 -4.2376618 -4.2423167 -4.25302 -4.2683821 -4.2811651 -4.2867279 -4.2910643][-4.1496954 -4.1905551 -4.2226014 -4.2514086 -4.2649264 -4.2656221 -4.2499495 -4.2383919 -4.2325892 -4.238338 -4.25226 -4.2738218 -4.2874618 -4.29079 -4.2935424][-4.1348896 -4.1746693 -4.2085366 -4.2431784 -4.2584934 -4.2528787 -4.2291579 -4.2136893 -4.213387 -4.2236552 -4.241786 -4.2671566 -4.2845888 -4.2882676 -4.2910171][-4.1339083 -4.1632719 -4.1913538 -4.220675 -4.2321796 -4.2206097 -4.1871085 -4.1703644 -4.1787205 -4.1969266 -4.2201414 -4.2500057 -4.2750807 -4.2857175 -4.2910604][-4.1518712 -4.1750388 -4.1957793 -4.2148023 -4.2125282 -4.1878905 -4.1468964 -4.1340547 -4.1572046 -4.1868935 -4.2120647 -4.2398815 -4.2667146 -4.2831531 -4.2933087][-4.1571884 -4.1833982 -4.2055011 -4.2131925 -4.1958551 -4.1588044 -4.1184964 -4.112957 -4.1559072 -4.200551 -4.22717 -4.2476397 -4.266099 -4.2817335 -4.2953892][-4.172123 -4.2004409 -4.2244015 -4.2244248 -4.1956949 -4.1467972 -4.1063352 -4.1068335 -4.1587787 -4.21478 -4.2432404 -4.2588973 -4.2691755 -4.2798204 -4.2945862][-4.179533 -4.2071919 -4.23167 -4.2284141 -4.1965308 -4.1477985 -4.11239 -4.1182251 -4.1668773 -4.2245474 -4.25557 -4.2694325 -4.2727289 -4.277544 -4.2903576][-4.1779575 -4.2015734 -4.2269225 -4.2284842 -4.2075572 -4.1677265 -4.1342793 -4.1354427 -4.1752129 -4.2280574 -4.2588477 -4.2761264 -4.2775183 -4.2782512 -4.2890329][-4.1882915 -4.2061453 -4.2299128 -4.2374611 -4.228611 -4.194963 -4.1564116 -4.1479197 -4.1805406 -4.2287703 -4.2592807 -4.2813516 -4.28494 -4.2843595 -4.2928476][-4.2169566 -4.2277389 -4.2420273 -4.24915 -4.2471452 -4.2213287 -4.1813378 -4.1672454 -4.1951518 -4.2358427 -4.2649264 -4.28767 -4.2925897 -4.2937174 -4.3006554][-4.2579761 -4.2624764 -4.2723374 -4.2787781 -4.2793384 -4.2632618 -4.2275696 -4.2104669 -4.2286363 -4.2567844 -4.27852 -4.2965541 -4.301981 -4.3054891 -4.3107882][-4.2893276 -4.2918186 -4.3005137 -4.305553 -4.3066969 -4.297122 -4.2702289 -4.2532849 -4.2621808 -4.2795057 -4.2930536 -4.3050256 -4.3105874 -4.31498 -4.3183837][-4.3027759 -4.3030734 -4.3096223 -4.31365 -4.3155317 -4.3107052 -4.2930417 -4.27968 -4.2838578 -4.2944446 -4.3044291 -4.311923 -4.3160396 -4.3197017 -4.3208303][-4.3070269 -4.3070555 -4.3107777 -4.3134556 -4.3158369 -4.3136697 -4.3043437 -4.2970557 -4.298173 -4.3021221 -4.3078442 -4.3126163 -4.3150978 -4.3172569 -4.3177252]]...]
INFO - root - 2017-12-06 00:14:38.321606: step 57810, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 63h:47m:01s remains)
INFO - root - 2017-12-06 00:14:46.879436: step 57820, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 63h:50m:05s remains)
INFO - root - 2017-12-06 00:14:55.450150: step 57830, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 64h:16m:20s remains)
INFO - root - 2017-12-06 00:15:03.887093: step 57840, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 63h:54m:57s remains)
INFO - root - 2017-12-06 00:15:12.511962: step 57850, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 63h:53m:38s remains)
INFO - root - 2017-12-06 00:15:21.011566: step 57860, loss = 2.02, batch loss = 1.96 (9.6 examples/sec; 0.833 sec/batch; 63h:31m:17s remains)
INFO - root - 2017-12-06 00:15:29.230929: step 57870, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 64h:05m:49s remains)
INFO - root - 2017-12-06 00:15:37.538271: step 57880, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 64h:13m:55s remains)
INFO - root - 2017-12-06 00:15:46.014344: step 57890, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.856 sec/batch; 65h:15m:29s remains)
INFO - root - 2017-12-06 00:15:54.492026: step 57900, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 65h:03m:04s remains)
2017-12-06 00:15:55.300527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1839824 -4.190269 -4.19136 -4.2055435 -4.2181191 -4.2171164 -4.2086077 -4.1987357 -4.1976871 -4.2045302 -4.2219014 -4.2427797 -4.2549887 -4.2528863 -4.2369084][-4.1680741 -4.1804566 -4.1795945 -4.1847453 -4.1869597 -4.177104 -4.1646557 -4.1535048 -4.1568179 -4.1710329 -4.1966758 -4.2240815 -4.2429695 -4.2410364 -4.2198472][-4.1583147 -4.1733227 -4.1681218 -4.1597791 -4.1441813 -4.111578 -4.0759087 -4.0572948 -4.0751009 -4.1138449 -4.1601634 -4.1999507 -4.2251158 -4.2211447 -4.1951156][-4.157701 -4.1732755 -4.1644769 -4.1429152 -4.0993214 -4.030098 -3.9560022 -3.9292672 -3.9791803 -4.0608673 -4.1325345 -4.1838584 -4.2111225 -4.2004461 -4.1667862][-4.1657753 -4.1794848 -4.1648436 -4.1318593 -4.0622816 -3.9613063 -3.8565903 -3.8352199 -3.9239213 -4.0444441 -4.1349235 -4.1855679 -4.2036142 -4.18158 -4.1408339][-4.1671286 -4.176723 -4.1606827 -4.126091 -4.0466266 -3.9379735 -3.8311014 -3.8293049 -3.9342768 -4.0636477 -4.1534166 -4.1908593 -4.1920233 -4.161159 -4.1144505][-4.1645908 -4.1744227 -4.1622815 -4.1331964 -4.0589752 -3.9611988 -3.87564 -3.8891635 -3.9887114 -4.1054745 -4.1843371 -4.202795 -4.1838212 -4.14576 -4.0955081][-4.1536417 -4.1710835 -4.1696906 -4.14875 -4.0891867 -4.014164 -3.955847 -3.9739597 -4.0576506 -4.1496744 -4.2091718 -4.2074451 -4.1719565 -4.130332 -4.0842915][-4.1267524 -4.15274 -4.161767 -4.1470184 -4.1024289 -4.0551734 -4.0336165 -4.0597253 -4.1265545 -4.1908512 -4.226748 -4.20422 -4.1560359 -4.1135736 -4.0769234][-4.0990896 -4.1321445 -4.1519537 -4.142746 -4.1105194 -4.0865231 -4.0936265 -4.1259003 -4.1753445 -4.2144151 -4.2269812 -4.1935349 -4.1470981 -4.1136904 -4.0918794][-4.1053119 -4.135891 -4.15925 -4.1537704 -4.1277623 -4.118443 -4.1377554 -4.1692953 -4.2040334 -4.22647 -4.22366 -4.1923223 -4.1572 -4.1383343 -4.12836][-4.1297741 -4.1538076 -4.1711311 -4.1677561 -4.1482644 -4.1495752 -4.1735353 -4.1989937 -4.2209406 -4.2306581 -4.2220416 -4.2029071 -4.184866 -4.178534 -4.1749058][-4.1776142 -4.1931419 -4.1998339 -4.1948552 -4.1813517 -4.191174 -4.2159972 -4.2338338 -4.2428136 -4.2422338 -4.2313862 -4.2235165 -4.2174134 -4.2197633 -4.2208686][-4.2313151 -4.23846 -4.2353063 -4.2281575 -4.2207193 -4.2331653 -4.2566209 -4.2691388 -4.2698665 -4.2637548 -4.2541842 -4.2509179 -4.2480984 -4.2503939 -4.2521639][-4.2704015 -4.2734766 -4.2694483 -4.2631946 -4.2592697 -4.2711978 -4.2903275 -4.2991776 -4.2959409 -4.2891903 -4.2817354 -4.2809548 -4.2795954 -4.2791991 -4.2812414]]...]
INFO - root - 2017-12-06 00:16:03.685821: step 57910, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.825 sec/batch; 62h:54m:34s remains)
INFO - root - 2017-12-06 00:16:12.159152: step 57920, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 64h:01m:35s remains)
INFO - root - 2017-12-06 00:16:20.644696: step 57930, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 64h:35m:37s remains)
INFO - root - 2017-12-06 00:16:29.133276: step 57940, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.805 sec/batch; 61h:24m:35s remains)
INFO - root - 2017-12-06 00:16:37.599388: step 57950, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 63h:51m:17s remains)
INFO - root - 2017-12-06 00:16:46.221298: step 57960, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 66h:41m:53s remains)
INFO - root - 2017-12-06 00:16:54.692746: step 57970, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 65h:45m:49s remains)
INFO - root - 2017-12-06 00:17:03.256841: step 57980, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 64h:21m:46s remains)
INFO - root - 2017-12-06 00:17:11.778482: step 57990, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 66h:18m:45s remains)
INFO - root - 2017-12-06 00:17:20.445435: step 58000, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 64h:16m:02s remains)
2017-12-06 00:17:21.198236: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2926626 -4.2934694 -4.2901764 -4.2886329 -4.2918563 -4.2947407 -4.2985911 -4.3040304 -4.3085876 -4.3120589 -4.3142428 -4.3153982 -4.3182235 -4.3230324 -4.3292909][-4.2554369 -4.2541533 -4.2493596 -4.2493324 -4.2533669 -4.2568445 -4.265255 -4.2783537 -4.2905145 -4.2959094 -4.2972045 -4.297554 -4.3009057 -4.3078041 -4.31809][-4.1987553 -4.1955676 -4.1907225 -4.193747 -4.2012553 -4.2094884 -4.2237358 -4.2442245 -4.2634082 -4.2719412 -4.2733669 -4.2744527 -4.2796865 -4.2896304 -4.3050632][-4.1306429 -4.1240325 -4.1188726 -4.1220684 -4.1300397 -4.1466546 -4.1735792 -4.2046566 -4.2303219 -4.2403736 -4.2412834 -4.2446122 -4.25463 -4.2702208 -4.2913389][-4.0636697 -4.0465722 -4.0359888 -4.0335255 -4.04145 -4.0724988 -4.1159482 -4.160428 -4.1941414 -4.2090578 -4.2063704 -4.2084908 -4.2238088 -4.2479281 -4.276083][-3.9918547 -3.9639223 -3.9409997 -3.9319472 -3.9453759 -3.994421 -4.056622 -4.1170516 -4.1601119 -4.1800404 -4.1714897 -4.1703181 -4.1886392 -4.2206707 -4.2575893][-3.9346049 -3.8953807 -3.8607688 -3.8490367 -3.8750203 -3.9384418 -4.0114212 -4.0807161 -4.1267891 -4.1485305 -4.1393356 -4.1384907 -4.1595378 -4.1990395 -4.2456508][-3.926532 -3.8865657 -3.8500035 -3.8391852 -3.8716035 -3.9319816 -4.0025067 -4.06982 -4.1092539 -4.1252403 -4.1173067 -4.1240344 -4.1492829 -4.1928549 -4.2456727][-3.9564772 -3.9329114 -3.9008293 -3.8896139 -3.9150872 -3.9579127 -4.0090413 -4.061872 -4.0966072 -4.1152372 -4.1185856 -4.1358161 -4.1623497 -4.2044873 -4.2575216][-3.9980388 -4.0002508 -3.9779515 -3.962569 -3.9699774 -3.9879153 -4.0140705 -4.0503721 -4.0789919 -4.0993009 -4.1114049 -4.1377783 -4.1660943 -4.2146053 -4.271328][-3.9808762 -4.0063477 -3.9987502 -3.9934304 -3.9956954 -3.9993598 -4.010859 -4.0340953 -4.057467 -4.0772643 -4.0934997 -4.1243415 -4.1595206 -4.2166686 -4.2768903][-3.9133072 -3.9426522 -3.9480572 -3.956707 -3.9669445 -3.9720113 -3.9908919 -4.0185828 -4.0434856 -4.0624928 -4.0795169 -4.1123548 -4.1569633 -4.2224407 -4.2822819][-3.8696713 -3.8807368 -3.8862276 -3.902905 -3.9281926 -3.9494722 -3.9895339 -4.0341163 -4.0633459 -4.077714 -4.0889025 -4.1208639 -4.1713119 -4.2375174 -4.2925358][-3.8743985 -3.8662877 -3.8636217 -3.8833265 -3.9202006 -3.9619086 -4.0254893 -4.0872021 -4.123363 -4.1310897 -4.1287088 -4.1462679 -4.188375 -4.2477441 -4.2976031][-3.9199653 -3.90537 -3.8971667 -3.9155304 -3.9541931 -4.0048571 -4.0758572 -4.1405396 -4.1761465 -4.1796079 -4.1679168 -4.1720157 -4.2019629 -4.2520461 -4.2992578]]...]
INFO - root - 2017-12-06 00:17:29.718519: step 58010, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 64h:34m:33s remains)
INFO - root - 2017-12-06 00:17:38.462133: step 58020, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.882 sec/batch; 67h:15m:55s remains)
INFO - root - 2017-12-06 00:17:46.863388: step 58030, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 64h:04m:00s remains)
INFO - root - 2017-12-06 00:17:55.518199: step 58040, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.881 sec/batch; 67h:07m:57s remains)
INFO - root - 2017-12-06 00:18:04.073683: step 58050, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 62h:15m:25s remains)
INFO - root - 2017-12-06 00:18:12.560975: step 58060, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 65h:48m:39s remains)
INFO - root - 2017-12-06 00:18:20.980497: step 58070, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 62h:31m:07s remains)
INFO - root - 2017-12-06 00:18:29.480664: step 58080, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 65h:14m:43s remains)
INFO - root - 2017-12-06 00:18:38.013492: step 58090, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 66h:43m:52s remains)
INFO - root - 2017-12-06 00:18:46.517144: step 58100, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 65h:41m:19s remains)
2017-12-06 00:18:47.270471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2974887 -4.2960196 -4.2956996 -4.2907586 -4.2849894 -4.2837005 -4.2847795 -4.2849627 -4.2835722 -4.283565 -4.2863669 -4.2914419 -4.2970657 -4.2912536 -4.2699084][-4.2922416 -4.2972746 -4.3004079 -4.2930737 -4.2816467 -4.2753205 -4.2743769 -4.2755289 -4.2769203 -4.2794538 -4.2836361 -4.2908511 -4.2985539 -4.2925491 -4.2685308][-4.2818575 -4.2925982 -4.2959437 -4.2830234 -4.2633057 -4.2490926 -4.2461624 -4.2496443 -4.2543077 -4.2585974 -4.2640615 -4.2729878 -4.2829714 -4.2785468 -4.2554345][-4.2654972 -4.2799473 -4.2816262 -4.2609482 -4.2288389 -4.2013059 -4.1954789 -4.2043405 -4.2141547 -4.221086 -4.2279186 -4.2387118 -4.2509532 -4.2475638 -4.2253413][-4.2436175 -4.2586775 -4.2585211 -4.2292085 -4.1788268 -4.1300588 -4.1161184 -4.1337981 -4.1568856 -4.1721411 -4.185739 -4.2037587 -4.2182951 -4.2132344 -4.1871705][-4.2098541 -4.2233982 -4.2169666 -4.1770267 -4.1079779 -4.0300183 -3.9987018 -4.0321183 -4.0770617 -4.1042871 -4.1275029 -4.1558943 -4.1736193 -4.1661205 -4.1328945][-4.1820884 -4.1877794 -4.1661425 -4.1098671 -4.018918 -3.9051707 -3.8435187 -3.8939428 -3.9745951 -4.0246859 -4.0633092 -4.1016536 -4.123354 -4.1142855 -4.0769796][-4.1782866 -4.17783 -4.1448364 -4.0778451 -3.9795554 -3.8475146 -3.760026 -3.8130038 -3.9141557 -3.9774771 -4.0239153 -4.0655618 -4.0920033 -4.0885763 -4.0587044][-4.190237 -4.1866655 -4.1515427 -4.0947871 -4.02386 -3.9325891 -3.8756218 -3.9143829 -3.9855254 -4.0310049 -4.0613894 -4.0852618 -4.1029005 -4.1024332 -4.0815554][-4.2025104 -4.194417 -4.1666708 -4.1345792 -4.0996761 -4.05853 -4.0403533 -4.0681152 -4.1073413 -4.1316328 -4.14418 -4.1515379 -4.1583705 -4.1580119 -4.14267][-4.2270555 -4.2214761 -4.207571 -4.1978426 -4.18733 -4.1763568 -4.1773086 -4.1935544 -4.2103643 -4.2202749 -4.2242708 -4.2270904 -4.2340937 -4.2360644 -4.223721][-4.2770319 -4.2747564 -4.2700343 -4.2693992 -4.267755 -4.2666273 -4.2709575 -4.2777858 -4.2817073 -4.284049 -4.2868648 -4.2921972 -4.3014569 -4.3054762 -4.2977743][-4.3214483 -4.3189 -4.3151231 -4.314218 -4.3136249 -4.3143706 -4.3160477 -4.3183322 -4.3182125 -4.3181214 -4.3210335 -4.3273249 -4.3362494 -4.340157 -4.3352742][-4.3441482 -4.3403168 -4.3359189 -4.3339758 -4.3329258 -4.3320007 -4.3324356 -4.3328357 -4.3323326 -4.3318791 -4.3337379 -4.3390346 -4.3455043 -4.3486285 -4.3463521][-4.3496838 -4.3460836 -4.3433771 -4.3416929 -4.3408575 -4.3406358 -4.3408556 -4.3416848 -4.3424835 -4.3429456 -4.3438368 -4.3458652 -4.3484488 -4.3494987 -4.3483639]]...]
INFO - root - 2017-12-06 00:18:55.846667: step 58110, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 65h:44m:04s remains)
INFO - root - 2017-12-06 00:19:04.624000: step 58120, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 66h:18m:39s remains)
INFO - root - 2017-12-06 00:19:13.184671: step 58130, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 65h:22m:47s remains)
INFO - root - 2017-12-06 00:19:21.674897: step 58140, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 65h:57m:12s remains)
INFO - root - 2017-12-06 00:19:30.326822: step 58150, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 65h:20m:08s remains)
INFO - root - 2017-12-06 00:19:38.984539: step 58160, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 65h:27m:20s remains)
INFO - root - 2017-12-06 00:19:47.282109: step 58170, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 63h:30m:57s remains)
INFO - root - 2017-12-06 00:19:55.816293: step 58180, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 64h:11m:15s remains)
INFO - root - 2017-12-06 00:20:04.415030: step 58190, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 67h:27m:14s remains)
INFO - root - 2017-12-06 00:20:12.875823: step 58200, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 62h:49m:11s remains)
2017-12-06 00:20:13.608838: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2702308 -4.2527833 -4.238842 -4.228292 -4.2225618 -4.2264857 -4.2401505 -4.2471123 -4.2438755 -4.251596 -4.2702088 -4.2807426 -4.2752414 -4.2637587 -4.2575989][-4.250495 -4.2290521 -4.2100663 -4.19604 -4.1831651 -4.1780181 -4.1890106 -4.1978674 -4.1962605 -4.2109003 -4.2394853 -4.257556 -4.2581677 -4.2441187 -4.2285347][-4.2375555 -4.21431 -4.1899858 -4.168653 -4.1487684 -4.1378379 -4.1468697 -4.15239 -4.1446619 -4.1619678 -4.2030725 -4.232378 -4.2402153 -4.2240777 -4.1948452][-4.2181692 -4.1950006 -4.16744 -4.1384587 -4.1145687 -4.1068373 -4.1205869 -4.1271305 -4.114912 -4.1261415 -4.168056 -4.200511 -4.2129021 -4.1936803 -4.1577611][-4.1973863 -4.1739674 -4.1448741 -4.1145425 -4.0886641 -4.0769339 -4.0884466 -4.1008978 -4.0978184 -4.10804 -4.1407127 -4.1702123 -4.189033 -4.1802664 -4.1465569][-4.1818933 -4.1535497 -4.1267934 -4.1039991 -4.0815849 -4.0636888 -4.0631518 -4.080956 -4.0938139 -4.1112003 -4.1383791 -4.1614904 -4.1824665 -4.1872692 -4.1658592][-4.1801953 -4.1405268 -4.1160197 -4.111196 -4.1079254 -4.093338 -4.0853524 -4.1037569 -4.1211357 -4.1381578 -4.1593733 -4.1796918 -4.2026329 -4.2196822 -4.21495][-4.1897292 -4.1397662 -4.1051359 -4.103498 -4.1154428 -4.1156831 -4.1126666 -4.1295824 -4.1438909 -4.1583176 -4.1744871 -4.1889668 -4.2165065 -4.2493649 -4.2597928][-4.2037754 -4.155026 -4.1132808 -4.1033764 -4.1123075 -4.1141424 -4.1143522 -4.1295252 -4.1409626 -4.1553864 -4.1673746 -4.1780376 -4.2094541 -4.2559543 -4.2818141][-4.2188644 -4.1873465 -4.1505857 -4.1339264 -4.1279135 -4.1215949 -4.1186476 -4.1339254 -4.144752 -4.1618547 -4.16964 -4.1762261 -4.2068467 -4.2557712 -4.2896967][-4.2383919 -4.2334814 -4.2106276 -4.194581 -4.1803021 -4.1633525 -4.1549559 -4.1611147 -4.1674991 -4.183589 -4.189774 -4.1941195 -4.2202659 -4.2647786 -4.2963591][-4.2577596 -4.272634 -4.2645621 -4.2539673 -4.2423944 -4.224503 -4.2127285 -4.2065768 -4.2030926 -4.2123308 -4.2176781 -4.2216358 -4.2432914 -4.2773781 -4.3009787][-4.2699175 -4.294601 -4.2961779 -4.2931323 -4.2889881 -4.2767415 -4.2673078 -4.2582946 -4.2499828 -4.2494617 -4.2494268 -4.2492251 -4.2630672 -4.2869511 -4.3053074][-4.2802811 -4.3052235 -4.3110147 -4.3127189 -4.3132911 -4.31014 -4.3061743 -4.30118 -4.2949452 -4.2874637 -4.2785306 -4.2719049 -4.2767334 -4.2910147 -4.3050766][-4.2979236 -4.3131347 -4.3163195 -4.3209829 -4.3263721 -4.3300557 -4.3315277 -4.3282862 -4.3240213 -4.3164654 -4.3051195 -4.2943463 -4.2932034 -4.2980409 -4.3051543]]...]
INFO - root - 2017-12-06 00:20:22.181906: step 58210, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 65h:18m:44s remains)
INFO - root - 2017-12-06 00:20:30.802877: step 58220, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 63h:01m:02s remains)
INFO - root - 2017-12-06 00:20:39.322553: step 58230, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 63h:57m:12s remains)
INFO - root - 2017-12-06 00:20:47.973197: step 58240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 65h:50m:18s remains)
INFO - root - 2017-12-06 00:20:56.504924: step 58250, loss = 2.04, batch loss = 1.99 (9.8 examples/sec; 0.817 sec/batch; 62h:12m:38s remains)
INFO - root - 2017-12-06 00:21:05.108015: step 58260, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 63h:36m:14s remains)
INFO - root - 2017-12-06 00:21:13.511452: step 58270, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 62h:24m:29s remains)
INFO - root - 2017-12-06 00:21:22.102508: step 58280, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 67h:02m:45s remains)
INFO - root - 2017-12-06 00:21:30.655679: step 58290, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 65h:07m:06s remains)
INFO - root - 2017-12-06 00:21:39.139413: step 58300, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 66h:33m:30s remains)
2017-12-06 00:21:39.881427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2025452 -4.209641 -4.2214952 -4.2323203 -4.236618 -4.2256961 -4.2250566 -4.2443924 -4.2507038 -4.2408466 -4.2224627 -4.2162328 -4.2232137 -4.2392497 -4.259922][-4.2000017 -4.2015452 -4.2094135 -4.2216563 -4.2285357 -4.2141314 -4.2131476 -4.2350965 -4.2425275 -4.2321119 -4.2134919 -4.2096319 -4.2196212 -4.2387843 -4.2583485][-4.1911206 -4.1867003 -4.1923461 -4.2067122 -4.2173719 -4.2036757 -4.2000422 -4.2186689 -4.2272038 -4.2190447 -4.2017627 -4.2007332 -4.21179 -4.2336788 -4.2537532][-4.1819291 -4.1778722 -4.1835704 -4.198247 -4.2059526 -4.1881218 -4.176621 -4.1888919 -4.2013407 -4.1940751 -4.1765475 -4.1798072 -4.1940508 -4.2188644 -4.2420864][-4.1704149 -4.1700435 -4.1741595 -4.1819134 -4.1799388 -4.15333 -4.1356235 -4.144309 -4.1578269 -4.1513715 -4.1369886 -4.1487422 -4.1706986 -4.2019815 -4.2301912][-4.1680818 -4.1682906 -4.1666927 -4.1636992 -4.1457763 -4.1058831 -4.0777221 -4.0767536 -4.0851717 -4.0804877 -4.0771704 -4.1068506 -4.1425996 -4.1872382 -4.2233243][-4.1692162 -4.16288 -4.1504073 -4.1341934 -4.102869 -4.0523491 -4.0161304 -4.0076928 -4.0115318 -4.0112567 -4.027267 -4.0783291 -4.1268959 -4.1826739 -4.2253728][-4.173883 -4.1561337 -4.1300874 -4.1047587 -4.0669522 -4.0160027 -3.984586 -3.9839485 -3.9928977 -3.9985127 -4.024663 -4.0784469 -4.1274738 -4.1839924 -4.2288818][-4.1886 -4.1667595 -4.1382532 -4.1132283 -4.0743589 -4.0308294 -4.0183387 -4.0363264 -4.0530806 -4.0583773 -4.0788212 -4.1192055 -4.159482 -4.2052665 -4.2444673][-4.2111859 -4.1885271 -4.1665249 -4.1496511 -4.1206751 -4.0929074 -4.0968294 -4.1223159 -4.1340866 -4.1311007 -4.1405487 -4.16789 -4.2026634 -4.239893 -4.2702775][-4.2374325 -4.219707 -4.2080708 -4.2002029 -4.1848378 -4.1706629 -4.1777973 -4.1937571 -4.1930375 -4.1819983 -4.1844168 -4.203691 -4.2338057 -4.2659764 -4.2902722][-4.262558 -4.2495346 -4.2445312 -4.24 -4.231977 -4.2274694 -4.2324491 -4.2380033 -4.2273474 -4.21426 -4.2172656 -4.2326541 -4.2579393 -4.2849808 -4.3047438][-4.2859154 -4.2776875 -4.2769151 -4.2757754 -4.2721214 -4.2736063 -4.2800393 -4.2824612 -4.2706442 -4.2593975 -4.2616014 -4.2723279 -4.2885709 -4.3072453 -4.3208013][-4.307611 -4.3030071 -4.3027186 -4.3045988 -4.3049197 -4.3091006 -4.3155336 -4.3190646 -4.3139338 -4.3074021 -4.3067589 -4.3101292 -4.3169746 -4.3253665 -4.3318472][-4.3221955 -4.3189654 -4.3174038 -4.3180766 -4.319294 -4.3222136 -4.32652 -4.3293676 -4.3279848 -4.3252616 -4.323565 -4.324193 -4.3270431 -4.33084 -4.334487]]...]
INFO - root - 2017-12-06 00:21:48.410056: step 58310, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 67h:28m:36s remains)
INFO - root - 2017-12-06 00:21:57.040327: step 58320, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 62h:37m:17s remains)
INFO - root - 2017-12-06 00:22:05.670784: step 58330, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 63h:42m:59s remains)
INFO - root - 2017-12-06 00:22:14.225456: step 58340, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 66h:48m:33s remains)
INFO - root - 2017-12-06 00:22:22.770887: step 58350, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 64h:22m:36s remains)
INFO - root - 2017-12-06 00:22:31.239217: step 58360, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 64h:25m:15s remains)
INFO - root - 2017-12-06 00:22:39.541555: step 58370, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.826 sec/batch; 62h:54m:43s remains)
INFO - root - 2017-12-06 00:22:48.211991: step 58380, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 65h:28m:41s remains)
INFO - root - 2017-12-06 00:22:56.645657: step 58390, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 65h:30m:54s remains)
INFO - root - 2017-12-06 00:23:05.148820: step 58400, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.834 sec/batch; 63h:31m:56s remains)
2017-12-06 00:23:06.002705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1452889 -4.1602082 -4.1829453 -4.1814737 -4.1740236 -4.185225 -4.210618 -4.2252393 -4.2300596 -4.2214909 -4.2070427 -4.1866856 -4.1717448 -4.1699591 -4.1848092][-4.13855 -4.1590571 -4.1871037 -4.1888833 -4.176404 -4.1819243 -4.2078986 -4.223218 -4.2295651 -4.2216763 -4.2041426 -4.1785226 -4.156487 -4.1528549 -4.1729212][-4.1328483 -4.1610494 -4.1894684 -4.1913776 -4.175354 -4.1727414 -4.1937737 -4.2086296 -4.2137456 -4.2037387 -4.1846509 -4.162199 -4.1369905 -4.1320424 -4.1537008][-4.1140795 -4.1435714 -4.1694088 -4.1732049 -4.1597991 -4.1517949 -4.1660252 -4.1806011 -4.18728 -4.1790266 -4.1619282 -4.1429873 -4.1180525 -4.1121864 -4.1345348][-4.1216531 -4.1444607 -4.16101 -4.1591206 -4.1446524 -4.1356215 -4.1503444 -4.1688123 -4.1789813 -4.1746521 -4.1591406 -4.1450253 -4.1281309 -4.1283703 -4.1500978][-4.15067 -4.1639037 -4.1651478 -4.1448021 -4.1172543 -4.1072445 -4.1291723 -4.1568489 -4.1729035 -4.1748476 -4.1660032 -4.15968 -4.1559262 -4.1627989 -4.1841669][-4.1456842 -4.1562042 -4.1495519 -4.1120663 -4.0621581 -4.04295 -4.0735631 -4.1150417 -4.1377754 -4.1444025 -4.1451015 -4.147141 -4.1512032 -4.1620545 -4.1855931][-4.1067142 -4.1275759 -4.1263571 -4.0858107 -4.0237975 -3.9909971 -4.017149 -4.0633979 -4.0920386 -4.0990877 -4.0972867 -4.1014156 -4.1137242 -4.1334419 -4.1623063][-4.0710249 -4.1072278 -4.1181045 -4.0894346 -4.0360947 -4.0020862 -4.0130248 -4.0445056 -4.0704179 -4.075738 -4.0643 -4.0614371 -4.0821276 -4.1104541 -4.1413555][-4.0499377 -4.0936246 -4.1081266 -4.0885143 -4.0486207 -4.0216885 -4.0238957 -4.0452757 -4.0677381 -4.0710917 -4.0528169 -4.04518 -4.0712433 -4.1050382 -4.1307073][-4.048068 -4.0890603 -4.1025968 -4.0907574 -4.0656629 -4.0477738 -4.0479431 -4.0640264 -4.0798516 -4.07701 -4.0554338 -4.0476027 -4.0750279 -4.1076827 -4.1283636][-4.0811048 -4.1116791 -4.1217093 -4.118165 -4.10833 -4.1004391 -4.1008406 -4.1098266 -4.1184239 -4.1125946 -4.0920868 -4.0822806 -4.1040258 -4.1336489 -4.1502013][-4.143332 -4.1602788 -4.1647296 -4.1638594 -4.16037 -4.1563148 -4.15502 -4.1584649 -4.1642036 -4.1615558 -4.1463184 -4.1374035 -4.1534171 -4.1801305 -4.1945057][-4.206923 -4.2147655 -4.2147245 -4.2124162 -4.2088127 -4.2055049 -4.2031045 -4.2030721 -4.2058039 -4.2071128 -4.20207 -4.1994634 -4.2109308 -4.2305379 -4.2423925][-4.2545571 -4.2582521 -4.2574697 -4.25616 -4.2537546 -4.2507486 -4.2475524 -4.2451644 -4.2449193 -4.2466345 -4.2478981 -4.2487173 -4.2547016 -4.2657719 -4.274179]]...]
INFO - root - 2017-12-06 00:23:14.564612: step 58410, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 62h:52m:30s remains)
INFO - root - 2017-12-06 00:23:22.908203: step 58420, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 62h:25m:43s remains)
INFO - root - 2017-12-06 00:23:31.443873: step 58430, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 63h:33m:22s remains)
INFO - root - 2017-12-06 00:23:40.019180: step 58440, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 65h:32m:06s remains)
INFO - root - 2017-12-06 00:23:48.625073: step 58450, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 64h:36m:55s remains)
INFO - root - 2017-12-06 00:23:57.117504: step 58460, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 64h:12m:54s remains)
INFO - root - 2017-12-06 00:24:05.596899: step 58470, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 64h:43m:05s remains)
INFO - root - 2017-12-06 00:24:14.086027: step 58480, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 65h:52m:56s remains)
INFO - root - 2017-12-06 00:24:22.588677: step 58490, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 64h:47m:12s remains)
INFO - root - 2017-12-06 00:24:31.084988: step 58500, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 65h:57m:00s remains)
2017-12-06 00:24:31.902489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3425074 -4.3382936 -4.3343644 -4.3295088 -4.3232837 -4.3169565 -4.3104649 -4.3065882 -4.3058796 -4.3082771 -4.3174858 -4.328732 -4.3370876 -4.3436594 -4.3460655][-4.3430228 -4.3364153 -4.3287029 -4.3182893 -4.3079653 -4.2999458 -4.293118 -4.2905269 -4.2924433 -4.2951736 -4.3046579 -4.3171229 -4.3279057 -4.3399448 -4.34778][-4.337697 -4.3263693 -4.3106532 -4.2892489 -4.2714863 -4.2595172 -4.2532225 -4.2578206 -4.26573 -4.2747393 -4.2910109 -4.3083458 -4.3226366 -4.3371549 -4.3465281][-4.3236246 -4.3039637 -4.2745719 -4.2384229 -4.2080808 -4.1901727 -4.1872211 -4.198103 -4.2147222 -4.239078 -4.2696104 -4.29618 -4.3166957 -4.3343406 -4.3435955][-4.294425 -4.2616377 -4.2178411 -4.1684713 -4.1248665 -4.1057692 -4.1091657 -4.1292882 -4.157125 -4.1962986 -4.2378531 -4.2752676 -4.3045063 -4.3278809 -4.3385544][-4.2546005 -4.2053185 -4.1465735 -4.0810056 -4.02408 -4.0077114 -4.0281582 -4.0675049 -4.1125894 -4.1628771 -4.214397 -4.2608194 -4.2970638 -4.3260336 -4.3349123][-4.2167206 -4.1500206 -4.07523 -3.988359 -3.9138737 -3.9055958 -3.9464014 -4.0026617 -4.05792 -4.1143947 -4.1732516 -4.2339721 -4.2847009 -4.3204894 -4.3271856][-4.1897631 -4.1024985 -4.0052218 -3.8923225 -3.8013835 -3.808393 -3.8764384 -3.9530814 -4.0234694 -4.0910196 -4.1553121 -4.2219625 -4.2822585 -4.3188772 -4.3243327][-4.182683 -4.0772605 -3.9586134 -3.8308663 -3.7354803 -3.7578642 -3.8492723 -3.9474144 -4.0425668 -4.11971 -4.1826448 -4.2433782 -4.2948275 -4.3246779 -4.329855][-4.190434 -4.0820503 -3.962286 -3.8505905 -3.7756195 -3.7975721 -3.8828964 -3.9832199 -4.0855865 -4.1658039 -4.224462 -4.2758455 -4.3165455 -4.3376384 -4.3412857][-4.1888013 -4.0914621 -3.9942098 -3.9176681 -3.877485 -3.9029298 -3.9738536 -4.0592637 -4.1500134 -4.2229047 -4.2690659 -4.3059964 -4.3347139 -4.3476596 -4.3473263][-4.1699319 -4.0899897 -4.0272303 -3.9884458 -3.9805148 -4.0111141 -4.0713682 -4.141386 -4.2150106 -4.2690687 -4.2964587 -4.3147359 -4.3308272 -4.3368459 -4.3344789][-4.13099 -4.0609822 -4.0247693 -4.0164843 -4.0312257 -4.0708904 -4.1326566 -4.2003903 -4.2643065 -4.3043227 -4.3159685 -4.3148031 -4.3141642 -4.310082 -4.3015642][-4.0962081 -4.0339165 -4.0122108 -4.018827 -4.04559 -4.0949326 -4.1605153 -4.2262874 -4.281744 -4.313303 -4.3141341 -4.2968855 -4.2811341 -4.2669144 -4.25188][-4.0953703 -4.0461311 -4.0357308 -4.0488768 -4.0774589 -4.1240091 -4.1827965 -4.238883 -4.2838726 -4.3062677 -4.30205 -4.2784142 -4.2541094 -4.23015 -4.20741]]...]
INFO - root - 2017-12-06 00:24:40.488268: step 58510, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 63h:37m:26s remains)
INFO - root - 2017-12-06 00:24:48.850458: step 58520, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 55h:46m:31s remains)
INFO - root - 2017-12-06 00:24:57.431597: step 58530, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 68h:40m:57s remains)
INFO - root - 2017-12-06 00:25:06.046403: step 58540, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 66h:19m:26s remains)
INFO - root - 2017-12-06 00:25:14.630024: step 58550, loss = 2.04, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 62h:52m:33s remains)
INFO - root - 2017-12-06 00:25:23.211426: step 58560, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 65h:35m:31s remains)
INFO - root - 2017-12-06 00:25:31.617948: step 58570, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 66h:30m:08s remains)
INFO - root - 2017-12-06 00:25:40.220622: step 58580, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 65h:44m:07s remains)
INFO - root - 2017-12-06 00:25:48.899196: step 58590, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 65h:23m:26s remains)
INFO - root - 2017-12-06 00:25:57.415827: step 58600, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 64h:35m:39s remains)
2017-12-06 00:25:58.117230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2360773 -4.2546306 -4.2587247 -4.2386103 -4.2001696 -4.15613 -4.1253023 -4.1192479 -4.1276283 -4.1421676 -4.1614866 -4.1910005 -4.2284765 -4.259501 -4.28094][-4.22139 -4.2491469 -4.261096 -4.2400322 -4.1986008 -4.1541252 -4.1265969 -4.1178131 -4.1239729 -4.1404419 -4.1608324 -4.1912961 -4.2308607 -4.2623639 -4.2865791][-4.2288837 -4.2515941 -4.2627015 -4.2464104 -4.2094765 -4.1679964 -4.13992 -4.1260619 -4.1330676 -4.1527514 -4.176043 -4.2088861 -4.2449665 -4.2724571 -4.2932868][-4.2410717 -4.2540703 -4.2617536 -4.2487707 -4.2158842 -4.17461 -4.13729 -4.1169572 -4.1286788 -4.1493373 -4.1728706 -4.2081504 -4.2482095 -4.2762537 -4.2974243][-4.2565913 -4.2600441 -4.2555218 -4.2304745 -4.1862216 -4.1312184 -4.0714951 -4.0340934 -4.0540118 -4.089303 -4.1239347 -4.173388 -4.2278571 -4.2653394 -4.2922177][-4.2497869 -4.2403316 -4.2130642 -4.1585641 -4.0873117 -4.0065627 -3.9093413 -3.8545051 -3.9059365 -3.9796927 -4.044014 -4.1178627 -4.1872993 -4.2377038 -4.2766891][-4.23146 -4.2089338 -4.1614742 -4.0833912 -3.9929936 -3.8973758 -3.7780695 -3.7187405 -3.8080189 -3.9148676 -4.0031185 -4.0943646 -4.168788 -4.2235446 -4.2669859][-4.2345567 -4.2094235 -4.1656861 -4.0972104 -4.0239992 -3.9612629 -3.8806376 -3.8422554 -3.9060934 -3.983855 -4.0486531 -4.1216426 -4.182435 -4.2307935 -4.2717576][-4.2622561 -4.2437577 -4.2151985 -4.1695848 -4.119998 -4.0846109 -4.0392547 -4.0149403 -4.0388837 -4.0737934 -4.1117969 -4.1665511 -4.2134461 -4.2546587 -4.2903481][-4.2865644 -4.2750406 -4.2604804 -4.2328486 -4.2006049 -4.1775732 -4.1481357 -4.1266327 -4.1246147 -4.1332679 -4.1561937 -4.1986446 -4.2380838 -4.2748747 -4.3069258][-4.2971659 -4.29038 -4.2816815 -4.2645993 -4.2443733 -4.2297111 -4.2094574 -4.1858239 -4.1681356 -4.1623063 -4.1769094 -4.210639 -4.2458925 -4.283319 -4.3151727][-4.2928104 -4.2860603 -4.2785983 -4.2684083 -4.2554097 -4.24456 -4.2335434 -4.2158184 -4.1931663 -4.1779528 -4.1855125 -4.2105169 -4.2432137 -4.2803769 -4.3136358][-4.28477 -4.277081 -4.2690558 -4.2615457 -4.2539506 -4.2480664 -4.2445993 -4.2336459 -4.2126985 -4.194406 -4.1958227 -4.2145638 -4.2426267 -4.2733078 -4.3038816][-4.2803626 -4.2741985 -4.2681079 -4.2674561 -4.2666268 -4.264255 -4.263195 -4.2564912 -4.2388477 -4.2207642 -4.2176266 -4.2311954 -4.2520666 -4.272594 -4.2974658][-4.2714052 -4.2674789 -4.2629347 -4.2642756 -4.2685366 -4.2724156 -4.2750473 -4.2729459 -4.2613959 -4.2452431 -4.239851 -4.2498026 -4.2643175 -4.2767854 -4.2961979]]...]
INFO - root - 2017-12-06 00:26:06.737105: step 58610, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 66h:16m:11s remains)
INFO - root - 2017-12-06 00:26:15.255073: step 58620, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 63h:54m:55s remains)
INFO - root - 2017-12-06 00:26:23.742952: step 58630, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 64h:45m:53s remains)
INFO - root - 2017-12-06 00:26:32.324880: step 58640, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 63h:50m:41s remains)
INFO - root - 2017-12-06 00:26:40.860510: step 58650, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 64h:46m:31s remains)
INFO - root - 2017-12-06 00:26:49.452067: step 58660, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 65h:36m:27s remains)
INFO - root - 2017-12-06 00:26:57.861795: step 58670, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 64h:25m:07s remains)
INFO - root - 2017-12-06 00:27:06.361748: step 58680, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.853 sec/batch; 64h:53m:21s remains)
INFO - root - 2017-12-06 00:27:14.901513: step 58690, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 63h:26m:41s remains)
INFO - root - 2017-12-06 00:27:23.548982: step 58700, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 65h:50m:43s remains)
2017-12-06 00:27:24.385773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1653676 -4.1576104 -4.1447778 -4.132443 -4.1281528 -4.1330686 -4.1429834 -4.1510859 -4.1571565 -4.1439195 -4.126451 -4.1208367 -4.1197867 -4.1174755 -4.1232648][-4.1380448 -4.1257277 -4.1119289 -4.1027942 -4.10426 -4.1092896 -4.1158872 -4.1245809 -4.1430135 -4.1447291 -4.1400995 -4.1405563 -4.1380377 -4.1319551 -4.1364193][-4.1093159 -4.0906854 -4.0774655 -4.0765734 -4.0831594 -4.0851641 -4.0837922 -4.0907812 -4.1196508 -4.1397142 -4.1501007 -4.1537318 -4.149107 -4.1416678 -4.1408596][-4.0921 -4.0679765 -4.0557938 -4.057373 -4.0626292 -4.055687 -4.0399151 -4.0460467 -4.0824704 -4.1196337 -4.143115 -4.1490259 -4.1461735 -4.1363525 -4.1220946][-4.0940557 -4.0677729 -4.0526824 -4.0433173 -4.0325136 -4.0049691 -3.972486 -3.9807918 -4.0325508 -4.0908384 -4.1335034 -4.1493263 -4.1487412 -4.1350565 -4.104795][-4.10139 -4.0812793 -4.0616975 -4.0366597 -4.0041361 -3.9514639 -3.8998425 -3.9086952 -3.9823554 -4.0613346 -4.1224456 -4.1530967 -4.1598611 -4.1465006 -4.1083975][-4.1158433 -4.1034408 -4.0777321 -4.0353661 -3.977608 -3.8928955 -3.8110993 -3.8214736 -3.91839 -4.0182042 -4.0971756 -4.1458135 -4.165894 -4.1580529 -4.1223722][-4.1324911 -4.1265335 -4.100492 -4.0516005 -3.9778023 -3.866992 -3.750638 -3.7590742 -3.8712106 -3.97879 -4.06326 -4.121098 -4.1530457 -4.1546659 -4.1305327][-4.1341953 -4.1347346 -4.1175594 -4.0793738 -4.0178628 -3.9240596 -3.8259206 -3.8291001 -3.9090221 -3.9820907 -4.040154 -4.0830512 -4.1111565 -4.1196847 -4.1132803][-4.1358323 -4.1345119 -4.1226745 -4.0970087 -4.0555634 -3.9955955 -3.934557 -3.9368186 -3.981931 -4.0223751 -4.0496674 -4.0648026 -4.0751705 -4.0756769 -4.0760965][-4.1355577 -4.1296387 -4.1238027 -4.1098094 -4.0830717 -4.0458584 -4.0141025 -4.021328 -4.0517731 -4.0760341 -4.0890212 -4.0890517 -4.0810394 -4.0649028 -4.0600867][-4.1321697 -4.1259012 -4.1278386 -4.125071 -4.1099992 -4.0872159 -4.0706925 -4.079761 -4.1011024 -4.1140432 -4.1202283 -4.121079 -4.1124139 -4.0917091 -4.0821323][-4.1278224 -4.1256504 -4.1354046 -4.1440954 -4.1399722 -4.1242485 -4.1087537 -4.1097221 -4.1211319 -4.1273913 -4.1359644 -4.145844 -4.1449132 -4.1296973 -4.1185827][-4.1374841 -4.1422033 -4.1570253 -4.1678782 -4.1646848 -4.1517653 -4.1354837 -4.1290669 -4.1305275 -4.131494 -4.1420693 -4.1582441 -4.163269 -4.1541452 -4.1461258][-4.1578813 -4.1688323 -4.1841607 -4.1940389 -4.1914268 -4.1823606 -4.1697721 -4.1614633 -4.1557517 -4.1534338 -4.1637883 -4.1787715 -4.1815367 -4.174655 -4.1698174]]...]
INFO - root - 2017-12-06 00:27:32.772726: step 58710, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.829 sec/batch; 63h:04m:53s remains)
INFO - root - 2017-12-06 00:27:41.233179: step 58720, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 64h:36m:26s remains)
INFO - root - 2017-12-06 00:27:49.674567: step 58730, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 64h:09m:29s remains)
INFO - root - 2017-12-06 00:27:58.142975: step 58740, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 66h:05m:04s remains)
INFO - root - 2017-12-06 00:28:06.651325: step 58750, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.818 sec/batch; 62h:11m:44s remains)
INFO - root - 2017-12-06 00:28:15.193917: step 58760, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.849 sec/batch; 64h:34m:17s remains)
INFO - root - 2017-12-06 00:28:23.654714: step 58770, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 64h:14m:24s remains)
INFO - root - 2017-12-06 00:28:32.249545: step 58780, loss = 2.02, batch loss = 1.96 (9.4 examples/sec; 0.852 sec/batch; 64h:48m:49s remains)
INFO - root - 2017-12-06 00:28:40.723219: step 58790, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 64h:06m:26s remains)
INFO - root - 2017-12-06 00:28:49.196142: step 58800, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 64h:33m:48s remains)
2017-12-06 00:28:49.958927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2413926 -4.243958 -4.2452183 -4.2446804 -4.2440181 -4.2446642 -4.2440176 -4.2486153 -4.2520914 -4.2449718 -4.2282224 -4.2121816 -4.2113037 -4.2141848 -4.2164474][-4.2215905 -4.2261233 -4.2295218 -4.2300549 -4.2287192 -4.2274055 -4.2246475 -4.2299256 -4.235045 -4.2265344 -4.2071805 -4.1910582 -4.1958418 -4.2029781 -4.2083921][-4.2128305 -4.2175 -4.2201567 -4.2188478 -4.2144885 -4.2112861 -4.207696 -4.2117095 -4.217227 -4.2099342 -4.1924081 -4.1779671 -4.1884241 -4.2011776 -4.2111073][-4.2022324 -4.2067089 -4.21106 -4.2072978 -4.1980128 -4.1917257 -4.18764 -4.1897793 -4.1965375 -4.1936235 -4.1802497 -4.1699181 -4.1806207 -4.1961651 -4.2106504][-4.1898308 -4.1928592 -4.1978731 -4.1901016 -4.1802111 -4.177525 -4.1730013 -4.1661944 -4.1690555 -4.1717453 -4.1638522 -4.15482 -4.1612763 -4.1810045 -4.2064877][-4.1698084 -4.1673126 -4.1667328 -4.1523581 -4.1407852 -4.1382766 -4.1241069 -4.0973954 -4.0967855 -4.1197724 -4.13157 -4.1282954 -4.129168 -4.1547394 -4.1942697][-4.1411142 -4.1387773 -4.1320772 -4.1128235 -4.1044316 -4.10178 -4.0660305 -4.0072727 -3.9969468 -4.0520315 -4.1023331 -4.1114697 -4.1058292 -4.13276 -4.1820097][-4.0820723 -4.0839481 -4.0703292 -4.0473962 -4.0432906 -4.0394959 -3.9752359 -3.8674746 -3.844424 -3.9510083 -4.0552931 -4.0893526 -4.0935 -4.126523 -4.1805263][-4.0274978 -4.0377874 -4.0250134 -4.0022516 -4.003345 -3.9978442 -3.9150677 -3.7673869 -3.7259965 -3.8717775 -4.0092697 -4.057703 -4.0725269 -4.1131907 -4.174098][-4.0166421 -4.035748 -4.0345922 -4.023231 -4.028636 -4.0300369 -3.9703889 -3.8551223 -3.8110032 -3.9153655 -4.0251694 -4.0642567 -4.0795012 -4.1195564 -4.1796823][-4.05338 -4.0730515 -4.0816388 -4.0806308 -4.0879769 -4.0958385 -4.0695057 -4.0001726 -3.9592261 -4.0082321 -4.0749693 -4.102664 -4.1182284 -4.1489863 -4.194911][-4.1079612 -4.1214209 -4.12814 -4.1267009 -4.1320758 -4.142508 -4.137351 -4.1008697 -4.0649824 -4.0786896 -4.112237 -4.1333618 -4.1531878 -4.177001 -4.2084908][-4.1474533 -4.1560059 -4.1618571 -4.1598554 -4.1635509 -4.1755896 -4.1816998 -4.1654673 -4.1387725 -4.1323714 -4.1384735 -4.1498733 -4.1692557 -4.1872511 -4.2088628][-4.189765 -4.193687 -4.1964602 -4.1957564 -4.1986752 -4.2074728 -4.21586 -4.2129359 -4.1969671 -4.1821661 -4.1706519 -4.1717687 -4.1878114 -4.2012691 -4.2159963][-4.2311411 -4.2342396 -4.2330761 -4.2323823 -4.2345428 -4.2407885 -4.247623 -4.2480459 -4.239994 -4.2262583 -4.2075868 -4.2004695 -4.2126365 -4.2249289 -4.2371826]]...]
INFO - root - 2017-12-06 00:28:58.614464: step 58810, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 67h:23m:28s remains)
INFO - root - 2017-12-06 00:29:07.124179: step 58820, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 65h:36m:54s remains)
INFO - root - 2017-12-06 00:29:15.742693: step 58830, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 64h:57m:09s remains)
INFO - root - 2017-12-06 00:29:24.173579: step 58840, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.736 sec/batch; 55h:58m:50s remains)
INFO - root - 2017-12-06 00:29:32.747795: step 58850, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.856 sec/batch; 65h:02m:07s remains)
INFO - root - 2017-12-06 00:29:41.258471: step 58860, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 67h:11m:50s remains)
INFO - root - 2017-12-06 00:29:49.655313: step 58870, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 65h:38m:44s remains)
INFO - root - 2017-12-06 00:29:58.243682: step 58880, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 65h:58m:07s remains)
INFO - root - 2017-12-06 00:30:06.814197: step 58890, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.814 sec/batch; 61h:50m:08s remains)
INFO - root - 2017-12-06 00:30:15.432056: step 58900, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 65h:02m:28s remains)
2017-12-06 00:30:16.187020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2881141 -4.2764249 -4.2685251 -4.2626643 -4.2605009 -4.2647419 -4.2739515 -4.2848191 -4.2918653 -4.2959714 -4.2986693 -4.2991834 -4.2997928 -4.3030529 -4.3082089][-4.273067 -4.2545977 -4.2437229 -4.238471 -4.23852 -4.2438765 -4.2528453 -4.2618 -4.26637 -4.2714629 -4.2788067 -4.2826328 -4.285264 -4.2924752 -4.3026433][-4.2503233 -4.2240133 -4.2099934 -4.2067738 -4.2090549 -4.2144485 -4.2208991 -4.2241607 -4.2243781 -4.2308588 -4.2449646 -4.2546124 -4.2618432 -4.2759914 -4.29313][-4.2278724 -4.19193 -4.1715436 -4.1654387 -4.1634955 -4.1646047 -4.1709638 -4.1739187 -4.17163 -4.1816936 -4.2026429 -4.2176018 -4.2311087 -4.2521868 -4.2762151][-4.2130356 -4.1637893 -4.1280508 -4.1078033 -4.0907145 -4.0829773 -4.0938311 -4.1063104 -4.1085396 -4.1228824 -4.1455564 -4.1607308 -4.1789455 -4.2093682 -4.2445025][-4.2116871 -4.1513081 -4.1014061 -4.06077 -4.0230451 -4.0036125 -4.020081 -4.0446172 -4.055624 -4.0725088 -4.0921865 -4.1089859 -4.1363835 -4.1761727 -4.2186246][-4.2138929 -4.1526213 -4.0969772 -4.0435867 -3.9898822 -3.9611011 -3.9831586 -4.0154 -4.0270891 -4.0367589 -4.0468988 -4.0691848 -4.1083174 -4.1577864 -4.2059159][-4.2082763 -4.1516571 -4.0984535 -4.0453386 -3.993329 -3.9701288 -3.9948363 -4.0193434 -4.0149484 -4.0061793 -4.0040035 -4.033319 -4.0877848 -4.1445694 -4.2014728][-4.2009187 -4.1474481 -4.0994663 -4.0557213 -4.0212331 -4.0107141 -4.0295115 -4.0325031 -4.0078478 -3.9816558 -3.9658787 -4.0021324 -4.0734644 -4.1422639 -4.2093883][-4.2027225 -4.1520004 -4.1071181 -4.0770082 -4.05844 -4.0548391 -4.0690093 -4.0595531 -4.0246668 -3.9985218 -3.9852457 -4.0247746 -4.1001 -4.1713367 -4.2371588][-4.2255011 -4.1814437 -4.1448517 -4.126966 -4.11882 -4.1133375 -4.1151819 -4.0979404 -4.0647125 -4.0451617 -4.043035 -4.0846443 -4.1517935 -4.2152619 -4.2708812][-4.257678 -4.2247143 -4.2005877 -4.1915402 -4.1876817 -4.1765995 -4.1661649 -4.1434345 -4.1103592 -4.0966487 -4.1099057 -4.154006 -4.2090783 -4.2609634 -4.3018165][-4.2888508 -4.2667618 -4.2536154 -4.2514372 -4.2516389 -4.2432508 -4.2297926 -4.2029896 -4.1719675 -4.1624489 -4.1814218 -4.2207241 -4.26208 -4.2991595 -4.3254251][-4.3070941 -4.2945876 -4.288528 -4.288548 -4.2894316 -4.2828979 -4.2703218 -4.2494612 -4.2285776 -4.2269659 -4.2419167 -4.2700362 -4.29975 -4.3222151 -4.3358922][-4.3164845 -4.3102307 -4.3079705 -4.3082347 -4.3075185 -4.3025365 -4.2926779 -4.2821994 -4.2734866 -4.2769079 -4.2878613 -4.3055716 -4.3242755 -4.335135 -4.3391151]]...]
INFO - root - 2017-12-06 00:30:24.770778: step 58910, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 64h:44m:29s remains)
INFO - root - 2017-12-06 00:30:33.238658: step 58920, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 64h:09m:01s remains)
INFO - root - 2017-12-06 00:30:41.624208: step 58930, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 64h:23m:47s remains)
INFO - root - 2017-12-06 00:30:50.251492: step 58940, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 64h:33m:24s remains)
INFO - root - 2017-12-06 00:30:58.785845: step 58950, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.884 sec/batch; 67h:12m:25s remains)
INFO - root - 2017-12-06 00:31:07.418286: step 58960, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 65h:23m:41s remains)
INFO - root - 2017-12-06 00:31:15.841477: step 58970, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 67h:13m:51s remains)
INFO - root - 2017-12-06 00:31:24.501124: step 58980, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 67h:35m:48s remains)
INFO - root - 2017-12-06 00:31:33.065846: step 58990, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 65h:04m:44s remains)
INFO - root - 2017-12-06 00:31:41.571543: step 59000, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 65h:20m:09s remains)
2017-12-06 00:31:42.320571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1517653 -4.1598759 -4.1666012 -4.1682286 -4.1654143 -4.155417 -4.1345024 -4.1241622 -4.1415315 -4.1594515 -4.154613 -4.1398954 -4.1273694 -4.1148987 -4.1133909][-4.13598 -4.1548672 -4.1749396 -4.1766376 -4.1688452 -4.1571565 -4.1394658 -4.1314874 -4.1464539 -4.1631866 -4.1583161 -4.142899 -4.1317372 -4.1189866 -4.1133633][-4.1186252 -4.1414633 -4.1687717 -4.167068 -4.1493134 -4.1378336 -4.130372 -4.1329145 -4.1523423 -4.1705275 -4.1670423 -4.1498213 -4.1402845 -4.1326933 -4.129437][-4.102572 -4.1227589 -4.1496282 -4.1415453 -4.1142321 -4.1026287 -4.1028776 -4.1191578 -4.14462 -4.1604347 -4.1523895 -4.1321416 -4.1246133 -4.1252861 -4.1273031][-4.0897975 -4.1114917 -4.133141 -4.1155725 -4.0812526 -4.0672398 -4.0710268 -4.0952663 -4.122282 -4.1322589 -4.1170716 -4.0962887 -4.0896368 -4.0963287 -4.0996637][-4.085814 -4.1120338 -4.1232848 -4.0918427 -4.0453925 -4.0279822 -4.03567 -4.0679817 -4.0966382 -4.1066618 -4.0901446 -4.0640459 -4.0516777 -4.0593319 -4.0668063][-4.0804176 -4.1067123 -4.1057763 -4.0571585 -3.9926233 -3.9674881 -3.9806719 -4.0260992 -4.0655913 -4.0846539 -4.0738792 -4.044744 -4.0223842 -4.0239272 -4.03683][-4.0834055 -4.0976524 -4.0814853 -4.0150619 -3.926229 -3.8867738 -3.9004204 -3.9616554 -4.0254364 -4.0626769 -4.0660458 -4.0367975 -4.0019679 -3.990129 -4.0052972][-4.1013165 -4.1024423 -4.0760942 -4.0038419 -3.9061871 -3.8523996 -3.8596287 -3.9310966 -4.0104022 -4.0574951 -4.0684714 -4.0405865 -3.9935267 -3.9636567 -3.9771209][-4.1246176 -4.1197486 -4.096242 -4.0398469 -3.9614253 -3.9133029 -3.9188471 -3.9731216 -4.0331235 -4.071413 -4.0820589 -4.0544376 -4.0006986 -3.9625235 -3.9754331][-4.1505666 -4.1426778 -4.1255126 -4.0885768 -4.0340266 -4.0009375 -4.0082006 -4.041491 -4.0751934 -4.0971079 -4.1011496 -4.0716686 -4.0203524 -3.99073 -4.0080276][-4.1823406 -4.1712527 -4.1566024 -4.1274633 -4.0863891 -4.0644288 -4.071918 -4.0933318 -4.1126571 -4.1274171 -4.1274333 -4.0964684 -4.0539083 -4.0383153 -4.0533147][-4.2140894 -4.2035418 -4.1890798 -4.1657104 -4.1353464 -4.1167722 -4.1154904 -4.1246076 -4.1365561 -4.1485724 -4.1493411 -4.1240921 -4.0920091 -4.0824962 -4.0893655][-4.232038 -4.225101 -4.2122545 -4.1949058 -4.1745963 -4.1609745 -4.1544237 -4.1554232 -4.1612248 -4.1679459 -4.1648383 -4.1443548 -4.1192045 -4.1077194 -4.1078768][-4.2295828 -4.2249346 -4.2167773 -4.2071571 -4.1974516 -4.1907978 -4.1865044 -4.1844392 -4.1854944 -4.1836514 -4.1734524 -4.1551623 -4.137876 -4.1259127 -4.1194534]]...]
INFO - root - 2017-12-06 00:31:50.864468: step 59010, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 63h:54m:37s remains)
INFO - root - 2017-12-06 00:31:59.534330: step 59020, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 67h:18m:24s remains)
INFO - root - 2017-12-06 00:32:08.115691: step 59030, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 66h:32m:17s remains)
INFO - root - 2017-12-06 00:32:16.580759: step 59040, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 66h:36m:17s remains)
INFO - root - 2017-12-06 00:32:25.131737: step 59050, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 64h:15m:17s remains)
INFO - root - 2017-12-06 00:32:33.716263: step 59060, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 64h:17m:01s remains)
INFO - root - 2017-12-06 00:32:42.085132: step 59070, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 62h:56m:45s remains)
INFO - root - 2017-12-06 00:32:50.634859: step 59080, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 63h:59m:42s remains)
INFO - root - 2017-12-06 00:32:59.200358: step 59090, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 66h:03m:06s remains)
INFO - root - 2017-12-06 00:33:07.730967: step 59100, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.830 sec/batch; 63h:02m:55s remains)
2017-12-06 00:33:08.537380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3326445 -4.3288317 -4.3268838 -4.3235521 -4.3185611 -4.3145237 -4.3146076 -4.3187647 -4.3245368 -4.3305717 -4.3373051 -4.3428335 -4.3456073 -4.3464494 -4.3474612][-4.3251648 -4.3185968 -4.3138275 -4.3062658 -4.2950468 -4.28477 -4.2807627 -4.2838182 -4.2940583 -4.3085175 -4.3253384 -4.3394108 -4.3463445 -4.3479462 -4.3482928][-4.3137622 -4.3006949 -4.2906532 -4.2770839 -4.25653 -4.233602 -4.2178164 -4.2173352 -4.2351108 -4.2645421 -4.2976975 -4.3263416 -4.3428011 -4.3491206 -4.3501525][-4.2982769 -4.2811856 -4.2667112 -4.2448435 -4.2086897 -4.1626244 -4.1240921 -4.1134725 -4.1404529 -4.1898832 -4.2453718 -4.2941194 -4.325738 -4.3419733 -4.3479004][-4.2775707 -4.2626338 -4.2439637 -4.2126722 -4.1563787 -4.0771122 -3.9990182 -3.9675078 -4.0108051 -4.091794 -4.1756644 -4.2463956 -4.2942777 -4.3235183 -4.3382788][-4.2532582 -4.2468529 -4.2298822 -4.1916857 -4.118494 -4.0050206 -3.8765979 -3.8184984 -3.8849196 -4.0025826 -4.1130538 -4.2018533 -4.2628889 -4.3028312 -4.3260312][-4.2322168 -4.2382545 -4.2285705 -4.1924696 -4.115818 -3.9925497 -3.848999 -3.780308 -3.8514462 -3.9734812 -4.0871468 -4.180687 -4.2474484 -4.2925806 -4.3180289][-4.2191014 -4.2395439 -4.2437315 -4.22005 -4.1564813 -4.0587754 -3.9524341 -3.9003663 -3.9424391 -4.02425 -4.1136718 -4.1946964 -4.2542496 -4.2931628 -4.3125648][-4.21757 -4.2468958 -4.2631645 -4.2543449 -4.2104373 -4.1457334 -4.0830731 -4.0509162 -4.069756 -4.1152229 -4.1738772 -4.2328949 -4.2766485 -4.30219 -4.310699][-4.2438169 -4.2685452 -4.2857652 -4.2866154 -4.2614012 -4.2236409 -4.187964 -4.1663375 -4.171916 -4.1957779 -4.2307405 -4.2689557 -4.297534 -4.3113108 -4.3104033][-4.2772284 -4.294672 -4.3092518 -4.3142838 -4.3033247 -4.2851462 -4.2658734 -4.2513928 -4.249979 -4.2589326 -4.2738767 -4.2932196 -4.3082585 -4.3130808 -4.3061662][-4.2954273 -4.3135018 -4.3284745 -4.3358879 -4.3330765 -4.3258572 -4.3152514 -4.3023667 -4.2932215 -4.2887392 -4.2885208 -4.294548 -4.3011465 -4.3021493 -4.2946663][-4.2907043 -4.3134503 -4.3307052 -4.3372941 -4.3379 -4.3362131 -4.3301821 -4.3180757 -4.3040943 -4.2918859 -4.2841792 -4.2832317 -4.2849011 -4.2847781 -4.2806621][-4.270247 -4.297318 -4.3167334 -4.3232741 -4.3247566 -4.3243723 -4.3191743 -4.30863 -4.2967381 -4.2849426 -4.2757363 -4.2705069 -4.2672319 -4.265193 -4.265151][-4.2639642 -4.288321 -4.3037052 -4.3063645 -4.3035951 -4.2997303 -4.2946548 -4.2891641 -4.2832565 -4.2756944 -4.2661877 -4.257236 -4.2502179 -4.2497969 -4.256537]]...]
INFO - root - 2017-12-06 00:33:16.966023: step 59110, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 63h:11m:48s remains)
INFO - root - 2017-12-06 00:33:25.583697: step 59120, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.861 sec/batch; 65h:21m:41s remains)
INFO - root - 2017-12-06 00:33:34.208672: step 59130, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 64h:51m:50s remains)
INFO - root - 2017-12-06 00:33:42.713024: step 59140, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 63h:55m:33s remains)
INFO - root - 2017-12-06 00:33:51.252689: step 59150, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 63h:47m:13s remains)
INFO - root - 2017-12-06 00:33:59.831283: step 59160, loss = 2.10, batch loss = 2.04 (10.5 examples/sec; 0.765 sec/batch; 58h:04m:54s remains)
INFO - root - 2017-12-06 00:34:08.176826: step 59170, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 63h:22m:16s remains)
INFO - root - 2017-12-06 00:34:16.843196: step 59180, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 64h:03m:24s remains)
INFO - root - 2017-12-06 00:34:25.422747: step 59190, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 62h:39m:38s remains)
INFO - root - 2017-12-06 00:34:33.965261: step 59200, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 65h:13m:23s remains)
2017-12-06 00:34:34.772679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2983127 -4.2981086 -4.2839184 -4.2650661 -4.2461572 -4.2362642 -4.2468491 -4.2641926 -4.2808571 -4.2946291 -4.3038721 -4.3032684 -4.2905254 -4.2695117 -4.243938][-4.306973 -4.3042932 -4.28819 -4.2678547 -4.2450767 -4.2303486 -4.2387986 -4.2556958 -4.274415 -4.2928834 -4.3057809 -4.30632 -4.2908163 -4.2622042 -4.2272487][-4.3078866 -4.3079939 -4.298533 -4.2822962 -4.2566986 -4.234024 -4.2339067 -4.2438736 -4.26369 -4.2888746 -4.3053403 -4.3067088 -4.2905407 -4.2569108 -4.2137146][-4.3027616 -4.3136687 -4.3168106 -4.3063817 -4.2785325 -4.2466044 -4.2332363 -4.2299485 -4.2500358 -4.2832508 -4.3049908 -4.31041 -4.2991323 -4.2656608 -4.2183504][-4.2900233 -4.3160615 -4.3305554 -4.3242073 -4.2939968 -4.2532053 -4.2227659 -4.2031198 -4.2226143 -4.2652841 -4.2967529 -4.3120141 -4.3104973 -4.2832294 -4.2391143][-4.2752633 -4.3123116 -4.3321867 -4.3246045 -4.2879815 -4.2368422 -4.1882176 -4.1529021 -4.1742334 -4.2299619 -4.2748632 -4.3012691 -4.3099008 -4.2924395 -4.2570925][-4.2592316 -4.300611 -4.3202214 -4.3077378 -4.2625632 -4.1987739 -4.1305537 -4.0826859 -4.1140032 -4.1874976 -4.248445 -4.2870955 -4.3042235 -4.295989 -4.2699223][-4.2573652 -4.2947097 -4.3095913 -4.2916307 -4.23674 -4.162004 -4.0857348 -4.040288 -4.0869517 -4.1722779 -4.2407389 -4.2838964 -4.3027444 -4.2977762 -4.275322][-4.2744284 -4.3040595 -4.3135786 -4.2921953 -4.2378883 -4.17154 -4.1132317 -4.0880432 -4.1315446 -4.1994243 -4.255373 -4.291997 -4.308094 -4.3029647 -4.2808471][-4.2875404 -4.312037 -4.3203707 -4.3016829 -4.25642 -4.20505 -4.1687875 -4.1575251 -4.1859913 -4.2282219 -4.2665453 -4.2945914 -4.3079548 -4.303762 -4.2840295][-4.2880888 -4.3096581 -4.3187394 -4.3058009 -4.2725434 -4.2356644 -4.2133994 -4.207582 -4.2227979 -4.245749 -4.270627 -4.28906 -4.2992105 -4.2965612 -4.2789888][-4.2863216 -4.304956 -4.3134046 -4.3051734 -4.2817316 -4.2546911 -4.238842 -4.2344928 -4.2408123 -4.2499995 -4.2607079 -4.26977 -4.2801771 -4.2813168 -4.2682719][-4.28929 -4.3034229 -4.307272 -4.2991867 -4.2816081 -4.26137 -4.2479305 -4.2433243 -4.244895 -4.2446809 -4.2454109 -4.2486877 -4.2595372 -4.2663422 -4.2608223][-4.2928767 -4.3024049 -4.3016992 -4.293395 -4.2790627 -4.2607312 -4.2454853 -4.2373233 -4.2354822 -4.2331095 -4.2316866 -4.2342591 -4.2470045 -4.2588677 -4.2576838][-4.28954 -4.2929454 -4.287951 -4.2775145 -4.2614665 -4.2409024 -4.2246304 -4.2159748 -4.2151608 -4.2177653 -4.2221751 -4.2298827 -4.2436604 -4.2534089 -4.2513819]]...]
INFO - root - 2017-12-06 00:34:43.272590: step 59210, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.810 sec/batch; 61h:27m:24s remains)
INFO - root - 2017-12-06 00:34:51.881217: step 59220, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 65h:05m:13s remains)
INFO - root - 2017-12-06 00:35:00.435504: step 59230, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 65h:38m:49s remains)
INFO - root - 2017-12-06 00:35:08.871338: step 59240, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.819 sec/batch; 62h:11m:12s remains)
INFO - root - 2017-12-06 00:35:17.408812: step 59250, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.875 sec/batch; 66h:23m:03s remains)
INFO - root - 2017-12-06 00:35:25.823663: step 59260, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.818 sec/batch; 62h:07m:11s remains)
INFO - root - 2017-12-06 00:35:34.334160: step 59270, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 64h:23m:47s remains)
INFO - root - 2017-12-06 00:35:42.902108: step 59280, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 65h:35m:39s remains)
INFO - root - 2017-12-06 00:35:51.451405: step 59290, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.810 sec/batch; 61h:26m:05s remains)
INFO - root - 2017-12-06 00:35:59.991216: step 59300, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 63h:05m:44s remains)
2017-12-06 00:36:00.747534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1915126 -4.1695795 -4.1549826 -4.1588812 -4.1906519 -4.2354064 -4.2748456 -4.2965121 -4.297698 -4.2810712 -4.2627616 -4.2524652 -4.2444859 -4.2394409 -4.249125][-4.1981988 -4.1866331 -4.1769986 -4.1777754 -4.2011924 -4.2355857 -4.2643275 -4.2728639 -4.257441 -4.2261534 -4.1989193 -4.1839571 -4.176609 -4.176898 -4.1993513][-4.2114758 -4.2054739 -4.1949039 -4.1893153 -4.1999564 -4.2162704 -4.2246218 -4.2088275 -4.1719704 -4.13274 -4.1062193 -4.0978045 -4.1018705 -4.1221991 -4.1669483][-4.2184076 -4.2156444 -4.2024426 -4.1890755 -4.1830659 -4.1741877 -4.1530247 -4.105443 -4.0544372 -4.0247068 -4.0136685 -4.0230308 -4.050498 -4.1002164 -4.16576][-4.2239814 -4.2229209 -4.2074032 -4.1821136 -4.1485467 -4.1019106 -4.0373054 -3.95602 -3.9157536 -3.927191 -3.9536526 -3.9913118 -4.045743 -4.1186767 -4.1949706][-4.2194982 -4.2130065 -4.1881814 -4.14473 -4.078846 -3.9891784 -3.8754368 -3.7731757 -3.7804341 -3.8649771 -3.9439888 -4.0129972 -4.0810475 -4.1594496 -4.231101][-4.1914835 -4.1721396 -4.1360264 -4.0790467 -3.9970589 -3.88194 -3.7366412 -3.6386278 -3.7163124 -3.8689361 -3.9822619 -4.0626392 -4.1329861 -4.2061968 -4.258101][-4.1497927 -4.1185713 -4.0759516 -4.0207729 -3.9534397 -3.8543642 -3.7352204 -3.6828327 -3.785835 -3.9350042 -4.0422921 -4.1195359 -4.1891494 -4.2467003 -4.2745104][-4.1019907 -4.0676279 -4.0296779 -3.9951246 -3.9631121 -3.9080069 -3.8453441 -3.8370516 -3.9207449 -4.0276818 -4.1076708 -4.1723685 -4.2332392 -4.2722573 -4.2811737][-4.0715985 -4.0479922 -4.0258656 -4.0190477 -4.0214424 -4.0082355 -3.9939876 -4.0125122 -4.0735908 -4.1389971 -4.1845493 -4.228826 -4.2721834 -4.2926989 -4.288393][-4.0888972 -4.0787573 -4.0704832 -4.0831227 -4.106142 -4.1201596 -4.1305418 -4.15577 -4.1975374 -4.2325869 -4.2561531 -4.2842827 -4.3081622 -4.3104024 -4.2942538][-4.146893 -4.1486506 -4.1495724 -4.1669812 -4.1953688 -4.2203646 -4.2378945 -4.2593336 -4.2844343 -4.3032484 -4.3160553 -4.3292403 -4.3337331 -4.3199153 -4.2923126][-4.2250619 -4.2287769 -4.2269135 -4.2370377 -4.2588778 -4.2833281 -4.3012128 -4.3184581 -4.3322039 -4.3401895 -4.3424554 -4.3389511 -4.3240561 -4.2973557 -4.26435][-4.2876682 -4.2883005 -4.2778587 -4.2745872 -4.2840095 -4.3022656 -4.3197193 -4.3346944 -4.3423991 -4.3427782 -4.3351846 -4.316328 -4.2838912 -4.2483296 -4.2155194][-4.3154225 -4.3134723 -4.2976031 -4.2858939 -4.2844968 -4.2943592 -4.3070822 -4.3161955 -4.3180881 -4.3114138 -4.2932663 -4.2644868 -4.2285414 -4.19585 -4.1715736]]...]
INFO - root - 2017-12-06 00:36:09.288067: step 59310, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 66h:17m:28s remains)
INFO - root - 2017-12-06 00:36:17.843373: step 59320, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 66h:10m:30s remains)
INFO - root - 2017-12-06 00:36:26.436134: step 59330, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 62h:30m:05s remains)
INFO - root - 2017-12-06 00:36:35.068795: step 59340, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 65h:37m:26s remains)
INFO - root - 2017-12-06 00:36:43.596126: step 59350, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 62h:38m:17s remains)
INFO - root - 2017-12-06 00:36:52.125341: step 59360, loss = 2.11, batch loss = 2.05 (9.6 examples/sec; 0.836 sec/batch; 63h:26m:45s remains)
INFO - root - 2017-12-06 00:37:00.509424: step 59370, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 65h:11m:37s remains)
INFO - root - 2017-12-06 00:37:08.858358: step 59380, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.847 sec/batch; 64h:13m:26s remains)
INFO - root - 2017-12-06 00:37:17.555355: step 59390, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 65h:20m:07s remains)
INFO - root - 2017-12-06 00:37:26.081312: step 59400, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 64h:32m:14s remains)
2017-12-06 00:37:26.944724: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2506719 -4.2499723 -4.2494888 -4.2496276 -4.249949 -4.2499332 -4.250123 -4.2508626 -4.2512512 -4.2499604 -4.2479353 -4.247324 -4.2480364 -4.2484441 -4.2480822][-4.2476249 -4.2482696 -4.2471581 -4.2460718 -4.2452812 -4.244813 -4.24555 -4.2470565 -4.2480645 -4.2468991 -4.2450624 -4.2448277 -4.2455664 -4.2455292 -4.244432][-4.244833 -4.2471318 -4.2449231 -4.2419605 -4.2397628 -4.238862 -4.2400942 -4.2424927 -4.2444787 -4.243834 -4.2419338 -4.2409124 -4.2405443 -4.2389708 -4.2358608][-4.2330542 -4.2365303 -4.2333584 -4.2290974 -4.2263308 -4.2257352 -4.2283549 -4.2330565 -4.2376494 -4.238625 -4.2371607 -4.235745 -4.2350531 -4.2324848 -4.2275839][-4.2214289 -4.2269197 -4.2226539 -4.2170515 -4.2133818 -4.21249 -4.2158842 -4.2240949 -4.2325358 -4.2358975 -4.2344 -4.2319431 -4.2313175 -4.2290053 -4.2244368][-4.2134914 -4.2193379 -4.2134705 -4.2066994 -4.20226 -4.1994534 -4.2018795 -4.2144938 -4.2281804 -4.2338557 -4.2313824 -4.2284656 -4.2293367 -4.2285013 -4.2246938][-4.1959558 -4.2000351 -4.1933055 -4.1846619 -4.1775317 -4.1710854 -4.1718526 -4.1884851 -4.2058434 -4.2126966 -4.2098274 -4.2081952 -4.2122874 -4.2136889 -4.2117157][-4.1739974 -4.1741405 -4.16616 -4.1556859 -4.1452723 -4.1342158 -4.131278 -4.1456757 -4.1597919 -4.1638575 -4.1599503 -4.1605725 -4.1688242 -4.1750455 -4.1792617][-4.1897006 -4.1886535 -4.1801925 -4.1691651 -4.1581159 -4.1464143 -4.1399364 -4.1453443 -4.1494532 -4.14622 -4.1383152 -4.13644 -4.1435742 -4.1510344 -4.1594][-4.2293472 -4.2290063 -4.2216296 -4.2127724 -4.2045627 -4.1968536 -4.1931357 -4.1952462 -4.1956258 -4.1908803 -4.1822128 -4.1760979 -4.1762834 -4.1775856 -4.1812615][-4.2595363 -4.2592959 -4.2538438 -4.2485609 -4.244606 -4.2423763 -4.2434659 -4.2465892 -4.2490125 -4.248179 -4.2435484 -4.2384515 -4.2355151 -4.2316403 -4.2283974][-4.2738557 -4.2739754 -4.2717152 -4.2702804 -4.2697067 -4.2707615 -4.2740555 -4.2774615 -4.2798471 -4.2802782 -4.2784872 -4.2754979 -4.2722492 -4.2668943 -4.2619467][-4.2770638 -4.2778726 -4.2775822 -4.2787714 -4.2802358 -4.2819581 -4.2844968 -4.2866688 -4.288157 -4.2883134 -4.2877073 -4.2866311 -4.2847958 -4.2815213 -4.278831][-4.2811351 -4.2824121 -4.2820849 -4.2831645 -4.2840281 -4.2838836 -4.2835841 -4.2837749 -4.2842479 -4.2840414 -4.2841382 -4.2843971 -4.2839932 -4.2830853 -4.2832117][-4.2858167 -4.2872291 -4.2866635 -4.2873158 -4.2873049 -4.285707 -4.2841272 -4.2837367 -4.2838054 -4.2832589 -4.2829504 -4.2824612 -4.2814231 -4.2806525 -4.2814331]]...]
INFO - root - 2017-12-06 00:37:35.372547: step 59410, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 63h:14m:56s remains)
INFO - root - 2017-12-06 00:37:43.873404: step 59420, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 64h:30m:00s remains)
INFO - root - 2017-12-06 00:37:52.514989: step 59430, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 65h:28m:14s remains)
INFO - root - 2017-12-06 00:38:00.955340: step 59440, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 62h:58m:06s remains)
INFO - root - 2017-12-06 00:38:09.464161: step 59450, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 64h:17m:01s remains)
INFO - root - 2017-12-06 00:38:18.115889: step 59460, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 64h:47m:05s remains)
INFO - root - 2017-12-06 00:38:26.575200: step 59470, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 63h:54m:22s remains)
INFO - root - 2017-12-06 00:38:34.927421: step 59480, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 64h:51m:12s remains)
INFO - root - 2017-12-06 00:38:43.484326: step 59490, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 63h:44m:13s remains)
INFO - root - 2017-12-06 00:38:52.016430: step 59500, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 64h:26m:10s remains)
2017-12-06 00:38:52.754236: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2514858 -4.2531753 -4.2491755 -4.242806 -4.2311854 -4.2207084 -4.2218475 -4.23011 -4.2386146 -4.2489619 -4.2581482 -4.2582188 -4.2450323 -4.2374449 -4.2449551][-4.2332706 -4.2273517 -4.2152057 -4.2038088 -4.1917553 -4.1820107 -4.1852069 -4.2012415 -4.2224221 -4.244174 -4.2591348 -4.2604656 -4.2515869 -4.2489853 -4.2547336][-4.2167583 -4.2119842 -4.1955914 -4.1778393 -4.1628742 -4.1503057 -4.1519818 -4.1722193 -4.2038865 -4.2379756 -4.2566485 -4.2575722 -4.2564473 -4.2631946 -4.2705564][-4.2029233 -4.2015953 -4.183105 -4.1587 -4.1356449 -4.1163611 -4.113843 -4.1334267 -4.1694193 -4.2115755 -4.2397881 -4.24682 -4.25388 -4.270504 -4.2831764][-4.1715651 -4.1749849 -4.1620264 -4.1419916 -4.1207089 -4.1014414 -4.09689 -4.1125851 -4.1450224 -4.1875839 -4.2222347 -4.23674 -4.2511811 -4.275064 -4.2929592][-4.127121 -4.1404991 -4.1424623 -4.1379886 -4.1301203 -4.1216373 -4.1186924 -4.1272087 -4.1512532 -4.1862864 -4.2179189 -4.2369823 -4.2557325 -4.282578 -4.3008542][-4.1128316 -4.1393471 -4.1579018 -4.1697621 -4.1751871 -4.1713185 -4.1637554 -4.1634183 -4.1773481 -4.2040672 -4.2299309 -4.2492447 -4.2681255 -4.2934151 -4.308259][-4.1493287 -4.1903534 -4.2122526 -4.219696 -4.2180157 -4.205658 -4.1860294 -4.1723804 -4.1776714 -4.2010179 -4.2291965 -4.2524614 -4.2722831 -4.29775 -4.3119631][-4.1906309 -4.2291827 -4.2422013 -4.2347293 -4.2161684 -4.1909475 -4.161592 -4.141387 -4.1487055 -4.1776285 -4.2160959 -4.2481866 -4.2727661 -4.3010263 -4.3149495][-4.180254 -4.2142816 -4.2215443 -4.2050924 -4.1735692 -4.1398888 -4.1058917 -4.0867834 -4.1030765 -4.1425118 -4.1929049 -4.235384 -4.2691569 -4.30229 -4.3172736][-4.1595411 -4.1841574 -4.1850657 -4.1627946 -4.1265535 -4.0912519 -4.0589519 -4.0467625 -4.073339 -4.119513 -4.1760883 -4.2245049 -4.2666173 -4.3037968 -4.3195281][-4.1526418 -4.1691785 -4.1660886 -4.1428618 -4.1095123 -4.0776124 -4.0497489 -4.0425239 -4.0717607 -4.1179104 -4.1736703 -4.2221789 -4.2657189 -4.3033504 -4.3199353][-4.1682844 -4.182292 -4.1754389 -4.1529303 -4.1254067 -4.0996675 -4.0789495 -4.0759473 -4.1017113 -4.140234 -4.1854978 -4.2262049 -4.2634425 -4.2970042 -4.3146954][-4.1994081 -4.2071552 -4.196795 -4.1761231 -4.1551232 -4.136117 -4.1238022 -4.1253047 -4.14618 -4.1733274 -4.2032609 -4.2303371 -4.2576771 -4.2853389 -4.3040667][-4.231811 -4.2352691 -4.2265115 -4.211863 -4.1985478 -4.1872764 -4.1814847 -4.1849308 -4.1972194 -4.2092867 -4.22216 -4.2356892 -4.2526131 -4.2745419 -4.2933164]]...]
INFO - root - 2017-12-06 00:39:01.315933: step 59510, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.863 sec/batch; 65h:28m:04s remains)
INFO - root - 2017-12-06 00:39:09.800095: step 59520, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 64h:49m:34s remains)
INFO - root - 2017-12-06 00:39:18.388696: step 59530, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 65h:11m:14s remains)
INFO - root - 2017-12-06 00:39:26.892925: step 59540, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 63h:40m:24s remains)
INFO - root - 2017-12-06 00:39:35.396905: step 59550, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 64h:39m:31s remains)
INFO - root - 2017-12-06 00:39:44.020146: step 59560, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 65h:24m:51s remains)
INFO - root - 2017-12-06 00:39:52.525280: step 59570, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 67h:48m:58s remains)
INFO - root - 2017-12-06 00:40:01.182319: step 59580, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.872 sec/batch; 66h:07m:39s remains)
INFO - root - 2017-12-06 00:40:09.510510: step 59590, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 63h:50m:18s remains)
INFO - root - 2017-12-06 00:40:18.057568: step 59600, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 64h:01m:00s remains)
2017-12-06 00:40:18.852361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3253078 -4.3313313 -4.3337522 -4.3327794 -4.3318405 -4.333303 -4.3322835 -4.3284955 -4.3248119 -4.3235478 -4.3207045 -4.3195 -4.3216991 -4.3254156 -4.3311076][-4.3107128 -4.314074 -4.3135886 -4.3117557 -4.3133664 -4.3179169 -4.3175507 -4.31281 -4.3096476 -4.3078289 -4.3022642 -4.2984824 -4.29999 -4.305944 -4.3171339][-4.2829967 -4.2801337 -4.2738595 -4.2698574 -4.2739778 -4.2847729 -4.2851057 -4.2769909 -4.2766647 -4.2781186 -4.2722049 -4.26926 -4.2734494 -4.2847395 -4.3034244][-4.239852 -4.2325859 -4.226687 -4.226964 -4.2396841 -4.2567596 -4.2551274 -4.2373219 -4.2372527 -4.2426014 -4.2380004 -4.2372513 -4.249505 -4.2700095 -4.2965164][-4.1908336 -4.1829181 -4.1851764 -4.19501 -4.2145362 -4.2336836 -4.2266922 -4.1978912 -4.1948886 -4.206634 -4.2037721 -4.2073312 -4.231421 -4.2648439 -4.2985225][-4.1380973 -4.1251187 -4.1347079 -4.1547642 -4.1779337 -4.194334 -4.1736484 -4.1292896 -4.1237378 -4.1519651 -4.1635962 -4.1776633 -4.2142119 -4.2608891 -4.3016105][-4.0972433 -4.072372 -4.0750723 -4.092804 -4.1126657 -4.116662 -4.072185 -4.0051479 -4.0074215 -4.0739822 -4.118288 -4.152689 -4.1998487 -4.25328 -4.2985506][-4.0907531 -4.0592146 -4.0455656 -4.0453043 -4.0482664 -4.0302768 -3.9465139 -3.8440678 -3.8645961 -3.9810262 -4.0639634 -4.118144 -4.1718864 -4.2310772 -4.2854047][-4.1324267 -4.1026034 -4.0788746 -4.0603881 -4.0450783 -4.0165462 -3.9220159 -3.8085463 -3.8421912 -3.9720757 -4.0616856 -4.1170239 -4.163445 -4.2180724 -4.2745795][-4.19148 -4.1705246 -4.1530662 -4.1356344 -4.1179209 -4.0998216 -4.0428848 -3.9735968 -3.9951518 -4.07864 -4.1333122 -4.165844 -4.1912847 -4.2298517 -4.2781219][-4.2246017 -4.2125468 -4.2019076 -4.1890969 -4.1771741 -4.1726213 -4.1502862 -4.1153879 -4.1237283 -4.1616545 -4.1867423 -4.2011318 -4.2163634 -4.2471418 -4.2875333][-4.2426167 -4.2333946 -4.2249703 -4.2099161 -4.2044196 -4.21087 -4.2040205 -4.1853638 -4.1864605 -4.2036848 -4.2147155 -4.2202992 -4.2330122 -4.2625422 -4.2991405][-4.2595286 -4.2451611 -4.2339582 -4.219841 -4.217432 -4.2262521 -4.2220917 -4.2113466 -4.2136836 -4.2266278 -4.23505 -4.2406387 -4.2525058 -4.2815385 -4.3145332][-4.27867 -4.2645955 -4.2532854 -4.2447743 -4.2444329 -4.2468672 -4.2413249 -4.2338691 -4.2375631 -4.2495008 -4.2598138 -4.2677112 -4.2778735 -4.3012094 -4.326241][-4.3014064 -4.291872 -4.2818847 -4.276917 -4.2763562 -4.2762628 -4.2718663 -4.2649665 -4.2674718 -4.2768188 -4.2866073 -4.2951431 -4.3037591 -4.3199873 -4.3357458]]...]
INFO - root - 2017-12-06 00:40:27.375566: step 59610, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 63h:39m:40s remains)
INFO - root - 2017-12-06 00:40:35.905917: step 59620, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 65h:09m:37s remains)
INFO - root - 2017-12-06 00:40:44.394468: step 59630, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 64h:29m:57s remains)
INFO - root - 2017-12-06 00:40:52.938778: step 59640, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 65h:42m:31s remains)
INFO - root - 2017-12-06 00:41:01.554350: step 59650, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 67h:53m:52s remains)
INFO - root - 2017-12-06 00:41:10.096007: step 59660, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 64h:09m:14s remains)
INFO - root - 2017-12-06 00:41:18.498416: step 59670, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 63h:55m:27s remains)
INFO - root - 2017-12-06 00:41:26.946575: step 59680, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 64h:56m:03s remains)
INFO - root - 2017-12-06 00:41:35.520036: step 59690, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 65h:05m:08s remains)
INFO - root - 2017-12-06 00:41:43.882731: step 59700, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 65h:19m:33s remains)
2017-12-06 00:41:44.657429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2346892 -4.2204452 -4.1983528 -4.1704211 -4.1493716 -4.14643 -4.1653166 -4.1860666 -4.190033 -4.1791463 -4.1498489 -4.1223445 -4.1119256 -4.1222067 -4.1316152][-4.2183723 -4.2022848 -4.1786871 -4.1513176 -4.1303387 -4.1193228 -4.1301031 -4.15278 -4.1714635 -4.1745462 -4.1482167 -4.11606 -4.0961485 -4.0961366 -4.0981474][-4.1994519 -4.1824727 -4.1604223 -4.1327858 -4.1070471 -4.0804858 -4.0770593 -4.105763 -4.1454387 -4.1691246 -4.1518059 -4.1151233 -4.0824175 -4.0682869 -4.0597134][-4.1893983 -4.1746206 -4.1565075 -4.12783 -4.0922022 -4.0453234 -4.0275297 -4.0650573 -4.1217055 -4.162828 -4.1599526 -4.1255031 -4.0829864 -4.0535197 -4.0367703][-4.1877928 -4.1764078 -4.1631947 -4.1344419 -4.0827584 -4.0102625 -3.9788909 -4.0234828 -4.0969911 -4.1496544 -4.1591129 -4.1331849 -4.0891867 -4.0472622 -4.0241981][-4.19863 -4.1853104 -4.17256 -4.1407018 -4.0685554 -3.9698684 -3.92664 -3.9807911 -4.0706878 -4.1324139 -4.1515517 -4.1351676 -4.0959406 -4.0457582 -4.0138645][-4.22079 -4.2021632 -4.1818748 -4.1443825 -4.0537624 -3.9302242 -3.874665 -3.935473 -4.0368495 -4.1075425 -4.1350126 -4.1280179 -4.0956044 -4.0430808 -4.0066724][-4.2522049 -4.23065 -4.2029676 -4.1597061 -4.0558219 -3.9158788 -3.8469915 -3.8955822 -3.9968138 -4.0759535 -4.1118031 -4.1112576 -4.0850296 -4.0372672 -4.0101838][-4.2874017 -4.2663026 -4.23296 -4.1881061 -4.0848589 -3.9453351 -3.861938 -3.8791935 -3.9662113 -4.0493832 -4.0871768 -4.084559 -4.0627732 -4.029623 -4.028338][-4.30865 -4.2942939 -4.2629089 -4.2235847 -4.13123 -4.0038362 -3.9145026 -3.9053962 -3.974673 -4.0511789 -4.07999 -4.0631676 -4.0409565 -4.0309663 -4.0590382][-4.303246 -4.2991652 -4.278728 -4.2528329 -4.180398 -4.0747228 -3.9916492 -3.9742196 -4.0261602 -4.0839586 -4.0954962 -4.0637932 -4.0436573 -4.0554085 -4.1010847][-4.2779408 -4.2823906 -4.2773924 -4.2676597 -4.2187371 -4.1398039 -4.0726686 -4.0555434 -4.0924945 -4.1295023 -4.123528 -4.0871892 -4.0718713 -4.0937605 -4.1421566][-4.2442 -4.2533031 -4.2607651 -4.2682176 -4.2439785 -4.190589 -4.1397262 -4.1259303 -4.1532168 -4.1753263 -4.1583905 -4.1205292 -4.10664 -4.1249695 -4.1651578][-4.2133317 -4.2229357 -4.2354336 -4.2529349 -4.2482786 -4.2181482 -4.1822605 -4.1730909 -4.1932778 -4.2081513 -4.1903558 -4.1534414 -4.1380019 -4.1481614 -4.1744366][-4.1953225 -4.2025061 -4.2146554 -4.2329154 -4.2383089 -4.2259126 -4.2049665 -4.1999555 -4.2146468 -4.2272329 -4.2146354 -4.1848621 -4.1680903 -4.1691661 -4.1829524]]...]
INFO - root - 2017-12-06 00:41:53.228685: step 59710, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 65h:58m:07s remains)
INFO - root - 2017-12-06 00:42:01.754021: step 59720, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 65h:03m:04s remains)
INFO - root - 2017-12-06 00:42:10.218738: step 59730, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 65h:01m:27s remains)
INFO - root - 2017-12-06 00:42:18.790377: step 59740, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 64h:38m:49s remains)
INFO - root - 2017-12-06 00:42:27.354975: step 59750, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 64h:05m:27s remains)
INFO - root - 2017-12-06 00:42:35.866056: step 59760, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 63h:51m:43s remains)
INFO - root - 2017-12-06 00:42:44.126625: step 59770, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 61h:52m:17s remains)
INFO - root - 2017-12-06 00:42:52.494745: step 59780, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 63h:41m:09s remains)
INFO - root - 2017-12-06 00:43:00.874550: step 59790, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.837 sec/batch; 63h:22m:08s remains)
INFO - root - 2017-12-06 00:43:09.301248: step 59800, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 0.765 sec/batch; 57h:58m:45s remains)
2017-12-06 00:43:10.084804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2747059 -4.2757869 -4.2834806 -4.293736 -4.3028374 -4.3075438 -4.3073721 -4.303391 -4.3015046 -4.3047438 -4.3089147 -4.3159108 -4.3251543 -4.334393 -4.3409944][-4.236515 -4.2362342 -4.2473712 -4.2631822 -4.2774496 -4.2844248 -4.2809348 -4.2707405 -4.2664351 -4.272872 -4.2800303 -4.28995 -4.3049178 -4.319253 -4.3304334][-4.2061005 -4.2037673 -4.2145824 -4.2302856 -4.2465229 -4.2537785 -4.245831 -4.2309189 -4.2313271 -4.2451878 -4.2551522 -4.2662826 -4.2834044 -4.3024325 -4.3176627][-4.1828928 -4.175364 -4.1814942 -4.1917334 -4.2060275 -4.211709 -4.2007575 -4.1834207 -4.1909065 -4.2151961 -4.2334714 -4.2494903 -4.2666674 -4.2876072 -4.3051329][-4.159039 -4.145184 -4.142808 -4.1484551 -4.1622877 -4.1662707 -4.151525 -4.1336432 -4.1551027 -4.1962385 -4.224906 -4.2422261 -4.2551336 -4.2736716 -4.2921915][-4.1431956 -4.1227226 -4.1120625 -4.1109848 -4.1189885 -4.1184034 -4.093442 -4.0670443 -4.1028137 -4.166976 -4.20521 -4.2235193 -4.2380238 -4.2580881 -4.2788043][-4.1377473 -4.1105371 -4.0928273 -4.0848784 -4.0797682 -4.0637345 -4.0064254 -3.9486389 -4.0020275 -4.1022835 -4.158164 -4.1844568 -4.2074213 -4.2341666 -4.2620893][-4.127037 -4.0931931 -4.0678544 -4.0518122 -4.0328488 -3.9899526 -3.8857892 -3.7873032 -3.8779926 -4.0255857 -4.1066747 -4.1428766 -4.1721568 -4.2079883 -4.2432218][-4.1188025 -4.0805988 -4.05584 -4.0412359 -4.0223312 -3.9724967 -3.8739343 -3.7994285 -3.8904803 -4.0284748 -4.1046076 -4.1374702 -4.1602054 -4.1930094 -4.2292976][-4.1524615 -4.1207514 -4.1038938 -4.098248 -4.0877333 -4.0528293 -3.9926968 -3.9548454 -4.0079885 -4.0941987 -4.1398745 -4.1560245 -4.170743 -4.1993132 -4.2301311][-4.2141452 -4.1913886 -4.1801682 -4.1753864 -4.163321 -4.1341553 -4.0934391 -4.0679469 -4.0955243 -4.1478648 -4.1733422 -4.178185 -4.1876416 -4.2158003 -4.2431707][-4.2696171 -4.2520027 -4.2435722 -4.2398038 -4.226613 -4.2011528 -4.1719737 -4.1531043 -4.1684556 -4.2009 -4.2147088 -4.2116852 -4.2204647 -4.2458291 -4.2688789][-4.3060193 -4.2945089 -4.2904797 -4.287828 -4.2784619 -4.2617178 -4.2427917 -4.2315035 -4.2398162 -4.2571793 -4.2643719 -4.260746 -4.2670836 -4.2862968 -4.3042293][-4.3306217 -4.3262482 -4.3254919 -4.3233681 -4.3173923 -4.3082066 -4.2993107 -4.2952404 -4.3006406 -4.3103623 -4.3133512 -4.3114061 -4.3154287 -4.3272524 -4.3374014][-4.3434458 -4.3404126 -4.3374319 -4.3345261 -4.3321185 -4.3278141 -4.3241062 -4.3238406 -4.3266234 -4.3313494 -4.3331141 -4.3346238 -4.3390069 -4.3474789 -4.3541007]]...]
INFO - root - 2017-12-06 00:43:18.408956: step 59810, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.818 sec/batch; 61h:56m:16s remains)
INFO - root - 2017-12-06 00:43:26.796120: step 59820, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.802 sec/batch; 60h:46m:42s remains)
INFO - root - 2017-12-06 00:43:35.339368: step 59830, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 64h:14m:06s remains)
INFO - root - 2017-12-06 00:43:43.777908: step 59840, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.833 sec/batch; 63h:04m:55s remains)
INFO - root - 2017-12-06 00:43:52.421319: step 59850, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 65h:19m:28s remains)
INFO - root - 2017-12-06 00:44:00.941096: step 59860, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 65h:00m:31s remains)
INFO - root - 2017-12-06 00:44:09.367650: step 59870, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 65h:43m:39s remains)
INFO - root - 2017-12-06 00:44:17.923708: step 59880, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 65h:47m:53s remains)
INFO - root - 2017-12-06 00:44:26.578790: step 59890, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 63h:43m:36s remains)
INFO - root - 2017-12-06 00:44:35.103780: step 59900, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 65h:25m:25s remains)
2017-12-06 00:44:35.959038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3222384 -4.3151612 -4.3133078 -4.3194184 -4.3270268 -4.3284535 -4.3231382 -4.3182211 -4.3176723 -4.3223877 -4.328145 -4.3293509 -4.3258929 -4.3220797 -4.3191781][-4.3027282 -4.2916379 -4.2891774 -4.2967997 -4.3075466 -4.3092136 -4.3028021 -4.2978344 -4.3002915 -4.3076658 -4.3136916 -4.3151875 -4.3099952 -4.30624 -4.3032556][-4.2718477 -4.2492065 -4.2422209 -4.2505083 -4.2637825 -4.2659354 -4.2572684 -4.2510715 -4.2531686 -4.2606988 -4.2705541 -4.2782044 -4.2778206 -4.2785239 -4.2787647][-4.230896 -4.1932459 -4.1764951 -4.1816497 -4.19621 -4.1982865 -4.1894255 -4.1813335 -4.18175 -4.1878886 -4.2023058 -4.2174611 -4.2241697 -4.2322206 -4.2373333][-4.1868844 -4.1349049 -4.1050916 -4.1053772 -4.119657 -4.1216536 -4.1112971 -4.1017265 -4.0993495 -4.1020374 -4.1156197 -4.1363339 -4.1506214 -4.1677265 -4.1803737][-4.156642 -4.0932536 -4.05017 -4.0433221 -4.0566783 -4.0579863 -4.0411139 -4.0257621 -4.021028 -4.0198522 -4.0287023 -4.0517364 -4.0730247 -4.0959721 -4.1128225][-4.146337 -4.0749397 -4.02005 -4.0009246 -4.0046 -4.0014172 -3.9795785 -3.9585724 -3.9485435 -3.9419029 -3.9485033 -3.9781349 -4.0102963 -4.0358071 -4.0461416][-4.1560607 -4.0831366 -4.0245876 -3.9944079 -3.9866481 -3.9773984 -3.9604502 -3.9514484 -3.94713 -3.9447014 -3.9559581 -3.9877717 -4.0220242 -4.036829 -4.0304604][-4.1737747 -4.1080074 -4.0551357 -4.0203443 -4.0017176 -3.9865484 -3.9761515 -3.9833689 -3.9944222 -4.0053434 -4.0276747 -4.058311 -4.0808315 -4.0780044 -4.0559359][-4.183012 -4.1292906 -4.0835395 -4.0446157 -4.0166149 -3.9962783 -3.9887474 -3.9981058 -4.0094109 -4.0250869 -4.05378 -4.0886045 -4.1060238 -4.0988245 -4.0746207][-4.1938324 -4.154016 -4.1188741 -4.0855303 -4.0629344 -4.0463576 -4.0371842 -4.03087 -4.0267944 -4.0341191 -4.05393 -4.0839429 -4.0997558 -4.0955887 -4.0768151][-4.2167659 -4.1861897 -4.1591582 -4.1351395 -4.1219344 -4.1108689 -4.0957651 -4.0719852 -4.0517173 -4.047286 -4.0530081 -4.0745783 -4.09204 -4.0977416 -4.09032][-4.250145 -4.2270865 -4.2071815 -4.1916871 -4.1850047 -4.1773643 -4.1591244 -4.1298647 -4.1032023 -4.0892062 -4.0845442 -4.0952992 -4.1074429 -4.113071 -4.1085372][-4.2850456 -4.267942 -4.2520313 -4.2406907 -4.2352552 -4.2283044 -4.21169 -4.1856747 -4.1623535 -4.1483545 -4.1422191 -4.1449056 -4.1485333 -4.1469679 -4.135674][-4.3129292 -4.2998724 -4.2867661 -4.2764516 -4.2709632 -4.2646146 -4.2521343 -4.234446 -4.2182522 -4.2092109 -4.2066941 -4.2071619 -4.2040143 -4.1920381 -4.170403]]...]
INFO - root - 2017-12-06 00:44:44.573279: step 59910, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 65h:25m:48s remains)
INFO - root - 2017-12-06 00:44:52.992104: step 59920, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 63h:57m:25s remains)
INFO - root - 2017-12-06 00:45:01.462262: step 59930, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 63h:52m:09s remains)
INFO - root - 2017-12-06 00:45:09.863724: step 59940, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.842 sec/batch; 63h:43m:42s remains)
INFO - root - 2017-12-06 00:45:18.397266: step 59950, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 65h:49m:38s remains)
INFO - root - 2017-12-06 00:45:26.921897: step 59960, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 65h:43m:41s remains)
INFO - root - 2017-12-06 00:45:35.388467: step 59970, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 64h:48m:51s remains)
INFO - root - 2017-12-06 00:45:43.899112: step 59980, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 65h:08m:11s remains)
INFO - root - 2017-12-06 00:45:52.524204: step 59990, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 64h:11m:58s remains)
INFO - root - 2017-12-06 00:46:01.123709: step 60000, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 64h:02m:13s remains)
2017-12-06 00:46:01.861971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3050413 -4.3055792 -4.3085766 -4.3104057 -4.3096137 -4.3079519 -4.3061509 -4.304853 -4.3075833 -4.311542 -4.3139544 -4.313139 -4.3116894 -4.3126216 -4.3121977][-4.2896805 -4.2916894 -4.2973413 -4.2991977 -4.2941608 -4.2889094 -4.285861 -4.287178 -4.2925687 -4.2984462 -4.3002071 -4.299078 -4.3001871 -4.3033967 -4.3022132][-4.262135 -4.2640138 -4.26951 -4.2688036 -4.2596741 -4.2514462 -4.24831 -4.25234 -4.2628212 -4.2741461 -4.2785077 -4.2774582 -4.2807155 -4.2876916 -4.286365][-4.2365985 -4.2367935 -4.2384505 -4.2307377 -4.2156148 -4.203445 -4.1984596 -4.2031317 -4.2194386 -4.2413168 -4.2547879 -4.2561307 -4.2610664 -4.2715664 -4.2700243][-4.2185383 -4.2161021 -4.209084 -4.1908116 -4.1688328 -4.1525106 -4.1437488 -4.1458945 -4.1659589 -4.1999488 -4.2278795 -4.2371545 -4.24378 -4.25672 -4.25625][-4.2005811 -4.1956496 -4.1783767 -4.1493015 -4.121757 -4.1052403 -4.096118 -4.0937614 -4.1132383 -4.1545086 -4.1952991 -4.2150326 -4.2225118 -4.2344909 -4.2365174][-4.1916571 -4.179235 -4.1493049 -4.1059046 -4.072423 -4.06528 -4.0675611 -4.0651507 -4.08098 -4.1236076 -4.1687417 -4.1917129 -4.1997662 -4.2096725 -4.2126756][-4.1900163 -4.1651697 -4.1282296 -4.0814819 -4.0481453 -4.0500073 -4.0611105 -4.0581021 -4.0672913 -4.1087017 -4.1562247 -4.1776562 -4.1845303 -4.1918125 -4.1917052][-4.1774888 -4.1510468 -4.122858 -4.0894704 -4.0637593 -4.0620685 -4.0667048 -4.057735 -4.0593109 -4.0974779 -4.1488681 -4.1730895 -4.1790662 -4.1839485 -4.1814][-4.1543994 -4.13963 -4.1343336 -4.1231394 -4.1026573 -4.0898786 -4.0830717 -4.0675292 -4.0607834 -4.0897655 -4.1392317 -4.1663022 -4.1744595 -4.1788597 -4.1784158][-4.1329832 -4.1361294 -4.1523581 -4.1594925 -4.146935 -4.1296673 -4.1143007 -4.0967584 -4.0883942 -4.1077962 -4.1445489 -4.168324 -4.1776557 -4.1834488 -4.1859818][-4.1240244 -4.1380191 -4.1661954 -4.185215 -4.1792765 -4.1598597 -4.1390686 -4.1250596 -4.1231694 -4.1387129 -4.1617126 -4.1787338 -4.1900449 -4.2001052 -4.2062564][-4.1257005 -4.1383381 -4.17006 -4.1962085 -4.1943007 -4.1754894 -4.1552386 -4.1438346 -4.1458268 -4.160099 -4.1767788 -4.1905389 -4.206058 -4.2211409 -4.2301922][-4.1335034 -4.1453881 -4.1759877 -4.2033176 -4.2017236 -4.1848168 -4.1700578 -4.1656194 -4.1691732 -4.179143 -4.1891842 -4.1999512 -4.2175674 -4.2347112 -4.2430763][-4.1579518 -4.1716094 -4.1968889 -4.2189064 -4.2174554 -4.2062736 -4.1983681 -4.1973944 -4.2009225 -4.2052765 -4.2098269 -4.2188778 -4.2345405 -4.24935 -4.2543983]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-0init/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 00:46:11.175054: step 60010, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 0.766 sec/batch; 57h:58m:01s remains)
INFO - root - 2017-12-06 00:46:19.590373: step 60020, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 63h:52m:20s remains)
INFO - root - 2017-12-06 00:46:27.831433: step 60030, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 63h:53m:14s remains)
INFO - root - 2017-12-06 00:46:36.302066: step 60040, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.823 sec/batch; 62h:15m:02s remains)
INFO - root - 2017-12-06 00:46:44.819361: step 60050, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.838 sec/batch; 63h:25m:57s remains)
INFO - root - 2017-12-06 00:46:53.322638: step 60060, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 63h:46m:57s remains)
INFO - root - 2017-12-06 00:47:01.708784: step 60070, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 64h:06m:19s remains)
INFO - root - 2017-12-06 00:47:10.308165: step 60080, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 64h:56m:37s remains)
INFO - root - 2017-12-06 00:47:18.876970: step 60090, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 64h:19m:40s remains)
INFO - root - 2017-12-06 00:47:27.414182: step 60100, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 64h:15m:21s remains)
2017-12-06 00:47:28.162650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1286144 -4.1663041 -4.1792789 -4.1594987 -4.1148114 -4.0919685 -4.1246786 -4.1683531 -4.2059326 -4.230989 -4.2381129 -4.2353935 -4.2359533 -4.2441716 -4.2595749][-4.1223021 -4.1634955 -4.1825695 -4.1637206 -4.1114535 -4.0838656 -4.1194677 -4.1627588 -4.1982808 -4.2195644 -4.2232175 -4.2196021 -4.2265115 -4.2395196 -4.2553024][-4.1306553 -4.1634188 -4.1781182 -4.1639686 -4.1108007 -4.0786233 -4.1144319 -4.1629562 -4.1999774 -4.2182155 -4.2216215 -4.2205763 -4.2337809 -4.2482524 -4.2623272][-4.1376138 -4.1583843 -4.1664586 -4.1530428 -4.0961528 -4.0555134 -4.0929184 -4.1533546 -4.1969647 -4.2143173 -4.2132287 -4.2107105 -4.2269063 -4.2413311 -4.2575383][-4.13299 -4.1491451 -4.1546879 -4.1350985 -4.067605 -4.0104194 -4.0414038 -4.1127286 -4.1668153 -4.1848197 -4.1791105 -4.1768618 -4.2003746 -4.2226214 -4.2452021][-4.1123376 -4.1274123 -4.1306725 -4.0996275 -4.0102754 -3.9161935 -3.9288316 -4.0184712 -4.1000309 -4.1326456 -4.132443 -4.1390705 -4.1739907 -4.205792 -4.2343426][-4.0978417 -4.1081724 -4.1028771 -4.0593395 -3.9448886 -3.8113723 -3.8077204 -3.9217086 -4.0314889 -4.0875807 -4.10113 -4.1174746 -4.1576948 -4.1929865 -4.2236862][-4.0940757 -4.1052136 -4.0972118 -4.0542269 -3.9414132 -3.8099196 -3.8075361 -3.9233098 -4.0308604 -4.0924454 -4.112464 -4.1298523 -4.1617403 -4.192399 -4.22104][-4.1161695 -4.1335649 -4.1317582 -4.1034489 -4.016058 -3.913902 -3.9101853 -3.9938331 -4.0765524 -4.1245856 -4.1416769 -4.1568956 -4.1782222 -4.2023139 -4.2268753][-4.1537547 -4.167037 -4.1639414 -4.1443009 -4.082356 -4.0083528 -4.0021911 -4.0561595 -4.1172452 -4.1521883 -4.1609216 -4.1721773 -4.1900873 -4.2120991 -4.2360659][-4.197319 -4.2013011 -4.1949258 -4.1800032 -4.1390667 -4.0882859 -4.0820928 -4.1175246 -4.1641641 -4.1906109 -4.1935573 -4.2016149 -4.2186356 -4.239306 -4.2601438][-4.234818 -4.2288704 -4.2212663 -4.211556 -4.1882586 -4.1600428 -4.1575003 -4.1809793 -4.2158341 -4.2347407 -4.2346349 -4.2368793 -4.2493391 -4.2666259 -4.2848835][-4.262167 -4.2542644 -4.2512116 -4.2484131 -4.2385268 -4.2261763 -4.225637 -4.2406878 -4.2640061 -4.2751975 -4.2724776 -4.2718019 -4.2794194 -4.2918673 -4.3057475][-4.2947545 -4.2899785 -4.2896438 -4.2886233 -4.2833996 -4.2779837 -4.2775974 -4.28553 -4.2993884 -4.3063731 -4.3040218 -4.3031034 -4.3066487 -4.31344 -4.3205781][-4.3245854 -4.3212056 -4.3195248 -4.3168077 -4.3132062 -4.3116293 -4.31212 -4.3163633 -4.3234611 -4.3281403 -4.3278918 -4.3272538 -4.3284373 -4.3308325 -4.3336554]]...]
INFO - root - 2017-12-06 00:47:36.722608: step 60110, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 65h:55m:07s remains)
INFO - root - 2017-12-06 00:47:45.133549: step 60120, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 65h:57m:11s remains)
INFO - root - 2017-12-06 00:47:53.731502: step 60130, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 64h:15m:04s remains)
INFO - root - 2017-12-06 00:48:02.107988: step 60140, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 63h:44m:14s remains)
INFO - root - 2017-12-06 00:48:10.504564: step 60150, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 63h:59m:19s remains)
INFO - root - 2017-12-06 00:48:19.092971: step 60160, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.812 sec/batch; 61h:24m:35s remains)
INFO - root - 2017-12-06 00:48:27.521258: step 60170, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 65h:23m:48s remains)
INFO - root - 2017-12-06 00:48:36.102357: step 60180, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 63h:00m:56s remains)
INFO - root - 2017-12-06 00:48:44.616543: step 60190, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 64h:14m:09s remains)
INFO - root - 2017-12-06 00:48:53.172362: step 60200, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 63h:42m:11s remains)
2017-12-06 00:48:53.927164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2332978 -4.2244887 -4.2343311 -4.2443171 -4.2370396 -4.228806 -4.224957 -4.2314787 -4.246223 -4.2610574 -4.2702818 -4.2629433 -4.2414231 -4.2099204 -4.1842127][-4.2050953 -4.2102852 -4.2359567 -4.25064 -4.2335954 -4.2124896 -4.2019992 -4.2116933 -4.2379241 -4.2606816 -4.2722673 -4.2642322 -4.2376065 -4.1962824 -4.1625061][-4.1695957 -4.1895785 -4.2282009 -4.2425337 -4.2123 -4.1686363 -4.1453671 -4.1620197 -4.2074418 -4.2441111 -4.2588382 -4.2471652 -4.2147622 -4.1647286 -4.1280317][-4.149055 -4.1802483 -4.2237663 -4.2322578 -4.1832514 -4.1092358 -4.0675607 -4.0993786 -4.1744823 -4.2304068 -4.2462091 -4.2266068 -4.1865878 -4.13706 -4.1039934][-4.1380568 -4.1736083 -4.2116413 -4.2092385 -4.1428032 -4.0382428 -3.9794886 -4.0304294 -4.135313 -4.2108684 -4.2295346 -4.203464 -4.1592388 -4.1137309 -4.087657][-4.1360574 -4.17027 -4.1982803 -4.1790681 -4.0901017 -3.9525909 -3.8803248 -3.9496248 -4.0772991 -4.1710558 -4.1971483 -4.1688995 -4.1216612 -4.0818071 -4.0660324][-4.1503177 -4.1803079 -4.1932182 -4.1565728 -4.0498757 -3.8965735 -3.8177042 -3.8895335 -4.0289931 -4.1388006 -4.1723475 -4.141202 -4.0885344 -4.0453615 -4.0422544][-4.1640229 -4.1887822 -4.1895103 -4.1411753 -4.0342379 -3.8961518 -3.8333044 -3.8948407 -4.0237474 -4.1340237 -4.1675062 -4.1296644 -4.0658627 -4.0110683 -4.0120044][-4.1625462 -4.1816196 -4.1728182 -4.1191115 -4.02693 -3.9276249 -3.8938303 -3.9419379 -4.0421829 -4.1369028 -4.1621461 -4.1168456 -4.0451031 -3.9832442 -3.9821515][-4.1547756 -4.1697121 -4.1577096 -4.1059237 -4.0374002 -3.9818988 -3.9772937 -4.0078297 -4.066143 -4.1287169 -4.1434197 -4.1030464 -4.039362 -3.9823265 -3.9760394][-4.1552129 -4.1671958 -4.1591926 -4.1157851 -4.0648742 -4.0361996 -4.045177 -4.0588775 -4.0783739 -4.1120057 -4.1221223 -4.0956249 -4.0511065 -4.0078068 -3.9973769][-4.1632524 -4.1739464 -4.1695533 -4.1357269 -4.0994172 -4.0901093 -4.1018248 -4.1029668 -4.0968671 -4.1077409 -4.1177654 -4.10932 -4.0806723 -4.0479836 -4.0330577][-4.1946759 -4.2021365 -4.194706 -4.169601 -4.1492143 -4.1525679 -4.1619239 -4.1536417 -4.1320858 -4.1290312 -4.1386323 -4.1421 -4.1255236 -4.1006994 -4.084446][-4.2419376 -4.2440844 -4.2356043 -4.2206793 -4.2134833 -4.2207131 -4.2260675 -4.215013 -4.1898651 -4.1804147 -4.1858745 -4.1920462 -4.18298 -4.1668234 -4.1567841][-4.2866478 -4.2864733 -4.2804375 -4.2735682 -4.2713723 -4.2763052 -4.27884 -4.2717161 -4.2541289 -4.242 -4.24078 -4.2440863 -4.2407475 -4.2334752 -4.2331629]]...]
INFO - root - 2017-12-06 00:49:02.412741: step 60210, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 64h:43m:25s remains)
INFO - root - 2017-12-06 00:49:10.873495: step 60220, loss = 2.10, batch loss = 2.05 (9.5 examples/sec; 0.841 sec/batch; 63h:36m:17s remains)
INFO - root - 2017-12-06 00:49:19.222602: step 60230, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 64h:15m:48s remains)
INFO - root - 2017-12-06 00:49:27.624076: step 60240, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 62h:39m:41s remains)
INFO - root - 2017-12-06 00:49:35.904253: step 60250, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 64h:29m:25s remains)
INFO - root - 2017-12-06 00:49:44.290271: step 60260, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 62h:03m:49s remains)
INFO - root - 2017-12-06 00:49:52.743801: step 60270, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 64h:03m:20s remains)
INFO - root - 2017-12-06 00:50:01.212771: step 60280, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 62h:55m:04s remains)
INFO - root - 2017-12-06 00:50:09.749128: step 60290, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.871 sec/batch; 65h:52m:21s remains)
INFO - root - 2017-12-06 00:50:18.209792: step 60300, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 63h:14m:37s remains)
2017-12-06 00:50:18.943134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1831803 -4.195281 -4.197341 -4.2042661 -4.2132511 -4.2246342 -4.2333236 -4.2391992 -4.2385988 -4.2387862 -4.2362757 -4.2384653 -4.2503915 -4.26677 -4.2888389][-4.2095375 -4.2090626 -4.1981969 -4.191812 -4.1949248 -4.2134695 -4.2294483 -4.23991 -4.23916 -4.2428508 -4.2383957 -4.2358871 -4.248323 -4.2655978 -4.287559][-4.2254448 -4.2106366 -4.1849737 -4.1681986 -4.1708379 -4.1929259 -4.2078538 -4.2183385 -4.2182431 -4.2268438 -4.2275143 -4.2248573 -4.238338 -4.2579107 -4.282681][-4.2269692 -4.2024646 -4.1662397 -4.1433749 -4.1453018 -4.1620512 -4.1707582 -4.1791029 -4.1871452 -4.2035184 -4.2154617 -4.2204022 -4.2360687 -4.2563109 -4.284153][-4.2190852 -4.1873116 -4.1414065 -4.1145878 -4.1120148 -4.1171389 -4.1135755 -4.1230726 -4.1461015 -4.1693645 -4.1935372 -4.2116504 -4.2344012 -4.256845 -4.286902][-4.1953616 -4.1578674 -4.1015244 -4.0667968 -4.0575523 -4.0471926 -4.0166926 -4.0256729 -4.075902 -4.1153922 -4.1535559 -4.190762 -4.2280245 -4.2556887 -4.286654][-4.164474 -4.1233878 -4.0644479 -4.0225954 -3.9998298 -3.9583249 -3.8811259 -3.8804915 -3.9696772 -4.0339661 -4.0897985 -4.1456342 -4.2000127 -4.240375 -4.2770314][-4.157186 -4.1124172 -4.0525589 -4.00108 -3.960192 -3.8867426 -3.7760289 -3.7775369 -3.8974195 -3.9800138 -4.0412178 -4.1045232 -4.1669884 -4.2189584 -4.2643061][-4.1808815 -4.1339478 -4.0709629 -4.0141664 -3.9668279 -3.90018 -3.8236332 -3.8338697 -3.9214339 -3.9831681 -4.0317507 -4.0905209 -4.1535749 -4.2109661 -4.2598333][-4.2291951 -4.18858 -4.132246 -4.0818367 -4.0440688 -4.00464 -3.9728122 -3.9807234 -4.0178957 -4.0500331 -4.0783968 -4.1230388 -4.17971 -4.2341442 -4.2743587][-4.2821584 -4.2540278 -4.2171583 -4.1860881 -4.162149 -4.1396623 -4.1280713 -4.1284475 -4.1352177 -4.1512065 -4.1674137 -4.1955676 -4.2366142 -4.2768879 -4.3012371][-4.3157463 -4.3018937 -4.2840366 -4.2703109 -4.2575822 -4.2463269 -4.2422128 -4.2379241 -4.2348652 -4.2438903 -4.2536163 -4.267333 -4.2894325 -4.3111968 -4.322197][-4.3336887 -4.3285093 -4.3221645 -4.3168044 -4.3118877 -4.3078408 -4.3072476 -4.303834 -4.2992687 -4.3041453 -4.3097153 -4.3147893 -4.3221703 -4.32986 -4.3338528][-4.3420596 -4.3394117 -4.3384671 -4.3382645 -4.337985 -4.3361845 -4.33585 -4.3351197 -4.3323231 -4.3338466 -4.3356152 -4.3363724 -4.3369713 -4.339386 -4.34112][-4.3444529 -4.3427243 -4.343792 -4.34463 -4.344418 -4.3424411 -4.3413892 -4.3420267 -4.3416729 -4.3418369 -4.3419428 -4.34192 -4.3417168 -4.3428078 -4.3439512]]...]
INFO - root - 2017-12-06 00:50:27.381198: step 60310, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 62h:56m:30s remains)
INFO - root - 2017-12-06 00:50:35.921171: step 60320, loss = 2.09, batch loss = 2.04 (9.7 examples/sec; 0.826 sec/batch; 62h:26m:51s remains)
INFO - root - 2017-12-06 00:50:44.483776: step 60330, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 64h:33m:38s remains)
INFO - root - 2017-12-06 00:50:52.903323: step 60340, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 63h:08m:32s remains)
INFO - root - 2017-12-06 00:51:01.383787: step 60350, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 64h:18m:27s remains)
INFO - root - 2017-12-06 00:51:09.828085: step 60360, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 64h:28m:52s remains)
INFO - root - 2017-12-06 00:51:18.283085: step 60370, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 63h:50m:44s remains)
INFO - root - 2017-12-06 00:51:26.666852: step 60380, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 62h:43m:06s remains)
INFO - root - 2017-12-06 00:51:35.150656: step 60390, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 63h:12m:48s remains)
INFO - root - 2017-12-06 00:51:43.590972: step 60400, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 64h:42m:23s remains)
2017-12-06 00:51:44.414645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2132807 -4.2197938 -4.2255626 -4.2340808 -4.2462568 -4.2565589 -4.2545691 -4.2450876 -4.2373505 -4.2336011 -4.2359281 -4.2375774 -4.236342 -4.2477503 -4.2717428][-4.2645235 -4.264523 -4.2611871 -4.2605443 -4.26596 -4.2719727 -4.2677236 -4.2581921 -4.2504311 -4.2466979 -4.2526073 -4.2589269 -4.260406 -4.2707853 -4.2888889][-4.2795959 -4.2735052 -4.2674308 -4.2634535 -4.2654123 -4.2685909 -4.2627211 -4.2527909 -4.2439404 -4.2397237 -4.2481833 -4.2626667 -4.2733488 -4.2838111 -4.2933321][-4.2397542 -4.2343211 -4.23324 -4.2315674 -4.2288027 -4.2224455 -4.2066569 -4.1964164 -4.1939435 -4.1940618 -4.209022 -4.234714 -4.2541933 -4.2643976 -4.266861][-4.1578054 -4.1606016 -4.1693177 -4.1692138 -4.1559076 -4.1284246 -4.0861549 -4.0669551 -4.0770683 -4.0942469 -4.1288013 -4.1737118 -4.2020187 -4.2127814 -4.2082639][-4.0887895 -4.0944142 -4.107935 -4.1038003 -4.0762858 -4.0217824 -3.9467807 -3.9137094 -3.9330356 -3.9654245 -4.0189557 -4.081152 -4.1159067 -4.1280823 -4.1163812][-4.0957403 -4.0998807 -4.1072621 -4.0952353 -4.0624642 -4.0036254 -3.9239893 -3.8887224 -3.9075608 -3.9330468 -3.9741201 -4.0222087 -4.0459561 -4.0540547 -4.0402136][-4.1380515 -4.14003 -4.1456347 -4.1381 -4.1226845 -4.0964718 -4.05436 -4.0350046 -4.0456142 -4.0509658 -4.0602832 -4.0718684 -4.0707278 -4.067699 -4.0571404][-4.1531882 -4.1557693 -4.163754 -4.1675148 -4.1722174 -4.1777387 -4.1735578 -4.1751194 -4.1850376 -4.1853313 -4.181056 -4.1741896 -4.161521 -4.1535654 -4.148489][-4.1360817 -4.1375122 -4.1490564 -4.1614914 -4.1774211 -4.2002096 -4.2204628 -4.2377481 -4.2503042 -4.2541318 -4.2515 -4.243979 -4.2319632 -4.2251506 -4.2251406][-4.1329751 -4.1299253 -4.1395864 -4.1517563 -4.1649618 -4.1885352 -4.2136683 -4.2363243 -4.2505751 -4.2612314 -4.2663136 -4.2635312 -4.2553463 -4.2508655 -4.2573762][-4.1550303 -4.1455321 -4.1441288 -4.1455956 -4.1496096 -4.1647525 -4.1833572 -4.2039957 -4.2181764 -4.2340493 -4.2480822 -4.2497387 -4.2445135 -4.2422523 -4.2507668][-4.1832881 -4.165916 -4.1505508 -4.1384921 -4.1344795 -4.1410618 -4.1500573 -4.16607 -4.1826525 -4.2015204 -4.2212167 -4.2242069 -4.2169085 -4.2152805 -4.2237997][-4.2083693 -4.1869059 -4.1601667 -4.1361976 -4.1247072 -4.1263428 -4.1289635 -4.1383133 -4.1534305 -4.1754518 -4.2003622 -4.2078185 -4.2005358 -4.1969881 -4.2034144][-4.220911 -4.2016144 -4.1751528 -4.14911 -4.1352448 -4.1327386 -4.1292224 -4.1265216 -4.133604 -4.1546922 -4.1828885 -4.19972 -4.19984 -4.1988683 -4.2037091]]...]
INFO - root - 2017-12-06 00:51:52.876263: step 60410, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 64h:20m:16s remains)
INFO - root - 2017-12-06 00:52:01.519910: step 60420, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.834 sec/batch; 63h:03m:09s remains)
INFO - root - 2017-12-06 00:52:10.036125: step 60430, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 65h:41m:14s remains)
INFO - root - 2017-12-06 00:52:18.460304: step 60440, loss = 2.11, batch loss = 2.05 (9.6 examples/sec; 0.833 sec/batch; 62h:56m:46s remains)
INFO - root - 2017-12-06 00:52:26.973701: step 60450, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 64h:33m:54s remains)
INFO - root - 2017-12-06 00:52:35.448034: step 60460, loss = 2.03, batch loss = 1.98 (9.8 examples/sec; 0.819 sec/batch; 61h:52m:22s remains)
INFO - root - 2017-12-06 00:52:43.658446: step 60470, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 62h:00m:18s remains)
INFO - root - 2017-12-06 00:52:52.050098: step 60480, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 63h:31m:47s remains)
INFO - root - 2017-12-06 00:53:00.444693: step 60490, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 63h:45m:30s remains)
INFO - root - 2017-12-06 00:53:08.949359: step 60500, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 63h:50m:33s remains)
2017-12-06 00:53:09.734810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23436 -4.2216039 -4.2017622 -4.1721745 -4.1355357 -4.1078234 -4.0882483 -4.0957508 -4.1310534 -4.1815495 -4.2165222 -4.2378426 -4.2627797 -4.2815876 -4.282927][-4.2465711 -4.235961 -4.2122602 -4.1751251 -4.1261673 -4.0861464 -4.06185 -4.0647926 -4.0967374 -4.15394 -4.2023797 -4.2340808 -4.2651119 -4.2870016 -4.2889][-4.2492924 -4.2395029 -4.2114286 -4.1664429 -4.1070375 -4.0525966 -4.0242124 -4.0301409 -4.0674658 -4.1301165 -4.1868925 -4.2233477 -4.2573872 -4.2835336 -4.2895851][-4.2509623 -4.2401409 -4.2068439 -4.1523423 -4.0840039 -4.014 -3.9781625 -3.9910612 -4.042357 -4.1123042 -4.1732984 -4.2128525 -4.2492595 -4.2766285 -4.2844987][-4.2498837 -4.2419243 -4.2069564 -4.1425166 -4.0596442 -3.9685254 -3.921509 -3.9494975 -4.0194111 -4.1029105 -4.1721931 -4.2139807 -4.2480235 -4.2725692 -4.2783222][-4.2388749 -4.2384148 -4.2098331 -4.1427364 -4.0453172 -3.9334676 -3.8718882 -3.9128566 -4.001915 -4.0987372 -4.1761866 -4.2194362 -4.2460833 -4.261148 -4.2631578][-4.2312279 -4.2357483 -4.2154269 -4.1498761 -4.0460529 -3.9220052 -3.843472 -3.8861446 -3.989886 -4.0971146 -4.1774731 -4.2192888 -4.2391839 -4.2437081 -4.2403688][-4.2284455 -4.2291856 -4.2126904 -4.1593332 -4.0672541 -3.9481382 -3.8629992 -3.8998525 -4.0067062 -4.1083336 -4.177978 -4.2142978 -4.2285995 -4.2267075 -4.2203531][-4.21636 -4.2086287 -4.1964726 -4.1639428 -4.0973921 -4.0034389 -3.9270153 -3.9521623 -4.0452051 -4.126678 -4.1769462 -4.2033386 -4.213582 -4.211729 -4.2056341][-4.200027 -4.181622 -4.1728215 -4.1634688 -4.1238623 -4.0602336 -4.0022912 -4.0110893 -4.0791726 -4.135869 -4.1651831 -4.1771712 -4.1859384 -4.1918 -4.1949472][-4.1891017 -4.1593285 -4.1517439 -4.1592197 -4.1482158 -4.1131606 -4.0720167 -4.0587144 -4.0928049 -4.1193871 -4.1276965 -4.1298409 -4.1451731 -4.1680074 -4.1874976][-4.191134 -4.1591315 -4.1477423 -4.16407 -4.1762948 -4.1646438 -4.1371694 -4.1046133 -4.1036644 -4.1061125 -4.1016054 -4.1006279 -4.1254268 -4.1632261 -4.1934772][-4.194694 -4.1647758 -4.1533055 -4.173173 -4.2018337 -4.2091107 -4.1946583 -4.1551037 -4.1301026 -4.1187634 -4.1101131 -4.1062994 -4.1331682 -4.1773505 -4.2115188][-4.1957922 -4.165483 -4.1579351 -4.1843138 -4.2248597 -4.2430758 -4.2367783 -4.199645 -4.1635962 -4.1430058 -4.1315565 -4.1288056 -4.153801 -4.1964164 -4.2295237][-4.1961451 -4.1682119 -4.1635795 -4.1965332 -4.2450118 -4.2701268 -4.2691231 -4.2425561 -4.2071762 -4.1848497 -4.174356 -4.1728835 -4.1913295 -4.2237635 -4.2509565]]...]
INFO - root - 2017-12-06 00:53:18.115190: step 60510, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 64h:36m:30s remains)
INFO - root - 2017-12-06 00:53:26.711372: step 60520, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 64h:04m:22s remains)
INFO - root - 2017-12-06 00:53:35.198402: step 60530, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 63h:46m:39s remains)
INFO - root - 2017-12-06 00:53:43.661078: step 60540, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 65h:35m:37s remains)
INFO - root - 2017-12-06 00:53:52.068603: step 60550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 65h:15m:12s remains)
INFO - root - 2017-12-06 00:54:00.566042: step 60560, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 64h:31m:40s remains)
INFO - root - 2017-12-06 00:54:09.018726: step 60570, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 65h:46m:17s remains)
INFO - root - 2017-12-06 00:54:17.439870: step 60580, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.889 sec/batch; 67h:08m:16s remains)
INFO - root - 2017-12-06 00:54:25.947473: step 60590, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 64h:27m:24s remains)
INFO - root - 2017-12-06 00:54:34.484048: step 60600, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 65h:24m:51s remains)
2017-12-06 00:54:35.240773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2590184 -4.26054 -4.2524266 -4.24266 -4.238533 -4.2346005 -4.2367988 -4.2449017 -4.2537508 -4.2535858 -4.2369704 -4.2144074 -4.1920109 -4.18774 -4.21379][-4.2589555 -4.2570429 -4.244947 -4.2294955 -4.2178144 -4.21074 -4.2102218 -4.2193985 -4.23905 -4.2572885 -4.2608204 -4.2550879 -4.2400694 -4.2326083 -4.243845][-4.2547474 -4.2483745 -4.2323661 -4.2122707 -4.1934166 -4.1811695 -4.1758785 -4.1788688 -4.2001271 -4.2324123 -4.2561007 -4.2703013 -4.2702022 -4.2700477 -4.2759562][-4.2425437 -4.2299652 -4.2100272 -4.1881804 -4.166163 -4.1486797 -4.134038 -4.1236215 -4.1354713 -4.1721864 -4.2110825 -4.24467 -4.2594934 -4.2709126 -4.2809372][-4.2311158 -4.2084031 -4.18168 -4.157908 -4.1362238 -4.1150346 -4.087153 -4.0550842 -4.0481806 -4.0814223 -4.1355228 -4.1931214 -4.2236691 -4.2449794 -4.2582822][-4.2356844 -4.20437 -4.1712937 -4.1432424 -4.1212745 -4.0935659 -4.0451245 -3.9799769 -3.9371815 -3.9556055 -4.0277491 -4.1216922 -4.1718426 -4.2035894 -4.2168531][-4.2568574 -4.2229133 -4.1858482 -4.1533866 -4.1239676 -4.0796456 -3.9990766 -3.8856421 -3.7888596 -3.7808752 -3.8765469 -4.0182486 -4.1009779 -4.1518097 -4.1721015][-4.2838979 -4.2525826 -4.2128139 -4.1752071 -4.1398988 -4.0839772 -3.9884582 -3.854641 -3.7331977 -3.7046263 -3.8007329 -3.9571853 -4.0523171 -4.1129446 -4.140861][-4.3060341 -4.2806382 -4.24266 -4.2065029 -4.1753597 -4.1302691 -4.0579524 -3.9616394 -3.8779147 -3.8534269 -3.9093978 -4.0155911 -4.0778618 -4.1176319 -4.1399608][-4.3230309 -4.3055654 -4.2759304 -4.2457991 -4.2206173 -4.1896381 -4.1478391 -4.093935 -4.04925 -4.0328369 -4.0556765 -4.1120243 -4.1383543 -4.149148 -4.159996][-4.3390026 -4.3286924 -4.308147 -4.2849956 -4.2635922 -4.2384644 -4.2104731 -4.1767898 -4.1507087 -4.1408205 -4.15101 -4.1840191 -4.1976585 -4.1957912 -4.1986623][-4.3503342 -4.3457384 -4.3315997 -4.3143444 -4.296278 -4.2714834 -4.243742 -4.2129636 -4.1890326 -4.1786995 -4.1876216 -4.217876 -4.2370834 -4.2423415 -4.2511892][-4.3556147 -4.3555479 -4.3479152 -4.3357196 -4.320406 -4.29678 -4.266407 -4.2328806 -4.203301 -4.1873741 -4.1965365 -4.2303462 -4.2626433 -4.2832313 -4.30123][-4.35321 -4.3587561 -4.3572664 -4.3495412 -4.3364973 -4.3169508 -4.2892118 -4.2520461 -4.2142916 -4.1918063 -4.1978092 -4.2304378 -4.270649 -4.3027563 -4.3276997][-4.3425441 -4.3526397 -4.3564582 -4.3536835 -4.3438716 -4.3267441 -4.3019323 -4.2627816 -4.2165594 -4.1858368 -4.186986 -4.217175 -4.2610397 -4.3008323 -4.3313041]]...]
INFO - root - 2017-12-06 00:54:43.788064: step 60610, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 64h:01m:05s remains)
INFO - root - 2017-12-06 00:54:52.311492: step 60620, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 62h:56m:45s remains)
INFO - root - 2017-12-06 00:55:00.733847: step 60630, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 64h:07m:13s remains)
INFO - root - 2017-12-06 00:55:09.199381: step 60640, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 63h:58m:31s remains)
INFO - root - 2017-12-06 00:55:17.623077: step 60650, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 64h:09m:14s remains)
INFO - root - 2017-12-06 00:55:25.941531: step 60660, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 65h:08m:13s remains)
INFO - root - 2017-12-06 00:55:33.958311: step 60670, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 64h:39m:06s remains)
INFO - root - 2017-12-06 00:55:42.388053: step 60680, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 63h:33m:53s remains)
INFO - root - 2017-12-06 00:55:50.753554: step 60690, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 64h:59m:25s remains)
INFO - root - 2017-12-06 00:55:59.203444: step 60700, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 63h:50m:58s remains)
2017-12-06 00:55:59.976920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2909436 -4.2951808 -4.2957296 -4.2964582 -4.2973304 -4.2982545 -4.2989855 -4.2992516 -4.2993684 -4.2995772 -4.29989 -4.3000455 -4.2998743 -4.2990217 -4.297482][-4.2883339 -4.2951093 -4.2968435 -4.2988496 -4.3009272 -4.3028312 -4.3036113 -4.3034215 -4.3036723 -4.3043861 -4.3052092 -4.3054495 -4.3049431 -4.3037486 -4.3016167][-4.2737546 -4.2842026 -4.289134 -4.2929039 -4.2961617 -4.298851 -4.2998257 -4.2993336 -4.3004923 -4.3027854 -4.304841 -4.3050442 -4.3036742 -4.3013606 -4.2981558][-4.2396255 -4.2545581 -4.2647624 -4.2708993 -4.2742548 -4.2766519 -4.2777872 -4.2777057 -4.2803884 -4.2845697 -4.2875752 -4.286953 -4.2839446 -4.280849 -4.2776618][-4.1976137 -4.2168612 -4.2326136 -4.2398663 -4.2418947 -4.2441015 -4.2452216 -4.2457442 -4.24878 -4.2526312 -4.2541 -4.2508092 -4.2453389 -4.2426419 -4.2431931][-4.1664634 -4.1868148 -4.2040381 -4.2105308 -4.2105165 -4.21231 -4.2139139 -4.215302 -4.2178574 -4.2194448 -4.2178655 -4.2124071 -4.2074385 -4.208931 -4.2157907][-4.1448059 -4.1647067 -4.1785574 -4.1803169 -4.1768036 -4.179038 -4.1843967 -4.189487 -4.1936741 -4.19396 -4.1910233 -4.1885228 -4.1909518 -4.2005644 -4.2136374][-4.1379352 -4.1555414 -4.1639166 -4.1617975 -4.1559014 -4.1595964 -4.171576 -4.1851826 -4.1957941 -4.1980033 -4.1964526 -4.2001877 -4.2103982 -4.224546 -4.2387404][-4.15714 -4.1700473 -4.174253 -4.1703382 -4.1635609 -4.1690016 -4.187645 -4.2103462 -4.2283983 -4.236177 -4.2385488 -4.2457705 -4.2571235 -4.2686095 -4.2786174][-4.1833606 -4.1956172 -4.2002382 -4.1958776 -4.1879182 -4.1929965 -4.2155557 -4.2444372 -4.2685857 -4.2812271 -4.2853761 -4.2915936 -4.2996387 -4.3057117 -4.3103175][-4.1988859 -4.2154384 -4.2233787 -4.2183518 -4.2079229 -4.2108984 -4.2324324 -4.262249 -4.2903991 -4.3064771 -4.3115916 -4.3155284 -4.3196564 -4.3217525 -4.3231115][-4.2077379 -4.2270555 -4.2362814 -4.2295613 -4.2187095 -4.2218041 -4.2415447 -4.2696996 -4.2962928 -4.3112926 -4.3152127 -4.3166132 -4.3175311 -4.3180413 -4.3193059][-4.2197633 -4.2386169 -4.2461209 -4.2406516 -4.2333579 -4.239532 -4.2596316 -4.2828526 -4.3022742 -4.3116965 -4.3124766 -4.3108449 -4.309011 -4.3093204 -4.3122134][-4.235836 -4.2494473 -4.25432 -4.2520671 -4.2508349 -4.2611055 -4.2802043 -4.2974682 -4.309371 -4.3135161 -4.3118186 -4.3074121 -4.3033471 -4.3035183 -4.3072147][-4.2447224 -4.2536511 -4.2600188 -4.2643495 -4.2690344 -4.2805233 -4.2962675 -4.3078084 -4.3142939 -4.3150291 -4.3119569 -4.3058915 -4.3003855 -4.2999067 -4.302856]]...]
INFO - root - 2017-12-06 00:56:08.430607: step 60710, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 65h:18m:11s remains)
INFO - root - 2017-12-06 00:56:16.966284: step 60720, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 65h:56m:04s remains)
INFO - root - 2017-12-06 00:56:25.532619: step 60730, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 65h:55m:01s remains)
INFO - root - 2017-12-06 00:56:34.023660: step 60740, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.825 sec/batch; 62h:17m:10s remains)
INFO - root - 2017-12-06 00:56:42.597376: step 60750, loss = 2.02, batch loss = 1.96 (9.3 examples/sec; 0.864 sec/batch; 65h:11m:18s remains)
INFO - root - 2017-12-06 00:56:51.053018: step 60760, loss = 2.03, batch loss = 1.98 (10.7 examples/sec; 0.748 sec/batch; 56h:28m:47s remains)
INFO - root - 2017-12-06 00:56:59.565483: step 60770, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 66h:13m:37s remains)
INFO - root - 2017-12-06 00:57:08.171691: step 60780, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 65h:44m:18s remains)
INFO - root - 2017-12-06 00:57:16.711438: step 60790, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 63h:24m:57s remains)
INFO - root - 2017-12-06 00:57:25.250884: step 60800, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 63h:55m:41s remains)
2017-12-06 00:57:26.077134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3055253 -4.3010325 -4.2948689 -4.2913885 -4.2796431 -4.2571592 -4.2371659 -4.2390428 -4.2540865 -4.2703652 -4.293716 -4.3178434 -4.3359933 -4.3392477 -4.3306246][-4.299397 -4.2917743 -4.2840357 -4.2796035 -4.2658038 -4.2387686 -4.2171412 -4.222693 -4.2441869 -4.2628574 -4.2881246 -4.3156605 -4.33688 -4.3422885 -4.3326797][-4.2919569 -4.2804103 -4.2688432 -4.2581677 -4.24004 -4.2109814 -4.1924171 -4.2035532 -4.2310786 -4.2552414 -4.28413 -4.3155417 -4.339 -4.3458638 -4.3348575][-4.2799788 -4.261107 -4.2402611 -4.2197456 -4.197433 -4.1696229 -4.1577444 -4.1769056 -4.2138958 -4.2480073 -4.28315 -4.3181572 -4.34303 -4.35007 -4.3383183][-4.2624769 -4.2302742 -4.1949291 -4.1625986 -4.1353855 -4.1125312 -4.113843 -4.1500306 -4.2037625 -4.2514367 -4.2932177 -4.3283873 -4.3522983 -4.3575478 -4.3435631][-4.2412233 -4.1921768 -4.1377854 -4.0875325 -4.0481124 -4.0267143 -4.0485234 -4.1149983 -4.1954637 -4.2574835 -4.3044786 -4.3405938 -4.3638792 -4.3665247 -4.3487997][-4.2208509 -4.1614804 -4.0935316 -4.0230603 -3.9630284 -3.935817 -3.9754007 -4.0683532 -4.16932 -4.2445121 -4.2980337 -4.3390574 -4.3657804 -4.3696332 -4.3512721][-4.2034421 -4.1450434 -4.072279 -3.9915581 -3.9177885 -3.8837166 -3.9272783 -4.0264468 -4.1323538 -4.2163148 -4.2761073 -4.322629 -4.3530846 -4.360342 -4.3470573][-4.1917138 -4.1446786 -4.0769639 -4.0014267 -3.9343169 -3.9042816 -3.9371731 -4.01733 -4.1079454 -4.1875229 -4.2472553 -4.2944 -4.3274961 -4.3395171 -4.333385][-4.191628 -4.1581168 -4.1017179 -4.039196 -3.9882412 -3.9651465 -3.9828815 -4.0349984 -4.1028814 -4.1712461 -4.2234678 -4.2640305 -4.2942853 -4.3090935 -4.3096972][-4.1872792 -4.163475 -4.1188769 -4.0700512 -4.0327892 -4.0153966 -4.0233164 -4.0518932 -4.0996 -4.1562462 -4.1997986 -4.2321334 -4.2579951 -4.2739539 -4.2793074][-4.179306 -4.1605339 -4.1244545 -4.0818629 -4.049068 -4.0312 -4.0289164 -4.0426116 -4.077847 -4.1271381 -4.1672826 -4.194241 -4.2157264 -4.2306037 -4.2403383][-4.1834512 -4.1676307 -4.1359577 -4.0953236 -4.0601339 -4.0344958 -4.0185623 -4.0170722 -4.0388784 -4.0806122 -4.11886 -4.1461725 -4.1672316 -4.1828218 -4.1956649][-4.2116413 -4.2010036 -4.1735029 -4.1340184 -4.095139 -4.0601459 -4.0294938 -4.0079393 -4.0073428 -4.0296359 -4.0561786 -4.0825133 -4.1069164 -4.12448 -4.1386232][-4.2494178 -4.2445631 -4.2245355 -4.1926889 -4.1571097 -4.1198812 -4.0823293 -4.0475836 -4.0246282 -4.0189314 -4.0202513 -4.031105 -4.0474396 -4.059865 -4.0719109]]...]
INFO - root - 2017-12-06 00:57:34.622756: step 60810, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 64h:00m:22s remains)
INFO - root - 2017-12-06 00:57:43.129084: step 60820, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 62h:57m:34s remains)
INFO - root - 2017-12-06 00:57:51.840983: step 60830, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.874 sec/batch; 65h:59m:02s remains)
INFO - root - 2017-12-06 00:58:00.391572: step 60840, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 65h:37m:08s remains)
INFO - root - 2017-12-06 00:58:08.924635: step 60850, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 65h:10m:30s remains)
INFO - root - 2017-12-06 00:58:17.456877: step 60860, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 64h:38m:32s remains)
INFO - root - 2017-12-06 00:58:25.747224: step 60870, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 62h:22m:52s remains)
INFO - root - 2017-12-06 00:58:34.171231: step 60880, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.826 sec/batch; 62h:17m:38s remains)
INFO - root - 2017-12-06 00:58:42.622425: step 60890, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 64h:04m:54s remains)
INFO - root - 2017-12-06 00:58:51.164897: step 60900, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 64h:29m:49s remains)
2017-12-06 00:58:51.955542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1397576 -4.1589146 -4.1956491 -4.2374144 -4.2764874 -4.3066797 -4.3073659 -4.2907047 -4.2712369 -4.2514067 -4.2320213 -4.214653 -4.203609 -4.1998277 -4.1924391][-4.1614718 -4.1802444 -4.2066746 -4.2401185 -4.2755017 -4.3002129 -4.2951889 -4.2752585 -4.259551 -4.247818 -4.2350841 -4.2183156 -4.2038417 -4.1970677 -4.1942229][-4.1780772 -4.1969647 -4.2164965 -4.2400665 -4.2651157 -4.2759643 -4.2563968 -4.2259846 -4.2137246 -4.2174888 -4.219666 -4.2123375 -4.1966329 -4.17992 -4.1741934][-4.19456 -4.2153492 -4.2295885 -4.24053 -4.2462621 -4.2342329 -4.1895061 -4.14386 -4.1410871 -4.1722646 -4.2023296 -4.2084594 -4.1877208 -4.1535454 -4.135324][-4.2141824 -4.2376132 -4.2406778 -4.2325792 -4.2179046 -4.1772537 -4.0963206 -4.0251336 -4.0391722 -4.11183 -4.1771722 -4.20523 -4.1886244 -4.1438794 -4.1134791][-4.235507 -4.2573996 -4.24581 -4.2189012 -4.1793213 -4.1067829 -3.9798834 -3.8720212 -3.9008832 -4.0257077 -4.1355262 -4.1911454 -4.1943374 -4.1604438 -4.1296139][-4.2516127 -4.265327 -4.244525 -4.2035222 -4.13732 -4.0240955 -3.8429108 -3.6928737 -3.7442403 -3.9318523 -4.0862675 -4.1647353 -4.1896596 -4.1762958 -4.1538796][-4.25862 -4.2597671 -4.2319984 -4.1841459 -4.0968943 -3.9448857 -3.7230902 -3.5439234 -3.6184466 -3.858654 -4.045867 -4.1362071 -4.1716394 -4.1711268 -4.1565814][-4.2617726 -4.255177 -4.2265315 -4.1782646 -4.0890255 -3.93811 -3.7338362 -3.5761566 -3.6490147 -3.87401 -4.049253 -4.1306286 -4.1667175 -4.1714745 -4.1593561][-4.2683759 -4.2596726 -4.234901 -4.1909142 -4.11874 -4.0068765 -3.8692625 -3.7717402 -3.8250556 -3.9790859 -4.0975113 -4.1495891 -4.1784248 -4.1845818 -4.174974][-4.2796526 -4.2712283 -4.2496276 -4.214622 -4.1662717 -4.0989628 -4.026011 -3.9769924 -4.008152 -4.0941467 -4.1563082 -4.1789975 -4.1948328 -4.1979046 -4.1895561][-4.2808356 -4.2753015 -4.2588139 -4.2346005 -4.2066474 -4.1748981 -4.1455917 -4.1223621 -4.1347294 -4.1763425 -4.2027659 -4.2041812 -4.2035794 -4.2000079 -4.1909375][-4.2678385 -4.2675614 -4.2584739 -4.2462983 -4.2327695 -4.2239432 -4.2203975 -4.2145538 -4.2161384 -4.228116 -4.2335472 -4.22639 -4.2160292 -4.2061996 -4.1944184][-4.2601051 -4.2648897 -4.2627435 -4.2563858 -4.2477574 -4.245379 -4.24934 -4.2529125 -4.2534852 -4.2551279 -4.2554297 -4.2484326 -4.2372847 -4.2264819 -4.2157683][-4.2765784 -4.281 -4.2775087 -4.27025 -4.2612557 -4.25711 -4.259685 -4.2663713 -4.2705851 -4.2727151 -4.2748041 -4.2736382 -4.2675529 -4.2629766 -4.2562046]]...]
INFO - root - 2017-12-06 00:59:00.365405: step 60910, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 63h:24m:29s remains)
INFO - root - 2017-12-06 00:59:08.996430: step 60920, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 65h:26m:24s remains)
INFO - root - 2017-12-06 00:59:17.573754: step 60930, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 65h:53m:10s remains)
INFO - root - 2017-12-06 00:59:26.183813: step 60940, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 65h:37m:55s remains)
INFO - root - 2017-12-06 00:59:34.774639: step 60950, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.867 sec/batch; 65h:24m:34s remains)
INFO - root - 2017-12-06 00:59:43.279107: step 60960, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 63h:31m:40s remains)
INFO - root - 2017-12-06 00:59:51.778997: step 60970, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 65h:35m:05s remains)
INFO - root - 2017-12-06 01:00:00.370649: step 60980, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.892 sec/batch; 67h:16m:10s remains)
INFO - root - 2017-12-06 01:00:09.038367: step 60990, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 64h:26m:33s remains)
INFO - root - 2017-12-06 01:00:17.668122: step 61000, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 65h:59m:54s remains)
2017-12-06 01:00:18.417400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2680306 -4.26733 -4.2662373 -4.2686081 -4.2683325 -4.2703161 -4.274262 -4.2733335 -4.2723818 -4.2733512 -4.2729478 -4.2707934 -4.2696242 -4.2693377 -4.2701063][-4.2769294 -4.2815676 -4.2836962 -4.2871342 -4.2872825 -4.28765 -4.2890005 -4.2863441 -4.2831759 -4.28143 -4.2785449 -4.2749343 -4.2723279 -4.2708821 -4.2711577][-4.2816133 -4.2891359 -4.2929182 -4.295464 -4.2944512 -4.293746 -4.295269 -4.2934442 -4.2899308 -4.287148 -4.2834425 -4.2793293 -4.2755857 -4.2732368 -4.2732363][-4.2760096 -4.2823596 -4.2855277 -4.2861176 -4.2830133 -4.2815452 -4.2854347 -4.2879949 -4.2879214 -4.2867002 -4.2837796 -4.2799988 -4.2759647 -4.2736554 -4.2739949][-4.2649508 -4.2672758 -4.2669072 -4.2634487 -4.2560735 -4.25069 -4.2554021 -4.2657218 -4.2738771 -4.2775326 -4.2778087 -4.2760057 -4.2735295 -4.2725711 -4.27362][-4.2562718 -4.2545867 -4.250545 -4.2435441 -4.2311497 -4.2186904 -4.2206573 -4.2373476 -4.25415 -4.2635865 -4.2672834 -4.2685375 -4.2687893 -4.2697511 -4.2708626][-4.2528648 -4.2492838 -4.2430143 -4.23499 -4.2213316 -4.205523 -4.2032847 -4.2197466 -4.2402744 -4.2526064 -4.2579126 -4.2609892 -4.2634554 -4.2653074 -4.2653][-4.2531347 -4.2493992 -4.2420449 -4.2342081 -4.2230368 -4.2089458 -4.2031789 -4.2133064 -4.2307539 -4.2421794 -4.2472668 -4.2516308 -4.2561626 -4.2589264 -4.2581391][-4.2551727 -4.2520308 -4.2445312 -4.2369862 -4.2289414 -4.218699 -4.2114491 -4.2142315 -4.2252617 -4.2333074 -4.2370992 -4.2418818 -4.2478395 -4.2514882 -4.2506113][-4.2574968 -4.2548857 -4.2480426 -4.2410431 -4.2354546 -4.2289963 -4.2226367 -4.2206311 -4.2253275 -4.2293448 -4.2312775 -4.23542 -4.24144 -4.2455416 -4.2459841][-4.2592506 -4.2578168 -4.2527056 -4.2471633 -4.243773 -4.2407866 -4.2364879 -4.2324352 -4.2319708 -4.2327576 -4.2334919 -4.2364092 -4.2409945 -4.24492 -4.2472081][-4.2610741 -4.2612453 -4.2585592 -4.2550273 -4.2530656 -4.25201 -4.249424 -4.2454772 -4.2429056 -4.242415 -4.2430897 -4.2451797 -4.2482634 -4.2512188 -4.2540455][-4.2618384 -4.2628603 -4.26191 -4.2601705 -4.259119 -4.25912 -4.258234 -4.2559295 -4.2541518 -4.2540507 -4.2550759 -4.2568479 -4.2590346 -4.2610316 -4.2628036][-4.26145 -4.2623587 -4.2620668 -4.2613854 -4.2612095 -4.2620716 -4.2626295 -4.2623978 -4.2621613 -4.2627516 -4.2639985 -4.265502 -4.266892 -4.2679248 -4.2683282][-4.2611394 -4.2618012 -4.2615571 -4.2612805 -4.261477 -4.2624044 -4.2633328 -4.2639365 -4.2643681 -4.2650447 -4.2660489 -4.2670031 -4.2676144 -4.2677717 -4.26741]]...]
INFO - root - 2017-12-06 01:00:26.918487: step 61010, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 64h:04m:27s remains)
INFO - root - 2017-12-06 01:00:35.447191: step 61020, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 65h:44m:16s remains)
INFO - root - 2017-12-06 01:00:44.037204: step 61030, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 64h:48m:41s remains)
INFO - root - 2017-12-06 01:00:52.543671: step 61040, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 62h:41m:54s remains)
INFO - root - 2017-12-06 01:01:01.137660: step 61050, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 67h:13m:44s remains)
INFO - root - 2017-12-06 01:01:09.711786: step 61060, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 64h:19m:39s remains)
INFO - root - 2017-12-06 01:01:18.240841: step 61070, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 63h:42m:49s remains)
INFO - root - 2017-12-06 01:01:26.778015: step 61080, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.798 sec/batch; 60h:09m:32s remains)
INFO - root - 2017-12-06 01:01:35.436838: step 61090, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.814 sec/batch; 61h:19m:53s remains)
INFO - root - 2017-12-06 01:01:44.031032: step 61100, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 64h:35m:41s remains)
2017-12-06 01:01:44.797905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2169814 -4.2259307 -4.233428 -4.2307954 -4.2263231 -4.2330561 -4.2467384 -4.254106 -4.2581134 -4.2619753 -4.262044 -4.2606707 -4.2653413 -4.2708158 -4.2662058][-4.2062664 -4.2097826 -4.2125525 -4.20593 -4.2006373 -4.2007737 -4.204968 -4.2125564 -4.2184439 -4.22118 -4.2212071 -4.2223854 -4.2300053 -4.238534 -4.2363892][-4.1902843 -4.1917043 -4.192265 -4.1832352 -4.1771579 -4.1738029 -4.1724181 -4.18429 -4.1942277 -4.1960425 -4.1963973 -4.2003512 -4.2080398 -4.2153511 -4.215055][-4.162672 -4.1623869 -4.1644945 -4.1591616 -4.1540165 -4.1491723 -4.1441803 -4.1610594 -4.178565 -4.186307 -4.1898065 -4.1959724 -4.2020841 -4.2038255 -4.2018085][-4.1312132 -4.1274376 -4.1343303 -4.1329722 -4.1241994 -4.1135788 -4.1033936 -4.1244545 -4.1519713 -4.1676168 -4.1788864 -4.1900544 -4.1896324 -4.1818523 -4.1769009][-4.1184611 -4.1108274 -4.1188517 -4.1155367 -4.0938735 -4.0635457 -4.034874 -4.0571694 -4.1054749 -4.1382723 -4.16172 -4.1768656 -4.172123 -4.1569576 -4.1503215][-4.1192169 -4.1089363 -4.1134295 -4.1048036 -4.0693865 -4.0117311 -3.9593468 -3.981447 -4.0543337 -4.1093235 -4.1432405 -4.161057 -4.1574783 -4.1399808 -4.1301389][-4.1189938 -4.1070018 -4.1075678 -4.0945063 -4.051651 -3.9845233 -3.9257557 -3.9477816 -4.0304732 -4.0996041 -4.1375294 -4.1545835 -4.1508965 -4.1320662 -4.119205][-4.1278009 -4.1250324 -4.1333103 -4.1275768 -4.1003771 -4.0560017 -4.0199294 -4.0336375 -4.093267 -4.1521468 -4.190063 -4.2084608 -4.2055283 -4.1783986 -4.1553741][-4.1542163 -4.1656876 -4.185041 -4.1900206 -4.1819339 -4.1628151 -4.1492429 -4.1566982 -4.1906629 -4.2284 -4.2544522 -4.2714524 -4.2677994 -4.2425213 -4.2162809][-4.1870308 -4.20538 -4.2244358 -4.2337012 -4.2350359 -4.2287383 -4.2239985 -4.2240615 -4.240232 -4.2641387 -4.2835193 -4.299633 -4.2960234 -4.2735782 -4.2459593][-4.197937 -4.2147441 -4.2324271 -4.2453065 -4.2510452 -4.2509727 -4.2463503 -4.2392621 -4.243855 -4.2577634 -4.274004 -4.2904215 -4.2875538 -4.2688994 -4.2465806][-4.1994491 -4.2161865 -4.2355409 -4.2517362 -4.2596745 -4.2595129 -4.2562389 -4.248373 -4.2488165 -4.2584891 -4.2716336 -4.2858453 -4.2847304 -4.2714825 -4.256197][-4.211628 -4.2284074 -4.2483597 -4.2648678 -4.2740579 -4.2745247 -4.271421 -4.26622 -4.2660823 -4.272068 -4.2825608 -4.29605 -4.2972965 -4.2895675 -4.2808914][-4.2430224 -4.2554646 -4.2704377 -4.2813168 -4.2870016 -4.2875824 -4.2856655 -4.280942 -4.2789125 -4.2841444 -4.2934365 -4.3032718 -4.3041849 -4.2980185 -4.2918391]]...]
INFO - root - 2017-12-06 01:01:53.389106: step 61110, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 63h:17m:59s remains)
INFO - root - 2017-12-06 01:02:01.887961: step 61120, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 63h:30m:19s remains)
INFO - root - 2017-12-06 01:02:10.436457: step 61130, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 64h:57m:57s remains)
INFO - root - 2017-12-06 01:02:18.917745: step 61140, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 66h:13m:14s remains)
INFO - root - 2017-12-06 01:02:27.543988: step 61150, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 63h:41m:46s remains)
INFO - root - 2017-12-06 01:02:36.096696: step 61160, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 62h:25m:59s remains)
INFO - root - 2017-12-06 01:02:44.575485: step 61170, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 68h:16m:00s remains)
INFO - root - 2017-12-06 01:02:53.365941: step 61180, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 66h:05m:38s remains)
INFO - root - 2017-12-06 01:03:01.901476: step 61190, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 64h:35m:20s remains)
INFO - root - 2017-12-06 01:03:10.391926: step 61200, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.799 sec/batch; 60h:11m:27s remains)
2017-12-06 01:03:11.247719: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1711369 -4.1968842 -4.2057219 -4.2026262 -4.18834 -4.1691203 -4.1614518 -4.1735663 -4.1983252 -4.220449 -4.2365451 -4.2412844 -4.2395787 -4.232686 -4.232904][-4.1639009 -4.1939449 -4.2062078 -4.2077918 -4.1946311 -4.1687365 -4.1507444 -4.1552663 -4.174387 -4.194241 -4.2136006 -4.2223082 -4.2259145 -4.2255869 -4.2312279][-4.1697679 -4.2081556 -4.2251067 -4.2300339 -4.2164378 -4.1835752 -4.1485257 -4.1342773 -4.1450386 -4.1692123 -4.1978221 -4.2145967 -4.2199287 -4.2254515 -4.236187][-4.2026143 -4.2408857 -4.2550821 -4.2578731 -4.2404666 -4.19764 -4.138607 -4.1033359 -4.1081772 -4.1415577 -4.1805034 -4.2065511 -4.2142463 -4.2240286 -4.2376084][-4.2226577 -4.2415061 -4.241292 -4.23252 -4.2057095 -4.1545734 -4.07829 -4.032155 -4.0399437 -4.0895166 -4.1405635 -4.1732416 -4.1854644 -4.1978874 -4.2126541][-4.2282467 -4.22374 -4.2032337 -4.1741719 -4.1285272 -4.065815 -3.9743402 -3.9133363 -3.9235091 -4.003087 -4.08457 -4.1377306 -4.1633358 -4.1813912 -4.1932249][-4.2395439 -4.2249255 -4.191391 -4.1424823 -4.0749478 -3.9879794 -3.8686063 -3.7749205 -3.7861698 -3.9026151 -4.0240269 -4.1116195 -4.1556706 -4.1796436 -4.1922169][-4.2582283 -4.2454991 -4.207552 -4.148922 -4.0691004 -3.9605372 -3.8089373 -3.6786308 -3.6897297 -3.8317237 -3.9781494 -4.0890117 -4.148809 -4.1818247 -4.2021537][-4.2599521 -4.2483377 -4.2171865 -4.17302 -4.1089931 -4.0082 -3.8653827 -3.7300346 -3.7242792 -3.8417294 -3.9722309 -4.0840015 -4.1527905 -4.1913733 -4.2138934][-4.2408338 -4.2266135 -4.203649 -4.1767592 -4.1453948 -4.0810251 -3.9800668 -3.8773851 -3.8499713 -3.9101372 -4.0002508 -4.0993462 -4.166316 -4.2039371 -4.2198634][-4.2150354 -4.1909513 -4.1673565 -4.1489382 -4.1404819 -4.1153374 -4.0684967 -4.0107369 -3.9820094 -4.001863 -4.0545115 -4.1288352 -4.1803064 -4.2070193 -4.2165618][-4.1854253 -4.1467948 -4.10964 -4.0887227 -4.0926957 -4.10209 -4.102809 -4.0938635 -4.0844665 -4.0955162 -4.1240888 -4.1676435 -4.1937442 -4.2089334 -4.2213569][-4.147573 -4.0945845 -4.0471773 -4.0230956 -4.0355167 -4.07311 -4.1079 -4.1325049 -4.1442571 -4.1582203 -4.1742678 -4.1837153 -4.1805525 -4.1877913 -4.2068806][-4.1335368 -4.0633044 -4.0091519 -3.9925823 -4.0188675 -4.0779157 -4.1281643 -4.1640553 -4.1844978 -4.1989446 -4.2012157 -4.1841068 -4.1585393 -4.1566873 -4.1782827][-4.1441903 -4.0698996 -4.0201864 -4.0172095 -4.0534954 -4.1137338 -4.1630979 -4.1956358 -4.2155495 -4.2232418 -4.2135534 -4.1853642 -4.1536026 -4.1421127 -4.1553144]]...]
INFO - root - 2017-12-06 01:03:19.797692: step 61210, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 64h:42m:30s remains)
INFO - root - 2017-12-06 01:03:28.307987: step 61220, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 63h:50m:33s remains)
INFO - root - 2017-12-06 01:03:36.831650: step 61230, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 66h:04m:42s remains)
INFO - root - 2017-12-06 01:03:45.234006: step 61240, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 65h:08m:18s remains)
INFO - root - 2017-12-06 01:03:53.834363: step 61250, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 65h:08m:18s remains)
INFO - root - 2017-12-06 01:04:02.360850: step 61260, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 64h:28m:29s remains)
INFO - root - 2017-12-06 01:04:10.854652: step 61270, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 63h:37m:47s remains)
INFO - root - 2017-12-06 01:04:19.513760: step 61280, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 63h:55m:13s remains)
INFO - root - 2017-12-06 01:04:28.108212: step 61290, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 63h:54m:01s remains)
INFO - root - 2017-12-06 01:04:36.725303: step 61300, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 66h:03m:09s remains)
2017-12-06 01:04:37.531213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1427779 -4.1496549 -4.1631851 -4.1786118 -4.189312 -4.1906385 -4.1856017 -4.1842189 -4.1838622 -4.1809683 -4.1801467 -4.19257 -4.2144022 -4.2359562 -4.2585773][-4.1321383 -4.1377792 -4.150588 -4.1657586 -4.1752739 -4.1736212 -4.1630244 -4.1568289 -4.1532626 -4.1489363 -4.1485953 -4.1646328 -4.1903048 -4.2151408 -4.24146][-4.1400404 -4.1466541 -4.1570396 -4.1673164 -4.1728907 -4.1661921 -4.1500235 -4.1384516 -4.1352091 -4.1351814 -4.1394238 -4.1586008 -4.1858492 -4.2117085 -4.2391453][-4.1590672 -4.1668267 -4.1717782 -4.1741753 -4.1701117 -4.156208 -4.1366735 -4.1259184 -4.12936 -4.1381979 -4.1485252 -4.1668978 -4.1895385 -4.2114024 -4.2368455][-4.1663165 -4.1727567 -4.1747227 -4.1666102 -4.1459942 -4.1214509 -4.103796 -4.1012373 -4.1177678 -4.1417375 -4.1600542 -4.17618 -4.19219 -4.2062092 -4.2264709][-4.1595621 -4.16423 -4.161902 -4.1421928 -4.1040392 -4.0685253 -4.0516152 -4.058145 -4.087956 -4.133348 -4.167304 -4.1876197 -4.1988597 -4.2034683 -4.215148][-4.1467142 -4.147891 -4.1407218 -4.1108961 -4.0616279 -4.0195723 -4.0014491 -4.0087614 -4.0405917 -4.0992036 -4.1477008 -4.1733432 -4.1795411 -4.179132 -4.1861024][-4.1062541 -4.1045752 -4.0968728 -4.0650892 -4.0159326 -3.9743681 -3.9556231 -3.960022 -3.9888129 -4.0530071 -4.1114154 -4.1405954 -4.1424866 -4.141994 -4.15277][-4.0836749 -4.0855994 -4.0883956 -4.0737934 -4.0396266 -4.0064464 -3.990099 -3.9924295 -4.01659 -4.0712996 -4.1216941 -4.1449509 -4.140954 -4.1396346 -4.1518717][-4.1182337 -4.12988 -4.1466632 -4.15172 -4.1331005 -4.1072845 -4.0920873 -4.0889506 -4.1024752 -4.1373487 -4.1709118 -4.1857042 -4.1787376 -4.1734695 -4.1809759][-4.180357 -4.1969552 -4.2174091 -4.2264009 -4.2113233 -4.1871667 -4.1704555 -4.1662178 -4.1760325 -4.1980128 -4.2163844 -4.2239466 -4.2187696 -4.2148013 -4.219358][-4.230454 -4.2463021 -4.263032 -4.268384 -4.251749 -4.2294192 -4.2140703 -4.213428 -4.226191 -4.2426496 -4.2532167 -4.2553253 -4.2510743 -4.2508712 -4.2557945][-4.2612038 -4.2745872 -4.2873197 -4.2900472 -4.2749667 -4.2555084 -4.2409549 -4.2403717 -4.2529683 -4.2703733 -4.2843 -4.2876124 -4.2851682 -4.2875018 -4.2919245][-4.26663 -4.2778435 -4.2878289 -4.2912083 -4.2828107 -4.2715478 -4.2625365 -4.2618623 -4.2742248 -4.2922125 -4.3058472 -4.3092589 -4.3075752 -4.3089461 -4.3107786][-4.2611709 -4.2697139 -4.2781324 -4.282146 -4.2788992 -4.2741203 -4.269927 -4.2688894 -4.2798529 -4.2964935 -4.3071289 -4.3094196 -4.3086061 -4.3101997 -4.3104715]]...]
INFO - root - 2017-12-06 01:04:46.008810: step 61310, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 63h:59m:51s remains)
INFO - root - 2017-12-06 01:04:54.565893: step 61320, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 62h:27m:59s remains)
INFO - root - 2017-12-06 01:05:03.088548: step 61330, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 63h:33m:28s remains)
INFO - root - 2017-12-06 01:05:11.755661: step 61340, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.863 sec/batch; 64h:59m:59s remains)
INFO - root - 2017-12-06 01:05:20.177524: step 61350, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 64h:30m:19s remains)
INFO - root - 2017-12-06 01:05:28.690016: step 61360, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 64h:57m:48s remains)
INFO - root - 2017-12-06 01:05:37.150408: step 61370, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 65h:11m:22s remains)
INFO - root - 2017-12-06 01:05:45.592271: step 61380, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 63h:49m:22s remains)
INFO - root - 2017-12-06 01:05:54.203117: step 61390, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.861 sec/batch; 64h:48m:10s remains)
INFO - root - 2017-12-06 01:06:02.627035: step 61400, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 0.784 sec/batch; 59h:00m:07s remains)
2017-12-06 01:06:03.467982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2699094 -4.2606688 -4.2573462 -4.2627583 -4.2682033 -4.2689018 -4.2713447 -4.2756572 -4.2758265 -4.2750478 -4.277195 -4.2789268 -4.2791753 -4.2812729 -4.2850347][-4.2741 -4.2637472 -4.2604117 -4.26639 -4.2705264 -4.27205 -4.2777224 -4.2844553 -4.2849073 -4.2826648 -4.2816381 -4.2810917 -4.2794428 -4.2798648 -4.283102][-4.2535644 -4.2425389 -4.2400188 -4.248744 -4.2555556 -4.2620769 -4.2748027 -4.2866812 -4.2895212 -4.2876158 -4.2845817 -4.2820158 -4.2794857 -4.2801137 -4.2842178][-4.2075496 -4.1987715 -4.1979136 -4.208878 -4.2183108 -4.2311106 -4.2521582 -4.2705545 -4.2774343 -4.276319 -4.2722011 -4.2687597 -4.2667418 -4.269475 -4.2769461][-4.1509891 -4.1467147 -4.1475868 -4.1574287 -4.1634407 -4.1772623 -4.2036881 -4.2279205 -4.239872 -4.2416677 -4.2382951 -4.234117 -4.2326407 -4.2395182 -4.2514305][-4.109869 -4.109108 -4.1080265 -4.1086774 -4.0999045 -4.1039505 -4.1267352 -4.1545658 -4.1745772 -4.1826792 -4.1821485 -4.1753674 -4.1741104 -4.1861782 -4.2042646][-4.1045985 -4.0989919 -4.0877714 -4.0712113 -4.0401449 -4.0239353 -4.031918 -4.0578103 -4.0877824 -4.1060886 -4.1130319 -4.1069961 -4.1060829 -4.1208143 -4.1398973][-4.1398869 -4.1243134 -4.1010246 -4.0691876 -4.0200224 -3.9821939 -3.9655523 -3.977222 -4.0119443 -4.0390329 -4.0539021 -4.0524521 -4.0527081 -4.0632358 -4.0729604][-4.1964912 -4.1756525 -4.1468024 -4.1106448 -4.0593748 -4.0150852 -3.9822664 -3.9755692 -4.0015593 -4.0249476 -4.0390034 -4.0368834 -4.0331249 -4.0319366 -4.0234532][-4.2472291 -4.2302566 -4.20464 -4.17406 -4.1338286 -4.0994277 -4.0689583 -4.0562315 -4.0672007 -4.0764961 -4.080904 -4.0722065 -4.0628371 -4.0489607 -4.0227265][-4.2667994 -4.2614579 -4.2472968 -4.2282414 -4.2025609 -4.1811953 -4.1596408 -4.1473227 -4.1465383 -4.1463714 -4.1448832 -4.1315393 -4.1196618 -4.1016479 -4.0699511][-4.2582626 -4.2617579 -4.2580752 -4.25047 -4.2368612 -4.2248311 -4.2117343 -4.2012649 -4.1948586 -4.1931014 -4.1930857 -4.1831284 -4.17432 -4.1577563 -4.1294937][-4.2392187 -4.2442741 -4.2448177 -4.2439394 -4.2397609 -4.2359786 -4.2306433 -4.2232456 -4.215548 -4.2159319 -4.21953 -4.2161989 -4.2126555 -4.2016239 -4.1813025][-4.226202 -4.2314215 -4.2329421 -4.23452 -4.2349081 -4.2352962 -4.2362504 -4.2334781 -4.2278156 -4.2307744 -4.2368221 -4.2371397 -4.235343 -4.2293615 -4.2179055][-4.2375388 -4.2445235 -4.2474656 -4.2499657 -4.2518024 -4.2539558 -4.258 -4.2587709 -4.2556033 -4.2587757 -4.2641096 -4.2645493 -4.2614121 -4.2563105 -4.2504125]]...]
INFO - root - 2017-12-06 01:06:11.970241: step 61410, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.828 sec/batch; 62h:21m:55s remains)
INFO - root - 2017-12-06 01:06:20.401626: step 61420, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 65h:17m:56s remains)
INFO - root - 2017-12-06 01:06:28.968154: step 61430, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 65h:10m:32s remains)
INFO - root - 2017-12-06 01:06:37.572137: step 61440, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.842 sec/batch; 63h:25m:52s remains)
INFO - root - 2017-12-06 01:06:46.087009: step 61450, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 63h:47m:31s remains)
INFO - root - 2017-12-06 01:06:54.493866: step 61460, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 64h:51m:09s remains)
INFO - root - 2017-12-06 01:07:02.876783: step 61470, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 62h:14m:56s remains)
INFO - root - 2017-12-06 01:07:11.441936: step 61480, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 64h:02m:31s remains)
INFO - root - 2017-12-06 01:07:19.968428: step 61490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 64h:50m:18s remains)
INFO - root - 2017-12-06 01:07:28.612121: step 61500, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 64h:45m:16s remains)
2017-12-06 01:07:29.409348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3079634 -4.2990222 -4.29736 -4.3020549 -4.3085265 -4.3118572 -4.3135915 -4.316709 -4.322165 -4.3260846 -4.3273849 -4.3238907 -4.3196659 -4.316915 -4.3179507][-4.2803473 -4.2692537 -4.2684212 -4.2745852 -4.2824707 -4.286437 -4.2883 -4.2932382 -4.3045006 -4.3131518 -4.3181753 -4.3118954 -4.3038497 -4.2970619 -4.2980118][-4.2567906 -4.2441812 -4.2413416 -4.2466569 -4.2541366 -4.2559986 -4.2512536 -4.2545066 -4.2742076 -4.2931304 -4.3031216 -4.2975068 -4.2885013 -4.2804708 -4.2821989][-4.2447543 -4.2265429 -4.21131 -4.2084761 -4.2148714 -4.2108746 -4.1911964 -4.1930847 -4.2252784 -4.25683 -4.2704325 -4.2663193 -4.26079 -4.2558537 -4.2579436][-4.2345638 -4.2003646 -4.166563 -4.1535668 -4.15901 -4.1466794 -4.109993 -4.1119785 -4.1589007 -4.2039351 -4.2204452 -4.21664 -4.2148719 -4.215889 -4.2170582][-4.2197618 -4.1681886 -4.1168127 -4.0972486 -4.0987072 -4.0740304 -4.0190911 -4.0123806 -4.0748353 -4.1357226 -4.1545372 -4.14934 -4.14969 -4.1576881 -4.1574788][-4.202724 -4.1361923 -4.0696507 -4.0385303 -4.0254278 -3.9823639 -3.9051175 -3.8813958 -3.952744 -4.0350847 -4.0688925 -4.0733008 -4.0815597 -4.0970092 -4.1015196][-4.1958413 -4.1193452 -4.04265 -3.9974589 -3.9606729 -3.8907781 -3.7758489 -3.7157831 -3.8020222 -3.9220843 -3.9879084 -4.0179238 -4.0370855 -4.0488224 -4.0547781][-4.1950526 -4.1181464 -4.0440063 -3.9987483 -3.9585161 -3.8821957 -3.7437396 -3.6523561 -3.7527475 -3.8997409 -3.9821196 -4.0209551 -4.035995 -4.0396652 -4.0463386][-4.1979408 -4.1259747 -4.0593381 -4.0222735 -3.9974012 -3.9465284 -3.8514204 -3.8035431 -3.8870173 -3.9931448 -4.0498323 -4.0693674 -4.0679927 -4.0622935 -4.0665054][-4.2176065 -4.1585913 -4.1030512 -4.0697541 -4.0468378 -4.0222373 -3.9851909 -3.9777408 -4.0375872 -4.0937033 -4.1189666 -4.1233349 -4.1137238 -4.0999842 -4.0984964][-4.2443166 -4.1992836 -4.155364 -4.1274557 -4.1094079 -4.1003895 -4.0949535 -4.1043005 -4.1395369 -4.167315 -4.1755896 -4.1760087 -4.1669989 -4.1550326 -4.1543555][-4.270278 -4.235611 -4.2040167 -4.1908159 -4.1862903 -4.1867843 -4.1902475 -4.1964974 -4.2136779 -4.2277007 -4.230124 -4.2254953 -4.2170448 -4.2144551 -4.2227921][-4.2914376 -4.2681203 -4.2493472 -4.2474742 -4.2533884 -4.2596617 -4.2634845 -4.264606 -4.272469 -4.27923 -4.2781787 -4.2709546 -4.2656369 -4.2708941 -4.2827148][-4.3093462 -4.2972636 -4.2896638 -4.2908559 -4.2944078 -4.2991748 -4.3031921 -4.3067183 -4.3099995 -4.3099871 -4.3052111 -4.2989793 -4.2985239 -4.3062372 -4.31535]]...]
INFO - root - 2017-12-06 01:07:37.860006: step 61510, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 64h:24m:43s remains)
INFO - root - 2017-12-06 01:07:46.470860: step 61520, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.818 sec/batch; 61h:35m:11s remains)
INFO - root - 2017-12-06 01:07:55.001150: step 61530, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 66h:18m:52s remains)
INFO - root - 2017-12-06 01:08:03.558389: step 61540, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 64h:03m:49s remains)
INFO - root - 2017-12-06 01:08:12.080515: step 61550, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 63h:49m:23s remains)
INFO - root - 2017-12-06 01:08:20.612849: step 61560, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 63h:19m:54s remains)
INFO - root - 2017-12-06 01:08:29.060807: step 61570, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 66h:02m:22s remains)
INFO - root - 2017-12-06 01:08:37.667571: step 61580, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 65h:09m:33s remains)
INFO - root - 2017-12-06 01:08:46.205621: step 61590, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 65h:57m:42s remains)
INFO - root - 2017-12-06 01:08:54.773053: step 61600, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 63h:27m:24s remains)
2017-12-06 01:08:55.624771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1490979 -4.1824484 -4.2092328 -4.2271571 -4.238409 -4.2407689 -4.237009 -4.2329125 -4.2288065 -4.2230091 -4.2136192 -4.2047119 -4.1934552 -4.1832194 -4.175622][-4.1116185 -4.1639481 -4.2150273 -4.2592134 -4.2924519 -4.3075581 -4.30874 -4.3023849 -4.2900763 -4.2735777 -4.2521648 -4.2311921 -4.2079725 -4.1839271 -4.1592216][-4.1416364 -4.1897 -4.2406473 -4.2868843 -4.3221955 -4.3382888 -4.3414812 -4.3374557 -4.3265376 -4.3114405 -4.291667 -4.2728815 -4.2526445 -4.2293096 -4.2015586][-4.2352586 -4.2585382 -4.283392 -4.3050523 -4.3206043 -4.32585 -4.3271217 -4.3272333 -4.324224 -4.3185773 -4.3094697 -4.30197 -4.2954321 -4.2862568 -4.271698][-4.2941351 -4.2954764 -4.2968092 -4.2932653 -4.2859697 -4.2773223 -4.2736917 -4.2758551 -4.2810726 -4.2875257 -4.2916136 -4.296258 -4.3028188 -4.3077621 -4.3084292][-4.2927408 -4.2826524 -4.2717853 -4.254355 -4.2292671 -4.2038755 -4.1895361 -4.1919165 -4.2064128 -4.2255797 -4.241827 -4.2574263 -4.2740879 -4.2893004 -4.3013215][-4.2524862 -4.2343607 -4.2148423 -4.1874151 -4.1470852 -4.1024351 -4.0732584 -4.0744796 -4.0997295 -4.1311183 -4.1577368 -4.1826415 -4.2084494 -4.2343984 -4.2588406][-4.1987848 -4.1707191 -4.1383677 -4.0956559 -4.0390434 -3.9761717 -3.9330411 -3.9350784 -3.9705136 -4.0095596 -4.0411115 -4.072679 -4.1079416 -4.1466756 -4.1853943][-4.1788387 -4.1499085 -4.1160207 -4.0725532 -4.0169258 -3.9574389 -3.919081 -3.9229407 -3.9526405 -3.9806817 -4.0020447 -4.0259886 -4.0572009 -4.095459 -4.1361709][-4.2161055 -4.1991615 -4.1776128 -4.1492848 -4.1130991 -4.0780196 -4.0617914 -4.07274 -4.092464 -4.1057377 -4.1132383 -4.1206169 -4.1322293 -4.1484022 -4.1671219][-4.2712183 -4.2673211 -4.2605438 -4.2484956 -4.2307739 -4.2146783 -4.211431 -4.2223425 -4.2340021 -4.2402182 -4.2424569 -4.2420721 -4.2420812 -4.2422767 -4.2429352][-4.312561 -4.31603 -4.31757 -4.3145242 -4.3071737 -4.3004456 -4.2999334 -4.3057528 -4.3116708 -4.3145995 -4.315114 -4.3133736 -4.3103008 -4.3052773 -4.3000264][-4.3289623 -4.3337479 -4.3361344 -4.3345914 -4.3307271 -4.3269572 -4.3254843 -4.3263154 -4.3281894 -4.3303494 -4.3328915 -4.3337669 -4.3318887 -4.3272681 -4.3230195][-4.325707 -4.327805 -4.3264132 -4.321619 -4.3162422 -4.31117 -4.3074737 -4.3057284 -4.305634 -4.3090034 -4.3162913 -4.3221612 -4.3233557 -4.3214993 -4.3217793][-4.3142471 -4.312963 -4.3075805 -4.299602 -4.2923779 -4.2855353 -4.28038 -4.27757 -4.277 -4.2822571 -4.2931347 -4.3014174 -4.3033643 -4.3030925 -4.3067431]]...]
INFO - root - 2017-12-06 01:09:04.135423: step 61610, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 64h:07m:04s remains)
INFO - root - 2017-12-06 01:09:12.595771: step 61620, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 65h:24m:46s remains)
INFO - root - 2017-12-06 01:09:21.066379: step 61630, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 62h:46m:51s remains)
INFO - root - 2017-12-06 01:09:29.710140: step 61640, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 67h:50m:47s remains)
INFO - root - 2017-12-06 01:09:38.200701: step 61650, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 63h:33m:53s remains)
INFO - root - 2017-12-06 01:09:46.684147: step 61660, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.802 sec/batch; 60h:20m:49s remains)
INFO - root - 2017-12-06 01:09:55.223190: step 61670, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 63h:58m:10s remains)
INFO - root - 2017-12-06 01:10:03.741223: step 61680, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 63h:22m:20s remains)
INFO - root - 2017-12-06 01:10:12.333026: step 61690, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 65h:26m:43s remains)
INFO - root - 2017-12-06 01:10:20.968131: step 61700, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 64h:11m:12s remains)
2017-12-06 01:10:21.737106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0801773 -4.0197177 -3.9768813 -3.9729803 -3.9820712 -3.9914889 -4.0222816 -4.0661507 -4.0809145 -4.0800443 -4.086544 -4.108953 -4.1179543 -4.1101565 -4.0954933][-4.0615091 -3.9947221 -3.9559023 -3.960192 -3.9714344 -3.9750841 -3.9989171 -4.0353665 -4.042861 -4.0417542 -4.0553346 -4.0874953 -4.1020241 -4.0930662 -4.0783539][-4.04963 -3.9980845 -3.9734273 -3.9790223 -3.9796772 -3.9650328 -3.9802639 -4.0167985 -4.0311093 -4.0397005 -4.0582547 -4.0864863 -4.0887456 -4.0651264 -4.038506][-4.055738 -4.0237584 -4.0078249 -4.0064092 -3.9910429 -3.9603429 -3.9651768 -4.0016737 -4.0340939 -4.0621653 -4.0868983 -4.1064649 -4.0935435 -4.0598226 -4.0221949][-4.0700507 -4.0474243 -4.031724 -4.0269303 -4.0051866 -3.9730992 -3.9698341 -4.0030756 -4.0519257 -4.0973015 -4.1281748 -4.1416717 -4.1274552 -4.0986047 -4.061481][-4.0846629 -4.0635643 -4.049077 -4.0543647 -4.0447731 -4.0175209 -4.0022135 -4.0211363 -4.0727763 -4.1289272 -4.1657138 -4.1815572 -4.1804409 -4.1693587 -4.145205][-4.1084423 -4.089581 -4.0837564 -4.1033759 -4.1083469 -4.0842633 -4.0528016 -4.0515819 -4.0927672 -4.1506524 -4.1897597 -4.2085023 -4.21896 -4.2218876 -4.2109904][-4.1329436 -4.121696 -4.1237216 -4.1489344 -4.1605549 -4.1403294 -4.1020308 -4.0872006 -4.1147161 -4.1650748 -4.2018995 -4.2204514 -4.2352037 -4.2449608 -4.2443957][-4.1414781 -4.1383572 -4.1450071 -4.1682959 -4.184701 -4.1733027 -4.1393175 -4.12051 -4.13839 -4.1797371 -4.2087364 -4.2230673 -4.2368855 -4.2499084 -4.2546926][-4.1550026 -4.1570511 -4.1623917 -4.17607 -4.192667 -4.1862836 -4.1579757 -4.1354547 -4.1447854 -4.1750507 -4.1971793 -4.2109885 -4.223146 -4.2346449 -4.2414494][-4.1648536 -4.1677904 -4.1707587 -4.1748967 -4.1865935 -4.1799517 -4.1560793 -4.1364541 -4.1445413 -4.1697178 -4.190238 -4.2036085 -4.2120934 -4.2163234 -4.2227888][-4.1769905 -4.1755424 -4.1737204 -4.1756229 -4.1824636 -4.1729088 -4.1497631 -4.1364026 -4.1494446 -4.1745605 -4.1964931 -4.2080622 -4.2112246 -4.2106719 -4.2165766][-4.1958609 -4.1926074 -4.1863089 -4.1870093 -4.1881146 -4.1717048 -4.1458454 -4.1367354 -4.1520481 -4.1757522 -4.1989403 -4.2075973 -4.2042875 -4.1995177 -4.2043576][-4.2174354 -4.21245 -4.2037072 -4.2014961 -4.1966438 -4.175786 -4.1495466 -4.1443067 -4.16115 -4.1819372 -4.2000532 -4.2058177 -4.2019892 -4.1974454 -4.2005649][-4.24701 -4.23741 -4.2259493 -4.2227287 -4.2178364 -4.1994739 -4.1816888 -4.1860828 -4.2038331 -4.2162247 -4.2247286 -4.223732 -4.2173386 -4.2104135 -4.2102761]]...]
INFO - root - 2017-12-06 01:10:30.151609: step 61710, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 63h:36m:53s remains)
INFO - root - 2017-12-06 01:10:38.652582: step 61720, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.883 sec/batch; 66h:25m:24s remains)
INFO - root - 2017-12-06 01:10:47.278228: step 61730, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 64h:26m:40s remains)
INFO - root - 2017-12-06 01:10:55.764967: step 61740, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 64h:02m:59s remains)
INFO - root - 2017-12-06 01:11:04.285397: step 61750, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 63h:09m:15s remains)
INFO - root - 2017-12-06 01:11:12.680310: step 61760, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 64h:22m:26s remains)
INFO - root - 2017-12-06 01:11:20.998274: step 61770, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 0.779 sec/batch; 58h:35m:34s remains)
INFO - root - 2017-12-06 01:11:29.410755: step 61780, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 61h:54m:47s remains)
INFO - root - 2017-12-06 01:11:37.741971: step 61790, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 64h:07m:57s remains)
INFO - root - 2017-12-06 01:11:46.218326: step 61800, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 64h:01m:40s remains)
2017-12-06 01:11:47.031199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25314 -4.2583547 -4.2657261 -4.2721605 -4.2764273 -4.27982 -4.28293 -4.2839155 -4.2842178 -4.2846503 -4.2846513 -4.2831969 -4.2805805 -4.2780495 -4.2757607][-4.2703857 -4.2763553 -4.2834687 -4.2895746 -4.2935405 -4.2964396 -4.2978859 -4.2973742 -4.2967377 -4.2964778 -4.2967558 -4.2968431 -4.295948 -4.2937455 -4.2908235][-4.2814922 -4.286562 -4.2917023 -4.2951593 -4.2955093 -4.2942095 -4.2917414 -4.2892494 -4.2882867 -4.2895942 -4.2940574 -4.2983284 -4.3000951 -4.298409 -4.2956262][-4.27437 -4.2783794 -4.2817264 -4.2819004 -4.2774076 -4.2702031 -4.2613521 -4.2541652 -4.2515244 -4.2554045 -4.2668481 -4.2792683 -4.2877531 -4.29021 -4.2898479][-4.253695 -4.2571783 -4.2588396 -4.2550254 -4.2442336 -4.2279949 -4.2082896 -4.1915741 -4.1844783 -4.1916142 -4.2120361 -4.2373767 -4.2578254 -4.2695622 -4.2757998][-4.2274761 -4.2304 -4.2300115 -4.2219682 -4.2048988 -4.1805854 -4.1490526 -4.1208353 -4.1059036 -4.1138635 -4.142724 -4.1797576 -4.2127414 -4.2360978 -4.2528539][-4.20294 -4.205214 -4.2044797 -4.1950269 -4.1754751 -4.14746 -4.1104817 -4.0765758 -4.05498 -4.0582495 -4.0866804 -4.1272807 -4.167521 -4.1991692 -4.2250276][-4.19364 -4.195827 -4.195159 -4.1866961 -4.1688371 -4.1443067 -4.1130896 -4.086009 -4.0674396 -4.06588 -4.0855012 -4.1163468 -4.1505041 -4.1801839 -4.2073956][-4.2126622 -4.2151332 -4.2125483 -4.2027597 -4.1851559 -4.1654787 -4.143919 -4.12935 -4.1218228 -4.1255326 -4.1412926 -4.160882 -4.1807785 -4.19744 -4.2152963][-4.2366123 -4.2378449 -4.2335296 -4.2226243 -4.2069607 -4.1922541 -4.1796632 -4.1762943 -4.1819596 -4.1940579 -4.2098322 -4.2214708 -4.228466 -4.2319031 -4.237287][-4.2549891 -4.2536788 -4.2476816 -4.2372794 -4.2255487 -4.2158079 -4.2099023 -4.2134 -4.2257667 -4.2416029 -4.2557058 -4.2625103 -4.2636476 -4.26068 -4.2570677][-4.2681079 -4.265604 -4.259788 -4.2515278 -4.2438884 -4.2377491 -4.2340994 -4.2369547 -4.2469049 -4.259057 -4.2692194 -4.2739563 -4.274581 -4.2708497 -4.2632794][-4.2753243 -4.2734585 -4.2697539 -4.2639184 -4.2587957 -4.2547927 -4.2509518 -4.2494159 -4.2518907 -4.2562027 -4.2600174 -4.2612424 -4.260314 -4.2558012 -4.2464314][-4.2794361 -4.2776008 -4.2751741 -4.2712412 -4.267725 -4.2652617 -4.2618566 -4.2574425 -4.252542 -4.2473712 -4.2421393 -4.236639 -4.2313085 -4.2244864 -4.2139549][-4.28682 -4.2837496 -4.2809825 -4.2777534 -4.2749767 -4.2728124 -4.2689323 -4.2631354 -4.2554803 -4.2467484 -4.2377954 -4.22955 -4.2221985 -4.2133074 -4.2011647]]...]
INFO - root - 2017-12-06 01:11:55.538510: step 61810, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 62h:41m:47s remains)
INFO - root - 2017-12-06 01:12:04.094362: step 61820, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 62h:01m:42s remains)
INFO - root - 2017-12-06 01:12:12.554924: step 61830, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 64h:24m:40s remains)
INFO - root - 2017-12-06 01:12:21.135988: step 61840, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 64h:12m:47s remains)
INFO - root - 2017-12-06 01:12:29.755939: step 61850, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 66h:32m:56s remains)
INFO - root - 2017-12-06 01:12:38.305116: step 61860, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 64h:09m:03s remains)
INFO - root - 2017-12-06 01:12:46.708318: step 61870, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.775 sec/batch; 58h:16m:57s remains)
INFO - root - 2017-12-06 01:12:55.241777: step 61880, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 64h:36m:58s remains)
INFO - root - 2017-12-06 01:13:03.790927: step 61890, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 63h:19m:07s remains)
INFO - root - 2017-12-06 01:13:12.294107: step 61900, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 65h:12m:03s remains)
2017-12-06 01:13:13.079721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2522979 -4.2643671 -4.26386 -4.2615862 -4.2654724 -4.265378 -4.2632437 -4.2666721 -4.2775807 -4.2895465 -4.2917142 -4.2853947 -4.2753034 -4.2630115 -4.2354541][-4.2429891 -4.250051 -4.2547207 -4.2615652 -4.2725825 -4.2675967 -4.2523069 -4.2429724 -4.2484765 -4.260705 -4.2684593 -4.2789426 -4.281599 -4.2738128 -4.2524366][-4.22619 -4.2351494 -4.2465529 -4.2606139 -4.2754884 -4.258678 -4.2185907 -4.1886191 -4.1876574 -4.2065315 -4.2308598 -4.2604046 -4.2784886 -4.2809153 -4.272346][-4.2160339 -4.226912 -4.2413492 -4.2591648 -4.270647 -4.2292852 -4.1499562 -4.0972681 -4.09665 -4.1368427 -4.1893659 -4.2408838 -4.2737064 -4.2846646 -4.2883906][-4.2166982 -4.2243409 -4.2383456 -4.2540984 -4.2499633 -4.1701765 -4.0428658 -3.9674494 -3.9920301 -4.0787187 -4.1631069 -4.2292013 -4.2698135 -4.2869611 -4.2983222][-4.217124 -4.2222466 -4.2282438 -4.2320814 -4.203218 -4.0908885 -3.9276481 -3.8541458 -3.9327569 -4.0638833 -4.1640525 -4.2313871 -4.2712746 -4.2887564 -4.3015742][-4.2150426 -4.2198873 -4.2170768 -4.2066493 -4.1588774 -4.0401378 -3.8958833 -3.8629217 -3.9680052 -4.095768 -4.1824322 -4.2416515 -4.2777162 -4.2928491 -4.3008337][-4.2012272 -4.2102418 -4.20607 -4.1853271 -4.1311569 -4.0344415 -3.9463561 -3.9555509 -4.0524879 -4.1428852 -4.2025046 -4.251442 -4.2838984 -4.2957935 -4.295435][-4.1833253 -4.200079 -4.2000556 -4.1772447 -4.1268587 -4.0610876 -4.0205193 -4.0511556 -4.122376 -4.1754 -4.2144585 -4.2536745 -4.279942 -4.2868977 -4.2800736][-4.182024 -4.2010846 -4.202621 -4.1787577 -4.1399822 -4.1018829 -4.0890746 -4.1198988 -4.1644874 -4.1959844 -4.2247572 -4.2563624 -4.2749705 -4.2751188 -4.263361][-4.2006145 -4.2176704 -4.2177534 -4.1957536 -4.168612 -4.1483483 -4.1453314 -4.1687312 -4.1952033 -4.2158809 -4.2399135 -4.2654042 -4.2761326 -4.2698369 -4.2593064][-4.2338319 -4.2475762 -4.2464151 -4.2280731 -4.2089725 -4.1958566 -4.1940455 -4.2088861 -4.2278671 -4.2479181 -4.2683868 -4.28364 -4.2856708 -4.2778745 -4.2724142][-4.2750287 -4.2866926 -4.2836895 -4.2688084 -4.2549095 -4.2463341 -4.2445555 -4.2543216 -4.2696714 -4.2875738 -4.3006845 -4.3045864 -4.3019743 -4.2963071 -4.2952304][-4.31244 -4.3215017 -4.3181529 -4.3065639 -4.2960634 -4.2900915 -4.2890391 -4.29581 -4.3075519 -4.3194337 -4.3241978 -4.3222589 -4.3189397 -4.3151951 -4.3157291][-4.3361917 -4.3416743 -4.3385739 -4.3293228 -4.320786 -4.3164067 -4.3164058 -4.3223715 -4.3299265 -4.3354287 -4.3354216 -4.3318472 -4.3287354 -4.3266435 -4.3274593]]...]
INFO - root - 2017-12-06 01:13:21.568236: step 61910, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 63h:21m:12s remains)
INFO - root - 2017-12-06 01:13:30.029308: step 61920, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 62h:44m:56s remains)
INFO - root - 2017-12-06 01:13:38.647624: step 61930, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 62h:16m:52s remains)
INFO - root - 2017-12-06 01:13:47.054979: step 61940, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 64h:17m:32s remains)
INFO - root - 2017-12-06 01:13:55.558850: step 61950, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 65h:16m:48s remains)
INFO - root - 2017-12-06 01:14:04.020791: step 61960, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 64h:21m:49s remains)
INFO - root - 2017-12-06 01:14:12.593497: step 61970, loss = 2.08, batch loss = 2.03 (10.5 examples/sec; 0.759 sec/batch; 57h:03m:10s remains)
INFO - root - 2017-12-06 01:14:21.096239: step 61980, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.805 sec/batch; 60h:31m:04s remains)
INFO - root - 2017-12-06 01:14:29.641604: step 61990, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 65h:26m:44s remains)
INFO - root - 2017-12-06 01:14:38.183620: step 62000, loss = 2.04, batch loss = 1.99 (9.8 examples/sec; 0.819 sec/batch; 61h:32m:00s remains)
2017-12-06 01:14:38.977874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0981336 -4.1170712 -4.1285062 -4.1445374 -4.1530046 -4.1448808 -4.1150866 -4.0722022 -4.0151162 -3.969986 -3.9826643 -4.0550995 -4.1497188 -4.2408185 -4.3092165][-4.0946479 -4.1169515 -4.1326303 -4.1500826 -4.1532259 -4.1382952 -4.1037993 -4.0532126 -3.9886808 -3.9379475 -3.9531255 -4.034111 -4.1386943 -4.236352 -4.3085532][-4.088552 -4.1100025 -4.1275473 -4.1448197 -4.1450529 -4.1315432 -4.0988007 -4.042757 -3.9694409 -3.9103687 -3.9289064 -4.0228038 -4.1408792 -4.2433567 -4.3144345][-4.0801563 -4.0979004 -4.1174889 -4.1350021 -4.1354413 -4.1252418 -4.0966997 -4.0330009 -3.9476464 -3.8788362 -3.9036393 -4.0164986 -4.1486831 -4.2553911 -4.3223119][-4.0751433 -4.0868587 -4.1039605 -4.1211729 -4.1214557 -4.1075563 -4.0774364 -4.0055237 -3.9135931 -3.8433492 -3.8799224 -4.0149455 -4.159451 -4.266427 -4.3275204][-4.0778446 -4.0854592 -4.0992713 -4.11233 -4.1050863 -4.0794067 -4.04011 -3.9679143 -3.880178 -3.8174033 -3.8670728 -4.0142488 -4.1645694 -4.2710137 -4.3296242][-4.0878878 -4.0924783 -4.1017785 -4.1086564 -4.0944285 -4.0518775 -3.9932206 -3.9225764 -3.8496761 -3.802659 -3.863025 -4.0138173 -4.1632042 -4.2691889 -4.3294396][-4.103631 -4.1058903 -4.11328 -4.1146584 -4.0951667 -4.0387635 -3.9606962 -3.8868942 -3.8261876 -3.7952228 -3.8624873 -4.0100913 -4.1541147 -4.2622786 -4.3278227][-4.1109333 -4.1148658 -4.1278148 -4.1344619 -4.1174512 -4.0575032 -3.9749026 -3.8993111 -3.8394272 -3.8098502 -3.8735929 -4.0100312 -4.1447268 -4.2543793 -4.325841][-4.1069884 -4.1173558 -4.1422291 -4.1613793 -4.1526766 -4.1027079 -4.0294437 -3.9548523 -3.8888826 -3.8495207 -3.8982174 -4.0154052 -4.1403189 -4.2500181 -4.3235979][-4.0984259 -4.1136732 -4.1492143 -4.1859226 -4.1882458 -4.1468344 -4.079144 -4.0100675 -3.946224 -3.8957858 -3.9216876 -4.0226011 -4.1409578 -4.2488508 -4.3213043][-4.0959663 -4.1101737 -4.1512227 -4.1971827 -4.2015934 -4.1634903 -4.1048589 -4.0484438 -3.9945464 -3.9358191 -3.9424093 -4.0318971 -4.146966 -4.25142 -4.321229][-4.1006451 -4.1112566 -4.1515026 -4.1944118 -4.1915874 -4.1533141 -4.1092033 -4.0636473 -4.0137568 -3.9529819 -3.9531488 -4.0407691 -4.1565228 -4.2584906 -4.3246107][-4.1063204 -4.116416 -4.1535611 -4.191072 -4.1821728 -4.1441894 -4.1042523 -4.05703 -4.0051975 -3.9502609 -3.9545026 -4.0459437 -4.165349 -4.2668538 -4.3294606][-4.1149631 -4.1282959 -4.1654687 -4.2021918 -4.1858726 -4.1426144 -4.1020374 -4.0490284 -3.9925771 -3.9491031 -3.9603231 -4.0537171 -4.1753111 -4.27482 -4.3336868]]...]
INFO - root - 2017-12-06 01:14:47.357194: step 62010, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 66h:47m:18s remains)
INFO - root - 2017-12-06 01:14:55.921813: step 62020, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 65h:59m:11s remains)
INFO - root - 2017-12-06 01:15:04.400251: step 62030, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 64h:30m:22s remains)
INFO - root - 2017-12-06 01:15:12.835862: step 62040, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.856 sec/batch; 64h:16m:39s remains)
INFO - root - 2017-12-06 01:15:21.331753: step 62050, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 65h:19m:18s remains)
INFO - root - 2017-12-06 01:15:29.826001: step 62060, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 64h:08m:49s remains)
INFO - root - 2017-12-06 01:15:38.297845: step 62070, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.764 sec/batch; 57h:21m:52s remains)
INFO - root - 2017-12-06 01:15:47.028193: step 62080, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 63h:07m:26s remains)
INFO - root - 2017-12-06 01:15:55.615076: step 62090, loss = 2.09, batch loss = 2.04 (9.0 examples/sec; 0.889 sec/batch; 66h:45m:28s remains)
INFO - root - 2017-12-06 01:16:04.219497: step 62100, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 65h:55m:44s remains)
2017-12-06 01:16:05.062373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3410606 -4.3428154 -4.3424363 -4.3424153 -4.3428426 -4.34285 -4.3420396 -4.341332 -4.3408556 -4.3405042 -4.3396397 -4.3385291 -4.3388429 -4.3417916 -4.3452644][-4.3211517 -4.3208885 -4.3197775 -4.321516 -4.3247318 -4.327908 -4.32847 -4.327445 -4.3265505 -4.325408 -4.3213296 -4.3172965 -4.3165402 -4.3213472 -4.3277178][-4.2926555 -4.2885609 -4.2869473 -4.2916183 -4.2984247 -4.3045883 -4.30572 -4.3036971 -4.3020716 -4.3006248 -4.2929878 -4.2850184 -4.2833219 -4.2904491 -4.3006411][-4.2607412 -4.2490559 -4.2427979 -4.2485929 -4.2587128 -4.2673979 -4.2716317 -4.2713871 -4.2696962 -4.2681146 -4.2578592 -4.2472334 -4.2443871 -4.251811 -4.2639575][-4.2349715 -4.214581 -4.1979346 -4.1990409 -4.2092667 -4.2198939 -4.2295856 -4.2347851 -4.2352433 -4.2349148 -4.2276359 -4.217021 -4.2110567 -4.2139297 -4.2246094][-4.2191873 -4.1930523 -4.165555 -4.1577864 -4.1651206 -4.1744556 -4.1887536 -4.2025304 -4.207201 -4.2064948 -4.2034054 -4.1953335 -4.1865988 -4.1809812 -4.1858816][-4.2129769 -4.1841297 -4.1481767 -4.13112 -4.1326923 -4.1404209 -4.1577411 -4.1774993 -4.1840487 -4.181499 -4.1839085 -4.1823821 -4.1751647 -4.1649909 -4.1660533][-4.2168131 -4.1867375 -4.1456084 -4.1168122 -4.1140037 -4.1215086 -4.1420712 -4.1649127 -4.1705761 -4.1653247 -4.1718364 -4.1785288 -4.1794848 -4.1759548 -4.1773968][-4.2230158 -4.1921329 -4.1471548 -4.10933 -4.1060872 -4.1183333 -4.1449728 -4.169795 -4.1745324 -4.1653771 -4.170506 -4.1826677 -4.1931171 -4.2005692 -4.2070994][-4.2285662 -4.1962118 -4.1521626 -4.1160812 -4.1143093 -4.1345506 -4.1647964 -4.1899681 -4.1970725 -4.1868 -4.1867 -4.1981387 -4.2146616 -4.2317109 -4.2422361][-4.2481451 -4.2147183 -4.1729207 -4.1407847 -4.1384373 -4.1610937 -4.190372 -4.21604 -4.226346 -4.2163692 -4.2093196 -4.2127352 -4.227026 -4.2460861 -4.2596188][-4.2661462 -4.2320652 -4.19287 -4.1643124 -4.160707 -4.179131 -4.2042646 -4.2309837 -4.2469482 -4.2422218 -4.2352042 -4.2279415 -4.2299714 -4.2408376 -4.2545633][-4.2720761 -4.2430596 -4.2118573 -4.191514 -4.1840334 -4.1903305 -4.2067027 -4.2324905 -4.2525697 -4.2571921 -4.2554426 -4.2464361 -4.2401581 -4.2407918 -4.2460151][-4.2616506 -4.2414603 -4.2221088 -4.2122068 -4.2030468 -4.1993341 -4.20536 -4.2239861 -4.2446656 -4.2596025 -4.2691116 -4.2659326 -4.2580953 -4.252377 -4.2494941][-4.2418318 -4.2319007 -4.2234573 -4.2204642 -4.212575 -4.200707 -4.1933365 -4.2000556 -4.2173276 -4.241004 -4.2638712 -4.2719307 -4.2709136 -4.2665558 -4.2592192]]...]
INFO - root - 2017-12-06 01:16:13.615539: step 62110, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 63h:42m:09s remains)
INFO - root - 2017-12-06 01:16:22.049794: step 62120, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 66h:16m:53s remains)
INFO - root - 2017-12-06 01:16:30.566810: step 62130, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.832 sec/batch; 62h:28m:35s remains)
INFO - root - 2017-12-06 01:16:39.126373: step 62140, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 63h:37m:03s remains)
INFO - root - 2017-12-06 01:16:47.520609: step 62150, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 64h:53m:48s remains)
INFO - root - 2017-12-06 01:16:56.155628: step 62160, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 63h:41m:46s remains)
INFO - root - 2017-12-06 01:17:04.596970: step 62170, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.742 sec/batch; 55h:43m:25s remains)
INFO - root - 2017-12-06 01:17:13.117230: step 62180, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 62h:07m:11s remains)
INFO - root - 2017-12-06 01:17:21.625076: step 62190, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 62h:37m:33s remains)
INFO - root - 2017-12-06 01:17:30.133312: step 62200, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 64h:07m:59s remains)
2017-12-06 01:17:30.891926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1925941 -4.2016497 -4.2125993 -4.2269373 -4.2324586 -4.2257552 -4.2250652 -4.2280092 -4.2286711 -4.2326751 -4.2443719 -4.25636 -4.2594962 -4.2485147 -4.2365608][-4.1928682 -4.1927047 -4.2007847 -4.2160349 -4.2300172 -4.2348337 -4.2405448 -4.2456174 -4.2466063 -4.24628 -4.253089 -4.2642937 -4.2706642 -4.2652845 -4.2548985][-4.1924958 -4.1820107 -4.1842976 -4.198566 -4.2166357 -4.2296476 -4.2429943 -4.2545652 -4.26182 -4.2618103 -4.2626243 -4.268292 -4.2699728 -4.26323 -4.2547421][-4.1900597 -4.1727729 -4.1681476 -4.1767015 -4.19452 -4.2110825 -4.2294326 -4.2480307 -4.2614393 -4.2646675 -4.2634645 -4.2660613 -4.2628355 -4.2516432 -4.2426777][-4.1889 -4.1696334 -4.1592422 -4.1590333 -4.1707287 -4.1848397 -4.2038279 -4.2255306 -4.243186 -4.2534823 -4.2566004 -4.2589359 -4.2517767 -4.2361717 -4.226192][-4.1876917 -4.1693821 -4.1568513 -4.1517129 -4.1533704 -4.15658 -4.1663795 -4.1833186 -4.2034359 -4.2252841 -4.2405758 -4.2482095 -4.2411318 -4.2260089 -4.2171741][-4.1860161 -4.1699157 -4.1564269 -4.1505222 -4.1458211 -4.1378222 -4.1297388 -4.130085 -4.1487088 -4.1828647 -4.2142291 -4.2305417 -4.2259893 -4.2110157 -4.2014275][-4.193985 -4.181253 -4.1694183 -4.16327 -4.1543384 -4.1348438 -4.105237 -4.0797272 -4.0890746 -4.1321316 -4.1771016 -4.2015839 -4.2011871 -4.184659 -4.1718178][-4.2063746 -4.1997604 -4.1895738 -4.1812086 -4.1678314 -4.1419811 -4.1020908 -4.0620403 -4.0629592 -4.1040149 -4.1516256 -4.1778164 -4.1795788 -4.1631923 -4.1453738][-4.2078247 -4.2084107 -4.2003484 -4.1901226 -4.176857 -4.1566653 -4.1257381 -4.0934868 -4.0901222 -4.1158447 -4.1526427 -4.1735983 -4.1728821 -4.1549935 -4.1334443][-4.1989012 -4.2034521 -4.1968646 -4.1853962 -4.1751976 -4.168191 -4.1560369 -4.139924 -4.1367354 -4.1481481 -4.170423 -4.1828647 -4.1773438 -4.1573119 -4.1348662][-4.1890979 -4.1916957 -4.1827188 -4.17062 -4.1663361 -4.174201 -4.1806464 -4.1800179 -4.1798038 -4.1830711 -4.1942759 -4.1998372 -4.1926613 -4.1744308 -4.1566238][-4.184845 -4.1815643 -4.1691456 -4.1570187 -4.1585054 -4.1790771 -4.1998367 -4.2116971 -4.21524 -4.2118697 -4.2111392 -4.2102575 -4.2031641 -4.1905632 -4.1813912][-4.1931891 -4.1868978 -4.1734157 -4.1598063 -4.1593156 -4.1807351 -4.2063456 -4.2241859 -4.2299528 -4.2230177 -4.2132049 -4.20579 -4.1998162 -4.1951904 -4.1975031][-4.2127657 -4.2053685 -4.1920514 -4.1776929 -4.1723304 -4.185308 -4.2045321 -4.2187524 -4.2220025 -4.2148991 -4.2036004 -4.1936603 -4.18812 -4.1878524 -4.1988397]]...]
INFO - root - 2017-12-06 01:17:39.456186: step 62210, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 62h:35m:57s remains)
INFO - root - 2017-12-06 01:17:48.128875: step 62220, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 63h:10m:59s remains)
INFO - root - 2017-12-06 01:17:56.512877: step 62230, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 63h:08m:31s remains)
INFO - root - 2017-12-06 01:18:05.073533: step 62240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 64h:40m:34s remains)
INFO - root - 2017-12-06 01:18:13.592178: step 62250, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 62h:57m:39s remains)
INFO - root - 2017-12-06 01:18:21.989807: step 62260, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 63h:00m:24s remains)
INFO - root - 2017-12-06 01:18:30.513854: step 62270, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 0.792 sec/batch; 59h:25m:55s remains)
INFO - root - 2017-12-06 01:18:39.026004: step 62280, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.865 sec/batch; 64h:56m:21s remains)
INFO - root - 2017-12-06 01:18:47.625797: step 62290, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 63h:55m:51s remains)
INFO - root - 2017-12-06 01:18:56.132542: step 62300, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.861 sec/batch; 64h:36m:30s remains)
2017-12-06 01:18:56.887404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2295427 -4.2431846 -4.2530894 -4.2545257 -4.2426438 -4.225421 -4.2139444 -4.2092738 -4.2080588 -4.2113338 -4.2160816 -4.2222061 -4.2337937 -4.2508802 -4.2724633][-4.238543 -4.243145 -4.2408862 -4.2275534 -4.2028203 -4.1803303 -4.1733055 -4.1830034 -4.1943426 -4.2078414 -4.2190561 -4.224802 -4.230473 -4.2419639 -4.2589097][-4.2528834 -4.2471948 -4.2293711 -4.1958804 -4.1516933 -4.1195016 -4.1154962 -4.1429133 -4.1719704 -4.2005944 -4.2232795 -4.233377 -4.2395496 -4.2496414 -4.2626853][-4.2682905 -4.2562256 -4.2279282 -4.1761069 -4.1105056 -4.0641131 -4.0571222 -4.098505 -4.1479545 -4.192832 -4.226388 -4.2429647 -4.2549725 -4.2687125 -4.2822518][-4.2741408 -4.2600379 -4.2290969 -4.1683974 -4.0884686 -4.0277591 -4.0097427 -4.0540204 -4.122345 -4.1828923 -4.2258778 -4.2507343 -4.2717061 -4.2905664 -4.3064828][-4.2633557 -4.251864 -4.2225513 -4.1609507 -4.0743213 -4.00103 -3.9668264 -4.0048738 -4.0886369 -4.1697264 -4.2249103 -4.2591605 -4.287889 -4.3104043 -4.32732][-4.2558618 -4.245492 -4.2177672 -4.1581726 -4.0709443 -3.9875836 -3.934741 -3.9595652 -4.0512462 -4.1528044 -4.2228694 -4.2682371 -4.3035746 -4.3275733 -4.3426371][-4.2617993 -4.2507591 -4.2255139 -4.1702476 -4.086688 -3.9996965 -3.9351678 -3.947928 -4.03376 -4.142046 -4.2220764 -4.274929 -4.3135324 -4.3368492 -4.3472624][-4.2784343 -4.2673697 -4.2409787 -4.1845207 -4.0987744 -4.0099978 -3.9470155 -3.9592447 -4.0353584 -4.1380763 -4.220078 -4.2751174 -4.3148503 -4.3365545 -4.3421793][-4.2930889 -4.27773 -4.2432237 -4.1780453 -4.086369 -3.9986668 -3.9475892 -3.9707787 -4.0423417 -4.1398869 -4.2221546 -4.277256 -4.3154497 -4.3328357 -4.3339648][-4.2892556 -4.2733316 -4.2369366 -4.170013 -4.0795703 -3.9998729 -3.9622371 -3.9916205 -4.0606942 -4.1550379 -4.23564 -4.2868204 -4.318707 -4.3306236 -4.3266821][-4.26934 -4.2581344 -4.2280889 -4.1694641 -4.0892644 -4.0249844 -4.0029707 -4.03764 -4.1042485 -4.1905575 -4.2622948 -4.3035321 -4.3241858 -4.32887 -4.3211107][-4.25586 -4.250608 -4.230474 -4.185576 -4.1225133 -4.0780807 -4.068315 -4.1033797 -4.1626511 -4.2329121 -4.288476 -4.3160591 -4.325841 -4.3239493 -4.3135982][-4.2563233 -4.2586446 -4.2512045 -4.223815 -4.1829233 -4.1562634 -4.152163 -4.1801047 -4.2247095 -4.2729845 -4.3083539 -4.3219814 -4.3240051 -4.3165088 -4.3036132][-4.2738037 -4.279604 -4.2791314 -4.2640643 -4.239614 -4.2249942 -4.22483 -4.2443533 -4.2733331 -4.3014536 -4.3212886 -4.32673 -4.3225431 -4.308423 -4.2917728]]...]
INFO - root - 2017-12-06 01:19:05.573112: step 62310, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 64h:14m:13s remains)
INFO - root - 2017-12-06 01:19:14.153896: step 62320, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.885 sec/batch; 66h:24m:25s remains)
INFO - root - 2017-12-06 01:19:22.613521: step 62330, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 63h:05m:43s remains)
INFO - root - 2017-12-06 01:19:31.122478: step 62340, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 63h:14m:45s remains)
INFO - root - 2017-12-06 01:19:39.826741: step 62350, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 64h:39m:39s remains)
INFO - root - 2017-12-06 01:19:48.328489: step 62360, loss = 2.04, batch loss = 1.99 (10.2 examples/sec; 0.786 sec/batch; 58h:56m:41s remains)
INFO - root - 2017-12-06 01:19:56.886609: step 62370, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 0.789 sec/batch; 59h:12m:29s remains)
INFO - root - 2017-12-06 01:20:05.458004: step 62380, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 62h:33m:44s remains)
INFO - root - 2017-12-06 01:20:14.060945: step 62390, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 63h:45m:46s remains)
INFO - root - 2017-12-06 01:20:22.582384: step 62400, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 62h:54m:54s remains)
2017-12-06 01:20:23.378579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0558739 -4.0378208 -4.0573282 -4.0900173 -4.1043057 -4.0847211 -4.0601792 -4.0583973 -4.0797114 -4.1225009 -4.1635342 -4.1886454 -4.2173543 -4.2535019 -4.2862196][-4.022243 -4.0070744 -4.0275412 -4.0660076 -4.0844879 -4.0616956 -4.0334206 -4.0346308 -4.0645661 -4.1183524 -4.1668491 -4.1964669 -4.2235618 -4.2581229 -4.2892308][-4.0184517 -4.00761 -4.0287709 -4.0636454 -4.06643 -4.0277014 -3.9914169 -4.0008035 -4.0488405 -4.119689 -4.1789675 -4.2100415 -4.2343554 -4.2655749 -4.294364][-4.0338478 -4.0249124 -4.0454764 -4.069787 -4.0438056 -3.9838347 -3.9384754 -3.9551256 -4.0227404 -4.1098962 -4.1807671 -4.21598 -4.2417841 -4.2737336 -4.3019443][-4.0496817 -4.0443587 -4.0585928 -4.0632629 -4.0130167 -3.9335821 -3.8785 -3.9013188 -3.988663 -4.0940995 -4.1754932 -4.2143011 -4.2448721 -4.279356 -4.3087115][-4.0628629 -4.0593872 -4.063776 -4.0490913 -3.9752538 -3.879046 -3.8200474 -3.8567798 -3.9661868 -4.0851011 -4.1685557 -4.2110639 -4.2479596 -4.2835741 -4.3133221][-4.07637 -4.0744791 -4.0702715 -4.0370111 -3.9423082 -3.8285232 -3.7657232 -3.8263748 -3.9567554 -4.0836525 -4.1653848 -4.2119474 -4.2543197 -4.2898059 -4.3181014][-4.0886679 -4.0893645 -4.0851789 -4.0426111 -3.9408324 -3.819761 -3.7537699 -3.8253429 -3.9641988 -4.0905895 -4.1708703 -4.2185111 -4.261651 -4.2944469 -4.3216538][-4.0853462 -4.0960026 -4.1022577 -4.0625906 -3.97747 -3.8812249 -3.8240242 -3.8816323 -4.0016613 -4.1128359 -4.1863222 -4.2289181 -4.2670012 -4.2963891 -4.322835][-4.0793414 -4.0949011 -4.1076412 -4.0760813 -4.0142155 -3.9520166 -3.9146543 -3.961941 -4.059494 -4.1478305 -4.2047844 -4.2390265 -4.2698183 -4.2964625 -4.3206372][-4.0844307 -4.09497 -4.1042786 -4.0736217 -4.0249214 -3.9833403 -3.9637609 -4.0124836 -4.1002674 -4.1712365 -4.2153764 -4.2410183 -4.2644835 -4.2888403 -4.3116565][-4.1185913 -4.118063 -4.1190863 -4.0873609 -4.0375633 -3.9991837 -3.9875975 -4.037508 -4.1148853 -4.1742206 -4.2116518 -4.2346549 -4.2558126 -4.2800941 -4.3036995][-4.1519928 -4.1474304 -4.1496439 -4.118845 -4.0659933 -4.0273323 -4.0211377 -4.063796 -4.124351 -4.1719117 -4.2050562 -4.22842 -4.2488246 -4.2738819 -4.2991829][-4.163157 -4.1577792 -4.1625495 -4.1345315 -4.0906773 -4.0653768 -4.0663905 -4.0969582 -4.1384869 -4.1727877 -4.1985927 -4.2218895 -4.2439966 -4.2686758 -4.2941484][-4.1667295 -4.1559253 -4.1572905 -4.1331758 -4.1024656 -4.0929575 -4.1007762 -4.1234446 -4.1521077 -4.1760759 -4.1932421 -4.2143307 -4.23829 -4.2625823 -4.28722]]...]
INFO - root - 2017-12-06 01:20:31.975186: step 62410, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 64h:24m:37s remains)
INFO - root - 2017-12-06 01:20:40.522519: step 62420, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 66h:06m:39s remains)
INFO - root - 2017-12-06 01:20:49.062145: step 62430, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 63h:51m:44s remains)
INFO - root - 2017-12-06 01:20:57.553451: step 62440, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 65h:57m:28s remains)
INFO - root - 2017-12-06 01:21:05.811664: step 62450, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 67h:46m:42s remains)
INFO - root - 2017-12-06 01:21:14.369258: step 62460, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 65h:00m:47s remains)
INFO - root - 2017-12-06 01:21:22.737079: step 62470, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.743 sec/batch; 55h:42m:17s remains)
INFO - root - 2017-12-06 01:21:31.248610: step 62480, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 66h:09m:44s remains)
INFO - root - 2017-12-06 01:21:39.804166: step 62490, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 66h:37m:17s remains)
INFO - root - 2017-12-06 01:21:48.327916: step 62500, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 64h:41m:05s remains)
2017-12-06 01:21:49.156433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1228938 -4.1433706 -4.1778955 -4.20146 -4.2179136 -4.2181549 -4.2007313 -4.1760387 -4.1602421 -4.1586914 -4.1500053 -4.1472077 -4.16758 -4.191659 -4.2222209][-4.0426211 -4.061923 -4.1013417 -4.1276188 -4.1457009 -4.1444278 -4.1220856 -4.0988626 -4.0868526 -4.0875831 -4.0822845 -4.0864897 -4.1157546 -4.1459007 -4.1865859][-3.9757264 -3.9918644 -4.0315943 -4.0637832 -4.084383 -4.0814505 -4.0590219 -4.0379491 -4.0343018 -4.0364947 -4.0349402 -4.0467987 -4.0858917 -4.1226664 -4.1689711][-3.9431405 -3.9657097 -4.0065069 -4.0395 -4.0595579 -4.0573835 -4.0354009 -4.0180578 -4.0232029 -4.0305748 -4.034317 -4.0513253 -4.0922146 -4.13005 -4.1697884][-3.9349256 -3.9670408 -4.0076065 -4.0350728 -4.0522666 -4.0443559 -4.011642 -3.99189 -4.0103145 -4.03194 -4.046206 -4.0715885 -4.1122594 -4.145896 -4.1745243][-3.9303322 -3.9658329 -4.0052176 -4.0312347 -4.044044 -4.020205 -3.9597919 -3.9260409 -3.9612746 -4.0126233 -4.0459747 -4.0861521 -4.1316509 -4.16223 -4.1861053][-3.9188769 -3.9527822 -3.997889 -4.0294456 -4.0338268 -3.9817281 -3.8783941 -3.8175185 -3.8689487 -3.9554627 -4.0111122 -4.0672021 -4.1242175 -4.1614432 -4.1880469][-3.9115486 -3.944263 -3.9942074 -4.0326495 -4.0344596 -3.9624751 -3.8393259 -3.7670856 -3.8214259 -3.9143147 -3.9682124 -4.0198541 -4.0806446 -4.1245818 -4.1596861][-3.9295623 -3.9542613 -3.9980626 -4.0414639 -4.0529432 -4.0006065 -3.9116085 -3.8620143 -3.8948896 -3.9564652 -3.9831576 -4.0093336 -4.0559011 -4.0990806 -4.1396279][-3.9649618 -3.9787078 -4.01581 -4.0586553 -4.0857015 -4.0647211 -4.0145221 -3.9876106 -4.001802 -4.0314131 -4.0356421 -4.036284 -4.0617347 -4.0972571 -4.1376877][-4.0173607 -4.0233655 -4.0583448 -4.0999975 -4.1381416 -4.1388531 -4.1130853 -4.0965819 -4.1002736 -4.1110063 -4.100904 -4.0887561 -4.101512 -4.131042 -4.1646252][-4.092669 -4.0924563 -4.1211762 -4.15924 -4.2001147 -4.2088575 -4.1926289 -4.1812806 -4.1822262 -4.1843419 -4.1746559 -4.1651621 -4.1746325 -4.1960764 -4.218132][-4.1761174 -4.1739292 -4.1953478 -4.2265577 -4.2591829 -4.2672696 -4.2578359 -4.2501025 -4.2486629 -4.2463455 -4.2412119 -4.236444 -4.2415257 -4.2531176 -4.26524][-4.2483211 -4.2518139 -4.2689748 -4.2861171 -4.301312 -4.3052769 -4.3002291 -4.294909 -4.2901287 -4.2867937 -4.2857957 -4.2831192 -4.284986 -4.2904668 -4.2983146][-4.2913857 -4.2982039 -4.3110847 -4.3189583 -4.3235507 -4.3238111 -4.3207726 -4.3163056 -4.3122082 -4.310082 -4.3112979 -4.3102174 -4.3110528 -4.3156428 -4.3217316]]...]
INFO - root - 2017-12-06 01:21:57.708622: step 62510, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 64h:26m:59s remains)
INFO - root - 2017-12-06 01:22:06.410304: step 62520, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 63h:36m:41s remains)
INFO - root - 2017-12-06 01:22:14.923147: step 62530, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 63h:53m:09s remains)
INFO - root - 2017-12-06 01:22:23.437793: step 62540, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 63h:27m:02s remains)
INFO - root - 2017-12-06 01:22:31.963044: step 62550, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 64h:47m:00s remains)
INFO - root - 2017-12-06 01:22:40.485496: step 62560, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 66h:50m:35s remains)
INFO - root - 2017-12-06 01:22:48.994010: step 62570, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.783 sec/batch; 58h:42m:51s remains)
INFO - root - 2017-12-06 01:22:57.478506: step 62580, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 63h:53m:17s remains)
INFO - root - 2017-12-06 01:23:06.091688: step 62590, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 63h:26m:12s remains)
INFO - root - 2017-12-06 01:23:14.633475: step 62600, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.906 sec/batch; 67h:55m:44s remains)
2017-12-06 01:23:15.473799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3203368 -4.3079352 -4.2932386 -4.2792263 -4.2697639 -4.2707758 -4.2855978 -4.3090854 -4.3302164 -4.3436117 -4.34925 -4.3510513 -4.35094 -4.3468533 -4.342237][-4.3090987 -4.2869892 -4.2607865 -4.23744 -4.2179079 -4.2106123 -4.2217259 -4.2525258 -4.2892089 -4.3209224 -4.342567 -4.3534112 -4.3579597 -4.35521 -4.3494558][-4.29216 -4.2609444 -4.2236328 -4.1875725 -4.1526027 -4.1265726 -4.1224961 -4.1569319 -4.216011 -4.2723408 -4.3136506 -4.3391337 -4.3557177 -4.3616776 -4.3573303][-4.2711768 -4.2360597 -4.1897464 -4.1367717 -4.0794039 -4.0285659 -4.0087843 -4.0407443 -4.1180634 -4.1988959 -4.2605867 -4.3047767 -4.3370371 -4.3588934 -4.3605027][-4.2540612 -4.2161756 -4.159183 -4.08756 -4.0050383 -3.9198773 -3.8714917 -3.9009275 -4.0017052 -4.111095 -4.1948638 -4.2566228 -4.3032746 -4.3391585 -4.3507562][-4.2291617 -4.1933904 -4.1358857 -4.0515518 -3.9379909 -3.8012221 -3.6945286 -3.7025414 -3.8468537 -4.0102754 -4.1257877 -4.2007647 -4.2568369 -4.3025432 -4.3232551][-4.2009826 -4.17377 -4.1297979 -4.0516782 -3.9253597 -3.7406819 -3.5489154 -3.4995956 -3.6865716 -3.9068439 -4.0557313 -4.143055 -4.2032123 -4.2513356 -4.2768812][-4.1960511 -4.1815548 -4.1603122 -4.1089044 -4.0072994 -3.8387885 -3.6406684 -3.5543213 -3.6996083 -3.8916483 -4.0284162 -4.1136246 -4.1685491 -4.2042027 -4.2248735][-4.2148619 -4.2112436 -4.2026353 -4.1780357 -4.1165414 -4.0057292 -3.8767662 -3.8172932 -3.8866243 -3.9891024 -4.0746269 -4.1372042 -4.1733403 -4.1882 -4.1994462][-4.2403407 -4.243506 -4.2434068 -4.2385736 -4.2096839 -4.1462703 -4.0773897 -4.0462947 -4.0737152 -4.1189036 -4.1620259 -4.19819 -4.2151 -4.2125592 -4.2118387][-4.2604036 -4.2684288 -4.2736993 -4.2781396 -4.2711263 -4.240612 -4.20701 -4.1893778 -4.197803 -4.2209811 -4.2411566 -4.2527127 -4.252533 -4.24337 -4.2367206][-4.2849932 -4.2932696 -4.2968059 -4.302896 -4.3037834 -4.2933073 -4.2789369 -4.2692771 -4.2737269 -4.2891526 -4.29946 -4.2958188 -4.2852039 -4.27525 -4.2660375][-4.3057041 -4.3121119 -4.3134575 -4.3147016 -4.3167968 -4.3152971 -4.3111787 -4.3100157 -4.3155613 -4.3246045 -4.3298707 -4.323504 -4.3126626 -4.3051009 -4.2966757][-4.3148918 -4.3164182 -4.3174181 -4.317349 -4.3201647 -4.3225045 -4.3216743 -4.3224797 -4.3277698 -4.3333964 -4.3372235 -4.3357096 -4.329525 -4.3228874 -4.3152108][-4.3260279 -4.3227491 -4.3218174 -4.3204379 -4.320919 -4.3222733 -4.3226986 -4.3248835 -4.3291731 -4.3321066 -4.3331466 -4.3318548 -4.3285031 -4.324182 -4.3200088]]...]
INFO - root - 2017-12-06 01:23:23.950421: step 62610, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 64h:06m:58s remains)
INFO - root - 2017-12-06 01:23:32.465845: step 62620, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 63h:17m:05s remains)
INFO - root - 2017-12-06 01:23:40.967869: step 62630, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.839 sec/batch; 62h:54m:06s remains)
INFO - root - 2017-12-06 01:23:49.550582: step 62640, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 63h:49m:55s remains)
INFO - root - 2017-12-06 01:23:58.084030: step 62650, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 64h:02m:09s remains)
INFO - root - 2017-12-06 01:24:06.573953: step 62660, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 63h:52m:20s remains)
INFO - root - 2017-12-06 01:24:14.931637: step 62670, loss = 2.03, batch loss = 1.97 (10.7 examples/sec; 0.751 sec/batch; 56h:16m:40s remains)
INFO - root - 2017-12-06 01:24:23.333402: step 62680, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 0.788 sec/batch; 59h:04m:21s remains)
INFO - root - 2017-12-06 01:24:31.820314: step 62690, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 64h:31m:11s remains)
INFO - root - 2017-12-06 01:24:40.350880: step 62700, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.872 sec/batch; 65h:21m:25s remains)
2017-12-06 01:24:41.154231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1435809 -4.1226325 -4.1181293 -4.1417937 -4.1819372 -4.2070527 -4.21092 -4.2178044 -4.2109914 -4.1929369 -4.1777954 -4.1571455 -4.1496658 -4.1511054 -4.1429954][-4.1415629 -4.1217995 -4.1168184 -4.1361375 -4.174623 -4.202539 -4.2061081 -4.2084632 -4.2057757 -4.2022882 -4.193716 -4.1739292 -4.163559 -4.1621184 -4.1530237][-4.1521025 -4.138011 -4.1324553 -4.1447892 -4.1767378 -4.201468 -4.2050042 -4.2055268 -4.211586 -4.222034 -4.2198129 -4.1984377 -4.1776872 -4.1702414 -4.1580281][-4.1809015 -4.1814289 -4.1777973 -4.1771073 -4.1882915 -4.1931734 -4.1903982 -4.1927605 -4.2090626 -4.2279968 -4.2298956 -4.2076244 -4.1785793 -4.1641159 -4.1428013][-4.1941729 -4.2063231 -4.2016363 -4.1831007 -4.1611981 -4.1380291 -4.1244473 -4.1307983 -4.1576309 -4.1870694 -4.19677 -4.1850929 -4.164012 -4.1498604 -4.1285496][-4.1816912 -4.1972876 -4.1839581 -4.139431 -4.0784335 -4.0208278 -3.9998679 -4.0132637 -4.0507674 -4.0940561 -4.1199446 -4.1300468 -4.1319118 -4.1313529 -4.1241989][-4.1521187 -4.1638608 -4.138535 -4.0688362 -3.9669042 -3.8811855 -3.8649435 -3.8950255 -3.9429059 -4.0027566 -4.0531955 -4.0893292 -4.1180806 -4.1366787 -4.1451497][-4.1337543 -4.1408415 -4.1168966 -4.0491176 -3.9428487 -3.8577981 -3.8510108 -3.8894794 -3.9357421 -4.0014472 -4.0614409 -4.1055903 -4.1422229 -4.1688704 -4.182682][-4.141531 -4.1502109 -4.1392694 -4.0983133 -4.021534 -3.956501 -3.9508612 -3.9858224 -4.021657 -4.0702853 -4.1170645 -4.1546383 -4.1855888 -4.2076221 -4.21871][-4.1548538 -4.16632 -4.1670885 -4.149724 -4.1057878 -4.0649557 -4.0604596 -4.0865889 -4.1138721 -4.1430755 -4.1740789 -4.2041693 -4.2259927 -4.2400489 -4.2416425][-4.1679945 -4.1832924 -4.1907444 -4.1887484 -4.1688557 -4.1462464 -4.1442852 -4.1652927 -4.1868839 -4.2052622 -4.2249303 -4.2411451 -4.2491665 -4.2510276 -4.2435689][-4.1703811 -4.1837077 -4.1897287 -4.1917434 -4.1832204 -4.1716704 -4.1763978 -4.1968188 -4.214726 -4.2282286 -4.2391772 -4.2433 -4.2407565 -4.2359228 -4.2250819][-4.1634216 -4.1702471 -4.1703849 -4.1694031 -4.1652708 -4.1629357 -4.1700244 -4.1857762 -4.1969881 -4.2031188 -4.207037 -4.2076573 -4.2054009 -4.2032189 -4.200069][-4.1635489 -4.1620421 -4.1554379 -4.1502166 -4.1484694 -4.149446 -4.1538754 -4.1613669 -4.1655536 -4.1669388 -4.1687942 -4.1720605 -4.1758828 -4.1808405 -4.1861863][-4.1773415 -4.1698704 -4.1605444 -4.1535349 -4.1507692 -4.1505709 -4.1521864 -4.1551476 -4.1573148 -4.1596251 -4.1632137 -4.1699762 -4.1788483 -4.1881304 -4.1962824]]...]
INFO - root - 2017-12-06 01:24:49.671189: step 62710, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 63h:47m:27s remains)
INFO - root - 2017-12-06 01:24:58.256326: step 62720, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 63h:48m:28s remains)
INFO - root - 2017-12-06 01:25:06.845846: step 62730, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 64h:26m:07s remains)
INFO - root - 2017-12-06 01:25:15.347404: step 62740, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 65h:12m:38s remains)
INFO - root - 2017-12-06 01:25:23.801284: step 62750, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 64h:04m:37s remains)
INFO - root - 2017-12-06 01:25:32.357492: step 62760, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 64h:27m:05s remains)
INFO - root - 2017-12-06 01:25:40.749683: step 62770, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 0.776 sec/batch; 58h:07m:20s remains)
INFO - root - 2017-12-06 01:25:49.350356: step 62780, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 62h:14m:08s remains)
INFO - root - 2017-12-06 01:25:57.825885: step 62790, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 64h:32m:59s remains)
INFO - root - 2017-12-06 01:26:06.282948: step 62800, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.856 sec/batch; 64h:05m:49s remains)
2017-12-06 01:26:07.062122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.246429 -4.2475328 -4.2471828 -4.2470098 -4.2474823 -4.2487292 -4.2503858 -4.2517805 -4.2523112 -4.2521276 -4.2515163 -4.2507257 -4.2501078 -4.2497911 -4.2491679][-4.2468681 -4.2484107 -4.2486959 -4.2494822 -4.2513728 -4.2542977 -4.2569737 -4.258904 -4.2597542 -4.2600656 -4.25967 -4.2585211 -4.2571259 -4.2559748 -4.2546306][-4.2412133 -4.2443171 -4.2465887 -4.2487931 -4.251935 -4.2552915 -4.2577052 -4.25864 -4.2590952 -4.2600179 -4.2610078 -4.2614193 -4.2605081 -4.2583227 -4.2551842][-4.223145 -4.2263575 -4.2307253 -4.2334952 -4.235682 -4.2376757 -4.239212 -4.2398562 -4.2417746 -4.2458854 -4.2514 -4.2562151 -4.2569656 -4.2533813 -4.2478204][-4.2007275 -4.2006631 -4.2011714 -4.1977506 -4.195024 -4.1942735 -4.1948352 -4.1961904 -4.1996036 -4.2075748 -4.2203941 -4.2339506 -4.2422452 -4.2419224 -4.2377257][-4.1768656 -4.1659336 -4.1550212 -4.1401048 -4.1281753 -4.12366 -4.1249523 -4.1283417 -4.1337767 -4.1456051 -4.1669822 -4.1936722 -4.2149782 -4.2245789 -4.2283874][-4.1621995 -4.1383376 -4.1143885 -4.0862103 -4.0635128 -4.0555906 -4.0583992 -4.0610294 -4.0637083 -4.073946 -4.0992961 -4.1381373 -4.1744232 -4.1995592 -4.2167768][-4.1729937 -4.1429157 -4.1120687 -4.0761008 -4.0479116 -4.0363116 -4.03525 -4.0315137 -4.0282845 -4.0325232 -4.0543933 -4.0947747 -4.1372738 -4.1713924 -4.1975026][-4.1965146 -4.170166 -4.1432328 -4.1122236 -4.08662 -4.0736217 -4.0665646 -4.0575895 -4.05263 -4.055769 -4.0725751 -4.1021357 -4.1346869 -4.1634345 -4.1863308][-4.215498 -4.2035193 -4.1919575 -4.1749096 -4.1572723 -4.144217 -4.1305537 -4.1169548 -4.1123476 -4.1173596 -4.1303878 -4.1462855 -4.1628451 -4.1772604 -4.1886706][-4.2257552 -4.23061 -4.2354779 -4.2339787 -4.2273016 -4.2154064 -4.1990123 -4.1865292 -4.1832423 -4.187367 -4.1941919 -4.1970577 -4.2004933 -4.2041817 -4.2064519][-4.2182627 -4.2335634 -4.2465963 -4.2540722 -4.254653 -4.2468543 -4.2350612 -4.2303309 -4.2342749 -4.2411718 -4.2447267 -4.2406411 -4.2382526 -4.2384553 -4.237752][-4.1985326 -4.2155776 -4.2274451 -4.2340431 -4.2344751 -4.2299194 -4.2244053 -4.2269664 -4.2379875 -4.2486615 -4.2528358 -4.2516842 -4.2530441 -4.2568207 -4.2571297][-4.1824093 -4.1941094 -4.2022209 -4.2071753 -4.2053318 -4.198132 -4.1944132 -4.2016172 -4.2162123 -4.2271328 -4.232583 -4.2394247 -4.2496295 -4.2577438 -4.2606916][-4.1742048 -4.1772094 -4.1782184 -4.1796093 -4.1740742 -4.1640782 -4.161859 -4.1724157 -4.1890969 -4.1982446 -4.2030029 -4.2153668 -4.231854 -4.2443275 -4.24935]]...]
INFO - root - 2017-12-06 01:26:15.700406: step 62810, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 67h:09m:41s remains)
INFO - root - 2017-12-06 01:26:24.200426: step 62820, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 64h:57m:58s remains)
INFO - root - 2017-12-06 01:26:32.795407: step 62830, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 63h:40m:49s remains)
INFO - root - 2017-12-06 01:26:41.388421: step 62840, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 64h:41m:56s remains)
INFO - root - 2017-12-06 01:26:50.065688: step 62850, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.833 sec/batch; 62h:25m:47s remains)
INFO - root - 2017-12-06 01:26:58.769217: step 62860, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 65h:30m:50s remains)
INFO - root - 2017-12-06 01:27:07.181988: step 62870, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 54h:30m:13s remains)
INFO - root - 2017-12-06 01:27:15.592681: step 62880, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 63h:47m:18s remains)
INFO - root - 2017-12-06 01:27:24.120677: step 62890, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 63h:01m:24s remains)
INFO - root - 2017-12-06 01:27:32.544729: step 62900, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 61h:42m:05s remains)
2017-12-06 01:27:33.351155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1799474 -4.1260734 -4.0942178 -4.1041217 -4.1472306 -4.194623 -4.2226696 -4.2214136 -4.2077465 -4.1995931 -4.1998553 -4.1991558 -4.1938987 -4.1946712 -4.2009292][-4.1601205 -4.1046081 -4.0810552 -4.107841 -4.1623836 -4.2062664 -4.2215819 -4.2095189 -4.1908851 -4.1853676 -4.1950603 -4.2009716 -4.1947474 -4.1896749 -4.1923046][-4.15868 -4.1160793 -4.1049948 -4.1402297 -4.1928887 -4.2203546 -4.2095904 -4.1769366 -4.1516657 -4.1565914 -4.186604 -4.2080512 -4.209753 -4.2058892 -4.2085881][-4.1713533 -4.1461535 -4.1465325 -4.1805544 -4.2194233 -4.2287674 -4.190896 -4.1296258 -4.0930305 -4.110395 -4.1627064 -4.206953 -4.2282705 -4.23802 -4.2468624][-4.2021918 -4.1888003 -4.1953998 -4.2233648 -4.246233 -4.2377539 -4.1748877 -4.0822825 -4.0269656 -4.0491705 -4.1203313 -4.1907845 -4.2344589 -4.2590675 -4.2770348][-4.2330933 -4.231298 -4.2427096 -4.2626443 -4.268683 -4.2410398 -4.1555486 -4.0355177 -3.960845 -3.9842634 -4.0718288 -4.1652923 -4.2275071 -4.262218 -4.2852049][-4.2572255 -4.2660751 -4.2775822 -4.287631 -4.2755227 -4.2236671 -4.1181016 -3.9776034 -3.8894703 -3.9207282 -4.024991 -4.1368051 -4.212266 -4.2529721 -4.2761788][-4.2732258 -4.2841668 -4.2887664 -4.2864404 -4.2605562 -4.1940179 -4.0796232 -3.9338896 -3.8432221 -3.8819556 -3.9976232 -4.1254978 -4.2084417 -4.2488661 -4.2679071][-4.2724333 -4.2796955 -4.2746167 -4.2631526 -4.2346992 -4.1731019 -4.0709424 -3.942482 -3.8674016 -3.9100809 -4.02188 -4.146441 -4.2259951 -4.26001 -4.2710586][-4.2641597 -4.2660732 -4.2548332 -4.2405839 -4.2214293 -4.177783 -4.0987325 -3.99877 -3.9468176 -3.989769 -4.0851865 -4.1871243 -4.2538691 -4.2778172 -4.2778745][-4.2595816 -4.2589021 -4.2432208 -4.2292709 -4.2194171 -4.191597 -4.1360941 -4.0696697 -4.0423412 -4.0831842 -4.15841 -4.2355442 -4.2887688 -4.302649 -4.2921472][-4.2557364 -4.2553859 -4.240591 -4.2293181 -4.2242885 -4.20718 -4.17107 -4.1332669 -4.1293564 -4.1695476 -4.226047 -4.280436 -4.3149209 -4.3171129 -4.2994065][-4.256928 -4.2548881 -4.2416086 -4.23131 -4.2280164 -4.2208643 -4.2058816 -4.1939044 -4.2055144 -4.240108 -4.2792611 -4.312324 -4.3273983 -4.3179927 -4.2962093][-4.2700434 -4.2660232 -4.2534494 -4.2409544 -4.2342081 -4.2313557 -4.2318535 -4.2374945 -4.2550778 -4.2823048 -4.3083267 -4.3258691 -4.3247676 -4.30589 -4.2852192][-4.279458 -4.2743859 -4.2616634 -4.2464724 -4.2363143 -4.2349725 -4.2415042 -4.2577376 -4.27969 -4.3010635 -4.3151689 -4.3177657 -4.3049822 -4.2834725 -4.2670727]]...]
INFO - root - 2017-12-06 01:27:41.956075: step 62910, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 64h:37m:09s remains)
INFO - root - 2017-12-06 01:27:50.534239: step 62920, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 64h:11m:39s remains)
INFO - root - 2017-12-06 01:27:59.110812: step 62930, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 63h:31m:56s remains)
INFO - root - 2017-12-06 01:28:07.694697: step 62940, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 64h:33m:27s remains)
INFO - root - 2017-12-06 01:28:16.254849: step 62950, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 63h:43m:10s remains)
INFO - root - 2017-12-06 01:28:24.825312: step 62960, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.872 sec/batch; 65h:18m:07s remains)
INFO - root - 2017-12-06 01:28:33.176059: step 62970, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 53h:49m:31s remains)
INFO - root - 2017-12-06 01:28:41.717615: step 62980, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 64h:01m:29s remains)
INFO - root - 2017-12-06 01:28:50.249621: step 62990, loss = 2.12, batch loss = 2.06 (9.6 examples/sec; 0.832 sec/batch; 62h:16m:07s remains)
INFO - root - 2017-12-06 01:28:58.681279: step 63000, loss = 2.04, batch loss = 1.98 (10.2 examples/sec; 0.782 sec/batch; 58h:34m:32s remains)
2017-12-06 01:28:59.497837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0597987 -4.10136 -4.1185279 -4.1121759 -4.0997934 -4.1021457 -4.1209097 -4.1337595 -4.1285386 -4.1243896 -4.1338983 -4.1365013 -4.1371627 -4.1500597 -4.1579833][-4.0896053 -4.1292434 -4.1361456 -4.12141 -4.1041937 -4.0988216 -4.1151733 -4.1320004 -4.1336541 -4.1343603 -4.1494241 -4.1548324 -4.1517038 -4.1569777 -4.1602311][-4.1093698 -4.1329432 -4.1290069 -4.1092567 -4.0934854 -4.0864305 -4.0936508 -4.1081262 -4.1232023 -4.1399755 -4.163178 -4.1670456 -4.156158 -4.150846 -4.1444316][-4.1049333 -4.1166677 -4.1088238 -4.0935669 -4.0837536 -4.0713429 -4.063776 -4.0720096 -4.1042066 -4.1395183 -4.1675158 -4.1636281 -4.1377888 -4.11881 -4.1016197][-4.086298 -4.0959458 -4.0935712 -4.083622 -4.0731039 -4.0444088 -4.0118713 -4.0135098 -4.0628152 -4.1141977 -4.1458087 -4.1360064 -4.1016531 -4.0786948 -4.0621982][-4.0608144 -4.0662746 -4.0647578 -4.0532751 -4.0279093 -3.9675238 -3.8950043 -3.8866162 -3.9648306 -4.0420756 -4.0844097 -4.0825243 -4.0650587 -4.0604696 -4.0559835][-4.06195 -4.0534468 -4.0438375 -4.0194335 -3.970259 -3.8718469 -3.7625644 -3.7491603 -3.8540258 -3.9578607 -4.0121446 -4.0281334 -4.0409184 -4.0682673 -4.086617][-4.1044016 -4.0804253 -4.0600266 -4.0280261 -3.9704866 -3.8702462 -3.7724583 -3.7626352 -3.8529298 -3.9446936 -3.9975486 -4.0263877 -4.0659113 -4.1185665 -4.15761][-4.1494827 -4.1279221 -4.1101847 -4.0786953 -4.0240808 -3.946909 -3.8859022 -3.8864219 -3.9425149 -3.9977143 -4.0312271 -4.0588236 -4.1070094 -4.172544 -4.22069][-4.1574011 -4.1463618 -4.1333385 -4.1030178 -4.0567884 -4.0063529 -3.9803357 -3.9948373 -4.0302248 -4.0520649 -4.0631151 -4.0852108 -4.1314168 -4.1925187 -4.2314563][-4.1372423 -4.1401572 -4.1337342 -4.1095891 -4.0778379 -4.0541344 -4.0564861 -4.0809174 -4.104353 -4.1109028 -4.1115131 -4.1210876 -4.1507139 -4.1886458 -4.205822][-4.1193647 -4.1418982 -4.143815 -4.124052 -4.1077256 -4.1068654 -4.1249204 -4.1469345 -4.1597729 -4.1604605 -4.1591177 -4.1602206 -4.1720533 -4.1866016 -4.1841822][-4.1201224 -4.1564116 -4.1635985 -4.1490059 -4.1445489 -4.152565 -4.1698956 -4.1822715 -4.1868711 -4.1867414 -4.1871486 -4.1868186 -4.1904993 -4.1939406 -4.1830378][-4.1428137 -4.1789761 -4.1862574 -4.1778822 -4.1787348 -4.1868911 -4.1973052 -4.2039528 -4.2070155 -4.2078886 -4.2087064 -4.2081404 -4.2088065 -4.2090411 -4.198925][-4.1928887 -4.2171764 -4.2223177 -4.21966 -4.2234511 -4.2295108 -4.233861 -4.23548 -4.2369657 -4.237906 -4.2381964 -4.2375445 -4.238205 -4.237565 -4.2327223]]...]
INFO - root - 2017-12-06 01:29:08.048049: step 63010, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 62h:40m:56s remains)
INFO - root - 2017-12-06 01:29:16.615508: step 63020, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 62h:01m:01s remains)
INFO - root - 2017-12-06 01:29:25.171672: step 63030, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 64h:25m:09s remains)
INFO - root - 2017-12-06 01:29:33.746871: step 63040, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 64h:29m:55s remains)
INFO - root - 2017-12-06 01:29:42.313287: step 63050, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 64h:19m:01s remains)
INFO - root - 2017-12-06 01:29:50.943329: step 63060, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 64h:42m:50s remains)
INFO - root - 2017-12-06 01:29:59.389473: step 63070, loss = 2.04, batch loss = 1.98 (10.7 examples/sec; 0.745 sec/batch; 55h:45m:18s remains)
INFO - root - 2017-12-06 01:30:07.912172: step 63080, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 65h:28m:13s remains)
INFO - root - 2017-12-06 01:30:16.441887: step 63090, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 63h:47m:48s remains)
INFO - root - 2017-12-06 01:30:25.116188: step 63100, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 64h:09m:51s remains)
2017-12-06 01:30:25.894779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1714745 -4.1554685 -4.1472793 -4.1543946 -4.1728888 -4.1969547 -4.21433 -4.21562 -4.2012939 -4.1826773 -4.168788 -4.1567106 -4.1502166 -4.1544294 -4.1637049][-4.1649041 -4.1425371 -4.1302004 -4.1326089 -4.1495881 -4.1758475 -4.1903625 -4.1845183 -4.1670632 -4.1518183 -4.1417933 -4.1319513 -4.12584 -4.1315923 -4.1428065][-4.1621819 -4.1296444 -4.1109223 -4.1100426 -4.1290364 -4.1559443 -4.1674423 -4.1584582 -4.146677 -4.1455541 -4.1410017 -4.128499 -4.1127038 -4.1075664 -4.1141806][-4.1475067 -4.1085677 -4.0865083 -4.0873113 -4.1110678 -4.13778 -4.1461043 -4.1335683 -4.12478 -4.1389332 -4.1430669 -4.1265206 -4.0949278 -4.0712237 -4.0681515][-4.12018 -4.085474 -4.073967 -4.0840917 -4.1079016 -4.1229577 -4.1155925 -4.0906835 -4.0797811 -4.1047244 -4.1205912 -4.1063862 -4.062942 -4.0262756 -4.0246363][-4.106039 -4.0892148 -4.0915246 -4.1026511 -4.1131706 -4.1011143 -4.0634222 -4.0182891 -4.0123844 -4.054872 -4.0871239 -4.0786982 -4.0358167 -4.0037231 -4.0100527][-4.1012983 -4.1022038 -4.1085091 -4.1077356 -4.09559 -4.0507469 -3.9720526 -3.9030862 -3.928463 -4.0067272 -4.0629606 -4.0724239 -4.0520906 -4.0387712 -4.0434866][-4.0853214 -4.0962954 -4.1035533 -4.0935626 -4.0643964 -3.996506 -3.8875265 -3.8151689 -3.8930142 -4.0088778 -4.0841045 -4.1112814 -4.1072164 -4.0934954 -4.0846796][-4.0757103 -4.097939 -4.1106429 -4.1006694 -4.0678434 -4.0035605 -3.9222713 -3.8906031 -3.9671133 -4.0653906 -4.1285906 -4.1501408 -4.1436229 -4.1233034 -4.1020989][-4.1019788 -4.12802 -4.139873 -4.1273818 -4.0984926 -4.0504856 -4.0042706 -4.000567 -4.0578723 -4.1231575 -4.1631432 -4.1725116 -4.1606989 -4.1338139 -4.1008229][-4.1435285 -4.1571856 -4.1520238 -4.1305814 -4.1065993 -4.0740366 -4.0550032 -4.0734439 -4.1221514 -4.1669273 -4.1887755 -4.1906433 -4.1753283 -4.1430187 -4.1045017][-4.1691141 -4.1664891 -4.1463375 -4.1216292 -4.1067514 -4.0942831 -4.0989513 -4.1320119 -4.1751289 -4.2088909 -4.2227049 -4.2201209 -4.2011313 -4.1681252 -4.1326323][-4.1922994 -4.1824327 -4.1593685 -4.1368985 -4.1275487 -4.1294785 -4.1469488 -4.1846523 -4.2219238 -4.2474132 -4.2569571 -4.2531629 -4.2321196 -4.2018261 -4.1733904][-4.2267494 -4.2146473 -4.1961513 -4.1781011 -4.169878 -4.1748395 -4.1927605 -4.2265644 -4.25534 -4.2718716 -4.27687 -4.2704782 -4.2504616 -4.2246966 -4.2004504][-4.2660646 -4.2536564 -4.2398648 -4.2274895 -4.2204967 -4.2233043 -4.2380919 -4.2625618 -4.2814312 -4.28895 -4.2881484 -4.2790184 -4.2609205 -4.2404351 -4.2207322]]...]
INFO - root - 2017-12-06 01:30:34.312869: step 63110, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 64h:49m:05s remains)
INFO - root - 2017-12-06 01:30:42.989398: step 63120, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 64h:22m:41s remains)
INFO - root - 2017-12-06 01:30:51.413907: step 63130, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 62h:59m:46s remains)
INFO - root - 2017-12-06 01:31:00.049238: step 63140, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 65h:00m:16s remains)
INFO - root - 2017-12-06 01:31:08.542669: step 63150, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.847 sec/batch; 63h:20m:15s remains)
INFO - root - 2017-12-06 01:31:17.049771: step 63160, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 61h:44m:13s remains)
INFO - root - 2017-12-06 01:31:25.615762: step 63170, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.802 sec/batch; 60h:02m:16s remains)
INFO - root - 2017-12-06 01:31:34.198832: step 63180, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 63h:07m:29s remains)
INFO - root - 2017-12-06 01:31:42.823147: step 63190, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 65h:03m:43s remains)
INFO - root - 2017-12-06 01:31:51.392792: step 63200, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 63h:27m:50s remains)
2017-12-06 01:31:52.086686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2958302 -4.2940884 -4.2937732 -4.2948642 -4.2920408 -4.2834125 -4.2718244 -4.2529826 -4.2302532 -4.2132807 -4.2017231 -4.1936584 -4.1922355 -4.2009768 -4.21893][-4.297576 -4.2965832 -4.2974515 -4.2978458 -4.2942524 -4.2886019 -4.2794862 -4.262835 -4.24455 -4.2309551 -4.2172065 -4.2045097 -4.1980562 -4.2041984 -4.2232285][-4.2998862 -4.30046 -4.3019528 -4.2993722 -4.2926326 -4.2868571 -4.2783828 -4.2647815 -4.252892 -4.2431855 -4.2281122 -4.2168145 -4.2130995 -4.2190404 -4.234777][-4.2854366 -4.286705 -4.2871614 -4.279851 -4.2687011 -4.2611375 -4.2540207 -4.2440691 -4.2382679 -4.2324386 -4.2209682 -4.2190089 -4.2227798 -4.22841 -4.23987][-4.2511539 -4.2524104 -4.2501421 -4.24009 -4.2288737 -4.2218218 -4.2174435 -4.2101369 -4.2058377 -4.1986823 -4.1906366 -4.197515 -4.2103643 -4.2173185 -4.2256145][-4.206954 -4.205874 -4.1997995 -4.1873407 -4.1760564 -4.1682453 -4.1639934 -4.1600537 -4.1570354 -4.1470795 -4.1385646 -4.1479716 -4.1647196 -4.17252 -4.1801767][-4.1639719 -4.1567707 -4.142478 -4.1231441 -4.1098328 -4.1007829 -4.0990357 -4.1043015 -4.1081653 -4.0994453 -4.0883508 -4.0914655 -4.1035643 -4.1087947 -4.11555][-4.1377053 -4.1209016 -4.093298 -4.0644903 -4.0505991 -4.0453053 -4.05097 -4.0669303 -4.0781384 -4.0703864 -4.0508175 -4.0453157 -4.0542431 -4.0608354 -4.0712557][-4.1330614 -4.1101656 -4.0732751 -4.0426173 -4.03764 -4.0421352 -4.0541353 -4.0760369 -4.0885663 -4.0772381 -4.0509315 -4.0413809 -4.0518837 -4.065691 -4.0854335][-4.1530013 -4.1331105 -4.095705 -4.0703297 -4.0759482 -4.0870667 -4.0993409 -4.1179824 -4.1278124 -4.1164694 -4.0913477 -4.084218 -4.0985107 -4.1196074 -4.1453109][-4.1832614 -4.1671214 -4.1311965 -4.111783 -4.12349 -4.1390376 -4.1498466 -4.1642294 -4.1736021 -4.1681986 -4.1524854 -4.1526456 -4.1691866 -4.1870174 -4.2018228][-4.2019367 -4.1859045 -4.1481566 -4.1292148 -4.1409435 -4.158442 -4.1706362 -4.1836114 -4.1975794 -4.201427 -4.1967554 -4.204391 -4.2214642 -4.2310843 -4.2325678][-4.2044559 -4.1862435 -4.1475353 -4.1284022 -4.1351776 -4.152504 -4.1654878 -4.1787133 -4.1994848 -4.2151656 -4.2198796 -4.227983 -4.237792 -4.2379026 -4.2265296][-4.1991115 -4.1792021 -4.1397295 -4.1209922 -4.1258092 -4.1467514 -4.1645808 -4.1822405 -4.2060165 -4.2254066 -4.228148 -4.2272091 -4.2231231 -4.2106447 -4.1865287][-4.1960564 -4.1788921 -4.1412883 -4.12597 -4.1347146 -4.1599779 -4.1812763 -4.1982446 -4.2163196 -4.2282863 -4.2198067 -4.2030811 -4.1869879 -4.1689157 -4.1422863]]...]
INFO - root - 2017-12-06 01:32:00.738434: step 63210, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 63h:23m:54s remains)
INFO - root - 2017-12-06 01:32:09.088065: step 63220, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 64h:06m:30s remains)
INFO - root - 2017-12-06 01:32:17.648145: step 63230, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 64h:09m:36s remains)
INFO - root - 2017-12-06 01:32:26.249406: step 63240, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 61h:32m:35s remains)
INFO - root - 2017-12-06 01:32:34.828321: step 63250, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 64h:30m:17s remains)
INFO - root - 2017-12-06 01:32:43.389504: step 63260, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 65h:30m:51s remains)
INFO - root - 2017-12-06 01:32:51.909134: step 63270, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.754 sec/batch; 56h:21m:53s remains)
INFO - root - 2017-12-06 01:33:00.567744: step 63280, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 64h:10m:47s remains)
INFO - root - 2017-12-06 01:33:09.081513: step 63290, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 62h:50m:33s remains)
INFO - root - 2017-12-06 01:33:17.728950: step 63300, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 63h:22m:20s remains)
2017-12-06 01:33:18.563319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2597876 -4.2595267 -4.2519526 -4.2437754 -4.2373238 -4.234025 -4.2439752 -4.253264 -4.2423515 -4.22271 -4.2131267 -4.2269821 -4.2567844 -4.2740474 -4.28177][-4.2558136 -4.2604184 -4.2570105 -4.2492461 -4.2357678 -4.2211118 -4.2236805 -4.2266231 -4.2185512 -4.2082286 -4.205193 -4.2230759 -4.256546 -4.2748895 -4.2822256][-4.2539082 -4.262198 -4.2641492 -4.2552147 -4.2306118 -4.2060146 -4.2003479 -4.1995592 -4.1993136 -4.1949954 -4.1954584 -4.2192683 -4.2571735 -4.2764688 -4.283977][-4.2521524 -4.2626834 -4.2671065 -4.2514768 -4.2163854 -4.1818318 -4.1674118 -4.1666536 -4.1754713 -4.1761904 -4.1801095 -4.2139478 -4.257216 -4.2784781 -4.2868977][-4.2450705 -4.2557383 -4.2594519 -4.2395029 -4.1966691 -4.1519213 -4.1211753 -4.111876 -4.122767 -4.1291685 -4.1436496 -4.1933174 -4.2478352 -4.2759824 -4.2869225][-4.2288389 -4.2382836 -4.2414861 -4.2213359 -4.1762772 -4.1188035 -4.0646725 -4.0323114 -4.0275688 -4.0343456 -4.0703654 -4.14648 -4.220892 -4.26068 -4.2783775][-4.1955428 -4.2084823 -4.2140326 -4.1949697 -4.1467247 -4.0787692 -4.003953 -3.9424982 -3.9085889 -3.9121165 -3.9758821 -4.0820556 -4.1788535 -4.2310143 -4.25655][-4.1467395 -4.1744108 -4.1928768 -4.1845427 -4.1444335 -4.0787678 -3.99735 -3.921998 -3.8692527 -3.8645983 -3.9323967 -4.0427284 -4.14363 -4.1997604 -4.2293959][-4.1341467 -4.174963 -4.209866 -4.2168851 -4.1933808 -4.1430578 -4.0740204 -4.0067883 -3.9606185 -3.9489307 -3.987915 -4.0692844 -4.1509247 -4.1980128 -4.2233262][-4.1497197 -4.18911 -4.2255349 -4.2423267 -4.233973 -4.20125 -4.1492333 -4.0941911 -4.0590587 -4.0412078 -4.0521517 -4.107286 -4.1737108 -4.2140121 -4.2351604][-4.1752367 -4.2075858 -4.2385473 -4.2546568 -4.2510643 -4.2285271 -4.19159 -4.1524205 -4.1285839 -4.110136 -4.1089945 -4.1477375 -4.201992 -4.2333989 -4.2500644][-4.2106609 -4.2294693 -4.2489347 -4.25854 -4.2566147 -4.2442107 -4.2225013 -4.1985388 -4.1822052 -4.1652508 -4.1598349 -4.1859508 -4.2268796 -4.2494178 -4.263968][-4.2383366 -4.2470536 -4.2574315 -4.2620769 -4.2610245 -4.2558665 -4.2464976 -4.235239 -4.2262182 -4.2125859 -4.2032814 -4.2177873 -4.2438478 -4.2600989 -4.2739024][-4.2700248 -4.2703834 -4.2726398 -4.2732735 -4.2732229 -4.2716775 -4.2680864 -4.2638435 -4.2595253 -4.2513657 -4.2442541 -4.2518935 -4.2675939 -4.2788239 -4.2878008][-4.2900152 -4.28702 -4.28626 -4.2854123 -4.284656 -4.2832584 -4.2821245 -4.2812142 -4.28041 -4.2771645 -4.2728953 -4.2760572 -4.2854295 -4.2936673 -4.2989159]]...]
INFO - root - 2017-12-06 01:33:27.052562: step 63310, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 64h:51m:46s remains)
INFO - root - 2017-12-06 01:33:35.529169: step 63320, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 65h:31m:06s remains)
INFO - root - 2017-12-06 01:33:44.060564: step 63330, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 63h:08m:51s remains)
INFO - root - 2017-12-06 01:33:52.642064: step 63340, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 63h:45m:12s remains)
INFO - root - 2017-12-06 01:34:01.285335: step 63350, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 65h:39m:53s remains)
INFO - root - 2017-12-06 01:34:09.842659: step 63360, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 65h:18m:57s remains)
INFO - root - 2017-12-06 01:34:18.313027: step 63370, loss = 2.05, batch loss = 1.99 (10.5 examples/sec; 0.765 sec/batch; 57h:11m:40s remains)
INFO - root - 2017-12-06 01:34:26.748125: step 63380, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 62h:27m:45s remains)
INFO - root - 2017-12-06 01:34:35.432037: step 63390, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 65h:43m:20s remains)
INFO - root - 2017-12-06 01:34:43.999305: step 63400, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 63h:22m:12s remains)
2017-12-06 01:34:44.783903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2655773 -4.2853484 -4.2957582 -4.304997 -4.3125834 -4.314055 -4.3110805 -4.3069148 -4.3053346 -4.30929 -4.3168468 -4.3252664 -4.3339396 -4.3355775 -4.3331451][-4.2698565 -4.2773185 -4.2765479 -4.2773037 -4.2809024 -4.283155 -4.2853823 -4.285027 -4.2834129 -4.2870917 -4.2954144 -4.3044128 -4.3119545 -4.3153262 -4.3173723][-4.2793226 -4.2714567 -4.2538614 -4.2408409 -4.2370462 -4.2373729 -4.2385383 -4.2345576 -4.2270718 -4.2298689 -4.2447314 -4.2605724 -4.27668 -4.2895045 -4.2993288][-4.283536 -4.2582345 -4.2247343 -4.196238 -4.1799312 -4.1722589 -4.1654725 -4.1495 -4.1299109 -4.13456 -4.1643367 -4.19759 -4.2305903 -4.2593622 -4.2807078][-4.2724791 -4.2325978 -4.181488 -4.1304445 -4.0948644 -4.0752096 -4.0433774 -3.9985762 -3.96692 -3.9867475 -4.0477691 -4.1087022 -4.1647511 -4.2182956 -4.2559819][-4.25014 -4.1969981 -4.1247797 -4.0419145 -3.9824042 -3.9429533 -3.8714325 -3.7799408 -3.7407894 -3.7904398 -3.9018793 -4.0015149 -4.0839143 -4.1650553 -4.2233973][-4.22512 -4.1601095 -4.0697761 -3.959619 -3.8757572 -3.8089147 -3.7001653 -3.5637212 -3.5215874 -3.611882 -3.781992 -3.9239664 -4.0274434 -4.1290903 -4.2032456][-4.2007527 -4.1368189 -4.046412 -3.9359353 -3.8549314 -3.7917628 -3.6900654 -3.5657089 -3.5409389 -3.6404834 -3.8148928 -3.9541693 -4.0487881 -4.1432362 -4.2117605][-4.1884155 -4.1492753 -4.090673 -4.0142336 -3.9591765 -3.914958 -3.8470385 -3.7720752 -3.7753983 -3.8473666 -3.9643483 -4.0644746 -4.1296306 -4.1954584 -4.2391553][-4.1800613 -4.1709423 -4.1461654 -4.1043711 -4.068924 -4.0377388 -3.9965646 -3.9612942 -3.9798834 -4.0269413 -4.0953894 -4.1573005 -4.1947241 -4.2291374 -4.2507944][-4.1817346 -4.1879559 -4.1776409 -4.1583967 -4.1451674 -4.130558 -4.1063347 -4.0864782 -4.1070924 -4.1388369 -4.1749649 -4.2092633 -4.2275782 -4.2386732 -4.2480464][-4.198472 -4.2076483 -4.2047052 -4.2058854 -4.213273 -4.2130175 -4.1978068 -4.1798563 -4.1923041 -4.2107844 -4.2252541 -4.2364163 -4.2409987 -4.2415752 -4.2474232][-4.2249613 -4.2302403 -4.2272558 -4.2367196 -4.2537627 -4.2606955 -4.25016 -4.2313776 -4.2331629 -4.2393432 -4.2415171 -4.2415819 -4.2428141 -4.2448606 -4.2519684][-4.2548785 -4.2543616 -4.2513452 -4.2596917 -4.2750325 -4.2855592 -4.2858953 -4.2754478 -4.2700586 -4.2668676 -4.2626963 -4.2584772 -4.2615418 -4.2641931 -4.26628][-4.2945976 -4.2943835 -4.2930121 -4.2995167 -4.3085532 -4.3180747 -4.3264713 -4.3236203 -4.3158026 -4.3089237 -4.3036647 -4.2995543 -4.3001018 -4.2998424 -4.2936435]]...]
INFO - root - 2017-12-06 01:34:53.269260: step 63410, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.828 sec/batch; 61h:55m:09s remains)
INFO - root - 2017-12-06 01:35:01.778537: step 63420, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 63h:28m:59s remains)
INFO - root - 2017-12-06 01:35:10.169096: step 63430, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 64h:47m:41s remains)
INFO - root - 2017-12-06 01:35:18.626563: step 63440, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 62h:23m:36s remains)
INFO - root - 2017-12-06 01:35:27.221348: step 63450, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 65h:41m:14s remains)
INFO - root - 2017-12-06 01:35:35.897627: step 63460, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 63h:20m:07s remains)
INFO - root - 2017-12-06 01:35:44.242356: step 63470, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.764 sec/batch; 57h:06m:02s remains)
INFO - root - 2017-12-06 01:35:52.717487: step 63480, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 66h:08m:04s remains)
INFO - root - 2017-12-06 01:36:01.249380: step 63490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 64h:25m:15s remains)
INFO - root - 2017-12-06 01:36:09.848665: step 63500, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 63h:45m:43s remains)
2017-12-06 01:36:10.663699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3078489 -4.2811031 -4.2442722 -4.2074795 -4.1889334 -4.1883698 -4.2158003 -4.2435026 -4.2424269 -4.2082748 -4.1576414 -4.10258 -4.0644279 -4.0578017 -4.0753865][-4.3053122 -4.2797232 -4.2451673 -4.2105985 -4.1970768 -4.2051187 -4.2376456 -4.2662735 -4.2649603 -4.2300715 -4.1847548 -4.1366067 -4.095211 -4.08534 -4.095046][-4.3017049 -4.2734547 -4.2363563 -4.1977773 -4.183301 -4.1902533 -4.2234793 -4.2568488 -4.2638226 -4.2362895 -4.20506 -4.1685925 -4.1345658 -4.1274447 -4.1330967][-4.2980404 -4.2675362 -4.2278028 -4.1806231 -4.1572447 -4.1544757 -4.1805854 -4.2203031 -4.2395339 -4.2259188 -4.2086105 -4.1875057 -4.168417 -4.1666174 -4.1683412][-4.2966609 -4.2661567 -4.224853 -4.1710296 -4.1394091 -4.1268148 -4.1404319 -4.1783195 -4.2111855 -4.2131543 -4.2098031 -4.2031336 -4.1997056 -4.2016973 -4.1964593][-4.2976413 -4.2691679 -4.2272258 -4.1720257 -4.1378884 -4.11695 -4.1148219 -4.1438971 -4.1812215 -4.1966205 -4.2068944 -4.21116 -4.2162666 -4.2191648 -4.2087145][-4.2996283 -4.2721524 -4.2318025 -4.1808767 -4.147954 -4.12562 -4.1179252 -4.1402297 -4.1696248 -4.1859236 -4.1998377 -4.2064061 -4.2071652 -4.2041397 -4.190032][-4.2980137 -4.2725759 -4.2355542 -4.18948 -4.1589637 -4.1426697 -4.1344266 -4.1472225 -4.1607184 -4.16577 -4.1730876 -4.1778283 -4.1747246 -4.1696239 -4.16144][-4.288269 -4.26344 -4.227622 -4.1849227 -4.1597695 -4.1505466 -4.145328 -4.1453738 -4.141161 -4.133635 -4.1334734 -4.1365314 -4.1284828 -4.1218386 -4.1243248][-4.2727151 -4.2455354 -4.2109737 -4.1707978 -4.1519241 -4.1468658 -4.148314 -4.1463237 -4.1308718 -4.1103959 -4.1044936 -4.1043496 -4.0876541 -4.0714006 -4.0788403][-4.26162 -4.2309942 -4.1934471 -4.1530371 -4.1413894 -4.1401558 -4.1485348 -4.160377 -4.147778 -4.118926 -4.1037836 -4.0940332 -4.0624967 -4.0300369 -4.0348411][-4.259254 -4.22869 -4.1900334 -4.1497416 -4.1422577 -4.14408 -4.1549945 -4.1743312 -4.1705766 -4.1421824 -4.12113 -4.10056 -4.0592823 -4.0155344 -4.0145259][-4.2620664 -4.2334938 -4.1969504 -4.1586857 -4.1521993 -4.1556053 -4.1687346 -4.1865492 -4.1859837 -4.1624169 -4.143333 -4.1200776 -4.0778127 -4.0329361 -4.0285606][-4.267199 -4.2360125 -4.2009039 -4.1663394 -4.1642838 -4.1710768 -4.1827435 -4.1947503 -4.1937237 -4.1762733 -4.1624465 -4.1431713 -4.1065512 -4.0709505 -4.0643454][-4.2753563 -4.2421074 -4.2066436 -4.1719313 -4.1682715 -4.1731033 -4.1816664 -4.1905179 -4.1890154 -4.1741052 -4.1643019 -4.1545596 -4.131475 -4.1098471 -4.100131]]...]
INFO - root - 2017-12-06 01:36:19.221090: step 63510, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 64h:37m:21s remains)
INFO - root - 2017-12-06 01:36:27.797482: step 63520, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 62h:53m:16s remains)
INFO - root - 2017-12-06 01:36:36.389944: step 63530, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 63h:17m:44s remains)
INFO - root - 2017-12-06 01:36:44.809838: step 63540, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 54h:33m:50s remains)
INFO - root - 2017-12-06 01:36:53.335747: step 63550, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 64h:54m:02s remains)
INFO - root - 2017-12-06 01:37:01.816388: step 63560, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 62h:00m:54s remains)
INFO - root - 2017-12-06 01:37:10.359377: step 63570, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.749 sec/batch; 55h:58m:49s remains)
INFO - root - 2017-12-06 01:37:18.857015: step 63580, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 63h:43m:54s remains)
INFO - root - 2017-12-06 01:37:27.382465: step 63590, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 63h:36m:39s remains)
INFO - root - 2017-12-06 01:37:35.795985: step 63600, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 64h:49m:38s remains)
2017-12-06 01:37:36.580542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1676455 -4.1639895 -4.1724548 -4.1781874 -4.1673532 -4.1403961 -4.114872 -4.1055818 -4.112102 -4.1319766 -4.1618681 -4.1807466 -4.18885 -4.1913552 -4.1941133][-4.1635041 -4.1628466 -4.1741495 -4.1770878 -4.1593719 -4.1298857 -4.1066146 -4.1010413 -4.1100774 -4.1311316 -4.1600971 -4.1797566 -4.1899729 -4.1982846 -4.2008576][-4.1650591 -4.1676435 -4.1761804 -4.1717124 -4.1431427 -4.1097012 -4.0855694 -4.0817924 -4.0972424 -4.1244559 -4.1537185 -4.1735425 -4.186028 -4.200634 -4.2090898][-4.1660438 -4.169383 -4.17471 -4.1638432 -4.1285005 -4.0921974 -4.0635824 -4.0595169 -4.0824285 -4.1147084 -4.141499 -4.1587596 -4.173337 -4.1958289 -4.210845][-4.1578298 -4.16301 -4.1678882 -4.1576228 -4.1215696 -4.0805869 -4.0433078 -4.0353451 -4.0672464 -4.1023569 -4.1249928 -4.1382804 -4.1532078 -4.1809769 -4.2029605][-4.1467485 -4.1627607 -4.1680288 -4.1578279 -4.1183968 -4.0671806 -4.0134611 -3.9955735 -4.0383205 -4.0849137 -4.1118393 -4.1241417 -4.1393847 -4.1701512 -4.1966987][-4.1382475 -4.1688914 -4.1765566 -4.1640534 -4.1174784 -4.0461073 -3.9629693 -3.9292245 -3.98947 -4.0607347 -4.1083317 -4.1287704 -4.1423817 -4.17039 -4.19574][-4.1219912 -4.1660061 -4.1848173 -4.1763783 -4.1255074 -4.0346417 -3.9226379 -3.8681719 -3.9381046 -4.0340538 -4.1063638 -4.1430383 -4.1562228 -4.1756315 -4.195868][-4.10199 -4.1565504 -4.1917396 -4.1956449 -4.1494646 -4.0584521 -3.946954 -3.8809552 -3.9260793 -4.0147419 -4.0980892 -4.1518741 -4.1726661 -4.1862545 -4.1994][-4.098495 -4.1542158 -4.195065 -4.2048211 -4.1676483 -4.0944333 -4.0116634 -3.9536533 -3.960211 -4.0171771 -4.0941234 -4.1548696 -4.1845593 -4.1972642 -4.2040544][-4.1121483 -4.1615191 -4.1964645 -4.2014189 -4.173182 -4.1217685 -4.0686369 -4.0233994 -4.003139 -4.0289092 -4.0902653 -4.1473904 -4.1825771 -4.195457 -4.1967287][-4.1362138 -4.1687756 -4.1907592 -4.1893663 -4.1665726 -4.1310406 -4.1010475 -4.0685863 -4.0368223 -4.0409322 -4.0814056 -4.1284552 -4.1633759 -4.1786442 -4.1781473][-4.1648636 -4.1770606 -4.1823821 -4.1726608 -4.1488795 -4.1232719 -4.1113114 -4.0951581 -4.0677114 -4.0607066 -4.0793042 -4.1116748 -4.1461978 -4.1650352 -4.166976][-4.1789379 -4.1766329 -4.1695533 -4.1508512 -4.1198754 -4.1020827 -4.1034985 -4.1037345 -4.0894623 -4.0776663 -4.0798821 -4.0976481 -4.1317172 -4.1574936 -4.1630921][-4.1813817 -4.1763382 -4.1631212 -4.13611 -4.0999532 -4.0866313 -4.0964661 -4.109272 -4.1051087 -4.0951128 -4.0903239 -4.0972686 -4.1250987 -4.1530294 -4.1609607]]...]
INFO - root - 2017-12-06 01:37:45.077425: step 63610, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 63h:05m:16s remains)
INFO - root - 2017-12-06 01:37:53.606515: step 63620, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 65h:22m:48s remains)
INFO - root - 2017-12-06 01:38:02.051961: step 63630, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 62h:15m:43s remains)
INFO - root - 2017-12-06 01:38:10.459611: step 63640, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.839 sec/batch; 62h:39m:58s remains)
INFO - root - 2017-12-06 01:38:18.901901: step 63650, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 65h:39m:33s remains)
INFO - root - 2017-12-06 01:38:27.516962: step 63660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 64h:05m:20s remains)
INFO - root - 2017-12-06 01:38:35.893267: step 63670, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.753 sec/batch; 56h:12m:19s remains)
INFO - root - 2017-12-06 01:38:44.271016: step 63680, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 62h:31m:14s remains)
INFO - root - 2017-12-06 01:38:52.696580: step 63690, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 63h:00m:09s remains)
INFO - root - 2017-12-06 01:39:01.003934: step 63700, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 61h:48m:24s remains)
2017-12-06 01:39:01.963173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1031966 -4.1148577 -4.1227541 -4.1095967 -4.0555477 -4.0069461 -4.0523462 -4.1372423 -4.1901731 -4.2063093 -4.2074552 -4.2145534 -4.2179618 -4.2204766 -4.2281466][-4.1094084 -4.1198568 -4.1258154 -4.1139579 -4.0653915 -4.0061393 -4.031023 -4.1167793 -4.1797781 -4.2016287 -4.20265 -4.2080789 -4.2141871 -4.218133 -4.2250428][-4.1090651 -4.1165605 -4.1188021 -4.109591 -4.0696716 -4.0083308 -4.010179 -4.087306 -4.1572742 -4.185648 -4.1889091 -4.1925449 -4.2008796 -4.2092128 -4.2177105][-4.1109457 -4.1137323 -4.1173735 -4.1131458 -4.0783339 -4.0172572 -3.9911623 -4.050096 -4.1239638 -4.1617565 -4.1699433 -4.1717806 -4.1795816 -4.1912122 -4.2033825][-4.1206946 -4.1181049 -4.1231289 -4.1211219 -4.0900092 -4.0285363 -3.9743595 -4.0019979 -4.0755134 -4.1270375 -4.1457095 -4.1484556 -4.1525674 -4.1658077 -4.1817517][-4.1258788 -4.1195312 -4.1245985 -4.1241546 -4.1013179 -4.0413594 -3.9624443 -3.9474158 -4.013556 -4.0820985 -4.1156745 -4.1248288 -4.1287789 -4.1412082 -4.1569514][-4.1223454 -4.1153932 -4.1205292 -4.1235003 -4.1092119 -4.0529046 -3.9589849 -3.9038348 -3.9527586 -4.0351267 -4.0843964 -4.1048031 -4.1113553 -4.1221032 -4.1379375][-4.1106315 -4.1050377 -4.1124177 -4.1206322 -4.1143813 -4.0666389 -3.9746337 -3.8898544 -3.9078877 -3.9896803 -4.0518794 -4.0863895 -4.0973935 -4.1080379 -4.1242952][-4.0958796 -4.0932541 -4.1036024 -4.117806 -4.118896 -4.0875721 -4.0165462 -3.9214029 -3.9029582 -3.9704461 -4.0341682 -4.0720763 -4.0858603 -4.0948505 -4.1106291][-4.0862622 -4.082983 -4.092968 -4.1112313 -4.120821 -4.1103554 -4.0699186 -3.9882014 -3.9427285 -3.9875684 -4.0403676 -4.0699482 -4.0801082 -4.0870986 -4.1032009][-4.0793929 -4.0739756 -4.0796566 -4.098258 -4.1167579 -4.1272826 -4.1141157 -4.0576706 -4.0000629 -4.0176086 -4.0523577 -4.0682206 -4.0758429 -4.0813985 -4.0938454][-4.0753036 -4.0661197 -4.0665412 -4.0843983 -4.1082458 -4.132298 -4.13736 -4.10265 -4.0462952 -4.0367517 -4.0566597 -4.06303 -4.0712147 -4.075841 -4.0841155][-4.077446 -4.068326 -4.0642581 -4.0785079 -4.1038332 -4.1328893 -4.1449971 -4.1215906 -4.06805 -4.039609 -4.0524068 -4.060638 -4.0714107 -4.0752583 -4.0796719][-4.0825768 -4.0775051 -4.0740762 -4.0852609 -4.1084046 -4.1372142 -4.1489725 -4.13125 -4.077713 -4.036706 -4.0471172 -4.0682039 -4.0815778 -4.0866957 -4.0920277][-4.0846348 -4.081687 -4.0825438 -4.0924149 -4.1121273 -4.1391487 -4.1532397 -4.1425486 -4.0921993 -4.0422268 -4.0492988 -4.083827 -4.1044292 -4.1169758 -4.1285214]]...]
INFO - root - 2017-12-06 01:39:10.346755: step 63710, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 61h:44m:47s remains)
INFO - root - 2017-12-06 01:39:18.798562: step 63720, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 63h:35m:15s remains)
INFO - root - 2017-12-06 01:39:27.240505: step 63730, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 62h:42m:43s remains)
INFO - root - 2017-12-06 01:39:35.990990: step 63740, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 63h:14m:57s remains)
INFO - root - 2017-12-06 01:39:44.351537: step 63750, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 64h:11m:50s remains)
INFO - root - 2017-12-06 01:39:52.738540: step 63760, loss = 2.07, batch loss = 2.02 (10.7 examples/sec; 0.748 sec/batch; 55h:48m:42s remains)
INFO - root - 2017-12-06 01:40:01.189878: step 63770, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 64h:19m:34s remains)
INFO - root - 2017-12-06 01:40:09.546434: step 63780, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 62h:30m:47s remains)
INFO - root - 2017-12-06 01:40:18.059924: step 63790, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 64h:02m:19s remains)
INFO - root - 2017-12-06 01:40:26.522750: step 63800, loss = 2.04, batch loss = 1.99 (9.9 examples/sec; 0.807 sec/batch; 60h:15m:05s remains)
2017-12-06 01:40:27.363670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2344694 -4.1670666 -4.0775161 -4.0054021 -3.9828727 -3.9988506 -4.0535779 -4.1136446 -4.1753039 -4.2182865 -4.2204261 -4.2005854 -4.1675472 -4.1246533 -4.0983381][-4.2301779 -4.16321 -4.0771604 -4.0066404 -3.985899 -4.0081973 -4.0588264 -4.1029029 -4.1594863 -4.1997352 -4.2023215 -4.1815367 -4.1412225 -4.0928841 -4.060401][-4.2332687 -4.1745181 -4.0991864 -4.0320377 -4.0050969 -4.0229 -4.062181 -4.0935354 -4.1475158 -4.1825185 -4.1846848 -4.1626182 -4.1202226 -4.0687733 -4.0330691][-4.2507882 -4.2035375 -4.1402764 -4.0760994 -4.0373878 -4.0344992 -4.046679 -4.0679584 -4.1293173 -4.167695 -4.1708 -4.1491275 -4.1074772 -4.0514221 -4.0140915][-4.2698321 -4.2336493 -4.1767936 -4.111659 -4.0578918 -4.0215526 -3.9979746 -4.0189419 -4.1026711 -4.160192 -4.1704497 -4.15151 -4.1096869 -4.0442791 -3.9920812][-4.2800779 -4.2498097 -4.1923504 -4.1193051 -4.0456834 -3.9701505 -3.9114366 -3.9393685 -4.0559382 -4.1417894 -4.1698895 -4.1602321 -4.1218734 -4.0465074 -3.9739468][-4.2849712 -4.2535343 -4.1885734 -4.0969439 -3.9947376 -3.880748 -3.7861745 -3.8224621 -3.982183 -4.1033535 -4.1568742 -4.1605482 -4.1346722 -4.0666013 -3.993011][-4.2848558 -4.2461624 -4.1723404 -4.0642409 -3.943295 -3.8118656 -3.7019832 -3.7452779 -3.9267716 -4.0720277 -4.1446495 -4.1622124 -4.1553578 -4.1059327 -4.0433092][-4.2785478 -4.2332988 -4.1568 -4.0525455 -3.9475231 -3.8472631 -3.7750864 -3.8176682 -3.9676633 -4.0941515 -4.1591916 -4.1791978 -4.1836181 -4.1489544 -4.096911][-4.2745605 -4.2290258 -4.1568108 -4.0671115 -3.9870458 -3.9247026 -3.8898036 -3.9245887 -4.0302353 -4.1267028 -4.1787682 -4.2018108 -4.216085 -4.1953859 -4.149446][-4.2840853 -4.2489829 -4.1883931 -4.1144824 -4.0525527 -4.0115347 -3.993444 -4.0184245 -4.0916438 -4.1630864 -4.2053375 -4.2320256 -4.2551341 -4.2441239 -4.2025518][-4.2996392 -4.2806177 -4.2393069 -4.1839714 -4.1360269 -4.10568 -4.0944738 -4.1112976 -4.1596222 -4.2076726 -4.238132 -4.2633052 -4.287653 -4.2830586 -4.2518458][-4.30785 -4.30148 -4.278708 -4.24296 -4.2075191 -4.1847048 -4.1765032 -4.1874762 -4.2169504 -4.24518 -4.2637486 -4.2842827 -4.3056521 -4.3044825 -4.2839012][-4.3096237 -4.310297 -4.3001976 -4.2806058 -4.2561984 -4.2367611 -4.2281065 -4.2331338 -4.2505074 -4.2665548 -4.2762809 -4.2911172 -4.3071985 -4.3100009 -4.2990742][-4.3068457 -4.308682 -4.3045325 -4.2948661 -4.2807178 -4.2676921 -4.261343 -4.2638922 -4.2732239 -4.280931 -4.2849617 -4.2912636 -4.3009129 -4.3052192 -4.3003283]]...]
INFO - root - 2017-12-06 01:40:36.008476: step 63810, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 66h:04m:07s remains)
INFO - root - 2017-12-06 01:40:44.616473: step 63820, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 64h:41m:00s remains)
INFO - root - 2017-12-06 01:40:53.035429: step 63830, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 63h:54m:10s remains)
INFO - root - 2017-12-06 01:41:01.473769: step 63840, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 64h:00m:42s remains)
INFO - root - 2017-12-06 01:41:09.932264: step 63850, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 64h:30m:14s remains)
INFO - root - 2017-12-06 01:41:18.445643: step 63860, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 62h:15m:11s remains)
INFO - root - 2017-12-06 01:41:26.937971: step 63870, loss = 2.05, batch loss = 2.00 (10.3 examples/sec; 0.778 sec/batch; 58h:03m:46s remains)
INFO - root - 2017-12-06 01:41:35.316809: step 63880, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 63h:03m:19s remains)
INFO - root - 2017-12-06 01:41:43.808365: step 63890, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.829 sec/batch; 61h:49m:54s remains)
INFO - root - 2017-12-06 01:41:52.425822: step 63900, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 64h:36m:07s remains)
2017-12-06 01:41:53.187244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0559177 -3.9830685 -3.9863486 -4.0396976 -4.0945458 -4.1278982 -4.1533217 -4.1769195 -4.1794376 -4.1750612 -4.1610165 -4.1416616 -4.1235223 -4.1135139 -4.1140532][-4.0440474 -3.9578872 -3.9461873 -3.9928393 -4.0489821 -4.0879464 -4.1175151 -4.1540036 -4.1668377 -4.1681571 -4.1507831 -4.1235671 -4.099761 -4.0858107 -4.0873728][-4.035223 -3.940799 -3.9153821 -3.9524431 -4.0012226 -4.0394039 -4.0748906 -4.1230073 -4.1464491 -4.1566381 -4.1388683 -4.1031451 -4.0712008 -4.0612054 -4.0721903][-4.0408077 -3.9471846 -3.9104445 -3.9350743 -3.972501 -4.0077729 -4.0437717 -4.0953188 -4.1292658 -4.149797 -4.1412373 -4.10525 -4.067503 -4.0589781 -4.0751123][-4.0589633 -3.9727213 -3.9282243 -3.9368024 -3.9605372 -3.9937594 -4.0305071 -4.0831571 -4.1243396 -4.1566458 -4.1556416 -4.1231871 -4.0882316 -4.0761623 -4.0840268][-4.0715013 -3.9902625 -3.9465356 -3.948092 -3.9578109 -3.985265 -4.0192647 -4.0739589 -4.1213765 -4.1585751 -4.1584263 -4.1267591 -4.0951481 -4.0821643 -4.0764251][-4.0643215 -3.9875112 -3.9537477 -3.9595671 -3.9596431 -3.9712234 -3.9967885 -4.0507679 -4.0999784 -4.1328387 -4.132853 -4.1047549 -4.0795217 -4.0669727 -4.0490322][-4.0540533 -3.9796543 -3.9587326 -3.9767694 -3.9757037 -3.973983 -3.9860487 -4.0338969 -4.0805507 -4.1019149 -4.0951014 -4.0707521 -4.0552654 -4.0470386 -4.0274487][-4.0573459 -3.9860883 -3.970741 -3.9981766 -4.0070052 -4.0086823 -4.0143447 -4.0510249 -4.0878158 -4.0965796 -4.0789485 -4.0531311 -4.0430179 -4.0425177 -4.0333581][-4.0740433 -4.0055628 -3.9921525 -4.0240488 -4.0442934 -4.0545735 -4.0604811 -4.0805655 -4.1062489 -4.1095586 -4.0872951 -4.0602713 -4.0547423 -4.0612121 -4.0595164][-4.0939083 -4.0279284 -4.0180607 -4.0484991 -4.0722175 -4.086422 -4.0899734 -4.098969 -4.1173658 -4.1218886 -4.1048107 -4.0844927 -4.0829558 -4.0887842 -4.0868411][-4.1006823 -4.0402884 -4.0383253 -4.0647845 -4.0862813 -4.102355 -4.1065063 -4.1084709 -4.1208906 -4.1273928 -4.1204352 -4.1143003 -4.1133671 -4.1142364 -4.1079707][-4.0978727 -4.042345 -4.04738 -4.0684223 -4.0836763 -4.1014543 -4.1104655 -4.112535 -4.1222053 -4.1320467 -4.1329575 -4.1338949 -4.1309619 -4.12335 -4.1174083][-4.105341 -4.0499573 -4.0523095 -4.06868 -4.0795579 -4.1002336 -4.1164222 -4.123383 -4.1330462 -4.1385541 -4.1363029 -4.1339436 -4.1281929 -4.1214571 -4.1229539][-4.1224413 -4.06664 -4.0646586 -4.0797567 -4.0886674 -4.1061277 -4.124876 -4.1370192 -4.147953 -4.1500111 -4.1428108 -4.1388144 -4.1304517 -4.1270418 -4.1376715]]...]
INFO - root - 2017-12-06 01:42:01.678220: step 63910, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 65h:20m:49s remains)
INFO - root - 2017-12-06 01:42:10.265309: step 63920, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 64h:33m:28s remains)
INFO - root - 2017-12-06 01:42:18.868885: step 63930, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 63h:27m:53s remains)
INFO - root - 2017-12-06 01:42:27.385418: step 63940, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 64h:03m:42s remains)
INFO - root - 2017-12-06 01:42:35.879961: step 63950, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 63h:56m:34s remains)
INFO - root - 2017-12-06 01:42:44.395173: step 63960, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 0.782 sec/batch; 58h:18m:13s remains)
INFO - root - 2017-12-06 01:42:52.931407: step 63970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 64h:20m:18s remains)
INFO - root - 2017-12-06 01:43:01.175074: step 63980, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.838 sec/batch; 62h:28m:24s remains)
INFO - root - 2017-12-06 01:43:09.733941: step 63990, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 64h:04m:27s remains)
INFO - root - 2017-12-06 01:43:18.436295: step 64000, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 65h:46m:08s remains)
2017-12-06 01:43:19.262706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2848744 -4.2865219 -4.27987 -4.269876 -4.2569079 -4.2469039 -4.2417035 -4.2415352 -4.2399511 -4.2238665 -4.1962776 -4.1742535 -4.1621742 -4.1621666 -4.1717782][-4.2712731 -4.2680721 -4.2585707 -4.2461414 -4.2331061 -4.2236805 -4.2191772 -4.2183595 -4.2159042 -4.1964931 -4.1631527 -4.1373491 -4.1217923 -4.1176643 -4.1313238][-4.2563872 -4.2466283 -4.2343807 -4.2234688 -4.2121325 -4.2010989 -4.1923409 -4.1839809 -4.1781116 -4.1627331 -4.1298556 -4.10347 -4.0852108 -4.0774946 -4.0971828][-4.2439036 -4.2273345 -4.2114677 -4.2005816 -4.1875172 -4.169414 -4.1514144 -4.1349988 -4.1275167 -4.1207557 -4.095623 -4.0699973 -4.0500979 -4.0445623 -4.0769863][-4.2298479 -4.2085795 -4.1878352 -4.1730461 -4.1542397 -4.1279249 -4.0976324 -4.072382 -4.0685596 -4.0727787 -4.0615063 -4.0424581 -4.0279932 -4.0294976 -4.0715694][-4.2132816 -4.1894107 -4.1657987 -4.1460328 -4.12057 -4.0839977 -4.0311956 -3.9787531 -3.9734275 -4.0034151 -4.022974 -4.0308347 -4.0405188 -4.0518417 -4.08597][-4.1956248 -4.1689591 -4.1400928 -4.1137142 -4.081203 -4.0293207 -3.9397476 -3.8420699 -3.8449607 -3.9337382 -4.0058651 -4.0487447 -4.0796046 -4.0965323 -4.11711][-4.1782537 -4.1509585 -4.1135445 -4.0762157 -4.0375381 -3.9751816 -3.8701773 -3.7576497 -3.786855 -3.928937 -4.0301313 -4.0792751 -4.1107903 -4.127636 -4.1415753][-4.1650991 -4.1412077 -4.1000466 -4.0617657 -4.0285006 -3.9828587 -3.912415 -3.8465188 -3.8818374 -3.9994507 -4.0793276 -4.1093855 -4.1326237 -4.1469011 -4.1542926][-4.161715 -4.1475482 -4.1104774 -4.0772166 -4.0509944 -4.0217628 -3.9807639 -3.9483056 -3.9765587 -4.0610256 -4.1181135 -4.1342206 -4.1479878 -4.1575356 -4.1631064][-4.1655192 -4.1609159 -4.1289988 -4.0974116 -4.0712032 -4.045867 -4.0186453 -4.0032315 -4.0294981 -4.0949464 -4.143971 -4.1565075 -4.1654992 -4.1743026 -4.1785011][-4.16161 -4.1609025 -4.1313267 -4.0996261 -4.0776114 -4.0607886 -4.0455666 -4.0383382 -4.059288 -4.1139126 -4.1611509 -4.173737 -4.1804867 -4.1821356 -4.1793327][-4.1525335 -4.1487088 -4.1169009 -4.0824766 -4.0671535 -4.0660806 -4.0605392 -4.0587788 -4.0771914 -4.12551 -4.1665239 -4.179656 -4.1909051 -4.1916485 -4.1837373][-4.1452308 -4.1371489 -4.1065855 -4.0743032 -4.0640063 -4.0648465 -4.0630841 -4.0675416 -4.084 -4.1223893 -4.1550007 -4.1694837 -4.1849866 -4.1911888 -4.1872435][-4.15651 -4.1535177 -4.1359596 -4.1087565 -4.0918403 -4.0826578 -4.0785627 -4.0886769 -4.10433 -4.1345253 -4.1594343 -4.168838 -4.1789546 -4.1837611 -4.185154]]...]
INFO - root - 2017-12-06 01:43:27.773177: step 64010, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 63h:28m:05s remains)
INFO - root - 2017-12-06 01:43:36.240302: step 64020, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 61h:09m:09s remains)
INFO - root - 2017-12-06 01:43:44.806030: step 64030, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 64h:16m:49s remains)
INFO - root - 2017-12-06 01:43:53.362256: step 64040, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 64h:25m:14s remains)
INFO - root - 2017-12-06 01:44:01.861995: step 64050, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 62h:50m:12s remains)
INFO - root - 2017-12-06 01:44:10.343619: step 64060, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 65h:03m:44s remains)
INFO - root - 2017-12-06 01:44:18.761922: step 64070, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.854 sec/batch; 63h:38m:44s remains)
INFO - root - 2017-12-06 01:44:27.235557: step 64080, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 63h:43m:11s remains)
INFO - root - 2017-12-06 01:44:35.752344: step 64090, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 63h:52m:28s remains)
INFO - root - 2017-12-06 01:44:44.324506: step 64100, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 62h:17m:02s remains)
2017-12-06 01:44:45.051465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1646724 -4.184866 -4.2003932 -4.1965632 -4.1957707 -4.2094011 -4.229651 -4.2399292 -4.2537785 -4.2662544 -4.2699571 -4.2556057 -4.226656 -4.1943526 -4.1823077][-4.1558576 -4.1735392 -4.1829596 -4.1699281 -4.165679 -4.1823921 -4.2117977 -4.2340364 -4.2569065 -4.2763057 -4.2835121 -4.2688375 -4.2379947 -4.2021403 -4.1812587][-4.1507368 -4.1639166 -4.1660109 -4.1458549 -4.1355453 -4.14726 -4.1807013 -4.2118564 -4.2412357 -4.2673221 -4.2789335 -4.2679563 -4.2403278 -4.2025847 -4.1775422][-4.157207 -4.1614795 -4.1567683 -4.1349125 -4.12076 -4.1225662 -4.1531434 -4.1862235 -4.2167277 -4.2453041 -4.2628088 -4.2613244 -4.242455 -4.2099285 -4.1844411][-4.1604371 -4.1562047 -4.1446953 -4.1231861 -4.1031146 -4.0949574 -4.1178808 -4.1462555 -4.1741467 -4.2005486 -4.2208352 -4.2303739 -4.22621 -4.2040443 -4.1799827][-4.1475325 -4.1390133 -4.1268725 -4.1071959 -4.0823793 -4.0652218 -4.0732841 -4.0895543 -4.1126809 -4.1410985 -4.1697116 -4.1859593 -4.1900835 -4.1784363 -4.1610265][-4.1390858 -4.1246643 -4.108624 -4.0836973 -4.0523953 -4.0244622 -4.0131621 -4.0169997 -4.0391092 -4.0776815 -4.1202469 -4.1457715 -4.1532159 -4.1480675 -4.1400237][-4.1455297 -4.1273155 -4.1020379 -4.0648651 -4.02152 -3.9769754 -3.9438274 -3.9377851 -3.9601405 -4.0028214 -4.05592 -4.0982924 -4.1166348 -4.12039 -4.1184316][-4.1877532 -4.1716719 -4.1435924 -4.0994892 -4.0498133 -3.998091 -3.9528251 -3.9336646 -3.9377732 -3.9604046 -4.0098553 -4.0646105 -4.0978208 -4.1064725 -4.1030498][-4.2255731 -4.2177124 -4.1975908 -4.161798 -4.1154776 -4.0709276 -4.0348892 -4.0155716 -4.0061784 -4.0037131 -4.0307641 -4.0790129 -4.1163163 -4.1254997 -4.1212964][-4.2230029 -4.229537 -4.2336698 -4.2195787 -4.1872735 -4.1530814 -4.1279669 -4.113565 -4.1024342 -4.0947189 -4.1060553 -4.1370778 -4.1635885 -4.1673465 -4.16172][-4.1820407 -4.2025681 -4.2387581 -4.2585192 -4.2474046 -4.2238326 -4.2068663 -4.1979322 -4.1915278 -4.187808 -4.1943083 -4.2100739 -4.220892 -4.2164669 -4.2089472][-4.1434731 -4.1685 -4.2280707 -4.2747364 -4.2832403 -4.2714224 -4.2615919 -4.25878 -4.2577477 -4.2580061 -4.2640567 -4.2685318 -4.2690997 -4.2616076 -4.2533092][-4.1507525 -4.1675406 -4.2256026 -4.2763085 -4.2964363 -4.2957439 -4.2914844 -4.2917371 -4.2945457 -4.2972364 -4.3042955 -4.30577 -4.3026137 -4.293993 -4.2829304][-4.2002292 -4.2058997 -4.2462506 -4.282299 -4.3021722 -4.3042431 -4.299952 -4.3010521 -4.3073473 -4.3125963 -4.3195581 -4.3215981 -4.3196816 -4.3112931 -4.2961388]]...]
INFO - root - 2017-12-06 01:44:53.642784: step 64110, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 61h:40m:17s remains)
INFO - root - 2017-12-06 01:45:02.099229: step 64120, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 62h:17m:49s remains)
INFO - root - 2017-12-06 01:45:10.707657: step 64130, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 63h:20m:38s remains)
INFO - root - 2017-12-06 01:45:19.156448: step 64140, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 63h:57m:47s remains)
INFO - root - 2017-12-06 01:45:27.687279: step 64150, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 62h:42m:56s remains)
INFO - root - 2017-12-06 01:45:36.123033: step 64160, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 64h:47m:53s remains)
INFO - root - 2017-12-06 01:45:44.629048: step 64170, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 61h:27m:43s remains)
INFO - root - 2017-12-06 01:45:53.002608: step 64180, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 62h:01m:06s remains)
INFO - root - 2017-12-06 01:46:01.545039: step 64190, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 62h:33m:44s remains)
INFO - root - 2017-12-06 01:46:09.910059: step 64200, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.818 sec/batch; 60h:56m:18s remains)
2017-12-06 01:46:10.714712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2597733 -4.2260313 -4.1934252 -4.1514587 -4.1028233 -4.0671678 -4.0673194 -4.0864305 -4.1295266 -4.1872473 -4.2126675 -4.2117267 -4.1961489 -4.1778336 -4.1639686][-4.2568336 -4.2192478 -4.1845479 -4.1438093 -4.0978308 -4.0588326 -4.0561733 -4.0782785 -4.1220765 -4.1772213 -4.1975594 -4.1892323 -4.1691856 -4.1524997 -4.1377192][-4.2618213 -4.2267885 -4.196805 -4.1646733 -4.1263838 -4.089653 -4.0855069 -4.103425 -4.1370792 -4.1772132 -4.1919365 -4.181715 -4.1584611 -4.135417 -4.1140289][-4.2692251 -4.2363515 -4.2099404 -4.1854949 -4.1576557 -4.1277046 -4.1156921 -4.1128783 -4.1169453 -4.131793 -4.1445765 -4.14398 -4.1235561 -4.1017418 -4.0873146][-4.2734694 -4.2394247 -4.2094669 -4.1864915 -4.1650529 -4.1373034 -4.1073093 -4.0731454 -4.0426445 -4.037662 -4.0548983 -4.0751653 -4.08125 -4.0834341 -4.0893216][-4.266324 -4.2280707 -4.1886482 -4.1580596 -4.1314812 -4.0964208 -4.0469823 -3.9889889 -3.9351518 -3.9211886 -3.9557519 -4.0110021 -4.0577517 -4.0924225 -4.1176348][-4.2601295 -4.2212505 -4.1763182 -4.1362805 -4.1044769 -4.0649638 -4.007247 -3.9379625 -3.8794618 -3.8691695 -3.9228015 -4.0015321 -4.0697908 -4.1235013 -4.1595397][-4.2643185 -4.2263541 -4.1768689 -4.1234074 -4.084166 -4.04345 -3.9872918 -3.9262161 -3.8861976 -3.896451 -3.9557788 -4.0298619 -4.0935516 -4.1494522 -4.1902943][-4.2713633 -4.2349005 -4.1830935 -4.1193938 -4.0744562 -4.0401335 -3.9982505 -3.9592178 -3.940922 -3.9599569 -4.0039864 -4.058392 -4.103507 -4.1456409 -4.17562][-4.2780843 -4.2451129 -4.1974182 -4.1385612 -4.0978894 -4.07395 -4.0494981 -4.02558 -4.0147042 -4.0244436 -4.0440097 -4.0753603 -4.1042776 -4.1268482 -4.1407924][-4.2815261 -4.2523751 -4.2142735 -4.1699481 -4.1376734 -4.119215 -4.1055555 -4.0890923 -4.0764771 -4.0739861 -4.0744095 -4.0914636 -4.1120167 -4.1208143 -4.1250663][-4.2860293 -4.2596054 -4.2306972 -4.2016716 -4.1784248 -4.1633534 -4.1536674 -4.1421781 -4.1331315 -4.1302743 -4.1281862 -4.1362596 -4.1460886 -4.1427097 -4.1392641][-4.2939482 -4.2701654 -4.2472882 -4.2275424 -4.2124376 -4.2039509 -4.1999879 -4.1941161 -4.1929049 -4.1969161 -4.1979985 -4.1991973 -4.1979837 -4.1898646 -4.1820345][-4.3049836 -4.2833114 -4.2636781 -4.249413 -4.2414532 -4.24111 -4.2423882 -4.2399068 -4.2407451 -4.24704 -4.2493358 -4.2473874 -4.243288 -4.2343388 -4.227457][-4.3167896 -4.2982454 -4.2813363 -4.2697997 -4.2639174 -4.2647109 -4.2668829 -4.2661924 -4.2665172 -4.2704682 -4.2716584 -4.2687488 -4.2646813 -4.2604852 -4.2600608]]...]
INFO - root - 2017-12-06 01:46:19.214045: step 64210, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 64h:12m:29s remains)
INFO - root - 2017-12-06 01:46:27.713499: step 64220, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 64h:03m:26s remains)
INFO - root - 2017-12-06 01:46:36.231304: step 64230, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 63h:54m:33s remains)
INFO - root - 2017-12-06 01:46:44.801634: step 64240, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 63h:08m:39s remains)
INFO - root - 2017-12-06 01:46:53.364458: step 64250, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 61h:54m:30s remains)
INFO - root - 2017-12-06 01:47:01.922697: step 64260, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 66h:13m:20s remains)
INFO - root - 2017-12-06 01:47:10.356847: step 64270, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.830 sec/batch; 61h:51m:32s remains)
INFO - root - 2017-12-06 01:47:18.652169: step 64280, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 0.790 sec/batch; 58h:51m:27s remains)
INFO - root - 2017-12-06 01:47:27.294774: step 64290, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 63h:45m:40s remains)
INFO - root - 2017-12-06 01:47:35.837216: step 64300, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 62h:11m:05s remains)
2017-12-06 01:47:36.582422: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.179152 -4.1769166 -4.1752338 -4.179213 -4.1926861 -4.20948 -4.2207775 -4.2244596 -4.2256131 -4.2293162 -4.2432046 -4.2565064 -4.2567635 -4.2574668 -4.271955][-4.1393824 -4.1344795 -4.1292105 -4.1304188 -4.145452 -4.1672606 -4.1831832 -4.1912079 -4.1972628 -4.2048078 -4.2223649 -4.2405977 -4.2443171 -4.2454891 -4.2600517][-4.1108942 -4.1090059 -4.1034679 -4.1065588 -4.1244626 -4.1467571 -4.1613479 -4.1683893 -4.1776857 -4.18913 -4.2097926 -4.2331753 -4.2411695 -4.2420421 -4.2552609][-4.0953445 -4.0987692 -4.0970407 -4.1033897 -4.1209111 -4.1353488 -4.1405606 -4.1453328 -4.1609378 -4.181942 -4.208806 -4.2353692 -4.2477484 -4.2483196 -4.2594767][-4.0538278 -4.0656815 -4.073144 -4.0864015 -4.1017537 -4.1057749 -4.0970922 -4.0964894 -4.1164827 -4.1515145 -4.1878114 -4.221139 -4.2399044 -4.2418704 -4.2543621][-3.9992027 -4.0172439 -4.0313978 -4.0429459 -4.0523677 -4.0450473 -4.0160174 -3.9985087 -4.0242434 -4.0776973 -4.12879 -4.171092 -4.2030816 -4.2165451 -4.2384291][-3.9799874 -3.9885778 -3.9947417 -3.9941318 -3.9930921 -3.971437 -3.9099972 -3.8600249 -3.8978631 -3.9824622 -4.0548038 -4.1069045 -4.15002 -4.1813083 -4.218749][-4.0008392 -3.9923885 -3.9884677 -3.978204 -3.9630718 -3.9198163 -3.8207784 -3.7287159 -3.779007 -3.8908548 -3.9778848 -4.0397668 -4.0942287 -4.1434793 -4.1967535][-4.0594382 -4.0420189 -4.0349584 -4.0280685 -4.0086455 -3.9582679 -3.8649139 -3.7786474 -3.8128245 -3.8983784 -3.9676669 -4.0199928 -4.0703793 -4.1203041 -4.1756144][-4.1320672 -4.1126413 -4.1039753 -4.101305 -4.0856986 -4.042727 -3.9804053 -3.9259393 -3.9371767 -3.9832747 -4.0263863 -4.0626345 -4.0978417 -4.1311607 -4.1764512][-4.1872926 -4.172781 -4.1644039 -4.1639891 -4.1553965 -4.1265745 -4.0880475 -4.0509138 -4.0439582 -4.0638404 -4.0881948 -4.11006 -4.1306944 -4.1501508 -4.185226][-4.2169485 -4.2104855 -4.20489 -4.2090311 -4.2081718 -4.1946073 -4.175128 -4.1504126 -4.1360278 -4.1412334 -4.1504765 -4.1579971 -4.1666794 -4.1760116 -4.2009168][-4.2310376 -4.2292938 -4.2266431 -4.2318172 -4.2346339 -4.2313414 -4.2245736 -4.2091 -4.1939311 -4.1920037 -4.194324 -4.1987357 -4.2067952 -4.2131052 -4.23116][-4.2610936 -4.2609506 -4.2600689 -4.2634358 -4.2656193 -4.2649107 -4.262619 -4.2535172 -4.2429547 -4.2411237 -4.24252 -4.2470412 -4.2558279 -4.262156 -4.2747636][-4.3058233 -4.3057632 -4.3054614 -4.3067284 -4.3078809 -4.3076534 -4.3059325 -4.2992215 -4.2914743 -4.2900662 -4.2922597 -4.2967043 -4.3042493 -4.3090577 -4.3159227]]...]
INFO - root - 2017-12-06 01:47:44.992333: step 64310, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 63h:53m:48s remains)
INFO - root - 2017-12-06 01:47:53.564992: step 64320, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.816 sec/batch; 60h:45m:51s remains)
INFO - root - 2017-12-06 01:48:02.052187: step 64330, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 63h:37m:35s remains)
INFO - root - 2017-12-06 01:48:10.602005: step 64340, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 64h:37m:51s remains)
INFO - root - 2017-12-06 01:48:19.159417: step 64350, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 62h:12m:15s remains)
INFO - root - 2017-12-06 01:48:27.660071: step 64360, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 62h:07m:53s remains)
INFO - root - 2017-12-06 01:48:36.164058: step 64370, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 62h:06m:13s remains)
INFO - root - 2017-12-06 01:48:44.677328: step 64380, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 64h:13m:16s remains)
INFO - root - 2017-12-06 01:48:53.192710: step 64390, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 65h:10m:16s remains)
INFO - root - 2017-12-06 01:49:01.673583: step 64400, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 63h:21m:20s remains)
2017-12-06 01:49:02.524743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2397556 -4.2378168 -4.2415848 -4.2462978 -4.2484608 -4.2484083 -4.2484164 -4.2510138 -4.2563233 -4.2627044 -4.2660661 -4.2642946 -4.2598658 -4.253922 -4.2469821][-4.2100534 -4.2070823 -4.2112741 -4.2187142 -4.2231655 -4.2245822 -4.22589 -4.2297115 -4.2375693 -4.24856 -4.2561193 -4.2563419 -4.2530003 -4.2463841 -4.2376089][-4.2055774 -4.1993818 -4.1992741 -4.2037821 -4.207345 -4.2069907 -4.2054319 -4.2075367 -4.2158027 -4.2308507 -4.243207 -4.2475739 -4.2483616 -4.2471538 -4.2410364][-4.2140503 -4.2017016 -4.192945 -4.1887069 -4.1854858 -4.1779475 -4.1698813 -4.1661949 -4.1727552 -4.1918077 -4.2086945 -4.2180834 -4.2254658 -4.2319646 -4.23119][-4.2211742 -4.2024856 -4.18385 -4.1665053 -4.1490397 -4.1288257 -4.1094437 -4.0943241 -4.0960188 -4.1188331 -4.1408281 -4.1572433 -4.1761775 -4.1938057 -4.1994557][-4.1940193 -4.1620469 -4.1265831 -4.0907407 -4.050468 -4.0084538 -3.9723928 -3.941505 -3.9390402 -3.9733074 -4.0078697 -4.0374656 -4.073884 -4.105598 -4.1223116][-4.1339211 -4.0850821 -4.0327148 -3.9830108 -3.9251153 -3.8606181 -3.8047309 -3.7539654 -3.7442787 -3.7909789 -3.8420651 -3.8893411 -3.9477267 -3.9987226 -4.0299091][-4.076798 -4.0256042 -3.9761925 -3.9380417 -3.8913424 -3.831105 -3.7790246 -3.7289109 -3.7149298 -3.7582757 -3.8110037 -3.862072 -3.9232287 -3.9774408 -4.0078545][-4.0592022 -4.0239925 -3.9975686 -3.9879179 -3.9706421 -3.9377964 -3.9109674 -3.8830743 -3.8736324 -3.9044752 -3.9433773 -3.9777272 -4.0152435 -4.0489659 -4.06397][-4.0640121 -4.0478911 -4.0425749 -4.0563278 -4.0621185 -4.0514855 -4.0441122 -4.0339022 -4.0287185 -4.0472841 -4.071301 -4.0887513 -4.1035104 -4.1181602 -4.1199017][-4.0913792 -4.0863476 -4.0918303 -4.11527 -4.1316352 -4.1328506 -4.1358109 -4.13594 -4.1349626 -4.1447659 -4.1581936 -4.166182 -4.168807 -4.1739745 -4.1699867][-4.1512518 -4.1495256 -4.1575737 -4.1800122 -4.1980486 -4.20508 -4.211699 -4.2158031 -4.2163224 -4.2203994 -4.2263908 -4.2289195 -4.2284222 -4.2305603 -4.2257066][-4.2247562 -4.22545 -4.233582 -4.2503595 -4.2635322 -4.2701015 -4.275475 -4.2788444 -4.2796555 -4.2809577 -4.2823191 -4.2822328 -4.2815447 -4.2833171 -4.2813492][-4.2868061 -4.2883673 -4.2941971 -4.304482 -4.3113961 -4.3138723 -4.3154387 -4.3160143 -4.3152804 -4.3148513 -4.3142939 -4.3140926 -4.3145723 -4.3176293 -4.3189564][-4.3281121 -4.3285193 -4.3314395 -4.3365641 -4.3389158 -4.33857 -4.3382177 -4.3378119 -4.337009 -4.3367457 -4.3365793 -4.3375974 -4.33967 -4.3433928 -4.3462253]]...]
INFO - root - 2017-12-06 01:49:11.015980: step 64410, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 62h:21m:16s remains)
INFO - root - 2017-12-06 01:49:19.395846: step 64420, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.850 sec/batch; 63h:16m:31s remains)
INFO - root - 2017-12-06 01:49:27.928952: step 64430, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 62h:57m:50s remains)
INFO - root - 2017-12-06 01:49:36.452699: step 64440, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 64h:42m:55s remains)
INFO - root - 2017-12-06 01:49:44.971611: step 64450, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.802 sec/batch; 59h:44m:58s remains)
INFO - root - 2017-12-06 01:49:53.531930: step 64460, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 63h:18m:07s remains)
INFO - root - 2017-12-06 01:50:02.058143: step 64470, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 63h:11m:51s remains)
INFO - root - 2017-12-06 01:50:10.498249: step 64480, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 62h:14m:05s remains)
INFO - root - 2017-12-06 01:50:19.021786: step 64490, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 62h:56m:59s remains)
INFO - root - 2017-12-06 01:50:27.490360: step 64500, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 64h:23m:06s remains)
2017-12-06 01:50:28.322649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2524281 -4.2476521 -4.2429705 -4.235436 -4.216084 -4.1971045 -4.1949215 -4.2121043 -4.2339373 -4.2426558 -4.2357616 -4.2108445 -4.1723218 -4.1431127 -4.1350279][-4.269105 -4.2586102 -4.2496519 -4.2433062 -4.229959 -4.219954 -4.2217655 -4.232655 -4.243444 -4.241015 -4.2269826 -4.2000122 -4.1592207 -4.1328516 -4.1303511][-4.27302 -4.2609944 -4.2516518 -4.2470813 -4.2382131 -4.2320604 -4.2327433 -4.2364459 -4.2424793 -4.2378116 -4.2250996 -4.2013879 -4.1604872 -4.1344252 -4.1319175][-4.2699823 -4.2589669 -4.2485914 -4.2434092 -4.2346864 -4.2279539 -4.2243514 -4.22094 -4.2260265 -4.2234511 -4.2142391 -4.1980925 -4.163198 -4.1367445 -4.1274848][-4.2684588 -4.2598476 -4.2487316 -4.239696 -4.2256994 -4.2118993 -4.1975536 -4.1822639 -4.1831656 -4.1843572 -4.1829271 -4.1761422 -4.1501851 -4.1252961 -4.1117959][-4.2653933 -4.2597923 -4.2485318 -4.2316518 -4.2060966 -4.1788688 -4.14603 -4.1086397 -4.0979252 -4.104053 -4.1168165 -4.1257529 -4.1158457 -4.0998416 -4.0899992][-4.2516518 -4.2468204 -4.2333217 -4.2063537 -4.1699591 -4.1300297 -4.077312 -4.0150442 -3.9896166 -4.0038157 -4.0382795 -4.0730681 -4.0889854 -4.0916719 -4.0937443][-4.234879 -4.2265267 -4.2085853 -4.1770363 -4.1385932 -4.0940905 -4.0311217 -3.954298 -3.9178138 -3.9391572 -3.9940093 -4.0527759 -4.0932937 -4.1144905 -4.1297765][-4.223352 -4.2093034 -4.1881156 -4.1587572 -4.1265068 -4.0898967 -4.0384526 -3.9774408 -3.949111 -3.9705241 -4.0236597 -4.0853276 -4.1356745 -4.1673422 -4.1900578][-4.2145987 -4.1959128 -4.1741018 -4.1511111 -4.1290584 -4.1076565 -4.0823903 -4.0549922 -4.0482564 -4.0687094 -4.1082611 -4.1564989 -4.199894 -4.2288575 -4.2515182][-4.2148519 -4.1984668 -4.1801863 -4.1651044 -4.155189 -4.1498203 -4.1459951 -4.143589 -4.1519256 -4.1712332 -4.1966834 -4.225646 -4.2537265 -4.2735186 -4.2892551][-4.2277403 -4.2175841 -4.20563 -4.1985688 -4.1986051 -4.203897 -4.2116156 -4.2197089 -4.2316489 -4.2460065 -4.2591624 -4.272346 -4.2836752 -4.291913 -4.299057][-4.2516656 -4.2487035 -4.2435875 -4.2404113 -4.2432189 -4.2518554 -4.2624097 -4.2717915 -4.2815061 -4.2899027 -4.2946157 -4.2980027 -4.2999768 -4.3023505 -4.3056197][-4.2698011 -4.2727914 -4.2732306 -4.2725482 -4.2742648 -4.2805738 -4.2889857 -4.2963223 -4.3033838 -4.3085966 -4.3099246 -4.3101158 -4.30994 -4.3114314 -4.3148117][-4.28227 -4.2863679 -4.2887464 -4.2888374 -4.2893276 -4.2926087 -4.298038 -4.303606 -4.3099694 -4.3151956 -4.3175392 -4.3185534 -4.31886 -4.32058 -4.3242593]]...]
INFO - root - 2017-12-06 01:50:36.787540: step 64510, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 63h:00m:20s remains)
INFO - root - 2017-12-06 01:50:45.149791: step 64520, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 0.807 sec/batch; 60h:06m:03s remains)
INFO - root - 2017-12-06 01:50:53.662404: step 64530, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 63h:51m:57s remains)
INFO - root - 2017-12-06 01:51:02.180741: step 64540, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 63h:22m:35s remains)
INFO - root - 2017-12-06 01:51:10.753275: step 64550, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 64h:15m:17s remains)
INFO - root - 2017-12-06 01:51:19.198479: step 64560, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.832 sec/batch; 61h:54m:09s remains)
INFO - root - 2017-12-06 01:51:27.668919: step 64570, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.834 sec/batch; 62h:06m:04s remains)
INFO - root - 2017-12-06 01:51:36.108147: step 64580, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 63h:25m:26s remains)
INFO - root - 2017-12-06 01:51:44.508118: step 64590, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 62h:03m:05s remains)
INFO - root - 2017-12-06 01:51:53.017806: step 64600, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 64h:24m:08s remains)
2017-12-06 01:51:53.783154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24938 -4.2032619 -4.1588383 -4.1259766 -4.1054378 -4.07798 -4.02853 -3.9942868 -4.0312729 -4.0955205 -4.1346793 -4.1515412 -4.1598821 -4.1675673 -4.1735153][-4.2532892 -4.2114091 -4.168911 -4.1404586 -4.1239014 -4.095551 -4.0422473 -4.0129089 -4.0520916 -4.1123624 -4.1447582 -4.1571474 -4.165781 -4.1674228 -4.1645527][-4.2575 -4.2205658 -4.174726 -4.1412845 -4.1255908 -4.0959029 -4.0404353 -4.0195327 -4.0641732 -4.1236353 -4.1482534 -4.1595364 -4.1704626 -4.1701751 -4.1613503][-4.2631083 -4.2252097 -4.1739922 -4.1355 -4.115324 -4.0795193 -4.0126648 -3.9907577 -4.0418448 -4.10369 -4.12718 -4.1420593 -4.1530094 -4.1498318 -4.1344638][-4.2671776 -4.2255058 -4.1661468 -4.1195807 -4.0869951 -4.0335126 -3.9446647 -3.9131896 -3.979475 -4.0546255 -4.085187 -4.1045108 -4.116333 -4.1106625 -4.0896511][-4.2674093 -4.2209496 -4.1546721 -4.0945115 -4.0435815 -3.9605505 -3.8243146 -3.7670848 -3.8640471 -3.9656978 -4.016386 -4.049592 -4.0653553 -4.062355 -4.0382628][-4.2655835 -4.2132797 -4.1410356 -4.063674 -3.980082 -3.8523865 -3.6483095 -3.5575507 -3.7194872 -3.884928 -3.9729834 -4.024477 -4.03906 -4.0364428 -4.0121317][-4.2631936 -4.2067914 -4.1263041 -4.0372982 -3.9414818 -3.80088 -3.5921249 -3.5174007 -3.7269306 -3.9180775 -4.0100894 -4.053647 -4.0568876 -4.0465975 -4.0170722][-4.2633371 -4.2087531 -4.1322012 -4.0546756 -3.9812303 -3.8916607 -3.7734389 -3.7526124 -3.9017229 -4.0385256 -4.0921574 -4.1014676 -4.0846953 -4.0676417 -4.0422339][-4.2713103 -4.2229733 -4.1623425 -4.1057839 -4.0596166 -4.0114913 -3.9567983 -3.9580662 -4.051168 -4.1399336 -4.1636982 -4.1422415 -4.1084709 -4.0932808 -4.08363][-4.2782326 -4.234777 -4.1886158 -4.1481533 -4.1214681 -4.095263 -4.0662394 -4.0610614 -4.11289 -4.1716952 -4.1840014 -4.1499262 -4.1138916 -4.1073728 -4.11193][-4.277267 -4.2348623 -4.1945429 -4.156384 -4.1342697 -4.1131191 -4.0816131 -4.0619144 -4.0938725 -4.1393647 -4.1495609 -4.1222272 -4.1063781 -4.1185269 -4.138536][-4.2715878 -4.2295728 -4.1882668 -4.1488471 -4.1233492 -4.0971909 -4.05739 -4.0307903 -4.0638905 -4.1156936 -4.1321082 -4.1207852 -4.1274219 -4.1530948 -4.1753812][-4.2716317 -4.23035 -4.1847906 -4.1445203 -4.1263189 -4.1066546 -4.0668354 -4.0383186 -4.0737729 -4.1338997 -4.1594172 -4.1607533 -4.1772432 -4.2008748 -4.2157173][-4.2766771 -4.2384443 -4.1990256 -4.1697106 -4.1600628 -4.1443586 -4.1136236 -4.0960603 -4.1306534 -4.1849637 -4.2112083 -4.2214627 -4.2386007 -4.2535849 -4.2577233]]...]
INFO - root - 2017-12-06 01:52:02.252455: step 64610, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 63h:41m:40s remains)
INFO - root - 2017-12-06 01:52:10.788870: step 64620, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 62h:49m:25s remains)
INFO - root - 2017-12-06 01:52:19.357114: step 64630, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 65h:23m:10s remains)
INFO - root - 2017-12-06 01:52:27.722673: step 64640, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 63h:35m:42s remains)
INFO - root - 2017-12-06 01:52:36.337653: step 64650, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 65h:15m:54s remains)
INFO - root - 2017-12-06 01:52:44.889260: step 64660, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 63h:32m:47s remains)
INFO - root - 2017-12-06 01:52:53.393416: step 64670, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 64h:13m:48s remains)
INFO - root - 2017-12-06 01:53:01.651937: step 64680, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 61h:04m:20s remains)
INFO - root - 2017-12-06 01:53:10.037792: step 64690, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 63h:32m:10s remains)
INFO - root - 2017-12-06 01:53:18.464001: step 64700, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 62h:53m:31s remains)
2017-12-06 01:53:19.184248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2047396 -4.1799893 -4.1740961 -4.1842566 -4.2081013 -4.2212839 -4.2095661 -4.1974764 -4.1857347 -4.1707115 -4.166934 -4.1815248 -4.2112374 -4.2230268 -4.1996284][-4.1825619 -4.1469846 -4.127419 -4.1314092 -4.1576262 -4.1823258 -4.1952205 -4.2081418 -4.2069025 -4.1936255 -4.1912851 -4.2106152 -4.2459097 -4.2548008 -4.229126][-4.1797161 -4.1419497 -4.1121168 -4.0954347 -4.1064625 -4.1378908 -4.1693511 -4.2031236 -4.2174125 -4.2149549 -4.2210865 -4.2431993 -4.2707062 -4.2696452 -4.2441435][-4.1796474 -4.1534238 -4.12459 -4.0920672 -4.0802326 -4.1032529 -4.1424408 -4.1915751 -4.2211642 -4.2298141 -4.2385569 -4.2590384 -4.270144 -4.25366 -4.2303162][-4.1712179 -4.1608248 -4.1424565 -4.1149178 -4.0915756 -4.0916905 -4.1179509 -4.1651497 -4.2047434 -4.2232604 -4.2334461 -4.2459664 -4.2436247 -4.2202406 -4.2082691][-4.1802945 -4.1797514 -4.1688766 -4.141161 -4.1087461 -4.082531 -4.0780325 -4.0989552 -4.1390576 -4.1771908 -4.2011876 -4.2088618 -4.1987648 -4.1781726 -4.188045][-4.1612611 -4.1690722 -4.1582012 -4.1271191 -4.0834103 -4.027267 -3.9766426 -3.9540112 -3.9971764 -4.0767384 -4.1309347 -4.14151 -4.1194682 -4.1042857 -4.1355677][-4.0929022 -4.1070595 -4.0948439 -4.0623031 -4.0137825 -3.9368496 -3.8383069 -3.7621343 -3.8118715 -3.9431763 -4.0286016 -4.0433388 -4.0185928 -4.0131254 -4.0645285][-4.0554209 -4.073245 -4.0589266 -4.0291553 -3.9859538 -3.9089522 -3.8039577 -3.72007 -3.7675495 -3.9055715 -3.9970551 -4.0130243 -3.9985983 -4.0010843 -4.05432][-4.0940819 -4.1073027 -4.0946131 -4.0753288 -4.0471478 -3.9964552 -3.9322345 -3.8856778 -3.9152114 -4.0030475 -4.0675206 -4.0845232 -4.0837469 -4.0866389 -4.121088][-4.1357222 -4.1436338 -4.1405287 -4.1347237 -4.1227188 -4.099896 -4.0694323 -4.045897 -4.0560684 -4.1005993 -4.1404448 -4.1567669 -4.161736 -4.1620183 -4.1749125][-4.1706042 -4.1812658 -4.1841078 -4.18725 -4.186667 -4.1772323 -4.16554 -4.1489887 -4.145288 -4.1652656 -4.1862803 -4.196331 -4.1958694 -4.1872458 -4.1862812][-4.1969647 -4.2131076 -4.2212367 -4.228404 -4.2328973 -4.2304211 -4.2301083 -4.2187395 -4.2075586 -4.2125807 -4.2171497 -4.2123055 -4.1978292 -4.1776066 -4.174439][-4.2148743 -4.2327328 -4.2437325 -4.2534208 -4.2571831 -4.2549033 -4.2553434 -4.249423 -4.2372446 -4.2295156 -4.2191534 -4.2023606 -4.1777272 -4.156178 -4.1548715][-4.2140431 -4.2257667 -4.2353077 -4.2446365 -4.2418833 -4.23368 -4.2320867 -4.2321186 -4.2250991 -4.216258 -4.2005367 -4.1742096 -4.1431313 -4.1242743 -4.1288595]]...]
INFO - root - 2017-12-06 01:53:27.681316: step 64710, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 65h:05m:27s remains)
INFO - root - 2017-12-06 01:53:36.216969: step 64720, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 63h:07m:13s remains)
INFO - root - 2017-12-06 01:53:44.845732: step 64730, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 64h:58m:05s remains)
INFO - root - 2017-12-06 01:53:53.319389: step 64740, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 62h:30m:01s remains)
INFO - root - 2017-12-06 01:54:01.832850: step 64750, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.875 sec/batch; 65h:06m:16s remains)
INFO - root - 2017-12-06 01:54:10.395468: step 64760, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 64h:54m:05s remains)
INFO - root - 2017-12-06 01:54:18.955707: step 64770, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 61h:49m:04s remains)
INFO - root - 2017-12-06 01:54:27.353943: step 64780, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 62h:49m:27s remains)
INFO - root - 2017-12-06 01:54:35.823150: step 64790, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 61h:57m:05s remains)
INFO - root - 2017-12-06 01:54:44.291756: step 64800, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 64h:05m:34s remains)
2017-12-06 01:54:45.097660: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2441659 -4.2479992 -4.2562947 -4.2637649 -4.2657175 -4.2596884 -4.2515826 -4.249095 -4.2420297 -4.2187347 -4.1860881 -4.1480694 -4.1186829 -4.1329 -4.1933432][-4.22473 -4.2187314 -4.2236109 -4.2328815 -4.2365174 -4.23139 -4.2282639 -4.233048 -4.2352276 -4.2190561 -4.190659 -4.1566076 -4.1300092 -4.1400461 -4.19492][-4.2063613 -4.1943288 -4.1912847 -4.1972346 -4.20101 -4.1945729 -4.1925979 -4.2030883 -4.2136607 -4.2020812 -4.1775365 -4.1474123 -4.1276097 -4.1420484 -4.1935482][-4.1952724 -4.1840296 -4.1761 -4.1745472 -4.1725006 -4.1624556 -4.15808 -4.1719446 -4.1896925 -4.1817431 -4.1574192 -4.1303978 -4.1181259 -4.1382971 -4.188406][-4.18848 -4.1791143 -4.1689839 -4.1597853 -4.1480937 -4.1274118 -4.1154242 -4.1317687 -4.1594629 -4.1614933 -4.1415148 -4.1170039 -4.1107106 -4.1352415 -4.1857677][-4.1787291 -4.1710014 -4.1618881 -4.1483068 -4.1255517 -4.0905337 -4.064117 -4.077898 -4.1167479 -4.13824 -4.1307063 -4.1131005 -4.1133952 -4.141212 -4.1933246][-4.1652665 -4.1652184 -4.158143 -4.1442356 -4.1168447 -4.071723 -4.0296426 -4.0323529 -4.0720739 -4.1051164 -4.10918 -4.0968056 -4.1028547 -4.138968 -4.1955762][-4.1649189 -4.1790948 -4.1770387 -4.1637936 -4.1369257 -4.0929365 -4.0462837 -4.0329161 -4.058157 -4.0893378 -4.0959969 -4.0843782 -4.0924253 -4.13469 -4.1960487][-4.1890454 -4.216167 -4.2216597 -4.2119718 -4.1888213 -4.1543984 -4.1135607 -4.0866389 -4.0950856 -4.1169577 -4.1183209 -4.1039257 -4.1082535 -4.1471047 -4.2052565][-4.2282925 -4.2581482 -4.2698569 -4.2633829 -4.245337 -4.220685 -4.1888909 -4.160593 -4.157886 -4.1662135 -4.1608367 -4.1475911 -4.1504846 -4.1819167 -4.2313356][-4.269165 -4.2938085 -4.30564 -4.3012438 -4.288754 -4.2717009 -4.2475581 -4.2245278 -4.2172871 -4.2143412 -4.202661 -4.193665 -4.1988478 -4.2245655 -4.2650776][-4.2986774 -4.3174763 -4.3260813 -4.323802 -4.3145437 -4.2995648 -4.2790489 -4.2625256 -4.2560139 -4.2470269 -4.2356853 -4.2322407 -4.2416945 -4.2654204 -4.299902][-4.3118944 -4.3261275 -4.3303232 -4.3286023 -4.3228893 -4.3077035 -4.2891254 -4.2786708 -4.2724986 -4.2629728 -4.2568035 -4.2580433 -4.2704449 -4.2932496 -4.3227673][-4.3063941 -4.3188839 -4.3205647 -4.3209453 -4.3201013 -4.3071933 -4.2915282 -4.2848606 -4.2811193 -4.27534 -4.2727308 -4.2756181 -4.2845516 -4.3016286 -4.3270173][-4.298027 -4.3066664 -4.3059669 -4.3062906 -4.3084559 -4.3010216 -4.2903872 -4.2847023 -4.2836633 -4.2819743 -4.2819943 -4.2838435 -4.2883449 -4.2999239 -4.3199649]]...]
INFO - root - 2017-12-06 01:54:53.588209: step 64810, loss = 2.11, batch loss = 2.05 (9.4 examples/sec; 0.849 sec/batch; 63h:06m:35s remains)
INFO - root - 2017-12-06 01:55:01.997968: step 64820, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 65h:36m:12s remains)
INFO - root - 2017-12-06 01:55:10.440202: step 64830, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 64h:01m:37s remains)
INFO - root - 2017-12-06 01:55:18.956456: step 64840, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 62h:03m:11s remains)
INFO - root - 2017-12-06 01:55:27.485696: step 64850, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.828 sec/batch; 61h:34m:24s remains)
INFO - root - 2017-12-06 01:55:35.882620: step 64860, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 62h:53m:02s remains)
INFO - root - 2017-12-06 01:55:44.327460: step 64870, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 61h:50m:47s remains)
INFO - root - 2017-12-06 01:55:52.724167: step 64880, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 62h:23m:07s remains)
INFO - root - 2017-12-06 01:56:01.301637: step 64890, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 64h:57m:04s remains)
INFO - root - 2017-12-06 01:56:09.821988: step 64900, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 63h:12m:28s remains)
2017-12-06 01:56:10.616809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2889423 -4.2853718 -4.2796788 -4.2751493 -4.2703185 -4.2659616 -4.2659378 -4.2704463 -4.2791104 -4.2895994 -4.2980657 -4.3048639 -4.3100185 -4.3119149 -4.3091841][-4.2857337 -4.2806869 -4.2740073 -4.2669997 -4.2570772 -4.2480321 -4.2472067 -4.2557578 -4.2709665 -4.2890186 -4.3046861 -4.3155813 -4.3204942 -4.3196011 -4.3141489][-4.2894063 -4.2809477 -4.2719722 -4.2599754 -4.2425437 -4.2234287 -4.2159805 -4.2260914 -4.2469125 -4.2743454 -4.3019681 -4.321137 -4.3280816 -4.326735 -4.3210244][-4.2879128 -4.2771029 -4.2640014 -4.2433743 -4.2126293 -4.1766958 -4.1548386 -4.163074 -4.1940994 -4.2377243 -4.2855325 -4.3183746 -4.3303075 -4.3301611 -4.3250632][-4.2826543 -4.2696934 -4.2491393 -4.2180462 -4.1734328 -4.1164541 -4.0696468 -4.0697522 -4.1123033 -4.1812634 -4.2546959 -4.306366 -4.3279605 -4.33111 -4.3276858][-4.2776322 -4.2633572 -4.2372351 -4.1974273 -4.1369872 -4.0563755 -3.9771957 -3.9649253 -4.0224252 -4.1166463 -4.2131329 -4.2848105 -4.3196468 -4.3285108 -4.3272533][-4.2726274 -4.2567811 -4.2249675 -4.1735282 -4.0941119 -3.9851274 -3.8705118 -3.84727 -3.9270945 -4.0481925 -4.1630759 -4.2514462 -4.3012123 -4.318202 -4.3218184][-4.2598567 -4.24028 -4.2038445 -4.1443839 -4.050302 -3.9138355 -3.7593288 -3.7257736 -3.8366709 -3.9891872 -4.1187606 -4.2166629 -4.2776303 -4.3020287 -4.311305][-4.2474647 -4.22542 -4.1901217 -4.1332612 -4.0427341 -3.9084933 -3.7489934 -3.7090676 -3.8241343 -3.98019 -4.1070127 -4.1995392 -4.2607141 -4.2868023 -4.2981009][-4.2467995 -4.2260633 -4.1966329 -4.1522589 -4.08321 -3.9843998 -3.874598 -3.8413606 -3.9137726 -4.0280151 -4.1283078 -4.2045212 -4.2566605 -4.278439 -4.2872663][-4.2503591 -4.2316952 -4.2074156 -4.1760721 -4.1301241 -4.0683393 -4.0105734 -3.9924805 -4.0237279 -4.0864635 -4.1545649 -4.2158403 -4.2592826 -4.2747765 -4.2792554][-4.2515321 -4.2342019 -4.2146883 -4.1937494 -4.1628008 -4.1251421 -4.1012745 -4.0985966 -4.1078792 -4.1354847 -4.1791177 -4.2287889 -4.2653394 -4.2745876 -4.2721205][-4.255877 -4.2368727 -4.2179842 -4.2016492 -4.18161 -4.1625361 -4.1567349 -4.1600952 -4.1602135 -4.1715231 -4.2040062 -4.2443876 -4.2732382 -4.277195 -4.266964][-4.26381 -4.2434015 -4.2222667 -4.2067218 -4.1934857 -4.1845098 -4.1858773 -4.1878052 -4.1818733 -4.1865072 -4.2146859 -4.25178 -4.2784281 -4.2828417 -4.2708788][-4.2765479 -4.2619095 -4.2427859 -4.2244282 -4.2105947 -4.2017722 -4.20409 -4.2046108 -4.1961164 -4.2009149 -4.2279658 -4.26088 -4.2838697 -4.289752 -4.2802067]]...]
INFO - root - 2017-12-06 01:56:19.190114: step 64910, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 64h:24m:44s remains)
INFO - root - 2017-12-06 01:56:27.500135: step 64920, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.742 sec/batch; 55h:09m:20s remains)
INFO - root - 2017-12-06 01:56:36.135967: step 64930, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 63h:12m:34s remains)
INFO - root - 2017-12-06 01:56:44.684070: step 64940, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 61h:56m:57s remains)
INFO - root - 2017-12-06 01:56:53.235220: step 64950, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 65h:54m:31s remains)
INFO - root - 2017-12-06 01:57:01.878089: step 64960, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 64h:40m:19s remains)
INFO - root - 2017-12-06 01:57:10.314682: step 64970, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 62h:30m:03s remains)
INFO - root - 2017-12-06 01:57:18.789580: step 64980, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 62h:43m:42s remains)
INFO - root - 2017-12-06 01:57:27.381765: step 64990, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 64h:16m:00s remains)
INFO - root - 2017-12-06 01:57:35.849560: step 65000, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 62h:51m:27s remains)
2017-12-06 01:57:36.686991: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1252623 -4.1100254 -4.1018105 -4.095562 -4.1008348 -4.1123877 -4.1140556 -4.1181083 -4.1277142 -4.12467 -4.1266236 -4.1344047 -4.1290207 -4.111702 -4.1041603][-4.1388454 -4.1225238 -4.1029811 -4.0893135 -4.092845 -4.1116109 -4.1270008 -4.1405311 -4.1506119 -4.1499453 -4.1535826 -4.160449 -4.1529813 -4.1367078 -4.1329312][-4.1806026 -4.1782889 -4.1629782 -4.1502709 -4.1495314 -4.1644273 -4.182868 -4.1992126 -4.2069573 -4.2020173 -4.198864 -4.1941962 -4.1811953 -4.1638231 -4.1598468][-4.2165437 -4.2286177 -4.2292747 -4.224297 -4.2211723 -4.2239828 -4.2338691 -4.2453623 -4.2526612 -4.24871 -4.239677 -4.223876 -4.2060957 -4.1837325 -4.1689134][-4.2338948 -4.2568612 -4.2732515 -4.2745919 -4.2644687 -4.2546577 -4.2494707 -4.2504 -4.2539887 -4.2551923 -4.2499127 -4.2356286 -4.22043 -4.2015753 -4.17907][-4.2278466 -4.2549176 -4.2742982 -4.2718425 -4.2519603 -4.228404 -4.2079349 -4.1998563 -4.2031455 -4.2121835 -4.2186065 -4.2178307 -4.2142859 -4.2057023 -4.1873536][-4.2033892 -4.2238789 -4.2341061 -4.220088 -4.1844826 -4.1416655 -4.10537 -4.0906987 -4.0986638 -4.1237969 -4.1554475 -4.1834636 -4.2020407 -4.2075291 -4.2012343][-4.1669106 -4.1795759 -4.1769485 -4.1485376 -4.0992465 -4.0409384 -3.9887352 -3.9627328 -3.9681385 -4.005754 -4.0632639 -4.126646 -4.1760321 -4.2020197 -4.2105002][-4.1345434 -4.138113 -4.1260543 -4.0940728 -4.0525823 -4.0019922 -3.9538827 -3.9262593 -3.9201117 -3.9451706 -4.0040541 -4.0841117 -4.1523418 -4.1898632 -4.2050304][-4.1221733 -4.1164441 -4.1038523 -4.0882034 -4.0742826 -4.0451283 -4.0154819 -3.9998691 -3.9845271 -3.9767156 -4.0013871 -4.0652232 -4.1298146 -4.1660857 -4.1800532][-4.1238761 -4.1224089 -4.1204782 -4.1244187 -4.12925 -4.1136861 -4.1003885 -4.1015029 -4.0926423 -4.0657878 -4.052671 -4.0784407 -4.120698 -4.145041 -4.1496029][-4.1636052 -4.1694889 -4.1756706 -4.1855631 -4.1890063 -4.1749892 -4.1683888 -4.1790562 -4.1809497 -4.1561289 -4.1262293 -4.126317 -4.1478109 -4.156558 -4.1514878][-4.2129917 -4.2231364 -4.2310014 -4.2351251 -4.2316651 -4.2202992 -4.2178564 -4.2284861 -4.2322426 -4.2177749 -4.1918931 -4.1810684 -4.1871767 -4.1883564 -4.1837635][-4.2550488 -4.2600365 -4.2615452 -4.2590685 -4.2537036 -4.2484407 -4.247838 -4.252553 -4.2527633 -4.243968 -4.2286148 -4.2191687 -4.2181435 -4.2198052 -4.224309][-4.2801991 -4.28171 -4.2795448 -4.2751431 -4.2719755 -4.273355 -4.2725511 -4.2734995 -4.2723742 -4.2659383 -4.2559881 -4.247066 -4.2434158 -4.2446585 -4.2509742]]...]
INFO - root - 2017-12-06 01:57:45.144576: step 65010, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.806 sec/batch; 59h:52m:28s remains)
INFO - root - 2017-12-06 01:57:53.704873: step 65020, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 66h:54m:15s remains)
INFO - root - 2017-12-06 01:58:02.226419: step 65030, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 66h:07m:21s remains)
INFO - root - 2017-12-06 01:58:10.818080: step 65040, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 65h:10m:29s remains)
INFO - root - 2017-12-06 01:58:19.421058: step 65050, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 63h:58m:57s remains)
INFO - root - 2017-12-06 01:58:28.042918: step 65060, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 64h:52m:59s remains)
INFO - root - 2017-12-06 01:58:36.618244: step 65070, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 63h:30m:33s remains)
INFO - root - 2017-12-06 01:58:45.095476: step 65080, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 65h:50m:53s remains)
INFO - root - 2017-12-06 01:58:53.652927: step 65090, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 61h:08m:33s remains)
INFO - root - 2017-12-06 01:59:02.203877: step 65100, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 65h:47m:49s remains)
2017-12-06 01:59:02.902706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2601 -4.2377386 -4.219069 -4.20279 -4.1932817 -4.2029352 -4.2154684 -4.220377 -4.2198267 -4.1898575 -4.1366663 -4.1121087 -4.146596 -4.1951218 -4.2250905][-4.2486744 -4.22307 -4.2029052 -4.1867709 -4.1756659 -4.18219 -4.19331 -4.19736 -4.1981554 -4.1664524 -4.1079607 -4.082139 -4.1213388 -4.1702337 -4.1959033][-4.2422657 -4.2176023 -4.2007074 -4.1879959 -4.1783977 -4.1788893 -4.1836586 -4.187757 -4.1931691 -4.1649609 -4.1108027 -4.0891519 -4.1217971 -4.1582017 -4.17605][-4.2390456 -4.2168612 -4.2022123 -4.1888213 -4.1768303 -4.1700292 -4.1719093 -4.1828818 -4.1962194 -4.1725774 -4.1251969 -4.107687 -4.1327996 -4.1580029 -4.1713409][-4.2427354 -4.2231221 -4.2066417 -4.1870441 -4.1663408 -4.1527085 -4.1515512 -4.1663251 -4.1834617 -4.16414 -4.1255908 -4.1081424 -4.1316066 -4.15707 -4.1705055][-4.2461038 -4.2260413 -4.2040572 -4.1749253 -4.1443262 -4.1265087 -4.1200552 -4.1307378 -4.147913 -4.1279116 -4.0984635 -4.0891848 -4.1185455 -4.1567173 -4.1761184][-4.2433052 -4.2219448 -4.1946874 -4.1549306 -4.1196122 -4.1003456 -4.0884948 -4.0953856 -4.1130419 -4.0980926 -4.0810289 -4.0826054 -4.1169038 -4.1644711 -4.18821][-4.2420607 -4.2169852 -4.1840863 -4.135375 -4.0978484 -4.0801907 -4.0644169 -4.0707979 -4.0900407 -4.0829391 -4.0830989 -4.0956564 -4.1304293 -4.177794 -4.2021813][-4.2438245 -4.2132592 -4.1744952 -4.1209488 -4.0814357 -4.0656753 -4.0518279 -4.0614338 -4.0794163 -4.0775971 -4.0908318 -4.1093044 -4.1407881 -4.1829481 -4.2085028][-4.2472115 -4.2097487 -4.1620069 -4.1050749 -4.0655208 -4.0571709 -4.0540838 -4.0650024 -4.0757475 -4.0782671 -4.0967011 -4.1161146 -4.14693 -4.1890893 -4.2164574][-4.2575455 -4.2174697 -4.1673183 -4.1119933 -4.081811 -4.0838928 -4.0895896 -4.098845 -4.1090579 -4.1142645 -4.1254454 -4.1353397 -4.1562853 -4.195302 -4.227561][-4.2687469 -4.2286849 -4.1782284 -4.1284914 -4.1051106 -4.1142 -4.1255836 -4.1376991 -4.1530471 -4.1565752 -4.1556282 -4.1539412 -4.1654286 -4.2018638 -4.2380166][-4.2686963 -4.2260571 -4.1721678 -4.1219616 -4.10026 -4.11245 -4.128623 -4.1449394 -4.1615906 -4.1618118 -4.152389 -4.1488366 -4.1590071 -4.1983333 -4.2384529][-4.2570691 -4.2087846 -4.1519065 -4.1008086 -4.0796685 -4.0952582 -4.1168818 -4.1345305 -4.1473579 -4.144362 -4.1338954 -4.1290488 -4.1410141 -4.1841269 -4.2285433][-4.2570696 -4.2095814 -4.1573482 -4.11169 -4.0916119 -4.1039591 -4.1233511 -4.1372747 -4.1470437 -4.1442175 -4.1359425 -4.1327996 -4.1498365 -4.1940956 -4.236115]]...]
INFO - root - 2017-12-06 01:59:11.341250: step 65110, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 64h:45m:10s remains)
INFO - root - 2017-12-06 01:59:19.836368: step 65120, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 63h:04m:38s remains)
INFO - root - 2017-12-06 01:59:28.409823: step 65130, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 62h:02m:16s remains)
INFO - root - 2017-12-06 01:59:36.967107: step 65140, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 62h:58m:57s remains)
INFO - root - 2017-12-06 01:59:45.573976: step 65150, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 65h:24m:51s remains)
INFO - root - 2017-12-06 01:59:54.120966: step 65160, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 65h:52m:26s remains)
INFO - root - 2017-12-06 02:00:02.817148: step 65170, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 65h:19m:21s remains)
INFO - root - 2017-12-06 02:00:11.264455: step 65180, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 65h:02m:10s remains)
INFO - root - 2017-12-06 02:00:19.773255: step 65190, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 62h:06m:31s remains)
INFO - root - 2017-12-06 02:00:28.276354: step 65200, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 64h:12m:56s remains)
2017-12-06 02:00:29.071322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3009677 -4.2934365 -4.2888703 -4.2874217 -4.2856693 -4.2805161 -4.2751369 -4.2688923 -4.2624254 -4.2627721 -4.273026 -4.288887 -4.3030725 -4.312273 -4.3197536][-4.2889686 -4.278903 -4.2737565 -4.2724547 -4.2671094 -4.2554865 -4.2426839 -4.2278781 -4.2134538 -4.2089162 -4.2199693 -4.2453961 -4.2698407 -4.289454 -4.3070927][-4.2746906 -4.2621446 -4.2540379 -4.2486095 -4.2362432 -4.2153454 -4.1918349 -4.1648145 -4.1400514 -4.1274104 -4.1399708 -4.1761336 -4.2132659 -4.2475419 -4.2806215][-4.2642579 -4.2460847 -4.2316036 -4.2211823 -4.2031674 -4.1742139 -4.1394086 -4.0986266 -4.06089 -4.0399756 -4.0565486 -4.1025562 -4.1492505 -4.1983132 -4.247745][-4.2551227 -4.2312722 -4.2102108 -4.1944327 -4.168788 -4.1318164 -4.0839663 -4.0243196 -3.9717185 -3.947459 -3.9731998 -4.0305147 -4.0888476 -4.1537819 -4.2171164][-4.2479973 -4.2224874 -4.196857 -4.1735964 -4.1378756 -4.0900173 -4.0271387 -3.9410081 -3.8699486 -3.8493133 -3.893904 -3.966053 -4.0368772 -4.1171103 -4.1951394][-4.2513466 -4.2258768 -4.1936903 -4.1591811 -4.112359 -4.0589366 -3.9882524 -3.879745 -3.7946692 -3.7900519 -3.859036 -3.9436612 -4.020874 -4.1084766 -4.1927156][-4.2602692 -4.229516 -4.1890178 -4.1422281 -4.0891933 -4.0416765 -3.98272 -3.884733 -3.8157563 -3.8372185 -3.9139493 -3.991622 -4.0567317 -4.1310225 -4.2065444][-4.2667994 -4.2280107 -4.18068 -4.1294107 -4.0833726 -4.0520124 -4.021678 -3.9585888 -3.9270806 -3.9620905 -4.0181532 -4.0683126 -4.1086059 -4.1590662 -4.2217388][-4.268795 -4.2249746 -4.1781449 -4.1322913 -4.1010752 -4.0885706 -4.0836954 -4.0577493 -4.0553732 -4.0880318 -4.1144605 -4.1359172 -4.1547933 -4.1850605 -4.2366266][-4.2780342 -4.236793 -4.194881 -4.1558733 -4.1325297 -4.1240277 -4.1265945 -4.1230116 -4.1384416 -4.1667 -4.1779904 -4.1861162 -4.1929564 -4.2116642 -4.2523909][-4.2992234 -4.2664981 -4.2299609 -4.1958919 -4.1745834 -4.1650548 -4.1661277 -4.1715651 -4.19402 -4.2191043 -4.226645 -4.2305984 -4.2322693 -4.2431774 -4.2734947][-4.3197675 -4.2990894 -4.2722816 -4.2457948 -4.2269049 -4.2159004 -4.2110162 -4.2128229 -4.2317476 -4.2521496 -4.2577515 -4.2600479 -4.2607841 -4.2671 -4.2878232][-4.3319945 -4.3245349 -4.31045 -4.2943192 -4.2815466 -4.2723641 -4.2641373 -4.2608333 -4.2728257 -4.2868185 -4.2888479 -4.2863827 -4.2840405 -4.28633 -4.3000212][-4.3346291 -4.3347883 -4.3288665 -4.3198261 -4.3119359 -4.306457 -4.3011265 -4.2975159 -4.3033419 -4.3116474 -4.3130069 -4.3093042 -4.3050933 -4.30473 -4.3130407]]...]
INFO - root - 2017-12-06 02:00:37.588552: step 65210, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 62h:22m:14s remains)
INFO - root - 2017-12-06 02:00:46.132889: step 65220, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 62h:31m:46s remains)
INFO - root - 2017-12-06 02:00:54.788305: step 65230, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.853 sec/batch; 63h:18m:58s remains)
INFO - root - 2017-12-06 02:01:03.157168: step 65240, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 52h:58m:04s remains)
INFO - root - 2017-12-06 02:01:11.429546: step 65250, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 66h:14m:17s remains)
INFO - root - 2017-12-06 02:01:20.003350: step 65260, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 64h:28m:54s remains)
INFO - root - 2017-12-06 02:01:28.505566: step 65270, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.822 sec/batch; 61h:00m:26s remains)
INFO - root - 2017-12-06 02:01:36.808379: step 65280, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 62h:00m:59s remains)
INFO - root - 2017-12-06 02:01:45.461383: step 65290, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 62h:16m:01s remains)
INFO - root - 2017-12-06 02:01:53.848253: step 65300, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 63h:22m:27s remains)
2017-12-06 02:01:54.678541: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2183571 -4.2365623 -4.2575893 -4.2706451 -4.28079 -4.2732787 -4.2376032 -4.1742582 -4.11945 -4.1067376 -4.1531391 -4.2255921 -4.2761111 -4.29927 -4.3037143][-4.2129669 -4.2320614 -4.2501569 -4.259891 -4.2670951 -4.2591653 -4.2200055 -4.1535006 -4.0957084 -4.085187 -4.1364245 -4.2134476 -4.2686224 -4.3000035 -4.3147893][-4.2198777 -4.237812 -4.2474236 -4.2474203 -4.2471104 -4.2315378 -4.1879539 -4.1165614 -4.0552154 -4.0451694 -4.1022615 -4.185607 -4.2480769 -4.290226 -4.3150067][-4.2348986 -4.2492123 -4.2461476 -4.231864 -4.2198019 -4.1915908 -4.1397762 -4.0590587 -3.9866095 -3.9804468 -4.05148 -4.1486726 -4.2258215 -4.2789569 -4.3113933][-4.2510891 -4.2624116 -4.2502117 -4.2233009 -4.1967454 -4.1564679 -4.0945663 -3.999784 -3.9111812 -3.9144268 -4.0078874 -4.1202431 -4.2103744 -4.2719045 -4.3076587][-4.2621546 -4.2737608 -4.2589631 -4.2232995 -4.1855521 -4.1376276 -4.0721631 -3.9773142 -3.8912559 -3.9079242 -4.0096073 -4.1197386 -4.2110572 -4.2739387 -4.309299][-4.2708697 -4.2835393 -4.2691641 -4.2294374 -4.1840343 -4.1313815 -4.0720253 -4.0016985 -3.9506266 -3.9804511 -4.0654349 -4.1526074 -4.2286611 -4.2829757 -4.3123159][-4.2668157 -4.2820153 -4.2714171 -4.2328162 -4.1784673 -4.1194096 -4.0685368 -4.0323796 -4.0252252 -4.0679369 -4.134243 -4.196363 -4.2528872 -4.2926197 -4.311615][-4.2550631 -4.2745824 -4.2701612 -4.2348127 -4.173687 -4.1091914 -4.0670643 -4.0590382 -4.0836735 -4.1344986 -4.187871 -4.2340603 -4.2742777 -4.3001914 -4.310082][-4.2515359 -4.2764177 -4.2793293 -4.2479019 -4.1846619 -4.119988 -4.0819144 -4.0864716 -4.1246138 -4.1763358 -4.222702 -4.2604656 -4.2892432 -4.3057947 -4.3103738][-4.2559571 -4.2815185 -4.287344 -4.2585564 -4.199296 -4.1424317 -4.1093445 -4.1141562 -4.1504936 -4.1988587 -4.2452855 -4.2802534 -4.3018446 -4.3103456 -4.3079238][-4.25541 -4.2771282 -4.2845368 -4.2627621 -4.2150154 -4.1695838 -4.1413827 -4.1425424 -4.1705246 -4.2150693 -4.2619834 -4.2953215 -4.3102674 -4.306952 -4.2946658][-4.2402673 -4.2575669 -4.2685022 -4.2582941 -4.2262106 -4.1908937 -4.1657376 -4.1630483 -4.1835427 -4.2247348 -4.2710638 -4.3014979 -4.3085256 -4.2937164 -4.2705264][-4.2173376 -4.227715 -4.2402797 -4.2423759 -4.226 -4.2006083 -4.1793661 -4.1754384 -4.1940894 -4.2322774 -4.272315 -4.295989 -4.2960238 -4.2735534 -4.241878][-4.1939626 -4.202909 -4.2187071 -4.2292342 -4.2235546 -4.2084656 -4.195096 -4.1947608 -4.212132 -4.2396631 -4.2671137 -4.28277 -4.2808938 -4.2574224 -4.2241306]]...]
INFO - root - 2017-12-06 02:02:03.053953: step 65310, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 63h:43m:56s remains)
INFO - root - 2017-12-06 02:02:11.641234: step 65320, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 61h:13m:44s remains)
INFO - root - 2017-12-06 02:02:20.142032: step 65330, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 63h:18m:26s remains)
INFO - root - 2017-12-06 02:02:28.567521: step 65340, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 63h:34m:47s remains)
INFO - root - 2017-12-06 02:02:36.936250: step 65350, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 63h:27m:22s remains)
INFO - root - 2017-12-06 02:02:45.363690: step 65360, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 64h:03m:14s remains)
INFO - root - 2017-12-06 02:02:53.885518: step 65370, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 62h:45m:54s remains)
INFO - root - 2017-12-06 02:03:02.215751: step 65380, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 62h:41m:53s remains)
INFO - root - 2017-12-06 02:03:10.782985: step 65390, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.878 sec/batch; 65h:07m:39s remains)
INFO - root - 2017-12-06 02:03:19.311405: step 65400, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 64h:46m:22s remains)
2017-12-06 02:03:20.130001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.241086 -4.2419028 -4.24141 -4.2416124 -4.2419481 -4.2421212 -4.2434678 -4.246201 -4.2496166 -4.2513471 -4.2539606 -4.2580018 -4.2607603 -4.2609873 -4.2583961][-4.2248969 -4.2251129 -4.22369 -4.2226219 -4.2200518 -4.2166882 -4.2156205 -4.2197804 -4.2278461 -4.2346454 -4.2414508 -4.2493949 -4.2544289 -4.2545218 -4.2505059][-4.2116318 -4.2108016 -4.2087803 -4.2061362 -4.1986613 -4.1904125 -4.1856074 -4.1907659 -4.2043095 -4.2179837 -4.2305579 -4.2425885 -4.24995 -4.2502427 -4.245544][-4.20748 -4.2063484 -4.2033048 -4.1976647 -4.1820407 -4.1637683 -4.1509161 -4.1564169 -4.1794753 -4.2038007 -4.2213917 -4.2360573 -4.2455482 -4.24911 -4.2474518][-4.2073603 -4.2053967 -4.1985183 -4.1848025 -4.1569538 -4.120482 -4.0911016 -4.0935583 -4.1276608 -4.1654439 -4.1917653 -4.2159643 -4.2353067 -4.2482829 -4.2528996][-4.1846175 -4.1815767 -4.1675129 -4.1403127 -4.0944004 -4.03349 -3.9803939 -3.9771967 -4.0274634 -4.0830994 -4.1224756 -4.1591868 -4.196785 -4.2265272 -4.2405295][-4.1506658 -4.1430173 -4.1179547 -4.0727124 -4.0034566 -3.9102266 -3.8237567 -3.8152654 -3.8936849 -3.9767203 -4.0325317 -4.0812473 -4.1391273 -4.1898985 -4.2180109][-4.1427736 -4.1286817 -4.0919523 -4.0373168 -3.9550664 -3.841222 -3.7287767 -3.7192283 -3.824234 -3.9280741 -3.9937651 -4.0471983 -4.1124792 -4.1728287 -4.2084866][-4.1877213 -4.169167 -4.1284595 -4.0774741 -4.0049772 -3.9049106 -3.8111072 -3.8136714 -3.9094942 -4.002243 -4.0581069 -4.1025524 -4.1578813 -4.2096395 -4.2378573][-4.247324 -4.2297873 -4.1962523 -4.1554651 -4.1043525 -4.0373225 -3.9794879 -3.9899907 -4.0555625 -4.1213636 -4.1614995 -4.1956005 -4.2357259 -4.2691851 -4.2795][-4.2745905 -4.2632394 -4.2420812 -4.2157269 -4.1839247 -4.1452065 -4.1146379 -4.1252146 -4.1638308 -4.2057438 -4.2324853 -4.2597537 -4.2875214 -4.3030968 -4.2968254][-4.2789936 -4.2740436 -4.2637544 -4.2477794 -4.2283335 -4.2088566 -4.1963172 -4.2024493 -4.2234812 -4.2468672 -4.2651334 -4.2870078 -4.3066597 -4.311233 -4.2957797][-4.2679353 -4.2681 -4.2628775 -4.2514696 -4.2393088 -4.2309079 -4.2297235 -4.2357988 -4.2478561 -4.2581983 -4.2678385 -4.2844357 -4.2999182 -4.2999105 -4.281312][-4.2419763 -4.2461758 -4.2411962 -4.2263823 -4.2117743 -4.2053275 -4.2087212 -4.2147765 -4.2229047 -4.2272582 -4.2325368 -4.2462678 -4.261199 -4.2642837 -4.2513509][-4.211679 -4.2216263 -4.2183414 -4.1994529 -4.1776304 -4.1655564 -4.1662292 -4.1697526 -4.1744394 -4.1764803 -4.1822758 -4.1961579 -4.214056 -4.2256041 -4.22401]]...]
INFO - root - 2017-12-06 02:03:28.582284: step 65410, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 62h:46m:17s remains)
INFO - root - 2017-12-06 02:03:37.143722: step 65420, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 63h:56m:22s remains)
INFO - root - 2017-12-06 02:03:45.741143: step 65430, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 63h:39m:50s remains)
INFO - root - 2017-12-06 02:03:54.358199: step 65440, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 62h:01m:43s remains)
INFO - root - 2017-12-06 02:04:02.879815: step 65450, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 63h:43m:45s remains)
INFO - root - 2017-12-06 02:04:11.323309: step 65460, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 65h:33m:56s remains)
INFO - root - 2017-12-06 02:04:19.764074: step 65470, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 63h:08m:12s remains)
INFO - root - 2017-12-06 02:04:28.262924: step 65480, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 66h:35m:36s remains)
INFO - root - 2017-12-06 02:04:36.824458: step 65490, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 62h:34m:29s remains)
INFO - root - 2017-12-06 02:04:45.344072: step 65500, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 66h:11m:13s remains)
2017-12-06 02:04:46.104989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2031198 -4.2118907 -4.2105641 -4.2018514 -4.1983089 -4.1898818 -4.183919 -4.19367 -4.2118731 -4.2227912 -4.2145371 -4.1938877 -4.1780505 -4.186049 -4.2077632][-4.2262764 -4.2389975 -4.2384262 -4.2264175 -4.2189178 -4.2113752 -4.209259 -4.2172 -4.23225 -4.2355547 -4.214489 -4.1798444 -4.1508145 -4.1496229 -4.1715569][-4.2339964 -4.2441344 -4.2387176 -4.2228951 -4.2128711 -4.208487 -4.216455 -4.2294216 -4.247468 -4.2498403 -4.22409 -4.1848383 -4.1491547 -4.1350927 -4.144393][-4.21829 -4.2269483 -4.2222862 -4.205677 -4.1907158 -4.1844082 -4.2007785 -4.2241216 -4.2468853 -4.2529583 -4.2314897 -4.2008619 -4.1713614 -4.1521339 -4.1453872][-4.1930161 -4.2027183 -4.2007127 -4.1857753 -4.1637897 -4.1546454 -4.1737866 -4.2005243 -4.2250338 -4.2364216 -4.2264366 -4.2149138 -4.1998444 -4.184114 -4.1683736][-4.1686349 -4.179842 -4.1813192 -4.1692166 -4.144505 -4.1286807 -4.1407738 -4.1604972 -4.1804338 -4.1985941 -4.2061224 -4.21285 -4.2111564 -4.2055674 -4.1915946][-4.1415405 -4.1539636 -4.1627822 -4.1583838 -4.1366353 -4.1156564 -4.1086783 -4.1068983 -4.113163 -4.1335464 -4.1616192 -4.1824379 -4.1897874 -4.1946859 -4.1841469][-4.1186123 -4.1298232 -4.1453023 -4.1509047 -4.1361346 -4.1134491 -4.0850706 -4.05561 -4.041409 -4.0601597 -4.1091795 -4.1413407 -4.1530566 -4.1587887 -4.145731][-4.1051779 -4.1135435 -4.1320605 -4.1457229 -4.1373773 -4.1191812 -4.0798006 -4.0307775 -3.9993329 -4.0169725 -4.0796394 -4.115171 -4.1267347 -4.1265216 -4.1078711][-4.10788 -4.1019373 -4.1116924 -4.1288776 -4.1275311 -4.1195951 -4.088562 -4.0463324 -4.0109406 -4.0182166 -4.0757918 -4.1029315 -4.1098881 -4.1080933 -4.0929255][-4.0988407 -4.0798683 -4.0836272 -4.1024981 -4.1122351 -4.1182203 -4.1047635 -4.079473 -4.0511327 -4.0499811 -4.0910835 -4.1026921 -4.1039972 -4.1114092 -4.1083236][-4.1066284 -4.0837741 -4.0821652 -4.1024928 -4.1188049 -4.1327424 -4.1293411 -4.1149249 -4.0934458 -4.0879421 -4.1168618 -4.11716 -4.1117997 -4.12873 -4.1387472][-4.1318703 -4.1104021 -4.1018257 -4.1227908 -4.1446505 -4.1619906 -4.1590695 -4.1443677 -4.1249652 -4.1175709 -4.1362133 -4.1296482 -4.1209478 -4.1433706 -4.1618509][-4.1442719 -4.1295991 -4.1203728 -4.1425943 -4.1697655 -4.1874781 -4.1771502 -4.1574645 -4.1409168 -4.1351752 -4.1487665 -4.1403275 -4.1269155 -4.1474276 -4.1717658][-4.1385722 -4.1278186 -4.1213636 -4.1442685 -4.1766343 -4.1953096 -4.1782432 -4.156662 -4.1457434 -4.1425533 -4.15117 -4.1411376 -4.1231208 -4.1373372 -4.1643457]]...]
INFO - root - 2017-12-06 02:04:54.829410: step 65510, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 63h:10m:00s remains)
INFO - root - 2017-12-06 02:05:03.194203: step 65520, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 0.799 sec/batch; 59h:17m:21s remains)
INFO - root - 2017-12-06 02:05:11.563895: step 65530, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.840 sec/batch; 62h:16m:55s remains)
INFO - root - 2017-12-06 02:05:20.100315: step 65540, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 62h:01m:53s remains)
INFO - root - 2017-12-06 02:05:28.785417: step 65550, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.845 sec/batch; 62h:38m:34s remains)
INFO - root - 2017-12-06 02:05:37.357670: step 65560, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.850 sec/batch; 63h:02m:24s remains)
INFO - root - 2017-12-06 02:05:45.882581: step 65570, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 68h:20m:37s remains)
INFO - root - 2017-12-06 02:05:54.310060: step 65580, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 64h:39m:49s remains)
INFO - root - 2017-12-06 02:06:02.820713: step 65590, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 63h:25m:15s remains)
INFO - root - 2017-12-06 02:06:11.393803: step 65600, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 63h:09m:37s remains)
2017-12-06 02:06:12.162939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1851087 -4.1787729 -4.1809826 -4.186204 -4.1751714 -4.1655293 -4.1547656 -4.1519179 -4.1927333 -4.2222047 -4.2192259 -4.1911159 -4.1596966 -4.1274762 -4.1091981][-4.1782832 -4.1765814 -4.1772962 -4.1748495 -4.1523371 -4.1308956 -4.1111331 -4.105773 -4.1490912 -4.1833863 -4.1936393 -4.1786351 -4.1578808 -4.1324978 -4.1196952][-4.1672745 -4.1641588 -4.1574416 -4.1446457 -4.1088858 -4.0712934 -4.0420623 -4.0421791 -4.0979171 -4.1455407 -4.1709166 -4.1707425 -4.1590338 -4.1378508 -4.128468][-4.1604962 -4.154326 -4.1381283 -4.1094031 -4.0531554 -3.9960623 -3.9601469 -3.9774938 -4.0533543 -4.1192307 -4.1613173 -4.1746821 -4.1685953 -4.1433797 -4.1270785][-4.1603 -4.1502156 -4.12287 -4.0752125 -3.9964991 -3.9216259 -3.885267 -3.9224362 -4.0200214 -4.1054368 -4.1611085 -4.1830854 -4.1768789 -4.145824 -4.1187291][-4.1627569 -4.148345 -4.1099992 -4.0502439 -3.9621551 -3.8847425 -3.8557951 -3.9061346 -4.01133 -4.101872 -4.1610718 -4.1845331 -4.1724362 -4.1362081 -4.1029973][-4.1697645 -4.1501331 -4.1058083 -4.0479836 -3.9779198 -3.9208694 -3.9036813 -3.9456935 -4.0302405 -4.1040316 -4.1524391 -4.1706595 -4.1548991 -4.1194868 -4.0881758][-4.1786857 -4.1566606 -4.1137815 -4.0674853 -4.0225177 -3.9863939 -3.9783852 -4.0080962 -4.0648813 -4.1144638 -4.1464338 -4.1564488 -4.138957 -4.1074891 -4.0801821][-4.1796665 -4.1609941 -4.1283937 -4.0941572 -4.0668187 -4.0460143 -4.0489674 -4.0713181 -4.1054521 -4.1326556 -4.1503382 -4.1515794 -4.1326056 -4.1034555 -4.0772581][-4.161458 -4.1511321 -4.134521 -4.1152282 -4.1031828 -4.0960674 -4.1058636 -4.1242123 -4.1457353 -4.1607857 -4.1712394 -4.1637764 -4.1400161 -4.1107159 -4.0835128][-4.1430321 -4.1413212 -4.1416211 -4.136014 -4.1349268 -4.1340318 -4.1408896 -4.153935 -4.1702604 -4.18139 -4.1872334 -4.1759763 -4.1529374 -4.1292086 -4.1064234][-4.1413441 -4.1499686 -4.1624584 -4.1657705 -4.1692204 -4.16854 -4.16546 -4.1670766 -4.1781459 -4.1868939 -4.193049 -4.1848855 -4.1683393 -4.1504955 -4.1352992][-4.1536236 -4.1680174 -4.1817136 -4.1886916 -4.1927471 -4.1896205 -4.1789923 -4.173738 -4.1805053 -4.18912 -4.1970067 -4.1913037 -4.1771197 -4.1630888 -4.1568313][-4.17322 -4.1873922 -4.1978207 -4.2040138 -4.2063522 -4.1999369 -4.1865759 -4.1800451 -4.1834197 -4.1896734 -4.1964254 -4.1920652 -4.1791282 -4.1667948 -4.1668344][-4.1890082 -4.1992435 -4.2069583 -4.2144022 -4.2168884 -4.2092562 -4.1940117 -4.1854806 -4.1834803 -4.1869192 -4.1934662 -4.1947017 -4.1880894 -4.1820827 -4.1878324]]...]
INFO - root - 2017-12-06 02:06:20.663586: step 65610, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.824 sec/batch; 61h:03m:45s remains)
INFO - root - 2017-12-06 02:06:29.385404: step 65620, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 62h:26m:56s remains)
INFO - root - 2017-12-06 02:06:37.854511: step 65630, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 62h:31m:56s remains)
INFO - root - 2017-12-06 02:06:46.445280: step 65640, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 64h:24m:54s remains)
INFO - root - 2017-12-06 02:06:54.976503: step 65650, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 63h:49m:25s remains)
INFO - root - 2017-12-06 02:07:03.448913: step 65660, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 61h:33m:48s remains)
INFO - root - 2017-12-06 02:07:11.967752: step 65670, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.819 sec/batch; 60h:43m:09s remains)
INFO - root - 2017-12-06 02:07:20.393688: step 65680, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 65h:33m:42s remains)
INFO - root - 2017-12-06 02:07:28.939988: step 65690, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 63h:41m:02s remains)
INFO - root - 2017-12-06 02:07:37.430016: step 65700, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 61h:45m:20s remains)
2017-12-06 02:07:38.213129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2862635 -4.2870426 -4.2820616 -4.2819586 -4.2847171 -4.2895608 -4.2925529 -4.2936158 -4.2889552 -4.2807178 -4.2745404 -4.2724762 -4.2771106 -4.2853565 -4.2914944][-4.2460885 -4.2412624 -4.2341285 -4.2313008 -4.2325912 -4.2416039 -4.2534657 -4.2596116 -4.2601905 -4.2546782 -4.2449417 -4.2432313 -4.2525134 -4.2662945 -4.2772722][-4.2128448 -4.2057 -4.1985168 -4.1884604 -4.1800904 -4.1906958 -4.2118626 -4.2186246 -4.2219744 -4.2217164 -4.2128921 -4.2165937 -4.2351923 -4.2560844 -4.2730055][-4.1639442 -4.1566095 -4.1481853 -4.1277065 -4.1064653 -4.12091 -4.15385 -4.1605511 -4.1599469 -4.1631904 -4.1635094 -4.1801453 -4.2090473 -4.2391534 -4.2668519][-4.1171432 -4.109314 -4.0980787 -4.0671663 -4.0320668 -4.0518923 -4.0968733 -4.101747 -4.0945606 -4.0974479 -4.1088705 -4.1453986 -4.1868887 -4.2280736 -4.2665854][-4.1061931 -4.1013422 -4.0929041 -4.0630374 -4.0203972 -4.036 -4.0769072 -4.0693035 -4.0518346 -4.05671 -4.0806642 -4.1337285 -4.1863232 -4.2366343 -4.2798576][-4.1234317 -4.1205659 -4.1144476 -4.090322 -4.0477095 -4.0557332 -4.0808654 -4.0581455 -4.0342388 -4.0385027 -4.0693493 -4.134973 -4.19116 -4.244946 -4.2928648][-4.14828 -4.154635 -4.156909 -4.1402359 -4.0999966 -4.1006646 -4.1170058 -4.0932641 -4.0727129 -4.0727162 -4.0968223 -4.15914 -4.2076721 -4.2551336 -4.301374][-4.169095 -4.17924 -4.1882615 -4.1788912 -4.14159 -4.1383762 -4.1510811 -4.1299858 -4.11219 -4.1081877 -4.1223655 -4.1723404 -4.2133045 -4.2581034 -4.3063755][-4.1853595 -4.1939359 -4.20624 -4.204371 -4.1753569 -4.1740017 -4.1927233 -4.1812534 -4.1638107 -4.1577773 -4.1634612 -4.2021623 -4.235857 -4.2764077 -4.319118][-4.2036204 -4.2116375 -4.2275414 -4.2324367 -4.2175879 -4.2220225 -4.2418294 -4.2364841 -4.220717 -4.2127104 -4.2151423 -4.2444715 -4.2669883 -4.2980709 -4.3336554][-4.2236423 -4.2312493 -4.2477112 -4.2579293 -4.2569523 -4.2700329 -4.2829118 -4.274591 -4.2555251 -4.2416148 -4.2415276 -4.2620392 -4.277565 -4.3041887 -4.3360424][-4.2550435 -4.2598462 -4.2725191 -4.2838373 -4.2890124 -4.3014197 -4.3084497 -4.3020844 -4.2874413 -4.2740641 -4.2702613 -4.2805281 -4.2913122 -4.3127379 -4.3377576][-4.2775087 -4.2789049 -4.2855434 -4.2949538 -4.3032393 -4.3115988 -4.31526 -4.31216 -4.3031769 -4.2929287 -4.2881927 -4.29538 -4.3072958 -4.3259058 -4.34415][-4.2910233 -4.2901068 -4.2925234 -4.2994075 -4.3089538 -4.3172965 -4.3206668 -4.3191433 -4.3113785 -4.3003116 -4.2940516 -4.3002944 -4.3127446 -4.32952 -4.3441672]]...]
INFO - root - 2017-12-06 02:07:46.831083: step 65710, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 65h:51m:48s remains)
INFO - root - 2017-12-06 02:07:55.482169: step 65720, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 62h:55m:30s remains)
INFO - root - 2017-12-06 02:08:03.978582: step 65730, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 62h:30m:35s remains)
INFO - root - 2017-12-06 02:08:12.439839: step 65740, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 64h:41m:36s remains)
INFO - root - 2017-12-06 02:08:21.006297: step 65750, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 63h:48m:15s remains)
INFO - root - 2017-12-06 02:08:29.475039: step 65760, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 61h:50m:52s remains)
INFO - root - 2017-12-06 02:08:37.998744: step 65770, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 63h:39m:49s remains)
INFO - root - 2017-12-06 02:08:46.349773: step 65780, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 62h:54m:31s remains)
INFO - root - 2017-12-06 02:08:54.963344: step 65790, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 63h:57m:42s remains)
INFO - root - 2017-12-06 02:09:03.548192: step 65800, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 64h:39m:47s remains)
2017-12-06 02:09:04.335315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2210703 -4.2181344 -4.21509 -4.212996 -4.211154 -4.2084055 -4.20683 -4.2076082 -4.2051253 -4.1964312 -4.1861024 -4.1815796 -4.1856327 -4.1965427 -4.2092233][-4.2285256 -4.2249813 -4.2217159 -4.2195945 -4.2166152 -4.2121582 -4.2094407 -4.2104397 -4.2041106 -4.1871428 -4.1686783 -4.1624031 -4.1679106 -4.1848927 -4.2020903][-4.2313557 -4.227417 -4.224329 -4.2212706 -4.21636 -4.2098646 -4.2064123 -4.2048254 -4.1917129 -4.1623044 -4.132936 -4.123641 -4.1300793 -4.1526318 -4.1777997][-4.226656 -4.2238865 -4.2215981 -4.2165 -4.2077804 -4.1974893 -4.1925898 -4.1884174 -4.1676364 -4.1254377 -4.0866923 -4.0728741 -4.0785289 -4.1030593 -4.1365366][-4.2101464 -4.20729 -4.2033124 -4.1926775 -4.1784406 -4.1651797 -4.1616888 -4.1605458 -4.1404123 -4.0943794 -4.0518141 -4.0380077 -4.0413871 -4.060904 -4.0931473][-4.172493 -4.1626277 -4.1503677 -4.131269 -4.1119294 -4.1019039 -4.107862 -4.1202826 -4.1119361 -4.0760665 -4.0393229 -4.0288377 -4.0281267 -4.0364642 -4.0618811][-4.1414733 -4.1228704 -4.1004252 -4.0709891 -4.0466022 -4.03854 -4.0538664 -4.0799208 -4.0882077 -4.0702443 -4.0451488 -4.0368915 -4.0288477 -4.0281467 -4.0492673][-4.1477814 -4.1305785 -4.108562 -4.0796261 -4.0527563 -4.041048 -4.0562449 -4.0844803 -4.1002769 -4.0963907 -4.0810537 -4.0709953 -4.053895 -4.0459094 -4.0668015][-4.1837912 -4.1726842 -4.1547723 -4.1305966 -4.1047773 -4.088975 -4.1001291 -4.126749 -4.1444221 -4.1476049 -4.1405907 -4.1324124 -4.112071 -4.1003747 -4.1166739][-4.213985 -4.204483 -4.1867642 -4.166574 -4.14699 -4.1341748 -4.1409035 -4.1636753 -4.1838355 -4.1952329 -4.1981568 -4.197576 -4.1836395 -4.1706166 -4.1771765][-4.2306633 -4.2202692 -4.2017655 -4.1853819 -4.1719608 -4.1619906 -4.1627936 -4.1801643 -4.2024584 -4.2220755 -4.2335753 -4.2382846 -4.2329855 -4.2228212 -4.22136][-4.2448096 -4.2405243 -4.2289219 -4.2186732 -4.2078214 -4.1974869 -4.1929636 -4.2019339 -4.2196932 -4.2384963 -4.2498093 -4.2550697 -4.2559114 -4.2492971 -4.2464423][-4.2493653 -4.2523012 -4.2492743 -4.2461605 -4.23975 -4.2313271 -4.2258134 -4.2289348 -4.2374296 -4.24574 -4.2490287 -4.2516618 -4.2534122 -4.2497387 -4.2482371][-4.2360883 -4.2461047 -4.251092 -4.2517567 -4.2481723 -4.2435584 -4.2408442 -4.2429447 -4.2449555 -4.2444296 -4.2415667 -4.2417183 -4.2428145 -4.2405086 -4.2404509][-4.2115674 -4.2233276 -4.2336173 -4.2387371 -4.2389441 -4.2386246 -4.2391739 -4.2391315 -4.235703 -4.2284226 -4.2207975 -4.2181392 -4.2194362 -4.2226691 -4.2279582]]...]
INFO - root - 2017-12-06 02:09:12.891780: step 65810, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 64h:30m:50s remains)
INFO - root - 2017-12-06 02:09:21.415650: step 65820, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 63h:47m:20s remains)
INFO - root - 2017-12-06 02:09:29.981720: step 65830, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 63h:02m:31s remains)
INFO - root - 2017-12-06 02:09:38.470483: step 65840, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.868 sec/batch; 64h:17m:09s remains)
INFO - root - 2017-12-06 02:09:46.888272: step 65850, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 63h:58m:10s remains)
INFO - root - 2017-12-06 02:09:55.473422: step 65860, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 64h:22m:19s remains)
INFO - root - 2017-12-06 02:10:03.975041: step 65870, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 62h:18m:18s remains)
INFO - root - 2017-12-06 02:10:12.553671: step 65880, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 65h:32m:53s remains)
INFO - root - 2017-12-06 02:10:21.058891: step 65890, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 63h:56m:36s remains)
INFO - root - 2017-12-06 02:10:29.636419: step 65900, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 62h:40m:29s remains)
2017-12-06 02:10:30.415215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1869216 -4.1670671 -4.1447377 -4.1286626 -4.1362653 -4.143127 -4.1271663 -4.107985 -4.0898709 -4.0824013 -4.09009 -4.1066852 -4.1298261 -4.1496205 -4.1681519][-4.1945457 -4.1756854 -4.1534925 -4.135747 -4.1383324 -4.143311 -4.1250887 -4.0953965 -4.0732336 -4.0728583 -4.094 -4.1278167 -4.1623669 -4.1799808 -4.1837583][-4.2055368 -4.1888752 -4.1693707 -4.1505423 -4.1464477 -4.1437674 -4.1201167 -4.0855465 -4.0620961 -4.0699067 -4.0939059 -4.1276331 -4.1644659 -4.1833868 -4.1774511][-4.211092 -4.1941376 -4.1779041 -4.1645517 -4.1562142 -4.1383166 -4.10529 -4.0711517 -4.0536475 -4.0675697 -4.086452 -4.1100578 -4.1403193 -4.1593075 -4.1576319][-4.2120261 -4.1925559 -4.1756616 -4.1648231 -4.1529255 -4.1252441 -4.0862341 -4.0553236 -4.0519733 -4.0734162 -4.0885859 -4.0982132 -4.1149707 -4.1305785 -4.1336756][-4.2145767 -4.1928926 -4.1746264 -4.1617174 -4.1473336 -4.1175694 -4.076581 -4.0463581 -4.0508027 -4.0740614 -4.084053 -4.0875869 -4.098424 -4.1123586 -4.119071][-4.2166338 -4.196537 -4.1771541 -4.1623807 -4.1450553 -4.120513 -4.0869861 -4.0585938 -4.0577674 -4.0705128 -4.0730867 -4.0734134 -4.0879288 -4.1075072 -4.1239924][-4.2211046 -4.2019382 -4.1778378 -4.1549554 -4.1353865 -4.1219959 -4.106555 -4.0877995 -4.0776772 -4.0759482 -4.0730486 -4.0760517 -4.0995884 -4.1281571 -4.1485004][-4.2198653 -4.2019949 -4.1754522 -4.1447263 -4.1251168 -4.1244936 -4.1279159 -4.1216455 -4.112721 -4.1037774 -4.0945587 -4.0985327 -4.1250014 -4.1522183 -4.1669254][-4.2081861 -4.1910787 -4.1649365 -4.13582 -4.121572 -4.1315522 -4.1456614 -4.1494741 -4.1454372 -4.1352863 -4.1238122 -4.1257105 -4.142272 -4.1590629 -4.1638093][-4.1929235 -4.1689153 -4.1425018 -4.1215906 -4.1193094 -4.1403871 -4.1606197 -4.1681476 -4.1691756 -4.162621 -4.1509123 -4.1460609 -4.1477017 -4.1514978 -4.1498222][-4.1888785 -4.1543837 -4.1219478 -4.1035995 -4.1092453 -4.1395249 -4.1719551 -4.1874452 -4.192668 -4.1888828 -4.1773796 -4.1640034 -4.1511669 -4.1492047 -4.1505661][-4.1951714 -4.1590977 -4.1263647 -4.1062193 -4.1102915 -4.13979 -4.1752362 -4.1981435 -4.2067089 -4.2076845 -4.2011771 -4.1858649 -4.167099 -4.1643429 -4.169971][-4.2084846 -4.1786661 -4.1552367 -4.1390142 -4.1374664 -4.1541996 -4.1824341 -4.2083268 -4.2214026 -4.225749 -4.2246218 -4.2118349 -4.1920714 -4.1883683 -4.19531][-4.2284231 -4.2041883 -4.1884265 -4.1776967 -4.1762977 -4.1867957 -4.2094622 -4.2339311 -4.2456961 -4.2474246 -4.2447233 -4.2335134 -4.2187276 -4.215476 -4.2202625]]...]
INFO - root - 2017-12-06 02:10:38.969212: step 65910, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 65h:17m:20s remains)
INFO - root - 2017-12-06 02:10:47.551541: step 65920, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 62h:51m:03s remains)
INFO - root - 2017-12-06 02:10:56.044193: step 65930, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 61h:18m:45s remains)
INFO - root - 2017-12-06 02:11:04.598833: step 65940, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 66h:32m:45s remains)
INFO - root - 2017-12-06 02:11:13.183172: step 65950, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 64h:37m:44s remains)
INFO - root - 2017-12-06 02:11:21.661864: step 65960, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 63h:04m:06s remains)
INFO - root - 2017-12-06 02:11:30.155522: step 65970, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 62h:05m:52s remains)
INFO - root - 2017-12-06 02:11:38.630129: step 65980, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 63h:40m:45s remains)
INFO - root - 2017-12-06 02:11:47.055000: step 65990, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 62h:31m:37s remains)
INFO - root - 2017-12-06 02:11:55.665624: step 66000, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 62h:32m:43s remains)
2017-12-06 02:11:56.414892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1894712 -4.1862264 -4.1728139 -4.1514277 -4.1226826 -4.0890565 -4.0592651 -4.0660162 -4.1209397 -4.2033663 -4.2813697 -4.3354106 -4.3600445 -4.3634295 -4.361062][-4.1733112 -4.1743407 -4.1685948 -4.1547666 -4.1248708 -4.0750003 -4.0260744 -4.0176582 -4.0709486 -4.1638036 -4.2577181 -4.32588 -4.3580227 -4.3648844 -4.362812][-4.1857514 -4.191421 -4.195838 -4.1883712 -4.1540585 -4.0896406 -4.0276117 -4.0046253 -4.0478215 -4.1401706 -4.2411666 -4.3176565 -4.3541803 -4.3643 -4.3632026][-4.23228 -4.2368097 -4.2421141 -4.2335944 -4.1917262 -4.1180887 -4.0487032 -4.0172334 -4.0514851 -4.1387768 -4.2386074 -4.3151321 -4.3513889 -4.3623667 -4.3623939][-4.2842674 -4.2830372 -4.2805591 -4.2617607 -4.2103152 -4.1320124 -4.0603404 -4.0256844 -4.0566568 -4.1417103 -4.2400308 -4.3156013 -4.3502436 -4.3600478 -4.3602505][-4.3117352 -4.3042064 -4.2925606 -4.2609153 -4.2002039 -4.1240644 -4.0542636 -4.0193548 -4.0523171 -4.1395049 -4.2390265 -4.3152566 -4.3494434 -4.3580561 -4.3578391][-4.3173261 -4.3031507 -4.2811069 -4.2342572 -4.161376 -4.0876222 -4.0217452 -3.9922664 -4.0304089 -4.1252422 -4.2282314 -4.3068285 -4.3447957 -4.355351 -4.3559194][-4.3111277 -4.2894464 -4.2569561 -4.1981177 -4.1160989 -4.0424232 -3.9806497 -3.9585745 -4.0030208 -4.1084933 -4.2173214 -4.2986307 -4.341207 -4.3543034 -4.3557539][-4.2986774 -4.274003 -4.2370696 -4.1751504 -4.0906591 -4.0147066 -3.9569545 -3.9448667 -3.9998248 -4.1129112 -4.2216992 -4.3012772 -4.3430982 -4.3557062 -4.3575459][-4.283133 -4.2645311 -4.2336287 -4.1788068 -4.0971251 -4.0185194 -3.96143 -3.9530315 -4.0123987 -4.1256075 -4.2298222 -4.3062067 -4.3449674 -4.3570704 -4.3592806][-4.2711959 -4.2650118 -4.2452116 -4.2033925 -4.1319275 -4.0547523 -3.996536 -3.9896309 -4.0491757 -4.1557903 -4.2493958 -4.3189034 -4.3525295 -4.3621397 -4.3624134][-4.2555175 -4.2663178 -4.2617011 -4.234273 -4.1775875 -4.1113935 -4.0602 -4.0576849 -4.1146736 -4.2077971 -4.2842178 -4.3409867 -4.36552 -4.3700223 -4.3669868][-4.2463489 -4.27169 -4.2826548 -4.2664461 -4.22258 -4.1682138 -4.1261439 -4.1258922 -4.1766891 -4.2543716 -4.3145924 -4.35973 -4.3762736 -4.3758612 -4.370594][-4.2529478 -4.28846 -4.3096561 -4.3014879 -4.2672324 -4.2231503 -4.1889033 -4.1891179 -4.2294288 -4.2901425 -4.3366318 -4.3710694 -4.3816485 -4.3791323 -4.3724737][-4.2782369 -4.314219 -4.3355379 -4.3308387 -4.3040123 -4.26884 -4.240798 -4.2393789 -4.2672892 -4.3117032 -4.3482761 -4.3734713 -4.3804111 -4.3778391 -4.3712049]]...]
INFO - root - 2017-12-06 02:12:04.946204: step 66010, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 61h:45m:09s remains)
INFO - root - 2017-12-06 02:12:13.509738: step 66020, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 64h:03m:53s remains)
INFO - root - 2017-12-06 02:12:22.041903: step 66030, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 64h:24m:08s remains)
INFO - root - 2017-12-06 02:12:30.500610: step 66040, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 63h:50m:05s remains)
