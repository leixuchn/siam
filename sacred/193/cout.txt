INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "193"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-baias-relu
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/Relu:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-10 07:21:27.391947: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:21:27.392015: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:21:27.392214: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:21:27.392247: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:21:27.392326: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:21:28.514467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-10 07:21:28.514679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 07:21:28.514752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 07:21:28.514816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/Relu:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>]
kkkkkkkkkkkkkkkkkkkkkkk [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>, <tf.VariabINFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-10 07:21:32.217913: step 0, loss = 0.75, batch loss = 0.69 (3.0 examples/sec; 2.672 sec/batch; 246h:47m:08s remains)
2017-12-10 07:21:32.670232: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00018833175 0.00020142911 0.0002100016 0.00021948731 0.00023084426 0.00024307986 0.00025418255 0.00026637569 0.00027909476 0.00028486262 0.00027988697 0.00026571969 0.00024324401 0.00021958817 0.0001972632][0.00020093464 0.00021650734 0.00022686638 0.00024118385 0.00026304313 0.0002879689 0.0003115538 0.0003312433 0.00034836555 0.00035234352 0.00034049127 0.00031607447 0.00028131565 0.00024756728 0.00021702511][0.00021070431 0.00022671615 0.00023851036 0.0002605349 0.00029647 0.00033722969 0.00037521025 0.00040507776 0.00042750276 0.00042904785 0.00040838777 0.00036928421 0.00031952347 0.000274511 0.00023507063][0.00021932578 0.0002344286 0.00024642673 0.00027636864 0.00032465236 0.00037957384 0.00043161164 0.00047791633 0.00051082828 0.00050858851 0.0004752911 0.000420535 0.00035569305 0.00029891549 0.00025149263][0.00022822829 0.0002418065 0.00025423861 0.00029167862 0.0003512179 0.00042049168 0.00049098139 0.00056386716 0.00061006064 0.00059628586 0.00054452615 0.00046961606 0.00038829632 0.00031923936 0.00026363155][0.00023947026 0.00025396721 0.00027069511 0.00031614324 0.0003848293 0.00046374023 0.00055554305 0.00066346215 0.00072059018 0.00067829411 0.00059617829 0.00050213881 0.00040939313 0.00033212313 0.00027069208][0.000256721 0.0002744938 0.00029814127 0.00034729321 0.00041606632 0.00050026318 0.00061471987 0.00075754058 0.00080539734 0.00071450672 0.00060498627 0.00050615508 0.00041346613 0.00033492155 0.00027113882][0.00027472342 0.00029639326 0.00032531677 0.00037214084 0.00043448855 0.000515281 0.00062704011 0.00075375062 0.00075537071 0.00064518978 0.000545536 0.00046666843 0.00039067236 0.00032264608 0.0002652668][0.00028966524 0.00031315334 0.0003410663 0.00037885748 0.00042823059 0.00048739035 0.00056304428 0.00063452852 0.00061001489 0.0005298336 0.0004664716 0.00041600244 0.00035877165 0.00030334963 0.00025493314][0.00029550013 0.00031992348 0.0003451893 0.0003766068 0.00041319925 0.00044765492 0.00048661837 0.00051507819 0.0004885681 0.0004395704 0.00040498545 0.00037763128 0.00033570817 0.0002889265 0.00024764342][0.00029422055 0.000320949 0.00034513735 0.00037184206 0.0003980597 0.00041452685 0.00042962845 0.00043579435 0.00041208597 0.00038058334 0.0003605083 0.00034624393 0.00031265538 0.000274951 0.00024037815][0.00028751456 0.00031317791 0.0003352208 0.00035484679 0.00036885898 0.00037532559 0.00037786874 0.00037353576 0.00035455363 0.00033771215 0.00032562271 0.00031613346 0.00028836605 0.00025918742 0.00023149484][0.00027444548 0.00029214515 0.00030607125 0.00031583049 0.00032170623 0.0003263714 0.00032759379 0.00032452893 0.00031481535 0.00031054582 0.00030415616 0.00029723169 0.000272532 0.000248285 0.00022506404][0.00026140158 0.00026962682 0.00027582821 0.00028041017 0.00028567287 0.00029173327 0.00029595921 0.00029931802 0.00030190218 0.00030797525 0.00030533495 0.00029744944 0.000269506 0.00024454287 0.00022208813][0.00025271217 0.00025957692 0.00026424165 0.00026851307 0.00027529665 0.00028195049 0.00028881343 0.00029879989 0.00031044445 0.00032264975 0.00031952138 0.00030624287 0.00027375406 0.00024609303 0.00022242237]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-baias-relu/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-baias-relu/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
le 'OptimizeLoss/siamese_fc/conv1/weights/Momentum:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/BatchNorm/beta/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/BatchNorm/gamma/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/gamma/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/gamma/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/weights/Momentum:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/BatchNorm/beta/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/BatchNorm/gamma/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset1/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset2/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b1/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b1/biases/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b2/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b2/biases/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/detection/biases/Momentum:0' shape=(1,) dtype=float32_ref>]
INFO - root - 2017-12-10 07:21:35.435640: step 10, loss = 0.75, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:24m:39s remains)
INFO - root - 2017-12-10 07:21:37.633294: step 20, loss = 0.75, batch loss = 0.69 (33.5 examples/sec; 0.239 sec/batch; 22h:03m:40s remains)
INFO - root - 2017-12-10 07:21:39.765878: step 30, loss = 0.75, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:25s remains)
INFO - root - 2017-12-10 07:21:41.953897: step 40, loss = 0.75, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:49m:14s remains)
INFO - root - 2017-12-10 07:21:44.159960: step 50, loss = 0.75, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:21m:39s remains)
INFO - root - 2017-12-10 07:21:46.338041: step 60, loss = 0.75, batch loss = 0.69 (33.9 examples/sec; 0.236 sec/batch; 21h:45m:53s remains)
INFO - root - 2017-12-10 07:21:48.548126: step 70, loss = 0.75, batch loss = 0.69 (35.7 examples/sec; 0.224 sec/batch; 20h:42m:49s remains)
INFO - root - 2017-12-10 07:21:50.700141: step 80, loss = 0.75, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:55s remains)
INFO - root - 2017-12-10 07:21:52.837842: step 90, loss = 0.75, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:20m:50s remains)
INFO - root - 2017-12-10 07:21:54.984891: step 100, loss = 0.76, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:12m:25s remains)
2017-12-10 07:21:55.326898: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.008296431 0.0090925824 0.0097540021 0.01025337 0.010518265 0.010540842 0.010384997 0.010166794 0.0099975485 0.00993625 0.0099754706 0.010041735 0.010038832 0.0099000055 0.009608794][0.0085607842 0.0094608041 0.010245343 0.010876979 0.011264222 0.01138711 0.011298544 0.011102347 0.010902395 0.010765994 0.010695466 0.010635629 0.010515266 0.010285508 0.0099460455][0.008540025 0.0094351685 0.010231486 0.010894774 0.011339429 0.011532061 0.011515605 0.011371547 0.011183272 0.01101385 0.010868784 0.010716965 0.010511628 0.010234461 0.0098917866][0.0084080044 0.0092195915 0.0099306395 0.010536786 0.010965201 0.011187704 0.011231177 0.011152117 0.011015599 0.010855282 0.010681479 0.010482247 0.010235375 0.00994645 0.0096313078][0.0082242051 0.0089323148 0.00953055 0.01004361 0.010418746 0.010638615 0.010716615 0.010696426 0.010613309 0.010480455 0.010304004 0.010080296 0.0098115942 0.009516973 0.0092198644][0.0080745006 0.0086928923 0.0091997674 0.0096218856 0.0099400012 0.010134171 0.010223015 0.0102316 0.010176316 0.010062157 0.0098805092 0.0096366545 0.0093407631 0.0090228943 0.00871205][0.0079362672 0.0084773041 0.008909462 0.0092646889 0.0095308786 0.0096993642 0.0097858449 0.0098047778 0.0097607821 0.0096542556 0.0094664749 0.0092018954 0.0088727521 0.0085194735 0.0081733111][0.0077927536 0.0082528973 0.0086092064 0.0088949539 0.009105524 0.0092349183 0.0093139261 0.0093481559 0.0093273167 0.0092409821 0.009067419 0.00880592 0.0084595084 0.0080690142 0.0076724119][0.0076404265 0.0080211377 0.0082924478 0.0084955534 0.0086418055 0.008732073 0.0087995166 0.0088451253 0.0088596838 0.0088185547 0.0086917169 0.0084639722 0.0081322482 0.0077279131 0.0072932718][0.0074984725 0.0078094681 0.008010596 0.00814465 0.00823299 0.0082843378 0.0083341291 0.00838786 0.00843701 0.0084540788 0.0083955266 0.0082277963 0.00794093 0.0075519457 0.0071025547][0.0074078767 0.0076678512 0.0078214733 0.0079165315 0.00796202 0.0079757143 0.0080057224 0.0080608279 0.00813291 0.0081990417 0.00820702 0.008115829 0.007895845 0.0075542484 0.0071285428][0.0073527717 0.0075940923 0.0077321841 0.0078099421 0.00783446 0.0078249415 0.0078357132 0.00788041 0.00795727 0.0080460319 0.0081073269 0.0080763008 0.0079269158 0.0076601766 0.0073066754][0.0073211743 0.0075577796 0.0076998575 0.0077806679 0.0078019858 0.0077858665 0.0077769887 0.0078060073 0.007869916 0.0079614352 0.0080482122 0.008061775 0.0079897931 0.0078029768 0.0075408509][0.0072918548 0.0075233486 0.0076672458 0.0077594696 0.0077967956 0.0077871871 0.007771485 0.0077779037 0.0078119091 0.0078802845 0.0079562664 0.0079861581 0.0079587651 0.007855298 0.0077002905][0.0072471211 0.0074681914 0.0076098391 0.0077095204 0.0077602263 0.00776235 0.007746404 0.0077407416 0.0077536316 0.0077957287 0.0078516565 0.0078792274 0.0078785587 0.0078292163 0.0077490192]]...]
INFO - root - 2017-12-10 07:21:57.522523: step 110, loss = 0.76, batch loss = 0.69 (36.6 examples/sec; 0.218 sec/batch; 20h:10m:10s remains)
INFO - root - 2017-12-10 07:21:59.692280: step 120, loss = 0.76, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:18m:00s remains)
INFO - root - 2017-12-10 07:22:01.871169: step 130, loss = 0.76, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:17s remains)
INFO - root - 2017-12-10 07:22:04.050267: step 140, loss = 0.76, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:42m:16s remains)
INFO - root - 2017-12-10 07:22:06.213602: step 150, loss = 0.76, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:44s remains)
INFO - root - 2017-12-10 07:22:08.416342: step 160, loss = 0.76, batch loss = 0.69 (34.7 examples/sec; 0.231 sec/batch; 21h:17m:08s remains)
INFO - root - 2017-12-10 07:22:10.599702: step 170, loss = 0.76, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:37s remains)
INFO - root - 2017-12-10 07:22:12.829255: step 180, loss = 0.76, batch loss = 0.69 (35.6 examples/sec; 0.225 sec/batch; 20h:45m:10s remains)
INFO - root - 2017-12-10 07:22:15.052029: step 190, loss = 0.76, batch loss = 0.69 (34.4 examples/sec; 0.232 sec/batch; 21h:27m:28s remains)
INFO - root - 2017-12-10 07:22:17.222159: step 200, loss = 0.76, batch loss = 0.69 (36.3 examples/sec; 0.221 sec/batch; 20h:21m:49s remains)
2017-12-10 07:22:17.584882: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0049478458 0.004910605 0.0049916268 0.0052024149 0.0054674051 0.0057437252 0.0059993737 0.0062592523 0.00648992 0.0067017889 0.0068891332 0.0070400755 0.0071486766 0.0072254953 0.0072409869][0.0044368682 0.0043545077 0.0043698978 0.0044985465 0.0046806093 0.0049043647 0.0051391963 0.005406464 0.0057025272 0.0060110516 0.0062977523 0.0065312083 0.0066899573 0.0067762011 0.0067656292][0.0037966659 0.0036547706 0.0036281147 0.003692447 0.0037786658 0.0038790668 0.0040242276 0.004250736 0.0045677628 0.0049297935 0.0052817171 0.0055955364 0.0058124675 0.0059061912 0.0058702566][0.003277757 0.0030932743 0.003020897 0.0030350343 0.0030419261 0.0030318324 0.0030768239 0.0032120594 0.00349219 0.0038330245 0.0041803247 0.0045202607 0.0047647106 0.004866227 0.0048310258][0.0029419358 0.0027087079 0.0026176707 0.0026116269 0.0025803109 0.0025036579 0.0024395636 0.0024554417 0.0026246258 0.0028751153 0.0031568096 0.0034538819 0.0036649921 0.0037359679 0.0036821063][0.0026836235 0.0023847774 0.0022566011 0.0022606235 0.0022529063 0.0021687795 0.0020717315 0.0020133234 0.0020692768 0.0021956987 0.0023835232 0.0026185655 0.0027991263 0.002874282 0.0028189942][0.0025512255 0.0021932526 0.0020189369 0.0020295565 0.0020884208 0.0020516291 0.0019852191 0.0018759652 0.0018279719 0.0018466347 0.0019045807 0.0020325263 0.0021374503 0.0021610593 0.0020919172][0.0025330617 0.0021263657 0.0019058287 0.0019279893 0.00205138 0.0021241906 0.002142366 0.0020613822 0.0020014581 0.001954308 0.0019109929 0.0019122022 0.0019188198 0.001860147 0.0017451039][0.0025134156 0.0021189868 0.0018877285 0.0019159806 0.0020705292 0.0022152993 0.0022942964 0.0022723242 0.0022330997 0.0021506269 0.0020569635 0.0019862182 0.0019266689 0.0018393697 0.0017184084][0.0025632835 0.0022293164 0.0020334653 0.0020806354 0.0022396247 0.0024241556 0.0025413262 0.002595945 0.0025899389 0.0025298707 0.0024601228 0.0023529362 0.0022613159 0.0021343871 0.0019892277][0.0027806307 0.0025417383 0.0024069436 0.0024882634 0.0026523811 0.0028452284 0.0030009067 0.0031081231 0.0031490261 0.0031337354 0.0030964515 0.0030120546 0.00293229 0.0028118212 0.002686268][0.0034003183 0.0032701364 0.0031915687 0.0032828229 0.0034084045 0.003569023 0.0037179769 0.0038309703 0.0038858396 0.0039168205 0.0038976697 0.0038268755 0.0037495664 0.0036254586 0.003504381][0.0042674867 0.0042747613 0.0042596925 0.0043606437 0.0044816113 0.0046297526 0.0047580581 0.0048629497 0.0049123717 0.0049770284 0.0049844766 0.0049356665 0.0048593241 0.00474325 0.00462196][0.0052592056 0.0053833 0.0054305703 0.0055463356 0.0056567234 0.0057706614 0.0058718175 0.0059323828 0.0059903027 0.0060536074 0.0060906061 0.0061044018 0.0060601546 0.0059798853 0.0058429213][0.00624684 0.0064506526 0.0065378612 0.0066523328 0.0067418041 0.0068358453 0.00688998 0.0069360724 0.0069834781 0.0070472225 0.0071034129 0.0071331472 0.0071344711 0.0071135717 0.00702106]]...]
INFO - root - 2017-12-10 07:22:19.821573: step 210, loss = 0.76, batch loss = 0.69 (35.8 examples/sec; 0.223 sec/batch; 20h:36m:42s remains)
INFO - root - 2017-12-10 07:22:22.016768: step 220, loss = 0.76, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:28s remains)
INFO - root - 2017-12-10 07:22:24.187613: step 230, loss = 0.76, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:17m:39s remains)
INFO - root - 2017-12-10 07:22:26.363891: step 240, loss = 0.76, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:54s remains)
INFO - root - 2017-12-10 07:22:28.538746: step 250, loss = 0.76, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:07s remains)
INFO - root - 2017-12-10 07:22:30.726523: step 260, loss = 0.76, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:46m:37s remains)
INFO - root - 2017-12-10 07:22:32.910521: step 270, loss = 0.76, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 21h:22m:17s remains)
INFO - root - 2017-12-10 07:22:35.136501: step 280, loss = 0.76, batch loss = 0.69 (34.6 examples/sec; 0.231 sec/batch; 21h:21m:30s remains)
INFO - root - 2017-12-10 07:22:37.391483: step 290, loss = 0.76, batch loss = 0.69 (34.8 examples/sec; 0.230 sec/batch; 21h:11m:55s remains)
INFO - root - 2017-12-10 07:22:39.659701: step 300, loss = 0.76, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:13m:37s remains)
2017-12-10 07:22:40.027490: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013964708 0.01444888 0.015046756 0.015675711 0.016395889 0.017244231 0.01816714 0.018696077 0.018775497 0.018356957 0.017661739 0.017080033 0.016476333 0.016083125 0.015942594][0.015754838 0.016279304 0.016956527 0.017829608 0.018775687 0.019711843 0.020698693 0.021496648 0.021726917 0.02129248 0.020526791 0.019784886 0.018951466 0.018326752 0.01790984][0.016867941 0.017020004 0.017450102 0.018314606 0.019321965 0.020420646 0.0215802 0.022523517 0.022932274 0.022722052 0.022058785 0.021229137 0.020306809 0.01960903 0.019067656][0.018091336 0.017902156 0.01802922 0.018670868 0.019532641 0.020558124 0.021782121 0.022752702 0.023363516 0.023425758 0.023097431 0.022378916 0.021481331 0.020648368 0.020036262][0.018229298 0.017754545 0.017628709 0.017900357 0.018522877 0.019470265 0.020706851 0.021793161 0.022714982 0.02336281 0.023590077 0.023216287 0.022491448 0.021580154 0.020932078][0.017580852 0.016664956 0.016112471 0.01601598 0.01636914 0.017108018 0.01831007 0.01970325 0.021096768 0.022274848 0.023075569 0.023283616 0.022945819 0.022130346 0.021414861][0.016440198 0.015226507 0.014270798 0.013737769 0.013651247 0.014068728 0.015152583 0.016599163 0.018260745 0.020036047 0.021525273 0.022441767 0.022696357 0.022227636 0.021710888][0.014951615 0.013469806 0.012034807 0.010940455 0.01038418 0.010469225 0.011413702 0.012839 0.014694952 0.016813762 0.01881934 0.020466374 0.021464804 0.021521663 0.021431003][0.012855471 0.011278572 0.0095115667 0.00804368 0.0070880218 0.0068049468 0.0074427756 0.00865714 0.01047211 0.012679968 0.014958228 0.016991781 0.018511042 0.019282196 0.019809909][0.010221594 0.0085185925 0.0067530582 0.0052263308 0.0041124378 0.0036555831 0.0038897772 0.0047016391 0.0062361592 0.0083382493 0.010822218 0.013158358 0.015091868 0.016406003 0.017400658][0.0077284174 0.006070659 0.0045082513 0.0031836974 0.0022340494 0.00175907 0.0016955604 0.001992827 0.0029199386 0.0044445293 0.00654635 0.0087386882 0.010805113 0.012479359 0.013854238][0.0053597167 0.003967138 0.0027518705 0.0017873116 0.0011671461 0.00081988639 0.00064826064 0.00067696045 0.001131027 0.0019774509 0.0033907581 0.0051275739 0.006933487 0.0085471766 0.0099817868][0.003266382 0.0024964323 0.0018102581 0.0012417898 0.00083409651 0.00062600285 0.00046100569 0.00041391744 0.00050659775 0.00079258409 0.0014767322 0.0025026167 0.0037550416 0.0050624888 0.0064130477][0.0021375353 0.0019499876 0.0017427475 0.0015553379 0.0013490508 0.0011170352 0.00080822216 0.00063048291 0.00053523178 0.00052760518 0.00069157459 0.0011208284 0.001754821 0.0025287787 0.0035551311][0.0025290193 0.0027555595 0.0028978987 0.0029588009 0.0028491765 0.0025538448 0.0020582683 0.0015440267 0.0011143941 0.00085406471 0.00074156158 0.00080472004 0.0010115056 0.0013182027 0.0019529322]]...]
INFO - root - 2017-12-10 07:22:42.254319: step 310, loss = 0.76, batch loss = 0.69 (35.0 examples/sec; 0.228 sec/batch; 21h:03m:58s remains)
INFO - root - 2017-12-10 07:22:44.478253: step 320, loss = 0.76, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 21h:25m:32s remains)
INFO - root - 2017-12-10 07:22:46.738446: step 330, loss = 0.76, batch loss = 0.69 (34.8 examples/sec; 0.230 sec/batch; 21h:11m:45s remains)
INFO - root - 2017-12-10 07:22:48.907571: step 340, loss = 0.76, batch loss = 0.68 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:56s remains)
INFO - root - 2017-12-10 07:22:51.105942: step 350, loss = 0.76, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:16s remains)
INFO - root - 2017-12-10 07:22:53.357084: step 360, loss = 0.76, batch loss = 0.69 (33.5 examples/sec; 0.238 sec/batch; 22h:00m:02s remains)
INFO - root - 2017-12-10 07:22:55.624081: step 370, loss = 0.76, batch loss = 0.69 (33.8 examples/sec; 0.236 sec/batch; 21h:48m:52s remains)
INFO - root - 2017-12-10 07:22:57.822350: step 380, loss = 0.77, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:15s remains)
INFO - root - 2017-12-10 07:23:00.285749: step 390, loss = 0.77, batch loss = 0.69 (35.6 examples/sec; 0.224 sec/batch; 20h:42m:36s remains)
INFO - root - 2017-12-10 07:23:02.835251: step 400, loss = 0.77, batch loss = 0.69 (19.2 examples/sec; 0.417 sec/batch; 38h:25m:40s remains)
2017-12-10 07:23:04.030252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00021722121 -0.00024794275 -0.0002983805 -0.0003194852 -0.0003083531 -0.00026868656 -0.00020605023 -0.0001184307 5.1222742e-08 0.00014346628 0.00029433845 0.00041909819 0.0004992832 0.00052300305 0.00048409635][-0.0002950991 -0.00032947888 -0.00037270936 -0.00037420366 -0.00033361849 -0.00026420166 -0.00017709401 -7.0990645e-05 6.399327e-05 0.00022602919 0.00039519253 0.000534598 0.0006210485 0.00064459955 0.00059623341][-0.00032826385 -0.00035802752 -0.00038451911 -0.00036304398 -0.00029322028 -0.00019753177 -9.1265538e-05 2.62924e-05 0.0001674057 0.00033266284 0.00050415494 0.00064285682 0.00072674127 0.00074164104 0.0006798231][-0.00029843487 -0.00031363266 -0.00032158894 -0.00027880934 -0.00018619071 -7.4141659e-05 4.0522311e-05 0.00015861541 0.00029272679 0.00044757989 0.00060630287 0.00073249079 0.00080536446 0.00080773118 0.000734468][-0.00020241144 -0.00019174081 -0.00017678249 -0.00011574687 -1.264934e-05 0.00010106969 0.00020931615 0.0003142287 0.00042954087 0.000560045 0.00069137989 0.00079085515 0.00084007438 0.00082627637 0.00074405596][-5.4355478e-05 -2.6112306e-05 2.6384369e-06 6.53537e-05 0.00016072602 0.00026167836 0.00035394263 0.00044136937 0.0005350234 0.00063726725 0.00073698536 0.00080944318 0.00083670742 0.00080610253 0.00071711629][0.00011401391 0.0001569374 0.00019343593 0.00024982076 0.00032745674 0.00040441053 0.00047211 0.00053700572 0.00060607167 0.00068045431 0.0007510914 0.00079527823 0.00080126151 0.00075961067 0.00066754594][0.0002620765 0.00030823611 0.0003397218 0.00037613185 0.00042339158 0.00047025597 0.00051569333 0.00056390837 0.00061530177 0.00067075947 0.00072016357 0.00074555306 0.00073688338 0.00068500359 0.00059120683][0.00034990674 0.00039124256 0.00041148905 0.00042521185 0.00044210744 0.00046428363 0.00049070246 0.00052318443 0.00056153792 0.00060235756 0.00063676806 0.00065168762 0.00063763955 0.00058842963 0.00050495495][0.00037030037 0.00040519587 0.00041830144 0.00041849329 0.00041806907 0.00042380835 0.00043675094 0.00045660767 0.0004825627 0.000510965 0.00053369254 0.00054146466 0.00052255369 0.00047700084 0.00040546525][0.00033722701 0.00036411243 0.00037442124 0.00036922004 0.00035999902 0.00035531074 0.00035774149 0.00036704005 0.00038331468 0.00040034414 0.0004153871 0.00041604741 0.00039655296 0.00035763555 0.00029819296][0.00026915525 0.00028972514 0.00029683625 0.00028954796 0.00027824566 0.00026906817 0.00026485953 0.0002657373 0.00027166447 0.00028100889 0.00028941059 0.0002880781 0.00027396018 0.00024509267 0.00020156032][0.00018346752 0.00019659149 0.00020139152 0.00019571185 0.00018618023 0.00017683068 0.00016993308 0.00016627088 0.00016550743 0.00016799569 0.00016975845 0.00016974052 0.0001616464 0.00014555012 0.00011914084][0.00010407693 0.00011059386 0.00011274125 0.00011081761 0.00010592397 9.8556513e-05 9.3031675e-05 8.7518711e-05 8.3221123e-05 8.0815749e-05 7.9324236e-05 7.8609912e-05 7.516914e-05 6.7759538e-05 5.4363394e-05][4.6721427e-05 4.9148686e-05 4.959153e-05 4.8354035e-05 4.4304878e-05 3.787037e-05 3.1990465e-05 2.7930364e-05 2.349331e-05 1.8992811e-05 1.7301296e-05 1.8058461e-05 1.8753693e-05 1.6897451e-05 1.1844793e-05]]...]
INFO - root - 2017-12-10 07:23:06.711724: step 410, loss = 0.77, batch loss = 0.69 (18.1 examples/sec; 0.441 sec/batch; 40h:40m:17s remains)
INFO - root - 2017-12-10 07:23:11.077712: step 420, loss = 0.77, batch loss = 0.69 (18.5 examples/sec; 0.432 sec/batch; 39h:51m:58s remains)
INFO - root - 2017-12-10 07:23:15.487437: step 430, loss = 0.77, batch loss = 0.69 (19.0 examples/sec; 0.420 sec/batch; 38h:46m:48s remains)
INFO - root - 2017-12-10 07:23:19.913097: step 440, loss = 0.77, batch loss = 0.69 (18.0 examples/sec; 0.444 sec/batch; 40h:59m:00s remains)
INFO - root - 2017-12-10 07:23:24.457480: step 450, loss = 0.77, batch loss = 0.69 (17.7 examples/sec; 0.453 sec/batch; 41h:45m:17s remains)
INFO - root - 2017-12-10 07:23:29.003189: step 460, loss = 0.77, batch loss = 0.69 (17.4 examples/sec; 0.459 sec/batch; 42h:20m:37s remains)
INFO - root - 2017-12-10 07:23:33.379349: step 470, loss = 0.77, batch loss = 0.69 (17.1 examples/sec; 0.467 sec/batch; 43h:03m:23s remains)
INFO - root - 2017-12-10 07:23:36.131508: step 480, loss = 0.77, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:49s remains)
INFO - root - 2017-12-10 07:23:38.378018: step 490, loss = 0.77, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:10s remains)
INFO - root - 2017-12-10 07:23:40.578636: step 500, loss = 0.78, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:48s remains)
2017-12-10 07:23:40.959074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00013001263 -0.00015877758 -0.0001776414 -0.00018446124 -0.00017718691 -0.00018451212 -0.00017874653 -0.00018687558 -0.00019530649 -0.00022493652 -0.00025703025 -0.00033323059 -0.00043987297 -0.00054925773 -0.00070847245][8.2744285e-05 5.4279808e-05 3.4102006e-05 2.6553869e-05 2.7234433e-05 2.6603695e-05 1.9945204e-05 4.3804757e-06 -2.827344e-05 -7.9137622e-05 -0.00015331293 -0.00025217771 -0.00037090282 -0.00050431304 -0.00064964336][0.00017457944 0.00013056654 0.00012076576 0.00011115707 0.00011425349 0.00010085455 8.4595289e-05 5.2700983e-05 -7.65291e-06 -9.0728165e-05 -0.00019547937 -0.00031184859 -0.00044682971 -0.00058815256 -0.00072719145][0.00017869682 0.00015500165 0.00014858996 0.00012292992 0.00011479831 9.2157396e-05 6.1173225e-05 6.8042427e-06 -7.556146e-05 -0.00017597957 -0.00028983562 -0.00042017631 -0.00057581963 -0.00074055744 -0.00090712786][0.00017307862 0.00017413055 0.00015234202 0.0001186342 8.1408769e-05 4.463247e-05 1.2017787e-05 -5.4386561e-05 -0.00014689763 -0.00025954284 -0.00038992229 -0.00054051343 -0.00071236363 -0.000889575 -0.0010752145][0.00012197462 8.1415521e-05 2.9588817e-05 -1.6978942e-05 -6.1291386e-05 -0.00011630997 -0.00017458771 -0.00023985258 -0.00031471124 -0.00041844102 -0.00054016162 -0.00067384471 -0.0008198719 -0.000988192 -0.0011684886][1.8314458e-05 -5.9941434e-05 -0.00014826877 -0.00023208233 -0.00030716835 -0.0003732628 -0.00042866368 -0.000496782 -0.00056885881 -0.00064656849 -0.00073924079 -0.00084399595 -0.00095961476 -0.0010922265 -0.0012436837][-0.0001982986 -0.00027973368 -0.00039898988 -0.00050533586 -0.00060082157 -0.00066984177 -0.00073106552 -0.00078978203 -0.00084937213 -0.00090683543 -0.00096701633 -0.0010245699 -0.001097074 -0.0011909849 -0.00131058][-0.00047684961 -0.00060946343 -0.00074244686 -0.00085978268 -0.00096615963 -0.001041973 -0.0011074673 -0.0011466692 -0.001191831 -0.0012294105 -0.0012676546 -0.0012905247 -0.0013293904 -0.0013883999 -0.0014693572][-0.00091028283 -0.0010657893 -0.0011976839 -0.0013129597 -0.0014012067 -0.0014603303 -0.0015016331 -0.0015192702 -0.0015515736 -0.0015693594 -0.0015899085 -0.0015976601 -0.0016143217 -0.0016382497 -0.0016722442][-0.0012820523 -0.0014250153 -0.0015522507 -0.0016440841 -0.0017051935 -0.0017262166 -0.0017381015 -0.001746715 -0.0017704837 -0.0017833099 -0.0017907986 -0.0017975002 -0.0018013032 -0.0018123245 -0.0018276001][-0.0015798213 -0.0016795618 -0.0017510331 -0.0018034459 -0.0018404222 -0.0018516795 -0.0018714729 -0.0018724077 -0.0018953239 -0.0018963713 -0.0018983015 -0.0018919155 -0.0018837579 -0.0018969454 -0.001909708][-0.0017702588 -0.0018209671 -0.0018564687 -0.001882741 -0.0019056108 -0.0019077214 -0.0019268154 -0.0019178535 -0.0019252818 -0.0019144913 -0.0019080717 -0.001902119 -0.0019034763 -0.0019174337 -0.0019401824][-0.0018787882 -0.0018950177 -0.0018874838 -0.0018930861 -0.001905461 -0.0019017563 -0.0019225091 -0.0018983425 -0.0019016369 -0.0018942214 -0.0018909997 -0.0018820207 -0.0018902404 -0.0019110841 -0.0019391925][-0.0019341683 -0.0019342797 -0.001932299 -0.0019268345 -0.001912881 -0.0019019354 -0.0019005234 -0.0018837263 -0.0018687233 -0.001868705 -0.0018704019 -0.0018644504 -0.0018801145 -0.0019073865 -0.0019392492]]...]
INFO - root - 2017-12-10 07:23:43.164100: step 510, loss = 0.78, batch loss = 0.69 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:01s remains)
INFO - root - 2017-12-10 07:23:45.361016: step 520, loss = 0.78, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:51s remains)
INFO - root - 2017-12-10 07:23:47.570347: step 530, loss = 0.78, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:36s remains)
INFO - root - 2017-12-10 07:23:49.786161: step 540, loss = 0.78, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:45s remains)
INFO - root - 2017-12-10 07:23:52.002986: step 550, loss = 0.78, batch loss = 0.69 (34.8 examples/sec; 0.230 sec/batch; 21h:10m:01s remains)
INFO - root - 2017-12-10 07:23:54.231965: step 560, loss = 0.78, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:27m:53s remains)
INFO - root - 2017-12-10 07:23:56.407418: step 570, loss = 0.78, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:21m:49s remains)
INFO - root - 2017-12-10 07:23:58.624085: step 580, loss = 0.78, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:40s remains)
INFO - root - 2017-12-10 07:24:00.850305: step 590, loss = 0.78, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:56s remains)
INFO - root - 2017-12-10 07:24:03.037960: step 600, loss = 0.81, batch loss = 0.73 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:05s remains)
2017-12-10 07:24:03.461316: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.025732391 0.036638416 0.05232475 0.071343385 0.093874447 0.11922589 0.1442287 0.16371988 0.17426695 0.17306073 0.16154054 0.14223444 0.12038974 0.10324647 0.091854051][0.050720081 0.075852886 0.1083361 0.14376256 0.18023616 0.2141659 0.24019036 0.25267115 0.24881604 0.22888443 0.19626822 0.15619384 0.11435801 0.07966809 0.05499921][0.093091093 0.13596953 0.18806152 0.24314487 0.29562217 0.33926103 0.36671463 0.37439987 0.35952038 0.32489637 0.27439156 0.21503155 0.1546903 0.1020072 0.0621168][0.15483642 0.21742576 0.28793272 0.35883027 0.42300212 0.47339347 0.50250977 0.50729364 0.4852595 0.44061393 0.37725353 0.3027342 0.22640562 0.15614669 0.10012566][0.23475961 0.31229949 0.39584118 0.47608802 0.54582894 0.59882408 0.62787914 0.630109 0.6041559 0.555308 0.48534265 0.40095639 0.31103897 0.22499396 0.15230313][0.33248532 0.41913718 0.50808263 0.58910704 0.6563769 0.70590174 0.73154628 0.73134434 0.704057 0.65540612 0.58497334 0.49701971 0.39871266 0.29958022 0.21104325][0.44150883 0.52842087 0.61232615 0.684809 0.74200547 0.78287816 0.80247843 0.79886323 0.77149314 0.72631681 0.6600492 0.57441688 0.47348076 0.36669859 0.26609775][0.54747623 0.62824476 0.70021164 0.75786269 0.80000657 0.82894826 0.84105492 0.83429 0.80879468 0.76935607 0.71065557 0.6320188 0.53348196 0.42432693 0.31583509][0.63565487 0.70667881 0.7645573 0.80699784 0.83516687 0.85274965 0.857936 0.84913927 0.82642245 0.79283249 0.74133265 0.66893226 0.57324141 0.46381894 0.35011277][0.7013976 0.76287913 0.80868107 0.83881885 0.85747629 0.86667776 0.86737263 0.85748953 0.83681762 0.80677962 0.75955367 0.6900388 0.59523833 0.48441672 0.36699396][0.74111181 0.79363018 0.82941377 0.8510614 0.86375588 0.86879003 0.86830032 0.85816604 0.8384769 0.80827808 0.76090711 0.69069481 0.59502614 0.4834685 0.36560276][0.76907367 0.81583 0.84583777 0.863816 0.87491477 0.87789893 0.8764624 0.86477625 0.84289408 0.80743748 0.75445729 0.67921507 0.58063573 0.46858415 0.35279945][0.7846365 0.82803237 0.85611314 0.87397718 0.88436842 0.88889438 0.88805163 0.87508065 0.85031259 0.80942261 0.75260586 0.67324692 0.57476866 0.46610659 0.35609689][0.78538585 0.82378173 0.84874547 0.86564243 0.87711763 0.8861028 0.88950121 0.87986743 0.85865575 0.82103467 0.76885372 0.69357413 0.60344684 0.50476474 0.40469608][0.78023952 0.80987155 0.829796 0.84485978 0.85741889 0.86819738 0.87396634 0.87167859 0.858406 0.82967466 0.78826332 0.7276423 0.655599 0.57294476 0.48643479]]...]
INFO - root - 2017-12-10 07:24:05.649230: step 610, loss = 0.78, batch loss = 0.69 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:58s remains)
INFO - root - 2017-12-10 07:24:07.860628: step 620, loss = 0.78, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:11s remains)
INFO - root - 2017-12-10 07:24:10.078456: step 630, loss = 0.78, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:34s remains)
INFO - root - 2017-12-10 07:24:12.254314: step 640, loss = 0.78, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:21s remains)
INFO - root - 2017-12-10 07:24:14.481764: step 650, loss = 0.78, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:56m:09s remains)
INFO - root - 2017-12-10 07:24:16.662220: step 660, loss = 0.78, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:31s remains)
INFO - root - 2017-12-10 07:24:18.871770: step 670, loss = 0.78, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:17m:41s remains)
INFO - root - 2017-12-10 07:24:21.109189: step 680, loss = 0.79, batch loss = 0.69 (34.6 examples/sec; 0.231 sec/batch; 21h:17m:18s remains)
INFO - root - 2017-12-10 07:24:23.319744: step 690, loss = 0.79, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:37s remains)
INFO - root - 2017-12-10 07:24:25.546703: step 700, loss = 0.79, batch loss = 0.69 (34.7 examples/sec; 0.231 sec/batch; 21h:14m:58s remains)
2017-12-10 07:24:25.898174: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04295015 0.043001313 0.043000489 0.04301798 0.04308331 0.04311784 0.043186042 0.043221731 0.043243583 0.043240193 0.043264288 0.043345489 0.043374907 0.04336898 0.043342903][0.043277651 0.043295152 0.043249942 0.043208648 0.043244544 0.043260291 0.043330565 0.043391153 0.043434493 0.043471742 0.043526556 0.04363298 0.043669939 0.043691263 0.043651983][0.043439172 0.043400757 0.0432747 0.043194514 0.043190498 0.043186378 0.043218296 0.043252558 0.043326858 0.043357953 0.04339955 0.043506425 0.043584682 0.043640856 0.043636631][0.043579351 0.043511383 0.04335361 0.043207109 0.04313447 0.043111205 0.043129135 0.043157343 0.043243974 0.043307982 0.043392021 0.043509275 0.043599397 0.043660019 0.043645002][0.043667946 0.043584175 0.043417096 0.043276161 0.043192443 0.043155793 0.043158684 0.0431672 0.043248005 0.043321885 0.043401025 0.04353331 0.043642897 0.043734338 0.043717232][0.043842278 0.043810215 0.043672785 0.043564186 0.043476723 0.043387886 0.043318793 0.043238122 0.043206666 0.043237831 0.043310545 0.043456834 0.043599747 0.043734811 0.04378818][0.043851506 0.043901414 0.043816131 0.043736048 0.043624226 0.043450143 0.043234203 0.042994738 0.042839896 0.042809296 0.042888205 0.04309243 0.043342356 0.043593124 0.043741621][0.0438712 0.044013217 0.043963805 0.043876167 0.043669112 0.043340348 0.042927269 0.042501036 0.042220835 0.042155959 0.042309452 0.042651527 0.043066509 0.04347609 0.043753371][0.043905359 0.04413965 0.044110984 0.043959711 0.0435943 0.043023337 0.042327575 0.041652329 0.04121083 0.04111984 0.041384056 0.04192974 0.042580262 0.043211725 0.043666791][0.043887161 0.044175737 0.044142969 0.04389758 0.04332966 0.042455398 0.0414116 0.04043537 0.039797854 0.039664879 0.040074416 0.040889379 0.041862022 0.042792562 0.043489069][0.043792833 0.044110868 0.044050775 0.043690268 0.042897362 0.041692946 0.040271148 0.038973108 0.038132023 0.03796431 0.038533561 0.039662093 0.041018818 0.042305525 0.043284189][0.043574713 0.043882027 0.043764092 0.043283667 0.042267036 0.040753324 0.038967498 0.037343096 0.036302291 0.036108904 0.036857594 0.038322803 0.040098619 0.041780155 0.043066926][0.043341145 0.043643668 0.043488048 0.042920586 0.041722532 0.039903503 0.037765957 0.035806015 0.034518387 0.034239974 0.03510255 0.036886297 0.039070778 0.041157212 0.042773467][0.043094043 0.043403938 0.043249093 0.0426535 0.041351378 0.03930375 0.036832124 0.034504294 0.032897439 0.032434385 0.033291183 0.03527743 0.037817925 0.040300153 0.042265024][0.042796277 0.043131348 0.043005228 0.042433627 0.041111838 0.038931895 0.036196489 0.033515975 0.031553622 0.030832844 0.031577758 0.033660207 0.036484305 0.039341196 0.041670248]]...]
INFO - root - 2017-12-10 07:24:28.104295: step 710, loss = 0.79, batch loss = 0.69 (35.8 examples/sec; 0.223 sec/batch; 20h:34m:28s remains)
INFO - root - 2017-12-10 07:24:30.332731: step 720, loss = 0.79, batch loss = 0.70 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:43s remains)
INFO - root - 2017-12-10 07:24:32.542901: step 730, loss = 0.79, batch loss = 0.70 (34.9 examples/sec; 0.229 sec/batch; 21h:06m:02s remains)
INFO - root - 2017-12-10 07:24:34.733958: step 740, loss = 0.80, batch loss = 0.70 (35.3 examples/sec; 0.227 sec/batch; 20h:53m:39s remains)
INFO - root - 2017-12-10 07:24:36.939969: step 750, loss = 0.80, batch loss = 0.71 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:51s remains)
INFO - root - 2017-12-10 07:24:39.128253: step 760, loss = 0.80, batch loss = 0.70 (35.8 examples/sec; 0.223 sec/batch; 20h:35m:31s remains)
INFO - root - 2017-12-10 07:24:41.372414: step 770, loss = 0.81, batch loss = 0.71 (37.8 examples/sec; 0.212 sec/batch; 19h:29m:25s remains)
INFO - root - 2017-12-10 07:24:43.591052: step 780, loss = 0.79, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:38s remains)
INFO - root - 2017-12-10 07:24:45.813011: step 790, loss = 0.79, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:50s remains)
INFO - root - 2017-12-10 07:24:48.031380: step 800, loss = 0.79, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:32s remains)
2017-12-10 07:24:48.390057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0035873125 -0.0035873125 -0.0035873125 -0.0035873125 -0.0035873109 -0.0035873011 -0.0035872762 -0.0035872343 -0.0035871803 -0.003587123 -0.0035870643 -0.0035870136 -0.0035869777 -0.0035869584 -0.0035869509][-0.0035873128 -0.0035873128 -0.0035873128 -0.0035873128 -0.0035873118 -0.0035873053 -0.0035872892 -0.0035872611 -0.0035872245 -0.0035871847 -0.0035871458 -0.0035871116 -0.0035870876 -0.0035870746 -0.0035870692][-0.003587313 -0.003587313 -0.003587313 -0.0035873128 -0.0035873125 -0.0035873097 -0.0035873021 -0.0035872879 -0.0035872683 -0.0035872466 -0.0035872252 -0.0035872066 -0.0035871931 -0.0035871856 -0.0035871824][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873128 -0.0035873121 -0.0035873095 -0.0035873044 -0.0035872969 -0.0035872883 -0.00358728 -0.0035872723 -0.0035872669 -0.0035872636 -0.0035872625][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873128 -0.0035873123 -0.0035873114 -0.0035873097 -0.0035873079 -0.003587306 -0.0035873044 -0.0035873032 -0.0035873023 -0.0035873021][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873128 -0.0035873128 -0.0035873128 -0.0035873128 -0.0035873128 -0.0035873128 -0.0035873125 -0.0035873123][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873128 -0.0035873128 -0.0035873128 -0.003587313 -0.0035873128 -0.0035873128 -0.0035873125][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873128 -0.0035873125][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873128 -0.0035873128][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873121 -0.0035873118 -0.0035873125 -0.0035873123][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873118 -0.0035873118 -0.0035873125 -0.0035873123][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873118 -0.0035873118 -0.0035873125 -0.0035873125][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873123 -0.0035873121 -0.0035873121 -0.0035873125 -0.0035873125][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873125 -0.0035873125 -0.0035873125 -0.0035873125][-0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.003587313 -0.0035873128 -0.0035873128 -0.0035873128 -0.0035873128 -0.0035873128 -0.0035873125 -0.0035873125 -0.0035873114 -0.0035873114]]...]
INFO - root - 2017-12-10 07:24:50.567411: step 810, loss = 0.79, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:27m:15s remains)
INFO - root - 2017-12-10 07:24:52.738629: step 820, loss = 0.79, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:19s remains)
INFO - root - 2017-12-10 07:24:54.966638: step 830, loss = 0.79, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:08s remains)
INFO - root - 2017-12-10 07:24:57.172599: step 840, loss = 0.79, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:08m:11s remains)
INFO - root - 2017-12-10 07:24:59.368218: step 850, loss = 0.79, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:10s remains)
INFO - root - 2017-12-10 07:25:01.576204: step 860, loss = 0.79, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:27m:29s remains)
INFO - root - 2017-12-10 07:25:03.780902: step 870, loss = 0.79, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:33s remains)
INFO - root - 2017-12-10 07:25:05.994871: step 880, loss = 0.79, batch loss = 0.69 (36.1 examples/sec; 0.221 sec/batch; 20h:23m:28s remains)
INFO - root - 2017-12-10 07:25:08.220941: step 890, loss = 0.79, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:09m:49s remains)
INFO - root - 2017-12-10 07:25:10.401894: step 900, loss = 0.79, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:46s remains)
2017-12-10 07:25:10.789154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0024813847 -0.0027794433 -0.0028978349 -0.0028547782 -0.0028402398 -0.002914242 -0.0030550389 -0.0032284709 -0.0033839897 -0.0034764176 -0.003481064 -0.0034411261 -0.0033936393 -0.0033665919 -0.003377181][-0.001169326 -0.0014973688 -0.0016380348 -0.0015802085 -0.0016072565 -0.0018194641 -0.0021784371 -0.0025860022 -0.0029270682 -0.0030954247 -0.0030560677 -0.0028791321 -0.0026907094 -0.0025863147 -0.0026169412][0.00053529162 0.00019956823 9.7386073e-06 3.2480573e-05 -8.5195992e-05 -0.00046328083 -0.001047407 -0.0016609197 -0.0021432827 -0.0023461042 -0.0021958831 -0.0017863672 -0.0013398686 -0.0010835477 -0.0011456432][0.0019900063 0.0016841847 0.0014381339 0.0014126734 0.0012192265 0.00074669439 5.3380383e-05 -0.00065182056 -0.0012011046 -0.0014123702 -0.0011667542 -0.00052509876 0.00018274598 0.00060940813 0.00055106869][0.0029824111 0.0027873758 0.0025540818 0.0025049979 0.0022862814 0.0018147179 0.0011454835 0.00046718586 -7.2567491e-05 -0.00025498029 0.00010879384 0.00095831556 0.0018495345 0.0023397454 0.0022368054][0.0037513659 0.0037023323 0.0035285386 0.0034864936 0.0032974132 0.0029061735 0.002349732 0.0017545936 0.0012402548 0.0010808101 0.0015380452 0.0025332486 0.0035216464 0.0039939191 0.00380072][0.0043121655 0.0044589085 0.0043553854 0.004321964 0.0041766125 0.0039048335 0.0035205269 0.0030711829 0.0026388303 0.0025336798 0.0030532768 0.004103459 0.0050992495 0.0055044112 0.0052295965][0.0046863765 0.0049765795 0.0049402541 0.0049104881 0.0048115957 0.0046498724 0.004426097 0.0041409349 0.0038561798 0.0038768146 0.0044715549 0.0055206777 0.0064494447 0.0067637884 0.0064470829][0.0049816989 0.0053399834 0.0053444761 0.0053369473 0.0053019379 0.0052176542 0.0051092231 0.0049533709 0.0047791121 0.0048866388 0.0054757348 0.0064108758 0.0071742586 0.0073898616 0.0071081254][0.00532807 0.0057712425 0.0058027203 0.0058096168 0.0058159544 0.0057703438 0.0057292958 0.0056405379 0.0055465526 0.0056950464 0.0061824084 0.0069111395 0.0074572335 0.00759864 0.0074053197][0.0058122636 0.0063035167 0.0063163419 0.0063148779 0.0063084927 0.0062603136 0.0062343408 0.0061974186 0.0061876024 0.0063421191 0.0066911173 0.0071707559 0.0074983309 0.0075862654 0.0074912175][0.0064502764 0.006977418 0.0069227763 0.00686573 0.006808517 0.0067126141 0.0066630966 0.0066289618 0.0066250325 0.0067177522 0.0069028507 0.0071634105 0.0073424452 0.007435353 0.0074450742][0.0071469764 0.0077128527 0.0076260394 0.0074899732 0.0073423921 0.0071643735 0.0070467847 0.0069587831 0.0069098878 0.0069230688 0.00696364 0.0070595606 0.0071533215 0.0072756871 0.0073734834][0.0076038255 0.0083137751 0.0083036218 0.0081797726 0.0079804864 0.0077212839 0.0075028394 0.0073340773 0.0072028986 0.0070939385 0.0069973371 0.0069673345 0.0069906567 0.0071034771 0.0072333352][0.0076187416 0.0084669478 0.0085772257 0.00853283 0.0083638448 0.0081122871 0.0078722481 0.00767484 0.0075103934 0.0073646237 0.007213511 0.0071003125 0.0070396005 0.0070820549 0.0071636443]]...]
INFO - root - 2017-12-10 07:25:12.949790: step 910, loss = 0.80, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:22s remains)
INFO - root - 2017-12-10 07:25:15.140626: step 920, loss = 0.78, batch loss = 0.68 (36.8 examples/sec; 0.218 sec/batch; 20h:02m:13s remains)
INFO - root - 2017-12-10 07:25:17.339291: step 930, loss = 0.80, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:54m:45s remains)
INFO - root - 2017-12-10 07:25:19.540075: step 940, loss = 0.80, batch loss = 0.69 (36.1 examples/sec; 0.222 sec/batch; 20h:25m:34s remains)
INFO - root - 2017-12-10 07:25:21.773432: step 950, loss = 0.80, batch loss = 0.69 (32.6 examples/sec; 0.246 sec/batch; 22h:37m:02s remains)
INFO - root - 2017-12-10 07:25:23.989899: step 960, loss = 0.80, batch loss = 0.69 (35.0 examples/sec; 0.228 sec/batch; 21h:01m:33s remains)
INFO - root - 2017-12-10 07:25:26.163666: step 970, loss = 0.80, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:48m:19s remains)
INFO - root - 2017-12-10 07:25:28.433142: step 980, loss = 0.80, batch loss = 0.69 (33.8 examples/sec; 0.237 sec/batch; 21h:48m:11s remains)
INFO - root - 2017-12-10 07:25:30.621896: step 990, loss = 0.80, batch loss = 0.69 (34.9 examples/sec; 0.229 sec/batch; 21h:04m:51s remains)
INFO - root - 2017-12-10 07:25:32.844280: step 1000, loss = 0.80, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:27m:04s remains)
2017-12-10 07:25:33.220342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383][-0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383 -0.0043914383]]...]
