INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "138"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 08:28:24.930036: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 08:28:24.930075: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 08:28:24.930082: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 08:28:24.930087: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 08:28:24.930091: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 08:28:28.529910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 6.38GiB
2017-12-07 08:28:28.530009: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x55b3fa2d7440
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 256, 6, 6), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 256, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 6, 6), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 256, 20, 20), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 256, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 20, 20), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-07 08:28:32.107727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 6.38GiB
2017-12-07 08:28:32.107785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 1
2017-12-07 08:28:32.107799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 0
2017-12-07 08:28:32.107816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 
2017-12-07 08:28:32.107829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y N 
2017-12-07 08:28:32.107834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   N Y 
2017-12-07 08:28:32.107842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
2017-12-07 08:28:32.107848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 08:29:02.000001: step 0, loss = 20.87, batch loss = 20.79 (0.4 examples/sec; 20.849 sec/batch; 1925h:37m:37s remains)
2017-12-07 08:29:03.110181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3620887 -4.4026985 -4.4816918 -4.5808592 -4.6600833 -4.6872973 -4.6577134 -4.5981765 -4.5563893 -4.5540619 -4.58536 -4.6198921 -4.6211405 -4.5773253 -4.5021148][-4.4088826 -4.4782 -4.5880351 -4.7083969 -4.7871513 -4.7891426 -4.7305412 -4.6669087 -4.6381803 -4.6495471 -4.6876979 -4.7257395 -4.7250319 -4.6675711 -4.5690656][-4.4484305 -4.5366869 -4.663177 -4.7837596 -4.834013 -4.782114 -4.6786213 -4.6129332 -4.61203 -4.6486716 -4.7030854 -4.7541065 -4.764564 -4.7121253 -4.6083078][-4.4740477 -4.5646772 -4.6934938 -4.7929945 -4.7844772 -4.6449223 -4.466054 -4.3739758 -4.3889632 -4.4624386 -4.5645814 -4.664094 -4.7205167 -4.6991429 -4.6080389][-4.471272 -4.5494814 -4.6708107 -4.7364049 -4.6542368 -4.4144673 -4.1462045 -4.0047193 -4.0159659 -4.1357961 -4.3159213 -4.495872 -4.6238937 -4.6466203 -4.5775571][-4.441916 -4.5041485 -4.618269 -4.65367 -4.5125403 -4.199059 -3.85927 -3.6576777 -3.6492553 -3.8179245 -4.0851979 -4.3507266 -4.543272 -4.5987434 -4.5459261][-4.403089 -4.4536324 -4.5647025 -4.5824842 -4.412703 -4.0651536 -3.6841793 -3.4337649 -3.4159117 -3.6358981 -3.9740231 -4.2976618 -4.518827 -4.5812712 -4.5317378][-4.3651958 -4.4114714 -4.5165391 -4.5304337 -4.3713503 -4.0421133 -3.6648645 -3.4051347 -3.4112377 -3.6793795 -4.0442219 -4.3587613 -4.54586 -4.580802 -4.5255103][-4.360075 -4.4031568 -4.4936433 -4.5147419 -4.4029341 -4.1416554 -3.8166087 -3.5928144 -3.6353264 -3.9142339 -4.2405996 -4.4750609 -4.5798497 -4.5725093 -4.5139771][-4.4071436 -4.4441113 -4.51296 -4.5417857 -4.4858708 -4.3073888 -4.0511575 -3.8688455 -3.9226928 -4.1687341 -4.4205337 -4.5599852 -4.5895743 -4.554975 -4.5024157][-4.4628558 -4.4946938 -4.5426512 -4.5763187 -4.5582151 -4.4361935 -4.2317595 -4.0830235 -4.1309605 -4.3262854 -4.5052872 -4.5791607 -4.57471 -4.540854 -4.50122][-4.4662619 -4.4962983 -4.539588 -4.5841413 -4.5875382 -4.4981976 -4.3330956 -4.2172356 -4.2552943 -4.3973374 -4.5169206 -4.5552988 -4.5489564 -4.5319624 -4.5061674][-4.401906 -4.4313059 -4.4871383 -4.5548558 -4.5814018 -4.5218205 -4.393187 -4.3023777 -4.3214321 -4.4111109 -4.4860506 -4.5095658 -4.51596 -4.5192146 -4.5067668][-4.3011236 -4.33033 -4.4035854 -4.492188 -4.5372915 -4.5022435 -4.3983192 -4.3115935 -4.3002553 -4.3455009 -4.3969107 -4.4291787 -4.4618745 -4.4907732 -4.4952865][-4.2182679 -4.2438464 -4.3281226 -4.4253244 -4.4724441 -4.43846 -4.3304362 -4.2257266 -4.1870785 -4.2139239 -4.271492 -4.3328948 -4.3992138 -4.45339 -4.4732251]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 08:29:11.904330: step 10, loss = 21.27, batch loss = 21.19 (10.8 examples/sec; 0.737 sec/batch; 68h:06m:18s remains)
INFO - root - 2017-12-07 08:29:19.112207: step 20, loss = 21.81, batch loss = 21.73 (11.4 examples/sec; 0.703 sec/batch; 64h:54m:00s remains)
INFO - root - 2017-12-07 08:29:26.440173: step 30, loss = 21.25, batch loss = 21.16 (10.7 examples/sec; 0.747 sec/batch; 69h:00m:46s remains)
INFO - root - 2017-12-07 08:29:33.793737: step 40, loss = 21.53, batch loss = 21.45 (10.6 examples/sec; 0.752 sec/batch; 69h:27m:40s remains)
INFO - root - 2017-12-07 08:29:41.174932: step 50, loss = 21.53, batch loss = 21.45 (10.5 examples/sec; 0.763 sec/batch; 70h:28m:27s remains)
INFO - root - 2017-12-07 08:29:48.489752: step 60, loss = 21.50, batch loss = 21.42 (11.4 examples/sec; 0.700 sec/batch; 64h:40m:43s remains)
INFO - root - 2017-12-07 08:29:55.831835: step 70, loss = 21.47, batch loss = 21.39 (10.8 examples/sec; 0.740 sec/batch; 68h:21m:39s remains)
INFO - root - 2017-12-07 08:30:03.187962: step 80, loss = 21.42, batch loss = 21.34 (10.2 examples/sec; 0.782 sec/batch; 72h:14m:21s remains)
INFO - root - 2017-12-07 08:30:10.511789: step 90, loss = 21.58, batch loss = 21.50 (10.9 examples/sec; 0.734 sec/batch; 67h:45m:57s remains)
INFO - root - 2017-12-07 08:30:17.886217: step 100, loss = 21.58, batch loss = 21.50 (10.6 examples/sec; 0.753 sec/batch; 69h:32m:05s remains)
2017-12-07 08:30:18.638267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3934731 -4.3685446 -4.2887135 -4.1919351 -4.1166792 -4.0867066 -4.1036139 -4.1601357 -4.2553234 -4.3545022 -4.428185 -4.47533 -4.4893084 -4.4464512 -4.3776131][-4.3779144 -4.3567171 -4.2799845 -4.1800022 -4.0990195 -4.0574064 -4.064846 -4.1201606 -4.2152271 -4.3146887 -4.386651 -4.4264631 -4.4392385 -4.4102926 -4.3606381][-4.3739257 -4.3661213 -4.3058443 -4.2165337 -4.134799 -4.0753083 -4.05743 -4.0926962 -4.1765876 -4.2734528 -4.339993 -4.3674364 -4.3711085 -4.3475618 -4.3085179][-4.3779621 -4.3839669 -4.3417635 -4.266818 -4.1864505 -4.1103749 -4.0646372 -4.0759869 -4.1464128 -4.2390118 -4.2979379 -4.3116693 -4.3041162 -4.2803855 -4.2424574][-4.3851438 -4.3973432 -4.3629522 -4.2902169 -4.2015061 -4.1078086 -4.0421681 -4.03879 -4.1004791 -4.1901836 -4.24752 -4.2567577 -4.2472863 -4.2285609 -4.1919947][-4.3922944 -4.403595 -4.3654828 -4.2797222 -4.1701107 -4.0565557 -3.9794078 -3.9698768 -4.0275855 -4.1196542 -4.1871095 -4.2069321 -4.2091618 -4.2040586 -4.1759224][-4.3936343 -4.4000053 -4.3504887 -4.2441344 -4.1099634 -3.9801939 -3.8991508 -3.8872404 -3.9415128 -4.04053 -4.1294169 -4.1727495 -4.1945481 -4.2030973 -4.1867785][-4.3886361 -4.3925333 -4.3360848 -4.2184644 -4.07242 -3.9380071 -3.8557959 -3.8355315 -3.8795621 -3.9822676 -4.0935826 -4.1660666 -4.2091031 -4.2242708 -4.210855][-4.3835874 -4.3893003 -4.3385911 -4.2293792 -4.0909019 -3.9629183 -3.8791604 -3.8453822 -3.8727407 -3.9674239 -4.0869465 -4.1811681 -4.2408404 -4.2536492 -4.2308145][-4.3835654 -4.3969922 -4.3626542 -4.2788949 -4.1680617 -4.0608888 -3.980571 -3.9311614 -3.9335382 -4.0038877 -4.1115031 -4.2126603 -4.2841582 -4.2962031 -4.2649279][-4.3871245 -4.4105153 -4.3968849 -4.3456206 -4.2726483 -4.1964684 -4.1264906 -4.067482 -4.0473289 -4.0883 -4.1728649 -4.2674022 -4.3403468 -4.3531909 -4.3199716][-4.3843637 -4.41184 -4.4137578 -4.3884745 -4.3489423 -4.3074512 -4.2620225 -4.2146826 -4.1910553 -4.2138505 -4.2747078 -4.3487229 -4.404346 -4.4103656 -4.3776073][-4.3721576 -4.398633 -4.40895 -4.4010744 -4.38468 -4.3718848 -4.3568983 -4.3362184 -4.3241158 -4.3388734 -4.3780112 -4.4239659 -4.4510808 -4.4424076 -4.4097366][-4.3545475 -4.3772326 -4.3912239 -4.3941264 -4.3898811 -4.3896313 -4.3920236 -4.3927536 -4.3948526 -4.407577 -4.430131 -4.4515309 -4.4552073 -4.4365845 -4.4067364][-4.3367739 -4.3537588 -4.3669515 -4.3741179 -4.3752871 -4.3776259 -4.3840566 -4.3925104 -4.4000559 -4.4082375 -4.4163632 -4.4203367 -4.4138112 -4.3975019 -4.3773966]]...]
INFO - root - 2017-12-07 08:30:25.988260: step 110, loss = 21.51, batch loss = 21.43 (10.4 examples/sec; 0.766 sec/batch; 70h:44m:07s remains)
INFO - root - 2017-12-07 08:30:33.416016: step 120, loss = 21.08, batch loss = 20.99 (10.0 examples/sec; 0.800 sec/batch; 73h:53m:35s remains)
INFO - root - 2017-12-07 08:30:40.717448: step 130, loss = 21.29, batch loss = 21.21 (10.4 examples/sec; 0.769 sec/batch; 71h:00m:19s remains)
INFO - root - 2017-12-07 08:30:47.952700: step 140, loss = 21.77, batch loss = 21.68 (10.6 examples/sec; 0.757 sec/batch; 69h:55m:04s remains)
INFO - root - 2017-12-07 08:30:55.056214: step 150, loss = 21.72, batch loss = 21.64 (10.2 examples/sec; 0.785 sec/batch; 72h:27m:47s remains)
INFO - root - 2017-12-07 08:31:02.381217: step 160, loss = 21.31, batch loss = 21.22 (10.0 examples/sec; 0.798 sec/batch; 73h:40m:19s remains)
INFO - root - 2017-12-07 08:31:09.720743: step 170, loss = 21.70, batch loss = 21.62 (10.5 examples/sec; 0.764 sec/batch; 70h:33m:47s remains)
INFO - root - 2017-12-07 08:31:17.063190: step 180, loss = 21.34, batch loss = 21.25 (10.4 examples/sec; 0.767 sec/batch; 70h:47m:39s remains)
INFO - root - 2017-12-07 08:31:24.460077: step 190, loss = 21.48, batch loss = 21.40 (10.2 examples/sec; 0.782 sec/batch; 72h:13m:18s remains)
INFO - root - 2017-12-07 08:31:31.723294: step 200, loss = 21.43, batch loss = 21.35 (10.9 examples/sec; 0.735 sec/batch; 67h:50m:33s remains)
2017-12-07 08:31:32.521920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6282353 -4.671639 -4.6525955 -4.5935621 -4.5244651 -4.4886532 -4.4594507 -4.4191861 -4.4099264 -4.4032941 -4.37546 -4.3470058 -4.3536835 -4.4002 -4.43679][-4.597476 -4.6334567 -4.6180987 -4.5618539 -4.4731126 -4.4057469 -4.346643 -4.2896028 -4.3016138 -4.3425293 -4.3421311 -4.3003902 -4.2754931 -4.3092418 -4.3607316][-4.4718409 -4.4869118 -4.484694 -4.457727 -4.3812814 -4.2947121 -4.2078004 -4.1360941 -4.1731248 -4.2758875 -4.3235364 -4.2798634 -4.2113419 -4.1997042 -4.2361584][-4.3206968 -4.3163004 -4.3282347 -4.3405271 -4.2965026 -4.2066092 -4.0965738 -4.0095196 -4.0588574 -4.208858 -4.3015332 -4.2684946 -4.1719303 -4.1134243 -4.12297][-4.2519388 -4.2330546 -4.2422948 -4.2616692 -4.22763 -4.1336846 -4.0087533 -3.9032001 -3.9404471 -4.1096439 -4.2381821 -4.235096 -4.1446719 -4.0687718 -4.0732894][-4.2740688 -4.2326431 -4.2197547 -4.2175021 -4.1697311 -4.0667529 -3.924685 -3.7909267 -3.7981234 -3.9686754 -4.1279469 -4.1720653 -4.1166868 -4.050283 -4.0670023][-4.3358779 -4.2787561 -4.2325215 -4.1969357 -4.1300287 -4.0256586 -3.8854368 -3.7338283 -3.7060404 -3.8548121 -4.022747 -4.1078029 -4.0923777 -4.0436225 -4.0636134][-4.3743625 -4.3150434 -4.2384834 -4.1744461 -4.1039109 -4.0313468 -3.9411666 -3.8206034 -3.7841949 -3.8812127 -4.0117116 -4.1034751 -4.1117549 -4.0774794 -4.0836911][-4.3899946 -4.3369336 -4.2454877 -4.168643 -4.1050949 -4.0766535 -4.049675 -3.9757984 -3.9547868 -4.0187125 -4.1066074 -4.17262 -4.162972 -4.1233706 -4.1139655][-4.460887 -4.4153233 -4.3192348 -4.2385206 -4.1828327 -4.1903009 -4.2052259 -4.1569543 -4.14984 -4.1975513 -4.2604957 -4.2991676 -4.2582479 -4.2025089 -4.1820936][-4.5803323 -4.5281467 -4.425745 -4.3452115 -4.2988186 -4.3302355 -4.3657265 -4.3295507 -4.3258524 -4.357975 -4.4003925 -4.419745 -4.3638549 -4.306006 -4.28797][-4.6823654 -4.6333656 -4.5315013 -4.4481058 -4.405664 -4.4460478 -4.4868474 -4.458353 -4.4547815 -4.4693375 -4.4893985 -4.4972973 -4.4482212 -4.4031405 -4.3920755][-4.7038436 -4.683135 -4.6118383 -4.5459247 -4.5082488 -4.53939 -4.5692372 -4.5447278 -4.5379572 -4.5359969 -4.5381675 -4.5408 -4.5095468 -4.4792647 -4.4694319][-4.6696353 -4.6802864 -4.6505432 -4.611259 -4.58049 -4.5955639 -4.6148081 -4.5987134 -4.5907598 -4.5799389 -4.5712624 -4.571434 -4.5582805 -4.5412693 -4.5339675][-4.5993524 -4.6148438 -4.6069689 -4.59167 -4.5782709 -4.5913625 -4.6099362 -4.6082592 -4.6046829 -4.594728 -4.5839405 -4.5796428 -4.5719733 -4.5590873 -4.5478191]]...]
INFO - root - 2017-12-07 08:31:39.721164: step 210, loss = 21.66, batch loss = 21.58 (11.5 examples/sec; 0.698 sec/batch; 64h:23m:14s remains)
INFO - root - 2017-12-07 08:31:47.039998: step 220, loss = 21.62, batch loss = 21.53 (10.8 examples/sec; 0.742 sec/batch; 68h:29m:20s remains)
INFO - root - 2017-12-07 08:31:54.353777: step 230, loss = 21.42, batch loss = 21.34 (11.7 examples/sec; 0.684 sec/batch; 63h:09m:01s remains)
INFO - root - 2017-12-07 08:32:01.656551: step 240, loss = 21.67, batch loss = 21.59 (11.6 examples/sec; 0.690 sec/batch; 63h:40m:20s remains)
INFO - root - 2017-12-07 08:32:08.988050: step 250, loss = 21.37, batch loss = 21.29 (11.3 examples/sec; 0.706 sec/batch; 65h:08m:41s remains)
INFO - root - 2017-12-07 08:32:16.325393: step 260, loss = 21.59, batch loss = 21.51 (11.2 examples/sec; 0.715 sec/batch; 66h:00m:47s remains)
INFO - root - 2017-12-07 08:32:23.484608: step 270, loss = 21.75, batch loss = 21.67 (11.4 examples/sec; 0.701 sec/batch; 64h:39m:07s remains)
INFO - root - 2017-12-07 08:32:30.792428: step 280, loss = 21.38, batch loss = 21.29 (11.7 examples/sec; 0.681 sec/batch; 62h:51m:05s remains)
INFO - root - 2017-12-07 08:32:38.227310: step 290, loss = 21.28, batch loss = 21.19 (11.1 examples/sec; 0.722 sec/batch; 66h:37m:03s remains)
INFO - root - 2017-12-07 08:32:45.566876: step 300, loss = 21.68, batch loss = 21.60 (10.6 examples/sec; 0.756 sec/batch; 69h:47m:20s remains)
2017-12-07 08:32:46.400232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1175375 -4.1275864 -4.1475573 -4.1826572 -4.212574 -4.2105432 -4.169004 -4.1010442 -4.0280671 -3.9939325 -4.021595 -4.107224 -4.1912708 -4.2315855 -4.23963][-4.245491 -4.2471457 -4.2362766 -4.2399273 -4.2579188 -4.2633209 -4.238512 -4.1870189 -4.1283436 -4.1055746 -4.1332207 -4.2009845 -4.260397 -4.2793741 -4.2703314][-4.3822603 -4.3604794 -4.3036532 -4.262157 -4.2622876 -4.2824736 -4.2933211 -4.2802091 -4.24851 -4.23451 -4.2525558 -4.293592 -4.3276644 -4.3273931 -4.2986445][-4.4668908 -4.4100885 -4.30121 -4.2030635 -4.1772251 -4.20981 -4.2570925 -4.2834597 -4.2772188 -4.2677808 -4.274663 -4.3029547 -4.3340311 -4.335175 -4.2995415][-4.45877 -4.3723121 -4.226326 -4.081346 -4.0245543 -4.0566549 -4.1224136 -4.17205 -4.1802731 -4.1675229 -4.1700864 -4.2125297 -4.2756042 -4.3054209 -4.2785015][-4.4085183 -4.3003845 -4.1338573 -3.9680774 -3.8895783 -3.9107332 -3.9707408 -4.0182147 -4.016046 -3.9815779 -3.9829502 -4.0619054 -4.1817555 -4.25513 -4.2496972][-4.3654089 -4.2288165 -4.0476646 -3.8880467 -3.8108525 -3.8250866 -3.8744113 -3.9065592 -3.8693287 -3.7913291 -3.7841637 -3.8923593 -4.0543818 -4.1667562 -4.1945853][-4.3442206 -4.1844029 -3.9907398 -3.84008 -3.7730937 -3.7870109 -3.8322809 -3.8553631 -3.7894158 -3.6772962 -3.6582012 -3.770838 -3.9431694 -4.0761251 -4.134089][-4.3532987 -4.203413 -4.0237823 -3.8878806 -3.8293023 -3.8405755 -3.8836763 -3.9041615 -3.8314078 -3.7164567 -3.6893721 -3.7852952 -3.9386916 -4.06734 -4.1334658][-4.3487167 -4.243566 -4.1188722 -4.0285563 -3.995532 -4.0134487 -4.0545483 -4.0699587 -4.0025086 -3.898257 -3.8615091 -3.9302163 -4.0500083 -4.1515961 -4.2014995][-4.3227367 -4.2756686 -4.2277884 -4.2042556 -4.212976 -4.2455292 -4.2789 -4.2783871 -4.2107196 -4.1073742 -4.0538664 -4.0986571 -4.1954355 -4.2790642 -4.3152537][-4.3024559 -4.2983675 -4.31047 -4.3384337 -4.3778734 -4.4219885 -4.4495997 -4.4351506 -4.3649631 -4.254602 -4.1781945 -4.1980281 -4.2786174 -4.3600969 -4.4002848][-4.2937121 -4.311296 -4.3467488 -4.3867345 -4.4251342 -4.4633193 -4.4852505 -4.4685197 -4.4072957 -4.3056993 -4.2243991 -4.2299094 -4.2950387 -4.3738561 -4.4227557][-4.3057709 -4.3271008 -4.3583856 -4.3866453 -4.4075012 -4.432807 -4.4525347 -4.4397826 -4.39215 -4.3105335 -4.2419267 -4.246068 -4.2972984 -4.3610439 -4.4075894][-4.3247023 -4.3286629 -4.340148 -4.3507853 -4.3608136 -4.3816142 -4.4051104 -4.4023862 -4.3695192 -4.3072019 -4.2571368 -4.2648048 -4.3000112 -4.3390374 -4.3706517]]...]
INFO - root - 2017-12-07 08:32:53.600551: step 310, loss = 21.75, batch loss = 21.67 (11.4 examples/sec; 0.701 sec/batch; 64h:43m:03s remains)
INFO - root - 2017-12-07 08:33:00.992056: step 320, loss = 21.47, batch loss = 21.38 (10.9 examples/sec; 0.734 sec/batch; 67h:43m:07s remains)
INFO - root - 2017-12-07 08:33:08.337146: step 330, loss = 21.11, batch loss = 21.03 (10.9 examples/sec; 0.734 sec/batch; 67h:41m:29s remains)
INFO - root - 2017-12-07 08:33:15.659082: step 340, loss = 21.27, batch loss = 21.19 (10.4 examples/sec; 0.768 sec/batch; 70h:52m:15s remains)
INFO - root - 2017-12-07 08:33:22.886786: step 350, loss = 21.36, batch loss = 21.28 (10.7 examples/sec; 0.746 sec/batch; 68h:51m:23s remains)
INFO - root - 2017-12-07 08:33:30.269144: step 360, loss = 21.73, batch loss = 21.65 (10.6 examples/sec; 0.754 sec/batch; 69h:31m:55s remains)
INFO - root - 2017-12-07 08:33:37.567193: step 370, loss = 21.32, batch loss = 21.23 (11.3 examples/sec; 0.706 sec/batch; 65h:08m:51s remains)
INFO - root - 2017-12-07 08:33:44.965401: step 380, loss = 21.76, batch loss = 21.67 (11.8 examples/sec; 0.679 sec/batch; 62h:38m:56s remains)
INFO - root - 2017-12-07 08:33:52.272987: step 390, loss = 21.49, batch loss = 21.41 (11.0 examples/sec; 0.729 sec/batch; 67h:14m:33s remains)
INFO - root - 2017-12-07 08:33:59.560072: step 400, loss = 21.44, batch loss = 21.35 (10.8 examples/sec; 0.740 sec/batch; 68h:18m:39s remains)
2017-12-07 08:34:00.296444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4886274 -4.4853973 -4.4820971 -4.5046592 -4.5363913 -4.5384073 -4.5335841 -4.5555134 -4.5849819 -4.5837154 -4.5764303 -4.6143889 -4.6908889 -4.7276525 -4.6704412][-4.5560341 -4.558794 -4.5610418 -4.5898137 -4.6183519 -4.5953116 -4.56065 -4.5723543 -4.6060185 -4.603816 -4.5917192 -4.6269031 -4.7005453 -4.7296004 -4.66329][-4.6135421 -4.6195087 -4.6182775 -4.6341443 -4.6334939 -4.5678048 -4.4969215 -4.5014706 -4.5560112 -4.5793672 -4.58662 -4.6263685 -4.6853704 -4.695436 -4.6194482][-4.61185 -4.6183109 -4.6056285 -4.596684 -4.5533967 -4.4390693 -4.3341489 -4.3400478 -4.4268141 -4.4864721 -4.5154581 -4.5547328 -4.5985832 -4.6051879 -4.5435667][-4.532526 -4.5282283 -4.4937363 -4.4631896 -4.3931689 -4.2500868 -4.1255894 -4.1336155 -4.2423224 -4.3253059 -4.3624969 -4.3968019 -4.4371843 -4.4651008 -4.4492092][-4.456121 -4.4321027 -4.373548 -4.3259168 -4.2437906 -4.08477 -3.9441946 -3.9438667 -4.0577145 -4.1518855 -4.1878681 -4.2165666 -4.2644897 -4.3271074 -4.369103][-4.3979115 -4.3578072 -4.2967768 -4.2587705 -4.1904144 -4.032733 -3.8776853 -3.851568 -3.942313 -4.032825 -4.0734239 -4.1095204 -4.1747079 -4.2659016 -4.3447862][-4.3112092 -4.2508721 -4.2178025 -4.2424889 -4.2433228 -4.1272864 -3.9715698 -3.9035273 -3.9379716 -4.00389 -4.057631 -4.1210775 -4.2095466 -4.3110476 -4.3903632][-4.1790996 -4.0834837 -4.0841622 -4.2031708 -4.3167706 -4.2794671 -4.1434116 -4.034 -4.004127 -4.0378633 -4.1046429 -4.1977177 -4.3065081 -4.40637 -4.4655395][-4.030777 -3.8983424 -3.9193575 -4.1222463 -4.3496909 -4.4047203 -4.3076129 -4.1797729 -4.108232 -4.1179557 -4.1894097 -4.2943783 -4.4081545 -4.5002227 -4.5391321][-3.9510179 -3.7850664 -3.7955587 -4.0313897 -4.3286614 -4.4582 -4.4095364 -4.2946787 -4.2183771 -4.2284908 -4.3067565 -4.411994 -4.5183535 -4.59667 -4.6143594][-3.9803038 -3.7948451 -3.7763226 -3.9941175 -4.2937193 -4.4541626 -4.4457746 -4.3632545 -4.3112612 -4.3414707 -4.4309592 -4.5336704 -4.6286492 -4.689539 -4.6848011][-4.1095538 -3.9353781 -3.8993 -4.0701094 -4.3138304 -4.4549384 -4.4679308 -4.420331 -4.3982687 -4.4450483 -4.5370569 -4.6317883 -4.7106762 -4.7518888 -4.72724][-4.3064561 -4.164608 -4.1260629 -4.2417407 -4.4013767 -4.4910355 -4.5034351 -4.4794817 -4.4748659 -4.5186019 -4.5952053 -4.6721282 -4.7320118 -4.754539 -4.7155681][-4.4884696 -4.3986297 -4.3727551 -4.4378271 -4.5136108 -4.5434809 -4.5365686 -4.5158844 -4.5096941 -4.5345035 -4.5842571 -4.6375604 -4.6771131 -4.6838846 -4.6393218]]...]
INFO - root - 2017-12-07 08:34:07.646292: step 410, loss = 21.46, batch loss = 21.38 (10.3 examples/sec; 0.774 sec/batch; 71h:23m:41s remains)
INFO - root - 2017-12-07 08:34:14.995532: step 420, loss = 21.61, batch loss = 21.53 (11.2 examples/sec; 0.717 sec/batch; 66h:09m:52s remains)
INFO - root - 2017-12-07 08:34:22.351950: step 430, loss = 21.42, batch loss = 21.34 (11.2 examples/sec; 0.712 sec/batch; 65h:42m:31s remains)
INFO - root - 2017-12-07 08:34:29.814331: step 440, loss = 21.69, batch loss = 21.61 (10.9 examples/sec; 0.732 sec/batch; 67h:30m:51s remains)
INFO - root - 2017-12-07 08:34:37.168475: step 450, loss = 21.34, batch loss = 21.26 (10.4 examples/sec; 0.766 sec/batch; 70h:37m:39s remains)
INFO - root - 2017-12-07 08:34:44.499762: step 460, loss = 21.61, batch loss = 21.53 (11.3 examples/sec; 0.709 sec/batch; 65h:22m:20s remains)
INFO - root - 2017-12-07 08:34:51.588141: step 470, loss = 21.45, batch loss = 21.36 (11.5 examples/sec; 0.699 sec/batch; 64h:26m:17s remains)
INFO - root - 2017-12-07 08:34:58.957022: step 480, loss = 21.31, batch loss = 21.23 (10.9 examples/sec; 0.735 sec/batch; 67h:46m:06s remains)
INFO - root - 2017-12-07 08:35:06.365298: step 490, loss = 21.17, batch loss = 21.08 (11.0 examples/sec; 0.726 sec/batch; 66h:59m:05s remains)
INFO - root - 2017-12-07 08:35:13.763206: step 500, loss = 21.73, batch loss = 21.65 (10.1 examples/sec; 0.792 sec/batch; 73h:00m:44s remains)
2017-12-07 08:35:14.547384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6168 -4.6917138 -4.7170196 -4.701705 -4.5876703 -4.4361234 -4.3371773 -4.3405643 -4.4010811 -4.4793949 -4.549911 -4.5695014 -4.5058007 -4.335876 -4.1393132][-4.5349627 -4.6257329 -4.6745377 -4.6759524 -4.5680132 -4.4101763 -4.3030496 -4.3033028 -4.3604293 -4.4339743 -4.4997277 -4.5266185 -4.491241 -4.3574662 -4.1842208][-4.4722981 -4.5645995 -4.6232295 -4.6273141 -4.5199471 -4.3634796 -4.2601714 -4.2609882 -4.3160219 -4.3832722 -4.4386673 -4.4705763 -4.4722857 -4.3990235 -4.2727761][-4.4333162 -4.516067 -4.5651112 -4.5500741 -4.4301739 -4.271698 -4.17671 -4.1864877 -4.248136 -4.3182917 -4.3729811 -4.4105582 -4.4426517 -4.4260187 -4.3439722][-4.4292545 -4.4843369 -4.4997969 -4.4427018 -4.2935214 -4.11964 -4.0307484 -4.0610032 -4.1471653 -4.2390943 -4.3127241 -4.3621712 -4.4117241 -4.430325 -4.3773336][-4.4510493 -4.46595 -4.4371572 -4.3328776 -4.1450424 -3.9427717 -3.8517973 -3.9070477 -4.0289612 -4.1589718 -4.26995 -4.3399005 -4.3944154 -4.4185619 -4.3734][-4.4422178 -4.4311838 -4.374896 -4.2393866 -4.0205755 -3.7893333 -3.6904187 -3.7719526 -3.9336269 -4.1059403 -4.2548056 -4.3436742 -4.3901148 -4.394413 -4.3435068][-4.3819766 -4.36905 -4.3055997 -4.1505165 -3.9131889 -3.669085 -3.577487 -3.6960618 -3.9002271 -4.108541 -4.2789454 -4.3722768 -4.3980355 -4.3747406 -4.319005][-4.2788839 -4.292222 -4.2414246 -4.0820003 -3.8539674 -3.6307063 -3.5768449 -3.7361913 -3.9683826 -4.190268 -4.3559084 -4.4309492 -4.4266891 -4.37655 -4.3193288][-4.1686659 -4.2239356 -4.1982422 -4.0592327 -3.8781238 -3.7102408 -3.7072968 -3.8885689 -4.1203036 -4.3275652 -4.4657984 -4.5097375 -4.4736781 -4.400115 -4.3387051][-4.1264138 -4.1971679 -4.1789041 -4.0686283 -3.9573472 -3.8608634 -3.9004941 -4.0794787 -4.2852383 -4.4560795 -4.5558438 -4.5722685 -4.5182776 -4.431931 -4.3644328][-4.1825352 -4.234201 -4.1994915 -4.1138449 -4.0694785 -4.0356779 -4.0986094 -4.2573161 -4.423058 -4.5441389 -4.5997596 -4.5944319 -4.5382237 -4.4520564 -4.3822207][-4.3097491 -4.3343129 -4.2805762 -4.2107668 -4.2046409 -4.2079687 -4.2727962 -4.3947716 -4.5113163 -4.5791321 -4.5930891 -4.5742483 -4.5267215 -4.4524727 -4.3882542][-4.4407625 -4.4484668 -4.3871174 -4.3301558 -4.3381872 -4.357583 -4.4066734 -4.4815283 -4.5465746 -4.56917 -4.5525 -4.5273008 -4.4915862 -4.4338579 -4.3809733][-4.5726671 -4.5693755 -4.5074968 -4.4591155 -4.4616871 -4.4723015 -4.4919972 -4.5217519 -4.5453758 -4.5390587 -4.5055838 -4.4721918 -4.4375434 -4.3912425 -4.3513021]]...]
INFO - root - 2017-12-07 08:35:21.928246: step 510, loss = 21.67, batch loss = 21.59 (10.8 examples/sec; 0.739 sec/batch; 68h:06m:50s remains)
INFO - root - 2017-12-07 08:35:29.295152: step 520, loss = 21.28, batch loss = 21.20 (10.3 examples/sec; 0.779 sec/batch; 71h:48m:11s remains)
INFO - root - 2017-12-07 08:35:36.594593: step 530, loss = 21.55, batch loss = 21.47 (11.0 examples/sec; 0.727 sec/batch; 67h:01m:27s remains)
INFO - root - 2017-12-07 08:35:43.962222: step 540, loss = 21.66, batch loss = 21.57 (11.2 examples/sec; 0.714 sec/batch; 65h:52m:14s remains)
INFO - root - 2017-12-07 08:35:51.367342: step 550, loss = 21.50, batch loss = 21.42 (10.2 examples/sec; 0.785 sec/batch; 72h:21m:42s remains)
INFO - root - 2017-12-07 08:35:58.765971: step 560, loss = 21.14, batch loss = 21.06 (11.2 examples/sec; 0.715 sec/batch; 65h:56m:36s remains)
INFO - root - 2017-12-07 08:36:06.074939: step 570, loss = 21.25, batch loss = 21.17 (11.0 examples/sec; 0.725 sec/batch; 66h:52m:34s remains)
INFO - root - 2017-12-07 08:36:13.524676: step 580, loss = 21.42, batch loss = 21.34 (11.4 examples/sec; 0.702 sec/batch; 64h:42m:16s remains)
INFO - root - 2017-12-07 08:36:20.897275: step 590, loss = 21.56, batch loss = 21.48 (10.4 examples/sec; 0.768 sec/batch; 70h:49m:46s remains)
INFO - root - 2017-12-07 08:36:28.302054: step 600, loss = 21.49, batch loss = 21.40 (11.6 examples/sec; 0.691 sec/batch; 63h:42m:59s remains)
2017-12-07 08:36:29.070267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4046469 -4.3558393 -4.3043346 -4.2813473 -4.3070889 -4.3412108 -4.3725753 -4.4143014 -4.4485221 -4.4689693 -4.4803672 -4.4360709 -4.3425908 -4.2670655 -4.23654][-4.4722052 -4.4313326 -4.3887434 -4.3722825 -4.39794 -4.4302311 -4.4561019 -4.4846172 -4.5009255 -4.5039172 -4.5071549 -4.4701896 -4.4036961 -4.3525658 -4.3218966][-4.504941 -4.4766817 -4.4500012 -4.4494667 -4.4784007 -4.501349 -4.5151048 -4.5287604 -4.5267096 -4.5179448 -4.519053 -4.4895535 -4.4378381 -4.4003029 -4.3779721][-4.4791632 -4.4564705 -4.438921 -4.4469333 -4.4689112 -4.4687777 -4.4651375 -4.4711847 -4.4696536 -4.4732451 -4.4932418 -4.4811087 -4.4320712 -4.3900728 -4.3696303][-4.4159303 -4.405736 -4.3955836 -4.4050641 -4.4072995 -4.3674 -4.3359056 -4.3356261 -4.3448334 -4.3729219 -4.4242277 -4.4439988 -4.4036231 -4.348743 -4.3154469][-4.329102 -4.3314948 -4.3235235 -4.3224945 -4.2926664 -4.20932 -4.1526895 -4.1529202 -4.1832838 -4.2374821 -4.32375 -4.3830323 -4.3575444 -4.2923632 -4.2437658][-4.2664576 -4.2799149 -4.2670655 -4.2340393 -4.1525369 -4.0267534 -3.953145 -3.9634821 -4.027853 -4.1114383 -4.2265077 -4.31234 -4.2876797 -4.2085042 -4.1555095][-4.2582593 -4.289588 -4.2785926 -4.2136579 -4.0868835 -3.9369271 -3.8580627 -3.8773007 -3.9609854 -4.0479445 -4.163578 -4.25382 -4.2236595 -4.1410155 -4.1045074][-4.3064222 -4.3420267 -4.330864 -4.2514005 -4.1129212 -3.9731114 -3.9062345 -3.9315982 -4.0153823 -4.0805974 -4.1646276 -4.2311106 -4.1871753 -4.1141038 -4.113595][-4.3642936 -4.38467 -4.3726683 -4.3082237 -4.1937008 -4.0865755 -4.0433779 -4.0779653 -4.1588063 -4.202486 -4.247602 -4.2734189 -4.2102423 -4.14576 -4.1728978][-4.3962426 -4.3955588 -4.387723 -4.3590236 -4.2943888 -4.2328405 -4.2132912 -4.2483029 -4.315434 -4.3405547 -4.3522129 -4.3445091 -4.2783132 -4.2311468 -4.2721992][-4.4018784 -4.3852863 -4.3849196 -4.3902383 -4.3770761 -4.3619847 -4.3631721 -4.3918061 -4.4373388 -4.4515615 -4.4465165 -4.4190578 -4.3554006 -4.3159046 -4.3499608][-4.3858628 -4.3732262 -4.3813109 -4.4068403 -4.4318595 -4.4564285 -4.4783964 -4.4991117 -4.5243497 -4.5336509 -4.5289855 -4.4993892 -4.4395194 -4.3980722 -4.4095516][-4.3659987 -4.3652234 -4.3733664 -4.401381 -4.4467711 -4.4975123 -4.5343413 -4.5500655 -4.5592856 -4.5618792 -4.55776 -4.5316634 -4.4807568 -4.4447021 -4.4429407][-4.3589439 -4.369575 -4.3703518 -4.3869772 -4.4282355 -4.4786038 -4.516027 -4.5288343 -4.5293212 -4.5244966 -4.5169625 -4.4919438 -4.448782 -4.423151 -4.42207]]...]
INFO - root - 2017-12-07 08:36:36.430478: step 610, loss = 21.57, batch loss = 21.49 (11.4 examples/sec; 0.702 sec/batch; 64h:40m:50s remains)
INFO - root - 2017-12-07 08:36:43.736400: step 620, loss = 21.47, batch loss = 21.39 (11.2 examples/sec; 0.714 sec/batch; 65h:47m:26s remains)
INFO - root - 2017-12-07 08:36:50.917512: step 630, loss = 21.39, batch loss = 21.30 (11.2 examples/sec; 0.711 sec/batch; 65h:34m:40s remains)
INFO - root - 2017-12-07 08:36:58.332606: step 640, loss = 21.60, batch loss = 21.52 (11.1 examples/sec; 0.724 sec/batch; 66h:43m:22s remains)
INFO - root - 2017-12-07 08:37:05.703955: step 650, loss = 21.49, batch loss = 21.40 (11.4 examples/sec; 0.704 sec/batch; 64h:55m:17s remains)
INFO - root - 2017-12-07 08:37:12.960723: step 660, loss = 21.39, batch loss = 21.30 (11.5 examples/sec; 0.693 sec/batch; 63h:55m:23s remains)
INFO - root - 2017-12-07 08:37:20.254365: step 670, loss = 21.91, batch loss = 21.82 (11.0 examples/sec; 0.730 sec/batch; 67h:18m:56s remains)
INFO - root - 2017-12-07 08:37:27.771581: step 680, loss = 21.73, batch loss = 21.65 (10.6 examples/sec; 0.753 sec/batch; 69h:25m:16s remains)
INFO - root - 2017-12-07 08:37:35.109902: step 690, loss = 21.40, batch loss = 21.32 (10.4 examples/sec; 0.771 sec/batch; 71h:01m:57s remains)
INFO - root - 2017-12-07 08:37:42.356925: step 700, loss = 21.20, batch loss = 21.12 (10.2 examples/sec; 0.784 sec/batch; 72h:16m:26s remains)
2017-12-07 08:37:43.227789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6376567 -4.7349749 -4.761837 -4.7534771 -4.7251315 -4.7376342 -4.7989078 -4.8513923 -4.8890409 -4.9163141 -4.9462118 -4.9241705 -4.8507948 -4.8044415 -4.789319][-4.6592135 -4.7470675 -4.7571049 -4.7217283 -4.6671095 -4.664587 -4.7399087 -4.8357482 -4.912817 -4.9437127 -4.9332829 -4.8640637 -4.7640266 -4.7091646 -4.71341][-4.6941624 -4.7617331 -4.767312 -4.7235446 -4.6455154 -4.5960135 -4.6348577 -4.7392869 -4.8532887 -4.9152517 -4.8970542 -4.8038511 -4.686265 -4.6214104 -4.6388397][-4.7410755 -4.7931771 -4.8040466 -4.7613125 -4.6547675 -4.5320888 -4.4932265 -4.571557 -4.719337 -4.8483143 -4.8732014 -4.7880421 -4.6544886 -4.5692377 -4.5851583][-4.7470875 -4.7725425 -4.7630577 -4.6960988 -4.5543714 -4.3711696 -4.2657351 -4.3158669 -4.5014291 -4.7170916 -4.8187528 -4.75581 -4.5930471 -4.4663277 -4.4672079][-4.6840949 -4.6724491 -4.6082454 -4.4838495 -4.301753 -4.0884156 -3.9574506 -3.9966443 -4.2120924 -4.5026269 -4.6828961 -4.6466441 -4.4478383 -4.2683697 -4.2470918][-4.6151133 -4.5707994 -4.4445519 -4.2527504 -4.0221195 -3.786032 -3.6451914 -3.6820951 -3.9132667 -4.2503357 -4.4933286 -4.4949007 -4.2898145 -4.08086 -4.0284538][-4.5596189 -4.5106344 -4.3697014 -4.1582541 -3.899282 -3.6394875 -3.472867 -3.4879575 -3.7204962 -4.0799503 -4.363605 -4.4118319 -4.2485824 -4.0550928 -3.9689786][-4.5260882 -4.5126529 -4.419064 -4.2530084 -4.016933 -3.7545676 -3.5565772 -3.5386481 -3.7546916 -4.1041808 -4.3870769 -4.4604015 -4.3606014 -4.2225609 -4.1316557][-4.5218358 -4.5479164 -4.5221996 -4.4356647 -4.2695432 -4.0477338 -3.8462286 -3.798665 -3.9722431 -4.268209 -4.502368 -4.5652719 -4.5143776 -4.4431524 -4.3810205][-4.5263343 -4.5691266 -4.5878158 -4.5714321 -4.4929013 -4.3503661 -4.1903276 -4.1286068 -4.2383857 -4.4442191 -4.5953226 -4.6238275 -4.5946951 -4.5676155 -4.5410061][-4.5491223 -4.58335 -4.6006474 -4.601706 -4.5764103 -4.5240359 -4.4491258 -4.4093847 -4.4622188 -4.5645189 -4.6237097 -4.6202755 -4.6018066 -4.5933847 -4.5852218][-4.5930166 -4.6110649 -4.6009521 -4.5713816 -4.5469193 -4.5566635 -4.57715 -4.5909314 -4.6106162 -4.6223211 -4.6051693 -4.582171 -4.5734034 -4.5646105 -4.5504327][-4.620913 -4.6396637 -4.6163096 -4.5554991 -4.5084157 -4.5400662 -4.6182289 -4.6763992 -4.67884 -4.628696 -4.5675516 -4.5434613 -4.5453792 -4.5308261 -4.5036707][-4.6069918 -4.6361661 -4.6187758 -4.5512748 -4.4903541 -4.5145373 -4.5943484 -4.6573448 -4.644136 -4.56824 -4.5033851 -4.4967747 -4.518589 -4.5125246 -4.4914675]]...]
INFO - root - 2017-12-07 08:37:50.491146: step 710, loss = 21.63, batch loss = 21.55 (11.2 examples/sec; 0.711 sec/batch; 65h:32m:59s remains)
INFO - root - 2017-12-07 08:37:57.947173: step 720, loss = 21.35, batch loss = 21.26 (11.1 examples/sec; 0.718 sec/batch; 66h:11m:47s remains)
INFO - root - 2017-12-07 08:38:05.283446: step 730, loss = 21.57, batch loss = 21.49 (10.6 examples/sec; 0.754 sec/batch; 69h:30m:34s remains)
INFO - root - 2017-12-07 08:38:12.628551: step 740, loss = 21.67, batch loss = 21.59 (11.3 examples/sec; 0.708 sec/batch; 65h:15m:12s remains)
INFO - root - 2017-12-07 08:38:19.956513: step 750, loss = 21.26, batch loss = 21.18 (11.8 examples/sec; 0.678 sec/batch; 62h:26m:09s remains)
INFO - root - 2017-12-07 08:38:27.191217: step 760, loss = 21.46, batch loss = 21.37 (10.8 examples/sec; 0.739 sec/batch; 68h:06m:23s remains)
INFO - root - 2017-12-07 08:38:34.548131: step 770, loss = 21.26, batch loss = 21.18 (11.5 examples/sec; 0.696 sec/batch; 64h:10m:02s remains)
INFO - root - 2017-12-07 08:38:41.900249: step 780, loss = 21.86, batch loss = 21.78 (12.0 examples/sec; 0.668 sec/batch; 61h:34m:22s remains)
INFO - root - 2017-12-07 08:38:49.139456: step 790, loss = 21.69, batch loss = 21.61 (11.0 examples/sec; 0.725 sec/batch; 66h:46m:29s remains)
INFO - root - 2017-12-07 08:38:56.495496: step 800, loss = 21.74, batch loss = 21.66 (11.2 examples/sec; 0.715 sec/batch; 65h:55m:12s remains)
2017-12-07 08:38:57.322297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4169745 -4.410152 -4.4099092 -4.373364 -4.2796264 -4.1636839 -4.0688229 -4.0240946 -4.0490503 -4.1407924 -4.2516422 -4.3262925 -4.3489237 -4.3345394 -4.3235612][-4.4621243 -4.4573035 -4.45792 -4.4239817 -4.3364091 -4.2254114 -4.1291828 -4.0780935 -4.0842328 -4.14632 -4.2296023 -4.2824335 -4.294426 -4.2766724 -4.2597609][-4.4927754 -4.4864955 -4.4787116 -4.4361572 -4.3529906 -4.2564898 -4.1706767 -4.1184664 -4.1104941 -4.1570549 -4.2380972 -4.2932148 -4.3023438 -4.2694855 -4.2258925][-4.4688249 -4.4462795 -4.4202948 -4.363625 -4.287004 -4.2192082 -4.1664481 -4.1326857 -4.1282077 -4.181673 -4.2849317 -4.3630877 -4.376936 -4.322319 -4.2473][-4.3792663 -4.3313222 -4.2843485 -4.2153549 -4.1460109 -4.1079187 -4.0943327 -4.0885115 -4.10308 -4.1849117 -4.329174 -4.4390044 -4.4565392 -4.381443 -4.2868967][-4.267242 -4.1905456 -4.120163 -4.0357838 -3.9635906 -3.9374511 -3.9465013 -3.9592834 -3.9988785 -4.1280169 -4.3317404 -4.4797034 -4.4999418 -4.4097815 -4.3081141][-4.2085667 -4.1146894 -4.0214109 -3.9138947 -3.8223448 -3.7802985 -3.7761149 -3.7777128 -3.8313618 -4.0124383 -4.2765093 -4.4564528 -4.4740758 -4.3726707 -4.2682271][-4.2396793 -4.1512723 -4.0448828 -3.9151211 -3.7969823 -3.7223268 -3.6821008 -3.6554308 -3.7076089 -3.914212 -4.1966615 -4.3725781 -4.3777843 -4.27295 -4.1672196][-4.3150859 -4.2472539 -4.1433058 -4.0111432 -3.8886075 -3.8042154 -3.7463763 -3.70465 -3.7459478 -3.9324825 -4.1759949 -4.314168 -4.307189 -4.2107348 -4.1028647][-4.3972883 -4.3536458 -4.2679119 -4.1566577 -4.0562119 -3.9831462 -3.9186349 -3.8646383 -3.8812449 -4.0201888 -4.202177 -4.3003769 -4.294539 -4.2155409 -4.1098709][-4.4784751 -4.4540663 -4.3922191 -4.3100185 -4.2386637 -4.18244 -4.1149311 -4.0459242 -4.0321517 -4.1190362 -4.2469368 -4.3180256 -4.3194509 -4.2573047 -4.1551189][-4.5632887 -4.5496478 -4.5092454 -4.4574037 -4.4153566 -4.37841 -4.3166656 -4.2428255 -4.2128792 -4.2665253 -4.358252 -4.4116311 -4.4136024 -4.3565598 -4.25321][-4.6152892 -4.6106539 -4.5910258 -4.5664725 -4.5460138 -4.5221224 -4.4724979 -4.4119473 -4.388742 -4.43042 -4.498806 -4.5396395 -4.5404787 -4.4871445 -4.3881617][-4.5910616 -4.5895371 -4.5861464 -4.5842528 -4.5818858 -4.5720768 -4.5429373 -4.5085154 -4.50343 -4.5389018 -4.586328 -4.6136661 -4.6151509 -4.5792818 -4.5048437][-4.4952569 -4.4901452 -4.4936123 -4.5025253 -4.5112863 -4.5140033 -4.504498 -4.4937258 -4.5011773 -4.5272322 -4.5546479 -4.5694442 -4.57662 -4.5714378 -4.5409546]]...]
INFO - root - 2017-12-07 08:39:04.798835: step 810, loss = 21.35, batch loss = 21.27 (10.8 examples/sec; 0.742 sec/batch; 68h:19m:41s remains)
INFO - root - 2017-12-07 08:39:12.178646: step 820, loss = 20.95, batch loss = 20.87 (11.1 examples/sec; 0.723 sec/batch; 66h:36m:13s remains)
INFO - root - 2017-12-07 08:39:19.524517: step 830, loss = 21.83, batch loss = 21.75 (10.9 examples/sec; 0.737 sec/batch; 67h:55m:46s remains)
INFO - root - 2017-12-07 08:39:26.746706: step 840, loss = 21.15, batch loss = 21.07 (10.7 examples/sec; 0.747 sec/batch; 68h:49m:58s remains)
INFO - root - 2017-12-07 08:39:34.068221: step 850, loss = 21.39, batch loss = 21.30 (11.0 examples/sec; 0.731 sec/batch; 67h:18m:07s remains)
INFO - root - 2017-12-07 08:39:41.433744: step 860, loss = 21.52, batch loss = 21.44 (11.1 examples/sec; 0.724 sec/batch; 66h:39m:13s remains)
INFO - root - 2017-12-07 08:39:48.780103: step 870, loss = 21.40, batch loss = 21.32 (11.3 examples/sec; 0.707 sec/batch; 65h:05m:13s remains)
INFO - root - 2017-12-07 08:39:56.102547: step 880, loss = 21.47, batch loss = 21.39 (10.6 examples/sec; 0.752 sec/batch; 69h:14m:12s remains)
INFO - root - 2017-12-07 08:40:03.513206: step 890, loss = 21.56, batch loss = 21.48 (11.1 examples/sec; 0.723 sec/batch; 66h:33m:17s remains)
INFO - root - 2017-12-07 08:40:10.869350: step 900, loss = 21.33, batch loss = 21.25 (10.9 examples/sec; 0.737 sec/batch; 67h:52m:05s remains)
2017-12-07 08:40:11.643763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4303327 -4.4357047 -4.4453397 -4.4610167 -4.4785237 -4.4936328 -4.5050411 -4.5126338 -4.5098891 -4.4948611 -4.475625 -4.4554114 -4.4306684 -4.3984952 -4.3629966][-4.4906578 -4.5042939 -4.5173349 -4.53218 -4.545455 -4.5602756 -4.5809984 -4.6073871 -4.6274514 -4.6289344 -4.6145511 -4.5917044 -4.5539279 -4.4923043 -4.4184418][-4.5419397 -4.5660262 -4.5788484 -4.5824389 -4.5737553 -4.5633736 -4.5742788 -4.6131043 -4.6673179 -4.711092 -4.7312088 -4.7356286 -4.709527 -4.6317711 -4.51583][-4.5707579 -4.6055894 -4.6154094 -4.6031194 -4.561758 -4.5046978 -4.4794717 -4.5077295 -4.5855365 -4.6764507 -4.7465258 -4.8009024 -4.8175597 -4.7585788 -4.6257191][-4.5732169 -4.610961 -4.6042991 -4.563951 -4.4830403 -4.3734241 -4.304317 -4.3125138 -4.4012074 -4.5237365 -4.6331944 -4.7333641 -4.7999773 -4.7894626 -4.6832948][-4.5619311 -4.5962734 -4.5597515 -4.4753871 -4.3428588 -4.1744118 -4.057703 -4.0508671 -4.1553717 -4.3035178 -4.4395862 -4.5691175 -4.6696386 -4.7070885 -4.6520262][-4.5641742 -4.6020541 -4.5409956 -4.4097695 -4.2186246 -3.9779894 -3.7996254 -3.7751853 -3.9060066 -4.0912986 -4.2596869 -4.4166026 -4.5379038 -4.5988293 -4.5781317][-4.5809765 -4.6338873 -4.5755978 -4.424962 -4.2011671 -3.9088473 -3.6624098 -3.5913908 -3.7284141 -3.9492483 -4.1551414 -4.3433056 -4.4859271 -4.5572877 -4.5440726][-4.5771651 -4.6415634 -4.6103764 -4.4882832 -4.2903223 -4.0077815 -3.731916 -3.6040506 -3.7059982 -3.9295876 -4.1509705 -4.3507342 -4.5044508 -4.5816927 -4.5636425][-4.5371327 -4.6012669 -4.6069512 -4.5489578 -4.428196 -4.2211051 -3.9839194 -3.8331347 -3.8774302 -4.0548263 -4.2487 -4.424593 -4.5614915 -4.628408 -4.6007576][-4.491703 -4.5484343 -4.5810561 -4.5801926 -4.5401373 -4.4294109 -4.2765794 -4.1567116 -4.1618605 -4.2750854 -4.4102449 -4.5346613 -4.6307812 -4.6707754 -4.6308284][-4.4717603 -4.5243287 -4.5682425 -4.5956354 -4.6004305 -4.5592513 -4.4871478 -4.4223118 -4.4186215 -4.4808235 -4.5598469 -4.6325479 -4.6844177 -4.6939158 -4.642911][-4.4587469 -4.5083108 -4.5537405 -4.5874147 -4.60711 -4.6012578 -4.5789433 -4.5536675 -4.5505614 -4.5810442 -4.6238546 -4.6645312 -4.6865454 -4.6752276 -4.620553][-4.4320312 -4.472084 -4.5121293 -4.5464578 -4.5711889 -4.5810719 -4.581543 -4.5765853 -4.5739489 -4.5841608 -4.6027141 -4.6225176 -4.627933 -4.6081691 -4.5590315][-4.3963037 -4.4215374 -4.4495397 -4.4787087 -4.502902 -4.5175834 -4.5256476 -4.5286808 -4.528553 -4.5298915 -4.5336041 -4.5368738 -4.5311165 -4.5095448 -4.4715133]]...]
INFO - root - 2017-12-07 08:40:18.960173: step 910, loss = 21.95, batch loss = 21.87 (10.8 examples/sec; 0.743 sec/batch; 68h:24m:45s remains)
INFO - root - 2017-12-07 08:40:26.249271: step 920, loss = 21.86, batch loss = 21.77 (10.4 examples/sec; 0.766 sec/batch; 70h:35m:06s remains)
INFO - root - 2017-12-07 08:40:33.594236: step 930, loss = 21.54, batch loss = 21.46 (10.7 examples/sec; 0.746 sec/batch; 68h:41m:31s remains)
INFO - root - 2017-12-07 08:40:40.814848: step 940, loss = 21.02, batch loss = 20.94 (12.8 examples/sec; 0.624 sec/batch; 57h:30m:42s remains)
INFO - root - 2017-12-07 08:40:48.144735: step 950, loss = 21.40, batch loss = 21.32 (10.3 examples/sec; 0.775 sec/batch; 71h:24m:06s remains)
INFO - root - 2017-12-07 08:40:55.534608: step 960, loss = 21.78, batch loss = 21.69 (11.3 examples/sec; 0.710 sec/batch; 65h:25m:43s remains)
INFO - root - 2017-12-07 08:41:02.848585: step 970, loss = 21.44, batch loss = 21.36 (10.8 examples/sec; 0.742 sec/batch; 68h:21m:26s remains)
INFO - root - 2017-12-07 08:41:10.153653: step 980, loss = 21.44, batch loss = 21.36 (11.0 examples/sec; 0.727 sec/batch; 66h:58m:30s remains)
INFO - root - 2017-12-07 08:41:17.473475: step 990, loss = 21.00, batch loss = 20.92 (11.6 examples/sec; 0.690 sec/batch; 63h:30m:54s remains)
INFO - root - 2017-12-07 08:41:24.733632: step 1000, loss = 21.00, batch loss = 20.92 (11.3 examples/sec; 0.708 sec/batch; 65h:13m:41s remains)
2017-12-07 08:41:25.510118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.413578 -4.4615612 -4.5350156 -4.6072178 -4.6346407 -4.5986629 -4.5203686 -4.4312677 -4.4040775 -4.4802766 -4.6056671 -4.7031107 -4.73266 -4.6513758 -4.4914665][-4.2224121 -4.2928491 -4.4061456 -4.5130363 -4.559124 -4.5299487 -4.4606781 -4.3847895 -4.3892317 -4.5071573 -4.6501455 -4.7425489 -4.7721291 -4.6901689 -4.5173578][-4.0738583 -4.1678195 -4.3182368 -4.4468274 -4.4983792 -4.4657178 -4.3831429 -4.2998714 -4.326714 -4.4904113 -4.661212 -4.7503848 -4.7749224 -4.6948423 -4.5251369][-3.9968915 -4.1011605 -4.2715654 -4.4105635 -4.4638166 -4.4120836 -4.2813334 -4.15576 -4.1783257 -4.3747625 -4.5805907 -4.6862578 -4.7182198 -4.6526775 -4.5051131][-4.0135574 -4.1051593 -4.2567492 -4.3836036 -4.4245229 -4.3292675 -4.1256814 -3.9425788 -3.948102 -4.1660075 -4.4103837 -4.5563335 -4.623034 -4.5925145 -4.481154][-4.1414242 -4.2255344 -4.321219 -4.3818846 -4.3498321 -4.1705971 -3.8880389 -3.6639538 -3.67168 -3.9216573 -4.2103963 -4.4115076 -4.5344596 -4.5523205 -4.4780736][-4.3518863 -4.4282126 -4.45283 -4.411046 -4.2716384 -4.0031066 -3.668524 -3.4360111 -3.4653261 -3.7512879 -4.0781555 -4.3273029 -4.5017676 -4.5545664 -4.4987173][-4.5213857 -4.5774078 -4.5507107 -4.4355631 -4.2269154 -3.9203696 -3.5833552 -3.3679962 -3.4139218 -3.710072 -4.047195 -4.3153315 -4.5093265 -4.569623 -4.5160055][-4.5838661 -4.6081004 -4.5617 -4.4364176 -4.2381444 -3.9715748 -3.6932991 -3.5198455 -3.5586257 -3.8139606 -4.1151147 -4.35925 -4.530354 -4.5724769 -4.5152159][-4.5405297 -4.5413113 -4.4997463 -4.4136171 -4.2874551 -4.1167192 -3.9347634 -3.8129196 -3.8245356 -4.0023642 -4.2342782 -4.4242764 -4.5493155 -4.5653949 -4.5067992][-4.4262195 -4.4223614 -4.4011784 -4.3744235 -4.3407788 -4.2753711 -4.1841197 -4.1017966 -4.0850577 -4.19348 -4.3619065 -4.5013828 -4.5845113 -4.5796046 -4.5161343][-4.2994914 -4.3005247 -4.3006563 -4.3251328 -4.3691082 -4.3872209 -4.36003 -4.304069 -4.2781334 -4.3570323 -4.4953518 -4.60528 -4.65961 -4.6348858 -4.5525947][-4.2173476 -4.225677 -4.2420363 -4.293879 -4.3780918 -4.4383593 -4.442431 -4.4048638 -4.3918023 -4.4777007 -4.6152377 -4.7127647 -4.7468967 -4.7018304 -4.5954323][-4.2558246 -4.2690716 -4.290772 -4.3400922 -4.4174657 -4.474874 -4.4807067 -4.4524827 -4.4577885 -4.559525 -4.7014146 -4.789752 -4.8050485 -4.7415519 -4.6193538][-4.3952909 -4.41053 -4.4300227 -4.4617329 -4.5078406 -4.5373459 -4.5281277 -4.5002255 -4.5136886 -4.6130409 -4.7422171 -4.8137112 -4.8099747 -4.7347069 -4.6114893]]...]
INFO - root - 2017-12-07 08:41:32.789939: step 1010, loss = 20.99, batch loss = 20.91 (10.8 examples/sec; 0.741 sec/batch; 68h:12m:20s remains)
INFO - root - 2017-12-07 08:41:40.067114: step 1020, loss = 21.43, batch loss = 21.35 (9.8 examples/sec; 0.819 sec/batch; 75h:22m:57s remains)
INFO - root - 2017-12-07 08:41:47.445494: step 1030, loss = 21.54, batch loss = 21.46 (10.9 examples/sec; 0.734 sec/batch; 67h:34m:53s remains)
INFO - root - 2017-12-07 08:41:54.700315: step 1040, loss = 21.15, batch loss = 21.07 (11.1 examples/sec; 0.718 sec/batch; 66h:04m:22s remains)
INFO - root - 2017-12-07 08:42:01.950120: step 1050, loss = 21.30, batch loss = 21.21 (10.5 examples/sec; 0.764 sec/batch; 70h:18m:50s remains)
INFO - root - 2017-12-07 08:42:09.348001: step 1060, loss = 21.70, batch loss = 21.61 (10.4 examples/sec; 0.769 sec/batch; 70h:46m:03s remains)
INFO - root - 2017-12-07 08:42:16.724552: step 1070, loss = 21.00, batch loss = 20.92 (10.3 examples/sec; 0.779 sec/batch; 71h:44m:37s remains)
INFO - root - 2017-12-07 08:42:24.012216: step 1080, loss = 21.70, batch loss = 21.62 (10.5 examples/sec; 0.760 sec/batch; 69h:57m:29s remains)
INFO - root - 2017-12-07 08:42:31.311335: step 1090, loss = 21.20, batch loss = 21.12 (11.0 examples/sec; 0.730 sec/batch; 67h:14m:08s remains)
INFO - root - 2017-12-07 08:42:38.607491: step 1100, loss = 21.15, batch loss = 21.07 (11.3 examples/sec; 0.705 sec/batch; 64h:53m:23s remains)
2017-12-07 08:42:39.352860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4800606 -4.3153734 -4.149334 -4.0822558 -4.0731797 -4.1003151 -4.2048082 -4.3280258 -4.4052062 -4.4617233 -4.5122194 -4.5560169 -4.5912013 -4.5819449 -4.5156488][-4.5131016 -4.374598 -4.2165456 -4.1187987 -4.0365682 -3.9761047 -4.033319 -4.1799216 -4.3215256 -4.4327316 -4.5073657 -4.5526175 -4.5834751 -4.58421 -4.5337882][-4.566628 -4.437808 -4.2707462 -4.1188192 -3.9568057 -3.8316088 -3.86686 -4.045279 -4.2429776 -4.39636 -4.4831114 -4.5253916 -4.55614 -4.5740147 -4.548945][-4.6274385 -4.5202475 -4.3604393 -4.1759043 -3.9616416 -3.7967005 -3.8126612 -3.9943035 -4.20873 -4.3717818 -4.4497857 -4.4797068 -4.5122514 -4.5494342 -4.5524845][-4.6317825 -4.5593138 -4.4271584 -4.2500143 -4.0267839 -3.8451655 -3.833374 -3.9813166 -4.178771 -4.3382907 -4.4099135 -4.4343338 -4.4697208 -4.5178595 -4.539125][-4.586854 -4.5562034 -4.4592481 -4.3093619 -4.0969019 -3.8981037 -3.8367751 -3.9159639 -4.0732222 -4.2336245 -4.32442 -4.3746061 -4.4323688 -4.4903274 -4.5184469][-4.518259 -4.5436459 -4.4942942 -4.3793979 -4.1814642 -3.9558952 -3.8226936 -3.8135638 -3.9205322 -4.0853662 -4.2123842 -4.3100753 -4.404923 -4.4790173 -4.5096135][-4.4599147 -4.5318794 -4.5341086 -4.4616795 -4.2894387 -4.0496888 -3.8556023 -3.7629168 -3.8122344 -3.9709709 -4.1272507 -4.2646303 -4.3905644 -4.4850783 -4.523447][-4.4516177 -4.5392022 -4.5729356 -4.5371866 -4.3969197 -4.1703453 -3.957237 -3.8142443 -3.80575 -3.9378996 -4.1025152 -4.2583981 -4.4005108 -4.512835 -4.5609231][-4.475493 -4.5553303 -4.6009121 -4.5884333 -4.4801059 -4.2885957 -4.0955544 -3.9453268 -3.8973632 -3.9859495 -4.1298695 -4.27642 -4.4155354 -4.5353708 -4.592937][-4.4846311 -4.5478368 -4.5981956 -4.6027455 -4.5253506 -4.3764753 -4.224184 -4.0939279 -4.0210657 -4.0519619 -4.14976 -4.2700267 -4.3963242 -4.5172935 -4.5827789][-4.4701586 -4.5138822 -4.5658722 -4.5839906 -4.5386176 -4.4345975 -4.3281703 -4.22907 -4.1410184 -4.1106925 -4.150475 -4.2375231 -4.3502007 -4.468555 -4.5367589][-4.4425492 -4.4744368 -4.5211411 -4.5451269 -4.5284266 -4.4720159 -4.4154544 -4.3538551 -4.2655387 -4.1890845 -4.1727152 -4.2198925 -4.3128276 -4.4213219 -4.4840093][-4.4116864 -4.4367456 -4.473568 -4.4925656 -4.4913468 -4.4715223 -4.4600835 -4.4376225 -4.3671513 -4.2718449 -4.2157412 -4.2276211 -4.2966585 -4.3861828 -4.4364777][-4.3878188 -4.4000483 -4.4205995 -4.4271569 -4.428165 -4.4314251 -4.4511709 -4.45291 -4.39825 -4.3021684 -4.2286224 -4.2237191 -4.2812424 -4.3594832 -4.4025083]]...]
INFO - root - 2017-12-07 08:42:46.648804: step 1110, loss = 21.14, batch loss = 21.06 (11.2 examples/sec; 0.712 sec/batch; 65h:35m:04s remains)
INFO - root - 2017-12-07 08:42:54.057356: step 1120, loss = 21.71, batch loss = 21.63 (11.1 examples/sec; 0.718 sec/batch; 66h:04m:19s remains)
INFO - root - 2017-12-07 08:43:01.402377: step 1130, loss = 21.61, batch loss = 21.52 (10.6 examples/sec; 0.758 sec/batch; 69h:47m:25s remains)
INFO - root - 2017-12-07 08:43:08.665426: step 1140, loss = 21.80, batch loss = 21.71 (10.7 examples/sec; 0.748 sec/batch; 68h:53m:38s remains)
INFO - root - 2017-12-07 08:43:15.953284: step 1150, loss = 21.65, batch loss = 21.57 (10.9 examples/sec; 0.735 sec/batch; 67h:39m:02s remains)
INFO - root - 2017-12-07 08:43:23.300766: step 1160, loss = 21.41, batch loss = 21.32 (10.7 examples/sec; 0.747 sec/batch; 68h:44m:36s remains)
INFO - root - 2017-12-07 08:43:30.612398: step 1170, loss = 21.34, batch loss = 21.26 (10.3 examples/sec; 0.774 sec/batch; 71h:11m:32s remains)
INFO - root - 2017-12-07 08:43:37.912966: step 1180, loss = 21.70, batch loss = 21.61 (11.3 examples/sec; 0.710 sec/batch; 65h:21m:15s remains)
INFO - root - 2017-12-07 08:43:45.312349: step 1190, loss = 21.36, batch loss = 21.28 (10.3 examples/sec; 0.778 sec/batch; 71h:34m:15s remains)
INFO - root - 2017-12-07 08:43:52.577839: step 1200, loss = 20.97, batch loss = 20.89 (10.4 examples/sec; 0.771 sec/batch; 70h:57m:44s remains)
2017-12-07 08:43:53.370442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3862195 -4.4183993 -4.4746933 -4.5401826 -4.5831165 -4.5908184 -4.550602 -4.4972219 -4.4857626 -4.5083394 -4.5380082 -4.5450349 -4.5219722 -4.4787469 -4.4233375][-4.2726521 -4.299757 -4.3663321 -4.4725347 -4.5589814 -4.5755334 -4.5240474 -4.4697208 -4.472487 -4.5136862 -4.5545306 -4.5625253 -4.5205922 -4.4471316 -4.3752379][-4.2144 -4.2322683 -4.3067737 -4.4447312 -4.5485625 -4.5415239 -4.4518704 -4.392108 -4.41806 -4.4856 -4.5428472 -4.5566382 -4.5055327 -4.4116759 -4.3268647][-4.1951013 -4.2132287 -4.3029089 -4.4461923 -4.521349 -4.4521337 -4.3051753 -4.2355437 -4.2912445 -4.3979611 -4.48624 -4.5148649 -4.4702821 -4.3760138 -4.286406][-4.1729293 -4.1980004 -4.2979689 -4.41374 -4.4260988 -4.2890892 -4.0908127 -4.0106668 -4.1011786 -4.2569489 -4.3899093 -4.4478602 -4.4243503 -4.3479276 -4.2610955][-4.1571541 -4.195425 -4.2882314 -4.3501077 -4.285018 -4.0908976 -3.8617733 -3.7724116 -3.88967 -4.0889153 -4.2644558 -4.3561511 -4.3601761 -4.3122439 -4.2388859][-4.17784 -4.23133 -4.3027339 -4.2986827 -4.1580448 -3.920862 -3.68298 -3.5988238 -3.7390647 -3.9656403 -4.1692977 -4.2890749 -4.3171787 -4.29398 -4.2406092][-4.2137475 -4.2788606 -4.3251009 -4.2770276 -4.0996981 -3.8536577 -3.6402712 -3.5829194 -3.7365344 -3.9670253 -4.1741495 -4.3005905 -4.3331208 -4.314929 -4.27933][-4.2646418 -4.3463197 -4.3814869 -4.3232679 -4.1534052 -3.9440625 -3.7869515 -3.7614021 -3.8950448 -4.0860987 -4.2558827 -4.3610382 -4.3832593 -4.3613386 -4.34026][-4.3483615 -4.4462886 -4.4760823 -4.4203258 -4.2782364 -4.1299953 -4.0423632 -4.0458169 -4.1409 -4.265686 -4.3704672 -4.434402 -4.4429631 -4.4228559 -4.4167805][-4.4450397 -4.5465074 -4.5709338 -4.5209613 -4.4105082 -4.3217678 -4.3010044 -4.3338318 -4.3918009 -4.4446659 -4.4735951 -4.4851093 -4.4771185 -4.4646716 -4.4741464][-4.5495591 -4.6412072 -4.6576943 -4.60914 -4.5183549 -4.4615517 -4.4761004 -4.5237956 -4.5482645 -4.5361147 -4.4996009 -4.47002 -4.4565687 -4.4605651 -4.4824247][-4.624156 -4.6871185 -4.6861305 -4.6317978 -4.5486555 -4.5000663 -4.5168314 -4.559732 -4.5619907 -4.5176253 -4.4586978 -4.4207091 -4.4131842 -4.4306679 -4.4508939][-4.61356 -4.6390595 -4.626461 -4.5786638 -4.5150962 -4.4770041 -4.4832749 -4.5121894 -4.5078311 -4.4645867 -4.4160881 -4.3871188 -4.3833642 -4.4052334 -4.4194326][-4.509469 -4.5129356 -4.5067935 -4.4872556 -4.4604416 -4.4407511 -4.4358945 -4.4452758 -4.4382606 -4.4076595 -4.3769593 -4.3610463 -4.3655005 -4.3963404 -4.4134378]]...]
INFO - root - 2017-12-07 08:44:00.731051: step 1210, loss = 21.63, batch loss = 21.55 (11.1 examples/sec; 0.722 sec/batch; 66h:27m:32s remains)
INFO - root - 2017-12-07 08:44:08.045212: step 1220, loss = 21.28, batch loss = 21.20 (11.2 examples/sec; 0.715 sec/batch; 65h:44m:59s remains)
INFO - root - 2017-12-07 08:44:15.396277: step 1230, loss = 21.16, batch loss = 21.07 (10.9 examples/sec; 0.736 sec/batch; 67h:46m:05s remains)
INFO - root - 2017-12-07 08:44:22.647187: step 1240, loss = 21.62, batch loss = 21.53 (10.8 examples/sec; 0.741 sec/batch; 68h:10m:14s remains)
INFO - root - 2017-12-07 08:44:29.850739: step 1250, loss = 21.75, batch loss = 21.67 (11.5 examples/sec; 0.696 sec/batch; 64h:03m:18s remains)
INFO - root - 2017-12-07 08:44:37.110898: step 1260, loss = 21.48, batch loss = 21.40 (11.5 examples/sec; 0.695 sec/batch; 63h:54m:21s remains)
INFO - root - 2017-12-07 08:44:44.457967: step 1270, loss = 21.42, batch loss = 21.34 (11.7 examples/sec; 0.685 sec/batch; 63h:02m:41s remains)
INFO - root - 2017-12-07 08:44:51.874241: step 1280, loss = 21.90, batch loss = 21.82 (10.7 examples/sec; 0.748 sec/batch; 68h:47m:32s remains)
INFO - root - 2017-12-07 08:44:59.244231: step 1290, loss = 21.42, batch loss = 21.34 (11.5 examples/sec; 0.699 sec/batch; 64h:16m:35s remains)
INFO - root - 2017-12-07 08:45:06.600308: step 1300, loss = 21.49, batch loss = 21.41 (10.9 examples/sec; 0.736 sec/batch; 67h:42m:07s remains)
2017-12-07 08:45:07.368555: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4590607 -4.4265852 -4.3895631 -4.3593311 -4.3321066 -4.2758303 -4.1798391 -4.07926 -4.038805 -4.1140933 -4.2465339 -4.3807921 -4.4903088 -4.5432758 -4.5225263][-4.4871626 -4.4532332 -4.4125075 -4.3829603 -4.3646736 -4.3082128 -4.2019324 -4.0971384 -4.0607619 -4.1417217 -4.2703 -4.396666 -4.500526 -4.5550056 -4.53383][-4.4840097 -4.4551005 -4.4204936 -4.4041791 -4.4054236 -4.353756 -4.2349253 -4.1184139 -4.07975 -4.1650772 -4.2914133 -4.4082017 -4.5051894 -4.5598259 -4.5388913][-4.4618216 -4.435 -4.4097471 -4.41225 -4.4382734 -4.3968692 -4.2696271 -4.1362662 -4.0851073 -4.1713133 -4.3019037 -4.4177957 -4.513413 -4.5665746 -4.5427833][-4.4243474 -4.388762 -4.3708291 -4.3899221 -4.4383469 -4.4116716 -4.282886 -4.130486 -4.0573969 -4.1385894 -4.2779613 -4.40598 -4.5135951 -4.5710111 -4.5459566][-4.3510408 -4.2994618 -4.2777548 -4.300323 -4.3621368 -4.3573809 -4.2428646 -4.0855389 -3.9989455 -4.0767083 -4.2253318 -4.3707747 -4.49814 -4.5671611 -4.546505][-4.281208 -4.2091646 -4.1655045 -4.167048 -4.2200651 -4.2294936 -4.1345282 -3.9879825 -3.9093077 -3.9960508 -4.1577611 -4.3223648 -4.4697566 -4.5541339 -4.542685][-4.2606244 -4.169601 -4.095171 -4.059628 -4.083004 -4.0894604 -4.0051465 -3.8731914 -3.8148429 -3.919081 -4.0932455 -4.272902 -4.4355788 -4.5351977 -4.5355225][-4.2778921 -4.1848788 -4.0948892 -4.0329213 -4.0255833 -4.0184312 -3.9354498 -3.8172145 -3.7799857 -3.8953433 -4.0674944 -4.2452784 -4.4099579 -4.5176754 -4.5272741][-4.3057966 -4.2276697 -4.1412115 -4.0745716 -4.0528078 -4.0338435 -3.9519718 -3.8496213 -3.8360183 -3.9616585 -4.1223836 -4.2779536 -4.4215627 -4.5170875 -4.5240922][-4.3517628 -4.2962122 -4.2206707 -4.15695 -4.1262455 -4.0898557 -4.0047641 -3.9201891 -3.9362195 -4.0822134 -4.2390332 -4.368423 -4.4770889 -4.5419602 -4.5309811][-4.4183397 -4.38493 -4.3232594 -4.2649965 -4.2238488 -4.1634984 -4.0716543 -4.0050411 -4.0471387 -4.209877 -4.3652849 -4.4743996 -4.5527978 -4.585381 -4.5495105][-4.5040274 -4.4827595 -4.4326711 -4.3818197 -4.3357959 -4.2597103 -4.1663895 -4.1154952 -4.1732192 -4.3363481 -4.48117 -4.5695491 -4.6228466 -4.6278906 -4.5697389][-4.5730629 -4.5615973 -4.5241036 -4.4844232 -4.4433165 -4.3690839 -4.2868085 -4.2534223 -4.317729 -4.4655018 -4.5879917 -4.651803 -4.6776614 -4.6549082 -4.5783973][-4.5910316 -4.5869222 -4.5666728 -4.5448065 -4.520371 -4.4679155 -4.4116845 -4.3982253 -4.4604979 -4.5791855 -4.6696267 -4.7064562 -4.7029638 -4.6542892 -4.5676236]]...]
INFO - root - 2017-12-07 08:45:14.829148: step 1310, loss = 21.56, batch loss = 21.48 (11.0 examples/sec; 0.727 sec/batch; 66h:51m:40s remains)
INFO - root - 2017-12-07 08:45:22.259792: step 1320, loss = 21.65, batch loss = 21.57 (10.6 examples/sec; 0.754 sec/batch; 69h:20m:47s remains)
INFO - root - 2017-12-07 08:45:29.694859: step 1330, loss = 22.14, batch loss = 22.06 (11.7 examples/sec; 0.686 sec/batch; 63h:04m:23s remains)
INFO - root - 2017-12-07 08:45:37.059969: step 1340, loss = 21.37, batch loss = 21.29 (11.3 examples/sec; 0.710 sec/batch; 65h:16m:51s remains)
INFO - root - 2017-12-07 08:45:44.375515: step 1350, loss = 21.69, batch loss = 21.61 (10.8 examples/sec; 0.738 sec/batch; 67h:51m:32s remains)
INFO - root - 2017-12-07 08:45:51.645532: step 1360, loss = 21.46, batch loss = 21.38 (10.7 examples/sec; 0.749 sec/batch; 68h:55m:41s remains)
INFO - root - 2017-12-07 08:45:59.065761: step 1370, loss = 21.26, batch loss = 21.18 (11.3 examples/sec; 0.706 sec/batch; 64h:58m:44s remains)
INFO - root - 2017-12-07 08:46:06.577526: step 1380, loss = 21.31, batch loss = 21.23 (10.6 examples/sec; 0.756 sec/batch; 69h:34m:45s remains)
INFO - root - 2017-12-07 08:46:13.843636: step 1390, loss = 21.02, batch loss = 20.94 (10.7 examples/sec; 0.745 sec/batch; 68h:33m:43s remains)
INFO - root - 2017-12-07 08:46:21.226204: step 1400, loss = 21.23, batch loss = 21.14 (11.0 examples/sec; 0.726 sec/batch; 66h:48m:30s remains)
2017-12-07 08:46:22.097026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6847935 -4.6732683 -4.7042255 -4.752696 -4.7812161 -4.7870851 -4.7823658 -4.7969189 -4.8493133 -4.8897595 -4.8900127 -4.846138 -4.7572227 -4.6282368 -4.4931469][-4.629137 -4.6020851 -4.6519866 -4.7360778 -4.785923 -4.7965751 -4.7845697 -4.7995696 -4.87188 -4.9336348 -4.9503279 -4.9134841 -4.8129454 -4.6596417 -4.5067697][-4.4699411 -4.4039478 -4.4675064 -4.5994015 -4.6903214 -4.7139425 -4.69418 -4.7102094 -4.8008323 -4.8848858 -4.9295597 -4.9208832 -4.8269095 -4.6644325 -4.5061035][-4.244257 -4.125381 -4.1895809 -4.3643918 -4.5057235 -4.5488167 -4.5228767 -4.5410976 -4.6401663 -4.7432728 -4.8222575 -4.856317 -4.7911873 -4.644135 -4.4965005][-4.0302367 -3.8825088 -3.9481511 -4.1465182 -4.3179116 -4.3654051 -4.3222656 -4.3247347 -4.4096437 -4.5234327 -4.6406393 -4.7265234 -4.7095881 -4.6040573 -4.4828033][-3.920054 -3.7830257 -3.851099 -4.039609 -4.1953821 -4.2129641 -4.1356163 -4.107069 -4.1647768 -4.2773809 -4.42972 -4.5694613 -4.6129289 -4.564497 -4.4763937][-3.9492195 -3.840759 -3.9036231 -4.0524864 -4.1504788 -4.1178913 -4.012372 -3.9671435 -4.006032 -4.1071157 -4.2742662 -4.4521122 -4.5493078 -4.5502262 -4.48537][-4.095902 -4.0128512 -4.0559497 -4.1478462 -4.1811895 -4.1121445 -4.0062766 -3.9720576 -4.0023937 -4.07244 -4.2163653 -4.4015946 -4.5309782 -4.5590954 -4.5013423][-4.2414045 -4.1672425 -4.17671 -4.2085042 -4.1923265 -4.1151643 -4.0371056 -4.0366468 -4.0754061 -4.1214571 -4.2318974 -4.4030027 -4.5379505 -4.5703611 -4.509697][-4.3478713 -4.2669835 -4.2338185 -4.2041245 -4.147758 -4.0694242 -4.0214033 -4.0583086 -4.1252747 -4.179956 -4.2754536 -4.4255724 -4.5472245 -4.5704141 -4.5058308][-4.4545503 -4.3500462 -4.2646813 -4.1715288 -4.0763931 -3.9847403 -3.9459887 -4.0076241 -4.1130781 -4.2069664 -4.3155813 -4.4513044 -4.5518818 -4.5608354 -4.4936152][-4.5664482 -4.44835 -4.3157496 -4.1652083 -4.0257053 -3.9011209 -3.8399038 -3.90086 -4.0375676 -4.18548 -4.3312721 -4.4692988 -4.5532136 -4.5472717 -4.4787588][-4.6609631 -4.5734015 -4.427454 -4.2356629 -4.0439773 -3.8666086 -3.7631118 -3.8081939 -3.96861 -4.1717744 -4.3571343 -4.498589 -4.564095 -4.5408154 -4.4675312][-4.7105656 -4.6887474 -4.5798264 -4.3866162 -4.157578 -3.9272156 -3.7848082 -3.8209698 -4.003511 -4.2447414 -4.44528 -4.5667977 -4.6005974 -4.5506225 -4.4620562][-4.6986361 -4.741715 -4.7028394 -4.5591059 -4.3391314 -4.0889788 -3.9260027 -3.9545188 -4.1435041 -4.3915524 -4.5766897 -4.6589007 -4.6536045 -4.5729251 -4.4616294]]...]
INFO - root - 2017-12-07 08:46:29.573220: step 1410, loss = 21.46, batch loss = 21.37 (10.5 examples/sec; 0.760 sec/batch; 69h:56m:10s remains)
INFO - root - 2017-12-07 08:46:36.856060: step 1420, loss = 21.91, batch loss = 21.83 (10.6 examples/sec; 0.758 sec/batch; 69h:40m:52s remains)
INFO - root - 2017-12-07 08:46:44.182538: step 1430, loss = 21.44, batch loss = 21.35 (11.2 examples/sec; 0.717 sec/batch; 65h:55m:34s remains)
INFO - root - 2017-12-07 08:46:51.505218: step 1440, loss = 21.22, batch loss = 21.14 (11.2 examples/sec; 0.715 sec/batch; 65h:44m:04s remains)
INFO - root - 2017-12-07 08:46:58.833269: step 1450, loss = 21.45, batch loss = 21.37 (11.8 examples/sec; 0.681 sec/batch; 62h:36m:11s remains)
INFO - root - 2017-12-07 08:47:06.260985: step 1460, loss = 21.76, batch loss = 21.68 (11.4 examples/sec; 0.702 sec/batch; 64h:32m:03s remains)
INFO - root - 2017-12-07 08:47:13.728550: step 1470, loss = 21.53, batch loss = 21.45 (11.2 examples/sec; 0.715 sec/batch; 65h:45m:42s remains)
INFO - root - 2017-12-07 08:47:21.136845: step 1480, loss = 21.48, batch loss = 21.39 (10.7 examples/sec; 0.751 sec/batch; 69h:03m:25s remains)
INFO - root - 2017-12-07 08:47:28.504660: step 1490, loss = 21.84, batch loss = 21.75 (11.1 examples/sec; 0.720 sec/batch; 66h:10m:21s remains)
INFO - root - 2017-12-07 08:47:35.886732: step 1500, loss = 21.43, batch loss = 21.35 (10.5 examples/sec; 0.762 sec/batch; 70h:02m:41s remains)
2017-12-07 08:47:36.608654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4750662 -4.4631948 -4.4968629 -4.5482478 -4.5507832 -4.5384474 -4.5698137 -4.6526566 -4.7077212 -4.6407633 -4.5021114 -4.4472814 -4.5174937 -4.591897 -4.6231189][-4.2482843 -4.2820849 -4.3644156 -4.442873 -4.4342933 -4.3896489 -4.4008074 -4.4941669 -4.5792332 -4.5448027 -4.4263353 -4.3695774 -4.4317269 -4.5057864 -4.5400081][-4.0445156 -4.1323571 -4.2744431 -4.3796349 -4.3509412 -4.2647004 -4.2400341 -4.3154325 -4.4022932 -4.3978133 -4.326385 -4.2961807 -4.3603911 -4.4337435 -4.4737229][-4.0220551 -4.1148081 -4.2693725 -4.3689785 -4.3136125 -4.195116 -4.1424112 -4.1902733 -4.2586007 -4.2713661 -4.25249 -4.2710013 -4.3547611 -4.4297285 -4.4742227][-4.1672373 -4.221374 -4.3410516 -4.3999114 -4.30522 -4.1543555 -4.0751343 -4.092886 -4.1420708 -4.1716857 -4.2093406 -4.2940655 -4.4157939 -4.50259 -4.5515323][-4.329751 -4.3446646 -4.417109 -4.4268074 -4.2934074 -4.1132674 -4.0106463 -4.000607 -4.0340281 -4.0757265 -4.1536312 -4.2979865 -4.4669218 -4.5845575 -4.6469145][-4.4011135 -4.3933039 -4.426589 -4.3983717 -4.2393661 -4.0387712 -3.9175658 -3.8892231 -3.9190173 -3.9710276 -4.0609541 -4.2230968 -4.4165249 -4.5742316 -4.6700182][-4.3685508 -4.3643923 -4.383152 -4.3360143 -4.1628847 -3.945231 -3.8065658 -3.772433 -3.8126857 -3.8753123 -3.9461486 -4.0701733 -4.2452927 -4.4316072 -4.5738831][-4.2972283 -4.3193865 -4.3551311 -4.3147459 -4.1468921 -3.9227889 -3.772656 -3.7379251 -3.7826471 -3.8365269 -3.8613875 -3.9163392 -4.0474362 -4.2427206 -4.4240527][-4.2656789 -4.3068452 -4.3685713 -4.3607192 -4.2255125 -4.0180421 -3.8716648 -3.8467894 -3.8956637 -3.9338427 -3.9178581 -3.920938 -4.016892 -4.2030916 -4.3956633][-4.2653823 -4.30141 -4.3857017 -4.4267197 -4.3476791 -4.1804986 -4.054122 -4.0545473 -4.1212997 -4.1516795 -4.1088142 -4.0834823 -4.1600323 -4.3280582 -4.5062032][-4.275259 -4.2955694 -4.3894706 -4.4714861 -4.4532552 -4.340632 -4.2459812 -4.2692547 -4.3550324 -4.3813305 -4.3173003 -4.2734466 -4.3382921 -4.4858775 -4.63542][-4.3238997 -4.3249483 -4.4070292 -4.5018759 -4.526535 -4.4708085 -4.4140959 -4.4471021 -4.5303216 -4.5460439 -4.4694691 -4.4131327 -4.4638391 -4.58439 -4.6983247][-4.4067082 -4.3938303 -4.4483743 -4.5258822 -4.5673327 -4.5569572 -4.5365205 -4.5708132 -4.636426 -4.6440077 -4.5737 -4.5157123 -4.5476489 -4.6356549 -4.7133803][-4.4845285 -4.4673538 -4.4946551 -4.5429492 -4.5804448 -4.5946751 -4.6003232 -4.6333933 -4.6818314 -4.6895022 -4.6417494 -4.5960913 -4.6103129 -4.6614933 -4.6992536]]...]
INFO - root - 2017-12-07 08:47:44.011996: step 1510, loss = 21.68, batch loss = 21.60 (10.9 examples/sec; 0.737 sec/batch; 67h:46m:10s remains)
INFO - root - 2017-12-07 08:47:51.348103: step 1520, loss = 21.38, batch loss = 21.29 (11.8 examples/sec; 0.678 sec/batch; 62h:17m:57s remains)
INFO - root - 2017-12-07 08:47:58.631541: step 1530, loss = 21.13, batch loss = 21.04 (11.3 examples/sec; 0.705 sec/batch; 64h:51m:10s remains)
INFO - root - 2017-12-07 08:48:05.889403: step 1540, loss = 21.36, batch loss = 21.27 (11.3 examples/sec; 0.706 sec/batch; 64h:53m:44s remains)
INFO - root - 2017-12-07 08:48:13.341926: step 1550, loss = 21.45, batch loss = 21.36 (10.8 examples/sec; 0.744 sec/batch; 68h:21m:14s remains)
INFO - root - 2017-12-07 08:48:20.567086: step 1560, loss = 21.09, batch loss = 21.01 (11.1 examples/sec; 0.718 sec/batch; 66h:02m:08s remains)
INFO - root - 2017-12-07 08:48:27.856946: step 1570, loss = 21.56, batch loss = 21.48 (11.4 examples/sec; 0.701 sec/batch; 64h:24m:44s remains)
INFO - root - 2017-12-07 08:48:35.091067: step 1580, loss = 21.61, batch loss = 21.53 (10.9 examples/sec; 0.736 sec/batch; 67h:40m:18s remains)
INFO - root - 2017-12-07 08:48:42.506644: step 1590, loss = 21.39, batch loss = 21.30 (10.6 examples/sec; 0.754 sec/batch; 69h:20m:38s remains)
INFO - root - 2017-12-07 08:48:49.881425: step 1600, loss = 21.72, batch loss = 21.63 (10.1 examples/sec; 0.790 sec/batch; 72h:35m:59s remains)
2017-12-07 08:48:50.694773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5597367 -4.589334 -4.6307096 -4.6842508 -4.7173266 -4.71576 -4.6860113 -4.6442423 -4.6036491 -4.5770235 -4.5662127 -4.5619555 -4.5602522 -4.5511231 -4.5227046][-4.6888027 -4.7406049 -4.8022704 -4.862669 -4.8718681 -4.8174515 -4.7348938 -4.670063 -4.6460328 -4.6557345 -4.6708446 -4.6686954 -4.6479921 -4.61653 -4.5678778][-4.7705412 -4.8293333 -4.8920569 -4.9377322 -4.9037952 -4.7803168 -4.6387024 -4.5666981 -4.5940571 -4.6782808 -4.7480536 -4.7640438 -4.7299032 -4.6781163 -4.6090689][-4.7674274 -4.8035212 -4.8354783 -4.8403473 -4.7578826 -4.5796208 -4.3985896 -4.3401732 -4.4349437 -4.6067543 -4.749917 -4.8068 -4.7794828 -4.7244096 -4.6459074][-4.674438 -4.6587448 -4.6258364 -4.5648708 -4.43712 -4.2264705 -4.029757 -3.9965837 -4.1466894 -4.3817563 -4.59446 -4.7132487 -4.7267737 -4.6994371 -4.6359892][-4.5826931 -4.5259809 -4.4277549 -4.2920985 -4.101212 -3.8381953 -3.6035264 -3.5800898 -3.7661583 -4.0388079 -4.3119788 -4.5076156 -4.5917225 -4.6151834 -4.5813189][-4.584496 -4.5310507 -4.4033866 -4.2086425 -3.9423003 -3.5990491 -3.3025913 -3.2745893 -3.4829912 -3.7696862 -4.0753069 -4.3302565 -4.4812379 -4.5478759 -4.5337281][-4.697155 -4.6859207 -4.5695257 -4.3536525 -4.04801 -3.6636086 -3.3395705 -3.3011289 -3.5019946 -3.774194 -4.0727482 -4.3423705 -4.5181227 -4.5874891 -4.5554967][-4.8379855 -4.871748 -4.7871909 -4.5874672 -4.3010674 -3.95509 -3.6717162 -3.6344986 -3.798244 -4.0266786 -4.2810659 -4.5187039 -4.6718578 -4.7069321 -4.6355448][-4.9032016 -4.9535513 -4.8984184 -4.7358446 -4.5145173 -4.2752571 -4.0897894 -4.0659704 -4.1782293 -4.3453126 -4.5298777 -4.7024035 -4.8065443 -4.8053308 -4.7097607][-4.8659225 -4.9108787 -4.8861947 -4.7844629 -4.64912 -4.5297832 -4.4459395 -4.4353642 -4.502058 -4.6094522 -4.7218728 -4.8197803 -4.8698316 -4.8466368 -4.7515411][-4.7790375 -4.8256783 -4.84099 -4.8107061 -4.750443 -4.70312 -4.67197 -4.6584444 -4.6933136 -4.7650061 -4.8297024 -4.8679972 -4.870131 -4.83303 -4.7497768][-4.6799974 -4.7249346 -4.7651639 -4.7849956 -4.7717628 -4.7509775 -4.7297316 -4.703979 -4.7151246 -4.7615805 -4.7984743 -4.804965 -4.78544 -4.7515931 -4.6894469][-4.567646 -4.598495 -4.6383486 -4.674839 -4.685101 -4.6781716 -4.6593966 -4.6281071 -4.6171694 -4.6333671 -4.6485381 -4.6472325 -4.6344876 -4.6209116 -4.5889311][-4.4933071 -4.5108633 -4.5334754 -4.5566578 -4.5657611 -4.5649137 -4.5551567 -4.5366564 -4.5216041 -4.5139909 -4.5080085 -4.5010424 -4.496448 -4.4971771 -4.4856386]]...]
INFO - root - 2017-12-07 08:48:58.079764: step 1610, loss = 21.77, batch loss = 21.69 (10.7 examples/sec; 0.749 sec/batch; 68h:49m:06s remains)
INFO - root - 2017-12-07 08:49:05.467815: step 1620, loss = 21.69, batch loss = 21.61 (10.8 examples/sec; 0.737 sec/batch; 67h:46m:38s remains)
INFO - root - 2017-12-07 08:49:12.789758: step 1630, loss = 21.52, batch loss = 21.43 (10.4 examples/sec; 0.773 sec/batch; 71h:00m:19s remains)
INFO - root - 2017-12-07 08:49:20.127175: step 1640, loss = 21.71, batch loss = 21.63 (10.8 examples/sec; 0.741 sec/batch; 68h:03m:39s remains)
INFO - root - 2017-12-07 08:49:27.458121: step 1650, loss = 21.36, batch loss = 21.27 (10.8 examples/sec; 0.741 sec/batch; 68h:07m:04s remains)
INFO - root - 2017-12-07 08:49:34.884217: step 1660, loss = 21.22, batch loss = 21.13 (11.5 examples/sec; 0.698 sec/batch; 64h:09m:53s remains)
INFO - root - 2017-12-07 08:49:42.211289: step 1670, loss = 21.62, batch loss = 21.54 (10.8 examples/sec; 0.740 sec/batch; 67h:57m:49s remains)
INFO - root - 2017-12-07 08:49:49.582027: step 1680, loss = 21.63, batch loss = 21.54 (11.4 examples/sec; 0.704 sec/batch; 64h:42m:16s remains)
INFO - root - 2017-12-07 08:49:56.921771: step 1690, loss = 21.20, batch loss = 21.11 (11.8 examples/sec; 0.681 sec/batch; 62h:33m:13s remains)
INFO - root - 2017-12-07 08:50:04.429769: step 1700, loss = 21.70, batch loss = 21.61 (11.3 examples/sec; 0.705 sec/batch; 64h:46m:31s remains)
2017-12-07 08:50:05.273933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3387232 -4.3472738 -4.3277431 -4.2895012 -4.2578506 -4.2362394 -4.2159996 -4.2059751 -4.2155542 -4.2266827 -4.2193937 -4.1945763 -4.1672416 -4.1623249 -4.1995363][-4.3597121 -4.4019279 -4.409658 -4.3823175 -4.349544 -4.3221307 -4.29383 -4.2712231 -4.2737126 -4.2973256 -4.3042374 -4.2849064 -4.2564063 -4.2443304 -4.2636452][-4.3850121 -4.4434114 -4.460824 -4.4343882 -4.398119 -4.3660283 -4.333787 -4.3094249 -4.3265257 -4.389061 -4.43457 -4.4318995 -4.4002767 -4.3685832 -4.3554955][-4.4281092 -4.4833393 -4.4835606 -4.4335489 -4.3787322 -4.3395782 -4.3094883 -4.2947893 -4.3422513 -4.4545474 -4.5467372 -4.5672975 -4.5383191 -4.49047 -4.4487782][-4.4979062 -4.5451331 -4.5149832 -4.4180045 -4.3184915 -4.2507906 -4.2038307 -4.1853981 -4.2601347 -4.4226713 -4.5584278 -4.6042738 -4.5942879 -4.5572858 -4.5150175][-4.5344963 -4.5687323 -4.5110121 -4.3742027 -4.2358675 -4.1320844 -4.0405841 -3.9782276 -4.05266 -4.2535233 -4.4288006 -4.5017266 -4.5199351 -4.5090389 -4.4853153][-4.519217 -4.533493 -4.4619222 -4.313067 -4.1584291 -4.0205011 -3.8634007 -3.7254441 -3.7598414 -3.9683123 -4.1695676 -4.2692466 -4.3187709 -4.3435531 -4.3527603][-4.4781594 -4.47297 -4.3973737 -4.259666 -4.1221123 -3.9843771 -3.8009112 -3.6155264 -3.5922852 -3.753824 -3.9318457 -4.0403638 -4.1166635 -4.1769247 -4.2257318][-4.4127841 -4.4084024 -4.3505206 -4.255918 -4.1735396 -4.0813603 -3.9320142 -3.7568991 -3.6859412 -3.7512646 -3.8505962 -3.9359388 -4.0256186 -4.1105213 -4.1894522][-4.3306527 -4.3286939 -4.2977238 -4.2663989 -4.2530303 -4.2171059 -4.1256924 -3.9989471 -3.9127016 -3.8997843 -3.9215536 -3.9744685 -4.06145 -4.1481862 -4.2271538][-4.2881083 -4.2760873 -4.249331 -4.2537751 -4.2860231 -4.2887821 -4.2442131 -4.171526 -4.1057992 -4.0755286 -4.0633025 -4.0934982 -4.1763687 -4.2605743 -4.323483][-4.3045697 -4.2815247 -4.2381048 -4.2388825 -4.2883258 -4.3131146 -4.2986684 -4.2659149 -4.22783 -4.2042232 -4.1811538 -4.1973453 -4.2749081 -4.3550491 -4.3988352][-4.3615932 -4.3426337 -4.2885141 -4.2720895 -4.3141203 -4.3457136 -4.3456078 -4.3341351 -4.3165588 -4.3021822 -4.2786326 -4.2864738 -4.3401504 -4.3859243 -4.3913689][-4.4073696 -4.4023647 -4.3539987 -4.3289328 -4.3570442 -4.3882203 -4.398509 -4.4041963 -4.4047914 -4.3963628 -4.3745475 -4.3742771 -4.3987026 -4.40204 -4.3680816][-4.4025955 -4.4044752 -4.3735881 -4.3633938 -4.3885951 -4.4173403 -4.4368668 -4.4560943 -4.4721489 -4.4654808 -4.4380946 -4.4263687 -4.4347138 -4.4271026 -4.3855562]]...]
INFO - root - 2017-12-07 08:50:12.688211: step 1710, loss = 21.36, batch loss = 21.28 (10.5 examples/sec; 0.765 sec/batch; 70h:19m:19s remains)
INFO - root - 2017-12-07 08:50:20.037681: step 1720, loss = 21.58, batch loss = 21.50 (11.6 examples/sec; 0.691 sec/batch; 63h:31m:01s remains)
INFO - root - 2017-12-07 08:50:27.312779: step 1730, loss = 21.43, batch loss = 21.35 (11.3 examples/sec; 0.707 sec/batch; 64h:56m:23s remains)
INFO - root - 2017-12-07 08:50:34.350030: step 1740, loss = 21.48, batch loss = 21.40 (10.6 examples/sec; 0.753 sec/batch; 69h:12m:48s remains)
INFO - root - 2017-12-07 08:50:41.797271: step 1750, loss = 21.82, batch loss = 21.74 (10.4 examples/sec; 0.767 sec/batch; 70h:26m:55s remains)
INFO - root - 2017-12-07 08:50:49.216329: step 1760, loss = 21.62, batch loss = 21.54 (10.9 examples/sec; 0.737 sec/batch; 67h:40m:12s remains)
INFO - root - 2017-12-07 08:50:56.540263: step 1770, loss = 21.34, batch loss = 21.26 (10.6 examples/sec; 0.753 sec/batch; 69h:12m:12s remains)
INFO - root - 2017-12-07 08:51:03.921274: step 1780, loss = 21.84, batch loss = 21.76 (10.9 examples/sec; 0.737 sec/batch; 67h:42m:30s remains)
INFO - root - 2017-12-07 08:51:11.296647: step 1790, loss = 21.55, batch loss = 21.47 (11.3 examples/sec; 0.709 sec/batch; 65h:09m:23s remains)
INFO - root - 2017-12-07 08:51:18.733763: step 1800, loss = 21.41, batch loss = 21.33 (11.3 examples/sec; 0.706 sec/batch; 64h:48m:51s remains)
2017-12-07 08:51:19.586542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3760653 -4.4093618 -4.463798 -4.444293 -4.3533864 -4.277545 -4.2446079 -4.2683921 -4.3180385 -4.348196 -4.3343172 -4.2910876 -4.278832 -4.3292103 -4.366509][-4.3876677 -4.4270658 -4.4882126 -4.4847212 -4.4102545 -4.3409243 -4.3027606 -4.3150187 -4.348845 -4.3707066 -4.3578191 -4.3229036 -4.3181539 -4.3553925 -4.3742671][-4.3929958 -4.4336619 -4.4946647 -4.4989305 -4.4344311 -4.3726873 -4.3398223 -4.3483157 -4.3750119 -4.393167 -4.3721509 -4.3354979 -4.3321304 -4.3508363 -4.3532443][-4.3911376 -4.4255514 -4.483602 -4.4866557 -4.4170437 -4.3504362 -4.3136492 -4.3105597 -4.328558 -4.3450441 -4.3239284 -4.3008151 -4.3138671 -4.333591 -4.335165][-4.3819976 -4.4103117 -4.4668612 -4.4611688 -4.3760018 -4.2950606 -4.2436261 -4.2190137 -4.2253141 -4.2383256 -4.223835 -4.2254691 -4.2651954 -4.2983747 -4.3120379][-4.3755469 -4.4030595 -4.4595003 -4.4420033 -4.3406291 -4.2462378 -4.1770616 -4.1309891 -4.1276188 -4.1339765 -4.1179223 -4.1389995 -4.2045393 -4.2544208 -4.2867107][-4.3752446 -4.4079461 -4.4685426 -4.4461374 -4.3389621 -4.2335072 -4.1403151 -4.0730815 -4.0599747 -4.0544667 -4.0293279 -4.0642881 -4.1476278 -4.2150841 -4.2648168][-4.3748631 -4.4162817 -4.4852762 -4.4716749 -4.3694782 -4.2504091 -4.1279535 -4.0454559 -4.0319147 -4.0231485 -3.9973533 -4.0418935 -4.1310768 -4.2079868 -4.2646556][-4.3715968 -4.4158111 -4.4922171 -4.4944639 -4.4045339 -4.2769575 -4.1411171 -4.0647054 -4.0596247 -4.0537357 -4.0382271 -4.0868597 -4.1687546 -4.24621 -4.30547][-4.3685837 -4.4075613 -4.4831581 -4.4978323 -4.423326 -4.2998385 -4.1697783 -4.1085267 -4.1049576 -4.0962734 -4.0963264 -4.14993 -4.2194519 -4.292367 -4.346849][-4.3619757 -4.3950067 -4.4630661 -4.4852834 -4.4274211 -4.3144069 -4.19821 -4.1471605 -4.135859 -4.1266794 -4.1532078 -4.2192669 -4.287611 -4.3584809 -4.3973374][-4.35352 -4.3804288 -4.4407864 -4.4658008 -4.4191308 -4.3155165 -4.2125463 -4.1668077 -4.1508574 -4.150094 -4.199698 -4.2795711 -4.3589869 -4.4306979 -4.4514632][-4.3437033 -4.366786 -4.4190254 -4.4401093 -4.394033 -4.2954292 -4.2041497 -4.1667681 -4.1583557 -4.168612 -4.2268257 -4.31644 -4.406517 -4.4724674 -4.4708848][-4.3362308 -4.35407 -4.3958836 -4.4063649 -4.350842 -4.2521467 -4.1695123 -4.1469913 -4.1600146 -4.1875215 -4.252399 -4.3516665 -4.4466062 -4.4938607 -4.4647307][-4.3341131 -4.3445745 -4.3752618 -4.37028 -4.2969317 -4.1887059 -4.1061811 -4.0973425 -4.136755 -4.1870518 -4.2663422 -4.3762879 -4.4719577 -4.5017886 -4.4533772]]...]
INFO - root - 2017-12-07 08:51:27.042092: step 1810, loss = 21.49, batch loss = 21.40 (10.2 examples/sec; 0.787 sec/batch; 72h:19m:43s remains)
INFO - root - 2017-12-07 08:51:34.388448: step 1820, loss = 21.22, batch loss = 21.13 (11.2 examples/sec; 0.713 sec/batch; 65h:30m:35s remains)
INFO - root - 2017-12-07 08:51:41.831804: step 1830, loss = 21.47, batch loss = 21.39 (10.9 examples/sec; 0.736 sec/batch; 67h:37m:08s remains)
INFO - root - 2017-12-07 08:51:49.214149: step 1840, loss = 21.34, batch loss = 21.26 (11.0 examples/sec; 0.726 sec/batch; 66h:38m:44s remains)
INFO - root - 2017-12-07 08:51:56.498784: step 1850, loss = 21.26, batch loss = 21.18 (11.3 examples/sec; 0.705 sec/batch; 64h:46m:49s remains)
INFO - root - 2017-12-07 08:52:03.899037: step 1860, loss = 20.94, batch loss = 20.85 (11.3 examples/sec; 0.709 sec/batch; 65h:09m:10s remains)
INFO - root - 2017-12-07 08:52:11.356597: step 1870, loss = 21.96, batch loss = 21.88 (10.7 examples/sec; 0.746 sec/batch; 68h:29m:43s remains)
INFO - root - 2017-12-07 08:52:18.684341: step 1880, loss = 21.86, batch loss = 21.78 (11.0 examples/sec; 0.728 sec/batch; 66h:52m:50s remains)
INFO - root - 2017-12-07 08:52:25.908044: step 1890, loss = 21.55, batch loss = 21.47 (10.9 examples/sec; 0.733 sec/batch; 67h:20m:21s remains)
INFO - root - 2017-12-07 08:52:33.196310: step 1900, loss = 21.14, batch loss = 21.05 (10.2 examples/sec; 0.788 sec/batch; 72h:20m:39s remains)
2017-12-07 08:52:33.925763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.393868 -4.3201261 -4.2982168 -4.3217139 -4.3701582 -4.4361544 -4.500566 -4.5388513 -4.5546112 -4.5460892 -4.5149922 -4.4897666 -4.4889827 -4.4923449 -4.4784026][-4.45765 -4.391191 -4.3708735 -4.3982706 -4.447813 -4.5099893 -4.5765414 -4.6253958 -4.6512418 -4.6448097 -4.6147461 -4.6023726 -4.6182594 -4.6360397 -4.6286917][-4.5216746 -4.47136 -4.4484415 -4.4614258 -4.4882154 -4.5214577 -4.5608606 -4.5938044 -4.6150904 -4.6169095 -4.6050062 -4.6136584 -4.6437383 -4.6689167 -4.6667061][-4.5577631 -4.5285263 -4.493526 -4.4686403 -4.4465618 -4.422245 -4.4004192 -4.3830533 -4.3878746 -4.4161773 -4.4574289 -4.5131497 -4.5660586 -4.5956111 -4.5933661][-4.5546517 -4.5360947 -4.4873519 -4.4240422 -4.3523784 -4.2678981 -4.1711092 -4.0818853 -4.0604682 -4.1293869 -4.2487731 -4.3686824 -4.4477334 -4.4728417 -4.4591966][-4.5351553 -4.5194216 -4.4591203 -4.3664694 -4.2523651 -4.11637 -3.9509964 -3.7875953 -3.733887 -3.838666 -4.0292053 -4.2067595 -4.3061228 -4.3187017 -4.2886124][-4.4959879 -4.482873 -4.4224648 -4.3193822 -4.18482 -4.0237284 -3.8204479 -3.6097474 -3.528796 -3.6501343 -3.8860657 -4.1098795 -4.2347021 -4.2452655 -4.2051935][-4.4301324 -4.4291229 -4.3928466 -4.3192306 -4.2094522 -4.0674238 -3.8697538 -3.6492677 -3.5486424 -3.6578329 -3.8955667 -4.1327105 -4.2714219 -4.2826509 -4.2359858][-4.3654914 -4.3794742 -4.3770857 -4.357636 -4.3057079 -4.2123857 -4.0506616 -3.8524342 -3.7488484 -3.8311715 -4.0334258 -4.2380476 -4.3584824 -4.3601193 -4.3068213][-4.330143 -4.3505235 -4.3752441 -4.4145064 -4.4334364 -4.404995 -4.2993164 -4.1470833 -4.0610518 -4.1193156 -4.2698913 -4.41138 -4.4832873 -4.457531 -4.384696][-4.354815 -4.3780117 -4.4193144 -4.4994678 -4.5717854 -4.594162 -4.5399628 -4.4367132 -4.3705926 -4.4016423 -4.4887938 -4.5552559 -4.5706882 -4.5213079 -4.4399219][-4.4171185 -4.4362221 -4.4784589 -4.5669403 -4.6520739 -4.6902242 -4.6667385 -4.6034718 -4.5555511 -4.5586491 -4.5841231 -4.5857062 -4.5610843 -4.5122652 -4.450532][-4.4504781 -4.4642711 -4.4967003 -4.563499 -4.62249 -4.6454163 -4.6343622 -4.6010513 -4.5707197 -4.5569391 -4.5428162 -4.5116043 -4.4774575 -4.4488292 -4.4199705][-4.4300237 -4.4452066 -4.4662466 -4.4971228 -4.5105391 -4.5041623 -4.4980226 -4.4927897 -4.4838753 -4.4649978 -4.4299855 -4.3901653 -4.37034 -4.371419 -4.3741412][-4.3427396 -4.3623395 -4.3749866 -4.3754382 -4.352479 -4.3275552 -4.3344345 -4.3649 -4.3890352 -4.3813815 -4.3414373 -4.3045597 -4.3022251 -4.3289485 -4.3560853]]...]
INFO - root - 2017-12-07 08:52:41.343912: step 1910, loss = 21.41, batch loss = 21.33 (10.9 examples/sec; 0.732 sec/batch; 67h:15m:00s remains)
INFO - root - 2017-12-07 08:52:48.703674: step 1920, loss = 21.28, batch loss = 21.20 (10.8 examples/sec; 0.743 sec/batch; 68h:12m:13s remains)
INFO - root - 2017-12-07 08:52:56.041149: step 1930, loss = 21.44, batch loss = 21.35 (12.0 examples/sec; 0.664 sec/batch; 60h:59m:57s remains)
INFO - root - 2017-12-07 08:53:03.386602: step 1940, loss = 21.43, batch loss = 21.35 (11.3 examples/sec; 0.707 sec/batch; 64h:52m:35s remains)
INFO - root - 2017-12-07 08:53:10.781741: step 1950, loss = 21.35, batch loss = 21.27 (10.9 examples/sec; 0.733 sec/batch; 67h:18m:21s remains)
INFO - root - 2017-12-07 08:53:18.199539: step 1960, loss = 21.35, batch loss = 21.26 (11.6 examples/sec; 0.687 sec/batch; 63h:03m:54s remains)
INFO - root - 2017-12-07 08:53:25.611632: step 1970, loss = 21.38, batch loss = 21.30 (11.1 examples/sec; 0.718 sec/batch; 65h:55m:23s remains)
INFO - root - 2017-12-07 08:53:32.852523: step 1980, loss = 21.41, batch loss = 21.33 (11.2 examples/sec; 0.714 sec/batch; 65h:32m:26s remains)
INFO - root - 2017-12-07 08:53:40.260176: step 1990, loss = 21.61, batch loss = 21.53 (11.3 examples/sec; 0.708 sec/batch; 65h:00m:19s remains)
INFO - root - 2017-12-07 08:53:47.532125: step 2000, loss = 21.36, batch loss = 21.28 (11.4 examples/sec; 0.700 sec/batch; 64h:13m:20s remains)
2017-12-07 08:53:48.325146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3482137 -4.3534155 -4.3636622 -4.3760705 -4.3891163 -4.4053111 -4.4271512 -4.4438267 -4.4561348 -4.4704733 -4.4918132 -4.5196676 -4.5409932 -4.540616 -4.5109711][-4.3626404 -4.3716311 -4.3855443 -4.4030046 -4.4188085 -4.4362721 -4.4568768 -4.4643607 -4.463501 -4.4691124 -4.4895968 -4.526351 -4.5632782 -4.5771914 -4.5540171][-4.3817077 -4.3909631 -4.4035463 -4.4184184 -4.4288497 -4.4372272 -4.4436078 -4.4284124 -4.403717 -4.3942957 -4.4073358 -4.4460306 -4.4963861 -4.5312066 -4.5323305][-4.4060359 -4.4104123 -4.4151449 -4.4166183 -4.4096045 -4.3967204 -4.3767214 -4.3346529 -4.2926807 -4.2789421 -4.2934361 -4.3356366 -4.3966565 -4.4487467 -4.4762707][-4.425632 -4.4220762 -4.4098191 -4.3854332 -4.3504376 -4.3096595 -4.2618275 -4.1980233 -4.1498742 -4.1429429 -4.1691628 -4.2264981 -4.3078203 -4.3808651 -4.4314051][-4.422936 -4.415287 -4.381043 -4.3171463 -4.2457857 -4.1743813 -4.1047106 -4.0287066 -3.9811027 -3.9857557 -4.0303445 -4.1189408 -4.2402463 -4.346571 -4.4177127][-4.3994117 -4.3999457 -4.3412066 -4.2252059 -4.1090755 -4.0094428 -3.9366055 -3.8691275 -3.8409643 -3.8719261 -3.9421804 -4.0653281 -4.2264948 -4.3642206 -4.4443793][-4.3619862 -4.3812003 -4.3044991 -4.1416597 -3.9897635 -3.8792722 -3.8255439 -3.7815585 -3.7826223 -3.8478603 -3.9490395 -4.1009083 -4.2848306 -4.4340806 -4.5015321][-4.3282671 -4.3718133 -4.2957687 -4.1147342 -3.9511418 -3.8498998 -3.8256814 -3.811029 -3.841825 -3.935878 -4.0658011 -4.2309933 -4.4060788 -4.5323229 -4.5611086][-4.3122439 -4.3651781 -4.3058991 -4.1469965 -4.0081658 -3.9355245 -3.9401007 -3.9557316 -4.0142179 -4.1262527 -4.2657919 -4.4149423 -4.5456848 -4.6193285 -4.59813][-4.3218441 -4.3715029 -4.3447089 -4.2471952 -4.1652637 -4.13318 -4.1558304 -4.1833181 -4.2410588 -4.3385696 -4.4567556 -4.5671234 -4.6399221 -4.6545072 -4.5933094][-4.3742442 -4.4166861 -4.4170218 -4.3774538 -4.3470216 -4.34523 -4.3713875 -4.3924527 -4.42704 -4.4838681 -4.5574684 -4.6195064 -4.6434693 -4.6185846 -4.5436435][-4.4466209 -4.476697 -4.4778 -4.4606552 -4.4522367 -4.4644227 -4.4878578 -4.5001807 -4.5125608 -4.5290904 -4.5566912 -4.5793076 -4.5770073 -4.5419478 -4.4784122][-4.482264 -4.5046306 -4.5001125 -4.4868574 -4.4819174 -4.4897561 -4.4990749 -4.4958086 -4.4881711 -4.4807959 -4.4842215 -4.4904842 -4.4853339 -4.4601054 -4.4189396][-4.4559278 -4.472672 -4.4686723 -4.4565449 -4.4452052 -4.4380574 -4.4307518 -4.4168468 -4.4027424 -4.391921 -4.3910804 -4.3957219 -4.3968821 -4.3878231 -4.3696904]]...]
INFO - root - 2017-12-07 08:53:55.779609: step 2010, loss = 21.33, batch loss = 21.25 (10.7 examples/sec; 0.747 sec/batch; 68h:31m:57s remains)
INFO - root - 2017-12-07 08:54:03.066399: step 2020, loss = 21.41, batch loss = 21.33 (11.4 examples/sec; 0.701 sec/batch; 64h:22m:39s remains)
INFO - root - 2017-12-07 08:54:10.485766: step 2030, loss = 21.29, batch loss = 21.20 (10.6 examples/sec; 0.752 sec/batch; 69h:01m:38s remains)
INFO - root - 2017-12-07 08:54:17.943659: step 2040, loss = 21.63, batch loss = 21.55 (10.5 examples/sec; 0.760 sec/batch; 69h:47m:03s remains)
INFO - root - 2017-12-07 08:54:25.108174: step 2050, loss = 21.17, batch loss = 21.09 (11.0 examples/sec; 0.725 sec/batch; 66h:33m:29s remains)
INFO - root - 2017-12-07 08:54:32.378746: step 2060, loss = 21.67, batch loss = 21.58 (10.4 examples/sec; 0.766 sec/batch; 70h:17m:46s remains)
INFO - root - 2017-12-07 08:54:39.665835: step 2070, loss = 21.49, batch loss = 21.40 (10.6 examples/sec; 0.758 sec/batch; 69h:34m:31s remains)
INFO - root - 2017-12-07 08:54:47.099114: step 2080, loss = 21.51, batch loss = 21.42 (10.7 examples/sec; 0.748 sec/batch; 68h:39m:29s remains)
INFO - root - 2017-12-07 08:54:54.470891: step 2090, loss = 21.44, batch loss = 21.36 (10.1 examples/sec; 0.794 sec/batch; 72h:52m:27s remains)
INFO - root - 2017-12-07 08:55:01.729532: step 2100, loss = 21.74, batch loss = 21.66 (10.1 examples/sec; 0.792 sec/batch; 72h:40m:18s remains)
2017-12-07 08:55:02.518487: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3413987 -4.3019891 -4.2861233 -4.3274908 -4.392158 -4.4064641 -4.3795381 -4.3604555 -4.3510704 -4.3667755 -4.4313254 -4.5396032 -4.6330848 -4.6648312 -4.6082792][-4.3734365 -4.3906488 -4.4304104 -4.4858732 -4.5191188 -4.4862247 -4.4304123 -4.427177 -4.4645615 -4.5193324 -4.5886536 -4.6660419 -4.7126751 -4.7095933 -4.6437979][-4.4098988 -4.4601617 -4.5340548 -4.6005344 -4.6070957 -4.5303307 -4.4440479 -4.4483118 -4.5240264 -4.6106434 -4.6789851 -4.7262893 -4.7333384 -4.701581 -4.6257896][-4.421339 -4.4551458 -4.5176187 -4.5742888 -4.5613012 -4.4555058 -4.3488526 -4.3601794 -4.4696617 -4.5948238 -4.6838713 -4.7334385 -4.736382 -4.6957512 -4.611001][-4.4276891 -4.423367 -4.445581 -4.4742351 -4.4366217 -4.304852 -4.1843753 -4.2026095 -4.3365846 -4.4917459 -4.6068406 -4.6802487 -4.7076097 -4.6841025 -4.6095853][-4.4457612 -4.432272 -4.429493 -4.4259558 -4.3445029 -4.1641536 -4.0109048 -4.0203815 -4.1624355 -4.3354492 -4.4794021 -4.5881271 -4.6520586 -4.6621952 -4.6184459][-4.463335 -4.488821 -4.4909997 -4.4617519 -4.3284883 -4.0851436 -3.87502 -3.8481951 -3.9823282 -4.1683378 -4.345902 -4.4988556 -4.5982571 -4.6370139 -4.6220984][-4.4517121 -4.5317903 -4.56137 -4.5317621 -4.3826652 -4.1121674 -3.8597505 -3.7810252 -3.8791614 -4.0544887 -4.243784 -4.4248652 -4.5463362 -4.5994639 -4.5989242][-4.3893089 -4.4980993 -4.5547628 -4.5496769 -4.4300861 -4.188314 -3.9383528 -3.8231707 -3.870996 -4.0072017 -4.1771955 -4.3595562 -4.4893556 -4.5466738 -4.5515442][-4.3339105 -4.4391971 -4.4986777 -4.5009952 -4.4094515 -4.2170634 -4.0036035 -3.8850875 -3.9008732 -3.9995012 -4.1384444 -4.3000245 -4.424036 -4.4803019 -4.4924207][-4.3826189 -4.467135 -4.4943662 -4.4624534 -4.3643146 -4.2006173 -4.0223713 -3.9212546 -3.9347196 -4.0207596 -4.133636 -4.2523065 -4.3405571 -4.3807926 -4.4066439][-4.5024047 -4.5619049 -4.5434475 -4.467453 -4.3537545 -4.2025938 -4.0446568 -3.9589598 -3.989769 -4.0932903 -4.1982994 -4.2658348 -4.283823 -4.271708 -4.2900934][-4.5867796 -4.6277018 -4.5847983 -4.4974933 -4.3973861 -4.2729759 -4.1391449 -4.073276 -4.1250896 -4.2427869 -4.3347559 -4.3464189 -4.2773566 -4.1819892 -4.1563568][-4.594759 -4.6378264 -4.6087708 -4.5468121 -4.4774365 -4.387219 -4.2898111 -4.2554884 -4.3234334 -4.4326 -4.4949512 -4.4529562 -4.304482 -4.124114 -4.0396075][-4.5403662 -4.5979948 -4.6107955 -4.5903473 -4.5466781 -4.4820037 -4.4252253 -4.4301372 -4.5090609 -4.5979142 -4.6332593 -4.5612655 -4.367465 -4.1322737 -4.0028648]]...]
INFO - root - 2017-12-07 08:55:09.783288: step 2110, loss = 21.25, batch loss = 21.17 (10.1 examples/sec; 0.795 sec/batch; 72h:57m:37s remains)
INFO - root - 2017-12-07 08:55:17.066063: step 2120, loss = 21.47, batch loss = 21.38 (10.7 examples/sec; 0.747 sec/batch; 68h:34m:14s remains)
INFO - root - 2017-12-07 08:55:24.396035: step 2130, loss = 21.16, batch loss = 21.08 (11.0 examples/sec; 0.727 sec/batch; 66h:40m:18s remains)
INFO - root - 2017-12-07 08:55:31.631165: step 2140, loss = 21.67, batch loss = 21.59 (11.8 examples/sec; 0.676 sec/batch; 62h:03m:51s remains)
INFO - root - 2017-12-07 08:55:38.925879: step 2150, loss = 21.56, batch loss = 21.48 (10.6 examples/sec; 0.758 sec/batch; 69h:32m:33s remains)
INFO - root - 2017-12-07 08:55:46.208334: step 2160, loss = 21.41, batch loss = 21.33 (11.2 examples/sec; 0.714 sec/batch; 65h:30m:19s remains)
INFO - root - 2017-12-07 08:55:53.545147: step 2170, loss = 21.67, batch loss = 21.58 (11.3 examples/sec; 0.706 sec/batch; 64h:46m:32s remains)
INFO - root - 2017-12-07 08:56:00.916724: step 2180, loss = 21.41, batch loss = 21.33 (10.5 examples/sec; 0.764 sec/batch; 70h:03m:47s remains)
INFO - root - 2017-12-07 08:56:08.287830: step 2190, loss = 21.63, batch loss = 21.55 (11.2 examples/sec; 0.711 sec/batch; 65h:14m:56s remains)
INFO - root - 2017-12-07 08:56:15.586574: step 2200, loss = 21.54, batch loss = 21.46 (11.3 examples/sec; 0.710 sec/batch; 65h:07m:07s remains)
2017-12-07 08:56:16.403395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7314415 -4.734015 -4.67334 -4.5727053 -4.4705334 -4.4196029 -4.4497986 -4.5296612 -4.5866408 -4.6075792 -4.6237025 -4.6458535 -4.6679692 -4.6883411 -4.7019444][-4.7019572 -4.7106309 -4.6550465 -4.55409 -4.4524245 -4.3982968 -4.4223762 -4.4978552 -4.5606441 -4.610971 -4.6739178 -4.7492852 -4.8109536 -4.84531 -4.8451409][-4.7007871 -4.7300997 -4.6719208 -4.5441618 -4.4084296 -4.3179255 -4.3146834 -4.3845468 -4.4695277 -4.5730195 -4.7027812 -4.8370395 -4.9341516 -4.9707508 -4.9391484][-4.7326627 -4.7818308 -4.7100973 -4.5398235 -4.350872 -4.2048388 -4.1612082 -4.2230721 -4.3417363 -4.5063753 -4.6979423 -4.8788104 -4.9953904 -5.0192084 -4.9464126][-4.7661009 -4.8231683 -4.7232 -4.4976964 -4.2448287 -4.0371056 -3.953289 -4.0128636 -4.172688 -4.3980484 -4.646699 -4.8665795 -4.9933834 -4.9913611 -4.863503][-4.7796245 -4.8252029 -4.6857886 -4.4051552 -4.0934877 -3.8311863 -3.7067707 -3.7620504 -3.9607124 -4.2474895 -4.5574226 -4.8202844 -4.9553847 -4.920249 -4.7287107][-4.7824454 -4.8023376 -4.62947 -4.3150482 -3.9716997 -3.6804626 -3.52604 -3.5726461 -3.7943783 -4.1208534 -4.471839 -4.7649508 -4.9041529 -4.8446517 -4.6098623][-4.7939939 -4.7947683 -4.6152949 -4.3021793 -3.9622445 -3.6757374 -3.514818 -3.5556371 -3.7737572 -4.0916891 -4.4319053 -4.7166147 -4.8518262 -4.7957344 -4.5703382][-4.8078308 -4.8157487 -4.6701889 -4.3997521 -4.0996828 -3.8462465 -3.6992154 -3.7320132 -3.9146843 -4.1731291 -4.4463692 -4.6797309 -4.802865 -4.7815666 -4.6260071][-4.795517 -4.8303924 -4.7454672 -4.5476623 -4.3152552 -4.1154847 -3.9984713 -4.024631 -4.1596251 -4.3381763 -4.5209541 -4.6823106 -4.7875814 -4.8100758 -4.7392573][-4.7308087 -4.7955489 -4.7750006 -4.6539817 -4.4902153 -4.3448224 -4.2648082 -4.2953591 -4.3973436 -4.5131097 -4.6216249 -4.7206793 -4.8080425 -4.8615956 -4.8495588][-4.6251736 -4.7094526 -4.7409453 -4.6856046 -4.5786362 -4.4769793 -4.4284954 -4.4659653 -4.5498714 -4.628828 -4.6914911 -4.750741 -4.8224549 -4.8827333 -4.8929868][-4.5140352 -4.5973706 -4.6553497 -4.6483583 -4.5912743 -4.5299454 -4.5093946 -4.551712 -4.6216059 -4.674304 -4.7066116 -4.741982 -4.7976813 -4.8448067 -4.8496962][-4.4263577 -4.4922047 -4.5513749 -4.5691032 -4.5474548 -4.5191231 -4.5195379 -4.5605063 -4.6131339 -4.6444044 -4.658792 -4.6846747 -4.7316127 -4.7648668 -4.7619939][-4.3741517 -4.4182549 -4.4655867 -4.4905705 -4.4883065 -4.4784942 -4.483705 -4.5116935 -4.5457444 -4.5660458 -4.5776944 -4.6034985 -4.6452246 -4.67224 -4.6739154]]...]
INFO - root - 2017-12-07 08:56:23.576413: step 2210, loss = 21.65, batch loss = 21.56 (11.3 examples/sec; 0.706 sec/batch; 64h:48m:14s remains)
INFO - root - 2017-12-07 08:56:30.896091: step 2220, loss = 21.47, batch loss = 21.39 (10.8 examples/sec; 0.740 sec/batch; 67h:53m:30s remains)
INFO - root - 2017-12-07 08:56:38.037806: step 2230, loss = 21.37, batch loss = 21.28 (11.2 examples/sec; 0.713 sec/batch; 65h:23m:15s remains)
INFO - root - 2017-12-07 08:56:45.395898: step 2240, loss = 21.40, batch loss = 21.32 (10.8 examples/sec; 0.742 sec/batch; 68h:03m:31s remains)
INFO - root - 2017-12-07 08:56:52.701930: step 2250, loss = 21.54, batch loss = 21.45 (11.0 examples/sec; 0.727 sec/batch; 66h:40m:15s remains)
INFO - root - 2017-12-07 08:57:00.045835: step 2260, loss = 21.46, batch loss = 21.38 (10.5 examples/sec; 0.761 sec/batch; 69h:48m:20s remains)
INFO - root - 2017-12-07 08:57:07.307812: step 2270, loss = 22.06, batch loss = 21.97 (11.2 examples/sec; 0.716 sec/batch; 65h:40m:43s remains)
INFO - root - 2017-12-07 08:57:14.624021: step 2280, loss = 21.35, batch loss = 21.26 (10.5 examples/sec; 0.758 sec/batch; 69h:34m:25s remains)
INFO - root - 2017-12-07 08:57:22.007956: step 2290, loss = 21.74, batch loss = 21.65 (11.3 examples/sec; 0.708 sec/batch; 64h:55m:59s remains)
INFO - root - 2017-12-07 08:57:29.286919: step 2300, loss = 21.47, batch loss = 21.39 (11.4 examples/sec; 0.700 sec/batch; 64h:10m:00s remains)
2017-12-07 08:57:30.101019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.408875 -4.3638334 -4.2603064 -4.194602 -4.2113686 -4.2881846 -4.3518147 -4.3768859 -4.3823824 -4.3529968 -4.2914472 -4.2319574 -4.1936312 -4.1947279 -4.2277608][-4.452795 -4.4271092 -4.3351703 -4.2697339 -4.2728882 -4.3314753 -4.3848143 -4.4166179 -4.4405813 -4.4232416 -4.35423 -4.2814007 -4.2342167 -4.2225027 -4.2399292][-4.4866772 -4.4617276 -4.3730469 -4.308291 -4.2958751 -4.3253608 -4.352716 -4.3696237 -4.3953505 -4.3940234 -4.3524423 -4.3106856 -4.2775993 -4.2525678 -4.2449532][-4.4966884 -4.45704 -4.3657904 -4.2983117 -4.2686458 -4.2705183 -4.2761221 -4.277916 -4.2928705 -4.2999897 -4.2893877 -4.2881937 -4.2753491 -4.2412224 -4.2136025][-4.4877381 -4.4277549 -4.3353968 -4.2701077 -4.23028 -4.2096157 -4.1953025 -4.1797481 -4.1798639 -4.1922626 -4.2095175 -4.2391033 -4.2424564 -4.2093148 -4.176322][-4.4652176 -4.3860707 -4.2890868 -4.2292247 -4.1909666 -4.1559148 -4.122489 -4.0931082 -4.0902162 -4.1218634 -4.1734548 -4.2283006 -4.2445159 -4.2157278 -4.1805906][-4.4568663 -4.3696036 -4.2632828 -4.1960988 -4.1531343 -4.1023827 -4.0489311 -4.0116014 -4.0191913 -4.0843983 -4.1786866 -4.2610388 -4.2915468 -4.2699423 -4.2338586][-4.4737649 -4.3947315 -4.2781067 -4.1853995 -4.1157851 -4.0385733 -3.9632149 -3.9211593 -3.9435315 -4.041544 -4.1763263 -4.2877111 -4.33715 -4.3261442 -4.289988][-4.4932041 -4.4329829 -4.3131447 -4.1949263 -4.0925288 -3.9909782 -3.903461 -3.8601356 -3.891825 -4.0063672 -4.1609516 -4.2838826 -4.3421993 -4.3385863 -4.3042951][-4.4898052 -4.453043 -4.347168 -4.2242556 -4.1068392 -4.00368 -3.9236228 -3.8817902 -3.9084618 -4.013423 -4.1541905 -4.2602773 -4.3107595 -4.307889 -4.2727089][-4.4550982 -4.4326248 -4.3502693 -4.24808 -4.1427913 -4.0592203 -3.9997346 -3.9697571 -3.9923835 -4.0748167 -4.1753907 -4.2379513 -4.2614326 -4.2464747 -4.2028451][-4.4004021 -4.3789415 -4.3203859 -4.2559853 -4.1861496 -4.1353531 -4.0991707 -4.0822134 -4.1023412 -4.1539068 -4.2015562 -4.2140746 -4.2029328 -4.166759 -4.1105275][-4.345046 -4.3130889 -4.2697721 -4.2425508 -4.2126107 -4.187253 -4.1611691 -4.1486359 -4.1688571 -4.1976304 -4.2082033 -4.1904211 -4.159503 -4.111824 -4.0528593][-4.3097825 -4.2688851 -4.2267575 -4.2134876 -4.2039285 -4.1932335 -4.1772227 -4.175251 -4.2079768 -4.2335334 -4.2257261 -4.193841 -4.1548538 -4.1079049 -4.0608644][-4.3236079 -4.2884016 -4.2370567 -4.2048264 -4.1803417 -4.1675658 -4.1666842 -4.1863594 -4.2413692 -4.2781672 -4.2662849 -4.2304249 -4.1913862 -4.1546974 -4.1309657]]...]
INFO - root - 2017-12-07 08:57:37.372432: step 2310, loss = 21.49, batch loss = 21.41 (10.9 examples/sec; 0.736 sec/batch; 67h:29m:40s remains)
INFO - root - 2017-12-07 08:57:44.779470: step 2320, loss = 21.20, batch loss = 21.12 (10.5 examples/sec; 0.765 sec/batch; 70h:11m:59s remains)
INFO - root - 2017-12-07 08:57:52.044415: step 2330, loss = 22.00, batch loss = 21.92 (10.6 examples/sec; 0.757 sec/batch; 69h:23m:41s remains)
INFO - root - 2017-12-07 08:57:59.397069: step 2340, loss = 21.56, batch loss = 21.47 (10.2 examples/sec; 0.787 sec/batch; 72h:13m:02s remains)
INFO - root - 2017-12-07 08:58:06.681417: step 2350, loss = 21.65, batch loss = 21.57 (11.1 examples/sec; 0.723 sec/batch; 66h:17m:34s remains)
INFO - root - 2017-12-07 08:58:14.094550: step 2360, loss = 21.48, batch loss = 21.40 (11.0 examples/sec; 0.727 sec/batch; 66h:41m:19s remains)
INFO - root - 2017-12-07 08:58:21.314828: step 2370, loss = 21.61, batch loss = 21.52 (10.4 examples/sec; 0.771 sec/batch; 70h:44m:02s remains)
INFO - root - 2017-12-07 08:58:28.805761: step 2380, loss = 21.27, batch loss = 21.19 (11.1 examples/sec; 0.720 sec/batch; 66h:01m:50s remains)
INFO - root - 2017-12-07 08:58:36.148531: step 2390, loss = 21.63, batch loss = 21.54 (11.4 examples/sec; 0.702 sec/batch; 64h:19m:56s remains)
INFO - root - 2017-12-07 08:58:43.479965: step 2400, loss = 21.30, batch loss = 21.21 (11.0 examples/sec; 0.725 sec/batch; 66h:26m:45s remains)
2017-12-07 08:58:44.280149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.435698 -4.3984928 -4.2783394 -4.1313982 -4.0345163 -4.0247235 -4.089076 -4.189477 -4.2883043 -4.3660164 -4.4158673 -4.4380727 -4.4395652 -4.4245162 -4.3877211][-4.4506874 -4.3997641 -4.2538481 -4.0823536 -3.971981 -3.9795876 -4.082798 -4.2160659 -4.3293705 -4.4025903 -4.4349585 -4.4558859 -4.4691639 -4.459826 -4.4136457][-4.48457 -4.4524808 -4.3042703 -4.1195607 -3.9996116 -4.0072937 -4.1090627 -4.2274165 -4.3215666 -4.3792224 -4.4061651 -4.4389315 -4.4688406 -4.4624233 -4.3993936][-4.4836292 -4.4902172 -4.3656149 -4.1881509 -4.066946 -4.0606303 -4.1240969 -4.1871285 -4.241755 -4.2895021 -4.3357129 -4.3988709 -4.4504375 -4.4420586 -4.354795][-4.4371519 -4.4779053 -4.3977637 -4.2504907 -4.130372 -4.0865507 -4.0790057 -4.0696568 -4.0865173 -4.1420174 -4.2319307 -4.3451757 -4.4321265 -4.4317155 -4.3331265][-4.3732686 -4.4352431 -4.3995423 -4.2842956 -4.154418 -4.0517259 -3.9606092 -3.8910432 -3.8937075 -3.9741216 -4.112165 -4.2760525 -4.4028292 -4.4288526 -4.3471541][-4.3442435 -4.3998518 -4.3858347 -4.2878246 -4.1407642 -3.9819825 -3.8295743 -3.7313588 -3.7423449 -3.8455176 -4.0084033 -4.1977935 -4.3502588 -4.4063125 -4.3615694][-4.3676324 -4.3954697 -4.3799343 -4.2867908 -4.1343732 -3.9583089 -3.7988267 -3.7134681 -3.7365994 -3.8311081 -3.9684119 -4.1352644 -4.2836609 -4.3597422 -4.3562636][-4.4190121 -4.42525 -4.40204 -4.3129749 -4.1750441 -4.0289888 -3.9161365 -3.8730783 -3.9004862 -3.9545338 -4.0252347 -4.1289959 -4.244523 -4.3249359 -4.35658][-4.4590664 -4.4608269 -4.43999 -4.3613634 -4.2477522 -4.1503048 -4.1004572 -4.1011739 -4.1226268 -4.1282582 -4.12985 -4.1683478 -4.2443066 -4.3219895 -4.3832078][-4.4671507 -4.479773 -4.4688358 -4.4078407 -4.3227873 -4.2697372 -4.2663355 -4.2888885 -4.2979851 -4.2684116 -4.228138 -4.2258973 -4.2726054 -4.3439145 -4.4239364][-4.4529982 -4.4884877 -4.4928107 -4.4524817 -4.3942552 -4.3666272 -4.3755021 -4.3922353 -4.3848085 -4.3344088 -4.2716441 -4.2424445 -4.2658644 -4.3338408 -4.4308877][-4.4158044 -4.4795837 -4.5056562 -4.4883094 -4.451973 -4.432601 -4.4310188 -4.4274888 -4.4036369 -4.339654 -4.2569261 -4.2004604 -4.2007756 -4.2641859 -4.3721347][-4.3478012 -4.4397416 -4.4916797 -4.4972982 -4.4791837 -4.4600387 -4.4383869 -4.4103127 -4.37093 -4.3009844 -4.20751 -4.1320171 -4.1195526 -4.1830497 -4.2923121][-4.2607727 -4.3680797 -4.4433775 -4.4668417 -4.4558949 -4.4288154 -4.38841 -4.3455849 -4.3023176 -4.2407351 -4.1586347 -4.0867329 -4.0774989 -4.1471539 -4.2554636]]...]
INFO - root - 2017-12-07 08:58:51.596490: step 2410, loss = 21.97, batch loss = 21.89 (10.2 examples/sec; 0.784 sec/batch; 71h:55m:53s remains)
INFO - root - 2017-12-07 08:58:58.920265: step 2420, loss = 21.12, batch loss = 21.04 (10.3 examples/sec; 0.780 sec/batch; 71h:32m:18s remains)
INFO - root - 2017-12-07 08:59:06.289928: step 2430, loss = 21.29, batch loss = 21.21 (10.9 examples/sec; 0.733 sec/batch; 67h:13m:37s remains)
INFO - root - 2017-12-07 08:59:13.588303: step 2440, loss = 21.32, batch loss = 21.24 (10.4 examples/sec; 0.767 sec/batch; 70h:18m:18s remains)
INFO - root - 2017-12-07 08:59:20.975149: step 2450, loss = 21.18, batch loss = 21.09 (10.9 examples/sec; 0.733 sec/batch; 67h:13m:35s remains)
INFO - root - 2017-12-07 08:59:28.380746: step 2460, loss = 21.21, batch loss = 21.13 (11.8 examples/sec; 0.678 sec/batch; 62h:12m:10s remains)
INFO - root - 2017-12-07 08:59:35.801359: step 2470, loss = 21.47, batch loss = 21.39 (10.1 examples/sec; 0.790 sec/batch; 72h:25m:49s remains)
INFO - root - 2017-12-07 08:59:43.025910: step 2480, loss = 21.32, batch loss = 21.24 (11.2 examples/sec; 0.716 sec/batch; 65h:38m:46s remains)
INFO - root - 2017-12-07 08:59:50.279487: step 2490, loss = 21.65, batch loss = 21.57 (11.4 examples/sec; 0.701 sec/batch; 64h:14m:43s remains)
INFO - root - 2017-12-07 08:59:57.623827: step 2500, loss = 21.50, batch loss = 21.42 (11.4 examples/sec; 0.701 sec/batch; 64h:17m:23s remains)
2017-12-07 08:59:58.426608: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3966403 -4.4430819 -4.4776568 -4.4959369 -4.5062532 -4.5026479 -4.48876 -4.4808211 -4.4946461 -4.51823 -4.527843 -4.51971 -4.4955416 -4.4595547 -4.42377][-4.5030317 -4.5738 -4.6132345 -4.627419 -4.6336818 -4.6221046 -4.5891995 -4.5652318 -4.5837455 -4.6276875 -4.6518564 -4.6476412 -4.6271796 -4.5920219 -4.5463543][-4.6323118 -4.717803 -4.74712 -4.740582 -4.7285047 -4.6985526 -4.6329622 -4.5743461 -4.5847087 -4.6490221 -4.6976018 -4.7113519 -4.7132797 -4.7040873 -4.6733704][-4.7116933 -4.8022003 -4.8175187 -4.7851839 -4.7385697 -4.6698995 -4.5544777 -4.44918 -4.4497166 -4.5456967 -4.6387777 -4.6856713 -4.7149577 -4.73799 -4.7377243][-4.7271557 -4.8033566 -4.8033161 -4.7536964 -4.6759014 -4.5705233 -4.42086 -4.2829137 -4.2739692 -4.3943205 -4.5307207 -4.6099992 -4.65846 -4.7040629 -4.7339249][-4.6536894 -4.6809468 -4.6474552 -4.5882435 -4.50536 -4.387619 -4.2335386 -4.0880642 -4.0704293 -4.2020774 -4.3772826 -4.4957 -4.5672617 -4.6272535 -4.6797614][-4.5514064 -4.518827 -4.4441977 -4.3714757 -4.2851062 -4.1559873 -3.9898651 -3.8268242 -3.7819986 -3.9046373 -4.1122389 -4.2829895 -4.3900075 -4.4546428 -4.5121965][-4.4867826 -4.4169579 -4.321784 -4.2457662 -4.1627588 -4.0330191 -3.8646958 -3.6952336 -3.6251938 -3.7193575 -3.9236033 -4.1219082 -4.2512665 -4.3132138 -4.3623176][-4.4966073 -4.4365244 -4.3573208 -4.292449 -4.2127309 -4.0905695 -3.9360948 -3.7786028 -3.6982028 -3.7547784 -3.9114463 -4.0875411 -4.214654 -4.2788515 -4.3262525][-4.5329928 -4.4961362 -4.4411688 -4.3893356 -4.3127503 -4.2002831 -4.059001 -3.9116287 -3.8227742 -3.8375995 -3.9362726 -4.085578 -4.2240477 -4.3185678 -4.3875628][-4.5471249 -4.528924 -4.4850688 -4.4370937 -4.3631492 -4.2716231 -4.1571136 -4.035017 -3.9500868 -3.9248879 -3.9575963 -4.0670052 -4.2134442 -4.346581 -4.4516134][-4.5757904 -4.5874071 -4.555994 -4.5063825 -4.4300132 -4.3513165 -4.2626643 -4.1770873 -4.1160827 -4.0723982 -4.0508857 -4.1118722 -4.2496753 -4.40711 -4.5384336][-4.60715 -4.6457825 -4.6225343 -4.5673285 -4.4882197 -4.4197497 -4.3624053 -4.31991 -4.2950506 -4.2607226 -4.2112594 -4.2224784 -4.3262544 -4.4778943 -4.613071][-4.6160226 -4.6684394 -4.6606541 -4.6179161 -4.5551176 -4.5045934 -4.4783955 -4.4751968 -4.4847198 -4.4744773 -4.4274869 -4.4058256 -4.4630585 -4.5730495 -4.6720834][-4.6138611 -4.6696386 -4.6807108 -4.6608787 -4.6260586 -4.6006079 -4.5970716 -4.6105838 -4.6320515 -4.6401782 -4.6138816 -4.5866933 -4.6046824 -4.6580591 -4.700654]]...]
INFO - root - 2017-12-07 09:00:05.839497: step 2510, loss = 21.75, batch loss = 21.67 (11.0 examples/sec; 0.725 sec/batch; 66h:28m:29s remains)
INFO - root - 2017-12-07 09:00:13.112240: step 2520, loss = 21.29, batch loss = 21.20 (11.3 examples/sec; 0.711 sec/batch; 65h:09m:54s remains)
INFO - root - 2017-12-07 09:00:20.421860: step 2530, loss = 21.45, batch loss = 21.37 (11.1 examples/sec; 0.719 sec/batch; 65h:56m:21s remains)
INFO - root - 2017-12-07 09:00:27.846812: step 2540, loss = 21.12, batch loss = 21.04 (11.2 examples/sec; 0.717 sec/batch; 65h:45m:35s remains)
INFO - root - 2017-12-07 09:00:35.287223: step 2550, loss = 21.82, batch loss = 21.74 (10.4 examples/sec; 0.771 sec/batch; 70h:41m:04s remains)
INFO - root - 2017-12-07 09:00:42.681169: step 2560, loss = 21.55, batch loss = 21.47 (11.0 examples/sec; 0.726 sec/batch; 66h:34m:43s remains)
INFO - root - 2017-12-07 09:00:50.003990: step 2570, loss = 21.55, batch loss = 21.47 (10.8 examples/sec; 0.741 sec/batch; 67h:54m:13s remains)
INFO - root - 2017-12-07 09:00:57.419192: step 2580, loss = 21.54, batch loss = 21.46 (10.5 examples/sec; 0.763 sec/batch; 69h:54m:52s remains)
INFO - root - 2017-12-07 09:01:04.808256: step 2590, loss = 21.33, batch loss = 21.25 (10.9 examples/sec; 0.737 sec/batch; 67h:31m:55s remains)
INFO - root - 2017-12-07 09:01:12.061052: step 2600, loss = 21.75, batch loss = 21.67 (11.3 examples/sec; 0.710 sec/batch; 65h:03m:25s remains)
2017-12-07 09:01:12.831210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3911724 -4.4121237 -4.4060497 -4.3689618 -4.3520765 -4.3720403 -4.3856645 -4.3411956 -4.2568321 -4.1723976 -4.128366 -4.1287551 -4.1759582 -4.2453165 -4.3091269][-4.3977189 -4.4194746 -4.4188056 -4.3978562 -4.3938546 -4.4020486 -4.3777823 -4.2911954 -4.1720276 -4.0655093 -4.0119534 -4.0090337 -4.0539188 -4.1163287 -4.1832504][-4.4457994 -4.4764991 -4.477891 -4.4534588 -4.4218683 -4.3736534 -4.2887511 -4.1723022 -4.0555887 -3.9734535 -3.9442625 -3.9506946 -3.988627 -4.0338135 -4.0930276][-4.4940381 -4.5367107 -4.5275803 -4.4717484 -4.3841591 -4.2706962 -4.1447449 -4.0337486 -3.956645 -3.929002 -3.9405303 -3.9589527 -3.9799986 -3.9986756 -4.0441818][-4.4893537 -4.5341363 -4.5046015 -4.4141893 -4.2840314 -4.1345129 -3.9973764 -3.9074359 -3.8779562 -3.903718 -3.9500751 -3.9778984 -3.9839864 -3.983665 -4.0219989][-4.4054842 -4.4359112 -4.3902335 -4.2939191 -4.1677108 -4.0199056 -3.883769 -3.8072288 -3.8129408 -3.8730767 -3.9400866 -3.9814565 -3.9960132 -4.0023088 -4.0434504][-4.279068 -4.303081 -4.2616396 -4.1863933 -4.0851684 -3.9492235 -3.8151541 -3.7497139 -3.7835493 -3.8645916 -3.945796 -4.0067487 -4.03803 -4.0518608 -4.0786858][-4.1935143 -4.2347832 -4.2074175 -4.1451507 -4.0572386 -3.9368496 -3.8241138 -3.7820394 -3.8363662 -3.9252048 -4.0113249 -4.0825353 -4.1160874 -4.1172833 -4.10641][-4.1783009 -4.2246284 -4.1979527 -4.1362467 -4.0647125 -3.9838264 -3.9208367 -3.9151974 -3.9812117 -4.0686741 -4.1504312 -4.2103066 -4.2258849 -4.2034383 -4.1561737][-4.2335443 -4.2702632 -4.2419009 -4.1873188 -4.1366348 -4.0955753 -4.0780063 -4.1028991 -4.1766262 -4.2582083 -4.3230815 -4.3450208 -4.323545 -4.27658 -4.2129569][-4.327467 -4.357132 -4.3339024 -4.2940607 -4.25801 -4.2349715 -4.2376995 -4.2767124 -4.3512516 -4.4200134 -4.4579525 -4.4380531 -4.386806 -4.331552 -4.2763095][-4.3948326 -4.424531 -4.4153681 -4.3964844 -4.3731737 -4.3549047 -4.3580241 -4.3947873 -4.4570675 -4.5052052 -4.5173974 -4.4759607 -4.4175539 -4.365941 -4.322536][-4.4209352 -4.4534926 -4.4602923 -4.4601574 -4.44739 -4.4297867 -4.4258332 -4.4513559 -4.4949207 -4.524653 -4.5245333 -4.482584 -4.4303322 -4.3866286 -4.352519][-4.4387641 -4.4640622 -4.4752011 -4.4805517 -4.4721665 -4.4560957 -4.4483013 -4.4633608 -4.4923668 -4.5131464 -4.5142779 -4.4871526 -4.4521255 -4.4238405 -4.4027109][-4.4347534 -4.4448414 -4.4498296 -4.4520683 -4.4461493 -4.4363408 -4.4325809 -4.4446621 -4.4671178 -4.485189 -4.4920235 -4.4831505 -4.4683003 -4.4569936 -4.4478197]]...]
INFO - root - 2017-12-07 09:01:20.177794: step 2610, loss = 21.20, batch loss = 21.11 (10.7 examples/sec; 0.750 sec/batch; 68h:45m:52s remains)
INFO - root - 2017-12-07 09:01:27.689466: step 2620, loss = 21.35, batch loss = 21.27 (10.1 examples/sec; 0.792 sec/batch; 72h:33m:02s remains)
INFO - root - 2017-12-07 09:01:35.159746: step 2630, loss = 21.24, batch loss = 21.16 (9.9 examples/sec; 0.805 sec/batch; 73h:44m:55s remains)
INFO - root - 2017-12-07 09:01:42.424925: step 2640, loss = 21.57, batch loss = 21.49 (10.5 examples/sec; 0.764 sec/batch; 69h:58m:35s remains)
INFO - root - 2017-12-07 09:01:49.656054: step 2650, loss = 21.34, batch loss = 21.26 (10.6 examples/sec; 0.757 sec/batch; 69h:22m:33s remains)
INFO - root - 2017-12-07 09:01:57.021338: step 2660, loss = 21.44, batch loss = 21.36 (10.7 examples/sec; 0.747 sec/batch; 68h:25m:37s remains)
INFO - root - 2017-12-07 09:02:04.328797: step 2670, loss = 21.35, batch loss = 21.26 (10.5 examples/sec; 0.762 sec/batch; 69h:46m:42s remains)
INFO - root - 2017-12-07 09:02:11.636236: step 2680, loss = 21.64, batch loss = 21.56 (11.5 examples/sec; 0.695 sec/batch; 63h:43m:02s remains)
INFO - root - 2017-12-07 09:02:18.818348: step 2690, loss = 21.66, batch loss = 21.58 (10.9 examples/sec; 0.731 sec/batch; 66h:57m:20s remains)
INFO - root - 2017-12-07 09:02:26.211489: step 2700, loss = 21.41, batch loss = 21.33 (10.5 examples/sec; 0.765 sec/batch; 70h:06m:54s remains)
2017-12-07 09:02:27.050371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2447696 -4.3011675 -4.3736835 -4.4467063 -4.487133 -4.5004373 -4.5257497 -4.5513029 -4.5587668 -4.5508714 -4.5383825 -4.5246606 -4.5107827 -4.4954987 -4.4805489][-4.2760682 -4.3501506 -4.4158092 -4.4703493 -4.4959235 -4.5015211 -4.5225706 -4.5480733 -4.5603642 -4.5589852 -4.5500975 -4.5403481 -4.5310268 -4.5184379 -4.5040317][-4.3596492 -4.433331 -4.4663429 -4.47734 -4.4696913 -4.4552236 -4.4643621 -4.4909544 -4.5145679 -4.5298805 -4.5376234 -4.5458846 -4.5480428 -4.5366573 -4.5205293][-4.450994 -4.5067968 -4.5017877 -4.4736609 -4.4349318 -4.3972683 -4.3894377 -4.4160581 -4.4509649 -4.4842858 -4.5109324 -4.5384078 -4.5487003 -4.5312243 -4.5098443][-4.4571695 -4.4900007 -4.4604244 -4.409759 -4.3449168 -4.2810493 -4.25444 -4.2838321 -4.3381381 -4.3983364 -4.4510384 -4.5034351 -4.5271692 -4.5058184 -4.4796438][-4.3495975 -4.3610797 -4.3263531 -4.2755966 -4.1939092 -4.0996418 -4.0446777 -4.0716677 -4.1507907 -4.2465248 -4.333302 -4.4147382 -4.459084 -4.4448805 -4.4226542][-4.20092 -4.1987109 -4.1705556 -4.1363053 -4.0488181 -3.9243906 -3.8363225 -3.8524129 -3.9494035 -4.07166 -4.1803684 -4.274282 -4.3325543 -4.3370376 -4.3337812][-4.1183634 -4.1124725 -4.1013074 -4.0930862 -4.0131307 -3.8666618 -3.7447076 -3.7362089 -3.8299341 -3.9595566 -4.0759397 -4.1681533 -4.2301126 -4.2506118 -4.255826][-4.1617169 -4.1603389 -4.177187 -4.2014136 -4.1410861 -3.9951231 -3.8600516 -3.8268101 -3.8928108 -3.9995816 -4.1003819 -4.1751933 -4.2267704 -4.2445621 -4.2275786][-4.3144894 -4.3061361 -4.3373284 -4.3794532 -4.3400664 -4.2214065 -4.1084614 -4.066051 -4.0959682 -4.1604609 -4.2244067 -4.2704163 -4.306201 -4.310317 -4.2567954][-4.4696059 -4.45062 -4.4731417 -4.509882 -4.4835544 -4.402091 -4.327837 -4.2889833 -4.293735 -4.3188319 -4.3415694 -4.356885 -4.3696623 -4.3470383 -4.2614589][-4.501895 -4.4789996 -4.478106 -4.4929628 -4.4690738 -4.4160051 -4.3713288 -4.3358927 -4.3280625 -4.3302822 -4.3254929 -4.3262777 -4.3300271 -4.2970033 -4.2139769][-4.4416013 -4.4105554 -4.3742075 -4.3565974 -4.3284354 -4.2939539 -4.2656732 -4.2348123 -4.2292004 -4.2302003 -4.224194 -4.2400355 -4.2672825 -4.2597032 -4.2124248][-4.3867011 -4.3516092 -4.2897406 -4.2485123 -4.2231588 -4.2109571 -4.201664 -4.1839495 -4.1887989 -4.1973066 -4.199204 -4.23111 -4.28373 -4.3088989 -4.2956562][-4.3769569 -4.3431 -4.273869 -4.2237139 -4.2092586 -4.2250915 -4.2436361 -4.2499413 -4.2700591 -4.2870154 -4.2910476 -4.3173923 -4.3694644 -4.40988 -4.4154987]]...]
INFO - root - 2017-12-07 09:02:34.484114: step 2710, loss = 21.77, batch loss = 21.69 (10.6 examples/sec; 0.752 sec/batch; 68h:51m:08s remains)
INFO - root - 2017-12-07 09:02:41.653247: step 2720, loss = 21.71, batch loss = 21.62 (14.5 examples/sec; 0.553 sec/batch; 50h:41m:21s remains)
INFO - root - 2017-12-07 09:02:48.923017: step 2730, loss = 21.12, batch loss = 21.04 (11.4 examples/sec; 0.703 sec/batch; 64h:24m:17s remains)
INFO - root - 2017-12-07 09:02:56.253361: step 2740, loss = 21.36, batch loss = 21.28 (11.3 examples/sec; 0.708 sec/batch; 64h:52m:49s remains)
INFO - root - 2017-12-07 09:03:03.548644: step 2750, loss = 21.27, batch loss = 21.19 (11.6 examples/sec; 0.689 sec/batch; 63h:05m:53s remains)
INFO - root - 2017-12-07 09:03:10.980136: step 2760, loss = 21.39, batch loss = 21.31 (11.2 examples/sec; 0.712 sec/batch; 65h:14m:49s remains)
INFO - root - 2017-12-07 09:03:18.192345: step 2770, loss = 21.44, batch loss = 21.36 (10.9 examples/sec; 0.732 sec/batch; 67h:04m:04s remains)
INFO - root - 2017-12-07 09:03:25.472689: step 2780, loss = 21.28, batch loss = 21.19 (11.3 examples/sec; 0.706 sec/batch; 64h:39m:33s remains)
INFO - root - 2017-12-07 09:03:32.796555: step 2790, loss = 21.66, batch loss = 21.58 (11.2 examples/sec; 0.712 sec/batch; 65h:15m:04s remains)
INFO - root - 2017-12-07 09:03:40.092343: step 2800, loss = 21.63, batch loss = 21.55 (11.1 examples/sec; 0.720 sec/batch; 65h:55m:16s remains)
2017-12-07 09:03:40.934661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.43136 -4.4108624 -4.4128313 -4.4562411 -4.5021553 -4.5332727 -4.5223794 -4.4574041 -4.3516407 -4.25886 -4.2280436 -4.2790775 -4.350605 -4.3956022 -4.453649][-4.470808 -4.4473619 -4.4531574 -4.5006185 -4.5461812 -4.56752 -4.5476627 -4.4819098 -4.3899646 -4.315999 -4.2950673 -4.3361769 -4.3872328 -4.4161973 -4.4664283][-4.4386244 -4.4262581 -4.4510689 -4.5149961 -4.5682917 -4.5830178 -4.5518012 -4.4798055 -4.4015927 -4.3568778 -4.359509 -4.3983855 -4.430295 -4.4474416 -4.4948611][-4.351881 -4.3439822 -4.3816471 -4.4631662 -4.5386391 -4.5671787 -4.5387468 -4.4618731 -4.3891964 -4.3677993 -4.390883 -4.42628 -4.4421411 -4.4546981 -4.5052834][-4.2751589 -4.2645459 -4.3048229 -4.3910527 -4.4701967 -4.4926558 -4.4478579 -4.3556156 -4.2877684 -4.3010125 -4.359972 -4.4112582 -4.43362 -4.4533362 -4.5033379][-4.2408686 -4.2250395 -4.2507348 -4.3109202 -4.3513083 -4.3353705 -4.2657862 -4.1664772 -4.1181459 -4.1768212 -4.2789435 -4.3540606 -4.3927169 -4.4224582 -4.46598][-4.2196631 -4.1991858 -4.207767 -4.2299542 -4.2130523 -4.1572003 -4.0760136 -3.9835169 -3.9578223 -4.0464377 -4.1769786 -4.2763143 -4.3308907 -4.3656578 -4.3969822][-4.2067914 -4.1969886 -4.2126851 -4.2271829 -4.1797462 -4.0990386 -4.0057745 -3.9111423 -3.8928752 -3.9912052 -4.1372476 -4.2593312 -4.329618 -4.3632483 -4.3742747][-4.206708 -4.2184191 -4.2581587 -4.2879148 -4.2474594 -4.1735697 -4.0873694 -3.9957561 -3.9715083 -4.0504432 -4.1839824 -4.3090472 -4.3847189 -4.414012 -4.4060154][-4.2190309 -4.2485256 -4.3065567 -4.3580713 -4.3466496 -4.2984643 -4.2296209 -4.1480932 -4.1178794 -4.1694789 -4.2781515 -4.3900156 -4.4587679 -4.48377 -4.4655113][-4.2478433 -4.2870159 -4.3589568 -4.4314981 -4.4462447 -4.4140553 -4.3522854 -4.2768583 -4.2388878 -4.2674932 -4.3553262 -4.4521518 -4.5140514 -4.5424905 -4.5283833][-4.26282 -4.3030076 -4.3836894 -4.4675083 -4.4989352 -4.4729743 -4.4167933 -4.3522663 -4.3092761 -4.3216171 -4.3926363 -4.4733272 -4.5287833 -4.5603824 -4.5531645][-4.2720809 -4.3086514 -4.3886156 -4.4701061 -4.5020986 -4.4712157 -4.416419 -4.366787 -4.3337293 -4.3525696 -4.4246759 -4.491518 -4.5314293 -4.551374 -4.5401974][-4.2800512 -4.3194542 -4.38423 -4.4385767 -4.4452329 -4.391892 -4.3290486 -4.2909341 -4.2761974 -4.3146944 -4.3981309 -4.4595423 -4.4888124 -4.4997211 -4.4927125][-4.2518816 -4.3123932 -4.3672667 -4.3973169 -4.383728 -4.3161664 -4.24595 -4.213953 -4.213264 -4.2591448 -4.3349934 -4.3781304 -4.3930531 -4.3989935 -4.4074478]]...]
INFO - root - 2017-12-07 09:03:48.218835: step 2810, loss = 21.28, batch loss = 21.20 (10.3 examples/sec; 0.779 sec/batch; 71h:19m:26s remains)
INFO - root - 2017-12-07 09:03:55.605284: step 2820, loss = 21.52, batch loss = 21.43 (11.0 examples/sec; 0.728 sec/batch; 66h:40m:56s remains)
INFO - root - 2017-12-07 09:04:03.081849: step 2830, loss = 21.64, batch loss = 21.56 (11.0 examples/sec; 0.731 sec/batch; 66h:53m:44s remains)
INFO - root - 2017-12-07 09:04:10.434637: step 2840, loss = 20.62, batch loss = 20.54 (11.3 examples/sec; 0.706 sec/batch; 64h:36m:27s remains)
INFO - root - 2017-12-07 09:04:17.738882: step 2850, loss = 21.32, batch loss = 21.24 (10.7 examples/sec; 0.745 sec/batch; 68h:12m:13s remains)
INFO - root - 2017-12-07 09:04:25.051700: step 2860, loss = 21.51, batch loss = 21.43 (11.7 examples/sec; 0.686 sec/batch; 62h:48m:47s remains)
INFO - root - 2017-12-07 09:04:32.217401: step 2870, loss = 21.36, batch loss = 21.27 (12.2 examples/sec; 0.658 sec/batch; 60h:15m:57s remains)
INFO - root - 2017-12-07 09:04:39.451241: step 2880, loss = 21.59, batch loss = 21.51 (11.2 examples/sec; 0.714 sec/batch; 65h:21m:38s remains)
INFO - root - 2017-12-07 09:04:46.760857: step 2890, loss = 20.91, batch loss = 20.82 (12.0 examples/sec; 0.667 sec/batch; 61h:03m:21s remains)
INFO - root - 2017-12-07 09:04:54.086885: step 2900, loss = 21.36, batch loss = 21.28 (11.7 examples/sec; 0.683 sec/batch; 62h:33m:14s remains)
2017-12-07 09:04:54.851451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3880577 -4.3744016 -4.3760118 -4.3890877 -4.3768373 -4.33413 -4.2652125 -4.182735 -4.1434636 -4.1871419 -4.2926078 -4.3979354 -4.4257631 -4.3719506 -4.2994533][-4.3787241 -4.3348265 -4.315876 -4.3272223 -4.3360381 -4.327806 -4.2850084 -4.202755 -4.1484308 -4.1898255 -4.3099012 -4.4321995 -4.4674768 -4.4140124 -4.333343][-4.3736787 -4.308322 -4.2721467 -4.281281 -4.3053 -4.3236709 -4.3072505 -4.2363043 -4.17565 -4.2095928 -4.3284192 -4.4549961 -4.5016818 -4.4575357 -4.3789139][-4.3754349 -4.3135281 -4.2740841 -4.2781839 -4.3004308 -4.318635 -4.3071942 -4.2452235 -4.1900954 -4.218822 -4.3275161 -4.4463964 -4.4980125 -4.4676919 -4.4030204][-4.3851948 -4.3509264 -4.3247471 -4.3206835 -4.3228345 -4.3155851 -4.28432 -4.2219496 -4.1779828 -4.2103896 -4.3117347 -4.4183745 -4.4638877 -4.4358644 -4.3782992][-4.4046817 -4.3979235 -4.3862214 -4.3722076 -4.3489351 -4.3113251 -4.2611461 -4.2061019 -4.1855588 -4.2334828 -4.334022 -4.4284563 -4.4602551 -4.4201374 -4.35449][-4.4267416 -4.4267297 -4.4160891 -4.3945808 -4.36064 -4.31702 -4.2750812 -4.2481723 -4.25911 -4.323163 -4.4171996 -4.4907746 -4.5041933 -4.4518676 -4.3760481][-4.4509807 -4.4490848 -4.4303808 -4.4004016 -4.3656354 -4.3343673 -4.3175139 -4.3242311 -4.3598447 -4.4296513 -4.5074306 -4.5548058 -4.5539069 -4.5020809 -4.428184][-4.4689064 -4.4737558 -4.4447746 -4.4019585 -4.3663549 -4.34946 -4.3506627 -4.369689 -4.4049249 -4.4632945 -4.5223255 -4.5529127 -4.5526209 -4.5175867 -4.4613471][-4.4587216 -4.4819336 -4.4501691 -4.3942294 -4.3529029 -4.3427367 -4.348012 -4.3565316 -4.3698497 -4.4036341 -4.4442778 -4.470561 -4.483459 -4.4760909 -4.4502292][-4.4238329 -4.4771986 -4.4563088 -4.3906951 -4.3347406 -4.3180556 -4.3183761 -4.3131275 -4.3017282 -4.3053036 -4.3214183 -4.3407979 -4.3668866 -4.3883014 -4.3989635][-4.3997893 -4.4696317 -4.4598422 -4.3891497 -4.3149633 -4.2830205 -4.2792869 -4.2742524 -4.2570806 -4.2392783 -4.2239652 -4.2175536 -4.2396727 -4.2790647 -4.3176427][-4.3967595 -4.4559808 -4.4498768 -4.3845243 -4.30392 -4.2584114 -4.2485886 -4.2476969 -4.2370248 -4.2095132 -4.1637554 -4.1249022 -4.1356983 -4.1809659 -4.2243428][-4.3932772 -4.4224114 -4.412178 -4.3633318 -4.2983184 -4.2542295 -4.2425761 -4.2461081 -4.2469597 -4.2206688 -4.1516876 -4.084578 -4.0840807 -4.1248212 -4.1469483][-4.3638058 -4.3582058 -4.344532 -4.3230705 -4.2953954 -4.2762694 -4.27641 -4.2890339 -4.3013086 -4.2787075 -4.1926255 -4.1037259 -4.0903111 -4.1185927 -4.1142912]]...]
INFO - root - 2017-12-07 09:05:02.297203: step 2910, loss = 21.50, batch loss = 21.41 (10.7 examples/sec; 0.751 sec/batch; 68h:46m:14s remains)
INFO - root - 2017-12-07 09:05:09.626446: step 2920, loss = 22.04, batch loss = 21.96 (10.7 examples/sec; 0.751 sec/batch; 68h:45m:25s remains)
INFO - root - 2017-12-07 09:05:16.935619: step 2930, loss = 22.13, batch loss = 22.04 (11.4 examples/sec; 0.703 sec/batch; 64h:19m:01s remains)
INFO - root - 2017-12-07 09:05:24.335285: step 2940, loss = 21.13, batch loss = 21.05 (10.2 examples/sec; 0.782 sec/batch; 71h:34m:47s remains)
INFO - root - 2017-12-07 09:05:31.560011: step 2950, loss = 21.67, batch loss = 21.59 (11.4 examples/sec; 0.702 sec/batch; 64h:13m:06s remains)
INFO - root - 2017-12-07 09:05:38.837595: step 2960, loss = 21.54, batch loss = 21.45 (10.8 examples/sec; 0.740 sec/batch; 67h:46m:34s remains)
INFO - root - 2017-12-07 09:05:46.124857: step 2970, loss = 21.68, batch loss = 21.59 (11.1 examples/sec; 0.718 sec/batch; 65h:41m:39s remains)
INFO - root - 2017-12-07 09:05:53.439471: step 2980, loss = 21.52, batch loss = 21.44 (11.2 examples/sec; 0.717 sec/batch; 65h:38m:12s remains)
INFO - root - 2017-12-07 09:06:00.821312: step 2990, loss = 21.70, batch loss = 21.61 (11.0 examples/sec; 0.725 sec/batch; 66h:19m:10s remains)
INFO - root - 2017-12-07 09:06:08.278940: step 3000, loss = 21.65, batch loss = 21.56 (11.7 examples/sec; 0.686 sec/batch; 62h:47m:10s remains)
2017-12-07 09:06:08.966105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.40543 -4.4323082 -4.496501 -4.5868831 -4.6256595 -4.5483375 -4.4029841 -4.2823782 -4.2043324 -4.1704788 -4.1679626 -4.1991835 -4.2640224 -4.3270431 -4.392828][-4.399303 -4.4383211 -4.5102129 -4.5996275 -4.6334152 -4.54637 -4.3904095 -4.271348 -4.2105703 -4.2011552 -4.2173018 -4.2576818 -4.324769 -4.383532 -4.4448042][-4.3863235 -4.4303613 -4.5060897 -4.602788 -4.6490793 -4.5754213 -4.43227 -4.3335724 -4.3008585 -4.3168893 -4.3437843 -4.37819 -4.4285507 -4.4699063 -4.5233502][-4.364398 -4.3934546 -4.4639525 -4.5733805 -4.642921 -4.5965872 -4.4844713 -4.420279 -4.4206219 -4.4604259 -4.4981651 -4.5262837 -4.5579395 -4.5808711 -4.6253753][-4.3196745 -4.32825 -4.3887229 -4.4937372 -4.5607195 -4.5143685 -4.4137106 -4.36994 -4.3989735 -4.4633832 -4.5208492 -4.5621333 -4.5991325 -4.6256866 -4.6756988][-4.2607446 -4.2733555 -4.3238144 -4.392293 -4.4124408 -4.3243637 -4.199306 -4.148325 -4.1972203 -4.2910604 -4.3729472 -4.4389868 -4.501266 -4.5486479 -4.6074533][-4.2215037 -4.2444644 -4.25742 -4.2382355 -4.1689405 -4.025846 -3.8710351 -3.8082986 -3.8827937 -4.0184531 -4.1369772 -4.2341075 -4.3285103 -4.4044085 -4.4679193][-4.2684402 -4.3026266 -4.2666211 -4.1498957 -3.9877923 -3.7971318 -3.6200328 -3.5513468 -3.6548061 -3.8351383 -3.9907198 -4.1073112 -4.2177124 -4.3078804 -4.3636942][-4.4121113 -4.4635606 -4.4089384 -4.2450433 -4.0352335 -3.8321905 -3.6593528 -3.5933814 -3.6983619 -3.8869052 -4.05424 -4.1703587 -4.2611771 -4.3265157 -4.3434057][-4.5536246 -4.6066251 -4.5562286 -4.4073281 -4.2150292 -4.0392146 -3.8972321 -3.8418636 -3.9195704 -4.0755806 -4.2306776 -4.3406162 -4.4044781 -4.43136 -4.3997717][-4.6234331 -4.6738257 -4.6535287 -4.5655813 -4.4384747 -4.3123889 -4.2082634 -4.1597319 -4.1958694 -4.2938385 -4.4009333 -4.4701328 -4.48684 -4.4679942 -4.3968863][-4.6198487 -4.6724377 -4.6877422 -4.6687479 -4.6215982 -4.5600615 -4.4977 -4.4515305 -4.443006 -4.4707441 -4.503 -4.4952188 -4.4422793 -4.3687267 -4.2580681][-4.5623846 -4.6077852 -4.6425228 -4.6691837 -4.6826315 -4.6753669 -4.6490393 -4.609488 -4.5830846 -4.5783024 -4.562664 -4.4899588 -4.3764238 -4.2571383 -4.1176038][-4.4798422 -4.5219727 -4.5664239 -4.6110258 -4.649632 -4.6696544 -4.6698675 -4.6548719 -4.6486893 -4.6548252 -4.6272283 -4.5266886 -4.3873205 -4.2487426 -4.09892][-4.3902292 -4.4205775 -4.4559646 -4.4915447 -4.5247602 -4.5506597 -4.5715795 -4.5920272 -4.6254783 -4.66412 -4.6581655 -4.5827737 -4.4751058 -4.36839 -4.2487535]]...]
INFO - root - 2017-12-07 09:06:16.307795: step 3010, loss = 21.38, batch loss = 21.29 (10.7 examples/sec; 0.744 sec/batch; 68h:07m:51s remains)
INFO - root - 2017-12-07 09:06:23.704135: step 3020, loss = 21.68, batch loss = 21.60 (10.8 examples/sec; 0.737 sec/batch; 67h:29m:08s remains)
INFO - root - 2017-12-07 09:06:31.066979: step 3030, loss = 21.52, batch loss = 21.43 (11.0 examples/sec; 0.727 sec/batch; 66h:31m:36s remains)
INFO - root - 2017-12-07 09:06:38.475029: step 3040, loss = 21.37, batch loss = 21.29 (11.0 examples/sec; 0.728 sec/batch; 66h:34m:45s remains)
INFO - root - 2017-12-07 09:06:45.756928: step 3050, loss = 21.24, batch loss = 21.15 (10.6 examples/sec; 0.754 sec/batch; 68h:59m:38s remains)
INFO - root - 2017-12-07 09:06:52.974199: step 3060, loss = 21.44, batch loss = 21.35 (10.9 examples/sec; 0.731 sec/batch; 66h:56m:21s remains)
INFO - root - 2017-12-07 09:07:00.315654: step 3070, loss = 21.47, batch loss = 21.39 (10.4 examples/sec; 0.772 sec/batch; 70h:40m:10s remains)
INFO - root - 2017-12-07 09:07:07.545223: step 3080, loss = 21.98, batch loss = 21.89 (10.7 examples/sec; 0.747 sec/batch; 68h:20m:51s remains)
INFO - root - 2017-12-07 09:07:14.866779: step 3090, loss = 21.52, batch loss = 21.44 (11.1 examples/sec; 0.718 sec/batch; 65h:43m:42s remains)
INFO - root - 2017-12-07 09:07:22.191666: step 3100, loss = 21.84, batch loss = 21.75 (10.7 examples/sec; 0.747 sec/batch; 68h:22m:12s remains)
2017-12-07 09:07:22.967637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6311131 -4.6456742 -4.6672111 -4.6873488 -4.7009234 -4.7068582 -4.7144217 -4.7345271 -4.7608223 -4.7851014 -4.7970333 -4.7963123 -4.7778282 -4.7340522 -4.6831551][-4.7096672 -4.7492266 -4.7876525 -4.8150964 -4.8251219 -4.8195763 -4.8146091 -4.8310437 -4.8643 -4.8957777 -4.9097357 -4.9031854 -4.86915 -4.79319 -4.6965961][-4.7065959 -4.7764759 -4.8316212 -4.8606715 -4.856163 -4.8211155 -4.781641 -4.778152 -4.8172398 -4.8715491 -4.9096975 -4.9160914 -4.87928 -4.7797613 -4.6393838][-4.634697 -4.7367458 -4.8024912 -4.8198962 -4.7841053 -4.7011 -4.60769 -4.5707307 -4.6198463 -4.7175169 -4.8024707 -4.8353519 -4.8061204 -4.7060409 -4.546844][-4.5269365 -4.6515718 -4.7176852 -4.7114105 -4.6331859 -4.4954176 -4.3419414 -4.2632442 -4.3196068 -4.4701595 -4.6125355 -4.6753087 -4.6582642 -4.5848055 -4.451076][-4.4476676 -4.5746861 -4.6286159 -4.5923824 -4.4736452 -4.2920518 -4.086072 -3.9599102 -4.0097022 -4.2025213 -4.393292 -4.4802146 -4.478395 -4.4496241 -4.37465][-4.4124794 -4.5200863 -4.5517249 -4.4830194 -4.3320704 -4.1253009 -3.8868246 -3.7203922 -3.7533906 -3.9652181 -4.1774616 -4.273253 -4.2884336 -4.3107095 -4.3101516][-4.4277978 -4.5093913 -4.51653 -4.4262686 -4.26404 -4.054975 -3.8110967 -3.62753 -3.6442018 -3.8517492 -4.0596581 -4.1539936 -4.1864319 -4.2432971 -4.3011312][-4.476594 -4.5405068 -4.5338774 -4.4422197 -4.2925439 -4.1035933 -3.8818605 -3.7134557 -3.7207682 -3.8974924 -4.0796132 -4.16918 -4.2117424 -4.2805538 -4.3633986][-4.5279493 -4.5877843 -4.5812 -4.5036964 -4.3795943 -4.22037 -4.0360541 -3.9003701 -3.9078112 -4.0517373 -4.2057924 -4.2883797 -4.3287072 -4.3878 -4.4682274][-4.576632 -4.6352015 -4.6363254 -4.5811763 -4.4901967 -4.3689327 -4.2320309 -4.1362238 -4.1496906 -4.2626095 -4.3872957 -4.4572639 -4.4815221 -4.514461 -4.574079][-4.5684619 -4.6282277 -4.6454983 -4.6227126 -4.5738215 -4.5004349 -4.4179006 -4.3622279 -4.3788424 -4.4613681 -4.5551863 -4.6079197 -4.6114225 -4.6121712 -4.6387229][-4.4892406 -4.5474892 -4.5821834 -4.5933323 -4.5877628 -4.5637207 -4.5322909 -4.511538 -4.5292711 -4.5826049 -4.6445465 -4.6793089 -4.67067 -4.6513076 -4.6510262][-4.4001012 -4.4441085 -4.4827342 -4.5154381 -4.5437794 -4.5608072 -4.5697756 -4.5753779 -4.5924096 -4.6200647 -4.6500282 -4.663136 -4.6476836 -4.6246238 -4.6154833][-4.3389487 -4.3675876 -4.4013572 -4.4406085 -4.4832754 -4.5181451 -4.5456133 -4.5655518 -4.582314 -4.5903158 -4.5897155 -4.5794997 -4.5613856 -4.5476193 -4.546905]]...]
INFO - root - 2017-12-07 09:07:30.312466: step 3110, loss = 21.35, batch loss = 21.27 (10.6 examples/sec; 0.755 sec/batch; 69h:04m:33s remains)
INFO - root - 2017-12-07 09:07:37.591280: step 3120, loss = 21.33, batch loss = 21.25 (11.2 examples/sec; 0.714 sec/batch; 65h:17m:41s remains)
INFO - root - 2017-12-07 09:07:44.898370: step 3130, loss = 21.79, batch loss = 21.70 (10.6 examples/sec; 0.755 sec/batch; 69h:03m:25s remains)
INFO - root - 2017-12-07 09:07:52.300499: step 3140, loss = 21.46, batch loss = 21.38 (10.4 examples/sec; 0.767 sec/batch; 70h:09m:39s remains)
INFO - root - 2017-12-07 09:07:59.695616: step 3150, loss = 20.80, batch loss = 20.72 (10.9 examples/sec; 0.731 sec/batch; 66h:53m:49s remains)
INFO - root - 2017-12-07 09:08:07.040357: step 3160, loss = 21.28, batch loss = 21.20 (10.6 examples/sec; 0.754 sec/batch; 68h:56m:53s remains)
INFO - root - 2017-12-07 09:08:14.328686: step 3170, loss = 21.44, batch loss = 21.36 (11.5 examples/sec; 0.698 sec/batch; 63h:52m:38s remains)
INFO - root - 2017-12-07 09:08:21.697345: step 3180, loss = 22.14, batch loss = 22.06 (11.4 examples/sec; 0.699 sec/batch; 63h:56m:28s remains)
INFO - root - 2017-12-07 09:08:29.144133: step 3190, loss = 21.59, batch loss = 21.51 (11.1 examples/sec; 0.723 sec/batch; 66h:07m:37s remains)
INFO - root - 2017-12-07 09:08:36.433442: step 3200, loss = 21.33, batch loss = 21.25 (11.3 examples/sec; 0.706 sec/batch; 64h:34m:37s remains)
2017-12-07 09:08:37.239326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6203728 -4.6710596 -4.7074904 -4.7217884 -4.7260222 -4.7318377 -4.7470231 -4.774992 -4.8142591 -4.8480806 -4.8607578 -4.8452578 -4.8011909 -4.744422 -4.7011437][-4.5382609 -4.6010079 -4.6591072 -4.6970167 -4.7085781 -4.7007179 -4.6944838 -4.7089248 -4.7515965 -4.8089333 -4.8538275 -4.85741 -4.8022189 -4.7062593 -4.6173873][-4.4693289 -4.5608544 -4.658534 -4.72796 -4.7444553 -4.7150412 -4.6753778 -4.6630344 -4.695796 -4.762166 -4.8298507 -4.853766 -4.7976332 -4.6690125 -4.534019][-4.4055123 -4.5359349 -4.6834984 -4.7887197 -4.8081474 -4.7480369 -4.6615734 -4.6126204 -4.6322618 -4.70811 -4.7969747 -4.8393979 -4.7845874 -4.6314149 -4.4568071][-4.3048515 -4.471951 -4.6645722 -4.7932544 -4.7971649 -4.6793809 -4.5187407 -4.423543 -4.4457369 -4.5621119 -4.7028494 -4.7845073 -4.746479 -4.5909839 -4.4017096][-4.1885381 -4.3734221 -4.5872617 -4.7107434 -4.6695142 -4.4605513 -4.1941786 -4.0369229 -4.0727034 -4.2582278 -4.4864397 -4.640399 -4.6466117 -4.5184622 -4.3475285][-4.1618915 -4.3378477 -4.5346704 -4.6157007 -4.4990654 -4.1802707 -3.794791 -3.5639644 -3.6104279 -3.8713765 -4.196578 -4.4381332 -4.5075846 -4.4269528 -4.2919221][-4.221261 -4.3767991 -4.5486059 -4.5924196 -4.4203162 -4.0224257 -3.5378838 -3.2271051 -3.2590613 -3.56591 -3.9652915 -4.2813168 -4.4099078 -4.38129 -4.2875376][-4.3131475 -4.4444509 -4.5996833 -4.6477566 -4.4933419 -4.1060514 -3.6007843 -3.2406011 -3.224854 -3.5087485 -3.9142518 -4.2539368 -4.4064307 -4.4066653 -4.3406124][-4.4255996 -4.5281286 -4.6586938 -4.7203546 -4.6249285 -4.320292 -3.8772409 -3.5285323 -3.4750857 -3.7019646 -4.0645924 -4.3789597 -4.5160394 -4.5132422 -4.4553804][-4.5230203 -4.6053634 -4.7093515 -4.7789 -4.742939 -4.5411148 -4.2025948 -3.9112968 -3.8487251 -4.0234008 -4.3264518 -4.5854788 -4.677917 -4.6455874 -4.575779][-4.6137514 -4.687974 -4.7736483 -4.8465762 -4.858191 -4.7531581 -4.5283031 -4.3099179 -4.2486577 -4.3752718 -4.6060219 -4.7853255 -4.8085747 -4.7237172 -4.6229086][-4.6650243 -4.74323 -4.8254404 -4.9002852 -4.9395585 -4.9034176 -4.7732038 -4.6290107 -4.5846052 -4.675631 -4.8368812 -4.9404631 -4.9079952 -4.7903724 -4.6724477][-4.6497521 -4.7316036 -4.817616 -4.89405 -4.9461088 -4.9509935 -4.891643 -4.8146133 -4.7907352 -4.8472261 -4.93943 -4.9846892 -4.9388576 -4.8438716 -4.75599][-4.6032209 -4.6806874 -4.7665606 -4.83843 -4.8917294 -4.9190297 -4.9076071 -4.8744287 -4.8483815 -4.848918 -4.858799 -4.8471045 -4.8071022 -4.770175 -4.7554007]]...]
INFO - root - 2017-12-07 09:08:44.507921: step 3210, loss = 21.48, batch loss = 21.40 (10.7 examples/sec; 0.746 sec/batch; 68h:16m:10s remains)
INFO - root - 2017-12-07 09:08:51.679107: step 3220, loss = 21.16, batch loss = 21.08 (11.1 examples/sec; 0.720 sec/batch; 65h:49m:56s remains)
INFO - root - 2017-12-07 09:08:59.081378: step 3230, loss = 21.31, batch loss = 21.23 (10.7 examples/sec; 0.748 sec/batch; 68h:25m:23s remains)
INFO - root - 2017-12-07 09:09:06.396933: step 3240, loss = 21.40, batch loss = 21.32 (10.8 examples/sec; 0.738 sec/batch; 67h:28m:03s remains)
INFO - root - 2017-12-07 09:09:13.711893: step 3250, loss = 21.20, batch loss = 21.11 (10.6 examples/sec; 0.752 sec/batch; 68h:45m:00s remains)
INFO - root - 2017-12-07 09:09:20.961563: step 3260, loss = 21.45, batch loss = 21.37 (11.2 examples/sec; 0.714 sec/batch; 65h:18m:51s remains)
INFO - root - 2017-12-07 09:09:28.255162: step 3270, loss = 21.95, batch loss = 21.87 (10.8 examples/sec; 0.741 sec/batch; 67h:43m:26s remains)
INFO - root - 2017-12-07 09:09:35.622341: step 3280, loss = 21.31, batch loss = 21.23 (10.5 examples/sec; 0.759 sec/batch; 69h:21m:56s remains)
INFO - root - 2017-12-07 09:09:42.970400: step 3290, loss = 21.27, batch loss = 21.19 (11.6 examples/sec; 0.689 sec/batch; 63h:00m:31s remains)
INFO - root - 2017-12-07 09:09:50.341232: step 3300, loss = 21.25, batch loss = 21.17 (11.2 examples/sec; 0.711 sec/batch; 65h:03m:05s remains)
2017-12-07 09:09:51.098715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3350978 -4.3719888 -4.4506 -4.5390573 -4.6113143 -4.6512017 -4.6591291 -4.6408844 -4.5835381 -4.4824471 -4.3608904 -4.2512593 -4.2027965 -4.2414207 -4.3345428][-4.3385892 -4.3984261 -4.4726539 -4.539834 -4.5934105 -4.6284785 -4.6490154 -4.653017 -4.6311693 -4.5866938 -4.5296397 -4.4678454 -4.421896 -4.4069505 -4.4237628][-4.3158016 -4.3811932 -4.4569855 -4.514535 -4.5506625 -4.5692515 -4.5823064 -4.5915766 -4.6000562 -4.6178384 -4.6382194 -4.6428385 -4.6193719 -4.5737538 -4.5313845][-4.3361044 -4.3850083 -4.4467583 -4.4857149 -4.4946365 -4.4787049 -4.4556017 -4.44194 -4.4631653 -4.5352144 -4.6315007 -4.709981 -4.7328391 -4.6980519 -4.6400084][-4.38038 -4.40188 -4.4340477 -4.4425564 -4.4160414 -4.351934 -4.2692556 -4.2068343 -4.2225485 -4.3400788 -4.5072179 -4.6572676 -4.7340164 -4.7332687 -4.6933174][-4.4183517 -4.4175167 -4.4122934 -4.3792849 -4.3095241 -4.1876564 -4.0329013 -3.9131289 -3.9230468 -4.0907598 -4.3288832 -4.5435243 -4.6643271 -4.6902618 -4.669323][-4.4683552 -4.462502 -4.4201946 -4.3377919 -4.2159262 -4.0284848 -3.7955296 -3.6140962 -3.6166689 -3.8296402 -4.129015 -4.395462 -4.5489063 -4.5936933 -4.5917597][-4.516192 -4.5225978 -4.4638805 -4.3508172 -4.1911306 -3.9515939 -3.6540074 -3.4157524 -3.3982527 -3.6237197 -3.945142 -4.2335963 -4.4074869 -4.4732442 -4.4979591][-4.5540628 -4.57614 -4.52524 -4.4167371 -4.2562628 -4.0056453 -3.690383 -3.436398 -3.4095664 -3.61874 -3.9142065 -4.176374 -4.3361912 -4.403687 -4.4433904][-4.6019511 -4.6250277 -4.5866876 -4.498301 -4.3599505 -4.1354685 -3.8535004 -3.6334884 -3.6187391 -3.7994642 -4.0431066 -4.2499952 -4.3690648 -4.418715 -4.4541616][-4.6317267 -4.6397228 -4.6095271 -4.54733 -4.4477072 -4.2765741 -4.0620985 -3.9040332 -3.9084682 -4.0523543 -4.2315774 -4.3711143 -4.4398403 -4.4635572 -4.4845171][-4.6204033 -4.6060448 -4.582016 -4.5523868 -4.5052428 -4.4058008 -4.274087 -4.1835656 -4.1990333 -4.293766 -4.3960843 -4.4591727 -4.4752755 -4.4734821 -4.4825296][-4.5584474 -4.5249829 -4.5030928 -4.5024152 -4.5094352 -4.4872026 -4.4401755 -4.4082723 -4.4209991 -4.4569631 -4.4757195 -4.4605231 -4.428113 -4.4103212 -4.42438][-4.4726624 -4.4309731 -4.4079819 -4.4238062 -4.467937 -4.5041103 -4.5222731 -4.5322866 -4.5383687 -4.5264149 -4.4813228 -4.4112659 -4.347971 -4.3239665 -4.3512945][-4.4059286 -4.3719125 -4.3451381 -4.3559675 -4.4044657 -4.4610777 -4.508153 -4.5370173 -4.5395942 -4.5067854 -4.4381595 -4.3544874 -4.2913241 -4.27605 -4.3167205]]...]
INFO - root - 2017-12-07 09:09:58.387000: step 3310, loss = 21.59, batch loss = 21.51 (10.0 examples/sec; 0.797 sec/batch; 72h:52m:43s remains)
INFO - root - 2017-12-07 09:10:05.599805: step 3320, loss = 21.52, batch loss = 21.44 (10.4 examples/sec; 0.770 sec/batch; 70h:21m:47s remains)
INFO - root - 2017-12-07 09:10:12.921795: step 3330, loss = 21.42, batch loss = 21.33 (10.7 examples/sec; 0.746 sec/batch; 68h:10m:49s remains)
INFO - root - 2017-12-07 09:10:20.266158: step 3340, loss = 21.58, batch loss = 21.49 (11.2 examples/sec; 0.711 sec/batch; 65h:02m:34s remains)
INFO - root - 2017-12-07 09:10:27.698928: step 3350, loss = 21.73, batch loss = 21.65 (10.3 examples/sec; 0.779 sec/batch; 71h:14m:11s remains)
INFO - root - 2017-12-07 09:10:35.040258: step 3360, loss = 21.58, batch loss = 21.50 (11.0 examples/sec; 0.729 sec/batch; 66h:39m:22s remains)
INFO - root - 2017-12-07 09:10:42.364733: step 3370, loss = 21.58, batch loss = 21.49 (10.8 examples/sec; 0.740 sec/batch; 67h:37m:06s remains)
INFO - root - 2017-12-07 09:10:49.720170: step 3380, loss = 21.35, batch loss = 21.27 (10.3 examples/sec; 0.778 sec/batch; 71h:06m:40s remains)
INFO - root - 2017-12-07 09:10:57.086510: step 3390, loss = 21.54, batch loss = 21.46 (10.9 examples/sec; 0.735 sec/batch; 67h:14m:15s remains)
INFO - root - 2017-12-07 09:11:04.448676: step 3400, loss = 21.46, batch loss = 21.38 (10.7 examples/sec; 0.749 sec/batch; 68h:28m:29s remains)
2017-12-07 09:11:05.201442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1987767 -4.30489 -4.4112549 -4.4573336 -4.4506154 -4.4608507 -4.4855509 -4.4958911 -4.486331 -4.4582281 -4.4372935 -4.4194903 -4.3763895 -4.3433619 -4.3487697][-4.1026907 -4.2458739 -4.3639736 -4.4041858 -4.4080286 -4.4457259 -4.4939518 -4.5011358 -4.4664545 -4.4188428 -4.3978572 -4.3882284 -4.33775 -4.2706966 -4.2463236][-4.1097064 -4.2614746 -4.3770928 -4.4148307 -4.4314351 -4.4784508 -4.5172973 -4.4945526 -4.4264059 -4.3699136 -4.3680835 -4.3760791 -4.3209877 -4.2296219 -4.1819382][-4.1981478 -4.3152857 -4.4085073 -4.4532247 -4.487154 -4.5221257 -4.5153961 -4.4448543 -4.3525596 -4.3111758 -4.3460741 -4.3737607 -4.3108711 -4.1983366 -4.1324186][-4.2870817 -4.3378062 -4.3859577 -4.4350967 -4.4763579 -4.4729519 -4.3990879 -4.2846861 -4.1969762 -4.20727 -4.3053622 -4.3661046 -4.3053665 -4.1799889 -4.10005][-4.3073606 -4.2938371 -4.3016768 -4.3441143 -4.3725772 -4.3195558 -4.1823292 -4.04134 -3.9827914 -4.0618 -4.2272182 -4.3333735 -4.2987909 -4.1917644 -4.1233816][-4.2792516 -4.2274179 -4.2101369 -4.2322335 -4.2320495 -4.1342082 -3.9595647 -3.8209634 -3.8134062 -3.9640412 -4.1885796 -4.33865 -4.3454638 -4.2813292 -4.242054][-4.2586136 -4.2007146 -4.1773038 -4.170722 -4.1332588 -4.0082722 -3.8351793 -3.7284679 -3.7764235 -3.9770904 -4.2321124 -4.4019523 -4.4403372 -4.4214554 -4.4139247][-4.2780414 -4.2332006 -4.2282376 -4.2161379 -4.1671238 -4.05559 -3.9203792 -3.8559859 -3.9240458 -4.1147938 -4.3375463 -4.4717121 -4.5008979 -4.5028181 -4.5179811][-4.3237929 -4.2845659 -4.3059087 -4.3181038 -4.2908487 -4.2190962 -4.1332927 -4.1018181 -4.1638465 -4.3072653 -4.4497533 -4.4980702 -4.4676719 -4.446465 -4.466054][-4.3452096 -4.2959189 -4.3325529 -4.38058 -4.3924985 -4.3656874 -4.3198404 -4.3043475 -4.3517704 -4.4389148 -4.4916825 -4.4449887 -4.3430367 -4.2863336 -4.3077312][-4.3194571 -4.2566118 -4.2951555 -4.3669038 -4.4166474 -4.43011 -4.4047713 -4.390502 -4.4243131 -4.4696751 -4.4543009 -4.3376107 -4.1894636 -4.1138225 -4.1411705][-4.2552333 -4.1842279 -4.212225 -4.2871685 -4.3498917 -4.3750834 -4.356267 -4.35273 -4.402535 -4.4473181 -4.4030609 -4.247098 -4.0717306 -3.9838772 -4.0079341][-4.1959429 -4.1267347 -4.1462975 -4.2078533 -4.2585111 -4.2693615 -4.2432814 -4.2518034 -4.3306904 -4.4041944 -4.3677368 -4.2084551 -4.0245094 -3.9244835 -3.9361157][-4.2091565 -4.1410065 -4.1510582 -4.2009788 -4.2405872 -4.2367263 -4.1969085 -4.2039909 -4.2902293 -4.3819971 -4.370472 -4.2421956 -4.0773311 -3.9704702 -3.9641125]]...]
INFO - root - 2017-12-07 09:11:12.557162: step 3410, loss = 21.43, batch loss = 21.35 (10.5 examples/sec; 0.764 sec/batch; 69h:50m:47s remains)
INFO - root - 2017-12-07 09:11:19.881098: step 3420, loss = 21.31, batch loss = 21.23 (10.6 examples/sec; 0.753 sec/batch; 68h:47m:58s remains)
INFO - root - 2017-12-07 09:11:27.134579: step 3430, loss = 21.55, batch loss = 21.47 (10.7 examples/sec; 0.748 sec/batch; 68h:23m:30s remains)
INFO - root - 2017-12-07 09:11:34.475183: step 3440, loss = 21.24, batch loss = 21.16 (11.4 examples/sec; 0.702 sec/batch; 64h:10m:50s remains)
INFO - root - 2017-12-07 09:11:41.930884: step 3450, loss = 21.91, batch loss = 21.82 (10.4 examples/sec; 0.766 sec/batch; 69h:59m:55s remains)
INFO - root - 2017-12-07 09:11:49.220970: step 3460, loss = 21.51, batch loss = 21.43 (11.0 examples/sec; 0.724 sec/batch; 66h:13m:07s remains)
INFO - root - 2017-12-07 09:11:56.459317: step 3470, loss = 21.57, batch loss = 21.49 (11.3 examples/sec; 0.711 sec/batch; 64h:59m:32s remains)
INFO - root - 2017-12-07 09:12:03.698212: step 3480, loss = 21.26, batch loss = 21.17 (10.8 examples/sec; 0.739 sec/batch; 67h:32m:41s remains)
INFO - root - 2017-12-07 09:12:10.920396: step 3490, loss = 21.60, batch loss = 21.51 (10.9 examples/sec; 0.734 sec/batch; 67h:03m:36s remains)
INFO - root - 2017-12-07 09:12:18.247203: step 3500, loss = 21.17, batch loss = 21.09 (11.4 examples/sec; 0.702 sec/batch; 64h:11m:53s remains)
2017-12-07 09:12:18.970190: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3299651 -4.3391476 -4.3544326 -4.3665953 -4.3733821 -4.3774538 -4.3800883 -4.3854809 -4.3924508 -4.398025 -4.4040804 -4.4135227 -4.4234252 -4.4271178 -4.4140735][-4.4073434 -4.42632 -4.4473476 -4.4611835 -4.4625068 -4.4499612 -4.4287376 -4.4151497 -4.4181581 -4.4366131 -4.4619522 -4.4907126 -4.5101738 -4.5118437 -4.48782][-4.4777489 -4.5073447 -4.5304365 -4.5338211 -4.5114727 -4.4619312 -4.3975754 -4.3538551 -4.3560529 -4.4016328 -4.4655743 -4.5316405 -4.5725594 -4.5748291 -4.5379329][-4.5230546 -4.5576081 -4.5668879 -4.536592 -4.4723568 -4.3782768 -4.271543 -4.1999664 -4.2028875 -4.27757 -4.3892932 -4.5090308 -4.5881481 -4.6040249 -4.5604005][-4.5282612 -4.5579777 -4.5377374 -4.45907 -4.3478813 -4.2213593 -4.0942593 -4.01184 -4.0176735 -4.1131496 -4.264432 -4.4338846 -4.5576043 -4.60035 -4.5625176][-4.4966054 -4.5167551 -4.4650083 -4.3386822 -4.1844258 -4.0398602 -3.9191282 -3.8516896 -3.8666875 -3.9657276 -4.1300478 -4.3212776 -4.4728642 -4.5436282 -4.5244045][-4.4492722 -4.4727106 -4.4114008 -4.258841 -4.0762749 -3.9248343 -3.8251739 -3.7845311 -3.8090634 -3.8978713 -4.0479951 -4.2268481 -4.3727531 -4.4506307 -4.44873][-4.4122648 -4.4537191 -4.410676 -4.2628188 -4.0732493 -3.920507 -3.8360839 -3.8178687 -3.8532424 -3.9300237 -4.0483494 -4.1858549 -4.2964487 -4.35571 -4.3587532][-4.395915 -4.461112 -4.4480915 -4.3243823 -4.1496196 -4.0046954 -3.9304404 -3.9250355 -3.9693718 -4.0337987 -4.1111107 -4.1954613 -4.2605743 -4.2897205 -4.2895851][-4.4084854 -4.489512 -4.4995689 -4.4051375 -4.2633796 -4.1459203 -4.0880294 -4.0886478 -4.1314754 -4.1781006 -4.2161474 -4.2565427 -4.2831993 -4.2838964 -4.2731013][-4.420022 -4.5023737 -4.5256376 -4.4611759 -4.363452 -4.2911334 -4.2634592 -4.2752409 -4.3139038 -4.344955 -4.3583117 -4.3663807 -4.3569508 -4.3266053 -4.2950783][-4.4006271 -4.4727106 -4.50042 -4.46358 -4.4054561 -4.3727379 -4.3743439 -4.4014716 -4.4438648 -4.4718714 -4.4761891 -4.4614291 -4.420434 -4.3645234 -4.315361][-4.3705068 -4.4282546 -4.4555817 -4.4367957 -4.4020572 -4.3876333 -4.40273 -4.4371715 -4.4798393 -4.5058155 -4.5064731 -4.4785256 -4.4218097 -4.3623505 -4.3148947][-4.3365326 -4.3810472 -4.4047036 -4.3959274 -4.3751283 -4.3652883 -4.3759317 -4.4028745 -4.4373031 -4.4582953 -4.4572949 -4.4340067 -4.3891845 -4.3476186 -4.3156424][-4.2988095 -4.3271046 -4.3353581 -4.3247361 -4.3122606 -4.3065052 -4.3104172 -4.3219738 -4.34127 -4.3573189 -4.3679347 -4.3743591 -4.3657846 -4.353291 -4.336472]]...]
INFO - root - 2017-12-07 09:12:26.258494: step 3510, loss = 21.26, batch loss = 21.17 (10.7 examples/sec; 0.745 sec/batch; 68h:04m:25s remains)
INFO - root - 2017-12-07 09:12:33.570731: step 3520, loss = 21.08, batch loss = 20.99 (10.6 examples/sec; 0.753 sec/batch; 68h:48m:43s remains)
INFO - root - 2017-12-07 09:12:40.969686: step 3530, loss = 21.69, batch loss = 21.61 (10.3 examples/sec; 0.779 sec/batch; 71h:13m:29s remains)
INFO - root - 2017-12-07 09:12:48.195955: step 3540, loss = 21.87, batch loss = 21.79 (11.2 examples/sec; 0.717 sec/batch; 65h:30m:17s remains)
INFO - root - 2017-12-07 09:12:55.453703: step 3550, loss = 20.88, batch loss = 20.80 (10.9 examples/sec; 0.732 sec/batch; 66h:51m:30s remains)
INFO - root - 2017-12-07 09:13:02.823567: step 3560, loss = 21.20, batch loss = 21.12 (11.1 examples/sec; 0.722 sec/batch; 65h:56m:35s remains)
INFO - root - 2017-12-07 09:13:10.180328: step 3570, loss = 21.22, batch loss = 21.14 (11.3 examples/sec; 0.705 sec/batch; 64h:27m:05s remains)
INFO - root - 2017-12-07 09:13:17.715199: step 3580, loss = 21.31, batch loss = 21.23 (10.2 examples/sec; 0.787 sec/batch; 71h:55m:59s remains)
INFO - root - 2017-12-07 09:13:25.027852: step 3590, loss = 21.41, batch loss = 21.33 (11.1 examples/sec; 0.719 sec/batch; 65h:42m:39s remains)
INFO - root - 2017-12-07 09:13:32.613120: step 3600, loss = 21.31, batch loss = 21.23 (10.4 examples/sec; 0.770 sec/batch; 70h:22m:07s remains)
2017-12-07 09:13:33.385948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5109119 -4.6627827 -4.79642 -4.8609695 -4.8408012 -4.7543969 -4.634069 -4.5097857 -4.4026337 -4.3241816 -4.251936 -4.1861939 -4.1495547 -4.1812387 -4.2815394][-4.3571825 -4.5449553 -4.7150135 -4.7973094 -4.7752709 -4.681499 -4.5681729 -4.4712815 -4.3996224 -4.3497295 -4.2883306 -4.2140284 -4.1653242 -4.1794233 -4.2526665][-4.2472444 -4.4510455 -4.6267776 -4.7035804 -4.6784859 -4.5968113 -4.5142865 -4.4591618 -4.4226289 -4.3933058 -4.3388739 -4.2705026 -4.2300076 -4.231832 -4.2689314][-4.2418747 -4.4475389 -4.6040173 -4.6525669 -4.6126704 -4.5399189 -4.4837704 -4.4552007 -4.4353237 -4.4121776 -4.3662519 -4.3271079 -4.3244624 -4.3327441 -4.3372521][-4.3177919 -4.5116429 -4.6362119 -4.6438446 -4.5760288 -4.4991326 -4.45011 -4.4211836 -4.3984113 -4.3789725 -4.3594942 -4.3743463 -4.4239326 -4.4466052 -4.4224277][-4.4213972 -4.5910316 -4.6693344 -4.6215925 -4.507937 -4.4032936 -4.3386989 -4.2935586 -4.2699761 -4.2837176 -4.325037 -4.4077621 -4.5015745 -4.5316978 -4.4814272][-4.5334382 -4.669076 -4.6891637 -4.571578 -4.3891487 -4.22514 -4.1182294 -4.05267 -4.0491056 -4.129981 -4.25412 -4.3950272 -4.5096354 -4.5381441 -4.4736466][-4.6156812 -4.7183094 -4.696476 -4.5307903 -4.2900033 -4.060874 -3.8994255 -3.8179049 -3.8477535 -3.9987538 -4.1887259 -4.3537922 -4.4551392 -4.4656625 -4.3920321][-4.6501036 -4.7343636 -4.7047057 -4.54163 -4.2943287 -4.0383406 -3.845202 -3.7577577 -3.8112073 -3.9940526 -4.2009358 -4.348515 -4.4052939 -4.37744 -4.2866635][-4.6405349 -4.725215 -4.7205205 -4.6067052 -4.4094019 -4.1829867 -4.0018573 -3.927716 -3.9914346 -4.1604805 -4.3368697 -4.4360538 -4.4299121 -4.3468919 -4.2292314][-4.6028738 -4.6938581 -4.7255311 -4.6732578 -4.5407314 -4.3650975 -4.2149596 -4.1602573 -4.2265472 -4.3695192 -4.5121927 -4.5803013 -4.5415883 -4.428885 -4.2995944][-4.5654831 -4.6566839 -4.7128916 -4.7048445 -4.6220427 -4.4895239 -4.3670764 -4.3200312 -4.3725381 -4.4801311 -4.5921793 -4.6519194 -4.625721 -4.5399356 -4.4387064][-4.5248508 -4.6103578 -4.6777234 -4.697175 -4.6550369 -4.5684462 -4.4820457 -4.4437981 -4.4739647 -4.5366654 -4.6020255 -4.6362715 -4.6194673 -4.5696239 -4.5091157][-4.4549861 -4.5325007 -4.6032963 -4.6432858 -4.6418915 -4.6102281 -4.5721765 -4.5544648 -4.5706429 -4.5981393 -4.6194859 -4.6201735 -4.5965157 -4.5640173 -4.5341258][-4.3776712 -4.4393864 -4.5036163 -4.5528035 -4.5779819 -4.5875597 -4.5918694 -4.5995455 -4.6139688 -4.6252942 -4.6249084 -4.6098495 -4.5841804 -4.5638819 -4.5589252]]...]
INFO - root - 2017-12-07 09:13:42.275038: step 3610, loss = 21.44, batch loss = 21.36 (10.2 examples/sec; 0.788 sec/batch; 71h:58m:53s remains)
INFO - root - 2017-12-07 09:13:51.395708: step 3620, loss = 20.95, batch loss = 20.87 (8.6 examples/sec; 0.927 sec/batch; 84h:38m:57s remains)
INFO - root - 2017-12-07 09:14:00.827946: step 3630, loss = 21.54, batch loss = 21.46 (8.3 examples/sec; 0.966 sec/batch; 88h:12m:19s remains)
INFO - root - 2017-12-07 09:14:10.212544: step 3640, loss = 21.58, batch loss = 21.50 (8.5 examples/sec; 0.941 sec/batch; 85h:56m:59s remains)
INFO - root - 2017-12-07 09:14:19.530291: step 3650, loss = 21.56, batch loss = 21.48 (8.4 examples/sec; 0.955 sec/batch; 87h:12m:58s remains)
INFO - root - 2017-12-07 09:14:29.052797: step 3660, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.973 sec/batch; 88h:52m:08s remains)
INFO - root - 2017-12-07 09:14:38.441015: step 3670, loss = 21.53, batch loss = 21.45 (8.5 examples/sec; 0.940 sec/batch; 85h:51m:25s remains)
INFO - root - 2017-12-07 09:14:47.835004: step 3680, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.931 sec/batch; 85h:01m:28s remains)
INFO - root - 2017-12-07 09:14:57.274102: step 3690, loss = 21.49, batch loss = 21.41 (8.5 examples/sec; 0.942 sec/batch; 86h:03m:53s remains)
INFO - root - 2017-12-07 09:15:06.576205: step 3700, loss = 21.27, batch loss = 21.19 (8.6 examples/sec; 0.933 sec/batch; 85h:14m:29s remains)
2017-12-07 09:15:07.541645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3856497 -4.3823009 -4.3655825 -4.3441968 -4.3368607 -4.3459959 -4.3856597 -4.4014215 -4.3437214 -4.274478 -4.2643661 -4.2830815 -4.3333821 -4.406363 -4.445765][-4.3354278 -4.33861 -4.3430362 -4.3429766 -4.3517919 -4.3661256 -4.4089179 -4.4184308 -4.3541436 -4.2775 -4.2471414 -4.2387314 -4.2920518 -4.3999324 -4.4759817][-4.3510256 -4.3562775 -4.3610144 -4.3660188 -4.3866181 -4.4122496 -4.4534736 -4.4512315 -4.3897657 -4.3159661 -4.2716045 -4.2472825 -4.3018322 -4.4236217 -4.512907][-4.4252596 -4.4236026 -4.4060807 -4.3911195 -4.4082618 -4.435781 -4.4579654 -4.4390783 -4.3890233 -4.3382936 -4.3108459 -4.3004308 -4.3588276 -4.4703536 -4.5476241][-4.5260758 -4.4991145 -4.42948 -4.3632708 -4.35316 -4.3645267 -4.3591423 -4.3234625 -4.2919154 -4.2852254 -4.3040318 -4.3323717 -4.3971086 -4.4847116 -4.5356512][-4.585803 -4.5195041 -4.3831387 -4.2521853 -4.2039852 -4.1911845 -4.1588559 -4.1109743 -4.1082778 -4.1647983 -4.248198 -4.3233051 -4.3920097 -4.45079 -4.4736228][-4.5664062 -4.4642615 -4.2763839 -4.094276 -4.0088673 -3.9724667 -3.9200177 -3.8720536 -3.9093571 -4.0362864 -4.1848917 -4.3024931 -4.3802042 -4.4217482 -4.4216151][-4.4853373 -4.3632727 -4.1611161 -3.9651494 -3.8630848 -3.8117132 -3.7509708 -3.7144406 -3.7906592 -3.97451 -4.1697359 -4.3140821 -4.3981781 -4.4267969 -4.4063458][-4.4378829 -4.3296285 -4.1621413 -4.000566 -3.9099236 -3.8579268 -3.7972 -3.7643161 -3.8424995 -4.0285988 -4.2244525 -4.365561 -4.44194 -4.4547544 -4.4168572][-4.4498858 -4.3784676 -4.2683883 -4.1584206 -4.0921879 -4.0506372 -4.0003057 -3.9679654 -4.0189285 -4.1560068 -4.3067589 -4.4193034 -4.4809165 -4.4844375 -4.4414172][-4.4971318 -4.4634957 -4.4053478 -4.3426471 -4.3040538 -4.2817497 -4.2511177 -4.2258525 -4.2447453 -4.3134685 -4.3970671 -4.4664598 -4.5077786 -4.5059361 -4.4686422][-4.5286279 -4.5274882 -4.511107 -4.4890037 -4.4777317 -4.4756956 -4.4657259 -4.4523387 -4.451715 -4.466588 -4.4892964 -4.5136003 -4.5293722 -4.5202646 -4.4892192][-4.5066113 -4.5239177 -4.5285921 -4.5286975 -4.5350289 -4.5472655 -4.5509896 -4.547503 -4.5412078 -4.5318589 -4.5240941 -4.5219526 -4.5208011 -4.507102 -4.481276][-4.4589319 -4.4734464 -4.4770408 -4.479672 -4.4872503 -4.4987874 -4.5030136 -4.5015283 -4.4954138 -4.4828272 -4.4697652 -4.4624186 -4.459794 -4.4522395 -4.438262][-4.4459319 -4.4490767 -4.4444222 -4.4425211 -4.4452929 -4.4509706 -4.4507051 -4.4479175 -4.4423194 -4.4322491 -4.4226003 -4.4180007 -4.4195881 -4.420207 -4.4184675]]...]
INFO - root - 2017-12-07 09:15:16.773118: step 3710, loss = 21.64, batch loss = 21.56 (9.0 examples/sec; 0.894 sec/batch; 81h:37m:01s remains)
INFO - root - 2017-12-07 09:15:26.141411: step 3720, loss = 21.44, batch loss = 21.36 (8.5 examples/sec; 0.946 sec/batch; 86h:24m:28s remains)
INFO - root - 2017-12-07 09:15:35.557114: step 3730, loss = 21.55, batch loss = 21.46 (9.0 examples/sec; 0.892 sec/batch; 81h:27m:33s remains)
INFO - root - 2017-12-07 09:15:44.920009: step 3740, loss = 21.66, batch loss = 21.58 (9.1 examples/sec; 0.876 sec/batch; 80h:01m:54s remains)
INFO - root - 2017-12-07 09:15:54.416310: step 3750, loss = 21.53, batch loss = 21.45 (8.8 examples/sec; 0.910 sec/batch; 83h:05m:48s remains)
INFO - root - 2017-12-07 09:16:03.802625: step 3760, loss = 21.30, batch loss = 21.21 (8.8 examples/sec; 0.909 sec/batch; 83h:00m:20s remains)
INFO - root - 2017-12-07 09:16:13.289836: step 3770, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.934 sec/batch; 85h:15m:47s remains)
INFO - root - 2017-12-07 09:16:22.733451: step 3780, loss = 21.45, batch loss = 21.36 (8.4 examples/sec; 0.958 sec/batch; 87h:27m:12s remains)
INFO - root - 2017-12-07 09:16:32.067557: step 3790, loss = 21.69, batch loss = 21.61 (8.7 examples/sec; 0.917 sec/batch; 83h:42m:46s remains)
INFO - root - 2017-12-07 09:16:41.324032: step 3800, loss = 21.31, batch loss = 21.22 (8.8 examples/sec; 0.911 sec/batch; 83h:08m:05s remains)
2017-12-07 09:16:42.271368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4181442 -4.3388081 -4.2533937 -4.2215004 -4.3259916 -4.4684367 -4.5776925 -4.6012063 -4.5145388 -4.3906126 -4.2850885 -4.2673521 -4.3382215 -4.4694948 -4.600843][-4.3767915 -4.2925658 -4.2079134 -4.1861963 -4.308322 -4.4623985 -4.5765018 -4.5870261 -4.4820342 -4.364922 -4.3013172 -4.3334394 -4.4314342 -4.5423412 -4.6185665][-4.4526691 -4.3706994 -4.2861829 -4.2642407 -4.3652706 -4.4927177 -4.582541 -4.5760431 -4.4772563 -4.3898888 -4.369791 -4.4257841 -4.5165863 -4.5965104 -4.6242313][-4.5879583 -4.5093951 -4.4241457 -4.3887296 -4.4345679 -4.4942565 -4.5307231 -4.5028086 -4.4296227 -4.3934827 -4.420249 -4.4850154 -4.5463729 -4.5890651 -4.5803676][-4.6825376 -4.613728 -4.5332661 -4.4778481 -4.4540005 -4.4248867 -4.385819 -4.324471 -4.2874565 -4.3215775 -4.4044089 -4.4862309 -4.5319395 -4.5567 -4.5339117][-4.6834693 -4.6313362 -4.5536427 -4.4661541 -4.3652291 -4.2457776 -4.1312656 -4.0433683 -4.0588045 -4.1849489 -4.3366776 -4.4552164 -4.5184956 -4.5520396 -4.5354586][-4.6176467 -4.5826454 -4.5019331 -4.3710341 -4.1919165 -3.9932351 -3.8168092 -3.7148595 -3.8011131 -4.0290351 -4.2553291 -4.4246593 -4.5245557 -4.5757656 -4.5707812][-4.5452714 -4.5340824 -4.4546242 -4.2902641 -4.0589304 -3.8094862 -3.5804284 -3.4604387 -3.609726 -3.9220319 -4.2140293 -4.4349151 -4.5607114 -4.6102729 -4.6052861][-4.4969 -4.52205 -4.4613886 -4.297286 -4.063942 -3.8135488 -3.5581951 -3.4314659 -3.6107368 -3.9513791 -4.2745333 -4.5146737 -4.6241517 -4.6387978 -4.6144085][-4.49253 -4.5651059 -4.5431347 -4.4154158 -4.2288356 -4.0133829 -3.7636142 -3.6497352 -3.8197191 -4.1283112 -4.4355254 -4.6426263 -4.690474 -4.6464796 -4.5889893][-4.5371637 -4.6536212 -4.6771455 -4.5991368 -4.4680614 -4.2854004 -4.0606346 -3.9782104 -4.1239314 -4.3764248 -4.62999 -4.7659464 -4.7361112 -4.6346874 -4.5423131][-4.5973639 -4.7406082 -4.8015862 -4.7660589 -4.6761069 -4.5187244 -4.3336172 -4.2878542 -4.4102106 -4.6074376 -4.7884717 -4.8398442 -4.7457204 -4.6065192 -4.4905019][-4.6256642 -4.7806034 -4.8682055 -4.8678875 -4.8102612 -4.6841154 -4.5475731 -4.5250745 -4.6180549 -4.7523866 -4.8492169 -4.82464 -4.6951656 -4.5456214 -4.4261746][-4.5970097 -4.7496877 -4.853879 -4.882154 -4.8559031 -4.7748718 -4.6870279 -4.6667328 -4.7104578 -4.7678342 -4.7806535 -4.70687 -4.5764351 -4.4459066 -4.3458409][-4.520278 -4.6512747 -4.751174 -4.7930484 -4.7910042 -4.7513518 -4.701138 -4.6755486 -4.6706347 -4.6610985 -4.6183982 -4.532475 -4.4289112 -4.3363767 -4.2694063]]...]
INFO - root - 2017-12-07 09:16:51.682665: step 3810, loss = 21.14, batch loss = 21.06 (8.8 examples/sec; 0.913 sec/batch; 83h:23m:28s remains)
INFO - root - 2017-12-07 09:17:00.979369: step 3820, loss = 21.38, batch loss = 21.29 (8.9 examples/sec; 0.900 sec/batch; 82h:11m:02s remains)
INFO - root - 2017-12-07 09:17:10.490392: step 3830, loss = 21.56, batch loss = 21.47 (8.5 examples/sec; 0.939 sec/batch; 85h:43m:40s remains)
INFO - root - 2017-12-07 09:17:19.884442: step 3840, loss = 21.44, batch loss = 21.36 (8.7 examples/sec; 0.916 sec/batch; 83h:37m:18s remains)
INFO - root - 2017-12-07 09:17:29.157198: step 3850, loss = 21.53, batch loss = 21.45 (9.1 examples/sec; 0.877 sec/batch; 80h:03m:11s remains)
INFO - root - 2017-12-07 09:17:38.455050: step 3860, loss = 21.79, batch loss = 21.70 (8.7 examples/sec; 0.915 sec/batch; 83h:34m:22s remains)
INFO - root - 2017-12-07 09:17:47.811676: step 3870, loss = 21.37, batch loss = 21.28 (8.7 examples/sec; 0.920 sec/batch; 83h:58m:24s remains)
INFO - root - 2017-12-07 09:17:57.313553: step 3880, loss = 21.44, batch loss = 21.36 (8.7 examples/sec; 0.920 sec/batch; 83h:57m:06s remains)
INFO - root - 2017-12-07 09:18:06.667235: step 3890, loss = 21.33, batch loss = 21.25 (8.4 examples/sec; 0.948 sec/batch; 86h:31m:01s remains)
INFO - root - 2017-12-07 09:18:15.949950: step 3900, loss = 21.18, batch loss = 21.10 (8.6 examples/sec; 0.925 sec/batch; 84h:26m:39s remains)
2017-12-07 09:18:16.861206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3659081 -4.3898077 -4.4265523 -4.4692616 -4.5122113 -4.5453982 -4.5547843 -4.5270662 -4.4636111 -4.3762784 -4.2922797 -4.2515965 -4.2731142 -4.3437147 -4.4288826][-4.3918877 -4.4336319 -4.4866986 -4.540379 -4.589076 -4.6251349 -4.6420546 -4.6260648 -4.5770488 -4.5073829 -4.4400411 -4.4071131 -4.4242816 -4.4778786 -4.5368533][-4.4560871 -4.5164452 -4.5747042 -4.6180339 -4.6469059 -4.6651335 -4.6767859 -4.6704469 -4.6411572 -4.5976253 -4.5553761 -4.5356445 -4.5512 -4.5895095 -4.6259036][-4.5519714 -4.6184096 -4.66346 -4.6739821 -4.6594405 -4.6363015 -4.6212173 -4.610796 -4.5975718 -4.5847406 -4.5779 -4.5837069 -4.607163 -4.6391187 -4.6641269][-4.6419539 -4.6944056 -4.7084403 -4.6716437 -4.6004233 -4.5198359 -4.4585471 -4.4286184 -4.425343 -4.4439545 -4.4796157 -4.5231061 -4.5674424 -4.6047354 -4.6323895][-4.6730018 -4.6995983 -4.6790609 -4.5998049 -4.4809327 -4.3495846 -4.2454996 -4.1975565 -4.205163 -4.2496181 -4.3196354 -4.3968916 -4.4656425 -4.5172172 -4.5572619][-4.6235304 -4.633718 -4.5936737 -4.4839721 -4.3258924 -4.1501193 -4.0111265 -3.9567854 -3.9877262 -4.0630531 -4.1631622 -4.2699413 -4.36389 -4.4378786 -4.49769][-4.5224867 -4.5448418 -4.506773 -4.3746376 -4.1738768 -3.9490771 -3.7774935 -3.722651 -3.7798967 -3.8892083 -4.0207582 -4.1589942 -4.2852631 -4.394455 -4.48413][-4.4359522 -4.4911509 -4.4708629 -4.3308454 -4.1027589 -3.8459032 -3.6561856 -3.6060069 -3.6816874 -3.8173194 -3.9771712 -4.1443038 -4.2962189 -4.4287281 -4.5333233][-4.4017086 -4.4942064 -4.5013304 -4.3769059 -4.1580491 -3.9043384 -3.7165277 -3.6737394 -3.7600894 -3.9118443 -4.0897675 -4.2673826 -4.415863 -4.5299783 -4.6096559][-4.4067307 -4.5287666 -4.5614748 -4.4661651 -4.283628 -4.0644183 -3.9013553 -3.8708849 -3.9593611 -4.1118183 -4.2872038 -4.4476724 -4.56178 -4.6243987 -4.6506152][-4.4481511 -4.5804443 -4.6296978 -4.5643773 -4.4278064 -4.2641521 -4.1465559 -4.1344495 -4.2170615 -4.3495979 -4.4909658 -4.6019497 -4.6571631 -4.658556 -4.6329226][-4.4994788 -4.6267076 -4.6856794 -4.6542563 -4.5684581 -4.4654608 -4.3955216 -4.3949866 -4.4570918 -4.5452905 -4.6242285 -4.6621437 -4.6512823 -4.6066623 -4.5562792][-4.5243683 -4.6290255 -4.6912441 -4.6921744 -4.6512556 -4.5949249 -4.5546441 -4.5495343 -4.5789194 -4.6169939 -4.6343317 -4.6108479 -4.5574412 -4.5005674 -4.459662][-4.507956 -4.5725107 -4.6208639 -4.6369839 -4.6258774 -4.6014571 -4.578382 -4.5646205 -4.5670314 -4.5739188 -4.5625954 -4.518352 -4.4589262 -4.41433 -4.3947606]]...]
INFO - root - 2017-12-07 09:18:26.242442: step 3910, loss = 22.12, batch loss = 22.04 (8.2 examples/sec; 0.978 sec/batch; 89h:16m:31s remains)
INFO - root - 2017-12-07 09:18:35.606007: step 3920, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.948 sec/batch; 86h:28m:54s remains)
INFO - root - 2017-12-07 09:18:45.014946: step 3930, loss = 21.61, batch loss = 21.52 (8.6 examples/sec; 0.929 sec/batch; 84h:47m:56s remains)
INFO - root - 2017-12-07 09:18:54.416741: step 3940, loss = 21.33, batch loss = 21.25 (8.0 examples/sec; 0.999 sec/batch; 91h:08m:15s remains)
INFO - root - 2017-12-07 09:19:03.718017: step 3950, loss = 21.66, batch loss = 21.58 (8.9 examples/sec; 0.895 sec/batch; 81h:39m:58s remains)
INFO - root - 2017-12-07 09:19:13.065505: step 3960, loss = 21.55, batch loss = 21.47 (9.1 examples/sec; 0.881 sec/batch; 80h:23m:02s remains)
INFO - root - 2017-12-07 09:19:22.440530: step 3970, loss = 21.20, batch loss = 21.11 (8.1 examples/sec; 0.994 sec/batch; 90h:40m:06s remains)
INFO - root - 2017-12-07 09:19:31.780680: step 3980, loss = 22.05, batch loss = 21.96 (8.7 examples/sec; 0.918 sec/batch; 83h:45m:57s remains)
INFO - root - 2017-12-07 09:19:41.306748: step 3990, loss = 21.47, batch loss = 21.39 (8.3 examples/sec; 0.958 sec/batch; 87h:27m:30s remains)
INFO - root - 2017-12-07 09:19:50.715408: step 4000, loss = 21.69, batch loss = 21.61 (8.3 examples/sec; 0.961 sec/batch; 87h:42m:25s remains)
2017-12-07 09:19:51.599528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2662158 -4.370008 -4.475369 -4.5522604 -4.608798 -4.6581006 -4.6948752 -4.70578 -4.6897345 -4.6630912 -4.6541038 -4.6734071 -4.6675448 -4.5953121 -4.5119953][-4.2867012 -4.4013762 -4.5075936 -4.5683479 -4.5933094 -4.6028252 -4.6116667 -4.6172252 -4.6045532 -4.5862274 -4.5949607 -4.6464472 -4.67444 -4.6147904 -4.516304][-4.2402682 -4.3577771 -4.463788 -4.5133076 -4.5141916 -4.4953723 -4.491374 -4.50341 -4.5019541 -4.4902315 -4.5032325 -4.5634904 -4.6146846 -4.581543 -4.4864559][-4.1403089 -4.2473574 -4.3544726 -4.4046755 -4.399076 -4.3734794 -4.3739758 -4.4026065 -4.4204445 -4.4235826 -4.4377546 -4.4872317 -4.5368438 -4.5153694 -4.4182124][-4.0298896 -4.1135807 -4.21842 -4.274601 -4.2732205 -4.2521291 -4.2627268 -4.3081512 -4.3487444 -4.3759007 -4.3999114 -4.4369941 -4.4693346 -4.4373779 -4.3269777][-3.97263 -4.0159397 -4.0933552 -4.1348872 -4.1265893 -4.1043987 -4.11831 -4.1793246 -4.2562265 -4.3292222 -4.38561 -4.4259171 -4.4371057 -4.3806925 -4.2523694][-3.9818454 -3.9851189 -4.01443 -4.0140271 -3.9773669 -3.9352994 -3.9355309 -4.0042305 -4.1223803 -4.2530007 -4.3604321 -4.4272604 -4.4371858 -4.3704696 -4.2420111][-4.0416245 -4.0220871 -4.006196 -3.9586077 -3.881459 -3.7986708 -3.7607427 -3.8163357 -3.9611092 -4.140028 -4.2978358 -4.4065661 -4.4438205 -4.3987474 -4.2984643][-4.121459 -4.0943937 -4.0495543 -3.9715137 -3.8679869 -3.7498813 -3.6629739 -3.6805003 -3.8217368 -4.0215425 -4.2081356 -4.3494687 -4.4242516 -4.4238992 -4.3740091][-4.194087 -4.173624 -4.1258483 -4.0425968 -3.931906 -3.7971191 -3.6746356 -3.6483526 -3.7617497 -3.9543819 -4.1417737 -4.2899642 -4.3881483 -4.4289532 -4.42864][-4.271512 -4.2582469 -4.2188883 -4.1464729 -4.0435824 -3.9103012 -3.7740564 -3.7200527 -3.8050449 -3.9795668 -4.1520157 -4.2876892 -4.3856835 -4.4416766 -4.4614315][-4.36467 -4.3498373 -4.3094778 -4.2426529 -4.1510348 -4.0337873 -3.9097753 -3.8540237 -3.9234188 -4.0801153 -4.2338958 -4.3481236 -4.4266162 -4.4678187 -4.4735065][-4.45622 -4.4397259 -4.3918552 -4.3187542 -4.2290888 -4.1255674 -4.0243459 -3.9843378 -4.0487838 -4.1888361 -4.3264151 -4.4219136 -4.4760818 -4.4896646 -4.4678783][-4.5085683 -4.4959273 -4.4394054 -4.3516774 -4.2542644 -4.1567698 -4.0760612 -4.0536442 -4.1150241 -4.2413721 -4.3675065 -4.4511657 -4.4892359 -4.4833832 -4.4442539][-4.50593 -4.4900174 -4.4204154 -4.3133917 -4.205195 -4.1139412 -4.055984 -4.0541477 -4.1171317 -4.2341619 -4.3549623 -4.4359412 -4.4704947 -4.4586143 -4.4151859]]...]
INFO - root - 2017-12-07 09:20:00.899441: step 4010, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.930 sec/batch; 84h:53m:16s remains)
INFO - root - 2017-12-07 09:20:10.309707: step 4020, loss = 21.09, batch loss = 21.01 (8.8 examples/sec; 0.911 sec/batch; 83h:08m:51s remains)
INFO - root - 2017-12-07 09:20:19.538351: step 4030, loss = 21.24, batch loss = 21.16 (8.9 examples/sec; 0.895 sec/batch; 81h:41m:22s remains)
INFO - root - 2017-12-07 09:20:28.969127: step 4040, loss = 21.52, batch loss = 21.44 (9.0 examples/sec; 0.889 sec/batch; 81h:09m:04s remains)
INFO - root - 2017-12-07 09:20:38.266274: step 4050, loss = 21.56, batch loss = 21.47 (8.6 examples/sec; 0.929 sec/batch; 84h:44m:51s remains)
INFO - root - 2017-12-07 09:20:47.680423: step 4060, loss = 21.51, batch loss = 21.43 (8.7 examples/sec; 0.923 sec/batch; 84h:11m:30s remains)
INFO - root - 2017-12-07 09:20:56.925887: step 4070, loss = 21.48, batch loss = 21.40 (8.7 examples/sec; 0.920 sec/batch; 83h:56m:40s remains)
INFO - root - 2017-12-07 09:21:06.219356: step 4080, loss = 21.55, batch loss = 21.47 (8.5 examples/sec; 0.944 sec/batch; 86h:09m:02s remains)
INFO - root - 2017-12-07 09:21:15.516462: step 4090, loss = 21.51, batch loss = 21.42 (8.6 examples/sec; 0.925 sec/batch; 84h:25m:25s remains)
INFO - root - 2017-12-07 09:21:24.749632: step 4100, loss = 21.54, batch loss = 21.46 (8.9 examples/sec; 0.898 sec/batch; 81h:53m:23s remains)
2017-12-07 09:21:25.636354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4017735 -4.3995285 -4.4141231 -4.4429574 -4.4690866 -4.4760942 -4.4733992 -4.4808908 -4.4938817 -4.4827113 -4.444241 -4.4040432 -4.3859096 -4.3920283 -4.4027696][-4.4433341 -4.4416766 -4.4628077 -4.5049844 -4.54294 -4.5494814 -4.5419765 -4.5517473 -4.5670471 -4.5510392 -4.5059085 -4.4621253 -4.4448175 -4.4553251 -4.467257][-4.4582372 -4.4529 -4.4773235 -4.5303111 -4.57662 -4.5763364 -4.55923 -4.5668573 -4.5789175 -4.5588107 -4.5191436 -4.4900584 -4.4849753 -4.4984775 -4.5069075][-4.4601669 -4.4504795 -4.4789553 -4.5357623 -4.57443 -4.548533 -4.5052557 -4.5003352 -4.5069971 -4.4913683 -4.4758234 -4.4785323 -4.4963527 -4.5118942 -4.5108066][-4.454648 -4.4423161 -4.4748654 -4.5261836 -4.5343704 -4.4594212 -4.3729854 -4.3515811 -4.3670826 -4.3810544 -4.4073186 -4.4419656 -4.4735093 -4.4800768 -4.463295][-4.442584 -4.4328413 -4.46652 -4.5037365 -4.4710269 -4.3427405 -4.2150054 -4.1895051 -4.2385645 -4.3109064 -4.3878026 -4.43917 -4.4595671 -4.4358525 -4.3958898][-4.4377 -4.4257183 -4.4494705 -4.4594555 -4.380621 -4.2060661 -4.049695 -4.0320148 -4.1281185 -4.2656279 -4.3818536 -4.4277239 -4.4137211 -4.3515406 -4.29545][-4.4283295 -4.4164195 -4.4274883 -4.4053149 -4.2824569 -4.0751 -3.9107449 -3.9122927 -4.0517282 -4.2346931 -4.3630981 -4.3888984 -4.3440108 -4.2590351 -4.2060423][-4.4023857 -4.3968658 -4.4059639 -4.3679266 -4.2303658 -4.0287304 -3.8845665 -3.9014792 -4.0484557 -4.2228403 -4.3245497 -4.3260136 -4.2741785 -4.1987972 -4.1679869][-4.3450108 -4.35053 -4.3681874 -4.3375578 -4.2220044 -4.0622377 -3.9526718 -3.9683771 -4.0793715 -4.1992583 -4.25248 -4.2422986 -4.2154856 -4.1819539 -4.1854482][-4.2770572 -4.2929831 -4.3198314 -4.3082218 -4.2351694 -4.1198044 -4.0238132 -4.0136929 -4.072433 -4.1347904 -4.1551151 -4.1562448 -4.1695218 -4.176827 -4.2076683][-4.2373338 -4.25683 -4.2811646 -4.2801275 -4.247673 -4.1683564 -4.080132 -4.0416994 -4.0557623 -4.0814557 -4.0873003 -4.1021452 -4.1420364 -4.1687231 -4.2125196][-4.2163424 -4.2286186 -4.2376723 -4.2384071 -4.235631 -4.1853476 -4.10796 -4.0580692 -4.0509353 -4.0620127 -4.0663624 -4.0918422 -4.1450882 -4.1785655 -4.2278514][-4.2352118 -4.2421746 -4.2394533 -4.2393212 -4.2507324 -4.2201362 -4.1582456 -4.1100416 -4.0962882 -4.1021323 -4.107491 -4.1416764 -4.2036481 -4.2413077 -4.2915673][-4.2906156 -4.2959189 -4.2888217 -4.2878761 -4.3019238 -4.2860475 -4.2451897 -4.208272 -4.194479 -4.1955366 -4.1955214 -4.2270317 -4.2891183 -4.3334837 -4.3852034]]...]
INFO - root - 2017-12-07 09:21:34.874187: step 4110, loss = 21.23, batch loss = 21.15 (8.5 examples/sec; 0.941 sec/batch; 85h:52m:06s remains)
INFO - root - 2017-12-07 09:21:44.167286: step 4120, loss = 21.08, batch loss = 21.00 (8.7 examples/sec; 0.924 sec/batch; 84h:17m:07s remains)
INFO - root - 2017-12-07 09:21:53.439014: step 4130, loss = 21.78, batch loss = 21.70 (8.4 examples/sec; 0.957 sec/batch; 87h:19m:02s remains)
INFO - root - 2017-12-07 09:22:02.589384: step 4140, loss = 21.47, batch loss = 21.38 (8.5 examples/sec; 0.941 sec/batch; 85h:50m:04s remains)
INFO - root - 2017-12-07 09:22:12.097442: step 4150, loss = 21.17, batch loss = 21.09 (8.6 examples/sec; 0.932 sec/batch; 85h:00m:55s remains)
INFO - root - 2017-12-07 09:22:21.608271: step 4160, loss = 21.91, batch loss = 21.83 (8.7 examples/sec; 0.916 sec/batch; 83h:30m:11s remains)
INFO - root - 2017-12-07 09:22:31.048447: step 4170, loss = 21.66, batch loss = 21.58 (8.6 examples/sec; 0.933 sec/batch; 85h:03m:47s remains)
INFO - root - 2017-12-07 09:22:40.488990: step 4180, loss = 21.61, batch loss = 21.52 (8.7 examples/sec; 0.922 sec/batch; 84h:07m:48s remains)
INFO - root - 2017-12-07 09:22:49.997246: step 4190, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.928 sec/batch; 84h:36m:55s remains)
INFO - root - 2017-12-07 09:22:59.353851: step 4200, loss = 21.69, batch loss = 21.61 (8.3 examples/sec; 0.963 sec/batch; 87h:47m:51s remains)
2017-12-07 09:23:00.238195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2812529 -4.3107696 -4.405376 -4.4785824 -4.4701052 -4.3683362 -4.2420583 -4.1781478 -4.200686 -4.2668633 -4.3221073 -4.3620958 -4.4101415 -4.4433994 -4.4356728][-4.2985945 -4.3437428 -4.4497185 -4.520843 -4.4958076 -4.3712735 -4.2309389 -4.1713996 -4.2011547 -4.2716317 -4.325346 -4.3512735 -4.3793149 -4.3989878 -4.3883624][-4.3660784 -4.4332805 -4.5333972 -4.580061 -4.5244689 -4.3762188 -4.2280087 -4.1718955 -4.202929 -4.270133 -4.317925 -4.332211 -4.3465147 -4.3598905 -4.3526525][-4.4571042 -4.5286446 -4.5939217 -4.5898666 -4.4944696 -4.327559 -4.1786332 -4.1235867 -4.1564536 -4.2327924 -4.2970557 -4.3281574 -4.3479438 -4.362442 -4.3530769][-4.5338345 -4.5868545 -4.5973277 -4.5282273 -4.3853908 -4.1977215 -4.0413017 -3.9830813 -4.028657 -4.1428638 -4.2661695 -4.3544474 -4.4049106 -4.4264188 -4.4039354][-4.5943017 -4.6195235 -4.5734024 -4.438261 -4.2435017 -4.0279112 -3.8580947 -3.7990437 -3.862751 -4.0200715 -4.2119546 -4.3714209 -4.4682784 -4.5056381 -4.4746704][-4.6327829 -4.633328 -4.5468273 -4.3697858 -4.1423659 -3.9066043 -3.7232897 -3.6557035 -3.7197797 -3.8911386 -4.1184192 -4.3279319 -4.4649324 -4.5279121 -4.509985][-4.6334887 -4.6248784 -4.5346537 -4.3628426 -4.142715 -3.911026 -3.7216733 -3.6333017 -3.6716595 -3.8256793 -4.0526795 -4.2795982 -4.4372983 -4.5213628 -4.5320206][-4.6254349 -4.6291747 -4.5709324 -4.4422178 -4.259757 -4.0499783 -3.8659678 -3.7551429 -3.7590494 -3.8850663 -4.091826 -4.3029237 -4.4473886 -4.5283618 -4.5584064][-4.6331124 -4.6589866 -4.6440654 -4.5723329 -4.4451861 -4.2721977 -4.1001673 -3.9751065 -3.9539521 -4.0552578 -4.2318873 -4.4036112 -4.5056062 -4.5546732 -4.5798941][-4.6450496 -4.6902 -4.7130251 -4.691752 -4.6148095 -4.4786162 -4.3262463 -4.2086387 -4.1892676 -4.2794089 -4.4240355 -4.5445805 -4.5930691 -4.5980711 -4.5985379][-4.635942 -4.6925359 -4.7417359 -4.7597504 -4.7248583 -4.6228466 -4.4916854 -4.3925118 -4.3875861 -4.4697394 -4.5784063 -4.6472573 -4.6517043 -4.6259832 -4.6055703][-4.5851107 -4.6422124 -4.702673 -4.74828 -4.7509251 -4.6890736 -4.5913854 -4.5183983 -4.522697 -4.5871019 -4.6551166 -4.6825447 -4.662559 -4.6274228 -4.6007504][-4.5046372 -4.5540638 -4.6138735 -4.6722474 -4.7025642 -4.6824012 -4.6280155 -4.5863619 -4.5938272 -4.6344733 -4.6668768 -4.6697888 -4.6443491 -4.6108904 -4.5827956][-4.42303 -4.457149 -4.503623 -4.5557194 -4.5943079 -4.6015019 -4.5831671 -4.5661979 -4.5702515 -4.5873456 -4.5976958 -4.593801 -4.5745559 -4.5472965 -4.5211139]]...]
INFO - root - 2017-12-07 09:23:09.615680: step 4210, loss = 22.14, batch loss = 22.06 (8.7 examples/sec; 0.920 sec/batch; 83h:56m:22s remains)
INFO - root - 2017-12-07 09:23:19.124706: step 4220, loss = 21.70, batch loss = 21.62 (8.8 examples/sec; 0.913 sec/batch; 83h:16m:50s remains)
INFO - root - 2017-12-07 09:23:28.594576: step 4230, loss = 21.36, batch loss = 21.27 (8.8 examples/sec; 0.908 sec/batch; 82h:46m:58s remains)
INFO - root - 2017-12-07 09:23:37.924543: step 4240, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.912 sec/batch; 83h:09m:13s remains)
INFO - root - 2017-12-07 09:23:47.294940: step 4250, loss = 21.21, batch loss = 21.12 (9.2 examples/sec; 0.868 sec/batch; 79h:10m:21s remains)
INFO - root - 2017-12-07 09:23:56.850565: step 4260, loss = 21.71, batch loss = 21.63 (8.5 examples/sec; 0.943 sec/batch; 86h:00m:26s remains)
INFO - root - 2017-12-07 09:24:06.178673: step 4270, loss = 21.37, batch loss = 21.29 (8.5 examples/sec; 0.940 sec/batch; 85h:43m:13s remains)
INFO - root - 2017-12-07 09:24:15.589112: step 4280, loss = 20.89, batch loss = 20.80 (8.5 examples/sec; 0.942 sec/batch; 85h:54m:50s remains)
INFO - root - 2017-12-07 09:24:25.028165: step 4290, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.925 sec/batch; 84h:19m:19s remains)
INFO - root - 2017-12-07 09:24:34.398640: step 4300, loss = 21.41, batch loss = 21.32 (8.3 examples/sec; 0.962 sec/batch; 87h:41m:22s remains)
2017-12-07 09:24:35.329066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4336767 -4.4401903 -4.4390845 -4.4353685 -4.43137 -4.4256139 -4.4183717 -4.4138589 -4.4128351 -4.409996 -4.4031439 -4.3911085 -4.3743882 -4.3542185 -4.3351164][-4.5264988 -4.5439291 -4.5446844 -4.5338168 -4.5131683 -4.4859729 -4.4643922 -4.4649806 -4.4861269 -4.5079966 -4.5165081 -4.5052614 -4.4759083 -4.433928 -4.3893576][-4.5847263 -4.6065626 -4.6050835 -4.58173 -4.5313377 -4.459589 -4.404047 -4.4107962 -4.4772615 -4.5595832 -4.616334 -4.6258245 -4.5914831 -4.5298729 -4.4584942][-4.5887337 -4.6002645 -4.5837412 -4.5372858 -4.4426389 -4.3035922 -4.1875415 -4.1833286 -4.3004732 -4.4739075 -4.6155982 -4.6729674 -4.6533237 -4.5899796 -4.5076556][-4.5722113 -4.5617361 -4.5102043 -4.4200683 -4.2625093 -4.0348144 -3.8374672 -3.8020012 -3.9596453 -4.2351055 -4.4873075 -4.616785 -4.6322465 -4.58919 -4.5184097][-4.5607777 -4.5289755 -4.434011 -4.2847934 -4.0468068 -3.7184539 -3.434926 -3.3603225 -3.5490255 -3.9211712 -4.287756 -4.4963326 -4.5564132 -4.5458407 -4.4987721][-4.5675955 -4.5332146 -4.4193058 -4.2331891 -3.9314909 -3.5131912 -3.144568 -3.0203714 -3.2195277 -3.6586921 -4.1083593 -4.3794031 -4.4837284 -4.5098057 -4.4865093][-4.5975285 -4.5814719 -4.4840288 -4.3112106 -4.0083079 -3.56485 -3.1515198 -2.9798069 -3.1586568 -3.608777 -4.077858 -4.3682189 -4.4933386 -4.5385346 -4.5200844][-4.6367011 -4.6501894 -4.5911469 -4.4711747 -4.2402611 -3.8665102 -3.4791632 -3.2774117 -3.4008973 -3.7957115 -4.2183223 -4.4855056 -4.602252 -4.6366196 -4.5970745][-4.65572 -4.7108779 -4.7011042 -4.6449285 -4.5140915 -4.2680759 -3.9744639 -3.7833025 -3.83545 -4.1160927 -4.4382663 -4.6518474 -4.7396841 -4.7397304 -4.6606536][-4.6119246 -4.7042875 -4.7483182 -4.7541528 -4.7103243 -4.58691 -4.413012 -4.2773509 -4.2793889 -4.4331107 -4.6331916 -4.7731509 -4.8157635 -4.769968 -4.6546526][-4.5112362 -4.6075306 -4.6823974 -4.7315636 -4.7463565 -4.7090874 -4.6350241 -4.5681596 -4.5554557 -4.6132665 -4.7029343 -4.7651005 -4.7610497 -4.68527 -4.5636106][-4.4072266 -4.4710474 -4.5338459 -4.5895352 -4.6272154 -4.6327205 -4.6152477 -4.5941591 -4.58436 -4.5941148 -4.616282 -4.6254992 -4.5967417 -4.5247025 -4.4330688][-4.3412609 -4.3658037 -4.3941603 -4.4274788 -4.4596386 -4.4785094 -4.4847345 -4.4839549 -4.4788761 -4.4720616 -4.4650021 -4.4511614 -4.4213033 -4.3753362 -4.3282619][-4.3131881 -4.317049 -4.3229313 -4.3346357 -4.3494596 -4.3612814 -4.3692503 -4.3731637 -4.37148 -4.363451 -4.3514209 -4.3364825 -4.31859 -4.2998013 -4.2857394]]...]
INFO - root - 2017-12-07 09:24:44.810928: step 4310, loss = 21.84, batch loss = 21.75 (8.7 examples/sec; 0.915 sec/batch; 83h:23m:28s remains)
INFO - root - 2017-12-07 09:24:54.139792: step 4320, loss = 21.66, batch loss = 21.58 (8.8 examples/sec; 0.909 sec/batch; 82h:49m:23s remains)
INFO - root - 2017-12-07 09:25:03.451916: step 4330, loss = 21.30, batch loss = 21.22 (8.8 examples/sec; 0.910 sec/batch; 82h:56m:12s remains)
INFO - root - 2017-12-07 09:25:12.767466: step 4340, loss = 21.62, batch loss = 21.54 (8.8 examples/sec; 0.909 sec/batch; 82h:50m:07s remains)
INFO - root - 2017-12-07 09:25:22.204515: step 4350, loss = 21.35, batch loss = 21.27 (8.1 examples/sec; 0.984 sec/batch; 89h:41m:28s remains)
INFO - root - 2017-12-07 09:25:31.699676: step 4360, loss = 22.08, batch loss = 22.00 (8.3 examples/sec; 0.964 sec/batch; 87h:49m:31s remains)
INFO - root - 2017-12-07 09:25:41.098965: step 4370, loss = 21.21, batch loss = 21.13 (7.8 examples/sec; 1.021 sec/batch; 93h:00m:57s remains)
INFO - root - 2017-12-07 09:25:50.506683: step 4380, loss = 21.19, batch loss = 21.11 (8.5 examples/sec; 0.938 sec/batch; 85h:27m:36s remains)
INFO - root - 2017-12-07 09:25:59.902241: step 4390, loss = 21.52, batch loss = 21.44 (8.0 examples/sec; 1.001 sec/batch; 91h:14m:22s remains)
INFO - root - 2017-12-07 09:26:09.376072: step 4400, loss = 21.32, batch loss = 21.23 (7.8 examples/sec; 1.030 sec/batch; 93h:51m:44s remains)
2017-12-07 09:26:10.263939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4871373 -4.4867845 -4.51143 -4.540679 -4.5587015 -4.5414815 -4.526783 -4.5088878 -4.4522333 -4.3723369 -4.3503366 -4.4198012 -4.510303 -4.5577745 -4.5364647][-4.4414783 -4.4550939 -4.5004306 -4.555572 -4.5948014 -4.576405 -4.5377173 -4.4939632 -4.4430809 -4.394804 -4.3972807 -4.476316 -4.552999 -4.573369 -4.5368233][-4.4036679 -4.4350686 -4.4750876 -4.517025 -4.5481367 -4.538188 -4.5158653 -4.4839959 -4.4489589 -4.4204946 -4.4233303 -4.4793725 -4.5186939 -4.5070276 -4.467052][-4.3469081 -4.3854842 -4.4069414 -4.42592 -4.4470196 -4.4551497 -4.4607077 -4.4480171 -4.4331193 -4.4288363 -4.43125 -4.4475431 -4.4339995 -4.3885822 -4.3546124][-4.268918 -4.3081226 -4.316133 -4.3249497 -4.3364205 -4.3334212 -4.3171849 -4.290987 -4.2920656 -4.328424 -4.3586693 -4.3587604 -4.3187461 -4.2625012 -4.2452703][-4.2176423 -4.2480216 -4.2471786 -4.24635 -4.2270412 -4.1652479 -4.0772247 -4.0089731 -4.0246668 -4.1185513 -4.2046871 -4.2334723 -4.219676 -4.1820459 -4.176723][-4.2052836 -4.2110515 -4.1930017 -4.159935 -4.0797944 -3.93156 -3.7586081 -3.6444724 -3.6747053 -3.8356175 -3.9983871 -4.0947609 -4.1426334 -4.1446257 -4.1532469][-4.2708788 -4.2546043 -4.2046142 -4.1164045 -3.9634385 -3.7402945 -3.51033 -3.3690805 -3.4104571 -3.6221101 -3.8488216 -4.0128736 -4.1234026 -4.1761508 -4.2133117][-4.4393225 -4.421123 -4.3540912 -4.2308359 -4.0419693 -3.7998226 -3.572613 -3.4406421 -3.4857063 -3.695894 -3.9335067 -4.1291785 -4.2725053 -4.3591166 -4.4164853][-4.5896945 -4.5962996 -4.5442929 -4.4281039 -4.2550054 -4.0542088 -3.884068 -3.7927625 -3.8317215 -3.9882312 -4.1837931 -4.3671374 -4.5124674 -4.6094971 -4.667459][-4.6339154 -4.676837 -4.656673 -4.5663204 -4.4298067 -4.2938466 -4.2014079 -4.1604404 -4.1869049 -4.2757711 -4.40936 -4.5560188 -4.6797414 -4.7637477 -4.8072429][-4.6213932 -4.68813 -4.696856 -4.6401105 -4.5496416 -4.475955 -4.4426308 -4.4332767 -4.4441285 -4.4820971 -4.5608435 -4.6562042 -4.730721 -4.76644 -4.7744112][-4.5824003 -4.648159 -4.6730995 -4.6512146 -4.6097388 -4.5848794 -4.5820875 -4.5806708 -4.5747471 -4.5785532 -4.6108866 -4.6515985 -4.666285 -4.644002 -4.6169605][-4.506732 -4.5599413 -4.5950627 -4.6050539 -4.6024566 -4.603663 -4.6077156 -4.6033511 -4.5892372 -4.578527 -4.5810518 -4.5819478 -4.5560126 -4.5017362 -4.4633675][-4.3982215 -4.4360452 -4.4735537 -4.5018845 -4.5211926 -4.5338793 -4.5413408 -4.5411863 -4.5343728 -4.5285664 -4.5255151 -4.515656 -4.485826 -4.4420881 -4.4244823]]...]
INFO - root - 2017-12-07 09:26:19.631565: step 4410, loss = 21.48, batch loss = 21.40 (8.3 examples/sec; 0.965 sec/batch; 87h:55m:03s remains)
INFO - root - 2017-12-07 09:26:29.133475: step 4420, loss = 21.34, batch loss = 21.26 (8.6 examples/sec; 0.930 sec/batch; 84h:47m:55s remains)
INFO - root - 2017-12-07 09:26:38.445224: step 4430, loss = 21.51, batch loss = 21.43 (8.6 examples/sec; 0.934 sec/batch; 85h:06m:40s remains)
INFO - root - 2017-12-07 09:26:47.951641: step 4440, loss = 21.36, batch loss = 21.27 (8.5 examples/sec; 0.945 sec/batch; 86h:07m:01s remains)
INFO - root - 2017-12-07 09:26:57.230116: step 4450, loss = 21.67, batch loss = 21.59 (8.7 examples/sec; 0.918 sec/batch; 83h:36m:44s remains)
INFO - root - 2017-12-07 09:27:06.534050: step 4460, loss = 21.60, batch loss = 21.52 (8.5 examples/sec; 0.936 sec/batch; 85h:16m:33s remains)
INFO - root - 2017-12-07 09:27:15.867040: step 4470, loss = 21.51, batch loss = 21.43 (8.7 examples/sec; 0.924 sec/batch; 84h:11m:03s remains)
INFO - root - 2017-12-07 09:27:25.277983: step 4480, loss = 21.15, batch loss = 21.07 (8.6 examples/sec; 0.928 sec/batch; 84h:32m:07s remains)
INFO - root - 2017-12-07 09:27:34.673468: step 4490, loss = 21.47, batch loss = 21.39 (8.4 examples/sec; 0.958 sec/batch; 87h:15m:14s remains)
INFO - root - 2017-12-07 09:27:44.045706: step 4500, loss = 21.56, batch loss = 21.48 (9.2 examples/sec; 0.868 sec/batch; 79h:05m:25s remains)
2017-12-07 09:27:44.982196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7025332 -4.6868062 -4.6315894 -4.6162815 -4.6608953 -4.708231 -4.66355 -4.5673723 -4.5067639 -4.5339184 -4.634738 -4.7157917 -4.7527995 -4.7476783 -4.6662922][-4.7421374 -4.7176356 -4.6345186 -4.5991855 -4.6459432 -4.70892 -4.66522 -4.5428438 -4.4531116 -4.4802175 -4.6170168 -4.7291141 -4.7885628 -4.8018017 -4.721375][-4.736743 -4.7014747 -4.5941825 -4.5386434 -4.5849466 -4.6660919 -4.642025 -4.5152864 -4.3981338 -4.4075322 -4.5569553 -4.6914954 -4.7791233 -4.8164821 -4.7498913][-4.6590877 -4.6121268 -4.492404 -4.4189782 -4.4560885 -4.5520854 -4.5705347 -4.4743886 -4.3441572 -4.3191018 -4.4452028 -4.5859556 -4.70741 -4.7810736 -4.7469969][-4.526298 -4.4622965 -4.3377643 -4.2391839 -4.2432022 -4.3343372 -4.4008527 -4.3644972 -4.2560029 -4.2145243 -4.3110433 -4.4519334 -4.6044426 -4.7165203 -4.7226124][-4.4140916 -4.328445 -4.181076 -4.0304813 -3.9722385 -4.0307 -4.1206479 -4.1425014 -4.0901356 -4.0842957 -4.1894493 -4.3443151 -4.5207977 -4.65902 -4.6951609][-4.369019 -4.2751813 -4.0972962 -3.8829246 -3.7569089 -3.7661841 -3.8386543 -3.8804035 -3.8870144 -3.9582489 -4.1154552 -4.2983456 -4.4871693 -4.6317682 -4.677093][-4.3502927 -4.2902374 -4.1103015 -3.8550475 -3.6752925 -3.626225 -3.6418295 -3.6561377 -3.6988816 -3.8559365 -4.0889449 -4.3081689 -4.5032196 -4.6383862 -4.6730928][-4.3011718 -4.3196039 -4.1965032 -3.9605255 -3.761698 -3.6549981 -3.593451 -3.5490327 -3.5910928 -3.8007798 -4.0951061 -4.3513532 -4.5543551 -4.6733656 -4.68506][-4.2666178 -4.3548422 -4.3132753 -4.1442471 -3.9693363 -3.8284006 -3.704783 -3.6024954 -3.620878 -3.8370092 -4.1474004 -4.4171062 -4.6217222 -4.7238803 -4.7094517][-4.2910323 -4.4075937 -4.4277525 -4.3387623 -4.2175484 -4.074038 -3.9208307 -3.7846322 -3.7801106 -3.9742172 -4.2612367 -4.5121884 -4.6976342 -4.777216 -4.7366953][-4.3509374 -4.4576273 -4.5077262 -4.4886069 -4.4374065 -4.3304625 -4.1904984 -4.0499668 -4.0248742 -4.1748219 -4.4077377 -4.6123266 -4.760613 -4.8133144 -4.7509675][-4.4220076 -4.5068054 -4.5648632 -4.5931234 -4.5986714 -4.5453377 -4.4504938 -4.3353682 -4.2945514 -4.3849549 -4.5450373 -4.689661 -4.7898908 -4.8112473 -4.7319813][-4.5229368 -4.5823774 -4.6297078 -4.6716986 -4.694962 -4.6745429 -4.6218829 -4.5451169 -4.5020876 -4.5415244 -4.635468 -4.7229195 -4.7741466 -4.7602773 -4.6668887][-4.619369 -4.6438541 -4.6606903 -4.6832662 -4.6918254 -4.6752944 -4.642375 -4.5964594 -4.5628152 -4.5732374 -4.623867 -4.6731715 -4.6904178 -4.6526041 -4.5580831]]...]
INFO - root - 2017-12-07 09:27:54.359676: step 4510, loss = 21.35, batch loss = 21.26 (8.7 examples/sec; 0.925 sec/batch; 84h:15m:42s remains)
INFO - root - 2017-12-07 09:28:03.665819: step 4520, loss = 21.51, batch loss = 21.43 (8.3 examples/sec; 0.958 sec/batch; 87h:17m:27s remains)
INFO - root - 2017-12-07 09:28:13.048189: step 4530, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.967 sec/batch; 88h:05m:19s remains)
INFO - root - 2017-12-07 09:28:22.387463: step 4540, loss = 20.81, batch loss = 20.72 (8.5 examples/sec; 0.944 sec/batch; 86h:01m:24s remains)
INFO - root - 2017-12-07 09:28:31.720325: step 4550, loss = 21.00, batch loss = 20.92 (8.5 examples/sec; 0.937 sec/batch; 85h:18m:57s remains)
INFO - root - 2017-12-07 09:28:41.094355: step 4560, loss = 21.85, batch loss = 21.77 (7.9 examples/sec; 1.008 sec/batch; 91h:49m:23s remains)
INFO - root - 2017-12-07 09:28:50.411970: step 4570, loss = 21.17, batch loss = 21.09 (8.5 examples/sec; 0.937 sec/batch; 85h:23m:31s remains)
INFO - root - 2017-12-07 09:28:59.802010: step 4580, loss = 21.68, batch loss = 21.59 (8.4 examples/sec; 0.955 sec/batch; 87h:01m:04s remains)
INFO - root - 2017-12-07 09:29:09.030673: step 4590, loss = 21.78, batch loss = 21.69 (8.2 examples/sec; 0.981 sec/batch; 89h:19m:11s remains)
INFO - root - 2017-12-07 09:29:18.488602: step 4600, loss = 21.42, batch loss = 21.34 (8.7 examples/sec; 0.924 sec/batch; 84h:07m:56s remains)
2017-12-07 09:29:19.428927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4181933 -4.4339762 -4.4393 -4.4422913 -4.4442306 -4.4432573 -4.434957 -4.4206967 -4.4100552 -4.3943677 -4.3744783 -4.3798633 -4.4257679 -4.5037875 -4.5789356][-4.5212097 -4.5419726 -4.5469475 -4.5461464 -4.5407715 -4.5307431 -4.5139947 -4.4914322 -4.4757462 -4.4545732 -4.4334316 -4.44357 -4.5022254 -4.5978255 -4.6776223][-4.6368589 -4.6509113 -4.6455445 -4.6337624 -4.6143303 -4.5885825 -4.560565 -4.5321989 -4.5159678 -4.4999633 -4.491518 -4.5174303 -4.5937386 -4.703928 -4.7805772][-4.6876888 -4.6864023 -4.6602025 -4.6322975 -4.5989943 -4.5572619 -4.5168729 -4.4871788 -4.4830809 -4.4941111 -4.5213552 -4.5793753 -4.6780434 -4.7968388 -4.8653359][-4.6298103 -4.6051855 -4.5481849 -4.49868 -4.452816 -4.3950071 -4.3362122 -4.3040209 -4.3263345 -4.390574 -4.4824462 -4.594902 -4.7214966 -4.8426886 -4.9037309][-4.4513407 -4.4103251 -4.3306723 -4.2669721 -4.2134814 -4.1363773 -4.0437727 -3.992712 -4.0392685 -4.1665568 -4.3389697 -4.5176997 -4.6705132 -4.7868547 -4.8402367][-4.2524533 -4.2124162 -4.1265025 -4.0551066 -3.9877341 -3.8762379 -3.72925 -3.6363993 -3.6898599 -3.8701472 -4.1208448 -4.3678212 -4.5443139 -4.6505675 -4.6940751][-4.137424 -4.1080794 -4.0279746 -3.9547873 -3.8764784 -3.7377787 -3.541626 -3.3991039 -3.4323792 -3.6322961 -3.9337454 -4.2296195 -4.4268322 -4.5292368 -4.5666981][-4.133934 -4.1259685 -4.0632205 -3.9980674 -3.9250538 -3.7894955 -3.585748 -3.416424 -3.4127965 -3.5916758 -3.8951983 -4.2029281 -4.407443 -4.505693 -4.5363169][-4.2338305 -4.2464719 -4.2038655 -4.1503863 -4.0928755 -3.9894619 -3.8252227 -3.6704116 -3.6391454 -3.773807 -4.0369964 -4.317812 -4.510314 -4.5952697 -4.6068425][-4.3853307 -4.4018474 -4.3717976 -4.3310122 -4.2933741 -4.2343864 -4.1329956 -4.0254278 -3.995048 -4.0876021 -4.2830796 -4.5018253 -4.6593347 -4.7224565 -4.7112031][-4.5040727 -4.5121903 -4.4971476 -4.4804649 -4.4676995 -4.448226 -4.4026947 -4.3467474 -4.3335981 -4.3848763 -4.4890704 -4.6155596 -4.7233648 -4.7745209 -4.762095][-4.5172138 -4.5200653 -4.5246143 -4.5301843 -4.5298519 -4.5278192 -4.5129266 -4.4957161 -4.5034161 -4.5155292 -4.5230336 -4.5505495 -4.6140008 -4.6839066 -4.7135396][-4.4183168 -4.4259162 -4.4538741 -4.4743304 -4.4737992 -4.4694405 -4.4640636 -4.4679365 -4.4833083 -4.4553776 -4.3825288 -4.3350854 -4.3805933 -4.5028782 -4.6083312][-4.2760611 -4.2959857 -4.3453431 -4.3746281 -4.368381 -4.3511829 -4.3404832 -4.3470154 -4.3540268 -4.2897944 -4.1642227 -4.0766253 -4.1305141 -4.320498 -4.5102625]]...]
INFO - root - 2017-12-07 09:29:28.789859: step 4610, loss = 21.25, batch loss = 21.17 (8.7 examples/sec; 0.923 sec/batch; 84h:02m:34s remains)
INFO - root - 2017-12-07 09:29:38.131681: step 4620, loss = 21.60, batch loss = 21.51 (8.2 examples/sec; 0.970 sec/batch; 88h:20m:57s remains)
INFO - root - 2017-12-07 09:29:47.432337: step 4630, loss = 21.77, batch loss = 21.69 (8.0 examples/sec; 0.999 sec/batch; 90h:58m:06s remains)
INFO - root - 2017-12-07 09:29:56.840263: step 4640, loss = 21.68, batch loss = 21.60 (8.2 examples/sec; 0.972 sec/batch; 88h:32m:12s remains)
INFO - root - 2017-12-07 09:30:06.126332: step 4650, loss = 21.56, batch loss = 21.47 (8.4 examples/sec; 0.950 sec/batch; 86h:28m:50s remains)
INFO - root - 2017-12-07 09:30:15.534200: step 4660, loss = 21.65, batch loss = 21.57 (8.1 examples/sec; 0.989 sec/batch; 90h:03m:12s remains)
INFO - root - 2017-12-07 09:30:24.801937: step 4670, loss = 21.32, batch loss = 21.24 (8.3 examples/sec; 0.962 sec/batch; 87h:34m:29s remains)
INFO - root - 2017-12-07 09:30:34.163687: step 4680, loss = 21.32, batch loss = 21.23 (8.0 examples/sec; 0.996 sec/batch; 90h:41m:18s remains)
INFO - root - 2017-12-07 09:30:43.507102: step 4690, loss = 21.27, batch loss = 21.19 (8.6 examples/sec; 0.931 sec/batch; 84h:44m:41s remains)
INFO - root - 2017-12-07 09:30:52.736292: step 4700, loss = 21.81, batch loss = 21.73 (8.7 examples/sec; 0.920 sec/batch; 83h:48m:59s remains)
2017-12-07 09:30:53.666233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2907629 -4.3130803 -4.3760915 -4.4257975 -4.4180455 -4.3767061 -4.3509026 -4.3638244 -4.4084163 -4.4491997 -4.4481516 -4.4036303 -4.3598022 -4.3689656 -4.4023376][-4.3108006 -4.3364315 -4.40662 -4.4710145 -4.4849925 -4.4549642 -4.4247956 -4.42129 -4.451591 -4.4876313 -4.4904394 -4.453721 -4.4065495 -4.4034657 -4.4323916][-4.2434077 -4.2726669 -4.3505416 -4.4377503 -4.4939461 -4.4932795 -4.453536 -4.4118652 -4.4113169 -4.4388885 -4.452702 -4.4346433 -4.3913975 -4.3762507 -4.39734][-4.132154 -4.1808376 -4.278285 -4.3877306 -4.4773555 -4.4994521 -4.4528856 -4.3861256 -4.370677 -4.4026632 -4.4302325 -4.4255152 -4.3829808 -4.3549204 -4.3625522][-4.0979133 -4.1711884 -4.2787342 -4.3802333 -4.4460912 -4.4333282 -4.3578277 -4.2905693 -4.300169 -4.3644347 -4.4125915 -4.4174581 -4.3728738 -4.3370285 -4.3397846][-4.1562848 -4.2362804 -4.3307314 -4.3877239 -4.3769875 -4.27627 -4.1506653 -4.1047106 -4.1683316 -4.2793474 -4.34944 -4.3559756 -4.303401 -4.26255 -4.2746549][-4.2707272 -4.3288636 -4.3762975 -4.3658051 -4.2747455 -4.092926 -3.9244242 -3.8999755 -4.0063758 -4.1453404 -4.2224879 -4.22243 -4.1659684 -4.1276517 -4.1539378][-4.3883929 -4.4179816 -4.417717 -4.3492217 -4.2072086 -3.9866023 -3.8001363 -3.7763176 -3.8824682 -4.0074959 -4.0676317 -4.055099 -4.00641 -3.9866714 -4.0317941][-4.447865 -4.4804425 -4.4795585 -4.401248 -4.2573647 -4.0500131 -3.8706613 -3.82506 -3.8900115 -3.9723825 -4.007123 -3.9876652 -3.953599 -3.9511607 -4.0023837][-4.4899092 -4.541718 -4.5664067 -4.5113034 -4.3957887 -4.2277637 -4.0675197 -3.9988194 -4.0164661 -4.0561981 -4.0756063 -4.0601115 -4.0388131 -4.0381894 -4.078692][-4.5022364 -4.5686779 -4.6156268 -4.5918436 -4.5176954 -4.4002709 -4.26759 -4.1883593 -4.1775908 -4.1945972 -4.2125936 -4.2066936 -4.1885858 -4.1780128 -4.2036138][-4.4710155 -4.5438948 -4.6065726 -4.6121316 -4.5776639 -4.5094838 -4.407958 -4.3349056 -4.3206062 -4.3365278 -4.3647943 -4.3715091 -4.3514 -4.3261018 -4.3372412][-4.4505405 -4.5283575 -4.5956545 -4.6124058 -4.593833 -4.5560789 -4.4884934 -4.4413943 -4.4447489 -4.4668407 -4.4958873 -4.5071754 -4.4862103 -4.4522057 -4.4520707][-4.4480605 -4.52336 -4.5850048 -4.6031218 -4.5897908 -4.5704517 -4.5388064 -4.5273614 -4.5547528 -4.5844336 -4.6075325 -4.6159925 -4.5949106 -4.5551858 -4.5397577][-4.4330893 -4.4913535 -4.5415773 -4.5659561 -4.5678325 -4.5662785 -4.5625887 -4.5778036 -4.6193905 -4.652771 -4.6664252 -4.6664381 -4.6441183 -4.6034994 -4.5763407]]...]
INFO - root - 2017-12-07 09:31:03.033092: step 4710, loss = 21.27, batch loss = 21.19 (8.2 examples/sec; 0.975 sec/batch; 88h:48m:44s remains)
INFO - root - 2017-12-07 09:31:12.340486: step 4720, loss = 21.15, batch loss = 21.06 (8.1 examples/sec; 0.992 sec/batch; 90h:21m:46s remains)
INFO - root - 2017-12-07 09:31:21.713347: step 4730, loss = 21.30, batch loss = 21.21 (8.7 examples/sec; 0.915 sec/batch; 83h:20m:35s remains)
INFO - root - 2017-12-07 09:31:31.102170: step 4740, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.963 sec/batch; 87h:42m:51s remains)
INFO - root - 2017-12-07 09:31:40.418014: step 4750, loss = 21.17, batch loss = 21.09 (8.3 examples/sec; 0.961 sec/batch; 87h:30m:16s remains)
INFO - root - 2017-12-07 09:31:49.824839: step 4760, loss = 21.36, batch loss = 21.27 (8.9 examples/sec; 0.902 sec/batch; 82h:08m:08s remains)
INFO - root - 2017-12-07 09:31:59.122672: step 4770, loss = 21.51, batch loss = 21.42 (8.6 examples/sec; 0.928 sec/batch; 84h:27m:09s remains)
INFO - root - 2017-12-07 09:32:08.493865: step 4780, loss = 21.50, batch loss = 21.41 (8.5 examples/sec; 0.946 sec/batch; 86h:09m:32s remains)
INFO - root - 2017-12-07 09:32:17.784284: step 4790, loss = 21.20, batch loss = 21.12 (8.3 examples/sec; 0.961 sec/batch; 87h:28m:28s remains)
INFO - root - 2017-12-07 09:32:27.146933: step 4800, loss = 21.19, batch loss = 21.10 (8.8 examples/sec; 0.905 sec/batch; 82h:25m:14s remains)
2017-12-07 09:32:28.071227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6089239 -4.6120882 -4.5958791 -4.5687618 -4.5589023 -4.5811009 -4.6142459 -4.6298971 -4.6240635 -4.5989828 -4.5511546 -4.4835386 -4.4119339 -4.3578377 -4.3338966][-4.5340095 -4.5551848 -4.5604715 -4.5544209 -4.5638957 -4.602716 -4.6484265 -4.6746392 -4.6777611 -4.6628885 -4.6270266 -4.5658593 -4.4881439 -4.4205666 -4.3905263][-4.4180431 -4.4616895 -4.4983311 -4.5229144 -4.552319 -4.5951109 -4.6333351 -4.6453209 -4.6339841 -4.6196818 -4.610754 -4.5909419 -4.5460467 -4.4940414 -4.4685025][-4.311811 -4.38256 -4.4506674 -4.5025544 -4.5455241 -4.5822449 -4.6031551 -4.591887 -4.551734 -4.5191045 -4.5235839 -4.5483479 -4.5557284 -4.5446825 -4.54106][-4.2322483 -4.3228846 -4.4044652 -4.4588952 -4.4887543 -4.4978514 -4.4927368 -4.470015 -4.4260731 -4.3905344 -4.3981719 -4.4456253 -4.4933877 -4.5247464 -4.5507293][-4.2100663 -4.2995305 -4.3589997 -4.3746734 -4.3593564 -4.3203769 -4.2762594 -4.2490487 -4.2356114 -4.2348938 -4.2605262 -4.3174462 -4.3812718 -4.4345779 -4.4807725][-4.2629309 -4.320745 -4.3222675 -4.2717237 -4.2007313 -4.1179237 -4.0380645 -4.0047536 -4.0288472 -4.0813503 -4.1395006 -4.2019348 -4.2603307 -4.31144 -4.3623276][-4.3659482 -4.3802576 -4.3193913 -4.2015247 -4.0737667 -3.9465303 -3.8330483 -3.7882264 -3.8388386 -3.94198 -4.0406995 -4.1147213 -4.1656032 -4.2090759 -4.2603607][-4.4818306 -4.4671464 -4.3751674 -4.22297 -4.0518107 -3.8767529 -3.7255507 -3.6581507 -3.7094908 -3.8370514 -3.9628096 -4.0512 -4.1061721 -4.1569438 -4.2203913][-4.5656428 -4.5474377 -4.4642534 -4.3277426 -4.1576176 -3.9632111 -3.7867641 -3.6928062 -3.7185006 -3.8266959 -3.9400349 -4.0215669 -4.0772376 -4.1396556 -4.2170868][-4.6270881 -4.6282754 -4.5780435 -4.4819541 -4.3363447 -4.1476288 -3.9649265 -3.8542142 -3.8501587 -3.9196582 -3.997663 -4.055769 -4.1002703 -4.1589417 -4.2280874][-4.665864 -4.7056346 -4.6984625 -4.6384859 -4.5129113 -4.3383665 -4.1666822 -4.061039 -4.0469408 -4.0889468 -4.1350822 -4.1663947 -4.190413 -4.2277718 -4.2649174][-4.6535096 -4.7158294 -4.7421274 -4.7066936 -4.6030803 -4.461637 -4.3325305 -4.2595258 -4.2573848 -4.2926364 -4.3229971 -4.3303866 -4.3261085 -4.3288584 -4.3239255][-4.5806026 -4.6314387 -4.6657362 -4.6434422 -4.56485 -4.4710588 -4.4054437 -4.3853335 -4.4111977 -4.456635 -4.489882 -4.488749 -4.4646826 -4.44156 -4.4097676][-4.4681206 -4.4922638 -4.5184174 -4.5019722 -4.4507403 -4.4100809 -4.4088883 -4.4385314 -4.4850984 -4.535501 -4.5742016 -4.5782018 -4.5551372 -4.5329037 -4.5069857]]...]
INFO - root - 2017-12-07 09:32:37.477298: step 4810, loss = 21.15, batch loss = 21.07 (8.4 examples/sec; 0.952 sec/batch; 86h:37m:55s remains)
INFO - root - 2017-12-07 09:32:46.822768: step 4820, loss = 21.77, batch loss = 21.68 (8.3 examples/sec; 0.958 sec/batch; 87h:12m:51s remains)
INFO - root - 2017-12-07 09:32:56.138764: step 4830, loss = 21.42, batch loss = 21.34 (8.1 examples/sec; 0.991 sec/batch; 90h:09m:24s remains)
INFO - root - 2017-12-07 09:33:05.443136: step 4840, loss = 21.70, batch loss = 21.61 (8.5 examples/sec; 0.943 sec/batch; 85h:49m:29s remains)
INFO - root - 2017-12-07 09:33:14.956037: step 4850, loss = 21.38, batch loss = 21.29 (7.5 examples/sec; 1.063 sec/batch; 96h:43m:41s remains)
INFO - root - 2017-12-07 09:33:24.421406: step 4860, loss = 21.85, batch loss = 21.77 (8.0 examples/sec; 1.006 sec/batch; 91h:30m:46s remains)
INFO - root - 2017-12-07 09:33:33.849447: step 4870, loss = 21.56, batch loss = 21.47 (7.9 examples/sec; 1.011 sec/batch; 92h:01m:13s remains)
INFO - root - 2017-12-07 09:33:43.243401: step 4880, loss = 21.21, batch loss = 21.13 (8.4 examples/sec; 0.950 sec/batch; 86h:25m:15s remains)
INFO - root - 2017-12-07 09:33:52.612363: step 4890, loss = 21.78, batch loss = 21.70 (8.7 examples/sec; 0.916 sec/batch; 83h:21m:54s remains)
INFO - root - 2017-12-07 09:34:01.952691: step 4900, loss = 21.23, batch loss = 21.15 (8.2 examples/sec; 0.975 sec/batch; 88h:41m:56s remains)
2017-12-07 09:34:02.865829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1493082 -4.2218461 -4.2794275 -4.2715654 -4.1853137 -4.1330838 -4.133472 -4.1426978 -4.1439934 -4.159471 -4.234736 -4.364675 -4.4527035 -4.4138255 -4.344276][-4.1837473 -4.2819133 -4.3530512 -4.3515353 -4.2578411 -4.1729093 -4.137579 -4.1418848 -4.1648817 -4.1898994 -4.2495375 -4.3780317 -4.5076761 -4.5271025 -4.4823432][-4.1888037 -4.313899 -4.4025 -4.39341 -4.2690182 -4.1399856 -4.0805078 -4.1063614 -4.1796026 -4.2422934 -4.3151484 -4.4575047 -4.6150241 -4.6702452 -4.6392331][-4.111434 -4.2615843 -4.3787065 -4.368444 -4.2166662 -4.0512571 -3.9748056 -4.0192943 -4.13049 -4.2336836 -4.3355346 -4.49232 -4.6516628 -4.7099953 -4.683001][-3.9986012 -4.1600633 -4.3000617 -4.2941389 -4.1263447 -3.9333031 -3.8372049 -3.8835478 -4.0149074 -4.15471 -4.2936597 -4.4594269 -4.5972476 -4.6315322 -4.5927806][-3.9439416 -4.0955567 -4.2401366 -4.2368293 -4.0584478 -3.8366163 -3.7083941 -3.7428927 -3.8909559 -4.0703244 -4.248198 -4.4193506 -4.5241914 -4.5165949 -4.4437842][-3.9757152 -4.1095872 -4.2438064 -4.2366705 -4.0490971 -3.8011742 -3.6402194 -3.6568484 -3.8119617 -4.0195704 -4.2263522 -4.4018703 -4.4765186 -4.4235353 -4.3087664][-4.0754194 -4.1773114 -4.2756324 -4.2452049 -4.0486717 -3.7951939 -3.6293578 -3.6396084 -3.7916703 -4.0044794 -4.2203159 -4.3970704 -4.4539137 -4.371695 -4.2247953][-4.1556835 -4.2085423 -4.2493706 -4.1846189 -3.993293 -3.7755117 -3.6522005 -3.6798167 -3.8193183 -4.0082078 -4.2021575 -4.3709974 -4.4299641 -4.3505344 -4.2034116][-4.1928182 -4.1849685 -4.1581721 -4.06353 -3.9061029 -3.7703836 -3.7251365 -3.777616 -3.8964436 -4.0404072 -4.1887217 -4.3384919 -4.4113154 -4.3604903 -4.2423553][-4.2188587 -4.1476884 -4.057642 -3.9454582 -3.8417215 -3.8069975 -3.8398826 -3.9105577 -4.0120869 -4.1144037 -4.2143435 -4.3390656 -4.4221144 -4.4021316 -4.3211541][-4.2863426 -4.182229 -4.05124 -3.9290512 -3.8664198 -3.9023476 -3.9786022 -4.0568237 -4.1527638 -4.228507 -4.2903953 -4.3886542 -4.4675436 -4.4633017 -4.4154224][-4.4093084 -4.304245 -4.1556497 -4.0246511 -3.9746418 -4.0338778 -4.1213942 -4.2067642 -4.3099027 -4.3726497 -4.4044528 -4.4689875 -4.51918 -4.5063276 -4.4798751][-4.5112576 -4.4299979 -4.2976837 -4.17904 -4.1346631 -4.1876316 -4.2686071 -4.3583736 -4.46213 -4.5041504 -4.499754 -4.518693 -4.5236087 -4.4927444 -4.4812145][-4.5510993 -4.5140467 -4.4331584 -4.3557062 -4.3251162 -4.3604984 -4.4204059 -4.4968858 -4.5751796 -4.5812564 -4.5381594 -4.512485 -4.4795661 -4.438817 -4.4381123]]...]
INFO - root - 2017-12-07 09:34:12.176538: step 4910, loss = 21.70, batch loss = 21.62 (8.4 examples/sec; 0.948 sec/batch; 86h:13m:52s remains)
INFO - root - 2017-12-07 09:34:21.524994: step 4920, loss = 21.51, batch loss = 21.42 (8.8 examples/sec; 0.912 sec/batch; 82h:59m:26s remains)
INFO - root - 2017-12-07 09:34:30.877749: step 4930, loss = 21.74, batch loss = 21.66 (8.9 examples/sec; 0.899 sec/batch; 81h:45m:39s remains)
INFO - root - 2017-12-07 09:34:40.250014: step 4940, loss = 21.45, batch loss = 21.37 (8.3 examples/sec; 0.969 sec/batch; 88h:10m:05s remains)
INFO - root - 2017-12-07 09:34:49.685696: step 4950, loss = 21.80, batch loss = 21.71 (9.0 examples/sec; 0.888 sec/batch; 80h:46m:25s remains)
INFO - root - 2017-12-07 09:34:58.946244: step 4960, loss = 21.56, batch loss = 21.47 (9.3 examples/sec; 0.865 sec/batch; 78h:39m:41s remains)
INFO - root - 2017-12-07 09:35:08.180642: step 4970, loss = 21.61, batch loss = 21.52 (9.7 examples/sec; 0.827 sec/batch; 75h:12m:35s remains)
INFO - root - 2017-12-07 09:35:17.441389: step 4980, loss = 21.72, batch loss = 21.64 (8.7 examples/sec; 0.920 sec/batch; 83h:39m:25s remains)
INFO - root - 2017-12-07 09:35:26.854216: step 4990, loss = 21.14, batch loss = 21.06 (8.8 examples/sec; 0.912 sec/batch; 82h:59m:40s remains)
INFO - root - 2017-12-07 09:35:36.350154: step 5000, loss = 21.81, batch loss = 21.72 (8.9 examples/sec; 0.903 sec/batch; 82h:09m:41s remains)
2017-12-07 09:35:37.330872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5330524 -4.5273361 -4.4289665 -4.2722616 -4.1456285 -4.0877266 -4.0726271 -4.1119046 -4.2070327 -4.3094783 -4.3950958 -4.4310689 -4.4175782 -4.387496 -4.3560123][-4.5452681 -4.565227 -4.4927049 -4.3389978 -4.2002263 -4.1434364 -4.1332345 -4.1469321 -4.188118 -4.2400918 -4.3177948 -4.3835115 -4.405509 -4.4022856 -4.3793907][-4.5324063 -4.5715508 -4.5366678 -4.4027843 -4.2541847 -4.1980805 -4.2101941 -4.2338095 -4.2445283 -4.2460322 -4.2976284 -4.3682141 -4.4063039 -4.4224114 -4.4133449][-4.4401894 -4.5025992 -4.5182033 -4.4265614 -4.2827396 -4.2231297 -4.262743 -4.3347988 -4.3713322 -4.3595443 -4.3854275 -4.4394078 -4.4708652 -4.4864283 -4.4779534][-4.3165388 -4.4168086 -4.4831252 -4.43464 -4.3063259 -4.2344675 -4.2792864 -4.3966284 -4.4868937 -4.4994459 -4.5110593 -4.5407009 -4.5515547 -4.5497894 -4.5308418][-4.2660747 -4.4043975 -4.4928727 -4.4567008 -4.3224735 -4.2005811 -4.1894584 -4.3069706 -4.4459414 -4.5106626 -4.5411577 -4.5738287 -4.58956 -4.5861721 -4.5629239][-4.2689295 -4.4267426 -4.5048304 -4.4455547 -4.2690277 -4.0451694 -3.9105756 -3.9847138 -4.1757679 -4.3381071 -4.4530911 -4.5480084 -4.6077414 -4.6210413 -4.595871][-4.2886634 -4.4288635 -4.4661613 -4.3667836 -4.1443253 -3.8120418 -3.5369039 -3.5611694 -3.817245 -4.0973296 -4.3073044 -4.4509916 -4.533145 -4.5592909 -4.5477033][-4.3665357 -4.4540386 -4.4203959 -4.2744875 -4.03489 -3.6675904 -3.3366959 -3.3568168 -3.6655021 -3.9972463 -4.2106705 -4.3069563 -4.3456511 -4.3801007 -4.4145455][-4.5083809 -4.5358615 -4.4421916 -4.2689447 -4.0533686 -3.7424603 -3.4684451 -3.5240836 -3.8334103 -4.1202 -4.2404237 -4.2176719 -4.1742697 -4.2028418 -4.2689028][-4.6668515 -4.6609645 -4.5572572 -4.3943782 -4.2132263 -3.9627094 -3.7509191 -3.817843 -4.0829616 -4.28982 -4.3014207 -4.1715 -4.0649762 -4.0899734 -4.1707673][-4.7630458 -4.7584691 -4.691905 -4.5846982 -4.4620137 -4.276526 -4.1108875 -4.1498065 -4.3299484 -4.44497 -4.3689613 -4.1680579 -4.0271449 -4.0552893 -4.1567225][-4.7406564 -4.7665911 -4.756412 -4.7180781 -4.6621284 -4.5468731 -4.4285116 -4.4354439 -4.5364523 -4.5979161 -4.5082793 -4.3111539 -4.1665425 -4.174015 -4.2542925][-4.6465592 -4.7036386 -4.7380738 -4.74708 -4.73414 -4.6716652 -4.5931005 -4.5802617 -4.6284914 -4.6708179 -4.6266875 -4.5028877 -4.4008222 -4.3890243 -4.4209518][-4.5520458 -4.6257815 -4.6872549 -4.725605 -4.7414813 -4.7204027 -4.6802049 -4.6630816 -4.678915 -4.70376 -4.6957088 -4.644577 -4.5956631 -4.5789709 -4.5693207]]...]
INFO - root - 2017-12-07 09:35:46.720309: step 5010, loss = 21.04, batch loss = 20.96 (8.3 examples/sec; 0.960 sec/batch; 87h:17m:54s remains)
INFO - root - 2017-12-07 09:35:56.030919: step 5020, loss = 21.46, batch loss = 21.37 (8.7 examples/sec; 0.919 sec/batch; 83h:34m:44s remains)
INFO - root - 2017-12-07 09:36:05.494616: step 5030, loss = 21.46, batch loss = 21.38 (8.2 examples/sec; 0.975 sec/batch; 88h:39m:28s remains)
INFO - root - 2017-12-07 09:36:14.885394: step 5040, loss = 21.23, batch loss = 21.14 (8.6 examples/sec; 0.929 sec/batch; 84h:30m:36s remains)
INFO - root - 2017-12-07 09:36:24.267918: step 5050, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.950 sec/batch; 86h:21m:56s remains)
INFO - root - 2017-12-07 09:36:33.704957: step 5060, loss = 21.37, batch loss = 21.28 (8.4 examples/sec; 0.950 sec/batch; 86h:25m:15s remains)
INFO - root - 2017-12-07 09:36:42.941912: step 5070, loss = 22.05, batch loss = 21.97 (8.6 examples/sec; 0.928 sec/batch; 84h:26m:29s remains)
INFO - root - 2017-12-07 09:36:52.201435: step 5080, loss = 21.60, batch loss = 21.52 (8.6 examples/sec; 0.933 sec/batch; 84h:51m:00s remains)
INFO - root - 2017-12-07 09:37:01.624234: step 5090, loss = 21.56, batch loss = 21.48 (8.6 examples/sec; 0.928 sec/batch; 84h:25m:12s remains)
INFO - root - 2017-12-07 09:37:11.154907: step 5100, loss = 21.41, batch loss = 21.32 (8.9 examples/sec; 0.897 sec/batch; 81h:32m:03s remains)
2017-12-07 09:37:12.095389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6233344 -4.6623569 -4.6610656 -4.6193471 -4.5675521 -4.5186443 -4.4906044 -4.4730983 -4.4535022 -4.4478593 -4.463851 -4.4769778 -4.4803758 -4.4961743 -4.5112238][-4.6530466 -4.6869946 -4.6856275 -4.6454363 -4.5805836 -4.5095682 -4.4611931 -4.4301872 -4.4097295 -4.413394 -4.4362907 -4.4589806 -4.4743414 -4.4964647 -4.5039239][-4.6001806 -4.6158113 -4.619916 -4.6067333 -4.555964 -4.4717274 -4.3930011 -4.3320732 -4.3095827 -4.3352909 -4.3775282 -4.4066539 -4.4262662 -4.4531021 -4.4621544][-4.5232959 -4.5170612 -4.5321474 -4.550149 -4.5011792 -4.3834696 -4.2617764 -4.1757684 -4.1682987 -4.2328067 -4.3034682 -4.3369632 -4.3507671 -4.3698492 -4.3745914][-4.4508171 -4.4318805 -4.4543424 -4.4723282 -4.3907328 -4.2260294 -4.0774155 -3.9967198 -4.0245647 -4.1387024 -4.2414823 -4.2843223 -4.2882013 -4.2870278 -4.2781682][-4.3932447 -4.3889923 -4.4083138 -4.3898091 -4.2608943 -4.0729809 -3.9288888 -3.8691344 -3.9321673 -4.0940065 -4.2350173 -4.2981062 -4.295527 -4.2674384 -4.2369976][-4.3701811 -4.4034848 -4.4086289 -4.3419223 -4.1868982 -4.0062838 -3.8691247 -3.8085916 -3.8873928 -4.0882058 -4.275 -4.36919 -4.3650188 -4.3101697 -4.2585287][-4.3668318 -4.4402375 -4.438026 -4.3500547 -4.1939416 -4.014133 -3.8587081 -3.7801766 -3.868629 -4.09789 -4.31882 -4.4346313 -4.4276876 -4.351068 -4.2845383][-4.34265 -4.4327922 -4.4398189 -4.3730359 -4.2342606 -4.0455065 -3.8742952 -3.8035724 -3.9190404 -4.1580014 -4.3743887 -4.4823604 -4.4666605 -4.3803873 -4.3103862][-4.29894 -4.3606038 -4.3720832 -4.3456697 -4.2453785 -4.0780678 -3.9415731 -3.9277673 -4.0752978 -4.291153 -4.4614806 -4.5344119 -4.5094104 -4.4308147 -4.3685374][-4.2655215 -4.2565665 -4.246882 -4.2521982 -4.2126889 -4.12059 -4.0727568 -4.1350589 -4.2920046 -4.4540854 -4.5557718 -4.5857654 -4.5564079 -4.4970355 -4.4511089][-4.2742367 -4.198236 -4.15751 -4.1731458 -4.1853943 -4.1756511 -4.2100253 -4.3182659 -4.455811 -4.5563793 -4.5983262 -4.5992479 -4.5747409 -4.539361 -4.5133419][-4.2971144 -4.201077 -4.1506577 -4.1643863 -4.2017446 -4.2355685 -4.2958651 -4.3974171 -4.49783 -4.5512886 -4.5592241 -4.5503592 -4.5377212 -4.5241456 -4.5154781][-4.292748 -4.2051845 -4.1606884 -4.1735978 -4.2234111 -4.2761064 -4.3300991 -4.3981285 -4.4562111 -4.4779663 -4.4730439 -4.4670219 -4.4659514 -4.4676018 -4.4718723][-4.2339568 -4.1574011 -4.1178656 -4.1312041 -4.1919303 -4.2618923 -4.321681 -4.374486 -4.4052448 -4.4055934 -4.3972816 -4.3979154 -4.4043365 -4.4128504 -4.422245]]...]
INFO - root - 2017-12-07 09:37:21.567773: step 5110, loss = 21.43, batch loss = 21.35 (7.9 examples/sec; 1.007 sec/batch; 91h:35m:26s remains)
INFO - root - 2017-12-07 09:37:30.977106: step 5120, loss = 21.37, batch loss = 21.29 (8.2 examples/sec; 0.981 sec/batch; 89h:14m:26s remains)
INFO - root - 2017-12-07 09:37:40.218413: step 5130, loss = 21.41, batch loss = 21.32 (8.0 examples/sec; 1.004 sec/batch; 91h:20m:34s remains)
INFO - root - 2017-12-07 09:37:49.654803: step 5140, loss = 21.31, batch loss = 21.23 (8.3 examples/sec; 0.969 sec/batch; 88h:06m:12s remains)
INFO - root - 2017-12-07 09:37:59.053947: step 5150, loss = 21.47, batch loss = 21.39 (8.1 examples/sec; 0.993 sec/batch; 90h:16m:06s remains)
INFO - root - 2017-12-07 09:38:08.696060: step 5160, loss = 21.92, batch loss = 21.84 (8.3 examples/sec; 0.967 sec/batch; 87h:53m:34s remains)
INFO - root - 2017-12-07 09:38:18.137154: step 5170, loss = 21.63, batch loss = 21.55 (8.0 examples/sec; 0.997 sec/batch; 90h:39m:25s remains)
INFO - root - 2017-12-07 09:38:27.293772: step 5180, loss = 21.33, batch loss = 21.24 (9.7 examples/sec; 0.824 sec/batch; 74h:54m:11s remains)
INFO - root - 2017-12-07 09:38:36.558869: step 5190, loss = 21.34, batch loss = 21.25 (8.5 examples/sec; 0.940 sec/batch; 85h:28m:05s remains)
INFO - root - 2017-12-07 09:38:45.922796: step 5200, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.936 sec/batch; 85h:08m:09s remains)
2017-12-07 09:38:46.869177: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4874806 -4.481214 -4.465539 -4.4427929 -4.4190588 -4.3991604 -4.3850965 -4.3741541 -4.3605809 -4.34232 -4.3216763 -4.30275 -4.2883496 -4.2773633 -4.2679281][-4.5377178 -4.5536389 -4.550499 -4.5262556 -4.4933705 -4.4649 -4.4467635 -4.4369087 -4.4249067 -4.4028144 -4.3716407 -4.3377743 -4.3087907 -4.287416 -4.2722116][-4.4889007 -4.5407915 -4.5655861 -4.547368 -4.5053673 -4.4652214 -4.4463367 -4.450573 -4.4605632 -4.4573846 -4.4332314 -4.3928037 -4.3490329 -4.3125997 -4.2868338][-4.335279 -4.4261136 -4.4855762 -4.4784403 -4.4247651 -4.3615756 -4.3350153 -4.3582177 -4.4091139 -4.455442 -4.4692822 -4.4435792 -4.3937554 -4.3423047 -4.3040996][-4.1491165 -4.2723832 -4.3580737 -4.3549113 -4.2799435 -4.1783056 -4.129591 -4.1635137 -4.2589197 -4.3717566 -4.4492736 -4.4633789 -4.4239621 -4.3642969 -4.316586][-4.0017343 -4.1526141 -4.2475033 -4.2312121 -4.1195216 -3.972815 -3.8995008 -3.9426599 -4.07661 -4.24708 -4.3846269 -4.4460382 -4.4296083 -4.3714962 -4.3200827][-3.9390416 -4.1071615 -4.1949267 -4.1488547 -3.9885266 -3.7972918 -3.7090125 -3.7707942 -3.9414 -4.1505523 -4.3244462 -4.4201288 -4.4254742 -4.374238 -4.3222466][-3.978435 -4.141212 -4.216002 -4.1483188 -3.956023 -3.7357526 -3.6335588 -3.707021 -3.9008012 -4.1262712 -4.3131366 -4.4248662 -4.4437151 -4.3969855 -4.3400459][-4.1125712 -4.2458282 -4.3070087 -4.2414041 -4.061533 -3.8509343 -3.7391558 -3.7910862 -3.9624236 -4.1663995 -4.3441944 -4.4607496 -4.4889054 -4.4474483 -4.3830972][-4.2874064 -4.3825026 -4.4276438 -4.3786106 -4.2468209 -4.0865307 -3.9859366 -3.9959226 -4.0929642 -4.22645 -4.3657546 -4.4762969 -4.5222878 -4.5021062 -4.4426637][-4.4132123 -4.4796128 -4.507369 -4.474052 -4.3983469 -4.3025336 -4.2311449 -4.2065139 -4.2163486 -4.2566962 -4.3346925 -4.4278932 -4.4996443 -4.5222654 -4.4894118][-4.4421372 -4.4849505 -4.4925137 -4.4704962 -4.4372015 -4.3878231 -4.3390584 -4.2902265 -4.2295322 -4.1971641 -4.2247353 -4.3042316 -4.4057217 -4.4809618 -4.49369][-4.4024758 -4.4174027 -4.4015718 -4.384428 -4.3712645 -4.3348351 -4.28123 -4.2019429 -4.0949917 -4.0289474 -4.0459757 -4.1364336 -4.2700944 -4.3917861 -4.4513507][-4.3493123 -4.3384089 -4.3049436 -4.2880936 -4.2777891 -4.2319942 -4.1520138 -4.0361042 -3.8994923 -3.8258893 -3.8601971 -3.9820793 -4.1477494 -4.30022 -4.3924327][-4.3032379 -4.2763567 -4.2407589 -4.2291112 -4.2263684 -4.1828489 -4.0903907 -3.9542644 -3.7996981 -3.71126 -3.750561 -3.8966286 -4.0865569 -4.2572837 -4.3664174]]...]
INFO - root - 2017-12-07 09:38:56.223709: step 5210, loss = 21.34, batch loss = 21.26 (8.9 examples/sec; 0.896 sec/batch; 81h:26m:55s remains)
INFO - root - 2017-12-07 09:39:05.524140: step 5220, loss = 21.59, batch loss = 21.51 (9.2 examples/sec; 0.869 sec/batch; 79h:02m:49s remains)
INFO - root - 2017-12-07 09:39:14.957399: step 5230, loss = 21.46, batch loss = 21.38 (8.7 examples/sec; 0.922 sec/batch; 83h:50m:01s remains)
INFO - root - 2017-12-07 09:39:24.456054: step 5240, loss = 21.50, batch loss = 21.41 (8.6 examples/sec; 0.932 sec/batch; 84h:43m:33s remains)
INFO - root - 2017-12-07 09:39:33.892319: step 5250, loss = 21.27, batch loss = 21.18 (8.6 examples/sec; 0.930 sec/batch; 84h:34m:57s remains)
INFO - root - 2017-12-07 09:39:43.290464: step 5260, loss = 21.69, batch loss = 21.61 (8.4 examples/sec; 0.952 sec/batch; 86h:32m:48s remains)
INFO - root - 2017-12-07 09:39:52.733999: step 5270, loss = 21.52, batch loss = 21.44 (8.3 examples/sec; 0.969 sec/batch; 88h:04m:52s remains)
INFO - root - 2017-12-07 09:40:02.258987: step 5280, loss = 21.71, batch loss = 21.63 (8.3 examples/sec; 0.958 sec/batch; 87h:06m:16s remains)
INFO - root - 2017-12-07 09:40:11.750720: step 5290, loss = 21.32, batch loss = 21.24 (8.2 examples/sec; 0.973 sec/batch; 88h:23m:40s remains)
INFO - root - 2017-12-07 09:40:21.107886: step 5300, loss = 21.32, batch loss = 21.24 (7.9 examples/sec; 1.009 sec/batch; 91h:40m:29s remains)
2017-12-07 09:40:22.088218: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.507606 -4.5618434 -4.6065307 -4.6300287 -4.6260471 -4.5969005 -4.5569682 -4.5201535 -4.4891863 -4.4707556 -4.4674907 -4.4728136 -4.471355 -4.4535494 -4.419724][-4.6631556 -4.7387924 -4.7822809 -4.7884054 -4.7624941 -4.7144947 -4.6641068 -4.6250558 -4.5929055 -4.5787406 -4.5887222 -4.6088939 -4.6124139 -4.583353 -4.5229173][-4.8150578 -4.8943243 -4.9016938 -4.8493996 -4.7734766 -4.7029753 -4.6560206 -4.6351376 -4.6204472 -4.6251383 -4.6616192 -4.7105832 -4.73234 -4.70268 -4.6221757][-4.9187074 -4.9827018 -4.9255376 -4.7811923 -4.6289225 -4.5307684 -4.5007224 -4.519856 -4.5467811 -4.58971 -4.6680126 -4.7541375 -4.7971759 -4.7707191 -4.678021][-4.9382644 -4.9738946 -4.84827 -4.609766 -4.3738422 -4.2467508 -4.2401152 -4.3144746 -4.4051018 -4.503562 -4.6375074 -4.7627921 -4.8192005 -4.7889709 -4.68397][-4.8797908 -4.88282 -4.7030578 -4.3912411 -4.07744 -3.9062862 -3.9057376 -4.0350642 -4.2091804 -4.3882003 -4.5955973 -4.7631187 -4.8246574 -4.7817459 -4.6607919][-4.795763 -4.7824349 -4.5840988 -4.235105 -3.8548717 -3.6174619 -3.5843527 -3.7400372 -3.9885755 -4.25379 -4.5408354 -4.7535181 -4.8204689 -4.7658558 -4.6291437][-4.7389712 -4.751585 -4.589201 -4.2518406 -3.8358383 -3.5242379 -3.4184704 -3.5480359 -3.8211012 -4.1379056 -4.4731717 -4.7104011 -4.7815127 -4.7243204 -4.5870419][-4.7228956 -4.7970681 -4.7094436 -4.4288192 -4.0225344 -3.6618314 -3.4777346 -3.5462606 -3.7949922 -4.1145639 -4.4448485 -4.6692138 -4.73176 -4.6742358 -4.5456314][-4.7042441 -4.8381119 -4.8322186 -4.6337762 -4.2862458 -3.9371853 -3.7198949 -3.7338774 -3.9331496 -4.2157154 -4.5014853 -4.6862288 -4.7274051 -4.6642985 -4.5396404][-4.6429119 -4.798636 -4.8464937 -4.7311797 -4.48052 -4.2089739 -4.0266447 -4.021482 -4.1699052 -4.395381 -4.6220946 -4.7618718 -4.7781329 -4.7031474 -4.5744119][-4.5626826 -4.6939235 -4.7513566 -4.6980424 -4.55346 -4.3916903 -4.2841969 -4.2883554 -4.3955927 -4.5618396 -4.7284336 -4.8261595 -4.8225985 -4.7402406 -4.6110106][-4.4917264 -4.5748324 -4.6137347 -4.5912857 -4.5251708 -4.4575815 -4.4212379 -4.4415879 -4.5167189 -4.6285591 -4.7406445 -4.8040738 -4.7926369 -4.7162623 -4.6000037][-4.4320531 -4.4708333 -4.485157 -4.4741116 -4.4508138 -4.4332561 -4.4298139 -4.447381 -4.4892697 -4.5542045 -4.6237826 -4.6658487 -4.6598196 -4.6067109 -4.5229545][-4.3766708 -4.3883829 -4.3888559 -4.3808951 -4.3713145 -4.3653631 -4.3631587 -4.3663454 -4.3809361 -4.4106183 -4.4482989 -4.4766054 -4.481205 -4.4583693 -4.4163523]]...]
INFO - root - 2017-12-07 09:40:31.541209: step 5310, loss = 22.19, batch loss = 22.11 (8.1 examples/sec; 0.988 sec/batch; 89h:47m:04s remains)
INFO - root - 2017-12-07 09:40:41.015507: step 5320, loss = 20.84, batch loss = 20.75 (8.4 examples/sec; 0.951 sec/batch; 86h:27m:29s remains)
INFO - root - 2017-12-07 09:40:50.571014: step 5330, loss = 21.46, batch loss = 21.37 (8.9 examples/sec; 0.901 sec/batch; 81h:53m:47s remains)
INFO - root - 2017-12-07 09:40:59.978441: step 5340, loss = 21.24, batch loss = 21.15 (8.4 examples/sec; 0.947 sec/batch; 86h:05m:13s remains)
INFO - root - 2017-12-07 09:41:09.374041: step 5350, loss = 21.52, batch loss = 21.44 (8.8 examples/sec; 0.908 sec/batch; 82h:29m:52s remains)
INFO - root - 2017-12-07 09:41:18.884313: step 5360, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.909 sec/batch; 82h:35m:50s remains)
INFO - root - 2017-12-07 09:41:28.435487: step 5370, loss = 21.58, batch loss = 21.50 (8.4 examples/sec; 0.954 sec/batch; 86h:41m:42s remains)
INFO - root - 2017-12-07 09:41:37.813085: step 5380, loss = 21.42, batch loss = 21.34 (8.6 examples/sec; 0.930 sec/batch; 84h:29m:06s remains)
INFO - root - 2017-12-07 09:41:47.195047: step 5390, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.976 sec/batch; 88h:39m:33s remains)
INFO - root - 2017-12-07 09:41:56.667138: step 5400, loss = 21.56, batch loss = 21.47 (8.0 examples/sec; 1.004 sec/batch; 91h:15m:40s remains)
2017-12-07 09:41:57.592013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3758974 -4.4657731 -4.5499973 -4.6153169 -4.6441331 -4.630559 -4.6142931 -4.6029763 -4.5871682 -4.5749984 -4.5826883 -4.6292572 -4.6899076 -4.7270107 -4.7144709][-4.436379 -4.5543308 -4.6588335 -4.740037 -4.7735195 -4.7385726 -4.6980543 -4.6754584 -4.659543 -4.6573858 -4.6782689 -4.7434263 -4.8173351 -4.8534036 -4.8282566][-4.4944324 -4.6230173 -4.7218056 -4.7861738 -4.79482 -4.7166095 -4.6337185 -4.5974274 -4.6044273 -4.64236 -4.6916604 -4.7714109 -4.8481178 -4.8834023 -4.8630071][-4.51648 -4.6298003 -4.69939 -4.7189832 -4.6769118 -4.54257 -4.4081922 -4.3584938 -4.4010406 -4.4969845 -4.5876303 -4.68087 -4.759079 -4.801919 -4.8044186][-4.4798579 -4.562459 -4.5997009 -4.5812197 -4.4953241 -4.3134913 -4.1262732 -4.0451632 -4.0984383 -4.2371435 -4.371603 -4.4897881 -4.5869875 -4.6573734 -4.6991043][-4.4213758 -4.4735503 -4.4889059 -4.4505286 -4.3353143 -4.1180878 -3.8798823 -3.7402747 -3.761739 -3.9164758 -4.1020951 -4.2731242 -4.4225416 -4.5442672 -4.6343241][-4.3788633 -4.4124556 -4.4150648 -4.3654323 -4.2331805 -3.9959648 -3.7214637 -3.5243106 -3.496208 -3.6491044 -3.8829827 -4.1167264 -4.3258376 -4.4982715 -4.6254325][-4.361876 -4.4182744 -4.4442544 -4.4144092 -4.3021712 -4.0864406 -3.8285625 -3.6204057 -3.549068 -3.6669872 -3.8988049 -4.1459441 -4.3670835 -4.5414281 -4.656589][-4.3905911 -4.5031061 -4.5820489 -4.5988593 -4.537621 -4.3752861 -4.1704321 -3.9842112 -3.8726697 -3.9155321 -4.0895052 -4.3084469 -4.5077453 -4.6502447 -4.716383][-4.4296532 -4.5817251 -4.6919556 -4.7344327 -4.7089982 -4.5889835 -4.4307952 -4.2721925 -4.1320243 -4.1000896 -4.2120562 -4.4142437 -4.615984 -4.7498317 -4.7823143][-4.4562879 -4.6213951 -4.7288189 -4.7665405 -4.7498469 -4.6516275 -4.5314379 -4.4170089 -4.2925291 -4.2266517 -4.2889829 -4.465754 -4.6654196 -4.8030853 -4.8325424][-4.4825463 -4.6384215 -4.7273755 -4.7472925 -4.7229314 -4.6410422 -4.56184 -4.5116143 -4.4452047 -4.3892779 -4.4143505 -4.5403376 -4.7031741 -4.8218923 -4.8482313][-4.4734015 -4.6003742 -4.6722717 -4.6863694 -4.666481 -4.612226 -4.574151 -4.5776381 -4.5678029 -4.5371814 -4.538538 -4.6082578 -4.7171183 -4.8029051 -4.8174906][-4.4470263 -4.5400891 -4.5998015 -4.6231332 -4.62213 -4.5985317 -4.5862479 -4.6064205 -4.6173754 -4.6024151 -4.5945063 -4.6271715 -4.6920452 -4.7477317 -4.7486596][-4.4197931 -4.473794 -4.5117292 -4.5332913 -4.54028 -4.5349083 -4.5322371 -4.5455728 -4.5538549 -4.5477428 -4.5423837 -4.5572219 -4.593709 -4.6291943 -4.6292291]]...]
INFO - root - 2017-12-07 09:42:06.958220: step 5410, loss = 21.52, batch loss = 21.43 (8.5 examples/sec; 0.942 sec/batch; 85h:33m:51s remains)
INFO - root - 2017-12-07 09:42:16.370095: step 5420, loss = 21.28, batch loss = 21.20 (8.7 examples/sec; 0.922 sec/batch; 83h:45m:01s remains)
INFO - root - 2017-12-07 09:42:25.776160: step 5430, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.930 sec/batch; 84h:28m:30s remains)
INFO - root - 2017-12-07 09:42:35.313926: step 5440, loss = 21.55, batch loss = 21.47 (8.6 examples/sec; 0.934 sec/batch; 84h:53m:53s remains)
INFO - root - 2017-12-07 09:42:44.773046: step 5450, loss = 21.72, batch loss = 21.64 (8.4 examples/sec; 0.957 sec/batch; 86h:56m:50s remains)
INFO - root - 2017-12-07 09:42:54.107962: step 5460, loss = 21.53, batch loss = 21.45 (8.4 examples/sec; 0.957 sec/batch; 86h:56m:01s remains)
INFO - root - 2017-12-07 09:43:03.385105: step 5470, loss = 21.54, batch loss = 21.46 (8.6 examples/sec; 0.931 sec/batch; 84h:34m:58s remains)
INFO - root - 2017-12-07 09:43:12.709205: step 5480, loss = 21.55, batch loss = 21.46 (8.2 examples/sec; 0.976 sec/batch; 88h:38m:56s remains)
INFO - root - 2017-12-07 09:43:22.122420: step 5490, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.953 sec/batch; 86h:34m:08s remains)
INFO - root - 2017-12-07 09:43:31.316073: step 5500, loss = 21.60, batch loss = 21.51 (8.7 examples/sec; 0.923 sec/batch; 83h:51m:31s remains)
2017-12-07 09:43:32.262129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.611176 -4.6353269 -4.5360556 -4.457427 -4.4360642 -4.4188805 -4.4442148 -4.4593787 -4.4429817 -4.4296579 -4.3889146 -4.3424935 -4.3163676 -4.2855592 -4.2819591][-4.6413107 -4.6530485 -4.5458322 -4.476511 -4.4695811 -4.455483 -4.4796448 -4.496943 -4.4806962 -4.461421 -4.4125261 -4.3599238 -4.3316875 -4.3078313 -4.3221812][-4.638886 -4.6405115 -4.5476322 -4.498239 -4.5078998 -4.505177 -4.5333972 -4.5529833 -4.538197 -4.516799 -4.4650388 -4.4105163 -4.3816638 -4.3649158 -4.3876853][-4.6091681 -4.6056862 -4.5336308 -4.4915838 -4.4925227 -4.4855113 -4.504355 -4.5141025 -4.5027175 -4.5017486 -4.4800253 -4.4447322 -4.423471 -4.4215484 -4.4503465][-4.54986 -4.5434442 -4.4826417 -4.4229527 -4.3825488 -4.3394351 -4.3226433 -4.3163185 -4.3216558 -4.3716154 -4.4151106 -4.4170985 -4.4075003 -4.4182 -4.4489403][-4.4643478 -4.4536152 -4.3897519 -4.2991443 -4.2019148 -4.0840111 -3.9937196 -3.970048 -4.0213871 -4.1549459 -4.2887506 -4.3424916 -4.3543 -4.3792791 -4.4072413][-4.3829513 -4.3649392 -4.2995572 -4.1876764 -4.0410247 -3.823704 -3.6180429 -3.5692806 -3.6898184 -3.9198897 -4.1379533 -4.2403727 -4.2926846 -4.3532367 -4.3919411][-4.3249245 -4.3126497 -4.2518845 -4.133482 -3.9681404 -3.6858525 -3.3836226 -3.3166165 -3.5096178 -3.8182652 -4.0773044 -4.1975856 -4.280416 -4.3674464 -4.412611][-4.2743764 -4.2923746 -4.260972 -4.1646676 -4.0256085 -3.7648702 -3.4566197 -3.3840673 -3.608954 -3.9411886 -4.1863394 -4.2777033 -4.3536639 -4.4316173 -4.4580483][-4.2365131 -4.2908711 -4.3115139 -4.26324 -4.170743 -3.980541 -3.7354608 -3.6597574 -3.8528383 -4.1483293 -4.3327594 -4.3602972 -4.39641 -4.447392 -4.4589477][-4.2495761 -4.32916 -4.3933845 -4.3934 -4.3519979 -4.2466464 -4.0866523 -4.0083332 -4.1292143 -4.3444924 -4.4411364 -4.3819361 -4.3608341 -4.3915014 -4.4111233][-4.2879934 -4.3833728 -4.4761162 -4.5193634 -4.5346613 -4.5081191 -4.4253116 -4.3438969 -4.3761597 -4.4887233 -4.4864578 -4.3464456 -4.277072 -4.3123679 -4.3730316][-4.3367095 -4.4359484 -4.5293941 -4.5876055 -4.6308718 -4.648519 -4.6065974 -4.5118251 -4.46678 -4.4910903 -4.426919 -4.2614036 -4.1860771 -4.2524462 -4.3668342][-4.4038219 -4.4833364 -4.5469255 -4.5831695 -4.6200156 -4.647583 -4.6187172 -4.5179839 -4.4295664 -4.4041018 -4.336556 -4.2169251 -4.183434 -4.2812204 -4.417932][-4.471034 -4.5156169 -4.5409956 -4.5491309 -4.5683374 -4.5855818 -4.5572152 -4.4666653 -4.3753328 -4.34019 -4.3057952 -4.2648592 -4.2932816 -4.4077668 -4.5273838]]...]
INFO - root - 2017-12-07 09:43:41.625687: step 5510, loss = 21.75, batch loss = 21.67 (8.6 examples/sec; 0.932 sec/batch; 84h:40m:01s remains)
INFO - root - 2017-12-07 09:43:50.918732: step 5520, loss = 21.60, batch loss = 21.52 (8.3 examples/sec; 0.960 sec/batch; 87h:12m:28s remains)
INFO - root - 2017-12-07 09:44:00.272471: step 5530, loss = 21.52, batch loss = 21.44 (8.3 examples/sec; 0.962 sec/batch; 87h:22m:41s remains)
INFO - root - 2017-12-07 09:44:09.575125: step 5540, loss = 21.54, batch loss = 21.46 (8.4 examples/sec; 0.953 sec/batch; 86h:34m:22s remains)
INFO - root - 2017-12-07 09:44:18.920788: step 5550, loss = 21.89, batch loss = 21.81 (8.4 examples/sec; 0.947 sec/batch; 86h:01m:38s remains)
INFO - root - 2017-12-07 09:44:28.231915: step 5560, loss = 21.78, batch loss = 21.70 (8.6 examples/sec; 0.932 sec/batch; 84h:39m:12s remains)
INFO - root - 2017-12-07 09:44:37.715015: step 5570, loss = 21.28, batch loss = 21.20 (8.8 examples/sec; 0.904 sec/batch; 82h:07m:52s remains)
INFO - root - 2017-12-07 09:44:46.993599: step 5580, loss = 21.23, batch loss = 21.15 (9.2 examples/sec; 0.874 sec/batch; 79h:20m:57s remains)
INFO - root - 2017-12-07 09:44:56.387932: step 5590, loss = 21.60, batch loss = 21.51 (8.9 examples/sec; 0.897 sec/batch; 81h:25m:19s remains)
INFO - root - 2017-12-07 09:45:05.629344: step 5600, loss = 21.64, batch loss = 21.56 (8.8 examples/sec; 0.912 sec/batch; 82h:46m:39s remains)
2017-12-07 09:45:06.677760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4175296 -4.4519396 -4.4951668 -4.53872 -4.5608425 -4.5511632 -4.5300865 -4.5266967 -4.5369062 -4.5523996 -4.5505061 -4.5140772 -4.4609323 -4.4157085 -4.3908987][-4.4486918 -4.4877987 -4.5436106 -4.6054115 -4.6379409 -4.6213918 -4.586607 -4.5720487 -4.5774446 -4.6029239 -4.6172495 -4.5858865 -4.5298653 -4.4770708 -4.4410849][-4.4652286 -4.5082979 -4.5720649 -4.64177 -4.6671228 -4.6263647 -4.56571 -4.5342441 -4.5394063 -4.5907769 -4.6376209 -4.6244197 -4.5762582 -4.51728 -4.4644604][-4.4647741 -4.5076613 -4.5708485 -4.6302171 -4.6207361 -4.5279927 -4.4219832 -4.3639684 -4.3775163 -4.4762745 -4.5787945 -4.6051741 -4.5789056 -4.5175624 -4.446557][-4.437418 -4.4704933 -4.5208511 -4.5531611 -4.4867487 -4.3190632 -4.1550021 -4.069088 -4.0993905 -4.2563624 -4.4258618 -4.511292 -4.5235987 -4.4744768 -4.4006548][-4.3795934 -4.3933864 -4.4290833 -4.4398184 -4.32539 -4.0952477 -3.8875568 -3.7791584 -3.8153591 -4.0069284 -4.2195258 -4.3551483 -4.4159007 -4.3991871 -4.34997][-4.3053474 -4.3116612 -4.3530879 -4.3671446 -4.2338676 -3.96693 -3.7222176 -3.5832999 -3.6023557 -3.8043842 -4.0388813 -4.20211 -4.2988205 -4.3192759 -4.3097615][-4.2708621 -4.2959819 -4.3711476 -4.4163203 -4.2940888 -4.0211706 -3.754221 -3.5891197 -3.5857987 -3.7802815 -4.0032792 -4.1487384 -4.2452669 -4.2804732 -4.2990847][-4.2961574 -4.3473716 -4.4636064 -4.5531187 -4.4671984 -4.2249494 -3.9621356 -3.7860658 -3.761878 -3.9297974 -4.114243 -4.2112207 -4.275022 -4.2962346 -4.3235512][-4.361928 -4.4238272 -4.5541396 -4.6706352 -4.6331038 -4.4526567 -4.2264748 -4.0613074 -4.0246377 -4.1567788 -4.3008175 -4.3531418 -4.3782763 -4.3741837 -4.3945065][-4.4409323 -4.5009155 -4.6194043 -4.7392445 -4.7463021 -4.6358194 -4.4640517 -4.3269811 -4.2876468 -4.3848004 -4.493083 -4.5149846 -4.5120821 -4.4934349 -4.5041351][-4.4986629 -4.5501823 -4.6458392 -4.75271 -4.7897644 -4.7373195 -4.6192856 -4.51579 -4.4849019 -4.5550303 -4.636023 -4.6468453 -4.6357369 -4.6169896 -4.6151876][-4.5089631 -4.5521541 -4.625741 -4.7147913 -4.7713504 -4.76457 -4.6972094 -4.6268768 -4.6039982 -4.6490045 -4.7041612 -4.7125354 -4.7023063 -4.6832776 -4.66107][-4.468967 -4.5059357 -4.5608144 -4.6292644 -4.6885214 -4.7092485 -4.6837916 -4.643764 -4.6253381 -4.642849 -4.6699371 -4.675302 -4.6682744 -4.6470776 -4.6089263][-4.3961725 -4.4224486 -4.4556837 -4.4974213 -4.5408478 -4.5687981 -4.5710163 -4.5576572 -4.5457134 -4.5455709 -4.5534782 -4.5573912 -4.5546112 -4.5358367 -4.496841]]...]
INFO - root - 2017-12-07 09:45:16.106227: step 5610, loss = 21.56, batch loss = 21.48 (8.3 examples/sec; 0.958 sec/batch; 87h:00m:07s remains)
INFO - root - 2017-12-07 09:45:25.387445: step 5620, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.926 sec/batch; 84h:06m:48s remains)
INFO - root - 2017-12-07 09:45:34.610628: step 5630, loss = 21.92, batch loss = 21.83 (9.0 examples/sec; 0.894 sec/batch; 81h:07m:49s remains)
INFO - root - 2017-12-07 09:45:43.916121: step 5640, loss = 21.06, batch loss = 20.97 (8.9 examples/sec; 0.900 sec/batch; 81h:41m:30s remains)
INFO - root - 2017-12-07 09:45:53.266966: step 5650, loss = 21.28, batch loss = 21.20 (8.6 examples/sec; 0.931 sec/batch; 84h:31m:45s remains)
INFO - root - 2017-12-07 09:46:02.569261: step 5660, loss = 21.77, batch loss = 21.69 (8.8 examples/sec; 0.907 sec/batch; 82h:20m:36s remains)
INFO - root - 2017-12-07 09:46:12.004395: step 5670, loss = 21.34, batch loss = 21.26 (9.0 examples/sec; 0.892 sec/batch; 81h:00m:13s remains)
INFO - root - 2017-12-07 09:46:21.253973: step 5680, loss = 21.28, batch loss = 21.19 (8.7 examples/sec; 0.917 sec/batch; 83h:13m:30s remains)
INFO - root - 2017-12-07 09:46:30.794388: step 5690, loss = 21.13, batch loss = 21.05 (8.7 examples/sec; 0.923 sec/batch; 83h:47m:34s remains)
INFO - root - 2017-12-07 09:46:40.043518: step 5700, loss = 21.35, batch loss = 21.27 (8.4 examples/sec; 0.951 sec/batch; 86h:22m:11s remains)
2017-12-07 09:46:40.912943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2232723 -4.1631804 -4.0967946 -4.0571184 -4.0628982 -4.0946479 -4.125524 -4.1511369 -4.1373682 -4.0994115 -4.0773444 -4.0577545 -4.0393052 -4.0285873 -4.0537653][-4.23663 -4.1884069 -4.129293 -4.0895214 -4.0850053 -4.0965276 -4.1142273 -4.1409111 -4.1450968 -4.1265388 -4.1126142 -4.0923414 -4.0675249 -4.0471373 -4.0577822][-4.2507796 -4.2280784 -4.1981044 -4.1798334 -4.177197 -4.169178 -4.159503 -4.1602793 -4.1594834 -4.1518736 -4.1449757 -4.1306381 -4.1090055 -4.0866132 -4.0872617][-4.2694736 -4.2720618 -4.2724833 -4.2797656 -4.2862659 -4.2692223 -4.2378354 -4.211586 -4.1989832 -4.1961632 -4.1931357 -4.1842966 -4.1682711 -4.1493616 -4.1471667][-4.2903209 -4.3120513 -4.3320389 -4.3515382 -4.35984 -4.3373909 -4.2966003 -4.2610946 -4.2451348 -4.2461739 -4.2500329 -4.250648 -4.2416592 -4.2278624 -4.2260079][-4.3016729 -4.3346467 -4.364037 -4.3843932 -4.3873105 -4.3586874 -4.3135018 -4.2757983 -4.2566991 -4.2565069 -4.266963 -4.2798386 -4.2841964 -4.2840548 -4.2897282][-4.3005629 -4.3297276 -4.3543658 -4.3647842 -4.3552256 -4.3178849 -4.2665696 -4.227108 -4.2050886 -4.2038522 -4.22234 -4.2494082 -4.2740068 -4.2954793 -4.3139281][-4.2985477 -4.311666 -4.3189545 -4.3101931 -4.2821422 -4.2360029 -4.18031 -4.1401744 -4.117547 -4.1181889 -4.1470523 -4.1900959 -4.2373366 -4.279973 -4.3095727][-4.301681 -4.2980871 -4.2886896 -4.2621193 -4.2168946 -4.1632633 -4.1054463 -4.0670938 -4.0467143 -4.0506787 -4.0869989 -4.1419759 -4.2062173 -4.2629409 -4.2992826][-4.2959347 -4.2857952 -4.2727385 -4.2423968 -4.192502 -4.1382117 -4.0825462 -4.047328 -4.030242 -4.0355496 -4.0730171 -4.130744 -4.1982822 -4.2570252 -4.2939062][-4.2890048 -4.2802329 -4.2733274 -4.250031 -4.208107 -4.163641 -4.1190395 -4.0916066 -4.0779166 -4.080369 -4.1118469 -4.16304 -4.2201009 -4.2696419 -4.3003044][-4.2902832 -4.2844925 -4.2871957 -4.2757077 -4.2503772 -4.2250762 -4.1981468 -4.1797552 -4.1664848 -4.1632481 -4.1839104 -4.2216539 -4.2613912 -4.2961493 -4.3149405][-4.2873917 -4.2869954 -4.2988443 -4.2995162 -4.2926936 -4.286531 -4.2738609 -4.2607594 -4.2468843 -4.2373309 -4.2456164 -4.2692986 -4.2947965 -4.3176055 -4.3250985][-4.2833385 -4.2841744 -4.2991629 -4.3074327 -4.3141174 -4.32046 -4.3155394 -4.3060765 -4.2928147 -4.2778621 -4.2757807 -4.2891397 -4.3067536 -4.3232627 -4.3243666][-4.2896042 -4.2840991 -4.2965317 -4.3078604 -4.3219719 -4.3332624 -4.3296466 -4.3216982 -4.3089237 -4.2912025 -4.2841511 -4.29237 -4.3056154 -4.3174882 -4.3151045]]...]
INFO - root - 2017-12-07 09:46:50.302349: step 5710, loss = 21.15, batch loss = 21.07 (8.8 examples/sec; 0.909 sec/batch; 82h:33m:29s remains)
INFO - root - 2017-12-07 09:46:59.663155: step 5720, loss = 21.52, batch loss = 21.43 (8.7 examples/sec; 0.923 sec/batch; 83h:47m:39s remains)
INFO - root - 2017-12-07 09:47:09.204570: step 5730, loss = 21.49, batch loss = 21.40 (8.5 examples/sec; 0.937 sec/batch; 85h:03m:38s remains)
INFO - root - 2017-12-07 09:47:18.593330: step 5740, loss = 21.51, batch loss = 21.43 (9.0 examples/sec; 0.890 sec/batch; 80h:45m:03s remains)
INFO - root - 2017-12-07 09:47:28.028819: step 5750, loss = 21.54, batch loss = 21.46 (8.7 examples/sec; 0.921 sec/batch; 83h:37m:33s remains)
INFO - root - 2017-12-07 09:47:37.472360: step 5760, loss = 21.87, batch loss = 21.79 (8.2 examples/sec; 0.970 sec/batch; 88h:01m:33s remains)
INFO - root - 2017-12-07 09:47:46.832027: step 5770, loss = 21.30, batch loss = 21.22 (8.4 examples/sec; 0.952 sec/batch; 86h:25m:12s remains)
INFO - root - 2017-12-07 09:47:56.288540: step 5780, loss = 21.48, batch loss = 21.40 (8.1 examples/sec; 0.984 sec/batch; 89h:19m:41s remains)
INFO - root - 2017-12-07 09:48:05.803930: step 5790, loss = 21.40, batch loss = 21.32 (8.2 examples/sec; 0.981 sec/batch; 89h:02m:31s remains)
INFO - root - 2017-12-07 09:48:15.074607: step 5800, loss = 21.43, batch loss = 21.35 (8.7 examples/sec; 0.923 sec/batch; 83h:46m:34s remains)
2017-12-07 09:48:15.979694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3852224 -4.2780757 -4.2014332 -4.2337413 -4.3652482 -4.5199356 -4.6524854 -4.7247968 -4.7305083 -4.6778007 -4.562078 -4.4486403 -4.4172931 -4.4390392 -4.443778][-4.4359379 -4.3841352 -4.3451171 -4.3744106 -4.4720759 -4.5810533 -4.676455 -4.7316027 -4.739604 -4.7087321 -4.6265559 -4.5257182 -4.4811554 -4.4864321 -4.4856377][-4.4974527 -4.4901271 -4.4833183 -4.4898577 -4.5203166 -4.5606694 -4.6128325 -4.6612587 -4.6898246 -4.6966681 -4.6619897 -4.5881515 -4.533052 -4.5184989 -4.5044475][-4.526444 -4.5420065 -4.5458317 -4.4997034 -4.4265685 -4.3817272 -4.3939381 -4.4442124 -4.4977946 -4.5448852 -4.5696368 -4.5513988 -4.5140829 -4.4933591 -4.4638543][-4.4966598 -4.5157933 -4.512588 -4.4111586 -4.2308702 -4.0932117 -4.0607524 -4.108943 -4.1818938 -4.2624683 -4.3519211 -4.4133272 -4.4204907 -4.40661 -4.3661084][-4.4536891 -4.4517369 -4.4126253 -4.2697077 -4.0173793 -3.7941334 -3.7059374 -3.7530441 -3.864094 -3.994137 -4.1571178 -4.3105578 -4.3740487 -4.3669038 -4.3183947][-4.4275084 -4.4008837 -4.316473 -4.1451674 -3.8612812 -3.5720272 -3.4113636 -3.4482875 -3.6216545 -3.8234773 -4.0579648 -4.2947206 -4.409318 -4.398313 -4.3380871][-4.4440303 -4.4094167 -4.3072324 -4.1405163 -3.8826478 -3.5870471 -3.3838112 -3.3978238 -3.610486 -3.8606908 -4.1129804 -4.36045 -4.4755177 -4.4407425 -4.3683476][-4.5449886 -4.5137243 -4.4157739 -4.2645698 -4.0511384 -3.80024 -3.6165733 -3.6259356 -3.8519783 -4.1205988 -4.3421097 -4.5254354 -4.5813828 -4.4992871 -4.4120464][-4.6897736 -4.6522436 -4.5510778 -4.4001684 -4.2138257 -4.023088 -3.8957412 -3.9215837 -4.1424823 -4.4104443 -4.5983486 -4.7034025 -4.6866384 -4.5642242 -4.4706149][-4.8136773 -4.7761045 -4.6891022 -4.5666118 -4.4231253 -4.2912283 -4.21008 -4.2335072 -4.4020991 -4.6177096 -4.754631 -4.7951589 -4.7366924 -4.6134663 -4.5350833][-4.8445706 -4.8321567 -4.7851515 -4.7200127 -4.6407504 -4.568552 -4.5215483 -4.5325532 -4.6296482 -4.7589984 -4.8284039 -4.8183832 -4.7480721 -4.6502814 -4.5905743][-4.7318015 -4.7587967 -4.7588654 -4.7476034 -4.7264462 -4.7054896 -4.6935229 -4.7097564 -4.7596493 -4.8133888 -4.8240881 -4.7839112 -4.7177172 -4.6477323 -4.5998187][-4.5586061 -4.6172752 -4.6561513 -4.6834145 -4.7031198 -4.7193413 -4.7338614 -4.7512541 -4.7660809 -4.7656269 -4.7408552 -4.69425 -4.6395273 -4.58577 -4.5397635][-4.3911748 -4.4429841 -4.4876432 -4.5258794 -4.5618067 -4.5953741 -4.6230984 -4.6399193 -4.6397753 -4.6219182 -4.5919681 -4.5539722 -4.5103512 -4.4652824 -4.424252]]...]
INFO - root - 2017-12-07 09:48:25.244174: step 5810, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.910 sec/batch; 82h:36m:05s remains)
INFO - root - 2017-12-07 09:48:34.697900: step 5820, loss = 21.58, batch loss = 21.50 (8.5 examples/sec; 0.939 sec/batch; 85h:14m:59s remains)
INFO - root - 2017-12-07 09:48:44.116568: step 5830, loss = 21.29, batch loss = 21.21 (8.5 examples/sec; 0.940 sec/batch; 85h:17m:02s remains)
INFO - root - 2017-12-07 09:48:53.724026: step 5840, loss = 21.92, batch loss = 21.84 (8.3 examples/sec; 0.962 sec/batch; 87h:17m:46s remains)
INFO - root - 2017-12-07 09:49:03.131823: step 5850, loss = 21.43, batch loss = 21.35 (8.4 examples/sec; 0.957 sec/batch; 86h:50m:42s remains)
INFO - root - 2017-12-07 09:49:12.506068: step 5860, loss = 21.78, batch loss = 21.70 (9.0 examples/sec; 0.892 sec/batch; 80h:53m:53s remains)
INFO - root - 2017-12-07 09:49:21.995268: step 5870, loss = 21.55, batch loss = 21.47 (8.8 examples/sec; 0.911 sec/batch; 82h:37m:56s remains)
INFO - root - 2017-12-07 09:49:31.345630: step 5880, loss = 21.69, batch loss = 21.61 (8.5 examples/sec; 0.937 sec/batch; 85h:00m:41s remains)
INFO - root - 2017-12-07 09:49:40.684028: step 5890, loss = 21.24, batch loss = 21.15 (8.5 examples/sec; 0.938 sec/batch; 85h:03m:54s remains)
INFO - root - 2017-12-07 09:49:49.996885: step 5900, loss = 21.96, batch loss = 21.87 (8.7 examples/sec; 0.919 sec/batch; 83h:24m:49s remains)
2017-12-07 09:49:50.933019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5665393 -4.6116929 -4.6330285 -4.6441765 -4.6544533 -4.6426086 -4.6079359 -4.5881228 -4.6038547 -4.6374526 -4.6641541 -4.668766 -4.6515937 -4.6134 -4.576582][-4.6687622 -4.699687 -4.6970396 -4.6930232 -4.707705 -4.7098331 -4.6856484 -4.679513 -4.7068415 -4.7448521 -4.7626042 -4.7513027 -4.7265487 -4.6923604 -4.6630821][-4.7296963 -4.7598252 -4.7453771 -4.7226844 -4.7159729 -4.6979795 -4.6611776 -4.6645575 -4.7155523 -4.7782745 -4.8018236 -4.7795367 -4.7386131 -4.6961241 -4.6727014][-4.6903629 -4.7442417 -4.7435775 -4.7118025 -4.6617827 -4.5833106 -4.5012131 -4.5002713 -4.5906091 -4.70979 -4.7685952 -4.7488279 -4.6837425 -4.6070743 -4.5681729][-4.5533094 -4.6408887 -4.6656332 -4.6365142 -4.5411415 -4.3864379 -4.2417521 -4.2239437 -4.3593383 -4.5432391 -4.6531272 -4.6551681 -4.5814605 -4.4712491 -4.400732][-4.4015479 -4.5155134 -4.562696 -4.533185 -4.3923965 -4.1630311 -3.9587898 -3.9227548 -4.0917416 -4.320435 -4.4788141 -4.5301628 -4.4875255 -4.3696523 -4.2589936][-4.3076491 -4.4278517 -4.476954 -4.4266863 -4.2413206 -3.955749 -3.7103109 -3.6588657 -3.8387516 -4.0899491 -4.293539 -4.4153204 -4.4298964 -4.3205786 -4.1732297][-4.3092046 -4.4194736 -4.4495611 -4.372735 -4.1676989 -3.86517 -3.6085954 -3.5515327 -3.7248285 -3.9689441 -4.1870956 -4.3522644 -4.4100409 -4.3190413 -4.1642814][-4.3857703 -4.4762239 -4.4779716 -4.3833275 -4.1930571 -3.9214797 -3.6937406 -3.6516058 -3.8132792 -4.0235786 -4.2042208 -4.3507833 -4.4089031 -4.3364244 -4.2090788][-4.493011 -4.5454316 -4.5117574 -4.40936 -4.2566528 -4.0493026 -3.8765717 -3.8596072 -4.0076 -4.1772151 -4.3043356 -4.406662 -4.4472318 -4.3935175 -4.3050375][-4.6222525 -4.630197 -4.569694 -4.4738846 -4.36718 -4.2297144 -4.1109247 -4.1086245 -4.2285547 -4.3531928 -4.4314203 -4.4952316 -4.5272255 -4.5009017 -4.4562984][-4.7257924 -4.7127047 -4.6526737 -4.5729833 -4.4984179 -4.4108849 -4.3350573 -4.3374863 -4.4191914 -4.4970288 -4.5391674 -4.5782022 -4.6085715 -4.608665 -4.600153][-4.7250957 -4.7266912 -4.7043138 -4.6562734 -4.5999789 -4.5381308 -4.4936728 -4.5045209 -4.556355 -4.5978031 -4.6132 -4.6313848 -4.6491489 -4.6515522 -4.6511827][-4.6823778 -4.7190452 -4.7398276 -4.7223525 -4.67242 -4.614718 -4.5798426 -4.5901356 -4.6242819 -4.6476641 -4.6463146 -4.6422243 -4.6390414 -4.6346498 -4.6368351][-4.6542459 -4.7058277 -4.7336564 -4.7175508 -4.6631641 -4.6034327 -4.5688314 -4.5723252 -4.5991216 -4.6218491 -4.62318 -4.6158848 -4.6064539 -4.6029859 -4.6111293]]...]
INFO - root - 2017-12-07 09:50:00.382145: step 5910, loss = 21.60, batch loss = 21.52 (8.7 examples/sec; 0.919 sec/batch; 83h:24m:04s remains)
INFO - root - 2017-12-07 09:50:09.831705: step 5920, loss = 21.37, batch loss = 21.28 (8.3 examples/sec; 0.963 sec/batch; 87h:20m:52s remains)
INFO - root - 2017-12-07 09:50:19.246998: step 5930, loss = 21.64, batch loss = 21.56 (8.1 examples/sec; 0.984 sec/batch; 89h:15m:50s remains)
INFO - root - 2017-12-07 09:50:28.557371: step 5940, loss = 21.11, batch loss = 21.03 (8.3 examples/sec; 0.964 sec/batch; 87h:26m:37s remains)
INFO - root - 2017-12-07 09:50:37.924081: step 5950, loss = 21.53, batch loss = 21.44 (8.4 examples/sec; 0.954 sec/batch; 86h:31m:20s remains)
INFO - root - 2017-12-07 09:50:47.280313: step 5960, loss = 21.64, batch loss = 21.55 (8.2 examples/sec; 0.977 sec/batch; 88h:38m:56s remains)
INFO - root - 2017-12-07 09:50:56.679513: step 5970, loss = 21.81, batch loss = 21.73 (8.4 examples/sec; 0.952 sec/batch; 86h:20m:13s remains)
INFO - root - 2017-12-07 09:51:06.146304: step 5980, loss = 21.25, batch loss = 21.17 (8.3 examples/sec; 0.964 sec/batch; 87h:26m:53s remains)
INFO - root - 2017-12-07 09:51:15.533204: step 5990, loss = 21.66, batch loss = 21.57 (8.7 examples/sec; 0.923 sec/batch; 83h:45m:18s remains)
INFO - root - 2017-12-07 09:51:24.946325: step 6000, loss = 21.45, batch loss = 21.37 (8.2 examples/sec; 0.975 sec/batch; 88h:23m:29s remains)
2017-12-07 09:51:25.844049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4096985 -4.4329276 -4.4878798 -4.5540934 -4.6040096 -4.6261477 -4.6396313 -4.6645083 -4.6883183 -4.6801615 -4.6242309 -4.5267329 -4.4283776 -4.37658 -4.393784][-4.2902141 -4.3084607 -4.3831739 -4.491334 -4.584754 -4.62783 -4.6315227 -4.6460552 -4.67721 -4.6815314 -4.6267538 -4.5164948 -4.4058647 -4.35662 -4.394464][-4.187777 -4.2072191 -4.2854462 -4.40793 -4.5324674 -4.6072931 -4.6276655 -4.6413608 -4.6646476 -4.6526494 -4.5803766 -4.4569149 -4.3404841 -4.2924089 -4.3441987][-4.1550412 -4.1815872 -4.2456632 -4.34908 -4.4742675 -4.5692081 -4.6142097 -4.6377392 -4.6531796 -4.6239724 -4.53706 -4.4156547 -4.3063321 -4.2587576 -4.3089337][-4.1943521 -4.2242351 -4.2615485 -4.3185287 -4.4061456 -4.4884148 -4.5451245 -4.5878725 -4.6149716 -4.5944881 -4.5226979 -4.4302263 -4.3429818 -4.2920303 -4.3244][-4.2807603 -4.3098125 -4.3217344 -4.3310318 -4.3707814 -4.4202857 -4.4601603 -4.4963183 -4.5221953 -4.5177326 -4.4860659 -4.4511776 -4.4116769 -4.3766713 -4.3982472][-4.3786416 -4.3917251 -4.373455 -4.3435483 -4.3458676 -4.3599315 -4.3636174 -4.3604231 -4.3566318 -4.3570013 -4.3749361 -4.4122448 -4.4384308 -4.4444818 -4.4793768][-4.4239755 -4.4278774 -4.3954086 -4.3483953 -4.3290629 -4.3149934 -4.273674 -4.2092943 -4.1554103 -4.1471505 -4.2059078 -4.3037629 -4.3893094 -4.444253 -4.5104465][-4.4165716 -4.4338689 -4.4145708 -4.3817186 -4.3620968 -4.3266559 -4.2385111 -4.1051736 -3.9889128 -3.95041 -4.02662 -4.1686358 -4.3032994 -4.4071765 -4.5072865][-4.380259 -4.4212351 -4.4268765 -4.4193997 -4.41423 -4.3692236 -4.2491622 -4.0643845 -3.8954685 -3.8208151 -3.8906498 -4.0483904 -4.2146578 -4.3585668 -4.484467][-4.3416882 -4.4135947 -4.4483376 -4.4632134 -4.4729877 -4.4290586 -4.2987571 -4.094532 -3.9030895 -3.8050258 -3.8529003 -3.9982095 -4.172792 -4.3374467 -4.4667311][-4.2953315 -4.3967843 -4.4561954 -4.4911313 -4.5201607 -4.4937 -4.3779268 -4.1860113 -4.0005937 -3.8928196 -3.9123681 -4.0309706 -4.200304 -4.3723755 -4.4867878][-4.1757355 -4.2908907 -4.3714914 -4.434433 -4.4931183 -4.4988647 -4.4229383 -4.2787976 -4.1288004 -4.0259967 -4.0173078 -4.1027584 -4.2517829 -4.4098387 -4.4937444][-4.0172677 -4.127739 -4.2212753 -4.3159227 -4.4047756 -4.4412832 -4.4128447 -4.3349795 -4.2422032 -4.1600137 -4.1313477 -4.1845312 -4.3001862 -4.4227571 -4.4679337][-3.9022727 -3.9983919 -4.08756 -4.1976533 -4.3021793 -4.3559146 -4.3632345 -4.3500104 -4.32596 -4.2816434 -4.2530661 -4.2867656 -4.3673124 -4.4431958 -4.4482775]]...]
INFO - root - 2017-12-07 09:51:35.194514: step 6010, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.955 sec/batch; 86h:34m:11s remains)
INFO - root - 2017-12-07 09:51:44.398717: step 6020, loss = 21.71, batch loss = 21.63 (8.5 examples/sec; 0.941 sec/batch; 85h:22m:24s remains)
INFO - root - 2017-12-07 09:51:53.867991: step 6030, loss = 21.23, batch loss = 21.15 (8.3 examples/sec; 0.966 sec/batch; 87h:33m:28s remains)
INFO - root - 2017-12-07 09:52:03.227043: step 6040, loss = 21.39, batch loss = 21.31 (8.1 examples/sec; 0.992 sec/batch; 89h:55m:08s remains)
INFO - root - 2017-12-07 09:52:12.633429: step 6050, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.956 sec/batch; 86h:41m:42s remains)
INFO - root - 2017-12-07 09:52:22.125179: step 6060, loss = 21.36, batch loss = 21.27 (8.4 examples/sec; 0.948 sec/batch; 86h:00m:15s remains)
INFO - root - 2017-12-07 09:52:31.543398: step 6070, loss = 21.36, batch loss = 21.27 (8.5 examples/sec; 0.945 sec/batch; 85h:42m:28s remains)
INFO - root - 2017-12-07 09:52:40.867783: step 6080, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.936 sec/batch; 84h:50m:13s remains)
INFO - root - 2017-12-07 09:52:50.118364: step 6090, loss = 21.21, batch loss = 21.13 (8.4 examples/sec; 0.950 sec/batch; 86h:06m:32s remains)
INFO - root - 2017-12-07 09:52:59.608543: step 6100, loss = 21.56, batch loss = 21.48 (8.3 examples/sec; 0.960 sec/batch; 87h:02m:36s remains)
2017-12-07 09:53:00.493337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.283884 -4.3709097 -4.4985461 -4.6156473 -4.6854715 -4.7042637 -4.6732092 -4.6116633 -4.5606184 -4.5671239 -4.6197548 -4.6642475 -4.66798 -4.6553178 -4.6579142][-4.2903547 -4.3864875 -4.5238285 -4.6452122 -4.7130809 -4.7263756 -4.6843643 -4.6021647 -4.5222297 -4.5142312 -4.585484 -4.6628652 -4.685894 -4.6709447 -4.6699204][-4.2990818 -4.3835697 -4.4976411 -4.5967846 -4.6498442 -4.65302 -4.6038108 -4.5231495 -4.4413013 -4.4251862 -4.4953971 -4.5922055 -4.64303 -4.6321211 -4.623075][-4.3151889 -4.37859 -4.4489264 -4.5082397 -4.5370693 -4.5219569 -4.4580054 -4.3870344 -4.337965 -4.3416948 -4.4040494 -4.4988418 -4.5703769 -4.5640736 -4.5415998][-4.3392935 -4.3838525 -4.4068151 -4.4152856 -4.4062271 -4.3640852 -4.28306 -4.2235718 -4.2257571 -4.2803354 -4.3529825 -4.4410405 -4.5152979 -4.5046148 -4.4661727][-4.3544374 -4.3863397 -4.3724556 -4.3336272 -4.2810416 -4.2118611 -4.122117 -4.0763855 -4.1239777 -4.2408438 -4.351954 -4.4408689 -4.4959383 -4.464076 -4.4075031][-4.3618321 -4.3859925 -4.355371 -4.2875948 -4.1956158 -4.0872769 -3.9708345 -3.9176016 -3.9855213 -4.165473 -4.3541856 -4.4833465 -4.5292983 -4.475533 -4.4016662][-4.3727674 -4.4024129 -4.3773308 -4.3054066 -4.1961994 -4.0492496 -3.887938 -3.7959976 -3.8466282 -4.0574007 -4.3222651 -4.5105228 -4.5697126 -4.516324 -4.4419103][-4.4038658 -4.4538951 -4.4534783 -4.3995533 -4.2984433 -4.1397734 -3.9455473 -3.8060751 -3.8110759 -4.0087333 -4.3055859 -4.5252604 -4.5944738 -4.5507178 -4.486547][-4.4522929 -4.5368066 -4.5726376 -4.5464034 -4.4654603 -4.3150177 -4.1063113 -3.925164 -3.8786647 -4.0435009 -4.3298993 -4.5433693 -4.6075997 -4.5751138 -4.5268559][-4.5046406 -4.6233196 -4.6920781 -4.6878242 -4.6227942 -4.4863663 -4.2817049 -4.08359 -4.005888 -4.1351743 -4.3834834 -4.561965 -4.6101303 -4.5848618 -4.5507979][-4.5463428 -4.6909204 -4.7858505 -4.8025169 -4.7531877 -4.6359892 -4.45255 -4.2647357 -4.1765475 -4.2671175 -4.4543147 -4.5812211 -4.6099749 -4.5877004 -4.562655][-4.559906 -4.715333 -4.8284936 -4.8647733 -4.8332834 -4.7383432 -4.5890374 -4.4336123 -4.3560319 -4.4131026 -4.5351596 -4.6123772 -4.6273355 -4.6107 -4.5926957][-4.5286722 -4.6764717 -4.7974095 -4.85385 -4.8476481 -4.7859082 -4.6822596 -4.5715942 -4.512939 -4.5420146 -4.6090732 -4.648819 -4.6552095 -4.6433215 -4.6280007][-4.46322 -4.5846133 -4.6973987 -4.7654567 -4.7818279 -4.7534 -4.6940913 -4.625103 -4.5836396 -4.59003 -4.6155477 -4.6290207 -4.6292105 -4.6197958 -4.604877]]...]
INFO - root - 2017-12-07 09:53:10.029992: step 6110, loss = 21.45, batch loss = 21.37 (8.3 examples/sec; 0.967 sec/batch; 87h:40m:17s remains)
INFO - root - 2017-12-07 09:53:19.313645: step 6120, loss = 21.89, batch loss = 21.81 (9.2 examples/sec; 0.867 sec/batch; 78h:37m:10s remains)
INFO - root - 2017-12-07 09:53:28.782987: step 6130, loss = 21.22, batch loss = 21.14 (8.3 examples/sec; 0.966 sec/batch; 87h:35m:34s remains)
INFO - root - 2017-12-07 09:53:38.247937: step 6140, loss = 21.37, batch loss = 21.29 (8.4 examples/sec; 0.948 sec/batch; 85h:57m:26s remains)
INFO - root - 2017-12-07 09:53:47.703515: step 6150, loss = 20.92, batch loss = 20.83 (8.3 examples/sec; 0.965 sec/batch; 87h:27m:34s remains)
INFO - root - 2017-12-07 09:53:57.117004: step 6160, loss = 21.50, batch loss = 21.42 (8.1 examples/sec; 0.993 sec/batch; 90h:02m:15s remains)
INFO - root - 2017-12-07 09:54:06.336069: step 6170, loss = 21.41, batch loss = 21.33 (8.4 examples/sec; 0.955 sec/batch; 86h:33m:28s remains)
INFO - root - 2017-12-07 09:54:15.442240: step 6180, loss = 21.68, batch loss = 21.59 (9.1 examples/sec; 0.878 sec/batch; 79h:35m:51s remains)
INFO - root - 2017-12-07 09:54:24.910864: step 6190, loss = 21.58, batch loss = 21.50 (8.6 examples/sec; 0.927 sec/batch; 84h:03m:02s remains)
INFO - root - 2017-12-07 09:54:34.136903: step 6200, loss = 21.74, batch loss = 21.65 (9.0 examples/sec; 0.886 sec/batch; 80h:16m:53s remains)
2017-12-07 09:54:35.036184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3608842 -4.3891897 -4.423665 -4.4330521 -4.4152908 -4.41578 -4.4283404 -4.3829036 -4.25968 -4.1591291 -4.1876631 -4.3193803 -4.46451 -4.5298028 -4.5222082][-4.3219161 -4.3856468 -4.4638352 -4.5035887 -4.4999104 -4.496933 -4.4865847 -4.4151888 -4.2737451 -4.1580215 -4.1797581 -4.3092279 -4.4530206 -4.5360479 -4.5572209][-4.3128843 -4.4117942 -4.522974 -4.5820727 -4.5822864 -4.5568538 -4.5017233 -4.3926344 -4.2331109 -4.1114645 -4.1290221 -4.2537909 -4.40117 -4.5117636 -4.5637269][-4.3343439 -4.4520273 -4.5730481 -4.6334305 -4.6234436 -4.560564 -4.4503655 -4.2962461 -4.12146 -4.0076723 -4.0334692 -4.1637411 -4.3278041 -4.4758081 -4.5558281][-4.4017129 -4.5165081 -4.6233449 -4.6613379 -4.6173625 -4.5035167 -4.3406582 -4.1577063 -3.9927652 -3.9148726 -3.9673963 -4.1067786 -4.2821817 -4.4544716 -4.5456457][-4.4805121 -4.5696726 -4.639873 -4.6381011 -4.5476356 -4.3804035 -4.1752071 -3.9821727 -3.8511822 -3.8316207 -3.9255395 -4.0814371 -4.2646203 -4.4416642 -4.5281286][-4.5079169 -4.5643682 -4.5960402 -4.5656695 -4.446569 -4.2466612 -4.0220351 -3.8445315 -3.7674713 -3.8064721 -3.9297984 -4.0942049 -4.2764511 -4.4407854 -4.5118093][-4.4679065 -4.50723 -4.5270295 -4.5003543 -4.3840151 -4.1770377 -3.9501076 -3.8010812 -3.7757978 -3.8545942 -3.9928484 -4.1595311 -4.3333478 -4.4720678 -4.5222583][-4.39607 -4.4291863 -4.4556975 -4.4524198 -4.3541689 -4.1497231 -3.9276242 -3.8087451 -3.8246138 -3.931073 -4.0811634 -4.24848 -4.4061694 -4.5105934 -4.5391932][-4.3398113 -4.3619614 -4.3942876 -4.4140487 -4.3342566 -4.1388531 -3.94143 -3.8669302 -3.9213734 -4.0503731 -4.209949 -4.36429 -4.4820147 -4.5376468 -4.5369072][-4.30586 -4.326715 -4.3675723 -4.4024515 -4.3359275 -4.1545959 -3.990746 -3.9578547 -4.0396838 -4.1874933 -4.3559928 -4.4830279 -4.5413671 -4.535563 -4.4950595][-4.2998328 -4.3202872 -4.3600497 -4.39204 -4.3335333 -4.1750064 -4.0482554 -4.0488691 -4.15314 -4.3185573 -4.4814172 -4.5656414 -4.5637107 -4.5031815 -4.4252248][-4.3033919 -4.3173285 -4.3419018 -4.3605318 -4.3126116 -4.1910639 -4.1093636 -4.1413689 -4.2625122 -4.4250941 -4.5546651 -4.5932593 -4.5518193 -4.4576621 -4.3498216][-4.3134618 -4.3332844 -4.3569469 -4.3713226 -4.3314619 -4.2374363 -4.1816578 -4.2208805 -4.3298969 -4.4588642 -4.5423536 -4.5514269 -4.494751 -4.3833561 -4.2546983][-4.3468733 -4.379034 -4.4143767 -4.4306474 -4.3919926 -4.3105993 -4.260139 -4.2837868 -4.3587346 -4.4396243 -4.4839473 -4.4845495 -4.4327168 -4.3283391 -4.2080178]]...]
INFO - root - 2017-12-07 09:54:44.454992: step 6210, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.931 sec/batch; 84h:23m:42s remains)
INFO - root - 2017-12-07 09:54:53.767245: step 6220, loss = 21.47, batch loss = 21.39 (8.3 examples/sec; 0.959 sec/batch; 86h:52m:29s remains)
INFO - root - 2017-12-07 09:55:03.171803: step 6230, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.928 sec/batch; 84h:06m:25s remains)
INFO - root - 2017-12-07 09:55:12.653685: step 6240, loss = 21.64, batch loss = 21.56 (8.6 examples/sec; 0.935 sec/batch; 84h:45m:11s remains)
INFO - root - 2017-12-07 09:55:22.057927: step 6250, loss = 21.09, batch loss = 21.01 (8.6 examples/sec; 0.926 sec/batch; 83h:53m:43s remains)
INFO - root - 2017-12-07 09:55:31.433857: step 6260, loss = 21.68, batch loss = 21.59 (8.6 examples/sec; 0.928 sec/batch; 84h:05m:46s remains)
INFO - root - 2017-12-07 09:55:40.734243: step 6270, loss = 21.24, batch loss = 21.15 (8.6 examples/sec; 0.928 sec/batch; 84h:05m:22s remains)
INFO - root - 2017-12-07 09:55:50.055964: step 6280, loss = 21.58, batch loss = 21.50 (8.9 examples/sec; 0.900 sec/batch; 81h:33m:58s remains)
INFO - root - 2017-12-07 09:55:59.341911: step 6290, loss = 21.47, batch loss = 21.39 (8.3 examples/sec; 0.960 sec/batch; 86h:58m:02s remains)
INFO - root - 2017-12-07 09:56:08.758079: step 6300, loss = 21.50, batch loss = 21.42 (8.8 examples/sec; 0.906 sec/batch; 82h:06m:39s remains)
2017-12-07 09:56:09.708677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3808188 -4.3945336 -4.3897352 -4.3772154 -4.374527 -4.3844771 -4.4050961 -4.4112554 -4.3912158 -4.3976984 -4.4408255 -4.46677 -4.4582233 -4.430068 -4.4039106][-4.4007249 -4.4184966 -4.402987 -4.3650665 -4.3225794 -4.289331 -4.2846847 -4.307035 -4.3317719 -4.3766284 -4.4431882 -4.4924221 -4.5018692 -4.484808 -4.4663062][-4.41796 -4.4394803 -4.4077063 -4.3384094 -4.2609286 -4.1969066 -4.1748 -4.2104778 -4.2671118 -4.3366976 -4.4207044 -4.4945436 -4.5296693 -4.5347414 -4.5341768][-4.4322028 -4.4560261 -4.4028049 -4.2951245 -4.1826329 -4.0992622 -4.0716081 -4.1160007 -4.1870618 -4.2655292 -4.35955 -4.452826 -4.5118642 -4.5411887 -4.5619955][-4.4469962 -4.473021 -4.4048748 -4.2648377 -4.1170454 -4.0121317 -3.9781675 -4.0220952 -4.0963664 -4.1789007 -4.2814488 -4.3865128 -4.4574404 -4.4960513 -4.524478][-4.45964 -4.4959059 -4.425539 -4.2655373 -4.0790763 -3.9331818 -3.8722727 -3.9027994 -3.9824717 -4.08676 -4.2162447 -4.337029 -4.4045677 -4.4225855 -4.426507][-4.4682374 -4.5140853 -4.4475913 -4.2758946 -4.0500193 -3.8496487 -3.7483697 -3.7652335 -3.8636777 -4.0096292 -4.1832256 -4.3232646 -4.3725419 -4.3429203 -4.294672][-4.4706979 -4.523664 -4.4669452 -4.2982788 -4.0531855 -3.8131785 -3.6819179 -3.6922231 -3.8077991 -3.9865253 -4.1871753 -4.3270206 -4.3430905 -4.2590542 -4.1571465][-4.4734077 -4.5360961 -4.5003881 -4.352325 -4.1121178 -3.8606465 -3.72043 -3.7291794 -3.8469546 -4.0279655 -4.2195868 -4.3291559 -4.3031826 -4.1822333 -4.05492][-4.4845428 -4.5617895 -4.5573869 -4.4441113 -4.22511 -3.9820101 -3.8480809 -3.8579912 -3.9681313 -4.1358747 -4.300868 -4.3643093 -4.2966137 -4.1578736 -4.0299735][-4.5004315 -4.5945539 -4.6246819 -4.5523748 -4.3662 -4.1456552 -4.0288358 -4.0429692 -4.1453257 -4.301209 -4.4361715 -4.45441 -4.3549 -4.2135139 -4.1026573][-4.5133867 -4.6131072 -4.6666055 -4.6313844 -4.487514 -4.3019285 -4.2077675 -4.2253628 -4.3191462 -4.4652786 -4.5776534 -4.5660739 -4.4553304 -4.3250647 -4.2373872][-4.5160017 -4.6023121 -4.6570067 -4.6489334 -4.5594368 -4.4267936 -4.3611274 -4.3825955 -4.4658594 -4.5970364 -4.6888924 -4.6638489 -4.5571194 -4.4418092 -4.3736238][-4.5033422 -4.5670481 -4.6082692 -4.6191483 -4.5895538 -4.5234418 -4.4888363 -4.5100718 -4.5749869 -4.6773252 -4.7437906 -4.7181854 -4.6283107 -4.5285645 -4.4691877][-4.4748287 -4.521102 -4.5459323 -4.5590911 -4.5635958 -4.5453773 -4.5323815 -4.5475969 -4.5907755 -4.6597967 -4.7026734 -4.6861243 -4.6251426 -4.5513225 -4.5018053]]...]
INFO - root - 2017-12-07 09:56:19.140073: step 6310, loss = 21.91, batch loss = 21.83 (8.1 examples/sec; 0.993 sec/batch; 89h:57m:32s remains)
INFO - root - 2017-12-07 09:56:28.532503: step 6320, loss = 21.38, batch loss = 21.29 (8.4 examples/sec; 0.955 sec/batch; 86h:33m:02s remains)
INFO - root - 2017-12-07 09:56:37.541647: step 6330, loss = 21.12, batch loss = 21.03 (9.1 examples/sec; 0.882 sec/batch; 79h:54m:52s remains)
INFO - root - 2017-12-07 09:56:46.863192: step 6340, loss = 21.67, batch loss = 21.59 (8.4 examples/sec; 0.958 sec/batch; 86h:47m:25s remains)
INFO - root - 2017-12-07 09:56:56.273428: step 6350, loss = 21.72, batch loss = 21.63 (8.0 examples/sec; 1.000 sec/batch; 90h:35m:13s remains)
INFO - root - 2017-12-07 09:57:05.556557: step 6360, loss = 21.51, batch loss = 21.43 (8.2 examples/sec; 0.971 sec/batch; 87h:59m:45s remains)
INFO - root - 2017-12-07 09:57:14.965088: step 6370, loss = 21.21, batch loss = 21.13 (8.2 examples/sec; 0.981 sec/batch; 88h:52m:38s remains)
INFO - root - 2017-12-07 09:57:24.348575: step 6380, loss = 21.24, batch loss = 21.16 (8.3 examples/sec; 0.960 sec/batch; 86h:58m:18s remains)
INFO - root - 2017-12-07 09:57:33.605222: step 6390, loss = 21.36, batch loss = 21.28 (9.0 examples/sec; 0.887 sec/batch; 80h:22m:13s remains)
INFO - root - 2017-12-07 09:57:43.050693: step 6400, loss = 21.15, batch loss = 21.06 (8.5 examples/sec; 0.938 sec/batch; 85h:00m:18s remains)
2017-12-07 09:57:43.967137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4132991 -4.481811 -4.570982 -4.6630187 -4.708631 -4.6534791 -4.5444231 -4.473918 -4.46084 -4.4778652 -4.4699731 -4.4288177 -4.4080319 -4.396842 -4.4153147][-4.4428911 -4.5267305 -4.6378522 -4.7386727 -4.7583833 -4.6449976 -4.4761972 -4.3785491 -4.3732109 -4.4187312 -4.4447141 -4.4262691 -4.4146996 -4.409873 -4.427175][-4.4827094 -4.5759749 -4.6869607 -4.7620406 -4.7278638 -4.5511255 -4.3361506 -4.2257853 -4.2378926 -4.3133969 -4.363245 -4.3626785 -4.360465 -4.36356 -4.383018][-4.5222335 -4.6135921 -4.6990423 -4.7162952 -4.6120453 -4.38386 -4.1475945 -4.0437822 -4.0848722 -4.1902571 -4.2547526 -4.2644143 -4.2647066 -4.2648406 -4.2771106][-4.5342245 -4.6054969 -4.6521091 -4.6152983 -4.4650612 -4.2197046 -3.9904661 -3.9055254 -3.9750123 -4.1037321 -4.1747036 -4.1821251 -4.1710348 -4.1607151 -4.1635818][-4.5006108 -4.5399857 -4.5524244 -4.4941778 -4.3407412 -4.1092992 -3.898917 -3.8311057 -3.9165125 -4.0589523 -4.134016 -4.1385126 -4.1186652 -4.114222 -4.1273217][-4.4221144 -4.4285626 -4.425993 -4.3802004 -4.2527156 -4.0432482 -3.843529 -3.7764802 -3.8557589 -3.9982302 -4.0867414 -4.1114683 -4.1123891 -4.1434841 -4.1940355][-4.3235869 -4.3159342 -4.3267808 -4.317081 -4.2242055 -4.0332065 -3.8359315 -3.7528806 -3.803252 -3.92647 -4.0325284 -4.098321 -4.1457796 -4.2314582 -4.3324504][-4.2516894 -4.2494183 -4.2838931 -4.3136487 -4.2626262 -4.1065526 -3.9262655 -3.8284895 -3.84401 -3.9336877 -4.0409107 -4.1399679 -4.2322965 -4.3609629 -4.4926772][-4.2246332 -4.2338676 -4.2843485 -4.3440456 -4.3430963 -4.2435107 -4.0991955 -3.9984798 -3.9915531 -4.0547471 -4.1506472 -4.2590995 -4.3683987 -4.500391 -4.6141229][-4.2720203 -4.2853031 -4.3304658 -4.3980403 -4.4357152 -4.39182 -4.2846069 -4.1880183 -4.1697817 -4.2165742 -4.3016868 -4.4069805 -4.5061374 -4.6026831 -4.6567035][-4.3855309 -4.3876877 -4.4065251 -4.4563766 -4.5048337 -4.4996762 -4.427249 -4.3389544 -4.3123517 -4.3432674 -4.4136496 -4.5076313 -4.5820842 -4.61901 -4.5975022][-4.4812975 -4.4734597 -4.4686685 -4.4973707 -4.5440922 -4.5706043 -4.5376039 -4.4706264 -4.4378448 -4.4435811 -4.4806266 -4.5407662 -4.5765905 -4.5563045 -4.4818544][-4.5200577 -4.501792 -4.4878407 -4.5096455 -4.5591564 -4.6112881 -4.6180263 -4.5805378 -4.549077 -4.5283084 -4.5195622 -4.530879 -4.5283303 -4.4778018 -4.3920207][-4.52148 -4.4967585 -4.4876781 -4.5138421 -4.5662322 -4.6274261 -4.6537061 -4.6349864 -4.6069822 -4.5701356 -4.526248 -4.4979234 -4.4685769 -4.40987 -4.338779]]...]
INFO - root - 2017-12-07 09:57:53.341426: step 6410, loss = 21.24, batch loss = 21.15 (9.2 examples/sec; 0.874 sec/batch; 79h:10m:38s remains)
INFO - root - 2017-12-07 09:58:02.715959: step 6420, loss = 21.47, batch loss = 21.38 (8.9 examples/sec; 0.897 sec/batch; 81h:13m:12s remains)
INFO - root - 2017-12-07 09:58:12.101126: step 6430, loss = 21.41, batch loss = 21.32 (8.8 examples/sec; 0.911 sec/batch; 82h:33m:09s remains)
INFO - root - 2017-12-07 09:58:21.429445: step 6440, loss = 21.77, batch loss = 21.69 (8.7 examples/sec; 0.922 sec/batch; 83h:30m:25s remains)
INFO - root - 2017-12-07 09:58:30.756782: step 6450, loss = 21.31, batch loss = 21.23 (8.7 examples/sec; 0.920 sec/batch; 83h:20m:29s remains)
INFO - root - 2017-12-07 09:58:40.165201: step 6460, loss = 21.68, batch loss = 21.60 (8.3 examples/sec; 0.964 sec/batch; 87h:20m:26s remains)
INFO - root - 2017-12-07 09:58:49.487043: step 6470, loss = 21.56, batch loss = 21.48 (8.8 examples/sec; 0.911 sec/batch; 82h:28m:25s remains)
INFO - root - 2017-12-07 09:58:58.780090: step 6480, loss = 21.12, batch loss = 21.04 (8.7 examples/sec; 0.917 sec/batch; 83h:01m:31s remains)
INFO - root - 2017-12-07 09:59:07.987561: step 6490, loss = 21.58, batch loss = 21.50 (8.9 examples/sec; 0.901 sec/batch; 81h:33m:19s remains)
INFO - root - 2017-12-07 09:59:17.468745: step 6500, loss = 21.74, batch loss = 21.66 (8.6 examples/sec; 0.931 sec/batch; 84h:17m:31s remains)
2017-12-07 09:59:18.369376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4756536 -4.5922489 -4.6946707 -4.7658014 -4.7600775 -4.6818256 -4.6277189 -4.6582012 -4.7473388 -4.8470078 -4.8987489 -4.8887858 -4.8552 -4.7952785 -4.7118349][-4.528789 -4.6542387 -4.74921 -4.800961 -4.7860327 -4.71519 -4.6827393 -4.7295251 -4.8255215 -4.9358339 -4.9897738 -4.9582686 -4.8963709 -4.8172445 -4.7151833][-4.6231833 -4.740644 -4.7944789 -4.7937856 -4.7466917 -4.6781487 -4.6623549 -4.7141008 -4.80069 -4.9055476 -4.9566231 -4.9232292 -4.8690629 -4.7995114 -4.7060947][-4.7230034 -4.8076181 -4.8009796 -4.7334032 -4.6345124 -4.5455689 -4.5287347 -4.5661125 -4.630518 -4.720974 -4.7749496 -4.7714405 -4.7692037 -4.7466884 -4.68864][-4.7966814 -4.8415651 -4.7798243 -4.6509585 -4.4907632 -4.3551373 -4.3063154 -4.3147655 -4.3681068 -4.462687 -4.5348058 -4.5800791 -4.6464052 -4.6842885 -4.6677408][-4.8358817 -4.84875 -4.7449641 -4.5640316 -4.3420115 -4.1509218 -4.0619016 -4.0515194 -4.1224027 -4.2438803 -4.341382 -4.431356 -4.5551705 -4.6394324 -4.6480908][-4.8394713 -4.8315234 -4.6988668 -4.475204 -4.1932368 -3.9419382 -3.812283 -3.8066256 -3.9256713 -4.0910211 -4.2159238 -4.3383226 -4.49553 -4.6039658 -4.6184831][-4.8151426 -4.8038845 -4.6636753 -4.4121203 -4.070312 -3.7438529 -3.560257 -3.576488 -3.7696779 -4.00105 -4.1661057 -4.3159647 -4.479507 -4.5833006 -4.5856957][-4.7850642 -4.7919621 -4.674716 -4.428813 -4.0575151 -3.6588101 -3.4013426 -3.419857 -3.6751456 -3.97674 -4.1945219 -4.3695889 -4.5180779 -4.5939808 -4.5698557][-4.7753162 -4.8216305 -4.7551994 -4.5549312 -4.2123146 -3.79093 -3.4860113 -3.4875376 -3.7521472 -4.0715466 -4.3113394 -4.4842968 -4.5966945 -4.6353049 -4.5869937][-4.7978945 -4.8952789 -4.8946257 -4.7636585 -4.4919171 -4.1085968 -3.8122065 -3.7898049 -3.9945009 -4.2542672 -4.4641371 -4.6023521 -4.6670284 -4.6702685 -4.6083345][-4.8215771 -4.9560146 -5.0124164 -4.9481916 -4.756794 -4.4605522 -4.2237124 -4.1851048 -4.2975121 -4.4520211 -4.5887527 -4.675591 -4.7011838 -4.6861973 -4.6291008][-4.8056931 -4.9536533 -5.037591 -5.013515 -4.8892231 -4.7010756 -4.5569453 -4.5252423 -4.563004 -4.6214752 -4.6786537 -4.7106094 -4.7094913 -4.6946397 -4.6563034][-4.7386541 -4.8746715 -4.9599495 -4.9540834 -4.8786492 -4.7800694 -4.71666 -4.7041922 -4.7141156 -4.7264786 -4.7335687 -4.7291141 -4.7154164 -4.7048864 -4.6824803][-4.62894 -4.7397766 -4.8130064 -4.82058 -4.7839723 -4.7431941 -4.726325 -4.7306304 -4.7408509 -4.7435346 -4.7324471 -4.7133718 -4.6979918 -4.692729 -4.6825266]]...]
INFO - root - 2017-12-07 09:59:27.624796: step 6510, loss = 21.27, batch loss = 21.19 (9.0 examples/sec; 0.890 sec/batch; 80h:35m:34s remains)
INFO - root - 2017-12-07 09:59:36.928322: step 6520, loss = 21.32, batch loss = 21.24 (9.0 examples/sec; 0.886 sec/batch; 80h:12m:13s remains)
INFO - root - 2017-12-07 09:59:46.311259: step 6530, loss = 21.25, batch loss = 21.16 (8.8 examples/sec; 0.905 sec/batch; 81h:54m:26s remains)
INFO - root - 2017-12-07 09:59:55.512114: step 6540, loss = 21.52, batch loss = 21.43 (9.0 examples/sec; 0.892 sec/batch; 80h:47m:10s remains)
INFO - root - 2017-12-07 10:00:04.944078: step 6550, loss = 21.85, batch loss = 21.77 (9.3 examples/sec; 0.861 sec/batch; 77h:56m:08s remains)
INFO - root - 2017-12-07 10:00:14.501965: step 6560, loss = 21.57, batch loss = 21.48 (8.5 examples/sec; 0.940 sec/batch; 85h:04m:03s remains)
INFO - root - 2017-12-07 10:00:23.906399: step 6570, loss = 21.95, batch loss = 21.87 (8.5 examples/sec; 0.937 sec/batch; 84h:51m:35s remains)
INFO - root - 2017-12-07 10:00:33.167813: step 6580, loss = 21.56, batch loss = 21.48 (8.9 examples/sec; 0.897 sec/batch; 81h:12m:19s remains)
INFO - root - 2017-12-07 10:00:42.490909: step 6590, loss = 21.33, batch loss = 21.25 (8.8 examples/sec; 0.908 sec/batch; 82h:13m:28s remains)
INFO - root - 2017-12-07 10:00:51.921579: step 6600, loss = 21.61, batch loss = 21.53 (8.0 examples/sec; 0.998 sec/batch; 90h:19m:56s remains)
2017-12-07 10:00:52.909021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4508767 -4.53897 -4.5721588 -4.5472431 -4.48652 -4.3933692 -4.2975116 -4.2330623 -4.2261267 -4.2368345 -4.26387 -4.3062429 -4.3300023 -4.334693 -4.290679][-4.4587283 -4.5343494 -4.5392365 -4.4879045 -4.4116812 -4.3154883 -4.2348356 -4.1956863 -4.2041621 -4.2137275 -4.2275081 -4.2638588 -4.2922335 -4.2941494 -4.2492952][-4.4609871 -4.5177388 -4.4952712 -4.4191837 -4.331593 -4.2445531 -4.1864057 -4.178987 -4.2065663 -4.2243705 -4.2349091 -4.2567062 -4.276413 -4.2746067 -4.2385173][-4.4580412 -4.4942746 -4.4423718 -4.3394752 -4.2444639 -4.1771183 -4.1500506 -4.1686397 -4.2095528 -4.2406058 -4.2598495 -4.2724757 -4.2850819 -4.290319 -4.2772336][-4.4508977 -4.4709439 -4.3969088 -4.2741008 -4.1744556 -4.1238308 -4.1182661 -4.1456113 -4.1855316 -4.2248969 -4.2583413 -4.2702994 -4.2812891 -4.3076396 -4.335834][-4.4446077 -4.4555392 -4.3743639 -4.2400823 -4.1256089 -4.0637112 -4.0496988 -4.0707536 -4.1070929 -4.1537032 -4.1979666 -4.2065511 -4.2135825 -4.2577553 -4.3246369][-4.4460678 -4.4556446 -4.3777084 -4.2345009 -4.09011 -3.9905305 -3.9447768 -3.9583871 -4.0005865 -4.0604224 -4.1127129 -4.1157422 -4.1176248 -4.1646023 -4.2456179][-4.4610648 -4.478096 -4.4103246 -4.2652545 -4.0962172 -3.9573789 -3.8796914 -3.891784 -3.945745 -4.0168324 -4.0722804 -4.0668354 -4.0603004 -4.0920124 -4.1556706][-4.4852095 -4.5174623 -4.4727383 -4.3491297 -4.1853533 -4.0298829 -3.9337778 -3.9403353 -3.9952652 -4.0680141 -4.1193213 -4.1057239 -4.0838027 -4.0868316 -4.1148176][-4.5029459 -4.5571165 -4.543129 -4.4548011 -4.3209519 -4.1720715 -4.0694227 -4.0592327 -4.0932064 -4.1506829 -4.1933103 -4.175251 -4.138617 -4.1178703 -4.1196218][-4.5038924 -4.5818658 -4.598474 -4.5459356 -4.4475546 -4.3171196 -4.2164392 -4.1827784 -4.1832504 -4.2106266 -4.2336273 -4.2129345 -4.176405 -4.1564889 -4.1607666][-4.49349 -4.587069 -4.6283431 -4.6089082 -4.5453768 -4.4389887 -4.3458767 -4.2964845 -4.2746081 -4.2758741 -4.274106 -4.2443681 -4.2126431 -4.2059617 -4.2271194][-4.4691744 -4.5620489 -4.6172295 -4.6268487 -4.60064 -4.5289831 -4.4554987 -4.40785 -4.3838248 -4.3764329 -4.3578582 -4.3206892 -4.29808 -4.3105268 -4.3483834][-4.4377394 -4.5191154 -4.5786433 -4.6089592 -4.6171002 -4.5883236 -4.547133 -4.5128489 -4.4940581 -4.4866061 -4.462832 -4.4245214 -4.4077706 -4.4284253 -4.472537][-4.4047794 -4.4695497 -4.5228519 -4.5583525 -4.5851545 -4.5858493 -4.5676575 -4.5443764 -4.5292726 -4.5263095 -4.5130219 -4.4883356 -4.4826956 -4.508944 -4.5540423]]...]
INFO - root - 2017-12-07 10:01:02.194050: step 6610, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.932 sec/batch; 84h:22m:18s remains)
INFO - root - 2017-12-07 10:01:11.522444: step 6620, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.946 sec/batch; 85h:40m:08s remains)
INFO - root - 2017-12-07 10:01:20.890636: step 6630, loss = 21.38, batch loss = 21.30 (8.9 examples/sec; 0.900 sec/batch; 81h:27m:49s remains)
INFO - root - 2017-12-07 10:01:30.306918: step 6640, loss = 21.22, batch loss = 21.13 (8.2 examples/sec; 0.976 sec/batch; 88h:22m:32s remains)
INFO - root - 2017-12-07 10:01:39.453876: step 6650, loss = 21.63, batch loss = 21.54 (8.4 examples/sec; 0.950 sec/batch; 85h:57m:24s remains)
INFO - root - 2017-12-07 10:01:48.791438: step 6660, loss = 21.11, batch loss = 21.03 (9.4 examples/sec; 0.853 sec/batch; 77h:13m:22s remains)
INFO - root - 2017-12-07 10:01:57.967102: step 6670, loss = 21.40, batch loss = 21.31 (8.3 examples/sec; 0.965 sec/batch; 87h:20m:05s remains)
INFO - root - 2017-12-07 10:02:07.358847: step 6680, loss = 21.62, batch loss = 21.54 (8.4 examples/sec; 0.950 sec/batch; 86h:01m:31s remains)
INFO - root - 2017-12-07 10:02:16.831019: step 6690, loss = 21.53, batch loss = 21.45 (8.5 examples/sec; 0.945 sec/batch; 85h:32m:56s remains)
INFO - root - 2017-12-07 10:02:26.221892: step 6700, loss = 21.64, batch loss = 21.56 (8.7 examples/sec; 0.922 sec/batch; 83h:27m:14s remains)
2017-12-07 10:02:27.147414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3583975 -4.348382 -4.3878074 -4.4708209 -4.5460763 -4.559094 -4.5080051 -4.4666634 -4.5144773 -4.6045847 -4.6692243 -4.6857224 -4.642529 -4.5520587 -4.4608321][-4.2855368 -4.287065 -4.355278 -4.4578872 -4.5366368 -4.5269051 -4.4427719 -4.3956213 -4.4880862 -4.6474814 -4.7645411 -4.7972593 -4.7340412 -4.6029811 -4.4741893][-4.2132812 -4.2456527 -4.3516889 -4.4557676 -4.5036297 -4.4431086 -4.3151093 -4.259068 -4.390501 -4.6179094 -4.794836 -4.8571558 -4.7946076 -4.6472182 -4.4920464][-4.1769323 -4.2389851 -4.3727622 -4.4655232 -4.468935 -4.3530612 -4.179646 -4.0989447 -4.2376642 -4.506197 -4.7399364 -4.8521795 -4.8232379 -4.6920161 -4.5283184][-4.1926923 -4.2644305 -4.3855839 -4.4400454 -4.3981781 -4.245945 -4.0401144 -3.930392 -4.0493417 -4.3141465 -4.5801325 -4.7483549 -4.7774267 -4.6948471 -4.5525613][-4.244679 -4.3072972 -4.3764353 -4.3667865 -4.2772579 -4.0944781 -3.8590865 -3.7359223 -3.8533072 -4.104722 -4.3779597 -4.584465 -4.6600318 -4.6306086 -4.5327287][-4.3022075 -4.3336153 -4.3378844 -4.265636 -4.1404047 -3.9340494 -3.6707029 -3.5587769 -3.7164059 -3.9767332 -4.250824 -4.4709 -4.5583286 -4.5482936 -4.4787164][-4.3834238 -4.3707004 -4.3341537 -4.2363024 -4.1073542 -3.9108753 -3.6475883 -3.5538726 -3.7393997 -3.9991004 -4.2624645 -4.4726968 -4.5385442 -4.5013657 -4.4180703][-4.4653363 -4.4173565 -4.3743625 -4.2923536 -4.1889172 -4.0270739 -3.7880511 -3.6962578 -3.8690352 -4.1111984 -4.3659978 -4.5657458 -4.6071663 -4.5190053 -4.3864584][-4.464221 -4.3974586 -4.375453 -4.3407593 -4.2825046 -4.1589303 -3.9489527 -3.8550081 -3.9997804 -4.2280765 -4.473371 -4.6563058 -4.6830521 -4.5622654 -4.3884358][-4.3987827 -4.316009 -4.3213325 -4.3516884 -4.3616128 -4.2890882 -4.1182146 -4.0229354 -4.1283541 -4.3364496 -4.5587974 -4.7058363 -4.711803 -4.5802178 -4.4006362][-4.3591337 -4.2711334 -4.2880316 -4.3597569 -4.4237456 -4.4049621 -4.2841797 -4.1931329 -4.25189 -4.4233623 -4.608954 -4.7129035 -4.6963744 -4.5708294 -4.4131393][-4.38093 -4.3068 -4.3196917 -4.3870215 -4.45906 -4.47249 -4.4064937 -4.3366952 -4.3577738 -4.4750371 -4.607029 -4.6704273 -4.6414747 -4.5361824 -4.4139743][-4.4352784 -4.3896646 -4.3993092 -4.4388671 -4.4782548 -4.488472 -4.4605255 -4.4246607 -4.4316764 -4.4946885 -4.5682058 -4.5975881 -4.5679045 -4.4894929 -4.4011731][-4.4551425 -4.4361467 -4.4445758 -4.4616861 -4.4711156 -4.4691 -4.4587283 -4.4476395 -4.4510565 -4.4751344 -4.5037365 -4.5099735 -4.4842844 -4.4299617 -4.3697805]]...]
INFO - root - 2017-12-07 10:02:36.464813: step 6710, loss = 21.26, batch loss = 21.17 (8.2 examples/sec; 0.981 sec/batch; 88h:49m:15s remains)
INFO - root - 2017-12-07 10:02:45.799620: step 6720, loss = 21.07, batch loss = 20.99 (8.1 examples/sec; 0.987 sec/batch; 89h:20m:35s remains)
INFO - root - 2017-12-07 10:02:55.222638: step 6730, loss = 21.27, batch loss = 21.19 (8.0 examples/sec; 0.995 sec/batch; 90h:03m:18s remains)
INFO - root - 2017-12-07 10:03:04.739045: step 6740, loss = 21.08, batch loss = 20.99 (8.0 examples/sec; 0.998 sec/batch; 90h:17m:56s remains)
INFO - root - 2017-12-07 10:03:13.973281: step 6750, loss = 21.55, batch loss = 21.47 (8.2 examples/sec; 0.981 sec/batch; 88h:46m:06s remains)
INFO - root - 2017-12-07 10:03:23.374663: step 6760, loss = 21.49, batch loss = 21.41 (8.5 examples/sec; 0.940 sec/batch; 85h:03m:13s remains)
INFO - root - 2017-12-07 10:03:32.787098: step 6770, loss = 21.42, batch loss = 21.34 (8.8 examples/sec; 0.912 sec/batch; 82h:32m:20s remains)
INFO - root - 2017-12-07 10:03:42.249017: step 6780, loss = 21.64, batch loss = 21.56 (8.7 examples/sec; 0.924 sec/batch; 83h:38m:12s remains)
INFO - root - 2017-12-07 10:03:51.658400: step 6790, loss = 21.61, batch loss = 21.53 (8.5 examples/sec; 0.941 sec/batch; 85h:09m:17s remains)
INFO - root - 2017-12-07 10:04:01.035561: step 6800, loss = 21.33, batch loss = 21.24 (8.2 examples/sec; 0.979 sec/batch; 88h:32m:40s remains)
2017-12-07 10:04:01.964380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4396267 -4.2721162 -4.1773276 -4.2723165 -4.4378996 -4.4756246 -4.3391519 -4.1852431 -4.1499438 -4.2257657 -4.3062434 -4.3190031 -4.2993665 -4.3331394 -4.4076061][-4.3672657 -4.20349 -4.1192913 -4.2387333 -4.4354763 -4.4956961 -4.3682117 -4.2136521 -4.1800504 -4.2603736 -4.3482895 -4.36549 -4.3367414 -4.3366404 -4.3755856][-4.3189077 -4.18273 -4.1223278 -4.2544465 -4.4555116 -4.5177875 -4.3941464 -4.24292 -4.2143488 -4.3007164 -4.3899832 -4.3938174 -4.3393331 -4.3046856 -4.3212614][-4.307086 -4.1945276 -4.1544824 -4.2850432 -4.4657378 -4.5035214 -4.3658671 -4.2095003 -4.185246 -4.2794952 -4.3762536 -4.3754811 -4.303834 -4.2545004 -4.2789326][-4.3301339 -4.2268786 -4.1916986 -4.3090534 -4.4527769 -4.4462876 -4.2769284 -4.1032548 -4.0706005 -4.1744337 -4.3022728 -4.3330793 -4.2756453 -4.2387681 -4.2897263][-4.3376284 -4.2371593 -4.1993904 -4.2982383 -4.3979616 -4.3385091 -4.125299 -3.9207668 -3.8742614 -4.0045805 -4.1994991 -4.3058591 -4.2908845 -4.2732797 -4.341332][-4.3421874 -4.2517653 -4.2108097 -4.2824802 -4.3356733 -4.2288556 -3.9761207 -3.7444901 -3.7023177 -3.8788149 -4.14741 -4.3237143 -4.346457 -4.337574 -4.4017725][-4.3803291 -4.3085737 -4.2668123 -4.3033886 -4.3125505 -4.1823549 -3.9274874 -3.7088966 -3.6961007 -3.9077744 -4.2004023 -4.3893766 -4.4201555 -4.4090452 -4.45498][-4.42604 -4.3786659 -4.3419557 -4.3485575 -4.3296552 -4.2097454 -4.0006204 -3.8315272 -3.8402719 -4.0335035 -4.2821164 -4.4371743 -4.4640985 -4.4551849 -4.481535][-4.466301 -4.4350319 -4.4020019 -4.3872242 -4.3538122 -4.2565827 -4.10615 -3.9868274 -3.9929512 -4.1307282 -4.3109722 -4.4310822 -4.4672117 -4.4717813 -4.4847021][-4.5019879 -4.4766116 -4.445118 -4.4194536 -4.3818173 -4.308032 -4.2066455 -4.12421 -4.1184921 -4.2017441 -4.3238487 -4.4226589 -4.47242 -4.4878821 -4.4929848][-4.5169578 -4.5012846 -4.4830217 -4.4637203 -4.4297214 -4.3722763 -4.3062453 -4.2566605 -4.2493563 -4.2955675 -4.3769021 -4.4561706 -4.4993153 -4.5008664 -4.4866428][-4.5127654 -4.5128069 -4.5107245 -4.4942222 -4.4573421 -4.4132357 -4.3828888 -4.3737483 -4.3758979 -4.39498 -4.4427857 -4.4976034 -4.5134268 -4.4789734 -4.42971][-4.4642158 -4.472259 -4.4834781 -4.4698415 -4.434411 -4.410151 -4.4133205 -4.4306264 -4.426621 -4.4140878 -4.4343038 -4.4723287 -4.4688926 -4.4100204 -4.3355575][-4.3598752 -4.364408 -4.3835464 -4.3766913 -4.3527532 -4.3487649 -4.3677483 -4.3804708 -4.3523545 -4.3109212 -4.3192096 -4.3633528 -4.3698564 -4.3162913 -4.2402034]]...]
INFO - root - 2017-12-07 10:04:11.345562: step 6810, loss = 21.27, batch loss = 21.19 (8.5 examples/sec; 0.936 sec/batch; 84h:43m:12s remains)
INFO - root - 2017-12-07 10:04:20.840700: step 6820, loss = 21.48, batch loss = 21.40 (8.6 examples/sec; 0.935 sec/batch; 84h:33m:40s remains)
INFO - root - 2017-12-07 10:04:30.197482: step 6830, loss = 21.22, batch loss = 21.14 (9.0 examples/sec; 0.891 sec/batch; 80h:35m:37s remains)
INFO - root - 2017-12-07 10:04:39.654853: step 6840, loss = 21.71, batch loss = 21.63 (8.5 examples/sec; 0.938 sec/batch; 84h:49m:49s remains)
INFO - root - 2017-12-07 10:04:49.112530: step 6850, loss = 21.42, batch loss = 21.34 (8.4 examples/sec; 0.956 sec/batch; 86h:27m:51s remains)
INFO - root - 2017-12-07 10:04:58.419690: step 6860, loss = 21.42, batch loss = 21.34 (8.4 examples/sec; 0.947 sec/batch; 85h:38m:44s remains)
INFO - root - 2017-12-07 10:05:07.796007: step 6870, loss = 21.34, batch loss = 21.26 (8.6 examples/sec; 0.936 sec/batch; 84h:37m:18s remains)
INFO - root - 2017-12-07 10:05:17.114556: step 6880, loss = 21.27, batch loss = 21.18 (8.8 examples/sec; 0.908 sec/batch; 82h:10m:10s remains)
INFO - root - 2017-12-07 10:05:26.492970: step 6890, loss = 21.15, batch loss = 21.07 (8.3 examples/sec; 0.963 sec/batch; 87h:03m:53s remains)
INFO - root - 2017-12-07 10:05:35.844312: step 6900, loss = 21.21, batch loss = 21.13 (8.7 examples/sec; 0.924 sec/batch; 83h:34m:07s remains)
2017-12-07 10:05:36.804324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4124513 -4.4199991 -4.4271946 -4.4414506 -4.4205513 -4.389379 -4.3880157 -4.4155383 -4.4669652 -4.5216422 -4.56771 -4.5874243 -4.5412946 -4.4243283 -4.3272176][-4.4755158 -4.4876952 -4.5104327 -4.53584 -4.4917717 -4.4229088 -4.396565 -4.4155421 -4.4872332 -4.5705824 -4.6344771 -4.6463385 -4.5631766 -4.3955173 -4.2885714][-4.5075846 -4.5297384 -4.5754538 -4.6124353 -4.54534 -4.4461842 -4.4006963 -4.4065962 -4.4966097 -4.605813 -4.6757236 -4.6660137 -4.5339203 -4.3164554 -4.2046032][-4.5144534 -4.5400715 -4.6001821 -4.6367841 -4.5478463 -4.4351335 -4.3832774 -4.3780527 -4.4755263 -4.5957427 -4.6640024 -4.6388621 -4.4729991 -4.2321048 -4.1208639][-4.5022168 -4.5259161 -4.5835762 -4.6006866 -4.487855 -4.3647957 -4.3020997 -4.2835345 -4.3851027 -4.5155878 -4.593328 -4.5761371 -4.4153938 -4.1910634 -4.0887442][-4.4753804 -4.4886088 -4.5209303 -4.5008636 -4.3627205 -4.2225018 -4.130271 -4.0945549 -4.2130804 -4.3745718 -4.4924865 -4.5228529 -4.41226 -4.2424369 -4.1511993][-4.4442616 -4.4395118 -4.4304447 -4.3546476 -4.1719923 -3.978909 -3.8170733 -3.7646308 -3.9274626 -4.1595039 -4.3585863 -4.4711986 -4.4411297 -4.3429708 -4.2734027][-4.4363251 -4.4354272 -4.4036803 -4.274899 -4.031848 -3.7622328 -3.5217462 -3.4689953 -3.6959426 -4.0113149 -4.2953343 -4.4780588 -4.5048528 -4.4428029 -4.3869066][-4.4464097 -4.471477 -4.4491749 -4.302402 -4.035275 -3.7439685 -3.4948888 -3.4718859 -3.7298188 -4.0628238 -4.3602562 -4.5439949 -4.5573783 -4.4758153 -4.421752][-4.4506259 -4.5022688 -4.5056524 -4.3715396 -4.122479 -3.875649 -3.677999 -3.68395 -3.9156809 -4.1887655 -4.4293504 -4.56297 -4.5257854 -4.4026513 -4.3506441][-4.45336 -4.521203 -4.551414 -4.4422708 -4.2306771 -4.0526776 -3.9149208 -3.9258971 -4.1014638 -4.2762518 -4.428082 -4.4986081 -4.4212604 -4.2834139 -4.2485023][-4.4723639 -4.5458908 -4.5917354 -4.5063457 -4.331686 -4.2121072 -4.1246195 -4.1482062 -4.2808104 -4.3669634 -4.4275103 -4.4321289 -4.330894 -4.206768 -4.1955643][-4.5000982 -4.578 -4.6327796 -4.5702477 -4.4267578 -4.3341775 -4.269639 -4.3063345 -4.4111104 -4.4419613 -4.4446764 -4.412889 -4.3176103 -4.2258935 -4.22741][-4.51929 -4.6041989 -4.6696658 -4.6419439 -4.5463572 -4.4794173 -4.4333076 -4.4707117 -4.540926 -4.5374289 -4.5143414 -4.4791 -4.4115658 -4.3487659 -4.3398733][-4.5151267 -4.603888 -4.6707749 -4.6760497 -4.6411505 -4.6192274 -4.6098881 -4.6513281 -4.6932197 -4.6762114 -4.6525245 -4.6257076 -4.5772815 -4.5208344 -4.4866071]]...]
INFO - root - 2017-12-07 10:05:46.207685: step 6910, loss = 21.50, batch loss = 21.42 (9.0 examples/sec; 0.892 sec/batch; 80h:38m:22s remains)
INFO - root - 2017-12-07 10:05:55.511128: step 6920, loss = 21.53, batch loss = 21.44 (8.9 examples/sec; 0.898 sec/batch; 81h:14m:15s remains)
INFO - root - 2017-12-07 10:06:04.935837: step 6930, loss = 21.42, batch loss = 21.34 (8.3 examples/sec; 0.967 sec/batch; 87h:28m:15s remains)
INFO - root - 2017-12-07 10:06:14.414290: step 6940, loss = 21.37, batch loss = 21.28 (8.3 examples/sec; 0.963 sec/batch; 87h:05m:36s remains)
INFO - root - 2017-12-07 10:06:23.814807: step 6950, loss = 21.86, batch loss = 21.78 (8.6 examples/sec; 0.926 sec/batch; 83h:44m:58s remains)
INFO - root - 2017-12-07 10:06:33.262878: step 6960, loss = 21.69, batch loss = 21.60 (8.2 examples/sec; 0.976 sec/batch; 88h:13m:32s remains)
INFO - root - 2017-12-07 10:06:42.543708: step 6970, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.937 sec/batch; 84h:41m:44s remains)
INFO - root - 2017-12-07 10:06:51.999452: step 6980, loss = 21.68, batch loss = 21.60 (8.4 examples/sec; 0.956 sec/batch; 86h:24m:09s remains)
INFO - root - 2017-12-07 10:07:01.316634: step 6990, loss = 21.05, batch loss = 20.97 (8.4 examples/sec; 0.956 sec/batch; 86h:24m:21s remains)
INFO - root - 2017-12-07 10:07:10.717132: step 7000, loss = 21.23, batch loss = 21.15 (8.1 examples/sec; 0.993 sec/batch; 89h:44m:55s remains)
2017-12-07 10:07:11.708464: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5382404 -4.573319 -4.600678 -4.6040354 -4.59491 -4.5946507 -4.6027093 -4.5921769 -4.5431304 -4.480587 -4.464427 -4.5338211 -4.6456022 -4.7226887 -4.7378082][-4.5132003 -4.56657 -4.6147981 -4.624342 -4.612246 -4.6125312 -4.6252408 -4.6045027 -4.5267777 -4.4397426 -4.4257374 -4.5236073 -4.6627817 -4.7404695 -4.7398005][-4.4933529 -4.571002 -4.6370711 -4.6402936 -4.6067157 -4.5882325 -4.5897985 -4.5528746 -4.4615364 -4.3805852 -4.3938723 -4.5205522 -4.6684527 -4.7301803 -4.7101173][-4.478878 -4.5671439 -4.6317911 -4.6135507 -4.5496964 -4.5033517 -4.4803553 -4.4230132 -4.3322315 -4.2848377 -4.3460927 -4.4964428 -4.6345825 -4.6754107 -4.6491661][-4.4326243 -4.5014048 -4.5395 -4.4953513 -4.4140029 -4.3529835 -4.3030462 -4.2139983 -4.1196542 -4.1115732 -4.2240219 -4.396091 -4.5287704 -4.5718503 -4.5696688][-4.3393803 -4.3617945 -4.3612275 -4.2968478 -4.21567 -4.1551666 -4.0713787 -3.932622 -3.8218729 -3.8515325 -4.0186987 -4.2281165 -4.386528 -4.4654446 -4.5051856][-4.2281122 -4.218895 -4.1930227 -4.1219268 -4.049263 -3.9917762 -3.8702891 -3.6771908 -3.5509243 -3.6162221 -3.8308215 -4.0837641 -4.2898579 -4.4186339 -4.4886708][-4.1801934 -4.1702514 -4.1429 -4.0796251 -4.0196152 -3.9651082 -3.8221059 -3.6085494 -3.4964948 -3.5910978 -3.8143952 -4.0675583 -4.2875938 -4.4322972 -4.4989834][-4.2333612 -4.2351389 -4.2164192 -4.1723919 -4.1319175 -4.0863314 -3.9528666 -3.7699788 -3.7046306 -3.818718 -4.0084658 -4.2002478 -4.3697791 -4.4784245 -4.5136957][-4.3664384 -4.3694277 -4.3457518 -4.3145418 -4.2948704 -4.2670703 -4.1634274 -4.0314851 -4.0106993 -4.1251907 -4.2625875 -4.3751025 -4.4739833 -4.5336995 -4.538136][-4.5123715 -4.5056233 -4.4690957 -4.4415269 -4.4418697 -4.4375272 -4.3705287 -4.2830229 -4.2814875 -4.3738227 -4.4560857 -4.5040421 -4.5518489 -4.5828786 -4.5740972][-4.5924854 -4.5861597 -4.5503483 -4.5324244 -4.5518904 -4.5665808 -4.5280175 -4.4688115 -4.4646773 -4.51921 -4.5501924 -4.5554576 -4.5787911 -4.6026783 -4.5945611][-4.5654688 -4.5720406 -4.5546975 -4.5543556 -4.584785 -4.6075568 -4.589633 -4.5522518 -4.5421023 -4.5627427 -4.5630207 -4.5534372 -4.5698662 -4.5909 -4.5808177][-4.4526544 -4.4696302 -4.472085 -4.4853692 -4.5142431 -4.5347209 -4.5318255 -4.5148778 -4.5049706 -4.5066528 -4.4994392 -4.4946671 -4.5115862 -4.5273986 -4.5143514][-4.3323669 -4.3477511 -4.3575897 -4.3714948 -4.3891296 -4.4008641 -4.4039283 -4.4004416 -4.3949957 -4.3914437 -4.3880181 -4.3927326 -4.4113221 -4.4245162 -4.4164572]]...]
INFO - root - 2017-12-07 10:07:21.132204: step 7010, loss = 21.65, batch loss = 21.57 (8.2 examples/sec; 0.970 sec/batch; 87h:43m:24s remains)
INFO - root - 2017-12-07 10:07:30.558798: step 7020, loss = 21.40, batch loss = 21.32 (8.7 examples/sec; 0.918 sec/batch; 82h:58m:01s remains)
INFO - root - 2017-12-07 10:07:39.997116: step 7030, loss = 21.35, batch loss = 21.27 (8.7 examples/sec; 0.922 sec/batch; 83h:20m:16s remains)
INFO - root - 2017-12-07 10:07:49.381053: step 7040, loss = 21.48, batch loss = 21.40 (8.5 examples/sec; 0.945 sec/batch; 85h:26m:13s remains)
INFO - root - 2017-12-07 10:07:58.844870: step 7050, loss = 21.36, batch loss = 21.28 (8.2 examples/sec; 0.974 sec/batch; 88h:03m:25s remains)
INFO - root - 2017-12-07 10:08:08.296879: step 7060, loss = 21.35, batch loss = 21.26 (8.5 examples/sec; 0.943 sec/batch; 85h:12m:49s remains)
INFO - root - 2017-12-07 10:08:17.634919: step 7070, loss = 21.57, batch loss = 21.48 (8.5 examples/sec; 0.943 sec/batch; 85h:13m:59s remains)
INFO - root - 2017-12-07 10:08:25.744321: step 7080, loss = 21.68, batch loss = 21.60 (11.5 examples/sec; 0.696 sec/batch; 62h:56m:25s remains)
INFO - root - 2017-12-07 10:08:32.891695: step 7090, loss = 21.46, batch loss = 21.37 (11.8 examples/sec; 0.680 sec/batch; 61h:25m:24s remains)
INFO - root - 2017-12-07 10:08:40.030672: step 7100, loss = 22.06, batch loss = 21.98 (11.3 examples/sec; 0.707 sec/batch; 63h:55m:09s remains)
2017-12-07 10:08:40.724220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3911223 -4.3630433 -4.3305545 -4.29329 -4.2868457 -4.3063464 -4.3220968 -4.3087192 -4.2842579 -4.2971106 -4.3319974 -4.3281207 -4.278307 -4.22764 -4.2260065][-4.3093839 -4.3249745 -4.3096933 -4.2558045 -4.2289486 -4.258255 -4.3035936 -4.3187637 -4.3192067 -4.350522 -4.3841672 -4.3668714 -4.3065085 -4.2495728 -4.2365556][-4.2270403 -4.2937689 -4.3042903 -4.2420378 -4.1851149 -4.1921806 -4.2303915 -4.2559962 -4.2978859 -4.3817463 -4.4353738 -4.4032731 -4.3243322 -4.2557597 -4.2368121][-4.1615849 -4.252409 -4.2829232 -4.2297659 -4.154552 -4.1178913 -4.1058497 -4.1095119 -4.1918569 -4.3528004 -4.4567928 -4.4277978 -4.3295507 -4.243228 -4.2175913][-4.1390071 -4.2125182 -4.248065 -4.2165861 -4.1372523 -4.0461621 -3.953557 -3.9086118 -4.0217309 -4.2695279 -4.4490981 -4.446342 -4.3386378 -4.2367053 -4.200757][-4.1875286 -4.2328711 -4.2611074 -4.2394161 -4.14603 -3.994149 -3.8181214 -3.7212887 -3.8489602 -4.1590943 -4.4036689 -4.4333668 -4.3275352 -4.2269812 -4.1906614][-4.3021932 -4.3341007 -4.3546772 -4.3246207 -4.2013769 -3.9881299 -3.7421327 -3.5981574 -3.7126479 -4.03245 -4.3031974 -4.361619 -4.27325 -4.1934705 -4.1785393][-4.4170833 -4.4632626 -4.4966178 -4.4640441 -4.3191113 -4.0708566 -3.7921889 -3.6176145 -3.6849504 -3.9473662 -4.1867352 -4.2515135 -4.1887503 -4.1437817 -4.1654873][-4.4283228 -4.513052 -4.589128 -4.5853257 -4.4598188 -4.2376909 -3.9951835 -3.8326604 -3.8395264 -3.9822268 -4.1176734 -4.1401978 -4.086473 -4.0714951 -4.1274781][-4.3142548 -4.4366136 -4.5627279 -4.6047788 -4.5257425 -4.368063 -4.2103715 -4.1089845 -4.0881724 -4.1155887 -4.1272845 -4.0840096 -4.024735 -4.0243726 -4.0938816][-4.1689463 -4.303978 -4.451303 -4.521029 -4.485486 -4.3902631 -4.3109221 -4.2778387 -4.2719502 -4.2495379 -4.1862488 -4.0995641 -4.0381365 -4.0437174 -4.1099272][-4.0731215 -4.1887212 -4.3341808 -4.41602 -4.4093251 -4.3504686 -4.3012328 -4.2971549 -4.3217039 -4.3161335 -4.25803 -4.1788163 -4.1280212 -4.1267395 -4.1672063][-4.0601726 -4.139533 -4.2784581 -4.3670583 -4.373909 -4.3271832 -4.2706003 -4.2542272 -4.2870607 -4.3119526 -4.2960815 -4.2584267 -4.2305908 -4.220511 -4.2244468][-4.1319342 -4.1744804 -4.2906852 -4.3729334 -4.3806939 -4.333631 -4.2661386 -4.2310157 -4.247241 -4.2681532 -4.2660694 -4.2544165 -4.246727 -4.2364736 -4.2194948][-4.2203951 -4.2223024 -4.30426 -4.3878703 -4.41323 -4.3748007 -4.3046112 -4.25877 -4.2489505 -4.2337561 -4.2061567 -4.1923656 -4.1978407 -4.1940455 -4.1689806]]...]
INFO - root - 2017-12-07 10:08:47.802556: step 7110, loss = 21.53, batch loss = 21.44 (12.6 examples/sec; 0.634 sec/batch; 57h:19m:34s remains)
INFO - root - 2017-12-07 10:08:55.002145: step 7120, loss = 21.18, batch loss = 21.10 (11.5 examples/sec; 0.696 sec/batch; 62h:53m:40s remains)
INFO - root - 2017-12-07 10:09:02.213864: step 7130, loss = 21.59, batch loss = 21.51 (11.1 examples/sec; 0.721 sec/batch; 65h:11m:50s remains)
INFO - root - 2017-12-07 10:09:09.427508: step 7140, loss = 21.57, batch loss = 21.48 (10.3 examples/sec; 0.773 sec/batch; 69h:52m:58s remains)
INFO - root - 2017-12-07 10:09:16.520491: step 7150, loss = 21.32, batch loss = 21.24 (10.2 examples/sec; 0.788 sec/batch; 71h:10m:40s remains)
INFO - root - 2017-12-07 10:09:23.426899: step 7160, loss = 21.62, batch loss = 21.54 (11.7 examples/sec; 0.682 sec/batch; 61h:36m:36s remains)
INFO - root - 2017-12-07 10:09:30.516397: step 7170, loss = 21.44, batch loss = 21.36 (11.9 examples/sec; 0.673 sec/batch; 60h:46m:55s remains)
INFO - root - 2017-12-07 10:09:37.682467: step 7180, loss = 21.45, batch loss = 21.37 (11.2 examples/sec; 0.715 sec/batch; 64h:36m:40s remains)
INFO - root - 2017-12-07 10:09:44.795562: step 7190, loss = 21.38, batch loss = 21.30 (11.2 examples/sec; 0.715 sec/batch; 64h:34m:28s remains)
INFO - root - 2017-12-07 10:09:51.896956: step 7200, loss = 21.50, batch loss = 21.42 (11.5 examples/sec; 0.698 sec/batch; 63h:05m:33s remains)
2017-12-07 10:09:52.621498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.530622 -4.5760827 -4.5998964 -4.6060176 -4.6077118 -4.6114411 -4.6100612 -4.5994239 -4.5906439 -4.5917597 -4.5968256 -4.5953197 -4.585361 -4.5640197 -4.5274448][-4.6191521 -4.6791086 -4.7056727 -4.7085352 -4.7029076 -4.698741 -4.6888628 -4.6722445 -4.6717591 -4.6954579 -4.7194409 -4.7237024 -4.7037926 -4.6617413 -4.60331][-4.66626 -4.7288647 -4.7422018 -4.7225256 -4.6856923 -4.6517062 -4.624383 -4.605679 -4.6234746 -4.6835747 -4.7418976 -4.7660909 -4.7461872 -4.6915913 -4.6237226][-4.6746016 -4.7299342 -4.7191448 -4.6607695 -4.5733061 -4.4932055 -4.4473381 -4.435122 -4.4757762 -4.5708985 -4.6613784 -4.7134342 -4.7040119 -4.6471577 -4.5814948][-4.6637807 -4.7002106 -4.6534414 -4.54397 -4.3983722 -4.2688546 -4.2108688 -4.221323 -4.2890172 -4.4081154 -4.5203137 -4.600987 -4.6145244 -4.5681643 -4.5103579][-4.6213408 -4.6156149 -4.5183907 -4.3557887 -4.1606808 -3.9861634 -3.917603 -3.9535844 -4.0458841 -4.1752281 -4.2970839 -4.4068832 -4.4654884 -4.4594021 -4.4253683][-4.5688014 -4.5200729 -4.3895645 -4.2049789 -3.9942682 -3.7962871 -3.7137606 -3.7589271 -3.8556659 -3.9768248 -4.0957332 -4.2231512 -4.3269215 -4.3701706 -4.3642573][-4.5439487 -4.4834371 -4.361156 -4.1983 -4.006886 -3.8108473 -3.7217736 -3.7601779 -3.8390892 -3.9325061 -4.03712 -4.1616654 -4.2826929 -4.3522968 -4.3609228][-4.5661016 -4.529243 -4.44676 -4.32527 -4.15921 -3.9767351 -3.9003463 -3.9381087 -3.9905944 -4.0461245 -4.1259217 -4.2302327 -4.3378606 -4.4039268 -4.4127154][-4.6108723 -4.6126738 -4.5755825 -4.4945183 -4.3483086 -4.1866503 -4.1412463 -4.1925659 -4.2241769 -4.2436419 -4.2995119 -4.3811731 -4.4630933 -4.5092134 -4.5041003][-4.631454 -4.6576819 -4.6434493 -4.5855951 -4.461987 -4.3359165 -4.3325105 -4.4037528 -4.4242945 -4.4221926 -4.46374 -4.5324321 -4.5927706 -4.6166773 -4.5928783][-4.6012244 -4.6341066 -4.6245461 -4.57975 -4.4876938 -4.4133687 -4.4475369 -4.5240912 -4.5293746 -4.5125408 -4.5473695 -4.616426 -4.6693521 -4.6802788 -4.6438069][-4.531981 -4.5650992 -4.5615945 -4.53482 -4.4837012 -4.4600906 -4.51063 -4.5734653 -4.5674033 -4.5441532 -4.5729518 -4.6372232 -4.6802626 -4.6763473 -4.6274481][-4.4542494 -4.479835 -4.4841642 -4.4760838 -4.45658 -4.4570341 -4.496819 -4.5353813 -4.5264316 -4.5052481 -4.5251441 -4.573874 -4.6018558 -4.5877442 -4.5390725][-4.3900104 -4.404511 -4.4101067 -4.4108133 -4.4057927 -4.4082026 -4.4265094 -4.4420156 -4.4349742 -4.4219456 -4.4324102 -4.4612932 -4.4765153 -4.4640222 -4.4315696]]...]
INFO - root - 2017-12-07 10:09:59.845008: step 7210, loss = 21.67, batch loss = 21.59 (11.2 examples/sec; 0.713 sec/batch; 64h:27m:23s remains)
INFO - root - 2017-12-07 10:10:07.095912: step 7220, loss = 21.80, batch loss = 21.71 (10.9 examples/sec; 0.736 sec/batch; 66h:32m:38s remains)
INFO - root - 2017-12-07 10:10:14.201375: step 7230, loss = 21.75, batch loss = 21.67 (10.5 examples/sec; 0.762 sec/batch; 68h:52m:48s remains)
INFO - root - 2017-12-07 10:10:21.336520: step 7240, loss = 21.27, batch loss = 21.19 (10.1 examples/sec; 0.789 sec/batch; 71h:18m:19s remains)
INFO - root - 2017-12-07 10:10:28.464644: step 7250, loss = 21.47, batch loss = 21.39 (10.4 examples/sec; 0.770 sec/batch; 69h:31m:41s remains)
INFO - root - 2017-12-07 10:10:35.607926: step 7260, loss = 21.38, batch loss = 21.30 (10.7 examples/sec; 0.750 sec/batch; 67h:45m:47s remains)
INFO - root - 2017-12-07 10:10:42.471567: step 7270, loss = 21.68, batch loss = 21.60 (11.1 examples/sec; 0.718 sec/batch; 64h:54m:11s remains)
INFO - root - 2017-12-07 10:10:49.546902: step 7280, loss = 21.64, batch loss = 21.56 (11.3 examples/sec; 0.709 sec/batch; 64h:04m:45s remains)
INFO - root - 2017-12-07 10:10:56.589569: step 7290, loss = 21.36, batch loss = 21.28 (11.4 examples/sec; 0.704 sec/batch; 63h:36m:36s remains)
INFO - root - 2017-12-07 10:11:03.704455: step 7300, loss = 21.42, batch loss = 21.33 (11.1 examples/sec; 0.722 sec/batch; 65h:11m:55s remains)
2017-12-07 10:11:04.442703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3551517 -4.3604136 -4.3820419 -4.4095678 -4.4261432 -4.4179363 -4.3661742 -4.266222 -4.1758709 -4.1589894 -4.202354 -4.2793665 -4.3520889 -4.3842292 -4.3737164][-4.3131576 -4.3132167 -4.3284459 -4.3544383 -4.3818717 -4.380393 -4.3165283 -4.1909113 -4.085886 -4.0699692 -4.1255984 -4.2186651 -4.3031826 -4.3403778 -4.3341208][-4.2466187 -4.254673 -4.2808414 -4.3244367 -4.3644476 -4.3549809 -4.2620392 -4.1164989 -4.0207992 -4.0238762 -4.0891757 -4.1748071 -4.2303362 -4.2364635 -4.2208257][-4.2003007 -4.2297578 -4.2785497 -4.3422289 -4.3852816 -4.3583741 -4.2352343 -4.0794592 -4.0039306 -4.0329161 -4.1053672 -4.1651254 -4.168643 -4.1325526 -4.1115155][-4.1541305 -4.2034025 -4.2699842 -4.3362055 -4.3706689 -4.3385086 -4.2118812 -4.0589743 -3.9889247 -4.0267639 -4.107656 -4.1510811 -4.1280179 -4.0895195 -4.0879264][-4.0959935 -4.1580181 -4.2215495 -4.2634091 -4.2716146 -4.2386246 -4.1386518 -4.0161772 -3.9528403 -3.9864488 -4.0677419 -4.107121 -4.0952177 -4.0943527 -4.1282454][-4.0650997 -4.1302042 -4.1674724 -4.1655536 -4.13682 -4.1001825 -4.0368457 -3.9627309 -3.925139 -3.9654453 -4.04655 -4.085175 -4.098022 -4.1373177 -4.1900263][-4.0895362 -4.1509666 -4.1577635 -4.1180668 -4.0643268 -4.0259719 -3.9899187 -3.9511 -3.9316726 -3.9736106 -4.0453877 -4.0771961 -4.1057563 -4.16716 -4.2182856][-4.1563044 -4.2190495 -4.20596 -4.1487112 -4.09548 -4.0660591 -4.04253 -4.0104485 -3.9833603 -4.0062318 -4.0561671 -4.0751472 -4.1023169 -4.1629429 -4.2008972][-4.2109776 -4.2847481 -4.2680035 -4.2126217 -4.1761236 -4.1564097 -4.1296592 -4.09023 -4.0548835 -4.0664358 -4.1039195 -4.1137381 -4.1272106 -4.1648374 -4.1752372][-4.2325044 -4.3204989 -4.3111596 -4.2712541 -4.2517409 -4.2320571 -4.1926937 -4.140398 -4.1050568 -4.1247935 -4.1701889 -4.1837964 -4.1848717 -4.1886683 -4.1620417][-4.2153931 -4.3163733 -4.3447165 -4.3433852 -4.3361955 -4.3009272 -4.2359657 -4.1637769 -4.1257029 -4.1504622 -4.2044139 -4.2233377 -4.2189889 -4.1989131 -4.1497631][-4.1473789 -4.270174 -4.3611741 -4.4123082 -4.4119754 -4.3528342 -4.2565756 -4.1635032 -4.1167164 -4.132226 -4.1765084 -4.1935625 -4.1937532 -4.1749644 -4.1370559][-4.1006923 -4.2382722 -4.3731003 -4.4595947 -4.4627862 -4.3814821 -4.2610054 -4.1555562 -4.0975347 -4.0965581 -4.1198206 -4.1283035 -4.1378336 -4.1321726 -4.1284761][-4.1253014 -4.2523251 -4.3866806 -4.4835577 -4.4909105 -4.3937039 -4.261333 -4.1586037 -4.0998034 -4.09514 -4.1081386 -4.1056585 -4.1147704 -4.109406 -4.1292086]]...]
INFO - root - 2017-12-07 10:11:11.529453: step 7310, loss = 21.90, batch loss = 21.82 (10.8 examples/sec; 0.741 sec/batch; 66h:54m:00s remains)
INFO - root - 2017-12-07 10:11:18.579724: step 7320, loss = 21.65, batch loss = 21.57 (11.4 examples/sec; 0.699 sec/batch; 63h:08m:33s remains)
INFO - root - 2017-12-07 10:11:25.733319: step 7330, loss = 21.22, batch loss = 21.13 (11.4 examples/sec; 0.699 sec/batch; 63h:07m:07s remains)
INFO - root - 2017-12-07 10:11:32.937519: step 7340, loss = 21.42, batch loss = 21.34 (10.9 examples/sec; 0.731 sec/batch; 66h:01m:46s remains)
INFO - root - 2017-12-07 10:11:39.975596: step 7350, loss = 21.78, batch loss = 21.70 (11.4 examples/sec; 0.705 sec/batch; 63h:37m:50s remains)
INFO - root - 2017-12-07 10:11:46.985996: step 7360, loss = 21.64, batch loss = 21.56 (12.2 examples/sec; 0.658 sec/batch; 59h:24m:54s remains)
INFO - root - 2017-12-07 10:11:53.964034: step 7370, loss = 21.43, batch loss = 21.34 (12.4 examples/sec; 0.644 sec/batch; 58h:11m:02s remains)
INFO - root - 2017-12-07 10:12:00.824117: step 7380, loss = 21.36, batch loss = 21.27 (11.8 examples/sec; 0.677 sec/batch; 61h:07m:23s remains)
INFO - root - 2017-12-07 10:12:07.741703: step 7390, loss = 22.02, batch loss = 21.93 (12.0 examples/sec; 0.665 sec/batch; 60h:02m:39s remains)
INFO - root - 2017-12-07 10:12:14.638379: step 7400, loss = 21.54, batch loss = 21.46 (11.7 examples/sec; 0.686 sec/batch; 61h:58m:43s remains)
2017-12-07 10:12:15.554932: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.328732 -4.4575348 -4.5525575 -4.5503774 -4.5054736 -4.4569616 -4.4127984 -4.3481979 -4.2948852 -4.35104 -4.4594722 -4.5422549 -4.57259 -4.5909824 -4.6204858][-4.3460407 -4.4952083 -4.6003771 -4.5943146 -4.5399752 -4.4979324 -4.4808006 -4.4335828 -4.3790789 -4.4190421 -4.5128036 -4.5949335 -4.6290841 -4.6456642 -4.674643][-4.3585258 -4.5188622 -4.6199756 -4.5973883 -4.5306144 -4.5066614 -4.5289164 -4.5066376 -4.4486237 -4.451623 -4.5066032 -4.5773 -4.61965 -4.6427145 -4.6650839][-4.3679147 -4.5277286 -4.6109114 -4.557085 -4.4664173 -4.4531341 -4.5097046 -4.516511 -4.464241 -4.4263172 -4.4241443 -4.4665852 -4.5179887 -4.5693622 -4.6059017][-4.3733263 -4.5253239 -4.5858707 -4.497005 -4.3710051 -4.3433032 -4.406466 -4.43232 -4.3900185 -4.3148084 -4.2514472 -4.2651334 -4.3411727 -4.4605904 -4.5593896][-4.3746357 -4.5195165 -4.5663638 -4.4549885 -4.2908216 -4.2156487 -4.2419009 -4.2614679 -4.2252107 -4.1280489 -4.0238943 -4.0238881 -4.1329441 -4.3352361 -4.527606][-4.3721733 -4.5146828 -4.5565324 -4.4319921 -4.2251921 -4.068944 -4.0117345 -3.9955149 -3.9645209 -3.8765521 -3.7838695 -3.8047011 -3.9472477 -4.2240195 -4.5147791][-4.3651719 -4.5070229 -4.54964 -4.4231448 -4.1893482 -3.9587002 -3.8131738 -3.7526402 -3.7223616 -3.663738 -3.6276469 -3.7017827 -3.8786397 -4.2056103 -4.5605235][-4.3576221 -4.5017939 -4.5573158 -4.451839 -4.2302904 -3.9692955 -3.7657659 -3.6655285 -3.6247621 -3.6092596 -3.6615095 -3.8027873 -3.9986043 -4.3222766 -4.6646729][-4.3542747 -4.5091882 -4.5923586 -4.5293555 -4.3530693 -4.1039009 -3.876982 -3.7505889 -3.694025 -3.7196689 -3.8549042 -4.0351257 -4.2103925 -4.4756107 -4.7467713][-4.3579483 -4.5264635 -4.6435475 -4.6345453 -4.526401 -4.3277974 -4.1173816 -3.9794703 -3.8953645 -3.9311016 -4.1027818 -4.2804179 -4.4104133 -4.5848007 -4.755][-4.3624187 -4.542851 -4.6902261 -4.7320042 -4.6922903 -4.5546126 -4.374464 -4.2326312 -4.128202 -4.1636405 -4.336442 -4.4903946 -4.5741653 -4.6529288 -4.7117963][-4.3619151 -4.5427794 -4.7005873 -4.7657471 -4.7638636 -4.6722422 -4.5263252 -4.398334 -4.3047452 -4.3455248 -4.4957533 -4.6133666 -4.6575532 -4.66566 -4.6419072][-4.3472481 -4.5118117 -4.6595669 -4.7248268 -4.7361774 -4.6795263 -4.575243 -4.4748263 -4.4057727 -4.4485087 -4.56203 -4.6392722 -4.6559629 -4.6298351 -4.5745549][-4.3143592 -4.4529815 -4.58818 -4.6534176 -4.6693416 -4.6299477 -4.545465 -4.4509058 -4.390213 -4.4298615 -4.5141683 -4.564219 -4.5710664 -4.5492539 -4.5142903]]...]
INFO - root - 2017-12-07 10:12:22.501391: step 7410, loss = 21.63, batch loss = 21.55 (10.9 examples/sec; 0.732 sec/batch; 66h:08m:34s remains)
INFO - root - 2017-12-07 10:12:29.602516: step 7420, loss = 21.39, batch loss = 21.30 (11.4 examples/sec; 0.704 sec/batch; 63h:33m:15s remains)
INFO - root - 2017-12-07 10:12:36.487329: step 7430, loss = 21.75, batch loss = 21.67 (11.4 examples/sec; 0.702 sec/batch; 63h:21m:04s remains)
INFO - root - 2017-12-07 10:12:43.394688: step 7440, loss = 21.19, batch loss = 21.10 (11.7 examples/sec; 0.681 sec/batch; 61h:30m:07s remains)
INFO - root - 2017-12-07 10:12:50.272670: step 7450, loss = 21.49, batch loss = 21.41 (11.3 examples/sec; 0.705 sec/batch; 63h:41m:51s remains)
INFO - root - 2017-12-07 10:12:57.220296: step 7460, loss = 21.39, batch loss = 21.30 (11.8 examples/sec; 0.680 sec/batch; 61h:24m:13s remains)
INFO - root - 2017-12-07 10:13:04.274497: step 7470, loss = 21.26, batch loss = 21.17 (12.4 examples/sec; 0.647 sec/batch; 58h:22m:43s remains)
INFO - root - 2017-12-07 10:13:11.191359: step 7480, loss = 21.60, batch loss = 21.51 (11.7 examples/sec; 0.685 sec/batch; 61h:52m:08s remains)
INFO - root - 2017-12-07 10:13:18.151215: step 7490, loss = 21.24, batch loss = 21.16 (12.1 examples/sec; 0.659 sec/batch; 59h:31m:09s remains)
INFO - root - 2017-12-07 10:13:25.045144: step 7500, loss = 21.39, batch loss = 21.30 (12.2 examples/sec; 0.658 sec/batch; 59h:26m:22s remains)
2017-12-07 10:13:25.746913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3716254 -4.3868251 -4.4216352 -4.4812765 -4.530973 -4.5294681 -4.4999657 -4.4830174 -4.4924769 -4.5116138 -4.5191107 -4.513824 -4.4855876 -4.4411469 -4.4109745][-4.4160924 -4.443027 -4.4874067 -4.5533786 -4.5953817 -4.575747 -4.5321188 -4.5099998 -4.5208254 -4.5425005 -4.5443316 -4.5283837 -4.4852791 -4.4230065 -4.3861647][-4.4520535 -4.4946465 -4.5461211 -4.6090188 -4.6327076 -4.5918388 -4.5364928 -4.5097246 -4.5198755 -4.5467567 -4.5499339 -4.5260086 -4.4641 -4.3764634 -4.3263574][-4.4729972 -4.5211439 -4.5706658 -4.6186433 -4.613636 -4.5453715 -4.4718933 -4.4354997 -4.4496908 -4.495163 -4.5248427 -4.5175624 -4.44941 -4.3403063 -4.2712173][-4.4507289 -4.4941907 -4.5326824 -4.5518422 -4.5051355 -4.4021463 -4.3070664 -4.2687035 -4.3069119 -4.3877664 -4.4545026 -4.4761271 -4.4195528 -4.3068123 -4.2289152][-4.409421 -4.4415269 -4.4566946 -4.4326177 -4.3347921 -4.18852 -4.0604534 -4.0184574 -4.0927496 -4.2244053 -4.3392372 -4.3976135 -4.3642588 -4.2664137 -4.1995411][-4.3957891 -4.4013424 -4.3811522 -4.3187575 -4.1838479 -3.9984269 -3.8238149 -3.7525141 -3.844347 -4.0285645 -4.2046781 -4.3081527 -4.3032546 -4.2300539 -4.1865997][-4.4088545 -4.376677 -4.3269343 -4.2548866 -4.1223297 -3.9297404 -3.7220197 -3.6032529 -3.6753263 -3.8767486 -4.0870137 -4.2144909 -4.2257681 -4.1732616 -4.1571441][-4.4363203 -4.3640175 -4.2933245 -4.2274194 -4.121563 -3.9653046 -3.7796288 -3.645288 -3.6762469 -3.8425128 -4.0311942 -4.1447659 -4.1481338 -4.0914626 -4.0761571][-4.4752131 -4.3669305 -4.2774172 -4.2172251 -4.1483812 -4.0595479 -3.9395394 -3.8308365 -3.8298125 -3.9381285 -4.07764 -4.1689796 -4.1658626 -4.09448 -4.0541129][-4.4911 -4.3723783 -4.2807412 -4.23298 -4.20206 -4.1774387 -4.1243458 -4.055954 -4.0385346 -4.0898681 -4.1728916 -4.245265 -4.253377 -4.1926718 -4.1483545][-4.4707437 -4.3648515 -4.2886972 -4.2586145 -4.2565236 -4.2710176 -4.2620511 -4.2336206 -4.2216306 -4.2366109 -4.2713017 -4.3166456 -4.3334661 -4.3013396 -4.2810907][-4.4671841 -4.3901253 -4.3382998 -4.3271236 -4.3456335 -4.37562 -4.3858151 -4.3811517 -4.3796015 -4.3817945 -4.391468 -4.4158478 -4.4346786 -4.4271026 -4.4302082][-4.4829369 -4.4428387 -4.4229684 -4.432034 -4.4639516 -4.5013995 -4.5204391 -4.5177345 -4.5062866 -4.4951134 -4.4937735 -4.5090909 -4.5264268 -4.5315356 -4.5443506][-4.4992995 -4.4825053 -4.479804 -4.4921823 -4.519032 -4.5537515 -4.5762038 -4.5710368 -4.5498943 -4.5336428 -4.53184 -4.5439768 -4.5564094 -4.5615005 -4.5702333]]...]
INFO - root - 2017-12-07 10:13:32.632248: step 7510, loss = 21.20, batch loss = 21.11 (12.2 examples/sec; 0.654 sec/batch; 59h:00m:25s remains)
INFO - root - 2017-12-07 10:13:39.599572: step 7520, loss = 21.68, batch loss = 21.60 (12.3 examples/sec; 0.652 sec/batch; 58h:48m:56s remains)
INFO - root - 2017-12-07 10:13:46.554481: step 7530, loss = 21.32, batch loss = 21.23 (12.4 examples/sec; 0.643 sec/batch; 58h:00m:50s remains)
INFO - root - 2017-12-07 10:13:53.468512: step 7540, loss = 21.63, batch loss = 21.55 (12.1 examples/sec; 0.660 sec/batch; 59h:34m:40s remains)
INFO - root - 2017-12-07 10:14:00.384991: step 7550, loss = 21.91, batch loss = 21.83 (12.2 examples/sec; 0.658 sec/batch; 59h:23m:21s remains)
INFO - root - 2017-12-07 10:14:07.425622: step 7560, loss = 21.56, batch loss = 21.48 (11.3 examples/sec; 0.705 sec/batch; 63h:37m:31s remains)
INFO - root - 2017-12-07 10:14:17.204310: step 7570, loss = 21.85, batch loss = 21.77 (10.1 examples/sec; 0.794 sec/batch; 71h:37m:16s remains)
INFO - root - 2017-12-07 10:14:26.024270: step 7580, loss = 21.51, batch loss = 21.42 (8.4 examples/sec; 0.949 sec/batch; 85h:41m:24s remains)
INFO - root - 2017-12-07 10:14:35.098988: step 7590, loss = 21.50, batch loss = 21.42 (8.8 examples/sec; 0.911 sec/batch; 82h:11m:56s remains)
INFO - root - 2017-12-07 10:14:44.302411: step 7600, loss = 21.31, batch loss = 21.23 (8.0 examples/sec; 1.002 sec/batch; 90h:24m:21s remains)
2017-12-07 10:14:45.282235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4680114 -4.4320607 -4.3801589 -4.3437 -4.3263712 -4.3545609 -4.4282761 -4.4870491 -4.4982843 -4.4932027 -4.5410357 -4.6433539 -4.6950979 -4.623508 -4.4795227][-4.4851975 -4.4358625 -4.3698483 -4.3180637 -4.2964845 -4.3225746 -4.3761454 -4.39695 -4.365222 -4.3384976 -4.4125686 -4.563416 -4.6421037 -4.570271 -4.4163394][-4.5481706 -4.5205193 -4.45668 -4.3711743 -4.2893734 -4.2494969 -4.2503614 -4.2544532 -4.22331 -4.2034311 -4.3053837 -4.4884591 -4.5851169 -4.5217252 -4.3732033][-4.6276217 -4.6373491 -4.5771151 -4.4529634 -4.295032 -4.1692057 -4.11871 -4.1386156 -4.1480289 -4.1497307 -4.2536259 -4.4281721 -4.5259314 -4.4879656 -4.370647][-4.6612144 -4.6861854 -4.6141524 -4.4619122 -4.2648087 -4.0960951 -4.0327482 -4.0932736 -4.1555772 -4.1681719 -4.235651 -4.3629136 -4.4552755 -4.4618683 -4.4012909][-4.634738 -4.6385751 -4.5307775 -4.3584042 -4.1600308 -3.9975221 -3.9588666 -4.0655723 -4.1690426 -4.1810474 -4.2059126 -4.285953 -4.3819118 -4.4427543 -4.4437175][-4.6203151 -4.5933704 -4.4323497 -4.2213988 -4.0052147 -3.8377633 -3.8161457 -3.9482384 -4.0695062 -4.0873213 -4.1081429 -4.1882787 -4.3116078 -4.4289432 -4.4825435][-4.6640425 -4.624258 -4.4286017 -4.1811872 -3.9200523 -3.7011528 -3.6540642 -3.7803631 -3.900332 -3.9395852 -4.0052695 -4.1285419 -4.289187 -4.4438372 -4.522933][-4.7345781 -4.7190795 -4.5386181 -4.2866406 -3.9807253 -3.6876469 -3.5889425 -3.6922421 -3.8032355 -3.87471 -4.0046611 -4.1762576 -4.3502612 -4.4968586 -4.5572538][-4.7760053 -4.8037405 -4.681282 -4.4652891 -4.1504097 -3.8173513 -3.6867754 -3.7679312 -3.8626916 -3.9513803 -4.1207638 -4.3176317 -4.4799013 -4.5850348 -4.5971804][-4.7614584 -4.8183136 -4.7645483 -4.6154385 -4.3435793 -4.0337658 -3.9127936 -3.9863358 -4.0670581 -4.1417527 -4.2959867 -4.4787283 -4.6163206 -4.6819568 -4.6533432][-4.7196593 -4.7821732 -4.7730179 -4.6884556 -4.4875073 -4.2498541 -4.1748986 -4.259491 -4.3319712 -4.36963 -4.4589791 -4.5864363 -4.6881146 -4.7279162 -4.68356][-4.6667066 -4.717505 -4.7126918 -4.6585989 -4.5214844 -4.3679008 -4.3524594 -4.4565749 -4.5272837 -4.5360913 -4.5624757 -4.6255822 -4.6846194 -4.7047849 -4.6618104][-4.5866938 -4.6227345 -4.6030655 -4.5500422 -4.4507136 -4.3609695 -4.390501 -4.5051923 -4.579484 -4.5856037 -4.5834827 -4.60107 -4.6240253 -4.6290154 -4.5904937][-4.4836597 -4.5017023 -4.4669466 -4.4100771 -4.3345919 -4.284759 -4.334991 -4.4422073 -4.5103016 -4.5213647 -4.5149436 -4.5141454 -4.5202451 -4.5204468 -4.4907608]]...]
INFO - root - 2017-12-07 10:14:54.425821: step 7610, loss = 21.38, batch loss = 21.30 (9.0 examples/sec; 0.891 sec/batch; 80h:27m:16s remains)
INFO - root - 2017-12-07 10:15:03.520072: step 7620, loss = 21.55, batch loss = 21.47 (9.2 examples/sec; 0.865 sec/batch; 78h:03m:45s remains)
INFO - root - 2017-12-07 10:15:12.610211: step 7630, loss = 21.15, batch loss = 21.07 (9.0 examples/sec; 0.888 sec/batch; 80h:10m:35s remains)
INFO - root - 2017-12-07 10:15:21.692559: step 7640, loss = 21.12, batch loss = 21.04 (9.4 examples/sec; 0.854 sec/batch; 77h:05m:41s remains)
INFO - root - 2017-12-07 10:15:30.785932: step 7650, loss = 21.52, batch loss = 21.44 (8.9 examples/sec; 0.895 sec/batch; 80h:43m:32s remains)
INFO - root - 2017-12-07 10:15:39.981854: step 7660, loss = 21.26, batch loss = 21.18 (8.9 examples/sec; 0.897 sec/batch; 80h:54m:28s remains)
INFO - root - 2017-12-07 10:15:49.127219: step 7670, loss = 21.43, batch loss = 21.35 (8.7 examples/sec; 0.923 sec/batch; 83h:15m:40s remains)
INFO - root - 2017-12-07 10:15:58.139939: step 7680, loss = 21.47, batch loss = 21.39 (8.2 examples/sec; 0.974 sec/batch; 87h:50m:43s remains)
INFO - root - 2017-12-07 10:16:07.276166: step 7690, loss = 21.61, batch loss = 21.53 (8.1 examples/sec; 0.984 sec/batch; 88h:48m:23s remains)
INFO - root - 2017-12-07 10:16:16.490360: step 7700, loss = 21.94, batch loss = 21.85 (8.5 examples/sec; 0.944 sec/batch; 85h:12m:51s remains)
2017-12-07 10:16:17.337592: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.552074 -4.5603714 -4.5372443 -4.5098844 -4.4952288 -4.4984021 -4.5067148 -4.5007854 -4.4697661 -4.4277754 -4.3944778 -4.3981228 -4.4535985 -4.5310297 -4.5903955][-4.6417761 -4.6612115 -4.6440554 -4.6335111 -4.6400294 -4.65329 -4.6537652 -4.6245613 -4.5707083 -4.5162115 -4.4735894 -4.4699306 -4.5219917 -4.5979419 -4.6518421][-4.670301 -4.7065382 -4.6983371 -4.7005863 -4.7233853 -4.7428102 -4.7317562 -4.679255 -4.6086955 -4.5570889 -4.5239582 -4.5200748 -4.562192 -4.6224623 -4.65658][-4.6164985 -4.6772342 -4.6871047 -4.6935873 -4.7125044 -4.716886 -4.681056 -4.6046829 -4.5266628 -4.4922786 -4.48366 -4.4914541 -4.5305653 -4.5770159 -4.5932269][-4.4721451 -4.5607915 -4.6001086 -4.6058841 -4.5942068 -4.5479879 -4.4635797 -4.3636293 -4.2963243 -4.3000131 -4.331512 -4.3626943 -4.4137816 -4.4626145 -4.4792094][-4.3150964 -4.4235587 -4.4959559 -4.502758 -4.4449806 -4.3191891 -4.16177 -4.0339541 -3.9920382 -4.0525289 -4.1383667 -4.207407 -4.2869606 -4.3566122 -4.3856149][-4.2438035 -4.372879 -4.478797 -4.4914484 -4.3826122 -4.1594591 -3.9085352 -3.7360773 -3.7086928 -3.8260818 -3.9822025 -4.1127658 -4.2403088 -4.3443804 -4.3870358][-4.2495489 -4.3957558 -4.5266309 -4.5547338 -4.4191532 -4.1191239 -3.7858379 -3.5644078 -3.5331414 -3.6902616 -3.9124575 -4.1045012 -4.2766671 -4.412003 -4.4656062][-4.2927203 -4.4225941 -4.5601306 -4.6183696 -4.505878 -4.1993709 -3.8432174 -3.6036279 -3.5673387 -3.7396173 -3.9850204 -4.1867576 -4.3530955 -4.485353 -4.538702][-4.32531 -4.4125314 -4.5402842 -4.6363444 -4.5873137 -4.3476548 -4.0476151 -3.8433938 -3.8179569 -3.9740748 -4.175631 -4.3118429 -4.4171047 -4.5155253 -4.5617251][-4.3013558 -4.338614 -4.446785 -4.5708394 -4.59312 -4.4538336 -4.2498226 -4.1084957 -4.1018047 -4.2216287 -4.3462605 -4.39506 -4.4347506 -4.5044546 -4.5484762][-4.2184305 -4.2159696 -4.30308 -4.4369454 -4.5096097 -4.4629393 -4.3526411 -4.2685204 -4.2776203 -4.3634696 -4.4248519 -4.4183969 -4.4274168 -4.4854255 -4.5262761][-4.156754 -4.1289077 -4.1936207 -4.3104615 -4.3903651 -4.4010429 -4.3616042 -4.3244429 -4.3508692 -4.4215212 -4.4557819 -4.434864 -4.4351096 -4.4783769 -4.5008273][-4.1535864 -4.1083236 -4.1524539 -4.2281623 -4.2748194 -4.3030157 -4.3142624 -4.3240342 -4.3722777 -4.4426374 -4.4750218 -4.4592409 -4.4496293 -4.461988 -4.4532714][-4.1557026 -4.1003008 -4.1373444 -4.1829381 -4.1962085 -4.2273836 -4.274776 -4.320775 -4.378943 -4.4371066 -4.4636168 -4.4519691 -4.4294605 -4.4102297 -4.3797565]]...]
INFO - root - 2017-12-07 10:16:26.574027: step 7710, loss = 21.11, batch loss = 21.03 (8.7 examples/sec; 0.923 sec/batch; 83h:16m:05s remains)
INFO - root - 2017-12-07 10:16:35.748070: step 7720, loss = 21.51, batch loss = 21.43 (8.7 examples/sec; 0.916 sec/batch; 82h:36m:33s remains)
INFO - root - 2017-12-07 10:16:44.992924: step 7730, loss = 21.11, batch loss = 21.03 (8.9 examples/sec; 0.894 sec/batch; 80h:39m:52s remains)
INFO - root - 2017-12-07 10:16:54.162969: step 7740, loss = 21.66, batch loss = 21.57 (9.0 examples/sec; 0.890 sec/batch; 80h:16m:13s remains)
INFO - root - 2017-12-07 10:17:03.327332: step 7750, loss = 22.13, batch loss = 22.05 (8.8 examples/sec; 0.909 sec/batch; 81h:59m:57s remains)
INFO - root - 2017-12-07 10:17:12.779163: step 7760, loss = 21.69, batch loss = 21.61 (8.2 examples/sec; 0.975 sec/batch; 87h:58m:52s remains)
INFO - root - 2017-12-07 10:17:22.287867: step 7770, loss = 21.10, batch loss = 21.01 (7.9 examples/sec; 1.009 sec/batch; 90h:58m:18s remains)
INFO - root - 2017-12-07 10:17:31.707101: step 7780, loss = 21.48, batch loss = 21.39 (8.3 examples/sec; 0.959 sec/batch; 86h:29m:19s remains)
INFO - root - 2017-12-07 10:17:41.104361: step 7790, loss = 21.63, batch loss = 21.55 (8.8 examples/sec; 0.908 sec/batch; 81h:56m:02s remains)
INFO - root - 2017-12-07 10:17:50.455157: step 7800, loss = 21.23, batch loss = 21.15 (9.1 examples/sec; 0.875 sec/batch; 78h:57m:20s remains)
2017-12-07 10:17:51.369532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3666573 -4.3346915 -4.29882 -4.2895393 -4.3239288 -4.3870106 -4.4423776 -4.4753437 -4.5020819 -4.5300264 -4.5371771 -4.5194111 -4.4920721 -4.4660015 -4.4537859][-4.3561807 -4.28094 -4.1985369 -4.1665707 -4.2071481 -4.2998533 -4.3916063 -4.4461708 -4.4792247 -4.50093 -4.4950771 -4.4654584 -4.4335012 -4.4141188 -4.4206443][-4.3468966 -4.2138228 -4.0743985 -4.0105882 -4.0528188 -4.1741343 -4.3004937 -4.3718629 -4.4005322 -4.4059982 -4.3899021 -4.3640785 -4.3510389 -4.3606567 -4.4019413][-4.3254986 -4.1556845 -3.9861429 -3.9087148 -3.9563777 -4.0967183 -4.2289042 -4.2781787 -4.2694473 -4.2470503 -4.22909 -4.2251282 -4.2466917 -4.2869854 -4.35873][-4.2911963 -4.1233835 -3.9694045 -3.9102745 -3.9650466 -4.1006618 -4.2005935 -4.1922207 -4.12084 -4.0658164 -4.0545568 -4.0834236 -4.1402249 -4.1967459 -4.2766218][-4.2837796 -4.1380973 -4.0255218 -3.9997411 -4.0489388 -4.14147 -4.1803818 -4.1128311 -3.9990222 -3.933636 -3.9401464 -4.0008674 -4.0770731 -4.1258583 -4.1874256][-4.32873 -4.2156677 -4.1527877 -4.1577225 -4.187861 -4.217504 -4.191011 -4.0903091 -3.9766614 -3.9267015 -3.9483914 -4.0196724 -4.0944238 -4.1278067 -4.1663604][-4.4154625 -4.3446779 -4.3280158 -4.3516016 -4.3587437 -4.3389397 -4.2761736 -4.1749144 -4.0882397 -4.0617003 -4.0857267 -4.1396766 -4.19128 -4.207757 -4.2267747][-4.5003347 -4.4721985 -4.4874392 -4.5140529 -4.499289 -4.4539886 -4.3881345 -4.3106403 -4.2576585 -4.2521853 -4.2735929 -4.3006134 -4.3187246 -4.3144631 -4.313683][-4.5457168 -4.5461912 -4.5712523 -4.5871692 -4.5555005 -4.503592 -4.4532061 -4.4076681 -4.3855276 -4.3959622 -4.4170523 -4.4267116 -4.4223576 -4.4058633 -4.3928318][-4.5578866 -4.5679326 -4.583827 -4.582438 -4.5411611 -4.4941235 -4.463419 -4.444437 -4.442873 -4.4593229 -4.479372 -4.4843655 -4.4750175 -4.4601135 -4.4510832][-4.5468626 -4.5585632 -4.5641561 -4.5489879 -4.5055189 -4.4676476 -4.4520454 -4.4479313 -4.4516463 -4.4617405 -4.4746704 -4.4812202 -4.4823585 -4.485671 -4.4950213][-4.5094409 -4.5157185 -4.5118237 -4.4876318 -4.4468808 -4.4188342 -4.4137545 -4.4176431 -4.421597 -4.4243054 -4.4299011 -4.4371924 -4.4498267 -4.4728761 -4.5017219][-4.4427776 -4.4387517 -4.4284062 -4.4035954 -4.3708258 -4.3502088 -4.3481741 -4.3530045 -4.3559961 -4.3564873 -4.3592696 -4.3664846 -4.3832359 -4.4132771 -4.4491][-4.3659515 -4.35346 -4.3422027 -4.3252459 -4.3068757 -4.2964 -4.2957454 -4.2985377 -4.3004365 -4.3012776 -4.3034077 -4.308516 -4.3195648 -4.3396163 -4.3638468]]...]
INFO - root - 2017-12-07 10:18:00.714395: step 7810, loss = 21.20, batch loss = 21.12 (8.3 examples/sec; 0.962 sec/batch; 86h:48m:31s remains)
INFO - root - 2017-12-07 10:18:10.247745: step 7820, loss = 21.18, batch loss = 21.10 (8.5 examples/sec; 0.942 sec/batch; 84h:58m:10s remains)
INFO - root - 2017-12-07 10:18:19.811139: step 7830, loss = 21.48, batch loss = 21.40 (8.2 examples/sec; 0.978 sec/batch; 88h:12m:34s remains)
INFO - root - 2017-12-07 10:18:29.373526: step 7840, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.947 sec/batch; 85h:22m:56s remains)
INFO - root - 2017-12-07 10:18:38.922426: step 7850, loss = 21.56, batch loss = 21.48 (8.4 examples/sec; 0.957 sec/batch; 86h:18m:43s remains)
INFO - root - 2017-12-07 10:18:48.390310: step 7860, loss = 21.13, batch loss = 21.05 (8.9 examples/sec; 0.903 sec/batch; 81h:27m:54s remains)
INFO - root - 2017-12-07 10:18:57.970064: step 7870, loss = 21.42, batch loss = 21.34 (8.9 examples/sec; 0.896 sec/batch; 80h:49m:58s remains)
INFO - root - 2017-12-07 10:19:07.313306: step 7880, loss = 21.21, batch loss = 21.13 (8.7 examples/sec; 0.924 sec/batch; 83h:20m:11s remains)
INFO - root - 2017-12-07 10:19:16.742056: step 7890, loss = 21.56, batch loss = 21.48 (8.1 examples/sec; 0.985 sec/batch; 88h:47m:50s remains)
INFO - root - 2017-12-07 10:19:26.140317: step 7900, loss = 21.27, batch loss = 21.19 (8.6 examples/sec; 0.933 sec/batch; 84h:06m:48s remains)
2017-12-07 10:19:27.044870: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.47958 -4.3550034 -4.2830172 -4.3103838 -4.381887 -4.4324808 -4.46357 -4.4957671 -4.5319018 -4.5559726 -4.5622754 -4.5630946 -4.5609822 -4.5332327 -4.4953308][-4.4207544 -4.2475324 -4.146966 -4.183538 -4.2886 -4.36583 -4.4151282 -4.4622555 -4.5064039 -4.53578 -4.5519624 -4.5716925 -4.5844078 -4.5523286 -4.4934464][-4.3320413 -4.1433954 -4.0442514 -4.09686 -4.2277126 -4.3189878 -4.36629 -4.4021416 -4.428618 -4.4491587 -4.4765654 -4.524889 -4.5668406 -4.5446296 -4.4722533][-4.2295737 -4.0621886 -3.9923732 -4.0619507 -4.1980824 -4.2808075 -4.3019929 -4.30452 -4.299006 -4.3055725 -4.3442931 -4.4188089 -4.4911551 -4.4952068 -4.4338784][-4.15865 -4.0355968 -4.00394 -4.0796924 -4.1963739 -4.250299 -4.2329035 -4.195106 -4.1591649 -4.1575732 -4.208384 -4.2982082 -4.39055 -4.4261255 -4.397243][-4.1329165 -4.059536 -4.05821 -4.1242433 -4.2051387 -4.2252369 -4.1800885 -4.1165767 -4.0611444 -4.0558434 -4.1206346 -4.2265682 -4.3316474 -4.3857908 -4.3845177][-4.1319876 -4.0926018 -4.1049519 -4.1492505 -4.1897745 -4.1828761 -4.1314893 -4.0691328 -4.0123878 -4.0112348 -4.0924811 -4.2166948 -4.3275247 -4.37845 -4.3818154][-4.1126037 -4.0867805 -4.101377 -4.1300073 -4.14816 -4.1361456 -4.1039162 -4.0677853 -4.0263133 -4.0288167 -4.1089158 -4.2293568 -4.3206339 -4.3403649 -4.3245969][-4.0703897 -4.0366411 -4.0438623 -4.0686784 -4.0915885 -4.1038828 -4.1109939 -4.1118073 -4.0888476 -4.0796442 -4.1217237 -4.1975007 -4.2424479 -4.2210712 -4.1869149][-4.0441689 -3.9814572 -3.9699523 -3.9957941 -4.042007 -4.0939851 -4.1426325 -4.1736808 -4.1563525 -4.1135483 -4.0896707 -4.1020207 -4.1036091 -4.0586891 -4.0246215][-4.06761 -3.9640076 -3.9204643 -3.9435043 -4.0203934 -4.1179662 -4.2012429 -4.2451062 -4.210752 -4.1155524 -4.0227547 -3.9876978 -3.9735167 -3.9346516 -3.9234087][-4.1379724 -3.9943235 -3.913028 -3.9285614 -4.0355244 -4.1780734 -4.2885995 -4.332397 -4.2702894 -4.1243768 -3.98778 -3.9368868 -3.9298835 -3.9134836 -3.9312992][-4.2420392 -4.0755811 -3.9682388 -3.9771538 -4.0999403 -4.2666826 -4.3922291 -4.4344196 -4.356144 -4.1911359 -4.0535569 -4.0168033 -4.0258455 -4.0286832 -4.0575604][-4.356195 -4.197598 -4.0906844 -4.0991716 -4.2182107 -4.3772793 -4.4986258 -4.5406876 -4.4694643 -4.3222365 -4.212297 -4.1970034 -4.2144752 -4.221806 -4.2405291][-4.4572058 -4.334476 -4.2489815 -4.2575612 -4.3533821 -4.4785175 -4.5781255 -4.6146894 -4.5609369 -4.4485154 -4.3710618 -4.3686094 -4.388298 -4.3958898 -4.4022903]]...]
INFO - root - 2017-12-07 10:19:36.441030: step 7910, loss = 21.30, batch loss = 21.22 (8.5 examples/sec; 0.945 sec/batch; 85h:13m:57s remains)
INFO - root - 2017-12-07 10:19:45.760809: step 7920, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.951 sec/batch; 85h:42m:45s remains)
INFO - root - 2017-12-07 10:19:55.201202: step 7930, loss = 21.18, batch loss = 21.10 (7.9 examples/sec; 1.016 sec/batch; 91h:33m:48s remains)
INFO - root - 2017-12-07 10:20:04.608631: step 7940, loss = 21.64, batch loss = 21.56 (8.1 examples/sec; 0.992 sec/batch; 89h:24m:30s remains)
INFO - root - 2017-12-07 10:20:13.967985: step 7950, loss = 21.86, batch loss = 21.78 (8.6 examples/sec; 0.930 sec/batch; 83h:48m:54s remains)
INFO - root - 2017-12-07 10:20:23.479350: step 7960, loss = 21.94, batch loss = 21.85 (8.0 examples/sec; 1.003 sec/batch; 90h:23m:53s remains)
INFO - root - 2017-12-07 10:20:32.870837: step 7970, loss = 21.40, batch loss = 21.31 (8.3 examples/sec; 0.961 sec/batch; 86h:36m:50s remains)
INFO - root - 2017-12-07 10:20:42.291257: step 7980, loss = 21.51, batch loss = 21.42 (8.7 examples/sec; 0.921 sec/batch; 83h:02m:25s remains)
INFO - root - 2017-12-07 10:20:51.452589: step 7990, loss = 21.63, batch loss = 21.55 (8.8 examples/sec; 0.907 sec/batch; 81h:44m:06s remains)
INFO - root - 2017-12-07 10:21:00.819319: step 8000, loss = 21.29, batch loss = 21.20 (9.5 examples/sec; 0.842 sec/batch; 75h:52m:14s remains)
2017-12-07 10:21:01.799325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4240141 -4.3839722 -4.3534966 -4.3681645 -4.4260426 -4.4912367 -4.5298762 -4.5449266 -4.5548396 -4.5604582 -4.56068 -4.5728903 -4.5857434 -4.5724773 -4.536274][-4.4529104 -4.4064193 -4.3789177 -4.4097714 -4.4810514 -4.5403824 -4.5660324 -4.5748348 -4.5782437 -4.5731359 -4.5714331 -4.5914731 -4.6124616 -4.601337 -4.5596371][-4.4573889 -4.4112306 -4.3972964 -4.4344778 -4.4827781 -4.492867 -4.4729958 -4.4615488 -4.4657164 -4.4793992 -4.5082073 -4.5573816 -4.6035981 -4.6142635 -4.5845647][-4.4153275 -4.3837032 -4.3891478 -4.419271 -4.4238229 -4.3690481 -4.288322 -4.24504 -4.2597208 -4.3201275 -4.39998 -4.4815297 -4.5517173 -4.591836 -4.5883789][-4.3598747 -4.3553963 -4.380157 -4.3951993 -4.3519197 -4.2363191 -4.0940609 -4.0164723 -4.0514584 -4.1730895 -4.3059468 -4.4111433 -4.4947071 -4.5570388 -4.57728][-4.3117261 -4.3414173 -4.3863959 -4.3872557 -4.304841 -4.1454716 -3.9565918 -3.8489327 -3.8994057 -4.0650544 -4.2292719 -4.3474674 -4.4476242 -4.5322919 -4.570262][-4.2929664 -4.3549862 -4.4116974 -4.3880577 -4.261889 -4.0665693 -3.8468077 -3.7133934 -3.7665346 -3.95497 -4.1362095 -4.27418 -4.410697 -4.5296702 -4.5845366][-4.2975731 -4.3788242 -4.4332252 -4.383111 -4.2255473 -4.0201459 -3.8012536 -3.6557982 -3.6972523 -3.8819757 -4.0643187 -4.2190356 -4.3878765 -4.5354748 -4.6013842][-4.299253 -4.3801923 -4.4272923 -4.3705258 -4.2219076 -4.0515928 -3.8796155 -3.7535079 -3.7740898 -3.9190691 -4.07166 -4.21134 -4.3688574 -4.5062437 -4.5670319][-4.2985983 -4.3600368 -4.3902411 -4.3299165 -4.2030773 -4.0836949 -3.9863944 -3.9150963 -3.9285395 -4.0259366 -4.1357293 -4.23699 -4.3475752 -4.4436851 -4.4840412][-4.3008394 -4.3307767 -4.3282385 -4.2486873 -4.1374359 -4.07249 -4.0586109 -4.0577369 -4.0821767 -4.143218 -4.2124681 -4.2696896 -4.330739 -4.3840075 -4.39852][-4.3197069 -4.3195696 -4.2792435 -4.1742468 -4.0768266 -4.0646377 -4.1139345 -4.1581707 -4.1843085 -4.2119884 -4.2457619 -4.2701015 -4.3051276 -4.3377109 -4.3284245][-4.3675241 -4.3574591 -4.29828 -4.1816545 -4.0939636 -4.1122541 -4.1833758 -4.2300062 -4.2398424 -4.2326303 -4.2363296 -4.2447619 -4.28136 -4.318182 -4.2971487][-4.4325604 -4.4384866 -4.3951035 -4.2955775 -4.2221951 -4.2472453 -4.3032813 -4.3213077 -4.2980728 -4.2531548 -4.2274451 -4.2230377 -4.2646217 -4.3077 -4.2826271][-4.4803209 -4.51761 -4.5122323 -4.4515285 -4.4023681 -4.4229212 -4.4476151 -4.4288597 -4.37731 -4.3089051 -4.2599759 -4.2348914 -4.2591453 -4.2931485 -4.2678723]]...]
INFO - root - 2017-12-07 10:21:11.198765: step 8010, loss = 21.63, batch loss = 21.55 (8.3 examples/sec; 0.958 sec/batch; 86h:22m:51s remains)
INFO - root - 2017-12-07 10:21:20.727645: step 8020, loss = 21.24, batch loss = 21.15 (8.0 examples/sec; 1.006 sec/batch; 90h:37m:57s remains)
INFO - root - 2017-12-07 10:21:30.138141: step 8030, loss = 21.35, batch loss = 21.27 (8.9 examples/sec; 0.899 sec/batch; 81h:01m:24s remains)
INFO - root - 2017-12-07 10:21:39.441247: step 8040, loss = 21.27, batch loss = 21.18 (8.9 examples/sec; 0.901 sec/batch; 81h:12m:37s remains)
INFO - root - 2017-12-07 10:21:48.814980: step 8050, loss = 21.83, batch loss = 21.74 (8.5 examples/sec; 0.946 sec/batch; 85h:17m:43s remains)
INFO - root - 2017-12-07 10:21:58.179859: step 8060, loss = 21.36, batch loss = 21.28 (8.1 examples/sec; 0.984 sec/batch; 88h:42m:37s remains)
INFO - root - 2017-12-07 10:22:07.582163: step 8070, loss = 21.88, batch loss = 21.79 (7.7 examples/sec; 1.046 sec/batch; 94h:13m:18s remains)
INFO - root - 2017-12-07 10:22:17.076049: step 8080, loss = 21.90, batch loss = 21.82 (8.4 examples/sec; 0.956 sec/batch; 86h:11m:42s remains)
INFO - root - 2017-12-07 10:22:26.304551: step 8090, loss = 21.35, batch loss = 21.27 (8.4 examples/sec; 0.953 sec/batch; 85h:54m:05s remains)
INFO - root - 2017-12-07 10:22:35.798439: step 8100, loss = 21.70, batch loss = 21.61 (8.1 examples/sec; 0.983 sec/batch; 88h:32m:19s remains)
2017-12-07 10:22:36.675264: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3279438 -4.3372231 -4.3498373 -4.3599148 -4.3788018 -4.4041696 -4.4111552 -4.4008484 -4.389472 -4.3795581 -4.3743086 -4.3699074 -4.3592219 -4.3488073 -4.3362355][-4.4993229 -4.5006223 -4.4973125 -4.4922996 -4.50017 -4.5168815 -4.5183425 -4.5049605 -4.4944954 -4.4855371 -4.4806681 -4.4761639 -4.4625096 -4.4487605 -4.4399848][-4.5728092 -4.5716419 -4.5624595 -4.5486174 -4.5442543 -4.5474129 -4.5409055 -4.5248375 -4.5196934 -4.523972 -4.5360994 -4.5518394 -4.5555224 -4.5508189 -4.5523696][-4.4992642 -4.5064092 -4.5023189 -4.482749 -4.4591208 -4.4388919 -4.4187746 -4.4005103 -4.3993959 -4.4175696 -4.4542966 -4.5067253 -4.5477619 -4.5643249 -4.5796256][-4.3434458 -4.3694806 -4.37908 -4.3552012 -4.3036122 -4.2448273 -4.1997123 -4.1797552 -4.1834736 -4.2122607 -4.2725616 -4.3642173 -4.4454174 -4.4798508 -4.499011][-4.17352 -4.226562 -4.2633133 -4.2491045 -4.1748738 -4.069 -3.9838037 -3.9537554 -3.9580402 -3.9936254 -4.07673 -4.2045341 -4.3191872 -4.3669753 -4.3808327][-4.0134106 -4.0934086 -4.1650934 -4.1747041 -4.0959167 -3.952635 -3.8221424 -3.7756712 -3.77708 -3.8187385 -3.9238935 -4.0787811 -4.2151513 -4.2726693 -4.2763352][-3.9047036 -3.9945388 -4.0943384 -4.1347647 -4.0771966 -3.9290359 -3.7729554 -3.7122343 -3.7120938 -3.7604034 -3.8791935 -4.0407515 -4.1752467 -4.2331357 -4.2250824][-3.8767154 -3.9482465 -4.061048 -4.1348724 -4.1238666 -4.0152946 -3.8722816 -3.8087952 -3.8085511 -3.858757 -3.9699647 -4.1070747 -4.2094078 -4.2495928 -4.233191][-3.9312041 -3.968096 -4.0752215 -4.170444 -4.2078743 -4.1584816 -4.0554538 -4.0032344 -4.0036206 -4.0474644 -4.1294985 -4.2187734 -4.2762313 -4.2927542 -4.2747936][-4.040112 -4.0491319 -4.1378503 -4.2314553 -4.292614 -4.289185 -4.2310944 -4.2005796 -4.2087636 -4.2456312 -4.2957282 -4.3409758 -4.3648748 -4.3695855 -4.3570261][-4.1435347 -4.1477242 -4.2225695 -4.3007588 -4.3558807 -4.3672695 -4.3385386 -4.3305874 -4.3531852 -4.3922262 -4.4285564 -4.453517 -4.4653978 -4.4715014 -4.4670577][-4.2180529 -4.231319 -4.2985935 -4.3587608 -4.3942776 -4.4000325 -4.3832269 -4.3902011 -4.4215255 -4.4588933 -4.488193 -4.5054007 -4.5125051 -4.5181608 -4.5159979][-4.2664771 -4.289155 -4.3511281 -4.396441 -4.415154 -4.4125471 -4.4029732 -4.4181218 -4.4464593 -4.4694085 -4.483645 -4.4870152 -4.4786458 -4.4679356 -4.4543362][-4.3128905 -4.335567 -4.3866 -4.4186482 -4.4243636 -4.4144263 -4.4073453 -4.4223447 -4.43955 -4.4431009 -4.4355707 -4.4171734 -4.3826656 -4.344779 -4.313333]]...]
INFO - root - 2017-12-07 10:22:46.025868: step 8110, loss = 21.22, batch loss = 21.14 (9.1 examples/sec; 0.878 sec/batch; 79h:04m:49s remains)
INFO - root - 2017-12-07 10:22:55.512522: step 8120, loss = 21.36, batch loss = 21.27 (8.6 examples/sec; 0.931 sec/batch; 83h:54m:44s remains)
INFO - root - 2017-12-07 10:23:04.808286: step 8130, loss = 21.52, batch loss = 21.44 (8.2 examples/sec; 0.980 sec/batch; 88h:20m:16s remains)
INFO - root - 2017-12-07 10:23:14.193071: step 8140, loss = 20.99, batch loss = 20.91 (7.8 examples/sec; 1.021 sec/batch; 91h:58m:45s remains)
INFO - root - 2017-12-07 10:23:23.552140: step 8150, loss = 21.13, batch loss = 21.05 (8.8 examples/sec; 0.911 sec/batch; 82h:02m:48s remains)
INFO - root - 2017-12-07 10:23:32.999062: step 8160, loss = 21.42, batch loss = 21.34 (8.6 examples/sec; 0.934 sec/batch; 84h:09m:01s remains)
INFO - root - 2017-12-07 10:23:42.337902: step 8170, loss = 21.26, batch loss = 21.18 (8.4 examples/sec; 0.951 sec/batch; 85h:40m:03s remains)
INFO - root - 2017-12-07 10:23:51.708376: step 8180, loss = 21.22, batch loss = 21.13 (8.1 examples/sec; 0.984 sec/batch; 88h:37m:26s remains)
INFO - root - 2017-12-07 10:24:01.190519: step 8190, loss = 21.31, batch loss = 21.23 (8.5 examples/sec; 0.938 sec/batch; 84h:29m:43s remains)
INFO - root - 2017-12-07 10:24:10.405518: step 8200, loss = 21.16, batch loss = 21.08 (8.9 examples/sec; 0.894 sec/batch; 80h:33m:12s remains)
2017-12-07 10:24:11.367760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4517274 -4.4780421 -4.4873872 -4.5086112 -4.5551424 -4.5964985 -4.598053 -4.5650949 -4.5504847 -4.56421 -4.5627956 -4.5315695 -4.5069904 -4.520638 -4.5540404][-4.525661 -4.53199 -4.5170789 -4.5289583 -4.577127 -4.6190739 -4.61679 -4.5690556 -4.545073 -4.56721 -4.5771332 -4.5379777 -4.4983196 -4.5098438 -4.5531983][-4.5919251 -4.5696177 -4.5204372 -4.5011 -4.5207543 -4.5371351 -4.525804 -4.4783154 -4.4584012 -4.4910192 -4.5074239 -4.4556117 -4.3971295 -4.4012761 -4.4536848][-4.6429591 -4.5947618 -4.5184808 -4.47155 -4.4585142 -4.4448609 -4.4210439 -4.3752713 -4.3587646 -4.3948627 -4.407434 -4.345294 -4.2749047 -4.2769656 -4.3403206][-4.6343141 -4.56749 -4.4787569 -4.4212132 -4.3958511 -4.3652539 -4.3226557 -4.2658281 -4.2455063 -4.2792425 -4.2875724 -4.2311144 -4.1757455 -4.1938887 -4.2708573][-4.5234733 -4.4459314 -4.3522587 -4.3008614 -4.2859535 -4.2536836 -4.1865158 -4.1006579 -4.0665455 -4.1022406 -4.1279678 -4.1084142 -4.0986295 -4.1434712 -4.2301178][-4.3238611 -4.2382989 -4.1482177 -4.1171279 -4.1313438 -4.1157002 -4.032517 -3.9105906 -3.8524585 -3.8907921 -3.9516144 -3.9946203 -4.0464578 -4.1195741 -4.2117858][-4.1485949 -4.0641737 -3.990339 -3.9890325 -4.041234 -4.0588732 -3.9752643 -3.8248947 -3.7392654 -3.7748094 -3.8715947 -3.9712298 -4.0675945 -4.1499496 -4.2336287][-4.1254444 -4.0501981 -3.9895444 -4.0060935 -4.0822997 -4.1271162 -4.0544524 -3.8919864 -3.785723 -3.81604 -3.9337769 -4.0630655 -4.1701622 -4.2373843 -4.2931147][-4.271904 -4.2016163 -4.1382976 -4.1500673 -4.2293172 -4.2869868 -4.2211771 -4.0587564 -3.9405966 -3.9670727 -4.0912886 -4.2211914 -4.3164682 -4.3593011 -4.3781958][-4.4656539 -4.4084759 -4.3454852 -4.3485093 -4.4253221 -4.4871454 -4.4336958 -4.2864957 -4.1684737 -4.1861262 -4.2907987 -4.3902016 -4.4535294 -4.4710221 -4.467104][-4.5886736 -4.5471263 -4.496172 -4.5010777 -4.5730863 -4.6297631 -4.5908847 -4.4782386 -4.3866744 -4.4016743 -4.4756927 -4.5323133 -4.5544538 -4.5482812 -4.5358939][-4.6230645 -4.5938725 -4.567009 -4.58549 -4.6514983 -4.6971703 -4.6683474 -4.5910749 -4.5333347 -4.548851 -4.5934281 -4.6164389 -4.6109967 -4.59136 -4.5771084][-4.5981631 -4.5909681 -4.5947037 -4.63197 -4.6917963 -4.7230611 -4.6985645 -4.6430626 -4.6059828 -4.6152887 -4.6361485 -4.6404204 -4.626235 -4.6031227 -4.5859966][-4.5391216 -4.5500216 -4.5724335 -4.6121359 -4.6534371 -4.6674075 -4.6449575 -4.6067796 -4.5844936 -4.5876045 -4.5929027 -4.5883455 -4.5746417 -4.5551877 -4.5364904]]...]
INFO - root - 2017-12-07 10:24:20.841391: step 8210, loss = 21.66, batch loss = 21.58 (8.3 examples/sec; 0.969 sec/batch; 87h:19m:15s remains)
INFO - root - 2017-12-07 10:24:30.257126: step 8220, loss = 21.56, batch loss = 21.48 (8.6 examples/sec; 0.928 sec/batch; 83h:34m:02s remains)
INFO - root - 2017-12-07 10:24:39.749799: step 8230, loss = 21.53, batch loss = 21.45 (9.0 examples/sec; 0.888 sec/batch; 80h:00m:24s remains)
INFO - root - 2017-12-07 10:24:49.128490: step 8240, loss = 21.64, batch loss = 21.55 (9.0 examples/sec; 0.888 sec/batch; 79h:58m:16s remains)
INFO - root - 2017-12-07 10:24:58.543851: step 8250, loss = 21.48, batch loss = 21.40 (8.3 examples/sec; 0.967 sec/batch; 87h:03m:52s remains)
INFO - root - 2017-12-07 10:25:07.957225: step 8260, loss = 21.32, batch loss = 21.24 (8.3 examples/sec; 0.966 sec/batch; 87h:02m:42s remains)
INFO - root - 2017-12-07 10:25:17.206164: step 8270, loss = 21.32, batch loss = 21.24 (8.0 examples/sec; 1.006 sec/batch; 90h:36m:54s remains)
INFO - root - 2017-12-07 10:25:26.657278: step 8280, loss = 21.81, batch loss = 21.73 (8.1 examples/sec; 0.985 sec/batch; 88h:41m:57s remains)
INFO - root - 2017-12-07 10:25:36.087866: step 8290, loss = 21.49, batch loss = 21.41 (8.4 examples/sec; 0.951 sec/batch; 85h:36m:57s remains)
INFO - root - 2017-12-07 10:25:45.317516: step 8300, loss = 21.13, batch loss = 21.05 (8.3 examples/sec; 0.959 sec/batch; 86h:23m:18s remains)
2017-12-07 10:25:46.196247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2990708 -4.3276477 -4.3613014 -4.3839488 -4.3890862 -4.4020972 -4.4223289 -4.4314356 -4.43599 -4.4472957 -4.464457 -4.472342 -4.4495807 -4.3988309 -4.3272839][-4.2347012 -4.2585359 -4.3008246 -4.3410931 -4.3660669 -4.3957887 -4.4289527 -4.4413738 -4.4452405 -4.449851 -4.4510269 -4.4394956 -4.3933907 -4.3238325 -4.2482057][-4.1815457 -4.2023644 -4.2459011 -4.2928858 -4.3235869 -4.3549948 -4.3869438 -4.3907247 -4.3875265 -4.3851142 -4.3770871 -4.3598347 -4.3098078 -4.2400203 -4.177855][-4.1837754 -4.2112789 -4.2552447 -4.3002367 -4.3220062 -4.3406668 -4.3585253 -4.3456426 -4.3325858 -4.3241043 -4.3172989 -4.3107018 -4.2737637 -4.2115555 -4.158536][-4.2169333 -4.2511911 -4.2924047 -4.3295069 -4.3363557 -4.3360481 -4.3326406 -4.3022261 -4.2825232 -4.277051 -4.2842288 -4.3012986 -4.2875962 -4.2326503 -4.1757054][-4.1996994 -4.2388577 -4.2816753 -4.3151479 -4.3084693 -4.2819009 -4.2409859 -4.1819048 -4.1521945 -4.1601772 -4.2003875 -4.2570052 -4.2811642 -4.2425652 -4.1791224][-4.1335006 -4.1709695 -4.2241349 -4.2672482 -4.2606893 -4.2113314 -4.1236343 -4.0267477 -3.9841962 -4.011734 -4.0939312 -4.194098 -4.2564082 -4.2389822 -4.1726465][-4.0604148 -4.0911579 -4.1563311 -4.2169266 -4.2179732 -4.1476774 -4.01158 -3.8716903 -3.8131065 -3.8517706 -3.9643931 -4.1023235 -4.2035871 -4.2193375 -4.1643538][-4.0408049 -4.05877 -4.1327348 -4.21626 -4.23486 -4.158885 -3.9944658 -3.8236876 -3.7497432 -3.7873113 -3.9024787 -4.0523515 -4.1722779 -4.2120032 -4.1757236][-4.0963597 -4.0945354 -4.1613078 -4.2589612 -4.29948 -4.2394252 -4.0828161 -3.9157903 -3.844748 -3.8809338 -3.9781466 -4.104661 -4.20648 -4.238235 -4.2092395][-4.1506982 -4.13677 -4.1959715 -4.30308 -4.3668528 -4.3373065 -4.2146597 -4.0758581 -4.015419 -4.0454154 -4.1158037 -4.2027149 -4.2706084 -4.2798576 -4.2484179][-4.2101321 -4.201117 -4.2593112 -4.3690248 -4.4545026 -4.4658961 -4.3946319 -4.2978206 -4.2464213 -4.25198 -4.2799673 -4.31317 -4.3386989 -4.3248553 -4.2912016][-4.2519941 -4.2604241 -4.3257923 -4.4323521 -4.5245514 -4.5649018 -4.5426631 -4.4946833 -4.4611926 -4.4468274 -4.4377055 -4.4243422 -4.4127884 -4.3829517 -4.3487754][-4.2429228 -4.2648978 -4.3422804 -4.4484277 -4.5375361 -4.5893879 -4.5956917 -4.5794249 -4.5599766 -4.5403461 -4.5244179 -4.5048995 -4.48311 -4.443882 -4.3964834][-4.2258248 -4.2399068 -4.3069654 -4.3974571 -4.4665828 -4.5095854 -4.5177269 -4.5046663 -4.48794 -4.4728651 -4.4762626 -4.4916973 -4.49512 -4.465313 -4.4084496]]...]
INFO - root - 2017-12-07 10:25:55.501132: step 8310, loss = 21.23, batch loss = 21.15 (9.5 examples/sec; 0.842 sec/batch; 75h:46m:50s remains)
INFO - root - 2017-12-07 10:26:04.854298: step 8320, loss = 21.17, batch loss = 21.09 (8.6 examples/sec; 0.929 sec/batch; 83h:37m:06s remains)
INFO - root - 2017-12-07 10:26:14.254341: step 8330, loss = 21.84, batch loss = 21.75 (8.5 examples/sec; 0.939 sec/batch; 84h:33m:39s remains)
INFO - root - 2017-12-07 10:26:23.539022: step 8340, loss = 21.19, batch loss = 21.11 (7.9 examples/sec; 1.015 sec/batch; 91h:23m:14s remains)
INFO - root - 2017-12-07 10:26:32.971465: step 8350, loss = 21.76, batch loss = 21.68 (8.7 examples/sec; 0.917 sec/batch; 82h:36m:33s remains)
INFO - root - 2017-12-07 10:26:42.423115: step 8360, loss = 21.59, batch loss = 21.51 (8.7 examples/sec; 0.921 sec/batch; 82h:57m:12s remains)
INFO - root - 2017-12-07 10:26:51.711304: step 8370, loss = 21.35, batch loss = 21.27 (8.9 examples/sec; 0.897 sec/batch; 80h:43m:44s remains)
INFO - root - 2017-12-07 10:27:01.148834: step 8380, loss = 21.83, batch loss = 21.75 (8.8 examples/sec; 0.904 sec/batch; 81h:23m:15s remains)
INFO - root - 2017-12-07 10:27:10.673120: step 8390, loss = 21.20, batch loss = 21.12 (8.1 examples/sec; 0.987 sec/batch; 88h:53m:42s remains)
INFO - root - 2017-12-07 10:27:20.076456: step 8400, loss = 21.45, batch loss = 21.36 (8.1 examples/sec; 0.993 sec/batch; 89h:24m:13s remains)
2017-12-07 10:27:21.099917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.450098 -4.49356 -4.4806709 -4.4055252 -4.3135052 -4.2678647 -4.2790422 -4.3060255 -4.3462014 -4.4307613 -4.5459666 -4.61662 -4.5941958 -4.4979382 -4.3892503][-4.4370375 -4.4982257 -4.5245595 -4.493216 -4.4217486 -4.3586874 -4.3285546 -4.3136268 -4.31925 -4.378408 -4.4944715 -4.5968127 -4.6165977 -4.5512381 -4.4493713][-4.424099 -4.5019474 -4.5608311 -4.5613623 -4.5044661 -4.4315634 -4.3793964 -4.3431683 -4.3192296 -4.3391237 -4.4289618 -4.5427704 -4.6061573 -4.5890155 -4.5112686][-4.4165006 -4.4956779 -4.561295 -4.5628819 -4.5007777 -4.4200392 -4.369082 -4.3437648 -4.3174081 -4.3100057 -4.3674736 -4.4729476 -4.5663195 -4.5974803 -4.5542][-4.4107852 -4.4820466 -4.5359764 -4.519053 -4.4369764 -4.3439021 -4.2965717 -4.2921805 -4.2798982 -4.2631421 -4.2992592 -4.3907804 -4.5041819 -4.5820346 -4.5836291][-4.4042573 -4.4669185 -4.5038056 -4.4643192 -4.3587942 -4.246901 -4.1824756 -4.170733 -4.1566529 -4.14091 -4.1812849 -4.2785754 -4.41645 -4.5407043 -4.5943227][-4.394815 -4.4471416 -4.4622774 -4.3965316 -4.2633681 -4.1227932 -4.0206327 -3.9735296 -3.9426074 -3.9457331 -4.0299563 -4.1661773 -4.3345551 -4.4923034 -4.5883775][-4.3836284 -4.4229321 -4.4183521 -4.3349667 -4.1884594 -4.0302649 -3.8982382 -3.8142598 -3.7656293 -3.7978849 -3.9431214 -4.1305041 -4.320498 -4.485538 -4.5967155][-4.3813825 -4.413693 -4.4043555 -4.327529 -4.1978307 -4.0508738 -3.914175 -3.8061986 -3.7421324 -3.7928557 -3.9765949 -4.1936183 -4.3858509 -4.5353312 -4.6344576][-4.3952322 -4.4327722 -4.4371853 -4.3880577 -4.296298 -4.1803522 -4.0589733 -3.9437373 -3.8724256 -3.925467 -4.1059432 -4.3145885 -4.4858832 -4.604671 -4.6750236][-4.415462 -4.4659367 -4.4924235 -4.4762979 -4.4233985 -4.3408132 -4.2414904 -4.133481 -4.0668612 -4.1114531 -4.2623253 -4.4400673 -4.5784106 -4.6587005 -4.6878457][-4.4228029 -4.4821796 -4.5270963 -4.5408421 -4.5209394 -4.4674344 -4.3897276 -4.2998924 -4.2502995 -4.2901344 -4.4115334 -4.5526834 -4.64879 -4.679462 -4.6540031][-4.4067411 -4.46586 -4.5212913 -4.5594683 -4.5678043 -4.5400839 -4.4866176 -4.4279656 -4.4101138 -4.4585156 -4.5577936 -4.6556087 -4.6968412 -4.6681709 -4.5864944][-4.3786697 -4.4308648 -4.4899821 -4.5422263 -4.5696664 -4.5645905 -4.5414653 -4.5255294 -4.5445752 -4.6011295 -4.6716847 -4.7142248 -4.6936107 -4.6123581 -4.49743][-4.3469005 -4.3882766 -4.442286 -4.4960027 -4.5330882 -4.5491805 -4.5595775 -4.583539 -4.6262164 -4.6720648 -4.6967921 -4.6752853 -4.6012712 -4.4992266 -4.3963027]]...]
INFO - root - 2017-12-07 10:27:30.607903: step 8410, loss = 21.66, batch loss = 21.58 (8.4 examples/sec; 0.956 sec/batch; 86h:02m:49s remains)
INFO - root - 2017-12-07 10:27:39.915562: step 8420, loss = 21.18, batch loss = 21.10 (8.3 examples/sec; 0.967 sec/batch; 87h:00m:51s remains)
INFO - root - 2017-12-07 10:27:49.426684: step 8430, loss = 21.36, batch loss = 21.28 (8.6 examples/sec; 0.926 sec/batch; 83h:21m:27s remains)
INFO - root - 2017-12-07 10:27:58.627637: step 8440, loss = 21.73, batch loss = 21.64 (8.8 examples/sec; 0.908 sec/batch; 81h:45m:01s remains)
INFO - root - 2017-12-07 10:28:08.012456: step 8450, loss = 21.47, batch loss = 21.38 (8.5 examples/sec; 0.940 sec/batch; 84h:39m:26s remains)
INFO - root - 2017-12-07 10:28:17.382560: step 8460, loss = 21.88, batch loss = 21.80 (8.6 examples/sec; 0.926 sec/batch; 83h:23m:16s remains)
INFO - root - 2017-12-07 10:28:26.824911: step 8470, loss = 21.50, batch loss = 21.41 (8.2 examples/sec; 0.974 sec/batch; 87h:40m:23s remains)
INFO - root - 2017-12-07 10:28:36.090071: step 8480, loss = 21.59, batch loss = 21.50 (8.6 examples/sec; 0.926 sec/batch; 83h:19m:59s remains)
INFO - root - 2017-12-07 10:28:45.555772: step 8490, loss = 21.75, batch loss = 21.67 (8.6 examples/sec; 0.926 sec/batch; 83h:21m:15s remains)
INFO - root - 2017-12-07 10:28:54.901539: step 8500, loss = 21.37, batch loss = 21.29 (8.9 examples/sec; 0.896 sec/batch; 80h:37m:27s remains)
2017-12-07 10:28:55.754027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4957867 -4.4819427 -4.4646182 -4.4501271 -4.4445987 -4.4480748 -4.4537916 -4.4577556 -4.4610872 -4.4661884 -4.4736781 -4.4831643 -4.4929762 -4.4990864 -4.4982438][-4.474442 -4.4584966 -4.4399681 -4.4265652 -4.422039 -4.424643 -4.427978 -4.4269376 -4.4222941 -4.4177847 -4.4183884 -4.4266019 -4.4394417 -4.4504495 -4.4533219][-4.4781265 -4.45616 -4.4346013 -4.4245243 -4.4263248 -4.4336686 -4.4374228 -4.4322295 -4.4211435 -4.4091697 -4.404 -4.4098868 -4.4224916 -4.4340658 -4.4369364][-4.4498391 -4.4143386 -4.3833952 -4.37545 -4.3910661 -4.4148846 -4.4306054 -4.431941 -4.425323 -4.4178348 -4.4170828 -4.4253464 -4.4381809 -4.44981 -4.4526796][-4.3869162 -4.3253994 -4.2667947 -4.2434759 -4.2624693 -4.2999249 -4.32956 -4.3425074 -4.3471394 -4.3536868 -4.3662004 -4.3812809 -4.3957949 -4.4098649 -4.4172945][-4.3156576 -4.2222967 -4.1264925 -4.07564 -4.0844164 -4.1227641 -4.155611 -4.1729741 -4.1836796 -4.2035708 -4.2316065 -4.2557683 -4.2745328 -4.2935219 -4.306828][-4.247282 -4.132792 -4.0139265 -3.9448502 -3.9415326 -3.9684606 -3.987886 -3.9940772 -3.9962461 -4.0207043 -4.0630069 -4.0986423 -4.1254215 -4.1511459 -4.1665969][-4.1835804 -4.0689359 -3.9591062 -3.8986275 -3.8966095 -3.9123936 -3.9113455 -3.8967226 -3.8813162 -3.9006345 -3.9506991 -3.9951062 -4.029809 -4.0611629 -4.0740309][-4.1561322 -4.0585794 -3.9790208 -3.9447124 -3.9561207 -3.9699562 -3.9577515 -3.9316916 -3.9053454 -3.9157505 -3.9626234 -4.0049591 -4.0385818 -4.06971 -4.0792212][-4.210165 -4.1392937 -4.0886278 -4.0749917 -4.0957961 -4.1118059 -4.1000896 -4.0762749 -4.051177 -4.0543513 -4.08848 -4.1185331 -4.1437883 -4.1704979 -4.1800766][-4.3267941 -4.2815466 -4.2496181 -4.244565 -4.2679286 -4.28825 -4.2857742 -4.2728505 -4.2577128 -4.2581158 -4.2768879 -4.2924142 -4.3061919 -4.3229208 -4.3291297][-4.4431229 -4.4177279 -4.3981476 -4.3953724 -4.4148946 -4.43352 -4.4357505 -4.4305634 -4.4240632 -4.4250278 -4.433486 -4.4393725 -4.4444289 -4.4508119 -4.451262][-4.5233068 -4.5092049 -4.4964895 -4.4919248 -4.5001845 -4.5066357 -4.5030875 -4.4961314 -4.4913263 -4.4909797 -4.4936242 -4.4946384 -4.4955206 -4.4966211 -4.4944863][-4.5332403 -4.526022 -4.5195146 -4.5136328 -4.5098281 -4.5018339 -4.4891548 -4.4770479 -4.4699779 -4.467916 -4.467926 -4.4665952 -4.4657211 -4.4665322 -4.4674044][-4.5047541 -4.4969358 -4.4896955 -4.4811397 -4.4707847 -4.4577417 -4.4441376 -4.4325237 -4.4255171 -4.4223294 -4.4196839 -4.4147596 -4.410924 -4.4115453 -4.4161463]]...]
INFO - root - 2017-12-07 10:29:05.137273: step 8510, loss = 21.71, batch loss = 21.63 (8.2 examples/sec; 0.977 sec/batch; 87h:55m:14s remains)
INFO - root - 2017-12-07 10:29:14.637525: step 8520, loss = 21.27, batch loss = 21.19 (8.6 examples/sec; 0.930 sec/batch; 83h:44m:18s remains)
INFO - root - 2017-12-07 10:29:24.075375: step 8530, loss = 21.36, batch loss = 21.28 (8.9 examples/sec; 0.895 sec/batch; 80h:30m:30s remains)
INFO - root - 2017-12-07 10:29:33.732545: step 8540, loss = 21.21, batch loss = 21.13 (7.9 examples/sec; 1.011 sec/batch; 91h:01m:07s remains)
INFO - root - 2017-12-07 10:29:43.128972: step 8550, loss = 21.31, batch loss = 21.23 (8.0 examples/sec; 0.997 sec/batch; 89h:42m:34s remains)
INFO - root - 2017-12-07 10:29:52.432666: step 8560, loss = 20.97, batch loss = 20.89 (8.8 examples/sec; 0.906 sec/batch; 81h:32m:57s remains)
INFO - root - 2017-12-07 10:30:01.677788: step 8570, loss = 21.32, batch loss = 21.23 (9.3 examples/sec; 0.859 sec/batch; 77h:15m:00s remains)
INFO - root - 2017-12-07 10:30:11.030395: step 8580, loss = 21.59, batch loss = 21.51 (8.7 examples/sec; 0.915 sec/batch; 82h:18m:03s remains)
INFO - root - 2017-12-07 10:30:20.448547: step 8590, loss = 21.55, batch loss = 21.47 (8.6 examples/sec; 0.931 sec/batch; 83h:46m:04s remains)
INFO - root - 2017-12-07 10:30:29.875215: step 8600, loss = 21.52, batch loss = 21.44 (8.7 examples/sec; 0.916 sec/batch; 82h:23m:26s remains)
2017-12-07 10:30:30.793821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4478364 -4.4487152 -4.4493728 -4.4514022 -4.456605 -4.4670014 -4.480267 -4.4936709 -4.5032315 -4.5097227 -4.5145578 -4.521256 -4.5260034 -4.5210729 -4.5062003][-4.5124035 -4.5167866 -4.5194411 -4.519846 -4.522191 -4.5349584 -4.5579262 -4.5868015 -4.6102386 -4.617928 -4.6151762 -4.6124339 -4.6163521 -4.6167865 -4.6075191][-4.5461569 -4.5578318 -4.566503 -4.567421 -4.56359 -4.570035 -4.5934668 -4.6347933 -4.6761856 -4.6915865 -4.6835074 -4.664535 -4.6561227 -4.659987 -4.6619225][-4.5180893 -4.5238552 -4.5275912 -4.51963 -4.4973941 -4.4873805 -4.5005121 -4.5469589 -4.6114106 -4.6511045 -4.6612053 -4.6387959 -4.6163869 -4.6154361 -4.6125789][-4.4354014 -4.4210377 -4.4111328 -4.3894157 -4.3472929 -4.3194904 -4.3140979 -4.3494611 -4.42426 -4.4859676 -4.5233984 -4.514184 -4.4924397 -4.4873948 -4.4656658][-4.3022804 -4.2584987 -4.2299557 -4.192122 -4.1354985 -4.0928435 -4.064662 -4.0818782 -4.1630831 -4.2516937 -4.3320184 -4.3597565 -4.3579564 -4.34502 -4.2824349][-4.1768408 -4.1060724 -4.0558643 -3.9981332 -3.9330578 -3.87768 -3.8264153 -3.826966 -3.9247456 -4.059597 -4.1942039 -4.263422 -4.2808981 -4.2540603 -4.147994][-4.151226 -4.0749068 -4.0180168 -3.9505386 -3.8857555 -3.8163052 -3.7326579 -3.6999609 -3.7923651 -3.9590845 -4.1332064 -4.2342644 -4.2631569 -4.227035 -4.1052976][-4.236135 -4.1858311 -4.1431541 -4.0867028 -4.0415611 -3.9776945 -3.8791482 -3.8157065 -3.8779156 -4.0395031 -4.2179494 -4.3239985 -4.3485923 -4.3023553 -4.1856775][-4.3825259 -4.3589721 -4.3288364 -4.284811 -4.2646456 -4.2340074 -4.1607442 -4.0968447 -4.1225224 -4.2400379 -4.3806105 -4.4612565 -4.4702168 -4.4215527 -4.3302593][-4.4672418 -4.4608259 -4.4401441 -4.4102535 -4.413384 -4.4204273 -4.3902979 -4.348505 -4.3508239 -4.4151468 -4.502562 -4.5516315 -4.5518551 -4.5109329 -4.4526014][-4.502985 -4.503067 -4.4870605 -4.4675751 -4.4845128 -4.5166473 -4.5266547 -4.5157113 -4.5159264 -4.5464449 -4.5912151 -4.6151605 -4.6095734 -4.5785718 -4.5399532][-4.5187931 -4.522604 -4.5096188 -4.4961796 -4.51502 -4.5508704 -4.5771637 -4.5854344 -4.5897079 -4.6045122 -4.62288 -4.63269 -4.6278081 -4.6113214 -4.5852728][-4.4808211 -4.4837389 -4.4773488 -4.4719243 -4.4862466 -4.5142884 -4.5406 -4.5538583 -4.5563507 -4.5587416 -4.5641689 -4.5786672 -4.5933552 -4.6052675 -4.5957742][-4.4230404 -4.4210286 -4.420217 -4.4202213 -4.4269657 -4.4442229 -4.4653983 -4.4793053 -4.4794469 -4.4753137 -4.4772892 -4.5029397 -4.545938 -4.5977349 -4.6149049]]...]
INFO - root - 2017-12-07 10:30:39.874709: step 8610, loss = 21.41, batch loss = 21.33 (8.8 examples/sec; 0.905 sec/batch; 81h:27m:39s remains)
INFO - root - 2017-12-07 10:30:49.317044: step 8620, loss = 21.72, batch loss = 21.64 (8.1 examples/sec; 0.983 sec/batch; 88h:26m:16s remains)
INFO - root - 2017-12-07 10:30:58.785785: step 8630, loss = 21.60, batch loss = 21.51 (7.9 examples/sec; 1.014 sec/batch; 91h:16m:02s remains)
INFO - root - 2017-12-07 10:31:08.133852: step 8640, loss = 21.18, batch loss = 21.10 (8.6 examples/sec; 0.927 sec/batch; 83h:22m:21s remains)
INFO - root - 2017-12-07 10:31:17.523508: step 8650, loss = 21.21, batch loss = 21.12 (9.3 examples/sec; 0.861 sec/batch; 77h:26m:52s remains)
INFO - root - 2017-12-07 10:31:26.911768: step 8660, loss = 21.50, batch loss = 21.41 (9.0 examples/sec; 0.889 sec/batch; 79h:55m:44s remains)
INFO - root - 2017-12-07 10:31:36.381700: step 8670, loss = 21.62, batch loss = 21.54 (8.2 examples/sec; 0.978 sec/batch; 88h:01m:01s remains)
INFO - root - 2017-12-07 10:31:45.782609: step 8680, loss = 21.39, batch loss = 21.31 (8.0 examples/sec; 0.997 sec/batch; 89h:38m:41s remains)
INFO - root - 2017-12-07 10:31:54.986833: step 8690, loss = 21.48, batch loss = 21.39 (8.6 examples/sec; 0.935 sec/batch; 84h:03m:53s remains)
INFO - root - 2017-12-07 10:32:04.519105: step 8700, loss = 21.29, batch loss = 21.21 (8.6 examples/sec; 0.932 sec/batch; 83h:52m:04s remains)
2017-12-07 10:32:05.510733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4508071 -4.4871817 -4.5159349 -4.5441508 -4.5746021 -4.5914645 -4.5995378 -4.6127076 -4.6295981 -4.6355166 -4.6277943 -4.6095123 -4.5827789 -4.5445347 -4.490067][-4.5843735 -4.6411619 -4.6771274 -4.71004 -4.7480178 -4.770349 -4.7744203 -4.7856812 -4.8191895 -4.8448157 -4.8481827 -4.8281994 -4.7819328 -4.7130141 -4.624527][-4.69451 -4.7536736 -4.7840462 -4.815218 -4.84949 -4.8617816 -4.8431792 -4.8468275 -4.9097214 -4.9766836 -5.0143967 -5.0081458 -4.9481711 -4.8522234 -4.7392282][-4.7516284 -4.8076415 -4.830523 -4.8511057 -4.8540459 -4.8156209 -4.7441111 -4.7343893 -4.84066 -4.96689 -5.0581503 -5.0815492 -5.0217366 -4.9166327 -4.7968307][-4.7545319 -4.8065324 -4.8191433 -4.8164086 -4.756866 -4.6278787 -4.4738436 -4.4415874 -4.5956297 -4.78657 -4.9325566 -4.9935431 -4.9581742 -4.8776321 -4.7803621][-4.7272539 -4.7697268 -4.7629371 -4.7273169 -4.5988703 -4.3741841 -4.1339478 -4.07714 -4.2725625 -4.5190563 -4.7007937 -4.7865205 -4.7838364 -4.7530327 -4.7038465][-4.6838479 -4.7045107 -4.6624823 -4.5893154 -4.4074678 -4.1081686 -3.7939458 -3.7107127 -3.9328647 -4.2181096 -4.4224377 -4.5278339 -4.563549 -4.5968065 -4.6065912][-4.6627 -4.6589952 -4.5852284 -4.488595 -4.2911439 -3.9706306 -3.6363988 -3.5527716 -3.7821786 -4.0706978 -4.269774 -4.3781791 -4.4345393 -4.5109558 -4.5552626][-4.6722555 -4.6594872 -4.5822177 -4.4925175 -4.3112183 -4.0241976 -3.7368989 -3.6883066 -3.9083495 -4.1772709 -4.3589516 -4.4522552 -4.4994807 -4.5710506 -4.5994744][-4.7285433 -4.7346334 -4.6757536 -4.5890746 -4.4012432 -4.135169 -3.9124618 -3.9153662 -4.1330509 -4.3873277 -4.5534792 -4.621954 -4.6422858 -4.6804671 -4.6711249][-4.8183131 -4.85577 -4.8222075 -4.7269125 -4.5179019 -4.264257 -4.100924 -4.1455975 -4.3498878 -4.5693612 -4.7049041 -4.7462087 -4.7423658 -4.7453327 -4.7076435][-4.8574915 -4.91838 -4.9062634 -4.8124247 -4.6109214 -4.3949409 -4.288229 -4.3512182 -4.5154529 -4.6762528 -4.7711363 -4.7914472 -4.7791562 -4.7652555 -4.7173719][-4.8257093 -4.8945274 -4.8960562 -4.8237138 -4.6747379 -4.5311551 -4.4796233 -4.5331159 -4.6326103 -4.7229962 -4.7775 -4.7881904 -4.7771711 -4.7573314 -4.703897][-4.7258472 -4.79813 -4.8163877 -4.7843542 -4.7097745 -4.6473126 -4.6348372 -4.6597061 -4.6922851 -4.7242908 -4.7500358 -4.7541037 -4.7396712 -4.7091174 -4.6468468][-4.5787659 -4.6447663 -4.6778464 -4.6797352 -4.6599059 -4.6520462 -4.6616135 -4.6631021 -4.65214 -4.6499157 -4.6568217 -4.6509161 -4.6275797 -4.588212 -4.52854]]...]
INFO - root - 2017-12-07 10:32:14.785382: step 8710, loss = 21.49, batch loss = 21.41 (8.3 examples/sec; 0.965 sec/batch; 86h:45m:10s remains)
INFO - root - 2017-12-07 10:32:24.212165: step 8720, loss = 21.95, batch loss = 21.86 (8.0 examples/sec; 1.000 sec/batch; 89h:53m:42s remains)
INFO - root - 2017-12-07 10:32:33.588222: step 8730, loss = 21.23, batch loss = 21.15 (8.2 examples/sec; 0.981 sec/batch; 88h:12m:49s remains)
INFO - root - 2017-12-07 10:32:43.120983: step 8740, loss = 21.57, batch loss = 21.49 (8.0 examples/sec; 0.999 sec/batch; 89h:51m:43s remains)
INFO - root - 2017-12-07 10:32:52.468321: step 8750, loss = 21.48, batch loss = 21.39 (7.9 examples/sec; 1.007 sec/batch; 90h:32m:47s remains)
INFO - root - 2017-12-07 10:33:01.935313: step 8760, loss = 21.80, batch loss = 21.71 (8.5 examples/sec; 0.942 sec/batch; 84h:42m:07s remains)
INFO - root - 2017-12-07 10:33:11.266657: step 8770, loss = 21.57, batch loss = 21.49 (9.3 examples/sec; 0.861 sec/batch; 77h:28m:03s remains)
INFO - root - 2017-12-07 10:33:20.657325: step 8780, loss = 21.45, batch loss = 21.36 (9.0 examples/sec; 0.893 sec/batch; 80h:19m:33s remains)
INFO - root - 2017-12-07 10:33:30.057025: step 8790, loss = 20.90, batch loss = 20.81 (9.0 examples/sec; 0.891 sec/batch; 80h:05m:49s remains)
INFO - root - 2017-12-07 10:33:39.575921: step 8800, loss = 21.43, batch loss = 21.35 (8.8 examples/sec; 0.909 sec/batch; 81h:41m:52s remains)
2017-12-07 10:33:40.565946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4311137 -4.4377675 -4.4437218 -4.443244 -4.4367118 -4.4272079 -4.4171896 -4.4111433 -4.4107618 -4.4190993 -4.4280047 -4.4285665 -4.4233956 -4.41675 -4.4123192][-4.5579777 -4.5695262 -4.5763311 -4.574367 -4.5633183 -4.5473671 -4.5341864 -4.5307479 -4.5390887 -4.5574608 -4.5713477 -4.5675125 -4.5535789 -4.5442672 -4.54186][-4.6680431 -4.6696014 -4.6641741 -4.6550379 -4.6382284 -4.6153007 -4.6043448 -4.6190581 -4.6538525 -4.6941428 -4.7133651 -4.6975908 -4.6707163 -4.6648145 -4.6791687][-4.6869717 -4.658391 -4.6304293 -4.6128125 -4.5922017 -4.5629911 -4.5543022 -4.5922041 -4.6616592 -4.727612 -4.7488904 -4.7154765 -4.6737447 -4.6804857 -4.7326045][-4.5842142 -4.5180159 -4.4769721 -4.4652915 -4.4479313 -4.4069147 -4.3835812 -4.4261441 -4.5179825 -4.6043639 -4.6309762 -4.5857129 -4.5338907 -4.5556693 -4.6524239][-4.3767085 -4.2965407 -4.2647285 -4.2692795 -4.2495575 -4.1802874 -4.118041 -4.1356363 -4.2283416 -4.3343477 -4.3805695 -4.3435693 -4.2976837 -4.3353853 -4.4654627][-4.1690736 -4.1043448 -4.0903244 -4.0984583 -4.056386 -3.9436922 -3.8315389 -3.8059394 -3.8846807 -4.0131226 -4.1014838 -4.0948057 -4.0662155 -4.1113529 -4.2489843][-4.0449524 -4.0255117 -4.0339727 -4.0324287 -3.96139 -3.807832 -3.6508451 -3.5792787 -3.630532 -3.7744081 -3.9151902 -3.9604642 -3.958426 -3.9960485 -4.1063638][-4.0386457 -4.088623 -4.1321812 -4.1217003 -4.0235238 -3.8432202 -3.6621485 -3.5541506 -3.560715 -3.6900077 -3.8671391 -3.9691017 -3.9992929 -4.0200143 -4.0805922][-4.0764971 -4.1998353 -4.2988119 -4.3065782 -4.205162 -4.0272503 -3.8541923 -3.735945 -3.698385 -3.7785146 -3.9456682 -4.0795488 -4.1348038 -4.1388817 -4.1571827][-4.09102 -4.261519 -4.4188361 -4.4752426 -4.4071584 -4.2640462 -4.1303816 -4.03232 -3.9744346 -3.9939167 -4.1013889 -4.216887 -4.2792964 -4.2815504 -4.2842045][-4.0830536 -4.244679 -4.4276452 -4.5366611 -4.5227327 -4.4308786 -4.3501525 -4.2928677 -4.2479529 -4.2355533 -4.2768307 -4.3359032 -4.3750639 -4.3807931 -4.393261][-4.0531049 -4.1536322 -4.316103 -4.4555879 -4.4962473 -4.4622712 -4.4375277 -4.4302998 -4.4284377 -4.427896 -4.4321337 -4.4273062 -4.4111228 -4.3989282 -4.4273438][-4.0540051 -4.0706444 -4.1844959 -4.3277578 -4.4061823 -4.4232793 -4.4445233 -4.472858 -4.5077672 -4.5319538 -4.5236039 -4.4711413 -4.4017653 -4.370245 -4.4185328][-4.1134405 -4.0661073 -4.1375074 -4.2710056 -4.3673191 -4.4165783 -4.4580321 -4.4882402 -4.5242133 -4.5536237 -4.5404129 -4.469461 -4.378696 -4.3466706 -4.418076]]...]
INFO - root - 2017-12-07 10:33:49.779022: step 8810, loss = 21.51, batch loss = 21.43 (9.0 examples/sec; 0.885 sec/batch; 79h:33m:06s remains)
INFO - root - 2017-12-07 10:33:59.082349: step 8820, loss = 21.43, batch loss = 21.35 (9.2 examples/sec; 0.872 sec/batch; 78h:26m:09s remains)
INFO - root - 2017-12-07 10:34:08.436672: step 8830, loss = 21.35, batch loss = 21.27 (8.5 examples/sec; 0.945 sec/batch; 84h:55m:41s remains)
INFO - root - 2017-12-07 10:34:18.016196: step 8840, loss = 21.26, batch loss = 21.18 (8.1 examples/sec; 0.990 sec/batch; 89h:02m:22s remains)
INFO - root - 2017-12-07 10:34:27.369155: step 8850, loss = 21.73, batch loss = 21.64 (7.9 examples/sec; 1.007 sec/batch; 90h:32m:07s remains)
INFO - root - 2017-12-07 10:34:36.818750: step 8860, loss = 21.26, batch loss = 21.18 (8.7 examples/sec; 0.922 sec/batch; 82h:53m:01s remains)
INFO - root - 2017-12-07 10:34:46.188050: step 8870, loss = 21.46, batch loss = 21.37 (8.9 examples/sec; 0.894 sec/batch; 80h:22m:43s remains)
INFO - root - 2017-12-07 10:34:55.593166: step 8880, loss = 21.44, batch loss = 21.35 (8.3 examples/sec; 0.961 sec/batch; 86h:21m:49s remains)
INFO - root - 2017-12-07 10:35:05.154532: step 8890, loss = 21.35, batch loss = 21.27 (8.2 examples/sec; 0.977 sec/batch; 87h:50m:59s remains)
INFO - root - 2017-12-07 10:35:14.430683: step 8900, loss = 21.05, batch loss = 20.97 (8.6 examples/sec; 0.931 sec/batch; 83h:39m:23s remains)
2017-12-07 10:35:15.373311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2616968 -4.27249 -4.3851995 -4.5156507 -4.6242657 -4.6737332 -4.6461291 -4.5791988 -4.5756097 -4.6355433 -4.6914868 -4.6864505 -4.5806646 -4.4128275 -4.2668619][-4.1018167 -4.1568928 -4.3255496 -4.4866796 -4.6138554 -4.6679835 -4.6305704 -4.5523295 -4.5405602 -4.6000943 -4.6513119 -4.6277757 -4.5002084 -4.319 -4.1759253][-4.0374908 -4.1332345 -4.3422341 -4.5237055 -4.6415229 -4.6633325 -4.5918283 -4.4912772 -4.4722223 -4.5312967 -4.5688028 -4.5318022 -4.4101872 -4.2416306 -4.1162119][-4.0297155 -4.1525154 -4.3812485 -4.5528579 -4.6186171 -4.571013 -4.4492245 -4.3305612 -4.3234344 -4.4061885 -4.4586926 -4.4435263 -4.3568244 -4.2204065 -4.1223965][-4.1616149 -4.2898908 -4.4811978 -4.578898 -4.5441389 -4.4031925 -4.2217731 -4.0816035 -4.0886726 -4.2137904 -4.3136349 -4.3510885 -4.3242011 -4.2452192 -4.1931443][-4.3307781 -4.4452481 -4.5711026 -4.5845656 -4.4652882 -4.25999 -4.0386181 -3.8752522 -3.8831844 -4.0448451 -4.1926532 -4.2791715 -4.314559 -4.3038483 -4.2986956][-4.4158883 -4.5038123 -4.56914 -4.5285997 -4.3715029 -4.1485453 -3.9185684 -3.7382669 -3.7353482 -3.9249735 -4.1134682 -4.2360911 -4.3196416 -4.3603907 -4.3732224][-4.4688163 -4.53617 -4.5509973 -4.4737887 -4.302156 -4.0854578 -3.8731773 -3.6967857 -3.6837168 -3.8893023 -4.0968657 -4.2280836 -4.3295655 -4.3878665 -4.3883352][-4.5117435 -4.5866861 -4.5898108 -4.5130816 -4.355082 -4.1638336 -3.9894538 -3.845566 -3.8319364 -4.0205054 -4.2073689 -4.3125434 -4.3966703 -4.443212 -4.4207225][-4.5421944 -4.6306815 -4.64348 -4.5859342 -4.45707 -4.2968884 -4.1591158 -4.0551028 -4.0517116 -4.2135878 -4.3713565 -4.4522247 -4.5140696 -4.5422139 -4.5007081][-4.5970378 -4.677845 -4.6874032 -4.6387024 -4.5378265 -4.4126849 -4.3132515 -4.2565746 -4.2707582 -4.3966236 -4.516901 -4.5725536 -4.6090889 -4.6215334 -4.5782518][-4.6613178 -4.7294025 -4.7410722 -4.7109957 -4.6434083 -4.5570478 -4.4955034 -4.4754848 -4.49525 -4.5667386 -4.6300707 -4.6517329 -4.6615634 -4.661696 -4.63031][-4.6927652 -4.7434254 -4.7635531 -4.7611403 -4.7307339 -4.6803036 -4.6463723 -4.6414666 -4.6521096 -4.675355 -4.6926732 -4.6905694 -4.6834736 -4.6761475 -4.6554][-4.6925516 -4.7154369 -4.7305226 -4.7418995 -4.7379484 -4.7171469 -4.7011724 -4.6988931 -4.6983476 -4.6942096 -4.6878114 -4.6760664 -4.6647863 -4.6589351 -4.6480117][-4.6414232 -4.6448011 -4.6515579 -4.664917 -4.6734753 -4.6697292 -4.6605182 -4.6514478 -4.6386237 -4.6221037 -4.6079063 -4.5959592 -4.5884 -4.5885496 -4.5875764]]...]
INFO - root - 2017-12-07 10:35:24.739204: step 8910, loss = 21.11, batch loss = 21.03 (8.3 examples/sec; 0.959 sec/batch; 86h:09m:29s remains)
INFO - root - 2017-12-07 10:35:34.250521: step 8920, loss = 21.29, batch loss = 21.20 (8.1 examples/sec; 0.993 sec/batch; 89h:14m:31s remains)
INFO - root - 2017-12-07 10:35:43.732394: step 8930, loss = 21.61, batch loss = 21.53 (8.5 examples/sec; 0.946 sec/batch; 85h:01m:50s remains)
INFO - root - 2017-12-07 10:35:53.145905: step 8940, loss = 21.40, batch loss = 21.32 (8.8 examples/sec; 0.907 sec/batch; 81h:33m:44s remains)
INFO - root - 2017-12-07 10:36:02.489269: step 8950, loss = 21.61, batch loss = 21.53 (9.1 examples/sec; 0.875 sec/batch; 78h:35m:57s remains)
INFO - root - 2017-12-07 10:36:11.881639: step 8960, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.948 sec/batch; 85h:11m:43s remains)
INFO - root - 2017-12-07 10:36:21.384549: step 8970, loss = 21.36, batch loss = 21.27 (7.9 examples/sec; 1.007 sec/batch; 90h:31m:10s remains)
INFO - root - 2017-12-07 10:36:30.922876: step 8980, loss = 21.60, batch loss = 21.52 (8.1 examples/sec; 0.983 sec/batch; 88h:20m:54s remains)
INFO - root - 2017-12-07 10:36:40.234207: step 8990, loss = 21.23, batch loss = 21.14 (8.8 examples/sec; 0.912 sec/batch; 81h:57m:14s remains)
INFO - root - 2017-12-07 10:36:49.595818: step 9000, loss = 21.77, batch loss = 21.69 (8.3 examples/sec; 0.967 sec/batch; 86h:53m:10s remains)
2017-12-07 10:36:50.589435: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1765294 -4.2763205 -4.298449 -4.2425332 -4.2018938 -4.2478366 -4.3589687 -4.4533191 -4.4725637 -4.4160061 -4.3392 -4.2808566 -4.2397 -4.213366 -4.224525][-4.2736106 -4.374351 -4.3912373 -4.3258514 -4.2762628 -4.3190026 -4.4309416 -4.52025 -4.5265069 -4.4521155 -4.3507409 -4.2616796 -4.189075 -4.1514573 -4.1845951][-4.3281007 -4.4133625 -4.4283595 -4.3683715 -4.3192906 -4.3462934 -4.4388394 -4.5157762 -4.5250278 -4.4616852 -4.3608046 -4.2501097 -4.1478758 -4.1051369 -4.1585412][-4.3774614 -4.4226842 -4.4134254 -4.3611765 -4.3257508 -4.3411884 -4.40285 -4.4566426 -4.4661455 -4.4240742 -4.3417077 -4.2230997 -4.1045866 -4.0652766 -4.1365595][-4.3831797 -4.3771257 -4.3312864 -4.288074 -4.2743645 -4.28185 -4.3042588 -4.325995 -4.3376312 -4.3270121 -4.2818022 -4.180542 -4.0656524 -4.0378633 -4.1241322][-4.312232 -4.2684622 -4.2095695 -4.197813 -4.2149563 -4.206563 -4.1749616 -4.1554627 -4.1752796 -4.2157874 -4.2336068 -4.18031 -4.0885205 -4.0685582 -4.1547718][-4.2315621 -4.1628141 -4.1052127 -4.1270304 -4.1704955 -4.1462903 -4.0641327 -4.0064044 -4.0386853 -4.1331282 -4.2164812 -4.21832 -4.1498671 -4.1209011 -4.1832333][-4.1769214 -4.1144595 -4.0663061 -4.0987315 -4.1426077 -4.1003728 -3.9764354 -3.8805318 -3.9160523 -4.0488157 -4.1851869 -4.2360339 -4.1881909 -4.1437392 -4.1722355][-4.1730647 -4.1390276 -4.1102829 -4.1369972 -4.1629591 -4.1091847 -3.9670913 -3.8416688 -3.8612292 -4.0018129 -4.167192 -4.2544532 -4.2293305 -4.1751323 -4.1686921][-4.2156043 -4.1970611 -4.1772456 -4.1911492 -4.2049575 -4.1593118 -4.0257568 -3.8897381 -3.8790202 -3.9956403 -4.1610107 -4.2692847 -4.2680192 -4.2143912 -4.1835165][-4.284173 -4.2751355 -4.2562556 -4.2519217 -4.2508459 -4.2068996 -4.0806437 -3.936317 -3.8886654 -3.9704647 -4.1294465 -4.2621593 -4.2972794 -4.2649641 -4.2319169][-4.3518958 -4.3533697 -4.3359995 -4.3176913 -4.3030057 -4.252285 -4.1224022 -3.9639864 -3.8826449 -3.9384732 -4.0974736 -4.2558651 -4.3244352 -4.3139029 -4.2852468][-4.4089961 -4.4215236 -4.4111943 -4.386035 -4.3539734 -4.2883892 -4.1506643 -3.9813631 -3.8808904 -3.9252124 -4.0878186 -4.2612 -4.3430991 -4.3356056 -4.3028412][-4.4346933 -4.4698939 -4.4798779 -4.4576492 -4.4098463 -4.3288207 -4.1917672 -4.0268507 -3.9255931 -3.9631972 -4.1183562 -4.287745 -4.3700972 -4.361598 -4.3265247][-4.4460654 -4.5020843 -4.5381122 -4.5305738 -4.48249 -4.4019866 -4.2830834 -4.141726 -4.0520129 -4.0778117 -4.2070956 -4.3492131 -4.4162145 -4.4058762 -4.3771791]]...]
INFO - root - 2017-12-07 10:36:59.982706: step 9010, loss = 21.65, batch loss = 21.57 (8.2 examples/sec; 0.974 sec/batch; 87h:31m:25s remains)
INFO - root - 2017-12-07 10:37:09.319518: step 9020, loss = 21.68, batch loss = 21.60 (8.2 examples/sec; 0.973 sec/batch; 87h:27m:30s remains)
INFO - root - 2017-12-07 10:37:18.641191: step 9030, loss = 21.74, batch loss = 21.66 (8.9 examples/sec; 0.897 sec/batch; 80h:34m:57s remains)
INFO - root - 2017-12-07 10:37:27.970194: step 9040, loss = 21.49, batch loss = 21.40 (8.5 examples/sec; 0.944 sec/batch; 84h:47m:41s remains)
INFO - root - 2017-12-07 10:37:37.409265: step 9050, loss = 21.24, batch loss = 21.15 (9.0 examples/sec; 0.887 sec/batch; 79h:43m:07s remains)
INFO - root - 2017-12-07 10:37:46.737564: step 9060, loss = 21.54, batch loss = 21.45 (8.6 examples/sec; 0.934 sec/batch; 83h:55m:15s remains)
INFO - root - 2017-12-07 10:37:56.125154: step 9070, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.930 sec/batch; 83h:31m:07s remains)
INFO - root - 2017-12-07 10:38:05.552317: step 9080, loss = 21.64, batch loss = 21.55 (8.4 examples/sec; 0.950 sec/batch; 85h:20m:46s remains)
INFO - root - 2017-12-07 10:38:14.930069: step 9090, loss = 21.76, batch loss = 21.68 (8.5 examples/sec; 0.943 sec/batch; 84h:43m:18s remains)
INFO - root - 2017-12-07 10:38:24.249236: step 9100, loss = 21.35, batch loss = 21.27 (8.3 examples/sec; 0.959 sec/batch; 86h:09m:20s remains)
2017-12-07 10:38:25.240044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3049655 -4.4170456 -4.5393376 -4.5636415 -4.4797783 -4.355649 -4.2501192 -4.1916804 -4.199297 -4.2743244 -4.358892 -4.3863339 -4.359539 -4.3114557 -4.2629175][-4.2875776 -4.3889995 -4.5173569 -4.5696697 -4.5122538 -4.3885813 -4.2589288 -4.1795011 -4.1836042 -4.257844 -4.326973 -4.3369465 -4.3104615 -4.2938366 -4.2863817][-4.2715259 -4.3541884 -4.4649429 -4.519743 -4.4787631 -4.361834 -4.2230644 -4.138257 -4.1502056 -4.2293053 -4.2854047 -4.2803063 -4.2503161 -4.252533 -4.28621][-4.2641416 -4.3215618 -4.3930445 -4.4164276 -4.36744 -4.2577457 -4.1308923 -4.0632057 -4.0999217 -4.1968517 -4.2545595 -4.245966 -4.2149768 -4.2230067 -4.2750168][-4.2681656 -4.3121748 -4.3503823 -4.327601 -4.2405143 -4.1128325 -3.986511 -3.9331615 -3.9989362 -4.1368604 -4.2309036 -4.2470288 -4.2287784 -4.2304535 -4.2625294][-4.2801266 -4.3284712 -4.359138 -4.3109117 -4.1829157 -4.0095606 -3.8434033 -3.7610571 -3.8240771 -4.0028248 -4.1658878 -4.2458949 -4.2638874 -4.2589536 -4.2474523][-4.2914648 -4.348278 -4.387311 -4.3422079 -4.2013497 -3.9946051 -3.7765789 -3.6278224 -3.637275 -3.8227971 -4.0549421 -4.2183404 -4.2911925 -4.2926307 -4.2433381][-4.2998714 -4.3582683 -4.40231 -4.3687096 -4.2479715 -4.0616493 -3.8456347 -3.6574593 -3.5970283 -3.7324252 -3.9784529 -4.1932364 -4.3100896 -4.3278441 -4.2636495][-4.3099895 -4.3650966 -4.4068112 -4.3802614 -4.2902255 -4.1612773 -3.9995911 -3.8203204 -3.7117543 -3.77357 -3.984102 -4.206624 -4.3460073 -4.3849611 -4.3309402][-4.3206592 -4.3741727 -4.4125624 -4.3845015 -4.3088784 -4.2255669 -4.1248503 -3.98687 -3.871866 -3.887558 -4.0541797 -4.2527256 -4.385273 -4.4321184 -4.3957186][-4.3276 -4.3821759 -4.4192877 -4.386879 -4.3122072 -4.2482815 -4.1863403 -4.094089 -4.0047483 -4.0099626 -4.1323185 -4.2752995 -4.3625584 -4.3930893 -4.3743505][-4.3309026 -4.38879 -4.4255939 -4.386538 -4.2997422 -4.2258477 -4.1697822 -4.1093163 -4.0641756 -4.0941229 -4.1874447 -4.266871 -4.293294 -4.2911062 -4.2839622][-4.3294659 -4.389297 -4.4289079 -4.3879843 -4.2891159 -4.1908932 -4.1189642 -4.0801129 -4.0876942 -4.1598053 -4.2404714 -4.26887 -4.2427211 -4.2088585 -4.2039952][-4.3228259 -4.3833141 -4.4339428 -4.4071593 -4.3147969 -4.2048025 -4.1168776 -4.0850859 -4.1262989 -4.2259979 -4.3003044 -4.2969432 -4.2411046 -4.1962838 -4.1948609][-4.3160281 -4.378809 -4.4480205 -4.4450746 -4.3649111 -4.2482753 -4.14782 -4.113606 -4.1650581 -4.2713428 -4.3426013 -4.3300681 -4.2733188 -4.23952 -4.2445316]]...]
INFO - root - 2017-12-07 10:38:34.464856: step 9110, loss = 21.47, batch loss = 21.39 (9.2 examples/sec; 0.869 sec/batch; 78h:02m:08s remains)
INFO - root - 2017-12-07 10:38:43.674724: step 9120, loss = 21.12, batch loss = 21.04 (8.8 examples/sec; 0.906 sec/batch; 81h:23m:29s remains)
INFO - root - 2017-12-07 10:38:52.936871: step 9130, loss = 21.45, batch loss = 21.36 (8.3 examples/sec; 0.967 sec/batch; 86h:52m:58s remains)
INFO - root - 2017-12-07 10:39:02.302650: step 9140, loss = 21.81, batch loss = 21.73 (8.6 examples/sec; 0.928 sec/batch; 83h:20m:09s remains)
INFO - root - 2017-12-07 10:39:11.825224: step 9150, loss = 21.54, batch loss = 21.46 (8.2 examples/sec; 0.971 sec/batch; 87h:14m:02s remains)
INFO - root - 2017-12-07 10:39:21.262157: step 9160, loss = 21.56, batch loss = 21.48 (8.0 examples/sec; 0.994 sec/batch; 89h:18m:51s remains)
INFO - root - 2017-12-07 10:39:30.679714: step 9170, loss = 21.85, batch loss = 21.76 (8.3 examples/sec; 0.964 sec/batch; 86h:34m:34s remains)
INFO - root - 2017-12-07 10:39:40.079666: step 9180, loss = 21.56, batch loss = 21.47 (8.5 examples/sec; 0.936 sec/batch; 84h:06m:07s remains)
INFO - root - 2017-12-07 10:39:49.338945: step 9190, loss = 21.48, batch loss = 21.40 (8.7 examples/sec; 0.918 sec/batch; 82h:25m:50s remains)
INFO - root - 2017-12-07 10:39:58.659946: step 9200, loss = 21.22, batch loss = 21.13 (8.7 examples/sec; 0.918 sec/batch; 82h:26m:24s remains)
2017-12-07 10:39:59.675286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.59981 -4.6876192 -4.7289486 -4.6999154 -4.6295705 -4.5810275 -4.5903673 -4.6457639 -4.6945977 -4.7197971 -4.7388396 -4.7598386 -4.7548971 -4.7048573 -4.6197634][-4.663332 -4.7396259 -4.7416906 -4.656302 -4.5289769 -4.4526558 -4.477479 -4.5788651 -4.6769872 -4.7356186 -4.7655149 -4.7818985 -4.7696838 -4.7167597 -4.6321354][-4.7111831 -4.7542839 -4.69485 -4.5443649 -4.3694353 -4.2743163 -4.3165884 -4.4628153 -4.6125665 -4.7093673 -4.7478638 -4.7514544 -4.7279887 -4.6803894 -4.6138177][-4.7537236 -4.7582841 -4.6338778 -4.4253392 -4.2188416 -4.1072326 -4.1510439 -4.3244333 -4.51367 -4.6429281 -4.6922846 -4.6882386 -4.6552224 -4.6101403 -4.5647607][-4.8051038 -4.7785654 -4.6059294 -4.3566151 -4.1249657 -3.986877 -4.0061975 -4.1749077 -4.3763523 -4.5268178 -4.5923452 -4.5963054 -4.566278 -4.5279207 -4.5035548][-4.8570838 -4.816843 -4.61972 -4.3455262 -4.08274 -3.8915868 -3.8488631 -3.975369 -4.1711369 -4.3477421 -4.4490271 -4.4862523 -4.4767342 -4.45513 -4.4523039][-4.8887544 -4.838562 -4.6260076 -4.3343086 -4.037231 -3.7867503 -3.6719558 -3.7547832 -3.965301 -4.19401 -4.3499613 -4.4261618 -4.4353938 -4.4253387 -4.4313326][-4.88365 -4.831934 -4.6208868 -4.327498 -4.0119152 -3.725657 -3.5705028 -3.6405103 -3.8814063 -4.1532192 -4.3412418 -4.4349437 -4.4557567 -4.4469595 -4.4438376][-4.8531981 -4.8168869 -4.6371641 -4.3773551 -4.0762663 -3.7926335 -3.6380479 -3.7015352 -3.9375436 -4.2045803 -4.389483 -4.4901972 -4.52524 -4.5159016 -4.4895277][-4.83828 -4.8309174 -4.70821 -4.5035782 -4.2328625 -3.9674597 -3.8288989 -3.8806849 -4.0779428 -4.2991095 -4.4586835 -4.5677972 -4.6268969 -4.6189065 -4.564888][-4.8297806 -4.8520842 -4.781713 -4.6277781 -4.3916378 -4.1611152 -4.0555334 -4.1036763 -4.2504625 -4.4108529 -4.5413966 -4.6582251 -4.7324638 -4.7164941 -4.6324544][-4.8053374 -4.8521914 -4.8181438 -4.7038922 -4.5155339 -4.3450422 -4.2874537 -4.3399858 -4.4414992 -4.5424013 -4.6343665 -4.7351294 -4.796113 -4.7624993 -4.662818][-4.7581224 -4.8182158 -4.8038063 -4.7186861 -4.5836654 -4.4768796 -4.4613118 -4.5144935 -4.5817428 -4.6375041 -4.6907692 -4.7592611 -4.7926226 -4.7493949 -4.6603994][-4.6723495 -4.7269068 -4.7126465 -4.6458106 -4.5586653 -4.5070481 -4.5202456 -4.5701418 -4.61518 -4.6423941 -4.6667638 -4.7036991 -4.7156954 -4.6798668 -4.6206946][-4.5520253 -4.5852737 -4.5673718 -4.5202322 -4.4754725 -4.463974 -4.4882755 -4.5259047 -4.5529013 -4.5648022 -4.5725865 -4.5859361 -4.5856676 -4.5651021 -4.538136]]...]
INFO - root - 2017-12-07 10:40:09.103697: step 9210, loss = 21.17, batch loss = 21.09 (8.2 examples/sec; 0.975 sec/batch; 87h:34m:11s remains)
INFO - root - 2017-12-07 10:40:18.378876: step 9220, loss = 21.31, batch loss = 21.23 (8.3 examples/sec; 0.963 sec/batch; 86h:29m:42s remains)
INFO - root - 2017-12-07 10:40:27.741463: step 9230, loss = 21.68, batch loss = 21.60 (8.6 examples/sec; 0.930 sec/batch; 83h:28m:50s remains)
INFO - root - 2017-12-07 10:40:37.061999: step 9240, loss = 21.67, batch loss = 21.59 (8.9 examples/sec; 0.901 sec/batch; 80h:56m:20s remains)
INFO - root - 2017-12-07 10:40:46.397527: step 9250, loss = 21.58, batch loss = 21.49 (8.5 examples/sec; 0.943 sec/batch; 84h:40m:54s remains)
INFO - root - 2017-12-07 10:40:55.665210: step 9260, loss = 21.39, batch loss = 21.30 (8.9 examples/sec; 0.903 sec/batch; 81h:05m:13s remains)
INFO - root - 2017-12-07 10:41:04.879861: step 9270, loss = 21.42, batch loss = 21.33 (8.9 examples/sec; 0.901 sec/batch; 80h:55m:33s remains)
INFO - root - 2017-12-07 10:41:14.451558: step 9280, loss = 21.52, batch loss = 21.44 (8.6 examples/sec; 0.935 sec/batch; 83h:58m:30s remains)
INFO - root - 2017-12-07 10:41:23.875550: step 9290, loss = 21.35, batch loss = 21.26 (8.3 examples/sec; 0.966 sec/batch; 86h:43m:54s remains)
INFO - root - 2017-12-07 10:41:33.162630: step 9300, loss = 21.53, batch loss = 21.45 (7.8 examples/sec; 1.029 sec/batch; 92h:24m:05s remains)
2017-12-07 10:41:34.109301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3159518 -4.2738352 -4.2483907 -4.2397895 -4.271687 -4.3269749 -4.3431063 -4.3272128 -4.3243737 -4.3282552 -4.3489079 -4.384028 -4.3964372 -4.380651 -4.3579473][-4.2624536 -4.2119803 -4.1736174 -4.1535854 -4.1845531 -4.2458582 -4.2608023 -4.2393312 -4.241178 -4.2674923 -4.3155522 -4.3721032 -4.3992829 -4.3894339 -4.3718781][-4.2787533 -4.2417593 -4.2059579 -4.1833954 -4.2056923 -4.2513247 -4.2449236 -4.2086034 -4.2138476 -4.258975 -4.32814 -4.3985729 -4.4313397 -4.416431 -4.39122][-4.3063436 -4.2959986 -4.2753677 -4.2557993 -4.2633128 -4.2808533 -4.244678 -4.1899772 -4.1961474 -4.2554636 -4.3382854 -4.4205937 -4.4598632 -4.4403605 -4.4057488][-4.2849441 -4.2988596 -4.292594 -4.2682648 -4.254487 -4.244792 -4.1879869 -4.1260686 -4.1405959 -4.2175975 -4.3166704 -4.4149356 -4.4652958 -4.4463863 -4.4073396][-4.2036548 -4.2333641 -4.2416716 -4.2104077 -4.1799345 -4.1510935 -4.0859842 -4.0283575 -4.0533895 -4.1478496 -4.2648206 -4.3800521 -4.4425044 -4.4293008 -4.3937635][-4.0893903 -4.1161528 -4.123035 -4.0834808 -4.048152 -4.0169549 -3.9635603 -3.9267495 -3.963146 -4.0714674 -4.2043147 -4.3323555 -4.4036822 -4.3985558 -4.3712659][-4.0143828 -4.0266271 -4.01803 -3.9722705 -3.94389 -3.9240787 -3.9010711 -3.8981667 -3.9458971 -4.0585561 -4.196414 -4.3233919 -4.3923955 -4.3885794 -4.363296][-4.0306592 -4.0463 -4.03535 -3.9926486 -3.9702048 -3.9597352 -3.9652498 -3.9922407 -4.0445352 -4.1469369 -4.2717209 -4.3805895 -4.4335341 -4.4190373 -4.3825426][-4.0810213 -4.1073418 -4.1023631 -4.0685554 -4.055665 -4.05916 -4.0920248 -4.1416688 -4.195302 -4.2814031 -4.3823867 -4.4652133 -4.4981112 -4.4687819 -4.415678][-4.1445451 -4.181046 -4.1831474 -4.1579618 -4.1554565 -4.174994 -4.2227664 -4.275744 -4.3215423 -4.386374 -4.4600029 -4.51854 -4.5377874 -4.5032372 -4.443357][-4.2315779 -4.2725873 -4.2770357 -4.2552962 -4.2537618 -4.2717328 -4.3053336 -4.3358736 -4.3662968 -4.4158115 -4.4730473 -4.5194612 -4.5355258 -4.5067229 -4.4519687][-4.2930527 -4.3192778 -4.3141489 -4.2908211 -4.28094 -4.2818484 -4.2795434 -4.2715683 -4.2874346 -4.3387089 -4.4057536 -4.4652247 -4.4958353 -4.4817624 -4.4389224][-4.3351851 -4.3360586 -4.3143821 -4.284554 -4.2649078 -4.24735 -4.2024775 -4.14983 -4.1489491 -4.2079673 -4.2995028 -4.3916311 -4.4494672 -4.4504914 -4.4155488][-4.3716712 -4.3561869 -4.3246813 -4.2889218 -4.2636704 -4.2362819 -4.1635637 -4.083179 -4.074935 -4.1429892 -4.2520909 -4.36722 -4.4379926 -4.4349694 -4.3917742]]...]
INFO - root - 2017-12-07 10:41:43.456681: step 9310, loss = 21.68, batch loss = 21.60 (8.7 examples/sec; 0.925 sec/batch; 83h:01m:22s remains)
INFO - root - 2017-12-07 10:41:52.604715: step 9320, loss = 21.19, batch loss = 21.11 (10.3 examples/sec; 0.775 sec/batch; 69h:32m:33s remains)
INFO - root - 2017-12-07 10:42:02.032923: step 9330, loss = 21.54, batch loss = 21.46 (8.9 examples/sec; 0.897 sec/batch; 80h:29m:49s remains)
INFO - root - 2017-12-07 10:42:11.467387: step 9340, loss = 21.82, batch loss = 21.74 (8.6 examples/sec; 0.935 sec/batch; 83h:55m:44s remains)
INFO - root - 2017-12-07 10:42:20.966671: step 9350, loss = 21.29, batch loss = 21.20 (8.7 examples/sec; 0.917 sec/batch; 82h:20m:12s remains)
INFO - root - 2017-12-07 10:42:30.365237: step 9360, loss = 21.28, batch loss = 21.20 (8.7 examples/sec; 0.923 sec/batch; 82h:51m:00s remains)
INFO - root - 2017-12-07 10:42:39.839790: step 9370, loss = 21.87, batch loss = 21.79 (7.9 examples/sec; 1.006 sec/batch; 90h:20m:03s remains)
INFO - root - 2017-12-07 10:42:49.242270: step 9380, loss = 21.08, batch loss = 21.00 (8.9 examples/sec; 0.895 sec/batch; 80h:22m:03s remains)
INFO - root - 2017-12-07 10:42:58.707403: step 9390, loss = 21.33, batch loss = 21.25 (8.8 examples/sec; 0.909 sec/batch; 81h:36m:28s remains)
INFO - root - 2017-12-07 10:43:08.029316: step 9400, loss = 21.48, batch loss = 21.39 (9.0 examples/sec; 0.887 sec/batch; 79h:39m:05s remains)
2017-12-07 10:43:09.013187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3607559 -4.3615212 -4.3683071 -4.3759022 -4.3809109 -4.3807392 -4.3741083 -4.3631058 -4.3541818 -4.34968 -4.3469992 -4.34361 -4.339673 -4.3364758 -4.3341565][-4.3707495 -4.3797603 -4.3959394 -4.410862 -4.4180045 -4.4138422 -4.4017606 -4.3897376 -4.3867521 -4.3909979 -4.3916669 -4.3823342 -4.3646359 -4.3460546 -4.3328586][-4.3879037 -4.4074287 -4.4347248 -4.4556203 -4.4589133 -4.4419031 -4.4172039 -4.4030666 -4.411541 -4.4352236 -4.45096 -4.4418249 -4.4100766 -4.3707752 -4.3409452][-4.4037461 -4.4317727 -4.4636159 -4.4828157 -4.4773221 -4.4480224 -4.4159951 -4.4054542 -4.4267688 -4.4694891 -4.5022073 -4.4989047 -4.4604535 -4.4045858 -4.3569994][-4.3960161 -4.4231963 -4.4455886 -4.4526639 -4.4374423 -4.4065433 -4.3831739 -4.384016 -4.4137945 -4.4649658 -4.5069184 -4.5133262 -4.4834433 -4.4280748 -4.3721442][-4.3513408 -4.3662934 -4.3750358 -4.372004 -4.35215 -4.3235469 -4.3105621 -4.3189087 -4.3496442 -4.3993816 -4.4394155 -4.4527483 -4.4423118 -4.40825 -4.3629107][-4.2554164 -4.2520132 -4.2582788 -4.2615237 -4.2430258 -4.2029552 -4.1769633 -4.178916 -4.2153211 -4.2781849 -4.3250132 -4.3458686 -4.3534336 -4.3468704 -4.3254805][-4.133193 -4.1109676 -4.1256938 -4.1479845 -4.1284275 -4.048368 -3.9699264 -3.940778 -3.987895 -4.0924029 -4.1778975 -4.2232785 -4.250824 -4.2692585 -4.2743058][-4.0155392 -3.98147 -4.008585 -4.0561147 -4.0382733 -3.9104397 -3.7623711 -3.6910241 -3.7476006 -3.904058 -4.0469322 -4.1298752 -4.1785831 -4.2136431 -4.2363453][-3.9159789 -3.8805873 -3.9184322 -3.9913893 -3.9951174 -3.8593116 -3.680764 -3.5899842 -3.6483333 -3.8310595 -4.0135255 -4.1233087 -4.180799 -4.2163186 -4.241766][-3.92098 -3.891489 -3.9284673 -4.0102015 -4.0375538 -3.9291825 -3.7678545 -3.682467 -3.7328429 -3.9060457 -4.0917649 -4.2031903 -4.2505207 -4.2710552 -4.2856746][-4.074666 -4.0566449 -4.0822315 -4.1478071 -4.1789551 -4.1049175 -3.9846981 -3.9163995 -3.9497793 -4.0841589 -4.2385345 -4.3292718 -4.3557487 -4.3527813 -4.3469367][-4.3026872 -4.2975416 -4.3103623 -4.3468552 -4.3647494 -4.3161197 -4.239017 -4.1940293 -4.2136393 -4.3020282 -4.4070168 -4.4643679 -4.4647045 -4.4359856 -4.4053278][-4.4875531 -4.4930549 -4.49864 -4.5105491 -4.5108867 -4.4756522 -4.4293313 -4.4069571 -4.4257932 -4.4821858 -4.5415645 -4.5641704 -4.5388989 -4.4887991 -4.4392428][-4.5357971 -4.5480409 -4.557127 -4.5654364 -4.5658183 -4.5470567 -4.5231919 -4.5169725 -4.5395494 -4.5782146 -4.6050143 -4.5995893 -4.5568366 -4.4966254 -4.4419489]]...]
INFO - root - 2017-12-07 10:43:18.373072: step 9410, loss = 21.47, batch loss = 21.39 (8.5 examples/sec; 0.941 sec/batch; 84h:26m:57s remains)
INFO - root - 2017-12-07 10:43:27.912080: step 9420, loss = 21.22, batch loss = 21.13 (8.3 examples/sec; 0.967 sec/batch; 86h:44m:51s remains)
INFO - root - 2017-12-07 10:43:37.145045: step 9430, loss = 21.58, batch loss = 21.50 (8.2 examples/sec; 0.973 sec/batch; 87h:21m:02s remains)
INFO - root - 2017-12-07 10:43:46.395817: step 9440, loss = 21.30, batch loss = 21.22 (8.7 examples/sec; 0.919 sec/batch; 82h:30m:37s remains)
INFO - root - 2017-12-07 10:43:55.812835: step 9450, loss = 21.19, batch loss = 21.11 (8.2 examples/sec; 0.978 sec/batch; 87h:44m:52s remains)
INFO - root - 2017-12-07 10:44:05.101199: step 9460, loss = 21.96, batch loss = 21.88 (8.8 examples/sec; 0.911 sec/batch; 81h:45m:01s remains)
INFO - root - 2017-12-07 10:44:14.350684: step 9470, loss = 21.17, batch loss = 21.09 (8.9 examples/sec; 0.898 sec/batch; 80h:33m:50s remains)
INFO - root - 2017-12-07 10:44:23.896836: step 9480, loss = 21.73, batch loss = 21.65 (8.9 examples/sec; 0.898 sec/batch; 80h:34m:57s remains)
INFO - root - 2017-12-07 10:44:33.359870: step 9490, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.951 sec/batch; 85h:21m:19s remains)
INFO - root - 2017-12-07 10:44:42.834507: step 9500, loss = 21.61, batch loss = 21.53 (8.4 examples/sec; 0.948 sec/batch; 85h:02m:24s remains)
2017-12-07 10:44:43.805674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3223238 -4.3189659 -4.3723574 -4.4546213 -4.5177236 -4.4895253 -4.3929338 -4.3325286 -4.3182855 -4.333972 -4.3837819 -4.4348 -4.4426255 -4.4256811 -4.4221797][-4.3874841 -4.3916535 -4.4371395 -4.5077953 -4.5381389 -4.4599652 -4.3179274 -4.2187333 -4.1820035 -4.1918015 -4.2348833 -4.2979422 -4.3689203 -4.4163094 -4.4394035][-4.5252867 -4.5157332 -4.5189843 -4.5554729 -4.5512881 -4.4392991 -4.2679987 -4.1342211 -4.0758448 -4.0948734 -4.146502 -4.222548 -4.3524656 -4.4592962 -4.4971633][-4.6616278 -4.6176968 -4.5557184 -4.5404906 -4.5119905 -4.3982706 -4.2372041 -4.1064057 -4.0502329 -4.0892105 -4.1596661 -4.2366471 -4.3794746 -4.5041966 -4.5329256][-4.6820383 -4.60523 -4.4844232 -4.4162254 -4.3669133 -4.2573218 -4.1150179 -4.0027509 -3.9638643 -4.0437579 -4.1667123 -4.2666249 -4.3991914 -4.5026336 -4.5059109][-4.5794415 -4.4982452 -4.3643074 -4.2658739 -4.1934261 -4.0800214 -3.9500427 -3.8450232 -3.8037419 -3.9040534 -4.0805354 -4.2345939 -4.37226 -4.4433527 -4.4209146][-4.4293184 -4.3672042 -4.2711115 -4.1909056 -4.1207614 -4.0111628 -3.8897223 -3.7796252 -3.7124498 -3.7937717 -3.9933891 -4.2044845 -4.3600984 -4.3995304 -4.34784][-4.3433118 -4.2919316 -4.2373953 -4.1952991 -4.1458473 -4.0490932 -3.9359219 -3.8183417 -3.7215264 -3.7711112 -3.9759927 -4.2374382 -4.4241552 -4.4459743 -4.3675666][-4.3662748 -4.3204851 -4.2937446 -4.2918863 -4.2784953 -4.2181616 -4.135931 -4.0164566 -3.8856783 -3.8797579 -4.0486341 -4.3167181 -4.5193224 -4.5428176 -4.4574132][-4.4410872 -4.4264059 -4.436336 -4.4738011 -4.4958467 -4.4783692 -4.4458809 -4.3464193 -4.1916981 -4.1195064 -4.2148046 -4.4295969 -4.6018286 -4.6206212 -4.5491767][-4.4960613 -4.5228205 -4.5674276 -4.6257248 -4.6532044 -4.6502395 -4.6512175 -4.5925288 -4.458662 -4.36146 -4.3930931 -4.531352 -4.6432252 -4.6446323 -4.5920076][-4.4907513 -4.5447006 -4.6163778 -4.6887994 -4.7145648 -4.7092352 -4.7232738 -4.7054982 -4.61739 -4.531641 -4.5228 -4.5856504 -4.6314478 -4.6111484 -4.5725508][-4.4105315 -4.4685907 -4.5526419 -4.6389751 -4.6787796 -4.684855 -4.7089353 -4.7201872 -4.6766338 -4.6139536 -4.5777297 -4.5730891 -4.5600553 -4.5239563 -4.4961858][-4.3055887 -4.3420835 -4.404469 -4.4760323 -4.5194788 -4.5363245 -4.5610065 -4.5839925 -4.5769687 -4.547 -4.5116658 -4.4772148 -4.4389892 -4.402246 -4.3827662][-4.2345943 -4.2443109 -4.2716327 -4.3072972 -4.3343673 -4.3494434 -4.3667068 -4.3850961 -4.3931746 -4.3877358 -4.3694768 -4.343142 -4.3143449 -4.2921643 -4.2820282]]...]
INFO - root - 2017-12-07 10:44:53.161173: step 9510, loss = 21.00, batch loss = 20.91 (8.3 examples/sec; 0.961 sec/batch; 86h:14m:43s remains)
INFO - root - 2017-12-07 10:45:02.583706: step 9520, loss = 21.31, batch loss = 21.23 (8.0 examples/sec; 1.004 sec/batch; 90h:05m:22s remains)
INFO - root - 2017-12-07 10:45:11.776427: step 9530, loss = 21.79, batch loss = 21.71 (8.3 examples/sec; 0.967 sec/batch; 86h:43m:36s remains)
INFO - root - 2017-12-07 10:45:21.172030: step 9540, loss = 21.67, batch loss = 21.58 (8.6 examples/sec; 0.932 sec/batch; 83h:35m:13s remains)
INFO - root - 2017-12-07 10:45:30.598907: step 9550, loss = 21.34, batch loss = 21.26 (8.7 examples/sec; 0.921 sec/batch; 82h:35m:59s remains)
INFO - root - 2017-12-07 10:45:39.910913: step 9560, loss = 22.02, batch loss = 21.94 (8.4 examples/sec; 0.951 sec/batch; 85h:19m:20s remains)
INFO - root - 2017-12-07 10:45:49.248840: step 9570, loss = 21.15, batch loss = 21.07 (8.4 examples/sec; 0.955 sec/batch; 85h:41m:12s remains)
INFO - root - 2017-12-07 10:45:58.598277: step 9580, loss = 21.25, batch loss = 21.17 (8.6 examples/sec; 0.933 sec/batch; 83h:42m:56s remains)
INFO - root - 2017-12-07 10:46:07.890001: step 9590, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.926 sec/batch; 83h:03m:24s remains)
INFO - root - 2017-12-07 10:46:17.216542: step 9600, loss = 22.06, batch loss = 21.97 (8.9 examples/sec; 0.903 sec/batch; 80h:57m:02s remains)
2017-12-07 10:46:18.122660: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3936071 -4.4497924 -4.4763389 -4.4370184 -4.3365707 -4.2352448 -4.1905084 -4.1847615 -4.1672421 -4.142962 -4.1311841 -4.1359143 -4.1600137 -4.1948042 -4.242878][-4.3917608 -4.4466043 -4.4699478 -4.4282255 -4.3265214 -4.2355928 -4.2100835 -4.2172451 -4.1978583 -4.1567421 -4.1234775 -4.1263995 -4.1766415 -4.2378736 -4.2974448][-4.3501487 -4.4042153 -4.4342337 -4.4075613 -4.3220005 -4.2518044 -4.2480583 -4.2695055 -4.2556133 -4.2157421 -4.1850648 -4.1991544 -4.2692351 -4.342751 -4.3951154][-4.295764 -4.3444581 -4.3815918 -4.371654 -4.3011937 -4.2427392 -4.2426381 -4.2668161 -4.2679987 -4.2551775 -4.2526073 -4.2882462 -4.3655772 -4.4319062 -4.4587173][-4.2375712 -4.2753158 -4.3098321 -4.3064413 -4.2443929 -4.1879735 -4.1743236 -4.1858511 -4.1995807 -4.2213783 -4.2565036 -4.31875 -4.4018707 -4.4630051 -4.4732819][-4.1827326 -4.2179308 -4.2483549 -4.2396259 -4.1747875 -4.1171961 -4.0919161 -4.0873475 -4.1004038 -4.1394248 -4.1958466 -4.277668 -4.3743963 -4.4478946 -4.4674215][-4.176084 -4.212781 -4.2322254 -4.2080255 -4.135509 -4.0872784 -4.0717616 -4.0568666 -4.0544844 -4.0850663 -4.1350846 -4.2185988 -4.3267703 -4.420382 -4.4669275][-4.1710114 -4.2042093 -4.2197042 -4.1890807 -4.117744 -4.0802526 -4.0691185 -4.0308976 -4.0022845 -4.0276866 -4.0861592 -4.1855836 -4.3092647 -4.4176984 -4.4818587][-4.1161165 -4.1517935 -4.1803164 -4.1619539 -4.0983372 -4.0634913 -4.04302 -3.9729056 -3.92253 -3.9614911 -4.056479 -4.1877904 -4.3214684 -4.4276919 -4.4859023][-4.0851283 -4.1206865 -4.1618514 -4.1667271 -4.1221972 -4.0967383 -4.0711031 -3.9775038 -3.91035 -3.9617128 -4.0897341 -4.237751 -4.3623424 -4.4493361 -4.4831467][-4.1275859 -4.1564612 -4.1984663 -4.2176123 -4.1891513 -4.1657724 -4.1329889 -4.0353155 -3.9747233 -4.0449619 -4.1899691 -4.3290563 -4.42832 -4.4897451 -4.5001779][-4.2465873 -4.2585564 -4.28235 -4.2904019 -4.2595053 -4.2318382 -4.2087436 -4.1433878 -4.1137829 -4.1904054 -4.3129172 -4.4105234 -4.4771271 -4.5215969 -4.5236592][-4.4031763 -4.386507 -4.3857322 -4.3794775 -4.3521929 -4.34052 -4.3460131 -4.319943 -4.3078852 -4.3629384 -4.4398937 -4.4954023 -4.5359693 -4.5616479 -4.5533304][-4.5256424 -4.4874063 -4.4772892 -4.4780293 -4.4785304 -4.4966192 -4.5181165 -4.5019689 -4.4828391 -4.5106664 -4.5597029 -4.5979457 -4.6221147 -4.6251636 -4.5956745][-4.6235228 -4.5707297 -4.5527115 -4.5626736 -4.5904646 -4.6303577 -4.6526651 -4.6337767 -4.6050892 -4.6112618 -4.6427975 -4.6760573 -4.6945972 -4.6846595 -4.6368566]]...]
INFO - root - 2017-12-07 10:46:27.478242: step 9610, loss = 21.41, batch loss = 21.33 (8.2 examples/sec; 0.980 sec/batch; 87h:52m:35s remains)
INFO - root - 2017-12-07 10:46:36.643740: step 9620, loss = 20.99, batch loss = 20.91 (8.5 examples/sec; 0.937 sec/batch; 83h:59m:56s remains)
INFO - root - 2017-12-07 10:46:45.871667: step 9630, loss = 21.59, batch loss = 21.51 (8.8 examples/sec; 0.908 sec/batch; 81h:25m:42s remains)
INFO - root - 2017-12-07 10:46:55.323108: step 9640, loss = 21.70, batch loss = 21.62 (8.3 examples/sec; 0.965 sec/batch; 86h:31m:28s remains)
INFO - root - 2017-12-07 10:47:04.551266: step 9650, loss = 21.65, batch loss = 21.57 (8.5 examples/sec; 0.941 sec/batch; 84h:20m:53s remains)
INFO - root - 2017-12-07 10:47:13.818985: step 9660, loss = 21.42, batch loss = 21.34 (8.8 examples/sec; 0.907 sec/batch; 81h:19m:54s remains)
INFO - root - 2017-12-07 10:47:23.186174: step 9670, loss = 21.61, batch loss = 21.52 (8.6 examples/sec; 0.930 sec/batch; 83h:24m:55s remains)
INFO - root - 2017-12-07 10:47:32.490383: step 9680, loss = 21.26, batch loss = 21.18 (9.8 examples/sec; 0.819 sec/batch; 73h:26m:32s remains)
INFO - root - 2017-12-07 10:47:41.865925: step 9690, loss = 21.92, batch loss = 21.84 (8.9 examples/sec; 0.894 sec/batch; 80h:12m:20s remains)
INFO - root - 2017-12-07 10:47:51.205081: step 9700, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.958 sec/batch; 85h:52m:09s remains)
2017-12-07 10:47:52.062026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7049403 -4.754334 -4.7567277 -4.7489438 -4.7369046 -4.7168651 -4.6877 -4.6592464 -4.6429811 -4.6378117 -4.6371107 -4.6229033 -4.5792046 -4.4968896 -4.3926492][-4.7674928 -4.8289561 -4.8279657 -4.8207664 -4.8110337 -4.7866893 -4.7399964 -4.7008634 -4.7027383 -4.73171 -4.7573748 -4.7565565 -4.70755 -4.5997448 -4.455729][-4.7646575 -4.8468695 -4.8388243 -4.8201346 -4.8005161 -4.7604208 -4.6863775 -4.6357255 -4.6759486 -4.7646527 -4.8359656 -4.8608723 -4.8109179 -4.6774082 -4.4984875][-4.6652579 -4.7761126 -4.7685804 -4.7370143 -4.6988616 -4.6236911 -4.4956841 -4.4140954 -4.4943638 -4.658886 -4.8025308 -4.8822613 -4.853344 -4.7077074 -4.5095091][-4.514483 -4.6489577 -4.6535716 -4.6185541 -4.5544095 -4.4213362 -4.2128358 -4.0787635 -4.195302 -4.4457788 -4.6794157 -4.8321786 -4.8421149 -4.7037792 -4.5029287][-4.4445677 -4.5768976 -4.5905647 -4.5433726 -4.4349232 -4.2281122 -3.9251165 -3.7239456 -3.8603683 -4.1809459 -4.4962564 -4.72435 -4.7862306 -4.6788883 -4.4934278][-4.468298 -4.561626 -4.5636806 -4.4885325 -4.325397 -4.0471563 -3.6626487 -3.4049022 -3.5490956 -3.909157 -4.2765913 -4.5661035 -4.690661 -4.6373777 -4.4831958][-4.4959893 -4.5367904 -4.5302577 -4.4418855 -4.2413483 -3.9136093 -3.486829 -3.2150853 -3.3738637 -3.7458959 -4.1292472 -4.4463434 -4.6099567 -4.6001453 -4.4738374][-4.4948254 -4.501029 -4.5094652 -4.4407191 -4.2300839 -3.8889248 -3.4773154 -3.2503934 -3.4306507 -3.7912793 -4.1467934 -4.4386425 -4.5916805 -4.5879254 -4.46974][-4.4432034 -4.4550467 -4.5162554 -4.5040073 -4.3227768 -4.0121732 -3.6510506 -3.4875858 -3.6914434 -4.0273161 -4.324214 -4.5460076 -4.6402664 -4.6027236 -4.4711881][-4.3586655 -4.3979511 -4.5316987 -4.5923905 -4.4674158 -4.2207065 -3.9321933 -3.8333752 -4.0547256 -4.3533974 -4.5741062 -4.7017941 -4.7147665 -4.6247029 -4.4716916][-4.2907209 -4.3596025 -4.5532284 -4.6717658 -4.6047297 -4.4277945 -4.2135525 -4.1588211 -4.359026 -4.5971537 -4.7458053 -4.7999849 -4.7535748 -4.6270657 -4.4623704][-4.2591362 -4.3415055 -4.5546732 -4.7006903 -4.6861486 -4.5787592 -4.4355421 -4.404336 -4.5448408 -4.7013803 -4.7884336 -4.8012147 -4.735086 -4.6015606 -4.4402714][-4.2692857 -4.3389921 -4.5311995 -4.6796708 -4.7125993 -4.6704192 -4.5936012 -4.5807781 -4.6600461 -4.7380395 -4.7716222 -4.7585917 -4.6903315 -4.5616589 -4.4100709][-4.360703 -4.4108291 -4.5541759 -4.6765618 -4.7315512 -4.7354622 -4.7085752 -4.7055383 -4.7381492 -4.7559538 -4.7468777 -4.7153182 -4.6456532 -4.5200686 -4.3783689]]...]
INFO - root - 2017-12-07 10:48:01.245230: step 9710, loss = 21.16, batch loss = 21.08 (9.1 examples/sec; 0.881 sec/batch; 79h:00m:20s remains)
INFO - root - 2017-12-07 10:48:10.649644: step 9720, loss = 21.66, batch loss = 21.57 (8.6 examples/sec; 0.932 sec/batch; 83h:34m:24s remains)
INFO - root - 2017-12-07 10:48:20.055850: step 9730, loss = 21.69, batch loss = 21.60 (9.2 examples/sec; 0.871 sec/batch; 78h:05m:42s remains)
INFO - root - 2017-12-07 10:48:29.519875: step 9740, loss = 21.46, batch loss = 21.37 (8.0 examples/sec; 0.994 sec/batch; 89h:08m:36s remains)
INFO - root - 2017-12-07 10:48:38.909807: step 9750, loss = 21.81, batch loss = 21.73 (8.3 examples/sec; 0.963 sec/batch; 86h:21m:22s remains)
INFO - root - 2017-12-07 10:48:48.252429: step 9760, loss = 21.39, batch loss = 21.31 (8.2 examples/sec; 0.975 sec/batch; 87h:25m:57s remains)
INFO - root - 2017-12-07 10:48:57.579536: step 9770, loss = 21.30, batch loss = 21.22 (8.2 examples/sec; 0.977 sec/batch; 87h:33m:33s remains)
INFO - root - 2017-12-07 10:49:06.959176: step 9780, loss = 20.95, batch loss = 20.87 (8.1 examples/sec; 0.986 sec/batch; 88h:22m:06s remains)
INFO - root - 2017-12-07 10:49:16.210175: step 9790, loss = 20.99, batch loss = 20.91 (8.6 examples/sec; 0.927 sec/batch; 83h:05m:19s remains)
INFO - root - 2017-12-07 10:49:25.529474: step 9800, loss = 21.55, batch loss = 21.47 (8.5 examples/sec; 0.942 sec/batch; 84h:28m:49s remains)
2017-12-07 10:49:26.477534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4263787 -4.4556885 -4.5136852 -4.5722694 -4.6088552 -4.6051455 -4.5803161 -4.5569129 -4.5543675 -4.5816803 -4.6124558 -4.6182766 -4.5941029 -4.5279369 -4.4178576][-4.6488137 -4.6800852 -4.7191606 -4.7426395 -4.7277012 -4.6690645 -4.606699 -4.5761147 -4.589354 -4.6367598 -4.6762595 -4.6774592 -4.6317558 -4.5302267 -4.384397][-4.7856402 -4.8055458 -4.8151555 -4.7970452 -4.7289853 -4.6145349 -4.5128961 -4.4706655 -4.4957542 -4.5621181 -4.6165953 -4.6309791 -4.5962276 -4.5062528 -4.3784928][-4.8077831 -4.8180718 -4.7968321 -4.7288647 -4.6022139 -4.4404492 -4.3237329 -4.294385 -4.3462048 -4.4374766 -4.5071735 -4.5386629 -4.5357933 -4.50151 -4.4454288][-4.7470946 -4.7569451 -4.7152681 -4.6046791 -4.4261279 -4.2351022 -4.134963 -4.1520019 -4.2516136 -4.3775926 -4.4622622 -4.5012612 -4.5248618 -4.5488839 -4.5676241][-4.66937 -4.6847458 -4.6367254 -4.5017242 -4.2895117 -4.0856361 -4.013557 -4.0842328 -4.23688 -4.398253 -4.498795 -4.5427241 -4.5768862 -4.6228948 -4.6659312][-4.606288 -4.6234236 -4.5741005 -4.4323978 -4.2095408 -4.0062509 -3.956846 -4.0660162 -4.2558885 -4.4384432 -4.544282 -4.587008 -4.6191592 -4.65481 -4.6732483][-4.5713606 -4.577177 -4.5223722 -4.3810587 -4.1587448 -3.9579377 -3.9147964 -4.036727 -4.240819 -4.4240751 -4.5193 -4.5552831 -4.5879321 -4.615396 -4.6069422][-4.5487175 -4.5366507 -4.4754329 -4.3379884 -4.1225986 -3.9223456 -3.8690443 -3.9840446 -4.1848388 -4.3566017 -4.440444 -4.4768858 -4.52202 -4.5571175 -4.5394583][-4.4954872 -4.4739494 -4.4200635 -4.3097091 -4.13025 -3.9492936 -3.8909211 -3.9912491 -4.1748581 -4.3235335 -4.3917603 -4.4233241 -4.46832 -4.5072889 -4.4921861][-4.4243155 -4.4085164 -4.3754511 -4.3145156 -4.2026091 -4.0739961 -4.0314016 -4.1134858 -4.2629914 -4.3778782 -4.42159 -4.4322705 -4.4569025 -4.4858556 -4.4724064][-4.3658528 -4.3615522 -4.3468342 -4.3354073 -4.2925277 -4.2269988 -4.2138743 -4.2822723 -4.3962502 -4.4795246 -4.4970946 -4.476 -4.4652796 -4.4663715 -4.4398012][-4.3065367 -4.3040576 -4.3001928 -4.3269043 -4.3369656 -4.3260531 -4.3440461 -4.403934 -4.4887447 -4.5486164 -4.5478125 -4.5005188 -4.4559565 -4.4252143 -4.38274][-4.2582817 -4.24131 -4.2354221 -4.2790713 -4.3203845 -4.3489876 -4.3916821 -4.4471154 -4.5121226 -4.5590625 -4.5534472 -4.4989767 -4.4391761 -4.3901548 -4.3447003][-4.2621241 -4.2222571 -4.2029653 -4.2395287 -4.290029 -4.3430076 -4.4008317 -4.4525852 -4.5086164 -4.5606503 -4.5715094 -4.5342813 -4.4793911 -4.4199996 -4.3666234]]...]
INFO - root - 2017-12-07 10:49:35.850228: step 9810, loss = 21.92, batch loss = 21.83 (8.2 examples/sec; 0.980 sec/batch; 87h:48m:11s remains)
INFO - root - 2017-12-07 10:49:45.083423: step 9820, loss = 21.44, batch loss = 21.36 (8.3 examples/sec; 0.962 sec/batch; 86h:14m:55s remains)
INFO - root - 2017-12-07 10:49:54.540350: step 9830, loss = 21.30, batch loss = 21.22 (8.9 examples/sec; 0.898 sec/batch; 80h:31m:09s remains)
INFO - root - 2017-12-07 10:50:03.811570: step 9840, loss = 21.17, batch loss = 21.08 (8.5 examples/sec; 0.940 sec/batch; 84h:12m:51s remains)
INFO - root - 2017-12-07 10:50:13.190876: step 9850, loss = 21.30, batch loss = 21.21 (8.8 examples/sec; 0.913 sec/batch; 81h:50m:26s remains)
INFO - root - 2017-12-07 10:50:22.642628: step 9860, loss = 21.49, batch loss = 21.41 (8.6 examples/sec; 0.930 sec/batch; 83h:20m:26s remains)
INFO - root - 2017-12-07 10:50:32.057161: step 9870, loss = 21.81, batch loss = 21.72 (8.2 examples/sec; 0.970 sec/batch; 86h:55m:27s remains)
INFO - root - 2017-12-07 10:50:41.559292: step 9880, loss = 21.77, batch loss = 21.69 (8.2 examples/sec; 0.976 sec/batch; 87h:28m:42s remains)
INFO - root - 2017-12-07 10:50:50.967991: step 9890, loss = 21.16, batch loss = 21.08 (8.5 examples/sec; 0.943 sec/batch; 84h:28m:27s remains)
INFO - root - 2017-12-07 10:51:00.314458: step 9900, loss = 21.48, batch loss = 21.40 (8.6 examples/sec; 0.930 sec/batch; 83h:21m:02s remains)
2017-12-07 10:51:01.210301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2436967 -4.2576952 -4.2977161 -4.3666468 -4.4420643 -4.480001 -4.4767962 -4.44244 -4.4006972 -4.3772025 -4.3859272 -4.4198446 -4.4596229 -4.4703565 -4.4367785][-4.1714544 -4.1778879 -4.2103367 -4.2879577 -4.3844347 -4.4438 -4.4545646 -4.4135194 -4.3463216 -4.2982526 -4.3016272 -4.3494482 -4.4125552 -4.4429679 -4.4144974][-4.1290684 -4.1310892 -4.1528 -4.2212558 -4.31592 -4.3896861 -4.4253011 -4.3924885 -4.3005676 -4.2143769 -4.1983862 -4.2538366 -4.3392248 -4.3987789 -4.3922033][-4.1528826 -4.1522923 -4.1609111 -4.2002993 -4.2602973 -4.3232751 -4.375464 -4.3593049 -4.2565289 -4.1364679 -4.1006546 -4.1627245 -4.2694855 -4.3637357 -4.3866463][-4.2180772 -4.2165442 -4.2195735 -4.2310071 -4.2421255 -4.2644734 -4.3041339 -4.2931628 -4.1912918 -4.0614648 -4.0247793 -4.1009316 -4.2295966 -4.3536777 -4.3957939][-4.2831249 -4.2732043 -4.2722626 -4.263761 -4.2210784 -4.1666045 -4.1541128 -4.1312857 -4.0416842 -3.9353805 -3.9291286 -4.0351181 -4.1918449 -4.3414068 -4.3888059][-4.312964 -4.2828779 -4.2817926 -4.2684321 -4.1828485 -4.0404444 -3.9560521 -3.918714 -3.8581126 -3.8049953 -3.8486292 -3.9853754 -4.1599011 -4.3129721 -4.3468094][-4.3444066 -4.290133 -4.2819805 -4.2699981 -4.1603737 -3.9570992 -3.8260398 -3.7930439 -3.7720983 -3.7699118 -3.8442559 -3.9839005 -4.1484804 -4.2830682 -4.3007874][-4.4064374 -4.3410773 -4.3162055 -4.2955937 -4.1861811 -3.9784307 -3.8476598 -3.8377006 -3.849072 -3.8657815 -3.9252443 -4.0283241 -4.155673 -4.2612696 -4.2687449][-4.4341807 -4.3906608 -4.3619676 -4.3371744 -4.25621 -4.1049762 -4.01586 -4.0253639 -4.0489893 -4.0558949 -4.067008 -4.1097245 -4.1861544 -4.2604766 -4.2574353][-4.3864574 -4.39087 -4.3812609 -4.3627696 -4.3191853 -4.2418084 -4.1947985 -4.201045 -4.2224708 -4.2144017 -4.175693 -4.1537752 -4.1837349 -4.2417617 -4.2371778][-4.2797952 -4.3391762 -4.3655219 -4.3601327 -4.3392367 -4.3022342 -4.2640395 -4.2501025 -4.2733054 -4.2676964 -4.1974206 -4.12115 -4.11803 -4.1794305 -4.190311][-4.1824617 -4.28552 -4.3437409 -4.3461828 -4.3254533 -4.286324 -4.2270646 -4.1933441 -4.2377524 -4.2694917 -4.20751 -4.1094165 -4.0896554 -4.1542096 -4.1811047][-4.1952715 -4.3041763 -4.3637733 -4.3551259 -4.3092656 -4.235445 -4.1440945 -4.1011448 -4.1746731 -4.2609191 -4.2477012 -4.1753287 -4.1625729 -4.2263503 -4.263773][-4.3203335 -4.3997769 -4.4280453 -4.391191 -4.3141427 -4.2117858 -4.1075978 -4.0667009 -4.1498089 -4.2709379 -4.317512 -4.3013072 -4.3120327 -4.36875 -4.4089546]]...]
INFO - root - 2017-12-07 10:51:10.536024: step 9910, loss = 21.26, batch loss = 21.17 (9.1 examples/sec; 0.882 sec/batch; 79h:04m:43s remains)
INFO - root - 2017-12-07 10:51:19.816671: step 9920, loss = 21.72, batch loss = 21.63 (8.7 examples/sec; 0.916 sec/batch; 82h:02m:03s remains)
INFO - root - 2017-12-07 10:51:29.200636: step 9930, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.947 sec/batch; 84h:51m:15s remains)
INFO - root - 2017-12-07 10:51:38.422211: step 9940, loss = 21.05, batch loss = 20.96 (8.4 examples/sec; 0.957 sec/batch; 85h:45m:28s remains)
INFO - root - 2017-12-07 10:51:47.809366: step 9950, loss = 21.83, batch loss = 21.75 (8.4 examples/sec; 0.953 sec/batch; 85h:25m:17s remains)
INFO - root - 2017-12-07 10:51:57.283980: step 9960, loss = 21.75, batch loss = 21.67 (8.9 examples/sec; 0.903 sec/batch; 80h:51m:34s remains)
INFO - root - 2017-12-07 10:52:06.614877: step 9970, loss = 21.57, batch loss = 21.49 (8.8 examples/sec; 0.911 sec/batch; 81h:34m:28s remains)
INFO - root - 2017-12-07 10:52:16.002073: step 9980, loss = 21.21, batch loss = 21.13 (8.7 examples/sec; 0.918 sec/batch; 82h:16m:08s remains)
INFO - root - 2017-12-07 10:52:25.330768: step 9990, loss = 21.45, batch loss = 21.37 (8.5 examples/sec; 0.939 sec/batch; 84h:05m:32s remains)
INFO - root - 2017-12-07 10:52:34.712145: step 10000, loss = 21.60, batch loss = 21.52 (8.3 examples/sec; 0.968 sec/batch; 86h:43m:45s remains)
2017-12-07 10:52:35.630884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.476728 -4.541863 -4.5856 -4.6104193 -4.6173263 -4.6039157 -4.5814972 -4.5761447 -4.5892777 -4.6026559 -4.6057887 -4.6015434 -4.5837445 -4.5444655 -4.4787226][-4.5448775 -4.6371341 -4.6949472 -4.7234755 -4.7235923 -4.6917167 -4.6454268 -4.6311765 -4.6637506 -4.7062445 -4.7299995 -4.7396636 -4.7265873 -4.6800776 -4.5884562][-4.5881739 -4.7044072 -4.7728128 -4.7970729 -4.7778416 -4.7144842 -4.6221166 -4.5779128 -4.6318412 -4.7245789 -4.7886806 -4.8198638 -4.8171577 -4.77557 -4.6719656][-4.6147084 -4.7481117 -4.8199134 -4.829258 -4.7760887 -4.6625571 -4.5000916 -4.4037781 -4.4761062 -4.6333656 -4.7579279 -4.8251047 -4.8441176 -4.8188577 -4.7143593][-4.6039591 -4.7252126 -4.7772703 -4.7540421 -4.652894 -4.47975 -4.2470875 -4.1055121 -4.2034984 -4.4363375 -4.6407347 -4.7661772 -4.8260469 -4.8273683 -4.7269549][-4.5417304 -4.6336846 -4.6642828 -4.6176538 -4.4759254 -4.2452903 -3.9491284 -3.7735283 -3.8948207 -4.1925774 -4.4746165 -4.6661105 -4.7726207 -4.7990208 -4.7033687][-4.466928 -4.5453706 -4.5742621 -4.515883 -4.3386965 -4.0527058 -3.7027023 -3.4979141 -3.6287532 -3.9723573 -4.3169703 -4.565115 -4.7066088 -4.7458897 -4.6572366][-4.4209733 -4.5106463 -4.5455527 -4.4792104 -4.2883425 -3.9852824 -3.6230257 -3.4198403 -3.5556636 -3.9160569 -4.2832179 -4.5481539 -4.691915 -4.7137189 -4.6153812][-4.4369974 -4.5652623 -4.6196823 -4.5609241 -4.3825846 -4.1023536 -3.7750535 -3.6028709 -3.7305021 -4.0595341 -4.392735 -4.6249576 -4.7354312 -4.7171621 -4.5969834][-4.4656372 -4.6279712 -4.694654 -4.6441307 -4.4901795 -4.2548347 -3.988251 -3.8606796 -3.9825881 -4.2611523 -4.5354571 -4.7191296 -4.7948546 -4.7465239 -4.6078162][-4.4669824 -4.6330605 -4.7004013 -4.665988 -4.55681 -4.3938046 -4.2168112 -4.14578 -4.2563896 -4.4669304 -4.661654 -4.7839446 -4.8236284 -4.7604203 -4.6151795][-4.4432726 -4.5962152 -4.66924 -4.6646137 -4.6086011 -4.5128722 -4.409903 -4.3730493 -4.4511371 -4.5862517 -4.7012992 -4.7672853 -4.7771516 -4.7096424 -4.5728793][-4.3986239 -4.5288606 -4.606575 -4.6289825 -4.610734 -4.5558167 -4.4946303 -4.471015 -4.5118208 -4.5847645 -4.641099 -4.6690226 -4.6613197 -4.5978713 -4.4825468][-4.3492446 -4.443377 -4.5126872 -4.5475039 -4.5506272 -4.5208526 -4.4827461 -4.4654093 -4.4813876 -4.5129709 -4.5308652 -4.5323987 -4.5145121 -4.4628205 -4.3799539][-4.2975016 -4.3458333 -4.3892984 -4.417254 -4.4260297 -4.4108872 -4.386107 -4.371695 -4.3741689 -4.3841453 -4.3872542 -4.383172 -4.3695045 -4.3382716 -4.2915621]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 10:52:45.680418: step 10010, loss = 21.68, batch loss = 21.60 (9.3 examples/sec; 0.864 sec/batch; 77h:24m:36s remains)
INFO - root - 2017-12-07 10:52:55.046698: step 10020, loss = 21.65, batch loss = 21.57 (9.3 examples/sec; 0.856 sec/batch; 76h:41m:52s remains)
INFO - root - 2017-12-07 10:53:04.439569: step 10030, loss = 21.68, batch loss = 21.60 (8.9 examples/sec; 0.897 sec/batch; 80h:23m:02s remains)
INFO - root - 2017-12-07 10:53:13.699401: step 10040, loss = 21.67, batch loss = 21.59 (8.1 examples/sec; 0.982 sec/batch; 87h:58m:08s remains)
INFO - root - 2017-12-07 10:53:23.099608: step 10050, loss = 21.58, batch loss = 21.50 (8.5 examples/sec; 0.945 sec/batch; 84h:37m:33s remains)
INFO - root - 2017-12-07 10:53:32.429108: step 10060, loss = 21.69, batch loss = 21.60 (9.0 examples/sec; 0.890 sec/batch; 79h:40m:35s remains)
INFO - root - 2017-12-07 10:53:41.764293: step 10070, loss = 21.79, batch loss = 21.71 (8.8 examples/sec; 0.905 sec/batch; 81h:05m:26s remains)
INFO - root - 2017-12-07 10:53:51.154529: step 10080, loss = 21.42, batch loss = 21.34 (8.7 examples/sec; 0.922 sec/batch; 82h:35m:16s remains)
INFO - root - 2017-12-07 10:54:00.517067: step 10090, loss = 21.77, batch loss = 21.68 (8.8 examples/sec; 0.913 sec/batch; 81h:46m:19s remains)
INFO - root - 2017-12-07 10:54:09.870626: step 10100, loss = 21.72, batch loss = 21.64 (8.6 examples/sec; 0.929 sec/batch; 83h:09m:51s remains)
2017-12-07 10:54:10.755180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2845926 -4.2867346 -4.3025341 -4.330533 -4.358645 -4.3932142 -4.4162011 -4.4032063 -4.375298 -4.3451548 -4.3228645 -4.3243647 -4.3512645 -4.3856478 -4.4158983][-4.3106518 -4.3144455 -4.3291054 -4.3494029 -4.3658805 -4.3860769 -4.4057374 -4.4045496 -4.3949652 -4.3874774 -4.3818827 -4.3852115 -4.3943453 -4.40282 -4.41191][-4.3281193 -4.3273649 -4.337317 -4.3458619 -4.3382788 -4.321135 -4.3165154 -4.3291121 -4.3623104 -4.409627 -4.4485822 -4.4678645 -4.4627576 -4.4392323 -4.4125557][-4.3338475 -4.3166018 -4.315506 -4.3114138 -4.2723427 -4.2029934 -4.15789 -4.1814628 -4.2691789 -4.3886127 -4.4886727 -4.5373664 -4.5319676 -4.4872084 -4.4242849][-4.336854 -4.2989721 -4.2827563 -4.2599339 -4.1749835 -4.0371828 -3.9408047 -3.9709373 -4.113975 -4.3047481 -4.4676008 -4.5554662 -4.567832 -4.5245218 -4.4448214][-4.3444719 -4.2940111 -4.262392 -4.2178664 -4.0830793 -3.8779745 -3.7300148 -3.7592275 -3.9438221 -4.1866136 -4.3937969 -4.5107288 -4.5434895 -4.5146379 -4.4437232][-4.3453631 -4.2937808 -4.2583876 -4.2055855 -4.0421381 -3.7916658 -3.5976171 -3.6090779 -3.8106987 -4.0838394 -4.3136654 -4.4387913 -4.4734306 -4.4526129 -4.4051342][-4.3351154 -4.2968073 -4.276123 -4.2372837 -4.080132 -3.8218179 -3.5973995 -3.5658243 -3.7443976 -4.0182371 -4.2521019 -4.3720827 -4.3960042 -4.3732848 -4.3478651][-4.3227673 -4.3060246 -4.3134084 -4.3085546 -4.1962085 -3.9827898 -3.7672651 -3.6941209 -3.8174219 -4.0472121 -4.2479072 -4.3393 -4.3375235 -4.302341 -4.2860589][-4.3025546 -4.3068824 -4.3493004 -4.3941355 -4.3505197 -4.2102985 -4.0322809 -3.9349873 -3.9933779 -4.14827 -4.2865043 -4.33376 -4.3020549 -4.2511592 -4.23299][-4.2827439 -4.2959251 -4.3662462 -4.4607635 -4.4837255 -4.41313 -4.2787814 -4.1749182 -4.1747246 -4.2440271 -4.3072252 -4.3095665 -4.260735 -4.2084956 -4.1925][-4.2948818 -4.2943716 -4.3647709 -4.481555 -4.5482292 -4.5300121 -4.4385509 -4.344861 -4.30445 -4.2985382 -4.2944012 -4.2623153 -4.2101288 -4.1675229 -4.1583271][-4.3342891 -4.3069715 -4.3514705 -4.4569912 -4.5362039 -4.5509663 -4.4997764 -4.4264851 -4.3639 -4.3065662 -4.25875 -4.2141523 -4.1781096 -4.1592641 -4.1680493][-4.3903236 -4.3478856 -4.365129 -4.4377475 -4.5009246 -4.5284843 -4.5097742 -4.4533486 -4.3746481 -4.2870865 -4.2246461 -4.193069 -4.189157 -4.2020493 -4.2328925][-4.4403267 -4.4032 -4.4096713 -4.4537182 -4.4936972 -4.5233941 -4.5274715 -4.4836259 -4.3982296 -4.3029146 -4.2467942 -4.2365 -4.2563329 -4.2857718 -4.3234239]]...]
INFO - root - 2017-12-07 10:54:20.032799: step 10110, loss = 21.63, batch loss = 21.55 (8.5 examples/sec; 0.936 sec/batch; 83h:51m:49s remains)
INFO - root - 2017-12-07 10:54:29.388089: step 10120, loss = 21.94, batch loss = 21.85 (8.2 examples/sec; 0.975 sec/batch; 87h:18m:27s remains)
INFO - root - 2017-12-07 10:54:38.621070: step 10130, loss = 21.40, batch loss = 21.32 (8.9 examples/sec; 0.895 sec/batch; 80h:10m:22s remains)
INFO - root - 2017-12-07 10:54:47.993795: step 10140, loss = 21.34, batch loss = 21.26 (8.8 examples/sec; 0.913 sec/batch; 81h:46m:21s remains)
INFO - root - 2017-12-07 10:54:57.449228: step 10150, loss = 21.40, batch loss = 21.31 (8.7 examples/sec; 0.916 sec/batch; 82h:00m:44s remains)
INFO - root - 2017-12-07 10:55:06.629318: step 10160, loss = 21.36, batch loss = 21.27 (8.3 examples/sec; 0.960 sec/batch; 85h:57m:30s remains)
INFO - root - 2017-12-07 10:55:16.044883: step 10170, loss = 21.42, batch loss = 21.34 (8.5 examples/sec; 0.937 sec/batch; 83h:51m:40s remains)
INFO - root - 2017-12-07 10:55:25.515779: step 10180, loss = 21.37, batch loss = 21.29 (8.8 examples/sec; 0.911 sec/batch; 81h:33m:25s remains)
INFO - root - 2017-12-07 10:55:34.990114: step 10190, loss = 21.48, batch loss = 21.40 (8.3 examples/sec; 0.967 sec/batch; 86h:35m:03s remains)
INFO - root - 2017-12-07 10:55:44.292227: step 10200, loss = 21.55, batch loss = 21.46 (8.3 examples/sec; 0.966 sec/batch; 86h:28m:11s remains)
2017-12-07 10:55:45.282885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6404219 -4.6403432 -4.6278076 -4.6212821 -4.633245 -4.6465054 -4.641305 -4.5897088 -4.518621 -4.4631972 -4.4298596 -4.4021564 -4.3785405 -4.3543329 -4.3381696][-4.6317277 -4.6134777 -4.5763144 -4.5508351 -4.56554 -4.5965514 -4.6035738 -4.5546126 -4.4774876 -4.4019575 -4.3424177 -4.2917914 -4.254261 -4.2269869 -4.2192063][-4.5581384 -4.5172014 -4.4540424 -4.4092207 -4.428297 -4.4805794 -4.5077906 -4.4825449 -4.4312911 -4.3664074 -4.2969832 -4.2328229 -4.1905775 -4.1632876 -4.1598244][-4.4574361 -4.4021444 -4.3256721 -4.2713728 -4.2940378 -4.3609729 -4.3992929 -4.3979883 -4.3831992 -4.3485961 -4.2897291 -4.2237077 -4.1789594 -4.1480246 -4.1407223][-4.3686662 -4.3100348 -4.2367268 -4.1898322 -4.2167397 -4.2764449 -4.2980471 -4.295166 -4.3016868 -4.3017364 -4.268805 -4.2152114 -4.1660028 -4.12371 -4.1099725][-4.3028374 -4.2486734 -4.1876769 -4.1534886 -4.175087 -4.2047482 -4.1857619 -4.1615973 -4.1784325 -4.2103362 -4.2093234 -4.1759367 -4.126575 -4.0772972 -4.064096][-4.24259 -4.2011042 -4.1564326 -4.1274481 -4.1298475 -4.1182985 -4.0558367 -4.0137329 -4.0442691 -4.106308 -4.137764 -4.1299314 -4.0905709 -4.0470953 -4.0454249][-4.1721487 -4.1467361 -4.1166883 -4.0893536 -4.0695047 -4.0271792 -3.9468615 -3.9133515 -3.9686804 -4.0573177 -4.1153374 -4.1308589 -4.1054735 -4.0697527 -4.0790772][-4.097507 -4.0864763 -4.0704679 -4.05342 -4.0308051 -3.988925 -3.9253037 -3.9189348 -3.9926865 -4.0882277 -4.1524215 -4.1735511 -4.1522675 -4.1176662 -4.1308417][-4.0709934 -4.0760179 -4.0772858 -4.0830793 -4.0784016 -4.0555172 -4.0140429 -4.0197558 -4.0822358 -4.1587787 -4.2111192 -4.228323 -4.2092094 -4.1759353 -4.1898961][-4.1099563 -4.1299634 -4.1463742 -4.1705012 -4.18112 -4.17284 -4.1439452 -4.144608 -4.1779661 -4.2230744 -4.2593946 -4.2733221 -4.2598581 -4.2324505 -4.2469826][-4.18629 -4.2164397 -4.2379541 -4.2621078 -4.2716627 -4.2649331 -4.2419181 -4.2368507 -4.2474837 -4.2688856 -4.2934246 -4.3046789 -4.2962132 -4.2747097 -4.2856455][-4.2975788 -4.3266759 -4.3432422 -4.3562574 -4.3566556 -4.345952 -4.3279619 -4.3227744 -4.3244381 -4.3342171 -4.34826 -4.3528814 -4.344523 -4.3257918 -4.3289247][-4.4203982 -4.4420381 -4.4509664 -4.4550562 -4.4519396 -4.4427805 -4.4310212 -4.425611 -4.4218583 -4.4213281 -4.422483 -4.4169779 -4.4053044 -4.3886719 -4.3877225][-4.4888954 -4.5053067 -4.5120382 -4.5141459 -4.5121832 -4.5052166 -4.4948797 -4.4851108 -4.4751205 -4.4670863 -4.4610624 -4.4518647 -4.4408665 -4.4292641 -4.427938]]...]
INFO - root - 2017-12-07 10:55:54.718923: step 10210, loss = 20.99, batch loss = 20.91 (8.3 examples/sec; 0.958 sec/batch; 85h:47m:13s remains)
INFO - root - 2017-12-07 10:56:04.144232: step 10220, loss = 21.26, batch loss = 21.18 (8.6 examples/sec; 0.933 sec/batch; 83h:30m:30s remains)
INFO - root - 2017-12-07 10:56:13.580859: step 10230, loss = 21.62, batch loss = 21.54 (8.6 examples/sec; 0.936 sec/batch; 83h:45m:28s remains)
INFO - root - 2017-12-07 10:56:22.988548: step 10240, loss = 21.49, batch loss = 21.40 (8.6 examples/sec; 0.929 sec/batch; 83h:08m:13s remains)
INFO - root - 2017-12-07 10:56:32.323785: step 10250, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.953 sec/batch; 85h:17m:42s remains)
INFO - root - 2017-12-07 10:56:41.727665: step 10260, loss = 21.57, batch loss = 21.49 (8.5 examples/sec; 0.938 sec/batch; 83h:56m:51s remains)
INFO - root - 2017-12-07 10:56:51.153239: step 10270, loss = 21.27, batch loss = 21.19 (8.6 examples/sec; 0.928 sec/batch; 83h:01m:25s remains)
INFO - root - 2017-12-07 10:57:00.516051: step 10280, loss = 21.30, batch loss = 21.22 (8.4 examples/sec; 0.954 sec/batch; 85h:24m:38s remains)
INFO - root - 2017-12-07 10:57:09.841074: step 10290, loss = 21.44, batch loss = 21.36 (8.9 examples/sec; 0.901 sec/batch; 80h:38m:12s remains)
INFO - root - 2017-12-07 10:57:19.160726: step 10300, loss = 21.31, batch loss = 21.23 (9.0 examples/sec; 0.892 sec/batch; 79h:47m:29s remains)
2017-12-07 10:57:20.053155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3618116 -4.4100037 -4.4702535 -4.5139542 -4.5333538 -4.5338974 -4.5268598 -4.524106 -4.5285273 -4.5335822 -4.5277886 -4.5040913 -4.4649205 -4.4193215 -4.3786263][-4.4020796 -4.4682407 -4.5433826 -4.5918283 -4.5990334 -4.5714235 -4.5283871 -4.4974623 -4.5021567 -4.5373139 -4.5761018 -4.5882211 -4.5585365 -4.4964433 -4.4263368][-4.4545374 -4.533637 -4.6082692 -4.6423759 -4.6180353 -4.5448127 -4.4564714 -4.3945122 -4.4011836 -4.4731054 -4.5688987 -4.6341238 -4.6314254 -4.5648971 -4.471869][-4.5168524 -4.5872974 -4.6377077 -4.6325908 -4.5531154 -4.4211435 -4.2908282 -4.2121129 -4.234158 -4.3486462 -4.499866 -4.6159186 -4.6451478 -4.5879965 -4.4910007][-4.55067 -4.5960827 -4.6118889 -4.5596409 -4.4193125 -4.2254772 -4.0532069 -3.965749 -4.0155497 -4.1829591 -4.3942533 -4.5598407 -4.616025 -4.5696769 -4.4804893][-4.5249949 -4.5541534 -4.5529933 -4.4742522 -4.293539 -4.0484786 -3.8278587 -3.719594 -3.7863698 -3.9982362 -4.267158 -4.4865193 -4.5728865 -4.5402093 -4.4615378][-4.445291 -4.48841 -4.5058088 -4.4362783 -4.24885 -3.9710584 -3.6991804 -3.5530689 -3.6079221 -3.8347449 -4.1408763 -4.4089618 -4.5344319 -4.5229993 -4.4555421][-4.34197 -4.4304047 -4.4984732 -4.4712276 -4.3135018 -4.0377684 -3.7411489 -3.56925 -3.6012697 -3.8096704 -4.1119709 -4.3934255 -4.5364709 -4.5352263 -4.4698944][-4.2521763 -4.3877392 -4.5100565 -4.5363927 -4.4347262 -4.2060761 -3.9347029 -3.7681875 -3.7819381 -3.9524527 -4.2118492 -4.4576259 -4.5776377 -4.5635266 -4.48851][-4.1817613 -4.3423405 -4.4991293 -4.5688591 -4.52623 -4.369885 -4.1593552 -4.0214062 -4.0263824 -4.1607714 -4.3650422 -4.5497365 -4.6242151 -4.5847526 -4.4961557][-4.1530976 -4.3210411 -4.4909167 -4.585743 -4.58787 -4.4989681 -4.3587866 -4.2644124 -4.2733808 -4.3761616 -4.5203557 -4.6318464 -4.6519194 -4.5845523 -4.4856][-4.2117162 -4.3645525 -4.5193038 -4.6139388 -4.6359987 -4.5914125 -4.5082793 -4.4549685 -4.4718146 -4.5468454 -4.6304636 -4.6692657 -4.6387076 -4.551434 -4.4530158][-4.3409634 -4.4537392 -4.5613465 -4.6278372 -4.648787 -4.6290994 -4.5834432 -4.5531592 -4.5665965 -4.6127887 -4.6468439 -4.6359239 -4.5780544 -4.4918666 -4.4110923][-4.4594216 -4.5132728 -4.5582051 -4.5832491 -4.5906844 -4.5844665 -4.5681725 -4.5562034 -4.5641389 -4.5849423 -4.5877514 -4.5567622 -4.49884 -4.432713 -4.3782611][-4.4923477 -4.4950953 -4.4918847 -4.4841948 -4.4771714 -4.4757166 -4.4780121 -4.482327 -4.4914832 -4.50071 -4.4948874 -4.4678364 -4.4266758 -4.3854456 -4.3545842]]...]
INFO - root - 2017-12-07 10:57:29.490270: step 10310, loss = 21.14, batch loss = 21.06 (7.9 examples/sec; 1.010 sec/batch; 90h:22m:46s remains)
INFO - root - 2017-12-07 10:57:38.796428: step 10320, loss = 21.47, batch loss = 21.39 (8.1 examples/sec; 0.986 sec/batch; 88h:11m:58s remains)
INFO - root - 2017-12-07 10:57:48.051062: step 10330, loss = 21.53, batch loss = 21.44 (10.8 examples/sec; 0.740 sec/batch; 66h:15m:23s remains)
INFO - root - 2017-12-07 10:57:57.480136: step 10340, loss = 21.88, batch loss = 21.79 (8.1 examples/sec; 0.987 sec/batch; 88h:21m:18s remains)
INFO - root - 2017-12-07 10:58:06.785418: step 10350, loss = 21.53, batch loss = 21.45 (8.0 examples/sec; 1.001 sec/batch; 89h:32m:48s remains)
INFO - root - 2017-12-07 10:58:16.115654: step 10360, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.951 sec/batch; 85h:06m:55s remains)
INFO - root - 2017-12-07 10:58:25.438698: step 10370, loss = 21.44, batch loss = 21.36 (8.7 examples/sec; 0.924 sec/batch; 82h:42m:21s remains)
INFO - root - 2017-12-07 10:58:34.803566: step 10380, loss = 21.07, batch loss = 20.99 (8.7 examples/sec; 0.917 sec/batch; 82h:04m:15s remains)
INFO - root - 2017-12-07 10:58:44.251370: step 10390, loss = 21.80, batch loss = 21.72 (8.0 examples/sec; 0.996 sec/batch; 89h:07m:13s remains)
INFO - root - 2017-12-07 10:58:53.688983: step 10400, loss = 21.38, batch loss = 21.30 (8.3 examples/sec; 0.960 sec/batch; 85h:56m:04s remains)
2017-12-07 10:58:54.587033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4943032 -4.48007 -4.4725852 -4.4817877 -4.4941025 -4.4993691 -4.4937816 -4.4962215 -4.51189 -4.5193744 -4.5151954 -4.5019555 -4.4882517 -4.4941306 -4.5311713][-4.4964886 -4.4856362 -4.4785161 -4.4812713 -4.4788089 -4.4727259 -4.4618831 -4.47946 -4.5258913 -4.5510869 -4.5407453 -4.5074487 -4.4786215 -4.4952564 -4.5701256][-4.4788976 -4.4820337 -4.4758925 -4.4553933 -4.4115429 -4.371387 -4.3412747 -4.3747935 -4.46036 -4.50429 -4.4821777 -4.4223652 -4.3819685 -4.4233565 -4.5525346][-4.4694767 -4.4866943 -4.47305 -4.4160128 -4.3216648 -4.2386332 -4.1787038 -4.2165866 -4.3311362 -4.3851638 -4.3503094 -4.2693963 -4.2276015 -4.3024149 -4.4886122][-4.458457 -4.4743671 -4.4391003 -4.3457527 -4.2167835 -4.1032381 -4.0159473 -4.0463448 -4.1653547 -4.2151933 -4.1736674 -4.0892415 -4.0594954 -4.1648073 -4.3911738][-4.4477282 -4.4581885 -4.4029808 -4.2879462 -4.1485481 -4.019608 -3.9147234 -3.9329844 -4.0338058 -4.0684266 -4.026166 -3.9515822 -3.9356589 -4.0540242 -4.2916589][-4.4330263 -4.441505 -4.3806682 -4.2677431 -4.1387525 -4.0028424 -3.8866613 -3.890305 -3.96236 -3.9795089 -3.9372923 -3.8721921 -3.8595433 -3.9697647 -4.1956387][-4.4163885 -4.4233484 -4.3664241 -4.2715821 -4.1594176 -4.0200968 -3.8966117 -3.8865814 -3.9337659 -3.9421878 -3.90233 -3.8403196 -3.8195162 -3.9081302 -4.1131577][-4.4169226 -4.4275508 -4.3824635 -4.3100905 -4.2133265 -4.0743008 -3.9471626 -3.9224854 -3.9480672 -3.9485104 -3.9039652 -3.8343456 -3.79709 -3.86478 -4.05609][-4.4218588 -4.443749 -4.4144497 -4.3589897 -4.2737012 -4.1457119 -4.0257649 -3.9899142 -3.9967012 -3.9839106 -3.9215984 -3.8295698 -3.7658641 -3.81246 -3.9991696][-4.4101682 -4.4494662 -4.4417892 -4.4018135 -4.3326235 -4.2293434 -4.1282754 -4.0911479 -4.0862279 -4.0600309 -3.978215 -3.8612416 -3.7699325 -3.7925708 -3.9685624][-4.391314 -4.446651 -4.4626193 -4.443819 -4.3990297 -4.3279724 -4.2535925 -4.2243104 -4.2173877 -4.1849127 -4.0938907 -3.9670408 -3.8632808 -3.8662076 -4.0153461][-4.3812327 -4.4365115 -4.462873 -4.458899 -4.4351048 -4.3906522 -4.3408785 -4.326407 -4.3295889 -4.3073497 -4.2317581 -4.1237416 -4.03472 -4.0310082 -4.1438975][-4.3779564 -4.4206095 -4.4428496 -4.4420357 -4.4275923 -4.3972964 -4.3647318 -4.3622594 -4.3758759 -4.3706484 -4.3265057 -4.258441 -4.2067423 -4.2130251 -4.2944403][-4.3678851 -4.3930249 -4.405962 -4.4045177 -4.3954296 -4.3762612 -4.3573 -4.3596225 -4.3733993 -4.3777075 -4.361073 -4.3307471 -4.3117704 -4.3253889 -4.377944]]...]
INFO - root - 2017-12-07 10:59:03.980562: step 10410, loss = 21.38, batch loss = 21.30 (9.0 examples/sec; 0.893 sec/batch; 79h:52m:09s remains)
INFO - root - 2017-12-07 10:59:13.319930: step 10420, loss = 21.60, batch loss = 21.52 (8.8 examples/sec; 0.905 sec/batch; 80h:56m:48s remains)
INFO - root - 2017-12-07 10:59:22.773466: step 10430, loss = 21.26, batch loss = 21.18 (8.7 examples/sec; 0.918 sec/batch; 82h:06m:13s remains)
INFO - root - 2017-12-07 10:59:32.132607: step 10440, loss = 21.42, batch loss = 21.34 (8.4 examples/sec; 0.949 sec/batch; 84h:52m:50s remains)
INFO - root - 2017-12-07 10:59:41.237994: step 10450, loss = 21.73, batch loss = 21.65 (8.6 examples/sec; 0.928 sec/batch; 83h:00m:40s remains)
INFO - root - 2017-12-07 10:59:50.703484: step 10460, loss = 21.24, batch loss = 21.16 (8.2 examples/sec; 0.979 sec/batch; 87h:36m:01s remains)
INFO - root - 2017-12-07 11:00:00.177288: step 10470, loss = 21.16, batch loss = 21.08 (8.0 examples/sec; 0.998 sec/batch; 89h:17m:17s remains)
INFO - root - 2017-12-07 11:00:09.584571: step 10480, loss = 21.49, batch loss = 21.41 (8.2 examples/sec; 0.977 sec/batch; 87h:21m:58s remains)
INFO - root - 2017-12-07 11:00:19.036644: step 10490, loss = 21.58, batch loss = 21.50 (8.4 examples/sec; 0.947 sec/batch; 84h:43m:30s remains)
INFO - root - 2017-12-07 11:00:28.572707: step 10500, loss = 21.51, batch loss = 21.43 (8.3 examples/sec; 0.961 sec/batch; 85h:58m:50s remains)
2017-12-07 11:00:29.471367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4648929 -4.5153117 -4.5693674 -4.6132989 -4.6407609 -4.64473 -4.6146207 -4.5451841 -4.4493752 -4.34261 -4.2371855 -4.1495533 -4.0953169 -4.0899315 -4.1121116][-4.4182453 -4.4952955 -4.570889 -4.62254 -4.6430416 -4.6269522 -4.5653625 -4.4644151 -4.3571978 -4.2693114 -4.20479 -4.1568718 -4.1262207 -4.1202188 -4.1245184][-4.3580565 -4.4618521 -4.5551457 -4.6101246 -4.6148238 -4.5686727 -4.4765768 -4.3632989 -4.272099 -4.2231889 -4.1998544 -4.1810355 -4.1619854 -4.1472 -4.1362677][-4.268816 -4.3914757 -4.5012827 -4.5674729 -4.5650625 -4.5008945 -4.39431 -4.2838368 -4.2142234 -4.19481 -4.193265 -4.1880851 -4.1752038 -4.1544614 -4.141314][-4.166543 -4.2936211 -4.4184823 -4.4993672 -4.4955626 -4.4267077 -4.3176494 -4.2129469 -4.1616507 -4.1602755 -4.1698766 -4.1735878 -4.1665568 -4.1455555 -4.141788][-4.0898004 -4.2081976 -4.3310571 -4.408896 -4.3934383 -4.322988 -4.2205939 -4.1271644 -4.0969567 -4.1112623 -4.1296439 -4.1423507 -4.1424484 -4.1307864 -4.1446018][-4.067194 -4.1536527 -4.2444568 -4.2903595 -4.248703 -4.1787243 -4.0918961 -4.0168619 -4.0109739 -4.0424366 -4.0730147 -4.1032405 -4.1195602 -4.1279712 -4.1636596][-4.1038809 -4.1348543 -4.1768627 -4.1828589 -4.1182079 -4.0497994 -3.9732177 -3.910181 -3.9250834 -3.9773834 -4.0327477 -4.0943823 -4.135211 -4.1633258 -4.2090135][-4.149374 -4.1270919 -4.128324 -4.1106277 -4.0439444 -3.9882722 -3.9216859 -3.8701856 -3.9021935 -3.9710088 -4.0470924 -4.132545 -4.1873331 -4.2218757 -4.2578459][-4.158463 -4.1128731 -4.0997725 -4.0845709 -4.0459909 -4.0244627 -3.9753256 -3.9362195 -3.9713726 -4.0330672 -4.1030874 -4.1831703 -4.2323456 -4.263402 -4.2848][-4.1570063 -4.1250792 -4.1222234 -4.1281147 -4.1299386 -4.1458 -4.1113434 -4.0749135 -4.088984 -4.1178346 -4.1619878 -4.2152143 -4.2473512 -4.2718477 -4.2853618][-4.1924968 -4.1819158 -4.1882892 -4.2099104 -4.2392097 -4.273757 -4.2434649 -4.1981325 -4.1791263 -4.1715569 -4.1901913 -4.2159362 -4.2343726 -4.2569079 -4.2744594][-4.2623191 -4.2637892 -4.2740183 -4.2970166 -4.328218 -4.353148 -4.3134432 -4.2572069 -4.2133679 -4.1868811 -4.1932087 -4.2002168 -4.210855 -4.236608 -4.2703042][-4.3372307 -4.3510289 -4.372129 -4.3960333 -4.4121876 -4.4073491 -4.348949 -4.283371 -4.2281485 -4.2013478 -4.2055511 -4.2008142 -4.2053661 -4.2354603 -4.289216][-4.3825502 -4.4062476 -4.4406877 -4.4706459 -4.4755111 -4.444253 -4.3714046 -4.3029547 -4.2498255 -4.2277889 -4.2296276 -4.2208552 -4.228302 -4.2659807 -4.3345666]]...]
INFO - root - 2017-12-07 11:00:38.802154: step 10510, loss = 21.88, batch loss = 21.80 (8.2 examples/sec; 0.975 sec/batch; 87h:13m:59s remains)
INFO - root - 2017-12-07 11:00:48.201656: step 10520, loss = 21.46, batch loss = 21.38 (8.4 examples/sec; 0.952 sec/batch; 85h:11m:25s remains)
INFO - root - 2017-12-07 11:00:57.469609: step 10530, loss = 21.29, batch loss = 21.21 (8.9 examples/sec; 0.903 sec/batch; 80h:45m:47s remains)
INFO - root - 2017-12-07 11:01:07.012342: step 10540, loss = 21.57, batch loss = 21.49 (8.6 examples/sec; 0.935 sec/batch; 83h:38m:39s remains)
INFO - root - 2017-12-07 11:01:16.405263: step 10550, loss = 21.17, batch loss = 21.09 (7.9 examples/sec; 1.016 sec/batch; 90h:49m:02s remains)
INFO - root - 2017-12-07 11:01:25.755144: step 10560, loss = 21.34, batch loss = 21.26 (8.2 examples/sec; 0.980 sec/batch; 87h:35m:54s remains)
INFO - root - 2017-12-07 11:01:35.258972: step 10570, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.949 sec/batch; 84h:53m:11s remains)
INFO - root - 2017-12-07 11:01:44.744526: step 10580, loss = 21.22, batch loss = 21.14 (9.1 examples/sec; 0.878 sec/batch; 78h:28m:13s remains)
INFO - root - 2017-12-07 11:01:54.204788: step 10590, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.909 sec/batch; 81h:14m:37s remains)
INFO - root - 2017-12-07 11:02:03.558506: step 10600, loss = 21.72, batch loss = 21.64 (8.4 examples/sec; 0.951 sec/batch; 85h:00m:21s remains)
2017-12-07 11:02:04.533517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4552155 -4.2949591 -4.1265411 -4.0026479 -3.9115653 -3.8313818 -3.7887352 -3.7944293 -3.8868818 -4.0714617 -4.2660909 -4.3998728 -4.4039483 -4.3201556 -4.2522278][-4.4477205 -4.31982 -4.2042918 -4.1267548 -4.0587606 -3.9837561 -3.9352672 -3.9345586 -4.0200744 -4.1810513 -4.3256779 -4.409008 -4.4041061 -4.36726 -4.3664112][-4.4294448 -4.3505807 -4.3118863 -4.3005147 -4.2660909 -4.1996808 -4.1446552 -4.1360168 -4.2042627 -4.3217688 -4.4048634 -4.4398828 -4.4397058 -4.451231 -4.4859667][-4.4153385 -4.3828888 -4.4154897 -4.4615345 -4.4454341 -4.3696241 -4.2930355 -4.2665534 -4.3146548 -4.3928514 -4.4365396 -4.4513493 -4.4680104 -4.5127296 -4.5536985][-4.4158726 -4.4126387 -4.4783063 -4.540339 -4.5075254 -4.3969817 -4.2886786 -4.2512565 -4.3058662 -4.3838186 -4.4268932 -4.4483213 -4.4746337 -4.517343 -4.5379629][-4.4169135 -4.4197536 -4.4691572 -4.4893107 -4.3966923 -4.224916 -4.0713024 -4.0360789 -4.136899 -4.2737336 -4.3686671 -4.4276071 -4.4674568 -4.49385 -4.4827161][-4.3929844 -4.3857603 -4.3905582 -4.3395686 -4.17585 -3.9409595 -3.7426457 -3.7099271 -3.856802 -4.0662551 -4.2408695 -4.3647928 -4.4452882 -4.4785442 -4.4498591][-4.3794036 -4.367002 -4.3384824 -4.2361455 -4.0293479 -3.767746 -3.5590136 -3.5349269 -3.6988473 -3.9350362 -4.1459923 -4.307579 -4.4263792 -4.4882078 -4.4724946][-4.3918662 -4.3839431 -4.3455544 -4.2256627 -4.0115752 -3.7640052 -3.5764413 -3.5698836 -3.7279029 -3.9419413 -4.1422873 -4.3078847 -4.440402 -4.5175767 -4.5159521][-4.4066911 -4.4069562 -4.3765454 -4.2720051 -4.0845847 -3.8768649 -3.7264676 -3.7306581 -3.8615057 -4.0275073 -4.1963515 -4.3548365 -4.4888382 -4.5663266 -4.5643935][-4.4009891 -4.4159722 -4.4143257 -4.3547111 -4.2191191 -4.0658984 -3.9602406 -3.9725604 -4.0701017 -4.1785235 -4.2981882 -4.4320583 -4.5529308 -4.6199989 -4.6117468][-4.3571081 -4.3828955 -4.411797 -4.4014449 -4.3222804 -4.2233396 -4.1610103 -4.1837406 -4.2619205 -4.3305521 -4.406147 -4.5061407 -4.5993705 -4.6433215 -4.62305][-4.2981873 -4.3204708 -4.3608761 -4.3828831 -4.3557477 -4.3144116 -4.2960453 -4.3249145 -4.38169 -4.4208775 -4.4652181 -4.5356574 -4.5965042 -4.6126723 -4.5812545][-4.2631488 -4.2793293 -4.3183517 -4.3543916 -4.3661733 -4.3792171 -4.4037271 -4.4361916 -4.467155 -4.4786205 -4.4963818 -4.5385933 -4.5678239 -4.5601125 -4.5221019][-4.2390046 -4.2545524 -4.2932615 -4.3340354 -4.3635144 -4.4046555 -4.4513021 -4.4811778 -4.4927106 -4.4863906 -4.4929261 -4.5238795 -4.5367384 -4.5153151 -4.4742165]]...]
INFO - root - 2017-12-07 11:02:13.955270: step 10610, loss = 21.56, batch loss = 21.48 (8.6 examples/sec; 0.928 sec/batch; 83h:00m:09s remains)
INFO - root - 2017-12-07 11:02:23.504896: step 10620, loss = 21.82, batch loss = 21.74 (8.4 examples/sec; 0.952 sec/batch; 85h:05m:16s remains)
INFO - root - 2017-12-07 11:02:32.854952: step 10630, loss = 21.54, batch loss = 21.46 (8.3 examples/sec; 0.970 sec/batch; 86h:41m:47s remains)
INFO - root - 2017-12-07 11:02:42.261255: step 10640, loss = 21.06, batch loss = 20.98 (7.8 examples/sec; 1.029 sec/batch; 91h:57m:29s remains)
INFO - root - 2017-12-07 11:02:51.749653: step 10650, loss = 21.74, batch loss = 21.65 (8.8 examples/sec; 0.912 sec/batch; 81h:32m:05s remains)
INFO - root - 2017-12-07 11:03:01.101179: step 10660, loss = 21.65, batch loss = 21.56 (8.4 examples/sec; 0.952 sec/batch; 85h:05m:49s remains)
INFO - root - 2017-12-07 11:03:10.597145: step 10670, loss = 21.26, batch loss = 21.18 (8.3 examples/sec; 0.966 sec/batch; 86h:19m:37s remains)
INFO - root - 2017-12-07 11:03:19.785624: step 10680, loss = 21.26, batch loss = 21.18 (8.7 examples/sec; 0.923 sec/batch; 82h:29m:27s remains)
INFO - root - 2017-12-07 11:03:29.211655: step 10690, loss = 21.26, batch loss = 21.17 (8.6 examples/sec; 0.934 sec/batch; 83h:30m:32s remains)
INFO - root - 2017-12-07 11:03:38.678219: step 10700, loss = 21.59, batch loss = 21.50 (9.0 examples/sec; 0.888 sec/batch; 79h:22m:03s remains)
2017-12-07 11:03:39.586918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0935264 -4.1845007 -4.2514224 -4.2816367 -4.2929931 -4.2904248 -4.2774754 -4.2648582 -4.2560482 -4.2434115 -4.2190647 -4.1797414 -4.1359544 -4.1038108 -4.0988283][-4.1550584 -4.2323265 -4.2880578 -4.3150969 -4.3255382 -4.3231587 -4.3119588 -4.3017712 -4.2942719 -4.2796087 -4.2472148 -4.1946635 -4.1356292 -4.0874152 -4.0709534][-4.2190433 -4.2857347 -4.3332667 -4.3500915 -4.3437119 -4.3252316 -4.3066478 -4.2986856 -4.2965531 -4.28443 -4.2520504 -4.2014012 -4.1492486 -4.1077008 -4.0938835][-4.2668223 -4.3121414 -4.3385577 -4.3312154 -4.2988749 -4.2624846 -4.2426057 -4.2478538 -4.2635217 -4.2655282 -4.2452559 -4.2111897 -4.1822591 -4.16444 -4.1646094][-4.2872777 -4.29683 -4.288589 -4.2526407 -4.2016025 -4.1617303 -4.1559672 -4.1859751 -4.2261558 -4.2475986 -4.2459731 -4.235487 -4.233633 -4.2406945 -4.2547188][-4.2930493 -4.2671828 -4.2264037 -4.1690655 -4.1113672 -4.0801244 -4.0943818 -4.1464405 -4.2041783 -4.2399287 -4.2535615 -4.26221 -4.2792621 -4.3011537 -4.3224306][-4.2864385 -4.2460704 -4.1939545 -4.1324506 -4.079206 -4.058424 -4.0829344 -4.1392579 -4.1958961 -4.2306871 -4.2469707 -4.2617397 -4.2849374 -4.31209 -4.336967][-4.2731938 -4.2412734 -4.1986084 -4.1473918 -4.1039305 -4.0880589 -4.1078644 -4.1497774 -4.18894 -4.2111158 -4.2215843 -4.2345619 -4.2579165 -4.2862687 -4.3127165][-4.2684484 -4.24985 -4.22146 -4.1840138 -4.1501579 -4.1349049 -4.1434097 -4.1656246 -4.1855454 -4.19644 -4.2039843 -4.2178516 -4.2424378 -4.2697773 -4.2920961][-4.2802138 -4.2681351 -4.2495847 -4.2248268 -4.2012858 -4.1878314 -4.1877136 -4.1954656 -4.2029204 -4.2093267 -4.2195444 -4.2380247 -4.2644668 -4.2882681 -4.3016372][-4.3050337 -4.2950311 -4.2826791 -4.2683568 -4.254446 -4.2438641 -4.2386827 -4.2378759 -4.2394319 -4.2460475 -4.2608418 -4.2836962 -4.3102231 -4.3291416 -4.3340921][-4.3321586 -4.3245621 -4.3169513 -4.3102956 -4.3031864 -4.2945771 -4.2861114 -4.2801714 -4.2788863 -4.285984 -4.30229 -4.325079 -4.3487682 -4.3639894 -4.3657408][-4.35547 -4.3501925 -4.3462815 -4.344841 -4.3426986 -4.3368111 -4.3278666 -4.3193722 -4.3157668 -4.3210659 -4.3341351 -4.3511167 -4.3681817 -4.3799958 -4.3830452][-4.3716817 -4.3692069 -4.3676386 -4.3693805 -4.3712134 -4.369575 -4.3633523 -4.354692 -4.3487916 -4.3503289 -4.3576894 -4.366962 -4.3763881 -4.3844323 -4.3896127][-4.3756547 -4.3750596 -4.3743553 -4.37712 -4.3810477 -4.3827825 -4.3794913 -4.3712239 -4.3629408 -4.3597565 -4.3611736 -4.3640151 -4.3673358 -4.3716807 -4.3774414]]...]
INFO - root - 2017-12-07 11:03:48.992657: step 10710, loss = 21.30, batch loss = 21.22 (8.2 examples/sec; 0.973 sec/batch; 86h:56m:12s remains)
INFO - root - 2017-12-07 11:03:58.345941: step 10720, loss = 21.64, batch loss = 21.56 (8.5 examples/sec; 0.942 sec/batch; 84h:13m:39s remains)
INFO - root - 2017-12-07 11:04:07.757880: step 10730, loss = 21.41, batch loss = 21.33 (9.1 examples/sec; 0.879 sec/batch; 78h:32m:57s remains)
INFO - root - 2017-12-07 11:04:17.146338: step 10740, loss = 21.82, batch loss = 21.74 (8.4 examples/sec; 0.947 sec/batch; 84h:39m:50s remains)
INFO - root - 2017-12-07 11:04:26.620042: step 10750, loss = 21.25, batch loss = 21.17 (8.7 examples/sec; 0.919 sec/batch; 82h:08m:05s remains)
INFO - root - 2017-12-07 11:04:35.856530: step 10760, loss = 21.23, batch loss = 21.15 (9.0 examples/sec; 0.885 sec/batch; 79h:05m:34s remains)
INFO - root - 2017-12-07 11:04:45.089913: step 10770, loss = 21.41, batch loss = 21.32 (9.1 examples/sec; 0.878 sec/batch; 78h:27m:38s remains)
INFO - root - 2017-12-07 11:04:54.671123: step 10780, loss = 21.53, batch loss = 21.44 (8.3 examples/sec; 0.963 sec/batch; 86h:04m:23s remains)
INFO - root - 2017-12-07 11:05:04.149492: step 10790, loss = 21.23, batch loss = 21.15 (8.0 examples/sec; 1.004 sec/batch; 89h:43m:17s remains)
INFO - root - 2017-12-07 11:05:13.610465: step 10800, loss = 21.49, batch loss = 21.41 (7.9 examples/sec; 1.017 sec/batch; 90h:54m:54s remains)
2017-12-07 11:05:14.545953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4933558 -4.5002823 -4.5181127 -4.5439692 -4.5638595 -4.5647097 -4.5339451 -4.4834814 -4.443265 -4.4389644 -4.457871 -4.4763703 -4.4762559 -4.4215627 -4.2937231][-4.4829612 -4.5037904 -4.54017 -4.5763779 -4.59174 -4.5793982 -4.5332222 -4.4719152 -4.42877 -4.4244156 -4.442008 -4.46046 -4.4518332 -4.3616438 -4.1897445][-4.4735503 -4.5078626 -4.5514355 -4.579658 -4.5722523 -4.5356112 -4.4719448 -4.4047451 -4.3712354 -4.3854742 -4.4121742 -4.4245167 -4.398756 -4.2885294 -4.113699][-4.4661369 -4.5065885 -4.5414991 -4.5400968 -4.490778 -4.4170136 -4.3329492 -4.2687058 -4.2631183 -4.3223124 -4.3825669 -4.4014997 -4.3742914 -4.2782736 -4.1476326][-4.4593878 -4.4997 -4.515183 -4.4721351 -4.3678579 -4.243269 -4.1333513 -4.0828071 -4.118784 -4.2297616 -4.3319883 -4.37204 -4.3682551 -4.3176303 -4.2460742][-4.4675651 -4.5017715 -4.4905343 -4.4074669 -4.2502937 -4.0672574 -3.923173 -3.8856921 -3.9572697 -4.091567 -4.2074614 -4.2619143 -4.2917304 -4.3025608 -4.2931323][-4.4964137 -4.517664 -4.477222 -4.3684192 -4.1817756 -3.958468 -3.7830493 -3.7515731 -3.8439178 -3.9704871 -4.0625935 -4.1163678 -4.1783223 -4.2477965 -4.2949533][-4.5434146 -4.5453725 -4.4781146 -4.3584981 -4.177011 -3.9615488 -3.7898548 -3.7643828 -3.8575928 -3.9530635 -4.004673 -4.0510545 -4.1318579 -4.232553 -4.3058243][-4.5763259 -4.5661154 -4.4870248 -4.3785038 -4.2378078 -4.08199 -3.9580264 -3.9394307 -4.0011244 -4.0433555 -4.0504618 -4.0891604 -4.175065 -4.2769518 -4.3441291][-4.5587831 -4.5576158 -4.4935827 -4.4155 -4.3275084 -4.2385273 -4.1691031 -4.1553326 -4.1761355 -4.1671486 -4.1421556 -4.17617 -4.258265 -4.3428144 -4.3870883][-4.4934864 -4.5103679 -4.4789691 -4.4425168 -4.407795 -4.3741121 -4.3490362 -4.3430095 -4.3293762 -4.2745738 -4.2232108 -4.2475886 -4.3202062 -4.3866115 -4.4082704][-4.4166207 -4.4537239 -4.4552808 -4.4534712 -4.452611 -4.4444928 -4.44139 -4.4416447 -4.4097261 -4.3273911 -4.2574897 -4.2712736 -4.3388467 -4.3987951 -4.4107985][-4.3700061 -4.415122 -4.422461 -4.4236031 -4.4222517 -4.4109764 -4.4084344 -4.4145293 -4.3933973 -4.3287516 -4.2747188 -4.2933512 -4.3597674 -4.4176588 -4.4289165][-4.3458219 -4.3731675 -4.36541 -4.3521585 -4.3323278 -4.310586 -4.3114133 -4.3361635 -4.3515296 -4.3359385 -4.3202596 -4.3499756 -4.41255 -4.4675684 -4.4822888][-4.2983913 -4.2988324 -4.2884793 -4.2783232 -4.2541409 -4.2379022 -4.2620144 -4.3185406 -4.37241 -4.3971038 -4.406837 -4.4354711 -4.4816327 -4.52327 -4.5365562]]...]
INFO - root - 2017-12-07 11:05:23.890683: step 10810, loss = 21.36, batch loss = 21.27 (8.8 examples/sec; 0.906 sec/batch; 80h:57m:14s remains)
INFO - root - 2017-12-07 11:05:33.341162: step 10820, loss = 21.34, batch loss = 21.25 (8.4 examples/sec; 0.947 sec/batch; 84h:39m:06s remains)
INFO - root - 2017-12-07 11:05:42.596592: step 10830, loss = 21.24, batch loss = 21.16 (8.9 examples/sec; 0.903 sec/batch; 80h:41m:14s remains)
INFO - root - 2017-12-07 11:05:51.939607: step 10840, loss = 21.65, batch loss = 21.57 (8.6 examples/sec; 0.931 sec/batch; 83h:12m:34s remains)
INFO - root - 2017-12-07 11:06:01.186982: step 10850, loss = 21.96, batch loss = 21.88 (8.8 examples/sec; 0.906 sec/batch; 80h:55m:46s remains)
INFO - root - 2017-12-07 11:06:10.428900: step 10860, loss = 21.60, batch loss = 21.52 (9.0 examples/sec; 0.885 sec/batch; 79h:05m:59s remains)
INFO - root - 2017-12-07 11:06:19.856544: step 10870, loss = 21.09, batch loss = 21.00 (8.7 examples/sec; 0.921 sec/batch; 82h:18m:18s remains)
INFO - root - 2017-12-07 11:06:29.243121: step 10880, loss = 21.51, batch loss = 21.43 (9.2 examples/sec; 0.874 sec/batch; 78h:03m:44s remains)
INFO - root - 2017-12-07 11:06:38.589112: step 10890, loss = 21.45, batch loss = 21.37 (8.3 examples/sec; 0.960 sec/batch; 85h:43m:34s remains)
INFO - root - 2017-12-07 11:06:48.004367: step 10900, loss = 21.26, batch loss = 21.18 (8.7 examples/sec; 0.924 sec/batch; 82h:31m:05s remains)
2017-12-07 11:06:48.945417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6766124 -4.6389694 -4.5660057 -4.5480895 -4.5983696 -4.6445508 -4.6431813 -4.5879369 -4.5095749 -4.4518485 -4.3992939 -4.3216238 -4.2765527 -4.3019452 -4.3836813][-4.5571136 -4.5252414 -4.4637461 -4.453826 -4.4993825 -4.5313606 -4.5199904 -4.4688582 -4.4135704 -4.395267 -4.3967328 -4.3712063 -4.352417 -4.3845739 -4.4560051][-4.4317026 -4.4115124 -4.3699217 -4.3739824 -4.4111929 -4.4213514 -4.396677 -4.3512182 -4.3246255 -4.3474469 -4.3979249 -4.4274721 -4.4471207 -4.4950557 -4.5596132][-4.3478465 -4.332417 -4.3018918 -4.3074536 -4.3287807 -4.3157024 -4.2775121 -4.2324238 -4.2236943 -4.2746878 -4.3584971 -4.436121 -4.4987893 -4.5690222 -4.6341724][-4.3339896 -4.3152161 -4.2760324 -4.26487 -4.2623429 -4.2304287 -4.1745725 -4.1111188 -4.0919557 -4.144094 -4.245317 -4.3624711 -4.4673171 -4.5595536 -4.628696][-4.3956194 -4.3686457 -4.295867 -4.2442207 -4.2171769 -4.1791224 -4.1052928 -4.0079284 -3.9516289 -3.9784021 -4.0799828 -4.2211823 -4.3554478 -4.466702 -4.5458803][-4.4416008 -4.42509 -4.3419423 -4.2667251 -4.2259088 -4.1843481 -4.0889673 -3.9426594 -3.8299298 -3.8127785 -3.9084466 -4.0694604 -4.2274213 -4.3586359 -4.453022][-4.4146256 -4.4493661 -4.4076047 -4.3485327 -4.3035426 -4.2488503 -4.12738 -3.9408927 -3.7852325 -3.737041 -3.8358848 -4.0172391 -4.1930995 -4.3349905 -4.4326591][-4.3881221 -4.4682021 -4.4693718 -4.42764 -4.3767014 -4.3117819 -4.1905828 -4.0069823 -3.851326 -3.7998762 -3.9067192 -4.1001372 -4.2809677 -4.421083 -4.50516][-4.3566809 -4.4459543 -4.4710426 -4.4403749 -4.3881693 -4.3334608 -4.252038 -4.1151814 -3.9916828 -3.9515679 -4.0590491 -4.2512627 -4.4250312 -4.552815 -4.6168246][-4.3110337 -4.391952 -4.4283395 -4.4121666 -4.3751211 -4.3564291 -4.3335538 -4.2563949 -4.1681309 -4.132411 -4.2240725 -4.3978372 -4.5478244 -4.6447258 -4.6813111][-4.28773 -4.3526297 -4.3866334 -4.3755918 -4.3570423 -4.3804679 -4.4083138 -4.3766518 -4.3105974 -4.2667561 -4.3298359 -4.4645319 -4.56834 -4.6169882 -4.6230574][-4.2598267 -4.315311 -4.3513007 -4.3405967 -4.3264289 -4.3696904 -4.4166608 -4.4040365 -4.3501658 -4.2957077 -4.3230305 -4.4091377 -4.4662147 -4.4811487 -4.4824109][-4.23845 -4.3015304 -4.3553047 -4.3500342 -4.3288441 -4.3604012 -4.3912082 -4.3713107 -4.3196378 -4.2513986 -4.2361603 -4.2700896 -4.2934179 -4.3081789 -4.3413739][-4.2296667 -4.310708 -4.3847008 -4.39193 -4.3633232 -4.3676758 -4.3675613 -4.3350568 -4.2901316 -4.217721 -4.1618586 -4.1454616 -4.1468792 -4.177825 -4.2539763]]...]
INFO - root - 2017-12-07 11:06:58.324505: step 10910, loss = 21.55, batch loss = 21.47 (8.2 examples/sec; 0.974 sec/batch; 87h:02m:54s remains)
INFO - root - 2017-12-07 11:07:07.904742: step 10920, loss = 21.45, batch loss = 21.37 (8.3 examples/sec; 0.964 sec/batch; 86h:09m:19s remains)
INFO - root - 2017-12-07 11:07:17.308774: step 10930, loss = 21.47, batch loss = 21.39 (8.0 examples/sec; 0.995 sec/batch; 88h:51m:50s remains)
INFO - root - 2017-12-07 11:07:26.634336: step 10940, loss = 21.43, batch loss = 21.34 (8.9 examples/sec; 0.902 sec/batch; 80h:32m:40s remains)
INFO - root - 2017-12-07 11:07:36.170154: step 10950, loss = 21.27, batch loss = 21.18 (8.9 examples/sec; 0.895 sec/batch; 79h:57m:57s remains)
INFO - root - 2017-12-07 11:07:45.692903: step 10960, loss = 21.52, batch loss = 21.44 (9.2 examples/sec; 0.872 sec/batch; 77h:52m:24s remains)
INFO - root - 2017-12-07 11:07:55.061775: step 10970, loss = 21.05, batch loss = 20.97 (8.7 examples/sec; 0.918 sec/batch; 81h:57m:21s remains)
INFO - root - 2017-12-07 11:08:04.535713: step 10980, loss = 21.11, batch loss = 21.03 (8.5 examples/sec; 0.945 sec/batch; 84h:22m:13s remains)
INFO - root - 2017-12-07 11:08:14.061804: step 10990, loss = 21.92, batch loss = 21.84 (8.3 examples/sec; 0.966 sec/batch; 86h:15m:23s remains)
INFO - root - 2017-12-07 11:08:23.485217: step 11000, loss = 21.30, batch loss = 21.22 (8.7 examples/sec; 0.919 sec/batch; 82h:03m:58s remains)
2017-12-07 11:08:24.531864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4867215 -4.5216489 -4.5501432 -4.5627451 -4.5505924 -4.5142112 -4.4586258 -4.4322548 -4.4461408 -4.4580569 -4.4515963 -4.4244676 -4.3793116 -4.3440847 -4.3462811][-4.526145 -4.5736718 -4.6105824 -4.6149349 -4.5825958 -4.5263143 -4.44114 -4.4017224 -4.4340715 -4.4563885 -4.4445291 -4.4003167 -4.3353281 -4.2946625 -4.3080144][-4.5847511 -4.6355491 -4.66474 -4.6442571 -4.5863218 -4.51744 -4.402276 -4.3463717 -4.4016113 -4.4413166 -4.4355149 -4.3877721 -4.3135376 -4.2688971 -4.2824364][-4.6370339 -4.6684127 -4.6681557 -4.6108265 -4.5295072 -4.4614658 -4.3253379 -4.2483015 -4.3200169 -4.3837485 -4.408061 -4.3927746 -4.3395648 -4.2999954 -4.3011322][-4.6674833 -4.6649785 -4.6280241 -4.534997 -4.4332428 -4.3663249 -4.2171187 -4.120101 -4.1972475 -4.2910032 -4.3718219 -4.4220362 -4.4134431 -4.3918872 -4.3815188][-4.6627431 -4.6345158 -4.5698128 -4.4502616 -4.3265367 -4.2432733 -4.0793161 -3.9664855 -4.0405765 -4.1719255 -4.3274312 -4.4524446 -4.4907103 -4.496613 -4.4842763][-4.632494 -4.5855913 -4.4962206 -4.3478074 -4.1860366 -4.0619287 -3.8753662 -3.7510622 -3.8179173 -3.9851685 -4.2203217 -4.4172492 -4.5004935 -4.5485687 -4.5544481][-4.6046038 -4.5467758 -4.44024 -4.2656097 -4.0681581 -3.9060113 -3.7072968 -3.5877762 -3.6548347 -3.8448117 -4.1233387 -4.35039 -4.4509692 -4.5275211 -4.55521][-4.5877566 -4.5378466 -4.4389434 -4.263988 -4.0565372 -3.8846686 -3.6983738 -3.603725 -3.6893997 -3.8829753 -4.1424322 -4.3352494 -4.4066377 -4.4679546 -4.4985805][-4.5895348 -4.5569992 -4.4800038 -4.3151436 -4.1033111 -3.9289269 -3.7590468 -3.6989799 -3.8224802 -4.0209384 -4.2328444 -4.3607635 -4.3796921 -4.3935423 -4.4104905][-4.6057305 -4.5982647 -4.5535889 -4.4092336 -4.2027459 -4.03639 -3.88559 -3.8519535 -4.00502 -4.1896033 -4.3311005 -4.381145 -4.3437719 -4.3097844 -4.3212924][-4.6372604 -4.66704 -4.663022 -4.5419712 -4.3379908 -4.1722546 -4.0322776 -4.0108309 -4.1726961 -4.3324776 -4.4104028 -4.400569 -4.3287578 -4.2761626 -4.2986121][-4.6656346 -4.7201252 -4.7396822 -4.6356368 -4.4373083 -4.265327 -4.1201959 -4.0905657 -4.23586 -4.3706765 -4.4184728 -4.3915677 -4.3254442 -4.2915769 -4.3372293][-4.6555696 -4.7207694 -4.7612109 -4.6977997 -4.542469 -4.387454 -4.2442741 -4.1991143 -4.2957759 -4.3858528 -4.4107518 -4.3843875 -4.3392849 -4.3307147 -4.3904991][-4.6125207 -4.6800127 -4.7403355 -4.7299471 -4.6399574 -4.5236096 -4.4003763 -4.3489585 -4.3908753 -4.4309788 -4.4388366 -4.4214683 -4.3965631 -4.3974175 -4.4475594]]...]
INFO - root - 2017-12-07 11:08:33.867211: step 11010, loss = 21.54, batch loss = 21.46 (8.0 examples/sec; 0.998 sec/batch; 89h:09m:04s remains)
INFO - root - 2017-12-07 11:08:43.254450: step 11020, loss = 21.77, batch loss = 21.69 (7.9 examples/sec; 1.018 sec/batch; 90h:54m:19s remains)
INFO - root - 2017-12-07 11:08:52.617389: step 11030, loss = 21.63, batch loss = 21.54 (8.2 examples/sec; 0.977 sec/batch; 87h:16m:04s remains)
INFO - root - 2017-12-07 11:09:01.892274: step 11040, loss = 21.72, batch loss = 21.64 (8.7 examples/sec; 0.918 sec/batch; 81h:55m:47s remains)
INFO - root - 2017-12-07 11:09:11.318647: step 11050, loss = 21.54, batch loss = 21.46 (8.3 examples/sec; 0.959 sec/batch; 85h:38m:14s remains)
INFO - root - 2017-12-07 11:09:20.699006: step 11060, loss = 21.19, batch loss = 21.11 (8.4 examples/sec; 0.956 sec/batch; 85h:23m:06s remains)
INFO - root - 2017-12-07 11:09:29.944312: step 11070, loss = 21.42, batch loss = 21.33 (9.1 examples/sec; 0.880 sec/batch; 78h:34m:40s remains)
INFO - root - 2017-12-07 11:09:39.381299: step 11080, loss = 21.83, batch loss = 21.74 (8.9 examples/sec; 0.902 sec/batch; 80h:33m:50s remains)
INFO - root - 2017-12-07 11:09:48.821428: step 11090, loss = 21.68, batch loss = 21.59 (8.7 examples/sec; 0.925 sec/batch; 82h:33m:36s remains)
INFO - root - 2017-12-07 11:09:58.254494: step 11100, loss = 21.50, batch loss = 21.42 (8.6 examples/sec; 0.928 sec/batch; 82h:52m:00s remains)
2017-12-07 11:09:59.246137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6102476 -4.6978974 -4.7646208 -4.801 -4.79648 -4.7402897 -4.6520281 -4.5860305 -4.5686526 -4.5847268 -4.5942907 -4.5896072 -4.5788774 -4.5667739 -4.5537724][-4.6774225 -4.7938972 -4.8751359 -4.9153814 -4.9033122 -4.8258362 -4.7074485 -4.6133666 -4.5863609 -4.6174145 -4.6553411 -4.672648 -4.6696177 -4.6538568 -4.6330194][-4.6572118 -4.7744784 -4.8521643 -4.8864031 -4.8614054 -4.7628155 -4.6207781 -4.5093856 -4.487988 -4.5519557 -4.6374011 -4.6937232 -4.7112885 -4.7076669 -4.6925535][-4.5830493 -4.6736174 -4.7324462 -4.7505817 -4.7023873 -4.5729146 -4.4051256 -4.2930098 -4.3065281 -4.4275603 -4.5715313 -4.6690745 -4.704421 -4.706327 -4.6878624][-4.4972467 -4.5512829 -4.5837054 -4.5665054 -4.4665933 -4.27826 -4.0726533 -3.9716196 -4.0463696 -4.2473135 -4.456708 -4.5921378 -4.6345782 -4.6247435 -4.5868673][-4.4395013 -4.4753437 -4.4898615 -4.4323716 -4.2661028 -4.0019245 -3.7425611 -3.6408648 -3.7663848 -4.0389104 -4.3071637 -4.4763513 -4.5222425 -4.4905066 -4.4267154][-4.4188657 -4.4598947 -4.4683414 -4.3863173 -4.1778984 -3.8629992 -3.5616562 -3.4433064 -3.5800905 -3.8815439 -4.180635 -4.3762503 -4.4270067 -4.3711033 -4.2796826][-4.4300508 -4.4847956 -4.4951739 -4.4147182 -4.2104774 -3.8972781 -3.5875649 -3.4480684 -3.559268 -3.8424006 -4.1402059 -4.3484354 -4.4006343 -4.3131137 -4.1772556][-4.4680586 -4.525167 -4.5244703 -4.4497075 -4.2847672 -4.0326076 -3.7713976 -3.6336925 -3.704118 -3.9310944 -4.1892629 -4.3786354 -4.4157672 -4.2916851 -4.1080189][-4.4907107 -4.545681 -4.5269475 -4.4486823 -4.3274035 -4.1667552 -4.0035906 -3.9098806 -3.9569874 -4.1166382 -4.3029189 -4.43437 -4.4331946 -4.2840767 -4.0841613][-4.463757 -4.5235538 -4.5011773 -4.422473 -4.3401194 -4.2697525 -4.2145715 -4.1885109 -4.2417178 -4.3544407 -4.4625268 -4.5078015 -4.4478374 -4.287004 -4.1106606][-4.4108057 -4.4796877 -4.4721136 -4.4076967 -4.3553767 -4.3459563 -4.36582 -4.3940425 -4.4590936 -4.5390878 -4.5791883 -4.5460415 -4.436069 -4.2828584 -4.1566648][-4.3667283 -4.4333162 -4.4364772 -4.3863072 -4.35056 -4.3695579 -4.4220762 -4.4757051 -4.5354419 -4.5759239 -4.5632772 -4.4844122 -4.3615565 -4.242157 -4.1779633][-4.35176 -4.3987408 -4.3930149 -4.3393135 -4.3002553 -4.324512 -4.3914223 -4.45662 -4.4958367 -4.4885979 -4.4329324 -4.3448296 -4.2510328 -4.1874967 -4.1801538][-4.3563237 -4.384851 -4.36355 -4.3021445 -4.2558608 -4.2753882 -4.3467975 -4.418303 -4.4393215 -4.3925261 -4.3060303 -4.2233362 -4.1709948 -4.1621814 -4.19665]]...]
INFO - root - 2017-12-07 11:10:08.622097: step 11110, loss = 22.13, batch loss = 22.05 (8.5 examples/sec; 0.939 sec/batch; 83h:49m:29s remains)
INFO - root - 2017-12-07 11:10:17.861499: step 11120, loss = 21.53, batch loss = 21.44 (8.7 examples/sec; 0.917 sec/batch; 81h:51m:30s remains)
INFO - root - 2017-12-07 11:10:27.358860: step 11130, loss = 21.50, batch loss = 21.42 (7.7 examples/sec; 1.044 sec/batch; 93h:14m:26s remains)
INFO - root - 2017-12-07 11:10:36.613998: step 11140, loss = 21.91, batch loss = 21.83 (8.8 examples/sec; 0.907 sec/batch; 80h:59m:37s remains)
INFO - root - 2017-12-07 11:10:46.015430: step 11150, loss = 21.46, batch loss = 21.38 (8.4 examples/sec; 0.956 sec/batch; 85h:18m:03s remains)
INFO - root - 2017-12-07 11:10:55.431073: step 11160, loss = 21.25, batch loss = 21.17 (8.5 examples/sec; 0.939 sec/batch; 83h:47m:13s remains)
INFO - root - 2017-12-07 11:11:04.849996: step 11170, loss = 21.62, batch loss = 21.53 (8.9 examples/sec; 0.898 sec/batch; 80h:07m:28s remains)
INFO - root - 2017-12-07 11:11:14.308992: step 11180, loss = 21.42, batch loss = 21.34 (8.8 examples/sec; 0.908 sec/batch; 81h:02m:23s remains)
INFO - root - 2017-12-07 11:11:23.789060: step 11190, loss = 21.44, batch loss = 21.35 (9.2 examples/sec; 0.872 sec/batch; 77h:50m:48s remains)
INFO - root - 2017-12-07 11:11:33.081036: step 11200, loss = 21.44, batch loss = 21.36 (8.4 examples/sec; 0.958 sec/batch; 85h:27m:58s remains)
2017-12-07 11:11:33.985149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.582231 -4.5051475 -4.4603529 -4.4415169 -4.4265914 -4.3994823 -4.3746681 -4.35803 -4.3482475 -4.3353224 -4.3137035 -4.2957482 -4.2952108 -4.2935166 -4.2832661][-4.597281 -4.5591025 -4.5461941 -4.5331478 -4.4948568 -4.4320674 -4.3802571 -4.3535991 -4.3512096 -4.3574314 -4.3576427 -4.3566742 -4.3569694 -4.34298 -4.3160806][-4.5806785 -4.5903015 -4.6056352 -4.5838037 -4.5061231 -4.4017072 -4.3266592 -4.3011765 -4.3251367 -4.3728833 -4.4119678 -4.4353113 -4.4411821 -4.4209976 -4.3791723][-4.5490785 -4.5971136 -4.6269565 -4.5843291 -4.4645476 -4.3229461 -4.2302928 -4.2102613 -4.2638893 -4.3579769 -4.4411926 -4.4908452 -4.5073466 -4.4891858 -4.4402447][-4.5187049 -4.5785747 -4.5950255 -4.5194135 -4.3661318 -4.2059011 -4.1079273 -4.0941081 -4.1666303 -4.2903328 -4.4029565 -4.4698319 -4.4956269 -4.4901886 -4.4567952][-4.501369 -4.5440493 -4.5213284 -4.4011841 -4.2168179 -4.0484457 -3.9529824 -3.9484768 -4.0392432 -4.1827779 -4.3116503 -4.3852615 -4.412385 -4.4226336 -4.4312358][-4.4947948 -4.5152593 -4.4573312 -4.3019242 -4.0921125 -3.911031 -3.8086791 -3.8073578 -3.9190526 -4.0822959 -4.2199464 -4.292129 -4.3096905 -4.330853 -4.3881927][-4.4851742 -4.5020785 -4.4417944 -4.2868032 -4.0773158 -3.8919601 -3.7771037 -3.7700107 -3.8958471 -4.0681839 -4.2005949 -4.2555227 -4.2460508 -4.2549305 -4.3345523][-4.4838243 -4.5196424 -4.4955363 -4.3844757 -4.2107396 -4.0398359 -3.9164171 -3.890604 -3.9980381 -4.148797 -4.2516146 -4.2674417 -4.2146282 -4.1893086 -4.2586875][-4.4897389 -4.55548 -4.58463 -4.5368948 -4.4158792 -4.2682471 -4.1374478 -4.0781322 -4.128005 -4.2142038 -4.2571273 -4.2161512 -4.115046 -4.0595818 -4.1233535][-4.480804 -4.5650921 -4.6361227 -4.64659 -4.5834956 -4.4716549 -4.3480835 -4.2615242 -4.2442818 -4.2455993 -4.2106814 -4.102901 -3.9570742 -3.8893919 -3.9710138][-4.458025 -4.5378337 -4.6209855 -4.6660805 -4.6514997 -4.5863042 -4.4969864 -4.4189081 -4.3677211 -4.3105612 -4.2163315 -4.0595183 -3.8841994 -3.8178499 -3.9257085][-4.4386182 -4.4979568 -4.5668855 -4.614542 -4.6243472 -4.6013961 -4.56489 -4.532414 -4.4953427 -4.4270577 -4.3133025 -4.1480145 -3.9788682 -3.926311 -4.0410938][-4.4215531 -4.4597569 -4.5058823 -4.5398917 -4.5559254 -4.5621562 -4.5738058 -4.5960488 -4.6004014 -4.5571342 -4.461041 -4.32136 -4.1800761 -4.13093 -4.2100077][-4.4008193 -4.41879 -4.4418907 -4.4596906 -4.4750938 -4.4988413 -4.5414052 -4.6006303 -4.6441 -4.6400633 -4.5844355 -4.4886212 -4.3774481 -4.3112779 -4.3162484]]...]
INFO - root - 2017-12-07 11:11:43.368044: step 11210, loss = 21.15, batch loss = 21.06 (8.6 examples/sec; 0.929 sec/batch; 82h:52m:23s remains)
INFO - root - 2017-12-07 11:11:52.716696: step 11220, loss = 21.61, batch loss = 21.52 (9.2 examples/sec; 0.868 sec/batch; 77h:27m:20s remains)
INFO - root - 2017-12-07 11:12:02.174782: step 11230, loss = 21.36, batch loss = 21.28 (8.5 examples/sec; 0.939 sec/batch; 83h:45m:45s remains)
INFO - root - 2017-12-07 11:12:11.636293: step 11240, loss = 20.98, batch loss = 20.90 (7.9 examples/sec; 1.012 sec/batch; 90h:20m:03s remains)
INFO - root - 2017-12-07 11:12:20.998921: step 11250, loss = 21.39, batch loss = 21.31 (7.8 examples/sec; 1.023 sec/batch; 91h:14m:49s remains)
INFO - root - 2017-12-07 11:12:30.299373: step 11260, loss = 21.08, batch loss = 21.00 (8.2 examples/sec; 0.972 sec/batch; 86h:43m:23s remains)
INFO - root - 2017-12-07 11:12:39.761140: step 11270, loss = 21.22, batch loss = 21.14 (8.2 examples/sec; 0.974 sec/batch; 86h:56m:09s remains)
INFO - root - 2017-12-07 11:12:49.108416: step 11280, loss = 21.95, batch loss = 21.87 (8.4 examples/sec; 0.958 sec/batch; 85h:27m:59s remains)
INFO - root - 2017-12-07 11:12:58.434519: step 11290, loss = 21.40, batch loss = 21.32 (8.5 examples/sec; 0.945 sec/batch; 84h:20m:12s remains)
INFO - root - 2017-12-07 11:13:07.771472: step 11300, loss = 21.58, batch loss = 21.50 (8.6 examples/sec; 0.932 sec/batch; 83h:07m:47s remains)
2017-12-07 11:13:08.731955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4945259 -4.5367212 -4.5640187 -4.5381813 -4.4979196 -4.4923496 -4.5041146 -4.4970007 -4.4579163 -4.3948569 -4.326057 -4.2743025 -4.2702284 -4.3155084 -4.3756566][-4.5050497 -4.5640249 -4.6347184 -4.6393113 -4.5658269 -4.4757862 -4.4304338 -4.4319119 -4.4388542 -4.3999166 -4.3248506 -4.2639842 -4.2611728 -4.3176208 -4.387629][-4.4249592 -4.4963794 -4.6137457 -4.6717634 -4.6001453 -4.4510221 -4.3508878 -4.3538718 -4.4048586 -4.387991 -4.29792 -4.2141852 -4.20935 -4.280139 -4.3622842][-4.27246 -4.3566465 -4.51022 -4.61814 -4.5628576 -4.3781581 -4.2408772 -4.2531819 -4.3564076 -4.3786526 -4.2896376 -4.1989303 -4.2009978 -4.2792206 -4.3577018][-4.1259637 -4.2290735 -4.4083915 -4.5390038 -4.48012 -4.2637835 -4.1011963 -4.12659 -4.2795391 -4.35336 -4.2902594 -4.2193851 -4.2376547 -4.3139648 -4.3753729][-4.0791407 -4.2130041 -4.4030809 -4.5102029 -4.3990541 -4.128592 -3.9337215 -3.9587135 -4.140234 -4.2568226 -4.2365794 -4.2137222 -4.264843 -4.3423786 -4.3885584][-4.1484022 -4.3037367 -4.4678426 -4.4913158 -4.2731013 -3.9250982 -3.6955383 -3.726191 -3.9375079 -4.0997419 -4.1326137 -4.1663213 -4.2530279 -4.3372049 -4.3765955][-4.2587857 -4.407146 -4.5108218 -4.4268856 -4.1055183 -3.7072206 -3.4814744 -3.5461597 -3.789119 -3.9753711 -4.0421786 -4.1211033 -4.2417192 -4.3359537 -4.371202][-4.3600578 -4.47078 -4.5120888 -4.3639669 -4.0129642 -3.6396263 -3.4694712 -3.5727158 -3.8075829 -3.9592819 -4.0125513 -4.1052275 -4.24866 -4.3589807 -4.3951011][-4.4601135 -4.5211506 -4.5300236 -4.3866138 -4.0806656 -3.7823927 -3.6766617 -3.7833166 -3.9598653 -4.0409384 -4.0543313 -4.13824 -4.2856207 -4.4038033 -4.437767][-4.5236063 -4.5472074 -4.5621185 -4.4774427 -4.2569814 -4.0329709 -3.9571357 -4.035491 -4.1452327 -4.1721554 -4.1625347 -4.2344604 -4.3689408 -4.47304 -4.4867463][-4.5098538 -4.517983 -4.5547123 -4.5355654 -4.4073224 -4.2528815 -4.1914182 -4.2390113 -4.3051586 -4.3100724 -4.2957654 -4.3470821 -4.4506345 -4.5239754 -4.5059776][-4.4049029 -4.4223247 -4.4911976 -4.524776 -4.4729357 -4.383306 -4.3397665 -4.3753009 -4.4303451 -4.4428658 -4.4320054 -4.4549088 -4.5168862 -4.5483756 -4.4944453][-4.2836685 -4.3346305 -4.4412823 -4.5134387 -4.5105433 -4.4631062 -4.4257298 -4.4542222 -4.5174208 -4.554018 -4.55531 -4.55046 -4.5631633 -4.5465479 -4.4646521][-4.2394848 -4.3359289 -4.4738169 -4.5631166 -4.5714836 -4.5280118 -4.4767733 -4.4911685 -4.5585079 -4.61466 -4.6286016 -4.6043453 -4.5766134 -4.5251794 -4.4381342]]...]
INFO - root - 2017-12-07 11:13:18.184860: step 11310, loss = 21.02, batch loss = 20.94 (8.2 examples/sec; 0.973 sec/batch; 86h:51m:12s remains)
INFO - root - 2017-12-07 11:13:27.460998: step 11320, loss = 20.86, batch loss = 20.78 (8.9 examples/sec; 0.896 sec/batch; 79h:55m:09s remains)
INFO - root - 2017-12-07 11:13:36.956026: step 11330, loss = 21.45, batch loss = 21.37 (8.9 examples/sec; 0.900 sec/batch; 80h:16m:17s remains)
INFO - root - 2017-12-07 11:13:46.420474: step 11340, loss = 21.18, batch loss = 21.10 (8.6 examples/sec; 0.926 sec/batch; 82h:36m:07s remains)
INFO - root - 2017-12-07 11:13:55.702021: step 11350, loss = 21.59, batch loss = 21.51 (8.8 examples/sec; 0.906 sec/batch; 80h:49m:13s remains)
INFO - root - 2017-12-07 11:14:05.029861: step 11360, loss = 21.83, batch loss = 21.75 (8.8 examples/sec; 0.906 sec/batch; 80h:48m:03s remains)
INFO - root - 2017-12-07 11:14:14.349623: step 11370, loss = 21.38, batch loss = 21.30 (8.7 examples/sec; 0.923 sec/batch; 82h:21m:25s remains)
INFO - root - 2017-12-07 11:14:23.592204: step 11380, loss = 21.75, batch loss = 21.67 (8.2 examples/sec; 0.979 sec/batch; 87h:18m:01s remains)
INFO - root - 2017-12-07 11:14:33.203308: step 11390, loss = 21.25, batch loss = 21.17 (8.2 examples/sec; 0.975 sec/batch; 86h:59m:02s remains)
INFO - root - 2017-12-07 11:14:42.431424: step 11400, loss = 21.38, batch loss = 21.30 (8.5 examples/sec; 0.941 sec/batch; 83h:54m:34s remains)
2017-12-07 11:14:43.268021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4766207 -4.4911914 -4.5031686 -4.5097508 -4.5136328 -4.5135574 -4.5058608 -4.4978418 -4.5096989 -4.5472293 -4.587471 -4.6105251 -4.6025252 -4.5559177 -4.4782915][-4.5615606 -4.5983825 -4.6267138 -4.6386023 -4.6366353 -4.625545 -4.6062517 -4.5932689 -4.6226764 -4.6910052 -4.7540016 -4.7854662 -4.7708125 -4.7020988 -4.5816326][-4.6062279 -4.675148 -4.7232184 -4.734302 -4.7122574 -4.6720648 -4.6221609 -4.5948725 -4.6521807 -4.7741976 -4.8833251 -4.9370513 -4.9220171 -4.833343 -4.6726112][-4.594399 -4.6935568 -4.7537074 -4.7508507 -4.6903906 -4.5961485 -4.4868574 -4.4307318 -4.5289564 -4.7365246 -4.9262962 -5.0265851 -5.0190272 -4.9130359 -4.7186294][-4.5538459 -4.6634636 -4.7120981 -4.6732659 -4.5590196 -4.3905582 -4.1958227 -4.0921106 -4.2293763 -4.54021 -4.834374 -5.0027175 -5.0213542 -4.9128914 -4.7012095][-4.5270829 -4.6154494 -4.6258392 -4.5361013 -4.3615956 -4.11309 -3.8282027 -3.6712494 -3.8345068 -4.2372818 -4.6329827 -4.8750043 -4.9322062 -4.8393006 -4.6323972][-4.5357695 -4.57651 -4.534409 -4.399404 -4.1751776 -3.8640208 -3.5129189 -3.319443 -3.5011477 -3.9674561 -4.43269 -4.7226362 -4.8084188 -4.7384491 -4.5508895][-4.5594993 -4.53663 -4.4510136 -4.3035769 -4.0737796 -3.7508183 -3.3852887 -3.1806512 -3.35998 -3.8342967 -4.3162 -4.6175995 -4.7157388 -4.6650276 -4.4988265][-4.58747 -4.5010314 -4.386488 -4.2638307 -4.0855303 -3.8139219 -3.4838428 -3.292697 -3.4584265 -3.901886 -4.3585839 -4.6390944 -4.7226915 -4.6650453 -4.498076][-4.6182942 -4.4785395 -4.3468142 -4.2683692 -4.176785 -3.9930048 -3.7244725 -3.5651846 -3.7290328 -4.1410012 -4.556747 -4.7885785 -4.8233075 -4.7239003 -4.5318027][-4.6284695 -4.4561448 -4.3173313 -4.2811193 -4.2812891 -4.1968622 -4.003231 -3.8884902 -4.050806 -4.4201045 -4.77519 -4.9405079 -4.9170179 -4.776196 -4.5638142][-4.6328807 -4.4637623 -4.3331261 -4.3228693 -4.3847561 -4.38059 -4.2660847 -4.1987209 -4.341012 -4.6391859 -4.9104357 -5.0072813 -4.9403481 -4.7807131 -4.5734582][-4.6189489 -4.49159 -4.3912144 -4.3923512 -4.4668441 -4.5004725 -4.4506526 -4.42008 -4.5174479 -4.7150135 -4.8900232 -4.9375834 -4.8663864 -4.7273045 -4.5549245][-4.5499611 -4.480082 -4.4246531 -4.4327941 -4.4887595 -4.5235868 -4.5117626 -4.5003004 -4.5443897 -4.6422815 -4.7356548 -4.764576 -4.7238021 -4.6340461 -4.5139771][-4.4602785 -4.4314504 -4.4148951 -4.4298558 -4.4655652 -4.4904232 -4.4937897 -4.4891129 -4.4993653 -4.5362263 -4.5817866 -4.6058249 -4.5930996 -4.5422835 -4.4637222]]...]
INFO - root - 2017-12-07 11:14:52.691920: step 11410, loss = 21.47, batch loss = 21.38 (8.7 examples/sec; 0.924 sec/batch; 82h:25m:41s remains)
INFO - root - 2017-12-07 11:15:02.165086: step 11420, loss = 21.25, batch loss = 21.16 (8.9 examples/sec; 0.897 sec/batch; 80h:02m:17s remains)
INFO - root - 2017-12-07 11:15:11.608129: step 11430, loss = 21.90, batch loss = 21.81 (8.5 examples/sec; 0.943 sec/batch; 84h:04m:01s remains)
INFO - root - 2017-12-07 11:15:21.030601: step 11440, loss = 21.21, batch loss = 21.12 (8.8 examples/sec; 0.913 sec/batch; 81h:23m:19s remains)
INFO - root - 2017-12-07 11:15:30.389015: step 11450, loss = 20.72, batch loss = 20.64 (8.5 examples/sec; 0.942 sec/batch; 84h:00m:33s remains)
INFO - root - 2017-12-07 11:15:39.689457: step 11460, loss = 21.08, batch loss = 21.00 (8.7 examples/sec; 0.923 sec/batch; 82h:18m:29s remains)
INFO - root - 2017-12-07 11:15:49.045666: step 11470, loss = 21.39, batch loss = 21.30 (8.3 examples/sec; 0.964 sec/batch; 85h:57m:02s remains)
INFO - root - 2017-12-07 11:15:58.336328: step 11480, loss = 21.14, batch loss = 21.05 (8.8 examples/sec; 0.907 sec/batch; 80h:51m:42s remains)
INFO - root - 2017-12-07 11:16:07.711277: step 11490, loss = 21.64, batch loss = 21.56 (9.2 examples/sec; 0.870 sec/batch; 77h:34m:44s remains)
INFO - root - 2017-12-07 11:16:16.932057: step 11500, loss = 21.61, batch loss = 21.53 (9.2 examples/sec; 0.874 sec/batch; 77h:55m:30s remains)
2017-12-07 11:16:17.862021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3566232 -4.3952088 -4.4428372 -4.4821119 -4.5019169 -4.4865985 -4.4525089 -4.4358363 -4.4431844 -4.4591017 -4.4728284 -4.4691849 -4.4555407 -4.4483404 -4.4382443][-4.4103384 -4.46042 -4.5100031 -4.5336218 -4.5263286 -4.4785833 -4.4221954 -4.4049249 -4.4291263 -4.4741726 -4.5151281 -4.5192337 -4.4955621 -4.4778423 -4.4606748][-4.4632807 -4.5193434 -4.562932 -4.5659661 -4.5300856 -4.4500904 -4.362956 -4.333365 -4.3696165 -4.4472904 -4.5253615 -4.5504761 -4.5328465 -4.5255494 -4.5182166][-4.5024781 -4.5560961 -4.5900536 -4.5741358 -4.5096717 -4.3875923 -4.2469878 -4.1811047 -4.2215133 -4.3316684 -4.45034 -4.5123429 -4.5264416 -4.5542016 -4.5734572][-4.5076241 -4.5479774 -4.5657153 -4.5308747 -4.4349113 -4.2590351 -4.0528965 -3.9397397 -3.980078 -4.1195946 -4.2763658 -4.38795 -4.4541159 -4.524931 -4.5721006][-4.4774151 -4.4889836 -4.4771819 -4.4178805 -4.288938 -4.0624218 -3.8061304 -3.6620936 -3.7070932 -3.8820281 -4.0827904 -4.2470202 -4.3602762 -4.453249 -4.5059361][-4.4297042 -4.4106765 -4.3709955 -4.293426 -4.1470356 -3.9007993 -3.6335688 -3.4878895 -3.542197 -3.7559047 -4.005898 -4.2107573 -4.3413 -4.41721 -4.4433255][-4.3980556 -4.3720388 -4.331862 -4.2670326 -4.1412516 -3.9211504 -3.6825829 -3.557301 -3.6156967 -3.8445845 -4.1183028 -4.3242593 -4.4305444 -4.46223 -4.4513836][-4.4127479 -4.4123645 -4.4022808 -4.372263 -4.2884927 -4.1158452 -3.91684 -3.8128889 -3.8661122 -4.075839 -4.3241906 -4.4871955 -4.5461674 -4.5416088 -4.5165582][-4.4697876 -4.5042615 -4.5230923 -4.5205755 -4.4699287 -4.3406377 -4.1844544 -4.1067133 -4.1523852 -4.3161793 -4.49835 -4.5968618 -4.61609 -4.6022248 -4.5894094][-4.534059 -4.5842128 -4.6110806 -4.6166348 -4.5825763 -4.492485 -4.3898664 -4.351862 -4.3959308 -4.5007305 -4.5975327 -4.63641 -4.6362915 -4.6283131 -4.6283612][-4.5574455 -4.6042366 -4.6251969 -4.6255841 -4.5984378 -4.5444045 -4.4973326 -4.5000896 -4.5450325 -4.6002722 -4.6312151 -4.6325674 -4.6233668 -4.6151052 -4.6094894][-4.5163746 -4.5573225 -4.5764265 -4.5765862 -4.559731 -4.537 -4.5299368 -4.5516896 -4.5853977 -4.6069751 -4.6071792 -4.5940428 -4.5773497 -4.5583353 -4.5359726][-4.4380684 -4.4742126 -4.498075 -4.5075951 -4.5059261 -4.5030422 -4.5082006 -4.5206532 -4.5306311 -4.5293512 -4.5187483 -4.5036163 -4.4850802 -4.4618163 -4.4342813][-4.3651981 -4.3894215 -4.410253 -4.4236879 -4.4293337 -4.4298544 -4.428793 -4.4252582 -4.4183106 -4.4077034 -4.3963323 -4.3855872 -4.3747463 -4.3623495 -4.3484974]]...]
INFO - root - 2017-12-07 11:16:27.202749: step 11510, loss = 21.40, batch loss = 21.31 (8.0 examples/sec; 1.006 sec/batch; 89h:43m:20s remains)
INFO - root - 2017-12-07 11:16:36.380787: step 11520, loss = 21.24, batch loss = 21.16 (8.5 examples/sec; 0.942 sec/batch; 84h:01m:56s remains)
INFO - root - 2017-12-07 11:16:45.906207: step 11530, loss = 21.69, batch loss = 21.60 (8.6 examples/sec; 0.926 sec/batch; 82h:33m:31s remains)
INFO - root - 2017-12-07 11:16:55.226356: step 11540, loss = 21.28, batch loss = 21.19 (8.4 examples/sec; 0.951 sec/batch; 84h:48m:05s remains)
INFO - root - 2017-12-07 11:17:04.460966: step 11550, loss = 21.31, batch loss = 21.23 (8.4 examples/sec; 0.948 sec/batch; 84h:31m:32s remains)
INFO - root - 2017-12-07 11:17:13.921518: step 11560, loss = 21.57, batch loss = 21.49 (8.8 examples/sec; 0.910 sec/batch; 81h:07m:40s remains)
INFO - root - 2017-12-07 11:17:23.395512: step 11570, loss = 21.30, batch loss = 21.22 (8.8 examples/sec; 0.911 sec/batch; 81h:13m:52s remains)
INFO - root - 2017-12-07 11:17:32.796369: step 11580, loss = 21.80, batch loss = 21.71 (9.0 examples/sec; 0.893 sec/batch; 79h:34m:00s remains)
INFO - root - 2017-12-07 11:17:42.074377: step 11590, loss = 21.06, batch loss = 20.98 (8.4 examples/sec; 0.948 sec/batch; 84h:31m:12s remains)
INFO - root - 2017-12-07 11:17:51.608122: step 11600, loss = 21.42, batch loss = 21.34 (8.1 examples/sec; 0.985 sec/batch; 87h:47m:46s remains)
2017-12-07 11:17:52.572614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3741755 -4.3862982 -4.3982091 -4.4054403 -4.40662 -4.406631 -4.40952 -4.4162097 -4.4238381 -4.4296288 -4.4330759 -4.4346375 -4.43707 -4.4412947 -4.4460235][-4.4113483 -4.42902 -4.4425406 -4.4465609 -4.4414039 -4.4344945 -4.4331865 -4.439487 -4.4516253 -4.4664907 -4.4812865 -4.4943895 -4.5075369 -4.5235338 -4.5417733][-4.4564433 -4.4751682 -4.480927 -4.4706635 -4.449738 -4.4305992 -4.4241295 -4.4332771 -4.4559007 -4.4854045 -4.5132627 -4.5338478 -4.5506878 -4.574163 -4.609159][-4.5074563 -4.5141382 -4.4959259 -4.4564543 -4.409956 -4.3754592 -4.3658113 -4.3820705 -4.4191546 -4.4618 -4.4903131 -4.4969783 -4.4958024 -4.5132017 -4.5636196][-4.5503211 -4.5334058 -4.4807429 -4.4051747 -4.3299961 -4.2816792 -4.2731018 -4.3007288 -4.3514695 -4.3950229 -4.3990221 -4.3622422 -4.3237648 -4.3324881 -4.4038825][-4.5569324 -4.5194216 -4.4426427 -4.342351 -4.2465692 -4.1882081 -4.1815815 -4.219121 -4.27659 -4.3030839 -4.2563467 -4.155695 -4.079371 -4.0965304 -4.2046661][-4.5023923 -4.4684157 -4.3984542 -4.2990766 -4.1978168 -4.1325455 -4.1226997 -4.1624 -4.2170572 -4.2132921 -4.1024551 -3.931915 -3.8261318 -3.8660152 -4.0190573][-4.4038515 -4.4038634 -4.3678403 -4.28911 -4.1938014 -4.1218019 -4.0999961 -4.1324639 -4.1812129 -4.1527181 -3.9928081 -3.7689486 -3.6390164 -3.6902089 -3.868439][-4.3098907 -4.35883 -4.3618555 -4.30616 -4.2173562 -4.1349959 -4.0934296 -4.1130772 -4.1607275 -4.1314678 -3.9634392 -3.7254725 -3.5812082 -3.6173277 -3.7822938][-4.2445455 -4.3393564 -4.3726544 -4.3347068 -4.250001 -4.1546564 -4.0910549 -4.0967402 -4.1488214 -4.1435986 -4.0142112 -3.8131843 -3.6798887 -3.6894927 -3.8099878][-4.1991191 -4.3256016 -4.3818359 -4.3605723 -4.2821436 -4.1780224 -4.0976133 -4.0903945 -4.144165 -4.1695347 -4.1042304 -3.9793494 -3.889497 -3.8897407 -3.9628875][-4.1875672 -4.3205404 -4.3886337 -4.3806949 -4.3120971 -4.2109981 -4.1273012 -4.1106315 -4.1570759 -4.201447 -4.1955204 -4.1528959 -4.1235027 -4.1357141 -4.1759496][-4.2349758 -4.3514895 -4.4133592 -4.4071121 -4.3467636 -4.2582626 -4.184442 -4.1642632 -4.1983142 -4.2463622 -4.2765813 -4.291276 -4.3082967 -4.3355341 -4.3583326][-4.3378186 -4.4201188 -4.4566317 -4.4376264 -4.3799229 -4.3080864 -4.2525229 -4.2374797 -4.2631593 -4.3063216 -4.3467269 -4.3799534 -4.4102969 -4.4384527 -4.4524055][-4.437573 -4.4793367 -4.4838691 -4.4499345 -4.3965154 -4.3437376 -4.309845 -4.3067832 -4.331007 -4.36681 -4.3995342 -4.4227462 -4.4398789 -4.4543176 -4.4605474]]...]
INFO - root - 2017-12-07 11:18:01.956227: step 11610, loss = 21.33, batch loss = 21.25 (8.3 examples/sec; 0.961 sec/batch; 85h:40m:48s remains)
INFO - root - 2017-12-07 11:18:11.320548: step 11620, loss = 21.48, batch loss = 21.40 (8.5 examples/sec; 0.936 sec/batch; 83h:26m:34s remains)
INFO - root - 2017-12-07 11:18:20.855750: step 11630, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.907 sec/batch; 80h:48m:09s remains)
INFO - root - 2017-12-07 11:18:30.311241: step 11640, loss = 21.20, batch loss = 21.12 (9.0 examples/sec; 0.884 sec/batch; 78h:47m:19s remains)
INFO - root - 2017-12-07 11:18:39.769836: step 11650, loss = 21.33, batch loss = 21.24 (9.0 examples/sec; 0.888 sec/batch; 79h:10m:54s remains)
INFO - root - 2017-12-07 11:18:49.190736: step 11660, loss = 21.49, batch loss = 21.40 (8.5 examples/sec; 0.939 sec/batch; 83h:41m:37s remains)
INFO - root - 2017-12-07 11:18:58.606797: step 11670, loss = 21.39, batch loss = 21.31 (8.0 examples/sec; 0.997 sec/batch; 88h:51m:33s remains)
INFO - root - 2017-12-07 11:19:07.857442: step 11680, loss = 21.25, batch loss = 21.16 (8.4 examples/sec; 0.947 sec/batch; 84h:24m:50s remains)
INFO - root - 2017-12-07 11:19:17.119347: step 11690, loss = 21.22, batch loss = 21.14 (8.6 examples/sec; 0.927 sec/batch; 82h:35m:58s remains)
INFO - root - 2017-12-07 11:19:26.415858: step 11700, loss = 21.24, batch loss = 21.16 (8.9 examples/sec; 0.896 sec/batch; 79h:49m:05s remains)
2017-12-07 11:19:27.377932: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.111577 -4.0838308 -4.1432505 -4.24849 -4.3465772 -4.380497 -4.3116879 -4.211885 -4.1604447 -4.1951604 -4.2843089 -4.3555818 -4.3848915 -4.3889108 -4.3658662][-4.0926771 -4.0638275 -4.1382265 -4.262331 -4.3716736 -4.4069018 -4.3388433 -4.2412481 -4.1889491 -4.2258778 -4.3266649 -4.4187994 -4.4642215 -4.46911 -4.4318504][-4.0942163 -4.0713787 -4.1512933 -4.2800622 -4.3907552 -4.4265761 -4.3670726 -4.28027 -4.2313557 -4.2652392 -4.3630767 -4.4671783 -4.5338244 -4.5491943 -4.4993877][-4.1016688 -4.0818167 -4.1471953 -4.2575364 -4.3526359 -4.3826327 -4.3341432 -4.2625284 -4.2202473 -4.2451706 -4.3332644 -4.4529443 -4.5555105 -4.5989952 -4.5523434][-4.0819268 -4.0639987 -4.1077237 -4.1794753 -4.2319584 -4.23118 -4.1804118 -4.1318045 -4.1169991 -4.1517596 -4.24611 -4.3898678 -4.5383716 -4.6147647 -4.5689578][-4.0322862 -4.011302 -4.0295696 -4.0478234 -4.0309978 -3.9702969 -3.9010978 -3.8903399 -3.9331012 -4.0091677 -4.1314778 -4.304996 -4.49221 -4.5882931 -4.5347009][-4.0031552 -3.9786012 -3.9790421 -3.9526949 -3.8694661 -3.7449803 -3.6496558 -3.6723764 -3.7682931 -3.8821712 -4.029563 -4.2235847 -4.4293756 -4.5295949 -4.4670091][-4.038167 -4.0124702 -4.0124679 -3.9765692 -3.8746464 -3.734751 -3.6417167 -3.6819506 -3.7886245 -3.8944206 -4.0288577 -4.2154288 -4.4124193 -4.5040035 -4.4357171][-4.143106 -4.1166906 -4.1252 -4.1021166 -4.0160289 -3.900588 -3.8343511 -3.8771374 -3.9605374 -4.02982 -4.1298962 -4.2883716 -4.4587817 -4.5383606 -4.4749951][-4.2846951 -4.2659698 -4.2854319 -4.2823343 -4.2242265 -4.1392612 -4.0868883 -4.1041346 -4.1390448 -4.1626949 -4.224144 -4.3466916 -4.485817 -4.5537581 -4.5116749][-4.4215775 -4.4152174 -4.439467 -4.4455042 -4.408627 -4.3473763 -4.2959456 -4.2694678 -4.2400026 -4.2084565 -4.229176 -4.3210053 -4.4445496 -4.5232778 -4.5265889][-4.5434141 -4.5367632 -4.5453348 -4.5414729 -4.5102239 -4.4614053 -4.4052744 -4.3346796 -4.2450914 -4.1658583 -4.1610403 -4.2492657 -4.3870864 -4.4965358 -4.5555806][-4.609899 -4.5868945 -4.564065 -4.540606 -4.5086894 -4.4669838 -4.4187169 -4.3342752 -4.2180557 -4.1229362 -4.1155043 -4.2133656 -4.3639684 -4.4917388 -4.5822315][-4.5883346 -4.5387111 -4.4880347 -4.457191 -4.4378347 -4.4128137 -4.3866563 -4.316627 -4.20325 -4.1152658 -4.1190262 -4.2210264 -4.3615937 -4.4810414 -4.5725684][-4.4994926 -4.4298549 -4.3754625 -4.3628707 -4.3720422 -4.3683729 -4.3637409 -4.3164597 -4.2238135 -4.1563325 -4.167016 -4.2467976 -4.3452406 -4.4299831 -4.4965134]]...]
INFO - root - 2017-12-07 11:19:36.599658: step 11710, loss = 21.90, batch loss = 21.81 (8.5 examples/sec; 0.945 sec/batch; 84h:13m:55s remains)
INFO - root - 2017-12-07 11:19:45.917585: step 11720, loss = 21.61, batch loss = 21.53 (8.0 examples/sec; 0.997 sec/batch; 88h:48m:08s remains)
INFO - root - 2017-12-07 11:19:55.132489: step 11730, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.978 sec/batch; 87h:07m:17s remains)
INFO - root - 2017-12-07 11:20:04.676077: step 11740, loss = 21.32, batch loss = 21.23 (7.9 examples/sec; 1.018 sec/batch; 90h:44m:01s remains)
INFO - root - 2017-12-07 11:20:14.080163: step 11750, loss = 21.53, batch loss = 21.44 (8.2 examples/sec; 0.980 sec/batch; 87h:17m:12s remains)
INFO - root - 2017-12-07 11:20:23.563858: step 11760, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.946 sec/batch; 84h:18m:35s remains)
INFO - root - 2017-12-07 11:20:33.007157: step 11770, loss = 21.61, batch loss = 21.53 (9.3 examples/sec; 0.864 sec/batch; 76h:59m:18s remains)
INFO - root - 2017-12-07 11:20:42.346168: step 11780, loss = 21.35, batch loss = 21.26 (9.1 examples/sec; 0.877 sec/batch; 78h:06m:45s remains)
INFO - root - 2017-12-07 11:20:51.600695: step 11790, loss = 21.19, batch loss = 21.11 (8.5 examples/sec; 0.939 sec/batch; 83h:41m:16s remains)
INFO - root - 2017-12-07 11:21:00.981136: step 11800, loss = 21.37, batch loss = 21.29 (8.8 examples/sec; 0.905 sec/batch; 80h:35m:26s remains)
2017-12-07 11:21:01.963745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6514339 -4.6458268 -4.5337682 -4.4342175 -4.4313278 -4.469717 -4.4677248 -4.4181442 -4.3781013 -4.4067974 -4.4707675 -4.4626384 -4.3604021 -4.2348995 -4.2000866][-4.57572 -4.5213857 -4.3725572 -4.2578244 -4.29678 -4.4259076 -4.5006032 -4.4876866 -4.4541926 -4.466001 -4.5156307 -4.5044746 -4.4168034 -4.3204503 -4.2969747][-4.4848442 -4.3933496 -4.2477975 -4.1545143 -4.2332072 -4.4194622 -4.5336919 -4.5263305 -4.4800181 -4.4691806 -4.512423 -4.5273771 -4.4879642 -4.4421329 -4.4314637][-4.3910584 -4.2993107 -4.1977773 -4.1463056 -4.2398906 -4.4230714 -4.5269771 -4.5082278 -4.4449339 -4.4132695 -4.4586987 -4.5120645 -4.5239353 -4.5129871 -4.5032606][-4.2984371 -4.2331691 -4.1812191 -4.1606541 -4.2386727 -4.3685188 -4.4242458 -4.3907948 -4.3306074 -4.3114209 -4.3792081 -4.471734 -4.5229745 -4.5258474 -4.5081043][-4.2498527 -4.2288013 -4.2184076 -4.2082205 -4.2403736 -4.2707939 -4.2318397 -4.1630626 -4.1285825 -4.1623225 -4.2767863 -4.4048491 -4.4780269 -4.48209 -4.455863][-4.2182736 -4.2417946 -4.2630186 -4.257792 -4.2373242 -4.1582866 -4.0129833 -3.9044271 -3.9144967 -4.0271506 -4.1838522 -4.3149505 -4.3822632 -4.3800788 -4.3575158][-4.1264815 -4.1818452 -4.23359 -4.2405024 -4.1772184 -4.0131445 -3.798831 -3.6852794 -3.7641516 -3.9633904 -4.1452265 -4.2533622 -4.2970729 -4.2870007 -4.2785745][-4.0478024 -4.1202216 -4.1989145 -4.2188334 -4.1201763 -3.906208 -3.6786225 -3.5999479 -3.7533867 -4.0272226 -4.2229271 -4.2990494 -4.3035603 -4.2701859 -4.2607141][-4.0900722 -4.155673 -4.2483163 -4.2857432 -4.18077 -3.9653668 -3.7620206 -3.7147617 -3.8916464 -4.1780977 -4.3588514 -4.399652 -4.3684158 -4.3124018 -4.2918305][-4.1886187 -4.2403116 -4.346189 -4.4138074 -4.3354497 -4.143857 -3.9607551 -3.9096098 -4.0490475 -4.2870927 -4.4338784 -4.4582105 -4.4224243 -4.376442 -4.3573852][-4.2797461 -4.3232355 -4.4352646 -4.5194845 -4.4703341 -4.3067412 -4.1417155 -4.0812783 -4.17171 -4.342804 -4.4514818 -4.4732041 -4.4586129 -4.4446521 -4.4322133][-4.3759308 -4.4144936 -4.5105667 -4.5788713 -4.5444937 -4.4174066 -4.2870207 -4.2373624 -4.2970681 -4.4116673 -4.4856191 -4.5032644 -4.50523 -4.5142016 -4.4983315][-4.4592047 -4.5040808 -4.576889 -4.6159248 -4.5869412 -4.4946094 -4.3956571 -4.355463 -4.4007058 -4.4825015 -4.5341206 -4.541152 -4.5451441 -4.5660233 -4.5505285][-4.524857 -4.5785885 -4.6355376 -4.658381 -4.6366558 -4.5651522 -4.4803767 -4.443356 -4.4813881 -4.5442829 -4.5714593 -4.5533581 -4.547061 -4.5773115 -4.57554]]...]
INFO - root - 2017-12-07 11:21:11.367114: step 11810, loss = 21.36, batch loss = 21.28 (8.7 examples/sec; 0.918 sec/batch; 81h:49m:07s remains)
INFO - root - 2017-12-07 11:21:20.831182: step 11820, loss = 21.29, batch loss = 21.21 (8.5 examples/sec; 0.944 sec/batch; 84h:07m:44s remains)
INFO - root - 2017-12-07 11:21:30.304225: step 11830, loss = 21.22, batch loss = 21.14 (8.3 examples/sec; 0.967 sec/batch; 86h:10m:16s remains)
INFO - root - 2017-12-07 11:21:39.620586: step 11840, loss = 21.77, batch loss = 21.68 (8.2 examples/sec; 0.970 sec/batch; 86h:22m:52s remains)
INFO - root - 2017-12-07 11:21:49.002571: step 11850, loss = 21.48, batch loss = 21.39 (7.9 examples/sec; 1.010 sec/batch; 89h:59m:37s remains)
INFO - root - 2017-12-07 11:21:58.361071: step 11860, loss = 20.88, batch loss = 20.80 (8.4 examples/sec; 0.956 sec/batch; 85h:09m:08s remains)
INFO - root - 2017-12-07 11:22:07.798858: step 11870, loss = 21.23, batch loss = 21.14 (8.3 examples/sec; 0.966 sec/batch; 86h:02m:34s remains)
INFO - root - 2017-12-07 11:22:17.214076: step 11880, loss = 21.39, batch loss = 21.31 (8.2 examples/sec; 0.972 sec/batch; 86h:35m:07s remains)
INFO - root - 2017-12-07 11:22:26.227520: step 11890, loss = 21.33, batch loss = 21.25 (9.9 examples/sec; 0.811 sec/batch; 72h:15m:21s remains)
INFO - root - 2017-12-07 11:22:35.685938: step 11900, loss = 21.17, batch loss = 21.08 (8.3 examples/sec; 0.967 sec/batch; 86h:05m:45s remains)
2017-12-07 11:22:36.657032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.482995 -4.5514154 -4.5671735 -4.513052 -4.428124 -4.3775196 -4.3872705 -4.4537168 -4.5429306 -4.6047096 -4.6236267 -4.6214457 -4.6215491 -4.6336179 -4.6552062][-4.4758306 -4.544765 -4.5653543 -4.5250154 -4.4616504 -4.4355373 -4.4623284 -4.5322924 -4.61392 -4.6673493 -4.6865449 -4.6923771 -4.6944103 -4.7012644 -4.7132897][-4.4645357 -4.5150557 -4.51917 -4.4770508 -4.4228706 -4.4049573 -4.4309998 -4.4934106 -4.5670166 -4.62155 -4.6533518 -4.6759429 -4.6865287 -4.6922684 -4.6981654][-4.4496169 -4.472981 -4.4556937 -4.4109206 -4.3601146 -4.3305936 -4.33243 -4.3687029 -4.4283257 -4.4846387 -4.5285549 -4.5676336 -4.5880671 -4.5954866 -4.6030464][-4.4450493 -4.4459224 -4.4109125 -4.3562388 -4.2870836 -4.2149134 -4.16547 -4.1646447 -4.2207279 -4.2936611 -4.3551207 -4.4063449 -4.4300122 -4.4371696 -4.4532204][-4.4556332 -4.4327712 -4.3680067 -4.2800751 -4.1678619 -4.0401907 -3.9410222 -3.9190311 -3.9964995 -4.1084394 -4.20016 -4.2661991 -4.292501 -4.2993217 -4.3185983][-4.4754705 -4.4308395 -4.3296037 -4.19791 -4.0364337 -3.8615909 -3.7305973 -3.7059429 -3.8203545 -3.9746416 -4.09664 -4.1827922 -4.2251544 -4.2419329 -4.25844][-4.5041051 -4.453804 -4.3347158 -4.1762609 -3.9808884 -3.7810409 -3.6421556 -3.6296439 -3.7749815 -3.9454231 -4.0676484 -4.1556234 -4.2135124 -4.2475114 -4.2594333][-4.54663 -4.5204859 -4.4226079 -4.2693071 -4.0604267 -3.8562622 -3.7286487 -3.7304409 -3.8782389 -4.0234914 -4.1045642 -4.1617751 -4.216712 -4.2620735 -4.2682519][-4.6061788 -4.62753 -4.5779753 -4.449934 -4.2417278 -4.0449667 -3.9334877 -3.9400949 -4.0649381 -4.1553435 -4.1699929 -4.1751671 -4.2080398 -4.2510252 -4.2491841][-4.6485667 -4.7033925 -4.6904445 -4.585959 -4.3889208 -4.2089067 -4.1197734 -4.1359191 -4.2361 -4.2746243 -4.2331328 -4.194129 -4.1994829 -4.2265029 -4.2117596][-4.6401606 -4.6917791 -4.6874757 -4.5979252 -4.4243541 -4.2732034 -4.2149792 -4.2523322 -4.339994 -4.3455439 -4.2722116 -4.2068825 -4.1875453 -4.1916084 -4.1682692][-4.5933967 -4.6222744 -4.6064692 -4.5252266 -4.3863249 -4.2749071 -4.2524457 -4.3124437 -4.3942719 -4.3818717 -4.289865 -4.2030029 -4.1591268 -4.1460586 -4.1299572][-4.5290966 -4.5253334 -4.485569 -4.4097214 -4.31594 -4.2609034 -4.2806811 -4.3603406 -4.4343786 -4.4154716 -4.3198094 -4.2138562 -4.142242 -4.114933 -4.1211514][-4.4550071 -4.4253254 -4.3693218 -4.3093147 -4.2707744 -4.2797914 -4.3407154 -4.4267087 -4.4835644 -4.4599104 -4.3709226 -4.2570219 -4.1668663 -4.1355934 -4.1697564]]...]
INFO - root - 2017-12-07 11:22:46.161963: step 11910, loss = 21.11, batch loss = 21.03 (8.4 examples/sec; 0.950 sec/batch; 84h:35m:42s remains)
INFO - root - 2017-12-07 11:22:55.534728: step 11920, loss = 21.55, batch loss = 21.47 (8.9 examples/sec; 0.895 sec/batch; 79h:42m:31s remains)
INFO - root - 2017-12-07 11:23:04.971082: step 11930, loss = 21.35, batch loss = 21.27 (8.1 examples/sec; 0.985 sec/batch; 87h:43m:24s remains)
INFO - root - 2017-12-07 11:23:14.408881: step 11940, loss = 21.30, batch loss = 21.22 (8.6 examples/sec; 0.932 sec/batch; 82h:59m:15s remains)
INFO - root - 2017-12-07 11:23:23.980021: step 11950, loss = 21.24, batch loss = 21.15 (8.3 examples/sec; 0.959 sec/batch; 85h:24m:48s remains)
INFO - root - 2017-12-07 11:23:33.243199: step 11960, loss = 21.71, batch loss = 21.62 (9.0 examples/sec; 0.887 sec/batch; 78h:59m:00s remains)
INFO - root - 2017-12-07 11:23:42.669934: step 11970, loss = 21.64, batch loss = 21.56 (8.6 examples/sec; 0.932 sec/batch; 83h:01m:08s remains)
INFO - root - 2017-12-07 11:23:52.020142: step 11980, loss = 21.82, batch loss = 21.74 (8.2 examples/sec; 0.976 sec/batch; 86h:56m:11s remains)
INFO - root - 2017-12-07 11:24:01.478886: step 11990, loss = 21.63, batch loss = 21.55 (7.9 examples/sec; 1.010 sec/batch; 89h:56m:43s remains)
INFO - root - 2017-12-07 11:24:10.794543: step 12000, loss = 21.22, batch loss = 21.13 (8.2 examples/sec; 0.971 sec/batch; 86h:25m:49s remains)
2017-12-07 11:24:11.643038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5336304 -4.584322 -4.6031451 -4.6028719 -4.6109605 -4.6122422 -4.5817671 -4.540874 -4.5338664 -4.5548544 -4.5692973 -4.5753064 -4.5773077 -4.5667152 -4.5567379][-4.5423651 -4.5895367 -4.5948634 -4.5727749 -4.5730715 -4.5707355 -4.5269389 -4.4616008 -4.4416041 -4.4740205 -4.5083404 -4.5306697 -4.5404463 -4.524744 -4.5194883][-4.5105071 -4.5392108 -4.5330944 -4.4986053 -4.4897232 -4.4772663 -4.4211884 -4.3417997 -4.3221922 -4.3738971 -4.4329433 -4.4717216 -4.4835534 -4.4574795 -4.4552064][-4.445694 -4.4530706 -4.457 -4.4426432 -4.4333177 -4.4001274 -4.3279419 -4.246613 -4.2389193 -4.3097215 -4.3859057 -4.4345155 -4.4470339 -4.418364 -4.4225192][-4.3644171 -4.3549647 -4.3860674 -4.4150095 -4.4128294 -4.35161 -4.2558851 -4.1716356 -4.1710539 -4.2424989 -4.3151913 -4.3646469 -4.3875 -4.3817487 -4.4119992][-4.2924042 -4.2718053 -4.3245282 -4.3887844 -4.3897481 -4.3011365 -4.1799788 -4.0897212 -4.0797853 -4.118957 -4.1569996 -4.1937604 -4.2414002 -4.2957954 -4.3821616][-4.263979 -4.240418 -4.2987466 -4.3707914 -4.3571005 -4.2477312 -4.11487 -4.026823 -3.9968793 -3.9752035 -3.9481602 -3.9584913 -4.0393562 -4.1735539 -4.3329563][-4.2897863 -4.27198 -4.3204312 -4.3667145 -4.3184018 -4.1961045 -4.0763083 -4.000834 -3.949985 -3.8675709 -3.7809274 -3.7729752 -3.8907633 -4.0964308 -4.3178973][-4.3436685 -4.3169961 -4.3278174 -4.3175058 -4.2276096 -4.1141329 -4.0394988 -3.9960043 -3.94093 -3.8262873 -3.715518 -3.7092957 -3.8500359 -4.0834951 -4.3264055][-4.3926854 -4.33072 -4.2811542 -4.2133856 -4.1049809 -4.0288572 -4.0167255 -4.0215321 -3.9909625 -3.8895249 -3.7917745 -3.7934463 -3.9218249 -4.1263356 -4.3363571][-4.4420323 -4.323267 -4.2043242 -4.09194 -3.9940863 -3.9711034 -4.0166926 -4.0700512 -4.0835948 -4.0283966 -3.9673216 -3.9728017 -4.0616136 -4.2048345 -4.3530154][-4.498004 -4.3286743 -4.1524296 -4.0095172 -3.9321408 -3.9567428 -4.0389414 -4.1244326 -4.1776824 -4.1666212 -4.1399803 -4.1463218 -4.1864924 -4.2577567 -4.336534][-4.5664806 -4.3794513 -4.1722546 -4.0127845 -3.9509218 -4.0076423 -4.106627 -4.2011833 -4.2714314 -4.28707 -4.2773094 -4.2856331 -4.2924972 -4.3075829 -4.324738][-4.6220355 -4.4507813 -4.2410159 -4.0771785 -4.0227242 -4.089592 -4.1856389 -4.2688532 -4.3392644 -4.3718481 -4.3717127 -4.3852534 -4.3759184 -4.3567247 -4.3326855][-4.6403651 -4.5044909 -4.3109455 -4.1526895 -4.0998955 -4.1579108 -4.2420115 -4.3132238 -4.3783927 -4.4197817 -4.4262071 -4.4450397 -4.4299693 -4.388515 -4.3353744]]...]
INFO - root - 2017-12-07 11:24:20.860373: step 12010, loss = 21.32, batch loss = 21.24 (8.8 examples/sec; 0.906 sec/batch; 80h:39m:55s remains)
INFO - root - 2017-12-07 11:24:30.231029: step 12020, loss = 21.54, batch loss = 21.46 (8.6 examples/sec; 0.934 sec/batch; 83h:07m:42s remains)
INFO - root - 2017-12-07 11:24:39.605099: step 12030, loss = 21.58, batch loss = 21.49 (8.4 examples/sec; 0.948 sec/batch; 84h:24m:54s remains)
INFO - root - 2017-12-07 11:24:49.011001: step 12040, loss = 21.23, batch loss = 21.14 (8.2 examples/sec; 0.971 sec/batch; 86h:25m:04s remains)
INFO - root - 2017-12-07 11:24:58.408222: step 12050, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.972 sec/batch; 86h:32m:49s remains)
INFO - root - 2017-12-07 11:25:07.816638: step 12060, loss = 21.29, batch loss = 21.21 (8.2 examples/sec; 0.975 sec/batch; 86h:44m:33s remains)
INFO - root - 2017-12-07 11:25:17.273007: step 12070, loss = 21.01, batch loss = 20.93 (7.9 examples/sec; 1.016 sec/batch; 90h:25m:37s remains)
INFO - root - 2017-12-07 11:25:26.677475: step 12080, loss = 21.49, batch loss = 21.41 (8.3 examples/sec; 0.966 sec/batch; 85h:56m:59s remains)
INFO - root - 2017-12-07 11:25:35.894563: step 12090, loss = 21.77, batch loss = 21.68 (8.8 examples/sec; 0.914 sec/batch; 81h:19m:05s remains)
INFO - root - 2017-12-07 11:25:45.222855: step 12100, loss = 21.61, batch loss = 21.52 (8.3 examples/sec; 0.960 sec/batch; 85h:28m:07s remains)
2017-12-07 11:25:46.122648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3090668 -4.3745584 -4.395752 -4.3739381 -4.3633347 -4.3975348 -4.4544272 -4.503315 -4.5267253 -4.5278969 -4.5081282 -4.4591713 -4.3930349 -4.3381534 -4.311389][-4.3988981 -4.4376917 -4.4420118 -4.4144988 -4.3898091 -4.3986659 -4.4408894 -4.4933715 -4.5356016 -4.5610514 -4.5573282 -4.5110893 -4.4344339 -4.3613071 -4.3112483][-4.4806809 -4.5078611 -4.5068359 -4.4747138 -4.42804 -4.3998733 -4.4110851 -4.4541574 -4.5139217 -4.5723295 -4.5980225 -4.5682306 -4.4890146 -4.3974128 -4.3247685][-4.5181508 -4.5474405 -4.5461922 -4.5014081 -4.4209886 -4.3517671 -4.3354578 -4.3743467 -4.4584012 -4.5569854 -4.6193805 -4.6187725 -4.5500822 -4.4451394 -4.3523297][-4.5305691 -4.5623703 -4.548799 -4.4683075 -4.3342562 -4.2206297 -4.1877637 -4.2372375 -4.3577 -4.50296 -4.6048522 -4.6400747 -4.5962729 -4.4932413 -4.3901634][-4.532712 -4.5598249 -4.519619 -4.3850784 -4.1801252 -4.0101852 -3.9552343 -4.0135403 -4.1725259 -4.3707557 -4.5206671 -4.6009407 -4.5992045 -4.5243387 -4.4315033][-4.5185986 -4.5288482 -4.4573894 -4.2691431 -3.990932 -3.7541468 -3.661526 -3.7198942 -3.9148016 -4.1699104 -4.3767824 -4.5070515 -4.5562453 -4.5303974 -4.4679327][-4.48037 -4.4728885 -4.3901281 -4.1877942 -3.8740039 -3.5814133 -3.4381292 -3.4791338 -3.6908484 -3.9852626 -4.242311 -4.4167533 -4.5090179 -4.533793 -4.5112309][-4.4139867 -4.3960218 -4.3384676 -4.1883593 -3.9149365 -3.614162 -3.4271381 -3.4297786 -3.617662 -3.9028602 -4.1747923 -4.3755441 -4.4975195 -4.5641026 -4.5818939][-4.3401074 -4.3116856 -4.29657 -4.2404361 -4.0751343 -3.8310847 -3.6314623 -3.5875602 -3.711587 -3.9284296 -4.1640358 -4.3625088 -4.5025544 -4.6060109 -4.6624122][-4.2954683 -4.2586112 -4.2804976 -4.3131981 -4.2692771 -4.1186419 -3.9394135 -3.8575556 -3.8996723 -4.0090909 -4.163558 -4.3246675 -4.4615068 -4.5869179 -4.6670771][-4.2992558 -4.2558589 -4.2959728 -4.3776484 -4.4126186 -4.3414378 -4.1994777 -4.1046038 -4.0825243 -4.0920353 -4.1570354 -4.2551355 -4.3521519 -4.4581785 -4.5289097][-4.3360014 -4.2881012 -4.3292813 -4.4222574 -4.4837675 -4.4544592 -4.3470087 -4.2596555 -4.20339 -4.1454606 -4.144515 -4.1823 -4.2156487 -4.266542 -4.3010445][-4.3601112 -4.31962 -4.3634305 -4.4518313 -4.5105743 -4.4963522 -4.4127531 -4.33235 -4.2496986 -4.1467881 -4.1068654 -4.1051531 -4.0869865 -4.085155 -4.0815196][-4.3357911 -4.3119593 -4.3653712 -4.4403872 -4.4790058 -4.4695106 -4.4091268 -4.3378658 -4.2369356 -4.108952 -4.0468197 -4.0171204 -3.9680111 -3.9372623 -3.9099605]]...]
INFO - root - 2017-12-07 11:25:55.533448: step 12110, loss = 21.54, batch loss = 21.46 (8.4 examples/sec; 0.955 sec/batch; 85h:00m:16s remains)
INFO - root - 2017-12-07 11:26:04.847704: step 12120, loss = 21.43, batch loss = 21.35 (8.9 examples/sec; 0.897 sec/batch; 79h:51m:57s remains)
INFO - root - 2017-12-07 11:26:14.241172: step 12130, loss = 21.35, batch loss = 21.27 (8.9 examples/sec; 0.896 sec/batch; 79h:45m:01s remains)
INFO - root - 2017-12-07 11:26:23.580326: step 12140, loss = 21.33, batch loss = 21.25 (8.7 examples/sec; 0.923 sec/batch; 82h:10m:13s remains)
INFO - root - 2017-12-07 11:26:33.049420: step 12150, loss = 21.35, batch loss = 21.27 (8.6 examples/sec; 0.929 sec/batch; 82h:40m:07s remains)
INFO - root - 2017-12-07 11:26:42.552108: step 12160, loss = 21.11, batch loss = 21.03 (8.1 examples/sec; 0.988 sec/batch; 87h:56m:47s remains)
INFO - root - 2017-12-07 11:26:51.797329: step 12170, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.944 sec/batch; 83h:58m:09s remains)
INFO - root - 2017-12-07 11:27:01.292986: step 12180, loss = 21.27, batch loss = 21.18 (7.9 examples/sec; 1.016 sec/batch; 90h:23m:09s remains)
INFO - root - 2017-12-07 11:27:10.724513: step 12190, loss = 21.15, batch loss = 21.07 (8.3 examples/sec; 0.964 sec/batch; 85h:45m:51s remains)
INFO - root - 2017-12-07 11:27:19.961045: step 12200, loss = 21.58, batch loss = 21.49 (8.5 examples/sec; 0.945 sec/batch; 84h:03m:49s remains)
2017-12-07 11:27:20.941433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2599216 -4.2777758 -4.3111606 -4.313282 -4.307858 -4.3175225 -4.3467245 -4.3770275 -4.3894472 -4.3923182 -4.3952107 -4.4026742 -4.4136219 -4.4230928 -4.4254942][-4.1987176 -4.2517815 -4.3271575 -4.3626642 -4.3706288 -4.3842478 -4.410789 -4.4359527 -4.4444237 -4.44104 -4.4342828 -4.4309306 -4.4326138 -4.4356108 -4.4344397][-4.121932 -4.224195 -4.3553314 -4.4387207 -4.4642453 -4.4767852 -4.4941635 -4.5129905 -4.5210018 -4.5113516 -4.4865484 -4.45576 -4.4300289 -4.414763 -4.4068394][-4.0717549 -4.210422 -4.3790321 -4.4907179 -4.513483 -4.5069451 -4.5074244 -4.5230951 -4.5418429 -4.5393243 -4.5043864 -4.4468484 -4.3892827 -4.3520136 -4.33864][-4.0878553 -4.2210155 -4.3700776 -4.4555731 -4.4445076 -4.4062662 -4.3917165 -4.409503 -4.4482374 -4.470675 -4.4462228 -4.3857059 -4.3152242 -4.2650423 -4.2497787][-4.1615524 -4.2425365 -4.3128414 -4.3226933 -4.2605515 -4.1954169 -4.1763887 -4.1987329 -4.2549639 -4.31102 -4.31693 -4.288518 -4.239306 -4.1926084 -4.1772132][-4.2649655 -4.26703 -4.2348928 -4.1568961 -4.0521126 -3.9758809 -3.9630868 -3.9854293 -4.0466452 -4.1307592 -4.1750031 -4.2008362 -4.1963568 -4.1649342 -4.149231][-4.3627396 -4.2956495 -4.1812572 -4.0424814 -3.9163265 -3.8382668 -3.8301344 -3.8425047 -3.8988729 -4.003274 -4.0844431 -4.1632013 -4.2008605 -4.1834435 -4.1650925][-4.4076028 -4.3107333 -4.1669846 -4.0185847 -3.8997436 -3.8247786 -3.8099837 -3.8078127 -3.8551962 -3.966399 -4.0705042 -4.1776466 -4.2335057 -4.2224855 -4.2031074][-4.405848 -4.3231025 -4.2041235 -4.0902734 -4.0005512 -3.9306509 -3.9011598 -3.8865714 -3.9276409 -4.0350528 -4.142684 -4.2443213 -4.2904158 -4.2785912 -4.2624803][-4.4032631 -4.3523517 -4.282443 -4.2198005 -4.168252 -4.1148305 -4.0829678 -4.0718021 -4.1109023 -4.2000551 -4.2873626 -4.3557563 -4.3736877 -4.3557544 -4.3440671][-4.4229712 -4.3989129 -4.369741 -4.3468971 -4.3270655 -4.2999263 -4.2861514 -4.2941012 -4.3333015 -4.3933687 -4.4419909 -4.4663024 -4.4566331 -4.4342422 -4.4254541][-4.4187641 -4.4121509 -4.4059029 -4.404778 -4.4056349 -4.4030328 -4.4124961 -4.4376583 -4.4692783 -4.4953971 -4.5038772 -4.4953923 -4.4732189 -4.4507489 -4.4417267][-4.33661 -4.3315597 -4.3299828 -4.3404794 -4.358676 -4.3788633 -4.4055061 -4.4345784 -4.4501834 -4.4476771 -4.430069 -4.4096742 -4.3928108 -4.3771563 -4.3692451][-4.1773109 -4.1563163 -4.1444407 -4.1638107 -4.2054925 -4.2531867 -4.2979217 -4.3270016 -4.3263426 -4.302114 -4.2672567 -4.2435184 -4.2383742 -4.2359853 -4.2388291]]...]
INFO - root - 2017-12-07 11:27:30.415588: step 12210, loss = 21.47, batch loss = 21.38 (8.6 examples/sec; 0.933 sec/batch; 83h:01m:51s remains)
INFO - root - 2017-12-07 11:27:39.963710: step 12220, loss = 21.28, batch loss = 21.20 (8.2 examples/sec; 0.974 sec/batch; 86h:41m:21s remains)
INFO - root - 2017-12-07 11:27:49.332013: step 12230, loss = 21.77, batch loss = 21.68 (8.4 examples/sec; 0.954 sec/batch; 84h:50m:26s remains)
INFO - root - 2017-12-07 11:27:58.578506: step 12240, loss = 21.04, batch loss = 20.96 (8.7 examples/sec; 0.921 sec/batch; 81h:54m:19s remains)
INFO - root - 2017-12-07 11:28:07.946368: step 12250, loss = 21.77, batch loss = 21.69 (8.7 examples/sec; 0.924 sec/batch; 82h:12m:47s remains)
INFO - root - 2017-12-07 11:28:17.384935: step 12260, loss = 21.44, batch loss = 21.36 (8.5 examples/sec; 0.937 sec/batch; 83h:22m:00s remains)
INFO - root - 2017-12-07 11:28:26.792809: step 12270, loss = 21.52, batch loss = 21.44 (8.9 examples/sec; 0.901 sec/batch; 80h:07m:43s remains)
INFO - root - 2017-12-07 11:28:36.234106: step 12280, loss = 21.08, batch loss = 21.00 (8.6 examples/sec; 0.928 sec/batch; 82h:31m:40s remains)
INFO - root - 2017-12-07 11:28:45.654633: step 12290, loss = 21.30, batch loss = 21.22 (8.4 examples/sec; 0.956 sec/batch; 85h:03m:50s remains)
INFO - root - 2017-12-07 11:28:55.097049: step 12300, loss = 21.12, batch loss = 21.03 (8.0 examples/sec; 1.000 sec/batch; 88h:58m:20s remains)
2017-12-07 11:28:55.932199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5426412 -4.4939928 -4.5046539 -4.5738187 -4.66362 -4.7280464 -4.7670236 -4.7843575 -4.7794056 -4.7168179 -4.6307845 -4.58425 -4.566237 -4.5570922 -4.536974][-4.4953742 -4.4164195 -4.4071655 -4.461885 -4.5553193 -4.6349764 -4.6896005 -4.7305565 -4.76762 -4.7599974 -4.7069855 -4.6562915 -4.6168976 -4.5842929 -4.549829][-4.4574337 -4.3473592 -4.3125815 -4.3506413 -4.4404078 -4.5168734 -4.5544624 -4.5985417 -4.6675825 -4.7137051 -4.7053843 -4.6678081 -4.6223464 -4.5765038 -4.5314422][-4.4606066 -4.3192682 -4.2508135 -4.2571959 -4.322248 -4.3692322 -4.374958 -4.4111738 -4.5023284 -4.5973625 -4.6380372 -4.6256557 -4.5912356 -4.5459881 -4.4977336][-4.4949408 -4.3347163 -4.2314405 -4.1973805 -4.2182345 -4.217977 -4.1873474 -4.2120371 -4.3193417 -4.4595618 -4.5528893 -4.5762076 -4.5612111 -4.5202432 -4.4655828][-4.5324755 -4.3627267 -4.215416 -4.1212144 -4.0794644 -4.0301685 -3.9717512 -3.9981022 -4.1333408 -4.3233404 -4.4711413 -4.53742 -4.5492077 -4.51302 -4.4457254][-4.569057 -4.3951945 -4.1988497 -4.02743 -3.9182751 -3.8294249 -3.7655272 -3.8183911 -3.9963717 -4.2340021 -4.4271188 -4.5300441 -4.5608406 -4.5243449 -4.4459605][-4.6073494 -4.4343896 -4.1989274 -3.9603662 -3.7955384 -3.6808677 -3.6297481 -3.726444 -3.9439592 -4.2102461 -4.4268909 -4.54407 -4.573472 -4.530714 -4.4522967][-4.6718984 -4.5129766 -4.2563686 -3.9715316 -3.7624352 -3.6292949 -3.5992639 -3.7344558 -3.9750729 -4.2456813 -4.4610806 -4.5677867 -4.5776129 -4.5272241 -4.4623761][-4.7780609 -4.6592646 -4.4142027 -4.1167469 -3.8838923 -3.7450242 -3.7383478 -3.8878434 -4.1183791 -4.3609705 -4.5425105 -4.6147838 -4.6011739 -4.5525203 -4.5142][-4.87144 -4.8219438 -4.6235566 -4.3448634 -4.1144228 -3.9857862 -3.99398 -4.1329637 -4.3212404 -4.5048413 -4.6275678 -4.658421 -4.6360512 -4.608933 -4.60491][-4.8981686 -4.9104271 -4.76678 -4.5293 -4.3270397 -4.224638 -4.2458558 -4.3634157 -4.4988809 -4.6130733 -4.6719584 -4.6703343 -4.6554246 -4.6563582 -4.6773949][-4.8574128 -4.8981729 -4.8019695 -4.6236286 -4.4716859 -4.4034286 -4.4366894 -4.530488 -4.6099272 -4.6541376 -4.6578989 -4.64116 -4.6420527 -4.6621957 -4.6880703][-4.7606549 -4.799016 -4.733376 -4.6113806 -4.5100479 -4.4742193 -4.5127864 -4.5796614 -4.6085248 -4.5941548 -4.5615172 -4.5477738 -4.5671663 -4.60383 -4.6286721][-4.6229429 -4.6440873 -4.59543 -4.5134306 -4.4467015 -4.4285426 -4.4652185 -4.5127149 -4.5097461 -4.459137 -4.4101233 -4.4058495 -4.4434967 -4.5021968 -4.5287523]]...]
INFO - root - 2017-12-07 11:29:05.259761: step 12310, loss = 21.56, batch loss = 21.48 (8.6 examples/sec; 0.928 sec/batch; 82h:34m:47s remains)
INFO - root - 2017-12-07 11:29:14.738178: step 12320, loss = 21.17, batch loss = 21.08 (8.9 examples/sec; 0.899 sec/batch; 79h:56m:17s remains)
INFO - root - 2017-12-07 11:29:24.338257: step 12330, loss = 21.45, batch loss = 21.37 (8.6 examples/sec; 0.935 sec/batch; 83h:09m:16s remains)
INFO - root - 2017-12-07 11:29:33.665473: step 12340, loss = 21.65, batch loss = 21.56 (8.8 examples/sec; 0.907 sec/batch; 80h:40m:35s remains)
INFO - root - 2017-12-07 11:29:43.107867: step 12350, loss = 21.32, batch loss = 21.24 (8.1 examples/sec; 0.989 sec/batch; 87h:58m:03s remains)
INFO - root - 2017-12-07 11:29:52.470223: step 12360, loss = 21.07, batch loss = 20.99 (8.6 examples/sec; 0.930 sec/batch; 82h:39m:59s remains)
INFO - root - 2017-12-07 11:30:01.965074: step 12370, loss = 21.29, batch loss = 21.21 (8.3 examples/sec; 0.965 sec/batch; 85h:49m:52s remains)
INFO - root - 2017-12-07 11:30:11.347791: step 12380, loss = 21.02, batch loss = 20.94 (9.0 examples/sec; 0.888 sec/batch; 78h:58m:30s remains)
INFO - root - 2017-12-07 11:30:20.711213: step 12390, loss = 21.80, batch loss = 21.71 (8.9 examples/sec; 0.895 sec/batch; 79h:36m:44s remains)
INFO - root - 2017-12-07 11:30:30.131812: step 12400, loss = 21.11, batch loss = 21.02 (8.7 examples/sec; 0.923 sec/batch; 82h:04m:16s remains)
2017-12-07 11:30:31.105127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2886238 -4.2892122 -4.3119683 -4.3437071 -4.3618608 -4.3714142 -4.3866377 -4.3832135 -4.3225026 -4.2611256 -4.2737422 -4.3318167 -4.3757677 -4.3814907 -4.3791027][-4.3940415 -4.3972864 -4.41304 -4.4298115 -4.4311852 -4.4296765 -4.4372773 -4.4231968 -4.3540792 -4.2950773 -4.3172607 -4.3840971 -4.4325838 -4.44026 -4.4357357][-4.5108929 -4.503006 -4.4956203 -4.4950466 -4.4897375 -4.4835944 -4.4796171 -4.4490061 -4.3770332 -4.3302188 -4.3662224 -4.4429407 -4.495441 -4.5031843 -4.4945064][-4.5516977 -4.5230546 -4.4876895 -4.4825869 -4.4927673 -4.4913325 -4.4667559 -4.4045596 -4.3203559 -4.2860546 -4.3433533 -4.4391642 -4.4998908 -4.5057578 -4.4891329][-4.497807 -4.4553738 -4.4003782 -4.4014506 -4.4389024 -4.44591 -4.3989506 -4.306859 -4.2052646 -4.1724768 -4.2434216 -4.3569589 -4.4263496 -4.430861 -4.4086723][-4.4155283 -4.3777227 -4.3165874 -4.318027 -4.3648553 -4.3645692 -4.29739 -4.1961565 -4.0960097 -4.0689006 -4.1493726 -4.2705264 -4.3374581 -4.3334923 -4.3069525][-4.3578329 -4.3405972 -4.2910542 -4.2885933 -4.3187175 -4.2971268 -4.2192736 -4.1286092 -4.045043 -4.0293827 -4.1115017 -4.2228961 -4.2727008 -4.2549095 -4.2222676][-4.3532076 -4.3492002 -4.3122663 -4.3052034 -4.317965 -4.2910652 -4.2273397 -4.1576982 -4.0892692 -4.0753217 -4.1400547 -4.2264 -4.2625737 -4.2492042 -4.2209153][-4.3728151 -4.3662562 -4.3295832 -4.3130651 -4.3191376 -4.311965 -4.2889514 -4.257195 -4.2128367 -4.1995797 -4.2331152 -4.2778134 -4.3015075 -4.3155088 -4.3153872][-4.360528 -4.3493371 -4.3085041 -4.2801185 -4.2819963 -4.2969766 -4.3114271 -4.3146958 -4.2979226 -4.2987375 -4.3120761 -4.3181286 -4.3282266 -4.3660283 -4.3986521][-4.3424587 -4.3371677 -4.2990861 -4.2622261 -4.2558942 -4.2764597 -4.3030229 -4.3156867 -4.3096218 -4.3220382 -4.3284459 -4.3148589 -4.3181748 -4.3642349 -4.4146409][-4.3659611 -4.3729382 -4.3477521 -4.3158808 -4.3045764 -4.31945 -4.3376713 -4.3355322 -4.3080206 -4.2972307 -4.2833023 -4.2569623 -4.2609377 -4.3166494 -4.3835516][-4.3987751 -4.4136705 -4.4083681 -4.3955517 -4.3865843 -4.3880515 -4.3890805 -4.3698864 -4.3148885 -4.2614083 -4.2123561 -4.1717029 -4.1769562 -4.2421994 -4.3281512][-4.4046564 -4.41963 -4.4273071 -4.4323888 -4.4254603 -4.4091573 -4.3928742 -4.3693967 -4.3126984 -4.2375689 -4.1659904 -4.1174293 -4.1201177 -4.18282 -4.2722445][-4.3878417 -4.3990316 -4.4111505 -4.4243789 -4.4158273 -4.3854947 -4.3601084 -4.3417716 -4.2979174 -4.2235126 -4.1485176 -4.1058378 -4.1107612 -4.1634908 -4.2371693]]...]
INFO - root - 2017-12-07 11:30:40.273187: step 12410, loss = 21.39, batch loss = 21.30 (8.5 examples/sec; 0.946 sec/batch; 84h:06m:39s remains)
INFO - root - 2017-12-07 11:30:49.680615: step 12420, loss = 20.99, batch loss = 20.91 (8.2 examples/sec; 0.980 sec/batch; 87h:09m:36s remains)
INFO - root - 2017-12-07 11:30:59.121945: step 12430, loss = 21.04, batch loss = 20.96 (8.3 examples/sec; 0.967 sec/batch; 85h:59m:51s remains)
INFO - root - 2017-12-07 11:31:08.494592: step 12440, loss = 21.31, batch loss = 21.22 (8.5 examples/sec; 0.942 sec/batch; 83h:44m:27s remains)
INFO - root - 2017-12-07 11:31:17.876855: step 12450, loss = 21.65, batch loss = 21.57 (9.1 examples/sec; 0.879 sec/batch; 78h:11m:12s remains)
INFO - root - 2017-12-07 11:31:27.277948: step 12460, loss = 21.51, batch loss = 21.43 (8.6 examples/sec; 0.934 sec/batch; 83h:01m:56s remains)
INFO - root - 2017-12-07 11:31:36.751310: step 12470, loss = 21.05, batch loss = 20.96 (8.3 examples/sec; 0.965 sec/batch; 85h:48m:38s remains)
INFO - root - 2017-12-07 11:31:46.192756: step 12480, loss = 21.42, batch loss = 21.34 (8.3 examples/sec; 0.966 sec/batch; 85h:50m:21s remains)
INFO - root - 2017-12-07 11:31:55.474305: step 12490, loss = 21.42, batch loss = 21.33 (7.8 examples/sec; 1.023 sec/batch; 90h:56m:29s remains)
INFO - root - 2017-12-07 11:32:04.768808: step 12500, loss = 21.13, batch loss = 21.04 (7.9 examples/sec; 1.010 sec/batch; 89h:46m:26s remains)
2017-12-07 11:32:05.659893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2747478 -4.2998009 -4.31702 -4.3183928 -4.3316 -4.3403568 -4.3300233 -4.3186641 -4.3158603 -4.3119822 -4.34408 -4.4055843 -4.4354596 -4.4021611 -4.3314581][-4.233417 -4.2423773 -4.2498994 -4.2642808 -4.3042293 -4.3260169 -4.3096218 -4.2767463 -4.2486587 -4.2308841 -4.2601919 -4.3213634 -4.3603888 -4.342546 -4.2905965][-4.2224374 -4.2170525 -4.2136178 -4.2436905 -4.3149085 -4.3514838 -4.3283558 -4.2756767 -4.225656 -4.1972823 -4.2173243 -4.2736139 -4.3188748 -4.3141689 -4.2765779][-4.2594786 -4.2457252 -4.236969 -4.2691531 -4.3439646 -4.3775907 -4.3472424 -4.2933145 -4.2503667 -4.2376909 -4.2614636 -4.3122277 -4.352963 -4.3519115 -4.3167686][-4.3320203 -4.3182797 -4.3155632 -4.3404222 -4.385921 -4.3819337 -4.322988 -4.2634983 -4.2457027 -4.2835784 -4.3430023 -4.3997965 -4.4344077 -4.4243937 -4.3738976][-4.3626227 -4.3629065 -4.38547 -4.4113712 -4.4194322 -4.3559084 -4.237782 -4.137897 -4.1263218 -4.221293 -4.3414035 -4.4284759 -4.4739227 -4.4611483 -4.3973684][-4.3107357 -4.3174267 -4.3572917 -4.3847327 -4.3652883 -4.2503128 -4.0662169 -3.9082246 -3.892066 -4.0467215 -4.2452 -4.3891268 -4.4738708 -4.4806776 -4.4222369][-4.2207766 -4.2191658 -4.255568 -4.2806206 -4.2547784 -4.1214137 -3.8999102 -3.700706 -3.6787961 -3.8708441 -4.1257706 -4.3214316 -4.4481168 -4.4833069 -4.4388957][-4.1929317 -4.186069 -4.2111282 -4.2398682 -4.2321916 -4.1197352 -3.9011662 -3.6918013 -3.6529477 -3.8198886 -4.0614495 -4.2681026 -4.4076748 -4.4556918 -4.4254184][-4.2495565 -4.2478809 -4.2621684 -4.2954459 -4.3185043 -4.254787 -4.0789752 -3.8923922 -3.8321888 -3.9218888 -4.0884056 -4.25998 -4.3781171 -4.4139609 -4.3883491][-4.3274708 -4.339035 -4.3488259 -4.3783417 -4.4116087 -4.3828058 -4.2539849 -4.1046519 -4.0396504 -4.0671291 -4.1592937 -4.2782197 -4.3547392 -4.3599386 -4.328866][-4.3623295 -4.4022069 -4.4295292 -4.4572358 -4.4732594 -4.4360194 -4.3225527 -4.2037392 -4.154891 -4.16778 -4.2247181 -4.3017683 -4.335103 -4.3024607 -4.2566223][-4.3614383 -4.4214735 -4.4686766 -4.4980125 -4.4930897 -4.4338541 -4.3268056 -4.2384582 -4.2154026 -4.2422228 -4.2970843 -4.3581891 -4.3623638 -4.2930107 -4.2218738][-4.3749232 -4.4336772 -4.4796376 -4.505929 -4.4917169 -4.4274049 -4.3413143 -4.2864842 -4.2796183 -4.3128719 -4.3724184 -4.4364052 -4.4309783 -4.3411889 -4.2512026][-4.3986588 -4.4449396 -4.4794946 -4.5009022 -4.4841795 -4.4252834 -4.3660426 -4.3436103 -4.3428936 -4.3669119 -4.4153495 -4.4731627 -4.4668803 -4.3808537 -4.295157]]...]
INFO - root - 2017-12-07 11:32:14.892634: step 12510, loss = 21.57, batch loss = 21.48 (8.5 examples/sec; 0.945 sec/batch; 84h:00m:13s remains)
INFO - root - 2017-12-07 11:32:24.297325: step 12520, loss = 21.48, batch loss = 21.40 (8.7 examples/sec; 0.917 sec/batch; 81h:31m:49s remains)
INFO - root - 2017-12-07 11:32:33.679532: step 12530, loss = 21.17, batch loss = 21.09 (9.1 examples/sec; 0.879 sec/batch; 78h:10m:03s remains)
INFO - root - 2017-12-07 11:32:43.091833: step 12540, loss = 21.40, batch loss = 21.31 (8.7 examples/sec; 0.923 sec/batch; 82h:04m:38s remains)
INFO - root - 2017-12-07 11:32:52.509164: step 12550, loss = 21.31, batch loss = 21.23 (8.4 examples/sec; 0.950 sec/batch; 84h:25m:37s remains)
INFO - root - 2017-12-07 11:33:01.873152: step 12560, loss = 21.71, batch loss = 21.63 (8.5 examples/sec; 0.940 sec/batch; 83h:32m:48s remains)
INFO - root - 2017-12-07 11:33:11.083295: step 12570, loss = 21.14, batch loss = 21.06 (9.3 examples/sec; 0.865 sec/batch; 76h:50m:46s remains)
INFO - root - 2017-12-07 11:33:20.215327: step 12580, loss = 21.67, batch loss = 21.59 (8.9 examples/sec; 0.894 sec/batch; 79h:27m:00s remains)
INFO - root - 2017-12-07 11:33:29.557336: step 12590, loss = 21.25, batch loss = 21.16 (8.6 examples/sec; 0.930 sec/batch; 82h:37m:11s remains)
INFO - root - 2017-12-07 11:33:39.059351: step 12600, loss = 21.52, batch loss = 21.44 (8.2 examples/sec; 0.970 sec/batch; 86h:13m:17s remains)
2017-12-07 11:33:40.235250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2256064 -4.2924623 -4.3416 -4.354856 -4.364429 -4.3671079 -4.35707 -4.3296161 -4.3184643 -4.3373666 -4.3449554 -4.3807387 -4.4480991 -4.5065861 -4.5357051][-4.2470522 -4.3086233 -4.3362174 -4.3385921 -4.3496633 -4.348536 -4.3237338 -4.2752719 -4.2527528 -4.2678213 -4.277061 -4.3143511 -4.3824186 -4.4520936 -4.50671][-4.2778811 -4.3469105 -4.3618093 -4.3528385 -4.3541746 -4.3366609 -4.2973962 -4.2462349 -4.2316489 -4.2562594 -4.2781444 -4.3124237 -4.3632274 -4.4262514 -4.4961858][-4.2270646 -4.3108492 -4.3366122 -4.3410778 -4.3404584 -4.3034372 -4.24608 -4.1921682 -4.1901164 -4.2355947 -4.2919531 -4.3483138 -4.3989167 -4.4486203 -4.5096245][-4.0987368 -4.2078719 -4.2636957 -4.2964082 -4.2940049 -4.2234015 -4.1310186 -4.059454 -4.0529861 -4.1104779 -4.2118526 -4.3274665 -4.4219079 -4.4894485 -4.5415411][-3.9860213 -4.1340971 -4.2287703 -4.277607 -4.2409773 -4.1028438 -3.9517152 -3.8469119 -3.8252549 -3.8892233 -4.0339084 -4.2219939 -4.390306 -4.5082579 -4.5723572][-3.917902 -4.1046419 -4.2363949 -4.2948747 -4.219389 -4.0205021 -3.8183389 -3.6821556 -3.641773 -3.6999466 -3.8664782 -4.1071935 -4.3417077 -4.5121617 -4.5955005][-3.9033136 -4.1203527 -4.2875543 -4.3604393 -4.2676616 -4.0496254 -3.8339562 -3.6763768 -3.6055219 -3.6365647 -3.7947836 -4.0555391 -4.3260293 -4.5193291 -4.6044383][-3.9577944 -4.1889968 -4.375421 -4.4591041 -4.3704133 -4.1745739 -3.9809434 -3.82146 -3.7258372 -3.7284865 -3.864037 -4.1132407 -4.3787203 -4.5550175 -4.6172423][-4.0658474 -4.2809658 -4.4600048 -4.5408 -4.4698157 -4.3241048 -4.1815343 -4.0484691 -3.9488287 -3.9291382 -4.0264859 -4.2356977 -4.4651814 -4.6034808 -4.6384897][-4.2312841 -4.4032197 -4.546382 -4.6075997 -4.5585475 -4.4704542 -4.3865681 -4.2979832 -4.2192779 -4.1940289 -4.2532196 -4.4038696 -4.5707965 -4.6573949 -4.6637712][-4.4240403 -4.5408807 -4.6378589 -4.6742778 -4.6470342 -4.6084685 -4.5700479 -4.521544 -4.4768953 -4.4634223 -4.4960418 -4.5857887 -4.6815505 -4.7150908 -4.6895227][-4.5802021 -4.6520123 -4.710567 -4.7299247 -4.7215052 -4.7140522 -4.6997752 -4.6715331 -4.6464105 -4.6390386 -4.6505713 -4.6902514 -4.7330074 -4.7377481 -4.6982241][-4.6396165 -4.6820397 -4.7147903 -4.7256241 -4.7292209 -4.7340918 -4.7276335 -4.706727 -4.6883478 -4.6809936 -4.6798763 -4.6893106 -4.70305 -4.6991863 -4.6623921][-4.5774884 -4.6030374 -4.6182442 -4.6236968 -4.6290588 -4.6342411 -4.6310859 -4.6197968 -4.6114492 -4.6087074 -4.6049447 -4.6021757 -4.6042824 -4.6021209 -4.5807734]]...]
INFO - root - 2017-12-07 11:33:49.599953: step 12610, loss = 21.58, batch loss = 21.50 (8.6 examples/sec; 0.927 sec/batch; 82h:23m:27s remains)
INFO - root - 2017-12-07 11:33:58.976910: step 12620, loss = 21.62, batch loss = 21.54 (8.9 examples/sec; 0.901 sec/batch; 80h:01m:55s remains)
INFO - root - 2017-12-07 11:34:08.283272: step 12630, loss = 21.31, batch loss = 21.22 (8.5 examples/sec; 0.945 sec/batch; 83h:58m:34s remains)
INFO - root - 2017-12-07 11:34:17.750182: step 12640, loss = 21.43, batch loss = 21.34 (8.9 examples/sec; 0.901 sec/batch; 80h:02m:56s remains)
INFO - root - 2017-12-07 11:34:27.177640: step 12650, loss = 21.60, batch loss = 21.52 (9.0 examples/sec; 0.890 sec/batch; 79h:03m:39s remains)
INFO - root - 2017-12-07 11:34:36.532810: step 12660, loss = 21.78, batch loss = 21.70 (8.6 examples/sec; 0.931 sec/batch; 82h:41m:51s remains)
INFO - root - 2017-12-07 11:34:45.866169: step 12670, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.972 sec/batch; 86h:23m:44s remains)
INFO - root - 2017-12-07 11:34:55.292136: step 12680, loss = 21.51, batch loss = 21.42 (8.0 examples/sec; 1.003 sec/batch; 89h:03m:45s remains)
INFO - root - 2017-12-07 11:35:04.670881: step 12690, loss = 21.23, batch loss = 21.15 (9.0 examples/sec; 0.894 sec/batch; 79h:23m:32s remains)
INFO - root - 2017-12-07 11:35:13.965200: step 12700, loss = 21.88, batch loss = 21.79 (8.9 examples/sec; 0.897 sec/batch; 79h:42m:39s remains)
2017-12-07 11:35:14.866172: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4549842 -4.4910803 -4.5350332 -4.5605521 -4.5517426 -4.5295959 -4.49625 -4.4707336 -4.4851213 -4.53204 -4.5827951 -4.6030393 -4.572463 -4.5126009 -4.4567566][-4.4971466 -4.5563674 -4.6126251 -4.6353626 -4.6130176 -4.5722542 -4.5218873 -4.4759436 -4.4693947 -4.5033875 -4.5495558 -4.5696869 -4.5365424 -4.4659691 -4.4095368][-4.5066528 -4.584445 -4.6471858 -4.6587248 -4.6084671 -4.5301952 -4.4477882 -4.3870106 -4.3834004 -4.4319592 -4.4991312 -4.5326128 -4.5038524 -4.42928 -4.3741632][-4.5072527 -4.5723863 -4.6181722 -4.610961 -4.5377727 -4.4271336 -4.3134818 -4.2522264 -4.2747312 -4.3509116 -4.449265 -4.5011663 -4.4749527 -4.3964038 -4.3383474][-4.4910059 -4.5179405 -4.52923 -4.5048766 -4.4280028 -4.3048434 -4.1662979 -4.1101665 -4.162982 -4.2579203 -4.3807144 -4.4587517 -4.4442992 -4.36696 -4.3026886][-4.4325347 -4.4211597 -4.4028592 -4.3708963 -4.3077006 -4.1902618 -4.0344343 -3.9808435 -4.0535283 -4.1530137 -4.2868648 -4.3901019 -4.3959823 -4.33227 -4.2682972][-4.3415976 -4.3060508 -4.2710147 -4.2377419 -4.1919932 -4.0926266 -3.9335771 -3.883956 -3.9705732 -4.0705848 -4.2063174 -4.3290882 -4.3607044 -4.3217 -4.2638011][-4.253768 -4.2132387 -4.1821914 -4.1593671 -4.126668 -4.047173 -3.9032893 -3.8624744 -3.9526081 -4.049345 -4.1783462 -4.3106694 -4.3644133 -4.3415961 -4.2869439][-4.206706 -4.1863809 -4.1763763 -4.16324 -4.1364555 -4.0822859 -3.9858546 -3.9639049 -4.0403438 -4.1187153 -4.2209167 -4.3411422 -4.4019604 -4.3792367 -4.32318][-4.218338 -4.2290173 -4.2381835 -4.2219224 -4.1967173 -4.1704974 -4.12783 -4.1265521 -4.179059 -4.2307892 -4.3015423 -4.3979897 -4.4590859 -4.4402657 -4.3877082][-4.2661114 -4.3121281 -4.3350139 -4.3146863 -4.2964506 -4.2971625 -4.2909017 -4.2940211 -4.3151546 -4.340116 -4.3796558 -4.4459982 -4.5062904 -4.50793 -4.4740076][-4.2931938 -4.3713408 -4.4151053 -4.4048252 -4.4051046 -4.4285479 -4.4412651 -4.4393139 -4.4344239 -4.4333115 -4.4402013 -4.4684563 -4.5138917 -4.5373359 -4.5339122][-4.2816172 -4.3786268 -4.4509273 -4.470129 -4.4899526 -4.5187955 -4.53103 -4.5199289 -4.4974537 -4.47738 -4.4620132 -4.4642344 -4.4957204 -4.5305243 -4.5473132][-4.241281 -4.3379574 -4.4327435 -4.481813 -4.5117259 -4.5270605 -4.5239277 -4.5036387 -4.4706054 -4.4405546 -4.4174361 -4.411212 -4.4351292 -4.46899 -4.4912124][-4.1896162 -4.2653112 -4.3619728 -4.424099 -4.4560304 -4.4594607 -4.4484129 -4.4281936 -4.3951354 -4.3615336 -4.3359609 -4.3261733 -4.3443522 -4.3747864 -4.3987541]]...]
INFO - root - 2017-12-07 11:35:24.187473: step 12710, loss = 21.33, batch loss = 21.24 (8.1 examples/sec; 0.992 sec/batch; 88h:08m:15s remains)
INFO - root - 2017-12-07 11:35:33.662475: step 12720, loss = 21.23, batch loss = 21.15 (8.0 examples/sec; 0.999 sec/batch; 88h:42m:40s remains)
INFO - root - 2017-12-07 11:35:43.084633: step 12730, loss = 21.46, batch loss = 21.37 (8.7 examples/sec; 0.917 sec/batch; 81h:28m:05s remains)
INFO - root - 2017-12-07 11:35:52.652570: step 12740, loss = 21.29, batch loss = 21.21 (8.2 examples/sec; 0.970 sec/batch; 86h:08m:22s remains)
INFO - root - 2017-12-07 11:36:01.904894: step 12750, loss = 21.56, batch loss = 21.48 (9.1 examples/sec; 0.881 sec/batch; 78h:15m:10s remains)
INFO - root - 2017-12-07 11:36:11.293647: step 12760, loss = 21.84, batch loss = 21.76 (8.7 examples/sec; 0.921 sec/batch; 81h:49m:48s remains)
INFO - root - 2017-12-07 11:36:20.744298: step 12770, loss = 21.53, batch loss = 21.45 (9.0 examples/sec; 0.890 sec/batch; 79h:03m:27s remains)
INFO - root - 2017-12-07 11:36:30.128673: step 12780, loss = 21.95, batch loss = 21.86 (9.1 examples/sec; 0.882 sec/batch; 78h:21m:41s remains)
INFO - root - 2017-12-07 11:36:39.517323: step 12790, loss = 21.50, batch loss = 21.41 (8.3 examples/sec; 0.968 sec/batch; 85h:55m:53s remains)
INFO - root - 2017-12-07 11:36:48.944284: step 12800, loss = 21.73, batch loss = 21.65 (8.3 examples/sec; 0.965 sec/batch; 85h:41m:16s remains)
2017-12-07 11:36:49.828257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3984056 -4.4842019 -4.5473409 -4.5760894 -4.5684338 -4.5478296 -4.5368047 -4.5336456 -4.5349789 -4.5310626 -4.52236 -4.4967113 -4.4470296 -4.3844213 -4.32664][-4.4003911 -4.5169721 -4.5915041 -4.6148853 -4.583756 -4.5197873 -4.4675279 -4.4576497 -4.50229 -4.5586157 -4.5767045 -4.5328259 -4.4494143 -4.361846 -4.2907882][-4.4250345 -4.5563173 -4.6300449 -4.6447058 -4.5932584 -4.4878922 -4.3851829 -4.3595014 -4.4452987 -4.5639539 -4.6178775 -4.572928 -4.4821138 -4.4018936 -4.3433261][-4.4534926 -4.5704556 -4.6227674 -4.6150966 -4.538475 -4.3963485 -4.2484879 -4.2054563 -4.3222656 -4.4899993 -4.5821948 -4.5548296 -4.4749913 -4.4165292 -4.3790359][-4.4873495 -4.5762696 -4.5900836 -4.53598 -4.4095216 -4.2145905 -4.0184073 -3.9611146 -4.1100512 -4.3300581 -4.4764962 -4.4916353 -4.4335113 -4.3905129 -4.3602657][-4.5191193 -4.5760717 -4.5430231 -4.4229145 -4.2216644 -3.9582667 -3.7141895 -3.65282 -3.8445461 -4.1276035 -4.3438444 -4.418416 -4.3893828 -4.3542266 -4.3192739][-4.5517111 -4.5849633 -4.5205736 -4.34817 -4.0790944 -3.7552993 -3.4775653 -3.4249845 -3.6560373 -3.9877853 -4.25574 -4.3770351 -4.3792682 -4.361773 -4.3387113][-4.5905542 -4.6264629 -4.5721459 -4.3981266 -4.1080723 -3.7618601 -3.4774365 -3.4337814 -3.6683493 -3.9996364 -4.2689676 -4.3940096 -4.4078307 -4.4127278 -4.4208255][-4.5913587 -4.6487985 -4.6435652 -4.5282464 -4.2829647 -3.9662631 -3.6972125 -3.6422167 -3.830991 -4.1138868 -4.3440804 -4.4371324 -4.433589 -4.4442616 -4.4804564][-4.5239186 -4.5912371 -4.6356263 -4.60518 -4.451726 -4.2079768 -3.9734659 -3.896286 -4.0152626 -4.2303524 -4.4116678 -4.4661546 -4.4345016 -4.4335127 -4.4790487][-4.4215574 -4.47455 -4.5395503 -4.5780149 -4.5214243 -4.3672647 -4.1851892 -4.10258 -4.1661949 -4.3193364 -4.4545431 -4.4779644 -4.4245181 -4.407526 -4.4435635][-4.3346786 -4.3658667 -4.4308977 -4.5054054 -4.519341 -4.445128 -4.3263578 -4.2648835 -4.3004518 -4.4013367 -4.488019 -4.4816918 -4.4143004 -4.3841414 -4.402411][-4.2866979 -4.3040624 -4.363101 -4.4461923 -4.4940677 -4.4743066 -4.4138436 -4.3837628 -4.4075723 -4.4642282 -4.5025616 -4.471725 -4.4014645 -4.3661542 -4.3669939][-4.2899914 -4.2977705 -4.345222 -4.4188414 -4.4728384 -4.481698 -4.4611359 -4.4539871 -4.4679933 -4.4888062 -4.4904556 -4.4508324 -4.3931737 -4.3620167 -4.3496966][-4.3454833 -4.3448653 -4.37766 -4.4334736 -4.4780045 -4.4947968 -4.4933696 -4.4962978 -4.50164 -4.4988174 -4.4794569 -4.4431853 -4.4058218 -4.3842969 -4.3657088]]...]
INFO - root - 2017-12-07 11:36:59.143491: step 12810, loss = 21.49, batch loss = 21.41 (8.6 examples/sec; 0.930 sec/batch; 82h:35m:10s remains)
INFO - root - 2017-12-07 11:37:08.471349: step 12820, loss = 21.44, batch loss = 21.36 (8.4 examples/sec; 0.955 sec/batch; 84h:50m:53s remains)
INFO - root - 2017-12-07 11:37:18.017272: step 12830, loss = 21.59, batch loss = 21.50 (8.0 examples/sec; 1.006 sec/batch; 89h:17m:28s remains)
INFO - root - 2017-12-07 11:37:27.455002: step 12840, loss = 21.47, batch loss = 21.39 (8.0 examples/sec; 1.002 sec/batch; 88h:57m:04s remains)
INFO - root - 2017-12-07 11:37:36.978342: step 12850, loss = 21.65, batch loss = 21.56 (8.6 examples/sec; 0.932 sec/batch; 82h:43m:18s remains)
INFO - root - 2017-12-07 11:37:46.359729: step 12860, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.943 sec/batch; 83h:41m:13s remains)
INFO - root - 2017-12-07 11:37:55.755327: step 12870, loss = 21.60, batch loss = 21.52 (7.9 examples/sec; 1.009 sec/batch; 89h:34m:03s remains)
INFO - root - 2017-12-07 11:38:05.161570: step 12880, loss = 21.42, batch loss = 21.33 (8.1 examples/sec; 0.986 sec/batch; 87h:30m:22s remains)
INFO - root - 2017-12-07 11:38:14.595796: step 12890, loss = 21.29, batch loss = 21.21 (8.6 examples/sec; 0.927 sec/batch; 82h:15m:45s remains)
INFO - root - 2017-12-07 11:38:23.960258: step 12900, loss = 21.68, batch loss = 21.60 (8.6 examples/sec; 0.933 sec/batch; 82h:51m:27s remains)
2017-12-07 11:38:24.951965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5638933 -4.6500053 -4.7150249 -4.7522044 -4.7555275 -4.7426076 -4.72344 -4.7041221 -4.7014227 -4.7151322 -4.7125416 -4.6513262 -4.5459542 -4.4746113 -4.4825954][-4.519577 -4.6504297 -4.755805 -4.800674 -4.7777977 -4.7310438 -4.6842151 -4.6425819 -4.6238766 -4.62599 -4.6136503 -4.5455441 -4.434464 -4.3716526 -4.4039116][-4.456749 -4.6108432 -4.7429142 -4.7849474 -4.7320552 -4.6490097 -4.5739284 -4.5121703 -4.4817491 -4.4779558 -4.4732561 -4.4288731 -4.339715 -4.2951822 -4.3397632][-4.400856 -4.56284 -4.700274 -4.7269053 -4.6438565 -4.530755 -4.4327226 -4.3521695 -4.315238 -4.3155894 -4.3366175 -4.34687 -4.31146 -4.2988253 -4.340126][-4.3412342 -4.4909635 -4.6130438 -4.6127553 -4.5004735 -4.3676243 -4.2526684 -4.15589 -4.12144 -4.1429734 -4.2037368 -4.2734208 -4.2939749 -4.3108521 -4.3376708][-4.2881713 -4.3985114 -4.4852662 -4.4549766 -4.3234048 -4.1792045 -4.0471673 -3.9349575 -3.9134412 -3.9746726 -4.0801067 -4.1915112 -4.243495 -4.2678976 -4.268074][-4.2671247 -4.3318949 -4.3811064 -4.3342748 -4.1991472 -4.0432324 -3.8813019 -3.7490983 -3.7484653 -3.8617802 -4.0145688 -4.1497207 -4.2140584 -4.2392206 -4.2240915][-4.2550783 -4.3085279 -4.3476419 -4.3033757 -4.1755128 -4.00748 -3.8131442 -3.6652839 -3.6868668 -3.8435674 -4.0280781 -4.1706834 -4.2384076 -4.2656236 -4.2511044][-4.2222886 -4.28932 -4.3411913 -4.3125963 -4.2064462 -4.0515552 -3.8612936 -3.7279294 -3.7695448 -3.9388421 -4.1174808 -4.2447 -4.304111 -4.3230405 -4.3015618][-4.1939178 -4.2688179 -4.3293033 -4.3201289 -4.2495761 -4.13734 -3.9882927 -3.8900449 -3.9439592 -4.1004839 -4.2476006 -4.3444133 -4.3907452 -4.3969259 -4.362895][-4.206089 -4.2572603 -4.296586 -4.296834 -4.2633228 -4.1997414 -4.1012449 -4.0423474 -4.1040211 -4.2389936 -4.3489609 -4.41361 -4.4494376 -4.4485221 -4.4063292][-4.2521653 -4.2532248 -4.2479892 -4.2429361 -4.2407246 -4.2191253 -4.1634154 -4.1351252 -4.1929607 -4.2991643 -4.3746233 -4.4140916 -4.4441023 -4.4460354 -4.4079866][-4.302896 -4.26144 -4.2239327 -4.2208247 -4.2507153 -4.2632174 -4.2325768 -4.2177405 -4.2586322 -4.3275018 -4.3690987 -4.3875022 -4.4119081 -4.4234657 -4.4012446][-4.3157506 -4.2552 -4.2231326 -4.2473111 -4.3157382 -4.3557229 -4.3301435 -4.3093944 -4.3277559 -4.3629832 -4.3749342 -4.3647695 -4.36637 -4.37685 -4.3712726][-4.264183 -4.1974564 -4.1878715 -4.2530274 -4.3605909 -4.4249392 -4.3975925 -4.3554978 -4.3497491 -4.3640318 -4.3610191 -4.3321915 -4.3147964 -4.3238678 -4.3360233]]...]
INFO - root - 2017-12-07 11:38:34.208689: step 12910, loss = 21.61, batch loss = 21.53 (8.5 examples/sec; 0.946 sec/batch; 83h:59m:29s remains)
INFO - root - 2017-12-07 11:38:43.504289: step 12920, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.926 sec/batch; 82h:14m:16s remains)
INFO - root - 2017-12-07 11:38:52.920692: step 12930, loss = 21.62, batch loss = 21.53 (8.7 examples/sec; 0.925 sec/batch; 82h:05m:11s remains)
INFO - root - 2017-12-07 11:39:02.428957: step 12940, loss = 21.05, batch loss = 20.97 (8.7 examples/sec; 0.916 sec/batch; 81h:21m:08s remains)
INFO - root - 2017-12-07 11:39:11.909290: step 12950, loss = 21.41, batch loss = 21.33 (8.7 examples/sec; 0.916 sec/batch; 81h:18m:09s remains)
INFO - root - 2017-12-07 11:39:21.333783: step 12960, loss = 21.82, batch loss = 21.73 (8.2 examples/sec; 0.980 sec/batch; 87h:00m:23s remains)
INFO - root - 2017-12-07 11:39:30.634831: step 12970, loss = 21.28, batch loss = 21.20 (9.0 examples/sec; 0.890 sec/batch; 78h:59m:11s remains)
INFO - root - 2017-12-07 11:39:39.862481: step 12980, loss = 21.11, batch loss = 21.03 (9.3 examples/sec; 0.864 sec/batch; 76h:41m:33s remains)
INFO - root - 2017-12-07 11:39:49.293914: step 12990, loss = 21.09, batch loss = 21.00 (8.6 examples/sec; 0.932 sec/batch; 82h:43m:42s remains)
INFO - root - 2017-12-07 11:39:58.700015: step 13000, loss = 22.19, batch loss = 22.11 (8.5 examples/sec; 0.943 sec/batch; 83h:39m:49s remains)
2017-12-07 11:39:59.668879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5756655 -4.5588641 -4.53726 -4.5216455 -4.5178566 -4.5153818 -4.4917626 -4.4480352 -4.4031734 -4.382442 -4.4123297 -4.4757261 -4.5522332 -4.62123 -4.6478491][-4.6731377 -4.6750627 -4.66949 -4.6617484 -4.65868 -4.6506433 -4.6105571 -4.5424476 -4.47219 -4.4276204 -4.4416757 -4.491281 -4.5530643 -4.6066456 -4.6184134][-4.67161 -4.6995397 -4.7265077 -4.7400441 -4.7401633 -4.7239437 -4.6676216 -4.5813527 -4.497354 -4.4416709 -4.4427953 -4.4757247 -4.5149288 -4.5431194 -4.5398126][-4.5844426 -4.6261554 -4.6895666 -4.7333236 -4.7343712 -4.6995096 -4.6211419 -4.5197287 -4.4410748 -4.3989887 -4.4063864 -4.4375253 -4.4600334 -4.4625325 -4.4442172][-4.4553785 -4.5078616 -4.5913968 -4.6355829 -4.5957432 -4.5037379 -4.3834229 -4.2744665 -4.2331753 -4.247067 -4.2991066 -4.3633518 -4.3959131 -4.3886023 -4.3613043][-4.3457041 -4.4171 -4.4969687 -4.4961586 -4.3712754 -4.1938925 -4.0222921 -3.9126427 -3.9258056 -4.0202131 -4.143887 -4.2698884 -4.342504 -4.348175 -4.3193965][-4.3391571 -4.4389343 -4.4969878 -4.4179168 -4.1914926 -3.9352379 -3.7254696 -3.62071 -3.6840966 -3.8558645 -4.0515032 -4.2384768 -4.3532658 -4.3767834 -4.3411288][-4.40475 -4.5354838 -4.5662932 -4.4103065 -4.1007929 -3.7859087 -3.5481739 -3.4502683 -3.5528874 -3.7923882 -4.0531006 -4.284266 -4.4242711 -4.4548168 -4.4036322][-4.4325457 -4.5869813 -4.6191864 -4.4512525 -4.1312132 -3.8047585 -3.5584402 -3.4658802 -3.5909767 -3.8617532 -4.1399779 -4.36575 -4.493784 -4.5153775 -4.4517889][-4.3913774 -4.559195 -4.633286 -4.5343685 -4.2911043 -4.0169206 -3.7957258 -3.7148757 -3.8322153 -4.0656571 -4.2849402 -4.4453969 -4.5276256 -4.5350723 -4.472898][-4.3071957 -4.4898529 -4.6087971 -4.5845532 -4.43054 -4.2265244 -4.0513496 -3.9942131 -4.0889359 -4.2509913 -4.3853946 -4.4756331 -4.5222621 -4.5289435 -4.4818258][-4.2483649 -4.4294086 -4.5489483 -4.5500436 -4.4547 -4.3211341 -4.2120304 -4.1910067 -4.2613616 -4.3515992 -4.4150038 -4.4623628 -4.4988995 -4.5154123 -4.4853883][-4.2121749 -4.3630357 -4.4439254 -4.4373546 -4.3846745 -4.3265104 -4.2968407 -4.3211069 -4.37657 -4.4181876 -4.43709 -4.4607062 -4.4884558 -4.5010161 -4.4740658][-4.1431813 -4.2558541 -4.2967315 -4.2809758 -4.2633185 -4.2681651 -4.3070288 -4.3741035 -4.4339719 -4.4613214 -4.4651351 -4.4734468 -4.482017 -4.4761529 -4.4431553][-4.0454354 -4.1477661 -4.1957331 -4.207715 -4.2295671 -4.2681689 -4.3259768 -4.3947334 -4.446064 -4.467279 -4.465198 -4.4584856 -4.4492993 -4.4358411 -4.412385]]...]
INFO - root - 2017-12-07 11:40:09.145168: step 13010, loss = 21.18, batch loss = 21.10 (9.1 examples/sec; 0.875 sec/batch; 77h:41m:48s remains)
INFO - root - 2017-12-07 11:40:18.434896: step 13020, loss = 21.40, batch loss = 21.32 (8.8 examples/sec; 0.911 sec/batch; 80h:49m:26s remains)
INFO - root - 2017-12-07 11:40:27.892905: step 13030, loss = 21.37, batch loss = 21.29 (7.9 examples/sec; 1.008 sec/batch; 89h:29m:37s remains)
INFO - root - 2017-12-07 11:40:37.258858: step 13040, loss = 21.38, batch loss = 21.30 (8.0 examples/sec; 0.994 sec/batch; 88h:11m:41s remains)
INFO - root - 2017-12-07 11:40:46.547108: step 13050, loss = 21.56, batch loss = 21.47 (8.3 examples/sec; 0.968 sec/batch; 85h:52m:17s remains)
INFO - root - 2017-12-07 11:40:55.916094: step 13060, loss = 21.61, batch loss = 21.53 (8.4 examples/sec; 0.953 sec/batch; 84h:35m:54s remains)
INFO - root - 2017-12-07 11:41:05.195273: step 13070, loss = 21.33, batch loss = 21.24 (8.1 examples/sec; 0.993 sec/batch; 88h:04m:27s remains)
INFO - root - 2017-12-07 11:41:14.802467: step 13080, loss = 20.93, batch loss = 20.85 (8.5 examples/sec; 0.939 sec/batch; 83h:17m:55s remains)
INFO - root - 2017-12-07 11:41:24.134746: step 13090, loss = 21.36, batch loss = 21.28 (8.6 examples/sec; 0.926 sec/batch; 82h:07m:52s remains)
INFO - root - 2017-12-07 11:41:33.417504: step 13100, loss = 21.43, batch loss = 21.34 (8.7 examples/sec; 0.917 sec/batch; 81h:23m:53s remains)
2017-12-07 11:41:34.419334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4681253 -4.5055161 -4.5327177 -4.550952 -4.5759912 -4.5963511 -4.6094952 -4.6241269 -4.6204028 -4.5920563 -4.5666695 -4.5659871 -4.590538 -4.6197758 -4.6388497][-4.5255051 -4.5784 -4.6121507 -4.6321068 -4.6586266 -4.6767163 -4.6838984 -4.6977282 -4.6902914 -4.6429377 -4.6020436 -4.605413 -4.65155 -4.705008 -4.7348533][-4.5612149 -4.6158953 -4.6403995 -4.6435986 -4.651794 -4.6518021 -4.6424823 -4.6627655 -4.6738415 -4.6251764 -4.5770388 -4.5798068 -4.6386833 -4.7151375 -4.76459][-4.564599 -4.6112633 -4.6196957 -4.5967975 -4.5669003 -4.5307379 -4.49681 -4.5315995 -4.5708046 -4.5197225 -4.4620256 -4.4599032 -4.5348935 -4.6484842 -4.7349715][-4.5537763 -4.5901051 -4.5822396 -4.5291281 -4.4492073 -4.367516 -4.299067 -4.3282914 -4.3680067 -4.2921042 -4.2259946 -4.2335577 -4.3432393 -4.5128088 -4.650465][-4.5448341 -4.5601292 -4.5207648 -4.4279242 -4.303875 -4.1931038 -4.1023726 -4.12153 -4.1495562 -4.0564346 -3.9981565 -4.0185251 -4.1435418 -4.3435144 -4.5134821][-4.5377607 -4.5128956 -4.4142962 -4.2597175 -4.0894957 -3.9678335 -3.893266 -3.943177 -3.9990935 -3.9309447 -3.8966155 -3.9102197 -4.0074568 -4.1851969 -4.3532181][-4.5019031 -4.4263654 -4.2656512 -4.0591283 -3.8641067 -3.7496219 -3.7122028 -3.8096404 -3.9054532 -3.8756328 -3.8727617 -3.890028 -3.9643159 -4.1090474 -4.2593565][-4.4550257 -4.3762574 -4.2324066 -4.062521 -3.9055743 -3.8099744 -3.778496 -3.8704412 -3.9592404 -3.9427853 -3.9660945 -4.0144367 -4.1012688 -4.2296863 -4.3498588][-4.467895 -4.455512 -4.4077821 -4.3326335 -4.2287383 -4.1256223 -4.0479736 -4.0786481 -4.1207428 -4.0928292 -4.1251736 -4.2110157 -4.329752 -4.4572864 -4.5457335][-4.5184994 -4.5765533 -4.6052876 -4.59342 -4.5195727 -4.4057159 -4.2887726 -4.2619781 -4.2582521 -4.2180252 -4.2469678 -4.3509636 -4.4819174 -4.601656 -4.6651669][-4.5667534 -4.6655583 -4.7313972 -4.7467132 -4.6977696 -4.5969071 -4.4738293 -4.4166718 -4.390954 -4.3574915 -4.3843145 -4.4740076 -4.5737758 -4.6512747 -4.679862][-4.5928063 -4.7105265 -4.7878256 -4.8059921 -4.7688661 -4.6864681 -4.5830069 -4.5300903 -4.512949 -4.5042987 -4.5333176 -4.5899005 -4.6352758 -4.6555033 -4.6466403][-4.5434766 -4.6388049 -4.6990347 -4.7110271 -4.6865635 -4.63953 -4.5833864 -4.5621123 -4.5656323 -4.569025 -4.5813808 -4.5950537 -4.5957017 -4.5830379 -4.5624924][-4.4554324 -4.5054693 -4.5393577 -4.5504 -4.54586 -4.5337095 -4.5171838 -4.5173311 -4.5254159 -4.5249033 -4.5201411 -4.5121326 -4.5010419 -4.4897318 -4.4794879]]...]
INFO - root - 2017-12-07 11:41:43.801246: step 13110, loss = 21.29, batch loss = 21.21 (8.2 examples/sec; 0.979 sec/batch; 86h:51m:28s remains)
INFO - root - 2017-12-07 11:41:53.094917: step 13120, loss = 21.43, batch loss = 21.35 (8.4 examples/sec; 0.950 sec/batch; 84h:15m:02s remains)
INFO - root - 2017-12-07 11:42:02.460174: step 13130, loss = 21.46, batch loss = 21.38 (8.6 examples/sec; 0.934 sec/batch; 82h:53m:52s remains)
INFO - root - 2017-12-07 11:42:11.851603: step 13140, loss = 22.11, batch loss = 22.03 (8.3 examples/sec; 0.964 sec/batch; 85h:33m:11s remains)
INFO - root - 2017-12-07 11:42:21.217852: step 13150, loss = 21.39, batch loss = 21.31 (8.7 examples/sec; 0.918 sec/batch; 81h:28m:10s remains)
INFO - root - 2017-12-07 11:42:30.598655: step 13160, loss = 21.54, batch loss = 21.46 (8.7 examples/sec; 0.916 sec/batch; 81h:14m:45s remains)
INFO - root - 2017-12-07 11:42:39.925760: step 13170, loss = 22.32, batch loss = 22.24 (9.3 examples/sec; 0.856 sec/batch; 75h:56m:21s remains)
INFO - root - 2017-12-07 11:42:49.506136: step 13180, loss = 21.77, batch loss = 21.68 (8.3 examples/sec; 0.962 sec/batch; 85h:21m:16s remains)
INFO - root - 2017-12-07 11:42:58.956873: step 13190, loss = 21.20, batch loss = 21.11 (8.0 examples/sec; 0.998 sec/batch; 88h:29m:29s remains)
INFO - root - 2017-12-07 11:43:08.424215: step 13200, loss = 21.68, batch loss = 21.59 (7.7 examples/sec; 1.036 sec/batch; 91h:52m:48s remains)
2017-12-07 11:43:09.317331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5129991 -4.5376263 -4.5482378 -4.5694809 -4.6008086 -4.6264253 -4.6431627 -4.6534986 -4.6598854 -4.6712871 -4.6882205 -4.686635 -4.6574688 -4.6184549 -4.5892172][-4.5522714 -4.5554123 -4.5408492 -4.5446095 -4.5711303 -4.6050153 -4.6314888 -4.6316843 -4.6246595 -4.6411309 -4.6774521 -4.6878467 -4.6563487 -4.6177969 -4.5962782][-4.5367994 -4.5066133 -4.4495482 -4.4164662 -4.4289427 -4.47944 -4.5256805 -4.51346 -4.4873948 -4.5154614 -4.584084 -4.6114292 -4.5717826 -4.5288324 -4.5194883][-4.4982066 -4.426353 -4.3177214 -4.235517 -4.227901 -4.2970939 -4.3676267 -4.3476057 -4.3060336 -4.3500867 -4.4586954 -4.5042496 -4.4479823 -4.3876038 -4.3860574][-4.4574475 -4.3400226 -4.1816382 -4.0518818 -4.0196695 -4.0900521 -4.1701465 -4.151547 -4.1126184 -4.18198 -4.3315325 -4.3969059 -4.3314533 -4.2596388 -4.2639937][-4.4330959 -4.2818751 -4.0872931 -3.9179831 -3.85049 -3.8895628 -3.9543569 -3.9517198 -3.9448497 -4.0500331 -4.226778 -4.2942963 -4.2111921 -4.1249166 -4.1331186][-4.4460816 -4.2842226 -4.0741525 -3.8793802 -3.7728543 -3.7650733 -3.7987785 -3.8142743 -3.844234 -3.9692276 -4.1476154 -4.1984315 -4.0967841 -4.00919 -4.0292754][-4.4864984 -4.330039 -4.1249423 -3.9342713 -3.8209071 -3.7895365 -3.79661 -3.8117833 -3.8440661 -3.9449806 -4.0930166 -4.1203451 -4.0097289 -3.9343574 -3.9728222][-4.5024619 -4.3611588 -4.1827765 -4.0341611 -3.9552283 -3.931839 -3.9287491 -3.9286647 -3.9285495 -3.9751861 -4.07826 -4.0847015 -3.9818113 -3.9297917 -3.991704][-4.5044131 -4.3759017 -4.2250147 -4.1327662 -4.1102219 -4.1116705 -4.1083426 -4.0865936 -4.0450788 -4.0451684 -4.1118755 -4.1042972 -4.0146894 -3.9863477 -4.0600944][-4.5098529 -4.3663597 -4.2151742 -4.1615038 -4.1897173 -4.2173343 -4.2186947 -4.1866784 -4.1334529 -4.1289425 -4.1894298 -4.1807995 -4.1017284 -4.0832944 -4.1442461][-4.5492921 -4.3744321 -4.1926661 -4.1369991 -4.1902013 -4.2410007 -4.2575097 -4.2333093 -4.196321 -4.2249584 -4.30487 -4.3088174 -4.2366223 -4.204524 -4.2299337][-4.5982623 -4.4214716 -4.2246304 -4.1532931 -4.2073522 -4.2730308 -4.3106093 -4.3021727 -4.2844968 -4.338429 -4.4267097 -4.4389968 -4.3687105 -4.3103786 -4.2965274][-4.5906162 -4.4614 -4.3055348 -4.2480674 -4.3017278 -4.3748927 -4.4298992 -4.4373894 -4.4293828 -4.4798284 -4.5492048 -4.5553708 -4.48804 -4.4132724 -4.3782811][-4.5313931 -4.468163 -4.3856363 -4.3629527 -4.4136248 -4.4806666 -4.5380816 -4.5602674 -4.5638452 -4.6006474 -4.6403751 -4.6310468 -4.5695343 -4.4992871 -4.4658518]]...]
INFO - root - 2017-12-07 11:43:18.706475: step 13210, loss = 21.54, batch loss = 21.46 (8.4 examples/sec; 0.949 sec/batch; 84h:12m:02s remains)
INFO - root - 2017-12-07 11:43:28.165524: step 13220, loss = 22.09, batch loss = 22.01 (8.5 examples/sec; 0.943 sec/batch; 83h:38m:31s remains)
INFO - root - 2017-12-07 11:43:37.519608: step 13230, loss = 21.30, batch loss = 21.22 (8.4 examples/sec; 0.950 sec/batch; 84h:15m:18s remains)
INFO - root - 2017-12-07 11:43:46.914790: step 13240, loss = 21.33, batch loss = 21.25 (9.0 examples/sec; 0.889 sec/batch; 78h:47m:43s remains)
INFO - root - 2017-12-07 11:43:56.429009: step 13250, loss = 21.70, batch loss = 21.61 (8.5 examples/sec; 0.943 sec/batch; 83h:36m:40s remains)
INFO - root - 2017-12-07 11:44:05.956207: step 13260, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.947 sec/batch; 83h:58m:42s remains)
INFO - root - 2017-12-07 11:44:15.148174: step 13270, loss = 21.30, batch loss = 21.22 (8.0 examples/sec; 1.000 sec/batch; 88h:37m:59s remains)
INFO - root - 2017-12-07 11:44:24.454513: step 13280, loss = 21.64, batch loss = 21.56 (8.4 examples/sec; 0.954 sec/batch; 84h:37m:45s remains)
INFO - root - 2017-12-07 11:44:33.780677: step 13290, loss = 21.92, batch loss = 21.84 (8.9 examples/sec; 0.900 sec/batch; 79h:48m:57s remains)
INFO - root - 2017-12-07 11:44:43.135175: step 13300, loss = 21.73, batch loss = 21.65 (8.9 examples/sec; 0.896 sec/batch; 79h:27m:53s remains)
2017-12-07 11:44:44.065836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3385329 -4.3843169 -4.4496684 -4.493372 -4.500977 -4.5394773 -4.6187973 -4.7007694 -4.7167807 -4.6727133 -4.611774 -4.5381832 -4.5260949 -4.5491791 -4.5864234][-4.37014 -4.43452 -4.4946837 -4.5088139 -4.4723873 -4.4861393 -4.5753865 -4.6662006 -4.6808281 -4.6181412 -4.5323849 -4.4511395 -4.4565735 -4.4853005 -4.5200176][-4.3738594 -4.4434018 -4.4924879 -4.4788418 -4.4009814 -4.3831906 -4.46433 -4.562067 -4.5923524 -4.5288382 -4.421165 -4.3342528 -4.3563204 -4.3957376 -4.4414849][-4.3496261 -4.4117889 -4.4416566 -4.4020314 -4.2860851 -4.2176566 -4.2615585 -4.3649945 -4.4430022 -4.4176354 -4.3143373 -4.2299776 -4.2499456 -4.2941337 -4.3604321][-4.3071666 -4.3491716 -4.3581872 -4.2972331 -4.1413245 -4.0086641 -3.9895256 -4.0920806 -4.2282882 -4.26743 -4.2096472 -4.150733 -4.16461 -4.2079792 -4.2982464][-4.2986212 -4.3138251 -4.2976122 -4.2071204 -4.006453 -3.8029373 -3.7020507 -3.7831728 -3.9641166 -4.0807195 -4.1095057 -4.1087766 -4.1307917 -4.1799884 -4.2901616][-4.3448458 -4.3194537 -4.2654605 -4.1343174 -3.8917794 -3.638278 -3.4748659 -3.5276594 -3.7303195 -3.9041693 -4.0104728 -4.0772867 -4.1351967 -4.2118907 -4.3404679][-4.4759235 -4.4108849 -4.3201575 -4.1619 -3.9092515 -3.6501899 -3.4755192 -3.5223148 -3.7219653 -3.8997064 -4.0221977 -4.1270852 -4.22183 -4.3259234 -4.45434][-4.6711512 -4.5937366 -4.485837 -4.3240767 -4.0909214 -3.8668401 -3.7345839 -3.797457 -3.9728041 -4.11125 -4.1987238 -4.2936783 -4.3907313 -4.4869781 -4.5794482][-4.8023829 -4.7399497 -4.6444693 -4.5121508 -4.3299475 -4.1627231 -4.0910544 -4.1669555 -4.3026333 -4.386426 -4.4259448 -4.486557 -4.5553436 -4.6166806 -4.6529794][-4.8424006 -4.8229103 -4.770843 -4.6964602 -4.5895562 -4.4892282 -4.4572096 -4.5156488 -4.5966268 -4.6261272 -4.6240139 -4.6467767 -4.681632 -4.7037058 -4.6845083][-4.8211784 -4.8506966 -4.8493819 -4.8240857 -4.7718992 -4.7168689 -4.7002492 -4.7313247 -4.7683258 -4.7628531 -4.7328663 -4.7191114 -4.7187061 -4.7082133 -4.657937][-4.7311716 -4.7855849 -4.8132105 -4.8147364 -4.792521 -4.7623639 -4.7491589 -4.7578688 -4.769197 -4.7530351 -4.716536 -4.6855497 -4.6616607 -4.6328769 -4.5789165][-4.5971379 -4.6443954 -4.6744661 -4.6847186 -4.6798453 -4.6671815 -4.65944 -4.6590328 -4.6596303 -4.6489115 -4.6255126 -4.5972624 -4.5645738 -4.5263205 -4.47876][-4.4469728 -4.4751105 -4.4967155 -4.5084133 -4.5122304 -4.5104704 -4.5080633 -4.5064058 -4.5050707 -4.5003424 -4.488173 -4.4663844 -4.4351215 -4.3997865 -4.3658886]]...]
INFO - root - 2017-12-07 11:44:53.476511: step 13310, loss = 21.25, batch loss = 21.17 (8.7 examples/sec; 0.923 sec/batch; 81h:47m:40s remains)
INFO - root - 2017-12-07 11:45:02.939388: step 13320, loss = 21.81, batch loss = 21.73 (8.6 examples/sec; 0.933 sec/batch; 82h:44m:18s remains)
INFO - root - 2017-12-07 11:45:12.266017: step 13330, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.972 sec/batch; 86h:12m:22s remains)
INFO - root - 2017-12-07 11:45:21.707925: step 13340, loss = 21.29, batch loss = 21.21 (9.0 examples/sec; 0.893 sec/batch; 79h:07m:45s remains)
INFO - root - 2017-12-07 11:45:31.058070: step 13350, loss = 21.57, batch loss = 21.49 (8.9 examples/sec; 0.900 sec/batch; 79h:47m:28s remains)
INFO - root - 2017-12-07 11:45:40.466307: step 13360, loss = 21.90, batch loss = 21.81 (8.7 examples/sec; 0.920 sec/batch; 81h:33m:01s remains)
INFO - root - 2017-12-07 11:45:49.945972: step 13370, loss = 21.17, batch loss = 21.09 (8.5 examples/sec; 0.936 sec/batch; 82h:57m:50s remains)
INFO - root - 2017-12-07 11:45:59.416357: step 13380, loss = 21.83, batch loss = 21.74 (8.4 examples/sec; 0.952 sec/batch; 84h:21m:32s remains)
INFO - root - 2017-12-07 11:46:08.803704: step 13390, loss = 21.69, batch loss = 21.61 (8.5 examples/sec; 0.940 sec/batch; 83h:19m:41s remains)
INFO - root - 2017-12-07 11:46:18.258265: step 13400, loss = 21.26, batch loss = 21.18 (8.5 examples/sec; 0.946 sec/batch; 83h:50m:55s remains)
2017-12-07 11:46:19.137874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4583178 -4.554143 -4.6440053 -4.6949639 -4.7024465 -4.6678004 -4.6126671 -4.5658212 -4.5216088 -4.4452696 -4.373755 -4.3240414 -4.3348246 -4.4236112 -4.5283966][-4.4891992 -4.5813046 -4.6548829 -4.6771817 -4.65719 -4.6091418 -4.5595 -4.5342035 -4.5267339 -4.4911532 -4.4480739 -4.4111028 -4.4267159 -4.5106945 -4.5918908][-4.5156565 -4.6048455 -4.6632643 -4.6652427 -4.6343093 -4.5971022 -4.5733132 -4.570384 -4.5819435 -4.5660934 -4.5343394 -4.5075345 -4.5267353 -4.5966597 -4.6518521][-4.5432916 -4.6278353 -4.6739011 -4.6614256 -4.6257682 -4.5964818 -4.5825257 -4.5779347 -4.5776844 -4.5540919 -4.51601 -4.5012479 -4.5346632 -4.6033053 -4.6520247][-4.5696087 -4.6529326 -4.6875467 -4.6546721 -4.5938525 -4.535059 -4.4821682 -4.4409814 -4.42061 -4.4018884 -4.3812456 -4.3987136 -4.4621267 -4.5462255 -4.60454][-4.5919 -4.6731224 -4.6885481 -4.6130056 -4.4878869 -4.3547587 -4.2272105 -4.1430297 -4.1366758 -4.1705413 -4.2105813 -4.2826738 -4.3823843 -4.4813695 -4.5441675][-4.6066813 -4.6794848 -4.6631474 -4.5217676 -4.302846 -4.0747366 -3.8756959 -3.7765408 -3.8295975 -3.962745 -4.0962114 -4.2301021 -4.3536196 -4.4461808 -4.4923425][-4.6166253 -4.6829233 -4.6418004 -4.4512825 -4.1606579 -3.86836 -3.6368308 -3.5491209 -3.6572587 -3.8692541 -4.0768147 -4.2515993 -4.3790851 -4.4483552 -4.4644189][-4.6229897 -4.6924682 -4.6537967 -4.4661613 -4.1718025 -3.877696 -3.6616492 -3.5942044 -3.7098038 -3.9304166 -4.154325 -4.3323393 -4.444869 -4.4863138 -4.4715252][-4.6191421 -4.6995535 -4.6895146 -4.5509052 -4.3097248 -4.0605612 -3.884829 -3.8284547 -3.9088914 -4.0846047 -4.2825356 -4.4396968 -4.5264788 -4.5412917 -4.5015244][-4.5940952 -4.6874118 -4.7167206 -4.6487756 -4.4881148 -4.3053918 -4.1783371 -4.1321573 -4.1689992 -4.2817411 -4.4298406 -4.5476704 -4.6007037 -4.5889931 -4.5281539][-4.5434384 -4.6420302 -4.7047453 -4.7029257 -4.6238942 -4.5120282 -4.4351349 -4.4080157 -4.4221268 -4.4841495 -4.5726752 -4.6360011 -4.6457486 -4.6022224 -4.5219765][-4.4769015 -4.5650344 -4.6401467 -4.6813793 -4.6672716 -4.6182532 -4.5865684 -4.58312 -4.5968647 -4.6274724 -4.6582489 -4.6638813 -4.6312418 -4.560636 -4.4719629][-4.4064941 -4.4758134 -4.5457182 -4.6038604 -4.6307139 -4.6286793 -4.6288743 -4.641233 -4.6566606 -4.6641283 -4.6491213 -4.6102524 -4.5515552 -4.4738779 -4.3956323][-4.3443093 -4.3906403 -4.4426365 -4.4923253 -4.5279059 -4.545177 -4.5581355 -4.5735474 -4.5854764 -4.5805607 -4.5482316 -4.4978342 -4.4409494 -4.3787341 -4.3242474]]...]
INFO - root - 2017-12-07 11:46:28.451707: step 13410, loss = 21.54, batch loss = 21.46 (8.4 examples/sec; 0.957 sec/batch; 84h:47m:43s remains)
INFO - root - 2017-12-07 11:46:37.944226: step 13420, loss = 21.12, batch loss = 21.04 (8.0 examples/sec; 1.004 sec/batch; 88h:59m:07s remains)
INFO - root - 2017-12-07 11:46:47.273657: step 13430, loss = 21.66, batch loss = 21.58 (9.2 examples/sec; 0.873 sec/batch; 77h:24m:49s remains)
INFO - root - 2017-12-07 11:46:56.555992: step 13440, loss = 21.40, batch loss = 21.32 (9.4 examples/sec; 0.848 sec/batch; 75h:10m:35s remains)
INFO - root - 2017-12-07 11:47:06.104735: step 13450, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.978 sec/batch; 86h:38m:01s remains)
INFO - root - 2017-12-07 11:47:15.393039: step 13460, loss = 21.18, batch loss = 21.10 (8.6 examples/sec; 0.929 sec/batch; 82h:20m:37s remains)
INFO - root - 2017-12-07 11:47:24.778011: step 13470, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.955 sec/batch; 84h:39m:45s remains)
INFO - root - 2017-12-07 11:47:34.168789: step 13480, loss = 21.80, batch loss = 21.72 (8.6 examples/sec; 0.927 sec/batch; 82h:11m:02s remains)
INFO - root - 2017-12-07 11:47:43.600079: step 13490, loss = 21.49, batch loss = 21.41 (8.5 examples/sec; 0.939 sec/batch; 83h:12m:01s remains)
INFO - root - 2017-12-07 11:47:52.805349: step 13500, loss = 21.56, batch loss = 21.47 (9.1 examples/sec; 0.881 sec/batch; 78h:01m:59s remains)
2017-12-07 11:47:53.770524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3793211 -4.393147 -4.402667 -4.4112067 -4.4185262 -4.4145203 -4.4036531 -4.3887849 -4.3573337 -4.3172331 -4.28733 -4.2784314 -4.2993226 -4.3311343 -4.35117][-4.4001431 -4.42226 -4.4356065 -4.4413729 -4.4436064 -4.43426 -4.4218779 -4.4055533 -4.3643 -4.3120713 -4.2769923 -4.2677875 -4.2899222 -4.3218827 -4.337945][-4.4066024 -4.4368916 -4.4546046 -4.45843 -4.4580317 -4.4478912 -4.4357481 -4.4187555 -4.3695216 -4.3097458 -4.2769637 -4.2724738 -4.2937531 -4.3184285 -4.3272967][-4.41642 -4.4529433 -4.4713583 -4.4712219 -4.4663935 -4.4534268 -4.4391694 -4.4186535 -4.3650775 -4.3056188 -4.2794704 -4.28444 -4.3060083 -4.3225856 -4.3251743][-4.4024606 -4.4393482 -4.4553261 -4.4512329 -4.4406991 -4.4195933 -4.3938551 -4.358726 -4.3009033 -4.2511311 -4.2422123 -4.2670054 -4.2990241 -4.3149824 -4.3160071][-4.3359332 -4.3680358 -4.3780551 -4.3719964 -4.3549833 -4.3210473 -4.2765346 -4.221673 -4.1641588 -4.1398773 -4.1639247 -4.2190337 -4.270071 -4.293087 -4.2978158][-4.2698035 -4.2862277 -4.2842951 -4.2709141 -4.2440543 -4.199584 -4.14428 -4.079742 -4.032268 -4.0413661 -4.100687 -4.1831326 -4.2515388 -4.2834682 -4.2930727][-4.2444005 -4.2418537 -4.224988 -4.2020483 -4.1683884 -4.1226707 -4.0704107 -4.0115824 -3.9776306 -4.0109386 -4.0909705 -4.1829057 -4.2555709 -4.2903695 -4.3005161][-4.2455239 -4.234189 -4.2104526 -4.1818666 -4.14869 -4.1121993 -4.0728722 -4.0301561 -4.0043964 -4.0392518 -4.1156826 -4.1986628 -4.2645497 -4.2962894 -4.3041029][-4.2741356 -4.2663174 -4.2431722 -4.2068152 -4.171154 -4.1412792 -4.1128893 -4.0896215 -4.0709124 -4.0957451 -4.1571364 -4.2240129 -4.279284 -4.3055873 -4.3104277][-4.3323536 -4.3287854 -4.3048925 -4.2593336 -4.2186222 -4.1884251 -4.1633482 -4.1548333 -4.1433945 -4.159584 -4.2062526 -4.2577224 -4.3016305 -4.3226838 -4.3256159][-4.3796687 -4.3780212 -4.3604045 -4.3155637 -4.27526 -4.2425852 -4.2132235 -4.2080884 -4.2021852 -4.2173114 -4.2548833 -4.2934337 -4.3234363 -4.3382511 -4.3413386][-4.4014063 -4.4000831 -4.3915091 -4.3543997 -4.3193836 -4.2885637 -4.2571192 -4.250432 -4.2489676 -4.2687736 -4.303617 -4.3326516 -4.347868 -4.3554659 -4.3585515][-4.3891077 -4.3861551 -4.3823681 -4.3567543 -4.3313708 -4.3078008 -4.2805028 -4.2722974 -4.2745075 -4.2983241 -4.3315191 -4.3545122 -4.3591313 -4.3602505 -4.36281][-4.3578639 -4.3574209 -4.3584251 -4.3459091 -4.330595 -4.3135734 -4.29256 -4.2846293 -4.2885804 -4.3105588 -4.338202 -4.3552146 -4.3531742 -4.34813 -4.3486042]]...]
INFO - root - 2017-12-07 11:48:03.176915: step 13510, loss = 21.58, batch loss = 21.50 (8.3 examples/sec; 0.966 sec/batch; 85h:35m:42s remains)
INFO - root - 2017-12-07 11:48:12.548675: step 13520, loss = 21.47, batch loss = 21.39 (8.7 examples/sec; 0.916 sec/batch; 81h:07m:32s remains)
INFO - root - 2017-12-07 11:48:21.834369: step 13530, loss = 21.57, batch loss = 21.48 (8.9 examples/sec; 0.899 sec/batch; 79h:39m:13s remains)
INFO - root - 2017-12-07 11:48:31.184802: step 13540, loss = 21.58, batch loss = 21.50 (8.4 examples/sec; 0.951 sec/batch; 84h:16m:14s remains)
INFO - root - 2017-12-07 11:48:40.651824: step 13550, loss = 21.47, batch loss = 21.38 (8.5 examples/sec; 0.940 sec/batch; 83h:17m:25s remains)
INFO - root - 2017-12-07 11:48:50.070535: step 13560, loss = 21.29, batch loss = 21.21 (8.9 examples/sec; 0.903 sec/batch; 79h:58m:26s remains)
INFO - root - 2017-12-07 11:48:59.570137: step 13570, loss = 21.25, batch loss = 21.16 (8.6 examples/sec; 0.934 sec/batch; 82h:45m:40s remains)
INFO - root - 2017-12-07 11:49:08.889572: step 13580, loss = 21.34, batch loss = 21.26 (9.0 examples/sec; 0.891 sec/batch; 78h:54m:45s remains)
INFO - root - 2017-12-07 11:49:18.315216: step 13590, loss = 21.45, batch loss = 21.37 (8.2 examples/sec; 0.973 sec/batch; 86h:09m:34s remains)
INFO - root - 2017-12-07 11:49:27.691342: step 13600, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.952 sec/batch; 84h:19m:12s remains)
2017-12-07 11:49:28.570398: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5499625 -4.4771504 -4.3992662 -4.3860044 -4.4523783 -4.5759211 -4.6529655 -4.6070209 -4.4774146 -4.3149281 -4.1518073 -4.0474319 -4.0367332 -4.1315465 -4.2962866][-4.7178421 -4.6641021 -4.5864658 -4.548718 -4.5776982 -4.6724806 -4.7412877 -4.714994 -4.6216631 -4.4998755 -4.3559618 -4.24532 -4.2072234 -4.247292 -4.3428845][-4.8202972 -4.7945232 -4.7300777 -4.6699829 -4.6424646 -4.6745324 -4.71248 -4.7116675 -4.68571 -4.6400261 -4.5470929 -4.4507527 -4.3987589 -4.3902588 -4.412776][-4.8240867 -4.8139715 -4.7568135 -4.6727166 -4.5765343 -4.5252337 -4.5115223 -4.5282159 -4.587337 -4.6503172 -4.6397824 -4.5851822 -4.5370541 -4.4942055 -4.4662027][-4.7400045 -4.7251077 -4.6534853 -4.5347118 -4.3731256 -4.245945 -4.1782756 -4.2020378 -4.3450227 -4.5247169 -4.6107564 -4.6160049 -4.5873623 -4.5271339 -4.4803915][-4.6329927 -4.6006846 -4.50011 -4.3414807 -4.126843 -3.9446776 -3.8338287 -3.8538084 -4.0579762 -4.3292871 -4.500257 -4.5719709 -4.5786819 -4.5207944 -4.4814196][-4.5453048 -4.4961882 -4.3680429 -4.1719947 -3.9180684 -3.6963859 -3.5556316 -3.5694535 -3.8082342 -4.1324396 -4.3672285 -4.5028648 -4.5527487 -4.5094576 -4.4778066][-4.4990821 -4.440434 -4.301981 -4.094593 -3.8352137 -3.5994296 -3.4365394 -3.4275532 -3.6513996 -3.9752126 -4.2380357 -4.4181223 -4.5079012 -4.4900532 -4.4627504][-4.5124345 -4.4685111 -4.3562622 -4.1844292 -3.9731374 -3.7733166 -3.6207895 -3.5779653 -3.722892 -3.9663572 -4.1897645 -4.3663659 -4.4707704 -4.4792347 -4.456687][-4.567081 -4.5566988 -4.4933033 -4.3809476 -4.2485294 -4.1260371 -4.0241265 -3.9738796 -4.03361 -4.1573968 -4.2870388 -4.4061952 -4.4848442 -4.4977412 -4.4752145][-4.6385288 -4.6636648 -4.63871 -4.5695143 -4.4942651 -4.4397769 -4.3967953 -4.370575 -4.3827963 -4.4085574 -4.4391017 -4.4811068 -4.5152068 -4.5186491 -4.50315][-4.6766849 -4.7125406 -4.700983 -4.6507878 -4.6033778 -4.590395 -4.5962086 -4.6081934 -4.6165538 -4.5963144 -4.5615621 -4.5427456 -4.5391 -4.5366478 -4.5368667][-4.6401014 -4.6435404 -4.6130829 -4.566525 -4.535851 -4.5513163 -4.5982237 -4.6516423 -4.685688 -4.6742039 -4.6300821 -4.5940022 -4.5812616 -4.5826721 -4.5943756][-4.5850744 -4.5406618 -4.4713249 -4.4086242 -4.3764563 -4.3979597 -4.4619164 -4.5394583 -4.6081285 -4.64369 -4.6389384 -4.6284533 -4.6301665 -4.6393337 -4.6510224][-4.556963 -4.4968257 -4.4058032 -4.3236766 -4.2775559 -4.28784 -4.3470936 -4.4241223 -4.5099163 -4.5912027 -4.6384025 -4.66522 -4.6843042 -4.6934624 -4.6937294]]...]
INFO - root - 2017-12-07 11:49:37.856247: step 13610, loss = 21.54, batch loss = 21.46 (8.5 examples/sec; 0.937 sec/batch; 83h:02m:38s remains)
INFO - root - 2017-12-07 11:49:47.184312: step 13620, loss = 21.31, batch loss = 21.23 (8.0 examples/sec; 0.997 sec/batch; 88h:16m:24s remains)
INFO - root - 2017-12-07 11:49:56.441511: step 13630, loss = 21.20, batch loss = 21.12 (8.3 examples/sec; 0.961 sec/batch; 85h:06m:34s remains)
INFO - root - 2017-12-07 11:50:05.644979: step 13640, loss = 21.41, batch loss = 21.33 (8.1 examples/sec; 0.985 sec/batch; 87h:15m:39s remains)
INFO - root - 2017-12-07 11:50:15.048652: step 13650, loss = 21.60, batch loss = 21.52 (8.6 examples/sec; 0.926 sec/batch; 82h:03m:02s remains)
INFO - root - 2017-12-07 11:50:24.463085: step 13660, loss = 21.37, batch loss = 21.29 (8.3 examples/sec; 0.968 sec/batch; 85h:42m:44s remains)
INFO - root - 2017-12-07 11:50:33.894838: step 13670, loss = 21.24, batch loss = 21.15 (8.4 examples/sec; 0.956 sec/batch; 84h:40m:56s remains)
INFO - root - 2017-12-07 11:50:43.220482: step 13680, loss = 21.78, batch loss = 21.70 (8.9 examples/sec; 0.899 sec/batch; 79h:38m:17s remains)
INFO - root - 2017-12-07 11:50:52.603884: step 13690, loss = 21.85, batch loss = 21.77 (9.0 examples/sec; 0.884 sec/batch; 78h:17m:12s remains)
INFO - root - 2017-12-07 11:51:02.022039: step 13700, loss = 21.77, batch loss = 21.69 (8.5 examples/sec; 0.937 sec/batch; 82h:56m:55s remains)
2017-12-07 11:51:03.044146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1566896 -4.1420164 -4.1331754 -4.1308532 -4.1577806 -4.2054963 -4.2358637 -4.2027726 -4.0998435 -3.9965005 -3.9769447 -4.0229149 -4.058609 -4.0863142 -4.1282926][-4.1217027 -4.1192245 -4.13611 -4.1552806 -4.1796026 -4.2031288 -4.2017946 -4.1507506 -4.0578656 -3.9628012 -3.9110222 -3.8971536 -3.8964081 -3.9437933 -4.0310855][-4.1954465 -4.2233205 -4.2685089 -4.3040051 -4.3183546 -4.319066 -4.2938442 -4.2290111 -4.138411 -4.0383806 -3.96055 -3.9156823 -3.9156847 -4.0015578 -4.1283355][-4.3355675 -4.3900886 -4.4503236 -4.4893832 -4.4915771 -4.4708333 -4.4217863 -4.3421946 -4.249814 -4.158515 -4.0955625 -4.0694942 -4.0930595 -4.1914845 -4.3126674][-4.41447 -4.46263 -4.5060239 -4.5252552 -4.5054021 -4.450017 -4.3682423 -4.2858534 -4.2309928 -4.2060742 -4.2102957 -4.2357411 -4.2865825 -4.3726854 -4.454567][-4.411602 -4.413166 -4.4077058 -4.37824 -4.3108821 -4.1991673 -4.0810294 -4.0226502 -4.0548911 -4.1423573 -4.2368956 -4.3195958 -4.3959761 -4.4574251 -4.4771395][-4.3636031 -4.3296237 -4.2838321 -4.2051873 -4.0797462 -3.9021964 -3.7411983 -3.7010257 -3.8144627 -4.0107684 -4.1959715 -4.33827 -4.4413795 -4.4682188 -4.3990469][-4.3110723 -4.2785845 -4.221663 -4.1150107 -3.9463689 -3.7167802 -3.5154018 -3.4727435 -3.6325803 -3.9144468 -4.1807928 -4.3657088 -4.4631538 -4.4227085 -4.2529011][-4.3114624 -4.3061724 -4.2666869 -4.1643267 -3.9862795 -3.7319219 -3.5057037 -3.4568844 -3.640975 -3.9717648 -4.2697639 -4.4382973 -4.4811039 -4.3717337 -4.1486087][-4.379703 -4.4190788 -4.415658 -4.3436246 -4.1829357 -3.9263682 -3.6875551 -3.628834 -3.8030276 -4.1119246 -4.36836 -4.4824595 -4.4867463 -4.3824372 -4.1995049][-4.4465585 -4.5243826 -4.5572748 -4.5215321 -4.3869328 -4.148973 -3.9214005 -3.8606594 -4.0032091 -4.2402425 -4.4166517 -4.4750586 -4.4697905 -4.4113913 -4.3052673][-4.4947844 -4.5841365 -4.6390743 -4.6290612 -4.52117 -4.3210416 -4.1354775 -4.0920768 -4.2029991 -4.3612666 -4.4565268 -4.4663177 -4.4537191 -4.436316 -4.4008756][-4.5091362 -4.5978374 -4.6648221 -4.6718 -4.5940123 -4.4478297 -4.3185344 -4.2934613 -4.3672404 -4.4553227 -4.4927449 -4.4797635 -4.4692597 -4.4794726 -4.4841595][-4.4831719 -4.5672688 -4.6311622 -4.6420889 -4.589385 -4.4934349 -4.4075603 -4.385282 -4.4171414 -4.4497018 -4.4565811 -4.4460282 -4.45206 -4.488049 -4.5202279][-4.4887962 -4.5638843 -4.608356 -4.5973716 -4.5383291 -4.4572382 -4.3840017 -4.3492365 -4.343205 -4.3346539 -4.321764 -4.3237152 -4.3630476 -4.4429293 -4.5148306]]...]
INFO - root - 2017-12-07 11:51:12.384581: step 13710, loss = 21.23, batch loss = 21.14 (8.6 examples/sec; 0.930 sec/batch; 82h:20m:13s remains)
INFO - root - 2017-12-07 11:51:21.664596: step 13720, loss = 21.05, batch loss = 20.97 (8.7 examples/sec; 0.920 sec/batch; 81h:27m:15s remains)
INFO - root - 2017-12-07 11:51:30.993232: step 13730, loss = 21.58, batch loss = 21.49 (8.4 examples/sec; 0.949 sec/batch; 84h:01m:19s remains)
INFO - root - 2017-12-07 11:51:40.292399: step 13740, loss = 21.79, batch loss = 21.70 (8.9 examples/sec; 0.899 sec/batch; 79h:33m:39s remains)
INFO - root - 2017-12-07 11:51:49.710750: step 13750, loss = 21.20, batch loss = 21.12 (8.6 examples/sec; 0.927 sec/batch; 82h:06m:13s remains)
INFO - root - 2017-12-07 11:51:59.070766: step 13760, loss = 21.73, batch loss = 21.64 (8.6 examples/sec; 0.927 sec/batch; 82h:03m:54s remains)
INFO - root - 2017-12-07 11:52:08.515678: step 13770, loss = 21.35, batch loss = 21.26 (8.3 examples/sec; 0.970 sec/batch; 85h:51m:10s remains)
INFO - root - 2017-12-07 11:52:17.938922: step 13780, loss = 21.73, batch loss = 21.65 (8.1 examples/sec; 0.991 sec/batch; 87h:43m:13s remains)
INFO - root - 2017-12-07 11:52:27.170172: step 13790, loss = 21.47, batch loss = 21.39 (8.3 examples/sec; 0.966 sec/batch; 85h:30m:14s remains)
INFO - root - 2017-12-07 11:52:36.474336: step 13800, loss = 21.61, batch loss = 21.53 (8.5 examples/sec; 0.945 sec/batch; 83h:38m:58s remains)
2017-12-07 11:52:37.363443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5972 -4.5846252 -4.5596313 -4.5408473 -4.537034 -4.5306807 -4.5262818 -4.5329947 -4.5469956 -4.560648 -4.567471 -4.5526752 -4.5088167 -4.4522634 -4.4066133][-4.6450524 -4.6357222 -4.6044297 -4.5756826 -4.56416 -4.5409255 -4.5213771 -4.5376153 -4.58457 -4.6315007 -4.6640263 -4.6560292 -4.5924 -4.5006409 -4.4241209][-4.6049824 -4.59583 -4.5631032 -4.52768 -4.5064878 -4.4603515 -4.4141364 -4.433939 -4.5146966 -4.5984316 -4.6687489 -4.6964922 -4.6491022 -4.5469503 -4.44486][-4.5052752 -4.4880757 -4.457377 -4.4282017 -4.41095 -4.3521509 -4.2815561 -4.3014536 -4.411623 -4.5261025 -4.6245871 -4.6859403 -4.6717596 -4.5840635 -4.4703579][-4.3694978 -4.3337188 -4.2935376 -4.2746987 -4.2726364 -4.2065268 -4.1112633 -4.1278844 -4.266468 -4.4121628 -4.5333657 -4.6178904 -4.6382656 -4.5844846 -4.48399][-4.2247872 -4.1626964 -4.0968604 -4.077106 -4.0904236 -4.0174475 -3.8979311 -3.9066305 -4.06337 -4.2328224 -4.3752961 -4.484509 -4.5454593 -4.5415077 -4.4759207][-4.1417289 -4.05833 -3.9673238 -3.9361413 -3.9531617 -3.8743229 -3.7350056 -3.7259409 -3.8786793 -4.0476632 -4.1999669 -4.3393226 -4.4443007 -4.4864969 -4.4564538][-4.1577077 -4.0718112 -3.9721336 -3.9247651 -3.9267015 -3.8436041 -3.7014635 -3.6807222 -3.8138216 -3.9555621 -4.093317 -4.2536469 -4.3928185 -4.4608569 -4.4445958][-4.2169905 -4.1538181 -4.0794821 -4.0334969 -4.0249453 -3.9458067 -3.807384 -3.7720649 -3.8756661 -3.9828544 -4.0999146 -4.2657714 -4.4175014 -4.4818311 -4.4525738][-4.2757392 -4.24157 -4.2076039 -4.1823506 -4.1761775 -4.1066141 -3.9678879 -3.918597 -4.0078745 -4.105638 -4.21594 -4.3708014 -4.4996386 -4.5297661 -4.470418][-4.3397255 -4.3325071 -4.3301249 -4.3176484 -4.3079181 -4.2468171 -4.1209755 -4.0748324 -4.1634293 -4.2683043 -4.3739715 -4.4991012 -4.5796332 -4.5652947 -4.4796262][-4.4174547 -4.4362745 -4.4530568 -4.4387822 -4.4121 -4.3506169 -4.2447515 -4.2143126 -4.30525 -4.4126196 -4.5038295 -4.588398 -4.620018 -4.5714779 -4.474246][-4.47682 -4.5072207 -4.5279655 -4.5146623 -4.4799757 -4.4233794 -4.3434448 -4.3286476 -4.4137678 -4.5122242 -4.5820513 -4.6261454 -4.62049 -4.5532026 -4.457181][-4.4945946 -4.5192413 -4.5298624 -4.5177212 -4.4876685 -4.444315 -4.3973465 -4.4004631 -4.4753094 -4.5574369 -4.6063824 -4.6187825 -4.5889974 -4.51555 -4.4295979][-4.4751391 -4.4879494 -4.4872284 -4.47326 -4.4477015 -4.4171638 -4.3946376 -4.4088607 -4.470355 -4.5353651 -4.5698009 -4.5678959 -4.5305581 -4.464118 -4.3960676]]...]
INFO - root - 2017-12-07 11:52:46.838259: step 13810, loss = 21.64, batch loss = 21.55 (8.7 examples/sec; 0.922 sec/batch; 81h:38m:37s remains)
INFO - root - 2017-12-07 11:52:56.278652: step 13820, loss = 21.24, batch loss = 21.15 (8.3 examples/sec; 0.965 sec/batch; 85h:24m:41s remains)
INFO - root - 2017-12-07 11:53:05.637861: step 13830, loss = 21.26, batch loss = 21.18 (8.2 examples/sec; 0.972 sec/batch; 86h:02m:30s remains)
INFO - root - 2017-12-07 11:53:14.942077: step 13840, loss = 21.08, batch loss = 21.00 (8.8 examples/sec; 0.914 sec/batch; 80h:51m:41s remains)
INFO - root - 2017-12-07 11:53:24.258824: step 13850, loss = 21.00, batch loss = 20.91 (8.4 examples/sec; 0.954 sec/batch; 84h:26m:33s remains)
INFO - root - 2017-12-07 11:53:33.639387: step 13860, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.968 sec/batch; 85h:39m:56s remains)
INFO - root - 2017-12-07 11:53:43.083481: step 13870, loss = 21.25, batch loss = 21.16 (8.8 examples/sec; 0.911 sec/batch; 80h:38m:38s remains)
INFO - root - 2017-12-07 11:53:52.442589: step 13880, loss = 21.39, batch loss = 21.30 (8.7 examples/sec; 0.921 sec/batch; 81h:29m:46s remains)
INFO - root - 2017-12-07 11:54:01.914849: step 13890, loss = 21.26, batch loss = 21.17 (8.2 examples/sec; 0.979 sec/batch; 86h:38m:17s remains)
INFO - root - 2017-12-07 11:54:11.386981: step 13900, loss = 21.02, batch loss = 20.94 (8.2 examples/sec; 0.976 sec/batch; 86h:21m:31s remains)
2017-12-07 11:54:12.301191: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5418696 -4.5810552 -4.6157422 -4.6309981 -4.6273155 -4.6143451 -4.603898 -4.6078725 -4.6280107 -4.6500459 -4.6558261 -4.6393728 -4.6127181 -4.5894728 -4.5764656][-4.5916567 -4.6563277 -4.7087836 -4.728178 -4.7114692 -4.6802454 -4.6612916 -4.677659 -4.7255549 -4.7689915 -4.775898 -4.739224 -4.6841226 -4.6394606 -4.6180959][-4.6701632 -4.7610669 -4.8263617 -4.8412051 -4.7966781 -4.7305236 -4.6960645 -4.7326436 -4.8228488 -4.8987236 -4.9101977 -4.8522725 -4.7686667 -4.7004819 -4.6633859][-4.7568226 -4.8592334 -4.9178562 -4.9061465 -4.809772 -4.6858683 -4.62183 -4.6805682 -4.8302507 -4.965436 -5.010314 -4.9552846 -4.857584 -4.767879 -4.7055826][-4.8145308 -4.9107094 -4.9385223 -4.8740439 -4.7026939 -4.5016627 -4.3940582 -4.4668856 -4.683723 -4.9033747 -5.0176487 -5.0018339 -4.9122882 -4.8077149 -4.7155385][-4.8433914 -4.9169116 -4.8915863 -4.7452431 -4.4778347 -4.1871443 -4.0280013 -4.10659 -4.3907251 -4.7138004 -4.9297643 -4.9857912 -4.9238172 -4.8100495 -4.6900434][-4.8538408 -4.8918595 -4.7987843 -4.556673 -4.1909714 -3.8183076 -3.6134057 -3.6936164 -4.0397048 -4.4697447 -4.7969251 -4.9335446 -4.9026709 -4.7800283 -4.6355038][-4.8345642 -4.8390193 -4.6902251 -4.3838415 -3.9667585 -3.5543456 -3.3210254 -3.3867006 -3.756135 -4.2491317 -4.6500268 -4.8403149 -4.824687 -4.6928744 -4.5355778][-4.8001866 -4.7985406 -4.6399541 -4.3459315 -3.9643693 -3.5898252 -3.3644929 -3.3920052 -3.7106056 -4.1738424 -4.5652075 -4.7509146 -4.7198224 -4.5722852 -4.4105449][-4.754755 -4.7778764 -4.6642976 -4.4526005 -4.1737967 -3.8937294 -3.7069283 -3.6842687 -3.898808 -4.2636809 -4.5846539 -4.7211123 -4.6485844 -4.4729433 -4.2986231][-4.6506023 -4.7282491 -4.6987257 -4.5999336 -4.4400229 -4.2617331 -4.1206818 -4.0603819 -4.17308 -4.432682 -4.6733685 -4.7508411 -4.6321383 -4.4237695 -4.2239556][-4.5229893 -4.6639447 -4.7173433 -4.7129292 -4.6454239 -4.5428467 -4.442112 -4.372849 -4.4223361 -4.6003823 -4.77216 -4.7978349 -4.6438942 -4.4092965 -4.1735592][-4.477047 -4.6440153 -4.727324 -4.7566161 -4.7300429 -4.6743665 -4.6152372 -4.5736589 -4.6080766 -4.7329473 -4.8442936 -4.8244448 -4.6469345 -4.3973584 -4.1310153][-4.5456495 -4.6684022 -4.7109084 -4.7098451 -4.6814289 -4.6534419 -4.6443496 -4.6594167 -4.7120142 -4.8063006 -4.874259 -4.8309751 -4.6569338 -4.4135084 -4.1381536][-4.634994 -4.664927 -4.6333342 -4.5755568 -4.5230517 -4.5062346 -4.5411 -4.6158032 -4.7006035 -4.786653 -4.8420544 -4.81241 -4.6825285 -4.4835129 -4.2372065]]...]
INFO - root - 2017-12-07 11:54:21.759399: step 13910, loss = 21.53, batch loss = 21.44 (8.7 examples/sec; 0.922 sec/batch; 81h:33m:21s remains)
INFO - root - 2017-12-07 11:54:31.202911: step 13920, loss = 21.66, batch loss = 21.58 (8.3 examples/sec; 0.961 sec/batch; 85h:04m:45s remains)
INFO - root - 2017-12-07 11:54:40.581792: step 13930, loss = 21.37, batch loss = 21.29 (9.8 examples/sec; 0.813 sec/batch; 71h:57m:39s remains)
INFO - root - 2017-12-07 11:54:49.708277: step 13940, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.914 sec/batch; 80h:53m:43s remains)
INFO - root - 2017-12-07 11:54:59.044015: step 13950, loss = 21.46, batch loss = 21.38 (8.1 examples/sec; 0.986 sec/batch; 87h:16m:54s remains)
INFO - root - 2017-12-07 11:55:08.339517: step 13960, loss = 21.47, batch loss = 21.39 (8.1 examples/sec; 0.986 sec/batch; 87h:12m:15s remains)
INFO - root - 2017-12-07 11:55:17.581295: step 13970, loss = 21.67, batch loss = 21.59 (8.3 examples/sec; 0.962 sec/batch; 85h:06m:29s remains)
INFO - root - 2017-12-07 11:55:26.886046: step 13980, loss = 21.20, batch loss = 21.12 (8.3 examples/sec; 0.963 sec/batch; 85h:09m:59s remains)
INFO - root - 2017-12-07 11:55:36.183296: step 13990, loss = 21.60, batch loss = 21.52 (8.3 examples/sec; 0.963 sec/batch; 85h:13m:41s remains)
INFO - root - 2017-12-07 11:55:45.469001: step 14000, loss = 21.29, batch loss = 21.20 (8.9 examples/sec; 0.899 sec/batch; 79h:32m:40s remains)
2017-12-07 11:55:46.461451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5336075 -4.5635228 -4.5901842 -4.6099463 -4.6235709 -4.6583824 -4.7170553 -4.7702594 -4.7745428 -4.6877847 -4.4990926 -4.3186049 -4.2663612 -4.3432446 -4.4721818][-4.4837217 -4.4821343 -4.4953766 -4.52885 -4.570601 -4.6243076 -4.678268 -4.7013993 -4.6466584 -4.4754114 -4.2125688 -4.0025673 -3.9797342 -4.1308956 -4.3543253][-4.4279585 -4.4015508 -4.3996406 -4.436069 -4.4901161 -4.5512567 -4.5974164 -4.5908618 -4.4777131 -4.2401385 -3.9431808 -3.741148 -3.7522421 -3.9534192 -4.2374635][-4.4335632 -4.4213042 -4.4252453 -4.448307 -4.4736156 -4.5007997 -4.5183964 -4.4844246 -4.3406138 -4.0954847 -3.8387797 -3.698081 -3.7415409 -3.9400454 -4.2122216][-4.4880486 -4.5077953 -4.5164447 -4.4970345 -4.4485455 -4.403 -4.3731456 -4.3112011 -4.1765609 -4.0082955 -3.8871465 -3.8686614 -3.9485617 -4.1027369 -4.2978907][-4.5235863 -4.557704 -4.5514321 -4.4779096 -4.3520603 -4.228478 -4.1362486 -4.042841 -3.9470682 -3.9020371 -3.9441447 -4.0510893 -4.1626754 -4.2712007 -4.3834605][-4.4640474 -4.4862828 -4.4589615 -4.3518367 -4.1811905 -4.0031166 -3.8595622 -3.760781 -3.735621 -3.8071046 -3.9567258 -4.1247683 -4.2467756 -4.3290052 -4.3881965][-4.3992119 -4.4303112 -4.4093728 -4.3004088 -4.1169057 -3.9103029 -3.7392747 -3.6659808 -3.7181242 -3.8583012 -4.0284328 -4.1758976 -4.2694426 -4.3258615 -4.3590293][-4.4154077 -4.4830046 -4.4866281 -4.3896894 -4.2027736 -3.9819014 -3.8058624 -3.7657475 -3.8667138 -4.0228457 -4.1621642 -4.2508936 -4.2932348 -4.3195009 -4.3460703][-4.4688683 -4.5574689 -4.569447 -4.4771523 -4.2954907 -4.0904908 -3.94175 -3.9348862 -4.0557389 -4.2027988 -4.3041677 -4.3360233 -4.3295951 -4.3285174 -4.35229][-4.518568 -4.6068492 -4.6193933 -4.5401044 -4.3886614 -4.2291536 -4.1257229 -4.1417055 -4.2504854 -4.36158 -4.4159179 -4.4056182 -4.3749318 -4.3693328 -4.4017863][-4.5096869 -4.5829763 -4.5972576 -4.542099 -4.4356117 -4.3310976 -4.2733641 -4.2998934 -4.383081 -4.4573441 -4.4826407 -4.4630318 -4.4394307 -4.4422083 -4.4773965][-4.4585414 -4.5145478 -4.5321169 -4.504261 -4.4493093 -4.4026628 -4.3873553 -4.4176583 -4.4690452 -4.5054994 -4.5137343 -4.5031023 -4.4956255 -4.5012031 -4.5216446][-4.4012389 -4.447463 -4.4728088 -4.4690633 -4.4525628 -4.4438057 -4.4523516 -4.4734116 -4.4895043 -4.4928637 -4.490828 -4.4889417 -4.489397 -4.4917507 -4.4967551][-4.3341179 -4.367105 -4.3928704 -4.4008627 -4.4008346 -4.4044042 -4.4152265 -4.4231839 -4.4184175 -4.4072976 -4.4009237 -4.3992233 -4.3980317 -4.3953218 -4.3920236]]...]
INFO - root - 2017-12-07 11:55:55.879406: step 14010, loss = 21.48, batch loss = 21.40 (8.0 examples/sec; 1.005 sec/batch; 88h:52m:51s remains)
INFO - root - 2017-12-07 11:56:05.397890: step 14020, loss = 21.33, batch loss = 21.25 (8.1 examples/sec; 0.985 sec/batch; 87h:09m:07s remains)
INFO - root - 2017-12-07 11:56:14.729719: step 14030, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.928 sec/batch; 82h:07m:50s remains)
INFO - root - 2017-12-07 11:56:23.991140: step 14040, loss = 21.30, batch loss = 21.21 (8.6 examples/sec; 0.928 sec/batch; 82h:04m:50s remains)
INFO - root - 2017-12-07 11:56:33.157222: step 14050, loss = 21.97, batch loss = 21.88 (8.1 examples/sec; 0.983 sec/batch; 86h:59m:45s remains)
INFO - root - 2017-12-07 11:56:42.411468: step 14060, loss = 21.40, batch loss = 21.32 (8.6 examples/sec; 0.932 sec/batch; 82h:26m:20s remains)
INFO - root - 2017-12-07 11:56:51.749854: step 14070, loss = 21.63, batch loss = 21.54 (8.7 examples/sec; 0.917 sec/batch; 81h:07m:35s remains)
INFO - root - 2017-12-07 11:57:01.081746: step 14080, loss = 21.78, batch loss = 21.69 (8.9 examples/sec; 0.896 sec/batch; 79h:12m:37s remains)
INFO - root - 2017-12-07 11:57:10.507945: step 14090, loss = 21.38, batch loss = 21.30 (8.6 examples/sec; 0.932 sec/batch; 82h:24m:39s remains)
INFO - root - 2017-12-07 11:57:20.074694: step 14100, loss = 21.23, batch loss = 21.15 (8.2 examples/sec; 0.977 sec/batch; 86h:25m:48s remains)
2017-12-07 11:57:21.123358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5239215 -4.5655222 -4.6025047 -4.6227784 -4.6224446 -4.6083465 -4.5742807 -4.5154958 -4.4636974 -4.4749541 -4.5597277 -4.6607862 -4.7125282 -4.7017245 -4.6451836][-4.51163 -4.5666785 -4.6185827 -4.6514621 -4.6659217 -4.6641068 -4.621244 -4.5309205 -4.4486871 -4.456954 -4.573307 -4.7085824 -4.7681289 -4.7413483 -4.6664853][-4.4915147 -4.5614872 -4.6169415 -4.6423264 -4.6559348 -4.6586895 -4.6132503 -4.5136337 -4.4266105 -4.4431171 -4.5747542 -4.7159262 -4.7692623 -4.7300863 -4.6496749][-4.4796343 -4.5570269 -4.5973587 -4.5894914 -4.5772071 -4.5680375 -4.5231647 -4.4401608 -4.3873153 -4.4362164 -4.5730157 -4.6927614 -4.7241282 -4.6781549 -4.605607][-4.4699097 -4.5450721 -4.555037 -4.5030985 -4.4527316 -4.4162483 -4.3611197 -4.2969265 -4.2898693 -4.385334 -4.5325155 -4.633543 -4.6504774 -4.6139674 -4.5623884][-4.4334583 -4.4836025 -4.4589453 -4.3705797 -4.2855196 -4.2070761 -4.1129627 -4.0429931 -4.0705371 -4.2167912 -4.3951683 -4.5085311 -4.5460291 -4.5444832 -4.5260348][-4.3491755 -4.3699064 -4.3184834 -4.2122765 -4.1079588 -3.9903793 -3.8456898 -3.7529693 -3.80379 -3.9991856 -4.2218485 -4.3778057 -4.4646158 -4.5055184 -4.5098214][-4.282711 -4.2833786 -4.2224045 -4.1160398 -4.0077229 -3.8708282 -3.6999514 -3.6025555 -3.6729171 -3.8932962 -4.1350451 -4.3180537 -4.4411139 -4.505373 -4.5092268][-4.3058691 -4.3001227 -4.2483759 -4.1588774 -4.0620637 -3.9359372 -3.7880213 -3.7252293 -3.8152466 -4.0200915 -4.2216692 -4.3706341 -4.4750495 -4.5242381 -4.5087781][-4.3945622 -4.3877449 -4.3476095 -4.2839952 -4.2144337 -4.1245031 -4.0292807 -4.0117626 -4.1075258 -4.2698131 -4.4041047 -4.4877329 -4.5369258 -4.5491619 -4.5102296][-4.4927454 -4.4803872 -4.4448576 -4.4044418 -4.3685589 -4.3193588 -4.2704234 -4.2768459 -4.3563457 -4.4674606 -4.5425124 -4.574204 -4.5806184 -4.5661011 -4.5140753][-4.5471892 -4.5345426 -4.5032964 -4.4839749 -4.4796767 -4.463347 -4.4427133 -4.4525161 -4.5043316 -4.5678658 -4.5977707 -4.6007543 -4.5896525 -4.5662775 -4.5108356][-4.5266356 -4.5264735 -4.5090852 -4.5064793 -4.519877 -4.5229316 -4.5165486 -4.5196614 -4.5441852 -4.5746913 -4.583168 -4.5800285 -4.5704074 -4.5462966 -4.4909687][-4.4255676 -4.4412146 -4.4420643 -4.4529619 -4.4730048 -4.4828558 -4.4819484 -4.4779525 -4.4836774 -4.4978571 -4.5045261 -4.5091386 -4.5100765 -4.4916267 -4.4432073][-4.3064427 -4.3279467 -4.3387437 -4.3541231 -4.3710876 -4.3809042 -4.3833361 -4.3800087 -4.3794785 -4.3880434 -4.3998389 -4.413909 -4.4225178 -4.4121933 -4.3779469]]...]
INFO - root - 2017-12-07 11:57:30.437214: step 14110, loss = 21.54, batch loss = 21.45 (8.8 examples/sec; 0.912 sec/batch; 80h:37m:00s remains)
INFO - root - 2017-12-07 11:57:39.859496: step 14120, loss = 21.76, batch loss = 21.67 (8.7 examples/sec; 0.916 sec/batch; 81h:00m:11s remains)
INFO - root - 2017-12-07 11:57:49.248719: step 14130, loss = 21.41, batch loss = 21.33 (9.0 examples/sec; 0.893 sec/batch; 78h:56m:04s remains)
INFO - root - 2017-12-07 11:57:58.450432: step 14140, loss = 21.42, batch loss = 21.33 (8.6 examples/sec; 0.932 sec/batch; 82h:26m:22s remains)
INFO - root - 2017-12-07 11:58:07.776313: step 14150, loss = 21.13, batch loss = 21.05 (9.4 examples/sec; 0.849 sec/batch; 75h:02m:47s remains)
INFO - root - 2017-12-07 11:58:17.141385: step 14160, loss = 21.43, batch loss = 21.35 (8.8 examples/sec; 0.910 sec/batch; 80h:30m:05s remains)
INFO - root - 2017-12-07 11:58:26.524950: step 14170, loss = 21.61, batch loss = 21.53 (8.4 examples/sec; 0.947 sec/batch; 83h:43m:08s remains)
INFO - root - 2017-12-07 11:58:36.058734: step 14180, loss = 21.38, batch loss = 21.30 (7.9 examples/sec; 1.009 sec/batch; 89h:12m:40s remains)
INFO - root - 2017-12-07 11:58:45.530631: step 14190, loss = 21.33, batch loss = 21.24 (8.3 examples/sec; 0.961 sec/batch; 84h:56m:06s remains)
INFO - root - 2017-12-07 11:58:54.742968: step 14200, loss = 21.72, batch loss = 21.64 (8.7 examples/sec; 0.922 sec/batch; 81h:31m:46s remains)
2017-12-07 11:58:55.694243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3638945 -4.375309 -4.3935575 -4.4052119 -4.4105339 -4.4178948 -4.4267044 -4.441431 -4.4640517 -4.494523 -4.5174537 -4.5150185 -4.4948006 -4.4743891 -4.4606471][-4.4544582 -4.4748983 -4.4912758 -4.4988542 -4.5102167 -4.5402722 -4.57015 -4.59413 -4.6205215 -4.6599946 -4.6956329 -4.6950717 -4.66377 -4.6265645 -4.5972438][-4.5417609 -4.5682764 -4.5750604 -4.570538 -4.5886974 -4.6497288 -4.7012172 -4.7193527 -4.7269859 -4.7574978 -4.7971096 -4.7980161 -4.7634969 -4.719645 -4.6833925][-4.5844846 -4.6121197 -4.606504 -4.5854192 -4.6066914 -4.6891809 -4.7439761 -4.7304316 -4.6962605 -4.702528 -4.7394323 -4.7424612 -4.7141647 -4.6791744 -4.6513233][-4.5662026 -4.5916033 -4.57887 -4.5448031 -4.5587978 -4.6347256 -4.6590309 -4.589077 -4.5008469 -4.4811549 -4.5221391 -4.5412407 -4.5356259 -4.5306163 -4.5297446][-4.5323696 -4.5516348 -4.5340633 -4.4890704 -4.4773211 -4.5110569 -4.4774356 -4.3413215 -4.2033024 -4.1664886 -4.2229447 -4.2758651 -4.3082585 -4.349865 -4.3881469][-4.5282445 -4.5430202 -4.5269408 -4.4814959 -4.4385724 -4.4101214 -4.3088903 -4.1131229 -3.9395261 -3.8945129 -3.9704647 -4.0636616 -4.1348786 -4.2156563 -4.2820711][-4.5428114 -4.5587397 -4.5540285 -4.5247903 -4.4689574 -4.388597 -4.2331491 -4.0017695 -3.8180108 -3.7744489 -3.864218 -3.9864066 -4.0739212 -4.160316 -4.2288365][-4.537075 -4.557961 -4.5695281 -4.5669775 -4.5243607 -4.4273176 -4.2539964 -4.0281706 -3.8677673 -3.8404088 -3.9325614 -4.058156 -4.1382117 -4.2052536 -4.2554407][-4.5060596 -4.5314927 -4.5594964 -4.587913 -4.5833154 -4.5167508 -4.36983 -4.1834903 -4.0662508 -4.0563879 -4.1333985 -4.2379613 -4.3006186 -4.3383646 -4.3622222][-4.469244 -4.4964395 -4.5359216 -4.5882449 -4.6272316 -4.61488 -4.5153265 -4.3799405 -4.3060718 -4.3053865 -4.3547435 -4.4286661 -4.477984 -4.4932961 -4.4919357][-4.4513311 -4.4748378 -4.5125928 -4.5702209 -4.6315756 -4.6634765 -4.6129513 -4.530129 -4.4934525 -4.4920993 -4.5115089 -4.5572386 -4.597333 -4.597877 -4.5713267][-4.455369 -4.4636426 -4.4839072 -4.5254569 -4.5838933 -4.6375537 -4.6287079 -4.5925093 -4.582417 -4.5752416 -4.5733624 -4.6013308 -4.6385307 -4.6349649 -4.5882726][-4.4770093 -4.4693646 -4.4665346 -4.4833922 -4.5225539 -4.5732665 -4.5803919 -4.5655026 -4.5664229 -4.5579128 -4.5533538 -4.5817685 -4.6280446 -4.6333041 -4.5711536][-4.499414 -4.479074 -4.4621215 -4.4643049 -4.485991 -4.5198565 -4.5166 -4.4938645 -4.4887438 -4.4784927 -4.4801106 -4.5197225 -4.5823569 -4.6015353 -4.5332232]]...]
INFO - root - 2017-12-07 11:59:04.972187: step 14210, loss = 21.80, batch loss = 21.71 (7.9 examples/sec; 1.009 sec/batch; 89h:11m:07s remains)
INFO - root - 2017-12-07 11:59:14.268504: step 14220, loss = 21.16, batch loss = 21.07 (8.1 examples/sec; 0.984 sec/batch; 86h:59m:31s remains)
INFO - root - 2017-12-07 11:59:23.512114: step 14230, loss = 21.93, batch loss = 21.84 (9.0 examples/sec; 0.887 sec/batch; 78h:23m:16s remains)
INFO - root - 2017-12-07 11:59:32.838424: step 14240, loss = 21.09, batch loss = 21.00 (8.1 examples/sec; 0.987 sec/batch; 87h:17m:42s remains)
INFO - root - 2017-12-07 11:59:42.220048: step 14250, loss = 21.53, batch loss = 21.45 (8.1 examples/sec; 0.990 sec/batch; 87h:33m:20s remains)
INFO - root - 2017-12-07 11:59:51.489830: step 14260, loss = 21.30, batch loss = 21.22 (8.0 examples/sec; 1.001 sec/batch; 88h:28m:24s remains)
INFO - root - 2017-12-07 12:00:00.907684: step 14270, loss = 21.18, batch loss = 21.10 (8.5 examples/sec; 0.942 sec/batch; 83h:17m:22s remains)
INFO - root - 2017-12-07 12:00:10.356736: step 14280, loss = 21.37, batch loss = 21.29 (8.8 examples/sec; 0.905 sec/batch; 80h:00m:32s remains)
INFO - root - 2017-12-07 12:00:19.729497: step 14290, loss = 21.28, batch loss = 21.19 (8.8 examples/sec; 0.911 sec/batch; 80h:31m:11s remains)
INFO - root - 2017-12-07 12:00:29.005393: step 14300, loss = 21.45, batch loss = 21.37 (9.0 examples/sec; 0.891 sec/batch; 78h:43m:11s remains)
2017-12-07 12:00:30.023484: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6564927 -4.6821551 -4.7074256 -4.7230849 -4.7000256 -4.665834 -4.6338491 -4.6389861 -4.6991248 -4.7654819 -4.8052506 -4.8166838 -4.7950611 -4.7105246 -4.57914][-4.6240439 -4.6643133 -4.708807 -4.7257204 -4.6801457 -4.6057029 -4.53916 -4.5479159 -4.6499915 -4.7615304 -4.8399687 -4.8826714 -4.8804417 -4.79385 -4.6419916][-4.5086484 -4.5491915 -4.6041622 -4.6212096 -4.5630908 -4.4641337 -4.3708119 -4.3760285 -4.5040154 -4.6550951 -4.7825603 -4.8754468 -4.9056582 -4.8263617 -4.6666274][-4.4509311 -4.4698682 -4.5147486 -4.5249619 -4.4640231 -4.3581705 -4.2521362 -4.2484159 -4.3744955 -4.536694 -4.6887183 -4.81858 -4.8725529 -4.8050752 -4.649766][-4.4667897 -4.4469028 -4.4613118 -4.4524484 -4.3824892 -4.2652278 -4.1477771 -4.1353636 -4.2512317 -4.4065266 -4.5618048 -4.7086234 -4.7814574 -4.7387075 -4.6054616][-4.4192824 -4.3717551 -4.3590779 -4.334404 -4.2446852 -4.0933571 -3.9433098 -3.9224176 -4.0428381 -4.21323 -4.3916345 -4.5666194 -4.6773539 -4.6766043 -4.5714984][-4.2615728 -4.2187572 -4.2173467 -4.2080913 -4.1105213 -3.9149494 -3.7178948 -3.6845641 -3.8221071 -4.0327344 -4.2627153 -4.4769888 -4.6270671 -4.6611381 -4.5690389][-4.1040034 -4.0881085 -4.1326675 -4.1702571 -4.0891042 -3.8727238 -3.6495316 -3.6190152 -3.7787454 -4.0276651 -4.2947545 -4.5188627 -4.6692009 -4.6967316 -4.5880818][-4.058928 -4.0656285 -4.154501 -4.2458348 -4.1969357 -3.9935243 -3.7871284 -3.7841311 -3.9646182 -4.2231092 -4.4804606 -4.664083 -4.7665277 -4.7512088 -4.6048708][-4.1270366 -4.1244578 -4.225255 -4.3485913 -4.3235126 -4.144949 -3.9722469 -3.9961858 -4.1957603 -4.4604716 -4.6963935 -4.8300633 -4.8785806 -4.8156691 -4.6273894][-4.235167 -4.2022648 -4.278317 -4.3989382 -4.3829122 -4.2327662 -4.0965877 -4.1399107 -4.3583488 -4.63857 -4.8672395 -4.9702845 -4.9813528 -4.8857861 -4.6646256][-4.3910527 -4.3325949 -4.371037 -4.4701452 -4.468133 -4.3587465 -4.2546773 -4.2914052 -4.485642 -4.741888 -4.94394 -5.020956 -5.0100293 -4.9055662 -4.6810632][-4.59248 -4.5347276 -4.5494943 -4.6237907 -4.6364346 -4.5692163 -4.4910545 -4.5020542 -4.6297946 -4.8101721 -4.9543133 -5.0044813 -4.9801521 -4.8758779 -4.6680408][-4.7285028 -4.6938052 -4.700593 -4.746417 -4.7611814 -4.7249422 -4.670619 -4.6623139 -4.7273068 -4.834218 -4.924962 -4.9556222 -4.9237461 -4.8201795 -4.6364584][-4.7201061 -4.711329 -4.7185435 -4.739934 -4.7464471 -4.7244887 -4.6853056 -4.6686788 -4.6989951 -4.7647166 -4.8259244 -4.8479962 -4.8173923 -4.7242527 -4.5758758]]...]
INFO - root - 2017-12-07 12:00:39.308062: step 14310, loss = 21.70, batch loss = 21.62 (8.9 examples/sec; 0.903 sec/batch; 79h:50m:19s remains)
INFO - root - 2017-12-07 12:00:48.612578: step 14320, loss = 21.60, batch loss = 21.52 (9.1 examples/sec; 0.880 sec/batch; 77h:44m:30s remains)
INFO - root - 2017-12-07 12:00:58.008654: step 14330, loss = 21.76, batch loss = 21.68 (8.1 examples/sec; 0.983 sec/batch; 86h:52m:43s remains)
INFO - root - 2017-12-07 12:01:07.497061: step 14340, loss = 21.69, batch loss = 21.61 (8.0 examples/sec; 1.001 sec/batch; 88h:25m:59s remains)
INFO - root - 2017-12-07 12:01:16.873116: step 14350, loss = 21.41, batch loss = 21.33 (8.3 examples/sec; 0.959 sec/batch; 84h:46m:19s remains)
INFO - root - 2017-12-07 12:01:26.092013: step 14360, loss = 21.50, batch loss = 21.41 (8.2 examples/sec; 0.981 sec/batch; 86h:42m:57s remains)
INFO - root - 2017-12-07 12:01:35.537416: step 14370, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.949 sec/batch; 83h:50m:48s remains)
INFO - root - 2017-12-07 12:01:44.896316: step 14380, loss = 21.71, batch loss = 21.62 (8.0 examples/sec; 0.996 sec/batch; 88h:03m:15s remains)
INFO - root - 2017-12-07 12:01:54.239836: step 14390, loss = 21.35, batch loss = 21.27 (8.9 examples/sec; 0.901 sec/batch; 79h:36m:06s remains)
INFO - root - 2017-12-07 12:02:03.631340: step 14400, loss = 21.64, batch loss = 21.55 (8.6 examples/sec; 0.928 sec/batch; 81h:57m:49s remains)
2017-12-07 12:02:04.597814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4948897 -4.5280547 -4.5625763 -4.6064544 -4.6133242 -4.5772123 -4.5688972 -4.6206818 -4.6846824 -4.710228 -4.703506 -4.6942506 -4.7092018 -4.7425404 -4.7480555][-4.5351181 -4.570096 -4.6144757 -4.6732283 -4.6753712 -4.6089187 -4.5672846 -4.6120663 -4.6907015 -4.7309093 -4.7249718 -4.7095718 -4.7223883 -4.7542176 -4.744668][-4.5156465 -4.5435338 -4.5929084 -4.6657066 -4.6674566 -4.5702329 -4.4880352 -4.5229669 -4.6340971 -4.7225266 -4.7475443 -4.7343168 -4.7322149 -4.7526159 -4.7387915][-4.488091 -4.499229 -4.5451121 -4.6228776 -4.6213889 -4.4947639 -4.3653135 -4.3828444 -4.5440307 -4.7165937 -4.8013124 -4.7934737 -4.7522678 -4.7361712 -4.7115164][-4.4867654 -4.4714141 -4.4862828 -4.53233 -4.505167 -4.3466077 -4.1649218 -4.15235 -4.3619461 -4.6275196 -4.7785358 -4.7738705 -4.6842113 -4.6273437 -4.5980511][-4.5031948 -4.4614682 -4.4374595 -4.4384766 -4.3775139 -4.1865163 -3.9424739 -3.8670082 -4.089323 -4.4351311 -4.6674342 -4.6980815 -4.5915184 -4.508152 -4.4754434][-4.5372224 -4.4711018 -4.4151516 -4.3768349 -4.28172 -4.0566511 -3.7421627 -3.5777383 -3.7714076 -4.1698542 -4.4873214 -4.5833349 -4.4926581 -4.3954859 -4.3545084][-4.5607047 -4.4773369 -4.3992524 -4.3263364 -4.1949358 -3.9346139 -3.5617847 -3.3214254 -3.4791393 -3.9028842 -4.2803779 -4.4411292 -4.3893933 -4.298285 -4.2519593][-4.5869169 -4.516932 -4.4500966 -4.3715205 -4.2286077 -3.9587376 -3.5681937 -3.296479 -3.4232683 -3.8323152 -4.21503 -4.4098554 -4.403203 -4.3312912 -4.2730012][-4.6216383 -4.5996428 -4.5811634 -4.5359397 -4.4162178 -4.1669655 -3.7969828 -3.5336754 -3.6370301 -3.9985063 -4.3320127 -4.5109854 -4.5277581 -4.4739537 -4.4019885][-4.639689 -4.6621094 -4.6870856 -4.6782994 -4.5897827 -4.3737435 -4.0450697 -3.8116479 -3.8996966 -4.2072368 -4.473865 -4.6110992 -4.6302047 -4.5930357 -4.5290422][-4.6344728 -4.6757483 -4.7151241 -4.7276745 -4.6724958 -4.5056181 -4.2483826 -4.0705934 -4.1460571 -4.390677 -4.5888777 -4.6787915 -4.6874442 -4.6634159 -4.6240106][-4.5938087 -4.6332211 -4.6677 -4.6928577 -4.6759615 -4.5774674 -4.4154425 -4.3059707 -4.3640828 -4.5344496 -4.6650639 -4.7133188 -4.708859 -4.6908588 -4.670197][-4.5217948 -4.5403428 -4.5478983 -4.557477 -4.5544581 -4.5112171 -4.4370446 -4.3916864 -4.4322925 -4.5336733 -4.6144419 -4.65099 -4.6601691 -4.6647015 -4.6657381][-4.4651041 -4.4661894 -4.4446788 -4.4235377 -4.411458 -4.3996873 -4.38925 -4.3917251 -4.4195266 -4.4676871 -4.5131874 -4.5504103 -4.583385 -4.6148291 -4.6356196]]...]
INFO - root - 2017-12-07 12:02:13.864007: step 14410, loss = 21.54, batch loss = 21.45 (8.2 examples/sec; 0.980 sec/batch; 86h:37m:55s remains)
INFO - root - 2017-12-07 12:02:23.139838: step 14420, loss = 21.87, batch loss = 21.78 (8.4 examples/sec; 0.958 sec/batch; 84h:38m:53s remains)
INFO - root - 2017-12-07 12:02:32.577215: step 14430, loss = 21.21, batch loss = 21.13 (8.5 examples/sec; 0.937 sec/batch; 82h:47m:55s remains)
INFO - root - 2017-12-07 12:02:41.914525: step 14440, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.930 sec/batch; 82h:10m:51s remains)
INFO - root - 2017-12-07 12:02:51.323328: step 14450, loss = 21.53, batch loss = 21.44 (8.1 examples/sec; 0.993 sec/batch; 87h:45m:24s remains)
INFO - root - 2017-12-07 12:03:00.488975: step 14460, loss = 21.42, batch loss = 21.34 (9.8 examples/sec; 0.818 sec/batch; 72h:16m:07s remains)
INFO - root - 2017-12-07 12:03:09.794247: step 14470, loss = 21.38, batch loss = 21.29 (8.7 examples/sec; 0.919 sec/batch; 81h:11m:54s remains)
INFO - root - 2017-12-07 12:03:19.027662: step 14480, loss = 21.09, batch loss = 21.01 (8.2 examples/sec; 0.977 sec/batch; 86h:20m:04s remains)
INFO - root - 2017-12-07 12:03:28.420965: step 14490, loss = 21.14, batch loss = 21.06 (8.8 examples/sec; 0.908 sec/batch; 80h:12m:08s remains)
INFO - root - 2017-12-07 12:03:37.900516: step 14500, loss = 21.71, batch loss = 21.62 (8.9 examples/sec; 0.894 sec/batch; 79h:00m:40s remains)
2017-12-07 12:03:38.866638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4564233 -4.4508753 -4.4420905 -4.4174995 -4.369781 -4.2867794 -4.2150421 -4.226934 -4.2989411 -4.3601179 -4.3967619 -4.42722 -4.4367514 -4.3966794 -4.3148046][-4.4814019 -4.4929466 -4.49178 -4.4610195 -4.3998094 -4.305408 -4.2236495 -4.2096868 -4.2519069 -4.2988448 -4.3308578 -4.367444 -4.3907275 -4.3651495 -4.2869897][-4.4777174 -4.4967351 -4.510603 -4.4877858 -4.4164414 -4.3104706 -4.220715 -4.1879954 -4.2091255 -4.2509346 -4.2905908 -4.3382683 -4.377923 -4.3776674 -4.3238978][-4.4523616 -4.4743176 -4.5053377 -4.4963808 -4.4151278 -4.2913542 -4.1897526 -4.1482615 -4.1618295 -4.2068691 -4.2591705 -4.3180161 -4.3739882 -4.404779 -4.3897719][-4.4043293 -4.4297185 -4.4714956 -4.4654098 -4.3670173 -4.217597 -4.1007066 -4.066319 -4.1020122 -4.1745548 -4.2454844 -4.3078866 -4.3650904 -4.4125605 -4.4307995][-4.3506789 -4.387033 -4.4307251 -4.4121885 -4.2879696 -4.1041288 -3.9684346 -3.9529173 -4.0438347 -4.1697669 -4.261178 -4.3095908 -4.3458195 -4.389132 -4.4263706][-4.3025451 -4.3621883 -4.4098911 -4.3797851 -4.2355795 -4.0248995 -3.8711946 -3.8684061 -4.0097461 -4.1821342 -4.2812319 -4.3036084 -4.3082395 -4.3368921 -4.3817158][-4.2840896 -4.3711357 -4.4255614 -4.3872762 -4.232049 -4.0067453 -3.8360412 -3.8274765 -3.9901147 -4.1833682 -4.2753148 -4.2720032 -4.2522464 -4.2660289 -4.3105011][-4.2998037 -4.4049721 -4.4662118 -4.4276481 -4.2801223 -4.0679984 -3.8936749 -3.8598027 -4.0013304 -4.1807666 -4.2540011 -4.2318258 -4.2009029 -4.2093287 -4.2544589][-4.3139338 -4.4247856 -4.4926286 -4.4681816 -4.349659 -4.17435 -4.0127492 -3.9466355 -4.033535 -4.1726446 -4.2277026 -4.2001805 -4.1701379 -4.1772056 -4.2192774][-4.3093925 -4.4047713 -4.4729581 -4.4677944 -4.3821054 -4.24522 -4.103601 -4.0165868 -4.0549164 -4.1582417 -4.2047663 -4.1840558 -4.1576357 -4.1578431 -4.1901922][-4.297965 -4.362927 -4.4191422 -4.4271464 -4.3707557 -4.2736654 -4.1632118 -4.0755463 -4.0818715 -4.1525278 -4.1891551 -4.1775126 -4.1582751 -4.1553583 -4.1823425][-4.289237 -4.3192277 -4.3513556 -4.3621559 -4.3342743 -4.2843089 -4.2202992 -4.1502204 -4.1331325 -4.1625824 -4.17405 -4.1669006 -4.1662474 -4.1792693 -4.2132354][-4.2795415 -4.2810578 -4.2868195 -4.2922587 -4.2878079 -4.2831717 -4.2680974 -4.2218108 -4.1821055 -4.1604562 -4.133831 -4.1270013 -4.1594763 -4.2121878 -4.2697396][-4.2763667 -4.2645836 -4.2525005 -4.2472291 -4.2526865 -4.2766972 -4.2960138 -4.2693734 -4.2081 -4.1338778 -4.0645833 -4.0502291 -4.112257 -4.2098451 -4.2989688]]...]
INFO - root - 2017-12-07 12:03:48.164173: step 14510, loss = 21.21, batch loss = 21.13 (8.7 examples/sec; 0.923 sec/batch; 81h:29m:32s remains)
INFO - root - 2017-12-07 12:03:57.430119: step 14520, loss = 21.88, batch loss = 21.80 (8.5 examples/sec; 0.944 sec/batch; 83h:24m:49s remains)
INFO - root - 2017-12-07 12:04:06.916071: step 14530, loss = 21.73, batch loss = 21.65 (8.4 examples/sec; 0.950 sec/batch; 83h:55m:22s remains)
INFO - root - 2017-12-07 12:04:16.331288: step 14540, loss = 21.34, batch loss = 21.25 (9.1 examples/sec; 0.881 sec/batch; 77h:51m:08s remains)
INFO - root - 2017-12-07 12:04:25.784826: step 14550, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.942 sec/batch; 83h:09m:29s remains)
INFO - root - 2017-12-07 12:04:35.117327: step 14560, loss = 21.55, batch loss = 21.47 (8.3 examples/sec; 0.962 sec/batch; 84h:58m:05s remains)
INFO - root - 2017-12-07 12:04:44.334134: step 14570, loss = 21.63, batch loss = 21.55 (8.3 examples/sec; 0.966 sec/batch; 85h:19m:08s remains)
INFO - root - 2017-12-07 12:04:53.785076: step 14580, loss = 21.79, batch loss = 21.71 (8.4 examples/sec; 0.957 sec/batch; 84h:33m:05s remains)
INFO - root - 2017-12-07 12:05:03.133128: step 14590, loss = 21.26, batch loss = 21.17 (9.0 examples/sec; 0.884 sec/batch; 78h:04m:22s remains)
INFO - root - 2017-12-07 12:05:12.699124: step 14600, loss = 21.12, batch loss = 21.04 (8.3 examples/sec; 0.964 sec/batch; 85h:06m:47s remains)
2017-12-07 12:05:13.601837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3984189 -4.4489655 -4.4790812 -4.4843121 -4.4764009 -4.4692874 -4.4688683 -4.47133 -4.4731503 -4.474535 -4.4833541 -4.5040975 -4.5229721 -4.5210423 -4.4960442][-4.5016165 -4.5792179 -4.6166229 -4.6096625 -4.5763845 -4.5415926 -4.5214477 -4.5162482 -4.5260506 -4.5498219 -4.5853357 -4.6267419 -4.649766 -4.6332893 -4.5815287][-4.5939722 -4.6951151 -4.7322836 -4.7037582 -4.6340179 -4.5536518 -4.4961219 -4.4748993 -4.4984884 -4.5581269 -4.6318178 -4.6971283 -4.7283077 -4.7076664 -4.64376][-4.6397357 -4.7437344 -4.76181 -4.701755 -4.5910397 -4.4623661 -4.366785 -4.3334126 -4.3773527 -4.4790597 -4.5911989 -4.6776772 -4.7218943 -4.7163739 -4.6684937][-4.6493058 -4.7368836 -4.7223449 -4.6243696 -4.4726143 -4.298244 -4.1695085 -4.1297812 -4.19486 -4.3317294 -4.46953 -4.5667462 -4.6244984 -4.6486692 -4.6438904][-4.6522636 -4.7220278 -4.6717567 -4.5365739 -4.3482165 -4.1424971 -3.9948368 -3.9497972 -4.0270896 -4.1839004 -4.3354812 -4.4355674 -4.501287 -4.549541 -4.5901804][-4.649497 -4.7054014 -4.6250319 -4.4575825 -4.2449217 -4.028214 -3.8725438 -3.817637 -3.8963876 -4.0687146 -4.2322974 -4.3344874 -4.3984127 -4.4498854 -4.5127482][-4.6372452 -4.6915531 -4.603323 -4.422555 -4.1978807 -3.9720061 -3.7947221 -3.7117414 -3.78625 -3.98045 -4.1646638 -4.2696681 -4.3212638 -4.3532686 -4.4099884][-4.61416 -4.6831722 -4.6129627 -4.4424748 -4.21854 -3.9765322 -3.7601917 -3.6397195 -3.7109027 -3.9338715 -4.1430783 -4.248704 -4.2765665 -4.273201 -4.2976594][-4.5635972 -4.6480961 -4.6138296 -4.4777694 -4.2726645 -4.0227966 -3.7763212 -3.6327391 -3.7087321 -3.9495215 -4.1643939 -4.251956 -4.2359428 -4.1823764 -4.1640835][-4.494945 -4.5932674 -4.6062541 -4.5241241 -4.3553309 -4.1185966 -3.8708236 -3.730001 -3.8065596 -4.0311971 -4.218143 -4.2631464 -4.1911182 -4.0908842 -4.0430918][-4.4289079 -4.536324 -4.5958009 -4.57769 -4.4638968 -4.265327 -4.0453949 -3.9240921 -3.9880161 -4.1653442 -4.302702 -4.3068132 -4.2019758 -4.0861249 -4.0375566][-4.3703 -4.4766593 -4.5643306 -4.6004372 -4.5512996 -4.4133415 -4.2490845 -4.161747 -4.209764 -4.3305187 -4.4163828 -4.3972068 -4.2945976 -4.200357 -4.1757851][-4.32561 -4.4227033 -4.5189762 -4.5884032 -4.5949688 -4.5250745 -4.4301867 -4.3875446 -4.4273014 -4.5024438 -4.5502563 -4.5251646 -4.4471955 -4.3901992 -4.389946][-4.2936435 -4.3795924 -4.4696307 -4.5486379 -4.5886974 -4.5742078 -4.54347 -4.5442038 -4.5851369 -4.6339731 -4.6609936 -4.6426568 -4.5972724 -4.5736613 -4.5826449]]...]
INFO - root - 2017-12-07 12:05:23.023103: step 14610, loss = 21.33, batch loss = 21.24 (8.7 examples/sec; 0.919 sec/batch; 81h:08m:34s remains)
INFO - root - 2017-12-07 12:05:32.373657: step 14620, loss = 21.23, batch loss = 21.14 (8.6 examples/sec; 0.934 sec/batch; 82h:26m:51s remains)
INFO - root - 2017-12-07 12:05:41.765770: step 14630, loss = 21.30, batch loss = 21.22 (8.5 examples/sec; 0.946 sec/batch; 83h:32m:03s remains)
INFO - root - 2017-12-07 12:05:51.113720: step 14640, loss = 21.81, batch loss = 21.72 (8.4 examples/sec; 0.951 sec/batch; 83h:56m:08s remains)
INFO - root - 2017-12-07 12:06:00.418584: step 14650, loss = 21.29, batch loss = 21.20 (10.0 examples/sec; 0.801 sec/batch; 70h:41m:53s remains)
INFO - root - 2017-12-07 12:06:09.733428: step 14660, loss = 21.77, batch loss = 21.69 (9.2 examples/sec; 0.868 sec/batch; 76h:40m:43s remains)
INFO - root - 2017-12-07 12:06:19.039318: step 14670, loss = 21.66, batch loss = 21.58 (9.0 examples/sec; 0.893 sec/batch; 78h:48m:32s remains)
INFO - root - 2017-12-07 12:06:28.523142: step 14680, loss = 21.52, batch loss = 21.44 (8.9 examples/sec; 0.900 sec/batch; 79h:29m:43s remains)
INFO - root - 2017-12-07 12:06:37.977264: step 14690, loss = 21.69, batch loss = 21.61 (8.5 examples/sec; 0.942 sec/batch; 83h:08m:53s remains)
INFO - root - 2017-12-07 12:06:47.313302: step 14700, loss = 21.51, batch loss = 21.42 (8.6 examples/sec; 0.935 sec/batch; 82h:34m:36s remains)
2017-12-07 12:06:48.301915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5886469 -4.6026 -4.5887976 -4.5541196 -4.511796 -4.4749613 -4.4462252 -4.4318185 -4.4422503 -4.4705486 -4.4981852 -4.5218349 -4.523478 -4.5121379 -4.49885][-4.5613155 -4.5742893 -4.5562053 -4.5222411 -4.4879808 -4.4603767 -4.43755 -4.426537 -4.4412494 -4.4739614 -4.5024095 -4.519588 -4.5140986 -4.4994192 -4.4884558][-4.49585 -4.4948359 -4.4727254 -4.4469466 -4.4318419 -4.4195786 -4.402761 -4.3929434 -4.4034615 -4.4252787 -4.4387546 -4.4393897 -4.4264345 -4.414463 -4.4120584][-4.3812585 -4.3556213 -4.3220663 -4.3016253 -4.3037219 -4.3026209 -4.290638 -4.2833252 -4.2871923 -4.2903109 -4.2864375 -4.2732224 -4.2597404 -4.2561307 -4.2610431][-4.2650642 -4.2154641 -4.1649246 -4.1419468 -4.1462188 -4.1414313 -4.1264162 -4.1238775 -4.1329322 -4.1358328 -4.1343551 -4.1230907 -4.1130395 -4.1117411 -4.1126089][-4.1583166 -4.1034017 -4.0572443 -4.0466275 -4.0577512 -4.0464191 -4.0206594 -4.0099897 -4.0155048 -4.0213723 -4.0285907 -4.0277214 -4.0261617 -4.0318742 -4.0381913][-4.146399 -4.1174932 -4.0952954 -4.1003146 -4.1117353 -4.0867987 -4.0373297 -4.0004373 -3.9911797 -4.0009665 -4.0230889 -4.0432081 -4.0636168 -4.08676 -4.1086192][-4.2579775 -4.256619 -4.2457566 -4.2436428 -4.2378788 -4.1939921 -4.1223192 -4.0604506 -4.0392184 -4.0582118 -4.1023512 -4.1483006 -4.1876497 -4.2203188 -4.2473083][-4.3462915 -4.3489795 -4.3287215 -4.3089223 -4.2914371 -4.2518864 -4.1907434 -4.1332612 -4.112133 -4.1319556 -4.1779761 -4.2250128 -4.2576194 -4.2787938 -4.2956247][-4.3620462 -4.3462877 -4.3080916 -4.27993 -4.2772236 -4.2740889 -4.2549257 -4.226881 -4.211751 -4.2170315 -4.2372575 -4.2553282 -4.2576261 -4.2529325 -4.2513738][-4.3666015 -4.3363571 -4.2822523 -4.251533 -4.2685056 -4.3003545 -4.3179374 -4.3142805 -4.3006721 -4.2832689 -4.2689581 -4.2532287 -4.2260704 -4.2013235 -4.1884365][-4.3818665 -4.3511372 -4.28627 -4.2482543 -4.2699413 -4.3133531 -4.3417244 -4.3470116 -4.3328853 -4.300056 -4.2667289 -4.2375779 -4.2048106 -4.1811008 -4.1712132][-4.4028883 -4.3768172 -4.3118162 -4.2697225 -4.2878795 -4.3252668 -4.3452196 -4.3494077 -4.3391352 -4.3070722 -4.2747393 -4.2525182 -4.2345109 -4.2289414 -4.2316389][-4.4271479 -4.4111218 -4.3574514 -4.3199735 -4.3307176 -4.3508563 -4.3538089 -4.3560548 -4.3586869 -4.3433928 -4.3244505 -4.3141575 -4.3109388 -4.3201566 -4.3307009][-4.448761 -4.4432135 -4.4015889 -4.3706241 -4.3721719 -4.3732076 -4.3616614 -4.3609347 -4.3707914 -4.3657022 -4.3536711 -4.3505239 -4.3584719 -4.3788948 -4.3917542]]...]
INFO - root - 2017-12-07 12:06:57.744677: step 14710, loss = 21.41, batch loss = 21.33 (8.2 examples/sec; 0.973 sec/batch; 85h:53m:28s remains)
INFO - root - 2017-12-07 12:07:07.051720: step 14720, loss = 21.32, batch loss = 21.24 (8.8 examples/sec; 0.908 sec/batch; 80h:11m:16s remains)
INFO - root - 2017-12-07 12:07:16.407792: step 14730, loss = 21.47, batch loss = 21.38 (8.7 examples/sec; 0.923 sec/batch; 81h:29m:20s remains)
INFO - root - 2017-12-07 12:07:25.819902: step 14740, loss = 21.37, batch loss = 21.29 (9.1 examples/sec; 0.882 sec/batch; 77h:53m:34s remains)
INFO - root - 2017-12-07 12:07:35.289139: step 14750, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.927 sec/batch; 81h:47m:56s remains)
INFO - root - 2017-12-07 12:07:44.632080: step 14760, loss = 21.15, batch loss = 21.07 (9.0 examples/sec; 0.893 sec/batch; 78h:50m:39s remains)
INFO - root - 2017-12-07 12:07:53.865881: step 14770, loss = 21.52, batch loss = 21.44 (8.7 examples/sec; 0.918 sec/batch; 81h:01m:16s remains)
INFO - root - 2017-12-07 12:08:03.324902: step 14780, loss = 21.21, batch loss = 21.13 (8.5 examples/sec; 0.936 sec/batch; 82h:37m:17s remains)
INFO - root - 2017-12-07 12:08:12.627653: step 14790, loss = 21.53, batch loss = 21.44 (8.3 examples/sec; 0.958 sec/batch; 84h:33m:14s remains)
INFO - root - 2017-12-07 12:08:22.063880: step 14800, loss = 21.64, batch loss = 21.56 (8.3 examples/sec; 0.961 sec/batch; 84h:45m:52s remains)
2017-12-07 12:08:23.141003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3018279 -4.2670612 -4.2458038 -4.244751 -4.2570858 -4.2598786 -4.2372241 -4.2050843 -4.1931214 -4.2136116 -4.2869148 -4.3966541 -4.4692736 -4.4769611 -4.4255533][-4.3934774 -4.3628979 -4.3469577 -4.3583493 -4.3896089 -4.412766 -4.4055405 -4.3778605 -4.3539543 -4.348743 -4.3866181 -4.4586272 -4.5038939 -4.4985757 -4.4434996][-4.4587631 -4.4403105 -4.43267 -4.4542623 -4.4982691 -4.5315948 -4.5240464 -4.4867382 -4.4510789 -4.4360766 -4.4610124 -4.515276 -4.544158 -4.5243645 -4.4591775][-4.4776216 -4.4661369 -4.4537463 -4.4669046 -4.5063887 -4.5359945 -4.5111375 -4.4492011 -4.403326 -4.4005241 -4.4484782 -4.5225172 -4.559247 -4.5344362 -4.4598684][-4.4106455 -4.397099 -4.3664231 -4.355576 -4.3814907 -4.4050117 -4.3595877 -4.2671542 -4.2170506 -4.2451525 -4.345943 -4.4714684 -4.5387783 -4.5214996 -4.4411716][-4.29075 -4.2776074 -4.2281857 -4.1901731 -4.201438 -4.2248149 -4.1676607 -4.0544519 -4.0119853 -4.0804882 -4.2412643 -4.4222679 -4.5218606 -4.5106993 -4.4212656][-4.1787763 -4.1769581 -4.1198392 -4.0593648 -4.0533128 -4.0742416 -4.0132122 -3.8977468 -3.8765559 -3.9858186 -4.1947808 -4.4148121 -4.5332408 -4.5218263 -4.41982][-4.099556 -4.1158209 -4.0636992 -3.9937024 -3.9727297 -3.9875629 -3.9307847 -3.8312395 -3.8377647 -3.9734669 -4.1988311 -4.4276328 -4.5482278 -4.5349312 -4.4266605][-4.1157207 -4.1429825 -4.0992885 -4.0279565 -3.9937396 -3.9992716 -3.9531591 -3.8823357 -3.9147825 -4.0574665 -4.2710247 -4.4824 -4.5888276 -4.5661945 -4.4512029][-4.1832972 -4.2109222 -4.1765237 -4.1130667 -4.074234 -4.0733376 -4.0441146 -4.0042605 -4.0478296 -4.1751089 -4.3555493 -4.5323076 -4.620038 -4.5935307 -4.4817061][-4.261282 -4.2862663 -4.2686472 -4.2268629 -4.1974597 -4.1969776 -4.1835871 -4.1653814 -4.2018747 -4.2968049 -4.4315906 -4.5649047 -4.6309757 -4.6034846 -4.5020943][-4.3556366 -4.3750205 -4.3739786 -4.3555365 -4.3381667 -4.3360643 -4.3286476 -4.3190832 -4.3417039 -4.4040146 -4.4975791 -4.5909624 -4.6350722 -4.6037431 -4.5106521][-4.3803673 -4.3916836 -4.3996663 -4.3981056 -4.3914165 -4.3871865 -4.3788528 -4.3701377 -4.382803 -4.4250121 -4.4915314 -4.5591645 -4.5899782 -4.5601573 -4.4816818][-4.3461876 -4.3483257 -4.3591747 -4.3674006 -4.3699017 -4.367188 -4.3575768 -4.3468995 -4.3534718 -4.3852587 -4.4368095 -4.4894371 -4.5119987 -4.4855738 -4.4230671][-4.2743254 -4.2733283 -4.2849727 -4.2972922 -4.3046403 -4.3028483 -4.2911105 -4.2761312 -4.2763233 -4.3017354 -4.34816 -4.3988686 -4.4235396 -4.4057031 -4.3599372]]...]
INFO - root - 2017-12-07 12:08:32.467193: step 14810, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.949 sec/batch; 83h:45m:46s remains)
INFO - root - 2017-12-07 12:08:41.907779: step 14820, loss = 21.93, batch loss = 21.85 (8.4 examples/sec; 0.956 sec/batch; 84h:23m:27s remains)
INFO - root - 2017-12-07 12:08:51.246963: step 14830, loss = 21.43, batch loss = 21.35 (8.2 examples/sec; 0.977 sec/batch; 86h:12m:20s remains)
INFO - root - 2017-12-07 12:09:00.631174: step 14840, loss = 21.30, batch loss = 21.22 (8.3 examples/sec; 0.963 sec/batch; 84h:56m:54s remains)
INFO - root - 2017-12-07 12:09:09.993261: step 14850, loss = 21.40, batch loss = 21.32 (8.2 examples/sec; 0.970 sec/batch; 85h:34m:13s remains)
INFO - root - 2017-12-07 12:09:19.381981: step 14860, loss = 21.75, batch loss = 21.67 (8.3 examples/sec; 0.964 sec/batch; 85h:03m:12s remains)
INFO - root - 2017-12-07 12:09:28.787362: step 14870, loss = 21.43, batch loss = 21.34 (8.5 examples/sec; 0.941 sec/batch; 82h:59m:45s remains)
INFO - root - 2017-12-07 12:09:38.283682: step 14880, loss = 21.34, batch loss = 21.25 (9.0 examples/sec; 0.894 sec/batch; 78h:50m:36s remains)
INFO - root - 2017-12-07 12:09:47.582461: step 14890, loss = 21.39, batch loss = 21.31 (8.9 examples/sec; 0.895 sec/batch; 78h:59m:08s remains)
INFO - root - 2017-12-07 12:09:56.970552: step 14900, loss = 21.46, batch loss = 21.37 (8.8 examples/sec; 0.914 sec/batch; 80h:37m:59s remains)
2017-12-07 12:09:57.958223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2649746 -4.1764579 -4.1336117 -4.1419621 -4.17938 -4.2216234 -4.2565961 -4.2703142 -4.2742229 -4.2939548 -4.3209076 -4.3367171 -4.3554006 -4.3770275 -4.3994455][-4.2575436 -4.1849303 -4.1656256 -4.1845274 -4.2039609 -4.2064576 -4.2059541 -4.2113266 -4.2292256 -4.2686625 -4.3129864 -4.3455615 -4.3784413 -4.4113922 -4.436955][-4.2488394 -4.1951275 -4.199017 -4.2296925 -4.2324162 -4.1992989 -4.1706829 -4.1756291 -4.2096772 -4.2603989 -4.3093081 -4.3472443 -4.3815107 -4.4130025 -4.4366207][-4.2324843 -4.1944566 -4.2159181 -4.2541556 -4.2511587 -4.205617 -4.16514 -4.1707249 -4.2151227 -4.269948 -4.3145185 -4.3485708 -4.3740411 -4.3909307 -4.4028983][-4.2161493 -4.1899886 -4.2082944 -4.2328086 -4.2238326 -4.1879263 -4.1609497 -4.1761317 -4.2310238 -4.2869387 -4.32343 -4.3483467 -4.3632717 -4.3593016 -4.3510771][-4.1936769 -4.1669192 -4.1609869 -4.1513 -4.1256866 -4.105042 -4.1090217 -4.149436 -4.2241254 -4.2930293 -4.3330579 -4.3563738 -4.3633995 -4.3357348 -4.3003507][-4.18621 -4.1489916 -4.1185322 -4.0742598 -4.0234976 -4.0037889 -4.0337033 -4.099093 -4.1937819 -4.2835512 -4.3399467 -4.3690977 -4.3664165 -4.3120437 -4.2442522][-4.1981826 -4.1559968 -4.121583 -4.0712709 -4.0095525 -3.9809666 -4.0119238 -4.0796371 -4.1702337 -4.2643256 -4.3363419 -4.3766851 -4.368845 -4.300849 -4.2076492][-4.2486482 -4.19682 -4.1595764 -4.1142883 -4.0547137 -4.0183358 -4.0362248 -4.09284 -4.1686025 -4.24837 -4.3218031 -4.3685126 -4.3609118 -4.2932186 -4.1927123][-4.3430605 -4.2715259 -4.214807 -4.1583157 -4.0962257 -4.0546527 -4.060441 -4.1074147 -4.1777391 -4.2490792 -4.3169541 -4.3587222 -4.3455191 -4.2814603 -4.1883526][-4.4224334 -4.3444166 -4.2714515 -4.1994414 -4.1352119 -4.0968328 -4.1027284 -4.1433249 -4.2067709 -4.2721391 -4.3342872 -4.3655748 -4.340508 -4.2714405 -4.1835814][-4.4246817 -4.3647985 -4.3012419 -4.2297993 -4.1718383 -4.145823 -4.1627984 -4.2038951 -4.259469 -4.319397 -4.3783817 -4.4044919 -4.3710227 -4.2900095 -4.1978478][-4.3627281 -4.3211732 -4.2794051 -4.2276263 -4.1916208 -4.188539 -4.2274027 -4.2804818 -4.3327742 -4.3819842 -4.429955 -4.4489388 -4.4126 -4.329793 -4.2441416][-4.2798309 -4.2374125 -4.2113252 -4.1904788 -4.1899672 -4.21839 -4.2772269 -4.3414521 -4.3890405 -4.4219589 -4.4489512 -4.4549575 -4.4212918 -4.3541818 -4.2920146][-4.2473307 -4.1904311 -4.1611462 -4.160152 -4.192966 -4.2481093 -4.3115983 -4.3659425 -4.3906517 -4.4021678 -4.4116235 -4.4088516 -4.3834791 -4.3425674 -4.3111897]]...]
INFO - root - 2017-12-07 12:10:07.129272: step 14910, loss = 21.51, batch loss = 21.43 (8.2 examples/sec; 0.973 sec/batch; 85h:49m:27s remains)
INFO - root - 2017-12-07 12:10:16.401839: step 14920, loss = 21.56, batch loss = 21.48 (8.8 examples/sec; 0.907 sec/batch; 79h:59m:58s remains)
INFO - root - 2017-12-07 12:10:25.762206: step 14930, loss = 21.42, batch loss = 21.33 (8.5 examples/sec; 0.944 sec/batch; 83h:14m:05s remains)
INFO - root - 2017-12-07 12:10:35.191567: step 14940, loss = 20.94, batch loss = 20.85 (8.1 examples/sec; 0.986 sec/batch; 86h:56m:51s remains)
INFO - root - 2017-12-07 12:10:44.634214: step 14950, loss = 21.65, batch loss = 21.57 (8.6 examples/sec; 0.927 sec/batch; 81h:48m:06s remains)
INFO - root - 2017-12-07 12:10:54.042933: step 14960, loss = 21.48, batch loss = 21.40 (8.9 examples/sec; 0.898 sec/batch; 79h:14m:24s remains)
INFO - root - 2017-12-07 12:11:03.349220: step 14970, loss = 21.10, batch loss = 21.02 (9.1 examples/sec; 0.879 sec/batch; 77h:29m:41s remains)
INFO - root - 2017-12-07 12:11:12.664021: step 14980, loss = 21.37, batch loss = 21.29 (8.4 examples/sec; 0.949 sec/batch; 83h:44m:34s remains)
INFO - root - 2017-12-07 12:11:22.114197: step 14990, loss = 21.21, batch loss = 21.13 (8.1 examples/sec; 0.982 sec/batch; 86h:34m:36s remains)
INFO - root - 2017-12-07 12:11:31.254009: step 15000, loss = 21.45, batch loss = 21.37 (8.6 examples/sec; 0.932 sec/batch; 82h:13m:32s remains)
2017-12-07 12:11:32.240417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.531301 -4.5857768 -4.6176867 -4.611445 -4.5200357 -4.3858724 -4.2767482 -4.2153964 -4.232985 -4.3225532 -4.4281263 -4.5332012 -4.6087947 -4.6055737 -4.531714][-4.5057621 -4.5902166 -4.6530738 -4.6585145 -4.5666785 -4.4415221 -4.3662534 -4.3533635 -4.39324 -4.463727 -4.5394568 -4.6190653 -4.673584 -4.6526704 -4.560967][-4.3736119 -4.4926286 -4.58428 -4.5948515 -4.5011382 -4.3836493 -4.3352332 -4.3702722 -4.4475865 -4.5266323 -4.6023569 -4.67733 -4.7171578 -4.6799889 -4.5718431][-4.2306662 -4.3680477 -4.4700713 -4.471806 -4.3647885 -4.236166 -4.1846905 -4.23879 -4.3466988 -4.4570956 -4.5693312 -4.6702318 -4.716918 -4.6755843 -4.5606136][-4.1562819 -4.2907634 -4.3710971 -4.3392081 -4.2029119 -4.0433736 -3.9515481 -3.9820466 -4.10013 -4.2535443 -4.4268537 -4.58106 -4.6604595 -4.6362305 -4.5319014][-4.1480169 -4.2540393 -4.2971406 -4.2294574 -4.0729265 -3.8836453 -3.7333679 -3.7131896 -3.8281403 -4.0222807 -4.2524695 -4.4577708 -4.5766296 -4.5810294 -4.4988527][-4.1614747 -4.2203932 -4.2353396 -4.1668024 -4.0291624 -3.835475 -3.6369936 -3.5630219 -3.6697586 -3.893718 -4.1522856 -4.3754148 -4.510745 -4.5356889 -4.4741516][-4.1969171 -4.2057619 -4.2041426 -4.1657295 -4.0812745 -3.9170384 -3.7057915 -3.6028345 -3.7008972 -3.9355071 -4.18858 -4.38844 -4.5032406 -4.523766 -4.4669814][-4.2742472 -4.2456841 -4.2363386 -4.2365727 -4.2143397 -4.1020107 -3.9248588 -3.8317389 -3.9166284 -4.1236167 -4.3375969 -4.4874759 -4.5553312 -4.5451303 -4.474268][-4.3835807 -4.3314896 -4.3140173 -4.3345509 -4.3513074 -4.2861609 -4.1624866 -4.1005325 -4.169827 -4.32622 -4.4911766 -4.5977774 -4.6252794 -4.5793557 -4.4848056][-4.490726 -4.43086 -4.4004593 -4.4156389 -4.4383554 -4.3989887 -4.3165288 -4.2802482 -4.3331213 -4.445776 -4.5813203 -4.6720486 -4.6803813 -4.6063724 -4.4870596][-4.5815415 -4.5318732 -4.496666 -4.498322 -4.5110879 -4.4846005 -4.4280071 -4.4027672 -4.4364247 -4.5196605 -4.6380906 -4.7185159 -4.7096262 -4.609952 -4.4717884][-4.6212959 -4.595974 -4.5711613 -4.5705862 -4.579463 -4.5681443 -4.5324931 -4.5126042 -4.528564 -4.5923924 -4.6875677 -4.7382245 -4.6981978 -4.5763855 -4.4353719][-4.6032476 -4.5958376 -4.5846109 -4.5877666 -4.5995731 -4.6071529 -4.5948343 -4.5844822 -4.594821 -4.6430593 -4.7051535 -4.7121921 -4.637538 -4.5076747 -4.3845987][-4.5364213 -4.5432911 -4.5414605 -4.54248 -4.5518475 -4.5677719 -4.5724697 -4.5714889 -4.5838346 -4.6233034 -4.6600671 -4.6378713 -4.5488663 -4.4311905 -4.3377829]]...]
INFO - root - 2017-12-07 12:11:41.716917: step 15010, loss = 21.19, batch loss = 21.11 (8.2 examples/sec; 0.972 sec/batch; 85h:44m:13s remains)
INFO - root - 2017-12-07 12:11:51.165589: step 15020, loss = 21.42, batch loss = 21.34 (8.2 examples/sec; 0.973 sec/batch; 85h:50m:25s remains)
INFO - root - 2017-12-07 12:12:00.772779: step 15030, loss = 21.36, batch loss = 21.28 (8.0 examples/sec; 0.998 sec/batch; 87h:59m:56s remains)
INFO - root - 2017-12-07 12:12:10.234569: step 15040, loss = 21.62, batch loss = 21.54 (8.0 examples/sec; 1.001 sec/batch; 88h:16m:48s remains)
INFO - root - 2017-12-07 12:12:19.632095: step 15050, loss = 21.24, batch loss = 21.15 (8.6 examples/sec; 0.935 sec/batch; 82h:29m:26s remains)
INFO - root - 2017-12-07 12:12:29.056098: step 15060, loss = 21.59, batch loss = 21.50 (8.2 examples/sec; 0.970 sec/batch; 85h:32m:35s remains)
INFO - root - 2017-12-07 12:12:38.411532: step 15070, loss = 22.04, batch loss = 21.95 (8.7 examples/sec; 0.920 sec/batch; 81h:05m:00s remains)
INFO - root - 2017-12-07 12:12:47.822438: step 15080, loss = 21.12, batch loss = 21.04 (8.3 examples/sec; 0.965 sec/batch; 85h:04m:09s remains)
INFO - root - 2017-12-07 12:12:57.457391: step 15090, loss = 21.54, batch loss = 21.45 (8.5 examples/sec; 0.942 sec/batch; 83h:00m:48s remains)
INFO - root - 2017-12-07 12:13:06.865020: step 15100, loss = 21.36, batch loss = 21.27 (8.5 examples/sec; 0.943 sec/batch; 83h:10m:17s remains)
2017-12-07 12:13:07.895488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5040951 -4.5669556 -4.6487122 -4.7325182 -4.7826834 -4.7707567 -4.7024164 -4.6246819 -4.5684533 -4.5438213 -4.5300546 -4.4999447 -4.4567356 -4.4276228 -4.4314547][-4.5223022 -4.5991874 -4.7013006 -4.798337 -4.8428321 -4.8031416 -4.6967311 -4.5908861 -4.5311818 -4.53282 -4.55052 -4.5282025 -4.4718571 -4.426753 -4.418961][-4.5105567 -4.5854931 -4.6853704 -4.7669187 -4.7850132 -4.7133079 -4.5801387 -4.4651823 -4.42541 -4.4807649 -4.5551558 -4.5548968 -4.4874716 -4.4225225 -4.3965249][-4.4697556 -4.5318108 -4.6128473 -4.6489053 -4.601048 -4.4723549 -4.3168087 -4.2194991 -4.2306309 -4.3539624 -4.4916296 -4.5237055 -4.4584808 -4.385077 -4.3521047][-4.4283175 -4.4853969 -4.5535364 -4.5333505 -4.3883967 -4.1710629 -3.982286 -3.9149189 -3.9978261 -4.191267 -4.378881 -4.4404945 -4.390008 -4.3254027 -4.3010068][-4.3916025 -4.4561615 -4.5217123 -4.4531765 -4.2153006 -3.9041657 -3.6744444 -3.6323802 -3.7833169 -4.0354614 -4.2531114 -4.3327942 -4.3019719 -4.2603445 -4.2610769][-4.3462229 -4.4130969 -4.4796128 -4.3929462 -4.1207652 -3.7706652 -3.5215943 -3.4902279 -3.6742644 -3.9429049 -4.1502061 -4.2203765 -4.2060423 -4.1998186 -4.2400832][-4.3077569 -4.3636332 -4.4266829 -4.3568625 -4.1231079 -3.8167911 -3.5912089 -3.550317 -3.7034452 -3.9306738 -4.0939097 -4.1434255 -4.1471357 -4.1801243 -4.2572842][-4.3151293 -4.3455186 -4.3905234 -4.3480225 -4.1845436 -3.9638727 -3.7867718 -3.7264736 -3.8095694 -3.9647658 -4.086472 -4.1346269 -4.1665597 -4.2326303 -4.3268461][-4.3779316 -4.3860459 -4.4115863 -4.3984156 -4.3110008 -4.1799355 -4.0580711 -3.9825752 -3.9913707 -4.0662026 -4.147306 -4.2055244 -4.2652245 -4.3477983 -4.4380922][-4.4740391 -4.4688826 -4.482748 -4.4963021 -4.4773388 -4.4254031 -4.3538661 -4.273 -4.2182403 -4.2178631 -4.2638364 -4.32935 -4.402102 -4.4811273 -4.5536942][-4.5647926 -4.5592947 -4.5686646 -4.5965152 -4.6132617 -4.6037951 -4.5646391 -4.4870424 -4.399714 -4.3590736 -4.3885169 -4.459074 -4.5310345 -4.5926404 -4.6411715][-4.6204987 -4.630084 -4.6499619 -4.6924825 -4.728241 -4.7363939 -4.7101569 -4.63648 -4.5391026 -4.4858947 -4.5119963 -4.5817542 -4.642705 -4.6776934 -4.6977835][-4.6296787 -4.6525965 -4.6854792 -4.7379975 -4.7811217 -4.7942867 -4.77046 -4.7036877 -4.6172624 -4.574348 -4.6048 -4.6664343 -4.70479 -4.7099237 -4.7046251][-4.5946364 -4.6157665 -4.6470981 -4.6909375 -4.7238145 -4.7284427 -4.7035141 -4.6552582 -4.6055231 -4.591908 -4.6247182 -4.6652203 -4.6761184 -4.6626873 -4.649456]]...]
INFO - root - 2017-12-07 12:13:17.536545: step 15110, loss = 21.14, batch loss = 21.06 (8.1 examples/sec; 0.988 sec/batch; 87h:08m:58s remains)
INFO - root - 2017-12-07 12:13:26.959884: step 15120, loss = 21.93, batch loss = 21.84 (7.9 examples/sec; 1.007 sec/batch; 88h:48m:04s remains)
INFO - root - 2017-12-07 12:13:36.385476: step 15130, loss = 21.59, batch loss = 21.51 (8.1 examples/sec; 0.984 sec/batch; 86h:42m:31s remains)
INFO - root - 2017-12-07 12:13:45.844300: step 15140, loss = 21.49, batch loss = 21.41 (8.5 examples/sec; 0.941 sec/batch; 82h:59m:13s remains)
INFO - root - 2017-12-07 12:13:55.420212: step 15150, loss = 21.41, batch loss = 21.32 (8.2 examples/sec; 0.973 sec/batch; 85h:48m:11s remains)
INFO - root - 2017-12-07 12:14:04.958246: step 15160, loss = 21.43, batch loss = 21.35 (8.3 examples/sec; 0.966 sec/batch; 85h:09m:42s remains)
INFO - root - 2017-12-07 12:14:14.201507: step 15170, loss = 21.66, batch loss = 21.57 (8.4 examples/sec; 0.955 sec/batch; 84h:10m:09s remains)
INFO - root - 2017-12-07 12:14:23.526776: step 15180, loss = 21.10, batch loss = 21.02 (9.5 examples/sec; 0.839 sec/batch; 73h:56m:37s remains)
INFO - root - 2017-12-07 12:14:32.951449: step 15190, loss = 21.18, batch loss = 21.10 (8.8 examples/sec; 0.913 sec/batch; 80h:27m:57s remains)
INFO - root - 2017-12-07 12:14:42.408134: step 15200, loss = 21.32, batch loss = 21.24 (8.8 examples/sec; 0.914 sec/batch; 80h:33m:02s remains)
2017-12-07 12:14:43.383274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2286873 -4.2237468 -4.2060466 -4.200532 -4.221313 -4.2330527 -4.2387748 -4.2342529 -4.2284236 -4.2522388 -4.29807 -4.3530388 -4.3951678 -4.4009566 -4.3920741][-4.3560185 -4.3447537 -4.2991066 -4.2602258 -4.2544374 -4.2566214 -4.2643709 -4.27782 -4.2935381 -4.3301063 -4.3812065 -4.428556 -4.4535704 -4.44178 -4.4173541][-4.4504824 -4.4415278 -4.3905735 -4.3383369 -4.3019934 -4.2717624 -4.2619267 -4.2793159 -4.3108072 -4.36548 -4.430337 -4.4814219 -4.4988418 -4.4769597 -4.4411941][-4.477879 -4.483387 -4.4481473 -4.4028573 -4.3434925 -4.2731819 -4.2369394 -4.2507944 -4.2976289 -4.3760772 -4.4584217 -4.5155649 -4.5280108 -4.4957614 -4.4505305][-4.4496927 -4.4750676 -4.4631586 -4.4263959 -4.3475232 -4.2414842 -4.1826243 -4.1944008 -4.2551455 -4.3581715 -4.4628263 -4.52753 -4.5387387 -4.4980907 -4.4449606][-4.3904991 -4.4285026 -4.4311457 -4.3890896 -4.29211 -4.1653337 -4.1004677 -4.1137285 -4.1796856 -4.2958937 -4.4174995 -4.4945574 -4.5149803 -4.4805813 -4.4298372][-4.30959 -4.3491478 -4.3570104 -4.3043904 -4.1920462 -4.0570188 -3.9865785 -3.9908168 -4.04871 -4.1660976 -4.3029971 -4.4052725 -4.45434 -4.4451361 -4.4115715][-4.2274361 -4.2561464 -4.2646418 -4.211287 -4.1024189 -3.9744933 -3.8930647 -3.8681791 -3.8998017 -4.0043354 -4.1544466 -4.2937603 -4.3833318 -4.4083524 -4.3974504][-4.1851788 -4.1944919 -4.200583 -4.1594028 -4.0713792 -3.9618099 -3.8718266 -3.8162465 -3.8146472 -3.8955655 -4.0480375 -4.2167025 -4.3397417 -4.3918118 -4.3966064][-4.2086558 -4.191926 -4.19882 -4.1890411 -4.1430259 -4.0640373 -3.9736285 -3.8954439 -3.8627615 -3.9170337 -4.0553756 -4.2263918 -4.3540483 -4.4094987 -4.4116144][-4.2698808 -4.2258258 -4.2377596 -4.2714081 -4.2819057 -4.2409797 -4.1604357 -4.0724778 -4.0218496 -4.0552459 -4.1729012 -4.3253965 -4.4298487 -4.4600406 -4.4375868][-4.3241267 -4.2674613 -4.2892175 -4.3580813 -4.4103484 -4.3992219 -4.3327527 -4.2469125 -4.191473 -4.2164922 -4.321002 -4.4489021 -4.5171876 -4.5092225 -4.4574733][-4.3807492 -4.3304105 -4.3570518 -4.4313993 -4.4931536 -4.4953423 -4.4462757 -4.3782558 -4.3391171 -4.3722448 -4.4661617 -4.5606451 -4.58449 -4.5371122 -4.4626813][-4.4708219 -4.4422159 -4.4654016 -4.5184541 -4.5581284 -4.553925 -4.5237713 -4.4923549 -4.4897285 -4.5328989 -4.6022606 -4.64751 -4.6212249 -4.5402646 -4.4536867][-4.5515704 -4.5453844 -4.5637474 -4.5908928 -4.6024461 -4.5888152 -4.5747871 -4.5769172 -4.6038833 -4.6444731 -4.6766653 -4.6696668 -4.6042094 -4.5094719 -4.4283652]]...]
INFO - root - 2017-12-07 12:14:52.736237: step 15210, loss = 21.22, batch loss = 21.14 (9.2 examples/sec; 0.874 sec/batch; 76h:59m:36s remains)
INFO - root - 2017-12-07 12:15:02.126235: step 15220, loss = 21.54, batch loss = 21.46 (8.7 examples/sec; 0.917 sec/batch; 80h:46m:28s remains)
INFO - root - 2017-12-07 12:15:11.509308: step 15230, loss = 20.91, batch loss = 20.82 (8.3 examples/sec; 0.962 sec/batch; 84h:45m:32s remains)
INFO - root - 2017-12-07 12:15:20.765522: step 15240, loss = 21.40, batch loss = 21.32 (8.3 examples/sec; 0.965 sec/batch; 85h:00m:19s remains)
INFO - root - 2017-12-07 12:15:30.182694: step 15250, loss = 21.47, batch loss = 21.39 (8.2 examples/sec; 0.971 sec/batch; 85h:31m:46s remains)
INFO - root - 2017-12-07 12:15:39.686091: step 15260, loss = 21.26, batch loss = 21.18 (8.3 examples/sec; 0.969 sec/batch; 85h:22m:41s remains)
INFO - root - 2017-12-07 12:15:49.218696: step 15270, loss = 21.49, batch loss = 21.40 (8.5 examples/sec; 0.944 sec/batch; 83h:13m:28s remains)
INFO - root - 2017-12-07 12:15:58.543323: step 15280, loss = 21.59, batch loss = 21.50 (8.2 examples/sec; 0.975 sec/batch; 85h:56m:06s remains)
INFO - root - 2017-12-07 12:16:07.893000: step 15290, loss = 21.46, batch loss = 21.38 (8.4 examples/sec; 0.958 sec/batch; 84h:23m:10s remains)
INFO - root - 2017-12-07 12:16:17.177285: step 15300, loss = 21.42, batch loss = 21.34 (9.0 examples/sec; 0.894 sec/batch; 78h:43m:39s remains)
2017-12-07 12:16:18.113155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.416913 -4.24322 -4.2043638 -4.3166661 -4.4501095 -4.5009456 -4.4561844 -4.3828163 -4.3718014 -4.4015317 -4.3872547 -4.2731 -4.1354661 -4.0676155 -4.0348344][-4.3885202 -4.2500515 -4.2476363 -4.3710785 -4.4801331 -4.4825878 -4.3976974 -4.3207512 -4.336422 -4.3972421 -4.4054246 -4.3113556 -4.1827431 -4.1071758 -4.088202][-4.369278 -4.2557273 -4.2724071 -4.4022512 -4.4957509 -4.4658875 -4.3575239 -4.2962618 -4.3506079 -4.4392805 -4.4543619 -4.3635011 -4.2355027 -4.15852 -4.1572223][-4.3328972 -4.2227106 -4.2362924 -4.3708482 -4.4738588 -4.442132 -4.3258634 -4.2820444 -4.3711109 -4.4873457 -4.5089383 -4.4185772 -4.2916641 -4.2176714 -4.2274475][-4.2635422 -4.1456265 -4.145216 -4.2755551 -4.3887019 -4.3585458 -4.2227416 -4.1707 -4.2892246 -4.4613 -4.5335321 -4.4753242 -4.362597 -4.2925777 -4.3071847][-4.2140021 -4.0887146 -4.06382 -4.158071 -4.245585 -4.18871 -4.0055618 -3.9148583 -4.05453 -4.3020997 -4.4697919 -4.4831824 -4.4004493 -4.3361487 -4.3534579][-4.2505078 -4.131609 -4.0719748 -4.0927277 -4.1077871 -3.9977827 -3.7559962 -3.6088645 -3.7479336 -4.0654473 -4.3461809 -4.4534569 -4.4080181 -4.344655 -4.359879][-4.3595028 -4.2651734 -4.1783557 -4.1205091 -4.0550237 -3.895709 -3.6125126 -3.416038 -3.5376503 -3.8972757 -4.2606058 -4.43601 -4.4055262 -4.3303781 -4.340209][-4.4566669 -4.3918452 -4.3009005 -4.2010441 -4.0904841 -3.9122636 -3.6272273 -3.4160852 -3.5143542 -3.864398 -4.231184 -4.4117184 -4.373683 -4.293601 -4.3092561][-4.5033846 -4.4595 -4.3792238 -4.2837691 -4.1759496 -4.0141292 -3.7669556 -3.57851 -3.6623089 -3.9699466 -4.2746658 -4.4005833 -4.3422389 -4.2708898 -4.2977142][-4.5086784 -4.4811549 -4.4259315 -4.3669567 -4.2929449 -4.1651878 -3.974875 -3.8312378 -3.902988 -4.1446552 -4.3535986 -4.4028125 -4.3284526 -4.2777267 -4.3128977][-4.5016155 -4.4883685 -4.4621429 -4.4449415 -4.4135861 -4.3285923 -4.2002859 -4.104136 -4.1542821 -4.3100924 -4.4185266 -4.4062862 -4.3311915 -4.2998009 -4.3364177][-4.4867291 -4.4882636 -4.4909139 -4.5071859 -4.5109868 -4.4721646 -4.4030948 -4.3494143 -4.3752732 -4.4479108 -4.47739 -4.4330034 -4.3661108 -4.3445477 -4.3742652][-4.4470477 -4.4622684 -4.4892116 -4.5295382 -4.5569186 -4.5524821 -4.5278244 -4.509645 -4.5260959 -4.5502253 -4.541266 -4.4929185 -4.4386806 -4.4186606 -4.4352655][-4.3734388 -4.3915272 -4.4359455 -4.4926968 -4.5354996 -4.5534344 -4.5586209 -4.5655227 -4.5814257 -4.5859213 -4.5670271 -4.53136 -4.4964814 -4.4807363 -4.4857378]]...]
INFO - root - 2017-12-07 12:16:27.560053: step 15310, loss = 21.60, batch loss = 21.52 (7.9 examples/sec; 1.009 sec/batch; 88h:55m:02s remains)
INFO - root - 2017-12-07 12:16:36.877345: step 15320, loss = 21.85, batch loss = 21.77 (8.7 examples/sec; 0.918 sec/batch; 80h:55m:19s remains)
INFO - root - 2017-12-07 12:16:46.323205: step 15330, loss = 20.87, batch loss = 20.79 (8.6 examples/sec; 0.927 sec/batch; 81h:39m:06s remains)
INFO - root - 2017-12-07 12:16:55.830188: step 15340, loss = 21.22, batch loss = 21.14 (8.8 examples/sec; 0.906 sec/batch; 79h:47m:15s remains)
INFO - root - 2017-12-07 12:17:05.164852: step 15350, loss = 21.80, batch loss = 21.71 (8.2 examples/sec; 0.972 sec/batch; 85h:36m:47s remains)
INFO - root - 2017-12-07 12:17:14.591908: step 15360, loss = 21.37, batch loss = 21.28 (8.0 examples/sec; 1.001 sec/batch; 88h:08m:39s remains)
INFO - root - 2017-12-07 12:17:23.922212: step 15370, loss = 21.54, batch loss = 21.46 (8.2 examples/sec; 0.974 sec/batch; 85h:48m:48s remains)
INFO - root - 2017-12-07 12:17:33.207813: step 15380, loss = 21.74, batch loss = 21.66 (8.5 examples/sec; 0.945 sec/batch; 83h:16m:45s remains)
INFO - root - 2017-12-07 12:17:42.415047: step 15390, loss = 21.80, batch loss = 21.72 (8.5 examples/sec; 0.941 sec/batch; 82h:52m:27s remains)
INFO - root - 2017-12-07 12:17:51.829686: step 15400, loss = 21.73, batch loss = 21.64 (8.6 examples/sec; 0.925 sec/batch; 81h:28m:49s remains)
2017-12-07 12:17:52.723286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3423305 -4.337491 -4.3407307 -4.3476229 -4.35756 -4.3672781 -4.3748527 -4.3795576 -4.3812051 -4.381834 -4.3826795 -4.3820405 -4.3768725 -4.3650122 -4.3487668][-4.364243 -4.3614016 -4.3721457 -4.3894196 -4.4073715 -4.41953 -4.4242744 -4.424695 -4.4240518 -4.4243131 -4.4236054 -4.4174824 -4.4064689 -4.3904366 -4.3712053][-4.3920131 -4.3976765 -4.4208112 -4.4491787 -4.4694843 -4.474268 -4.4665575 -4.4572172 -4.4538188 -4.4547582 -4.4540043 -4.4447412 -4.4311986 -4.4149323 -4.3954177][-4.4025755 -4.42054 -4.4561481 -4.4883819 -4.4964232 -4.4752569 -4.4388909 -4.4131479 -4.4084368 -4.4135385 -4.4184613 -4.4159045 -4.4112382 -4.4053578 -4.3939466][-4.3765635 -4.4028916 -4.4433208 -4.4676785 -4.4496546 -4.3868155 -4.3105211 -4.2688432 -4.2715306 -4.2899103 -4.3098941 -4.3244777 -4.3372574 -4.3488259 -4.35351][-4.3112135 -4.3364573 -4.36954 -4.3755903 -4.3248148 -4.2135429 -4.0942249 -4.0427008 -4.0660028 -4.1138768 -4.16268 -4.2015958 -4.230792 -4.2542977 -4.2729063][-4.2099147 -4.2219582 -4.2400675 -4.22648 -4.1474333 -3.9935129 -3.8376544 -3.7844057 -3.837255 -3.926578 -4.014811 -4.0790367 -4.1160245 -4.1392775 -4.1613083][-4.1238766 -4.1220374 -4.1315923 -4.1112242 -4.0198188 -3.8426762 -3.6687264 -3.6231406 -3.6993504 -3.8174813 -3.9361625 -4.0141196 -4.0444932 -4.0523129 -4.0626287][-4.0849423 -4.0759511 -4.0896711 -4.0800557 -3.9962633 -3.8223128 -3.6542203 -3.6198063 -3.6982884 -3.8141382 -3.9358797 -4.011682 -4.0266962 -4.0109882 -3.9965117][-4.0881195 -4.081831 -4.1123495 -4.1288595 -4.0681906 -3.9156806 -3.7681088 -3.7467611 -3.8192587 -3.9141669 -4.0158916 -4.0725064 -4.066359 -4.0266323 -3.9845147][-4.1288586 -4.1382208 -4.1957245 -4.2500834 -4.2251315 -4.1063929 -3.9854975 -3.9726241 -4.0358853 -4.10879 -4.1800804 -4.2050385 -4.1703105 -4.1042533 -4.0335259][-4.1857672 -4.2098413 -4.2882314 -4.376987 -4.39005 -4.3102827 -4.2191448 -4.2044644 -4.2453771 -4.2931838 -4.3351545 -4.3375092 -4.28847 -4.2100129 -4.1219196][-4.2193809 -4.2452621 -4.3293352 -4.4416347 -4.4898129 -4.4520974 -4.3934674 -4.3704548 -4.3768835 -4.3935342 -4.4119997 -4.40695 -4.3658824 -4.2969561 -4.2116904][-4.22396 -4.2314372 -4.3006272 -4.4191656 -4.4949322 -4.495225 -4.4655771 -4.4346094 -4.4062696 -4.3943715 -4.4023814 -4.406 -4.38644 -4.3405576 -4.2733541][-4.1945987 -4.1696687 -4.2080946 -4.3181915 -4.4141331 -4.4457803 -4.4408393 -4.4106812 -4.3649263 -4.3416343 -4.3562989 -4.381207 -4.3882356 -4.3667946 -4.3215652]]...]
INFO - root - 2017-12-07 12:18:02.057345: step 15410, loss = 21.49, batch loss = 21.41 (8.6 examples/sec; 0.934 sec/batch; 82h:18m:34s remains)
INFO - root - 2017-12-07 12:18:11.507996: step 15420, loss = 20.95, batch loss = 20.87 (8.8 examples/sec; 0.905 sec/batch; 79h:42m:22s remains)
INFO - root - 2017-12-07 12:18:21.079641: step 15430, loss = 21.68, batch loss = 21.60 (8.3 examples/sec; 0.960 sec/batch; 84h:33m:40s remains)
INFO - root - 2017-12-07 12:18:30.590903: step 15440, loss = 21.39, batch loss = 21.31 (8.1 examples/sec; 0.989 sec/batch; 87h:05m:02s remains)
INFO - root - 2017-12-07 12:18:40.134401: step 15450, loss = 21.13, batch loss = 21.05 (7.9 examples/sec; 1.015 sec/batch; 89h:24m:34s remains)
INFO - root - 2017-12-07 12:18:49.689719: step 15460, loss = 21.68, batch loss = 21.59 (8.2 examples/sec; 0.973 sec/batch; 85h:40m:51s remains)
INFO - root - 2017-12-07 12:18:58.977477: step 15470, loss = 21.68, batch loss = 21.60 (8.5 examples/sec; 0.941 sec/batch; 82h:52m:37s remains)
INFO - root - 2017-12-07 12:19:08.382708: step 15480, loss = 21.44, batch loss = 21.35 (7.8 examples/sec; 1.030 sec/batch; 90h:43m:54s remains)
INFO - root - 2017-12-07 12:19:17.503322: step 15490, loss = 21.59, batch loss = 21.51 (9.7 examples/sec; 0.821 sec/batch; 72h:16m:45s remains)
INFO - root - 2017-12-07 12:19:26.858319: step 15500, loss = 21.60, batch loss = 21.52 (8.3 examples/sec; 0.961 sec/batch; 84h:36m:22s remains)
2017-12-07 12:19:27.798219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3554916 -4.3752613 -4.4047365 -4.4662151 -4.5166531 -4.5461111 -4.572679 -4.5515084 -4.5249186 -4.5343723 -4.5560617 -4.5588508 -4.566011 -4.586556 -4.548511][-4.3050914 -4.3256831 -4.3590674 -4.4330239 -4.510221 -4.5662518 -4.603044 -4.5895014 -4.5734897 -4.601027 -4.6340413 -4.6377506 -4.6290822 -4.6089587 -4.5232382][-4.3150125 -4.349021 -4.4025292 -4.4793892 -4.552371 -4.5875053 -4.5931449 -4.5707436 -4.5753374 -4.640028 -4.7018576 -4.7148647 -4.6853242 -4.6133471 -4.4860058][-4.3336725 -4.4002881 -4.4798069 -4.5315094 -4.5495725 -4.5082459 -4.4417448 -4.3976502 -4.4366221 -4.5666389 -4.6822343 -4.7150817 -4.6758337 -4.5712318 -4.4308429][-4.405251 -4.4950266 -4.5645986 -4.5496349 -4.4736128 -4.3312683 -4.1881542 -4.1268911 -4.2044687 -4.4048676 -4.5838389 -4.6417785 -4.6112657 -4.5145693 -4.3989449][-4.5241432 -4.5954037 -4.6047187 -4.4961939 -4.3204279 -4.0891008 -3.9003806 -3.8576589 -3.9762788 -4.225101 -4.45385 -4.5291152 -4.5056009 -4.4423318 -4.3853912][-4.6087842 -4.6223378 -4.5528011 -4.3762174 -4.1413369 -3.8610947 -3.6620164 -3.6676526 -3.8218052 -4.0787349 -4.3213458 -4.4076862 -4.3959694 -4.3824449 -4.4000921][-4.660975 -4.6130018 -4.4844041 -4.2825184 -4.0300179 -3.7309656 -3.5352132 -3.5896132 -3.7679474 -4.0054088 -4.2395859 -4.3472962 -4.368504 -4.4036279 -4.4705968][-4.6442943 -4.5540786 -4.4009228 -4.2153769 -3.9929943 -3.7225893 -3.5498171 -3.6276784 -3.8126304 -4.0247469 -4.242208 -4.3767109 -4.4423451 -4.5107183 -4.5823507][-4.5736332 -4.4604764 -4.312871 -4.1682897 -4.0127635 -3.8226507 -3.7009282 -3.7760587 -3.9416909 -4.128974 -4.3262506 -4.4670424 -4.5516696 -4.6184683 -4.6592836][-4.5161119 -4.3967361 -4.2598634 -4.1503062 -4.0618963 -3.9717615 -3.9292579 -4.0101771 -4.1454906 -4.2948465 -4.443737 -4.5422406 -4.5957465 -4.629405 -4.6342669][-4.4572353 -4.3359489 -4.2105508 -4.1310863 -4.1011772 -4.1045852 -4.1510873 -4.2537456 -4.3594828 -4.4565582 -4.5365996 -4.563787 -4.5528703 -4.5446157 -4.5292559][-4.3859086 -4.2648706 -4.1470237 -4.0918193 -4.1081443 -4.1841593 -4.2975392 -4.4118218 -4.4887619 -4.5350118 -4.5567141 -4.5247526 -4.4598503 -4.4259791 -4.4135928][-4.3322477 -4.2122521 -4.1036496 -4.0713577 -4.1221642 -4.2402005 -4.3745351 -4.4698143 -4.5184007 -4.5372009 -4.5327449 -4.4720364 -4.3793707 -4.3369737 -4.3329883][-4.3462224 -4.2443738 -4.1630964 -4.1558189 -4.2196 -4.3332386 -4.4321451 -4.4809594 -4.5039406 -4.5156264 -4.5095549 -4.443604 -4.34665 -4.3053541 -4.304625]]...]
INFO - root - 2017-12-07 12:19:37.283199: step 15510, loss = 21.60, batch loss = 21.51 (8.4 examples/sec; 0.950 sec/batch; 83h:41m:29s remains)
INFO - root - 2017-12-07 12:19:46.609276: step 15520, loss = 21.43, batch loss = 21.35 (8.4 examples/sec; 0.948 sec/batch; 83h:29m:02s remains)
INFO - root - 2017-12-07 12:19:56.052449: step 15530, loss = 21.29, batch loss = 21.21 (8.5 examples/sec; 0.936 sec/batch; 82h:26m:40s remains)
INFO - root - 2017-12-07 12:20:05.417708: step 15540, loss = 21.44, batch loss = 21.36 (8.3 examples/sec; 0.959 sec/batch; 84h:24m:54s remains)
INFO - root - 2017-12-07 12:20:14.805849: step 15550, loss = 21.40, batch loss = 21.32 (8.6 examples/sec; 0.927 sec/batch; 81h:38m:03s remains)
INFO - root - 2017-12-07 12:20:24.304334: step 15560, loss = 21.54, batch loss = 21.45 (8.7 examples/sec; 0.917 sec/batch; 80h:42m:17s remains)
INFO - root - 2017-12-07 12:20:33.640482: step 15570, loss = 21.50, batch loss = 21.42 (9.1 examples/sec; 0.876 sec/batch; 77h:06m:37s remains)
INFO - root - 2017-12-07 12:20:42.975213: step 15580, loss = 21.45, batch loss = 21.36 (8.5 examples/sec; 0.941 sec/batch; 82h:50m:36s remains)
INFO - root - 2017-12-07 12:20:52.437140: step 15590, loss = 21.88, batch loss = 21.80 (8.2 examples/sec; 0.976 sec/batch; 85h:53m:57s remains)
INFO - root - 2017-12-07 12:21:01.676477: step 15600, loss = 20.91, batch loss = 20.82 (8.0 examples/sec; 0.997 sec/batch; 87h:44m:24s remains)
2017-12-07 12:21:02.565264: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1959743 -4.3228869 -4.4419565 -4.5112581 -4.5129728 -4.4685493 -4.403007 -4.3463683 -4.3119626 -4.2902975 -4.2809467 -4.2903104 -4.3125038 -4.3328848 -4.3293061][-4.1395884 -4.2766528 -4.4238477 -4.5157256 -4.5160527 -4.454308 -4.37054 -4.3113041 -4.2880611 -4.2798719 -4.2762623 -4.2819018 -4.3016992 -4.3322158 -4.3483329][-4.1610847 -4.2808247 -4.4116874 -4.4854383 -4.4631014 -4.3821125 -4.2962132 -4.2584162 -4.2641425 -4.2747188 -4.2752686 -4.2764244 -4.2991157 -4.3462896 -4.3800488][-4.2493615 -4.3394122 -4.4199204 -4.4323678 -4.356369 -4.2514119 -4.1863742 -4.1955304 -4.243484 -4.2751875 -4.2818041 -4.2869945 -4.3174553 -4.3690548 -4.4066715][-4.3557725 -4.4045534 -4.4236636 -4.3629532 -4.2264094 -4.0994062 -4.06112 -4.1173944 -4.2075248 -4.2738228 -4.30878 -4.3331428 -4.3627996 -4.3902097 -4.4029579][-4.4339647 -4.4400225 -4.3997507 -4.2767587 -4.0935025 -3.9500957 -3.9253747 -4.0115147 -4.1459374 -4.2732816 -4.3622627 -4.4068704 -4.4172869 -4.4011383 -4.3787689][-4.4668984 -4.4449282 -4.3624287 -4.1983433 -3.9868813 -3.8265302 -3.7964609 -3.8992472 -4.0870814 -4.2914171 -4.4323926 -4.4737334 -4.444571 -4.388833 -4.3484249][-4.4466438 -4.4136567 -4.3204846 -4.1511679 -3.9362204 -3.7599897 -3.7094169 -3.8137863 -4.044354 -4.3083978 -4.477283 -4.4975281 -4.4257631 -4.3435092 -4.3036637][-4.4003177 -4.3620477 -4.2823372 -4.144855 -3.9593911 -3.7865984 -3.7168391 -3.8036685 -4.0319085 -4.3001876 -4.4593344 -4.4533663 -4.35247 -4.2571526 -4.227757][-4.3671169 -4.3242941 -4.2678242 -4.1806278 -4.0471411 -3.9041553 -3.8353441 -3.8962009 -4.07784 -4.295527 -4.413094 -4.3765454 -4.2530112 -4.1418409 -4.112802][-4.3631029 -4.3251767 -4.2937775 -4.2521005 -4.1701932 -4.070097 -4.0152779 -4.0500774 -4.1718407 -4.3230338 -4.3915644 -4.329855 -4.1912575 -4.061552 -4.0143924][-4.4019117 -4.3812695 -4.3744655 -4.361269 -4.3163238 -4.2528219 -4.2101665 -4.2202616 -4.2881088 -4.3840728 -4.4167929 -4.3461971 -4.2143812 -4.086709 -4.0265522][-4.4877448 -4.4869223 -4.4959497 -4.4920444 -4.4650216 -4.4272237 -4.3979349 -4.3953819 -4.4279013 -4.4852095 -4.494905 -4.4289927 -4.3246522 -4.2246685 -4.1661248][-4.596673 -4.6115561 -4.6172404 -4.6067939 -4.5887194 -4.5719094 -4.5580759 -4.5525894 -4.5619245 -4.5878053 -4.5795341 -4.52227 -4.4558644 -4.4024224 -4.3616972][-4.6531811 -4.6802258 -4.6709585 -4.651135 -4.6438303 -4.6503396 -4.65571 -4.6545205 -4.6503963 -4.6502123 -4.6258183 -4.574378 -4.5406833 -4.5303125 -4.5065827]]...]
INFO - root - 2017-12-07 12:21:12.039131: step 15610, loss = 21.61, batch loss = 21.53 (8.5 examples/sec; 0.946 sec/batch; 83h:17m:25s remains)
INFO - root - 2017-12-07 12:21:21.439847: step 15620, loss = 21.37, batch loss = 21.29 (8.1 examples/sec; 0.992 sec/batch; 87h:20m:23s remains)
INFO - root - 2017-12-07 12:21:30.740116: step 15630, loss = 21.47, batch loss = 21.38 (9.0 examples/sec; 0.887 sec/batch; 78h:06m:21s remains)
INFO - root - 2017-12-07 12:21:40.123503: step 15640, loss = 21.34, batch loss = 21.26 (9.1 examples/sec; 0.877 sec/batch; 77h:13m:07s remains)
INFO - root - 2017-12-07 12:21:49.564834: step 15650, loss = 21.77, batch loss = 21.68 (8.9 examples/sec; 0.898 sec/batch; 79h:01m:49s remains)
INFO - root - 2017-12-07 12:21:58.941927: step 15660, loss = 21.52, batch loss = 21.43 (8.3 examples/sec; 0.961 sec/batch; 84h:33m:29s remains)
INFO - root - 2017-12-07 12:22:08.235543: step 15670, loss = 21.38, batch loss = 21.30 (8.9 examples/sec; 0.901 sec/batch; 79h:19m:51s remains)
INFO - root - 2017-12-07 12:22:17.615932: step 15680, loss = 21.29, batch loss = 21.21 (8.3 examples/sec; 0.959 sec/batch; 84h:25m:43s remains)
INFO - root - 2017-12-07 12:22:26.904381: step 15690, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.970 sec/batch; 85h:21m:47s remains)
INFO - root - 2017-12-07 12:22:36.358590: step 15700, loss = 21.49, batch loss = 21.40 (8.5 examples/sec; 0.944 sec/batch; 83h:04m:05s remains)
2017-12-07 12:22:37.295376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6327777 -4.6725197 -4.7168922 -4.7363715 -4.7241988 -4.7033854 -4.7062793 -4.728466 -4.744966 -4.7644224 -4.7967858 -4.8185434 -4.8056889 -4.7481513 -4.6729751][-4.6290884 -4.6574183 -4.6938362 -4.7110629 -4.6910882 -4.66011 -4.6633582 -4.6915517 -4.7181425 -4.7490244 -4.7859678 -4.8092542 -4.8032804 -4.757483 -4.6922355][-4.5798869 -4.610085 -4.65832 -4.68417 -4.6474161 -4.5848603 -4.569437 -4.6038623 -4.6581759 -4.7094374 -4.7406793 -4.7613087 -4.7750807 -4.7576647 -4.7121582][-4.5257945 -4.5627351 -4.6354141 -4.6695971 -4.6027851 -4.4908442 -4.4470592 -4.4926314 -4.582963 -4.6532245 -4.67276 -4.6921859 -4.7327604 -4.7485976 -4.7258863][-4.4688854 -4.5059772 -4.5988522 -4.6298409 -4.5188909 -4.349194 -4.2769222 -4.3378892 -4.4685965 -4.569324 -4.5895443 -4.6056075 -4.6526241 -4.6795168 -4.67866][-4.4121585 -4.4365149 -4.5362082 -4.5486569 -4.3867021 -4.16191 -4.0593524 -4.1180167 -4.2757826 -4.4213629 -4.4740791 -4.4954476 -4.5298023 -4.5460715 -4.565948][-4.3726583 -4.3888488 -4.4798279 -4.4572277 -4.242979 -3.9681926 -3.8249393 -3.8565073 -4.0249252 -4.2051415 -4.2933679 -4.3309741 -4.3674173 -4.3867941 -4.4355922][-4.3696008 -4.3846521 -4.4551125 -4.3960915 -4.1482511 -3.8403816 -3.6529047 -3.6520355 -3.8160853 -3.996887 -4.0857415 -4.1329403 -4.1932411 -4.2493205 -4.3407521][-4.408205 -4.4344325 -4.4911485 -4.4208188 -4.1839404 -3.8913321 -3.6931987 -3.6687336 -3.8001318 -3.9324677 -3.9830835 -4.0245156 -4.1053562 -4.200295 -4.3204966][-4.4623904 -4.5039935 -4.5611367 -4.5105271 -4.3261042 -4.0939612 -3.9282186 -3.8945391 -3.9655879 -4.0207028 -4.0291247 -4.069315 -4.1654596 -4.2752314 -4.3890047][-4.487432 -4.5252371 -4.5912333 -4.5825081 -4.467679 -4.313849 -4.2023 -4.1684976 -4.1776676 -4.1652308 -4.1499205 -4.2034793 -4.3166461 -4.4230165 -4.5066504][-4.4639244 -4.4940677 -4.5732188 -4.610342 -4.5657954 -4.4876552 -4.4357891 -4.4169803 -4.3943624 -4.3468409 -4.3215842 -4.3821945 -4.4977174 -4.5830989 -4.623301][-4.4283414 -4.4742875 -4.5722642 -4.6377778 -4.6327057 -4.5989618 -4.58266 -4.5831428 -4.5659637 -4.5214763 -4.4977713 -4.5483818 -4.6433258 -4.69853 -4.6967282][-4.4259872 -4.5016227 -4.6121216 -4.6785269 -4.6752806 -4.6402745 -4.6192961 -4.6230769 -4.6224728 -4.6023111 -4.588635 -4.6263809 -4.7007189 -4.737071 -4.7113543][-4.4374123 -4.5198669 -4.6196775 -4.669373 -4.6535807 -4.6049905 -4.571701 -4.5727506 -4.5856028 -4.5888886 -4.5905881 -4.6230307 -4.6798387 -4.7014961 -4.6648641]]...]
INFO - root - 2017-12-07 12:22:46.777642: step 15710, loss = 21.12, batch loss = 21.04 (8.1 examples/sec; 0.982 sec/batch; 86h:23m:08s remains)
INFO - root - 2017-12-07 12:22:56.076294: step 15720, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.911 sec/batch; 80h:10m:01s remains)
INFO - root - 2017-12-07 12:23:05.587652: step 15730, loss = 21.23, batch loss = 21.14 (8.4 examples/sec; 0.956 sec/batch; 84h:04m:37s remains)
INFO - root - 2017-12-07 12:23:14.934383: step 15740, loss = 21.07, batch loss = 20.98 (8.3 examples/sec; 0.958 sec/batch; 84h:18m:16s remains)
INFO - root - 2017-12-07 12:23:24.297638: step 15750, loss = 21.18, batch loss = 21.09 (8.5 examples/sec; 0.942 sec/batch; 82h:50m:44s remains)
INFO - root - 2017-12-07 12:23:33.693173: step 15760, loss = 21.27, batch loss = 21.19 (8.9 examples/sec; 0.897 sec/batch; 78h:53m:48s remains)
INFO - root - 2017-12-07 12:23:42.981262: step 15770, loss = 21.04, batch loss = 20.96 (9.2 examples/sec; 0.873 sec/batch; 76h:47m:57s remains)
INFO - root - 2017-12-07 12:23:52.434244: step 15780, loss = 21.28, batch loss = 21.20 (8.6 examples/sec; 0.931 sec/batch; 81h:54m:06s remains)
INFO - root - 2017-12-07 12:24:01.689939: step 15790, loss = 21.87, batch loss = 21.79 (8.7 examples/sec; 0.922 sec/batch; 81h:05m:50s remains)
INFO - root - 2017-12-07 12:24:11.083242: step 15800, loss = 21.54, batch loss = 21.45 (9.1 examples/sec; 0.879 sec/batch; 77h:22m:04s remains)
2017-12-07 12:24:12.055268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3443923 -4.2957363 -4.3128633 -4.3801074 -4.4181123 -4.3949919 -4.3551173 -4.3067622 -4.238152 -4.1612911 -4.1213813 -4.144269 -4.2125688 -4.2825403 -4.320909][-4.3641171 -4.3436146 -4.3923478 -4.4824471 -4.5352988 -4.5173244 -4.4590154 -4.3884273 -4.3114758 -4.2418771 -4.2173791 -4.2612643 -4.3511567 -4.427381 -4.4491839][-4.3888702 -4.3855534 -4.4372778 -4.5156269 -4.5650592 -4.55066 -4.4823828 -4.4063849 -4.3429909 -4.2993603 -4.2977777 -4.3592486 -4.4594855 -4.5373039 -4.5517521][-4.40235 -4.3934507 -4.416698 -4.4556565 -4.4852958 -4.46192 -4.3795433 -4.3050494 -4.2727213 -4.2737846 -4.3089681 -4.3897963 -4.4938412 -4.5695682 -4.5874228][-4.3842058 -4.3678074 -4.3527489 -4.3410583 -4.3289723 -4.2723269 -4.1685996 -4.106854 -4.1255665 -4.1883159 -4.2681446 -4.3624396 -4.4540696 -4.5099616 -4.5255837][-4.3612552 -4.3479462 -4.30334 -4.2462287 -4.1798182 -4.0714712 -3.9413056 -3.9034913 -3.9825168 -4.103322 -4.2174158 -4.3122935 -4.3796926 -4.40221 -4.4031014][-4.3511324 -4.3296237 -4.2593083 -4.1674948 -4.0556769 -3.9061768 -3.7678716 -3.7706614 -3.9082639 -4.0701423 -4.20068 -4.2866759 -4.3281059 -4.319346 -4.307518][-4.3420606 -4.299613 -4.2154188 -4.1179047 -4.0009403 -3.8529594 -3.732645 -3.7683523 -3.9289894 -4.0928416 -4.2110009 -4.2787886 -4.2998185 -4.2808189 -4.280591][-4.351203 -4.2881112 -4.2008519 -4.1190863 -4.030365 -3.913624 -3.8214583 -3.8662806 -4.0121379 -4.1470389 -4.2342672 -4.2815104 -4.2952867 -4.2899828 -4.3209014][-4.3791327 -4.3102355 -4.2296824 -4.1699204 -4.1162791 -4.0382442 -3.973165 -4.0157113 -4.1303196 -4.2229376 -4.2717676 -4.3021555 -4.3209729 -4.3395486 -4.396841][-4.3929796 -4.3351364 -4.277555 -4.2477226 -4.2274384 -4.1809912 -4.1344938 -4.1696734 -4.2489834 -4.2964339 -4.3061733 -4.31757 -4.3369141 -4.3639259 -4.4260316][-4.4127035 -4.3726163 -4.3442287 -4.3421769 -4.3385167 -4.2976866 -4.2489562 -4.2647333 -4.3070536 -4.3182759 -4.3055019 -4.3093405 -4.3313694 -4.36027 -4.4125338][-4.452601 -4.42542 -4.41719 -4.4291339 -4.4240303 -4.3713145 -4.3134522 -4.312727 -4.3296938 -4.3222914 -4.3066506 -4.3141026 -4.3414454 -4.3654227 -4.3969216][-4.4958129 -4.4600954 -4.4465418 -4.4516916 -4.4365668 -4.378757 -4.3249788 -4.3226237 -4.3326416 -4.3263383 -4.3242788 -4.3433862 -4.3747463 -4.3898916 -4.3964968][-4.5176635 -4.4685044 -4.4362087 -4.4184909 -4.3874173 -4.3321033 -4.2948089 -4.3024106 -4.3178377 -4.3206215 -4.33198 -4.359921 -4.3951979 -4.4105906 -4.40601]]...]
INFO - root - 2017-12-07 12:24:21.445411: step 15810, loss = 21.79, batch loss = 21.71 (8.6 examples/sec; 0.929 sec/batch; 81h:41m:14s remains)
INFO - root - 2017-12-07 12:24:30.816193: step 15820, loss = 21.05, batch loss = 20.97 (9.2 examples/sec; 0.870 sec/batch; 76h:34m:17s remains)
INFO - root - 2017-12-07 12:24:40.207162: step 15830, loss = 21.29, batch loss = 21.21 (8.4 examples/sec; 0.957 sec/batch; 84h:11m:01s remains)
INFO - root - 2017-12-07 12:24:49.508938: step 15840, loss = 21.39, batch loss = 21.31 (8.8 examples/sec; 0.907 sec/batch; 79h:48m:34s remains)
INFO - root - 2017-12-07 12:24:58.826822: step 15850, loss = 21.36, batch loss = 21.28 (8.3 examples/sec; 0.968 sec/batch; 85h:09m:05s remains)
INFO - root - 2017-12-07 12:25:08.110708: step 15860, loss = 21.33, batch loss = 21.25 (8.7 examples/sec; 0.920 sec/batch; 80h:54m:14s remains)
INFO - root - 2017-12-07 12:25:17.614359: step 15870, loss = 21.39, batch loss = 21.30 (8.3 examples/sec; 0.964 sec/batch; 84h:48m:13s remains)
INFO - root - 2017-12-07 12:25:26.664106: step 15880, loss = 21.61, batch loss = 21.53 (8.7 examples/sec; 0.921 sec/batch; 81h:00m:34s remains)
INFO - root - 2017-12-07 12:25:36.114946: step 15890, loss = 21.57, batch loss = 21.49 (8.9 examples/sec; 0.903 sec/batch; 79h:24m:47s remains)
INFO - root - 2017-12-07 12:25:45.684184: step 15900, loss = 21.51, batch loss = 21.42 (8.7 examples/sec; 0.917 sec/batch; 80h:36m:44s remains)
2017-12-07 12:25:46.529819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4546247 -4.4397197 -4.4290819 -4.425024 -4.4300427 -4.4402852 -4.4538608 -4.4593258 -4.4485917 -4.4357967 -4.4403563 -4.4740963 -4.5206013 -4.5569754 -4.5647168][-4.51438 -4.530683 -4.5386591 -4.5343404 -4.5353832 -4.5458736 -4.5668583 -4.5775323 -4.5595541 -4.5245323 -4.5107284 -4.5416565 -4.5983291 -4.6507549 -4.66539][-4.54414 -4.5936007 -4.6147065 -4.5972481 -4.5808678 -4.5834541 -4.6099405 -4.6328473 -4.622663 -4.5795016 -4.5525889 -4.5840945 -4.6568151 -4.728323 -4.75037][-4.5298147 -4.5964122 -4.6158228 -4.5763478 -4.5354271 -4.5190549 -4.5376554 -4.5692019 -4.5897403 -4.57745 -4.5714703 -4.6209264 -4.7130718 -4.8037195 -4.8329315][-4.4690537 -4.5295539 -4.5317297 -4.47058 -4.4080596 -4.3642554 -4.3558073 -4.3873043 -4.4542251 -4.5103278 -4.5600328 -4.6365085 -4.7331548 -4.8275223 -4.8671846][-4.3843293 -4.4240489 -4.3989544 -4.3155246 -4.2315331 -4.15501 -4.109385 -4.1326475 -4.2466984 -4.3850102 -4.503231 -4.6015191 -4.6789312 -4.7539234 -4.8026767][-4.30452 -4.321723 -4.2741756 -4.1713166 -4.0608296 -3.9424243 -3.8517461 -3.8565738 -4.0009813 -4.1983185 -4.3702497 -4.4853358 -4.5420465 -4.5947437 -4.6538172][-4.2498589 -4.2595248 -4.2009244 -4.079668 -3.9395041 -3.7835033 -3.653439 -3.6337593 -3.7776814 -3.989177 -4.1820903 -4.310791 -4.3694787 -4.4268632 -4.5074062][-4.2541876 -4.2832675 -4.2361326 -4.1067109 -3.9492033 -3.7743618 -3.6233172 -3.5756412 -3.6803586 -3.857326 -4.0332975 -4.1653128 -4.2454238 -4.3341537 -4.443954][-4.3145361 -4.3888454 -4.3825727 -4.2690582 -4.1173677 -3.946914 -3.7951112 -3.7242339 -3.771955 -3.8908441 -4.0240474 -4.1363745 -4.2279162 -4.3385277 -4.4601264][-4.3819242 -4.5137062 -4.5627413 -4.4837871 -4.355588 -4.2068262 -4.0709147 -3.9917209 -3.9973783 -4.0656514 -4.1486664 -4.2262297 -4.3104234 -4.4146142 -4.51611][-4.4115934 -4.5832896 -4.6834469 -4.6475911 -4.5534186 -4.4398561 -4.3354554 -4.2702394 -4.263092 -4.3015 -4.3439994 -4.3879728 -4.4530787 -4.5285921 -4.5890403][-4.3932161 -4.559082 -4.6830993 -4.6908131 -4.6421442 -4.5767903 -4.5172195 -4.4811211 -4.4824696 -4.5099559 -4.5295181 -4.5510941 -4.5956674 -4.6403809 -4.6624403][-4.3683462 -4.4910159 -4.6058912 -4.643084 -4.6346884 -4.612453 -4.5906749 -4.5796108 -4.5909362 -4.6150775 -4.6291265 -4.6480303 -4.6890478 -4.7246256 -4.7286425][-4.3808184 -4.4492579 -4.5302396 -4.5725365 -4.5853767 -4.5873928 -4.5833683 -4.5815525 -4.5942335 -4.6145716 -4.6297793 -4.65626 -4.705111 -4.7447371 -4.7453]]...]
INFO - root - 2017-12-07 12:25:55.978853: step 15910, loss = 21.46, batch loss = 21.38 (8.8 examples/sec; 0.912 sec/batch; 80h:09m:40s remains)
INFO - root - 2017-12-07 12:26:05.344437: step 15920, loss = 21.44, batch loss = 21.36 (8.7 examples/sec; 0.915 sec/batch; 80h:29m:54s remains)
INFO - root - 2017-12-07 12:26:14.850239: step 15930, loss = 21.16, batch loss = 21.08 (8.8 examples/sec; 0.908 sec/batch; 79h:49m:37s remains)
INFO - root - 2017-12-07 12:26:24.319213: step 15940, loss = 21.64, batch loss = 21.56 (8.6 examples/sec; 0.935 sec/batch; 82h:14m:49s remains)
INFO - root - 2017-12-07 12:26:33.751753: step 15950, loss = 21.15, batch loss = 21.06 (8.2 examples/sec; 0.974 sec/batch; 85h:40m:26s remains)
INFO - root - 2017-12-07 12:26:43.139000: step 15960, loss = 21.37, batch loss = 21.28 (8.3 examples/sec; 0.969 sec/batch; 85h:10m:55s remains)
INFO - root - 2017-12-07 12:26:52.554567: step 15970, loss = 21.71, batch loss = 21.62 (8.2 examples/sec; 0.975 sec/batch; 85h:43m:28s remains)
INFO - root - 2017-12-07 12:27:01.884790: step 15980, loss = 21.17, batch loss = 21.08 (8.5 examples/sec; 0.936 sec/batch; 82h:20m:11s remains)
INFO - root - 2017-12-07 12:27:11.274415: step 15990, loss = 21.62, batch loss = 21.54 (8.3 examples/sec; 0.962 sec/batch; 84h:37m:03s remains)
INFO - root - 2017-12-07 12:27:20.563572: step 16000, loss = 21.41, batch loss = 21.33 (8.5 examples/sec; 0.936 sec/batch; 82h:16m:06s remains)
2017-12-07 12:27:21.456289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5189352 -4.5810051 -4.5966039 -4.5326385 -4.40277 -4.2764473 -4.22802 -4.2810445 -4.3871875 -4.4603996 -4.47865 -4.4637713 -4.46152 -4.4717374 -4.42247][-4.5347495 -4.592195 -4.5919862 -4.5021544 -4.3428445 -4.1936245 -4.1354551 -4.1958365 -4.3227282 -4.4215727 -4.4569526 -4.4628491 -4.4772167 -4.4783783 -4.3968263][-4.5835266 -4.6504645 -4.6345882 -4.5116258 -4.3225794 -4.1555939 -4.0841184 -4.1347346 -4.2613435 -4.3706026 -4.4287786 -4.4688778 -4.4932938 -4.4606533 -4.3240895][-4.6475463 -4.7154069 -4.6735539 -4.5109472 -4.2909923 -4.10548 -4.0117278 -4.0348258 -4.14339 -4.258163 -4.3579812 -4.4535637 -4.4955845 -4.4320517 -4.2455425][-4.6731024 -4.7253828 -4.6491504 -4.450458 -4.206677 -4.0069866 -3.8898077 -3.8798842 -3.9683571 -4.0951271 -4.2492819 -4.404007 -4.4673185 -4.3942394 -4.1941919][-4.6370463 -4.6654568 -4.5644712 -4.3535686 -4.1119328 -3.9099288 -3.770009 -3.7324791 -3.8173234 -3.9709637 -4.1792684 -4.3802066 -4.4594712 -4.4004931 -4.2249851][-4.6104126 -4.6058288 -4.4876571 -4.2919555 -4.0877051 -3.9094014 -3.7692752 -3.7253208 -3.8248477 -4.0029364 -4.234447 -4.4364552 -4.5039635 -4.4549747 -4.3172526][-4.6307788 -4.5881481 -4.4586916 -4.2994618 -4.1649828 -4.0484405 -3.9445004 -3.9112129 -4.00829 -4.1691766 -4.3593874 -4.5024362 -4.5252528 -4.4732604 -4.3728824][-4.6403136 -4.577889 -4.4637542 -4.3575587 -4.291935 -4.2378922 -4.1728415 -4.1511269 -4.2287993 -4.3443131 -4.4621453 -4.5231218 -4.48858 -4.4195347 -4.3407269][-4.5746584 -4.5233831 -4.4603853 -4.4212012 -4.4059243 -4.3840837 -4.332129 -4.3030791 -4.3504486 -4.4205127 -4.4812279 -4.4811635 -4.4094486 -4.3274093 -4.2594571][-4.4611449 -4.4466629 -4.4490733 -4.4775238 -4.4959521 -4.474894 -4.4016161 -4.337419 -4.3454928 -4.3865404 -4.4234729 -4.4019485 -4.3232317 -4.2327685 -4.1710467][-4.3706703 -4.3927522 -4.44344 -4.5142093 -4.5451503 -4.50807 -4.4042125 -4.3033891 -4.2873554 -4.333467 -4.385066 -4.3709354 -4.2891212 -4.1739359 -4.0940828][-4.3270221 -4.3592696 -4.4159102 -4.492178 -4.5251789 -4.4811187 -4.3674407 -4.2591767 -4.2474537 -4.3233581 -4.4037313 -4.400116 -4.3073 -4.1619444 -4.0562553][-4.2938714 -4.3201232 -4.3641849 -4.4329829 -4.4654422 -4.4244232 -4.3227797 -4.2314329 -4.2401729 -4.3476882 -4.4500203 -4.4505458 -4.34946 -4.1951385 -4.0839491][-4.2960944 -4.309968 -4.3378663 -4.3909645 -4.4141884 -4.3768773 -4.2915788 -4.2235384 -4.2488723 -4.3639441 -4.4648566 -4.4648213 -4.3711047 -4.2407918 -4.1527581]]...]
INFO - root - 2017-12-07 12:27:30.923640: step 16010, loss = 21.41, batch loss = 21.32 (8.5 examples/sec; 0.936 sec/batch; 82h:17m:28s remains)
INFO - root - 2017-12-07 12:27:40.345575: step 16020, loss = 21.57, batch loss = 21.49 (8.5 examples/sec; 0.941 sec/batch; 82h:45m:12s remains)
INFO - root - 2017-12-07 12:27:49.706378: step 16030, loss = 21.42, batch loss = 21.33 (8.7 examples/sec; 0.914 sec/batch; 80h:22m:44s remains)
INFO - root - 2017-12-07 12:27:59.023462: step 16040, loss = 21.31, batch loss = 21.23 (8.8 examples/sec; 0.910 sec/batch; 80h:00m:31s remains)
INFO - root - 2017-12-07 12:28:08.401421: step 16050, loss = 21.43, batch loss = 21.35 (8.8 examples/sec; 0.914 sec/batch; 80h:19m:53s remains)
INFO - root - 2017-12-07 12:28:17.820832: step 16060, loss = 21.76, batch loss = 21.68 (8.0 examples/sec; 0.998 sec/batch; 87h:43m:30s remains)
INFO - root - 2017-12-07 12:28:27.374344: step 16070, loss = 21.42, batch loss = 21.34 (8.7 examples/sec; 0.916 sec/batch; 80h:32m:25s remains)
INFO - root - 2017-12-07 12:28:36.778349: step 16080, loss = 21.44, batch loss = 21.35 (8.2 examples/sec; 0.972 sec/batch; 85h:25m:33s remains)
INFO - root - 2017-12-07 12:28:46.280496: step 16090, loss = 21.60, batch loss = 21.52 (8.5 examples/sec; 0.944 sec/batch; 82h:57m:40s remains)
INFO - root - 2017-12-07 12:28:55.687643: step 16100, loss = 21.14, batch loss = 21.06 (7.7 examples/sec; 1.033 sec/batch; 90h:45m:42s remains)
2017-12-07 12:28:56.623990: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3118215 -4.3310714 -4.3750162 -4.4002032 -4.3825121 -4.3566957 -4.3154244 -4.2900672 -4.3208041 -4.3698983 -4.4122028 -4.3926783 -4.3485665 -4.3330889 -4.3059][-4.380775 -4.4281125 -4.481256 -4.502213 -4.4699459 -4.4180169 -4.3501773 -4.3038712 -4.310483 -4.3394537 -4.37254 -4.373383 -4.3590422 -4.3615775 -4.3409557][-4.3671126 -4.4258175 -4.4791379 -4.5011811 -4.4720869 -4.4243279 -4.3571358 -4.3027153 -4.2867603 -4.2882047 -4.3065686 -4.3263931 -4.345953 -4.3712869 -4.368576][-4.3083267 -4.353148 -4.3872752 -4.3932762 -4.3644252 -4.3285632 -4.2746954 -4.2229171 -4.1972547 -4.1900258 -4.2148108 -4.2615385 -4.3186932 -4.3665509 -4.3845387][-4.2360096 -4.2551823 -4.2650347 -4.24725 -4.2077675 -4.1678977 -4.115315 -4.0642056 -4.0445175 -4.0575519 -4.1133218 -4.1980062 -4.2910328 -4.3547869 -4.3809853][-4.1812482 -4.1697598 -4.1599445 -4.1256566 -4.0741439 -4.019877 -3.9596829 -3.9099798 -3.912904 -3.9697914 -4.0714326 -4.2004576 -4.3276486 -4.402966 -4.4257007][-4.2098823 -4.1723337 -4.138308 -4.0903 -4.0238333 -3.943604 -3.8696878 -3.8260596 -3.8619602 -3.9670322 -4.1082811 -4.2647972 -4.4059224 -4.4732122 -4.4797907][-4.3189325 -4.2761226 -4.2233624 -4.1613641 -4.07219 -3.9532247 -3.8491418 -3.8026431 -3.8673973 -4.0110121 -4.1710362 -4.3195434 -4.4317756 -4.4582424 -4.4337263][-4.4428372 -4.4136848 -4.3565416 -4.2898974 -4.191021 -4.0506296 -3.9290762 -3.8841224 -3.9596834 -4.1092572 -4.2492428 -4.3421793 -4.3855038 -4.3538 -4.2959294][-4.5172334 -4.5103054 -4.4693456 -4.4185987 -4.3400779 -4.2179193 -4.1113205 -4.075994 -4.1388903 -4.2522182 -4.3364491 -4.3494067 -4.3164425 -4.2420688 -4.1631384][-4.5308342 -4.5360818 -4.5185819 -4.4978328 -4.4536915 -4.3647313 -4.284595 -4.2571716 -4.2941241 -4.3610754 -4.3972144 -4.3567452 -4.2813272 -4.1992879 -4.1198077][-4.5110369 -4.5130506 -4.5132585 -4.5200305 -4.504972 -4.4442983 -4.3871789 -4.3659697 -4.3828917 -4.4210787 -4.4404659 -4.3872271 -4.301806 -4.2331614 -4.1705523][-4.4934659 -4.488668 -4.4974813 -4.5179586 -4.5153451 -4.4683557 -4.4264388 -4.4147944 -4.4288669 -4.46168 -4.4850049 -4.4434838 -4.3657451 -4.3079886 -4.25879][-4.4856405 -4.4728723 -4.4790068 -4.4955707 -4.488656 -4.446013 -4.416419 -4.4206228 -4.44441 -4.4815884 -4.5144353 -4.4970455 -4.4403191 -4.3886347 -4.3388081][-4.4697151 -4.4452615 -4.4369693 -4.4401841 -4.4292126 -4.3995533 -4.3878937 -4.4115052 -4.4448633 -4.4827018 -4.5195904 -4.5227356 -4.49274 -4.4512572 -4.4002786]]...]
INFO - root - 2017-12-07 12:29:05.939046: step 16110, loss = 21.51, batch loss = 21.43 (8.4 examples/sec; 0.958 sec/batch; 84h:09m:05s remains)
INFO - root - 2017-12-07 12:29:15.436386: step 16120, loss = 21.17, batch loss = 21.09 (8.2 examples/sec; 0.974 sec/batch; 85h:34m:30s remains)
INFO - root - 2017-12-07 12:29:24.833974: step 16130, loss = 21.39, batch loss = 21.30 (8.8 examples/sec; 0.906 sec/batch; 79h:38m:53s remains)
INFO - root - 2017-12-07 12:29:34.113904: step 16140, loss = 21.93, batch loss = 21.84 (8.9 examples/sec; 0.900 sec/batch; 79h:04m:27s remains)
INFO - root - 2017-12-07 12:29:43.545513: step 16150, loss = 21.33, batch loss = 21.25 (8.8 examples/sec; 0.907 sec/batch; 79h:42m:27s remains)
INFO - root - 2017-12-07 12:29:52.943678: step 16160, loss = 21.51, batch loss = 21.43 (8.5 examples/sec; 0.941 sec/batch; 82h:40m:49s remains)
INFO - root - 2017-12-07 12:30:02.448090: step 16170, loss = 21.15, batch loss = 21.07 (8.2 examples/sec; 0.976 sec/batch; 85h:45m:29s remains)
INFO - root - 2017-12-07 12:30:12.046230: step 16180, loss = 21.64, batch loss = 21.56 (8.2 examples/sec; 0.977 sec/batch; 85h:50m:32s remains)
INFO - root - 2017-12-07 12:30:21.416539: step 16190, loss = 21.60, batch loss = 21.52 (8.4 examples/sec; 0.954 sec/batch; 83h:50m:57s remains)
INFO - root - 2017-12-07 12:30:30.839208: step 16200, loss = 21.46, batch loss = 21.37 (9.1 examples/sec; 0.882 sec/batch; 77h:29m:12s remains)
2017-12-07 12:30:31.771333: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.38564 -4.4157944 -4.409224 -4.4038877 -4.4114079 -4.428019 -4.4479265 -4.4428754 -4.3880267 -4.306283 -4.2424917 -4.2117305 -4.2110534 -4.2459865 -4.311841][-4.3468857 -4.3193078 -4.2692776 -4.2538633 -4.2828722 -4.3315153 -4.3788023 -4.3914976 -4.3584352 -4.3075762 -4.2854323 -4.2937841 -4.3119254 -4.337666 -4.3840022][-4.3284078 -4.2613878 -4.1760263 -4.1449871 -4.1800933 -4.2512417 -4.3256521 -4.3609495 -4.3541508 -4.3396893 -4.3535004 -4.3842864 -4.4039817 -4.41329 -4.43666][-4.3310323 -4.2720785 -4.1903219 -4.1554766 -4.1822472 -4.2488074 -4.3196921 -4.3551025 -4.3613477 -4.3700161 -4.3998718 -4.4304309 -4.4389572 -4.4362507 -4.4505548][-4.3344297 -4.3170404 -4.2759671 -4.2593031 -4.2734604 -4.3064427 -4.3342724 -4.340652 -4.3428764 -4.3632493 -4.3970237 -4.4184542 -4.4155974 -4.4063277 -4.4178619][-4.3083892 -4.3406715 -4.3622537 -4.3779988 -4.3778129 -4.356689 -4.31301 -4.2657638 -4.2518616 -4.2804675 -4.3237815 -4.3505559 -4.3512244 -4.342834 -4.3461761][-4.2645783 -4.334208 -4.4028282 -4.4342976 -4.403285 -4.31377 -4.1949015 -4.1015821 -4.0858111 -4.1431885 -4.2236462 -4.2848434 -4.3113585 -4.3076611 -4.2911482][-4.2406354 -4.3251696 -4.4076409 -4.4253483 -4.3507962 -4.2016873 -4.0340548 -3.9233854 -3.9234324 -4.0193229 -4.1475821 -4.2527833 -4.3084 -4.3051305 -4.26705][-4.2560735 -4.3433523 -4.4160743 -4.4086466 -4.3040094 -4.131681 -3.9566662 -3.854682 -3.8700521 -3.982784 -4.1307526 -4.25784 -4.3263025 -4.3170276 -4.2617292][-4.3042574 -4.3828239 -4.4296403 -4.396039 -4.2841611 -4.129724 -3.9884605 -3.9166441 -3.9432297 -4.0503039 -4.1831422 -4.2986488 -4.3595324 -4.342104 -4.28088][-4.3493443 -4.4040675 -4.419301 -4.370944 -4.2757473 -4.165648 -4.0782971 -4.0428343 -4.0800381 -4.1752238 -4.2793703 -4.3641086 -4.4031873 -4.3782916 -4.3222327][-4.3895864 -4.4141455 -4.4017916 -4.3527465 -4.2906961 -4.2317963 -4.1900463 -4.1792383 -4.223208 -4.3091345 -4.3898344 -4.4465628 -4.4678497 -4.4424286 -4.3978138][-4.4544563 -4.4548512 -4.4258914 -4.3855314 -4.3566146 -4.3390083 -4.3285394 -4.33034 -4.3714757 -4.4445529 -4.5055885 -4.5425868 -4.5527163 -4.5306621 -4.496429][-4.5412292 -4.5321565 -4.5040426 -4.4802012 -4.4760089 -4.4803586 -4.4838657 -4.4894714 -4.5201864 -4.571516 -4.60982 -4.6291556 -4.6325068 -4.6179748 -4.5920062][-4.6131845 -4.60488 -4.5872784 -4.5793147 -4.5860958 -4.5937572 -4.597187 -4.6008329 -4.6192093 -4.6491323 -4.669775 -4.6791134 -4.6818652 -4.6750808 -4.6552782]]...]
INFO - root - 2017-12-07 12:30:40.897120: step 16210, loss = 21.29, batch loss = 21.21 (7.9 examples/sec; 1.009 sec/batch; 88h:41m:24s remains)
INFO - root - 2017-12-07 12:30:50.330879: step 16220, loss = 21.68, batch loss = 21.60 (8.3 examples/sec; 0.963 sec/batch; 84h:35m:47s remains)
INFO - root - 2017-12-07 12:30:59.658617: step 16230, loss = 21.70, batch loss = 21.62 (8.3 examples/sec; 0.968 sec/batch; 85h:03m:29s remains)
INFO - root - 2017-12-07 12:31:09.004221: step 16240, loss = 20.78, batch loss = 20.70 (8.1 examples/sec; 0.986 sec/batch; 86h:35m:52s remains)
INFO - root - 2017-12-07 12:31:18.306705: step 16250, loss = 21.33, batch loss = 21.25 (8.4 examples/sec; 0.952 sec/batch; 83h:36m:24s remains)
INFO - root - 2017-12-07 12:31:27.761187: step 16260, loss = 21.47, batch loss = 21.39 (8.6 examples/sec; 0.934 sec/batch; 82h:03m:44s remains)
INFO - root - 2017-12-07 12:31:36.942427: step 16270, loss = 21.14, batch loss = 21.06 (9.2 examples/sec; 0.874 sec/batch; 76h:47m:27s remains)
INFO - root - 2017-12-07 12:31:46.406544: step 16280, loss = 21.29, batch loss = 21.21 (9.0 examples/sec; 0.893 sec/batch; 78h:26m:52s remains)
INFO - root - 2017-12-07 12:31:55.803302: step 16290, loss = 21.64, batch loss = 21.56 (8.7 examples/sec; 0.924 sec/batch; 81h:11m:46s remains)
INFO - root - 2017-12-07 12:32:05.270248: step 16300, loss = 21.25, batch loss = 21.16 (8.4 examples/sec; 0.953 sec/batch; 83h:39m:43s remains)
2017-12-07 12:32:06.211556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5458183 -4.568224 -4.5918117 -4.6085234 -4.6157389 -4.6204419 -4.6265635 -4.6362209 -4.6436987 -4.6419158 -4.6282177 -4.6106114 -4.6002164 -4.6008081 -4.6015582][-4.62947 -4.6491766 -4.6712947 -4.6892157 -4.6970024 -4.6948352 -4.6876192 -4.6902261 -4.7021861 -4.70776 -4.6964245 -4.6770334 -4.6708288 -4.6870284 -4.7086535][-4.6665478 -4.6832533 -4.6991725 -4.7080021 -4.7026963 -4.6804647 -4.6514211 -4.648097 -4.6726604 -4.6927657 -4.6852183 -4.6632528 -4.6605439 -4.6927629 -4.7406049][-4.6729412 -4.6962805 -4.70874 -4.6968193 -4.6571436 -4.598814 -4.5467615 -4.5481772 -4.5998764 -4.6443782 -4.6410627 -4.6092768 -4.5960908 -4.6289759 -4.6917005][-4.6621165 -4.6955376 -4.7030554 -4.6590338 -4.5677776 -4.4665141 -4.4001565 -4.4214768 -4.5143914 -4.5947003 -4.6053677 -4.5673857 -4.5322962 -4.5437722 -4.5984068][-4.601933 -4.6354852 -4.6286187 -4.5433955 -4.3954568 -4.2514892 -4.1770525 -4.2241993 -4.3669662 -4.4972572 -4.5403051 -4.5093441 -4.4555049 -4.4404039 -4.4806561][-4.4704404 -4.4944696 -4.4654388 -4.3386469 -4.1407714 -3.9605429 -3.8768604 -3.9433348 -4.129746 -4.3148923 -4.4056215 -4.3963995 -4.3386173 -4.3107953 -4.3473353][-4.3356266 -4.3635483 -4.3285346 -4.1838169 -3.9565768 -3.7453094 -3.6398032 -3.7036991 -3.9155154 -4.1437984 -4.2763548 -4.2892632 -4.2366576 -4.2067251 -4.2447553][-4.2898207 -4.3396392 -4.3300357 -4.21095 -3.9969862 -3.775034 -3.642838 -3.6810324 -3.887527 -4.1234722 -4.2655854 -4.2814083 -4.2329535 -4.2035708 -4.2336631][-4.3538 -4.4216056 -4.4429975 -4.3769112 -4.2171111 -4.0206256 -3.8813777 -3.8936267 -4.0717373 -4.2743311 -4.3786273 -4.366034 -4.3050771 -4.2671933 -4.2832522][-4.4564915 -4.5178766 -4.5491238 -4.5291395 -4.4351683 -4.2905345 -4.1685438 -4.1681252 -4.3148737 -4.4730468 -4.525774 -4.475934 -4.3988109 -4.3557553 -4.3645368][-4.5357552 -4.5768061 -4.6015053 -4.6114111 -4.58088 -4.5030918 -4.415822 -4.4114323 -4.5226421 -4.63438 -4.6471939 -4.5821419 -4.5068645 -4.4705691 -4.478344][-4.5963149 -4.6142931 -4.6267257 -4.6491132 -4.6651773 -4.6496291 -4.6057348 -4.6044369 -4.6772804 -4.738811 -4.7216783 -4.6547079 -4.58949 -4.5630832 -4.57014][-4.6254368 -4.6282272 -4.6264429 -4.6445851 -4.6760335 -4.6942468 -4.6850166 -4.6918426 -4.7369103 -4.7644882 -4.734786 -4.6720009 -4.6121316 -4.5862112 -4.5873628][-4.5927057 -4.5946856 -4.5871792 -4.5913868 -4.610918 -4.6294222 -4.6334362 -4.6442871 -4.6720972 -4.6864543 -4.6645112 -4.6178684 -4.5686288 -4.5420833 -4.5371323]]...]
INFO - root - 2017-12-07 12:32:15.670824: step 16310, loss = 21.26, batch loss = 21.18 (8.8 examples/sec; 0.909 sec/batch; 79h:49m:54s remains)
INFO - root - 2017-12-07 12:32:24.906829: step 16320, loss = 21.25, batch loss = 21.16 (8.7 examples/sec; 0.920 sec/batch; 80h:48m:09s remains)
INFO - root - 2017-12-07 12:32:34.269374: step 16330, loss = 21.67, batch loss = 21.59 (8.8 examples/sec; 0.914 sec/batch; 80h:14m:49s remains)
INFO - root - 2017-12-07 12:32:43.786421: step 16340, loss = 21.67, batch loss = 21.58 (8.4 examples/sec; 0.953 sec/batch; 83h:43m:23s remains)
INFO - root - 2017-12-07 12:32:53.328575: step 16350, loss = 21.64, batch loss = 21.56 (7.8 examples/sec; 1.020 sec/batch; 89h:34m:10s remains)
INFO - root - 2017-12-07 12:33:02.493150: step 16360, loss = 22.04, batch loss = 21.95 (8.8 examples/sec; 0.906 sec/batch; 79h:32m:20s remains)
INFO - root - 2017-12-07 12:33:12.060837: step 16370, loss = 21.39, batch loss = 21.31 (8.6 examples/sec; 0.932 sec/batch; 81h:49m:38s remains)
INFO - root - 2017-12-07 12:33:21.290543: step 16380, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.968 sec/batch; 84h:58m:41s remains)
INFO - root - 2017-12-07 12:33:30.855451: step 16390, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.932 sec/batch; 81h:47m:44s remains)
INFO - root - 2017-12-07 12:33:40.358080: step 16400, loss = 21.42, batch loss = 21.34 (8.5 examples/sec; 0.943 sec/batch; 82h:49m:14s remains)
2017-12-07 12:33:41.308985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6590052 -4.6893778 -4.6769919 -4.6228943 -4.5549827 -4.476614 -4.4073973 -4.4056396 -4.4933009 -4.6157217 -4.705142 -4.7169461 -4.6298189 -4.4701953 -4.3313508][-4.6911373 -4.7254286 -4.73048 -4.689559 -4.6132183 -4.53196 -4.4892821 -4.5194712 -4.6144943 -4.7279706 -4.8056679 -4.8047738 -4.6994195 -4.5070124 -4.3322787][-4.6658726 -4.6836815 -4.7152319 -4.718502 -4.6508055 -4.552021 -4.5106878 -4.567863 -4.6766953 -4.7817135 -4.8413 -4.8262248 -4.7216821 -4.5235071 -4.3286386][-4.5828471 -4.5815105 -4.6359673 -4.6875315 -4.6233969 -4.4696832 -4.376586 -4.4453988 -4.6025038 -4.7550411 -4.8326821 -4.818233 -4.7242656 -4.5326171 -4.3266735][-4.515388 -4.4976635 -4.5483317 -4.6116242 -4.5252328 -4.2898817 -4.108542 -4.167603 -4.4020677 -4.6636763 -4.808238 -4.8150835 -4.7320571 -4.5491915 -4.3394332][-4.517189 -4.489922 -4.5159163 -4.5501132 -4.4171562 -4.0984163 -3.8176165 -3.8302085 -4.10928 -4.4710035 -4.7028503 -4.7619715 -4.711288 -4.55556 -4.3592167][-4.52019 -4.5016694 -4.5116649 -4.507144 -4.3259664 -3.9428437 -3.5697331 -3.5083423 -3.7851441 -4.2056575 -4.5227504 -4.6561031 -4.6568389 -4.5389051 -4.3673048][-4.5258226 -4.5205278 -4.5190344 -4.4754181 -4.2586641 -3.8555615 -3.4574461 -3.3636308 -3.6258175 -4.0540533 -4.40763 -4.5841866 -4.6144605 -4.5154629 -4.3599315][-4.5563488 -4.569674 -4.5642896 -4.5070567 -4.3050914 -3.9676354 -3.6487327 -3.5798054 -3.7930694 -4.1410389 -4.4377007 -4.5936818 -4.6171517 -4.5107212 -4.352232][-4.5703325 -4.6130228 -4.6189981 -4.5646582 -4.3962703 -4.1489224 -3.930068 -3.8841574 -4.0262074 -4.2655139 -4.4872775 -4.6200023 -4.6366172 -4.5184555 -4.3497267][-4.5392852 -4.6085863 -4.63418 -4.5912971 -4.4556847 -4.2800541 -4.1339769 -4.0945749 -4.1751366 -4.3367152 -4.5150409 -4.6397533 -4.6520963 -4.523582 -4.3486309][-4.4657 -4.5419531 -4.5867066 -4.57755 -4.4940562 -4.3793726 -4.2756925 -4.2300668 -4.2698364 -4.39052 -4.5482159 -4.6578178 -4.6517487 -4.5154085 -4.3456516][-4.402245 -4.4621434 -4.5183568 -4.5425053 -4.4922018 -4.3886957 -4.2756453 -4.214869 -4.2494783 -4.3708029 -4.5307994 -4.6338744 -4.6197238 -4.4931569 -4.3415074][-4.4366012 -4.4681773 -4.5166526 -4.5512948 -4.5056548 -4.3841643 -4.2520008 -4.1997042 -4.2594419 -4.3894706 -4.5396 -4.6267681 -4.6070671 -4.4912286 -4.3503737][-4.48314 -4.4789286 -4.5063152 -4.5389061 -4.5024176 -4.3908648 -4.2755003 -4.2528872 -4.3336735 -4.4527664 -4.5735092 -4.6391497 -4.6156273 -4.5051928 -4.3655949]]...]
INFO - root - 2017-12-07 12:33:50.769703: step 16410, loss = 21.55, batch loss = 21.47 (8.1 examples/sec; 0.993 sec/batch; 87h:10m:54s remains)
INFO - root - 2017-12-07 12:34:00.122561: step 16420, loss = 21.67, batch loss = 21.59 (8.2 examples/sec; 0.978 sec/batch; 85h:51m:35s remains)
INFO - root - 2017-12-07 12:34:09.576971: step 16430, loss = 21.17, batch loss = 21.09 (8.6 examples/sec; 0.927 sec/batch; 81h:20m:44s remains)
INFO - root - 2017-12-07 12:34:19.007500: step 16440, loss = 21.28, batch loss = 21.20 (8.6 examples/sec; 0.929 sec/batch; 81h:32m:21s remains)
INFO - root - 2017-12-07 12:34:28.458136: step 16450, loss = 21.43, batch loss = 21.34 (8.2 examples/sec; 0.973 sec/batch; 85h:27m:53s remains)
INFO - root - 2017-12-07 12:34:37.840083: step 16460, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.909 sec/batch; 79h:46m:40s remains)
INFO - root - 2017-12-07 12:34:47.286935: step 16470, loss = 21.40, batch loss = 21.32 (8.6 examples/sec; 0.934 sec/batch; 81h:59m:50s remains)
INFO - root - 2017-12-07 12:34:56.720351: step 16480, loss = 21.38, batch loss = 21.30 (9.0 examples/sec; 0.894 sec/batch; 78h:27m:10s remains)
INFO - root - 2017-12-07 12:35:06.035335: step 16490, loss = 21.26, batch loss = 21.18 (8.4 examples/sec; 0.952 sec/batch; 83h:32m:51s remains)
INFO - root - 2017-12-07 12:35:15.402874: step 16500, loss = 21.63, batch loss = 21.55 (8.8 examples/sec; 0.910 sec/batch; 79h:52m:59s remains)
2017-12-07 12:35:16.396672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7835989 -4.770318 -4.750742 -4.74097 -4.7499218 -4.7619185 -4.7701058 -4.763793 -4.7505379 -4.7464623 -4.7556973 -4.7781334 -4.8094873 -4.8266149 -4.8070722][-4.7338529 -4.7000122 -4.6554866 -4.631084 -4.6528554 -4.6955652 -4.731164 -4.7291794 -4.701324 -4.6790838 -4.6859078 -4.7325 -4.7995696 -4.8345027 -4.7952271][-4.6016331 -4.5761318 -4.5304704 -4.4979434 -4.5292349 -4.5947452 -4.645505 -4.6400962 -4.5982084 -4.5652003 -4.5758886 -4.6462193 -4.737751 -4.7785912 -4.7123575][-4.4420228 -4.454349 -4.4406667 -4.4250731 -4.4678326 -4.532464 -4.5667548 -4.5398865 -4.4820752 -4.4454217 -4.4668841 -4.5547695 -4.6521335 -4.6894741 -4.5942864][-4.2992129 -4.3517141 -4.380022 -4.3987031 -4.4502373 -4.4774556 -4.4520159 -4.3784523 -4.3000355 -4.2698331 -4.31848 -4.4268808 -4.5253053 -4.5578742 -4.4500394][-4.2239838 -4.2849994 -4.3306961 -4.3676805 -4.4047685 -4.364337 -4.2527504 -4.116848 -4.0210986 -4.01811 -4.1194272 -4.2656035 -4.3759108 -4.4161515 -4.3270235][-4.2431335 -4.2808609 -4.3097897 -4.3386521 -4.3474069 -4.2529573 -4.0800734 -3.8970289 -3.788871 -3.8174548 -3.9732554 -4.1616893 -4.2920208 -4.3514414 -4.3067708][-4.2924113 -4.2930937 -4.2956104 -4.3084197 -4.2977915 -4.1899004 -4.0120029 -3.8267384 -3.7251625 -3.7726281 -3.9479396 -4.1483183 -4.2862115 -4.3635168 -4.3648105][-4.3197541 -4.2937803 -4.2829714 -4.2861681 -4.2637577 -4.1655931 -4.0257459 -3.8889275 -3.8210013 -3.8755481 -4.0312538 -4.2109747 -4.3394184 -4.4265003 -4.4639215][-4.3518929 -4.313262 -4.3050346 -4.3094306 -4.285408 -4.2053242 -4.1124144 -4.0408764 -4.0163665 -4.068316 -4.1871028 -4.32698 -4.43477 -4.521699 -4.5853095][-4.4218745 -4.3669271 -4.3446364 -4.3369813 -4.309526 -4.2525978 -4.207437 -4.1987357 -4.2168717 -4.2668519 -4.3524704 -4.4464788 -4.5250263 -4.6043944 -4.6821556][-4.5343261 -4.466002 -4.40656 -4.3619113 -4.3217859 -4.2903738 -4.2982349 -4.3454194 -4.3988724 -4.4528794 -4.5087757 -4.5583816 -4.6041508 -4.6655726 -4.737638][-4.6699657 -4.6063004 -4.5184765 -4.4377136 -4.3848348 -4.3829017 -4.4413548 -4.5291243 -4.6027675 -4.6528058 -4.6816778 -4.6952147 -4.7085209 -4.7376494 -4.7797241][-4.7485437 -4.7061467 -4.6224804 -4.5425887 -4.5033364 -4.5331488 -4.6206226 -4.7176871 -4.78335 -4.8127394 -4.8143096 -4.8020964 -4.7877207 -4.7788377 -4.7809029][-4.7395492 -4.7215662 -4.6651635 -4.6144977 -4.6039863 -4.6497684 -4.7314992 -4.8044047 -4.8420606 -4.8466487 -4.828145 -4.8010478 -4.7688789 -4.7353497 -4.7128305]]...]
INFO - root - 2017-12-07 12:35:25.870388: step 16510, loss = 21.22, batch loss = 21.14 (9.0 examples/sec; 0.891 sec/batch; 78h:13m:38s remains)
INFO - root - 2017-12-07 12:35:35.296294: step 16520, loss = 21.61, batch loss = 21.53 (8.9 examples/sec; 0.901 sec/batch; 79h:04m:40s remains)
INFO - root - 2017-12-07 12:35:44.813205: step 16530, loss = 21.22, batch loss = 21.14 (8.1 examples/sec; 0.982 sec/batch; 86h:13m:39s remains)
INFO - root - 2017-12-07 12:35:54.275997: step 16540, loss = 21.50, batch loss = 21.41 (7.7 examples/sec; 1.041 sec/batch; 91h:22m:33s remains)
INFO - root - 2017-12-07 12:36:03.543983: step 16550, loss = 21.23, batch loss = 21.15 (9.9 examples/sec; 0.812 sec/batch; 71h:16m:04s remains)
INFO - root - 2017-12-07 12:36:12.936999: step 16560, loss = 21.53, batch loss = 21.44 (8.5 examples/sec; 0.937 sec/batch; 82h:15m:32s remains)
INFO - root - 2017-12-07 12:36:22.300008: step 16570, loss = 21.41, batch loss = 21.32 (8.3 examples/sec; 0.967 sec/batch; 84h:49m:32s remains)
INFO - root - 2017-12-07 12:36:31.822473: step 16580, loss = 21.45, batch loss = 21.37 (8.7 examples/sec; 0.920 sec/batch; 80h:46m:18s remains)
INFO - root - 2017-12-07 12:36:41.374724: step 16590, loss = 21.67, batch loss = 21.59 (8.5 examples/sec; 0.940 sec/batch; 82h:26m:57s remains)
INFO - root - 2017-12-07 12:36:50.737944: step 16600, loss = 21.19, batch loss = 21.11 (9.2 examples/sec; 0.869 sec/batch; 76h:16m:09s remains)
2017-12-07 12:36:51.753358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7739582 -4.6338487 -4.4742885 -4.3193178 -4.2625685 -4.3856997 -4.54959 -4.6129441 -4.6178007 -4.5949378 -4.5406246 -4.4780979 -4.4513631 -4.4575586 -4.4880428][-4.7444792 -4.6262426 -4.4981813 -4.3667541 -4.3079019 -4.3987336 -4.5343933 -4.6023073 -4.63652 -4.6384888 -4.585207 -4.5211339 -4.49784 -4.4968891 -4.4952011][-4.6913471 -4.6128054 -4.5284529 -4.4178157 -4.3426533 -4.3813953 -4.4652472 -4.524538 -4.5895462 -4.6269755 -4.5925679 -4.5394144 -4.5283713 -4.5277457 -4.4977875][-4.6452389 -4.6075282 -4.5562739 -4.4529405 -4.3563919 -4.3427896 -4.3635321 -4.394454 -4.4811349 -4.5619879 -4.5644755 -4.5350447 -4.5402431 -4.5444055 -4.4961157][-4.6223044 -4.609798 -4.5686922 -4.4603047 -4.3412719 -4.2775087 -4.22335 -4.1992655 -4.2928648 -4.4263721 -4.5042057 -4.5426688 -4.5842972 -4.5928926 -4.5184522][-4.5801992 -4.56571 -4.5161242 -4.4106731 -4.29522 -4.2009616 -4.0691819 -3.9681392 -4.0510983 -4.2356615 -4.3965473 -4.5324192 -4.6325269 -4.6467762 -4.5460429][-4.5085645 -4.4628038 -4.3861136 -4.2846723 -4.1873665 -4.0760694 -3.8702908 -3.6939168 -3.7799656 -4.03178 -4.2721896 -4.4973111 -4.6600008 -4.6908083 -4.5902724][-4.4226222 -4.341785 -4.2517943 -4.1647177 -4.0861468 -3.9555302 -3.683053 -3.4363542 -3.5183082 -3.8296366 -4.1327019 -4.4146614 -4.6231031 -4.676928 -4.604733][-4.2911038 -4.1913157 -4.1283679 -4.0929785 -4.0723329 -3.9746883 -3.7044661 -3.4362373 -3.4856186 -3.7847486 -4.0891104 -4.3629136 -4.5673933 -4.6278358 -4.5858412][-4.1759844 -4.0870423 -4.0708423 -4.092514 -4.1373019 -4.1186347 -3.9180861 -3.6818976 -3.6885681 -3.9068007 -4.1468954 -4.3705568 -4.5485396 -4.6074243 -4.58336][-4.1648865 -4.0877266 -4.0870433 -4.1252766 -4.2107534 -4.2852397 -4.1897311 -4.0168419 -4.0000529 -4.1193824 -4.2543311 -4.4045897 -4.545126 -4.5936651 -4.5738015][-4.2627912 -4.1606903 -4.1243091 -4.1410203 -4.248301 -4.416132 -4.4466 -4.3672128 -4.3662195 -4.4142127 -4.4426584 -4.4964976 -4.5694814 -4.5856156 -4.5587506][-4.438355 -4.3012266 -4.2053909 -4.1760535 -4.2755642 -4.4967451 -4.6212006 -4.6155486 -4.6201406 -4.6143537 -4.5708714 -4.5576239 -4.5721197 -4.55766 -4.5338755][-4.6236606 -4.474339 -4.3316088 -4.2550621 -4.3286166 -4.556036 -4.726305 -4.7497797 -4.7348585 -4.6901751 -4.6201277 -4.5782475 -4.5527668 -4.5145683 -4.4978313][-4.748457 -4.6194248 -4.4669633 -4.367116 -4.4142456 -4.6096869 -4.7727718 -4.7991142 -4.7674518 -4.71048 -4.64582 -4.5971656 -4.546227 -4.4871588 -4.467525]]...]
INFO - root - 2017-12-07 12:37:01.175502: step 16610, loss = 20.97, batch loss = 20.88 (8.5 examples/sec; 0.937 sec/batch; 82h:11m:27s remains)
INFO - root - 2017-12-07 12:37:10.581975: step 16620, loss = 21.52, batch loss = 21.43 (8.9 examples/sec; 0.900 sec/batch; 78h:56m:18s remains)
INFO - root - 2017-12-07 12:37:19.876548: step 16630, loss = 21.64, batch loss = 21.56 (8.9 examples/sec; 0.902 sec/batch; 79h:10m:14s remains)
INFO - root - 2017-12-07 12:37:29.431730: step 16640, loss = 21.64, batch loss = 21.55 (8.0 examples/sec; 0.995 sec/batch; 87h:16m:10s remains)
INFO - root - 2017-12-07 12:37:38.786880: step 16650, loss = 21.63, batch loss = 21.54 (7.9 examples/sec; 1.012 sec/batch; 88h:46m:56s remains)
INFO - root - 2017-12-07 12:37:48.175866: step 16660, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.976 sec/batch; 85h:39m:17s remains)
INFO - root - 2017-12-07 12:37:57.673754: step 16670, loss = 21.54, batch loss = 21.45 (8.0 examples/sec; 0.999 sec/batch; 87h:37m:11s remains)
INFO - root - 2017-12-07 12:38:07.106771: step 16680, loss = 21.28, batch loss = 21.20 (8.9 examples/sec; 0.901 sec/batch; 79h:04m:42s remains)
INFO - root - 2017-12-07 12:38:16.557893: step 16690, loss = 21.22, batch loss = 21.14 (8.8 examples/sec; 0.911 sec/batch; 79h:56m:47s remains)
INFO - root - 2017-12-07 12:38:26.013276: step 16700, loss = 21.05, batch loss = 20.97 (8.3 examples/sec; 0.962 sec/batch; 84h:24m:48s remains)
2017-12-07 12:38:26.909276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2767377 -4.274509 -4.3023124 -4.335465 -4.3547978 -4.3570185 -4.3553243 -4.3590665 -4.3693333 -4.37806 -4.3790293 -4.3765855 -4.3566179 -4.3224874 -4.3068185][-4.2675786 -4.2562351 -4.2755408 -4.3073754 -4.3347049 -4.3507762 -4.3599381 -4.3687911 -4.3810639 -4.3894024 -4.3923874 -4.39527 -4.3799915 -4.3472753 -4.32155][-4.289906 -4.2773905 -4.2860131 -4.3093867 -4.3381734 -4.3626413 -4.3766804 -4.3855419 -4.3946085 -4.3982553 -4.4031472 -4.4167209 -4.4162822 -4.3943648 -4.3582811][-4.3416948 -4.3338594 -4.3318644 -4.3403087 -4.3601327 -4.3797803 -4.3877339 -4.3913732 -4.4015455 -4.4147038 -4.4398417 -4.4777346 -4.4981942 -4.4862089 -4.4352694][-4.403069 -4.4043059 -4.39278 -4.380404 -4.3736463 -4.3595667 -4.3285089 -4.2951317 -4.2970109 -4.3481121 -4.4274592 -4.5072951 -4.5504265 -4.5482931 -4.4948235][-4.4398 -4.4454284 -4.4204965 -4.3766413 -4.3204861 -4.2413397 -4.1374254 -4.0331073 -4.027472 -4.1688237 -4.3482776 -4.4815984 -4.5365634 -4.5290961 -4.4779158][-4.4545593 -4.4701366 -4.4343977 -4.35234 -4.2343383 -4.0764852 -3.8801517 -3.6856222 -3.6835217 -3.9576614 -4.2653356 -4.4550753 -4.5141706 -4.483799 -4.4205136][-4.4850655 -4.522984 -4.4865022 -4.376925 -4.2122889 -3.9931314 -3.7147923 -3.44032 -3.4430747 -3.8128817 -4.2045689 -4.4217939 -4.475214 -4.4105659 -4.3162351][-4.5381947 -4.59857 -4.5665822 -4.4436355 -4.2511067 -3.9892535 -3.6594081 -3.3543842 -3.3697267 -3.7673621 -4.1790843 -4.3956566 -4.4357371 -4.3334742 -4.2032752][-4.6002321 -4.6746106 -4.6511774 -4.5321674 -4.3362484 -4.0593176 -3.7218089 -3.4366748 -3.4616098 -3.8241076 -4.2035756 -4.4055581 -4.4375582 -4.3230696 -4.1868606][-4.652916 -4.7359438 -4.7292986 -4.6394339 -4.4762573 -4.2352333 -3.9534841 -3.7353983 -3.7614787 -4.0332246 -4.3182058 -4.4653525 -4.4714136 -4.3607368 -4.2497368][-4.6726012 -4.7554045 -4.768549 -4.7197938 -4.6132345 -4.4476871 -4.2672625 -4.1435242 -4.1741791 -4.3423982 -4.5079823 -4.5815616 -4.5601845 -4.4687939 -4.3902025][-4.6472573 -4.7241731 -4.7529597 -4.7395611 -4.6880927 -4.6062145 -4.530159 -4.4893937 -4.5192194 -4.5987887 -4.6673679 -4.6929369 -4.6727843 -4.6178007 -4.5657887][-4.5944715 -4.6600761 -4.6949582 -4.6994886 -4.6842051 -4.6621594 -4.650445 -4.6507306 -4.6689653 -4.6939054 -4.7115812 -4.7185383 -4.70869 -4.6794877 -4.6413541][-4.5292163 -4.5771823 -4.6081433 -4.6192203 -4.6205649 -4.6225376 -4.6288671 -4.6358843 -4.6449795 -4.6534767 -4.6607556 -4.6660357 -4.6623631 -4.6455364 -4.6192751]]...]
INFO - root - 2017-12-07 12:38:36.317467: step 16710, loss = 21.58, batch loss = 21.50 (8.3 examples/sec; 0.964 sec/batch; 84h:35m:15s remains)
INFO - root - 2017-12-07 12:38:45.726840: step 16720, loss = 21.45, batch loss = 21.37 (8.1 examples/sec; 0.982 sec/batch; 86h:08m:39s remains)
INFO - root - 2017-12-07 12:38:55.059183: step 16730, loss = 21.34, batch loss = 21.26 (8.5 examples/sec; 0.937 sec/batch; 82h:12m:29s remains)
INFO - root - 2017-12-07 12:39:04.499802: step 16740, loss = 21.83, batch loss = 21.74 (8.6 examples/sec; 0.934 sec/batch; 81h:54m:23s remains)
INFO - root - 2017-12-07 12:39:13.821541: step 16750, loss = 21.42, batch loss = 21.34 (8.7 examples/sec; 0.916 sec/batch; 80h:18m:52s remains)
INFO - root - 2017-12-07 12:39:23.293947: step 16760, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.932 sec/batch; 81h:44m:37s remains)
INFO - root - 2017-12-07 12:39:32.665444: step 16770, loss = 21.29, batch loss = 21.20 (8.5 examples/sec; 0.940 sec/batch; 82h:24m:50s remains)
INFO - root - 2017-12-07 12:39:42.046407: step 16780, loss = 21.82, batch loss = 21.73 (8.7 examples/sec; 0.920 sec/batch; 80h:40m:32s remains)
INFO - root - 2017-12-07 12:39:51.434003: step 16790, loss = 22.08, batch loss = 21.99 (8.4 examples/sec; 0.950 sec/batch; 83h:20m:35s remains)
INFO - root - 2017-12-07 12:40:00.941794: step 16800, loss = 21.21, batch loss = 21.13 (7.9 examples/sec; 1.017 sec/batch; 89h:11m:07s remains)
2017-12-07 12:40:01.909061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3558483 -4.4180126 -4.4837685 -4.5452051 -4.5768557 -4.5803742 -4.5811443 -4.5853148 -4.5821466 -4.5740247 -4.5419726 -4.4897909 -4.4641333 -4.4557309 -4.4523559][-4.4370594 -4.5041623 -4.5689712 -4.6223006 -4.6481786 -4.643836 -4.639256 -4.641417 -4.6346507 -4.6300159 -4.6056757 -4.5511918 -4.5334511 -4.5476727 -4.5626774][-4.4838686 -4.5486732 -4.6099257 -4.6511989 -4.6571651 -4.6215873 -4.58082 -4.5686755 -4.5670261 -4.5826721 -4.5872865 -4.5511851 -4.5491419 -4.5890565 -4.6275463][-4.490274 -4.5465403 -4.6013002 -4.6337438 -4.6193681 -4.550096 -4.4733281 -4.4495111 -4.4528847 -4.4838686 -4.5143781 -4.5037332 -4.5210304 -4.5805392 -4.6464682][-4.4734592 -4.5114756 -4.5498624 -4.5655775 -4.5244908 -4.4146647 -4.3040266 -4.271801 -4.2867665 -4.3281493 -4.3742771 -4.3880672 -4.4264345 -4.498383 -4.5874214][-4.4280038 -4.4438 -4.4578466 -4.4427986 -4.3542266 -4.1764345 -4.0130997 -3.9728174 -4.0266695 -4.1083794 -4.1772623 -4.2196178 -4.2822967 -4.3624806 -4.4621887][-4.3410821 -4.3396134 -4.3336916 -4.2952604 -4.167367 -3.92934 -3.7292442 -3.6935005 -3.8042085 -3.9465938 -4.03583 -4.0972271 -4.1809139 -4.2639475 -4.363946][-4.2967319 -4.280097 -4.2481833 -4.1896839 -4.0486917 -3.8019128 -3.6125407 -3.5905335 -3.7281961 -3.8938081 -3.9735136 -4.0362635 -4.139596 -4.2344227 -4.338933][-4.3953929 -4.3682394 -4.3073177 -4.2306247 -4.1002297 -3.8904119 -3.7464783 -3.7304785 -3.8386168 -3.962513 -3.9903529 -4.0261984 -4.1307287 -4.2334752 -4.3491869][-4.5513477 -4.5087957 -4.4238629 -4.3409204 -4.2363062 -4.081183 -3.9868894 -3.9715946 -4.0311189 -4.0903358 -4.0705476 -4.0870695 -4.1823192 -4.2822518 -4.3972526][-4.6673985 -4.610981 -4.5216689 -4.4503059 -4.379807 -4.2861738 -4.2326179 -4.2129769 -4.22727 -4.237277 -4.203814 -4.2260242 -4.3109517 -4.3928323 -4.481988][-4.7278843 -4.6846876 -4.6183085 -4.5725427 -4.5382562 -4.4975891 -4.4673047 -4.4354682 -4.4195747 -4.4132366 -4.4023795 -4.4361253 -4.4922009 -4.5314827 -4.5699043][-4.6837678 -4.6728077 -4.6493235 -4.6354027 -4.6239638 -4.6046743 -4.5784264 -4.5474343 -4.5381713 -4.5509443 -4.5723882 -4.6039877 -4.6198492 -4.6109171 -4.5979433][-4.5619946 -4.580956 -4.59805 -4.6116076 -4.6097655 -4.5909925 -4.5635614 -4.5446167 -4.55349 -4.5799909 -4.60556 -4.615932 -4.5984416 -4.5612335 -4.5221515][-4.4322796 -4.462522 -4.4983306 -4.5265441 -4.5324287 -4.5179911 -4.4963255 -4.4836636 -4.4889808 -4.5000572 -4.5009475 -4.48355 -4.4484348 -4.4062181 -4.36836]]...]
INFO - root - 2017-12-07 12:40:11.335405: step 16810, loss = 21.46, batch loss = 21.38 (8.8 examples/sec; 0.908 sec/batch; 79h:37m:46s remains)
INFO - root - 2017-12-07 12:40:20.694813: step 16820, loss = 21.08, batch loss = 21.00 (9.0 examples/sec; 0.885 sec/batch; 77h:34m:08s remains)
INFO - root - 2017-12-07 12:40:30.058211: step 16830, loss = 21.47, batch loss = 21.38 (8.2 examples/sec; 0.970 sec/batch; 85h:04m:23s remains)
INFO - root - 2017-12-07 12:40:39.443481: step 16840, loss = 22.43, batch loss = 22.35 (9.5 examples/sec; 0.842 sec/batch; 73h:48m:00s remains)
INFO - root - 2017-12-07 12:40:48.999523: step 16850, loss = 21.19, batch loss = 21.11 (8.0 examples/sec; 0.998 sec/batch; 87h:30m:01s remains)
INFO - root - 2017-12-07 12:40:58.383681: step 16860, loss = 22.02, batch loss = 21.94 (8.1 examples/sec; 0.990 sec/batch; 86h:50m:24s remains)
INFO - root - 2017-12-07 12:41:07.690885: step 16870, loss = 21.43, batch loss = 21.35 (8.1 examples/sec; 0.991 sec/batch; 86h:53m:36s remains)
INFO - root - 2017-12-07 12:41:17.191462: step 16880, loss = 21.75, batch loss = 21.66 (8.1 examples/sec; 0.987 sec/batch; 86h:33m:20s remains)
INFO - root - 2017-12-07 12:41:26.603294: step 16890, loss = 21.86, batch loss = 21.78 (8.9 examples/sec; 0.904 sec/batch; 79h:14m:54s remains)
INFO - root - 2017-12-07 12:41:35.804727: step 16900, loss = 21.77, batch loss = 21.69 (8.7 examples/sec; 0.923 sec/batch; 80h:53m:45s remains)
2017-12-07 12:41:36.741689: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4944282 -4.5392618 -4.5541921 -4.5262775 -4.4525633 -4.3572335 -4.2724419 -4.2392592 -4.2680397 -4.3225746 -4.3709464 -4.4372287 -4.5033851 -4.5043058 -4.4381833][-4.5126882 -4.5549207 -4.5798187 -4.5443592 -4.4497752 -4.3447361 -4.2772355 -4.2684927 -4.2991781 -4.3261981 -4.3480167 -4.413393 -4.4910684 -4.494585 -4.4035525][-4.5231214 -4.5774484 -4.6099033 -4.5614119 -4.4487352 -4.3379703 -4.2808757 -4.2815728 -4.304018 -4.3075304 -4.3138061 -4.3807631 -4.4698672 -4.4803357 -4.3683972][-4.4687181 -4.5637317 -4.61866 -4.5705633 -4.4584947 -4.3506808 -4.2975826 -4.2968397 -4.3092442 -4.3050604 -4.3218632 -4.3982673 -4.4860997 -4.5043607 -4.3917036][-4.3605256 -4.5037909 -4.5865316 -4.5457568 -4.4418149 -4.3371453 -4.2818594 -4.2753558 -4.28684 -4.3000431 -4.3490334 -4.4307871 -4.4993124 -4.5171471 -4.4215279][-4.2141938 -4.3750763 -4.4636583 -4.4198074 -4.3109269 -4.1985092 -4.1324248 -4.121449 -4.15396 -4.2177458 -4.3110571 -4.3974094 -4.4507189 -4.4714494 -4.4092913][-4.1338053 -4.2568846 -4.3177729 -4.2540646 -4.1170969 -3.9719718 -3.8714128 -3.8379722 -3.8920515 -4.0303669 -4.1905036 -4.3073516 -4.37125 -4.3966875 -4.3644795][-4.1418767 -4.196393 -4.2160654 -4.1372848 -3.9822195 -3.8122587 -3.6745503 -3.6019106 -3.6506238 -3.8439989 -4.0643167 -4.2083669 -4.2722507 -4.2793484 -4.2574825][-4.1884623 -4.1959095 -4.2169218 -4.1800179 -4.0730186 -3.9341607 -3.7911258 -3.6803699 -3.6817083 -3.8459628 -4.055603 -4.1838355 -4.2155089 -4.1766076 -4.1375084][-4.2539372 -4.2681475 -4.34292 -4.3812933 -4.3416867 -4.2451124 -4.1083035 -3.9730439 -3.9262438 -4.0256538 -4.1819305 -4.2675223 -4.2447143 -4.1374316 -4.0542536][-4.3237948 -4.3714848 -4.4893904 -4.5669026 -4.5539412 -4.477231 -4.3567834 -4.231863 -4.1703672 -4.2223706 -4.3241816 -4.3697095 -4.3054557 -4.1512189 -4.0390368][-4.350275 -4.4213285 -4.5531077 -4.6425281 -4.6448674 -4.5919938 -4.502481 -4.3976917 -4.3244481 -4.3354578 -4.3906369 -4.4122133 -4.3522248 -4.2180228 -4.1267457][-4.32279 -4.4004903 -4.5209708 -4.6035714 -4.6134477 -4.5748329 -4.5102849 -4.4312787 -4.3663225 -4.3599434 -4.3849487 -4.3926363 -4.3594217 -4.2889433 -4.2445865][-4.2835612 -4.3436422 -4.4301667 -4.4894648 -4.4969735 -4.4673634 -4.4257779 -4.3780212 -4.3390784 -4.33488 -4.3410892 -4.3354321 -4.32472 -4.317627 -4.3268313][-4.2685328 -4.3020391 -4.3532486 -4.3933439 -4.40321 -4.3898492 -4.3732648 -4.3488164 -4.3202372 -4.30227 -4.2817869 -4.2607493 -4.2624364 -4.300859 -4.3648834]]...]
INFO - root - 2017-12-07 12:41:46.248268: step 16910, loss = 21.32, batch loss = 21.24 (8.2 examples/sec; 0.975 sec/batch; 85h:27m:28s remains)
INFO - root - 2017-12-07 12:41:55.650735: step 16920, loss = 21.56, batch loss = 21.47 (8.4 examples/sec; 0.955 sec/batch; 83h:41m:07s remains)
INFO - root - 2017-12-07 12:42:04.915433: step 16930, loss = 21.49, batch loss = 21.41 (9.0 examples/sec; 0.893 sec/batch; 78h:16m:55s remains)
INFO - root - 2017-12-07 12:42:14.316592: step 16940, loss = 21.58, batch loss = 21.50 (8.1 examples/sec; 0.990 sec/batch; 86h:47m:45s remains)
INFO - root - 2017-12-07 12:42:23.698278: step 16950, loss = 21.59, batch loss = 21.50 (8.2 examples/sec; 0.973 sec/batch; 85h:18m:42s remains)
INFO - root - 2017-12-07 12:42:33.146310: step 16960, loss = 21.49, batch loss = 21.41 (8.3 examples/sec; 0.962 sec/batch; 84h:20m:43s remains)
INFO - root - 2017-12-07 12:42:42.693718: step 16970, loss = 21.53, batch loss = 21.45 (8.5 examples/sec; 0.943 sec/batch; 82h:38m:56s remains)
INFO - root - 2017-12-07 12:42:52.122668: step 16980, loss = 21.20, batch loss = 21.11 (8.8 examples/sec; 0.908 sec/batch; 79h:35m:46s remains)
INFO - root - 2017-12-07 12:43:01.549136: step 16990, loss = 21.57, batch loss = 21.49 (8.6 examples/sec; 0.932 sec/batch; 81h:40m:37s remains)
INFO - root - 2017-12-07 12:43:11.076442: step 17000, loss = 21.50, batch loss = 21.42 (8.0 examples/sec; 0.994 sec/batch; 87h:09m:04s remains)
2017-12-07 12:43:12.124875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4697065 -4.5622158 -4.6576204 -4.7158518 -4.6904159 -4.5874329 -4.4540162 -4.3234463 -4.2418189 -4.2226062 -4.2751932 -4.4028668 -4.5128217 -4.5408597 -4.529058][-4.5170512 -4.5939794 -4.6880336 -4.7319946 -4.673408 -4.5489974 -4.4282045 -4.3351665 -4.2812347 -4.271636 -4.322453 -4.4401951 -4.5390844 -4.5610681 -4.5430861][-4.4686942 -4.5354185 -4.6261868 -4.6654005 -4.6014919 -4.4802303 -4.392241 -4.3589311 -4.3521061 -4.3521805 -4.3867879 -4.4837565 -4.5700169 -4.5838413 -4.5526695][-4.315064 -4.3981338 -4.5044641 -4.5568209 -4.5030875 -4.3862872 -4.3177824 -4.339314 -4.3899889 -4.4087572 -4.4320369 -4.5141997 -4.592411 -4.6006231 -4.5562921][-4.1369181 -4.2506027 -4.3759208 -4.431633 -4.3763218 -4.2490759 -4.1754751 -4.2279058 -4.3241086 -4.3659463 -4.3911166 -4.4683247 -4.5443258 -4.558876 -4.5218482][-4.0356913 -4.1646624 -4.2810822 -4.3046556 -4.22024 -4.0695081 -3.9764407 -4.0328074 -4.1489511 -4.2073565 -4.2481346 -4.3385029 -4.4272652 -4.4690042 -4.4646149][-4.0288229 -4.1550527 -4.2398129 -4.2078905 -4.084775 -3.9172978 -3.805963 -3.8457563 -3.9506078 -4.0089741 -4.07215 -4.1915421 -4.3118858 -4.3972592 -4.4366088][-4.0925012 -4.20828 -4.2528162 -4.16824 -4.0169182 -3.8489838 -3.7297316 -3.7487955 -3.8261776 -3.8726611 -3.955188 -4.1071582 -4.2643695 -4.39085 -4.4676619][-4.1958222 -4.2925529 -4.2993793 -4.1822968 -4.0224547 -3.874543 -3.7726212 -3.7906256 -3.8437724 -3.8676374 -3.9490323 -4.1128511 -4.2895193 -4.4355264 -4.5306339][-4.3148956 -4.381525 -4.3583922 -4.2379069 -4.0962086 -3.979424 -3.9076102 -3.9352093 -3.9708734 -3.9684143 -4.0270452 -4.1766777 -4.347764 -4.4878058 -4.577601][-4.4427142 -4.4781561 -4.4357762 -4.3291326 -4.2126851 -4.114913 -4.0593534 -4.0904322 -4.1182666 -4.1039295 -4.1400332 -4.2601337 -4.4063454 -4.5233092 -4.5919123][-4.5419965 -4.5616846 -4.5143147 -4.4283271 -4.3372893 -4.2529173 -4.2060924 -4.2346 -4.2636967 -4.2548409 -4.2746153 -4.3527541 -4.4562831 -4.5412092 -4.5828304][-4.5636778 -4.5843267 -4.5543976 -4.5061212 -4.4571724 -4.4024873 -4.371 -4.3945608 -4.4199152 -4.4128661 -4.4122987 -4.4413309 -4.4941297 -4.5458479 -4.5621033][-4.5084524 -4.5405426 -4.545496 -4.5458283 -4.5443993 -4.5243049 -4.5070953 -4.518075 -4.527051 -4.5102491 -4.4894733 -4.4838519 -4.504065 -4.5375791 -4.539948][-4.4174223 -4.4527159 -4.4823933 -4.5151162 -4.5413675 -4.5410557 -4.528975 -4.5242329 -4.51505 -4.488626 -4.4613352 -4.4512105 -4.4727359 -4.5133715 -4.5193305]]...]
INFO - root - 2017-12-07 12:43:21.513282: step 17010, loss = 21.83, batch loss = 21.75 (8.5 examples/sec; 0.943 sec/batch; 82h:36m:25s remains)
INFO - root - 2017-12-07 12:43:30.944109: step 17020, loss = 21.71, batch loss = 21.63 (8.7 examples/sec; 0.923 sec/batch; 80h:50m:47s remains)
INFO - root - 2017-12-07 12:43:40.353423: step 17030, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.938 sec/batch; 82h:12m:41s remains)
INFO - root - 2017-12-07 12:43:49.627826: step 17040, loss = 21.51, batch loss = 21.43 (9.5 examples/sec; 0.841 sec/batch; 73h:40m:44s remains)
INFO - root - 2017-12-07 12:43:59.260093: step 17050, loss = 21.92, batch loss = 21.83 (8.7 examples/sec; 0.923 sec/batch; 80h:54m:52s remains)
INFO - root - 2017-12-07 12:44:08.600889: step 17060, loss = 21.47, batch loss = 21.39 (8.5 examples/sec; 0.941 sec/batch; 82h:27m:56s remains)
INFO - root - 2017-12-07 12:44:17.845403: step 17070, loss = 21.41, batch loss = 21.33 (8.3 examples/sec; 0.964 sec/batch; 84h:27m:08s remains)
INFO - root - 2017-12-07 12:44:27.330509: step 17080, loss = 21.27, batch loss = 21.18 (8.1 examples/sec; 0.985 sec/batch; 86h:16m:10s remains)
INFO - root - 2017-12-07 12:44:36.721358: step 17090, loss = 21.81, batch loss = 21.72 (8.7 examples/sec; 0.924 sec/batch; 80h:59m:40s remains)
INFO - root - 2017-12-07 12:44:46.163824: step 17100, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.909 sec/batch; 79h:40m:17s remains)
2017-12-07 12:44:47.105641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4978695 -4.5047016 -4.523376 -4.5321784 -4.529964 -4.5283804 -4.53638 -4.550992 -4.5553007 -4.5301785 -4.4712753 -4.3930931 -4.3257508 -4.2899346 -4.2809014][-4.5155916 -4.531004 -4.5564933 -4.5714097 -4.5766268 -4.5743833 -4.5743027 -4.58494 -4.5867267 -4.5586839 -4.4984317 -4.4204078 -4.3512135 -4.3159976 -4.3096309][-4.5489211 -4.5662503 -4.5866785 -4.595809 -4.5988398 -4.5920234 -4.5804172 -4.5816956 -4.5808134 -4.5584316 -4.5181227 -4.463469 -4.4100795 -4.3842525 -4.3866763][-4.5569158 -4.5719728 -4.5781074 -4.5657864 -4.5440893 -4.5139847 -4.4822941 -4.4748411 -4.4858313 -4.493257 -4.4994268 -4.4882607 -4.4582257 -4.4397068 -4.4486179][-4.5230722 -4.5224838 -4.4963489 -4.4334989 -4.3576145 -4.285933 -4.2361813 -4.2378187 -4.2909474 -4.3541608 -4.4135008 -4.4383893 -4.4228592 -4.4049797 -4.4190483][-4.4440904 -4.4190025 -4.3540144 -4.2426434 -4.116931 -4.0137172 -3.9576776 -3.9844069 -4.0941091 -4.2127919 -4.3085551 -4.34893 -4.3325319 -4.3126607 -4.3365889][-4.3525953 -4.3077965 -4.2262182 -4.1004462 -3.9564905 -3.840404 -3.7841051 -3.8331747 -3.9921691 -4.1492739 -4.2528172 -4.2741394 -4.2286868 -4.1930814 -4.2213769][-4.3132124 -4.2646313 -4.1924763 -4.087492 -3.9543917 -3.8407056 -3.7865005 -3.8403747 -4.012156 -4.17415 -4.2577372 -4.2339211 -4.1367731 -4.072402 -4.1004181][-4.3290467 -4.2888236 -4.2406669 -4.1747947 -4.07414 -3.9750018 -3.9229891 -3.9700618 -4.1160774 -4.2541485 -4.30797 -4.2421994 -4.0961132 -3.9948943 -4.0109491][-4.3362427 -4.3080692 -4.2878489 -4.2627249 -4.2050209 -4.1287055 -4.0872931 -4.1321607 -4.2410226 -4.3422909 -4.3648891 -4.269805 -4.0919876 -3.9610174 -3.9649718][-4.3201032 -4.2957072 -4.2867737 -4.2913036 -4.280448 -4.2420669 -4.2238932 -4.2716088 -4.3518548 -4.4208379 -4.4189129 -4.3087268 -4.1328521 -4.0122128 -4.0276794][-4.3210077 -4.2898288 -4.2770238 -4.2979879 -4.3329649 -4.3422556 -4.3535681 -4.3979206 -4.4512205 -4.49623 -4.4821424 -4.3730659 -4.2261171 -4.1488671 -4.1920714][-4.3599329 -4.3293557 -4.3208547 -4.3597445 -4.4219151 -4.4549594 -4.4733491 -4.4983635 -4.5272732 -4.5571718 -4.5390615 -4.442749 -4.330411 -4.2908049 -4.3480439][-4.423831 -4.3981705 -4.4001956 -4.4493823 -4.5124912 -4.53582 -4.538569 -4.5417261 -4.5575485 -4.582376 -4.5708704 -4.5008531 -4.4255619 -4.4045582 -4.4486604][-4.4882627 -4.462954 -4.46842 -4.5175438 -4.5686431 -4.5731034 -4.5580521 -4.5480785 -4.5643897 -4.5951123 -4.5982685 -4.5623412 -4.5226312 -4.5109868 -4.532752]]...]
INFO - root - 2017-12-07 12:44:56.645394: step 17110, loss = 21.43, batch loss = 21.35 (8.0 examples/sec; 0.997 sec/batch; 87h:20m:18s remains)
INFO - root - 2017-12-07 12:45:06.134143: step 17120, loss = 21.29, batch loss = 21.20 (8.7 examples/sec; 0.924 sec/batch; 80h:54m:42s remains)
INFO - root - 2017-12-07 12:45:15.727951: step 17130, loss = 21.58, batch loss = 21.49 (8.6 examples/sec; 0.932 sec/batch; 81h:38m:45s remains)
INFO - root - 2017-12-07 12:45:25.055715: step 17140, loss = 21.84, batch loss = 21.76 (8.5 examples/sec; 0.940 sec/batch; 82h:19m:06s remains)
INFO - root - 2017-12-07 12:45:34.478424: step 17150, loss = 21.27, batch loss = 21.18 (8.2 examples/sec; 0.981 sec/batch; 85h:54m:25s remains)
INFO - root - 2017-12-07 12:45:43.864796: step 17160, loss = 21.36, batch loss = 21.27 (8.8 examples/sec; 0.913 sec/batch; 79h:56m:48s remains)
INFO - root - 2017-12-07 12:45:53.322883: step 17170, loss = 21.54, batch loss = 21.46 (9.0 examples/sec; 0.890 sec/batch; 77h:59m:41s remains)
INFO - root - 2017-12-07 12:46:02.827755: step 17180, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.937 sec/batch; 82h:02m:11s remains)
INFO - root - 2017-12-07 12:46:12.373306: step 17190, loss = 21.41, batch loss = 21.33 (8.4 examples/sec; 0.955 sec/batch; 83h:37m:13s remains)
INFO - root - 2017-12-07 12:46:21.885915: step 17200, loss = 21.71, batch loss = 21.62 (8.1 examples/sec; 0.991 sec/batch; 86h:47m:41s remains)
2017-12-07 12:46:22.810348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3434687 -4.3962922 -4.4665594 -4.5221519 -4.525332 -4.4693961 -4.3897619 -4.3435588 -4.3604107 -4.4227028 -4.4773731 -4.4866238 -4.4524651 -4.40062 -4.3574777][-4.3431187 -4.4006929 -4.4755926 -4.5271549 -4.5077 -4.409266 -4.2775 -4.1914248 -4.2040405 -4.3035755 -4.4180565 -4.480628 -4.4768491 -4.4325657 -4.3822575][-4.3395433 -4.3978844 -4.4719129 -4.5145035 -4.4705987 -4.3335238 -4.1538563 -4.0232935 -4.0181065 -4.1429758 -4.3133268 -4.4331703 -4.4678845 -4.441277 -4.394803][-4.338666 -4.3940983 -4.4618306 -4.4916115 -4.4262528 -4.2620687 -4.0497975 -3.8809865 -3.8505986 -3.9870958 -4.2023315 -4.3736558 -4.4450431 -4.4371986 -4.3986359][-4.342279 -4.3907166 -4.4465532 -4.4614935 -4.3817158 -4.2062507 -3.982461 -3.7908487 -3.7354093 -3.8717685 -4.1187758 -4.3324828 -4.4347043 -4.4399934 -4.4050016][-4.3506527 -4.3866491 -4.4257326 -4.4264655 -4.3425512 -4.1725469 -3.9580877 -3.7627983 -3.686986 -3.8108504 -4.0722089 -4.31708 -4.445343 -4.4596195 -4.4218721][-4.3745914 -4.3897095 -4.4053936 -4.391818 -4.3112464 -4.1566553 -3.9626181 -3.7773457 -3.6881657 -3.7893829 -4.0429244 -4.3007274 -4.4517655 -4.4798636 -4.442378][-4.4276466 -4.4131079 -4.3997507 -4.3721514 -4.2995415 -4.1645908 -3.991246 -3.8177166 -3.7170336 -3.784852 -4.0073991 -4.258213 -4.4276853 -4.4783306 -4.4515953][-4.4887419 -4.4502478 -4.4123378 -4.3755946 -4.3152165 -4.2026281 -4.0458384 -3.8762009 -3.7593892 -3.7869215 -3.9634492 -4.1895804 -4.3673286 -4.442358 -4.4358706][-4.5246134 -4.4786816 -4.4328833 -4.3974991 -4.3535376 -4.26467 -4.122333 -3.9488659 -3.8097777 -3.7964516 -3.9272964 -4.1222248 -4.2938895 -4.3830943 -4.3964839][-4.5212836 -4.4840465 -4.4469843 -4.4230795 -4.3968945 -4.332119 -4.20807 -4.0351586 -3.8782163 -3.8312719 -3.9250374 -4.0911412 -4.2465153 -4.3331079 -4.3547306][-4.4825726 -4.4611759 -4.4393234 -4.4286127 -4.4155931 -4.3727612 -4.27872 -4.1294403 -3.9778404 -3.911963 -3.9747195 -4.1123972 -4.2456932 -4.3175578 -4.3336039][-4.431097 -4.4244576 -4.4126697 -4.4066916 -4.3967919 -4.369832 -4.3136148 -4.21379 -4.1004462 -4.0399632 -4.0789709 -4.1846809 -4.2882876 -4.3382587 -4.3397827][-4.404242 -4.4031758 -4.3899193 -4.3768611 -4.3615041 -4.3429036 -4.3187437 -4.2736564 -4.2157807 -4.1816435 -4.2086973 -4.28155 -4.350307 -4.3747644 -4.3610315][-4.4207435 -4.4107437 -4.3847957 -4.3577857 -4.3337865 -4.3167281 -4.3093786 -4.3020468 -4.2919359 -4.2910748 -4.317687 -4.3618989 -4.3961978 -4.3992319 -4.3769908]]...]
INFO - root - 2017-12-07 12:46:32.251455: step 17210, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.953 sec/batch; 83h:27m:09s remains)
INFO - root - 2017-12-07 12:46:41.714484: step 17220, loss = 21.65, batch loss = 21.57 (8.4 examples/sec; 0.956 sec/batch; 83h:41m:54s remains)
INFO - root - 2017-12-07 12:46:51.189558: step 17230, loss = 21.47, batch loss = 21.39 (7.9 examples/sec; 1.012 sec/batch; 88h:37m:22s remains)
INFO - root - 2017-12-07 12:47:00.467142: step 17240, loss = 21.35, batch loss = 21.27 (9.0 examples/sec; 0.889 sec/batch; 77h:52m:28s remains)
INFO - root - 2017-12-07 12:47:09.807955: step 17250, loss = 21.58, batch loss = 21.49 (8.2 examples/sec; 0.975 sec/batch; 85h:23m:06s remains)
INFO - root - 2017-12-07 12:47:19.387656: step 17260, loss = 21.23, batch loss = 21.14 (8.3 examples/sec; 0.968 sec/batch; 84h:47m:30s remains)
INFO - root - 2017-12-07 12:47:28.909985: step 17270, loss = 21.14, batch loss = 21.06 (7.9 examples/sec; 1.017 sec/batch; 89h:01m:34s remains)
INFO - root - 2017-12-07 12:47:38.390134: step 17280, loss = 21.37, batch loss = 21.29 (8.1 examples/sec; 0.989 sec/batch; 86h:33m:28s remains)
INFO - root - 2017-12-07 12:47:47.752197: step 17290, loss = 21.47, batch loss = 21.38 (9.4 examples/sec; 0.851 sec/batch; 74h:30m:13s remains)
INFO - root - 2017-12-07 12:47:57.045662: step 17300, loss = 21.54, batch loss = 21.45 (8.9 examples/sec; 0.894 sec/batch; 78h:15m:48s remains)
2017-12-07 12:47:57.973583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.46148 -4.4200816 -4.3756828 -4.3522964 -4.3698649 -4.4372125 -4.5347872 -4.5966806 -4.58277 -4.51548 -4.4358544 -4.3872266 -4.3760362 -4.3805218 -4.3519034][-4.5463142 -4.5117717 -4.4709897 -4.4495139 -4.4540825 -4.4938312 -4.5671606 -4.6247392 -4.6141138 -4.5296388 -4.4122896 -4.3377142 -4.3209696 -4.3323627 -4.3215194][-4.6124959 -4.573401 -4.518764 -4.4821854 -4.4609647 -4.4632096 -4.5048871 -4.56336 -4.584528 -4.5280766 -4.40636 -4.313067 -4.2835441 -4.2949133 -4.3029189][-4.6460528 -4.5779734 -4.4885883 -4.4220071 -4.37471 -4.3487659 -4.3603096 -4.4158993 -4.4772878 -4.4823294 -4.4025245 -4.318491 -4.2784224 -4.2795138 -4.2982507][-4.6563592 -4.52989 -4.3795471 -4.2641683 -4.1903996 -4.1535439 -4.1533365 -4.2094541 -4.30677 -4.3852086 -4.3835325 -4.3478813 -4.3217978 -4.320715 -4.3440785][-4.6619039 -4.463129 -4.2347946 -4.0542746 -3.9514863 -3.9234576 -3.9382529 -4.0027528 -4.12039 -4.2475581 -4.3147783 -4.333921 -4.339726 -4.3552432 -4.39087][-4.6423492 -4.4032879 -4.1296234 -3.9091585 -3.7923717 -3.7836576 -3.8238854 -3.8900948 -4.0056238 -4.151041 -4.2629523 -4.3288264 -4.3728385 -4.4117627 -4.4534073][-4.567708 -4.3269253 -4.0552359 -3.8346434 -3.7219365 -3.7320352 -3.7928455 -3.8510714 -3.948843 -4.089292 -4.221868 -4.3262506 -4.4158335 -4.4827437 -4.5243483][-4.4372993 -4.2336335 -4.0261145 -3.861115 -3.7788086 -3.8033338 -3.8686445 -3.9087946 -3.9733 -4.0780668 -4.1918044 -4.308485 -4.4294715 -4.5136566 -4.5506163][-4.312942 -4.1855087 -4.0889072 -4.0135937 -3.9721804 -3.9974031 -4.0495348 -4.0712018 -4.1064072 -4.1742973 -4.254643 -4.3572021 -4.4709916 -4.5415511 -4.5632925][-4.2471094 -4.1970263 -4.1946669 -4.1908851 -4.182694 -4.2115273 -4.265481 -4.2921629 -4.3193893 -4.3603849 -4.4007573 -4.4596214 -4.529582 -4.568078 -4.5746222][-4.2062173 -4.2127566 -4.2690516 -4.3124695 -4.3481307 -4.4131846 -4.4988513 -4.5526285 -4.5851293 -4.6004996 -4.5928006 -4.5882483 -4.5922933 -4.5898728 -4.5805812][-4.1852932 -4.2337437 -4.3126607 -4.3671513 -4.4231567 -4.5123434 -4.6284652 -4.7214637 -4.778801 -4.786006 -4.7437787 -4.6896667 -4.6435 -4.6078925 -4.5795107][-4.1487751 -4.2256007 -4.3078632 -4.3547344 -4.3944678 -4.468401 -4.5899096 -4.7157173 -4.8029261 -4.8075247 -4.7372646 -4.654058 -4.5887237 -4.547008 -4.5190778][-4.0834823 -4.1798153 -4.2718978 -4.3265929 -4.35262 -4.4037328 -4.513176 -4.6437321 -4.7304745 -4.7159519 -4.6180372 -4.5178871 -4.4490347 -4.4158993 -4.4048162]]...]
INFO - root - 2017-12-07 12:48:07.367638: step 17310, loss = 21.80, batch loss = 21.72 (8.4 examples/sec; 0.950 sec/batch; 83h:08m:06s remains)
INFO - root - 2017-12-07 12:48:16.707027: step 17320, loss = 21.59, batch loss = 21.51 (8.7 examples/sec; 0.920 sec/batch; 80h:35m:13s remains)
INFO - root - 2017-12-07 12:48:26.127138: step 17330, loss = 21.51, batch loss = 21.43 (8.8 examples/sec; 0.911 sec/batch; 79h:43m:09s remains)
INFO - root - 2017-12-07 12:48:35.426074: step 17340, loss = 21.01, batch loss = 20.92 (8.5 examples/sec; 0.946 sec/batch; 82h:48m:18s remains)
INFO - root - 2017-12-07 12:48:44.666368: step 17350, loss = 21.50, batch loss = 21.42 (8.7 examples/sec; 0.920 sec/batch; 80h:30m:00s remains)
INFO - root - 2017-12-07 12:48:54.121221: step 17360, loss = 21.27, batch loss = 21.19 (8.8 examples/sec; 0.909 sec/batch; 79h:33m:11s remains)
INFO - root - 2017-12-07 12:49:03.621541: step 17370, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.946 sec/batch; 82h:46m:37s remains)
INFO - root - 2017-12-07 12:49:13.050338: step 17380, loss = 21.83, batch loss = 21.75 (8.4 examples/sec; 0.955 sec/batch; 83h:35m:19s remains)
INFO - root - 2017-12-07 12:49:22.428793: step 17390, loss = 21.34, batch loss = 21.26 (8.7 examples/sec; 0.921 sec/batch; 80h:36m:50s remains)
INFO - root - 2017-12-07 12:49:31.886030: step 17400, loss = 21.67, batch loss = 21.59 (8.7 examples/sec; 0.921 sec/batch; 80h:36m:42s remains)
2017-12-07 12:49:32.817928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7042189 -4.6605554 -4.6013355 -4.520236 -4.4215817 -4.3744521 -4.4129977 -4.5215144 -4.6518674 -4.7548752 -4.8028064 -4.7962627 -4.7348342 -4.6712127 -4.6392961][-4.6477776 -4.542099 -4.427475 -4.3247328 -4.250875 -4.2506919 -4.3481803 -4.5139251 -4.6822329 -4.805006 -4.8490181 -4.8140755 -4.7196174 -4.6356578 -4.5982275][-4.641643 -4.4986329 -4.3278832 -4.1822071 -4.1011891 -4.1194015 -4.2486029 -4.4483676 -4.6399012 -4.778791 -4.8225522 -4.7636342 -4.6459942 -4.5477371 -4.5044327][-4.6686559 -4.52164 -4.3168454 -4.1301084 -4.020083 -4.0210114 -4.1406116 -4.336256 -4.5308628 -4.678236 -4.7248697 -4.6595516 -4.5422964 -4.449367 -4.4132175][-4.6765561 -4.527597 -4.3051219 -4.0895567 -3.9462345 -3.914923 -4.0062089 -4.1784744 -4.368073 -4.52081 -4.5752721 -4.5263796 -4.4443407 -4.388855 -4.3801742][-4.6457233 -4.4800854 -4.238852 -3.9983802 -3.8286242 -3.7759905 -3.8424845 -3.9908133 -4.1706166 -4.3228517 -4.3885188 -4.3780394 -4.3565235 -4.3571243 -4.3905849][-4.6065669 -4.4249172 -4.1698036 -3.9098215 -3.7282202 -3.6776497 -3.7443912 -3.8869724 -4.053947 -4.1910453 -4.2589221 -4.2836919 -4.3114595 -4.3539929 -4.4201245][-4.6035709 -4.4399219 -4.2009344 -3.9431822 -3.76266 -3.7238743 -3.8056283 -3.9529757 -4.0975189 -4.1948009 -4.2382689 -4.270072 -4.3181748 -4.3744612 -4.4441962][-4.6487694 -4.550971 -4.3697777 -4.1401043 -3.9617474 -3.9126327 -3.9860268 -4.120615 -4.2283649 -4.2733874 -4.278955 -4.3016515 -4.34949 -4.3980951 -4.4525852][-4.699832 -4.6848006 -4.5809832 -4.39825 -4.2281308 -4.15787 -4.2007747 -4.3035307 -4.3744941 -4.3839464 -4.3710217 -4.3827305 -4.4116697 -4.4338589 -4.4568429][-4.7071242 -4.7585421 -4.729023 -4.6087093 -4.4703422 -4.3932624 -4.4078341 -4.4820733 -4.5328174 -4.5324368 -4.5187159 -4.5177469 -4.517838 -4.50663 -4.4983106][-4.6541338 -4.7406697 -4.76581 -4.7097144 -4.6182861 -4.5525155 -4.5514045 -4.6034822 -4.643002 -4.6433606 -4.6353517 -4.632462 -4.6213994 -4.5930719 -4.5640163][-4.5576625 -4.6449127 -4.6942806 -4.6858149 -4.6402206 -4.5976305 -4.595058 -4.6297626 -4.6567092 -4.6557961 -4.6514421 -4.654345 -4.6496763 -4.6242056 -4.5879335][-4.4445767 -4.5080795 -4.5524673 -4.5667815 -4.5571661 -4.5429664 -4.5478921 -4.5703359 -4.5851183 -4.5830731 -4.5812917 -4.5873432 -4.5883851 -4.5709572 -4.5392413][-4.3457637 -4.3780851 -4.4042773 -4.420444 -4.4277864 -4.4323692 -4.442935 -4.4567409 -4.4635715 -4.4616842 -4.4606557 -4.4644318 -4.4654012 -4.4555264 -4.436532]]...]
INFO - root - 2017-12-07 12:49:42.273639: step 17410, loss = 21.11, batch loss = 21.03 (8.4 examples/sec; 0.950 sec/batch; 83h:06m:46s remains)
INFO - root - 2017-12-07 12:49:51.739213: step 17420, loss = 21.77, batch loss = 21.68 (8.5 examples/sec; 0.940 sec/batch; 82h:15m:25s remains)
INFO - root - 2017-12-07 12:50:01.286715: step 17430, loss = 21.34, batch loss = 21.26 (7.9 examples/sec; 1.012 sec/batch; 88h:32m:11s remains)
INFO - root - 2017-12-07 12:50:10.654203: step 17440, loss = 21.68, batch loss = 21.60 (8.4 examples/sec; 0.950 sec/batch; 83h:05m:54s remains)
INFO - root - 2017-12-07 12:50:19.962695: step 17450, loss = 21.34, batch loss = 21.25 (7.9 examples/sec; 1.013 sec/batch; 88h:38m:35s remains)
INFO - root - 2017-12-07 12:50:29.375589: step 17460, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.953 sec/batch; 83h:24m:00s remains)
INFO - root - 2017-12-07 12:50:38.918900: step 17470, loss = 21.34, batch loss = 21.25 (7.8 examples/sec; 1.020 sec/batch; 89h:13m:37s remains)
INFO - root - 2017-12-07 12:50:48.441236: step 17480, loss = 21.64, batch loss = 21.56 (8.3 examples/sec; 0.965 sec/batch; 84h:24m:19s remains)
INFO - root - 2017-12-07 12:50:57.858988: step 17490, loss = 21.62, batch loss = 21.54 (9.0 examples/sec; 0.892 sec/batch; 78h:03m:25s remains)
INFO - root - 2017-12-07 12:51:07.314031: step 17500, loss = 21.68, batch loss = 21.60 (9.0 examples/sec; 0.885 sec/batch; 77h:24m:29s remains)
2017-12-07 12:51:08.310516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.448195 -4.4651985 -4.48238 -4.5004959 -4.5244141 -4.5364804 -4.5276141 -4.5092483 -4.4886661 -4.4657941 -4.4420047 -4.4193525 -4.3986335 -4.3780336 -4.358222][-4.5289822 -4.5585661 -4.5771804 -4.595428 -4.6280608 -4.6450319 -4.6195703 -4.5813155 -4.5632429 -4.5640678 -4.564539 -4.551383 -4.5256934 -4.49095 -4.4509926][-4.6177449 -4.6701226 -4.688858 -4.6928873 -4.7196617 -4.7345657 -4.6847153 -4.6151805 -4.5986323 -4.6423807 -4.6932406 -4.7093124 -4.6886544 -4.6440253 -4.5844207][-4.6765208 -4.7722082 -4.802382 -4.7811179 -4.7774649 -4.7779045 -4.7054439 -4.6057458 -4.5916491 -4.6855545 -4.7938337 -4.84352 -4.8284717 -4.7720051 -4.6911945][-4.6168823 -4.7746725 -4.844152 -4.81353 -4.7733688 -4.7410884 -4.630702 -4.4985189 -4.4958162 -4.649147 -4.8202014 -4.9038377 -4.8918114 -4.813077 -4.6942754][-4.3993049 -4.619071 -4.7467837 -4.741446 -4.6832318 -4.5969853 -4.4028716 -4.2065554 -4.2183242 -4.4468675 -4.6997395 -4.8390088 -4.8459587 -4.7385993 -4.5478396][-4.1219587 -4.3641186 -4.5267668 -4.5639329 -4.5153913 -4.3706794 -4.0608134 -3.7733316 -3.8017106 -4.1201963 -4.4703884 -4.6863346 -4.7338319 -4.6037321 -4.3207355][-3.9358644 -4.1476927 -4.3003073 -4.3721066 -4.3563828 -4.1815987 -3.7837255 -3.423315 -3.4658575 -3.8582339 -4.2830877 -4.5606346 -4.6513047 -4.5153122 -4.15938][-3.9797485 -4.130393 -4.2377667 -4.3187771 -4.345015 -4.1954789 -3.7967739 -3.4217291 -3.453002 -3.8490682 -4.2787852 -4.568254 -4.6856027 -4.5678239 -4.1978459][-4.19971 -4.2916627 -4.3447194 -4.4117718 -4.4739408 -4.3898296 -4.0749774 -3.747298 -3.7404795 -4.05324 -4.3992743 -4.6392837 -4.7583852 -4.6858191 -4.3788881][-4.4597988 -4.5036597 -4.5013714 -4.5344696 -4.610301 -4.59752 -4.40433 -4.1604195 -4.1158905 -4.307467 -4.5235882 -4.6719723 -4.7697482 -4.7543259 -4.5637274][-4.6357274 -4.6420183 -4.5981121 -4.5969958 -4.6671505 -4.6997628 -4.6106162 -4.4594007 -4.4063611 -4.5001912 -4.6002264 -4.6589761 -4.7212353 -4.747673 -4.6673136][-4.6926818 -4.6736364 -4.6140313 -4.5974288 -4.6532927 -4.6987367 -4.6713438 -4.5971656 -4.5635753 -4.60708 -4.6399384 -4.642643 -4.6696286 -4.7038579 -4.6820059][-4.7203932 -4.689199 -4.6355233 -4.6200132 -4.6621456 -4.7029724 -4.7042522 -4.6802773 -4.6716442 -4.6969333 -4.7074571 -4.6948209 -4.6991897 -4.7162747 -4.70768][-4.7415895 -4.709713 -4.6708708 -4.6614518 -4.6894865 -4.7192073 -4.7302814 -4.7311382 -4.7370739 -4.7543855 -4.76189 -4.7548308 -4.7512088 -4.7500043 -4.7370014]]...]
INFO - root - 2017-12-07 12:51:17.734945: step 17510, loss = 21.39, batch loss = 21.31 (8.6 examples/sec; 0.925 sec/batch; 80h:57m:46s remains)
INFO - root - 2017-12-07 12:51:27.151894: step 17520, loss = 21.11, batch loss = 21.03 (8.7 examples/sec; 0.917 sec/batch; 80h:14m:46s remains)
INFO - root - 2017-12-07 12:51:36.479721: step 17530, loss = 21.31, batch loss = 21.23 (8.3 examples/sec; 0.963 sec/batch; 84h:13m:58s remains)
INFO - root - 2017-12-07 12:51:45.969467: step 17540, loss = 21.70, batch loss = 21.62 (8.4 examples/sec; 0.949 sec/batch; 83h:00m:51s remains)
INFO - root - 2017-12-07 12:51:55.399387: step 17550, loss = 21.39, batch loss = 21.30 (8.7 examples/sec; 0.924 sec/batch; 80h:48m:09s remains)
INFO - root - 2017-12-07 12:52:04.953263: step 17560, loss = 21.27, batch loss = 21.19 (8.2 examples/sec; 0.970 sec/batch; 84h:51m:23s remains)
INFO - root - 2017-12-07 12:52:14.261005: step 17570, loss = 21.85, batch loss = 21.77 (8.5 examples/sec; 0.938 sec/batch; 82h:03m:28s remains)
INFO - root - 2017-12-07 12:52:23.733498: step 17580, loss = 21.48, batch loss = 21.40 (8.5 examples/sec; 0.946 sec/batch; 82h:45m:43s remains)
INFO - root - 2017-12-07 12:52:33.092238: step 17590, loss = 21.87, batch loss = 21.79 (7.8 examples/sec; 1.024 sec/batch; 89h:32m:17s remains)
INFO - root - 2017-12-07 12:52:42.611811: step 17600, loss = 21.76, batch loss = 21.68 (8.6 examples/sec; 0.925 sec/batch; 80h:55m:20s remains)
2017-12-07 12:52:43.525004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3742871 -4.416512 -4.4456325 -4.4544115 -4.4456754 -4.4291964 -4.4019055 -4.3634548 -4.3254538 -4.3151188 -4.3426185 -4.3903012 -4.4348636 -4.46412 -4.4586363][-4.3617997 -4.4136558 -4.4558167 -4.4765 -4.4752126 -4.4516397 -4.4012103 -4.3370647 -4.2804446 -4.2668152 -4.3016424 -4.3632307 -4.4228272 -4.4684148 -4.47569][-4.3076282 -4.3527651 -4.39814 -4.4346609 -4.4556084 -4.4374804 -4.3692622 -4.28025 -4.2087212 -4.195498 -4.239645 -4.3146315 -4.3895907 -4.4534993 -4.4781327][-4.2274194 -4.2521987 -4.2869034 -4.3290057 -4.3661876 -4.3575482 -4.2854986 -4.1841063 -4.1059222 -4.0906734 -4.1359653 -4.2172413 -4.3075285 -4.3984861 -4.4503117][-4.160974 -4.1546106 -4.1636443 -4.1935019 -4.2277761 -4.2240453 -4.1644278 -4.0751338 -4.0085611 -3.9948871 -4.0294886 -4.103416 -4.2043757 -4.3275633 -4.413353][-4.1410728 -4.0960131 -4.0699129 -4.0757127 -4.0921021 -4.0849447 -4.0407977 -3.9796445 -3.943558 -3.9481237 -3.9845223 -4.0533466 -4.1607084 -4.3050938 -4.4122777][-4.1564369 -4.0701251 -4.0094204 -3.9986274 -4.0081244 -4.0018778 -3.9670081 -3.9234166 -3.9042523 -3.9226429 -3.9721873 -4.0531325 -4.174242 -4.3294559 -4.4418449][-4.1663613 -4.0406232 -3.9558828 -3.9487095 -3.9781618 -3.9939549 -3.9652228 -3.9135416 -3.8758817 -3.8792539 -3.9315593 -4.026907 -4.1671252 -4.3336959 -4.4518051][-4.1805177 -4.0361609 -3.9507542 -3.9619424 -4.0191526 -4.0593152 -4.03113 -3.9551759 -3.8844807 -3.8607774 -3.9021134 -3.9955325 -4.1357751 -4.2991138 -4.4217181][-4.2345424 -4.1036696 -4.0411215 -4.0769711 -4.1540351 -4.2034473 -4.1637115 -4.0598207 -3.9617877 -3.921761 -3.9526894 -4.0314503 -4.1482854 -4.2829065 -4.3906684][-4.2951527 -4.1948495 -4.1693587 -4.2396278 -4.3418612 -4.400394 -4.3467941 -4.2183971 -4.0974231 -4.0429759 -4.0617061 -4.1263852 -4.2224879 -4.3265843 -4.4066997][-4.3448005 -4.2819648 -4.2917595 -4.3890343 -4.5086365 -4.5752378 -4.5189195 -4.3822861 -4.2525964 -4.1893535 -4.1994381 -4.2589884 -4.3478646 -4.4324865 -4.4814973][-4.39332 -4.3675637 -4.4032345 -4.5054083 -4.6152072 -4.6733942 -4.6207123 -4.4962788 -4.3808775 -4.3260422 -4.337491 -4.3946972 -4.4731894 -4.5366488 -4.5548558][-4.4272165 -4.4251127 -4.4696503 -4.5586138 -4.6469965 -4.6930757 -4.6528926 -4.5538816 -4.4624219 -4.4222732 -4.4357457 -4.4819856 -4.5353546 -4.5701594 -4.5637836][-4.43524 -4.4370565 -4.4710183 -4.5354719 -4.6045375 -4.6481738 -4.633275 -4.5691242 -4.5015321 -4.4663072 -4.4704103 -4.4955354 -4.5206342 -4.5309348 -4.5121312]]...]
INFO - root - 2017-12-07 12:52:52.866429: step 17610, loss = 21.31, batch loss = 21.23 (8.6 examples/sec; 0.932 sec/batch; 81h:32m:23s remains)
INFO - root - 2017-12-07 12:53:02.350740: step 17620, loss = 21.58, batch loss = 21.50 (8.3 examples/sec; 0.960 sec/batch; 83h:56m:14s remains)
INFO - root - 2017-12-07 12:53:11.816634: step 17630, loss = 22.09, batch loss = 22.00 (8.3 examples/sec; 0.959 sec/batch; 83h:52m:19s remains)
INFO - root - 2017-12-07 12:53:21.325171: step 17640, loss = 21.40, batch loss = 21.32 (8.7 examples/sec; 0.918 sec/batch; 80h:18m:06s remains)
INFO - root - 2017-12-07 12:53:30.645865: step 17650, loss = 21.25, batch loss = 21.17 (8.5 examples/sec; 0.941 sec/batch; 82h:18m:06s remains)
INFO - root - 2017-12-07 12:53:40.362249: step 17660, loss = 21.58, batch loss = 21.50 (8.5 examples/sec; 0.945 sec/batch; 82h:38m:36s remains)
INFO - root - 2017-12-07 12:53:49.858091: step 17670, loss = 21.60, batch loss = 21.52 (8.5 examples/sec; 0.942 sec/batch; 82h:25m:06s remains)
INFO - root - 2017-12-07 12:53:59.345447: step 17680, loss = 21.49, batch loss = 21.41 (7.9 examples/sec; 1.013 sec/batch; 88h:35m:00s remains)
INFO - root - 2017-12-07 12:54:08.818170: step 17690, loss = 21.43, batch loss = 21.35 (8.1 examples/sec; 0.991 sec/batch; 86h:38m:14s remains)
INFO - root - 2017-12-07 12:54:18.172415: step 17700, loss = 21.72, batch loss = 21.64 (8.1 examples/sec; 0.986 sec/batch; 86h:13m:27s remains)
2017-12-07 12:54:19.072706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33184 -4.33558 -4.2929869 -4.2178288 -4.1607594 -4.1622558 -4.188808 -4.21248 -4.24647 -4.2914982 -4.3483672 -4.4219718 -4.4655547 -4.4354572 -4.3522015][-4.2258263 -4.2184448 -4.1708856 -4.095521 -4.0493808 -4.0628748 -4.101366 -4.1394176 -4.1903558 -4.2522588 -4.3288813 -4.4238653 -4.4808793 -4.4469786 -4.346838][-4.1861887 -4.1702347 -4.1117959 -4.0307975 -3.9946663 -4.0228553 -4.0790572 -4.1360817 -4.2018533 -4.2770815 -4.360908 -4.451745 -4.5020103 -4.4704633 -4.380743][-4.149056 -4.1358094 -4.0767984 -3.9957223 -3.9648612 -3.9945679 -4.0527363 -4.1144681 -4.1819439 -4.258184 -4.3338189 -4.4033065 -4.4437089 -4.4338737 -4.3776779][-4.1445661 -4.13046 -4.0760303 -4.0037966 -3.9778752 -4.00192 -4.0535941 -4.1052732 -4.157135 -4.2146473 -4.2683444 -4.3115644 -4.3450532 -4.36295 -4.3500142][-4.1683755 -4.1432405 -4.0922613 -4.0286283 -4.00156 -4.0171108 -4.0602288 -4.1021924 -4.13709 -4.1669192 -4.1842747 -4.1938457 -4.2222219 -4.2693563 -4.308671][-4.1446424 -4.1144028 -4.0829229 -4.04025 -4.0126028 -4.0120692 -4.0331693 -4.0608082 -4.0846004 -4.0974746 -4.0944676 -4.0895858 -4.1157465 -4.1683283 -4.2324133][-4.102695 -4.0627761 -4.0534983 -4.0418863 -4.0299826 -4.0233541 -4.0247164 -4.0372481 -4.0495334 -4.0512943 -4.0484958 -4.0523248 -4.0825968 -4.1202507 -4.1680403][-4.0952559 -4.0438738 -4.0443959 -4.05627 -4.0616832 -4.057127 -4.0542359 -4.0670776 -4.0725689 -4.055748 -4.0431714 -4.0451274 -4.0745893 -4.1021295 -4.1364484][-4.1274309 -4.0757127 -4.0843086 -4.1065421 -4.1140728 -4.1014934 -4.0943327 -4.1149831 -4.1249695 -4.1046596 -4.0862894 -4.0754685 -4.0871024 -4.1051745 -4.1345105][-4.1741452 -4.1347661 -4.1471438 -4.1654725 -4.1647768 -4.1389413 -4.1271014 -4.1546168 -4.1761703 -4.1733274 -4.1657057 -4.1424932 -4.1269307 -4.1245718 -4.1438293][-4.1977673 -4.1795812 -4.1928167 -4.2025366 -4.1981206 -4.1728053 -4.1638 -4.1966681 -4.2220192 -4.2291546 -4.2245321 -4.1872025 -4.151051 -4.1334262 -4.1454926][-4.1642709 -4.1841331 -4.2136612 -4.221827 -4.2178483 -4.200706 -4.1999564 -4.2364006 -4.2631965 -4.2737646 -4.26484 -4.21523 -4.1668005 -4.1400657 -4.1453776][-4.1179671 -4.1650085 -4.2122984 -4.2287183 -4.2336168 -4.2233725 -4.21931 -4.2437048 -4.26837 -4.2858138 -4.2789168 -4.2320681 -4.1896586 -4.1648679 -4.1673665][-4.1026621 -4.1519213 -4.2036443 -4.2303128 -4.2501512 -4.2427921 -4.223702 -4.2196231 -4.2309618 -4.2484303 -4.2476659 -4.21679 -4.1908441 -4.1837859 -4.2021985]]...]
INFO - root - 2017-12-07 12:54:28.478065: step 17710, loss = 21.60, batch loss = 21.52 (8.5 examples/sec; 0.939 sec/batch; 82h:07m:53s remains)
INFO - root - 2017-12-07 12:54:37.900583: step 17720, loss = 21.46, batch loss = 21.37 (9.1 examples/sec; 0.880 sec/batch; 76h:55m:22s remains)
INFO - root - 2017-12-07 12:54:47.350588: step 17730, loss = 21.39, batch loss = 21.31 (9.0 examples/sec; 0.893 sec/batch; 78h:05m:51s remains)
INFO - root - 2017-12-07 12:54:56.818472: step 17740, loss = 21.27, batch loss = 21.19 (8.4 examples/sec; 0.956 sec/batch; 83h:36m:43s remains)
INFO - root - 2017-12-07 12:55:06.228566: step 17750, loss = 21.66, batch loss = 21.58 (8.8 examples/sec; 0.910 sec/batch; 79h:33m:40s remains)
INFO - root - 2017-12-07 12:55:15.621531: step 17760, loss = 21.33, batch loss = 21.25 (8.1 examples/sec; 0.992 sec/batch; 86h:41m:17s remains)
INFO - root - 2017-12-07 12:55:25.018188: step 17770, loss = 21.23, batch loss = 21.15 (8.5 examples/sec; 0.942 sec/batch; 82h:19m:26s remains)
INFO - root - 2017-12-07 12:55:34.402225: step 17780, loss = 21.09, batch loss = 21.01 (8.9 examples/sec; 0.898 sec/batch; 78h:32m:40s remains)
INFO - root - 2017-12-07 12:55:43.718845: step 17790, loss = 21.42, batch loss = 21.34 (8.5 examples/sec; 0.944 sec/batch; 82h:28m:54s remains)
INFO - root - 2017-12-07 12:55:53.116961: step 17800, loss = 21.25, batch loss = 21.16 (8.4 examples/sec; 0.951 sec/batch; 83h:07m:25s remains)
2017-12-07 12:55:54.044764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3323708 -4.3618765 -4.3948245 -4.4142876 -4.4111061 -4.39747 -4.3852372 -4.37933 -4.3833113 -4.3932104 -4.4026275 -4.4071441 -4.4056306 -4.40369 -4.4064026][-4.3419971 -4.3666263 -4.3938909 -4.4097309 -4.406723 -4.3951492 -4.3854637 -4.3829274 -4.39039 -4.4021611 -4.4093938 -4.4070449 -4.3971562 -4.3879533 -4.3840828][-4.3527131 -4.3863235 -4.421771 -4.4444776 -4.4453225 -4.4325266 -4.4197984 -4.4193182 -4.4356585 -4.4566369 -4.4646997 -4.4523535 -4.4260116 -4.3996406 -4.3820987][-4.3539362 -4.4064989 -4.4599118 -4.4892907 -4.4775081 -4.4341688 -4.391006 -4.3833313 -4.4208832 -4.4745221 -4.5058465 -4.4977841 -4.4600282 -4.4153481 -4.3829508][-4.3369017 -4.410955 -4.480865 -4.5053153 -4.4543242 -4.3471627 -4.2491722 -4.2296691 -4.3014841 -4.4096627 -4.4888763 -4.5078321 -4.4748955 -4.420557 -4.3775954][-4.3097939 -4.400043 -4.4749532 -4.477972 -4.36728 -4.1805191 -4.025455 -4.0014277 -4.1130204 -4.2786412 -4.4139452 -4.47552 -4.4667845 -4.4175625 -4.3699455][-4.2841611 -4.3767991 -4.4445496 -4.4209414 -4.2534876 -3.9979944 -3.802901 -3.7852416 -3.9335983 -4.1439543 -4.3226285 -4.42254 -4.4421778 -4.4056897 -4.3578687][-4.2700763 -4.3536582 -4.4112291 -4.3749704 -4.179883 -3.885648 -3.6672027 -3.6489158 -3.8125353 -4.044838 -4.2486897 -4.3721604 -4.4122362 -4.3860517 -4.3388724][-4.2742329 -4.341722 -4.3939047 -4.3673954 -4.1850948 -3.8918614 -3.6638105 -3.6282802 -3.7767959 -4.0068893 -4.2201538 -4.3525286 -4.398097 -4.3725386 -4.3231444][-4.2877197 -4.336482 -4.3886013 -4.3907609 -4.2613635 -4.0200129 -3.8159254 -3.7695718 -3.8839879 -4.0808678 -4.2703619 -4.3825088 -4.4105821 -4.373003 -4.3185987][-4.2978849 -4.3314781 -4.3862023 -4.4215593 -4.365212 -4.2145839 -4.0739779 -4.040206 -4.1191154 -4.2555432 -4.3830037 -4.4444475 -4.4363775 -4.3811536 -4.322484][-4.3017969 -4.3224621 -4.3731 -4.4273443 -4.4333334 -4.3772464 -4.314456 -4.3053622 -4.3529134 -4.4234304 -4.4788928 -4.4844584 -4.4431825 -4.377646 -4.3216252][-4.3039951 -4.3134122 -4.349793 -4.4003558 -4.43578 -4.4417534 -4.4374142 -4.4474053 -4.4710674 -4.4921403 -4.4962678 -4.4677958 -4.4130764 -4.351974 -4.3080544][-4.3106675 -4.312561 -4.3311744 -4.3612518 -4.3900805 -4.4108043 -4.4278841 -4.4447956 -4.4548054 -4.4521251 -4.43803 -4.4064522 -4.3605666 -4.3163357 -4.28956][-4.3183184 -4.316505 -4.322032 -4.3314433 -4.3397827 -4.3466763 -4.3564782 -4.3682017 -4.3728456 -4.3676286 -4.3600597 -4.3440714 -4.3169718 -4.2921205 -4.280261]]...]
INFO - root - 2017-12-07 12:56:03.383469: step 17810, loss = 21.59, batch loss = 21.51 (8.8 examples/sec; 0.911 sec/batch; 79h:39m:07s remains)
INFO - root - 2017-12-07 12:56:12.705753: step 17820, loss = 21.17, batch loss = 21.09 (8.4 examples/sec; 0.948 sec/batch; 82h:52m:27s remains)
INFO - root - 2017-12-07 12:56:22.105077: step 17830, loss = 21.26, batch loss = 21.18 (8.5 examples/sec; 0.946 sec/batch; 82h:43m:41s remains)
INFO - root - 2017-12-07 12:56:31.597282: step 17840, loss = 21.64, batch loss = 21.56 (8.5 examples/sec; 0.944 sec/batch; 82h:30m:40s remains)
INFO - root - 2017-12-07 12:56:41.069430: step 17850, loss = 20.95, batch loss = 20.87 (8.6 examples/sec; 0.933 sec/batch; 81h:30m:22s remains)
INFO - root - 2017-12-07 12:56:50.359195: step 17860, loss = 21.37, batch loss = 21.29 (9.0 examples/sec; 0.888 sec/batch; 77h:36m:50s remains)
INFO - root - 2017-12-07 12:56:59.803415: step 17870, loss = 21.01, batch loss = 20.93 (8.4 examples/sec; 0.949 sec/batch; 82h:56m:05s remains)
INFO - root - 2017-12-07 12:57:09.147080: step 17880, loss = 21.52, batch loss = 21.44 (8.8 examples/sec; 0.906 sec/batch; 79h:09m:09s remains)
INFO - root - 2017-12-07 12:57:18.587978: step 17890, loss = 21.66, batch loss = 21.58 (8.5 examples/sec; 0.943 sec/batch; 82h:26m:56s remains)
INFO - root - 2017-12-07 12:57:27.978301: step 17900, loss = 21.70, batch loss = 21.61 (8.4 examples/sec; 0.947 sec/batch; 82h:47m:12s remains)
2017-12-07 12:57:28.951268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1489015 -4.1198721 -4.1990461 -4.3467917 -4.4794321 -4.5419493 -4.5272269 -4.4422646 -4.3288054 -4.2171817 -4.0929136 -4.0126696 -4.038568 -4.1688972 -4.3420596][-4.2324533 -4.216208 -4.2950926 -4.4301019 -4.5416932 -4.5844722 -4.5582552 -4.4755268 -4.3677053 -4.2517948 -4.1188307 -4.034976 -4.0687866 -4.20358 -4.3602529][-4.3846507 -4.3812671 -4.4341712 -4.5199122 -4.5840845 -4.6048551 -4.5841842 -4.5333605 -4.4622622 -4.3624597 -4.230979 -4.14458 -4.1774554 -4.3026862 -4.4347219][-4.5420132 -4.5388083 -4.5491714 -4.5688467 -4.5750723 -4.573473 -4.5683832 -4.5627551 -4.5428615 -4.479064 -4.3700147 -4.2934604 -4.3230538 -4.4345288 -4.5471582][-4.6091928 -4.5957 -4.563026 -4.5210466 -4.4728055 -4.4482069 -4.4570704 -4.4903603 -4.516448 -4.4970818 -4.4300718 -4.3818121 -4.416532 -4.5204444 -4.6223769][-4.5459023 -4.5324178 -4.4862919 -4.4106674 -4.3293347 -4.2913618 -4.3105288 -4.367527 -4.4199739 -4.4382811 -4.4199195 -4.4054518 -4.4410782 -4.5234418 -4.6090693][-4.3839908 -4.3907785 -4.3592029 -4.2737403 -4.1794176 -4.1453295 -4.1813445 -4.2540331 -4.3201981 -4.3627634 -4.3826551 -4.3972611 -4.43337 -4.4943309 -4.5635991][-4.2151327 -4.2616515 -4.2605834 -4.1774464 -4.0766115 -4.0442467 -4.0883937 -4.1675377 -4.2414527 -4.2996254 -4.3455796 -4.3864112 -4.4287395 -4.4744005 -4.5263319][-4.1505771 -4.2233539 -4.2378068 -4.1592765 -4.0561118 -4.0163937 -4.0466971 -4.1156449 -4.1910782 -4.2574062 -4.3118458 -4.3609085 -4.4010572 -4.4324427 -4.4731388][-4.2055345 -4.2662907 -4.2733374 -4.2011433 -4.096765 -4.0344653 -4.0265436 -4.0660033 -4.1352005 -4.2026892 -4.2505426 -4.2886648 -4.312191 -4.3304758 -4.37364][-4.28456 -4.325706 -4.3298316 -4.2726607 -4.1718712 -4.082581 -4.0276146 -4.028769 -4.0814867 -4.1369848 -4.1633134 -4.1730351 -4.1668539 -4.1728306 -4.2292681][-4.326447 -4.3616862 -4.3755608 -4.3449535 -4.2648754 -4.1692986 -4.0864873 -4.0593677 -4.0922127 -4.1245055 -4.1114397 -4.0743623 -4.0346932 -4.0374203 -4.1175437][-4.3406992 -4.3781266 -4.3971453 -4.3832426 -4.3256116 -4.2422724 -4.1618476 -4.1331854 -4.1613927 -4.1746435 -4.1211839 -4.0401134 -3.9769011 -3.9875627 -4.0891318][-4.3683882 -4.3994451 -4.4122934 -4.3973532 -4.3470511 -4.2741761 -4.208797 -4.2047429 -4.2584586 -4.2744431 -4.1944652 -4.0846496 -4.0172648 -4.053371 -4.1759353][-4.415247 -4.4314961 -4.43221 -4.4109187 -4.3601761 -4.2929997 -4.2448449 -4.2731452 -4.3629432 -4.3930812 -4.3072019 -4.1960635 -4.1571045 -4.24448 -4.3972921]]...]
INFO - root - 2017-12-07 12:57:38.582350: step 17910, loss = 21.56, batch loss = 21.47 (8.4 examples/sec; 0.948 sec/batch; 82h:51m:52s remains)
INFO - root - 2017-12-07 12:57:48.013852: step 17920, loss = 21.97, batch loss = 21.89 (9.0 examples/sec; 0.886 sec/batch; 77h:24m:30s remains)
INFO - root - 2017-12-07 12:57:57.514875: step 17930, loss = 20.90, batch loss = 20.82 (8.0 examples/sec; 0.996 sec/batch; 87h:01m:03s remains)
INFO - root - 2017-12-07 12:58:06.795067: step 17940, loss = 21.40, batch loss = 21.32 (8.5 examples/sec; 0.937 sec/batch; 81h:53m:48s remains)
INFO - root - 2017-12-07 12:58:16.198093: step 17950, loss = 21.34, batch loss = 21.26 (8.8 examples/sec; 0.908 sec/batch; 79h:19m:33s remains)
INFO - root - 2017-12-07 12:58:25.473015: step 17960, loss = 21.40, batch loss = 21.31 (8.5 examples/sec; 0.936 sec/batch; 81h:47m:55s remains)
INFO - root - 2017-12-07 12:58:34.934906: step 17970, loss = 21.41, batch loss = 21.32 (8.9 examples/sec; 0.894 sec/batch; 78h:07m:50s remains)
INFO - root - 2017-12-07 12:58:44.223434: step 17980, loss = 21.33, batch loss = 21.25 (8.6 examples/sec; 0.925 sec/batch; 80h:49m:03s remains)
INFO - root - 2017-12-07 12:58:53.619122: step 17990, loss = 21.38, batch loss = 21.29 (8.7 examples/sec; 0.920 sec/batch; 80h:23m:57s remains)
INFO - root - 2017-12-07 12:59:02.945341: step 18000, loss = 21.26, batch loss = 21.17 (8.4 examples/sec; 0.954 sec/batch; 83h:18m:07s remains)
2017-12-07 12:59:03.880090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.37687 -4.4459186 -4.5285144 -4.568049 -4.5699191 -4.5709157 -4.5722189 -4.5702424 -4.5711408 -4.5815587 -4.591877 -4.5868964 -4.572206 -4.5736752 -4.5962133][-4.3398094 -4.3982053 -4.4874163 -4.5432692 -4.5615492 -4.5825753 -4.6012945 -4.6094236 -4.61122 -4.6146464 -4.6175828 -4.609271 -4.5849481 -4.5737491 -4.5908504][-4.3603277 -4.38459 -4.4512353 -4.5068064 -4.5395904 -4.58301 -4.6219411 -4.6400647 -4.6375036 -4.6229744 -4.6047268 -4.5880041 -4.5645642 -4.55801 -4.5828819][-4.3945565 -4.3740635 -4.3931441 -4.4225259 -4.4542961 -4.5130029 -4.5732522 -4.6093731 -4.61215 -4.5887465 -4.5518012 -4.5207062 -4.4965596 -4.4976492 -4.5324955][-4.3965178 -4.3213229 -4.2767644 -4.2582259 -4.2700748 -4.3319154 -4.4126782 -4.473033 -4.4918718 -4.4758239 -4.4352384 -4.3883181 -4.3501024 -4.3444543 -4.3807535][-4.364037 -4.2456274 -4.1452475 -4.0754743 -4.0580378 -4.1145825 -4.2108045 -4.2939768 -4.3310251 -4.3271856 -4.28815 -4.2199588 -4.1555557 -4.1321111 -4.1669455][-4.3369632 -4.2083831 -4.0740047 -3.960866 -3.911792 -3.9588323 -4.0638947 -4.1680336 -4.2233858 -4.2301016 -4.188736 -4.0966196 -4.0033865 -3.9651935 -4.0033154][-4.3575377 -4.2442722 -4.1006174 -3.9626369 -3.8868132 -3.921037 -4.0287514 -4.1459947 -4.2136269 -4.2266908 -4.1787577 -4.0657549 -3.9476984 -3.8999765 -3.945744][-4.43149 -4.3426356 -4.2054996 -4.0619969 -3.9735808 -3.997328 -4.10468 -4.225637 -4.2967052 -4.3113875 -4.2605906 -4.1376495 -4.0020742 -3.9466362 -3.9943292][-4.509933 -4.4436712 -4.324604 -4.1949821 -4.1091118 -4.1210408 -4.2151127 -4.3287439 -4.3987479 -4.4151516 -4.3740854 -4.2661896 -4.1343031 -4.07136 -4.1007738][-4.5440593 -4.51134 -4.4319887 -4.3373485 -4.2676511 -4.2671556 -4.3318582 -4.4244657 -4.4921207 -4.512795 -4.4858327 -4.4024291 -4.2876692 -4.2164326 -4.2150459][-4.5187082 -4.5213561 -4.4909024 -4.4408574 -4.3993711 -4.3957734 -4.4296718 -4.4937868 -4.5573263 -4.5873985 -4.5780773 -4.5231323 -4.4340997 -4.3607373 -4.3282347][-4.4456716 -4.4683943 -4.4747148 -4.4614553 -4.4474726 -4.4475627 -4.4582553 -4.4929338 -4.5444093 -4.581151 -4.5923066 -4.5699482 -4.5144157 -4.4536128 -4.4062619][-4.3644376 -4.3916316 -4.41304 -4.4195504 -4.42161 -4.4251089 -4.4243793 -4.43846 -4.4738307 -4.5082183 -4.5281687 -4.5252042 -4.4954853 -4.4528155 -4.4115953][-4.2872314 -4.3078718 -4.3280625 -4.3392653 -4.3463664 -4.3520608 -4.3522973 -4.3601632 -4.3821883 -4.4054928 -4.4199047 -4.4206867 -4.4068866 -4.3846817 -4.3624711]]...]
INFO - root - 2017-12-07 12:59:13.125507: step 18010, loss = 21.49, batch loss = 21.41 (8.9 examples/sec; 0.896 sec/batch; 78h:16m:28s remains)
INFO - root - 2017-12-07 12:59:22.370745: step 18020, loss = 21.42, batch loss = 21.33 (8.8 examples/sec; 0.907 sec/batch; 79h:13m:32s remains)
INFO - root - 2017-12-07 12:59:31.752759: step 18030, loss = 21.26, batch loss = 21.18 (8.7 examples/sec; 0.922 sec/batch; 80h:29m:46s remains)
INFO - root - 2017-12-07 12:59:41.176387: step 18040, loss = 21.75, batch loss = 21.67 (8.5 examples/sec; 0.938 sec/batch; 81h:56m:11s remains)
INFO - root - 2017-12-07 12:59:50.520538: step 18050, loss = 21.63, batch loss = 21.55 (8.4 examples/sec; 0.952 sec/batch; 83h:08m:28s remains)
INFO - root - 2017-12-07 12:59:59.784504: step 18060, loss = 21.93, batch loss = 21.84 (8.3 examples/sec; 0.969 sec/batch; 84h:40m:37s remains)
INFO - root - 2017-12-07 13:00:09.162263: step 18070, loss = 21.36, batch loss = 21.28 (8.3 examples/sec; 0.961 sec/batch; 83h:57m:36s remains)
INFO - root - 2017-12-07 13:00:18.446082: step 18080, loss = 21.71, batch loss = 21.62 (9.0 examples/sec; 0.886 sec/batch; 77h:23m:59s remains)
INFO - root - 2017-12-07 13:00:27.781878: step 18090, loss = 21.48, batch loss = 21.40 (9.0 examples/sec; 0.885 sec/batch; 77h:16m:01s remains)
INFO - root - 2017-12-07 13:00:37.085294: step 18100, loss = 21.60, batch loss = 21.52 (8.8 examples/sec; 0.908 sec/batch; 79h:18m:03s remains)
2017-12-07 13:00:38.048720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4328103 -4.4423413 -4.4253683 -4.4079242 -4.4165487 -4.4176941 -4.4163284 -4.4522104 -4.4891071 -4.4915638 -4.4890895 -4.4789205 -4.4433751 -4.4058533 -4.3722091][-4.4084282 -4.4193864 -4.3993225 -4.3805141 -4.3914685 -4.3897963 -4.3879957 -4.4377279 -4.4784374 -4.4687834 -4.4584603 -4.4495487 -4.41535 -4.3799891 -4.3507276][-4.4185262 -4.4276047 -4.4061432 -4.386919 -4.3915014 -4.3834066 -4.3841105 -4.4395847 -4.4751954 -4.4517794 -4.4308696 -4.4213266 -4.3953462 -4.3710237 -4.3581095][-4.4424896 -4.447084 -4.4203625 -4.3893189 -4.3710027 -4.3505464 -4.3558445 -4.4134059 -4.44085 -4.4053435 -4.3694172 -4.3529725 -4.3389492 -4.3344817 -4.3439875][-4.4525208 -4.4436297 -4.3993344 -4.3403344 -4.2877374 -4.2509174 -4.2519283 -4.2988648 -4.3140969 -4.2751775 -4.2258034 -4.1975808 -4.2024088 -4.2273407 -4.260664][-4.421247 -4.3935108 -4.3233924 -4.2295723 -4.1498609 -4.1032481 -4.0881014 -4.1070585 -4.1035924 -4.069787 -4.0198135 -3.9842358 -4.010097 -4.069056 -4.1253552][-4.3697958 -4.3220758 -4.2294645 -4.1114717 -4.0197372 -3.969964 -3.9364657 -3.9233489 -3.9031978 -3.8851981 -3.8482292 -3.8163915 -3.8640432 -3.9540544 -4.0335331][-4.3444548 -4.2847304 -4.1794162 -4.0541539 -3.958292 -3.9034436 -3.8539205 -3.8199451 -3.7972548 -3.8053548 -3.7943568 -3.7772522 -3.843415 -3.9500287 -4.0390224][-4.3482952 -4.299695 -4.2104392 -4.1042585 -4.0105076 -3.9442592 -3.8791547 -3.8331378 -3.8146966 -3.8432786 -3.861866 -3.8673668 -3.938374 -4.0397978 -4.1234765][-4.3934946 -4.3719363 -4.319346 -4.2495265 -4.163002 -4.08327 -4.0073819 -3.9608359 -3.9523296 -3.9935222 -4.03755 -4.0632615 -4.1207318 -4.1895089 -4.2545223][-4.422883 -4.4247456 -4.4074354 -4.375093 -4.3096819 -4.2367768 -4.1728239 -4.1453247 -4.1534891 -4.1969728 -4.2453055 -4.2741752 -4.3082852 -4.3403754 -4.3828273][-4.4028687 -4.4120617 -4.4183807 -4.4151621 -4.3818254 -4.3364558 -4.3001442 -4.2955575 -4.3117247 -4.348887 -4.3868437 -4.4097209 -4.4274611 -4.437181 -4.4591084][-4.3546057 -4.353272 -4.3630543 -4.3680511 -4.3543968 -4.3343482 -4.3257484 -4.3388686 -4.35488 -4.3845792 -4.4148369 -4.4323878 -4.4471574 -4.4518766 -4.4600868][-4.3153095 -4.29482 -4.2964349 -4.2993464 -4.2930603 -4.2855587 -4.2915425 -4.3085184 -4.3166866 -4.3406363 -4.3702784 -4.3877826 -4.4038529 -4.41072 -4.4134235][-4.3139372 -4.2903371 -4.2918849 -4.2981 -4.2971911 -4.2876568 -4.2854447 -4.2889791 -4.2810884 -4.2973952 -4.3342414 -4.3632522 -4.3881469 -4.4020824 -4.4074354]]...]
INFO - root - 2017-12-07 13:00:47.279964: step 18110, loss = 21.51, batch loss = 21.43 (8.8 examples/sec; 0.910 sec/batch; 79h:27m:34s remains)
INFO - root - 2017-12-07 13:00:56.702597: step 18120, loss = 21.33, batch loss = 21.25 (8.6 examples/sec; 0.933 sec/batch; 81h:26m:10s remains)
INFO - root - 2017-12-07 13:01:06.072204: step 18130, loss = 21.35, batch loss = 21.27 (8.5 examples/sec; 0.944 sec/batch; 82h:27m:10s remains)
INFO - root - 2017-12-07 13:01:15.500349: step 18140, loss = 21.43, batch loss = 21.34 (7.9 examples/sec; 1.007 sec/batch; 87h:53m:47s remains)
INFO - root - 2017-12-07 13:01:24.846927: step 18150, loss = 21.49, batch loss = 21.41 (8.2 examples/sec; 0.971 sec/batch; 84h:45m:43s remains)
INFO - root - 2017-12-07 13:01:34.247424: step 18160, loss = 21.30, batch loss = 21.22 (8.5 examples/sec; 0.936 sec/batch; 81h:45m:24s remains)
INFO - root - 2017-12-07 13:01:43.616251: step 18170, loss = 21.71, batch loss = 21.63 (8.5 examples/sec; 0.941 sec/batch; 82h:10m:01s remains)
INFO - root - 2017-12-07 13:01:53.021647: step 18180, loss = 21.34, batch loss = 21.25 (8.4 examples/sec; 0.951 sec/batch; 83h:01m:58s remains)
INFO - root - 2017-12-07 13:02:02.374458: step 18190, loss = 21.36, batch loss = 21.28 (8.6 examples/sec; 0.927 sec/batch; 80h:56m:07s remains)
INFO - root - 2017-12-07 13:02:11.782139: step 18200, loss = 21.56, batch loss = 21.47 (9.2 examples/sec; 0.869 sec/batch; 75h:50m:09s remains)
2017-12-07 13:02:12.696891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2959604 -4.2453828 -4.2289381 -4.2840061 -4.3579736 -4.4151034 -4.4818511 -4.5007439 -4.4192591 -4.3198271 -4.2529716 -4.1837821 -4.1220446 -4.1210694 -4.1790032][-4.2696462 -4.2614274 -4.2598729 -4.3050165 -4.3570232 -4.3956666 -4.4555712 -4.4755883 -4.4112954 -4.3291287 -4.2598982 -4.16507 -4.0728483 -4.0629416 -4.1414566][-4.2857709 -4.2975712 -4.2869735 -4.2946024 -4.3033514 -4.3155165 -4.3659062 -4.4029841 -4.3811231 -4.3245358 -4.2555895 -4.1504741 -4.0562348 -4.0596676 -4.1583176][-4.3237147 -4.3398685 -4.3025684 -4.2521095 -4.2021065 -4.1783485 -4.2146564 -4.28059 -4.3177609 -4.2986341 -4.2416115 -4.1461616 -4.070159 -4.0901828 -4.1900153][-4.3565383 -4.3678961 -4.2962875 -4.182025 -4.070363 -4.0068278 -4.0281992 -4.122797 -4.217195 -4.2404122 -4.21254 -4.1487556 -4.0983167 -4.1184783 -4.1943784][-4.3827415 -4.3816867 -4.2747164 -4.1051726 -3.945734 -3.8516049 -3.862103 -3.9727416 -4.1009889 -4.1596441 -4.1752477 -4.157444 -4.1286654 -4.1301575 -4.1644559][-4.3825397 -4.3777375 -4.2552371 -4.0533247 -3.8717556 -3.7699015 -3.7756567 -3.8792193 -4.0052075 -4.0833635 -4.142354 -4.16534 -4.1453538 -4.1228304 -4.1218843][-4.3417196 -4.340683 -4.2279134 -4.0256877 -3.8566794 -3.7759075 -3.7831509 -3.8543766 -3.9423182 -4.02023 -4.1039796 -4.1514134 -4.135581 -4.0998292 -4.0851755][-4.2766523 -4.2896676 -4.2160349 -4.06087 -3.939846 -3.8971446 -3.9053497 -3.9284916 -3.9574649 -4.0103073 -4.0879531 -4.1397195 -4.1327591 -4.0986886 -4.0855527][-4.2100782 -4.2374992 -4.2188621 -4.145206 -4.0908823 -4.0811605 -4.0898442 -4.0819902 -4.0722189 -4.1043849 -4.1639524 -4.2047 -4.2000756 -4.1619782 -4.1394258][-4.1478505 -4.1886988 -4.2258043 -4.2324834 -4.2315569 -4.23884 -4.2563329 -4.2585349 -4.2491961 -4.2747846 -4.3176365 -4.3428183 -4.3298206 -4.2783232 -4.2353725][-4.1568146 -4.2118297 -4.2884455 -4.3393245 -4.3544168 -4.3578215 -4.3844647 -4.4124675 -4.4227405 -4.4468088 -4.4765277 -4.4861131 -4.4572539 -4.3931403 -4.3371458][-4.2475777 -4.3122382 -4.4028578 -4.4600735 -4.4638386 -4.4524312 -4.4770255 -4.5164952 -4.5404725 -4.5654435 -4.589107 -4.5915041 -4.5492196 -4.4814191 -4.4259605][-4.3262448 -4.3869042 -4.4664049 -4.513207 -4.50654 -4.4875746 -4.5019975 -4.5375543 -4.5702667 -4.5994186 -4.6209779 -4.6213608 -4.5797253 -4.522675 -4.479588][-4.3796091 -4.420013 -4.471034 -4.5026832 -4.4949675 -4.4778318 -4.4803925 -4.5022173 -4.5367351 -4.571538 -4.596149 -4.5977926 -4.56737 -4.526814 -4.4950056]]...]
INFO - root - 2017-12-07 13:02:21.922114: step 18210, loss = 21.56, batch loss = 21.48 (8.8 examples/sec; 0.913 sec/batch; 79h:43m:05s remains)
INFO - root - 2017-12-07 13:02:31.237231: step 18220, loss = 21.32, batch loss = 21.23 (8.4 examples/sec; 0.950 sec/batch; 82h:55m:44s remains)
INFO - root - 2017-12-07 13:02:40.565896: step 18230, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.926 sec/batch; 80h:49m:00s remains)
INFO - root - 2017-12-07 13:02:49.912081: step 18240, loss = 21.69, batch loss = 21.61 (9.0 examples/sec; 0.885 sec/batch; 77h:15m:25s remains)
INFO - root - 2017-12-07 13:02:59.254205: step 18250, loss = 22.06, batch loss = 21.98 (8.9 examples/sec; 0.894 sec/batch; 78h:03m:30s remains)
INFO - root - 2017-12-07 13:03:08.770704: step 18260, loss = 21.51, batch loss = 21.42 (8.6 examples/sec; 0.926 sec/batch; 80h:50m:51s remains)
INFO - root - 2017-12-07 13:03:17.996717: step 18270, loss = 21.45, batch loss = 21.37 (8.1 examples/sec; 0.984 sec/batch; 85h:50m:56s remains)
INFO - root - 2017-12-07 13:03:27.193404: step 18280, loss = 21.43, batch loss = 21.34 (8.0 examples/sec; 0.997 sec/batch; 87h:02m:25s remains)
INFO - root - 2017-12-07 13:03:36.447729: step 18290, loss = 21.60, batch loss = 21.52 (8.4 examples/sec; 0.951 sec/batch; 83h:00m:17s remains)
INFO - root - 2017-12-07 13:03:45.613617: step 18300, loss = 21.56, batch loss = 21.48 (8.9 examples/sec; 0.899 sec/batch; 78h:25m:31s remains)
2017-12-07 13:03:46.540080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5525465 -4.5853848 -4.5947108 -4.5829568 -4.5638795 -4.5360022 -4.4982195 -4.4602637 -4.430377 -4.4338803 -4.4738727 -4.530684 -4.5773845 -4.5928159 -4.5685744][-4.5839334 -4.6158209 -4.6243296 -4.6085443 -4.5820918 -4.5450196 -4.5069628 -4.4752841 -4.4562616 -4.4761066 -4.5328326 -4.5977983 -4.64306 -4.6573267 -4.6347437][-4.584486 -4.614522 -4.6241632 -4.6000676 -4.5517712 -4.4885969 -4.4437814 -4.4322724 -4.4506812 -4.5085826 -4.5868759 -4.6496291 -4.6760716 -4.6767197 -4.656343][-4.5522122 -4.56607 -4.561244 -4.5122142 -4.4266491 -4.3231249 -4.2561746 -4.2599611 -4.3310575 -4.4489913 -4.5672021 -4.6416755 -4.6577225 -4.6507516 -4.6372309][-4.5329905 -4.5265818 -4.4965882 -4.4126997 -4.2828922 -4.1276321 -4.0132818 -4.0002851 -4.1072607 -4.2860961 -4.4628944 -4.580214 -4.6172132 -4.6248507 -4.62152][-4.5374856 -4.5178866 -4.4717131 -4.3578482 -4.1822124 -3.9720764 -3.7897863 -3.722867 -3.842026 -4.079627 -4.329186 -4.5151896 -4.596447 -4.634582 -4.6425958][-4.5470276 -4.5227542 -4.4715905 -4.3379927 -4.1253076 -3.8727787 -3.632484 -3.5130396 -3.6392853 -3.9273558 -4.2400718 -4.4910479 -4.6150618 -4.677599 -4.691093][-4.583178 -4.566093 -4.5278668 -4.4057088 -4.19646 -3.9393501 -3.6737437 -3.5202456 -3.6350389 -3.9392018 -4.282598 -4.567853 -4.7050328 -4.7593131 -4.7577038][-4.6273937 -4.6112471 -4.5843558 -4.4984484 -4.3330736 -4.1163144 -3.8735595 -3.7160563 -3.7986643 -4.0681915 -4.38793 -4.6563525 -4.7774081 -4.8065181 -4.7860346][-4.6755552 -4.6704941 -4.652277 -4.6002331 -4.489512 -4.3339915 -4.1459908 -4.0081873 -4.0462527 -4.2395077 -4.4888196 -4.6990418 -4.7906971 -4.80166 -4.7702279][-4.7157149 -4.748477 -4.7570062 -4.7356858 -4.66758 -4.5610976 -4.4306173 -4.328351 -4.33631 -4.4546666 -4.6190982 -4.755929 -4.809793 -4.8009424 -4.7518773][-4.6749039 -4.7336068 -4.7650485 -4.7646837 -4.724669 -4.653739 -4.5706859 -4.5067215 -4.5066228 -4.57516 -4.6702304 -4.7443986 -4.7686052 -4.7502995 -4.694067][-4.5649033 -4.6163335 -4.6487455 -4.654839 -4.6321478 -4.5891385 -4.542944 -4.510211 -4.5102425 -4.5445395 -4.5932088 -4.6315002 -4.6444154 -4.6305757 -4.5855279][-4.4484067 -4.4792266 -4.5018015 -4.5074005 -4.496 -4.4752474 -4.4560938 -4.4437294 -4.4446406 -4.4584651 -4.477694 -4.493937 -4.4997435 -4.4908137 -4.4610739][-4.3514838 -4.3611484 -4.3701982 -4.3719416 -4.36809 -4.3623142 -4.3577929 -4.3548961 -4.3555908 -4.3590355 -4.3635449 -4.3683181 -4.370533 -4.3666725 -4.3529425]]...]
INFO - root - 2017-12-07 13:03:55.935032: step 18310, loss = 21.54, batch loss = 21.45 (8.6 examples/sec; 0.933 sec/batch; 81h:27m:48s remains)
INFO - root - 2017-12-07 13:04:05.175284: step 18320, loss = 21.64, batch loss = 21.56 (9.4 examples/sec; 0.853 sec/batch; 74h:26m:32s remains)
INFO - root - 2017-12-07 13:04:14.579908: step 18330, loss = 21.57, batch loss = 21.48 (9.3 examples/sec; 0.860 sec/batch; 75h:02m:54s remains)
INFO - root - 2017-12-07 13:04:23.914499: step 18340, loss = 21.17, batch loss = 21.09 (8.6 examples/sec; 0.927 sec/batch; 80h:52m:23s remains)
INFO - root - 2017-12-07 13:04:33.210271: step 18350, loss = 21.58, batch loss = 21.50 (8.5 examples/sec; 0.944 sec/batch; 82h:20m:44s remains)
INFO - root - 2017-12-07 13:04:42.553481: step 18360, loss = 21.21, batch loss = 21.12 (8.3 examples/sec; 0.966 sec/batch; 84h:20m:11s remains)
INFO - root - 2017-12-07 13:04:51.831850: step 18370, loss = 21.27, batch loss = 21.19 (8.0 examples/sec; 1.000 sec/batch; 87h:14m:38s remains)
INFO - root - 2017-12-07 13:05:01.155035: step 18380, loss = 21.50, batch loss = 21.41 (8.1 examples/sec; 0.989 sec/batch; 86h:17m:07s remains)
INFO - root - 2017-12-07 13:05:10.513352: step 18390, loss = 21.42, batch loss = 21.34 (8.3 examples/sec; 0.966 sec/batch; 84h:17m:19s remains)
INFO - root - 2017-12-07 13:05:19.877453: step 18400, loss = 21.77, batch loss = 21.69 (8.8 examples/sec; 0.909 sec/batch; 79h:17m:25s remains)
2017-12-07 13:05:20.865985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3461933 -4.367322 -4.4059172 -4.4581451 -4.5056248 -4.5271883 -4.4981451 -4.4439917 -4.4207964 -4.4496846 -4.5269079 -4.582531 -4.5860043 -4.5536981 -4.5088253][-4.3660512 -4.39976 -4.4408574 -4.479094 -4.5070014 -4.5180526 -4.4828639 -4.4248762 -4.4012222 -4.4301143 -4.4958405 -4.5403347 -4.5334744 -4.4991941 -4.4639592][-4.3887134 -4.4537997 -4.5202379 -4.5598054 -4.5682249 -4.5501347 -4.4842281 -4.4048519 -4.3664875 -4.3818541 -4.4411941 -4.4952178 -4.500495 -4.4735923 -4.441474][-4.4242721 -4.5196571 -4.6128621 -4.6542811 -4.6387367 -4.5797462 -4.4748397 -4.3727589 -4.3176174 -4.3162093 -4.3707671 -4.4463305 -4.4799852 -4.4736843 -4.4469833][-4.4555173 -4.5530248 -4.6473546 -4.6733551 -4.626709 -4.5337281 -4.4116635 -4.3142858 -4.2680869 -4.2686181 -4.331049 -4.4282732 -4.4863343 -4.4955878 -4.4711928][-4.4823422 -4.5520792 -4.6090164 -4.5833058 -4.4884529 -4.3702083 -4.2603183 -4.2008138 -4.1943612 -4.22581 -4.3115158 -4.4294958 -4.5066547 -4.5275526 -4.5047164][-4.4838209 -4.5155616 -4.5101104 -4.4085965 -4.2588253 -4.1275716 -4.0476112 -4.04373 -4.0895009 -4.1598449 -4.2694497 -4.403368 -4.5030675 -4.5457673 -4.5328135][-4.4576197 -4.4606152 -4.3974771 -4.22993 -4.04475 -3.9177957 -3.8718672 -3.9186387 -4.0107713 -4.1124005 -4.237556 -4.379313 -4.4994736 -4.5621471 -4.5529647][-4.4393826 -4.4298887 -4.3346672 -4.1359487 -3.9438469 -3.8288608 -3.8038125 -3.879045 -3.997551 -4.1187925 -4.2519193 -4.3930159 -4.5167608 -4.5821609 -4.5649195][-4.46143 -4.4484625 -4.341434 -4.1429229 -3.9648831 -3.8690798 -3.8591998 -3.9483998 -4.0724692 -4.1948447 -4.3256879 -4.4488697 -4.5531449 -4.6023741 -4.5681157][-4.5093884 -4.5071354 -4.4145122 -4.2482052 -4.1054096 -4.0314918 -4.0240774 -4.0978284 -4.1970525 -4.2958708 -4.4057755 -4.4984026 -4.5722222 -4.6014533 -4.5541024][-4.5432267 -4.5674667 -4.5201654 -4.419096 -4.3294835 -4.2737241 -4.2504344 -4.2787213 -4.3268361 -4.3807688 -4.4540024 -4.5132055 -4.5582271 -4.5740685 -4.5249391][-4.5471406 -4.5946574 -4.5948095 -4.557806 -4.5151877 -4.4753256 -4.4368382 -4.42067 -4.4187541 -4.4315906 -4.4730239 -4.5074482 -4.5349021 -4.54527 -4.5021105][-4.53878 -4.5874133 -4.6079984 -4.6054564 -4.5900888 -4.5619059 -4.518424 -4.4792438 -4.4484644 -4.4392104 -4.4644809 -4.488368 -4.5110383 -4.5236239 -4.4891987][-4.5179582 -4.543561 -4.55817 -4.56069 -4.5532403 -4.53143 -4.4909821 -4.4478126 -4.4110332 -4.400394 -4.4262805 -4.4528904 -4.4819922 -4.5010357 -4.4741564]]...]
INFO - root - 2017-12-07 13:05:30.290362: step 18410, loss = 21.07, batch loss = 20.99 (8.3 examples/sec; 0.970 sec/batch; 84h:35m:50s remains)
INFO - root - 2017-12-07 13:05:39.658474: step 18420, loss = 21.48, batch loss = 21.39 (8.7 examples/sec; 0.916 sec/batch; 79h:54m:21s remains)
INFO - root - 2017-12-07 13:05:49.179264: step 18430, loss = 21.22, batch loss = 21.14 (8.3 examples/sec; 0.967 sec/batch; 84h:20m:40s remains)
INFO - root - 2017-12-07 13:05:58.518183: step 18440, loss = 21.67, batch loss = 21.59 (8.8 examples/sec; 0.913 sec/batch; 79h:41m:08s remains)
INFO - root - 2017-12-07 13:06:07.657186: step 18450, loss = 21.36, batch loss = 21.28 (9.2 examples/sec; 0.866 sec/batch; 75h:32m:12s remains)
INFO - root - 2017-12-07 13:06:17.088603: step 18460, loss = 21.72, batch loss = 21.64 (8.5 examples/sec; 0.939 sec/batch; 81h:52m:57s remains)
INFO - root - 2017-12-07 13:06:26.442682: step 18470, loss = 21.64, batch loss = 21.55 (9.4 examples/sec; 0.848 sec/batch; 73h:59m:13s remains)
INFO - root - 2017-12-07 13:06:35.825513: step 18480, loss = 21.78, batch loss = 21.70 (8.9 examples/sec; 0.898 sec/batch; 78h:21m:38s remains)
INFO - root - 2017-12-07 13:06:45.109599: step 18490, loss = 21.35, batch loss = 21.26 (8.6 examples/sec; 0.930 sec/batch; 81h:06m:02s remains)
INFO - root - 2017-12-07 13:06:54.533588: step 18500, loss = 21.53, batch loss = 21.44 (7.9 examples/sec; 1.011 sec/batch; 88h:11m:53s remains)
2017-12-07 13:06:55.505415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3157125 -4.3492489 -4.3772006 -4.397099 -4.4108834 -4.4210143 -4.4275603 -4.4267836 -4.4154086 -4.3943572 -4.3701015 -4.349072 -4.3186722 -4.2848964 -4.2710843][-4.3524323 -4.3821054 -4.4017057 -4.414206 -4.4251151 -4.4386783 -4.4590945 -4.4823551 -4.4954705 -4.4885788 -4.4648986 -4.4358296 -4.3908587 -4.3369651 -4.3008442][-4.3766694 -4.3842125 -4.3771691 -4.368454 -4.3652844 -4.3703837 -4.3952565 -4.4445591 -4.4983344 -4.5280104 -4.5231905 -4.4979076 -4.4469924 -4.3792019 -4.3248224][-4.3915014 -4.3639646 -4.3188004 -4.2773328 -4.245914 -4.2214031 -4.2239838 -4.2789969 -4.3739276 -4.4588161 -4.4947162 -4.4911122 -4.4508982 -4.3863015 -4.3293576][-4.3999715 -4.3394651 -4.25187 -4.1687732 -4.0981 -4.0278044 -3.9819045 -4.0136447 -4.1348147 -4.2819247 -4.3755088 -4.4078674 -4.3917918 -4.3478823 -4.308588][-4.4000363 -4.31788 -4.1911025 -4.0634003 -3.9520407 -3.8382974 -3.7396293 -3.7325916 -3.8635852 -4.067831 -4.22383 -4.2948904 -4.3033152 -4.2847171 -4.272665][-4.3884635 -4.3074374 -4.1618371 -4.0022221 -3.8604252 -3.7176282 -3.5798807 -3.5334554 -3.6576095 -3.8977137 -4.1037645 -4.2065845 -4.2300453 -4.2288351 -4.2390041][-4.3612127 -4.3059239 -4.1749353 -4.0147781 -3.8649197 -3.7116582 -3.5539608 -3.4770486 -3.579654 -3.8252466 -4.0555887 -4.1776195 -4.2065597 -4.2115369 -4.2311649][-4.3346725 -4.3159709 -4.2292237 -4.1051364 -3.9772019 -3.8354955 -3.6779065 -3.5840895 -3.6561079 -3.8765612 -4.0983524 -4.2214861 -4.2487159 -4.251749 -4.26673][-4.334177 -4.3423615 -4.3059144 -4.2376728 -4.1548409 -4.0449452 -3.9081321 -3.8151839 -3.8568347 -4.0300417 -4.2141109 -4.320353 -4.3422756 -4.3395305 -4.3376708][-4.3773532 -4.3922977 -4.3889933 -4.3694887 -4.3351326 -4.26853 -4.1696019 -4.0943065 -4.1112595 -4.2257042 -4.3541412 -4.4323831 -4.4481392 -4.4372716 -4.4080558][-4.4554543 -4.4686646 -4.478291 -4.4849191 -4.4828897 -4.453876 -4.3949709 -4.3412604 -4.336359 -4.3918853 -4.4634824 -4.5156536 -4.5295162 -4.5080771 -4.4404845][-4.5096922 -4.5312538 -4.5497832 -4.5660176 -4.5743318 -4.5619478 -4.5263524 -4.4852271 -4.4634748 -4.4760785 -4.5096536 -4.5502806 -4.5676866 -4.5317259 -4.4169116][-4.485723 -4.5274296 -4.5653706 -4.5948138 -4.6063814 -4.5932932 -4.5593452 -4.5171556 -4.4831247 -4.4740386 -4.4936395 -4.5373511 -4.5594287 -4.5039554 -4.3396444][-4.3589144 -4.4240727 -4.4960742 -4.5588422 -4.5899467 -4.5816627 -4.5459704 -4.49788 -4.4535141 -4.4317317 -4.4455352 -4.492754 -4.5135379 -4.4364629 -4.2307286]]...]
INFO - root - 2017-12-07 13:07:05.037929: step 18510, loss = 21.29, batch loss = 21.20 (8.7 examples/sec; 0.921 sec/batch; 80h:20m:16s remains)
INFO - root - 2017-12-07 13:07:14.430649: step 18520, loss = 21.27, batch loss = 21.18 (8.6 examples/sec; 0.925 sec/batch; 80h:41m:03s remains)
INFO - root - 2017-12-07 13:07:24.015901: step 18530, loss = 21.40, batch loss = 21.32 (8.3 examples/sec; 0.962 sec/batch; 83h:55m:59s remains)
INFO - root - 2017-12-07 13:07:33.527955: step 18540, loss = 21.43, batch loss = 21.35 (8.1 examples/sec; 0.982 sec/batch; 85h:38m:22s remains)
INFO - root - 2017-12-07 13:07:42.887710: step 18550, loss = 21.35, batch loss = 21.27 (8.7 examples/sec; 0.920 sec/batch; 80h:11m:25s remains)
INFO - root - 2017-12-07 13:07:52.293582: step 18560, loss = 21.24, batch loss = 21.16 (8.6 examples/sec; 0.930 sec/batch; 81h:06m:38s remains)
INFO - root - 2017-12-07 13:08:01.708494: step 18570, loss = 21.54, batch loss = 21.46 (8.2 examples/sec; 0.977 sec/batch; 85h:12m:36s remains)
INFO - root - 2017-12-07 13:08:11.062182: step 18580, loss = 21.24, batch loss = 21.16 (8.4 examples/sec; 0.949 sec/batch; 82h:45m:50s remains)
INFO - root - 2017-12-07 13:08:20.423707: step 18590, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.910 sec/batch; 79h:21m:18s remains)
INFO - root - 2017-12-07 13:08:29.938479: step 18600, loss = 21.66, batch loss = 21.57 (8.5 examples/sec; 0.937 sec/batch; 81h:41m:03s remains)
2017-12-07 13:08:30.950756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4234328 -4.4270449 -4.4618077 -4.5485363 -4.6437716 -4.6875877 -4.6702619 -4.6288509 -4.6177282 -4.6383328 -4.6386986 -4.6045208 -4.5678468 -4.5542374 -4.5041089][-4.3796253 -4.3859873 -4.4381132 -4.5542521 -4.6706204 -4.71197 -4.6668081 -4.5942216 -4.5747995 -4.6071706 -4.6229553 -4.5968256 -4.5533109 -4.5301085 -4.4791684][-4.3633318 -4.3697414 -4.4270573 -4.5506144 -4.6661572 -4.6941705 -4.6238933 -4.5339537 -4.5154729 -4.5597987 -4.5945487 -4.5814023 -4.5320792 -4.4951854 -4.4424353][-4.3842354 -4.379705 -4.4252191 -4.5314422 -4.6251497 -4.6298761 -4.5366907 -4.4423776 -4.4361677 -4.4984684 -4.5574651 -4.5624723 -4.5126486 -4.4656267 -4.4137545][-4.4457583 -4.41558 -4.4290667 -4.493063 -4.5458865 -4.5195928 -4.4092636 -4.3189011 -4.3313375 -4.4139247 -4.4994564 -4.5330124 -4.5029359 -4.4637637 -4.4202433][-4.4755812 -4.4225249 -4.4042931 -4.4176517 -4.4150186 -4.3460159 -4.2184772 -4.1372395 -4.1727958 -4.282311 -4.4033375 -4.4813933 -4.4972043 -4.4916968 -4.4680276][-4.4166031 -4.3582687 -4.3294692 -4.3088131 -4.2522779 -4.1278481 -3.9754381 -3.9052584 -3.9693384 -4.1146135 -4.2766776 -4.4041004 -4.4747434 -4.5147347 -4.5130677][-4.2905865 -4.2475548 -4.2355685 -4.2131653 -4.124342 -3.9453738 -3.7587609 -3.6989794 -3.7983336 -3.9850242 -4.1827893 -4.3355675 -4.4286165 -4.4865475 -4.4880414][-4.1583681 -4.1437488 -4.1660876 -4.1729941 -4.0886364 -3.8830922 -3.6742148 -3.6225526 -3.7480304 -3.9629815 -4.1706634 -4.3025556 -4.3660083 -4.3961687 -4.376647][-4.0928211 -4.1056118 -4.1643372 -4.2108316 -4.1568208 -3.9643369 -3.7646258 -3.72306 -3.8563676 -4.069797 -4.2485313 -4.3187394 -4.3182945 -4.2973948 -4.252142][-4.1012506 -4.1421013 -4.2324491 -4.3095412 -4.2930784 -4.1419291 -3.9767394 -3.9492433 -4.0737171 -4.2571235 -4.3830886 -4.3863416 -4.3266454 -4.2654519 -4.2063642][-4.1843262 -4.2522917 -4.3612709 -4.4452715 -4.451798 -4.3459663 -4.2218847 -4.2035437 -4.3049197 -4.4435472 -4.5138297 -4.4715066 -4.3816471 -4.3076525 -4.2516103][-4.3399239 -4.4197073 -4.5175271 -4.5795369 -4.5864143 -4.5167046 -4.4323173 -4.4211593 -4.4960675 -4.5878992 -4.6129603 -4.5526533 -4.4654918 -4.40559 -4.3666654][-4.4960861 -4.5775 -4.6497278 -4.6800728 -4.6780896 -4.6346231 -4.5855532 -4.5810194 -4.6310992 -4.686379 -4.688056 -4.6328239 -4.5687132 -4.5330153 -4.512651][-4.6022539 -4.6828794 -4.7357368 -4.7497692 -4.7411389 -4.7068367 -4.6704235 -4.6592665 -4.6840191 -4.7189059 -4.7214437 -4.688436 -4.6507077 -4.6307359 -4.6158643]]...]
INFO - root - 2017-12-07 13:08:40.464911: step 18610, loss = 21.54, batch loss = 21.46 (8.4 examples/sec; 0.955 sec/batch; 83h:18m:02s remains)
INFO - root - 2017-12-07 13:08:49.821779: step 18620, loss = 21.20, batch loss = 21.12 (8.2 examples/sec; 0.972 sec/batch; 84h:43m:08s remains)
INFO - root - 2017-12-07 13:08:59.042975: step 18630, loss = 21.31, batch loss = 21.23 (9.3 examples/sec; 0.861 sec/batch; 75h:05m:15s remains)
INFO - root - 2017-12-07 13:09:08.418055: step 18640, loss = 21.62, batch loss = 21.54 (8.8 examples/sec; 0.906 sec/batch; 79h:00m:54s remains)
INFO - root - 2017-12-07 13:09:17.794681: step 18650, loss = 21.48, batch loss = 21.40 (8.6 examples/sec; 0.930 sec/batch; 81h:06m:15s remains)
INFO - root - 2017-12-07 13:09:27.235322: step 18660, loss = 21.43, batch loss = 21.34 (8.3 examples/sec; 0.963 sec/batch; 83h:59m:20s remains)
INFO - root - 2017-12-07 13:09:36.553572: step 18670, loss = 21.45, batch loss = 21.37 (8.2 examples/sec; 0.980 sec/batch; 85h:25m:53s remains)
INFO - root - 2017-12-07 13:09:45.778301: step 18680, loss = 21.47, batch loss = 21.39 (8.7 examples/sec; 0.923 sec/batch; 80h:29m:48s remains)
INFO - root - 2017-12-07 13:09:55.123150: step 18690, loss = 21.62, batch loss = 21.54 (8.9 examples/sec; 0.898 sec/batch; 78h:17m:41s remains)
INFO - root - 2017-12-07 13:10:04.508564: step 18700, loss = 21.79, batch loss = 21.71 (8.6 examples/sec; 0.929 sec/batch; 80h:58m:05s remains)
2017-12-07 13:10:05.460850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.41825 -4.3899035 -4.384244 -4.3699975 -4.3247242 -4.3551917 -4.4944496 -4.6234436 -4.6760778 -4.6424141 -4.5484281 -4.480382 -4.4615045 -4.4698639 -4.4746208][-4.297009 -4.243928 -4.2120137 -4.198236 -4.171505 -4.205915 -4.3381276 -4.479475 -4.5827489 -4.6090221 -4.549067 -4.4708476 -4.4110065 -4.3771834 -4.3584247][-4.1902151 -4.12195 -4.072298 -4.0630159 -4.0546727 -4.07603 -4.1707573 -4.3019705 -4.4460597 -4.53917 -4.5323029 -4.45435 -4.3521423 -4.2682962 -4.2231016][-4.1265974 -4.0503378 -3.9895248 -3.9842522 -3.9907446 -3.99993 -4.0570641 -4.1692791 -4.33262 -4.4743524 -4.5155387 -4.4524765 -4.3285222 -4.2057729 -4.141223][-4.1281247 -4.0467854 -3.9779716 -3.969451 -3.9808419 -3.9758706 -3.9920812 -4.070291 -4.2236471 -4.3876743 -4.4650221 -4.4342928 -4.3248954 -4.1997347 -4.1417871][-4.1896172 -4.1160674 -4.0389237 -4.0138383 -4.0159178 -3.9912534 -3.9625506 -3.9909816 -4.10469 -4.2648592 -4.3655448 -4.3772559 -4.311595 -4.2158289 -4.1792722][-4.2754602 -4.2421808 -4.1797361 -4.139936 -4.1125226 -4.0465994 -3.9587646 -3.9181759 -3.9731693 -4.1134529 -4.2340832 -4.2950177 -4.2871923 -4.2305307 -4.2077188][-4.3431449 -4.3680882 -4.3507476 -4.3188891 -4.2630062 -4.1444259 -3.9858458 -3.8694763 -3.866466 -3.9874554 -4.1284785 -4.2334766 -4.2783427 -4.2533584 -4.2320309][-4.3928332 -4.4602036 -4.4907746 -4.4892287 -4.4301481 -4.2801118 -4.0705128 -3.8979692 -3.8598015 -3.97473 -4.13419 -4.2622161 -4.3330398 -4.3317146 -4.3108535][-4.4494691 -4.5242968 -4.5753245 -4.6019111 -4.5676351 -4.428546 -4.2042341 -4.0015454 -3.9463995 -4.0588932 -4.225718 -4.347528 -4.4137878 -4.4293375 -4.426054][-4.4838848 -4.5467825 -4.5910287 -4.6302271 -4.6318254 -4.5378246 -4.3415666 -4.1414957 -4.0752916 -4.1667113 -4.3132505 -4.4097271 -4.4547009 -4.4767852 -4.5014458][-4.4998465 -4.5520983 -4.5775318 -4.6086588 -4.6336951 -4.598947 -4.4675012 -4.3099923 -4.2448626 -4.2997913 -4.4041567 -4.4680948 -4.488296 -4.4985905 -4.5357018][-4.5368671 -4.5781078 -4.5838428 -4.594862 -4.6207533 -4.6279769 -4.5699749 -4.4801707 -4.4313145 -4.449965 -4.503973 -4.5360975 -4.5369 -4.5270071 -4.5499544][-4.5701346 -4.5987167 -4.5904346 -4.5839882 -4.59894 -4.6228848 -4.6166377 -4.5858192 -4.5618114 -4.5608182 -4.5755329 -4.581974 -4.5712557 -4.5493245 -4.5502858][-4.5561962 -4.565361 -4.5525475 -4.5425506 -4.550766 -4.5744834 -4.5878458 -4.5862 -4.5800986 -4.5773787 -4.5755606 -4.5698757 -4.5572104 -4.5363989 -4.5236011]]...]
INFO - root - 2017-12-07 13:10:14.943367: step 18710, loss = 21.89, batch loss = 21.80 (8.4 examples/sec; 0.958 sec/batch; 83h:27m:43s remains)
INFO - root - 2017-12-07 13:10:24.442433: step 18720, loss = 21.53, batch loss = 21.44 (8.3 examples/sec; 0.962 sec/batch; 83h:52m:09s remains)
INFO - root - 2017-12-07 13:10:33.877102: step 18730, loss = 21.27, batch loss = 21.19 (8.4 examples/sec; 0.954 sec/batch; 83h:11m:04s remains)
INFO - root - 2017-12-07 13:10:43.327329: step 18740, loss = 21.08, batch loss = 20.99 (8.2 examples/sec; 0.972 sec/batch; 84h:44m:57s remains)
INFO - root - 2017-12-07 13:10:52.787075: step 18750, loss = 21.72, batch loss = 21.64 (8.5 examples/sec; 0.936 sec/batch; 81h:35m:46s remains)
INFO - root - 2017-12-07 13:11:02.094324: step 18760, loss = 21.40, batch loss = 21.32 (9.1 examples/sec; 0.881 sec/batch; 76h:47m:01s remains)
INFO - root - 2017-12-07 13:11:11.377317: step 18770, loss = 21.56, batch loss = 21.47 (9.2 examples/sec; 0.868 sec/batch; 75h:41m:02s remains)
INFO - root - 2017-12-07 13:11:20.467823: step 18780, loss = 21.39, batch loss = 21.31 (8.7 examples/sec; 0.918 sec/batch; 79h:58m:03s remains)
INFO - root - 2017-12-07 13:11:29.844066: step 18790, loss = 21.41, batch loss = 21.33 (8.9 examples/sec; 0.896 sec/batch; 78h:06m:59s remains)
INFO - root - 2017-12-07 13:11:39.228605: step 18800, loss = 21.37, batch loss = 21.29 (8.3 examples/sec; 0.966 sec/batch; 84h:10m:31s remains)
2017-12-07 13:11:40.215921: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3735256 -4.459415 -4.5183105 -4.5356107 -4.5112467 -4.451674 -4.3817449 -4.3170362 -4.2675648 -4.2428885 -4.2468963 -4.2571011 -4.2440906 -4.2137794 -4.2122369][-4.3319945 -4.424036 -4.4989042 -4.53627 -4.5251102 -4.466001 -4.389955 -4.3195605 -4.2687831 -4.2508755 -4.2595496 -4.266593 -4.2448215 -4.1994667 -4.1830435][-4.2910261 -4.3876448 -4.4763527 -4.526125 -4.5161042 -4.4460664 -4.3577843 -4.2868018 -4.2489495 -4.2524366 -4.2769742 -4.2922034 -4.2698879 -4.2115712 -4.1731324][-4.2751474 -4.3753834 -4.4699421 -4.5156431 -4.4816937 -4.3751178 -4.256094 -4.1801229 -4.1637669 -4.2001176 -4.2538319 -4.2889509 -4.279727 -4.2256322 -4.1797061][-4.2694578 -4.37056 -4.4602933 -4.483593 -4.4051023 -4.2474236 -4.0936713 -4.0179548 -4.0314212 -4.1030569 -4.1855483 -4.2460155 -4.2667603 -4.2420473 -4.2087555][-4.2876592 -4.3930125 -4.4709215 -4.4597149 -4.3303089 -4.1236815 -3.9390604 -3.8630843 -3.89955 -3.9934726 -4.0920558 -4.1778307 -4.2360544 -4.24676 -4.2317972][-4.358984 -4.4555511 -4.5070586 -4.4523892 -4.2706628 -4.0197277 -3.8066916 -3.7262983 -3.7802448 -3.8928156 -4.0065737 -4.1205745 -4.2207556 -4.2676582 -4.2677379][-4.4186664 -4.4965267 -4.5191226 -4.4304438 -4.2163262 -3.9400971 -3.7093456 -3.626878 -3.7018242 -3.8403311 -3.9748249 -4.1126056 -4.24907 -4.3287635 -4.3379145][-4.4497809 -4.516161 -4.52862 -4.4387755 -4.24125 -3.9879975 -3.7728832 -3.6969249 -3.7738819 -3.90885 -4.0286865 -4.1480188 -4.281074 -4.3720274 -4.382946][-4.4557009 -4.5134273 -4.51969 -4.4448233 -4.2939863 -4.1014853 -3.9422107 -3.8970063 -3.9651475 -4.0588713 -4.1212187 -4.1852226 -4.28315 -4.3655124 -4.3724594][-4.4151535 -4.4689608 -4.4724083 -4.414834 -4.3107657 -4.1765723 -4.0705543 -4.0542951 -4.1070938 -4.1507249 -4.1604786 -4.1877337 -4.2677016 -4.3503647 -4.361083][-4.396553 -4.4430957 -4.4434333 -4.4044456 -4.3428211 -4.2568378 -4.1838889 -4.168653 -4.187408 -4.1822643 -4.1606817 -4.1767054 -4.2518196 -4.336297 -4.3560534][-4.4050689 -4.4406772 -4.436902 -4.415935 -4.3914657 -4.3476686 -4.2988043 -4.2724886 -4.2551737 -4.2168341 -4.1822166 -4.1939158 -4.2590084 -4.3349581 -4.3581715][-4.4160366 -4.4466686 -4.4533043 -4.45564 -4.4613304 -4.4487257 -4.4103627 -4.3591609 -4.2988648 -4.2352133 -4.2032537 -4.2314253 -4.3056011 -4.3800178 -4.4050517][-4.4458618 -4.4813681 -4.5061669 -4.5318007 -4.5596232 -4.5649352 -4.5265174 -4.4454975 -4.3471489 -4.2692485 -4.2512393 -4.2998476 -4.37743 -4.4376855 -4.453259]]...]
INFO - root - 2017-12-07 13:11:49.673962: step 18810, loss = 21.65, batch loss = 21.57 (8.4 examples/sec; 0.948 sec/batch; 82h:34m:29s remains)
INFO - root - 2017-12-07 13:11:59.126032: step 18820, loss = 21.76, batch loss = 21.68 (8.9 examples/sec; 0.896 sec/batch; 78h:03m:13s remains)
INFO - root - 2017-12-07 13:12:08.628331: step 18830, loss = 21.41, batch loss = 21.33 (8.8 examples/sec; 0.906 sec/batch; 78h:56m:04s remains)
INFO - root - 2017-12-07 13:12:18.197451: step 18840, loss = 21.34, batch loss = 21.26 (8.9 examples/sec; 0.902 sec/batch; 78h:36m:19s remains)
INFO - root - 2017-12-07 13:12:27.601177: step 18850, loss = 21.68, batch loss = 21.60 (8.9 examples/sec; 0.904 sec/batch; 78h:43m:18s remains)
INFO - root - 2017-12-07 13:12:36.986341: step 18860, loss = 20.98, batch loss = 20.90 (8.1 examples/sec; 0.991 sec/batch; 86h:20m:35s remains)
INFO - root - 2017-12-07 13:12:46.434384: step 18870, loss = 21.31, batch loss = 21.23 (8.5 examples/sec; 0.940 sec/batch; 81h:55m:32s remains)
INFO - root - 2017-12-07 13:12:55.721171: step 18880, loss = 21.48, batch loss = 21.40 (9.0 examples/sec; 0.892 sec/batch; 77h:41m:31s remains)
INFO - root - 2017-12-07 13:13:05.028973: step 18890, loss = 20.75, batch loss = 20.67 (8.6 examples/sec; 0.930 sec/batch; 81h:03m:26s remains)
INFO - root - 2017-12-07 13:13:14.369286: step 18900, loss = 21.29, batch loss = 21.21 (8.7 examples/sec; 0.925 sec/batch; 80h:33m:48s remains)
2017-12-07 13:13:15.275485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4342656 -4.4402657 -4.4426131 -4.4409657 -4.4379745 -4.4365058 -4.4376659 -4.4391761 -4.4392915 -4.437211 -4.4340849 -4.4325104 -4.4336443 -4.4368734 -4.4397507][-4.49647 -4.5104856 -4.5167785 -4.51638 -4.5117197 -4.5062141 -4.5013008 -4.49501 -4.4856262 -4.474854 -4.4658017 -4.4613276 -4.4616446 -4.4652362 -4.4694138][-4.5558658 -4.5675707 -4.5738716 -4.5780244 -4.5784588 -4.5728841 -4.5597949 -4.5394135 -4.5174026 -4.5009828 -4.4933171 -4.4942255 -4.5002112 -4.5058341 -4.5082388][-4.5558438 -4.5590539 -4.5632038 -4.5722852 -4.5824223 -4.5848737 -4.5688691 -4.5374985 -4.5076456 -4.4923925 -4.4922915 -4.5033045 -4.5222616 -4.5391684 -4.5495839][-4.4313021 -4.4292712 -4.4390841 -4.4510846 -4.4614735 -4.4693828 -4.4533486 -4.4193392 -4.3942118 -4.388906 -4.3958731 -4.4102325 -4.4357576 -4.4676638 -4.500237][-4.2209544 -4.1998749 -4.2136636 -4.2173467 -4.2094374 -4.2096739 -4.18854 -4.1580782 -4.1566143 -4.1862535 -4.21787 -4.2449265 -4.2822351 -4.3316827 -4.3859549][-4.0533266 -3.9840045 -3.9794202 -3.9608672 -3.9177077 -3.8941612 -3.8496644 -3.8105941 -3.8402524 -3.9201484 -3.9915183 -4.0488729 -4.1182008 -4.2039132 -4.2868891][-4.0225039 -3.9065735 -3.8705947 -3.8239141 -3.7463644 -3.6958747 -3.618021 -3.5481882 -3.5825753 -3.6832056 -3.769423 -3.8406954 -3.9351385 -4.064867 -4.1931367][-4.1063981 -3.9747381 -3.9174805 -3.8524673 -3.7622719 -3.71039 -3.6318057 -3.5556593 -3.5849218 -3.6743526 -3.7356639 -3.7820427 -3.863708 -4.0016088 -4.1501522][-4.2115879 -4.0887685 -4.0268226 -3.9673793 -3.8909106 -3.8553188 -3.8026292 -3.7495682 -3.7838669 -3.8598697 -3.8934023 -3.904264 -3.9510353 -4.0573211 -4.1874847][-4.2941289 -4.1876755 -4.1265 -4.0778275 -4.0242662 -4.0089149 -3.98488 -3.9458048 -3.9715316 -4.0306849 -4.0504603 -4.0453606 -4.0694294 -4.1499987 -4.2562451][-4.3847661 -4.2946095 -4.2289562 -4.175272 -4.1283941 -4.1289439 -4.1331067 -4.1034083 -4.1064239 -4.1433845 -4.1634507 -4.1588988 -4.172893 -4.2515178 -4.3548913][-4.5284967 -4.4592166 -4.3875918 -4.309813 -4.242228 -4.2393465 -4.2603827 -4.2453952 -4.2391748 -4.2687769 -4.30127 -4.3048906 -4.3144169 -4.394608 -4.4895563][-4.6383252 -4.5926805 -4.53119 -4.4446692 -4.3609681 -4.3465381 -4.3688493 -4.3665652 -4.3626719 -4.3934441 -4.441524 -4.4603372 -4.4730062 -4.5468106 -4.6131845][-4.647717 -4.62797 -4.5940924 -4.5255389 -4.4489474 -4.4302406 -4.4454885 -4.4495835 -4.4461 -4.4690032 -4.5138931 -4.53894 -4.5515804 -4.60913 -4.6482525]]...]
INFO - root - 2017-12-07 13:13:24.716179: step 18910, loss = 21.35, batch loss = 21.27 (8.4 examples/sec; 0.950 sec/batch; 82h:47m:13s remains)
INFO - root - 2017-12-07 13:13:34.058848: step 18920, loss = 21.17, batch loss = 21.09 (8.2 examples/sec; 0.975 sec/batch; 84h:57m:35s remains)
INFO - root - 2017-12-07 13:13:43.414986: step 18930, loss = 21.25, batch loss = 21.16 (8.4 examples/sec; 0.958 sec/batch; 83h:26m:09s remains)
INFO - root - 2017-12-07 13:13:52.645177: step 18940, loss = 21.36, batch loss = 21.27 (8.5 examples/sec; 0.939 sec/batch; 81h:47m:42s remains)
INFO - root - 2017-12-07 13:14:02.031789: step 18950, loss = 21.23, batch loss = 21.15 (8.5 examples/sec; 0.945 sec/batch; 82h:16m:51s remains)
INFO - root - 2017-12-07 13:14:11.392491: step 18960, loss = 21.52, batch loss = 21.44 (9.0 examples/sec; 0.888 sec/batch; 77h:21m:48s remains)
INFO - root - 2017-12-07 13:14:20.559450: step 18970, loss = 21.71, batch loss = 21.63 (9.1 examples/sec; 0.882 sec/batch; 76h:50m:43s remains)
INFO - root - 2017-12-07 13:14:29.888715: step 18980, loss = 21.67, batch loss = 21.58 (9.0 examples/sec; 0.890 sec/batch; 77h:32m:03s remains)
INFO - root - 2017-12-07 13:14:39.186759: step 18990, loss = 21.53, batch loss = 21.44 (8.9 examples/sec; 0.895 sec/batch; 77h:58m:34s remains)
INFO - root - 2017-12-07 13:14:48.490358: step 19000, loss = 21.45, batch loss = 21.37 (8.9 examples/sec; 0.898 sec/batch; 78h:10m:06s remains)
2017-12-07 13:14:49.433621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4950547 -4.5691309 -4.5845609 -4.5208712 -4.4237294 -4.3090124 -4.2472987 -4.2728057 -4.3553038 -4.4448838 -4.5130582 -4.5575528 -4.5708323 -4.5387468 -4.4652338][-4.5120711 -4.5575161 -4.5427279 -4.4674377 -4.3748231 -4.2785983 -4.2223978 -4.232306 -4.2951374 -4.3732529 -4.4467125 -4.512104 -4.5578055 -4.5587187 -4.4965534][-4.5664239 -4.6047225 -4.5751414 -4.4912128 -4.3966293 -4.3023033 -4.2234526 -4.1875997 -4.2121339 -4.2727041 -4.3563671 -4.4470649 -4.5251889 -4.558126 -4.5101871][-4.636384 -4.6883583 -4.6583552 -4.5639977 -4.4509177 -4.3267536 -4.1986771 -4.1162167 -4.1138344 -4.1677737 -4.2672405 -4.3831768 -4.4862452 -4.5411077 -4.5039916][-4.6853943 -4.7405415 -4.6945825 -4.5707479 -4.4175239 -4.2466526 -4.0794525 -3.99029 -4.01171 -4.0962596 -4.2275968 -4.3640337 -4.4732094 -4.5285516 -4.491035][-4.6769624 -4.7105823 -4.6258311 -4.4547076 -4.2530975 -4.0443387 -3.8677423 -3.8214052 -3.9209998 -4.076766 -4.2501988 -4.3944988 -4.4851584 -4.5190449 -4.4724064][-4.6349831 -4.6409955 -4.51925 -4.3034816 -4.06102 -3.8263516 -3.6515362 -3.6550646 -3.8432956 -4.0784516 -4.2887692 -4.4253592 -4.4834347 -4.4924469 -4.4436917][-4.5937834 -4.5762558 -4.4303732 -4.1941705 -3.9470644 -3.7173064 -3.5535293 -3.5855379 -3.8157783 -4.0862856 -4.2997093 -4.41194 -4.4467854 -4.4532285 -4.4182076][-4.5634975 -4.5324292 -4.3862514 -4.1723061 -3.9717469 -3.792666 -3.6636279 -3.6936941 -3.8899629 -4.1157713 -4.284605 -4.3703566 -4.4100714 -4.4384494 -4.4211006][-4.5468431 -4.5246253 -4.4146147 -4.2562094 -4.1169472 -3.9830155 -3.8722405 -3.8748639 -4.0016785 -4.1520419 -4.2709351 -4.3496728 -4.4155684 -4.4650149 -4.4491611][-4.5415425 -4.5468421 -4.4929457 -4.3920074 -4.2856092 -4.1559591 -4.0305219 -4.0032043 -4.086585 -4.1994739 -4.3041306 -4.3945332 -4.4788 -4.5243964 -4.4886451][-4.542088 -4.5752983 -4.5670629 -4.5025849 -4.4024763 -4.2549229 -4.10713 -4.0661063 -4.1442523 -4.2656155 -4.3850646 -4.4879031 -4.5702333 -4.5909863 -4.5256343][-4.5597143 -4.6160545 -4.6366606 -4.59059 -4.4876008 -4.3266072 -4.1689029 -4.1249456 -4.2084222 -4.3470483 -4.4803915 -4.5821977 -4.6447663 -4.6358132 -4.5480742][-4.5832357 -4.6536016 -4.6898012 -4.6567307 -4.5610456 -4.4120893 -4.2758737 -4.24731 -4.3346391 -4.4717631 -4.5928249 -4.6685834 -4.6953225 -4.6563678 -4.5567727][-4.5893664 -4.6624236 -4.70613 -4.6900253 -4.6203947 -4.5134292 -4.4258871 -4.4219179 -4.4995565 -4.6051278 -4.6867733 -4.7224154 -4.7116995 -4.6499677 -4.5504432]]...]
INFO - root - 2017-12-07 13:14:58.776189: step 19010, loss = 21.34, batch loss = 21.26 (9.1 examples/sec; 0.880 sec/batch; 76h:38m:02s remains)
INFO - root - 2017-12-07 13:15:08.102823: step 19020, loss = 21.57, batch loss = 21.49 (9.1 examples/sec; 0.878 sec/batch; 76h:24m:51s remains)
INFO - root - 2017-12-07 13:15:17.485256: step 19030, loss = 21.45, batch loss = 21.37 (8.5 examples/sec; 0.941 sec/batch; 81h:56m:20s remains)
INFO - root - 2017-12-07 13:15:26.889101: step 19040, loss = 21.56, batch loss = 21.47 (8.0 examples/sec; 0.996 sec/batch; 86h:45m:24s remains)
INFO - root - 2017-12-07 13:15:36.234858: step 19050, loss = 21.73, batch loss = 21.65 (7.6 examples/sec; 1.058 sec/batch; 92h:07m:21s remains)
INFO - root - 2017-12-07 13:15:45.568308: step 19060, loss = 21.43, batch loss = 21.35 (8.7 examples/sec; 0.918 sec/batch; 79h:53m:11s remains)
INFO - root - 2017-12-07 13:15:54.870240: step 19070, loss = 21.37, batch loss = 21.28 (8.9 examples/sec; 0.903 sec/batch; 78h:37m:55s remains)
INFO - root - 2017-12-07 13:16:04.371964: step 19080, loss = 21.55, batch loss = 21.47 (8.7 examples/sec; 0.922 sec/batch; 80h:17m:26s remains)
INFO - root - 2017-12-07 13:16:13.692640: step 19090, loss = 21.27, batch loss = 21.19 (8.0 examples/sec; 0.997 sec/batch; 86h:46m:04s remains)
INFO - root - 2017-12-07 13:16:23.041974: step 19100, loss = 21.20, batch loss = 21.11 (8.4 examples/sec; 0.950 sec/batch; 82h:41m:54s remains)
2017-12-07 13:16:23.938728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4271903 -4.4493232 -4.4688463 -4.485445 -4.5001907 -4.5112062 -4.5104074 -4.499413 -4.4967241 -4.5165386 -4.5441751 -4.5576029 -4.5431566 -4.5066514 -4.4592276][-4.5403371 -4.5739455 -4.5961332 -4.6100168 -4.6160879 -4.6168871 -4.5955529 -4.5562987 -4.5425858 -4.57932 -4.6390152 -4.6769123 -4.6685338 -4.6216712 -4.5537119][-4.6592493 -4.6888442 -4.6978769 -4.6949058 -4.6813397 -4.6628337 -4.6096368 -4.5282788 -4.495923 -4.5586114 -4.6658764 -4.7418261 -4.7483997 -4.7003818 -4.6236057][-4.7290745 -4.743649 -4.7332668 -4.7108736 -4.6744189 -4.6338291 -4.545804 -4.4223104 -4.3769755 -4.4743457 -4.6330366 -4.7470436 -4.7679381 -4.7254791 -4.6498747][-4.6989574 -4.6882596 -4.6561136 -4.6146326 -4.557488 -4.5008597 -4.3963552 -4.2566762 -4.2179427 -4.3512177 -4.5476623 -4.68492 -4.7201385 -4.6958981 -4.6338091][-4.5952988 -4.5456829 -4.4784532 -4.408308 -4.327939 -4.2622428 -4.1682305 -4.0510569 -4.0408697 -4.1996756 -4.4046249 -4.5424695 -4.5951877 -4.6102805 -4.5812249][-4.5079575 -4.4174018 -4.3067012 -4.1965885 -4.0827494 -3.9983993 -3.9156845 -3.8391986 -3.8787858 -4.0593252 -4.2461834 -4.3623271 -4.4309807 -4.4947195 -4.5099926][-4.5112033 -4.395607 -4.2472687 -4.0983148 -3.9510746 -3.8368554 -3.7456543 -3.6987193 -3.7963393 -4.009891 -4.1765356 -4.2589607 -4.3251605 -4.4119859 -4.4527307][-4.5915356 -4.473012 -4.3037887 -4.1381316 -3.9828386 -3.8504963 -3.7439108 -3.7084908 -3.8548851 -4.1092134 -4.2674146 -4.317637 -4.3524203 -4.413815 -4.43701][-4.6471968 -4.5457559 -4.381372 -4.2250371 -4.083837 -3.9548812 -3.8481607 -3.820611 -3.9926524 -4.2716141 -4.4316883 -4.469306 -4.4719682 -4.4829116 -4.460793][-4.6492748 -4.5901184 -4.4629331 -4.3368816 -4.2162056 -4.0992165 -4.0062518 -3.9860771 -4.14457 -4.4010978 -4.55093 -4.5896707 -4.5777969 -4.5511813 -4.49262][-4.6279287 -4.6205726 -4.5551357 -4.4826341 -4.4012656 -4.3121715 -4.2401581 -4.2167411 -4.319365 -4.4979954 -4.6022 -4.627522 -4.6106896 -4.572578 -4.5017872][-4.5835338 -4.6126876 -4.6053925 -4.594058 -4.5680742 -4.5222831 -4.4754133 -4.4439354 -4.479238 -4.5594068 -4.6001782 -4.5998106 -4.5772114 -4.5402479 -4.4758539][-4.5035973 -4.5408044 -4.5632763 -4.5870385 -4.6022706 -4.597877 -4.5785608 -4.550889 -4.5434275 -4.5521145 -4.5460153 -4.5271244 -4.5024824 -4.4709029 -4.4212637][-4.4109979 -4.4366345 -4.460762 -4.4875236 -4.5121574 -4.5270047 -4.5294647 -4.5172558 -4.5012956 -4.4827509 -4.4598012 -4.4382219 -4.4172397 -4.3923693 -4.359405]]...]
INFO - root - 2017-12-07 13:16:33.368884: step 19110, loss = 21.17, batch loss = 21.09 (8.5 examples/sec; 0.937 sec/batch; 81h:35m:41s remains)
INFO - root - 2017-12-07 13:16:42.667374: step 19120, loss = 21.62, batch loss = 21.53 (8.5 examples/sec; 0.945 sec/batch; 82h:15m:34s remains)
INFO - root - 2017-12-07 13:16:51.953248: step 19130, loss = 21.29, batch loss = 21.21 (8.7 examples/sec; 0.925 sec/batch; 80h:29m:18s remains)
INFO - root - 2017-12-07 13:17:01.291896: step 19140, loss = 21.13, batch loss = 21.05 (9.8 examples/sec; 0.819 sec/batch; 71h:19m:39s remains)
INFO - root - 2017-12-07 13:17:10.737448: step 19150, loss = 21.37, batch loss = 21.28 (8.5 examples/sec; 0.944 sec/batch; 82h:12m:35s remains)
INFO - root - 2017-12-07 13:17:20.155972: step 19160, loss = 21.42, batch loss = 21.33 (8.7 examples/sec; 0.915 sec/batch; 79h:40m:28s remains)
INFO - root - 2017-12-07 13:17:29.518731: step 19170, loss = 20.88, batch loss = 20.79 (8.6 examples/sec; 0.929 sec/batch; 80h:49m:56s remains)
INFO - root - 2017-12-07 13:17:39.048587: step 19180, loss = 21.77, batch loss = 21.69 (8.6 examples/sec; 0.931 sec/batch; 81h:01m:25s remains)
INFO - root - 2017-12-07 13:17:48.199335: step 19190, loss = 21.46, batch loss = 21.38 (9.4 examples/sec; 0.847 sec/batch; 73h:42m:37s remains)
INFO - root - 2017-12-07 13:17:57.584134: step 19200, loss = 21.50, batch loss = 21.42 (7.8 examples/sec; 1.021 sec/batch; 88h:50m:26s remains)
2017-12-07 13:17:58.482462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6882291 -4.718574 -4.7267652 -4.6871867 -4.6081619 -4.5571785 -4.587182 -4.6862426 -4.7562928 -4.725296 -4.6125493 -4.46179 -4.3180952 -4.224998 -4.196342][-4.76031 -4.77179 -4.7499294 -4.6758981 -4.5621824 -4.4845223 -4.5103712 -4.630208 -4.7354026 -4.7353811 -4.6347847 -4.4787135 -4.3239737 -4.2254233 -4.1966734][-4.8016338 -4.7808833 -4.7306108 -4.6395211 -4.5039434 -4.3938842 -4.39971 -4.5310483 -4.6735125 -4.7151666 -4.6398053 -4.4901819 -4.33109 -4.2302203 -4.2041979][-4.8033576 -4.7518239 -4.6910024 -4.6029239 -4.4513378 -4.3012986 -4.2766342 -4.4123778 -4.5902591 -4.677742 -4.6389284 -4.5058956 -4.3463836 -4.2412138 -4.2146358][-4.7193227 -4.6541815 -4.6112742 -4.5514274 -4.405468 -4.2372241 -4.1968865 -4.3322368 -4.5216708 -4.6332903 -4.6296983 -4.5231686 -4.3704729 -4.2601256 -4.2267423][-4.5705628 -4.5270824 -4.5305929 -4.5058632 -4.3738089 -4.2084527 -4.1663489 -4.2892137 -4.4576483 -4.566072 -4.5920863 -4.521965 -4.3895226 -4.2804227 -4.2386093][-4.3650451 -4.36845 -4.4374065 -4.4474978 -4.3333721 -4.1872783 -4.1484237 -4.2552776 -4.3972659 -4.49955 -4.5501232 -4.5135517 -4.4040661 -4.298862 -4.249373][-4.1148567 -4.1706133 -4.3107176 -4.3624 -4.2826548 -4.1721544 -4.1364908 -4.2113461 -4.3165436 -4.4131532 -4.4908648 -4.4906325 -4.4079914 -4.3118186 -4.2589021][-3.9312119 -4.0187798 -4.2034011 -4.2871079 -4.2423906 -4.1643 -4.1202326 -4.143507 -4.2026949 -4.29374 -4.4019876 -4.4435029 -4.3945637 -4.3149877 -4.2647371][-3.8496525 -3.9440246 -4.1314397 -4.2164626 -4.1837864 -4.125958 -4.0816607 -4.0773163 -4.1187882 -4.2181568 -4.3484635 -4.4143281 -4.384625 -4.3155894 -4.2680016][-3.8228922 -3.9046624 -4.0729108 -4.1530428 -4.1327763 -4.09419 -4.0663538 -4.0695124 -4.1197543 -4.2274022 -4.35813 -4.4203053 -4.3883386 -4.3187842 -4.2701755][-3.8701339 -3.9318347 -4.0692835 -4.1444139 -4.1421828 -4.1187878 -4.1047764 -4.1245027 -4.1869993 -4.28979 -4.3982391 -4.4387631 -4.3952975 -4.3210077 -4.2705774][-3.9790037 -4.018971 -4.1171 -4.1750135 -4.1846108 -4.1693769 -4.1621065 -4.19556 -4.266232 -4.3585019 -4.4379678 -4.4516163 -4.3943868 -4.3159695 -4.2667246][-4.0969467 -4.1198616 -4.1822534 -4.217566 -4.2361307 -4.2355742 -4.2378888 -4.278223 -4.34581 -4.4212246 -4.4710832 -4.4569573 -4.3841639 -4.3039026 -4.259686][-4.1805053 -4.2070165 -4.2525048 -4.26407 -4.2839036 -4.3005424 -4.3147469 -4.3531938 -4.4045296 -4.4601493 -4.4893456 -4.455554 -4.3712859 -4.2914157 -4.253665]]...]
INFO - root - 2017-12-07 13:18:07.866141: step 19210, loss = 21.31, batch loss = 21.23 (8.8 examples/sec; 0.905 sec/batch; 78h:46m:06s remains)
INFO - root - 2017-12-07 13:18:17.330998: step 19220, loss = 21.49, batch loss = 21.41 (8.1 examples/sec; 0.983 sec/batch; 85h:30m:24s remains)
INFO - root - 2017-12-07 13:18:26.840186: step 19230, loss = 21.62, batch loss = 21.54 (8.6 examples/sec; 0.932 sec/batch; 81h:07m:24s remains)
INFO - root - 2017-12-07 13:18:36.192247: step 19240, loss = 22.02, batch loss = 21.94 (8.7 examples/sec; 0.918 sec/batch; 79h:53m:39s remains)
INFO - root - 2017-12-07 13:18:45.453636: step 19250, loss = 21.83, batch loss = 21.74 (9.0 examples/sec; 0.887 sec/batch; 77h:11m:23s remains)
INFO - root - 2017-12-07 13:18:54.721831: step 19260, loss = 21.19, batch loss = 21.11 (9.4 examples/sec; 0.855 sec/batch; 74h:24m:20s remains)
INFO - root - 2017-12-07 13:19:04.146098: step 19270, loss = 21.70, batch loss = 21.61 (8.0 examples/sec; 0.999 sec/batch; 86h:54m:42s remains)
INFO - root - 2017-12-07 13:19:13.466640: step 19280, loss = 21.35, batch loss = 21.26 (8.3 examples/sec; 0.963 sec/batch; 83h:44m:50s remains)
INFO - root - 2017-12-07 13:19:22.888500: step 19290, loss = 21.00, batch loss = 20.91 (8.4 examples/sec; 0.958 sec/batch; 83h:19m:14s remains)
INFO - root - 2017-12-07 13:19:32.158408: step 19300, loss = 21.49, batch loss = 21.41 (8.3 examples/sec; 0.961 sec/batch; 83h:38m:05s remains)
2017-12-07 13:19:33.048569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.635078 -4.6906471 -4.7244992 -4.711494 -4.6415987 -4.5769281 -4.6068163 -4.6925321 -4.7378273 -4.749609 -4.7591758 -4.752974 -4.6836329 -4.5445123 -4.407043][-4.6656122 -4.7048779 -4.7342315 -4.7152758 -4.6245804 -4.5326819 -4.5509 -4.6468315 -4.7172246 -4.7662358 -4.8201742 -4.8385024 -4.7467961 -4.5394144 -4.3249636][-4.6487708 -4.6599493 -4.6834736 -4.6711178 -4.5748315 -4.4499493 -4.4161911 -4.4778624 -4.5643497 -4.6758738 -4.8118882 -4.8917456 -4.8124442 -4.5640798 -4.277143][-4.5991325 -4.5739713 -4.5890784 -4.5928087 -4.5095439 -4.3563089 -4.2413049 -4.2253895 -4.3018856 -4.4662118 -4.6855273 -4.8440375 -4.8201189 -4.588902 -4.2720685][-4.5363092 -4.4713411 -4.4731731 -4.4965496 -4.4387555 -4.2695332 -4.0717931 -3.9600973 -3.9970698 -4.1802149 -4.4494605 -4.6778135 -4.7395134 -4.5884809 -4.3120351][-4.4733396 -4.3669415 -4.3408766 -4.3673086 -4.3297944 -4.1581788 -3.9098668 -3.7286739 -3.7288718 -3.9109931 -4.19484 -4.4646783 -4.605144 -4.5584788 -4.3779812][-4.428731 -4.2954087 -4.2352843 -4.2432466 -4.2111759 -4.059545 -3.8199944 -3.6260133 -3.6114464 -3.7800186 -4.0468116 -4.3142676 -4.4827504 -4.5102234 -4.4301858][-4.4118843 -4.2841496 -4.2035875 -4.1848383 -4.1488953 -4.0355058 -3.8660746 -3.7253075 -3.7209494 -3.8544259 -4.0629697 -4.27376 -4.4029541 -4.4449515 -4.427331][-4.4575744 -4.3658943 -4.2806063 -4.2259207 -4.1637945 -4.0720572 -3.9891582 -3.9398813 -3.9709742 -4.0681748 -4.1970944 -4.3139 -4.3594346 -4.3648872 -4.3536887][-4.5770526 -4.53911 -4.4554334 -4.3538527 -4.2373576 -4.1238036 -4.0886397 -4.117538 -4.1943364 -4.2750573 -4.3398118 -4.3729019 -4.345078 -4.310246 -4.2831388][-4.6969261 -4.7004948 -4.6171479 -4.4753208 -4.3115482 -4.1680045 -4.1447763 -4.215632 -4.3189669 -4.3919334 -4.4176054 -4.4012189 -4.3450341 -4.3033543 -4.2744913][-4.7463737 -4.7769971 -4.7105422 -4.5665264 -4.3981004 -4.2590475 -4.24714 -4.3343844 -4.4335012 -4.4808393 -4.4651856 -4.4088917 -4.343123 -4.3151069 -4.2973108][-4.7340155 -4.7897878 -4.76178 -4.6538959 -4.5193939 -4.41489 -4.417902 -4.4950538 -4.5582709 -4.5627232 -4.5078797 -4.4233847 -4.35681 -4.342669 -4.3392949][-4.6721067 -4.738029 -4.7463841 -4.6912279 -4.6101332 -4.5484095 -4.5532637 -4.5937638 -4.6082044 -4.5745482 -4.5001678 -4.4103327 -4.35035 -4.343205 -4.3491826][-4.5754414 -4.6279364 -4.6512995 -4.6399164 -4.6074486 -4.5779133 -4.5732789 -4.5744853 -4.5500317 -4.4955711 -4.4219403 -4.3482347 -4.3040524 -4.2990704 -4.3060446]]...]
INFO - root - 2017-12-07 13:19:42.408410: step 19310, loss = 21.27, batch loss = 21.19 (8.7 examples/sec; 0.917 sec/batch; 79h:44m:08s remains)
INFO - root - 2017-12-07 13:19:51.766855: step 19320, loss = 21.37, batch loss = 21.29 (8.9 examples/sec; 0.900 sec/batch; 78h:16m:26s remains)
INFO - root - 2017-12-07 13:20:01.098481: step 19330, loss = 21.28, batch loss = 21.20 (8.7 examples/sec; 0.923 sec/batch; 80h:19m:06s remains)
INFO - root - 2017-12-07 13:20:10.574403: step 19340, loss = 21.79, batch loss = 21.70 (8.4 examples/sec; 0.951 sec/batch; 82h:41m:50s remains)
INFO - root - 2017-12-07 13:20:20.100418: step 19350, loss = 21.73, batch loss = 21.65 (8.7 examples/sec; 0.922 sec/batch; 80h:13m:54s remains)
INFO - root - 2017-12-07 13:20:29.368153: step 19360, loss = 21.18, batch loss = 21.10 (8.3 examples/sec; 0.969 sec/batch; 84h:19m:47s remains)
INFO - root - 2017-12-07 13:20:38.860878: step 19370, loss = 21.26, batch loss = 21.18 (8.8 examples/sec; 0.906 sec/batch; 78h:48m:06s remains)
INFO - root - 2017-12-07 13:20:48.261420: step 19380, loss = 21.62, batch loss = 21.53 (9.0 examples/sec; 0.888 sec/batch; 77h:15m:48s remains)
INFO - root - 2017-12-07 13:20:57.861632: step 19390, loss = 21.63, batch loss = 21.55 (8.2 examples/sec; 0.970 sec/batch; 84h:22m:39s remains)
INFO - root - 2017-12-07 13:21:07.106236: step 19400, loss = 21.00, batch loss = 20.91 (8.2 examples/sec; 0.975 sec/batch; 84h:50m:18s remains)
2017-12-07 13:21:08.109706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3051181 -4.3857374 -4.4658041 -4.4778285 -4.4236073 -4.34653 -4.2829933 -4.2395144 -4.229485 -4.239892 -4.2452731 -4.281527 -4.3698406 -4.4964929 -4.590219][-4.2505422 -4.3651137 -4.4721384 -4.5015125 -4.461936 -4.4031262 -4.3575006 -4.3142662 -4.2832861 -4.2757335 -4.2740288 -4.315846 -4.4079037 -4.5158324 -4.5695262][-4.1914554 -4.3449078 -4.4702716 -4.5026536 -4.4653831 -4.4177666 -4.3895183 -4.3510213 -4.309967 -4.3009734 -4.3141747 -4.3693624 -4.4525747 -4.5129128 -4.4981356][-4.1950669 -4.3618145 -4.481164 -4.4989839 -4.4487424 -4.3948212 -4.3714409 -4.3393488 -4.299819 -4.29107 -4.3175354 -4.3881521 -4.4724298 -4.4994845 -4.4353185][-4.2630372 -4.4031477 -4.4868736 -4.4739 -4.3959107 -4.3175688 -4.2869372 -4.2697568 -4.2477293 -4.2368789 -4.2632675 -4.3494859 -4.4583898 -4.4967241 -4.4351344][-4.3352866 -4.4215293 -4.4470897 -4.389668 -4.2754297 -4.1680179 -4.1250653 -4.130815 -4.1472321 -4.1500649 -4.17523 -4.270256 -4.4049325 -4.4787631 -4.4526167][-4.3555889 -4.392447 -4.3658938 -4.2689643 -4.1216168 -3.9812894 -3.9139695 -3.93299 -3.9983275 -4.0480571 -4.1017947 -4.2099247 -4.3537674 -4.4547219 -4.470119][-4.3441458 -4.3542476 -4.3040371 -4.1862564 -4.0061822 -3.8143718 -3.6959882 -3.7047102 -3.8152976 -3.9386902 -4.0572233 -4.1934528 -4.3323574 -4.4369345 -4.478734][-4.3353157 -4.35174 -4.3184466 -4.2076235 -3.9991443 -3.7461262 -3.5646935 -3.5472972 -3.6866982 -3.8825812 -4.0762177 -4.2426963 -4.3590527 -4.4355669 -4.475595][-4.370223 -4.4189367 -4.4359283 -4.3666477 -4.1604853 -3.8750205 -3.6561887 -3.6170397 -3.7598038 -3.9815466 -4.2016497 -4.3628321 -4.4385586 -4.4713621 -4.4933782][-4.4408374 -4.5161891 -4.5775943 -4.555871 -4.3852592 -4.1182251 -3.90971 -3.8695717 -3.9954276 -4.19336 -4.3801632 -4.4997888 -4.5311785 -4.5261049 -4.5240622][-4.502 -4.5825768 -4.6595235 -4.6712785 -4.5559759 -4.3512526 -4.1918159 -4.1658859 -4.2648826 -4.4111557 -4.53674 -4.6057544 -4.6074734 -4.5808764 -4.5546036][-4.5405264 -4.6171274 -4.6882348 -4.7145672 -4.6539016 -4.5288815 -4.4302626 -4.4176989 -4.48295 -4.5725031 -4.6392283 -4.6651621 -4.6490288 -4.6118879 -4.5688953][-4.5275507 -4.5919375 -4.6490378 -4.6763062 -4.6531425 -4.5948772 -4.5500112 -4.5505438 -4.5886922 -4.6331391 -4.6571546 -4.6549363 -4.6288695 -4.5876293 -4.5405178][-4.4567213 -4.5045052 -4.5487294 -4.575953 -4.5782323 -4.5652523 -4.5561738 -4.5624318 -4.5788836 -4.5920444 -4.5908651 -4.5742016 -4.5445166 -4.5061769 -4.46671]]...]
INFO - root - 2017-12-07 13:21:17.448177: step 19410, loss = 21.27, batch loss = 21.19 (9.0 examples/sec; 0.888 sec/batch; 77h:14m:51s remains)
INFO - root - 2017-12-07 13:21:26.868389: step 19420, loss = 21.80, batch loss = 21.72 (8.3 examples/sec; 0.967 sec/batch; 84h:05m:25s remains)
INFO - root - 2017-12-07 13:21:36.388697: step 19430, loss = 21.63, batch loss = 21.54 (8.2 examples/sec; 0.979 sec/batch; 85h:06m:43s remains)
INFO - root - 2017-12-07 13:21:45.730431: step 19440, loss = 21.35, batch loss = 21.27 (9.0 examples/sec; 0.886 sec/batch; 77h:04m:39s remains)
INFO - root - 2017-12-07 13:21:55.026974: step 19450, loss = 21.43, batch loss = 21.35 (9.0 examples/sec; 0.892 sec/batch; 77h:34m:03s remains)
INFO - root - 2017-12-07 13:22:04.365411: step 19460, loss = 21.49, batch loss = 21.40 (8.7 examples/sec; 0.923 sec/batch; 80h:14m:34s remains)
INFO - root - 2017-12-07 13:22:13.842472: step 19470, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.963 sec/batch; 83h:44m:37s remains)
INFO - root - 2017-12-07 13:22:23.329076: step 19480, loss = 21.42, batch loss = 21.34 (8.2 examples/sec; 0.973 sec/batch; 84h:36m:12s remains)
INFO - root - 2017-12-07 13:22:32.559625: step 19490, loss = 21.75, batch loss = 21.66 (8.9 examples/sec; 0.900 sec/batch; 78h:15m:02s remains)
INFO - root - 2017-12-07 13:22:41.837820: step 19500, loss = 21.24, batch loss = 21.16 (8.9 examples/sec; 0.901 sec/batch; 78h:22m:27s remains)
2017-12-07 13:22:42.802274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4207573 -4.4313602 -4.4340715 -4.4289184 -4.4203286 -4.4134417 -4.4104538 -4.407352 -4.3989582 -4.3865185 -4.3728209 -4.3601093 -4.35182 -4.3493223 -4.3498664][-4.4873977 -4.4969754 -4.4948516 -4.4869933 -4.4801054 -4.4779654 -4.479845 -4.4777236 -4.4646149 -4.443203 -4.41731 -4.39161 -4.3728509 -4.3632708 -4.3602157][-4.5430827 -4.5350094 -4.5138707 -4.4925947 -4.4791536 -4.4777174 -4.4898777 -4.5064154 -4.5148668 -4.5112066 -4.4917955 -4.4609671 -4.4304347 -4.406723 -4.3916264][-4.536056 -4.5058417 -4.465663 -4.429431 -4.3994894 -4.3808379 -4.3891835 -4.4256821 -4.4748259 -4.5217056 -4.5443382 -4.5353384 -4.5085721 -4.4757824 -4.4459615][-4.4478269 -4.4104633 -4.3656206 -4.321701 -4.2713575 -4.2186289 -4.2026005 -4.2440362 -4.3288646 -4.4348211 -4.5187631 -4.5541606 -4.5530367 -4.5291238 -4.4953055][-4.315093 -4.2891164 -4.2486658 -4.2011805 -4.1339669 -4.049015 -4.0045509 -4.0375566 -4.1372461 -4.2810822 -4.4117012 -4.4877892 -4.5200644 -4.5209818 -4.5008297][-4.19104 -4.178153 -4.14058 -4.0891032 -4.0106635 -3.9061303 -3.8426144 -3.8658741 -3.9695005 -4.1279025 -4.2736793 -4.3641934 -4.4166665 -4.4439759 -4.4495072][-4.109292 -4.0990877 -4.0633125 -4.0127716 -3.9293485 -3.8142805 -3.7344136 -3.7428284 -3.8455482 -4.0085115 -4.1540689 -4.2437911 -4.3034506 -4.3480616 -4.37591][-4.0657873 -4.0605 -4.0384355 -4.0088458 -3.9394674 -3.8274536 -3.7310836 -3.7101581 -3.7928016 -3.9418349 -4.0766525 -4.1643205 -4.2298737 -4.2854929 -4.3255911][-4.0670767 -4.0638165 -4.0644426 -4.07394 -4.0466208 -3.9697094 -3.8832998 -3.8428974 -3.8899279 -3.9970984 -4.098804 -4.1714172 -4.2316961 -4.2846136 -4.3230324][-4.1461115 -4.1365943 -4.1470842 -4.1878567 -4.2085686 -4.1854548 -4.1396608 -4.1115856 -4.1355429 -4.1944418 -4.2495642 -4.2896285 -4.32375 -4.3521414 -4.368937][-4.2872782 -4.2714677 -4.2812 -4.3346996 -4.3894715 -4.4151483 -4.4155588 -4.4114542 -4.4211655 -4.4374919 -4.447216 -4.4504085 -4.4508104 -4.44586 -4.4334531][-4.4368148 -4.41825 -4.4234748 -4.4704194 -4.5311279 -4.5769272 -4.601366 -4.6103106 -4.6095567 -4.5984364 -4.5790982 -4.5575013 -4.5340414 -4.505549 -4.47362][-4.5272446 -4.5173979 -4.5204377 -4.5481191 -4.5867186 -4.6183624 -4.6360526 -4.6392446 -4.6292529 -4.60891 -4.5836892 -4.5586734 -4.5320787 -4.5006237 -4.4665689][-4.519444 -4.5181766 -4.5249057 -4.5396023 -4.5548377 -4.563796 -4.5643544 -4.5560608 -4.5399184 -4.5196733 -4.4999733 -4.48312 -4.4659266 -4.4444027 -4.4199443]]...]
INFO - root - 2017-12-07 13:22:52.139552: step 19510, loss = 21.45, batch loss = 21.37 (8.3 examples/sec; 0.962 sec/batch; 83h:38m:16s remains)
INFO - root - 2017-12-07 13:23:01.557992: step 19520, loss = 21.12, batch loss = 21.04 (8.3 examples/sec; 0.963 sec/batch; 83h:45m:50s remains)
INFO - root - 2017-12-07 13:23:10.938358: step 19530, loss = 21.29, batch loss = 21.21 (8.7 examples/sec; 0.915 sec/batch; 79h:30m:12s remains)
INFO - root - 2017-12-07 13:23:20.277252: step 19540, loss = 21.58, batch loss = 21.50 (8.2 examples/sec; 0.981 sec/batch; 85h:17m:02s remains)
INFO - root - 2017-12-07 13:23:29.742223: step 19550, loss = 21.60, batch loss = 21.52 (8.6 examples/sec; 0.932 sec/batch; 80h:59m:01s remains)
INFO - root - 2017-12-07 13:23:39.173312: step 19560, loss = 21.35, batch loss = 21.26 (8.4 examples/sec; 0.949 sec/batch; 82h:31m:51s remains)
INFO - root - 2017-12-07 13:23:48.643618: step 19570, loss = 21.34, batch loss = 21.26 (8.1 examples/sec; 0.986 sec/batch; 85h:41m:51s remains)
INFO - root - 2017-12-07 13:23:58.027606: step 19580, loss = 21.83, batch loss = 21.74 (8.6 examples/sec; 0.935 sec/batch; 81h:18m:32s remains)
INFO - root - 2017-12-07 13:24:07.398237: step 19590, loss = 21.26, batch loss = 21.17 (8.8 examples/sec; 0.907 sec/batch; 78h:49m:45s remains)
INFO - root - 2017-12-07 13:24:16.870609: step 19600, loss = 21.61, batch loss = 21.53 (8.4 examples/sec; 0.954 sec/batch; 82h:55m:20s remains)
2017-12-07 13:24:17.750684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3138275 -4.3345485 -4.3647203 -4.4016113 -4.4412451 -4.4650841 -4.4690046 -4.4649558 -4.4704723 -4.49055 -4.5089879 -4.5148578 -4.4864159 -4.4247255 -4.3536811][-4.3722324 -4.4105921 -4.4511585 -4.4923353 -4.5240674 -4.5243454 -4.4967594 -4.4704571 -4.4818549 -4.5314741 -4.5765777 -4.5843229 -4.5346756 -4.4454193 -4.3509741][-4.4042978 -4.451479 -4.4959726 -4.5353165 -4.5491581 -4.5150537 -4.447299 -4.3902659 -4.4031305 -4.4829569 -4.5576019 -4.5742884 -4.5269632 -4.4405389 -4.3385243][-4.4052224 -4.4409933 -4.4781613 -4.5127983 -4.5088029 -4.441421 -4.3412557 -4.2548175 -4.2557459 -4.3483181 -4.4398093 -4.4694557 -4.4503379 -4.4019237 -4.3191214][-4.3845806 -4.3970432 -4.416297 -4.441473 -4.42453 -4.3328528 -4.2098341 -4.0942717 -4.0726051 -4.1710124 -4.2803235 -4.3304462 -4.3506608 -4.3517933 -4.3038769][-4.3752007 -4.3634229 -4.3593192 -4.369276 -4.3413949 -4.2226729 -4.0687981 -3.9190722 -3.8782463 -4.0014296 -4.146801 -4.2320242 -4.2925625 -4.332799 -4.3134909][-4.4021578 -4.3722653 -4.3388963 -4.3216181 -4.2755303 -4.1283841 -3.9416881 -3.7660127 -3.72792 -3.8929672 -4.085629 -4.2081928 -4.2968035 -4.3541408 -4.3469181][-4.4412842 -4.4078684 -4.3552971 -4.3134341 -4.2452559 -4.0713096 -3.8523107 -3.662396 -3.6431544 -3.8417645 -4.0723529 -4.2293906 -4.3350244 -4.3929863 -4.3858733][-4.4773755 -4.4543862 -4.4046788 -4.3550673 -4.2738705 -4.0957432 -3.8723173 -3.6897793 -3.6878641 -3.8801327 -4.1028743 -4.2630215 -4.3651361 -4.409965 -4.4035988][-4.5021057 -4.5004363 -4.4685144 -4.42799 -4.3504963 -4.1967988 -4.0079627 -3.8602984 -3.8692379 -4.0232806 -4.19479 -4.3191223 -4.3881769 -4.4083891 -4.405858][-4.5018778 -4.5216417 -4.5174146 -4.5010409 -4.446682 -4.3364787 -4.2024412 -4.0962181 -4.1026974 -4.2008762 -4.3059344 -4.3802209 -4.4121103 -4.4136963 -4.4142637][-4.4675226 -4.497333 -4.5182772 -4.5332189 -4.5157075 -4.4599576 -4.38637 -4.3233 -4.3262839 -4.3739862 -4.42101 -4.4525919 -4.4555912 -4.4397068 -4.4264617][-4.4155197 -4.441195 -4.4728293 -4.5082121 -4.524394 -4.5183973 -4.4986339 -4.4778676 -4.4859791 -4.5064125 -4.5222082 -4.5265226 -4.5056672 -4.4670682 -4.4274616][-4.3618412 -4.3771663 -4.404808 -4.4435997 -4.47724 -4.5007958 -4.5151262 -4.5247393 -4.5426478 -4.5559988 -4.5607243 -4.5521383 -4.5178556 -4.4658 -4.4116292][-4.3263645 -4.3318582 -4.3484988 -4.3768411 -4.40896 -4.4389567 -4.4644251 -4.4872737 -4.5100012 -4.5228095 -4.5244541 -4.5110426 -4.4759383 -4.4290056 -4.3824568]]...]
INFO - root - 2017-12-07 13:24:27.003930: step 19610, loss = 21.65, batch loss = 21.57 (8.6 examples/sec; 0.927 sec/batch; 80h:32m:54s remains)
INFO - root - 2017-12-07 13:24:36.491932: step 19620, loss = 21.22, batch loss = 21.14 (8.1 examples/sec; 0.991 sec/batch; 86h:09m:02s remains)
INFO - root - 2017-12-07 13:24:45.931631: step 19630, loss = 21.29, batch loss = 21.21 (8.2 examples/sec; 0.970 sec/batch; 84h:16m:45s remains)
INFO - root - 2017-12-07 13:24:55.334519: step 19640, loss = 21.28, batch loss = 21.20 (8.1 examples/sec; 0.984 sec/batch; 85h:29m:29s remains)
INFO - root - 2017-12-07 13:25:04.718625: step 19650, loss = 21.39, batch loss = 21.30 (8.3 examples/sec; 0.964 sec/batch; 83h:48m:56s remains)
INFO - root - 2017-12-07 13:25:14.058454: step 19660, loss = 21.14, batch loss = 21.06 (8.5 examples/sec; 0.945 sec/batch; 82h:08m:23s remains)
INFO - root - 2017-12-07 13:25:23.373122: step 19670, loss = 21.74, batch loss = 21.65 (8.6 examples/sec; 0.929 sec/batch; 80h:44m:44s remains)
INFO - root - 2017-12-07 13:25:32.750706: step 19680, loss = 21.03, batch loss = 20.94 (8.7 examples/sec; 0.923 sec/batch; 80h:10m:00s remains)
INFO - root - 2017-12-07 13:25:42.164660: step 19690, loss = 21.17, batch loss = 21.08 (8.5 examples/sec; 0.940 sec/batch; 81h:39m:45s remains)
INFO - root - 2017-12-07 13:25:51.614539: step 19700, loss = 21.49, batch loss = 21.40 (9.0 examples/sec; 0.889 sec/batch; 77h:17m:08s remains)
2017-12-07 13:25:52.595115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4260874 -4.436677 -4.4344139 -4.4278088 -4.4428024 -4.4702826 -4.4940605 -4.5017209 -4.4833584 -4.4392772 -4.4139161 -4.4458103 -4.4708276 -4.4715686 -4.4879036][-4.4022765 -4.4230981 -4.431149 -4.424408 -4.4278679 -4.440115 -4.4408555 -4.433073 -4.4163365 -4.3760662 -4.3501554 -4.3833866 -4.4215856 -4.4452496 -4.4749675][-4.3960538 -4.4188156 -4.422967 -4.4078665 -4.3973126 -4.3897567 -4.3661604 -4.3474569 -4.3381386 -4.3100314 -4.2879391 -4.3127012 -4.3529058 -4.396358 -4.4407616][-4.4092593 -4.4300361 -4.4150214 -4.3793187 -4.3451138 -4.3117704 -4.2706404 -4.2514329 -4.2578006 -4.2492795 -4.2419362 -4.2625322 -4.3004289 -4.354104 -4.4038954][-4.4350986 -4.4456387 -4.4045291 -4.3418946 -4.2794981 -4.2209663 -4.1705337 -4.1600094 -4.1846337 -4.2028742 -4.2181683 -4.2445965 -4.28469 -4.3435884 -4.3888369][-4.4583559 -4.45279 -4.3889108 -4.3000293 -4.2096071 -4.1284075 -4.0744448 -4.0727973 -4.1092038 -4.1542311 -4.1979527 -4.2426534 -4.296855 -4.3611784 -4.3987064][-4.4647307 -4.4434857 -4.3671203 -4.260901 -4.14571 -4.0422688 -3.988236 -3.9965291 -4.0449553 -4.1132374 -4.18722 -4.258265 -4.33099 -4.3983512 -4.4272466][-4.4568019 -4.42353 -4.346601 -4.2372756 -4.1037669 -3.9794562 -3.9243908 -3.947418 -4.0120821 -4.0965214 -4.1931405 -4.2889643 -4.3753829 -4.4399943 -4.4586873][-4.4498754 -4.4111314 -4.3414974 -4.2400432 -4.1016803 -3.9708242 -3.9198389 -3.9562619 -4.0344753 -4.1242537 -4.2290606 -4.338398 -4.426393 -4.4773645 -4.4809942][-4.4612446 -4.4248357 -4.3656487 -4.2762327 -4.1425619 -4.0188718 -3.9760506 -4.0167174 -4.101759 -4.188931 -4.2825174 -4.3867264 -4.4623909 -4.4925694 -4.4794393][-4.4822021 -4.459116 -4.4110937 -4.3334212 -4.2126107 -4.1087556 -4.0765634 -4.1102748 -4.1854787 -4.2562089 -4.3210115 -4.4022245 -4.4583554 -4.4696236 -4.4488082][-4.48827 -4.4866533 -4.456233 -4.393692 -4.2938313 -4.2125363 -4.1893916 -4.2150388 -4.2714939 -4.3148046 -4.343595 -4.3903189 -4.4204526 -4.4184461 -4.4031467][-4.4669085 -4.4842491 -4.4760695 -4.434402 -4.3604488 -4.299819 -4.2792754 -4.2945023 -4.3303237 -4.3499374 -4.3516088 -4.3640237 -4.3676534 -4.3594136 -4.357326][-4.4243526 -4.4484324 -4.4595718 -4.4443531 -4.4025321 -4.3618917 -4.3388195 -4.337666 -4.3510103 -4.3531752 -4.3413296 -4.3311734 -4.3163118 -4.305943 -4.3147407][-4.376976 -4.3919826 -4.4105482 -4.4177332 -4.40636 -4.3863058 -4.3640885 -4.3481607 -4.3418841 -4.3334508 -4.3171282 -4.2984858 -4.2794938 -4.270916 -4.2819457]]...]
INFO - root - 2017-12-07 13:26:01.999971: step 19710, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.929 sec/batch; 80h:42m:48s remains)
INFO - root - 2017-12-07 13:26:11.417598: step 19720, loss = 21.47, batch loss = 21.39 (8.5 examples/sec; 0.942 sec/batch; 81h:52m:08s remains)
INFO - root - 2017-12-07 13:26:20.801381: step 19730, loss = 21.14, batch loss = 21.06 (8.7 examples/sec; 0.919 sec/batch; 79h:51m:55s remains)
INFO - root - 2017-12-07 13:26:30.132604: step 19740, loss = 21.78, batch loss = 21.70 (9.0 examples/sec; 0.892 sec/batch; 77h:32m:12s remains)
INFO - root - 2017-12-07 13:26:39.417274: step 19750, loss = 21.10, batch loss = 21.02 (9.2 examples/sec; 0.871 sec/batch; 75h:41m:17s remains)
INFO - root - 2017-12-07 13:26:48.833075: step 19760, loss = 21.34, batch loss = 21.26 (8.6 examples/sec; 0.930 sec/batch; 80h:47m:08s remains)
INFO - root - 2017-12-07 13:26:58.224887: step 19770, loss = 21.56, batch loss = 21.48 (8.6 examples/sec; 0.932 sec/batch; 80h:59m:26s remains)
INFO - root - 2017-12-07 13:27:07.672749: step 19780, loss = 21.80, batch loss = 21.72 (8.7 examples/sec; 0.923 sec/batch; 80h:12m:04s remains)
INFO - root - 2017-12-07 13:27:16.955347: step 19790, loss = 21.30, batch loss = 21.22 (8.7 examples/sec; 0.924 sec/batch; 80h:15m:51s remains)
INFO - root - 2017-12-07 13:27:26.429277: step 19800, loss = 22.04, batch loss = 21.95 (8.2 examples/sec; 0.976 sec/batch; 84h:44m:58s remains)
2017-12-07 13:27:27.424200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3295665 -4.3824205 -4.5168304 -4.668355 -4.756146 -4.7788048 -4.7836289 -4.8071613 -4.8121181 -4.7580128 -4.6707826 -4.5971484 -4.5267396 -4.4324455 -4.3478847][-4.262639 -4.30077 -4.4246154 -4.5757976 -4.6668553 -4.6921425 -4.6956892 -4.7211652 -4.7345004 -4.6978192 -4.6273174 -4.5476737 -4.4380145 -4.2957993 -4.19046][-4.3023777 -4.3242164 -4.4106369 -4.5232143 -4.592227 -4.6205478 -4.638731 -4.6768618 -4.7031169 -4.6774349 -4.6075816 -4.4996061 -4.3343949 -4.1416292 -4.0249758][-4.406836 -4.4222608 -4.4660878 -4.523396 -4.55373 -4.5684166 -4.5801077 -4.6119695 -4.6485715 -4.6505246 -4.6000023 -4.4788184 -4.2780404 -4.0547266 -3.9270015][-4.4892478 -4.5008354 -4.501256 -4.5021954 -4.48376 -4.4544377 -4.420383 -4.4154878 -4.4565954 -4.5111456 -4.5224357 -4.4387622 -4.252409 -4.0320344 -3.8964677][-4.5134015 -4.507133 -4.4598017 -4.4092355 -4.34226 -4.253819 -4.1586466 -4.109396 -4.1523676 -4.2674251 -4.365952 -4.36033 -4.2318273 -4.0475645 -3.9203691][-4.5108566 -4.474225 -4.3826523 -4.292088 -4.1865768 -4.0431738 -3.898747 -3.8245673 -3.8846128 -4.061759 -4.2375693 -4.2986326 -4.2171741 -4.0667996 -3.9622722][-4.5071716 -4.4491444 -4.3401957 -4.23657 -4.1133785 -3.9299955 -3.7439647 -3.6557958 -3.7418654 -3.9767413 -4.2044754 -4.2955885 -4.2283998 -4.0894485 -4.005784][-4.5293236 -4.4721513 -4.3721948 -4.2742791 -4.1470432 -3.9402509 -3.7167861 -3.612572 -3.7161112 -3.9836397 -4.2332678 -4.3321791 -4.2680054 -4.1346874 -4.0668659][-4.568675 -4.5368948 -4.4682827 -4.3875146 -4.2695036 -4.0725403 -3.8517613 -3.7518032 -3.8564796 -4.103858 -4.3190637 -4.3929825 -4.330101 -4.2123342 -4.1526804][-4.5763054 -4.5913706 -4.5723095 -4.5234289 -4.4311838 -4.271122 -4.0847745 -3.9996629 -4.0792551 -4.2623544 -4.4089751 -4.4467115 -4.390964 -4.2959661 -4.2344222][-4.543025 -4.5932636 -4.6149411 -4.5995107 -4.539031 -4.41955 -4.2681403 -4.1854372 -4.225718 -4.3530607 -4.4635911 -4.496696 -4.4575639 -4.3809276 -4.3133297][-4.500576 -4.5548377 -4.5953507 -4.6062727 -4.5770626 -4.4999862 -4.39485 -4.328084 -4.3423133 -4.4270744 -4.5141268 -4.5424371 -4.5002828 -4.4241033 -4.3608937][-4.467504 -4.504467 -4.5498419 -4.5833111 -4.581161 -4.53769 -4.4725618 -4.4271626 -4.4261231 -4.4680724 -4.5177574 -4.5234418 -4.4653411 -4.3860407 -4.3379307][-4.4362121 -4.4535875 -4.4905715 -4.5290232 -4.5399394 -4.5215487 -4.493506 -4.480525 -4.4837437 -4.4937634 -4.4983134 -4.4721107 -4.4019895 -4.327033 -4.2899532]]...]
INFO - root - 2017-12-07 13:27:36.564409: step 19810, loss = 21.64, batch loss = 21.56 (9.0 examples/sec; 0.892 sec/batch; 77h:27m:11s remains)
INFO - root - 2017-12-07 13:27:45.931532: step 19820, loss = 21.71, batch loss = 21.62 (8.3 examples/sec; 0.968 sec/batch; 84h:02m:44s remains)
INFO - root - 2017-12-07 13:27:55.420260: step 19830, loss = 21.46, batch loss = 21.37 (8.0 examples/sec; 0.999 sec/batch; 86h:45m:40s remains)
INFO - root - 2017-12-07 13:28:04.620312: step 19840, loss = 21.11, batch loss = 21.03 (8.0 examples/sec; 1.005 sec/batch; 87h:18m:54s remains)
INFO - root - 2017-12-07 13:28:14.031478: step 19850, loss = 21.65, batch loss = 21.57 (8.7 examples/sec; 0.919 sec/batch; 79h:47m:32s remains)
INFO - root - 2017-12-07 13:28:23.330015: step 19860, loss = 21.23, batch loss = 21.14 (9.2 examples/sec; 0.871 sec/batch; 75h:39m:47s remains)
INFO - root - 2017-12-07 13:28:32.733932: step 19870, loss = 21.56, batch loss = 21.48 (8.7 examples/sec; 0.918 sec/batch; 79h:42m:06s remains)
INFO - root - 2017-12-07 13:28:42.240953: step 19880, loss = 21.35, batch loss = 21.27 (8.2 examples/sec; 0.978 sec/batch; 84h:57m:48s remains)
INFO - root - 2017-12-07 13:28:51.653412: step 19890, loss = 21.74, batch loss = 21.65 (8.2 examples/sec; 0.976 sec/batch; 84h:45m:01s remains)
INFO - root - 2017-12-07 13:29:01.122137: step 19900, loss = 21.82, batch loss = 21.74 (9.0 examples/sec; 0.891 sec/batch; 77h:19m:37s remains)
2017-12-07 13:29:02.033551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3943815 -4.4313607 -4.4405608 -4.410161 -4.3604136 -4.33249 -4.3560295 -4.4027462 -4.4483781 -4.505167 -4.5505362 -4.5132208 -4.3857112 -4.2576256 -4.1899414][-4.4331174 -4.4634519 -4.447638 -4.3852215 -4.3157516 -4.2863078 -4.306756 -4.342473 -4.3819737 -4.4467568 -4.5138941 -4.5045309 -4.3964138 -4.2789011 -4.2220459][-4.4487581 -4.4634013 -4.4185224 -4.3274488 -4.2420006 -4.2104139 -4.2245679 -4.2495584 -4.2900591 -4.3765645 -4.4740381 -4.4914141 -4.3951197 -4.2756519 -4.2153711][-4.4448433 -4.439651 -4.360837 -4.2355256 -4.1283522 -4.0893397 -4.1038537 -4.130455 -4.1808109 -4.2926378 -4.4177141 -4.4556222 -4.3597946 -4.2245517 -4.1471024][-4.4263411 -4.4041371 -4.3004351 -4.1473413 -4.0162559 -3.9631147 -3.9834242 -4.0258946 -4.0934358 -4.2221732 -4.3625116 -4.4159656 -4.3289762 -4.194437 -4.1141319][-4.4121733 -4.384306 -4.2736187 -4.1117535 -3.9659474 -3.8956575 -3.9140081 -3.9658461 -4.0423603 -4.1741552 -4.3222046 -4.3922791 -4.3245254 -4.2046838 -4.1306376][-4.4264436 -4.4011683 -4.2990093 -4.15049 -4.0093122 -3.9306312 -3.9390161 -3.9862754 -4.0582275 -4.1797905 -4.3238392 -4.4007707 -4.3366942 -4.205338 -4.1083355][-4.4448767 -4.4275351 -4.3451552 -4.2251444 -4.1015511 -4.019733 -4.0132217 -4.0482869 -4.1143613 -4.2330904 -4.3783088 -4.4605365 -4.3921108 -4.2345181 -4.0938945][-4.4589682 -4.4495697 -4.3884583 -4.2951341 -4.1837258 -4.0911403 -4.0627718 -4.082283 -4.1417146 -4.2635541 -4.4175062 -4.5156884 -4.4601088 -4.2955503 -4.1290169][-4.4881043 -4.4857235 -4.4418225 -4.3683019 -4.2656116 -4.1642141 -4.1187925 -4.1242962 -4.1660552 -4.268867 -4.4128814 -4.5185838 -4.4818635 -4.3263865 -4.1508145][-4.5087719 -4.514523 -4.4901795 -4.4431477 -4.3640261 -4.2743392 -4.2293329 -4.2281175 -4.2408915 -4.2955856 -4.4016643 -4.497499 -4.4731665 -4.3250556 -4.141665][-4.4979343 -4.51038 -4.5085444 -4.4945931 -4.4499378 -4.3834128 -4.3461752 -4.3405972 -4.3286433 -4.3368239 -4.4019151 -4.4824634 -4.4669471 -4.3251748 -4.1383619][-4.4494085 -4.4644284 -4.4807549 -4.4987044 -4.4957223 -4.4662023 -4.4475508 -4.4448047 -4.4230332 -4.405118 -4.4408712 -4.5046959 -4.4884734 -4.3477097 -4.1591473][-4.3783536 -4.3911514 -4.4179358 -4.4597735 -4.4959364 -4.5090127 -4.5151095 -4.5209155 -4.5046411 -4.4849405 -4.5069 -4.5558548 -4.5390105 -4.4088216 -4.2330141][-4.3236127 -4.3338833 -4.3612037 -4.4081926 -4.460526 -4.4981365 -4.5198693 -4.5313473 -4.5256658 -4.5189266 -4.5431938 -4.5884175 -4.5895538 -4.5010653 -4.36724]]...]
INFO - root - 2017-12-07 13:29:11.386251: step 19910, loss = 21.52, batch loss = 21.44 (8.7 examples/sec; 0.917 sec/batch; 79h:35m:04s remains)
INFO - root - 2017-12-07 13:29:20.780057: step 19920, loss = 21.50, batch loss = 21.41 (8.5 examples/sec; 0.937 sec/batch; 81h:23m:20s remains)
INFO - root - 2017-12-07 13:29:30.090269: step 19930, loss = 21.04, batch loss = 20.96 (9.1 examples/sec; 0.881 sec/batch; 76h:30m:26s remains)
INFO - root - 2017-12-07 13:29:39.417545: step 19940, loss = 21.69, batch loss = 21.61 (8.8 examples/sec; 0.911 sec/batch; 79h:06m:07s remains)
INFO - root - 2017-12-07 13:29:48.724768: step 19950, loss = 21.39, batch loss = 21.31 (8.7 examples/sec; 0.919 sec/batch; 79h:46m:28s remains)
INFO - root - 2017-12-07 13:29:58.108047: step 19960, loss = 21.35, batch loss = 21.27 (8.4 examples/sec; 0.957 sec/batch; 83h:06m:58s remains)
INFO - root - 2017-12-07 13:30:07.523042: step 19970, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.979 sec/batch; 85h:00m:09s remains)
INFO - root - 2017-12-07 13:30:17.006215: step 19980, loss = 21.43, batch loss = 21.35 (8.4 examples/sec; 0.955 sec/batch; 82h:56m:09s remains)
INFO - root - 2017-12-07 13:30:26.364100: step 19990, loss = 21.24, batch loss = 21.15 (8.2 examples/sec; 0.980 sec/batch; 85h:04m:14s remains)
INFO - root - 2017-12-07 13:30:35.769375: step 20000, loss = 20.98, batch loss = 20.89 (8.2 examples/sec; 0.973 sec/batch; 84h:28m:17s remains)
2017-12-07 13:30:36.644729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4239788 -4.4303236 -4.4333887 -4.4295692 -4.4242749 -4.4202867 -4.4135723 -4.4050918 -4.4010773 -4.3997011 -4.4011354 -4.4126043 -4.4374275 -4.4631667 -4.4696321][-4.4999385 -4.5003543 -4.4937325 -4.4911184 -4.4966598 -4.5054359 -4.4982729 -4.4773459 -4.4695654 -4.4745622 -4.4885426 -4.5128078 -4.5488577 -4.58 -4.5787625][-4.5754771 -4.5661254 -4.5491762 -4.5514011 -4.5698357 -4.5867505 -4.5675378 -4.5239372 -4.5162873 -4.5496645 -4.5999665 -4.6499534 -4.6977296 -4.7274046 -4.7079558][-4.6256948 -4.6133995 -4.5929542 -4.5979075 -4.6139631 -4.6168861 -4.5675755 -4.4962597 -4.5022068 -4.5947514 -4.7157993 -4.8102674 -4.8706937 -4.8911343 -4.8447595][-4.6131554 -4.6139593 -4.6089616 -4.619307 -4.6132078 -4.5746222 -4.4723916 -4.3584709 -4.3785429 -4.5419211 -4.7455368 -4.8961267 -4.9772496 -4.995985 -4.9348][-4.5426488 -4.5590234 -4.5776443 -4.5878859 -4.5379429 -4.4315486 -4.245863 -4.0616703 -4.080483 -4.3039532 -4.5770755 -4.7870545 -4.9110751 -4.9582305 -4.9206758][-4.4563165 -4.4776659 -4.5076165 -4.5039425 -4.4047184 -4.2303171 -3.9511418 -3.6808538 -3.6881247 -3.9567213 -4.2752023 -4.5320296 -4.7075357 -4.8038406 -4.8216743][-4.390511 -4.4052978 -4.4284706 -4.4041157 -4.2747521 -4.0618091 -3.717701 -3.3763511 -3.3742375 -3.6605763 -3.9833329 -4.2571306 -4.4739394 -4.6252985 -4.7111282][-4.39684 -4.391057 -4.3848004 -4.3360047 -4.2044144 -4.0007739 -3.6590986 -3.307682 -3.3165491 -3.5922451 -3.8804247 -4.1365852 -4.36229 -4.5426965 -4.6711941][-4.49565 -4.4638147 -4.4152455 -4.342175 -4.2267365 -4.0697646 -3.8012154 -3.5205698 -3.5543113 -3.795979 -4.0350852 -4.2573409 -4.4576564 -4.6217995 -4.7406573][-4.6301112 -4.5832481 -4.4956231 -4.3985567 -4.2984133 -4.1964493 -4.0238571 -3.8334315 -3.8715339 -4.0555253 -4.2506943 -4.4545255 -4.6360307 -4.776053 -4.8576221][-4.72708 -4.6819139 -4.5769324 -4.4708829 -4.3912592 -4.3441615 -4.2654905 -4.154952 -4.1697893 -4.2787576 -4.4241014 -4.6012917 -4.7606955 -4.8724203 -4.9142923][-4.7687254 -4.7450662 -4.6555138 -4.5656509 -4.5170722 -4.5164824 -4.508667 -4.4578414 -4.4443913 -4.4870071 -4.5802822 -4.71696 -4.8362913 -4.9034824 -4.9003139][-4.7208037 -4.730031 -4.6848083 -4.6353416 -4.6160254 -4.6352086 -4.6556096 -4.6336951 -4.6033492 -4.6051168 -4.6592689 -4.7566552 -4.8375854 -4.8672309 -4.8354058][-4.6035862 -4.6326022 -4.6350679 -4.6307292 -4.6355085 -4.6588674 -4.6809621 -4.6706853 -4.6404176 -4.6270366 -4.6532354 -4.710197 -4.7553058 -4.763978 -4.7293119]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 13:30:46.585188: step 20010, loss = 21.37, batch loss = 21.29 (8.5 examples/sec; 0.943 sec/batch; 81h:52m:20s remains)
INFO - root - 2017-12-07 13:30:55.807604: step 20020, loss = 21.16, batch loss = 21.08 (8.6 examples/sec; 0.928 sec/batch; 80h:32m:31s remains)
INFO - root - 2017-12-07 13:31:05.287091: step 20030, loss = 21.55, batch loss = 21.47 (8.9 examples/sec; 0.897 sec/batch; 77h:49m:12s remains)
INFO - root - 2017-12-07 13:31:14.786456: step 20040, loss = 21.08, batch loss = 21.00 (8.5 examples/sec; 0.940 sec/batch; 81h:37m:40s remains)
INFO - root - 2017-12-07 13:31:24.391937: step 20050, loss = 21.58, batch loss = 21.50 (8.2 examples/sec; 0.970 sec/batch; 84h:10m:42s remains)
INFO - root - 2017-12-07 13:31:33.922451: step 20060, loss = 21.37, batch loss = 21.29 (8.1 examples/sec; 0.984 sec/batch; 85h:23m:27s remains)
INFO - root - 2017-12-07 13:31:43.323083: step 20070, loss = 21.64, batch loss = 21.55 (8.0 examples/sec; 1.002 sec/batch; 86h:57m:43s remains)
INFO - root - 2017-12-07 13:31:52.770604: step 20080, loss = 21.66, batch loss = 21.58 (8.5 examples/sec; 0.941 sec/batch; 81h:39m:25s remains)
INFO - root - 2017-12-07 13:32:02.135539: step 20090, loss = 21.31, batch loss = 21.23 (8.3 examples/sec; 0.968 sec/batch; 84h:02m:23s remains)
INFO - root - 2017-12-07 13:32:11.435835: step 20100, loss = 21.58, batch loss = 21.50 (8.7 examples/sec; 0.920 sec/batch; 79h:52m:24s remains)
2017-12-07 13:32:12.365593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4093919 -4.5388069 -4.6029196 -4.5578127 -4.4396381 -4.32268 -4.2591057 -4.2365603 -4.2384348 -4.269022 -4.3123264 -4.3531284 -4.3665648 -4.3548694 -4.335958][-4.4349566 -4.5504527 -4.5868874 -4.5158176 -4.3756251 -4.2325597 -4.1364975 -4.0850549 -4.0733719 -4.1136746 -4.1821027 -4.2518106 -4.3017449 -4.3285313 -4.3361878][-4.4501734 -4.539948 -4.5479589 -4.4564714 -4.305727 -4.1426511 -4.0082178 -3.9230046 -3.9070134 -3.973309 -4.0835409 -4.1866374 -4.2662935 -4.3282762 -4.3678112][-4.4757743 -4.5384812 -4.5072331 -4.3893003 -4.2309937 -4.0532823 -3.8799729 -3.7630939 -3.7553649 -3.8687034 -4.0392547 -4.1724772 -4.2465792 -4.3021312 -4.3505006][-4.5668578 -4.5933418 -4.5106397 -4.3532457 -4.1806955 -3.9940498 -3.8000379 -3.6673408 -3.677165 -3.8360171 -4.0546594 -4.1988168 -4.2365084 -4.2494769 -4.2815909][-4.648035 -4.63666 -4.5110569 -4.3243456 -4.1426392 -3.9607077 -3.771455 -3.6475329 -3.6758158 -3.8586316 -4.0945835 -4.2330413 -4.2362895 -4.2089357 -4.2174277][-4.6518216 -4.6189952 -4.4820552 -4.2986608 -4.12702 -3.9583993 -3.787282 -3.6847987 -3.72866 -3.9080155 -4.1234431 -4.2368608 -4.2183895 -4.170393 -4.1606584][-4.6111517 -4.5809073 -4.4633293 -4.3125057 -4.1725965 -4.0312505 -3.8939934 -3.8276813 -3.8897841 -4.0435877 -4.2003751 -4.2679725 -4.2365785 -4.1889386 -4.1711922][-4.5576291 -4.5474086 -4.4611673 -4.3492322 -4.2491112 -4.1530313 -4.0734663 -4.0640097 -4.1515255 -4.2722845 -4.3562317 -4.3703904 -4.3291478 -4.2877307 -4.2638631][-4.5397973 -4.5518284 -4.4895525 -4.3984275 -4.3269529 -4.2766719 -4.255569 -4.2953548 -4.3947191 -4.47951 -4.5056634 -4.4821825 -4.4340768 -4.3886776 -4.345171][-4.5588818 -4.5850677 -4.5420527 -4.465095 -4.4109383 -4.3900433 -4.4011574 -4.4577036 -4.5413089 -4.5873303 -4.5771933 -4.5398264 -4.4936261 -4.4417496 -4.3727632][-4.5514359 -4.5782223 -4.5546608 -4.5025663 -4.470407 -4.4696918 -4.4912758 -4.5396137 -4.5981121 -4.6179037 -4.5941329 -4.5590181 -4.5196023 -4.4619474 -4.3743739][-4.5071597 -4.5302858 -4.5294342 -4.5111227 -4.5034938 -4.5128736 -4.5284405 -4.5559006 -4.5903721 -4.5991449 -4.5816512 -4.5624871 -4.5337977 -4.4741554 -4.3745217][-4.4587946 -4.4721417 -4.4878316 -4.5008106 -4.5168648 -4.5361071 -4.5493107 -4.5598845 -4.56974 -4.5641484 -4.5512819 -4.551178 -4.5423579 -4.4930992 -4.3894291][-4.4328179 -4.4305792 -4.4478259 -4.477169 -4.5086465 -4.5402975 -4.5636387 -4.5679951 -4.5504389 -4.5133152 -4.4830623 -4.4854693 -4.4964032 -4.472682 -4.3835559]]...]
INFO - root - 2017-12-07 13:32:21.766986: step 20110, loss = 21.29, batch loss = 21.20 (9.2 examples/sec; 0.865 sec/batch; 75h:04m:23s remains)
INFO - root - 2017-12-07 13:32:31.051543: step 20120, loss = 21.42, batch loss = 21.34 (8.9 examples/sec; 0.901 sec/batch; 78h:08m:31s remains)
INFO - root - 2017-12-07 13:32:40.555398: step 20130, loss = 21.37, batch loss = 21.29 (8.2 examples/sec; 0.981 sec/batch; 85h:06m:29s remains)
INFO - root - 2017-12-07 13:32:50.067193: step 20140, loss = 21.12, batch loss = 21.04 (8.2 examples/sec; 0.971 sec/batch; 84h:16m:36s remains)
INFO - root - 2017-12-07 13:32:59.572420: step 20150, loss = 21.46, batch loss = 21.38 (8.5 examples/sec; 0.943 sec/batch; 81h:50m:21s remains)
INFO - root - 2017-12-07 13:33:09.060759: step 20160, loss = 21.26, batch loss = 21.18 (8.9 examples/sec; 0.895 sec/batch; 77h:37m:48s remains)
INFO - root - 2017-12-07 13:33:18.314671: step 20170, loss = 21.76, batch loss = 21.68 (8.6 examples/sec; 0.930 sec/batch; 80h:41m:04s remains)
INFO - root - 2017-12-07 13:33:27.623430: step 20180, loss = 21.22, batch loss = 21.14 (8.8 examples/sec; 0.905 sec/batch; 78h:30m:01s remains)
INFO - root - 2017-12-07 13:33:37.075521: step 20190, loss = 21.27, batch loss = 21.19 (8.1 examples/sec; 0.988 sec/batch; 85h:41m:28s remains)
INFO - root - 2017-12-07 13:33:46.628938: step 20200, loss = 21.22, batch loss = 21.14 (7.8 examples/sec; 1.024 sec/batch; 88h:49m:36s remains)
2017-12-07 13:33:47.597712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3749809 -4.4221106 -4.465745 -4.4863768 -4.4653354 -4.4117746 -4.3422132 -4.2903519 -4.2854257 -4.3232994 -4.364563 -4.3822808 -4.3920722 -4.4070306 -4.424583][-4.4511933 -4.4728417 -4.4898577 -4.4922376 -4.4614758 -4.4003868 -4.3223867 -4.2646818 -4.2602763 -4.3027158 -4.3450985 -4.3594713 -4.3662548 -4.3850818 -4.4094324][-4.5192604 -4.5184035 -4.5110741 -4.4950027 -4.4578261 -4.4007759 -4.3296452 -4.2749906 -4.2679262 -4.3048825 -4.3423963 -4.3537574 -4.3595638 -4.3786888 -4.4002004][-4.5217576 -4.509758 -4.4882455 -4.4564629 -4.4112062 -4.358295 -4.3009076 -4.2612448 -4.2648482 -4.3089042 -4.3528762 -4.3700109 -4.3777452 -4.3928208 -4.4027143][-4.4449782 -4.4367709 -4.416399 -4.3745813 -4.3145247 -4.2530975 -4.1997471 -4.176486 -4.2082944 -4.2849832 -4.3585758 -4.3967323 -4.411253 -4.4202662 -4.4152279][-4.3420153 -4.34908 -4.3429074 -4.2957492 -4.2093225 -4.1173439 -4.0455713 -4.025538 -4.0876074 -4.2063789 -4.3164921 -4.3771377 -4.3981309 -4.4066391 -4.3992777][-4.2587252 -4.2810011 -4.29033 -4.233211 -4.1056724 -3.9617357 -3.8557365 -3.836623 -3.9364247 -4.1034446 -4.2415204 -4.30318 -4.3107033 -4.3139381 -4.319068][-4.2298684 -4.2589946 -4.2781153 -4.2117462 -4.0459461 -3.8537197 -3.7156582 -3.6960764 -3.8282166 -4.0317035 -4.1800756 -4.2252021 -4.2052784 -4.1930313 -4.2100759][-4.2643852 -4.2988143 -4.3266449 -4.2646074 -4.093936 -3.8918493 -3.7408526 -3.7086785 -3.8341074 -4.0281496 -4.1536093 -4.166873 -4.1177406 -4.0851288 -4.1040359][-4.340683 -4.3782945 -4.4070988 -4.3538532 -4.2056875 -4.0317526 -3.8983128 -3.8608718 -3.9565315 -4.1032853 -4.1727948 -4.1357217 -4.053844 -3.9984677 -4.0068097][-4.4202104 -4.461154 -4.4872565 -4.4469814 -4.3376422 -4.2128568 -4.11286 -4.0705695 -4.1207824 -4.2015529 -4.2068086 -4.1229062 -4.0198379 -3.9545774 -3.952332][-4.4694967 -4.5102315 -4.5375142 -4.5183835 -4.4573941 -4.3873377 -4.3227735 -4.2757854 -4.2768531 -4.2914658 -4.24766 -4.1427813 -4.0404224 -3.9802485 -3.9749732][-4.4800835 -4.5122528 -4.537271 -4.5365996 -4.51715 -4.4959068 -4.4713492 -4.439775 -4.4211 -4.4014077 -4.3423586 -4.2470126 -4.1577711 -4.0997267 -4.083744][-4.4521704 -4.4783077 -4.5031624 -4.5158772 -4.5240889 -4.5370641 -4.547019 -4.5434165 -4.5347414 -4.5172663 -4.475122 -4.4114575 -4.3461361 -4.2934394 -4.268044][-4.4012332 -4.4232941 -4.4488873 -4.47057 -4.4941368 -4.5251212 -4.5553532 -4.5741873 -4.5836797 -4.5833187 -4.5678959 -4.5382576 -4.5006762 -4.46358 -4.439683]]...]
INFO - root - 2017-12-07 13:33:56.878258: step 20210, loss = 21.61, batch loss = 21.53 (8.2 examples/sec; 0.975 sec/batch; 84h:33m:28s remains)
INFO - root - 2017-12-07 13:34:06.182200: step 20220, loss = 21.83, batch loss = 21.74 (8.7 examples/sec; 0.917 sec/batch; 79h:34m:10s remains)
INFO - root - 2017-12-07 13:34:15.417310: step 20230, loss = 21.33, batch loss = 21.25 (9.1 examples/sec; 0.882 sec/batch; 76h:31m:47s remains)
INFO - root - 2017-12-07 13:34:24.765700: step 20240, loss = 21.40, batch loss = 21.32 (8.8 examples/sec; 0.910 sec/batch; 78h:54m:40s remains)
INFO - root - 2017-12-07 13:34:34.169473: step 20250, loss = 20.95, batch loss = 20.87 (8.2 examples/sec; 0.970 sec/batch; 84h:09m:22s remains)
INFO - root - 2017-12-07 13:34:43.478468: step 20260, loss = 21.78, batch loss = 21.70 (8.2 examples/sec; 0.978 sec/batch; 84h:51m:38s remains)
INFO - root - 2017-12-07 13:34:52.934503: step 20270, loss = 21.54, batch loss = 21.45 (7.8 examples/sec; 1.023 sec/batch; 88h:41m:50s remains)
INFO - root - 2017-12-07 13:35:02.323421: step 20280, loss = 21.76, batch loss = 21.68 (8.2 examples/sec; 0.977 sec/batch; 84h:44m:25s remains)
INFO - root - 2017-12-07 13:35:11.642674: step 20290, loss = 21.40, batch loss = 21.31 (8.5 examples/sec; 0.946 sec/batch; 82h:03m:44s remains)
INFO - root - 2017-12-07 13:35:20.989865: step 20300, loss = 21.80, batch loss = 21.72 (8.7 examples/sec; 0.918 sec/batch; 79h:37m:01s remains)
2017-12-07 13:35:21.922958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5285039 -4.556108 -4.5761852 -4.5852833 -4.583806 -4.5699925 -4.5573988 -4.5617752 -4.5849209 -4.6020651 -4.5785651 -4.507452 -4.4010081 -4.314611 -4.2909784][-4.5626559 -4.5875168 -4.6004133 -4.5991011 -4.5881257 -4.5629554 -4.5347404 -4.5286531 -4.5603323 -4.5984035 -4.5942583 -4.53364 -4.4285493 -4.3365784 -4.3017077][-4.5741515 -4.5953689 -4.5936708 -4.5706429 -4.5361347 -4.4944758 -4.4545841 -4.4449658 -4.489027 -4.5503454 -4.570416 -4.5279517 -4.4370894 -4.3551121 -4.323113][-4.5634012 -4.5730867 -4.5453286 -4.4835062 -4.4081368 -4.3417139 -4.2959557 -4.2947688 -4.360168 -4.4521747 -4.50474 -4.49544 -4.4370537 -4.3758554 -4.3537307][-4.5487862 -4.5466 -4.4926538 -4.387682 -4.2604938 -4.1551528 -4.0934334 -4.0958033 -4.1801076 -4.3049421 -4.3997784 -4.4419637 -4.4314308 -4.3928123 -4.3737316][-4.5239139 -4.5186362 -4.4557438 -4.3230324 -4.1509452 -3.9995985 -3.9084873 -3.9028554 -3.9973903 -4.1474195 -4.2826166 -4.3773866 -4.4165983 -4.4024758 -4.3856292][-4.4748859 -4.4742184 -4.4205327 -4.2872891 -4.0930314 -3.8999927 -3.7717865 -3.7535131 -3.8565075 -4.024302 -4.1896133 -4.3229418 -4.3995385 -4.4104347 -4.4014754][-4.429584 -4.4402056 -4.4152923 -4.311368 -4.1296167 -3.9192989 -3.7537518 -3.7034364 -3.7933595 -3.9586844 -4.1361685 -4.2891493 -4.3870034 -4.4173527 -4.4165668][-4.3899369 -4.4087253 -4.4168758 -4.3636675 -4.2326679 -4.0535464 -3.8879786 -3.8110359 -3.8593879 -3.9860573 -4.14086 -4.28816 -4.3898754 -4.4312682 -4.4378409][-4.3497853 -4.3626728 -4.3827715 -4.3678961 -4.2957711 -4.1802492 -4.056798 -3.9807682 -3.995208 -4.0768561 -4.1892681 -4.3057227 -4.3940434 -4.4378 -4.4521847][-4.32112 -4.3291106 -4.3399596 -4.3326421 -4.2938027 -4.23982 -4.1781154 -4.12967 -4.1309953 -4.1794133 -4.2457027 -4.3153539 -4.3736095 -4.4087982 -4.4305544][-4.2795463 -4.2931881 -4.2897739 -4.2603154 -4.2170763 -4.196 -4.1986418 -4.206151 -4.2264466 -4.259325 -4.2844977 -4.3032408 -4.3272915 -4.3570232 -4.3928142][-4.2015266 -4.2422171 -4.2421112 -4.1844521 -4.1043367 -4.069479 -4.1048284 -4.1704283 -4.2341638 -4.2779789 -4.2870641 -4.2793531 -4.2853441 -4.31436 -4.3658862][-4.1357694 -4.2094855 -4.2337503 -4.1729164 -4.0653529 -3.9895492 -4.0016642 -4.0803738 -4.1720848 -4.239059 -4.2662473 -4.2719221 -4.2826862 -4.3075137 -4.356905][-4.1246414 -4.2128439 -4.2596331 -4.2181606 -4.1158128 -4.0164928 -3.9891787 -4.040441 -4.1239295 -4.1972847 -4.2441955 -4.2751255 -4.302587 -4.3273158 -4.3649445]]...]
INFO - root - 2017-12-07 13:35:31.163005: step 20310, loss = 21.34, batch loss = 21.26 (9.0 examples/sec; 0.889 sec/batch; 77h:06m:29s remains)
INFO - root - 2017-12-07 13:35:40.442979: step 20320, loss = 21.07, batch loss = 20.98 (8.6 examples/sec; 0.932 sec/batch; 80h:47m:06s remains)
INFO - root - 2017-12-07 13:35:49.846502: step 20330, loss = 21.30, batch loss = 21.21 (8.7 examples/sec; 0.922 sec/batch; 79h:55m:19s remains)
INFO - root - 2017-12-07 13:35:59.369060: step 20340, loss = 21.70, batch loss = 21.62 (8.8 examples/sec; 0.910 sec/batch; 78h:53m:38s remains)
INFO - root - 2017-12-07 13:36:08.721978: step 20350, loss = 20.82, batch loss = 20.74 (9.0 examples/sec; 0.889 sec/batch; 77h:05m:43s remains)
INFO - root - 2017-12-07 13:36:17.972530: step 20360, loss = 21.13, batch loss = 21.05 (8.7 examples/sec; 0.918 sec/batch; 79h:35m:49s remains)
INFO - root - 2017-12-07 13:36:27.458471: step 20370, loss = 21.42, batch loss = 21.34 (8.4 examples/sec; 0.955 sec/batch; 82h:48m:33s remains)
INFO - root - 2017-12-07 13:36:36.826003: step 20380, loss = 21.67, batch loss = 21.59 (8.7 examples/sec; 0.923 sec/batch; 80h:02m:53s remains)
INFO - root - 2017-12-07 13:36:46.112442: step 20390, loss = 21.64, batch loss = 21.56 (9.0 examples/sec; 0.893 sec/batch; 77h:25m:57s remains)
INFO - root - 2017-12-07 13:36:55.529472: step 20400, loss = 21.65, batch loss = 21.57 (8.3 examples/sec; 0.958 sec/batch; 83h:04m:02s remains)
2017-12-07 13:36:56.565596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9706628 -4.0180907 -4.0436215 -4.0419755 -4.0580091 -4.1104631 -4.1620564 -4.1799641 -4.1618772 -4.1265931 -4.1022649 -4.0961561 -4.1140046 -4.1457348 -4.1743][-3.8829365 -3.9054406 -3.9074993 -3.9013615 -3.9328179 -4.0056825 -4.0674891 -4.0730786 -4.0256658 -3.9618926 -3.9246297 -3.9253366 -3.9780667 -4.0727153 -4.1799622][-3.8662732 -3.8741095 -3.8624496 -3.8554564 -3.8936076 -3.9706812 -4.0252562 -4.0010247 -3.9102705 -3.8153324 -3.7733557 -3.7938812 -3.8806291 -4.0170856 -4.1749644][-3.9309354 -3.9197588 -3.8974416 -3.8953791 -3.9374671 -3.9994788 -4.0198774 -3.9492755 -3.8121488 -3.688668 -3.6524668 -3.7083607 -3.8345354 -4.002048 -4.1845016][-4.071054 -4.0456824 -4.0199466 -4.0228071 -4.0569329 -4.0794191 -4.0376277 -3.9098179 -3.7396681 -3.6100912 -3.5971458 -3.6951776 -3.8549304 -4.0399246 -4.2292018][-4.2617393 -4.2378974 -4.2208939 -4.2255549 -4.2357836 -4.20065 -4.081666 -3.8976367 -3.7152495 -3.6107516 -3.6451905 -3.7903008 -3.972173 -4.152359 -4.3208079][-4.4204192 -4.4047546 -4.3989215 -4.4031453 -4.3890834 -4.31098 -4.1452484 -3.9369826 -3.7662375 -3.6985776 -3.7760463 -3.9520741 -4.1394153 -4.29758 -4.4231935][-4.5122237 -4.51225 -4.5229645 -4.5287585 -4.4978032 -4.3979106 -4.2200551 -4.0179958 -3.867831 -3.8256803 -3.9213681 -4.1029139 -4.2816725 -4.4121237 -4.4943333][-4.5305152 -4.5440922 -4.5712924 -4.5851808 -4.5537214 -4.4590864 -4.3056245 -4.1397181 -4.0189571 -3.9908111 -4.0787568 -4.2371206 -4.38581 -4.4774976 -4.5166698][-4.4683452 -4.4911385 -4.5296869 -4.5581026 -4.5466151 -4.485889 -4.3858504 -4.279098 -4.1991243 -4.1803637 -4.2423167 -4.353776 -4.4509554 -4.4944706 -4.4947748][-4.3741937 -4.4101453 -4.4584851 -4.5008225 -4.5142226 -4.4927797 -4.4470644 -4.3963132 -4.3548136 -4.343554 -4.3778291 -4.4372053 -4.4789286 -4.4789224 -4.452673][-4.3067918 -4.3587837 -4.4123988 -4.4572468 -4.4809022 -4.4823537 -4.470171 -4.4523187 -4.4325361 -4.4223146 -4.4326172 -4.4514332 -4.4551764 -4.4347878 -4.4044943][-4.2598624 -4.3227315 -4.3760653 -4.4130912 -4.4300094 -4.43219 -4.42672 -4.4156766 -4.398869 -4.3837171 -4.3785095 -4.3777933 -4.3715897 -4.3594174 -4.3490973][-4.2275796 -4.2883315 -4.3371248 -4.3648219 -4.3708987 -4.363636 -4.3497028 -4.3296161 -4.3037205 -4.2815 -4.2718992 -4.2724075 -4.2755451 -4.2822523 -4.2964582][-4.2240615 -4.2725086 -4.3131218 -4.3343081 -4.3347111 -4.3208728 -4.2973976 -4.2646408 -4.2255635 -4.1952324 -4.1864638 -4.1963339 -4.2126012 -4.2330766 -4.2614226]]...]
INFO - root - 2017-12-07 13:37:05.889666: step 20410, loss = 21.87, batch loss = 21.79 (8.5 examples/sec; 0.938 sec/batch; 81h:19m:02s remains)
INFO - root - 2017-12-07 13:37:15.228024: step 20420, loss = 22.00, batch loss = 21.92 (9.1 examples/sec; 0.878 sec/batch; 76h:08m:19s remains)
INFO - root - 2017-12-07 13:37:24.529650: step 20430, loss = 21.59, batch loss = 21.51 (8.6 examples/sec; 0.930 sec/batch; 80h:35m:01s remains)
INFO - root - 2017-12-07 13:37:33.849484: step 20440, loss = 21.25, batch loss = 21.17 (9.0 examples/sec; 0.889 sec/batch; 77h:03m:08s remains)
INFO - root - 2017-12-07 13:37:43.286515: step 20450, loss = 21.12, batch loss = 21.03 (8.5 examples/sec; 0.946 sec/batch; 81h:59m:52s remains)
INFO - root - 2017-12-07 13:37:52.611697: step 20460, loss = 21.35, batch loss = 21.27 (8.7 examples/sec; 0.923 sec/batch; 80h:01m:10s remains)
INFO - root - 2017-12-07 13:38:02.002439: step 20470, loss = 21.55, batch loss = 21.47 (8.5 examples/sec; 0.944 sec/batch; 81h:49m:02s remains)
INFO - root - 2017-12-07 13:38:11.277282: step 20480, loss = 21.52, batch loss = 21.43 (8.4 examples/sec; 0.947 sec/batch; 82h:05m:29s remains)
INFO - root - 2017-12-07 13:38:20.719875: step 20490, loss = 21.17, batch loss = 21.09 (8.5 examples/sec; 0.944 sec/batch; 81h:50m:16s remains)
INFO - root - 2017-12-07 13:38:30.069878: step 20500, loss = 21.38, batch loss = 21.30 (8.3 examples/sec; 0.969 sec/batch; 83h:57m:16s remains)
2017-12-07 13:38:30.952049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.311842 -4.3496456 -4.4011183 -4.4495168 -4.492353 -4.5338678 -4.5564785 -4.5348482 -4.5027771 -4.5094628 -4.524672 -4.524291 -4.4970007 -4.4445367 -4.4211912][-4.2995915 -4.347559 -4.4058104 -4.4405537 -4.4533215 -4.4907837 -4.550848 -4.579566 -4.5707693 -4.560184 -4.5441542 -4.5180221 -4.4732633 -4.4155364 -4.3867893][-4.2900648 -4.3503776 -4.4226336 -4.4488659 -4.4227281 -4.4208894 -4.4731588 -4.5257168 -4.5433874 -4.5323739 -4.5052423 -4.4705715 -4.422091 -4.3711915 -4.3560877][-4.2743611 -4.34515 -4.4278893 -4.4499955 -4.3913236 -4.33046 -4.3324094 -4.3804779 -4.4327922 -4.4550457 -4.4456668 -4.4121714 -4.3564925 -4.3114104 -4.3168988][-4.2463908 -4.3159828 -4.388216 -4.3906193 -4.2985435 -4.1852369 -4.136848 -4.1781554 -4.2751851 -4.3521066 -4.3781719 -4.3489709 -4.2804165 -4.2251639 -4.225812][-4.2391391 -4.2959781 -4.3341465 -4.2877231 -4.1525717 -3.9956713 -3.9012392 -3.932333 -4.07053 -4.2114429 -4.2882304 -4.2813096 -4.2145534 -4.1448455 -4.1196294][-4.2728238 -4.3037505 -4.2939878 -4.1908236 -4.0166931 -3.8294332 -3.6901069 -3.6925671 -3.8522744 -4.0537624 -4.1967559 -4.2344942 -4.1860776 -4.1018605 -4.042624][-4.3373585 -4.3474722 -4.3021522 -4.1702719 -3.9895735 -3.8009729 -3.6380908 -3.6025629 -3.7433746 -3.9606359 -4.1412864 -4.205873 -4.1584344 -4.0468264 -3.9545603][-4.3807564 -4.3963284 -4.3551555 -4.2386932 -4.0877857 -3.9269962 -3.7789202 -3.7253613 -3.8163259 -3.9889326 -4.1521544 -4.2117925 -4.1496587 -4.0058761 -3.8864226][-4.3677783 -4.4155245 -4.4169059 -4.3509483 -4.24327 -4.1072979 -3.9824846 -3.9250493 -3.9693403 -4.0767832 -4.1940827 -4.2441707 -4.1861639 -4.035099 -3.9049697][-4.3284988 -4.4027343 -4.454267 -4.4535613 -4.402833 -4.3015165 -4.20189 -4.1429482 -4.1411023 -4.17646 -4.2333241 -4.2731419 -4.2452736 -4.131618 -4.0197635][-4.3109379 -4.3788853 -4.4519086 -4.5000176 -4.5049829 -4.4533219 -4.3941317 -4.3547831 -4.3316674 -4.3149805 -4.3172255 -4.3355303 -4.3315635 -4.2651668 -4.1790318][-4.3001513 -4.3425665 -4.4091892 -4.4712868 -4.5077496 -4.5017271 -4.490263 -4.4863086 -4.4731131 -4.4506817 -4.439229 -4.44849 -4.4561028 -4.4205284 -4.3506436][-4.2765636 -4.286777 -4.3302016 -4.3868337 -4.432189 -4.4530616 -4.4777646 -4.5071278 -4.5171418 -4.5145378 -4.5211878 -4.5402136 -4.5602231 -4.5455236 -4.4921975][-4.2410383 -4.2316351 -4.2508254 -4.2925105 -4.3334756 -4.360919 -4.4026074 -4.4596014 -4.4974279 -4.5168238 -4.5438051 -4.5788116 -4.6120272 -4.6165686 -4.5870023]]...]
INFO - root - 2017-12-07 13:38:40.432986: step 20510, loss = 21.91, batch loss = 21.83 (8.0 examples/sec; 0.995 sec/batch; 86h:12m:06s remains)
INFO - root - 2017-12-07 13:38:49.774287: step 20520, loss = 21.62, batch loss = 21.54 (8.1 examples/sec; 0.989 sec/batch; 85h:42m:48s remains)
INFO - root - 2017-12-07 13:38:58.996092: step 20530, loss = 20.99, batch loss = 20.91 (8.1 examples/sec; 0.982 sec/batch; 85h:06m:51s remains)
INFO - root - 2017-12-07 13:39:08.301501: step 20540, loss = 21.52, batch loss = 21.43 (8.5 examples/sec; 0.940 sec/batch; 81h:29m:11s remains)
INFO - root - 2017-12-07 13:39:17.616946: step 20550, loss = 21.27, batch loss = 21.19 (8.7 examples/sec; 0.921 sec/batch; 79h:46m:57s remains)
INFO - root - 2017-12-07 13:39:26.950465: step 20560, loss = 21.34, batch loss = 21.25 (8.5 examples/sec; 0.942 sec/batch; 81h:37m:30s remains)
INFO - root - 2017-12-07 13:39:36.324401: step 20570, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.931 sec/batch; 80h:41m:13s remains)
INFO - root - 2017-12-07 13:39:45.624514: step 20580, loss = 21.69, batch loss = 21.60 (8.4 examples/sec; 0.955 sec/batch; 82h:46m:05s remains)
INFO - root - 2017-12-07 13:39:55.064828: step 20590, loss = 21.89, batch loss = 21.81 (9.4 examples/sec; 0.848 sec/batch; 73h:30m:41s remains)
INFO - root - 2017-12-07 13:40:04.525809: step 20600, loss = 21.54, batch loss = 21.46 (9.0 examples/sec; 0.886 sec/batch; 76h:45m:57s remains)
2017-12-07 13:40:05.446470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4302926 -4.47654 -4.5116348 -4.5233684 -4.5217319 -4.5294886 -4.5513492 -4.5680819 -4.5670295 -4.562994 -4.5682573 -4.5778065 -4.5798941 -4.5682716 -4.5445204][-4.4928789 -4.5590844 -4.6040473 -4.609551 -4.5938358 -4.5938988 -4.6217537 -4.64551 -4.6450729 -4.6480751 -4.6677766 -4.6885853 -4.6912394 -4.673471 -4.6411662][-4.5319357 -4.6125455 -4.6544433 -4.6367064 -4.5882635 -4.5582447 -4.5725012 -4.5905948 -4.5900636 -4.608953 -4.6560235 -4.7003036 -4.7098789 -4.6905341 -4.6538234][-4.5482779 -4.6310778 -4.6495304 -4.58491 -4.4841981 -4.4067817 -4.3950496 -4.4047976 -4.4125371 -4.4579983 -4.5387559 -4.6081009 -4.6241918 -4.6010156 -4.5604177][-4.564611 -4.63746 -4.6140857 -4.4847775 -4.3168387 -4.178103 -4.1288719 -4.1297731 -4.1633081 -4.2540588 -4.3779035 -4.4758234 -4.4963384 -4.4643221 -4.4233279][-4.6030116 -4.6668639 -4.598557 -4.4065261 -4.1748352 -3.9759965 -3.8821821 -3.873054 -3.9379473 -4.0797286 -4.2492709 -4.3750672 -4.400568 -4.3597064 -4.3191524][-4.6599832 -4.7176962 -4.6119618 -4.37457 -4.1057997 -3.8850839 -3.7806921 -3.7747955 -3.86016 -4.0300217 -4.2173123 -4.3387589 -4.3485918 -4.2911048 -4.2534933][-4.7165484 -4.7723751 -4.6436229 -4.3867245 -4.1188021 -3.9178545 -3.8274889 -3.8240666 -3.9042597 -4.0624032 -4.2199874 -4.2982311 -4.2765846 -4.2112603 -4.2003484][-4.7459111 -4.8037829 -4.672121 -4.4220963 -4.1814551 -4.0169492 -3.9407501 -3.923558 -3.9701109 -4.0883689 -4.1966128 -4.2262774 -4.178463 -4.1228609 -4.1512237][-4.7222962 -4.7723479 -4.655292 -4.4417605 -4.250628 -4.1235209 -4.0475888 -3.9946411 -3.9909933 -4.0709157 -4.1516581 -4.1553893 -4.0910945 -4.0413189 -4.0896225][-4.6665463 -4.7094212 -4.6212416 -4.4538321 -4.3019037 -4.1859989 -4.0921607 -4.0013738 -3.9589477 -4.0214319 -4.109098 -4.1203537 -4.04986 -3.9942956 -4.0385256][-4.6196718 -4.6712122 -4.6195064 -4.4885788 -4.3434987 -4.20121 -4.0771265 -3.9690797 -3.9203413 -3.9882495 -4.1035929 -4.142458 -4.0744586 -4.006628 -4.033195][-4.5902448 -4.6549706 -4.638 -4.5344119 -4.3820748 -4.2112851 -4.0713024 -3.9719024 -3.9424891 -4.022089 -4.1592875 -4.227572 -4.1756477 -4.1022711 -4.1091957][-4.5830846 -4.6599984 -4.6672039 -4.5880051 -4.4435096 -4.2767696 -4.1532855 -4.0864086 -4.0866275 -4.1667194 -4.295032 -4.3720331 -4.3390565 -4.2704449 -4.2621942][-4.5779943 -4.657505 -4.6762156 -4.6225963 -4.5135946 -4.3925691 -4.3164406 -4.291142 -4.3120871 -4.3778944 -4.4755449 -4.5446653 -4.5334368 -4.4848413 -4.4675512]]...]
INFO - root - 2017-12-07 13:40:14.821329: step 20610, loss = 21.64, batch loss = 21.56 (8.8 examples/sec; 0.908 sec/batch; 78h:39m:19s remains)
INFO - root - 2017-12-07 13:40:24.229177: step 20620, loss = 21.50, batch loss = 21.42 (8.6 examples/sec; 0.929 sec/batch; 80h:31m:31s remains)
INFO - root - 2017-12-07 13:40:33.575428: step 20630, loss = 21.60, batch loss = 21.51 (8.4 examples/sec; 0.947 sec/batch; 82h:03m:13s remains)
INFO - root - 2017-12-07 13:40:43.076104: step 20640, loss = 21.30, batch loss = 21.22 (8.6 examples/sec; 0.932 sec/batch; 80h:45m:10s remains)
INFO - root - 2017-12-07 13:40:52.458739: step 20650, loss = 21.28, batch loss = 21.19 (8.8 examples/sec; 0.909 sec/batch; 78h:45m:23s remains)
INFO - root - 2017-12-07 13:41:01.979568: step 20660, loss = 21.21, batch loss = 21.13 (8.1 examples/sec; 0.983 sec/batch; 85h:11m:19s remains)
INFO - root - 2017-12-07 13:41:11.352292: step 20670, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.934 sec/batch; 80h:56m:41s remains)
INFO - root - 2017-12-07 13:41:20.708513: step 20680, loss = 21.61, batch loss = 21.53 (8.0 examples/sec; 0.998 sec/batch; 86h:25m:59s remains)
INFO - root - 2017-12-07 13:41:30.086949: step 20690, loss = 21.41, batch loss = 21.33 (8.5 examples/sec; 0.936 sec/batch; 81h:06m:27s remains)
INFO - root - 2017-12-07 13:41:39.214098: step 20700, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.908 sec/batch; 78h:40m:09s remains)
2017-12-07 13:41:40.126173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5914593 -4.6646633 -4.7294569 -4.7287188 -4.6517029 -4.5087829 -4.3344293 -4.2602515 -4.329349 -4.4366083 -4.5603828 -4.6791377 -4.7285013 -4.6840382 -4.5565505][-4.5864625 -4.694262 -4.7810078 -4.8030691 -4.760973 -4.6670189 -4.5491309 -4.5075588 -4.5681996 -4.6635909 -4.7632232 -4.8377447 -4.8405967 -4.7524672 -4.592442][-4.5528526 -4.6734419 -4.7797861 -4.8375964 -4.8386626 -4.7763076 -4.6751904 -4.6164579 -4.6438904 -4.7395492 -4.848803 -4.91125 -4.892487 -4.7846236 -4.6148553][-4.503932 -4.624989 -4.7437568 -4.8305149 -4.8607574 -4.8076148 -4.6934252 -4.5878835 -4.5696878 -4.6778784 -4.8210063 -4.8908224 -4.8648696 -4.7530575 -4.5912189][-4.4613624 -4.5900006 -4.7175965 -4.8074446 -4.8229618 -4.7358503 -4.5752 -4.4068537 -4.3536339 -4.5016694 -4.70401 -4.8026161 -4.7870679 -4.6810145 -4.528933][-4.4363995 -4.5759697 -4.7077 -4.7744923 -4.7281175 -4.5609059 -4.3191376 -4.0802469 -4.0146317 -4.226943 -4.5055747 -4.663918 -4.6922588 -4.606461 -4.4621787][-4.4293737 -4.5714164 -4.6950555 -4.7224579 -4.5967655 -4.3360877 -4.0034218 -3.7022932 -3.6458907 -3.912014 -4.2515855 -4.4863405 -4.5868406 -4.5416169 -4.4122152][-4.4357271 -4.5687208 -4.6790514 -4.6765952 -4.4970059 -4.1739588 -3.7782969 -3.4459741 -3.4072108 -3.68249 -4.0330343 -4.3275719 -4.4989657 -4.5015111 -4.3952103][-4.4466662 -4.5597229 -4.6558981 -4.6438208 -4.4607835 -4.1405344 -3.7510629 -3.4552577 -3.4544404 -3.6938064 -3.9973786 -4.293088 -4.4860888 -4.5103588 -4.4151478][-4.4474978 -4.5390482 -4.6206145 -4.6133761 -4.4668016 -4.2070041 -3.8865225 -3.6739297 -3.7167957 -3.9093156 -4.1452394 -4.3924484 -4.5511265 -4.5597758 -4.4560928][-4.4355683 -4.5145216 -4.5878873 -4.59589 -4.5058489 -4.3325539 -4.1101327 -3.982271 -4.04267 -4.1861615 -4.3563137 -4.5294876 -4.627624 -4.6045952 -4.4842796][-4.400785 -4.4768324 -4.5510554 -4.5821109 -4.5508361 -4.4627552 -4.3381276 -4.2825222 -4.3468928 -4.4518385 -4.5606508 -4.652813 -4.6802716 -4.6142511 -4.4768119][-4.3460431 -4.4118209 -4.4866819 -4.5403409 -4.5543594 -4.5261831 -4.4694147 -4.4550204 -4.5146785 -4.5980487 -4.6668596 -4.6990762 -4.6713552 -4.5732627 -4.434247][-4.2866235 -4.3304229 -4.3934226 -4.4582958 -4.5032082 -4.5144038 -4.4980512 -4.4977622 -4.5380669 -4.5947032 -4.6337533 -4.6347103 -4.5855575 -4.4855771 -4.3689065][-4.2406516 -4.2576671 -4.2955508 -4.3480253 -4.3996196 -4.4328747 -4.4445062 -4.4526606 -4.4738441 -4.5012803 -4.5155668 -4.503572 -4.4573207 -4.378768 -4.2982082]]...]
INFO - root - 2017-12-07 13:41:49.280925: step 20710, loss = 21.59, batch loss = 21.51 (9.8 examples/sec; 0.818 sec/batch; 70h:51m:20s remains)
INFO - root - 2017-12-07 13:41:58.534369: step 20720, loss = 21.12, batch loss = 21.03 (8.7 examples/sec; 0.924 sec/batch; 79h:59m:16s remains)
INFO - root - 2017-12-07 13:42:07.793655: step 20730, loss = 21.39, batch loss = 21.31 (9.6 examples/sec; 0.833 sec/batch; 72h:06m:41s remains)
INFO - root - 2017-12-07 13:42:17.225342: step 20740, loss = 21.42, batch loss = 21.33 (9.0 examples/sec; 0.891 sec/batch; 77h:11m:26s remains)
INFO - root - 2017-12-07 13:42:26.839321: step 20750, loss = 21.52, batch loss = 21.44 (7.4 examples/sec; 1.075 sec/batch; 93h:07m:15s remains)
INFO - root - 2017-12-07 13:42:36.095228: step 20760, loss = 20.90, batch loss = 20.82 (8.0 examples/sec; 0.995 sec/batch; 86h:11m:27s remains)
INFO - root - 2017-12-07 13:42:45.296648: step 20770, loss = 21.55, batch loss = 21.46 (8.6 examples/sec; 0.934 sec/batch; 80h:52m:47s remains)
INFO - root - 2017-12-07 13:42:54.617456: step 20780, loss = 21.58, batch loss = 21.50 (8.5 examples/sec; 0.937 sec/batch; 81h:06m:50s remains)
INFO - root - 2017-12-07 13:43:04.131908: step 20790, loss = 21.66, batch loss = 21.58 (8.4 examples/sec; 0.952 sec/batch; 82h:26m:18s remains)
INFO - root - 2017-12-07 13:43:13.649717: step 20800, loss = 21.36, batch loss = 21.28 (8.1 examples/sec; 0.994 sec/batch; 86h:02m:26s remains)
2017-12-07 13:43:14.723651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5777888 -4.5422149 -4.5407171 -4.5672398 -4.5884781 -4.5890164 -4.5797443 -4.5897231 -4.625905 -4.6448574 -4.6133404 -4.5477042 -4.5045233 -4.4755239 -4.4579196][-4.5695596 -4.5439005 -4.5660448 -4.6064773 -4.6170607 -4.584599 -4.5354548 -4.5203977 -4.5520787 -4.5764794 -4.5456181 -4.4764562 -4.438931 -4.4312778 -4.4403582][-4.5282774 -4.5223289 -4.5722275 -4.619123 -4.60847 -4.5399175 -4.4620581 -4.4388728 -4.4798145 -4.5170751 -4.4898014 -4.416441 -4.3730521 -4.36991 -4.3915653][-4.4577084 -4.4683061 -4.5418549 -4.58956 -4.552268 -4.4522476 -4.3598409 -4.3400626 -4.3962469 -4.4534793 -4.4438353 -4.385725 -4.3484817 -4.3512225 -4.3760242][-4.3799944 -4.3957829 -4.4755888 -4.5148005 -4.4523559 -4.3320069 -4.237679 -4.2232933 -4.2870359 -4.3590269 -4.3774667 -4.3547683 -4.3442855 -4.3664951 -4.4013724][-4.3354607 -4.3474379 -4.4183226 -4.4370456 -4.3412771 -4.1913218 -4.0869551 -4.0708232 -4.1326194 -4.2181973 -4.2789731 -4.3160229 -4.35636 -4.4134579 -4.4718971][-4.3095393 -4.3244934 -4.3886166 -4.391736 -4.2638965 -4.0740371 -3.9434202 -3.9156628 -3.9739115 -4.0796008 -4.1952333 -4.3042383 -4.4033937 -4.4963965 -4.5743122][-4.266654 -4.300221 -4.3765979 -4.3884664 -4.2557297 -4.0471058 -3.9012146 -3.8650229 -3.9133663 -4.0267229 -4.1786017 -4.3369174 -4.4734793 -4.579649 -4.6543269][-4.2002649 -4.2568231 -4.3579679 -4.3981128 -4.2923303 -4.0995388 -3.9645772 -3.9353576 -3.9754655 -4.0742517 -4.2174625 -4.3738184 -4.5103669 -4.6141505 -4.6780076][-4.1704588 -4.255619 -4.3849597 -4.4551373 -4.3828044 -4.2182212 -4.1050177 -4.0905533 -4.1226873 -4.1903129 -4.2888966 -4.3999963 -4.5068078 -4.6000934 -4.6585145][-4.2319136 -4.3454986 -4.4918361 -4.5814228 -4.5374675 -4.4034629 -4.3091388 -4.2970595 -4.3129911 -4.3439717 -4.3873329 -4.4365654 -4.4981394 -4.5719495 -4.623436][-4.3573184 -4.4684281 -4.6024475 -4.6840267 -4.6557441 -4.5583773 -4.4885321 -4.4655752 -4.4489517 -4.4362659 -4.4302845 -4.4352951 -4.4721947 -4.5426741 -4.5983105][-4.4858289 -4.5685534 -4.6584373 -4.6926756 -4.6485133 -4.5756416 -4.5327244 -4.5043354 -4.4629059 -4.4170322 -4.3792043 -4.3686843 -4.4132257 -4.5036044 -4.5765462][-4.5428462 -4.5895991 -4.6229153 -4.5932479 -4.509964 -4.4424744 -4.427556 -4.4203563 -4.3925638 -4.351397 -4.3127284 -4.3106165 -4.3740435 -4.4825025 -4.5639291][-4.5476213 -4.5628724 -4.546813 -4.4557409 -4.3280253 -4.2527966 -4.2594233 -4.2892566 -4.308444 -4.3118305 -4.3059115 -4.327023 -4.3982973 -4.500771 -4.5728683]]...]
INFO - root - 2017-12-07 13:43:24.163221: step 20810, loss = 21.43, batch loss = 21.35 (8.5 examples/sec; 0.940 sec/batch; 81h:21m:17s remains)
INFO - root - 2017-12-07 13:43:33.736005: step 20820, loss = 21.35, batch loss = 21.27 (8.4 examples/sec; 0.955 sec/batch; 82h:39m:56s remains)
INFO - root - 2017-12-07 13:43:43.237953: step 20830, loss = 21.18, batch loss = 21.10 (8.4 examples/sec; 0.948 sec/batch; 82h:02m:59s remains)
INFO - root - 2017-12-07 13:43:52.530110: step 20840, loss = 21.58, batch loss = 21.50 (8.4 examples/sec; 0.958 sec/batch; 82h:56m:10s remains)
INFO - root - 2017-12-07 13:44:01.893598: step 20850, loss = 21.25, batch loss = 21.17 (8.8 examples/sec; 0.908 sec/batch; 78h:35m:31s remains)
INFO - root - 2017-12-07 13:44:11.221947: step 20860, loss = 21.47, batch loss = 21.39 (9.3 examples/sec; 0.857 sec/batch; 74h:09m:04s remains)
INFO - root - 2017-12-07 13:44:20.512163: step 20870, loss = 21.10, batch loss = 21.02 (8.4 examples/sec; 0.948 sec/batch; 82h:03m:05s remains)
INFO - root - 2017-12-07 13:44:30.041521: step 20880, loss = 21.32, batch loss = 21.24 (8.2 examples/sec; 0.973 sec/batch; 84h:13m:23s remains)
INFO - root - 2017-12-07 13:44:39.435574: step 20890, loss = 21.12, batch loss = 21.04 (8.0 examples/sec; 1.004 sec/batch; 86h:56m:26s remains)
INFO - root - 2017-12-07 13:44:48.966902: step 20900, loss = 21.55, batch loss = 21.46 (7.9 examples/sec; 1.009 sec/batch; 87h:19m:11s remains)
2017-12-07 13:44:49.874076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0332689 -4.3267622 -4.5485592 -4.6597371 -4.6670375 -4.6254687 -4.5824485 -4.5479493 -4.5164709 -4.5031853 -4.4962964 -4.4834342 -4.4706426 -4.4623451 -4.4498425][-4.0033011 -4.3131723 -4.5485406 -4.66794 -4.680716 -4.6406031 -4.6005049 -4.5773878 -4.5631232 -4.56734 -4.5690994 -4.5556469 -4.532979 -4.5061989 -4.4792776][-4.1594391 -4.4495583 -4.6596289 -4.741 -4.7100978 -4.62594 -4.5518045 -4.5191936 -4.5192728 -4.5478349 -4.5695548 -4.5640311 -4.536067 -4.4924812 -4.4520688][-4.4184837 -4.6575055 -4.8057857 -4.8152657 -4.7069874 -4.5496368 -4.4210382 -4.3662672 -4.3835182 -4.4546781 -4.5191126 -4.54241 -4.5187478 -4.4561362 -4.394515][-4.6453824 -4.8109627 -4.8753009 -4.8008246 -4.614409 -4.3886309 -4.2104993 -4.1351051 -4.1770983 -4.3086562 -4.4366188 -4.5096211 -4.5074444 -4.4352226 -4.3531418][-4.7627106 -4.8487105 -4.8285336 -4.6790223 -4.4334345 -4.1647277 -3.960386 -3.8778057 -3.9538219 -4.1530919 -4.3498869 -4.480731 -4.5166588 -4.4541683 -4.3657293][-4.7360587 -4.7593107 -4.6786284 -4.4825225 -4.2024546 -3.9111381 -3.6976862 -3.6228442 -3.7410841 -4.0044675 -4.2597733 -4.4409781 -4.5178671 -4.4815431 -4.4045892][-4.6428123 -4.63797 -4.5414815 -4.3390312 -4.0522289 -3.7499018 -3.529006 -3.4576855 -3.6025252 -3.9010711 -4.1871715 -4.3942804 -4.4976339 -4.4884872 -4.4293852][-4.5875187 -4.5926943 -4.5174475 -4.3433642 -4.082077 -3.7952068 -3.5846725 -3.5168147 -3.6502237 -3.9297972 -4.2055421 -4.4080734 -4.5118942 -4.5105276 -4.4525013][-4.5829954 -4.6151848 -4.5758715 -4.4468417 -4.240108 -4.0080647 -3.8396745 -3.7877536 -3.8870349 -4.1021428 -4.3263359 -4.4941111 -4.574492 -4.55824 -4.4804678][-4.5898762 -4.64473 -4.6395192 -4.5588551 -4.415113 -4.2546353 -4.1419344 -4.1138449 -4.1823139 -4.3300929 -4.4928894 -4.6122632 -4.6550584 -4.610631 -4.5071139][-4.5698929 -4.6393194 -4.66979 -4.6421566 -4.5622873 -4.4655938 -4.3972154 -4.3853989 -4.4332876 -4.5318995 -4.6405311 -4.709456 -4.7087173 -4.6363778 -4.52073][-4.5076551 -4.5715494 -4.6252031 -4.6459928 -4.6274381 -4.5866437 -4.5511074 -4.5466595 -4.5783839 -4.6391854 -4.70065 -4.7230515 -4.6867476 -4.6011267 -4.4975395][-4.4329672 -4.4718633 -4.5210295 -4.5632086 -4.5848889 -4.5851927 -4.5752406 -4.5747213 -4.5940619 -4.6262417 -4.6504993 -4.6423621 -4.5937467 -4.5210285 -4.4515][-4.3886132 -4.4045734 -4.435317 -4.4713058 -4.5011868 -4.5185127 -4.5234032 -4.5261941 -4.5354714 -4.546104 -4.5469365 -4.527091 -4.4871111 -4.4413624 -4.4069772]]...]
INFO - root - 2017-12-07 13:44:59.352065: step 20910, loss = 21.13, batch loss = 21.05 (8.4 examples/sec; 0.957 sec/batch; 82h:50m:00s remains)
INFO - root - 2017-12-07 13:45:08.726340: step 20920, loss = 21.56, batch loss = 21.48 (8.7 examples/sec; 0.919 sec/batch; 79h:31m:53s remains)
INFO - root - 2017-12-07 13:45:18.139581: step 20930, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.909 sec/batch; 78h:39m:14s remains)
INFO - root - 2017-12-07 13:45:27.530901: step 20940, loss = 21.39, batch loss = 21.31 (8.9 examples/sec; 0.894 sec/batch; 77h:21m:43s remains)
INFO - root - 2017-12-07 13:45:36.883095: step 20950, loss = 21.53, batch loss = 21.44 (8.5 examples/sec; 0.938 sec/batch; 81h:13m:08s remains)
INFO - root - 2017-12-07 13:45:46.373003: step 20960, loss = 21.76, batch loss = 21.68 (8.3 examples/sec; 0.959 sec/batch; 82h:59m:45s remains)
INFO - root - 2017-12-07 13:45:55.828494: step 20970, loss = 21.22, batch loss = 21.14 (8.0 examples/sec; 0.995 sec/batch; 86h:04m:23s remains)
INFO - root - 2017-12-07 13:46:05.287076: step 20980, loss = 21.73, batch loss = 21.65 (8.3 examples/sec; 0.961 sec/batch; 83h:08m:23s remains)
INFO - root - 2017-12-07 13:46:14.857181: step 20990, loss = 21.79, batch loss = 21.71 (9.0 examples/sec; 0.893 sec/batch; 77h:15m:10s remains)
INFO - root - 2017-12-07 13:46:24.310423: step 21000, loss = 21.52, batch loss = 21.43 (8.4 examples/sec; 0.958 sec/batch; 82h:53m:20s remains)
2017-12-07 13:46:25.243413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4426651 -4.4695535 -4.5053768 -4.5222387 -4.5064316 -4.4748316 -4.467134 -4.4671974 -4.4576359 -4.4396167 -4.4291229 -4.4594574 -4.5100794 -4.55678 -4.5922613][-4.4036245 -4.4433384 -4.4950104 -4.5187826 -4.4955864 -4.4505816 -4.4331417 -4.4275417 -4.4211068 -4.4098411 -4.4139934 -4.465147 -4.53195 -4.5947204 -4.6431255][-4.3660464 -4.42345 -4.4842849 -4.4998279 -4.4589372 -4.3960409 -4.3620267 -4.3535247 -4.3737879 -4.3938951 -4.4188285 -4.48028 -4.5566425 -4.634306 -4.6873584][-4.3431921 -4.4238839 -4.4967251 -4.4965 -4.4229751 -4.3292341 -4.259882 -4.2419548 -4.3072996 -4.3846025 -4.4394035 -4.499795 -4.5714536 -4.6533465 -4.7034845][-4.3056216 -4.4058003 -4.5001945 -4.49609 -4.3853765 -4.2390919 -4.1078625 -4.068006 -4.1805363 -4.3270593 -4.4227281 -4.4899549 -4.55914 -4.6433792 -4.691483][-4.218955 -4.3211865 -4.4493217 -4.4583392 -4.3166146 -4.1181231 -3.9232271 -3.8634624 -4.018908 -4.2276282 -4.3662348 -4.4524026 -4.5292592 -4.6143832 -4.6631212][-4.0990663 -4.1995363 -4.3553138 -4.3802142 -4.2234769 -3.99162 -3.753201 -3.6861882 -3.8769488 -4.1283216 -4.292172 -4.393055 -4.4717789 -4.5477591 -4.6016865][-4.044951 -4.1372275 -4.2854795 -4.3059349 -4.1516585 -3.9194925 -3.6722803 -3.6125517 -3.8262231 -4.083703 -4.2370796 -4.3415279 -4.417542 -4.4790387 -4.538363][-4.0711274 -4.1606913 -4.2832661 -4.2962995 -4.1760731 -3.9955602 -3.7884116 -3.7451143 -3.9437642 -4.1476183 -4.237865 -4.3174176 -4.382194 -4.4298573 -4.4971895][-4.1478953 -4.2300668 -4.3254876 -4.3405142 -4.2720914 -4.1685104 -4.028903 -4.0015669 -4.1500335 -4.2562113 -4.2534962 -4.2977514 -4.3534837 -4.4031262 -4.4910665][-4.23383 -4.2940745 -4.3677788 -4.3931479 -4.3708205 -4.3352518 -4.2605205 -4.2413411 -4.330718 -4.3475661 -4.2722206 -4.2917781 -4.3514457 -4.417891 -4.5271292][-4.3208418 -4.3442554 -4.3941188 -4.4278097 -4.4387975 -4.4473085 -4.420012 -4.411592 -4.4573755 -4.4186788 -4.3147421 -4.3250742 -4.3879776 -4.4657249 -4.5836444][-4.4150019 -4.3937392 -4.4065709 -4.4340887 -4.4615393 -4.4908838 -4.4932432 -4.4934473 -4.5097752 -4.4520617 -4.3598204 -4.3736835 -4.4349036 -4.51261 -4.6246][-4.4846864 -4.4385991 -4.4291458 -4.4543777 -4.488102 -4.5193334 -4.5302095 -4.5300655 -4.5277858 -4.4750276 -4.4125271 -4.429811 -4.482306 -4.5460281 -4.6318865][-4.5101428 -4.4676437 -4.4582634 -4.48379 -4.5146961 -4.5356827 -4.5418344 -4.5389838 -4.5285048 -4.4858866 -4.4420753 -4.4493275 -4.4842505 -4.529274 -4.5859756]]...]
INFO - root - 2017-12-07 13:46:34.650429: step 21010, loss = 21.12, batch loss = 21.04 (9.1 examples/sec; 0.881 sec/batch; 76h:11m:44s remains)
INFO - root - 2017-12-07 13:46:43.961838: step 21020, loss = 22.01, batch loss = 21.92 (8.6 examples/sec; 0.935 sec/batch; 80h:54m:05s remains)
INFO - root - 2017-12-07 13:46:53.385827: step 21030, loss = 21.52, batch loss = 21.44 (8.5 examples/sec; 0.947 sec/batch; 81h:53m:27s remains)
INFO - root - 2017-12-07 13:47:02.648109: step 21040, loss = 21.51, batch loss = 21.43 (8.6 examples/sec; 0.934 sec/batch; 80h:46m:02s remains)
INFO - root - 2017-12-07 13:47:12.114167: step 21050, loss = 21.64, batch loss = 21.56 (8.8 examples/sec; 0.908 sec/batch; 78h:35m:31s remains)
INFO - root - 2017-12-07 13:47:21.616007: step 21060, loss = 21.85, batch loss = 21.76 (9.0 examples/sec; 0.888 sec/batch; 76h:51m:54s remains)
INFO - root - 2017-12-07 13:47:30.960092: step 21070, loss = 21.69, batch loss = 21.61 (9.1 examples/sec; 0.875 sec/batch; 75h:39m:22s remains)
INFO - root - 2017-12-07 13:47:40.365752: step 21080, loss = 21.63, batch loss = 21.55 (8.4 examples/sec; 0.958 sec/batch; 82h:52m:11s remains)
INFO - root - 2017-12-07 13:47:49.856703: step 21090, loss = 21.70, batch loss = 21.61 (8.1 examples/sec; 0.985 sec/batch; 85h:13m:12s remains)
INFO - root - 2017-12-07 13:47:59.233088: step 21100, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.948 sec/batch; 82h:00m:57s remains)
2017-12-07 13:48:00.118960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.373817 -4.3932643 -4.41067 -4.4301047 -4.4403243 -4.4355621 -4.4148679 -4.384407 -4.3661218 -4.381031 -4.4154224 -4.4385214 -4.4346318 -4.4127836 -4.3905478][-4.4322481 -4.456286 -4.4732418 -4.4901762 -4.4929838 -4.47969 -4.4517212 -4.4123268 -4.3827085 -4.3903356 -4.423398 -4.4429803 -4.4305334 -4.3954949 -4.365953][-4.52818 -4.5536003 -4.5708308 -4.5812817 -4.567173 -4.5334525 -4.4935718 -4.454731 -4.4293242 -4.4321437 -4.453784 -4.4601936 -4.4417348 -4.4110479 -4.391818][-4.5977879 -4.6195054 -4.6303835 -4.61846 -4.5631142 -4.4863806 -4.4285955 -4.4066257 -4.4147243 -4.4402776 -4.46617 -4.4673319 -4.4504738 -4.4326186 -4.4295821][-4.5971913 -4.6051836 -4.5984645 -4.5449414 -4.4249735 -4.288662 -4.2083507 -4.208571 -4.2657356 -4.3430705 -4.4106307 -4.4431081 -4.4497042 -4.4479818 -4.4530997][-4.5439081 -4.5358109 -4.5037451 -4.3952074 -4.1948423 -3.9911821 -3.8793278 -3.8874826 -3.9828877 -4.1219244 -4.2680774 -4.3801465 -4.440237 -4.4614248 -4.4673958][-4.4782276 -4.4530807 -4.3888121 -4.2241936 -3.9518609 -3.6928184 -3.5578926 -3.5730336 -3.7004509 -3.8881481 -4.1062965 -4.3019409 -4.4161091 -4.4553633 -4.4593778][-4.4372554 -4.3958912 -4.3017182 -4.1040764 -3.797545 -3.5156589 -3.3735659 -3.3961487 -3.542475 -3.7506204 -3.9937859 -4.2228136 -4.356328 -4.3975229 -4.3993587][-4.4438066 -4.3870187 -4.2685728 -4.0647211 -3.7760811 -3.526314 -3.4166417 -3.4533997 -3.5935626 -3.7815006 -3.9894381 -4.184803 -4.2971082 -4.3316236 -4.3374662][-4.4740114 -4.4124408 -4.2916403 -4.1200271 -3.9036348 -3.735141 -3.6867521 -3.7361915 -3.8390331 -3.9725819 -4.1157503 -4.2496448 -4.3237042 -4.3428073 -4.344398][-4.4942594 -4.4466772 -4.3533831 -4.2415628 -4.119803 -4.0397434 -4.0457115 -4.0952964 -4.1469312 -4.2116318 -4.2890339 -4.3711724 -4.4172254 -4.418673 -4.4062562][-4.5031281 -4.47782 -4.4271636 -4.3791585 -4.33458 -4.3123894 -4.3350687 -4.366631 -4.3756056 -4.3882718 -4.4157534 -4.4592381 -4.4861665 -4.4749241 -4.4540577][-4.5208726 -4.5114374 -4.4913831 -4.4778767 -4.4640212 -4.4528069 -4.45919 -4.4645939 -4.4456415 -4.4247117 -4.4151878 -4.4302154 -4.4482794 -4.4437051 -4.4399958][-4.5310926 -4.5286484 -4.5198832 -4.5090313 -4.4826431 -4.4475231 -4.4247122 -4.40973 -4.3851032 -4.353826 -4.3225174 -4.3181124 -4.3355436 -4.3497429 -4.3776207][-4.5130239 -4.5136266 -4.5094624 -4.49507 -4.4456916 -4.3775649 -4.3269768 -4.3040128 -4.2987609 -4.2845554 -4.2532887 -4.239223 -4.2500567 -4.2685652 -4.3161569]]...]
INFO - root - 2017-12-07 13:48:09.629572: step 21110, loss = 21.45, batch loss = 21.37 (8.0 examples/sec; 1.000 sec/batch; 86h:29m:18s remains)
INFO - root - 2017-12-07 13:48:19.079765: step 21120, loss = 21.61, batch loss = 21.53 (7.7 examples/sec; 1.033 sec/batch; 89h:19m:58s remains)
INFO - root - 2017-12-07 13:48:28.593115: step 21130, loss = 21.82, batch loss = 21.74 (7.9 examples/sec; 1.009 sec/batch; 87h:13m:37s remains)
INFO - root - 2017-12-07 13:48:38.042599: step 21140, loss = 20.85, batch loss = 20.77 (9.0 examples/sec; 0.893 sec/batch; 77h:15m:32s remains)
INFO - root - 2017-12-07 13:48:47.360142: step 21150, loss = 22.00, batch loss = 21.91 (8.3 examples/sec; 0.968 sec/batch; 83h:44m:06s remains)
INFO - root - 2017-12-07 13:48:56.885356: step 21160, loss = 21.65, batch loss = 21.57 (8.5 examples/sec; 0.941 sec/batch; 81h:25m:20s remains)
INFO - root - 2017-12-07 13:49:06.319130: step 21170, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.968 sec/batch; 83h:44m:45s remains)
INFO - root - 2017-12-07 13:49:15.720762: step 21180, loss = 21.51, batch loss = 21.43 (9.2 examples/sec; 0.867 sec/batch; 74h:58m:54s remains)
INFO - root - 2017-12-07 13:49:24.981471: step 21190, loss = 21.31, batch loss = 21.23 (8.7 examples/sec; 0.924 sec/batch; 79h:55m:43s remains)
INFO - root - 2017-12-07 13:49:34.339080: step 21200, loss = 21.66, batch loss = 21.57 (9.0 examples/sec; 0.891 sec/batch; 77h:02m:43s remains)
2017-12-07 13:49:35.255492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6673093 -4.6886783 -4.7130165 -4.6948104 -4.626864 -4.5476341 -4.5178652 -4.5664353 -4.6612992 -4.7252941 -4.7276964 -4.6746254 -4.57484 -4.47872 -4.4400225][-4.5018039 -4.5294614 -4.5759811 -4.5896459 -4.5451756 -4.4819455 -4.4758878 -4.5494413 -4.6742759 -4.7711134 -4.7833157 -4.7119455 -4.5783858 -4.4491544 -4.37512][-4.38723 -4.434751 -4.4974127 -4.5305185 -4.495441 -4.4356484 -4.4441061 -4.529799 -4.6598206 -4.7616825 -4.7711787 -4.6830773 -4.527277 -4.383883 -4.3052316][-4.3529849 -4.4176803 -4.4638734 -4.4749351 -4.420342 -4.3483458 -4.3594995 -4.4465523 -4.5597506 -4.6372395 -4.6302495 -4.5399723 -4.3975344 -4.27378 -4.2199354][-4.3620138 -4.4184318 -4.4031968 -4.3442068 -4.245585 -4.15701 -4.1665363 -4.2528691 -4.348546 -4.3987927 -4.3828797 -4.3198376 -4.2354021 -4.162437 -4.1457253][-4.3754005 -4.3887892 -4.2838807 -4.1415124 -3.9980855 -3.89927 -3.9070296 -3.9957609 -4.0909672 -4.1396484 -4.1396365 -4.1257606 -4.1130619 -4.0898356 -4.0991635][-4.399673 -4.3591833 -4.1708817 -3.9594846 -3.7835817 -3.6794064 -3.6876569 -3.7809436 -3.8902531 -3.9668751 -4.0058312 -4.0453439 -4.088244 -4.092967 -4.1066666][-4.4714069 -4.3936534 -4.1600323 -3.9129028 -3.7215509 -3.6163466 -3.6258602 -3.7215428 -3.8446648 -3.9556732 -4.0456944 -4.1365018 -4.2125659 -4.2160511 -4.1995158][-4.5774727 -4.50031 -4.2811985 -4.0506258 -3.8719883 -3.7689025 -3.7679081 -3.8467071 -3.9596658 -4.08208 -4.203342 -4.3221612 -4.404254 -4.3926167 -4.3399472][-4.682044 -4.6300697 -4.4647584 -4.2793579 -4.1259413 -4.0268478 -4.0072613 -4.0571189 -4.1432872 -4.250844 -4.3712993 -4.4919658 -4.5708966 -4.5578527 -4.494216][-4.7532506 -4.731885 -4.6267829 -4.4949303 -4.3788009 -4.3003731 -4.2751594 -4.2979169 -4.3521976 -4.43107 -4.5290456 -4.6312256 -4.6980305 -4.6874361 -4.6269779][-4.7716579 -4.7708955 -4.7143259 -4.6336908 -4.5631084 -4.5198131 -4.5060477 -4.5163908 -4.5462971 -4.5941448 -4.653995 -4.7150965 -4.7512555 -4.7357 -4.6843681][-4.7116542 -4.7210808 -4.7000117 -4.66371 -4.6343656 -4.6227245 -4.6228471 -4.6279988 -4.639503 -4.6585331 -4.6806107 -4.7006445 -4.7059407 -4.6844406 -4.6433072][-4.5748925 -4.5906081 -4.5930595 -4.5881252 -4.5858769 -4.5903072 -4.5948629 -4.5954194 -4.5949869 -4.5952878 -4.5933394 -4.58756 -4.5741324 -4.5506029 -4.5193439][-4.40225 -4.4166365 -4.4281 -4.4352636 -4.4408684 -4.4451084 -4.44544 -4.4420004 -4.4369922 -4.430738 -4.4215617 -4.4092731 -4.3940053 -4.37634 -4.357276]]...]
INFO - root - 2017-12-07 13:49:44.651653: step 21210, loss = 21.39, batch loss = 21.31 (8.7 examples/sec; 0.919 sec/batch; 79h:28m:23s remains)
INFO - root - 2017-12-07 13:49:54.017146: step 21220, loss = 21.04, batch loss = 20.95 (8.2 examples/sec; 0.972 sec/batch; 84h:01m:55s remains)
INFO - root - 2017-12-07 13:50:03.486881: step 21230, loss = 21.53, batch loss = 21.44 (8.4 examples/sec; 0.955 sec/batch; 82h:33m:58s remains)
INFO - root - 2017-12-07 13:50:12.829513: step 21240, loss = 21.67, batch loss = 21.59 (8.2 examples/sec; 0.980 sec/batch; 84h:45m:52s remains)
INFO - root - 2017-12-07 13:50:22.128476: step 21250, loss = 21.33, batch loss = 21.25 (9.0 examples/sec; 0.890 sec/batch; 76h:58m:39s remains)
INFO - root - 2017-12-07 13:50:31.513069: step 21260, loss = 21.71, batch loss = 21.63 (8.7 examples/sec; 0.919 sec/batch; 79h:26m:25s remains)
INFO - root - 2017-12-07 13:50:40.874579: step 21270, loss = 21.66, batch loss = 21.58 (8.7 examples/sec; 0.915 sec/batch; 79h:06m:42s remains)
INFO - root - 2017-12-07 13:50:50.155630: step 21280, loss = 21.71, batch loss = 21.63 (8.6 examples/sec; 0.935 sec/batch; 80h:50m:33s remains)
INFO - root - 2017-12-07 13:50:59.497009: step 21290, loss = 21.78, batch loss = 21.69 (7.9 examples/sec; 1.017 sec/batch; 87h:54m:52s remains)
INFO - root - 2017-12-07 13:51:08.904724: step 21300, loss = 21.24, batch loss = 21.16 (8.6 examples/sec; 0.925 sec/batch; 79h:58m:31s remains)
2017-12-07 13:51:09.836982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.548245 -4.5582476 -4.5931969 -4.623529 -4.6116996 -4.57525 -4.54382 -4.5134449 -4.4996991 -4.5076327 -4.5404606 -4.5701666 -4.5245757 -4.3987303 -4.2861333][-4.4888744 -4.530849 -4.5822268 -4.6059155 -4.5643458 -4.4882264 -4.426722 -4.3959761 -4.4057274 -4.4355454 -4.4844356 -4.5363278 -4.5077729 -4.391048 -4.2828522][-4.4116769 -4.4935269 -4.5662122 -4.5823946 -4.507308 -4.3916063 -4.3040304 -4.2714448 -4.2988796 -4.3508897 -4.4173388 -4.4915924 -4.4829073 -4.379056 -4.2797451][-4.3648605 -4.4713173 -4.54941 -4.5492373 -4.4494119 -4.3117118 -4.2075906 -4.1636 -4.1898308 -4.2567778 -4.3482752 -4.4505634 -4.4646006 -4.3725462 -4.2785554][-4.41746 -4.5222807 -4.5685205 -4.5286012 -4.4054627 -4.2642908 -4.1550694 -4.0915389 -4.1018252 -4.1753387 -4.2955194 -4.4258671 -4.4597363 -4.3744164 -4.2799635][-4.5175543 -4.5691767 -4.5445695 -4.4551129 -4.3271847 -4.2132654 -4.1299729 -4.0689211 -4.0711317 -4.1464529 -4.2807364 -4.4185257 -4.4583654 -4.3770576 -4.2816668][-4.5116105 -4.4844809 -4.4070983 -4.3087173 -4.2079239 -4.1359344 -4.0911794 -4.0505929 -4.0560107 -4.1343026 -4.2695613 -4.3982763 -4.4381704 -4.368679 -4.2802339][-4.3709855 -4.2882905 -4.2068157 -4.1523595 -4.0959167 -4.0539689 -4.0336695 -4.0158372 -4.033823 -4.1161346 -4.2440104 -4.3582277 -4.3991542 -4.349544 -4.2752542][-4.1823626 -4.0888572 -4.035078 -4.0396562 -4.0318847 -4.0155549 -4.0186138 -4.0327511 -4.0695863 -4.1509862 -4.2588234 -4.347765 -4.3783488 -4.3372974 -4.2716346][-4.0678186 -4.0014729 -3.9758804 -4.0112934 -4.0388985 -4.0515394 -4.0830956 -4.1282358 -4.1748948 -4.24082 -4.3203626 -4.3798089 -4.3901291 -4.3409972 -4.27265][-4.0940943 -4.0657277 -4.0475578 -4.0727825 -4.111629 -4.1485429 -4.206553 -4.2721167 -4.3117847 -4.3412395 -4.387527 -4.4256473 -4.4186988 -4.355907 -4.2779531][-4.2112322 -4.2155256 -4.2045484 -4.2152028 -4.2448506 -4.282114 -4.3437881 -4.405683 -4.423461 -4.41308 -4.4301634 -4.4577904 -4.4455819 -4.3738456 -4.2850986][-4.3711495 -4.3957109 -4.4000278 -4.4058895 -4.4123807 -4.42459 -4.4602828 -4.4890628 -4.4716983 -4.42976 -4.4261417 -4.4537253 -4.4508309 -4.38183 -4.2891688][-4.5269637 -4.5558419 -4.5565963 -4.5459433 -4.520227 -4.5061522 -4.5216436 -4.5226784 -4.4830713 -4.4284396 -4.4111547 -4.4358358 -4.4402394 -4.3765993 -4.2868271][-4.6097188 -4.6183939 -4.58957 -4.5447016 -4.4909635 -4.47599 -4.5098109 -4.5217214 -4.4927459 -4.4540091 -4.4371338 -4.448977 -4.4401746 -4.3679767 -4.2802477]]...]
INFO - root - 2017-12-07 13:51:19.244779: step 21310, loss = 20.96, batch loss = 20.88 (8.0 examples/sec; 0.996 sec/batch; 86h:07m:06s remains)
INFO - root - 2017-12-07 13:51:28.623290: step 21320, loss = 21.90, batch loss = 21.82 (7.9 examples/sec; 1.008 sec/batch; 87h:08m:57s remains)
INFO - root - 2017-12-07 13:51:37.901714: step 21330, loss = 20.99, batch loss = 20.91 (8.4 examples/sec; 0.958 sec/batch; 82h:48m:05s remains)
INFO - root - 2017-12-07 13:51:47.347319: step 21340, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.973 sec/batch; 84h:05m:11s remains)
INFO - root - 2017-12-07 13:51:56.513410: step 21350, loss = 21.74, batch loss = 21.66 (9.0 examples/sec; 0.889 sec/batch; 76h:49m:11s remains)
INFO - root - 2017-12-07 13:52:05.912685: step 21360, loss = 20.94, batch loss = 20.86 (7.9 examples/sec; 1.012 sec/batch; 87h:30m:18s remains)
INFO - root - 2017-12-07 13:52:15.330411: step 21370, loss = 21.66, batch loss = 21.58 (8.4 examples/sec; 0.953 sec/batch; 82h:22m:16s remains)
INFO - root - 2017-12-07 13:52:24.764397: step 21380, loss = 21.71, batch loss = 21.63 (8.3 examples/sec; 0.964 sec/batch; 83h:18m:21s remains)
INFO - root - 2017-12-07 13:52:33.998515: step 21390, loss = 21.62, batch loss = 21.54 (8.5 examples/sec; 0.938 sec/batch; 81h:06m:13s remains)
INFO - root - 2017-12-07 13:52:43.361718: step 21400, loss = 21.69, batch loss = 21.60 (9.2 examples/sec; 0.874 sec/batch; 75h:30m:46s remains)
2017-12-07 13:52:44.298413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5700469 -4.56665 -4.5325956 -4.5246067 -4.5513663 -4.5708604 -4.5568132 -4.5348825 -4.5408463 -4.564456 -4.5353703 -4.4106674 -4.2412171 -4.1290016 -4.143959][-4.57815 -4.5367885 -4.4638586 -4.4235344 -4.4383297 -4.4703565 -4.4844251 -4.492743 -4.5156407 -4.5442619 -4.5077085 -4.3739495 -4.2143269 -4.1237168 -4.15781][-4.48923 -4.4051447 -4.2971449 -4.2280731 -4.2304382 -4.2844572 -4.356421 -4.425055 -4.4817486 -4.5207291 -4.4880204 -4.3676457 -4.2382727 -4.1814179 -4.2373238][-4.3764548 -4.2411137 -4.0970926 -4.0004878 -3.9836059 -4.040761 -4.1535711 -4.2889628 -4.409739 -4.4900503 -4.4816194 -4.3886552 -4.2944765 -4.2700515 -4.3378434][-4.3176312 -4.14283 -3.96445 -3.837914 -3.77922 -3.7885017 -3.8813069 -4.0538039 -4.2588463 -4.4232216 -4.4708734 -4.4139776 -4.3477006 -4.34358 -4.4094553][-4.316143 -4.1145058 -3.9158835 -3.7674553 -3.6578465 -3.5809379 -3.5989754 -3.7659142 -4.0384359 -4.2988782 -4.4257474 -4.4200878 -4.3786383 -4.3797359 -4.4310746][-4.3352318 -4.1182566 -3.908145 -3.7478728 -3.6094553 -3.4614778 -3.3951437 -3.5216951 -3.8150215 -4.1377711 -4.3391247 -4.3973904 -4.388206 -4.3894811 -4.4155064][-4.3453479 -4.1253457 -3.911597 -3.7544916 -3.6279602 -3.4786634 -3.3818364 -3.4650545 -3.7236319 -4.0417304 -4.2749352 -4.3794637 -4.3995514 -4.3947053 -4.3854809][-4.3479238 -4.1492248 -3.950592 -3.8224959 -3.7486391 -3.6574736 -3.5898993 -3.6428409 -3.8256371 -4.0690384 -4.2711439 -4.3790612 -4.4087486 -4.3965616 -4.3631368][-4.3422937 -4.1936917 -4.0403571 -3.9633851 -3.9511094 -3.9280939 -3.9088237 -3.9398398 -4.0270147 -4.1583633 -4.289259 -4.370892 -4.39838 -4.3929391 -4.3673344][-4.3260989 -4.2399759 -4.145124 -4.1179471 -4.1502886 -4.1775632 -4.1981206 -4.215436 -4.2243738 -4.250082 -4.3000565 -4.3464684 -4.3733182 -4.3835196 -4.3850265][-4.3025746 -4.27772 -4.2358084 -4.2336621 -4.2766123 -4.3229303 -4.3656569 -4.3818398 -4.3515477 -4.3111873 -4.2952895 -4.3030882 -4.3262906 -4.3530359 -4.3774719][-4.2922435 -4.3093109 -4.3068972 -4.3090682 -4.3328247 -4.3647223 -4.4019618 -4.4173794 -4.38529 -4.3286276 -4.2828441 -4.260848 -4.2657256 -4.2867923 -4.31321][-4.3217649 -4.3570986 -4.3777461 -4.3805852 -4.3859911 -4.3944588 -4.4073095 -4.4095511 -4.3817263 -4.3336887 -4.2826777 -4.2378597 -4.2078733 -4.1962461 -4.201088][-4.3709221 -4.4043784 -4.4297466 -4.4435515 -4.4565368 -4.4616847 -4.454484 -4.4314885 -4.3917875 -4.34819 -4.3016615 -4.2434583 -4.1807141 -4.1293187 -4.108026]]...]
INFO - root - 2017-12-07 13:52:53.617857: step 21410, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.929 sec/batch; 80h:17m:29s remains)
INFO - root - 2017-12-07 13:53:02.926533: step 21420, loss = 21.79, batch loss = 21.71 (8.6 examples/sec; 0.929 sec/batch; 80h:18m:05s remains)
INFO - root - 2017-12-07 13:53:12.297660: step 21430, loss = 21.35, batch loss = 21.27 (8.7 examples/sec; 0.918 sec/batch; 79h:21m:37s remains)
INFO - root - 2017-12-07 13:53:21.572847: step 21440, loss = 21.88, batch loss = 21.80 (8.9 examples/sec; 0.897 sec/batch; 77h:29m:49s remains)
INFO - root - 2017-12-07 13:53:30.908347: step 21450, loss = 21.60, batch loss = 21.52 (9.9 examples/sec; 0.810 sec/batch; 69h:59m:57s remains)
INFO - root - 2017-12-07 13:53:40.282225: step 21460, loss = 21.25, batch loss = 21.17 (8.4 examples/sec; 0.957 sec/batch; 82h:41m:12s remains)
INFO - root - 2017-12-07 13:53:49.599897: step 21470, loss = 21.39, batch loss = 21.31 (8.7 examples/sec; 0.918 sec/batch; 79h:19m:26s remains)
INFO - root - 2017-12-07 13:53:58.954667: step 21480, loss = 21.29, batch loss = 21.21 (7.9 examples/sec; 1.017 sec/batch; 87h:52m:48s remains)
INFO - root - 2017-12-07 13:54:08.347318: step 21490, loss = 21.48, batch loss = 21.39 (8.3 examples/sec; 0.969 sec/batch; 83h:44m:04s remains)
INFO - root - 2017-12-07 13:54:17.816289: step 21500, loss = 21.74, batch loss = 21.66 (8.8 examples/sec; 0.911 sec/batch; 78h:39m:29s remains)
2017-12-07 13:54:18.759310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3617449 -4.2587895 -4.1903067 -4.16132 -4.1886024 -4.2173762 -4.2363777 -4.2453213 -4.2318645 -4.2155538 -4.2138062 -4.2234221 -4.237905 -4.2540431 -4.2588224][-4.4144206 -4.3079567 -4.2439742 -4.2398763 -4.286664 -4.3257179 -4.3463821 -4.3487554 -4.3233447 -4.2873135 -4.2640676 -4.2619348 -4.2788906 -4.3075233 -4.3292041][-4.4459767 -4.362483 -4.3201733 -4.3490906 -4.4207277 -4.4692931 -4.4841046 -4.4674158 -4.41686 -4.353457 -4.3084645 -4.299839 -4.3244147 -4.3701434 -4.4109168][-4.4572411 -4.4079814 -4.3852148 -4.4278178 -4.5025992 -4.5457768 -4.543273 -4.4975 -4.41735 -4.3328938 -4.279819 -4.2810116 -4.3244648 -4.3892674 -4.4385223][-4.4378495 -4.41714 -4.4071617 -4.4472709 -4.5082393 -4.5272493 -4.4887476 -4.401207 -4.2938743 -4.2061658 -4.1649179 -4.1905217 -4.2609987 -4.3450146 -4.3959188][-4.4128742 -4.4052935 -4.398149 -4.4227076 -4.45314 -4.429863 -4.3439393 -4.2199469 -4.1084232 -4.04831 -4.0419941 -4.1038451 -4.2075233 -4.3067141 -4.3519845][-4.3842373 -4.3604879 -4.3402929 -4.3434134 -4.3465629 -4.290062 -4.17393 -4.0411739 -3.9548833 -3.9438682 -3.9806232 -4.0737796 -4.2034531 -4.3096156 -4.3473577][-4.3218603 -4.2693381 -4.2363815 -4.226058 -4.2142653 -4.1427369 -4.018136 -3.9008417 -3.8596132 -3.9024839 -3.971632 -4.07253 -4.2035093 -4.3095689 -4.3537941][-4.22078 -4.1440868 -4.1132531 -4.1050582 -4.0872169 -4.0150313 -3.9021575 -3.8181748 -3.8253014 -3.9069707 -3.9845769 -4.0665641 -4.1758008 -4.2773261 -4.340477][-4.1086893 -4.0146885 -3.9956417 -3.9989486 -3.9861865 -3.9304478 -3.8432789 -3.7923868 -3.8294406 -3.9220564 -3.9889522 -4.0424 -4.1220937 -4.2126794 -4.2913389][-4.032269 -3.938616 -3.9383109 -3.9631121 -3.9641576 -3.9291437 -3.866066 -3.8325765 -3.8708761 -3.9463644 -3.9883485 -4.0133438 -4.0641289 -4.1378779 -4.2199955][-4.0287542 -3.9560494 -3.9744358 -4.0166054 -4.0266562 -3.9991338 -3.9441977 -3.9139895 -3.9398043 -3.9899173 -4.0026464 -4.0006742 -4.0225739 -4.0741806 -4.1448889][-4.0947113 -4.0511479 -4.0817246 -4.1299763 -4.141 -4.1110053 -4.0587168 -4.0327859 -4.0533795 -4.0845032 -4.0698223 -4.0377955 -4.0286322 -4.0535836 -4.103714][-4.2207689 -4.2071548 -4.2403059 -4.2776108 -4.27801 -4.2397337 -4.1899748 -4.1746206 -4.1986561 -4.2154608 -4.1744204 -4.1122575 -4.0775971 -4.0857139 -4.1208525][-4.3653975 -4.3626556 -4.3796282 -4.3915434 -4.3827071 -4.34718 -4.3060603 -4.2996812 -4.3240151 -4.3285904 -4.2687573 -4.1851473 -4.1377888 -4.142499 -4.1722717]]...]
INFO - root - 2017-12-07 13:54:28.274062: step 21510, loss = 21.54, batch loss = 21.45 (8.2 examples/sec; 0.973 sec/batch; 84h:03m:18s remains)
INFO - root - 2017-12-07 13:54:37.702438: step 21520, loss = 21.44, batch loss = 21.35 (8.7 examples/sec; 0.922 sec/batch; 79h:39m:38s remains)
INFO - root - 2017-12-07 13:54:47.221827: step 21530, loss = 21.45, batch loss = 21.37 (8.5 examples/sec; 0.941 sec/batch; 81h:18m:14s remains)
INFO - root - 2017-12-07 13:54:56.610590: step 21540, loss = 21.38, batch loss = 21.30 (8.5 examples/sec; 0.939 sec/batch; 81h:08m:24s remains)
INFO - root - 2017-12-07 13:55:06.047969: step 21550, loss = 21.14, batch loss = 21.05 (8.3 examples/sec; 0.968 sec/batch; 83h:35m:57s remains)
INFO - root - 2017-12-07 13:55:15.267879: step 21560, loss = 21.61, batch loss = 21.52 (8.2 examples/sec; 0.971 sec/batch; 83h:54m:24s remains)
INFO - root - 2017-12-07 13:55:24.622001: step 21570, loss = 21.61, batch loss = 21.53 (8.9 examples/sec; 0.899 sec/batch; 77h:41m:17s remains)
INFO - root - 2017-12-07 13:55:33.976979: step 21580, loss = 21.80, batch loss = 21.72 (9.3 examples/sec; 0.863 sec/batch; 74h:31m:39s remains)
INFO - root - 2017-12-07 13:55:43.442323: step 21590, loss = 21.24, batch loss = 21.16 (8.9 examples/sec; 0.896 sec/batch; 77h:22m:56s remains)
INFO - root - 2017-12-07 13:55:52.866015: step 21600, loss = 21.33, batch loss = 21.25 (8.7 examples/sec; 0.922 sec/batch; 79h:37m:45s remains)
2017-12-07 13:55:53.821601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.45697 -4.4639521 -4.457489 -4.4292917 -4.3867073 -4.3540449 -4.3544817 -4.4019971 -4.483274 -4.5584874 -4.5836396 -4.5326247 -4.4370356 -4.3495622 -4.2947969][-4.5444832 -4.5559092 -4.5431519 -4.5092945 -4.4552684 -4.3996124 -4.3765712 -4.4140491 -4.5013862 -4.5913267 -4.6240292 -4.5646472 -4.4533396 -4.3526382 -4.2933445][-4.5178537 -4.5218949 -4.4968038 -4.4548244 -4.3906908 -4.3220363 -4.296186 -4.3523669 -4.4738965 -4.599637 -4.6527252 -4.591321 -4.4655147 -4.3514781 -4.2878141][-4.3480392 -4.34091 -4.306004 -4.2635655 -4.2039156 -4.1420422 -4.1389341 -4.2399821 -4.4160366 -4.5883193 -4.6678128 -4.6134477 -4.4816675 -4.3587661 -4.2894068][-4.1283946 -4.1162038 -4.0808029 -4.0493007 -4.00541 -3.9573612 -3.9766142 -4.1153173 -4.3389273 -4.5521274 -4.6578722 -4.6203856 -4.4959445 -4.3731141 -4.29899][-3.9599569 -3.945266 -3.9224219 -3.9077981 -3.874573 -3.8273621 -3.8482823 -3.9975336 -4.2475991 -4.4919395 -4.6253214 -4.6128025 -4.5047927 -4.3888421 -4.3125343][-3.8824308 -3.8657579 -3.8627234 -3.8647261 -3.8295794 -3.7660587 -3.7684226 -3.90636 -4.1638231 -4.4287057 -4.5864606 -4.5962763 -4.5030856 -4.3956518 -4.32126][-3.9146795 -3.8937695 -3.9100761 -3.9218898 -3.8735204 -3.7864592 -3.766412 -3.8838873 -4.1311436 -4.3954406 -4.5558243 -4.5702491 -4.4827352 -4.3840609 -4.3183708][-3.9861419 -3.9682784 -3.9966991 -4.0067825 -3.9389577 -3.8287067 -3.7887352 -3.8906951 -4.1248107 -4.3769164 -4.524694 -4.529531 -4.4423323 -4.3548765 -4.3046508][-4.0381079 -4.022903 -4.0544195 -4.0627089 -3.9884567 -3.8675923 -3.813771 -3.903439 -4.1221619 -4.3558655 -4.4848824 -4.4780474 -4.394278 -4.3216324 -4.287991][-4.101244 -4.0897322 -4.1190152 -4.130095 -4.066082 -3.9473336 -3.8760681 -3.9402907 -4.1317863 -4.3406734 -4.453692 -4.442553 -4.3658438 -4.3036928 -4.2783027][-4.1911478 -4.181613 -4.2085619 -4.2301846 -4.1909866 -4.0887074 -4.0079508 -4.0436196 -4.1960535 -4.3685007 -4.4610653 -4.4450645 -4.3703179 -4.3075261 -4.2785072][-4.3259387 -4.3121128 -4.3346295 -4.3658051 -4.3523684 -4.2781725 -4.2085977 -4.2283745 -4.3401031 -4.4638958 -4.5192785 -4.4821372 -4.3957129 -4.3227568 -4.2838397][-4.4612117 -4.4492273 -4.4668512 -4.4973369 -4.4938397 -4.4411244 -4.3912792 -4.4093571 -4.4912591 -4.5697689 -4.58394 -4.5196848 -4.4189048 -4.3363791 -4.2893939][-4.4811311 -4.4761081 -4.4886813 -4.5114098 -4.50818 -4.4722576 -4.4503236 -4.489161 -4.5709267 -4.6305709 -4.6207738 -4.539176 -4.4306097 -4.3439369 -4.2930875]]...]
INFO - root - 2017-12-07 13:56:03.395835: step 21610, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.974 sec/batch; 84h:06m:20s remains)
INFO - root - 2017-12-07 13:56:12.794105: step 21620, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.973 sec/batch; 84h:02m:45s remains)
INFO - root - 2017-12-07 13:56:22.295759: step 21630, loss = 21.35, batch loss = 21.26 (8.2 examples/sec; 0.980 sec/batch; 84h:36m:23s remains)
INFO - root - 2017-12-07 13:56:31.700888: step 21640, loss = 21.67, batch loss = 21.59 (8.4 examples/sec; 0.949 sec/batch; 81h:56m:14s remains)
INFO - root - 2017-12-07 13:56:41.119961: step 21650, loss = 21.13, batch loss = 21.05 (9.2 examples/sec; 0.867 sec/batch; 74h:53m:01s remains)
INFO - root - 2017-12-07 13:56:50.432145: step 21660, loss = 21.46, batch loss = 21.37 (8.4 examples/sec; 0.948 sec/batch; 81h:52m:50s remains)
INFO - root - 2017-12-07 13:56:59.755701: step 21670, loss = 21.18, batch loss = 21.09 (8.7 examples/sec; 0.918 sec/batch; 79h:13m:25s remains)
INFO - root - 2017-12-07 13:57:09.033134: step 21680, loss = 21.41, batch loss = 21.32 (9.8 examples/sec; 0.817 sec/batch; 70h:34m:45s remains)
INFO - root - 2017-12-07 13:57:18.585523: step 21690, loss = 21.73, batch loss = 21.64 (7.9 examples/sec; 1.017 sec/batch; 87h:46m:12s remains)
INFO - root - 2017-12-07 13:57:28.007211: step 21700, loss = 22.03, batch loss = 21.95 (8.0 examples/sec; 1.001 sec/batch; 86h:27m:42s remains)
2017-12-07 13:57:28.935396: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5035968 -4.4874716 -4.4620337 -4.451992 -4.4407935 -4.4336915 -4.4358687 -4.4460673 -4.4804516 -4.5128841 -4.5405903 -4.5520864 -4.5312157 -4.472281 -4.4050536][-4.4888487 -4.4732351 -4.4456596 -4.4317937 -4.420094 -4.4063 -4.3999338 -4.4080625 -4.4462147 -4.4778876 -4.5065937 -4.5338244 -4.5359125 -4.4877176 -4.4188743][-4.4663906 -4.45175 -4.4236922 -4.4048939 -4.3919597 -4.363637 -4.3345728 -4.3329711 -4.3706112 -4.4053545 -4.4500284 -4.5017138 -4.5305285 -4.4991169 -4.4361463][-4.4217143 -4.4088488 -4.383781 -4.3708177 -4.3655753 -4.3239784 -4.2686172 -4.2492547 -4.2819023 -4.323442 -4.3887119 -4.4663534 -4.5215096 -4.5106268 -4.4579158][-4.3545456 -4.3457761 -4.3261805 -4.317822 -4.3058672 -4.235826 -4.1492138 -4.1175137 -4.1584849 -4.219348 -4.3049631 -4.4087267 -4.4937325 -4.507833 -4.468986][-4.2719154 -4.2711291 -4.2580938 -4.2399755 -4.1931114 -4.0779562 -3.9558816 -3.9215617 -3.988853 -4.0849924 -4.19783 -4.3295159 -4.4446268 -4.4837747 -4.4620738][-4.2074265 -4.2284245 -4.2213392 -4.1802797 -4.0920396 -3.9378796 -3.787643 -3.7550719 -3.8527672 -3.9830828 -4.1231833 -4.278914 -4.4159131 -4.4686351 -4.4565797][-4.2182517 -4.2673125 -4.2670221 -4.2080059 -4.0948124 -3.9190722 -3.7540846 -3.7173188 -3.8224053 -3.9692318 -4.128334 -4.2971182 -4.4370313 -4.4842424 -4.465096][-4.2949581 -4.3651361 -4.3802733 -4.3325281 -4.2290397 -4.0569215 -3.8887305 -3.8325043 -3.9086859 -4.0449886 -4.2082558 -4.3689418 -4.4856381 -4.508996 -4.472949][-4.3762631 -4.4561429 -4.494082 -4.4772868 -4.4001594 -4.2397366 -4.070066 -3.9877455 -4.0272379 -4.1466208 -4.3009224 -4.4359703 -4.515996 -4.5145044 -4.4700465][-4.4227343 -4.4981518 -4.551652 -4.5587592 -4.4948068 -4.3414106 -4.1764612 -4.091383 -4.12019 -4.2339077 -4.3725595 -4.4776311 -4.5260139 -4.5114784 -4.468574][-4.4402246 -4.5014491 -4.5564733 -4.5746 -4.5180836 -4.3788548 -4.2389531 -4.1817989 -4.22877 -4.3406181 -4.4504185 -4.521709 -4.5450969 -4.5224261 -4.4784613][-4.4381776 -4.4907069 -4.5422935 -4.5592961 -4.5096927 -4.4028983 -4.3121629 -4.2965479 -4.3591208 -4.4528923 -4.5237484 -4.5599947 -4.5640144 -4.5351281 -4.4893832][-4.4515362 -4.5031424 -4.5496411 -4.5595064 -4.5182648 -4.4458795 -4.3961883 -4.4050207 -4.4653215 -4.5284286 -4.5636034 -4.5749359 -4.566884 -4.5349393 -4.4898858][-4.4902062 -4.5343471 -4.5766497 -4.5863018 -4.56067 -4.5160465 -4.487175 -4.4966726 -4.5348611 -4.5610847 -4.566885 -4.561934 -4.5476408 -4.516552 -4.475131]]...]
INFO - root - 2017-12-07 13:57:38.260985: step 21710, loss = 21.52, batch loss = 21.44 (8.2 examples/sec; 0.973 sec/batch; 84h:00m:33s remains)
INFO - root - 2017-12-07 13:57:47.640025: step 21720, loss = 21.37, batch loss = 21.29 (8.8 examples/sec; 0.913 sec/batch; 78h:51m:24s remains)
INFO - root - 2017-12-07 13:57:57.034403: step 21730, loss = 21.70, batch loss = 21.61 (8.8 examples/sec; 0.912 sec/batch; 78h:42m:39s remains)
INFO - root - 2017-12-07 13:58:06.227347: step 21740, loss = 21.71, batch loss = 21.63 (9.1 examples/sec; 0.877 sec/batch; 75h:40m:40s remains)
INFO - root - 2017-12-07 13:58:15.738277: step 21750, loss = 21.47, batch loss = 21.39 (8.5 examples/sec; 0.936 sec/batch; 80h:47m:19s remains)
INFO - root - 2017-12-07 13:58:24.955638: step 21760, loss = 21.40, batch loss = 21.32 (9.0 examples/sec; 0.891 sec/batch; 76h:52m:52s remains)
INFO - root - 2017-12-07 13:58:34.264935: step 21770, loss = 21.26, batch loss = 21.18 (8.7 examples/sec; 0.923 sec/batch; 79h:39m:43s remains)
INFO - root - 2017-12-07 13:58:43.768466: step 21780, loss = 21.43, batch loss = 21.34 (8.3 examples/sec; 0.970 sec/batch; 83h:41m:43s remains)
INFO - root - 2017-12-07 13:58:53.094047: step 21790, loss = 21.50, batch loss = 21.41 (8.4 examples/sec; 0.956 sec/batch; 82h:31m:48s remains)
INFO - root - 2017-12-07 13:59:02.721797: step 21800, loss = 21.07, batch loss = 20.99 (8.6 examples/sec; 0.932 sec/batch; 80h:27m:26s remains)
2017-12-07 13:59:03.748928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.473814 -4.4087996 -4.3423381 -4.2859564 -4.259439 -4.2606473 -4.2729297 -4.306251 -4.3582664 -4.3952012 -4.4267883 -4.4108033 -4.3672605 -4.3693066 -4.3890266][-4.4495597 -4.3730621 -4.300662 -4.2413883 -4.2088733 -4.2038569 -4.2184839 -4.2592549 -4.3362346 -4.39103 -4.4348841 -4.4332709 -4.3882723 -4.3955398 -4.4331942][-4.3855433 -4.3183475 -4.2632113 -4.2226877 -4.1977453 -4.1892819 -4.194231 -4.2229495 -4.3046246 -4.3650303 -4.4148183 -4.4365048 -4.4077506 -4.4259777 -4.4767451][-4.2934184 -4.2569036 -4.2338667 -4.2174454 -4.2008638 -4.1856365 -4.1684756 -4.176053 -4.2572222 -4.3281183 -4.3848281 -4.4276466 -4.4207768 -4.4448681 -4.491817][-4.2219768 -4.2221546 -4.2277675 -4.2187052 -4.1969867 -4.1737757 -4.1396103 -4.1354971 -4.2145171 -4.29742 -4.3663759 -4.42298 -4.4329414 -4.449141 -4.4699354][-4.239789 -4.2648864 -4.2813635 -4.2557631 -4.2048259 -4.1581459 -4.1110787 -4.1044664 -4.1758108 -4.2666354 -4.3513474 -4.4220257 -4.4482989 -4.4538503 -4.4402966][-4.3284831 -4.3483782 -4.351614 -4.3007913 -4.2148209 -4.1289139 -4.0570235 -4.0440521 -4.1058779 -4.203722 -4.3083882 -4.3988233 -4.445899 -4.4466434 -4.4082737][-4.4143376 -4.3969822 -4.3621249 -4.2903576 -4.19778 -4.0980439 -4.0133476 -3.9950049 -4.0493145 -4.1473064 -4.2580962 -4.35771 -4.41515 -4.4093809 -4.3588514][-4.4666224 -4.3970695 -4.3087091 -4.2172942 -4.139688 -4.0628324 -3.9973278 -3.9892006 -4.0415297 -4.1282969 -4.2245951 -4.3121133 -4.3613791 -4.3472109 -4.2987175][-4.4466505 -4.3350749 -4.2062097 -4.1058688 -4.0569715 -4.029079 -4.0156722 -4.0398493 -4.096796 -4.16517 -4.2315016 -4.2890291 -4.3166561 -4.297287 -4.2628741][-4.3756304 -4.2422271 -4.1005797 -4.0110011 -3.999521 -4.0286279 -4.0793753 -4.1482849 -4.2164826 -4.2664557 -4.2983079 -4.3186154 -4.3165073 -4.2906566 -4.267612][-4.30938 -4.1891356 -4.0650163 -3.9997523 -4.0181642 -4.0821533 -4.170259 -4.2667937 -4.3434072 -4.381196 -4.3904262 -4.3846498 -4.358839 -4.3264036 -4.3050432][-4.2468705 -4.1734304 -4.1009879 -4.0783291 -4.1204233 -4.192946 -4.2802405 -4.3738842 -4.4462695 -4.4779482 -4.4837961 -4.47375 -4.4400868 -4.4030585 -4.3727422][-4.1886377 -4.1630464 -4.14972 -4.1715689 -4.2347207 -4.3102417 -4.3872576 -4.467031 -4.5266142 -4.5498495 -4.5571547 -4.5516748 -4.5201564 -4.4817 -4.4393296][-4.1537452 -4.1624174 -4.1961675 -4.2513943 -4.3246422 -4.3941159 -4.4554081 -4.5159879 -4.5567818 -4.5669484 -4.57124 -4.5705237 -4.54823 -4.5166039 -4.4746027]]...]
INFO - root - 2017-12-07 13:59:13.068568: step 21810, loss = 21.47, batch loss = 21.39 (7.8 examples/sec; 1.024 sec/batch; 88h:23m:59s remains)
INFO - root - 2017-12-07 13:59:22.404932: step 21820, loss = 21.28, batch loss = 21.20 (7.8 examples/sec; 1.022 sec/batch; 88h:13m:20s remains)
INFO - root - 2017-12-07 13:59:31.958189: step 21830, loss = 21.29, batch loss = 21.21 (8.4 examples/sec; 0.954 sec/batch; 82h:21m:36s remains)
INFO - root - 2017-12-07 13:59:41.371776: step 21840, loss = 21.55, batch loss = 21.47 (8.7 examples/sec; 0.924 sec/batch; 79h:42m:04s remains)
INFO - root - 2017-12-07 13:59:50.804802: step 21850, loss = 21.48, batch loss = 21.40 (8.4 examples/sec; 0.950 sec/batch; 82h:00m:49s remains)
INFO - root - 2017-12-07 14:00:00.205859: step 21860, loss = 21.34, batch loss = 21.26 (8.4 examples/sec; 0.953 sec/batch; 82h:15m:59s remains)
INFO - root - 2017-12-07 14:00:09.508697: step 21870, loss = 21.28, batch loss = 21.20 (8.9 examples/sec; 0.897 sec/batch; 77h:25m:46s remains)
INFO - root - 2017-12-07 14:00:18.880779: step 21880, loss = 21.94, batch loss = 21.86 (9.1 examples/sec; 0.884 sec/batch; 76h:14m:05s remains)
INFO - root - 2017-12-07 14:00:28.457004: step 21890, loss = 21.22, batch loss = 21.14 (8.9 examples/sec; 0.902 sec/batch; 77h:51m:39s remains)
INFO - root - 2017-12-07 14:00:37.949277: step 21900, loss = 21.51, batch loss = 21.42 (8.2 examples/sec; 0.973 sec/batch; 83h:57m:54s remains)
2017-12-07 14:00:38.965900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5726604 -4.622242 -4.641253 -4.629952 -4.5953951 -4.5428476 -4.4837427 -4.4540672 -4.464292 -4.5168829 -4.53659 -4.46484 -4.32121 -4.1686711 -4.1257167][-4.5473537 -4.5727234 -4.5673065 -4.548418 -4.5353465 -4.5188365 -4.4943466 -4.475194 -4.462337 -4.4771452 -4.4794445 -4.43329 -4.366178 -4.3061624 -4.3090463][-4.4969182 -4.5085278 -4.5027313 -4.5054855 -4.5347662 -4.5587406 -4.5492544 -4.5071268 -4.4503331 -4.4286189 -4.4333415 -4.4455557 -4.4780912 -4.5140786 -4.5554948][-4.3552508 -4.378315 -4.4084711 -4.4576516 -4.5361185 -4.59497 -4.590045 -4.5133977 -4.4112573 -4.3669019 -4.4015069 -4.4914889 -4.6033354 -4.6855297 -4.7265644][-4.1361575 -4.1694341 -4.2394466 -4.3268857 -4.4370704 -4.514565 -4.5122166 -4.41478 -4.2943063 -4.26234 -4.3541751 -4.5165629 -4.6609387 -4.7210073 -4.7137108][-3.9076419 -3.9363024 -4.0190487 -4.1122618 -4.2205992 -4.2936077 -4.2859554 -4.1878414 -4.0839086 -4.0963678 -4.2589526 -4.4788733 -4.6185846 -4.6196423 -4.547236][-3.7816052 -3.7917867 -3.8504922 -3.9101074 -3.9796486 -4.0242548 -3.9973559 -3.9071481 -3.8465858 -3.9204221 -4.1463013 -4.4020514 -4.5222831 -4.4720745 -4.3659549][-3.7860451 -3.7704604 -3.7959628 -3.823503 -3.8563497 -3.8784323 -3.83859 -3.7596688 -3.7449939 -3.8644855 -4.1116371 -4.3610692 -4.4515085 -4.3781734 -4.2884569][-3.8564925 -3.8186059 -3.82576 -3.8554664 -3.8906412 -3.9183865 -3.8906488 -3.8304317 -3.8414 -3.9709053 -4.189815 -4.3920822 -4.4487534 -4.3770485 -4.3306065][-3.9826448 -3.9221723 -3.9156961 -3.9513919 -3.9991221 -4.0432296 -4.0489187 -4.013483 -4.0252795 -4.1274309 -4.2892833 -4.432601 -4.4680152 -4.4229307 -4.422492][-4.1522107 -4.0795803 -4.0624185 -4.0883183 -4.1306052 -4.1775618 -4.2114568 -4.198854 -4.1961732 -4.2569222 -4.3652778 -4.4610391 -4.4867353 -4.4769087 -4.5086946][-4.3552151 -4.3067102 -4.2932239 -4.2939672 -4.3045325 -4.3269434 -4.3571763 -4.3443708 -4.31557 -4.3341494 -4.4035153 -4.4690967 -4.4904637 -4.5029817 -4.5463119][-4.5275826 -4.5112567 -4.5124912 -4.5000582 -4.4801564 -4.4662032 -4.4662275 -4.4307151 -4.3669591 -4.3462491 -4.3929076 -4.4446864 -4.4640923 -4.4837351 -4.5277858][-4.5742245 -4.5809264 -4.6066966 -4.6073236 -4.5806971 -4.543582 -4.5126977 -4.4477191 -4.3510761 -4.3042269 -4.3487644 -4.4058533 -4.42752 -4.44338 -4.4825315][-4.5305481 -4.5538554 -4.6069751 -4.6332555 -4.6145911 -4.5700321 -4.5201941 -4.4330387 -4.3125944 -4.2486253 -4.2952452 -4.364284 -4.3898983 -4.4017849 -4.4401445]]...]
INFO - root - 2017-12-07 14:00:48.254474: step 21910, loss = 21.76, batch loss = 21.68 (8.5 examples/sec; 0.938 sec/batch; 80h:57m:26s remains)
INFO - root - 2017-12-07 14:00:57.730651: step 21920, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.955 sec/batch; 82h:23m:16s remains)
INFO - root - 2017-12-07 14:01:07.169102: step 21930, loss = 21.31, batch loss = 21.23 (8.1 examples/sec; 0.986 sec/batch; 85h:01m:45s remains)
INFO - root - 2017-12-07 14:01:16.643103: step 21940, loss = 21.61, batch loss = 21.53 (8.0 examples/sec; 1.005 sec/batch; 86h:42m:14s remains)
INFO - root - 2017-12-07 14:01:26.067350: step 21950, loss = 21.59, batch loss = 21.50 (8.4 examples/sec; 0.951 sec/batch; 82h:01m:16s remains)
INFO - root - 2017-12-07 14:01:35.531010: step 21960, loss = 21.68, batch loss = 21.60 (8.3 examples/sec; 0.959 sec/batch; 82h:43m:30s remains)
INFO - root - 2017-12-07 14:01:44.823839: step 21970, loss = 21.68, batch loss = 21.60 (8.8 examples/sec; 0.909 sec/batch; 78h:24m:13s remains)
INFO - root - 2017-12-07 14:01:54.113947: step 21980, loss = 21.50, batch loss = 21.41 (8.2 examples/sec; 0.972 sec/batch; 83h:52m:31s remains)
INFO - root - 2017-12-07 14:02:03.606982: step 21990, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.948 sec/batch; 81h:47m:36s remains)
INFO - root - 2017-12-07 14:02:13.053261: step 22000, loss = 21.44, batch loss = 21.36 (8.8 examples/sec; 0.910 sec/batch; 78h:31m:33s remains)
2017-12-07 14:02:14.041681: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5464535 -4.6488829 -4.6814933 -4.66481 -4.6358504 -4.606286 -4.584621 -4.5942411 -4.6467209 -4.7119417 -4.7611609 -4.8027725 -4.843781 -4.8631325 -4.8476434][-4.5904789 -4.6921015 -4.7140188 -4.6956348 -4.6719222 -4.6330357 -4.5862608 -4.5890627 -4.6698079 -4.7794676 -4.867537 -4.9346237 -4.9946623 -5.0105147 -4.9653654][-4.6191297 -4.7227736 -4.7431483 -4.7299194 -4.7093525 -4.6469259 -4.5532336 -4.5269 -4.6184797 -4.7595487 -4.8793049 -4.9710493 -5.0483627 -5.0520492 -4.9606791][-4.64879 -4.7584834 -4.7753859 -4.7513843 -4.705873 -4.5986247 -4.4536462 -4.3957472 -4.492075 -4.6564584 -4.8050747 -4.9248352 -5.0205412 -5.0037022 -4.8630986][-4.6844172 -4.789155 -4.7834578 -4.7159491 -4.6051922 -4.4320755 -4.2386947 -4.1612053 -4.2691736 -4.4606223 -4.6506119 -4.8122458 -4.9296269 -4.9001408 -4.7362094][-4.7124147 -4.79499 -4.7442136 -4.6099997 -4.4169073 -4.172472 -3.9437978 -3.8681951 -3.9951057 -4.2166476 -4.4493651 -4.6482725 -4.7750282 -4.7396965 -4.5925665][-4.7234225 -4.781806 -4.6896095 -4.4972253 -4.234777 -3.932303 -3.6810877 -3.6128178 -3.75857 -4.0126867 -4.2818522 -4.4996319 -4.6159263 -4.5768371 -4.4758348][-4.7257881 -4.7805877 -4.6786218 -4.4644637 -4.1671805 -3.8324933 -3.5638552 -3.4785361 -3.6094294 -3.8683567 -4.1485758 -4.3577151 -4.4563103 -4.4229603 -4.3634567][-4.7295475 -4.8135571 -4.7470341 -4.5559049 -4.2624865 -3.9235396 -3.6544478 -3.5482745 -3.6380348 -3.8558829 -4.0963078 -4.2617192 -4.3339996 -4.3026495 -4.2543612][-4.7302728 -4.8597431 -4.8565674 -4.7152309 -4.4462585 -4.118854 -3.8682408 -3.7626214 -3.8141639 -3.9568012 -4.1184449 -4.2303491 -4.2869949 -4.2680674 -4.2277107][-4.7035246 -4.8648763 -4.9156485 -4.8283348 -4.598845 -4.3006406 -4.0808916 -3.9895601 -4.0156732 -4.0847549 -4.1636453 -4.2339349 -4.2946181 -4.31469 -4.3079338][-4.6430888 -4.8096771 -4.8934031 -4.8636041 -4.7035728 -4.467236 -4.2972746 -4.2421246 -4.2792249 -4.3189659 -4.3417578 -4.3740268 -4.4260511 -4.4665575 -4.4792213][-4.560153 -4.7117882 -4.8113289 -4.8379188 -4.7681823 -4.6219969 -4.5117092 -4.4942989 -4.5469241 -4.5806322 -4.5796733 -4.584765 -4.6114583 -4.6400933 -4.648066][-4.46524 -4.5907264 -4.6917953 -4.7571721 -4.7640557 -4.7064905 -4.6469836 -4.6404171 -4.6799436 -4.703948 -4.6987972 -4.6936626 -4.6978283 -4.7074528 -4.7061834][-4.3802276 -4.4711409 -4.5587535 -4.6331463 -4.6781287 -4.6791263 -4.6580896 -4.6523342 -4.6706491 -4.6844072 -4.6803389 -4.6701875 -4.6571012 -4.6449823 -4.6285882]]...]
INFO - root - 2017-12-07 14:02:23.440947: step 22010, loss = 21.19, batch loss = 21.11 (8.5 examples/sec; 0.940 sec/batch; 81h:05m:42s remains)
INFO - root - 2017-12-07 14:02:32.843548: step 22020, loss = 21.24, batch loss = 21.16 (8.4 examples/sec; 0.957 sec/batch; 82h:33m:55s remains)
INFO - root - 2017-12-07 14:02:42.223295: step 22030, loss = 21.02, batch loss = 20.94 (8.6 examples/sec; 0.926 sec/batch; 79h:53m:35s remains)
INFO - root - 2017-12-07 14:02:51.643309: step 22040, loss = 21.16, batch loss = 21.08 (8.8 examples/sec; 0.905 sec/batch; 78h:00m:33s remains)
INFO - root - 2017-12-07 14:03:01.031310: step 22050, loss = 21.49, batch loss = 21.41 (8.6 examples/sec; 0.925 sec/batch; 79h:47m:28s remains)
INFO - root - 2017-12-07 14:03:10.640231: step 22060, loss = 21.80, batch loss = 21.72 (7.9 examples/sec; 1.011 sec/batch; 87h:08m:47s remains)
INFO - root - 2017-12-07 14:03:19.934053: step 22070, loss = 21.24, batch loss = 21.15 (8.2 examples/sec; 0.974 sec/batch; 84h:00m:11s remains)
INFO - root - 2017-12-07 14:03:29.180445: step 22080, loss = 21.31, batch loss = 21.23 (8.4 examples/sec; 0.951 sec/batch; 81h:58m:37s remains)
INFO - root - 2017-12-07 14:03:38.512153: step 22090, loss = 21.45, batch loss = 21.37 (8.8 examples/sec; 0.912 sec/batch; 78h:40m:02s remains)
INFO - root - 2017-12-07 14:03:47.898270: step 22100, loss = 21.99, batch loss = 21.91 (8.1 examples/sec; 0.986 sec/batch; 85h:02m:43s remains)
2017-12-07 14:03:48.812452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5951371 -4.6101656 -4.6145935 -4.5923429 -4.5491943 -4.5308104 -4.5362329 -4.5395179 -4.548707 -4.5601926 -4.5505986 -4.5034795 -4.4469614 -4.4199915 -4.4153][-4.5314116 -4.5654674 -4.575202 -4.5593476 -4.5223494 -4.5042872 -4.4991846 -4.4880371 -4.4948769 -4.5218329 -4.5368109 -4.5092449 -4.4575696 -4.4256949 -4.41825][-4.3901415 -4.462326 -4.4906116 -4.496902 -4.4730086 -4.4423094 -4.4121866 -4.38367 -4.3868966 -4.426681 -4.4751916 -4.4957476 -4.4780946 -4.4547114 -4.4374738][-4.223618 -4.3468218 -4.411068 -4.4443259 -4.4218 -4.3527083 -4.2739267 -4.2331247 -4.2556691 -4.3209867 -4.403646 -4.4722762 -4.4960246 -4.4863329 -4.4574671][-4.0774412 -4.2360964 -4.3339691 -4.38678 -4.3592873 -4.2483377 -4.1179652 -4.065536 -4.1243296 -4.2385874 -4.3577456 -4.4562063 -4.5061903 -4.5047574 -4.4690032][-3.998528 -4.1610365 -4.2757497 -4.3311315 -4.2948151 -4.1548066 -3.97652 -3.8995602 -3.9877303 -4.1615505 -4.323823 -4.4391418 -4.5005641 -4.5050526 -4.4698915][-4.0014582 -4.1538324 -4.2771354 -4.3274255 -4.2746968 -4.111021 -3.894259 -3.7780516 -3.8665602 -4.0799642 -4.2804985 -4.4129052 -4.4823809 -4.4942932 -4.4634347][-4.0673532 -4.1994987 -4.3255205 -4.3711057 -4.3037081 -4.1278481 -3.892597 -3.7473888 -3.8180366 -4.0343189 -4.2423191 -4.3828015 -4.4594355 -4.4801955 -4.4561257][-4.1920767 -4.2914424 -4.3985391 -4.4364877 -4.3778062 -4.2316322 -4.0185642 -3.8652878 -3.9126828 -4.0982747 -4.273232 -4.3886681 -4.453022 -4.4729495 -4.4529233][-4.3210554 -4.3902483 -4.465632 -4.4949322 -4.4656262 -4.3792677 -4.2199645 -4.0862546 -4.1203423 -4.2712369 -4.3995295 -4.4689941 -4.4985895 -4.4973564 -4.4649863][-4.4203386 -4.4609628 -4.5032473 -4.5263538 -4.5284505 -4.4977021 -4.3932476 -4.2964697 -4.3347211 -4.4641123 -4.55867 -4.5897608 -4.5840764 -4.5496135 -4.4873719][-4.5049825 -4.5239911 -4.5369716 -4.5500603 -4.567183 -4.5676894 -4.504982 -4.4442134 -4.485064 -4.5929518 -4.6624703 -4.6709242 -4.6424317 -4.5819292 -4.4942846][-4.5637875 -4.568861 -4.56595 -4.5702753 -4.582943 -4.5883327 -4.5541134 -4.5209794 -4.5519919 -4.62486 -4.6675377 -4.6626806 -4.6260061 -4.5577579 -4.4683352][-4.5539274 -4.5604172 -4.5635085 -4.5672765 -4.5676289 -4.5631366 -4.5435925 -4.5271659 -4.5409083 -4.5753951 -4.5929914 -4.5833979 -4.552125 -4.4955621 -4.4257703][-4.4855785 -4.5043154 -4.523047 -4.5325747 -4.5266638 -4.5125422 -4.4958282 -4.4830918 -4.4823594 -4.4934745 -4.4987903 -4.4913621 -4.47073 -4.4332328 -4.3887768]]...]
INFO - root - 2017-12-07 14:03:58.206707: step 22110, loss = 21.10, batch loss = 21.02 (8.7 examples/sec; 0.923 sec/batch; 79h:35m:50s remains)
INFO - root - 2017-12-07 14:04:07.544414: step 22120, loss = 21.24, batch loss = 21.15 (8.6 examples/sec; 0.925 sec/batch; 79h:47m:05s remains)
INFO - root - 2017-12-07 14:04:16.951763: step 22130, loss = 21.25, batch loss = 21.17 (8.4 examples/sec; 0.952 sec/batch; 82h:02m:29s remains)
INFO - root - 2017-12-07 14:04:26.347317: step 22140, loss = 21.33, batch loss = 21.25 (8.4 examples/sec; 0.950 sec/batch; 81h:52m:29s remains)
INFO - root - 2017-12-07 14:04:35.796264: step 22150, loss = 21.54, batch loss = 21.45 (8.4 examples/sec; 0.947 sec/batch; 81h:38m:08s remains)
INFO - root - 2017-12-07 14:04:45.176624: step 22160, loss = 21.55, batch loss = 21.47 (8.7 examples/sec; 0.921 sec/batch; 79h:22m:26s remains)
INFO - root - 2017-12-07 14:04:54.365353: step 22170, loss = 21.43, batch loss = 21.35 (9.3 examples/sec; 0.861 sec/batch; 74h:11m:00s remains)
INFO - root - 2017-12-07 14:05:03.753928: step 22180, loss = 21.67, batch loss = 21.59 (8.4 examples/sec; 0.948 sec/batch; 81h:41m:20s remains)
INFO - root - 2017-12-07 14:05:13.152386: step 22190, loss = 21.16, batch loss = 21.07 (8.6 examples/sec; 0.930 sec/batch; 80h:11m:31s remains)
INFO - root - 2017-12-07 14:05:22.442779: step 22200, loss = 21.25, batch loss = 21.17 (8.7 examples/sec; 0.920 sec/batch; 79h:20m:19s remains)
2017-12-07 14:05:23.385838: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4387226 -4.3258634 -4.2418561 -4.2298808 -4.2893686 -4.3669629 -4.4284964 -4.46807 -4.4694457 -4.4308343 -4.3621535 -4.2833295 -4.2276535 -4.2137442 -4.247694][-4.373 -4.2644081 -4.181489 -4.1764922 -4.24446 -4.3226528 -4.3831477 -4.4393387 -4.4771748 -4.4738793 -4.4251914 -4.3495717 -4.2955427 -4.283587 -4.3100853][-4.2387948 -4.1548748 -4.1004934 -4.1321406 -4.2356939 -4.3345647 -4.4004049 -4.4664412 -4.5302486 -4.5562935 -4.5268893 -4.4642038 -4.418066 -4.408474 -4.4259896][-4.1178765 -4.0505195 -4.0256653 -4.1026917 -4.2499986 -4.3705888 -4.4298091 -4.4794722 -4.5446258 -4.5914006 -4.5905519 -4.555604 -4.5205755 -4.5053716 -4.510417][-4.0895653 -4.0239096 -4.0186467 -4.1254272 -4.2921457 -4.4036007 -4.4246268 -4.4288306 -4.4747891 -4.5371222 -4.5742931 -4.5788145 -4.5570736 -4.5277743 -4.5139704][-4.1753654 -4.109324 -4.1078968 -4.2106342 -4.3504329 -4.4127541 -4.3725038 -4.3237734 -4.3508239 -4.4308238 -4.5117774 -4.5618272 -4.556427 -4.5087256 -4.463171][-4.3274355 -4.2687416 -4.2521644 -4.3099194 -4.3780832 -4.3649993 -4.2596855 -4.1686792 -4.1905208 -4.2962627 -4.4269915 -4.5300331 -4.550612 -4.4923992 -4.4134617][-4.4787755 -4.42888 -4.3860331 -4.3754721 -4.349658 -4.2473421 -4.0772753 -3.9535682 -3.9834018 -4.1258759 -4.3102508 -4.4702439 -4.5301194 -4.483274 -4.3918042][-4.579968 -4.5403495 -4.4766831 -4.4050927 -4.293056 -4.1069932 -3.8780637 -3.7303593 -3.770699 -3.9464755 -4.1706662 -4.3738918 -4.4761391 -4.4618678 -4.3861766][-4.6221395 -4.6040378 -4.5406251 -4.4354167 -4.2671757 -4.0263848 -3.7673984 -3.6112554 -3.6528254 -3.8341517 -4.0614014 -4.2750912 -4.4055367 -4.4292355 -4.3804955][-4.6141496 -4.6214175 -4.5725932 -4.4627361 -4.2825613 -4.04391 -3.8105683 -3.6776094 -3.7135172 -3.8652637 -4.0501537 -4.2318945 -4.3633428 -4.4109192 -4.377008][-4.5684819 -4.5982838 -4.56913 -4.4729333 -4.3150048 -4.1244459 -3.9589987 -3.8752861 -3.9127271 -4.0253925 -4.1502013 -4.2742877 -4.376689 -4.4202619 -4.3793178][-4.5443358 -4.588562 -4.5732388 -4.4918461 -4.3626776 -4.228653 -4.1325421 -4.0971274 -4.1377373 -4.2178698 -4.2926745 -4.3610215 -4.4181919 -4.4329615 -4.3731995][-4.5839968 -4.6357479 -4.6247897 -4.5525417 -4.44741 -4.3560791 -4.3059263 -4.2973919 -4.3285685 -4.3742065 -4.4121366 -4.4438148 -4.4624043 -4.4453387 -4.3713174][-4.6564407 -4.703239 -4.6906276 -4.6321216 -4.5556221 -4.4955373 -4.4660454 -4.4583359 -4.4662809 -4.4786582 -4.4945369 -4.5129251 -4.5149446 -4.4797211 -4.4023228]]...]
INFO - root - 2017-12-07 14:05:32.825689: step 22210, loss = 21.56, batch loss = 21.47 (8.5 examples/sec; 0.945 sec/batch; 81h:25m:24s remains)
INFO - root - 2017-12-07 14:05:42.132645: step 22220, loss = 21.34, batch loss = 21.25 (8.9 examples/sec; 0.900 sec/batch; 77h:34m:46s remains)
INFO - root - 2017-12-07 14:05:51.572756: step 22230, loss = 21.63, batch loss = 21.55 (8.8 examples/sec; 0.910 sec/batch; 78h:26m:00s remains)
INFO - root - 2017-12-07 14:06:00.990914: step 22240, loss = 20.92, batch loss = 20.84 (8.8 examples/sec; 0.912 sec/batch; 78h:35m:42s remains)
INFO - root - 2017-12-07 14:06:10.265461: step 22250, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.945 sec/batch; 81h:28m:22s remains)
INFO - root - 2017-12-07 14:06:19.734261: step 22260, loss = 21.39, batch loss = 21.30 (7.8 examples/sec; 1.026 sec/batch; 88h:27m:35s remains)
INFO - root - 2017-12-07 14:06:29.112144: step 22270, loss = 21.09, batch loss = 21.00 (8.2 examples/sec; 0.977 sec/batch; 84h:11m:42s remains)
INFO - root - 2017-12-07 14:06:38.459650: step 22280, loss = 21.44, batch loss = 21.36 (8.5 examples/sec; 0.944 sec/batch; 81h:23m:19s remains)
INFO - root - 2017-12-07 14:06:47.838453: step 22290, loss = 21.42, batch loss = 21.34 (9.1 examples/sec; 0.875 sec/batch; 75h:21m:52s remains)
INFO - root - 2017-12-07 14:06:57.201785: step 22300, loss = 21.49, batch loss = 21.40 (8.3 examples/sec; 0.963 sec/batch; 82h:56m:47s remains)
2017-12-07 14:06:58.168497: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.564785 -4.5633492 -4.5601044 -4.581862 -4.6306782 -4.6831288 -4.6870289 -4.6288686 -4.5561981 -4.5318923 -4.5722475 -4.6428523 -4.7013607 -4.7261543 -4.7004261][-4.5867882 -4.5981965 -4.6076846 -4.6355257 -4.6714416 -4.6829715 -4.6291666 -4.5221033 -4.4365754 -4.428597 -4.49598 -4.5929656 -4.6755672 -4.7187872 -4.6981149][-4.5595651 -4.5880623 -4.6202726 -4.65385 -4.6565614 -4.5955877 -4.4706078 -4.3351564 -4.275105 -4.3104167 -4.4063559 -4.5177283 -4.6169715 -4.6775079 -4.666594][-4.5099659 -4.5619946 -4.6250949 -4.6584039 -4.6042871 -4.447031 -4.2483444 -4.1045055 -4.0937319 -4.1822443 -4.2949448 -4.4015794 -4.5047221 -4.5778728 -4.5845551][-4.4541578 -4.5289044 -4.6147327 -4.6301742 -4.5038567 -4.2568889 -4.0078259 -3.8835621 -3.9421697 -4.0894732 -4.2119913 -4.3030052 -4.3934512 -4.4643779 -4.48418][-4.4138155 -4.5012922 -4.591537 -4.5713139 -4.3699059 -4.0500226 -3.7700379 -3.6779652 -3.815001 -4.0258255 -4.16404 -4.2415385 -4.3122687 -4.3723788 -4.3998818][-4.4091382 -4.5021472 -4.583334 -4.5270061 -4.2681713 -3.8928795 -3.5755272 -3.4873142 -3.6695957 -3.9220033 -4.0737271 -4.146944 -4.2075491 -4.2657566 -4.3129268][-4.4310975 -4.5206194 -4.5935426 -4.5243216 -4.2529268 -3.8639965 -3.5307729 -3.4267011 -3.6019659 -3.8466868 -4.0013995 -4.0839491 -4.1490641 -4.2089572 -4.2648764][-4.4575791 -4.54251 -4.6142297 -4.5630121 -4.3308 -3.9836984 -3.6743484 -3.5544162 -3.6784577 -3.8708081 -4.009 -4.1051836 -4.1871481 -4.24704 -4.2887969][-4.4852743 -4.5600886 -4.628932 -4.6060247 -4.4422946 -4.176333 -3.9210887 -3.7900162 -3.843725 -3.9606261 -4.0662794 -4.1756277 -4.2867208 -4.3553228 -4.37604][-4.5125618 -4.5670953 -4.6205692 -4.6190681 -4.5327382 -4.3721209 -4.1932154 -4.0680914 -4.0638056 -4.1179161 -4.1976819 -4.3171258 -4.4568319 -4.5333033 -4.52414][-4.5173116 -4.54355 -4.5689321 -4.5772266 -4.562623 -4.5101652 -4.4160929 -4.3148403 -4.2806311 -4.2992935 -4.3590126 -4.4719319 -4.6169634 -4.6901298 -4.6561193][-4.4817452 -4.483839 -4.4866114 -4.5025373 -4.5444322 -4.5783124 -4.5564909 -4.4846644 -4.4396019 -4.4359174 -4.4702544 -4.55562 -4.6797957 -4.7441907 -4.7078567][-4.4126005 -4.4026232 -4.3977442 -4.4242854 -4.4987836 -4.5838532 -4.6150312 -4.5816112 -4.5481396 -4.5394115 -4.5527711 -4.5997391 -4.679584 -4.7235122 -4.6932116][-4.3211432 -4.3059287 -4.302948 -4.3378682 -4.4208713 -4.5199981 -4.5794411 -4.5839968 -4.5798612 -4.584249 -4.5904994 -4.6061921 -4.6394634 -4.6554637 -4.630147]]...]
INFO - root - 2017-12-07 14:07:07.551626: step 22310, loss = 21.60, batch loss = 21.51 (8.6 examples/sec; 0.926 sec/batch; 79h:47m:50s remains)
INFO - root - 2017-12-07 14:07:16.951382: step 22320, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.913 sec/batch; 78h:37m:49s remains)
INFO - root - 2017-12-07 14:07:26.417224: step 22330, loss = 21.62, batch loss = 21.53 (8.5 examples/sec; 0.945 sec/batch; 81h:27m:04s remains)
INFO - root - 2017-12-07 14:07:35.894017: step 22340, loss = 21.65, batch loss = 21.56 (8.1 examples/sec; 0.984 sec/batch; 84h:45m:33s remains)
INFO - root - 2017-12-07 14:07:45.384774: step 22350, loss = 21.40, batch loss = 21.32 (8.8 examples/sec; 0.911 sec/batch; 78h:29m:06s remains)
INFO - root - 2017-12-07 14:07:54.713780: step 22360, loss = 21.78, batch loss = 21.70 (8.6 examples/sec; 0.926 sec/batch; 79h:44m:31s remains)
INFO - root - 2017-12-07 14:08:04.135877: step 22370, loss = 21.59, batch loss = 21.50 (9.0 examples/sec; 0.890 sec/batch; 76h:38m:04s remains)
INFO - root - 2017-12-07 14:08:13.467292: step 22380, loss = 21.07, batch loss = 20.98 (8.2 examples/sec; 0.972 sec/batch; 83h:45m:05s remains)
INFO - root - 2017-12-07 14:08:23.028353: step 22390, loss = 21.44, batch loss = 21.36 (8.3 examples/sec; 0.970 sec/batch; 83h:31m:18s remains)
INFO - root - 2017-12-07 14:08:32.385019: step 22400, loss = 21.39, batch loss = 21.31 (7.8 examples/sec; 1.031 sec/batch; 88h:48m:26s remains)
2017-12-07 14:08:33.297089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.389627 -4.421206 -4.46192 -4.4969463 -4.5191045 -4.5374765 -4.5523267 -4.5486045 -4.5295086 -4.5082736 -4.5017905 -4.5049376 -4.5103064 -4.5159802 -4.507153][-4.4356956 -4.4901419 -4.5414386 -4.5720272 -4.5767131 -4.5700808 -4.5532923 -4.5161519 -4.4713082 -4.4388494 -4.4317 -4.4337282 -4.4460287 -4.4714508 -4.4779716][-4.5110707 -4.557487 -4.5815072 -4.5752649 -4.5406094 -4.4919868 -4.43131 -4.3650117 -4.316678 -4.3078413 -4.331676 -4.3453918 -4.3673897 -4.4120431 -4.4347467][-4.5677457 -4.5640073 -4.530508 -4.4767861 -4.404799 -4.3182263 -4.2284856 -4.1606445 -4.1374106 -4.1711483 -4.2336106 -4.2625484 -4.2984986 -4.361104 -4.4020123][-4.5453315 -4.4771762 -4.3883219 -4.2993493 -4.2082067 -4.1050773 -4.018373 -3.9812646 -3.9993341 -4.0691109 -4.1508255 -4.1875148 -4.2389507 -4.321816 -4.3866177][-4.4357104 -4.3230205 -4.1964488 -4.0875983 -3.9962823 -3.9041166 -3.8581865 -3.8781204 -3.9429688 -4.0350609 -4.1112928 -4.1433611 -4.2054081 -4.3033428 -4.3865442][-4.26621 -4.1447349 -4.0065246 -3.9011321 -3.841188 -3.7909291 -3.8040724 -3.8818994 -3.976316 -4.0729194 -4.1344242 -4.1603355 -4.2266622 -4.3273082 -4.4112968][-4.0821085 -3.9847975 -3.8769686 -3.8235326 -3.8331323 -3.8378057 -3.8878987 -3.9832072 -4.0779247 -4.1605606 -4.2045546 -4.2292919 -4.2942038 -4.3811669 -4.4491014][-3.9459929 -3.8991206 -3.8638322 -3.8906775 -3.9627266 -3.9950776 -4.0392141 -4.1165676 -4.1921053 -4.2485971 -4.2747259 -4.3037581 -4.3668137 -4.4356771 -4.4874516][-3.8954797 -3.9046185 -3.9385 -4.0182018 -4.1071773 -4.1395817 -4.1674228 -4.2201762 -4.2794824 -4.3157043 -4.3283315 -4.3578877 -4.4148965 -4.470459 -4.5195923][-3.9231694 -3.9612641 -4.0169926 -4.1017175 -4.1731625 -4.2078609 -4.2424417 -4.2914023 -4.3462877 -4.37576 -4.3781815 -4.3941431 -4.43852 -4.4940891 -4.5545478][-4.0023832 -4.0363693 -4.0732865 -4.1309328 -4.1777129 -4.2254257 -4.2890143 -4.3539724 -4.4101481 -4.4383445 -4.4304671 -4.4257631 -4.4573641 -4.5210943 -4.5906687][-4.0972633 -4.1152396 -4.1262789 -4.15553 -4.189898 -4.2530165 -4.338881 -4.4127936 -4.4634762 -4.4922915 -4.4842963 -4.4668674 -4.4906793 -4.5547223 -4.6142969][-4.1646109 -4.166996 -4.1659074 -4.1898551 -4.2315688 -4.3013048 -4.3805275 -4.4345193 -4.475153 -4.5172143 -4.5332885 -4.5230031 -4.5404153 -4.5859957 -4.6137571][-4.2005329 -4.1892815 -4.1863093 -4.2259693 -4.2835112 -4.3524919 -4.4117866 -4.4408855 -4.4731846 -4.5328732 -4.5752516 -4.5776377 -4.5819659 -4.5942307 -4.5852928]]...]
INFO - root - 2017-12-07 14:08:42.743007: step 22410, loss = 21.46, batch loss = 21.38 (8.5 examples/sec; 0.945 sec/batch; 81h:22m:56s remains)
INFO - root - 2017-12-07 14:08:52.309097: step 22420, loss = 21.81, batch loss = 21.73 (8.8 examples/sec; 0.906 sec/batch; 77h:59m:58s remains)
INFO - root - 2017-12-07 14:09:01.687280: step 22430, loss = 20.72, batch loss = 20.64 (8.2 examples/sec; 0.975 sec/batch; 83h:56m:36s remains)
INFO - root - 2017-12-07 14:09:11.129933: step 22440, loss = 21.60, batch loss = 21.52 (8.2 examples/sec; 0.972 sec/batch; 83h:43m:27s remains)
INFO - root - 2017-12-07 14:09:20.459108: step 22450, loss = 21.43, batch loss = 21.35 (8.7 examples/sec; 0.920 sec/batch; 79h:13m:13s remains)
INFO - root - 2017-12-07 14:09:29.936130: step 22460, loss = 21.48, batch loss = 21.40 (8.6 examples/sec; 0.933 sec/batch; 80h:22m:01s remains)
INFO - root - 2017-12-07 14:09:39.447212: step 22470, loss = 21.70, batch loss = 21.62 (8.9 examples/sec; 0.898 sec/batch; 77h:21m:23s remains)
INFO - root - 2017-12-07 14:09:48.665889: step 22480, loss = 21.94, batch loss = 21.85 (8.4 examples/sec; 0.947 sec/batch; 81h:32m:40s remains)
INFO - root - 2017-12-07 14:09:57.966376: step 22490, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.942 sec/batch; 81h:08m:52s remains)
INFO - root - 2017-12-07 14:10:07.299523: step 22500, loss = 21.31, batch loss = 21.23 (8.6 examples/sec; 0.928 sec/batch; 79h:55m:10s remains)
2017-12-07 14:10:08.272962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.35561 -4.3504863 -4.3494163 -4.3551807 -4.371573 -4.3911896 -4.40942 -4.4322195 -4.459815 -4.4754448 -4.460825 -4.4269528 -4.4041438 -4.4029803 -4.3930874][-4.3119097 -4.3095455 -4.3092637 -4.3104267 -4.3217659 -4.3446136 -4.3731766 -4.4074678 -4.4436126 -4.4651947 -4.45957 -4.4354453 -4.4132161 -4.4017854 -4.384119][-4.2812476 -4.2812986 -4.2810969 -4.2774053 -4.2815175 -4.300879 -4.3302879 -4.36648 -4.4032841 -4.4282951 -4.4383469 -4.4398608 -4.4348946 -4.4201121 -4.3897243][-4.2677493 -4.268043 -4.2689242 -4.2627726 -4.2616425 -4.2754288 -4.2995892 -4.3292389 -4.3560781 -4.3739877 -4.3942342 -4.4256973 -4.4496384 -4.4421921 -4.4006991][-4.2668834 -4.2697525 -4.2771053 -4.2781711 -4.2816629 -4.2928267 -4.3063669 -4.31863 -4.321146 -4.3164835 -4.33021 -4.3765435 -4.424438 -4.4288745 -4.3809218][-4.2692084 -4.2787447 -4.298389 -4.31655 -4.3328347 -4.3404765 -4.3329654 -4.3113022 -4.2740355 -4.2348452 -4.2293668 -4.2761836 -4.3423033 -4.3669434 -4.3287792][-4.2698078 -4.2874022 -4.32112 -4.3575392 -4.3862553 -4.3876843 -4.3539777 -4.2914047 -4.20941 -4.137291 -4.1150341 -4.1595807 -4.2448134 -4.2994175 -4.2863522][-4.268549 -4.29356 -4.3354955 -4.3813524 -4.4184995 -4.4206243 -4.3769846 -4.2913508 -4.1770182 -4.0782933 -4.0396295 -4.0762572 -4.1771383 -4.2694907 -4.2900739][-4.2743382 -4.3048182 -4.3461337 -4.3895497 -4.429009 -4.4379697 -4.3996072 -4.3056197 -4.1660357 -4.0389585 -3.9752984 -3.9957328 -4.11281 -4.2534533 -4.3155069][-4.2927423 -4.3286591 -4.3634019 -4.3906217 -4.4141626 -4.41685 -4.3792429 -4.2793994 -4.1213765 -3.9732239 -3.89 -3.8982511 -4.0306044 -4.2189093 -4.3222847][-4.3224435 -4.3625274 -4.3860278 -4.3829465 -4.3705091 -4.3545785 -4.3179688 -4.2252545 -4.0677633 -3.9142728 -3.8214927 -3.8197136 -3.9521706 -4.1646838 -4.2971745][-4.3433561 -4.387085 -4.3999038 -4.3677979 -4.3225236 -4.294395 -4.2661228 -4.1853547 -4.0312061 -3.8659008 -3.7601991 -3.7541161 -3.8824234 -4.1022129 -4.251328][-4.350131 -4.3961768 -4.4058414 -4.3606014 -4.2989349 -4.267437 -4.2503643 -4.1878824 -4.0500207 -3.8827341 -3.77174 -3.7695258 -3.8877521 -4.0848451 -4.2170472][-4.3551474 -4.3957911 -4.4023032 -4.3570747 -4.2989473 -4.2816224 -4.2950516 -4.2797751 -4.19075 -4.0439196 -3.9284151 -3.9049807 -3.9738283 -4.1015329 -4.1726508][-4.3574142 -4.3830957 -4.385675 -4.3538375 -4.3200154 -4.3274326 -4.3681159 -4.390667 -4.346673 -4.2275214 -4.1027908 -4.0364118 -4.0401182 -4.0937085 -4.1055584]]...]
INFO - root - 2017-12-07 14:10:17.843756: step 22510, loss = 22.02, batch loss = 21.94 (8.1 examples/sec; 0.988 sec/batch; 85h:05m:51s remains)
INFO - root - 2017-12-07 14:10:27.293388: step 22520, loss = 21.59, batch loss = 21.50 (8.3 examples/sec; 0.959 sec/batch; 82h:36m:10s remains)
INFO - root - 2017-12-07 14:10:36.648264: step 22530, loss = 21.18, batch loss = 21.09 (8.9 examples/sec; 0.903 sec/batch; 77h:42m:55s remains)
INFO - root - 2017-12-07 14:10:46.004355: step 22540, loss = 21.70, batch loss = 21.62 (9.2 examples/sec; 0.874 sec/batch; 75h:12m:39s remains)
INFO - root - 2017-12-07 14:10:55.524308: step 22550, loss = 21.44, batch loss = 21.36 (8.4 examples/sec; 0.952 sec/batch; 81h:58m:17s remains)
INFO - root - 2017-12-07 14:11:05.035745: step 22560, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.930 sec/batch; 80h:04m:15s remains)
INFO - root - 2017-12-07 14:11:14.390927: step 22570, loss = 21.00, batch loss = 20.92 (8.6 examples/sec; 0.930 sec/batch; 80h:02m:44s remains)
INFO - root - 2017-12-07 14:11:23.762117: step 22580, loss = 21.31, batch loss = 21.23 (9.3 examples/sec; 0.856 sec/batch; 73h:41m:34s remains)
INFO - root - 2017-12-07 14:11:33.220951: step 22590, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.970 sec/batch; 83h:32m:18s remains)
INFO - root - 2017-12-07 14:11:42.586392: step 22600, loss = 21.46, batch loss = 21.38 (8.4 examples/sec; 0.948 sec/batch; 81h:35m:54s remains)
2017-12-07 14:11:43.498458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5422421 -4.4817314 -4.3818321 -4.2950239 -4.2548504 -4.2713017 -4.3340034 -4.384099 -4.3962584 -4.3815207 -4.3554087 -4.3383789 -4.3432155 -4.3492012 -4.3575511][-4.5482721 -4.444314 -4.2896414 -4.1574268 -4.1117382 -4.1632586 -4.2762709 -4.3698125 -4.4085135 -4.4132833 -4.3964009 -4.36889 -4.3400097 -4.3029957 -4.2731094][-4.5161266 -4.412302 -4.247386 -4.0956631 -4.0473895 -4.1160116 -4.2556391 -4.3753304 -4.4321189 -4.4557467 -4.4547095 -4.4228864 -4.3594937 -4.2740135 -4.2086449][-4.4549165 -4.4076338 -4.285677 -4.1457067 -4.0872717 -4.1280355 -4.23166 -4.3208227 -4.3574233 -4.388814 -4.4217439 -4.4237828 -4.3659906 -4.2522926 -4.15666][-4.363677 -4.3915153 -4.3428955 -4.2411795 -4.1782432 -4.1686492 -4.1965842 -4.2089195 -4.1914926 -4.2135577 -4.286046 -4.3547893 -4.3521023 -4.2609882 -4.1700506][-4.2895279 -4.3663607 -4.3805513 -4.3265042 -4.26746 -4.2057042 -4.1413932 -4.0567613 -3.9744651 -3.9831777 -4.0942354 -4.2373204 -4.3209119 -4.306201 -4.2698154][-4.2753754 -4.362967 -4.4067655 -4.3810325 -4.3110566 -4.1912222 -4.0275683 -3.8408639 -3.7077303 -3.7285707 -3.8969982 -4.1170273 -4.2858906 -4.3612852 -4.3938088][-4.2867479 -4.36979 -4.428925 -4.4171143 -4.3253007 -4.1479015 -3.9062536 -3.6578319 -3.510062 -3.5570803 -3.7789268 -4.0579476 -4.2869225 -4.42332 -4.4967132][-4.2105861 -4.3206482 -4.4289589 -4.4571476 -4.36085 -4.1519589 -3.8886681 -3.6510124 -3.5296712 -3.593996 -3.8225498 -4.1068811 -4.3427415 -4.4829345 -4.5461054][-4.0485778 -4.2032757 -4.3862805 -4.489852 -4.4419994 -4.2563038 -4.0283332 -3.8452744 -3.7653697 -3.8294396 -4.0209227 -4.2601943 -4.4453297 -4.52716 -4.5272703][-3.943253 -4.1190624 -4.3463426 -4.5141449 -4.5391932 -4.4151626 -4.2430296 -4.1100016 -4.060235 -4.1226368 -4.2787919 -4.4657054 -4.576355 -4.57069 -4.4898758][-3.9841356 -4.1511836 -4.3645568 -4.5432386 -4.6143064 -4.5560241 -4.45179 -4.3700814 -4.3489871 -4.409019 -4.5308876 -4.657382 -4.6890764 -4.6046009 -4.4635139][-4.0894017 -4.2343297 -4.4021688 -4.5492873 -4.62717 -4.6196446 -4.5860958 -4.5649443 -4.5761909 -4.6280351 -4.7019429 -4.7572169 -4.7234511 -4.5966897 -4.437171][-4.1552072 -4.2681413 -4.3911443 -4.5034008 -4.5752311 -4.6012559 -4.6211658 -4.6476135 -4.6794739 -4.7154522 -4.742043 -4.7384882 -4.6695728 -4.5395427 -4.396863][-4.1784153 -4.2564173 -4.3510423 -4.4499354 -4.5273628 -4.5746517 -4.6114616 -4.6427231 -4.6650624 -4.6777525 -4.6694493 -4.6339555 -4.5583725 -4.4507256 -4.3432488]]...]
INFO - root - 2017-12-07 14:11:52.894160: step 22610, loss = 21.47, batch loss = 21.39 (8.6 examples/sec; 0.932 sec/batch; 80h:12m:05s remains)
INFO - root - 2017-12-07 14:12:02.356859: step 22620, loss = 21.66, batch loss = 21.58 (8.1 examples/sec; 0.993 sec/batch; 85h:26m:04s remains)
INFO - root - 2017-12-07 14:12:11.813270: step 22630, loss = 21.08, batch loss = 21.00 (8.1 examples/sec; 0.983 sec/batch; 84h:36m:44s remains)
INFO - root - 2017-12-07 14:12:21.330655: step 22640, loss = 21.45, batch loss = 21.37 (8.3 examples/sec; 0.959 sec/batch; 82h:34m:58s remains)
INFO - root - 2017-12-07 14:12:30.585696: step 22650, loss = 21.33, batch loss = 21.25 (8.6 examples/sec; 0.930 sec/batch; 80h:01m:47s remains)
INFO - root - 2017-12-07 14:12:40.121174: step 22660, loss = 21.36, batch loss = 21.27 (8.3 examples/sec; 0.959 sec/batch; 82h:33m:57s remains)
INFO - root - 2017-12-07 14:12:49.608491: step 22670, loss = 21.41, batch loss = 21.32 (8.7 examples/sec; 0.916 sec/batch; 78h:52m:34s remains)
INFO - root - 2017-12-07 14:12:58.980179: step 22680, loss = 21.72, batch loss = 21.64 (8.5 examples/sec; 0.944 sec/batch; 81h:13m:47s remains)
INFO - root - 2017-12-07 14:13:08.235877: step 22690, loss = 21.18, batch loss = 21.10 (9.0 examples/sec; 0.885 sec/batch; 76h:09m:48s remains)
INFO - root - 2017-12-07 14:13:17.798009: step 22700, loss = 21.45, batch loss = 21.37 (8.2 examples/sec; 0.970 sec/batch; 83h:29m:41s remains)
2017-12-07 14:13:18.744358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4498115 -4.5111852 -4.5642424 -4.5885973 -4.5710125 -4.518455 -4.476553 -4.4820061 -4.5187664 -4.5523977 -4.5706162 -4.5793333 -4.5904417 -4.5864773 -4.5504489][-4.5157723 -4.5938778 -4.6524816 -4.6717587 -4.6332073 -4.5470362 -4.4763007 -4.4705572 -4.5098863 -4.5490637 -4.5734639 -4.5910134 -4.6077571 -4.5892148 -4.5269895][-4.5607657 -4.646697 -4.6972718 -4.6956139 -4.6213264 -4.4882503 -4.3831916 -4.3700218 -4.4194641 -4.4765506 -4.5220833 -4.5581331 -4.5823212 -4.5462952 -4.4502425][-4.574501 -4.6590881 -4.69459 -4.6635575 -4.5440421 -4.3545723 -4.2097526 -4.1958561 -4.2655196 -4.3489413 -4.4246793 -4.4830441 -4.5088186 -4.4559307 -4.333746][-4.5618978 -4.64113 -4.6580243 -4.5897026 -4.4195275 -4.1731787 -3.9845812 -3.9716549 -4.0707469 -4.190414 -4.3011656 -4.3816109 -4.4042783 -4.3356509 -4.1972103][-4.5416408 -4.6105905 -4.6018467 -4.4889441 -4.2663336 -3.9703875 -3.7417226 -3.7310226 -3.8709972 -4.0407653 -4.1908007 -4.2868609 -4.2986255 -4.2198348 -4.0790844][-4.5216508 -4.5778441 -4.5453129 -4.396945 -4.1369562 -3.8116152 -3.5637479 -3.5613427 -3.746953 -3.9670792 -4.1465073 -4.2378364 -4.2251072 -4.1400256 -4.0149126][-4.5101714 -4.5637355 -4.5308495 -4.3821383 -4.1279964 -3.8168755 -3.5808663 -3.5785265 -3.7659085 -3.9972551 -4.1737041 -4.2398987 -4.20011 -4.1198149 -4.02688][-4.5085511 -4.5716019 -4.5600114 -4.4428906 -4.2347212 -3.97895 -3.7755017 -3.7519205 -3.8849525 -4.0746279 -4.2212024 -4.2624559 -4.2124739 -4.1521263 -4.1031537][-4.5103683 -4.585073 -4.6019635 -4.5282755 -4.3786755 -4.1881647 -4.023066 -3.97621 -4.047833 -4.1817451 -4.2915893 -4.3171549 -4.2697811 -4.221664 -4.1990881][-4.5071068 -4.5873413 -4.6274452 -4.5949321 -4.5008426 -4.3740759 -4.2561393 -4.2110367 -4.2487116 -4.3341551 -4.4050078 -4.4135036 -4.367126 -4.3183513 -4.2982421][-4.489356 -4.5602317 -4.6099157 -4.6140823 -4.5789571 -4.5224142 -4.4711876 -4.4549165 -4.4794645 -4.5207224 -4.5438008 -4.5311475 -4.4864149 -4.434917 -4.4065208][-4.460196 -4.5128613 -4.5600033 -4.5920539 -4.6085429 -4.613194 -4.6206121 -4.6344485 -4.6510744 -4.6584229 -4.6469646 -4.6231618 -4.5887308 -4.5482535 -4.5182152][-4.4302 -4.4666114 -4.5051494 -4.5473666 -4.5870347 -4.6183057 -4.6460209 -4.6631188 -4.668582 -4.6628122 -4.64727 -4.6325641 -4.6197233 -4.6019568 -4.5810924][-4.3929224 -4.4144883 -4.4387808 -4.4712133 -4.5049644 -4.531395 -4.5508876 -4.5581269 -4.5570855 -4.5546865 -4.5512867 -4.5503573 -4.5515823 -4.5488791 -4.5396709]]...]
INFO - root - 2017-12-07 14:13:28.199772: step 22710, loss = 21.14, batch loss = 21.06 (8.0 examples/sec; 0.996 sec/batch; 85h:43m:13s remains)
INFO - root - 2017-12-07 14:13:37.525595: step 22720, loss = 21.37, batch loss = 21.29 (8.5 examples/sec; 0.939 sec/batch; 80h:49m:43s remains)
INFO - root - 2017-12-07 14:13:46.820034: step 22730, loss = 21.43, batch loss = 21.35 (8.8 examples/sec; 0.914 sec/batch; 78h:37m:06s remains)
INFO - root - 2017-12-07 14:13:56.257875: step 22740, loss = 21.84, batch loss = 21.75 (8.5 examples/sec; 0.941 sec/batch; 80h:57m:54s remains)
INFO - root - 2017-12-07 14:14:05.747219: step 22750, loss = 21.31, batch loss = 21.23 (9.0 examples/sec; 0.890 sec/batch; 76h:32m:34s remains)
INFO - root - 2017-12-07 14:14:15.243307: step 22760, loss = 20.71, batch loss = 20.63 (8.0 examples/sec; 1.004 sec/batch; 86h:23m:59s remains)
INFO - root - 2017-12-07 14:14:24.567454: step 22770, loss = 21.86, batch loss = 21.78 (8.3 examples/sec; 0.968 sec/batch; 83h:16m:07s remains)
INFO - root - 2017-12-07 14:14:33.996927: step 22780, loss = 21.16, batch loss = 21.08 (8.4 examples/sec; 0.948 sec/batch; 81h:33m:32s remains)
INFO - root - 2017-12-07 14:14:43.297982: step 22790, loss = 21.08, batch loss = 20.99 (9.0 examples/sec; 0.888 sec/batch; 76h:26m:11s remains)
INFO - root - 2017-12-07 14:14:52.666060: step 22800, loss = 20.79, batch loss = 20.71 (8.5 examples/sec; 0.946 sec/batch; 81h:23m:45s remains)
2017-12-07 14:14:53.596046: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4352455 -4.4628787 -4.4830618 -4.4935212 -4.4856687 -4.4712496 -4.4520817 -4.4264369 -4.4071069 -4.4041419 -4.4029036 -4.4042835 -4.3951435 -4.3709307 -4.3439116][-4.4600072 -4.4851265 -4.5022092 -4.5128961 -4.5052476 -4.4938283 -4.4786358 -4.4538569 -4.4454308 -4.465981 -4.4870744 -4.5052381 -4.4968462 -4.4585385 -4.4049988][-4.4483 -4.4579468 -4.4546995 -4.4543443 -4.4394431 -4.4283762 -4.4192033 -4.4065537 -4.4250493 -4.4823875 -4.5394416 -4.5853958 -4.5876579 -4.5466604 -4.4760828][-4.3854189 -4.3948913 -4.3808737 -4.366569 -4.3348079 -4.3117175 -4.3019495 -4.3058572 -4.3565989 -4.4481826 -4.5385952 -4.6085448 -4.6231279 -4.5886874 -4.5165753][-4.3219671 -4.3518367 -4.3339138 -4.2980266 -4.2398224 -4.1925807 -4.1674523 -4.1794653 -4.2569609 -4.3725324 -4.4896817 -4.5799088 -4.6062188 -4.5817637 -4.5159631][-4.3169713 -4.3680739 -4.3340054 -4.2654047 -4.1731892 -4.0918636 -4.03462 -4.0354915 -4.1178083 -4.238687 -4.3769464 -4.496654 -4.5500131 -4.5442157 -4.4904976][-4.3160372 -4.3804822 -4.3381929 -4.2507548 -4.1350183 -4.0219235 -3.927057 -3.8970387 -3.9548411 -4.0562682 -4.2005224 -4.3535533 -4.4515343 -4.4791 -4.4489212][-4.294157 -4.3674994 -4.3372717 -4.2618532 -4.1476722 -4.0182881 -3.8845034 -3.8024681 -3.8146117 -3.8849154 -4.0232444 -4.2006521 -4.3441548 -4.4121242 -4.411479][-4.2994776 -4.3729949 -4.3631759 -4.3192768 -4.2294283 -4.10209 -3.9390874 -3.8018935 -3.7607973 -3.8050683 -3.9379971 -4.125392 -4.2950349 -4.3900962 -4.405735][-4.3517461 -4.410274 -4.4076662 -4.390492 -4.3362365 -4.2323089 -4.0693173 -3.9099379 -3.8382416 -3.8750937 -4.0106368 -4.1935363 -4.3561583 -4.4420519 -4.4424472][-4.4688272 -4.5038919 -4.4922562 -4.4836683 -4.4554014 -4.379231 -4.2371349 -4.0898566 -4.0185452 -4.0644159 -4.206212 -4.3729434 -4.5010495 -4.5475769 -4.5097718][-4.5669627 -4.5883503 -4.5705433 -4.5606971 -4.5453377 -4.4920731 -4.3777857 -4.2615767 -4.209281 -4.2680063 -4.4085379 -4.5464506 -4.6299548 -4.6335464 -4.5638437][-4.5849156 -4.6044097 -4.5956025 -4.5932093 -4.588923 -4.5578322 -4.47909 -4.3994384 -4.3679323 -4.4253812 -4.5423322 -4.6404619 -4.6832609 -4.6574879 -4.5745397][-4.5827708 -4.6068244 -4.6140327 -4.6261806 -4.631752 -4.6125627 -4.5573893 -4.5004792 -4.4772563 -4.5178571 -4.5981464 -4.6566772 -4.6681118 -4.6260853 -4.5445781][-4.538157 -4.5619454 -4.5739322 -4.5854411 -4.587842 -4.5714455 -4.5328169 -4.495542 -4.4815936 -4.5097256 -4.5609512 -4.59136 -4.5852852 -4.5443258 -4.4808989]]...]
INFO - root - 2017-12-07 14:15:02.927679: step 22810, loss = 21.49, batch loss = 21.41 (8.4 examples/sec; 0.949 sec/batch; 81h:37m:21s remains)
INFO - root - 2017-12-07 14:15:12.348054: step 22820, loss = 21.34, batch loss = 21.25 (8.3 examples/sec; 0.958 sec/batch; 82h:25m:58s remains)
INFO - root - 2017-12-07 14:15:21.749335: step 22830, loss = 21.75, batch loss = 21.67 (8.1 examples/sec; 0.983 sec/batch; 84h:31m:40s remains)
INFO - root - 2017-12-07 14:15:31.117614: step 22840, loss = 21.67, batch loss = 21.59 (8.1 examples/sec; 0.988 sec/batch; 84h:59m:33s remains)
INFO - root - 2017-12-07 14:15:40.408458: step 22850, loss = 21.08, batch loss = 21.00 (8.6 examples/sec; 0.925 sec/batch; 79h:35m:43s remains)
INFO - root - 2017-12-07 14:15:49.919997: step 22860, loss = 22.18, batch loss = 22.10 (8.8 examples/sec; 0.910 sec/batch; 78h:17m:54s remains)
INFO - root - 2017-12-07 14:15:59.340043: step 22870, loss = 21.37, batch loss = 21.28 (8.9 examples/sec; 0.895 sec/batch; 76h:58m:41s remains)
INFO - root - 2017-12-07 14:16:08.771889: step 22880, loss = 21.89, batch loss = 21.81 (8.9 examples/sec; 0.901 sec/batch; 77h:28m:13s remains)
INFO - root - 2017-12-07 14:16:18.051722: step 22890, loss = 22.14, batch loss = 22.05 (8.5 examples/sec; 0.936 sec/batch; 80h:31m:23s remains)
INFO - root - 2017-12-07 14:16:27.391331: step 22900, loss = 21.34, batch loss = 21.26 (8.7 examples/sec; 0.924 sec/batch; 79h:27m:19s remains)
2017-12-07 14:16:28.305269: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.48623 -4.5090547 -4.5281997 -4.5350895 -4.5279751 -4.5170126 -4.51443 -4.5225377 -4.5341797 -4.5372725 -4.5285878 -4.5173697 -4.4985404 -4.469677 -4.4426055][-4.5635676 -4.5720749 -4.57216 -4.5631194 -4.5474677 -4.5350451 -4.535614 -4.5526948 -4.5744328 -4.5809503 -4.5757909 -4.5788136 -4.5763474 -4.5572867 -4.5308218][-4.55621 -4.55028 -4.5362206 -4.5160823 -4.4956689 -4.4823785 -4.4824171 -4.5077314 -4.5448523 -4.5581145 -4.55969 -4.5846086 -4.6066642 -4.6029491 -4.5813508][-4.4606094 -4.4554257 -4.4413433 -4.4130435 -4.3743043 -4.3345037 -4.3091 -4.3364716 -4.40539 -4.4466848 -4.4748149 -4.53163 -4.57701 -4.582047 -4.5597992][-4.317625 -4.328227 -4.3243728 -4.2811666 -4.2027965 -4.1058373 -4.0248227 -4.0455294 -4.1632996 -4.2629194 -4.3476634 -4.4505877 -4.5142879 -4.5121207 -4.4708562][-4.1949792 -4.2352877 -4.2482166 -4.1841745 -4.0518894 -3.877172 -3.7200022 -3.720027 -3.8856158 -4.0616593 -4.2268457 -4.3828726 -4.4564514 -4.4326572 -4.3521914][-4.146944 -4.22252 -4.2581849 -4.184689 -4.0191674 -3.7872171 -3.5606551 -3.5216672 -3.697721 -3.9298291 -4.169755 -4.3687897 -4.44094 -4.3883958 -4.2651753][-4.1780419 -4.2792568 -4.3337536 -4.2680473 -4.1065397 -3.870666 -3.627655 -3.5568182 -3.7023759 -3.9436285 -4.2188148 -4.4291377 -4.4854679 -4.4090977 -4.2622175][-4.2624335 -4.3580818 -4.408679 -4.3562922 -4.2307138 -4.0491409 -3.8566556 -3.7858269 -3.88071 -4.078763 -4.3308253 -4.517633 -4.5538158 -4.4693322 -4.3259859][-4.3832607 -4.4471517 -4.4748797 -4.43353 -4.3577 -4.26057 -4.1575475 -4.116818 -4.1668272 -4.2918315 -4.4693856 -4.5984979 -4.6067524 -4.5225229 -4.3971839][-4.50003 -4.5364528 -4.5426455 -4.51101 -4.4737864 -4.4412379 -4.4104323 -4.4035106 -4.4329906 -4.500144 -4.6001673 -4.6655388 -4.6433582 -4.5560656 -4.4448051][-4.538332 -4.5592637 -4.5557508 -4.5327177 -4.5127382 -4.50382 -4.4994297 -4.5056129 -4.5246263 -4.5607634 -4.614172 -4.6435394 -4.6130466 -4.5361857 -4.4438725][-4.4798384 -4.4930315 -4.4880404 -4.4734659 -4.4608183 -4.4541659 -4.4497614 -4.4516878 -4.45946 -4.4764223 -4.505074 -4.5208235 -4.5011287 -4.4532127 -4.3954577][-4.3987455 -4.4006472 -4.3922391 -4.3821125 -4.3758779 -4.3734522 -4.3721046 -4.3728218 -4.3721957 -4.3735919 -4.3829923 -4.3877048 -4.3774228 -4.3573041 -4.3353257][-4.3561454 -4.3493161 -4.3368483 -4.3272419 -4.3243532 -4.3258085 -4.3289919 -4.3310442 -4.3276291 -4.321661 -4.3188829 -4.3140631 -4.3060012 -4.3011508 -4.302177]]...]
INFO - root - 2017-12-07 14:16:37.694962: step 22910, loss = 21.25, batch loss = 21.17 (8.6 examples/sec; 0.926 sec/batch; 79h:39m:44s remains)
INFO - root - 2017-12-07 14:16:47.149167: step 22920, loss = 21.47, batch loss = 21.39 (8.5 examples/sec; 0.945 sec/batch; 81h:18m:21s remains)
INFO - root - 2017-12-07 14:16:56.649946: step 22930, loss = 21.53, batch loss = 21.44 (8.6 examples/sec; 0.930 sec/batch; 79h:57m:22s remains)
INFO - root - 2017-12-07 14:17:05.887668: step 22940, loss = 21.97, batch loss = 21.88 (8.7 examples/sec; 0.919 sec/batch; 79h:01m:41s remains)
INFO - root - 2017-12-07 14:17:15.314141: step 22950, loss = 21.32, batch loss = 21.24 (8.1 examples/sec; 0.992 sec/batch; 85h:16m:39s remains)
INFO - root - 2017-12-07 14:17:24.674720: step 22960, loss = 21.65, batch loss = 21.56 (8.0 examples/sec; 0.996 sec/batch; 85h:38m:23s remains)
INFO - root - 2017-12-07 14:17:34.096079: step 22970, loss = 21.14, batch loss = 21.06 (8.1 examples/sec; 0.987 sec/batch; 84h:51m:20s remains)
INFO - root - 2017-12-07 14:17:43.483738: step 22980, loss = 21.33, batch loss = 21.24 (8.8 examples/sec; 0.909 sec/batch; 78h:07m:48s remains)
INFO - root - 2017-12-07 14:17:52.935038: step 22990, loss = 21.54, batch loss = 21.46 (8.3 examples/sec; 0.964 sec/batch; 82h:52m:21s remains)
INFO - root - 2017-12-07 14:18:02.261876: step 23000, loss = 21.39, batch loss = 21.30 (8.0 examples/sec; 1.004 sec/batch; 86h:21m:10s remains)
2017-12-07 14:18:03.190328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3187737 -4.3984981 -4.4337583 -4.394784 -4.3174324 -4.2496238 -4.2181082 -4.2223415 -4.224937 -4.1807771 -4.1539607 -4.1832247 -4.1964908 -4.1793566 -4.1677341][-4.3246455 -4.4272304 -4.4845319 -4.4625058 -4.39667 -4.322773 -4.2654552 -4.2530122 -4.2624559 -4.2366781 -4.2262244 -4.26795 -4.2930365 -4.2830248 -4.2610598][-4.354394 -4.4709411 -4.5416985 -4.52555 -4.4535346 -4.3643579 -4.2852392 -4.2600017 -4.2782197 -4.2789736 -4.3000488 -4.3616428 -4.3918104 -4.3734121 -4.3347392][-4.3851972 -4.4991918 -4.5643549 -4.5509772 -4.4721041 -4.3672085 -4.2725334 -4.2355685 -4.260612 -4.2921357 -4.3460636 -4.4237251 -4.4533029 -4.4330268 -4.3942828][-4.3867188 -4.4678535 -4.5032463 -4.4986486 -4.4340305 -4.325017 -4.2182217 -4.1677155 -4.1938777 -4.2474427 -4.3225036 -4.4116158 -4.4454026 -4.4329772 -4.4087062][-4.36942 -4.4139347 -4.4166794 -4.4171143 -4.3717694 -4.266407 -4.1585617 -4.1048045 -4.1331835 -4.1986661 -4.2813554 -4.3719416 -4.4060307 -4.4010282 -4.3960724][-4.3594532 -4.3865452 -4.3849626 -4.3970151 -4.367516 -4.2668476 -4.168612 -4.12454 -4.1528687 -4.220561 -4.2950492 -4.3703303 -4.3978109 -4.4036059 -4.4175773][-4.34855 -4.3589058 -4.3571949 -4.3768005 -4.357646 -4.2646923 -4.1873512 -4.1655149 -4.1950693 -4.2570214 -4.3120584 -4.3613648 -4.3852768 -4.4092059 -4.442719][-4.3440661 -4.3292642 -4.3143554 -4.3243 -4.300354 -4.2122431 -4.1607141 -4.1725698 -4.2139025 -4.2680054 -4.3023238 -4.3307719 -4.3620009 -4.4087195 -4.4535866][-4.3432555 -4.3125196 -4.2813711 -4.2822571 -4.2596822 -4.1829195 -4.1516805 -4.189292 -4.2457533 -4.3004503 -4.3314977 -4.3607297 -4.4054093 -4.4564896 -4.4845047][-4.2965174 -4.2524796 -4.2081184 -4.2151904 -4.2165308 -4.1702471 -4.1504679 -4.1850533 -4.2411294 -4.2976732 -4.3407378 -4.3943868 -4.4614058 -4.5045605 -4.5031462][-4.2228436 -4.1716318 -4.1294065 -4.1593332 -4.2035818 -4.1980667 -4.1772738 -4.17534 -4.1973143 -4.2418771 -4.291647 -4.3686271 -4.4610491 -4.5068 -4.4907193][-4.1860757 -4.1249995 -4.0890803 -4.1449604 -4.2390327 -4.2758193 -4.252728 -4.20911 -4.1840105 -4.2030492 -4.2415862 -4.3166413 -4.4134316 -4.4617171 -4.4469557][-4.168685 -4.0848107 -4.0347428 -4.0979633 -4.2311988 -4.3093166 -4.3048859 -4.2498879 -4.1951942 -4.1909714 -4.2165947 -4.2715688 -4.3436279 -4.3723621 -4.3536935][-4.1866608 -4.0823679 -4.0077438 -4.0611634 -4.2165079 -4.3242764 -4.3425245 -4.30095 -4.2426786 -4.2303004 -4.252327 -4.2881775 -4.3206825 -4.3110075 -4.2741175]]...]
INFO - root - 2017-12-07 14:18:12.744404: step 23010, loss = 21.65, batch loss = 21.56 (8.5 examples/sec; 0.936 sec/batch; 80h:30m:14s remains)
INFO - root - 2017-12-07 14:18:22.251636: step 23020, loss = 21.64, batch loss = 21.55 (8.6 examples/sec; 0.925 sec/batch; 79h:30m:28s remains)
INFO - root - 2017-12-07 14:18:31.770613: step 23030, loss = 21.63, batch loss = 21.55 (8.4 examples/sec; 0.949 sec/batch; 81h:37m:16s remains)
INFO - root - 2017-12-07 14:18:41.210731: step 23040, loss = 21.63, batch loss = 21.54 (8.1 examples/sec; 0.983 sec/batch; 84h:31m:03s remains)
INFO - root - 2017-12-07 14:18:50.638606: step 23050, loss = 21.58, batch loss = 21.50 (8.8 examples/sec; 0.908 sec/batch; 78h:01m:07s remains)
INFO - root - 2017-12-07 14:19:00.099179: step 23060, loss = 21.33, batch loss = 21.25 (8.8 examples/sec; 0.913 sec/batch; 78h:27m:01s remains)
INFO - root - 2017-12-07 14:19:09.568843: step 23070, loss = 21.58, batch loss = 21.49 (8.4 examples/sec; 0.953 sec/batch; 81h:57m:17s remains)
INFO - root - 2017-12-07 14:19:18.864391: step 23080, loss = 21.77, batch loss = 21.69 (8.0 examples/sec; 0.998 sec/batch; 85h:49m:09s remains)
INFO - root - 2017-12-07 14:19:28.209554: step 23090, loss = 20.88, batch loss = 20.80 (8.2 examples/sec; 0.981 sec/batch; 84h:17m:44s remains)
INFO - root - 2017-12-07 14:19:37.484312: step 23100, loss = 21.19, batch loss = 21.11 (8.4 examples/sec; 0.957 sec/batch; 82h:14m:20s remains)
2017-12-07 14:19:38.498811: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5723147 -4.5086308 -4.4996495 -4.5384121 -4.5879054 -4.625618 -4.659236 -4.6508694 -4.6165066 -4.5778809 -4.5449281 -4.5235825 -4.5165629 -4.5167623 -4.5141139][-4.4940119 -4.4183636 -4.3906803 -4.4139338 -4.4577851 -4.4922147 -4.53201 -4.5349789 -4.5120974 -4.4887137 -4.4649339 -4.44442 -4.4385681 -4.444592 -4.4486003][-4.4420853 -4.3666072 -4.3207026 -4.3178234 -4.3390889 -4.3590961 -4.3959389 -4.42078 -4.4289374 -4.4316545 -4.4162574 -4.3919125 -4.3878465 -4.4040961 -4.4201865][-4.4001608 -4.3312039 -4.2808371 -4.2660708 -4.266098 -4.2607145 -4.2724681 -4.3089776 -4.3539791 -4.3859906 -4.3781233 -4.3485212 -4.3442554 -4.3715367 -4.405211][-4.376368 -4.31093 -4.2608767 -4.2436566 -4.2151809 -4.1526728 -4.1069427 -4.1424441 -4.2304325 -4.3042488 -4.3185492 -4.29954 -4.302537 -4.3436027 -4.3980861][-4.3951659 -4.3282752 -4.2730842 -4.2425756 -4.1619906 -4.0044847 -3.8706989 -3.8967147 -4.0379667 -4.1698279 -4.2303529 -4.2431579 -4.2656174 -4.3271422 -4.40735][-4.4401069 -4.3595619 -4.2884674 -4.2321258 -4.0919271 -3.8449578 -3.6360209 -3.6702023 -3.8804553 -4.0834312 -4.1996284 -4.246191 -4.2851772 -4.362793 -4.462863][-4.4812083 -4.3774719 -4.2931423 -4.2171669 -4.03592 -3.7493794 -3.5254126 -3.5927317 -3.8683481 -4.1302004 -4.2827406 -4.33953 -4.3746648 -4.4472413 -4.5446754][-4.4959712 -4.3716211 -4.2912107 -4.2261648 -4.0522203 -3.7906485 -3.6004524 -3.6815343 -3.9665923 -4.2432542 -4.40028 -4.4436288 -4.4602709 -4.5079684 -4.5791483][-4.4753056 -4.3435721 -4.2839813 -4.2627606 -4.1407647 -3.9392059 -3.7926857 -3.8513451 -4.0816841 -4.3217769 -4.4526772 -4.4693184 -4.4642076 -4.4882569 -4.5353694][-4.4365435 -4.3155532 -4.279911 -4.3039122 -4.2415943 -4.09929 -3.9828362 -4.0051241 -4.160356 -4.3450236 -4.4380174 -4.4242744 -4.4025478 -4.4115176 -4.4436607][-4.3845539 -4.2823787 -4.2628293 -4.3112531 -4.2893682 -4.1910248 -4.0951815 -4.0933971 -4.1945 -4.3350806 -4.3952227 -4.3617206 -4.3346729 -4.3349857 -4.350533][-4.3356013 -4.2250552 -4.1915197 -4.2343369 -4.2326875 -4.1731448 -4.1106496 -4.1047955 -4.177557 -4.2914405 -4.3388042 -4.3138957 -4.2995596 -4.2981482 -4.2896729][-4.3418374 -4.1979475 -4.1218114 -4.1306448 -4.1280823 -4.0964804 -4.0688353 -4.0657468 -4.1164136 -4.21129 -4.2654724 -4.2760649 -4.291965 -4.2930055 -4.2599845][-4.3797398 -4.2066321 -4.1000876 -4.0855417 -4.0828438 -4.0570312 -4.026309 -3.999403 -4.0151939 -4.0941606 -4.1768742 -4.2533827 -4.3256707 -4.3452349 -4.2984371]]...]
INFO - root - 2017-12-07 14:19:47.943121: step 23110, loss = 21.80, batch loss = 21.71 (8.7 examples/sec; 0.920 sec/batch; 79h:03m:34s remains)
INFO - root - 2017-12-07 14:19:57.164848: step 23120, loss = 21.16, batch loss = 21.08 (8.4 examples/sec; 0.955 sec/batch; 82h:06m:27s remains)
INFO - root - 2017-12-07 14:20:06.346954: step 23130, loss = 21.57, batch loss = 21.48 (9.0 examples/sec; 0.891 sec/batch; 76h:34m:56s remains)
INFO - root - 2017-12-07 14:20:15.766608: step 23140, loss = 21.37, batch loss = 21.28 (8.3 examples/sec; 0.967 sec/batch; 83h:06m:35s remains)
INFO - root - 2017-12-07 14:20:25.282029: step 23150, loss = 21.68, batch loss = 21.60 (8.5 examples/sec; 0.937 sec/batch; 80h:32m:45s remains)
INFO - root - 2017-12-07 14:20:34.635446: step 23160, loss = 21.51, batch loss = 21.43 (8.5 examples/sec; 0.941 sec/batch; 80h:50m:40s remains)
INFO - root - 2017-12-07 14:20:43.956229: step 23170, loss = 21.31, batch loss = 21.23 (8.6 examples/sec; 0.935 sec/batch; 80h:21m:53s remains)
INFO - root - 2017-12-07 14:20:53.284534: step 23180, loss = 21.69, batch loss = 21.61 (8.7 examples/sec; 0.916 sec/batch; 78h:41m:19s remains)
INFO - root - 2017-12-07 14:21:02.619394: step 23190, loss = 21.44, batch loss = 21.35 (8.8 examples/sec; 0.905 sec/batch; 77h:42m:57s remains)
INFO - root - 2017-12-07 14:21:11.873668: step 23200, loss = 21.46, batch loss = 21.38 (8.4 examples/sec; 0.951 sec/batch; 81h:41m:03s remains)
2017-12-07 14:21:12.798040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.41981 -4.4728484 -4.5322857 -4.5762911 -4.5965648 -4.6003885 -4.5933337 -4.5798807 -4.5771132 -4.5941043 -4.6159039 -4.6135211 -4.5800982 -4.5246353 -4.4642353][-4.463418 -4.5655451 -4.6673779 -4.7329526 -4.7491379 -4.7291212 -4.6885538 -4.6414337 -4.6263828 -4.6663704 -4.7302971 -4.7560358 -4.7197542 -4.6378417 -4.5397129][-4.4915547 -4.6365957 -4.7631612 -4.82311 -4.8040161 -4.7327561 -4.640471 -4.5540361 -4.5298853 -4.6066313 -4.7373252 -4.8171091 -4.8009849 -4.7168837 -4.6021013][-4.4876862 -4.652576 -4.7745686 -4.8003736 -4.7233973 -4.5871782 -4.4367433 -4.3148623 -4.2892609 -4.4106283 -4.6176844 -4.7640152 -4.7863741 -4.7274752 -4.6264162][-4.4541354 -4.6162581 -4.7083364 -4.6765909 -4.5297236 -4.32806 -4.1253924 -3.9762418 -3.9549854 -4.1172276 -4.392736 -4.6043334 -4.6771188 -4.6653347 -4.6046028][-4.4118581 -4.5593038 -4.6091418 -4.5096707 -4.2942452 -4.041657 -3.8067257 -3.6463981 -3.6392112 -3.8334713 -4.15155 -4.4106522 -4.5326967 -4.5712662 -4.5569906][-4.3865385 -4.5263357 -4.5495391 -4.4032149 -4.1437559 -3.863467 -3.6147218 -3.4583671 -3.4736972 -3.6876495 -4.0140047 -4.2884383 -4.4405637 -4.5083246 -4.5196719][-4.3911719 -4.5376377 -4.5698323 -4.4281244 -4.1752253 -3.9036763 -3.6592183 -3.50687 -3.5340562 -3.7462494 -4.0496287 -4.310782 -4.4638839 -4.5257435 -4.5235877][-4.4086347 -4.5668874 -4.6338964 -4.5479245 -4.3602057 -4.1457238 -3.9355593 -3.7913923 -3.8108087 -3.993839 -4.2460546 -4.4648128 -4.586791 -4.6092949 -4.5609579][-4.4122615 -4.5710058 -4.6726465 -4.6592493 -4.5668068 -4.4440379 -4.3023214 -4.1823678 -4.1814566 -4.3124914 -4.4924893 -4.6447864 -4.7142029 -4.6870642 -4.5925155][-4.3959136 -4.5379133 -4.6571083 -4.7015057 -4.6898522 -4.6509314 -4.5851307 -4.509335 -4.498343 -4.5772672 -4.6868639 -4.7688737 -4.7822552 -4.7134624 -4.5916953][-4.3658891 -4.475678 -4.5871568 -4.6568341 -4.6870165 -4.6942039 -4.6802378 -4.6515174 -4.6513314 -4.7021627 -4.7624035 -4.790381 -4.7606893 -4.6694479 -4.5468731][-4.3342218 -4.4002833 -4.4811249 -4.5449862 -4.58298 -4.599782 -4.6039205 -4.60155 -4.6129875 -4.6488504 -4.6809435 -4.6835003 -4.6429791 -4.5624833 -4.4693093][-4.3165092 -4.3372579 -4.3758755 -4.4138083 -4.4376874 -4.4443069 -4.4438396 -4.4464703 -4.4593897 -4.4844518 -4.5049148 -4.5064478 -4.4831705 -4.4395585 -4.3933964][-4.316762 -4.308475 -4.3137083 -4.3240628 -4.3311176 -4.3302727 -4.3267913 -4.3284655 -4.3378434 -4.3532062 -4.3669248 -4.372262 -4.3680816 -4.3576183 -4.3486791]]...]
INFO - root - 2017-12-07 14:21:22.203123: step 23210, loss = 21.55, batch loss = 21.47 (8.8 examples/sec; 0.913 sec/batch; 78h:28m:16s remains)
INFO - root - 2017-12-07 14:21:31.505592: step 23220, loss = 21.58, batch loss = 21.50 (8.8 examples/sec; 0.911 sec/batch; 78h:15m:21s remains)
INFO - root - 2017-12-07 14:21:40.930189: step 23230, loss = 21.62, batch loss = 21.54 (8.5 examples/sec; 0.944 sec/batch; 81h:04m:29s remains)
INFO - root - 2017-12-07 14:21:50.265727: step 23240, loss = 21.48, batch loss = 21.40 (8.8 examples/sec; 0.910 sec/batch; 78h:09m:11s remains)
INFO - root - 2017-12-07 14:21:59.795706: step 23250, loss = 21.12, batch loss = 21.03 (8.6 examples/sec; 0.930 sec/batch; 79h:55m:50s remains)
INFO - root - 2017-12-07 14:22:09.231618: step 23260, loss = 21.28, batch loss = 21.20 (8.1 examples/sec; 0.989 sec/batch; 84h:58m:18s remains)
INFO - root - 2017-12-07 14:22:18.655748: step 23270, loss = 21.53, batch loss = 21.45 (8.0 examples/sec; 1.000 sec/batch; 85h:54m:16s remains)
INFO - root - 2017-12-07 14:22:28.052397: step 23280, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.963 sec/batch; 82h:44m:05s remains)
INFO - root - 2017-12-07 14:22:37.352406: step 23290, loss = 21.18, batch loss = 21.10 (8.0 examples/sec; 1.000 sec/batch; 85h:51m:13s remains)
INFO - root - 2017-12-07 14:22:46.545532: step 23300, loss = 21.87, batch loss = 21.78 (9.5 examples/sec; 0.840 sec/batch; 72h:10m:15s remains)
2017-12-07 14:22:47.464342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5697026 -4.5780525 -4.5661354 -4.5584059 -4.5620079 -4.5700307 -4.5782189 -4.5864573 -4.5913181 -4.5899277 -4.5922265 -4.5947671 -4.581553 -4.5327935 -4.4528155][-4.685195 -4.6656427 -4.6175265 -4.5993018 -4.6102176 -4.6227217 -4.6324019 -4.6482115 -4.6734362 -4.6960793 -4.7199483 -4.7384624 -4.7293425 -4.6593676 -4.5362682][-4.7380514 -4.653079 -4.5544252 -4.5364237 -4.5731692 -4.5971246 -4.6007509 -4.6179085 -4.6678472 -4.7270932 -4.7797761 -4.8174233 -4.8246865 -4.7475262 -4.5884438][-4.70414 -4.5358295 -4.3810849 -4.3685269 -4.44069 -4.4817681 -4.475142 -4.4807057 -4.5416784 -4.6333141 -4.7184415 -4.7899437 -4.842483 -4.7890844 -4.6128216][-4.5792985 -4.3397546 -4.1482706 -4.1481414 -4.2602091 -4.326303 -4.3097477 -4.2852983 -4.3196125 -4.4102054 -4.5182981 -4.6316061 -4.7545443 -4.7674508 -4.6127267][-4.445189 -4.1676807 -3.9606273 -3.9621794 -4.0935564 -4.1734014 -4.132535 -4.0587659 -4.0442481 -4.1233854 -4.2655 -4.4281268 -4.6140351 -4.6994281 -4.5916028][-4.3817682 -4.1001921 -3.8895507 -3.8714905 -3.9835567 -4.0403886 -3.9467885 -3.8158569 -3.7619843 -3.8473747 -4.0522032 -4.27462 -4.4942546 -4.62211 -4.5596662][-4.402216 -4.1570864 -3.9719324 -3.9398458 -4.0159035 -4.0214934 -3.8612015 -3.6854036 -3.623461 -3.7408214 -4.0107656 -4.2665577 -4.460669 -4.5678759 -4.5206814][-4.4934955 -4.3157144 -4.1767731 -4.1428833 -4.1820779 -4.1334496 -3.9231033 -3.7241893 -3.6768804 -3.8419523 -4.1647992 -4.4224119 -4.5516119 -4.5889163 -4.5129848][-4.5995259 -4.5028229 -4.4195647 -4.3967962 -4.4074636 -4.3285942 -4.114707 -3.9200418 -3.8804338 -4.0585513 -4.382637 -4.613328 -4.6856942 -4.6578703 -4.539938][-4.6482525 -4.6156325 -4.5835552 -4.5863733 -4.595624 -4.5297933 -4.3678727 -4.2115602 -4.171072 -4.314219 -4.5731845 -4.7427983 -4.7674637 -4.6995335 -4.56014][-4.6244183 -4.6239214 -4.62367 -4.6476021 -4.6720967 -4.6523333 -4.5724692 -4.4757361 -4.4406662 -4.5330143 -4.7007718 -4.7956471 -4.7785854 -4.6883769 -4.5516367][-4.5227885 -4.537787 -4.5560937 -4.5894403 -4.6262188 -4.6475205 -4.6358342 -4.5934896 -4.568491 -4.6127677 -4.6978416 -4.7364268 -4.701201 -4.6158342 -4.503973][-4.3853526 -4.4036813 -4.4276333 -4.4611926 -4.5012293 -4.5404134 -4.5623593 -4.5563526 -4.543056 -4.5532088 -4.5798216 -4.5854864 -4.5566359 -4.500608 -4.42895][-4.2809 -4.2908564 -4.3062825 -4.3285108 -4.3569183 -4.3885031 -4.4137378 -4.424437 -4.4244318 -4.4248734 -4.4264851 -4.4213943 -4.4072809 -4.3825383 -4.3476863]]...]
INFO - root - 2017-12-07 14:22:56.801520: step 23310, loss = 21.38, batch loss = 21.29 (8.5 examples/sec; 0.938 sec/batch; 80h:33m:28s remains)
INFO - root - 2017-12-07 14:23:06.139093: step 23320, loss = 21.26, batch loss = 21.18 (8.3 examples/sec; 0.965 sec/batch; 82h:54m:01s remains)
INFO - root - 2017-12-07 14:23:15.491299: step 23330, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.934 sec/batch; 80h:13m:29s remains)
INFO - root - 2017-12-07 14:23:24.856049: step 23340, loss = 21.39, batch loss = 21.31 (8.3 examples/sec; 0.959 sec/batch; 82h:20m:39s remains)
INFO - root - 2017-12-07 14:23:34.267943: step 23350, loss = 21.72, batch loss = 21.63 (8.6 examples/sec; 0.933 sec/batch; 80h:07m:42s remains)
INFO - root - 2017-12-07 14:23:43.715427: step 23360, loss = 21.33, batch loss = 21.24 (8.6 examples/sec; 0.930 sec/batch; 79h:53m:50s remains)
INFO - root - 2017-12-07 14:23:53.023068: step 23370, loss = 21.76, batch loss = 21.68 (9.1 examples/sec; 0.880 sec/batch; 75h:33m:09s remains)
INFO - root - 2017-12-07 14:24:02.199317: step 23380, loss = 21.61, batch loss = 21.52 (9.1 examples/sec; 0.875 sec/batch; 75h:05m:27s remains)
INFO - root - 2017-12-07 14:24:11.688380: step 23390, loss = 21.28, batch loss = 21.19 (8.4 examples/sec; 0.947 sec/batch; 81h:17m:47s remains)
INFO - root - 2017-12-07 14:24:21.150477: step 23400, loss = 21.64, batch loss = 21.56 (8.1 examples/sec; 0.991 sec/batch; 85h:03m:02s remains)
2017-12-07 14:24:22.084210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.40419 -4.5307341 -4.6641297 -4.744863 -4.7091408 -4.5727906 -4.4469972 -4.4138422 -4.4786229 -4.5644035 -4.6069531 -4.6206026 -4.619884 -4.611659 -4.6003385][-4.3800912 -4.505723 -4.6332951 -4.690134 -4.6269932 -4.4829574 -4.3698297 -4.3542414 -4.4296002 -4.5146241 -4.5495934 -4.5544086 -4.5543408 -4.5605178 -4.5709953][-4.3109584 -4.4549913 -4.5939989 -4.635705 -4.5475149 -4.3969154 -4.2923012 -4.27989 -4.3458538 -4.4186654 -4.4468493 -4.455277 -4.4713407 -4.5033431 -4.5355625][-4.2594714 -4.4235692 -4.5694251 -4.59391 -4.474648 -4.3068113 -4.1975751 -4.1779976 -4.2293 -4.2919536 -4.3291025 -4.3619404 -4.4132152 -4.4810762 -4.5300241][-4.2585578 -4.4148121 -4.5449662 -4.542809 -4.3912811 -4.1953182 -4.0617256 -4.0218534 -4.0571566 -4.1249776 -4.1968379 -4.27865 -4.3811607 -4.4896984 -4.5533748][-4.3218589 -4.4465847 -4.5393949 -4.5011148 -4.3213015 -4.0975547 -3.9289074 -3.8572595 -3.8728652 -3.9476297 -4.0587592 -4.1926727 -4.3450184 -4.4882317 -4.5663791][-4.4077134 -4.5025897 -4.5589356 -4.4890804 -4.2920413 -4.0523343 -3.8516912 -3.7459548 -3.7372184 -3.8126512 -3.94829 -4.114162 -4.2976928 -4.46564 -4.5613046][-4.4505415 -4.5264926 -4.5616565 -4.4820457 -4.2983294 -4.0687771 -3.8606532 -3.7327616 -3.7030878 -3.7702832 -3.9066815 -4.0732856 -4.2593117 -4.4358478 -4.5479913][-4.4659495 -4.5209646 -4.5405045 -4.4689031 -4.3228974 -4.1302533 -3.9408674 -3.8069637 -3.7571306 -3.8050578 -3.9253533 -4.0753427 -4.2449036 -4.4129605 -4.5304036][-4.4335089 -4.4620957 -4.468668 -4.4145513 -4.3192263 -4.1854086 -4.0355425 -3.9058237 -3.8326631 -3.8507645 -3.9467659 -4.0753517 -4.2229633 -4.3744078 -4.4917655][-4.3747888 -4.3668418 -4.3629746 -4.3287969 -4.2817268 -4.2132816 -4.1143165 -3.998358 -3.9070594 -3.9013317 -3.9829483 -4.0983267 -4.2211957 -4.346251 -4.4553251][-4.3494172 -4.3176079 -4.3078432 -4.2824068 -4.2640381 -4.24904 -4.200386 -4.1041722 -4.0043592 -3.9900451 -4.0733032 -4.1847286 -4.2809453 -4.36815 -4.4550061][-4.3539195 -4.3277 -4.3293142 -4.3176541 -4.3204422 -4.3425689 -4.330883 -4.2496123 -4.1485376 -4.1310096 -4.2122765 -4.3132772 -4.3791881 -4.4277349 -4.4891186][-4.3979325 -4.3957162 -4.4157009 -4.4205627 -4.4418111 -4.4872231 -4.4976249 -4.4292016 -4.3361392 -4.3153214 -4.3767958 -4.4509511 -4.4844441 -4.5042505 -4.5469508][-4.4913244 -4.5122142 -4.5479884 -4.5710168 -4.6086936 -4.6588936 -4.665071 -4.594347 -4.5070958 -4.4806514 -4.5153327 -4.5534096 -4.5581417 -4.5613885 -4.5948172]]...]
INFO - root - 2017-12-07 14:24:31.388281: step 23410, loss = 21.60, batch loss = 21.52 (8.4 examples/sec; 0.957 sec/batch; 82h:12m:19s remains)
INFO - root - 2017-12-07 14:24:40.693922: step 23420, loss = 21.43, batch loss = 21.35 (8.1 examples/sec; 0.983 sec/batch; 84h:24m:37s remains)
INFO - root - 2017-12-07 14:24:49.972637: step 23430, loss = 21.69, batch loss = 21.61 (9.0 examples/sec; 0.891 sec/batch; 76h:32m:12s remains)
INFO - root - 2017-12-07 14:24:59.469937: step 23440, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.926 sec/batch; 79h:31m:00s remains)
INFO - root - 2017-12-07 14:25:08.921185: step 23450, loss = 21.68, batch loss = 21.59 (8.7 examples/sec; 0.917 sec/batch; 78h:42m:22s remains)
INFO - root - 2017-12-07 14:25:18.147194: step 23460, loss = 21.39, batch loss = 21.31 (8.6 examples/sec; 0.929 sec/batch; 79h:46m:25s remains)
INFO - root - 2017-12-07 14:25:27.488416: step 23470, loss = 21.47, batch loss = 21.39 (8.6 examples/sec; 0.928 sec/batch; 79h:41m:45s remains)
INFO - root - 2017-12-07 14:25:36.780906: step 23480, loss = 21.72, batch loss = 21.63 (8.4 examples/sec; 0.954 sec/batch; 81h:53m:50s remains)
INFO - root - 2017-12-07 14:25:46.153979: step 23490, loss = 21.30, batch loss = 21.21 (8.5 examples/sec; 0.946 sec/batch; 81h:11m:34s remains)
INFO - root - 2017-12-07 14:25:55.519288: step 23500, loss = 21.53, batch loss = 21.45 (9.2 examples/sec; 0.869 sec/batch; 74h:34m:46s remains)
2017-12-07 14:25:56.540975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3187418 -4.4216108 -4.5202813 -4.5968547 -4.624753 -4.5988026 -4.5335178 -4.4810362 -4.4744148 -4.4878521 -4.5338039 -4.5995407 -4.6495032 -4.6640558 -4.6439342][-4.1946397 -4.3154936 -4.441844 -4.5401096 -4.5720062 -4.5316792 -4.4382472 -4.3724074 -4.3765655 -4.4118624 -4.48497 -4.5793438 -4.6511097 -4.6741457 -4.6569977][-4.1068416 -4.2200036 -4.3510256 -4.4546509 -4.4833407 -4.4278035 -4.3051343 -4.2292252 -4.2507429 -4.310605 -4.4025655 -4.5147877 -4.6038513 -4.6414614 -4.6362147][-4.1223392 -4.2046175 -4.3083072 -4.3878441 -4.3957219 -4.3202553 -4.1709867 -4.0923395 -4.1383533 -4.2230382 -4.3224916 -4.4330754 -4.5245557 -4.5730157 -4.5853286][-4.1861224 -4.2371964 -4.3135314 -4.36362 -4.3417611 -4.2407708 -4.0726867 -3.997493 -4.0702915 -4.1738071 -4.2671056 -4.3541255 -4.4301391 -4.4795337 -4.5103388][-4.2098184 -4.2478051 -4.3093786 -4.333672 -4.277247 -4.142489 -3.9541152 -3.8787131 -3.97744 -4.1051679 -4.2037754 -4.2763748 -4.3409705 -4.3913851 -4.43939][-4.1850595 -4.2186208 -4.2683358 -4.2666221 -4.1844406 -4.0318708 -3.8328686 -3.7537417 -3.8722382 -4.0348477 -4.1597304 -4.2414083 -4.308744 -4.3629055 -4.4178047][-4.1469326 -4.1749582 -4.2093067 -4.1865916 -4.0962744 -3.9616983 -3.7905233 -3.7277656 -3.8564849 -4.039258 -4.1790719 -4.2679987 -4.3413806 -4.3987765 -4.4507027][-4.0942531 -4.1110477 -4.1294188 -4.0958309 -4.0132236 -3.9190207 -3.8098392 -3.7909503 -3.9272804 -4.1053209 -4.2320695 -4.3139353 -4.3958154 -4.4619427 -4.5095363][-4.0471759 -4.0502639 -4.05905 -4.0265632 -3.9619176 -3.9106996 -3.8704123 -3.9032183 -4.0431614 -4.2003493 -4.2973909 -4.3640985 -4.4517179 -4.5241919 -4.5655727][-4.0478096 -4.0494905 -4.0599909 -4.0412564 -4.0010948 -3.9867797 -3.9957488 -4.0578408 -4.1820774 -4.3029242 -4.3683028 -4.41929 -4.5019178 -4.5704312 -4.6001773][-4.1493373 -4.1580124 -4.1750493 -4.1745887 -4.1562643 -4.1606832 -4.1848593 -4.2373548 -4.3223429 -4.4003038 -4.4414415 -4.4805546 -4.5459342 -4.5951109 -4.6038914][-4.3199067 -4.3396006 -4.3609185 -4.3713045 -4.3657355 -4.372704 -4.3890328 -4.4131408 -4.4512467 -4.4894624 -4.5126619 -4.5385556 -4.5778794 -4.5964532 -4.5773363][-4.4683447 -4.4956112 -4.517364 -4.5315385 -4.5330787 -4.5365524 -4.5380921 -4.5359793 -4.5369773 -4.5419846 -4.5455408 -4.5527916 -4.5638237 -4.55458 -4.5145802][-4.53335 -4.5543289 -4.5699787 -4.579596 -4.5814734 -4.5783606 -4.5668669 -4.5494366 -4.5324378 -4.5205259 -4.5118489 -4.5065784 -4.5008 -4.4781756 -4.434042]]...]
INFO - root - 2017-12-07 14:26:05.791847: step 23510, loss = 21.50, batch loss = 21.42 (7.9 examples/sec; 1.013 sec/batch; 86h:57m:06s remains)
INFO - root - 2017-12-07 14:26:15.086911: step 23520, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.973 sec/batch; 83h:30m:55s remains)
INFO - root - 2017-12-07 14:26:24.570157: step 23530, loss = 21.27, batch loss = 21.19 (8.6 examples/sec; 0.933 sec/batch; 80h:04m:46s remains)
INFO - root - 2017-12-07 14:26:33.896112: step 23540, loss = 21.71, batch loss = 21.63 (8.6 examples/sec; 0.933 sec/batch; 80h:04m:35s remains)
INFO - root - 2017-12-07 14:26:43.379497: step 23550, loss = 21.17, batch loss = 21.09 (8.3 examples/sec; 0.961 sec/batch; 82h:29m:09s remains)
INFO - root - 2017-12-07 14:26:52.700970: step 23560, loss = 21.27, batch loss = 21.19 (8.8 examples/sec; 0.906 sec/batch; 77h:42m:44s remains)
INFO - root - 2017-12-07 14:27:01.962213: step 23570, loss = 21.19, batch loss = 21.11 (8.9 examples/sec; 0.901 sec/batch; 77h:21m:03s remains)
INFO - root - 2017-12-07 14:27:11.304320: step 23580, loss = 21.33, batch loss = 21.24 (9.0 examples/sec; 0.892 sec/batch; 76h:34m:42s remains)
INFO - root - 2017-12-07 14:27:20.829119: step 23590, loss = 21.61, batch loss = 21.53 (8.4 examples/sec; 0.954 sec/batch; 81h:52m:51s remains)
INFO - root - 2017-12-07 14:27:30.307006: step 23600, loss = 21.16, batch loss = 21.07 (8.3 examples/sec; 0.960 sec/batch; 82h:22m:06s remains)
2017-12-07 14:27:31.317503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3563018 -4.3532462 -4.3758373 -4.4074507 -4.4236035 -4.420259 -4.4213319 -4.4417739 -4.4716015 -4.4862876 -4.4788175 -4.4594903 -4.4545736 -4.4752851 -4.4906254][-4.3591633 -4.3609724 -4.397697 -4.4348454 -4.43526 -4.4061708 -4.3847613 -4.399147 -4.4379582 -4.457583 -4.4483657 -4.4267726 -4.41989 -4.4388962 -4.4555821][-4.3596635 -4.3648095 -4.4094191 -4.442399 -4.41801 -4.35966 -4.3145943 -4.3212538 -4.3692675 -4.3961911 -4.3944764 -4.3867803 -4.3878808 -4.3940167 -4.3866677][-4.362411 -4.3638997 -4.3988476 -4.4131536 -4.3637071 -4.284863 -4.2239227 -4.2211261 -4.2741151 -4.3081932 -4.3173289 -4.3310094 -4.3467412 -4.3356204 -4.290699][-4.3698268 -4.3640828 -4.3763208 -4.3634949 -4.2950664 -4.2077165 -4.1364961 -4.1173711 -4.1651936 -4.2031255 -4.2210708 -4.2561846 -4.292623 -4.2754188 -4.1996241][-4.3795595 -4.36966 -4.359921 -4.3205595 -4.2380462 -4.1426663 -4.054781 -4.0069032 -4.0432014 -4.0967579 -4.141993 -4.2077932 -4.2668939 -4.2544651 -4.1635852][-4.3867273 -4.3809052 -4.3611393 -4.3053575 -4.215363 -4.1144605 -4.0074525 -3.9264483 -3.9485674 -4.02511 -4.1102839 -4.2032757 -4.2714486 -4.2642455 -4.1767688][-4.3862314 -4.3897 -4.3735304 -4.3171277 -4.2305913 -4.1311088 -4.0145464 -3.9103897 -3.9159718 -4.0060468 -4.1229095 -4.2305441 -4.2926126 -4.2850804 -4.214447][-4.3749237 -4.3848906 -4.376152 -4.3287711 -4.2523766 -4.156682 -4.0386143 -3.925117 -3.9068983 -3.9863763 -4.1092544 -4.2135668 -4.2619333 -4.2521291 -4.2064748][-4.3603716 -4.3755531 -4.3779874 -4.3472209 -4.2877541 -4.1987858 -4.0831704 -3.9715109 -3.9370058 -3.9912407 -4.0896578 -4.1644673 -4.1861653 -4.16556 -4.141788][-4.3493986 -4.3723278 -4.3918657 -4.3862796 -4.348628 -4.2672758 -4.1488976 -4.0377197 -3.9954107 -4.0260983 -4.0865631 -4.12337 -4.1174674 -4.089963 -4.0806966][-4.34069 -4.3699584 -4.4057751 -4.4218807 -4.3973818 -4.3164563 -4.1883326 -4.0754242 -4.0366564 -4.0603142 -4.0991917 -4.1149859 -4.1017933 -4.0816493 -4.0838723][-4.3298745 -4.3632126 -4.4114089 -4.4427667 -4.4248514 -4.3424759 -4.2116213 -4.1058397 -4.0770693 -4.1038351 -4.1370893 -4.1532731 -4.1540031 -4.1572742 -4.1737385][-4.3187914 -4.3506331 -4.4058266 -4.449996 -4.4451871 -4.3765736 -4.2661848 -4.1824322 -4.1626558 -4.1811047 -4.1987343 -4.2127795 -4.2341566 -4.2653775 -4.2963729][-4.3089576 -4.3349743 -4.3896823 -4.4416513 -4.4511814 -4.4072895 -4.3336139 -4.2793593 -4.2691913 -4.277173 -4.2799253 -4.2916183 -4.3306632 -4.3810725 -4.4184608]]...]
INFO - root - 2017-12-07 14:27:40.666042: step 23610, loss = 21.66, batch loss = 21.57 (8.4 examples/sec; 0.955 sec/batch; 81h:55m:09s remains)
INFO - root - 2017-12-07 14:27:49.967494: step 23620, loss = 21.15, batch loss = 21.07 (8.8 examples/sec; 0.907 sec/batch; 77h:48m:45s remains)
INFO - root - 2017-12-07 14:27:59.295831: step 23630, loss = 21.38, batch loss = 21.29 (8.2 examples/sec; 0.978 sec/batch; 83h:55m:13s remains)
INFO - root - 2017-12-07 14:28:08.653914: step 23640, loss = 21.45, batch loss = 21.37 (8.3 examples/sec; 0.966 sec/batch; 82h:54m:17s remains)
INFO - root - 2017-12-07 14:28:17.955036: step 23650, loss = 21.38, batch loss = 21.30 (8.2 examples/sec; 0.976 sec/batch; 83h:43m:41s remains)
INFO - root - 2017-12-07 14:28:27.332009: step 23660, loss = 21.60, batch loss = 21.52 (8.6 examples/sec; 0.931 sec/batch; 79h:53m:56s remains)
INFO - root - 2017-12-07 14:28:36.735717: step 23670, loss = 21.76, batch loss = 21.67 (8.8 examples/sec; 0.909 sec/batch; 77h:56m:13s remains)
INFO - root - 2017-12-07 14:28:46.174563: step 23680, loss = 21.22, batch loss = 21.13 (8.6 examples/sec; 0.931 sec/batch; 79h:53m:39s remains)
INFO - root - 2017-12-07 14:28:55.548696: step 23690, loss = 21.41, batch loss = 21.32 (8.5 examples/sec; 0.940 sec/batch; 80h:38m:19s remains)
INFO - root - 2017-12-07 14:29:04.996893: step 23700, loss = 21.42, batch loss = 21.33 (8.5 examples/sec; 0.943 sec/batch; 80h:54m:04s remains)
2017-12-07 14:29:05.941281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4249406 -4.409925 -4.3457961 -4.2755523 -4.2662997 -4.344696 -4.4526191 -4.542264 -4.5891247 -4.590795 -4.5515041 -4.4805393 -4.4177146 -4.4016123 -4.4447112][-4.5031939 -4.500607 -4.4641247 -4.40591 -4.38828 -4.4377966 -4.5107279 -4.5748868 -4.6174426 -4.6308036 -4.61724 -4.5867691 -4.5618391 -4.5700369 -4.62017][-4.5451651 -4.5582514 -4.5428762 -4.490356 -4.4502964 -4.4516344 -4.4734421 -4.5031266 -4.5415821 -4.5748382 -4.59106 -4.589633 -4.580852 -4.5956116 -4.6445][-4.5657053 -4.5919952 -4.5852785 -4.5294766 -4.4603691 -4.4036312 -4.3592925 -4.3370914 -4.3623214 -4.4205775 -4.4722614 -4.4899459 -4.4792604 -4.4839153 -4.5273976][-4.5697637 -4.5952244 -4.5788851 -4.5069928 -4.402873 -4.291841 -4.191155 -4.1277523 -4.1508102 -4.2397914 -4.3198323 -4.3431993 -4.318099 -4.3043146 -4.3411932][-4.5305982 -4.5471706 -4.5145073 -4.4178143 -4.28026 -4.1324191 -4.0038185 -3.9329607 -3.9804318 -4.1104388 -4.2183838 -4.2477994 -4.210144 -4.1746435 -4.1902723][-4.4513512 -4.474256 -4.4350958 -4.3166289 -4.1522884 -3.9813876 -3.8403251 -3.7776146 -3.8449118 -3.9965546 -4.1193075 -4.160068 -4.1309476 -4.0921011 -4.0917053][-4.3734221 -4.4122934 -4.3806038 -4.2529397 -4.0763965 -3.8940759 -3.7448995 -3.6804814 -3.7280121 -3.8489037 -3.9555535 -4.0057397 -4.0078816 -3.9967968 -4.0078316][-4.324038 -4.3781781 -4.3699169 -4.260808 -4.1034741 -3.9401507 -3.8114448 -3.7615294 -3.7817459 -3.843312 -3.8935366 -3.9136221 -3.9188075 -3.9259832 -3.950223][-4.2996669 -4.3671412 -4.3927493 -4.3261881 -4.2150679 -4.0953126 -4.0106 -3.9901385 -3.986284 -3.9822264 -3.9559412 -3.9184389 -3.9036827 -3.9117341 -3.9427814][-4.2823482 -4.3501105 -4.4049726 -4.3858309 -4.3278589 -4.2570848 -4.2188244 -4.2249656 -4.2062321 -4.1559839 -4.0736265 -3.9916606 -3.9536448 -3.9453247 -3.9715827][-4.26042 -4.3327742 -4.4158344 -4.4406576 -4.427485 -4.3971314 -4.3974805 -4.4267654 -4.4125905 -4.3557177 -4.2582779 -4.1532817 -4.0826316 -4.0328403 -4.0344582][-4.248311 -4.3368244 -4.44517 -4.50047 -4.5117054 -4.5006547 -4.5127668 -4.5428462 -4.5330377 -4.4908719 -4.4131866 -4.3163185 -4.2294154 -4.1479759 -4.1339788][-4.2571769 -4.3587279 -4.4821839 -4.5555716 -4.5797648 -4.5755267 -4.577867 -4.5848136 -4.5659056 -4.5393429 -4.4970832 -4.4324961 -4.3577652 -4.2766232 -4.2569118][-4.2686038 -4.3776064 -4.5087123 -4.5945439 -4.629467 -4.6321297 -4.6257043 -4.6131639 -4.5852337 -4.5643649 -4.5427155 -4.502409 -4.44993 -4.3896475 -4.3701916]]...]
INFO - root - 2017-12-07 14:29:15.381138: step 23710, loss = 21.31, batch loss = 21.22 (9.1 examples/sec; 0.882 sec/batch; 75h:36m:54s remains)
INFO - root - 2017-12-07 14:29:24.876971: step 23720, loss = 21.60, batch loss = 21.52 (8.2 examples/sec; 0.971 sec/batch; 83h:15m:39s remains)
INFO - root - 2017-12-07 14:29:34.277564: step 23730, loss = 21.12, batch loss = 21.03 (8.4 examples/sec; 0.954 sec/batch; 81h:51m:02s remains)
INFO - root - 2017-12-07 14:29:43.692462: step 23740, loss = 21.35, batch loss = 21.26 (9.0 examples/sec; 0.887 sec/batch; 76h:03m:28s remains)
INFO - root - 2017-12-07 14:29:53.110774: step 23750, loss = 21.57, batch loss = 21.48 (8.8 examples/sec; 0.905 sec/batch; 77h:38m:18s remains)
INFO - root - 2017-12-07 14:30:02.561232: step 23760, loss = 21.26, batch loss = 21.18 (8.5 examples/sec; 0.938 sec/batch; 80h:25m:57s remains)
INFO - root - 2017-12-07 14:30:11.930009: step 23770, loss = 21.24, batch loss = 21.15 (8.3 examples/sec; 0.968 sec/batch; 82h:59m:56s remains)
INFO - root - 2017-12-07 14:30:21.419315: step 23780, loss = 21.87, batch loss = 21.79 (8.2 examples/sec; 0.977 sec/batch; 83h:45m:57s remains)
INFO - root - 2017-12-07 14:30:30.793071: step 23790, loss = 21.39, batch loss = 21.31 (8.7 examples/sec; 0.917 sec/batch; 78h:36m:40s remains)
INFO - root - 2017-12-07 14:30:40.042328: step 23800, loss = 21.19, batch loss = 21.11 (9.2 examples/sec; 0.873 sec/batch; 74h:52m:06s remains)
2017-12-07 14:30:41.003499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4973679 -4.5470266 -4.58767 -4.6050344 -4.6001878 -4.5956273 -4.5966 -4.5916095 -4.5660095 -4.507484 -4.41629 -4.31608 -4.2507339 -4.2453837 -4.294414][-4.5149875 -4.5524521 -4.5758524 -4.5836673 -4.5829582 -4.58659 -4.5838084 -4.5687723 -4.5432034 -4.4961038 -4.4210844 -4.3337731 -4.267776 -4.2539949 -4.2926579][-4.5363846 -4.550621 -4.5553689 -4.5620365 -4.5716324 -4.5790358 -4.5618205 -4.5293303 -4.5085931 -4.4909058 -4.4560928 -4.4053373 -4.3553352 -4.3349476 -4.3541307][-4.5379763 -4.5436912 -4.54442 -4.5497046 -4.5490279 -4.5321784 -4.4851913 -4.433578 -4.4244905 -4.454267 -4.4826975 -4.4915686 -4.4725285 -4.4467378 -4.4395022][-4.5006051 -4.5289268 -4.5425091 -4.5306067 -4.4783731 -4.3955016 -4.3029504 -4.2363334 -4.2445078 -4.325448 -4.4249125 -4.50259 -4.5248871 -4.5018616 -4.4725962][-4.3794813 -4.449204 -4.4943132 -4.463068 -4.3446021 -4.1811695 -4.0416479 -3.9690125 -3.9955866 -4.1147461 -4.2722964 -4.4084287 -4.4727759 -4.465126 -4.430706][-4.15648 -4.2740092 -4.3741016 -4.3549352 -4.2019892 -3.9839268 -3.8125312 -3.7429481 -3.7856989 -3.9211836 -4.104969 -4.2719746 -4.36719 -4.3836083 -4.3624864][-3.916503 -4.074657 -4.2384748 -4.2670684 -4.1334791 -3.9144454 -3.74119 -3.6792614 -3.7236331 -3.8455184 -4.0175934 -4.1847367 -4.2975092 -4.342494 -4.34802][-3.7751627 -3.9517224 -4.1602511 -4.2482319 -4.1753945 -4.0033741 -3.8570619 -3.7967606 -3.8154511 -3.8940487 -4.0272546 -4.1740823 -4.2897682 -4.3579574 -4.3880491][-3.8046527 -3.9700284 -4.1830363 -4.3025074 -4.2860036 -4.1735225 -4.0616064 -3.9921906 -3.9689307 -3.9946208 -4.0822754 -4.2006435 -4.3079557 -4.3863587 -4.4298716][-3.9826455 -4.1169262 -4.3017745 -4.41658 -4.4217057 -4.3456407 -4.2535677 -4.1731682 -4.1196446 -4.1134539 -4.1732111 -4.2664094 -4.3542705 -4.42642 -4.4653077][-4.2200007 -4.3168278 -4.4554052 -4.5416117 -4.5403824 -4.4780145 -4.4007697 -4.3279576 -4.2768741 -4.2700648 -4.3155141 -4.3783402 -4.4290195 -4.4738593 -4.4943004][-4.41221 -4.4828043 -4.5798135 -4.6378589 -4.6272879 -4.5724974 -4.5120358 -4.4647784 -4.440176 -4.4457674 -4.4720907 -4.4893227 -4.4892187 -4.5001807 -4.5041828][-4.4954829 -4.5577641 -4.6373606 -4.6860504 -4.6791677 -4.6358824 -4.5934663 -4.572382 -4.5729485 -4.5820088 -4.5768161 -4.5417962 -4.4945803 -4.48304 -4.4866719][-4.4524927 -4.523303 -4.6082468 -4.6636338 -4.6668687 -4.6356516 -4.6062603 -4.6030374 -4.6194263 -4.6276751 -4.5980544 -4.52916 -4.4577045 -4.4406409 -4.4556594]]...]
INFO - root - 2017-12-07 14:30:50.138483: step 23810, loss = 21.14, batch loss = 21.06 (8.0 examples/sec; 1.004 sec/batch; 86h:07m:10s remains)
INFO - root - 2017-12-07 14:30:59.405994: step 23820, loss = 21.34, batch loss = 21.25 (8.6 examples/sec; 0.934 sec/batch; 80h:07m:27s remains)
INFO - root - 2017-12-07 14:31:08.790234: step 23830, loss = 21.57, batch loss = 21.48 (8.7 examples/sec; 0.915 sec/batch; 78h:25m:42s remains)
INFO - root - 2017-12-07 14:31:18.116862: step 23840, loss = 21.75, batch loss = 21.66 (8.8 examples/sec; 0.906 sec/batch; 77h:39m:06s remains)
INFO - root - 2017-12-07 14:31:27.529648: step 23850, loss = 21.39, batch loss = 21.30 (8.5 examples/sec; 0.937 sec/batch; 80h:18m:23s remains)
INFO - root - 2017-12-07 14:31:36.848176: step 23860, loss = 21.43, batch loss = 21.34 (9.2 examples/sec; 0.869 sec/batch; 74h:32m:33s remains)
INFO - root - 2017-12-07 14:31:46.216628: step 23870, loss = 21.38, batch loss = 21.29 (9.2 examples/sec; 0.872 sec/batch; 74h:45m:21s remains)
INFO - root - 2017-12-07 14:31:55.542076: step 23880, loss = 21.26, batch loss = 21.17 (8.5 examples/sec; 0.944 sec/batch; 80h:56m:51s remains)
INFO - root - 2017-12-07 14:32:04.870851: step 23890, loss = 21.58, batch loss = 21.50 (8.6 examples/sec; 0.930 sec/batch; 79h:45m:32s remains)
INFO - root - 2017-12-07 14:32:14.117452: step 23900, loss = 21.59, batch loss = 21.51 (8.6 examples/sec; 0.933 sec/batch; 79h:58m:57s remains)
2017-12-07 14:32:15.045551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.419724 -4.4233637 -4.4218788 -4.4320335 -4.4334555 -4.4306459 -4.4287577 -4.4192772 -4.4364557 -4.4919491 -4.5517859 -4.5956192 -4.6051321 -4.5706177 -4.4949265][-4.4057288 -4.3991618 -4.391757 -4.4107556 -4.4124088 -4.3887572 -4.3409982 -4.269465 -4.2662673 -4.363801 -4.4899664 -4.5934973 -4.6353197 -4.6112494 -4.53582][-4.4137406 -4.3871326 -4.3640985 -4.3893881 -4.3968964 -4.3559151 -4.24975 -4.0942974 -4.0677691 -4.2140255 -4.4086442 -4.5784068 -4.6584058 -4.64675 -4.5683618][-4.3826833 -4.3279672 -4.2888842 -4.3159947 -4.33402 -4.2960892 -4.1592593 -3.9391062 -3.9053955 -4.1133108 -4.3751211 -4.6058922 -4.7268224 -4.724328 -4.6272554][-4.3075242 -4.2159567 -4.1575117 -4.1731234 -4.1921549 -4.1693187 -4.0312648 -3.7689729 -3.7302761 -3.999177 -4.319777 -4.5987339 -4.7687597 -4.7922072 -4.6863542][-4.2461758 -4.1279764 -4.0623384 -4.06153 -4.0580406 -4.0213022 -3.8678939 -3.5541396 -3.4842534 -3.7925599 -4.1569257 -4.4674716 -4.6860166 -4.7578464 -4.677916][-4.2423663 -4.1326823 -4.0837517 -4.0645542 -4.0128107 -3.9221787 -3.7275257 -3.3650477 -3.2622349 -3.599443 -4.0000892 -4.326128 -4.5687485 -4.6731763 -4.6232276][-4.27782 -4.1909518 -4.1751122 -4.1554503 -4.0732927 -3.9427712 -3.7302327 -3.370553 -3.2699113 -3.6285117 -4.0509825 -4.3678689 -4.5839748 -4.66565 -4.6042728][-4.3089352 -4.2292805 -4.2434759 -4.2516208 -4.1865544 -4.0624061 -3.874382 -3.5642931 -3.4874206 -3.8485041 -4.271244 -4.5636306 -4.7311511 -4.7603865 -4.653451][-4.3622103 -4.2738905 -4.2923541 -4.3256745 -4.294075 -4.1952758 -4.0425844 -3.7894335 -3.7233677 -4.0447226 -4.4313707 -4.6933475 -4.8287568 -4.8310771 -4.6988926][-4.4497623 -4.3566332 -4.3571224 -4.3976703 -4.3938961 -4.3237367 -4.1999969 -3.9855833 -3.9049015 -4.1417747 -4.4483891 -4.6582241 -4.7718434 -4.7815237 -4.6725445][-4.5200295 -4.44936 -4.4439268 -4.4878922 -4.5092931 -4.4739962 -4.3779817 -4.1931291 -4.0869007 -4.2185988 -4.4172187 -4.5591197 -4.6485124 -4.6722631 -4.6068354][-4.5458622 -4.5148892 -4.5190992 -4.5605946 -4.5912895 -4.5826483 -4.5199227 -4.3752294 -4.2673821 -4.3211718 -4.4281158 -4.5111637 -4.575139 -4.5991883 -4.5611525][-4.524642 -4.5263348 -4.5378 -4.5642786 -4.5841389 -4.5821719 -4.5447373 -4.4442177 -4.3620028 -4.3907256 -4.4539127 -4.5081844 -4.5584612 -4.5741458 -4.53946][-4.4780622 -4.4911942 -4.5026274 -4.5128264 -4.5131259 -4.5002975 -4.4696856 -4.3997779 -4.3474717 -4.3824492 -4.4411826 -4.493742 -4.544282 -4.555419 -4.5117855]]...]
INFO - root - 2017-12-07 14:32:24.385892: step 23910, loss = 21.50, batch loss = 21.41 (9.0 examples/sec; 0.886 sec/batch; 75h:55m:58s remains)
INFO - root - 2017-12-07 14:32:33.654258: step 23920, loss = 21.36, batch loss = 21.27 (8.8 examples/sec; 0.909 sec/batch; 77h:53m:05s remains)
INFO - root - 2017-12-07 14:32:43.090231: step 23930, loss = 21.30, batch loss = 21.21 (8.2 examples/sec; 0.973 sec/batch; 83h:23m:44s remains)
INFO - root - 2017-12-07 14:32:52.385471: step 23940, loss = 21.18, batch loss = 21.10 (8.6 examples/sec; 0.933 sec/batch; 79h:58m:02s remains)
INFO - root - 2017-12-07 14:33:01.727287: step 23950, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.950 sec/batch; 81h:24m:45s remains)
INFO - root - 2017-12-07 14:33:11.216109: step 23960, loss = 21.35, batch loss = 21.27 (7.9 examples/sec; 1.011 sec/batch; 86h:40m:32s remains)
INFO - root - 2017-12-07 14:33:20.693766: step 23970, loss = 21.46, batch loss = 21.38 (8.4 examples/sec; 0.954 sec/batch; 81h:46m:01s remains)
INFO - root - 2017-12-07 14:33:29.883330: step 23980, loss = 21.53, batch loss = 21.45 (8.1 examples/sec; 0.987 sec/batch; 84h:32m:43s remains)
INFO - root - 2017-12-07 14:33:39.188160: step 23990, loss = 21.70, batch loss = 21.62 (8.7 examples/sec; 0.920 sec/batch; 78h:50m:04s remains)
INFO - root - 2017-12-07 14:33:48.654997: step 24000, loss = 21.59, batch loss = 21.51 (8.2 examples/sec; 0.972 sec/batch; 83h:19m:00s remains)
2017-12-07 14:33:49.586974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4352012 -4.510077 -4.5234122 -4.4544268 -4.3740349 -4.3261714 -4.2986431 -4.3028731 -4.3316345 -4.367135 -4.4110756 -4.4395523 -4.4424276 -4.4650846 -4.4884887][-4.4044366 -4.4527955 -4.4459782 -4.3835711 -4.31577 -4.2693777 -4.2434449 -4.2661247 -4.33183 -4.3976626 -4.4459968 -4.4576325 -4.4476609 -4.4661813 -4.488554][-4.3867216 -4.417501 -4.4074917 -4.3712015 -4.3298173 -4.2880263 -4.2612028 -4.2959785 -4.3912649 -4.4756274 -4.510303 -4.4919004 -4.4630132 -4.4732447 -4.4876328][-4.3662477 -4.3912373 -4.3926864 -4.3864694 -4.3617511 -4.3116364 -4.2629714 -4.2820892 -4.3812637 -4.4666839 -4.4796863 -4.4372358 -4.4116106 -4.4483662 -4.4862661][-4.3496141 -4.3676906 -4.3775954 -4.3816791 -4.3473592 -4.2655249 -4.1668358 -4.1373038 -4.214313 -4.299202 -4.31382 -4.2783437 -4.286026 -4.3816967 -4.4729977][-4.3586245 -4.3737078 -4.3774023 -4.3644443 -4.2984734 -4.1742249 -4.0126009 -3.9166777 -3.9576554 -4.0573483 -4.1197734 -4.1351824 -4.189106 -4.3309011 -4.45837][-4.4103332 -4.4322557 -4.42413 -4.3856587 -4.2930079 -4.1412125 -3.9327831 -3.7776356 -3.7749457 -3.8853266 -4.0167189 -4.1088123 -4.2063913 -4.3517346 -4.4632139][-4.5013962 -4.5348043 -4.5170736 -4.45928 -4.3550758 -4.1980247 -3.9760959 -3.7945235 -3.7614655 -3.8744447 -4.0658464 -4.2363849 -4.3715043 -4.4879222 -4.5368228][-4.5787992 -4.6216512 -4.6021714 -4.5367589 -4.4314194 -4.2844944 -4.0796967 -3.9057353 -3.8613653 -3.9716852 -4.1997256 -4.428009 -4.5941329 -4.6831946 -4.6682453][-4.5795059 -4.6323576 -4.6247163 -4.5713582 -4.4851809 -4.3720016 -4.2149148 -4.0771451 -4.0338078 -4.1247616 -4.3452916 -4.58041 -4.7437463 -4.8085003 -4.7605057][-4.5195928 -4.5774713 -4.5941505 -4.5762076 -4.5333629 -4.4730411 -4.3805642 -4.289772 -4.2478294 -4.3042583 -4.4760909 -4.6583529 -4.769855 -4.8017311 -4.7488956][-4.4960232 -4.5510263 -4.58688 -4.601923 -4.5999756 -4.5845046 -4.5402174 -4.4771953 -4.4290223 -4.4535093 -4.5692196 -4.6780558 -4.72211 -4.7208366 -4.6747293][-4.51327 -4.5602822 -4.5981994 -4.6262918 -4.6446815 -4.6471882 -4.6140051 -4.54848 -4.4909358 -4.5098267 -4.6073284 -4.6780567 -4.6862411 -4.6705208 -4.6296139][-4.4951806 -4.530549 -4.5591307 -4.582593 -4.6022305 -4.6026039 -4.5592518 -4.4774508 -4.4189568 -4.4657011 -4.5961637 -4.6813745 -4.6942282 -4.6835012 -4.64126][-4.4343657 -4.4599504 -4.4790587 -4.4947481 -4.5089664 -4.5012546 -4.445437 -4.3525615 -4.301331 -4.3881936 -4.5715137 -4.6964159 -4.7337723 -4.7311463 -4.6802945]]...]
INFO - root - 2017-12-07 14:33:58.938902: step 24010, loss = 21.40, batch loss = 21.32 (8.2 examples/sec; 0.975 sec/batch; 83h:35m:29s remains)
INFO - root - 2017-12-07 14:34:08.442635: step 24020, loss = 21.26, batch loss = 21.17 (8.6 examples/sec; 0.925 sec/batch; 79h:18m:08s remains)
INFO - root - 2017-12-07 14:34:17.834221: step 24030, loss = 21.22, batch loss = 21.14 (8.6 examples/sec; 0.935 sec/batch; 80h:05m:47s remains)
INFO - root - 2017-12-07 14:34:27.181711: step 24040, loss = 21.56, batch loss = 21.47 (9.1 examples/sec; 0.883 sec/batch; 75h:41m:11s remains)
INFO - root - 2017-12-07 14:34:36.461509: step 24050, loss = 21.57, batch loss = 21.49 (9.0 examples/sec; 0.887 sec/batch; 76h:00m:50s remains)
INFO - root - 2017-12-07 14:34:46.047711: step 24060, loss = 21.83, batch loss = 21.75 (8.1 examples/sec; 0.983 sec/batch; 84h:14m:59s remains)
INFO - root - 2017-12-07 14:34:55.436532: step 24070, loss = 21.66, batch loss = 21.58 (8.6 examples/sec; 0.930 sec/batch; 79h:43m:09s remains)
INFO - root - 2017-12-07 14:35:04.813113: step 24080, loss = 21.42, batch loss = 21.34 (8.0 examples/sec; 1.000 sec/batch; 85h:38m:01s remains)
INFO - root - 2017-12-07 14:35:14.284277: step 24090, loss = 21.44, batch loss = 21.35 (8.3 examples/sec; 0.967 sec/batch; 82h:52m:18s remains)
INFO - root - 2017-12-07 14:35:23.609689: step 24100, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.952 sec/batch; 81h:31m:27s remains)
2017-12-07 14:35:24.514933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5021076 -4.4702888 -4.4452538 -4.4578562 -4.5060887 -4.5390978 -4.5214972 -4.4704 -4.4285388 -4.4248033 -4.4606919 -4.5060196 -4.5196385 -4.4806957 -4.419066][-4.5166512 -4.5079374 -4.49805 -4.5062585 -4.51993 -4.4939175 -4.4147081 -4.3248844 -4.2779284 -4.2942505 -4.3636994 -4.4489965 -4.4983783 -4.480999 -4.4254174][-4.4782786 -4.4920278 -4.4973125 -4.5022993 -4.4820204 -4.3979297 -4.2641692 -4.1503258 -4.1130981 -4.1552534 -4.2585731 -4.3867397 -4.4786162 -4.4897342 -4.4426661][-4.4055796 -4.4541163 -4.4836984 -4.488925 -4.4358726 -4.2977448 -4.1179051 -3.9869347 -3.9580221 -4.0119748 -4.1301455 -4.2895637 -4.4252148 -4.4764385 -4.4544029][-4.3184304 -4.4041791 -4.4606171 -4.4674673 -4.389441 -4.215179 -4.006608 -3.865346 -3.8382814 -3.8822088 -3.9865272 -4.1497064 -4.3116322 -4.4027314 -4.42024][-4.2466016 -4.3529224 -4.4251785 -4.4336386 -4.3519855 -4.1784325 -3.97433 -3.837333 -3.8029087 -3.8162074 -3.8787546 -4.0093889 -4.1637464 -4.2769632 -4.3348832][-4.2139015 -4.3280225 -4.4040217 -4.4171472 -4.3575349 -4.2241797 -4.059516 -3.9377215 -3.8833442 -3.8545346 -3.86552 -3.9406145 -4.0545487 -4.160109 -4.242527][-4.242475 -4.3456306 -4.4163742 -4.4373918 -4.4125633 -4.3381734 -4.22907 -4.1258516 -4.0463009 -3.9801271 -3.948909 -3.967387 -4.0268912 -4.1052351 -4.196238][-4.3161888 -4.3927069 -4.4469643 -4.4737625 -4.4819636 -4.4636035 -4.4116392 -4.3353748 -4.2471738 -4.165534 -4.1106195 -4.0829983 -4.0865712 -4.1244154 -4.2055297][-4.3976145 -4.43983 -4.4709344 -4.4956059 -4.5225387 -4.5421863 -4.5366788 -4.4936676 -4.4211545 -4.34938 -4.2917261 -4.2411613 -4.2085714 -4.2108378 -4.2690029][-4.4469967 -4.4617567 -4.4722939 -4.4869509 -4.5140023 -4.546917 -4.5679612 -4.5565324 -4.5140224 -4.4650736 -4.4170132 -4.3669329 -4.3287334 -4.3194642 -4.3604822][-4.4348865 -4.432951 -4.4339695 -4.4412355 -4.4606376 -4.4898343 -4.5173173 -4.5266008 -4.513237 -4.4879241 -4.4544363 -4.417532 -4.391521 -4.3877177 -4.4205074][-4.3784842 -4.3679204 -4.3662438 -4.3708377 -4.382091 -4.3979483 -4.4143119 -4.4252195 -4.4260511 -4.4171042 -4.3997312 -4.3819556 -4.3741169 -4.3819108 -4.4135113][-4.3086338 -4.29484 -4.2947521 -4.3001356 -4.3054624 -4.3067751 -4.3046551 -4.3037653 -4.305687 -4.3077226 -4.3072939 -4.3067732 -4.312263 -4.3290253 -4.3625879][-4.2463717 -4.2359457 -4.2429795 -4.2527933 -4.2555943 -4.2456126 -4.22586 -4.2096157 -4.2076278 -4.2195082 -4.2366557 -4.2498827 -4.2626925 -4.2839837 -4.3202381]]...]
INFO - root - 2017-12-07 14:35:33.904024: step 24110, loss = 21.53, batch loss = 21.45 (9.1 examples/sec; 0.882 sec/batch; 75h:35m:19s remains)
INFO - root - 2017-12-07 14:35:43.216843: step 24120, loss = 21.40, batch loss = 21.32 (8.6 examples/sec; 0.935 sec/batch; 80h:05m:59s remains)
INFO - root - 2017-12-07 14:35:52.528499: step 24130, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.947 sec/batch; 81h:06m:37s remains)
INFO - root - 2017-12-07 14:36:01.961064: step 24140, loss = 21.12, batch loss = 21.04 (8.4 examples/sec; 0.952 sec/batch; 81h:35m:09s remains)
INFO - root - 2017-12-07 14:36:11.195408: step 24150, loss = 21.13, batch loss = 21.04 (8.4 examples/sec; 0.955 sec/batch; 81h:48m:25s remains)
INFO - root - 2017-12-07 14:36:20.496599: step 24160, loss = 21.42, batch loss = 21.34 (8.2 examples/sec; 0.977 sec/batch; 83h:42m:35s remains)
INFO - root - 2017-12-07 14:36:29.944438: step 24170, loss = 21.09, batch loss = 21.01 (8.4 examples/sec; 0.951 sec/batch; 81h:26m:27s remains)
INFO - root - 2017-12-07 14:36:39.522404: step 24180, loss = 21.17, batch loss = 21.09 (8.0 examples/sec; 0.995 sec/batch; 85h:13m:58s remains)
INFO - root - 2017-12-07 14:36:48.808325: step 24190, loss = 21.36, batch loss = 21.28 (8.9 examples/sec; 0.902 sec/batch; 77h:15m:49s remains)
INFO - root - 2017-12-07 14:36:58.298766: step 24200, loss = 21.35, batch loss = 21.27 (8.9 examples/sec; 0.896 sec/batch; 76h:43m:24s remains)
2017-12-07 14:36:59.277256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4616365 -4.2587161 -4.1913376 -4.2970386 -4.4554448 -4.5465918 -4.5640554 -4.5426235 -4.5303707 -4.5623112 -4.6054664 -4.624886 -4.6495528 -4.6959019 -4.7625146][-4.3877349 -4.2034397 -4.1597366 -4.2825379 -4.452867 -4.5558572 -4.5845208 -4.5726342 -4.5646172 -4.5939579 -4.6282806 -4.6461458 -4.6782732 -4.7203627 -4.7637615][-4.3509021 -4.2302465 -4.2194638 -4.3262086 -4.4604473 -4.5448794 -4.5731888 -4.5696158 -4.5674071 -4.5879574 -4.6079459 -4.6267052 -4.6672335 -4.6999335 -4.7104626][-4.3586164 -4.3072352 -4.3112097 -4.3590231 -4.4086976 -4.4426517 -4.4641695 -4.4797287 -4.5005021 -4.5279942 -4.5472646 -4.5779562 -4.6313305 -4.6583543 -4.6419325][-4.3742738 -4.3550038 -4.337676 -4.2984128 -4.2464571 -4.2204823 -4.241642 -4.2944489 -4.3570323 -4.4109669 -4.4476256 -4.5015903 -4.5786963 -4.6180487 -4.5987325][-4.369771 -4.3417168 -4.2787714 -4.1569185 -4.02204 -3.9459171 -3.9622948 -4.0415764 -4.1366539 -4.2181077 -4.2800231 -4.363472 -4.4746366 -4.5488358 -4.5587335][-4.3343315 -4.2831707 -4.1885762 -4.026814 -3.8572843 -3.7490473 -3.74399 -3.8166823 -3.9155731 -4.0124426 -4.1020465 -4.2205181 -4.3678875 -4.4776597 -4.5195951][-4.2886915 -4.2335052 -4.1459885 -3.9968152 -3.8406892 -3.7283511 -3.7015982 -3.7500343 -3.8299334 -3.9240351 -4.0305634 -4.1697316 -4.3301444 -4.4497843 -4.5027938][-4.2571778 -4.2129917 -4.1559 -4.0506625 -3.9398546 -3.8572588 -3.8336689 -3.8641353 -3.9157791 -3.9864473 -4.0833564 -4.2096024 -4.348259 -4.4489369 -4.4955335][-4.2528362 -4.2288332 -4.2128706 -4.1659627 -4.1154747 -4.0792818 -4.074151 -4.0951791 -4.1199145 -4.157548 -4.2246161 -4.3167472 -4.4142332 -4.4764118 -4.4997997][-4.2979755 -4.2880106 -4.30773 -4.3157883 -4.3193054 -4.3219876 -4.3343296 -4.3520169 -4.3594465 -4.369122 -4.3997583 -4.4492903 -4.5053687 -4.53366 -4.5364084][-4.3754873 -4.3532381 -4.3819666 -4.4247026 -4.4608 -4.4785042 -4.4926395 -4.5078425 -4.5134258 -4.5132689 -4.5171304 -4.5299244 -4.5573168 -4.5721459 -4.5769553][-4.4637003 -4.3920588 -4.3944135 -4.4461474 -4.4968867 -4.5154018 -4.5220017 -4.5381494 -4.5562253 -4.5662727 -4.5602274 -4.5486045 -4.5598683 -4.5776505 -4.6029835][-4.5514388 -4.401557 -4.3532381 -4.3997951 -4.4625554 -4.4833279 -4.4825964 -4.4982343 -4.5257678 -4.5496778 -4.544847 -4.5226717 -4.5291085 -4.5602188 -4.6167026][-4.5686021 -4.3514314 -4.2623878 -4.3066506 -4.3813915 -4.4079719 -4.4036622 -4.4123721 -4.4362564 -4.46421 -4.4648113 -4.4460526 -4.4589643 -4.5072265 -4.59149]]...]
INFO - root - 2017-12-07 14:37:08.551305: step 24210, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.963 sec/batch; 82h:26m:09s remains)
INFO - root - 2017-12-07 14:37:17.876804: step 24220, loss = 21.40, batch loss = 21.32 (9.0 examples/sec; 0.892 sec/batch; 76h:22m:09s remains)
INFO - root - 2017-12-07 14:37:26.899613: step 24230, loss = 21.28, batch loss = 21.19 (8.8 examples/sec; 0.912 sec/batch; 78h:06m:20s remains)
INFO - root - 2017-12-07 14:37:36.282912: step 24240, loss = 21.83, batch loss = 21.75 (8.9 examples/sec; 0.901 sec/batch; 77h:10m:39s remains)
INFO - root - 2017-12-07 14:37:45.883561: step 24250, loss = 21.17, batch loss = 21.09 (8.6 examples/sec; 0.926 sec/batch; 79h:16m:41s remains)
INFO - root - 2017-12-07 14:37:55.268957: step 24260, loss = 21.75, batch loss = 21.67 (8.3 examples/sec; 0.959 sec/batch; 82h:05m:32s remains)
INFO - root - 2017-12-07 14:38:04.644543: step 24270, loss = 21.68, batch loss = 21.60 (8.7 examples/sec; 0.920 sec/batch; 78h:44m:45s remains)
INFO - root - 2017-12-07 14:38:14.033809: step 24280, loss = 21.18, batch loss = 21.09 (8.3 examples/sec; 0.968 sec/batch; 82h:54m:14s remains)
INFO - root - 2017-12-07 14:38:23.354565: step 24290, loss = 21.53, batch loss = 21.45 (7.9 examples/sec; 1.016 sec/batch; 86h:57m:18s remains)
INFO - root - 2017-12-07 14:38:32.768240: step 24300, loss = 21.54, batch loss = 21.45 (7.9 examples/sec; 1.017 sec/batch; 87h:03m:30s remains)
2017-12-07 14:38:33.662445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6030087 -4.5902853 -4.4834666 -4.352128 -4.2619219 -4.2224784 -4.2077365 -4.2735577 -4.4453211 -4.6534886 -4.756083 -4.6843638 -4.5533729 -4.4629841 -4.4120135][-4.6704326 -4.6742468 -4.5722423 -4.4322829 -4.32514 -4.2746258 -4.2525229 -4.3047347 -4.4512849 -4.61969 -4.7012286 -4.6504593 -4.5564337 -4.486443 -4.434948][-4.7025533 -4.7222171 -4.6365008 -4.5072746 -4.3984461 -4.3376312 -4.2991972 -4.3130012 -4.3982959 -4.49553 -4.5398345 -4.5197139 -4.4958367 -4.4796433 -4.4494243][-4.6639671 -4.6755371 -4.5990291 -4.4927859 -4.3966131 -4.3294921 -4.2747226 -4.2546988 -4.2835331 -4.3095055 -4.3124771 -4.3210654 -4.3625917 -4.403254 -4.4083147][-4.5838065 -4.5638666 -4.4760642 -4.3790197 -4.2895417 -4.2126474 -4.14309 -4.1017461 -4.1087141 -4.1064506 -4.09105 -4.1245065 -4.2043939 -4.2734194 -4.3006][-4.5241189 -4.4735727 -4.3671284 -4.2664104 -4.1721234 -4.0755997 -3.9775312 -3.9130282 -3.9247556 -3.9479866 -3.9569817 -4.0232129 -4.1153846 -4.1703258 -4.1755443][-4.5393147 -4.4865904 -4.378262 -4.26843 -4.1512284 -4.0168352 -3.8688607 -3.7665946 -3.7814736 -3.8540637 -3.9233518 -4.0359297 -4.1357183 -4.1526885 -4.10384][-4.6167984 -4.5936007 -4.5017 -4.3806186 -4.2259374 -4.0438418 -3.8470004 -3.7097745 -3.723192 -3.8348768 -3.9601121 -4.1153531 -4.2301297 -4.2248473 -4.13893][-4.694365 -4.7213349 -4.6695652 -4.5633817 -4.3945503 -4.1872082 -3.9612381 -3.7963624 -3.7960687 -3.9134307 -4.0559506 -4.220437 -4.3410587 -4.3343611 -4.2444181][-4.7422881 -4.8231044 -4.8264794 -4.7612753 -4.6057611 -4.3935094 -4.1620264 -3.9918036 -3.9811506 -4.0812221 -4.2047896 -4.3475251 -4.4590373 -4.4619474 -4.3910246][-4.7302213 -4.848135 -4.9039893 -4.8936419 -4.7806325 -4.5919051 -4.3816981 -4.2319794 -4.2237191 -4.3014083 -4.3894596 -4.4872375 -4.5683794 -4.5775504 -4.5330849][-4.6512694 -4.7766705 -4.8640728 -4.9051056 -4.8554611 -4.72239 -4.559597 -4.4459476 -4.4426928 -4.4972696 -4.5504742 -4.59858 -4.6340017 -4.6336355 -4.6058674][-4.534008 -4.6422162 -4.7291861 -4.7912383 -4.7930317 -4.7270117 -4.6292157 -4.5615578 -4.5656891 -4.5973573 -4.6169457 -4.6221762 -4.6148663 -4.5953135 -4.5710411][-4.4166808 -4.4982109 -4.5664997 -4.6178584 -4.6356516 -4.611659 -4.5660281 -4.536695 -4.5484362 -4.565043 -4.5611963 -4.5397477 -4.5091062 -4.4784837 -4.4544053][-4.3277864 -4.3831859 -4.4276214 -4.4527688 -4.4542527 -4.43337 -4.4044056 -4.3913665 -4.4075603 -4.42317 -4.4191484 -4.3992796 -4.3739567 -4.350625 -4.3339944]]...]
INFO - root - 2017-12-07 14:38:43.072865: step 24310, loss = 21.47, batch loss = 21.39 (9.5 examples/sec; 0.842 sec/batch; 72h:02m:49s remains)
INFO - root - 2017-12-07 14:38:52.406477: step 24320, loss = 21.09, batch loss = 21.01 (9.0 examples/sec; 0.889 sec/batch; 76h:05m:23s remains)
INFO - root - 2017-12-07 14:39:01.791760: step 24330, loss = 21.42, batch loss = 21.34 (8.6 examples/sec; 0.929 sec/batch; 79h:32m:39s remains)
INFO - root - 2017-12-07 14:39:11.118856: step 24340, loss = 21.76, batch loss = 21.68 (8.5 examples/sec; 0.939 sec/batch; 80h:20m:22s remains)
INFO - root - 2017-12-07 14:39:20.452858: step 24350, loss = 21.57, batch loss = 21.49 (8.7 examples/sec; 0.923 sec/batch; 79h:01m:24s remains)
INFO - root - 2017-12-07 14:39:29.687952: step 24360, loss = 21.65, batch loss = 21.56 (8.7 examples/sec; 0.916 sec/batch; 78h:22m:58s remains)
INFO - root - 2017-12-07 14:39:39.084043: step 24370, loss = 21.54, batch loss = 21.45 (8.4 examples/sec; 0.955 sec/batch; 81h:42m:27s remains)
INFO - root - 2017-12-07 14:39:48.547649: step 24380, loss = 21.75, batch loss = 21.66 (8.6 examples/sec; 0.925 sec/batch; 79h:09m:52s remains)
INFO - root - 2017-12-07 14:39:58.022268: step 24390, loss = 21.89, batch loss = 21.81 (8.7 examples/sec; 0.922 sec/batch; 78h:53m:22s remains)
INFO - root - 2017-12-07 14:40:07.395324: step 24400, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.941 sec/batch; 80h:30m:29s remains)
2017-12-07 14:40:08.303532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5613356 -4.5770407 -4.5967307 -4.6125884 -4.6265445 -4.5951934 -4.5312476 -4.4846678 -4.4639044 -4.4770627 -4.5183344 -4.5547705 -4.5761232 -4.58475 -4.5776148][-4.6123347 -4.6316142 -4.6464276 -4.65113 -4.66716 -4.6379418 -4.5679221 -4.522119 -4.4982543 -4.5118289 -4.5612745 -4.5977945 -4.6132841 -4.6105433 -4.5860906][-4.5937061 -4.64224 -4.6706471 -4.662683 -4.657094 -4.605154 -4.5226855 -4.4821 -4.4629836 -4.479372 -4.5379095 -4.5782137 -4.5898395 -4.5704179 -4.5177259][-4.5271769 -4.6009984 -4.6355824 -4.6041069 -4.5587535 -4.4717445 -4.3826313 -4.3547406 -4.3464274 -4.3697753 -4.4453969 -4.507966 -4.5349951 -4.5140138 -4.4394751][-4.4531 -4.5255284 -4.5406666 -4.4728813 -4.3767838 -4.2494574 -4.1633677 -4.1557918 -4.1659522 -4.2070637 -4.3153324 -4.4282937 -4.5028882 -4.5082 -4.4317255][-4.3794618 -4.4244494 -4.4010086 -4.2874889 -4.132585 -3.9655349 -3.8881106 -3.9047759 -3.9460411 -4.0264673 -4.1832118 -4.3633323 -4.5016317 -4.5458827 -4.4783][-4.2999043 -4.3050246 -4.2390561 -4.0829277 -3.8792436 -3.6803412 -3.6063144 -3.636137 -3.7049215 -3.828124 -4.0288882 -4.2670197 -4.4611192 -4.5386152 -4.4838243][-4.2471304 -4.2343655 -4.1563993 -3.9917233 -3.7763205 -3.574584 -3.5005341 -3.5265749 -3.6038332 -3.7467256 -3.9579551 -4.2062855 -4.4057775 -4.4802132 -4.4282594][-4.24425 -4.2434053 -4.1941972 -4.0703654 -3.8959208 -3.7328024 -3.6672873 -3.6844823 -3.7550652 -3.8881814 -4.0713243 -4.2692537 -4.408711 -4.43639 -4.371769][-4.28856 -4.2866182 -4.2683973 -4.2073083 -4.1134281 -4.0147762 -3.9633591 -3.9659278 -4.015996 -4.1188025 -4.2542458 -4.3782873 -4.4441743 -4.432878 -4.3714247][-4.3818359 -4.3557029 -4.340466 -4.3236346 -4.3096852 -4.2809644 -4.2482328 -4.2317648 -4.2444005 -4.3021107 -4.3876734 -4.4529624 -4.4750314 -4.4583187 -4.4267635][-4.4878793 -4.4435563 -4.4139175 -4.4077997 -4.4371967 -4.4528236 -4.43455 -4.4070883 -4.3886561 -4.4104309 -4.4582067 -4.48784 -4.4915552 -4.4842324 -4.486011][-4.5351582 -4.5044436 -4.4768271 -4.4696188 -4.4999008 -4.5124812 -4.4832692 -4.4432993 -4.40892 -4.4149389 -4.449101 -4.4724588 -4.4812689 -4.4864216 -4.5060558][-4.5418248 -4.5434179 -4.5311284 -4.5181556 -4.5206594 -4.4990869 -4.4443212 -4.3928084 -4.3573456 -4.3676977 -4.410603 -4.4500208 -4.472703 -4.4844522 -4.5055938][-4.5552521 -4.5732951 -4.5682559 -4.5454254 -4.516798 -4.4638319 -4.39398 -4.3408356 -4.3155956 -4.3383789 -4.397264 -4.4591227 -4.49847 -4.5140867 -4.5280561]]...]
INFO - root - 2017-12-07 14:40:17.582456: step 24410, loss = 21.48, batch loss = 21.39 (8.7 examples/sec; 0.918 sec/batch; 78h:35m:18s remains)
INFO - root - 2017-12-07 14:40:26.989387: step 24420, loss = 22.12, batch loss = 22.03 (9.0 examples/sec; 0.885 sec/batch; 75h:44m:57s remains)
INFO - root - 2017-12-07 14:40:36.436035: step 24430, loss = 21.76, batch loss = 21.68 (8.6 examples/sec; 0.932 sec/batch; 79h:43m:47s remains)
INFO - root - 2017-12-07 14:40:45.893889: step 24440, loss = 21.59, batch loss = 21.51 (8.4 examples/sec; 0.954 sec/batch; 81h:38m:04s remains)
INFO - root - 2017-12-07 14:40:55.320702: step 24450, loss = 21.67, batch loss = 21.58 (8.7 examples/sec; 0.919 sec/batch; 78h:40m:03s remains)
INFO - root - 2017-12-07 14:41:04.777151: step 24460, loss = 21.27, batch loss = 21.18 (8.5 examples/sec; 0.944 sec/batch; 80h:44m:28s remains)
INFO - root - 2017-12-07 14:41:14.192368: step 24470, loss = 21.50, batch loss = 21.41 (8.5 examples/sec; 0.937 sec/batch; 80h:12m:51s remains)
INFO - root - 2017-12-07 14:41:23.546168: step 24480, loss = 21.36, batch loss = 21.28 (8.6 examples/sec; 0.930 sec/batch; 79h:31m:57s remains)
INFO - root - 2017-12-07 14:41:33.071753: step 24490, loss = 21.52, batch loss = 21.44 (8.6 examples/sec; 0.929 sec/batch; 79h:31m:33s remains)
INFO - root - 2017-12-07 14:41:42.232546: step 24500, loss = 21.27, batch loss = 21.19 (8.4 examples/sec; 0.953 sec/batch; 81h:29m:36s remains)
2017-12-07 14:41:43.185163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4949279 -4.4464774 -4.3848023 -4.3039026 -4.3305554 -4.4092808 -4.4441981 -4.4471359 -4.4264731 -4.393014 -4.3323975 -4.30968 -4.3446507 -4.3967881 -4.49324][-4.5337834 -4.4754872 -4.3915391 -4.285449 -4.3095546 -4.3908691 -4.4167528 -4.4054036 -4.3793092 -4.3512669 -4.2900691 -4.2655835 -4.2987409 -4.350574 -4.4594049][-4.5356073 -4.4841466 -4.397697 -4.2944417 -4.3222079 -4.3954473 -4.4062376 -4.3894858 -4.3762007 -4.3683052 -4.3221622 -4.2988968 -4.3158727 -4.3483467 -4.4450493][-4.494977 -4.4668617 -4.4083543 -4.3464308 -4.3943758 -4.454185 -4.444561 -4.43143 -4.4402938 -4.4496312 -4.4236145 -4.409946 -4.4181108 -4.4325356 -4.4983296][-4.4354286 -4.4312572 -4.4155788 -4.4044776 -4.4678216 -4.5000677 -4.4562521 -4.4456024 -4.4770269 -4.497571 -4.49126 -4.49927 -4.5240226 -4.5456243 -4.591826][-4.3925338 -4.3949914 -4.4100261 -4.4286733 -4.4780855 -4.4672942 -4.38681 -4.375999 -4.4255676 -4.450809 -4.4563007 -4.4959044 -4.5604033 -4.6116638 -4.66566][-4.3532715 -4.3294334 -4.3465629 -4.3650031 -4.3779154 -4.3313813 -4.2286663 -4.2129188 -4.2775874 -4.3070025 -4.3237967 -4.4045639 -4.5204821 -4.6080608 -4.6829553][-4.3067818 -4.2379327 -4.236176 -4.2321491 -4.2068286 -4.1500921 -4.0482941 -4.02708 -4.1010008 -4.1373854 -4.1644316 -4.2775383 -4.432126 -4.5380788 -4.6269484][-4.277143 -4.17711 -4.1620526 -4.1412053 -4.0942183 -4.04519 -3.9519849 -3.9125314 -3.9712481 -4.0038342 -4.0284433 -4.1516314 -4.3138709 -4.4156613 -4.5089507][-4.267415 -4.1669359 -4.1598177 -4.144166 -4.1022987 -4.0726948 -3.9888608 -3.9284115 -3.957026 -3.9670436 -3.9740047 -4.0732956 -4.2024732 -4.2781954 -4.36456][-4.285944 -4.1965961 -4.19851 -4.1981373 -4.1828356 -4.1819263 -4.116611 -4.0544066 -4.0555639 -4.0344977 -4.0184951 -4.0681782 -4.1355376 -4.1726046 -4.2456021][-4.3528876 -4.2688718 -4.2617884 -4.269804 -4.2950349 -4.3301687 -4.2896805 -4.2304382 -4.19809 -4.1504292 -4.1235671 -4.1300974 -4.1400228 -4.1452832 -4.20144][-4.4565024 -4.37085 -4.3354588 -4.3307209 -4.3797503 -4.4433274 -4.4281521 -4.37916 -4.3286185 -4.2804365 -4.2665358 -4.2543807 -4.2316484 -4.2223768 -4.259644][-4.546916 -4.4714875 -4.4140229 -4.3840179 -4.4278159 -4.4967861 -4.5038095 -4.4725337 -4.4215436 -4.38596 -4.3877859 -4.37038 -4.3358412 -4.3270359 -4.3518763][-4.5985441 -4.548203 -4.4949355 -4.4577265 -4.4863415 -4.5411406 -4.5587854 -4.5422597 -4.496407 -4.4663115 -4.4681816 -4.4465804 -4.4141908 -4.416502 -4.4395289]]...]
INFO - root - 2017-12-07 14:41:52.531951: step 24510, loss = 21.24, batch loss = 21.15 (8.5 examples/sec; 0.946 sec/batch; 80h:53m:48s remains)
INFO - root - 2017-12-07 14:42:02.028530: step 24520, loss = 21.36, batch loss = 21.28 (7.9 examples/sec; 1.009 sec/batch; 86h:19m:45s remains)
INFO - root - 2017-12-07 14:42:11.504492: step 24530, loss = 22.09, batch loss = 22.00 (8.1 examples/sec; 0.986 sec/batch; 84h:19m:37s remains)
INFO - root - 2017-12-07 14:42:20.709525: step 24540, loss = 20.98, batch loss = 20.90 (8.1 examples/sec; 0.982 sec/batch; 84h:02m:32s remains)
INFO - root - 2017-12-07 14:42:30.096017: step 24550, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.934 sec/batch; 79h:53m:38s remains)
INFO - root - 2017-12-07 14:42:39.575310: step 24560, loss = 20.99, batch loss = 20.91 (8.5 examples/sec; 0.944 sec/batch; 80h:45m:28s remains)
INFO - root - 2017-12-07 14:42:48.962633: step 24570, loss = 21.16, batch loss = 21.08 (8.7 examples/sec; 0.915 sec/batch; 78h:14m:49s remains)
INFO - root - 2017-12-07 14:42:58.153599: step 24580, loss = 21.20, batch loss = 21.12 (9.7 examples/sec; 0.826 sec/batch; 70h:39m:25s remains)
INFO - root - 2017-12-07 14:43:07.346185: step 24590, loss = 21.47, batch loss = 21.39 (9.3 examples/sec; 0.864 sec/batch; 73h:52m:48s remains)
INFO - root - 2017-12-07 14:43:16.852342: step 24600, loss = 21.78, batch loss = 21.69 (8.7 examples/sec; 0.918 sec/batch; 78h:32m:02s remains)
2017-12-07 14:43:17.780251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3062468 -4.3814607 -4.4974165 -4.6155753 -4.6953664 -4.722466 -4.7063746 -4.6806083 -4.677434 -4.6852283 -4.67264 -4.6284113 -4.5736108 -4.5339637 -4.5238028][-4.3214335 -4.4280887 -4.5825925 -4.7276649 -4.8148837 -4.8288183 -4.7802887 -4.7178745 -4.70487 -4.7394881 -4.764473 -4.7394729 -4.682673 -4.6274748 -4.5991039][-4.3344755 -4.464375 -4.6396947 -4.7844553 -4.8504715 -4.8287544 -4.7272882 -4.6125908 -4.58757 -4.6656494 -4.7515821 -4.7658095 -4.7225318 -4.6624675 -4.6153641][-4.345983 -4.48401 -4.6562591 -4.7737484 -4.7958775 -4.7241516 -4.5598373 -4.3917146 -4.36426 -4.4967055 -4.643496 -4.6863508 -4.6553512 -4.59986 -4.5434046][-4.3586054 -4.4896679 -4.6384106 -4.707026 -4.6689835 -4.5380716 -4.318284 -4.1251979 -4.125998 -4.3237467 -4.5187712 -4.5565605 -4.5090685 -4.4584417 -4.4068117][-4.368773 -4.4830031 -4.5915003 -4.5945444 -4.4859233 -4.2959518 -4.0433626 -3.8691664 -3.9380035 -4.2116508 -4.4379277 -4.4391589 -4.344933 -4.2900991 -4.2468133][-4.3741212 -4.4733276 -4.5439005 -4.4885983 -4.3179865 -4.0756345 -3.8032346 -3.6570911 -3.7879286 -4.121985 -4.3681312 -4.3371615 -4.199707 -4.1314473 -4.0868168][-4.3840971 -4.486001 -4.5500813 -4.481163 -4.2893019 -4.0178967 -3.7313747 -3.5855803 -3.7209389 -4.0549431 -4.2918482 -4.2356977 -4.0669751 -3.9801614 -3.9389927][-4.4020228 -4.5243373 -4.6204925 -4.5954747 -4.444623 -4.1958237 -3.9166574 -3.7509887 -3.8261502 -4.0933762 -4.2833471 -4.201231 -4.0021377 -3.886817 -3.8620534][-4.4175143 -4.5624518 -4.7022991 -4.7446589 -4.6685538 -4.4775453 -4.2226663 -4.0267859 -4.0149961 -4.1956758 -4.3526883 -4.281682 -4.0787764 -3.93649 -3.9212024][-4.4189806 -4.5698628 -4.732378 -4.8219409 -4.8087978 -4.6788635 -4.4578819 -4.2406268 -4.156373 -4.26854 -4.4224992 -4.4125905 -4.2594671 -4.1202316 -4.104125][-4.400712 -4.5325465 -4.68668 -4.7925134 -4.8228583 -4.753736 -4.5886807 -4.388947 -4.2728043 -4.33377 -4.48215 -4.54295 -4.474402 -4.36876 -4.335968][-4.3680077 -4.4663057 -4.5934129 -4.6970143 -4.754158 -4.7411733 -4.6455412 -4.5020247 -4.4027042 -4.436615 -4.5614777 -4.6587653 -4.6603217 -4.5963907 -4.5484328][-4.332849 -4.3988848 -4.4961128 -4.5889473 -4.6591096 -4.6856813 -4.6496787 -4.567976 -4.5066977 -4.5321321 -4.6243105 -4.718627 -4.7545166 -4.7292538 -4.6898351][-4.3035846 -4.3496356 -4.4278517 -4.5113239 -4.5843568 -4.6274567 -4.6242108 -4.5839653 -4.5530677 -4.5741863 -4.6334939 -4.7011628 -4.7431145 -4.7512026 -4.7427635]]...]
INFO - root - 2017-12-07 14:43:27.250353: step 24610, loss = 21.28, batch loss = 21.20 (8.1 examples/sec; 0.991 sec/batch; 84h:47m:18s remains)
INFO - root - 2017-12-07 14:43:36.582236: step 24620, loss = 21.93, batch loss = 21.84 (8.2 examples/sec; 0.970 sec/batch; 82h:56m:17s remains)
INFO - root - 2017-12-07 14:43:46.056516: step 24630, loss = 21.20, batch loss = 21.11 (8.5 examples/sec; 0.946 sec/batch; 80h:55m:07s remains)
INFO - root - 2017-12-07 14:43:55.387315: step 24640, loss = 22.06, batch loss = 21.98 (8.8 examples/sec; 0.912 sec/batch; 77h:58m:19s remains)
INFO - root - 2017-12-07 14:44:04.736526: step 24650, loss = 21.30, batch loss = 21.22 (8.6 examples/sec; 0.934 sec/batch; 79h:51m:22s remains)
INFO - root - 2017-12-07 14:44:14.137058: step 24660, loss = 21.08, batch loss = 20.99 (9.2 examples/sec; 0.868 sec/batch; 74h:11m:42s remains)
INFO - root - 2017-12-07 14:44:23.453449: step 24670, loss = 21.09, batch loss = 21.01 (8.6 examples/sec; 0.935 sec/batch; 79h:54m:48s remains)
INFO - root - 2017-12-07 14:44:32.849214: step 24680, loss = 21.73, batch loss = 21.64 (8.7 examples/sec; 0.921 sec/batch; 78h:43m:48s remains)
INFO - root - 2017-12-07 14:44:42.311281: step 24690, loss = 21.18, batch loss = 21.10 (7.9 examples/sec; 1.007 sec/batch; 86h:06m:54s remains)
INFO - root - 2017-12-07 14:44:51.795016: step 24700, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.955 sec/batch; 81h:38m:10s remains)
2017-12-07 14:44:52.785003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3832622 -4.39786 -4.4149532 -4.43416 -4.4489965 -4.4579782 -4.4637141 -4.4676089 -4.4693255 -4.46961 -4.4679713 -4.4647408 -4.4615841 -4.4624376 -4.4674044][-4.4270754 -4.4491253 -4.474185 -4.4975724 -4.5134196 -4.5200863 -4.518702 -4.5150504 -4.5133362 -4.5161457 -4.5211549 -4.5211468 -4.5151229 -4.5117288 -4.5161042][-4.4534163 -4.4850254 -4.5117283 -4.5291014 -4.5349779 -4.52787 -4.5033274 -4.4754615 -4.4609046 -4.4707012 -4.4977927 -4.5197477 -4.5227418 -4.5164752 -4.5185027][-4.4852037 -4.5257378 -4.5465465 -4.547255 -4.5296421 -4.4927921 -4.424962 -4.3534403 -4.3201852 -4.3471708 -4.4173841 -4.4887438 -4.51685 -4.5034261 -4.4894209][-4.5166016 -4.557951 -4.5593677 -4.5250115 -4.4609375 -4.3740196 -4.2490382 -4.1293411 -4.0879683 -4.1480055 -4.2773318 -4.4182515 -4.4856486 -4.4602275 -4.4137731][-4.5108361 -4.5384989 -4.5097952 -4.436039 -4.3245006 -4.1900272 -4.0171409 -3.8644962 -3.8312893 -3.9276769 -4.1077251 -4.320159 -4.4381485 -4.4126968 -4.34152][-4.4877958 -4.50493 -4.457653 -4.3598061 -4.213717 -4.0387235 -3.8283432 -3.6556487 -3.6357121 -3.7575877 -3.965497 -4.2291908 -4.3895626 -4.371007 -4.2855539][-4.4861174 -4.4982309 -4.4435859 -4.3351412 -4.1697783 -3.9667473 -3.7345471 -3.5576904 -3.5549622 -3.6901603 -3.9012561 -4.1790667 -4.3522773 -4.3383675 -4.2522][-4.5096731 -4.5250382 -4.479198 -4.3823133 -4.2281032 -4.0295796 -3.8098223 -3.6521554 -3.6597931 -3.7825582 -3.9684014 -4.2195973 -4.377418 -4.3702707 -4.2981725][-4.5415058 -4.5736375 -4.5588455 -4.496428 -4.37673 -4.2097831 -4.0318794 -3.9164281 -3.934247 -4.0335312 -4.1784182 -4.3679252 -4.482367 -4.4756646 -4.4192519][-4.5561457 -4.602828 -4.6194954 -4.5964265 -4.521215 -4.4036951 -4.2839608 -4.2197247 -4.2498865 -4.3286982 -4.4303746 -4.545301 -4.6068339 -4.5981774 -4.556983][-4.5509014 -4.6012831 -4.634325 -4.639935 -4.6082606 -4.5463791 -4.4845543 -4.4610038 -4.4909611 -4.5454025 -4.6065464 -4.660574 -4.6844168 -4.6761403 -4.646049][-4.5357327 -4.5834484 -4.6152177 -4.6268792 -4.6183968 -4.5957546 -4.5738111 -4.5695934 -4.5868716 -4.6122284 -4.6355686 -4.6500034 -4.6514196 -4.6379528 -4.6095042][-4.4880624 -4.5227423 -4.5419092 -4.5442986 -4.536612 -4.5252748 -4.5172272 -4.5170364 -4.5229621 -4.5283632 -4.5277448 -4.5219293 -4.5123415 -4.4939694 -4.4665508][-4.4028454 -4.4142642 -4.41658 -4.4085832 -4.3961616 -4.3864861 -4.3848882 -4.3903165 -4.3980737 -4.40109 -4.3938208 -4.3818803 -4.3697858 -4.3537445 -4.3357525]]...]
INFO - root - 2017-12-07 14:45:02.287054: step 24710, loss = 21.69, batch loss = 21.61 (7.8 examples/sec; 1.019 sec/batch; 87h:08m:49s remains)
INFO - root - 2017-12-07 14:45:11.678294: step 24720, loss = 21.51, batch loss = 21.42 (8.2 examples/sec; 0.976 sec/batch; 83h:25m:54s remains)
INFO - root - 2017-12-07 14:45:21.119898: step 24730, loss = 21.28, batch loss = 21.19 (8.2 examples/sec; 0.975 sec/batch; 83h:19m:22s remains)
INFO - root - 2017-12-07 14:45:30.433230: step 24740, loss = 21.71, batch loss = 21.63 (8.4 examples/sec; 0.954 sec/batch; 81h:31m:38s remains)
INFO - root - 2017-12-07 14:45:39.838873: step 24750, loss = 21.24, batch loss = 21.16 (8.9 examples/sec; 0.903 sec/batch; 77h:10m:37s remains)
INFO - root - 2017-12-07 14:45:49.190804: step 24760, loss = 22.13, batch loss = 22.05 (8.7 examples/sec; 0.918 sec/batch; 78h:27m:53s remains)
INFO - root - 2017-12-07 14:45:58.642189: step 24770, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.937 sec/batch; 80h:03m:51s remains)
INFO - root - 2017-12-07 14:46:07.993874: step 24780, loss = 21.78, batch loss = 21.69 (8.8 examples/sec; 0.910 sec/batch; 77h:45m:41s remains)
INFO - root - 2017-12-07 14:46:17.346808: step 24790, loss = 21.35, batch loss = 21.27 (8.8 examples/sec; 0.909 sec/batch; 77h:40m:52s remains)
INFO - root - 2017-12-07 14:46:26.712590: step 24800, loss = 21.50, batch loss = 21.42 (8.7 examples/sec; 0.922 sec/batch; 78h:46m:03s remains)
2017-12-07 14:46:27.648165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4200988 -4.3891253 -4.39055 -4.438314 -4.4772267 -4.4770827 -4.4434114 -4.4293756 -4.4458504 -4.4563456 -4.458765 -4.4630451 -4.4520664 -4.4229069 -4.3925691][-4.386611 -4.3346887 -4.3150887 -4.3631525 -4.4128623 -4.4174643 -4.3777952 -4.366097 -4.4001164 -4.4294295 -4.4366574 -4.4364591 -4.4182792 -4.3756647 -4.3284335][-4.3828244 -4.3097811 -4.2589211 -4.2900681 -4.3393393 -4.346406 -4.3052435 -4.2955704 -4.3465543 -4.4038486 -4.4305735 -4.4360223 -4.4135985 -4.3590488 -4.2988677][-4.4000921 -4.3202395 -4.241714 -4.2406726 -4.2697272 -4.2660809 -4.2203584 -4.2013078 -4.2518849 -4.3289967 -4.3855915 -4.4105573 -4.3924074 -4.3337507 -4.2722683][-4.3978024 -4.3361821 -4.2430229 -4.1982694 -4.1852956 -4.1610327 -4.1119461 -4.0802703 -4.1138692 -4.18754 -4.2645788 -4.3161383 -4.3202152 -4.2817154 -4.2357287][-4.3700228 -4.3388042 -4.2426877 -4.1552429 -4.0944324 -4.0477004 -3.9995084 -3.9691849 -3.9915817 -4.0447388 -4.1163912 -4.1841631 -4.2167253 -4.2149539 -4.2022309][-4.3387537 -4.3338861 -4.233748 -4.1115808 -4.0147548 -3.9498253 -3.9062386 -3.8922462 -3.9191632 -3.94875 -3.9931147 -4.0580392 -4.1042361 -4.1254554 -4.1448278][-4.3109322 -4.3219619 -4.2224803 -4.0874171 -3.9765701 -3.8993933 -3.8619578 -3.870821 -3.9131596 -3.9332895 -3.9492538 -3.9913785 -4.0215845 -4.041151 -4.078362][-4.2833729 -4.2979212 -4.2134571 -4.0957737 -3.9982855 -3.9232073 -3.8984654 -3.9274521 -3.9901586 -4.0206032 -4.0173864 -4.0155926 -4.0013433 -3.998534 -4.0333552][-4.2766938 -4.2891045 -4.23076 -4.15449 -4.0905228 -4.0275793 -4.0063987 -4.0371041 -4.10927 -4.1594648 -4.1522288 -4.1071329 -4.0404944 -4.0075679 -4.029438][-4.30016 -4.309689 -4.2735057 -4.2353868 -4.2027292 -4.15454 -4.1278963 -4.13964 -4.19968 -4.2589335 -4.263001 -4.209868 -4.1269937 -4.0837708 -4.0926714][-4.3062549 -4.3180437 -4.2949605 -4.2715263 -4.2508726 -4.218575 -4.1942525 -4.1876168 -4.2157183 -4.2533875 -4.264246 -4.2425761 -4.2022247 -4.1843719 -4.1940904][-4.269031 -4.2926641 -4.2762275 -4.2468696 -4.2227545 -4.2081995 -4.2026114 -4.1961927 -4.1898956 -4.1815171 -4.1767564 -4.1970506 -4.2393537 -4.2741656 -4.2918344][-4.209971 -4.246974 -4.2334776 -4.1940942 -4.1667047 -4.1701784 -4.1908031 -4.1965885 -4.1691294 -4.1165109 -4.0822439 -4.1277819 -4.2485003 -4.3432684 -4.3706965][-4.1873155 -4.222836 -4.2013788 -4.152822 -4.127336 -4.1477995 -4.19077 -4.2079034 -4.173635 -4.1008344 -4.0382633 -4.0797987 -4.2374749 -4.3757439 -4.4165473]]...]
INFO - root - 2017-12-07 14:46:37.015174: step 24810, loss = 21.67, batch loss = 21.58 (9.0 examples/sec; 0.889 sec/batch; 76h:01m:00s remains)
INFO - root - 2017-12-07 14:46:46.687261: step 24820, loss = 21.47, batch loss = 21.38 (8.7 examples/sec; 0.915 sec/batch; 78h:10m:41s remains)
INFO - root - 2017-12-07 14:46:56.086914: step 24830, loss = 21.23, batch loss = 21.15 (8.0 examples/sec; 0.997 sec/batch; 85h:12m:33s remains)
INFO - root - 2017-12-07 14:47:04.988912: step 24840, loss = 21.31, batch loss = 21.23 (8.4 examples/sec; 0.953 sec/batch; 81h:25m:07s remains)
INFO - root - 2017-12-07 14:47:14.205098: step 24850, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.953 sec/batch; 81h:26m:14s remains)
INFO - root - 2017-12-07 14:47:23.685591: step 24860, loss = 21.49, batch loss = 21.41 (7.7 examples/sec; 1.040 sec/batch; 88h:52m:06s remains)
INFO - root - 2017-12-07 14:47:33.185097: step 24870, loss = 21.72, batch loss = 21.64 (8.2 examples/sec; 0.980 sec/batch; 83h:44m:31s remains)
INFO - root - 2017-12-07 14:47:42.612013: step 24880, loss = 21.51, batch loss = 21.42 (8.8 examples/sec; 0.914 sec/batch; 78h:04m:00s remains)
INFO - root - 2017-12-07 14:47:52.090639: step 24890, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.958 sec/batch; 81h:49m:09s remains)
INFO - root - 2017-12-07 14:48:01.597042: step 24900, loss = 21.22, batch loss = 21.14 (8.5 examples/sec; 0.940 sec/batch; 80h:17m:26s remains)
2017-12-07 14:48:02.507736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5040474 -4.5676184 -4.563693 -4.5105939 -4.4456148 -4.3952513 -4.40262 -4.4539967 -4.4864478 -4.5190458 -4.5680127 -4.6217089 -4.6504908 -4.6482091 -4.6216168][-4.5357037 -4.5867028 -4.5620637 -4.48658 -4.3968153 -4.3232894 -4.3305039 -4.405601 -4.460062 -4.4963293 -4.5344214 -4.5925341 -4.6407061 -4.6551933 -4.6246328][-4.530251 -4.5661731 -4.5236969 -4.4300742 -4.3234 -4.2417397 -4.2585411 -4.3625894 -4.4500279 -4.49861 -4.5269203 -4.5692344 -4.6099782 -4.6180935 -4.5727692][-4.5116735 -4.517343 -4.4490271 -4.3379397 -4.2217693 -4.14043 -4.16144 -4.2801085 -4.3993735 -4.4750857 -4.5111475 -4.53584 -4.5547781 -4.5436959 -4.4811773][-4.5038643 -4.4678488 -4.3679314 -4.23583 -4.1061792 -4.012773 -4.0124841 -4.1157494 -4.2530532 -4.3632855 -4.4275107 -4.4589124 -4.47086 -4.4469748 -4.3686051][-4.5156765 -4.4563136 -4.3347311 -4.1757216 -4.0116167 -3.8731213 -3.8170514 -3.8803411 -4.0280228 -4.1823759 -4.2973275 -4.3669515 -4.3914266 -4.3573217 -4.2619686][-4.5484834 -4.4952683 -4.3654013 -4.1761379 -3.9611614 -3.7573969 -3.6333885 -3.6559551 -3.8161466 -4.0208607 -4.1951475 -4.3127961 -4.3501067 -4.3002892 -4.1871777][-4.5765343 -4.5433946 -4.4251537 -4.2257981 -3.9733407 -3.7146192 -3.5366893 -3.5302823 -3.6976051 -3.9399405 -4.1612263 -4.3126221 -4.3473735 -4.2752542 -4.1518617][-4.5428925 -4.5282869 -4.4429054 -4.2777839 -4.0445585 -3.7908328 -3.6068649 -3.5883224 -3.7407157 -3.9765923 -4.2033091 -4.3543711 -4.3733325 -4.2834387 -4.1609635][-4.4376783 -4.4363351 -4.393919 -4.2948885 -4.1378202 -3.9559138 -3.8201668 -3.8073585 -3.9194264 -4.0989161 -4.284163 -4.41023 -4.4193339 -4.3295417 -4.2158751][-4.3039117 -4.3087544 -4.3046322 -4.2759719 -4.2117958 -4.1214981 -4.0511374 -4.0557952 -4.1352963 -4.2597485 -4.3944345 -4.4807677 -4.47595 -4.39462 -4.2936215][-4.197854 -4.2085729 -4.2337375 -4.2637677 -4.2801495 -4.2711115 -4.2634816 -4.3009324 -4.378397 -4.4725466 -4.5595231 -4.5978832 -4.5660553 -4.4802904 -4.3808556][-4.1755385 -4.19842 -4.2448459 -4.31146 -4.3783369 -4.4256759 -4.4666758 -4.5329537 -4.6152134 -4.691133 -4.7420468 -4.7434921 -4.684319 -4.5834703 -4.474113][-4.2643881 -4.297389 -4.34729 -4.4195476 -4.4971924 -4.5632653 -4.6199837 -4.6869044 -4.7548966 -4.8078742 -4.8339272 -4.8176203 -4.7502842 -4.6513019 -4.5486732][-4.415225 -4.4473705 -4.4862404 -4.5379262 -4.5921659 -4.6391482 -4.6768546 -4.7158971 -4.7506065 -4.7716341 -4.7716479 -4.7458048 -4.6940351 -4.6308336 -4.5722194]]...]
INFO - root - 2017-12-07 14:48:11.974724: step 24910, loss = 21.35, batch loss = 21.27 (8.4 examples/sec; 0.954 sec/batch; 81h:32m:09s remains)
INFO - root - 2017-12-07 14:48:21.328101: step 24920, loss = 21.35, batch loss = 21.27 (8.6 examples/sec; 0.929 sec/batch; 79h:24m:07s remains)
INFO - root - 2017-12-07 14:48:30.684910: step 24930, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.954 sec/batch; 81h:29m:47s remains)
INFO - root - 2017-12-07 14:48:40.051189: step 24940, loss = 21.34, batch loss = 21.26 (8.4 examples/sec; 0.955 sec/batch; 81h:33m:17s remains)
INFO - root - 2017-12-07 14:48:49.419881: step 24950, loss = 21.87, batch loss = 21.78 (8.9 examples/sec; 0.902 sec/batch; 77h:04m:26s remains)
INFO - root - 2017-12-07 14:48:58.766554: step 24960, loss = 21.45, batch loss = 21.37 (8.6 examples/sec; 0.926 sec/batch; 79h:08m:33s remains)
INFO - root - 2017-12-07 14:49:08.149053: step 24970, loss = 21.48, batch loss = 21.40 (8.3 examples/sec; 0.969 sec/batch; 82h:46m:52s remains)
INFO - root - 2017-12-07 14:49:17.536807: step 24980, loss = 20.96, batch loss = 20.88 (8.1 examples/sec; 0.992 sec/batch; 84h:46m:44s remains)
INFO - root - 2017-12-07 14:49:26.956676: step 24990, loss = 20.86, batch loss = 20.78 (8.0 examples/sec; 0.999 sec/batch; 85h:17m:47s remains)
INFO - root - 2017-12-07 14:49:36.418684: step 25000, loss = 21.80, batch loss = 21.72 (8.1 examples/sec; 0.983 sec/batch; 84h:00m:11s remains)
2017-12-07 14:49:37.327503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5935779 -4.6670275 -4.672461 -4.6497135 -4.6472077 -4.651361 -4.6025105 -4.4986873 -4.3915968 -4.3200173 -4.3055992 -4.3255115 -4.3555722 -4.4127049 -4.4854121][-4.6855297 -4.6961527 -4.64951 -4.6066332 -4.612524 -4.630939 -4.599834 -4.5141916 -4.4175735 -4.3505983 -4.3364396 -4.3504171 -4.372622 -4.4186878 -4.4758539][-4.7216272 -4.6830826 -4.6043873 -4.5567536 -4.5764947 -4.6121454 -4.59973 -4.5404687 -4.4727235 -4.4213357 -4.4045 -4.4094439 -4.427731 -4.4705753 -4.5201359][-4.7099595 -4.6548734 -4.5713143 -4.52867 -4.5507684 -4.5800848 -4.5611391 -4.5157113 -4.4842763 -4.4663272 -4.4615207 -4.4672527 -4.4911151 -4.5380363 -4.5806556][-4.6788878 -4.6251407 -4.5396113 -4.4901958 -4.491745 -4.4860392 -4.4326963 -4.3808894 -4.383142 -4.4132719 -4.4423294 -4.4690027 -4.5110536 -4.5645652 -4.5947394][-4.6199021 -4.5757771 -4.4817228 -4.4179816 -4.3939533 -4.3442287 -4.2404904 -4.1602507 -4.1763206 -4.2477493 -4.3214483 -4.3867483 -4.4559836 -4.5132208 -4.523828][-4.5287795 -4.49832 -4.3978877 -4.3213277 -4.2738023 -4.1753936 -4.0090284 -3.8863244 -3.9076629 -4.0238643 -4.1546755 -4.2658529 -4.3589382 -4.4084735 -4.3915091][-4.441617 -4.4340115 -4.3400555 -4.2548337 -4.1821337 -4.0358534 -3.8165717 -3.6647711 -3.7059314 -3.8798113 -4.0644855 -4.2038522 -4.2956061 -4.3148966 -4.2614179][-4.40191 -4.4257374 -4.3551826 -4.2786975 -4.2013845 -4.0437994 -3.8174381 -3.6718001 -3.7358341 -3.9451475 -4.1429186 -4.2650785 -4.3192492 -4.2905135 -4.1994429][-4.4318442 -4.4776983 -4.4361787 -4.3861942 -4.3346362 -4.210216 -4.0172262 -3.89107 -3.9511473 -4.1463838 -4.31262 -4.3853564 -4.3886161 -4.3250732 -4.2231908][-4.530437 -4.5752487 -4.5456295 -4.5164838 -4.4997582 -4.4295197 -4.2913189 -4.1849957 -4.21329 -4.3533278 -4.465724 -4.4904075 -4.4568591 -4.3837719 -4.2988811][-4.6572666 -4.6759577 -4.6386285 -4.6177721 -4.6259127 -4.6066279 -4.5331311 -4.4624887 -4.4703794 -4.5538063 -4.6141853 -4.6066523 -4.5559268 -4.4830804 -4.4093132][-4.7316203 -4.720078 -4.67538 -4.6585474 -4.6761894 -4.6864614 -4.6625185 -4.6311927 -4.6406627 -4.69028 -4.7212386 -4.7057986 -4.6578655 -4.590014 -4.5184236][-4.7215757 -4.6928878 -4.65356 -4.6447349 -4.663939 -4.682981 -4.6828794 -4.6752543 -4.687201 -4.7171488 -4.73392 -4.7238846 -4.6894765 -4.6344485 -4.5718174][-4.6422806 -4.6115417 -4.5887818 -4.5922422 -4.6083913 -4.6230493 -4.6285839 -4.6308422 -4.6410584 -4.6561956 -4.6638327 -4.6584492 -4.6372528 -4.6006827 -4.5565372]]...]
INFO - root - 2017-12-07 14:49:46.892005: step 25010, loss = 21.33, batch loss = 21.24 (8.2 examples/sec; 0.973 sec/batch; 83h:08m:18s remains)
INFO - root - 2017-12-07 14:49:56.076074: step 25020, loss = 21.44, batch loss = 21.35 (9.5 examples/sec; 0.838 sec/batch; 71h:33m:05s remains)
INFO - root - 2017-12-07 14:50:05.481290: step 25030, loss = 21.30, batch loss = 21.22 (9.2 examples/sec; 0.869 sec/batch; 74h:11m:35s remains)
INFO - root - 2017-12-07 14:50:14.886372: step 25040, loss = 21.53, batch loss = 21.45 (8.4 examples/sec; 0.955 sec/batch; 81h:33m:34s remains)
INFO - root - 2017-12-07 14:50:24.409822: step 25050, loss = 21.22, batch loss = 21.14 (8.2 examples/sec; 0.970 sec/batch; 82h:50m:18s remains)
INFO - root - 2017-12-07 14:50:33.689948: step 25060, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.926 sec/batch; 79h:06m:41s remains)
INFO - root - 2017-12-07 14:50:43.104504: step 25070, loss = 20.96, batch loss = 20.88 (7.7 examples/sec; 1.036 sec/batch; 88h:30m:19s remains)
INFO - root - 2017-12-07 14:50:52.544917: step 25080, loss = 21.40, batch loss = 21.32 (8.5 examples/sec; 0.937 sec/batch; 80h:01m:29s remains)
INFO - root - 2017-12-07 14:51:02.014249: step 25090, loss = 21.30, batch loss = 21.21 (8.5 examples/sec; 0.943 sec/batch; 80h:30m:44s remains)
INFO - root - 2017-12-07 14:51:11.403436: step 25100, loss = 22.28, batch loss = 22.20 (8.0 examples/sec; 0.998 sec/batch; 85h:13m:47s remains)
2017-12-07 14:51:12.288062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4206648 -4.4194202 -4.3762655 -4.3253851 -4.2896719 -4.2446218 -4.1832418 -4.1697316 -4.2196927 -4.2583547 -4.28765 -4.3244867 -4.3738565 -4.4267821 -4.4545155][-4.4340882 -4.4354477 -4.3858604 -4.3203397 -4.2688904 -4.2073731 -4.1283646 -4.1078568 -4.1682353 -4.2259541 -4.2691054 -4.3134823 -4.3697071 -4.4254293 -4.4535923][-4.4093261 -4.4132237 -4.3659487 -4.3044157 -4.2617579 -4.2057686 -4.12393 -4.1000447 -4.16502 -4.2321558 -4.2778587 -4.3179111 -4.3714414 -4.4255481 -4.4520082][-4.3874626 -4.3919225 -4.3507261 -4.3011436 -4.2718167 -4.2240047 -4.142508 -4.114408 -4.1780605 -4.2474103 -4.2912016 -4.3265228 -4.3770275 -4.4286294 -4.4531932][-4.3789363 -4.3756862 -4.3352847 -4.2936144 -4.2698874 -4.2223759 -4.1372604 -4.1040244 -4.1671228 -4.2393403 -4.2852559 -4.32558 -4.3809729 -4.43394 -4.45825][-4.3716469 -4.3523169 -4.3042922 -4.2617846 -4.2362 -4.1822319 -4.0872359 -4.0485768 -4.1166329 -4.197741 -4.2538691 -4.31064 -4.380084 -4.4403763 -4.4683695][-4.3555708 -4.3163548 -4.2558594 -4.2067504 -4.1782508 -4.1168008 -4.0093579 -3.9672892 -4.0471158 -4.1443243 -4.2182074 -4.2970772 -4.3833022 -4.45207 -4.483695][-4.3316779 -4.2774949 -4.2080917 -4.1565785 -4.1329226 -4.0721135 -3.9588141 -3.9192853 -4.0136023 -4.1271029 -4.2154756 -4.3054972 -4.393281 -4.4567013 -4.4826317][-4.3062725 -4.249732 -4.1860795 -4.1459351 -4.1387081 -4.0910463 -3.9863732 -3.95685 -4.0588326 -4.1732006 -4.2537971 -4.3268509 -4.3932171 -4.4398947 -4.4615717][-4.2869415 -4.2484512 -4.2085738 -4.1931925 -4.2098417 -4.1853762 -4.1004658 -4.0779839 -4.1649737 -4.2500877 -4.2972655 -4.3395834 -4.3855591 -4.4250979 -4.4506378][-4.2922106 -4.2789483 -4.2640586 -4.2699785 -4.303195 -4.2970452 -4.2289042 -4.2016892 -4.2544622 -4.2970104 -4.3124328 -4.3389044 -4.3799729 -4.4160089 -4.436904][-4.3092189 -4.3048668 -4.2978168 -4.3099279 -4.3472691 -4.3489294 -4.2896185 -4.2527881 -4.2749634 -4.291059 -4.2959924 -4.3228431 -4.362617 -4.3878374 -4.3953328][-4.3099885 -4.30092 -4.2933764 -4.3064485 -4.3446164 -4.3510351 -4.2998362 -4.2621431 -4.2767348 -4.2928152 -4.3003278 -4.3208923 -4.3429565 -4.34624 -4.3394871][-4.3019948 -4.2941675 -4.2940755 -4.3131437 -4.3531189 -4.36384 -4.3218975 -4.2907486 -4.31141 -4.3372736 -4.3429341 -4.3402262 -4.3302741 -4.3088236 -4.292594][-4.3116012 -4.3090987 -4.3178616 -4.3397603 -4.3752561 -4.3840714 -4.3451328 -4.3161478 -4.3403478 -4.3755198 -4.3817768 -4.3639307 -4.3343372 -4.2996511 -4.2790923]]...]
INFO - root - 2017-12-07 14:51:21.735407: step 25110, loss = 21.25, batch loss = 21.17 (9.1 examples/sec; 0.874 sec/batch; 74h:39m:53s remains)
INFO - root - 2017-12-07 14:51:31.139366: step 25120, loss = 20.96, batch loss = 20.88 (8.6 examples/sec; 0.927 sec/batch; 79h:08m:08s remains)
INFO - root - 2017-12-07 14:51:40.590655: step 25130, loss = 21.49, batch loss = 21.41 (8.2 examples/sec; 0.975 sec/batch; 83h:12m:29s remains)
INFO - root - 2017-12-07 14:51:49.931728: step 25140, loss = 21.13, batch loss = 21.05 (8.5 examples/sec; 0.941 sec/batch; 80h:22m:32s remains)
INFO - root - 2017-12-07 14:51:59.301312: step 25150, loss = 21.02, batch loss = 20.94 (8.3 examples/sec; 0.960 sec/batch; 81h:57m:01s remains)
INFO - root - 2017-12-07 14:52:08.700802: step 25160, loss = 21.88, batch loss = 21.79 (8.9 examples/sec; 0.896 sec/batch; 76h:30m:11s remains)
INFO - root - 2017-12-07 14:52:18.130826: step 25170, loss = 21.36, batch loss = 21.27 (9.0 examples/sec; 0.887 sec/batch; 75h:44m:44s remains)
INFO - root - 2017-12-07 14:52:27.589804: step 25180, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.932 sec/batch; 79h:34m:07s remains)
INFO - root - 2017-12-07 14:52:36.800446: step 25190, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.957 sec/batch; 81h:40m:16s remains)
INFO - root - 2017-12-07 14:52:46.260337: step 25200, loss = 21.26, batch loss = 21.17 (8.1 examples/sec; 0.987 sec/batch; 84h:14m:46s remains)
2017-12-07 14:52:47.283185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4972506 -4.5795293 -4.627759 -4.6296515 -4.6064134 -4.5415673 -4.433444 -4.3307252 -4.2879429 -4.3207512 -4.4326472 -4.5859146 -4.7007256 -4.7254753 -4.6806216][-4.5095611 -4.5774903 -4.5981359 -4.569345 -4.5214958 -4.4279256 -4.2900052 -4.175725 -4.1484728 -4.2141447 -4.3583841 -4.5405717 -4.6708622 -4.6972032 -4.6445379][-4.5156302 -4.5668006 -4.5592155 -4.5038548 -4.4417639 -4.3403616 -4.1973367 -4.081954 -4.0731215 -4.1680641 -4.3364239 -4.5337825 -4.6673279 -4.6886845 -4.6266508][-4.5277791 -4.5667896 -4.538198 -4.4631062 -4.393424 -4.2984762 -4.1653929 -4.0568404 -4.0663552 -4.1812778 -4.3575253 -4.5576825 -4.694654 -4.7111883 -4.6327376][-4.53402 -4.5627689 -4.514277 -4.4117918 -4.3177834 -4.2167239 -4.0931311 -3.9993033 -4.0338297 -4.1700459 -4.3489175 -4.5452805 -4.69045 -4.7096229 -4.6153522][-4.5239654 -4.5374365 -4.4655342 -4.3248076 -4.1875796 -4.0537052 -3.9171362 -3.8320885 -3.9049642 -4.0786982 -4.2650533 -4.4460392 -4.5886617 -4.6152725 -4.522943][-4.5084457 -4.5101323 -4.4218922 -4.2525573 -4.078167 -3.9076712 -3.7459738 -3.6606836 -3.7617981 -3.9609904 -4.131391 -4.2682562 -4.3868971 -4.4267292 -4.3608975][-4.5016074 -4.5060091 -4.4223 -4.251925 -4.0675974 -3.8871813 -3.7279892 -3.657115 -3.7545013 -3.9208498 -4.0276604 -4.0973973 -4.1873846 -4.2479286 -4.2166042][-4.5051317 -4.5273294 -4.4688196 -4.319241 -4.1435432 -3.9810045 -3.858228 -3.8147819 -3.882812 -3.9745309 -4.0016 -4.0204434 -4.0987372 -4.1851597 -4.180943][-4.5012589 -4.5442219 -4.5184703 -4.4019208 -4.2518592 -4.1237206 -4.042623 -4.0162354 -4.0428557 -4.0654507 -4.0436335 -4.0442772 -4.1213918 -4.2268047 -4.2459655][-4.4744482 -4.5257239 -4.5267811 -4.4539967 -4.348218 -4.2639456 -4.2183609 -4.1986604 -4.1917133 -4.1730127 -4.1360068 -4.1376624 -4.2061348 -4.3112669 -4.3533096][-4.4281893 -4.4734483 -4.4902649 -4.460619 -4.4069066 -4.3710561 -4.3625379 -4.3588824 -4.3407612 -4.3079557 -4.2752767 -4.2817087 -4.3300123 -4.4106054 -4.4553938][-4.3801632 -4.4092131 -4.4318051 -4.4361672 -4.4274354 -4.4337106 -4.4607315 -4.4835072 -4.4749217 -4.4412346 -4.4133482 -4.4203405 -4.4459157 -4.4908075 -4.5174727][-4.3405867 -4.3549871 -4.37872 -4.4029393 -4.4217472 -4.4501891 -4.4963322 -4.5428481 -4.5576634 -4.5380487 -4.5147295 -4.5168285 -4.5277529 -4.5447111 -4.5519147][-4.3154907 -4.3185906 -4.3383327 -4.3679185 -4.39383 -4.422915 -4.4679689 -4.5237842 -4.5599542 -4.5612545 -4.5479517 -4.5516524 -4.5690427 -4.5829377 -4.5805674]]...]
INFO - root - 2017-12-07 14:52:56.748461: step 25210, loss = 21.57, batch loss = 21.49 (8.8 examples/sec; 0.910 sec/batch; 77h:41m:34s remains)
INFO - root - 2017-12-07 14:53:06.136820: step 25220, loss = 21.12, batch loss = 21.04 (8.5 examples/sec; 0.946 sec/batch; 80h:43m:35s remains)
INFO - root - 2017-12-07 14:53:15.553980: step 25230, loss = 21.39, batch loss = 21.31 (8.9 examples/sec; 0.902 sec/batch; 76h:59m:06s remains)
INFO - root - 2017-12-07 14:53:24.922163: step 25240, loss = 21.61, batch loss = 21.53 (9.2 examples/sec; 0.871 sec/batch; 74h:18m:20s remains)
INFO - root - 2017-12-07 14:53:34.366378: step 25250, loss = 21.15, batch loss = 21.07 (8.5 examples/sec; 0.945 sec/batch; 80h:40m:18s remains)
INFO - root - 2017-12-07 14:53:43.773355: step 25260, loss = 21.20, batch loss = 21.11 (8.3 examples/sec; 0.965 sec/batch; 82h:19m:44s remains)
INFO - root - 2017-12-07 14:53:53.212704: step 25270, loss = 21.53, batch loss = 21.45 (8.9 examples/sec; 0.902 sec/batch; 76h:57m:23s remains)
INFO - root - 2017-12-07 14:54:02.600933: step 25280, loss = 21.49, batch loss = 21.41 (9.0 examples/sec; 0.890 sec/batch; 75h:55m:42s remains)
INFO - root - 2017-12-07 14:54:11.818014: step 25290, loss = 21.68, batch loss = 21.59 (8.8 examples/sec; 0.904 sec/batch; 77h:09m:01s remains)
INFO - root - 2017-12-07 14:54:21.348829: step 25300, loss = 21.86, batch loss = 21.78 (8.3 examples/sec; 0.966 sec/batch; 82h:25m:45s remains)
2017-12-07 14:54:22.306985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5205045 -4.5004535 -4.4632716 -4.4274073 -4.4098215 -4.4290853 -4.4521632 -4.4544926 -4.47358 -4.5181928 -4.5701289 -4.6120567 -4.6230707 -4.596158 -4.5346322][-4.5534315 -4.5228872 -4.4552431 -4.3861818 -4.3511181 -4.3797894 -4.4229264 -4.4368486 -4.4615817 -4.5067949 -4.5585308 -4.6059804 -4.6323991 -4.6217895 -4.5685687][-4.5628629 -4.5168037 -4.4246683 -4.3371615 -4.2932096 -4.3263249 -4.3789549 -4.4000697 -4.4334731 -4.4847822 -4.5402164 -4.5962858 -4.6371737 -4.6404748 -4.5958304][-4.52399 -4.454855 -4.3489323 -4.2519417 -4.1949248 -4.2074609 -4.2393436 -4.2578287 -4.3125277 -4.3986235 -4.4895563 -4.5716257 -4.6238456 -4.6323481 -4.5950994][-4.4394836 -4.3472476 -4.2335238 -4.130846 -4.0571556 -4.0300989 -4.0186057 -4.029954 -4.1185222 -4.2664514 -4.42218 -4.5415092 -4.596848 -4.5979996 -4.5627956][-4.3070545 -4.1900177 -4.0657878 -3.9519436 -3.8539429 -3.7789202 -3.7255464 -3.7429297 -3.8827863 -4.1138577 -4.3562937 -4.523241 -4.5802865 -4.5631442 -4.5197082][-4.217011 -4.0977921 -3.9715738 -3.8442092 -3.7174156 -3.5975978 -3.521554 -3.5616908 -3.7498891 -4.0438175 -4.3491931 -4.5489111 -4.6026006 -4.5632672 -4.5025826][-4.2160988 -4.1251984 -4.0164571 -3.8919003 -3.7515111 -3.6112378 -3.5372038 -3.6008 -3.8012877 -4.0939784 -4.394865 -4.5876536 -4.6305618 -4.5775213 -4.5034685][-4.2768011 -4.2186708 -4.1335311 -4.0254817 -3.8927736 -3.7595072 -3.7025902 -3.7757752 -3.9542637 -4.2027769 -4.4573264 -4.6205239 -4.6531334 -4.598732 -4.5176916][-4.3782177 -4.3406429 -4.2797556 -4.2005286 -4.0967975 -3.9937115 -3.9602418 -4.0284314 -4.1701155 -4.3616748 -4.5547843 -4.6758647 -4.6955204 -4.6419883 -4.5521703][-4.4798484 -4.4601336 -4.4294744 -4.3876982 -4.3242073 -4.2581196 -4.2370992 -4.2827573 -4.3782668 -4.510643 -4.6424413 -4.7209663 -4.7275486 -4.6754227 -4.5798788][-4.5680022 -4.56302 -4.5552225 -4.541791 -4.510262 -4.4693923 -4.448638 -4.4666424 -4.5193319 -4.5991721 -4.680325 -4.7282214 -4.7270551 -4.6769485 -4.5811257][-4.5958781 -4.5959888 -4.5966635 -4.5971193 -4.5850635 -4.5600643 -4.5386472 -4.5376596 -4.5595818 -4.6001792 -4.64662 -4.6772685 -4.6739168 -4.6281972 -4.5422544][-4.5334945 -4.5311761 -4.533102 -4.5389261 -4.5365934 -4.52191 -4.5031543 -4.4927411 -4.4961052 -4.5129385 -4.5390325 -4.560967 -4.5607505 -4.5284686 -4.4673853][-4.4241729 -4.4200115 -4.420557 -4.42494 -4.4243321 -4.415205 -4.4018583 -4.3913589 -4.3890967 -4.3967919 -4.4130125 -4.4294462 -4.4337735 -4.4193864 -4.3881321]]...]
INFO - root - 2017-12-07 14:54:31.808591: step 25310, loss = 21.23, batch loss = 21.15 (8.5 examples/sec; 0.942 sec/batch; 80h:21m:34s remains)
INFO - root - 2017-12-07 14:54:41.233283: step 25320, loss = 21.44, batch loss = 21.35 (8.4 examples/sec; 0.947 sec/batch; 80h:50m:14s remains)
INFO - root - 2017-12-07 14:54:50.665576: step 25330, loss = 21.63, batch loss = 21.55 (8.3 examples/sec; 0.969 sec/batch; 82h:39m:56s remains)
INFO - root - 2017-12-07 14:55:00.132899: step 25340, loss = 21.45, batch loss = 21.37 (8.1 examples/sec; 0.993 sec/batch; 84h:41m:54s remains)
INFO - root - 2017-12-07 14:55:09.536886: step 25350, loss = 21.56, batch loss = 21.47 (8.3 examples/sec; 0.961 sec/batch; 82h:01m:14s remains)
INFO - root - 2017-12-07 14:55:18.709682: step 25360, loss = 21.55, batch loss = 21.46 (9.2 examples/sec; 0.872 sec/batch; 74h:21m:15s remains)
INFO - root - 2017-12-07 14:55:28.095422: step 25370, loss = 21.61, batch loss = 21.52 (8.8 examples/sec; 0.913 sec/batch; 77h:54m:12s remains)
INFO - root - 2017-12-07 14:55:37.439329: step 25380, loss = 21.16, batch loss = 21.08 (8.4 examples/sec; 0.953 sec/batch; 81h:15m:45s remains)
INFO - root - 2017-12-07 14:55:46.779041: step 25390, loss = 21.87, batch loss = 21.79 (8.5 examples/sec; 0.939 sec/batch; 80h:07m:26s remains)
INFO - root - 2017-12-07 14:55:56.122203: step 25400, loss = 21.57, batch loss = 21.48 (9.0 examples/sec; 0.885 sec/batch; 75h:28m:08s remains)
2017-12-07 14:55:57.034938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4655871 -4.4891186 -4.5260596 -4.56008 -4.5809078 -4.5703187 -4.5287042 -4.4988055 -4.5044312 -4.5232353 -4.5447073 -4.5505047 -4.5232129 -4.4828086 -4.4337134][-4.488349 -4.516974 -4.5618191 -4.5953293 -4.61203 -4.5874677 -4.51509 -4.4647689 -4.4803529 -4.5221829 -4.5713396 -4.595098 -4.5658436 -4.518734 -4.4572725][-4.5076532 -4.5223894 -4.5529628 -4.5708284 -4.5759444 -4.5401049 -4.4413753 -4.3701515 -4.396636 -4.4638076 -4.5446525 -4.5974088 -4.5788803 -4.5387373 -4.4777088][-4.5141034 -4.5136337 -4.5210214 -4.5091848 -4.4856129 -4.4310241 -4.3093243 -4.2208242 -4.2605681 -4.3569393 -4.4687214 -4.5543628 -4.55765 -4.5365067 -4.4885035][-4.4694142 -4.4633536 -4.451993 -4.4091167 -4.3446169 -4.2611842 -4.1179881 -4.01194 -4.0653424 -4.1991639 -4.3491292 -4.4748034 -4.5128589 -4.5187688 -4.4901462][-4.37456 -4.3663912 -4.3437371 -4.2756662 -4.1754417 -4.0588651 -3.8957472 -3.773375 -3.8334227 -4.0090923 -4.2066193 -4.3796244 -4.4631104 -4.4990115 -4.4885888][-4.2768927 -4.2782693 -4.260716 -4.1857181 -4.0692172 -3.9224043 -3.7348371 -3.5944743 -3.6469574 -3.8512418 -4.0866752 -4.29927 -4.4250174 -4.4868 -4.4888954][-4.2021203 -4.2255597 -4.23302 -4.1826077 -4.0814376 -3.9317079 -3.7420919 -3.6053677 -3.6519113 -3.8525119 -4.0824146 -4.290513 -4.4243236 -4.4905534 -4.4931026][-4.1686878 -4.2242632 -4.2703967 -4.2692046 -4.2051764 -4.0766044 -3.9147358 -3.8102345 -3.8664417 -4.0366855 -4.2143068 -4.3712354 -4.4706693 -4.51305 -4.5024176][-4.23299 -4.289115 -4.3471837 -4.3770008 -4.3383522 -4.2342076 -4.1127267 -4.0543423 -4.1378589 -4.2818861 -4.3987641 -4.4918838 -4.538383 -4.5444479 -4.5156989][-4.3674784 -4.4091396 -4.4509015 -4.4773116 -4.4405627 -4.35278 -4.2674046 -4.24584 -4.348248 -4.4713035 -4.543076 -4.58787 -4.5902534 -4.5666347 -4.5246649][-4.506093 -4.5306616 -4.54834 -4.5571775 -4.5146942 -4.4427762 -4.3881083 -4.3899279 -4.4880571 -4.5840788 -4.6237197 -4.637394 -4.6134524 -4.571938 -4.5228987][-4.5860825 -4.61017 -4.6198382 -4.6191468 -4.5805049 -4.5311995 -4.504704 -4.5211282 -4.5993714 -4.6630263 -4.6770892 -4.6681986 -4.6263728 -4.5688314 -4.5117731][-4.6210942 -4.6499176 -4.6678257 -4.6721773 -4.6458168 -4.615344 -4.6035156 -4.6223745 -4.6757836 -4.7130408 -4.70974 -4.6848683 -4.6294403 -4.5578823 -4.49391][-4.6329556 -4.6541252 -4.6752105 -4.6851983 -4.6712337 -4.6510549 -4.6408868 -4.6500573 -4.6781292 -4.6954012 -4.6846213 -4.656929 -4.6022916 -4.5312366 -4.470129]]...]
INFO - root - 2017-12-07 14:56:06.359993: step 25410, loss = 21.32, batch loss = 21.23 (8.4 examples/sec; 0.955 sec/batch; 81h:29m:31s remains)
INFO - root - 2017-12-07 14:56:15.908696: step 25420, loss = 21.48, batch loss = 21.39 (8.2 examples/sec; 0.974 sec/batch; 83h:07m:20s remains)
INFO - root - 2017-12-07 14:56:25.392008: step 25430, loss = 21.07, batch loss = 20.99 (8.0 examples/sec; 0.999 sec/batch; 85h:14m:45s remains)
INFO - root - 2017-12-07 14:56:34.714891: step 25440, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.957 sec/batch; 81h:39m:59s remains)
INFO - root - 2017-12-07 14:56:44.016817: step 25450, loss = 21.12, batch loss = 21.04 (8.6 examples/sec; 0.932 sec/batch; 79h:28m:47s remains)
INFO - root - 2017-12-07 14:56:53.240136: step 25460, loss = 21.66, batch loss = 21.58 (8.2 examples/sec; 0.977 sec/batch; 83h:17m:33s remains)
INFO - root - 2017-12-07 14:57:02.661262: step 25470, loss = 21.39, batch loss = 21.31 (8.6 examples/sec; 0.928 sec/batch; 79h:10m:41s remains)
INFO - root - 2017-12-07 14:57:12.136115: step 25480, loss = 21.69, batch loss = 21.61 (8.8 examples/sec; 0.909 sec/batch; 77h:32m:58s remains)
INFO - root - 2017-12-07 14:57:21.552044: step 25490, loss = 21.49, batch loss = 21.41 (8.5 examples/sec; 0.937 sec/batch; 79h:55m:11s remains)
INFO - root - 2017-12-07 14:57:30.834241: step 25500, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.931 sec/batch; 79h:24m:44s remains)
2017-12-07 14:57:31.786886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.636765 -4.7083459 -4.7166719 -4.6831794 -4.64597 -4.6122341 -4.6029983 -4.6157451 -4.6053715 -4.58877 -4.59153 -4.610096 -4.646163 -4.6755552 -4.6657176][-4.677 -4.7463689 -4.7441187 -4.7005115 -4.6608748 -4.6261215 -4.6177411 -4.6401343 -4.6359406 -4.6252217 -4.6366081 -4.6620965 -4.7077451 -4.7419567 -4.730011][-4.6687536 -4.715343 -4.6882467 -4.6324992 -4.6000028 -4.5830932 -4.5887079 -4.6131454 -4.6066647 -4.5999017 -4.6228108 -4.667757 -4.7336221 -4.7776451 -4.7665348][-4.6186662 -4.6292195 -4.5710049 -4.4996424 -4.4718571 -4.4758 -4.4957786 -4.5127988 -4.4998527 -4.4972687 -4.5340772 -4.6118469 -4.7108254 -4.7721591 -4.7678108][-4.5564222 -4.5253 -4.4320221 -4.332849 -4.2874446 -4.284781 -4.2926693 -4.2855721 -4.2668576 -4.2774186 -4.3384356 -4.4654679 -4.6151824 -4.706882 -4.7238097][-4.5288024 -4.4755898 -4.3549376 -4.2168441 -4.1232781 -4.0725741 -4.0308347 -3.9855175 -3.9701161 -4.0118747 -4.1111107 -4.2951908 -4.5024905 -4.6288252 -4.6712027][-4.571 -4.527051 -4.3923159 -4.2072582 -4.0438414 -3.925194 -3.8283548 -3.7530332 -3.7518809 -3.82977 -3.9611609 -4.1787691 -4.4188724 -4.5706749 -4.63378][-4.6600289 -4.6519995 -4.5244017 -4.3105822 -4.0938272 -3.925566 -3.7975347 -3.7115827 -3.7222633 -3.8174551 -3.9545429 -4.1678486 -4.4044552 -4.5597162 -4.6293168][-4.7320523 -4.7661519 -4.6688323 -4.4659696 -4.2457619 -4.0722818 -3.9464414 -3.8673272 -3.8771782 -3.9647598 -4.0868998 -4.2689118 -4.4705873 -4.604229 -4.6593695][-4.7485042 -4.805697 -4.741096 -4.5739088 -4.3893886 -4.2477641 -4.150207 -4.0962214 -4.1063857 -4.1785564 -4.280118 -4.4196949 -4.5715942 -4.6691632 -4.6948023][-4.706985 -4.7758074 -4.743587 -4.6267009 -4.5009694 -4.4069967 -4.3471661 -4.323236 -4.3360543 -4.3894119 -4.4665766 -4.5620756 -4.6622405 -4.7172937 -4.70945][-4.645184 -4.7200017 -4.717463 -4.6510048 -4.5783796 -4.5229363 -4.4893532 -4.4833975 -4.4961753 -4.5327559 -4.5885921 -4.6526971 -4.7133164 -4.7325449 -4.6978369][-4.571486 -4.6380672 -4.6488185 -4.615653 -4.5786114 -4.5461483 -4.52434 -4.5215631 -4.5287356 -4.5524392 -4.5929642 -4.6388841 -4.6766725 -4.6771493 -4.6326528][-4.47926 -4.5199924 -4.5277457 -4.5129876 -4.4999709 -4.4845428 -4.4707422 -4.4670444 -4.4685636 -4.4807196 -4.5055928 -4.5356369 -4.5589046 -4.5549154 -4.518754][-4.3924031 -4.4052572 -4.4050922 -4.3991218 -4.3990574 -4.3966303 -4.3912954 -4.3887191 -4.387609 -4.3921919 -4.4033074 -4.4169407 -4.4274249 -4.4235768 -4.4027009]]...]
INFO - root - 2017-12-07 14:57:41.164859: step 25510, loss = 21.24, batch loss = 21.16 (8.3 examples/sec; 0.965 sec/batch; 82h:18m:34s remains)
INFO - root - 2017-12-07 14:57:50.749684: step 25520, loss = 21.39, batch loss = 21.31 (8.7 examples/sec; 0.916 sec/batch; 78h:04m:18s remains)
INFO - root - 2017-12-07 14:58:00.029533: step 25530, loss = 21.50, batch loss = 21.42 (9.2 examples/sec; 0.871 sec/batch; 74h:16m:22s remains)
INFO - root - 2017-12-07 14:58:09.635775: step 25540, loss = 21.24, batch loss = 21.15 (8.0 examples/sec; 1.002 sec/batch; 85h:25m:03s remains)
INFO - root - 2017-12-07 14:58:18.859271: step 25550, loss = 21.25, batch loss = 21.17 (8.7 examples/sec; 0.921 sec/batch; 78h:32m:51s remains)
INFO - root - 2017-12-07 14:58:28.347327: step 25560, loss = 21.39, batch loss = 21.31 (9.5 examples/sec; 0.840 sec/batch; 71h:34m:41s remains)
INFO - root - 2017-12-07 14:58:37.677373: step 25570, loss = 21.52, batch loss = 21.44 (8.5 examples/sec; 0.942 sec/batch; 80h:21m:03s remains)
INFO - root - 2017-12-07 14:58:46.994995: step 25580, loss = 21.47, batch loss = 21.39 (8.4 examples/sec; 0.948 sec/batch; 80h:47m:11s remains)
INFO - root - 2017-12-07 14:58:56.523961: step 25590, loss = 21.68, batch loss = 21.59 (7.8 examples/sec; 1.022 sec/batch; 87h:06m:48s remains)
INFO - root - 2017-12-07 14:59:06.000353: step 25600, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.948 sec/batch; 80h:47m:28s remains)
2017-12-07 14:59:07.023996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4801416 -4.4726868 -4.4673982 -4.48589 -4.5269775 -4.5777693 -4.60555 -4.60635 -4.5889874 -4.5613141 -4.5303149 -4.5280647 -4.5660381 -4.6026406 -4.6355634][-4.4639878 -4.450635 -4.4383397 -4.4591632 -4.5078831 -4.5641332 -4.585681 -4.5708418 -4.5297785 -4.4771991 -4.4373903 -4.4483123 -4.5099144 -4.576036 -4.6318831][-4.4252987 -4.4134812 -4.4055209 -4.4296293 -4.4764457 -4.5257354 -4.5353203 -4.5059 -4.4452267 -4.3766565 -4.3418384 -4.3741884 -4.4615607 -4.55597 -4.6237197][-4.3989234 -4.4009733 -4.4044919 -4.4185872 -4.438602 -4.4582224 -4.445425 -4.4026031 -4.3283362 -4.254529 -4.2348547 -4.2877736 -4.3887529 -4.4934573 -4.5554733][-4.408093 -4.4265618 -4.44018 -4.4286127 -4.3995676 -4.3703685 -4.3290243 -4.2838292 -4.214582 -4.1532826 -4.1576414 -4.2295551 -4.3390346 -4.4378495 -4.4841614][-4.4192152 -4.4506736 -4.47409 -4.4411297 -4.3707042 -4.2973242 -4.23098 -4.1961145 -4.1550422 -4.1266508 -4.1617308 -4.2519188 -4.3634963 -4.4439173 -4.4697571][-4.4279 -4.4686842 -4.4982929 -4.4512639 -4.3566937 -4.2544246 -4.1741185 -4.1529188 -4.1461468 -4.1491446 -4.2026262 -4.3000178 -4.4074097 -4.4692855 -4.4803615][-4.4437947 -4.4814591 -4.5066462 -4.4521637 -4.3503556 -4.2405796 -4.1591487 -4.145268 -4.1524143 -4.1589227 -4.1998315 -4.286694 -4.3946996 -4.462966 -4.4885206][-4.4690166 -4.4826565 -4.488658 -4.4335017 -4.3397737 -4.2341471 -4.1521106 -4.1326332 -4.1320076 -4.1199117 -4.1289706 -4.1993532 -4.3173094 -4.4159884 -4.4810247][-4.51279 -4.4994869 -4.4839458 -4.4305763 -4.3490314 -4.2458048 -4.1577225 -4.128592 -4.1245532 -4.105001 -4.0904493 -4.140841 -4.2582006 -4.3722048 -4.4593344][-4.5525351 -4.5227652 -4.4894328 -4.4402065 -4.3748574 -4.2819552 -4.19324 -4.1591406 -4.1623363 -4.1556711 -4.1353755 -4.1614318 -4.2510681 -4.3442554 -4.4168305][-4.5647259 -4.5320282 -4.4940071 -4.45199 -4.4014721 -4.324687 -4.2439709 -4.20892 -4.2179356 -4.226305 -4.2133284 -4.22164 -4.269804 -4.3183937 -4.35303][-4.5262465 -4.5042744 -4.4738054 -4.4424706 -4.4067717 -4.3539209 -4.2957573 -4.2709994 -4.2826371 -4.2978921 -4.2915411 -4.2857556 -4.2965403 -4.3039236 -4.3044991][-4.4738417 -4.4585605 -4.432941 -4.4089317 -4.3873177 -4.3641539 -4.3396964 -4.3377151 -4.3554912 -4.3707738 -4.3652377 -4.343235 -4.3209066 -4.2977047 -4.280427][-4.4722471 -4.4525242 -4.4155941 -4.3821259 -4.363102 -4.3607779 -4.3696008 -4.3957138 -4.4234123 -4.4389377 -4.4336743 -4.4019494 -4.3566251 -4.311213 -4.2849693]]...]
INFO - root - 2017-12-07 14:59:16.500666: step 25610, loss = 21.68, batch loss = 21.60 (8.3 examples/sec; 0.968 sec/batch; 82h:31m:34s remains)
INFO - root - 2017-12-07 14:59:25.875999: step 25620, loss = 21.25, batch loss = 21.17 (8.0 examples/sec; 0.998 sec/batch; 85h:03m:34s remains)
INFO - root - 2017-12-07 14:59:35.390816: step 25630, loss = 21.25, batch loss = 21.17 (8.5 examples/sec; 0.940 sec/batch; 80h:09m:34s remains)
INFO - root - 2017-12-07 14:59:44.672149: step 25640, loss = 21.39, batch loss = 21.31 (8.5 examples/sec; 0.938 sec/batch; 79h:59m:43s remains)
INFO - root - 2017-12-07 14:59:54.120591: step 25650, loss = 21.44, batch loss = 21.36 (8.7 examples/sec; 0.922 sec/batch; 78h:33m:22s remains)
INFO - root - 2017-12-07 15:00:03.411971: step 25660, loss = 21.09, batch loss = 21.01 (8.8 examples/sec; 0.914 sec/batch; 77h:55m:14s remains)
INFO - root - 2017-12-07 15:00:12.768723: step 25670, loss = 21.81, batch loss = 21.73 (8.4 examples/sec; 0.952 sec/batch; 81h:09m:59s remains)
INFO - root - 2017-12-07 15:00:22.092785: step 25680, loss = 21.42, batch loss = 21.34 (9.0 examples/sec; 0.887 sec/batch; 75h:34m:34s remains)
INFO - root - 2017-12-07 15:00:31.636972: step 25690, loss = 21.69, batch loss = 21.61 (8.5 examples/sec; 0.939 sec/batch; 80h:03m:27s remains)
INFO - root - 2017-12-07 15:00:41.042745: step 25700, loss = 21.72, batch loss = 21.64 (8.8 examples/sec; 0.911 sec/batch; 77h:36m:14s remains)
2017-12-07 15:00:41.983888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5594897 -4.5555868 -4.5334415 -4.5362873 -4.5782547 -4.6288037 -4.6699581 -4.7039552 -4.7289848 -4.7096648 -4.6226654 -4.4782252 -4.3144217 -4.1696935 -4.0823388][-4.5569272 -4.5640712 -4.5357842 -4.5145683 -4.5169821 -4.5311794 -4.5683355 -4.6353903 -4.7141719 -4.7546277 -4.7162237 -4.5838604 -4.3872719 -4.1892252 -4.0639515][-4.5131817 -4.5164547 -4.4748468 -4.422307 -4.3646808 -4.3160663 -4.334331 -4.4348207 -4.5766239 -4.6945858 -4.7317719 -4.6473179 -4.4537511 -4.22952 -4.0819178][-4.4685974 -4.4539804 -4.399096 -4.314342 -4.1830649 -4.0537844 -4.0408816 -4.1682572 -4.36866 -4.5549436 -4.6612153 -4.6372886 -4.4800105 -4.27016 -4.1293354][-4.42387 -4.3837237 -4.3170409 -4.2029285 -4.0002432 -3.791255 -3.7412498 -3.8854489 -4.1312332 -4.3683658 -4.5305734 -4.57561 -4.4883828 -4.3340449 -4.2266774][-4.368609 -4.3076587 -4.2437205 -4.1196108 -3.8751285 -3.6116951 -3.5254107 -3.6619568 -3.9188576 -4.1737671 -4.3684125 -4.4809685 -4.4828906 -4.4049006 -4.3392782][-4.3403792 -4.2841449 -4.2475529 -4.1446252 -3.9043691 -3.6262016 -3.5079937 -3.6101589 -3.8361745 -4.0618052 -4.2476025 -4.3945951 -4.4690785 -4.4623256 -4.4350271][-4.3548965 -4.3217387 -4.3262281 -4.2678237 -4.0721259 -3.8197346 -3.6833763 -3.7380071 -3.9068568 -4.0750289 -4.220695 -4.3618412 -4.4714274 -4.51297 -4.5164127][-4.3872752 -4.37889 -4.41809 -4.4105058 -4.2834806 -4.0843992 -3.9496562 -3.9584341 -4.0663791 -4.1828613 -4.2920446 -4.4082355 -4.5139565 -4.5662627 -4.5776248][-4.4135756 -4.4168077 -4.4771824 -4.514883 -4.4612532 -4.3275657 -4.2103868 -4.1890264 -4.2500534 -4.3328037 -4.417727 -4.5056839 -4.5825706 -4.6116962 -4.6052179][-4.3943858 -4.40472 -4.4746566 -4.545959 -4.5552874 -4.4869909 -4.4012284 -4.368753 -4.3994656 -4.4581079 -4.5190072 -4.5739164 -4.6136961 -4.61133 -4.5823965][-4.3383431 -4.359036 -4.4277344 -4.5126948 -4.563879 -4.5479922 -4.4965591 -4.4666152 -4.4755449 -4.5066938 -4.5367436 -4.5553803 -4.5650988 -4.5453348 -4.505734][-4.3062382 -4.3341484 -4.3820882 -4.4456005 -4.5012522 -4.5104074 -4.4825826 -4.456439 -4.4457631 -4.4486737 -4.4496894 -4.44474 -4.4502282 -4.4448938 -4.4200444][-4.3099508 -4.3349051 -4.3504972 -4.3708305 -4.4019566 -4.4151382 -4.4039583 -4.38372 -4.35915 -4.3436289 -4.3353996 -4.3325429 -4.3604879 -4.3912773 -4.3931303][-4.3255153 -4.3411665 -4.3328276 -4.3193259 -4.3273125 -4.3420515 -4.347403 -4.3358235 -4.3050337 -4.2840405 -4.2823362 -4.2932944 -4.3418059 -4.3980622 -4.4172573]]...]
INFO - root - 2017-12-07 15:00:51.310454: step 25710, loss = 21.14, batch loss = 21.06 (8.8 examples/sec; 0.912 sec/batch; 77h:42m:52s remains)
INFO - root - 2017-12-07 15:01:00.684881: step 25720, loss = 21.23, batch loss = 21.15 (8.8 examples/sec; 0.907 sec/batch; 77h:19m:19s remains)
INFO - root - 2017-12-07 15:01:10.087852: step 25730, loss = 21.86, batch loss = 21.78 (7.7 examples/sec; 1.044 sec/batch; 88h:55m:27s remains)
INFO - root - 2017-12-07 15:01:19.508172: step 25740, loss = 21.80, batch loss = 21.72 (8.2 examples/sec; 0.979 sec/batch; 83h:24m:05s remains)
INFO - root - 2017-12-07 15:01:28.862649: step 25750, loss = 20.99, batch loss = 20.91 (8.4 examples/sec; 0.956 sec/batch; 81h:29m:52s remains)
INFO - root - 2017-12-07 15:01:38.331028: step 25760, loss = 21.47, batch loss = 21.39 (8.0 examples/sec; 0.999 sec/batch; 85h:08m:47s remains)
INFO - root - 2017-12-07 15:01:47.561263: step 25770, loss = 21.49, batch loss = 21.41 (8.3 examples/sec; 0.967 sec/batch; 82h:23m:48s remains)
INFO - root - 2017-12-07 15:01:56.985715: step 25780, loss = 21.75, batch loss = 21.67 (8.3 examples/sec; 0.959 sec/batch; 81h:44m:18s remains)
INFO - root - 2017-12-07 15:02:06.312462: step 25790, loss = 21.47, batch loss = 21.39 (8.4 examples/sec; 0.950 sec/batch; 80h:55m:54s remains)
INFO - root - 2017-12-07 15:02:15.625760: step 25800, loss = 21.21, batch loss = 21.13 (8.4 examples/sec; 0.951 sec/batch; 81h:03m:27s remains)
2017-12-07 15:02:16.630535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5310435 -4.5647626 -4.5149503 -4.415134 -4.3157892 -4.2718964 -4.2895422 -4.3279514 -4.3802795 -4.421041 -4.4308066 -4.4091759 -4.3657694 -4.3511772 -4.3773508][-4.539546 -4.5730233 -4.5229511 -4.4249458 -4.3283658 -4.2815924 -4.2921944 -4.3316016 -4.4007249 -4.4680123 -4.506011 -4.4999671 -4.4561563 -4.4209738 -4.4195204][-4.5411344 -4.566504 -4.5063953 -4.4002194 -4.2956586 -4.2352605 -4.2370033 -4.2852836 -4.3780136 -4.47466 -4.5387292 -4.5527987 -4.5217357 -4.4797173 -4.4594603][-4.5449009 -4.56656 -4.4994783 -4.3820395 -4.2611289 -4.1816854 -4.1762638 -4.2375145 -4.3520708 -4.4693437 -4.5496287 -4.5826893 -4.5693526 -4.5242143 -4.4810543][-4.5543661 -4.5747323 -4.501164 -4.3676362 -4.2231908 -4.1211843 -4.1082191 -4.1827259 -4.31655 -4.4455585 -4.5292349 -4.5696592 -4.5677381 -4.5215845 -4.4631538][-4.5612683 -4.5809479 -4.5036583 -4.3595467 -4.1952338 -4.0695972 -4.041564 -4.1144676 -4.2508726 -4.3730245 -4.4443445 -4.48399 -4.4990683 -4.4678049 -4.4132986][-4.5627794 -4.5845075 -4.5103426 -4.3617864 -4.1753688 -4.0141058 -3.9494731 -4.001667 -4.134326 -4.2572436 -4.33174 -4.3846679 -4.4227672 -4.4050651 -4.3511691][-4.5561948 -4.5781822 -4.5084014 -4.3565021 -4.1465921 -3.9464445 -3.8432951 -3.8776691 -4.0147643 -4.1511741 -4.2362633 -4.2948933 -4.3359737 -4.3164368 -4.2612658][-4.5423317 -4.5664511 -4.5094843 -4.3660865 -4.1460209 -3.9148786 -3.7743163 -3.7864795 -3.9219584 -4.0712528 -4.1692395 -4.2333312 -4.2670836 -4.235126 -4.1778431][-4.5326695 -4.5681396 -4.53384 -4.4082222 -4.1840754 -3.9242697 -3.7445433 -3.732923 -3.8697207 -4.0415254 -4.1688352 -4.24799 -4.2659197 -4.2041211 -4.1279783][-4.53765 -4.59217 -4.5820622 -4.4768867 -4.2592692 -3.9920628 -3.7954454 -3.7707608 -3.9061666 -4.0879235 -4.228723 -4.3060117 -4.295228 -4.1968284 -4.1000738][-4.5559864 -4.629776 -4.6402326 -4.5562325 -4.3610425 -4.1102524 -3.9157467 -3.8803983 -4.0023952 -4.1781349 -4.3219228 -4.3956413 -4.3679304 -4.2471766 -4.1360335][-4.5776596 -4.6631885 -4.6841564 -4.6155429 -4.4489546 -4.2320561 -4.0621853 -4.0268159 -4.1361513 -4.30317 -4.445416 -4.5128789 -4.4728894 -4.3419657 -4.2194958][-4.5895662 -4.6778417 -4.70402 -4.6501627 -4.5194612 -4.3503404 -4.220345 -4.1926785 -4.2841606 -4.4275241 -4.5474715 -4.5908995 -4.5351038 -4.4063907 -4.285871][-4.5835981 -4.6712379 -4.7038522 -4.6630888 -4.5550919 -4.4153757 -4.3074155 -4.2778068 -4.3491473 -4.4720221 -4.572906 -4.5976777 -4.534389 -4.4222493 -4.3189244]]...]
INFO - root - 2017-12-07 15:02:25.988488: step 25810, loss = 21.63, batch loss = 21.55 (8.5 examples/sec; 0.939 sec/batch; 79h:57m:10s remains)
INFO - root - 2017-12-07 15:02:35.401404: step 25820, loss = 21.60, batch loss = 21.52 (8.1 examples/sec; 0.982 sec/batch; 83h:39m:40s remains)
INFO - root - 2017-12-07 15:02:44.792018: step 25830, loss = 21.50, batch loss = 21.42 (8.7 examples/sec; 0.915 sec/batch; 77h:55m:16s remains)
INFO - root - 2017-12-07 15:02:54.158859: step 25840, loss = 21.56, batch loss = 21.48 (8.7 examples/sec; 0.920 sec/batch; 78h:24m:34s remains)
INFO - root - 2017-12-07 15:03:03.388684: step 25850, loss = 21.19, batch loss = 21.10 (8.5 examples/sec; 0.940 sec/batch; 80h:03m:35s remains)
INFO - root - 2017-12-07 15:03:12.970503: step 25860, loss = 21.54, batch loss = 21.46 (8.0 examples/sec; 1.004 sec/batch; 85h:31m:10s remains)
INFO - root - 2017-12-07 15:03:22.304937: step 25870, loss = 21.57, batch loss = 21.49 (8.7 examples/sec; 0.917 sec/batch; 78h:04m:18s remains)
INFO - root - 2017-12-07 15:03:31.622377: step 25880, loss = 21.74, batch loss = 21.66 (8.5 examples/sec; 0.943 sec/batch; 80h:20m:01s remains)
INFO - root - 2017-12-07 15:03:41.147690: step 25890, loss = 21.62, batch loss = 21.54 (8.4 examples/sec; 0.956 sec/batch; 81h:24m:44s remains)
INFO - root - 2017-12-07 15:03:50.533715: step 25900, loss = 21.43, batch loss = 21.35 (8.8 examples/sec; 0.913 sec/batch; 77h:45m:42s remains)
2017-12-07 15:03:51.484674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.460741 -4.4323163 -4.4182129 -4.4340563 -4.4851465 -4.5381341 -4.5593538 -4.5734525 -4.5812078 -4.5868411 -4.599009 -4.6115484 -4.6432419 -4.6718726 -4.6608028][-4.4579668 -4.4234662 -4.4106541 -4.4250484 -4.4763174 -4.5316572 -4.5558028 -4.57271 -4.5719843 -4.566093 -4.5865092 -4.6216493 -4.6927247 -4.7583966 -4.7653518][-4.4828987 -4.4622979 -4.4542112 -4.4458036 -4.4622355 -4.4913135 -4.5036464 -4.5117617 -4.4905453 -4.4653082 -4.4977088 -4.5688138 -4.6863489 -4.7898469 -4.8161221][-4.5298076 -4.5282059 -4.5145426 -4.4580603 -4.4043841 -4.3724637 -4.3500032 -4.3389792 -4.3035097 -4.2746077 -4.3327823 -4.4507208 -4.6111465 -4.74469 -4.7909374][-4.5759892 -4.5918894 -4.5653925 -4.4556441 -4.3216405 -4.2058978 -4.1242003 -4.0803337 -4.0333238 -4.0153942 -4.1169591 -4.2967763 -4.502418 -4.6680779 -4.7372766][-4.6063881 -4.630188 -4.5855894 -4.4280705 -4.229908 -4.0461435 -3.9168122 -3.8395863 -3.7700114 -3.7530365 -3.8970861 -4.1411376 -4.398406 -4.6023259 -4.6965513][-4.6348872 -4.6556997 -4.58481 -4.3822594 -4.1419497 -3.9242601 -3.7799215 -3.6867533 -3.5937178 -3.5619094 -3.7287269 -4.0195031 -4.3199377 -4.5622973 -4.6802697][-4.6566777 -4.6654315 -4.5727658 -4.3450532 -4.1023622 -3.8940177 -3.769871 -3.68313 -3.5819583 -3.5421834 -3.7207193 -4.0313182 -4.3465662 -4.600553 -4.7163811][-4.663043 -4.6637669 -4.57415 -4.3603034 -4.1488585 -3.9773738 -3.89528 -3.8371017 -3.7541862 -3.7234542 -3.8940473 -4.1839962 -4.4730449 -4.7007837 -4.7861524][-4.6688957 -4.6730771 -4.6081028 -4.4441 -4.2860923 -4.1664934 -4.1268406 -4.1002827 -4.0457397 -4.0284343 -4.1649332 -4.3948317 -4.6216836 -4.7967634 -4.8440881][-4.6686387 -4.6881442 -4.6641164 -4.5764279 -4.4888005 -4.4258242 -4.4164658 -4.4094958 -4.3763142 -4.366118 -4.454258 -4.6029229 -4.7469554 -4.8492942 -4.8527422][-4.6414061 -4.671988 -4.6816812 -4.6602697 -4.6343036 -4.6182466 -4.6256647 -4.6285629 -4.6093483 -4.5974255 -4.6333184 -4.6997523 -4.7633491 -4.7982907 -4.7694054][-4.5818639 -4.6113429 -4.6324782 -4.6419125 -4.6476994 -4.65382 -4.6636686 -4.6662335 -4.6535254 -4.6390729 -4.6407709 -4.6539388 -4.6662884 -4.6637411 -4.6281557][-4.4974895 -4.518425 -4.5384455 -4.553751 -4.5664177 -4.5767245 -4.5846152 -4.5868168 -4.5803404 -4.5697865 -4.5609369 -4.5530691 -4.5439148 -4.5277624 -4.49881][-4.4167619 -4.4251666 -4.4365368 -4.4454579 -4.4527245 -4.4590459 -4.4642797 -4.4675803 -4.4668679 -4.4625783 -4.4556589 -4.4465547 -4.435833 -4.4233136 -4.4082837]]...]
INFO - root - 2017-12-07 15:04:00.900996: step 25910, loss = 21.81, batch loss = 21.73 (8.8 examples/sec; 0.908 sec/batch; 77h:19m:37s remains)
INFO - root - 2017-12-07 15:04:10.376221: step 25920, loss = 21.86, batch loss = 21.77 (8.3 examples/sec; 0.960 sec/batch; 81h:43m:03s remains)
INFO - root - 2017-12-07 15:04:19.804654: step 25930, loss = 20.94, batch loss = 20.86 (8.8 examples/sec; 0.911 sec/batch; 77h:33m:41s remains)
INFO - root - 2017-12-07 15:04:29.177978: step 25940, loss = 21.53, batch loss = 21.45 (8.7 examples/sec; 0.921 sec/batch; 78h:27m:20s remains)
INFO - root - 2017-12-07 15:04:38.756819: step 25950, loss = 21.14, batch loss = 21.06 (8.3 examples/sec; 0.959 sec/batch; 81h:41m:47s remains)
INFO - root - 2017-12-07 15:04:48.143987: step 25960, loss = 21.77, batch loss = 21.69 (8.3 examples/sec; 0.962 sec/batch; 81h:56m:27s remains)
INFO - root - 2017-12-07 15:04:57.510498: step 25970, loss = 21.35, batch loss = 21.26 (8.6 examples/sec; 0.934 sec/batch; 79h:29m:47s remains)
INFO - root - 2017-12-07 15:05:06.869060: step 25980, loss = 21.30, batch loss = 21.22 (8.4 examples/sec; 0.955 sec/batch; 81h:21m:07s remains)
INFO - root - 2017-12-07 15:05:16.277117: step 25990, loss = 21.46, batch loss = 21.37 (8.8 examples/sec; 0.906 sec/batch; 77h:06m:39s remains)
INFO - root - 2017-12-07 15:05:25.695994: step 26000, loss = 21.78, batch loss = 21.70 (8.9 examples/sec; 0.903 sec/batch; 76h:54m:44s remains)
2017-12-07 15:05:26.692802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5242882 -4.4607191 -4.323812 -4.2081475 -4.1681237 -4.1940188 -4.2856522 -4.4029341 -4.5084848 -4.6126337 -4.7396297 -4.8698206 -4.9440331 -4.9336352 -4.8384738][-4.4914641 -4.4029284 -4.2562857 -4.14445 -4.0896425 -4.0920644 -4.1821184 -4.3200092 -4.4567676 -4.588789 -4.7417355 -4.8969469 -4.9806709 -4.9684 -4.8714147][-4.5096493 -4.4412994 -4.3258042 -4.2418437 -4.182251 -4.1483154 -4.1997018 -4.3203878 -4.45919 -4.5931425 -4.7391748 -4.8716354 -4.9163527 -4.8722796 -4.7654366][-4.5865717 -4.5710483 -4.5076408 -4.456491 -4.4050126 -4.35568 -4.3654628 -4.4440165 -4.5531025 -4.6577959 -4.7620034 -4.8320594 -4.8057671 -4.7075415 -4.5816298][-4.6655636 -4.6921296 -4.6610675 -4.6147332 -4.553874 -4.4933419 -4.4698153 -4.5015845 -4.5693226 -4.6341686 -4.6976953 -4.728508 -4.6735573 -4.5566387 -4.4235806][-4.6831183 -4.7259417 -4.7063394 -4.6469407 -4.5500765 -4.4472322 -4.3632631 -4.3300586 -4.3592567 -4.41249 -4.4861803 -4.5477223 -4.54112 -4.4636135 -4.3391476][-4.609467 -4.6334705 -4.606914 -4.5336428 -4.3992014 -4.2390275 -4.0758448 -3.9601665 -3.9509056 -4.0246506 -4.1620293 -4.3122954 -4.4052987 -4.40406 -4.2955084][-4.4807734 -4.4719877 -4.4287839 -4.3388362 -4.1757689 -3.9740689 -3.7571456 -3.5886779 -3.5636842 -3.6769538 -3.8857207 -4.11783 -4.2972026 -4.3667045 -4.2785139][-4.3914461 -4.3832841 -4.3538027 -4.2717428 -4.1070213 -3.89465 -3.6691985 -3.5006819 -3.4873281 -3.627564 -3.8605652 -4.1097164 -4.3130445 -4.4170747 -4.361618][-4.3843327 -4.4207397 -4.4419079 -4.4008551 -4.268136 -4.0783849 -3.8837583 -3.7505438 -3.7556045 -3.8897319 -4.0893745 -4.29023 -4.4525585 -4.5437684 -4.5209084][-4.4248996 -4.5081477 -4.5741024 -4.5755649 -4.4904861 -4.3481083 -4.2081695 -4.12397 -4.1420832 -4.2463603 -4.3858619 -4.5180554 -4.6215038 -4.68406 -4.6825566][-4.4641762 -4.5737929 -4.6654348 -4.6980219 -4.6607533 -4.5733218 -4.4899054 -4.4494591 -4.4757843 -4.5512347 -4.6402712 -4.7235236 -4.7926321 -4.8393135 -4.85162][-4.4665828 -4.5811009 -4.6824374 -4.7380052 -4.7440085 -4.7108426 -4.6743712 -4.6620274 -4.6868706 -4.7383938 -4.7971697 -4.8551717 -4.907517 -4.9439583 -4.9586983][-4.4236212 -4.5186496 -4.6128368 -4.6829348 -4.7256293 -4.7419624 -4.7431865 -4.7436614 -4.7538295 -4.7756519 -4.8053522 -4.8415351 -4.8786097 -4.9037905 -4.9129763][-4.3516908 -4.4120817 -4.4818478 -4.5478506 -4.6054506 -4.64793 -4.6702118 -4.6757245 -4.6735225 -4.6717854 -4.6751342 -4.686501 -4.7026753 -4.7157121 -4.7230043]]...]
INFO - root - 2017-12-07 15:05:36.096513: step 26010, loss = 21.70, batch loss = 21.62 (8.3 examples/sec; 0.967 sec/batch; 82h:20m:56s remains)
INFO - root - 2017-12-07 15:05:45.600686: step 26020, loss = 21.10, batch loss = 21.01 (8.0 examples/sec; 1.004 sec/batch; 85h:27m:35s remains)
INFO - root - 2017-12-07 15:05:55.124624: step 26030, loss = 21.41, batch loss = 21.33 (8.3 examples/sec; 0.966 sec/batch; 82h:15m:50s remains)
INFO - root - 2017-12-07 15:06:04.421711: step 26040, loss = 21.92, batch loss = 21.83 (8.6 examples/sec; 0.927 sec/batch; 78h:55m:47s remains)
INFO - root - 2017-12-07 15:06:13.691254: step 26050, loss = 21.40, batch loss = 21.32 (8.4 examples/sec; 0.947 sec/batch; 80h:37m:03s remains)
INFO - root - 2017-12-07 15:06:23.072751: step 26060, loss = 21.32, batch loss = 21.23 (8.4 examples/sec; 0.953 sec/batch; 81h:05m:53s remains)
INFO - root - 2017-12-07 15:06:32.505507: step 26070, loss = 21.29, batch loss = 21.20 (8.6 examples/sec; 0.932 sec/batch; 79h:20m:52s remains)
INFO - root - 2017-12-07 15:06:41.753344: step 26080, loss = 21.46, batch loss = 21.37 (8.7 examples/sec; 0.914 sec/batch; 77h:49m:25s remains)
INFO - root - 2017-12-07 15:06:51.100473: step 26090, loss = 21.62, batch loss = 21.54 (8.2 examples/sec; 0.970 sec/batch; 82h:32m:34s remains)
INFO - root - 2017-12-07 15:07:00.553285: step 26100, loss = 21.15, batch loss = 21.07 (8.1 examples/sec; 0.992 sec/batch; 84h:23m:34s remains)
2017-12-07 15:07:01.441122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4861345 -4.4851856 -4.4943986 -4.5036812 -4.4926257 -4.4554906 -4.3987393 -4.3765783 -4.4340353 -4.5336828 -4.6233487 -4.668961 -4.6598635 -4.6164565 -4.5531011][-4.4864836 -4.4651136 -4.4550371 -4.444643 -4.4153976 -4.3635092 -4.3024073 -4.2906446 -4.361125 -4.4686313 -4.562819 -4.6172433 -4.623848 -4.602942 -4.5744581][-4.4570208 -4.4265523 -4.408987 -4.3808532 -4.326189 -4.2507324 -4.1805196 -4.1797085 -4.2639995 -4.3741655 -4.4682684 -4.5323215 -4.560411 -4.5777564 -4.595222][-4.4335237 -4.4152575 -4.4051261 -4.3638844 -4.2821264 -4.1773338 -4.0895023 -4.0920081 -4.184895 -4.2926655 -4.3850427 -4.4554596 -4.5042624 -4.5600848 -4.6107435][-4.4225712 -4.4293537 -4.4388633 -4.392272 -4.2828031 -4.1429873 -4.0300903 -4.0272694 -4.1223783 -4.2294736 -4.3232822 -4.3988934 -4.4598823 -4.5368 -4.5911884][-4.4203629 -4.4431782 -4.4537988 -4.3857803 -4.2376318 -4.0556769 -3.9174414 -3.9153662 -4.0267987 -4.1556187 -4.2703543 -4.35476 -4.412199 -4.4744706 -4.4936709][-4.4342055 -4.43831 -4.4039211 -4.280539 -4.0818715 -3.8653083 -3.7231927 -3.7473164 -3.9002285 -4.0806494 -4.2402773 -4.3339658 -4.3640275 -4.3746071 -4.3322506][-4.4591575 -4.4205041 -4.3170104 -4.1303158 -3.8975222 -3.6760578 -3.5571377 -3.6204743 -3.8194098 -4.0573435 -4.2672753 -4.3649898 -4.3519187 -4.2963138 -4.1943007][-4.4813643 -4.413003 -4.2702703 -4.064064 -3.8417072 -3.6483588 -3.559761 -3.6443658 -3.8549774 -4.1140389 -4.3409219 -4.4228029 -4.363862 -4.2539067 -4.1125937][-4.49338 -4.4359746 -4.3135157 -4.1437516 -3.9655707 -3.8084953 -3.7368407 -3.8131912 -3.9938526 -4.2241311 -4.4203715 -4.4653325 -4.3699312 -4.2281904 -4.0657368][-4.4865637 -4.4743304 -4.4118228 -4.3050823 -4.1792541 -4.0549617 -3.988147 -4.0407038 -4.1736927 -4.3466163 -4.4826994 -4.4842196 -4.3701835 -4.2233992 -4.0584459][-4.449656 -4.4870234 -4.4836526 -4.430799 -4.3471646 -4.2552023 -4.2032003 -4.2447562 -4.3381648 -4.4516468 -4.5288115 -4.5037808 -4.3965011 -4.2688208 -4.1209688][-4.3854809 -4.4554286 -4.4890432 -4.4685225 -4.4150095 -4.3572555 -4.3378787 -4.3908167 -4.4674363 -4.5384169 -4.5718222 -4.5414844 -4.4634891 -4.368329 -4.2459784][-4.3195548 -4.4017668 -4.4529538 -4.4525275 -4.4215636 -4.3924222 -4.4018283 -4.4697719 -4.5453863 -4.5941663 -4.6029468 -4.580493 -4.5392766 -4.4782557 -4.3809466][-4.2631125 -4.3413892 -4.3995066 -4.4141541 -4.3999968 -4.385149 -4.4019752 -4.4669504 -4.5424414 -4.5892444 -4.5945778 -4.5844488 -4.5708275 -4.5375857 -4.4724908]]...]
INFO - root - 2017-12-07 15:07:10.766322: step 26110, loss = 21.66, batch loss = 21.58 (8.6 examples/sec; 0.935 sec/batch; 79h:32m:02s remains)
INFO - root - 2017-12-07 15:07:20.176790: step 26120, loss = 21.15, batch loss = 21.07 (8.6 examples/sec; 0.925 sec/batch; 78h:45m:13s remains)
INFO - root - 2017-12-07 15:07:29.672326: step 26130, loss = 21.50, batch loss = 21.41 (8.4 examples/sec; 0.957 sec/batch; 81h:28m:17s remains)
INFO - root - 2017-12-07 15:07:39.151020: step 26140, loss = 21.31, batch loss = 21.22 (8.5 examples/sec; 0.936 sec/batch; 79h:40m:17s remains)
INFO - root - 2017-12-07 15:07:48.467931: step 26150, loss = 21.53, batch loss = 21.44 (8.4 examples/sec; 0.947 sec/batch; 80h:34m:42s remains)
INFO - root - 2017-12-07 15:07:57.942629: step 26160, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.950 sec/batch; 80h:51m:37s remains)
INFO - root - 2017-12-07 15:08:07.377373: step 26170, loss = 21.33, batch loss = 21.25 (8.4 examples/sec; 0.951 sec/batch; 80h:53m:16s remains)
INFO - root - 2017-12-07 15:08:16.558528: step 26180, loss = 21.40, batch loss = 21.32 (8.9 examples/sec; 0.896 sec/batch; 76h:12m:23s remains)
INFO - root - 2017-12-07 15:08:25.904034: step 26190, loss = 21.45, batch loss = 21.37 (8.8 examples/sec; 0.906 sec/batch; 77h:07m:18s remains)
INFO - root - 2017-12-07 15:08:35.312695: step 26200, loss = 21.50, batch loss = 21.42 (8.6 examples/sec; 0.926 sec/batch; 78h:48m:11s remains)
2017-12-07 15:08:36.308645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3630948 -4.3914876 -4.4146056 -4.4202971 -4.4150887 -4.4086666 -4.4127522 -4.425766 -4.43366 -4.4267368 -4.4077382 -4.3885584 -4.3805261 -4.3831782 -4.3868594][-4.4416971 -4.4939542 -4.5301609 -4.5375261 -4.5219054 -4.5028524 -4.5053005 -4.5315609 -4.5500131 -4.5411806 -4.5136986 -4.4865203 -4.4798117 -4.4885445 -4.4978242][-4.529768 -4.6043129 -4.6491451 -4.6502833 -4.6135683 -4.56536 -4.5495386 -4.5823789 -4.6186767 -4.6187611 -4.5953221 -4.5761337 -4.5824523 -4.6043072 -4.6232004][-4.5933747 -4.6843472 -4.7381725 -4.7360592 -4.6738958 -4.5799117 -4.51778 -4.5347409 -4.5883026 -4.6118708 -4.61208 -4.6222644 -4.6547179 -4.6936216 -4.7205029][-4.6192603 -4.7265234 -4.7987676 -4.8053632 -4.71865 -4.5615067 -4.4207053 -4.3911424 -4.4564638 -4.5155296 -4.5582933 -4.61785 -4.6853247 -4.7372427 -4.7613564][-4.6172342 -4.7353716 -4.8201952 -4.829299 -4.705565 -4.4667592 -4.22632 -4.13365 -4.2152858 -4.3332152 -4.4424191 -4.5643487 -4.6651654 -4.7171474 -4.7281528][-4.5962658 -4.7046714 -4.7761879 -4.7639337 -4.5937095 -4.2782111 -3.945236 -3.7965984 -3.9063363 -4.1080022 -4.3041511 -4.4881153 -4.608264 -4.6480784 -4.6463137][-4.5718122 -4.6544142 -4.6919088 -4.6436229 -4.4413576 -4.0912261 -3.7084703 -3.51611 -3.6418183 -3.9214697 -4.20458 -4.4414787 -4.5691738 -4.5866251 -4.5674105][-4.5625563 -4.6254845 -4.6403208 -4.5757155 -4.379878 -4.0528655 -3.6753354 -3.4568524 -3.5646846 -3.8779154 -4.2165489 -4.4802618 -4.5978565 -4.5829377 -4.5348096][-4.5697093 -4.6272807 -4.6487932 -4.6095266 -4.4671154 -4.2110195 -3.8886757 -3.6665816 -3.7174232 -3.9925766 -4.3254066 -4.5793161 -4.679359 -4.6419806 -4.5679865][-4.5762215 -4.6369381 -4.6724105 -4.66849 -4.5874686 -4.41198 -4.1650968 -3.967804 -3.966223 -4.1675911 -4.4480867 -4.6650453 -4.7475972 -4.7075253 -4.6249542][-4.5604191 -4.6199718 -4.6622949 -4.6861272 -4.6655707 -4.5773826 -4.4233384 -4.2771444 -4.2464533 -4.369348 -4.5707784 -4.7307138 -4.7861814 -4.7457428 -4.666451][-4.5200167 -4.5778179 -4.6253061 -4.6672783 -4.6906404 -4.6749768 -4.6080146 -4.5211039 -4.4816737 -4.5339675 -4.6461482 -4.7423983 -4.77252 -4.7384152 -4.6718559][-4.4668541 -4.5204368 -4.5661454 -4.60656 -4.6411667 -4.6626635 -4.6551895 -4.621182 -4.5903354 -4.5975604 -4.6384311 -4.6805367 -4.6925392 -4.6696687 -4.6232028][-4.4034071 -4.4502821 -4.4947805 -4.5333743 -4.5684915 -4.5994716 -4.6176796 -4.6154795 -4.6004238 -4.5890508 -4.5870686 -4.5903535 -4.5875468 -4.572207 -4.5418153]]...]
INFO - root - 2017-12-07 15:08:45.771161: step 26210, loss = 21.37, batch loss = 21.29 (8.8 examples/sec; 0.914 sec/batch; 77h:45m:04s remains)
INFO - root - 2017-12-07 15:08:55.084457: step 26220, loss = 21.68, batch loss = 21.60 (9.7 examples/sec; 0.826 sec/batch; 70h:16m:11s remains)
INFO - root - 2017-12-07 15:09:04.442955: step 26230, loss = 21.42, batch loss = 21.34 (8.6 examples/sec; 0.930 sec/batch; 79h:07m:59s remains)
INFO - root - 2017-12-07 15:09:13.855284: step 26240, loss = 21.66, batch loss = 21.58 (8.1 examples/sec; 0.986 sec/batch; 83h:51m:11s remains)
INFO - root - 2017-12-07 15:09:23.222381: step 26250, loss = 21.44, batch loss = 21.36 (8.4 examples/sec; 0.953 sec/batch; 81h:05m:03s remains)
INFO - root - 2017-12-07 15:09:32.561375: step 26260, loss = 21.61, batch loss = 21.52 (8.8 examples/sec; 0.911 sec/batch; 77h:27m:51s remains)
INFO - root - 2017-12-07 15:09:42.068144: step 26270, loss = 21.62, batch loss = 21.54 (9.1 examples/sec; 0.878 sec/batch; 74h:43m:12s remains)
INFO - root - 2017-12-07 15:09:51.497282: step 26280, loss = 21.03, batch loss = 20.95 (9.0 examples/sec; 0.886 sec/batch; 75h:19m:27s remains)
INFO - root - 2017-12-07 15:10:00.929779: step 26290, loss = 21.75, batch loss = 21.67 (8.4 examples/sec; 0.949 sec/batch; 80h:41m:53s remains)
INFO - root - 2017-12-07 15:10:10.416496: step 26300, loss = 21.41, batch loss = 21.33 (8.1 examples/sec; 0.983 sec/batch; 83h:34m:27s remains)
2017-12-07 15:10:11.460941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4017253 -4.3908195 -4.4073834 -4.4451046 -4.4836297 -4.5452561 -4.5565972 -4.4875112 -4.40375 -4.3103323 -4.2379131 -4.2628713 -4.2921095 -4.3280959 -4.4371729][-4.3545523 -4.370892 -4.4078946 -4.4556184 -4.463654 -4.484962 -4.4942765 -4.4314671 -4.3334851 -4.2286458 -4.1386762 -4.116981 -4.0989246 -4.1126461 -4.2236032][-4.3202596 -4.375814 -4.4441152 -4.5019588 -4.4760284 -4.4466729 -4.4315367 -4.3712893 -4.2928572 -4.2177911 -4.1334114 -4.0649838 -3.9849832 -3.9489744 -4.0297394][-4.3192229 -4.4108777 -4.5079904 -4.5683894 -4.5156121 -4.4348187 -4.3667493 -4.2861271 -4.246604 -4.2414217 -4.2014232 -4.1254306 -4.0170178 -3.9417772 -3.9761124][-4.3344922 -4.4330306 -4.5336676 -4.5810194 -4.5110607 -4.3847718 -4.2459779 -4.1198859 -4.1157017 -4.2068768 -4.2505684 -4.2296219 -4.1672192 -4.0937696 -4.0770655][-4.3596225 -4.429584 -4.4976015 -4.5051918 -4.404922 -4.2371545 -4.0383024 -3.8656905 -3.8827412 -4.0614753 -4.1844292 -4.2372866 -4.2656937 -4.237124 -4.19548][-4.4441009 -4.467586 -4.4752836 -4.425446 -4.2856464 -4.0952587 -3.8737395 -3.6629016 -3.6701822 -3.890645 -4.053493 -4.1421547 -4.2312779 -4.2383409 -4.200254][-4.5622711 -4.5609217 -4.5225887 -4.4311 -4.2640028 -4.0733123 -3.8744369 -3.6587861 -3.6347811 -3.8455024 -4.0011392 -4.0770226 -4.1629786 -4.1673789 -4.1460261][-4.6326752 -4.6513147 -4.6188273 -4.5322495 -4.3675785 -4.1977043 -4.0508022 -3.8713872 -3.8115938 -3.9442759 -4.040731 -4.0778251 -4.13741 -4.13248 -4.1359215][-4.5915651 -4.6683464 -4.6912389 -4.6584654 -4.5335755 -4.3922319 -4.2886724 -4.1570225 -4.0739346 -4.1180296 -4.1465845 -4.1490779 -4.183742 -4.1789136 -4.2023273][-4.4560089 -4.5897765 -4.6804166 -4.7169 -4.6578755 -4.5594578 -4.4904032 -4.4029269 -4.3193808 -4.3030748 -4.2915344 -4.2792754 -4.2962523 -4.2956271 -4.3213735][-4.3294625 -4.4808025 -4.6022148 -4.6820207 -4.6889668 -4.645257 -4.6061573 -4.5563111 -4.4964366 -4.4602795 -4.4368572 -4.4230366 -4.4282336 -4.4304137 -4.4413447][-4.2833242 -4.4156866 -4.5231929 -4.6014366 -4.6399188 -4.6383829 -4.6278787 -4.6132326 -4.5860004 -4.553822 -4.5320139 -4.5248184 -4.5260153 -4.5279107 -4.5160933][-4.2827992 -4.3886251 -4.46922 -4.5238247 -4.5572786 -4.5717254 -4.5849404 -4.6005239 -4.5973811 -4.5709162 -4.55177 -4.5514441 -4.5551186 -4.5587997 -4.5341606][-4.2994637 -4.377008 -4.4323826 -4.4640493 -4.4778066 -4.4850016 -4.5025339 -4.52783 -4.5345936 -4.5170212 -4.5087724 -4.518023 -4.5255718 -4.5265055 -4.49703]]...]
INFO - root - 2017-12-07 15:10:20.843177: step 26310, loss = 21.68, batch loss = 21.60 (8.2 examples/sec; 0.980 sec/batch; 83h:23m:28s remains)
INFO - root - 2017-12-07 15:10:30.197833: step 26320, loss = 21.78, batch loss = 21.70 (7.8 examples/sec; 1.020 sec/batch; 86h:43m:32s remains)
INFO - root - 2017-12-07 15:10:39.660079: step 26330, loss = 21.44, batch loss = 21.36 (8.1 examples/sec; 0.982 sec/batch; 83h:32m:35s remains)
INFO - root - 2017-12-07 15:10:49.126882: step 26340, loss = 20.99, batch loss = 20.90 (8.6 examples/sec; 0.935 sec/batch; 79h:30m:18s remains)
INFO - root - 2017-12-07 15:10:58.567958: step 26350, loss = 21.01, batch loss = 20.92 (8.9 examples/sec; 0.902 sec/batch; 76h:42m:17s remains)
INFO - root - 2017-12-07 15:11:07.955853: step 26360, loss = 21.84, batch loss = 21.76 (8.1 examples/sec; 0.985 sec/batch; 83h:47m:37s remains)
INFO - root - 2017-12-07 15:11:17.362416: step 26370, loss = 21.61, batch loss = 21.52 (8.4 examples/sec; 0.955 sec/batch; 81h:12m:24s remains)
INFO - root - 2017-12-07 15:11:26.709341: step 26380, loss = 21.18, batch loss = 21.10 (8.4 examples/sec; 0.950 sec/batch; 80h:46m:11s remains)
INFO - root - 2017-12-07 15:11:35.896154: step 26390, loss = 21.61, batch loss = 21.53 (8.9 examples/sec; 0.900 sec/batch; 76h:33m:53s remains)
INFO - root - 2017-12-07 15:11:45.182569: step 26400, loss = 21.50, batch loss = 21.42 (9.4 examples/sec; 0.853 sec/batch; 72h:31m:25s remains)
2017-12-07 15:11:46.114920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3894386 -4.4084544 -4.4239492 -4.4285827 -4.4247074 -4.4320369 -4.46032 -4.4934554 -4.5058012 -4.4835238 -4.4324336 -4.3666019 -4.3031197 -4.2643089 -4.2581148][-4.4290013 -4.4441 -4.4474874 -4.435678 -4.4139318 -4.4081974 -4.4400172 -4.5021944 -4.5628915 -4.5859852 -4.5584893 -4.4931946 -4.4123206 -4.3444071 -4.3075171][-4.4463167 -4.4435482 -4.419888 -4.380415 -4.3317504 -4.3008814 -4.3202014 -4.40533 -4.5308375 -4.6314721 -4.66094 -4.619226 -4.5337591 -4.4402585 -4.3687663][-4.4493423 -4.4186935 -4.3560419 -4.2801108 -4.201261 -4.1389241 -4.1300139 -4.2199278 -4.4043841 -4.5937982 -4.7010484 -4.7034383 -4.6312685 -4.5268464 -4.4303927][-4.4495707 -4.3994708 -4.29573 -4.1733227 -4.0516472 -3.9415064 -3.8829236 -3.9547217 -4.1784225 -4.4479465 -4.6379938 -4.6982546 -4.6587262 -4.5661736 -4.4662056][-4.4618545 -4.4162931 -4.2887669 -4.1173739 -3.9337306 -3.748687 -3.6136918 -3.6422238 -3.8770416 -4.2024317 -4.4654989 -4.5898757 -4.5984688 -4.540585 -4.462522][-4.4830232 -4.4706693 -4.3550797 -4.1574407 -3.9128163 -3.6442795 -3.4225874 -3.391607 -3.6056304 -3.9462068 -4.250701 -4.4254246 -4.4803743 -4.4656186 -4.4255772][-4.5006824 -4.5349517 -4.4687743 -4.2910776 -4.0216455 -3.6993515 -3.4185107 -3.333272 -3.5002575 -3.8096924 -4.1085796 -4.2970104 -4.3722992 -4.3841839 -4.375175][-4.4949179 -4.5704193 -4.5768518 -4.4685869 -4.2319727 -3.9094555 -3.6114275 -3.4891155 -3.596395 -3.8427637 -4.096879 -4.2622461 -4.3268323 -4.3374214 -4.3354263][-4.4548049 -4.5488214 -4.6215558 -4.60799 -4.4567981 -4.1932349 -3.9222176 -3.780848 -3.8256578 -3.9959023 -4.1865649 -4.3077178 -4.3425083 -4.3322644 -4.3160968][-4.3917356 -4.4785376 -4.5836091 -4.6467652 -4.600142 -4.4401093 -4.2397933 -4.1087294 -4.106492 -4.2034893 -4.3217616 -4.3861003 -4.3831048 -4.3481574 -4.3129854][-4.3421044 -4.4065065 -4.5039287 -4.5950665 -4.6256523 -4.573144 -4.467392 -4.3787289 -4.3595037 -4.4013538 -4.4506655 -4.4561687 -4.4170065 -4.3635516 -4.3161149][-4.3177257 -4.3590708 -4.4255581 -4.4991007 -4.5552435 -4.5734668 -4.5525231 -4.5225244 -4.5162539 -4.5310516 -4.5353417 -4.5026221 -4.4418216 -4.3774762 -4.3231397][-4.3033171 -4.3270307 -4.3594289 -4.3954234 -4.436193 -4.4736233 -4.4982753 -4.513659 -4.53105 -4.54505 -4.5392561 -4.5034571 -4.4478774 -4.3886847 -4.3366294][-4.290463 -4.2997265 -4.3051257 -4.3082676 -4.3208756 -4.3431387 -4.3693776 -4.3963966 -4.4241614 -4.4465432 -4.4534464 -4.4389138 -4.4084806 -4.3713021 -4.335906]]...]
INFO - root - 2017-12-07 15:11:55.510627: step 26410, loss = 21.79, batch loss = 21.71 (8.1 examples/sec; 0.983 sec/batch; 83h:36m:07s remains)
INFO - root - 2017-12-07 15:12:04.855149: step 26420, loss = 21.27, batch loss = 21.18 (9.0 examples/sec; 0.891 sec/batch; 75h:43m:05s remains)
INFO - root - 2017-12-07 15:12:14.236041: step 26430, loss = 21.91, batch loss = 21.83 (8.7 examples/sec; 0.917 sec/batch; 77h:56m:06s remains)
INFO - root - 2017-12-07 15:12:23.697231: step 26440, loss = 21.25, batch loss = 21.16 (8.9 examples/sec; 0.895 sec/batch; 76h:03m:40s remains)
INFO - root - 2017-12-07 15:12:33.234359: step 26450, loss = 21.19, batch loss = 21.11 (8.4 examples/sec; 0.950 sec/batch; 80h:47m:34s remains)
INFO - root - 2017-12-07 15:12:42.682317: step 26460, loss = 21.59, batch loss = 21.50 (8.2 examples/sec; 0.972 sec/batch; 82h:38m:41s remains)
INFO - root - 2017-12-07 15:12:52.261267: step 26470, loss = 21.31, batch loss = 21.23 (8.2 examples/sec; 0.974 sec/batch; 82h:48m:44s remains)
INFO - root - 2017-12-07 15:13:01.635954: step 26480, loss = 21.57, batch loss = 21.49 (9.1 examples/sec; 0.884 sec/batch; 75h:08m:18s remains)
INFO - root - 2017-12-07 15:13:11.115935: step 26490, loss = 21.69, batch loss = 21.60 (8.0 examples/sec; 0.997 sec/batch; 84h:46m:51s remains)
INFO - root - 2017-12-07 15:13:20.660890: step 26500, loss = 21.45, batch loss = 21.36 (8.1 examples/sec; 0.994 sec/batch; 84h:27m:36s remains)
2017-12-07 15:13:21.622192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5731735 -4.5728583 -4.5459948 -4.504086 -4.4868131 -4.4976482 -4.5029907 -4.4815702 -4.451375 -4.4381504 -4.4561839 -4.5042496 -4.5419054 -4.5278082 -4.4472427][-4.6262746 -4.6069493 -4.556488 -4.4925437 -4.4476357 -4.429821 -4.4354563 -4.4492774 -4.4679365 -4.4871697 -4.51311 -4.5478292 -4.5536628 -4.5035086 -4.3966804][-4.629025 -4.5965 -4.5310478 -4.4480629 -4.367857 -4.3084421 -4.29939 -4.3474512 -4.4249511 -4.4905019 -4.5358233 -4.5602803 -4.5410519 -4.4738512 -4.3674755][-4.608624 -4.5888977 -4.5292735 -4.4354668 -4.3141718 -4.199646 -4.1510768 -4.2011604 -4.3158884 -4.4282374 -4.505878 -4.5427489 -4.5235453 -4.4608688 -4.3671484][-4.5778766 -4.5872059 -4.5472183 -4.4488635 -4.2906485 -4.1200066 -4.0127006 -4.0278482 -4.1507916 -4.3014641 -4.4247828 -4.4971423 -4.502398 -4.45994 -4.381012][-4.5597849 -4.5818138 -4.5474439 -4.4407091 -4.258513 -4.0528145 -3.8979356 -3.8645458 -3.9726925 -4.1528172 -4.3243308 -4.4372749 -4.4690208 -4.4471211 -4.3877339][-4.5823069 -4.5888863 -4.5374084 -4.416523 -4.2239676 -4.0112686 -3.8340671 -3.7619357 -3.844924 -4.0332818 -4.2321973 -4.3635445 -4.4031024 -4.3911953 -4.3549995][-4.6176333 -4.5978818 -4.5239449 -4.3922195 -4.2040009 -4.0053697 -3.8351545 -3.756722 -3.8303556 -4.0141664 -4.2092237 -4.3323026 -4.3693843 -4.3635473 -4.3500876][-4.6026468 -4.5694752 -4.4906888 -4.3682642 -4.2092104 -4.054985 -3.9363251 -3.9010224 -3.9813223 -4.1369314 -4.2848406 -4.3675938 -4.3945866 -4.398128 -4.4028177][-4.5686526 -4.5417161 -4.4715419 -4.3638124 -4.2388878 -4.1357012 -4.0770726 -4.0889144 -4.1712689 -4.2835236 -4.36597 -4.40752 -4.4400816 -4.4728341 -4.5014548][-4.5482607 -4.5333948 -4.4672647 -4.3631692 -4.2473063 -4.1559396 -4.1138544 -4.1438637 -4.2301893 -4.32241 -4.378221 -4.4149051 -4.4714913 -4.5374222 -4.5842795][-4.5049987 -4.4835877 -4.4101462 -4.308301 -4.1987929 -4.1013808 -4.0466633 -4.0713468 -4.16007 -4.2547212 -4.3151526 -4.3638835 -4.4405556 -4.5281181 -4.589045][-4.4405818 -4.3963 -4.3135433 -4.2303023 -4.1518016 -4.0723724 -4.0191164 -4.0381327 -4.1155534 -4.1942325 -4.2428603 -4.2857456 -4.3588152 -4.4484177 -4.51655][-4.4139528 -4.34675 -4.2585273 -4.2000294 -4.1615157 -4.1172428 -4.0860724 -4.1040287 -4.1568136 -4.1979895 -4.2155042 -4.2361584 -4.2911839 -4.3671641 -4.4298][-4.4402995 -4.3630009 -4.2759576 -4.2304597 -4.2186575 -4.20994 -4.213901 -4.2456231 -4.2817278 -4.2878838 -4.2700105 -4.2624512 -4.2956071 -4.3536773 -4.3997545]]...]
INFO - root - 2017-12-07 15:13:31.240603: step 26510, loss = 21.64, batch loss = 21.55 (8.3 examples/sec; 0.963 sec/batch; 81h:51m:02s remains)
INFO - root - 2017-12-07 15:13:40.655173: step 26520, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.956 sec/batch; 81h:13m:13s remains)
INFO - root - 2017-12-07 15:13:50.100268: step 26530, loss = 21.53, batch loss = 21.44 (8.3 examples/sec; 0.969 sec/batch; 82h:23m:17s remains)
INFO - root - 2017-12-07 15:13:59.492898: step 26540, loss = 21.41, batch loss = 21.33 (8.3 examples/sec; 0.963 sec/batch; 81h:50m:42s remains)
INFO - root - 2017-12-07 15:14:08.785195: step 26550, loss = 21.57, batch loss = 21.49 (8.9 examples/sec; 0.896 sec/batch; 76h:08m:41s remains)
INFO - root - 2017-12-07 15:14:18.261361: step 26560, loss = 21.33, batch loss = 21.25 (8.3 examples/sec; 0.965 sec/batch; 81h:59m:24s remains)
INFO - root - 2017-12-07 15:14:27.588545: step 26570, loss = 22.00, batch loss = 21.92 (9.0 examples/sec; 0.884 sec/batch; 75h:07m:57s remains)
INFO - root - 2017-12-07 15:14:36.871901: step 26580, loss = 21.79, batch loss = 21.71 (9.0 examples/sec; 0.892 sec/batch; 75h:48m:06s remains)
INFO - root - 2017-12-07 15:14:46.211363: step 26590, loss = 21.51, batch loss = 21.42 (8.4 examples/sec; 0.951 sec/batch; 80h:47m:16s remains)
INFO - root - 2017-12-07 15:14:55.586143: step 26600, loss = 21.71, batch loss = 21.63 (8.8 examples/sec; 0.910 sec/batch; 77h:20m:19s remains)
2017-12-07 15:14:56.583974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4403715 -4.486342 -4.5193954 -4.5451078 -4.5775771 -4.6138778 -4.6344881 -4.623251 -4.5830441 -4.5322504 -4.497582 -4.5032864 -4.54857 -4.6086845 -4.6559992][-4.4856086 -4.54309 -4.5754433 -4.5922236 -4.6106639 -4.6251469 -4.622479 -4.6028037 -4.5774975 -4.5512905 -4.5240669 -4.5083947 -4.5263195 -4.5774779 -4.6390638][-4.4920192 -4.5468554 -4.5707884 -4.5705214 -4.5569658 -4.5215859 -4.4641929 -4.4143353 -4.4075112 -4.4366169 -4.4570189 -4.4405951 -4.4199018 -4.4412475 -4.5100708][-4.4827485 -4.5183215 -4.5278826 -4.5058684 -4.445395 -4.3410172 -4.2149916 -4.1299686 -4.1460419 -4.2420726 -4.3340206 -4.3361731 -4.2657337 -4.2250705 -4.2744317][-4.4942212 -4.5117497 -4.5079527 -4.4583473 -4.3324227 -4.137002 -3.9327002 -3.8141661 -3.8559632 -4.0137753 -4.1772051 -4.2099705 -4.1007872 -3.992023 -4.0070262][-4.529685 -4.5371757 -4.5167704 -4.4302411 -4.2332458 -3.946557 -3.6639555 -3.5113144 -3.5716846 -3.7770431 -3.9978666 -4.0643077 -3.9409771 -3.7875066 -3.7682867][-4.5659657 -4.5895028 -4.5729914 -4.4724607 -4.2436113 -3.9121246 -3.5800695 -3.3938277 -3.4502068 -3.6755896 -3.9202714 -4.0009084 -3.8723047 -3.6910543 -3.6352179][-4.5917616 -4.665359 -4.6881671 -4.6092186 -4.3955789 -4.0756779 -3.7424195 -3.5397773 -3.5674319 -3.7646427 -3.9831755 -4.0565796 -3.9479215 -3.7809696 -3.7081292][-4.5745878 -4.702785 -4.7726126 -4.7295809 -4.5562415 -4.2926021 -4.0101938 -3.8226497 -3.81757 -3.9554119 -4.1142559 -4.1709919 -4.1029205 -3.986305 -3.9224682][-4.5054474 -4.6651273 -4.774363 -4.7776127 -4.6669769 -4.4948211 -4.3133168 -4.1831512 -4.1626682 -4.2343969 -4.321002 -4.3527074 -4.3203387 -4.2548556 -4.2118487][-4.4584913 -4.6168804 -4.7375984 -4.7692556 -4.7060671 -4.6105704 -4.52417 -4.4636126 -4.4528766 -4.4833207 -4.5176678 -4.5308738 -4.5241919 -4.5025353 -4.4847503][-4.4680452 -4.5916009 -4.6840229 -4.7088928 -4.663763 -4.6074963 -4.5719919 -4.5562053 -4.5629468 -4.5853386 -4.6055222 -4.6185207 -4.6272912 -4.6293159 -4.6283455][-4.5280218 -4.6077476 -4.6524677 -4.6472316 -4.5956764 -4.5439057 -4.5155349 -4.5087056 -4.5221028 -4.5486917 -4.576745 -4.6033831 -4.6244988 -4.6357651 -4.6383309][-4.6141143 -4.6558418 -4.6513209 -4.6048441 -4.5288448 -4.4590883 -4.4139743 -4.3967876 -4.4074631 -4.4387975 -4.4780507 -4.5176153 -4.5481491 -4.56184 -4.558897][-4.6729231 -4.6818342 -4.6453362 -4.5720906 -4.4802222 -4.3995333 -4.3467016 -4.3268428 -4.3377428 -4.3705778 -4.4116759 -4.4513731 -4.4785304 -4.4839072 -4.4679341]]...]
INFO - root - 2017-12-07 15:15:06.005679: step 26610, loss = 21.64, batch loss = 21.56 (8.2 examples/sec; 0.971 sec/batch; 82h:31m:28s remains)
INFO - root - 2017-12-07 15:15:15.577890: step 26620, loss = 21.21, batch loss = 21.13 (8.4 examples/sec; 0.949 sec/batch; 80h:35m:56s remains)
INFO - root - 2017-12-07 15:15:24.848511: step 26630, loss = 21.22, batch loss = 21.14 (9.0 examples/sec; 0.892 sec/batch; 75h:47m:03s remains)
INFO - root - 2017-12-07 15:15:34.085844: step 26640, loss = 21.61, batch loss = 21.53 (8.5 examples/sec; 0.937 sec/batch; 79h:35m:18s remains)
INFO - root - 2017-12-07 15:15:43.558913: step 26650, loss = 21.62, batch loss = 21.54 (8.7 examples/sec; 0.919 sec/batch; 78h:05m:23s remains)
INFO - root - 2017-12-07 15:15:52.983591: step 26660, loss = 21.42, batch loss = 21.34 (7.8 examples/sec; 1.029 sec/batch; 87h:23m:46s remains)
INFO - root - 2017-12-07 15:16:02.525189: step 26670, loss = 21.49, batch loss = 21.40 (8.5 examples/sec; 0.942 sec/batch; 79h:59m:26s remains)
INFO - root - 2017-12-07 15:16:11.923059: step 26680, loss = 21.25, batch loss = 21.17 (8.5 examples/sec; 0.946 sec/batch; 80h:23m:36s remains)
INFO - root - 2017-12-07 15:16:21.337950: step 26690, loss = 20.99, batch loss = 20.91 (8.3 examples/sec; 0.963 sec/batch; 81h:46m:45s remains)
INFO - root - 2017-12-07 15:16:30.607574: step 26700, loss = 21.28, batch loss = 21.20 (8.4 examples/sec; 0.956 sec/batch; 81h:12m:18s remains)
2017-12-07 15:16:31.533319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5404339 -4.561049 -4.549962 -4.550755 -4.5860376 -4.625309 -4.6419196 -4.6499381 -4.6592455 -4.6673613 -4.6586556 -4.6336546 -4.6047678 -4.5885472 -4.5908546][-4.6348848 -4.6601648 -4.64059 -4.6333694 -4.669807 -4.7079277 -4.7126312 -4.7016697 -4.6949973 -4.693419 -4.67517 -4.6397781 -4.6045551 -4.5890083 -4.6037316][-4.6853042 -4.7223721 -4.7123551 -4.7082047 -4.733768 -4.7414312 -4.70168 -4.6434937 -4.6029568 -4.5841312 -4.5595555 -4.5291567 -4.512094 -4.5206966 -4.563189][-4.6938438 -4.7421045 -4.7545166 -4.7596612 -4.7672949 -4.7253022 -4.6226969 -4.5104308 -4.4387579 -4.4075551 -4.3834338 -4.36893 -4.3852468 -4.4332833 -4.5128665][-4.6836257 -4.7371874 -4.7631702 -4.7630706 -4.7355332 -4.6335173 -4.4700618 -4.3275061 -4.2606773 -4.2483392 -4.2419505 -4.2462296 -4.2911596 -4.3708611 -4.481523][-4.6296759 -4.6807022 -4.70707 -4.6869969 -4.6147933 -4.452683 -4.239141 -4.0937862 -4.071424 -4.1213245 -4.1665521 -4.1990981 -4.2594013 -4.3475251 -4.4654393][-4.4672122 -4.5211935 -4.5611191 -4.5372567 -4.4323015 -4.2143049 -3.9527173 -3.8158202 -3.8591306 -3.9988058 -4.1203294 -4.1904125 -4.2509332 -4.3210478 -4.4183683][-4.2356458 -4.2991 -4.3744178 -4.3787088 -4.2683234 -4.0083208 -3.707468 -3.5868661 -3.700084 -3.9253454 -4.1111217 -4.2030835 -4.2436543 -4.2780862 -4.3409076][-4.0819755 -4.1339931 -4.2330213 -4.2667484 -4.1703806 -3.9078426 -3.6213183 -3.5485632 -3.7201421 -3.9837592 -4.1788926 -4.2543025 -4.2548637 -4.2474413 -4.2766705][-4.1248784 -4.1266565 -4.2012453 -4.2302356 -4.149539 -3.9334426 -3.7274961 -3.7306418 -3.9299541 -4.1700444 -4.3231874 -4.3577104 -4.3180237 -4.2815418 -4.290185][-4.2812591 -4.2205553 -4.2411466 -4.24283 -4.1772838 -4.0305037 -3.9228678 -3.9894876 -4.1832695 -4.3701558 -4.4766541 -4.4870892 -4.4320655 -4.3906074 -4.3934226][-4.3986845 -4.3017297 -4.2889037 -4.2799435 -4.23876 -4.158205 -4.1242609 -4.2153716 -4.367919 -4.4881215 -4.5518827 -4.5548153 -4.5098624 -4.4825358 -4.4932537][-4.4512558 -4.3722239 -4.3698344 -4.3776097 -4.36308 -4.3280516 -4.3301563 -4.41133 -4.5026464 -4.5507827 -4.567132 -4.5508385 -4.5102429 -4.4959488 -4.5170536][-4.4963727 -4.4725032 -4.5005717 -4.5209966 -4.5133705 -4.4969258 -4.5129285 -4.5751624 -4.6152763 -4.6071253 -4.5768375 -4.5302134 -4.4803309 -4.4659362 -4.4889789][-4.5329909 -4.5484381 -4.5824428 -4.5932937 -4.5789614 -4.5674095 -4.587738 -4.635006 -4.648407 -4.6136141 -4.5568614 -4.4889855 -4.4327469 -4.417305 -4.4358749]]...]
INFO - root - 2017-12-07 15:16:40.799059: step 26710, loss = 21.40, batch loss = 21.31 (8.4 examples/sec; 0.954 sec/batch; 80h:59m:58s remains)
INFO - root - 2017-12-07 15:16:50.387689: step 26720, loss = 21.65, batch loss = 21.57 (8.2 examples/sec; 0.976 sec/batch; 82h:54m:55s remains)
INFO - root - 2017-12-07 15:16:59.934474: step 26730, loss = 21.08, batch loss = 21.00 (8.2 examples/sec; 0.979 sec/batch; 83h:09m:00s remains)
INFO - root - 2017-12-07 15:17:09.338023: step 26740, loss = 21.53, batch loss = 21.45 (8.4 examples/sec; 0.952 sec/batch; 80h:51m:51s remains)
INFO - root - 2017-12-07 15:17:18.767544: step 26750, loss = 21.52, batch loss = 21.44 (8.9 examples/sec; 0.896 sec/batch; 76h:05m:43s remains)
INFO - root - 2017-12-07 15:17:28.242411: step 26760, loss = 21.63, batch loss = 21.55 (8.7 examples/sec; 0.916 sec/batch; 77h:47m:12s remains)
INFO - root - 2017-12-07 15:17:37.700267: step 26770, loss = 21.22, batch loss = 21.14 (8.7 examples/sec; 0.921 sec/batch; 78h:12m:40s remains)
INFO - root - 2017-12-07 15:17:47.150729: step 26780, loss = 21.57, batch loss = 21.49 (8.0 examples/sec; 0.997 sec/batch; 84h:42m:14s remains)
INFO - root - 2017-12-07 15:17:56.603022: step 26790, loss = 21.38, batch loss = 21.29 (8.0 examples/sec; 0.994 sec/batch; 84h:25m:23s remains)
INFO - root - 2017-12-07 15:18:05.863872: step 26800, loss = 21.67, batch loss = 21.59 (8.4 examples/sec; 0.948 sec/batch; 80h:28m:27s remains)
2017-12-07 15:18:06.918716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6858139 -4.6992021 -4.6374211 -4.5701427 -4.5203552 -4.4813075 -4.4593897 -4.4692593 -4.5016446 -4.5120006 -4.4789443 -4.4316454 -4.4246516 -4.404449 -4.3012824][-4.7094789 -4.7494831 -4.7070131 -4.6423922 -4.5652442 -4.4758015 -4.4134536 -4.4156613 -4.4736385 -4.531846 -4.5298595 -4.4711556 -4.4321933 -4.3869119 -4.2776942][-4.7467794 -4.8103876 -4.7712789 -4.6845074 -4.559917 -4.4087896 -4.3067608 -4.3121138 -4.4129319 -4.5360579 -4.5843382 -4.5228219 -4.4463539 -4.3717504 -4.2565489][-4.7894497 -4.8632841 -4.8109503 -4.6991239 -4.5433993 -4.3519959 -4.2110772 -4.202148 -4.3244257 -4.5001569 -4.6017494 -4.5457697 -4.4318614 -4.3240004 -4.20298][-4.8150806 -4.8731322 -4.7886748 -4.6528959 -4.4876542 -4.2784691 -4.0972204 -4.0528641 -4.1831303 -4.4148574 -4.5894389 -4.574101 -4.443995 -4.3001275 -4.1637936][-4.8133636 -4.8372893 -4.7112665 -4.5503392 -4.3834505 -4.1673236 -3.9492233 -3.8702118 -4.0160484 -4.3077149 -4.5600638 -4.601932 -4.4716163 -4.29476 -4.1495943][-4.798543 -4.7948875 -4.63565 -4.4426565 -4.2584133 -4.0294986 -3.7827094 -3.6837766 -3.863081 -4.2151265 -4.5268383 -4.6116691 -4.4923863 -4.3080492 -4.1767197][-4.7815113 -4.7668924 -4.584856 -4.3495779 -4.1195149 -3.8563709 -3.5838833 -3.4884794 -3.7253811 -4.1411853 -4.4995918 -4.6259155 -4.546308 -4.3981185 -4.3033948][-4.7740073 -4.7714911 -4.5954366 -4.3415866 -4.0731721 -3.7829318 -3.5070744 -3.43646 -3.7102454 -4.1450081 -4.5078149 -4.656168 -4.6266274 -4.5368543 -4.4837275][-4.7721534 -4.8049541 -4.6694069 -4.4381948 -4.1701803 -3.8802505 -3.6195369 -3.565093 -3.8219244 -4.2210827 -4.5513859 -4.70086 -4.7104926 -4.6650333 -4.6299853][-4.769341 -4.8446126 -4.7692947 -4.5935993 -4.3610311 -4.0938368 -3.8560047 -3.8056767 -4.0265479 -4.3655167 -4.6390028 -4.7676492 -4.7941189 -4.763999 -4.720252][-4.7616868 -4.8706856 -4.8531151 -4.7421026 -4.5678296 -4.351151 -4.1602106 -4.1250191 -4.29982 -4.5521083 -4.7365608 -4.8169007 -4.830658 -4.7921824 -4.7314305][-4.7379718 -4.8604565 -4.8775177 -4.8166246 -4.7060814 -4.5594931 -4.430028 -4.4068627 -4.5215578 -4.674377 -4.7687626 -4.8004484 -4.7981772 -4.7507014 -4.679184][-4.6933517 -4.8090448 -4.838387 -4.8064575 -4.7434874 -4.6554732 -4.5660424 -4.531898 -4.5826206 -4.6582079 -4.6998086 -4.7106376 -4.708076 -4.6627817 -4.5866652][-4.6312938 -4.7266617 -4.751029 -4.7247448 -4.6771741 -4.6131158 -4.5314541 -4.4722724 -4.4740739 -4.509306 -4.5389056 -4.5596938 -4.5803332 -4.5530114 -4.4787927]]...]
INFO - root - 2017-12-07 15:18:16.403506: step 26810, loss = 21.25, batch loss = 21.17 (8.9 examples/sec; 0.894 sec/batch; 75h:56m:09s remains)
INFO - root - 2017-12-07 15:18:25.942206: step 26820, loss = 21.52, batch loss = 21.44 (8.7 examples/sec; 0.920 sec/batch; 78h:08m:57s remains)
INFO - root - 2017-12-07 15:18:35.448343: step 26830, loss = 21.37, batch loss = 21.29 (8.3 examples/sec; 0.962 sec/batch; 81h:42m:14s remains)
INFO - root - 2017-12-07 15:18:44.842240: step 26840, loss = 21.18, batch loss = 21.09 (8.4 examples/sec; 0.949 sec/batch; 80h:36m:17s remains)
INFO - root - 2017-12-07 15:18:54.226008: step 26850, loss = 21.66, batch loss = 21.58 (8.8 examples/sec; 0.914 sec/batch; 77h:35m:19s remains)
INFO - root - 2017-12-07 15:19:03.656687: step 26860, loss = 21.46, batch loss = 21.38 (8.6 examples/sec; 0.934 sec/batch; 79h:16m:43s remains)
INFO - root - 2017-12-07 15:19:13.182383: step 26870, loss = 21.58, batch loss = 21.50 (8.5 examples/sec; 0.939 sec/batch; 79h:42m:50s remains)
INFO - root - 2017-12-07 15:19:22.559399: step 26880, loss = 21.88, batch loss = 21.79 (9.1 examples/sec; 0.883 sec/batch; 74h:59m:32s remains)
INFO - root - 2017-12-07 15:19:32.043151: step 26890, loss = 21.00, batch loss = 20.92 (8.5 examples/sec; 0.943 sec/batch; 80h:04m:43s remains)
INFO - root - 2017-12-07 15:19:41.378852: step 26900, loss = 20.83, batch loss = 20.75 (8.1 examples/sec; 0.993 sec/batch; 84h:15m:42s remains)
2017-12-07 15:19:42.338082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4909229 -4.5016894 -4.4977541 -4.4869485 -4.4711246 -4.44849 -4.4244485 -4.4099908 -4.4143639 -4.4391251 -4.4706292 -4.4976544 -4.5091105 -4.5015411 -4.4780755][-4.5592709 -4.5651259 -4.55603 -4.5469232 -4.533391 -4.4989295 -4.4512415 -4.4306726 -4.4637566 -4.528574 -4.5873833 -4.625843 -4.636169 -4.6151118 -4.5672612][-4.5639582 -4.5584607 -4.542511 -4.5388484 -4.5349336 -4.4836917 -4.3989086 -4.3731866 -4.456131 -4.5789075 -4.6640396 -4.7053404 -4.7130165 -4.6815443 -4.6116834][-4.5180769 -4.5050826 -4.4817123 -4.481092 -4.4862819 -4.4102349 -4.2685747 -4.227252 -4.3714871 -4.5713978 -4.6937404 -4.7418776 -4.7465529 -4.6993785 -4.5976992][-4.482317 -4.4584236 -4.4081059 -4.3845053 -4.3754444 -4.2592659 -4.0477219 -3.9766731 -4.1739631 -4.4573717 -4.639761 -4.7232251 -4.7454481 -4.6907196 -4.5567045][-4.4517188 -4.4102311 -4.3182163 -4.250556 -4.2025909 -4.0343318 -3.7494187 -3.6322131 -3.8481474 -4.1991549 -4.4576731 -4.6036286 -4.6689019 -4.634531 -4.4961681][-4.4190907 -4.3809085 -4.2762127 -4.1834784 -4.0945711 -3.8738356 -3.5309095 -3.364707 -3.561193 -3.9408188 -4.262073 -4.4684324 -4.5742545 -4.5691462 -4.4493937][-4.4414849 -4.42533 -4.3451252 -4.2621369 -4.1550508 -3.9119415 -3.5587885 -3.36941 -3.5254238 -3.8894877 -4.2305779 -4.4562488 -4.5640697 -4.56173 -4.4559264][-4.5232511 -4.5265884 -4.4860268 -4.4422293 -4.3547149 -4.1347537 -3.8280747 -3.6519225 -3.7581573 -4.0581131 -4.356813 -4.5468736 -4.6198964 -4.6025925 -4.5043097][-4.5902467 -4.6116118 -4.6115336 -4.6090732 -4.5555706 -4.3761477 -4.1267591 -3.9765339 -4.0340343 -4.2454662 -4.4543381 -4.5685854 -4.5927424 -4.566246 -4.4883938][-4.5771089 -4.621799 -4.6581707 -4.6931486 -4.672977 -4.5329709 -4.3303113 -4.1981688 -4.2144194 -4.3421879 -4.4616165 -4.5064068 -4.4975 -4.4827633 -4.4452586][-4.4983864 -4.5719376 -4.6321182 -4.6844058 -4.6812205 -4.5668058 -4.3941522 -4.2716107 -4.2562428 -4.3266964 -4.3954654 -4.4191957 -4.427124 -4.4552407 -4.4698935][-4.3774471 -4.4940443 -4.5714169 -4.6135983 -4.5956144 -4.4789834 -4.3205509 -4.210681 -4.1895704 -4.2387867 -4.2980733 -4.3448267 -4.4049225 -4.4862633 -4.54313][-4.2734938 -4.4273629 -4.5103879 -4.5289431 -4.4900308 -4.3803635 -4.254282 -4.1747217 -4.1594777 -4.19404 -4.2446404 -4.3078995 -4.4032845 -4.5122027 -4.5856566][-4.2439823 -4.4093075 -4.4797497 -4.4699812 -4.4146719 -4.3224359 -4.2380881 -4.1935911 -4.1885457 -4.212152 -4.2513843 -4.3142595 -4.4112997 -4.5118208 -4.5734954]]...]
INFO - root - 2017-12-07 15:19:51.560687: step 26910, loss = 21.32, batch loss = 21.24 (9.1 examples/sec; 0.882 sec/batch; 74h:52m:17s remains)
INFO - root - 2017-12-07 15:20:00.929700: step 26920, loss = 21.28, batch loss = 21.19 (9.1 examples/sec; 0.879 sec/batch; 74h:36m:46s remains)
INFO - root - 2017-12-07 15:20:10.344848: step 26930, loss = 21.65, batch loss = 21.57 (9.1 examples/sec; 0.884 sec/batch; 75h:01m:34s remains)
INFO - root - 2017-12-07 15:20:19.749147: step 26940, loss = 21.42, batch loss = 21.34 (8.7 examples/sec; 0.924 sec/batch; 78h:23m:48s remains)
INFO - root - 2017-12-07 15:20:29.121729: step 26950, loss = 21.28, batch loss = 21.19 (8.6 examples/sec; 0.933 sec/batch; 79h:10m:51s remains)
INFO - root - 2017-12-07 15:20:38.465595: step 26960, loss = 21.37, batch loss = 21.29 (8.3 examples/sec; 0.963 sec/batch; 81h:42m:01s remains)
INFO - root - 2017-12-07 15:20:47.827171: step 26970, loss = 21.53, batch loss = 21.44 (8.7 examples/sec; 0.925 sec/batch; 78h:28m:50s remains)
INFO - root - 2017-12-07 15:20:57.121730: step 26980, loss = 21.71, batch loss = 21.63 (8.9 examples/sec; 0.901 sec/batch; 76h:25m:48s remains)
INFO - root - 2017-12-07 15:21:06.602153: step 26990, loss = 21.58, batch loss = 21.50 (8.3 examples/sec; 0.963 sec/batch; 81h:41m:20s remains)
INFO - root - 2017-12-07 15:21:15.773519: step 27000, loss = 22.00, batch loss = 21.92 (8.6 examples/sec; 0.925 sec/batch; 78h:29m:51s remains)
2017-12-07 15:21:16.642897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1544094 -4.2768459 -4.4312487 -4.5410643 -4.5432978 -4.4809513 -4.4238248 -4.3756008 -4.3516865 -4.3716793 -4.430335 -4.4966235 -4.51461 -4.4804659 -4.4457436][-4.1754556 -4.290081 -4.4345794 -4.5313 -4.5240192 -4.4608483 -4.393486 -4.3292775 -4.3036966 -4.3376346 -4.419909 -4.5160642 -4.5581975 -4.5235162 -4.46695][-4.2525992 -4.3647337 -4.4965591 -4.5839739 -4.5741115 -4.5054607 -4.4140482 -4.3346119 -4.3261847 -4.3997908 -4.5118532 -4.6155195 -4.6536808 -4.6079583 -4.5281625][-4.3440628 -4.4810953 -4.6166191 -4.6941485 -4.6635914 -4.5523486 -4.3971667 -4.2738843 -4.2667446 -4.3729615 -4.5072379 -4.6084809 -4.647253 -4.62078 -4.5561891][-4.3908749 -4.5584855 -4.7018075 -4.7586412 -4.6834278 -4.4988842 -4.2518773 -4.0509195 -4.0166879 -4.1508837 -4.3246326 -4.4543533 -4.5276709 -4.5516591 -4.5331497][-4.3978958 -4.580061 -4.7141938 -4.7254829 -4.5836487 -4.3195696 -3.9785135 -3.6956058 -3.6389375 -3.8271644 -4.0806489 -4.2773952 -4.4072137 -4.4820609 -4.5029135][-4.4067445 -4.5875335 -4.6998167 -4.6585479 -4.4552622 -4.1310072 -3.7205946 -3.3767438 -3.3143411 -3.5676908 -3.9099336 -4.1796327 -4.3604093 -4.4653153 -4.49911][-4.4210014 -4.5968833 -4.702703 -4.6473327 -4.4385061 -4.1169853 -3.7034907 -3.3559885 -3.3015718 -3.5793128 -3.9507017 -4.2433305 -4.4258375 -4.5116262 -4.5203705][-4.4495425 -4.6049919 -4.7087278 -4.6681809 -4.4993033 -4.2355924 -3.8873324 -3.5956686 -3.5590312 -3.8064859 -4.134439 -4.3912892 -4.5332427 -4.5676622 -4.5357814][-4.5276508 -4.6498637 -4.7424459 -4.7184567 -4.5944023 -4.3957148 -4.1336145 -3.9219887 -3.904573 -4.0887733 -4.3254066 -4.5107765 -4.6011834 -4.5910816 -4.5330796][-4.6053729 -4.7040467 -4.7905769 -4.7942743 -4.7210674 -4.5869765 -4.4124141 -4.2822146 -4.2769237 -4.380311 -4.5007849 -4.5942273 -4.6260762 -4.5820742 -4.5119276][-4.6267071 -4.7005734 -4.7763572 -4.806025 -4.7825828 -4.7142057 -4.6223545 -4.559742 -4.5560956 -4.5797229 -4.59205 -4.5993524 -4.5862384 -4.5314064 -4.4648628][-4.58036 -4.6273589 -4.6808047 -4.7169452 -4.725286 -4.707365 -4.67545 -4.6569486 -4.6550436 -4.6321764 -4.5785403 -4.5318518 -4.4990005 -4.4536448 -4.3994207][-4.456038 -4.4879689 -4.5256948 -4.5603871 -4.5852046 -4.5949154 -4.5938215 -4.600666 -4.6092439 -4.5788064 -4.5051541 -4.4407897 -4.4066906 -4.3741155 -4.328599][-4.3139009 -4.3325329 -4.3550024 -4.3794823 -4.4039779 -4.4228764 -4.4357872 -4.4595566 -4.4861379 -4.4722528 -4.4088774 -4.3514285 -4.327353 -4.3070121 -4.268014]]...]
INFO - root - 2017-12-07 15:21:26.018417: step 27010, loss = 21.61, batch loss = 21.53 (8.9 examples/sec; 0.896 sec/batch; 76h:00m:06s remains)
INFO - root - 2017-12-07 15:21:35.386056: step 27020, loss = 21.25, batch loss = 21.17 (8.8 examples/sec; 0.904 sec/batch; 76h:43m:25s remains)
INFO - root - 2017-12-07 15:21:44.785342: step 27030, loss = 21.21, batch loss = 21.13 (8.5 examples/sec; 0.946 sec/batch; 80h:15m:40s remains)
INFO - root - 2017-12-07 15:21:54.165933: step 27040, loss = 21.76, batch loss = 21.67 (8.5 examples/sec; 0.945 sec/batch; 80h:13m:06s remains)
INFO - root - 2017-12-07 15:22:03.623861: step 27050, loss = 21.38, batch loss = 21.30 (8.3 examples/sec; 0.963 sec/batch; 81h:40m:59s remains)
INFO - root - 2017-12-07 15:22:13.120346: step 27060, loss = 21.19, batch loss = 21.10 (8.7 examples/sec; 0.920 sec/batch; 78h:01m:23s remains)
INFO - root - 2017-12-07 15:22:22.602762: step 27070, loss = 21.06, batch loss = 20.98 (8.2 examples/sec; 0.978 sec/batch; 82h:59m:25s remains)
INFO - root - 2017-12-07 15:22:32.073923: step 27080, loss = 21.59, batch loss = 21.51 (8.7 examples/sec; 0.918 sec/batch; 77h:55m:12s remains)
INFO - root - 2017-12-07 15:22:41.275513: step 27090, loss = 21.32, batch loss = 21.24 (8.9 examples/sec; 0.899 sec/batch; 76h:17m:19s remains)
INFO - root - 2017-12-07 15:22:50.718422: step 27100, loss = 21.19, batch loss = 21.10 (8.2 examples/sec; 0.972 sec/batch; 82h:27m:07s remains)
2017-12-07 15:22:51.654797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.363874 -4.3710022 -4.3802733 -4.3889713 -4.3970475 -4.4035225 -4.407805 -4.4101067 -4.4108973 -4.4125113 -4.415401 -4.4180803 -4.41926 -4.4172883 -4.411387][-4.4242163 -4.4382777 -4.448657 -4.45666 -4.4634333 -4.469316 -4.4730005 -4.47444 -4.4768319 -4.4837046 -4.4928069 -4.4994574 -4.5008812 -4.4952474 -4.4842358][-4.4981761 -4.5127325 -4.5130634 -4.506896 -4.5014586 -4.4993191 -4.4961796 -4.492413 -4.4964819 -4.5147243 -4.5393686 -4.555922 -4.5603428 -4.5532737 -4.5390515][-4.551569 -4.5585718 -4.5364323 -4.5012627 -4.4698625 -4.4456639 -4.4205604 -4.3966966 -4.3962197 -4.4327526 -4.4879556 -4.5284996 -4.5462589 -4.5478826 -4.5411544][-4.55833 -4.5514555 -4.5018663 -4.4306211 -4.366828 -4.3138337 -4.2579832 -4.2034154 -4.1914034 -4.245491 -4.339633 -4.4188118 -4.4639721 -4.4846849 -4.4931636][-4.5277491 -4.5082388 -4.438961 -4.3387918 -4.2478228 -4.1700125 -4.0882311 -4.0096097 -3.9859879 -4.0479345 -4.1702476 -4.2873182 -4.3681941 -4.4129367 -4.43587][-4.4874091 -4.469676 -4.4006596 -4.2874293 -4.1744828 -4.0732007 -3.9768348 -3.897831 -3.8756928 -3.9333658 -4.0544834 -4.187603 -4.2977924 -4.3673611 -4.4045515][-4.4497232 -4.4482074 -4.3995137 -4.2900052 -4.1583495 -4.0362568 -3.9434779 -3.8947763 -3.8949225 -3.9458516 -4.0400267 -4.1558938 -4.2704597 -4.3503504 -4.3946247][-4.4153767 -4.4326749 -4.4063106 -4.3098807 -4.1744609 -4.0504036 -3.9798861 -3.9781148 -4.0143785 -4.0644093 -4.1219263 -4.19505 -4.2841272 -4.3546705 -4.39766][-4.3911781 -4.4105749 -4.3958006 -4.3158336 -4.1972346 -4.0976863 -4.0598059 -4.0961208 -4.1588116 -4.2057433 -4.229517 -4.2566051 -4.308847 -4.3637738 -4.4053893][-4.3731875 -4.3756709 -4.3539238 -4.2876768 -4.2041593 -4.1479807 -4.138391 -4.1836352 -4.2433233 -4.2792325 -4.2844763 -4.2848296 -4.31116 -4.3530736 -4.3950028][-4.3603826 -4.3367705 -4.2958536 -4.2354383 -4.1853557 -4.1711159 -4.1730704 -4.1886907 -4.2044468 -4.2152958 -4.2236137 -4.238101 -4.2729344 -4.3196507 -4.3676214][-4.3458314 -4.3123212 -4.2619081 -4.2038479 -4.1721158 -4.175869 -4.167563 -4.1326466 -4.0854568 -4.0593591 -4.0754867 -4.1318235 -4.2148395 -4.2959056 -4.3594527][-4.3150373 -4.2994647 -4.2681394 -4.2254405 -4.1969371 -4.1898417 -4.15627 -4.0737462 -3.9765141 -3.9236186 -3.9518046 -4.0545778 -4.1933722 -4.314683 -4.3880315][-4.2693534 -4.2771554 -4.2842607 -4.2752457 -4.2557812 -4.2319155 -4.1737852 -4.0629678 -3.9451721 -3.8840647 -3.9212894 -4.050559 -4.218936 -4.3574843 -4.4256334]]...]
INFO - root - 2017-12-07 15:23:00.968799: step 27110, loss = 21.33, batch loss = 21.25 (8.2 examples/sec; 0.979 sec/batch; 83h:00m:27s remains)
INFO - root - 2017-12-07 15:23:10.356900: step 27120, loss = 21.23, batch loss = 21.15 (8.6 examples/sec; 0.931 sec/batch; 79h:00m:10s remains)
INFO - root - 2017-12-07 15:23:19.747900: step 27130, loss = 21.63, batch loss = 21.55 (8.5 examples/sec; 0.941 sec/batch; 79h:49m:19s remains)
INFO - root - 2017-12-07 15:23:29.103891: step 27140, loss = 21.11, batch loss = 21.03 (8.7 examples/sec; 0.918 sec/batch; 77h:54m:15s remains)
INFO - root - 2017-12-07 15:23:38.564933: step 27150, loss = 21.03, batch loss = 20.95 (8.5 examples/sec; 0.938 sec/batch; 79h:32m:03s remains)
INFO - root - 2017-12-07 15:23:47.959302: step 27160, loss = 21.42, batch loss = 21.33 (8.7 examples/sec; 0.919 sec/batch; 77h:54m:47s remains)
INFO - root - 2017-12-07 15:23:57.231385: step 27170, loss = 21.38, batch loss = 21.30 (8.7 examples/sec; 0.916 sec/batch; 77h:41m:19s remains)
INFO - root - 2017-12-07 15:24:06.639024: step 27180, loss = 20.97, batch loss = 20.89 (8.3 examples/sec; 0.966 sec/batch; 81h:57m:23s remains)
INFO - root - 2017-12-07 15:24:16.200071: step 27190, loss = 21.51, batch loss = 21.43 (8.2 examples/sec; 0.976 sec/batch; 82h:43m:54s remains)
INFO - root - 2017-12-07 15:24:25.468025: step 27200, loss = 21.23, batch loss = 21.15 (8.8 examples/sec; 0.913 sec/batch; 77h:27m:31s remains)
2017-12-07 15:24:26.417858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4363427 -4.4651828 -4.4900465 -4.5084796 -4.5133576 -4.5204506 -4.5330248 -4.5355587 -4.5249434 -4.4932804 -4.4437571 -4.4021707 -4.3782668 -4.364336 -4.356915][-4.45503 -4.5082517 -4.5398431 -4.5477815 -4.5316639 -4.5303106 -4.5528464 -4.5678158 -4.5705605 -4.5489306 -4.4981232 -4.4505558 -4.4224725 -4.4047565 -4.3942938][-4.4646583 -4.5290952 -4.5492043 -4.5295663 -4.4803305 -4.4662457 -4.49762 -4.5186462 -4.5360789 -4.5402637 -4.5091357 -4.4731331 -4.4513016 -4.4348912 -4.4249229][-4.458478 -4.5106053 -4.4949131 -4.4337606 -4.3501153 -4.3252573 -4.3577642 -4.3674812 -4.3907375 -4.4317966 -4.4409947 -4.4354749 -4.4321718 -4.4246612 -4.4215903][-4.4432034 -4.4661026 -4.4011416 -4.2920089 -4.1751542 -4.138979 -4.1661873 -4.1572218 -4.1874032 -4.2756124 -4.3346791 -4.3666267 -4.3856134 -4.3907981 -4.39875][-4.4312005 -4.427115 -4.3167315 -4.1559529 -3.9966314 -3.9388297 -3.960784 -3.9530034 -4.0151649 -4.1588016 -4.2585511 -4.3134408 -4.3431168 -4.3569884 -4.3757434][-4.4364862 -4.4253349 -4.2854586 -4.07611 -3.8710451 -3.7896454 -3.8157372 -3.8418481 -3.9581823 -4.1488609 -4.2577772 -4.2980466 -4.315587 -4.3308339 -4.3595238][-4.4660926 -4.4644804 -4.3148494 -4.0786252 -3.8519258 -3.766643 -3.8146696 -3.8924685 -4.0567193 -4.2560277 -4.3241911 -4.3036833 -4.2857208 -4.3038058 -4.3525143][-4.4979587 -4.4970708 -4.3463583 -4.1140652 -3.9039824 -3.8382154 -3.9109395 -4.0298362 -4.210063 -4.3711562 -4.3596745 -4.2529988 -4.2024832 -4.2328429 -4.3145266][-4.4951429 -4.4708166 -4.3199534 -4.1198511 -3.9621058 -3.9335835 -4.0231695 -4.1509967 -4.2975354 -4.3850832 -4.2902126 -4.1177311 -4.0581927 -4.1048207 -4.2186584][-4.4450703 -4.3934283 -4.2529125 -4.1081462 -4.0262146 -4.0457392 -4.1366744 -4.22955 -4.2970872 -4.3004942 -4.1497135 -3.9515035 -3.9022365 -3.9613044 -4.0981488][-4.3685379 -4.3105164 -4.2016516 -4.1176581 -4.10602 -4.1623397 -4.2316804 -4.253336 -4.2301183 -4.1755838 -4.0115395 -3.8138294 -3.7664626 -3.8303585 -3.9837685][-4.3182383 -4.2758703 -4.2069979 -4.1726785 -4.2091432 -4.2853837 -4.3208113 -4.2661467 -4.1785297 -4.1085114 -3.9657986 -3.779696 -3.7134488 -3.7605321 -3.9096031][-4.3190918 -4.29689 -4.2604127 -4.2607384 -4.328054 -4.4170074 -4.429698 -4.3265676 -4.2052379 -4.1421618 -4.0322537 -3.862128 -3.7691715 -3.7845757 -3.8985932][-4.3557215 -4.3453445 -4.3218918 -4.3370695 -4.4122181 -4.5042615 -4.5161433 -4.4065862 -4.2807755 -4.227078 -4.1433105 -3.9982243 -3.9062414 -3.9053144 -3.968246]]...]
INFO - root - 2017-12-07 15:24:35.708144: step 27210, loss = 21.01, batch loss = 20.93 (8.2 examples/sec; 0.974 sec/batch; 82h:35m:59s remains)
INFO - root - 2017-12-07 15:24:45.082152: step 27220, loss = 20.74, batch loss = 20.65 (7.7 examples/sec; 1.037 sec/batch; 87h:57m:09s remains)
INFO - root - 2017-12-07 15:24:54.383612: step 27230, loss = 21.56, batch loss = 21.47 (8.8 examples/sec; 0.914 sec/batch; 77h:29m:29s remains)
INFO - root - 2017-12-07 15:25:03.706914: step 27240, loss = 21.77, batch loss = 21.69 (8.9 examples/sec; 0.894 sec/batch; 75h:49m:01s remains)
INFO - root - 2017-12-07 15:25:13.046635: step 27250, loss = 21.83, batch loss = 21.75 (8.9 examples/sec; 0.894 sec/batch; 75h:49m:05s remains)
INFO - root - 2017-12-07 15:25:22.453594: step 27260, loss = 21.37, batch loss = 21.28 (8.0 examples/sec; 1.003 sec/batch; 85h:00m:31s remains)
INFO - root - 2017-12-07 15:25:32.003922: step 27270, loss = 21.22, batch loss = 21.14 (8.4 examples/sec; 0.958 sec/batch; 81h:13m:54s remains)
INFO - root - 2017-12-07 15:25:41.378572: step 27280, loss = 21.49, batch loss = 21.41 (8.7 examples/sec; 0.920 sec/batch; 77h:58m:43s remains)
INFO - root - 2017-12-07 15:25:50.764856: step 27290, loss = 21.04, batch loss = 20.96 (9.1 examples/sec; 0.883 sec/batch; 74h:51m:23s remains)
INFO - root - 2017-12-07 15:25:59.994868: step 27300, loss = 21.59, batch loss = 21.51 (9.1 examples/sec; 0.881 sec/batch; 74h:39m:19s remains)
2017-12-07 15:26:01.017072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5235071 -4.5157166 -4.5030556 -4.4870634 -4.4881968 -4.4980078 -4.4998059 -4.5105743 -4.5207214 -4.507493 -4.4834871 -4.4752417 -4.484457 -4.4982753 -4.507061][-4.5899367 -4.5990577 -4.6004477 -4.5824265 -4.5691471 -4.5653276 -4.5493789 -4.5409656 -4.5412807 -4.527761 -4.5111551 -4.5151715 -4.5372038 -4.5507665 -4.5372047][-4.5707655 -4.5990052 -4.6139789 -4.5895138 -4.5586886 -4.5391893 -4.5034227 -4.4780631 -4.4777164 -4.4762096 -4.4756441 -4.4943395 -4.5243416 -4.5312109 -4.5008407][-4.481133 -4.5189619 -4.5309825 -4.4847889 -4.4349217 -4.4076219 -4.3547883 -4.3107238 -4.3093414 -4.3260846 -4.3582211 -4.4120913 -4.467958 -4.4837966 -4.4510274][-4.3492327 -4.38179 -4.3763995 -4.2998528 -4.2347307 -4.2068872 -4.1342182 -4.0691118 -4.0677724 -4.109179 -4.1971135 -4.3149638 -4.4184241 -4.4579988 -4.425745][-4.2252817 -4.2407465 -4.211762 -4.1011109 -4.0142632 -3.975924 -3.8788333 -3.8036251 -3.8165302 -3.8853695 -4.0316558 -4.216723 -4.3690567 -4.4429822 -4.4211922][-4.19521 -4.1892228 -4.1270323 -3.9750903 -3.8522406 -3.7822921 -3.6607041 -3.6005263 -3.6469488 -3.7424657 -3.9283013 -4.151679 -4.3302469 -4.4347563 -4.4338412][-4.2582932 -4.2382522 -4.1619058 -3.9988308 -3.8646321 -3.7799959 -3.6673968 -3.6432178 -3.7107148 -3.8050365 -3.9840474 -4.191186 -4.3583479 -4.4745197 -4.4873896][-4.3364973 -4.3144679 -4.25867 -4.1426806 -4.0474305 -3.9849796 -3.9115658 -3.9124038 -3.9675424 -4.0345488 -4.1722145 -4.3288894 -4.4598336 -4.5630836 -4.5704536][-4.3670635 -4.3576336 -4.3444786 -4.3065381 -4.2817769 -4.2698503 -4.2494845 -4.2625713 -4.2934809 -4.3322473 -4.419117 -4.5119739 -4.5901079 -4.6501117 -4.6253614][-4.3046131 -4.3264003 -4.3658109 -4.4068308 -4.4574771 -4.5084457 -4.5416365 -4.5666528 -4.5788417 -4.5970554 -4.640821 -4.6727109 -4.6918211 -4.6982279 -4.6365128][-4.1952887 -4.2445087 -4.3227949 -4.4142489 -4.5095644 -4.6001019 -4.6700792 -4.7071881 -4.7120442 -4.7168756 -4.7330527 -4.7290792 -4.7150297 -4.6967044 -4.6259251][-4.12108 -4.1850948 -4.2724905 -4.3682003 -4.4662037 -4.5625114 -4.6474481 -4.6993747 -4.7108121 -4.7132096 -4.7221484 -4.714406 -4.7034822 -4.6933122 -4.6346726][-4.1337943 -4.194345 -4.2674279 -4.3437114 -4.4262161 -4.5133843 -4.5965948 -4.652276 -4.6705141 -4.6800075 -4.701251 -4.716826 -4.731616 -4.7391863 -4.6897793][-4.2208509 -4.2725544 -4.3284025 -4.3868651 -4.4541526 -4.5254855 -4.5923238 -4.6353245 -4.6541996 -4.6733413 -4.7111893 -4.7524242 -4.7850561 -4.795599 -4.7408619]]...]
INFO - root - 2017-12-07 15:26:10.429221: step 27310, loss = 21.25, batch loss = 21.17 (8.5 examples/sec; 0.946 sec/batch; 80h:10m:42s remains)
INFO - root - 2017-12-07 15:26:19.767141: step 27320, loss = 21.75, batch loss = 21.67 (9.0 examples/sec; 0.889 sec/batch; 75h:21m:10s remains)
INFO - root - 2017-12-07 15:26:29.083845: step 27330, loss = 21.41, batch loss = 21.33 (8.7 examples/sec; 0.924 sec/batch; 78h:19m:56s remains)
INFO - root - 2017-12-07 15:26:38.468401: step 27340, loss = 21.40, batch loss = 21.32 (8.3 examples/sec; 0.965 sec/batch; 81h:49m:03s remains)
INFO - root - 2017-12-07 15:26:47.867959: step 27350, loss = 21.35, batch loss = 21.27 (8.3 examples/sec; 0.959 sec/batch; 81h:18m:55s remains)
INFO - root - 2017-12-07 15:26:57.314016: step 27360, loss = 21.23, batch loss = 21.14 (8.0 examples/sec; 0.994 sec/batch; 84h:16m:54s remains)
INFO - root - 2017-12-07 15:27:06.669986: step 27370, loss = 21.41, batch loss = 21.33 (8.2 examples/sec; 0.971 sec/batch; 82h:19m:36s remains)
INFO - root - 2017-12-07 15:27:16.188825: step 27380, loss = 21.10, batch loss = 21.01 (8.3 examples/sec; 0.960 sec/batch; 81h:19m:33s remains)
INFO - root - 2017-12-07 15:27:25.483125: step 27390, loss = 21.22, batch loss = 21.13 (8.2 examples/sec; 0.974 sec/batch; 82h:32m:53s remains)
INFO - root - 2017-12-07 15:27:34.938138: step 27400, loss = 20.94, batch loss = 20.86 (8.6 examples/sec; 0.932 sec/batch; 79h:01m:20s remains)
2017-12-07 15:27:35.904676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5390878 -4.5223556 -4.4956269 -4.5140185 -4.5653939 -4.5809927 -4.5557523 -4.5335612 -4.526505 -4.5478582 -4.583467 -4.6003385 -4.5801888 -4.5300379 -4.4735541][-4.4628339 -4.4339375 -4.4127822 -4.4520712 -4.5185566 -4.5379815 -4.5058064 -4.4788637 -4.4791746 -4.5255704 -4.5940032 -4.6289153 -4.6041536 -4.5258055 -4.4335828][-4.3628321 -4.315382 -4.2996774 -4.3621368 -4.4453392 -4.4705114 -4.4332347 -4.3982482 -4.3986349 -4.4646978 -4.5708132 -4.6344237 -4.6171155 -4.5230842 -4.40332][-4.2410984 -4.1793518 -4.17598 -4.2662878 -4.3714066 -4.4035754 -4.3589005 -4.3114452 -4.3045354 -4.380836 -4.5163465 -4.6117067 -4.613379 -4.5227041 -4.3883476][-4.1559811 -4.0785689 -4.0834036 -4.1915107 -4.3067923 -4.3324089 -4.2701182 -4.2061181 -4.19608 -4.2857351 -4.4463639 -4.5744014 -4.6060071 -4.5332007 -4.3991771][-4.1671576 -4.0787745 -4.074059 -4.1714659 -4.2707348 -4.2704463 -4.1763883 -4.0901389 -4.0842118 -4.194272 -4.3797436 -4.539618 -4.6031609 -4.5495639 -4.4214535][-4.2615371 -4.1732244 -4.1503253 -4.211462 -4.2695966 -4.2291765 -4.0981388 -3.9918294 -3.9993393 -4.1332045 -4.336442 -4.5132709 -4.5874281 -4.5360804 -4.4107318][-4.3491058 -4.2775669 -4.2498536 -4.2693172 -4.2647018 -4.1658969 -3.9920111 -3.8696265 -3.9044147 -4.0711627 -4.2888088 -4.4678569 -4.5385017 -4.4881692 -4.3732457][-4.3484268 -4.3086457 -4.2999387 -4.2974792 -4.2453485 -4.1066475 -3.9089961 -3.7827663 -3.8352647 -4.0129328 -4.2254176 -4.3911257 -4.458353 -4.4216614 -4.3322291][-4.2534909 -4.250679 -4.2785525 -4.2839332 -4.2212181 -4.0769558 -3.8915465 -3.78687 -3.8436966 -4.0054255 -4.1890182 -4.3204246 -4.3707752 -4.338119 -4.2665043][-4.1273422 -4.1504269 -4.2092652 -4.2337241 -4.1792865 -4.0431342 -3.8814669 -3.8085153 -3.8755293 -4.0325994 -4.19534 -4.28696 -4.3057008 -4.26192 -4.1964583][-4.0573559 -4.0962315 -4.1758947 -4.223866 -4.1930051 -4.07952 -3.9443178 -3.8980174 -3.9712362 -4.1235905 -4.2599936 -4.3021717 -4.2743006 -4.2134018 -4.1555142][-4.0842676 -4.1427593 -4.2320385 -4.2937374 -4.2839961 -4.1969061 -4.0892544 -4.0644546 -4.1369286 -4.2705784 -4.3648467 -4.349493 -4.2746172 -4.2028012 -4.1635346][-4.1946754 -4.272882 -4.3547378 -4.4014668 -4.38906 -4.30793 -4.2111921 -4.1915751 -4.2611861 -4.38718 -4.4584689 -4.4128761 -4.3165727 -4.2527251 -4.2450418][-4.3514342 -4.4467058 -4.50851 -4.5167089 -4.476491 -4.3785167 -4.2771835 -4.2482305 -4.3046045 -4.4325714 -4.5083241 -4.4652863 -4.372148 -4.3230433 -4.3457036]]...]
INFO - root - 2017-12-07 15:27:45.250607: step 27410, loss = 21.11, batch loss = 21.03 (9.1 examples/sec; 0.880 sec/batch; 74h:33m:00s remains)
INFO - root - 2017-12-07 15:27:54.538203: step 27420, loss = 21.33, batch loss = 21.24 (8.4 examples/sec; 0.949 sec/batch; 80h:25m:30s remains)
INFO - root - 2017-12-07 15:28:03.855059: step 27430, loss = 21.54, batch loss = 21.46 (8.5 examples/sec; 0.940 sec/batch; 79h:39m:52s remains)
INFO - root - 2017-12-07 15:28:13.267577: step 27440, loss = 21.49, batch loss = 21.41 (8.6 examples/sec; 0.926 sec/batch; 78h:29m:40s remains)
INFO - root - 2017-12-07 15:28:22.605552: step 27450, loss = 21.38, batch loss = 21.29 (8.3 examples/sec; 0.959 sec/batch; 81h:13m:32s remains)
INFO - root - 2017-12-07 15:28:32.194964: step 27460, loss = 21.42, batch loss = 21.33 (8.6 examples/sec; 0.929 sec/batch; 78h:45m:32s remains)
INFO - root - 2017-12-07 15:28:41.645905: step 27470, loss = 22.02, batch loss = 21.94 (8.4 examples/sec; 0.953 sec/batch; 80h:46m:06s remains)
INFO - root - 2017-12-07 15:28:51.131486: step 27480, loss = 21.32, batch loss = 21.23 (8.2 examples/sec; 0.979 sec/batch; 82h:55m:55s remains)
INFO - root - 2017-12-07 15:29:00.307724: step 27490, loss = 21.12, batch loss = 21.04 (8.8 examples/sec; 0.910 sec/batch; 77h:04m:44s remains)
INFO - root - 2017-12-07 15:29:09.586944: step 27500, loss = 21.25, batch loss = 21.17 (8.7 examples/sec; 0.923 sec/batch; 78h:12m:21s remains)
2017-12-07 15:29:10.630078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2274008 -4.291122 -4.322844 -4.3110986 -4.2954922 -4.3036442 -4.3482461 -4.4205041 -4.47326 -4.4855423 -4.4517326 -4.3689523 -4.2600331 -4.1783209 -4.1922936][-4.185936 -4.2453713 -4.2782683 -4.2695913 -4.2524257 -4.2667141 -4.3280425 -4.4156394 -4.4680142 -4.4607067 -4.4009147 -4.3135405 -4.2236247 -4.1637259 -4.1835608][-4.1316853 -4.183599 -4.2238736 -4.2275987 -4.21684 -4.2368717 -4.3010373 -4.3795247 -4.4087176 -4.3706989 -4.2944627 -4.2300634 -4.190383 -4.17214 -4.1923537][-4.0659013 -4.1138482 -4.1657572 -4.1837745 -4.1733408 -4.1860781 -4.2400403 -4.3035641 -4.3126388 -4.2505665 -4.1693106 -4.1430721 -4.1667719 -4.1979938 -4.2183838][-4.0120106 -4.0535183 -4.112205 -4.1344972 -4.1123104 -4.1014233 -4.1318579 -4.1804819 -4.1882954 -4.1296329 -4.0625215 -4.0741663 -4.1516824 -4.2241173 -4.2537513][-3.9850903 -4.0167217 -4.0758896 -4.097199 -4.0585012 -4.0137911 -4.0035939 -4.0286937 -4.048573 -4.0234237 -3.9898143 -4.0275536 -4.1293583 -4.2203207 -4.2652063][-3.9998257 -4.016099 -4.0728679 -4.0968342 -4.0485687 -3.9708607 -3.9070947 -3.8914554 -3.9200587 -3.9438968 -3.9583311 -4.0129862 -4.1113262 -4.1939178 -4.244978][-4.070549 -4.0703225 -4.1164689 -4.1413417 -4.0971031 -4.0007353 -3.8869288 -3.8206434 -3.8428693 -3.9094629 -3.9751673 -4.0459232 -4.123981 -4.1768003 -4.2172][-4.1907105 -4.1782169 -4.2108231 -4.2361512 -4.2061858 -4.113441 -3.9777274 -3.8754265 -3.8800721 -3.963681 -4.0557384 -4.127584 -4.1699948 -4.1795478 -4.1964149][-4.3088984 -4.2889867 -4.3139238 -4.3485727 -4.344183 -4.2746673 -4.1484356 -4.0332594 -4.013947 -4.086525 -4.1792521 -4.2346869 -4.2353225 -4.2035251 -4.1969242][-4.3922877 -4.3743296 -4.3972025 -4.4404221 -4.458559 -4.4198232 -4.3214583 -4.2123485 -4.1740737 -4.2230873 -4.30145 -4.3360319 -4.3042784 -4.2438436 -4.2205119][-4.4505024 -4.4419641 -4.4666648 -4.5089526 -4.5340004 -4.5168252 -4.4482379 -4.3557882 -4.3069096 -4.3309374 -4.3893189 -4.4075484 -4.3620467 -4.2920489 -4.2608228][-4.4674745 -4.46688 -4.4920764 -4.5252914 -4.5452275 -4.5406022 -4.4973445 -4.4251862 -4.3741374 -4.3761759 -4.4093037 -4.4133625 -4.3687296 -4.3066516 -4.278327][-4.4442105 -4.44533 -4.4643264 -4.4825368 -4.4921365 -4.4924784 -4.4670606 -4.414145 -4.367918 -4.355732 -4.3649144 -4.3565607 -4.3197217 -4.2749753 -4.2547913][-4.3879685 -4.384479 -4.3931942 -4.3998432 -4.4002938 -4.39658 -4.3770771 -4.3392954 -4.3039932 -4.2880583 -4.2834396 -4.2714806 -4.2486925 -4.2251859 -4.2155457]]...]
INFO - root - 2017-12-07 15:29:19.941634: step 27510, loss = 21.45, batch loss = 21.37 (8.9 examples/sec; 0.903 sec/batch; 76h:29m:43s remains)
INFO - root - 2017-12-07 15:29:29.103298: step 27520, loss = 21.25, batch loss = 21.17 (9.4 examples/sec; 0.849 sec/batch; 71h:53m:31s remains)
INFO - root - 2017-12-07 15:29:38.616586: step 27530, loss = 21.91, batch loss = 21.82 (8.3 examples/sec; 0.966 sec/batch; 81h:50m:06s remains)
INFO - root - 2017-12-07 15:29:48.051960: step 27540, loss = 21.48, batch loss = 21.40 (7.9 examples/sec; 1.009 sec/batch; 85h:27m:19s remains)
INFO - root - 2017-12-07 15:29:57.401871: step 27550, loss = 21.28, batch loss = 21.20 (8.2 examples/sec; 0.980 sec/batch; 83h:01m:46s remains)
INFO - root - 2017-12-07 15:30:06.821982: step 27560, loss = 21.35, batch loss = 21.27 (8.8 examples/sec; 0.911 sec/batch; 77h:07m:38s remains)
INFO - root - 2017-12-07 15:30:16.136098: step 27570, loss = 21.44, batch loss = 21.36 (8.8 examples/sec; 0.905 sec/batch; 76h:39m:03s remains)
INFO - root - 2017-12-07 15:30:25.621162: step 27580, loss = 21.66, batch loss = 21.58 (8.0 examples/sec; 0.998 sec/batch; 84h:30m:06s remains)
INFO - root - 2017-12-07 15:30:35.090326: step 27590, loss = 21.57, batch loss = 21.49 (8.1 examples/sec; 0.993 sec/batch; 84h:06m:35s remains)
INFO - root - 2017-12-07 15:30:44.486667: step 27600, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.932 sec/batch; 78h:57m:13s remains)
2017-12-07 15:30:45.431202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.37942 -4.4285312 -4.521018 -4.5582175 -4.5227342 -4.4792504 -4.435782 -4.3642898 -4.2808628 -4.2267609 -4.2313619 -4.3053861 -4.4310508 -4.5454097 -4.5972056][-4.3790507 -4.4718575 -4.58699 -4.6054544 -4.5235338 -4.4341383 -4.3601441 -4.2668262 -4.1703715 -4.1220107 -4.1433687 -4.2335744 -4.3686771 -4.4877748 -4.5504055][-4.4014211 -4.5354819 -4.6659226 -4.6480918 -4.4941926 -4.3410912 -4.2466435 -4.1777716 -4.122036 -4.1094427 -4.1484437 -4.2315192 -4.3398924 -4.4359522 -4.4963455][-4.4229951 -4.571692 -4.7018747 -4.6482496 -4.4269543 -4.2120757 -4.1080484 -4.0981584 -4.1278782 -4.1760039 -4.2326708 -4.2890482 -4.3384767 -4.3832679 -4.4287081][-4.3917027 -4.5337954 -4.6551743 -4.58825 -4.3301115 -4.0654345 -3.9498944 -4.0032978 -4.1314464 -4.2491961 -4.3255754 -4.3545151 -4.3424354 -4.3303523 -4.3561087][-4.2925148 -4.4306607 -4.5547819 -4.5019169 -4.2346387 -3.9214082 -3.7712169 -3.8649988 -4.08514 -4.2820497 -4.3922791 -4.4114008 -4.3630562 -4.310914 -4.3171892][-4.1401205 -4.2918043 -4.4357557 -4.4178681 -4.1627355 -3.8010139 -3.5932589 -3.6903634 -3.9727445 -4.2432981 -4.397542 -4.4318848 -4.385654 -4.3258719 -4.3198528][-3.9936798 -4.1516976 -4.3080883 -4.3239341 -4.1068325 -3.7379029 -3.485168 -3.5489023 -3.8324766 -4.1338425 -4.3249364 -4.394743 -4.3820939 -4.3412905 -4.3326521][-3.9315143 -4.0675988 -4.2116652 -4.2450442 -4.0837517 -3.7743244 -3.5341167 -3.5524507 -3.7708797 -4.0307879 -4.2228913 -4.3285303 -4.3639588 -4.3571138 -4.35534][-3.9890187 -4.0799246 -4.1905956 -4.218452 -4.107985 -3.8903697 -3.7157829 -3.7125959 -3.8332031 -3.9972377 -4.1497288 -4.2777171 -4.3653727 -4.4022403 -4.4136767][-4.1502619 -4.1911325 -4.2600989 -4.2746768 -4.2026215 -4.0662737 -3.9620643 -3.954567 -3.9929786 -4.0627742 -4.1660252 -4.2980866 -4.4200315 -4.4875937 -4.503232][-4.327529 -4.3516212 -4.4042463 -4.4189143 -4.3720369 -4.2786579 -4.2085776 -4.1909933 -4.180079 -4.1915636 -4.2585382 -4.3776069 -4.5025654 -4.5755005 -4.5840125][-4.4380794 -4.4722528 -4.5302238 -4.5516496 -4.523787 -4.4543767 -4.3938193 -4.354928 -4.3188672 -4.3090887 -4.3554573 -4.4527349 -4.560338 -4.6257238 -4.6252084][-4.4852519 -4.5211377 -4.5782118 -4.6066904 -4.6000929 -4.5605197 -4.5106792 -4.4534259 -4.4016647 -4.3875971 -4.4268394 -4.5043006 -4.5860724 -4.6348009 -4.6277575][-4.51727 -4.5398464 -4.5808158 -4.6075525 -4.6174283 -4.6085243 -4.5795341 -4.5218444 -4.461823 -4.4417453 -4.470602 -4.5260129 -4.5808845 -4.6099863 -4.5973959]]...]
INFO - root - 2017-12-07 15:30:54.730249: step 27610, loss = 21.52, batch loss = 21.44 (8.0 examples/sec; 1.000 sec/batch; 84h:42m:36s remains)
INFO - root - 2017-12-07 15:31:04.118526: step 27620, loss = 21.65, batch loss = 21.57 (8.2 examples/sec; 0.972 sec/batch; 82h:19m:18s remains)
INFO - root - 2017-12-07 15:31:13.454695: step 27630, loss = 21.39, batch loss = 21.31 (8.9 examples/sec; 0.894 sec/batch; 75h:45m:03s remains)
INFO - root - 2017-12-07 15:31:22.856313: step 27640, loss = 21.59, batch loss = 21.50 (9.0 examples/sec; 0.884 sec/batch; 74h:53m:40s remains)
INFO - root - 2017-12-07 15:31:32.172863: step 27650, loss = 21.25, batch loss = 21.17 (8.6 examples/sec; 0.935 sec/batch; 79h:12m:36s remains)
INFO - root - 2017-12-07 15:31:41.542187: step 27660, loss = 21.63, batch loss = 21.55 (8.1 examples/sec; 0.989 sec/batch; 83h:44m:44s remains)
INFO - root - 2017-12-07 15:31:50.917503: step 27670, loss = 21.52, batch loss = 21.43 (8.0 examples/sec; 1.005 sec/batch; 85h:05m:37s remains)
INFO - root - 2017-12-07 15:32:00.368763: step 27680, loss = 21.06, batch loss = 20.98 (8.6 examples/sec; 0.933 sec/batch; 79h:02m:16s remains)
INFO - root - 2017-12-07 15:32:09.695924: step 27690, loss = 21.17, batch loss = 21.09 (8.5 examples/sec; 0.942 sec/batch; 79h:44m:27s remains)
INFO - root - 2017-12-07 15:32:18.982940: step 27700, loss = 21.34, batch loss = 21.25 (8.8 examples/sec; 0.904 sec/batch; 76h:34m:30s remains)
2017-12-07 15:32:19.874744: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1638975 -4.2818007 -4.3622684 -4.379252 -4.3470211 -4.2771716 -4.2140651 -4.1961684 -4.2065048 -4.2096705 -4.2208009 -4.2841959 -4.3658476 -4.4077058 -4.3945704][-4.2499294 -4.3805604 -4.4701557 -4.5070224 -4.501832 -4.4438477 -4.37254 -4.3441072 -4.3513112 -4.3527422 -4.3532171 -4.3930674 -4.4417005 -4.4483209 -4.4077706][-4.2730489 -4.4015107 -4.4979186 -4.5632296 -4.593955 -4.5438857 -4.4447923 -4.3954616 -4.4092045 -4.4259815 -4.4288211 -4.457819 -4.4882483 -4.46968 -4.4062157][-4.2813969 -4.3998537 -4.5051332 -4.5927591 -4.633656 -4.5510192 -4.3884826 -4.3036704 -4.3400321 -4.4010587 -4.4288516 -4.4673867 -4.4972253 -4.4664831 -4.3885078][-4.3194594 -4.4260893 -4.5348573 -4.6216316 -4.6289444 -4.4696584 -4.2199168 -4.09644 -4.175004 -4.311677 -4.3900819 -4.4507308 -4.4846773 -4.44584 -4.3624525][-4.3537083 -4.4351311 -4.5279932 -4.5835872 -4.52754 -4.2863269 -3.9680207 -3.8216383 -3.9442725 -4.16369 -4.3093204 -4.4062524 -4.4553776 -4.4224544 -4.3469191][-4.3942022 -4.4346852 -4.4804096 -4.4775252 -4.3535028 -4.0654645 -3.7310472 -3.590817 -3.7420225 -4.0139012 -4.2167621 -4.3543196 -4.4295774 -4.4181895 -4.3585844][-4.4525137 -4.4524589 -4.4381428 -4.3716059 -4.2002711 -3.910707 -3.6119261 -3.5033212 -3.6650913 -3.9470613 -4.1721787 -4.3308263 -4.4272776 -4.4397392 -4.3931837][-4.5407834 -4.521944 -4.4665074 -4.3615627 -4.1722217 -3.9030983 -3.653254 -3.5814214 -3.7443707 -4.0091252 -4.2187381 -4.3669128 -4.4630728 -4.4794827 -4.4290509][-4.5947514 -4.5843782 -4.5290885 -4.4304752 -4.26132 -4.0284209 -3.8249421 -3.7873971 -3.9505844 -4.1803665 -4.3379965 -4.4452138 -4.5174971 -4.5140095 -4.4441748][-4.5247736 -4.5401182 -4.5186243 -4.4623981 -4.3461432 -4.168901 -4.0159979 -4.0123219 -4.1791377 -4.3742876 -4.4737082 -4.5306559 -4.5624104 -4.5204892 -4.4257903][-4.3862753 -4.4174118 -4.4303994 -4.4202952 -4.3630548 -4.2452025 -4.1340904 -4.1480565 -4.319088 -4.5034752 -4.5764079 -4.596735 -4.5854163 -4.4995985 -4.3851013][-4.2812948 -4.3113689 -4.3381124 -4.3515525 -4.3322167 -4.2565107 -4.1694779 -4.1809087 -4.3485646 -4.5389862 -4.6131673 -4.61529 -4.5725203 -4.4624071 -4.348392][-4.2486491 -4.2685471 -4.2913885 -4.3047037 -4.2951183 -4.2370143 -4.1569772 -4.1544337 -4.3022861 -4.4868188 -4.5664854 -4.5681181 -4.5218339 -4.4240007 -4.3345871][-4.2578521 -4.2674241 -4.2829247 -4.2917366 -4.280724 -4.2240758 -4.1387987 -4.1143479 -4.2246356 -4.3788738 -4.4569082 -4.4746261 -4.4549637 -4.4010878 -4.34921]]...]
INFO - root - 2017-12-07 15:32:29.238392: step 27710, loss = 21.59, batch loss = 21.51 (8.7 examples/sec; 0.916 sec/batch; 77h:33m:49s remains)
INFO - root - 2017-12-07 15:32:38.516231: step 27720, loss = 21.54, batch loss = 21.45 (8.3 examples/sec; 0.966 sec/batch; 81h:49m:29s remains)
INFO - root - 2017-12-07 15:32:47.862118: step 27730, loss = 21.51, batch loss = 21.43 (8.4 examples/sec; 0.954 sec/batch; 80h:45m:24s remains)
INFO - root - 2017-12-07 15:32:57.146541: step 27740, loss = 21.22, batch loss = 21.13 (8.6 examples/sec; 0.935 sec/batch; 79h:11m:15s remains)
INFO - root - 2017-12-07 15:33:06.669571: step 27750, loss = 21.60, batch loss = 21.52 (8.7 examples/sec; 0.919 sec/batch; 77h:49m:47s remains)
INFO - root - 2017-12-07 15:33:16.136017: step 27760, loss = 20.76, batch loss = 20.68 (8.8 examples/sec; 0.905 sec/batch; 76h:34m:24s remains)
INFO - root - 2017-12-07 15:33:25.534772: step 27770, loss = 21.63, batch loss = 21.55 (8.7 examples/sec; 0.920 sec/batch; 77h:50m:43s remains)
INFO - root - 2017-12-07 15:33:34.792561: step 27780, loss = 21.36, batch loss = 21.28 (8.6 examples/sec; 0.929 sec/batch; 78h:37m:09s remains)
INFO - root - 2017-12-07 15:33:44.177044: step 27790, loss = 21.20, batch loss = 21.11 (8.3 examples/sec; 0.969 sec/batch; 81h:59m:24s remains)
INFO - root - 2017-12-07 15:33:53.424613: step 27800, loss = 21.53, batch loss = 21.44 (8.7 examples/sec; 0.918 sec/batch; 77h:43m:10s remains)
2017-12-07 15:33:54.450112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3936133 -4.381742 -4.3792305 -4.3845382 -4.3926997 -4.4049172 -4.4190311 -4.43769 -4.4610944 -4.4823375 -4.5029254 -4.5243473 -4.50362 -4.4083214 -4.2779794][-4.4118404 -4.3951149 -4.3890376 -4.39219 -4.4020138 -4.424994 -4.4570813 -4.4907532 -4.5229764 -4.5471277 -4.5637388 -4.5748448 -4.5355411 -4.4127173 -4.2615142][-4.4025497 -4.3803954 -4.3731928 -4.3763618 -4.3854094 -4.4094987 -4.449398 -4.4948225 -4.5348611 -4.5602131 -4.57529 -4.5839615 -4.5361357 -4.400526 -4.2451162][-4.3630271 -4.3386173 -4.3344464 -4.3390851 -4.34176 -4.3499026 -4.3762913 -4.4229913 -4.4700866 -4.499907 -4.5196943 -4.5354462 -4.49246 -4.3615685 -4.2191033][-4.3123975 -4.289279 -4.2917566 -4.2998714 -4.2936153 -4.2719393 -4.2597618 -4.2867312 -4.33606 -4.3795452 -4.4191675 -4.4533467 -4.4240761 -4.3091722 -4.1912465][-4.2716413 -4.2485461 -4.2541838 -4.2633905 -4.2464819 -4.1878619 -4.119019 -4.1087189 -4.1613069 -4.2373815 -4.3178339 -4.3827639 -4.3726392 -4.2779937 -4.1865468][-4.2514286 -4.222847 -4.2222304 -4.2257748 -4.1940336 -4.0970311 -3.9687095 -3.9144764 -3.9745817 -4.0998931 -4.2370214 -4.3402658 -4.3528852 -4.2797556 -4.2132425][-4.2476988 -4.2100787 -4.1943154 -4.1829557 -4.1316214 -4.0021753 -3.8283858 -3.7392783 -3.8132691 -3.9948206 -4.1910424 -4.3308187 -4.36362 -4.3075352 -4.2577314][-4.2471352 -4.2029705 -4.1743751 -4.1497602 -4.0851378 -3.9471633 -3.767741 -3.6738935 -3.7590418 -3.9727721 -4.1975102 -4.3519945 -4.3912654 -4.339817 -4.2952518][-4.2510457 -4.207603 -4.1791863 -4.1539845 -4.0899563 -3.9663794 -3.8153448 -3.7416275 -3.8276286 -4.0334754 -4.2404633 -4.3770494 -4.4054832 -4.3497934 -4.3020582][-4.260932 -4.2243886 -4.204711 -4.1863885 -4.131959 -4.0339923 -3.9248149 -3.8811777 -3.9556789 -4.1172838 -4.2706351 -4.369184 -4.3861365 -4.3327985 -4.2800865][-4.2833443 -4.25095 -4.2385187 -4.2297211 -4.1948047 -4.1324978 -4.0671444 -4.0488124 -4.0981159 -4.1941881 -4.2826104 -4.3442135 -4.3630977 -4.326273 -4.2700262][-4.3150458 -4.2841234 -4.2737193 -4.2735853 -4.2616959 -4.2319465 -4.1973491 -4.18788 -4.2053003 -4.240416 -4.2807579 -4.3289967 -4.3695455 -4.3609643 -4.3022013][-4.3546414 -4.3216028 -4.31133 -4.3213134 -4.3307858 -4.3208961 -4.29393 -4.2720037 -4.2565217 -4.2524714 -4.2736392 -4.3262467 -4.3897705 -4.4026747 -4.3397679][-4.3886647 -4.3534961 -4.3410444 -4.3558764 -4.3769245 -4.3718953 -4.3359389 -4.292572 -4.2565064 -4.2426848 -4.2690887 -4.3279696 -4.3982229 -4.4192452 -4.3571329]]...]
INFO - root - 2017-12-07 15:34:03.870873: step 27810, loss = 21.59, batch loss = 21.51 (8.2 examples/sec; 0.975 sec/batch; 82h:31m:22s remains)
INFO - root - 2017-12-07 15:34:13.252901: step 27820, loss = 21.14, batch loss = 21.06 (8.2 examples/sec; 0.978 sec/batch; 82h:44m:52s remains)
INFO - root - 2017-12-07 15:34:22.384925: step 27830, loss = 21.17, batch loss = 21.08 (8.9 examples/sec; 0.902 sec/batch; 76h:20m:49s remains)
INFO - root - 2017-12-07 15:34:31.784875: step 27840, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.931 sec/batch; 78h:45m:31s remains)
INFO - root - 2017-12-07 15:34:41.130260: step 27850, loss = 21.27, batch loss = 21.18 (8.8 examples/sec; 0.907 sec/batch; 76h:47m:26s remains)
INFO - root - 2017-12-07 15:34:50.489273: step 27860, loss = 21.53, batch loss = 21.45 (8.4 examples/sec; 0.958 sec/batch; 81h:01m:37s remains)
INFO - root - 2017-12-07 15:35:00.017692: step 27870, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.944 sec/batch; 79h:54m:47s remains)
INFO - root - 2017-12-07 15:35:09.248524: step 27880, loss = 21.69, batch loss = 21.61 (8.9 examples/sec; 0.903 sec/batch; 76h:23m:26s remains)
INFO - root - 2017-12-07 15:35:18.628338: step 27890, loss = 21.28, batch loss = 21.20 (8.5 examples/sec; 0.940 sec/batch; 79h:31m:36s remains)
INFO - root - 2017-12-07 15:35:28.177321: step 27900, loss = 21.64, batch loss = 21.56 (8.2 examples/sec; 0.977 sec/batch; 82h:41m:41s remains)
2017-12-07 15:35:29.191460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.78273 -4.7307038 -4.6563063 -4.6156988 -4.62522 -4.6474118 -4.6559706 -4.6550565 -4.6594968 -4.6588836 -4.6195526 -4.5470896 -4.4482727 -4.337203 -4.25819][-4.7296991 -4.6931152 -4.6346049 -4.62255 -4.6660709 -4.7044764 -4.7153354 -4.7258706 -4.7478924 -4.7465987 -4.695569 -4.6073155 -4.4923825 -4.3709106 -4.2840271][-4.5646415 -4.5518 -4.5350747 -4.5726781 -4.6508288 -4.692245 -4.6861124 -4.6851311 -4.7172375 -4.7435603 -4.7320952 -4.6715474 -4.5589991 -4.4249711 -4.3219309][-4.3508339 -4.3684654 -4.407073 -4.4968758 -4.5914016 -4.6172619 -4.5889425 -4.5653067 -4.5860972 -4.6332526 -4.6862864 -4.6927905 -4.6107922 -4.4760509 -4.3585453][-4.1301022 -4.1680117 -4.2382712 -4.356988 -4.4541969 -4.4624472 -4.4255104 -4.3956246 -4.4045186 -4.4522271 -4.5518203 -4.6311855 -4.6031461 -4.4929361 -4.3774977][-3.952909 -3.996552 -4.0501242 -4.1525445 -4.2413898 -4.2382274 -4.20121 -4.1883454 -4.217041 -4.2709031 -4.3872328 -4.5123949 -4.5363979 -4.4635859 -4.36481][-3.900969 -3.9365044 -3.949111 -4.0016818 -4.0704584 -4.0558805 -4.0036817 -4.0097055 -4.0929322 -4.18257 -4.2989578 -4.4289927 -4.4709644 -4.4183693 -4.3315372][-3.9586022 -3.9829905 -3.9679227 -3.9724679 -4.0071111 -3.9679918 -3.8814058 -3.8928671 -4.0427804 -4.192565 -4.30645 -4.4107342 -4.4407759 -4.3856144 -4.2960153][-4.0899625 -4.1029806 -4.0914893 -4.0760541 -4.0676703 -3.9836602 -3.8521903 -3.8504117 -4.0439124 -4.2490044 -4.3656139 -4.4402704 -4.4518666 -4.385767 -4.2856879][-4.2478395 -4.2549391 -4.2667871 -4.2605953 -4.2187762 -4.0914021 -3.9296668 -3.911912 -4.1157136 -4.353364 -4.4766412 -4.5211506 -4.5044818 -4.4230027 -4.3135753][-4.3844252 -4.3981228 -4.4351373 -4.4497604 -4.3962822 -4.2555346 -4.1021285 -4.0796757 -4.2533236 -4.4779811 -4.5955696 -4.6128964 -4.5677562 -4.4759364 -4.3675179][-4.5114202 -4.5296292 -4.5744958 -4.599874 -4.5547194 -4.4382634 -4.322927 -4.2994261 -4.4092779 -4.5729761 -4.6676011 -4.6722488 -4.6163936 -4.5270476 -4.429203][-4.59387 -4.6168675 -4.6515131 -4.6703315 -4.6407533 -4.5678477 -4.499217 -4.478313 -4.5232096 -4.6086559 -4.6688175 -4.6721892 -4.6263018 -4.5518441 -4.4697485][-4.5846043 -4.6154757 -4.6424613 -4.654407 -4.640429 -4.6054969 -4.5719428 -4.55474 -4.5584249 -4.5853696 -4.6124697 -4.6151667 -4.5852156 -4.5312614 -4.4690356][-4.5098176 -4.5406303 -4.5629535 -4.5720472 -4.568892 -4.5571671 -4.5452347 -4.5363903 -4.5303793 -4.5330043 -4.5385375 -4.5340791 -4.5118809 -4.4755793 -4.4352179]]...]
INFO - root - 2017-12-07 15:35:38.586221: step 27910, loss = 21.31, batch loss = 21.22 (8.5 examples/sec; 0.938 sec/batch; 79h:22m:57s remains)
INFO - root - 2017-12-07 15:35:47.948355: step 27920, loss = 21.26, batch loss = 21.18 (8.4 examples/sec; 0.955 sec/batch; 80h:49m:40s remains)
INFO - root - 2017-12-07 15:35:57.288902: step 27930, loss = 21.55, batch loss = 21.47 (8.6 examples/sec; 0.932 sec/batch; 78h:52m:58s remains)
INFO - root - 2017-12-07 15:36:06.840449: step 27940, loss = 21.45, batch loss = 21.37 (7.9 examples/sec; 1.010 sec/batch; 85h:25m:14s remains)
INFO - root - 2017-12-07 15:36:15.986891: step 27950, loss = 21.44, batch loss = 21.36 (8.9 examples/sec; 0.897 sec/batch; 75h:51m:32s remains)
INFO - root - 2017-12-07 15:36:25.294132: step 27960, loss = 21.74, batch loss = 21.66 (9.0 examples/sec; 0.894 sec/batch; 75h:36m:22s remains)
INFO - root - 2017-12-07 15:36:34.577440: step 27970, loss = 21.70, batch loss = 21.61 (8.4 examples/sec; 0.957 sec/batch; 80h:55m:48s remains)
INFO - root - 2017-12-07 15:36:43.888746: step 27980, loss = 21.60, batch loss = 21.51 (8.5 examples/sec; 0.942 sec/batch; 79h:43m:21s remains)
INFO - root - 2017-12-07 15:36:53.321096: step 27990, loss = 21.41, batch loss = 21.33 (8.7 examples/sec; 0.920 sec/batch; 77h:49m:10s remains)
INFO - root - 2017-12-07 15:37:02.631798: step 28000, loss = 21.19, batch loss = 21.10 (8.9 examples/sec; 0.896 sec/batch; 75h:46m:34s remains)
2017-12-07 15:37:03.615290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2617054 -4.346508 -4.365067 -4.3640118 -4.4387522 -4.5297866 -4.5656157 -4.549336 -4.521286 -4.5092344 -4.5480332 -4.5955009 -4.5866809 -4.5626869 -4.5851669][-4.2534814 -4.3679466 -4.4093485 -4.41755 -4.4742694 -4.5222254 -4.5105343 -4.4571166 -4.4327512 -4.4570336 -4.5186968 -4.575285 -4.5854878 -4.587162 -4.6188884][-4.31741 -4.45271 -4.5149879 -4.528039 -4.5398211 -4.5096211 -4.4231372 -4.3192654 -4.2960477 -4.3617625 -4.4630628 -4.5503845 -4.5961833 -4.6259861 -4.6507993][-4.3946996 -4.5212407 -4.582396 -4.5835385 -4.5386372 -4.426837 -4.2656641 -4.1165819 -4.0972309 -4.2121968 -4.369072 -4.5020285 -4.5896568 -4.6329474 -4.6260948][-4.4375539 -4.5352893 -4.574388 -4.5514231 -4.4514904 -4.268549 -4.0433006 -3.8611546 -3.8538003 -4.0212426 -4.2422485 -4.4276495 -4.5567164 -4.6012797 -4.5544658][-4.4335551 -4.5022092 -4.5171585 -4.4707522 -4.3327837 -4.0997367 -3.8224347 -3.6073089 -3.6094971 -3.8274496 -4.113029 -4.3536124 -4.5239668 -4.5736756 -4.5047555][-4.4131203 -4.4655385 -4.4710364 -4.4197931 -4.27414 -4.0239854 -3.7159705 -3.4736946 -3.482717 -3.7412448 -4.0688214 -4.3414197 -4.5353127 -4.5912037 -4.5192342][-4.4220691 -4.4738307 -4.482533 -4.4430728 -4.3182797 -4.0820994 -3.7749505 -3.5257678 -3.5348125 -3.8036067 -4.1331758 -4.3955269 -4.576046 -4.6258168 -4.557723][-4.4267879 -4.4886136 -4.5095444 -4.4982958 -4.4158506 -4.2136254 -3.9256163 -3.6858323 -3.6875925 -3.9382904 -4.237401 -4.4533124 -4.5845823 -4.6088004 -4.5437675][-4.4120016 -4.4834218 -4.5186577 -4.5453525 -4.51638 -4.3559527 -4.0920973 -3.8601062 -3.8437748 -4.0523615 -4.2958217 -4.4474325 -4.5200238 -4.5174928 -4.4603229][-4.3805542 -4.4532704 -4.4929624 -4.5450611 -4.5595813 -4.4417496 -4.209269 -3.9862568 -3.9415579 -4.093359 -4.2742405 -4.3755903 -4.4177232 -4.4115553 -4.3664155][-4.2891583 -4.3662176 -4.418211 -4.4812522 -4.51361 -4.4285741 -4.2352452 -4.031539 -3.961648 -4.0600839 -4.196136 -4.2814612 -4.3334904 -4.3487349 -4.317915][-4.1286817 -4.2217054 -4.3004003 -4.3705378 -4.4058495 -4.3513751 -4.21213 -4.0506158 -3.9699445 -4.0279217 -4.1371436 -4.2235675 -4.2935705 -4.3316946 -4.3139987][-4.0141311 -4.1093874 -4.19956 -4.263545 -4.2948356 -4.2741694 -4.1994114 -4.0955396 -4.0209756 -4.0454159 -4.1263561 -4.2028084 -4.2740216 -4.320713 -4.3194308][-4.0315247 -4.0963783 -4.1604948 -4.19745 -4.2145886 -4.2150617 -4.1854563 -4.123477 -4.058239 -4.0569663 -4.1142788 -4.1773744 -4.2423596 -4.2916651 -4.3083725]]...]
INFO - root - 2017-12-07 15:37:13.017205: step 28010, loss = 21.74, batch loss = 21.65 (8.6 examples/sec; 0.933 sec/batch; 78h:54m:42s remains)
INFO - root - 2017-12-07 15:37:22.389867: step 28020, loss = 21.42, batch loss = 21.34 (8.3 examples/sec; 0.961 sec/batch; 81h:19m:05s remains)
INFO - root - 2017-12-07 15:37:31.545002: step 28030, loss = 21.27, batch loss = 21.18 (8.5 examples/sec; 0.939 sec/batch; 79h:26m:46s remains)
INFO - root - 2017-12-07 15:37:40.906169: step 28040, loss = 21.62, batch loss = 21.54 (9.1 examples/sec; 0.879 sec/batch; 74h:18m:26s remains)
INFO - root - 2017-12-07 15:37:50.300154: step 28050, loss = 21.13, batch loss = 21.04 (9.0 examples/sec; 0.884 sec/batch; 74h:47m:40s remains)
INFO - root - 2017-12-07 15:37:59.631290: step 28060, loss = 21.67, batch loss = 21.59 (8.7 examples/sec; 0.917 sec/batch; 77h:33m:26s remains)
INFO - root - 2017-12-07 15:38:09.044959: step 28070, loss = 21.29, batch loss = 21.21 (8.5 examples/sec; 0.938 sec/batch; 79h:21m:03s remains)
INFO - root - 2017-12-07 15:38:18.492217: step 28080, loss = 21.89, batch loss = 21.80 (8.5 examples/sec; 0.941 sec/batch; 79h:32m:47s remains)
INFO - root - 2017-12-07 15:38:27.934224: step 28090, loss = 21.30, batch loss = 21.21 (8.7 examples/sec; 0.915 sec/batch; 77h:23m:44s remains)
INFO - root - 2017-12-07 15:38:37.356461: step 28100, loss = 21.85, batch loss = 21.77 (8.5 examples/sec; 0.937 sec/batch; 79h:12m:45s remains)
2017-12-07 15:38:38.339793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2127476 -4.2237811 -4.1802816 -4.1193466 -4.0780044 -4.0993166 -4.193347 -4.3061709 -4.37615 -4.3815708 -4.3405404 -4.2664423 -4.1720953 -4.0870156 -4.0494671][-4.1894317 -4.1870871 -4.1345582 -4.0678978 -4.0209928 -4.0508657 -4.1723075 -4.3109469 -4.3957543 -4.4117208 -4.3722682 -4.2806206 -4.1523995 -4.0425677 -4.0095072][-4.1773095 -4.1605086 -4.0975595 -4.0198579 -3.970823 -4.017776 -4.1660552 -4.315731 -4.4027772 -4.4338889 -4.415204 -4.3362827 -4.2077632 -4.0942516 -4.0643344][-4.1785488 -4.145617 -4.0650191 -3.9675007 -3.914722 -3.9758549 -4.1365013 -4.283946 -4.3736105 -4.4307437 -4.4551339 -4.4240632 -4.3310947 -4.2311115 -4.1944189][-4.2070279 -4.1600757 -4.0565863 -3.9334021 -3.867192 -3.9223118 -4.0666313 -4.1918941 -4.2782731 -4.3606033 -4.4360743 -4.4747443 -4.4454956 -4.380157 -4.3397207][-4.2583451 -4.2059112 -4.0881577 -3.9467168 -3.8592238 -3.8790131 -3.9691205 -4.0461659 -4.1159778 -4.2095137 -4.3208656 -4.4187284 -4.456697 -4.4426775 -4.418643][-4.2983503 -4.2533741 -4.1390381 -3.9918718 -3.8790483 -3.8384285 -3.8470795 -3.8663158 -3.9176183 -4.0098519 -4.127068 -4.2435122 -4.3225822 -4.3592396 -4.3698936][-4.3240647 -4.2946963 -4.1933279 -4.049818 -3.9165776 -3.8174055 -3.7528238 -3.7299016 -3.772222 -3.8587894 -3.9561763 -4.0430717 -4.1148162 -4.1739845 -4.2166324][-4.3538685 -4.3420329 -4.257082 -4.1264443 -3.989125 -3.8627987 -3.7665319 -3.7355995 -3.7863588 -3.8721356 -3.9395418 -3.96633 -3.9856498 -4.0260754 -4.0771132][-4.3861213 -4.3903217 -4.3262186 -4.2173724 -4.0924683 -3.96995 -3.8806231 -3.8668242 -3.9331861 -4.02105 -4.0644584 -4.0373917 -4.0001473 -4.0062475 -4.0484018][-4.4011436 -4.4168382 -4.3772 -4.3011703 -4.2044859 -4.1075978 -4.0425844 -4.0476713 -4.1141138 -4.1875572 -4.2087073 -4.1544695 -4.0943871 -4.0833631 -4.1159716][-4.4048128 -4.4232183 -4.4073496 -4.3692775 -4.3117142 -4.2504478 -4.2140536 -4.2305532 -4.2827606 -4.3273873 -4.3264432 -4.2687206 -4.2146482 -4.2030993 -4.226522][-4.4191275 -4.4360332 -4.4369478 -4.4289722 -4.4083233 -4.3829365 -4.372972 -4.3945832 -4.4292216 -4.4469695 -4.4321489 -4.3849382 -4.3455815 -4.3338342 -4.3428926][-4.4410954 -4.4575329 -4.4667873 -4.4746017 -4.4794779 -4.4828677 -4.4933014 -4.5160875 -4.5380049 -4.5415163 -4.5236955 -4.490777 -4.4632397 -4.4472561 -4.4385433][-4.4418592 -4.459331 -4.4716573 -4.4842091 -4.5001154 -4.5170507 -4.5359087 -4.557292 -4.5734396 -4.5746307 -4.5609779 -4.5392542 -4.5198255 -4.5028639 -4.4857526]]...]
INFO - root - 2017-12-07 15:38:47.626060: step 28110, loss = 21.53, batch loss = 21.45 (8.4 examples/sec; 0.950 sec/batch; 80h:21m:37s remains)
INFO - root - 2017-12-07 15:38:56.844101: step 28120, loss = 21.50, batch loss = 21.42 (8.7 examples/sec; 0.923 sec/batch; 78h:03m:17s remains)
INFO - root - 2017-12-07 15:39:06.116133: step 28130, loss = 21.95, batch loss = 21.86 (8.8 examples/sec; 0.906 sec/batch; 76h:37m:44s remains)
INFO - root - 2017-12-07 15:39:15.473613: step 28140, loss = 21.64, batch loss = 21.56 (8.1 examples/sec; 0.986 sec/batch; 83h:20m:53s remains)
INFO - root - 2017-12-07 15:39:24.911111: step 28150, loss = 21.35, batch loss = 21.27 (8.0 examples/sec; 0.998 sec/batch; 84h:19m:54s remains)
INFO - root - 2017-12-07 15:39:34.229062: step 28160, loss = 21.56, batch loss = 21.48 (8.6 examples/sec; 0.934 sec/batch; 78h:57m:44s remains)
INFO - root - 2017-12-07 15:39:43.628807: step 28170, loss = 21.55, batch loss = 21.46 (8.2 examples/sec; 0.976 sec/batch; 82h:32m:29s remains)
INFO - root - 2017-12-07 15:39:53.058587: step 28180, loss = 21.72, batch loss = 21.64 (8.8 examples/sec; 0.908 sec/batch; 76h:44m:58s remains)
INFO - root - 2017-12-07 15:40:02.271786: step 28190, loss = 21.31, batch loss = 21.23 (8.7 examples/sec; 0.924 sec/batch; 78h:04m:06s remains)
INFO - root - 2017-12-07 15:40:11.520405: step 28200, loss = 21.69, batch loss = 21.61 (9.9 examples/sec; 0.809 sec/batch; 68h:20m:54s remains)
2017-12-07 15:40:12.450151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.60045 -4.5811019 -4.5563436 -4.5071297 -4.4717178 -4.488451 -4.5016427 -4.5034938 -4.5221786 -4.5256805 -4.4914327 -4.4525838 -4.4258723 -4.4353471 -4.5169249][-4.49737 -4.46757 -4.4486551 -4.421277 -4.41139 -4.4515166 -4.47034 -4.4646392 -4.4790969 -4.4698591 -4.4176478 -4.3669238 -4.3384581 -4.3634195 -4.470922][-4.36849 -4.3307171 -4.3315229 -4.346241 -4.3710155 -4.4241495 -4.4351292 -4.4167218 -4.4249568 -4.4048157 -4.3413205 -4.280539 -4.2474346 -4.2812529 -4.3970757][-4.2438927 -4.1956511 -4.2227044 -4.281137 -4.3291979 -4.3796287 -4.3773861 -4.3440804 -4.3376732 -4.3041258 -4.2396965 -4.18928 -4.1654181 -4.2055922 -4.3124895][-4.1409316 -4.0744538 -4.1129723 -4.1936483 -4.2445273 -4.2784452 -4.2646289 -4.2266827 -4.2129288 -4.1772041 -4.1229677 -4.0941133 -4.08317 -4.12151 -4.2184515][-4.0618596 -3.9854717 -4.020422 -4.0965071 -4.1294403 -4.1346173 -4.1073136 -4.084691 -4.0952215 -4.0893431 -4.0632596 -4.0568719 -4.0499458 -4.078104 -4.1732697][-4.0219665 -3.949477 -3.9828432 -4.0487618 -4.056807 -4.0179377 -3.9565883 -3.9453447 -3.9970179 -4.0436821 -4.0644751 -4.0853415 -4.0836396 -4.1060452 -4.2039094][-4.0352778 -3.9677901 -3.9969392 -4.0572691 -4.0565352 -3.9862983 -3.8872592 -3.8715403 -3.9444501 -4.0255241 -4.0812006 -4.1155891 -4.111434 -4.1309161 -4.2307639][-4.0909429 -4.0162926 -4.0322871 -4.0898895 -4.1019845 -4.0409861 -3.9445775 -3.9285557 -3.9994466 -4.0834794 -4.1431351 -4.1661482 -4.1425138 -4.1512928 -4.2460771][-4.1689434 -4.0938244 -4.0937657 -4.1381588 -4.1586905 -4.1246185 -4.0644245 -4.0692787 -4.1365366 -4.2072968 -4.2528272 -4.2570395 -4.2164254 -4.2132812 -4.2914438][-4.2393632 -4.186439 -4.183888 -4.2142248 -4.2314267 -4.2136359 -4.1895213 -4.2180047 -4.2808909 -4.3339996 -4.3596964 -4.3485107 -4.3037925 -4.2916727 -4.3429885][-4.2988553 -4.2814021 -4.2937603 -4.3223939 -4.3363848 -4.3232646 -4.3160043 -4.3494487 -4.3982816 -4.429141 -4.4335766 -4.4103513 -4.368906 -4.3517761 -4.3744259][-4.3467131 -4.3562455 -4.3811374 -4.408999 -4.4202309 -4.4108658 -4.406435 -4.4253654 -4.4477391 -4.4550748 -4.446187 -4.422184 -4.3928771 -4.37918 -4.3839254][-4.3739085 -4.3904457 -4.4153543 -4.43598 -4.4430079 -4.4364705 -4.4295526 -4.4302354 -4.4294825 -4.4222326 -4.41028 -4.3932471 -4.3761816 -4.3668537 -4.3638158][-4.3773928 -4.3891983 -4.4057055 -4.4172425 -4.4197626 -4.4137917 -4.4051571 -4.39758 -4.3892493 -4.3802252 -4.3710675 -4.3602509 -4.3498192 -4.3425713 -4.3380709]]...]
INFO - root - 2017-12-07 15:40:21.924483: step 28210, loss = 21.14, batch loss = 21.06 (8.4 examples/sec; 0.951 sec/batch; 80h:24m:33s remains)
INFO - root - 2017-12-07 15:40:31.254749: step 28220, loss = 21.44, batch loss = 21.36 (8.7 examples/sec; 0.918 sec/batch; 77h:36m:19s remains)
INFO - root - 2017-12-07 15:40:40.575186: step 28230, loss = 21.49, batch loss = 21.40 (8.7 examples/sec; 0.916 sec/batch; 77h:26m:12s remains)
INFO - root - 2017-12-07 15:40:49.701604: step 28240, loss = 21.31, batch loss = 21.22 (9.1 examples/sec; 0.883 sec/batch; 74h:38m:46s remains)
INFO - root - 2017-12-07 15:40:59.119204: step 28250, loss = 21.00, batch loss = 20.91 (8.5 examples/sec; 0.942 sec/batch; 79h:35m:09s remains)
INFO - root - 2017-12-07 15:41:08.585584: step 28260, loss = 21.39, batch loss = 21.31 (8.1 examples/sec; 0.989 sec/batch; 83h:36m:36s remains)
INFO - root - 2017-12-07 15:41:17.948040: step 28270, loss = 21.38, batch loss = 21.30 (8.0 examples/sec; 1.003 sec/batch; 84h:47m:05s remains)
INFO - root - 2017-12-07 15:41:27.327869: step 28280, loss = 21.47, batch loss = 21.39 (8.3 examples/sec; 0.966 sec/batch; 81h:37m:51s remains)
INFO - root - 2017-12-07 15:41:36.800220: step 28290, loss = 21.30, batch loss = 21.22 (8.2 examples/sec; 0.978 sec/batch; 82h:38m:37s remains)
INFO - root - 2017-12-07 15:41:46.069985: step 28300, loss = 21.19, batch loss = 21.10 (8.4 examples/sec; 0.954 sec/batch; 80h:38m:54s remains)
2017-12-07 15:41:47.105428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4604077 -4.5003619 -4.5406637 -4.5613256 -4.5541582 -4.5274415 -4.4742188 -4.3929152 -4.34402 -4.355731 -4.4358816 -4.544301 -4.6247392 -4.6345043 -4.5816231][-4.4707003 -4.4829144 -4.5055623 -4.515038 -4.4916987 -4.4396124 -4.3580384 -4.26674 -4.2371445 -4.2981791 -4.4297633 -4.5649529 -4.6408796 -4.6293197 -4.5511446][-4.4897065 -4.4881282 -4.4970222 -4.4858294 -4.4306369 -4.3420081 -4.2328172 -4.138031 -4.1289253 -4.2267303 -4.3925996 -4.5420647 -4.6043463 -4.5760641 -4.49034][-4.5317583 -4.5235596 -4.5146 -4.4721408 -4.3799729 -4.2580214 -4.1246657 -4.0235057 -4.0115447 -4.11003 -4.2797079 -4.43231 -4.4996672 -4.4908242 -4.4352789][-4.5690722 -4.5519862 -4.527463 -4.4660087 -4.3565841 -4.2242904 -4.0858126 -3.9786396 -3.944814 -4.0111055 -4.1548395 -4.3080959 -4.4051023 -4.4479871 -4.4498191][-4.5621929 -4.5406036 -4.5142317 -4.4525976 -4.3368368 -4.1911139 -4.0347385 -3.9069393 -3.8451843 -3.8890743 -4.0310292 -4.208303 -4.353569 -4.4520755 -4.5029283][-4.5139894 -4.4952 -4.4816861 -4.4311357 -4.3048358 -4.1273308 -3.9271419 -3.7683878 -3.6961994 -3.7619791 -3.952863 -4.1763382 -4.3601112 -4.4837546 -4.5528054][-4.4638939 -4.4517846 -4.4527216 -4.4135261 -4.2764907 -4.0678253 -3.836164 -3.6754124 -3.6345973 -3.7585239 -4.001637 -4.2398024 -4.4108033 -4.5093126 -4.5594363][-4.4441934 -4.4431448 -4.458374 -4.4312835 -4.2907314 -4.0679531 -3.832808 -3.7032847 -3.7181835 -3.8933249 -4.1510844 -4.3588271 -4.4723215 -4.5140185 -4.5228043][-4.4403238 -4.4654822 -4.5051813 -4.4963913 -4.3579316 -4.1287303 -3.9009674 -3.8023818 -3.8614941 -4.0641947 -4.3111291 -4.4789562 -4.5360012 -4.5222225 -4.4809346][-4.446116 -4.5081878 -4.5748076 -4.5788336 -4.4424133 -4.2116308 -4.0006514 -3.932878 -4.0283256 -4.2407589 -4.4616241 -4.5879235 -4.5999784 -4.5421395 -4.4562736][-4.4566526 -4.5437078 -4.6198254 -4.62363 -4.4980936 -4.2928314 -4.1227393 -4.0951581 -4.2175293 -4.4170513 -4.5907722 -4.6655321 -4.6357737 -4.5433683 -4.4304333][-4.4680896 -4.5557375 -4.6185045 -4.61553 -4.519443 -4.3724914 -4.2654724 -4.2762594 -4.3993506 -4.5566044 -4.6660967 -4.6867685 -4.6216731 -4.5103436 -4.3953319][-4.4781208 -4.5517015 -4.5943036 -4.5886903 -4.5338016 -4.457325 -4.413044 -4.4439883 -4.5388651 -4.6360922 -4.6823959 -4.6581759 -4.572649 -4.4625492 -4.3676624][-4.4797521 -4.5364933 -4.563961 -4.5585036 -4.5320358 -4.5027657 -4.493423 -4.5190678 -4.5724044 -4.6190457 -4.6277709 -4.5864334 -4.5060754 -4.4196792 -4.358439]]...]
INFO - root - 2017-12-07 15:41:56.389444: step 28310, loss = 21.40, batch loss = 21.32 (9.5 examples/sec; 0.842 sec/batch; 71h:08m:07s remains)
INFO - root - 2017-12-07 15:42:05.890827: step 28320, loss = 21.18, batch loss = 21.09 (8.6 examples/sec; 0.929 sec/batch; 78h:27m:49s remains)
INFO - root - 2017-12-07 15:42:15.368657: step 28330, loss = 21.74, batch loss = 21.66 (8.2 examples/sec; 0.973 sec/batch; 82h:13m:13s remains)
INFO - root - 2017-12-07 15:42:24.706343: step 28340, loss = 21.20, batch loss = 21.12 (8.3 examples/sec; 0.967 sec/batch; 81h:41m:16s remains)
INFO - root - 2017-12-07 15:42:34.116580: step 28350, loss = 21.47, batch loss = 21.39 (8.5 examples/sec; 0.937 sec/batch; 79h:10m:49s remains)
INFO - root - 2017-12-07 15:42:43.542400: step 28360, loss = 21.41, batch loss = 21.32 (9.0 examples/sec; 0.892 sec/batch; 75h:21m:49s remains)
INFO - root - 2017-12-07 15:42:52.932685: step 28370, loss = 21.62, batch loss = 21.54 (8.8 examples/sec; 0.913 sec/batch; 77h:07m:11s remains)
INFO - root - 2017-12-07 15:43:02.291552: step 28380, loss = 21.29, batch loss = 21.20 (8.7 examples/sec; 0.917 sec/batch; 77h:30m:23s remains)
INFO - root - 2017-12-07 15:43:11.700709: step 28390, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.950 sec/batch; 80h:13m:07s remains)
INFO - root - 2017-12-07 15:43:21.100857: step 28400, loss = 21.27, batch loss = 21.19 (8.3 examples/sec; 0.969 sec/batch; 81h:53m:07s remains)
2017-12-07 15:43:22.131309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4072061 -4.4143949 -4.4162292 -4.415308 -4.4194303 -4.4305563 -4.4417825 -4.43759 -4.4116058 -4.3667383 -4.3189859 -4.2802677 -4.26248 -4.262774 -4.28262][-4.4522109 -4.4721274 -4.4764094 -4.4709148 -4.4707441 -4.4830413 -4.4993968 -4.505712 -4.4902258 -4.4519792 -4.4055457 -4.3646331 -4.335012 -4.3133063 -4.3025184][-4.4753542 -4.5064454 -4.5101047 -4.4893851 -4.4666772 -4.4620652 -4.475728 -4.5042553 -4.5245194 -4.520647 -4.4993973 -4.4722414 -4.4424596 -4.4035554 -4.3624125][-4.4762325 -4.5088525 -4.50383 -4.4574838 -4.3928609 -4.3424044 -4.328618 -4.3747544 -4.4490275 -4.5100913 -4.5420389 -4.5471878 -4.5309348 -4.4875178 -4.4251509][-4.4712286 -4.4958129 -4.4702158 -4.3898611 -4.2740731 -4.1605005 -4.095583 -4.1393127 -4.2639556 -4.4035015 -4.5069771 -4.5556993 -4.5602584 -4.5253305 -4.4604654][-4.4678941 -4.4765992 -4.4188628 -4.2970552 -4.1300411 -3.9596198 -3.8490269 -3.8810661 -4.0411711 -4.2469287 -4.4149289 -4.4998469 -4.5158806 -4.4909286 -4.4423523][-4.4677682 -4.4633832 -4.3766003 -4.215116 -4.0016584 -3.7878289 -3.6473551 -3.668817 -3.8524833 -4.1091943 -4.3310938 -4.4451537 -4.4604421 -4.4323988 -4.3990889][-4.4704494 -4.4731035 -4.3875856 -4.2177124 -3.9837956 -3.7466478 -3.5866518 -3.5909626 -3.7765303 -4.0531015 -4.3018022 -4.4323187 -4.4430828 -4.4040208 -4.3746934][-4.4821196 -4.5132957 -4.4642272 -4.3254137 -4.1092238 -3.8733189 -3.6989121 -3.6776745 -3.8373537 -4.09192 -4.3265367 -4.4500504 -4.4532104 -4.4042234 -4.3717709][-4.4919314 -4.5553184 -4.5514522 -4.460556 -4.29054 -4.0832243 -3.9071734 -3.8576345 -3.9707754 -4.1774578 -4.3759565 -4.4822106 -4.4831271 -4.4328761 -4.3947425][-4.487576 -4.5711932 -4.6046591 -4.5649481 -4.4603615 -4.3143768 -4.1702051 -4.1057529 -4.1587725 -4.2908273 -4.4272418 -4.5051966 -4.5094004 -4.4694481 -4.4287629][-4.4690318 -4.5574083 -4.6155319 -4.6205225 -4.5813603 -4.5070667 -4.4156489 -4.3523707 -4.3513322 -4.4053636 -4.4717264 -4.514966 -4.517467 -4.4846916 -4.4408503][-4.43541 -4.5104933 -4.5747409 -4.6074405 -4.6127329 -4.5890613 -4.537991 -4.4770217 -4.4380536 -4.4358377 -4.453342 -4.4722223 -4.4684815 -4.4371009 -4.3946805][-4.3882403 -4.4331675 -4.4842811 -4.5268507 -4.5579524 -4.5617709 -4.5293856 -4.4623318 -4.3938832 -4.3554873 -4.3502603 -4.3621554 -4.3569393 -4.3298264 -4.2998934][-4.33323 -4.3372078 -4.3560014 -4.3874092 -4.4260354 -4.4432778 -4.4194283 -4.3488369 -4.2691817 -4.2254453 -4.2331457 -4.2624683 -4.2638946 -4.2390575 -4.2178392]]...]
INFO - root - 2017-12-07 15:43:31.502767: step 28410, loss = 21.43, batch loss = 21.35 (8.1 examples/sec; 0.989 sec/batch; 83h:32m:11s remains)
INFO - root - 2017-12-07 15:43:40.949460: step 28420, loss = 21.68, batch loss = 21.59 (8.2 examples/sec; 0.976 sec/batch; 82h:26m:03s remains)
INFO - root - 2017-12-07 15:43:50.345898: step 28430, loss = 21.71, batch loss = 21.63 (9.6 examples/sec; 0.836 sec/batch; 70h:38m:36s remains)
INFO - root - 2017-12-07 15:43:59.687791: step 28440, loss = 21.67, batch loss = 21.59 (9.2 examples/sec; 0.869 sec/batch; 73h:26m:04s remains)
INFO - root - 2017-12-07 15:44:08.995964: step 28450, loss = 21.51, batch loss = 21.43 (9.7 examples/sec; 0.826 sec/batch; 69h:46m:57s remains)
INFO - root - 2017-12-07 15:44:18.443277: step 28460, loss = 21.49, batch loss = 21.41 (8.4 examples/sec; 0.951 sec/batch; 80h:17m:39s remains)
INFO - root - 2017-12-07 15:44:27.683510: step 28470, loss = 21.60, batch loss = 21.52 (8.5 examples/sec; 0.938 sec/batch; 79h:12m:51s remains)
INFO - root - 2017-12-07 15:44:37.011015: step 28480, loss = 21.25, batch loss = 21.17 (8.4 examples/sec; 0.947 sec/batch; 79h:57m:39s remains)
INFO - root - 2017-12-07 15:44:46.345351: step 28490, loss = 21.32, batch loss = 21.24 (7.9 examples/sec; 1.010 sec/batch; 85h:16m:15s remains)
INFO - root - 2017-12-07 15:44:55.834018: step 28500, loss = 21.68, batch loss = 21.59 (7.7 examples/sec; 1.037 sec/batch; 87h:36m:29s remains)
2017-12-07 15:44:56.712790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.517982 -4.49929 -4.4581103 -4.4507179 -4.502213 -4.5782394 -4.6247258 -4.623518 -4.5853291 -4.5437 -4.5398688 -4.5982361 -4.6586022 -4.6485338 -4.5798922][-4.6223736 -4.6087003 -4.5516748 -4.5231466 -4.5587945 -4.6196637 -4.6472478 -4.63114 -4.5967841 -4.5793428 -4.6077394 -4.6906786 -4.7615037 -4.7433772 -4.6614642][-4.6646318 -4.6564903 -4.5934381 -4.5529428 -4.5648384 -4.583611 -4.5610137 -4.5163388 -4.4960494 -4.5269847 -4.6078825 -4.7138929 -4.7854137 -4.7673831 -4.6943569][-4.6590261 -4.6508775 -4.5847378 -4.5386624 -4.5231085 -4.4806681 -4.3726363 -4.2693558 -4.2609091 -4.3598089 -4.5126314 -4.648612 -4.7262945 -4.7220955 -4.6722226][-4.6180735 -4.602551 -4.5297613 -4.476254 -4.4351773 -4.3276167 -4.116353 -3.9314866 -3.931284 -4.1079764 -4.3416786 -4.5255394 -4.6307769 -4.6572695 -4.6273484][-4.5519905 -4.5340118 -4.45205 -4.383625 -4.314466 -4.1486096 -3.8387318 -3.5623491 -3.5646584 -3.8165329 -4.1334081 -4.3813567 -4.5403247 -4.6127915 -4.5997915][-4.5062642 -4.4996533 -4.4158297 -4.3278618 -4.2245955 -4.0098262 -3.6313369 -3.2954071 -3.3090079 -3.6229196 -4.0006957 -4.2983189 -4.5024204 -4.6131282 -4.6121268][-4.472631 -4.4917021 -4.4327469 -4.3498135 -4.2358866 -4.0078011 -3.6271608 -3.2950573 -3.3181787 -3.6392398 -4.0176234 -4.3166208 -4.5347 -4.6670322 -4.6767182][-4.4598742 -4.5113363 -4.5050745 -4.4601817 -4.3601489 -4.1485553 -3.8196151 -3.541275 -3.55826 -3.824784 -4.1408668 -4.3987374 -4.6043367 -4.7429342 -4.7568388][-4.4908481 -4.5705667 -4.6188564 -4.615356 -4.5333939 -4.3468857 -4.097012 -3.9084787 -3.9350069 -4.1278172 -4.344214 -4.5241513 -4.6867881 -4.8049288 -4.8047924][-4.5219431 -4.6076336 -4.6858878 -4.7207446 -4.6757941 -4.5446229 -4.3911633 -4.2963238 -4.3278708 -4.4409571 -4.549233 -4.6394839 -4.739789 -4.8157949 -4.7884274][-4.5089607 -4.5759578 -4.6547756 -4.7193265 -4.7272458 -4.6717196 -4.6028986 -4.5642776 -4.5757632 -4.6125588 -4.6373353 -4.661284 -4.7063723 -4.7379022 -4.6865578][-4.4549675 -4.4973011 -4.5565777 -4.6220613 -4.6611071 -4.66105 -4.6461921 -4.63215 -4.6215072 -4.6098914 -4.593173 -4.5856361 -4.5940948 -4.5903907 -4.5301757][-4.3743992 -4.3992376 -4.4388046 -4.4886761 -4.53027 -4.5506611 -4.5552225 -4.5478735 -4.5271168 -4.497683 -4.4692154 -4.4518232 -4.4432158 -4.4266205 -4.3826294][-4.2988372 -4.3107929 -4.3332615 -4.3612661 -4.3863292 -4.4010906 -4.4041529 -4.3974285 -4.3799858 -4.3551569 -4.3316512 -4.3171468 -4.3100772 -4.3029752 -4.2887859]]...]
INFO - root - 2017-12-07 15:45:05.995369: step 28510, loss = 21.21, batch loss = 21.12 (8.7 examples/sec; 0.915 sec/batch; 77h:14m:42s remains)
INFO - root - 2017-12-07 15:45:15.468567: step 28520, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.932 sec/batch; 78h:40m:10s remains)
INFO - root - 2017-12-07 15:45:25.093000: step 28530, loss = 21.19, batch loss = 21.11 (8.1 examples/sec; 0.994 sec/batch; 83h:53m:28s remains)
INFO - root - 2017-12-07 15:45:34.455589: step 28540, loss = 21.76, batch loss = 21.68 (8.4 examples/sec; 0.952 sec/batch; 80h:24m:21s remains)
INFO - root - 2017-12-07 15:45:43.735224: step 28550, loss = 21.33, batch loss = 21.24 (9.0 examples/sec; 0.894 sec/batch; 75h:27m:41s remains)
INFO - root - 2017-12-07 15:45:53.109235: step 28560, loss = 21.61, batch loss = 21.53 (8.3 examples/sec; 0.964 sec/batch; 81h:23m:01s remains)
INFO - root - 2017-12-07 15:46:02.442238: step 28570, loss = 21.42, batch loss = 21.34 (7.9 examples/sec; 1.013 sec/batch; 85h:31m:49s remains)
INFO - root - 2017-12-07 15:46:11.868153: step 28580, loss = 21.24, batch loss = 21.16 (8.2 examples/sec; 0.972 sec/batch; 82h:01m:08s remains)
INFO - root - 2017-12-07 15:46:21.320244: step 28590, loss = 21.92, batch loss = 21.83 (8.2 examples/sec; 0.979 sec/batch; 82h:38m:51s remains)
INFO - root - 2017-12-07 15:46:30.845083: step 28600, loss = 21.61, batch loss = 21.52 (8.7 examples/sec; 0.917 sec/batch; 77h:25m:32s remains)
2017-12-07 15:46:31.783663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6371422 -4.7012997 -4.6471653 -4.5122528 -4.32278 -4.1458569 -4.0671573 -4.0876989 -4.1667724 -4.2932005 -4.4039216 -4.4597263 -4.4955816 -4.507184 -4.5217981][-4.6774416 -4.7551141 -4.6934986 -4.5272326 -4.301095 -4.0933495 -4.0041556 -4.0328555 -4.1214128 -4.2563319 -4.368382 -4.4296627 -4.4714236 -4.4852343 -4.4944811][-4.6987863 -4.7801094 -4.725759 -4.545002 -4.2824044 -4.0314369 -3.9118366 -3.9320228 -4.0318003 -4.173852 -4.2979422 -4.3861437 -4.4506955 -4.4780612 -4.4858036][-4.6890163 -4.7665424 -4.725666 -4.5423245 -4.2500873 -3.9597259 -3.8092513 -3.823513 -3.9432669 -4.1007271 -4.2444315 -4.3589725 -4.4377527 -4.4770184 -4.4837718][-4.6680675 -4.7372718 -4.7102456 -4.5331149 -4.2263522 -3.9121532 -3.739202 -3.7500863 -3.8879256 -4.060524 -4.2234459 -4.3518791 -4.4243979 -4.4591174 -4.4598227][-4.6734629 -4.7366338 -4.7223954 -4.556787 -4.2520666 -3.9313121 -3.740402 -3.7409008 -3.880116 -4.0530791 -4.2216573 -4.3490705 -4.4071517 -4.4297929 -4.4210219][-4.7024851 -4.7717896 -4.7751079 -4.6242709 -4.3289 -4.0044513 -3.7928464 -3.7751555 -3.8978744 -4.0542812 -4.2141471 -4.3377051 -4.3907437 -4.4048781 -4.3859324][-4.7157288 -4.8037276 -4.8336143 -4.7016811 -4.4183307 -4.0932693 -3.8701761 -3.8406861 -3.9424255 -4.0737896 -4.2149138 -4.3360558 -4.3925281 -4.4005537 -4.3733387][-4.6846671 -4.79346 -4.8563614 -4.7520142 -4.4950595 -4.1903768 -3.9765372 -3.9401987 -4.0183506 -4.12013 -4.2360439 -4.3530459 -4.411993 -4.4124894 -4.3841538][-4.6248651 -4.7349887 -4.8289895 -4.7660222 -4.5614758 -4.3061986 -4.1199975 -4.073585 -4.114862 -4.17437 -4.2562261 -4.3667483 -4.4299111 -4.4295053 -4.4076915][-4.5820932 -4.6728415 -4.7799368 -4.760366 -4.6238141 -4.4310088 -4.2723894 -4.2057753 -4.2028279 -4.2137351 -4.2574897 -4.3564038 -4.4238305 -4.4326439 -4.4278045][-4.5668669 -4.6422992 -4.7464848 -4.7584691 -4.6882038 -4.5602121 -4.4286523 -4.3395538 -4.2922387 -4.2572446 -4.2668333 -4.3509555 -4.4204235 -4.4465046 -4.4651346][-4.5233555 -4.6120987 -4.7173781 -4.7518387 -4.7355032 -4.668191 -4.5683951 -4.4698529 -4.3910861 -4.3270321 -4.3184013 -4.3870234 -4.4520416 -4.4916534 -4.5280566][-4.43403 -4.5585203 -4.6730547 -4.7236676 -4.7403879 -4.7151923 -4.6472139 -4.5605378 -4.478642 -4.4110217 -4.3993669 -4.4520197 -4.507741 -4.5545321 -4.5998988][-4.3547807 -4.5078244 -4.6239862 -4.6800494 -4.7082472 -4.7012129 -4.654696 -4.5912027 -4.5285153 -4.47876 -4.473979 -4.5114093 -4.5555043 -4.6015511 -4.6473207]]...]
INFO - root - 2017-12-07 15:46:41.216409: step 28610, loss = 21.55, batch loss = 21.47 (8.6 examples/sec; 0.926 sec/batch; 78h:08m:01s remains)
INFO - root - 2017-12-07 15:46:50.620646: step 28620, loss = 21.49, batch loss = 21.41 (8.9 examples/sec; 0.903 sec/batch; 76h:12m:07s remains)
INFO - root - 2017-12-07 15:47:00.016381: step 28630, loss = 21.58, batch loss = 21.50 (8.5 examples/sec; 0.946 sec/batch; 79h:49m:08s remains)
INFO - root - 2017-12-07 15:47:09.400387: step 28640, loss = 21.13, batch loss = 21.05 (8.8 examples/sec; 0.905 sec/batch; 76h:23m:53s remains)
INFO - root - 2017-12-07 15:47:18.722371: step 28650, loss = 21.56, batch loss = 21.48 (8.5 examples/sec; 0.936 sec/batch; 78h:58m:45s remains)
INFO - root - 2017-12-07 15:47:28.241233: step 28660, loss = 21.71, batch loss = 21.63 (8.4 examples/sec; 0.950 sec/batch; 80h:10m:47s remains)
INFO - root - 2017-12-07 15:47:37.761665: step 28670, loss = 21.28, batch loss = 21.20 (8.5 examples/sec; 0.936 sec/batch; 79h:00m:53s remains)
INFO - root - 2017-12-07 15:47:47.208594: step 28680, loss = 20.80, batch loss = 20.72 (8.2 examples/sec; 0.973 sec/batch; 82h:08m:58s remains)
INFO - root - 2017-12-07 15:47:56.684743: step 28690, loss = 21.50, batch loss = 21.42 (8.6 examples/sec; 0.926 sec/batch; 78h:10m:51s remains)
INFO - root - 2017-12-07 15:48:06.050010: step 28700, loss = 20.99, batch loss = 20.91 (8.5 examples/sec; 0.944 sec/batch; 79h:37m:29s remains)
2017-12-07 15:48:06.954526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.496007 -4.5623174 -4.5948505 -4.6103611 -4.6203856 -4.6247611 -4.6157341 -4.5859146 -4.5640974 -4.5621843 -4.5552816 -4.53318 -4.4978633 -4.4569373 -4.4141474][-4.5492911 -4.6059556 -4.6272993 -4.6451931 -4.6658621 -4.6663747 -4.6269655 -4.5620723 -4.5370231 -4.570931 -4.6068263 -4.5997572 -4.5491266 -4.4879446 -4.4306278][-4.5631213 -4.6105452 -4.6211839 -4.6381826 -4.659246 -4.6338716 -4.5427957 -4.4435277 -4.4287915 -4.5069761 -4.5931416 -4.6012735 -4.5276904 -4.44438 -4.3849106][-4.5429978 -4.5870857 -4.5836329 -4.5779481 -4.5706925 -4.4893255 -4.3329439 -4.224906 -4.2577276 -4.3935995 -4.5280595 -4.5454249 -4.4413285 -4.3346496 -4.2850122][-4.5074925 -4.5528731 -4.5183873 -4.4576941 -4.3972054 -4.2477722 -4.0311885 -3.9417307 -4.0542569 -4.2551274 -4.4305954 -4.4598274 -4.3372545 -4.2141743 -4.177505][-4.4743576 -4.5196896 -4.4403772 -4.3122463 -4.200726 -4.0014448 -3.7429261 -3.6692641 -3.8433948 -4.0879469 -4.2918181 -4.3516426 -4.2528944 -4.1426067 -4.1211786][-4.4466968 -4.4889383 -4.3724451 -4.1936283 -4.0503931 -3.829102 -3.5515838 -3.4750485 -3.663208 -3.9224932 -4.1534662 -4.2681785 -4.2324724 -4.1598969 -4.1486697][-4.428606 -4.4626441 -4.3330503 -4.1426163 -3.9962685 -3.7879744 -3.5277722 -3.442632 -3.6047261 -3.853256 -4.1060076 -4.2758842 -4.3039927 -4.2601113 -4.2371569][-4.418911 -4.4383397 -4.3203158 -4.1562943 -4.036901 -3.8755913 -3.6784329 -3.6076465 -3.7316475 -3.9520926 -4.2010736 -4.3918886 -4.4470267 -4.3991342 -4.3402843][-4.4144707 -4.4195194 -4.3244143 -4.2051773 -4.1287613 -4.028307 -3.915643 -3.8898442 -3.991087 -4.1720948 -4.3786178 -4.536799 -4.5710826 -4.4982028 -4.4042997][-4.4072008 -4.4112587 -4.3464909 -4.2738023 -4.2411113 -4.2008796 -4.1708069 -4.1975169 -4.2828274 -4.4021096 -4.5254064 -4.607091 -4.5935068 -4.5024819 -4.40276][-4.3852596 -4.4025712 -4.3765287 -4.3487296 -4.3563504 -4.3686171 -4.3988943 -4.45647 -4.5112963 -4.5480986 -4.5696406 -4.5661902 -4.5137091 -4.4311676 -4.3578978][-4.3460603 -4.3790779 -4.3927917 -4.4047055 -4.4398355 -4.480454 -4.5312018 -4.584939 -4.597929 -4.5645909 -4.5131636 -4.4597988 -4.3989882 -4.3430414 -4.3057408][-4.3009205 -4.3374696 -4.3726335 -4.4099188 -4.4573107 -4.5027809 -4.5443764 -4.5709834 -4.549439 -4.484283 -4.4093165 -4.3493471 -4.30371 -4.2766604 -4.2670417][-4.2702708 -4.2938519 -4.3265362 -4.3645768 -4.4053974 -4.4392438 -4.4611034 -4.4647532 -4.4345608 -4.3786521 -4.3212113 -4.2803059 -4.2558331 -4.2478538 -4.2512903]]...]
INFO - root - 2017-12-07 15:48:16.410023: step 28710, loss = 21.30, batch loss = 21.21 (8.2 examples/sec; 0.980 sec/batch; 82h:42m:29s remains)
INFO - root - 2017-12-07 15:48:25.920063: step 28720, loss = 21.50, batch loss = 21.41 (8.3 examples/sec; 0.967 sec/batch; 81h:36m:12s remains)
INFO - root - 2017-12-07 15:48:35.374392: step 28730, loss = 21.41, batch loss = 21.32 (8.2 examples/sec; 0.974 sec/batch; 82h:12m:45s remains)
INFO - root - 2017-12-07 15:48:44.758969: step 28740, loss = 21.24, batch loss = 21.15 (8.7 examples/sec; 0.919 sec/batch; 77h:31m:48s remains)
INFO - root - 2017-12-07 15:48:53.962465: step 28750, loss = 21.55, batch loss = 21.46 (8.8 examples/sec; 0.911 sec/batch; 76h:54m:02s remains)
INFO - root - 2017-12-07 15:49:03.379400: step 28760, loss = 21.73, batch loss = 21.65 (8.3 examples/sec; 0.960 sec/batch; 80h:57m:46s remains)
INFO - root - 2017-12-07 15:49:12.720729: step 28770, loss = 21.31, batch loss = 21.22 (8.7 examples/sec; 0.917 sec/batch; 77h:23m:55s remains)
INFO - root - 2017-12-07 15:49:22.079654: step 28780, loss = 21.31, batch loss = 21.23 (8.6 examples/sec; 0.930 sec/batch; 78h:27m:29s remains)
INFO - root - 2017-12-07 15:49:31.369256: step 28790, loss = 21.82, batch loss = 21.73 (9.5 examples/sec; 0.843 sec/batch; 71h:07m:11s remains)
INFO - root - 2017-12-07 15:49:40.860567: step 28800, loss = 21.20, batch loss = 21.11 (9.2 examples/sec; 0.866 sec/batch; 73h:03m:02s remains)
2017-12-07 15:49:41.855600: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3787284 -4.3298531 -4.2612796 -4.1932244 -4.14198 -4.1435051 -4.1955786 -4.2680416 -4.3597431 -4.4515524 -4.5226989 -4.5754628 -4.5777631 -4.5246878 -4.4450088][-4.3421984 -4.2835569 -4.2252135 -4.1851196 -4.1656418 -4.1857352 -4.2352085 -4.2960467 -4.3798761 -4.4640665 -4.5366721 -4.5906625 -4.5869493 -4.5297079 -4.4485126][-4.2657919 -4.1998258 -4.1579838 -4.1517291 -4.165236 -4.1932807 -4.2098641 -4.231811 -4.3041844 -4.3967881 -4.4909892 -4.5674548 -4.5769334 -4.5261989 -4.4440618][-4.2126184 -4.1481185 -4.1208067 -4.1359782 -4.161931 -4.1711583 -4.1306982 -4.1091213 -4.1882334 -4.3163137 -4.4539404 -4.56696 -4.5952291 -4.5461454 -4.4535909][-4.2161317 -4.1641932 -4.1489968 -4.1700277 -4.1851907 -4.1488204 -4.0366187 -3.9656162 -4.0456266 -4.206265 -4.3856821 -4.5402336 -4.6000471 -4.5627608 -4.4672379][-4.2554893 -4.2218633 -4.2131581 -4.2223716 -4.2066679 -4.1161246 -3.944128 -3.8331585 -3.9023042 -4.0744357 -4.2778397 -4.4652243 -4.5600629 -4.5472441 -4.4652629][-4.3041816 -4.2780504 -4.2662253 -4.2592659 -4.219965 -4.1086531 -3.9242203 -3.7974648 -3.8414085 -3.9919863 -4.19323 -4.3930926 -4.507122 -4.5162268 -4.4534745][-4.37812 -4.3545275 -4.3434687 -4.333045 -4.2930164 -4.1863217 -4.0087533 -3.8744452 -3.8814061 -3.9902425 -4.1666451 -4.352529 -4.4630523 -4.4824266 -4.4347825][-4.4840894 -4.4676337 -4.4648447 -4.4630647 -4.4332852 -4.3313041 -4.1528397 -4.0099182 -3.9848533 -4.0564971 -4.2051916 -4.3645177 -4.4596257 -4.4785647 -4.4319458][-4.5876074 -4.57914 -4.5839581 -4.5890532 -4.5657482 -4.4601221 -4.2745767 -4.1264224 -4.0792885 -4.124825 -4.2551255 -4.3956161 -4.48257 -4.5064368 -4.4561934][-4.6462712 -4.64172 -4.6490474 -4.6558089 -4.6361775 -4.5290413 -4.3428988 -4.1988435 -4.1470122 -4.1852555 -4.3077941 -4.4355469 -4.5193076 -4.5451493 -4.4877024][-4.632267 -4.6150618 -4.6197734 -4.6312208 -4.6215739 -4.5218678 -4.3481059 -4.224999 -4.1939163 -4.2482162 -4.3721995 -4.4915466 -4.5719371 -4.5920596 -4.5230775][-4.5292459 -4.4906168 -4.4949861 -4.5203581 -4.5301752 -4.4486866 -4.2997723 -4.205267 -4.2028542 -4.2811761 -4.4129457 -4.5321546 -4.6146092 -4.6293974 -4.5500083][-4.3754711 -4.3241506 -4.3364191 -4.3832355 -4.414083 -4.3525057 -4.2304893 -4.1617889 -4.1843686 -4.2836523 -4.4232235 -4.5431252 -4.6278524 -4.6434336 -4.5631771][-4.2322774 -4.179121 -4.2147646 -4.2919059 -4.3426766 -4.2955093 -4.1912823 -4.1400847 -4.1771131 -4.2833972 -4.4155259 -4.5227747 -4.604012 -4.6279836 -4.5630841]]...]
INFO - root - 2017-12-07 15:49:51.193301: step 28810, loss = 21.49, batch loss = 21.41 (9.9 examples/sec; 0.809 sec/batch; 68h:13m:36s remains)
INFO - root - 2017-12-07 15:50:00.652479: step 28820, loss = 21.30, batch loss = 21.22 (9.1 examples/sec; 0.874 sec/batch; 73h:45m:24s remains)
INFO - root - 2017-12-07 15:50:10.097044: step 28830, loss = 21.31, batch loss = 21.23 (8.8 examples/sec; 0.912 sec/batch; 76h:56m:13s remains)
INFO - root - 2017-12-07 15:50:19.527540: step 28840, loss = 21.00, batch loss = 20.92 (8.1 examples/sec; 0.982 sec/batch; 82h:48m:52s remains)
INFO - root - 2017-12-07 15:50:29.181119: step 28850, loss = 21.51, batch loss = 21.43 (8.6 examples/sec; 0.934 sec/batch; 78h:44m:56s remains)
INFO - root - 2017-12-07 15:50:38.480467: step 28860, loss = 21.99, batch loss = 21.91 (8.2 examples/sec; 0.972 sec/batch; 82h:00m:01s remains)
INFO - root - 2017-12-07 15:50:47.887087: step 28870, loss = 21.62, batch loss = 21.54 (8.2 examples/sec; 0.971 sec/batch; 81h:53m:48s remains)
INFO - root - 2017-12-07 15:50:57.406597: step 28880, loss = 21.48, batch loss = 21.39 (9.0 examples/sec; 0.893 sec/batch; 75h:20m:41s remains)
INFO - root - 2017-12-07 15:51:06.929292: step 28890, loss = 21.21, batch loss = 21.13 (8.4 examples/sec; 0.955 sec/batch; 80h:32m:58s remains)
INFO - root - 2017-12-07 15:51:16.317055: step 28900, loss = 21.51, batch loss = 21.43 (8.3 examples/sec; 0.959 sec/batch; 80h:53m:22s remains)
2017-12-07 15:51:17.277914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2358937 -4.2755814 -4.3340573 -4.3653455 -4.3818407 -4.3996682 -4.4069076 -4.4274344 -4.4623408 -4.486577 -4.4978552 -4.522007 -4.5438547 -4.5233722 -4.4480147][-4.1028 -4.172266 -4.2660332 -4.3277645 -4.3693509 -4.4015903 -4.4143376 -4.4376564 -4.4840655 -4.5131035 -4.5248985 -4.5514555 -4.5720081 -4.5465751 -4.4614911][-4.0691957 -4.1580858 -4.2643766 -4.3295345 -4.3642297 -4.3767552 -4.3669472 -4.3778124 -4.430233 -4.4691925 -4.4973912 -4.5440812 -4.5779595 -4.5576625 -4.4720883][-4.1353693 -4.2324672 -4.3230886 -4.3559589 -4.3489885 -4.3192973 -4.2760172 -4.26873 -4.3211942 -4.3733754 -4.4313946 -4.5103965 -4.5671778 -4.5577803 -4.4755473][-4.235949 -4.320632 -4.3752341 -4.3640337 -4.3088121 -4.2411175 -4.1740875 -4.1495643 -4.194139 -4.2606683 -4.3549433 -4.47255 -4.5525289 -4.5501332 -4.4709439][-4.2910585 -4.3425198 -4.3605347 -4.3258176 -4.24924 -4.1662626 -4.0838413 -4.033308 -4.0537934 -4.127697 -4.2579193 -4.4154763 -4.5188909 -4.524457 -4.4532628][-4.2646518 -4.2761073 -4.2787981 -4.2558942 -4.1927614 -4.1101065 -4.0079 -3.9165328 -3.909312 -3.9982994 -4.167882 -4.3622751 -4.4835429 -4.4944019 -4.4317942][-4.1726389 -4.1584368 -4.1686139 -4.1772766 -4.141819 -4.0636158 -3.9521008 -3.8456578 -3.8446379 -3.9629369 -4.1541142 -4.3552785 -4.4716883 -4.47536 -4.4137688][-4.1098447 -4.0919166 -4.1094475 -4.143136 -4.1283937 -4.0618029 -3.9722426 -3.8965411 -3.9371529 -4.079258 -4.2510991 -4.4151444 -4.4947371 -4.4725647 -4.4022098][-4.13906 -4.1295328 -4.14514 -4.1853218 -4.178021 -4.1221995 -4.0652289 -4.0344496 -4.1184459 -4.2676191 -4.4002633 -4.5104485 -4.5401311 -4.4847832 -4.4018931][-4.2487006 -4.25115 -4.2591391 -4.288681 -4.2730203 -4.2100773 -4.1620178 -4.1501865 -4.2477837 -4.3911395 -4.4994245 -4.5800319 -4.5800982 -4.5025582 -4.4094682][-4.4274912 -4.4327111 -4.4289885 -4.4419446 -4.4151134 -4.3412676 -4.2841158 -4.2692308 -4.3532062 -4.4761987 -4.5661421 -4.6270456 -4.607903 -4.518013 -4.4179463][-4.5633411 -4.5719795 -4.5661411 -4.5705156 -4.5480442 -4.4837437 -4.4274225 -4.4071283 -4.4600387 -4.5425472 -4.6010551 -4.6340456 -4.5983448 -4.5061941 -4.4100327][-4.5887194 -4.5985231 -4.5983539 -4.6020432 -4.5921926 -4.5534253 -4.51324 -4.493876 -4.5135403 -4.5486708 -4.5699487 -4.573123 -4.5296512 -4.45184 -4.376853][-4.5132604 -4.5182395 -4.5231676 -4.5326381 -4.5373244 -4.5239639 -4.5035992 -4.4883885 -4.4838538 -4.4831314 -4.4775815 -4.4637532 -4.4269385 -4.3771777 -4.3343482]]...]
INFO - root - 2017-12-07 15:51:26.745077: step 28910, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.955 sec/batch; 80h:34m:33s remains)
INFO - root - 2017-12-07 15:51:36.155026: step 28920, loss = 21.73, batch loss = 21.64 (8.3 examples/sec; 0.959 sec/batch; 80h:54m:29s remains)
INFO - root - 2017-12-07 15:51:45.588238: step 28930, loss = 21.60, batch loss = 21.52 (8.4 examples/sec; 0.956 sec/batch; 80h:36m:08s remains)
INFO - root - 2017-12-07 15:51:54.861552: step 28940, loss = 21.25, batch loss = 21.16 (7.7 examples/sec; 1.035 sec/batch; 87h:18m:02s remains)
INFO - root - 2017-12-07 15:52:04.342552: step 28950, loss = 20.64, batch loss = 20.56 (8.4 examples/sec; 0.950 sec/batch; 80h:04m:33s remains)
INFO - root - 2017-12-07 15:52:13.549468: step 28960, loss = 21.51, batch loss = 21.42 (8.7 examples/sec; 0.921 sec/batch; 77h:41m:01s remains)
INFO - root - 2017-12-07 15:52:22.836221: step 28970, loss = 21.37, batch loss = 21.29 (8.2 examples/sec; 0.970 sec/batch; 81h:46m:42s remains)
INFO - root - 2017-12-07 15:52:32.375106: step 28980, loss = 21.39, batch loss = 21.30 (7.9 examples/sec; 1.018 sec/batch; 85h:50m:03s remains)
INFO - root - 2017-12-07 15:52:41.690305: step 28990, loss = 21.06, batch loss = 20.98 (8.2 examples/sec; 0.973 sec/batch; 82h:00m:06s remains)
INFO - root - 2017-12-07 15:52:51.112245: step 29000, loss = 21.28, batch loss = 21.20 (8.9 examples/sec; 0.899 sec/batch; 75h:47m:35s remains)
2017-12-07 15:52:52.145325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5315752 -4.4742227 -4.4146242 -4.3687439 -4.3664374 -4.4062729 -4.4393487 -4.4349117 -4.3904438 -4.3376331 -4.326005 -4.3663378 -4.427186 -4.4615531 -4.4644365][-4.4774847 -4.4329996 -4.378242 -4.3367815 -4.3491492 -4.4209476 -4.4793425 -4.4813342 -4.4403019 -4.3998194 -4.4003263 -4.443975 -4.4993057 -4.5132222 -4.4795656][-4.4032383 -4.3856115 -4.3585958 -4.3374629 -4.3552957 -4.4363594 -4.5089259 -4.5187798 -4.4821277 -4.4561844 -4.4754424 -4.5300517 -4.580821 -4.5740018 -4.5061631][-4.3515196 -4.3675756 -4.3750882 -4.3754435 -4.3778129 -4.4210787 -4.4766469 -4.4885221 -4.4495583 -4.4272418 -4.4639597 -4.54335 -4.6081824 -4.6048784 -4.5309672][-4.3267231 -4.3682656 -4.4031048 -4.415854 -4.3856053 -4.3579588 -4.3692126 -4.3715034 -4.3168974 -4.2744875 -4.3181939 -4.4299865 -4.5298057 -4.5615773 -4.518342][-4.3097687 -4.3629789 -4.4129095 -4.4267945 -4.36167 -4.2612157 -4.2203836 -4.2126637 -4.1492147 -4.0836182 -4.1164136 -4.2409568 -4.3695359 -4.4457078 -4.4516306][-4.3055978 -4.3557248 -4.4021254 -4.4057903 -4.316679 -4.1760125 -4.1102519 -4.1068816 -4.0503922 -3.970758 -3.9785383 -4.0843763 -4.2128243 -4.3180356 -4.3709254][-4.296411 -4.3319626 -4.3607941 -4.3518476 -4.2600446 -4.1290894 -4.0787659 -4.089242 -4.041234 -3.9584179 -3.9435422 -4.0230322 -4.136879 -4.2463803 -4.3240376][-4.2826118 -4.3003716 -4.3119607 -4.2962089 -4.218502 -4.1242657 -4.1030254 -4.1180577 -4.0737209 -4.00101 -3.9851766 -4.055541 -4.1580019 -4.2551975 -4.3279686][-4.3009119 -4.3104053 -4.3102503 -4.2925382 -4.2313728 -4.1728616 -4.1696796 -4.1738434 -4.1322122 -4.0854468 -4.0925274 -4.171175 -4.2649031 -4.3333974 -4.3727226][-4.3473678 -4.3644028 -4.36032 -4.3387632 -4.283905 -4.2373543 -4.2258968 -4.211113 -4.1816397 -4.1771564 -4.218966 -4.3055925 -4.3838277 -4.4223208 -4.4278421][-4.3740726 -4.40133 -4.3978562 -4.3667216 -4.3051667 -4.2484422 -4.21498 -4.1914926 -4.1933789 -4.2403522 -4.315052 -4.395505 -4.4474921 -4.4593873 -4.4431486][-4.3619385 -4.3923597 -4.3849363 -4.3395209 -4.2705488 -4.2077379 -4.161438 -4.1435022 -4.1754251 -4.2613449 -4.3564305 -4.4245133 -4.4508414 -4.4433923 -4.4114032][-4.3171911 -4.3442731 -4.3294334 -4.274447 -4.2069693 -4.147954 -4.1025391 -4.0978069 -4.1491408 -4.2544074 -4.363965 -4.4279647 -4.4403777 -4.4182019 -4.3704019][-4.2508922 -4.2725735 -4.2593908 -4.2152362 -4.1671357 -4.1211491 -4.0824337 -4.0885167 -4.1520157 -4.2666407 -4.3815732 -4.441288 -4.4466925 -4.4134226 -4.3513689]]...]
INFO - root - 2017-12-07 15:53:01.481736: step 29010, loss = 21.87, batch loss = 21.79 (8.4 examples/sec; 0.956 sec/batch; 80h:35m:36s remains)
INFO - root - 2017-12-07 15:53:10.678760: step 29020, loss = 21.06, batch loss = 20.98 (8.7 examples/sec; 0.919 sec/batch; 77h:26m:02s remains)
INFO - root - 2017-12-07 15:53:20.089747: step 29030, loss = 21.44, batch loss = 21.35 (8.6 examples/sec; 0.933 sec/batch; 78h:41m:03s remains)
INFO - root - 2017-12-07 15:53:29.603170: step 29040, loss = 21.23, batch loss = 21.15 (8.6 examples/sec; 0.926 sec/batch; 78h:04m:54s remains)
INFO - root - 2017-12-07 15:53:39.203132: step 29050, loss = 21.57, batch loss = 21.48 (8.1 examples/sec; 0.992 sec/batch; 83h:36m:26s remains)
INFO - root - 2017-12-07 15:53:48.476220: step 29060, loss = 21.37, batch loss = 21.29 (8.3 examples/sec; 0.962 sec/batch; 81h:04m:39s remains)
INFO - root - 2017-12-07 15:53:57.822602: step 29070, loss = 21.41, batch loss = 21.32 (8.1 examples/sec; 0.983 sec/batch; 82h:51m:26s remains)
INFO - root - 2017-12-07 15:54:07.232289: step 29080, loss = 21.41, batch loss = 21.33 (8.2 examples/sec; 0.973 sec/batch; 82h:01m:26s remains)
INFO - root - 2017-12-07 15:54:16.642543: step 29090, loss = 21.33, batch loss = 21.24 (8.5 examples/sec; 0.943 sec/batch; 79h:29m:54s remains)
INFO - root - 2017-12-07 15:54:25.935790: step 29100, loss = 21.34, batch loss = 21.26 (8.6 examples/sec; 0.929 sec/batch; 78h:16m:33s remains)
2017-12-07 15:54:26.836520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3607483 -4.4066815 -4.4466529 -4.4635382 -4.4609694 -4.4502416 -4.4415655 -4.4417853 -4.440042 -4.4277492 -4.4102588 -4.3799405 -4.3422465 -4.302587 -4.2597671][-4.4391475 -4.5019693 -4.5471668 -4.5593209 -4.5469427 -4.5239367 -4.5076828 -4.5111938 -4.5235395 -4.5289116 -4.5211048 -4.4898429 -4.4465914 -4.3944044 -4.3317471][-4.5009651 -4.5609021 -4.594151 -4.5896721 -4.5569019 -4.5124 -4.4826112 -4.4883866 -4.5236063 -4.5616493 -4.5803003 -4.5725203 -4.5518284 -4.5087938 -4.4344239][-4.5343475 -4.5784349 -4.5901494 -4.5634785 -4.5046706 -4.4332066 -4.3820834 -4.3815851 -4.4400659 -4.5163593 -4.5683336 -4.5990305 -4.6227446 -4.6126189 -4.5466514][-4.5430908 -4.5730805 -4.5638738 -4.5126586 -4.4207115 -4.3143311 -4.2289009 -4.2046118 -4.2754331 -4.3900938 -4.4828744 -4.5579162 -4.6278734 -4.6593184 -4.6197162][-4.5273356 -4.5473347 -4.5209675 -4.44445 -4.3176851 -4.1801429 -4.0605321 -4.0030909 -4.0710707 -4.2123241 -4.3458214 -4.4602008 -4.5587292 -4.6183219 -4.6070914][-4.4934826 -4.5058312 -4.46337 -4.3631282 -4.2067132 -4.0437651 -3.8912668 -3.7943726 -3.843549 -3.9993885 -4.1773024 -4.3333364 -4.4480872 -4.5207605 -4.5306411][-4.4665651 -4.4723196 -4.415874 -4.3026595 -4.1343617 -3.9583292 -3.7716775 -3.6226783 -3.63132 -3.7798858 -3.9927454 -4.1968613 -4.3401418 -4.4352164 -4.4667435][-4.474206 -4.4723444 -4.4047351 -4.299 -4.1584587 -4.0061874 -3.8107042 -3.6215189 -3.5777688 -3.6820567 -3.8790975 -4.0918007 -4.2535958 -4.3730493 -4.4299593][-4.52489 -4.5083804 -4.4348812 -4.3500271 -4.2592945 -4.1557603 -3.9874196 -3.7966545 -3.7116892 -3.7509885 -3.8819847 -4.0439272 -4.1815777 -4.2986908 -4.3728824][-4.5695429 -4.5383692 -4.4680681 -4.4099874 -4.3686128 -4.31307 -4.1880684 -4.0210958 -3.9056594 -3.8826261 -3.9385142 -4.0293455 -4.1200266 -4.21215 -4.2924767][-4.5705829 -4.526382 -4.4639325 -4.4328761 -4.4310174 -4.4150829 -4.3416853 -4.2160621 -4.0917339 -4.0291824 -4.0273781 -4.0568085 -4.1045294 -4.1703057 -4.2518826][-4.5379992 -4.4837008 -4.4326439 -4.4216747 -4.4337974 -4.427206 -4.3913 -4.3173747 -4.2238035 -4.1769829 -4.1677675 -4.1688895 -4.1903353 -4.2369976 -4.3128743][-4.4919486 -4.4208231 -4.3752904 -4.3780441 -4.3918896 -4.3793144 -4.3637142 -4.334796 -4.2823491 -4.2712 -4.286365 -4.2942538 -4.3216715 -4.3800817 -4.4644504][-4.4151354 -4.3285232 -4.2797236 -4.2852836 -4.2941847 -4.2690296 -4.2606931 -4.2638879 -4.2455029 -4.263844 -4.3073521 -4.3374338 -4.391078 -4.485014 -4.5965996]]...]
INFO - root - 2017-12-07 15:54:36.230475: step 29110, loss = 21.75, batch loss = 21.67 (9.0 examples/sec; 0.891 sec/batch; 75h:03m:27s remains)
INFO - root - 2017-12-07 15:54:45.618653: step 29120, loss = 21.06, batch loss = 20.98 (8.7 examples/sec; 0.914 sec/batch; 77h:04m:00s remains)
INFO - root - 2017-12-07 15:54:54.884483: step 29130, loss = 21.77, batch loss = 21.69 (8.5 examples/sec; 0.941 sec/batch; 79h:17m:02s remains)
INFO - root - 2017-12-07 15:55:04.299756: step 29140, loss = 21.18, batch loss = 21.09 (8.6 examples/sec; 0.927 sec/batch; 78h:05m:00s remains)
INFO - root - 2017-12-07 15:55:13.650492: step 29150, loss = 21.32, batch loss = 21.24 (8.3 examples/sec; 0.960 sec/batch; 80h:55m:46s remains)
INFO - root - 2017-12-07 15:55:22.760385: step 29160, loss = 21.55, batch loss = 21.47 (10.1 examples/sec; 0.788 sec/batch; 66h:26m:16s remains)
INFO - root - 2017-12-07 15:55:32.205341: step 29170, loss = 21.36, batch loss = 21.27 (8.9 examples/sec; 0.895 sec/batch; 75h:25m:03s remains)
INFO - root - 2017-12-07 15:55:41.449194: step 29180, loss = 21.47, batch loss = 21.39 (9.0 examples/sec; 0.891 sec/batch; 75h:04m:46s remains)
INFO - root - 2017-12-07 15:55:50.808010: step 29190, loss = 21.48, batch loss = 21.40 (8.9 examples/sec; 0.902 sec/batch; 75h:59m:33s remains)
INFO - root - 2017-12-07 15:56:00.164900: step 29200, loss = 21.58, batch loss = 21.50 (8.5 examples/sec; 0.946 sec/batch; 79h:40m:09s remains)
2017-12-07 15:56:01.061773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.493433 -4.61983 -4.7192717 -4.7851624 -4.8069043 -4.7685351 -4.6693783 -4.6395135 -4.6913023 -4.6705818 -4.6059136 -4.5816989 -4.581255 -4.5744653 -4.5932608][-4.4926195 -4.629395 -4.7238913 -4.773241 -4.7875795 -4.75595 -4.6323013 -4.5858541 -4.6600337 -4.6363015 -4.5579553 -4.5462775 -4.5478611 -4.5154777 -4.5200071][-4.4843221 -4.6190228 -4.698493 -4.71884 -4.7128663 -4.6848326 -4.5435119 -4.4810724 -4.5615005 -4.5335813 -4.462039 -4.4829717 -4.4994092 -4.4350338 -4.4005733][-4.4997525 -4.6204753 -4.6814785 -4.6692643 -4.629 -4.5778804 -4.4148545 -4.3495674 -4.4341836 -4.4116411 -4.3642726 -4.4188528 -4.4592638 -4.3823352 -4.3113446][-4.5292082 -4.6232734 -4.6615319 -4.62583 -4.5523777 -4.4483004 -4.2426538 -4.1745172 -4.2706385 -4.2675819 -4.2518473 -4.3314424 -4.401 -4.3534913 -4.278686][-4.5427222 -4.5986118 -4.5982528 -4.5315161 -4.4230108 -4.2525544 -3.9882441 -3.91128 -4.0403996 -4.0802522 -4.0955086 -4.1835332 -4.2807755 -4.2998986 -4.2564068][-4.5513544 -4.5765824 -4.5247326 -4.4038486 -4.2405992 -3.9999089 -3.67376 -3.5936782 -3.7948744 -3.9185212 -3.9754555 -4.061451 -4.1788716 -4.2796679 -4.2858028][-4.571681 -4.5926309 -4.5085382 -4.3337646 -4.1050391 -3.7985456 -3.4201746 -3.3345108 -3.6108091 -3.834166 -3.9446726 -4.0289149 -4.1526661 -4.3090186 -4.3491282][-4.608407 -4.6518059 -4.578836 -4.3973055 -4.1380315 -3.8098304 -3.4223733 -3.3201942 -3.6056526 -3.8784742 -4.0255733 -4.11179 -4.2301149 -4.3934765 -4.433351][-4.6428823 -4.7173233 -4.6848774 -4.5443978 -4.3161869 -4.0293307 -3.690619 -3.5733106 -3.7970376 -4.0481157 -4.2039857 -4.2968264 -4.4007244 -4.5221658 -4.5294771][-4.6502109 -4.74414 -4.75411 -4.6775455 -4.5243745 -4.3221779 -4.0690665 -3.9550941 -4.0869713 -4.2638168 -4.3862643 -4.4725623 -4.5528021 -4.6145477 -4.5880079][-4.6204734 -4.7248878 -4.7673955 -4.7513714 -4.6844997 -4.5794339 -4.42758 -4.3437967 -4.4017448 -4.4887867 -4.5485115 -4.6064711 -4.6568222 -4.6730571 -4.6308537][-4.5611496 -4.6619587 -4.7221074 -4.7435775 -4.7326179 -4.69418 -4.6223497 -4.5791693 -4.5992093 -4.6279693 -4.6425791 -4.6715326 -4.6968484 -4.6903214 -4.6495223][-4.4888892 -4.570169 -4.6285357 -4.6599379 -4.6697855 -4.6630006 -4.6371779 -4.6200724 -4.6239281 -4.6267867 -4.6241989 -4.6351748 -4.6437936 -4.6327834 -4.6053853][-4.420301 -4.4732385 -4.5169334 -4.5426764 -4.553885 -4.5553508 -4.5484662 -4.5437474 -4.5453262 -4.5452924 -4.5421867 -4.5431328 -4.5418944 -4.5330954 -4.5186515]]...]
INFO - root - 2017-12-07 15:56:10.469254: step 29210, loss = 20.98, batch loss = 20.89 (8.7 examples/sec; 0.920 sec/batch; 77h:31m:18s remains)
INFO - root - 2017-12-07 15:56:19.908408: step 29220, loss = 21.40, batch loss = 21.31 (8.4 examples/sec; 0.948 sec/batch; 79h:51m:48s remains)
INFO - root - 2017-12-07 15:56:29.258142: step 29230, loss = 21.75, batch loss = 21.66 (8.7 examples/sec; 0.919 sec/batch; 77h:24m:17s remains)
INFO - root - 2017-12-07 15:56:38.694506: step 29240, loss = 21.52, batch loss = 21.44 (9.0 examples/sec; 0.887 sec/batch; 74h:40m:48s remains)
INFO - root - 2017-12-07 15:56:48.172763: step 29250, loss = 21.50, batch loss = 21.42 (8.6 examples/sec; 0.936 sec/batch; 78h:48m:48s remains)
INFO - root - 2017-12-07 15:56:57.620042: step 29260, loss = 21.59, batch loss = 21.50 (8.0 examples/sec; 0.999 sec/batch; 84h:07m:36s remains)
INFO - root - 2017-12-07 15:57:06.861921: step 29270, loss = 21.61, batch loss = 21.52 (8.6 examples/sec; 0.930 sec/batch; 78h:19m:26s remains)
INFO - root - 2017-12-07 15:57:16.279788: step 29280, loss = 21.38, batch loss = 21.30 (8.4 examples/sec; 0.958 sec/batch; 80h:40m:54s remains)
INFO - root - 2017-12-07 15:57:25.798390: step 29290, loss = 21.12, batch loss = 21.03 (8.7 examples/sec; 0.914 sec/batch; 77h:01m:11s remains)
INFO - root - 2017-12-07 15:57:35.091715: step 29300, loss = 21.21, batch loss = 21.13 (8.4 examples/sec; 0.952 sec/batch; 80h:08m:50s remains)
2017-12-07 15:57:36.058452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4984016 -4.490726 -4.3881874 -4.2468605 -4.1648569 -4.1948285 -4.2899189 -4.386869 -4.4410744 -4.4366493 -4.4235005 -4.4471178 -4.4845934 -4.50067 -4.4984388][-4.4946642 -4.5210962 -4.4561787 -4.3389993 -4.2614 -4.2785635 -4.3587408 -4.4513159 -4.51049 -4.5215831 -4.5214882 -4.5403028 -4.5518456 -4.5313191 -4.5001287][-4.4780602 -4.5468268 -4.526844 -4.4305558 -4.3348966 -4.3077655 -4.3525753 -4.4376526 -4.5177522 -4.5721188 -4.6145272 -4.6538286 -4.65719 -4.6083484 -4.5476642][-4.4636812 -4.572165 -4.5806265 -4.4742041 -4.3215485 -4.2200866 -4.215322 -4.2880363 -4.3866029 -4.4875355 -4.5900636 -4.6872158 -4.7346234 -4.7051873 -4.6366563][-4.4560094 -4.5856304 -4.5905595 -4.4438272 -4.219552 -4.0439057 -3.9895446 -4.0383415 -4.1325259 -4.2514324 -4.4007549 -4.57378 -4.7130551 -4.7592144 -4.7239151][-4.4330072 -4.5545926 -4.5331421 -4.3435845 -4.0677776 -3.8430388 -3.7484603 -3.7668583 -3.8357611 -3.9368761 -4.0959835 -4.3241463 -4.5622211 -4.7139797 -4.7485971][-4.3994365 -4.4973788 -4.450058 -4.2422018 -3.9501858 -3.7026238 -3.575273 -3.5610185 -3.5956571 -3.6569409 -3.7933462 -4.037746 -4.343503 -4.585094 -4.6897526][-4.3712187 -4.4553232 -4.4103694 -4.2290273 -3.9650805 -3.7194858 -3.5647597 -3.5116978 -3.5048406 -3.5155327 -3.6101823 -3.8429751 -4.1776152 -4.4676819 -4.6108513][-4.3701091 -4.4612522 -4.4514937 -4.332159 -4.1235108 -3.8940601 -3.7149334 -3.6228316 -3.5768039 -3.5439031 -3.6008787 -3.8121033 -4.1458549 -4.4429154 -4.5780497][-4.4101534 -4.5163794 -4.5484109 -4.4907846 -4.3362088 -4.1290007 -3.9411891 -3.8282759 -3.7627194 -3.7131047 -3.7525818 -3.9391706 -4.2452059 -4.5093884 -4.6006012][-4.4626479 -4.5700231 -4.6287403 -4.6166663 -4.5069823 -4.3297873 -4.15759 -4.0554523 -4.0061917 -3.9755745 -4.0089903 -4.1483455 -4.3768735 -4.5579739 -4.5852561][-4.4785538 -4.5631828 -4.626524 -4.6493745 -4.5921307 -4.4703274 -4.3466921 -4.2806945 -4.2652454 -4.2606039 -4.2748365 -4.3307977 -4.4347687 -4.4989772 -4.4623871][-4.4260058 -4.4598842 -4.5018582 -4.5487342 -4.5580378 -4.519835 -4.469943 -4.451632 -4.4643617 -4.4716167 -4.4527411 -4.4153819 -4.3889661 -4.340899 -4.2576838][-4.3164086 -4.2831459 -4.2816162 -4.3311462 -4.3984618 -4.4453783 -4.4701166 -4.4901853 -4.5152731 -4.5229359 -4.4840951 -4.3972616 -4.2988682 -4.191678 -4.0892229][-4.2122025 -4.1088839 -4.0418057 -4.0568304 -4.150166 -4.2645216 -4.3533583 -4.4032831 -4.4363642 -4.4521384 -4.4279842 -4.3560481 -4.2626324 -4.1590776 -4.0635271]]...]
INFO - root - 2017-12-07 15:57:45.318497: step 29310, loss = 21.54, batch loss = 21.46 (8.3 examples/sec; 0.964 sec/batch; 81h:10m:13s remains)
INFO - root - 2017-12-07 15:57:54.652165: step 29320, loss = 21.42, batch loss = 21.33 (8.3 examples/sec; 0.967 sec/batch; 81h:26m:24s remains)
INFO - root - 2017-12-07 15:58:03.950734: step 29330, loss = 21.37, batch loss = 21.29 (8.5 examples/sec; 0.940 sec/batch; 79h:10m:50s remains)
INFO - root - 2017-12-07 15:58:13.385808: step 29340, loss = 21.56, batch loss = 21.48 (8.3 examples/sec; 0.964 sec/batch; 81h:12m:20s remains)
INFO - root - 2017-12-07 15:58:22.792879: step 29350, loss = 20.98, batch loss = 20.90 (8.4 examples/sec; 0.950 sec/batch; 79h:58m:54s remains)
INFO - root - 2017-12-07 15:58:32.192816: step 29360, loss = 21.61, batch loss = 21.53 (8.2 examples/sec; 0.980 sec/batch; 82h:31m:33s remains)
INFO - root - 2017-12-07 15:58:41.317831: step 29370, loss = 21.54, batch loss = 21.46 (8.7 examples/sec; 0.920 sec/batch; 77h:27m:21s remains)
INFO - root - 2017-12-07 15:58:50.631484: step 29380, loss = 21.16, batch loss = 21.08 (8.1 examples/sec; 0.986 sec/batch; 83h:03m:40s remains)
INFO - root - 2017-12-07 15:58:59.993460: step 29390, loss = 21.47, batch loss = 21.39 (8.4 examples/sec; 0.949 sec/batch; 79h:54m:45s remains)
INFO - root - 2017-12-07 15:59:09.379152: step 29400, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.908 sec/batch; 76h:27m:15s remains)
2017-12-07 15:59:10.308388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4310074 -4.4882832 -4.5350728 -4.5448 -4.5263853 -4.4983797 -4.4891262 -4.4966393 -4.5108504 -4.5175524 -4.5145626 -4.5014687 -4.4761586 -4.4387469 -4.3955765][-4.4503551 -4.5361619 -4.6025786 -4.6083255 -4.5704913 -4.5193758 -4.4992695 -4.5073471 -4.5355706 -4.5636926 -4.5785551 -4.5738883 -4.5427623 -4.4909225 -4.429688][-4.446228 -4.5522141 -4.6293893 -4.6248136 -4.5641737 -4.4911609 -4.4635525 -4.474884 -4.5155525 -4.5594749 -4.5926185 -4.6050854 -4.5852861 -4.538384 -4.4690394][-4.4353309 -4.5499 -4.6275072 -4.6073251 -4.5221882 -4.4292707 -4.3959937 -4.4128757 -4.4619236 -4.5145555 -4.5565972 -4.5843449 -4.5859408 -4.5615292 -4.4993505][-4.4245672 -4.5335774 -4.6012983 -4.554728 -4.4359365 -4.3195591 -4.27919 -4.3051596 -4.368856 -4.4403119 -4.4936051 -4.5300965 -4.5501509 -4.5527372 -4.5083966][-4.394372 -4.4970403 -4.5438046 -4.4532995 -4.2861586 -4.1350112 -4.0825171 -4.12835 -4.2303357 -4.342926 -4.41999 -4.4688749 -4.5046024 -4.5270352 -4.5003939][-4.3397641 -4.44143 -4.4566069 -4.3041983 -4.0742035 -3.8875971 -3.8350701 -3.9227543 -4.0871668 -4.2544775 -4.3656116 -4.4350877 -4.4845963 -4.5144053 -4.4929237][-4.301065 -4.4078245 -4.3862867 -4.1737933 -3.8851712 -3.6693435 -3.6245227 -3.7567213 -3.9841702 -4.2051897 -4.352407 -4.4435525 -4.4987659 -4.5217152 -4.4932985][-4.3245487 -4.426405 -4.3672576 -4.1182032 -3.8069232 -3.58715 -3.5602326 -3.7210391 -3.9842825 -4.2315888 -4.398715 -4.4975362 -4.5408907 -4.5426984 -4.4992752][-4.4030352 -4.4877462 -4.4017444 -4.1493268 -3.8586159 -3.6678376 -3.6641045 -3.8281126 -4.0844975 -4.3199868 -4.4783111 -4.5617185 -4.5797539 -4.5569196 -4.4992666][-4.504293 -4.5759425 -4.4918284 -4.2723327 -4.0351973 -3.8861661 -3.8925769 -4.0299864 -4.2416062 -4.4328604 -4.5547953 -4.6056166 -4.5954022 -4.5540257 -4.4882445][-4.6106796 -4.6692529 -4.6048756 -4.4395576 -4.2674055 -4.1613674 -4.1721907 -4.2787752 -4.433085 -4.5638146 -4.6340365 -4.6430936 -4.6032195 -4.5445266 -4.4715943][-4.6752443 -4.7249484 -4.6899204 -4.5887527 -4.4844575 -4.4199157 -4.4326029 -4.5111279 -4.6107807 -4.6823645 -4.6993637 -4.6654925 -4.5976133 -4.5217843 -4.445292][-4.6664195 -4.7113991 -4.7051835 -4.6600823 -4.6101227 -4.5764236 -4.5841656 -4.6313276 -4.6838846 -4.7083721 -4.68591 -4.6268129 -4.5486407 -4.4707966 -4.4028535][-4.570693 -4.6072278 -4.6162014 -4.6025758 -4.5820312 -4.5654616 -4.5674267 -4.5902786 -4.6125975 -4.6124725 -4.5787635 -4.5229464 -4.459547 -4.4001775 -4.3533683]]...]
INFO - root - 2017-12-07 15:59:19.717847: step 29410, loss = 21.43, batch loss = 21.35 (7.8 examples/sec; 1.028 sec/batch; 86h:35m:27s remains)
INFO - root - 2017-12-07 15:59:28.971250: step 29420, loss = 21.23, batch loss = 21.14 (8.9 examples/sec; 0.894 sec/batch; 75h:16m:01s remains)
INFO - root - 2017-12-07 15:59:38.401208: step 29430, loss = 21.13, batch loss = 21.05 (8.2 examples/sec; 0.976 sec/batch; 82h:09m:07s remains)
INFO - root - 2017-12-07 15:59:47.628317: step 29440, loss = 21.34, batch loss = 21.26 (8.3 examples/sec; 0.969 sec/batch; 81h:36m:50s remains)
INFO - root - 2017-12-07 15:59:57.043908: step 29450, loss = 21.31, batch loss = 21.23 (8.5 examples/sec; 0.946 sec/batch; 79h:36m:18s remains)
INFO - root - 2017-12-07 16:00:06.423670: step 29460, loss = 21.19, batch loss = 21.10 (8.9 examples/sec; 0.897 sec/batch; 75h:32m:39s remains)
INFO - root - 2017-12-07 16:00:15.573944: step 29470, loss = 21.16, batch loss = 21.08 (9.1 examples/sec; 0.874 sec/batch; 73h:36m:09s remains)
INFO - root - 2017-12-07 16:00:24.964713: step 29480, loss = 21.33, batch loss = 21.25 (9.2 examples/sec; 0.872 sec/batch; 73h:23m:18s remains)
INFO - root - 2017-12-07 16:00:34.279124: step 29490, loss = 21.52, batch loss = 21.44 (8.9 examples/sec; 0.903 sec/batch; 76h:01m:52s remains)
INFO - root - 2017-12-07 16:00:43.637296: step 29500, loss = 21.48, batch loss = 21.39 (8.4 examples/sec; 0.955 sec/batch; 80h:21m:34s remains)
2017-12-07 16:00:44.548586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.741374 -4.6258259 -4.4961147 -4.4223175 -4.4797368 -4.584547 -4.6467776 -4.6876163 -4.7089915 -4.6784868 -4.63669 -4.5913453 -4.5190172 -4.4588671 -4.41387][-4.6241794 -4.4996634 -4.3797297 -4.3202419 -4.3878117 -4.4895196 -4.5468788 -4.5985756 -4.627429 -4.6025352 -4.5614204 -4.5003796 -4.4111171 -4.3406596 -4.2899785][-4.4591618 -4.3479152 -4.2576609 -4.2139626 -4.2766166 -4.3595247 -4.4002423 -4.4494338 -4.4683609 -4.4510756 -4.4335585 -4.3926277 -4.3321762 -4.28416 -4.2357183][-4.302362 -4.2265882 -4.1718717 -4.1354661 -4.1743565 -4.2256637 -4.2483978 -4.2863994 -4.2939777 -4.2992091 -4.3255363 -4.3212857 -4.30896 -4.2991471 -4.2672915][-4.19376 -4.1556907 -4.1342745 -4.1025224 -4.1086373 -4.1179905 -4.119782 -4.1442351 -4.1586661 -4.2041178 -4.2756796 -4.3022623 -4.3156633 -4.3259106 -4.3094583][-4.1188388 -4.1008797 -4.1052337 -4.0882344 -4.0733576 -4.0349379 -3.9950304 -3.9923496 -4.0156221 -4.1026134 -4.2045383 -4.2475843 -4.2607942 -4.2631941 -4.2467613][-4.089191 -4.0714693 -4.0851793 -4.0842175 -4.056994 -3.9755583 -3.8932631 -3.8695393 -3.9053617 -4.0069809 -4.1021814 -4.1373668 -4.1362586 -4.1278863 -4.1114297][-4.1194577 -4.0845938 -4.0839167 -4.0839834 -4.051363 -3.9581521 -3.8669009 -3.8528824 -3.9025404 -3.9788282 -4.0236726 -4.0268292 -4.0155015 -4.0153565 -4.0148325][-4.2118492 -4.1567593 -4.1356826 -4.1316519 -4.1133356 -4.0516343 -3.9874403 -3.9962037 -4.0446906 -4.0671992 -4.0420723 -4.0002475 -3.9851263 -4.0106983 -4.0442481][-4.3252087 -4.2682991 -4.2422523 -4.2381344 -4.2410083 -4.2188964 -4.1835175 -4.2002354 -4.23495 -4.216867 -4.1438441 -4.0674734 -4.0508142 -4.1094604 -4.1856427][-4.4203057 -4.3826919 -4.3630805 -4.3501868 -4.3534131 -4.3500185 -4.3335948 -4.3521128 -4.37681 -4.3489342 -4.2694707 -4.1880794 -4.171382 -4.2420182 -4.3355756][-4.4830256 -4.4785657 -4.4760413 -4.45851 -4.4554362 -4.4556713 -4.4511285 -4.4689507 -4.4854646 -4.4636927 -4.4027038 -4.3378668 -4.3231359 -4.3814011 -4.4650135][-4.5047178 -4.5252509 -4.53987 -4.5309234 -4.5297279 -4.5327115 -4.5349178 -4.5500011 -4.561595 -4.5505052 -4.513906 -4.4710207 -4.4565959 -4.4891472 -4.5430093][-4.4715657 -4.4975367 -4.5166254 -4.5182853 -4.523952 -4.5311007 -4.5382204 -4.5528903 -4.5663095 -4.5706592 -4.5593023 -4.5357394 -4.5173936 -4.5168419 -4.5298705][-4.3941417 -4.4129443 -4.4291449 -4.4367318 -4.4461541 -4.4544649 -4.4626021 -4.4741755 -4.4861703 -4.4969859 -4.4991217 -4.4896612 -4.4751682 -4.4621925 -4.4539471]]...]
INFO - root - 2017-12-07 16:00:54.011343: step 29510, loss = 21.27, batch loss = 21.19 (8.7 examples/sec; 0.916 sec/batch; 77h:07m:50s remains)
INFO - root - 2017-12-07 16:01:03.383790: step 29520, loss = 21.43, batch loss = 21.35 (8.3 examples/sec; 0.966 sec/batch; 81h:15m:52s remains)
INFO - root - 2017-12-07 16:01:12.765922: step 29530, loss = 21.60, batch loss = 21.52 (7.9 examples/sec; 1.015 sec/batch; 85h:24m:43s remains)
INFO - root - 2017-12-07 16:01:22.073550: step 29540, loss = 21.32, batch loss = 21.23 (7.7 examples/sec; 1.034 sec/batch; 86h:59m:41s remains)
INFO - root - 2017-12-07 16:01:31.464894: step 29550, loss = 21.36, batch loss = 21.28 (8.0 examples/sec; 0.995 sec/batch; 83h:44m:41s remains)
INFO - root - 2017-12-07 16:01:40.662269: step 29560, loss = 20.87, batch loss = 20.78 (9.2 examples/sec; 0.871 sec/batch; 73h:18m:36s remains)
INFO - root - 2017-12-07 16:01:50.113541: step 29570, loss = 22.11, batch loss = 22.03 (8.3 examples/sec; 0.966 sec/batch; 81h:18m:26s remains)
INFO - root - 2017-12-07 16:01:59.434756: step 29580, loss = 21.10, batch loss = 21.02 (8.1 examples/sec; 0.987 sec/batch; 83h:02m:02s remains)
INFO - root - 2017-12-07 16:02:08.707443: step 29590, loss = 21.23, batch loss = 21.14 (8.3 examples/sec; 0.963 sec/batch; 80h:59m:20s remains)
INFO - root - 2017-12-07 16:02:18.009860: step 29600, loss = 21.18, batch loss = 21.10 (9.3 examples/sec; 0.864 sec/batch; 72h:43m:11s remains)
2017-12-07 16:02:18.978073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4127803 -4.4656067 -4.4884863 -4.5385089 -4.6213093 -4.6741257 -4.6663184 -4.6374774 -4.6338062 -4.6524467 -4.6369262 -4.5568342 -4.4413533 -4.3569384 -4.349113][-4.5164423 -4.5939574 -4.6530871 -4.7179751 -4.7782779 -4.7836375 -4.741395 -4.7144613 -4.7338624 -4.7721519 -4.7672472 -4.6851139 -4.5539403 -4.4471183 -4.4213028][-4.5964322 -4.7110209 -4.8132653 -4.8758049 -4.8755746 -4.8007674 -4.7080283 -4.6816626 -4.7346444 -4.811862 -4.8536048 -4.8173122 -4.7102079 -4.5986571 -4.5471449][-4.629621 -4.7870483 -4.9183779 -4.9504991 -4.8618708 -4.6851206 -4.5296879 -4.5044456 -4.6088986 -4.7538114 -4.8727 -4.9084606 -4.8477297 -4.7447538 -4.6674314][-4.639452 -4.8180189 -4.9344707 -4.900259 -4.7113085 -4.429512 -4.2019668 -4.175427 -4.3559132 -4.6054187 -4.819438 -4.9311147 -4.91966 -4.8313255 -4.735743][-4.6714725 -4.8422823 -4.8965416 -4.7631168 -4.4662361 -4.0776792 -3.7660024 -3.7281966 -3.9960804 -4.3738089 -4.6884208 -4.869175 -4.9013696 -4.830421 -4.7290535][-4.6970453 -4.8515406 -4.83686 -4.6124616 -4.2251339 -3.73442 -3.3194003 -3.2450194 -3.5895872 -4.0880318 -4.4900851 -4.7302051 -4.8013458 -4.7423906 -4.6450114][-4.7182083 -4.87668 -4.8458896 -4.5985632 -4.1734757 -3.6056838 -3.0914745 -2.9633682 -3.3481472 -3.9165683 -4.3667207 -4.6434612 -4.7373266 -4.6715951 -4.56319][-4.7696362 -4.94746 -4.956049 -4.7589493 -4.3675795 -3.8005657 -3.267395 -3.1175604 -3.4806161 -4.0154314 -4.428925 -4.6853132 -4.7663774 -4.6731272 -4.5289412][-4.8367467 -5.0203452 -5.0747418 -4.9495273 -4.6270652 -4.137413 -3.6866298 -3.5709777 -3.8754804 -4.2906933 -4.5849495 -4.7586937 -4.796504 -4.6746984 -4.5006094][-4.8660922 -5.0250654 -5.0996466 -5.0293641 -4.7838664 -4.4202232 -4.1190233 -4.0741005 -4.3097968 -4.5767493 -4.7161188 -4.7715497 -4.752223 -4.6179171 -4.4404807][-4.8297772 -4.9357271 -4.9936872 -4.9440432 -4.7680273 -4.550981 -4.4236412 -4.4646778 -4.6425953 -4.7796087 -4.7812047 -4.7276225 -4.6703148 -4.56191 -4.4187784][-4.7271338 -4.7690992 -4.7873678 -4.7373476 -4.6248007 -4.5519214 -4.583035 -4.7003694 -4.8338432 -4.8720856 -4.777081 -4.65586 -4.5999846 -4.5615988 -4.4966326][-4.6050396 -4.5875235 -4.5565972 -4.4891171 -4.4125981 -4.4354749 -4.5693045 -4.7361145 -4.8459659 -4.8303638 -4.6931739 -4.5569048 -4.52973 -4.5692849 -4.5961075][-4.5107679 -4.4584107 -4.3987164 -4.3241262 -4.2655368 -4.3241572 -4.4871631 -4.6587148 -4.74676 -4.7035685 -4.5611095 -4.4459338 -4.4475212 -4.5245686 -4.6072512]]...]
INFO - root - 2017-12-07 16:02:28.333892: step 29610, loss = 21.67, batch loss = 21.59 (8.5 examples/sec; 0.939 sec/batch; 78h:59m:49s remains)
INFO - root - 2017-12-07 16:02:37.641921: step 29620, loss = 21.80, batch loss = 21.71 (8.6 examples/sec; 0.928 sec/batch; 78h:05m:51s remains)
INFO - root - 2017-12-07 16:02:47.081192: step 29630, loss = 21.37, batch loss = 21.29 (8.9 examples/sec; 0.899 sec/batch; 75h:36m:52s remains)
INFO - root - 2017-12-07 16:02:56.503027: step 29640, loss = 21.50, batch loss = 21.41 (8.8 examples/sec; 0.908 sec/batch; 76h:22m:27s remains)
INFO - root - 2017-12-07 16:03:05.940868: step 29650, loss = 21.24, batch loss = 21.16 (8.0 examples/sec; 0.997 sec/batch; 83h:52m:17s remains)
INFO - root - 2017-12-07 16:03:15.375283: step 29660, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.951 sec/batch; 79h:59m:30s remains)
INFO - root - 2017-12-07 16:03:24.667339: step 29670, loss = 21.33, batch loss = 21.24 (8.1 examples/sec; 0.992 sec/batch; 83h:25m:21s remains)
INFO - root - 2017-12-07 16:03:33.787607: step 29680, loss = 21.01, batch loss = 20.93 (8.7 examples/sec; 0.920 sec/batch; 77h:24m:12s remains)
INFO - root - 2017-12-07 16:03:43.142840: step 29690, loss = 21.38, batch loss = 21.30 (8.0 examples/sec; 0.996 sec/batch; 83h:46m:45s remains)
INFO - root - 2017-12-07 16:03:52.527346: step 29700, loss = 21.43, batch loss = 21.35 (7.9 examples/sec; 1.011 sec/batch; 85h:00m:57s remains)
2017-12-07 16:03:53.465471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5698977 -4.7220182 -4.7588868 -4.7214365 -4.6724219 -4.6291947 -4.5801997 -4.5280795 -4.48025 -4.428782 -4.3540111 -4.2560973 -4.1822848 -4.1723027 -4.2347074][-4.5002127 -4.7197766 -4.8088827 -4.7720904 -4.70256 -4.6603889 -4.6270776 -4.5707788 -4.4969306 -4.41386 -4.2980094 -4.16029 -4.0632005 -4.0497065 -4.1211505][-4.463304 -4.7088566 -4.8166361 -4.7693143 -4.6659508 -4.6069117 -4.5911942 -4.5671411 -4.5117226 -4.4199228 -4.2765751 -4.110383 -3.991648 -3.9749582 -4.0514059][-4.4934416 -4.7120318 -4.7941856 -4.7194242 -4.5787888 -4.488646 -4.4780512 -4.5000534 -4.4980431 -4.445106 -4.3262424 -4.1739416 -4.04931 -4.0218511 -4.0760508][-4.56472 -4.7191448 -4.7503757 -4.643651 -4.4710808 -4.344748 -4.3240585 -4.3770866 -4.4286704 -4.441195 -4.3943944 -4.3006349 -4.193686 -4.1495996 -4.1620793][-4.6308031 -4.727387 -4.7152338 -4.5886512 -4.3993907 -4.2377696 -4.1808133 -4.2197175 -4.28936 -4.3637915 -4.4208112 -4.4311004 -4.37395 -4.3175054 -4.2851677][-4.6818352 -4.738564 -4.7020555 -4.571312 -4.383872 -4.2029953 -4.1003013 -4.0853019 -4.1272483 -4.2240252 -4.3622856 -4.4815083 -4.5012622 -4.4606428 -4.4078627][-4.6867414 -4.7213349 -4.6756124 -4.5567045 -4.39481 -4.2260466 -4.0940042 -4.0130644 -3.9978659 -4.0735621 -4.2478108 -4.446456 -4.5515871 -4.5557542 -4.51175][-4.6363258 -4.65609 -4.6066146 -4.5077505 -4.3850689 -4.2526321 -4.1184392 -3.9928432 -3.92708 -3.9751475 -4.1566324 -4.3961134 -4.5681624 -4.6307564 -4.6167345][-4.6015625 -4.5862169 -4.5136704 -4.4179544 -4.3233943 -4.2304244 -4.1225433 -3.9977264 -3.9199274 -3.9583132 -4.1272521 -4.361371 -4.5532379 -4.6535897 -4.6701441][-4.6115165 -4.5694528 -4.4721756 -4.3630381 -4.2762341 -4.2059436 -4.1278276 -4.03173 -3.9785066 -4.0283952 -4.1757712 -4.3673644 -4.5261822 -4.6188564 -4.6407003][-4.6306152 -4.5856223 -4.485579 -4.3711081 -4.282486 -4.2159381 -4.1591063 -4.101697 -4.0891895 -4.1555538 -4.2751527 -4.4079061 -4.5117612 -4.5692391 -4.5690675][-4.6286364 -4.5924606 -4.5062456 -4.4007683 -4.3109994 -4.237576 -4.1880174 -4.1662731 -4.1938858 -4.2743053 -4.3676891 -4.4508891 -4.5122256 -4.5397062 -4.5069928][-4.5935926 -4.581779 -4.5244584 -4.439662 -4.3503528 -4.2657795 -4.2136374 -4.2089553 -4.2540784 -4.3350992 -4.4102511 -4.4686379 -4.5136309 -4.5291805 -4.4739566][-4.5043669 -4.5037394 -4.4684176 -4.4022951 -4.3211169 -4.243567 -4.2047029 -4.2125373 -4.2570605 -4.329144 -4.3972454 -4.4559612 -4.5014119 -4.5097084 -4.4421754]]...]
INFO - root - 2017-12-07 16:04:02.992363: step 29710, loss = 20.99, batch loss = 20.90 (8.7 examples/sec; 0.923 sec/batch; 77h:35m:43s remains)
INFO - root - 2017-12-07 16:04:12.446165: step 29720, loss = 21.51, batch loss = 21.43 (8.3 examples/sec; 0.965 sec/batch; 81h:09m:01s remains)
INFO - root - 2017-12-07 16:04:21.976762: step 29730, loss = 21.63, batch loss = 21.55 (8.1 examples/sec; 0.990 sec/batch; 83h:16m:23s remains)
INFO - root - 2017-12-07 16:04:31.250486: step 29740, loss = 21.72, batch loss = 21.63 (8.3 examples/sec; 0.963 sec/batch; 81h:01m:03s remains)
INFO - root - 2017-12-07 16:04:40.709369: step 29750, loss = 21.78, batch loss = 21.70 (8.9 examples/sec; 0.901 sec/batch; 75h:45m:55s remains)
INFO - root - 2017-12-07 16:04:50.031982: step 29760, loss = 21.84, batch loss = 21.75 (9.1 examples/sec; 0.876 sec/batch; 73h:40m:28s remains)
INFO - root - 2017-12-07 16:04:59.324023: step 29770, loss = 21.53, batch loss = 21.45 (8.9 examples/sec; 0.899 sec/batch; 75h:38m:21s remains)
INFO - root - 2017-12-07 16:05:08.746274: step 29780, loss = 22.04, batch loss = 21.96 (8.5 examples/sec; 0.938 sec/batch; 78h:54m:04s remains)
INFO - root - 2017-12-07 16:05:18.153633: step 29790, loss = 21.21, batch loss = 21.13 (8.2 examples/sec; 0.977 sec/batch; 82h:10m:38s remains)
INFO - root - 2017-12-07 16:05:27.582830: step 29800, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.947 sec/batch; 79h:37m:15s remains)
2017-12-07 16:05:28.531647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3385706 -4.3665791 -4.374826 -4.3570528 -4.3406348 -4.3455305 -4.3674703 -4.3908134 -4.403697 -4.3939953 -4.3707352 -4.3531456 -4.3337159 -4.3057637 -4.278553][-4.4167194 -4.4702983 -4.4928036 -4.4769979 -4.4567857 -4.4594927 -4.4730177 -4.4746423 -4.4685235 -4.4656796 -4.4720683 -4.4819531 -4.4722996 -4.4299521 -4.3714318][-4.5079713 -4.5838323 -4.6196194 -4.6013 -4.5666289 -4.54651 -4.5244946 -4.4768229 -4.4387765 -4.4650311 -4.545723 -4.6212125 -4.6404476 -4.5863924 -4.4895258][-4.5690012 -4.6512895 -4.6833858 -4.6485119 -4.58554 -4.5182085 -4.4264851 -4.3028154 -4.2334166 -4.3161645 -4.5061011 -4.6736121 -4.7419977 -4.6954155 -4.5762143][-4.6082067 -4.6851544 -4.6878314 -4.6106811 -4.496027 -4.347064 -4.1500597 -3.9448533 -3.8803363 -4.05566 -4.3603363 -4.6132984 -4.7385836 -4.7236323 -4.6074448][-4.6438508 -4.7083278 -4.6548209 -4.5068955 -4.3152828 -4.0572076 -3.7357306 -3.4697258 -3.4638703 -3.7571423 -4.15235 -4.4605513 -4.6356273 -4.6719427 -4.5885105][-4.6774955 -4.7372642 -4.6397314 -4.4298153 -4.1653142 -3.8040545 -3.3734403 -3.0816815 -3.1543002 -3.5359683 -3.9647634 -4.2892308 -4.4958234 -4.5787234 -4.5394678][-4.7001939 -4.7836409 -4.6936274 -4.4733539 -4.1805344 -3.7807879 -3.3287158 -3.0743897 -3.2114961 -3.6043034 -3.9978166 -4.294446 -4.4840808 -4.5564151 -4.5225997][-4.6769023 -4.7994127 -4.7555871 -4.573977 -4.3135142 -3.9659114 -3.600647 -3.4423234 -3.6080794 -3.9347432 -4.2392 -4.4714789 -4.6103821 -4.6331873 -4.5689554][-4.5975695 -4.7441931 -4.7529111 -4.646071 -4.4716768 -4.2334986 -3.9908724 -3.9156809 -4.0665827 -4.2959824 -4.4926362 -4.6418591 -4.7250481 -4.7118416 -4.6293392][-4.5386357 -4.6720448 -4.6924109 -4.647923 -4.5717769 -4.4511719 -4.3121996 -4.273984 -4.3810258 -4.5285645 -4.6445379 -4.726685 -4.7645116 -4.7325506 -4.6504526][-4.5903826 -4.6738605 -4.6598687 -4.6263986 -4.6042385 -4.5607595 -4.4864173 -4.4511447 -4.501307 -4.58346 -4.651001 -4.697505 -4.7133279 -4.6796 -4.6093578][-4.7083287 -4.7396488 -4.6801796 -4.6168146 -4.58804 -4.5649037 -4.52278 -4.4894404 -4.4981718 -4.53254 -4.5698047 -4.5988808 -4.6040497 -4.5728688 -4.5179267][-4.7671242 -4.7642684 -4.6770291 -4.5791039 -4.5177088 -4.4865952 -4.464673 -4.4484534 -4.4461818 -4.4588604 -4.4795489 -4.4974179 -4.493494 -4.4619861 -4.4187322][-4.7199893 -4.7005458 -4.6109662 -4.5025249 -4.4185505 -4.3727217 -4.356967 -4.3548522 -4.3549509 -4.3572412 -4.3654895 -4.3762922 -4.3747387 -4.3560343 -4.3328915]]...]
INFO - root - 2017-12-07 16:05:37.938841: step 29810, loss = 21.83, batch loss = 21.74 (8.3 examples/sec; 0.968 sec/batch; 81h:22m:53s remains)
INFO - root - 2017-12-07 16:05:47.246724: step 29820, loss = 21.51, batch loss = 21.43 (9.0 examples/sec; 0.889 sec/batch; 74h:42m:21s remains)
INFO - root - 2017-12-07 16:05:56.459892: step 29830, loss = 21.40, batch loss = 21.32 (9.3 examples/sec; 0.864 sec/batch; 72h:39m:23s remains)
INFO - root - 2017-12-07 16:06:05.802080: step 29840, loss = 21.28, batch loss = 21.20 (8.6 examples/sec; 0.933 sec/batch; 78h:28m:17s remains)
INFO - root - 2017-12-07 16:06:15.088240: step 29850, loss = 21.67, batch loss = 21.59 (8.5 examples/sec; 0.939 sec/batch; 78h:56m:04s remains)
INFO - root - 2017-12-07 16:06:24.519769: step 29860, loss = 21.53, batch loss = 21.45 (8.5 examples/sec; 0.938 sec/batch; 78h:49m:29s remains)
INFO - root - 2017-12-07 16:06:33.900651: step 29870, loss = 21.43, batch loss = 21.34 (8.8 examples/sec; 0.910 sec/batch; 76h:30m:28s remains)
INFO - root - 2017-12-07 16:06:43.279491: step 29880, loss = 21.32, batch loss = 21.23 (9.6 examples/sec; 0.836 sec/batch; 70h:14m:50s remains)
INFO - root - 2017-12-07 16:06:52.669417: step 29890, loss = 21.86, batch loss = 21.78 (8.4 examples/sec; 0.954 sec/batch; 80h:12m:51s remains)
INFO - root - 2017-12-07 16:07:02.076645: step 29900, loss = 21.31, batch loss = 21.22 (8.5 examples/sec; 0.946 sec/batch; 79h:30m:02s remains)
2017-12-07 16:07:03.042704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3718052 -4.3844442 -4.4482689 -4.4923682 -4.496192 -4.4644289 -4.4122615 -4.4203568 -4.5016847 -4.5400457 -4.5076222 -4.4743738 -4.4815912 -4.5437145 -4.6282883][-4.3511462 -4.3490419 -4.4177251 -4.4802661 -4.5117011 -4.5083561 -4.4729652 -4.4736476 -4.5222149 -4.53277 -4.4878688 -4.447278 -4.4382243 -4.4802766 -4.5539203][-4.2860403 -4.2683411 -4.3393288 -4.4177036 -4.4776907 -4.5037408 -4.4980593 -4.5203919 -4.5601497 -4.5529251 -4.4960856 -4.4412584 -4.4002218 -4.4025068 -4.4540024][-4.2142835 -4.1831179 -4.2436881 -4.3232613 -4.39116 -4.4119873 -4.4157553 -4.4775882 -4.5429983 -4.5440588 -4.4912214 -4.4338379 -4.3728065 -4.3393388 -4.3734651][-4.1670694 -4.1333675 -4.1809874 -4.2449274 -4.2837305 -4.2398772 -4.2023435 -4.2913971 -4.4040341 -4.438664 -4.4129362 -4.3705707 -4.3090882 -4.2611666 -4.2904797][-4.1929612 -4.1532559 -4.1827192 -4.2153525 -4.1959848 -4.0491986 -3.9253988 -4.0042686 -4.1651392 -4.2604375 -4.2898617 -4.2896295 -4.2587 -4.2244763 -4.2542996][-4.2406378 -4.184216 -4.1979327 -4.2098403 -4.13683 -3.8984571 -3.6852272 -3.7228322 -3.91187 -4.0692711 -4.1620712 -4.214457 -4.235393 -4.2368255 -4.26758][-4.2713089 -4.2074642 -4.2115536 -4.2157359 -4.110311 -3.8256328 -3.5652072 -3.5600872 -3.755312 -3.9556031 -4.0884948 -4.1727462 -4.2346492 -4.2659297 -4.2860084][-4.3957438 -4.3505607 -4.33364 -4.3160491 -4.1991181 -3.9304774 -3.694092 -3.6709621 -3.8359346 -4.0311332 -4.1610141 -4.2462187 -4.3277087 -4.3720217 -4.3765345][-4.5691028 -4.5624027 -4.5223446 -4.47162 -4.3622317 -4.1586442 -3.9857266 -3.9527946 -4.0505524 -4.1879573 -4.2803617 -4.3577929 -4.4622331 -4.5279031 -4.5335069][-4.6688275 -4.7017746 -4.6492438 -4.5749755 -4.4871697 -4.3615689 -4.258204 -4.2267532 -4.26511 -4.337894 -4.3912787 -4.4641442 -4.5812316 -4.6524124 -4.6540742][-4.7045469 -4.7770853 -4.7502184 -4.6884389 -4.6348023 -4.5781097 -4.5309539 -4.5102353 -4.5189257 -4.5456223 -4.5610623 -4.6060905 -4.6919942 -4.72961 -4.7030249][-4.6673961 -4.7626429 -4.7792425 -4.7561278 -4.7447133 -4.7381 -4.7237649 -4.7082214 -4.699132 -4.6900687 -4.6697054 -4.6758661 -4.715426 -4.7195392 -4.6749916][-4.5708232 -4.6471243 -4.6767907 -4.676826 -4.6888533 -4.7080703 -4.7141623 -4.7053738 -4.6901994 -4.668344 -4.6434431 -4.6392174 -4.6575751 -4.6503987 -4.6058521][-4.50676 -4.5557933 -4.5822091 -4.5946155 -4.6154261 -4.6377511 -4.6466413 -4.6420779 -4.6291919 -4.6127443 -4.6006608 -4.5997109 -4.605629 -4.5910854 -4.5508127]]...]
INFO - root - 2017-12-07 16:07:12.318036: step 29910, loss = 21.71, batch loss = 21.62 (8.9 examples/sec; 0.895 sec/batch; 75h:15m:13s remains)
INFO - root - 2017-12-07 16:07:21.803555: step 29920, loss = 21.73, batch loss = 21.65 (8.6 examples/sec; 0.928 sec/batch; 77h:58m:24s remains)
INFO - root - 2017-12-07 16:07:31.243446: step 29930, loss = 21.96, batch loss = 21.88 (8.5 examples/sec; 0.942 sec/batch; 79h:11m:29s remains)
INFO - root - 2017-12-07 16:07:40.550198: step 29940, loss = 21.49, batch loss = 21.41 (8.7 examples/sec; 0.921 sec/batch; 77h:26m:21s remains)
INFO - root - 2017-12-07 16:07:49.899302: step 29950, loss = 21.64, batch loss = 21.56 (8.5 examples/sec; 0.945 sec/batch; 79h:25m:33s remains)
INFO - root - 2017-12-07 16:07:59.335106: step 29960, loss = 21.40, batch loss = 21.32 (8.3 examples/sec; 0.967 sec/batch; 81h:15m:29s remains)
INFO - root - 2017-12-07 16:08:08.872418: step 29970, loss = 21.73, batch loss = 21.65 (8.5 examples/sec; 0.940 sec/batch; 78h:59m:24s remains)
INFO - root - 2017-12-07 16:08:18.263832: step 29980, loss = 21.55, batch loss = 21.47 (8.6 examples/sec; 0.931 sec/batch; 78h:16m:18s remains)
INFO - root - 2017-12-07 16:08:27.739540: step 29990, loss = 21.34, batch loss = 21.26 (8.5 examples/sec; 0.943 sec/batch; 79h:12m:48s remains)
INFO - root - 2017-12-07 16:08:37.277802: step 30000, loss = 21.44, batch loss = 21.36 (8.2 examples/sec; 0.972 sec/batch; 81h:41m:58s remains)
2017-12-07 16:08:38.324819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1980777 -4.2161078 -4.2236524 -4.239686 -4.2880659 -4.355957 -4.4192691 -4.46707 -4.4811306 -4.4820642 -4.5052624 -4.5383582 -4.5641212 -4.5530667 -4.5247226][-4.2809415 -4.31964 -4.3447256 -4.3682189 -4.4073987 -4.4552832 -4.49791 -4.5278912 -4.5370779 -4.5395236 -4.5548573 -4.5773187 -4.58736 -4.5629425 -4.5293183][-4.3468761 -4.3996811 -4.437417 -4.4625297 -4.4820933 -4.4941649 -4.5009308 -4.5057654 -4.5129118 -4.5227275 -4.5365062 -4.5505915 -4.5469031 -4.5113215 -4.4742084][-4.3621325 -4.4160352 -4.4536395 -4.4699693 -4.4684725 -4.447351 -4.4222989 -4.4119267 -4.4283028 -4.4599428 -4.4861417 -4.4996095 -4.4874358 -4.4454846 -4.4075179][-4.3528719 -4.394537 -4.4192944 -4.4141946 -4.3846736 -4.3335848 -4.2861943 -4.2717228 -4.3074465 -4.3719697 -4.4255233 -4.4520035 -4.4416952 -4.4025393 -4.3658266][-4.3599329 -4.3758106 -4.371922 -4.3266368 -4.2530007 -4.1678128 -4.110127 -4.1138573 -4.187573 -4.295197 -4.3861308 -4.4296384 -4.418355 -4.3726087 -4.32386][-4.3645945 -4.3466392 -4.3040586 -4.2129722 -4.0902267 -3.97245 -3.9163291 -3.9552107 -4.0794725 -4.2325869 -4.3555236 -4.4102669 -4.3910837 -4.3281851 -4.2625484][-4.3443089 -4.2979369 -4.2253051 -4.1066103 -3.9580071 -3.8315947 -3.7918048 -3.8598027 -4.0128517 -4.1789713 -4.303277 -4.3564053 -4.3350329 -4.2700725 -4.2105775][-4.3119431 -4.2599082 -4.1830697 -4.0733175 -3.9404597 -3.8390687 -3.8244429 -3.900991 -4.0407715 -4.1686344 -4.2462835 -4.2737865 -4.2557511 -4.2157459 -4.1905575][-4.2876596 -4.2538171 -4.1978469 -4.118022 -4.0205011 -3.9505892 -3.9576349 -4.0335913 -4.1395783 -4.2049456 -4.2165756 -4.2027807 -4.1843696 -4.1762958 -4.186275][-4.2714396 -4.2595053 -4.2259178 -4.1725445 -4.1064005 -4.06089 -4.0841384 -4.16506 -4.2533932 -4.2768297 -4.2360482 -4.1801472 -4.1529374 -4.1692009 -4.2070112][-4.274044 -4.28218 -4.2713051 -4.2448735 -4.2088203 -4.1852589 -4.2172637 -4.2965693 -4.370471 -4.36701 -4.2947431 -4.2095346 -4.173748 -4.202949 -4.2583451][-4.3321009 -4.36372 -4.3784375 -4.3750825 -4.3609138 -4.3498435 -4.3758264 -4.434556 -4.4835653 -4.4617581 -4.3793759 -4.286552 -4.2427559 -4.2664638 -4.3214493][-4.432445 -4.4769812 -4.5054574 -4.5130568 -4.5085316 -4.499361 -4.5087876 -4.5376682 -4.5599189 -4.5302153 -4.4584694 -4.3757215 -4.3234115 -4.3256016 -4.3628225][-4.4993 -4.5421948 -4.5735455 -4.5848846 -4.5798521 -4.563911 -4.5571885 -4.5657539 -4.5698018 -4.5407062 -4.4897208 -4.4227924 -4.3631287 -4.3440247 -4.363059]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 16:08:48.503159: step 30010, loss = 21.16, batch loss = 21.08 (8.8 examples/sec; 0.906 sec/batch; 76h:05m:24s remains)
INFO - root - 2017-12-07 16:08:57.886189: step 30020, loss = 21.67, batch loss = 21.59 (8.4 examples/sec; 0.950 sec/batch; 79h:47m:28s remains)
INFO - root - 2017-12-07 16:09:07.251276: step 30030, loss = 21.65, batch loss = 21.57 (8.2 examples/sec; 0.975 sec/batch; 81h:55m:08s remains)
INFO - root - 2017-12-07 16:09:16.709765: step 30040, loss = 21.79, batch loss = 21.70 (8.8 examples/sec; 0.904 sec/batch; 75h:58m:01s remains)
INFO - root - 2017-12-07 16:09:26.171302: step 30050, loss = 21.13, batch loss = 21.04 (9.0 examples/sec; 0.890 sec/batch; 74h:45m:15s remains)
INFO - root - 2017-12-07 16:09:35.634817: step 30060, loss = 21.39, batch loss = 21.30 (8.6 examples/sec; 0.926 sec/batch; 77h:47m:11s remains)
INFO - root - 2017-12-07 16:09:45.184016: step 30070, loss = 21.14, batch loss = 21.05 (8.5 examples/sec; 0.941 sec/batch; 79h:01m:15s remains)
INFO - root - 2017-12-07 16:09:54.639619: step 30080, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.976 sec/batch; 82h:00m:45s remains)
INFO - root - 2017-12-07 16:10:03.852649: step 30090, loss = 21.84, batch loss = 21.76 (8.3 examples/sec; 0.965 sec/batch; 81h:02m:05s remains)
INFO - root - 2017-12-07 16:10:13.457638: step 30100, loss = 21.83, batch loss = 21.74 (8.2 examples/sec; 0.976 sec/batch; 82h:00m:01s remains)
2017-12-07 16:10:14.436500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5168324 -4.4515791 -4.3969994 -4.3644862 -4.333199 -4.3239861 -4.3767371 -4.4656096 -4.5210323 -4.4950447 -4.4340978 -4.4086494 -4.413837 -4.4069719 -4.3886347][-4.4887819 -4.4359212 -4.3901572 -4.3602524 -4.3405666 -4.3552804 -4.420651 -4.5107217 -4.5718737 -4.5646057 -4.5221138 -4.5014729 -4.5001326 -4.4891567 -4.4651084][-4.4648018 -4.4094024 -4.3634195 -4.3351207 -4.3262196 -4.3586378 -4.4329376 -4.5242062 -4.5887079 -4.6041141 -4.5904245 -4.5736918 -4.554956 -4.52643 -4.4881988][-4.432704 -4.3497767 -4.2816234 -4.2405496 -4.2303567 -4.27315 -4.36037 -4.4650931 -4.54412 -4.595027 -4.6286173 -4.6352811 -4.6096444 -4.5514183 -4.4752107][-4.4139447 -4.297997 -4.1973405 -4.1296134 -4.0992832 -4.1272941 -4.2056332 -4.3134975 -4.4126205 -4.5166645 -4.6239972 -4.6884165 -4.6730132 -4.5624585 -4.4082532][-4.444973 -4.3107743 -4.1853843 -4.0938811 -4.0383024 -4.0210586 -4.0426207 -4.1109219 -4.2110391 -4.3655982 -4.5516357 -4.6936049 -4.7080617 -4.5452504 -4.2957158][-4.5273309 -4.3935528 -4.2580252 -4.1572404 -4.0873346 -4.0195155 -3.9612291 -3.9578466 -4.0286279 -4.2032089 -4.4349256 -4.6347365 -4.6919966 -4.5090756 -4.1862421][-4.60833 -4.494751 -4.3663874 -4.2681317 -4.1931357 -4.0956526 -3.9859457 -3.9280658 -3.9673295 -4.12967 -4.356854 -4.5693254 -4.6617422 -4.50092 -4.1605415][-4.6247711 -4.5388741 -4.4356413 -4.3575897 -4.2942629 -4.2056804 -4.0997119 -4.0276394 -4.0419583 -4.1667566 -4.3550415 -4.5500841 -4.6633115 -4.5593719 -4.2745829][-4.5775857 -4.5040307 -4.4201035 -4.3681307 -4.3401933 -4.30248 -4.2496128 -4.1970253 -4.1883149 -4.2585292 -4.3932886 -4.5586042 -4.6771231 -4.6366568 -4.4486][-4.4989758 -4.4187522 -4.3274865 -4.2784829 -4.280611 -4.30541 -4.3270826 -4.3269238 -4.32683 -4.3643522 -4.4535308 -4.5791345 -4.6768246 -4.6711254 -4.5650206][-4.4089561 -4.3089271 -4.1904721 -4.1158919 -4.1152372 -4.1742282 -4.2594433 -4.331553 -4.3847785 -4.4439197 -4.5233874 -4.6169453 -4.6794872 -4.6760225 -4.6140876][-4.3450527 -4.2315431 -4.088666 -3.9769821 -3.9478328 -4.0104671 -4.1308393 -4.2582397 -4.3689666 -4.4695811 -4.5623274 -4.6410723 -4.679544 -4.6726766 -4.6374221][-4.3595343 -4.2572808 -4.1168337 -3.9840322 -3.9263864 -3.9767523 -4.1031208 -4.2501397 -4.3822269 -4.4877181 -4.5685573 -4.634306 -4.6658764 -4.6666026 -4.6485682][-4.4471011 -4.3774662 -4.2627587 -4.1289992 -4.0531545 -4.0897923 -4.2056537 -4.34323 -4.4592872 -4.5264211 -4.5532794 -4.5821204 -4.6028194 -4.6144338 -4.6091552]]...]
INFO - root - 2017-12-07 16:10:23.673041: step 30110, loss = 21.03, batch loss = 20.95 (8.5 examples/sec; 0.938 sec/batch; 78h:45m:08s remains)
INFO - root - 2017-12-07 16:10:33.048619: step 30120, loss = 21.55, batch loss = 21.47 (8.9 examples/sec; 0.896 sec/batch; 75h:13m:57s remains)
INFO - root - 2017-12-07 16:10:42.518831: step 30130, loss = 21.37, batch loss = 21.29 (8.8 examples/sec; 0.907 sec/batch; 76h:11m:36s remains)
INFO - root - 2017-12-07 16:10:51.892139: step 30140, loss = 21.37, batch loss = 21.29 (8.1 examples/sec; 0.984 sec/batch; 82h:36m:41s remains)
INFO - root - 2017-12-07 16:11:01.334366: step 30150, loss = 21.54, batch loss = 21.46 (8.6 examples/sec; 0.929 sec/batch; 78h:00m:48s remains)
INFO - root - 2017-12-07 16:11:10.759531: step 30160, loss = 21.49, batch loss = 21.40 (8.2 examples/sec; 0.977 sec/batch; 82h:02m:15s remains)
INFO - root - 2017-12-07 16:11:20.128329: step 30170, loss = 21.67, batch loss = 21.58 (8.4 examples/sec; 0.953 sec/batch; 80h:00m:50s remains)
INFO - root - 2017-12-07 16:11:29.566302: step 30180, loss = 21.54, batch loss = 21.46 (8.7 examples/sec; 0.917 sec/batch; 76h:59m:43s remains)
INFO - root - 2017-12-07 16:11:38.882822: step 30190, loss = 21.41, batch loss = 21.32 (8.3 examples/sec; 0.969 sec/batch; 81h:22m:47s remains)
INFO - root - 2017-12-07 16:11:48.037092: step 30200, loss = 21.32, batch loss = 21.23 (9.2 examples/sec; 0.867 sec/batch; 72h:46m:40s remains)
2017-12-07 16:11:48.976115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4735961 -4.37604 -4.2605791 -4.2114348 -4.266932 -4.38183 -4.50614 -4.558919 -4.4564061 -4.2649546 -4.0760126 -3.9601114 -3.9556832 -4.0379524 -4.1476703][-4.479948 -4.3692374 -4.2282243 -4.1493278 -4.17959 -4.2825637 -4.4230032 -4.52872 -4.512187 -4.4041553 -4.2547178 -4.1135173 -4.0432162 -4.0753212 -4.1694131][-4.5152817 -4.4198117 -4.2796111 -4.1845608 -4.1822109 -4.2434983 -4.3538165 -4.4716606 -4.5183434 -4.4929967 -4.4031587 -4.2739496 -4.1705651 -4.1602831 -4.2324042][-4.5681796 -4.4941463 -4.3569374 -4.2441406 -4.2040486 -4.2085147 -4.2578812 -4.3504953 -4.4308949 -4.4750319 -4.4576788 -4.38049 -4.2879176 -4.2600231 -4.3086047][-4.616159 -4.5532279 -4.4086719 -4.267148 -4.1760845 -4.1112862 -4.0863452 -4.1436415 -4.2466793 -4.3488684 -4.3994837 -4.3863759 -4.3273449 -4.2963428 -4.3297839][-4.64596 -4.5879259 -4.4276776 -4.2413054 -4.0772371 -3.931232 -3.840354 -3.8887391 -4.0348458 -4.1952882 -4.2956991 -4.323143 -4.2776761 -4.2293353 -4.2512479][-4.6642647 -4.6135416 -4.4476576 -4.2214541 -3.9796553 -3.7520592 -3.6160722 -3.68893 -3.9042039 -4.1191463 -4.2389345 -4.2645988 -4.1866317 -4.0830979 -4.0842061][-4.6773119 -4.6460667 -4.4980578 -4.2664657 -3.9799323 -3.6976101 -3.5423498 -3.6446905 -3.9101992 -4.1505785 -4.2622485 -4.2541509 -4.1124482 -3.9299765 -3.8930783][-4.6874342 -4.6777329 -4.5609932 -4.3585062 -4.0793705 -3.7849257 -3.6291246 -3.7445734 -4.0214295 -4.2564139 -4.3461375 -4.2968326 -4.0984578 -3.855391 -3.7807298][-4.6842031 -4.6912456 -4.6020594 -4.4467864 -4.2212362 -3.966116 -3.8363118 -3.9502747 -4.2006092 -4.3996825 -4.4600506 -4.3814092 -4.1655707 -3.9175749 -3.831085][-4.667345 -4.6849475 -4.6159015 -4.5033412 -4.3467836 -4.1675277 -4.0908971 -4.1994128 -4.3991032 -4.5425944 -4.5666013 -4.4772482 -4.2905989 -4.0912437 -4.0203724][-4.6363549 -4.6671906 -4.6221189 -4.5450211 -4.4519205 -4.3531432 -4.3310194 -4.4289579 -4.5712481 -4.6589093 -4.6549282 -4.5756564 -4.4480639 -4.3216381 -4.2736144][-4.5801563 -4.6251664 -4.6121264 -4.572031 -4.5279188 -4.4884844 -4.495532 -4.5648341 -4.6465592 -4.6878209 -4.6729827 -4.6219993 -4.55751 -4.4962497 -4.4681621][-4.4887943 -4.5352163 -4.5468507 -4.5375094 -4.5251508 -4.5170751 -4.5280137 -4.5596061 -4.5896616 -4.5979037 -4.5813217 -4.5558634 -4.5354748 -4.5170646 -4.5056291][-4.3853173 -4.4166341 -4.4335365 -4.4396348 -4.4432726 -4.447021 -4.4525595 -4.4585056 -4.4592705 -4.4512157 -4.4381485 -4.4290347 -4.4275308 -4.427928 -4.4267406]]...]
INFO - root - 2017-12-07 16:11:58.508011: step 30210, loss = 21.46, batch loss = 21.38 (7.8 examples/sec; 1.023 sec/batch; 85h:51m:37s remains)
INFO - root - 2017-12-07 16:12:07.844272: step 30220, loss = 21.59, batch loss = 21.51 (8.3 examples/sec; 0.965 sec/batch; 80h:59m:20s remains)
INFO - root - 2017-12-07 16:12:17.149555: step 30230, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.948 sec/batch; 79h:38m:16s remains)
INFO - root - 2017-12-07 16:12:26.525545: step 30240, loss = 21.68, batch loss = 21.60 (8.5 examples/sec; 0.938 sec/batch; 78h:46m:59s remains)
INFO - root - 2017-12-07 16:12:35.999054: step 30250, loss = 21.58, batch loss = 21.50 (8.8 examples/sec; 0.910 sec/batch; 76h:23m:43s remains)
INFO - root - 2017-12-07 16:12:45.473324: step 30260, loss = 21.30, batch loss = 21.21 (8.4 examples/sec; 0.957 sec/batch; 80h:18m:34s remains)
INFO - root - 2017-12-07 16:12:54.998340: step 30270, loss = 21.02, batch loss = 20.94 (7.8 examples/sec; 1.021 sec/batch; 85h:44m:30s remains)
INFO - root - 2017-12-07 16:13:04.426421: step 30280, loss = 21.76, batch loss = 21.68 (8.8 examples/sec; 0.912 sec/batch; 76h:34m:59s remains)
INFO - root - 2017-12-07 16:13:13.762398: step 30290, loss = 21.23, batch loss = 21.15 (9.3 examples/sec; 0.860 sec/batch; 72h:13m:14s remains)
INFO - root - 2017-12-07 16:13:23.273844: step 30300, loss = 21.28, batch loss = 21.20 (8.5 examples/sec; 0.943 sec/batch; 79h:07m:58s remains)
2017-12-07 16:13:24.267382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33623 -4.3338647 -4.3365097 -4.3437128 -4.3518815 -4.35937 -4.3650813 -4.3680029 -4.3671737 -4.3682027 -4.3782706 -4.3960462 -4.41209 -4.4128919 -4.395833][-4.353302 -4.3509054 -4.3573604 -4.3711491 -4.3857403 -4.398262 -4.4044967 -4.4042912 -4.401031 -4.4044189 -4.4257264 -4.4615812 -4.4900894 -4.4894061 -4.4549842][-4.3734951 -4.369463 -4.3772769 -4.396256 -4.4173031 -4.4327421 -4.4313984 -4.4177723 -4.4074197 -4.4157796 -4.4584622 -4.5235319 -4.5693684 -4.5686874 -4.5158963][-4.3916173 -4.3809114 -4.3832474 -4.4008818 -4.4232016 -4.4341397 -4.4124055 -4.3712454 -4.3485618 -4.3690343 -4.4494624 -4.5577574 -4.6283531 -4.6313534 -4.5622959][-4.4003406 -4.3778768 -4.365572 -4.3729849 -4.3911285 -4.3922281 -4.3433304 -4.2677045 -4.2299681 -4.2654233 -4.3888841 -4.5446296 -4.6446657 -4.6582022 -4.5804944][-4.3905816 -4.3542428 -4.3223057 -4.3149166 -4.326941 -4.317019 -4.2514014 -4.1623049 -4.1234446 -4.1712933 -4.3166313 -4.494132 -4.6097178 -4.6355128 -4.5615263][-4.3587046 -4.3104548 -4.2586946 -4.2350745 -4.235837 -4.2120104 -4.1511273 -4.0887728 -4.0787897 -4.1398973 -4.2779894 -4.4356985 -4.5359097 -4.5646248 -4.5079184][-4.3141437 -4.2566242 -4.1880655 -4.1469212 -4.1312065 -4.0912333 -4.0500913 -4.0412688 -4.07926 -4.1524715 -4.2579408 -4.3632383 -4.4265356 -4.4546418 -4.4291177][-4.2696414 -4.2020369 -4.1208034 -4.0662909 -4.0412159 -3.9948788 -3.9759269 -4.009829 -4.0782242 -4.1483855 -4.2177629 -4.2734833 -4.3046737 -4.3364968 -4.3487177][-4.2393475 -4.1584315 -4.0711093 -4.0182543 -4.0066104 -3.9707406 -3.9561594 -3.9821484 -4.0370502 -4.09235 -4.1534891 -4.2009749 -4.2274141 -4.2671313 -4.3067737][-4.2506852 -4.1547446 -4.0719857 -4.0415411 -4.0624957 -4.0448713 -4.0094037 -3.9809687 -3.9922843 -4.0399213 -4.123034 -4.2026691 -4.2517743 -4.2994795 -4.3445253][-4.3085566 -4.2004609 -4.1267471 -4.1264777 -4.1838508 -4.1923361 -4.1383538 -4.0525379 -4.0165691 -4.059236 -4.1646585 -4.2786827 -4.3575454 -4.4119148 -4.4445519][-4.3854308 -4.2771497 -4.2130613 -4.2301836 -4.3042803 -4.3320341 -4.2794151 -4.174386 -4.119206 -4.1652956 -4.2798271 -4.4036341 -4.4940786 -4.5426269 -4.5511856][-4.4619226 -4.3738475 -4.3207183 -4.3345332 -4.39665 -4.4343896 -4.4064827 -4.3262806 -4.2806468 -4.3263483 -4.4300241 -4.5425863 -4.6255059 -4.654264 -4.6345277][-4.5316482 -4.4756708 -4.4352083 -4.4355083 -4.47404 -4.5159745 -4.5216374 -4.4849329 -4.4600883 -4.4938273 -4.5683966 -4.6518664 -4.7095861 -4.714519 -4.6729717]]...]
INFO - root - 2017-12-07 16:13:33.710244: step 30310, loss = 21.48, batch loss = 21.40 (7.8 examples/sec; 1.030 sec/batch; 86h:29m:14s remains)
INFO - root - 2017-12-07 16:13:42.998407: step 30320, loss = 21.63, batch loss = 21.55 (8.6 examples/sec; 0.930 sec/batch; 78h:04m:29s remains)
INFO - root - 2017-12-07 16:13:52.486060: step 30330, loss = 21.24, batch loss = 21.15 (8.5 examples/sec; 0.946 sec/batch; 79h:26m:03s remains)
INFO - root - 2017-12-07 16:14:02.002768: step 30340, loss = 21.52, batch loss = 21.44 (8.1 examples/sec; 0.989 sec/batch; 82h:58m:38s remains)
INFO - root - 2017-12-07 16:14:11.555043: step 30350, loss = 21.62, batch loss = 21.53 (8.3 examples/sec; 0.963 sec/batch; 80h:51m:49s remains)
INFO - root - 2017-12-07 16:14:20.968009: step 30360, loss = 21.58, batch loss = 21.50 (8.9 examples/sec; 0.894 sec/batch; 75h:02m:03s remains)
INFO - root - 2017-12-07 16:14:30.309771: step 30370, loss = 21.53, batch loss = 21.44 (8.9 examples/sec; 0.901 sec/batch; 75h:35m:36s remains)
INFO - root - 2017-12-07 16:14:39.798864: step 30380, loss = 21.65, batch loss = 21.57 (8.5 examples/sec; 0.942 sec/batch; 79h:02m:22s remains)
INFO - root - 2017-12-07 16:14:49.163849: step 30390, loss = 21.86, batch loss = 21.77 (8.0 examples/sec; 0.998 sec/batch; 83h:45m:25s remains)
INFO - root - 2017-12-07 16:14:58.591382: step 30400, loss = 21.52, batch loss = 21.44 (7.8 examples/sec; 1.021 sec/batch; 85h:39m:01s remains)
2017-12-07 16:14:59.558424: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5355592 -4.5367584 -4.5066004 -4.46643 -4.4602566 -4.490509 -4.5263724 -4.5399184 -4.541101 -4.5349932 -4.5117335 -4.4700012 -4.419663 -4.37948 -4.3567371][-4.5076785 -4.5042267 -4.4756255 -4.4465857 -4.4560118 -4.4960504 -4.5299864 -4.5312963 -4.5310655 -4.541985 -4.5380397 -4.5047479 -4.4465337 -4.3911633 -4.3568168][-4.4553981 -4.4508662 -4.4387341 -4.4406118 -4.4717145 -4.51029 -4.5215988 -4.4929843 -4.4852304 -4.5189629 -4.5507026 -4.5485573 -4.505054 -4.4515228 -4.4155269][-4.4156833 -4.413681 -4.4119878 -4.4240441 -4.4467559 -4.4631891 -4.4496136 -4.408874 -4.410501 -4.4727635 -4.5420341 -4.5805707 -4.5726147 -4.5426841 -4.51788][-4.4051313 -4.3983421 -4.3772655 -4.3486409 -4.311666 -4.2851934 -4.2683754 -4.2614641 -4.3094926 -4.4107261 -4.5102229 -4.5766 -4.6009545 -4.6013832 -4.5959778][-4.4142036 -4.3893895 -4.3224659 -4.2229619 -4.1072693 -4.0407491 -4.0452461 -4.1054215 -4.2146921 -4.3521113 -4.4709716 -4.5439177 -4.5719972 -4.5836883 -4.5969667][-4.422925 -4.3845725 -4.2819047 -4.1245031 -3.9512351 -3.8625736 -3.8918781 -4.0078759 -4.157207 -4.3143458 -4.4414949 -4.5052862 -4.5017195 -4.4883952 -4.5053368][-4.4299531 -4.403616 -4.3077207 -4.1377382 -3.9408691 -3.8300056 -3.8495967 -3.9689565 -4.1222649 -4.2871089 -4.4169073 -4.4618497 -4.4082489 -4.355319 -4.3740396][-4.4415283 -4.44896 -4.3986115 -4.25979 -4.0674267 -3.9256742 -3.8946149 -3.9697919 -4.1021686 -4.2634854 -4.3833685 -4.407949 -4.3198524 -4.2498546 -4.287683][-4.4501462 -4.4919887 -4.4991403 -4.4174438 -4.2528834 -4.09188 -4.0093455 -4.03493 -4.1392941 -4.2753587 -4.3574758 -4.3545237 -4.2598662 -4.1990833 -4.2536097][-4.4466405 -4.50307 -4.5480175 -4.5222239 -4.4023037 -4.25538 -4.1607308 -4.1605411 -4.2334433 -4.3136334 -4.3241334 -4.2786374 -4.1935678 -4.1498127 -4.2065806][-4.4314046 -4.4838591 -4.5426593 -4.5583472 -4.4961629 -4.4023314 -4.3412519 -4.3383145 -4.3648186 -4.35387 -4.2690153 -4.1780133 -4.1107106 -4.0903249 -4.1525974][-4.408277 -4.4506345 -4.5128078 -4.5596008 -4.5586557 -4.5389514 -4.5388694 -4.548861 -4.5207868 -4.4068031 -4.2312541 -4.103858 -4.058013 -4.0683041 -4.1423059][-4.3790312 -4.41449 -4.4782686 -4.5456538 -4.5931478 -4.6380954 -4.6961527 -4.71607 -4.6362748 -4.4447045 -4.2223506 -4.09577 -4.0829911 -4.126718 -4.2055349][-4.3450136 -4.3702531 -4.4302397 -4.50673 -4.5860739 -4.67336 -4.7579846 -4.7644234 -4.6397443 -4.4158535 -4.204072 -4.1148915 -4.1433911 -4.2193575 -4.299129]]...]
INFO - root - 2017-12-07 16:15:08.855573: step 30410, loss = 21.44, batch loss = 21.36 (8.9 examples/sec; 0.899 sec/batch; 75h:27m:10s remains)
INFO - root - 2017-12-07 16:15:18.211218: step 30420, loss = 21.76, batch loss = 21.68 (9.0 examples/sec; 0.889 sec/batch; 74h:37m:19s remains)
INFO - root - 2017-12-07 16:15:27.619430: step 30430, loss = 21.84, batch loss = 21.76 (8.4 examples/sec; 0.948 sec/batch; 79h:34m:05s remains)
INFO - root - 2017-12-07 16:15:37.055051: step 30440, loss = 21.18, batch loss = 21.10 (8.6 examples/sec; 0.926 sec/batch; 77h:41m:03s remains)
INFO - root - 2017-12-07 16:15:46.320253: step 30450, loss = 21.57, batch loss = 21.49 (8.9 examples/sec; 0.900 sec/batch; 75h:29m:31s remains)
INFO - root - 2017-12-07 16:15:55.920825: step 30460, loss = 21.60, batch loss = 21.52 (8.2 examples/sec; 0.979 sec/batch; 82h:10m:14s remains)
INFO - root - 2017-12-07 16:16:05.404132: step 30470, loss = 21.38, batch loss = 21.29 (8.5 examples/sec; 0.944 sec/batch; 79h:13m:02s remains)
INFO - root - 2017-12-07 16:16:14.616303: step 30480, loss = 21.40, batch loss = 21.32 (8.6 examples/sec; 0.934 sec/batch; 78h:23m:23s remains)
INFO - root - 2017-12-07 16:16:23.913568: step 30490, loss = 21.31, batch loss = 21.23 (8.8 examples/sec; 0.909 sec/batch; 76h:13m:47s remains)
INFO - root - 2017-12-07 16:16:33.271379: step 30500, loss = 21.58, batch loss = 21.50 (9.0 examples/sec; 0.889 sec/batch; 74h:36m:34s remains)
2017-12-07 16:16:34.228509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.258543 -4.2638493 -4.2803764 -4.2968736 -4.30988 -4.3159461 -4.3143849 -4.30909 -4.3056178 -4.3022189 -4.2943377 -4.2815132 -4.267858 -4.255785 -4.2488585][-4.2511487 -4.2745323 -4.3091908 -4.3378062 -4.354681 -4.3583431 -4.3518682 -4.3448682 -4.3460636 -4.3517151 -4.3493838 -4.3325405 -4.3030972 -4.2691979 -4.2416639][-4.2641964 -4.3101282 -4.3633652 -4.3976693 -4.4049478 -4.3884792 -4.3601189 -4.3410273 -4.3494272 -4.38176 -4.4121971 -4.416522 -4.3857617 -4.3296614 -4.2717428][-4.3064017 -4.3648272 -4.4223232 -4.4428935 -4.4183412 -4.35781 -4.28529 -4.2390914 -4.2523437 -4.3277783 -4.424087 -4.4892116 -4.4902458 -4.42935 -4.3446527][-4.35857 -4.4128404 -4.4524207 -4.4355478 -4.3560238 -4.229454 -4.0935836 -4.0082884 -4.0228472 -4.1500835 -4.3337712 -4.490478 -4.5561495 -4.5172911 -4.4254422][-4.3996153 -4.4371381 -4.4458652 -4.3803082 -4.2345123 -4.0277262 -3.8144279 -3.6806831 -3.6933131 -3.872432 -4.1443276 -4.3973947 -4.5385928 -4.5404925 -4.4674621][-4.4195361 -4.4468493 -4.4370179 -4.33939 -4.1433034 -3.8645499 -3.5766184 -3.3918896 -3.396013 -3.6137695 -3.9512477 -4.2753563 -4.4732685 -4.5104394 -4.465137][-4.4279656 -4.4586868 -4.4538841 -4.3563333 -4.1466289 -3.8350396 -3.5105963 -3.3006742 -3.3004498 -3.529038 -3.8835232 -4.2288222 -4.4420953 -4.4898858 -4.456501][-4.438292 -4.4813771 -4.4971123 -4.4224977 -4.2340651 -3.9401908 -3.6319838 -3.4339478 -3.43829 -3.6495991 -3.9722302 -4.2884979 -4.4802327 -4.5160265 -4.4717951][-4.4474139 -4.5084076 -4.5499716 -4.510715 -4.369988 -4.1375618 -3.8888431 -3.7263045 -3.73052 -3.8987215 -4.157517 -4.4128985 -4.5612164 -4.5737777 -4.5069571][-4.4449835 -4.5193539 -4.5846744 -4.5860925 -4.5083818 -4.3619027 -4.1989326 -4.0884986 -4.0905685 -4.2026482 -4.3781037 -4.5513644 -4.6420879 -4.6248741 -4.53482][-4.4187875 -4.4967318 -4.5771551 -4.6149607 -4.5977769 -4.5342331 -4.4547868 -4.3969274 -4.3971429 -4.4580116 -4.5551734 -4.6474347 -4.6794763 -4.6316061 -4.5270371][-4.3654776 -4.4354491 -4.519701 -4.5818348 -4.6092978 -4.6065736 -4.5872583 -4.5682173 -4.56802 -4.59271 -4.6336713 -4.6649995 -4.6520257 -4.5840631 -4.4818935][-4.2960396 -4.3479815 -4.4214568 -4.4876943 -4.5342727 -4.5624914 -4.5766783 -4.5815749 -4.5840545 -4.5892882 -4.5955563 -4.5901341 -4.555387 -4.4898305 -4.4120922][-4.2287951 -4.25956 -4.3109393 -4.3609743 -4.4009705 -4.4315419 -4.4536333 -4.4662309 -4.4707251 -4.47032 -4.4654088 -4.4513712 -4.4224796 -4.3815432 -4.3412557]]...]
INFO - root - 2017-12-07 16:16:43.597165: step 30510, loss = 21.82, batch loss = 21.74 (8.4 examples/sec; 0.948 sec/batch; 79h:29m:28s remains)
INFO - root - 2017-12-07 16:16:52.927799: step 30520, loss = 21.66, batch loss = 21.57 (8.2 examples/sec; 0.980 sec/batch; 82h:14m:19s remains)
INFO - root - 2017-12-07 16:17:02.300310: step 30530, loss = 21.74, batch loss = 21.66 (8.8 examples/sec; 0.909 sec/batch; 76h:13m:23s remains)
INFO - root - 2017-12-07 16:17:11.436833: step 30540, loss = 21.69, batch loss = 21.61 (8.6 examples/sec; 0.931 sec/batch; 78h:06m:01s remains)
INFO - root - 2017-12-07 16:17:20.857939: step 30550, loss = 21.20, batch loss = 21.12 (8.4 examples/sec; 0.956 sec/batch; 80h:13m:29s remains)
INFO - root - 2017-12-07 16:17:30.328378: step 30560, loss = 21.42, batch loss = 21.34 (9.0 examples/sec; 0.886 sec/batch; 74h:16m:21s remains)
INFO - root - 2017-12-07 16:17:39.753587: step 30570, loss = 21.05, batch loss = 20.97 (8.7 examples/sec; 0.924 sec/batch; 77h:28m:08s remains)
INFO - root - 2017-12-07 16:17:49.117360: step 30580, loss = 21.77, batch loss = 21.69 (8.9 examples/sec; 0.902 sec/batch; 75h:40m:09s remains)
INFO - root - 2017-12-07 16:17:58.438740: step 30590, loss = 21.18, batch loss = 21.10 (8.9 examples/sec; 0.902 sec/batch; 75h:37m:22s remains)
INFO - root - 2017-12-07 16:18:07.712071: step 30600, loss = 21.62, batch loss = 21.54 (8.4 examples/sec; 0.956 sec/batch; 80h:10m:27s remains)
2017-12-07 16:18:08.679570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3503675 -4.4028029 -4.4569645 -4.5079136 -4.5460825 -4.560358 -4.5491509 -4.5293641 -4.5169764 -4.5188317 -4.5363593 -4.559176 -4.5694041 -4.5575042 -4.53593][-4.4998488 -4.5675631 -4.634244 -4.6921372 -4.7254348 -4.7206244 -4.6817727 -4.6418533 -4.6323256 -4.6505818 -4.6800666 -4.7042227 -4.7081475 -4.6871271 -4.6502748][-4.5931945 -4.6605964 -4.7235703 -4.7727461 -4.7826848 -4.7365041 -4.6499891 -4.5835023 -4.5913634 -4.6587214 -4.7342219 -4.7813115 -4.787653 -4.7574658 -4.7047167][-4.5871305 -4.6354184 -4.6727643 -4.6938787 -4.6659408 -4.5682693 -4.427856 -4.3330703 -4.3640361 -4.4995594 -4.6527219 -4.7512708 -4.7704258 -4.7286048 -4.6594291][-4.5175242 -4.5360374 -4.5323429 -4.5118585 -4.4443383 -4.3088608 -4.1344867 -4.0189033 -4.0643063 -4.2566357 -4.4916749 -4.6566591 -4.6983418 -4.6449428 -4.56302][-4.44491 -4.432189 -4.3782144 -4.302135 -4.19316 -4.0386381 -3.8570764 -3.7298281 -3.7729039 -3.9885974 -4.2834387 -4.5161757 -4.5949593 -4.5463924 -4.468204][-4.4272566 -4.399334 -4.3065577 -4.1790504 -4.0354185 -3.875515 -3.6977386 -3.5546193 -3.5678146 -3.7602143 -4.0662656 -4.3415637 -4.4598031 -4.4336896 -4.3726249][-4.4827676 -4.4629769 -4.3526063 -4.1835418 -4.0118756 -3.8546627 -3.6868815 -3.5281563 -3.4965448 -3.6371861 -3.9131148 -4.1974254 -4.3425622 -4.3429356 -4.3004513][-4.5727072 -4.5818758 -4.4778624 -4.2871828 -4.0983453 -3.9445834 -3.7876165 -3.6220245 -3.5534794 -3.6430962 -3.8722816 -4.1417384 -4.29947 -4.3226805 -4.2962084][-4.6385593 -4.6854348 -4.6186738 -4.44761 -4.2667651 -4.1222692 -3.9755485 -3.8122523 -3.7200587 -3.768795 -3.9521651 -4.1918707 -4.3505535 -4.395731 -4.389348][-4.6584411 -4.7336082 -4.7187872 -4.5980849 -4.4445424 -4.3168058 -4.1899509 -4.0445461 -3.9489627 -3.9773571 -4.1285238 -4.3347631 -4.4805613 -4.5338221 -4.5367856][-4.648253 -4.7372355 -4.7712493 -4.7098937 -4.5945535 -4.4925842 -4.4009132 -4.2953296 -4.2190881 -4.2398672 -4.3601842 -4.5164361 -4.6168146 -4.64207 -4.6235242][-4.6171212 -4.7077894 -4.7793131 -4.7800136 -4.716516 -4.6484289 -4.5945234 -4.5340438 -4.4858646 -4.4978313 -4.5734115 -4.6577911 -4.68818 -4.6651373 -4.61212][-4.573442 -4.644732 -4.7288551 -4.77753 -4.7649441 -4.7247677 -4.6907759 -4.6581469 -4.6355724 -4.6443038 -4.6804538 -4.7052851 -4.68709 -4.6392546 -4.5763631][-4.5254917 -4.5557437 -4.6162615 -4.6786113 -4.70069 -4.6867876 -4.6694431 -4.6585536 -4.6588445 -4.6690769 -4.6795325 -4.6718845 -4.6396489 -4.5983295 -4.5534792]]...]
INFO - root - 2017-12-07 16:18:18.015580: step 30610, loss = 21.14, batch loss = 21.06 (8.9 examples/sec; 0.899 sec/batch; 75h:25m:12s remains)
INFO - root - 2017-12-07 16:18:27.446150: step 30620, loss = 21.51, batch loss = 21.42 (8.4 examples/sec; 0.950 sec/batch; 79h:37m:41s remains)
INFO - root - 2017-12-07 16:18:36.843482: step 30630, loss = 21.53, batch loss = 21.44 (8.7 examples/sec; 0.919 sec/batch; 77h:04m:49s remains)
INFO - root - 2017-12-07 16:18:46.186872: step 30640, loss = 21.52, batch loss = 21.44 (7.7 examples/sec; 1.042 sec/batch; 87h:21m:27s remains)
INFO - root - 2017-12-07 16:18:55.543861: step 30650, loss = 21.47, batch loss = 21.39 (8.0 examples/sec; 1.003 sec/batch; 84h:04m:15s remains)
INFO - root - 2017-12-07 16:19:04.953474: step 30660, loss = 21.27, batch loss = 21.19 (8.3 examples/sec; 0.964 sec/batch; 80h:48m:47s remains)
INFO - root - 2017-12-07 16:19:14.326543: step 30670, loss = 21.36, batch loss = 21.28 (8.6 examples/sec; 0.928 sec/batch; 77h:48m:21s remains)
INFO - root - 2017-12-07 16:19:23.815684: step 30680, loss = 21.34, batch loss = 21.26 (8.0 examples/sec; 0.998 sec/batch; 83h:37m:46s remains)
INFO - root - 2017-12-07 16:19:33.116118: step 30690, loss = 21.43, batch loss = 21.35 (8.3 examples/sec; 0.964 sec/batch; 80h:48m:33s remains)
INFO - root - 2017-12-07 16:19:42.397049: step 30700, loss = 21.61, batch loss = 21.53 (9.6 examples/sec; 0.830 sec/batch; 69h:32m:43s remains)
2017-12-07 16:19:43.345338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5382576 -4.5478096 -4.5552859 -4.5636659 -4.5774269 -4.6098137 -4.6496119 -4.6676135 -4.663806 -4.6545777 -4.6411142 -4.6369662 -4.6647406 -4.7174072 -4.756784][-4.5583768 -4.5583887 -4.5535398 -4.5476341 -4.5535359 -4.5973358 -4.6624146 -4.6917753 -4.6840281 -4.6741166 -4.6655221 -4.6730156 -4.7254186 -4.816648 -4.8925691][-4.551733 -4.5468216 -4.5240736 -4.4850693 -4.4630919 -4.50028 -4.5775986 -4.6114726 -4.5989347 -4.5881319 -4.5866647 -4.6108408 -4.6893969 -4.8176918 -4.9332476][-4.5368657 -4.5250063 -4.4784484 -4.4015336 -4.3470068 -4.3594975 -4.4207149 -4.4441381 -4.4278121 -4.4195518 -4.4280281 -4.4726572 -4.5739884 -4.7268543 -4.8681359][-4.4971457 -4.4653773 -4.3998203 -4.3097754 -4.242918 -4.2221603 -4.2376571 -4.2368736 -4.2247624 -4.2250233 -4.2404661 -4.2942157 -4.40169 -4.5565147 -4.7019548][-4.4204 -4.3572612 -4.2798972 -4.2015114 -4.1417437 -4.0865116 -4.045898 -4.0275788 -4.0427332 -4.06584 -4.0834184 -4.1253681 -4.2133904 -4.3516564 -4.4887533][-4.3456488 -4.2574625 -4.1684294 -4.0941505 -4.0275154 -3.9386427 -3.8574364 -3.8389239 -3.8943377 -3.9462757 -3.9617097 -3.9869998 -4.0587587 -4.1882229 -4.3196435][-4.3422446 -4.254787 -4.156426 -4.0629983 -3.9673624 -3.8510218 -3.7571344 -3.7562222 -3.8508306 -3.9260769 -3.9379129 -3.9559765 -4.0312014 -4.1679506 -4.2994657][-4.4156871 -4.3467855 -4.2416673 -4.1185684 -3.9945054 -3.8753018 -3.8071516 -3.8426609 -3.9672172 -4.0552187 -4.0647855 -4.0826855 -4.1673336 -4.3076253 -4.4256411][-4.4961171 -4.4438944 -4.3379774 -4.200038 -4.072793 -3.9819791 -3.9635313 -4.0415554 -4.1837187 -4.2744112 -4.2804012 -4.2918067 -4.3755817 -4.5054283 -4.5989509][-4.539228 -4.5070348 -4.4200859 -4.2995429 -4.198719 -4.1528769 -4.1823931 -4.2820563 -4.4079661 -4.4748335 -4.467701 -4.4672785 -4.5364509 -4.6450348 -4.716929][-4.5396214 -4.5387182 -4.49345 -4.4154 -4.3522892 -4.3391943 -4.3865261 -4.4721365 -4.5504622 -4.576746 -4.557837 -4.5521708 -4.6044979 -4.6885972 -4.7425623][-4.4932866 -4.5225143 -4.5243979 -4.4958529 -4.4673424 -4.4661522 -4.5001249 -4.5474286 -4.5731106 -4.5663519 -4.5469456 -4.5476217 -4.5911064 -4.6538129 -4.6897311][-4.4094725 -4.4479456 -4.4778438 -4.4860187 -4.4836216 -4.4849668 -4.49698 -4.5114784 -4.5106554 -4.49713 -4.4852252 -4.4907475 -4.5223265 -4.5627866 -4.581759][-4.3342857 -4.3599067 -4.388905 -4.4078913 -4.4175954 -4.4238205 -4.4309268 -4.4376674 -4.43707 -4.4311724 -4.4253831 -4.4256868 -4.4365029 -4.4517684 -4.45653]]...]
INFO - root - 2017-12-07 16:19:52.643515: step 30710, loss = 21.46, batch loss = 21.37 (8.5 examples/sec; 0.944 sec/batch; 79h:09m:11s remains)
INFO - root - 2017-12-07 16:20:01.973797: step 30720, loss = 21.73, batch loss = 21.65 (8.6 examples/sec; 0.930 sec/batch; 77h:59m:46s remains)
INFO - root - 2017-12-07 16:20:11.368982: step 30730, loss = 22.05, batch loss = 21.97 (8.8 examples/sec; 0.906 sec/batch; 75h:55m:39s remains)
INFO - root - 2017-12-07 16:20:20.689720: step 30740, loss = 21.38, batch loss = 21.30 (9.3 examples/sec; 0.861 sec/batch; 72h:08m:54s remains)
INFO - root - 2017-12-07 16:20:30.131019: step 30750, loss = 21.58, batch loss = 21.50 (8.6 examples/sec; 0.930 sec/batch; 77h:57m:58s remains)
INFO - root - 2017-12-07 16:20:39.586358: step 30760, loss = 21.70, batch loss = 21.62 (8.1 examples/sec; 0.986 sec/batch; 82h:39m:17s remains)
INFO - root - 2017-12-07 16:20:48.988030: step 30770, loss = 21.57, batch loss = 21.49 (7.7 examples/sec; 1.044 sec/batch; 87h:29m:13s remains)
INFO - root - 2017-12-07 16:20:58.345083: step 30780, loss = 21.45, batch loss = 21.37 (8.2 examples/sec; 0.981 sec/batch; 82h:15m:09s remains)
INFO - root - 2017-12-07 16:21:07.568243: step 30790, loss = 21.46, batch loss = 21.38 (8.9 examples/sec; 0.895 sec/batch; 74h:59m:00s remains)
INFO - root - 2017-12-07 16:21:16.982074: step 30800, loss = 21.58, batch loss = 21.50 (8.1 examples/sec; 0.986 sec/batch; 82h:40m:24s remains)
2017-12-07 16:21:17.893324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5432091 -4.6060429 -4.6397681 -4.6242146 -4.5830555 -4.5520306 -4.5585594 -4.5829597 -4.5974441 -4.6020384 -4.5985379 -4.5940003 -4.5572052 -4.5078268 -4.4974103][-4.578445 -4.6344228 -4.654212 -4.6110172 -4.5295987 -4.4639354 -4.4613943 -4.4968176 -4.5200129 -4.5228777 -4.5232987 -4.5299511 -4.4916091 -4.4424191 -4.4508119][-4.5712514 -4.6120915 -4.6192832 -4.5610933 -4.45408 -4.3618345 -4.3447967 -4.37618 -4.3956122 -4.3970966 -4.4131842 -4.449893 -4.4283495 -4.39008 -4.4110389][-4.5586863 -4.5805588 -4.5684109 -4.4935174 -4.3711715 -4.2669291 -4.2349153 -4.2520237 -4.2636914 -4.2672715 -4.301506 -4.3697705 -4.3769088 -4.3587823 -4.38127][-4.5626645 -4.5587416 -4.51593 -4.4139929 -4.2781529 -4.170938 -4.1242433 -4.1209245 -4.127388 -4.1354675 -4.1766987 -4.2564287 -4.2938051 -4.305501 -4.3308597][-4.5647559 -4.5401025 -4.4640765 -4.3314924 -4.1847973 -4.0764723 -4.0098052 -3.980804 -3.9852998 -3.9983146 -4.034925 -4.1017509 -4.1542282 -4.19726 -4.2371106][-4.5404482 -4.4980192 -4.3923364 -4.2370024 -4.0962405 -4.0114322 -3.9518092 -3.9156194 -3.9213643 -3.9322307 -3.9508286 -3.9849391 -4.0313239 -4.0952034 -4.1533432][-4.4627357 -4.3974857 -4.2712164 -4.1126661 -3.994894 -3.9594009 -3.9473484 -3.9430594 -3.9675248 -3.9727178 -3.9598088 -3.9468026 -3.9607818 -4.0203776 -4.0936947][-4.3787088 -4.3019729 -4.1750913 -4.02286 -3.9281814 -3.9399557 -3.980788 -4.0214181 -4.0717764 -4.0666742 -4.012022 -3.9522762 -3.9261608 -3.9641025 -4.0479913][-4.3845029 -4.3212466 -4.2060103 -4.0608473 -3.9812994 -4.0148945 -4.080862 -4.1386056 -4.1917176 -4.1715164 -4.0821462 -3.9943573 -3.9412723 -3.9597957 -4.0527005][-4.4615645 -4.4347296 -4.3441958 -4.2099996 -4.1293287 -4.1490564 -4.198225 -4.2401562 -4.2749705 -4.2435422 -4.1494246 -4.0685329 -4.0143352 -4.02084 -4.1096663][-4.5263286 -4.5321031 -4.47264 -4.3554144 -4.2680907 -4.2612381 -4.287816 -4.3152075 -4.3364153 -4.3103995 -4.2464342 -4.2034192 -4.1692619 -4.16627 -4.2316151][-4.545331 -4.5743575 -4.5501385 -4.4651017 -4.387753 -4.3736911 -4.3975887 -4.4219265 -4.4338217 -4.4213157 -4.397799 -4.3968821 -4.3889036 -4.3831439 -4.4176497][-4.5143576 -4.5550485 -4.5625725 -4.5202351 -4.4724512 -4.4664617 -4.4921942 -4.5162067 -4.5248971 -4.5255651 -4.5284576 -4.5446925 -4.5460324 -4.5377874 -4.5466218][-4.4584513 -4.4914908 -4.5118561 -4.5053949 -4.4904895 -4.4936371 -4.5105433 -4.5273714 -4.5384421 -4.5504231 -4.5589404 -4.5616693 -4.5508423 -4.5364294 -4.5326653]]...]
INFO - root - 2017-12-07 16:21:27.322855: step 30810, loss = 20.89, batch loss = 20.81 (8.9 examples/sec; 0.902 sec/batch; 75h:35m:57s remains)
INFO - root - 2017-12-07 16:21:36.789685: step 30820, loss = 21.63, batch loss = 21.54 (8.6 examples/sec; 0.935 sec/batch; 78h:19m:09s remains)
INFO - root - 2017-12-07 16:21:46.179557: step 30830, loss = 21.81, batch loss = 21.73 (8.4 examples/sec; 0.949 sec/batch; 79h:33m:26s remains)
INFO - root - 2017-12-07 16:21:55.660859: step 30840, loss = 21.61, batch loss = 21.52 (8.3 examples/sec; 0.961 sec/batch; 80h:30m:22s remains)
INFO - root - 2017-12-07 16:22:05.126727: step 30850, loss = 21.26, batch loss = 21.17 (8.3 examples/sec; 0.963 sec/batch; 80h:40m:29s remains)
INFO - root - 2017-12-07 16:22:14.658401: step 30860, loss = 21.61, batch loss = 21.53 (8.9 examples/sec; 0.902 sec/batch; 75h:32m:42s remains)
INFO - root - 2017-12-07 16:22:24.030597: step 30870, loss = 21.47, batch loss = 21.39 (8.0 examples/sec; 0.994 sec/batch; 83h:18m:40s remains)
INFO - root - 2017-12-07 16:22:33.241180: step 30880, loss = 21.76, batch loss = 21.67 (8.9 examples/sec; 0.898 sec/batch; 75h:12m:56s remains)
INFO - root - 2017-12-07 16:22:42.448653: step 30890, loss = 21.15, batch loss = 21.07 (9.2 examples/sec; 0.871 sec/batch; 72h:57m:45s remains)
INFO - root - 2017-12-07 16:22:51.877211: step 30900, loss = 21.78, batch loss = 21.70 (8.5 examples/sec; 0.938 sec/batch; 78h:33m:59s remains)
2017-12-07 16:22:52.825363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.14803 -4.1544061 -4.2354074 -4.3185463 -4.3520408 -4.3295751 -4.2835908 -4.2560534 -4.2701921 -4.3225784 -4.3877559 -4.4360838 -4.4468145 -4.434988 -4.4281797][-4.1308079 -4.1440248 -4.2267294 -4.3065238 -4.3325896 -4.2991157 -4.2416325 -4.2085867 -4.2262182 -4.2922788 -4.3732052 -4.4167843 -4.4059815 -4.3736768 -4.363441][-4.1574926 -4.1705742 -4.2343087 -4.2903852 -4.2976441 -4.2536039 -4.1881871 -4.1532207 -4.17327 -4.2504635 -4.3435707 -4.3828068 -4.3519158 -4.2993722 -4.281846][-4.1943407 -4.210535 -4.2558088 -4.2883639 -4.2775254 -4.2220569 -4.1463952 -4.10592 -4.12816 -4.2147918 -4.3134212 -4.3453732 -4.2949157 -4.2199793 -4.1910172][-4.2484374 -4.27116 -4.3071256 -4.3221021 -4.2951035 -4.2251005 -4.12742 -4.0629969 -4.07891 -4.1741343 -4.2785349 -4.3074646 -4.2470174 -4.1557155 -4.1139307][-4.316783 -4.3458586 -4.3745885 -4.367187 -4.3113046 -4.2153668 -4.0904026 -4.0001574 -4.0075779 -4.109437 -4.2210078 -4.2568645 -4.2039137 -4.1101408 -4.0598278][-4.3900084 -4.4204879 -4.4443197 -4.414258 -4.3233023 -4.1942954 -4.0462461 -3.9414454 -3.9415534 -4.0404015 -4.1528811 -4.1975479 -4.159317 -4.066153 -4.0055256][-4.4434714 -4.4692707 -4.4929681 -4.4598136 -4.3541789 -4.2064567 -4.0478058 -3.9356608 -3.9256668 -4.0119295 -4.1181588 -4.1678491 -4.1424074 -4.0511341 -3.9755025][-4.4603415 -4.480382 -4.5072618 -4.49032 -4.4007878 -4.2625928 -4.1141586 -4.0031395 -3.9765964 -4.0390477 -4.1294565 -4.1803331 -4.16747 -4.0863137 -4.0053911][-4.4444685 -4.4614978 -4.4936352 -4.5027008 -4.4500155 -4.3451633 -4.2238345 -4.1220255 -4.0809431 -4.1162972 -4.1845541 -4.2312961 -4.2249246 -4.1542311 -4.0755215][-4.4135 -4.4249206 -4.4595857 -4.49306 -4.4830179 -4.4199471 -4.3307281 -4.2454834 -4.2031779 -4.2226648 -4.2738419 -4.3163834 -4.3130269 -4.24824 -4.1735291][-4.3884511 -4.389739 -4.4187717 -4.4651322 -4.4906049 -4.4701552 -4.4155269 -4.3536949 -4.3216243 -4.3355823 -4.3751459 -4.4136376 -4.4132981 -4.3571143 -4.2902093][-4.3807173 -4.3738809 -4.39426 -4.4401307 -4.483654 -4.4944515 -4.4722457 -4.4387255 -4.4245391 -4.4404664 -4.4731979 -4.504612 -4.5025368 -4.4534693 -4.3959379][-4.3784733 -4.3687558 -4.3821707 -4.4194803 -4.46382 -4.4897037 -4.4926639 -4.486793 -4.4921489 -4.5137067 -4.5433631 -4.568265 -4.5638971 -4.5226655 -4.4760442][-4.377758 -4.3660483 -4.3729129 -4.3988662 -4.4339848 -4.4624872 -4.4790888 -4.4909654 -4.5071859 -4.528996 -4.5534773 -4.5734167 -4.5728564 -4.5480204 -4.5180078]]...]
INFO - root - 2017-12-07 16:23:02.101640: step 30910, loss = 21.74, batch loss = 21.65 (8.4 examples/sec; 0.951 sec/batch; 79h:41m:43s remains)
INFO - root - 2017-12-07 16:23:11.558756: step 30920, loss = 21.47, batch loss = 21.39 (8.3 examples/sec; 0.965 sec/batch; 80h:52m:46s remains)
INFO - root - 2017-12-07 16:23:20.854051: step 30930, loss = 21.14, batch loss = 21.05 (8.5 examples/sec; 0.939 sec/batch; 78h:37m:32s remains)
INFO - root - 2017-12-07 16:23:30.182663: step 30940, loss = 21.70, batch loss = 21.61 (9.2 examples/sec; 0.865 sec/batch; 72h:27m:02s remains)
INFO - root - 2017-12-07 16:23:39.621876: step 30950, loss = 21.45, batch loss = 21.37 (8.8 examples/sec; 0.907 sec/batch; 75h:56m:27s remains)
INFO - root - 2017-12-07 16:23:49.110402: step 30960, loss = 21.35, batch loss = 21.27 (8.4 examples/sec; 0.955 sec/batch; 79h:59m:58s remains)
INFO - root - 2017-12-07 16:23:58.536475: step 30970, loss = 21.75, batch loss = 21.66 (8.2 examples/sec; 0.981 sec/batch; 82h:08m:00s remains)
INFO - root - 2017-12-07 16:24:07.935784: step 30980, loss = 21.31, batch loss = 21.23 (7.9 examples/sec; 1.015 sec/batch; 84h:59m:30s remains)
INFO - root - 2017-12-07 16:24:17.262998: step 30990, loss = 21.29, batch loss = 21.21 (9.1 examples/sec; 0.879 sec/batch; 73h:36m:43s remains)
INFO - root - 2017-12-07 16:24:26.773080: step 31000, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.958 sec/batch; 80h:14m:53s remains)
2017-12-07 16:24:27.646633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3681426 -4.4252205 -4.5108624 -4.5988703 -4.6519508 -4.657568 -4.6331439 -4.6130862 -4.62416 -4.6629395 -4.6961589 -4.6891861 -4.6312423 -4.5408 -4.4496555][-4.382175 -4.4454489 -4.5351892 -4.6199603 -4.6619239 -4.6489363 -4.604331 -4.5680509 -4.5677409 -4.5977659 -4.6299396 -4.6384268 -4.6143851 -4.5574508 -4.4850664][-4.4234037 -4.4961858 -4.5817971 -4.6412516 -4.6423097 -4.5886135 -4.5190606 -4.4787507 -4.4883881 -4.5311666 -4.5765095 -4.604723 -4.609519 -4.5786157 -4.519794][-4.4667 -4.5416994 -4.60671 -4.6222715 -4.56789 -4.4694595 -4.3829451 -4.3554482 -4.3936415 -4.4658756 -4.5332017 -4.57376 -4.5904636 -4.5718403 -4.5223308][-4.4776793 -4.5366907 -4.5649433 -4.5330987 -4.4305158 -4.2919869 -4.1848145 -4.1625862 -4.2304993 -4.3474956 -4.4580727 -4.5277743 -4.5612407 -4.5490808 -4.497993][-4.4486165 -4.4891715 -4.4866872 -4.4226842 -4.282711 -4.0971441 -3.94122 -3.8945358 -3.9775994 -4.1415238 -4.310472 -4.4348168 -4.5103583 -4.5179205 -4.462997][-4.4095488 -4.4426041 -4.4252582 -4.3420372 -4.1719379 -3.9383307 -3.7298257 -3.652369 -3.7431443 -3.9386122 -4.1467481 -4.3194637 -4.4435663 -4.4813738 -4.4314218][-4.39056 -4.4317636 -4.41828 -4.332396 -4.1532025 -3.8973336 -3.6655517 -3.5749657 -3.66613 -3.8674853 -4.0854034 -4.2776175 -4.4240046 -4.4758439 -4.4294744][-4.3954768 -4.4539514 -4.4658327 -4.4100833 -4.2665925 -4.040894 -3.8269463 -3.7339306 -3.8013904 -3.9730067 -4.1724992 -4.3530054 -4.4863434 -4.5245614 -4.4699669][-4.39951 -4.4719906 -4.5176611 -4.5123339 -4.4319577 -4.269516 -4.0974188 -4.0080757 -4.0447526 -4.1776652 -4.3506374 -4.50954 -4.6121435 -4.6185827 -4.540102][-4.3992143 -4.4823184 -4.5626731 -4.6084523 -4.5868258 -4.4830632 -4.3511128 -4.2693119 -4.2854028 -4.3883944 -4.5367904 -4.6725173 -4.7436829 -4.7142005 -4.6028047][-4.4071431 -4.5017624 -4.6106353 -4.6918554 -4.7048635 -4.6356735 -4.5294352 -4.4554772 -4.4591722 -4.5396776 -4.6641431 -4.77638 -4.8219123 -4.7659397 -4.6318765][-4.4181252 -4.5159397 -4.6337647 -4.7249422 -4.7456369 -4.6849742 -4.5884109 -4.5213785 -4.5238547 -4.59608 -4.7053843 -4.80128 -4.8319397 -4.7678 -4.6332021][-4.4371424 -4.5283489 -4.6321363 -4.7029343 -4.7027283 -4.632618 -4.5424647 -4.4929662 -4.5105934 -4.5841808 -4.6778631 -4.7516346 -4.7712173 -4.7155414 -4.6039062][-4.4658775 -4.5438166 -4.6130581 -4.6346169 -4.5860991 -4.4889617 -4.3999548 -4.3723588 -4.4172459 -4.5060105 -4.5964952 -4.6549983 -4.6720152 -4.6334023 -4.5511284]]...]
INFO - root - 2017-12-07 16:24:37.006240: step 31010, loss = 21.37, batch loss = 21.29 (9.0 examples/sec; 0.891 sec/batch; 74h:35m:50s remains)
INFO - root - 2017-12-07 16:24:46.379682: step 31020, loss = 21.43, batch loss = 21.34 (8.7 examples/sec; 0.921 sec/batch; 77h:05m:46s remains)
INFO - root - 2017-12-07 16:24:55.726664: step 31030, loss = 21.61, batch loss = 21.53 (8.5 examples/sec; 0.941 sec/batch; 78h:47m:34s remains)
INFO - root - 2017-12-07 16:25:05.097969: step 31040, loss = 21.16, batch loss = 21.08 (8.0 examples/sec; 0.995 sec/batch; 83h:19m:32s remains)
INFO - root - 2017-12-07 16:25:14.444904: step 31050, loss = 21.36, batch loss = 21.28 (8.9 examples/sec; 0.895 sec/batch; 74h:54m:57s remains)
INFO - root - 2017-12-07 16:25:23.706545: step 31060, loss = 21.31, batch loss = 21.23 (8.4 examples/sec; 0.954 sec/batch; 79h:52m:19s remains)
INFO - root - 2017-12-07 16:25:33.131180: step 31070, loss = 21.60, batch loss = 21.52 (8.7 examples/sec; 0.920 sec/batch; 76h:59m:30s remains)
INFO - root - 2017-12-07 16:25:42.559824: step 31080, loss = 21.28, batch loss = 21.19 (8.4 examples/sec; 0.956 sec/batch; 80h:01m:20s remains)
INFO - root - 2017-12-07 16:25:51.979943: step 31090, loss = 21.67, batch loss = 21.59 (8.4 examples/sec; 0.955 sec/batch; 79h:56m:11s remains)
INFO - root - 2017-12-07 16:26:01.346149: step 31100, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.966 sec/batch; 80h:52m:32s remains)
2017-12-07 16:26:02.432132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4400835 -4.3574257 -4.2689261 -4.2172232 -4.184876 -4.1468306 -4.1546483 -4.228157 -4.3303933 -4.4281783 -4.4917269 -4.4925442 -4.4326987 -4.3359075 -4.2620864][-4.5201578 -4.4517865 -4.3787832 -4.341701 -4.3242822 -4.2899685 -4.2902408 -4.3580546 -4.464098 -4.5631752 -4.6173182 -4.60088 -4.514617 -4.3848963 -4.2905769][-4.6002808 -4.5602188 -4.5272512 -4.5251007 -4.5212975 -4.4665704 -4.420886 -4.4544673 -4.5667515 -4.6900668 -4.7595797 -4.7365813 -4.6231146 -4.4594116 -4.3392229][-4.6424427 -4.6269011 -4.6305752 -4.6601772 -4.6513186 -4.54167 -4.4123888 -4.3905139 -4.5165496 -4.7044144 -4.8444114 -4.86069 -4.7500639 -4.5699043 -4.427794][-4.6375251 -4.6315913 -4.6411176 -4.6644354 -4.6079779 -4.4045944 -4.1703277 -4.0881815 -4.2389412 -4.5206013 -4.7746749 -4.8809314 -4.8319044 -4.6877851 -4.5572248][-4.5936503 -4.5807304 -4.5702186 -4.5541043 -4.4278712 -4.1300611 -3.805974 -3.6804471 -3.8495162 -4.2056127 -4.5569124 -4.7554927 -4.7993035 -4.7303557 -4.6367874][-4.5550003 -4.5392647 -4.5142822 -4.47154 -4.3117924 -3.9773414 -3.6131113 -3.450947 -3.594871 -3.9575095 -4.3445611 -4.5957952 -4.7039342 -4.6927671 -4.6295605][-4.5173593 -4.5132389 -4.4925866 -4.4503822 -4.3084292 -4.01468 -3.6815767 -3.5015397 -3.5784492 -3.8769238 -4.2411585 -4.5051808 -4.636498 -4.6492114 -4.6009445][-4.4587622 -4.461916 -4.4467077 -4.4084296 -4.2991834 -4.0890989 -3.8536272 -3.7154694 -3.7554152 -3.983326 -4.2961121 -4.5349607 -4.6401944 -4.6341043 -4.5788851][-4.4119115 -4.4009628 -4.3862605 -4.3632674 -4.2968121 -4.1706042 -4.04456 -3.9848654 -4.0413108 -4.2228804 -4.4579053 -4.6218557 -4.6589675 -4.6097889 -4.5471406][-4.4022951 -4.3542342 -4.326438 -4.3294544 -4.3167667 -4.2667665 -4.2331886 -4.251276 -4.3406162 -4.4824276 -4.615243 -4.6715531 -4.6373878 -4.5645642 -4.5137095][-4.4202728 -4.3595657 -4.3261251 -4.3454037 -4.36913 -4.3707438 -4.3925619 -4.458704 -4.5649958 -4.6638174 -4.6949463 -4.6475687 -4.5639524 -4.5000544 -4.4766865][-4.4408355 -4.4066863 -4.3842812 -4.4006476 -4.4271755 -4.4475851 -4.48738 -4.5629296 -4.66377 -4.7294822 -4.6984448 -4.5884228 -4.4782853 -4.4318113 -4.4361992][-4.4440985 -4.4399042 -4.43272 -4.443563 -4.4681168 -4.5014544 -4.5438395 -4.6031852 -4.679348 -4.7240548 -4.6771283 -4.5494671 -4.4238949 -4.3789911 -4.3994884][-4.4635849 -4.4582787 -4.4550872 -4.4657288 -4.499373 -4.550983 -4.5953755 -4.6299882 -4.6707649 -4.6983805 -4.662931 -4.5531573 -4.4271207 -4.3651891 -4.372592]]...]
INFO - root - 2017-12-07 16:26:11.731762: step 31110, loss = 21.26, batch loss = 21.18 (9.0 examples/sec; 0.889 sec/batch; 74h:25m:05s remains)
INFO - root - 2017-12-07 16:26:21.000039: step 31120, loss = 21.07, batch loss = 20.99 (8.4 examples/sec; 0.954 sec/batch; 79h:53m:21s remains)
INFO - root - 2017-12-07 16:26:30.176471: step 31130, loss = 21.28, batch loss = 21.20 (8.3 examples/sec; 0.959 sec/batch; 80h:17m:51s remains)
INFO - root - 2017-12-07 16:26:39.621469: step 31140, loss = 21.61, batch loss = 21.53 (8.8 examples/sec; 0.912 sec/batch; 76h:21m:53s remains)
INFO - root - 2017-12-07 16:26:48.935737: step 31150, loss = 21.69, batch loss = 21.61 (9.0 examples/sec; 0.891 sec/batch; 74h:34m:08s remains)
INFO - root - 2017-12-07 16:26:58.328314: step 31160, loss = 21.95, batch loss = 21.87 (8.2 examples/sec; 0.972 sec/batch; 81h:22m:29s remains)
INFO - root - 2017-12-07 16:27:07.646984: step 31170, loss = 21.70, batch loss = 21.62 (8.7 examples/sec; 0.919 sec/batch; 76h:53m:23s remains)
INFO - root - 2017-12-07 16:27:17.013315: step 31180, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.930 sec/batch; 77h:47m:59s remains)
INFO - root - 2017-12-07 16:27:26.493337: step 31190, loss = 21.48, batch loss = 21.40 (8.6 examples/sec; 0.925 sec/batch; 77h:25m:17s remains)
INFO - root - 2017-12-07 16:27:35.698973: step 31200, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.942 sec/batch; 78h:51m:57s remains)
2017-12-07 16:27:36.626609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5579085 -4.585022 -4.6025567 -4.6042562 -4.5922084 -4.5685968 -4.5583158 -4.5895591 -4.670835 -4.7714162 -4.83586 -4.8139944 -4.7341957 -4.6640511 -4.5940909][-4.4882526 -4.5211082 -4.5401845 -4.53571 -4.5196948 -4.4977322 -4.5031037 -4.5737491 -4.6964288 -4.8190813 -4.8834052 -4.8434 -4.7334023 -4.641942 -4.5663252][-4.4621868 -4.5256462 -4.5546927 -4.5368571 -4.4986334 -4.4495239 -4.4368124 -4.5160675 -4.6551633 -4.7770691 -4.8291478 -4.7850118 -4.6663914 -4.5649881 -4.5005441][-4.4729257 -4.5831361 -4.6357265 -4.6098857 -4.5321865 -4.41242 -4.3325791 -4.38676 -4.5329943 -4.65661 -4.7019053 -4.6722717 -4.5681052 -4.4634056 -4.4132953][-4.4803829 -4.6228318 -4.6868911 -4.6436105 -4.5117707 -4.2955112 -4.1202331 -4.1430621 -4.3293719 -4.4987755 -4.5671473 -4.5672126 -4.4860134 -4.3707113 -4.3146167][-4.4514718 -4.5921497 -4.6366758 -4.5556812 -4.3656521 -4.05984 -3.7966156 -3.8022838 -4.0651064 -4.326859 -4.4549942 -4.495429 -4.4394608 -4.3100333 -4.226645][-4.4241176 -4.5432348 -4.547565 -4.4156165 -4.1723881 -3.8078644 -3.4874544 -3.4789426 -3.8065832 -4.16654 -4.3725471 -4.4608936 -4.4283428 -4.2929626 -4.1860342][-4.4471216 -4.5429583 -4.5216894 -4.3615351 -4.0898256 -3.7098534 -3.3696308 -3.338567 -3.6764965 -4.0876346 -4.3519216 -4.4685268 -4.4470663 -4.3231297 -4.2216063][-4.5076952 -4.6010551 -4.5981488 -4.46403 -4.2132554 -3.8681688 -3.5515954 -3.4930928 -3.7691908 -4.1482258 -4.4129882 -4.5214076 -4.5008497 -4.4097424 -4.3451214][-4.5553894 -4.6475792 -4.6819654 -4.6126447 -4.4336352 -4.1691971 -3.9163287 -3.8474522 -4.0357461 -4.3273911 -4.537549 -4.606771 -4.5831056 -4.5324669 -4.5127516][-4.5579281 -4.631371 -4.6918941 -4.6924725 -4.6139565 -4.4604392 -4.3000178 -4.2485271 -4.3596568 -4.5383162 -4.6556325 -4.669548 -4.6410074 -4.620563 -4.6298351][-4.5654526 -4.603261 -4.6597152 -4.700182 -4.7036042 -4.6541739 -4.5848112 -4.5618839 -4.614337 -4.6901183 -4.7196441 -4.6892223 -4.6578259 -4.653532 -4.6742291][-4.5912914 -4.5983396 -4.6327796 -4.6844082 -4.731451 -4.7492261 -4.739315 -4.72834 -4.73428 -4.7400384 -4.72082 -4.6731234 -4.6409626 -4.6359644 -4.6518159][-4.5651269 -4.5642385 -4.592495 -4.6519189 -4.7179523 -4.7621803 -4.76894 -4.7475061 -4.7208467 -4.6968594 -4.668642 -4.6276832 -4.5952067 -4.5802336 -4.5853572][-4.4535441 -4.4576793 -4.5020356 -4.5722313 -4.6370792 -4.6789894 -4.6862664 -4.6617365 -4.6307955 -4.6087766 -4.5913324 -4.562706 -4.5269814 -4.4986553 -4.4934487]]...]
INFO - root - 2017-12-07 16:27:46.048692: step 31210, loss = 21.39, batch loss = 21.31 (8.6 examples/sec; 0.933 sec/batch; 78h:06m:42s remains)
INFO - root - 2017-12-07 16:27:55.254350: step 31220, loss = 21.32, batch loss = 21.23 (8.4 examples/sec; 0.954 sec/batch; 79h:49m:35s remains)
INFO - root - 2017-12-07 16:28:04.390664: step 31230, loss = 21.47, batch loss = 21.39 (8.1 examples/sec; 0.989 sec/batch; 82h:45m:26s remains)
INFO - root - 2017-12-07 16:28:13.790437: step 31240, loss = 20.98, batch loss = 20.90 (8.1 examples/sec; 0.982 sec/batch; 82h:09m:00s remains)
INFO - root - 2017-12-07 16:28:23.204683: step 31250, loss = 21.63, batch loss = 21.55 (8.4 examples/sec; 0.951 sec/batch; 79h:35m:40s remains)
INFO - root - 2017-12-07 16:28:32.635529: step 31260, loss = 21.74, batch loss = 21.66 (8.3 examples/sec; 0.965 sec/batch; 80h:44m:49s remains)
INFO - root - 2017-12-07 16:28:42.143277: step 31270, loss = 21.11, batch loss = 21.03 (8.4 examples/sec; 0.957 sec/batch; 80h:03m:32s remains)
INFO - root - 2017-12-07 16:28:51.427409: step 31280, loss = 21.15, batch loss = 21.07 (8.8 examples/sec; 0.906 sec/batch; 75h:46m:37s remains)
INFO - root - 2017-12-07 16:29:00.710276: step 31290, loss = 21.28, batch loss = 21.19 (9.0 examples/sec; 0.885 sec/batch; 74h:00m:20s remains)
INFO - root - 2017-12-07 16:29:10.175246: step 31300, loss = 21.57, batch loss = 21.49 (9.1 examples/sec; 0.881 sec/batch; 73h:41m:25s remains)
2017-12-07 16:29:11.099356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.285202 -4.2799306 -4.2516413 -4.2120805 -4.187108 -4.1910467 -4.2176938 -4.2590995 -4.319828 -4.3858995 -4.4061661 -4.3550329 -4.2442913 -4.105947 -4.0056276][-4.1247611 -4.1283064 -4.1316075 -4.1300764 -4.1316357 -4.1394725 -4.1569257 -4.1917558 -4.2543035 -4.3376684 -4.386497 -4.3505855 -4.2376461 -4.08331 -3.9663484][-4.0347948 -4.0474668 -4.0766287 -4.1091576 -4.1377077 -4.1488419 -4.1468415 -4.1558571 -4.2006407 -4.2939396 -4.3789225 -4.3841333 -4.3036671 -4.1670494 -4.0539603][-4.0395222 -4.0571346 -4.096302 -4.1432586 -4.1868854 -4.198977 -4.1726966 -4.1414452 -4.1553903 -4.2543335 -4.3797421 -4.4475365 -4.4274683 -4.3306465 -4.2346468][-4.1329832 -4.1407452 -4.1699162 -4.2089968 -4.2423887 -4.2298746 -4.1574621 -4.071167 -4.0528703 -4.1620159 -4.3319392 -4.465559 -4.5076375 -4.4544849 -4.3821473][-4.2287831 -4.22325 -4.2357173 -4.2581735 -4.2638712 -4.2040486 -4.0684829 -3.9249489 -3.8869326 -4.0185895 -4.2338619 -4.4181433 -4.4987941 -4.4669895 -4.405695][-4.28351 -4.269968 -4.27007 -4.2781897 -4.2555833 -4.1452751 -3.9457741 -3.7539585 -3.7036996 -3.8553104 -4.1032934 -4.3175278 -4.4185653 -4.3925934 -4.3286681][-4.3396831 -4.3105907 -4.291245 -4.2844582 -4.2424631 -4.098825 -3.8591814 -3.6413505 -3.5835831 -3.7404609 -3.9981246 -4.2206697 -4.3265343 -4.3009114 -4.2328439][-4.3769474 -4.3266525 -4.2907414 -4.2832556 -4.2482877 -4.1127763 -3.8860617 -3.6847706 -3.6317029 -3.7697814 -3.997926 -4.1912374 -4.273819 -4.2329326 -4.1537633][-4.407177 -4.3395429 -4.2968326 -4.3005228 -4.2907057 -4.1954794 -4.0304151 -3.8847446 -3.8447704 -3.9427667 -4.1090355 -4.2429972 -4.2789216 -4.2085323 -4.1125579][-4.4283109 -4.3451877 -4.2984509 -4.3119955 -4.326498 -4.2812929 -4.1934071 -4.1164484 -4.093008 -4.1455164 -4.2397084 -4.3065858 -4.2973356 -4.204371 -4.1016593][-4.3898959 -4.3061848 -4.2663889 -4.2902131 -4.3230205 -4.315063 -4.2869458 -4.2633567 -4.2527065 -4.2660704 -4.2971277 -4.3110266 -4.277154 -4.1859741 -4.092741][-4.3132463 -4.2459245 -4.2189317 -4.2468824 -4.2835522 -4.2904305 -4.2871675 -4.2820926 -4.2730308 -4.2665076 -4.2670035 -4.2605896 -4.228858 -4.159308 -4.0885649][-4.2541203 -4.2060738 -4.1891646 -4.214797 -4.2478619 -4.2582507 -4.2573996 -4.2484961 -4.2333775 -4.2185545 -4.2078114 -4.1972728 -4.1744232 -4.1286268 -4.0881205][-4.2452526 -4.2144957 -4.2063193 -4.2285037 -4.2577024 -4.2685905 -4.2610884 -4.2405834 -4.2192278 -4.2076483 -4.1999931 -4.1858644 -4.1634126 -4.1346693 -4.1230974]]...]
INFO - root - 2017-12-07 16:29:20.380388: step 31310, loss = 21.30, batch loss = 21.21 (9.0 examples/sec; 0.892 sec/batch; 74h:36m:48s remains)
INFO - root - 2017-12-07 16:29:29.652964: step 31320, loss = 22.02, batch loss = 21.93 (9.0 examples/sec; 0.886 sec/batch; 74h:08m:11s remains)
INFO - root - 2017-12-07 16:29:39.066783: step 31330, loss = 21.56, batch loss = 21.47 (8.2 examples/sec; 0.973 sec/batch; 81h:25m:27s remains)
INFO - root - 2017-12-07 16:29:48.535893: step 31340, loss = 21.60, batch loss = 21.52 (8.8 examples/sec; 0.913 sec/batch; 76h:24m:53s remains)
INFO - root - 2017-12-07 16:29:57.911458: step 31350, loss = 21.63, batch loss = 21.55 (8.8 examples/sec; 0.905 sec/batch; 75h:44m:00s remains)
INFO - root - 2017-12-07 16:30:07.134477: step 31360, loss = 21.41, batch loss = 21.33 (8.5 examples/sec; 0.940 sec/batch; 78h:35m:33s remains)
INFO - root - 2017-12-07 16:30:16.593984: step 31370, loss = 21.18, batch loss = 21.10 (8.8 examples/sec; 0.906 sec/batch; 75h:46m:41s remains)
INFO - root - 2017-12-07 16:30:26.097629: step 31380, loss = 21.75, batch loss = 21.66 (8.5 examples/sec; 0.945 sec/batch; 79h:01m:07s remains)
INFO - root - 2017-12-07 16:30:35.625212: step 31390, loss = 21.73, batch loss = 21.65 (8.6 examples/sec; 0.932 sec/batch; 77h:59m:14s remains)
INFO - root - 2017-12-07 16:30:44.919330: step 31400, loss = 21.38, batch loss = 21.30 (9.4 examples/sec; 0.853 sec/batch; 71h:20m:59s remains)
2017-12-07 16:30:45.818413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2528906 -4.3081803 -4.3986382 -4.4834304 -4.5300288 -4.5346222 -4.526937 -4.5296431 -4.5843992 -4.669385 -4.7221804 -4.7582378 -4.802074 -4.8250551 -4.7840977][-4.2890816 -4.3529029 -4.4368124 -4.5102744 -4.5519066 -4.5560722 -4.53581 -4.5103154 -4.53746 -4.6131005 -4.6705589 -4.7107296 -4.7497306 -4.7580252 -4.693481][-4.3265996 -4.4042006 -4.486917 -4.5470023 -4.5732317 -4.5677834 -4.5335259 -4.4794655 -4.4791856 -4.5460072 -4.6078615 -4.650713 -4.681776 -4.6765218 -4.6001258][-4.3508177 -4.4363151 -4.5112371 -4.5478172 -4.5396261 -4.5074959 -4.4579377 -4.3917828 -4.3950748 -4.4872842 -4.5735054 -4.6265535 -4.6473632 -4.6216073 -4.5337076][-4.365603 -4.4462819 -4.4976821 -4.4948936 -4.4351892 -4.3581991 -4.286603 -4.22292 -4.2619276 -4.411221 -4.53633 -4.6013079 -4.6106825 -4.5630355 -4.4689932][-4.3808985 -4.4494972 -4.4662848 -4.4141011 -4.2988329 -4.1726427 -4.073523 -4.0120087 -4.0939908 -4.3090858 -4.4770713 -4.557096 -4.5613689 -4.49691 -4.4026442][-4.3976054 -4.4578185 -4.4424858 -4.34359 -4.1818557 -4.0146875 -3.8910315 -3.8249416 -3.934639 -4.2056208 -4.4182787 -4.5176058 -4.5237813 -4.4447093 -4.3430123][-4.4033184 -4.467298 -4.4430704 -4.3260183 -4.1416326 -3.9478347 -3.805742 -3.7259619 -3.8295302 -4.124166 -4.3743148 -4.4939137 -4.5030375 -4.4070892 -4.2900481][-4.392262 -4.4732161 -4.4722166 -4.3812494 -4.2144623 -4.0209231 -3.8743026 -3.7784724 -3.8415511 -4.105979 -4.3579669 -4.4782529 -4.4834504 -4.3781524 -4.2497187][-4.3783121 -4.48188 -4.522841 -4.4842515 -4.3671093 -4.1988158 -4.0590687 -3.9516757 -3.967412 -4.1666412 -4.3713336 -4.4561152 -4.4456544 -4.3473072 -4.2282186][-4.3714089 -4.4951224 -4.5756712 -4.5858827 -4.5175462 -4.3779707 -4.2438354 -4.1307168 -4.1158643 -4.2492127 -4.383431 -4.4132528 -4.3773661 -4.2889524 -4.194026][-4.3653746 -4.5015783 -4.6114249 -4.6584039 -4.628037 -4.51319 -4.3787537 -4.2601595 -4.227931 -4.3087149 -4.3772721 -4.3556552 -4.2942696 -4.2125206 -4.1402516][-4.3526525 -4.4911275 -4.6172271 -4.6875577 -4.6854911 -4.5944457 -4.4647808 -4.3507671 -4.3138943 -4.3613744 -4.3900142 -4.338654 -4.2583046 -4.1756248 -4.11268][-4.3400955 -4.4658074 -4.5923371 -4.6726456 -4.6898818 -4.6272469 -4.51963 -4.4290056 -4.4067264 -4.44643 -4.465199 -4.41155 -4.3265643 -4.2415361 -4.1768494][-4.3227654 -4.426753 -4.542057 -4.6242361 -4.656755 -4.6251063 -4.5537238 -4.499476 -4.5019164 -4.5484295 -4.574059 -4.5384226 -4.4696407 -4.3969674 -4.336287]]...]
INFO - root - 2017-12-07 16:30:55.330715: step 31410, loss = 20.96, batch loss = 20.88 (8.5 examples/sec; 0.945 sec/batch; 79h:01m:23s remains)
INFO - root - 2017-12-07 16:31:04.612923: step 31420, loss = 21.50, batch loss = 21.42 (8.6 examples/sec; 0.933 sec/batch; 78h:00m:10s remains)
INFO - root - 2017-12-07 16:31:13.843609: step 31430, loss = 21.28, batch loss = 21.20 (8.6 examples/sec; 0.928 sec/batch; 77h:38m:36s remains)
INFO - root - 2017-12-07 16:31:23.231321: step 31440, loss = 21.33, batch loss = 21.25 (8.1 examples/sec; 0.992 sec/batch; 82h:55m:31s remains)
INFO - root - 2017-12-07 16:31:32.558167: step 31450, loss = 21.29, batch loss = 21.20 (8.1 examples/sec; 0.992 sec/batch; 82h:57m:16s remains)
INFO - root - 2017-12-07 16:31:41.821189: step 31460, loss = 21.47, batch loss = 21.39 (8.1 examples/sec; 0.990 sec/batch; 82h:45m:20s remains)
INFO - root - 2017-12-07 16:31:51.215813: step 31470, loss = 21.35, batch loss = 21.27 (8.4 examples/sec; 0.950 sec/batch; 79h:26m:54s remains)
INFO - root - 2017-12-07 16:32:00.660150: step 31480, loss = 21.23, batch loss = 21.15 (8.3 examples/sec; 0.966 sec/batch; 80h:46m:47s remains)
INFO - root - 2017-12-07 16:32:10.127807: step 31490, loss = 21.54, batch loss = 21.46 (8.4 examples/sec; 0.948 sec/batch; 79h:15m:37s remains)
INFO - root - 2017-12-07 16:32:19.582530: step 31500, loss = 21.40, batch loss = 21.32 (8.6 examples/sec; 0.932 sec/batch; 77h:53m:35s remains)
2017-12-07 16:32:20.506957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3272882 -4.3169117 -4.3214068 -4.324811 -4.3175311 -4.2978196 -4.2763638 -4.25866 -4.2385406 -4.2253518 -4.2019653 -4.162 -4.1437044 -4.1614614 -4.2195926][-4.3449922 -4.3380909 -4.3380203 -4.3342061 -4.3243532 -4.3029141 -4.2834077 -4.2716331 -4.2587743 -4.2554269 -4.24266 -4.2170219 -4.208375 -4.2334843 -4.2896514][-4.363328 -4.365294 -4.3699703 -4.3694696 -4.3626375 -4.3423986 -4.3242307 -4.3138461 -4.306643 -4.3128734 -4.3140855 -4.3053956 -4.3059759 -4.3314834 -4.3742642][-4.3372235 -4.3536096 -4.3717556 -4.3851557 -4.38858 -4.37565 -4.3603458 -4.3493795 -4.3456841 -4.3593259 -4.3738422 -4.3786221 -4.3829536 -4.4022961 -4.4288979][-4.2591882 -4.2885728 -4.3198752 -4.3476081 -4.3615117 -4.3565583 -4.3441892 -4.3329015 -4.333744 -4.3532958 -4.3746071 -4.3821568 -4.3803382 -4.3890214 -4.4062963][-4.190557 -4.2222929 -4.2530994 -4.2767534 -4.2829494 -4.2717295 -4.2517138 -4.2372732 -4.2444587 -4.2726197 -4.2992439 -4.3045135 -4.2948942 -4.2970638 -4.3165407][-4.1455569 -4.1658587 -4.1835318 -4.1892667 -4.1767364 -4.149838 -4.1159 -4.0948777 -4.1051245 -4.1375837 -4.1634655 -4.1646791 -4.1563358 -4.1687517 -4.209281][-4.1209269 -4.1257577 -4.1252956 -4.1109142 -4.0809321 -4.0411444 -3.9986806 -3.974354 -3.985347 -4.0153718 -4.0336795 -4.0299778 -4.0315266 -4.0665593 -4.1376181][-4.1729832 -4.1640253 -4.1447277 -4.1121459 -4.071857 -4.0314875 -3.9958241 -3.9805205 -3.9947212 -4.0197334 -4.0264931 -4.0151854 -4.0230684 -4.0711384 -4.154099][-4.2542305 -4.2469487 -4.2269063 -4.1969481 -4.1641822 -4.1350093 -4.1129951 -4.1083694 -4.1226172 -4.1405096 -4.1384311 -4.1200514 -4.1249809 -4.1669679 -4.2359519][-4.3492627 -4.3467097 -4.3313112 -4.3091769 -4.285624 -4.2645788 -4.2505198 -4.2489562 -4.2589107 -4.2719293 -4.2679558 -4.2453117 -4.238224 -4.2569847 -4.2960234][-4.4147396 -4.4162822 -4.4076128 -4.3945889 -4.3805447 -4.3682442 -4.3597956 -4.3570261 -4.3614497 -4.3707013 -4.3668871 -4.3395643 -4.3164077 -4.3068471 -4.3160887][-4.4340906 -4.4318304 -4.4227309 -4.4117022 -4.4021587 -4.3960052 -4.3927007 -4.3906384 -4.3937144 -4.4038725 -4.4038472 -4.37934 -4.3558192 -4.3407869 -4.3444686][-4.443821 -4.4265833 -4.4118595 -4.4008484 -4.39449 -4.3907571 -4.3881845 -4.3869295 -4.3931074 -4.4094062 -4.4173293 -4.4017954 -4.387176 -4.3773108 -4.3819675][-4.4310069 -4.4066663 -4.3898611 -4.3829927 -4.3824334 -4.3812985 -4.3780422 -4.3765574 -4.3853407 -4.4063344 -4.4200144 -4.4118543 -4.40322 -4.3969383 -4.399642]]...]
INFO - root - 2017-12-07 16:32:29.922248: step 31510, loss = 21.14, batch loss = 21.05 (8.7 examples/sec; 0.919 sec/batch; 76h:50m:21s remains)
INFO - root - 2017-12-07 16:32:39.408432: step 31520, loss = 20.84, batch loss = 20.76 (9.3 examples/sec; 0.864 sec/batch; 72h:13m:15s remains)
INFO - root - 2017-12-07 16:32:48.673265: step 31530, loss = 21.40, batch loss = 21.32 (8.5 examples/sec; 0.943 sec/batch; 78h:51m:59s remains)
INFO - root - 2017-12-07 16:32:57.984505: step 31540, loss = 21.37, batch loss = 21.28 (8.7 examples/sec; 0.916 sec/batch; 76h:36m:31s remains)
INFO - root - 2017-12-07 16:33:07.250047: step 31550, loss = 21.42, batch loss = 21.33 (9.0 examples/sec; 0.891 sec/batch; 74h:29m:36s remains)
INFO - root - 2017-12-07 16:33:16.633891: step 31560, loss = 21.40, batch loss = 21.31 (8.7 examples/sec; 0.919 sec/batch; 76h:51m:32s remains)
INFO - root - 2017-12-07 16:33:26.159989: step 31570, loss = 22.10, batch loss = 22.02 (8.6 examples/sec; 0.935 sec/batch; 78h:10m:22s remains)
INFO - root - 2017-12-07 16:33:35.329222: step 31580, loss = 21.37, batch loss = 21.28 (8.3 examples/sec; 0.963 sec/batch; 80h:30m:47s remains)
INFO - root - 2017-12-07 16:33:44.741923: step 31590, loss = 21.67, batch loss = 21.59 (8.0 examples/sec; 0.996 sec/batch; 83h:17m:22s remains)
INFO - root - 2017-12-07 16:33:54.114174: step 31600, loss = 21.45, batch loss = 21.37 (8.7 examples/sec; 0.920 sec/batch; 76h:53m:03s remains)
2017-12-07 16:33:55.057489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7678504 -4.791749 -4.7858925 -4.7749357 -4.7415471 -4.698338 -4.6993318 -4.7579093 -4.7999296 -4.7690754 -4.6873031 -4.6346436 -4.6408725 -4.662528 -4.6539774][-4.7308035 -4.7841158 -4.8066173 -4.8098049 -4.7819643 -4.7427464 -4.7383747 -4.7766743 -4.7787175 -4.6914806 -4.5642328 -4.5080538 -4.5495615 -4.6020985 -4.5866957][-4.6447334 -4.7198048 -4.7573204 -4.7592249 -4.7125373 -4.6373472 -4.5854955 -4.5771465 -4.5503478 -4.4572058 -4.3440852 -4.3165832 -4.3940072 -4.4729981 -4.456984][-4.5518718 -4.6382895 -4.67083 -4.6484752 -4.5555716 -4.4075656 -4.2674 -4.1959076 -4.1712689 -4.1435323 -4.1105013 -4.134542 -4.2326632 -4.3242159 -4.3177352][-4.4995146 -4.5759339 -4.5862317 -4.5289369 -4.3829579 -4.1536078 -3.913641 -3.7774167 -3.7773778 -3.8547888 -3.9315896 -4.0088239 -4.1117654 -4.2068958 -4.2231593][-4.4855261 -4.5311036 -4.5128036 -4.428813 -4.2517748 -3.970854 -3.6507468 -3.4566636 -3.4786119 -3.6493661 -3.8224034 -3.9440935 -4.0469456 -4.1468129 -4.195303][-4.5025959 -4.5144691 -4.4781389 -4.3867769 -4.2079334 -3.9121649 -3.548593 -3.3108475 -3.3366199 -3.5569303 -3.7874813 -3.9422717 -4.0556912 -4.1669054 -4.2436428][-4.4985275 -4.4885378 -4.4517622 -4.3813033 -4.240416 -3.9821889 -3.6322687 -3.3839874 -3.3999956 -3.6209507 -3.8670304 -4.0461459 -4.1803761 -4.298564 -4.3735003][-4.4360805 -4.4188108 -4.4041243 -4.3831906 -4.3109689 -4.129282 -3.843781 -3.6280112 -3.6414113 -3.8322546 -4.058454 -4.2408776 -4.3815379 -4.484621 -4.5224614][-4.3408346 -4.3388886 -4.366539 -4.4058952 -4.4020643 -4.3005795 -4.10386 -3.9524021 -3.970897 -4.1130271 -4.2874732 -4.4425364 -4.5616989 -4.6232915 -4.6030703][-4.2573695 -4.2846222 -4.353219 -4.4363174 -4.4803543 -4.4477549 -4.3443546 -4.2685523 -4.2951417 -4.3825526 -4.4914632 -4.59872 -4.6712184 -4.6700754 -4.5847034][-4.1976905 -4.24518 -4.3349714 -4.4410944 -4.5147243 -4.5337892 -4.507247 -4.4970555 -4.5323343 -4.574266 -4.6189828 -4.671001 -4.6878572 -4.6197858 -4.4747438][-4.1766415 -4.2332444 -4.3248811 -4.4297452 -4.5074854 -4.5488563 -4.565022 -4.5914869 -4.6281128 -4.6371007 -4.632741 -4.6365051 -4.6036773 -4.4783731 -4.287858][-4.2208986 -4.2773433 -4.3461471 -4.4195371 -4.4715619 -4.4991841 -4.5174646 -4.5516338 -4.5860395 -4.5824256 -4.5547428 -4.5258307 -4.4536724 -4.2893 -4.0781183][-4.3419733 -4.3785796 -4.4036064 -4.422719 -4.4263792 -4.4156661 -4.4085116 -4.4294934 -4.4580588 -4.45261 -4.4191165 -4.3743968 -4.2819381 -4.1085606 -3.9067283]]...]
INFO - root - 2017-12-07 16:34:04.538758: step 31610, loss = 21.39, batch loss = 21.31 (9.3 examples/sec; 0.865 sec/batch; 72h:17m:00s remains)
INFO - root - 2017-12-07 16:34:13.925744: step 31620, loss = 21.27, batch loss = 21.19 (8.9 examples/sec; 0.902 sec/batch; 75h:24m:59s remains)
INFO - root - 2017-12-07 16:34:23.184560: step 31630, loss = 21.82, batch loss = 21.74 (8.8 examples/sec; 0.911 sec/batch; 76h:09m:41s remains)
INFO - root - 2017-12-07 16:34:32.533738: step 31640, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.914 sec/batch; 76h:21m:56s remains)
INFO - root - 2017-12-07 16:34:42.007209: step 31650, loss = 20.85, batch loss = 20.77 (8.4 examples/sec; 0.955 sec/batch; 79h:46m:24s remains)
INFO - root - 2017-12-07 16:34:51.578914: step 31660, loss = 21.45, batch loss = 21.37 (8.2 examples/sec; 0.981 sec/batch; 81h:59m:11s remains)
INFO - root - 2017-12-07 16:35:00.920042: step 31670, loss = 21.59, batch loss = 21.51 (8.8 examples/sec; 0.914 sec/batch; 76h:21m:15s remains)
INFO - root - 2017-12-07 16:35:10.365441: step 31680, loss = 21.68, batch loss = 21.60 (8.6 examples/sec; 0.931 sec/batch; 77h:46m:35s remains)
INFO - root - 2017-12-07 16:35:19.623522: step 31690, loss = 21.63, batch loss = 21.55 (8.9 examples/sec; 0.895 sec/batch; 74h:48m:16s remains)
INFO - root - 2017-12-07 16:35:29.149558: step 31700, loss = 21.61, batch loss = 21.53 (8.3 examples/sec; 0.960 sec/batch; 80h:14m:54s remains)
2017-12-07 16:35:30.051425: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3194327 -4.3523803 -4.4318871 -4.5302773 -4.6083384 -4.6468539 -4.6462183 -4.6262722 -4.6191635 -4.64695 -4.6937971 -4.7381105 -4.7671404 -4.7656488 -4.7423868][-4.3035207 -4.3391805 -4.4246411 -4.5190544 -4.5813546 -4.6042032 -4.5874057 -4.5443826 -4.519114 -4.5587993 -4.651278 -4.7536254 -4.835556 -4.8766804 -4.8801527][-4.2933273 -4.3324075 -4.4172087 -4.4945259 -4.5271883 -4.5224409 -4.4820213 -4.4139519 -4.3678875 -4.4079933 -4.5271072 -4.670063 -4.8023272 -4.9036627 -4.9512944][-4.2874413 -4.3314915 -4.4146767 -4.4746318 -4.47332 -4.4296274 -4.3585658 -4.2706614 -4.2098293 -4.2451806 -4.369669 -4.52227 -4.6790032 -4.8300829 -4.9228392][-4.2857323 -4.3331881 -4.4135208 -4.4575858 -4.4220786 -4.337038 -4.2358809 -4.1330137 -4.0659914 -4.1076989 -4.2413177 -4.3869324 -4.5347095 -4.7016187 -4.8260288][-4.2883325 -4.3333278 -4.4042225 -4.4306116 -4.3668036 -4.2492251 -4.1262932 -4.0176015 -3.9551983 -4.0120716 -4.1634336 -4.3065891 -4.4324746 -4.5886207 -4.7291422][-4.2969594 -4.3344159 -4.3910775 -4.4005275 -4.3230147 -4.1935334 -4.0643435 -3.9602034 -3.9039297 -3.955909 -4.101809 -4.2428045 -4.3578997 -4.4977083 -4.6491742][-4.3060679 -4.3366561 -4.3825917 -4.3862286 -4.3142991 -4.1930652 -4.0711274 -3.9781408 -3.9256253 -3.9517164 -4.0619178 -4.1912827 -4.3009486 -4.4144487 -4.5567169][-4.3107686 -4.3365068 -4.3793683 -4.3919616 -4.3437095 -4.2478647 -4.1430359 -4.0562329 -3.9973049 -3.9910727 -4.0566239 -4.1627336 -4.2588415 -4.3292532 -4.4303622][-4.3097329 -4.3306174 -4.3743215 -4.40176 -4.3846784 -4.3278 -4.2534189 -4.1716509 -4.0980968 -4.0642729 -4.0914526 -4.1613975 -4.2244544 -4.2410622 -4.2830744][-4.3060484 -4.3189764 -4.3603177 -4.3984632 -4.4107718 -4.3986535 -4.3666396 -4.3000445 -4.2127986 -4.15658 -4.1484408 -4.1654406 -4.1703315 -4.1292777 -4.1157103][-4.3009949 -4.3094406 -4.3471022 -4.3913407 -4.4269 -4.4549928 -4.4679575 -4.4291492 -4.3428149 -4.2688713 -4.2254124 -4.1797996 -4.1102557 -4.018012 -3.9821572][-4.2978582 -4.303937 -4.3383503 -4.3850589 -4.4348021 -4.490581 -4.538271 -4.5314207 -4.4636397 -4.3907804 -4.3316689 -4.2494636 -4.1241374 -3.9994211 -3.9725487][-4.2966285 -4.3005195 -4.3299789 -4.3733783 -4.4266987 -4.49475 -4.5606222 -4.5781584 -4.5366645 -4.486269 -4.4457173 -4.3747106 -4.2471609 -4.1261063 -4.1184382][-4.2992239 -4.3007607 -4.3249507 -4.3615956 -4.4105296 -4.4774241 -4.5470562 -4.5809126 -4.5690293 -4.5513434 -4.5454769 -4.5146132 -4.4284668 -4.3432312 -4.3537478]]...]
INFO - root - 2017-12-07 16:35:39.518191: step 31710, loss = 21.37, batch loss = 21.29 (8.1 examples/sec; 0.983 sec/batch; 82h:06m:52s remains)
INFO - root - 2017-12-07 16:35:48.867082: step 31720, loss = 21.51, batch loss = 21.43 (8.9 examples/sec; 0.895 sec/batch; 74h:48m:07s remains)
INFO - root - 2017-12-07 16:35:58.188556: step 31730, loss = 21.22, batch loss = 21.14 (8.4 examples/sec; 0.949 sec/batch; 79h:16m:00s remains)
INFO - root - 2017-12-07 16:36:07.586312: step 31740, loss = 21.24, batch loss = 21.16 (8.2 examples/sec; 0.971 sec/batch; 81h:09m:05s remains)
INFO - root - 2017-12-07 16:36:16.853331: step 31750, loss = 21.54, batch loss = 21.45 (8.3 examples/sec; 0.964 sec/batch; 80h:33m:32s remains)
INFO - root - 2017-12-07 16:36:26.185414: step 31760, loss = 21.31, batch loss = 21.22 (8.7 examples/sec; 0.918 sec/batch; 76h:42m:34s remains)
INFO - root - 2017-12-07 16:36:35.551336: step 31770, loss = 21.09, batch loss = 21.01 (8.5 examples/sec; 0.943 sec/batch; 78h:44m:09s remains)
INFO - root - 2017-12-07 16:36:45.051720: step 31780, loss = 21.31, batch loss = 21.23 (8.2 examples/sec; 0.974 sec/batch; 81h:20m:07s remains)
INFO - root - 2017-12-07 16:36:54.438012: step 31790, loss = 21.43, batch loss = 21.34 (8.1 examples/sec; 0.982 sec/batch; 82h:00m:45s remains)
INFO - root - 2017-12-07 16:37:03.925999: step 31800, loss = 21.58, batch loss = 21.49 (8.6 examples/sec; 0.931 sec/batch; 77h:44m:17s remains)
2017-12-07 16:37:04.879522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3678694 -4.4396272 -4.516748 -4.5601082 -4.5635104 -4.5479174 -4.5019851 -4.4048495 -4.243268 -4.076807 -4.00796 -4.0654478 -4.1894021 -4.3207779 -4.4078383][-4.4252429 -4.5214405 -4.6052856 -4.63077 -4.5957766 -4.5380955 -4.4690132 -4.380012 -4.2612915 -4.1475062 -4.1076055 -4.1659775 -4.2719178 -4.3817248 -4.4533782][-4.4978123 -4.595645 -4.6563668 -4.6419439 -4.5614567 -4.4619861 -4.3727374 -4.3028026 -4.2497907 -4.2216291 -4.2363095 -4.307415 -4.4005041 -4.4811649 -4.5194592][-4.5481939 -4.6268816 -4.6462121 -4.5852251 -4.4618807 -4.3194342 -4.2031198 -4.14698 -4.1587682 -4.223134 -4.309279 -4.4097629 -4.5033197 -4.56079 -4.5603223][-4.5556464 -4.6076818 -4.589633 -4.4908791 -4.3244643 -4.1213117 -3.9531984 -3.8984132 -3.9722505 -4.1289573 -4.2939343 -4.4305816 -4.526197 -4.5697184 -4.5457964][-4.5253711 -4.5659909 -4.53065 -4.4117994 -4.2069769 -3.9359581 -3.7034512 -3.640661 -3.7749565 -4.0208235 -4.2656541 -4.4403143 -4.5289321 -4.5495238 -4.5060248][-4.4759159 -4.5324078 -4.5067625 -4.3885312 -4.162046 -3.8419347 -3.5583177 -3.480226 -3.6462157 -3.9420173 -4.231503 -4.430841 -4.5113244 -4.5139084 -4.4628863][-4.4472504 -4.5364733 -4.539784 -4.4417171 -4.2240658 -3.8989353 -3.6004014 -3.5071702 -3.6662414 -3.9581623 -4.2400923 -4.4350448 -4.5058656 -4.4991922 -4.4481616][-4.4540067 -4.5693264 -4.6029329 -4.5408711 -4.3712034 -4.09921 -3.839114 -3.7478127 -3.8767467 -4.1278181 -4.367734 -4.5281734 -4.5717063 -4.5401869 -4.4716072][-4.4828167 -4.6061759 -4.660284 -4.6315622 -4.5176659 -4.3186193 -4.1167488 -4.034698 -4.1257915 -4.3260393 -4.5225387 -4.6423006 -4.651474 -4.5861244 -4.4907069][-4.5080247 -4.62669 -4.6968513 -4.6970353 -4.624496 -4.4805226 -4.3275867 -4.2567654 -4.314229 -4.4668069 -4.6240759 -4.7107434 -4.6906195 -4.5958014 -4.4806647][-4.4928303 -4.5998049 -4.6872787 -4.7209034 -4.6852417 -4.5850706 -4.4759235 -4.4257326 -4.4634004 -4.568666 -4.6750588 -4.7193327 -4.6716738 -4.5602689 -4.4432778][-4.4276066 -4.5093651 -4.6032495 -4.6689625 -4.6740141 -4.618525 -4.5504079 -4.5226383 -4.549623 -4.6100979 -4.657701 -4.654716 -4.5886021 -4.4855142 -4.3905773][-4.3388844 -4.3832388 -4.4617028 -4.5429754 -4.5870647 -4.5809894 -4.5535946 -4.5441976 -4.5633526 -4.5890589 -4.5902128 -4.5547328 -4.4890504 -4.4140759 -4.3522115][-4.263351 -4.2787147 -4.3328505 -4.408638 -4.473906 -4.5068688 -4.5140281 -4.51901 -4.5317903 -4.537694 -4.5189309 -4.4773083 -4.42817 -4.383256 -4.3466554]]...]
INFO - root - 2017-12-07 16:37:14.205515: step 31810, loss = 21.49, batch loss = 21.41 (8.3 examples/sec; 0.970 sec/batch; 80h:59m:00s remains)
INFO - root - 2017-12-07 16:37:23.579969: step 31820, loss = 21.35, batch loss = 21.27 (7.8 examples/sec; 1.023 sec/batch; 85h:28m:40s remains)
INFO - root - 2017-12-07 16:37:32.831123: step 31830, loss = 21.49, batch loss = 21.41 (8.5 examples/sec; 0.940 sec/batch; 78h:30m:52s remains)
INFO - root - 2017-12-07 16:37:42.134239: step 31840, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.934 sec/batch; 77h:58m:59s remains)
INFO - root - 2017-12-07 16:37:51.576852: step 31850, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.913 sec/batch; 76h:14m:21s remains)
INFO - root - 2017-12-07 16:38:00.974406: step 31860, loss = 21.42, batch loss = 21.34 (8.6 examples/sec; 0.932 sec/batch; 77h:52m:06s remains)
INFO - root - 2017-12-07 16:38:10.241112: step 31870, loss = 21.74, batch loss = 21.66 (8.6 examples/sec; 0.925 sec/batch; 77h:14m:40s remains)
INFO - root - 2017-12-07 16:38:19.533496: step 31880, loss = 21.28, batch loss = 21.20 (8.8 examples/sec; 0.910 sec/batch; 76h:01m:17s remains)
INFO - root - 2017-12-07 16:38:28.856939: step 31890, loss = 21.39, batch loss = 21.30 (8.5 examples/sec; 0.941 sec/batch; 78h:32m:07s remains)
INFO - root - 2017-12-07 16:38:38.275851: step 31900, loss = 21.44, batch loss = 21.35 (8.4 examples/sec; 0.957 sec/batch; 79h:56m:39s remains)
2017-12-07 16:38:39.158646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.495769 -4.5461516 -4.55145 -4.5276117 -4.4767733 -4.4512682 -4.5069828 -4.6039972 -4.65518 -4.626502 -4.5603414 -4.5298758 -4.5553684 -4.6036072 -4.6244755][-4.5008888 -4.5408177 -4.5350609 -4.502337 -4.4307323 -4.3785443 -4.4357615 -4.5532851 -4.6139007 -4.576355 -4.4885955 -4.4298282 -4.4389067 -4.48537 -4.514154][-4.4917417 -4.503108 -4.4658489 -4.408319 -4.3082309 -4.2207212 -4.2717805 -4.4097285 -4.4886823 -4.4659643 -4.3780904 -4.2992811 -4.2903938 -4.3291812 -4.3586864][-4.46686 -4.4367385 -4.3479838 -4.2484689 -4.1226196 -4.0170612 -4.0713792 -4.2288518 -4.3264456 -4.3318028 -4.2594786 -4.1672611 -4.1417832 -4.1726403 -4.1968822][-4.4510713 -4.3795214 -4.2351394 -4.0924344 -3.9585772 -3.8607962 -3.919528 -4.0740085 -4.1698756 -4.1961341 -4.1481547 -4.0522842 -4.009974 -4.0330834 -4.0547724][-4.4488835 -4.3434134 -4.1545 -3.9784894 -3.8506107 -3.7720113 -3.8189836 -3.9354351 -4.0034995 -4.0409966 -4.0360422 -3.9685102 -3.9300294 -3.9584081 -3.9896657][-4.4474378 -4.3217816 -4.1150022 -3.9293396 -3.817147 -3.7562466 -3.7851009 -3.8571105 -3.8897579 -3.9275007 -3.9635355 -3.9432454 -3.9260976 -3.9661617 -4.0132594][-4.4463477 -4.3205051 -4.1266966 -3.9603415 -3.8658502 -3.8153005 -3.8327057 -3.880837 -3.9023561 -3.9440019 -3.998625 -4.0042906 -4.0024929 -4.0450273 -4.0955057][-4.4478822 -4.3312578 -4.1577568 -4.0146451 -3.9299459 -3.8851719 -3.9137239 -3.9724946 -4.013135 -4.0682588 -4.1273947 -4.138402 -4.1423159 -4.175971 -4.2043495][-4.4558649 -4.3458662 -4.1842356 -4.0575428 -3.9821329 -3.9546123 -4.0115147 -4.097796 -4.1602359 -4.2125058 -4.250278 -4.2415895 -4.2319279 -4.2428679 -4.2328987][-4.4620757 -4.3571429 -4.2012367 -4.0864482 -4.0264268 -4.0274644 -4.1147509 -4.2237277 -4.29074 -4.3139853 -4.3060045 -4.2581034 -4.2130513 -4.191102 -4.1521215][-4.4434566 -4.3319559 -4.1742821 -4.0660295 -4.0237279 -4.0544839 -4.1763239 -4.3135428 -4.3829465 -4.3710957 -4.3216839 -4.2524319 -4.1853366 -4.1467223 -4.1030321][-4.4118419 -4.29214 -4.1302357 -4.0179963 -3.9799218 -4.0254922 -4.1772695 -4.3469548 -4.4198012 -4.3764925 -4.2961755 -4.2299461 -4.1829977 -4.1598349 -4.1358457][-4.4095449 -4.296814 -4.1392374 -4.0132532 -3.9526911 -3.9861813 -4.1491542 -4.3439741 -4.4233227 -4.3631606 -4.2641754 -4.2115812 -4.2054925 -4.2170897 -4.22062][-4.43976 -4.3422976 -4.1908507 -4.0458279 -3.9530096 -3.9701028 -4.1364226 -4.3474612 -4.4392624 -4.3826294 -4.281249 -4.2400041 -4.2613249 -4.2966843 -4.3183928]]...]
INFO - root - 2017-12-07 16:38:48.484692: step 31910, loss = 21.72, batch loss = 21.64 (9.0 examples/sec; 0.893 sec/batch; 74h:33m:17s remains)
INFO - root - 2017-12-07 16:38:57.770590: step 31920, loss = 21.40, batch loss = 21.32 (8.3 examples/sec; 0.966 sec/batch; 80h:37m:55s remains)
INFO - root - 2017-12-07 16:39:07.186792: step 31930, loss = 21.29, batch loss = 21.20 (8.1 examples/sec; 0.983 sec/batch; 82h:05m:30s remains)
INFO - root - 2017-12-07 16:39:16.432227: step 31940, loss = 21.35, batch loss = 21.26 (8.8 examples/sec; 0.910 sec/batch; 75h:57m:00s remains)
INFO - root - 2017-12-07 16:39:25.902693: step 31950, loss = 21.23, batch loss = 21.15 (8.3 examples/sec; 0.963 sec/batch; 80h:22m:25s remains)
INFO - root - 2017-12-07 16:39:35.328856: step 31960, loss = 20.89, batch loss = 20.81 (8.9 examples/sec; 0.903 sec/batch; 75h:24m:47s remains)
INFO - root - 2017-12-07 16:39:44.764788: step 31970, loss = 21.20, batch loss = 21.11 (8.8 examples/sec; 0.907 sec/batch; 75h:43m:23s remains)
INFO - root - 2017-12-07 16:39:54.163689: step 31980, loss = 21.37, batch loss = 21.29 (8.2 examples/sec; 0.978 sec/batch; 81h:40m:22s remains)
INFO - root - 2017-12-07 16:40:03.429648: step 31990, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.925 sec/batch; 77h:14m:01s remains)
INFO - root - 2017-12-07 16:40:12.811299: step 32000, loss = 21.33, batch loss = 21.25 (8.9 examples/sec; 0.902 sec/batch; 75h:17m:14s remains)
2017-12-07 16:40:13.730416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3080292 -4.3359709 -4.3728523 -4.4106336 -4.445322 -4.4685664 -4.4833503 -4.4912877 -4.4833694 -4.4693117 -4.4622111 -4.4581261 -4.4504046 -4.4320784 -4.4045434][-4.3283644 -4.3819532 -4.4389768 -4.488472 -4.5291743 -4.5544624 -4.5680103 -4.573483 -4.5563464 -4.533782 -4.523396 -4.5242314 -4.521852 -4.5010228 -4.459404][-4.3585644 -4.4361386 -4.503943 -4.544116 -4.5609159 -4.5607085 -4.5524821 -4.5486259 -4.5317235 -4.5125904 -4.50555 -4.520061 -4.5415444 -4.5427427 -4.5072384][-4.3902488 -4.4858227 -4.5535088 -4.5666752 -4.5349007 -4.4822731 -4.4352179 -4.4207397 -4.4168406 -4.4171152 -4.4200191 -4.4515686 -4.5050845 -4.5432572 -4.5294337][-4.4173098 -4.5138278 -4.5638208 -4.5367565 -4.4502172 -4.34532 -4.2625909 -4.238658 -4.2484641 -4.275826 -4.2998724 -4.3553028 -4.4401245 -4.5118203 -4.5233889][-4.4267964 -4.5035529 -4.5203094 -4.453907 -4.3347578 -4.2120624 -4.1271687 -4.1053038 -4.1224728 -4.1672854 -4.2148142 -4.2916894 -4.3920054 -4.4770918 -4.5010581][-4.4198365 -4.4676895 -4.4523568 -4.3623347 -4.2368865 -4.1262927 -4.0632625 -4.05488 -4.0742874 -4.1195292 -4.1779919 -4.2656951 -4.3673038 -4.451468 -4.4752226][-4.4139671 -4.4400296 -4.4030542 -4.3019857 -4.1794033 -4.081152 -4.0340161 -4.0377808 -4.0584855 -4.08707 -4.1359735 -4.2232437 -4.3246031 -4.4113474 -4.4413686][-4.4265475 -4.4508252 -4.4113288 -4.3108883 -4.1916738 -4.0936513 -4.0393209 -4.0341749 -4.0511341 -4.0629473 -4.0952044 -4.176291 -4.2843328 -4.38186 -4.4240594][-4.4505305 -4.4901457 -4.4637256 -4.3708787 -4.2535982 -4.1514592 -4.0787444 -4.04776 -4.0520291 -4.0558953 -4.07115 -4.1379766 -4.2502942 -4.3631635 -4.4199471][-4.4635277 -4.5177121 -4.5053148 -4.419188 -4.3015766 -4.1947227 -4.1116934 -4.0637989 -4.0663619 -4.0791845 -4.0858073 -4.1300216 -4.2326384 -4.3545594 -4.4206805][-4.4623132 -4.5192289 -4.5105247 -4.4237337 -4.2989483 -4.184123 -4.1008339 -4.0609193 -4.0806088 -4.1176763 -4.1302176 -4.1524076 -4.2305908 -4.3458538 -4.4125323][-4.4572649 -4.5041914 -4.4847155 -4.3908863 -4.2642522 -4.1509771 -4.0808611 -4.0700178 -4.1193266 -4.1816316 -4.2035055 -4.2072477 -4.2467084 -4.3332381 -4.3900375][-4.4603672 -4.4900312 -4.4475183 -4.34098 -4.22123 -4.127841 -4.0920091 -4.1268182 -4.2133665 -4.2941132 -4.3179021 -4.3005314 -4.2926769 -4.3279924 -4.3584909][-4.4732265 -4.49237 -4.4275546 -4.303525 -4.1846757 -4.1144814 -4.1184411 -4.1992974 -4.3191695 -4.4124222 -4.4312358 -4.3925428 -4.3420238 -4.3200769 -4.3166151]]...]
INFO - root - 2017-12-07 16:40:23.186948: step 32010, loss = 21.51, batch loss = 21.43 (8.1 examples/sec; 0.985 sec/batch; 82h:13m:47s remains)
INFO - root - 2017-12-07 16:40:32.523450: step 32020, loss = 21.74, batch loss = 21.66 (8.2 examples/sec; 0.979 sec/batch; 81h:43m:59s remains)
INFO - root - 2017-12-07 16:40:41.874141: step 32030, loss = 21.47, batch loss = 21.38 (8.6 examples/sec; 0.930 sec/batch; 77h:37m:30s remains)
INFO - root - 2017-12-07 16:40:51.131722: step 32040, loss = 21.51, batch loss = 21.42 (8.6 examples/sec; 0.925 sec/batch; 77h:14m:27s remains)
INFO - root - 2017-12-07 16:41:00.535039: step 32050, loss = 21.48, batch loss = 21.39 (8.2 examples/sec; 0.975 sec/batch; 81h:22m:38s remains)
INFO - root - 2017-12-07 16:41:09.886029: step 32060, loss = 21.55, batch loss = 21.47 (8.1 examples/sec; 0.990 sec/batch; 82h:36m:25s remains)
INFO - root - 2017-12-07 16:41:19.165836: step 32070, loss = 21.55, batch loss = 21.47 (8.7 examples/sec; 0.920 sec/batch; 76h:44m:09s remains)
INFO - root - 2017-12-07 16:41:28.519262: step 32080, loss = 21.26, batch loss = 21.17 (8.4 examples/sec; 0.952 sec/batch; 79h:24m:38s remains)
INFO - root - 2017-12-07 16:41:37.957538: step 32090, loss = 21.54, batch loss = 21.46 (8.2 examples/sec; 0.978 sec/batch; 81h:35m:49s remains)
INFO - root - 2017-12-07 16:41:47.274671: step 32100, loss = 21.59, batch loss = 21.50 (8.5 examples/sec; 0.943 sec/batch; 78h:42m:08s remains)
2017-12-07 16:41:48.281664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3675928 -4.3628364 -4.321228 -4.2812042 -4.2929726 -4.3344879 -4.3616662 -4.3512669 -4.318706 -4.301085 -4.3166919 -4.3414922 -4.3383937 -4.3267894 -4.3152428][-4.324749 -4.3094959 -4.2501779 -4.2064729 -4.2371092 -4.2957816 -4.3208618 -4.3000269 -4.2611518 -4.2467546 -4.2777281 -4.3158689 -4.3078084 -4.2773213 -4.2456107][-4.2740364 -4.2509785 -4.1809287 -4.1367035 -4.1789184 -4.2446079 -4.2611346 -4.2306843 -4.1892204 -4.1777883 -4.2181273 -4.2672744 -4.2677817 -4.2408748 -4.2044921][-4.2614293 -4.2346687 -4.1630096 -4.1203852 -4.1606641 -4.2187705 -4.2235751 -4.188005 -4.1481185 -4.1405582 -4.1812768 -4.2304688 -4.239862 -4.2261567 -4.1963191][-4.2887812 -4.2661295 -4.2006559 -4.1584234 -4.1836505 -4.2218065 -4.2173357 -4.1839285 -4.1528039 -4.1536617 -4.1933942 -4.2364931 -4.2487111 -4.2390032 -4.2086196][-4.3094234 -4.306427 -4.2597852 -4.2220936 -4.2292256 -4.2452931 -4.2323012 -4.2003193 -4.1755242 -4.183095 -4.22227 -4.261776 -4.2733788 -4.2576437 -4.2228279][-4.2883534 -4.3025789 -4.2814116 -4.2586007 -4.2616239 -4.2661028 -4.2443104 -4.2105269 -4.1886172 -4.2009912 -4.2408895 -4.2792821 -4.287715 -4.2644944 -4.2278371][-4.2406273 -4.2569866 -4.2581825 -4.2574949 -4.271924 -4.2757182 -4.2477932 -4.2123957 -4.192565 -4.2066751 -4.2484035 -4.2902579 -4.2993517 -4.2735004 -4.2382011][-4.2060294 -4.2047567 -4.21159 -4.2305813 -4.26782 -4.2830105 -4.2546363 -4.2139921 -4.1896548 -4.2010756 -4.2421365 -4.2859898 -4.2997084 -4.2809892 -4.2547016][-4.2039032 -4.1768775 -4.1731496 -4.1991758 -4.2535386 -4.28253 -4.2590327 -4.2141943 -4.1813235 -4.1849318 -4.2209859 -4.264173 -4.2877083 -4.2865691 -4.2781544][-4.2206035 -4.1804042 -4.1635232 -4.1821737 -4.2342873 -4.2688432 -4.25577 -4.21631 -4.1789923 -4.1725569 -4.1973562 -4.2348523 -4.2666645 -4.2867103 -4.3005147][-4.2457614 -4.2097411 -4.1903796 -4.1987429 -4.2336712 -4.2591782 -4.2497416 -4.2167978 -4.1815643 -4.1715384 -4.1836014 -4.2062206 -4.2335196 -4.2620363 -4.2888789][-4.2643538 -4.2460179 -4.2374105 -4.2385168 -4.251225 -4.2599821 -4.2472806 -4.2180862 -4.1898 -4.1822147 -4.1866331 -4.1936131 -4.2068453 -4.2271743 -4.247088][-4.2549529 -4.2562795 -4.2640972 -4.26411 -4.2605667 -4.2599468 -4.25194 -4.2352562 -4.2213783 -4.22021 -4.2196612 -4.213603 -4.2112679 -4.2162519 -4.2181835][-4.2197242 -4.2358027 -4.2548332 -4.2528367 -4.237329 -4.2380714 -4.2450337 -4.251399 -4.2621469 -4.2738643 -4.2693009 -4.250113 -4.23158 -4.2228885 -4.2105083]]...]
INFO - root - 2017-12-07 16:41:57.698080: step 32110, loss = 21.37, batch loss = 21.29 (9.2 examples/sec; 0.866 sec/batch; 72h:17m:25s remains)
INFO - root - 2017-12-07 16:42:07.136509: step 32120, loss = 21.19, batch loss = 21.11 (9.1 examples/sec; 0.880 sec/batch; 73h:27m:03s remains)
INFO - root - 2017-12-07 16:42:16.649457: step 32130, loss = 21.56, batch loss = 21.48 (8.5 examples/sec; 0.947 sec/batch; 78h:58m:26s remains)
INFO - root - 2017-12-07 16:42:25.836709: step 32140, loss = 21.86, batch loss = 21.78 (8.8 examples/sec; 0.908 sec/batch; 75h:45m:53s remains)
INFO - root - 2017-12-07 16:42:35.237382: step 32150, loss = 21.82, batch loss = 21.74 (8.7 examples/sec; 0.924 sec/batch; 77h:07m:28s remains)
INFO - root - 2017-12-07 16:42:44.608119: step 32160, loss = 21.43, batch loss = 21.35 (8.4 examples/sec; 0.955 sec/batch; 79h:38m:14s remains)
INFO - root - 2017-12-07 16:42:53.933055: step 32170, loss = 21.12, batch loss = 21.03 (8.1 examples/sec; 0.986 sec/batch; 82h:14m:12s remains)
INFO - root - 2017-12-07 16:43:03.191973: step 32180, loss = 21.16, batch loss = 21.08 (8.0 examples/sec; 0.996 sec/batch; 83h:05m:05s remains)
INFO - root - 2017-12-07 16:43:12.484117: step 32190, loss = 21.79, batch loss = 21.71 (8.4 examples/sec; 0.953 sec/batch; 79h:32m:19s remains)
INFO - root - 2017-12-07 16:43:21.763820: step 32200, loss = 21.09, batch loss = 21.01 (8.1 examples/sec; 0.989 sec/batch; 82h:30m:29s remains)
2017-12-07 16:43:22.747393: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3106165 -4.3213592 -4.3302503 -4.3330874 -4.3311248 -4.3260565 -4.3200316 -4.3162279 -4.3150868 -4.3145914 -4.3129134 -4.3107018 -4.310523 -4.3128262 -4.3158054][-4.3320627 -4.3572006 -4.3793378 -4.3903928 -4.3889012 -4.3777294 -4.3653784 -4.36071 -4.3634315 -4.3664484 -4.3633575 -4.35331 -4.3416419 -4.333735 -4.3295937][-4.368959 -4.4103355 -4.441371 -4.446702 -4.4257107 -4.3918695 -4.3684993 -4.3721547 -4.3956137 -4.4179373 -4.4235916 -4.4094758 -4.3845568 -4.3606024 -4.3431921][-4.4090066 -4.4573269 -4.4779258 -4.4482718 -4.37669 -4.299613 -4.2635345 -4.2910852 -4.3583078 -4.4227128 -4.4558749 -4.4512172 -4.420445 -4.3823514 -4.3503027][-4.4337339 -4.4702711 -4.45232 -4.3560948 -4.2093163 -4.0793848 -4.0352559 -4.0975027 -4.2202959 -4.3404608 -4.4169235 -4.4407125 -4.4234676 -4.3858905 -4.3465848][-4.4392457 -4.4489017 -4.3760438 -4.2032514 -3.9823782 -3.8103273 -3.7692397 -3.867497 -4.0395546 -4.2103009 -4.3288074 -4.3850155 -4.3930988 -4.3706627 -4.3359637][-4.4420195 -4.4246955 -4.3056636 -4.0755467 -3.804857 -3.6093113 -3.5768194 -3.702733 -3.9098628 -4.1185603 -4.2664475 -4.3421564 -4.3661757 -4.3565335 -4.3296738][-4.4607105 -4.4331741 -4.2998905 -4.0555639 -3.7732127 -3.5691538 -3.5337594 -3.6618793 -3.8783176 -4.1029067 -4.26392 -4.342423 -4.3648076 -4.35391 -4.328537][-4.495966 -4.4766178 -4.3633118 -4.1506472 -3.8988171 -3.708498 -3.6655588 -3.7731802 -3.968827 -4.1760588 -4.3208561 -4.378211 -4.3780961 -4.3500848 -4.3170848][-4.5327654 -4.5235848 -4.4384761 -4.2768803 -4.0854936 -3.939769 -3.9057565 -3.9919918 -4.1488857 -4.3086262 -4.4071808 -4.4232864 -4.3879547 -4.33555 -4.2911768][-4.5632815 -4.5484471 -4.4717097 -4.3480406 -4.2193255 -4.1353722 -4.1311412 -4.208334 -4.3270669 -4.4333229 -4.4810724 -4.45734 -4.3923907 -4.3210177 -4.2677298][-4.5761156 -4.5371947 -4.4455585 -4.335021 -4.2527289 -4.2276659 -4.2636609 -4.3469772 -4.441504 -4.5107327 -4.5258627 -4.4816957 -4.4057226 -4.3289 -4.2744341][-4.5510125 -4.4801688 -4.3589578 -4.2386489 -4.1766386 -4.1912818 -4.265029 -4.3670359 -4.4580808 -4.5163713 -4.5267057 -4.4890852 -4.4258895 -4.3607588 -4.314126][-4.4722939 -4.3895783 -4.2505636 -4.1159396 -4.0525565 -4.0797315 -4.17345 -4.2922225 -4.3912129 -4.4563241 -4.4794807 -4.4634638 -4.42604 -4.3824749 -4.3497119][-4.3548522 -4.2959051 -4.1721177 -4.0434489 -3.9781432 -4.0029745 -4.098351 -4.2208552 -4.32209 -4.3891993 -4.4185863 -4.4160123 -4.3982682 -4.3737249 -4.3547878]]...]
INFO - root - 2017-12-07 16:43:32.159588: step 32210, loss = 21.64, batch loss = 21.56 (9.0 examples/sec; 0.885 sec/batch; 73h:51m:18s remains)
INFO - root - 2017-12-07 16:43:41.540403: step 32220, loss = 21.31, batch loss = 21.23 (8.8 examples/sec; 0.905 sec/batch; 75h:31m:08s remains)
INFO - root - 2017-12-07 16:43:50.970775: step 32230, loss = 21.52, batch loss = 21.44 (8.7 examples/sec; 0.925 sec/batch; 77h:07m:29s remains)
INFO - root - 2017-12-07 16:44:00.309934: step 32240, loss = 21.07, batch loss = 20.99 (8.4 examples/sec; 0.955 sec/batch; 79h:40m:27s remains)
INFO - root - 2017-12-07 16:44:09.585386: step 32250, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.948 sec/batch; 79h:04m:13s remains)
INFO - root - 2017-12-07 16:44:18.835737: step 32260, loss = 21.46, batch loss = 21.38 (8.9 examples/sec; 0.901 sec/batch; 75h:06m:50s remains)
INFO - root - 2017-12-07 16:44:28.038681: step 32270, loss = 21.47, batch loss = 21.38 (8.8 examples/sec; 0.905 sec/batch; 75h:28m:30s remains)
INFO - root - 2017-12-07 16:44:37.472366: step 32280, loss = 21.66, batch loss = 21.58 (8.7 examples/sec; 0.918 sec/batch; 76h:33m:30s remains)
INFO - root - 2017-12-07 16:44:46.858442: step 32290, loss = 21.23, batch loss = 21.14 (8.3 examples/sec; 0.967 sec/batch; 80h:38m:23s remains)
INFO - root - 2017-12-07 16:44:56.365482: step 32300, loss = 21.09, batch loss = 21.00 (8.1 examples/sec; 0.990 sec/batch; 82h:31m:59s remains)
2017-12-07 16:44:57.255548: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2133455 -4.1673508 -4.1868262 -4.2309513 -4.2609963 -4.2860708 -4.3206511 -4.3810043 -4.4275813 -4.3849735 -4.2727847 -4.1548657 -4.086484 -4.0836053 -4.0952168][-4.2431879 -4.2208858 -4.2550755 -4.2888489 -4.2842607 -4.2745156 -4.302351 -4.3554192 -4.38938 -4.3622036 -4.2914715 -4.1933036 -4.1015453 -4.060895 -4.0524559][-4.2681732 -4.2743931 -4.3230968 -4.3489475 -4.309463 -4.2575159 -4.2605867 -4.2917013 -4.316834 -4.324923 -4.3125343 -4.2499285 -4.1511359 -4.0777669 -4.0446725][-4.2812457 -4.3041811 -4.3576961 -4.3860135 -4.3372445 -4.2628622 -4.2316613 -4.2245927 -4.2400947 -4.2900615 -4.3366885 -4.3142552 -4.2253518 -4.13527 -4.0795074][-4.2917976 -4.3109312 -4.3552489 -4.3884535 -4.3509359 -4.270195 -4.20543 -4.1640935 -4.1811419 -4.2697268 -4.3608818 -4.3723106 -4.3052363 -4.22084 -4.1582108][-4.3077073 -4.3135037 -4.3373928 -4.3648911 -4.3289309 -4.2301993 -4.133728 -4.08142 -4.1239471 -4.2491331 -4.3670497 -4.399591 -4.3556266 -4.2931771 -4.2426882][-4.3379636 -4.3307128 -4.3309927 -4.3360715 -4.2723861 -4.1310549 -3.9990926 -3.9550998 -4.0422516 -4.2037554 -4.3392415 -4.3866158 -4.3631687 -4.328012 -4.3004756][-4.3732576 -4.367115 -4.3532948 -4.3305459 -4.2221742 -4.0290504 -3.8635557 -3.8301415 -3.9572303 -4.1469665 -4.2962408 -4.3569922 -4.3488359 -4.3380718 -4.3339524][-4.3824515 -4.406806 -4.4020929 -4.3664207 -4.2286911 -4.0048227 -3.8204212 -3.7860932 -3.9212368 -4.1142297 -4.2696013 -4.3416195 -4.339386 -4.3370214 -4.3460994][-4.3683524 -4.4302125 -4.4479918 -4.4219933 -4.2927184 -4.0825577 -3.910202 -3.8689423 -3.9720488 -4.1317205 -4.2737784 -4.348403 -4.3472166 -4.3402009 -4.3491316][-4.3362937 -4.4116588 -4.441854 -4.436657 -4.3485293 -4.1918674 -4.0609908 -4.0210462 -4.0783191 -4.1812663 -4.2945948 -4.368454 -4.3736486 -4.3604493 -4.3589778][-4.3127122 -4.3659396 -4.3848076 -4.3993149 -4.3633618 -4.2737045 -4.1913252 -4.1570745 -4.173996 -4.2222457 -4.3038335 -4.3793778 -4.4004574 -4.3848281 -4.3693571][-4.3235183 -4.3306794 -4.3272157 -4.3501358 -4.3475161 -4.3089585 -4.2635951 -4.2307372 -4.2173891 -4.2291842 -4.2903419 -4.3711109 -4.409349 -4.3937173 -4.3642163][-4.3590207 -4.3285704 -4.3098474 -4.3318624 -4.3387508 -4.3182573 -4.2844863 -4.2452526 -4.215714 -4.213109 -4.2682958 -4.3548946 -4.4038615 -4.3894911 -4.3543978][-4.4168329 -4.3682508 -4.3476186 -4.3664103 -4.3617086 -4.3297539 -4.2836266 -4.2308545 -4.1931963 -4.1928511 -4.2555637 -4.3487144 -4.4009247 -4.3848224 -4.3492723]]...]
INFO - root - 2017-12-07 16:45:06.634150: step 32310, loss = 21.77, batch loss = 21.69 (8.7 examples/sec; 0.924 sec/batch; 77h:01m:17s remains)
INFO - root - 2017-12-07 16:45:16.045171: step 32320, loss = 21.22, batch loss = 21.14 (8.2 examples/sec; 0.977 sec/batch; 81h:29m:54s remains)
INFO - root - 2017-12-07 16:45:25.285579: step 32330, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.963 sec/batch; 80h:19m:25s remains)
INFO - root - 2017-12-07 16:45:34.657996: step 32340, loss = 21.29, batch loss = 21.21 (8.7 examples/sec; 0.923 sec/batch; 76h:59m:30s remains)
INFO - root - 2017-12-07 16:45:43.911907: step 32350, loss = 21.16, batch loss = 21.07 (8.8 examples/sec; 0.908 sec/batch; 75h:42m:21s remains)
INFO - root - 2017-12-07 16:45:53.299735: step 32360, loss = 21.49, batch loss = 21.41 (8.7 examples/sec; 0.920 sec/batch; 76h:44m:35s remains)
INFO - root - 2017-12-07 16:46:02.717063: step 32370, loss = 21.33, batch loss = 21.24 (8.3 examples/sec; 0.964 sec/batch; 80h:21m:34s remains)
INFO - root - 2017-12-07 16:46:12.033587: step 32380, loss = 21.08, batch loss = 20.99 (8.1 examples/sec; 0.992 sec/batch; 82h:42m:12s remains)
INFO - root - 2017-12-07 16:46:21.558197: step 32390, loss = 21.25, batch loss = 21.17 (8.0 examples/sec; 0.999 sec/batch; 83h:19m:09s remains)
INFO - root - 2017-12-07 16:46:30.886472: step 32400, loss = 21.59, batch loss = 21.50 (8.6 examples/sec; 0.930 sec/batch; 77h:33m:18s remains)
2017-12-07 16:46:31.870223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4336028 -4.4299541 -4.4297843 -4.4440088 -4.4810023 -4.53892 -4.5947504 -4.6245832 -4.62677 -4.6011529 -4.5656395 -4.5405941 -4.542592 -4.5636764 -4.5764027][-4.4821749 -4.4876137 -4.4932704 -4.5136409 -4.5508566 -4.6056376 -4.6569462 -4.6848435 -4.6946492 -4.6797619 -4.6519976 -4.6269507 -4.6252818 -4.6394544 -4.6342483][-4.5252614 -4.5353956 -4.5421467 -4.5621305 -4.5824919 -4.6041961 -4.6190486 -4.6262136 -4.6477394 -4.6627593 -4.6677289 -4.6612945 -4.6666479 -4.680975 -4.658742][-4.5426121 -4.5442395 -4.5406194 -4.5576611 -4.5577788 -4.5298924 -4.4838958 -4.4518318 -4.4861727 -4.5477772 -4.6075616 -4.6402369 -4.6615605 -4.6829309 -4.6544428][-4.5421729 -4.5248847 -4.49531 -4.5006208 -4.481132 -4.3983793 -4.2782154 -4.1924486 -4.2354913 -4.3543935 -4.494031 -4.5881643 -4.6363544 -4.6633811 -4.6297274][-4.539506 -4.5026107 -4.4367619 -4.4130425 -4.3665948 -4.2261705 -4.0283394 -3.8794329 -3.9186387 -4.0905871 -4.3195024 -4.495574 -4.59234 -4.6362076 -4.6024714][-4.5443082 -4.4996495 -4.4002442 -4.3304467 -4.2457914 -4.0607638 -3.8057823 -3.5976915 -3.6135809 -3.8105683 -4.0992365 -4.3462377 -4.497983 -4.5754986 -4.5629482][-4.5567551 -4.5311513 -4.4177151 -4.3032231 -4.18391 -3.9909427 -3.7352209 -3.5041106 -3.485281 -3.656832 -3.9343877 -4.1960306 -4.3781066 -4.4949427 -4.525877][-4.5635395 -4.5759625 -4.4785204 -4.3486142 -4.2197676 -4.0626903 -3.8709579 -3.6787033 -3.6382029 -3.7506127 -3.9587295 -4.173264 -4.3443737 -4.4779081 -4.539463][-4.5526848 -4.6033516 -4.5476251 -4.4417148 -4.3363528 -4.2356882 -4.1239991 -3.9930928 -3.9466243 -4.0001431 -4.1262836 -4.271594 -4.4039106 -4.5244074 -4.5827966][-4.5219913 -4.5978742 -4.5898376 -4.5335026 -4.4728708 -4.4212461 -4.3622108 -4.2771192 -4.2304831 -4.2424212 -4.3036127 -4.3852553 -4.4751992 -4.5656776 -4.5971632][-4.4747252 -4.5537333 -4.5773826 -4.5658116 -4.5492334 -4.5364909 -4.5116496 -4.463788 -4.4278617 -4.4172654 -4.43386 -4.4695773 -4.5232153 -4.5767827 -4.5757875][-4.4181433 -4.4800243 -4.5126939 -4.5263104 -4.5430865 -4.562593 -4.5659828 -4.5453773 -4.5184317 -4.4964128 -4.4899788 -4.5016069 -4.5292015 -4.5512285 -4.5302725][-4.3649645 -4.39985 -4.4237561 -4.4423132 -4.4738989 -4.5128756 -4.5349765 -4.5311675 -4.5108066 -4.4865613 -4.4707952 -4.4705219 -4.481987 -4.4866338 -4.4631753][-4.3288689 -4.3383937 -4.3462114 -4.3563337 -4.3818932 -4.4177113 -4.4439063 -4.4513707 -4.4443016 -4.4303112 -4.4158249 -4.4083538 -4.4082789 -4.4055786 -4.3904185]]...]
INFO - root - 2017-12-07 16:46:41.284445: step 32410, loss = 21.11, batch loss = 21.02 (9.0 examples/sec; 0.891 sec/batch; 74h:18m:04s remains)
INFO - root - 2017-12-07 16:46:50.486363: step 32420, loss = 21.40, batch loss = 21.31 (9.1 examples/sec; 0.879 sec/batch; 73h:14m:49s remains)
INFO - root - 2017-12-07 16:46:59.792120: step 32430, loss = 21.51, batch loss = 21.43 (9.1 examples/sec; 0.876 sec/batch; 73h:02m:02s remains)
INFO - root - 2017-12-07 16:47:09.017678: step 32440, loss = 21.39, batch loss = 21.30 (8.6 examples/sec; 0.931 sec/batch; 77h:38m:05s remains)
INFO - root - 2017-12-07 16:47:18.330353: step 32450, loss = 21.50, batch loss = 21.42 (8.3 examples/sec; 0.959 sec/batch; 79h:57m:47s remains)
INFO - root - 2017-12-07 16:47:27.721171: step 32460, loss = 21.57, batch loss = 21.48 (8.3 examples/sec; 0.969 sec/batch; 80h:47m:51s remains)
INFO - root - 2017-12-07 16:47:37.159425: step 32470, loss = 21.73, batch loss = 21.65 (8.5 examples/sec; 0.944 sec/batch; 78h:38m:25s remains)
INFO - root - 2017-12-07 16:47:46.437904: step 32480, loss = 21.81, batch loss = 21.73 (8.7 examples/sec; 0.919 sec/batch; 76h:37m:08s remains)
INFO - root - 2017-12-07 16:47:55.790657: step 32490, loss = 21.06, batch loss = 20.98 (8.7 examples/sec; 0.918 sec/batch; 76h:30m:32s remains)
INFO - root - 2017-12-07 16:48:05.336299: step 32500, loss = 21.47, batch loss = 21.39 (8.6 examples/sec; 0.932 sec/batch; 77h:38m:00s remains)
2017-12-07 16:48:06.323105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4517665 -4.4487739 -4.4386444 -4.4325862 -4.4315448 -4.41317 -4.3690972 -4.3011413 -4.2423587 -4.2405291 -4.281229 -4.3507705 -4.4275832 -4.4853449 -4.5092068][-4.4466991 -4.4425826 -4.42803 -4.4169621 -4.4147143 -4.3970523 -4.3511419 -4.2777019 -4.2187829 -4.2270436 -4.2838054 -4.3663974 -4.4493232 -4.5103235 -4.5372534][-4.3445764 -4.3729005 -4.3832903 -4.3785644 -4.3681297 -4.3380356 -4.2776527 -4.1885972 -4.1264677 -4.147862 -4.2309451 -4.3432131 -4.4482207 -4.5218439 -4.5533848][-4.2026391 -4.2706242 -4.3197188 -4.326334 -4.302454 -4.2467 -4.1554151 -4.0345888 -3.9596009 -3.9928536 -4.1073222 -4.2624922 -4.4018831 -4.4954066 -4.537117][-4.0827465 -4.167933 -4.234056 -4.2396426 -4.1974463 -4.1130919 -3.985853 -3.8264029 -3.7348926 -3.77876 -3.9263306 -4.1306205 -4.3132334 -4.4368744 -4.5009923][-4.0234647 -4.1072059 -4.1660962 -4.1533604 -4.0868497 -3.9760861 -3.8205342 -3.6350665 -3.5340283 -3.5828023 -3.7484276 -3.9829659 -4.1972866 -4.3550153 -4.4532237][-4.0707068 -4.1410627 -4.1793995 -4.1459994 -4.0595317 -3.9298525 -3.7668846 -3.5906076 -3.4990683 -3.5482445 -3.7059324 -3.9299386 -4.1428103 -4.3130326 -4.4296527][-4.191988 -4.2507272 -4.2680821 -4.2205276 -4.12791 -4.0003605 -3.8625154 -3.7323775 -3.6690836 -3.7116609 -3.8370106 -4.016408 -4.1929016 -4.3415465 -4.4465642][-4.30237 -4.364306 -4.3739505 -4.3202176 -4.2328286 -4.1245618 -4.0309219 -3.9636765 -3.9370019 -3.9715889 -4.055902 -4.1780529 -4.3012509 -4.4074831 -4.4811945][-4.3641677 -4.4432158 -4.4600244 -4.4124365 -4.3438458 -4.2693858 -4.2203193 -4.2029719 -4.2043152 -4.2306819 -4.2772088 -4.3429408 -4.4077706 -4.4640379 -4.5017452][-4.3901553 -4.4935827 -4.5298519 -4.5014133 -4.4590154 -4.4194436 -4.3994594 -4.3998003 -4.4068007 -4.4205041 -4.4348388 -4.4549122 -4.4728432 -4.4885187 -4.4990911][-4.3902669 -4.5140457 -4.5698051 -4.5628405 -4.5418773 -4.525691 -4.5150514 -4.505805 -4.49517 -4.4867644 -4.4736605 -4.4663815 -4.4644775 -4.46884 -4.4757934][-4.3989682 -4.5015745 -4.5526829 -4.5484767 -4.528347 -4.5142384 -4.4954982 -4.4644394 -4.4339004 -4.4130168 -4.3918552 -4.3842077 -4.3917365 -4.4137 -4.4396014][-4.4202113 -4.4816852 -4.5012836 -4.4711409 -4.4253087 -4.3882794 -4.3457537 -4.2923083 -4.2566729 -4.2481461 -4.2486563 -4.2676988 -4.3029509 -4.3537273 -4.4036956][-4.4499216 -4.4770818 -4.4658113 -4.4047165 -4.32687 -4.2578969 -4.1896749 -4.1218309 -4.0939794 -4.11132 -4.1440754 -4.19572 -4.2573071 -4.3291988 -4.3930154]]...]
INFO - root - 2017-12-07 16:48:15.653721: step 32510, loss = 21.55, batch loss = 21.46 (8.4 examples/sec; 0.949 sec/batch; 79h:06m:54s remains)
INFO - root - 2017-12-07 16:48:24.943672: step 32520, loss = 21.41, batch loss = 21.33 (8.4 examples/sec; 0.947 sec/batch; 78h:56m:00s remains)
INFO - root - 2017-12-07 16:48:34.173337: step 32530, loss = 21.48, batch loss = 21.40 (8.3 examples/sec; 0.967 sec/batch; 80h:35m:54s remains)
INFO - root - 2017-12-07 16:48:43.556440: step 32540, loss = 21.32, batch loss = 21.24 (8.2 examples/sec; 0.976 sec/batch; 81h:17m:04s remains)
INFO - root - 2017-12-07 16:48:52.790216: step 32550, loss = 21.29, batch loss = 21.21 (9.7 examples/sec; 0.821 sec/batch; 68h:23m:23s remains)
INFO - root - 2017-12-07 16:49:02.062646: step 32560, loss = 21.68, batch loss = 21.60 (8.9 examples/sec; 0.900 sec/batch; 74h:57m:13s remains)
INFO - root - 2017-12-07 16:49:11.475957: step 32570, loss = 21.33, batch loss = 21.24 (8.5 examples/sec; 0.938 sec/batch; 78h:06m:30s remains)
INFO - root - 2017-12-07 16:49:20.899233: step 32580, loss = 21.09, batch loss = 21.00 (8.3 examples/sec; 0.961 sec/batch; 80h:06m:03s remains)
INFO - root - 2017-12-07 16:49:30.223613: step 32590, loss = 21.55, batch loss = 21.46 (8.8 examples/sec; 0.906 sec/batch; 75h:29m:07s remains)
INFO - root - 2017-12-07 16:49:39.523934: step 32600, loss = 21.73, batch loss = 21.65 (8.8 examples/sec; 0.911 sec/batch; 75h:53m:58s remains)
2017-12-07 16:49:40.372172: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4217005 -4.4896417 -4.594964 -4.7193332 -4.8152962 -4.7841935 -4.6049552 -4.4586611 -4.4467988 -4.4555831 -4.4472942 -4.4411263 -4.449975 -4.4865937 -4.517674][-4.4518056 -4.528223 -4.6222034 -4.714191 -4.7722178 -4.7188616 -4.5403295 -4.4068894 -4.4063997 -4.412158 -4.3990445 -4.3909612 -4.40728 -4.4656959 -4.5162988][-4.4228649 -4.5013647 -4.5871325 -4.6606593 -4.7009168 -4.6437817 -4.4833183 -4.3663421 -4.366786 -4.3604412 -4.3353109 -4.3268237 -4.3553858 -4.44125 -4.5108089][-4.3291254 -4.3960762 -4.4759336 -4.5517082 -4.6005445 -4.5570517 -4.4211764 -4.3168836 -4.3050179 -4.2777867 -4.2442412 -4.2517862 -4.3113046 -4.4352345 -4.52573][-4.2193041 -4.2687244 -4.349102 -4.4420633 -4.5109692 -4.4815335 -4.3633904 -4.2640667 -4.2277894 -4.1750388 -4.1441288 -4.1902847 -4.3031464 -4.4659462 -4.5739532][-4.1455274 -4.175159 -4.2480874 -4.3405337 -4.4016666 -4.3678927 -4.2609253 -4.1780496 -4.1524997 -4.120152 -4.122714 -4.2089138 -4.3547382 -4.517818 -4.6119118][-4.1287842 -4.131906 -4.1742864 -4.2300415 -4.24134 -4.1777453 -4.0782595 -4.02762 -4.0535169 -4.0932093 -4.1528788 -4.2630792 -4.4081 -4.5343552 -4.5913391][-4.1914353 -4.1608233 -4.1575432 -4.1535883 -4.0815039 -3.9612923 -3.8598454 -3.8439651 -3.9351492 -4.0664625 -4.1887231 -4.3081965 -4.4291682 -4.5036435 -4.5140934][-4.2930732 -4.2430878 -4.2056718 -4.1499615 -4.0038495 -3.8335285 -3.7363591 -3.7520862 -3.8940513 -4.0963936 -4.2613645 -4.3709636 -4.443337 -4.44978 -4.4064231][-4.3679571 -4.3263192 -4.2798252 -4.2027764 -4.02539 -3.8360641 -3.7535956 -3.7889724 -3.9417689 -4.15504 -4.319376 -4.406774 -4.4306636 -4.3799481 -4.30561][-4.4152269 -4.4009218 -4.3641567 -4.2906356 -4.1290331 -3.9639421 -3.9083722 -3.9493916 -4.072648 -4.2424254 -4.3691564 -4.4232359 -4.4094329 -4.329711 -4.2562056][-4.4183931 -4.4372878 -4.4277015 -4.3828821 -4.2742591 -4.1672268 -4.1444149 -4.1800141 -4.2528253 -4.3521204 -4.4182482 -4.4337325 -4.4024849 -4.3284707 -4.2748647][-4.3761997 -4.4179316 -4.4342842 -4.4200091 -4.3649845 -4.3128529 -4.3137584 -4.3343945 -4.3530383 -4.3865209 -4.4054127 -4.4053 -4.390522 -4.3533978 -4.3283086][-4.3317404 -4.3724647 -4.3955469 -4.3958015 -4.3732128 -4.3557744 -4.3722138 -4.3844547 -4.3708854 -4.3699756 -4.3754783 -4.3834925 -4.3989291 -4.3982558 -4.3855019][-4.2858028 -4.3036275 -4.3186455 -4.3238077 -4.3176293 -4.3214974 -4.3541651 -4.3736277 -4.3529425 -4.34361 -4.35519 -4.3759012 -4.4132681 -4.4392047 -4.4343605]]...]
INFO - root - 2017-12-07 16:49:49.766346: step 32610, loss = 21.82, batch loss = 21.74 (8.2 examples/sec; 0.972 sec/batch; 81h:00m:33s remains)
INFO - root - 2017-12-07 16:49:59.199382: step 32620, loss = 21.25, batch loss = 21.16 (8.0 examples/sec; 1.004 sec/batch; 83h:39m:14s remains)
INFO - root - 2017-12-07 16:50:08.562730: step 32630, loss = 21.62, batch loss = 21.53 (8.3 examples/sec; 0.967 sec/batch; 80h:33m:53s remains)
INFO - root - 2017-12-07 16:50:17.942700: step 32640, loss = 21.57, batch loss = 21.49 (8.6 examples/sec; 0.931 sec/batch; 77h:30m:22s remains)
INFO - root - 2017-12-07 16:50:27.373436: step 32650, loss = 21.27, batch loss = 21.19 (8.0 examples/sec; 0.997 sec/batch; 83h:00m:08s remains)
INFO - root - 2017-12-07 16:50:36.567009: step 32660, loss = 21.38, batch loss = 21.30 (8.3 examples/sec; 0.968 sec/batch; 80h:36m:30s remains)
INFO - root - 2017-12-07 16:50:45.944175: step 32670, loss = 21.29, batch loss = 21.21 (8.6 examples/sec; 0.930 sec/batch; 77h:25m:29s remains)
INFO - root - 2017-12-07 16:50:55.401161: step 32680, loss = 21.37, batch loss = 21.29 (9.3 examples/sec; 0.863 sec/batch; 71h:50m:35s remains)
INFO - root - 2017-12-07 16:51:04.696158: step 32690, loss = 21.66, batch loss = 21.58 (8.3 examples/sec; 0.960 sec/batch; 79h:54m:52s remains)
INFO - root - 2017-12-07 16:51:14.120189: step 32700, loss = 21.34, batch loss = 21.26 (8.7 examples/sec; 0.919 sec/batch; 76h:32m:49s remains)
2017-12-07 16:51:15.062325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4488521 -4.5573354 -4.6658292 -4.7368355 -4.7577624 -4.7303329 -4.6719294 -4.6192675 -4.5863156 -4.5535 -4.5287924 -4.5162239 -4.4827666 -4.4533248 -4.4630194][-4.380971 -4.4891076 -4.6019478 -4.6866093 -4.7194877 -4.6877241 -4.6176376 -4.552022 -4.4932351 -4.4335074 -4.3965445 -4.3654437 -4.2959771 -4.2360692 -4.2322412][-4.3584752 -4.4473171 -4.5484281 -4.6399794 -4.682478 -4.6436262 -4.5551133 -4.4665504 -4.3856349 -4.321249 -4.2892675 -4.2471256 -4.156774 -4.0845122 -4.070776][-4.3759437 -4.4403472 -4.5217085 -4.6042151 -4.6393981 -4.5834317 -4.4716873 -4.3581176 -4.2791424 -4.2530169 -4.2544446 -4.2222877 -4.1424403 -4.0805559 -4.0614715][-4.3983049 -4.4384036 -4.4900661 -4.535068 -4.5335541 -4.4520197 -4.3338561 -4.2294412 -4.1955423 -4.2354627 -4.271452 -4.2497249 -4.194108 -4.1488523 -4.1263289][-4.4058404 -4.4335661 -4.4555955 -4.4445987 -4.3851762 -4.2684412 -4.149663 -4.0808764 -4.1120958 -4.2116504 -4.2693949 -4.2643194 -4.2453933 -4.224957 -4.2076144][-4.4083323 -4.4415889 -4.4456406 -4.384325 -4.259645 -4.0861821 -3.9425397 -3.9065726 -4.0002966 -4.1476178 -4.2376738 -4.2757244 -4.3055115 -4.3135214 -4.3024855][-4.4199939 -4.4700742 -4.4738398 -4.3891997 -4.2155595 -3.9746871 -3.7834134 -3.7609742 -3.8983002 -4.0805354 -4.2173338 -4.3124261 -4.3831935 -4.4011354 -4.3806882][-4.4356046 -4.5014353 -4.519033 -4.4417982 -4.2541127 -3.9709289 -3.7445216 -3.7206149 -3.8665195 -4.0626249 -4.2353258 -4.373229 -4.4713945 -4.4887886 -4.4517951][-4.4404874 -4.5117025 -4.5465736 -4.496273 -4.327332 -4.0470328 -3.8249583 -3.8020878 -3.9380016 -4.13183 -4.3137226 -4.461287 -4.5680203 -4.5805855 -4.5296793][-4.4368944 -4.5034127 -4.548223 -4.525383 -4.3941917 -4.1576591 -3.9719794 -3.9565425 -4.0773263 -4.255096 -4.4193773 -4.5491805 -4.6447573 -4.6478181 -4.5883179][-4.430933 -4.4891906 -4.5362039 -4.5311646 -4.4377327 -4.2604613 -4.1247725 -4.1209874 -4.2259078 -4.3779349 -4.5086613 -4.6090565 -4.6839161 -4.6759682 -4.6130543][-4.4273858 -4.4791951 -4.5250068 -4.5291085 -4.4667134 -4.3469143 -4.2668085 -4.2856221 -4.3787618 -4.4963117 -4.5850477 -4.6534147 -4.710649 -4.6992846 -4.6343093][-4.42648 -4.4767756 -4.5243711 -4.5374007 -4.5030618 -4.4313478 -4.3984332 -4.4368634 -4.5124578 -4.585001 -4.6261649 -4.6643686 -4.7148457 -4.713973 -4.6540818][-4.4243479 -4.4742646 -4.5246 -4.5481057 -4.5396523 -4.50533 -4.4964337 -4.5309143 -4.567327 -4.5839596 -4.5807991 -4.605619 -4.66943 -4.697875 -4.6580667]]...]
INFO - root - 2017-12-07 16:51:24.498892: step 32710, loss = 21.16, batch loss = 21.07 (8.4 examples/sec; 0.952 sec/batch; 79h:14m:45s remains)
INFO - root - 2017-12-07 16:51:33.879783: step 32720, loss = 21.54, batch loss = 21.45 (8.8 examples/sec; 0.911 sec/batch; 75h:52m:03s remains)
INFO - root - 2017-12-07 16:51:43.428579: step 32730, loss = 21.38, batch loss = 21.30 (8.4 examples/sec; 0.947 sec/batch; 78h:51m:24s remains)
INFO - root - 2017-12-07 16:51:52.746407: step 32740, loss = 21.01, batch loss = 20.93 (8.8 examples/sec; 0.905 sec/batch; 75h:21m:43s remains)
INFO - root - 2017-12-07 16:52:02.248456: step 32750, loss = 21.68, batch loss = 21.60 (8.2 examples/sec; 0.973 sec/batch; 80h:58m:45s remains)
INFO - root - 2017-12-07 16:52:11.583365: step 32760, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.925 sec/batch; 77h:02m:31s remains)
INFO - root - 2017-12-07 16:52:20.965672: step 32770, loss = 21.89, batch loss = 21.81 (8.7 examples/sec; 0.920 sec/batch; 76h:35m:56s remains)
INFO - root - 2017-12-07 16:52:30.299301: step 32780, loss = 21.88, batch loss = 21.80 (7.9 examples/sec; 1.013 sec/batch; 84h:17m:52s remains)
INFO - root - 2017-12-07 16:52:39.456016: step 32790, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.950 sec/batch; 79h:06m:57s remains)
INFO - root - 2017-12-07 16:52:48.916219: step 32800, loss = 21.51, batch loss = 21.42 (8.2 examples/sec; 0.978 sec/batch; 81h:24m:20s remains)
2017-12-07 16:52:49.867835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4430451 -4.4458728 -4.4159284 -4.3713007 -4.339498 -4.3309536 -4.3207822 -4.2834196 -4.2167206 -4.1794138 -4.2125053 -4.2856231 -4.3715768 -4.4319081 -4.4270921][-4.3977313 -4.383316 -4.3388581 -4.2847824 -4.2610917 -4.2877831 -4.3321037 -4.3398867 -4.2962222 -4.26044 -4.269516 -4.3139372 -4.3810029 -4.4298415 -4.4163284][-4.3332162 -4.29272 -4.2266803 -4.1613221 -4.1470642 -4.2100997 -4.3095794 -4.3627553 -4.3408327 -4.3021336 -4.2848439 -4.2981367 -4.3388076 -4.372539 -4.361412][-4.2907271 -4.2330146 -4.1516819 -4.0752759 -4.054328 -4.117342 -4.2284946 -4.2986579 -4.2898531 -4.2536035 -4.2268147 -4.2253165 -4.2464523 -4.2707887 -4.27683][-4.2995844 -4.2485361 -4.17082 -4.0862551 -4.0369186 -4.0514164 -4.118957 -4.1748538 -4.178112 -4.16208 -4.154326 -4.1610742 -4.1793 -4.2058396 -4.2316995][-4.3114891 -4.2832909 -4.2140179 -4.1085234 -4.0049644 -3.9423769 -3.9500437 -3.9930551 -4.0287476 -4.062768 -4.105195 -4.1500473 -4.1977506 -4.2493925 -4.2907758][-4.3179851 -4.312994 -4.2417355 -4.0932169 -3.9118352 -3.7658162 -3.7229733 -3.7697558 -3.8578556 -3.964344 -4.0789623 -4.1858468 -4.28781 -4.3777027 -4.421452][-4.3381948 -4.3513923 -4.2794466 -4.0948839 -3.8474383 -3.6365643 -3.5597143 -3.617265 -3.7529557 -3.914346 -4.0783272 -4.2306056 -4.3765144 -4.4934525 -4.5266886][-4.3841729 -4.408031 -4.3466797 -4.1612649 -3.8933372 -3.656074 -3.5604844 -3.618392 -3.7691238 -3.9412818 -4.104135 -4.2550559 -4.4071441 -4.5273027 -4.5491428][-4.4565773 -4.4849272 -4.4423795 -4.2934537 -4.0618129 -3.8424935 -3.7430835 -3.7845125 -3.90933 -4.050704 -4.1806459 -4.2990985 -4.4227347 -4.5220752 -4.5320935][-4.5164347 -4.546186 -4.530477 -4.4459496 -4.29463 -4.1344132 -4.0482678 -4.06147 -4.1366534 -4.2297053 -4.3189979 -4.3967934 -4.4731221 -4.5324364 -4.5284185][-4.5290279 -4.5515943 -4.5540628 -4.5246696 -4.4562216 -4.371623 -4.3163476 -4.3113871 -4.3446021 -4.3963251 -4.4522381 -4.499167 -4.5382929 -4.5635834 -4.5532436][-4.502614 -4.5158691 -4.5245 -4.5242162 -4.5089512 -4.4839015 -4.4622827 -4.4546051 -4.4643688 -4.4882083 -4.5185347 -4.545773 -4.5663471 -4.5760918 -4.56828][-4.454145 -4.462193 -4.4733071 -4.4834242 -4.4888091 -4.4901981 -4.4875326 -4.48367 -4.4849072 -4.493732 -4.5075526 -4.52324 -4.537672 -4.5463028 -4.5480437][-4.3822646 -4.3890262 -4.39906 -4.4089112 -4.4158587 -4.420969 -4.4231048 -4.4221244 -4.4225774 -4.4267411 -4.4349227 -4.4472342 -4.4617682 -4.4760189 -4.4885378]]...]
INFO - root - 2017-12-07 16:52:59.191388: step 32810, loss = 21.42, batch loss = 21.34 (8.3 examples/sec; 0.969 sec/batch; 80h:37m:55s remains)
INFO - root - 2017-12-07 16:53:08.608166: step 32820, loss = 21.39, batch loss = 21.31 (7.6 examples/sec; 1.059 sec/batch; 88h:10m:43s remains)
INFO - root - 2017-12-07 16:53:17.915647: step 32830, loss = 21.43, batch loss = 21.35 (8.5 examples/sec; 0.941 sec/batch; 78h:19m:00s remains)
INFO - root - 2017-12-07 16:53:27.281006: step 32840, loss = 21.69, batch loss = 21.61 (8.4 examples/sec; 0.957 sec/batch; 79h:39m:24s remains)
INFO - root - 2017-12-07 16:53:36.759098: step 32850, loss = 21.22, batch loss = 21.13 (8.6 examples/sec; 0.935 sec/batch; 77h:49m:36s remains)
INFO - root - 2017-12-07 16:53:46.080386: step 32860, loss = 21.47, batch loss = 21.39 (8.3 examples/sec; 0.964 sec/batch; 80h:15m:15s remains)
INFO - root - 2017-12-07 16:53:55.368606: step 32870, loss = 21.28, batch loss = 21.20 (8.4 examples/sec; 0.947 sec/batch; 78h:48m:58s remains)
INFO - root - 2017-12-07 16:54:04.686707: step 32880, loss = 21.81, batch loss = 21.72 (8.8 examples/sec; 0.913 sec/batch; 75h:56m:48s remains)
INFO - root - 2017-12-07 16:54:13.993336: step 32890, loss = 21.44, batch loss = 21.36 (9.0 examples/sec; 0.884 sec/batch; 73h:35m:51s remains)
INFO - root - 2017-12-07 16:54:23.353674: step 32900, loss = 21.61, batch loss = 21.53 (8.4 examples/sec; 0.951 sec/batch; 79h:06m:28s remains)
2017-12-07 16:54:24.327068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.404829 -4.4047651 -4.4163036 -4.4425159 -4.4708204 -4.4852214 -4.4886961 -4.4815946 -4.4634657 -4.449151 -4.43838 -4.438849 -4.4627814 -4.5080571 -4.5570731][-4.4531527 -4.4593077 -4.4826794 -4.5202041 -4.554831 -4.5701323 -4.570261 -4.5553455 -4.5253806 -4.4984784 -4.4857545 -4.4914694 -4.5165195 -4.5552192 -4.5903378][-4.5012383 -4.5099478 -4.5415897 -4.5850172 -4.6144509 -4.6124735 -4.5843997 -4.5325918 -4.4682827 -4.423264 -4.4186783 -4.446012 -4.4880228 -4.537889 -4.5740786][-4.5311732 -4.5318518 -4.5562015 -4.5905466 -4.601388 -4.5685797 -4.4963675 -4.3908725 -4.280817 -4.2172165 -4.2309303 -4.2983208 -4.3785043 -4.4610186 -4.5178332][-4.5087562 -4.486207 -4.4955683 -4.5222669 -4.5254126 -4.4754376 -4.3684077 -4.2146544 -4.067697 -3.9949114 -4.0298538 -4.1392465 -4.2556233 -4.3661213 -4.4419169][-4.41229 -4.3575187 -4.3557982 -4.3918462 -4.4124603 -4.3695078 -4.2413526 -4.0495892 -3.8817368 -3.8184843 -3.8867555 -4.0405655 -4.1851768 -4.3070517 -4.3878293][-4.2667546 -4.18167 -4.1777744 -4.2344956 -4.2857394 -4.2616115 -4.1211205 -3.9052706 -3.7358611 -3.701807 -3.8147836 -4.0102596 -4.1772795 -4.3036661 -4.3825808][-4.1271544 -4.0333271 -4.0412192 -4.1229019 -4.2078071 -4.21034 -4.0765257 -3.8652833 -3.7153585 -3.7120569 -3.8501904 -4.05345 -4.2222586 -4.34546 -4.417335][-4.0485096 -3.9816906 -4.0203233 -4.1254573 -4.2350426 -4.265028 -4.1620345 -3.9871879 -3.8652821 -3.86795 -3.9872909 -4.15436 -4.2984138 -4.4052029 -4.4649367][-4.0489798 -4.04345 -4.1213012 -4.2308497 -4.3311138 -4.363205 -4.2863126 -4.1479621 -4.0442376 -4.0343719 -4.1130471 -4.224905 -4.3344436 -4.4228349 -4.4798412][-4.086575 -4.14944 -4.2519593 -4.3410788 -4.402041 -4.4164677 -4.3556962 -4.2438426 -4.1504712 -4.1254048 -4.1633549 -4.2268848 -4.3107338 -4.39417 -4.4600687][-4.1180663 -4.2203417 -4.32185 -4.3788347 -4.3968563 -4.392015 -4.3418584 -4.2487497 -4.1613631 -4.1205225 -4.128283 -4.16678 -4.2460232 -4.3403573 -4.4213953][-4.1522579 -4.2573547 -4.3383641 -4.3653336 -4.3551917 -4.3454742 -4.3183303 -4.2515879 -4.1660643 -4.0991864 -4.073266 -4.0933213 -4.1733513 -4.279552 -4.3719764][-4.2050104 -4.3021107 -4.3674641 -4.3767061 -4.3542542 -4.3485169 -4.3510485 -4.3200336 -4.2436409 -4.1535311 -4.0979791 -4.09604 -4.1592712 -4.255136 -4.3363047][-4.2692041 -4.3595319 -4.4237051 -4.4351263 -4.4164443 -4.4113421 -4.4228354 -4.4085774 -4.3374162 -4.2377195 -4.1700559 -4.1467552 -4.1787071 -4.2444568 -4.3018384]]...]
INFO - root - 2017-12-07 16:54:33.727728: step 32910, loss = 21.31, batch loss = 21.22 (8.9 examples/sec; 0.898 sec/batch; 74h:44m:00s remains)
INFO - root - 2017-12-07 16:54:43.050112: step 32920, loss = 21.51, batch loss = 21.42 (8.9 examples/sec; 0.898 sec/batch; 74h:43m:54s remains)
INFO - root - 2017-12-07 16:54:52.331431: step 32930, loss = 21.61, batch loss = 21.53 (8.7 examples/sec; 0.918 sec/batch; 76h:25m:22s remains)
INFO - root - 2017-12-07 16:55:01.586710: step 32940, loss = 21.31, batch loss = 21.23 (8.9 examples/sec; 0.903 sec/batch; 75h:09m:49s remains)
INFO - root - 2017-12-07 16:55:11.085779: step 32950, loss = 21.34, batch loss = 21.26 (8.4 examples/sec; 0.958 sec/batch; 79h:42m:24s remains)
INFO - root - 2017-12-07 16:55:20.334609: step 32960, loss = 21.35, batch loss = 21.27 (8.4 examples/sec; 0.948 sec/batch; 78h:53m:16s remains)
INFO - root - 2017-12-07 16:55:29.576670: step 32970, loss = 21.63, batch loss = 21.54 (8.2 examples/sec; 0.979 sec/batch; 81h:27m:34s remains)
INFO - root - 2017-12-07 16:55:38.955971: step 32980, loss = 20.86, batch loss = 20.78 (7.7 examples/sec; 1.041 sec/batch; 86h:35m:02s remains)
INFO - root - 2017-12-07 16:55:48.285903: step 32990, loss = 21.40, batch loss = 21.31 (8.6 examples/sec; 0.931 sec/batch; 77h:26m:03s remains)
INFO - root - 2017-12-07 16:55:57.639220: step 33000, loss = 21.30, batch loss = 21.21 (8.7 examples/sec; 0.924 sec/batch; 76h:53m:07s remains)
2017-12-07 16:55:58.534641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6413064 -4.6798868 -4.7330794 -4.7657995 -4.7754631 -4.7616143 -4.7360306 -4.7206779 -4.7402534 -4.7896528 -4.8097558 -4.7755389 -4.7126875 -4.6612048 -4.6416092][-4.6306591 -4.65656 -4.6761432 -4.6616063 -4.6456695 -4.6367245 -4.6356015 -4.6496468 -4.7093692 -4.8118768 -4.8765798 -4.8570962 -4.7775412 -4.6967597 -4.6584163][-4.5529523 -4.5585551 -4.5474482 -4.499228 -4.4538302 -4.4270787 -4.4187565 -4.4387183 -4.5254335 -4.6822095 -4.8086615 -4.8251948 -4.755022 -4.6693869 -4.6260757][-4.4382672 -4.4413795 -4.4510059 -4.4349704 -4.3902631 -4.3236246 -4.2556047 -4.2309284 -4.31035 -4.5073977 -4.6903119 -4.7414012 -4.6900244 -4.6164689 -4.5810137][-4.33869 -4.3362446 -4.3896236 -4.4488392 -4.4353576 -4.3184576 -4.142756 -4.02258 -4.0762081 -4.3231306 -4.5813689 -4.6773272 -4.6387963 -4.5697179 -4.533309][-4.3127985 -4.2961864 -4.3660293 -4.4739089 -4.4783397 -4.2890859 -3.9659445 -3.7093494 -3.7305117 -4.0573597 -4.440248 -4.629591 -4.6254029 -4.5566764 -4.5041084][-4.364522 -4.3287206 -4.3623614 -4.441844 -4.4104133 -4.12506 -3.6437294 -3.2336144 -3.2140896 -3.6437125 -4.1962328 -4.539649 -4.6131978 -4.562233 -4.4931083][-4.4256382 -4.3618336 -4.3173585 -4.3074093 -4.209321 -3.8649502 -3.3135834 -2.8214712 -2.7815182 -3.2831442 -3.9526145 -4.4113731 -4.5531735 -4.5292783 -4.4626083][-4.5032635 -4.4239249 -4.3171954 -4.2315435 -4.1018353 -3.7920151 -3.3157496 -2.8702316 -2.8332286 -3.288693 -3.9019561 -4.3237767 -4.4560308 -4.4530368 -4.419467][-4.6465816 -4.5999165 -4.4931111 -4.3870897 -4.2525854 -4.0022664 -3.653543 -3.3366127 -3.3305883 -3.6703434 -4.09203 -4.34126 -4.3917184 -4.4026937 -4.4253988][-4.8329949 -4.840827 -4.7557988 -4.6380215 -4.4736233 -4.2310476 -3.9665267 -3.7874732 -3.8566141 -4.1297936 -4.376895 -4.4455595 -4.4049625 -4.4270644 -4.5054922][-4.9665976 -5.010932 -4.9304667 -4.7868004 -4.5905643 -4.3487406 -4.1414008 -4.0685987 -4.2067113 -4.45927 -4.6084614 -4.5692129 -4.4794674 -4.5132961 -4.6249146][-4.9901805 -5.0580769 -4.9903517 -4.8460531 -4.6637363 -4.4715552 -4.332304 -4.3168063 -4.4586048 -4.6588993 -4.7422414 -4.6611247 -4.56976 -4.6162472 -4.7318144][-4.9315681 -5.0108027 -4.9762354 -4.869873 -4.7398944 -4.6237879 -4.5554008 -4.5606427 -4.6465731 -4.7525949 -4.7728314 -4.6857429 -4.614768 -4.6621566 -4.7573681][-4.8542733 -4.9230742 -4.9221563 -4.8726425 -4.8046122 -4.7471166 -4.7134008 -4.7040248 -4.7186804 -4.7378263 -4.7142162 -4.6394691 -4.58952 -4.6230888 -4.690062]]...]
INFO - root - 2017-12-07 16:56:07.881494: step 33010, loss = 21.46, batch loss = 21.37 (8.9 examples/sec; 0.903 sec/batch; 75h:08m:20s remains)
INFO - root - 2017-12-07 16:56:17.175954: step 33020, loss = 21.41, batch loss = 21.32 (8.8 examples/sec; 0.904 sec/batch; 75h:13m:40s remains)
INFO - root - 2017-12-07 16:56:26.320139: step 33030, loss = 21.50, batch loss = 21.42 (9.0 examples/sec; 0.890 sec/batch; 74h:00m:18s remains)
INFO - root - 2017-12-07 16:56:35.690860: step 33040, loss = 21.44, batch loss = 21.36 (8.1 examples/sec; 0.993 sec/batch; 82h:38m:23s remains)
INFO - root - 2017-12-07 16:56:45.078822: step 33050, loss = 21.22, batch loss = 21.14 (8.4 examples/sec; 0.956 sec/batch; 79h:32m:24s remains)
INFO - root - 2017-12-07 16:56:54.489545: step 33060, loss = 21.86, batch loss = 21.77 (8.6 examples/sec; 0.935 sec/batch; 77h:43m:53s remains)
INFO - root - 2017-12-07 16:57:03.631140: step 33070, loss = 21.59, batch loss = 21.50 (8.9 examples/sec; 0.899 sec/batch; 74h:46m:29s remains)
INFO - root - 2017-12-07 16:57:12.950690: step 33080, loss = 21.10, batch loss = 21.01 (8.8 examples/sec; 0.910 sec/batch; 75h:42m:10s remains)
INFO - root - 2017-12-07 16:57:22.355842: step 33090, loss = 21.40, batch loss = 21.32 (8.5 examples/sec; 0.942 sec/batch; 78h:18m:21s remains)
INFO - root - 2017-12-07 16:57:31.704737: step 33100, loss = 21.44, batch loss = 21.36 (8.5 examples/sec; 0.942 sec/batch; 78h:21m:06s remains)
2017-12-07 16:57:32.695316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6439481 -4.605207 -4.5529771 -4.5354 -4.5720787 -4.6167731 -4.6090302 -4.5459747 -4.4546633 -4.3699536 -4.351428 -4.4155722 -4.4992471 -4.5325027 -4.5051517][-4.6401119 -4.5799775 -4.4953461 -4.4679589 -4.5168171 -4.5572639 -4.5072026 -4.395792 -4.3005776 -4.2578988 -4.2834644 -4.3690333 -4.4473691 -4.4633217 -4.4196219][-4.5916042 -4.5079818 -4.4005461 -4.3727722 -4.4281912 -4.4444871 -4.342484 -4.2030029 -4.13681 -4.158843 -4.2325273 -4.3282232 -4.3878932 -4.37414 -4.3118162][-4.532753 -4.4289074 -4.3155541 -4.295279 -4.3461847 -4.329464 -4.192142 -4.0509357 -4.02057 -4.0924158 -4.202157 -4.3072228 -4.3564162 -4.3247781 -4.2606096][-4.4885054 -4.3664746 -4.2517242 -4.2323189 -4.2682595 -4.2283983 -4.0881333 -3.9606724 -3.9490349 -4.0384889 -4.1619864 -4.2753348 -4.3283005 -4.3042307 -4.26339][-4.4755297 -4.3368564 -4.2205296 -4.1979194 -4.2175813 -4.1708393 -4.0510817 -3.9466848 -3.9445367 -4.035502 -4.1515603 -4.2535563 -4.3031712 -4.2932062 -4.27959][-4.4674268 -4.31589 -4.203805 -4.1867933 -4.2042613 -4.1622014 -4.064734 -3.9791968 -3.9863253 -4.0771656 -4.1748743 -4.2518291 -4.2945132 -4.3016415 -4.3072362][-4.447618 -4.2923746 -4.1961379 -4.1961465 -4.2243638 -4.1939764 -4.1071153 -4.0253415 -4.0310192 -4.1155968 -4.195137 -4.2466049 -4.2794895 -4.3021064 -4.32538][-4.4173727 -4.2781668 -4.2096949 -4.2338734 -4.2747054 -4.2498198 -4.1603532 -4.0667171 -4.0544639 -4.1182337 -4.1825113 -4.2200718 -4.2512321 -4.2919273 -4.3359694][-4.3842244 -4.2753973 -4.2354741 -4.27834 -4.3230042 -4.2996497 -4.2076874 -4.099329 -4.05534 -4.0892196 -4.1471763 -4.1945925 -4.2400804 -4.2956333 -4.3573594][-4.3194432 -4.2320328 -4.215879 -4.2699251 -4.3137441 -4.2989583 -4.2283607 -4.1341338 -4.0755234 -4.0868049 -4.1466651 -4.21051 -4.2619882 -4.3061962 -4.3497739][-4.1971507 -4.1046286 -4.0995278 -4.1582003 -4.2030349 -4.2107263 -4.1895814 -4.149415 -4.1150517 -4.12123 -4.1794424 -4.2463202 -4.2828517 -4.287045 -4.2768841][-4.1008682 -3.99621 -3.979773 -4.0182042 -4.0520458 -4.0818658 -4.1168685 -4.14488 -4.1561942 -4.1622696 -4.2012172 -4.2588344 -4.28301 -4.2548118 -4.1905479][-4.0993166 -4.0062361 -3.9822786 -3.9853573 -3.9885008 -4.0266428 -4.105886 -4.1879997 -4.2317548 -4.2185531 -4.2104144 -4.2480545 -4.2785749 -4.2563782 -4.1703978][-4.1838689 -4.1133709 -4.0907884 -4.0667896 -4.0412221 -4.0761528 -4.1806049 -4.2910175 -4.3423738 -4.2976632 -4.2351341 -4.2438636 -4.2844162 -4.2885275 -4.212029]]...]
INFO - root - 2017-12-07 16:57:41.941232: step 33110, loss = 21.60, batch loss = 21.52 (8.9 examples/sec; 0.900 sec/batch; 74h:50m:57s remains)
INFO - root - 2017-12-07 16:57:51.300697: step 33120, loss = 21.26, batch loss = 21.18 (8.5 examples/sec; 0.946 sec/batch; 78h:41m:25s remains)
INFO - root - 2017-12-07 16:58:00.571136: step 33130, loss = 21.68, batch loss = 21.60 (8.8 examples/sec; 0.909 sec/batch; 75h:33m:04s remains)
INFO - root - 2017-12-07 16:58:10.018896: step 33140, loss = 21.07, batch loss = 20.99 (9.0 examples/sec; 0.887 sec/batch; 73h:46m:18s remains)
INFO - root - 2017-12-07 16:58:19.402474: step 33150, loss = 21.79, batch loss = 21.71 (8.9 examples/sec; 0.901 sec/batch; 74h:57m:35s remains)
INFO - root - 2017-12-07 16:58:28.892599: step 33160, loss = 21.11, batch loss = 21.03 (8.6 examples/sec; 0.931 sec/batch; 77h:24m:54s remains)
INFO - root - 2017-12-07 16:58:38.115012: step 33170, loss = 21.53, batch loss = 21.44 (8.4 examples/sec; 0.957 sec/batch; 79h:33m:10s remains)
INFO - root - 2017-12-07 16:58:47.530057: step 33180, loss = 21.56, batch loss = 21.48 (8.3 examples/sec; 0.966 sec/batch; 80h:17m:44s remains)
INFO - root - 2017-12-07 16:58:56.812277: step 33190, loss = 21.42, batch loss = 21.33 (8.8 examples/sec; 0.910 sec/batch; 75h:37m:47s remains)
INFO - root - 2017-12-07 16:59:06.070342: step 33200, loss = 21.41, batch loss = 21.33 (8.5 examples/sec; 0.946 sec/batch; 78h:38m:40s remains)
2017-12-07 16:59:06.994438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5408559 -4.5716043 -4.5831265 -4.584 -4.5828238 -4.5790071 -4.5714951 -4.5788093 -4.6244268 -4.6901135 -4.7310333 -4.720727 -4.6679668 -4.6067915 -4.566371][-4.6204486 -4.63691 -4.626739 -4.6086864 -4.5950332 -4.5821748 -4.5739732 -4.5920978 -4.6578121 -4.738945 -4.774147 -4.7386451 -4.6578155 -4.5817437 -4.5322046][-4.65737 -4.6544228 -4.6225419 -4.5812726 -4.5434318 -4.5112052 -4.4966788 -4.5286932 -4.6263576 -4.7411747 -4.7902284 -4.7458091 -4.6463857 -4.5560131 -4.4933786][-4.646461 -4.6371932 -4.5964537 -4.5305948 -4.4515729 -4.37049 -4.3233414 -4.3634477 -4.5112982 -4.6928878 -4.7914591 -4.7613392 -4.6383395 -4.5056868 -4.4077907][-4.602633 -4.592649 -4.5403066 -4.4388065 -4.2979836 -4.1314735 -4.0146313 -4.0474977 -4.26329 -4.5450525 -4.7342305 -4.7530146 -4.612886 -4.421073 -4.2705922][-4.5536838 -4.5338511 -4.4506712 -4.2985716 -4.079349 -3.8031051 -3.585186 -3.5867734 -3.8670096 -4.2603726 -4.5686646 -4.6809173 -4.5732784 -4.3591952 -4.1671481][-4.5274057 -4.4996171 -4.3902636 -4.1974521 -3.9103065 -3.53613 -3.2268481 -3.1921773 -3.5087376 -3.9752595 -4.3718719 -4.5782909 -4.5422912 -4.3617396 -4.1594338][-4.5265255 -4.50942 -4.4124446 -4.2257981 -3.9268532 -3.5251098 -3.1862068 -3.1271408 -3.4222484 -3.8710854 -4.2672076 -4.5071111 -4.5373082 -4.4289842 -4.2661319][-4.530261 -4.5365191 -4.4778252 -4.3343511 -4.0842366 -3.7396107 -3.4475923 -3.3911061 -3.6238816 -3.9841018 -4.3045382 -4.5115314 -4.5721946 -4.5337639 -4.4338551][-4.4904666 -4.5287924 -4.5213728 -4.4439659 -4.2854905 -4.0597072 -3.8697982 -3.8475468 -4.02188 -4.2794623 -4.4961028 -4.6212096 -4.6519504 -4.6336951 -4.5799222][-4.3884878 -4.4666271 -4.5175352 -4.5162754 -4.4643121 -4.3671823 -4.2783508 -4.2876596 -4.4113278 -4.5803485 -4.7093635 -4.7562809 -4.7375746 -4.71239 -4.6897535][-4.2603788 -4.3710127 -4.4652514 -4.5205607 -4.5426316 -4.5309052 -4.5052691 -4.521235 -4.5948896 -4.6952724 -4.774754 -4.7917652 -4.7609921 -4.7411036 -4.7409067][-4.1564231 -4.2710223 -4.3797569 -4.45831 -4.5085363 -4.5297866 -4.5353432 -4.55461 -4.5968986 -4.6514406 -4.7017212 -4.7169676 -4.7006626 -4.6932235 -4.6979566][-4.1386814 -4.2296138 -4.3264837 -4.3984261 -4.4407086 -4.4573445 -4.4740467 -4.5054908 -4.5414028 -4.5696692 -4.5924373 -4.603519 -4.6028709 -4.6063824 -4.6076746][-4.2207575 -4.2708125 -4.3401632 -4.3918552 -4.4104109 -4.4033346 -4.4129915 -4.4494209 -4.4862866 -4.5046372 -4.5106344 -4.5145097 -4.5238023 -4.5368395 -4.5393009]]...]
INFO - root - 2017-12-07 16:59:16.441012: step 33210, loss = 21.32, batch loss = 21.24 (8.2 examples/sec; 0.982 sec/batch; 81h:35m:56s remains)
INFO - root - 2017-12-07 16:59:25.948482: step 33220, loss = 21.35, batch loss = 21.27 (8.2 examples/sec; 0.971 sec/batch; 80h:41m:23s remains)
INFO - root - 2017-12-07 16:59:35.300883: step 33230, loss = 21.45, batch loss = 21.37 (7.9 examples/sec; 1.008 sec/batch; 83h:49m:58s remains)
INFO - root - 2017-12-07 16:59:44.685767: step 33240, loss = 21.64, batch loss = 21.55 (7.9 examples/sec; 1.011 sec/batch; 84h:02m:41s remains)
INFO - root - 2017-12-07 16:59:53.953677: step 33250, loss = 21.15, batch loss = 21.06 (8.9 examples/sec; 0.904 sec/batch; 75h:07m:59s remains)
INFO - root - 2017-12-07 17:00:03.255442: step 33260, loss = 21.13, batch loss = 21.05 (8.5 examples/sec; 0.947 sec/batch; 78h:40m:47s remains)
INFO - root - 2017-12-07 17:00:12.475691: step 33270, loss = 21.55, batch loss = 21.46 (9.9 examples/sec; 0.811 sec/batch; 67h:23m:23s remains)
INFO - root - 2017-12-07 17:00:21.872278: step 33280, loss = 21.54, batch loss = 21.46 (8.3 examples/sec; 0.966 sec/batch; 80h:16m:50s remains)
INFO - root - 2017-12-07 17:00:31.206373: step 33290, loss = 21.32, batch loss = 21.23 (9.9 examples/sec; 0.805 sec/batch; 66h:52m:55s remains)
INFO - root - 2017-12-07 17:00:40.291877: step 33300, loss = 21.31, batch loss = 21.23 (10.4 examples/sec; 0.767 sec/batch; 63h:46m:54s remains)
2017-12-07 17:00:41.211456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7454348 -4.7545214 -4.7550569 -4.7490339 -4.6900797 -4.604095 -4.5498118 -4.5778451 -4.6912584 -4.8043537 -4.866807 -4.8632164 -4.7749243 -4.6312566 -4.4726644][-4.770412 -4.7913036 -4.7968845 -4.7761459 -4.6772156 -4.5367084 -4.4379678 -4.4452629 -4.570991 -4.7058263 -4.7945089 -4.8261881 -4.7680211 -4.6411238 -4.4844875][-4.7257209 -4.7704873 -4.792233 -4.761745 -4.62657 -4.4329538 -4.2873178 -4.2694712 -4.4013305 -4.5590844 -4.6766496 -4.7488022 -4.7279654 -4.6265392 -4.4835529][-4.6412678 -4.7042212 -4.7386231 -4.6997633 -4.5375586 -4.3066874 -4.1293035 -4.0924072 -4.2265434 -4.40439 -4.5556445 -4.673964 -4.6925974 -4.6136384 -4.4831095][-4.5813985 -4.6444011 -4.6751137 -4.6226811 -4.4429803 -4.1969905 -4.010921 -3.9688151 -4.10253 -4.2915268 -4.4688683 -4.6255536 -4.6764936 -4.6136026 -4.4906926][-4.5642924 -4.5984087 -4.5991907 -4.5230594 -4.3355575 -4.0925045 -3.9162812 -3.8879285 -4.0276756 -4.2212577 -4.4146767 -4.5938358 -4.6648922 -4.6151023 -4.4984322][-4.5892158 -4.5755472 -4.5292759 -4.4147773 -4.2073402 -3.9643512 -3.800262 -3.7986169 -3.953923 -4.1549497 -4.36039 -4.5477929 -4.6285286 -4.592989 -4.4888091][-4.6345987 -4.578764 -4.4851527 -4.3294559 -4.1038122 -3.8702958 -3.723228 -3.7379932 -3.8939171 -4.0892506 -4.2961693 -4.4814548 -4.5641565 -4.5435143 -4.4592385][-4.618021 -4.54463 -4.4235067 -4.2492142 -4.0353041 -3.8447385 -3.7356825 -3.762054 -3.9008014 -4.0710192 -4.2588654 -4.4258666 -4.4989443 -4.4872589 -4.4225616][-4.5593944 -4.4847131 -4.3631697 -4.2020087 -4.028862 -3.900681 -3.8396008 -3.8791163 -4.0037575 -4.1482339 -4.3028731 -4.4299555 -4.4704862 -4.4489617 -4.3921385][-4.5543242 -4.4810367 -4.365664 -4.2332139 -4.1111164 -4.0461926 -4.0331082 -4.086133 -4.1972718 -4.3084288 -4.41364 -4.4845839 -4.4784393 -4.4338508 -4.3737974][-4.6086764 -4.534904 -4.4222136 -4.3157387 -4.2350078 -4.2129793 -4.226099 -4.276588 -4.3648567 -4.4428024 -4.5062561 -4.5354619 -4.4978461 -4.4332371 -4.36577][-4.6882176 -4.6199393 -4.5170445 -4.4322824 -4.3738441 -4.36192 -4.3653741 -4.3885474 -4.448575 -4.5078373 -4.5555587 -4.568789 -4.5176373 -4.4381313 -4.3625593][-4.7311225 -4.6779723 -4.5951457 -4.5328064 -4.492167 -4.4780345 -4.456181 -4.4436569 -4.4798293 -4.5364475 -4.5866351 -4.5939159 -4.5326886 -4.4392438 -4.3574905][-4.6908512 -4.6559529 -4.6002665 -4.5631814 -4.54141 -4.5239372 -4.4816785 -4.4457121 -4.4706287 -4.5354438 -4.595603 -4.6016064 -4.5329914 -4.4304504 -4.347702]]...]
INFO - root - 2017-12-07 17:00:50.523123: step 33310, loss = 21.51, batch loss = 21.43 (8.6 examples/sec; 0.935 sec/batch; 77h:44m:00s remains)
INFO - root - 2017-12-07 17:01:00.020874: step 33320, loss = 21.55, batch loss = 21.47 (7.9 examples/sec; 1.011 sec/batch; 84h:02m:53s remains)
INFO - root - 2017-12-07 17:01:09.455900: step 33330, loss = 21.62, batch loss = 21.54 (8.5 examples/sec; 0.936 sec/batch; 77h:47m:15s remains)
INFO - root - 2017-12-07 17:01:18.819780: step 33340, loss = 21.56, batch loss = 21.47 (8.3 examples/sec; 0.962 sec/batch; 79h:56m:50s remains)
INFO - root - 2017-12-07 17:01:28.267994: step 33350, loss = 21.17, batch loss = 21.09 (8.4 examples/sec; 0.957 sec/batch; 79h:30m:59s remains)
INFO - root - 2017-12-07 17:01:37.566483: step 33360, loss = 21.18, batch loss = 21.10 (8.3 examples/sec; 0.962 sec/batch; 79h:55m:16s remains)
INFO - root - 2017-12-07 17:01:47.052046: step 33370, loss = 21.18, batch loss = 21.10 (9.0 examples/sec; 0.889 sec/batch; 73h:53m:22s remains)
INFO - root - 2017-12-07 17:01:56.423643: step 33380, loss = 21.87, batch loss = 21.79 (8.8 examples/sec; 0.906 sec/batch; 75h:15m:52s remains)
INFO - root - 2017-12-07 17:02:05.820633: step 33390, loss = 21.48, batch loss = 21.40 (8.7 examples/sec; 0.920 sec/batch; 76h:25m:19s remains)
INFO - root - 2017-12-07 17:02:15.383023: step 33400, loss = 21.43, batch loss = 21.35 (8.2 examples/sec; 0.970 sec/batch; 80h:34m:27s remains)
2017-12-07 17:02:16.295809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3664651 -4.38481 -4.3969908 -4.4394593 -4.51693 -4.5769324 -4.6070809 -4.6057339 -4.5991197 -4.6119304 -4.6290731 -4.6321898 -4.6422572 -4.6468258 -4.5785294][-4.3655348 -4.3883524 -4.4238391 -4.483726 -4.5695825 -4.6219335 -4.6270218 -4.6120424 -4.6111412 -4.6476126 -4.6940384 -4.7157054 -4.7222753 -4.6976604 -4.5834503][-4.3988051 -4.428668 -4.4854436 -4.54034 -4.5851641 -4.5710773 -4.5073104 -4.4611754 -4.472806 -4.5585222 -4.665236 -4.72266 -4.7341771 -4.694057 -4.5649619][-4.4623294 -4.4999046 -4.5541477 -4.5670972 -4.5217261 -4.4002671 -4.2493167 -4.1736665 -4.2108374 -4.3599534 -4.54454 -4.6489091 -4.6667833 -4.6268873 -4.5166903][-4.5402927 -4.5694551 -4.5841627 -4.5276089 -4.3819385 -4.1578994 -3.935236 -3.8465533 -3.9150476 -4.1166162 -4.3720679 -4.52871 -4.55641 -4.5266418 -4.4597187][-4.5935121 -4.5949554 -4.5515766 -4.4348583 -4.2263942 -3.9413764 -3.6708963 -3.5762162 -3.6724474 -3.9012544 -4.1926165 -4.3920693 -4.4426656 -4.4405775 -4.4310808][-4.6255188 -4.604537 -4.5165486 -4.3696508 -4.1412296 -3.8383446 -3.5450602 -3.4472 -3.5694332 -3.8084054 -4.1017809 -4.3243575 -4.4066486 -4.436615 -4.4668078][-4.6191187 -4.583849 -4.4847927 -4.3455219 -4.1380167 -3.8680685 -3.6028252 -3.5149579 -3.6454041 -3.8762689 -4.142704 -4.3617587 -4.4714837 -4.5270467 -4.5685921][-4.563724 -4.5133538 -4.4229302 -4.3139625 -4.162035 -3.972558 -3.7858605 -3.7279155 -3.8431129 -4.0451779 -4.2732081 -4.468802 -4.5861239 -4.6495647 -4.67865][-4.4959397 -4.4258046 -4.336318 -4.2541556 -4.1711512 -4.0871954 -4.0118852 -4.0070486 -4.0970511 -4.246757 -4.4173388 -4.5596662 -4.6465225 -4.6903872 -4.6932492][-4.4165807 -4.3220353 -4.224905 -4.1647482 -4.1527271 -4.1792674 -4.2206779 -4.2800927 -4.3538895 -4.4454207 -4.5446529 -4.6138639 -4.6377792 -4.6383209 -4.61233][-4.325799 -4.2159181 -4.1166625 -4.0747714 -4.1177917 -4.22465 -4.3443475 -4.4416771 -4.5018272 -4.5532951 -4.6031837 -4.6149888 -4.5775175 -4.5357938 -4.4897604][-4.2566433 -4.1395698 -4.046875 -4.0172043 -4.0861692 -4.226244 -4.370842 -4.4679203 -4.5133781 -4.5474257 -4.5765085 -4.5599365 -4.49 -4.4272733 -4.3769221][-4.2581596 -4.1507578 -4.0783272 -4.0639253 -4.1381421 -4.27089 -4.3884563 -4.4495473 -4.4710245 -4.4921513 -4.5108433 -4.4857612 -4.40999 -4.3460979 -4.3019719][-4.3358889 -4.2521482 -4.2085118 -4.210187 -4.274641 -4.3710108 -4.433589 -4.4446993 -4.4383163 -4.4435873 -4.4501038 -4.4215794 -4.3550034 -4.3039355 -4.2723804]]...]
INFO - root - 2017-12-07 17:02:25.783950: step 33410, loss = 21.67, batch loss = 21.59 (8.6 examples/sec; 0.932 sec/batch; 77h:27m:40s remains)
INFO - root - 2017-12-07 17:02:35.178627: step 33420, loss = 21.62, batch loss = 21.53 (8.9 examples/sec; 0.898 sec/batch; 74h:37m:57s remains)
INFO - root - 2017-12-07 17:02:44.613932: step 33430, loss = 21.77, batch loss = 21.69 (8.6 examples/sec; 0.933 sec/batch; 77h:29m:09s remains)
INFO - root - 2017-12-07 17:02:53.913257: step 33440, loss = 21.48, batch loss = 21.40 (8.1 examples/sec; 0.982 sec/batch; 81h:35m:00s remains)
INFO - root - 2017-12-07 17:03:03.374065: step 33450, loss = 21.48, batch loss = 21.40 (8.3 examples/sec; 0.966 sec/batch; 80h:13m:58s remains)
INFO - root - 2017-12-07 17:03:12.742274: step 33460, loss = 21.04, batch loss = 20.96 (8.7 examples/sec; 0.921 sec/batch; 76h:31m:23s remains)
INFO - root - 2017-12-07 17:03:22.156762: step 33470, loss = 21.20, batch loss = 21.11 (8.9 examples/sec; 0.899 sec/batch; 74h:38m:26s remains)
INFO - root - 2017-12-07 17:03:31.468485: step 33480, loss = 21.62, batch loss = 21.54 (8.1 examples/sec; 0.987 sec/batch; 81h:56m:41s remains)
INFO - root - 2017-12-07 17:03:40.770668: step 33490, loss = 21.61, batch loss = 21.52 (8.4 examples/sec; 0.949 sec/batch; 78h:48m:04s remains)
INFO - root - 2017-12-07 17:03:50.179987: step 33500, loss = 21.56, batch loss = 21.48 (8.6 examples/sec; 0.925 sec/batch; 76h:51m:26s remains)
2017-12-07 17:03:51.164716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.368495 -4.44477 -4.5200658 -4.5469675 -4.5157409 -4.4485655 -4.3823547 -4.3589745 -4.3877497 -4.4453273 -4.4744806 -4.448173 -4.3951826 -4.3316832 -4.262928][-4.4264073 -4.5207148 -4.5930672 -4.5897608 -4.5076542 -4.3849888 -4.2775874 -4.2383633 -4.2756348 -4.3581057 -4.4103718 -4.3887615 -4.3346353 -4.2785263 -4.215374][-4.4716315 -4.5757513 -4.6294913 -4.5804415 -4.4387732 -4.2626715 -4.1262536 -4.0932164 -4.1624432 -4.2831125 -4.3573165 -4.3267126 -4.2563448 -4.2008243 -4.1538157][-4.494143 -4.5917435 -4.6119986 -4.5057664 -4.3022757 -4.0756855 -3.915087 -3.8971698 -4.0083475 -4.1821809 -4.2940869 -4.265152 -4.1841235 -4.1302156 -4.1073875][-4.498692 -4.5798287 -4.5607386 -4.3985791 -4.1437931 -3.871882 -3.6809351 -3.6667559 -3.8086214 -4.039361 -4.2100396 -4.2122021 -4.1463938 -4.1065059 -4.1124792][-4.5012479 -4.5649343 -4.5149369 -4.3175106 -4.0355949 -3.7347929 -3.5184474 -3.4984629 -3.6515143 -3.9195471 -4.1450014 -4.1932116 -4.1610842 -4.1448092 -4.1782794][-4.5140462 -4.569643 -4.5080757 -4.3030386 -4.0240178 -3.7251658 -3.5072067 -3.4854889 -3.6336029 -3.9049406 -4.1500854 -4.2307787 -4.2348008 -4.2449579 -4.2997103][-4.5338221 -4.59263 -4.5356112 -4.3438926 -4.0935698 -3.8307927 -3.6428685 -3.6285868 -3.7626452 -4.0083218 -4.2304826 -4.3165555 -4.3443327 -4.37685 -4.44351][-4.5507045 -4.6138887 -4.5677195 -4.3977723 -4.1829486 -3.9714532 -3.833957 -3.8377795 -3.962234 -4.1751471 -4.3528986 -4.4240494 -4.4612265 -4.5052576 -4.5688782][-4.5523124 -4.6193032 -4.5875473 -4.4415359 -4.2560034 -4.0906444 -4.00547 -4.040153 -4.1689296 -4.3524146 -4.4820576 -4.5296869 -4.560637 -4.595799 -4.6313114][-4.5255418 -4.5950828 -4.5859652 -4.4740696 -4.3188133 -4.191237 -4.1565113 -4.227591 -4.3598146 -4.505147 -4.5833945 -4.6010547 -4.6102457 -4.6172576 -4.6080647][-4.4705224 -4.5399847 -4.5632458 -4.5052605 -4.3988166 -4.309731 -4.3083825 -4.3961816 -4.5093756 -4.6021123 -4.6293712 -4.616643 -4.6002984 -4.57316 -4.5176115][-4.3969593 -4.4633 -4.5199461 -4.5284691 -4.4916415 -4.4468927 -4.4564548 -4.5244842 -4.5911584 -4.6237397 -4.6094513 -4.5774035 -4.5426884 -4.4886794 -4.398097][-4.3284979 -4.3875074 -4.4682379 -4.5361972 -4.5682406 -4.5654955 -4.571609 -4.5965233 -4.6036191 -4.5846748 -4.5484347 -4.5129046 -4.4726572 -4.4067588 -4.3042445][-4.2859449 -4.3369546 -4.4258418 -4.5229325 -4.593956 -4.617476 -4.6179934 -4.6057963 -4.5674148 -4.52131 -4.4849925 -4.4593062 -4.4220676 -4.3574133 -4.2640061]]...]
INFO - root - 2017-12-07 17:04:00.584064: step 33510, loss = 21.66, batch loss = 21.57 (8.2 examples/sec; 0.972 sec/batch; 80h:45m:02s remains)
INFO - root - 2017-12-07 17:04:09.989864: step 33520, loss = 21.44, batch loss = 21.35 (8.3 examples/sec; 0.962 sec/batch; 79h:55m:01s remains)
INFO - root - 2017-12-07 17:04:19.598334: step 33530, loss = 21.22, batch loss = 21.14 (8.7 examples/sec; 0.922 sec/batch; 76h:33m:50s remains)
INFO - root - 2017-12-07 17:04:28.968979: step 33540, loss = 21.13, batch loss = 21.05 (8.6 examples/sec; 0.936 sec/batch; 77h:41m:51s remains)
INFO - root - 2017-12-07 17:04:38.337071: step 33550, loss = 21.66, batch loss = 21.58 (8.8 examples/sec; 0.909 sec/batch; 75h:30m:46s remains)
INFO - root - 2017-12-07 17:04:47.962701: step 33560, loss = 21.37, batch loss = 21.28 (8.0 examples/sec; 1.004 sec/batch; 83h:21m:57s remains)
INFO - root - 2017-12-07 17:04:57.384664: step 33570, loss = 21.48, batch loss = 21.39 (8.6 examples/sec; 0.933 sec/batch; 77h:29m:39s remains)
INFO - root - 2017-12-07 17:05:06.752401: step 33580, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.929 sec/batch; 77h:10m:21s remains)
INFO - root - 2017-12-07 17:05:16.178263: step 33590, loss = 21.54, batch loss = 21.46 (8.5 examples/sec; 0.940 sec/batch; 78h:01m:10s remains)
INFO - root - 2017-12-07 17:05:25.648670: step 33600, loss = 21.37, batch loss = 21.29 (8.0 examples/sec; 0.999 sec/batch; 82h:55m:48s remains)
2017-12-07 17:05:26.517184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4493532 -4.3586869 -4.2637014 -4.209774 -4.2011528 -4.2209787 -4.2174096 -4.19182 -4.1690149 -4.1696057 -4.1908622 -4.1809053 -4.1422009 -4.1113997 -4.1045761][-4.4215851 -4.3264394 -4.2411485 -4.2046957 -4.2107682 -4.2412357 -4.2396765 -4.2080946 -4.161612 -4.1238184 -4.1072922 -4.0776443 -4.0348907 -4.01205 -4.0268226][-4.4568858 -4.3749142 -4.2969751 -4.2521915 -4.2442737 -4.2751141 -4.2821465 -4.2502484 -4.1840262 -4.1196036 -4.0718932 -4.01185 -3.9519975 -3.930131 -3.967905][-4.5198584 -4.4547224 -4.3810592 -4.3165889 -4.2759795 -4.2855034 -4.2856131 -4.2508745 -4.1835618 -4.1200652 -4.0657339 -3.9872394 -3.9177668 -3.902741 -3.9605241][-4.5690951 -4.5096574 -4.4320893 -4.3539562 -4.2822466 -4.2525744 -4.2288394 -4.1976175 -4.1586509 -4.1248 -4.0836015 -4.0062938 -3.9426861 -3.9416459 -4.01228][-4.569047 -4.4897504 -4.3866587 -4.2941513 -4.2066083 -4.1499429 -4.1119432 -4.0992723 -4.1076474 -4.1110535 -4.0870829 -4.0253181 -3.9848611 -4.0104876 -4.0958786][-4.5214171 -4.4132285 -4.2819996 -4.1837168 -4.1076474 -4.0515885 -4.0020542 -3.9825389 -4.0041065 -4.0257568 -4.0266056 -4.0148082 -4.0363069 -4.1141896 -4.2155962][-4.4277711 -4.3038859 -4.1716032 -4.0911016 -4.0522828 -4.023138 -3.9584475 -3.8914785 -3.87727 -3.9005804 -3.9461191 -4.013979 -4.1126261 -4.2309394 -4.3242431][-4.2981052 -4.1708632 -4.06112 -4.0132751 -4.0142484 -4.0162416 -3.95048 -3.849117 -3.8089814 -3.8468685 -3.9387383 -4.0637279 -4.19861 -4.3153319 -4.3813372][-4.2759261 -4.1633663 -4.0764532 -4.0455222 -4.0569348 -4.0724154 -4.0144796 -3.9063246 -3.8642786 -3.9166179 -4.0273576 -4.1681519 -4.3018985 -4.3979349 -4.4377027][-4.3983583 -4.3119035 -4.2302408 -4.1833844 -4.1675544 -4.1686072 -4.1102166 -3.9978316 -3.9509833 -4.0055232 -4.1212745 -4.266223 -4.3982625 -4.4839029 -4.5123181][-4.5251007 -4.4691319 -4.3914356 -4.3255525 -4.2733116 -4.2449794 -4.1800437 -4.0706582 -4.0213447 -4.065927 -4.170238 -4.3054023 -4.4321923 -4.5143194 -4.5414491][-4.5966845 -4.5712843 -4.507915 -4.438519 -4.3681889 -4.3212881 -4.2666969 -4.1890297 -4.1537914 -4.1807532 -4.24822 -4.3411107 -4.4337053 -4.492208 -4.507185][-4.6016841 -4.5947843 -4.5476456 -4.4931531 -4.439445 -4.4053946 -4.3765621 -4.3386292 -4.321476 -4.3318629 -4.3609037 -4.4055533 -4.4521093 -4.4747062 -4.4679394][-4.5535035 -4.5593567 -4.5375767 -4.5146484 -4.4969587 -4.488637 -4.4795165 -4.4617558 -4.4480853 -4.4425678 -4.4466071 -4.4604135 -4.4754505 -4.4725618 -4.4500184]]...]
INFO - root - 2017-12-07 17:05:36.036993: step 33610, loss = 21.28, batch loss = 21.20 (8.8 examples/sec; 0.911 sec/batch; 75h:38m:21s remains)
INFO - root - 2017-12-07 17:05:45.499940: step 33620, loss = 21.37, batch loss = 21.29 (8.5 examples/sec; 0.944 sec/batch; 78h:20m:15s remains)
INFO - root - 2017-12-07 17:05:54.904010: step 33630, loss = 21.36, batch loss = 21.28 (8.0 examples/sec; 1.003 sec/batch; 83h:17m:05s remains)
INFO - root - 2017-12-07 17:06:04.195358: step 33640, loss = 21.26, batch loss = 21.17 (8.8 examples/sec; 0.904 sec/batch; 75h:03m:04s remains)
INFO - root - 2017-12-07 17:06:13.504064: step 33650, loss = 21.69, batch loss = 21.60 (8.9 examples/sec; 0.897 sec/batch; 74h:29m:48s remains)
INFO - root - 2017-12-07 17:06:22.802212: step 33660, loss = 21.74, batch loss = 21.66 (8.8 examples/sec; 0.908 sec/batch; 75h:20m:15s remains)
INFO - root - 2017-12-07 17:06:32.218881: step 33670, loss = 21.56, batch loss = 21.47 (8.5 examples/sec; 0.943 sec/batch; 78h:17m:52s remains)
INFO - root - 2017-12-07 17:06:41.681608: step 33680, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.950 sec/batch; 78h:49m:26s remains)
INFO - root - 2017-12-07 17:06:50.972151: step 33690, loss = 21.30, batch loss = 21.21 (8.3 examples/sec; 0.961 sec/batch; 79h:47m:56s remains)
INFO - root - 2017-12-07 17:07:00.446469: step 33700, loss = 21.80, batch loss = 21.71 (8.2 examples/sec; 0.979 sec/batch; 81h:16m:16s remains)
2017-12-07 17:07:01.445092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5029821 -4.553462 -4.6288018 -4.6857672 -4.6760225 -4.6339431 -4.6434646 -4.6852961 -4.6920633 -4.6728654 -4.6561313 -4.6286716 -4.5736337 -4.5009165 -4.43197][-4.4749322 -4.5348816 -4.62645 -4.7096357 -4.7163639 -4.6742649 -4.6867642 -4.7391186 -4.7532339 -4.7498326 -4.7618704 -4.7509594 -4.6861768 -4.5838733 -4.4811039][-4.4161038 -4.4766617 -4.5645084 -4.6562786 -4.6853933 -4.6577759 -4.6633525 -4.6988039 -4.702117 -4.7091861 -4.766149 -4.8117428 -4.7865343 -4.68695 -4.5538306][-4.3510394 -4.4064116 -4.4789934 -4.5583448 -4.598999 -4.5852847 -4.5694633 -4.5703945 -4.5547237 -4.5622487 -4.6535473 -4.76429 -4.8152165 -4.7611508 -4.6226439][-4.30948 -4.3518615 -4.39875 -4.4352508 -4.4422321 -4.4103937 -4.3577957 -4.3290777 -4.3145232 -4.3312235 -4.4387326 -4.5863671 -4.7132006 -4.73658 -4.6346264][-4.3199887 -4.3441663 -4.3560519 -4.3222413 -4.2454638 -4.1542716 -4.0520105 -4.0062437 -4.0185523 -4.0735788 -4.2006426 -4.3515472 -4.5148263 -4.6037116 -4.5573549][-4.3940272 -4.404273 -4.3784647 -4.2719159 -4.0951729 -3.9203062 -3.7591991 -3.6927674 -3.7292855 -3.8458586 -4.0262923 -4.1807151 -4.3420496 -4.451458 -4.4389052][-4.4944105 -4.5018897 -4.4594207 -4.32077 -4.0849113 -3.8444827 -3.62856 -3.5348923 -3.5850108 -3.7634933 -4.0152059 -4.1848407 -4.3189573 -4.39784 -4.37223][-4.5825157 -4.60036 -4.5703979 -4.4479218 -4.2202091 -3.9678903 -3.726393 -3.6129372 -3.6677337 -3.8823011 -4.1802535 -4.3599916 -4.4554186 -4.47462 -4.4029121][-4.6315379 -4.66793 -4.6678529 -4.5941887 -4.429317 -4.2193613 -3.99052 -3.8660233 -3.9038417 -4.1082244 -4.4029684 -4.5795007 -4.6369605 -4.6025233 -4.4909577][-4.6357284 -4.6903481 -4.723793 -4.7075405 -4.6260562 -4.4968977 -4.3256416 -4.2120848 -4.2173748 -4.3586416 -4.5897017 -4.7365818 -4.7626667 -4.6957421 -4.5724449][-4.5886993 -4.654067 -4.7106738 -4.7375126 -4.72522 -4.6819553 -4.598073 -4.5262179 -4.5085044 -4.5703855 -4.7017975 -4.7939591 -4.7960348 -4.7234311 -4.61211][-4.4956961 -4.5575156 -4.6189885 -4.6651011 -4.6930218 -4.7108397 -4.7067909 -4.6918254 -4.675652 -4.67967 -4.720149 -4.755362 -4.7450004 -4.6873126 -4.6009641][-4.391253 -4.4355736 -4.4842896 -4.5269213 -4.5645843 -4.6062026 -4.6452556 -4.6722794 -4.6756582 -4.6636362 -4.6576486 -4.6568875 -4.6423831 -4.6033392 -4.5403366][-4.3108273 -4.332787 -4.3605285 -4.3873029 -4.4147449 -4.4483757 -4.4870338 -4.521512 -4.5374842 -4.5333138 -4.5220022 -4.5132174 -4.5029607 -4.4806256 -4.4395118]]...]
INFO - root - 2017-12-07 17:07:10.928541: step 33710, loss = 21.74, batch loss = 21.66 (8.0 examples/sec; 0.998 sec/batch; 82h:49m:50s remains)
INFO - root - 2017-12-07 17:07:20.377533: step 33720, loss = 21.53, batch loss = 21.45 (8.9 examples/sec; 0.902 sec/batch; 74h:50m:42s remains)
INFO - root - 2017-12-07 17:07:29.834118: step 33730, loss = 21.35, batch loss = 21.27 (8.8 examples/sec; 0.914 sec/batch; 75h:51m:47s remains)
INFO - root - 2017-12-07 17:07:39.320197: step 33740, loss = 21.29, batch loss = 21.21 (8.6 examples/sec; 0.931 sec/batch; 77h:16m:37s remains)
INFO - root - 2017-12-07 17:07:48.778074: step 33750, loss = 22.03, batch loss = 21.95 (8.3 examples/sec; 0.961 sec/batch; 79h:45m:03s remains)
INFO - root - 2017-12-07 17:07:58.295442: step 33760, loss = 21.56, batch loss = 21.48 (8.1 examples/sec; 0.991 sec/batch; 82h:15m:20s remains)
INFO - root - 2017-12-07 17:08:07.755504: step 33770, loss = 21.64, batch loss = 21.56 (8.1 examples/sec; 0.987 sec/batch; 81h:55m:19s remains)
INFO - root - 2017-12-07 17:08:17.028522: step 33780, loss = 21.54, batch loss = 21.45 (8.2 examples/sec; 0.975 sec/batch; 80h:53m:57s remains)
INFO - root - 2017-12-07 17:08:26.395281: step 33790, loss = 21.10, batch loss = 21.01 (8.5 examples/sec; 0.942 sec/batch; 78h:08m:02s remains)
INFO - root - 2017-12-07 17:08:35.805674: step 33800, loss = 21.33, batch loss = 21.24 (8.4 examples/sec; 0.947 sec/batch; 78h:33m:38s remains)
2017-12-07 17:08:36.673544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6904459 -4.708282 -4.6916294 -4.653204 -4.579596 -4.4750719 -4.39261 -4.3754063 -4.4185734 -4.4964476 -4.5547819 -4.5537949 -4.5231719 -4.4847183 -4.4530115][-4.7146997 -4.7270231 -4.698566 -4.6450291 -4.5580306 -4.4470587 -4.358459 -4.3353076 -4.3831577 -4.4802527 -4.5520239 -4.545918 -4.5140176 -4.4809537 -4.4507461][-4.715498 -4.7234364 -4.6882234 -4.6195288 -4.5163994 -4.4001412 -4.3073111 -4.2805696 -4.3259983 -4.4313927 -4.5199842 -4.524498 -4.4996552 -4.4810977 -4.4690814][-4.7248855 -4.7358947 -4.6943541 -4.6050954 -4.4750943 -4.3477125 -4.2566085 -4.2335167 -4.2773423 -4.3809929 -4.480238 -4.5024281 -4.4899325 -4.4847736 -4.4921923][-4.7226033 -4.7266126 -4.6627297 -4.5414453 -4.3776917 -4.2341695 -4.1484413 -4.1343346 -4.1919632 -4.3177333 -4.4424429 -4.4942484 -4.4905944 -4.4777169 -4.4867516][-4.6756935 -4.6599846 -4.5714207 -4.4244108 -4.23636 -4.0746093 -3.9756515 -3.9483075 -4.0290618 -4.2102184 -4.3886251 -4.4904094 -4.5015774 -4.4658551 -4.4518147][-4.6056523 -4.5824637 -4.4922295 -4.3379693 -4.1345792 -3.9457345 -3.7994409 -3.7252316 -3.818759 -4.0627708 -4.3056011 -4.4673195 -4.5052295 -4.4559031 -4.4130864][-4.5707412 -4.5662065 -4.5041633 -4.3646913 -4.1632581 -3.9567018 -3.7602806 -3.6337831 -3.7141876 -3.9705927 -4.2377715 -4.4262862 -4.47905 -4.4335718 -4.3810415][-4.5705757 -4.6034966 -4.5879011 -4.4829082 -4.3047333 -4.1114855 -3.8994796 -3.7559347 -3.8106596 -4.0173407 -4.2457938 -4.4033623 -4.4354124 -4.3931727 -4.3507423][-4.5683622 -4.6430273 -4.6819954 -4.6225 -4.4829888 -4.3201833 -4.1271038 -4.0097108 -4.0577154 -4.2008381 -4.3601232 -4.449718 -4.42735 -4.3673673 -4.3279014][-4.5426269 -4.6449647 -4.7229643 -4.7080317 -4.6162496 -4.4990935 -4.343317 -4.2576275 -4.3026228 -4.3977695 -4.5051394 -4.5461059 -4.4809389 -4.3954153 -4.3379741][-4.5113564 -4.611969 -4.693594 -4.7056723 -4.6633139 -4.6082888 -4.5126662 -4.4579616 -4.4925232 -4.5504642 -4.6219692 -4.6413774 -4.5624218 -4.4630909 -4.3814273][-4.4997849 -4.5752373 -4.6367631 -4.6541762 -4.6421428 -4.6341228 -4.5989742 -4.5780716 -4.6050706 -4.6391311 -4.68138 -4.6858749 -4.6105113 -4.5121264 -4.4180312][-4.4867668 -4.5368609 -4.5781507 -4.5918837 -4.5839739 -4.5843582 -4.5776887 -4.5759497 -4.5979195 -4.6222267 -4.6507983 -4.6508517 -4.592545 -4.5104675 -4.4223151][-4.450223 -4.4796243 -4.5121937 -4.5295768 -4.5244284 -4.5196362 -4.5170641 -4.5197625 -4.5359273 -4.5550108 -4.5749712 -4.5717378 -4.5312667 -4.471715 -4.4029346]]...]
INFO - root - 2017-12-07 17:08:46.036546: step 33810, loss = 21.40, batch loss = 21.32 (8.7 examples/sec; 0.917 sec/batch; 76h:05m:39s remains)
INFO - root - 2017-12-07 17:08:55.379113: step 33820, loss = 21.58, batch loss = 21.49 (8.8 examples/sec; 0.904 sec/batch; 75h:00m:30s remains)
INFO - root - 2017-12-07 17:09:04.800935: step 33830, loss = 21.83, batch loss = 21.75 (8.5 examples/sec; 0.943 sec/batch; 78h:15m:03s remains)
INFO - root - 2017-12-07 17:09:14.284321: step 33840, loss = 21.09, batch loss = 21.01 (8.3 examples/sec; 0.958 sec/batch; 79h:30m:42s remains)
INFO - root - 2017-12-07 17:09:23.716519: step 33850, loss = 21.52, batch loss = 21.44 (8.1 examples/sec; 0.983 sec/batch; 81h:33m:20s remains)
INFO - root - 2017-12-07 17:09:33.031397: step 33860, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.951 sec/batch; 78h:52m:01s remains)
INFO - root - 2017-12-07 17:09:42.209270: step 33870, loss = 21.33, batch loss = 21.24 (9.2 examples/sec; 0.866 sec/batch; 71h:48m:30s remains)
INFO - root - 2017-12-07 17:09:51.614135: step 33880, loss = 21.71, batch loss = 21.63 (8.7 examples/sec; 0.921 sec/batch; 76h:22m:22s remains)
INFO - root - 2017-12-07 17:10:01.013702: step 33890, loss = 21.39, batch loss = 21.31 (8.3 examples/sec; 0.968 sec/batch; 80h:19m:20s remains)
INFO - root - 2017-12-07 17:10:10.389078: step 33900, loss = 21.67, batch loss = 21.58 (8.3 examples/sec; 0.961 sec/batch; 79h:40m:21s remains)
2017-12-07 17:10:11.352899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4301081 -4.4556861 -4.4834175 -4.5050936 -4.5176177 -4.51683 -4.5066004 -4.4959755 -4.4946942 -4.5054216 -4.5178638 -4.5219212 -4.5188723 -4.5044942 -4.4742637][-4.47624 -4.5114875 -4.5475264 -4.5780234 -4.5891223 -4.5715981 -4.5376806 -4.5107012 -4.5143328 -4.5475926 -4.5830312 -4.5964303 -4.5888147 -4.5662293 -4.5271225][-4.513906 -4.5549717 -4.5960212 -4.6329021 -4.6357102 -4.5909753 -4.5250974 -4.477592 -4.4920278 -4.5642867 -4.642982 -4.6813769 -4.6664987 -4.6198707 -4.5621386][-4.5398846 -4.5748229 -4.6044631 -4.6305704 -4.6073737 -4.5219684 -4.4173965 -4.344316 -4.374227 -4.5020514 -4.6464462 -4.7374773 -4.73155 -4.6547804 -4.5709138][-4.5516233 -4.5605578 -4.5530839 -4.5358753 -4.4541435 -4.3096938 -4.155344 -4.0457406 -4.0872297 -4.278235 -4.502315 -4.6773858 -4.718843 -4.636548 -4.5403776][-4.5703092 -4.5458612 -4.4849949 -4.4005437 -4.2388244 -4.0306935 -3.8214281 -3.6631894 -3.7040911 -3.9429388 -4.2375402 -4.5033908 -4.6205268 -4.5676 -4.4831595][-4.6201019 -4.5827861 -4.4888892 -4.3570614 -4.1419344 -3.9001758 -3.6557984 -3.4465053 -3.4571815 -3.6990693 -4.0264935 -4.3485885 -4.5229592 -4.5071597 -4.4441934][-4.6893482 -4.6649418 -4.5761213 -4.4488463 -4.243022 -4.0285411 -3.8022466 -3.57554 -3.53861 -3.7355828 -4.0404425 -4.3508468 -4.522182 -4.5125742 -4.4513488][-4.7443261 -4.7370219 -4.6612463 -4.5596871 -4.4023318 -4.25624 -4.1085014 -3.9306593 -3.8773394 -4.0247908 -4.2785988 -4.5270324 -4.6395545 -4.5893016 -4.4911885][-4.7748408 -4.771296 -4.686717 -4.5890069 -4.4722028 -4.3957868 -4.3538833 -4.2737427 -4.2432079 -4.3577557 -4.5585761 -4.737319 -4.7840157 -4.6819587 -4.5335779][-4.7848692 -4.7894726 -4.6983166 -4.5930071 -4.4963589 -4.4664583 -4.5083079 -4.5163879 -4.5099998 -4.5886779 -4.733665 -4.8544221 -4.8601556 -4.7353578 -4.5629015][-4.7598433 -4.7783222 -4.7033043 -4.6049376 -4.525897 -4.5182991 -4.5944023 -4.6463137 -4.6494007 -4.6913066 -4.7812042 -4.8549542 -4.8474369 -4.7384443 -4.5815649][-4.6813498 -4.7021718 -4.6521263 -4.5756774 -4.5193982 -4.5267897 -4.6050673 -4.667974 -4.6763644 -4.6926279 -4.740819 -4.7816696 -4.7763638 -4.7026768 -4.587358][-4.5854683 -4.5913668 -4.5599375 -4.5129862 -4.4841037 -4.5007586 -4.5605412 -4.6117129 -4.6235127 -4.6295528 -4.65082 -4.67022 -4.6699233 -4.6316733 -4.5622106][-4.5106506 -4.5013595 -4.4819274 -4.46134 -4.4534116 -4.4674735 -4.498786 -4.5251889 -4.533596 -4.5353756 -4.541254 -4.5481095 -4.5499811 -4.5353479 -4.5008674]]...]
INFO - root - 2017-12-07 17:10:20.828523: step 33910, loss = 21.43, batch loss = 21.35 (8.3 examples/sec; 0.969 sec/batch; 80h:23m:11s remains)
INFO - root - 2017-12-07 17:10:30.244153: step 33920, loss = 21.19, batch loss = 21.10 (8.3 examples/sec; 0.969 sec/batch; 80h:23m:34s remains)
INFO - root - 2017-12-07 17:10:39.580909: step 33930, loss = 21.61, batch loss = 21.53 (8.7 examples/sec; 0.925 sec/batch; 76h:41m:04s remains)
INFO - root - 2017-12-07 17:10:48.990065: step 33940, loss = 21.26, batch loss = 21.18 (9.1 examples/sec; 0.877 sec/batch; 72h:44m:55s remains)
INFO - root - 2017-12-07 17:10:58.486328: step 33950, loss = 21.41, batch loss = 21.32 (8.9 examples/sec; 0.903 sec/batch; 74h:54m:51s remains)
INFO - root - 2017-12-07 17:11:08.012916: step 33960, loss = 21.02, batch loss = 20.93 (8.2 examples/sec; 0.975 sec/batch; 80h:52m:58s remains)
INFO - root - 2017-12-07 17:11:17.362338: step 33970, loss = 21.65, batch loss = 21.57 (8.5 examples/sec; 0.941 sec/batch; 78h:00m:58s remains)
INFO - root - 2017-12-07 17:11:26.687245: step 33980, loss = 21.07, batch loss = 20.99 (8.5 examples/sec; 0.936 sec/batch; 77h:37m:55s remains)
INFO - root - 2017-12-07 17:11:35.892444: step 33990, loss = 21.61, batch loss = 21.53 (10.6 examples/sec; 0.757 sec/batch; 62h:43m:58s remains)
INFO - root - 2017-12-07 17:11:45.200983: step 34000, loss = 21.63, batch loss = 21.54 (8.2 examples/sec; 0.973 sec/batch; 80h:38m:11s remains)
2017-12-07 17:11:46.196515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3196979 -4.3500667 -4.367228 -4.3437271 -4.2796955 -4.20809 -4.1623783 -4.1512423 -4.1967325 -4.28575 -4.3795633 -4.4547091 -4.4728765 -4.436204 -4.3789115][-4.325284 -4.3614697 -4.3848267 -4.3628736 -4.2883387 -4.1943212 -4.1266685 -4.1063166 -4.16019 -4.268877 -4.3840008 -4.4761944 -4.5049376 -4.4732938 -4.4152784][-4.3268652 -4.369976 -4.4002628 -4.37666 -4.2852936 -4.1629043 -4.0702982 -4.044281 -4.1127324 -4.2441869 -4.3818288 -4.4929957 -4.5383563 -4.5186458 -4.4615793][-4.3290534 -4.3787203 -4.4154234 -4.3873239 -4.2746048 -4.1190066 -3.997833 -3.9670916 -4.0536308 -4.2083926 -4.3669982 -4.4939289 -4.5538363 -4.545506 -4.4868107][-4.3321981 -4.3867354 -4.4289427 -4.3959947 -4.2637935 -4.0779929 -3.9305439 -3.8948119 -3.9956484 -4.1683755 -4.3426485 -4.4804878 -4.5493298 -4.5468669 -4.4822879][-4.3345642 -4.3927393 -4.4381285 -4.4009891 -4.2555823 -4.050169 -3.8859932 -3.8469748 -3.9552517 -4.1373205 -4.3195491 -4.46185 -4.5322847 -4.5279593 -4.4535966][-4.3364754 -4.3957849 -4.4408154 -4.3987064 -4.2434459 -4.0258665 -3.8524575 -3.8101192 -3.9201496 -4.1071067 -4.294631 -4.4401832 -4.5086 -4.4969845 -4.4109526][-4.3376107 -4.394906 -4.4345393 -4.3840575 -4.2188811 -3.9937267 -3.8178933 -3.7735758 -3.8819 -4.0720763 -4.2646365 -4.4150343 -4.4827766 -4.4637322 -4.3671207][-4.3380327 -4.391346 -4.4228215 -4.3633108 -4.1913619 -3.9667921 -3.7979999 -3.7557216 -3.8600807 -4.0487251 -4.2404943 -4.3915434 -4.4589472 -4.4367819 -4.3366451][-4.3382177 -4.3877492 -4.4127417 -4.3483758 -4.1766076 -3.9602091 -3.803391 -3.7646949 -3.8629565 -4.0451779 -4.230227 -4.3765659 -4.4437909 -4.4246049 -4.331109][-4.3375435 -4.3847203 -4.4081874 -4.3484449 -4.1899667 -3.9931529 -3.85427 -3.8210702 -3.908643 -4.0741258 -4.2409315 -4.3723955 -4.4366188 -4.4256992 -4.3505578][-4.3347311 -4.3819528 -4.4116874 -4.3723693 -4.2485695 -4.0902019 -3.9782469 -3.9512959 -4.0202613 -4.1524849 -4.2842717 -4.387331 -4.4420605 -4.442585 -4.396214][-4.3300896 -4.3777103 -4.4189873 -4.4112492 -4.3360848 -4.2264295 -4.143383 -4.1198168 -4.1653805 -4.2565613 -4.3464179 -4.4160519 -4.457891 -4.469244 -4.4520507][-4.3221493 -4.3673744 -4.4168439 -4.4381313 -4.4109855 -4.3504205 -4.2956204 -4.2745414 -4.297235 -4.3480773 -4.3972359 -4.4350023 -4.4624619 -4.4805865 -4.4877338][-4.3118653 -4.3495255 -4.3978281 -4.4352241 -4.443275 -4.4235878 -4.3951159 -4.3780804 -4.3830433 -4.4006991 -4.4168115 -4.4294734 -4.4441156 -4.4641404 -4.4854198]]...]
INFO - root - 2017-12-07 17:11:55.632399: step 34010, loss = 21.61, batch loss = 21.53 (8.0 examples/sec; 0.995 sec/batch; 82h:27m:59s remains)
INFO - root - 2017-12-07 17:12:05.086081: step 34020, loss = 21.43, batch loss = 21.34 (8.3 examples/sec; 0.967 sec/batch; 80h:12m:42s remains)
INFO - root - 2017-12-07 17:12:14.649436: step 34030, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.909 sec/batch; 75h:21m:21s remains)
INFO - root - 2017-12-07 17:12:24.165682: step 34040, loss = 21.50, batch loss = 21.42 (8.3 examples/sec; 0.963 sec/batch; 79h:50m:20s remains)
INFO - root - 2017-12-07 17:12:33.559884: step 34050, loss = 21.39, batch loss = 21.30 (8.3 examples/sec; 0.959 sec/batch; 79h:29m:21s remains)
INFO - root - 2017-12-07 17:12:43.003050: step 34060, loss = 21.18, batch loss = 21.10 (9.3 examples/sec; 0.862 sec/batch; 71h:26m:29s remains)
INFO - root - 2017-12-07 17:12:52.321192: step 34070, loss = 21.80, batch loss = 21.72 (8.5 examples/sec; 0.936 sec/batch; 77h:34m:21s remains)
INFO - root - 2017-12-07 17:13:01.844996: step 34080, loss = 21.63, batch loss = 21.54 (8.5 examples/sec; 0.941 sec/batch; 77h:59m:15s remains)
INFO - root - 2017-12-07 17:13:11.418753: step 34090, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.945 sec/batch; 78h:21m:55s remains)
INFO - root - 2017-12-07 17:13:20.661114: step 34100, loss = 21.67, batch loss = 21.59 (8.5 examples/sec; 0.942 sec/batch; 78h:04m:48s remains)
2017-12-07 17:13:21.678882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3618956 -4.371089 -4.3807764 -4.3783488 -4.3616405 -4.3392015 -4.3196855 -4.3163567 -4.3335471 -4.3608871 -4.3882275 -4.413394 -4.4385319 -4.4684734 -4.5011168][-4.4129319 -4.4347029 -4.4540544 -4.4578657 -4.4444141 -4.4216533 -4.4007277 -4.3954148 -4.4129419 -4.4473643 -4.4888854 -4.5295033 -4.56642 -4.5990024 -4.6189833][-4.4547596 -4.4920721 -4.5184155 -4.5183949 -4.4912877 -4.4476528 -4.4051347 -4.3870215 -4.4100962 -4.4705806 -4.546268 -4.6128974 -4.6581273 -4.6773491 -4.6609612][-4.4621692 -4.5070753 -4.5320196 -4.5155115 -4.457407 -4.3726511 -4.2896156 -4.2495861 -4.2853971 -4.3907323 -4.5188303 -4.6207428 -4.6751924 -4.6789031 -4.6217947][-4.42374 -4.4609704 -4.4705276 -4.4252114 -4.3256965 -4.1908116 -4.0584316 -3.9927497 -4.0448647 -4.19941 -4.3821173 -4.5227966 -4.5987887 -4.6102161 -4.5424266][-4.3712945 -4.3933377 -4.3827724 -4.3090463 -4.1742473 -3.9958296 -3.8156447 -3.7219927 -3.7856998 -3.9789391 -4.2017555 -4.3740854 -4.4789596 -4.5227766 -4.4824009][-4.337676 -4.3504848 -4.3260407 -4.234129 -4.0764809 -3.868679 -3.6543357 -3.539083 -3.6061826 -3.8147733 -4.0529847 -4.2429547 -4.3748083 -4.4571524 -4.4596133][-4.3317246 -4.3455944 -4.3203759 -4.2255607 -4.05948 -3.8418791 -3.6168287 -3.4930315 -3.5506487 -3.7445493 -3.973496 -4.1713753 -4.3251181 -4.4364543 -4.4719534][-4.3448138 -4.3700223 -4.363101 -4.2940059 -4.1547489 -3.967773 -3.7769952 -3.672004 -3.715976 -3.8699985 -4.0578442 -4.2304049 -4.3712792 -4.47383 -4.5056667][-4.3539367 -4.3964238 -4.4183331 -4.3925209 -4.3057146 -4.1796956 -4.0537686 -3.9900057 -4.0251594 -4.1312537 -4.2585835 -4.3724632 -4.4625583 -4.5220304 -4.5226851][-4.356565 -4.4146461 -4.4617605 -4.4748583 -4.4380083 -4.3669481 -4.2936387 -4.258646 -4.2807059 -4.3426 -4.4159985 -4.4731584 -4.50952 -4.5220642 -4.4901609][-4.3568258 -4.4171624 -4.47525 -4.5134687 -4.5136065 -4.4815593 -4.4418125 -4.4208446 -4.4276028 -4.453063 -4.4828262 -4.4961581 -4.4935694 -4.4756956 -4.43][-4.3396659 -4.3870993 -4.438211 -4.481782 -4.5013733 -4.4957261 -4.4798684 -4.4674468 -4.4619322 -4.4598346 -4.4555168 -4.4404922 -4.4203234 -4.3974171 -4.3630166][-4.31302 -4.3378673 -4.3679609 -4.3971834 -4.4145808 -4.4177971 -4.4137616 -4.40597 -4.3950429 -4.380053 -4.3614154 -4.34055 -4.3253441 -4.3171797 -4.308342][-4.2765207 -4.2769628 -4.2878551 -4.3035879 -4.3166051 -4.323184 -4.3250179 -4.3211684 -4.3127084 -4.2997174 -4.2835197 -4.2683849 -4.261929 -4.2657046 -4.2745843]]...]
INFO - root - 2017-12-07 17:13:31.004653: step 34110, loss = 21.74, batch loss = 21.65 (8.7 examples/sec; 0.923 sec/batch; 76h:28m:13s remains)
INFO - root - 2017-12-07 17:13:40.356019: step 34120, loss = 21.24, batch loss = 21.16 (8.2 examples/sec; 0.971 sec/batch; 80h:31m:07s remains)
INFO - root - 2017-12-07 17:13:49.778070: step 34130, loss = 21.30, batch loss = 21.22 (7.9 examples/sec; 1.009 sec/batch; 83h:39m:35s remains)
INFO - root - 2017-12-07 17:13:59.225420: step 34140, loss = 21.29, batch loss = 21.21 (8.5 examples/sec; 0.940 sec/batch; 77h:54m:21s remains)
INFO - root - 2017-12-07 17:14:08.673409: step 34150, loss = 21.27, batch loss = 21.18 (8.6 examples/sec; 0.927 sec/batch; 76h:48m:30s remains)
INFO - root - 2017-12-07 17:14:18.177549: step 34160, loss = 21.82, batch loss = 21.73 (8.2 examples/sec; 0.971 sec/batch; 80h:28m:16s remains)
INFO - root - 2017-12-07 17:14:27.454815: step 34170, loss = 21.70, batch loss = 21.62 (8.2 examples/sec; 0.973 sec/batch; 80h:37m:48s remains)
INFO - root - 2017-12-07 17:14:36.889557: step 34180, loss = 21.40, batch loss = 21.31 (8.5 examples/sec; 0.936 sec/batch; 77h:32m:56s remains)
INFO - root - 2017-12-07 17:14:46.390545: step 34190, loss = 21.62, batch loss = 21.54 (8.9 examples/sec; 0.903 sec/batch; 74h:50m:49s remains)
INFO - root - 2017-12-07 17:14:55.779657: step 34200, loss = 21.34, batch loss = 21.26 (8.8 examples/sec; 0.912 sec/batch; 75h:35m:06s remains)
2017-12-07 17:14:56.715818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7819595 -4.7662215 -4.7505589 -4.7377224 -4.7247167 -4.6726751 -4.5803638 -4.4859462 -4.4555469 -4.5223856 -4.60323 -4.6599994 -4.6959329 -4.6635704 -4.5589752][-4.8722706 -4.8604803 -4.8346515 -4.803133 -4.7664022 -4.6922021 -4.5862141 -4.4983368 -4.4934306 -4.5947108 -4.700985 -4.7591581 -4.78971 -4.7519016 -4.6308036][-4.9022193 -4.8929071 -4.8415728 -4.7693615 -4.6855874 -4.5768466 -4.4612551 -4.4005132 -4.4490209 -4.6007252 -4.7442174 -4.8161492 -4.8445759 -4.8049583 -4.6768723][-4.8425207 -4.8307886 -4.74274 -4.6138358 -4.4645 -4.3096557 -4.1841636 -4.1579671 -4.2693758 -4.4646339 -4.6366467 -4.7347307 -4.784369 -4.7685585 -4.6629915][-4.7217755 -4.7003431 -4.5810533 -4.3998919 -4.1815419 -3.9746957 -3.8371239 -3.8433683 -4.0088458 -4.22603 -4.4040766 -4.5299687 -4.622417 -4.6577635 -4.6022882][-4.6097012 -4.5714307 -4.4367113 -4.2218695 -3.9448073 -3.6894503 -3.5393858 -3.5710392 -3.7748978 -3.999402 -4.17003 -4.3179188 -4.4563918 -4.5463772 -4.5416713][-4.5680122 -4.5125933 -4.3765464 -4.1451464 -3.8249705 -3.529747 -3.3669424 -3.411186 -3.6343217 -3.8660457 -4.0373287 -4.200458 -4.3648381 -4.4836254 -4.5063114][-4.6248527 -4.5576148 -4.4278178 -4.1968064 -3.8633647 -3.5526941 -3.384038 -3.4254866 -3.6381946 -3.8699298 -4.0484271 -4.2156415 -4.378664 -4.490356 -4.50604][-4.7101932 -4.6379328 -4.5191259 -4.3109303 -4.0111737 -3.7273369 -3.5745304 -3.6096358 -3.7899051 -4.0030832 -4.1828909 -4.3438392 -4.4886103 -4.5658693 -4.5431242][-4.7412815 -4.6756973 -4.5763097 -4.4145422 -4.1911712 -3.9815645 -3.8763785 -3.9120078 -4.0436592 -4.2149425 -4.3785286 -4.5232062 -4.6402397 -4.6704612 -4.5985837][-4.669384 -4.6264358 -4.5564184 -4.4529734 -4.3249111 -4.2142973 -4.1788034 -4.2265434 -4.3152509 -4.4400387 -4.5777016 -4.6973991 -4.7786264 -4.7634144 -4.6520643][-4.5278435 -4.5215387 -4.4880524 -4.4358068 -4.380372 -4.3419313 -4.3574762 -4.4193959 -4.4898705 -4.5894828 -4.7062383 -4.7977543 -4.8445916 -4.8032331 -4.6799488][-4.3978314 -4.4195604 -4.4193234 -4.3985395 -4.3757167 -4.3621211 -4.3911796 -4.456615 -4.5226765 -4.6119051 -4.713788 -4.786644 -4.82047 -4.7862749 -4.6808858][-4.3270354 -4.3561339 -4.3745193 -4.3710585 -4.3578987 -4.337254 -4.3447752 -4.3911829 -4.4458685 -4.5261617 -4.621314 -4.69347 -4.7404246 -4.7347307 -4.6580954][-4.3175116 -4.3479781 -4.3827405 -4.40011 -4.3940177 -4.3462811 -4.2915516 -4.2737842 -4.2868314 -4.3545003 -4.4604344 -4.5552821 -4.6344738 -4.6628408 -4.6132293]]...]
INFO - root - 2017-12-07 17:15:06.194404: step 34210, loss = 21.57, batch loss = 21.49 (8.6 examples/sec; 0.934 sec/batch; 77h:21m:34s remains)
INFO - root - 2017-12-07 17:15:15.533574: step 34220, loss = 21.44, batch loss = 21.36 (8.9 examples/sec; 0.899 sec/batch; 74h:28m:46s remains)
INFO - root - 2017-12-07 17:15:25.032179: step 34230, loss = 21.27, batch loss = 21.19 (8.7 examples/sec; 0.923 sec/batch; 76h:30m:09s remains)
INFO - root - 2017-12-07 17:15:34.510883: step 34240, loss = 21.82, batch loss = 21.73 (8.0 examples/sec; 1.004 sec/batch; 83h:11m:28s remains)
INFO - root - 2017-12-07 17:15:43.980260: step 34250, loss = 21.49, batch loss = 21.40 (8.3 examples/sec; 0.961 sec/batch; 79h:35m:51s remains)
INFO - root - 2017-12-07 17:15:53.242700: step 34260, loss = 21.47, batch loss = 21.38 (8.4 examples/sec; 0.950 sec/batch; 78h:43m:25s remains)
INFO - root - 2017-12-07 17:16:02.774150: step 34270, loss = 21.74, batch loss = 21.65 (8.5 examples/sec; 0.944 sec/batch; 78h:13m:34s remains)
INFO - root - 2017-12-07 17:16:12.140820: step 34280, loss = 21.35, batch loss = 21.27 (9.0 examples/sec; 0.892 sec/batch; 73h:55m:36s remains)
INFO - root - 2017-12-07 17:16:21.575208: step 34290, loss = 21.55, batch loss = 21.47 (8.2 examples/sec; 0.978 sec/batch; 81h:02m:56s remains)
INFO - root - 2017-12-07 17:16:30.813033: step 34300, loss = 21.35, batch loss = 21.26 (8.2 examples/sec; 0.981 sec/batch; 81h:14m:35s remains)
2017-12-07 17:16:31.819758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4032388 -4.4389982 -4.4526162 -4.445622 -4.434391 -4.4329081 -4.4459729 -4.46858 -4.4798546 -4.4628687 -4.4184923 -4.3744235 -4.3502684 -4.3343158 -4.3231835][-4.420506 -4.4569278 -4.4677854 -4.4581323 -4.4471312 -4.4472575 -4.4631729 -4.4892921 -4.5005655 -4.4824142 -4.4356995 -4.3857656 -4.3585496 -4.3439255 -4.3329468][-4.4010611 -4.4182725 -4.4198365 -4.4126372 -4.4077311 -4.4083595 -4.4196386 -4.4427104 -4.4572811 -4.4538736 -4.4260058 -4.389339 -4.3694973 -4.3593488 -4.3500748][-4.3384175 -4.3340697 -4.336597 -4.3460426 -4.3541965 -4.3546948 -4.3552771 -4.3662615 -4.3812222 -4.3954682 -4.3986387 -4.3945618 -4.3973942 -4.397913 -4.3921089][-4.2660007 -4.2565455 -4.2706375 -4.2971187 -4.3083949 -4.2972078 -4.2774181 -4.2669907 -4.2785974 -4.309207 -4.3482337 -4.3907065 -4.432436 -4.4552746 -4.4583421][-4.2297511 -4.2196383 -4.2316065 -4.2451496 -4.2312531 -4.1891704 -4.1417885 -4.1159234 -4.1361628 -4.1922731 -4.2688718 -4.3598719 -4.4477863 -4.5012765 -4.5182252][-4.2546535 -4.2413125 -4.2298021 -4.2013373 -4.1421309 -4.06059 -3.9881732 -3.9569979 -3.9932809 -4.0739207 -4.1744566 -4.2971678 -4.4189854 -4.4947338 -4.5189748][-4.3274312 -4.3098316 -4.2703791 -4.1979022 -4.0966625 -3.9789753 -3.8852534 -3.8431649 -3.8778987 -3.965235 -4.07133 -4.1994929 -4.3235078 -4.3941336 -4.4111743][-4.4169693 -4.3997884 -4.3469844 -4.2537565 -4.1338053 -3.9921451 -3.8759346 -3.8069 -3.8129311 -3.8800495 -3.9695015 -4.0739017 -4.1590352 -4.1941657 -4.2021427][-4.478816 -4.474227 -4.4350071 -4.3587027 -4.2501588 -4.1032352 -3.9658649 -3.8592188 -3.8191562 -3.8442965 -3.89382 -3.9450474 -3.9618454 -3.9498777 -3.9659495][-4.4957323 -4.510632 -4.5022726 -4.4638782 -4.3834395 -4.2485175 -4.102293 -3.9705229 -3.8965502 -3.8825791 -3.8790169 -3.8550954 -3.7922833 -3.7389841 -3.7756186][-4.4822145 -4.5143905 -4.5365844 -4.5368128 -4.49325 -4.3882852 -4.254416 -4.1249828 -4.0428238 -4.0026298 -3.9459653 -3.8505213 -3.7308102 -3.6627696 -3.7199802][-4.4653034 -4.5050945 -4.54716 -4.5787168 -4.5719228 -4.5057549 -4.4003439 -4.2937407 -4.2225132 -4.1759748 -4.0920048 -3.9590716 -3.8199522 -3.7579472 -3.8263388][-4.4543571 -4.489655 -4.534575 -4.5803814 -4.5990357 -4.5703907 -4.5038848 -4.4326315 -4.3827844 -4.3412695 -4.2595258 -4.1336565 -4.0143661 -3.9787297 -4.0572524][-4.4348865 -4.45424 -4.4843874 -4.5246882 -4.555645 -4.5597568 -4.537807 -4.5072289 -4.4827781 -4.4577155 -4.4047356 -4.3238225 -4.2563133 -4.2587252 -4.338861]]...]
INFO - root - 2017-12-07 17:16:41.343360: step 34310, loss = 21.14, batch loss = 21.06 (7.9 examples/sec; 1.011 sec/batch; 83h:43m:36s remains)
INFO - root - 2017-12-07 17:16:50.805866: step 34320, loss = 21.46, batch loss = 21.38 (8.1 examples/sec; 0.987 sec/batch; 81h:47m:01s remains)
INFO - root - 2017-12-07 17:17:00.299982: step 34330, loss = 21.50, batch loss = 21.42 (8.8 examples/sec; 0.906 sec/batch; 75h:00m:30s remains)
INFO - root - 2017-12-07 17:17:09.638225: step 34340, loss = 21.52, batch loss = 21.44 (8.0 examples/sec; 0.999 sec/batch; 82h:46m:08s remains)
INFO - root - 2017-12-07 17:17:19.092991: step 34350, loss = 21.56, batch loss = 21.47 (8.0 examples/sec; 1.003 sec/batch; 83h:04m:34s remains)
INFO - root - 2017-12-07 17:17:28.397149: step 34360, loss = 21.37, batch loss = 21.28 (8.5 examples/sec; 0.939 sec/batch; 77h:44m:55s remains)
INFO - root - 2017-12-07 17:17:37.936191: step 34370, loss = 21.37, batch loss = 21.29 (8.9 examples/sec; 0.900 sec/batch; 74h:30m:04s remains)
INFO - root - 2017-12-07 17:17:47.354792: step 34380, loss = 21.76, batch loss = 21.67 (8.4 examples/sec; 0.950 sec/batch; 78h:41m:51s remains)
INFO - root - 2017-12-07 17:17:56.881614: step 34390, loss = 21.78, batch loss = 21.70 (7.9 examples/sec; 1.007 sec/batch; 83h:21m:50s remains)
INFO - root - 2017-12-07 17:18:06.133581: step 34400, loss = 21.66, batch loss = 21.58 (8.7 examples/sec; 0.925 sec/batch; 76h:33m:59s remains)
2017-12-07 17:18:07.028385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4955487 -4.4838481 -4.4642177 -4.4511962 -4.4532533 -4.4638047 -4.4729037 -4.4787431 -4.4834743 -4.4887862 -4.4957113 -4.5027828 -4.4998813 -4.4787235 -4.4423871][-4.5851707 -4.5709486 -4.5445008 -4.5309787 -4.5434818 -4.5626726 -4.5688677 -4.5668335 -4.566195 -4.5682817 -4.57342 -4.5853581 -4.5888515 -4.5640922 -4.5125947][-4.6553378 -4.6380391 -4.6036134 -4.5849652 -4.6023459 -4.6245842 -4.61957 -4.6050105 -4.6047544 -4.6113124 -4.6184077 -4.6370721 -4.649581 -4.6314178 -4.57674][-4.6536722 -4.6166768 -4.5657091 -4.5342116 -4.5486913 -4.5659285 -4.5401387 -4.5136318 -4.5327449 -4.5714951 -4.6027369 -4.6382208 -4.6582832 -4.646554 -4.5975971][-4.5797873 -4.5054183 -4.430469 -4.3809242 -4.3849931 -4.3872986 -4.330627 -4.2884841 -4.3388133 -4.4344759 -4.5176959 -4.5895576 -4.623107 -4.6165924 -4.5730057][-4.4616146 -4.340137 -4.2404323 -4.1747389 -4.1679149 -4.1465554 -4.0457335 -3.9801998 -4.0627089 -4.2202687 -4.3630667 -4.4834409 -4.5433354 -4.5505486 -4.5139813][-4.3322859 -4.1659908 -4.0472918 -3.9775476 -3.9707346 -3.9279735 -3.7794819 -3.684932 -3.7916026 -4.0013647 -4.198442 -4.3681436 -4.4588614 -4.4818063 -4.4490652][-4.2559571 -4.0692019 -3.9484994 -3.8964031 -3.9057553 -3.8549738 -3.6742761 -3.5510046 -3.6580853 -3.8924212 -4.1231928 -4.3284345 -4.4395618 -4.4669476 -4.4274306][-4.2482848 -4.0779696 -3.9746888 -3.9517546 -3.9868877 -3.9497988 -3.7729502 -3.6368856 -3.7196999 -3.9328246 -4.1574755 -4.3657928 -4.4753094 -4.494525 -4.4438176][-4.2912602 -4.157619 -4.07708 -4.0737562 -4.1259432 -4.1191249 -3.9883397 -3.8750231 -3.9317725 -4.0912018 -4.26411 -4.4308352 -4.5145674 -4.5183716 -4.4633865][-4.3942256 -4.2964954 -4.2292466 -4.2229156 -4.2660956 -4.2798576 -4.2125907 -4.1521006 -4.2013555 -4.3078756 -4.409431 -4.5064697 -4.5464964 -4.5312047 -4.4747705][-4.5166225 -4.4485946 -4.392077 -4.3706107 -4.3844061 -4.3943195 -4.3713884 -4.3618426 -4.4185181 -4.4933763 -4.541285 -4.5757451 -4.5766821 -4.5443225 -4.4845958][-4.5754309 -4.5369925 -4.500205 -4.4769115 -4.4704266 -4.4692254 -4.4658713 -4.4829679 -4.5403032 -4.5963888 -4.6184244 -4.6190815 -4.6002545 -4.5584469 -4.4953833][-4.5518641 -4.5394478 -4.527966 -4.5167875 -4.5084419 -4.5039277 -4.5056224 -4.5258994 -4.5695267 -4.6087546 -4.6210256 -4.6125059 -4.5906658 -4.549397 -4.4892993][-4.4768825 -4.4771938 -4.4790659 -4.4770293 -4.4730277 -4.4699416 -4.4715347 -4.48423 -4.5093126 -4.5327015 -4.5425005 -4.539855 -4.5294967 -4.5020947 -4.4573584]]...]
INFO - root - 2017-12-07 17:18:16.379352: step 34410, loss = 21.70, batch loss = 21.62 (8.4 examples/sec; 0.955 sec/batch; 79h:06m:19s remains)
INFO - root - 2017-12-07 17:18:25.960808: step 34420, loss = 21.42, batch loss = 21.34 (8.0 examples/sec; 1.003 sec/batch; 83h:00m:26s remains)
INFO - root - 2017-12-07 17:18:35.358637: step 34430, loss = 21.49, batch loss = 21.41 (8.1 examples/sec; 0.988 sec/batch; 81h:49m:47s remains)
INFO - root - 2017-12-07 17:18:44.901476: step 34440, loss = 21.31, batch loss = 21.23 (7.9 examples/sec; 1.009 sec/batch; 83h:32m:29s remains)
INFO - root - 2017-12-07 17:18:54.269825: step 34450, loss = 21.50, batch loss = 21.42 (8.6 examples/sec; 0.929 sec/batch; 76h:52m:45s remains)
INFO - root - 2017-12-07 17:19:03.648101: step 34460, loss = 21.85, batch loss = 21.77 (8.2 examples/sec; 0.972 sec/batch; 80h:29m:52s remains)
INFO - root - 2017-12-07 17:19:13.095958: step 34470, loss = 21.15, batch loss = 21.07 (8.2 examples/sec; 0.972 sec/batch; 80h:27m:41s remains)
INFO - root - 2017-12-07 17:19:22.429139: step 34480, loss = 21.31, batch loss = 21.22 (8.1 examples/sec; 0.986 sec/batch; 81h:36m:36s remains)
INFO - root - 2017-12-07 17:19:31.859922: step 34490, loss = 21.51, batch loss = 21.42 (9.3 examples/sec; 0.858 sec/batch; 70h:59m:30s remains)
INFO - root - 2017-12-07 17:19:41.394910: step 34500, loss = 21.62, batch loss = 21.54 (9.1 examples/sec; 0.877 sec/batch; 72h:33m:58s remains)
2017-12-07 17:19:42.460852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3611035 -4.3246846 -4.3056159 -4.2965169 -4.3056269 -4.3482652 -4.4003963 -4.4252467 -4.370923 -4.2711945 -4.1764722 -4.1223488 -4.1369567 -4.2107129 -4.2952924][-4.4786959 -4.4371815 -4.4094872 -4.3937807 -4.3984504 -4.4290042 -4.4651523 -4.4853096 -4.4395485 -4.3480754 -4.2475481 -4.1736984 -4.1699762 -4.2438922 -4.3463221][-4.5467563 -4.5188642 -4.5042543 -4.4984822 -4.5036592 -4.5180874 -4.53133 -4.5378547 -4.5021353 -4.4319506 -4.3464923 -4.2736859 -4.257019 -4.3116336 -4.3956723][-4.5586181 -4.5654578 -4.5794849 -4.5872474 -4.5850019 -4.5726395 -4.5569215 -4.5524178 -4.5371103 -4.503562 -4.45224 -4.3994493 -4.3790112 -4.4054942 -4.4478359][-4.5387893 -4.574008 -4.5999556 -4.5927091 -4.5490546 -4.4867892 -4.4432492 -4.45638 -4.503993 -4.5443811 -4.5470147 -4.5169907 -4.4858704 -4.4739208 -4.4691744][-4.5067849 -4.5383477 -4.5312428 -4.4601994 -4.3340197 -4.2000017 -4.1336336 -4.1946082 -4.3478756 -4.500741 -4.58039 -4.5786333 -4.5337892 -4.48245 -4.4382262][-4.4764528 -4.465857 -4.3890214 -4.2295923 -4.0137787 -3.8102078 -3.7188365 -3.8202097 -4.0747509 -4.3448005 -4.5144677 -4.5569086 -4.5150962 -4.445827 -4.3858628][-4.4205685 -4.3538313 -4.214757 -3.9990344 -3.7393484 -3.5011539 -3.3844588 -3.4888494 -3.7928152 -4.1398277 -4.3826089 -4.4720545 -4.4502964 -4.3879881 -4.3362579][-4.3501139 -4.2495265 -4.0939136 -3.8872321 -3.6542168 -3.4357684 -3.3113313 -3.3895223 -3.67817 -4.0328579 -4.29896 -4.4099865 -4.4042354 -4.357192 -4.3202963][-4.308135 -4.214859 -4.0905128 -3.9381914 -3.7715516 -3.6070035 -3.4952128 -3.5394838 -3.7699542 -4.0746384 -4.3102932 -4.4058857 -4.3977442 -4.3600216 -4.33441][-4.3155336 -4.2547979 -4.1771159 -4.0828085 -3.9808309 -3.8748989 -3.7922311 -3.8196745 -3.9903402 -4.2218757 -4.3935423 -4.4462347 -4.4185281 -4.3801737 -4.3611708][-4.3590956 -4.3367949 -4.3055243 -4.2622256 -4.215539 -4.1658964 -4.1226406 -4.1477485 -4.2621202 -4.4048028 -4.4903784 -4.4855886 -4.4318557 -4.3886995 -4.37568][-4.4058223 -4.4130177 -4.4157839 -4.4111567 -4.4069 -4.4031754 -4.397315 -4.4244261 -4.4889679 -4.5455637 -4.5468311 -4.4887118 -4.4146681 -4.370636 -4.3667746][-4.434504 -4.4488215 -4.47005 -4.4924569 -4.518136 -4.5441451 -4.5587626 -4.5773792 -4.5978117 -4.5914345 -4.5397711 -4.455411 -4.3782396 -4.3408828 -4.3476191][-4.4396849 -4.4395595 -4.4595232 -4.4947181 -4.5369716 -4.5760374 -4.5941353 -4.596724 -4.5846262 -4.5460906 -4.4792252 -4.3992767 -4.3358469 -4.3106704 -4.3248234]]...]
INFO - root - 2017-12-07 17:19:51.400265: step 34510, loss = 21.49, batch loss = 21.41 (8.4 examples/sec; 0.952 sec/batch; 78h:46m:00s remains)
INFO - root - 2017-12-07 17:20:00.781638: step 34520, loss = 21.40, batch loss = 21.32 (9.1 examples/sec; 0.883 sec/batch; 73h:04m:36s remains)
INFO - root - 2017-12-07 17:20:10.180860: step 34530, loss = 20.95, batch loss = 20.87 (9.2 examples/sec; 0.870 sec/batch; 72h:00m:46s remains)
INFO - root - 2017-12-07 17:20:19.610929: step 34540, loss = 21.34, batch loss = 21.26 (8.8 examples/sec; 0.909 sec/batch; 75h:12m:36s remains)
INFO - root - 2017-12-07 17:20:28.962455: step 34550, loss = 21.91, batch loss = 21.83 (8.2 examples/sec; 0.979 sec/batch; 80h:59m:38s remains)
INFO - root - 2017-12-07 17:20:38.347936: step 34560, loss = 21.38, batch loss = 21.30 (8.2 examples/sec; 0.978 sec/batch; 80h:55m:31s remains)
INFO - root - 2017-12-07 17:20:47.753781: step 34570, loss = 21.39, batch loss = 21.30 (8.6 examples/sec; 0.926 sec/batch; 76h:39m:53s remains)
INFO - root - 2017-12-07 17:20:57.308957: step 34580, loss = 21.71, batch loss = 21.63 (8.3 examples/sec; 0.963 sec/batch; 79h:41m:30s remains)
INFO - root - 2017-12-07 17:21:06.721239: step 34590, loss = 21.69, batch loss = 21.61 (8.3 examples/sec; 0.961 sec/batch; 79h:29m:02s remains)
INFO - root - 2017-12-07 17:21:16.161987: step 34600, loss = 21.20, batch loss = 21.12 (8.2 examples/sec; 0.979 sec/batch; 81h:01m:13s remains)
2017-12-07 17:21:17.074579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.60137 -4.6881781 -4.72778 -4.6945362 -4.6165891 -4.5435386 -4.5371995 -4.6115818 -4.7304068 -4.8309827 -4.8650217 -4.83826 -4.7939558 -4.7537017 -4.6950769][-4.635602 -4.7273674 -4.7846823 -4.7758527 -4.7116861 -4.6382766 -4.6203818 -4.6733365 -4.7610354 -4.8447714 -4.8995337 -4.914897 -4.8853993 -4.8039341 -4.6620421][-4.6857729 -4.7903218 -4.8685017 -4.8813334 -4.8158283 -4.7245464 -4.6850715 -4.7112494 -4.7635727 -4.832437 -4.917017 -4.9709859 -4.9243045 -4.7545323 -4.4948606][-4.7696943 -4.8740454 -4.9380407 -4.9187932 -4.7987132 -4.6503739 -4.582149 -4.6038437 -4.661417 -4.7553515 -4.8864312 -4.961586 -4.8633685 -4.58502 -4.213304][-4.8390355 -4.9188328 -4.9256525 -4.821713 -4.5957041 -4.3516879 -4.2484307 -4.3004336 -4.4224095 -4.5856495 -4.7729731 -4.86841 -4.7388377 -4.399919 -3.9788032][-4.8475652 -4.8759747 -4.8127589 -4.6169691 -4.2730227 -3.9124546 -3.7573771 -3.8550489 -4.0828657 -4.3437743 -4.593184 -4.7210588 -4.6106782 -4.297936 -3.921267][-4.8031182 -4.7861323 -4.6810403 -4.4330249 -4.00996 -3.54813 -3.3293719 -3.4604943 -3.7970581 -4.1631908 -4.4662161 -4.6155515 -4.5361791 -4.2915049 -4.0187359][-4.7311311 -4.7048297 -4.6105456 -4.3789492 -3.9602978 -3.4703708 -3.2065442 -3.3302469 -3.7123969 -4.1330161 -4.4501204 -4.5891566 -4.5257616 -4.3499274 -4.192502][-4.6077456 -4.6172657 -4.591382 -4.4357724 -4.096015 -3.6598291 -3.3878317 -3.4612584 -3.7992256 -4.191546 -4.4693074 -4.5748186 -4.5226474 -4.4104023 -4.3562813][-4.4229259 -4.48906 -4.5631037 -4.5133371 -4.2894983 -3.9554 -3.7125108 -3.7270439 -3.9700298 -4.2732725 -4.4836097 -4.5616655 -4.5372438 -4.4831858 -4.4929461][-4.2129297 -4.3163719 -4.4704266 -4.520215 -4.4207525 -4.2180634 -4.0407839 -4.0161762 -4.16197 -4.3703971 -4.5222783 -4.5907183 -4.5999627 -4.584744 -4.608726][-4.0493622 -4.1448755 -4.3289123 -4.4552407 -4.481389 -4.42071 -4.3280482 -4.2919159 -4.3637094 -4.500422 -4.6169691 -4.6807861 -4.69927 -4.6838045 -4.6834207][-4.0212336 -4.0694828 -4.2272744 -4.3871179 -4.5039191 -4.5573506 -4.5502505 -4.5309319 -4.56307 -4.655786 -4.7521009 -4.8029518 -4.7951937 -4.7417035 -4.6964388][-4.1454492 -4.1466231 -4.249629 -4.3848534 -4.5151882 -4.6112914 -4.6587491 -4.670629 -4.6917744 -4.749896 -4.8158116 -4.83757 -4.7903337 -4.6971059 -4.6162004][-4.360302 -4.3353548 -4.3857117 -4.4641328 -4.5450892 -4.6135526 -4.6602378 -4.676753 -4.6783657 -4.6914706 -4.7133851 -4.706563 -4.6417723 -4.5427866 -4.4602289]]...]
INFO - root - 2017-12-07 17:21:26.413919: step 34610, loss = 21.40, batch loss = 21.32 (8.3 examples/sec; 0.970 sec/batch; 80h:13m:49s remains)
INFO - root - 2017-12-07 17:21:35.780112: step 34620, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.972 sec/batch; 80h:26m:05s remains)
INFO - root - 2017-12-07 17:21:45.203800: step 34630, loss = 21.70, batch loss = 21.62 (8.0 examples/sec; 1.004 sec/batch; 83h:04m:07s remains)
INFO - root - 2017-12-07 17:21:54.725886: step 34640, loss = 21.31, batch loss = 21.23 (9.1 examples/sec; 0.882 sec/batch; 72h:57m:49s remains)
INFO - root - 2017-12-07 17:22:03.997720: step 34650, loss = 21.51, batch loss = 21.43 (9.3 examples/sec; 0.865 sec/batch; 71h:33m:11s remains)
INFO - root - 2017-12-07 17:22:13.438801: step 34660, loss = 21.56, batch loss = 21.48 (8.4 examples/sec; 0.947 sec/batch; 78h:23m:13s remains)
INFO - root - 2017-12-07 17:22:22.940706: step 34670, loss = 21.76, batch loss = 21.67 (8.1 examples/sec; 0.992 sec/batch; 82h:06m:34s remains)
INFO - root - 2017-12-07 17:22:32.033531: step 34680, loss = 21.05, batch loss = 20.97 (10.0 examples/sec; 0.798 sec/batch; 66h:00m:39s remains)
INFO - root - 2017-12-07 17:22:41.475471: step 34690, loss = 21.30, batch loss = 21.22 (7.9 examples/sec; 1.009 sec/batch; 83h:26m:29s remains)
INFO - root - 2017-12-07 17:22:50.891605: step 34700, loss = 21.38, batch loss = 21.30 (8.6 examples/sec; 0.932 sec/batch; 77h:04m:21s remains)
2017-12-07 17:22:51.767542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3373289 -4.3485594 -4.3598118 -4.36608 -4.3672924 -4.3653183 -4.3632169 -4.3603258 -4.35333 -4.3396935 -4.325098 -4.3167534 -4.3149543 -4.3164029 -4.3144522][-4.383687 -4.4116769 -4.4341116 -4.4441977 -4.44563 -4.4422603 -4.4375396 -4.4314923 -4.4204774 -4.402514 -4.3834448 -4.3716874 -4.3677182 -4.3669171 -4.3601513][-4.4446511 -4.482482 -4.505568 -4.5096455 -4.504045 -4.4944921 -4.4836473 -4.4729009 -4.4603953 -4.4455681 -4.4323215 -4.42766 -4.4298363 -4.4315181 -4.4222808][-4.4879131 -4.5164933 -4.5241728 -4.5085006 -4.4838133 -4.4562755 -4.4268794 -4.4030008 -4.393012 -4.4001937 -4.4194593 -4.4470754 -4.4722691 -4.4842758 -4.475831][-4.4835215 -4.4843097 -4.4658947 -4.4235153 -4.3717737 -4.3115249 -4.2415175 -4.1874447 -4.1806588 -4.2325191 -4.3164439 -4.4008102 -4.459722 -4.4848866 -4.4795585][-4.4271913 -4.3943262 -4.3495035 -4.2855587 -4.2123342 -4.118917 -3.9979277 -3.8961778 -3.8797071 -3.980118 -4.1436005 -4.2953773 -4.3878064 -4.4203439 -4.4149203][-4.3440628 -4.2878451 -4.2206612 -4.1394029 -4.0539374 -3.9439006 -3.7910509 -3.6458855 -3.6042893 -3.7302494 -3.9558992 -4.1675563 -4.2938938 -4.3363676 -4.3293538][-4.2765341 -4.2142129 -4.1357203 -4.0426645 -3.9519184 -3.8433657 -3.6921024 -3.53054 -3.4594007 -3.5697589 -3.8062017 -4.046371 -4.2028003 -4.2655764 -4.2654982][-4.2584019 -4.2096643 -4.140686 -4.0558348 -3.9754264 -3.885287 -3.7614403 -3.6158924 -3.52966 -3.5971453 -3.7909307 -4.0129919 -4.1740241 -4.246594 -4.2506051][-4.2759967 -4.2539015 -4.2121944 -4.1581097 -4.1119943 -4.0616732 -3.9848289 -3.8806973 -3.8013272 -3.8230479 -3.9456234 -4.1071291 -4.2291074 -4.2760415 -4.2607875][-4.269021 -4.2710843 -4.2632241 -4.2524471 -4.2549272 -4.2554827 -4.2315693 -4.1735644 -4.1100397 -4.0966854 -4.148272 -4.2321215 -4.2889414 -4.2868571 -4.2386022][-4.2193179 -4.2317247 -4.2554965 -4.2882919 -4.3288555 -4.3588834 -4.3629518 -4.3327832 -4.2824597 -4.2512426 -4.2555671 -4.2803383 -4.2838588 -4.2445135 -4.1766253][-4.1527619 -4.162848 -4.2028708 -4.2599812 -4.3094969 -4.3320718 -4.3293924 -4.3056893 -4.2675805 -4.2392869 -4.2296381 -4.2286239 -4.2075386 -4.1587629 -4.0976315][-4.1226835 -4.1289515 -4.1721816 -4.2294583 -4.2591753 -4.2504935 -4.2280707 -4.2050433 -4.1800294 -4.1671743 -4.1659589 -4.1644511 -4.1440349 -4.1092811 -4.0744953][-4.1493111 -4.1555138 -4.1997647 -4.246491 -4.2496595 -4.2148142 -4.1844773 -4.1695018 -4.1584468 -4.1588268 -4.1662536 -4.1698136 -4.1584511 -4.1427312 -4.1345716]]...]
INFO - root - 2017-12-07 17:23:00.956021: step 34710, loss = 21.37, batch loss = 21.29 (9.1 examples/sec; 0.882 sec/batch; 72h:57m:33s remains)
INFO - root - 2017-12-07 17:23:10.483037: step 34720, loss = 21.49, batch loss = 21.41 (8.9 examples/sec; 0.896 sec/batch; 74h:07m:09s remains)
INFO - root - 2017-12-07 17:23:19.994793: step 34730, loss = 21.61, batch loss = 21.53 (8.8 examples/sec; 0.905 sec/batch; 74h:49m:13s remains)
INFO - root - 2017-12-07 17:23:29.384970: step 34740, loss = 21.37, batch loss = 21.29 (9.3 examples/sec; 0.857 sec/batch; 70h:54m:55s remains)
INFO - root - 2017-12-07 17:23:38.717868: step 34750, loss = 21.44, batch loss = 21.35 (8.1 examples/sec; 0.987 sec/batch; 81h:37m:09s remains)
INFO - root - 2017-12-07 17:23:48.162937: step 34760, loss = 21.26, batch loss = 21.18 (8.4 examples/sec; 0.958 sec/batch; 79h:12m:26s remains)
INFO - root - 2017-12-07 17:23:57.624702: step 34770, loss = 21.43, batch loss = 21.34 (8.1 examples/sec; 0.988 sec/batch; 81h:43m:18s remains)
INFO - root - 2017-12-07 17:24:06.937954: step 34780, loss = 21.61, batch loss = 21.53 (8.1 examples/sec; 0.990 sec/batch; 81h:52m:34s remains)
INFO - root - 2017-12-07 17:24:16.319695: step 34790, loss = 21.83, batch loss = 21.75 (8.4 examples/sec; 0.950 sec/batch; 78h:33m:38s remains)
INFO - root - 2017-12-07 17:24:25.750834: step 34800, loss = 21.64, batch loss = 21.56 (8.4 examples/sec; 0.950 sec/batch; 78h:33m:47s remains)
2017-12-07 17:24:26.679315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3141017 -4.3471913 -4.3478575 -4.30661 -4.2566915 -4.2821879 -4.3837008 -4.4989352 -4.5784278 -4.5902381 -4.5620737 -4.5243316 -4.4931602 -4.4932027 -4.5004668][-4.4652548 -4.5326157 -4.55696 -4.536087 -4.4931474 -4.490653 -4.5320363 -4.5737591 -4.5954485 -4.586072 -4.5566616 -4.5192013 -4.4783144 -4.4549546 -4.4303484][-4.6270819 -4.7141953 -4.7432714 -4.7134728 -4.6416211 -4.5800128 -4.5388947 -4.49969 -4.4750957 -4.4758396 -4.4827108 -4.4691973 -4.4259577 -4.3761768 -4.3219423][-4.729702 -4.8037372 -4.8072429 -4.7335896 -4.5996 -4.4652758 -4.3565612 -4.2754283 -4.2491078 -4.2979431 -4.3679705 -4.3950024 -4.3595824 -4.2914124 -4.220715][-4.7322755 -4.7619176 -4.7218256 -4.5984516 -4.407546 -4.2279682 -4.1026797 -4.0382862 -4.043376 -4.1363277 -4.2518406 -4.3181448 -4.3105235 -4.249465 -4.185185][-4.6754079 -4.6536274 -4.57223 -4.4130292 -4.1919804 -4.0050859 -3.9001522 -3.874629 -3.9077089 -4.0147605 -4.1536889 -4.2662039 -4.3128123 -4.2912683 -4.25908][-4.6496229 -4.6082768 -4.5001178 -4.3122954 -4.0760078 -3.894803 -3.8100438 -3.8062255 -3.8442001 -3.9462585 -4.1065779 -4.2714319 -4.3765798 -4.4015412 -4.4025288][-4.691843 -4.6696486 -4.5495062 -4.3397889 -4.0994706 -3.9312086 -3.8620055 -3.867142 -3.9021344 -3.9978073 -4.1718969 -4.3632178 -4.4859881 -4.5199642 -4.5269403][-4.782764 -4.7874556 -4.6636271 -4.4428792 -4.2097063 -4.0605278 -4.0143561 -4.0396891 -4.086164 -4.1817694 -4.3448267 -4.5069246 -4.5820551 -4.5750194 -4.5590773][-4.8515792 -4.8750882 -4.7556391 -4.534852 -4.3135371 -4.1884861 -4.1788549 -4.246254 -4.3251653 -4.4219775 -4.5424428 -4.624177 -4.6109285 -4.5409446 -4.49313][-4.8295975 -4.8503437 -4.7324224 -4.5180588 -4.3162174 -4.2242589 -4.2649536 -4.3929558 -4.5181751 -4.6207647 -4.6940451 -4.6924386 -4.5993738 -4.4774604 -4.4050884][-4.7531462 -4.7378669 -4.5937319 -4.366879 -4.1702595 -4.0971551 -4.1770334 -4.3669591 -4.5510154 -4.6823926 -4.7448974 -4.7052975 -4.5812616 -4.4496422 -4.3775978][-4.6824603 -4.6190619 -4.4269609 -4.1631131 -3.9472194 -3.8691721 -3.9619477 -4.190309 -4.4228721 -4.5960054 -4.6841216 -4.6568727 -4.55218 -4.4512668 -4.40602][-4.6352892 -4.5377016 -4.3037567 -4.0037627 -3.7712126 -3.6967976 -3.806035 -4.0572076 -4.3109903 -4.4964218 -4.5901775 -4.5799508 -4.5144582 -4.46358 -4.4543667][-4.6333752 -4.531291 -4.2826872 -3.9609313 -3.7172766 -3.6539748 -3.7848258 -4.051003 -4.3038006 -4.4591575 -4.51376 -4.4996319 -4.472373 -4.4753366 -4.5015121]]...]
INFO - root - 2017-12-07 17:24:36.165241: step 34810, loss = 21.31, batch loss = 21.23 (9.0 examples/sec; 0.891 sec/batch; 73h:42m:56s remains)
INFO - root - 2017-12-07 17:24:45.541020: step 34820, loss = 21.67, batch loss = 21.59 (8.0 examples/sec; 0.998 sec/batch; 82h:31m:46s remains)
INFO - root - 2017-12-07 17:24:54.859193: step 34830, loss = 21.35, batch loss = 21.27 (8.8 examples/sec; 0.909 sec/batch; 75h:10m:21s remains)
INFO - root - 2017-12-07 17:25:04.288859: step 34840, loss = 20.94, batch loss = 20.86 (9.1 examples/sec; 0.881 sec/batch; 72h:49m:33s remains)
INFO - root - 2017-12-07 17:25:13.643005: step 34850, loss = 21.52, batch loss = 21.44 (8.6 examples/sec; 0.931 sec/batch; 76h:59m:00s remains)
INFO - root - 2017-12-07 17:25:22.995869: step 34860, loss = 21.66, batch loss = 21.57 (8.7 examples/sec; 0.921 sec/batch; 76h:07m:54s remains)
INFO - root - 2017-12-07 17:25:32.466044: step 34870, loss = 21.28, batch loss = 21.20 (8.3 examples/sec; 0.963 sec/batch; 79h:38m:38s remains)
INFO - root - 2017-12-07 17:25:41.863540: step 34880, loss = 21.39, batch loss = 21.31 (8.2 examples/sec; 0.980 sec/batch; 81h:01m:18s remains)
INFO - root - 2017-12-07 17:25:51.287848: step 34890, loss = 21.56, batch loss = 21.48 (8.2 examples/sec; 0.979 sec/batch; 80h:54m:13s remains)
INFO - root - 2017-12-07 17:26:00.873753: step 34900, loss = 21.04, batch loss = 20.96 (8.0 examples/sec; 0.998 sec/batch; 82h:28m:19s remains)
2017-12-07 17:26:01.819247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0115395 -4.0031633 -4.0657177 -4.1682739 -4.2710404 -4.3367834 -4.3228312 -4.2403173 -4.1382179 -4.0733991 -4.0790181 -4.1392503 -4.2257776 -4.3263364 -4.4350691][-4.1498356 -4.1278896 -4.1475344 -4.20541 -4.2849822 -4.3447876 -4.3351793 -4.2575684 -4.1475182 -4.0603004 -4.0445161 -4.0973525 -4.1882396 -4.298243 -4.4122052][-4.307888 -4.2383089 -4.1891537 -4.1971688 -4.2627735 -4.3383365 -4.3667164 -4.3399911 -4.2676497 -4.1884341 -4.158638 -4.1928205 -4.270216 -4.3703365 -4.47564][-4.3929873 -4.2633495 -4.1488461 -4.1115627 -4.1598997 -4.2472525 -4.3220577 -4.3683867 -4.3719006 -4.3355312 -4.3055239 -4.3158374 -4.36787 -4.449007 -4.5419607][-4.39278 -4.229784 -4.0902033 -4.0297093 -4.0517883 -4.1230617 -4.2167492 -4.3219957 -4.4073272 -4.42893 -4.4000731 -4.373641 -4.384 -4.4395885 -4.5273032][-4.3704357 -4.2132831 -4.09054 -4.02576 -4.0115728 -4.0372152 -4.1095772 -4.22952 -4.358644 -4.4195647 -4.3875089 -4.3240108 -4.2928524 -4.3287539 -4.4243803][-4.3684549 -4.2421327 -4.1532354 -4.0897956 -4.0376139 -4.0147567 -4.0518055 -4.1567893 -4.283268 -4.3439198 -4.2997832 -4.2113686 -4.1534882 -4.1805186 -4.2920074][-4.3917255 -4.2924929 -4.2289805 -4.1650295 -4.0830712 -4.02566 -4.0380216 -4.12975 -4.2416983 -4.2867641 -4.2329597 -4.1354666 -4.0661168 -4.0916414 -4.2149248][-4.4400082 -4.3549089 -4.297646 -4.2308669 -4.1349015 -4.0549512 -4.0446906 -4.1203647 -4.2175355 -4.2511187 -4.2007046 -4.111722 -4.0466719 -4.0767736 -4.203856][-4.4607468 -4.4019942 -4.3545284 -4.2948017 -4.2065258 -4.1196184 -4.0830288 -4.12929 -4.2056379 -4.2340274 -4.2037077 -4.1405287 -4.089982 -4.1189752 -4.2275028][-4.3757944 -4.3700628 -4.3588982 -4.327529 -4.2716651 -4.2008777 -4.1552958 -4.1730742 -4.2251515 -4.2560368 -4.2611065 -4.2429967 -4.21938 -4.23552 -4.2945652][-4.1992235 -4.2423034 -4.2801132 -4.2940903 -4.2881265 -4.2568316 -4.2298107 -4.2445431 -4.2896791 -4.331604 -4.37289 -4.3991771 -4.4008427 -4.3937416 -4.3834581][-4.04463 -4.101553 -4.1694841 -4.2239561 -4.2642455 -4.2718129 -4.2729211 -4.3037763 -4.3612442 -4.4235578 -4.491591 -4.5456548 -4.5632524 -4.5387125 -4.4718919][-4.0100942 -4.0419374 -4.1015091 -4.1669908 -4.2230511 -4.245348 -4.2636886 -4.3151975 -4.393611 -4.4774389 -4.5614138 -4.6289549 -4.6553254 -4.62171 -4.5187593][-4.1198969 -4.1021194 -4.1176424 -4.1524348 -4.1829767 -4.1904073 -4.2112923 -4.278286 -4.372086 -4.4662666 -4.5570307 -4.6289506 -4.652905 -4.6098356 -4.4865894]]...]
INFO - root - 2017-12-07 17:26:11.298443: step 34910, loss = 21.62, batch loss = 21.54 (9.1 examples/sec; 0.882 sec/batch; 72h:54m:26s remains)
INFO - root - 2017-12-07 17:26:20.659768: step 34920, loss = 21.71, batch loss = 21.63 (9.1 examples/sec; 0.882 sec/batch; 72h:55m:24s remains)
INFO - root - 2017-12-07 17:26:30.180461: step 34930, loss = 21.66, batch loss = 21.58 (8.7 examples/sec; 0.916 sec/batch; 75h:45m:20s remains)
INFO - root - 2017-12-07 17:26:39.577994: step 34940, loss = 21.50, batch loss = 21.42 (8.3 examples/sec; 0.960 sec/batch; 79h:21m:41s remains)
INFO - root - 2017-12-07 17:26:48.963160: step 34950, loss = 21.85, batch loss = 21.77 (8.1 examples/sec; 0.991 sec/batch; 81h:54m:05s remains)
INFO - root - 2017-12-07 17:26:58.502329: step 34960, loss = 21.30, batch loss = 21.22 (8.6 examples/sec; 0.931 sec/batch; 76h:55m:55s remains)
INFO - root - 2017-12-07 17:27:07.998593: step 34970, loss = 21.54, batch loss = 21.46 (8.6 examples/sec; 0.928 sec/batch; 76h:40m:59s remains)
INFO - root - 2017-12-07 17:27:17.501413: step 34980, loss = 21.54, batch loss = 21.45 (8.5 examples/sec; 0.936 sec/batch; 77h:23m:12s remains)
INFO - root - 2017-12-07 17:27:26.920507: step 34990, loss = 21.48, batch loss = 21.39 (9.2 examples/sec; 0.873 sec/batch; 72h:10m:48s remains)
INFO - root - 2017-12-07 17:27:36.321109: step 35000, loss = 21.61, batch loss = 21.52 (8.2 examples/sec; 0.970 sec/batch; 80h:08m:51s remains)
2017-12-07 17:27:37.299022: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4011445 -4.394104 -4.3635173 -4.3778481 -4.4535618 -4.5384121 -4.5245528 -4.4540038 -4.4025674 -4.3339553 -4.2746696 -4.2615185 -4.3486972 -4.4502249 -4.5438485][-4.2961721 -4.304327 -4.2901769 -4.3246503 -4.4392347 -4.537334 -4.4976187 -4.388042 -4.3126836 -4.249393 -4.2231722 -4.242866 -4.3383923 -4.4403634 -4.5379076][-4.1597748 -4.2110553 -4.2228503 -4.26697 -4.3785739 -4.4490957 -4.38557 -4.2735643 -4.2165737 -4.1959662 -4.2120032 -4.261116 -4.3509541 -4.4270263 -4.5061736][-4.03529 -4.1195416 -4.1373916 -4.1764932 -4.2493358 -4.2665963 -4.1961746 -4.10995 -4.0927424 -4.1307354 -4.1831121 -4.2525158 -4.3317132 -4.3750668 -4.4360938][-3.9485071 -4.0152736 -4.01238 -4.0337372 -4.0508938 -4.0134778 -3.9454339 -3.8842952 -3.8931351 -3.9737825 -4.0495162 -4.1382661 -4.2280231 -4.2607203 -4.3260283][-3.9654591 -3.9750142 -3.92983 -3.9210877 -3.8718448 -3.779007 -3.7052832 -3.6561165 -3.6812932 -3.7813649 -3.8552461 -3.9493141 -4.0711012 -4.1308026 -4.2239656][-4.0740218 -4.0261841 -3.9541337 -3.9275928 -3.8307304 -3.691761 -3.5969155 -3.5504858 -3.593116 -3.6920598 -3.7493887 -3.8340342 -3.9806929 -4.0695014 -4.1794081][-4.2025657 -4.1418643 -4.0775967 -4.0550709 -3.965045 -3.8327713 -3.7266109 -3.6741166 -3.717792 -3.7959282 -3.8389444 -3.924458 -4.0877886 -4.1816287 -4.2684679][-4.269237 -4.2362728 -4.2033372 -4.1973696 -4.1446786 -4.0592847 -3.981642 -3.945281 -3.9845638 -4.0261784 -4.0390105 -4.1035566 -4.2440944 -4.3200507 -4.3809314][-4.2827463 -4.3059616 -4.3167157 -4.3234529 -4.297605 -4.2488441 -4.2033019 -4.19093 -4.2245088 -4.2256331 -4.1969023 -4.2269583 -4.3220634 -4.3659768 -4.4116912][-4.2548819 -4.3391953 -4.3939009 -4.4136643 -4.4047837 -4.3754854 -4.349649 -4.3560972 -4.384212 -4.3627381 -4.309999 -4.32873 -4.407795 -4.4378929 -4.4747972][-4.2221336 -4.3310647 -4.3996968 -4.4188504 -4.4193964 -4.4051828 -4.3973866 -4.4173779 -4.4446211 -4.4243145 -4.3756852 -4.4027085 -4.4897828 -4.5291185 -4.5675859][-4.2772641 -4.3716326 -4.4266009 -4.4346251 -4.4373522 -4.4345951 -4.4392815 -4.4599566 -4.4823036 -4.4732857 -4.4445825 -4.4837308 -4.5759292 -4.6198711 -4.6477342][-4.4142203 -4.4775314 -4.5130782 -4.5183282 -4.5263767 -4.5350814 -4.5490708 -4.5690827 -4.5868025 -4.5877461 -4.5774841 -4.613667 -4.687921 -4.7198372 -4.7215137][-4.53696 -4.5666223 -4.5833659 -4.5863967 -4.5942574 -4.6059413 -4.6215048 -4.6374044 -4.6486692 -4.6501503 -4.6459227 -4.6657948 -4.7116327 -4.731812 -4.722558]]...]
INFO - root - 2017-12-07 17:27:46.618800: step 35010, loss = 21.36, batch loss = 21.28 (8.5 examples/sec; 0.942 sec/batch; 77h:48m:39s remains)
INFO - root - 2017-12-07 17:27:55.883869: step 35020, loss = 21.33, batch loss = 21.25 (8.0 examples/sec; 0.995 sec/batch; 82h:15m:06s remains)
INFO - root - 2017-12-07 17:28:05.160192: step 35030, loss = 21.32, batch loss = 21.24 (8.3 examples/sec; 0.969 sec/batch; 80h:05m:58s remains)
INFO - root - 2017-12-07 17:28:14.647278: step 35040, loss = 21.42, batch loss = 21.34 (8.1 examples/sec; 0.983 sec/batch; 81h:15m:03s remains)
INFO - root - 2017-12-07 17:28:24.095848: step 35050, loss = 21.54, batch loss = 21.45 (9.0 examples/sec; 0.891 sec/batch; 73h:38m:34s remains)
INFO - root - 2017-12-07 17:28:33.593324: step 35060, loss = 21.30, batch loss = 21.21 (8.4 examples/sec; 0.949 sec/batch; 78h:22m:50s remains)
INFO - root - 2017-12-07 17:28:43.091103: step 35070, loss = 22.12, batch loss = 22.04 (8.4 examples/sec; 0.952 sec/batch; 78h:39m:02s remains)
INFO - root - 2017-12-07 17:28:52.462959: step 35080, loss = 21.19, batch loss = 21.11 (8.3 examples/sec; 0.959 sec/batch; 79h:14m:35s remains)
INFO - root - 2017-12-07 17:29:01.980211: step 35090, loss = 21.58, batch loss = 21.49 (8.4 examples/sec; 0.956 sec/batch; 78h:58m:13s remains)
INFO - root - 2017-12-07 17:29:11.432081: step 35100, loss = 21.28, batch loss = 21.19 (8.7 examples/sec; 0.915 sec/batch; 75h:36m:25s remains)
2017-12-07 17:29:12.405272: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4567556 -4.5179334 -4.5711846 -4.5907178 -4.5714207 -4.536716 -4.507618 -4.484849 -4.4634852 -4.4462843 -4.4504714 -4.4924011 -4.5559254 -4.5945983 -4.5857][-4.4621525 -4.5241055 -4.57103 -4.5824771 -4.5570688 -4.5265512 -4.5137129 -4.5037789 -4.4861674 -4.4717655 -4.4789639 -4.526433 -4.5951548 -4.6367135 -4.6299939][-4.4439611 -4.489717 -4.5152369 -4.509263 -4.475564 -4.455543 -4.4693427 -4.4810576 -4.4702907 -4.4570007 -4.4614148 -4.5058727 -4.5733161 -4.6217465 -4.6347232][-4.3976574 -4.4287992 -4.4408059 -4.4283314 -4.4011207 -4.3959131 -4.4241571 -4.43954 -4.4215565 -4.3985968 -4.3937607 -4.4273663 -4.4887557 -4.545413 -4.5866013][-4.3339396 -4.35443 -4.358294 -4.3462329 -4.3419056 -4.3595796 -4.3785973 -4.3626513 -4.3145976 -4.27554 -4.26614 -4.3007388 -4.3713622 -4.4511 -4.5280404][-4.2803116 -4.2786765 -4.2691388 -4.2558403 -4.27485 -4.3098793 -4.2952828 -4.2137733 -4.1172695 -4.0692444 -4.075592 -4.1382151 -4.248004 -4.3760281 -4.494101][-4.2524729 -4.2031927 -4.1732941 -4.1608953 -4.19917 -4.2437344 -4.1861539 -4.0235167 -3.8682504 -3.8183918 -3.8536317 -3.9600062 -4.1350074 -4.3317757 -4.4919443][-4.2688632 -4.1721277 -4.1193681 -4.1086693 -4.1581726 -4.2123609 -4.134727 -3.9210091 -3.7145548 -3.6600912 -3.7167392 -3.848515 -4.0667973 -4.3044133 -4.4788322][-4.3453965 -4.2517977 -4.1864643 -4.1610069 -4.1933622 -4.2504582 -4.1961379 -3.9963055 -3.7819118 -3.7184126 -3.7775898 -3.9061663 -4.1140385 -4.3255048 -4.4610291][-4.4513779 -4.4006352 -4.3288159 -4.2650709 -4.2462668 -4.2929249 -4.2930746 -4.1617985 -3.9886112 -3.9300711 -3.9887455 -4.1123347 -4.2867718 -4.4358282 -4.5063276][-4.5135131 -4.49953 -4.4225287 -4.3227863 -4.2515059 -4.2862811 -4.3491955 -4.3075266 -4.2010689 -4.1626892 -4.2211027 -4.341289 -4.4839787 -4.5806065 -4.6020293][-4.5324044 -4.5434761 -4.4794774 -4.3775611 -4.2863622 -4.3127527 -4.4143009 -4.4383183 -4.3858609 -4.3575549 -4.3949842 -4.4912939 -4.5978489 -4.656251 -4.6547618][-4.5201044 -4.5464635 -4.51253 -4.4418588 -4.3712296 -4.3987741 -4.5006366 -4.5384459 -4.4978819 -4.4561076 -4.4600339 -4.524745 -4.6056232 -4.6496329 -4.6476879][-4.4735112 -4.499825 -4.4905763 -4.4609647 -4.4342394 -4.469492 -4.5435534 -4.5628304 -4.518847 -4.4669771 -4.4529209 -4.4999976 -4.5695286 -4.6103921 -4.6097283][-4.428236 -4.4363337 -4.4305058 -4.426435 -4.4331226 -4.4692283 -4.512372 -4.5155535 -4.4815989 -4.4440227 -4.4336734 -4.4663353 -4.5172696 -4.5493054 -4.54729]]...]
INFO - root - 2017-12-07 17:29:21.721939: step 35110, loss = 21.56, batch loss = 21.48 (8.1 examples/sec; 0.992 sec/batch; 81h:58m:59s remains)
INFO - root - 2017-12-07 17:29:31.118045: step 35120, loss = 20.77, batch loss = 20.69 (8.0 examples/sec; 1.002 sec/batch; 82h:46m:56s remains)
INFO - root - 2017-12-07 17:29:40.433342: step 35130, loss = 21.68, batch loss = 21.60 (8.5 examples/sec; 0.938 sec/batch; 77h:28m:22s remains)
INFO - root - 2017-12-07 17:29:49.898430: step 35140, loss = 21.79, batch loss = 21.70 (8.7 examples/sec; 0.916 sec/batch; 75h:40m:16s remains)
INFO - root - 2017-12-07 17:29:59.310969: step 35150, loss = 21.47, batch loss = 21.38 (8.6 examples/sec; 0.932 sec/batch; 76h:57m:18s remains)
INFO - root - 2017-12-07 17:30:08.751478: step 35160, loss = 21.62, batch loss = 21.53 (8.3 examples/sec; 0.965 sec/batch; 79h:42m:08s remains)
INFO - root - 2017-12-07 17:30:18.031097: step 35170, loss = 21.28, batch loss = 21.20 (8.7 examples/sec; 0.922 sec/batch; 76h:08m:35s remains)
INFO - root - 2017-12-07 17:30:27.402571: step 35180, loss = 21.45, batch loss = 21.36 (9.1 examples/sec; 0.874 sec/batch; 72h:12m:52s remains)
INFO - root - 2017-12-07 17:30:36.747851: step 35190, loss = 21.17, batch loss = 21.08 (8.7 examples/sec; 0.916 sec/batch; 75h:41m:18s remains)
INFO - root - 2017-12-07 17:30:46.158930: step 35200, loss = 21.92, batch loss = 21.84 (8.9 examples/sec; 0.901 sec/batch; 74h:26m:12s remains)
2017-12-07 17:30:47.100207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.39912 -4.4241519 -4.44769 -4.4673133 -4.4822164 -4.4931374 -4.4942369 -4.4913721 -4.496562 -4.508369 -4.5181775 -4.511363 -4.4895487 -4.46681 -4.4568481][-4.3927188 -4.4304705 -4.4632764 -4.4880371 -4.4998379 -4.5075912 -4.507174 -4.505702 -4.5185118 -4.5385289 -4.5477896 -4.5269852 -4.486949 -4.4543452 -4.44688][-4.4014568 -4.4503932 -4.4915509 -4.5232043 -4.531168 -4.5265245 -4.5088472 -4.4954448 -4.5137973 -4.5465393 -4.5551019 -4.5114 -4.4461842 -4.4025397 -4.4045472][-4.4106493 -4.4568372 -4.4959183 -4.5367451 -4.5546632 -4.54494 -4.5006495 -4.4551539 -4.4637122 -4.5046873 -4.5168414 -4.4663472 -4.3905931 -4.3409882 -4.351131][-4.4061632 -4.43791 -4.4673347 -4.513164 -4.5449557 -4.5370626 -4.4718437 -4.393744 -4.3790812 -4.4094205 -4.4232287 -4.3963747 -4.3476238 -4.3048835 -4.3145776][-4.3865509 -4.4040895 -4.4188 -4.4521132 -4.4746194 -4.4541807 -4.3716197 -4.2787466 -4.2561502 -4.2786942 -4.3006496 -4.3151331 -4.3136287 -4.2860303 -4.2861366][-4.3683338 -4.3735166 -4.3722596 -4.377624 -4.3624191 -4.2987661 -4.187079 -4.094954 -4.095314 -4.1409831 -4.1915355 -4.2439942 -4.2750692 -4.2567821 -4.241878][-4.3556809 -4.3486333 -4.3271976 -4.3014936 -4.2467327 -4.1432037 -4.0072346 -3.9219797 -3.9568925 -4.0415225 -4.1249871 -4.1962438 -4.23132 -4.2065043 -4.1751418][-4.3198504 -4.2961593 -4.2518311 -4.2071323 -4.145319 -4.0470829 -3.928956 -3.8678248 -3.9225385 -4.0255742 -4.1214027 -4.1864891 -4.2001767 -4.1477332 -4.0944247][-4.2806535 -4.2371421 -4.1703024 -4.1179633 -4.0767112 -4.0213351 -3.9560521 -3.9369981 -3.9970322 -4.0902472 -4.1715465 -4.2068505 -4.1852541 -4.1029181 -4.0346217][-4.2727971 -4.22822 -4.1522508 -4.0949574 -4.066299 -4.0407252 -4.0166912 -4.0323091 -4.091753 -4.1663618 -4.2274127 -4.2349229 -4.1910934 -4.10004 -4.0348377][-4.2869854 -4.2623587 -4.2008495 -4.150682 -4.1234236 -4.0983539 -4.0818925 -4.1070786 -4.1618867 -4.2247238 -4.2751274 -4.2710857 -4.2268453 -4.152164 -4.103013][-4.3057251 -4.3057771 -4.2778206 -4.2544756 -4.2388735 -4.2147794 -4.1934056 -4.2069941 -4.2430644 -4.284996 -4.321485 -4.3176088 -4.2874436 -4.2429285 -4.2179585][-4.3384047 -4.3527756 -4.3516178 -4.3549275 -4.3565559 -4.3447561 -4.3289757 -4.3328137 -4.3475971 -4.3625064 -4.377655 -4.3738322 -4.35871 -4.3401623 -4.3336477][-4.3848639 -4.4041886 -4.4153981 -4.4304624 -4.4410157 -4.4380994 -4.4291782 -4.4262934 -4.425662 -4.4226694 -4.4217978 -4.4157386 -4.4062109 -4.397624 -4.3951931]]...]
INFO - root - 2017-12-07 17:30:56.623359: step 35210, loss = 21.31, batch loss = 21.23 (8.5 examples/sec; 0.939 sec/batch; 77h:30m:15s remains)
INFO - root - 2017-12-07 17:31:06.042247: step 35220, loss = 21.38, batch loss = 21.30 (8.8 examples/sec; 0.907 sec/batch; 74h:52m:19s remains)
INFO - root - 2017-12-07 17:31:15.269428: step 35230, loss = 21.27, batch loss = 21.19 (8.5 examples/sec; 0.939 sec/batch; 77h:32m:24s remains)
INFO - root - 2017-12-07 17:31:24.540539: step 35240, loss = 21.68, batch loss = 21.59 (8.4 examples/sec; 0.954 sec/batch; 78h:44m:53s remains)
INFO - root - 2017-12-07 17:31:33.962059: step 35250, loss = 21.46, batch loss = 21.37 (9.2 examples/sec; 0.871 sec/batch; 71h:56m:58s remains)
INFO - root - 2017-12-07 17:31:43.321204: step 35260, loss = 21.83, batch loss = 21.75 (9.4 examples/sec; 0.855 sec/batch; 70h:33m:57s remains)
INFO - root - 2017-12-07 17:31:52.691568: step 35270, loss = 21.65, batch loss = 21.56 (8.7 examples/sec; 0.916 sec/batch; 75h:36m:11s remains)
INFO - root - 2017-12-07 17:32:02.082391: step 35280, loss = 21.83, batch loss = 21.75 (8.4 examples/sec; 0.958 sec/batch; 79h:05m:40s remains)
INFO - root - 2017-12-07 17:32:11.565813: step 35290, loss = 21.74, batch loss = 21.66 (8.3 examples/sec; 0.961 sec/batch; 79h:18m:09s remains)
INFO - root - 2017-12-07 17:32:20.870800: step 35300, loss = 21.59, batch loss = 21.50 (8.6 examples/sec; 0.928 sec/batch; 76h:37m:57s remains)
2017-12-07 17:32:21.833466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4808555 -4.4571166 -4.4697442 -4.4702067 -4.4011765 -4.2564 -4.1127124 -4.0866904 -4.2166328 -4.4067707 -4.5485349 -4.6215267 -4.6626987 -4.6566343 -4.5773153][-4.5113988 -4.4601812 -4.4480171 -4.4447947 -4.3870296 -4.2386284 -4.0770679 -4.0382981 -4.1691113 -4.3671694 -4.5119948 -4.5901995 -4.6381087 -4.6403213 -4.5630703][-4.5104985 -4.447144 -4.4107957 -4.3961711 -4.3513288 -4.2157764 -4.0600634 -4.01884 -4.1457353 -4.342875 -4.484364 -4.55958 -4.6043391 -4.608088 -4.5371914][-4.4850698 -4.4180813 -4.3621407 -4.3310204 -4.2944522 -4.1797223 -4.0453019 -4.0107908 -4.135407 -4.3304539 -4.4701166 -4.5408807 -4.5750651 -4.5731936 -4.5078268][-4.4384494 -4.364377 -4.29168 -4.2444377 -4.2112484 -4.118938 -4.0107713 -3.984319 -4.1044888 -4.2947145 -4.4381037 -4.5109992 -4.5359659 -4.5273457 -4.4702964][-4.3957505 -4.306263 -4.2137318 -4.1452627 -4.1024055 -4.0196576 -3.9275322 -3.9028943 -4.0159588 -4.2089195 -4.3747482 -4.469883 -4.49923 -4.4884367 -4.43654][-4.3652034 -4.2607903 -4.1541944 -4.0711956 -4.0118256 -3.9226708 -3.8334367 -3.8117733 -3.9291458 -4.1388493 -4.3353229 -4.456964 -4.4926429 -4.4787807 -4.4218841][-4.3632774 -4.2604127 -4.1567965 -4.078691 -4.0133915 -3.9181 -3.8312492 -3.8253398 -3.9627507 -4.191236 -4.4015813 -4.5255156 -4.5503507 -4.5188279 -4.4406266][-4.3822231 -4.3016129 -4.2223096 -4.1709328 -4.1225591 -4.0423045 -3.9723675 -3.9850402 -4.1285596 -4.3452582 -4.5346603 -4.6345043 -4.6386576 -4.5857549 -4.4841676][-4.391223 -4.3523088 -4.3138719 -4.3003507 -4.2814603 -4.2323923 -4.188045 -4.2072558 -4.3262172 -4.4948306 -4.6351819 -4.7017055 -4.6938214 -4.6344757 -4.5245152][-4.354651 -4.3678803 -4.3800607 -4.4033394 -4.4069662 -4.3769155 -4.3394585 -4.3420048 -4.4175634 -4.5326157 -4.6309028 -4.6819773 -4.6818995 -4.6354733 -4.5357871][-4.2823596 -4.3382192 -4.3937821 -4.4405308 -4.4468203 -4.4103041 -4.3587861 -4.3368654 -4.3792992 -4.4671097 -4.5521679 -4.6083388 -4.6263003 -4.5995545 -4.5163455][-4.2412543 -4.31849 -4.3947225 -4.441678 -4.4319019 -4.3733225 -4.2998605 -4.2588425 -4.2864718 -4.3700771 -4.4633322 -4.5337291 -4.5670357 -4.5513492 -4.4791365][-4.2559056 -4.3294272 -4.3951416 -4.4209008 -4.3891654 -4.31245 -4.2270026 -4.17827 -4.2012324 -4.2909083 -4.4017062 -4.4896722 -4.5322032 -4.5143766 -4.4418778][-4.320416 -4.3682055 -4.4027047 -4.4007549 -4.3555822 -4.2776623 -4.195714 -4.1462374 -4.1612725 -4.2467837 -4.3636451 -4.4613605 -4.5088549 -4.4864249 -4.4120393]]...]
INFO - root - 2017-12-07 17:32:31.327794: step 35310, loss = 21.98, batch loss = 21.90 (8.0 examples/sec; 1.002 sec/batch; 82h:45m:09s remains)
INFO - root - 2017-12-07 17:32:40.769770: step 35320, loss = 21.54, batch loss = 21.46 (7.9 examples/sec; 1.018 sec/batch; 84h:01m:11s remains)
INFO - root - 2017-12-07 17:32:50.206136: step 35330, loss = 21.53, batch loss = 21.45 (8.4 examples/sec; 0.954 sec/batch; 78h:44m:27s remains)
INFO - root - 2017-12-07 17:32:59.693552: step 35340, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.933 sec/batch; 76h:58m:42s remains)
INFO - root - 2017-12-07 17:33:09.222814: step 35350, loss = 21.17, batch loss = 21.09 (8.4 examples/sec; 0.949 sec/batch; 78h:19m:56s remains)
INFO - root - 2017-12-07 17:33:18.530796: step 35360, loss = 21.39, batch loss = 21.30 (8.3 examples/sec; 0.959 sec/batch; 79h:08m:27s remains)
INFO - root - 2017-12-07 17:33:27.784741: step 35370, loss = 21.52, batch loss = 21.44 (10.4 examples/sec; 0.768 sec/batch; 63h:22m:02s remains)
INFO - root - 2017-12-07 17:33:37.231744: step 35380, loss = 21.10, batch loss = 21.01 (9.3 examples/sec; 0.857 sec/batch; 70h:42m:25s remains)
INFO - root - 2017-12-07 17:33:46.705834: step 35390, loss = 21.28, batch loss = 21.19 (8.7 examples/sec; 0.924 sec/batch; 76h:17m:35s remains)
INFO - root - 2017-12-07 17:33:56.231430: step 35400, loss = 21.16, batch loss = 21.08 (8.3 examples/sec; 0.963 sec/batch; 79h:29m:07s remains)
2017-12-07 17:33:57.166804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.44807 -4.5799789 -4.658916 -4.6744695 -4.631217 -4.5622282 -4.5081592 -4.4909973 -4.5054712 -4.5403938 -4.5916576 -4.6364169 -4.6673784 -4.6739068 -4.6442246][-4.4675155 -4.5932589 -4.6570711 -4.6578631 -4.589602 -4.4833612 -4.408093 -4.3977456 -4.444303 -4.5139179 -4.5810103 -4.628633 -4.6588373 -4.6602831 -4.6176004][-4.462111 -4.5827909 -4.6385241 -4.6317139 -4.5423036 -4.3992581 -4.3035092 -4.3070245 -4.3955259 -4.5080938 -4.5831304 -4.6157804 -4.6314306 -4.6205692 -4.5673079][-4.4756651 -4.6051245 -4.6539993 -4.626966 -4.5031548 -4.3136382 -4.190558 -4.2084494 -4.3475552 -4.5101905 -4.5937505 -4.6053352 -4.59398 -4.55486 -4.480474][-4.5217075 -4.6592207 -4.686882 -4.6178932 -4.4468679 -4.2075567 -4.0540504 -4.0841317 -4.274312 -4.4921803 -4.5996332 -4.6030688 -4.5608106 -4.4841185 -4.3796897][-4.5660038 -4.6870494 -4.6713672 -4.5517654 -4.3375897 -4.058074 -3.8722839 -3.9049261 -4.1349168 -4.4068856 -4.5640278 -4.5925274 -4.5355015 -4.4269423 -4.2967172][-4.5691333 -4.6445937 -4.5787115 -4.425005 -4.1904984 -3.8881714 -3.6730754 -3.6963806 -3.9491711 -4.2630653 -4.4776816 -4.5567036 -4.5101581 -4.3947306 -4.2634139][-4.5436988 -4.5728884 -4.4747462 -4.3123846 -4.0834479 -3.7860692 -3.5621634 -3.581382 -3.839088 -4.1586285 -4.3931246 -4.492588 -4.4504414 -4.3434687 -4.243331][-4.5350294 -4.551342 -4.4565043 -4.3159537 -4.1199512 -3.8582687 -3.6580012 -3.6744604 -3.9000559 -4.1714854 -4.3628154 -4.4278975 -4.3637967 -4.2656097 -4.2097726][-4.56193 -4.6015019 -4.5415635 -4.4389973 -4.2842464 -4.0686469 -3.9006007 -3.9054642 -4.0772686 -4.2750044 -4.3939538 -4.4027157 -4.3132024 -4.2291379 -4.209631][-4.5906692 -4.6640038 -4.6401496 -4.5710006 -4.450109 -4.2788982 -4.1502447 -4.1576471 -4.2877097 -4.41642 -4.4670339 -4.4297762 -4.3327003 -4.2709713 -4.267457][-4.5895486 -4.6843657 -4.6823592 -4.6341 -4.5416112 -4.4182472 -4.3388057 -4.3610015 -4.4614253 -4.5371251 -4.539845 -4.4806108 -4.3954234 -4.3533974 -4.343092][-4.5543284 -4.6542406 -4.6576982 -4.6167474 -4.5428591 -4.4556632 -4.4130359 -4.4477758 -4.5298934 -4.573606 -4.5551062 -4.4985747 -4.4396334 -4.414186 -4.3905549][-4.5048561 -4.597796 -4.5954795 -4.5493445 -4.475399 -4.398591 -4.3705921 -4.4127741 -4.4872909 -4.5161972 -4.4929566 -4.4520278 -4.4211617 -4.4094157 -4.3808827][-4.4538622 -4.5375504 -4.532815 -4.4799786 -4.3915644 -4.2965441 -4.2550144 -4.2959127 -4.3742766 -4.4036131 -4.3810697 -4.3529258 -4.3427529 -4.3440061 -4.3250642]]...]
INFO - root - 2017-12-07 17:34:06.721751: step 35410, loss = 21.63, batch loss = 21.54 (8.5 examples/sec; 0.939 sec/batch; 77h:31m:53s remains)
INFO - root - 2017-12-07 17:34:16.200555: step 35420, loss = 21.52, batch loss = 21.44 (8.3 examples/sec; 0.968 sec/batch; 79h:53m:07s remains)
INFO - root - 2017-12-07 17:34:25.552446: step 35430, loss = 21.51, batch loss = 21.42 (8.2 examples/sec; 0.976 sec/batch; 80h:30m:00s remains)
INFO - root - 2017-12-07 17:34:34.876858: step 35440, loss = 21.78, batch loss = 21.70 (7.9 examples/sec; 1.017 sec/batch; 83h:55m:24s remains)
INFO - root - 2017-12-07 17:34:44.382372: step 35450, loss = 21.37, batch loss = 21.29 (8.3 examples/sec; 0.963 sec/batch; 79h:29m:11s remains)
INFO - root - 2017-12-07 17:34:53.880010: step 35460, loss = 21.45, batch loss = 21.37 (9.2 examples/sec; 0.871 sec/batch; 71h:54m:14s remains)
INFO - root - 2017-12-07 17:35:03.244021: step 35470, loss = 21.50, batch loss = 21.42 (9.2 examples/sec; 0.865 sec/batch; 71h:23m:40s remains)
INFO - root - 2017-12-07 17:35:12.693501: step 35480, loss = 21.76, batch loss = 21.68 (8.2 examples/sec; 0.970 sec/batch; 80h:03m:01s remains)
INFO - root - 2017-12-07 17:35:22.086686: step 35490, loss = 21.52, batch loss = 21.44 (8.7 examples/sec; 0.918 sec/batch; 75h:42m:10s remains)
INFO - root - 2017-12-07 17:35:31.484019: step 35500, loss = 21.51, batch loss = 21.43 (8.8 examples/sec; 0.910 sec/batch; 75h:06m:32s remains)
2017-12-07 17:35:32.462310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5522714 -4.6817069 -4.7626505 -4.7798648 -4.7298908 -4.6382623 -4.5707498 -4.5972328 -4.6587934 -4.691011 -4.6994543 -4.6803551 -4.6608009 -4.6813607 -4.7228146][-4.5632582 -4.718051 -4.8146529 -4.826261 -4.7482772 -4.6140652 -4.5126829 -4.5491767 -4.6391344 -4.6773839 -4.679481 -4.6449037 -4.6271267 -4.6946192 -4.7922611][-4.5508561 -4.7002935 -4.7859864 -4.781126 -4.6856103 -4.5302353 -4.4114871 -4.4588385 -4.5675507 -4.5988393 -4.589673 -4.5452194 -4.5361748 -4.6537118 -4.80467][-4.532341 -4.6488333 -4.6941261 -4.6565261 -4.5504532 -4.3939328 -4.2733727 -4.3276973 -4.4378786 -4.4456468 -4.4153218 -4.3685069 -4.375761 -4.5345626 -4.7242403][-4.5312257 -4.6145544 -4.6219854 -4.5551648 -4.4384632 -4.2747207 -4.1336236 -4.1682591 -4.2545028 -4.2258964 -4.1683531 -4.1283236 -4.1686273 -4.3654056 -4.5855][-4.5407977 -4.6035485 -4.5900412 -4.5059729 -4.3720312 -4.1816363 -3.9927666 -3.9805455 -4.03512 -3.9868708 -3.9209373 -3.9018812 -3.978157 -4.1955609 -4.4272733][-4.5514827 -4.6020513 -4.5777073 -4.4812822 -4.3197021 -4.094985 -3.868504 -3.8135605 -3.8432682 -3.8012996 -3.7502155 -3.7610936 -3.8579855 -4.0622139 -4.2765307][-4.5581141 -4.5931649 -4.5575047 -4.4506826 -4.2713394 -4.0432239 -3.8302104 -3.767698 -3.7817256 -3.7487605 -3.713001 -3.742774 -3.8317375 -3.9908977 -4.1760068][-4.5466104 -4.5645375 -4.5194383 -4.4162612 -4.2543306 -4.0734739 -3.9225686 -3.8834715 -3.8901269 -3.8668547 -3.8508346 -3.8951836 -3.9568386 -4.0512686 -4.1959634][-4.5237284 -4.5355258 -4.5022464 -4.4330263 -4.3256173 -4.2143421 -4.12206 -4.0921421 -4.0802889 -4.0649109 -4.0802579 -4.1503711 -4.19229 -4.2241735 -4.3113608][-4.4921055 -4.5098915 -4.503365 -4.4795384 -4.4322786 -4.3805227 -4.3266563 -4.2957687 -4.2703443 -4.2660627 -4.3059335 -4.3868923 -4.4091611 -4.3957644 -4.42661][-4.444437 -4.46833 -4.4873657 -4.5015178 -4.5013871 -4.4922018 -4.4683967 -4.44434 -4.4223638 -4.4324608 -4.4835839 -4.5529304 -4.5577178 -4.5265937 -4.522047][-4.39191 -4.4131227 -4.4433427 -4.4767041 -4.501153 -4.5153179 -4.51445 -4.5052252 -4.4963484 -4.5157495 -4.5644097 -4.61559 -4.6179476 -4.5981069 -4.5848484][-4.3582225 -4.3692913 -4.3956528 -4.4302526 -4.460362 -4.4820776 -4.4929333 -4.4979973 -4.5064 -4.5346947 -4.5781322 -4.61301 -4.6152868 -4.6063709 -4.5932837][-4.3487182 -4.3479633 -4.3617735 -4.386631 -4.4128051 -4.4342766 -4.449573 -4.4618545 -4.4780574 -4.5050249 -4.5358338 -4.5544939 -4.552268 -4.543107 -4.5301242]]...]
INFO - root - 2017-12-07 17:35:41.935835: step 35510, loss = 21.37, batch loss = 21.28 (8.7 examples/sec; 0.920 sec/batch; 75h:51m:54s remains)
INFO - root - 2017-12-07 17:35:51.338750: step 35520, loss = 21.43, batch loss = 21.34 (8.6 examples/sec; 0.930 sec/batch; 76h:42m:17s remains)
INFO - root - 2017-12-07 17:36:00.610060: step 35530, loss = 21.62, batch loss = 21.54 (8.9 examples/sec; 0.903 sec/batch; 74h:27m:26s remains)
INFO - root - 2017-12-07 17:36:10.059535: step 35540, loss = 21.77, batch loss = 21.69 (8.3 examples/sec; 0.965 sec/batch; 79h:36m:37s remains)
INFO - root - 2017-12-07 17:36:19.234340: step 35550, loss = 21.30, batch loss = 21.21 (8.4 examples/sec; 0.954 sec/batch; 78h:43m:47s remains)
INFO - root - 2017-12-07 17:36:28.614855: step 35560, loss = 21.01, batch loss = 20.93 (8.0 examples/sec; 1.003 sec/batch; 82h:42m:45s remains)
INFO - root - 2017-12-07 17:36:38.010746: step 35570, loss = 21.58, batch loss = 21.49 (7.9 examples/sec; 1.018 sec/batch; 83h:56m:07s remains)
INFO - root - 2017-12-07 17:36:47.494628: step 35580, loss = 21.22, batch loss = 21.13 (8.2 examples/sec; 0.979 sec/batch; 80h:46m:10s remains)
INFO - root - 2017-12-07 17:36:56.921611: step 35590, loss = 21.55, batch loss = 21.47 (9.0 examples/sec; 0.893 sec/batch; 73h:41m:26s remains)
INFO - root - 2017-12-07 17:37:06.193574: step 35600, loss = 21.30, batch loss = 21.22 (8.5 examples/sec; 0.940 sec/batch; 77h:31m:36s remains)
2017-12-07 17:37:07.093268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5220718 -4.6558847 -4.7211347 -4.6884069 -4.6159391 -4.5456414 -4.5388064 -4.6114111 -4.6971545 -4.7480435 -4.7445326 -4.6680908 -4.5566726 -4.44574 -4.3555775][-4.5304084 -4.6630507 -4.71014 -4.6501989 -4.5588455 -4.4850097 -4.4807534 -4.56244 -4.6604414 -4.7244968 -4.73691 -4.6668139 -4.5479274 -4.4346075 -4.3499641][-4.5317931 -4.6574411 -4.6817207 -4.5940208 -4.4812288 -4.3939867 -4.3698397 -4.4319425 -4.5277309 -4.6105652 -4.6611986 -4.6289287 -4.5315208 -4.4302926 -4.350348][-4.5349383 -4.6540837 -4.6622348 -4.5510254 -4.4115219 -4.3027821 -4.256557 -4.2977014 -4.3856459 -4.4818673 -4.5634384 -4.5723023 -4.5086565 -4.42885 -4.355844][-4.53489 -4.6520791 -4.6554565 -4.5270538 -4.3544807 -4.2148428 -4.1503377 -4.1823993 -4.2716923 -4.3758063 -4.4726019 -4.5140405 -4.4851108 -4.4273586 -4.3630805][-4.5224729 -4.6335616 -4.6314807 -4.4820805 -4.266746 -4.0878315 -4.0088143 -4.04709 -4.1575861 -4.281549 -4.3932233 -4.4631829 -4.4641352 -4.4248872 -4.3703227][-4.4940848 -4.5894485 -4.5715742 -4.3913841 -4.1238837 -3.8954117 -3.7906098 -3.8308871 -3.9749417 -4.143322 -4.2926812 -4.3989978 -4.4312477 -4.4104414 -4.3680758][-4.4574995 -4.5419812 -4.5145531 -4.3137655 -4.0052242 -3.7319763 -3.5927629 -3.6187968 -3.7855818 -3.9998853 -4.19742 -4.3395057 -4.3952785 -4.3856611 -4.3542666][-4.4315772 -4.52021 -4.5111179 -4.3313389 -4.0327291 -3.7586102 -3.6099548 -3.6157115 -3.769469 -3.9847906 -4.194438 -4.3437715 -4.3976264 -4.3806276 -4.3514547][-4.4217281 -4.5297236 -4.5669274 -4.4568324 -4.2276254 -4.000845 -3.8667831 -3.8477292 -3.9455819 -4.1048894 -4.2798142 -4.4103012 -4.4487071 -4.414681 -4.3746133][-4.4290686 -4.5512929 -4.6344986 -4.6047263 -4.4645596 -4.2963829 -4.1728687 -4.1222186 -4.1545067 -4.2501621 -4.3820057 -4.4912181 -4.5204234 -4.4777179 -4.4244642][-4.4348278 -4.5530033 -4.6550088 -4.6791229 -4.610219 -4.4936719 -4.3821406 -4.3121819 -4.3069658 -4.3661895 -4.4693546 -4.5584936 -4.5785565 -4.5337358 -4.471612][-4.4205303 -4.5234737 -4.6199036 -4.6653118 -4.6394897 -4.5671387 -4.4831777 -4.4221148 -4.4120989 -4.4574275 -4.5373559 -4.5973058 -4.5972857 -4.5473528 -4.4813619][-4.3986039 -4.4835539 -4.5650373 -4.6109362 -4.6045852 -4.5635796 -4.5135231 -4.4784822 -4.4817286 -4.5227671 -4.5775361 -4.6025982 -4.5776477 -4.5216622 -4.4592676][-4.3784127 -4.4498787 -4.5221648 -4.5690126 -4.5743332 -4.55307 -4.5283394 -4.5176511 -4.530858 -4.5599661 -4.5836592 -4.5757318 -4.5332503 -4.4772305 -4.4267344]]...]
INFO - root - 2017-12-07 17:37:16.541942: step 35610, loss = 21.24, batch loss = 21.15 (8.8 examples/sec; 0.905 sec/batch; 74h:37m:56s remains)
INFO - root - 2017-12-07 17:37:25.874507: step 35620, loss = 21.66, batch loss = 21.58 (8.5 examples/sec; 0.938 sec/batch; 77h:23m:16s remains)
INFO - root - 2017-12-07 17:37:35.222065: step 35630, loss = 21.43, batch loss = 21.35 (8.2 examples/sec; 0.972 sec/batch; 80h:09m:22s remains)
INFO - root - 2017-12-07 17:37:44.418887: step 35640, loss = 21.25, batch loss = 21.17 (8.4 examples/sec; 0.949 sec/batch; 78h:17m:16s remains)
INFO - root - 2017-12-07 17:37:53.827323: step 35650, loss = 21.24, batch loss = 21.16 (8.3 examples/sec; 0.964 sec/batch; 79h:27m:18s remains)
INFO - root - 2017-12-07 17:38:03.290625: step 35660, loss = 21.43, batch loss = 21.35 (8.3 examples/sec; 0.964 sec/batch; 79h:28m:35s remains)
INFO - root - 2017-12-07 17:38:12.772254: step 35670, loss = 21.51, batch loss = 21.43 (8.6 examples/sec; 0.928 sec/batch; 76h:31m:44s remains)
INFO - root - 2017-12-07 17:38:22.240379: step 35680, loss = 21.47, batch loss = 21.39 (8.4 examples/sec; 0.953 sec/batch; 78h:35m:31s remains)
INFO - root - 2017-12-07 17:38:31.732222: step 35690, loss = 21.74, batch loss = 21.66 (8.1 examples/sec; 0.984 sec/batch; 81h:05m:17s remains)
INFO - root - 2017-12-07 17:38:41.281064: step 35700, loss = 21.62, batch loss = 21.54 (8.0 examples/sec; 0.997 sec/batch; 82h:09m:44s remains)
2017-12-07 17:38:42.231236: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.228055 -4.1413288 -4.1104617 -4.163959 -4.2509918 -4.2978783 -4.2824793 -4.2480545 -4.2585135 -4.30128 -4.3600373 -4.4215307 -4.4251862 -4.3842893 -4.3603458][-4.2716479 -4.1886225 -4.1518049 -4.1787434 -4.2216744 -4.2269144 -4.192657 -4.1653519 -4.2051086 -4.2846589 -4.3701882 -4.4434333 -4.4530129 -4.4052429 -4.3666039][-4.3575177 -4.3165693 -4.2937484 -4.3012176 -4.2915096 -4.232564 -4.1504688 -4.0988708 -4.1341448 -4.2251039 -4.3341408 -4.4376841 -4.4816837 -4.4552021 -4.4094343][-4.40235 -4.4250908 -4.4414678 -4.4530015 -4.4036617 -4.2736359 -4.1183715 -4.0128508 -4.013412 -4.0973654 -4.2282758 -4.3727 -4.4683504 -4.4817929 -4.4444656][-4.3897557 -4.4697924 -4.5360837 -4.5711594 -4.503633 -4.315238 -4.0812249 -3.9042282 -3.8546798 -3.9254012 -4.0799489 -4.2669144 -4.4126439 -4.4681129 -4.45073][-4.3230119 -4.4397349 -4.5509744 -4.6152849 -4.5500956 -4.3313618 -4.0435238 -3.8081918 -3.7171857 -3.7786543 -3.9555414 -4.1802182 -4.3628082 -4.4495196 -4.452157][-4.22366 -4.3564196 -4.49289 -4.5697088 -4.5050821 -4.2804437 -3.9787595 -3.7275131 -3.6328273 -3.7117491 -3.9193211 -4.172914 -4.3746338 -4.4688191 -4.4761739][-4.1517982 -4.2919726 -4.4299026 -4.4905372 -4.4102263 -4.1890497 -3.9079518 -3.686337 -3.6319861 -3.7544358 -3.9934351 -4.2585196 -4.4568009 -4.5373874 -4.5357127][-4.1567245 -4.303257 -4.4269495 -4.4567423 -4.3582368 -4.15053 -3.9066157 -3.731544 -3.7219677 -3.8774195 -4.12522 -4.3751636 -4.5476871 -4.6069069 -4.6024261][-4.2084007 -4.3628721 -4.4654894 -4.4642711 -4.3589473 -4.1810331 -3.9834971 -3.8471508 -3.8543987 -4.0040874 -4.2248888 -4.43491 -4.5685639 -4.6102805 -4.6224012][-4.2660351 -4.4177523 -4.5019126 -4.4868927 -4.3943882 -4.2644343 -4.12031 -4.0122952 -4.0068145 -4.1121621 -4.2732015 -4.4243922 -4.5153513 -4.5512705 -4.5933661][-4.3201795 -4.4551449 -4.524591 -4.50608 -4.4308858 -4.3441181 -4.2494168 -4.1707973 -4.1555395 -4.2141218 -4.311748 -4.4014878 -4.4525762 -4.4840879 -4.5438561][-4.389605 -4.49719 -4.5395179 -4.5001516 -4.4167857 -4.3407435 -4.2798619 -4.2443218 -4.2539005 -4.3093491 -4.3802476 -4.4318619 -4.4524946 -4.4676137 -4.5124125][-4.4682207 -4.5331345 -4.5283651 -4.4519553 -4.3442016 -4.2607722 -4.225606 -4.2404757 -4.30068 -4.3905549 -4.4730296 -4.5177646 -4.5210223 -4.5111151 -4.5173764][-4.5024943 -4.5097017 -4.4515896 -4.3462067 -4.2283964 -4.1443849 -4.1304688 -4.1874843 -4.29446 -4.42069 -4.5249128 -4.5807137 -4.58111 -4.5491548 -4.5151038]]...]
INFO - root - 2017-12-07 17:38:51.407872: step 35710, loss = 21.25, batch loss = 21.17 (8.6 examples/sec; 0.935 sec/batch; 77h:05m:36s remains)
INFO - root - 2017-12-07 17:39:00.683296: step 35720, loss = 21.58, batch loss = 21.50 (8.7 examples/sec; 0.920 sec/batch; 75h:50m:35s remains)
INFO - root - 2017-12-07 17:39:10.021090: step 35730, loss = 21.06, batch loss = 20.97 (9.2 examples/sec; 0.865 sec/batch; 71h:19m:28s remains)
INFO - root - 2017-12-07 17:39:19.192239: step 35740, loss = 21.44, batch loss = 21.36 (9.1 examples/sec; 0.879 sec/batch; 72h:28m:15s remains)
INFO - root - 2017-12-07 17:39:28.546016: step 35750, loss = 21.43, batch loss = 21.35 (8.8 examples/sec; 0.914 sec/batch; 75h:21m:43s remains)
INFO - root - 2017-12-07 17:39:37.833091: step 35760, loss = 20.99, batch loss = 20.91 (8.6 examples/sec; 0.927 sec/batch; 76h:22m:35s remains)
INFO - root - 2017-12-07 17:39:47.127840: step 35770, loss = 21.52, batch loss = 21.44 (8.3 examples/sec; 0.964 sec/batch; 79h:27m:22s remains)
INFO - root - 2017-12-07 17:39:56.473699: step 35780, loss = 22.07, batch loss = 21.98 (8.1 examples/sec; 0.983 sec/batch; 81h:01m:15s remains)
INFO - root - 2017-12-07 17:40:05.894162: step 35790, loss = 21.40, batch loss = 21.31 (7.9 examples/sec; 1.008 sec/batch; 83h:04m:46s remains)
INFO - root - 2017-12-07 17:40:15.237837: step 35800, loss = 21.46, batch loss = 21.38 (8.5 examples/sec; 0.938 sec/batch; 77h:19m:16s remains)
2017-12-07 17:40:16.197742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4787688 -4.5690365 -4.657969 -4.721549 -4.7644939 -4.7516112 -4.6410413 -4.4613743 -4.3023143 -4.2541828 -4.3229465 -4.4621444 -4.6046996 -4.7075233 -4.7059011][-4.6314855 -4.7289834 -4.816999 -4.8603611 -4.8591866 -4.812068 -4.7191105 -4.6125264 -4.5340676 -4.5206556 -4.5743451 -4.6589189 -4.7351308 -4.7880521 -4.7661371][-4.7355556 -4.8426585 -4.9428368 -4.9783669 -4.9349165 -4.8389091 -4.729692 -4.646215 -4.6100225 -4.62492 -4.6917033 -4.7674913 -4.8094459 -4.8231239 -4.7762818][-4.8076582 -4.9161634 -5.01265 -5.0194812 -4.9185596 -4.7609463 -4.6101851 -4.5166783 -4.5094681 -4.5693231 -4.6843276 -4.79367 -4.8336744 -4.8219566 -4.7498832][-4.7769542 -4.87191 -4.940845 -4.8873487 -4.7024369 -4.4681034 -4.2697797 -4.1660161 -4.20787 -4.34757 -4.5426364 -4.7091284 -4.770936 -4.7660904 -4.7002983][-4.7102084 -4.7765036 -4.7929554 -4.6495175 -4.3525486 -4.0176129 -3.7601981 -3.665297 -3.8044016 -4.0643382 -4.3455782 -4.5556788 -4.6358004 -4.6517606 -4.6232481][-4.6740785 -4.7230291 -4.7081485 -4.5131311 -4.1441336 -3.7217517 -3.3925936 -3.2937589 -3.5166864 -3.8730843 -4.2058997 -4.4305463 -4.5135803 -4.533494 -4.5295744][-4.6367645 -4.6959767 -4.6942182 -4.5231576 -4.17159 -3.7394905 -3.3944623 -3.3081658 -3.555191 -3.9232481 -4.2305207 -4.4166026 -4.4797354 -4.4781532 -4.4702764][-4.5974016 -4.6817369 -4.7128267 -4.603209 -4.3251648 -3.9478655 -3.639286 -3.5793452 -3.8133578 -4.142416 -4.3965464 -4.5267572 -4.5534477 -4.5124674 -4.4742756][-4.5758305 -4.6835151 -4.750936 -4.7124438 -4.5326447 -4.2457194 -3.9960074 -3.9401159 -4.1015224 -4.3397551 -4.5289316 -4.6177897 -4.6216726 -4.5558228 -4.4908061][-4.5462031 -4.664711 -4.7534122 -4.7661352 -4.6783423 -4.5041418 -4.34243 -4.2953267 -4.3626447 -4.478313 -4.5817957 -4.6277847 -4.6212187 -4.5567875 -4.4914865][-4.5081754 -4.6313505 -4.7209625 -4.7477651 -4.7058797 -4.6088629 -4.5091038 -4.4577584 -4.4530563 -4.4783826 -4.5156889 -4.5287886 -4.5218244 -4.4838085 -4.4524837][-4.4782286 -4.6093922 -4.7039108 -4.7379742 -4.7168975 -4.6524205 -4.559566 -4.468214 -4.3912463 -4.350462 -4.3466506 -4.3446574 -4.3502359 -4.3505759 -4.3698239][-4.4288874 -4.54248 -4.6310997 -4.6758976 -4.6823378 -4.649313 -4.5626421 -4.442708 -4.3166227 -4.2257328 -4.1922226 -4.1883216 -4.2129822 -4.2499986 -4.309608][-4.3656206 -4.4300709 -4.4781585 -4.5089626 -4.5310888 -4.5317974 -4.4771333 -4.3718333 -4.2388434 -4.1218033 -4.0616546 -4.0529552 -4.0990019 -4.1735406 -4.271699]]...]
INFO - root - 2017-12-07 17:40:25.527384: step 35810, loss = 21.22, batch loss = 21.14 (8.6 examples/sec; 0.932 sec/batch; 76h:49m:35s remains)
INFO - root - 2017-12-07 17:40:35.013617: step 35820, loss = 21.66, batch loss = 21.58 (8.3 examples/sec; 0.965 sec/batch; 79h:34m:00s remains)
INFO - root - 2017-12-07 17:40:44.412076: step 35830, loss = 21.30, batch loss = 21.21 (8.4 examples/sec; 0.954 sec/batch; 78h:37m:10s remains)
INFO - root - 2017-12-07 17:40:53.577369: step 35840, loss = 21.16, batch loss = 21.08 (9.1 examples/sec; 0.880 sec/batch; 72h:31m:04s remains)
INFO - root - 2017-12-07 17:41:03.047456: step 35850, loss = 21.17, batch loss = 21.09 (8.6 examples/sec; 0.934 sec/batch; 76h:58m:40s remains)
INFO - root - 2017-12-07 17:41:12.444497: step 35860, loss = 21.60, batch loss = 21.51 (8.5 examples/sec; 0.946 sec/batch; 77h:55m:59s remains)
INFO - root - 2017-12-07 17:41:21.837385: step 35870, loss = 21.37, batch loss = 21.28 (8.5 examples/sec; 0.936 sec/batch; 77h:07m:46s remains)
INFO - root - 2017-12-07 17:41:31.272829: step 35880, loss = 21.43, batch loss = 21.34 (8.7 examples/sec; 0.918 sec/batch; 75h:37m:24s remains)
INFO - root - 2017-12-07 17:41:40.705180: step 35890, loss = 21.56, batch loss = 21.48 (8.2 examples/sec; 0.977 sec/batch; 80h:30m:20s remains)
INFO - root - 2017-12-07 17:41:50.111838: step 35900, loss = 21.72, batch loss = 21.64 (8.6 examples/sec; 0.925 sec/batch; 76h:11m:57s remains)
2017-12-07 17:41:51.017233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2759457 -4.3237815 -4.3636179 -4.3930941 -4.408648 -4.3995938 -4.3616705 -4.3173137 -4.2961841 -4.3369861 -4.4023538 -4.461071 -4.5295415 -4.5670972 -4.5536242][-4.3676171 -4.3994455 -4.4109664 -4.4006615 -4.3662395 -4.3238182 -4.2839236 -4.2608705 -4.2674732 -4.3303676 -4.4178967 -4.4978719 -4.571558 -4.6050544 -4.5907731][-4.4463425 -4.4492879 -4.4279346 -4.3910937 -4.3229513 -4.2508764 -4.2039218 -4.1919932 -4.21748 -4.2921267 -4.3936443 -4.4961462 -4.5888777 -4.6418319 -4.647254][-4.5052495 -4.479979 -4.4354992 -4.3858571 -4.30294 -4.2041225 -4.12875 -4.1037521 -4.1327362 -4.2148767 -4.3287282 -4.4467092 -4.5567303 -4.638556 -4.6747313][-4.5440607 -4.5109453 -4.4608655 -4.4018865 -4.3051167 -4.1701555 -4.0422144 -3.9896972 -4.027276 -4.1402984 -4.2816529 -4.4093723 -4.5222397 -4.6215529 -4.6883688][-4.5509262 -4.5361176 -4.4918942 -4.4156175 -4.2921572 -4.1144481 -3.93439 -3.8605082 -3.9231591 -4.0886049 -4.2688475 -4.4035888 -4.5027146 -4.6008329 -4.6896582][-4.5122428 -4.5354295 -4.5038586 -4.4030366 -4.2474828 -4.040648 -3.8425403 -3.7695017 -3.8582084 -4.0644155 -4.2711506 -4.4083138 -4.4816327 -4.5557671 -4.6510434][-4.4160891 -4.48744 -4.4872384 -4.3837361 -4.2213693 -4.0225968 -3.8489347 -3.7954803 -3.8867807 -4.0887451 -4.2910066 -4.4159083 -4.455091 -4.4937778 -4.5763636][-4.3032064 -4.4098687 -4.4488997 -4.3757954 -4.2450118 -4.089159 -3.9537921 -3.9058218 -3.9697816 -4.1365829 -4.312984 -4.4090028 -4.4115415 -4.4210415 -4.4868875][-4.2461576 -4.3595939 -4.4190769 -4.3820429 -4.3006754 -4.2008915 -4.1043692 -4.0527496 -4.0758262 -4.1951532 -4.3350897 -4.3982277 -4.3726792 -4.361002 -4.4087839][-4.2604861 -4.3573365 -4.4100957 -4.3909087 -4.3503408 -4.3034043 -4.2489738 -4.2046566 -4.1986742 -4.2643318 -4.3488145 -4.3689413 -4.3205581 -4.297986 -4.3372993][-4.2943139 -4.3730955 -4.4119782 -4.3981867 -4.3825574 -4.3724251 -4.3557386 -4.3304667 -4.3115544 -4.3228416 -4.3376889 -4.3059368 -4.2399797 -4.2205367 -4.2712283][-4.3193092 -4.3910685 -4.4243269 -4.4165454 -4.4144273 -4.4204249 -4.4226241 -4.4115982 -4.389142 -4.362536 -4.3210015 -4.2535477 -4.1890388 -4.1912775 -4.2645688][-4.3418336 -4.4001236 -4.4235306 -4.4155021 -4.4122663 -4.4081626 -4.4019432 -4.394475 -4.3865166 -4.36801 -4.3260345 -4.2622547 -4.2146211 -4.237299 -4.3207126][-4.3531523 -4.3924747 -4.4062428 -4.3913617 -4.3704576 -4.3355484 -4.3019724 -4.2921724 -4.3163438 -4.3435292 -4.3429451 -4.3152547 -4.2953734 -4.3297262 -4.4030423]]...]
INFO - root - 2017-12-07 17:42:00.337462: step 35910, loss = 21.72, batch loss = 21.64 (8.5 examples/sec; 0.945 sec/batch; 77h:50m:16s remains)
INFO - root - 2017-12-07 17:42:09.615520: step 35920, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.931 sec/batch; 76h:43m:04s remains)
INFO - root - 2017-12-07 17:42:18.913456: step 35930, loss = 21.39, batch loss = 21.31 (8.7 examples/sec; 0.914 sec/batch; 75h:20m:09s remains)
INFO - root - 2017-12-07 17:42:28.316157: step 35940, loss = 21.82, batch loss = 21.74 (8.7 examples/sec; 0.919 sec/batch; 75h:43m:30s remains)
INFO - root - 2017-12-07 17:42:37.542460: step 35950, loss = 21.21, batch loss = 21.13 (8.9 examples/sec; 0.901 sec/batch; 74h:13m:34s remains)
INFO - root - 2017-12-07 17:42:47.023394: step 35960, loss = 21.72, batch loss = 21.64 (8.6 examples/sec; 0.934 sec/batch; 76h:58m:19s remains)
INFO - root - 2017-12-07 17:42:56.412197: step 35970, loss = 21.12, batch loss = 21.04 (8.6 examples/sec; 0.931 sec/batch; 76h:39m:21s remains)
INFO - root - 2017-12-07 17:43:05.785183: step 35980, loss = 21.28, batch loss = 21.19 (8.1 examples/sec; 0.982 sec/batch; 80h:51m:40s remains)
INFO - root - 2017-12-07 17:43:15.125447: step 35990, loss = 21.49, batch loss = 21.40 (8.4 examples/sec; 0.950 sec/batch; 78h:13m:26s remains)
INFO - root - 2017-12-07 17:43:24.484829: step 36000, loss = 21.74, batch loss = 21.66 (9.2 examples/sec; 0.873 sec/batch; 71h:52m:30s remains)
2017-12-07 17:43:25.437276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.38737 -4.4181471 -4.4321508 -4.42101 -4.4203596 -4.4342027 -4.45324 -4.4860477 -4.4882851 -4.4433508 -4.403686 -4.384716 -4.3658581 -4.3472567 -4.3130507][-4.3876519 -4.4213457 -4.4248323 -4.3792977 -4.3354893 -4.313817 -4.3119917 -4.3591833 -4.4029365 -4.3936391 -4.3666835 -4.3370309 -4.3168211 -4.319313 -4.3046603][-4.3907423 -4.4281592 -4.4280133 -4.3564186 -4.2668791 -4.201057 -4.1755333 -4.2397785 -4.3288951 -4.3608031 -4.3506775 -4.3172016 -4.3102055 -4.348423 -4.3618865][-4.3941116 -4.4346485 -4.4376073 -4.3573895 -4.2392097 -4.1396203 -4.0969319 -4.1688828 -4.2796545 -4.32943 -4.32317 -4.2882018 -4.3010077 -4.37677 -4.4179974][-4.3960371 -4.4382162 -4.445941 -4.366787 -4.236412 -4.1187329 -4.0617385 -4.1246929 -4.2295289 -4.2780337 -4.2721009 -4.2459712 -4.281558 -4.38091 -4.4349594][-4.3967848 -4.4388051 -4.4495883 -4.3731847 -4.2416873 -4.1205711 -4.0520487 -4.0897975 -4.1621947 -4.1899047 -4.1801782 -4.1720495 -4.2328858 -4.3442659 -4.4065409][-4.3973694 -4.4400635 -4.4540753 -4.3827167 -4.25906 -4.1419611 -4.0605831 -4.0592971 -4.0885239 -4.0918665 -4.0824065 -4.09725 -4.1790147 -4.2939916 -4.3614464][-4.398592 -4.4452872 -4.4632435 -4.3995385 -4.2895241 -4.18018 -4.0902824 -4.056375 -4.0575595 -4.0566192 -4.0615783 -4.0986881 -4.1858721 -4.284832 -4.3501568][-4.4023242 -4.4528632 -4.473897 -4.418479 -4.3267417 -4.2295327 -4.1414204 -4.0931582 -4.0879011 -4.099299 -4.1225891 -4.1683159 -4.2395258 -4.3106909 -4.3663068][-4.4047947 -4.4547238 -4.4738469 -4.4221492 -4.3439031 -4.2537246 -4.1704907 -4.129457 -4.14055 -4.1775947 -4.2213564 -4.2679439 -4.3111129 -4.3494964 -4.3879824][-4.4015217 -4.4449916 -4.45776 -4.4069748 -4.3349009 -4.2452183 -4.1661963 -4.1426587 -4.177083 -4.2365656 -4.2995234 -4.3497272 -4.3712921 -4.380672 -4.3989835][-4.3930144 -4.42385 -4.4287534 -4.3840642 -4.3198757 -4.2303257 -4.1497588 -4.1320128 -4.1754613 -4.2404113 -4.3097358 -4.3636808 -4.3768535 -4.3677545 -4.3672523][-4.3796368 -4.3947973 -4.3916922 -4.3589439 -4.3103652 -4.22895 -4.1431823 -4.1126561 -4.1408548 -4.1886306 -4.2485952 -4.29957 -4.3137321 -4.301549 -4.2966619][-4.3643088 -4.3625441 -4.3499756 -4.326787 -4.297049 -4.2353377 -4.1532173 -4.1018767 -4.0916572 -4.1017847 -4.1383381 -4.180407 -4.1998367 -4.1998706 -4.208838][-4.3513045 -4.3338742 -4.3087826 -4.2858529 -4.2720356 -4.2407007 -4.18002 -4.1230621 -4.0805321 -4.0545607 -4.0652628 -4.0909438 -4.110343 -4.124763 -4.1528196]]...]
INFO - root - 2017-12-07 17:43:34.771544: step 36010, loss = 20.98, batch loss = 20.89 (7.9 examples/sec; 1.018 sec/batch; 83h:52m:46s remains)
INFO - root - 2017-12-07 17:43:44.059962: step 36020, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.949 sec/batch; 78h:09m:10s remains)
INFO - root - 2017-12-07 17:43:53.381452: step 36030, loss = 21.32, batch loss = 21.24 (9.1 examples/sec; 0.879 sec/batch; 72h:21m:46s remains)
INFO - root - 2017-12-07 17:44:02.761480: step 36040, loss = 21.48, batch loss = 21.40 (8.6 examples/sec; 0.927 sec/batch; 76h:21m:37s remains)
INFO - root - 2017-12-07 17:44:12.051764: step 36050, loss = 21.44, batch loss = 21.35 (8.7 examples/sec; 0.916 sec/batch; 75h:26m:14s remains)
INFO - root - 2017-12-07 17:44:21.371674: step 36060, loss = 21.45, batch loss = 21.37 (8.4 examples/sec; 0.957 sec/batch; 78h:48m:54s remains)
INFO - root - 2017-12-07 17:44:30.662648: step 36070, loss = 21.88, batch loss = 21.80 (8.4 examples/sec; 0.950 sec/batch; 78h:13m:16s remains)
INFO - root - 2017-12-07 17:44:40.052859: step 36080, loss = 21.55, batch loss = 21.47 (8.9 examples/sec; 0.900 sec/batch; 74h:06m:15s remains)
INFO - root - 2017-12-07 17:44:49.651357: step 36090, loss = 21.30, batch loss = 21.22 (8.3 examples/sec; 0.960 sec/batch; 79h:04m:02s remains)
INFO - root - 2017-12-07 17:44:59.086391: step 36100, loss = 21.22, batch loss = 21.14 (8.4 examples/sec; 0.951 sec/batch; 78h:16m:59s remains)
2017-12-07 17:45:00.059891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.467412 -4.5081711 -4.5365086 -4.5272708 -4.50021 -4.4957747 -4.5142536 -4.5188632 -4.5105023 -4.5171309 -4.5315905 -4.5314922 -4.5086961 -4.4835606 -4.4754977][-4.3663707 -4.426455 -4.4738588 -4.466043 -4.4359922 -4.4425097 -4.4830213 -4.5023012 -4.5032096 -4.5196204 -4.5423064 -4.5439992 -4.5154781 -4.4844637 -4.4780035][-4.2670889 -4.3313832 -4.3856897 -4.3748608 -4.3388057 -4.3526998 -4.4123058 -4.4520583 -4.4751816 -4.5117612 -4.5427136 -4.5437794 -4.51169 -4.4778767 -4.4730482][-4.1759419 -4.2221212 -4.2670155 -4.2487168 -4.2066112 -4.2241669 -4.2976747 -4.3622589 -4.4226975 -4.4949 -4.542233 -4.5427504 -4.507957 -4.4691653 -4.4565773][-4.1264758 -4.1382923 -4.16195 -4.1368961 -4.0933166 -4.1080875 -4.1817 -4.2604442 -4.352077 -4.459363 -4.5288367 -4.5369806 -4.50881 -4.4675689 -4.4390359][-4.1804843 -4.1551695 -4.1496797 -4.1153846 -4.0685668 -4.0656743 -4.1123543 -4.1809916 -4.2835474 -4.4137292 -4.5084958 -4.5352945 -4.5225596 -4.4833355 -4.4375587][-4.280756 -4.2232232 -4.1812968 -4.1366482 -4.0884304 -4.0573945 -4.05564 -4.0939283 -4.1929207 -4.3398046 -4.4662247 -4.5225925 -4.5292563 -4.4941721 -4.4334736][-4.3797417 -4.3045015 -4.2342281 -4.1860285 -4.1393819 -4.0768881 -4.0170188 -4.012125 -4.0934062 -4.2447281 -4.3983412 -4.4888091 -4.519032 -4.4940662 -4.4309521][-4.4586368 -4.3844428 -4.3075013 -4.2601337 -4.2042022 -4.1073475 -3.9989636 -3.9580581 -4.0179243 -4.1583219 -4.3197374 -4.4343967 -4.4872746 -4.478385 -4.4264054][-4.465147 -4.4068627 -4.3479166 -4.3141427 -4.2552752 -4.1403747 -4.0119009 -3.9530573 -3.9904673 -4.0994644 -4.2363911 -4.3478422 -4.412096 -4.4191408 -4.3837471][-4.4187632 -4.3810372 -4.3539634 -4.3444748 -4.29792 -4.1956539 -4.0816989 -4.0200186 -4.02844 -4.0851731 -4.1678448 -4.2478476 -4.3135347 -4.3398285 -4.3210692][-4.3519726 -4.3310833 -4.3335371 -4.3458309 -4.316505 -4.2409315 -4.1614418 -4.1078525 -4.0847411 -4.0871725 -4.1123638 -4.153625 -4.2188406 -4.2690368 -4.2701316][-4.2637739 -4.2544909 -4.2808132 -4.3140469 -4.3069816 -4.2660489 -4.227478 -4.1871958 -4.13867 -4.0961609 -4.0730371 -4.0823345 -4.1473155 -4.22105 -4.2424073][-4.191546 -4.1886859 -4.2373762 -4.2907329 -4.3004646 -4.2866535 -4.28279 -4.2586632 -4.1986232 -4.1294641 -4.07433 -4.0592666 -4.1213117 -4.2107878 -4.2491703][-4.166748 -4.1671638 -4.2288671 -4.2855654 -4.2882595 -4.2771063 -4.2900639 -4.2802691 -4.2306814 -4.1695876 -4.1137266 -4.0907478 -4.1475277 -4.2431831 -4.2921147]]...]
INFO - root - 2017-12-07 17:45:09.623327: step 36110, loss = 21.38, batch loss = 21.30 (8.8 examples/sec; 0.910 sec/batch; 74h:54m:00s remains)
INFO - root - 2017-12-07 17:45:19.170752: step 36120, loss = 21.32, batch loss = 21.23 (8.1 examples/sec; 0.988 sec/batch; 81h:20m:25s remains)
INFO - root - 2017-12-07 17:45:28.683135: step 36130, loss = 21.52, batch loss = 21.43 (8.3 examples/sec; 0.962 sec/batch; 79h:09m:33s remains)
INFO - root - 2017-12-07 17:45:37.999263: step 36140, loss = 21.61, batch loss = 21.53 (8.0 examples/sec; 0.995 sec/batch; 81h:55m:20s remains)
INFO - root - 2017-12-07 17:45:47.307759: step 36150, loss = 21.14, batch loss = 21.06 (8.4 examples/sec; 0.957 sec/batch; 78h:44m:20s remains)
INFO - root - 2017-12-07 17:45:56.774556: step 36160, loss = 21.59, batch loss = 21.51 (8.4 examples/sec; 0.949 sec/batch; 78h:07m:51s remains)
INFO - root - 2017-12-07 17:46:06.119983: step 36170, loss = 21.38, batch loss = 21.29 (8.6 examples/sec; 0.931 sec/batch; 76h:35m:57s remains)
INFO - root - 2017-12-07 17:46:15.612406: step 36180, loss = 21.37, batch loss = 21.29 (8.3 examples/sec; 0.963 sec/batch; 79h:17m:29s remains)
INFO - root - 2017-12-07 17:46:25.137082: step 36190, loss = 21.32, batch loss = 21.23 (9.6 examples/sec; 0.836 sec/batch; 68h:47m:05s remains)
INFO - root - 2017-12-07 17:46:34.522537: step 36200, loss = 21.38, batch loss = 21.30 (8.9 examples/sec; 0.899 sec/batch; 74h:00m:41s remains)
2017-12-07 17:46:35.503480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.387 -4.44816 -4.517674 -4.5639739 -4.5907979 -4.59975 -4.5678577 -4.4505949 -4.2665892 -4.0767503 -3.9540889 -3.9666257 -4.1031318 -4.2604895 -4.3887095][-4.4444923 -4.5047326 -4.5513687 -4.5685468 -4.562613 -4.5434923 -4.5037823 -4.3952751 -4.2168689 -4.0250974 -3.907568 -3.9431703 -4.1023273 -4.2664351 -4.3877082][-4.494401 -4.5498738 -4.5767584 -4.5653329 -4.5231566 -4.4727507 -4.4336944 -4.3553557 -4.2088437 -4.038928 -3.9420447 -3.9947555 -4.1494107 -4.2894554 -4.3749495][-4.5114183 -4.5563421 -4.5609579 -4.5222774 -4.448329 -4.3705974 -4.3396368 -4.3129163 -4.2373791 -4.1388187 -4.0865116 -4.12769 -4.216814 -4.2736197 -4.2862477][-4.4866424 -4.515574 -4.5010805 -4.4425454 -4.3456464 -4.247407 -4.2259769 -4.2482533 -4.2487082 -4.2434959 -4.2476311 -4.2507629 -4.2269135 -4.1630759 -4.0940938][-4.4242449 -4.4461942 -4.4295158 -4.3651867 -4.2517529 -4.1346307 -4.1111631 -4.1583424 -4.2125392 -4.2792354 -4.3229356 -4.276547 -4.1448264 -3.9817762 -3.8650618][-4.3283429 -4.364841 -4.3794513 -4.3323011 -4.2116637 -4.0729036 -4.027565 -4.0680447 -4.1406107 -4.2412076 -4.303875 -4.2285557 -4.038609 -3.8274918 -3.7055137][-4.1924973 -4.2731252 -4.362299 -4.3667994 -4.2576566 -4.0970445 -4.0161982 -4.027606 -4.0862389 -4.1766291 -4.2329774 -4.1563859 -3.9696052 -3.7724643 -3.6920462][-4.03414 -4.164516 -4.3371878 -4.4179621 -4.3444333 -4.1676083 -4.0376759 -3.9975264 -4.02044 -4.0887594 -4.1505356 -4.1224179 -4.0088525 -3.8846717 -3.866796][-3.8913736 -4.047925 -4.2802234 -4.4444203 -4.4314933 -4.2577977 -4.0780282 -3.979548 -3.9647927 -4.0222545 -4.110076 -4.1628017 -4.1617541 -4.133914 -4.1651487][-3.8220165 -3.9584258 -4.2017059 -4.4229703 -4.4744167 -4.3240571 -4.1177945 -3.9837718 -3.9530501 -4.0202446 -4.1411643 -4.2649097 -4.3605528 -4.4092069 -4.4566793][-3.8830543 -3.9689145 -4.1738863 -4.3969846 -4.4778357 -4.3451076 -4.1283517 -3.9810717 -3.9592957 -4.0510769 -4.2015958 -4.3654141 -4.5103431 -4.592175 -4.6288643][-4.0708971 -4.1046071 -4.2472825 -4.4268255 -4.4948597 -4.3694663 -4.1617002 -4.0220709 -4.0206218 -4.1287651 -4.280045 -4.4377694 -4.5734143 -4.6441555 -4.6552463][-4.2994356 -4.2917862 -4.3721967 -4.4932737 -4.5410104 -4.4518185 -4.3044181 -4.20975 -4.2288361 -4.3203554 -4.4217558 -4.5187912 -4.5944304 -4.6221762 -4.6035638][-4.4791641 -4.4499884 -4.4812708 -4.5464869 -4.5737181 -4.526825 -4.455225 -4.4203472 -4.4552031 -4.5190482 -4.5616679 -4.5877876 -4.5962377 -4.5781713 -4.5376172]]...]
INFO - root - 2017-12-07 17:46:44.967254: step 36210, loss = 21.82, batch loss = 21.73 (8.5 examples/sec; 0.937 sec/batch; 77h:05m:34s remains)
INFO - root - 2017-12-07 17:46:54.533080: step 36220, loss = 21.73, batch loss = 21.65 (8.0 examples/sec; 0.997 sec/batch; 82h:04m:27s remains)
INFO - root - 2017-12-07 17:47:03.947630: step 36230, loss = 21.64, batch loss = 21.56 (8.2 examples/sec; 0.976 sec/batch; 80h:17m:00s remains)
INFO - root - 2017-12-07 17:47:13.406158: step 36240, loss = 22.25, batch loss = 22.17 (8.7 examples/sec; 0.915 sec/batch; 75h:19m:06s remains)
INFO - root - 2017-12-07 17:47:22.703424: step 36250, loss = 21.58, batch loss = 21.49 (8.8 examples/sec; 0.910 sec/batch; 74h:52m:52s remains)
INFO - root - 2017-12-07 17:47:32.378953: step 36260, loss = 21.49, batch loss = 21.41 (8.2 examples/sec; 0.979 sec/batch; 80h:34m:59s remains)
INFO - root - 2017-12-07 17:47:42.042014: step 36270, loss = 21.37, batch loss = 21.29 (8.0 examples/sec; 0.999 sec/batch; 82h:09m:52s remains)
INFO - root - 2017-12-07 17:47:51.488038: step 36280, loss = 21.06, batch loss = 20.97 (8.6 examples/sec; 0.932 sec/batch; 76h:40m:51s remains)
INFO - root - 2017-12-07 17:48:00.930694: step 36290, loss = 21.08, batch loss = 20.99 (8.6 examples/sec; 0.934 sec/batch; 76h:53m:02s remains)
INFO - root - 2017-12-07 17:48:10.238679: step 36300, loss = 21.14, batch loss = 21.05 (8.5 examples/sec; 0.947 sec/batch; 77h:52m:42s remains)
2017-12-07 17:48:11.170211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.464088 -4.4463077 -4.4392915 -4.4213324 -4.38096 -4.3415961 -4.2983665 -4.2389541 -4.2447805 -4.3484058 -4.4690766 -4.5529656 -4.5970292 -4.5847869 -4.5081735][-4.5863218 -4.57319 -4.5515566 -4.5215325 -4.4844718 -4.4641724 -4.438292 -4.3847337 -4.3846931 -4.4700685 -4.5680318 -4.6269045 -4.6472106 -4.620276 -4.5334868][-4.6649227 -4.6466565 -4.6014628 -4.5478616 -4.5044856 -4.4982882 -4.5009642 -4.4759793 -4.487361 -4.5629406 -4.6410456 -4.6768265 -4.6775432 -4.6422815 -4.5557666][-4.6634059 -4.6369491 -4.5650043 -4.4731917 -4.3979979 -4.3828139 -4.4089875 -4.4227815 -4.4583216 -4.5394797 -4.6156864 -4.6500607 -4.6547728 -4.6286297 -4.5534267][-4.5743151 -4.5418782 -4.4492049 -4.3203459 -4.2033358 -4.1596985 -4.18447 -4.2129083 -4.2602353 -4.3566847 -4.4643059 -4.5461345 -4.5989547 -4.605288 -4.5445042][-4.440002 -4.3935013 -4.2822208 -4.1268587 -3.9793477 -3.9087465 -3.9149706 -3.9289625 -3.9653749 -4.07653 -4.2408552 -4.4114165 -4.5493197 -4.6051521 -4.5539503][-4.3076563 -4.2429895 -4.1181388 -3.949091 -3.7858377 -3.6991334 -3.6861229 -3.6808991 -3.7027617 -3.8259072 -4.0432668 -4.2981977 -4.5192375 -4.627408 -4.5852251][-4.2266912 -4.1634984 -4.0506115 -3.8901634 -3.7237475 -3.6213286 -3.5826473 -3.5551457 -3.5660057 -3.6952028 -3.9386234 -4.2330112 -4.4980035 -4.6426678 -4.6149626][-4.2293477 -4.1927967 -4.117228 -3.9934192 -3.8531075 -3.7549062 -3.6956203 -3.6449795 -3.6410871 -3.7563522 -3.9829946 -4.2573624 -4.509789 -4.6544914 -4.634789][-4.3220963 -4.3208365 -4.2877684 -4.2122884 -4.1192856 -4.0474391 -3.9887331 -3.9287767 -3.91262 -3.9966104 -4.1702862 -4.380558 -4.574687 -4.6812034 -4.6517315][-4.4594526 -4.4853964 -4.4892211 -4.4602075 -4.4118257 -4.3670769 -4.3208227 -4.2683749 -4.2493782 -4.3006206 -4.4112296 -4.5431018 -4.6594553 -4.7081194 -4.657896][-4.5778241 -4.6121125 -4.6360092 -4.6377153 -4.6175008 -4.5886569 -4.5546603 -4.5192542 -4.5065169 -4.534183 -4.5916357 -4.6549706 -4.7015276 -4.6989441 -4.6336336][-4.6098518 -4.6343594 -4.65859 -4.6733618 -4.6710043 -4.6549697 -4.6309214 -4.6077414 -4.5985131 -4.6105208 -4.6344748 -4.6552558 -4.6589494 -4.6298594 -4.5673614][-4.5489173 -4.559525 -4.5736222 -4.5864048 -4.5911956 -4.5861287 -4.5744104 -4.5627913 -4.5574055 -4.5609884 -4.5679121 -4.5687981 -4.555933 -4.5246868 -4.4799495][-4.453238 -4.4526892 -4.4554095 -4.4602795 -4.46495 -4.4670553 -4.4659266 -4.4634905 -4.4615393 -4.4615545 -4.4619293 -4.458621 -4.4480047 -4.4293246 -4.4066052]]...]
INFO - root - 2017-12-07 17:48:20.525151: step 36310, loss = 21.50, batch loss = 21.41 (8.1 examples/sec; 0.983 sec/batch; 80h:50m:39s remains)
INFO - root - 2017-12-07 17:48:30.088672: step 36320, loss = 21.85, batch loss = 21.76 (7.9 examples/sec; 1.013 sec/batch; 83h:21m:15s remains)
INFO - root - 2017-12-07 17:48:39.426525: step 36330, loss = 21.60, batch loss = 21.51 (8.4 examples/sec; 0.957 sec/batch; 78h:43m:42s remains)
INFO - root - 2017-12-07 17:48:48.766876: step 36340, loss = 21.58, batch loss = 21.49 (8.0 examples/sec; 1.004 sec/batch; 82h:34m:45s remains)
INFO - root - 2017-12-07 17:48:57.932007: step 36350, loss = 21.81, batch loss = 21.73 (8.5 examples/sec; 0.938 sec/batch; 77h:09m:55s remains)
INFO - root - 2017-12-07 17:49:07.308048: step 36360, loss = 21.43, batch loss = 21.35 (8.7 examples/sec; 0.917 sec/batch; 75h:24m:04s remains)
INFO - root - 2017-12-07 17:49:16.673108: step 36370, loss = 21.68, batch loss = 21.60 (8.3 examples/sec; 0.969 sec/batch; 79h:41m:26s remains)
INFO - root - 2017-12-07 17:49:26.007060: step 36380, loss = 21.47, batch loss = 21.38 (8.5 examples/sec; 0.944 sec/batch; 77h:38m:51s remains)
INFO - root - 2017-12-07 17:49:35.394149: step 36390, loss = 21.41, batch loss = 21.33 (9.3 examples/sec; 0.864 sec/batch; 71h:02m:13s remains)
INFO - root - 2017-12-07 17:49:44.775264: step 36400, loss = 21.40, batch loss = 21.31 (9.4 examples/sec; 0.849 sec/batch; 69h:47m:29s remains)
2017-12-07 17:49:45.797616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2206149 -4.1452317 -4.0987358 -4.1214705 -4.201467 -4.2750278 -4.2893958 -4.2269678 -4.112452 -4.0067353 -3.983743 -4.05749 -4.1713152 -4.2539382 -4.2826605][-4.2279358 -4.1543837 -4.12414 -4.1665506 -4.2488847 -4.3015437 -4.2881422 -4.2163572 -4.1158981 -4.0280614 -4.0040712 -4.0543156 -4.1372738 -4.209147 -4.2535386][-4.2409487 -4.1809478 -4.1695046 -4.2205238 -4.28657 -4.29932 -4.2522163 -4.1827312 -4.1176805 -4.0746603 -4.0729218 -4.1187096 -4.185349 -4.2474804 -4.2958617][-4.2578335 -4.2284713 -4.2378573 -4.2811117 -4.3085065 -4.2668586 -4.1852293 -4.1216426 -4.0922203 -4.0962052 -4.12791 -4.1898274 -4.2602963 -4.3235664 -4.3700256][-4.2784209 -4.2897482 -4.3212109 -4.3424864 -4.3080568 -4.1975112 -4.0771432 -4.01146 -4.0019345 -4.0379767 -4.1017804 -4.1932511 -4.2909884 -4.3713923 -4.4198279][-4.2815738 -4.330719 -4.3807454 -4.3744369 -4.2692437 -4.0854983 -3.9241543 -3.8464131 -3.8403187 -3.8873262 -3.9683254 -4.0895042 -4.2284055 -4.3415036 -4.4034081][-4.2708611 -4.3438282 -4.4066143 -4.3840656 -4.2340493 -4.006659 -3.8196228 -3.725765 -3.703258 -3.7350001 -3.8079898 -3.9386885 -4.1024137 -4.2407641 -4.3237605][-4.2656994 -4.3452954 -4.4149313 -4.3964348 -4.247076 -4.0242739 -3.8356152 -3.7192855 -3.6577997 -3.6491141 -3.6943698 -3.8139772 -3.9787552 -4.1257339 -4.2277932][-4.2797918 -4.353426 -4.4237542 -4.4184942 -4.2987738 -4.1141963 -3.9488859 -3.8242671 -3.7302508 -3.6854856 -3.7066252 -3.814666 -3.9703252 -4.1076016 -4.2069106][-4.3425946 -4.4046254 -4.4641519 -4.4652553 -4.3807878 -4.2511506 -4.1334286 -4.0327916 -3.9373825 -3.8781247 -3.8859105 -3.9781942 -4.1106873 -4.2194066 -4.2878542][-4.421792 -4.4625487 -4.4977317 -4.4982343 -4.4511046 -4.3796082 -4.3150816 -4.2531023 -4.1800508 -4.1294646 -4.1360116 -4.2099066 -4.3098006 -4.3827043 -4.4130626][-4.4737711 -4.4885 -4.49006 -4.4802456 -4.4638042 -4.4396043 -4.4126382 -4.3796086 -4.3320231 -4.3016458 -4.3201561 -4.3846011 -4.4553609 -4.4959555 -4.4977188][-4.50545 -4.4985838 -4.4646778 -4.4336863 -4.433908 -4.4448647 -4.4421654 -4.4230642 -4.3916807 -4.3807712 -4.4139004 -4.4713292 -4.5140162 -4.5210505 -4.5008316][-4.4948959 -4.4668369 -4.4038682 -4.3498335 -4.3532438 -4.3827343 -4.387774 -4.3638754 -4.3323731 -4.3306036 -4.3742638 -4.42612 -4.44522 -4.4247479 -4.3906083][-4.4295607 -4.3800383 -4.3028688 -4.2356439 -4.2327905 -4.2644687 -4.2687111 -4.2430468 -4.21789 -4.2268033 -4.2765169 -4.3236752 -4.328445 -4.2914829 -4.247117]]...]
INFO - root - 2017-12-07 17:49:55.091738: step 36410, loss = 21.54, batch loss = 21.46 (8.2 examples/sec; 0.971 sec/batch; 79h:53m:45s remains)
INFO - root - 2017-12-07 17:50:04.609024: step 36420, loss = 21.74, batch loss = 21.66 (8.6 examples/sec; 0.928 sec/batch; 76h:19m:40s remains)
INFO - root - 2017-12-07 17:50:14.040435: step 36430, loss = 21.46, batch loss = 21.38 (9.1 examples/sec; 0.880 sec/batch; 72h:23m:53s remains)
INFO - root - 2017-12-07 17:50:23.570688: step 36440, loss = 21.39, batch loss = 21.31 (7.8 examples/sec; 1.023 sec/batch; 84h:06m:16s remains)
INFO - root - 2017-12-07 17:50:32.914020: step 36450, loss = 21.48, batch loss = 21.39 (8.5 examples/sec; 0.940 sec/batch; 77h:20m:29s remains)
INFO - root - 2017-12-07 17:50:42.230311: step 36460, loss = 21.88, batch loss = 21.80 (7.5 examples/sec; 1.063 sec/batch; 87h:24m:45s remains)
INFO - root - 2017-12-07 17:50:51.661831: step 36470, loss = 21.08, batch loss = 21.00 (8.4 examples/sec; 0.951 sec/batch; 78h:14m:18s remains)
INFO - root - 2017-12-07 17:51:01.081540: step 36480, loss = 21.56, batch loss = 21.48 (8.7 examples/sec; 0.924 sec/batch; 75h:59m:29s remains)
INFO - root - 2017-12-07 17:51:10.558666: step 36490, loss = 21.48, batch loss = 21.40 (8.5 examples/sec; 0.937 sec/batch; 77h:03m:49s remains)
INFO - root - 2017-12-07 17:51:19.960438: step 36500, loss = 21.64, batch loss = 21.56 (8.4 examples/sec; 0.952 sec/batch; 78h:17m:50s remains)
2017-12-07 17:51:20.910508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3242826 -4.3278928 -4.3403635 -4.3633885 -4.388402 -4.3838234 -4.3198075 -4.210515 -4.1348338 -4.1441913 -4.222796 -4.3332381 -4.436388 -4.491549 -4.4720974][-4.2929783 -4.28567 -4.3238225 -4.3928852 -4.4613395 -4.4891853 -4.4369993 -4.32183 -4.23569 -4.2369761 -4.3047533 -4.3997126 -4.4868884 -4.5322762 -4.5091205][-4.2602844 -4.2412047 -4.2923741 -4.3860555 -4.4785452 -4.5196667 -4.4652061 -4.3436332 -4.2628074 -4.2813034 -4.3634167 -4.4586625 -4.5360994 -4.5738416 -4.5457034][-4.239048 -4.2078676 -4.244103 -4.3219185 -4.3942547 -4.4009256 -4.3109322 -4.1848254 -4.1458459 -4.2326727 -4.3721128 -4.494173 -4.5764055 -4.6094255 -4.5723529][-4.2451572 -4.1955552 -4.1936464 -4.2206693 -4.2285962 -4.1574144 -4.0080242 -3.8864865 -3.9262662 -4.1201973 -4.3439116 -4.5112791 -4.6129756 -4.6441946 -4.5911021][-4.2654405 -4.1945071 -4.1514835 -4.1224065 -4.0594535 -3.9104676 -3.7097735 -3.6001976 -3.713913 -3.99311 -4.2763267 -4.4863582 -4.622108 -4.6654177 -4.6020403][-4.2880521 -4.2076306 -4.1460123 -4.0827827 -3.97222 -3.7812948 -3.562974 -3.4616761 -3.5988197 -3.8898966 -4.1774688 -4.4121275 -4.5878005 -4.6601954 -4.6057][-4.3278418 -4.2610135 -4.2044268 -4.1269503 -3.9948092 -3.8031497 -3.6058249 -3.5132821 -3.62342 -3.8647728 -4.1163712 -4.3524022 -4.5511174 -4.6491437 -4.6134315][-4.4027147 -4.3674088 -4.3274131 -4.2390904 -4.094502 -3.9246731 -3.7798276 -3.716918 -3.7879791 -3.9558339 -4.1549382 -4.364151 -4.548295 -4.6458054 -4.6210427][-4.4793425 -4.4813342 -4.4608045 -4.3679714 -4.2165909 -4.06975 -3.984329 -3.967834 -4.0186062 -4.1258 -4.2719669 -4.4364467 -4.5799842 -4.652101 -4.6193237][-4.4944377 -4.5188546 -4.5101805 -4.4275494 -4.2905655 -4.17057 -4.1280594 -4.1456623 -4.1904416 -4.2678576 -4.3856621 -4.520546 -4.6290889 -4.667501 -4.6095395][-4.4276452 -4.4521623 -4.4504886 -4.3917589 -4.2911983 -4.2020683 -4.1758609 -4.2005959 -4.2488379 -4.3345137 -4.4615822 -4.591012 -4.6766205 -4.6824713 -4.5933814][-4.2980566 -4.3122592 -4.3189211 -4.2923541 -4.2353234 -4.1728511 -4.1441779 -4.1582289 -4.2133322 -4.3315434 -4.4923091 -4.6294875 -4.6980524 -4.678093 -4.5682931][-4.2036915 -4.2098117 -4.2223897 -4.2189703 -4.1965761 -4.1605544 -4.1332812 -4.1319504 -4.1793041 -4.3097405 -4.4868255 -4.6272583 -4.6863585 -4.6546021 -4.5414138][-4.25307 -4.2595835 -4.2679086 -4.2680678 -4.26492 -4.2582026 -4.2472878 -4.2343755 -4.25171 -4.3458633 -4.4888515 -4.604744 -4.6496587 -4.6158428 -4.5137014]]...]
INFO - root - 2017-12-07 17:51:30.219621: step 36510, loss = 20.90, batch loss = 20.82 (9.1 examples/sec; 0.883 sec/batch; 72h:37m:35s remains)
INFO - root - 2017-12-07 17:51:39.613767: step 36520, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.949 sec/batch; 78h:01m:07s remains)
INFO - root - 2017-12-07 17:51:48.985595: step 36530, loss = 21.36, batch loss = 21.27 (7.9 examples/sec; 1.016 sec/batch; 83h:32m:37s remains)
INFO - root - 2017-12-07 17:51:58.335113: step 36540, loss = 21.72, batch loss = 21.64 (9.0 examples/sec; 0.893 sec/batch; 73h:26m:30s remains)
INFO - root - 2017-12-07 17:52:07.764334: step 36550, loss = 21.10, batch loss = 21.02 (8.9 examples/sec; 0.903 sec/batch; 74h:15m:45s remains)
INFO - root - 2017-12-07 17:52:16.974078: step 36560, loss = 21.45, batch loss = 21.37 (8.8 examples/sec; 0.904 sec/batch; 74h:20m:47s remains)
INFO - root - 2017-12-07 17:52:26.444606: step 36570, loss = 21.63, batch loss = 21.55 (8.6 examples/sec; 0.935 sec/batch; 76h:52m:06s remains)
INFO - root - 2017-12-07 17:52:35.799217: step 36580, loss = 21.44, batch loss = 21.36 (9.5 examples/sec; 0.839 sec/batch; 68h:56m:50s remains)
INFO - root - 2017-12-07 17:52:45.191774: step 36590, loss = 21.61, batch loss = 21.52 (8.7 examples/sec; 0.919 sec/batch; 75h:33m:16s remains)
INFO - root - 2017-12-07 17:52:54.508121: step 36600, loss = 21.73, batch loss = 21.65 (8.5 examples/sec; 0.944 sec/batch; 77h:35m:41s remains)
2017-12-07 17:52:55.547938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3002839 -4.2710128 -4.2647643 -4.2532964 -4.2305408 -4.2312675 -4.2645226 -4.3292794 -4.3849072 -4.3806162 -4.3081942 -4.2350068 -4.226 -4.2689409 -4.3165221][-4.2712574 -4.2474847 -4.2327862 -4.1927323 -4.1387525 -4.1129174 -4.1340318 -4.200408 -4.2597222 -4.2531967 -4.1729307 -4.0978179 -4.0980339 -4.1636658 -4.2350965][-4.28264 -4.271935 -4.2512274 -4.1873851 -4.0990272 -4.0345178 -4.0267673 -4.0768995 -4.1251049 -4.1164732 -4.0466247 -3.991375 -4.0143676 -4.1034575 -4.1921387][-4.3079295 -4.3142023 -4.296639 -4.2234325 -4.1155634 -4.0283856 -4.0060062 -4.0477977 -4.0910797 -4.0853252 -4.0290418 -3.9899321 -4.0221534 -4.114069 -4.1992555][-4.2957497 -4.3245034 -4.3228726 -4.2553635 -4.1444111 -4.05507 -4.0377464 -4.0888939 -4.1423326 -4.1432829 -4.0910344 -4.0499854 -4.0709186 -4.1467004 -4.2171388][-4.2341976 -4.2951603 -4.3208413 -4.2682986 -4.1635818 -4.08134 -4.07409 -4.13772 -4.2025232 -4.2068129 -4.1535273 -4.1108336 -4.1228576 -4.1791577 -4.2318649][-4.157526 -4.2518897 -4.3096609 -4.2731819 -4.177557 -4.1013513 -4.0995264 -4.1666679 -4.2325234 -4.2337437 -4.1818027 -4.1471224 -4.1613955 -4.2036133 -4.23757][-4.0881882 -4.2034936 -4.2895589 -4.2730651 -4.1904345 -4.1189518 -4.1151662 -4.172699 -4.2277026 -4.2182884 -4.166873 -4.1419735 -4.1634707 -4.1986318 -4.2218366][-4.0832844 -4.1920185 -4.2882175 -4.2851782 -4.2066379 -4.1260643 -4.1038542 -4.1400123 -4.1806993 -4.1669955 -4.1229081 -4.1093526 -4.1394143 -4.1776004 -4.2010117][-4.15784 -4.231174 -4.3074594 -4.3007216 -4.2168565 -4.1192508 -4.072041 -4.0894961 -4.1287446 -4.1296377 -4.1033382 -4.0983071 -4.1282821 -4.1676145 -4.1930065][-4.2418642 -4.2703967 -4.3143172 -4.2999549 -4.2203298 -4.1164403 -4.0473661 -4.0461965 -4.0875607 -4.1068521 -4.0948596 -4.0915637 -4.1131778 -4.149931 -4.1789012][-4.2839408 -4.2782369 -4.3011703 -4.291894 -4.22702 -4.1205873 -4.0236974 -3.995609 -4.0331659 -4.06791 -4.0708122 -4.0709491 -4.0846734 -4.1164861 -4.1532345][-4.28688 -4.2651353 -4.2912784 -4.3055725 -4.264451 -4.1595144 -4.0335355 -3.973937 -4.0008159 -4.0408716 -4.0511255 -4.04994 -4.0565777 -4.081471 -4.1229134][-4.292047 -4.2672162 -4.3010869 -4.3384624 -4.3254657 -4.23065 -4.0873294 -4.0031624 -4.019721 -4.0577283 -4.0603766 -4.0456324 -4.0414386 -4.0575237 -4.098567][-4.3149362 -4.29309 -4.331058 -4.38253 -4.3885155 -4.3030081 -4.1526227 -4.0548506 -4.0660133 -4.1091948 -4.1092296 -4.0822625 -4.0639858 -4.0664473 -4.0965047]]...]
INFO - root - 2017-12-07 17:53:04.859924: step 36610, loss = 21.59, batch loss = 21.51 (8.0 examples/sec; 0.995 sec/batch; 81h:46m:10s remains)
INFO - root - 2017-12-07 17:53:14.226984: step 36620, loss = 21.39, batch loss = 21.31 (8.5 examples/sec; 0.944 sec/batch; 77h:33m:17s remains)
INFO - root - 2017-12-07 17:53:23.638922: step 36630, loss = 22.08, batch loss = 22.00 (9.4 examples/sec; 0.847 sec/batch; 69h:37m:16s remains)
INFO - root - 2017-12-07 17:53:33.002869: step 36640, loss = 21.36, batch loss = 21.28 (8.9 examples/sec; 0.894 sec/batch; 73h:29m:51s remains)
INFO - root - 2017-12-07 17:53:42.505066: step 36650, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.968 sec/batch; 79h:32m:30s remains)
INFO - root - 2017-12-07 17:53:51.740815: step 36660, loss = 21.45, batch loss = 21.37 (8.7 examples/sec; 0.919 sec/batch; 75h:32m:30s remains)
INFO - root - 2017-12-07 17:54:00.978727: step 36670, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.941 sec/batch; 77h:20m:53s remains)
INFO - root - 2017-12-07 17:54:10.392346: step 36680, loss = 21.59, batch loss = 21.51 (8.2 examples/sec; 0.970 sec/batch; 79h:43m:44s remains)
INFO - root - 2017-12-07 17:54:19.686099: step 36690, loss = 21.34, batch loss = 21.26 (8.5 examples/sec; 0.938 sec/batch; 77h:04m:25s remains)
INFO - root - 2017-12-07 17:54:29.175122: step 36700, loss = 20.63, batch loss = 20.54 (8.3 examples/sec; 0.967 sec/batch; 79h:25m:35s remains)
2017-12-07 17:54:30.162277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5054436 -4.5326548 -4.5565639 -4.5654631 -4.5219889 -4.4209986 -4.3221436 -4.2570629 -4.2139959 -4.1938429 -4.2083874 -4.2776814 -4.3938236 -4.5176458 -4.6261458][-4.5394382 -4.57537 -4.5988574 -4.6127644 -4.5789251 -4.4816732 -4.3744555 -4.302465 -4.2517104 -4.2230024 -4.2220988 -4.2628326 -4.3494 -4.4616466 -4.5753174][-4.5759745 -4.6053166 -4.6121683 -4.61362 -4.5924931 -4.522625 -4.4309926 -4.3606977 -4.3042974 -4.2637234 -4.2428236 -4.2498522 -4.2925234 -4.3783526 -4.4868097][-4.60286 -4.6130214 -4.5865784 -4.5558524 -4.5321388 -4.4920716 -4.4325242 -4.3782854 -4.3270168 -4.2816873 -4.2479544 -4.2379003 -4.256968 -4.3264809 -4.4240794][-4.5947361 -4.585516 -4.5348263 -4.4803009 -4.4422874 -4.4071512 -4.3601413 -4.3160682 -4.2778668 -4.2447433 -4.21817 -4.2206707 -4.2542291 -4.3351831 -4.4289556][-4.5654225 -4.5467563 -4.4924922 -4.4383326 -4.3883982 -4.3288164 -4.2600021 -4.2065892 -4.1773429 -4.1701541 -4.1730332 -4.2084413 -4.2798934 -4.3900156 -4.4869475][-4.533999 -4.5197 -4.4810185 -4.4450388 -4.39165 -4.296845 -4.1796184 -4.0920281 -4.0541658 -4.0702066 -4.10951 -4.1769767 -4.2847204 -4.4291253 -4.539669][-4.4947977 -4.5015168 -4.4883275 -4.4723487 -4.4209695 -4.3019891 -4.14253 -4.016964 -3.9617586 -3.9859028 -4.04936 -4.131032 -4.2543292 -4.4184017 -4.5460863][-4.4464383 -4.4714203 -4.4859352 -4.48783 -4.4409976 -4.3195505 -4.1499333 -4.0114326 -3.9493752 -3.976047 -4.0477004 -4.1240258 -4.2336512 -4.3833976 -4.51217][-4.4052515 -4.4383812 -4.4860473 -4.5141983 -4.4775558 -4.3630638 -4.2008729 -4.0647492 -4.0039334 -4.0316286 -4.1036363 -4.174046 -4.2613115 -4.3685837 -4.4659133][-4.3854284 -4.427628 -4.5059257 -4.5581942 -4.5344682 -4.4335718 -4.2880979 -4.1579905 -4.0928545 -4.1127567 -4.1809926 -4.2537966 -4.3297658 -4.3955441 -4.4436603][-4.3798079 -4.4388804 -4.5436783 -4.61137 -4.5975027 -4.5145307 -4.39435 -4.2770271 -4.2029014 -4.1973991 -4.2461786 -4.3193207 -4.3995061 -4.4515715 -4.4669743][-4.3598742 -4.4433422 -4.57144 -4.6523719 -4.6453457 -4.5747442 -4.4760113 -4.3737216 -4.2945371 -4.2612176 -4.277719 -4.3341136 -4.4181819 -4.4876065 -4.5126963][-4.3601713 -4.4470677 -4.5666537 -4.6425972 -4.6421967 -4.5881371 -4.5096064 -4.4219441 -4.345356 -4.2980824 -4.2879987 -4.3127007 -4.3796577 -4.4637575 -4.5207653][-4.4070897 -4.4718933 -4.5489883 -4.5932584 -4.5932679 -4.5654655 -4.5206385 -4.4597368 -4.3990226 -4.3510361 -4.3209553 -4.3044267 -4.3298726 -4.4037118 -4.4761362]]...]
INFO - root - 2017-12-07 17:54:39.581921: step 36710, loss = 21.31, batch loss = 21.22 (8.7 examples/sec; 0.920 sec/batch; 75h:36m:46s remains)
INFO - root - 2017-12-07 17:54:48.936655: step 36720, loss = 21.47, batch loss = 21.39 (8.4 examples/sec; 0.947 sec/batch; 77h:50m:09s remains)
INFO - root - 2017-12-07 17:54:58.316092: step 36730, loss = 21.07, batch loss = 20.99 (8.5 examples/sec; 0.940 sec/batch; 77h:11m:35s remains)
INFO - root - 2017-12-07 17:55:07.815028: step 36740, loss = 21.34, batch loss = 21.25 (8.4 examples/sec; 0.954 sec/batch; 78h:20m:11s remains)
INFO - root - 2017-12-07 17:55:17.156900: step 36750, loss = 21.52, batch loss = 21.44 (8.6 examples/sec; 0.932 sec/batch; 76h:34m:29s remains)
INFO - root - 2017-12-07 17:55:26.583845: step 36760, loss = 21.66, batch loss = 21.58 (8.5 examples/sec; 0.944 sec/batch; 77h:32m:13s remains)
INFO - root - 2017-12-07 17:55:35.917508: step 36770, loss = 21.20, batch loss = 21.11 (8.9 examples/sec; 0.898 sec/batch; 73h:44m:50s remains)
INFO - root - 2017-12-07 17:55:45.380797: step 36780, loss = 21.62, batch loss = 21.54 (8.7 examples/sec; 0.921 sec/batch; 75h:38m:23s remains)
INFO - root - 2017-12-07 17:55:54.775875: step 36790, loss = 21.81, batch loss = 21.72 (8.3 examples/sec; 0.960 sec/batch; 78h:52m:05s remains)
INFO - root - 2017-12-07 17:56:04.338664: step 36800, loss = 21.29, batch loss = 21.21 (8.4 examples/sec; 0.955 sec/batch; 78h:25m:27s remains)
2017-12-07 17:56:05.388841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6225529 -4.6021791 -4.6157765 -4.6666713 -4.7343254 -4.7962346 -4.8301783 -4.8301458 -4.8010745 -4.7533164 -4.6854806 -4.5982823 -4.5050845 -4.433629 -4.4234142][-4.471015 -4.4504843 -4.5011725 -4.60099 -4.701407 -4.77358 -4.8008709 -4.7855783 -4.75087 -4.7089386 -4.6426277 -4.5361395 -4.4038196 -4.2995524 -4.2912259][-4.30135 -4.3060312 -4.4085793 -4.5566468 -4.6758165 -4.7357955 -4.7308106 -4.6851463 -4.6434612 -4.6219077 -4.5800152 -4.4717236 -4.3074622 -4.1717219 -4.1736493][-4.1550674 -4.1980653 -4.355372 -4.5432134 -4.6701789 -4.7048206 -4.6509042 -4.5631266 -4.5152612 -4.5234785 -4.5186477 -4.4216313 -4.2378821 -4.0828085 -4.1018028][-4.1278362 -4.1973524 -4.3818049 -4.5767593 -4.6809907 -4.6639585 -4.5422497 -4.4056706 -4.3592281 -4.4078655 -4.4581127 -4.3997021 -4.2286925 -4.0753107 -4.1061788][-4.2426648 -4.3096094 -4.4594455 -4.5901814 -4.611732 -4.5047078 -4.3097954 -4.1433573 -4.1294246 -4.24376 -4.3740005 -4.3909025 -4.2794237 -4.1583996 -4.1889243][-4.3716249 -4.4021254 -4.4707894 -4.4893947 -4.3880467 -4.1753073 -3.9189823 -3.7554011 -3.8087747 -4.0111203 -4.2342706 -4.3518219 -4.339406 -4.277144 -4.2979007][-4.4486403 -4.448842 -4.4557619 -4.385736 -4.1794596 -3.8706994 -3.5527129 -3.389957 -3.5098968 -3.8006649 -4.1135483 -4.3265224 -4.406796 -4.39612 -4.4007239][-4.4515414 -4.4655151 -4.4901242 -4.4262905 -4.200768 -3.849514 -3.4874587 -3.3137214 -3.4565315 -3.7875316 -4.1385765 -4.38508 -4.4898438 -4.4833198 -4.4591117][-4.3560519 -4.4147968 -4.5151062 -4.5361819 -4.3795037 -4.0566692 -3.6962354 -3.5142653 -3.639761 -3.9520311 -4.2758956 -4.4863067 -4.5489182 -4.5062404 -4.455636][-4.2444344 -4.3452554 -4.5053024 -4.5991812 -4.5250249 -4.2731581 -3.9666443 -3.8084536 -3.9085011 -4.1612363 -4.4074736 -4.5420251 -4.5412807 -4.4633937 -4.4098206][-4.2012291 -4.3201489 -4.4946303 -4.6119032 -4.5876136 -4.412765 -4.1927457 -4.0917597 -4.1735291 -4.3454185 -4.4834461 -4.5220757 -4.4640188 -4.3760457 -4.3498125][-4.2434411 -4.3596711 -4.5206189 -4.6306639 -4.6275692 -4.5104113 -4.3702722 -4.3303251 -4.400095 -4.48998 -4.5188551 -4.4699926 -4.3758507 -4.3009157 -4.308938][-4.3355489 -4.4287834 -4.5592847 -4.6496029 -4.6544347 -4.5823665 -4.5067282 -4.5112939 -4.5657797 -4.5874968 -4.5370464 -4.4297452 -4.323679 -4.2724652 -4.3066912][-4.4318657 -4.4860048 -4.5711517 -4.6328926 -4.6397977 -4.6055193 -4.5789232 -4.6065469 -4.6440673 -4.6287684 -4.5505943 -4.432117 -4.3376389 -4.3075957 -4.34664]]...]
INFO - root - 2017-12-07 17:56:14.896231: step 36810, loss = 21.56, batch loss = 21.48 (8.3 examples/sec; 0.963 sec/batch; 79h:03m:46s remains)
INFO - root - 2017-12-07 17:56:24.657634: step 36820, loss = 21.35, batch loss = 21.27 (9.2 examples/sec; 0.867 sec/batch; 71h:14m:32s remains)
INFO - root - 2017-12-07 17:56:34.063573: step 36830, loss = 21.51, batch loss = 21.43 (8.2 examples/sec; 0.972 sec/batch; 79h:47m:28s remains)
INFO - root - 2017-12-07 17:56:43.473859: step 36840, loss = 21.59, batch loss = 21.50 (8.7 examples/sec; 0.915 sec/batch; 75h:10m:52s remains)
INFO - root - 2017-12-07 17:56:52.964557: step 36850, loss = 21.25, batch loss = 21.16 (8.5 examples/sec; 0.940 sec/batch; 77h:10m:00s remains)
INFO - root - 2017-12-07 17:57:02.437332: step 36860, loss = 21.65, batch loss = 21.57 (8.3 examples/sec; 0.968 sec/batch; 79h:29m:21s remains)
INFO - root - 2017-12-07 17:57:11.762717: step 36870, loss = 21.40, batch loss = 21.32 (8.4 examples/sec; 0.956 sec/batch; 78h:28m:39s remains)
INFO - root - 2017-12-07 17:57:21.211584: step 36880, loss = 21.53, batch loss = 21.45 (8.7 examples/sec; 0.923 sec/batch; 75h:48m:27s remains)
INFO - root - 2017-12-07 17:57:30.635882: step 36890, loss = 21.17, batch loss = 21.09 (8.9 examples/sec; 0.895 sec/batch; 73h:29m:26s remains)
INFO - root - 2017-12-07 17:57:40.071977: step 36900, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.925 sec/batch; 75h:57m:28s remains)
2017-12-07 17:57:41.187229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0739651 -4.1605463 -4.3351731 -4.5047417 -4.6200361 -4.658586 -4.6412315 -4.59544 -4.5204549 -4.4222269 -4.3653932 -4.3150029 -4.2439013 -4.2142138 -4.2358232][-4.2203431 -4.3138461 -4.478056 -4.6313167 -4.7243381 -4.7295775 -4.6742458 -4.600975 -4.4963493 -4.3628554 -4.2802749 -4.2340622 -4.1853595 -4.1855597 -4.2396574][-4.3766685 -4.4609337 -4.5949969 -4.7028666 -4.7378736 -4.6823049 -4.5808249 -4.4970865 -4.4060316 -4.2828112 -4.1978416 -4.1658616 -4.1467762 -4.1754861 -4.2578096][-4.4269776 -4.4975233 -4.6056371 -4.6663246 -4.632875 -4.506496 -4.362648 -4.2917051 -4.264173 -4.2113118 -4.1638317 -4.1528912 -4.1524115 -4.1939883 -4.2914362][-4.3789129 -4.4367065 -4.523344 -4.5425687 -4.4406676 -4.2375607 -4.0482049 -3.9952362 -4.0621967 -4.1344824 -4.1845284 -4.2273893 -4.2466173 -4.2850428 -4.3751693][-4.2583508 -4.2986164 -4.3665771 -4.3637438 -4.2203774 -3.9585762 -3.7085853 -3.6403809 -3.7836165 -3.9924092 -4.1793747 -4.3171043 -4.3756804 -4.4060326 -4.4702139][-4.089673 -4.1102624 -4.1795859 -4.1929874 -4.0612769 -3.7933073 -3.5079806 -3.3996067 -3.565172 -3.8528941 -4.1407738 -4.3631573 -4.4614844 -4.4831448 -4.5162][-3.9036584 -3.913115 -4.0214906 -4.1078763 -4.0601311 -3.8707922 -3.6277621 -3.5007658 -3.6207628 -3.8766778 -4.1556211 -4.3924251 -4.5017848 -4.5161185 -4.5243745][-3.7672212 -3.7640817 -3.9197066 -4.0899463 -4.1469951 -4.0727506 -3.9265919 -3.8183956 -3.869252 -4.0278478 -4.2281365 -4.428494 -4.52508 -4.5260172 -4.5050945][-3.738189 -3.7345858 -3.90797 -4.1135268 -4.23475 -4.2537966 -4.212286 -4.1582851 -4.1767578 -4.2594829 -4.3807874 -4.5142765 -4.5632625 -4.5315127 -4.4717865][-3.8666351 -3.8795216 -4.0399351 -4.2290487 -4.3570876 -4.4121466 -4.426693 -4.4138947 -4.4304457 -4.4849334 -4.5621791 -4.6287479 -4.613811 -4.5342369 -4.4347672][-4.1141315 -4.1436319 -4.27806 -4.4266086 -4.5224695 -4.5578041 -4.56856 -4.5664854 -4.5931425 -4.6514549 -4.7130704 -4.7325335 -4.6615086 -4.5396781 -4.4174638][-4.3416367 -4.3852611 -4.4979296 -4.6093659 -4.6660757 -4.6693683 -4.6599827 -4.6633778 -4.6998339 -4.755487 -4.7967882 -4.7796931 -4.6771045 -4.5367985 -4.4144769][-4.5008454 -4.5454707 -4.6365957 -4.714756 -4.7409811 -4.7211785 -4.6977682 -4.6990871 -4.7279625 -4.7609935 -4.7728438 -4.7350583 -4.63094 -4.4971004 -4.3876762][-4.5760908 -4.6067371 -4.6615896 -4.6999488 -4.703176 -4.6821661 -4.6660185 -4.6698136 -4.6860142 -4.6963429 -4.6857729 -4.6375403 -4.541738 -4.4179535 -4.3071566]]...]
INFO - root - 2017-12-07 17:57:50.573148: step 36910, loss = 21.22, batch loss = 21.14 (8.4 examples/sec; 0.953 sec/batch; 78h:15m:22s remains)
INFO - root - 2017-12-07 17:57:59.932434: step 36920, loss = 21.38, batch loss = 21.30 (8.6 examples/sec; 0.933 sec/batch; 76h:37m:19s remains)
INFO - root - 2017-12-07 17:58:09.187158: step 36930, loss = 21.79, batch loss = 21.71 (8.7 examples/sec; 0.920 sec/batch; 75h:30m:49s remains)
INFO - root - 2017-12-07 17:58:18.544207: step 36940, loss = 21.37, batch loss = 21.28 (8.8 examples/sec; 0.910 sec/batch; 74h:42m:42s remains)
INFO - root - 2017-12-07 17:58:27.936871: step 36950, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.956 sec/batch; 78h:28m:22s remains)
INFO - root - 2017-12-07 17:58:37.435257: step 36960, loss = 21.60, batch loss = 21.51 (8.8 examples/sec; 0.914 sec/batch; 75h:02m:16s remains)
INFO - root - 2017-12-07 17:58:46.718404: step 36970, loss = 21.43, batch loss = 21.35 (9.0 examples/sec; 0.889 sec/batch; 72h:59m:11s remains)
INFO - root - 2017-12-07 17:58:56.091463: step 36980, loss = 21.26, batch loss = 21.17 (8.5 examples/sec; 0.945 sec/batch; 77h:34m:55s remains)
INFO - root - 2017-12-07 17:59:05.567176: step 36990, loss = 21.71, batch loss = 21.62 (8.1 examples/sec; 0.989 sec/batch; 81h:12m:25s remains)
INFO - root - 2017-12-07 17:59:14.851688: step 37000, loss = 21.40, batch loss = 21.32 (8.7 examples/sec; 0.921 sec/batch; 75h:37m:59s remains)
2017-12-07 17:59:15.793102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3988814 -4.3035293 -4.2132173 -4.19177 -4.2202544 -4.2708907 -4.3079352 -4.3100581 -4.2945123 -4.2973123 -4.3386841 -4.4119835 -4.4994507 -4.5512857 -4.5253811][-4.4171176 -4.3525009 -4.2833486 -4.278769 -4.3165255 -4.3502631 -4.3518414 -4.3161783 -4.2726736 -4.2638569 -4.3141236 -4.4074736 -4.516891 -4.5940714 -4.586884][-4.4070907 -4.390255 -4.3594036 -4.3767076 -4.4195313 -4.4303374 -4.4017296 -4.3388462 -4.2657557 -4.2326808 -4.271441 -4.3661718 -4.4834113 -4.5782309 -4.6000195][-4.3524652 -4.3618627 -4.3665743 -4.4095392 -4.4663014 -4.4748545 -4.4304466 -4.3369765 -4.2228541 -4.1653705 -4.2023587 -4.2944503 -4.406301 -4.5105486 -4.557889][-4.2548409 -4.2619109 -4.2933702 -4.3670669 -4.4473023 -4.464736 -4.4061837 -4.2834682 -4.1437144 -4.0839367 -4.1272273 -4.208066 -4.3064513 -4.4180956 -4.48069][-4.1831851 -4.1717296 -4.2105255 -4.3037171 -4.3972673 -4.4025841 -4.3046379 -4.151927 -4.0183234 -3.9804316 -4.0278831 -4.0893106 -4.17992 -4.3091931 -4.388308][-4.1889415 -4.1424117 -4.1565018 -4.2509274 -4.3458586 -4.3256297 -4.17743 -4.0049062 -3.9057696 -3.8987036 -3.9416237 -3.996249 -4.0936489 -4.2396121 -4.3347869][-4.2516322 -4.1495471 -4.1139412 -4.1928945 -4.2846236 -4.2534089 -4.0952415 -3.9490623 -3.9121232 -3.9445863 -3.9901514 -4.0520277 -4.1455646 -4.2713366 -4.3602715][-4.3436975 -4.1864166 -4.0986447 -4.1532397 -4.2426152 -4.229816 -4.1183558 -4.0364876 -4.0569105 -4.1143022 -4.1506519 -4.1944981 -4.2537827 -4.3308907 -4.40112][-4.4502268 -4.2635574 -4.1486759 -4.1826696 -4.26805 -4.2888503 -4.250999 -4.2328825 -4.2748308 -4.3257694 -4.3282185 -4.3175025 -4.3231783 -4.3594766 -4.4167876][-4.5256572 -4.3479 -4.2458215 -4.2780428 -4.3573713 -4.3969975 -4.4034343 -4.41167 -4.44155 -4.4757485 -4.4567385 -4.4004641 -4.3606677 -4.3708606 -4.4170332][-4.553648 -4.41216 -4.3392315 -4.3779488 -4.445354 -4.4792848 -4.4897437 -4.4968133 -4.5106111 -4.5327282 -4.5108333 -4.4370708 -4.3742948 -4.3710866 -4.4084315][-4.547924 -4.4525585 -4.411551 -4.4507713 -4.5015445 -4.5179152 -4.5177641 -4.5201659 -4.527081 -4.5416803 -4.5269327 -4.465621 -4.4086027 -4.4063334 -4.4391856][-4.5325313 -4.4851217 -4.473196 -4.5061135 -4.5380716 -4.5389223 -4.5276542 -4.5230417 -4.5261126 -4.5376573 -4.5365596 -4.5055208 -4.4729838 -4.4762383 -4.5008082][-4.5174689 -4.5029678 -4.5045152 -4.5230608 -4.5359526 -4.5280242 -4.5113349 -4.500658 -4.4994159 -4.5074921 -4.5139847 -4.5054665 -4.4926782 -4.4959645 -4.5068436]]...]
INFO - root - 2017-12-07 17:59:25.320836: step 37010, loss = 21.75, batch loss = 21.67 (8.1 examples/sec; 0.984 sec/batch; 80h:44m:56s remains)
INFO - root - 2017-12-07 17:59:34.721911: step 37020, loss = 21.19, batch loss = 21.11 (8.3 examples/sec; 0.967 sec/batch; 79h:19m:54s remains)
INFO - root - 2017-12-07 17:59:44.331905: step 37030, loss = 21.28, batch loss = 21.20 (8.4 examples/sec; 0.954 sec/batch; 78h:20m:19s remains)
INFO - root - 2017-12-07 17:59:53.710788: step 37040, loss = 21.44, batch loss = 21.36 (8.8 examples/sec; 0.914 sec/batch; 74h:58m:45s remains)
INFO - root - 2017-12-07 18:00:03.091213: step 37050, loss = 21.16, batch loss = 21.07 (9.0 examples/sec; 0.886 sec/batch; 72h:41m:44s remains)
INFO - root - 2017-12-07 18:00:12.505650: step 37060, loss = 21.62, batch loss = 21.53 (8.9 examples/sec; 0.904 sec/batch; 74h:09m:17s remains)
INFO - root - 2017-12-07 18:00:21.906283: step 37070, loss = 21.47, batch loss = 21.38 (8.5 examples/sec; 0.945 sec/batch; 77h:31m:29s remains)
INFO - root - 2017-12-07 18:00:31.156689: step 37080, loss = 21.84, batch loss = 21.76 (9.0 examples/sec; 0.891 sec/batch; 73h:04m:39s remains)
INFO - root - 2017-12-07 18:00:40.553699: step 37090, loss = 22.00, batch loss = 21.92 (9.5 examples/sec; 0.840 sec/batch; 68h:56m:37s remains)
INFO - root - 2017-12-07 18:00:49.808504: step 37100, loss = 21.32, batch loss = 21.24 (8.8 examples/sec; 0.908 sec/batch; 74h:28m:29s remains)
2017-12-07 18:00:50.733174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5180559 -4.3896341 -4.3530183 -4.3759356 -4.4091191 -4.4389834 -4.4660158 -4.4880185 -4.5052824 -4.5218468 -4.5335474 -4.5367346 -4.5281372 -4.5038767 -4.462532][-4.4727631 -4.3741684 -4.3573022 -4.3901711 -4.4348593 -4.4786305 -4.516386 -4.5395308 -4.5531931 -4.5661139 -4.57419 -4.5715733 -4.5559635 -4.5260558 -4.4786911][-4.4868093 -4.4209762 -4.4200225 -4.4550433 -4.4967351 -4.530097 -4.5515137 -4.5595121 -4.569304 -4.589529 -4.6029263 -4.5918608 -4.5584888 -4.5179067 -4.4748435][-4.5456352 -4.4917722 -4.4922771 -4.5217786 -4.5450354 -4.5389137 -4.5106559 -4.486104 -4.4983778 -4.54434 -4.5808764 -4.5692329 -4.5137033 -4.4516783 -4.408936][-4.6020417 -4.5303116 -4.5167794 -4.5324264 -4.5246387 -4.4588752 -4.3598557 -4.295948 -4.3234897 -4.4191189 -4.5084085 -4.5249619 -4.4636369 -4.3749032 -4.3164206][-4.6137385 -4.5061746 -4.4593115 -4.4468832 -4.4010463 -4.278729 -4.119915 -4.0322089 -4.0898838 -4.2501655 -4.4129238 -4.4854159 -4.4430156 -4.3315487 -4.2394252][-4.5474472 -4.3928351 -4.2986465 -4.2552338 -4.1884317 -4.0409632 -3.8444364 -3.728466 -3.7894306 -3.9852319 -4.21287 -4.3600197 -4.3697972 -4.266789 -4.1582451][-4.4216776 -4.2186084 -4.0842824 -4.0293489 -3.9745431 -3.8336644 -3.6092012 -3.4442446 -3.4715374 -3.6836896 -3.9811873 -4.2209015 -4.3067822 -4.2402549 -4.1338525][-4.3168058 -4.0881042 -3.9353144 -3.8873756 -3.8631423 -3.7483459 -3.5164628 -3.3143482 -3.3074493 -3.5320487 -3.8871593 -4.1934319 -4.3307929 -4.2954588 -4.1928625][-4.2621193 -4.0447154 -3.8989477 -3.8656855 -3.877053 -3.803432 -3.5961227 -3.3875642 -3.3540719 -3.5674663 -3.93116 -4.2483034 -4.3976545 -4.3811522 -4.2839775][-4.2640634 -4.0950489 -3.972451 -3.9478896 -3.9764037 -3.938977 -3.7816398 -3.6073046 -3.578928 -3.7767971 -4.1088705 -4.3860908 -4.5054908 -4.4803457 -4.3792758][-4.3446469 -4.236165 -4.1425848 -4.1197023 -4.1439428 -4.1265063 -4.0248585 -3.9117193 -3.910254 -4.0793109 -4.3348303 -4.5249219 -4.5816469 -4.5327444 -4.4361019][-4.4490561 -4.3999372 -4.3489542 -4.3383932 -4.3564444 -4.3487439 -4.2894688 -4.2241769 -4.231945 -4.3428221 -4.4914742 -4.5802636 -4.5830407 -4.528543 -4.46006][-4.5102358 -4.5131936 -4.50706 -4.5144477 -4.5296016 -4.5241179 -4.4881334 -4.4493203 -4.4477448 -4.4908757 -4.5400376 -4.5524483 -4.5286078 -4.4863367 -4.4514146][-4.4858055 -4.5185189 -4.5412793 -4.5614519 -4.5736723 -4.5673709 -4.5455866 -4.5239215 -4.5128288 -4.5073323 -4.4917274 -4.4630666 -4.4324141 -4.405056 -4.3951735]]...]
INFO - root - 2017-12-07 18:01:00.068017: step 37110, loss = 21.37, batch loss = 21.28 (8.7 examples/sec; 0.921 sec/batch; 75h:36m:19s remains)
INFO - root - 2017-12-07 18:01:09.479336: step 37120, loss = 21.21, batch loss = 21.12 (8.7 examples/sec; 0.915 sec/batch; 75h:02m:23s remains)
INFO - root - 2017-12-07 18:01:18.823442: step 37130, loss = 21.26, batch loss = 21.18 (8.6 examples/sec; 0.930 sec/batch; 76h:18m:31s remains)
INFO - root - 2017-12-07 18:01:28.149039: step 37140, loss = 22.12, batch loss = 22.04 (8.8 examples/sec; 0.906 sec/batch; 74h:19m:26s remains)
INFO - root - 2017-12-07 18:01:37.619548: step 37150, loss = 21.85, batch loss = 21.77 (8.7 examples/sec; 0.920 sec/batch; 75h:28m:06s remains)
INFO - root - 2017-12-07 18:01:46.853132: step 37160, loss = 21.19, batch loss = 21.11 (9.4 examples/sec; 0.852 sec/batch; 69h:54m:21s remains)
INFO - root - 2017-12-07 18:01:56.281249: step 37170, loss = 21.27, batch loss = 21.19 (8.4 examples/sec; 0.949 sec/batch; 77h:53m:35s remains)
INFO - root - 2017-12-07 18:02:05.639760: step 37180, loss = 21.36, batch loss = 21.28 (8.5 examples/sec; 0.943 sec/batch; 77h:23m:35s remains)
INFO - root - 2017-12-07 18:02:14.954480: step 37190, loss = 21.85, batch loss = 21.77 (8.9 examples/sec; 0.899 sec/batch; 73h:42m:40s remains)
INFO - root - 2017-12-07 18:02:24.290236: step 37200, loss = 21.70, batch loss = 21.62 (8.9 examples/sec; 0.903 sec/batch; 74h:02m:57s remains)
2017-12-07 18:02:25.247497: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4615836 -4.4398408 -4.4316936 -4.4545665 -4.5016651 -4.5323853 -4.5263062 -4.4870996 -4.4310641 -4.3823481 -4.3644352 -4.38302 -4.4149995 -4.4280267 -4.4141388][-4.3872609 -4.3455229 -4.3340425 -4.3841195 -4.4826937 -4.5515165 -4.5498347 -4.49033 -4.4059939 -4.3340969 -4.3073521 -4.3327093 -4.3742514 -4.3921041 -4.3841887][-4.2880683 -4.2280869 -4.2138252 -4.2851763 -4.4300838 -4.5387754 -4.5469308 -4.4765859 -4.3756695 -4.2900949 -4.2597308 -4.2902436 -4.3326182 -4.3465614 -4.3418694][-4.1957574 -4.1265059 -4.1133165 -4.1878991 -4.3428411 -4.4638524 -4.477499 -4.41767 -4.3339782 -4.2635903 -4.2459159 -4.2836876 -4.320282 -4.3224888 -4.3139281][-4.1382904 -4.0676851 -4.0580869 -4.11658 -4.2375889 -4.3270631 -4.3287621 -4.2941418 -4.2633567 -4.2463961 -4.2661138 -4.3179841 -4.3499069 -4.3404031 -4.3206191][-4.1200824 -4.0484014 -4.03636 -4.0691509 -4.1400065 -4.1785421 -4.1543689 -4.1368084 -4.1622066 -4.2148862 -4.2891808 -4.3677411 -4.4054141 -4.3914123 -4.3566995][-4.1241426 -4.0444036 -4.0143671 -4.0194397 -4.0539784 -4.0586963 -4.0128365 -3.9934044 -4.0445127 -4.1485252 -4.275867 -4.39325 -4.4551706 -4.4505987 -4.4047256][-4.1332221 -4.0419559 -3.9842691 -3.9645729 -3.9868221 -3.9874005 -3.9361017 -3.9023714 -3.9422445 -4.058672 -4.2160149 -4.3701715 -4.4690547 -4.4873862 -4.4420452][-4.1554008 -4.0538473 -3.9721291 -3.9384682 -3.9663072 -3.9827905 -3.9381566 -3.8859832 -3.8913393 -3.9865041 -4.1444006 -4.3172011 -4.445941 -4.4879231 -4.4507275][-4.2004652 -4.099627 -4.0103421 -3.9748867 -4.0127358 -4.0459061 -4.0149069 -3.9547074 -3.9289627 -3.9915812 -4.1282635 -4.2916694 -4.4248137 -4.4739761 -4.4428315][-4.2631931 -4.1814461 -4.1065006 -4.0794621 -4.1184726 -4.1555939 -4.1399879 -4.0900187 -4.0518417 -4.082685 -4.1833477 -4.3148584 -4.4249492 -4.4614625 -4.4315944][-4.342515 -4.2912889 -4.2455049 -4.2322206 -4.2608423 -4.2853713 -4.2788243 -4.2457685 -4.2104321 -4.2181811 -4.2794619 -4.3690686 -4.4419808 -4.4550743 -4.422493][-4.4272728 -4.4045334 -4.385211 -4.3808389 -4.3921361 -4.39709 -4.3922367 -4.3754425 -4.3516397 -4.3483334 -4.375937 -4.4226637 -4.4575057 -4.4500003 -4.4161782][-4.4690537 -4.4628744 -4.4587321 -4.45776 -4.4556503 -4.4477606 -4.4433322 -4.4390559 -4.4288611 -4.4235764 -4.4282894 -4.4386106 -4.4408097 -4.4212041 -4.3905377][-4.4389114 -4.4378381 -4.4380622 -4.4374795 -4.4323125 -4.4244738 -4.4230351 -4.4266968 -4.4266248 -4.4236827 -4.4186606 -4.4097033 -4.3943348 -4.371748 -4.346992]]...]
INFO - root - 2017-12-07 18:02:34.613059: step 37210, loss = 21.55, batch loss = 21.46 (9.2 examples/sec; 0.873 sec/batch; 71h:35m:19s remains)
INFO - root - 2017-12-07 18:02:44.076204: step 37220, loss = 21.44, batch loss = 21.36 (8.5 examples/sec; 0.942 sec/batch; 77h:16m:57s remains)
INFO - root - 2017-12-07 18:02:53.419872: step 37230, loss = 21.55, batch loss = 21.47 (8.8 examples/sec; 0.910 sec/batch; 74h:39m:03s remains)
INFO - root - 2017-12-07 18:03:02.789095: step 37240, loss = 21.53, batch loss = 21.45 (8.3 examples/sec; 0.959 sec/batch; 78h:41m:11s remains)
INFO - root - 2017-12-07 18:03:12.215483: step 37250, loss = 21.41, batch loss = 21.32 (8.1 examples/sec; 0.990 sec/batch; 81h:13m:55s remains)
INFO - root - 2017-12-07 18:03:21.690539: step 37260, loss = 21.70, batch loss = 21.62 (8.0 examples/sec; 0.998 sec/batch; 81h:53m:12s remains)
INFO - root - 2017-12-07 18:03:31.018295: step 37270, loss = 21.44, batch loss = 21.36 (9.9 examples/sec; 0.807 sec/batch; 66h:10m:27s remains)
INFO - root - 2017-12-07 18:03:40.394830: step 37280, loss = 21.24, batch loss = 21.16 (8.7 examples/sec; 0.915 sec/batch; 75h:03m:02s remains)
INFO - root - 2017-12-07 18:03:49.778385: step 37290, loss = 21.43, batch loss = 21.34 (8.3 examples/sec; 0.967 sec/batch; 79h:16m:15s remains)
INFO - root - 2017-12-07 18:03:59.189814: step 37300, loss = 21.35, batch loss = 21.27 (8.6 examples/sec; 0.933 sec/batch; 76h:30m:31s remains)
2017-12-07 18:04:00.055127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3371973 -4.3399062 -4.3501487 -4.3624806 -4.3753405 -4.3880916 -4.3981805 -4.3998666 -4.3919926 -4.3784008 -4.3658319 -4.3570457 -4.3522258 -4.3480425 -4.339571][-4.4249067 -4.4375968 -4.4579949 -4.4824991 -4.5084939 -4.5341668 -4.5577822 -4.5699148 -4.5620193 -4.539063 -4.5117574 -4.4890194 -4.4743052 -4.46079 -4.4381962][-4.5492654 -4.5674996 -4.5935531 -4.6250391 -4.6592169 -4.6936016 -4.7251472 -4.7394786 -4.7247834 -4.6951408 -4.669683 -4.6537447 -4.6457052 -4.6315513 -4.5931668][-4.6298723 -4.6447067 -4.6676369 -4.7017903 -4.7423887 -4.7758265 -4.7893586 -4.7744837 -4.7362447 -4.7117577 -4.7313313 -4.7722182 -4.7971268 -4.7884135 -4.734756][-4.6326318 -4.6397562 -4.6566782 -4.6928196 -4.7374268 -4.7569561 -4.7192435 -4.6337428 -4.53799 -4.5120635 -4.6087918 -4.7601423 -4.85245 -4.8543434 -4.7871189][-4.557013 -4.5474486 -4.553165 -4.5900941 -4.6391587 -4.6405644 -4.54035 -4.3653259 -4.1883931 -4.1390305 -4.3100524 -4.6000237 -4.7893181 -4.8102646 -4.7329159][-4.4430661 -4.4055204 -4.3886862 -4.4174228 -4.4603972 -4.4470096 -4.3070703 -4.0695858 -3.8287024 -3.7444096 -3.9535656 -4.3595285 -4.6509614 -4.7018838 -4.6227584][-4.3573647 -4.2969494 -4.2566648 -4.2726374 -4.3038363 -4.2836008 -4.1385479 -3.8826723 -3.6198487 -3.5078309 -3.694715 -4.1306295 -4.484489 -4.5712886 -4.5082116][-4.3521357 -4.2906628 -4.2376528 -4.2412791 -4.2576089 -4.23088 -4.0964231 -3.8530564 -3.6083541 -3.4963374 -3.6241951 -4.0014133 -4.3544917 -4.4690623 -4.4368205][-4.443646 -4.4117842 -4.3673148 -4.35643 -4.3375173 -4.28402 -4.1598148 -3.95094 -3.753546 -3.6614258 -3.7321894 -4.0120344 -4.3182416 -4.4414349 -4.4443283][-4.5782905 -4.6069918 -4.5982423 -4.5748763 -4.5085206 -4.4158893 -4.29732 -4.1324372 -3.9935725 -3.93493 -3.9692223 -4.1466146 -4.372982 -4.4864349 -4.5223069][-4.6452622 -4.7306509 -4.7680244 -4.7505274 -4.6619124 -4.5541372 -4.4496746 -4.3291411 -4.24858 -4.2378054 -4.2725086 -4.3718042 -4.5012574 -4.5783792 -4.6216273][-4.632462 -4.7301922 -4.7901831 -4.7889671 -4.7172 -4.6346393 -4.5629854 -4.4891019 -4.46087 -4.4941506 -4.5421081 -4.5846109 -4.6222491 -4.6489663 -4.6792283][-4.6143022 -4.6869674 -4.7306404 -4.7270222 -4.6762285 -4.6322408 -4.605516 -4.5808725 -4.5876484 -4.63281 -4.6740961 -4.6746869 -4.6514149 -4.6429133 -4.6684442][-4.6063962 -4.6461277 -4.6588335 -4.6358938 -4.5849352 -4.5540662 -4.5524468 -4.5648932 -4.5937271 -4.6316319 -4.6587009 -4.6435971 -4.6060724 -4.5950022 -4.6332736]]...]
INFO - root - 2017-12-07 18:04:09.548172: step 37310, loss = 21.50, batch loss = 21.42 (8.5 examples/sec; 0.936 sec/batch; 76h:44m:52s remains)
INFO - root - 2017-12-07 18:04:18.925944: step 37320, loss = 21.51, batch loss = 21.43 (8.0 examples/sec; 0.995 sec/batch; 81h:34m:11s remains)
INFO - root - 2017-12-07 18:04:28.539051: step 37330, loss = 21.23, batch loss = 21.15 (8.3 examples/sec; 0.963 sec/batch; 78h:58m:56s remains)
INFO - root - 2017-12-07 18:04:37.886718: step 37340, loss = 21.44, batch loss = 21.35 (9.1 examples/sec; 0.882 sec/batch; 72h:20m:27s remains)
INFO - root - 2017-12-07 18:04:47.215066: step 37350, loss = 21.44, batch loss = 21.35 (8.9 examples/sec; 0.896 sec/batch; 73h:27m:42s remains)
INFO - root - 2017-12-07 18:04:56.529654: step 37360, loss = 21.51, batch loss = 21.43 (8.3 examples/sec; 0.965 sec/batch; 79h:08m:52s remains)
INFO - root - 2017-12-07 18:05:06.055416: step 37370, loss = 21.60, batch loss = 21.52 (8.2 examples/sec; 0.972 sec/batch; 79h:38m:44s remains)
INFO - root - 2017-12-07 18:05:15.290198: step 37380, loss = 21.58, batch loss = 21.50 (8.1 examples/sec; 0.993 sec/batch; 81h:23m:40s remains)
INFO - root - 2017-12-07 18:05:24.654251: step 37390, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.981 sec/batch; 80h:26m:59s remains)
INFO - root - 2017-12-07 18:05:33.964895: step 37400, loss = 21.60, batch loss = 21.52 (8.7 examples/sec; 0.916 sec/batch; 75h:05m:49s remains)
2017-12-07 18:05:34.912586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5510688 -4.5682173 -4.6104226 -4.6713519 -4.6903362 -4.6267157 -4.5103292 -4.43223 -4.4503732 -4.527184 -4.5727878 -4.55428 -4.5056014 -4.4697981 -4.4608207][-4.559494 -4.5750256 -4.613883 -4.6687412 -4.6724281 -4.5819354 -4.4398518 -4.3439174 -4.3611994 -4.4582753 -4.5288873 -4.5224996 -4.4722729 -4.4307404 -4.4169993][-4.5239944 -4.5422106 -4.583209 -4.6335096 -4.6221714 -4.5047679 -4.341217 -4.2374563 -4.258502 -4.3683071 -4.4564881 -4.4585142 -4.409071 -4.3634171 -4.3506832][-4.4445724 -4.4816546 -4.5473776 -4.6079345 -4.5851331 -4.4278569 -4.2209563 -4.0992789 -4.12609 -4.2525063 -4.3633223 -4.3852086 -4.3480291 -4.3084736 -4.302032][-4.3526807 -4.423027 -4.5355697 -4.616797 -4.5802169 -4.3704395 -4.0922146 -3.925168 -3.945658 -4.0930376 -4.242547 -4.3091869 -4.3110762 -4.2983537 -4.3095732][-4.2748551 -4.3790197 -4.5409517 -4.6343508 -4.5652618 -4.2931175 -3.9335194 -3.7047215 -3.7167459 -3.9015236 -4.1127615 -4.2474437 -4.3044991 -4.32434 -4.3508992][-4.2266364 -4.3553238 -4.54753 -4.6316714 -4.5073366 -4.1698194 -3.7476363 -3.4791493 -3.5086436 -3.7590909 -4.0476408 -4.2400045 -4.3232722 -4.3477035 -4.3746314][-4.1837845 -4.3160534 -4.5128074 -4.5854897 -4.4292154 -4.0802045 -3.6685586 -3.4141672 -3.4718723 -3.770792 -4.0980916 -4.2966371 -4.3541765 -4.3489614 -4.3672781][-4.1277962 -4.2364492 -4.4074807 -4.4781446 -4.3580751 -4.1026797 -3.8041351 -3.6229787 -3.6852536 -3.9534667 -4.2319493 -4.3723493 -4.3737369 -4.3349586 -4.3446665][-4.0964746 -4.1740794 -4.3053918 -4.3827853 -4.3387575 -4.2221951 -4.0719953 -3.9719648 -4.0115695 -4.1849036 -4.3599033 -4.4189053 -4.3703008 -4.3127046 -4.3072147][-4.104929 -4.1676917 -4.2766352 -4.3640347 -4.3811874 -4.3619728 -4.3127918 -4.2601504 -4.2570586 -4.3173957 -4.3820472 -4.3753829 -4.3039207 -4.2480578 -4.2311692][-4.1718307 -4.2305055 -4.324338 -4.40176 -4.4282594 -4.4271388 -4.406559 -4.3684397 -4.3260875 -4.2956591 -4.2784505 -4.2398224 -4.1735754 -4.1330719 -4.1147914][-4.3194132 -4.3582664 -4.410244 -4.4433403 -4.439671 -4.4165983 -4.3856335 -4.3454223 -4.2879295 -4.2221308 -4.1705203 -4.1303539 -4.0858569 -4.0641556 -4.0522566][-4.4604864 -4.4518428 -4.434371 -4.4063115 -4.36905 -4.3366337 -4.3120046 -4.2869525 -4.2535591 -4.215559 -4.186336 -4.167027 -4.1435876 -4.1264653 -4.1131458][-4.4828272 -4.4169297 -4.3399196 -4.2729211 -4.2244854 -4.2070117 -4.2149949 -4.2308631 -4.2490516 -4.2730174 -4.2908607 -4.2970247 -4.2857523 -4.2577462 -4.2218633]]...]
INFO - root - 2017-12-07 18:05:44.399825: step 37410, loss = 21.48, batch loss = 21.39 (8.6 examples/sec; 0.929 sec/batch; 76h:10m:41s remains)
INFO - root - 2017-12-07 18:05:53.876301: step 37420, loss = 21.47, batch loss = 21.39 (8.6 examples/sec; 0.927 sec/batch; 75h:57m:07s remains)
INFO - root - 2017-12-07 18:06:03.208351: step 37430, loss = 21.72, batch loss = 21.64 (8.1 examples/sec; 0.985 sec/batch; 80h:45m:57s remains)
INFO - root - 2017-12-07 18:06:12.710132: step 37440, loss = 21.48, batch loss = 21.40 (8.5 examples/sec; 0.937 sec/batch; 76h:48m:10s remains)
INFO - root - 2017-12-07 18:06:22.009197: step 37450, loss = 21.48, batch loss = 21.40 (8.5 examples/sec; 0.942 sec/batch; 77h:11m:54s remains)
INFO - root - 2017-12-07 18:06:31.242543: step 37460, loss = 21.28, batch loss = 21.20 (8.8 examples/sec; 0.906 sec/batch; 74h:16m:42s remains)
INFO - root - 2017-12-07 18:06:40.607837: step 37470, loss = 21.17, batch loss = 21.09 (8.8 examples/sec; 0.906 sec/batch; 74h:12m:40s remains)
INFO - root - 2017-12-07 18:06:49.910889: step 37480, loss = 21.37, batch loss = 21.29 (8.9 examples/sec; 0.899 sec/batch; 73h:39m:35s remains)
INFO - root - 2017-12-07 18:06:59.301016: step 37490, loss = 21.28, batch loss = 21.19 (8.7 examples/sec; 0.918 sec/batch; 75h:12m:18s remains)
INFO - root - 2017-12-07 18:07:08.658210: step 37500, loss = 21.52, batch loss = 21.44 (8.7 examples/sec; 0.925 sec/batch; 75h:46m:38s remains)
2017-12-07 18:07:09.611688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4617963 -4.4377575 -4.4456868 -4.4885416 -4.5017085 -4.4367361 -4.3590446 -4.2974181 -4.2381129 -4.2277403 -4.2547383 -4.2914042 -4.2937036 -4.2264771 -4.180759][-4.580792 -4.5667515 -4.5709915 -4.6068897 -4.6124182 -4.5368667 -4.4434562 -4.3669782 -4.2989578 -4.27878 -4.2896771 -4.3151379 -4.3196626 -4.255168 -4.1945767][-4.6238155 -4.6073761 -4.6046128 -4.6415534 -4.65592 -4.5959134 -4.5094657 -4.4300919 -4.3568645 -4.3215704 -4.3170958 -4.33691 -4.3491263 -4.2989373 -4.2387562][-4.5734348 -4.536325 -4.5069346 -4.5279489 -4.5456333 -4.51062 -4.450881 -4.3956537 -4.3435535 -4.3151417 -4.3139458 -4.3321042 -4.3448176 -4.3137684 -4.2726154][-4.462255 -4.3839407 -4.3058729 -4.2766438 -4.2720447 -4.2617474 -4.2498889 -4.2498584 -4.2479792 -4.2509336 -4.2735524 -4.2959843 -4.29956 -4.2857404 -4.2789903][-4.365818 -4.240737 -4.1049309 -4.0012379 -3.9483888 -3.9558291 -4.006918 -4.08266 -4.14537 -4.1940484 -4.2551837 -4.2874889 -4.2755 -4.2662563 -4.2875919][-4.3043985 -4.142633 -3.9592555 -3.7759848 -3.65511 -3.6630645 -3.7715535 -3.919677 -4.0463371 -4.1421466 -4.2479844 -4.3028669 -4.2876868 -4.2808723 -4.315506][-4.2643471 -4.1062484 -3.9249363 -3.7074227 -3.5368786 -3.5359883 -3.67532 -3.8638327 -4.0236673 -4.1325932 -4.2493649 -4.31412 -4.3067522 -4.3056207 -4.3396139][-4.2997079 -4.1865826 -4.046546 -3.8373554 -3.6465793 -3.6293857 -3.7680211 -3.9583428 -4.1118 -4.1983232 -4.287488 -4.3440876 -4.3439379 -4.3451347 -4.3627825][-4.3795681 -4.3229017 -4.2423258 -4.0697718 -3.8811436 -3.844116 -3.9582441 -4.1196589 -4.2386527 -4.2874174 -4.3326011 -4.3683071 -4.37189 -4.375514 -4.3773127][-4.4767537 -4.4675727 -4.4472013 -4.3385787 -4.1910586 -4.1498575 -4.2276464 -4.3364844 -4.3987179 -4.4021306 -4.3998556 -4.4027948 -4.3966451 -4.3927593 -4.3788528][-4.5417285 -4.5587168 -4.5717883 -4.520124 -4.429554 -4.39769 -4.4381185 -4.4890628 -4.5029902 -4.4798388 -4.4501824 -4.4271297 -4.4077277 -4.3908491 -4.3614788][-4.50851 -4.5324984 -4.5523343 -4.534935 -4.4896269 -4.467185 -4.4771633 -4.4868979 -4.4763341 -4.4484806 -4.4127908 -4.3813515 -4.3590608 -4.3395576 -4.3064432][-4.4068995 -4.4304042 -4.4472046 -4.4442754 -4.4248567 -4.4105539 -4.4071341 -4.4025135 -4.3887444 -4.3655181 -4.3332195 -4.3037858 -4.2869792 -4.2755022 -4.2490382][-4.2966595 -4.3109303 -4.3175273 -4.3131762 -4.3029027 -4.2951632 -4.2926788 -4.29214 -4.2886219 -4.279635 -4.2625117 -4.2466364 -4.2406235 -4.24098 -4.2253256]]...]
INFO - root - 2017-12-07 18:07:19.059274: step 37510, loss = 21.39, batch loss = 21.30 (8.6 examples/sec; 0.931 sec/batch; 76h:18m:31s remains)
INFO - root - 2017-12-07 18:07:28.553829: step 37520, loss = 21.31, batch loss = 21.23 (8.3 examples/sec; 0.962 sec/batch; 78h:50m:50s remains)
INFO - root - 2017-12-07 18:07:37.918068: step 37530, loss = 21.38, batch loss = 21.30 (8.4 examples/sec; 0.954 sec/batch; 78h:11m:46s remains)
INFO - root - 2017-12-07 18:07:47.255892: step 37540, loss = 21.31, batch loss = 21.22 (9.3 examples/sec; 0.856 sec/batch; 70h:08m:06s remains)
INFO - root - 2017-12-07 18:07:56.635837: step 37550, loss = 21.24, batch loss = 21.16 (9.2 examples/sec; 0.874 sec/batch; 71h:34m:22s remains)
INFO - root - 2017-12-07 18:08:06.161032: step 37560, loss = 21.63, batch loss = 21.55 (8.4 examples/sec; 0.953 sec/batch; 78h:02m:43s remains)
INFO - root - 2017-12-07 18:08:15.779122: step 37570, loss = 21.91, batch loss = 21.83 (7.8 examples/sec; 1.022 sec/batch; 83h:45m:28s remains)
INFO - root - 2017-12-07 18:08:25.121540: step 37580, loss = 21.42, batch loss = 21.34 (8.7 examples/sec; 0.920 sec/batch; 75h:22m:41s remains)
INFO - root - 2017-12-07 18:08:34.416384: step 37590, loss = 21.76, batch loss = 21.68 (8.2 examples/sec; 0.979 sec/batch; 80h:14m:19s remains)
INFO - root - 2017-12-07 18:08:43.860236: step 37600, loss = 21.84, batch loss = 21.76 (8.5 examples/sec; 0.942 sec/batch; 77h:08m:29s remains)
2017-12-07 18:08:44.838447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3385677 -4.4187 -4.4546666 -4.4107356 -4.2962532 -4.194036 -4.1609488 -4.1752758 -4.230854 -4.32042 -4.3946805 -4.4191103 -4.4082017 -4.3881254 -4.3742275][-4.3211837 -4.3855438 -4.4020829 -4.3368707 -4.21299 -4.1220117 -4.1197729 -4.1653461 -4.2372956 -4.32267 -4.3830781 -4.3986096 -4.3844318 -4.3701682 -4.3674784][-4.3527994 -4.3962183 -4.3958144 -4.3182454 -4.1904707 -4.1084728 -4.1203241 -4.1732416 -4.2414742 -4.3114367 -4.3559575 -4.3663239 -4.3543806 -4.3480144 -4.3558011][-4.4192467 -4.4471574 -4.4381447 -4.3533478 -4.2140737 -4.1204309 -4.1158462 -4.1533432 -4.2105155 -4.2724724 -4.31646 -4.3373151 -4.3368917 -4.3383069 -4.3504729][-4.4684067 -4.494956 -4.4883528 -4.4050722 -4.2596884 -4.145699 -4.1044827 -4.107821 -4.1448689 -4.206295 -4.2660913 -4.3136172 -4.3337226 -4.3437915 -4.3551397][-4.484889 -4.5219231 -4.5242229 -4.4503169 -4.3103805 -4.1851673 -4.1121974 -4.0800557 -4.0918212 -4.1480627 -4.2220664 -4.2934804 -4.333396 -4.3532581 -4.3637514][-4.4928327 -4.5322242 -4.5273886 -4.4502482 -4.3187518 -4.2010412 -4.1258564 -4.08867 -4.0970111 -4.1533589 -4.2312713 -4.3044343 -4.3436975 -4.3630023 -4.3707][-4.5029082 -4.5282841 -4.5020471 -4.4088593 -4.2829542 -4.1890359 -4.1411276 -4.1283045 -4.1528544 -4.2115788 -4.2822385 -4.3381495 -4.3574166 -4.36581 -4.37065][-4.4980259 -4.5094919 -4.4645305 -4.3535676 -4.2306871 -4.1629596 -4.1487145 -4.1633291 -4.2034154 -4.2611103 -4.3234868 -4.3611612 -4.3589344 -4.3564196 -4.3625422][-4.4686117 -4.4605751 -4.4032831 -4.2831659 -4.1668077 -4.1259456 -4.1415882 -4.1742773 -4.2198143 -4.2753839 -4.3369284 -4.3674068 -4.352716 -4.3440447 -4.3533764][-4.3971758 -4.3644652 -4.2982736 -4.1811996 -4.0810266 -4.0690103 -4.110043 -4.1525745 -4.1979742 -4.25705 -4.3282132 -4.364141 -4.3477917 -4.3370605 -4.34847][-4.3032775 -4.2509584 -4.1808619 -4.0739784 -3.9940836 -4.0045819 -4.0605078 -4.1068611 -4.1548467 -4.227448 -4.3163652 -4.3634548 -4.3507195 -4.3385739 -4.3487][-4.2428236 -4.1879873 -4.1212 -4.026166 -3.9638543 -3.9879763 -4.0486417 -4.0920448 -4.1391764 -4.2228355 -4.3245378 -4.3789487 -4.368485 -4.3517179 -4.3554125][-4.246254 -4.199193 -4.1445084 -4.0650172 -4.0178418 -4.0478296 -4.1036668 -4.1336293 -4.166872 -4.2445636 -4.3415027 -4.3928547 -4.3830767 -4.363503 -4.3619943][-4.2806568 -4.2534318 -4.2162175 -4.1462116 -4.0956326 -4.1133184 -4.1553974 -4.1724019 -4.1924114 -4.2559428 -4.335783 -4.3766818 -4.37016 -4.358181 -4.3611536]]...]
INFO - root - 2017-12-07 18:08:54.184668: step 37610, loss = 21.21, batch loss = 21.13 (8.1 examples/sec; 0.992 sec/batch; 81h:13m:56s remains)
INFO - root - 2017-12-07 18:09:03.510309: step 37620, loss = 21.50, batch loss = 21.42 (8.3 examples/sec; 0.964 sec/batch; 78h:59m:26s remains)
INFO - root - 2017-12-07 18:09:13.125806: step 37630, loss = 21.57, batch loss = 21.49 (8.5 examples/sec; 0.940 sec/batch; 76h:58m:48s remains)
INFO - root - 2017-12-07 18:09:22.562075: step 37640, loss = 21.31, batch loss = 21.22 (8.3 examples/sec; 0.962 sec/batch; 78h:49m:40s remains)
INFO - root - 2017-12-07 18:09:31.795456: step 37650, loss = 21.63, batch loss = 21.55 (8.2 examples/sec; 0.980 sec/batch; 80h:17m:23s remains)
INFO - root - 2017-12-07 18:09:41.288598: step 37660, loss = 21.05, batch loss = 20.96 (8.6 examples/sec; 0.925 sec/batch; 75h:47m:41s remains)
INFO - root - 2017-12-07 18:09:50.577557: step 37670, loss = 21.80, batch loss = 21.72 (9.0 examples/sec; 0.886 sec/batch; 72h:34m:58s remains)
INFO - root - 2017-12-07 18:09:59.919073: step 37680, loss = 21.32, batch loss = 21.24 (8.9 examples/sec; 0.900 sec/batch; 73h:43m:54s remains)
INFO - root - 2017-12-07 18:10:09.282382: step 37690, loss = 21.72, batch loss = 21.64 (8.1 examples/sec; 0.984 sec/batch; 80h:37m:00s remains)
INFO - root - 2017-12-07 18:10:18.610168: step 37700, loss = 21.48, batch loss = 21.40 (8.4 examples/sec; 0.957 sec/batch; 78h:21m:14s remains)
2017-12-07 18:10:19.602354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3130312 -4.3039641 -4.3102484 -4.3554916 -4.4363103 -4.5226922 -4.5860357 -4.6082172 -4.5965204 -4.580606 -4.583138 -4.6029921 -4.6192417 -4.6214542 -4.614532][-4.3564754 -4.3989878 -4.4491577 -4.51548 -4.5968442 -4.6767874 -4.7381606 -4.7684684 -4.7724361 -4.7716093 -4.7797623 -4.7923207 -4.7972341 -4.7931342 -4.7910633][-4.4292789 -4.5285239 -4.6159983 -4.6766319 -4.7145348 -4.7443881 -4.7786703 -4.8178005 -4.857439 -4.898356 -4.927844 -4.9293509 -4.9051447 -4.8781486 -4.8742042][-4.4841685 -4.618434 -4.7162023 -4.7369213 -4.6940246 -4.6423831 -4.633666 -4.686214 -4.7800708 -4.8876724 -4.9598212 -4.9582067 -4.8946252 -4.8187046 -4.78661][-4.4914002 -4.6272607 -4.7060919 -4.6639881 -4.5270643 -4.3906727 -4.34228 -4.4138703 -4.5709209 -4.7623672 -4.8991623 -4.9148178 -4.8155456 -4.6721349 -4.5817575][-4.4559884 -4.5759563 -4.6224642 -4.5188503 -4.3018012 -4.0946512 -4.0139561 -4.0995269 -4.3095045 -4.5779209 -4.7773976 -4.8156848 -4.6940622 -4.4998951 -4.3611121][-4.4086833 -4.5076995 -4.5276332 -4.3817215 -4.1096907 -3.8464026 -3.7346516 -3.8194733 -4.060142 -4.38265 -4.6227422 -4.6654696 -4.5284615 -4.3216248 -4.176537][-4.3706541 -4.4524245 -4.4551897 -4.2930593 -4.0015273 -3.7127557 -3.577764 -3.6472909 -3.8920355 -4.2334018 -4.4840484 -4.5142055 -4.366745 -4.1721435 -4.0501142][-4.35783 -4.428812 -4.4241276 -4.272594 -4.0059528 -3.7394116 -3.6037703 -3.6438172 -3.8504355 -4.1601348 -4.3915834 -4.4127812 -4.268671 -4.0920844 -3.9862206][-4.3777132 -4.44201 -4.4397693 -4.3199306 -4.1145506 -3.9111764 -3.8015258 -3.8178737 -3.9646096 -4.1994867 -4.3827343 -4.3987088 -4.2692151 -4.1070476 -3.9933128][-4.4120412 -4.4726286 -4.4792576 -4.4008918 -4.2650557 -4.1309609 -4.0578842 -4.0714936 -4.1794796 -4.3422418 -4.4613729 -4.4552007 -4.3298688 -4.1722412 -4.0352139][-4.4441886 -4.4989204 -4.5128565 -4.4764767 -4.4062781 -4.3335156 -4.2935281 -4.3121619 -4.3925056 -4.4896035 -4.5350337 -4.4868035 -4.3505373 -4.193006 -4.0430565][-4.4780664 -4.5182247 -4.5309834 -4.5275245 -4.5126257 -4.49107 -4.4775724 -4.4977269 -4.5471907 -4.5782504 -4.5518184 -4.4649439 -4.327878 -4.1915817 -4.0604639][-4.489593 -4.5050578 -4.5037804 -4.5134096 -4.529573 -4.5382466 -4.5392284 -4.5581236 -4.5860729 -4.5781422 -4.5151582 -4.4197717 -4.3131876 -4.2283063 -4.1447554][-4.4657025 -4.4539442 -4.4299636 -4.4324441 -4.4543653 -4.4732943 -4.4802036 -4.5004368 -4.5293427 -4.525754 -4.4714055 -4.389997 -4.31874 -4.289959 -4.2633133]]...]
INFO - root - 2017-12-07 18:10:29.077970: step 37710, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.932 sec/batch; 76h:20m:12s remains)
INFO - root - 2017-12-07 18:10:38.359512: step 37720, loss = 21.71, batch loss = 21.62 (8.6 examples/sec; 0.935 sec/batch; 76h:33m:28s remains)
INFO - root - 2017-12-07 18:10:47.708969: step 37730, loss = 21.41, batch loss = 21.33 (9.1 examples/sec; 0.883 sec/batch; 72h:15m:38s remains)
INFO - root - 2017-12-07 18:10:57.197413: step 37740, loss = 21.41, batch loss = 21.33 (8.9 examples/sec; 0.897 sec/batch; 73h:27m:30s remains)
INFO - root - 2017-12-07 18:11:06.488849: step 37750, loss = 21.30, batch loss = 21.22 (8.5 examples/sec; 0.936 sec/batch; 76h:39m:14s remains)
INFO - root - 2017-12-07 18:11:15.964130: step 37760, loss = 21.74, batch loss = 21.65 (8.2 examples/sec; 0.981 sec/batch; 80h:17m:47s remains)
INFO - root - 2017-12-07 18:11:25.437906: step 37770, loss = 21.23, batch loss = 21.15 (7.9 examples/sec; 1.014 sec/batch; 83h:02m:35s remains)
INFO - root - 2017-12-07 18:11:34.835898: step 37780, loss = 21.12, batch loss = 21.04 (8.4 examples/sec; 0.952 sec/batch; 77h:56m:06s remains)
INFO - root - 2017-12-07 18:11:43.970316: step 37790, loss = 21.57, batch loss = 21.48 (9.1 examples/sec; 0.884 sec/batch; 72h:19m:56s remains)
INFO - root - 2017-12-07 18:11:53.190965: step 37800, loss = 21.71, batch loss = 21.63 (8.8 examples/sec; 0.913 sec/batch; 74h:44m:21s remains)
2017-12-07 18:11:54.242353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.37892 -4.3756719 -4.3359308 -4.2747087 -4.2378979 -4.2292 -4.2274837 -4.2510066 -4.28856 -4.3053865 -4.2982335 -4.3029737 -4.308917 -4.2993169 -4.2758641][-4.3156667 -4.3492413 -4.3518305 -4.3218756 -4.2905235 -4.2716794 -4.26444 -4.2857704 -4.3181853 -4.3219075 -4.2999072 -4.2981024 -4.30493 -4.3085 -4.3031149][-4.2736139 -4.3374305 -4.37504 -4.3748937 -4.3458629 -4.3058724 -4.2787981 -4.2819963 -4.3013949 -4.2943673 -4.2634788 -4.2593703 -4.2675405 -4.2773108 -4.2862773][-4.2708 -4.3416548 -4.3913093 -4.3989768 -4.3566422 -4.2889566 -4.2358541 -4.2133174 -4.2159624 -4.2056413 -4.1842146 -4.1954722 -4.2131925 -4.2253871 -4.242229][-4.2921023 -4.3431225 -4.3799696 -4.3747392 -4.3104978 -4.2200718 -4.1488786 -4.104917 -4.0916967 -4.0860333 -4.0880408 -4.1246629 -4.1579952 -4.1744795 -4.1978645][-4.3286967 -4.3366141 -4.338191 -4.3098955 -4.2276077 -4.1252155 -4.0465751 -3.9931419 -3.9742765 -3.981765 -4.0110412 -4.07076 -4.1188293 -4.1449347 -4.1777306][-4.3399825 -4.2947788 -4.2546377 -4.2062063 -4.1199303 -4.0180573 -3.9383979 -3.8918419 -3.8845801 -3.9137998 -3.9679339 -4.0443215 -4.1060429 -4.1509929 -4.2000566][-4.3032432 -4.217587 -4.151166 -4.0977631 -4.0237403 -3.9353321 -3.8609891 -3.8338642 -3.8550398 -3.9082136 -3.9761641 -4.0587068 -4.1270576 -4.1855545 -4.2446284][-4.241662 -4.1409588 -4.0746841 -4.0345745 -3.9828124 -3.919296 -3.8648229 -3.8661096 -3.9185443 -3.9875104 -4.0525718 -4.1233988 -4.1770768 -4.2242241 -4.2737403][-4.1780143 -4.0957623 -4.0569048 -4.0422988 -4.0093331 -3.9671185 -3.9374776 -3.9624426 -4.0319967 -4.1014776 -4.152297 -4.1989856 -4.2232432 -4.2418828 -4.2701745][-4.1343226 -4.09617 -4.0965061 -4.1070409 -4.0848966 -4.0551295 -4.0476489 -4.0863872 -4.15546 -4.2107048 -4.2409029 -4.2565212 -4.2477293 -4.2343354 -4.2367787][-4.0999284 -4.111918 -4.1531796 -4.1903777 -4.1810532 -4.1612573 -4.1678905 -4.2081966 -4.2607656 -4.2906575 -4.2902594 -4.2662678 -4.2268486 -4.1969228 -4.1854239][-4.0756168 -4.1270356 -4.2030244 -4.2640514 -4.2703438 -4.2541928 -4.2582688 -4.2834716 -4.30665 -4.3058543 -4.2754774 -4.2164297 -4.1608534 -4.1405559 -4.1399331][-4.0608239 -4.139091 -4.2379141 -4.3124838 -4.3314528 -4.3172221 -4.3082485 -4.3047447 -4.2926784 -4.2656093 -4.2148833 -4.1359725 -4.0792966 -4.083631 -4.1098804][-4.0594964 -4.1500883 -4.2581692 -4.3313022 -4.356647 -4.3461018 -4.3229918 -4.2897048 -4.2512088 -4.2133608 -4.1598697 -4.081111 -4.0318112 -4.0563359 -4.1092434]]...]
INFO - root - 2017-12-07 18:12:03.678421: step 37810, loss = 21.28, batch loss = 21.20 (8.1 examples/sec; 0.985 sec/batch; 80h:39m:16s remains)
INFO - root - 2017-12-07 18:12:13.077071: step 37820, loss = 21.56, batch loss = 21.48 (8.9 examples/sec; 0.899 sec/batch; 73h:34m:33s remains)
INFO - root - 2017-12-07 18:12:22.512327: step 37830, loss = 21.12, batch loss = 21.04 (8.5 examples/sec; 0.938 sec/batch; 76h:47m:03s remains)
INFO - root - 2017-12-07 18:12:31.859700: step 37840, loss = 21.11, batch loss = 21.03 (8.2 examples/sec; 0.970 sec/batch; 79h:25m:55s remains)
INFO - root - 2017-12-07 18:12:41.243042: step 37850, loss = 21.22, batch loss = 21.14 (8.3 examples/sec; 0.965 sec/batch; 78h:59m:43s remains)
INFO - root - 2017-12-07 18:12:50.757135: step 37860, loss = 21.89, batch loss = 21.81 (9.1 examples/sec; 0.876 sec/batch; 71h:40m:26s remains)
INFO - root - 2017-12-07 18:13:00.250298: step 37870, loss = 21.06, batch loss = 20.97 (8.8 examples/sec; 0.907 sec/batch; 74h:14m:41s remains)
INFO - root - 2017-12-07 18:13:09.715026: step 37880, loss = 21.84, batch loss = 21.76 (8.8 examples/sec; 0.905 sec/batch; 74h:06m:01s remains)
INFO - root - 2017-12-07 18:13:19.127180: step 37890, loss = 21.21, batch loss = 21.13 (8.1 examples/sec; 0.992 sec/batch; 81h:11m:11s remains)
INFO - root - 2017-12-07 18:13:28.508623: step 37900, loss = 21.55, batch loss = 21.47 (7.9 examples/sec; 1.015 sec/batch; 83h:04m:51s remains)
2017-12-07 18:13:29.424155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3103676 -4.4114237 -4.505362 -4.557982 -4.5336227 -4.4368834 -4.284266 -4.1628709 -4.1581469 -4.2688942 -4.3500676 -4.3234639 -4.254879 -4.21539 -4.2134433][-4.2964396 -4.3826809 -4.456439 -4.4851141 -4.4390373 -4.3286581 -4.1678619 -4.043303 -4.0527439 -4.1962 -4.3251624 -4.3498921 -4.3020587 -4.2556872 -4.235167][-4.2958903 -4.3793159 -4.4477043 -4.4670773 -4.4157414 -4.30625 -4.1425138 -4.007689 -4.0144072 -4.1659636 -4.3224316 -4.3916698 -4.3666615 -4.302618 -4.2469625][-4.3073182 -4.39839 -4.4772186 -4.5063314 -4.4628944 -4.360024 -4.1939554 -4.0452404 -4.0361176 -4.1692238 -4.3204651 -4.404201 -4.3958941 -4.326664 -4.25108][-4.3225121 -4.422781 -4.512629 -4.5499187 -4.5054207 -4.3967547 -4.2175927 -4.0495186 -4.0199037 -4.1307139 -4.2705364 -4.3552542 -4.3638053 -4.31236 -4.2469931][-4.3315496 -4.4318495 -4.5176897 -4.5403738 -4.47132 -4.3348017 -4.1268463 -3.9270551 -3.8683348 -3.9674673 -4.116467 -4.2164478 -4.2529497 -4.2416925 -4.2186065][-4.3272834 -4.4182663 -4.4843626 -4.4743805 -4.371665 -4.2073307 -3.9722197 -3.7317195 -3.6252713 -3.7092738 -3.8849149 -4.0236974 -4.1034775 -4.1478457 -4.179502][-4.3165779 -4.39493 -4.4419656 -4.4037819 -4.2825918 -4.1186981 -3.890964 -3.6324327 -3.4853563 -3.5367966 -3.7149329 -3.8806434 -3.99497 -4.080286 -4.1468062][-4.3090806 -4.3855844 -4.4304619 -4.3906174 -4.2817841 -4.1472096 -3.9543335 -3.7129889 -3.5583415 -3.5789864 -3.71627 -3.8635802 -3.972578 -4.0552149 -4.1200633][-4.3154044 -4.4010143 -4.4598784 -4.4423776 -4.3634105 -4.2592912 -4.0969567 -3.8850467 -3.7468214 -3.7502019 -3.8411741 -3.9525244 -4.0352726 -4.091495 -4.1366515][-4.3340211 -4.4424233 -4.5289421 -4.5436091 -4.4886513 -4.3972111 -4.2501554 -4.0615926 -3.9512203 -3.9680409 -4.0442085 -4.1321645 -4.1859016 -4.2122431 -4.2421355][-4.3571854 -4.49021 -4.6089373 -4.6601238 -4.6270442 -4.5412326 -4.4008245 -4.2313714 -4.1512237 -4.1993942 -4.2842894 -4.3607554 -4.3916678 -4.3934145 -4.4090719][-4.3660884 -4.5050249 -4.638701 -4.717175 -4.7095118 -4.6407847 -4.5222473 -4.3826027 -4.3323946 -4.4001336 -4.4905453 -4.5542359 -4.5657673 -4.5509233 -4.5566416][-4.357501 -4.4777088 -4.5975404 -4.6799531 -4.6883888 -4.6399727 -4.5519605 -4.4497581 -4.4297166 -4.5056047 -4.5899677 -4.6335454 -4.6290646 -4.6109576 -4.6150126][-4.3438239 -4.4395418 -4.5310183 -4.5943093 -4.5965676 -4.5539131 -4.4871807 -4.4140115 -4.4167805 -4.4965787 -4.5755372 -4.6067405 -4.5968566 -4.585011 -4.5905557]]...]
INFO - root - 2017-12-07 18:13:38.700428: step 37910, loss = 21.88, batch loss = 21.80 (8.6 examples/sec; 0.925 sec/batch; 75h:41m:01s remains)
INFO - root - 2017-12-07 18:13:48.126527: step 37920, loss = 21.52, batch loss = 21.44 (7.9 examples/sec; 1.016 sec/batch; 83h:08m:32s remains)
INFO - root - 2017-12-07 18:13:57.459444: step 37930, loss = 21.74, batch loss = 21.66 (8.2 examples/sec; 0.972 sec/batch; 79h:29m:38s remains)
INFO - root - 2017-12-07 18:14:06.801331: step 37940, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.913 sec/batch; 74h:43m:21s remains)
INFO - root - 2017-12-07 18:14:16.156956: step 37950, loss = 21.81, batch loss = 21.73 (8.6 examples/sec; 0.926 sec/batch; 75h:46m:30s remains)
INFO - root - 2017-12-07 18:14:25.638481: step 37960, loss = 21.56, batch loss = 21.48 (8.2 examples/sec; 0.970 sec/batch; 79h:21m:48s remains)
INFO - root - 2017-12-07 18:14:34.913695: step 37970, loss = 21.64, batch loss = 21.56 (8.2 examples/sec; 0.970 sec/batch; 79h:23m:50s remains)
INFO - root - 2017-12-07 18:14:44.267990: step 37980, loss = 21.43, batch loss = 21.35 (8.8 examples/sec; 0.908 sec/batch; 74h:15m:37s remains)
INFO - root - 2017-12-07 18:14:53.523832: step 37990, loss = 21.63, batch loss = 21.55 (8.8 examples/sec; 0.908 sec/batch; 74h:16m:51s remains)
INFO - root - 2017-12-07 18:15:03.000338: step 38000, loss = 21.64, batch loss = 21.56 (8.5 examples/sec; 0.938 sec/batch; 76h:44m:03s remains)
2017-12-07 18:15:04.022068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4728994 -4.4708242 -4.4339314 -4.3694515 -4.2958846 -4.2669744 -4.30855 -4.3704762 -4.4160695 -4.4281216 -4.3787746 -4.2996469 -4.2686443 -4.294044 -4.3084688][-4.4485822 -4.4236112 -4.3570132 -4.274024 -4.203186 -4.1898713 -4.2402658 -4.2964296 -4.3333273 -4.3573723 -4.3410482 -4.287672 -4.2468467 -4.2315207 -4.2064757][-4.4602675 -4.4256845 -4.3461585 -4.2580981 -4.1921277 -4.182786 -4.2203641 -4.2553411 -4.2780638 -4.3144612 -4.3405094 -4.3287621 -4.2853532 -4.2273269 -4.1638312][-4.4789119 -4.4532847 -4.3836451 -4.3024807 -4.2353878 -4.2105212 -4.2216964 -4.238564 -4.2589979 -4.312614 -4.3752432 -4.3978329 -4.3525548 -4.2639694 -4.1802688][-4.455965 -4.4394503 -4.3874631 -4.3058271 -4.2158766 -4.1459928 -4.12801 -4.159224 -4.219934 -4.3089995 -4.3893123 -4.4117265 -4.3384271 -4.2227015 -4.1420159][-4.3736486 -4.3553519 -4.3127007 -4.2186365 -4.0846133 -3.9545279 -3.9177396 -3.9960749 -4.134634 -4.2804937 -4.3688936 -4.3648915 -4.2453442 -4.105763 -4.0436535][-4.2905436 -4.2678843 -4.2336121 -4.1293015 -3.9530461 -3.7668893 -3.71208 -3.8293288 -4.0306597 -4.2241459 -4.3248444 -4.3113351 -4.1823115 -4.0480995 -4.0147028][-4.261445 -4.252665 -4.239429 -4.1424346 -3.9486945 -3.7287233 -3.6492219 -3.75904 -3.9614918 -4.1554103 -4.2579432 -4.2574568 -4.1620154 -4.0621958 -4.0491219][-4.304944 -4.3105736 -4.3156748 -4.2357483 -4.0528045 -3.8341308 -3.7356877 -3.8087163 -3.9702425 -4.1297407 -4.2219677 -4.2321143 -4.1720695 -4.1062732 -4.0980368][-4.4061565 -4.4108896 -4.4185309 -4.3622522 -4.2180119 -4.0316267 -3.9246383 -3.950314 -4.0512524 -4.1639948 -4.2388258 -4.2508187 -4.2110457 -4.1642046 -4.1480479][-4.5077214 -4.50809 -4.5165663 -4.4926343 -4.4079504 -4.2781973 -4.1854949 -4.1769519 -4.2211146 -4.2855673 -4.340219 -4.350112 -4.3192778 -4.2686739 -4.2146][-4.5608063 -4.5562043 -4.5626144 -4.5596318 -4.5200281 -4.4466834 -4.3911519 -4.3832178 -4.40123 -4.4356308 -4.4710422 -4.4770384 -4.4503293 -4.3940439 -4.3142705][-4.5989909 -4.5879736 -4.58213 -4.5733805 -4.545629 -4.509459 -4.4930835 -4.5060215 -4.524971 -4.5446248 -4.5616546 -4.5608611 -4.5367737 -4.4836631 -4.4037523][-4.6308532 -4.6124368 -4.598393 -4.5874186 -4.5718226 -4.56727 -4.5865536 -4.6245713 -4.6543059 -4.6668091 -4.6610289 -4.642334 -4.6026893 -4.533268 -4.442596][-4.5927157 -4.5596046 -4.5455742 -4.5452027 -4.5480866 -4.5677562 -4.60831 -4.6653314 -4.7136278 -4.7302442 -4.7101946 -4.6787615 -4.6239519 -4.5309553 -4.4196863]]...]
INFO - root - 2017-12-07 18:15:13.404942: step 38010, loss = 20.95, batch loss = 20.87 (9.0 examples/sec; 0.891 sec/batch; 72h:54m:35s remains)
INFO - root - 2017-12-07 18:15:22.801518: step 38020, loss = 20.80, batch loss = 20.72 (8.6 examples/sec; 0.928 sec/batch; 75h:56m:41s remains)
INFO - root - 2017-12-07 18:15:32.272879: step 38030, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.929 sec/batch; 76h:01m:02s remains)
INFO - root - 2017-12-07 18:15:41.643797: step 38040, loss = 21.77, batch loss = 21.69 (8.0 examples/sec; 0.994 sec/batch; 81h:19m:50s remains)
INFO - root - 2017-12-07 18:15:51.111069: step 38050, loss = 21.23, batch loss = 21.15 (7.6 examples/sec; 1.059 sec/batch; 86h:37m:13s remains)
INFO - root - 2017-12-07 18:16:00.476931: step 38060, loss = 21.22, batch loss = 21.14 (8.4 examples/sec; 0.948 sec/batch; 77h:30m:29s remains)
INFO - root - 2017-12-07 18:16:09.869876: step 38070, loss = 21.42, batch loss = 21.33 (8.4 examples/sec; 0.954 sec/batch; 78h:02m:48s remains)
INFO - root - 2017-12-07 18:16:19.370962: step 38080, loss = 21.56, batch loss = 21.47 (8.2 examples/sec; 0.971 sec/batch; 79h:24m:25s remains)
INFO - root - 2017-12-07 18:16:28.688912: step 38090, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.960 sec/batch; 78h:29m:10s remains)
INFO - root - 2017-12-07 18:16:38.125209: step 38100, loss = 21.06, batch loss = 20.98 (10.0 examples/sec; 0.800 sec/batch; 65h:23m:51s remains)
2017-12-07 18:16:39.039037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.372509 -4.3939004 -4.4885888 -4.6382017 -4.7356081 -4.7132454 -4.647007 -4.6330118 -4.6073604 -4.5176983 -4.3839283 -4.3454752 -4.4517379 -4.5518203 -4.584908][-4.3958626 -4.4528389 -4.5688939 -4.6953306 -4.742816 -4.675065 -4.5768542 -4.5522513 -4.5406017 -4.4695935 -4.3392563 -4.2944708 -4.4057817 -4.516376 -4.5594196][-4.4384966 -4.5400066 -4.6646051 -4.7377872 -4.7108893 -4.5882449 -4.4657288 -4.4488821 -4.4746313 -4.4417057 -4.3333869 -4.2893677 -4.387351 -4.5012622 -4.5702772][-4.4928842 -4.624187 -4.7241468 -4.7190127 -4.6065879 -4.42427 -4.2779732 -4.279501 -4.3621917 -4.3894606 -4.3275251 -4.29635 -4.3754888 -4.491188 -4.5944228][-4.5632644 -4.701663 -4.7530184 -4.6524997 -4.4527054 -4.2122974 -4.0378485 -4.0588236 -4.2089558 -4.3146715 -4.322958 -4.3211546 -4.3892579 -4.5028133 -4.6224556][-4.6100364 -4.742969 -4.740025 -4.55182 -4.2743917 -3.9809103 -3.7786593 -3.8233974 -4.0575953 -4.2656403 -4.3588557 -4.3952036 -4.4595509 -4.55748 -4.6524477][-4.6010857 -4.7288551 -4.6869388 -4.4367628 -4.1036477 -3.77358 -3.5598025 -3.6319315 -3.9403009 -4.2345514 -4.3948636 -4.4644642 -4.531467 -4.61026 -4.6606393][-4.5847807 -4.7213635 -4.6766043 -4.4054661 -4.0402508 -3.6905525 -3.478301 -3.564796 -3.9016132 -4.2332578 -4.424314 -4.5098128 -4.5774236 -4.6384983 -4.6533804][-4.5986462 -4.7565956 -4.7462993 -4.506454 -4.1547933 -3.8165088 -3.6122503 -3.6852155 -3.9935851 -4.302629 -4.4832969 -4.568017 -4.6298523 -4.6687222 -4.6574936][-4.6175494 -4.7862468 -4.8187065 -4.6455646 -4.3582926 -4.0709634 -3.8884547 -3.9348879 -4.1786761 -4.4175243 -4.548532 -4.613831 -4.6620893 -4.6806755 -4.6559458][-4.6191974 -4.7738996 -4.8302937 -4.7282634 -4.5329752 -4.3257637 -4.1811743 -4.2068172 -4.3784246 -4.52944 -4.5973129 -4.6332283 -4.6598783 -4.65957 -4.6291995][-4.5785685 -4.7051983 -4.7695308 -4.7310023 -4.63121 -4.5156507 -4.4254866 -4.4439631 -4.549459 -4.6178079 -4.62544 -4.6229029 -4.6175528 -4.5974097 -4.5617661][-4.4886827 -4.5844388 -4.6478176 -4.65404 -4.6273761 -4.5883212 -4.5493631 -4.5613017 -4.61133 -4.6228356 -4.5933204 -4.559423 -4.5245461 -4.4873648 -4.4501724][-4.3762527 -4.4416723 -4.4949913 -4.5222721 -4.5313816 -4.5313406 -4.5190568 -4.5191131 -4.53073 -4.5181861 -4.4824233 -4.4413834 -4.3983569 -4.3607068 -4.332396][-4.2826052 -4.3189163 -4.3540912 -4.3795571 -4.3943982 -4.4020739 -4.3971434 -4.3898945 -4.3857722 -4.3712883 -4.3453927 -4.3160815 -4.286428 -4.2636452 -4.2496061]]...]
INFO - root - 2017-12-07 18:16:48.617245: step 38110, loss = 21.73, batch loss = 21.64 (7.9 examples/sec; 1.012 sec/batch; 82h:45m:33s remains)
INFO - root - 2017-12-07 18:16:57.987775: step 38120, loss = 21.49, batch loss = 21.41 (8.6 examples/sec; 0.932 sec/batch; 76h:11m:34s remains)
INFO - root - 2017-12-07 18:17:07.212331: step 38130, loss = 21.34, batch loss = 21.26 (8.5 examples/sec; 0.944 sec/batch; 77h:12m:47s remains)
INFO - root - 2017-12-07 18:17:16.550356: step 38140, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.979 sec/batch; 80h:02m:53s remains)
INFO - root - 2017-12-07 18:17:25.975225: step 38150, loss = 21.59, batch loss = 21.51 (8.6 examples/sec; 0.935 sec/batch; 76h:25m:37s remains)
INFO - root - 2017-12-07 18:17:35.356354: step 38160, loss = 21.40, batch loss = 21.31 (8.3 examples/sec; 0.965 sec/batch; 78h:53m:51s remains)
INFO - root - 2017-12-07 18:17:44.753011: step 38170, loss = 22.12, batch loss = 22.04 (9.1 examples/sec; 0.883 sec/batch; 72h:09m:07s remains)
INFO - root - 2017-12-07 18:17:54.112184: step 38180, loss = 21.49, batch loss = 21.41 (9.3 examples/sec; 0.861 sec/batch; 70h:23m:38s remains)
INFO - root - 2017-12-07 18:18:03.489772: step 38190, loss = 21.77, batch loss = 21.68 (8.9 examples/sec; 0.903 sec/batch; 73h:50m:33s remains)
INFO - root - 2017-12-07 18:18:12.914884: step 38200, loss = 21.61, batch loss = 21.53 (8.5 examples/sec; 0.944 sec/batch; 77h:11m:46s remains)
2017-12-07 18:18:13.881581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.448163 -4.4713345 -4.4781904 -4.4744959 -4.4748135 -4.4690976 -4.4610562 -4.4708509 -4.4879026 -4.5030565 -4.5199394 -4.5353508 -4.5519595 -4.56138 -4.5489984][-4.5372009 -4.5855713 -4.6068926 -4.6063976 -4.6020746 -4.5782595 -4.5502334 -4.5687571 -4.6123013 -4.6457872 -4.6667433 -4.673923 -4.6801109 -4.6805253 -4.6557322][-4.6636815 -4.7433629 -4.7762604 -4.7616477 -4.7304683 -4.6697321 -4.610446 -4.6395783 -4.7267675 -4.787044 -4.8031311 -4.7854118 -4.7680669 -4.7569938 -4.7247682][-4.7895417 -4.8846965 -4.9031467 -4.8444915 -4.760088 -4.6510091 -4.5562248 -4.5972457 -4.7365551 -4.8248906 -4.8300323 -4.7880192 -4.7556481 -4.7430944 -4.7160497][-4.8693762 -4.9511151 -4.9305968 -4.8120985 -4.6635847 -4.50131 -4.3613272 -4.3963027 -4.5818467 -4.704792 -4.7135868 -4.6785965 -4.6575618 -4.6516728 -4.6323519][-4.85531 -4.8894567 -4.8143907 -4.64231 -4.4423838 -4.2342563 -4.0416379 -4.0497742 -4.28085 -4.4566278 -4.5003848 -4.5197883 -4.5409875 -4.5386715 -4.5178204][-4.7523012 -4.7154584 -4.5848927 -4.3822112 -4.1679373 -3.9464533 -3.7093265 -3.6681652 -3.9256115 -4.1590652 -4.2622833 -4.3819804 -4.488903 -4.5092397 -4.4758935][-4.6435885 -4.5531478 -4.3877058 -4.1769133 -3.9722908 -3.7676876 -3.5099764 -3.4149423 -3.6676743 -3.9360731 -4.0995388 -4.322403 -4.5221138 -4.580297 -4.5330915][-4.6249337 -4.535315 -4.3779325 -4.1822786 -3.9986217 -3.832963 -3.603544 -3.4907041 -3.697176 -3.9391088 -4.1070919 -4.3589 -4.598032 -4.6925068 -4.647068][-4.70092 -4.6582804 -4.5493784 -4.3870163 -4.2244062 -4.0985456 -3.9237156 -3.8252947 -3.963757 -4.1269517 -4.24261 -4.4408135 -4.6463232 -4.757843 -4.739861][-4.7822866 -4.7962112 -4.7458277 -4.6261487 -4.4824781 -4.3821526 -4.2641768 -4.2000089 -4.2806077 -4.3605781 -4.4023614 -4.5049829 -4.6345987 -4.7386413 -4.7558751][-4.7897749 -4.8406696 -4.8394051 -4.7675 -4.6554136 -4.5813236 -4.5214448 -4.5049024 -4.5568337 -4.5769067 -4.5549664 -4.5726986 -4.6309147 -4.7136216 -4.7496786][-4.7215071 -4.7863245 -4.8126578 -4.781188 -4.7116094 -4.6661911 -4.6543155 -4.6800647 -4.7256632 -4.7180471 -4.6639004 -4.6308618 -4.6394181 -4.6921692 -4.7283764][-4.6239786 -4.6842527 -4.7170911 -4.707459 -4.6711097 -4.6478028 -4.6559267 -4.6920838 -4.7264748 -4.7136831 -4.6631627 -4.6189747 -4.6034455 -4.6271892 -4.6512656][-4.5325913 -4.5767241 -4.6061821 -4.6098018 -4.5983191 -4.589251 -4.5954056 -4.6147885 -4.6283846 -4.6157255 -4.5832496 -4.550684 -4.5330558 -4.5387607 -4.5503283]]...]
INFO - root - 2017-12-07 18:18:23.129169: step 38210, loss = 21.49, batch loss = 21.41 (9.6 examples/sec; 0.833 sec/batch; 68h:04m:35s remains)
INFO - root - 2017-12-07 18:18:32.408066: step 38220, loss = 20.73, batch loss = 20.65 (9.0 examples/sec; 0.891 sec/batch; 72h:49m:39s remains)
INFO - root - 2017-12-07 18:18:41.829125: step 38230, loss = 21.28, batch loss = 21.20 (8.7 examples/sec; 0.918 sec/batch; 75h:04m:46s remains)
INFO - root - 2017-12-07 18:18:51.245914: step 38240, loss = 21.25, batch loss = 21.17 (8.1 examples/sec; 0.994 sec/batch; 81h:13m:17s remains)
INFO - root - 2017-12-07 18:19:00.675805: step 38250, loss = 20.94, batch loss = 20.86 (7.8 examples/sec; 1.023 sec/batch; 83h:35m:36s remains)
INFO - root - 2017-12-07 18:19:10.178020: step 38260, loss = 21.27, batch loss = 21.19 (8.7 examples/sec; 0.918 sec/batch; 75h:02m:28s remains)
INFO - root - 2017-12-07 18:19:19.579429: step 38270, loss = 21.39, batch loss = 21.31 (8.7 examples/sec; 0.921 sec/batch; 75h:17m:46s remains)
INFO - root - 2017-12-07 18:19:29.051057: step 38280, loss = 21.38, batch loss = 21.30 (8.5 examples/sec; 0.940 sec/batch; 76h:49m:14s remains)
INFO - root - 2017-12-07 18:19:38.578612: step 38290, loss = 21.17, batch loss = 21.08 (8.3 examples/sec; 0.961 sec/batch; 78h:33m:27s remains)
INFO - root - 2017-12-07 18:19:48.050668: step 38300, loss = 21.39, batch loss = 21.30 (8.7 examples/sec; 0.921 sec/batch; 75h:17m:23s remains)
2017-12-07 18:19:48.999355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4807267 -4.4889288 -4.479032 -4.4527559 -4.4199333 -4.3945603 -4.3999972 -4.4319639 -4.4293361 -4.3531246 -4.2421451 -4.1737046 -4.1556692 -4.1647696 -4.1703768][-4.5406318 -4.5460148 -4.5286746 -4.4936452 -4.4582157 -4.4315214 -4.4267025 -4.4457397 -4.4425397 -4.367239 -4.2617517 -4.2070289 -4.1998448 -4.2069745 -4.1991234][-4.5791464 -4.5748591 -4.5439172 -4.4983835 -4.4640985 -4.4423933 -4.4257703 -4.415441 -4.3938146 -4.3193126 -4.2210655 -4.1827054 -4.1984591 -4.2272787 -4.2361908][-4.5591912 -4.5372553 -4.4898629 -4.4316807 -4.3901153 -4.3660879 -4.3370714 -4.3000851 -4.2694945 -4.216012 -4.1496029 -4.1423697 -4.1841464 -4.2322726 -4.2648149][-4.4770064 -4.4336419 -4.37234 -4.3006697 -4.2392945 -4.2021852 -4.1637859 -4.1218395 -4.11541 -4.1135006 -4.1009278 -4.1334882 -4.1980171 -4.2542272 -4.3010478][-4.3246479 -4.2737885 -4.217175 -4.1413121 -4.0558262 -3.99649 -3.9471512 -3.9128909 -3.9497864 -4.0122905 -4.059185 -4.137114 -4.2287536 -4.2948003 -4.3492537][-4.1289973 -4.0856009 -4.0562849 -3.9984984 -3.9038143 -3.8238401 -3.7579117 -3.7274346 -3.8055866 -3.9316006 -4.0354075 -4.155695 -4.2757158 -4.3596845 -4.4230113][-3.9720013 -3.9486198 -3.9572771 -3.9312375 -3.8438673 -3.7583809 -3.6868768 -3.6612463 -3.7659559 -3.9302583 -4.0602365 -4.1894751 -4.3136368 -4.4050322 -4.4759359][-3.9023566 -3.9199383 -3.9715016 -3.9812729 -3.9196727 -3.8482497 -3.7875655 -3.766968 -3.864434 -4.0167837 -4.1217852 -4.2087884 -4.2952495 -4.3720832 -4.448894][-3.9637845 -4.0265117 -4.1050119 -4.13069 -4.0900278 -4.0367851 -3.9973562 -3.9854181 -4.053628 -4.1547241 -4.2011442 -4.218359 -4.2442741 -4.2890978 -4.3655334][-4.0974364 -4.187768 -4.267652 -4.2849684 -4.2489324 -4.2122712 -4.1967316 -4.1977897 -4.2330256 -4.2673135 -4.2491136 -4.2074165 -4.1864705 -4.2043014 -4.2783666][-4.2004576 -4.3005834 -4.370254 -4.3721156 -4.3322191 -4.3000126 -4.2980556 -4.3090143 -4.3151307 -4.288516 -4.2184477 -4.1497869 -4.1236734 -4.147923 -4.2356429][-4.2790866 -4.3558946 -4.4058781 -4.3947721 -4.3456097 -4.3023472 -4.2937779 -4.3014188 -4.2914677 -4.2395811 -4.1588116 -4.1049323 -4.1103363 -4.1643348 -4.2670803][-4.3025551 -4.3384657 -4.3683319 -4.3611279 -4.3224711 -4.2766428 -4.2573633 -4.256927 -4.2444773 -4.2010236 -4.1417813 -4.117599 -4.1482224 -4.2128315 -4.3015862][-4.2924638 -4.2918448 -4.3082018 -4.3192086 -4.3063145 -4.26926 -4.2413836 -4.2340221 -4.2280579 -4.2072315 -4.17605 -4.1687922 -4.1940951 -4.2312717 -4.2730842]]...]
INFO - root - 2017-12-07 18:19:58.055071: step 38310, loss = 21.32, batch loss = 21.23 (8.6 examples/sec; 0.935 sec/batch; 76h:24m:47s remains)
INFO - root - 2017-12-07 18:20:07.292092: step 38320, loss = 21.68, batch loss = 21.59 (8.3 examples/sec; 0.964 sec/batch; 78h:44m:32s remains)
INFO - root - 2017-12-07 18:20:16.749709: step 38330, loss = 21.58, batch loss = 21.50 (8.7 examples/sec; 0.923 sec/batch; 75h:27m:45s remains)
INFO - root - 2017-12-07 18:20:26.237707: step 38340, loss = 21.73, batch loss = 21.64 (8.1 examples/sec; 0.985 sec/batch; 80h:28m:16s remains)
INFO - root - 2017-12-07 18:20:35.602757: step 38350, loss = 20.93, batch loss = 20.84 (8.6 examples/sec; 0.929 sec/batch; 75h:53m:48s remains)
INFO - root - 2017-12-07 18:20:45.144901: step 38360, loss = 21.61, batch loss = 21.52 (7.8 examples/sec; 1.020 sec/batch; 83h:18m:45s remains)
INFO - root - 2017-12-07 18:20:54.521690: step 38370, loss = 21.71, batch loss = 21.63 (8.3 examples/sec; 0.968 sec/batch; 79h:03m:14s remains)
INFO - root - 2017-12-07 18:21:03.973510: step 38380, loss = 21.92, batch loss = 21.84 (8.0 examples/sec; 0.994 sec/batch; 81h:12m:10s remains)
INFO - root - 2017-12-07 18:21:13.359336: step 38390, loss = 21.16, batch loss = 21.07 (8.3 examples/sec; 0.968 sec/batch; 79h:05m:43s remains)
INFO - root - 2017-12-07 18:21:22.831864: step 38400, loss = 21.47, batch loss = 21.38 (8.7 examples/sec; 0.921 sec/batch; 75h:12m:14s remains)
2017-12-07 18:21:23.869098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1855621 -4.2041168 -4.2513595 -4.3320951 -4.397572 -4.4619584 -4.5007749 -4.4696 -4.3987336 -4.3073049 -4.25718 -4.2984853 -4.3776641 -4.4544973 -4.4982963][-4.2423868 -4.2820616 -4.3363786 -4.4117494 -4.4648309 -4.5175505 -4.5515413 -4.5187621 -4.4396648 -4.3394942 -4.2889905 -4.3319387 -4.4097672 -4.485343 -4.5247836][-4.3245258 -4.3976784 -4.4592648 -4.5110269 -4.5229397 -4.5302644 -4.5395756 -4.5143461 -4.4541359 -4.3674736 -4.3244252 -4.3664489 -4.4388714 -4.5095525 -4.5420995][-4.3970027 -4.501646 -4.5602555 -4.5664167 -4.5128751 -4.4582481 -4.4319887 -4.4194255 -4.406744 -4.3689308 -4.3509917 -4.3939738 -4.4542475 -4.5093737 -4.5324259][-4.4611554 -4.57508 -4.6101718 -4.553926 -4.4290876 -4.3112493 -4.2389688 -4.2271724 -4.2729616 -4.3156109 -4.3525276 -4.4117417 -4.4672828 -4.504796 -4.5152388][-4.5182242 -4.6120715 -4.60222 -4.4807134 -4.2976403 -4.1378713 -4.0203424 -3.9867654 -4.0836315 -4.2190418 -4.3278003 -4.4150238 -4.4792504 -4.5120945 -4.5128036][-4.524435 -4.5847158 -4.53435 -4.37023 -4.1631374 -3.9954627 -3.8471048 -3.7745695 -3.894537 -4.1031446 -4.274251 -4.3922491 -4.4788475 -4.5286179 -4.5288253][-4.473742 -4.5010962 -4.4303107 -4.2571936 -4.0565557 -3.9064369 -3.7555003 -3.651814 -3.7620649 -3.9943049 -4.1925111 -4.3290019 -4.4402843 -4.5253987 -4.5423365][-4.4237761 -4.4350653 -4.365087 -4.1964884 -4.0012188 -3.8640857 -3.7361782 -3.639539 -3.7248149 -3.9349918 -4.1233425 -4.2570763 -4.3753171 -4.4870687 -4.5284033][-4.4210525 -4.4430671 -4.3940244 -4.2463551 -4.0581656 -3.9136863 -3.7933829 -3.7062793 -3.763577 -3.9408102 -4.1099582 -4.2324858 -4.3404431 -4.4490395 -4.4968219][-4.4801126 -4.5254216 -4.500617 -4.384727 -4.2204981 -4.0710969 -3.941745 -3.8463018 -3.8672731 -4.0079751 -4.1562061 -4.2682576 -4.361805 -4.447042 -4.4762936][-4.5513525 -4.6124563 -4.6012764 -4.524622 -4.4112549 -4.2877846 -4.1589265 -4.0486865 -4.0311379 -4.130743 -4.2514753 -4.3477826 -4.4254484 -4.4827256 -4.48345][-4.5907764 -4.6535106 -4.648541 -4.6127272 -4.5662932 -4.4997206 -4.39408 -4.2755809 -4.2251091 -4.2827239 -4.3753128 -4.454463 -4.514852 -4.5473938 -4.5199347][-4.5990458 -4.6525311 -4.6489272 -4.6384945 -4.6417203 -4.6341147 -4.5677862 -4.4577212 -4.3901 -4.4168606 -4.4963274 -4.577157 -4.633956 -4.6478214 -4.585578][-4.5829453 -4.6142259 -4.6062875 -4.6075888 -4.6363106 -4.6688375 -4.641593 -4.5558162 -4.4894214 -4.4997859 -4.5782089 -4.6775661 -4.7439079 -4.7458992 -4.6491809]]...]
INFO - root - 2017-12-07 18:21:33.080654: step 38410, loss = 21.80, batch loss = 21.71 (8.2 examples/sec; 0.977 sec/batch; 79h:48m:08s remains)
INFO - root - 2017-12-07 18:21:42.323159: step 38420, loss = 21.61, batch loss = 21.52 (8.5 examples/sec; 0.937 sec/batch; 76h:33m:24s remains)
INFO - root - 2017-12-07 18:21:51.619245: step 38430, loss = 21.27, batch loss = 21.19 (8.7 examples/sec; 0.923 sec/batch; 75h:23m:08s remains)
INFO - root - 2017-12-07 18:22:00.906105: step 38440, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.946 sec/batch; 77h:17m:45s remains)
INFO - root - 2017-12-07 18:22:10.361611: step 38450, loss = 21.75, batch loss = 21.67 (8.2 examples/sec; 0.975 sec/batch; 79h:39m:41s remains)
INFO - root - 2017-12-07 18:22:19.734947: step 38460, loss = 21.60, batch loss = 21.51 (9.0 examples/sec; 0.889 sec/batch; 72h:38m:06s remains)
INFO - root - 2017-12-07 18:22:29.261276: step 38470, loss = 21.44, batch loss = 21.36 (8.8 examples/sec; 0.914 sec/batch; 74h:39m:06s remains)
INFO - root - 2017-12-07 18:22:38.601287: step 38480, loss = 21.56, batch loss = 21.48 (8.6 examples/sec; 0.936 sec/batch; 76h:24m:29s remains)
INFO - root - 2017-12-07 18:22:48.041044: step 38490, loss = 21.30, batch loss = 21.22 (8.0 examples/sec; 1.004 sec/batch; 81h:58m:09s remains)
INFO - root - 2017-12-07 18:22:57.452634: step 38500, loss = 21.73, batch loss = 21.65 (8.0 examples/sec; 1.000 sec/batch; 81h:41m:12s remains)
2017-12-07 18:22:58.361983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3722253 -4.3756404 -4.377841 -4.3999019 -4.4374943 -4.4657764 -4.4965739 -4.5199971 -4.54011 -4.5434 -4.4922957 -4.4137664 -4.3472357 -4.3019185 -4.2890172][-4.3170319 -4.3046927 -4.31069 -4.3561807 -4.4144149 -4.4425731 -4.4613438 -4.4776244 -4.4979553 -4.5143323 -4.4898024 -4.4401488 -4.3875961 -4.3413172 -4.3242197][-4.2912965 -4.2537227 -4.2567577 -4.3183584 -4.3883848 -4.4119191 -4.4163 -4.4220943 -4.4366727 -4.45467 -4.4497838 -4.4309611 -4.4008875 -4.3652859 -4.3521314][-4.2767439 -4.216538 -4.2079549 -4.2675757 -4.335566 -4.3496928 -4.3339534 -4.3216052 -4.330368 -4.3494768 -4.3587418 -4.3764148 -4.3884387 -4.3830862 -4.3840442][-4.2295127 -4.1598768 -4.1423306 -4.1907635 -4.2526231 -4.2587523 -4.2165823 -4.1719785 -4.1739888 -4.2079 -4.2420897 -4.3029742 -4.37034 -4.4130917 -4.4407177][-4.1227388 -4.0589867 -4.0501013 -4.0983195 -4.1647162 -4.1733742 -4.1041508 -4.0091925 -3.9889417 -4.0460982 -4.1216125 -4.2261438 -4.34187 -4.4293113 -4.4879937][-3.9755576 -3.9318874 -3.9478748 -4.0098433 -4.0900626 -4.1119552 -4.0308852 -3.8873198 -3.8248186 -3.8912141 -4.0042291 -4.1383443 -4.2801709 -4.3957 -4.4825106][-3.8900495 -3.8699026 -3.9108009 -3.9857318 -4.0738435 -4.1111078 -4.0426474 -3.8829904 -3.7786283 -3.824805 -3.9455938 -4.0803618 -4.2170839 -4.3386378 -4.440825][-3.9031096 -3.9100466 -3.9675364 -4.0455709 -4.1314039 -4.1808519 -4.1418686 -4.0040727 -3.8822241 -3.8909693 -3.9827018 -4.0868192 -4.1947942 -4.3038969 -4.4047685][-3.9922302 -4.0225825 -4.085247 -4.1536994 -4.227633 -4.2811556 -4.2672019 -4.1626868 -4.0422678 -4.0168543 -4.0694523 -4.1374607 -4.2177534 -4.3121047 -4.4041328][-4.1183615 -4.1640863 -4.2258396 -4.2783875 -4.3354154 -4.38278 -4.3784547 -4.3004341 -4.1912479 -4.144021 -4.1667743 -4.2123218 -4.276885 -4.3581309 -4.4372058][-4.2386484 -4.2900858 -4.3464093 -4.3831158 -4.4225221 -4.4573 -4.452075 -4.3951783 -4.306181 -4.2530856 -4.2570987 -4.2894073 -4.3462176 -4.415514 -4.479363][-4.3592319 -4.4010983 -4.43784 -4.4512477 -4.4671545 -4.484365 -4.4779162 -4.443902 -4.3845267 -4.3353648 -4.32225 -4.341373 -4.3933573 -4.4561272 -4.5080581][-4.4668851 -4.4908738 -4.5018396 -4.4920211 -4.4838238 -4.4827027 -4.4755177 -4.4624691 -4.4340544 -4.392344 -4.3608603 -4.362988 -4.408371 -4.4687438 -4.5137534][-4.5292406 -4.5384703 -4.5330677 -4.5102911 -4.4851189 -4.4684124 -4.4596047 -4.4580441 -4.4509077 -4.4210672 -4.3794847 -4.3677211 -4.4061174 -4.4667754 -4.5113587]]...]
INFO - root - 2017-12-07 18:23:07.699684: step 38510, loss = 21.45, batch loss = 21.37 (9.2 examples/sec; 0.866 sec/batch; 70h:44m:07s remains)
INFO - root - 2017-12-07 18:23:17.055741: step 38520, loss = 21.35, batch loss = 21.26 (9.0 examples/sec; 0.887 sec/batch; 72h:24m:49s remains)
INFO - root - 2017-12-07 18:23:26.386462: step 38530, loss = 21.87, batch loss = 21.79 (9.1 examples/sec; 0.882 sec/batch; 72h:00m:30s remains)
INFO - root - 2017-12-07 18:23:35.728410: step 38540, loss = 21.61, batch loss = 21.52 (9.1 examples/sec; 0.878 sec/batch; 71h:41m:03s remains)
INFO - root - 2017-12-07 18:23:45.217334: step 38550, loss = 21.20, batch loss = 21.11 (8.4 examples/sec; 0.949 sec/batch; 77h:30m:43s remains)
INFO - root - 2017-12-07 18:23:54.647822: step 38560, loss = 21.29, batch loss = 21.20 (8.2 examples/sec; 0.981 sec/batch; 80h:05m:55s remains)
INFO - root - 2017-12-07 18:24:03.995950: step 38570, loss = 21.58, batch loss = 21.49 (8.7 examples/sec; 0.920 sec/batch; 75h:06m:34s remains)
INFO - root - 2017-12-07 18:24:13.372148: step 38580, loss = 21.06, batch loss = 20.98 (8.9 examples/sec; 0.904 sec/batch; 73h:47m:34s remains)
INFO - root - 2017-12-07 18:24:22.643027: step 38590, loss = 21.30, batch loss = 21.22 (8.5 examples/sec; 0.945 sec/batch; 77h:08m:54s remains)
INFO - root - 2017-12-07 18:24:32.082488: step 38600, loss = 21.72, batch loss = 21.64 (8.9 examples/sec; 0.897 sec/batch; 73h:12m:31s remains)
2017-12-07 18:24:33.021721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5584917 -4.604001 -4.5783825 -4.4714589 -4.342361 -4.228785 -4.159236 -4.14658 -4.20084 -4.2575312 -4.3040714 -4.3652244 -4.4143419 -4.4417462 -4.4750624][-4.6093554 -4.6620111 -4.6156044 -4.4575849 -4.2754464 -4.1378331 -4.077776 -4.0947962 -4.17864 -4.247766 -4.2837892 -4.3213983 -4.3610935 -4.3924193 -4.4333377][-4.6623731 -4.7138906 -4.6425056 -4.4469881 -4.2425256 -4.1099877 -4.0707016 -4.1059947 -4.2007585 -4.2744737 -4.2965927 -4.3025217 -4.3222733 -4.3491783 -4.3864841][-4.653513 -4.7014575 -4.6209812 -4.4267416 -4.243659 -4.1451988 -4.1276522 -4.1622252 -4.2449179 -4.3091669 -4.3163486 -4.2894711 -4.2787161 -4.2870107 -4.3073492][-4.5670214 -4.6180496 -4.5639553 -4.41751 -4.2789259 -4.1976838 -4.1659818 -4.1784229 -4.2486777 -4.318572 -4.3320317 -4.2912526 -4.2460594 -4.2140031 -4.1966839][-4.4515738 -4.5150728 -4.5081763 -4.4207253 -4.3067918 -4.195569 -4.1081872 -4.082109 -4.1461029 -4.2451563 -4.3011909 -4.2884588 -4.2286706 -4.1574988 -4.1022873][-4.3550358 -4.4290466 -4.4568586 -4.4030914 -4.282927 -4.12184 -3.9714906 -3.9055922 -3.967993 -4.1080818 -4.2278953 -4.2642207 -4.2038569 -4.1104155 -4.0477309][-4.2835822 -4.3689623 -4.4153452 -4.3712058 -4.2352881 -4.0335035 -3.8359118 -3.7390139 -3.8103194 -3.9957063 -4.1723523 -4.2451782 -4.1823816 -4.08349 -4.0437527][-4.2622242 -4.3677783 -4.4198465 -4.3710856 -4.2232 -4.0091524 -3.8018641 -3.7010555 -3.7861979 -3.9850335 -4.1663146 -4.2357235 -4.1710067 -4.0918694 -4.0991144][-4.3195596 -4.4387455 -4.483429 -4.4229307 -4.2791657 -4.0923324 -3.9212365 -3.8419456 -3.9136915 -4.065887 -4.1964383 -4.2453132 -4.1971622 -4.1558065 -4.2105975][-4.4240556 -4.536232 -4.5684719 -4.50637 -4.3840203 -4.2385578 -4.1119928 -4.049139 -4.0868139 -4.1734405 -4.2531877 -4.2975864 -4.2862458 -4.2866569 -4.3651881][-4.4993434 -4.58395 -4.6128378 -4.5718718 -4.48469 -4.37526 -4.2808661 -4.2312722 -4.2460709 -4.2913928 -4.3469329 -4.40346 -4.4356084 -4.4687147 -4.5423827][-4.4941816 -4.5561337 -4.596127 -4.5947127 -4.5535827 -4.4798846 -4.4115763 -4.3764 -4.3855858 -4.4179645 -4.468864 -4.5359941 -4.5937996 -4.6403627 -4.6869578][-4.4295144 -4.4816694 -4.5389919 -4.5787039 -4.5829048 -4.5455194 -4.5004592 -4.4777756 -4.4862623 -4.5168881 -4.5667176 -4.6293759 -4.6839185 -4.7208824 -4.7325878][-4.3593793 -4.4062295 -4.4697704 -4.5307512 -4.565475 -4.5591917 -4.5345688 -4.5175886 -4.5201521 -4.5435548 -4.5835814 -4.6228638 -4.6497207 -4.663837 -4.6574779]]...]
INFO - root - 2017-12-07 18:24:42.361347: step 38610, loss = 21.99, batch loss = 21.90 (9.1 examples/sec; 0.884 sec/batch; 72h:09m:46s remains)
INFO - root - 2017-12-07 18:24:51.496922: step 38620, loss = 20.86, batch loss = 20.78 (8.8 examples/sec; 0.914 sec/batch; 74h:37m:40s remains)
INFO - root - 2017-12-07 18:25:00.937779: step 38630, loss = 21.26, batch loss = 21.17 (8.3 examples/sec; 0.961 sec/batch; 78h:26m:44s remains)
INFO - root - 2017-12-07 18:25:10.364267: step 38640, loss = 21.68, batch loss = 21.59 (8.4 examples/sec; 0.958 sec/batch; 78h:11m:12s remains)
INFO - root - 2017-12-07 18:25:19.911828: step 38650, loss = 21.60, batch loss = 21.52 (8.3 examples/sec; 0.966 sec/batch; 78h:50m:40s remains)
INFO - root - 2017-12-07 18:25:29.400565: step 38660, loss = 21.22, batch loss = 21.14 (8.7 examples/sec; 0.922 sec/batch; 75h:16m:36s remains)
INFO - root - 2017-12-07 18:25:38.752584: step 38670, loss = 21.48, batch loss = 21.40 (9.3 examples/sec; 0.858 sec/batch; 70h:03m:36s remains)
INFO - root - 2017-12-07 18:25:48.127694: step 38680, loss = 21.25, batch loss = 21.16 (8.6 examples/sec; 0.935 sec/batch; 76h:19m:24s remains)
INFO - root - 2017-12-07 18:25:57.603490: step 38690, loss = 21.41, batch loss = 21.33 (8.4 examples/sec; 0.955 sec/batch; 77h:57m:53s remains)
INFO - root - 2017-12-07 18:26:07.024128: step 38700, loss = 21.13, batch loss = 21.04 (8.3 examples/sec; 0.963 sec/batch; 78h:37m:45s remains)
2017-12-07 18:26:08.052214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.13575 -4.1483774 -4.2014265 -4.2922854 -4.39864 -4.4670386 -4.4642625 -4.4241071 -4.392849 -4.3885751 -4.4020405 -4.4222145 -4.4440479 -4.4646153 -4.4583635][-4.1082311 -4.1355672 -4.1991038 -4.2929406 -4.3981285 -4.4555426 -4.4379568 -4.3773346 -4.3254056 -4.3167887 -4.3409152 -4.3756742 -4.412478 -4.4353824 -4.4212174][-4.1345787 -4.1752052 -4.24458 -4.3363695 -4.4300323 -4.4637275 -4.4179754 -4.3300066 -4.2600183 -4.2552195 -4.2967463 -4.3451686 -4.3905025 -4.4160609 -4.4027634][-4.2353177 -4.2827973 -4.3496571 -4.429213 -4.4956861 -4.4898829 -4.40617 -4.2932959 -4.2197876 -4.230763 -4.290514 -4.3473296 -4.3947024 -4.424346 -4.4206829][-4.3419 -4.3899684 -4.447113 -4.5051756 -4.5337296 -4.4814906 -4.35729 -4.2205024 -4.1491308 -4.1788225 -4.2571363 -4.326128 -4.3842454 -4.4308219 -4.4520364][-4.4081678 -4.4555655 -4.4979782 -4.5251031 -4.506299 -4.4007626 -4.230669 -4.0681043 -4.0061626 -4.0675554 -4.1776538 -4.2758842 -4.3603849 -4.4342418 -4.4844842][-4.4295387 -4.4810724 -4.5066891 -4.4967628 -4.4280453 -4.2679057 -4.0529437 -3.8733087 -3.8313031 -3.9369264 -4.0933013 -4.2335982 -4.3473539 -4.4420419 -4.5079885][-4.4073458 -4.4748869 -4.4954104 -4.460814 -4.3566041 -4.1622415 -3.9218976 -3.7415428 -3.7232327 -3.8645859 -4.0586219 -4.230093 -4.3612056 -4.4617424 -4.5236678][-4.3754964 -4.4608569 -4.4896712 -4.4503021 -4.3373022 -4.1420426 -3.9099593 -3.7527096 -3.7584813 -3.9104455 -4.1076293 -4.2764297 -4.4030323 -4.4942584 -4.5365543][-4.3744974 -4.4625621 -4.5013795 -4.4737139 -4.3772469 -4.2108159 -4.0168142 -3.8984897 -3.923542 -4.0604062 -4.2246208 -4.3601031 -4.4602771 -4.5279574 -4.5401168][-4.4019508 -4.4767814 -4.514811 -4.4991961 -4.4284492 -4.3057027 -4.1666975 -4.0950174 -4.1358786 -4.2493019 -4.367311 -4.4564557 -4.5185275 -4.5512238 -4.5279026][-4.4413404 -4.4926467 -4.5200186 -4.5107417 -4.4655638 -4.389926 -4.3090487 -4.2811079 -4.3291292 -4.4125366 -4.4836984 -4.5303698 -4.5548029 -4.5501332 -4.4949465][-4.4647269 -4.4986982 -4.5192595 -4.5162096 -4.4901977 -4.4489889 -4.4111805 -4.4071693 -4.4450407 -4.4944491 -4.5296841 -4.5474429 -4.5451107 -4.5132804 -4.4418988][-4.4427242 -4.4714236 -4.49752 -4.509439 -4.5020046 -4.4820642 -4.463141 -4.4600496 -4.4770036 -4.49763 -4.5108876 -4.512598 -4.4951286 -4.4516091 -4.3834686][-4.3910971 -4.4135623 -4.4466386 -4.4740109 -4.4832249 -4.4781594 -4.4686952 -4.4635119 -4.4659796 -4.4691367 -4.4684439 -4.4584975 -4.4332318 -4.3912745 -4.3398967]]...]
INFO - root - 2017-12-07 18:26:17.522874: step 38710, loss = 21.54, batch loss = 21.46 (8.2 examples/sec; 0.980 sec/batch; 80h:00m:44s remains)
INFO - root - 2017-12-07 18:26:26.992383: step 38720, loss = 22.07, batch loss = 21.99 (8.6 examples/sec; 0.928 sec/batch; 75h:42m:18s remains)
INFO - root - 2017-12-07 18:26:36.303433: step 38730, loss = 21.93, batch loss = 21.85 (8.9 examples/sec; 0.902 sec/batch; 73h:35m:20s remains)
INFO - root - 2017-12-07 18:26:45.717273: step 38740, loss = 21.26, batch loss = 21.18 (9.2 examples/sec; 0.871 sec/batch; 71h:06m:40s remains)
INFO - root - 2017-12-07 18:26:55.111236: step 38750, loss = 21.58, batch loss = 21.50 (8.8 examples/sec; 0.904 sec/batch; 73h:48m:10s remains)
INFO - root - 2017-12-07 18:27:04.589669: step 38760, loss = 21.44, batch loss = 21.36 (8.0 examples/sec; 0.995 sec/batch; 81h:12m:54s remains)
INFO - root - 2017-12-07 18:27:13.893430: step 38770, loss = 21.59, batch loss = 21.51 (8.0 examples/sec; 1.006 sec/batch; 82h:04m:03s remains)
INFO - root - 2017-12-07 18:27:23.421116: step 38780, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.962 sec/batch; 78h:30m:54s remains)
INFO - root - 2017-12-07 18:27:32.700203: step 38790, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.912 sec/batch; 74h:24m:24s remains)
INFO - root - 2017-12-07 18:27:42.172233: step 38800, loss = 21.52, batch loss = 21.43 (8.3 examples/sec; 0.960 sec/batch; 78h:21m:05s remains)
2017-12-07 18:27:43.124508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3742528 -4.4151754 -4.428658 -4.3892436 -4.3109574 -4.2384048 -4.2122183 -4.2526789 -4.3260508 -4.36756 -4.3742428 -4.3746963 -4.3732772 -4.3528509 -4.2907815][-4.4272995 -4.49154 -4.51292 -4.4697609 -4.3827562 -4.3016996 -4.2673321 -4.2995296 -4.3631563 -4.398653 -4.4075036 -4.4134088 -4.41487 -4.3896241 -4.3191009][-4.47152 -4.551199 -4.5773306 -4.5371995 -4.4570136 -4.3832078 -4.3437414 -4.3551731 -4.3935986 -4.4203787 -4.4403963 -4.4627948 -4.4740319 -4.4505429 -4.3809266][-4.4761095 -4.5555544 -4.5820642 -4.5509944 -4.4888287 -4.4318514 -4.3895111 -4.3747559 -4.386313 -4.4142723 -4.4634404 -4.5210147 -4.5556097 -4.5446129 -4.4849143][-4.4281783 -4.4889221 -4.5040784 -4.4749002 -4.42369 -4.3726993 -4.3172612 -4.2737451 -4.2662625 -4.3094168 -4.4032097 -4.51032 -4.5794387 -4.5900517 -4.5450931][-4.3444896 -4.3735685 -4.3662996 -4.3257256 -4.266273 -4.1977096 -4.1124544 -4.0385613 -4.0225015 -4.0919247 -4.2358909 -4.3956146 -4.5018749 -4.5375328 -4.5110092][-4.2778311 -4.2800579 -4.2520795 -4.1949248 -4.1137867 -4.0094948 -3.8837595 -3.7798343 -3.7591434 -3.8515315 -4.0319796 -4.2275548 -4.3583136 -4.4093285 -4.3965478][-4.2589374 -4.2521334 -4.2168708 -4.1476 -4.0429873 -3.9036343 -3.7482924 -3.6313453 -3.6176155 -3.7278924 -3.9222028 -4.1221905 -4.24989 -4.2927151 -4.2737532][-4.274673 -4.2723994 -4.2410088 -4.16902 -4.05363 -3.9061763 -3.7588942 -3.666589 -3.6846185 -3.8115833 -3.9959874 -4.1672988 -4.2644849 -4.2775269 -4.2304263][-4.2975464 -4.2992616 -4.2715135 -4.201335 -4.09084 -3.9646552 -3.8598971 -3.8205757 -3.8816216 -4.0153255 -4.1673141 -4.2881002 -4.3410769 -4.3159332 -4.2304039][-4.3227253 -4.3255858 -4.2983942 -4.2294588 -4.1282244 -4.030529 -3.9731467 -3.9837465 -4.0720978 -4.1902471 -4.2938848 -4.3611779 -4.3750782 -4.3195572 -4.2011385][-4.3683133 -4.3785343 -4.356 -4.2893019 -4.1941857 -4.1128693 -4.0817089 -4.1155753 -4.2030416 -4.2882586 -4.3429518 -4.3677778 -4.3573761 -4.288703 -4.158205][-4.4298458 -4.4537454 -4.4421029 -4.3847065 -4.3012447 -4.2316332 -4.2086058 -4.2413549 -4.3069577 -4.3540421 -4.37026 -4.3673272 -4.3462925 -4.2846303 -4.1711249][-4.4784107 -4.5174804 -4.52175 -4.4831762 -4.4226108 -4.3709588 -4.3535142 -4.3769212 -4.4164643 -4.4338536 -4.427474 -4.4110689 -4.389204 -4.3458381 -4.26766][-4.492126 -4.5423837 -4.5591741 -4.539835 -4.5026393 -4.4693007 -4.4571433 -4.4715343 -4.4923797 -4.495347 -4.4834805 -4.4669414 -4.4504347 -4.4265842 -4.3846869]]...]
INFO - root - 2017-12-07 18:27:52.510642: step 38810, loss = 21.61, batch loss = 21.52 (8.2 examples/sec; 0.971 sec/batch; 79h:14m:01s remains)
INFO - root - 2017-12-07 18:28:01.791847: step 38820, loss = 21.51, batch loss = 21.43 (8.3 examples/sec; 0.964 sec/batch; 78h:40m:10s remains)
INFO - root - 2017-12-07 18:28:11.122655: step 38830, loss = 21.41, batch loss = 21.33 (8.4 examples/sec; 0.958 sec/batch; 78h:09m:02s remains)
INFO - root - 2017-12-07 18:28:20.536751: step 38840, loss = 21.46, batch loss = 21.38 (8.4 examples/sec; 0.957 sec/batch; 78h:03m:51s remains)
INFO - root - 2017-12-07 18:28:29.928712: step 38850, loss = 21.26, batch loss = 21.17 (8.9 examples/sec; 0.896 sec/batch; 73h:04m:01s remains)
INFO - root - 2017-12-07 18:28:39.334319: step 38860, loss = 21.15, batch loss = 21.06 (9.5 examples/sec; 0.845 sec/batch; 68h:57m:21s remains)
INFO - root - 2017-12-07 18:28:48.755747: step 38870, loss = 21.15, batch loss = 21.06 (9.2 examples/sec; 0.870 sec/batch; 70h:56m:26s remains)
INFO - root - 2017-12-07 18:28:58.263902: step 38880, loss = 21.23, batch loss = 21.15 (8.5 examples/sec; 0.945 sec/batch; 77h:04m:35s remains)
INFO - root - 2017-12-07 18:29:07.575608: step 38890, loss = 21.66, batch loss = 21.58 (8.4 examples/sec; 0.954 sec/batch; 77h:46m:53s remains)
INFO - root - 2017-12-07 18:29:16.978161: step 38900, loss = 21.62, batch loss = 21.54 (8.8 examples/sec; 0.911 sec/batch; 74h:17m:46s remains)
2017-12-07 18:29:17.886678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2517905 -4.3065391 -4.392046 -4.4594836 -4.4830837 -4.4848609 -4.4797487 -4.4725165 -4.4707565 -4.4701061 -4.45499 -4.413579 -4.3587527 -4.3111434 -4.2848868][-4.2529912 -4.3172417 -4.4042277 -4.4663391 -4.4909611 -4.5061693 -4.5187583 -4.5287828 -4.5399489 -4.5495758 -4.5445561 -4.5078211 -4.4445515 -4.3821015 -4.3417645][-4.2653804 -4.3314867 -4.4045329 -4.4421859 -4.447154 -4.4544463 -4.4741116 -4.5048704 -4.5397482 -4.5722342 -4.5913367 -4.5747795 -4.5194583 -4.4546647 -4.4046154][-4.3011227 -4.3507209 -4.3957696 -4.4000068 -4.3729954 -4.353157 -4.3663082 -4.4151945 -4.474544 -4.5290117 -4.5689082 -4.5719938 -4.5338831 -4.4808931 -4.4329963][-4.3578734 -4.3781414 -4.3852019 -4.3484445 -4.2803116 -4.2212906 -4.2193756 -4.282124 -4.3631926 -4.4385962 -4.4949474 -4.5107865 -4.4862 -4.445003 -4.4007373][-4.4042482 -4.3865566 -4.3450046 -4.2507763 -4.1208529 -4.007205 -3.9807777 -4.0551386 -4.1703477 -4.2914042 -4.3885937 -4.4317493 -4.424047 -4.3862376 -4.3344779][-4.3961711 -4.3486929 -4.2632313 -4.1113553 -3.9129403 -3.7383966 -3.6820619 -3.7631779 -3.9173298 -4.0984693 -4.2507329 -4.3349886 -4.3577561 -4.3290372 -4.2697792][-4.3398094 -4.2901716 -4.1969266 -4.022646 -3.7824078 -3.5655487 -3.4893053 -3.5726991 -3.7497611 -3.9629753 -4.1371 -4.2386208 -4.2836771 -4.2675815 -4.2131786][-4.3138576 -4.2967877 -4.2394609 -4.0948887 -3.8605816 -3.6303232 -3.5398414 -3.613487 -3.7875383 -3.9935789 -4.1512394 -4.2384696 -4.2791734 -4.2619305 -4.2161279][-4.3553467 -4.385561 -4.3818531 -4.297081 -4.1093693 -3.8926044 -3.786644 -3.8297236 -3.9750073 -4.1484733 -4.2748685 -4.3380938 -4.3598151 -4.3330135 -4.2948132][-4.4461 -4.5001364 -4.5290828 -4.4992981 -4.3770671 -4.2015486 -4.0884066 -4.0935059 -4.1914706 -4.3153324 -4.40118 -4.4355369 -4.4360542 -4.4075484 -4.3849378][-4.5697784 -4.6227756 -4.6547465 -4.6559029 -4.5897727 -4.4629536 -4.3522081 -4.31962 -4.3595428 -4.4226775 -4.4626164 -4.4691911 -4.4531264 -4.4303513 -4.4275541][-4.6647353 -4.6973505 -4.7091575 -4.7082782 -4.6672196 -4.5774245 -4.4785528 -4.4222393 -4.4153566 -4.4315166 -4.4445453 -4.4453549 -4.4301009 -4.4130182 -4.4138112][-4.6747184 -4.6797137 -4.6644454 -4.6425114 -4.6004462 -4.5315127 -4.4541836 -4.4024682 -4.3818955 -4.3812141 -4.3912711 -4.4042315 -4.40117 -4.384232 -4.3711219][-4.5779028 -4.562078 -4.5285058 -4.4894733 -4.440093 -4.3810258 -4.3286676 -4.3036637 -4.2992659 -4.3031559 -4.3166151 -4.3418722 -4.3580952 -4.346797 -4.3198237]]...]
INFO - root - 2017-12-07 18:29:27.429633: step 38910, loss = 21.47, batch loss = 21.39 (8.0 examples/sec; 0.998 sec/batch; 81h:24m:01s remains)
INFO - root - 2017-12-07 18:29:36.739036: step 38920, loss = 21.20, batch loss = 21.12 (8.1 examples/sec; 0.983 sec/batch; 80h:07m:56s remains)
INFO - root - 2017-12-07 18:29:45.879898: step 38930, loss = 21.39, batch loss = 21.31 (8.1 examples/sec; 0.994 sec/batch; 81h:02m:22s remains)
INFO - root - 2017-12-07 18:29:55.394034: step 38940, loss = 21.50, batch loss = 21.42 (8.3 examples/sec; 0.958 sec/batch; 78h:08m:52s remains)
INFO - root - 2017-12-07 18:30:04.799766: step 38950, loss = 21.78, batch loss = 21.70 (8.0 examples/sec; 0.996 sec/batch; 81h:10m:58s remains)
INFO - root - 2017-12-07 18:30:14.313768: step 38960, loss = 21.44, batch loss = 21.36 (8.0 examples/sec; 0.995 sec/batch; 81h:10m:14s remains)
INFO - root - 2017-12-07 18:30:23.759671: step 38970, loss = 21.46, batch loss = 21.38 (8.2 examples/sec; 0.976 sec/batch; 79h:33m:21s remains)
INFO - root - 2017-12-07 18:30:33.123124: step 38980, loss = 21.67, batch loss = 21.59 (8.5 examples/sec; 0.936 sec/batch; 76h:21m:08s remains)
INFO - root - 2017-12-07 18:30:42.453946: step 38990, loss = 21.37, batch loss = 21.29 (9.1 examples/sec; 0.879 sec/batch; 71h:40m:30s remains)
INFO - root - 2017-12-07 18:30:51.887055: step 39000, loss = 21.13, batch loss = 21.05 (8.6 examples/sec; 0.931 sec/batch; 75h:54m:06s remains)
2017-12-07 18:30:52.873308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2813554 -4.1531296 -4.06954 -4.10205 -4.1999688 -4.2742724 -4.3024445 -4.3224025 -4.3552241 -4.4123564 -4.4599771 -4.4389577 -4.3454456 -4.2720881 -4.3032069][-4.328548 -4.1762028 -4.0906549 -4.1445122 -4.2687254 -4.364027 -4.4101911 -4.4440961 -4.4769635 -4.5095677 -4.5210247 -4.4700642 -4.3587694 -4.2806883 -4.3105125][-4.30238 -4.1249475 -4.0545697 -4.1473684 -4.3004341 -4.4024806 -4.4511261 -4.4983792 -4.5469289 -4.57226 -4.5501747 -4.4569693 -4.3163548 -4.2271972 -4.2542081][-4.2478132 -4.0506687 -3.9935806 -4.1102486 -4.259387 -4.3284612 -4.3526912 -4.4122815 -4.497107 -4.5475454 -4.5238485 -4.4066615 -4.2415619 -4.1406302 -4.1698709][-4.2357149 -4.0312715 -3.9646475 -4.048851 -4.124042 -4.1010981 -4.0668888 -4.1339245 -4.2777305 -4.4008427 -4.4341135 -4.3382425 -4.1641297 -4.0452509 -4.066998][-4.2197986 -4.024034 -3.9373283 -3.9535661 -3.924053 -3.7905593 -3.6803808 -3.7431488 -3.9532661 -4.1793609 -4.3151121 -4.2853289 -4.1256795 -3.987546 -3.9876199][-4.23633 -4.0688691 -3.966043 -3.9157548 -3.8005 -3.583096 -3.4005876 -3.4335146 -3.6818187 -3.9953251 -4.23175 -4.2795072 -4.1532216 -4.010839 -3.9953008][-4.3198028 -4.2007747 -4.1002417 -4.0141249 -3.8589981 -3.607898 -3.3846035 -3.3798103 -3.6270261 -3.9765558 -4.2659144 -4.3628111 -4.2631044 -4.1158915 -4.0785217][-4.3816671 -4.3257356 -4.2554932 -4.1747041 -4.0280433 -3.7915161 -3.5639627 -3.5255868 -3.7384398 -4.0719862 -4.3647785 -4.4829564 -4.4065485 -4.2541413 -4.1827974][-4.4594574 -4.4751735 -4.4514971 -4.39922 -4.2896214 -4.1014624 -3.9036982 -3.8406341 -3.981411 -4.2382689 -4.4780107 -4.5833912 -4.5244284 -4.3837161 -4.2977839][-4.5352206 -4.6094074 -4.6262083 -4.6080117 -4.5482244 -4.4353938 -4.3097711 -4.2601256 -4.336062 -4.4905128 -4.6358109 -4.6911139 -4.629662 -4.50186 -4.4129248][-4.5777287 -4.6578593 -4.6821957 -4.6827955 -4.6668525 -4.6266403 -4.5782247 -4.563683 -4.6059761 -4.6793189 -4.7404666 -4.7493343 -4.6883636 -4.5806503 -4.4943805][-4.620326 -4.6699624 -4.6781731 -4.6829672 -4.6947317 -4.701797 -4.7044759 -4.7128749 -4.7295027 -4.7427812 -4.7442737 -4.726686 -4.6767392 -4.59466 -4.5167489][-4.6569147 -4.6478076 -4.6183596 -4.605794 -4.6167812 -4.6365275 -4.6563106 -4.6721683 -4.6785641 -4.6716132 -4.6558013 -4.6358 -4.5987244 -4.5339375 -4.462451][-4.6476483 -4.5770478 -4.5074792 -4.46953 -4.4620352 -4.4687686 -4.4797668 -4.4895306 -4.4941196 -4.4925427 -4.4873161 -4.4816852 -4.4641132 -4.4238038 -4.3755507]]...]
INFO - root - 2017-12-07 18:31:02.313315: step 39010, loss = 21.74, batch loss = 21.66 (8.7 examples/sec; 0.916 sec/batch; 74h:40m:16s remains)
INFO - root - 2017-12-07 18:31:11.626876: step 39020, loss = 21.59, batch loss = 21.51 (9.2 examples/sec; 0.865 sec/batch; 70h:32m:57s remains)
INFO - root - 2017-12-07 18:31:20.950403: step 39030, loss = 21.26, batch loss = 21.18 (8.6 examples/sec; 0.926 sec/batch; 75h:30m:11s remains)
INFO - root - 2017-12-07 18:31:30.393300: step 39040, loss = 21.53, batch loss = 21.44 (8.0 examples/sec; 0.994 sec/batch; 81h:02m:28s remains)
INFO - root - 2017-12-07 18:31:39.718550: step 39050, loss = 21.11, batch loss = 21.02 (8.3 examples/sec; 0.967 sec/batch; 78h:50m:12s remains)
INFO - root - 2017-12-07 18:31:49.305106: step 39060, loss = 21.71, batch loss = 21.62 (8.2 examples/sec; 0.971 sec/batch; 79h:07m:31s remains)
INFO - root - 2017-12-07 18:31:58.735614: step 39070, loss = 21.63, batch loss = 21.55 (9.2 examples/sec; 0.872 sec/batch; 71h:05m:04s remains)
INFO - root - 2017-12-07 18:32:08.259160: step 39080, loss = 21.49, batch loss = 21.41 (8.3 examples/sec; 0.963 sec/batch; 78h:29m:40s remains)
INFO - root - 2017-12-07 18:32:17.733666: step 39090, loss = 21.53, batch loss = 21.44 (8.7 examples/sec; 0.920 sec/batch; 74h:57m:01s remains)
INFO - root - 2017-12-07 18:32:27.011683: step 39100, loss = 21.13, batch loss = 21.05 (8.4 examples/sec; 0.947 sec/batch; 77h:10m:03s remains)
2017-12-07 18:32:27.968061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.59819 -4.4740367 -4.3661413 -4.3305149 -4.2693591 -4.1509004 -4.1645889 -4.2934742 -4.3661656 -4.4086623 -4.4120045 -4.3239255 -4.2138705 -4.2444229 -4.3639627][-4.6808834 -4.564858 -4.4417953 -4.381124 -4.3129792 -4.2075496 -4.2175851 -4.3459792 -4.4360271 -4.4898276 -4.5031295 -4.433135 -4.3403926 -4.3604112 -4.4667959][-4.7419271 -4.6367164 -4.508286 -4.4205842 -4.3267946 -4.21271 -4.2129602 -4.3509207 -4.475121 -4.5522752 -4.5795436 -4.5345068 -4.4689846 -4.4674258 -4.5228257][-4.7488961 -4.6405611 -4.5049357 -4.3867979 -4.24593 -4.1005311 -4.0911632 -4.2517428 -4.4224491 -4.5355053 -4.5860205 -4.5629745 -4.5158577 -4.4901657 -4.4924932][-4.7163954 -4.5956407 -4.4525795 -4.3037581 -4.1054339 -3.9119542 -3.8829279 -4.0484138 -4.2406621 -4.379961 -4.4595561 -4.4632225 -4.4383097 -4.4156518 -4.4017124][-4.6783967 -4.5439515 -4.3843479 -4.193778 -3.9307134 -3.6841667 -3.6286383 -3.7750125 -3.9528744 -4.1012354 -4.2172217 -4.2652645 -4.2865186 -4.3051939 -4.3145084][-4.6781569 -4.5459404 -4.368094 -4.1273351 -3.7996643 -3.5059731 -3.4272671 -3.5516791 -3.7085533 -3.8688602 -4.0316076 -4.1333447 -4.2041206 -4.2688956 -4.301044][-4.7461462 -4.6520953 -4.479239 -4.2048392 -3.8299189 -3.504355 -3.41397 -3.5309205 -3.6794424 -3.8512571 -4.0434256 -4.171382 -4.2538543 -4.3273692 -4.3624153][-4.8401322 -4.8162546 -4.6852732 -4.4180927 -4.0354791 -3.7046113 -3.6128907 -3.7247946 -3.8609114 -4.0197749 -4.198391 -4.3116384 -4.3706226 -4.4259281 -4.4587035][-4.8644886 -4.9104166 -4.8429346 -4.625649 -4.2858014 -3.99466 -3.9290671 -4.0360975 -4.1470027 -4.2669621 -4.401618 -4.4844561 -4.5148358 -4.5383453 -4.5563941][-4.7806978 -4.8600688 -4.8431325 -4.6945729 -4.4432359 -4.2472835 -4.2506971 -4.3691735 -4.45531 -4.5328388 -4.6190491 -4.6674838 -4.6648846 -4.6418357 -4.6252975][-4.64235 -4.7097836 -4.7065845 -4.6162243 -4.4655762 -4.3831758 -4.4671226 -4.6025019 -4.6663513 -4.7033434 -4.7448092 -4.7558942 -4.7179418 -4.6534939 -4.6103783][-4.5099435 -4.538166 -4.5176659 -4.4576316 -4.3820233 -4.3843565 -4.5243545 -4.6678114 -4.7125044 -4.7168097 -4.7225013 -4.702754 -4.6423192 -4.5633054 -4.5258565][-4.4046926 -4.3873534 -4.3388 -4.2847795 -4.2441144 -4.2811885 -4.4379716 -4.5844579 -4.6231327 -4.6126151 -4.5991549 -4.5623436 -4.4968615 -4.4280677 -4.4223566][-4.320755 -4.2745228 -4.2084646 -4.1523232 -4.1166668 -4.1500387 -4.2935953 -4.4383731 -4.4819059 -4.4697266 -4.4501243 -4.410603 -4.3525472 -4.3015513 -4.3244209]]...]
INFO - root - 2017-12-07 18:32:37.260770: step 39110, loss = 21.79, batch loss = 21.71 (8.4 examples/sec; 0.954 sec/batch; 77h:45m:35s remains)
INFO - root - 2017-12-07 18:32:46.705287: step 39120, loss = 21.50, batch loss = 21.42 (8.1 examples/sec; 0.984 sec/batch; 80h:09m:46s remains)
INFO - root - 2017-12-07 18:32:56.133693: step 39130, loss = 21.73, batch loss = 21.65 (8.0 examples/sec; 1.001 sec/batch; 81h:34m:22s remains)
INFO - root - 2017-12-07 18:33:05.419712: step 39140, loss = 21.52, batch loss = 21.44 (9.0 examples/sec; 0.888 sec/batch; 72h:20m:13s remains)
INFO - root - 2017-12-07 18:33:14.834830: step 39150, loss = 20.98, batch loss = 20.90 (8.6 examples/sec; 0.930 sec/batch; 75h:46m:51s remains)
INFO - root - 2017-12-07 18:33:24.124318: step 39160, loss = 21.84, batch loss = 21.75 (8.6 examples/sec; 0.935 sec/batch; 76h:08m:58s remains)
INFO - root - 2017-12-07 18:33:33.378741: step 39170, loss = 21.52, batch loss = 21.44 (8.7 examples/sec; 0.922 sec/batch; 75h:05m:09s remains)
INFO - root - 2017-12-07 18:33:42.717601: step 39180, loss = 21.40, batch loss = 21.32 (8.4 examples/sec; 0.957 sec/batch; 77h:57m:12s remains)
INFO - root - 2017-12-07 18:33:52.028930: step 39190, loss = 21.90, batch loss = 21.81 (9.4 examples/sec; 0.847 sec/batch; 68h:58m:34s remains)
INFO - root - 2017-12-07 18:34:01.393695: step 39200, loss = 21.86, batch loss = 21.78 (9.2 examples/sec; 0.867 sec/batch; 70h:38m:43s remains)
2017-12-07 18:34:02.346470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4237428 -4.4728112 -4.5206513 -4.5394726 -4.5394993 -4.5647159 -4.6530638 -4.7527804 -4.7578964 -4.7016926 -4.6659136 -4.6517186 -4.6590738 -4.6655397 -4.5986643][-4.4801292 -4.5360503 -4.5579534 -4.53396 -4.4949903 -4.5148559 -4.6523485 -4.8098922 -4.83189 -4.7642226 -4.700057 -4.6378894 -4.6016979 -4.5987797 -4.5272336][-4.4605489 -4.5208225 -4.5254807 -4.4739923 -4.408546 -4.4155321 -4.5733666 -4.7711949 -4.83298 -4.7907104 -4.7172918 -4.6079926 -4.5145478 -4.4878397 -4.4178534][-4.4125905 -4.4681 -4.4582448 -4.3920741 -4.3103929 -4.28325 -4.391789 -4.5659757 -4.6657619 -4.6919227 -4.6590528 -4.5557685 -4.4497323 -4.4123435 -4.3546944][-4.3886909 -4.4322681 -4.4050212 -4.3258743 -4.2223544 -4.1260958 -4.1057353 -4.1824865 -4.31516 -4.4508843 -4.5074644 -4.4662485 -4.4012017 -4.3753471 -4.3278351][-4.3972197 -4.428997 -4.381166 -4.2803574 -4.1395965 -3.9463387 -3.7507672 -3.6920557 -3.8657715 -4.1520152 -4.3298883 -4.3639078 -4.3447423 -4.3252368 -4.268714][-4.4228153 -4.46206 -4.4159737 -4.3049712 -4.1259956 -3.8346646 -3.4681568 -3.2722852 -3.4780731 -3.9149144 -4.2269421 -4.3346577 -4.3420238 -4.3102479 -4.2182846][-4.4389052 -4.5072813 -4.509428 -4.436718 -4.2626395 -3.9276114 -3.4604921 -3.1567154 -3.3367064 -3.8355083 -4.2333694 -4.395134 -4.4115877 -4.3584108 -4.2285995][-4.4383121 -4.5196452 -4.5765934 -4.5686831 -4.4521723 -4.1738114 -3.7522323 -3.4395986 -3.5478635 -3.9767449 -4.360693 -4.5324011 -4.5422173 -4.4684649 -4.316968][-4.4647412 -4.5244603 -4.5885563 -4.6133757 -4.5592222 -4.3947468 -4.1190863 -3.8897731 -3.9321759 -4.216507 -4.5058422 -4.6491528 -4.6528006 -4.5767241 -4.4350023][-4.5509353 -4.5901022 -4.62214 -4.6284928 -4.5989494 -4.5274677 -4.3968449 -4.2721729 -4.2725773 -4.4076548 -4.5683379 -4.6572642 -4.6603966 -4.6060719 -4.5028772][-4.6174088 -4.6576972 -4.6657653 -4.6450667 -4.6126814 -4.5809255 -4.5358481 -4.4887276 -4.4778833 -4.5152855 -4.569489 -4.6005878 -4.5964503 -4.5628858 -4.4983249][-4.5889134 -4.6309919 -4.6351337 -4.6057668 -4.566823 -4.5395288 -4.5221524 -4.5096736 -4.5043197 -4.5080476 -4.514472 -4.5128 -4.4996462 -4.4747982 -4.4346747][-4.4927011 -4.5218258 -4.5217781 -4.4891882 -4.4436188 -4.4087591 -4.3930445 -4.3897629 -4.3916988 -4.3959708 -4.3974504 -4.391655 -4.379673 -4.3664656 -4.3522849][-4.379981 -4.39142 -4.3837419 -4.3514624 -4.308847 -4.276793 -4.2645712 -4.2666025 -4.2777224 -4.2925153 -4.3026066 -4.303031 -4.2983875 -4.2999821 -4.3122344]]...]
INFO - root - 2017-12-07 18:34:11.696395: step 39210, loss = 21.38, batch loss = 21.30 (8.4 examples/sec; 0.947 sec/batch; 77h:11m:24s remains)
INFO - root - 2017-12-07 18:34:20.998845: step 39220, loss = 21.30, batch loss = 21.22 (9.0 examples/sec; 0.892 sec/batch; 72h:41m:54s remains)
INFO - root - 2017-12-07 18:34:30.345637: step 39230, loss = 21.77, batch loss = 21.69 (9.7 examples/sec; 0.821 sec/batch; 66h:52m:36s remains)
INFO - root - 2017-12-07 18:34:39.784690: step 39240, loss = 21.46, batch loss = 21.38 (8.6 examples/sec; 0.928 sec/batch; 75h:34m:41s remains)
INFO - root - 2017-12-07 18:34:49.105294: step 39250, loss = 21.59, batch loss = 21.51 (8.2 examples/sec; 0.976 sec/batch; 79h:28m:10s remains)
INFO - root - 2017-12-07 18:34:58.478913: step 39260, loss = 21.60, batch loss = 21.51 (7.9 examples/sec; 1.018 sec/batch; 82h:55m:25s remains)
INFO - root - 2017-12-07 18:35:07.917398: step 39270, loss = 21.78, batch loss = 21.70 (8.4 examples/sec; 0.956 sec/batch; 77h:50m:45s remains)
INFO - root - 2017-12-07 18:35:17.308791: step 39280, loss = 21.23, batch loss = 21.15 (8.3 examples/sec; 0.968 sec/batch; 78h:50m:06s remains)
INFO - root - 2017-12-07 18:35:26.941110: step 39290, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.946 sec/batch; 77h:02m:06s remains)
INFO - root - 2017-12-07 18:35:36.333780: step 39300, loss = 21.40, batch loss = 21.31 (8.0 examples/sec; 1.000 sec/batch; 81h:27m:25s remains)
2017-12-07 18:35:37.257231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3522162 -4.3701477 -4.38614 -4.4050159 -4.4310942 -4.4546471 -4.4606762 -4.4469895 -4.4327879 -4.4356055 -4.4501705 -4.4631257 -4.4674659 -4.4571619 -4.4303279][-4.4245658 -4.4418359 -4.451757 -4.4609432 -4.47728 -4.4906764 -4.47966 -4.450902 -4.4384761 -4.4592848 -4.4937615 -4.5210118 -4.5371208 -4.5383158 -4.515522][-4.4935184 -4.4934464 -4.4827681 -4.4706511 -4.470645 -4.470932 -4.4466758 -4.4139361 -4.4181294 -4.4709892 -4.5333591 -4.5720811 -4.5901046 -4.5964069 -4.578259][-4.5121584 -4.4860592 -4.4482818 -4.4116778 -4.3971176 -4.3909431 -4.3663154 -4.3431172 -4.3676653 -4.4523435 -4.5414853 -4.5866919 -4.5933943 -4.59094 -4.5730333][-4.4588566 -4.4155526 -4.3633556 -4.3156376 -4.2959261 -4.2900596 -4.2720852 -4.2554073 -4.2830529 -4.3796616 -4.4839163 -4.5322962 -4.5307803 -4.5186877 -4.4985871][-4.3448691 -4.3049612 -4.2599144 -4.2155519 -4.1931949 -4.1856546 -4.1742353 -4.1606402 -4.1809754 -4.2732506 -4.3745685 -4.4207726 -4.4224033 -4.4125776 -4.3936234][-4.1910338 -4.1681023 -4.1420279 -4.1063042 -4.0839367 -4.0814953 -4.0883679 -4.0892968 -4.1135273 -4.1959605 -4.2745466 -4.30735 -4.3095694 -4.3039541 -4.2859697][-4.0135031 -4.0117888 -4.0229445 -4.0143013 -4.0038252 -4.0162821 -4.0424142 -4.0634446 -4.1026115 -4.1780691 -4.2285318 -4.2375464 -4.2253056 -4.2085466 -4.1793785][-3.8522406 -3.8868763 -3.962517 -4.0096397 -4.0256634 -4.0484891 -4.0698895 -4.0833969 -4.1114693 -4.1669497 -4.194932 -4.1836929 -4.14847 -4.11343 -4.0738826][-3.8061471 -3.8893654 -4.0360708 -4.1334405 -4.1551056 -4.1507282 -4.1319914 -4.1001754 -4.0786715 -4.0999856 -4.1216369 -4.1048713 -4.0525994 -4.0067148 -3.96708][-3.9035556 -4.0202012 -4.2074938 -4.3119135 -4.2906585 -4.2194524 -4.1394844 -4.0477757 -3.9625075 -3.9485159 -3.9877989 -4.0011482 -3.9691656 -3.9450426 -3.9338164][-4.0671682 -4.1809096 -4.3634853 -4.4362974 -4.3461647 -4.1980433 -4.0633168 -3.9280424 -3.8061798 -3.7900898 -3.8792374 -3.9592838 -3.9827912 -3.9984009 -4.0212469][-4.2420478 -4.3164864 -4.4482136 -4.4721742 -4.340765 -4.1643863 -4.0141873 -3.8690004 -3.7476237 -3.7567482 -3.8988607 -4.037549 -4.1047177 -4.1407995 -4.1789722][-4.387866 -4.4145617 -4.4769554 -4.4701905 -4.3640323 -4.2349081 -4.1138272 -3.97564 -3.8611634 -3.8829505 -4.0380778 -4.1907978 -4.2640066 -4.2989779 -4.3454204][-4.4985652 -4.5015812 -4.513505 -4.4908929 -4.4302874 -4.366972 -4.2883673 -4.1707325 -4.067883 -4.090126 -4.2320743 -4.36816 -4.4210978 -4.4362545 -4.475625]]...]
INFO - root - 2017-12-07 18:35:46.610871: step 39310, loss = 21.48, batch loss = 21.40 (8.2 examples/sec; 0.971 sec/batch; 79h:05m:44s remains)
INFO - root - 2017-12-07 18:35:55.915490: step 39320, loss = 21.62, batch loss = 21.54 (8.6 examples/sec; 0.934 sec/batch; 76h:03m:47s remains)
INFO - root - 2017-12-07 18:36:05.189526: step 39330, loss = 21.51, batch loss = 21.42 (8.5 examples/sec; 0.942 sec/batch; 76h:40m:30s remains)
INFO - root - 2017-12-07 18:36:14.362615: step 39340, loss = 21.53, batch loss = 21.44 (9.0 examples/sec; 0.884 sec/batch; 71h:59m:14s remains)
INFO - root - 2017-12-07 18:36:23.528545: step 39350, loss = 21.60, batch loss = 21.51 (8.8 examples/sec; 0.905 sec/batch; 73h:43m:16s remains)
INFO - root - 2017-12-07 18:36:32.889276: step 39360, loss = 21.01, batch loss = 20.92 (8.5 examples/sec; 0.944 sec/batch; 76h:51m:28s remains)
INFO - root - 2017-12-07 18:36:42.280880: step 39370, loss = 21.37, batch loss = 21.28 (9.0 examples/sec; 0.889 sec/batch; 72h:21m:03s remains)
INFO - root - 2017-12-07 18:36:51.675415: step 39380, loss = 21.13, batch loss = 21.05 (8.5 examples/sec; 0.940 sec/batch; 76h:32m:44s remains)
INFO - root - 2017-12-07 18:37:00.934348: step 39390, loss = 21.39, batch loss = 21.31 (8.5 examples/sec; 0.938 sec/batch; 76h:24m:00s remains)
INFO - root - 2017-12-07 18:37:10.526309: step 39400, loss = 21.38, batch loss = 21.30 (7.8 examples/sec; 1.029 sec/batch; 83h:45m:29s remains)
2017-12-07 18:37:11.408768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4194975 -4.4877515 -4.5340786 -4.5559144 -4.5489655 -4.51515 -4.4727607 -4.4629307 -4.4957805 -4.5251188 -4.5214143 -4.50023 -4.4823132 -4.48249 -4.49657][-4.4649916 -4.5693488 -4.6531854 -4.7038717 -4.7048159 -4.6524692 -4.5782447 -4.553452 -4.6049376 -4.6627312 -4.6656661 -4.6415424 -4.6272726 -4.6364393 -4.6571646][-4.4888144 -4.6339145 -4.7564526 -4.8300023 -4.8254042 -4.7404189 -4.6187568 -4.5596819 -4.6225791 -4.7239037 -4.7583103 -4.749033 -4.7494159 -4.7712483 -4.7951379][-4.5016227 -4.6731296 -4.8113923 -4.8784132 -4.8419418 -4.7055359 -4.5180979 -4.4013052 -4.4621754 -4.6227422 -4.7224021 -4.7571082 -4.7837934 -4.8168759 -4.8444371][-4.4962797 -4.6531205 -4.7538581 -4.7657347 -4.6668558 -4.4682379 -4.2123222 -4.0344167 -4.0953627 -4.3332043 -4.529037 -4.6304331 -4.6867876 -4.7266817 -4.7591228][-4.4784937 -4.5790157 -4.5949655 -4.5208111 -4.3610525 -4.1230297 -3.8258934 -3.599402 -3.6517506 -3.9489834 -4.23233 -4.4046607 -4.4903564 -4.5297265 -4.5620704][-4.4536867 -4.489748 -4.4181 -4.2685328 -4.0789995 -3.8467116 -3.5549922 -3.3057694 -3.3315992 -3.6370113 -3.9707284 -4.2066393 -4.3242264 -4.3578606 -4.37478][-4.4382143 -4.4358368 -4.3240695 -4.1507883 -3.9766302 -3.7876973 -3.5426147 -3.3061256 -3.2923336 -3.542702 -3.8672864 -4.13221 -4.2731161 -4.3042884 -4.3066578][-4.4428043 -4.4530463 -4.3743029 -4.2474494 -4.1292171 -3.9964051 -3.7997103 -3.5798759 -3.5142336 -3.6730165 -3.9397016 -4.1900907 -4.337966 -4.3771 -4.3818097][-4.4517469 -4.4998665 -4.4822688 -4.4195528 -4.3544536 -4.2626967 -4.1041384 -3.9073763 -3.8187261 -3.9228737 -4.1445603 -4.3642936 -4.4922442 -4.5247288 -4.5229611][-4.4437962 -4.5172272 -4.5417409 -4.5223923 -4.4922643 -4.43559 -4.3229117 -4.1707096 -4.0970335 -4.1900635 -4.3822155 -4.5591393 -4.6452436 -4.6534905 -4.6314521][-4.4186363 -4.5024805 -4.5525641 -4.5670853 -4.57018 -4.5565658 -4.5002708 -4.4057012 -4.3555937 -4.4262395 -4.5600061 -4.6680155 -4.7045026 -4.6933441 -4.6626811][-4.3845 -4.4677644 -4.5310445 -4.5699043 -4.6021733 -4.628396 -4.6259646 -4.5858989 -4.5521975 -4.5781851 -4.628345 -4.6565142 -4.6493564 -4.6307993 -4.6061134][-4.3451772 -4.4187932 -4.4840455 -4.5335789 -4.5773892 -4.6197596 -4.64453 -4.6385546 -4.6128821 -4.596518 -4.5792027 -4.5525761 -4.5204206 -4.500401 -4.4851565][-4.304368 -4.3581982 -4.410038 -4.4521136 -4.4870992 -4.5189314 -4.5408721 -4.5426817 -4.5204635 -4.4863248 -4.4432659 -4.39986 -4.3650351 -4.3483119 -4.3418946]]...]
INFO - root - 2017-12-07 18:37:20.831335: step 39410, loss = 21.50, batch loss = 21.42 (9.1 examples/sec; 0.881 sec/batch; 71h:44m:32s remains)
INFO - root - 2017-12-07 18:37:30.261918: step 39420, loss = 21.59, batch loss = 21.51 (9.2 examples/sec; 0.869 sec/batch; 70h:44m:08s remains)
INFO - root - 2017-12-07 18:37:39.660035: step 39430, loss = 21.33, batch loss = 21.24 (8.7 examples/sec; 0.916 sec/batch; 74h:35m:06s remains)
INFO - root - 2017-12-07 18:37:48.984471: step 39440, loss = 21.19, batch loss = 21.11 (8.4 examples/sec; 0.956 sec/batch; 77h:47m:28s remains)
INFO - root - 2017-12-07 18:37:58.265429: step 39450, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.911 sec/batch; 74h:07m:40s remains)
INFO - root - 2017-12-07 18:38:07.699373: step 39460, loss = 21.11, batch loss = 21.03 (8.4 examples/sec; 0.949 sec/batch; 77h:14m:27s remains)
INFO - root - 2017-12-07 18:38:17.205042: step 39470, loss = 21.72, batch loss = 21.64 (8.6 examples/sec; 0.933 sec/batch; 75h:55m:04s remains)
INFO - root - 2017-12-07 18:38:26.681351: step 39480, loss = 21.50, batch loss = 21.41 (8.4 examples/sec; 0.957 sec/batch; 77h:53m:56s remains)
INFO - root - 2017-12-07 18:38:36.090057: step 39490, loss = 21.10, batch loss = 21.02 (8.9 examples/sec; 0.903 sec/batch; 73h:31m:34s remains)
INFO - root - 2017-12-07 18:38:45.319158: step 39500, loss = 21.25, batch loss = 21.16 (9.1 examples/sec; 0.877 sec/batch; 71h:24m:12s remains)
2017-12-07 18:38:46.307151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4337459 -4.4752769 -4.5225134 -4.5519347 -4.5459614 -4.5257816 -4.5272293 -4.5451856 -4.5418911 -4.5339 -4.5556135 -4.6119165 -4.6689711 -4.6979756 -4.6996155][-4.5114574 -4.5632043 -4.6219573 -4.6747227 -4.6865592 -4.6634984 -4.6560292 -4.6809182 -4.7012811 -4.7037735 -4.7148395 -4.7477646 -4.7775888 -4.779829 -4.7573662][-4.5368586 -4.5740671 -4.6253605 -4.6838131 -4.7052908 -4.6803823 -4.6595578 -4.6796303 -4.7108293 -4.7229304 -4.7331934 -4.7543144 -4.7616687 -4.741209 -4.6986728][-4.5071735 -4.5114064 -4.5322556 -4.5660777 -4.571836 -4.5341015 -4.4953852 -4.5101867 -4.5583186 -4.5931048 -4.6218963 -4.6558723 -4.6729827 -4.6604037 -4.617147][-4.4499207 -4.4260526 -4.4138069 -4.4051847 -4.3678975 -4.2971916 -4.2314081 -4.240747 -4.3212814 -4.4059153 -4.48136 -4.5494947 -4.5968685 -4.6123033 -4.5769415][-4.3877831 -4.3551722 -4.3200378 -4.263835 -4.1643677 -4.0388575 -3.930285 -3.9251425 -4.0380926 -4.1846628 -4.3246474 -4.4433713 -4.5374112 -4.6013088 -4.5926237][-4.3330417 -4.3022161 -4.2516885 -4.1590843 -4.0103774 -3.8395596 -3.6912205 -3.6580703 -3.7767406 -3.9642618 -4.1583996 -4.3271661 -4.4695082 -4.59039 -4.6245728][-4.3030267 -4.2845058 -4.2355318 -4.1366534 -3.9770331 -3.7948151 -3.6293225 -3.5625868 -3.6528006 -3.8414478 -4.0579004 -4.2512584 -4.4122963 -4.560966 -4.6233582][-4.2925081 -4.290029 -4.2578239 -4.1830635 -4.0505748 -3.8943744 -3.7473638 -3.6645885 -3.7041326 -3.8459759 -4.04185 -4.2312217 -4.3870506 -4.5290294 -4.597549][-4.3086867 -4.3120217 -4.3078113 -4.2823215 -4.2027221 -4.0929551 -3.9870107 -3.9162307 -3.9111736 -3.9779665 -4.1136427 -4.2714667 -4.4066935 -4.5229139 -4.575521][-4.3557239 -4.3719678 -4.4069142 -4.4391179 -4.4126573 -4.3375268 -4.2644725 -4.2237926 -4.2095375 -4.2188883 -4.2815447 -4.3785429 -4.4729686 -4.5548234 -4.5840788][-4.4124556 -4.4413328 -4.4999943 -4.5595741 -4.5592704 -4.4999762 -4.4473934 -4.4403706 -4.4482789 -4.4451008 -4.458602 -4.4888058 -4.5272794 -4.5761003 -4.5911555][-4.4653516 -4.5000429 -4.5529919 -4.5924911 -4.5823874 -4.5290856 -4.49283 -4.5099444 -4.5411267 -4.5519724 -4.5499535 -4.5296645 -4.5092983 -4.5266953 -4.5395737][-4.4927559 -4.5260606 -4.5535893 -4.5440454 -4.5035095 -4.459516 -4.4500837 -4.4867043 -4.5250931 -4.5401278 -4.5335832 -4.4875841 -4.4297743 -4.4260044 -4.447228][-4.4802518 -4.5064907 -4.5072832 -4.4539628 -4.3873887 -4.3594265 -4.3829808 -4.4332175 -4.4604297 -4.4579864 -4.4452739 -4.4026861 -4.3493056 -4.3471951 -4.3817558]]...]
INFO - root - 2017-12-07 18:38:55.737116: step 39510, loss = 21.43, batch loss = 21.34 (8.2 examples/sec; 0.972 sec/batch; 79h:06m:08s remains)
INFO - root - 2017-12-07 18:39:05.128623: step 39520, loss = 21.52, batch loss = 21.43 (8.4 examples/sec; 0.958 sec/batch; 77h:57m:23s remains)
INFO - root - 2017-12-07 18:39:14.490043: step 39530, loss = 21.16, batch loss = 21.08 (8.0 examples/sec; 1.001 sec/batch; 81h:26m:11s remains)
INFO - root - 2017-12-07 18:39:23.812842: step 39540, loss = 21.67, batch loss = 21.59 (8.1 examples/sec; 0.982 sec/batch; 79h:56m:02s remains)
INFO - root - 2017-12-07 18:39:33.269545: step 39550, loss = 21.03, batch loss = 20.94 (8.4 examples/sec; 0.958 sec/batch; 77h:56m:17s remains)
INFO - root - 2017-12-07 18:39:42.689688: step 39560, loss = 21.51, batch loss = 21.42 (8.8 examples/sec; 0.908 sec/batch; 73h:51m:11s remains)
INFO - root - 2017-12-07 18:39:52.110682: step 39570, loss = 21.59, batch loss = 21.51 (8.3 examples/sec; 0.965 sec/batch; 78h:30m:17s remains)
INFO - root - 2017-12-07 18:40:01.308008: step 39580, loss = 21.62, batch loss = 21.54 (8.8 examples/sec; 0.904 sec/batch; 73h:33m:30s remains)
INFO - root - 2017-12-07 18:40:10.683636: step 39590, loss = 21.23, batch loss = 21.15 (8.7 examples/sec; 0.922 sec/batch; 75h:01m:54s remains)
INFO - root - 2017-12-07 18:40:20.090947: step 39600, loss = 21.78, batch loss = 21.70 (8.3 examples/sec; 0.960 sec/batch; 78h:06m:41s remains)
2017-12-07 18:40:21.131455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.895689 -4.8009024 -4.6855764 -4.675631 -4.7355838 -4.7639256 -4.7641115 -4.7717628 -4.7962027 -4.8164821 -4.8168173 -4.8095336 -4.8088803 -4.8139491 -4.794487][-4.9097247 -4.81865 -4.7015648 -4.6867003 -4.7436628 -4.7688017 -4.7749615 -4.794013 -4.824482 -4.8395586 -4.8146515 -4.7584867 -4.7038021 -4.6810045 -4.6689353][-4.8863897 -4.8225212 -4.7177391 -4.6807146 -4.6958485 -4.68399 -4.6858864 -4.7304711 -4.79192 -4.8328671 -4.8150363 -4.7314343 -4.6190982 -4.5464797 -4.5230956][-4.8272419 -4.7968783 -4.7015347 -4.6243691 -4.5640106 -4.4797468 -4.4585962 -4.539104 -4.6686583 -4.7850523 -4.826263 -4.7638321 -4.62495 -4.5091448 -4.4719548][-4.6994896 -4.6874752 -4.5903049 -4.4678807 -4.3309584 -4.16235 -4.0943131 -4.2083488 -4.432548 -4.65655 -4.7924156 -4.7906322 -4.6636152 -4.5306659 -4.4951205][-4.5190086 -4.5055289 -4.4025731 -4.2485871 -4.0586915 -3.8159053 -3.6806912 -3.8081055 -4.1246104 -4.4488559 -4.6681428 -4.7235165 -4.6224117 -4.502183 -4.4948006][-4.3749113 -4.3540163 -4.2552052 -4.0906072 -3.8710523 -3.5784447 -3.3747597 -3.4880621 -3.8628581 -4.2399006 -4.4860096 -4.5580692 -4.4768229 -4.3830523 -4.4087143][-4.3721924 -4.3472629 -4.2598238 -4.0981231 -3.8726876 -3.5734382 -3.3338165 -3.4091754 -3.7766666 -4.135675 -4.3386917 -4.3742838 -4.2891378 -4.2148752 -4.2589936][-4.5028338 -4.476151 -4.4022794 -4.2588835 -4.056982 -3.7978721 -3.5722556 -3.605988 -3.90839 -4.1997018 -4.3248954 -4.3007174 -4.1989751 -4.1306596 -4.1742992][-4.6319032 -4.6037645 -4.5423126 -4.4314828 -4.28057 -4.0902658 -3.91138 -3.9155123 -4.1353917 -4.3528 -4.4192204 -4.3585691 -4.2549 -4.1960759 -4.2311983][-4.6745281 -4.651269 -4.6095157 -4.5475211 -4.4697909 -4.3637948 -4.2471123 -4.2339878 -4.3703885 -4.5182147 -4.5562935 -4.4936695 -4.4089546 -4.3668365 -4.3928275][-4.6631055 -4.6532578 -4.6333733 -4.6209249 -4.6177764 -4.5890708 -4.5269656 -4.5023904 -4.5677772 -4.6597347 -4.6931777 -4.66159 -4.616982 -4.5971522 -4.6124725][-4.6441941 -4.6358852 -4.615025 -4.6301613 -4.6859651 -4.7138152 -4.6868591 -4.6542034 -4.6698303 -4.7183661 -4.7487173 -4.751019 -4.7488408 -4.7544823 -4.7625885][-4.6359367 -4.6089253 -4.55862 -4.5735769 -4.6637697 -4.72673 -4.7232571 -4.6942687 -4.6864643 -4.7010403 -4.712965 -4.7204838 -4.7321477 -4.746665 -4.7482944][-4.618515 -4.5663977 -4.4816632 -4.4812188 -4.5790157 -4.6523237 -4.6609941 -4.6436687 -4.63616 -4.6402946 -4.6430922 -4.6467757 -4.6514215 -4.653379 -4.6418486]]...]
INFO - root - 2017-12-07 18:40:30.559215: step 39610, loss = 21.05, batch loss = 20.97 (8.4 examples/sec; 0.950 sec/batch; 77h:17m:18s remains)
INFO - root - 2017-12-07 18:40:39.923483: step 39620, loss = 21.46, batch loss = 21.38 (8.7 examples/sec; 0.922 sec/batch; 75h:01m:23s remains)
INFO - root - 2017-12-07 18:40:49.506712: step 39630, loss = 21.40, batch loss = 21.31 (8.7 examples/sec; 0.924 sec/batch; 75h:10m:58s remains)
INFO - root - 2017-12-07 18:40:58.855082: step 39640, loss = 21.80, batch loss = 21.72 (8.9 examples/sec; 0.903 sec/batch; 73h:27m:27s remains)
INFO - root - 2017-12-07 18:41:08.146066: step 39650, loss = 21.87, batch loss = 21.79 (8.8 examples/sec; 0.910 sec/batch; 74h:01m:10s remains)
INFO - root - 2017-12-07 18:41:17.670877: step 39660, loss = 21.65, batch loss = 21.57 (8.5 examples/sec; 0.944 sec/batch; 76h:48m:15s remains)
INFO - root - 2017-12-07 18:41:27.053617: step 39670, loss = 21.80, batch loss = 21.71 (8.5 examples/sec; 0.942 sec/batch; 76h:38m:24s remains)
INFO - root - 2017-12-07 18:41:36.498017: step 39680, loss = 21.80, batch loss = 21.72 (8.4 examples/sec; 0.952 sec/batch; 77h:27m:28s remains)
INFO - root - 2017-12-07 18:41:45.737719: step 39690, loss = 21.60, batch loss = 21.51 (8.4 examples/sec; 0.953 sec/batch; 77h:33m:12s remains)
INFO - root - 2017-12-07 18:41:55.036862: step 39700, loss = 21.38, batch loss = 21.30 (9.1 examples/sec; 0.883 sec/batch; 71h:48m:55s remains)
2017-12-07 18:41:56.037447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8690572 -4.7646112 -4.5541739 -4.3930626 -4.353538 -4.373631 -4.4178042 -4.4326186 -4.3998833 -4.3641853 -4.3731804 -4.4498358 -4.5174823 -4.5297885 -4.5319629][-4.80308 -4.7278128 -4.5525069 -4.4047561 -4.360878 -4.3593163 -4.3608518 -4.3265948 -4.25343 -4.1967859 -4.1934462 -4.25473 -4.3214111 -4.3530946 -4.4115438][-4.6822505 -4.649766 -4.5214453 -4.39839 -4.360796 -4.3568454 -4.3394279 -4.2777958 -4.1790519 -4.1043983 -4.0724506 -4.0993128 -4.151412 -4.1907306 -4.2945056][-4.5531044 -4.5806966 -4.5173378 -4.4264836 -4.3921103 -4.3876023 -4.3578324 -4.2851968 -4.1920943 -4.1343646 -4.1026664 -4.1070633 -4.1431494 -4.1784821 -4.2931685][-4.4546514 -4.5483465 -4.5490136 -4.4830151 -4.4365592 -4.4191093 -4.3683443 -4.277072 -4.1871963 -4.1585383 -4.1687369 -4.199966 -4.2563534 -4.3079257 -4.4230118][-4.4514594 -4.5702252 -4.5997849 -4.527627 -4.4443965 -4.393342 -4.30018 -4.1525116 -4.0139608 -3.9936252 -4.0905666 -4.2186007 -4.342402 -4.4323378 -4.5478497][-4.45536 -4.5768285 -4.6077209 -4.5122237 -4.3882618 -4.2993522 -4.1598849 -3.9277558 -3.6912992 -3.6545217 -3.8653879 -4.1369967 -4.3554368 -4.4828897 -4.5824232][-4.42209 -4.5260224 -4.5368056 -4.4245877 -4.290658 -4.1934848 -4.0272923 -3.7204051 -3.3856606 -3.3281226 -3.6449933 -4.0442648 -4.3441367 -4.4958811 -4.5668163][-4.4040952 -4.475852 -4.4615192 -4.3488936 -4.2316365 -4.1544142 -3.9975643 -3.6786904 -3.3339434 -3.3048515 -3.6794159 -4.1226606 -4.4371061 -4.564435 -4.5669227][-4.4542227 -4.5091405 -4.4824548 -4.379323 -4.28327 -4.2289619 -4.1054177 -3.842083 -3.5783515 -3.6119967 -3.983079 -4.3845105 -4.6465597 -4.7128344 -4.627553][-4.592793 -4.64669 -4.6185474 -4.5195684 -4.4261241 -4.375977 -4.2914457 -4.1158233 -3.9580383 -4.0322495 -4.336401 -4.6371021 -4.8124619 -4.8162103 -4.6877685][-4.7728343 -4.8222432 -4.7949677 -4.6996074 -4.6000314 -4.5444512 -4.4954357 -4.4038482 -4.3324594 -4.4138932 -4.6295853 -4.8183985 -4.886148 -4.8181734 -4.6867032][-4.8714547 -4.903265 -4.8722191 -4.784925 -4.6805139 -4.6116233 -4.5775452 -4.5327516 -4.5069814 -4.5836906 -4.7474217 -4.8657646 -4.8498349 -4.7272682 -4.6139688][-4.833951 -4.839674 -4.7973528 -4.7141767 -4.6077251 -4.519455 -4.4718509 -4.4336324 -4.4243884 -4.5111351 -4.6769786 -4.7828121 -4.7321649 -4.59276 -4.5114093][-4.6950269 -4.667644 -4.6057734 -4.5325356 -4.4529729 -4.3735652 -4.3130279 -4.2628632 -4.2514858 -4.3566551 -4.5493183 -4.6664677 -4.6168156 -4.4926271 -4.4359021]]...]
INFO - root - 2017-12-07 18:42:05.427745: step 39710, loss = 21.56, batch loss = 21.47 (8.0 examples/sec; 1.000 sec/batch; 81h:19m:10s remains)
INFO - root - 2017-12-07 18:42:15.093768: step 39720, loss = 21.76, batch loss = 21.68 (8.2 examples/sec; 0.976 sec/batch; 79h:23m:11s remains)
INFO - root - 2017-12-07 18:42:24.512881: step 39730, loss = 21.57, batch loss = 21.48 (8.1 examples/sec; 0.986 sec/batch; 80h:09m:28s remains)
INFO - root - 2017-12-07 18:42:33.872087: step 39740, loss = 21.65, batch loss = 21.57 (8.3 examples/sec; 0.964 sec/batch; 78h:24m:35s remains)
INFO - root - 2017-12-07 18:42:43.062562: step 39750, loss = 21.67, batch loss = 21.58 (8.6 examples/sec; 0.932 sec/batch; 75h:46m:33s remains)
INFO - root - 2017-12-07 18:42:52.407319: step 39760, loss = 21.11, batch loss = 21.03 (8.6 examples/sec; 0.935 sec/batch; 75h:59m:46s remains)
INFO - root - 2017-12-07 18:43:01.820544: step 39770, loss = 21.83, batch loss = 21.75 (8.6 examples/sec; 0.926 sec/batch; 75h:18m:52s remains)
INFO - root - 2017-12-07 18:43:11.122194: step 39780, loss = 21.78, batch loss = 21.70 (9.0 examples/sec; 0.892 sec/batch; 72h:32m:17s remains)
INFO - root - 2017-12-07 18:43:20.471887: step 39790, loss = 21.53, batch loss = 21.44 (8.5 examples/sec; 0.942 sec/batch; 76h:37m:21s remains)
INFO - root - 2017-12-07 18:43:29.919307: step 39800, loss = 21.42, batch loss = 21.34 (8.3 examples/sec; 0.965 sec/batch; 78h:25m:12s remains)
2017-12-07 18:43:30.873796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.64537 -4.6445293 -4.6518135 -4.7249846 -4.860539 -4.9666228 -4.9948454 -4.9377375 -4.8245463 -4.7295842 -4.670754 -4.6404953 -4.6438279 -4.6690459 -4.6625366][-4.5820751 -4.6254525 -4.6634254 -4.7473507 -4.8691273 -4.9294324 -4.8972225 -4.7836208 -4.6612754 -4.6101823 -4.5979018 -4.5861387 -4.5864148 -4.6115203 -4.6295643][-4.6183791 -4.7109346 -4.7791033 -4.84955 -4.9105959 -4.8751273 -4.7489738 -4.5828428 -4.4932027 -4.5480809 -4.6314888 -4.6596079 -4.6458197 -4.6278868 -4.6233873][-4.7307415 -4.8657556 -4.9542556 -4.997539 -4.9710512 -4.8066244 -4.5615082 -4.3397222 -4.302319 -4.4897318 -4.6990547 -4.7981529 -4.7844343 -4.7115483 -4.6522703][-4.8558464 -5.0043254 -5.08234 -5.0791807 -4.9527159 -4.6547589 -4.2856245 -3.997653 -3.9982321 -4.2998524 -4.6243711 -4.811203 -4.8408465 -4.7581787 -4.6646976][-4.9272842 -5.0437403 -5.075644 -5.0199361 -4.8143873 -4.4183497 -3.94233 -3.5769458 -3.5870872 -3.9606044 -4.36917 -4.6453509 -4.75722 -4.7170048 -4.6289334][-4.8834128 -4.9416285 -4.9215794 -4.8382587 -4.6032686 -4.1703224 -3.6321976 -3.2069664 -3.2174096 -3.6251495 -4.0857658 -4.4428968 -4.6421466 -4.6613703 -4.5947027][-4.76999 -4.774538 -4.7277603 -4.6562719 -4.4501467 -4.0507841 -3.5079887 -3.061306 -3.0729442 -3.4805129 -3.9618852 -4.3785038 -4.6328325 -4.6788025 -4.6127672][-4.6864023 -4.6626663 -4.6201668 -4.5897374 -4.45022 -4.1281505 -3.6317422 -3.2090116 -3.2257202 -3.60661 -4.0792637 -4.5067115 -4.75317 -4.7717037 -4.6744156][-4.66893 -4.6448197 -4.62594 -4.6418443 -4.5710588 -4.3314466 -3.90955 -3.5486622 -3.5718062 -3.9044821 -4.3252196 -4.6936374 -4.8718152 -4.8296189 -4.6930952][-4.6832013 -4.6786804 -4.6924338 -4.75152 -4.744688 -4.5802584 -4.2414169 -3.948997 -3.9643023 -4.2255607 -4.547389 -4.799088 -4.8790121 -4.7813773 -4.6241832][-4.657814 -4.6853275 -4.7342706 -4.8303356 -4.8835897 -4.7921672 -4.5368948 -4.3029575 -4.2943854 -4.4684691 -4.6686306 -4.7897282 -4.7807026 -4.655273 -4.5034413][-4.5837293 -4.6308608 -4.6955171 -4.80611 -4.9002662 -4.8779688 -4.7097983 -4.5283747 -4.4880018 -4.5729184 -4.6703229 -4.7037559 -4.6473331 -4.5246167 -4.3896904][-4.5123429 -4.5412526 -4.5796328 -4.6700592 -4.7809877 -4.8210087 -4.7412348 -4.6095867 -4.5398512 -4.5486422 -4.5797038 -4.5774384 -4.5153785 -4.4137106 -4.2987928][-4.4874821 -4.4587913 -4.43451 -4.4811597 -4.5884662 -4.6724076 -4.6664157 -4.5818486 -4.4988985 -4.4641876 -4.4689445 -4.4672546 -4.4216113 -4.3458672 -4.2518525]]...]
INFO - root - 2017-12-07 18:43:40.347447: step 39810, loss = 21.02, batch loss = 20.94 (8.6 examples/sec; 0.928 sec/batch; 75h:27m:12s remains)
INFO - root - 2017-12-07 18:43:49.769498: step 39820, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.963 sec/batch; 78h:19m:55s remains)
INFO - root - 2017-12-07 18:43:59.208828: step 39830, loss = 21.72, batch loss = 21.63 (8.1 examples/sec; 0.993 sec/batch; 80h:42m:12s remains)
INFO - root - 2017-12-07 18:44:08.749072: step 39840, loss = 21.62, batch loss = 21.54 (8.0 examples/sec; 0.998 sec/batch; 81h:06m:22s remains)
INFO - root - 2017-12-07 18:44:18.085849: step 39850, loss = 21.63, batch loss = 21.54 (8.5 examples/sec; 0.939 sec/batch; 76h:18m:34s remains)
INFO - root - 2017-12-07 18:44:27.486197: step 39860, loss = 21.17, batch loss = 21.09 (9.4 examples/sec; 0.850 sec/batch; 69h:05m:04s remains)
INFO - root - 2017-12-07 18:44:36.886314: step 39870, loss = 21.81, batch loss = 21.73 (8.7 examples/sec; 0.923 sec/batch; 75h:01m:05s remains)
INFO - root - 2017-12-07 18:44:46.295694: step 39880, loss = 21.20, batch loss = 21.12 (8.7 examples/sec; 0.921 sec/batch; 74h:51m:11s remains)
INFO - root - 2017-12-07 18:44:55.597496: step 39890, loss = 21.63, batch loss = 21.54 (8.9 examples/sec; 0.899 sec/batch; 73h:04m:33s remains)
INFO - root - 2017-12-07 18:45:04.957650: step 39900, loss = 21.30, batch loss = 21.22 (8.7 examples/sec; 0.915 sec/batch; 74h:24m:28s remains)
2017-12-07 18:45:05.938988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5526032 -4.6305685 -4.7303586 -4.7809434 -4.7971497 -4.8028617 -4.7822247 -4.70075 -4.5794477 -4.4898968 -4.5048966 -4.5776 -4.6192636 -4.630589 -4.62372][-4.5124931 -4.6023135 -4.711329 -4.7626691 -4.77709 -4.7813663 -4.7545109 -4.661232 -4.5327768 -4.4370384 -4.4505486 -4.5156436 -4.5506735 -4.5744042 -4.5985994][-4.4317861 -4.5283265 -4.6346946 -4.6876082 -4.7064056 -4.7104344 -4.6747317 -4.5826864 -4.4875522 -4.4355855 -4.4721918 -4.5263453 -4.5418324 -4.5627184 -4.5968957][-4.2967577 -4.3987975 -4.5057063 -4.5716524 -4.6094232 -4.6121464 -4.552485 -4.4489532 -4.4005723 -4.4319205 -4.5252428 -4.5822048 -4.578402 -4.5839586 -4.6064448][-4.1527247 -4.2601337 -4.3710942 -4.4517179 -4.50123 -4.4801984 -4.3607054 -4.2088828 -4.1874866 -4.3159266 -4.4970016 -4.5952144 -4.6057696 -4.6111145 -4.6180048][-4.1041088 -4.2078834 -4.3070912 -4.3715215 -4.3869162 -4.2931533 -4.0753117 -3.851377 -3.8440146 -4.070312 -4.3631392 -4.543108 -4.6043067 -4.6281672 -4.61949][-4.1632814 -4.2613335 -4.3312097 -4.3419209 -4.2734957 -4.070797 -3.7460728 -3.4648013 -3.4815807 -3.7957158 -4.1868157 -4.4515204 -4.5706329 -4.61403 -4.589499][-4.303863 -4.4005318 -4.432857 -4.3704195 -4.2071919 -3.9147382 -3.5251458 -3.2195084 -3.2566288 -3.6205218 -4.0659132 -4.3833003 -4.5346384 -4.5787239 -4.5401573][-4.479074 -4.5808911 -4.5846553 -4.4696903 -4.2589355 -3.94165 -3.5492091 -3.2410438 -3.266403 -3.6215622 -4.0726833 -4.4059448 -4.5590606 -4.5854759 -4.5262752][-4.6205978 -4.7290182 -4.734971 -4.6192136 -4.4288821 -4.1587038 -3.8183172 -3.5252604 -3.5088828 -3.8027673 -4.216949 -4.535758 -4.6677909 -4.6624937 -4.5712237][-4.6637359 -4.77592 -4.8156409 -4.7478771 -4.6219344 -4.4330449 -4.1730251 -3.9133546 -3.8553293 -4.0678596 -4.4171815 -4.700861 -4.8030777 -4.7643552 -4.6416039][-4.5994272 -4.7123303 -4.8003011 -4.7964568 -4.7414136 -4.6398506 -4.4720287 -4.2621064 -4.1757464 -4.306879 -4.5782 -4.8144083 -4.8878694 -4.8301158 -4.6948023][-4.4789553 -4.5897422 -4.7144461 -4.760417 -4.76102 -4.7352443 -4.6575632 -4.5094457 -4.4120464 -4.4698391 -4.6632533 -4.8473711 -4.8929834 -4.8285937 -4.7045827][-4.3958054 -4.5010781 -4.6353559 -4.7036629 -4.7356429 -4.7583122 -4.746357 -4.6547632 -4.5592413 -4.5637684 -4.6928205 -4.8318014 -4.8606629 -4.8019671 -4.6994038][-4.397254 -4.4876881 -4.6004095 -4.6626968 -4.6985044 -4.7324877 -4.7468834 -4.6984668 -4.6265893 -4.6134815 -4.7009563 -4.8069496 -4.830852 -4.7830172 -4.6967826]]...]
INFO - root - 2017-12-07 18:45:15.315859: step 39910, loss = 21.69, batch loss = 21.60 (8.7 examples/sec; 0.920 sec/batch; 74h:44m:42s remains)
INFO - root - 2017-12-07 18:45:24.733596: step 39920, loss = 21.71, batch loss = 21.62 (8.7 examples/sec; 0.924 sec/batch; 75h:05m:24s remains)
INFO - root - 2017-12-07 18:45:34.194734: step 39930, loss = 21.25, batch loss = 21.17 (8.6 examples/sec; 0.933 sec/batch; 75h:48m:38s remains)
INFO - root - 2017-12-07 18:45:43.573465: step 39940, loss = 21.70, batch loss = 21.62 (8.9 examples/sec; 0.898 sec/batch; 72h:58m:26s remains)
INFO - root - 2017-12-07 18:45:53.051141: step 39950, loss = 21.71, batch loss = 21.63 (8.6 examples/sec; 0.929 sec/batch; 75h:28m:43s remains)
INFO - root - 2017-12-07 18:46:02.484872: step 39960, loss = 21.71, batch loss = 21.63 (8.0 examples/sec; 0.998 sec/batch; 81h:07m:01s remains)
INFO - root - 2017-12-07 18:46:11.877545: step 39970, loss = 21.35, batch loss = 21.27 (7.8 examples/sec; 1.027 sec/batch; 83h:26m:50s remains)
INFO - root - 2017-12-07 18:46:21.184491: step 39980, loss = 21.39, batch loss = 21.31 (8.5 examples/sec; 0.945 sec/batch; 76h:45m:28s remains)
INFO - root - 2017-12-07 18:46:30.564622: step 39990, loss = 21.01, batch loss = 20.92 (9.2 examples/sec; 0.867 sec/batch; 70h:26m:11s remains)
INFO - root - 2017-12-07 18:46:40.121501: step 40000, loss = 21.70, batch loss = 21.61 (8.3 examples/sec; 0.958 sec/batch; 77h:50m:53s remains)
2017-12-07 18:46:41.120679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8597479 -4.9499249 -4.9930563 -4.9157577 -4.7397509 -4.5886793 -4.5364528 -4.6127481 -4.7114959 -4.7592349 -4.7967067 -4.8022633 -4.7511182 -4.6649928 -4.6043797][-4.8353672 -4.93033 -4.9932194 -4.967454 -4.8450131 -4.6968346 -4.6191797 -4.6747975 -4.781662 -4.8570447 -4.8897643 -4.8667126 -4.7813616 -4.6701035 -4.6126323][-4.7743363 -4.8589129 -4.9246955 -4.9409757 -4.8713894 -4.7346659 -4.62736 -4.6405959 -4.7466774 -4.8602943 -4.9098907 -4.8760004 -4.7636385 -4.6272569 -4.5772285][-4.7060385 -4.7710719 -4.822577 -4.8518324 -4.8080893 -4.6755877 -4.530818 -4.4775376 -4.5591855 -4.7122064 -4.8117909 -4.8091197 -4.6949248 -4.5335188 -4.4794335][-4.6628122 -4.6978035 -4.7111921 -4.7112231 -4.6461444 -4.4950404 -4.3106551 -4.1846766 -4.2353549 -4.4383507 -4.6231189 -4.687995 -4.5861721 -4.3908005 -4.3142195][-4.63709 -4.6439762 -4.6116362 -4.5606403 -4.4472232 -4.263073 -4.0434322 -3.8657484 -3.9072266 -4.1696181 -4.4392519 -4.56113 -4.4626236 -4.2320261 -4.1322994][-4.6242905 -4.6197133 -4.5605597 -4.4694438 -4.3111167 -4.0963154 -3.8420732 -3.6226313 -3.6609859 -3.9648066 -4.2844748 -4.432477 -4.3336606 -4.092041 -3.9860847][-4.6284075 -4.6261253 -4.5648031 -4.4678016 -4.30533 -4.0911756 -3.8166645 -3.5687437 -3.5990882 -3.9015486 -4.2161508 -4.35264 -4.2554684 -4.0335822 -3.9337783][-4.6402545 -4.6508322 -4.6114783 -4.5457397 -4.4312882 -4.2636585 -4.0098486 -3.7739916 -3.789113 -4.032197 -4.2783957 -4.370204 -4.2734518 -4.08848 -3.9994049][-4.6482072 -4.6778398 -4.6727257 -4.648809 -4.5960917 -4.4893146 -4.2866993 -4.091414 -4.0872889 -4.2469945 -4.4055047 -4.4525537 -4.3700447 -4.2332788 -4.1597929][-4.6319895 -4.6789503 -4.6999359 -4.7012982 -4.6878581 -4.6306515 -4.4924393 -4.3566241 -4.3470507 -4.4425025 -4.5445509 -4.5810833 -4.5326385 -4.446898 -4.3948059][-4.5926857 -4.6557388 -4.698071 -4.7148161 -4.7180715 -4.6910338 -4.6126041 -4.5402851 -4.5414672 -4.5986524 -4.6720333 -4.7146697 -4.6999421 -4.6566367 -4.6289167][-4.5583973 -4.6375194 -4.698452 -4.7245383 -4.7237082 -4.7038774 -4.6650691 -4.6436133 -4.6534576 -4.6786957 -4.7263632 -4.7695932 -4.7762318 -4.7657533 -4.7638278][-4.5318956 -4.6182141 -4.6861711 -4.7068038 -4.6837916 -4.6527691 -4.6363492 -4.6556015 -4.6743155 -4.6746759 -4.6966205 -4.7319884 -4.7509694 -4.763782 -4.7798834][-4.514123 -4.5986691 -4.6607604 -4.6634703 -4.6116896 -4.5640841 -4.557126 -4.6045871 -4.630836 -4.6132412 -4.617641 -4.6435032 -4.664279 -4.6814995 -4.6943913]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 18:46:51.031654: step 40010, loss = 21.27, batch loss = 21.18 (8.6 examples/sec; 0.933 sec/batch; 75h:49m:19s remains)
INFO - root - 2017-12-07 18:47:00.469483: step 40020, loss = 21.83, batch loss = 21.75 (8.2 examples/sec; 0.972 sec/batch; 79h:00m:32s remains)
INFO - root - 2017-12-07 18:47:09.850354: step 40030, loss = 21.39, batch loss = 21.31 (8.6 examples/sec; 0.928 sec/batch; 75h:22m:12s remains)
INFO - root - 2017-12-07 18:47:19.043886: step 40040, loss = 21.49, batch loss = 21.41 (9.3 examples/sec; 0.860 sec/batch; 69h:49m:31s remains)
INFO - root - 2017-12-07 18:47:28.380018: step 40050, loss = 21.53, batch loss = 21.45 (9.1 examples/sec; 0.881 sec/batch; 71h:31m:55s remains)
INFO - root - 2017-12-07 18:47:37.606379: step 40060, loss = 21.45, batch loss = 21.36 (8.7 examples/sec; 0.920 sec/batch; 74h:42m:35s remains)
INFO - root - 2017-12-07 18:47:46.985483: step 40070, loss = 21.35, batch loss = 21.27 (8.8 examples/sec; 0.906 sec/batch; 73h:37m:28s remains)
INFO - root - 2017-12-07 18:47:56.468836: step 40080, loss = 21.58, batch loss = 21.50 (8.4 examples/sec; 0.948 sec/batch; 77h:00m:07s remains)
INFO - root - 2017-12-07 18:48:05.896468: step 40090, loss = 21.47, batch loss = 21.39 (8.2 examples/sec; 0.977 sec/batch; 79h:21m:06s remains)
INFO - root - 2017-12-07 18:48:15.470037: step 40100, loss = 21.58, batch loss = 21.50 (8.4 examples/sec; 0.949 sec/batch; 77h:02m:30s remains)
2017-12-07 18:48:16.409701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4623857 -4.44258 -4.4137182 -4.4232535 -4.4229345 -4.3577995 -4.2430172 -4.1798706 -4.2257719 -4.3140697 -4.3952589 -4.4738317 -4.5622025 -4.6499982 -4.7104516][-4.3957086 -4.3882575 -4.3882847 -4.4468117 -4.5079007 -4.4830518 -4.3783469 -4.3039885 -4.3173018 -4.3646793 -4.4253917 -4.4974794 -4.5768151 -4.6464982 -4.7057095][-4.3234382 -4.3579211 -4.3847971 -4.4504266 -4.5257621 -4.5285778 -4.4554243 -4.3946342 -4.3933063 -4.4180012 -4.46031 -4.5157127 -4.5828547 -4.6423182 -4.6966643][-4.2396703 -4.3159189 -4.3497791 -4.3808842 -4.4363294 -4.4521842 -4.4040437 -4.3692617 -4.387166 -4.4243145 -4.4606032 -4.4982328 -4.5566077 -4.6213751 -4.6812358][-4.1881552 -4.2705078 -4.2833261 -4.2598062 -4.2744665 -4.2808361 -4.2316017 -4.2131758 -4.27503 -4.3616471 -4.4173312 -4.4480028 -4.4978461 -4.5650053 -4.6259623][-4.2045474 -4.2655206 -4.2437 -4.1674876 -4.1299191 -4.0953155 -4.0151448 -3.9915683 -4.0974245 -4.249815 -4.3432431 -4.3820066 -4.4258194 -4.4820962 -4.53208][-4.2374778 -4.2627831 -4.215364 -4.1175194 -4.0439687 -3.9496179 -3.8086514 -3.750746 -3.88584 -4.1046681 -4.2396774 -4.2933416 -4.3393927 -4.3907256 -4.4361663][-4.2681422 -4.2542086 -4.2004647 -4.1154294 -4.0440164 -3.9205875 -3.7490759 -3.6653905 -3.7945087 -4.0263519 -4.1608939 -4.2102947 -4.2601638 -4.3118882 -4.3633404][-4.3089886 -4.2681303 -4.218781 -4.1586313 -4.1235313 -4.0354667 -3.9053431 -3.8434405 -3.93999 -4.1169615 -4.1971989 -4.2111778 -4.238852 -4.2681212 -4.3110061][-4.3378196 -4.2874284 -4.2501893 -4.2225108 -4.2329674 -4.2054281 -4.1467381 -4.1261144 -4.1921725 -4.2905016 -4.2928681 -4.2562447 -4.2455525 -4.2397428 -4.258872][-4.3479261 -4.3060932 -4.286375 -4.289959 -4.3322845 -4.3476114 -4.340148 -4.3493571 -4.3890181 -4.4094253 -4.3391824 -4.2638712 -4.2360058 -4.2210541 -4.2269335][-4.3406129 -4.3079572 -4.3041883 -4.3319516 -4.3843403 -4.4127312 -4.4280639 -4.4549713 -4.4845772 -4.4559255 -4.3491845 -4.2609639 -4.2357478 -4.2301207 -4.234973][-4.3493433 -4.3301415 -4.3302851 -4.3589234 -4.4021177 -4.4248109 -4.4439182 -4.4763312 -4.50841 -4.4735465 -4.3740959 -4.29542 -4.271059 -4.2707515 -4.277987][-4.394937 -4.3914328 -4.3866186 -4.3940506 -4.4130917 -4.4252911 -4.4401784 -4.4708323 -4.5063357 -4.4914665 -4.4308472 -4.37747 -4.3491783 -4.3354645 -4.331089][-4.4269357 -4.434248 -4.4212527 -4.4099865 -4.4135094 -4.4225354 -4.4313111 -4.4538884 -4.4875116 -4.4955282 -4.4699025 -4.4376378 -4.4074106 -4.3818469 -4.365355]]...]
INFO - root - 2017-12-07 18:48:25.915932: step 40110, loss = 21.80, batch loss = 21.72 (8.6 examples/sec; 0.929 sec/batch; 75h:29m:04s remains)
INFO - root - 2017-12-07 18:48:35.304454: step 40120, loss = 21.42, batch loss = 21.34 (8.9 examples/sec; 0.894 sec/batch; 72h:37m:13s remains)
INFO - root - 2017-12-07 18:48:44.792619: step 40130, loss = 21.24, batch loss = 21.15 (8.2 examples/sec; 0.973 sec/batch; 79h:02m:22s remains)
INFO - root - 2017-12-07 18:48:54.365816: step 40140, loss = 21.75, batch loss = 21.66 (8.3 examples/sec; 0.963 sec/batch; 78h:11m:47s remains)
INFO - root - 2017-12-07 18:49:03.832889: step 40150, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.936 sec/batch; 76h:01m:19s remains)
INFO - root - 2017-12-07 18:49:13.182023: step 40160, loss = 21.23, batch loss = 21.15 (8.3 examples/sec; 0.962 sec/batch; 78h:07m:39s remains)
INFO - root - 2017-12-07 18:49:22.520625: step 40170, loss = 21.34, batch loss = 21.26 (8.5 examples/sec; 0.943 sec/batch; 76h:32m:10s remains)
INFO - root - 2017-12-07 18:49:31.927517: step 40180, loss = 21.26, batch loss = 21.18 (9.0 examples/sec; 0.891 sec/batch; 72h:21m:18s remains)
INFO - root - 2017-12-07 18:49:41.389740: step 40190, loss = 21.43, batch loss = 21.34 (8.3 examples/sec; 0.966 sec/batch; 78h:26m:24s remains)
INFO - root - 2017-12-07 18:49:50.772310: step 40200, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.929 sec/batch; 75h:24m:00s remains)
2017-12-07 18:49:51.684788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1830125 -4.1683803 -4.1948962 -4.2476487 -4.3100433 -4.3829265 -4.4460611 -4.4829664 -4.5113845 -4.5609236 -4.6344376 -4.6784444 -4.6552296 -4.5768304 -4.5130887][-4.2094617 -4.2207937 -4.2641125 -4.3188086 -4.3730788 -4.4297214 -4.480937 -4.5178509 -4.5570087 -4.6136923 -4.6760707 -4.7008176 -4.6638122 -4.5777712 -4.5031967][-4.2476172 -4.2925997 -4.3596463 -4.4204655 -4.4590182 -4.4787841 -4.4928336 -4.50924 -4.5494204 -4.6144691 -4.6725082 -4.6813169 -4.6376038 -4.5594516 -4.4949884][-4.291925 -4.37071 -4.4686818 -4.5427771 -4.5646405 -4.536366 -4.4867783 -4.445631 -4.4568906 -4.5301204 -4.613462 -4.6395741 -4.6058726 -4.5408888 -4.4922628][-4.3314457 -4.4365454 -4.5583096 -4.6373115 -4.6347222 -4.5533972 -4.4306617 -4.311831 -4.2755547 -4.3613167 -4.5027285 -4.5816221 -4.5673394 -4.502223 -4.4593558][-4.3562646 -4.4710565 -4.5948563 -4.6553273 -4.6123676 -4.4820008 -4.3071628 -4.1405382 -4.0815988 -4.1969719 -4.4077349 -4.5447178 -4.5403733 -4.45129 -4.3932514][-4.3680549 -4.4845381 -4.5939732 -4.6160588 -4.5204659 -4.3466887 -4.1440029 -3.9711387 -3.9280391 -4.0865149 -4.3570747 -4.5375295 -4.5323544 -4.4125619 -4.3315678][-4.3781629 -4.4929543 -4.5835428 -4.5639224 -4.416328 -4.2037058 -3.987479 -3.8358126 -3.8354437 -4.0378618 -4.3474612 -4.5523529 -4.5499234 -4.4228911 -4.3350778][-4.3922172 -4.5045676 -4.5782585 -4.5280938 -4.3499041 -4.1170349 -3.9010262 -3.7811737 -3.8270419 -4.0585141 -4.3789272 -4.5867023 -4.5917578 -4.4775071 -4.3915806][-4.4098392 -4.520473 -4.5801678 -4.5144506 -4.3352981 -4.1148853 -3.923182 -3.8373384 -3.903415 -4.1155643 -4.3885145 -4.5607066 -4.5652514 -4.4744453 -4.4055738][-4.4260774 -4.5326195 -4.5787425 -4.5094972 -4.3564849 -4.1836972 -4.0421162 -3.9868753 -4.0356321 -4.1675682 -4.32701 -4.4229627 -4.4192176 -4.3733845 -4.3560109][-4.4351449 -4.5409164 -4.5791731 -4.5149484 -4.3976903 -4.2796946 -4.1932292 -4.1626053 -4.1775413 -4.206718 -4.2289848 -4.2270713 -4.2056761 -4.2145114 -4.26357][-4.435801 -4.5465217 -4.5892191 -4.5403986 -4.4554157 -4.3755088 -4.3239264 -4.3031726 -4.2861691 -4.2340612 -4.1450849 -4.0464516 -3.9834819 -4.0197854 -4.1202531][-4.4300227 -4.5470228 -4.6075621 -4.5839691 -4.5228238 -4.4573922 -4.4129248 -4.3840952 -4.343926 -4.255826 -4.1158776 -3.9502866 -3.8334372 -3.8558745 -3.9741921][-4.4201932 -4.544591 -4.6300135 -4.6313667 -4.5738177 -4.4961491 -4.4354973 -4.3925619 -4.3478131 -4.2727365 -4.1453919 -3.9648876 -3.8081598 -3.7877276 -3.875716]]...]
INFO - root - 2017-12-07 18:50:00.926308: step 40210, loss = 21.35, batch loss = 21.27 (8.2 examples/sec; 0.970 sec/batch; 78h:44m:10s remains)
INFO - root - 2017-12-07 18:50:10.375169: step 40220, loss = 21.56, batch loss = 21.48 (9.0 examples/sec; 0.887 sec/batch; 72h:00m:53s remains)
INFO - root - 2017-12-07 18:50:19.775389: step 40230, loss = 21.72, batch loss = 21.64 (8.6 examples/sec; 0.929 sec/batch; 75h:26m:12s remains)
INFO - root - 2017-12-07 18:50:29.168905: step 40240, loss = 21.75, batch loss = 21.66 (8.6 examples/sec; 0.933 sec/batch; 75h:43m:52s remains)
INFO - root - 2017-12-07 18:50:38.546700: step 40250, loss = 21.51, batch loss = 21.43 (8.8 examples/sec; 0.912 sec/batch; 74h:02m:51s remains)
INFO - root - 2017-12-07 18:50:47.958518: step 40260, loss = 21.47, batch loss = 21.39 (8.6 examples/sec; 0.932 sec/batch; 75h:39m:35s remains)
INFO - root - 2017-12-07 18:50:57.301772: step 40270, loss = 21.62, batch loss = 21.54 (8.5 examples/sec; 0.942 sec/batch; 76h:29m:53s remains)
INFO - root - 2017-12-07 18:51:06.681337: step 40280, loss = 21.75, batch loss = 21.66 (8.2 examples/sec; 0.978 sec/batch; 79h:23m:06s remains)
INFO - root - 2017-12-07 18:51:16.125274: step 40290, loss = 21.49, batch loss = 21.41 (7.7 examples/sec; 1.035 sec/batch; 83h:58m:15s remains)
INFO - root - 2017-12-07 18:51:25.497538: step 40300, loss = 21.28, batch loss = 21.20 (8.4 examples/sec; 0.949 sec/batch; 77h:03m:47s remains)
2017-12-07 18:51:26.482926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6274681 -4.6898327 -4.724689 -4.7324142 -4.6942158 -4.623456 -4.5619874 -4.5569577 -4.6175294 -4.7090664 -4.7938061 -4.849575 -4.858232 -4.8309908 -4.7805176][-4.6264353 -4.69352 -4.732223 -4.732018 -4.6822386 -4.5921617 -4.5135393 -4.4868708 -4.5355554 -4.6382113 -4.7408442 -4.8220367 -4.855372 -4.8548508 -4.8268743][-4.5322871 -4.6131797 -4.6865425 -4.7126765 -4.6604943 -4.5326142 -4.4103765 -4.3507609 -4.4013572 -4.5416141 -4.672987 -4.7724814 -4.812582 -4.8239212 -4.8229423][-4.4358048 -4.5348225 -4.6536074 -4.7145987 -4.6526108 -4.4705396 -4.2894669 -4.1913595 -4.2479086 -4.4292059 -4.5992422 -4.729136 -4.777389 -4.78844 -4.8035784][-4.4059811 -4.5143833 -4.6536765 -4.7212009 -4.6329169 -4.4050393 -4.1853971 -4.0516615 -4.096025 -4.3008647 -4.5093117 -4.682116 -4.7488871 -4.7461596 -4.7538762][-4.4498992 -4.5454264 -4.67028 -4.7113342 -4.581799 -4.3213067 -4.0818191 -3.9104488 -3.9229977 -4.1388 -4.3920159 -4.6214733 -4.7188148 -4.7016864 -4.6830139][-4.5004582 -4.5784173 -4.6776338 -4.6832542 -4.5085859 -4.2119966 -3.9367743 -3.7139275 -3.7010972 -3.9460471 -4.2607136 -4.5539012 -4.6928787 -4.682786 -4.6470671][-4.5249882 -4.5890718 -4.6591358 -4.6385078 -4.4416895 -4.1315012 -3.8313155 -3.5783968 -3.5634296 -3.8331275 -4.1864252 -4.511045 -4.6776366 -4.6884241 -4.6524291][-4.5447369 -4.5840931 -4.6151366 -4.5831776 -4.4150476 -4.1591735 -3.8942099 -3.6632168 -3.6523242 -3.8892653 -4.2118373 -4.509491 -4.6643891 -4.68583 -4.6560464][-4.5560336 -4.5625143 -4.5543332 -4.5211453 -4.414279 -4.25316 -4.0623865 -3.8867159 -3.8809657 -4.05837 -4.3132324 -4.5415792 -4.6425824 -4.6472864 -4.616817][-4.5333381 -4.5168233 -4.4906287 -4.4667845 -4.4207454 -4.356904 -4.2574697 -4.157042 -4.1653085 -4.2869987 -4.4566708 -4.5856018 -4.6060872 -4.5746946 -4.5342941][-4.4784746 -4.4658871 -4.4501 -4.4419947 -4.43765 -4.4426312 -4.4200468 -4.3844953 -4.4127274 -4.4940529 -4.5849376 -4.6223984 -4.5736389 -4.5043912 -4.45021][-4.4306059 -4.434082 -4.4363303 -4.4438167 -4.4631925 -4.5075059 -4.5400043 -4.5547547 -4.5978084 -4.6501479 -4.68275 -4.6619511 -4.5807323 -4.496274 -4.4405217][-4.4464011 -4.4711647 -4.4892364 -4.5075665 -4.5374713 -4.5955315 -4.65332 -4.691721 -4.7275743 -4.7482553 -4.7397943 -4.6934013 -4.6142349 -4.5415034 -4.5009727][-4.5049353 -4.5548539 -4.5817852 -4.5969257 -4.617775 -4.6627884 -4.7170515 -4.75803 -4.7789283 -4.7750173 -4.7460165 -4.6965141 -4.6355214 -4.586432 -4.5676727]]...]
INFO - root - 2017-12-07 18:51:35.876110: step 40310, loss = 21.37, batch loss = 21.29 (8.7 examples/sec; 0.923 sec/batch; 74h:56m:22s remains)
INFO - root - 2017-12-07 18:51:45.243176: step 40320, loss = 21.65, batch loss = 21.56 (8.4 examples/sec; 0.953 sec/batch; 77h:18m:37s remains)
INFO - root - 2017-12-07 18:51:54.785955: step 40330, loss = 21.52, batch loss = 21.44 (8.8 examples/sec; 0.914 sec/batch; 74h:08m:49s remains)
INFO - root - 2017-12-07 18:52:04.216283: step 40340, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.934 sec/batch; 75h:49m:44s remains)
INFO - root - 2017-12-07 18:52:13.481387: step 40350, loss = 21.49, batch loss = 21.41 (8.3 examples/sec; 0.965 sec/batch; 78h:17m:20s remains)
INFO - root - 2017-12-07 18:52:22.892447: step 40360, loss = 21.44, batch loss = 21.36 (9.0 examples/sec; 0.891 sec/batch; 72h:17m:15s remains)
INFO - root - 2017-12-07 18:52:32.306116: step 40370, loss = 21.37, batch loss = 21.29 (8.4 examples/sec; 0.955 sec/batch; 77h:28m:26s remains)
INFO - root - 2017-12-07 18:52:41.661000: step 40380, loss = 21.60, batch loss = 21.52 (8.6 examples/sec; 0.927 sec/batch; 75h:11m:51s remains)
INFO - root - 2017-12-07 18:52:51.154787: step 40390, loss = 21.21, batch loss = 21.13 (9.2 examples/sec; 0.872 sec/batch; 70h:46m:52s remains)
INFO - root - 2017-12-07 18:53:00.517108: step 40400, loss = 21.54, batch loss = 21.45 (8.6 examples/sec; 0.935 sec/batch; 75h:49m:28s remains)
2017-12-07 18:53:01.475277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6021795 -4.5396338 -4.4899936 -4.4932823 -4.5653229 -4.6518884 -4.68745 -4.6673932 -4.6101608 -4.5069852 -4.4211512 -4.3990006 -4.3739414 -4.3390093 -4.3124871][-4.6934381 -4.6235785 -4.5448456 -4.5154982 -4.5610871 -4.6358414 -4.6869645 -4.69569 -4.6641183 -4.5778346 -4.4897628 -4.4506354 -4.4226975 -4.3952127 -4.36329][-4.7547073 -4.6900978 -4.600491 -4.5386157 -4.5349474 -4.5669432 -4.6065512 -4.6344619 -4.642148 -4.6033063 -4.539403 -4.4975715 -4.4723887 -4.4532 -4.4110703][-4.7692547 -4.7154832 -4.6329865 -4.5523605 -4.4924331 -4.4541736 -4.4437218 -4.4641004 -4.503757 -4.52489 -4.5179324 -4.5080924 -4.5024333 -4.49356 -4.4440107][-4.7187977 -4.677774 -4.6058397 -4.511848 -4.3968353 -4.2849679 -4.2155614 -4.2166486 -4.2763839 -4.3478665 -4.4105263 -4.4567327 -4.4845481 -4.4918537 -4.4497046][-4.6329927 -4.6033435 -4.5283566 -4.40463 -4.2267661 -4.0536871 -3.9484859 -3.950068 -4.0389919 -4.1544089 -4.273169 -4.3701634 -4.4314013 -4.462904 -4.4490423][-4.5651317 -4.54275 -4.4604287 -4.3068995 -4.0793953 -3.8694122 -3.7472782 -3.7535563 -3.8700802 -4.0098753 -4.1497569 -4.2673111 -4.3446956 -4.3990431 -4.4245481][-4.522254 -4.5082707 -4.4283066 -4.2698417 -4.0361233 -3.8317935 -3.7206795 -3.7333794 -3.8559337 -3.9834068 -4.1007361 -4.203444 -4.2664447 -4.316442 -4.3601246][-4.4933372 -4.4853034 -4.4164577 -4.2760363 -4.0723028 -3.9086092 -3.8416777 -3.877321 -3.9962287 -4.0918307 -4.1617723 -4.22812 -4.259901 -4.2806 -4.3074365][-4.5049052 -4.5095181 -4.4556704 -4.3400812 -4.1742854 -4.0435896 -4.0021582 -4.043396 -4.1414323 -4.2049179 -4.2382274 -4.2803125 -4.3021021 -4.3093324 -4.315196][-4.5456848 -4.5681167 -4.5310197 -4.4430113 -4.3187318 -4.2175722 -4.1837673 -4.2067351 -4.2677045 -4.3024578 -4.30979 -4.3333969 -4.3594279 -4.3734231 -4.3711219][-4.556354 -4.602221 -4.5885773 -4.5349374 -4.461904 -4.3975916 -4.3624554 -4.3455472 -4.3539872 -4.361392 -4.357008 -4.3705759 -4.4037895 -4.4271178 -4.4217062][-4.5428939 -4.613699 -4.6272697 -4.6100688 -4.5811038 -4.5394979 -4.4818354 -4.4090071 -4.35525 -4.3281813 -4.3209887 -4.3431959 -4.3973708 -4.4413919 -4.4472642][-4.524786 -4.5926719 -4.6228719 -4.6344547 -4.6324835 -4.6002536 -4.5293903 -4.4290171 -4.3375869 -4.2798905 -4.2653933 -4.2970972 -4.3663836 -4.42613 -4.4443159][-4.5118279 -4.5480657 -4.5688596 -4.5855432 -4.589993 -4.5643725 -4.5035081 -4.4180307 -4.3310504 -4.2677193 -4.2500653 -4.2840209 -4.3476849 -4.40128 -4.4182835]]...]
INFO - root - 2017-12-07 18:53:11.054049: step 40410, loss = 21.83, batch loss = 21.75 (8.1 examples/sec; 0.983 sec/batch; 79h:44m:20s remains)
INFO - root - 2017-12-07 18:53:20.531479: step 40420, loss = 21.64, batch loss = 21.56 (8.4 examples/sec; 0.949 sec/batch; 77h:01m:48s remains)
INFO - root - 2017-12-07 18:53:30.065509: step 40430, loss = 21.56, batch loss = 21.47 (8.4 examples/sec; 0.954 sec/batch; 77h:23m:07s remains)
INFO - root - 2017-12-07 18:53:39.583567: step 40440, loss = 21.77, batch loss = 21.69 (9.0 examples/sec; 0.888 sec/batch; 72h:00m:33s remains)
INFO - root - 2017-12-07 18:53:49.028303: step 40450, loss = 21.14, batch loss = 21.05 (8.8 examples/sec; 0.907 sec/batch; 73h:34m:46s remains)
INFO - root - 2017-12-07 18:53:58.452109: step 40460, loss = 21.92, batch loss = 21.84 (8.4 examples/sec; 0.948 sec/batch; 76h:52m:22s remains)
INFO - root - 2017-12-07 18:54:07.857023: step 40470, loss = 21.47, batch loss = 21.39 (8.6 examples/sec; 0.928 sec/batch; 75h:14m:35s remains)
INFO - root - 2017-12-07 18:54:17.307268: step 40480, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.933 sec/batch; 75h:39m:49s remains)
INFO - root - 2017-12-07 18:54:26.769018: step 40490, loss = 21.44, batch loss = 21.35 (8.5 examples/sec; 0.938 sec/batch; 76h:05m:51s remains)
INFO - root - 2017-12-07 18:54:36.170194: step 40500, loss = 21.27, batch loss = 21.19 (8.4 examples/sec; 0.956 sec/batch; 77h:32m:13s remains)
2017-12-07 18:54:37.167575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8784869 -3.909929 -3.964457 -4.04135 -4.1540289 -4.2886209 -4.417305 -4.5002685 -4.531981 -4.5174 -4.4551497 -4.3661323 -4.2937675 -4.25566 -4.2357645][-4.0236058 -4.0539589 -4.1096048 -4.1968088 -4.305716 -4.4226594 -4.5416818 -4.6254296 -4.6476336 -4.6223636 -4.5646715 -4.4990368 -4.4431849 -4.3953819 -4.3516641][-4.19388 -4.2129321 -4.2608085 -4.3549767 -4.4652653 -4.5640478 -4.645822 -4.6889548 -4.6816659 -4.65345 -4.6247168 -4.6061149 -4.584064 -4.5397596 -4.478539][-4.3783369 -4.3957877 -4.4309473 -4.4954748 -4.5637894 -4.6149659 -4.6386256 -4.6317635 -4.608201 -4.6007442 -4.62073 -4.6551833 -4.667872 -4.6322775 -4.5597239][-4.5326543 -4.5642891 -4.5892119 -4.5953507 -4.5688257 -4.52105 -4.4696412 -4.4330192 -4.4280744 -4.4660659 -4.5369763 -4.6103377 -4.6476164 -4.6282878 -4.5690227][-4.594336 -4.6260514 -4.6335282 -4.5793381 -4.4477234 -4.2876205 -4.1677127 -4.1292048 -4.1725335 -4.2737665 -4.39807 -4.50246 -4.5564919 -4.5581541 -4.5259438][-4.5709205 -4.5808077 -4.5569897 -4.4526381 -4.2429161 -4.004168 -3.8454196 -3.8200207 -3.9151127 -4.0749903 -4.2453723 -4.3779068 -4.4445724 -4.4597726 -4.4437294][-4.487165 -4.4876094 -4.4502153 -4.3271265 -4.0896282 -3.8219166 -3.6490662 -3.6261315 -3.7410252 -3.9193692 -4.1061378 -4.25608 -4.3344216 -4.3583 -4.3467345][-4.4020839 -4.4237728 -4.4080005 -4.3057642 -4.0855026 -3.8250988 -3.6479871 -3.6139905 -3.7184942 -3.8865809 -4.0684648 -4.2216053 -4.301826 -4.3178916 -4.2882867][-4.3704295 -4.4303374 -4.4589076 -4.4004593 -4.2218628 -3.9888649 -3.8152864 -3.76576 -3.8483903 -3.9975936 -4.1616077 -4.2978783 -4.3615785 -4.3580966 -4.3088408][-4.3932042 -4.4831648 -4.5478582 -4.5331154 -4.4077978 -4.2219577 -4.0750556 -4.0266714 -4.0899839 -4.2146473 -4.3495865 -4.447042 -4.4756293 -4.4516325 -4.3970757][-4.4207535 -4.5139694 -4.591145 -4.6107917 -4.5484815 -4.4356923 -4.344523 -4.3189774 -4.3715329 -4.4725256 -4.57514 -4.631144 -4.622797 -4.5762267 -4.5117512][-4.4253511 -4.5079994 -4.5826325 -4.6240711 -4.6172924 -4.577261 -4.54234 -4.5361595 -4.5690231 -4.6309052 -4.6915402 -4.7160621 -4.6926947 -4.6375604 -4.5654373][-4.4009471 -4.4657745 -4.5251355 -4.5671792 -4.5892959 -4.5984635 -4.6047015 -4.612566 -4.6253815 -4.6448956 -4.6621637 -4.6620626 -4.6349311 -4.5810289 -4.510108][-4.3395853 -4.3750825 -4.4104247 -4.4399319 -4.4657865 -4.4905753 -4.5129342 -4.5290341 -4.5366464 -4.538579 -4.5355434 -4.5240393 -4.4975014 -4.4520369 -4.39536]]...]
INFO - root - 2017-12-07 18:54:46.628018: step 40510, loss = 21.47, batch loss = 21.38 (8.2 examples/sec; 0.975 sec/batch; 79h:03m:54s remains)
INFO - root - 2017-12-07 18:54:55.977656: step 40520, loss = 21.59, batch loss = 21.51 (8.3 examples/sec; 0.965 sec/batch; 78h:17m:22s remains)
INFO - root - 2017-12-07 18:55:05.392595: step 40530, loss = 21.65, batch loss = 21.57 (8.8 examples/sec; 0.913 sec/batch; 74h:03m:18s remains)
INFO - root - 2017-12-07 18:55:14.947289: step 40540, loss = 21.34, batch loss = 21.26 (8.0 examples/sec; 0.999 sec/batch; 81h:03m:09s remains)
INFO - root - 2017-12-07 18:55:24.201884: step 40550, loss = 21.12, batch loss = 21.04 (7.6 examples/sec; 1.048 sec/batch; 84h:59m:37s remains)
INFO - root - 2017-12-07 18:55:33.616134: step 40560, loss = 21.32, batch loss = 21.23 (8.3 examples/sec; 0.961 sec/batch; 77h:56m:04s remains)
INFO - root - 2017-12-07 18:55:42.906991: step 40570, loss = 21.25, batch loss = 21.16 (8.5 examples/sec; 0.945 sec/batch; 76h:39m:43s remains)
INFO - root - 2017-12-07 18:55:52.325746: step 40580, loss = 21.17, batch loss = 21.09 (8.7 examples/sec; 0.923 sec/batch; 74h:50m:50s remains)
INFO - root - 2017-12-07 18:56:01.666853: step 40590, loss = 21.38, batch loss = 21.30 (8.6 examples/sec; 0.935 sec/batch; 75h:47m:02s remains)
INFO - root - 2017-12-07 18:56:10.964685: step 40600, loss = 21.44, batch loss = 21.35 (8.5 examples/sec; 0.941 sec/batch; 76h:19m:07s remains)
2017-12-07 18:56:11.930475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4095392 -4.4102387 -4.4143209 -4.4445953 -4.4937592 -4.5264783 -4.5237694 -4.5014224 -4.4748182 -4.4425263 -4.396183 -4.3329563 -4.2661209 -4.220108 -4.2071533][-4.4036465 -4.4089131 -4.4252753 -4.4712305 -4.5303879 -4.55692 -4.5336375 -4.4926958 -4.4685745 -4.4608312 -4.439827 -4.3803878 -4.2905312 -4.2126718 -4.1808128][-4.378531 -4.3848076 -4.4086208 -4.4576054 -4.5042067 -4.4988427 -4.4371357 -4.3771009 -4.3718934 -4.4145722 -4.4476995 -4.4229121 -4.3386045 -4.2447309 -4.1962476][-4.351162 -4.35923 -4.3891754 -4.4326763 -4.4473004 -4.3846512 -4.261734 -4.1707473 -4.1891518 -4.2958794 -4.3990645 -4.4327168 -4.3891478 -4.3108768 -4.2583108][-4.3424907 -4.3571253 -4.3920274 -4.4237108 -4.3949976 -4.2643127 -4.0739107 -3.9487803 -3.9893756 -4.1536345 -4.3168807 -4.4065523 -4.4199843 -4.3848515 -4.3436012][-4.3715024 -4.3950844 -4.4311461 -4.4468985 -4.3709311 -4.1731811 -3.9202826 -3.7582498 -3.8044164 -4.0032768 -4.2072396 -4.341362 -4.4107184 -4.4288912 -4.4092674][-4.40233 -4.4267163 -4.4604635 -4.4651818 -4.3568993 -4.1144061 -3.8195281 -3.6233459 -3.652977 -3.8563879 -4.0841951 -4.2557821 -4.37184 -4.4370756 -4.4411035][-4.4056277 -4.4186625 -4.4432955 -4.4415507 -4.3265367 -4.0796919 -3.7875428 -3.5914531 -3.6091347 -3.7972755 -4.0239286 -4.2071352 -4.3405185 -4.425446 -4.4422746][-4.3993945 -4.40235 -4.4203725 -4.4229574 -4.3311939 -4.1238618 -3.879225 -3.7181506 -3.7343042 -3.8862822 -4.07117 -4.2214756 -4.3341274 -4.4095125 -4.4257817][-4.4001155 -4.4055629 -4.4286914 -4.4462204 -4.3926992 -4.241972 -4.0598021 -3.9493184 -3.9722137 -4.0828772 -4.2011504 -4.2850885 -4.3478489 -4.396277 -4.4113111][-4.4185524 -4.4366393 -4.4690514 -4.5009604 -4.4814763 -4.3809075 -4.2527609 -4.1855578 -4.2151012 -4.2910891 -4.3469529 -4.3653407 -4.375474 -4.3955684 -4.414279][-4.454742 -4.4832935 -4.52047 -4.5619879 -4.5664616 -4.4954195 -4.3903856 -4.3326273 -4.3518047 -4.3997436 -4.4211845 -4.4100513 -4.3966212 -4.40385 -4.4275737][-4.4615316 -4.4885249 -4.5195565 -4.5654974 -4.5881591 -4.5419087 -4.4555879 -4.4016881 -4.4104757 -4.4426455 -4.4540439 -4.440187 -4.4235668 -4.4247127 -4.443665][-4.4403343 -4.4468346 -4.4610896 -4.5103297 -4.5564137 -4.548564 -4.50156 -4.4688807 -4.4763508 -4.4952703 -4.496346 -4.4792724 -4.4575334 -4.4448433 -4.4440532][-4.4121413 -4.3891916 -4.38602 -4.4432211 -4.5171566 -4.5514293 -4.5483432 -4.5438452 -4.5547414 -4.55855 -4.540009 -4.5075021 -4.4700074 -4.437726 -4.4166675]]...]
INFO - root - 2017-12-07 18:56:21.299843: step 40610, loss = 21.40, batch loss = 21.32 (8.6 examples/sec; 0.930 sec/batch; 75h:26m:42s remains)
INFO - root - 2017-12-07 18:56:30.769202: step 40620, loss = 21.28, batch loss = 21.19 (8.8 examples/sec; 0.913 sec/batch; 74h:02m:06s remains)
INFO - root - 2017-12-07 18:56:40.121082: step 40630, loss = 22.13, batch loss = 22.04 (8.6 examples/sec; 0.931 sec/batch; 75h:28m:26s remains)
INFO - root - 2017-12-07 18:56:49.367687: step 40640, loss = 21.23, batch loss = 21.15 (8.3 examples/sec; 0.966 sec/batch; 78h:20m:45s remains)
INFO - root - 2017-12-07 18:56:58.568398: step 40650, loss = 21.52, batch loss = 21.44 (8.8 examples/sec; 0.912 sec/batch; 73h:54m:53s remains)
INFO - root - 2017-12-07 18:57:08.016996: step 40660, loss = 21.14, batch loss = 21.06 (8.8 examples/sec; 0.904 sec/batch; 73h:17m:43s remains)
INFO - root - 2017-12-07 18:57:17.426359: step 40670, loss = 21.96, batch loss = 21.88 (8.7 examples/sec; 0.922 sec/batch; 74h:43m:42s remains)
INFO - root - 2017-12-07 18:57:26.965436: step 40680, loss = 21.62, batch loss = 21.54 (8.3 examples/sec; 0.964 sec/batch; 78h:10m:53s remains)
INFO - root - 2017-12-07 18:57:36.355650: step 40690, loss = 21.77, batch loss = 21.68 (8.2 examples/sec; 0.981 sec/batch; 79h:31m:49s remains)
INFO - root - 2017-12-07 18:57:45.746936: step 40700, loss = 21.64, batch loss = 21.56 (8.8 examples/sec; 0.914 sec/batch; 74h:05m:30s remains)
2017-12-07 18:57:46.805086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4962039 -4.5041528 -4.5124855 -4.52235 -4.532486 -4.5426645 -4.554863 -4.5691667 -4.5800371 -4.5829349 -4.5764003 -4.5629835 -4.5367165 -4.5056729 -4.4888983][-4.5047951 -4.5215721 -4.538372 -4.5565367 -4.573205 -4.5870819 -4.5996141 -4.6117516 -4.619267 -4.6169147 -4.603096 -4.580996 -4.5462379 -4.5080523 -4.4831862][-4.5031924 -4.5300674 -4.554049 -4.5778012 -4.5965328 -4.6074104 -4.6113148 -4.6127396 -4.6112704 -4.6041031 -4.5896654 -4.5689855 -4.5335555 -4.4854484 -4.4416347][-4.4793377 -4.5115929 -4.5361838 -4.5578833 -4.5719519 -4.5729723 -4.5610433 -4.5470848 -4.5382309 -4.5351992 -4.5362043 -4.5349007 -4.5119944 -4.4575715 -4.3883595][-4.4451308 -4.4788241 -4.5005374 -4.5119805 -4.5095797 -4.4874325 -4.4521656 -4.4259281 -4.4218125 -4.4357457 -4.46128 -4.4881153 -4.4888415 -4.4412875 -4.3586183][-4.4321356 -4.4697447 -4.4887667 -4.4823318 -4.4465961 -4.3802547 -4.3068333 -4.2681675 -4.2848382 -4.3302145 -4.3818083 -4.433177 -4.4597735 -4.4343929 -4.3634973][-4.4343204 -4.4764223 -4.4922504 -4.4581838 -4.3693438 -4.2344327 -4.1021547 -4.0441871 -4.0939527 -4.1901846 -4.2773914 -4.3525558 -4.4029155 -4.4046774 -4.3646116][-4.4271464 -4.4709239 -4.4759388 -4.4046683 -4.2546954 -4.0461922 -3.8454764 -3.7581334 -3.8461938 -4.0162344 -4.1592388 -4.260704 -4.3202338 -4.3332653 -4.3245811][-4.4121466 -4.4566941 -4.4477115 -4.3427191 -4.1482668 -3.893636 -3.6466537 -3.5344322 -3.6548903 -3.8992429 -4.1022387 -4.2190609 -4.2616663 -4.2578983 -4.26043][-4.380672 -4.4245815 -4.4075794 -4.2892146 -4.0875921 -3.8359962 -3.5917611 -3.4785891 -3.6092095 -3.8836784 -4.1117239 -4.2212605 -4.2315288 -4.1967068 -4.1867766][-4.3549638 -4.4021726 -4.394505 -4.2962604 -4.1266804 -3.916167 -3.7077129 -3.6106334 -3.7277308 -3.9761322 -4.1802526 -4.25498 -4.2229571 -4.1518059 -4.1132064][-4.3748789 -4.4243808 -4.4371223 -4.3803687 -4.2679176 -4.1175404 -3.9580379 -3.8776085 -3.9539609 -4.1262155 -4.2652888 -4.2950821 -4.2297835 -4.1337261 -4.0680137][-4.434473 -4.4838939 -4.517539 -4.5019031 -4.4422784 -4.3475471 -4.2347822 -4.1667523 -4.1908884 -4.2680087 -4.3293576 -4.3192625 -4.2372479 -4.1318617 -4.0556283][-4.50049 -4.5492897 -4.5933843 -4.6021347 -4.5746889 -4.5194659 -4.449224 -4.4019823 -4.3971653 -4.4066119 -4.4056029 -4.36491 -4.2767363 -4.1751075 -4.1036415][-4.5471034 -4.5943236 -4.6383634 -4.6588216 -4.6521754 -4.626977 -4.5936527 -4.5711765 -4.5587282 -4.5363221 -4.5017762 -4.4456396 -4.3597822 -4.2672615 -4.2023726]]...]
INFO - root - 2017-12-07 18:57:56.386326: step 40710, loss = 21.59, batch loss = 21.50 (8.2 examples/sec; 0.978 sec/batch; 79h:16m:07s remains)
INFO - root - 2017-12-07 18:58:05.744959: step 40720, loss = 21.28, batch loss = 21.20 (7.9 examples/sec; 1.013 sec/batch; 82h:05m:15s remains)
INFO - root - 2017-12-07 18:58:15.056945: step 40730, loss = 21.41, batch loss = 21.33 (7.9 examples/sec; 1.015 sec/batch; 82h:13m:57s remains)
INFO - root - 2017-12-07 18:58:24.457506: step 40740, loss = 21.43, batch loss = 21.35 (8.1 examples/sec; 0.984 sec/batch; 79h:46m:11s remains)
INFO - root - 2017-12-07 18:58:33.943057: step 40750, loss = 21.58, batch loss = 21.50 (8.3 examples/sec; 0.962 sec/batch; 77h:57m:16s remains)
INFO - root - 2017-12-07 18:58:43.349097: step 40760, loss = 21.28, batch loss = 21.19 (8.7 examples/sec; 0.916 sec/batch; 74h:13m:41s remains)
INFO - root - 2017-12-07 18:58:52.893229: step 40770, loss = 21.48, batch loss = 21.40 (8.5 examples/sec; 0.942 sec/batch; 76h:18m:03s remains)
INFO - root - 2017-12-07 18:59:02.048170: step 40780, loss = 21.02, batch loss = 20.94 (9.3 examples/sec; 0.862 sec/batch; 69h:53m:10s remains)
INFO - root - 2017-12-07 18:59:11.437527: step 40790, loss = 21.55, batch loss = 21.47 (8.6 examples/sec; 0.936 sec/batch; 75h:49m:03s remains)
INFO - root - 2017-12-07 18:59:20.958522: step 40800, loss = 21.17, batch loss = 21.09 (8.4 examples/sec; 0.956 sec/batch; 77h:28m:39s remains)
2017-12-07 18:59:21.919033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2782583 -4.286881 -4.3273563 -4.3373151 -4.3203759 -4.3315959 -4.3383293 -4.3162184 -4.2966104 -4.3015103 -4.2972536 -4.266541 -4.2070909 -4.1708 -4.2016482][-4.2851539 -4.2760525 -4.3119197 -4.3312516 -4.3176889 -4.3403072 -4.3595533 -4.3445735 -4.3111858 -4.2937021 -4.2820206 -4.2530375 -4.1843057 -4.1287174 -4.1500506][-4.3511353 -4.3370075 -4.361001 -4.3751097 -4.3530421 -4.37579 -4.4078135 -4.411901 -4.3786721 -4.3435493 -4.3156667 -4.2769628 -4.1958842 -4.122261 -4.1326261][-4.4150195 -4.3979812 -4.4123135 -4.4262362 -4.3995371 -4.4114861 -4.4434414 -4.4591007 -4.4337068 -4.3940377 -4.3532019 -4.3061881 -4.2226262 -4.1453142 -4.1477284][-4.3839498 -4.3736496 -4.3870525 -4.4051504 -4.3768568 -4.3761683 -4.4008942 -4.418478 -4.4005432 -4.3661113 -4.3259792 -4.2822804 -4.21554 -4.1555104 -4.1573143][-4.2442322 -4.2482209 -4.2711225 -4.2865825 -4.2432151 -4.21953 -4.2276268 -4.2402329 -4.2299995 -4.2126379 -4.196733 -4.1839266 -4.1627817 -4.1491132 -4.16437][-4.1317959 -4.1440759 -4.172771 -4.165462 -4.0787544 -4.0141306 -4.0016165 -4.01707 -4.0224915 -4.0331855 -4.06121 -4.0965328 -4.1328611 -4.174108 -4.2093577][-4.1433239 -4.1654458 -4.1896439 -4.14189 -4.0015211 -3.8899851 -3.8654256 -3.8974178 -3.9243267 -3.9558179 -4.0155964 -4.0862818 -4.1653624 -4.2431388 -4.2881145][-4.2205396 -4.2615156 -4.2813015 -4.2044692 -4.0437984 -3.9118052 -3.8886478 -3.9403896 -3.9765289 -4.0028458 -4.0594587 -4.1326017 -4.2212024 -4.3117962 -4.35747][-4.2897425 -4.3505988 -4.3771839 -4.3126173 -4.1850843 -4.0743456 -4.0546279 -4.1031041 -4.1273565 -4.1269207 -4.1494951 -4.2016511 -4.2770982 -4.3598495 -4.4043355][-4.3496809 -4.4196734 -4.4634781 -4.4419732 -4.373219 -4.3033934 -4.2803249 -4.2966142 -4.2940545 -4.2639008 -4.2517586 -4.2831025 -4.3409066 -4.4043136 -4.4413176][-4.3581605 -4.4347787 -4.4954157 -4.51214 -4.4855886 -4.4417253 -4.4073734 -4.3883061 -4.3633251 -4.3190694 -4.2899055 -4.3178434 -4.3749127 -4.4327769 -4.464016][-4.3048153 -4.3691735 -4.4251122 -4.4531837 -4.4434166 -4.4112573 -4.3722796 -4.3382845 -4.3086634 -4.2658224 -4.2263961 -4.2480035 -4.3109064 -4.383976 -4.4266787][-4.26875 -4.2951322 -4.32479 -4.3468962 -4.34803 -4.3252873 -4.2902074 -4.2579556 -4.2268567 -4.1810489 -4.12582 -4.1250043 -4.1807423 -4.2713914 -4.3458414][-4.2630076 -4.2538013 -4.2573438 -4.2794318 -4.3002238 -4.2901273 -4.2587357 -4.2254472 -4.1865225 -4.1335073 -4.0633831 -4.0344973 -4.0711045 -4.1664591 -4.2692094]]...]
INFO - root - 2017-12-07 18:59:31.352910: step 40810, loss = 21.20, batch loss = 21.12 (8.6 examples/sec; 0.930 sec/batch; 75h:20m:12s remains)
INFO - root - 2017-12-07 18:59:40.804979: step 40820, loss = 21.45, batch loss = 21.37 (8.3 examples/sec; 0.963 sec/batch; 78h:03m:32s remains)
INFO - root - 2017-12-07 18:59:50.219071: step 40830, loss = 21.47, batch loss = 21.39 (8.2 examples/sec; 0.979 sec/batch; 79h:19m:02s remains)
INFO - root - 2017-12-07 18:59:59.675157: step 40840, loss = 21.43, batch loss = 21.34 (8.1 examples/sec; 0.993 sec/batch; 80h:29m:05s remains)
INFO - root - 2017-12-07 19:00:08.911999: step 40850, loss = 21.59, batch loss = 21.50 (8.5 examples/sec; 0.943 sec/batch; 76h:23m:51s remains)
INFO - root - 2017-12-07 19:00:18.296188: step 40860, loss = 21.27, batch loss = 21.19 (8.6 examples/sec; 0.928 sec/batch; 75h:12m:56s remains)
INFO - root - 2017-12-07 19:00:27.787834: step 40870, loss = 21.44, batch loss = 21.35 (8.5 examples/sec; 0.944 sec/batch; 76h:29m:01s remains)
INFO - root - 2017-12-07 19:00:36.906141: step 40880, loss = 21.62, batch loss = 21.54 (8.9 examples/sec; 0.903 sec/batch; 73h:07m:41s remains)
INFO - root - 2017-12-07 19:00:46.266384: step 40890, loss = 21.33, batch loss = 21.24 (8.1 examples/sec; 0.984 sec/batch; 79h:43m:18s remains)
INFO - root - 2017-12-07 19:00:55.376707: step 40900, loss = 21.56, batch loss = 21.47 (8.7 examples/sec; 0.918 sec/batch; 74h:21m:32s remains)
2017-12-07 19:00:56.353878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3854852 -4.4176116 -4.4374981 -4.4547815 -4.4912629 -4.5053926 -4.49244 -4.4807725 -4.4825807 -4.4808893 -4.4576092 -4.4194665 -4.3925037 -4.4106846 -4.4338474][-4.4491353 -4.4823265 -4.5123796 -4.5498643 -4.6024151 -4.6245165 -4.61647 -4.6036916 -4.5995541 -4.5921025 -4.5560389 -4.4973717 -4.4454365 -4.44664 -4.4687839][-4.5592337 -4.5849538 -4.6051846 -4.6326232 -4.669075 -4.679739 -4.6684175 -4.6519189 -4.6456614 -4.6400819 -4.5993443 -4.5266929 -4.4610157 -4.4574823 -4.4957232][-4.6372166 -4.6463842 -4.6440482 -4.6394067 -4.6369004 -4.6244578 -4.6090636 -4.5972805 -4.6021309 -4.6056724 -4.5636892 -4.479661 -4.4045439 -4.4017329 -4.4572382][-4.6238456 -4.624095 -4.608077 -4.5739784 -4.53461 -4.49708 -4.4761286 -4.4685397 -4.4845204 -4.5033817 -4.4722981 -4.3917303 -4.3116245 -4.2964025 -4.34856][-4.4984946 -4.5073481 -4.4907436 -4.443409 -4.38747 -4.3337507 -4.2953143 -4.270112 -4.2811165 -4.3157935 -4.3188734 -4.26993 -4.1942482 -4.1589079 -4.193778][-4.2862844 -4.3181081 -4.3158145 -4.2807446 -4.2416921 -4.191936 -4.1256948 -4.0541644 -4.036252 -4.0850368 -4.1380649 -4.1382551 -4.0794783 -4.0311379 -4.0504241][-4.07276 -4.1270795 -4.1396003 -4.1303706 -4.1272492 -4.09173 -3.9903982 -3.8576248 -3.8031406 -3.8684971 -3.9729297 -4.0255156 -3.9990559 -3.9604368 -3.9812906][-3.9092252 -3.9670322 -3.9881909 -4.0139484 -4.0630307 -4.0590553 -3.93889 -3.7552891 -3.6659725 -3.7383103 -3.8802125 -3.9809577 -3.99865 -3.9887586 -4.0203891][-3.8703663 -3.9224007 -3.9473963 -3.9990008 -4.0981617 -4.1462717 -4.05037 -3.8641798 -3.7583251 -3.8120968 -3.9504471 -4.0727296 -4.1277056 -4.1390066 -4.1601548][-4.00733 -4.0400763 -4.0468163 -4.091733 -4.2081847 -4.3003216 -4.2506614 -4.1028256 -4.0083 -4.0328512 -4.1345382 -4.2457652 -4.3120089 -4.3297834 -4.3352647][-4.2065578 -4.2187686 -4.2025685 -4.227952 -4.3363872 -4.443018 -4.4307976 -4.3386588 -4.2800555 -4.2883868 -4.3461351 -4.4179745 -4.4609389 -4.4709005 -4.4653358][-4.3720469 -4.3715754 -4.3474088 -4.3640451 -4.4543381 -4.5467663 -4.548193 -4.5034804 -4.4928727 -4.5091672 -4.5362082 -4.5576138 -4.5538673 -4.5443974 -4.5360985][-4.4378762 -4.4351225 -4.4163737 -4.4338965 -4.5052977 -4.5716419 -4.5688658 -4.5496092 -4.574286 -4.6051083 -4.6160479 -4.5981789 -4.5576406 -4.5384483 -4.5410542][-4.4243789 -4.4233518 -4.4180021 -4.4431982 -4.5014877 -4.5430059 -4.5281792 -4.5128965 -4.5440083 -4.5762377 -4.5796342 -4.5455036 -4.4938197 -4.4778862 -4.4896269]]...]
INFO - root - 2017-12-07 19:01:05.636708: step 40910, loss = 21.38, batch loss = 21.29 (8.5 examples/sec; 0.945 sec/batch; 76h:31m:53s remains)
INFO - root - 2017-12-07 19:01:15.072994: step 40920, loss = 21.80, batch loss = 21.72 (8.4 examples/sec; 0.958 sec/batch; 77h:35m:33s remains)
INFO - root - 2017-12-07 19:01:24.534850: step 40930, loss = 21.44, batch loss = 21.36 (8.7 examples/sec; 0.919 sec/batch; 74h:27m:26s remains)
INFO - root - 2017-12-07 19:01:33.986871: step 40940, loss = 21.68, batch loss = 21.60 (8.8 examples/sec; 0.907 sec/batch; 73h:26m:04s remains)
INFO - root - 2017-12-07 19:01:43.354232: step 40950, loss = 21.73, batch loss = 21.65 (8.9 examples/sec; 0.900 sec/batch; 72h:55m:25s remains)
INFO - root - 2017-12-07 19:01:52.886370: step 40960, loss = 21.21, batch loss = 21.13 (8.4 examples/sec; 0.958 sec/batch; 77h:34m:50s remains)
INFO - root - 2017-12-07 19:02:02.390021: step 40970, loss = 21.18, batch loss = 21.10 (8.2 examples/sec; 0.975 sec/batch; 78h:56m:47s remains)
INFO - root - 2017-12-07 19:02:11.528546: step 40980, loss = 21.14, batch loss = 21.06 (8.6 examples/sec; 0.928 sec/batch; 75h:06m:57s remains)
INFO - root - 2017-12-07 19:02:20.961817: step 40990, loss = 21.25, batch loss = 21.16 (8.5 examples/sec; 0.946 sec/batch; 76h:37m:56s remains)
INFO - root - 2017-12-07 19:02:30.436531: step 41000, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.957 sec/batch; 77h:31m:45s remains)
2017-12-07 19:02:31.396518: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.432631 -4.4758029 -4.458643 -4.3843689 -4.2552786 -4.1384268 -4.0466027 -4.077106 -4.2867694 -4.4274735 -4.4386249 -4.4307857 -4.4303432 -4.436029 -4.4361067][-4.4764409 -4.512001 -4.4782352 -4.3843322 -4.2215109 -4.08135 -3.967962 -3.9936426 -4.2495131 -4.4206848 -4.4276357 -4.4119997 -4.40235 -4.3977985 -4.3866992][-4.5484319 -4.5810018 -4.5440278 -4.4549952 -4.2944851 -4.1594 -4.0430117 -4.0574245 -4.3136878 -4.4845591 -4.4799366 -4.4452138 -4.4085989 -4.3730679 -4.3338389][-4.6132746 -4.6386566 -4.5987711 -4.5296144 -4.4073811 -4.3058128 -4.1943398 -4.186523 -4.4058633 -4.5591154 -4.5543566 -4.5097175 -4.4470477 -4.3767171 -4.3046532][-4.6328073 -4.6360168 -4.5772877 -4.5168815 -4.4343705 -4.3625417 -4.241024 -4.2012577 -4.3860049 -4.5374932 -4.5626292 -4.536942 -4.4713664 -4.3889489 -4.3071055][-4.611 -4.5828166 -4.4906387 -4.41469 -4.3426266 -4.267427 -4.102684 -4.0228677 -4.2019448 -4.3754959 -4.4488869 -4.4690952 -4.4289875 -4.3694463 -4.3199196][-4.5925741 -4.5390329 -4.408433 -4.2917128 -4.1924586 -4.0857267 -3.8682156 -3.7567019 -3.9479363 -4.1389656 -4.2422438 -4.3067446 -4.3025084 -4.2858014 -4.2978988][-4.6055093 -4.5311432 -4.3555231 -4.1781945 -4.0285172 -3.8978891 -3.666338 -3.5518339 -3.7547741 -3.9389474 -4.030261 -4.1120129 -4.1367278 -4.1591797 -4.2303405][-4.6355677 -4.5435696 -4.3268037 -4.0968623 -3.9094357 -3.7914915 -3.6068134 -3.5159845 -3.7029445 -3.835423 -3.861331 -3.9223702 -3.9649127 -4.0157375 -4.1274757][-4.6659141 -4.5694437 -4.33796 -4.0897794 -3.8900104 -3.7982659 -3.6795459 -3.614928 -3.7654998 -3.8308573 -3.770288 -3.7904186 -3.8514321 -3.9221396 -4.0487413][-4.6742663 -4.594202 -4.3874297 -4.16194 -3.9630332 -3.8745041 -3.7888651 -3.7389488 -3.8642662 -3.886498 -3.7585936 -3.7436128 -3.824106 -3.9046252 -4.0280519][-4.6621556 -4.6274748 -4.4885073 -4.3209219 -4.1283193 -4.0079265 -3.9024467 -3.8444722 -3.9589269 -3.9734359 -3.8208926 -3.79003 -3.8837121 -3.9657016 -4.0809674][-4.6398234 -4.664856 -4.6151423 -4.5195589 -4.3421621 -4.1852756 -4.0403352 -3.9675362 -4.0786986 -4.113245 -3.9808481 -3.9500897 -4.04158 -4.1176314 -4.2233543][-4.6060448 -4.674058 -4.6974864 -4.6664529 -4.5232992 -4.362998 -4.2121458 -4.1492639 -4.2577453 -4.3153586 -4.2295089 -4.2050681 -4.2745366 -4.3365388 -4.4246812][-4.5741162 -4.6590257 -4.7228251 -4.736248 -4.6419282 -4.5162396 -4.4042635 -4.3772397 -4.4758835 -4.5378075 -4.4905868 -4.4657321 -4.5025606 -4.54725 -4.6097312]]...]
INFO - root - 2017-12-07 19:02:40.905296: step 41010, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.947 sec/batch; 76h:40m:27s remains)
INFO - root - 2017-12-07 19:02:50.240430: step 41020, loss = 21.24, batch loss = 21.15 (8.6 examples/sec; 0.934 sec/batch; 75h:38m:22s remains)
INFO - root - 2017-12-07 19:02:59.550056: step 41030, loss = 21.87, batch loss = 21.79 (8.1 examples/sec; 0.990 sec/batch; 80h:09m:43s remains)
INFO - root - 2017-12-07 19:03:08.996490: step 41040, loss = 20.98, batch loss = 20.90 (8.6 examples/sec; 0.932 sec/batch; 75h:29m:33s remains)
INFO - root - 2017-12-07 19:03:18.291267: step 41050, loss = 21.48, batch loss = 21.40 (8.1 examples/sec; 0.982 sec/batch; 79h:30m:20s remains)
INFO - root - 2017-12-07 19:03:27.609088: step 41060, loss = 21.50, batch loss = 21.41 (8.5 examples/sec; 0.940 sec/batch; 76h:07m:28s remains)
INFO - root - 2017-12-07 19:03:36.745219: step 41070, loss = 21.87, batch loss = 21.79 (8.5 examples/sec; 0.946 sec/batch; 76h:35m:04s remains)
INFO - root - 2017-12-07 19:03:46.187941: step 41080, loss = 21.57, batch loss = 21.49 (8.9 examples/sec; 0.895 sec/batch; 72h:27m:35s remains)
INFO - root - 2017-12-07 19:03:55.448934: step 41090, loss = 21.45, batch loss = 21.37 (8.8 examples/sec; 0.908 sec/batch; 73h:27m:41s remains)
INFO - root - 2017-12-07 19:04:04.910192: step 41100, loss = 21.35, batch loss = 21.27 (8.8 examples/sec; 0.914 sec/batch; 73h:59m:56s remains)
2017-12-07 19:04:05.833953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7266755 -4.8193145 -4.8336568 -4.769454 -4.6715145 -4.61371 -4.6408296 -4.6822243 -4.6757803 -4.6383991 -4.5641365 -4.4420857 -4.3019471 -4.2355261 -4.3204947][-4.7911129 -4.8786564 -4.8824687 -4.81494 -4.7169981 -4.6464491 -4.6471758 -4.6765118 -4.680903 -4.661612 -4.6218696 -4.5527048 -4.4557395 -4.3961091 -4.4492087][-4.7733145 -4.8364811 -4.8139687 -4.7322702 -4.6386986 -4.5765815 -4.5696316 -4.5915279 -4.605588 -4.6059475 -4.6048074 -4.6114335 -4.6003404 -4.5829663 -4.6107821][-4.7070842 -4.7284565 -4.6551881 -4.5360231 -4.4330368 -4.3772678 -4.3669667 -4.3802156 -4.4118953 -4.4449573 -4.479362 -4.5506239 -4.6293921 -4.6770735 -4.7091942][-4.6409411 -4.6201839 -4.4939752 -4.3266921 -4.1956787 -4.1201658 -4.0721807 -4.04839 -4.091291 -4.1799283 -4.2707958 -4.3886232 -4.5202861 -4.6188178 -4.6777391][-4.5767269 -4.5282717 -4.3687654 -4.1640263 -4.0000224 -3.8808873 -3.7628973 -3.6779413 -3.7130432 -3.8636074 -4.0388317 -4.2008286 -4.3412628 -4.4439673 -4.5148082][-4.5448909 -4.4992528 -4.3509994 -4.1483517 -3.9663925 -3.7985473 -3.6094527 -3.4719558 -3.4990532 -3.7042406 -3.964458 -4.1585035 -4.2618809 -4.3017588 -4.3252034][-4.6026778 -4.5870175 -4.4772339 -4.2996488 -4.1125474 -3.9117515 -3.6833045 -3.5255 -3.550621 -3.7845957 -4.105516 -4.3296847 -4.3965669 -4.3564625 -4.2966514][-4.711102 -4.7308712 -4.6569486 -4.5052633 -4.3276458 -4.1355004 -3.9300361 -3.792484 -3.8099327 -4.0149608 -4.3243575 -4.5503006 -4.5995 -4.5150576 -4.3944044][-4.8037157 -4.8567247 -4.8197117 -4.7034483 -4.5594225 -4.419054 -4.2847295 -4.19863 -4.2051558 -4.3348217 -4.5540214 -4.7223816 -4.7436376 -4.6404014 -4.4966893][-4.826231 -4.9059377 -4.9088655 -4.8412013 -4.7468781 -4.6681566 -4.6074781 -4.5765276 -4.5832891 -4.6437798 -4.7553325 -4.83648 -4.8179545 -4.7097116 -4.5713224][-4.7582922 -4.8451915 -4.8806238 -4.8610644 -4.8138452 -4.7764683 -4.7587423 -4.7603378 -4.7709174 -4.7918205 -4.8274903 -4.8412986 -4.7974133 -4.70161 -4.5884466][-4.6349583 -4.7064776 -4.7513428 -4.7622623 -4.7486887 -4.73313 -4.7291708 -4.7391157 -4.7520633 -4.760499 -4.76274 -4.7456532 -4.6984191 -4.6259718 -4.5450063][-4.490005 -4.5335193 -4.5662193 -4.5823336 -4.5836768 -4.5796046 -4.5793114 -4.5862837 -4.596766 -4.60414 -4.6026659 -4.5872488 -4.5560708 -4.5129294 -4.4643841][-4.3691697 -4.3843045 -4.3974991 -4.4045825 -4.4063773 -4.4053159 -4.404901 -4.4074092 -4.4125919 -4.4182935 -4.4207335 -4.4168658 -4.4060187 -4.3895335 -4.3697739]]...]
INFO - root - 2017-12-07 19:04:15.127700: step 41110, loss = 21.44, batch loss = 21.35 (8.4 examples/sec; 0.947 sec/batch; 76h:39m:02s remains)
INFO - root - 2017-12-07 19:04:24.404854: step 41120, loss = 21.86, batch loss = 21.78 (8.8 examples/sec; 0.912 sec/batch; 73h:49m:38s remains)
INFO - root - 2017-12-07 19:04:33.774680: step 41130, loss = 21.78, batch loss = 21.70 (9.0 examples/sec; 0.891 sec/batch; 72h:07m:03s remains)
INFO - root - 2017-12-07 19:04:43.120214: step 41140, loss = 21.38, batch loss = 21.30 (8.9 examples/sec; 0.896 sec/batch; 72h:30m:46s remains)
INFO - root - 2017-12-07 19:04:52.500433: step 41150, loss = 21.07, batch loss = 20.99 (8.4 examples/sec; 0.951 sec/batch; 76h:56m:43s remains)
INFO - root - 2017-12-07 19:05:01.909713: step 41160, loss = 21.17, batch loss = 21.08 (8.3 examples/sec; 0.967 sec/batch; 78h:14m:13s remains)
INFO - root - 2017-12-07 19:05:11.212239: step 41170, loss = 21.48, batch loss = 21.40 (8.4 examples/sec; 0.950 sec/batch; 76h:53m:16s remains)
INFO - root - 2017-12-07 19:05:20.483332: step 41180, loss = 21.19, batch loss = 21.11 (8.1 examples/sec; 0.985 sec/batch; 79h:44m:17s remains)
INFO - root - 2017-12-07 19:05:29.900993: step 41190, loss = 21.21, batch loss = 21.12 (8.6 examples/sec; 0.934 sec/batch; 75h:34m:30s remains)
INFO - root - 2017-12-07 19:05:39.151414: step 41200, loss = 21.27, batch loss = 21.19 (8.5 examples/sec; 0.943 sec/batch; 76h:19m:05s remains)
2017-12-07 19:05:40.077201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0950031 -4.1037078 -4.1314964 -4.1643829 -4.1998453 -4.2396455 -4.2820129 -4.321352 -4.3497715 -4.3618879 -4.3592863 -4.3449283 -4.3312902 -4.3268948 -4.3305798][-4.12617 -4.1428719 -4.17799 -4.2137136 -4.24754 -4.2837405 -4.3225269 -4.3586307 -4.3840685 -4.3940363 -4.3895688 -4.3724895 -4.3547211 -4.3454189 -4.3438745][-4.23363 -4.2673426 -4.3086567 -4.3361397 -4.3498635 -4.361928 -4.3813324 -4.4083571 -4.4335446 -4.4470615 -4.445251 -4.4267592 -4.4025149 -4.3829737 -4.37031][-4.37314 -4.4139042 -4.4438591 -4.4416809 -4.4136758 -4.3845668 -4.37678 -4.3986998 -4.4394932 -4.4778953 -4.4986463 -4.4925733 -4.4677987 -4.4382319 -4.4121094][-4.4670072 -4.4936604 -4.4885073 -4.4364767 -4.3543296 -4.2804446 -4.2480865 -4.2716408 -4.3410382 -4.4246025 -4.4926877 -4.52302 -4.5150819 -4.48667 -4.4535322][-4.468111 -4.4684973 -4.4183464 -4.312973 -4.1839557 -4.0790939 -4.0320172 -4.0568757 -4.1480331 -4.2715659 -4.392662 -4.4752283 -4.5026112 -4.4932094 -4.4677987][-4.3827629 -4.3654609 -4.283432 -4.1425476 -3.9899631 -3.8778441 -3.8286331 -3.8521101 -3.9480565 -4.0866656 -4.2397218 -4.3646026 -4.4275918 -4.4442263 -4.4369431][-4.2683039 -4.2571793 -4.1681423 -4.0163193 -3.86139 -3.7550452 -3.7070742 -3.7260692 -3.8159471 -3.9498658 -4.107574 -4.2469869 -4.3258147 -4.3575783 -4.3650026][-4.1845164 -4.2039742 -4.1355057 -3.9973645 -3.856365 -3.7610264 -3.7121873 -3.720906 -3.7959838 -3.9118285 -4.0512781 -4.1750689 -4.2437029 -4.271152 -4.2794938][-4.1569519 -4.2159524 -4.1895027 -4.0901041 -3.9836795 -3.9131021 -3.8735728 -3.8733494 -3.9236498 -4.0047922 -4.1020575 -4.1832194 -4.2208667 -4.2287321 -4.2243443][-4.2030387 -4.2881055 -4.3026314 -4.2496262 -4.1826706 -4.1389084 -4.1139784 -4.1087179 -4.1316934 -4.1690826 -4.2103686 -4.2368779 -4.2379556 -4.2260122 -4.2095838][-4.2989798 -4.3856039 -4.4238758 -4.4094925 -4.3760505 -4.3503466 -4.3326907 -4.3210945 -4.3188086 -4.317205 -4.3102937 -4.2918563 -4.2659135 -4.2430058 -4.2239337][-4.3691816 -4.4370618 -4.478406 -4.4877496 -4.4799833 -4.4684319 -4.4540367 -4.4356523 -4.4162469 -4.392652 -4.3621354 -4.3257117 -4.2923322 -4.2695665 -4.2552032][-4.3828306 -4.4199491 -4.4472141 -4.4610329 -4.4658518 -4.4644027 -4.454216 -4.4349942 -4.4122329 -4.3875213 -4.3599935 -4.3317618 -4.3087564 -4.2946668 -4.2877011][-4.3616867 -4.373044 -4.3858256 -4.3967338 -4.4054227 -4.409502 -4.4048243 -4.3911219 -4.3742356 -4.3585825 -4.3445311 -4.3327141 -4.3242865 -4.3192978 -4.3170137]]...]
INFO - root - 2017-12-07 19:05:49.454716: step 41210, loss = 21.58, batch loss = 21.49 (8.5 examples/sec; 0.945 sec/batch; 76h:27m:31s remains)
INFO - root - 2017-12-07 19:05:58.859523: step 41220, loss = 21.38, batch loss = 21.30 (8.2 examples/sec; 0.974 sec/batch; 78h:48m:50s remains)
INFO - root - 2017-12-07 19:06:08.154484: step 41230, loss = 21.61, batch loss = 21.52 (8.9 examples/sec; 0.904 sec/batch; 73h:07m:45s remains)
INFO - root - 2017-12-07 19:06:17.430994: step 41240, loss = 21.49, batch loss = 21.41 (10.0 examples/sec; 0.798 sec/batch; 64h:36m:05s remains)
INFO - root - 2017-12-07 19:06:26.916385: step 41250, loss = 21.31, batch loss = 21.23 (8.7 examples/sec; 0.919 sec/batch; 74h:23m:01s remains)
INFO - root - 2017-12-07 19:06:36.266080: step 41260, loss = 21.29, batch loss = 21.21 (8.8 examples/sec; 0.908 sec/batch; 73h:27m:33s remains)
INFO - root - 2017-12-07 19:06:45.614059: step 41270, loss = 21.07, batch loss = 20.99 (8.9 examples/sec; 0.903 sec/batch; 73h:04m:46s remains)
INFO - root - 2017-12-07 19:06:55.095364: step 41280, loss = 21.86, batch loss = 21.77 (8.3 examples/sec; 0.964 sec/batch; 77h:57m:16s remains)
INFO - root - 2017-12-07 19:07:04.273977: step 41290, loss = 21.52, batch loss = 21.43 (9.4 examples/sec; 0.850 sec/batch; 68h:47m:26s remains)
INFO - root - 2017-12-07 19:07:13.750447: step 41300, loss = 21.71, batch loss = 21.62 (8.2 examples/sec; 0.976 sec/batch; 78h:55m:31s remains)
2017-12-07 19:07:14.752106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4223819 -4.4058814 -4.3786263 -4.3526154 -4.3727517 -4.4538493 -4.5207405 -4.5434089 -4.5415831 -4.4959431 -4.4198251 -4.3556857 -4.2875066 -4.1830697 -4.0676847][-4.4657412 -4.4893379 -4.4947324 -4.48196 -4.4870138 -4.5377312 -4.5626493 -4.5454021 -4.5500803 -4.5571837 -4.5153122 -4.4364843 -4.3284531 -4.2001462 -4.0726047][-4.4663358 -4.4959154 -4.5100241 -4.5168605 -4.5292411 -4.5621443 -4.5539346 -4.5096674 -4.5296679 -4.5983033 -4.6106057 -4.5454097 -4.4249244 -4.2922792 -4.1711636][-4.4249 -4.4622374 -4.4936318 -4.5283332 -4.5533123 -4.5621147 -4.520905 -4.4614892 -4.499455 -4.6166167 -4.674787 -4.6296487 -4.5149179 -4.3911014 -4.2842474][-4.3851786 -4.450707 -4.5138078 -4.5638 -4.5671043 -4.513207 -4.4210806 -4.3587451 -4.4263635 -4.5839043 -4.6762009 -4.6548085 -4.5629668 -4.457057 -4.3561831][-4.3726883 -4.4758148 -4.5625515 -4.594048 -4.5343304 -4.3857789 -4.2387795 -4.1931353 -4.3050103 -4.5003991 -4.6182089 -4.6205873 -4.555459 -4.4675469 -4.3635459][-4.353426 -4.4593554 -4.5316806 -4.5138984 -4.38318 -4.162652 -3.9952149 -3.9911747 -4.151577 -4.3803535 -4.5233393 -4.5437036 -4.4884877 -4.4014697 -4.2922077][-4.3287797 -4.3904719 -4.4047084 -4.3269048 -4.1498952 -3.908047 -3.7584238 -3.8081553 -4.0064182 -4.2517486 -4.4139667 -4.4426303 -4.3777614 -4.2806478 -4.1765108][-4.3186584 -4.3080282 -4.2558908 -4.1461945 -3.9745715 -3.7631409 -3.6557007 -3.7465034 -3.9543619 -4.1850362 -4.3431277 -4.3637848 -4.280643 -4.1772304 -4.0952134][-4.3528357 -4.2895937 -4.2004471 -4.09661 -3.969239 -3.8132436 -3.7473946 -3.8548574 -4.0450087 -4.2306089 -4.3498979 -4.3426094 -4.2412562 -4.1395407 -4.0820365][-4.4151812 -4.3401985 -4.2511477 -4.1746068 -4.0963426 -3.9871371 -3.9425561 -4.0391288 -4.1939931 -4.3241391 -4.391345 -4.357224 -4.2550449 -4.1712894 -4.1404629][-4.4859247 -4.4286489 -4.3608236 -4.31058 -4.2674308 -4.1939273 -4.1596079 -4.2247729 -4.3322763 -4.4132819 -4.4421859 -4.40428 -4.3274975 -4.2759991 -4.2701221][-4.5632696 -4.5398817 -4.5075345 -4.480689 -4.4574594 -4.4084907 -4.377636 -4.4046354 -4.4590445 -4.4972444 -4.5007625 -4.4725585 -4.4320827 -4.4078007 -4.4102097][-4.5910983 -4.6150608 -4.6310158 -4.6252913 -4.6150846 -4.5845394 -4.5516763 -4.5429988 -4.5523143 -4.5558758 -4.5407891 -4.5204716 -4.5009565 -4.4817128 -4.46897][-4.4928231 -4.563684 -4.6338978 -4.6518545 -4.6581645 -4.6488924 -4.6191154 -4.5957971 -4.5870695 -4.57423 -4.5449286 -4.5185323 -4.4946375 -4.4596467 -4.4235549]]...]
INFO - root - 2017-12-07 19:07:24.133120: step 41310, loss = 22.02, batch loss = 21.93 (8.1 examples/sec; 0.982 sec/batch; 79h:24m:11s remains)
INFO - root - 2017-12-07 19:07:33.418098: step 41320, loss = 21.55, batch loss = 21.47 (8.7 examples/sec; 0.918 sec/batch; 74h:13m:42s remains)
INFO - root - 2017-12-07 19:07:42.861077: step 41330, loss = 21.39, batch loss = 21.30 (9.3 examples/sec; 0.864 sec/batch; 69h:55m:04s remains)
INFO - root - 2017-12-07 19:07:52.242643: step 41340, loss = 21.31, batch loss = 21.23 (9.0 examples/sec; 0.887 sec/batch; 71h:42m:13s remains)
INFO - root - 2017-12-07 19:08:01.692443: step 41350, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.931 sec/batch; 75h:20m:04s remains)
INFO - root - 2017-12-07 19:08:11.018075: step 41360, loss = 20.89, batch loss = 20.81 (8.6 examples/sec; 0.930 sec/batch; 75h:14m:00s remains)
INFO - root - 2017-12-07 19:08:20.428992: step 41370, loss = 21.24, batch loss = 21.16 (8.7 examples/sec; 0.923 sec/batch; 74h:36m:40s remains)
INFO - root - 2017-12-07 19:08:29.847351: step 41380, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.905 sec/batch; 73h:10m:58s remains)
INFO - root - 2017-12-07 19:08:39.205270: step 41390, loss = 21.22, batch loss = 21.14 (8.9 examples/sec; 0.895 sec/batch; 72h:22m:24s remains)
INFO - root - 2017-12-07 19:08:48.598462: step 41400, loss = 21.11, batch loss = 21.02 (8.7 examples/sec; 0.922 sec/batch; 74h:33m:45s remains)
2017-12-07 19:08:49.570345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.649241 -4.7093549 -4.7620554 -4.780694 -4.730998 -4.6427083 -4.559679 -4.5098805 -4.5449648 -4.6562448 -4.799026 -4.9146624 -4.9522815 -4.936862 -4.8941827][-4.5039425 -4.5948029 -4.655458 -4.6522322 -4.5554719 -4.4200168 -4.3216949 -4.2982693 -4.3741636 -4.518424 -4.690824 -4.8329763 -4.8719344 -4.8435335 -4.7944794][-4.3642621 -4.4867291 -4.5552592 -4.5417924 -4.4274821 -4.2675047 -4.1427646 -4.1117449 -4.1825333 -4.3156838 -4.488533 -4.6451163 -4.6977344 -4.6776562 -4.6464195][-4.2807794 -4.4276824 -4.5057487 -4.4886479 -4.3735642 -4.2023625 -4.0497627 -3.9841824 -4.0130568 -4.1119003 -4.2834949 -4.4643116 -4.5456963 -4.5445065 -4.5358691][-4.2668376 -4.4226575 -4.4978094 -4.4579029 -4.3233085 -4.1364851 -3.9656007 -3.8741374 -3.8731942 -3.9644248 -4.1575747 -4.3761578 -4.4930105 -4.5049849 -4.5044842][-4.328217 -4.4535484 -4.4805951 -4.3868346 -4.220808 -4.0256681 -3.8529553 -3.752665 -3.7457175 -3.8653321 -4.10966 -4.380352 -4.5338697 -4.5510769 -4.5430436][-4.4312291 -4.48225 -4.4334936 -4.2879548 -4.1136823 -3.9346588 -3.7725735 -3.6708057 -3.6668077 -3.8234859 -4.1177034 -4.4303885 -4.6128926 -4.63366 -4.6151652][-4.5385089 -4.5181389 -4.411078 -4.2409482 -4.0829778 -3.9376502 -3.8053608 -3.7252324 -3.7301676 -3.8949289 -4.1861682 -4.4907689 -4.6802597 -4.7139049 -4.6949139][-4.6253414 -4.589108 -4.4782295 -4.3223348 -4.1937404 -4.0822415 -3.9849348 -3.9326012 -3.9408092 -4.0804844 -4.3201818 -4.5694404 -4.7360506 -4.7718678 -4.7423463][-4.6696978 -4.6651316 -4.5958977 -4.4820714 -4.3841238 -4.2982149 -4.2308674 -4.2010126 -4.207931 -4.3079739 -4.4802742 -4.6529331 -4.7646813 -4.7718458 -4.7134633][-4.6724057 -4.7016287 -4.6771727 -4.6086211 -4.5455461 -4.4944448 -4.4618716 -4.4533629 -4.4534888 -4.5032272 -4.6012573 -4.6913481 -4.73255 -4.6979141 -4.61398][-4.6430564 -4.6887975 -4.6934028 -4.6654968 -4.6408772 -4.625123 -4.6199126 -4.61852 -4.6004977 -4.5975618 -4.6241446 -4.6462927 -4.6356654 -4.5811391 -4.4998422][-4.591918 -4.6444311 -4.6648455 -4.6594238 -4.6552429 -4.6568074 -4.6628304 -4.6593471 -4.6214967 -4.5768323 -4.5541558 -4.5395894 -4.5128417 -4.4666677 -4.4095092][-4.5144324 -4.560298 -4.5806613 -4.5795627 -4.5788441 -4.5861697 -4.597209 -4.5934567 -4.5518165 -4.4930754 -4.4516325 -4.4270205 -4.4058075 -4.3811808 -4.3523011][-4.4145708 -4.4413233 -4.4520679 -4.451169 -4.4533858 -4.4648156 -4.4805188 -4.4830313 -4.4539351 -4.4072723 -4.3712687 -4.3497849 -4.3366575 -4.3273535 -4.3170891]]...]
INFO - root - 2017-12-07 19:08:59.002289: step 41410, loss = 21.29, batch loss = 21.21 (8.9 examples/sec; 0.903 sec/batch; 73h:01m:00s remains)
INFO - root - 2017-12-07 19:09:08.340566: step 41420, loss = 21.84, batch loss = 21.76 (8.5 examples/sec; 0.939 sec/batch; 75h:54m:36s remains)
INFO - root - 2017-12-07 19:09:17.661959: step 41430, loss = 21.72, batch loss = 21.63 (8.2 examples/sec; 0.975 sec/batch; 78h:50m:09s remains)
INFO - root - 2017-12-07 19:09:26.938689: step 41440, loss = 21.34, batch loss = 21.26 (8.3 examples/sec; 0.964 sec/batch; 77h:57m:16s remains)
INFO - root - 2017-12-07 19:09:36.290844: step 41450, loss = 21.30, batch loss = 21.21 (8.8 examples/sec; 0.913 sec/batch; 73h:46m:48s remains)
INFO - root - 2017-12-07 19:09:45.686899: step 41460, loss = 21.64, batch loss = 21.56 (9.0 examples/sec; 0.887 sec/batch; 71h:40m:43s remains)
INFO - root - 2017-12-07 19:09:55.022298: step 41470, loss = 21.49, batch loss = 21.41 (8.7 examples/sec; 0.924 sec/batch; 74h:39m:50s remains)
INFO - root - 2017-12-07 19:10:04.300520: step 41480, loss = 21.50, batch loss = 21.41 (8.7 examples/sec; 0.916 sec/batch; 74h:04m:54s remains)
INFO - root - 2017-12-07 19:10:13.742737: step 41490, loss = 21.42, batch loss = 21.34 (8.9 examples/sec; 0.903 sec/batch; 72h:58m:03s remains)
INFO - root - 2017-12-07 19:10:23.209496: step 41500, loss = 21.29, batch loss = 21.21 (8.2 examples/sec; 0.980 sec/batch; 79h:14m:15s remains)
2017-12-07 19:10:24.107513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.37964 -4.4232526 -4.4662209 -4.5170755 -4.5618486 -4.5238891 -4.411057 -4.2959485 -4.2626848 -4.3166041 -4.4195957 -4.5174012 -4.5595942 -4.5163069 -4.3774943][-4.3503332 -4.3781152 -4.4275455 -4.5230293 -4.59973 -4.5517311 -4.4133034 -4.2965503 -4.2712917 -4.3233566 -4.4251642 -4.5246296 -4.5705032 -4.5356026 -4.4153552][-4.3211417 -4.34163 -4.4018831 -4.5204535 -4.6000476 -4.5408883 -4.4000316 -4.2974544 -4.2797723 -4.3223829 -4.4086332 -4.5007625 -4.56633 -4.5715609 -4.4997749][-4.3197675 -4.3396435 -4.4002161 -4.4987569 -4.5366697 -4.450388 -4.3171039 -4.2405548 -4.2438326 -4.2978287 -4.3804584 -4.46181 -4.5397816 -4.5861754 -4.5691953][-4.362596 -4.3809161 -4.4239922 -4.4573183 -4.4080577 -4.2694168 -4.1355596 -4.0989189 -4.1524491 -4.2497249 -4.3519578 -4.4255381 -4.4972076 -4.5625324 -4.5884662][-4.4196963 -4.4242563 -4.435277 -4.3919992 -4.2514496 -4.0512371 -3.9053047 -3.903923 -4.0148935 -4.1633391 -4.2973018 -4.3760781 -4.4359603 -4.4959111 -4.5357523][-4.4329877 -4.4145875 -4.4026694 -4.3173442 -4.1260786 -3.8862488 -3.7244668 -3.7389863 -3.8889127 -4.0738287 -4.23906 -4.3323784 -4.381145 -4.4197149 -4.4476242][-4.3792815 -4.3546982 -4.3441372 -4.2534552 -4.0500469 -3.7995384 -3.6293573 -3.6474018 -3.82017 -4.0316429 -4.21843 -4.3206515 -4.3541675 -4.3651853 -4.3734941][-4.2784958 -4.2717619 -4.2880735 -4.2170367 -4.0271239 -3.7897775 -3.6260314 -3.6543217 -3.8430665 -4.0666661 -4.2553406 -4.3475394 -4.3557343 -4.33629 -4.3234863][-4.1506462 -4.1767879 -4.2367053 -4.2055874 -4.0521641 -3.8557551 -3.7252619 -3.7799764 -3.973613 -4.1863003 -4.3509521 -4.4160795 -4.3980918 -4.3522377 -4.3186979][-4.0706887 -4.1201019 -4.2001972 -4.1987247 -4.0951214 -3.9648488 -3.88935 -3.9759994 -4.16899 -4.3526306 -4.47354 -4.5051117 -4.4698949 -4.4130735 -4.3689141][-4.1221538 -4.1731153 -4.2292838 -4.2249088 -4.1639891 -4.1073046 -4.0856962 -4.1880417 -4.3638206 -4.5066223 -4.5733752 -4.5649567 -4.5170326 -4.4640503 -4.4273415][-4.2665405 -4.3037806 -4.3182087 -4.2938495 -4.2577682 -4.2548389 -4.2707939 -4.3651686 -4.5044246 -4.599823 -4.6112351 -4.5623131 -4.5039964 -4.4648204 -4.4512687][-4.3942318 -4.416749 -4.3988185 -4.35392 -4.3265166 -4.3519516 -4.3895149 -4.4658012 -4.56456 -4.6183467 -4.5875397 -4.5090923 -4.4471083 -4.4273629 -4.4410672][-4.4909816 -4.5120268 -4.4769511 -4.4112639 -4.3773994 -4.401999 -4.4378915 -4.488708 -4.5466671 -4.5663438 -4.5128078 -4.423707 -4.3679171 -4.3694577 -4.4081817]]...]
INFO - root - 2017-12-07 19:10:33.300633: step 41510, loss = 21.50, batch loss = 21.42 (8.8 examples/sec; 0.914 sec/batch; 73h:51m:03s remains)
INFO - root - 2017-12-07 19:10:42.824756: step 41520, loss = 21.50, batch loss = 21.41 (8.5 examples/sec; 0.942 sec/batch; 76h:07m:45s remains)
INFO - root - 2017-12-07 19:10:52.253268: step 41530, loss = 21.75, batch loss = 21.67 (8.9 examples/sec; 0.899 sec/batch; 72h:38m:11s remains)
INFO - root - 2017-12-07 19:11:01.872337: step 41540, loss = 20.96, batch loss = 20.88 (8.4 examples/sec; 0.952 sec/batch; 76h:54m:29s remains)
INFO - root - 2017-12-07 19:11:11.398842: step 41550, loss = 21.56, batch loss = 21.48 (8.6 examples/sec; 0.925 sec/batch; 74h:47m:31s remains)
INFO - root - 2017-12-07 19:11:20.766256: step 41560, loss = 21.53, batch loss = 21.45 (8.3 examples/sec; 0.963 sec/batch; 77h:50m:57s remains)
INFO - root - 2017-12-07 19:11:30.043446: step 41570, loss = 21.54, batch loss = 21.46 (8.5 examples/sec; 0.938 sec/batch; 75h:48m:28s remains)
INFO - root - 2017-12-07 19:11:39.511975: step 41580, loss = 21.32, batch loss = 21.23 (8.7 examples/sec; 0.924 sec/batch; 74h:42m:01s remains)
INFO - root - 2017-12-07 19:11:48.801948: step 41590, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.932 sec/batch; 75h:20m:06s remains)
INFO - root - 2017-12-07 19:11:58.020904: step 41600, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.931 sec/batch; 75h:14m:14s remains)
2017-12-07 19:11:58.971431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5783787 -4.4961653 -4.3935347 -4.3170815 -4.3021665 -4.3250227 -4.3536296 -4.3790727 -4.4286132 -4.5266738 -4.6629081 -4.7656274 -4.7605586 -4.6499496 -4.5026832][-4.6312962 -4.5485878 -4.4432564 -4.3574514 -4.3354492 -4.3589754 -4.3898215 -4.4195929 -4.4695315 -4.5580931 -4.670579 -4.7448893 -4.743052 -4.6752744 -4.563035][-4.6611667 -4.5855904 -4.4783778 -4.3766861 -4.3287592 -4.3351932 -4.3737965 -4.4327884 -4.5102291 -4.6039758 -4.6904445 -4.7174077 -4.6811414 -4.6205459 -4.5366893][-4.6344943 -4.5796285 -4.4862466 -4.3828635 -4.3101549 -4.2823296 -4.310276 -4.3866568 -4.4871655 -4.5930419 -4.6746378 -4.6825094 -4.6222024 -4.5509815 -4.4719696][-4.5639963 -4.5279675 -4.4436216 -4.332799 -4.2296309 -4.1557302 -4.155808 -4.2391276 -4.3597579 -4.4847088 -4.5845871 -4.6114721 -4.565928 -4.5010824 -4.4243245][-4.46374 -4.4382768 -4.3525372 -4.23179 -4.111166 -4.0054431 -3.9765522 -4.059104 -4.1984463 -4.3468647 -4.4721375 -4.5264945 -4.5024338 -4.4435978 -4.3738008][-4.369132 -4.3598189 -4.2793646 -4.1604891 -4.0350757 -3.9047644 -3.8348882 -3.8926377 -4.0371447 -4.2038279 -4.3478165 -4.4253216 -4.4219904 -4.368042 -4.3042555][-4.36255 -4.3630857 -4.2759886 -4.1393585 -3.9908953 -3.8374557 -3.7364886 -3.7706358 -3.9179683 -4.0972862 -4.2574558 -4.3667836 -4.395668 -4.3472672 -4.2748327][-4.4283113 -4.4166608 -4.3157377 -4.1672878 -4.0193548 -3.8838713 -3.7919626 -3.8107579 -3.9367461 -4.1046963 -4.2629189 -4.3883176 -4.445497 -4.4075561 -4.3201923][-4.5173783 -4.497539 -4.4025006 -4.2735634 -4.1581459 -4.0623894 -3.9909289 -3.9759629 -4.0407181 -4.1605911 -4.2889614 -4.4018989 -4.4721236 -4.4583607 -4.3751755][-4.5965557 -4.5608745 -4.4666157 -4.3550668 -4.2635455 -4.192564 -4.1367617 -4.0937142 -4.1057191 -4.1875567 -4.2998481 -4.4049845 -4.4833179 -4.4939022 -4.4261074][-4.6073346 -4.5469847 -4.4565997 -4.3827286 -4.3388782 -4.3041744 -4.2629776 -4.1955709 -4.1729555 -4.2419844 -4.3554196 -4.45022 -4.5167589 -4.5208516 -4.4572997][-4.5579104 -4.4866619 -4.4248319 -4.4165721 -4.443542 -4.4491696 -4.4049878 -4.30267 -4.2463841 -4.304677 -4.4071703 -4.473001 -4.5022197 -4.4818263 -4.4217196][-4.518261 -4.4462762 -4.4055743 -4.4317279 -4.4876785 -4.5084691 -4.4611216 -4.352849 -4.2920389 -4.3464365 -4.4282341 -4.4520559 -4.4400997 -4.4099946 -4.3719788][-4.5129609 -4.43504 -4.3930178 -4.4091439 -4.4442492 -4.4499421 -4.3990812 -4.3107858 -4.2727509 -4.3336363 -4.3944912 -4.37704 -4.3370051 -4.3266315 -4.3375916]]...]
INFO - root - 2017-12-07 19:12:08.424463: step 41610, loss = 21.09, batch loss = 21.01 (8.0 examples/sec; 0.999 sec/batch; 80h:45m:16s remains)
INFO - root - 2017-12-07 19:12:17.756988: step 41620, loss = 21.84, batch loss = 21.76 (8.4 examples/sec; 0.952 sec/batch; 76h:57m:24s remains)
INFO - root - 2017-12-07 19:12:27.211046: step 41630, loss = 21.66, batch loss = 21.58 (8.1 examples/sec; 0.992 sec/batch; 80h:08m:51s remains)
INFO - root - 2017-12-07 19:12:36.592227: step 41640, loss = 21.89, batch loss = 21.81 (8.5 examples/sec; 0.936 sec/batch; 75h:36m:54s remains)
INFO - root - 2017-12-07 19:12:45.939867: step 41650, loss = 21.53, batch loss = 21.45 (8.9 examples/sec; 0.896 sec/batch; 72h:22m:45s remains)
INFO - root - 2017-12-07 19:12:55.374319: step 41660, loss = 21.41, batch loss = 21.32 (8.3 examples/sec; 0.958 sec/batch; 77h:25m:42s remains)
INFO - root - 2017-12-07 19:13:04.820639: step 41670, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.956 sec/batch; 77h:15m:52s remains)
INFO - root - 2017-12-07 19:13:14.236220: step 41680, loss = 21.82, batch loss = 21.73 (9.3 examples/sec; 0.857 sec/batch; 69h:13m:52s remains)
INFO - root - 2017-12-07 19:13:23.649060: step 41690, loss = 21.32, batch loss = 21.23 (8.9 examples/sec; 0.897 sec/batch; 72h:27m:03s remains)
INFO - root - 2017-12-07 19:13:32.919562: step 41700, loss = 21.77, batch loss = 21.69 (8.4 examples/sec; 0.956 sec/batch; 77h:14m:10s remains)
2017-12-07 19:13:33.823836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4964104 -4.4053531 -4.2676377 -4.1450911 -4.087029 -4.074935 -4.151937 -4.307848 -4.4283814 -4.4864478 -4.499999 -4.4665151 -4.4058514 -4.3447757 -4.3186207][-4.4503894 -4.3724694 -4.2802024 -4.2087336 -4.1896982 -4.1827645 -4.2292233 -4.3249264 -4.3844581 -4.4312029 -4.479919 -4.4860177 -4.46343 -4.4341717 -4.4254332][-4.3445635 -4.2878461 -4.2560987 -4.262351 -4.3045654 -4.3244271 -4.3469296 -4.3770385 -4.3689818 -4.3890734 -4.4475846 -4.4756036 -4.4877834 -4.4964776 -4.5084138][-4.1936822 -4.1514258 -4.1658049 -4.2334361 -4.3259077 -4.3734202 -4.3839684 -4.3659997 -4.3179703 -4.3259397 -4.3936181 -4.4453731 -4.4885874 -4.5221224 -4.5409555][-4.0568833 -4.0321593 -4.0717092 -4.1661491 -4.2804832 -4.3424735 -4.3396964 -4.2827458 -4.2142863 -4.2221837 -4.3069859 -4.3953958 -4.4730124 -4.5253305 -4.5450673][-3.9935358 -3.989044 -4.0294418 -4.1130066 -4.2171516 -4.2711773 -4.2494712 -4.1671033 -4.0938549 -4.1021042 -4.2004232 -4.3202109 -4.4287124 -4.5062866 -4.5328269][-3.9970636 -4.0113292 -4.0436125 -4.0994072 -4.1707129 -4.1904254 -4.137022 -4.0444751 -3.9835465 -3.997057 -4.1029711 -4.2373919 -4.365345 -4.4711876 -4.5098166][-4.0653348 -4.08958 -4.1199179 -4.1578522 -4.1939974 -4.1698866 -4.0776529 -3.9790931 -3.9307785 -3.9460726 -4.047689 -4.1769438 -4.3135309 -4.4488626 -4.5034766][-4.2083225 -4.2246408 -4.2411814 -4.2573113 -4.2618971 -4.2056489 -4.0867467 -3.9835467 -3.9371564 -3.9454818 -4.0275855 -4.1344929 -4.2711105 -4.4333372 -4.5104752][-4.4045396 -4.3995771 -4.3890166 -4.3767323 -4.3516593 -4.2755651 -4.1460819 -4.0371337 -3.9815121 -3.9774508 -4.0375528 -4.1204805 -4.2496386 -4.4233866 -4.5210767][-4.5750566 -4.5538473 -4.528511 -4.499126 -4.45773 -4.3756542 -4.2535057 -4.1440921 -4.0722613 -4.0513091 -4.0950003 -4.1644993 -4.2806315 -4.4375367 -4.534883][-4.6335526 -4.6003218 -4.5776758 -4.555995 -4.5245342 -4.4570837 -4.3607945 -4.2612543 -4.1798978 -4.1456146 -4.1826773 -4.2530847 -4.3529735 -4.469017 -4.5400243][-4.5741372 -4.529285 -4.5172362 -4.5225272 -4.527812 -4.4980888 -4.4372964 -4.3516531 -4.2669072 -4.2272239 -4.264276 -4.3394446 -4.4209957 -4.494843 -4.5376511][-4.4476428 -4.3941059 -4.384974 -4.4117689 -4.4599848 -4.4759903 -4.4487 -4.3752718 -4.2899904 -4.2477574 -4.2831788 -4.3557434 -4.4220381 -4.4756789 -4.5136509][-4.3360519 -4.2680988 -4.2326274 -4.2438917 -4.3069506 -4.3562374 -4.3593488 -4.3061061 -4.23478 -4.2009459 -4.2376747 -4.3015018 -4.3511214 -4.3939219 -4.4367642]]...]
INFO - root - 2017-12-07 19:13:43.353593: step 41710, loss = 21.31, batch loss = 21.23 (8.9 examples/sec; 0.897 sec/batch; 72h:28m:04s remains)
INFO - root - 2017-12-07 19:13:52.895196: step 41720, loss = 21.62, batch loss = 21.53 (8.4 examples/sec; 0.957 sec/batch; 77h:16m:41s remains)
INFO - root - 2017-12-07 19:14:02.315851: step 41730, loss = 21.51, batch loss = 21.43 (8.3 examples/sec; 0.966 sec/batch; 78h:02m:46s remains)
INFO - root - 2017-12-07 19:14:11.828661: step 41740, loss = 21.35, batch loss = 21.27 (7.6 examples/sec; 1.059 sec/batch; 85h:30m:32s remains)
INFO - root - 2017-12-07 19:14:21.221416: step 41750, loss = 21.38, batch loss = 21.30 (8.1 examples/sec; 0.987 sec/batch; 79h:42m:06s remains)
INFO - root - 2017-12-07 19:14:30.594768: step 41760, loss = 21.35, batch loss = 21.27 (8.7 examples/sec; 0.915 sec/batch; 73h:53m:02s remains)
INFO - root - 2017-12-07 19:14:39.969478: step 41770, loss = 21.79, batch loss = 21.71 (8.5 examples/sec; 0.936 sec/batch; 75h:34m:19s remains)
INFO - root - 2017-12-07 19:14:49.434199: step 41780, loss = 21.05, batch loss = 20.96 (8.5 examples/sec; 0.946 sec/batch; 76h:23m:34s remains)
INFO - root - 2017-12-07 19:14:58.982315: step 41790, loss = 21.55, batch loss = 21.46 (8.5 examples/sec; 0.947 sec/batch; 76h:26m:15s remains)
INFO - root - 2017-12-07 19:15:08.420120: step 41800, loss = 21.78, batch loss = 21.70 (8.8 examples/sec; 0.908 sec/batch; 73h:21m:25s remains)
2017-12-07 19:15:09.353843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4544692 -4.488862 -4.5695281 -4.6714392 -4.7508979 -4.7778473 -4.7554636 -4.6952572 -4.6283379 -4.5663443 -4.5133152 -4.5060954 -4.5305395 -4.54327 -4.5553203][-4.457262 -4.4941096 -4.569859 -4.6742783 -4.7538781 -4.76302 -4.7141366 -4.6250153 -4.5395641 -4.4838347 -4.4664884 -4.4977293 -4.5344214 -4.5367122 -4.53756][-4.4543867 -4.4783511 -4.5267429 -4.6046524 -4.6540723 -4.6246867 -4.54606 -4.4452 -4.3668575 -4.3477283 -4.3910413 -4.4668989 -4.5138659 -4.510551 -4.5041609][-4.4466114 -4.4327445 -4.4271455 -4.4469337 -4.4405785 -4.3635559 -4.2639375 -4.1756659 -4.1330113 -4.1804614 -4.29889 -4.4170985 -4.4788141 -4.4808874 -4.4719043][-4.4371271 -4.3791742 -4.3105888 -4.2564163 -4.1830931 -4.0611572 -3.9511175 -3.8813019 -3.881465 -4.0043721 -4.1999969 -4.3640232 -4.4532342 -4.47077 -4.4593186][-4.421629 -4.3359389 -4.2244592 -4.1089544 -3.976356 -3.8222804 -3.7025523 -3.6347973 -3.6562545 -3.8352082 -4.0876579 -4.2899146 -4.4138107 -4.4522643 -4.4455147][-4.3884497 -4.3108788 -4.2108822 -4.0842977 -3.9318597 -3.7746046 -3.6459312 -3.5557704 -3.5705738 -3.7605932 -4.0140781 -4.2168221 -4.3541393 -4.4054112 -4.4165411][-4.3683925 -4.3270926 -4.2755218 -4.178061 -4.0459991 -3.9112444 -3.7812431 -3.6690674 -3.6722133 -3.8378575 -4.0441 -4.2124195 -4.3328886 -4.3760033 -4.4035521][-4.3708239 -4.3656087 -4.3612723 -4.3019447 -4.2099881 -4.1125717 -3.996489 -3.8901539 -3.899591 -4.0270944 -4.1647668 -4.2847137 -4.3678188 -4.3871074 -4.4165368][-4.357492 -4.374445 -4.4040961 -4.384944 -4.3401532 -4.2814021 -4.19301 -4.1214428 -4.152081 -4.2417431 -4.3110824 -4.3789158 -4.4212575 -4.4183264 -4.4463496][-4.3370538 -4.353548 -4.4002204 -4.4174118 -4.4121752 -4.3861971 -4.3361068 -4.3107 -4.3593087 -4.4126372 -4.4265819 -4.4517756 -4.4705396 -4.4663458 -4.5017209][-4.3335338 -4.3369708 -4.3908048 -4.4337444 -4.4513407 -4.4432507 -4.4269238 -4.4397035 -4.4930887 -4.5171719 -4.4963555 -4.5013537 -4.5231285 -4.5384841 -4.5875306][-4.3443041 -4.336688 -4.3901739 -4.4455719 -4.470624 -4.4645147 -4.4605088 -4.4834695 -4.5245285 -4.5257878 -4.4899168 -4.500165 -4.5488548 -4.603014 -4.6751237][-4.3529739 -4.3424783 -4.3959885 -4.4587007 -4.49049 -4.4854021 -4.4795079 -4.4919152 -4.51196 -4.4928489 -4.4489985 -4.4660225 -4.5371346 -4.6255636 -4.71719][-4.3937531 -4.3889327 -4.4392905 -4.5045094 -4.5428872 -4.5415082 -4.5341544 -4.5371637 -4.5418105 -4.5097837 -4.4624743 -4.4751391 -4.5467958 -4.6448469 -4.7369928]]...]
INFO - root - 2017-12-07 19:15:18.868658: step 41810, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.935 sec/batch; 75h:28m:53s remains)
INFO - root - 2017-12-07 19:15:28.279646: step 41820, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.932 sec/batch; 75h:15m:57s remains)
INFO - root - 2017-12-07 19:15:37.773907: step 41830, loss = 21.38, batch loss = 21.29 (8.6 examples/sec; 0.933 sec/batch; 75h:17m:35s remains)
INFO - root - 2017-12-07 19:15:47.252055: step 41840, loss = 21.51, batch loss = 21.43 (8.1 examples/sec; 0.991 sec/batch; 80h:00m:34s remains)
INFO - root - 2017-12-07 19:15:56.614845: step 41850, loss = 21.53, batch loss = 21.44 (8.0 examples/sec; 1.004 sec/batch; 81h:05m:31s remains)
INFO - root - 2017-12-07 19:16:06.098018: step 41860, loss = 21.38, batch loss = 21.30 (8.0 examples/sec; 0.995 sec/batch; 80h:21m:06s remains)
INFO - root - 2017-12-07 19:16:15.606507: step 41870, loss = 21.32, batch loss = 21.24 (8.1 examples/sec; 0.989 sec/batch; 79h:49m:34s remains)
INFO - root - 2017-12-07 19:16:25.208199: step 41880, loss = 21.26, batch loss = 21.18 (8.0 examples/sec; 1.000 sec/batch; 80h:45m:36s remains)
INFO - root - 2017-12-07 19:16:34.668418: step 41890, loss = 21.19, batch loss = 21.11 (8.6 examples/sec; 0.934 sec/batch; 75h:25m:09s remains)
INFO - root - 2017-12-07 19:16:43.979327: step 41900, loss = 21.29, batch loss = 21.20 (8.7 examples/sec; 0.918 sec/batch; 74h:04m:51s remains)
2017-12-07 19:16:44.921365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4531679 -4.4740758 -4.521697 -4.5171309 -4.4624147 -4.4348178 -4.4499865 -4.4555454 -4.4045076 -4.3723397 -4.4373779 -4.5390625 -4.5645 -4.5060434 -4.4356647][-4.43122 -4.4739141 -4.5171394 -4.4862919 -4.42362 -4.4049654 -4.4353418 -4.4593062 -4.4200311 -4.3682141 -4.3976884 -4.4815297 -4.5226622 -4.4945974 -4.45467][-4.3932996 -4.4575176 -4.4920387 -4.4390826 -4.3646684 -4.3331885 -4.3590159 -4.4049807 -4.4094262 -4.3782773 -4.3896365 -4.4474354 -4.4865975 -4.4728966 -4.4468527][-4.3600688 -4.4204488 -4.4246311 -4.3534694 -4.2749782 -4.2277722 -4.2384753 -4.3000255 -4.3536849 -4.3663254 -4.3865657 -4.4328761 -4.4701695 -4.4623251 -4.4387059][-4.351656 -4.3678007 -4.3182549 -4.2241549 -4.141408 -4.079854 -4.0828562 -4.1611314 -4.2513762 -4.3081074 -4.3535385 -4.4088769 -4.4544106 -4.4493766 -4.4212141][-4.3738055 -4.3341393 -4.2245164 -4.0965338 -3.9970529 -3.9181123 -3.9226046 -4.021646 -4.1355219 -4.2240467 -4.29475 -4.3691468 -4.4265785 -4.4139705 -4.3645167][-4.3985848 -4.3375182 -4.200129 -4.0499868 -3.9332042 -3.8351369 -3.8320718 -3.9356499 -4.0517516 -4.1555729 -4.2445364 -4.3285489 -4.3842525 -4.3553705 -4.2842627][-4.4285893 -4.3799577 -4.2545748 -4.116745 -4.0016475 -3.8934991 -3.8713512 -3.9501972 -4.0405555 -4.1369119 -4.2262239 -4.2988968 -4.3267455 -4.2715821 -4.1837525][-4.4850645 -4.4492207 -4.338974 -4.2272587 -4.1357927 -4.0394087 -4.0075645 -4.0599585 -4.1243315 -4.1997175 -4.260673 -4.2919083 -4.2671041 -4.1708274 -4.0651917][-4.5592451 -4.5321922 -4.4345069 -4.3514357 -4.2953086 -4.2275567 -4.1927066 -4.2198081 -4.268415 -4.3193879 -4.3361254 -4.3156238 -4.2376189 -4.1037354 -3.9818323][-4.6330471 -4.6150894 -4.5340514 -4.4714441 -4.4411983 -4.4003043 -4.361515 -4.3575935 -4.3848934 -4.41251 -4.399137 -4.3525338 -4.2558622 -4.1168323 -3.9914954][-4.6671028 -4.663753 -4.6042395 -4.5521021 -4.5331969 -4.5140023 -4.4806223 -4.45447 -4.4592829 -4.4721923 -4.4500537 -4.3970709 -4.3073869 -4.191937 -4.0847349][-4.6340265 -4.6569471 -4.6352968 -4.607306 -4.5997391 -4.5947332 -4.56817 -4.5306425 -4.5130448 -4.5170054 -4.50424 -4.4574986 -4.3797245 -4.2895589 -4.2074666][-4.5425396 -4.5889521 -4.6086693 -4.6180415 -4.6322155 -4.6411352 -4.6241221 -4.5855494 -4.5553875 -4.5547366 -4.5555987 -4.5235205 -4.4592557 -4.3871331 -4.326673][-4.4254408 -4.4735804 -4.51805 -4.5593338 -4.59749 -4.6251454 -4.6280727 -4.6076703 -4.5856915 -4.5868006 -4.5949979 -4.5768805 -4.5239143 -4.4622173 -4.4229531]]...]
INFO - root - 2017-12-07 19:16:54.218114: step 41910, loss = 21.56, batch loss = 21.48 (8.5 examples/sec; 0.940 sec/batch; 75h:51m:05s remains)
INFO - root - 2017-12-07 19:17:03.613478: step 41920, loss = 21.64, batch loss = 21.55 (8.2 examples/sec; 0.977 sec/batch; 78h:49m:44s remains)
INFO - root - 2017-12-07 19:17:12.946159: step 41930, loss = 21.45, batch loss = 21.36 (9.4 examples/sec; 0.847 sec/batch; 68h:23m:16s remains)
INFO - root - 2017-12-07 19:17:22.362290: step 41940, loss = 21.41, batch loss = 21.33 (8.7 examples/sec; 0.917 sec/batch; 74h:01m:14s remains)
INFO - root - 2017-12-07 19:17:31.816819: step 41950, loss = 21.47, batch loss = 21.38 (8.8 examples/sec; 0.908 sec/batch; 73h:18m:29s remains)
INFO - root - 2017-12-07 19:17:41.261147: step 41960, loss = 21.78, batch loss = 21.70 (8.5 examples/sec; 0.946 sec/batch; 76h:21m:49s remains)
INFO - root - 2017-12-07 19:17:50.669176: step 41970, loss = 21.40, batch loss = 21.32 (8.7 examples/sec; 0.921 sec/batch; 74h:21m:05s remains)
INFO - root - 2017-12-07 19:18:00.135801: step 41980, loss = 21.35, batch loss = 21.27 (8.0 examples/sec; 0.999 sec/batch; 80h:36m:53s remains)
INFO - root - 2017-12-07 19:18:09.502820: step 41990, loss = 21.62, batch loss = 21.54 (8.1 examples/sec; 0.982 sec/batch; 79h:12m:49s remains)
INFO - root - 2017-12-07 19:18:18.772664: step 42000, loss = 21.96, batch loss = 21.87 (8.6 examples/sec; 0.931 sec/batch; 75h:06m:07s remains)
2017-12-07 19:18:19.625280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4843416 -4.5719619 -4.628623 -4.611269 -4.5495424 -4.4933748 -4.4187865 -4.3273754 -4.2402163 -4.1394491 -4.0566874 -4.0685539 -4.1789403 -4.2909484 -4.3764706][-4.5083108 -4.55384 -4.5833073 -4.564743 -4.5111132 -4.4699373 -4.4115758 -4.341897 -4.2802572 -4.2027764 -4.1383638 -4.1488595 -4.233799 -4.2962894 -4.3413076][-4.53078 -4.5215821 -4.5143552 -4.4989109 -4.4736032 -4.4669905 -4.4368644 -4.396709 -4.3666177 -4.3162231 -4.2666593 -4.2551613 -4.2842464 -4.28903 -4.2973585][-4.5868092 -4.5154524 -4.454514 -4.425837 -4.4246011 -4.4522572 -4.4436269 -4.4263887 -4.4268322 -4.4080849 -4.3718371 -4.3286462 -4.2909608 -4.2504935 -4.2420611][-4.6481037 -4.53199 -4.4140654 -4.3481193 -4.3419561 -4.3715467 -4.3512206 -4.3304162 -4.3550854 -4.3798637 -4.38037 -4.3419347 -4.2738333 -4.2121353 -4.1987677][-4.6777015 -4.5560055 -4.4047766 -4.2858276 -4.234128 -4.2206097 -4.1481528 -4.1015582 -4.1613383 -4.2506557 -4.3052459 -4.3023882 -4.2419124 -4.181807 -4.1731415][-4.6385331 -4.5409169 -4.3826327 -4.2180333 -4.1143045 -4.0460844 -3.9045365 -3.8207779 -3.9319479 -4.1038246 -4.2191529 -4.2716608 -4.2427292 -4.1959715 -4.1895561][-4.5724883 -4.5022397 -4.3486376 -4.1552906 -4.0144148 -3.8988578 -3.6971698 -3.5756483 -3.7387989 -3.9984884 -4.1824756 -4.2926083 -4.295023 -4.2570157 -4.2401156][-4.5567775 -4.4930477 -4.3393846 -4.1392279 -3.9848359 -3.8526618 -3.6451364 -3.5278006 -3.7209871 -4.0223336 -4.2471266 -4.3835483 -4.3902893 -4.3448076 -4.3108611][-4.5915256 -4.5308485 -4.3822689 -4.20424 -4.0719609 -3.9685783 -3.8190949 -3.7453222 -3.9120286 -4.1738882 -4.3745632 -4.4832077 -4.4746957 -4.4240613 -4.3891191][-4.6271033 -4.5796056 -4.4585624 -4.3286238 -4.2449536 -4.1929965 -4.1187496 -4.0830622 -4.1873817 -4.3591342 -4.4931083 -4.5506978 -4.5237761 -4.4684038 -4.4403453][-4.6270671 -4.6047125 -4.5285215 -4.4588771 -4.4276814 -4.424181 -4.4091644 -4.3978238 -4.4451675 -4.5277357 -4.5858736 -4.5880213 -4.538466 -4.47159 -4.4488616][-4.5987616 -4.5942564 -4.556179 -4.5319018 -4.5351524 -4.5564718 -4.5677357 -4.566751 -4.5799279 -4.6015205 -4.6016507 -4.5696077 -4.5137553 -4.4487619 -4.4352455][-4.561635 -4.5573368 -4.5335779 -4.5266585 -4.5409169 -4.5642204 -4.5776982 -4.5776916 -4.5753517 -4.5701389 -4.5506239 -4.5206389 -4.4812322 -4.4361639 -4.4332552][-4.5087981 -4.499382 -4.478363 -4.4719963 -4.4825177 -4.4971437 -4.505827 -4.5078282 -4.5070434 -4.5032506 -4.4930973 -4.479579 -4.4602871 -4.4374485 -4.4413819]]...]
INFO - root - 2017-12-07 19:18:28.757399: step 42010, loss = 21.34, batch loss = 21.26 (9.1 examples/sec; 0.880 sec/batch; 70h:59m:31s remains)
INFO - root - 2017-12-07 19:18:38.146238: step 42020, loss = 21.37, batch loss = 21.29 (8.5 examples/sec; 0.945 sec/batch; 76h:13m:06s remains)
INFO - root - 2017-12-07 19:18:47.643833: step 42030, loss = 21.32, batch loss = 21.24 (8.7 examples/sec; 0.924 sec/batch; 74h:35m:20s remains)
INFO - root - 2017-12-07 19:18:57.083285: step 42040, loss = 21.65, batch loss = 21.57 (8.4 examples/sec; 0.953 sec/batch; 76h:55m:50s remains)
INFO - root - 2017-12-07 19:19:06.542002: step 42050, loss = 21.29, batch loss = 21.21 (8.3 examples/sec; 0.962 sec/batch; 77h:39m:02s remains)
INFO - root - 2017-12-07 19:19:16.026150: step 42060, loss = 21.73, batch loss = 21.65 (7.7 examples/sec; 1.042 sec/batch; 84h:03m:31s remains)
INFO - root - 2017-12-07 19:19:25.401676: step 42070, loss = 21.59, batch loss = 21.51 (8.2 examples/sec; 0.972 sec/batch; 78h:22m:45s remains)
INFO - root - 2017-12-07 19:19:34.797047: step 42080, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.973 sec/batch; 78h:31m:15s remains)
INFO - root - 2017-12-07 19:19:44.152444: step 42090, loss = 21.50, batch loss = 21.41 (9.0 examples/sec; 0.890 sec/batch; 71h:46m:11s remains)
INFO - root - 2017-12-07 19:19:53.456739: step 42100, loss = 21.71, batch loss = 21.62 (8.5 examples/sec; 0.939 sec/batch; 75h:44m:54s remains)
2017-12-07 19:19:54.383450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3824849 -4.42442 -4.41844 -4.3499517 -4.269238 -4.199 -4.1533008 -4.1506791 -4.1827679 -4.2379384 -4.2905354 -4.3137007 -4.3162737 -4.273849 -4.2146869][-4.3948054 -4.4191527 -4.4017558 -4.334116 -4.2509952 -4.1915731 -4.14248 -4.1198378 -4.1468735 -4.2080555 -4.2572861 -4.2663641 -4.2699413 -4.2437778 -4.2064719][-4.4064527 -4.4035096 -4.3747382 -4.316329 -4.2496538 -4.2155023 -4.1791859 -4.1525631 -4.1766114 -4.2387643 -4.2860327 -4.2847357 -4.28047 -4.25747 -4.2272177][-4.3686433 -4.3389082 -4.3026681 -4.2603536 -4.2226467 -4.2201152 -4.2064795 -4.1893311 -4.2182913 -4.2859178 -4.3362103 -4.3338747 -4.3240819 -4.2975039 -4.265873][-4.2916231 -4.2395205 -4.1901741 -4.150713 -4.1275516 -4.1437058 -4.1523819 -4.159833 -4.2161832 -4.3027234 -4.3640409 -4.3707709 -4.3581839 -4.3263221 -4.2927175][-4.2330661 -4.1795826 -4.1156011 -4.0631 -4.0299964 -4.0373259 -4.0510144 -4.0753875 -4.1575975 -4.2565331 -4.3247414 -4.3426104 -4.3308444 -4.2985654 -4.267055][-4.2126536 -4.1759219 -4.0961604 -4.0200238 -3.9615548 -3.9384756 -3.9393876 -3.9724329 -4.0693569 -4.1725473 -4.241498 -4.2630205 -4.2503595 -4.2176914 -4.1942782][-4.224421 -4.2066007 -4.1138492 -4.02772 -3.9669201 -3.931227 -3.9235868 -3.9611826 -4.0521894 -4.1401424 -4.1862984 -4.1890922 -4.1698089 -4.1377039 -4.1278973][-4.2545018 -4.2483053 -4.1519208 -4.0691657 -4.0303364 -4.0031714 -3.9975348 -4.0410881 -4.1188211 -4.1877294 -4.2036304 -4.1746507 -4.1437969 -4.1112928 -4.1109314][-4.2941895 -4.2994123 -4.2122684 -4.1367607 -4.1178179 -4.0903354 -4.0726018 -4.1084714 -4.1737332 -4.2337027 -4.2304392 -4.183403 -4.1613131 -4.1416183 -4.1483512][-4.3431921 -4.3741512 -4.3176389 -4.255991 -4.2429361 -4.20557 -4.1672993 -4.1803155 -4.2267685 -4.2711067 -4.247798 -4.1918941 -4.1901522 -4.1953163 -4.2114377][-4.3947954 -4.4545426 -4.4361992 -4.3910093 -4.3766665 -4.3326354 -4.2830281 -4.2793851 -4.312191 -4.3382888 -4.2961564 -4.2354331 -4.2468057 -4.270153 -4.2864013][-4.4170704 -4.5096555 -4.5306072 -4.504899 -4.4899373 -4.4522324 -4.4102411 -4.4014215 -4.4232254 -4.4319148 -4.3812623 -4.3280296 -4.3481436 -4.3735671 -4.3757553][-4.4155836 -4.53233 -4.5838041 -4.5779309 -4.5640845 -4.5405784 -4.5147219 -4.5050545 -4.5176258 -4.521709 -4.4851365 -4.449852 -4.46723 -4.4753714 -4.4535117][-4.3870125 -4.4973612 -4.5560236 -4.5649824 -4.5591249 -4.5536518 -4.5455251 -4.5372906 -4.5433764 -4.5529656 -4.5453663 -4.5377183 -4.5495262 -4.5381455 -4.4992261]]...]
INFO - root - 2017-12-07 19:20:03.253324: step 42110, loss = 21.60, batch loss = 21.52 (8.4 examples/sec; 0.948 sec/batch; 76h:28m:31s remains)
INFO - root - 2017-12-07 19:20:12.667657: step 42120, loss = 21.35, batch loss = 21.27 (8.3 examples/sec; 0.959 sec/batch; 77h:20m:28s remains)
INFO - root - 2017-12-07 19:20:22.119618: step 42130, loss = 21.86, batch loss = 21.78 (8.5 examples/sec; 0.937 sec/batch; 75h:33m:03s remains)
INFO - root - 2017-12-07 19:20:31.554706: step 42140, loss = 21.76, batch loss = 21.67 (8.8 examples/sec; 0.909 sec/batch; 73h:18m:25s remains)
INFO - root - 2017-12-07 19:20:40.991160: step 42150, loss = 21.32, batch loss = 21.24 (8.7 examples/sec; 0.921 sec/batch; 74h:18m:51s remains)
INFO - root - 2017-12-07 19:20:50.456114: step 42160, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.914 sec/batch; 73h:44m:05s remains)
INFO - root - 2017-12-07 19:20:59.848678: step 42170, loss = 21.93, batch loss = 21.84 (8.5 examples/sec; 0.936 sec/batch; 75h:28m:20s remains)
INFO - root - 2017-12-07 19:21:09.319120: step 42180, loss = 21.48, batch loss = 21.39 (8.1 examples/sec; 0.989 sec/batch; 79h:44m:51s remains)
INFO - root - 2017-12-07 19:21:18.865353: step 42190, loss = 21.77, batch loss = 21.68 (8.3 examples/sec; 0.964 sec/batch; 77h:43m:03s remains)
INFO - root - 2017-12-07 19:21:28.343016: step 42200, loss = 21.54, batch loss = 21.46 (8.5 examples/sec; 0.941 sec/batch; 75h:54m:04s remains)
2017-12-07 19:21:29.420520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.39331 -4.3969092 -4.3666091 -4.3003569 -4.22308 -4.1540394 -4.1221943 -4.134306 -4.1699333 -4.2159014 -4.2729425 -4.3201103 -4.3368788 -4.3422079 -4.3660522][-4.3196335 -4.345993 -4.3419318 -4.2887945 -4.2062206 -4.1239676 -4.0851188 -4.0915904 -4.1192074 -4.1485043 -4.1836205 -4.2124958 -4.2183485 -4.21413 -4.224638][-4.2900677 -4.3412886 -4.3626275 -4.3276877 -4.24963 -4.1663346 -4.1217909 -4.11296 -4.120173 -4.1250148 -4.1313462 -4.1355824 -4.1308708 -4.1218719 -4.1179056][-4.2971654 -4.3658619 -4.406116 -4.388916 -4.3192377 -4.2406173 -4.1867042 -4.1525555 -4.1365952 -4.1314878 -4.1374965 -4.1454926 -4.1516404 -4.1441021 -4.117753][-4.327477 -4.402626 -4.4446197 -4.4272609 -4.3560424 -4.2759743 -4.2042894 -4.13555 -4.0945935 -4.0962448 -4.1298952 -4.1686115 -4.2027416 -4.2013426 -4.1521473][-4.3413777 -4.4117522 -4.4418592 -4.4074645 -4.3193369 -4.2235041 -4.1230946 -4.0196686 -3.9668853 -3.995542 -4.0744886 -4.1607637 -4.22999 -4.2446709 -4.1840649][-4.3264594 -4.3915377 -4.4146667 -4.3719015 -4.2758217 -4.16489 -4.0315681 -3.8922 -3.8351355 -3.8998857 -4.0327239 -4.1729107 -4.2738686 -4.30064 -4.2310991][-4.2971396 -4.3542271 -4.3755522 -4.3414841 -4.2653055 -4.1647749 -4.0170727 -3.8550305 -3.792896 -3.8729653 -4.0243182 -4.183723 -4.2939649 -4.3269706 -4.2615047][-4.2756557 -4.3258672 -4.3471031 -4.3302774 -4.2890348 -4.2205129 -4.0902796 -3.9351728 -3.8707509 -3.9373155 -4.0711741 -4.2185326 -4.3224854 -4.3568 -4.3015065][-4.284451 -4.33661 -4.3633747 -4.3646069 -4.354784 -4.3206997 -4.2259655 -4.0993209 -4.0392928 -4.0831966 -4.194551 -4.323657 -4.4150615 -4.4368119 -4.3809605][-4.3156395 -4.3682785 -4.40205 -4.4182343 -4.4306469 -4.4245334 -4.3688231 -4.2797489 -4.2272258 -4.2452559 -4.3236279 -4.4190712 -4.4840136 -4.4867792 -4.4303412][-4.3353391 -4.3832526 -4.42197 -4.447762 -4.4716105 -4.4823179 -4.4602361 -4.4094481 -4.3699613 -4.3670459 -4.4088583 -4.4652405 -4.5038176 -4.4997673 -4.4535427][-4.3250151 -4.3655009 -4.4071317 -4.4401264 -4.4696836 -4.488616 -4.4877577 -4.4647312 -4.4397125 -4.4254146 -4.4375329 -4.4620433 -4.4844618 -4.4857283 -4.457128][-4.2774181 -4.3064241 -4.3463626 -4.3832974 -4.416955 -4.4393921 -4.4470644 -4.4378057 -4.4211535 -4.3975887 -4.3847027 -4.3844619 -4.4011602 -4.4176388 -4.4145813][-4.2194629 -4.2413154 -4.2771769 -4.3122807 -4.3433647 -4.3620629 -4.36952 -4.3676529 -4.36018 -4.3368483 -4.30863 -4.2904816 -4.3040338 -4.3333154 -4.3555012]]...]
INFO - root - 2017-12-07 19:21:38.879245: step 42210, loss = 21.57, batch loss = 21.49 (8.8 examples/sec; 0.908 sec/batch; 73h:13m:28s remains)
INFO - root - 2017-12-07 19:21:48.155089: step 42220, loss = 21.27, batch loss = 21.19 (9.2 examples/sec; 0.871 sec/batch; 70h:12m:22s remains)
INFO - root - 2017-12-07 19:21:57.708106: step 42230, loss = 21.52, batch loss = 21.44 (8.7 examples/sec; 0.920 sec/batch; 74h:11m:31s remains)
INFO - root - 2017-12-07 19:22:07.191372: step 42240, loss = 21.51, batch loss = 21.43 (8.6 examples/sec; 0.936 sec/batch; 75h:26m:04s remains)
INFO - root - 2017-12-07 19:22:16.647395: step 42250, loss = 21.54, batch loss = 21.46 (8.5 examples/sec; 0.936 sec/batch; 75h:27m:09s remains)
INFO - root - 2017-12-07 19:22:26.019705: step 42260, loss = 21.76, batch loss = 21.68 (8.7 examples/sec; 0.921 sec/batch; 74h:15m:57s remains)
INFO - root - 2017-12-07 19:22:35.495999: step 42270, loss = 21.12, batch loss = 21.03 (8.3 examples/sec; 0.969 sec/batch; 78h:09m:12s remains)
INFO - root - 2017-12-07 19:22:44.894586: step 42280, loss = 21.14, batch loss = 21.05 (8.7 examples/sec; 0.914 sec/batch; 73h:43m:17s remains)
INFO - root - 2017-12-07 19:22:54.252859: step 42290, loss = 21.74, batch loss = 21.66 (8.5 examples/sec; 0.942 sec/batch; 75h:57m:50s remains)
INFO - root - 2017-12-07 19:23:03.751468: step 42300, loss = 21.09, batch loss = 21.01 (8.1 examples/sec; 0.989 sec/batch; 79h:44m:22s remains)
2017-12-07 19:23:04.716831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7190995 -4.718667 -4.6764011 -4.6564617 -4.6836696 -4.7014656 -4.661654 -4.5867758 -4.5373821 -4.5307384 -4.5541139 -4.5941119 -4.5868788 -4.5087047 -4.4042697][-4.6902018 -4.7200532 -4.6904039 -4.6623845 -4.6728473 -4.6562738 -4.5684805 -4.4739585 -4.4675765 -4.5378742 -4.6193466 -4.6829538 -4.6676683 -4.5562253 -4.4127183][-4.5286942 -4.6039872 -4.6206789 -4.614779 -4.6206617 -4.5663214 -4.4247918 -4.3131852 -4.3608046 -4.517302 -4.6569505 -4.7361932 -4.7203455 -4.6026187 -4.4414258][-4.2877507 -4.3995667 -4.4746804 -4.5058413 -4.5153975 -4.4372883 -4.2565112 -4.128686 -4.2021341 -4.4047852 -4.5769348 -4.66844 -4.68045 -4.6076379 -4.4745502][-4.1090207 -4.2319427 -4.3434691 -4.3981886 -4.3981781 -4.2918897 -4.0900059 -3.953913 -4.0246167 -4.2313628 -4.4100327 -4.51279 -4.5693164 -4.5683 -4.4944925][-4.0693283 -4.1767268 -4.2817569 -4.3163419 -4.2676482 -4.1116805 -3.8999424 -3.7764261 -3.85154 -4.0607171 -4.2454762 -4.3653135 -4.461062 -4.5165224 -4.4940662][-4.1434326 -4.2312374 -4.291389 -4.2641625 -4.1318879 -3.9062257 -3.6909766 -3.6045258 -3.7127063 -3.9476948 -4.1560416 -4.3005185 -4.4136577 -4.4852972 -4.47885][-4.2276783 -4.32653 -4.3525105 -4.2681589 -4.0660019 -3.79562 -3.5999336 -3.5578852 -3.6921644 -3.9366457 -4.1576905 -4.3210359 -4.4358253 -4.4931874 -4.470726][-4.2590981 -4.3823605 -4.4106741 -4.3169255 -4.1079826 -3.8611672 -3.7111185 -3.6857245 -3.7962132 -4.0029407 -4.2067847 -4.3778472 -4.4931226 -4.5329108 -4.4867411][-4.2278886 -4.3685656 -4.4074297 -4.34113 -4.1936011 -4.0431929 -3.9600911 -3.9230466 -3.9660776 -4.0990982 -4.2637854 -4.4290991 -4.5460052 -4.58049 -4.5209475][-4.197988 -4.3302455 -4.3707075 -4.3510604 -4.3045349 -4.2796793 -4.2603555 -4.1983595 -4.1638269 -4.2185879 -4.3377876 -4.4859643 -4.5972123 -4.6281424 -4.5587988][-4.2593684 -4.3629417 -4.3944507 -4.4020166 -4.4205503 -4.4658318 -4.4722633 -4.3940439 -4.3171325 -4.3275037 -4.4174047 -4.5452075 -4.64211 -4.6631317 -4.5827017][-4.41121 -4.4938984 -4.5110474 -4.5108724 -4.5238323 -4.5553994 -4.5439506 -4.4606619 -4.3826032 -4.3867564 -4.4660673 -4.5735822 -4.6508489 -4.6599603 -4.573884][-4.5686436 -4.6250615 -4.6251621 -4.6013722 -4.5753012 -4.5565343 -4.5118341 -4.4312654 -4.3724561 -4.389801 -4.4706059 -4.5638 -4.621119 -4.6152296 -4.5285587][-4.615562 -4.6397529 -4.6244717 -4.58542 -4.5359964 -4.4872174 -4.4290395 -4.3640823 -4.3304105 -4.3604345 -4.4374709 -4.5128284 -4.5504346 -4.5326624 -4.4557023]]...]
INFO - root - 2017-12-07 19:23:14.358000: step 42310, loss = 21.15, batch loss = 21.07 (8.5 examples/sec; 0.942 sec/batch; 75h:56m:57s remains)
INFO - root - 2017-12-07 19:23:23.704578: step 42320, loss = 21.29, batch loss = 21.20 (8.2 examples/sec; 0.977 sec/batch; 78h:44m:04s remains)
INFO - root - 2017-12-07 19:23:33.119866: step 42330, loss = 21.36, batch loss = 21.27 (9.1 examples/sec; 0.881 sec/batch; 71h:01m:15s remains)
INFO - root - 2017-12-07 19:23:42.491211: step 42340, loss = 21.40, batch loss = 21.32 (8.7 examples/sec; 0.925 sec/batch; 74h:31m:57s remains)
INFO - root - 2017-12-07 19:23:51.994733: step 42350, loss = 21.36, batch loss = 21.28 (8.8 examples/sec; 0.913 sec/batch; 73h:35m:26s remains)
INFO - root - 2017-12-07 19:24:01.392177: step 42360, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.947 sec/batch; 76h:21m:08s remains)
INFO - root - 2017-12-07 19:24:10.733468: step 42370, loss = 21.28, batch loss = 21.20 (8.3 examples/sec; 0.964 sec/batch; 77h:41m:23s remains)
INFO - root - 2017-12-07 19:24:20.085213: step 42380, loss = 21.71, batch loss = 21.63 (8.7 examples/sec; 0.919 sec/batch; 74h:04m:33s remains)
INFO - root - 2017-12-07 19:24:29.424957: step 42390, loss = 21.21, batch loss = 21.13 (8.7 examples/sec; 0.921 sec/batch; 74h:15m:26s remains)
INFO - root - 2017-12-07 19:24:38.812637: step 42400, loss = 21.22, batch loss = 21.13 (8.7 examples/sec; 0.915 sec/batch; 73h:45m:17s remains)
2017-12-07 19:24:39.695100: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5271111 -4.5866094 -4.6272955 -4.6525354 -4.66123 -4.6551976 -4.6489449 -4.6477733 -4.6568394 -4.6767874 -4.6866622 -4.6622066 -4.5971265 -4.5080652 -4.4223452][-4.6695361 -4.7443357 -4.7601719 -4.7312388 -4.6793733 -4.6306176 -4.6110935 -4.62045 -4.6581163 -4.7205715 -4.7719326 -4.7674112 -4.6953826 -4.5780649 -4.45995][-4.8102479 -4.8858881 -4.8562064 -4.742662 -4.6022973 -4.4955363 -4.4605894 -4.4895682 -4.5705094 -4.6955132 -4.808352 -4.8409472 -4.7755466 -4.6396556 -4.4932666][-4.859129 -4.9118276 -4.8349323 -4.6480985 -4.4350042 -4.2849174 -4.246284 -4.3044753 -4.4341054 -4.623898 -4.797081 -4.8697186 -4.8223133 -4.6824846 -4.5186949][-4.801053 -4.8013816 -4.6738033 -4.4390779 -4.1888466 -4.0204983 -3.9875498 -4.0693288 -4.2331681 -4.4743433 -4.6989031 -4.8155003 -4.8021116 -4.67741 -4.5138469][-4.7037969 -4.6362696 -4.4587297 -4.1976089 -3.9366622 -3.7577825 -3.7185271 -3.8049614 -3.9901841 -4.2767019 -4.5529652 -4.7195563 -4.7479854 -4.6407256 -4.4813089][-4.6432381 -4.5227966 -4.3101711 -4.0461631 -3.7951245 -3.6107435 -3.5530241 -3.6257744 -3.8223171 -4.152976 -4.4829969 -4.6868615 -4.7331147 -4.6207638 -4.4495597][-4.661046 -4.5320544 -4.3128018 -4.0607009 -3.8249106 -3.638741 -3.5620008 -3.6129651 -3.8059018 -4.1667953 -4.5397449 -4.7587609 -4.7894549 -4.6466665 -4.4445534][-4.7333136 -4.642314 -4.453927 -4.2241726 -4.0029907 -3.8216424 -3.735683 -3.7673678 -3.9425535 -4.2978148 -4.6739225 -4.8724217 -4.8637466 -4.6843133 -4.4524293][-4.7793097 -4.7500763 -4.6189265 -4.4249797 -4.228075 -4.0655117 -3.9811749 -3.9936311 -4.1317806 -4.4386282 -4.7695136 -4.92442 -4.8820763 -4.6889248 -4.453917][-4.7540379 -4.770534 -4.7025623 -4.564405 -4.4166627 -4.2996073 -4.2378778 -4.2383552 -4.3304086 -4.5562506 -4.8030529 -4.9063044 -4.8509412 -4.6724911 -4.4635463][-4.6764035 -4.714673 -4.7024946 -4.6370564 -4.564651 -4.5150404 -4.4952974 -4.4988976 -4.5509295 -4.6841273 -4.8282371 -4.8762379 -4.8161292 -4.6646609 -4.4904203][-4.5773439 -4.6110492 -4.6264553 -4.6188517 -4.6152873 -4.6274858 -4.6478271 -4.6614237 -4.6871729 -4.747488 -4.8089957 -4.8166475 -4.7587414 -4.6372008 -4.4980693][-4.4742584 -4.4918079 -4.5056915 -4.5174012 -4.5392742 -4.5705004 -4.5999994 -4.6178122 -4.633955 -4.6574435 -4.67618 -4.6694274 -4.6272769 -4.5460405 -4.4507017][-4.3755875 -4.379303 -4.3825665 -4.3904905 -4.4078526 -4.429791 -4.4480734 -4.460053 -4.4705186 -4.4805222 -4.4870386 -4.4832582 -4.4625421 -4.4206929 -4.368227]]...]
INFO - root - 2017-12-07 19:24:49.069157: step 42410, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.973 sec/batch; 78h:25m:46s remains)
INFO - root - 2017-12-07 19:24:58.478955: step 42420, loss = 20.84, batch loss = 20.76 (8.3 examples/sec; 0.958 sec/batch; 77h:12m:05s remains)
INFO - root - 2017-12-07 19:25:07.815192: step 42430, loss = 21.46, batch loss = 21.38 (8.2 examples/sec; 0.972 sec/batch; 78h:19m:17s remains)
INFO - root - 2017-12-07 19:25:17.300631: step 42440, loss = 21.41, batch loss = 21.32 (8.0 examples/sec; 1.000 sec/batch; 80h:33m:53s remains)
INFO - root - 2017-12-07 19:25:26.663109: step 42450, loss = 21.64, batch loss = 21.55 (8.9 examples/sec; 0.903 sec/batch; 72h:47m:37s remains)
INFO - root - 2017-12-07 19:25:36.155550: step 42460, loss = 21.29, batch loss = 21.21 (8.5 examples/sec; 0.945 sec/batch; 76h:09m:21s remains)
INFO - root - 2017-12-07 19:25:45.549847: step 42470, loss = 21.13, batch loss = 21.05 (9.1 examples/sec; 0.882 sec/batch; 71h:02m:04s remains)
INFO - root - 2017-12-07 19:25:54.710683: step 42480, loss = 21.72, batch loss = 21.64 (8.7 examples/sec; 0.920 sec/batch; 74h:05m:41s remains)
INFO - root - 2017-12-07 19:26:04.097844: step 42490, loss = 21.54, batch loss = 21.46 (8.2 examples/sec; 0.980 sec/batch; 78h:56m:57s remains)
INFO - root - 2017-12-07 19:26:13.631109: step 42500, loss = 21.44, batch loss = 21.36 (8.7 examples/sec; 0.919 sec/batch; 73h:59m:57s remains)
2017-12-07 19:26:14.618049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.48126 -4.4824605 -4.5204782 -4.55116 -4.5592856 -4.5518832 -4.5637665 -4.58318 -4.5919623 -4.589674 -4.5297956 -4.4248447 -4.351335 -4.3385425 -4.3803477][-4.4365544 -4.4190941 -4.46056 -4.5126252 -4.5306921 -4.5049024 -4.4784184 -4.4716105 -4.4888797 -4.5100474 -4.4648337 -4.3532748 -4.2624187 -4.2410932 -4.2812762][-4.4225812 -4.3719535 -4.3921714 -4.4429293 -4.4587312 -4.4075 -4.3335757 -4.2965174 -4.3365269 -4.4141488 -4.4291687 -4.355011 -4.270412 -4.2393446 -4.2599421][-4.4488916 -4.365335 -4.35373 -4.38378 -4.3834739 -4.3064733 -4.1893163 -4.12349 -4.183629 -4.3182664 -4.4043 -4.3858485 -4.3184905 -4.2766285 -4.2764378][-4.4962268 -4.3923273 -4.3467417 -4.3421474 -4.3121538 -4.2075925 -4.055789 -3.9653575 -4.0335512 -4.20315 -4.3439755 -4.3791885 -4.3272543 -4.2696671 -4.2466][-4.5113187 -4.408586 -4.3399982 -4.2991595 -4.2300653 -4.0947714 -3.9208112 -3.8209414 -3.8964663 -4.0847321 -4.2593369 -4.3379474 -4.3038135 -4.2342205 -4.1918149][-4.4454169 -4.3554096 -4.2846293 -4.2258892 -4.1326075 -3.9789877 -3.7975585 -3.701185 -3.7916143 -3.9990563 -4.200273 -4.3141694 -4.3006406 -4.2262645 -4.1712823][-4.3285222 -4.2546506 -4.2027 -4.1602526 -4.081903 -3.9423277 -3.7771115 -3.690146 -3.7838206 -3.9946721 -4.2053633 -4.3355684 -4.3340454 -4.2567582 -4.1899886][-4.2422328 -4.1852612 -4.16505 -4.1664257 -4.1357608 -4.0422449 -3.9183657 -3.8505223 -3.924319 -4.0953341 -4.27017 -4.3816576 -4.3781729 -4.3014231 -4.2295914][-4.2374811 -4.1878095 -4.1845875 -4.2214165 -4.2340317 -4.1835504 -4.1016216 -4.0570097 -4.1098065 -4.2236385 -4.3415279 -4.4245682 -4.4267411 -4.3649969 -4.2967696][-4.2827778 -4.2333736 -4.2258759 -4.2730746 -4.30611 -4.2779989 -4.2180672 -4.1885915 -4.2300105 -4.3077474 -4.3931766 -4.4689589 -4.4865127 -4.4384604 -4.3633204][-4.3389206 -4.2920575 -4.2687497 -4.3045111 -4.3401575 -4.32112 -4.2678127 -4.24342 -4.2849655 -4.3613091 -4.4531803 -4.5419168 -4.563283 -4.5044575 -4.40244][-4.4001722 -4.3618784 -4.3299627 -4.3462152 -4.372191 -4.348763 -4.2835875 -4.2479897 -4.2903156 -4.3870063 -4.5051661 -4.6021667 -4.5990596 -4.5014243 -4.3629065][-4.4595428 -4.4450588 -4.4193215 -4.4217772 -4.4358954 -4.3986135 -4.3092585 -4.2492576 -4.280529 -4.3871708 -4.5152869 -4.5963511 -4.5463438 -4.40169 -4.2376723][-4.4740496 -4.4839606 -4.4754148 -4.4787846 -4.49361 -4.453897 -4.3540869 -4.2814951 -4.2977786 -4.3912454 -4.504374 -4.5563407 -4.4699669 -4.3048062 -4.1448789]]...]
INFO - root - 2017-12-07 19:26:24.306339: step 42510, loss = 21.90, batch loss = 21.81 (8.2 examples/sec; 0.977 sec/batch; 78h:39m:53s remains)
INFO - root - 2017-12-07 19:26:33.686284: step 42520, loss = 21.14, batch loss = 21.06 (8.4 examples/sec; 0.955 sec/batch; 76h:53m:10s remains)
INFO - root - 2017-12-07 19:26:43.104942: step 42530, loss = 21.50, batch loss = 21.42 (8.5 examples/sec; 0.940 sec/batch; 75h:41m:47s remains)
INFO - root - 2017-12-07 19:26:52.552527: step 42540, loss = 21.28, batch loss = 21.20 (8.3 examples/sec; 0.959 sec/batch; 77h:13m:24s remains)
INFO - root - 2017-12-07 19:27:01.987966: step 42550, loss = 21.52, batch loss = 21.44 (9.5 examples/sec; 0.838 sec/batch; 67h:31m:37s remains)
INFO - root - 2017-12-07 19:27:11.403784: step 42560, loss = 21.46, batch loss = 21.37 (8.9 examples/sec; 0.898 sec/batch; 72h:18m:59s remains)
INFO - root - 2017-12-07 19:27:20.862820: step 42570, loss = 20.81, batch loss = 20.72 (8.2 examples/sec; 0.977 sec/batch; 78h:42m:43s remains)
INFO - root - 2017-12-07 19:27:30.108150: step 42580, loss = 21.27, batch loss = 21.19 (8.6 examples/sec; 0.927 sec/batch; 74h:39m:35s remains)
INFO - root - 2017-12-07 19:27:39.569646: step 42590, loss = 21.45, batch loss = 21.37 (8.5 examples/sec; 0.946 sec/batch; 76h:10m:34s remains)
INFO - root - 2017-12-07 19:27:48.923830: step 42600, loss = 21.29, batch loss = 21.20 (8.1 examples/sec; 0.990 sec/batch; 79h:41m:57s remains)
2017-12-07 19:27:49.851626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.350596 -4.3870282 -4.431366 -4.4538388 -4.4520612 -4.4352493 -4.4361591 -4.4654765 -4.5003266 -4.5415921 -4.5512042 -4.5172787 -4.4810309 -4.453198 -4.4545712][-4.3038516 -4.3479586 -4.3972821 -4.4089761 -4.38882 -4.3506889 -4.3429909 -4.3846784 -4.44027 -4.4998546 -4.5058937 -4.4556289 -4.4080257 -4.3719316 -4.3820219][-4.2562089 -4.3120513 -4.3696632 -4.3686986 -4.3190861 -4.2449956 -4.2151427 -4.2655954 -4.3533 -4.4502726 -4.4727826 -4.4225764 -4.3665915 -4.3120713 -4.3105063][-4.2263856 -4.2943139 -4.3601875 -4.3500504 -4.2679663 -4.1490707 -4.0862961 -4.1327963 -4.2476358 -4.3904562 -4.4517736 -4.4252329 -4.3745165 -4.29863 -4.26093][-4.2200837 -4.2933378 -4.3608847 -4.342577 -4.2343597 -4.0816255 -3.9928048 -4.02319 -4.1415682 -4.3074231 -4.4051685 -4.4160576 -4.3907313 -4.3146505 -4.247952][-4.2364655 -4.3104496 -4.3715773 -4.3426929 -4.2164741 -4.0485263 -3.9518518 -3.9656663 -4.0668449 -4.2262406 -4.3393145 -4.3810482 -4.3889318 -4.3412328 -4.2820292][-4.2697291 -4.3462467 -4.3980713 -4.3488526 -4.1958284 -4.0103364 -3.9027088 -3.8961918 -3.9744978 -4.1176448 -4.2400746 -4.3059578 -4.3412261 -4.332015 -4.3084769][-4.3120494 -4.3945956 -4.4361491 -4.3564506 -4.1670761 -3.9569075 -3.8265958 -3.795428 -3.851707 -3.9838676 -4.11127 -4.1865034 -4.2315817 -4.2515793 -4.2717538][-4.3511534 -4.4395366 -4.4736371 -4.3665438 -4.1459856 -3.9097359 -3.7524734 -3.7065151 -3.7583241 -3.8924255 -4.0156608 -4.0803142 -4.1153607 -4.1481986 -4.20077][-4.3837309 -4.4731417 -4.5048056 -4.390893 -4.1685028 -3.9320171 -3.7679627 -3.7193353 -3.7654479 -3.8931563 -3.9994776 -4.0432754 -4.0662179 -4.0999012 -4.1666756][-4.40524 -4.4902782 -4.5297642 -4.4426427 -4.2607374 -4.0638628 -3.9205136 -3.8717823 -3.8904603 -3.9830923 -4.0628591 -4.096509 -4.122756 -4.1586857 -4.2253833][-4.4135232 -4.4941311 -4.5514622 -4.5142751 -4.3964005 -4.2534056 -4.1392059 -4.0872335 -4.0741286 -4.1248431 -4.1863394 -4.2289538 -4.2732315 -4.3135037 -4.3647776][-4.4065604 -4.475843 -4.5422711 -4.5503736 -4.4978309 -4.4137769 -4.3341908 -4.2840385 -4.2519464 -4.2674909 -4.3082805 -4.3548694 -4.4070196 -4.4413319 -4.4672737][-4.397542 -4.4477253 -4.5102258 -4.5466495 -4.5475879 -4.51941 -4.4795589 -4.44561 -4.4177175 -4.41627 -4.4348989 -4.4668765 -4.5056634 -4.5234103 -4.5234065][-4.4003363 -4.4341831 -4.485527 -4.5301433 -4.5586195 -4.5660195 -4.5570374 -4.5417051 -4.5284858 -4.5270143 -4.5329437 -4.5471616 -4.5658259 -4.5693784 -4.5575333]]...]
INFO - root - 2017-12-07 19:27:59.324470: step 42610, loss = 21.60, batch loss = 21.52 (8.4 examples/sec; 0.949 sec/batch; 76h:24m:01s remains)
INFO - root - 2017-12-07 19:28:08.797407: step 42620, loss = 21.42, batch loss = 21.34 (8.2 examples/sec; 0.978 sec/batch; 78h:43m:06s remains)
INFO - root - 2017-12-07 19:28:17.945264: step 42630, loss = 21.46, batch loss = 21.37 (8.7 examples/sec; 0.920 sec/batch; 74h:06m:23s remains)
INFO - root - 2017-12-07 19:28:27.310892: step 42640, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.948 sec/batch; 76h:20m:24s remains)
INFO - root - 2017-12-07 19:28:36.810742: step 42650, loss = 21.38, batch loss = 21.30 (8.4 examples/sec; 0.947 sec/batch; 76h:16m:56s remains)
INFO - root - 2017-12-07 19:28:46.294286: step 42660, loss = 21.56, batch loss = 21.48 (8.8 examples/sec; 0.910 sec/batch; 73h:14m:36s remains)
INFO - root - 2017-12-07 19:28:55.725289: step 42670, loss = 21.93, batch loss = 21.84 (8.6 examples/sec; 0.927 sec/batch; 74h:36m:59s remains)
INFO - root - 2017-12-07 19:29:05.134300: step 42680, loss = 21.35, batch loss = 21.27 (8.0 examples/sec; 1.005 sec/batch; 80h:56m:32s remains)
INFO - root - 2017-12-07 19:29:14.663554: step 42690, loss = 21.57, batch loss = 21.49 (8.0 examples/sec; 1.005 sec/batch; 80h:56m:01s remains)
INFO - root - 2017-12-07 19:29:24.044860: step 42700, loss = 21.53, batch loss = 21.45 (8.8 examples/sec; 0.908 sec/batch; 73h:06m:30s remains)
2017-12-07 19:29:24.986370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4768195 -4.4919877 -4.4973373 -4.4915853 -4.4901457 -4.492074 -4.4872832 -4.4702535 -4.4434581 -4.4167695 -4.4185085 -4.4797521 -4.5641026 -4.5879574 -4.5155377][-4.4720922 -4.4935746 -4.4961367 -4.4808216 -4.46967 -4.4686222 -4.4696174 -4.4664311 -4.453125 -4.434412 -4.4450445 -4.513114 -4.5828314 -4.5732474 -4.4482164][-4.4506674 -4.4714818 -4.4677682 -4.4384875 -4.4106612 -4.3981361 -4.4025583 -4.4233146 -4.4458346 -4.4578338 -4.4899707 -4.5569119 -4.5947714 -4.5422554 -4.3752522][-4.4236832 -4.4411788 -4.4281788 -4.3788562 -4.3249278 -4.2859125 -4.2792306 -4.3223305 -4.3968492 -4.4645219 -4.533277 -4.6021385 -4.6132832 -4.531487 -4.3539033][-4.3968377 -4.4105811 -4.3831172 -4.3055482 -4.2128649 -4.1255307 -4.0806375 -4.1304822 -4.2620931 -4.3987594 -4.5141087 -4.6001387 -4.6158886 -4.5396605 -4.3856192][-4.3621016 -4.3738232 -4.3338323 -4.2263374 -4.0829782 -3.9265871 -3.8260605 -3.8747621 -4.0602574 -4.2611094 -4.4177713 -4.5327072 -4.583838 -4.5454769 -4.4364023][-4.3434639 -4.3552604 -4.31524 -4.18783 -3.9899483 -3.760004 -3.6091008 -3.6600518 -3.8848948 -4.1266584 -4.3097477 -4.4521785 -4.5415649 -4.5420437 -4.4691153][-4.3577061 -4.3797364 -4.357358 -4.2294188 -3.9931555 -3.7147961 -3.5432458 -3.5963812 -3.825263 -4.0696859 -4.2571855 -4.4091973 -4.5132408 -4.5311174 -4.4732509][-4.3989096 -4.4349408 -4.4399228 -4.3354869 -4.1071572 -3.8399954 -3.686883 -3.7332547 -3.9196868 -4.1253395 -4.2915411 -4.4266453 -4.5120015 -4.5212879 -4.4647322][-4.4495206 -4.4921918 -4.519691 -4.4567947 -4.2863274 -4.08588 -3.9768715 -4.0069137 -4.1284027 -4.272521 -4.3970442 -4.4909019 -4.5309014 -4.510438 -4.4433022][-4.4783177 -4.5243812 -4.5645571 -4.5441165 -4.4496241 -4.3343105 -4.2739763 -4.2875991 -4.3455772 -4.4222484 -4.495162 -4.5430121 -4.5404153 -4.491415 -4.41002][-4.4632516 -4.5108843 -4.55694 -4.5695872 -4.5424986 -4.5016074 -4.4773617 -4.472569 -4.47734 -4.4971213 -4.5289078 -4.5534949 -4.5367985 -4.4767079 -4.3867826][-4.3927512 -4.4408469 -4.4933186 -4.5280395 -4.5445809 -4.5505733 -4.5455732 -4.5262518 -4.4988995 -4.4871879 -4.5043216 -4.5333204 -4.5277562 -4.4706368 -4.3802986][-4.2780805 -4.3212967 -4.3773723 -4.4227729 -4.4604468 -4.4881406 -4.4911337 -4.4653468 -4.4278398 -4.4113078 -4.4352937 -4.4814591 -4.4981532 -4.4611912 -4.392127][-4.1873918 -4.2187743 -4.2667837 -4.3043809 -4.3380189 -4.3633237 -4.3621583 -4.3340778 -4.3009534 -4.2939792 -4.331192 -4.3968358 -4.443749 -4.4444613 -4.4170432]]...]
INFO - root - 2017-12-07 19:29:34.277377: step 42710, loss = 21.21, batch loss = 21.13 (8.3 examples/sec; 0.962 sec/batch; 77h:27m:35s remains)
INFO - root - 2017-12-07 19:29:43.704266: step 42720, loss = 21.41, batch loss = 21.33 (8.8 examples/sec; 0.906 sec/batch; 72h:57m:51s remains)
INFO - root - 2017-12-07 19:29:52.979479: step 42730, loss = 20.97, batch loss = 20.89 (9.2 examples/sec; 0.867 sec/batch; 69h:45m:33s remains)
INFO - root - 2017-12-07 19:30:02.563206: step 42740, loss = 21.61, batch loss = 21.52 (8.5 examples/sec; 0.946 sec/batch; 76h:08m:23s remains)
INFO - root - 2017-12-07 19:30:11.988875: step 42750, loss = 21.78, batch loss = 21.69 (8.3 examples/sec; 0.959 sec/batch; 77h:11m:42s remains)
INFO - root - 2017-12-07 19:30:21.463024: step 42760, loss = 21.35, batch loss = 21.27 (7.9 examples/sec; 1.010 sec/batch; 81h:15m:00s remains)
INFO - root - 2017-12-07 19:30:30.815214: step 42770, loss = 21.57, batch loss = 21.49 (7.9 examples/sec; 1.008 sec/batch; 81h:05m:12s remains)
INFO - root - 2017-12-07 19:30:40.214907: step 42780, loss = 21.15, batch loss = 21.06 (8.7 examples/sec; 0.922 sec/batch; 74h:09m:42s remains)
INFO - root - 2017-12-07 19:30:49.714850: step 42790, loss = 21.42, batch loss = 21.34 (8.5 examples/sec; 0.942 sec/batch; 75h:49m:50s remains)
INFO - root - 2017-12-07 19:30:59.129508: step 42800, loss = 21.34, batch loss = 21.26 (8.0 examples/sec; 0.998 sec/batch; 80h:16m:49s remains)
2017-12-07 19:31:00.118467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.10075 -4.1080012 -4.130125 -4.172658 -4.2076735 -4.230051 -4.2439022 -4.2353849 -4.2281146 -4.2501345 -4.274292 -4.2845497 -4.2675805 -4.22498 -4.180614][-4.1282482 -4.1360521 -4.1539183 -4.1959786 -4.2272015 -4.2415228 -4.2383304 -4.1986675 -4.159821 -4.164166 -4.1884537 -4.2163258 -4.2294059 -4.206152 -4.1798148][-4.1679373 -4.1861172 -4.208292 -4.2486858 -4.2594657 -4.2339296 -4.1935816 -4.124362 -4.0651312 -4.0689297 -4.0999379 -4.1473312 -4.1975737 -4.199245 -4.1913462][-4.1803064 -4.2085247 -4.240416 -4.2789059 -4.2645087 -4.1902943 -4.1102939 -4.0229363 -3.9617605 -3.9880376 -4.0343046 -4.0899448 -4.1628132 -4.1821094 -4.19309][-4.1702461 -4.1957955 -4.2227607 -4.2494283 -4.2171245 -4.119988 -4.0332794 -3.9583907 -3.910603 -3.9523432 -4.0035396 -4.0507092 -4.1265073 -4.1542678 -4.1780334][-4.1634383 -4.1731815 -4.1870637 -4.19614 -4.15255 -4.0550389 -3.9833066 -3.9373989 -3.9105077 -3.9564023 -4.0084643 -4.0556426 -4.1320767 -4.1615372 -4.1844072][-4.1986723 -4.1926465 -4.19126 -4.1831455 -4.135962 -4.0527377 -3.9987214 -3.976058 -3.965512 -4.0042348 -4.0528784 -4.1054339 -4.1809287 -4.2120776 -4.2289515][-4.269299 -4.2533092 -4.2340484 -4.2128835 -4.1725383 -4.1143088 -4.0769048 -4.0680079 -4.0695491 -4.0989079 -4.143661 -4.1966181 -4.2630005 -4.2888951 -4.2948313][-4.3166194 -4.3065667 -4.275188 -4.2430305 -4.2096529 -4.1757088 -4.1569018 -4.1638823 -4.18119 -4.2117968 -4.2568526 -4.30111 -4.3413787 -4.3488955 -4.3414536][-4.3268404 -4.3372993 -4.3083777 -4.2677274 -4.2366352 -4.2185607 -4.2152071 -4.2388568 -4.2686691 -4.29672 -4.3354106 -4.3608479 -4.3694458 -4.3613515 -4.3546796][-4.312129 -4.3518472 -4.3363876 -4.2956548 -4.2700152 -4.264657 -4.2722912 -4.3038378 -4.3304729 -4.3403926 -4.3616176 -4.3731771 -4.3698854 -4.363555 -4.365294][-4.292542 -4.3590627 -4.3604708 -4.3271813 -4.3099003 -4.3146234 -4.3274684 -4.3575139 -4.3747621 -4.3677082 -4.3734903 -4.3820806 -4.3826118 -4.3812313 -4.3829823][-4.2910128 -4.3742604 -4.391099 -4.3679681 -4.3574576 -4.3678203 -4.380302 -4.4011378 -4.4101419 -4.397634 -4.3941588 -4.4002762 -4.4014392 -4.3963966 -4.3900752][-4.3140669 -4.392951 -4.4177289 -4.4024787 -4.3928542 -4.401649 -4.4108677 -4.4200716 -4.4251933 -4.4178724 -4.412694 -4.4156427 -4.41254 -4.4009943 -4.3900228][-4.339654 -4.4005857 -4.4263654 -4.4166422 -4.4006829 -4.3959332 -4.3941293 -4.39281 -4.39887 -4.4042168 -4.4072552 -4.4142609 -4.413784 -4.4057531 -4.3986745]]...]
INFO - root - 2017-12-07 19:31:09.651555: step 42810, loss = 21.52, batch loss = 21.43 (8.6 examples/sec; 0.927 sec/batch; 74h:37m:04s remains)
INFO - root - 2017-12-07 19:31:19.079582: step 42820, loss = 21.44, batch loss = 21.35 (9.1 examples/sec; 0.884 sec/batch; 71h:07m:01s remains)
INFO - root - 2017-12-07 19:31:28.348783: step 42830, loss = 21.36, batch loss = 21.28 (8.7 examples/sec; 0.915 sec/batch; 73h:35m:32s remains)
INFO - root - 2017-12-07 19:31:37.689973: step 42840, loss = 21.69, batch loss = 21.61 (8.9 examples/sec; 0.897 sec/batch; 72h:09m:15s remains)
INFO - root - 2017-12-07 19:31:47.024990: step 42850, loss = 21.29, batch loss = 21.21 (8.6 examples/sec; 0.925 sec/batch; 74h:26m:58s remains)
INFO - root - 2017-12-07 19:31:56.446082: step 42860, loss = 21.51, batch loss = 21.43 (8.3 examples/sec; 0.964 sec/batch; 77h:32m:35s remains)
INFO - root - 2017-12-07 19:32:05.811900: step 42870, loss = 21.42, batch loss = 21.33 (8.3 examples/sec; 0.961 sec/batch; 77h:21m:04s remains)
INFO - root - 2017-12-07 19:32:15.214415: step 42880, loss = 21.75, batch loss = 21.66 (7.8 examples/sec; 1.029 sec/batch; 82h:48m:53s remains)
INFO - root - 2017-12-07 19:32:24.582897: step 42890, loss = 21.65, batch loss = 21.57 (8.8 examples/sec; 0.912 sec/batch; 73h:23m:48s remains)
INFO - root - 2017-12-07 19:32:34.099373: step 42900, loss = 21.30, batch loss = 21.21 (8.4 examples/sec; 0.953 sec/batch; 76h:40m:14s remains)
2017-12-07 19:32:35.127005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3158679 -4.3500295 -4.3798018 -4.3889718 -4.390038 -4.3909125 -4.3914313 -4.393208 -4.4030628 -4.4255123 -4.4558291 -4.4892764 -4.5221725 -4.5463862 -4.5519643][-4.4365168 -4.4692736 -4.4981751 -4.5133905 -4.5265679 -4.534483 -4.5314031 -4.5197296 -4.5081277 -4.5096183 -4.5278239 -4.5594368 -4.59856 -4.6281171 -4.6301303][-4.498579 -4.5230722 -4.5509915 -4.5761909 -4.604805 -4.6191421 -4.6097713 -4.5793743 -4.5370111 -4.5082078 -4.5079422 -4.53411 -4.5788231 -4.6153588 -4.6181226][-4.4333863 -4.4449592 -4.4757633 -4.5149632 -4.555553 -4.56666 -4.5440311 -4.497683 -4.4331279 -4.387856 -4.3805723 -4.4085555 -4.4638367 -4.5071659 -4.5104127][-4.25991 -4.2635555 -4.3026137 -4.3549747 -4.3971605 -4.3868194 -4.3371577 -4.2739115 -4.2031279 -4.1658115 -4.1713943 -4.212152 -4.2833667 -4.3360262 -4.3390145][-4.0396852 -4.0369415 -4.0769305 -4.1335979 -4.1695323 -4.1317463 -4.053853 -3.9782677 -3.9180384 -3.915597 -3.9586883 -4.0252028 -4.1133981 -4.1694336 -4.1583819][-3.8918252 -3.893682 -3.931531 -3.9872603 -4.0157013 -3.9603791 -3.8717642 -3.79693 -3.755764 -3.789257 -3.8637357 -3.9437094 -4.0312476 -4.0787005 -4.0500855][-3.9201729 -3.938237 -3.9783292 -4.0276732 -4.0425496 -3.980907 -3.9038215 -3.8451676 -3.8229165 -3.8676567 -3.9419188 -4.0134416 -4.0860257 -4.1219158 -4.0845842][-4.0501046 -4.0836983 -4.1277761 -4.1699829 -4.1715217 -4.1176028 -4.0678439 -4.0367503 -4.0306649 -4.0634265 -4.1125622 -4.1637139 -4.2207122 -4.2491689 -4.2121048][-4.1909461 -4.2306318 -4.2785149 -4.3143992 -4.308928 -4.2711148 -4.2466049 -4.2369018 -4.2419209 -4.2589159 -4.2793965 -4.3053346 -4.3408933 -4.3573122 -4.3265672][-4.2815609 -4.3121147 -4.3544297 -4.3795 -4.3717527 -4.3520522 -4.3453431 -4.3509827 -4.3655291 -4.3765855 -4.3798056 -4.3823829 -4.3910451 -4.3893642 -4.3635125][-4.346035 -4.3636074 -4.393681 -4.4090824 -4.4056444 -4.4040222 -4.4130492 -4.4311624 -4.4516072 -4.4601731 -4.4527478 -4.4392085 -4.4289145 -4.4163756 -4.3968959][-4.421032 -4.4295659 -4.45395 -4.469821 -4.4760079 -4.485043 -4.4990592 -4.5171781 -4.5334635 -4.5378976 -4.5271921 -4.510685 -4.4969254 -4.4866657 -4.4788303][-4.4660239 -4.467114 -4.4843912 -4.4994559 -4.5111485 -4.5227032 -4.5334406 -4.5435524 -4.5505676 -4.5502234 -4.542387 -4.5339952 -4.5292821 -4.5287719 -4.5316215][-4.4464874 -4.4414549 -4.4486361 -4.4569511 -4.4648438 -4.4717665 -4.4767146 -4.4801235 -4.481668 -4.4806051 -4.4782372 -4.4775176 -4.4792929 -4.4830723 -4.4882607]]...]
INFO - root - 2017-12-07 19:32:44.535036: step 42910, loss = 21.88, batch loss = 21.79 (8.5 examples/sec; 0.945 sec/batch; 76h:02m:52s remains)
INFO - root - 2017-12-07 19:32:53.860831: step 42920, loss = 21.64, batch loss = 21.56 (8.6 examples/sec; 0.927 sec/batch; 74h:35m:16s remains)
INFO - root - 2017-12-07 19:33:03.193273: step 42930, loss = 21.78, batch loss = 21.70 (9.3 examples/sec; 0.858 sec/batch; 68h:59m:45s remains)
INFO - root - 2017-12-07 19:33:12.572044: step 42940, loss = 21.43, batch loss = 21.34 (8.3 examples/sec; 0.961 sec/batch; 77h:19m:30s remains)
INFO - root - 2017-12-07 19:33:21.891564: step 42950, loss = 21.72, batch loss = 21.64 (8.3 examples/sec; 0.968 sec/batch; 77h:49m:34s remains)
INFO - root - 2017-12-07 19:33:31.139647: step 42960, loss = 21.89, batch loss = 21.81 (8.3 examples/sec; 0.964 sec/batch; 77h:31m:07s remains)
INFO - root - 2017-12-07 19:33:40.366084: step 42970, loss = 21.89, batch loss = 21.81 (8.4 examples/sec; 0.947 sec/batch; 76h:09m:44s remains)
INFO - root - 2017-12-07 19:33:49.867838: step 42980, loss = 21.64, batch loss = 21.56 (8.6 examples/sec; 0.934 sec/batch; 75h:05m:34s remains)
INFO - root - 2017-12-07 19:33:59.334809: step 42990, loss = 21.26, batch loss = 21.18 (8.5 examples/sec; 0.939 sec/batch; 75h:32m:32s remains)
INFO - root - 2017-12-07 19:34:08.787718: step 43000, loss = 21.55, batch loss = 21.47 (8.9 examples/sec; 0.898 sec/batch; 72h:11m:53s remains)
2017-12-07 19:34:09.723685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4812927 -4.5450039 -4.5836458 -4.5884109 -4.5703921 -4.5461636 -4.50003 -4.4550128 -4.4551821 -4.478569 -4.5176764 -4.5703611 -4.5921135 -4.5501966 -4.4676042][-4.5548353 -4.6054196 -4.6094303 -4.570096 -4.5250845 -4.4897246 -4.4498892 -4.4252009 -4.4502277 -4.50159 -4.5504827 -4.5917883 -4.6007586 -4.545136 -4.4550004][-4.5890079 -4.6221676 -4.5995688 -4.5263653 -4.4446645 -4.38297 -4.3261209 -4.2842636 -4.2913914 -4.3542457 -4.4382949 -4.5194745 -4.5702267 -4.5397096 -4.4564095][-4.6151371 -4.6190434 -4.563633 -4.4651861 -4.3564773 -4.2687912 -4.1834092 -4.100975 -4.062788 -4.1137509 -4.2264194 -4.3661337 -4.488348 -4.5172753 -4.4621024][-4.643887 -4.6251616 -4.5334606 -4.4101181 -4.2799158 -4.1692529 -4.0599446 -3.9490895 -3.8797421 -3.920419 -4.0478511 -4.22839 -4.4062347 -4.4907284 -4.4653826][-4.6738195 -4.6559496 -4.5493326 -4.4086776 -4.2539086 -4.1121335 -3.97459 -3.8458142 -3.7750237 -3.8268061 -3.9706404 -4.1707687 -4.3712754 -4.4818535 -4.4685879][-4.7069435 -4.7160158 -4.6383018 -4.5098176 -4.3363113 -4.1514921 -3.9639657 -3.79775 -3.726738 -3.800118 -3.966588 -4.1806359 -4.3835363 -4.4924383 -4.4731412][-4.727262 -4.7757363 -4.7562432 -4.6687732 -4.4918585 -4.2657447 -4.0202193 -3.8081245 -3.736002 -3.8260586 -4.0000534 -4.211834 -4.4021983 -4.4974184 -4.4701695][-4.7166376 -4.80447 -4.8355651 -4.7871342 -4.6198425 -4.3778267 -4.1024289 -3.8745749 -3.8176014 -3.9131672 -4.066998 -4.2498059 -4.41012 -4.4860315 -4.4569798][-4.6702118 -4.8002157 -4.8716269 -4.8511066 -4.704227 -4.4765377 -4.211586 -4.0034842 -3.9744973 -4.0684085 -4.1846933 -4.3192959 -4.431138 -4.4772463 -4.445488][-4.5840321 -4.7677965 -4.8822441 -4.8891306 -4.7750731 -4.5873446 -4.3586884 -4.179698 -4.1638756 -4.2438536 -4.3279786 -4.4218073 -4.4855962 -4.4954367 -4.4505186][-4.4747081 -4.7171097 -4.8805671 -4.9178004 -4.8391433 -4.6944828 -4.5058889 -4.3468323 -4.3155532 -4.3629551 -4.42645 -4.5039468 -4.5436139 -4.5281887 -4.4663734][-4.3585229 -4.6386213 -4.8503366 -4.92352 -4.8839068 -4.782258 -4.6377239 -4.5002031 -4.4384446 -4.4352326 -4.4711213 -4.5380082 -4.5714445 -4.5460858 -4.4751043][-4.2287197 -4.5100765 -4.7542443 -4.8694215 -4.8779011 -4.82852 -4.74429 -4.6487036 -4.5735183 -4.5230727 -4.5134974 -4.5483522 -4.5673885 -4.5371437 -4.4660292][-4.0847216 -4.3458362 -4.5986204 -4.7483983 -4.8013554 -4.8053861 -4.7888031 -4.7482424 -4.6854272 -4.6122355 -4.5614147 -4.5551815 -4.5508251 -4.5135245 -4.4443035]]...]
INFO - root - 2017-12-07 19:34:19.089764: step 43010, loss = 21.59, batch loss = 21.51 (8.7 examples/sec; 0.924 sec/batch; 74h:20m:24s remains)
INFO - root - 2017-12-07 19:34:28.566740: step 43020, loss = 21.51, batch loss = 21.43 (8.6 examples/sec; 0.926 sec/batch; 74h:25m:23s remains)
INFO - root - 2017-12-07 19:34:37.894590: step 43030, loss = 21.47, batch loss = 21.38 (8.8 examples/sec; 0.904 sec/batch; 72h:43m:27s remains)
INFO - root - 2017-12-07 19:34:47.266676: step 43040, loss = 21.58, batch loss = 21.50 (8.2 examples/sec; 0.972 sec/batch; 78h:09m:19s remains)
INFO - root - 2017-12-07 19:34:56.693066: step 43050, loss = 21.55, batch loss = 21.47 (8.1 examples/sec; 0.986 sec/batch; 79h:17m:39s remains)
INFO - root - 2017-12-07 19:35:06.155439: step 43060, loss = 21.46, batch loss = 21.38 (8.1 examples/sec; 0.991 sec/batch; 79h:39m:04s remains)
INFO - root - 2017-12-07 19:35:15.699580: step 43070, loss = 21.16, batch loss = 21.08 (8.6 examples/sec; 0.936 sec/batch; 75h:13m:27s remains)
INFO - root - 2017-12-07 19:35:25.131643: step 43080, loss = 21.61, batch loss = 21.52 (8.4 examples/sec; 0.957 sec/batch; 76h:56m:45s remains)
INFO - root - 2017-12-07 19:35:34.696103: step 43090, loss = 21.50, batch loss = 21.42 (8.3 examples/sec; 0.960 sec/batch; 77h:12m:46s remains)
INFO - root - 2017-12-07 19:35:44.172267: step 43100, loss = 21.54, batch loss = 21.46 (8.0 examples/sec; 1.002 sec/batch; 80h:33m:09s remains)
2017-12-07 19:35:45.204901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2904806 -4.2586012 -4.2186618 -4.1919522 -4.2094951 -4.2816072 -4.380939 -4.4631968 -4.496026 -4.4792571 -4.4289546 -4.4037781 -4.4255438 -4.4431472 -4.4415956][-4.3390975 -4.278357 -4.2295585 -4.21322 -4.228343 -4.2702823 -4.3257036 -4.3865743 -4.4388103 -4.4679294 -4.4628849 -4.4647679 -4.4980483 -4.5179076 -4.5176644][-4.3454423 -4.248662 -4.1860046 -4.1826158 -4.19951 -4.2063375 -4.209547 -4.2413783 -4.3078027 -4.3869333 -4.4497395 -4.49927 -4.5515161 -4.5756879 -4.5768542][-4.2832422 -4.161356 -4.0938239 -4.10306 -4.1211252 -4.0961313 -4.0585957 -4.0703812 -4.1460476 -4.2626019 -4.3860979 -4.4873667 -4.5683637 -4.6062832 -4.609333][-4.1858249 -4.0616832 -4.0085011 -4.0341315 -4.0451708 -3.983376 -3.9119039 -3.910331 -3.989331 -4.127234 -4.2917519 -4.43833 -4.5569053 -4.6153 -4.6190376][-4.1113429 -4.0187454 -3.994488 -4.0319381 -4.0257087 -3.9280727 -3.8242023 -3.79802 -3.8583443 -3.9944348 -4.1778016 -4.3637867 -4.5271416 -4.6129909 -4.6173129][-4.108119 -4.0711331 -4.0790133 -4.114749 -4.078824 -3.9511096 -3.8242328 -3.7681203 -3.7984295 -3.9130933 -4.0924568 -4.3013172 -4.5022283 -4.61297 -4.6224036][-4.1921439 -4.2035708 -4.2331758 -4.2509761 -4.1802592 -4.0308685 -3.8952789 -3.8223336 -3.8295271 -3.9141014 -4.0688858 -4.2759457 -4.4913144 -4.6190162 -4.6406889][-4.3061228 -4.35092 -4.389729 -4.3829436 -4.2851982 -4.1309543 -4.0014467 -3.9232936 -3.9086823 -3.9549947 -4.0690145 -4.2573261 -4.4729142 -4.6189079 -4.665122][-4.4078522 -4.4740477 -4.5147572 -4.486218 -4.3722725 -4.2251549 -4.109242 -4.0298662 -3.988158 -3.9878864 -4.0547657 -4.214643 -4.4215293 -4.58553 -4.6644726][-4.4941559 -4.5685887 -4.6077633 -4.5656343 -4.4442716 -4.3037715 -4.1962295 -4.1102872 -4.0379114 -3.9914994 -4.0104165 -4.1363478 -4.3269615 -4.5044684 -4.614974][-4.5631328 -4.6378675 -4.6702638 -4.6207252 -4.4960532 -4.3584371 -4.2547417 -4.1665134 -4.0750775 -3.9978182 -3.9792442 -4.0673494 -4.2300658 -4.4051709 -4.53449][-4.61237 -4.6840534 -4.7064705 -4.6528416 -4.5301175 -4.4020224 -4.3142128 -4.2369404 -4.14746 -4.0668988 -4.0305476 -4.0815725 -4.2026176 -4.3489933 -4.4704032][-4.6351194 -4.6918736 -4.7015867 -4.6518984 -4.5421576 -4.4331784 -4.3686676 -4.3101845 -4.2381706 -4.1771679 -4.1450458 -4.1723261 -4.2534137 -4.3580208 -4.4492707][-4.606832 -4.6414709 -4.638907 -4.6003609 -4.5166259 -4.4363308 -4.3925605 -4.3493476 -4.2981553 -4.2668581 -4.250845 -4.2689543 -4.33062 -4.4077306 -4.4679008]]...]
INFO - root - 2017-12-07 19:35:54.619075: step 43110, loss = 21.37, batch loss = 21.28 (8.8 examples/sec; 0.907 sec/batch; 72h:55m:09s remains)
INFO - root - 2017-12-07 19:36:04.223461: step 43120, loss = 21.44, batch loss = 21.36 (8.3 examples/sec; 0.968 sec/batch; 77h:49m:17s remains)
INFO - root - 2017-12-07 19:36:13.771714: step 43130, loss = 21.84, batch loss = 21.76 (8.5 examples/sec; 0.941 sec/batch; 75h:39m:04s remains)
INFO - root - 2017-12-07 19:36:22.932265: step 43140, loss = 21.54, batch loss = 21.45 (9.5 examples/sec; 0.844 sec/batch; 67h:48m:06s remains)
INFO - root - 2017-12-07 19:36:32.319312: step 43150, loss = 21.40, batch loss = 21.32 (9.0 examples/sec; 0.888 sec/batch; 71h:22m:25s remains)
INFO - root - 2017-12-07 19:36:41.646886: step 43160, loss = 21.53, batch loss = 21.45 (8.8 examples/sec; 0.913 sec/batch; 73h:20m:47s remains)
INFO - root - 2017-12-07 19:36:51.017656: step 43170, loss = 21.71, batch loss = 21.62 (8.7 examples/sec; 0.923 sec/batch; 74h:11m:43s remains)
INFO - root - 2017-12-07 19:37:00.431608: step 43180, loss = 21.44, batch loss = 21.36 (8.1 examples/sec; 0.985 sec/batch; 79h:11m:15s remains)
INFO - root - 2017-12-07 19:37:09.876693: step 43190, loss = 21.46, batch loss = 21.38 (8.1 examples/sec; 0.984 sec/batch; 79h:04m:51s remains)
INFO - root - 2017-12-07 19:37:19.325397: step 43200, loss = 21.64, batch loss = 21.56 (8.4 examples/sec; 0.949 sec/batch; 76h:15m:56s remains)
2017-12-07 19:37:20.247090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3313966 -4.368052 -4.3561563 -4.3121414 -4.292026 -4.3095961 -4.3406234 -4.3609738 -4.3643909 -4.3577271 -4.3718557 -4.4011827 -4.4293056 -4.4371285 -4.4007487][-4.3387837 -4.3749704 -4.3669667 -4.3213949 -4.2797055 -4.2600513 -4.2645741 -4.2860956 -4.31623 -4.3489604 -4.3905563 -4.41401 -4.4040813 -4.3767424 -4.3425684][-4.377008 -4.3903408 -4.354805 -4.2763915 -4.1916676 -4.1354718 -4.1407418 -4.2006316 -4.2786427 -4.3585906 -4.4301648 -4.45466 -4.4153028 -4.3454251 -4.2880645][-4.417942 -4.4049907 -4.3369312 -4.2194281 -4.0888653 -4.0005417 -4.0143838 -4.1129527 -4.2296262 -4.3413687 -4.4348826 -4.4762869 -4.4400125 -4.3549409 -4.2754145][-4.4505372 -4.4274211 -4.3452578 -4.2089996 -4.0494914 -3.9342124 -3.9428966 -4.0507331 -4.1774607 -4.2906795 -4.3851471 -4.4430022 -4.4389715 -4.3807764 -4.3069973][-4.46202 -4.4526424 -4.3845444 -4.2543283 -4.0858541 -3.9529328 -3.9391737 -4.0237131 -4.1315894 -4.2247109 -4.3020678 -4.36778 -4.4065895 -4.3998828 -4.3519297][-4.4206591 -4.4443135 -4.40773 -4.3002162 -4.1421595 -4.0053229 -3.9600766 -3.9951305 -4.0638103 -4.1324911 -4.1945338 -4.2674503 -4.3478889 -4.3954186 -4.3804169][-4.336338 -4.385931 -4.3836427 -4.3124971 -4.1842794 -4.0560083 -3.9794226 -3.9602404 -3.984962 -4.0308232 -4.084219 -4.1651111 -4.2751803 -4.3643336 -4.3806376][-4.290834 -4.3404121 -4.3578854 -4.3191204 -4.2216749 -4.1002045 -3.9958181 -3.9338744 -3.9261668 -3.9581358 -4.0090556 -4.0966325 -4.2242913 -4.3389096 -4.3801723][-4.31482 -4.3464355 -4.3678541 -4.3509436 -4.2770133 -4.1621284 -4.0409818 -3.9537265 -3.9271598 -3.9525886 -4.0027275 -4.0893564 -4.217979 -4.3396873 -4.3974676][-4.3632665 -4.3722491 -4.3943276 -4.4001031 -4.3535151 -4.2575755 -4.1385803 -4.0405049 -3.9996138 -4.0160818 -4.0597248 -4.1343036 -4.2464933 -4.3599377 -4.4281754][-4.3704786 -4.369894 -4.4019742 -4.4332705 -4.4172792 -4.3519654 -4.25642 -4.1658087 -4.1181736 -4.1251607 -4.1584873 -4.2118654 -4.2945518 -4.3855395 -4.4506721][-4.3375363 -4.3429995 -4.3892035 -4.43672 -4.4368234 -4.3918066 -4.3236303 -4.2572179 -4.2213893 -4.2325068 -4.2594604 -4.2881384 -4.3384662 -4.4033308 -4.4585314][-4.31152 -4.3223457 -4.370892 -4.413785 -4.4055295 -4.3569818 -4.301383 -4.2634954 -4.2563629 -4.2870865 -4.316143 -4.3282471 -4.3566613 -4.408145 -4.4628682][-4.3041177 -4.3077769 -4.3506131 -4.3820777 -4.353868 -4.2846646 -4.225718 -4.2141342 -4.2451534 -4.3026719 -4.3363624 -4.3336806 -4.341598 -4.3838091 -4.4451914]]...]
INFO - root - 2017-12-07 19:37:29.706005: step 43210, loss = 21.50, batch loss = 21.42 (8.3 examples/sec; 0.963 sec/batch; 77h:22m:33s remains)
INFO - root - 2017-12-07 19:37:39.209141: step 43220, loss = 21.49, batch loss = 21.41 (8.7 examples/sec; 0.918 sec/batch; 73h:46m:22s remains)
INFO - root - 2017-12-07 19:37:48.668288: step 43230, loss = 21.20, batch loss = 21.12 (8.9 examples/sec; 0.896 sec/batch; 72h:00m:59s remains)
INFO - root - 2017-12-07 19:37:57.959405: step 43240, loss = 21.22, batch loss = 21.13 (8.5 examples/sec; 0.938 sec/batch; 75h:23m:10s remains)
INFO - root - 2017-12-07 19:38:07.365043: step 43250, loss = 21.77, batch loss = 21.68 (8.5 examples/sec; 0.943 sec/batch; 75h:45m:00s remains)
INFO - root - 2017-12-07 19:38:16.817506: step 43260, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.968 sec/batch; 77h:46m:46s remains)
INFO - root - 2017-12-07 19:38:26.252179: step 43270, loss = 21.19, batch loss = 21.11 (8.5 examples/sec; 0.939 sec/batch; 75h:27m:47s remains)
INFO - root - 2017-12-07 19:38:35.569037: step 43280, loss = 21.58, batch loss = 21.50 (9.1 examples/sec; 0.883 sec/batch; 70h:55m:22s remains)
INFO - root - 2017-12-07 19:38:44.981847: step 43290, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.958 sec/batch; 76h:57m:50s remains)
INFO - root - 2017-12-07 19:38:54.497374: step 43300, loss = 21.44, batch loss = 21.36 (8.4 examples/sec; 0.947 sec/batch; 76h:03m:40s remains)
2017-12-07 19:38:55.461437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2736454 -4.3072448 -4.3495235 -4.3753438 -4.3680449 -4.34171 -4.3345146 -4.3548121 -4.3821192 -4.3936877 -4.3741412 -4.3353009 -4.3053775 -4.3056655 -4.325171][-4.3018031 -4.3320003 -4.3696828 -4.3984861 -4.4012814 -4.3834705 -4.3769927 -4.3909159 -4.4110703 -4.4181304 -4.398315 -4.3624935 -4.3373342 -4.3439789 -4.3722558][-4.4216032 -4.43795 -4.453733 -4.4700508 -4.4749494 -4.4647851 -4.4568734 -4.4621673 -4.4764504 -4.48592 -4.4756522 -4.4499788 -4.429965 -4.4330435 -4.4532967][-4.5262828 -4.5231943 -4.5127554 -4.5114827 -4.5128207 -4.50332 -4.4856887 -4.478559 -4.4911542 -4.510766 -4.5130057 -4.4940882 -4.4725275 -4.4673538 -4.4765186][-4.5293345 -4.5159526 -4.4924784 -4.4826427 -4.4801641 -4.4593873 -4.4161625 -4.3866396 -4.3992844 -4.4408298 -4.4656305 -4.4578471 -4.4392 -4.4324408 -4.4313521][-4.3997364 -4.3966055 -4.3870449 -4.3847427 -4.3780589 -4.3303881 -4.2440715 -4.180759 -4.1931005 -4.2679582 -4.3319397 -4.35249 -4.3572164 -4.3642969 -4.3503232][-4.1455884 -4.167201 -4.1866641 -4.2014132 -4.1942167 -4.1169596 -3.981266 -3.8743367 -3.8750379 -3.9793146 -4.0951419 -4.1662993 -4.2160945 -4.2482829 -4.2178507][-3.9812136 -4.0121579 -4.0459914 -4.0727611 -4.0686655 -3.9744465 -3.8053141 -3.6624465 -3.6436265 -3.758136 -3.9121947 -4.0285063 -4.1169243 -4.1677361 -4.1266327][-4.0367131 -4.07411 -4.11148 -4.1481643 -4.1479836 -4.0487828 -3.8697357 -3.7128146 -3.6770539 -3.7766261 -3.9333498 -4.0654278 -4.1657276 -4.2166748 -4.1736469][-4.2032061 -4.2532125 -4.2922506 -4.338614 -4.3467555 -4.2584496 -4.0926695 -3.942621 -3.8963377 -3.9633222 -4.0893254 -4.2070303 -4.2946844 -4.3302493 -4.2861829][-4.3500423 -4.40054 -4.431973 -4.4811044 -4.5055895 -4.4499655 -4.3237658 -4.2009621 -4.1543312 -4.1873407 -4.2667961 -4.3531275 -4.4204583 -4.443295 -4.4052768][-4.4554996 -4.48654 -4.4959044 -4.5328393 -4.5675373 -4.5491862 -4.47382 -4.3918262 -4.356523 -4.3668251 -4.4049187 -4.4579368 -4.5078154 -4.5274568 -4.50032][-4.5120234 -4.504838 -4.4810328 -4.4918389 -4.5170908 -4.5172577 -4.4800234 -4.4349437 -4.4178386 -4.4256945 -4.4470129 -4.4825473 -4.5233502 -4.5451369 -4.5306306][-4.4554009 -4.3990221 -4.3483005 -4.3404465 -4.3544869 -4.3606362 -4.3476868 -4.3297415 -4.3289046 -4.344089 -4.3675418 -4.4021611 -4.4431467 -4.4734163 -4.4762769][-4.3217592 -4.2222104 -4.1546192 -4.1398621 -4.1550951 -4.1756926 -4.1888242 -4.1938787 -4.2017136 -4.2181268 -4.2436814 -4.2793474 -4.3205481 -4.3562183 -4.37218]]...]
INFO - root - 2017-12-07 19:39:04.923905: step 43310, loss = 21.19, batch loss = 21.10 (8.6 examples/sec; 0.928 sec/batch; 74h:34m:45s remains)
INFO - root - 2017-12-07 19:39:14.328434: step 43320, loss = 21.53, batch loss = 21.45 (8.3 examples/sec; 0.964 sec/batch; 77h:24m:40s remains)
INFO - root - 2017-12-07 19:39:23.903168: step 43330, loss = 21.47, batch loss = 21.39 (8.0 examples/sec; 1.005 sec/batch; 80h:42m:09s remains)
INFO - root - 2017-12-07 19:39:33.293247: step 43340, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.938 sec/batch; 75h:21m:06s remains)
INFO - root - 2017-12-07 19:39:42.666229: step 43350, loss = 21.19, batch loss = 21.11 (9.0 examples/sec; 0.893 sec/batch; 71h:44m:00s remains)
INFO - root - 2017-12-07 19:39:52.092187: step 43360, loss = 21.50, batch loss = 21.42 (9.1 examples/sec; 0.877 sec/batch; 70h:25m:14s remains)
INFO - root - 2017-12-07 19:40:01.522978: step 43370, loss = 21.95, batch loss = 21.87 (8.5 examples/sec; 0.939 sec/batch; 75h:27m:16s remains)
INFO - root - 2017-12-07 19:40:10.962958: step 43380, loss = 21.56, batch loss = 21.48 (8.4 examples/sec; 0.951 sec/batch; 76h:23m:28s remains)
INFO - root - 2017-12-07 19:40:20.420434: step 43390, loss = 21.76, batch loss = 21.68 (8.7 examples/sec; 0.925 sec/batch; 74h:15m:05s remains)
INFO - root - 2017-12-07 19:40:29.908625: step 43400, loss = 21.55, batch loss = 21.46 (8.1 examples/sec; 0.988 sec/batch; 79h:21m:34s remains)
2017-12-07 19:40:30.834610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4091673 -4.4215221 -4.4443736 -4.4549685 -4.4415731 -4.4199214 -4.3927035 -4.3473277 -4.3125243 -4.3345027 -4.4038839 -4.4622135 -4.4796233 -4.4674172 -4.4329228][-4.4048448 -4.4140716 -4.4265928 -4.4187307 -4.3844047 -4.36125 -4.3601379 -4.356946 -4.3498054 -4.3725204 -4.4338722 -4.4880748 -4.4922056 -4.4593344 -4.4117718][-4.4160891 -4.4176068 -4.4096742 -4.377224 -4.3245296 -4.3038235 -4.3304658 -4.3682675 -4.3930945 -4.4258595 -4.4826508 -4.5328765 -4.530035 -4.4829545 -4.42221][-4.4474096 -4.4344163 -4.3994184 -4.3381629 -4.2630882 -4.2341619 -4.266489 -4.3208961 -4.3698621 -4.4279513 -4.5002489 -4.555439 -4.5551152 -4.5084004 -4.4441457][-4.4905872 -4.4594231 -4.395215 -4.3029461 -4.2006631 -4.1510696 -4.1712422 -4.2279463 -4.2976933 -4.3905187 -4.490644 -4.55115 -4.5515409 -4.5091543 -4.44773][-4.5126743 -4.4618411 -4.3658142 -4.2437949 -4.1157331 -4.04173 -4.0471969 -4.1053586 -4.1936507 -4.3192039 -4.4488382 -4.520978 -4.5263529 -4.4842744 -4.4191465][-4.4919033 -4.4253645 -4.3080339 -4.1670756 -4.0202007 -3.9213142 -3.9099588 -3.966125 -4.0685749 -4.2185736 -4.373004 -4.4637842 -4.4795694 -4.433918 -4.3600163][-4.4593849 -4.3887262 -4.2666206 -4.1166377 -3.9512303 -3.8233676 -3.787709 -3.8376617 -3.9532948 -4.1229911 -4.2953086 -4.4006958 -4.4224391 -4.3748446 -4.297081][-4.4347439 -4.375586 -4.2699308 -4.1317096 -3.96422 -3.8190842 -3.76044 -3.7976873 -3.916548 -4.0920906 -4.2680788 -4.3775783 -4.401813 -4.3591814 -4.2856832][-4.4100337 -4.3744531 -4.308116 -4.2111845 -4.0725803 -3.9354506 -3.8616838 -3.8776836 -3.9815612 -4.1407104 -4.301816 -4.4044909 -4.434453 -4.405776 -4.340847][-4.4032812 -4.3925066 -4.3681703 -4.3202658 -4.2262554 -4.1164222 -4.0377297 -4.0276847 -4.1016192 -4.2275739 -4.3624945 -4.4538927 -4.4877634 -4.4662247 -4.3990593][-4.4422693 -4.4523792 -4.4557805 -4.4410114 -4.3838367 -4.301414 -4.2220364 -4.1886425 -4.2329569 -4.327836 -4.4371934 -4.5128527 -4.5367169 -4.5013876 -4.416862][-4.504818 -4.5278296 -4.5474882 -4.5481949 -4.5115514 -4.449194 -4.3735743 -4.3258119 -4.3469934 -4.4197855 -4.5119395 -4.5735378 -4.5773191 -4.5174842 -4.4105473][-4.5410151 -4.5699711 -4.5989785 -4.6103239 -4.5916538 -4.5513172 -4.4915681 -4.4449673 -4.4519248 -4.5063624 -4.5795159 -4.6202 -4.595983 -4.5121822 -4.3938255][-4.52233 -4.5519385 -4.5860972 -4.6105766 -4.6162877 -4.6046319 -4.570776 -4.5361195 -4.5348516 -4.5698581 -4.6180053 -4.6359172 -4.5929217 -4.5055394 -4.4004784]]...]
INFO - root - 2017-12-07 19:40:40.383810: step 43410, loss = 21.14, batch loss = 21.06 (8.2 examples/sec; 0.974 sec/batch; 78h:13m:19s remains)
INFO - root - 2017-12-07 19:40:49.859085: step 43420, loss = 21.49, batch loss = 21.40 (7.8 examples/sec; 1.027 sec/batch; 82h:27m:01s remains)
INFO - root - 2017-12-07 19:40:59.339809: step 43430, loss = 21.21, batch loss = 21.13 (8.3 examples/sec; 0.961 sec/batch; 77h:10m:44s remains)
INFO - root - 2017-12-07 19:41:08.803163: step 43440, loss = 21.75, batch loss = 21.67 (8.6 examples/sec; 0.932 sec/batch; 74h:48m:22s remains)
INFO - root - 2017-12-07 19:41:17.807321: step 43450, loss = 21.90, batch loss = 21.81 (8.5 examples/sec; 0.940 sec/batch; 75h:26m:29s remains)
INFO - root - 2017-12-07 19:41:27.258504: step 43460, loss = 21.36, batch loss = 21.27 (8.2 examples/sec; 0.973 sec/batch; 78h:05m:30s remains)
INFO - root - 2017-12-07 19:41:36.715806: step 43470, loss = 21.68, batch loss = 21.59 (9.0 examples/sec; 0.893 sec/batch; 71h:43m:52s remains)
INFO - root - 2017-12-07 19:41:46.213537: step 43480, loss = 21.46, batch loss = 21.37 (8.5 examples/sec; 0.938 sec/batch; 75h:18m:54s remains)
INFO - root - 2017-12-07 19:41:55.483312: step 43490, loss = 21.44, batch loss = 21.36 (8.5 examples/sec; 0.942 sec/batch; 75h:35m:36s remains)
INFO - root - 2017-12-07 19:42:04.797308: step 43500, loss = 21.36, batch loss = 21.27 (8.8 examples/sec; 0.912 sec/batch; 73h:14m:44s remains)
2017-12-07 19:42:05.725811: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3759212 -4.4000959 -4.4206729 -4.4415941 -4.4594274 -4.4757051 -4.4911518 -4.4839258 -4.4474368 -4.4112115 -4.4063449 -4.419179 -4.3892632 -4.2930374 -4.1786442][-4.414907 -4.4544339 -4.4859371 -4.5089273 -4.5153475 -4.511755 -4.5047469 -4.4734688 -4.413938 -4.3528414 -4.3248649 -4.3241773 -4.2878962 -4.1907487 -4.078723][-4.4741178 -4.5206456 -4.5538349 -4.5673113 -4.5490413 -4.5115881 -4.4749889 -4.4228964 -4.353837 -4.2878666 -4.25208 -4.2439823 -4.2100482 -4.13061 -4.0455551][-4.5087476 -4.5477 -4.5663123 -4.5568871 -4.5089464 -4.4406648 -4.3881359 -4.3376575 -4.281374 -4.2314453 -4.2004418 -4.1852131 -4.158464 -4.1166868 -4.0880771][-4.5078659 -4.5282464 -4.5189791 -4.4778204 -4.4043665 -4.3212624 -4.2708874 -4.2424517 -4.2185593 -4.1984067 -4.1791568 -4.1556816 -4.1327267 -4.131146 -4.1699095][-4.4995909 -4.5013709 -4.4635792 -4.3907313 -4.2961411 -4.2011232 -4.144783 -4.1353769 -4.159296 -4.1835332 -4.1853156 -4.1613479 -4.1358643 -4.1518378 -4.2299614][-4.5038819 -4.4881306 -4.427989 -4.3327541 -4.2193022 -4.1022706 -4.0090446 -3.9887254 -4.0573483 -4.1393428 -4.1810102 -4.177496 -4.1533275 -4.16493 -4.2445679][-4.5073633 -4.4757376 -4.40061 -4.2972455 -4.171453 -4.0228829 -3.8725717 -3.8185158 -3.9257402 -4.0812063 -4.1810346 -4.2056141 -4.1799235 -4.1744246 -4.2333145][-4.5048919 -4.4552984 -4.3672714 -4.27062 -4.1525745 -3.9954143 -3.8228979 -3.7558122 -3.8873594 -4.0894775 -4.2246947 -4.260376 -4.2233396 -4.1988616 -4.237834][-4.4929991 -4.4258723 -4.3300128 -4.2542596 -4.1703706 -4.0480604 -3.910599 -3.8635876 -3.9771104 -4.1562634 -4.2743597 -4.2922988 -4.2382226 -4.2039819 -4.2370334][-4.4671574 -4.3935475 -4.2986903 -4.2470655 -4.2071424 -4.1349854 -4.0495152 -4.0261121 -4.0940089 -4.2120342 -4.2910733 -4.2881269 -4.2330022 -4.2069769 -4.2442575][-4.4188843 -4.3463554 -4.2657681 -4.2358017 -4.230083 -4.2010193 -4.16193 -4.1619706 -4.1989751 -4.263278 -4.3011761 -4.2755423 -4.2267132 -4.2160478 -4.2571049][-4.3571959 -4.2965035 -4.2413397 -4.2286654 -4.2441039 -4.2432146 -4.2350821 -4.2544351 -4.2840438 -4.3146629 -4.3138847 -4.2646737 -4.2268395 -4.2373385 -4.280108][-4.2880025 -4.2488422 -4.2305169 -4.2397237 -4.2646794 -4.2631359 -4.2519045 -4.2759523 -4.3174605 -4.3469038 -4.3299751 -4.2723646 -4.24871 -4.2709932 -4.29946][-4.2496371 -4.2545524 -4.2817874 -4.3068123 -4.3155923 -4.28564 -4.2485552 -4.26408 -4.318387 -4.360374 -4.3415256 -4.2800584 -4.2583694 -4.2788 -4.2924557]]...]
INFO - root - 2017-12-07 19:42:15.228684: step 43510, loss = 21.63, batch loss = 21.54 (8.3 examples/sec; 0.964 sec/batch; 77h:21m:39s remains)
INFO - root - 2017-12-07 19:42:24.640310: step 43520, loss = 21.40, batch loss = 21.31 (8.1 examples/sec; 0.985 sec/batch; 79h:01m:57s remains)
INFO - root - 2017-12-07 19:42:33.841519: step 43530, loss = 21.60, batch loss = 21.51 (8.8 examples/sec; 0.905 sec/batch; 72h:39m:19s remains)
INFO - root - 2017-12-07 19:42:43.328026: step 43540, loss = 21.58, batch loss = 21.50 (8.3 examples/sec; 0.965 sec/batch; 77h:26m:40s remains)
INFO - root - 2017-12-07 19:42:52.668010: step 43550, loss = 21.41, batch loss = 21.32 (8.1 examples/sec; 0.987 sec/batch; 79h:12m:02s remains)
INFO - root - 2017-12-07 19:43:02.192908: step 43560, loss = 21.57, batch loss = 21.48 (8.0 examples/sec; 0.998 sec/batch; 80h:04m:39s remains)
INFO - root - 2017-12-07 19:43:11.599418: step 43570, loss = 21.27, batch loss = 21.19 (9.0 examples/sec; 0.885 sec/batch; 71h:03m:04s remains)
INFO - root - 2017-12-07 19:43:21.049214: step 43580, loss = 20.93, batch loss = 20.84 (8.8 examples/sec; 0.908 sec/batch; 72h:50m:41s remains)
INFO - root - 2017-12-07 19:43:30.482150: step 43590, loss = 21.93, batch loss = 21.85 (8.5 examples/sec; 0.940 sec/batch; 75h:24m:13s remains)
INFO - root - 2017-12-07 19:43:39.958205: step 43600, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.966 sec/batch; 77h:32m:04s remains)
2017-12-07 19:43:40.823192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5956812 -4.5719423 -4.5885038 -4.6172237 -4.6253843 -4.6074619 -4.5618553 -4.5217695 -4.5108595 -4.5189838 -4.5357203 -4.5596 -4.5697565 -4.5538974 -4.4991827][-4.6059933 -4.5857272 -4.5994377 -4.6256213 -4.6383333 -4.6382627 -4.6245651 -4.6152534 -4.6221461 -4.636014 -4.6514487 -4.66082 -4.6567378 -4.6364355 -4.5954356][-4.5217681 -4.5179591 -4.5415277 -4.57563 -4.5994992 -4.6154981 -4.6220641 -4.6285214 -4.6436343 -4.66547 -4.6910458 -4.7055116 -4.709259 -4.7023363 -4.6823087][-4.4025106 -4.41666 -4.4509225 -4.4882307 -4.5180206 -4.5382061 -4.54284 -4.5412846 -4.55022 -4.5760674 -4.6112227 -4.6348782 -4.6487274 -4.65719 -4.6556435][-4.265584 -4.2811642 -4.3089104 -4.334496 -4.3586974 -4.373528 -4.3631258 -4.3445573 -4.3510413 -4.3870068 -4.4356766 -4.4743457 -4.4994259 -4.5218339 -4.534996][-4.1201067 -4.11529 -4.1213508 -4.1258817 -4.1433635 -4.1496034 -4.1178722 -4.0798631 -4.0857711 -4.1340704 -4.1981173 -4.2575507 -4.3053217 -4.3524103 -4.3883524][-4.0132537 -3.9761047 -3.951062 -3.9360785 -3.9529738 -3.9540713 -3.9034562 -3.8498683 -3.8542001 -3.9060378 -3.97519 -4.053061 -4.1325336 -4.2191796 -4.2935443][-4.0398536 -3.9688511 -3.9030354 -3.8623881 -3.8737364 -3.8731103 -3.8191752 -3.765295 -3.7688019 -3.8141205 -3.8775935 -3.9652305 -4.0695381 -4.1896877 -4.2987862][-4.1933012 -4.1008806 -3.9981945 -3.9280627 -3.9231603 -3.921627 -3.8827686 -3.8449247 -3.8496711 -3.883086 -3.9358494 -4.0205736 -4.1305547 -4.2571115 -4.3693342][-4.3625655 -4.2788248 -4.1738582 -4.0961905 -4.0774231 -4.0724888 -4.0496387 -4.0311408 -4.040545 -4.0678954 -4.1148367 -4.1916046 -4.290453 -4.3957767 -4.4760156][-4.4840484 -4.4295173 -4.3600626 -4.30745 -4.29279 -4.28693 -4.2709465 -4.2596211 -4.2675796 -4.2906547 -4.331912 -4.3941178 -4.4678106 -4.535275 -4.5675631][-4.535562 -4.5126672 -4.4850345 -4.46855 -4.4698439 -4.4677339 -4.4524803 -4.4368877 -4.4331121 -4.4436979 -4.4705267 -4.5106463 -4.5539842 -4.5839591 -4.5747957][-4.5269685 -4.5237021 -4.5232735 -4.5311074 -4.5449424 -4.5495057 -4.5385733 -4.5209484 -4.5056686 -4.5000176 -4.508028 -4.5259967 -4.5436339 -4.5470939 -4.5134759][-4.4904542 -4.4992361 -4.5087938 -4.5214891 -4.5366559 -4.5463552 -4.54557 -4.5339684 -4.5136309 -4.4932184 -4.4809771 -4.4769115 -4.4734654 -4.4593663 -4.4174967][-4.4479465 -4.4603438 -4.4621592 -4.4610915 -4.4675732 -4.4822164 -4.4998379 -4.5075169 -4.4945173 -4.4661555 -4.4343305 -4.4073839 -4.3835816 -4.3569674 -4.3207746]]...]
INFO - root - 2017-12-07 19:43:50.152801: step 43610, loss = 21.42, batch loss = 21.34 (9.0 examples/sec; 0.890 sec/batch; 71h:22m:59s remains)
INFO - root - 2017-12-07 19:43:59.762060: step 43620, loss = 21.22, batch loss = 21.14 (8.4 examples/sec; 0.957 sec/batch; 76h:47m:02s remains)
INFO - root - 2017-12-07 19:44:09.280560: step 43630, loss = 21.42, batch loss = 21.34 (8.2 examples/sec; 0.971 sec/batch; 77h:52m:36s remains)
INFO - root - 2017-12-07 19:44:18.572481: step 43640, loss = 21.51, batch loss = 21.43 (8.1 examples/sec; 0.986 sec/batch; 79h:06m:35s remains)
INFO - root - 2017-12-07 19:44:28.015049: step 43650, loss = 21.46, batch loss = 21.37 (8.2 examples/sec; 0.974 sec/batch; 78h:08m:13s remains)
INFO - root - 2017-12-07 19:44:37.164958: step 43660, loss = 21.66, batch loss = 21.57 (8.4 examples/sec; 0.947 sec/batch; 75h:58m:42s remains)
INFO - root - 2017-12-07 19:44:46.556765: step 43670, loss = 21.15, batch loss = 21.07 (8.3 examples/sec; 0.968 sec/batch; 77h:38m:16s remains)
INFO - root - 2017-12-07 19:44:56.110629: step 43680, loss = 21.70, batch loss = 21.62 (7.8 examples/sec; 1.029 sec/batch; 82h:34m:46s remains)
INFO - root - 2017-12-07 19:45:05.542179: step 43690, loss = 21.38, batch loss = 21.30 (8.6 examples/sec; 0.929 sec/batch; 74h:31m:36s remains)
INFO - root - 2017-12-07 19:45:15.011285: step 43700, loss = 21.87, batch loss = 21.78 (8.5 examples/sec; 0.946 sec/batch; 75h:55m:13s remains)
2017-12-07 19:45:15.917230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5388141 -4.5081205 -4.4174132 -4.353231 -4.3504124 -4.3548141 -4.3838863 -4.4009385 -4.4043403 -4.4213724 -4.4295492 -4.4318886 -4.4114523 -4.3904181 -4.385582][-4.5375261 -4.4922795 -4.3871431 -4.3192987 -4.3183141 -4.3312073 -4.3665676 -4.3841195 -4.3888621 -4.4089351 -4.42629 -4.4454904 -4.445385 -4.4333673 -4.4248114][-4.5283356 -4.4665923 -4.3561521 -4.2982316 -4.307529 -4.3211541 -4.3441324 -4.3454938 -4.345768 -4.3787031 -4.4171815 -4.4577603 -4.476409 -4.4684858 -4.4546208][-4.5269904 -4.466002 -4.3691258 -4.3258653 -4.3351169 -4.3290391 -4.3139596 -4.2755613 -4.2599115 -4.3175216 -4.4036236 -4.4875393 -4.5322309 -4.5162482 -4.4861336][-4.5441818 -4.506063 -4.4344211 -4.3961115 -4.3810444 -4.3347073 -4.2666483 -4.1755996 -4.1326246 -4.2150726 -4.3646631 -4.5097442 -4.589489 -4.5601621 -4.5029793][-4.568634 -4.5606437 -4.50813 -4.4553409 -4.3903794 -4.285326 -4.1623116 -4.0218091 -3.9600129 -4.0711765 -4.2818408 -4.4869418 -4.6022811 -4.5697179 -4.4980183][-4.5757751 -4.5892215 -4.5351219 -4.4473214 -4.3218641 -4.1566858 -3.9952059 -3.8335543 -3.7780972 -3.9265828 -4.1837859 -4.4302125 -4.5721793 -4.5518475 -4.4836159][-4.5534644 -4.5681939 -4.4991508 -4.3862309 -4.2287974 -4.0362267 -3.8706932 -3.7245109 -3.6943429 -3.8694429 -4.1415339 -4.3948665 -4.5463438 -4.543375 -4.4856973][-4.5006051 -4.5067735 -4.434319 -4.3325424 -4.1960769 -4.0272045 -3.8997755 -3.8020532 -3.7890439 -3.939409 -4.1708064 -4.3928766 -4.5398746 -4.553504 -4.5118518][-4.4576287 -4.4609785 -4.4037209 -4.33421 -4.2379889 -4.1052127 -4.0161033 -3.9646461 -3.9555321 -4.0489392 -4.2028952 -4.3633132 -4.4941583 -4.528585 -4.5149603][-4.4595828 -4.468049 -4.4224458 -4.3700218 -4.2927046 -4.1683264 -4.0812712 -4.0548353 -4.0624123 -4.1206689 -4.2048445 -4.2920852 -4.3968711 -4.4498491 -4.4677472][-4.4823823 -4.4886012 -4.4371576 -4.3822851 -4.3086815 -4.1791534 -4.0730715 -4.04757 -4.075736 -4.1406407 -4.1971831 -4.2314882 -4.3062539 -4.37072 -4.4140964][-4.4882145 -4.4794078 -4.4127831 -4.35299 -4.2884564 -4.1692162 -4.0571961 -4.0233421 -4.0598059 -4.142149 -4.20143 -4.2111044 -4.2601418 -4.3239617 -4.3815885][-4.4917636 -4.46957 -4.3964233 -4.3392458 -4.2862897 -4.1857042 -4.0872197 -4.048686 -4.0783229 -4.163846 -4.2312012 -4.2324934 -4.253139 -4.2935867 -4.3470907][-4.5155277 -4.4844208 -4.40529 -4.3479204 -4.2951822 -4.2049022 -4.1306934 -4.10494 -4.128078 -4.1978931 -4.25721 -4.2553511 -4.2482347 -4.2536879 -4.2887716]]...]
INFO - root - 2017-12-07 19:45:25.336271: step 43710, loss = 21.79, batch loss = 21.71 (8.4 examples/sec; 0.952 sec/batch; 76h:22m:54s remains)
INFO - root - 2017-12-07 19:45:34.761802: step 43720, loss = 21.65, batch loss = 21.57 (8.6 examples/sec; 0.927 sec/batch; 74h:20m:36s remains)
INFO - root - 2017-12-07 19:45:44.242735: step 43730, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.955 sec/batch; 76h:35m:02s remains)
INFO - root - 2017-12-07 19:45:53.896482: step 43740, loss = 21.82, batch loss = 21.73 (8.4 examples/sec; 0.954 sec/batch; 76h:31m:30s remains)
INFO - root - 2017-12-07 19:46:03.430458: step 43750, loss = 21.25, batch loss = 21.16 (8.6 examples/sec; 0.927 sec/batch; 74h:19m:37s remains)
INFO - root - 2017-12-07 19:46:12.746578: step 43760, loss = 21.79, batch loss = 21.71 (8.1 examples/sec; 0.983 sec/batch; 78h:48m:12s remains)
INFO - root - 2017-12-07 19:46:22.148636: step 43770, loss = 21.75, batch loss = 21.67 (8.2 examples/sec; 0.973 sec/batch; 78h:04m:31s remains)
INFO - root - 2017-12-07 19:46:31.660557: step 43780, loss = 21.61, batch loss = 21.52 (7.9 examples/sec; 1.016 sec/batch; 81h:28m:56s remains)
INFO - root - 2017-12-07 19:46:41.086140: step 43790, loss = 21.12, batch loss = 21.03 (8.4 examples/sec; 0.956 sec/batch; 76h:41m:29s remains)
INFO - root - 2017-12-07 19:46:50.511287: step 43800, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.944 sec/batch; 75h:42m:50s remains)
2017-12-07 19:46:51.404465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3922472 -4.3927183 -4.4010534 -4.409492 -4.4170246 -4.4227104 -4.4264851 -4.4313822 -4.4387217 -4.4449005 -4.4653096 -4.5176969 -4.5673981 -4.572269 -4.5488515][-4.4690013 -4.4704905 -4.4786172 -4.4855237 -4.4876933 -4.4895577 -4.4913034 -4.4934053 -4.4950676 -4.4961877 -4.5166254 -4.5709662 -4.6155682 -4.6018267 -4.5467606][-4.5802712 -4.5885367 -4.5964937 -4.5970187 -4.5889344 -4.5810857 -4.5733695 -4.5618048 -4.5478234 -4.541635 -4.5710626 -4.6421275 -4.695848 -4.6737585 -4.5934844][-4.6781025 -4.6931386 -4.6972389 -4.6852555 -4.6571507 -4.6242857 -4.5866294 -4.5482793 -4.5187025 -4.5197468 -4.5847678 -4.7030334 -4.7864904 -4.7695417 -4.67563][-4.7110267 -4.7253718 -4.7158108 -4.6830726 -4.6251054 -4.5513196 -4.4631763 -4.3908567 -4.3554735 -4.37844 -4.5011992 -4.6899967 -4.8145027 -4.8102674 -4.7129574][-4.6789684 -4.6922622 -4.6716785 -4.6211257 -4.5326281 -4.4142814 -4.2688456 -4.1617432 -4.1249962 -4.1681161 -4.3470869 -4.6026192 -4.7661333 -4.784914 -4.6985378][-4.6194968 -4.6299667 -4.5928292 -4.5150881 -4.3882365 -4.2244081 -4.0327373 -3.8998487 -3.8624132 -3.9186852 -4.1358457 -4.4340162 -4.6310225 -4.6876249 -4.6257958][-4.5573068 -4.5550194 -4.4892941 -4.3682623 -4.1922631 -3.9908769 -3.7829089 -3.6450176 -3.6130919 -3.6782038 -3.9072218 -4.2133 -4.4378219 -4.5501127 -4.5332127][-4.5232744 -4.5176754 -4.4426 -4.29742 -4.0881848 -3.8696756 -3.6703532 -3.5467229 -3.5309384 -3.611306 -3.8304048 -4.1079473 -4.332972 -4.4792089 -4.5065875][-4.53533 -4.5456343 -4.491209 -4.3555779 -4.1435513 -3.9276195 -3.7497883 -3.6587536 -3.6772373 -3.7770545 -3.9684215 -4.1873178 -4.3752656 -4.5076327 -4.5478625][-4.5564737 -4.5879188 -4.5620246 -4.4557166 -4.2765532 -4.0977182 -3.9651203 -3.9210615 -3.9678402 -4.0682678 -4.2190104 -4.3816662 -4.52024 -4.6065264 -4.6257567][-4.5607529 -4.6089911 -4.6097245 -4.5435886 -4.4267669 -4.3179584 -4.2536445 -4.2524943 -4.2991233 -4.3692803 -4.4729366 -4.5880628 -4.6764479 -4.7097669 -4.6957994][-4.535614 -4.6059775 -4.6280456 -4.5921712 -4.5281224 -4.4804325 -4.4672575 -4.4814358 -4.5064049 -4.5397334 -4.6013885 -4.6743627 -4.7191095 -4.7158012 -4.6840491][-4.4911013 -4.5785594 -4.6120987 -4.5872893 -4.5417519 -4.5138173 -4.5123129 -4.5197411 -4.5261631 -4.5401044 -4.5720363 -4.6071458 -4.619525 -4.6042218 -4.5800433][-4.4358249 -4.5307131 -4.5726194 -4.5563731 -4.5154247 -4.4838367 -4.4712181 -4.46229 -4.4561877 -4.4601159 -4.471097 -4.4791322 -4.4756279 -4.4633656 -4.4541154]]...]
INFO - root - 2017-12-07 19:47:00.813857: step 43810, loss = 21.89, batch loss = 21.80 (9.0 examples/sec; 0.893 sec/batch; 71h:35m:14s remains)
INFO - root - 2017-12-07 19:47:10.239230: step 43820, loss = 21.72, batch loss = 21.64 (8.5 examples/sec; 0.937 sec/batch; 75h:07m:56s remains)
INFO - root - 2017-12-07 19:47:19.607185: step 43830, loss = 21.39, batch loss = 21.31 (8.8 examples/sec; 0.909 sec/batch; 72h:53m:05s remains)
INFO - root - 2017-12-07 19:47:28.778814: step 43840, loss = 21.98, batch loss = 21.89 (8.7 examples/sec; 0.920 sec/batch; 73h:43m:55s remains)
INFO - root - 2017-12-07 19:47:38.022875: step 43850, loss = 20.91, batch loss = 20.82 (8.8 examples/sec; 0.911 sec/batch; 73h:03m:04s remains)
INFO - root - 2017-12-07 19:47:47.300616: step 43860, loss = 21.58, batch loss = 21.50 (8.1 examples/sec; 0.985 sec/batch; 78h:58m:56s remains)
INFO - root - 2017-12-07 19:47:56.680303: step 43870, loss = 21.50, batch loss = 21.41 (8.7 examples/sec; 0.923 sec/batch; 73h:59m:33s remains)
INFO - root - 2017-12-07 19:48:06.139776: step 43880, loss = 21.38, batch loss = 21.30 (8.7 examples/sec; 0.921 sec/batch; 73h:51m:04s remains)
INFO - root - 2017-12-07 19:48:15.607088: step 43890, loss = 21.68, batch loss = 21.60 (8.5 examples/sec; 0.937 sec/batch; 75h:05m:42s remains)
INFO - root - 2017-12-07 19:48:25.192198: step 43900, loss = 21.76, batch loss = 21.68 (8.2 examples/sec; 0.970 sec/batch; 77h:46m:58s remains)
2017-12-07 19:48:26.127250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2882743 -4.3226733 -4.3816538 -4.4425206 -4.4583616 -4.4142561 -4.3308949 -4.2471046 -4.21375 -4.23981 -4.3239017 -4.4366531 -4.529026 -4.5610576 -4.5509148][-4.2947307 -4.323379 -4.3803325 -4.4374285 -4.4447961 -4.3832197 -4.2770867 -4.1787453 -4.1435785 -4.18152 -4.27781 -4.4002786 -4.4961782 -4.5254726 -4.5209756][-4.3059011 -4.3321757 -4.3781471 -4.4149032 -4.4006238 -4.3225713 -4.219408 -4.1437535 -4.1316094 -4.1863065 -4.283155 -4.4054251 -4.4997473 -4.5210791 -4.5090947][-4.3131495 -4.3315654 -4.3598208 -4.3722529 -4.3325143 -4.2394557 -4.1469159 -4.1022587 -4.1209869 -4.197751 -4.2999282 -4.4233336 -4.5214958 -4.5453382 -4.5322375][-4.2946887 -4.2922592 -4.3036132 -4.3074613 -4.255609 -4.1460228 -4.041647 -3.9950211 -4.0271463 -4.1281228 -4.2573528 -4.3991227 -4.5139575 -4.5548 -4.5562654][-4.2382894 -4.2199306 -4.228457 -4.245285 -4.1987257 -4.06776 -3.9161503 -3.8204439 -3.8319125 -3.9535403 -4.1290259 -4.3075566 -4.4481173 -4.5127244 -4.5393777][-4.1554208 -4.1393552 -4.1628237 -4.2050028 -4.1697311 -4.0130463 -3.8028431 -3.6439655 -3.6219881 -3.7608383 -3.9868312 -4.2095065 -4.3748126 -4.4550824 -4.5011883][-4.0600758 -4.0667334 -4.1215358 -4.1945429 -4.177773 -4.013649 -3.7697492 -3.5707285 -3.5248749 -3.6748147 -3.9342148 -4.1848488 -4.3593135 -4.4411392 -4.48729][-3.9939909 -4.0273328 -4.1094036 -4.2086015 -4.2249937 -4.0904584 -3.8571949 -3.6481106 -3.588547 -3.7380016 -4.0003896 -4.2539549 -4.4211926 -4.4927526 -4.5190382][-3.9863102 -4.0323253 -4.1279497 -4.2433152 -4.2966876 -4.2228847 -4.0421534 -3.8543761 -3.7902737 -3.9150646 -4.1426692 -4.3689694 -4.5175481 -4.576767 -4.5814877][-4.055573 -4.1025934 -4.1963687 -4.3111796 -4.3898106 -4.3759041 -4.2645454 -4.124958 -4.0679765 -4.1535912 -4.3159323 -4.4848714 -4.6011224 -4.6460462 -4.6381159][-4.2248139 -4.2657409 -4.3393679 -4.4262004 -4.4949169 -4.5068989 -4.45047 -4.3652968 -4.325563 -4.37585 -4.473949 -4.5824847 -4.6615324 -4.6891665 -4.6714163][-4.4247975 -4.4489927 -4.4914365 -4.5367107 -4.5734448 -4.5840688 -4.5620875 -4.5253558 -4.5069489 -4.5346837 -4.5852542 -4.6468773 -4.6945419 -4.7030783 -4.6732359][-4.5465918 -4.5472088 -4.5547223 -4.5612979 -4.5665059 -4.5677037 -4.563972 -4.5586653 -4.5592308 -4.5782452 -4.6045604 -4.6380625 -4.6640697 -4.6585846 -4.619307][-4.5184197 -4.5047145 -4.4925027 -4.4784613 -4.4681325 -4.4663606 -4.4739933 -4.4843979 -4.4936862 -4.5062943 -4.5196629 -4.5365338 -4.54905 -4.5397692 -4.5060005]]...]
INFO - root - 2017-12-07 19:48:35.612957: step 43910, loss = 21.34, batch loss = 21.26 (8.5 examples/sec; 0.938 sec/batch; 75h:09m:34s remains)
INFO - root - 2017-12-07 19:48:45.125771: step 43920, loss = 21.55, batch loss = 21.47 (8.5 examples/sec; 0.937 sec/batch; 75h:06m:18s remains)
INFO - root - 2017-12-07 19:48:54.434568: step 43930, loss = 21.64, batch loss = 21.56 (8.9 examples/sec; 0.898 sec/batch; 71h:59m:48s remains)
INFO - root - 2017-12-07 19:49:03.924922: step 43940, loss = 21.49, batch loss = 21.40 (8.4 examples/sec; 0.951 sec/batch; 76h:14m:36s remains)
INFO - root - 2017-12-07 19:49:13.394509: step 43950, loss = 21.30, batch loss = 21.21 (8.3 examples/sec; 0.961 sec/batch; 77h:00m:36s remains)
INFO - root - 2017-12-07 19:49:22.849062: step 43960, loss = 21.54, batch loss = 21.46 (9.5 examples/sec; 0.843 sec/batch; 67h:32m:23s remains)
INFO - root - 2017-12-07 19:49:32.255373: step 43970, loss = 21.25, batch loss = 21.17 (8.2 examples/sec; 0.972 sec/batch; 77h:55m:15s remains)
INFO - root - 2017-12-07 19:49:41.774826: step 43980, loss = 21.48, batch loss = 21.39 (8.5 examples/sec; 0.936 sec/batch; 75h:03m:08s remains)
INFO - root - 2017-12-07 19:49:51.041506: step 43990, loss = 21.92, batch loss = 21.83 (8.1 examples/sec; 0.988 sec/batch; 79h:09m:14s remains)
INFO - root - 2017-12-07 19:50:00.432746: step 44000, loss = 21.43, batch loss = 21.34 (8.1 examples/sec; 0.984 sec/batch; 78h:52m:03s remains)
2017-12-07 19:50:01.351772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6787872 -4.6705027 -4.6456676 -4.6024013 -4.6037812 -4.6326642 -4.6447525 -4.6297951 -4.5946517 -4.575747 -4.5679135 -4.5586472 -4.5706315 -4.6 -4.6434817][-4.5919232 -4.6353335 -4.6722651 -4.6797676 -4.6963134 -4.6904888 -4.6492548 -4.6242981 -4.6200275 -4.619473 -4.5848117 -4.5337462 -4.5364776 -4.5746503 -4.6273208][-4.4384804 -4.4947429 -4.5684566 -4.6293921 -4.6707625 -4.6341939 -4.5525908 -4.5585251 -4.6287861 -4.6658096 -4.6131663 -4.5223317 -4.5119214 -4.5604148 -4.6244888][-4.2977004 -4.3272638 -4.4034319 -4.5015688 -4.5603309 -4.4950352 -4.377089 -4.4169846 -4.5587282 -4.6372342 -4.594408 -4.5016837 -4.493073 -4.5433869 -4.5988145][-4.1896372 -4.1811914 -4.2433262 -4.3576379 -4.422832 -4.3272729 -4.1590271 -4.1938257 -4.37457 -4.4972754 -4.5004463 -4.4496279 -4.4610767 -4.5053205 -4.5311656][-4.0826435 -4.0608764 -4.1243887 -4.2510891 -4.3132877 -4.1820974 -3.9537902 -3.9518602 -4.1506672 -4.3179426 -4.3787074 -4.3800807 -4.406146 -4.4290943 -4.4224496][-4.0190721 -4.0012279 -4.0680814 -4.1879368 -4.2320967 -4.0678463 -3.7969079 -3.7650418 -3.9884868 -4.212914 -4.32378 -4.3478117 -4.3529458 -4.3413229 -4.3150783][-4.0701938 -4.0440359 -4.0869303 -4.1740875 -4.2007513 -4.0461578 -3.7932177 -3.7595918 -3.9941974 -4.2554932 -4.3932981 -4.4050264 -4.3615961 -4.3125453 -4.279964][-4.1991296 -4.1418457 -4.1398392 -4.1959152 -4.2324429 -4.1367974 -3.9527512 -3.9382615 -4.1575246 -4.4155831 -4.5517955 -4.5344076 -4.4466867 -4.3727465 -4.3395882][-4.2961817 -4.2063146 -4.1684427 -4.2110629 -4.2727513 -4.2495384 -4.1387825 -4.1360817 -4.3212972 -4.5458741 -4.6590886 -4.620697 -4.5176849 -4.4400578 -4.4110708][-4.348907 -4.2516975 -4.1993704 -4.2322965 -4.3056145 -4.3341651 -4.2790222 -4.2709885 -4.3997188 -4.5622153 -4.6369195 -4.5929322 -4.4985986 -4.4259882 -4.399684][-4.4039416 -4.3270688 -4.2789688 -4.2996058 -4.361464 -4.4108686 -4.39692 -4.3820662 -4.44118 -4.5202012 -4.5422206 -4.4918728 -4.40431 -4.3322296 -4.3069186][-4.46963 -4.4289269 -4.3975348 -4.4047804 -4.4381719 -4.4829168 -4.5021768 -4.4953389 -4.4986835 -4.4991903 -4.4721203 -4.409153 -4.3162956 -4.237133 -4.209888][-4.5169983 -4.5032744 -4.488348 -4.4844985 -4.4912615 -4.5216117 -4.5625277 -4.5764079 -4.5550671 -4.5102453 -4.4597754 -4.395638 -4.306601 -4.2241859 -4.1827931][-4.5126 -4.5085559 -4.5028305 -4.4942942 -4.4885907 -4.5090384 -4.5607953 -4.6002355 -4.587162 -4.5313563 -4.4722166 -4.4111633 -4.3327127 -4.2565594 -4.1981049]]...]
INFO - root - 2017-12-07 19:50:10.722635: step 44010, loss = 21.67, batch loss = 21.58 (8.3 examples/sec; 0.965 sec/batch; 77h:18m:26s remains)
INFO - root - 2017-12-07 19:50:20.134318: step 44020, loss = 21.74, batch loss = 21.66 (8.9 examples/sec; 0.903 sec/batch; 72h:20m:52s remains)
INFO - root - 2017-12-07 19:50:29.427785: step 44030, loss = 21.59, batch loss = 21.51 (9.3 examples/sec; 0.864 sec/batch; 69h:16m:15s remains)
INFO - root - 2017-12-07 19:50:38.871698: step 44040, loss = 21.46, batch loss = 21.37 (9.0 examples/sec; 0.888 sec/batch; 71h:10m:22s remains)
INFO - root - 2017-12-07 19:50:48.399673: step 44050, loss = 21.32, batch loss = 21.23 (8.4 examples/sec; 0.955 sec/batch; 76h:31m:11s remains)
INFO - root - 2017-12-07 19:50:57.968590: step 44060, loss = 21.82, batch loss = 21.73 (8.4 examples/sec; 0.956 sec/batch; 76h:37m:26s remains)
INFO - root - 2017-12-07 19:51:07.347002: step 44070, loss = 21.46, batch loss = 21.38 (8.0 examples/sec; 0.994 sec/batch; 79h:37m:45s remains)
INFO - root - 2017-12-07 19:51:16.774497: step 44080, loss = 21.54, batch loss = 21.46 (8.5 examples/sec; 0.937 sec/batch; 75h:03m:18s remains)
INFO - root - 2017-12-07 19:51:26.281297: step 44090, loss = 21.52, batch loss = 21.43 (8.5 examples/sec; 0.938 sec/batch; 75h:08m:26s remains)
INFO - root - 2017-12-07 19:51:35.599680: step 44100, loss = 21.62, batch loss = 21.54 (9.0 examples/sec; 0.892 sec/batch; 71h:29m:25s remains)
2017-12-07 19:51:36.620463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6386991 -4.4197283 -4.204845 -4.1976771 -4.3271184 -4.4173551 -4.4681883 -4.4696693 -4.4405451 -4.4565706 -4.5076342 -4.5454879 -4.5654793 -4.55518 -4.5486021][-4.5345654 -4.2801909 -4.0416107 -4.0324159 -4.17632 -4.3086753 -4.4108019 -4.4353285 -4.4236507 -4.4705014 -4.5425291 -4.6116586 -4.6568952 -4.6408153 -4.60521][-4.4584026 -4.21346 -3.9872961 -3.9740124 -4.105258 -4.241488 -4.3566194 -4.3890591 -4.4057574 -4.5046515 -4.62198 -4.7241964 -4.7652316 -4.7096109 -4.6281352][-4.4211659 -4.2265644 -4.0447211 -4.0329809 -4.1444263 -4.2562943 -4.3421788 -4.3522444 -4.3860288 -4.5357795 -4.6988811 -4.8199649 -4.8334608 -4.7219815 -4.6024323][-4.370914 -4.2309351 -4.1028547 -4.0992241 -4.1847043 -4.2564592 -4.2848883 -4.248054 -4.27678 -4.4424391 -4.6129904 -4.717793 -4.6871629 -4.5342965 -4.4212928][-4.3237257 -4.2216568 -4.137651 -4.1436596 -4.2030416 -4.2227697 -4.1716571 -4.0669832 -4.0735841 -4.228632 -4.3788562 -4.4454775 -4.3677945 -4.1930447 -4.1166959][-4.2966595 -4.2052555 -4.1434989 -4.1502533 -4.1740484 -4.125947 -3.9858422 -3.8186424 -3.8314543 -4.0121636 -4.1751752 -4.23584 -4.1394453 -3.967593 -3.9249234][-4.2894559 -4.1937857 -4.1361356 -4.1371484 -4.1363821 -4.0396628 -3.8374152 -3.637434 -3.6740656 -3.8883965 -4.0673127 -4.1323085 -4.0354095 -3.8784003 -3.8611333][-4.3555546 -4.2498503 -4.1876254 -4.1897306 -4.1897154 -4.0937004 -3.8991354 -3.7150931 -3.7647271 -3.976547 -4.14483 -4.2091179 -4.1157074 -3.9605358 -3.9403105][-4.4420452 -4.3251543 -4.2519255 -4.2546844 -4.2648697 -4.1976252 -4.0570583 -3.9259393 -3.981647 -4.1700292 -4.3292313 -4.4173646 -4.35771 -4.2122326 -4.1634536][-4.4660707 -4.3422322 -4.26477 -4.2763824 -4.2995987 -4.2651668 -4.1847415 -4.1117821 -4.1654482 -4.317287 -4.464222 -4.5715518 -4.5518527 -4.4392214 -4.3835454][-4.4984355 -4.39183 -4.3262167 -4.3507304 -4.3892646 -4.3863792 -4.3548942 -4.3275313 -4.3738327 -4.4812188 -4.5941725 -4.6850262 -4.6834931 -4.6083221 -4.5606742][-4.5398741 -4.4712739 -4.4287457 -4.4643083 -4.5145073 -4.5298085 -4.5202274 -4.5074 -4.5346 -4.5981364 -4.6708717 -4.7278376 -4.7252059 -4.6788449 -4.6435857][-4.4966855 -4.4739895 -4.4688191 -4.5198412 -4.5776582 -4.5994682 -4.5923662 -4.578196 -4.58168 -4.6054521 -4.6382837 -4.6594009 -4.648181 -4.6202536 -4.5980363][-4.4089293 -4.4156775 -4.437079 -4.4915657 -4.5448127 -4.5675979 -4.5657139 -4.5569663 -4.54949 -4.544744 -4.5420132 -4.5315466 -4.5067272 -4.47824 -4.4535794]]...]
INFO - root - 2017-12-07 19:51:46.036762: step 44110, loss = 21.67, batch loss = 21.58 (8.4 examples/sec; 0.955 sec/batch; 76h:29m:40s remains)
INFO - root - 2017-12-07 19:51:55.383821: step 44120, loss = 21.36, batch loss = 21.28 (9.0 examples/sec; 0.886 sec/batch; 70h:59m:25s remains)
INFO - root - 2017-12-07 19:52:04.688639: step 44130, loss = 21.19, batch loss = 21.11 (8.4 examples/sec; 0.955 sec/batch; 76h:28m:35s remains)
INFO - root - 2017-12-07 19:52:14.178622: step 44140, loss = 21.14, batch loss = 21.06 (8.2 examples/sec; 0.977 sec/batch; 78h:15m:17s remains)
INFO - root - 2017-12-07 19:52:23.675029: step 44150, loss = 21.40, batch loss = 21.31 (8.4 examples/sec; 0.953 sec/batch; 76h:20m:58s remains)
INFO - root - 2017-12-07 19:52:33.054986: step 44160, loss = 21.78, batch loss = 21.70 (8.9 examples/sec; 0.898 sec/batch; 71h:56m:56s remains)
INFO - root - 2017-12-07 19:52:42.418204: step 44170, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.932 sec/batch; 74h:40m:37s remains)
INFO - root - 2017-12-07 19:52:51.771328: step 44180, loss = 21.51, batch loss = 21.42 (8.7 examples/sec; 0.922 sec/batch; 73h:50m:23s remains)
INFO - root - 2017-12-07 19:53:01.301466: step 44190, loss = 21.46, batch loss = 21.38 (8.6 examples/sec; 0.929 sec/batch; 74h:24m:01s remains)
INFO - root - 2017-12-07 19:53:10.789597: step 44200, loss = 21.18, batch loss = 21.10 (8.2 examples/sec; 0.974 sec/batch; 78h:01m:15s remains)
2017-12-07 19:53:11.741958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4088459 -4.4381738 -4.4590359 -4.4581079 -4.4232912 -4.3566427 -4.2954922 -4.2660375 -4.2697811 -4.3216591 -4.3426108 -4.26754 -4.1523209 -4.0627575 -4.033093][-4.4135337 -4.4383936 -4.4534211 -4.45856 -4.4430766 -4.3956032 -4.3445778 -4.3252006 -4.3378377 -4.3848429 -4.3935575 -4.3107691 -4.1999078 -4.1317811 -4.1241913][-4.4209919 -4.4508061 -4.4706492 -4.4875622 -4.4912915 -4.4572387 -4.4032989 -4.3717861 -4.370358 -4.3990331 -4.4036975 -4.3367295 -4.2603183 -4.2425218 -4.2648773][-4.4242759 -4.4565153 -4.4787526 -4.5009723 -4.5103345 -4.4789906 -4.4155746 -4.3705764 -4.3580503 -4.378777 -4.3942542 -4.3579755 -4.3265796 -4.3573346 -4.4010177][-4.4166937 -4.4339094 -4.4350085 -4.4344726 -4.4214282 -4.37551 -4.3045764 -4.2594285 -4.2523265 -4.2874036 -4.33115 -4.3414526 -4.3647161 -4.4350858 -4.4899411][-4.3986406 -4.38455 -4.3435163 -4.2953405 -4.2335792 -4.1527929 -4.0723763 -4.0363507 -4.04553 -4.1139269 -4.2026253 -4.2709022 -4.3597322 -4.4718275 -4.541317][-4.3875427 -4.3520226 -4.2801642 -4.1911297 -4.0803857 -3.9556775 -3.8588789 -3.8257012 -3.8491321 -3.9491506 -4.0800219 -4.1953621 -4.3352566 -4.4749961 -4.5485935][-4.3961029 -4.3714185 -4.3095961 -4.2188087 -4.0873303 -3.9254665 -3.8004644 -3.7485256 -3.7673976 -3.8835316 -4.0412946 -4.1834712 -4.3433619 -4.4771948 -4.528337][-4.4194136 -4.4289174 -4.4091358 -4.3547997 -4.2368665 -4.059835 -3.9063489 -3.824713 -3.8300827 -3.9518633 -4.1219926 -4.2707987 -4.4185009 -4.5182939 -4.5320282][-4.4390473 -4.4823608 -4.50507 -4.4932337 -4.4061222 -4.2487612 -4.1063094 -4.0272455 -4.0356693 -4.1533442 -4.3058004 -4.4308743 -4.5399127 -4.5922518 -4.5701613][-4.4422507 -4.5016289 -4.5490875 -4.5708151 -4.5234084 -4.416049 -4.3223124 -4.2768474 -4.3025188 -4.4037151 -4.5123644 -4.5890808 -4.6455684 -4.6525803 -4.6071496][-4.4297123 -4.4873857 -4.543581 -4.5892534 -4.5841427 -4.5359426 -4.495656 -4.4843173 -4.5192132 -4.5937243 -4.6519642 -4.6772285 -4.6833758 -4.6561813 -4.6037807][-4.406877 -4.454237 -4.5103636 -4.5710506 -4.6005287 -4.60162 -4.5998 -4.6047416 -4.6291618 -4.666893 -4.6835313 -4.6747217 -4.6524653 -4.6134152 -4.5678992][-4.3786659 -4.4123983 -4.462667 -4.5262594 -4.5763078 -4.606916 -4.6219082 -4.624579 -4.6277981 -4.6370487 -4.6356106 -4.6197076 -4.5934114 -4.5596433 -4.5290308][-4.3464231 -4.3671784 -4.40551 -4.4578414 -4.5067825 -4.5424819 -4.5581217 -4.5537229 -4.5430684 -4.538168 -4.5330591 -4.5215306 -4.5029058 -4.4823055 -4.4659152]]...]
INFO - root - 2017-12-07 19:53:21.073391: step 44210, loss = 21.43, batch loss = 21.35 (8.4 examples/sec; 0.957 sec/batch; 76h:38m:11s remains)
INFO - root - 2017-12-07 19:53:30.481963: step 44220, loss = 21.29, batch loss = 21.21 (7.8 examples/sec; 1.032 sec/batch; 82h:36m:53s remains)
INFO - root - 2017-12-07 19:53:39.900280: step 44230, loss = 21.38, batch loss = 21.30 (8.5 examples/sec; 0.940 sec/batch; 75h:18m:30s remains)
INFO - root - 2017-12-07 19:53:49.205507: step 44240, loss = 21.86, batch loss = 21.78 (8.8 examples/sec; 0.909 sec/batch; 72h:45m:46s remains)
INFO - root - 2017-12-07 19:53:58.645423: step 44250, loss = 21.34, batch loss = 21.25 (8.2 examples/sec; 0.970 sec/batch; 77h:42m:00s remains)
INFO - root - 2017-12-07 19:54:08.156853: step 44260, loss = 21.14, batch loss = 21.05 (8.0 examples/sec; 0.997 sec/batch; 79h:50m:15s remains)
INFO - root - 2017-12-07 19:54:17.473906: step 44270, loss = 21.42, batch loss = 21.34 (8.9 examples/sec; 0.901 sec/batch; 72h:06m:18s remains)
INFO - root - 2017-12-07 19:54:26.917634: step 44280, loss = 21.64, batch loss = 21.55 (9.5 examples/sec; 0.844 sec/batch; 67h:31m:59s remains)
INFO - root - 2017-12-07 19:54:36.322631: step 44290, loss = 21.20, batch loss = 21.11 (8.5 examples/sec; 0.946 sec/batch; 75h:42m:36s remains)
INFO - root - 2017-12-07 19:54:45.810597: step 44300, loss = 21.70, batch loss = 21.62 (8.5 examples/sec; 0.944 sec/batch; 75h:33m:17s remains)
2017-12-07 19:54:46.766357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4639869 -4.4846706 -4.5041389 -4.5180264 -4.5259013 -4.5297432 -4.5272164 -4.5150685 -4.4958234 -4.4753447 -4.4584174 -4.4500456 -4.4524689 -4.4624133 -4.472559][-4.5759287 -4.6068406 -4.6244802 -4.6295753 -4.6267333 -4.6212797 -4.6142941 -4.5987554 -4.5751204 -4.5512638 -4.5338411 -4.5303297 -4.542758 -4.5617094 -4.5723271][-4.6549292 -4.6944723 -4.7008286 -4.6847277 -4.657548 -4.6315012 -4.6175742 -4.6044884 -4.5855432 -4.5683217 -4.561233 -4.5729871 -4.6073761 -4.6457009 -4.6612921][-4.6601176 -4.69497 -4.6738009 -4.6169405 -4.5471921 -4.4903922 -4.4732294 -4.4764671 -4.4768977 -4.4783525 -4.4949784 -4.5381904 -4.6092744 -4.677978 -4.7057223][-4.5977921 -4.6180906 -4.5556369 -4.4427948 -4.3203497 -4.2313476 -4.2177167 -4.242712 -4.2680659 -4.2973523 -4.3510485 -4.44165 -4.55947 -4.6594925 -4.7019033][-4.5010924 -4.5107222 -4.4025645 -4.2275796 -4.0529909 -3.9406521 -3.9352896 -3.9781146 -4.0336847 -4.1000443 -4.1938696 -4.3289967 -4.4816489 -4.6004329 -4.654417][-4.4113283 -4.4138241 -4.2677765 -4.0443249 -3.8363867 -3.7166142 -3.7170649 -3.7651763 -3.8491998 -3.9515409 -4.0724993 -4.2301006 -4.3941307 -4.5154829 -4.574163][-4.3460073 -4.3492742 -4.1980224 -3.973861 -3.7706861 -3.6604657 -3.6600451 -3.6991756 -3.799365 -3.9251173 -4.0522656 -4.2046103 -4.357585 -4.4642286 -4.5072703][-4.3314867 -4.3413115 -4.2197146 -4.0422344 -3.8771136 -3.7880642 -3.7833412 -3.8101532 -3.909956 -4.0332789 -4.1429057 -4.2698169 -4.394259 -4.4669795 -4.4730759][-4.3722305 -4.3910952 -4.320188 -4.2111197 -4.0960026 -4.0275445 -4.0135608 -4.0258265 -4.1044693 -4.2026367 -4.2861834 -4.3835392 -4.4756413 -4.5057292 -4.4661088][-4.423389 -4.452786 -4.4335275 -4.3859549 -4.3124323 -4.2557869 -4.2319942 -4.2333155 -4.2818379 -4.3446403 -4.40353 -4.4781156 -4.5443664 -4.5388045 -4.4637079][-4.4509349 -4.4791236 -4.4855585 -4.4679127 -4.4163656 -4.3672113 -4.3417082 -4.3402638 -4.3645196 -4.4000349 -4.4495459 -4.5195146 -4.5753756 -4.5509744 -4.4564085][-4.4187641 -4.4306965 -4.4369736 -4.4231806 -4.3762207 -4.3309441 -4.3117652 -4.3196206 -4.3434567 -4.3824277 -4.447588 -4.5297322 -4.5861378 -4.5511603 -4.4414759][-4.3462272 -4.3375235 -4.3339882 -4.3167949 -4.2720423 -4.2337255 -4.2272925 -4.2518072 -4.2932129 -4.35735 -4.4495792 -4.5465522 -4.6011968 -4.5531416 -4.4256091][-4.2590384 -4.2418962 -4.2403111 -4.23346 -4.2040873 -4.1831965 -4.1945877 -4.2371993 -4.3005676 -4.3900938 -4.5002217 -4.5966864 -4.6343503 -4.5632577 -4.4129891]]...]
INFO - root - 2017-12-07 19:54:56.206290: step 44310, loss = 21.01, batch loss = 20.93 (8.8 examples/sec; 0.910 sec/batch; 72h:49m:06s remains)
INFO - root - 2017-12-07 19:55:05.749309: step 44320, loss = 20.94, batch loss = 20.86 (8.7 examples/sec; 0.918 sec/batch; 73h:27m:58s remains)
INFO - root - 2017-12-07 19:55:15.259887: step 44330, loss = 21.41, batch loss = 21.32 (8.3 examples/sec; 0.963 sec/batch; 77h:03m:53s remains)
INFO - root - 2017-12-07 19:55:24.601082: step 44340, loss = 20.98, batch loss = 20.89 (8.0 examples/sec; 1.004 sec/batch; 80h:23m:26s remains)
INFO - root - 2017-12-07 19:55:33.986255: step 44350, loss = 21.23, batch loss = 21.15 (9.2 examples/sec; 0.865 sec/batch; 69h:16m:07s remains)
INFO - root - 2017-12-07 19:55:43.417792: step 44360, loss = 20.88, batch loss = 20.80 (8.9 examples/sec; 0.896 sec/batch; 71h:40m:43s remains)
INFO - root - 2017-12-07 19:55:52.842999: step 44370, loss = 21.62, batch loss = 21.53 (8.7 examples/sec; 0.916 sec/batch; 73h:20m:29s remains)
INFO - root - 2017-12-07 19:56:02.311799: step 44380, loss = 21.28, batch loss = 21.20 (7.8 examples/sec; 1.022 sec/batch; 81h:49m:20s remains)
INFO - root - 2017-12-07 19:56:11.781777: step 44390, loss = 21.51, batch loss = 21.43 (8.3 examples/sec; 0.960 sec/batch; 76h:50m:30s remains)
INFO - root - 2017-12-07 19:56:21.174617: step 44400, loss = 21.72, batch loss = 21.64 (8.9 examples/sec; 0.901 sec/batch; 72h:05m:22s remains)
2017-12-07 19:56:22.196417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3386326 -4.3381295 -4.3333416 -4.3339829 -4.34827 -4.3721747 -4.3867922 -4.3762674 -4.3495069 -4.3212876 -4.3077035 -4.3113413 -4.3228951 -4.337009 -4.3430977][-4.3064604 -4.2923717 -4.2721496 -4.2642059 -4.2826548 -4.3231182 -4.3564777 -4.3614426 -4.3431797 -4.3139815 -4.291985 -4.2841043 -4.2888904 -4.3041053 -4.317359][-4.2823195 -4.2524557 -4.2134876 -4.1942573 -4.2137842 -4.2687883 -4.3232417 -4.3511457 -4.3529115 -4.3321481 -4.3039341 -4.2792563 -4.2693057 -4.2797723 -4.2974186][-4.2748108 -4.2392149 -4.1917367 -4.1640949 -4.1774349 -4.2320428 -4.2948484 -4.3382425 -4.3582711 -4.3515553 -4.3256359 -4.2903614 -4.2666063 -4.2699628 -4.2878857][-4.2940269 -4.2689676 -4.228929 -4.1981635 -4.1973543 -4.2327814 -4.2823443 -4.3217463 -4.3470426 -4.3517251 -4.3353271 -4.3015666 -4.2726741 -4.270215 -4.2847686][-4.3348885 -4.3302231 -4.3051991 -4.2737303 -4.2546296 -4.2610536 -4.28395 -4.3047729 -4.3218336 -4.3289986 -4.3203073 -4.2933621 -4.2664628 -4.2626276 -4.276423][-4.3800478 -4.38483 -4.3653526 -4.3310847 -4.297338 -4.2817235 -4.283061 -4.2875481 -4.2958093 -4.3015313 -4.2953997 -4.2738867 -4.2523723 -4.2528992 -4.2742023][-4.3940287 -4.3848372 -4.3558545 -4.3208251 -4.2888207 -4.2744403 -4.2754321 -4.2774053 -4.2832255 -4.2891722 -4.2855053 -4.2700353 -4.2565427 -4.2643557 -4.296833][-4.3596978 -4.3185105 -4.2742186 -4.2510777 -4.2463903 -4.2614012 -4.2845407 -4.2980409 -4.3087316 -4.3185229 -4.3209958 -4.315433 -4.3100305 -4.31798 -4.3491392][-4.2841058 -4.20238 -4.1415768 -4.1401978 -4.1807923 -4.2422009 -4.299788 -4.3328719 -4.3546095 -4.3721728 -4.384757 -4.391675 -4.393517 -4.3955646 -4.4132237][-4.1968784 -4.0815105 -4.0049443 -4.0202775 -4.1005745 -4.2037187 -4.2930431 -4.34669 -4.3820844 -4.4093347 -4.43261 -4.4531522 -4.4638958 -4.4602284 -4.4610672][-4.1465168 -4.02163 -3.9402428 -3.9604893 -4.0532451 -4.1696239 -4.269115 -4.3318257 -4.3762655 -4.4095535 -4.4364758 -4.4626222 -4.4779811 -4.4700108 -4.4572883][-4.1610956 -4.0587635 -3.9928598 -4.0116372 -4.0891647 -4.1851707 -4.2654877 -4.3179221 -4.3591781 -4.3887229 -4.4063993 -4.4226251 -4.4306064 -4.4146342 -4.38887][-4.2031364 -4.1402617 -4.1034632 -4.1242766 -4.1777687 -4.2384691 -4.2848635 -4.3127646 -4.3380256 -4.3533688 -4.354279 -4.354167 -4.3493862 -4.3202281 -4.2769232][-4.2179723 -4.1870008 -4.1769686 -4.2033377 -4.2390881 -4.2720261 -4.29012 -4.2949758 -4.3016229 -4.3036366 -4.2955556 -4.2894406 -4.2804618 -4.2413359 -4.1785359]]...]
INFO - root - 2017-12-07 19:56:31.577260: step 44410, loss = 21.49, batch loss = 21.41 (8.2 examples/sec; 0.972 sec/batch; 77h:45m:44s remains)
INFO - root - 2017-12-07 19:56:41.008578: step 44420, loss = 21.61, batch loss = 21.53 (7.5 examples/sec; 1.073 sec/batch; 85h:50m:36s remains)
INFO - root - 2017-12-07 19:56:50.466614: step 44430, loss = 21.04, batch loss = 20.96 (8.3 examples/sec; 0.962 sec/batch; 76h:58m:52s remains)
INFO - root - 2017-12-07 19:56:59.862577: step 44440, loss = 21.69, batch loss = 21.61 (8.5 examples/sec; 0.945 sec/batch; 75h:34m:57s remains)
INFO - root - 2017-12-07 19:57:09.273086: step 44450, loss = 21.46, batch loss = 21.38 (8.4 examples/sec; 0.952 sec/batch; 76h:12m:34s remains)
INFO - root - 2017-12-07 19:57:18.825597: step 44460, loss = 21.56, batch loss = 21.47 (8.2 examples/sec; 0.977 sec/batch; 78h:11m:08s remains)
INFO - root - 2017-12-07 19:57:28.342100: step 44470, loss = 21.15, batch loss = 21.06 (8.7 examples/sec; 0.923 sec/batch; 73h:49m:24s remains)
INFO - root - 2017-12-07 19:57:37.601304: step 44480, loss = 21.59, batch loss = 21.51 (9.3 examples/sec; 0.860 sec/batch; 68h:46m:16s remains)
INFO - root - 2017-12-07 19:57:47.045007: step 44490, loss = 21.90, batch loss = 21.82 (8.7 examples/sec; 0.919 sec/batch; 73h:32m:41s remains)
INFO - root - 2017-12-07 19:57:56.241021: step 44500, loss = 21.57, batch loss = 21.48 (8.9 examples/sec; 0.899 sec/batch; 71h:56m:24s remains)
2017-12-07 19:57:57.155908: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2952051 -4.2970848 -4.3080277 -4.3208828 -4.334609 -4.3500986 -4.3665805 -4.3848081 -4.4008818 -4.4124703 -4.4206409 -4.4264336 -4.4296594 -4.4276404 -4.4190607][-4.2950373 -4.3066173 -4.3341665 -4.3677945 -4.4025831 -4.4360175 -4.4662018 -4.4921212 -4.5093169 -4.5142612 -4.515121 -4.5183816 -4.5262179 -4.5329704 -4.5263062][-4.3042312 -4.3328466 -4.3868876 -4.4511242 -4.513319 -4.5666652 -4.603332 -4.6193156 -4.6140308 -4.5881886 -4.5633922 -4.5589738 -4.5768509 -4.6066027 -4.6190829][-4.3317456 -4.380445 -4.4614444 -4.5504613 -4.6263714 -4.6793461 -4.6994267 -4.6814404 -4.6343865 -4.5675936 -4.5144329 -4.5048828 -4.5392671 -4.6015387 -4.6460891][-4.3705096 -4.435379 -4.526721 -4.6119132 -4.6638427 -4.6769147 -4.6467381 -4.5763597 -4.4952035 -4.4167314 -4.3655009 -4.3707037 -4.4327416 -4.5318236 -4.6152334][-4.412528 -4.4803128 -4.5613813 -4.6116185 -4.6034565 -4.5395427 -4.4315977 -4.3027148 -4.2039771 -4.147604 -4.1273317 -4.1581526 -4.2587223 -4.4011836 -4.5296521][-4.4368896 -4.492125 -4.5489621 -4.5526066 -4.4729896 -4.3242097 -4.1453485 -3.9789577 -3.8902338 -3.8850913 -3.9110241 -3.9652112 -4.1002884 -4.2762723 -4.4405923][-4.4236255 -4.4537263 -4.483892 -4.4554281 -4.327414 -4.1229267 -3.9136894 -3.7453372 -3.6820142 -3.7248173 -3.7888443 -3.8603137 -4.0198488 -4.2130976 -4.3956289][-4.3896003 -4.3948035 -4.411675 -4.3847361 -4.2549038 -4.0404515 -3.8426189 -3.6946349 -3.6412261 -3.6902146 -3.7579818 -3.8327065 -3.9999931 -4.1989031 -4.3896985][-4.3687253 -4.3605065 -4.3780828 -4.3764992 -4.2820969 -4.1052575 -3.9565387 -3.8539863 -3.8103504 -3.8396702 -3.8881204 -3.9542696 -4.1044054 -4.2844238 -4.4576716][-4.3715448 -4.3638058 -4.3879395 -4.413569 -4.3700681 -4.2614012 -4.1800256 -4.1357059 -4.1134639 -4.1186485 -4.1378231 -4.185626 -4.3013449 -4.4392238 -4.5646548][-4.3848376 -4.3843985 -4.4143133 -4.459362 -4.4633174 -4.425343 -4.4028172 -4.4038825 -4.4024606 -4.3911834 -4.3851347 -4.4102888 -4.481699 -4.5641012 -4.6270447][-4.3937645 -4.4018869 -4.4305854 -4.4757795 -4.504488 -4.5137444 -4.5297556 -4.5555034 -4.5675931 -4.5553193 -4.5398431 -4.5456986 -4.5729547 -4.5989046 -4.6053591][-4.3842044 -4.3974552 -4.4209871 -4.4558825 -4.4883866 -4.5142136 -4.5383878 -4.5596981 -4.5673943 -4.5573015 -4.5417256 -4.5336628 -4.5302286 -4.5211883 -4.5006533][-4.3515072 -4.3642325 -4.38077 -4.4027987 -4.424861 -4.4425921 -4.4539971 -4.4591656 -4.4564991 -4.4466157 -4.4328213 -4.4197493 -4.4074826 -4.3939877 -4.3779488]]...]
INFO - root - 2017-12-07 19:58:06.378194: step 44510, loss = 21.47, batch loss = 21.38 (9.1 examples/sec; 0.876 sec/batch; 70h:04m:38s remains)
INFO - root - 2017-12-07 19:58:15.592851: step 44520, loss = 21.33, batch loss = 21.24 (8.9 examples/sec; 0.897 sec/batch; 71h:43m:38s remains)
INFO - root - 2017-12-07 19:58:24.814358: step 44530, loss = 21.24, batch loss = 21.15 (8.8 examples/sec; 0.909 sec/batch; 72h:45m:03s remains)
INFO - root - 2017-12-07 19:58:34.209674: step 44540, loss = 21.31, batch loss = 21.23 (8.5 examples/sec; 0.939 sec/batch; 75h:07m:07s remains)
INFO - root - 2017-12-07 19:58:43.562976: step 44550, loss = 21.57, batch loss = 21.49 (8.7 examples/sec; 0.924 sec/batch; 73h:52m:33s remains)
INFO - root - 2017-12-07 19:58:52.819382: step 44560, loss = 21.48, batch loss = 21.40 (8.5 examples/sec; 0.942 sec/batch; 75h:18m:31s remains)
INFO - root - 2017-12-07 19:59:02.252027: step 44570, loss = 21.35, batch loss = 21.27 (8.1 examples/sec; 0.993 sec/batch; 79h:26m:02s remains)
INFO - root - 2017-12-07 19:59:11.598356: step 44580, loss = 21.34, batch loss = 21.26 (8.3 examples/sec; 0.959 sec/batch; 76h:42m:51s remains)
INFO - root - 2017-12-07 19:59:20.957090: step 44590, loss = 21.14, batch loss = 21.05 (8.7 examples/sec; 0.923 sec/batch; 73h:48m:19s remains)
INFO - root - 2017-12-07 19:59:30.217822: step 44600, loss = 21.78, batch loss = 21.69 (8.7 examples/sec; 0.921 sec/batch; 73h:40m:00s remains)
2017-12-07 19:59:31.109138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4690065 -4.4906521 -4.5066118 -4.5148482 -4.5211844 -4.5293317 -4.5346193 -4.5321751 -4.5149584 -4.4935341 -4.4765067 -4.4630961 -4.4545231 -4.4467592 -4.4315562][-4.5312133 -4.5617094 -4.5781522 -4.5834517 -4.5891156 -4.6009517 -4.6090713 -4.6035671 -4.5798922 -4.5509396 -4.5280466 -4.5101218 -4.5008569 -4.4960794 -4.4790645][-4.5850849 -4.6126018 -4.6106987 -4.5917735 -4.5778551 -4.5789485 -4.5843921 -4.5860524 -4.5753226 -4.559093 -4.5405846 -4.5233383 -4.5213585 -4.5291657 -4.5163465][-4.6118107 -4.6226254 -4.5897741 -4.5297918 -4.4756651 -4.4496636 -4.4478908 -4.4710231 -4.501574 -4.5207863 -4.5161538 -4.5022879 -4.5086312 -4.5311918 -4.5276623][-4.5992312 -4.5800333 -4.5092297 -4.4018321 -4.2988887 -4.2397146 -4.2287726 -4.2779145 -4.362864 -4.4321532 -4.4520483 -4.4480953 -4.459702 -4.4905753 -4.4993529][-4.5366521 -4.4767365 -4.3684249 -4.2217932 -4.0813951 -3.9922836 -3.9638181 -4.0274196 -4.161675 -4.2848639 -4.3407311 -4.3559594 -4.3731728 -4.4079723 -4.4311438][-4.4464083 -4.3535171 -4.2234759 -4.06073 -3.9054031 -3.7927105 -3.7339423 -3.7934911 -3.961159 -4.1297054 -4.2181 -4.2495947 -4.2714319 -4.3130045 -4.3546429][-4.3751121 -4.2780666 -4.1493917 -3.985904 -3.8284485 -3.7000835 -3.6152041 -3.6682682 -3.8517666 -4.0391316 -4.1345625 -4.161912 -4.1842632 -4.2386055 -4.2949786][-4.3312297 -4.2562509 -4.1495647 -4.0032625 -3.8629537 -3.7426896 -3.6622982 -3.7196202 -3.8926048 -4.0504246 -4.1051931 -4.0957966 -4.10681 -4.1692243 -4.2304482][-4.3370395 -4.2880116 -4.2126994 -4.1059484 -4.0065207 -3.921195 -3.8710816 -3.9314783 -4.0662189 -4.1575146 -4.143774 -4.080873 -4.0703707 -4.1300678 -4.1852145][-4.3967447 -4.35406 -4.3010178 -4.2425084 -4.1942344 -4.1501093 -4.1288533 -4.1806154 -4.2679491 -4.297051 -4.2277184 -4.1305127 -4.1098886 -4.1668806 -4.2138095][-4.4875946 -4.442049 -4.4005237 -4.3751526 -4.3607965 -4.3456078 -4.3412786 -4.3793421 -4.4293542 -4.4212871 -4.3313279 -4.2333393 -4.2223239 -4.282362 -4.3242679][-4.5581636 -4.5253983 -4.5006008 -4.4941487 -4.4918547 -4.4846435 -4.4827409 -4.5061841 -4.5323663 -4.515738 -4.4451561 -4.3810415 -4.3901329 -4.4461732 -4.4721532][-4.5409994 -4.5331779 -4.5335836 -4.5440159 -4.5502787 -4.5492663 -4.5480356 -4.5572448 -4.5647445 -4.5518579 -4.5181713 -4.4982333 -4.5234303 -4.5639987 -4.5667562][-4.4616027 -4.4678221 -4.4810033 -4.5003667 -4.5169725 -4.5281968 -4.5338955 -4.5352368 -4.5305767 -4.5209203 -4.511076 -4.5137591 -4.5362964 -4.5570159 -4.546268]]...]
INFO - root - 2017-12-07 19:59:40.454812: step 44610, loss = 21.71, batch loss = 21.63 (8.9 examples/sec; 0.898 sec/batch; 71h:46m:58s remains)
INFO - root - 2017-12-07 19:59:49.753787: step 44620, loss = 21.27, batch loss = 21.19 (8.9 examples/sec; 0.898 sec/batch; 71h:49m:03s remains)
INFO - root - 2017-12-07 19:59:59.035781: step 44630, loss = 21.23, batch loss = 21.15 (8.9 examples/sec; 0.896 sec/batch; 71h:39m:58s remains)
INFO - root - 2017-12-07 20:00:08.480544: step 44640, loss = 21.76, batch loss = 21.68 (8.2 examples/sec; 0.975 sec/batch; 77h:56m:24s remains)
INFO - root - 2017-12-07 20:00:17.848318: step 44650, loss = 21.56, batch loss = 21.47 (8.6 examples/sec; 0.932 sec/batch; 74h:29m:39s remains)
INFO - root - 2017-12-07 20:00:27.226875: step 44660, loss = 21.20, batch loss = 21.12 (8.6 examples/sec; 0.927 sec/batch; 74h:06m:50s remains)
INFO - root - 2017-12-07 20:00:36.593778: step 44670, loss = 21.21, batch loss = 21.12 (8.9 examples/sec; 0.895 sec/batch; 71h:35m:40s remains)
INFO - root - 2017-12-07 20:00:46.004703: step 44680, loss = 21.36, batch loss = 21.27 (8.9 examples/sec; 0.903 sec/batch; 72h:09m:25s remains)
INFO - root - 2017-12-07 20:00:55.148055: step 44690, loss = 21.40, batch loss = 21.32 (8.7 examples/sec; 0.918 sec/batch; 73h:22m:22s remains)
INFO - root - 2017-12-07 20:01:04.535567: step 44700, loss = 21.73, batch loss = 21.64 (8.2 examples/sec; 0.974 sec/batch; 77h:54m:13s remains)
2017-12-07 20:01:05.533848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6455574 -4.6155429 -4.5896459 -4.5775084 -4.5686307 -4.5544286 -4.5321364 -4.5182471 -4.5266528 -4.551652 -4.5780358 -4.5992336 -4.6053553 -4.5879588 -4.546411][-4.7079821 -4.6878157 -4.666316 -4.6544051 -4.6369815 -4.6033707 -4.5647507 -4.5488582 -4.5675993 -4.6016941 -4.6321878 -4.6546168 -4.6657648 -4.6606374 -4.6342583][-4.7264171 -4.7258258 -4.7206612 -4.7163115 -4.6772394 -4.5944371 -4.5191674 -4.5041671 -4.5462356 -4.5988164 -4.6338463 -4.6555972 -4.6754618 -4.6955471 -4.7014771][-4.6635294 -4.6913118 -4.7113242 -4.7169123 -4.6417608 -4.4849944 -4.3540697 -4.3399444 -4.4224567 -4.5173206 -4.5812268 -4.6166797 -4.6520166 -4.6980748 -4.7337556][-4.5370364 -4.5884457 -4.6230378 -4.6323481 -4.5253344 -4.2971468 -4.1047516 -4.0802126 -4.1982174 -4.3442163 -4.4562359 -4.5234351 -4.5860758 -4.6595364 -4.7174392][-4.4158397 -4.4678597 -4.4979033 -4.5004473 -4.3711352 -4.0926061 -3.844631 -3.7880561 -3.910531 -4.09344 -4.2493224 -4.3552551 -4.454092 -4.5616159 -4.6468544][-4.3343897 -4.3785577 -4.4064431 -4.4017973 -4.2584639 -3.9605289 -3.6849704 -3.5969479 -3.6961589 -3.8839478 -4.0573492 -4.1810594 -4.3004661 -4.42716 -4.5313506][-4.2865753 -4.329216 -4.3829932 -4.3926497 -4.2577043 -3.9823406 -3.7253437 -3.6295502 -3.692385 -3.8431137 -3.9938381 -4.1108522 -4.2278662 -4.3436847 -4.4388008][-4.2392468 -4.2828746 -4.3850374 -4.4464993 -4.3608279 -4.1497569 -3.9493721 -3.8689828 -3.894443 -3.9791474 -4.0806327 -4.1887627 -4.3073053 -4.40243 -4.4629984][-4.1932859 -4.237237 -4.3791642 -4.5010567 -4.4844956 -4.3567972 -4.2336025 -4.1877356 -4.1877346 -4.1916976 -4.2185936 -4.3161173 -4.4525356 -4.5449972 -4.5831909][-4.15843 -4.2232962 -4.3801055 -4.5265632 -4.5492926 -4.484129 -4.4338531 -4.4345212 -4.43059 -4.3705478 -4.3245316 -4.4039321 -4.5569558 -4.6638041 -4.70533][-4.1606627 -4.252244 -4.4022574 -4.5355015 -4.5664034 -4.52997 -4.5208249 -4.5600467 -4.5768428 -4.5005889 -4.4153891 -4.4719496 -4.6203437 -4.730351 -4.7754178][-4.2386904 -4.3449907 -4.4776254 -4.5849671 -4.6110172 -4.5752583 -4.564435 -4.6116447 -4.6506104 -4.5976076 -4.5153031 -4.5521674 -4.6775379 -4.7753553 -4.8077893][-4.3759274 -4.4740877 -4.5802422 -4.6571145 -4.6726 -4.6319113 -4.6067739 -4.6378922 -4.6737051 -4.6410675 -4.5785327 -4.6032996 -4.6998224 -4.777226 -4.7928357][-4.48784 -4.5694242 -4.6433945 -4.682282 -4.6764388 -4.6317196 -4.597734 -4.6055226 -4.6177292 -4.5875673 -4.545661 -4.56928 -4.64508 -4.7063117 -4.7165742]]...]
INFO - root - 2017-12-07 20:01:14.890237: step 44710, loss = 21.68, batch loss = 21.59 (8.4 examples/sec; 0.951 sec/batch; 76h:02m:24s remains)
INFO - root - 2017-12-07 20:01:24.199320: step 44720, loss = 21.35, batch loss = 21.27 (8.6 examples/sec; 0.933 sec/batch; 74h:34m:32s remains)
INFO - root - 2017-12-07 20:01:33.585401: step 44730, loss = 21.40, batch loss = 21.32 (8.7 examples/sec; 0.920 sec/batch; 73h:32m:15s remains)
INFO - root - 2017-12-07 20:01:42.794909: step 44740, loss = 21.51, batch loss = 21.43 (8.6 examples/sec; 0.932 sec/batch; 74h:31m:49s remains)
INFO - root - 2017-12-07 20:01:52.204277: step 44750, loss = 21.45, batch loss = 21.37 (8.6 examples/sec; 0.932 sec/batch; 74h:28m:02s remains)
INFO - root - 2017-12-07 20:02:01.727923: step 44760, loss = 21.62, batch loss = 21.54 (8.8 examples/sec; 0.912 sec/batch; 72h:54m:04s remains)
INFO - root - 2017-12-07 20:02:11.134556: step 44770, loss = 21.00, batch loss = 20.92 (8.6 examples/sec; 0.933 sec/batch; 74h:35m:56s remains)
INFO - root - 2017-12-07 20:02:20.456296: step 44780, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.952 sec/batch; 76h:06m:38s remains)
INFO - root - 2017-12-07 20:02:29.562114: step 44790, loss = 21.38, batch loss = 21.30 (8.2 examples/sec; 0.980 sec/batch; 78h:20m:02s remains)
INFO - root - 2017-12-07 20:02:38.916117: step 44800, loss = 21.49, batch loss = 21.41 (8.3 examples/sec; 0.960 sec/batch; 76h:43m:54s remains)
2017-12-07 20:02:39.907337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3026323 -4.3600597 -4.4105129 -4.4411488 -4.4559588 -4.4607639 -4.4558225 -4.446887 -4.4423661 -4.4432135 -4.4416518 -4.4256516 -4.3955221 -4.3632751 -4.3393645][-4.3676968 -4.4656835 -4.5454321 -4.5909395 -4.61056 -4.6093845 -4.5868139 -4.5564723 -4.5416117 -4.5501342 -4.5591722 -4.5421944 -4.4970307 -4.4452796 -4.4067683][-4.4418011 -4.570437 -4.6599631 -4.6958237 -4.6958847 -4.6657972 -4.6067319 -4.545908 -4.5261426 -4.5593328 -4.5967526 -4.5929251 -4.5429659 -4.4788566 -4.4317555][-4.5169969 -4.643456 -4.6994991 -4.6812992 -4.6257944 -4.5457916 -4.44753 -4.367156 -4.3607268 -4.4385781 -4.5220795 -4.5499625 -4.5139351 -4.4518747 -4.4079332][-4.5753365 -4.6637106 -4.648315 -4.5451355 -4.4140744 -4.2833543 -4.1659489 -4.091495 -4.114574 -4.2399292 -4.3709679 -4.4412661 -4.4358497 -4.3899641 -4.3588643][-4.5911789 -4.621067 -4.5260711 -4.33925 -4.1370053 -3.9659393 -3.848043 -3.7978258 -3.8590581 -4.02603 -4.2002373 -4.3187714 -4.3560185 -4.3371148 -4.321579][-4.5540862 -4.5396 -4.39825 -4.1640034 -3.9123633 -3.7077796 -3.589411 -3.5665586 -3.669158 -3.8701265 -4.0773735 -4.2358408 -4.3134089 -4.3233662 -4.3178387][-4.4870605 -4.4709444 -4.3409829 -4.1108937 -3.8427238 -3.6178386 -3.4962013 -3.4986374 -3.636796 -3.8506064 -4.060564 -4.2254558 -4.3184986 -4.3452387 -4.3398333][-4.42408 -4.4374957 -4.3542805 -4.1636753 -3.9175549 -3.7071028 -3.602447 -3.6360157 -3.7902656 -3.9786959 -4.1451359 -4.2697983 -4.3435431 -4.3678789 -4.3559241][-4.3874311 -4.432672 -4.3939285 -4.2521458 -4.0568624 -3.8966787 -3.8321867 -3.8913124 -4.031003 -4.1615305 -4.2553921 -4.3166342 -4.3532844 -4.3647542 -4.3490019][-4.3774948 -4.4385748 -4.4276071 -4.3309526 -4.1996078 -4.1036091 -4.0783234 -4.1385155 -4.2352505 -4.2994533 -4.3274164 -4.3352628 -4.3370972 -4.3335767 -4.3195338][-4.3746634 -4.4349518 -4.4357953 -4.375752 -4.3034997 -4.26351 -4.2662292 -4.3117232 -4.3620257 -4.3773737 -4.3660955 -4.3449125 -4.324604 -4.3089495 -4.2967067][-4.366313 -4.4192634 -4.4251242 -4.39123 -4.3590961 -4.3553557 -4.3761334 -4.4118772 -4.4363956 -4.4307723 -4.4062676 -4.3754473 -4.3454242 -4.3215456 -4.3058228][-4.348536 -4.3919368 -4.4014177 -4.3861027 -4.3765936 -4.3893733 -4.4159636 -4.4462233 -4.4632397 -4.457427 -4.4376526 -4.4106331 -4.3801689 -4.351368 -4.3280463][-4.3233976 -4.3525534 -4.360323 -4.3526511 -4.3499537 -4.3620625 -4.3822417 -4.4041777 -4.4180465 -4.4192805 -4.4116015 -4.3963494 -4.3742943 -4.3485136 -4.3230267]]...]
INFO - root - 2017-12-07 20:02:49.221299: step 44810, loss = 21.14, batch loss = 21.06 (8.4 examples/sec; 0.948 sec/batch; 75h:43m:09s remains)
INFO - root - 2017-12-07 20:02:58.559159: step 44820, loss = 21.35, batch loss = 21.27 (8.1 examples/sec; 0.989 sec/batch; 79h:03m:16s remains)
INFO - root - 2017-12-07 20:03:07.908667: step 44830, loss = 21.47, batch loss = 21.39 (7.8 examples/sec; 1.028 sec/batch; 82h:09m:56s remains)
INFO - root - 2017-12-07 20:03:17.168938: step 44840, loss = 21.63, batch loss = 21.54 (8.5 examples/sec; 0.939 sec/batch; 75h:01m:21s remains)
INFO - root - 2017-12-07 20:03:26.591506: step 44850, loss = 21.87, batch loss = 21.79 (8.7 examples/sec; 0.924 sec/batch; 73h:52m:08s remains)
INFO - root - 2017-12-07 20:03:35.844966: step 44860, loss = 21.10, batch loss = 21.01 (8.7 examples/sec; 0.925 sec/batch; 73h:52m:36s remains)
INFO - root - 2017-12-07 20:03:45.086323: step 44870, loss = 21.39, batch loss = 21.31 (8.2 examples/sec; 0.974 sec/batch; 77h:50m:07s remains)
INFO - root - 2017-12-07 20:03:54.402145: step 44880, loss = 21.63, batch loss = 21.54 (9.0 examples/sec; 0.891 sec/batch; 71h:09m:15s remains)
INFO - root - 2017-12-07 20:04:03.494682: step 44890, loss = 21.57, batch loss = 21.48 (8.6 examples/sec; 0.927 sec/batch; 74h:02m:38s remains)
INFO - root - 2017-12-07 20:04:12.724936: step 44900, loss = 21.35, batch loss = 21.26 (8.4 examples/sec; 0.957 sec/batch; 76h:26m:48s remains)
2017-12-07 20:04:13.757280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4428487 -4.4444556 -4.4717841 -4.51609 -4.5807939 -4.662745 -4.7395883 -4.771687 -4.7484035 -4.7056189 -4.6776438 -4.6868739 -4.7004266 -4.6763592 -4.6295609][-4.4056582 -4.4200459 -4.4606962 -4.5170736 -4.5905709 -4.6782594 -4.75382 -4.7813044 -4.7574134 -4.71271 -4.6840248 -4.6996078 -4.7205076 -4.6947 -4.6420856][-4.3575974 -4.4061103 -4.4652772 -4.5211415 -4.5821047 -4.6537628 -4.7128334 -4.7321134 -4.713129 -4.6740918 -4.6463571 -4.6652565 -4.6888471 -4.6609797 -4.6045957][-4.3293829 -4.4161706 -4.493422 -4.5409417 -4.5732512 -4.6039586 -4.6144466 -4.5993938 -4.5816994 -4.560679 -4.5470095 -4.5781064 -4.6094065 -4.5899405 -4.5466375][-4.2812123 -4.3869338 -4.4837475 -4.5325241 -4.5308061 -4.4870157 -4.4029646 -4.3198953 -4.3005347 -4.3197241 -4.3521447 -4.4260373 -4.4862537 -4.4893904 -4.4717326][-4.1733508 -4.2947631 -4.4232378 -4.4884048 -4.4465742 -4.29554 -4.0826478 -3.9218338 -3.9172387 -4.0048308 -4.1134267 -4.2587523 -4.3715153 -4.4039488 -4.4021425][-4.0227833 -4.174202 -4.3402944 -4.4167819 -4.3285947 -4.0766177 -3.7527328 -3.5380392 -3.5660014 -3.7249098 -3.9084682 -4.1247277 -4.2886219 -4.3412118 -4.3331151][-3.9335496 -4.0981493 -4.276896 -4.34502 -4.2299938 -3.9475548 -3.5999241 -3.387342 -3.438864 -3.623455 -3.839479 -4.0900435 -4.2710495 -4.3112407 -4.2705517][-3.9949594 -4.1436582 -4.3001938 -4.3516974 -4.2504692 -4.0252581 -3.7510538 -3.5841532 -3.6197181 -3.7643571 -3.9553366 -4.1857634 -4.3387318 -4.3359895 -4.2479095][-4.1525593 -4.2639751 -4.3746977 -4.403173 -4.3355207 -4.2027669 -4.0342493 -3.9175794 -3.9276338 -4.0263381 -4.1760044 -4.35387 -4.4514842 -4.4026704 -4.2789516][-4.3325224 -4.3981385 -4.4529929 -4.4484591 -4.3966694 -4.3260856 -4.2394762 -4.1727705 -4.1871028 -4.2740617 -4.3965 -4.5183387 -4.5590172 -4.4826784 -4.3512216][-4.4823403 -4.5081449 -4.5165882 -4.4818969 -4.4210477 -4.361454 -4.3075676 -4.2751904 -4.3094811 -4.4034367 -4.5093117 -4.5858169 -4.5915279 -4.5171337 -4.4088593][-4.5396123 -4.5432906 -4.5281715 -4.47903 -4.4056587 -4.3319516 -4.2707787 -4.2429609 -4.2859855 -4.3865647 -4.4858537 -4.5476446 -4.5528603 -4.5014849 -4.4305935][-4.5059872 -4.5081859 -4.4949894 -4.4569192 -4.3944798 -4.3262892 -4.2633848 -4.22912 -4.2611313 -4.3466234 -4.4339614 -4.4919786 -4.5044909 -4.4708028 -4.4274158][-4.43982 -4.4502411 -4.4531379 -4.4440112 -4.4160018 -4.382 -4.3416486 -4.3094387 -4.317347 -4.3631988 -4.4158044 -4.4533348 -4.4589972 -4.4294877 -4.3993206]]...]
INFO - root - 2017-12-07 20:04:23.042118: step 44910, loss = 21.42, batch loss = 21.34 (8.3 examples/sec; 0.960 sec/batch; 76h:41m:44s remains)
INFO - root - 2017-12-07 20:04:32.478285: step 44920, loss = 21.19, batch loss = 21.11 (8.9 examples/sec; 0.896 sec/batch; 71h:33m:58s remains)
INFO - root - 2017-12-07 20:04:41.878160: step 44930, loss = 21.60, batch loss = 21.52 (8.7 examples/sec; 0.921 sec/batch; 73h:35m:53s remains)
INFO - root - 2017-12-07 20:04:51.256151: step 44940, loss = 21.29, batch loss = 21.21 (8.1 examples/sec; 0.982 sec/batch; 78h:26m:45s remains)
INFO - root - 2017-12-07 20:05:00.566456: step 44950, loss = 21.98, batch loss = 21.90 (8.9 examples/sec; 0.898 sec/batch; 71h:43m:51s remains)
INFO - root - 2017-12-07 20:05:10.086395: step 44960, loss = 21.33, batch loss = 21.25 (8.7 examples/sec; 0.924 sec/batch; 73h:46m:51s remains)
INFO - root - 2017-12-07 20:05:19.663474: step 44970, loss = 21.19, batch loss = 21.11 (8.5 examples/sec; 0.938 sec/batch; 74h:55m:06s remains)
INFO - root - 2017-12-07 20:05:29.093651: step 44980, loss = 21.68, batch loss = 21.60 (8.8 examples/sec; 0.913 sec/batch; 72h:54m:13s remains)
INFO - root - 2017-12-07 20:05:38.385160: step 44990, loss = 21.86, batch loss = 21.78 (9.2 examples/sec; 0.872 sec/batch; 69h:38m:58s remains)
INFO - root - 2017-12-07 20:05:47.665519: step 45000, loss = 21.24, batch loss = 21.16 (8.5 examples/sec; 0.943 sec/batch; 75h:18m:09s remains)
2017-12-07 20:05:48.600197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4008889 -4.3680673 -4.3836637 -4.4374838 -4.4946265 -4.5527773 -4.6289921 -4.6680932 -4.649724 -4.6139178 -4.5808082 -4.5799804 -4.6080852 -4.64592 -4.6638732][-4.3747315 -4.3246412 -4.3305349 -4.3803959 -4.4383097 -4.5008917 -4.5784354 -4.6018524 -4.5803633 -4.57148 -4.5530396 -4.5476804 -4.5674753 -4.5918188 -4.597434][-4.3562307 -4.2851262 -4.26362 -4.2882929 -4.3268743 -4.3775258 -4.4425406 -4.4589396 -4.4534345 -4.4885521 -4.5027428 -4.5025463 -4.5195346 -4.53136 -4.5247593][-4.344645 -4.2681293 -4.2289829 -4.2314525 -4.2479897 -4.268681 -4.3004146 -4.3045082 -4.30805 -4.3653326 -4.4027371 -4.4096913 -4.4257455 -4.4331527 -4.4279685][-4.344337 -4.2839475 -4.2363138 -4.2095284 -4.1928787 -4.1669326 -4.15322 -4.1393623 -4.1390419 -4.191433 -4.2378874 -4.2538805 -4.2663445 -4.265101 -4.2631292][-4.3664856 -4.3326 -4.2664409 -4.1836061 -4.1098881 -4.031642 -3.9789796 -3.9570823 -3.9616337 -4.0126481 -4.0679255 -4.099484 -4.1104245 -4.0957818 -4.0937719][-4.3864908 -4.3756475 -4.2903109 -4.1520858 -4.0210605 -3.9007814 -3.8238378 -3.8073609 -3.8342419 -3.9115975 -3.9989457 -4.0556364 -4.0649562 -4.0312581 -4.0150576][-4.4006882 -4.4062405 -4.3221116 -4.1702228 -4.0175543 -3.886503 -3.8128994 -3.8043332 -3.8453989 -3.9411042 -4.0556326 -4.1287017 -4.1301432 -4.0797048 -4.0488982][-4.43145 -4.4492135 -4.3941875 -4.2778864 -4.1430554 -4.0221391 -3.9568126 -3.9417357 -3.97154 -4.05855 -4.1771812 -4.2550507 -4.247314 -4.1974726 -4.1692123][-4.4629197 -4.4865937 -4.4708323 -4.4147758 -4.3249526 -4.2266059 -4.1654716 -4.1385379 -4.1532321 -4.223279 -4.32315 -4.3787222 -4.3511362 -4.3021927 -4.2847643][-4.4724178 -4.4939289 -4.5099359 -4.5131712 -4.4859605 -4.4307179 -4.3829961 -4.3539133 -4.3569455 -4.3972645 -4.4504671 -4.458919 -4.4059858 -4.36322 -4.3688531][-4.4472575 -4.4527392 -4.4721127 -4.5045342 -4.5272779 -4.5221972 -4.5045109 -4.4900007 -4.4865994 -4.4935622 -4.499877 -4.476737 -4.4222054 -4.3974371 -4.429172][-4.4240813 -4.4059825 -4.4058471 -4.4333234 -4.4716482 -4.4983993 -4.511024 -4.513381 -4.5008278 -4.4748182 -4.4494805 -4.4187713 -4.3823614 -4.3786745 -4.426825][-4.4339175 -4.4034843 -4.3906436 -4.4031491 -4.433197 -4.4639339 -4.4846621 -4.4889808 -4.4655542 -4.4160223 -4.3678355 -4.3366494 -4.3223739 -4.3370123 -4.3894963][-4.4442916 -4.4213023 -4.4095316 -4.4081469 -4.4190087 -4.4357977 -4.4479508 -4.4481344 -4.4277129 -4.3816724 -4.3308911 -4.3013535 -4.3014359 -4.3264132 -4.37021]]...]
INFO - root - 2017-12-07 20:05:58.039847: step 45010, loss = 21.59, batch loss = 21.51 (8.3 examples/sec; 0.958 sec/batch; 76h:30m:43s remains)
INFO - root - 2017-12-07 20:06:07.508305: step 45020, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.931 sec/batch; 74h:22m:13s remains)
INFO - root - 2017-12-07 20:06:16.884180: step 45030, loss = 21.08, batch loss = 21.00 (8.4 examples/sec; 0.951 sec/batch; 75h:55m:41s remains)
INFO - root - 2017-12-07 20:06:26.216047: step 45040, loss = 21.49, batch loss = 21.41 (9.0 examples/sec; 0.893 sec/batch; 71h:16m:53s remains)
INFO - root - 2017-12-07 20:06:35.533258: step 45050, loss = 21.62, batch loss = 21.54 (8.2 examples/sec; 0.979 sec/batch; 78h:10m:30s remains)
INFO - root - 2017-12-07 20:06:44.901802: step 45060, loss = 21.42, batch loss = 21.34 (8.2 examples/sec; 0.975 sec/batch; 77h:49m:00s remains)
INFO - root - 2017-12-07 20:06:54.283942: step 45070, loss = 21.23, batch loss = 21.14 (8.6 examples/sec; 0.930 sec/batch; 74h:16m:33s remains)
INFO - root - 2017-12-07 20:07:03.760436: step 45080, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.946 sec/batch; 75h:32m:36s remains)
INFO - root - 2017-12-07 20:07:13.108863: step 45090, loss = 22.07, batch loss = 21.99 (8.7 examples/sec; 0.915 sec/batch; 73h:04m:45s remains)
INFO - root - 2017-12-07 20:07:22.323070: step 45100, loss = 21.43, batch loss = 21.34 (9.0 examples/sec; 0.891 sec/batch; 71h:08m:00s remains)
2017-12-07 20:07:23.325231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5870042 -4.5531726 -4.4805579 -4.3841486 -4.3762469 -4.4796224 -4.5691476 -4.6020608 -4.595993 -4.5806737 -4.5714464 -4.5714588 -4.5786519 -4.5735116 -4.5598216][-4.6769996 -4.6283317 -4.5313711 -4.4063339 -4.3780336 -4.4732127 -4.5617671 -4.5815029 -4.5530291 -4.5264397 -4.5184417 -4.525773 -4.5385027 -4.533906 -4.5144362][-4.8113337 -4.7794156 -4.679987 -4.5312648 -4.458365 -4.4970112 -4.54395 -4.536006 -4.4928184 -4.4682364 -4.4725313 -4.4937773 -4.5144162 -4.5096416 -4.4791746][-4.9103246 -4.907753 -4.8210769 -4.6568732 -4.5330892 -4.4903746 -4.4716768 -4.433619 -4.3976455 -4.4029222 -4.4394407 -4.4868035 -4.5190835 -4.5126414 -4.4608974][-4.8858571 -4.8919234 -4.8121295 -4.6387725 -4.4793396 -4.371789 -4.296452 -4.2398033 -4.2266 -4.2809582 -4.3710093 -4.4634628 -4.5215578 -4.5174584 -4.4389553][-4.7775736 -4.771019 -4.6765366 -4.4809837 -4.2961407 -4.1490612 -4.0343122 -3.9610717 -3.9655056 -4.0688171 -4.2203403 -4.3717694 -4.4735165 -4.4848409 -4.3928542][-4.6567979 -4.6302238 -4.5006423 -4.2671781 -4.0575089 -3.8817568 -3.7349467 -3.6414816 -3.6560278 -3.7950335 -3.9942052 -4.1994252 -4.3531041 -4.4028792 -4.337986][-4.5987463 -4.5599074 -4.3950582 -4.129261 -3.9040236 -3.714787 -3.5532985 -3.4506977 -3.4766448 -3.6365037 -3.8560596 -4.0869064 -4.2677164 -4.3461108 -4.3261971][-4.6113253 -4.5718246 -4.3939881 -4.1271472 -3.9221454 -3.7639291 -3.62919 -3.544723 -3.5868757 -3.7518091 -3.9674325 -4.1882415 -4.3472137 -4.4095287 -4.399982][-4.615407 -4.5677924 -4.3963327 -4.1759453 -4.0516582 -3.9873309 -3.9227397 -3.872333 -3.9105985 -4.0457039 -4.2293773 -4.4139309 -4.5298991 -4.5598817 -4.5369349][-4.5904546 -4.5270467 -4.3772521 -4.2337723 -4.2179375 -4.2625084 -4.2726116 -4.2517686 -4.2682657 -4.3457689 -4.4647808 -4.5868511 -4.6531672 -4.6524391 -4.6119866][-4.5362473 -4.459126 -4.3367987 -4.2673922 -4.3383245 -4.46099 -4.5239258 -4.5257545 -4.52182 -4.5453038 -4.5964646 -4.653563 -4.6771383 -4.6512589 -4.5930562][-4.4506688 -4.3588057 -4.2519212 -4.2232366 -4.3297033 -4.4841208 -4.5813432 -4.6074133 -4.6009445 -4.6026096 -4.6196318 -4.6395726 -4.6372461 -4.5954666 -4.5260816][-4.3510618 -4.2434473 -4.1409354 -4.1240311 -4.2261066 -4.3737993 -4.4819775 -4.5260978 -4.5324035 -4.5402575 -4.5545688 -4.5642905 -4.5518913 -4.5031905 -4.4353151][-4.2860403 -4.1734304 -4.076261 -4.0612149 -4.145648 -4.2694039 -4.3671227 -4.41188 -4.4249849 -4.4377756 -4.4524169 -4.4610271 -4.4488974 -4.4077296 -4.355031]]...]
INFO - root - 2017-12-07 20:07:32.586006: step 45110, loss = 21.69, batch loss = 21.60 (8.9 examples/sec; 0.898 sec/batch; 71h:41m:03s remains)
INFO - root - 2017-12-07 20:07:41.998280: step 45120, loss = 21.38, batch loss = 21.30 (9.2 examples/sec; 0.868 sec/batch; 69h:19m:39s remains)
INFO - root - 2017-12-07 20:07:51.337612: step 45130, loss = 21.56, batch loss = 21.47 (8.7 examples/sec; 0.916 sec/batch; 73h:05m:31s remains)
INFO - root - 2017-12-07 20:08:00.638437: step 45140, loss = 21.57, batch loss = 21.49 (8.8 examples/sec; 0.904 sec/batch; 72h:10m:33s remains)
INFO - root - 2017-12-07 20:08:10.058625: step 45150, loss = 21.48, batch loss = 21.40 (8.9 examples/sec; 0.898 sec/batch; 71h:39m:00s remains)
INFO - root - 2017-12-07 20:08:19.510109: step 45160, loss = 21.53, batch loss = 21.45 (8.5 examples/sec; 0.937 sec/batch; 74h:45m:21s remains)
INFO - root - 2017-12-07 20:08:28.839617: step 45170, loss = 21.54, batch loss = 21.46 (8.3 examples/sec; 0.966 sec/batch; 77h:03m:41s remains)
INFO - root - 2017-12-07 20:08:38.172225: step 45180, loss = 21.21, batch loss = 21.13 (8.1 examples/sec; 0.993 sec/batch; 79h:14m:00s remains)
INFO - root - 2017-12-07 20:08:47.699292: step 45190, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.973 sec/batch; 77h:38m:30s remains)
INFO - root - 2017-12-07 20:08:56.985905: step 45200, loss = 21.39, batch loss = 21.31 (8.3 examples/sec; 0.960 sec/batch; 76h:36m:26s remains)
2017-12-07 20:08:57.948414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4848471 -4.4220567 -4.3700829 -4.3380084 -4.3391161 -4.299305 -4.2592506 -4.2630672 -4.2964826 -4.3544593 -4.4348774 -4.5375471 -4.62086 -4.6164536 -4.5310168][-4.5226779 -4.4808946 -4.4389639 -4.4039612 -4.386622 -4.3206086 -4.2476506 -4.2366672 -4.2763772 -4.3583412 -4.4658618 -4.5780368 -4.6615949 -4.6522551 -4.5563693][-4.5042024 -4.4978075 -4.4884825 -4.4678388 -4.4385467 -4.3512716 -4.2435412 -4.2039509 -4.2361155 -4.3281755 -4.4602442 -4.5894742 -4.6777849 -4.6704717 -4.5708156][-4.4152689 -4.4470205 -4.4713807 -4.46733 -4.4392638 -4.3534451 -4.2300773 -4.1711321 -4.1937108 -4.2821903 -4.429646 -4.57713 -4.6751161 -4.6780529 -4.5827723][-4.2927051 -4.3473177 -4.3836255 -4.3877859 -4.3721189 -4.3077927 -4.1880817 -4.1244721 -4.1415086 -4.2223268 -4.3796387 -4.54974 -4.662797 -4.6810908 -4.5948977][-4.1787825 -4.2272844 -4.2464318 -4.2399454 -4.234 -4.1875234 -4.0761724 -4.0252318 -4.0580215 -4.1387234 -4.3024392 -4.4976044 -4.6342454 -4.6729827 -4.5988188][-4.1242356 -4.1385016 -4.1200118 -4.0889649 -4.0834174 -4.0395241 -3.9281325 -3.887404 -3.9443502 -4.03255 -4.2006793 -4.4200497 -4.5842471 -4.6460166 -4.5868087][-4.1549706 -4.1321068 -4.0765781 -4.0200057 -3.9984765 -3.9350314 -3.8045826 -3.7556844 -3.8214784 -3.9128878 -4.0845871 -4.3244128 -4.5139661 -4.6004667 -4.5591865][-4.2469883 -4.1968031 -4.1206245 -4.04794 -4.0019946 -3.9118791 -3.762521 -3.69139 -3.7366998 -3.8193583 -3.99452 -4.248805 -4.4563546 -4.5594049 -4.5304747][-4.3689661 -4.3015604 -4.2223516 -4.1521425 -4.0912914 -3.9862337 -3.8385296 -3.7523923 -3.7668478 -3.8408191 -4.0145249 -4.2622991 -4.4627595 -4.5556741 -4.5197024][-4.4811954 -4.40984 -4.3420119 -4.2837205 -4.2157478 -4.10296 -3.9721093 -3.8985353 -3.9143231 -4.0003996 -4.1642284 -4.3738279 -4.5313745 -4.5849752 -4.5244894][-4.5607042 -4.4976606 -4.4479284 -4.4008846 -4.3261728 -4.2100739 -4.1039591 -4.0677071 -4.1172667 -4.2310247 -4.3811054 -4.534997 -4.6282978 -4.6253648 -4.5322828][-4.6169772 -4.5805306 -4.5531645 -4.518455 -4.4437256 -4.3346872 -4.255033 -4.2530556 -4.3320265 -4.4595556 -4.5876217 -4.6873388 -4.7196336 -4.6632614 -4.540647][-4.6700521 -4.6660309 -4.6639376 -4.641108 -4.5725708 -4.4791679 -4.4208989 -4.4348445 -4.5153737 -4.6246834 -4.7145867 -4.7689776 -4.7622409 -4.6757035 -4.5404453][-4.7193928 -4.7347951 -4.7454181 -4.7297463 -4.6735845 -4.60592 -4.5713177 -4.59333 -4.6597061 -4.7337127 -4.7790813 -4.7929115 -4.7598705 -4.6604004 -4.5257187]]...]
INFO - root - 2017-12-07 20:09:07.249402: step 45210, loss = 21.42, batch loss = 21.33 (8.5 examples/sec; 0.945 sec/batch; 75h:24m:30s remains)
INFO - root - 2017-12-07 20:09:16.580859: step 45220, loss = 21.15, batch loss = 21.07 (8.3 examples/sec; 0.968 sec/batch; 77h:13m:22s remains)
INFO - root - 2017-12-07 20:09:25.906138: step 45230, loss = 21.79, batch loss = 21.71 (8.5 examples/sec; 0.944 sec/batch; 75h:21m:44s remains)
INFO - root - 2017-12-07 20:09:35.116095: step 45240, loss = 21.34, batch loss = 21.26 (8.9 examples/sec; 0.902 sec/batch; 71h:59m:08s remains)
INFO - root - 2017-12-07 20:09:44.496209: step 45250, loss = 21.60, batch loss = 21.52 (8.4 examples/sec; 0.950 sec/batch; 75h:48m:18s remains)
INFO - root - 2017-12-07 20:09:53.838542: step 45260, loss = 21.62, batch loss = 21.54 (8.0 examples/sec; 0.998 sec/batch; 79h:38m:43s remains)
INFO - root - 2017-12-07 20:10:03.181040: step 45270, loss = 21.57, batch loss = 21.48 (8.8 examples/sec; 0.905 sec/batch; 72h:11m:19s remains)
INFO - root - 2017-12-07 20:10:12.454498: step 45280, loss = 21.43, batch loss = 21.35 (9.3 examples/sec; 0.859 sec/batch; 68h:30m:34s remains)
INFO - root - 2017-12-07 20:10:21.758327: step 45290, loss = 21.21, batch loss = 21.13 (8.7 examples/sec; 0.921 sec/batch; 73h:27m:40s remains)
INFO - root - 2017-12-07 20:10:31.096298: step 45300, loss = 21.27, batch loss = 21.19 (8.5 examples/sec; 0.942 sec/batch; 75h:08m:40s remains)
2017-12-07 20:10:32.064329: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3479633 -4.3508334 -4.383709 -4.413455 -4.4099722 -4.3755312 -4.3689609 -4.4316697 -4.5645304 -4.6976585 -4.7657962 -4.76328 -4.6993675 -4.6049004 -4.5077858][-4.2987547 -4.3167953 -4.3746505 -4.4184961 -4.4090257 -4.3571639 -4.3490133 -4.4340534 -4.6030679 -4.7614675 -4.8384209 -4.8349028 -4.7584419 -4.6444182 -4.5285039][-4.2661366 -4.3134413 -4.3981695 -4.4485922 -4.4247627 -4.3465056 -4.3196063 -4.4027977 -4.5820465 -4.7515717 -4.8413081 -4.8501863 -4.7801509 -4.6626487 -4.5385065][-4.2930222 -4.3608875 -4.4517617 -4.487905 -4.4361367 -4.3252096 -4.2652607 -4.3229527 -4.4897313 -4.6603932 -4.7691383 -4.8080812 -4.7697215 -4.6701946 -4.5471315][-4.3408308 -4.3945465 -4.4602942 -4.4672275 -4.389298 -4.2545476 -4.1627941 -4.1823215 -4.3181458 -4.4806876 -4.615664 -4.71016 -4.7306867 -4.6691818 -4.5545526][-4.3556213 -4.3804603 -4.4144645 -4.3938289 -4.2973118 -4.152699 -4.0345383 -4.01195 -4.1124787 -4.2688766 -4.4383197 -4.5911961 -4.6702576 -4.6495576 -4.5506253][-4.297492 -4.3147149 -4.3308821 -4.2940769 -4.1858892 -4.0408912 -3.9121282 -3.8703527 -3.9595029 -4.1224155 -4.3204951 -4.5063562 -4.6129584 -4.6159334 -4.5351925][-4.2183404 -4.2528152 -4.2734895 -4.226449 -4.1040173 -3.954648 -3.8218136 -3.7789631 -3.8723476 -4.042984 -4.2514591 -4.44648 -4.5580983 -4.5739341 -4.5134029][-4.2066374 -4.2637391 -4.2854462 -4.2248697 -4.0845714 -3.922291 -3.7829108 -3.7369251 -3.8296502 -4.0016785 -4.2104082 -4.4014015 -4.50881 -4.53406 -4.4927869][-4.2669969 -4.3219509 -4.3359408 -4.2756324 -4.1454139 -3.9864182 -3.8494005 -3.8056154 -3.9017968 -4.0726037 -4.2604914 -4.4182396 -4.4973922 -4.5159168 -4.4829307][-4.322957 -4.3776007 -4.403018 -4.3750091 -4.2833743 -4.1460829 -4.0269132 -4.0082273 -4.1187944 -4.2783632 -4.4175935 -4.5135965 -4.5450191 -4.5359478 -4.4927711][-4.38131 -4.4395146 -4.4882832 -4.5004911 -4.4524455 -4.3457084 -4.250999 -4.2572279 -4.3665204 -4.4938617 -4.5777059 -4.6204872 -4.6165891 -4.5777936 -4.5141468][-4.4672995 -4.5235443 -4.5827308 -4.61912 -4.6057472 -4.5363207 -4.4733253 -4.4894466 -4.574564 -4.65306 -4.6904421 -4.6992111 -4.6748362 -4.6143389 -4.5314474][-4.5509253 -4.6015353 -4.6555333 -4.6943479 -4.6977687 -4.6593819 -4.6203804 -4.63126 -4.6817837 -4.7224393 -4.7374344 -4.7319965 -4.6978364 -4.6267762 -4.5331044][-4.6411133 -4.6792822 -4.7168179 -4.7443271 -4.7473779 -4.7206049 -4.6853986 -4.6771879 -4.6967134 -4.7152834 -4.7232919 -4.715107 -4.6788864 -4.6061654 -4.5126815]]...]
INFO - root - 2017-12-07 20:10:41.476303: step 45310, loss = 21.64, batch loss = 21.56 (8.3 examples/sec; 0.966 sec/batch; 77h:04m:13s remains)
INFO - root - 2017-12-07 20:10:50.847951: step 45320, loss = 21.75, batch loss = 21.67 (8.7 examples/sec; 0.921 sec/batch; 73h:29m:13s remains)
INFO - root - 2017-12-07 20:11:00.226615: step 45330, loss = 21.30, batch loss = 21.22 (8.3 examples/sec; 0.960 sec/batch; 76h:33m:14s remains)
INFO - root - 2017-12-07 20:11:09.584455: step 45340, loss = 21.37, batch loss = 21.29 (9.1 examples/sec; 0.884 sec/batch; 70h:29m:55s remains)
INFO - root - 2017-12-07 20:11:18.815997: step 45350, loss = 21.23, batch loss = 21.15 (9.6 examples/sec; 0.831 sec/batch; 66h:18m:28s remains)
INFO - root - 2017-12-07 20:11:28.169870: step 45360, loss = 21.38, batch loss = 21.30 (8.9 examples/sec; 0.894 sec/batch; 71h:18m:19s remains)
INFO - root - 2017-12-07 20:11:37.521270: step 45370, loss = 22.08, batch loss = 22.00 (8.2 examples/sec; 0.973 sec/batch; 77h:36m:46s remains)
INFO - root - 2017-12-07 20:11:46.815155: step 45380, loss = 21.68, batch loss = 21.60 (9.3 examples/sec; 0.863 sec/batch; 68h:51m:29s remains)
INFO - root - 2017-12-07 20:11:56.060603: step 45390, loss = 21.25, batch loss = 21.17 (8.2 examples/sec; 0.971 sec/batch; 77h:24m:07s remains)
INFO - root - 2017-12-07 20:12:05.336354: step 45400, loss = 21.38, batch loss = 21.30 (8.4 examples/sec; 0.951 sec/batch; 75h:52m:45s remains)
2017-12-07 20:12:06.292918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3093705 -4.2865529 -4.292429 -4.3253217 -4.355958 -4.3708816 -4.3757863 -4.3890781 -4.37844 -4.3170509 -4.2324462 -4.1711888 -4.185235 -4.2358537 -4.261672][-4.3601346 -4.3457251 -4.3410726 -4.3526807 -4.3699646 -4.3911643 -4.4149952 -4.4433255 -4.4439645 -4.3868961 -4.3016572 -4.2395215 -4.247642 -4.2881222 -4.2989626][-4.4420748 -4.4397016 -4.4231997 -4.4091244 -4.4062414 -4.4267535 -4.46784 -4.5130777 -4.5291681 -4.4829841 -4.4019876 -4.3378673 -4.3282356 -4.3397603 -4.3215065][-4.5091681 -4.511198 -4.4844327 -4.45105 -4.426928 -4.436182 -4.4826684 -4.5368257 -4.5643091 -4.5333366 -4.4685841 -4.41434 -4.3904757 -4.3684783 -4.3226624][-4.5217595 -4.516572 -4.4752836 -4.4186287 -4.3648596 -4.3510609 -4.3957367 -4.4524655 -4.4837074 -4.46457 -4.4234543 -4.4000745 -4.3902164 -4.36016 -4.3090343][-4.43995 -4.4192691 -4.3581657 -4.2698827 -4.1784048 -4.1422987 -4.1947083 -4.2647681 -4.2966409 -4.2798891 -4.2556543 -4.2748628 -4.3100977 -4.3038507 -4.2706838][-4.2982559 -4.2736459 -4.2017112 -4.0816479 -3.9500129 -3.8955369 -3.9663413 -4.0611362 -4.0979815 -4.0743351 -4.0566888 -4.1146784 -4.2025681 -4.2374544 -4.2372627][-4.1885524 -4.1740909 -4.1038623 -3.960644 -3.7974312 -3.7307277 -3.8164549 -3.9341912 -3.9835854 -3.9581208 -3.9428873 -4.0174294 -4.1319833 -4.1996341 -4.2302394][-4.1463661 -4.1416945 -4.0807033 -3.9390137 -3.774554 -3.7083349 -3.7913232 -3.9137611 -3.9742804 -3.9592938 -3.9503188 -4.0198503 -4.1312833 -4.2143912 -4.2674971][-4.1695948 -4.171567 -4.1235089 -4.0088849 -3.8759017 -3.8225298 -3.8834605 -3.9824855 -4.0411353 -4.0430374 -4.0499983 -4.1143456 -4.2123742 -4.2960434 -4.3537221][-4.2234764 -4.2363153 -4.2066426 -4.1300888 -4.0429068 -4.0066738 -4.0386491 -4.0995231 -4.1447749 -4.1614642 -4.1884003 -4.2514429 -4.3311343 -4.3971953 -4.4349513][-4.2850056 -4.31099 -4.3036795 -4.2672749 -4.2255931 -4.2075543 -4.2191057 -4.2496738 -4.2812576 -4.3034053 -4.3369746 -4.3868222 -4.4347 -4.4634595 -4.4672117][-4.3386378 -4.3723044 -4.3823328 -4.3745084 -4.3616967 -4.3574758 -4.3611245 -4.3743567 -4.3908277 -4.4054 -4.4311762 -4.462276 -4.480885 -4.4780893 -4.4565048][-4.3555856 -4.3843713 -4.401073 -4.4082727 -4.4090376 -4.4131718 -4.4156475 -4.4176722 -4.4202647 -4.4231215 -4.4373965 -4.4542308 -4.4586697 -4.447988 -4.4238391][-4.3251009 -4.3456817 -4.364449 -4.380167 -4.3894973 -4.396843 -4.3963027 -4.3903642 -4.3850737 -4.3822231 -4.3893256 -4.3994908 -4.4052553 -4.4054813 -4.3942614]]...]
INFO - root - 2017-12-07 20:12:15.650742: step 45410, loss = 21.26, batch loss = 21.18 (9.0 examples/sec; 0.890 sec/batch; 70h:58m:37s remains)
INFO - root - 2017-12-07 20:12:25.178488: step 45420, loss = 21.52, batch loss = 21.44 (8.9 examples/sec; 0.903 sec/batch; 72h:02m:56s remains)
INFO - root - 2017-12-07 20:12:34.679848: step 45430, loss = 21.20, batch loss = 21.12 (8.6 examples/sec; 0.932 sec/batch; 74h:17m:55s remains)
INFO - root - 2017-12-07 20:12:44.084038: step 45440, loss = 21.41, batch loss = 21.33 (8.8 examples/sec; 0.909 sec/batch; 72h:27m:57s remains)
INFO - root - 2017-12-07 20:12:53.442995: step 45450, loss = 21.78, batch loss = 21.70 (8.5 examples/sec; 0.940 sec/batch; 74h:56m:43s remains)
INFO - root - 2017-12-07 20:13:02.814987: step 45460, loss = 21.21, batch loss = 21.13 (8.4 examples/sec; 0.951 sec/batch; 75h:49m:24s remains)
INFO - root - 2017-12-07 20:13:12.257893: step 45470, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.948 sec/batch; 75h:35m:49s remains)
INFO - root - 2017-12-07 20:13:21.676690: step 45480, loss = 21.17, batch loss = 21.09 (9.1 examples/sec; 0.882 sec/batch; 70h:19m:31s remains)
INFO - root - 2017-12-07 20:13:31.030843: step 45490, loss = 21.40, batch loss = 21.31 (8.9 examples/sec; 0.900 sec/batch; 71h:45m:00s remains)
INFO - root - 2017-12-07 20:13:40.387748: step 45500, loss = 21.22, batch loss = 21.14 (8.6 examples/sec; 0.931 sec/batch; 74h:11m:00s remains)
2017-12-07 20:13:41.351728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4661422 -4.4720936 -4.4753385 -4.4708157 -4.4676886 -4.4709392 -4.463531 -4.4436 -4.4099388 -4.3702054 -4.3358135 -4.2969728 -4.2611747 -4.2494569 -4.2879429][-4.5207863 -4.5032682 -4.4783983 -4.4584575 -4.4585853 -4.4783115 -4.4890184 -4.47466 -4.4321527 -4.3810387 -4.3393593 -4.2926702 -4.2460523 -4.2261992 -4.2672229][-4.4961123 -4.4461665 -4.3909845 -4.3536658 -4.3581672 -4.4017682 -4.4453158 -4.4545679 -4.4214635 -4.374064 -4.33602 -4.2919021 -4.243855 -4.2211485 -4.2616205][-4.3851786 -4.3025012 -4.2185211 -4.1635213 -4.1728883 -4.2418408 -4.3245997 -4.3742309 -4.3726525 -4.3484497 -4.327301 -4.29519 -4.2542291 -4.2330065 -4.2669907][-4.2368131 -4.1271772 -4.0168762 -3.9421172 -3.953629 -4.0444546 -4.1636653 -4.2611475 -4.3058333 -4.3146796 -4.3150659 -4.2977595 -4.2649894 -4.2425885 -4.2605891][-4.1315703 -4.0031872 -3.8666821 -3.7686114 -3.7738883 -3.8761635 -4.0210657 -4.1598878 -4.2515726 -4.295455 -4.3152094 -4.3102083 -4.2835493 -4.2575264 -4.2584515][-4.0951877 -3.9508414 -3.7913024 -3.6738994 -3.6685517 -3.7728004 -3.9315848 -4.0994797 -4.2285075 -4.300848 -4.3321543 -4.3359327 -4.3159537 -4.2880926 -4.2741671][-4.1287432 -3.9804809 -3.815042 -3.6888928 -3.6637776 -3.7511616 -3.9065223 -4.0865536 -4.2355289 -4.3202748 -4.3517537 -4.3591113 -4.3455539 -4.318047 -4.2940793][-4.2397084 -4.1098657 -3.9584124 -3.8334115 -3.783236 -3.8363793 -3.9659975 -4.1301389 -4.2691755 -4.3449879 -4.3683987 -4.3767681 -4.3697062 -4.3466005 -4.3212523][-4.3826652 -4.2887726 -4.1713009 -4.064764 -4.0010653 -4.01619 -4.098907 -4.2141633 -4.31308 -4.3640342 -4.37773 -4.387486 -4.3875079 -4.3715811 -4.3507533][-4.503016 -4.4517651 -4.3803697 -4.3095717 -4.2535939 -4.2402205 -4.2698431 -4.3202572 -4.3655953 -4.3859296 -4.3887558 -4.3959532 -4.3978124 -4.3861156 -4.3708611][-4.5498466 -4.524673 -4.4904828 -4.4597054 -4.4321027 -4.4173155 -4.4209275 -4.4312859 -4.4380736 -4.4320979 -4.4207721 -4.4175191 -4.4135275 -4.4007378 -4.3891082][-4.5056815 -4.4799438 -4.4635806 -4.46657 -4.4762936 -4.4835773 -4.4928164 -4.5010324 -4.4989538 -4.4808149 -4.4566455 -4.440588 -4.4287176 -4.4123535 -4.4008904][-4.4284363 -4.3954473 -4.3837271 -4.4053788 -4.4398985 -4.4671421 -4.4910092 -4.512392 -4.5150714 -4.4923077 -4.4606547 -4.4399166 -4.4301071 -4.4186153 -4.4104967][-4.361412 -4.3275123 -4.3189626 -4.3497305 -4.3940048 -4.4271312 -4.4544983 -4.47845 -4.4792638 -4.4537988 -4.4254489 -4.4175415 -4.428751 -4.4359231 -4.43748]]...]
INFO - root - 2017-12-07 20:13:50.634981: step 45510, loss = 21.38, batch loss = 21.29 (8.4 examples/sec; 0.952 sec/batch; 75h:55m:19s remains)
INFO - root - 2017-12-07 20:14:00.081638: step 45520, loss = 21.24, batch loss = 21.16 (8.7 examples/sec; 0.921 sec/batch; 73h:25m:30s remains)
INFO - root - 2017-12-07 20:14:09.434073: step 45530, loss = 21.67, batch loss = 21.59 (8.6 examples/sec; 0.934 sec/batch; 74h:29m:03s remains)
INFO - root - 2017-12-07 20:14:18.939641: step 45540, loss = 21.41, batch loss = 21.32 (8.5 examples/sec; 0.937 sec/batch; 74h:40m:49s remains)
INFO - root - 2017-12-07 20:14:28.367964: step 45550, loss = 21.16, batch loss = 21.07 (8.8 examples/sec; 0.910 sec/batch; 72h:30m:01s remains)
INFO - root - 2017-12-07 20:14:37.600476: step 45560, loss = 21.44, batch loss = 21.35 (9.0 examples/sec; 0.893 sec/batch; 71h:09m:42s remains)
INFO - root - 2017-12-07 20:14:46.979659: step 45570, loss = 21.43, batch loss = 21.35 (8.4 examples/sec; 0.951 sec/batch; 75h:47m:53s remains)
INFO - root - 2017-12-07 20:14:56.452974: step 45580, loss = 21.71, batch loss = 21.63 (8.5 examples/sec; 0.946 sec/batch; 75h:24m:26s remains)
INFO - root - 2017-12-07 20:15:05.872541: step 45590, loss = 21.14, batch loss = 21.05 (8.1 examples/sec; 0.983 sec/batch; 78h:19m:12s remains)
INFO - root - 2017-12-07 20:15:15.218667: step 45600, loss = 21.50, batch loss = 21.42 (8.3 examples/sec; 0.962 sec/batch; 76h:40m:08s remains)
2017-12-07 20:15:16.157212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3912315 -4.3605123 -4.3667164 -4.3991733 -4.4345717 -4.445425 -4.44435 -4.4379487 -4.4296179 -4.4230752 -4.4190049 -4.417017 -4.4161835 -4.4131737 -4.3970194][-4.3476486 -4.3106446 -4.3371315 -4.4038315 -4.4629197 -4.4780512 -4.4740763 -4.4618678 -4.4469495 -4.4366608 -4.4304643 -4.4264917 -4.4229736 -4.4151621 -4.385951][-4.2991929 -4.2545605 -4.3051848 -4.406435 -4.4828892 -4.4981418 -4.4898529 -4.475533 -4.4611077 -4.4546037 -4.4503417 -4.4431367 -4.4324465 -4.41422 -4.3673739][-4.283144 -4.2375789 -4.3015227 -4.4104362 -4.4776306 -4.4805703 -4.4652796 -4.4579592 -4.4633117 -4.4818254 -4.4953494 -4.4922471 -4.47394 -4.4421463 -4.3789349][-4.2981648 -4.2607622 -4.3174024 -4.3969355 -4.423945 -4.3920569 -4.3527474 -4.3520727 -4.3940554 -4.4626565 -4.5169072 -4.5349607 -4.5192084 -4.4838967 -4.4198418][-4.3209739 -4.2846627 -4.3005919 -4.3152871 -4.2826128 -4.2010536 -4.1279631 -4.1362982 -4.22694 -4.3607459 -4.4691434 -4.5202718 -4.52022 -4.4961491 -4.4511485][-4.3594985 -4.3135834 -4.2673092 -4.2004781 -4.1025119 -3.9680927 -3.859858 -3.8793979 -4.0207415 -4.2157722 -4.3729711 -4.4539895 -4.4674706 -4.4573 -4.4459348][-4.4016728 -4.3630304 -4.2807059 -4.1590075 -4.0153322 -3.84661 -3.7104371 -3.7228727 -3.8790193 -4.094677 -4.2744861 -4.3713903 -4.3851352 -4.3690891 -4.3739038][-4.3850641 -4.3846917 -4.3239703 -4.2054205 -4.057404 -3.891269 -3.7553816 -3.7413061 -3.8531766 -4.02837 -4.1911812 -4.2857995 -4.2908053 -4.2536163 -4.2461061][-4.3004084 -4.3484507 -4.3555913 -4.2937403 -4.1751089 -4.0319686 -3.911283 -3.8661022 -3.9087968 -4.0101285 -4.12603 -4.19948 -4.1925535 -4.1401553 -4.1155214][-4.182044 -4.264039 -4.3491006 -4.367691 -4.2982941 -4.1839824 -4.0745673 -3.9971538 -3.9734781 -4.0041203 -4.0735397 -4.125216 -4.1115556 -4.05471 -4.0206571][-4.0303807 -4.1355672 -4.2889624 -4.3857985 -4.3719316 -4.2924008 -4.1954927 -4.0971522 -4.0286412 -4.0124788 -4.0503378 -4.0884857 -4.0716953 -4.0104327 -3.9652216][-3.8999128 -4.0157323 -4.2050524 -4.3485003 -4.3773189 -4.3338332 -4.2567358 -4.1631603 -4.0826921 -4.047235 -4.0674648 -4.0975542 -4.0826921 -4.0236783 -3.970706][-3.906657 -4.0088649 -4.1892715 -4.3358974 -4.3772931 -4.3497109 -4.2857819 -4.2049909 -4.13202 -4.0983658 -4.1159868 -4.1491871 -4.1476493 -4.1072583 -4.056385][-4.0532494 -4.1182742 -4.2577105 -4.3827772 -4.4250293 -4.4074669 -4.3534417 -4.2794061 -4.2083421 -4.1759286 -4.1960897 -4.2402997 -4.2623181 -4.2517042 -4.2089548]]...]
INFO - root - 2017-12-07 20:15:25.441329: step 45610, loss = 21.65, batch loss = 21.57 (8.4 examples/sec; 0.949 sec/batch; 75h:35m:39s remains)
INFO - root - 2017-12-07 20:15:34.890088: step 45620, loss = 21.33, batch loss = 21.25 (8.1 examples/sec; 0.989 sec/batch; 78h:50m:01s remains)
INFO - root - 2017-12-07 20:15:44.285229: step 45630, loss = 21.55, batch loss = 21.47 (8.3 examples/sec; 0.963 sec/batch; 76h:46m:09s remains)
INFO - root - 2017-12-07 20:15:53.761125: step 45640, loss = 21.73, batch loss = 21.64 (8.9 examples/sec; 0.896 sec/batch; 71h:25m:47s remains)
INFO - root - 2017-12-07 20:16:03.168201: step 45650, loss = 21.53, batch loss = 21.44 (8.4 examples/sec; 0.952 sec/batch; 75h:52m:49s remains)
INFO - root - 2017-12-07 20:16:12.601691: step 45660, loss = 21.35, batch loss = 21.27 (8.4 examples/sec; 0.950 sec/batch; 75h:43m:42s remains)
INFO - root - 2017-12-07 20:16:22.064463: step 45670, loss = 21.13, batch loss = 21.05 (8.2 examples/sec; 0.973 sec/batch; 77h:30m:54s remains)
INFO - root - 2017-12-07 20:16:31.360113: step 45680, loss = 21.58, batch loss = 21.49 (8.7 examples/sec; 0.920 sec/batch; 73h:18m:03s remains)
INFO - root - 2017-12-07 20:16:40.719897: step 45690, loss = 21.45, batch loss = 21.37 (8.5 examples/sec; 0.943 sec/batch; 75h:05m:21s remains)
INFO - root - 2017-12-07 20:16:50.186520: step 45700, loss = 21.64, batch loss = 21.56 (8.8 examples/sec; 0.911 sec/batch; 72h:34m:56s remains)
2017-12-07 20:16:51.185583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3755136 -4.4221687 -4.4666033 -4.4886055 -4.4873815 -4.4703803 -4.4490776 -4.4379306 -4.434176 -4.4280224 -4.4224634 -4.4207287 -4.4211717 -4.4200954 -4.4157195][-4.3979697 -4.4726768 -4.5352073 -4.5571365 -4.5457444 -4.5150146 -4.4789929 -4.461328 -4.4600186 -4.456955 -4.4547639 -4.4593282 -4.4663086 -4.4716082 -4.4722033][-4.4120917 -4.5079269 -4.5751038 -4.5838075 -4.5566697 -4.5141177 -4.4645448 -4.4364481 -4.4336314 -4.4359765 -4.4431014 -4.4610238 -4.4812264 -4.498486 -4.5083656][-4.419311 -4.5199218 -4.5740385 -4.5581317 -4.5149078 -4.4687395 -4.4150486 -4.3748984 -4.3639727 -4.3732109 -4.3969541 -4.4352121 -4.4726086 -4.5038033 -4.523243][-4.4163089 -4.5038476 -4.533164 -4.4935551 -4.438448 -4.396831 -4.3530278 -4.3060403 -4.28435 -4.2989573 -4.33979 -4.3978982 -4.4490361 -4.4888368 -4.5122981][-4.3976068 -4.4608073 -4.4609547 -4.4069076 -4.3485889 -4.3160038 -4.2870288 -4.2325425 -4.1963048 -4.2127018 -4.2659431 -4.3370123 -4.39084 -4.4263015 -4.444479][-4.3614531 -4.396421 -4.370265 -4.3113475 -4.2516284 -4.2231879 -4.2005119 -4.1328936 -4.0848374 -4.1021709 -4.1644073 -4.2420712 -4.2901993 -4.31257 -4.3208132][-4.3257904 -4.3450842 -4.30952 -4.2564058 -4.1938972 -4.1580935 -4.1209307 -4.0294981 -3.9764111 -3.9973211 -4.0649824 -4.1473651 -4.1899385 -4.1939588 -4.1799455][-4.3114591 -4.3407493 -4.3204951 -4.2802539 -4.2111688 -4.154099 -4.0810852 -3.958832 -3.9073904 -3.9363143 -4.0034742 -4.0820107 -4.1153531 -4.0919595 -4.0369611][-4.3253207 -4.3819332 -4.3885994 -4.3594918 -4.2819314 -4.2013288 -4.0942535 -3.9560516 -3.9226723 -3.9671125 -4.0298667 -4.0936346 -4.111412 -4.0544024 -3.9490552][-4.3537893 -4.4300289 -4.45241 -4.4288688 -4.3580379 -4.2769914 -4.1595783 -4.0252247 -4.0103278 -4.0635629 -4.1122456 -4.1501236 -4.1497593 -4.0744839 -3.9455402][-4.38044 -4.4674754 -4.4997535 -4.4853597 -4.4369774 -4.3724494 -4.2606378 -4.1413007 -4.1381092 -4.190311 -4.2236743 -4.2414074 -4.2384987 -4.1810989 -4.076972][-4.3966146 -4.4874611 -4.528131 -4.5238047 -4.4967542 -4.4490733 -4.3526931 -4.2601929 -4.2707267 -4.3225493 -4.3485265 -4.3603168 -4.3694048 -4.3501415 -4.2961025][-4.39871 -4.4882908 -4.5383878 -4.5448728 -4.5291829 -4.4882064 -4.4073286 -4.3456812 -4.3753228 -4.4318981 -4.4562044 -4.4684291 -4.4884357 -4.4977388 -4.483963][-4.3872089 -4.4660287 -4.5203862 -4.5351067 -4.5235076 -4.4841042 -4.4177728 -4.3832688 -4.4290075 -4.4892879 -4.5118604 -4.5228586 -4.5453997 -4.5652261 -4.5698242]]...]
INFO - root - 2017-12-07 20:17:00.461767: step 45710, loss = 21.32, batch loss = 21.24 (9.1 examples/sec; 0.883 sec/batch; 70h:18m:44s remains)
INFO - root - 2017-12-07 20:17:09.774030: step 45720, loss = 20.99, batch loss = 20.91 (8.6 examples/sec; 0.925 sec/batch; 73h:41m:33s remains)
INFO - root - 2017-12-07 20:17:19.255945: step 45730, loss = 21.67, batch loss = 21.59 (8.4 examples/sec; 0.947 sec/batch; 75h:28m:12s remains)
INFO - root - 2017-12-07 20:17:28.780945: step 45740, loss = 21.20, batch loss = 21.11 (8.1 examples/sec; 0.992 sec/batch; 79h:01m:37s remains)
INFO - root - 2017-12-07 20:17:38.265954: step 45750, loss = 21.52, batch loss = 21.44 (8.2 examples/sec; 0.980 sec/batch; 78h:03m:14s remains)
INFO - root - 2017-12-07 20:17:47.699231: step 45760, loss = 21.45, batch loss = 21.37 (8.1 examples/sec; 0.983 sec/batch; 78h:18m:47s remains)
INFO - root - 2017-12-07 20:17:57.121827: step 45770, loss = 21.29, batch loss = 21.21 (8.3 examples/sec; 0.964 sec/batch; 76h:44m:37s remains)
INFO - root - 2017-12-07 20:18:06.555618: step 45780, loss = 21.53, batch loss = 21.44 (8.5 examples/sec; 0.941 sec/batch; 74h:58m:47s remains)
INFO - root - 2017-12-07 20:18:16.015021: step 45790, loss = 21.35, batch loss = 21.27 (8.7 examples/sec; 0.917 sec/batch; 73h:01m:57s remains)
INFO - root - 2017-12-07 20:18:25.503550: step 45800, loss = 21.39, batch loss = 21.31 (8.3 examples/sec; 0.966 sec/batch; 76h:56m:38s remains)
2017-12-07 20:18:26.469380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3912616 -4.4486532 -4.4808598 -4.4932089 -4.5072579 -4.5151477 -4.5117173 -4.5228758 -4.5534434 -4.5806165 -4.5836711 -4.5770717 -4.5861807 -4.6128225 -4.5938559][-4.3649592 -4.4399476 -4.492887 -4.5194445 -4.5304828 -4.5188789 -4.4948635 -4.4980645 -4.539093 -4.5822983 -4.5913157 -4.5780239 -4.5796475 -4.5993915 -4.5591063][-4.3845124 -4.4690719 -4.5332627 -4.5588083 -4.5471458 -4.4957108 -4.4365339 -4.4326978 -4.5061021 -4.5974226 -4.6376362 -4.6287847 -4.6211834 -4.6232824 -4.5501714][-4.4208674 -4.503345 -4.5603886 -4.5597219 -4.4950666 -4.3757443 -4.2636676 -4.2579908 -4.3928843 -4.5720234 -4.6760755 -4.6869144 -4.6686373 -4.6403718 -4.5274334][-4.4601626 -4.5247984 -4.5583162 -4.5141416 -4.3736014 -4.15944 -3.9726541 -3.9619887 -4.173429 -4.4656639 -4.6576433 -4.7004242 -4.6706882 -4.6109333 -4.4703107][-4.4617386 -4.5037627 -4.5121236 -4.4304304 -4.2204337 -3.9090791 -3.6336756 -3.6005535 -3.8743649 -4.2779036 -4.5629926 -4.6385841 -4.590589 -4.5004 -4.3557324][-4.3910427 -4.4215965 -4.4279079 -4.3442678 -4.1076889 -3.7300587 -3.3699141 -3.2892365 -3.5873141 -4.0659738 -4.42066 -4.514636 -4.442059 -4.3312368 -4.2121615][-4.2800655 -4.3053455 -4.3243766 -4.2683058 -4.0557618 -3.6725013 -3.2758613 -3.1509044 -3.4304256 -3.9233642 -4.3023396 -4.4044223 -4.3202868 -4.20692 -4.1293144][-4.1457357 -4.1712503 -4.2025094 -4.1783547 -4.0156455 -3.6910319 -3.3467836 -3.2340491 -3.477031 -3.9195242 -4.2677965 -4.3663797 -4.2913027 -4.190526 -4.148818][-4.0825076 -4.1157088 -4.1525044 -4.1440969 -4.0227194 -3.7825265 -3.5471807 -3.4914696 -3.6889095 -4.030293 -4.2966232 -4.3677959 -4.3045087 -4.2208948 -4.2071977][-4.1612363 -4.19752 -4.223484 -4.2032151 -4.0955248 -3.9255564 -3.795732 -3.7977366 -3.9479835 -4.176847 -4.3433275 -4.3723159 -4.3138828 -4.2470078 -4.2518153][-4.2861733 -4.3208013 -4.329783 -4.2906919 -4.1895227 -4.0736046 -4.0196123 -4.0570874 -4.1679521 -4.3056211 -4.3881197 -4.3805609 -4.3240023 -4.2707591 -4.2829809][-4.363297 -4.3922071 -4.3938136 -4.3502517 -4.2674522 -4.1999359 -4.1947742 -4.2478333 -4.3281384 -4.4022655 -4.4271612 -4.3954797 -4.3391771 -4.295475 -4.3068719][-4.388556 -4.4125323 -4.4125462 -4.377924 -4.3224869 -4.295331 -4.3196287 -4.3762279 -4.4325595 -4.4668827 -4.459826 -4.4164925 -4.3603287 -4.3195057 -4.3239727][-4.3931551 -4.4081273 -4.4051709 -4.3816195 -4.3516922 -4.3527603 -4.3923178 -4.4454975 -4.4832792 -4.4944353 -4.4729924 -4.4281735 -4.3767953 -4.3382554 -4.3349161]]...]
INFO - root - 2017-12-07 20:18:35.949271: step 45810, loss = 21.11, batch loss = 21.03 (8.7 examples/sec; 0.924 sec/batch; 73h:35m:08s remains)
INFO - root - 2017-12-07 20:18:45.338435: step 45820, loss = 21.45, batch loss = 21.37 (8.2 examples/sec; 0.976 sec/batch; 77h:44m:38s remains)
INFO - root - 2017-12-07 20:18:54.855339: step 45830, loss = 21.67, batch loss = 21.59 (8.3 examples/sec; 0.968 sec/batch; 77h:05m:10s remains)
INFO - root - 2017-12-07 20:19:04.299098: step 45840, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.926 sec/batch; 73h:43m:22s remains)
INFO - root - 2017-12-07 20:19:13.824429: step 45850, loss = 21.53, batch loss = 21.45 (8.3 examples/sec; 0.965 sec/batch; 76h:48m:50s remains)
INFO - root - 2017-12-07 20:19:23.198629: step 45860, loss = 21.27, batch loss = 21.19 (8.4 examples/sec; 0.957 sec/batch; 76h:13m:25s remains)
INFO - root - 2017-12-07 20:19:32.695118: step 45870, loss = 21.49, batch loss = 21.40 (8.3 examples/sec; 0.962 sec/batch; 76h:36m:21s remains)
INFO - root - 2017-12-07 20:19:42.070824: step 45880, loss = 21.35, batch loss = 21.26 (8.5 examples/sec; 0.947 sec/batch; 75h:21m:41s remains)
INFO - root - 2017-12-07 20:19:51.501464: step 45890, loss = 21.47, batch loss = 21.38 (8.9 examples/sec; 0.896 sec/batch; 71h:20m:09s remains)
INFO - root - 2017-12-07 20:20:00.846259: step 45900, loss = 21.55, batch loss = 21.47 (9.0 examples/sec; 0.892 sec/batch; 71h:02m:54s remains)
2017-12-07 20:20:01.778850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4959164 -4.5249314 -4.5332494 -4.5633569 -4.6211748 -4.6298714 -4.59931 -4.5677643 -4.5710449 -4.5998664 -4.6142435 -4.613986 -4.5937676 -4.5509448 -4.4833813][-4.5931187 -4.6210427 -4.5933356 -4.5990734 -4.6674204 -4.6844554 -4.6582551 -4.6385455 -4.66462 -4.7195992 -4.7357035 -4.7232738 -4.6874371 -4.6258531 -4.53493][-4.6959205 -4.704155 -4.6187453 -4.5711675 -4.6200824 -4.617578 -4.5829682 -4.5753207 -4.6324668 -4.7414455 -4.7851143 -4.7734752 -4.7309418 -4.6591716 -4.5601115][-4.7322769 -4.7040076 -4.5688014 -4.473938 -4.4828548 -4.42404 -4.3571806 -4.3441014 -4.4270077 -4.6119542 -4.7144036 -4.7294436 -4.6985621 -4.6308827 -4.5382533][-4.6459031 -4.5757103 -4.4184809 -4.3159337 -4.2953134 -4.1652622 -4.0576673 -4.0187187 -4.110055 -4.3734136 -4.5467477 -4.6096139 -4.6108637 -4.5628552 -4.4863496][-4.5082564 -4.4075389 -4.2473559 -4.1659412 -4.1227818 -3.9172914 -3.7783504 -3.6999302 -3.7860737 -4.1156874 -4.3483391 -4.4625187 -4.5088773 -4.4906387 -4.4342813][-4.4103851 -4.2946572 -4.1288462 -4.0692978 -4.006216 -3.7413106 -3.5916448 -3.4785643 -3.5652871 -3.9412463 -4.2067003 -4.3609324 -4.4522386 -4.4590054 -4.4131231][-4.4129786 -4.2913313 -4.113142 -4.0605745 -3.9788046 -3.6894922 -3.5545032 -3.4391053 -3.547972 -3.9464536 -4.2076554 -4.3688178 -4.4779625 -4.4862094 -4.4280858][-4.5308943 -4.4297328 -4.2544274 -4.2032976 -4.1141515 -3.840694 -3.7259426 -3.6218016 -3.7492611 -4.1251531 -4.3428245 -4.4728479 -4.5599442 -4.5471468 -4.4653549][-4.6547594 -4.5909443 -4.4501605 -4.4165049 -4.3504343 -4.1407418 -4.0500922 -3.9589658 -4.0705018 -4.3629928 -4.5089922 -4.5852003 -4.6292028 -4.5943246 -4.4983134][-4.6790175 -4.6453156 -4.5538335 -4.5439086 -4.52239 -4.4027362 -4.3413124 -4.2681074 -4.3434949 -4.5320158 -4.609427 -4.6375203 -4.6449862 -4.6005692 -4.5064182][-4.6074133 -4.5926337 -4.5472536 -4.5595012 -4.577744 -4.5407133 -4.5150838 -4.4686356 -4.5063806 -4.5993028 -4.6242123 -4.6221762 -4.6087942 -4.5668793 -4.4863839][-4.4964128 -4.4916172 -4.4805455 -4.505651 -4.5397043 -4.5485115 -4.5492229 -4.5273805 -4.538878 -4.5701752 -4.570025 -4.559341 -4.5426164 -4.5109 -4.4501734][-4.408215 -4.4015603 -4.4028473 -4.4241786 -4.4494643 -4.4625392 -4.4667692 -4.4581275 -4.458446 -4.4676061 -4.46817 -4.4680882 -4.4647236 -4.4497619 -4.4133568][-4.3728547 -4.3636241 -4.3628235 -4.3718624 -4.3810086 -4.3838882 -4.38166 -4.3732533 -4.3681855 -4.3706055 -4.37754 -4.3888168 -4.3977203 -4.3971105 -4.3835883]]...]
INFO - root - 2017-12-07 20:20:11.084331: step 45910, loss = 21.38, batch loss = 21.30 (8.8 examples/sec; 0.910 sec/batch; 72h:26m:04s remains)
INFO - root - 2017-12-07 20:20:20.502263: step 45920, loss = 21.33, batch loss = 21.25 (9.0 examples/sec; 0.887 sec/batch; 70h:36m:18s remains)
INFO - root - 2017-12-07 20:20:29.958434: step 45930, loss = 21.53, batch loss = 21.45 (8.7 examples/sec; 0.924 sec/batch; 73h:32m:35s remains)
INFO - root - 2017-12-07 20:20:39.462627: step 45940, loss = 21.81, batch loss = 21.73 (8.5 examples/sec; 0.937 sec/batch; 74h:36m:09s remains)
INFO - root - 2017-12-07 20:20:48.849567: step 45950, loss = 21.19, batch loss = 21.10 (7.9 examples/sec; 1.012 sec/batch; 80h:35m:23s remains)
INFO - root - 2017-12-07 20:20:58.287800: step 45960, loss = 21.74, batch loss = 21.66 (8.0 examples/sec; 0.998 sec/batch; 79h:23m:53s remains)
INFO - root - 2017-12-07 20:21:07.838717: step 45970, loss = 21.71, batch loss = 21.63 (8.5 examples/sec; 0.943 sec/batch; 75h:01m:09s remains)
INFO - root - 2017-12-07 20:21:17.328723: step 45980, loss = 21.55, batch loss = 21.47 (8.1 examples/sec; 0.988 sec/batch; 78h:37m:51s remains)
INFO - root - 2017-12-07 20:21:26.843940: step 45990, loss = 21.17, batch loss = 21.09 (8.0 examples/sec; 1.001 sec/batch; 79h:37m:51s remains)
INFO - root - 2017-12-07 20:21:36.281519: step 46000, loss = 21.18, batch loss = 21.09 (8.4 examples/sec; 0.958 sec/batch; 76h:14m:43s remains)
2017-12-07 20:21:37.172862: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4088836 -4.4512548 -4.46065 -4.4812407 -4.4701056 -4.4042826 -4.3855867 -4.4188361 -4.4275603 -4.4303985 -4.4367828 -4.4149179 -4.3454337 -4.2506609 -4.1708035][-4.423811 -4.4641452 -4.4596343 -4.4631357 -4.4338455 -4.3405156 -4.2819705 -4.2948985 -4.3343463 -4.3968229 -4.4598284 -4.4637027 -4.4007459 -4.3211603 -4.2770848][-4.3424287 -4.3904076 -4.3858938 -4.3923068 -4.3626361 -4.2596664 -4.1796026 -4.181107 -4.2443285 -4.3480473 -4.4567294 -4.4960737 -4.4559832 -4.405242 -4.4032583][-4.3072367 -4.3634796 -4.3587313 -4.3638015 -4.3205366 -4.1977491 -4.0936565 -4.078289 -4.1482749 -4.2720242 -4.4143286 -4.5026832 -4.5076661 -4.4917264 -4.5139117][-4.3833947 -4.442111 -4.4275045 -4.4059291 -4.3120785 -4.13855 -3.9901457 -3.9459374 -4.0130916 -4.15035 -4.3236895 -4.4719768 -4.5417347 -4.5649891 -4.5932341][-4.5302286 -4.5761747 -4.532896 -4.4534211 -4.2771482 -4.0367002 -3.8432016 -3.7780921 -3.8535745 -4.0177636 -4.228559 -4.4322424 -4.5578108 -4.6072493 -4.6224909][-4.6631155 -4.6834688 -4.6038737 -4.4550147 -4.1972556 -3.9061241 -3.696075 -3.6436434 -3.7529519 -3.956969 -4.1977563 -4.4236126 -4.5650649 -4.6075921 -4.5911412][-4.7597551 -4.7573442 -4.6501069 -4.4461432 -4.1352038 -3.8203201 -3.6193957 -3.601439 -3.7538311 -3.9934731 -4.2449822 -4.4608355 -4.5864377 -4.602397 -4.5501394][-4.8052759 -4.7902875 -4.6797228 -4.4684339 -4.1554632 -3.842082 -3.6488636 -3.6502635 -3.8234715 -4.0777125 -4.3263183 -4.5271311 -4.639039 -4.630312 -4.5477753][-4.7944932 -4.7873769 -4.7096925 -4.5432963 -4.2737336 -3.9834914 -3.7940714 -3.7895291 -3.9548771 -4.1972876 -4.4280777 -4.6098194 -4.7087379 -4.6802683 -4.5752168][-4.7463574 -4.7642703 -4.7365694 -4.6338191 -4.42459 -4.1729259 -3.9974558 -3.982866 -4.1266975 -4.3397455 -4.5364785 -4.6879506 -4.7660475 -4.722456 -4.6076803][-4.6668439 -4.7108865 -4.72843 -4.6859059 -4.5458622 -4.3491921 -4.197525 -4.1671281 -4.2731471 -4.4464626 -4.6082659 -4.729393 -4.7861648 -4.7369804 -4.624783][-4.5833387 -4.6464171 -4.6953783 -4.7002463 -4.6265178 -4.4956908 -4.3784952 -4.3314304 -4.3865619 -4.5089059 -4.6351748 -4.7306991 -4.7697029 -4.7189159 -4.6116405][-4.5208011 -4.5900397 -4.648519 -4.6741629 -4.645359 -4.5718679 -4.4931269 -4.4415197 -4.4520383 -4.5228229 -4.615098 -4.6886392 -4.7103944 -4.6562715 -4.5515361][-4.4669051 -4.5332212 -4.5874653 -4.615324 -4.6055064 -4.5629015 -4.5084667 -4.4595809 -4.4467025 -4.4829197 -4.5495877 -4.6070323 -4.61479 -4.555717 -4.4553881]]...]
INFO - root - 2017-12-07 20:21:46.614369: step 46010, loss = 21.20, batch loss = 21.12 (8.3 examples/sec; 0.966 sec/batch; 76h:51m:32s remains)
INFO - root - 2017-12-07 20:21:55.984990: step 46020, loss = 21.51, batch loss = 21.43 (8.1 examples/sec; 0.986 sec/batch; 78h:26m:44s remains)
INFO - root - 2017-12-07 20:22:05.402747: step 46030, loss = 21.39, batch loss = 21.31 (8.6 examples/sec; 0.927 sec/batch; 73h:46m:41s remains)
INFO - root - 2017-12-07 20:22:14.816688: step 46040, loss = 21.31, batch loss = 21.23 (9.1 examples/sec; 0.881 sec/batch; 70h:08m:14s remains)
INFO - root - 2017-12-07 20:22:24.140318: step 46050, loss = 21.28, batch loss = 21.20 (8.5 examples/sec; 0.938 sec/batch; 74h:36m:16s remains)
INFO - root - 2017-12-07 20:22:33.592604: step 46060, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.947 sec/batch; 75h:21m:41s remains)
INFO - root - 2017-12-07 20:22:43.128442: step 46070, loss = 21.64, batch loss = 21.56 (8.5 examples/sec; 0.941 sec/batch; 74h:54m:23s remains)
INFO - root - 2017-12-07 20:22:52.416389: step 46080, loss = 21.49, batch loss = 21.41 (8.5 examples/sec; 0.940 sec/batch; 74h:48m:34s remains)
INFO - root - 2017-12-07 20:23:01.769820: step 46090, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.950 sec/batch; 75h:32m:43s remains)
INFO - root - 2017-12-07 20:23:11.212868: step 46100, loss = 21.38, batch loss = 21.29 (7.9 examples/sec; 1.009 sec/batch; 80h:17m:16s remains)
2017-12-07 20:23:12.190367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.563632 -4.640305 -4.6861496 -4.7101617 -4.7074447 -4.6602526 -4.5926094 -4.58943 -4.6542511 -4.705596 -4.7273316 -4.7243915 -4.6917653 -4.6457553 -4.5939546][-4.4986491 -4.5875249 -4.6364279 -4.6497412 -4.6337686 -4.5824122 -4.5157595 -4.5217524 -4.6131115 -4.7016931 -4.7579346 -4.7784481 -4.7566047 -4.7107587 -4.6508346][-4.4515171 -4.5600572 -4.6172833 -4.6177545 -4.5730462 -4.4910817 -4.3960009 -4.3831182 -4.485044 -4.6156797 -4.7172771 -4.7712469 -4.7663908 -4.7216754 -4.656671][-4.4420056 -4.5631032 -4.6216955 -4.6043434 -4.523663 -4.3928909 -4.2409029 -4.1832137 -4.2772694 -4.4370823 -4.5801277 -4.6712894 -4.691268 -4.6545506 -4.5913773][-4.464726 -4.5863171 -4.6368532 -4.5970874 -4.4757886 -4.2841249 -4.0609584 -3.9552398 -4.0486145 -4.2302728 -4.4020352 -4.5256934 -4.5726528 -4.552104 -4.5029225][-4.4852042 -4.5869756 -4.61016 -4.5353079 -4.3704224 -4.1235976 -3.8453596 -3.7164614 -3.8395653 -4.0593009 -4.2552767 -4.4006114 -4.4682107 -4.4701939 -4.4495463][-4.4762626 -4.5371652 -4.5134096 -4.3973918 -4.1977963 -3.9244864 -3.6363282 -3.5210469 -3.694792 -3.9639053 -4.1823354 -4.3415232 -4.4294643 -4.4593654 -4.4716172][-4.4457226 -4.4652495 -4.4105163 -4.28711 -4.093823 -3.8462906 -3.5995064 -3.5159311 -3.7101316 -3.9979892 -4.2209311 -4.3757896 -4.4743233 -4.5279441 -4.5652385][-4.4198589 -4.4185381 -4.3716459 -4.2933064 -4.1605082 -3.9808249 -3.7957754 -3.7356913 -3.9047828 -4.1628008 -4.3647428 -4.49856 -4.5924592 -4.6521225 -4.6904116][-4.4200749 -4.4153309 -4.3942637 -4.3723612 -4.3110447 -4.2008085 -4.076406 -4.0468078 -4.1890526 -4.3940225 -4.5514755 -4.6454053 -4.7119236 -4.7463546 -4.7548819][-4.4628935 -4.4694843 -4.4687791 -4.471633 -4.4405441 -4.3651023 -4.2789707 -4.286139 -4.4297051 -4.5891933 -4.6870928 -4.7278352 -4.747983 -4.7379761 -4.7062516][-4.53792 -4.5688963 -4.5852203 -4.5843248 -4.5385537 -4.4519248 -4.3711524 -4.3972135 -4.5433259 -4.6653032 -4.7061477 -4.7005897 -4.6782804 -4.6315856 -4.5746307][-4.6001592 -4.6562915 -4.6919508 -4.6839104 -4.60862 -4.489542 -4.3887124 -4.402101 -4.5245633 -4.607872 -4.6085019 -4.5818758 -4.5458107 -4.4937277 -4.4393134][-4.62747 -4.7000241 -4.752192 -4.7472587 -4.6579533 -4.5179787 -4.3984623 -4.3860812 -4.4712825 -4.5245395 -4.5097332 -4.4816322 -4.4512324 -4.41522 -4.3816838][-4.6300879 -4.7101154 -4.766367 -4.7681966 -4.6899219 -4.5621133 -4.4531226 -4.4304981 -4.4828005 -4.5146289 -4.4986825 -4.478548 -4.4608197 -4.4438472 -4.4296546]]...]
INFO - root - 2017-12-07 20:23:21.505544: step 46110, loss = 21.55, batch loss = 21.47 (9.2 examples/sec; 0.870 sec/batch; 69h:12m:34s remains)
INFO - root - 2017-12-07 20:23:30.862015: step 46120, loss = 21.27, batch loss = 21.19 (8.9 examples/sec; 0.900 sec/batch; 71h:35m:51s remains)
INFO - root - 2017-12-07 20:23:40.367908: step 46130, loss = 21.58, batch loss = 21.50 (8.2 examples/sec; 0.981 sec/batch; 78h:00m:33s remains)
INFO - root - 2017-12-07 20:23:49.832073: step 46140, loss = 21.43, batch loss = 21.35 (8.5 examples/sec; 0.938 sec/batch; 74h:34m:48s remains)
INFO - root - 2017-12-07 20:23:59.054792: step 46150, loss = 21.37, batch loss = 21.29 (8.7 examples/sec; 0.917 sec/batch; 72h:57m:19s remains)
INFO - root - 2017-12-07 20:24:08.455178: step 46160, loss = 21.43, batch loss = 21.35 (8.5 examples/sec; 0.937 sec/batch; 74h:31m:34s remains)
INFO - root - 2017-12-07 20:24:17.902992: step 46170, loss = 21.41, batch loss = 21.33 (8.5 examples/sec; 0.937 sec/batch; 74h:31m:43s remains)
INFO - root - 2017-12-07 20:24:27.398354: step 46180, loss = 21.50, batch loss = 21.42 (8.8 examples/sec; 0.913 sec/batch; 72h:38m:52s remains)
INFO - root - 2017-12-07 20:24:37.046870: step 46190, loss = 21.67, batch loss = 21.58 (8.7 examples/sec; 0.922 sec/batch; 73h:19m:24s remains)
INFO - root - 2017-12-07 20:24:46.492219: step 46200, loss = 21.18, batch loss = 21.10 (8.7 examples/sec; 0.924 sec/batch; 73h:29m:00s remains)
2017-12-07 20:24:47.467918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5277705 -4.5585847 -4.5610361 -4.5510035 -4.5579152 -4.6002665 -4.6692333 -4.7128329 -4.7043428 -4.6666665 -4.6324286 -4.6017094 -4.5641193 -4.5384922 -4.5310144][-4.5257087 -4.596889 -4.6408987 -4.6462622 -4.6281114 -4.6184387 -4.6397638 -4.6584592 -4.6402082 -4.6050644 -4.5881438 -4.579576 -4.55712 -4.5363092 -4.5302629][-4.5371127 -4.6370325 -4.708395 -4.72527 -4.6934123 -4.6462183 -4.6229124 -4.6049447 -4.5553317 -4.4988313 -4.4784145 -4.4874015 -4.4988613 -4.5144997 -4.5371475][-4.56884 -4.67505 -4.7322569 -4.7309527 -4.6849732 -4.6206889 -4.5736537 -4.5304322 -4.4610553 -4.3946562 -4.3710461 -4.3930349 -4.4391303 -4.4991021 -4.5616832][-4.6103272 -4.7015195 -4.7093859 -4.6521196 -4.5670304 -4.4812226 -4.4240694 -4.38143 -4.3309941 -4.29289 -4.2847762 -4.31305 -4.376174 -4.4686594 -4.565618][-4.6566353 -4.7067146 -4.6363797 -4.4896741 -4.3311343 -4.2013445 -4.1355815 -4.1155086 -4.11637 -4.1409373 -4.17508 -4.2272949 -4.3150406 -4.440886 -4.5705924][-4.7426033 -4.730979 -4.5668526 -4.3116503 -4.0504622 -3.8489738 -3.7565246 -3.7563422 -3.8163872 -3.91738 -4.0234466 -4.1408563 -4.2860804 -4.455061 -4.6121259][-4.8683734 -4.8137221 -4.5909467 -4.265295 -3.9196062 -3.6408062 -3.5036368 -3.5039487 -3.6037126 -3.756568 -3.9175293 -4.0973997 -4.3025732 -4.51237 -4.6809707][-4.969059 -4.9215107 -4.7212205 -4.4188323 -4.0689821 -3.758601 -3.5936389 -3.5898156 -3.6989071 -3.8460255 -3.9924121 -4.1648922 -4.3678937 -4.5687122 -4.7132664][-4.9940872 -4.9877853 -4.8668103 -4.650641 -4.363524 -4.07587 -3.9173219 -3.9239545 -4.0421705 -4.1667047 -4.2590303 -4.3623314 -4.49425 -4.6263409 -4.7093449][-4.9431572 -4.9802189 -4.9350371 -4.7998347 -4.5827985 -4.3437047 -4.217216 -4.2521038 -4.3847008 -4.495286 -4.5421009 -4.57491 -4.6212506 -4.6687717 -4.6890736][-4.8565493 -4.9159179 -4.9135466 -4.8297191 -4.6699133 -4.4876742 -4.3998256 -4.4502835 -4.5735168 -4.6612988 -4.6784925 -4.6703019 -4.6617517 -4.6510448 -4.6354914][-4.76495 -4.8222241 -4.8291883 -4.7710919 -4.6571121 -4.5305114 -4.4745679 -4.52111 -4.6153507 -4.6726594 -4.6707511 -4.6413946 -4.6065288 -4.5699468 -4.5486417][-4.669816 -4.7301006 -4.7451491 -4.7089415 -4.6354842 -4.554481 -4.5190096 -4.5507956 -4.6110144 -4.6357632 -4.6078372 -4.5509448 -4.4928803 -4.4463482 -4.4357491][-4.556016 -4.6304717 -4.6653633 -4.6568942 -4.618638 -4.5693436 -4.5411787 -4.5563588 -4.59064 -4.5916257 -4.5424366 -4.4619536 -4.3872442 -4.3366003 -4.3339648]]...]
INFO - root - 2017-12-07 20:24:57.043776: step 46210, loss = 21.17, batch loss = 21.09 (8.4 examples/sec; 0.948 sec/batch; 75h:21m:14s remains)
INFO - root - 2017-12-07 20:25:06.533828: step 46220, loss = 20.90, batch loss = 20.81 (8.2 examples/sec; 0.970 sec/batch; 77h:09m:34s remains)
INFO - root - 2017-12-07 20:25:15.825183: step 46230, loss = 21.32, batch loss = 21.23 (8.6 examples/sec; 0.930 sec/batch; 73h:55m:05s remains)
INFO - root - 2017-12-07 20:25:25.318933: step 46240, loss = 21.33, batch loss = 21.25 (8.7 examples/sec; 0.915 sec/batch; 72h:43m:57s remains)
INFO - root - 2017-12-07 20:25:34.546964: step 46250, loss = 21.44, batch loss = 21.35 (8.9 examples/sec; 0.894 sec/batch; 71h:05m:20s remains)
INFO - root - 2017-12-07 20:25:43.924906: step 46260, loss = 21.52, batch loss = 21.44 (8.8 examples/sec; 0.904 sec/batch; 71h:53m:11s remains)
INFO - root - 2017-12-07 20:25:53.341003: step 46270, loss = 21.37, batch loss = 21.29 (8.2 examples/sec; 0.978 sec/batch; 77h:46m:07s remains)
INFO - root - 2017-12-07 20:26:02.872774: step 46280, loss = 21.78, batch loss = 21.70 (7.9 examples/sec; 1.009 sec/batch; 80h:14m:40s remains)
INFO - root - 2017-12-07 20:26:12.404961: step 46290, loss = 21.40, batch loss = 21.32 (9.0 examples/sec; 0.891 sec/batch; 70h:49m:49s remains)
INFO - root - 2017-12-07 20:26:21.685880: step 46300, loss = 21.32, batch loss = 21.24 (9.1 examples/sec; 0.883 sec/batch; 70h:11m:46s remains)
2017-12-07 20:26:22.707869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5381722 -4.5504322 -4.5571866 -4.5436263 -4.4851584 -4.4061389 -4.3576918 -4.3627009 -4.422586 -4.4905849 -4.5355053 -4.5573025 -4.5440145 -4.5182562 -4.5209761][-4.5565224 -4.5813708 -4.5987482 -4.5888567 -4.5363646 -4.4707251 -4.4319811 -4.4200134 -4.4270349 -4.4394627 -4.4530225 -4.4639559 -4.4607673 -4.4502735 -4.4595785][-4.56934 -4.6163316 -4.6410928 -4.6291838 -4.5743456 -4.5045347 -4.4555902 -4.4195681 -4.3864427 -4.3546877 -4.3319778 -4.3201075 -4.31903 -4.3320856 -4.3659739][-4.5323234 -4.6003222 -4.6290045 -4.6100812 -4.5417938 -4.4542379 -4.3891263 -4.3420835 -4.2992587 -4.2520995 -4.2055769 -4.1740036 -4.1809454 -4.2279782 -4.2940941][-4.4535222 -4.5227079 -4.5432796 -4.5141845 -4.4332409 -4.3335352 -4.2613397 -4.2206511 -4.1940227 -4.1548033 -4.1005063 -4.0628753 -4.08329 -4.1600661 -4.2520928][-4.3635616 -4.4119096 -4.4158549 -4.3773808 -4.2912908 -4.1821451 -4.100235 -4.0721412 -4.076087 -4.0593777 -4.0164046 -3.9858091 -4.0156794 -4.105731 -4.2155232][-4.2837119 -4.3113022 -4.305078 -4.2562327 -4.1517472 -4.0168023 -3.9180782 -3.9131818 -3.9625013 -3.9838879 -3.9748034 -3.9662857 -4.0008354 -4.0910754 -4.2081671][-4.2260656 -4.2382808 -4.2332258 -4.1803174 -4.0624647 -3.9181943 -3.827209 -3.8584769 -3.9467695 -4.0069842 -4.0411773 -4.0534663 -4.082067 -4.1546988 -4.25288][-4.1986566 -4.2074962 -4.2198529 -4.1929741 -4.1047316 -4.0000067 -3.9484053 -3.9933224 -4.0746984 -4.1370335 -4.1826448 -4.1941447 -4.2137156 -4.2718043 -4.3439655][-4.1872039 -4.2162561 -4.2666788 -4.2918639 -4.2655954 -4.2202024 -4.1950412 -4.2067347 -4.2243485 -4.2399712 -4.2619276 -4.27339 -4.3101063 -4.379096 -4.4424138][-4.1737051 -4.2253203 -4.306251 -4.3747568 -4.4027538 -4.4024453 -4.3818526 -4.3417764 -4.2857728 -4.248374 -4.2448268 -4.2678933 -4.3368216 -4.4290867 -4.4995766][-4.1257563 -4.187438 -4.283143 -4.3725171 -4.4247642 -4.4433084 -4.4247408 -4.3661842 -4.2848625 -4.2293172 -4.2151861 -4.2431655 -4.3217192 -4.4200182 -4.49491][-4.0706706 -4.131165 -4.2259188 -4.3112249 -4.360817 -4.3816648 -4.3793955 -4.3529806 -4.3087335 -4.2749453 -4.2558832 -4.2625465 -4.3098879 -4.3827744 -4.4489655][-4.0686545 -4.1198297 -4.1970487 -4.2630563 -4.295033 -4.3085642 -4.3252778 -4.3498425 -4.3663468 -4.3701692 -4.3533845 -4.328939 -4.3258543 -4.3555131 -4.39362][-4.1371589 -4.1791673 -4.23181 -4.2685394 -4.2744803 -4.2701287 -4.2894588 -4.3387413 -4.39048 -4.420887 -4.41469 -4.3750029 -4.3342066 -4.3246727 -4.3256392]]...]
INFO - root - 2017-12-07 20:26:32.024985: step 46310, loss = 21.04, batch loss = 20.96 (7.9 examples/sec; 1.017 sec/batch; 80h:49m:31s remains)
INFO - root - 2017-12-07 20:26:41.442128: step 46320, loss = 21.48, batch loss = 21.40 (8.4 examples/sec; 0.953 sec/batch; 75h:44m:20s remains)
INFO - root - 2017-12-07 20:26:50.794940: step 46330, loss = 21.75, batch loss = 21.67 (8.4 examples/sec; 0.948 sec/batch; 75h:22m:08s remains)
INFO - root - 2017-12-07 20:27:00.334363: step 46340, loss = 21.43, batch loss = 21.35 (8.3 examples/sec; 0.966 sec/batch; 76h:48m:02s remains)
INFO - root - 2017-12-07 20:27:09.489251: step 46350, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.956 sec/batch; 75h:57m:36s remains)
INFO - root - 2017-12-07 20:27:18.913878: step 46360, loss = 21.44, batch loss = 21.36 (8.5 examples/sec; 0.941 sec/batch; 74h:48m:31s remains)
INFO - root - 2017-12-07 20:27:28.275937: step 46370, loss = 21.33, batch loss = 21.25 (9.2 examples/sec; 0.870 sec/batch; 69h:07m:48s remains)
INFO - root - 2017-12-07 20:27:37.585410: step 46380, loss = 21.12, batch loss = 21.04 (8.9 examples/sec; 0.900 sec/batch; 71h:31m:04s remains)
INFO - root - 2017-12-07 20:27:47.016313: step 46390, loss = 21.63, batch loss = 21.54 (8.6 examples/sec; 0.927 sec/batch; 73h:40m:31s remains)
INFO - root - 2017-12-07 20:27:56.373693: step 46400, loss = 21.41, batch loss = 21.33 (8.8 examples/sec; 0.909 sec/batch; 72h:16m:29s remains)
2017-12-07 20:27:57.335120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2806749 -4.285068 -4.3071427 -4.3187566 -4.3351984 -4.3175898 -4.2658954 -4.21794 -4.1839113 -4.1874909 -4.2308426 -4.2822089 -4.2809057 -4.2107253 -4.1285524][-4.2253308 -4.2078943 -4.2114835 -4.22338 -4.2572756 -4.2502756 -4.1978016 -4.145184 -4.1153479 -4.140626 -4.2143292 -4.2982416 -4.316813 -4.2541642 -4.1751027][-4.2101846 -4.1551347 -4.1288161 -4.1333079 -4.1776671 -4.1815586 -4.1364651 -4.0875664 -4.0687184 -4.1170177 -4.20458 -4.2939119 -4.3165421 -4.2531333 -4.172617][-4.2540669 -4.1630716 -4.107585 -4.0991483 -4.1381764 -4.1486487 -4.1144257 -4.0746083 -4.0683246 -4.1241493 -4.2012434 -4.2702913 -4.2833972 -4.2192311 -4.1386371][-4.3298154 -4.2233353 -4.1565995 -4.13831 -4.1553807 -4.151526 -4.1156597 -4.0803709 -4.0876827 -4.1468973 -4.2133317 -4.2684345 -4.2717819 -4.2085791 -4.1237941][-4.3703952 -4.2763925 -4.2238054 -4.2087994 -4.1984024 -4.1592746 -4.1047597 -4.0705857 -4.0935755 -4.1613455 -4.2352862 -4.3002067 -4.3131461 -4.2652903 -4.1800413][-4.3374662 -4.2741847 -4.265379 -4.2809305 -4.2588682 -4.1855617 -4.108788 -4.0731635 -4.1048088 -4.1731906 -4.2522511 -4.32288 -4.3466163 -4.3287196 -4.2644353][-4.2594676 -4.2251019 -4.2648897 -4.3192015 -4.3002605 -4.2058082 -4.1173215 -4.0859747 -4.1248875 -4.1923146 -4.2695189 -4.3299 -4.3519306 -4.3568749 -4.3140903][-4.2280097 -4.1973462 -4.239543 -4.2909613 -4.2686872 -4.1783857 -4.1048112 -4.0955367 -4.1530218 -4.2269635 -4.2977648 -4.3368316 -4.3451967 -4.3518629 -4.3158083][-4.2765245 -4.23529 -4.2408409 -4.2486296 -4.214767 -4.145081 -4.0970688 -4.1117129 -4.1920438 -4.2768464 -4.3321404 -4.3407083 -4.3249974 -4.3110476 -4.2674012][-4.3623662 -4.3156495 -4.2796388 -4.2409797 -4.2019935 -4.1533341 -4.117877 -4.1396623 -4.230361 -4.3160386 -4.3538814 -4.3402166 -4.3070474 -4.2690353 -4.2114182][-4.4279995 -4.3809052 -4.3257818 -4.2662644 -4.2328238 -4.1990352 -4.1659012 -4.1803179 -4.2563381 -4.3238935 -4.3421435 -4.3206387 -4.2885923 -4.2491035 -4.1954365][-4.44043 -4.3943434 -4.3385305 -4.2757745 -4.2447538 -4.2176223 -4.1889396 -4.1931314 -4.2357173 -4.2682886 -4.2673254 -4.2558937 -4.2546954 -4.2471657 -4.2281613][-4.4124537 -4.36065 -4.3023586 -4.2314467 -4.1911845 -4.1707339 -4.1595907 -4.1576734 -4.1618309 -4.1639271 -4.1594596 -4.1759892 -4.2164812 -4.2449274 -4.2582526][-4.372726 -4.2971535 -4.2159805 -4.126513 -4.0801926 -4.0785866 -4.0986161 -4.1012225 -4.0895228 -4.0832014 -4.085804 -4.1244717 -4.1831756 -4.2163787 -4.2330184]]...]
INFO - root - 2017-12-07 20:28:06.697591: step 46410, loss = 21.20, batch loss = 21.11 (8.5 examples/sec; 0.940 sec/batch; 74h:44m:24s remains)
INFO - root - 2017-12-07 20:28:16.207346: step 46420, loss = 22.23, batch loss = 22.14 (8.0 examples/sec; 0.996 sec/batch; 79h:08m:28s remains)
INFO - root - 2017-12-07 20:28:25.339787: step 46430, loss = 21.94, batch loss = 21.85 (8.5 examples/sec; 0.937 sec/batch; 74h:27m:46s remains)
INFO - root - 2017-12-07 20:28:34.710081: step 46440, loss = 21.54, batch loss = 21.46 (8.4 examples/sec; 0.947 sec/batch; 75h:14m:35s remains)
INFO - root - 2017-12-07 20:28:44.170901: step 46450, loss = 21.24, batch loss = 21.15 (9.1 examples/sec; 0.880 sec/batch; 69h:55m:30s remains)
INFO - root - 2017-12-07 20:28:53.639542: step 46460, loss = 21.75, batch loss = 21.66 (8.7 examples/sec; 0.915 sec/batch; 72h:44m:02s remains)
INFO - root - 2017-12-07 20:29:03.000082: step 46470, loss = 21.01, batch loss = 20.93 (8.3 examples/sec; 0.961 sec/batch; 76h:21m:33s remains)
INFO - root - 2017-12-07 20:29:12.408348: step 46480, loss = 21.76, batch loss = 21.68 (8.1 examples/sec; 0.983 sec/batch; 78h:04m:53s remains)
INFO - root - 2017-12-07 20:29:21.694443: step 46490, loss = 21.46, batch loss = 21.37 (8.2 examples/sec; 0.975 sec/batch; 77h:27m:59s remains)
INFO - root - 2017-12-07 20:29:31.001322: step 46500, loss = 20.95, batch loss = 20.87 (8.8 examples/sec; 0.908 sec/batch; 72h:06m:15s remains)
2017-12-07 20:29:31.930275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3438468 -4.4052958 -4.4658623 -4.5023375 -4.5062046 -4.4857993 -4.4621477 -4.4589281 -4.4706926 -4.4796448 -4.4691372 -4.4422469 -4.4118457 -4.3880062 -4.3657589][-4.4086995 -4.5186481 -4.6181126 -4.6647005 -4.6461282 -4.5865669 -4.5360847 -4.5345612 -4.5610576 -4.5712256 -4.5404077 -4.4827771 -4.4274483 -4.3967166 -4.378715][-4.4642286 -4.631865 -4.768075 -4.8072038 -4.7443113 -4.6328425 -4.5590849 -4.5819058 -4.6458626 -4.6659813 -4.6136742 -4.5192585 -4.426373 -4.3786783 -4.3670354][-4.50831 -4.7257328 -4.8777318 -4.8849087 -4.7580128 -4.5824986 -4.4856906 -4.5482693 -4.6716466 -4.7246647 -4.6780205 -4.572403 -4.4479179 -4.3710265 -4.3575954][-4.522891 -4.7518773 -4.8757176 -4.8199162 -4.6135654 -4.3612547 -4.2331548 -4.3377228 -4.540308 -4.6685886 -4.6839676 -4.6205683 -4.4945936 -4.389246 -4.3652353][-4.4871483 -4.6692328 -4.7193303 -4.58312 -4.2989407 -3.9756413 -3.8180568 -3.963186 -4.2531514 -4.4807191 -4.59672 -4.6177549 -4.52592 -4.4115748 -4.3806267][-4.40752 -4.5102124 -4.4859614 -4.2888179 -3.9474254 -3.5711505 -3.4037509 -3.5943727 -3.9591451 -4.2642035 -4.4636941 -4.5600605 -4.5092497 -4.4037757 -4.3780665][-4.3348689 -4.3876457 -4.3440108 -4.1393852 -3.77868 -3.378583 -3.2199502 -3.4409778 -3.8338881 -4.158535 -4.3822689 -4.5006084 -4.4593477 -4.3598733 -4.344388][-4.3231821 -4.3799133 -4.3763695 -4.2118711 -3.8665795 -3.4672649 -3.322196 -3.5455728 -3.9172888 -4.2210584 -4.427012 -4.516139 -4.4380198 -4.3173919 -4.2983489][-4.3577614 -4.4447565 -4.4968071 -4.3914723 -4.0902572 -3.7194884 -3.5879679 -3.7845111 -4.10695 -4.3796039 -4.55438 -4.591722 -4.4580441 -4.302568 -4.2666011][-4.3896832 -4.4937787 -4.5827289 -4.5390105 -4.309864 -4.0015306 -3.8906577 -4.0493283 -4.3123503 -4.5455747 -4.6769166 -4.66016 -4.491261 -4.3212485 -4.2690182][-4.3963356 -4.4935861 -4.5901361 -4.5968242 -4.4571643 -4.2440619 -4.1704621 -4.2980733 -4.4989657 -4.6686149 -4.7361388 -4.6770906 -4.5110164 -4.3540478 -4.2917938][-4.3857875 -4.4652381 -4.5518379 -4.5892406 -4.5298295 -4.4143476 -4.3780479 -4.4708333 -4.6031222 -4.6969385 -4.7058787 -4.6303859 -4.4930859 -4.3655748 -4.30153][-4.3658419 -4.4252067 -4.4911432 -4.5324674 -4.5230641 -4.479928 -4.47167 -4.5268793 -4.594388 -4.6263552 -4.6029091 -4.5321193 -4.43123 -4.3377542 -4.282403][-4.3292851 -4.3663411 -4.4063 -4.4341035 -4.4430089 -4.4398484 -4.4467072 -4.4729056 -4.4961824 -4.495719 -4.466032 -4.4116774 -4.345211 -4.2860689 -4.249052]]...]
INFO - root - 2017-12-07 20:29:41.270073: step 46510, loss = 21.51, batch loss = 21.43 (8.3 examples/sec; 0.964 sec/batch; 76h:34m:28s remains)
INFO - root - 2017-12-07 20:29:50.547861: step 46520, loss = 21.28, batch loss = 21.20 (8.5 examples/sec; 0.938 sec/batch; 74h:31m:28s remains)
INFO - root - 2017-12-07 20:29:59.844574: step 46530, loss = 21.42, batch loss = 21.34 (8.4 examples/sec; 0.955 sec/batch; 75h:53m:07s remains)
INFO - root - 2017-12-07 20:30:09.287468: step 46540, loss = 21.38, batch loss = 21.30 (8.7 examples/sec; 0.923 sec/batch; 73h:17m:38s remains)
INFO - root - 2017-12-07 20:30:18.748544: step 46550, loss = 21.53, batch loss = 21.45 (8.0 examples/sec; 1.002 sec/batch; 79h:34m:03s remains)
INFO - root - 2017-12-07 20:30:28.105274: step 46560, loss = 21.57, batch loss = 21.49 (8.5 examples/sec; 0.943 sec/batch; 74h:52m:47s remains)
INFO - root - 2017-12-07 20:30:37.543749: step 46570, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.952 sec/batch; 75h:36m:41s remains)
INFO - root - 2017-12-07 20:30:46.940199: step 46580, loss = 21.44, batch loss = 21.35 (8.2 examples/sec; 0.976 sec/batch; 77h:29m:08s remains)
INFO - root - 2017-12-07 20:30:56.304244: step 46590, loss = 21.49, batch loss = 21.41 (8.2 examples/sec; 0.975 sec/batch; 77h:28m:16s remains)
INFO - root - 2017-12-07 20:31:05.453530: step 46600, loss = 21.53, batch loss = 21.44 (9.0 examples/sec; 0.891 sec/batch; 70h:47m:14s remains)
2017-12-07 20:31:06.322319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6137567 -4.6247635 -4.5823364 -4.4808779 -4.3805194 -4.3242283 -4.3174939 -4.3612595 -4.4573436 -4.5990763 -4.7097712 -4.7189965 -4.6544437 -4.5964761 -4.5855002][-4.6938772 -4.6912918 -4.6467662 -4.5668883 -4.4918556 -4.4446073 -4.4353914 -4.4648128 -4.5455923 -4.6665626 -4.7508183 -4.7350378 -4.654223 -4.6041923 -4.6158895][-4.7243428 -4.7196012 -4.6924019 -4.6485472 -4.5944195 -4.532876 -4.4867339 -4.4853969 -4.5561566 -4.6758881 -4.7538681 -4.7214589 -4.6229982 -4.5816164 -4.6187406][-4.6589336 -4.6642075 -4.679193 -4.6908565 -4.6639266 -4.5737433 -4.4580379 -4.4010196 -4.4700427 -4.622395 -4.7262325 -4.6868925 -4.5615692 -4.5227747 -4.5823555][-4.5064993 -4.5256715 -4.5909982 -4.6541276 -4.6461067 -4.5215621 -4.3273449 -4.20939 -4.293539 -4.5118079 -4.6704888 -4.6340384 -4.47206 -4.4219203 -4.4942675][-4.3566494 -4.3879995 -4.4808221 -4.5563941 -4.5322604 -4.3590875 -4.089335 -3.9257362 -4.0404615 -4.3408437 -4.5715532 -4.5518289 -4.35341 -4.2774611 -4.3532982][-4.2991967 -4.3371639 -4.4161358 -4.4477386 -4.3580441 -4.1085672 -3.7658405 -3.5827215 -3.7486291 -4.1368375 -4.45012 -4.46962 -4.2455983 -4.1311226 -4.1955895][-4.3334475 -4.366848 -4.4057827 -4.3702936 -4.1984091 -3.8711324 -3.4786618 -3.298636 -3.5147433 -3.9755118 -4.3685756 -4.446291 -4.2167673 -4.0554013 -4.087852][-4.4209995 -4.4428883 -4.43953 -4.3582034 -4.1465411 -3.7913561 -3.3961751 -3.2321124 -3.459698 -3.9376438 -4.3754268 -4.51581 -4.3123355 -4.1186686 -4.1106281][-4.5219312 -4.5365191 -4.5078297 -4.4213753 -4.2390385 -3.928467 -3.57547 -3.4259486 -3.6247544 -4.0591812 -4.4812388 -4.65513 -4.5022221 -4.3108182 -4.2679973][-4.5955191 -4.62212 -4.59463 -4.5396318 -4.4327645 -4.2164974 -3.9341321 -3.794867 -3.9330416 -4.2704434 -4.6145177 -4.7765713 -4.6831994 -4.5374813 -4.4873037][-4.6381578 -4.6952038 -4.6956458 -4.6803613 -4.6424422 -4.5204425 -4.3155022 -4.1872969 -4.2556729 -4.4748249 -4.7120824 -4.8321157 -4.783648 -4.6967344 -4.6630034][-4.6307406 -4.7085662 -4.7387214 -4.7461729 -4.7419624 -4.6849465 -4.5554862 -4.4542394 -4.4771795 -4.6050224 -4.7502465 -4.8197093 -4.7845764 -4.7293458 -4.7075171][-4.5728378 -4.6438193 -4.6825604 -4.6916585 -4.6923404 -4.6690593 -4.6003156 -4.5378065 -4.5464134 -4.6232891 -4.7085037 -4.7392621 -4.7042418 -4.6585264 -4.6396904][-4.4811163 -4.5247374 -4.5519094 -4.5565672 -4.559123 -4.5558758 -4.5282059 -4.4987516 -4.5075727 -4.5540366 -4.5990148 -4.6067686 -4.5747967 -4.5381241 -4.5234632]]...]
INFO - root - 2017-12-07 20:31:15.691572: step 46610, loss = 21.99, batch loss = 21.91 (8.2 examples/sec; 0.976 sec/batch; 77h:28m:31s remains)
INFO - root - 2017-12-07 20:31:25.126919: step 46620, loss = 21.36, batch loss = 21.28 (8.1 examples/sec; 0.988 sec/batch; 78h:27m:30s remains)
INFO - root - 2017-12-07 20:31:34.478621: step 46630, loss = 21.60, batch loss = 21.52 (9.0 examples/sec; 0.892 sec/batch; 70h:50m:51s remains)
INFO - root - 2017-12-07 20:31:43.700580: step 46640, loss = 20.79, batch loss = 20.70 (9.0 examples/sec; 0.891 sec/batch; 70h:43m:05s remains)
INFO - root - 2017-12-07 20:31:53.103758: step 46650, loss = 21.01, batch loss = 20.92 (8.9 examples/sec; 0.902 sec/batch; 71h:38m:38s remains)
INFO - root - 2017-12-07 20:32:02.421701: step 46660, loss = 21.42, batch loss = 21.34 (8.5 examples/sec; 0.945 sec/batch; 75h:00m:34s remains)
INFO - root - 2017-12-07 20:32:11.727984: step 46670, loss = 21.32, batch loss = 21.24 (8.6 examples/sec; 0.932 sec/batch; 73h:58m:30s remains)
INFO - root - 2017-12-07 20:32:21.169821: step 46680, loss = 21.07, batch loss = 20.98 (8.2 examples/sec; 0.981 sec/batch; 77h:53m:32s remains)
INFO - root - 2017-12-07 20:32:30.431814: step 46690, loss = 21.40, batch loss = 21.31 (8.0 examples/sec; 0.996 sec/batch; 79h:03m:50s remains)
INFO - root - 2017-12-07 20:32:39.894159: step 46700, loss = 21.14, batch loss = 21.06 (8.2 examples/sec; 0.974 sec/batch; 77h:21m:20s remains)
2017-12-07 20:32:40.943203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5346437 -4.5028539 -4.4368687 -4.36434 -4.3210669 -4.3135638 -4.3004818 -4.3152018 -4.3439174 -4.3569551 -4.3601928 -4.3744059 -4.3925672 -4.4272156 -4.47206][-4.5440292 -4.5193729 -4.4572482 -4.373929 -4.3197103 -4.3120322 -4.3106561 -4.353971 -4.4152627 -4.4457111 -4.4515328 -4.4604163 -4.4654551 -4.4913344 -4.5493917][-4.5321093 -4.5234022 -4.4743567 -4.3900456 -4.3215408 -4.2915192 -4.2766352 -4.3235321 -4.4099655 -4.4710293 -4.4978566 -4.5177631 -4.5275278 -4.5485148 -4.6032495][-4.5353794 -4.547945 -4.5138483 -4.4264894 -4.3281469 -4.2373161 -4.1668119 -4.1858368 -4.2793674 -4.3743582 -4.4438682 -4.5056572 -4.5516043 -4.5918489 -4.6424417][-4.5561037 -4.59745 -4.5810175 -4.4912424 -4.354701 -4.179625 -4.0173903 -3.9759412 -4.054101 -4.1827178 -4.3171916 -4.4448338 -4.5434294 -4.6194849 -4.6736073][-4.5700536 -4.64007 -4.6435661 -4.5549517 -4.3858953 -4.1404762 -3.897697 -3.7989788 -3.8555462 -4.0101213 -4.2042046 -4.389307 -4.5260916 -4.6286426 -4.6854596][-4.5903525 -4.6747775 -4.6849566 -4.5896063 -4.4011555 -4.1364388 -3.8803117 -3.7727942 -3.8249884 -3.9893844 -4.2030621 -4.4030151 -4.5399518 -4.6357965 -4.67785][-4.6101689 -4.6883459 -4.6905589 -4.5884433 -4.4029565 -4.1696205 -3.9677863 -3.9041197 -3.9697938 -4.1241283 -4.3067641 -4.4674444 -4.5703263 -4.6346087 -4.6465693][-4.619792 -4.6643515 -4.643404 -4.5339618 -4.3569207 -4.1656418 -4.03869 -4.0382071 -4.1261816 -4.2685895 -4.4049668 -4.51105 -4.5791717 -4.6186543 -4.6038389][-4.6177168 -4.6200266 -4.5656776 -4.44413 -4.2662315 -4.097086 -4.024056 -4.0693283 -4.174305 -4.3186641 -4.4339285 -4.511754 -4.567595 -4.5989332 -4.567627][-4.6059375 -4.584362 -4.5089116 -4.3773 -4.1906428 -4.0269723 -3.9810555 -4.0484967 -4.1627016 -4.3140287 -4.433188 -4.5084538 -4.5570035 -4.5751 -4.5278311][-4.586319 -4.5699205 -4.4936128 -4.3640637 -4.1872945 -4.043447 -4.0171537 -4.092855 -4.210165 -4.3615618 -4.4851351 -4.5588474 -4.5847087 -4.5716763 -4.5071487][-4.5644403 -4.5709128 -4.5180926 -4.4111595 -4.2748752 -4.1790862 -4.1755276 -4.2499123 -4.3601952 -4.4965591 -4.6113605 -4.6733885 -4.6664209 -4.6138015 -4.5341773][-4.5396371 -4.5760903 -4.5643535 -4.4974875 -4.414237 -4.3684015 -4.3790579 -4.4465494 -4.5434785 -4.6563873 -4.7516994 -4.7954659 -4.7626767 -4.6841989 -4.5983524][-4.5101328 -4.5631623 -4.5813375 -4.5491586 -4.5069766 -4.4931283 -4.5110407 -4.5725636 -4.6564231 -4.7491827 -4.8237319 -4.8475056 -4.8008909 -4.7166233 -4.6375484]]...]
INFO - root - 2017-12-07 20:32:50.251657: step 46710, loss = 21.55, batch loss = 21.46 (9.2 examples/sec; 0.866 sec/batch; 68h:43m:54s remains)
INFO - root - 2017-12-07 20:32:59.626101: step 46720, loss = 21.29, batch loss = 21.21 (8.6 examples/sec; 0.932 sec/batch; 74h:00m:46s remains)
INFO - root - 2017-12-07 20:33:08.877619: step 46730, loss = 21.50, batch loss = 21.42 (9.1 examples/sec; 0.883 sec/batch; 70h:04m:42s remains)
INFO - root - 2017-12-07 20:33:18.310984: step 46740, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.963 sec/batch; 76h:27m:23s remains)
INFO - root - 2017-12-07 20:33:27.729029: step 46750, loss = 21.12, batch loss = 21.03 (8.7 examples/sec; 0.919 sec/batch; 72h:56m:03s remains)
INFO - root - 2017-12-07 20:33:37.070543: step 46760, loss = 21.73, batch loss = 21.65 (8.2 examples/sec; 0.977 sec/batch; 77h:31m:01s remains)
INFO - root - 2017-12-07 20:33:46.252267: step 46770, loss = 21.64, batch loss = 21.55 (8.7 examples/sec; 0.916 sec/batch; 72h:41m:55s remains)
INFO - root - 2017-12-07 20:33:55.607120: step 46780, loss = 21.47, batch loss = 21.39 (9.1 examples/sec; 0.880 sec/batch; 69h:52m:06s remains)
INFO - root - 2017-12-07 20:34:05.094613: step 46790, loss = 21.27, batch loss = 21.18 (8.8 examples/sec; 0.907 sec/batch; 71h:56m:49s remains)
INFO - root - 2017-12-07 20:34:14.575021: step 46800, loss = 21.14, batch loss = 21.06 (8.2 examples/sec; 0.980 sec/batch; 77h:45m:29s remains)
2017-12-07 20:34:15.627041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4222846 -4.5047665 -4.5532842 -4.5543995 -4.5295081 -4.5081139 -4.5058088 -4.506279 -4.5021667 -4.4892187 -4.4664626 -4.4563165 -4.478972 -4.5298538 -4.5779219][-4.3591404 -4.4415069 -4.4735146 -4.4601817 -4.4427905 -4.4431691 -4.4584074 -4.4678822 -4.4663029 -4.4525752 -4.4249282 -4.3982873 -4.3991628 -4.4352622 -4.4920793][-4.3076086 -4.3667011 -4.3637309 -4.3308277 -4.3196306 -4.3413143 -4.3699603 -4.3852859 -4.3893332 -4.3829803 -4.3655629 -4.3435855 -4.3361354 -4.3598938 -4.4163628][-4.2861056 -4.3080349 -4.2597432 -4.1961975 -4.1746621 -4.2026124 -4.2342906 -4.2489071 -4.2612724 -4.2742186 -4.2862086 -4.2943482 -4.2991238 -4.3181033 -4.3697009][-4.2688642 -4.2588391 -4.1815181 -4.094667 -4.0526056 -4.0614543 -4.0753403 -4.072742 -4.0844593 -4.1149921 -4.1539268 -4.1925831 -4.2215 -4.2553678 -4.3131657][-4.2449951 -4.2287111 -4.1588674 -4.074234 -4.0117283 -3.982353 -3.9590294 -3.9236097 -3.9143982 -3.93789 -3.9750066 -4.025672 -4.084146 -4.1550879 -4.2391844][-4.2406731 -4.2410073 -4.1999083 -4.1315475 -4.0559559 -3.9899435 -3.9293277 -3.8650813 -3.8255317 -3.8166742 -3.8263743 -3.8761485 -3.9690263 -4.0876541 -4.2000222][-4.2687306 -4.2889786 -4.2768893 -4.2297926 -4.15256 -4.0712438 -3.9976561 -3.9303429 -3.8730919 -3.8270712 -3.7973068 -3.8305104 -3.939198 -4.0838785 -4.2039347][-4.3094277 -4.3449817 -4.3559833 -4.3347254 -4.2751646 -4.2008891 -4.1337504 -4.0774856 -4.0138431 -3.9350784 -3.8683982 -3.8818691 -3.9919634 -4.1409926 -4.2520914][-4.3578234 -4.3954358 -4.4125657 -4.4072223 -4.3739433 -4.3259392 -4.2822738 -4.2427506 -4.1764832 -4.0795221 -3.9983013 -4.0052166 -4.1127896 -4.2473865 -4.3308206][-4.377212 -4.4064312 -4.42077 -4.4249277 -4.4177175 -4.4073792 -4.4013987 -4.3834815 -4.3196716 -4.2214432 -4.1473804 -4.1584716 -4.2486329 -4.3358188 -4.359201][-4.3543081 -4.3746185 -4.3938341 -4.4161081 -4.4328055 -4.4510369 -4.4702253 -4.46446 -4.4039373 -4.3165736 -4.2641921 -4.2795177 -4.335115 -4.3529758 -4.29806][-4.3324203 -4.3469329 -4.3726063 -4.4085665 -4.4366078 -4.4609208 -4.4824963 -4.4793496 -4.4323521 -4.3720794 -4.3456903 -4.3551693 -4.3630123 -4.3114958 -4.1949387][-4.3322673 -4.3430042 -4.3675103 -4.40216 -4.425355 -4.4387093 -4.4477353 -4.4427233 -4.4178286 -4.3959079 -4.3977051 -4.4007959 -4.3699875 -4.2744288 -4.1322231][-4.3156209 -4.3181758 -4.3376513 -4.3648815 -4.3796377 -4.3815603 -4.3766036 -4.3705063 -4.3711548 -4.3932033 -4.4292178 -4.4374938 -4.3832874 -4.2631593 -4.1170106]]...]
INFO - root - 2017-12-07 20:34:25.156282: step 46810, loss = 21.85, batch loss = 21.77 (7.8 examples/sec; 1.023 sec/batch; 81h:13m:12s remains)
INFO - root - 2017-12-07 20:34:34.539246: step 46820, loss = 21.26, batch loss = 21.18 (8.6 examples/sec; 0.926 sec/batch; 73h:30m:20s remains)
INFO - root - 2017-12-07 20:34:43.879447: step 46830, loss = 21.35, batch loss = 21.27 (8.6 examples/sec; 0.931 sec/batch; 73h:53m:59s remains)
INFO - root - 2017-12-07 20:34:53.048313: step 46840, loss = 21.39, batch loss = 21.31 (8.6 examples/sec; 0.928 sec/batch; 73h:36m:03s remains)
INFO - root - 2017-12-07 20:35:02.373069: step 46850, loss = 21.38, batch loss = 21.30 (8.9 examples/sec; 0.897 sec/batch; 71h:11m:12s remains)
INFO - root - 2017-12-07 20:35:11.785982: step 46860, loss = 21.72, batch loss = 21.63 (9.2 examples/sec; 0.870 sec/batch; 69h:00m:10s remains)
INFO - root - 2017-12-07 20:35:21.312156: step 46870, loss = 21.30, batch loss = 21.22 (8.4 examples/sec; 0.947 sec/batch; 75h:08m:14s remains)
INFO - root - 2017-12-07 20:35:30.722656: step 46880, loss = 21.59, batch loss = 21.51 (8.2 examples/sec; 0.975 sec/batch; 77h:19m:42s remains)
INFO - root - 2017-12-07 20:35:40.100075: step 46890, loss = 21.16, batch loss = 21.08 (7.7 examples/sec; 1.034 sec/batch; 82h:03m:35s remains)
INFO - root - 2017-12-07 20:35:49.572793: step 46900, loss = 21.83, batch loss = 21.75 (8.3 examples/sec; 0.970 sec/batch; 76h:54m:50s remains)
2017-12-07 20:35:50.478784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5385118 -4.49482 -4.4145646 -4.3831172 -4.3937769 -4.3877282 -4.3583865 -4.310318 -4.2620788 -4.2375727 -4.2490611 -4.2644882 -4.2212977 -4.1807146 -4.2330084][-4.6716304 -4.6312542 -4.5544853 -4.5098429 -4.5057778 -4.4855189 -4.4458437 -4.4064059 -4.3866396 -4.3907514 -4.4179173 -4.4190488 -4.3304005 -4.2482686 -4.2738962][-4.6375189 -4.6058421 -4.5524898 -4.5306873 -4.5504375 -4.5501323 -4.5259376 -4.5032916 -4.4948196 -4.5144053 -4.5602474 -4.56684 -4.4711261 -4.3849711 -4.4034619][-4.4247508 -4.4072948 -4.3825369 -4.4058414 -4.4851789 -4.5420218 -4.5487814 -4.5183368 -4.4736891 -4.4741096 -4.5382729 -4.5816112 -4.5343776 -4.496623 -4.5308347][-4.1979752 -4.1949687 -4.1821084 -4.2346692 -4.3605928 -4.4648418 -4.4765334 -4.390996 -4.2663183 -4.2270622 -4.3187237 -4.4302855 -4.4733138 -4.5136976 -4.5734973][-4.0708613 -4.090929 -4.0920248 -4.16156 -4.302227 -4.4069672 -4.3657541 -4.1757722 -3.9495842 -3.8726766 -4.0127659 -4.2125711 -4.3409295 -4.4251914 -4.479816][-4.018846 -4.057272 -4.08522 -4.1709414 -4.2911568 -4.3374066 -4.1967278 -3.8964586 -3.5984669 -3.5175722 -3.7260323 -4.0174384 -4.2014561 -4.2774773 -4.2984109][-4.0380988 -4.091476 -4.15858 -4.2624145 -4.3429155 -4.2979441 -4.05168 -3.678761 -3.3596616 -3.2955606 -3.5526915 -3.9039285 -4.1160979 -4.1691651 -4.1646347][-4.1396146 -4.1978555 -4.2932382 -4.4086089 -4.4592838 -4.356257 -4.0679584 -3.6996806 -3.4121904 -3.359092 -3.6006408 -3.9362829 -4.1349964 -4.1586 -4.1348648][-4.3195887 -4.3632226 -4.4489927 -4.5499868 -4.5850425 -4.480638 -4.225306 -3.9218507 -3.691226 -3.6354783 -3.8049593 -4.0595112 -4.2171316 -4.221571 -4.1917987][-4.51853 -4.5403514 -4.5913796 -4.6610732 -4.6926856 -4.6257868 -4.4478431 -4.2346797 -4.0683126 -4.0113254 -4.0943913 -4.2370405 -4.3318391 -4.3187089 -4.2883687][-4.651823 -4.6650677 -4.6885605 -4.7310338 -4.7619066 -4.7362719 -4.636025 -4.5125933 -4.41675 -4.3820052 -4.4127507 -4.461771 -4.4878373 -4.4529476 -4.4261594][-4.6874804 -4.7044172 -4.712162 -4.7366357 -4.7613964 -4.7504473 -4.689764 -4.6216984 -4.577477 -4.5706954 -4.5873413 -4.5928955 -4.5859318 -4.5579982 -4.5615177][-4.6245475 -4.6521564 -4.6569052 -4.6792197 -4.7080894 -4.7035775 -4.6572433 -4.6116385 -4.5917721 -4.603601 -4.6266475 -4.6293278 -4.6232495 -4.6214905 -4.661674][-4.5133195 -4.54035 -4.5401282 -4.5659246 -4.6067395 -4.6134791 -4.5751863 -4.5398474 -4.53478 -4.5600438 -4.5930786 -4.60234 -4.60612 -4.6276631 -4.6905274]]...]
INFO - root - 2017-12-07 20:35:59.773651: step 46910, loss = 21.63, batch loss = 21.55 (8.4 examples/sec; 0.951 sec/batch; 75h:25m:57s remains)
INFO - root - 2017-12-07 20:36:09.294592: step 46920, loss = 21.30, batch loss = 21.22 (8.0 examples/sec; 1.000 sec/batch; 79h:20m:00s remains)
INFO - root - 2017-12-07 20:36:18.777540: step 46930, loss = 21.53, batch loss = 21.45 (8.8 examples/sec; 0.912 sec/batch; 72h:19m:07s remains)
INFO - root - 2017-12-07 20:36:28.021111: step 46940, loss = 21.27, batch loss = 21.19 (9.7 examples/sec; 0.825 sec/batch; 65h:27m:36s remains)
INFO - root - 2017-12-07 20:36:37.156649: step 46950, loss = 21.66, batch loss = 21.58 (8.5 examples/sec; 0.939 sec/batch; 74h:28m:30s remains)
INFO - root - 2017-12-07 20:36:46.571452: step 46960, loss = 21.56, batch loss = 21.48 (8.0 examples/sec; 1.001 sec/batch; 79h:23m:28s remains)
INFO - root - 2017-12-07 20:36:55.954839: step 46970, loss = 21.87, batch loss = 21.79 (8.5 examples/sec; 0.943 sec/batch; 74h:47m:01s remains)
INFO - root - 2017-12-07 20:37:05.374018: step 46980, loss = 21.66, batch loss = 21.57 (8.7 examples/sec; 0.921 sec/batch; 73h:04m:26s remains)
INFO - root - 2017-12-07 20:37:14.845089: step 46990, loss = 21.88, batch loss = 21.80 (9.1 examples/sec; 0.880 sec/batch; 69h:46m:52s remains)
INFO - root - 2017-12-07 20:37:24.241809: step 47000, loss = 21.60, batch loss = 21.52 (8.5 examples/sec; 0.943 sec/batch; 74h:45m:21s remains)
2017-12-07 20:37:25.227862: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3729148 -4.3595381 -4.3770022 -4.409831 -4.4274292 -4.4306908 -4.4464 -4.4958744 -4.5385141 -4.5543132 -4.5675526 -4.5802832 -4.5686793 -4.53791 -4.5010386][-4.4441652 -4.4454322 -4.4653039 -4.4829354 -4.4880581 -4.4890962 -4.5053644 -4.5568123 -4.6050835 -4.6231251 -4.6280379 -4.6366682 -4.6233087 -4.5812631 -4.5175118][-4.5135264 -4.5339947 -4.5477657 -4.5485697 -4.54761 -4.5532165 -4.5745215 -4.6227231 -4.6642842 -4.6724687 -4.6621428 -4.6558137 -4.6285439 -4.5682797 -4.48175][-4.5348606 -4.5551739 -4.556839 -4.5494275 -4.5538363 -4.5650988 -4.5871367 -4.6244349 -4.6482277 -4.6394219 -4.6133876 -4.5971303 -4.5618711 -4.4939408 -4.4084549][-4.5027361 -4.522378 -4.51973 -4.5124593 -4.5139642 -4.5066752 -4.5061984 -4.5160713 -4.5106864 -4.4812918 -4.4472404 -4.4437714 -4.4364791 -4.3976564 -4.3483562][-4.4169965 -4.44938 -4.4619217 -4.4595718 -4.4359679 -4.3754783 -4.320971 -4.2909694 -4.262136 -4.23213 -4.2161036 -4.2551289 -4.3031774 -4.3111811 -4.3008041][-4.2864718 -4.331068 -4.3679333 -4.3737516 -4.3157835 -4.1964374 -4.0822358 -4.0162687 -3.9839993 -3.9728479 -3.9902754 -4.0760121 -4.1704173 -4.2056193 -4.2115526][-4.1240921 -4.176374 -4.2378654 -4.2543154 -4.1708875 -4.0199518 -3.8761353 -3.79334 -3.7678032 -3.7752724 -3.8152273 -3.927932 -4.0418558 -4.0799794 -4.0862517][-3.9698975 -4.0175786 -4.0911789 -4.1181755 -4.0392275 -3.9134321 -3.8002026 -3.7352805 -3.7211435 -3.7354047 -3.7765427 -3.8804002 -3.9776404 -3.997762 -3.9900951][-3.8925648 -3.9218817 -3.9740849 -4.0010982 -3.9641857 -3.9247398 -3.9004335 -3.8847058 -3.8810296 -3.880219 -3.884593 -3.9319382 -3.9767585 -3.9621081 -3.92965][-3.9223974 -3.9273939 -3.9402125 -3.9694877 -3.9977689 -4.0519419 -4.1074557 -4.1291242 -4.1200557 -4.0910749 -4.0516491 -4.0370646 -4.0264707 -3.9798436 -3.9318724][-4.0329 -4.0211024 -4.007607 -4.0401068 -4.1127582 -4.2148457 -4.306108 -4.3455453 -4.3382916 -4.3031077 -4.2459617 -4.1927977 -4.1402912 -4.0724092 -4.0253515][-4.177249 -4.1508675 -4.1258025 -4.1490388 -4.2207603 -4.32388 -4.4300742 -4.4991765 -4.5212235 -4.4971647 -4.4270082 -4.3424249 -4.258215 -4.1780343 -4.1411643][-4.2602057 -4.236681 -4.2275786 -4.2433805 -4.2889719 -4.36894 -4.476172 -4.57553 -4.6317739 -4.614141 -4.5214853 -4.4061308 -4.3101029 -4.2423949 -4.2304249][-4.2387362 -4.2493372 -4.2762194 -4.292593 -4.3121176 -4.3664331 -4.4634957 -4.5776749 -4.6584115 -4.6414118 -4.5266933 -4.3897104 -4.2927217 -4.2458382 -4.26119]]...]
INFO - root - 2017-12-07 20:37:34.641447: step 47010, loss = 21.65, batch loss = 21.57 (8.3 examples/sec; 0.967 sec/batch; 76h:43m:05s remains)
INFO - root - 2017-12-07 20:37:43.980807: step 47020, loss = 21.23, batch loss = 21.15 (8.5 examples/sec; 0.942 sec/batch; 74h:39m:40s remains)
INFO - root - 2017-12-07 20:37:53.453434: step 47030, loss = 21.42, batch loss = 21.34 (8.5 examples/sec; 0.942 sec/batch; 74h:40m:02s remains)
INFO - root - 2017-12-07 20:38:02.872256: step 47040, loss = 21.51, batch loss = 21.42 (8.3 examples/sec; 0.966 sec/batch; 76h:38m:08s remains)
INFO - root - 2017-12-07 20:38:12.142968: step 47050, loss = 21.52, batch loss = 21.43 (9.0 examples/sec; 0.891 sec/batch; 70h:38m:18s remains)
INFO - root - 2017-12-07 20:38:21.420952: step 47060, loss = 21.11, batch loss = 21.03 (9.2 examples/sec; 0.866 sec/batch; 68h:40m:13s remains)
INFO - root - 2017-12-07 20:38:30.866554: step 47070, loss = 21.53, batch loss = 21.44 (8.4 examples/sec; 0.958 sec/batch; 75h:55m:16s remains)
INFO - root - 2017-12-07 20:38:40.280122: step 47080, loss = 21.35, batch loss = 21.27 (8.2 examples/sec; 0.974 sec/batch; 77h:14m:51s remains)
INFO - root - 2017-12-07 20:38:49.592083: step 47090, loss = 21.68, batch loss = 21.59 (8.4 examples/sec; 0.952 sec/batch; 75h:27m:38s remains)
INFO - root - 2017-12-07 20:38:58.965641: step 47100, loss = 21.47, batch loss = 21.39 (8.7 examples/sec; 0.923 sec/batch; 73h:08m:44s remains)
2017-12-07 20:38:59.944789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2762313 -4.2955828 -4.3191605 -4.339282 -4.3543491 -4.3623571 -4.3639212 -4.3608975 -4.357111 -4.3528523 -4.3450136 -4.3334785 -4.3193789 -4.302371 -4.281249][-4.3592377 -4.4068403 -4.4485493 -4.4751782 -4.48796 -4.4902821 -4.4850774 -4.4780278 -4.4761457 -4.480423 -4.4845362 -4.4803176 -4.4605484 -4.4218583 -4.3685417][-4.4631176 -4.531559 -4.5819178 -4.6012816 -4.593245 -4.5663991 -4.5329156 -4.5125279 -4.5234261 -4.5641508 -4.6126375 -4.641994 -4.6312857 -4.574472 -4.4835749][-4.5431609 -4.6059766 -4.6396255 -4.6294942 -4.578382 -4.4948473 -4.404366 -4.3550873 -4.3882046 -4.495183 -4.6222892 -4.7141323 -4.73653 -4.6822309 -4.5708857][-4.5522242 -4.5850434 -4.5864611 -4.5428076 -4.4482121 -4.2995968 -4.1348071 -4.0385365 -4.0880761 -4.2700353 -4.4863358 -4.6507807 -4.7200689 -4.6891942 -4.5852094][-4.4789405 -4.4852848 -4.4645314 -4.4022756 -4.2825837 -4.0847588 -3.8503354 -3.6993091 -3.7509453 -3.9927037 -4.2857518 -4.5131073 -4.6257081 -4.6241817 -4.5423603][-4.3403635 -4.3489919 -4.3324547 -4.273324 -4.1492271 -3.9251928 -3.6425138 -3.4462605 -3.4881086 -3.7594714 -4.0957046 -4.3622456 -4.5064931 -4.5328989 -4.4793429][-4.1964254 -4.2384205 -4.25125 -4.2127652 -4.0992928 -3.8704469 -3.5653317 -3.34312 -3.3711023 -3.6421337 -3.9874516 -4.2693858 -4.4321756 -4.4768896 -4.4417][-4.1085505 -4.1900291 -4.239038 -4.2333269 -4.1497035 -3.9448235 -3.6513882 -3.4309704 -3.4494352 -3.7008457 -4.0313845 -4.3097115 -4.4737587 -4.5147362 -4.472343][-4.0938029 -4.2034693 -4.283989 -4.3198776 -4.2884889 -4.1424122 -3.9036517 -3.7145035 -3.7259622 -3.9383078 -4.2244143 -4.4675117 -4.6066465 -4.6242504 -4.55713][-4.1718364 -4.2932634 -4.3919735 -4.4560046 -4.4696207 -4.3927283 -4.2331939 -4.0957394 -4.1048007 -4.2698851 -4.4913707 -4.669023 -4.7533612 -4.731595 -4.6368527][-4.3167849 -4.4265471 -4.5168691 -4.5791206 -4.6095572 -4.5869865 -4.5071359 -4.4273534 -4.4359932 -4.5506444 -4.6978426 -4.79657 -4.8127875 -4.7479992 -4.636395][-4.4469504 -4.5217185 -4.57771 -4.6108747 -4.6305261 -4.6353936 -4.6150951 -4.5847659 -4.5926204 -4.6546063 -4.7253218 -4.7513742 -4.7139521 -4.6279593 -4.5259962][-4.4899106 -4.5193653 -4.5270543 -4.5188303 -4.5136881 -4.522428 -4.5337577 -4.5373096 -4.54665 -4.5677009 -4.57863 -4.556036 -4.4979177 -4.423183 -4.3555875][-4.4411154 -4.4373403 -4.4105296 -4.3752275 -4.3535304 -4.3568497 -4.37472 -4.3891225 -4.3957863 -4.393456 -4.3735628 -4.3343587 -4.2862597 -4.2462525 -4.2222309]]...]
INFO - root - 2017-12-07 20:39:09.328764: step 47110, loss = 21.43, batch loss = 21.35 (8.4 examples/sec; 0.950 sec/batch; 75h:19m:45s remains)
INFO - root - 2017-12-07 20:39:18.594004: step 47120, loss = 21.63, batch loss = 21.55 (7.9 examples/sec; 1.007 sec/batch; 79h:49m:06s remains)
INFO - root - 2017-12-07 20:39:27.968097: step 47130, loss = 21.62, batch loss = 21.54 (7.9 examples/sec; 1.010 sec/batch; 80h:02m:57s remains)
INFO - root - 2017-12-07 20:39:37.359124: step 47140, loss = 21.57, batch loss = 21.48 (8.1 examples/sec; 0.982 sec/batch; 77h:48m:31s remains)
INFO - root - 2017-12-07 20:39:46.557552: step 47150, loss = 21.45, batch loss = 21.37 (8.8 examples/sec; 0.914 sec/batch; 72h:25m:28s remains)
INFO - root - 2017-12-07 20:39:56.101432: step 47160, loss = 21.36, batch loss = 21.27 (8.1 examples/sec; 0.986 sec/batch; 78h:10m:09s remains)
INFO - root - 2017-12-07 20:40:05.486463: step 47170, loss = 21.49, batch loss = 21.40 (8.4 examples/sec; 0.950 sec/batch; 75h:19m:21s remains)
INFO - root - 2017-12-07 20:40:14.849243: step 47180, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.942 sec/batch; 74h:37m:56s remains)
INFO - root - 2017-12-07 20:40:24.237686: step 47190, loss = 21.79, batch loss = 21.71 (9.0 examples/sec; 0.891 sec/batch; 70h:38m:53s remains)
INFO - root - 2017-12-07 20:40:33.584284: step 47200, loss = 21.30, batch loss = 21.22 (9.2 examples/sec; 0.871 sec/batch; 69h:01m:52s remains)
2017-12-07 20:40:34.528671: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5235538 -4.5413 -4.564599 -4.5892615 -4.616302 -4.6366982 -4.6405158 -4.6333714 -4.6255655 -4.6149721 -4.6029539 -4.5956097 -4.5880041 -4.5632129 -4.5148954][-4.582273 -4.6151853 -4.6468253 -4.6783147 -4.7194428 -4.7536263 -4.75458 -4.7432308 -4.74364 -4.7407107 -4.7259865 -4.7128882 -4.6984134 -4.6602769 -4.5827231][-4.626955 -4.6691313 -4.6974664 -4.716989 -4.7512255 -4.7805214 -4.7549739 -4.7262678 -4.7490354 -4.7817769 -4.7881613 -4.7847929 -4.7727489 -4.7309594 -4.6310606][-4.6471825 -4.6832108 -4.7060313 -4.7099514 -4.7168665 -4.7076888 -4.6205282 -4.5593109 -4.6203403 -4.71294 -4.7589817 -4.7794547 -4.7785482 -4.7467303 -4.6442976][-4.6076 -4.6184163 -4.642808 -4.6453996 -4.6207771 -4.5481033 -4.3773727 -4.2787151 -4.3895779 -4.5537944 -4.6473322 -4.6974297 -4.7104063 -4.696466 -4.6139612][-4.5015159 -4.4738693 -4.4987249 -4.5037684 -4.4492288 -4.3217473 -4.0827866 -3.9554026 -4.1074767 -4.3244443 -4.448946 -4.5281157 -4.5653682 -4.5762444 -4.5320559][-4.3725319 -4.3112655 -4.3300319 -4.3272738 -4.244338 -4.0779305 -3.799067 -3.6527805 -3.8206782 -4.05424 -4.1892014 -4.2932882 -4.3706245 -4.4173827 -4.4221096][-4.2818084 -4.2032914 -4.2071867 -4.1867442 -4.082653 -3.8938458 -3.6027093 -3.4547513 -3.6270707 -3.8677604 -4.0181265 -4.1441455 -4.2562146 -4.3275404 -4.3595476][-4.2976279 -4.2219748 -4.205193 -4.1664791 -4.0642157 -3.8836815 -3.617511 -3.4909315 -3.6631117 -3.9095449 -4.075757 -4.2034807 -4.3120794 -4.3709984 -4.3893003][-4.4282775 -4.3745093 -4.347846 -4.31122 -4.2375941 -4.0876055 -3.85962 -3.7519336 -3.8952556 -4.1203547 -4.282938 -4.3917174 -4.4669986 -4.4907331 -4.4726934][-4.5687237 -4.5447912 -4.5256162 -4.5013685 -4.4536576 -4.3293471 -4.135149 -4.0363889 -4.1352849 -4.315433 -4.4610968 -4.5520248 -4.5982366 -4.5907245 -4.5437317][-4.6277151 -4.6320324 -4.6254563 -4.6090488 -4.5736418 -4.4722877 -4.320313 -4.2464242 -4.313025 -4.4422588 -4.5586863 -4.6318712 -4.6570525 -4.6297255 -4.5679784][-4.6114192 -4.6269197 -4.6244683 -4.612309 -4.5867324 -4.5165586 -4.4197693 -4.3840456 -4.4380908 -4.5257249 -4.6044545 -4.6497855 -4.6530552 -4.6123705 -4.5451145][-4.5402942 -4.5464544 -4.5373812 -4.5256548 -4.5149207 -4.4860916 -4.4495468 -4.4519472 -4.501132 -4.55777 -4.5999622 -4.6148653 -4.5974884 -4.5505943 -4.4888825][-4.4631357 -4.4604273 -4.449995 -4.4409 -4.4408264 -4.4397416 -4.4390683 -4.4544 -4.48708 -4.5151949 -4.5292525 -4.524425 -4.4999332 -4.4621563 -4.4204717]]...]
INFO - root - 2017-12-07 20:40:43.943107: step 47210, loss = 21.74, batch loss = 21.66 (8.5 examples/sec; 0.936 sec/batch; 74h:12m:36s remains)
INFO - root - 2017-12-07 20:40:53.382890: step 47220, loss = 21.25, batch loss = 21.16 (8.5 examples/sec; 0.939 sec/batch; 74h:23m:47s remains)
INFO - root - 2017-12-07 20:41:02.856408: step 47230, loss = 21.30, batch loss = 21.21 (8.7 examples/sec; 0.915 sec/batch; 72h:32m:37s remains)
INFO - root - 2017-12-07 20:41:12.299981: step 47240, loss = 21.44, batch loss = 21.35 (8.4 examples/sec; 0.947 sec/batch; 75h:04m:04s remains)
INFO - root - 2017-12-07 20:41:21.660012: step 47250, loss = 21.52, batch loss = 21.44 (8.3 examples/sec; 0.963 sec/batch; 76h:20m:18s remains)
INFO - root - 2017-12-07 20:41:30.911565: step 47260, loss = 21.79, batch loss = 21.71 (8.0 examples/sec; 0.996 sec/batch; 78h:53m:29s remains)
INFO - root - 2017-12-07 20:41:40.404657: step 47270, loss = 21.23, batch loss = 21.15 (8.0 examples/sec; 1.000 sec/batch; 79h:11m:47s remains)
INFO - root - 2017-12-07 20:41:49.796192: step 47280, loss = 21.46, batch loss = 21.38 (8.5 examples/sec; 0.944 sec/batch; 74h:45m:48s remains)
INFO - root - 2017-12-07 20:41:59.092133: step 47290, loss = 21.60, batch loss = 21.51 (8.4 examples/sec; 0.953 sec/batch; 75h:29m:16s remains)
INFO - root - 2017-12-07 20:42:08.459514: step 47300, loss = 21.59, batch loss = 21.51 (8.3 examples/sec; 0.960 sec/batch; 76h:04m:11s remains)
2017-12-07 20:42:09.396133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.464664 -4.42453 -4.4335766 -4.4727249 -4.5077777 -4.5181093 -4.5060654 -4.48728 -4.4622746 -4.423995 -4.3953428 -4.4004641 -4.4375439 -4.4955521 -4.5632839][-4.4356842 -4.3971977 -4.4118161 -4.4588146 -4.4986534 -4.5072064 -4.4915619 -4.4700966 -4.4393969 -4.3994365 -4.3707933 -4.3710051 -4.4069309 -4.481884 -4.585041][-4.4078059 -4.3731837 -4.3896589 -4.44524 -4.4885035 -4.4827094 -4.4474406 -4.4142585 -4.3727264 -4.3259759 -4.3006611 -4.3021221 -4.3332591 -4.4179158 -4.5458016][-4.3760228 -4.3540492 -4.3788543 -4.4491453 -4.4995708 -4.4755015 -4.4124193 -4.3605061 -4.3051319 -4.247457 -4.2177505 -4.2151866 -4.2404194 -4.3272042 -4.4584465][-4.3086138 -4.3007765 -4.3340716 -4.4117546 -4.4644661 -4.4265342 -4.349472 -4.2960715 -4.2489653 -4.2006512 -4.1766081 -4.17283 -4.1932235 -4.2667732 -4.3684111][-4.2272224 -4.223773 -4.2515321 -4.3155589 -4.3508711 -4.2956758 -4.2182541 -4.185885 -4.1809416 -4.1788144 -4.1852541 -4.1919966 -4.2091575 -4.2520876 -4.2960835][-4.1878338 -4.1809764 -4.1920958 -4.2229033 -4.2174826 -4.1302042 -4.0421147 -4.0254931 -4.0658407 -4.1219625 -4.1752148 -4.2113686 -4.2389393 -4.2564454 -4.2488432][-4.2187033 -4.2141414 -4.2145977 -4.2251444 -4.1929731 -4.0705442 -3.9489405 -3.9122913 -3.9604142 -4.0468125 -4.1410513 -4.2137837 -4.2619743 -4.2781687 -4.2490215][-4.2980838 -4.3061447 -4.3093281 -4.320755 -4.29145 -4.1601205 -4.0042782 -3.918052 -3.9288857 -4.0021429 -4.1112971 -4.2159057 -4.2929845 -4.3330612 -4.3146582][-4.3888493 -4.3958259 -4.4045906 -4.425931 -4.4200773 -4.3133593 -4.1575642 -4.0420489 -4.0120854 -4.052774 -4.1488485 -4.2589483 -4.3505731 -4.4138145 -4.4195476][-4.4546523 -4.4582696 -4.4726248 -4.49895 -4.507143 -4.4330297 -4.305716 -4.1998563 -4.1614833 -4.1847587 -4.261694 -4.3603196 -4.4496307 -4.5168715 -4.537962][-4.4730787 -4.4826155 -4.5087261 -4.5364046 -4.5403891 -4.4807935 -4.3819609 -4.304533 -4.28264 -4.3080282 -4.3717394 -4.4520822 -4.5283036 -4.5867519 -4.610076][-4.4371777 -4.4576592 -4.4979939 -4.5250239 -4.5154047 -4.4496207 -4.3639531 -4.3106885 -4.3072538 -4.339623 -4.3969846 -4.4664731 -4.5356879 -4.58859 -4.6075411][-4.3651972 -4.3809595 -4.4289308 -4.4588056 -4.4390764 -4.3575687 -4.2688279 -4.2262888 -4.2321548 -4.2680631 -4.3254356 -4.3966603 -4.4700928 -4.5242367 -4.5322986][-4.3012447 -4.3076439 -4.3581839 -4.3971682 -4.376009 -4.2781186 -4.1712818 -4.1207371 -4.1242847 -4.1610518 -4.2232366 -4.2982335 -4.3769779 -4.43184 -4.4262652]]...]
INFO - root - 2017-12-07 20:42:18.803769: step 47310, loss = 21.42, batch loss = 21.34 (8.3 examples/sec; 0.969 sec/batch; 76h:44m:19s remains)
INFO - root - 2017-12-07 20:42:28.046967: step 47320, loss = 21.86, batch loss = 21.78 (7.9 examples/sec; 1.013 sec/batch; 80h:13m:04s remains)
INFO - root - 2017-12-07 20:42:37.425791: step 47330, loss = 21.40, batch loss = 21.32 (8.1 examples/sec; 0.993 sec/batch; 78h:39m:35s remains)
INFO - root - 2017-12-07 20:42:46.834085: step 47340, loss = 21.73, batch loss = 21.64 (8.2 examples/sec; 0.971 sec/batch; 76h:56m:35s remains)
INFO - root - 2017-12-07 20:42:56.209209: step 47350, loss = 21.47, batch loss = 21.39 (8.6 examples/sec; 0.931 sec/batch; 73h:42m:58s remains)
INFO - root - 2017-12-07 20:43:05.545924: step 47360, loss = 21.01, batch loss = 20.92 (8.3 examples/sec; 0.964 sec/batch; 76h:19m:50s remains)
INFO - root - 2017-12-07 20:43:14.832705: step 47370, loss = 21.58, batch loss = 21.50 (8.5 examples/sec; 0.941 sec/batch; 74h:30m:42s remains)
INFO - root - 2017-12-07 20:43:24.111951: step 47380, loss = 21.38, batch loss = 21.30 (8.8 examples/sec; 0.909 sec/batch; 71h:58m:20s remains)
INFO - root - 2017-12-07 20:43:33.457402: step 47390, loss = 21.28, batch loss = 21.20 (8.5 examples/sec; 0.938 sec/batch; 74h:16m:01s remains)
INFO - root - 2017-12-07 20:43:42.922869: step 47400, loss = 21.25, batch loss = 21.17 (8.1 examples/sec; 0.987 sec/batch; 78h:11m:48s remains)
2017-12-07 20:43:43.846026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.54298 -4.5509048 -4.5364175 -4.4931421 -4.4560375 -4.4604888 -4.4919028 -4.4981275 -4.4970369 -4.5146308 -4.52319 -4.4903541 -4.4328203 -4.3703175 -4.2842784][-4.4664812 -4.4560089 -4.444119 -4.4018841 -4.344264 -4.32477 -4.3414087 -4.3459754 -4.3347034 -4.3357892 -4.3489895 -4.3262033 -4.276372 -4.2170982 -4.1223869][-4.31968 -4.2975678 -4.2949333 -4.2593679 -4.1893458 -4.1562729 -4.1664982 -4.177207 -4.169404 -4.1730652 -4.2107568 -4.2075281 -4.15447 -4.0804515 -3.9753244][-4.1899042 -4.1877689 -4.2147026 -4.1855025 -4.0893631 -4.0237145 -4.0042639 -4.0103312 -4.0271339 -4.0688148 -4.1647916 -4.20773 -4.164465 -4.0816913 -3.9785495][-4.1828027 -4.2329974 -4.2860084 -4.2316632 -4.0718036 -3.9358211 -3.8591063 -3.8425093 -3.8985565 -4.0120211 -4.1852026 -4.2892942 -4.2787309 -4.20762 -4.1164856][-4.3069177 -4.3902793 -4.4226675 -4.2977433 -4.0533404 -3.8541934 -3.7280569 -3.6863489 -3.786582 -3.9784017 -4.2171388 -4.3702831 -4.4050341 -4.372057 -4.3055854][-4.4741306 -4.5382671 -4.513042 -4.3208985 -4.030664 -3.8166356 -3.6731632 -3.6146829 -3.7568674 -4.0135541 -4.2799683 -4.4493737 -4.5189109 -4.5259237 -4.4773135][-4.5716162 -4.598886 -4.5435152 -4.3493786 -4.0791807 -3.8946803 -3.7574739 -3.6937122 -3.8712521 -4.1661582 -4.4163766 -4.5587478 -4.6184492 -4.6273112 -4.5753608][-4.5925412 -4.5996523 -4.5532627 -4.4089413 -4.1981411 -4.0525894 -3.9284368 -3.8763561 -4.0719261 -4.3638473 -4.56634 -4.6568446 -4.6791687 -4.6681743 -4.6152763][-4.592216 -4.6123323 -4.5932469 -4.4943409 -4.3256578 -4.2008533 -4.0972238 -4.0774589 -4.2681141 -4.5097265 -4.6500516 -4.7018976 -4.7052193 -4.691741 -4.6563373][-4.5891519 -4.6258411 -4.6273479 -4.5561118 -4.4124379 -4.30525 -4.2404647 -4.2587008 -4.4110932 -4.5689983 -4.6489348 -4.6836905 -4.6948853 -4.6986294 -4.6912479][-4.55291 -4.5944219 -4.6001425 -4.5428634 -4.4257536 -4.3524837 -4.3374729 -4.3761611 -4.4741955 -4.5554571 -4.5920696 -4.61928 -4.6408749 -4.6580939 -4.6708517][-4.4962912 -4.5277643 -4.5179067 -4.454215 -4.356173 -4.3209209 -4.348175 -4.3927841 -4.4538341 -4.5026326 -4.5277214 -4.5483055 -4.5648675 -4.5728168 -4.5859365][-4.4433537 -4.4640336 -4.4372244 -4.3608947 -4.2683053 -4.2523479 -4.2963214 -4.3371859 -4.3889551 -4.4462037 -4.4831066 -4.5010505 -4.5028238 -4.4872761 -4.4850125][-4.4116926 -4.4405251 -4.4156423 -4.332057 -4.2326875 -4.2071848 -4.2376137 -4.2698812 -4.3310061 -4.4158893 -4.4724712 -4.4911742 -4.4804387 -4.4404979 -4.4151473]]...]
INFO - root - 2017-12-07 20:43:53.231287: step 47410, loss = 21.23, batch loss = 21.15 (8.7 examples/sec; 0.915 sec/batch; 72h:28m:25s remains)
INFO - root - 2017-12-07 20:44:02.547800: step 47420, loss = 21.35, batch loss = 21.27 (8.6 examples/sec; 0.934 sec/batch; 73h:59m:31s remains)
INFO - root - 2017-12-07 20:44:12.146232: step 47430, loss = 21.11, batch loss = 21.03 (8.0 examples/sec; 1.004 sec/batch; 79h:29m:12s remains)
INFO - root - 2017-12-07 20:44:21.489336: step 47440, loss = 21.29, batch loss = 21.21 (8.5 examples/sec; 0.936 sec/batch; 74h:08m:04s remains)
INFO - root - 2017-12-07 20:44:30.909450: step 47450, loss = 21.46, batch loss = 21.37 (8.6 examples/sec; 0.936 sec/batch; 74h:05m:06s remains)
INFO - root - 2017-12-07 20:44:39.764571: step 47460, loss = 21.34, batch loss = 21.26 (9.4 examples/sec; 0.848 sec/batch; 67h:07m:44s remains)
INFO - root - 2017-12-07 20:44:49.180233: step 47470, loss = 21.74, batch loss = 21.66 (8.6 examples/sec; 0.928 sec/batch; 73h:27m:48s remains)
INFO - root - 2017-12-07 20:44:58.424292: step 47480, loss = 21.66, batch loss = 21.58 (8.9 examples/sec; 0.898 sec/batch; 71h:04m:07s remains)
INFO - root - 2017-12-07 20:45:07.936691: step 47490, loss = 21.48, batch loss = 21.40 (8.4 examples/sec; 0.951 sec/batch; 75h:15m:16s remains)
INFO - root - 2017-12-07 20:45:17.338919: step 47500, loss = 21.61, batch loss = 21.53 (8.5 examples/sec; 0.943 sec/batch; 74h:39m:14s remains)
2017-12-07 20:45:18.340966: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4694939 -4.4535146 -4.373466 -4.2841811 -4.2603974 -4.3257518 -4.4273353 -4.484107 -4.4811006 -4.4594541 -4.4518452 -4.4703932 -4.4896407 -4.464613 -4.3926482][-4.4547548 -4.3870683 -4.2443018 -4.1044183 -4.066308 -4.1501007 -4.2782969 -4.365716 -4.3923826 -4.4070034 -4.4339395 -4.4644403 -4.4735241 -4.4263244 -4.3418818][-4.4049606 -4.3006792 -4.1419544 -4.0101104 -3.988292 -4.0816865 -4.2138796 -4.3080797 -4.3466997 -4.37974 -4.4212351 -4.4440432 -4.4212918 -4.3375964 -4.2405081][-4.323359 -4.187674 -4.0444832 -3.9607575 -3.9736872 -4.0707254 -4.1906891 -4.27775 -4.3175159 -4.3595085 -4.4108105 -4.4255009 -4.3694444 -4.2517333 -4.1436682][-4.2176862 -4.0592518 -3.9416909 -3.9128952 -3.9581676 -4.0423622 -4.12407 -4.1801934 -4.2141075 -4.2730107 -4.3528419 -4.3867841 -4.3313036 -4.2111387 -4.1064215][-4.1381903 -3.9722776 -3.8827171 -3.896446 -3.9550939 -3.9943707 -3.9954667 -3.981823 -3.9963987 -4.0859308 -4.2177029 -4.3018823 -4.2893085 -4.2045078 -4.1234641][-4.1426034 -3.9884439 -3.9282811 -3.9707971 -4.0226021 -3.9923055 -3.8867788 -3.78178 -3.7644515 -3.8773856 -4.0537486 -4.1873584 -4.2322631 -4.2063775 -4.1727524][-4.1998138 -4.0665145 -4.0301995 -4.0921483 -4.1378112 -4.0570464 -3.8691511 -3.6944408 -3.650022 -3.7692723 -3.9606483 -4.11258 -4.1887727 -4.2060318 -4.2129679][-4.2712255 -4.1604404 -4.1399045 -4.2088237 -4.2538438 -4.1580396 -3.9428163 -3.7475634 -3.7005844 -3.819741 -4.0015721 -4.1395822 -4.2086306 -4.2317796 -4.2487035][-4.3550472 -4.2776375 -4.2688713 -4.3317609 -4.373476 -4.2820578 -4.0760689 -3.8926587 -3.8575413 -3.9766164 -4.1416059 -4.2525339 -4.2988811 -4.3076787 -4.3138442][-4.4136848 -4.3816652 -4.3889976 -4.4445744 -4.4832582 -4.409987 -4.2330546 -4.0695043 -4.0379643 -4.1458664 -4.2921805 -4.3858733 -4.4183373 -4.4110284 -4.3923488][-4.4301386 -4.4400835 -4.4645615 -4.512496 -4.5463171 -4.4975395 -4.36338 -4.2278256 -4.1896172 -4.26675 -4.3836555 -4.4636 -4.4893937 -4.4700704 -4.4274044][-4.3998146 -4.433423 -4.4707527 -4.5160542 -4.5485182 -4.5277991 -4.4448962 -4.349515 -4.306457 -4.3402963 -4.4057274 -4.4517827 -4.4590821 -4.4269753 -4.3779912][-4.3154135 -4.3544636 -4.4013085 -4.4544444 -4.4987593 -4.5101895 -4.4771318 -4.42479 -4.3867049 -4.3813791 -4.384244 -4.3727551 -4.3366718 -4.2803669 -4.235393][-4.2343154 -4.2649422 -4.3091488 -4.3633056 -4.4165716 -4.4532309 -4.4594045 -4.4450288 -4.4191341 -4.3858948 -4.3290939 -4.2564721 -4.1760154 -4.10761 -4.0845566]]...]
INFO - root - 2017-12-07 20:45:27.732714: step 47510, loss = 21.53, batch loss = 21.45 (8.7 examples/sec; 0.914 sec/batch; 72h:23m:15s remains)
INFO - root - 2017-12-07 20:45:36.993894: step 47520, loss = 21.54, batch loss = 21.46 (8.7 examples/sec; 0.922 sec/batch; 72h:58m:42s remains)
INFO - root - 2017-12-07 20:45:46.408888: step 47530, loss = 21.49, batch loss = 21.41 (8.5 examples/sec; 0.936 sec/batch; 74h:04m:33s remains)
INFO - root - 2017-12-07 20:45:55.788676: step 47540, loss = 21.63, batch loss = 21.55 (8.5 examples/sec; 0.940 sec/batch; 74h:25m:21s remains)
INFO - root - 2017-12-07 20:46:05.262004: step 47550, loss = 21.49, batch loss = 21.41 (8.5 examples/sec; 0.936 sec/batch; 74h:06m:42s remains)
INFO - root - 2017-12-07 20:46:14.606164: step 47560, loss = 21.82, batch loss = 21.74 (8.8 examples/sec; 0.910 sec/batch; 71h:59m:28s remains)
INFO - root - 2017-12-07 20:46:23.831435: step 47570, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.948 sec/batch; 74h:59m:40s remains)
INFO - root - 2017-12-07 20:46:33.222344: step 47580, loss = 22.03, batch loss = 21.95 (8.8 examples/sec; 0.913 sec/batch; 72h:13m:24s remains)
INFO - root - 2017-12-07 20:46:42.589047: step 47590, loss = 21.24, batch loss = 21.16 (8.5 examples/sec; 0.938 sec/batch; 74h:13m:03s remains)
INFO - root - 2017-12-07 20:46:52.021278: step 47600, loss = 21.03, batch loss = 20.94 (9.1 examples/sec; 0.878 sec/batch; 69h:28m:51s remains)
2017-12-07 20:46:53.089876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4045396 -4.5007963 -4.5631981 -4.6044655 -4.6509404 -4.6643829 -4.5941358 -4.516861 -4.5459352 -4.6342754 -4.6646128 -4.6285172 -4.5985489 -4.608078 -4.6650052][-4.37274 -4.4836745 -4.5781307 -4.642797 -4.683126 -4.677846 -4.5966878 -4.5087767 -4.5387826 -4.6530681 -4.7119179 -4.6809888 -4.6422305 -4.635365 -4.6746311][-4.3464603 -4.4854994 -4.6217742 -4.6922474 -4.692421 -4.6318617 -4.5153446 -4.4112487 -4.4467092 -4.5962329 -4.7006187 -4.702848 -4.6795182 -4.6634436 -4.67317][-4.3615 -4.5211434 -4.667521 -4.6990833 -4.6222854 -4.4770474 -4.3026028 -4.173769 -4.2237062 -4.4294081 -4.6100941 -4.6863322 -4.70622 -4.679894 -4.6309357][-4.4092703 -4.5557303 -4.6619015 -4.6191955 -4.4639091 -4.2509065 -4.0231986 -3.8586466 -3.9199061 -4.1932445 -4.46813 -4.63996 -4.7104268 -4.6598654 -4.5290823][-4.481606 -4.5850439 -4.6155491 -4.4883895 -4.2759314 -4.0308189 -3.76893 -3.5666051 -3.6384771 -3.97621 -4.3317308 -4.5818615 -4.6844535 -4.599647 -4.3977141][-4.5689287 -4.6264305 -4.5899682 -4.414958 -4.189055 -3.9439979 -3.6728497 -3.4466822 -3.5224042 -3.8893933 -4.2705703 -4.5415578 -4.6407175 -4.5266886 -4.2945356][-4.6624684 -4.694169 -4.6248388 -4.4525132 -4.2504654 -4.0274076 -3.7766175 -3.5700171 -3.640996 -3.9622011 -4.2861786 -4.512907 -4.588696 -4.4767971 -4.266777][-4.7513123 -4.7711926 -4.6906362 -4.5413589 -4.3724976 -4.1784735 -3.970437 -3.8261349 -3.8945985 -4.1207314 -4.3387022 -4.4944258 -4.5530796 -4.4760017 -4.3168907][-4.8036194 -4.812418 -4.7237229 -4.5917339 -4.4584627 -4.307117 -4.16084 -4.092361 -4.1599793 -4.2938261 -4.4137325 -4.5086217 -4.5576563 -4.5171509 -4.4056921][-4.7878656 -4.7882671 -4.7006326 -4.5835395 -4.490077 -4.4009347 -4.3259315 -4.3176923 -4.3765659 -4.44211 -4.4961696 -4.5519657 -4.594614 -4.5800548 -4.5051641][-4.6983738 -4.6982856 -4.6286569 -4.5406227 -4.4954739 -4.4751143 -4.4605708 -4.4779258 -4.5092506 -4.5219684 -4.5288982 -4.5531912 -4.591002 -4.6054258 -4.573][-4.5582976 -4.567153 -4.53004 -4.4828215 -4.4786673 -4.5012441 -4.5131378 -4.523088 -4.5201859 -4.4976463 -4.4711132 -4.463798 -4.4930291 -4.5371556 -4.5513287][-4.4214644 -4.4433475 -4.4386959 -4.4236569 -4.4321413 -4.4562221 -4.4616895 -4.4515543 -4.4243035 -4.3864751 -4.3476562 -4.3235 -4.3455577 -4.4092765 -4.4550943][-4.3380208 -4.3802743 -4.4085979 -4.4106326 -4.4055877 -4.4015441 -4.3834577 -4.3595514 -4.3305907 -4.2974367 -4.2604475 -4.2321014 -4.2492404 -4.3126922 -4.359374]]...]
INFO - root - 2017-12-07 20:47:02.435800: step 47610, loss = 21.51, batch loss = 21.43 (8.4 examples/sec; 0.954 sec/batch; 75h:29m:28s remains)
INFO - root - 2017-12-07 20:47:11.941183: step 47620, loss = 21.44, batch loss = 21.35 (8.4 examples/sec; 0.949 sec/batch; 75h:05m:20s remains)
INFO - root - 2017-12-07 20:47:21.438032: step 47630, loss = 21.74, batch loss = 21.65 (8.5 examples/sec; 0.947 sec/batch; 74h:54m:37s remains)
INFO - root - 2017-12-07 20:47:30.651186: step 47640, loss = 21.43, batch loss = 21.35 (8.8 examples/sec; 0.911 sec/batch; 72h:03m:55s remains)
INFO - root - 2017-12-07 20:47:40.029792: step 47650, loss = 21.40, batch loss = 21.32 (8.5 examples/sec; 0.945 sec/batch; 74h:45m:39s remains)
INFO - root - 2017-12-07 20:47:49.400862: step 47660, loss = 21.14, batch loss = 21.06 (8.3 examples/sec; 0.959 sec/batch; 75h:53m:51s remains)
INFO - root - 2017-12-07 20:47:58.635621: step 47670, loss = 21.39, batch loss = 21.30 (8.9 examples/sec; 0.904 sec/batch; 71h:30m:01s remains)
INFO - root - 2017-12-07 20:48:08.310372: step 47680, loss = 21.58, batch loss = 21.50 (8.6 examples/sec; 0.934 sec/batch; 73h:55m:07s remains)
INFO - root - 2017-12-07 20:48:17.750780: step 47690, loss = 21.66, batch loss = 21.58 (8.5 examples/sec; 0.945 sec/batch; 74h:46m:01s remains)
INFO - root - 2017-12-07 20:48:27.354843: step 47700, loss = 21.61, batch loss = 21.53 (8.4 examples/sec; 0.952 sec/batch; 75h:19m:11s remains)
2017-12-07 20:48:28.414575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3577433 -4.3591371 -4.3304653 -4.3136654 -4.3050647 -4.295104 -4.2712078 -4.2537937 -4.2557797 -4.2579637 -4.2494788 -4.19849 -4.1200652 -4.0781589 -4.0935669][-4.2995143 -4.3087759 -4.2945213 -4.2946944 -4.3095818 -4.323842 -4.3231096 -4.3364544 -4.374887 -4.4178915 -4.4429073 -4.4019833 -4.3119621 -4.242918 -4.22739][-4.1897273 -4.2290034 -4.2522206 -4.2748938 -4.2974973 -4.3009467 -4.2916794 -4.3181386 -4.3893981 -4.4817495 -4.5644121 -4.5626636 -4.4833302 -4.400867 -4.3593431][-4.0718327 -4.1570292 -4.2365623 -4.2795987 -4.2704024 -4.1960373 -4.1104646 -4.1038237 -4.187829 -4.33162 -4.4877076 -4.5506654 -4.5085645 -4.4332 -4.3856621][-4.0499334 -4.1639042 -4.2780695 -4.3175659 -4.2527518 -4.078114 -3.8865564 -3.8078697 -3.868135 -4.0389476 -4.2557883 -4.38778 -4.4036555 -4.3657207 -4.3405428][-4.1228595 -4.2321053 -4.3224144 -4.3188987 -4.2070575 -3.9857473 -3.7473154 -3.6202538 -3.641053 -3.8016987 -4.0383959 -4.20834 -4.26934 -4.2788944 -4.2948947][-4.22717 -4.3076463 -4.3433919 -4.281188 -4.1397772 -3.9292324 -3.7236884 -3.6092112 -3.6114576 -3.7425449 -3.9598739 -4.1285496 -4.2001238 -4.2303305 -4.2704673][-4.3298864 -4.3974757 -4.4074507 -4.3184319 -4.1706777 -3.9908898 -3.8414965 -3.7637205 -3.7590237 -3.8505988 -4.0303454 -4.1824179 -4.2503262 -4.2769547 -4.3079405][-4.4313459 -4.4925447 -4.4978771 -4.4219022 -4.3069792 -4.1803164 -4.0885262 -4.0429349 -4.0277691 -4.0681872 -4.1906514 -4.30669 -4.3626308 -4.3810115 -4.393609][-4.5251989 -4.5681353 -4.565352 -4.5103316 -4.4398441 -4.3712487 -4.3304105 -4.3132458 -4.2950206 -4.2951765 -4.35442 -4.4138737 -4.4425263 -4.450532 -4.453033][-4.5991378 -4.6223593 -4.611969 -4.5735469 -4.5370317 -4.5112844 -4.5044489 -4.50537 -4.488698 -4.4656386 -4.4726319 -4.4767685 -4.4737144 -4.4711752 -4.4752846][-4.6066446 -4.6102281 -4.5995383 -4.5807304 -4.5726733 -4.5778685 -4.5927167 -4.6041088 -4.5916252 -4.5608139 -4.5364022 -4.504993 -4.4797196 -4.4714422 -4.4835029][-4.5200834 -4.5101814 -4.5035572 -4.5009813 -4.509963 -4.5304928 -4.5576386 -4.5819535 -4.5873561 -4.5710926 -4.544538 -4.5058184 -4.4733524 -4.4610968 -4.4735074][-4.4109092 -4.40016 -4.4004478 -4.4063396 -4.4175396 -4.4346824 -4.4578 -4.4848585 -4.5055475 -4.5106139 -4.5006504 -4.4760633 -4.4513974 -4.4392028 -4.4450164][-4.3584633 -4.3534355 -4.3577766 -4.3643427 -4.3702683 -4.3766046 -4.3869686 -4.4034743 -4.4217539 -4.4334788 -4.4349685 -4.4266009 -4.4161057 -4.4099784 -4.4114432]]...]
INFO - root - 2017-12-07 20:48:37.735030: step 47710, loss = 21.14, batch loss = 21.06 (8.1 examples/sec; 0.988 sec/batch; 78h:09m:37s remains)
INFO - root - 2017-12-07 20:48:47.187534: step 47720, loss = 21.68, batch loss = 21.60 (8.4 examples/sec; 0.947 sec/batch; 74h:54m:42s remains)
INFO - root - 2017-12-07 20:48:56.623689: step 47730, loss = 21.21, batch loss = 21.13 (8.6 examples/sec; 0.929 sec/batch; 73h:30m:08s remains)
INFO - root - 2017-12-07 20:49:06.190671: step 47740, loss = 21.30, batch loss = 21.21 (8.1 examples/sec; 0.983 sec/batch; 77h:45m:57s remains)
INFO - root - 2017-12-07 20:49:15.536797: step 47750, loss = 20.98, batch loss = 20.90 (8.7 examples/sec; 0.924 sec/batch; 73h:06m:15s remains)
INFO - root - 2017-12-07 20:49:24.991513: step 47760, loss = 21.10, batch loss = 21.02 (8.4 examples/sec; 0.954 sec/batch; 75h:29m:24s remains)
INFO - root - 2017-12-07 20:49:34.312006: step 47770, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.928 sec/batch; 73h:21m:59s remains)
INFO - root - 2017-12-07 20:49:43.701107: step 47780, loss = 21.50, batch loss = 21.41 (9.1 examples/sec; 0.882 sec/batch; 69h:43m:20s remains)
INFO - root - 2017-12-07 20:49:53.169691: step 47790, loss = 21.37, batch loss = 21.29 (8.2 examples/sec; 0.976 sec/batch; 77h:12m:16s remains)
INFO - root - 2017-12-07 20:50:02.506632: step 47800, loss = 21.58, batch loss = 21.49 (8.7 examples/sec; 0.921 sec/batch; 72h:52m:21s remains)
2017-12-07 20:50:03.476387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8011508 -4.8808408 -4.851264 -4.66403 -4.4698572 -4.3929524 -4.4850755 -4.6184087 -4.6647549 -4.666811 -4.6488829 -4.6207609 -4.6081438 -4.6397161 -4.6536775][-4.9061308 -5.0056133 -4.9785843 -4.7870307 -4.5667052 -4.4446769 -4.5041423 -4.6361523 -4.7092676 -4.7415905 -4.726234 -4.6705327 -4.6169133 -4.6210876 -4.6388078][-4.91977 -5.0201659 -5.0145273 -4.866488 -4.648726 -4.479774 -4.4768682 -4.5801978 -4.6828413 -4.7589602 -4.7599053 -4.6821675 -4.5791359 -4.5398726 -4.5627565][-4.8423471 -4.9141321 -4.9349051 -4.8620057 -4.6819682 -4.478641 -4.394701 -4.4421835 -4.5685558 -4.6910944 -4.7173967 -4.6374116 -4.5009127 -4.4185424 -4.438868][-4.7287979 -4.7538214 -4.7806439 -4.7592983 -4.6057496 -4.3706956 -4.2017412 -4.183075 -4.3436518 -4.5387988 -4.610096 -4.5457697 -4.4044194 -4.3004637 -4.3158097][-4.6311684 -4.6097622 -4.6033649 -4.565249 -4.3949637 -4.1382484 -3.9038537 -3.8248639 -4.0277586 -4.3207808 -4.4656816 -4.4354591 -4.31214 -4.2197523 -4.2443943][-4.5925503 -4.5331726 -4.472219 -4.3720846 -4.16166 -3.8889639 -3.599822 -3.4673195 -3.6989615 -4.078177 -4.29759 -4.3056612 -4.2090445 -4.1593084 -4.2151551][-4.6052036 -4.5367408 -4.4528141 -4.3251615 -4.1070089 -3.833441 -3.5000868 -3.3192129 -3.5392733 -3.9380212 -4.1893191 -4.211307 -4.1368713 -4.144814 -4.2393436][-4.6430116 -4.6035275 -4.5529571 -4.4691033 -4.3065209 -4.0583472 -3.7140594 -3.511265 -3.664782 -3.98836 -4.1993618 -4.2048988 -4.149591 -4.2067537 -4.3208184][-4.6661057 -4.6643372 -4.6594005 -4.6434345 -4.5665636 -4.3725963 -4.0628271 -3.8663177 -3.947988 -4.168725 -4.3073831 -4.2870517 -4.2488179 -4.3293271 -4.4375668][-4.6526027 -4.67916 -4.6992626 -4.7228246 -4.7139115 -4.5959415 -4.3737369 -4.2300124 -4.2691035 -4.4111419 -4.4960756 -4.4666061 -4.4416022 -4.5123911 -4.5849442][-4.631959 -4.6809645 -4.7044744 -4.7213345 -4.728415 -4.6729493 -4.5629168 -4.5026951 -4.5280256 -4.613946 -4.6687508 -4.6534805 -4.6464262 -4.6935668 -4.7159705][-4.6092868 -4.6724439 -4.6920438 -4.6765933 -4.65651 -4.633728 -4.62445 -4.6476784 -4.6726303 -4.7142911 -4.7461829 -4.7507119 -4.7630668 -4.7863607 -4.7634206][-4.5931182 -4.6668925 -4.6815019 -4.6304417 -4.5689864 -4.547441 -4.6019092 -4.6796427 -4.7020679 -4.7102723 -4.7237687 -4.7419009 -4.7649236 -4.7670813 -4.7159572][-4.5783572 -4.6530833 -4.6565218 -4.5778732 -4.4822741 -4.444077 -4.5154209 -4.6081467 -4.6254859 -4.6194272 -4.6261635 -4.6501513 -4.6715593 -4.65674 -4.6001139]]...]
INFO - root - 2017-12-07 20:50:12.719711: step 47810, loss = 21.34, batch loss = 21.26 (8.6 examples/sec; 0.936 sec/batch; 73h:59m:20s remains)
INFO - root - 2017-12-07 20:50:22.212580: step 47820, loss = 21.37, batch loss = 21.29 (8.4 examples/sec; 0.953 sec/batch; 75h:21m:46s remains)
INFO - root - 2017-12-07 20:50:31.575554: step 47830, loss = 21.74, batch loss = 21.66 (8.8 examples/sec; 0.914 sec/batch; 72h:17m:44s remains)
INFO - root - 2017-12-07 20:50:40.970647: step 47840, loss = 21.55, batch loss = 21.47 (8.8 examples/sec; 0.912 sec/batch; 72h:05m:58s remains)
INFO - root - 2017-12-07 20:50:50.358961: step 47850, loss = 21.43, batch loss = 21.34 (8.8 examples/sec; 0.914 sec/batch; 72h:13m:50s remains)
INFO - root - 2017-12-07 20:50:59.729824: step 47860, loss = 20.92, batch loss = 20.84 (8.8 examples/sec; 0.914 sec/batch; 72h:14m:06s remains)
INFO - root - 2017-12-07 20:51:09.134053: step 47870, loss = 21.59, batch loss = 21.51 (8.1 examples/sec; 0.985 sec/batch; 77h:52m:38s remains)
INFO - root - 2017-12-07 20:51:18.494458: step 47880, loss = 21.29, batch loss = 21.21 (8.2 examples/sec; 0.977 sec/batch; 77h:13m:22s remains)
INFO - root - 2017-12-07 20:51:27.828853: step 47890, loss = 21.11, batch loss = 21.03 (8.4 examples/sec; 0.953 sec/batch; 75h:19m:48s remains)
INFO - root - 2017-12-07 20:51:37.270146: step 47900, loss = 21.31, batch loss = 21.23 (8.8 examples/sec; 0.907 sec/batch; 71h:43m:19s remains)
2017-12-07 20:51:38.292533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5572557 -4.5712628 -4.5819983 -4.5973759 -4.6163993 -4.623477 -4.6145372 -4.6063256 -4.6043825 -4.5900168 -4.5607142 -4.5206671 -4.4830523 -4.4573207 -4.4386363][-4.59688 -4.6020403 -4.6013179 -4.6026845 -4.6120567 -4.6181259 -4.6151881 -4.6171989 -4.6188769 -4.5864162 -4.5233779 -4.4409542 -4.3642063 -4.3123026 -4.2867618][-4.6150279 -4.6105461 -4.6013026 -4.5828943 -4.5651193 -4.5522614 -4.5514107 -4.56887 -4.5798411 -4.5275769 -4.4233847 -4.2924352 -4.1776218 -4.1001568 -4.06953][-4.6145325 -4.60641 -4.5948796 -4.5605111 -4.5089617 -4.4616084 -4.4480119 -4.4745536 -4.4988685 -4.4368553 -4.3006763 -4.1308608 -3.9906881 -3.8980415 -3.8654413][-4.6007838 -4.5912638 -4.5701222 -4.5151372 -4.428484 -4.3405647 -4.3005409 -4.3271379 -4.3738513 -4.3332038 -4.2046647 -4.0344677 -3.8962629 -3.8056393 -3.7752657][-4.580442 -4.5690513 -4.5194588 -4.4249821 -4.2979145 -4.1795158 -4.1223822 -4.1521254 -4.2287297 -4.2380414 -4.1636872 -4.0449834 -3.9456747 -3.881021 -3.8606806][-4.5732007 -4.5487094 -4.4524865 -4.3000555 -4.1338921 -4.0069561 -3.9604869 -4.0087457 -4.1172328 -4.1845727 -4.1852403 -4.1502433 -4.1126909 -4.0876966 -4.0840492][-4.5913939 -4.5391178 -4.3867373 -4.1753349 -3.9841707 -3.8708663 -3.855577 -3.932339 -4.0668936 -4.1792789 -4.2420144 -4.2820826 -4.3022342 -4.3163738 -4.330709][-4.6167378 -4.5353689 -4.34141 -4.0971074 -3.9043181 -3.8174779 -3.8332403 -3.9276628 -4.0712991 -4.2034836 -4.2930288 -4.3692441 -4.4239435 -4.4653978 -4.4930997][-4.6194091 -4.5222812 -4.3157773 -4.0732789 -3.9060915 -3.85504 -3.8975413 -3.9955695 -4.1302319 -4.2578611 -4.3401356 -4.4110765 -4.4717121 -4.5249009 -4.5585837][-4.5913777 -4.5074363 -4.3299141 -4.1267624 -3.9976072 -3.9715238 -4.0203123 -4.10251 -4.21334 -4.3270974 -4.3967028 -4.4491186 -4.4968429 -4.5432849 -4.5670276][-4.5309267 -4.4860063 -4.3737583 -4.2347131 -4.1409883 -4.1137285 -4.1377869 -4.1907711 -4.2782788 -4.3828392 -4.450388 -4.4929528 -4.5249519 -4.5513039 -4.5523067][-4.4381175 -4.4448137 -4.4176974 -4.3568058 -4.29432 -4.2507558 -4.2361379 -4.2583513 -4.329052 -4.4252844 -4.4924822 -4.5297847 -4.5461888 -4.5492992 -4.5307665][-4.3557973 -4.3935571 -4.4268646 -4.4207487 -4.3741946 -4.3193922 -4.2874436 -4.3027811 -4.3734894 -4.4652128 -4.5266666 -4.5503783 -4.54711 -4.5311508 -4.5080318][-4.3109589 -4.345665 -4.3930922 -4.4021292 -4.3589525 -4.3141785 -4.2985892 -4.3343196 -4.4187193 -4.5060434 -4.5554495 -4.5591044 -4.534677 -4.5048318 -4.4846988]]...]
INFO - root - 2017-12-07 20:51:47.685082: step 47910, loss = 21.34, batch loss = 21.25 (8.0 examples/sec; 0.994 sec/batch; 78h:37m:00s remains)
INFO - root - 2017-12-07 20:51:57.007438: step 47920, loss = 21.70, batch loss = 21.62 (8.2 examples/sec; 0.979 sec/batch; 77h:24m:13s remains)
INFO - root - 2017-12-07 20:52:06.275757: step 47930, loss = 21.39, batch loss = 21.30 (8.7 examples/sec; 0.915 sec/batch; 72h:18m:05s remains)
INFO - root - 2017-12-07 20:52:15.693242: step 47940, loss = 21.72, batch loss = 21.63 (8.8 examples/sec; 0.909 sec/batch; 71h:49m:38s remains)
INFO - root - 2017-12-07 20:52:25.299895: step 47950, loss = 21.40, batch loss = 21.31 (8.2 examples/sec; 0.972 sec/batch; 76h:50m:58s remains)
INFO - root - 2017-12-07 20:52:34.752934: step 47960, loss = 21.82, batch loss = 21.73 (8.3 examples/sec; 0.964 sec/batch; 76h:09m:44s remains)
INFO - root - 2017-12-07 20:52:44.082321: step 47970, loss = 20.96, batch loss = 20.88 (8.4 examples/sec; 0.953 sec/batch; 75h:20m:04s remains)
INFO - root - 2017-12-07 20:52:53.284044: step 47980, loss = 21.62, batch loss = 21.54 (8.5 examples/sec; 0.943 sec/batch; 74h:30m:40s remains)
INFO - root - 2017-12-07 20:53:02.716396: step 47990, loss = 21.36, batch loss = 21.28 (9.1 examples/sec; 0.875 sec/batch; 69h:10m:35s remains)
INFO - root - 2017-12-07 20:53:12.076057: step 48000, loss = 21.79, batch loss = 21.71 (8.5 examples/sec; 0.947 sec/batch; 74h:49m:06s remains)
2017-12-07 20:53:13.012602: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2808094 -4.2945089 -4.3136091 -4.3331447 -4.3548384 -4.3767385 -4.3978815 -4.4179959 -4.441195 -4.4673648 -4.4880142 -4.4898291 -4.4669428 -4.4245548 -4.3747573][-4.3295784 -4.3672681 -4.4027982 -4.4329581 -4.4617076 -4.4874191 -4.5096359 -4.52936 -4.5562119 -4.5947452 -4.6337419 -4.647747 -4.6204824 -4.5534344 -4.4673514][-4.3917933 -4.4592524 -4.5129066 -4.5524712 -4.5819664 -4.5988503 -4.6046495 -4.6043987 -4.6165967 -4.6530404 -4.7065697 -4.7436066 -4.7330623 -4.6631989 -4.5547428][-4.4562311 -4.5542216 -4.6223755 -4.6651645 -4.6797528 -4.6643953 -4.6274939 -4.5820055 -4.5618215 -4.5835509 -4.6461778 -4.7158308 -4.744247 -4.701942 -4.598968][-4.4966211 -4.6125922 -4.6770215 -4.7015004 -4.6718516 -4.5941658 -4.4908557 -4.3860388 -4.3373685 -4.3572955 -4.4429307 -4.5589266 -4.6419687 -4.6516495 -4.5792317][-4.5095119 -4.6322651 -4.6740084 -4.65902 -4.5632739 -4.4039645 -4.2269711 -4.0702662 -4.0088277 -4.0472145 -4.1715007 -4.3405924 -4.4803247 -4.5462704 -4.5112886][-4.5162311 -4.6459646 -4.6611524 -4.5983381 -4.4273739 -4.1869774 -3.9563801 -3.7801139 -3.7323232 -3.8025913 -3.9600983 -4.1664863 -4.3449183 -4.44503 -4.4307661][-4.5131311 -4.6476388 -4.6427693 -4.5399427 -4.3122463 -4.0208578 -3.7805731 -3.6327727 -3.6322193 -3.7416105 -3.9066844 -4.1104774 -4.2914391 -4.3900204 -4.3713188][-4.5099974 -4.6565819 -4.6572332 -4.5478187 -4.3027658 -3.9964323 -3.774755 -3.6770377 -3.73042 -3.8696036 -4.0163679 -4.1841116 -4.3343005 -4.4049487 -4.3652787][-4.5067649 -4.6650615 -4.69125 -4.6111236 -4.3967729 -4.1182785 -3.9324536 -3.8737822 -3.9552634 -4.101881 -4.2190847 -4.33836 -4.4442782 -4.480432 -4.4190717][-4.4789515 -4.6379766 -4.6933494 -4.6648946 -4.5253086 -4.3242054 -4.1881137 -4.1483994 -4.2223558 -4.3429775 -4.4213405 -4.4952106 -4.5623279 -4.5735765 -4.5026922][-4.4435134 -4.593646 -4.6778765 -4.7073684 -4.6561961 -4.5491877 -4.4635463 -4.4247956 -4.45708 -4.520535 -4.5579858 -4.6007032 -4.64353 -4.6411867 -4.5688243][-4.408103 -4.5293312 -4.6160316 -4.6686773 -4.6706953 -4.6306686 -4.583889 -4.5474663 -4.5467653 -4.5674419 -4.5856133 -4.6175809 -4.6481462 -4.6375051 -4.5686193][-4.3660431 -4.4424319 -4.5024543 -4.5435638 -4.5583596 -4.5497222 -4.5273733 -4.502275 -4.4934068 -4.499486 -4.517189 -4.5478911 -4.5712671 -4.5582509 -4.5014052][-4.329041 -4.36439 -4.3929386 -4.4124365 -4.422019 -4.4227686 -4.4149966 -4.4052491 -4.4022741 -4.4085183 -4.4257321 -4.4488616 -4.46228 -4.4491057 -4.4088359]]...]
INFO - root - 2017-12-07 20:53:22.408056: step 48010, loss = 21.45, batch loss = 21.37 (8.5 examples/sec; 0.942 sec/batch; 74h:26m:00s remains)
INFO - root - 2017-12-07 20:53:31.839467: step 48020, loss = 21.63, batch loss = 21.54 (8.7 examples/sec; 0.917 sec/batch; 72h:26m:14s remains)
INFO - root - 2017-12-07 20:53:41.239043: step 48030, loss = 21.32, batch loss = 21.23 (8.4 examples/sec; 0.958 sec/batch; 75h:41m:02s remains)
INFO - root - 2017-12-07 20:53:50.793532: step 48040, loss = 21.51, batch loss = 21.43 (8.1 examples/sec; 0.993 sec/batch; 78h:26m:12s remains)
INFO - root - 2017-12-07 20:54:00.323943: step 48050, loss = 22.05, batch loss = 21.97 (8.3 examples/sec; 0.962 sec/batch; 76h:00m:36s remains)
INFO - root - 2017-12-07 20:54:09.841090: step 48060, loss = 21.39, batch loss = 21.31 (8.2 examples/sec; 0.977 sec/batch; 77h:12m:05s remains)
INFO - root - 2017-12-07 20:54:19.356113: step 48070, loss = 21.39, batch loss = 21.30 (8.6 examples/sec; 0.929 sec/batch; 73h:24m:35s remains)
INFO - root - 2017-12-07 20:54:28.676085: step 48080, loss = 21.49, batch loss = 21.41 (8.2 examples/sec; 0.975 sec/batch; 77h:00m:25s remains)
INFO - root - 2017-12-07 20:54:37.933885: step 48090, loss = 21.62, batch loss = 21.54 (8.6 examples/sec; 0.932 sec/batch; 73h:36m:22s remains)
INFO - root - 2017-12-07 20:54:47.401307: step 48100, loss = 21.78, batch loss = 21.70 (8.8 examples/sec; 0.908 sec/batch; 71h:43m:03s remains)
2017-12-07 20:54:48.348425: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6013436 -4.7586446 -4.8731923 -4.9096417 -4.8715124 -4.7754159 -4.6943154 -4.6830854 -4.7003479 -4.719718 -4.7113452 -4.6940351 -4.6983438 -4.7076397 -4.6646481][-4.6222258 -4.7755923 -4.8622303 -4.851748 -4.7667294 -4.6443281 -4.5556507 -4.5493045 -4.5832582 -4.6091409 -4.5879736 -4.5647092 -4.5828648 -4.6197596 -4.5879064][-4.6144466 -4.7314973 -4.7693043 -4.7105565 -4.5906286 -4.4643893 -4.3905106 -4.3936982 -4.4274 -4.4367332 -4.39369 -4.3675904 -4.4059682 -4.4756474 -4.4773283][-4.562983 -4.629559 -4.6274881 -4.5509529 -4.4259572 -4.3107724 -4.2535686 -4.2597389 -4.2777824 -4.2516031 -4.1856313 -4.1657653 -4.22406 -4.314467 -4.3550076][-4.5056915 -4.5304785 -4.5073891 -4.4311571 -4.3128071 -4.2009082 -4.1347213 -4.1285534 -4.1358719 -4.0895472 -4.0200639 -4.0140557 -4.0853739 -4.18739 -4.2676587][-4.46939 -4.46708 -4.432 -4.3555946 -4.2463694 -4.1321139 -4.0338864 -4.0075245 -4.0302534 -4.0084667 -3.9664171 -3.9767718 -4.0486856 -4.1535296 -4.2637177][-4.4306264 -4.4034348 -4.3523426 -4.2722917 -4.1764021 -4.0660167 -3.9379058 -3.9017367 -3.9633369 -3.9949546 -3.995585 -4.0162625 -4.0749631 -4.1672454 -4.2830358][-4.4009314 -4.348084 -4.2825241 -4.1962256 -4.1058226 -3.9936271 -3.8495271 -3.8247724 -3.9301293 -4.0052729 -4.0377765 -4.0585308 -4.0992103 -4.169693 -4.2653322][-4.389379 -4.3131275 -4.2421618 -4.160253 -4.0789738 -3.9812191 -3.860518 -3.8662956 -3.9841459 -4.0592833 -4.09279 -4.1057038 -4.1345887 -4.1886539 -4.2540832][-4.3959279 -4.3132615 -4.2525897 -4.1903214 -4.1335073 -4.0721292 -4.0003252 -4.0307097 -4.1201696 -4.1571951 -4.1693668 -4.1688504 -4.1915207 -4.23755 -4.2725673][-4.4492607 -4.3903122 -4.3530145 -4.3046808 -4.2586913 -4.21857 -4.182363 -4.2185516 -4.2700768 -4.2735968 -4.2699194 -4.2567072 -4.2708626 -4.3091259 -4.3173838][-4.5318251 -4.52086 -4.5139174 -4.46954 -4.4092031 -4.3577795 -4.3307738 -4.3647327 -4.3921638 -4.3790951 -4.365809 -4.3402257 -4.3441873 -4.3758936 -4.3724389][-4.5927229 -4.6309905 -4.6534333 -4.6102653 -4.5281878 -4.454999 -4.4288254 -4.463552 -4.4800982 -4.4576068 -4.4319811 -4.3930039 -4.3855252 -4.4100609 -4.4098792][-4.6166582 -4.6887641 -4.7279382 -4.6853952 -4.5916414 -4.5093026 -4.49253 -4.5358396 -4.5505414 -4.5246525 -4.4947605 -4.4556589 -4.4418855 -4.4579396 -4.4584389][-4.5817823 -4.6592121 -4.6989732 -4.6646819 -4.5879765 -4.5238748 -4.5194983 -4.559402 -4.5696478 -4.54886 -4.5292034 -4.5056934 -4.493639 -4.4988866 -4.4952865]]...]
INFO - root - 2017-12-07 20:54:57.815479: step 48110, loss = 21.65, batch loss = 21.57 (8.8 examples/sec; 0.912 sec/batch; 72h:02m:15s remains)
INFO - root - 2017-12-07 20:55:07.173175: step 48120, loss = 21.21, batch loss = 21.13 (8.4 examples/sec; 0.951 sec/batch; 75h:08m:30s remains)
INFO - root - 2017-12-07 20:55:16.631589: step 48130, loss = 21.57, batch loss = 21.48 (8.3 examples/sec; 0.969 sec/batch; 76h:31m:48s remains)
INFO - root - 2017-12-07 20:55:26.153971: step 48140, loss = 21.62, batch loss = 21.54 (8.9 examples/sec; 0.904 sec/batch; 71h:22m:46s remains)
INFO - root - 2017-12-07 20:55:35.582697: step 48150, loss = 21.43, batch loss = 21.35 (8.2 examples/sec; 0.973 sec/batch; 76h:49m:55s remains)
INFO - root - 2017-12-07 20:55:45.022742: step 48160, loss = 21.60, batch loss = 21.52 (8.2 examples/sec; 0.980 sec/batch; 77h:22m:36s remains)
INFO - root - 2017-12-07 20:55:54.366419: step 48170, loss = 21.57, batch loss = 21.49 (7.9 examples/sec; 1.017 sec/batch; 80h:18m:15s remains)
INFO - root - 2017-12-07 20:56:03.698729: step 48180, loss = 21.63, batch loss = 21.55 (8.4 examples/sec; 0.954 sec/batch; 75h:22m:11s remains)
INFO - root - 2017-12-07 20:56:13.124433: step 48190, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.941 sec/batch; 74h:19m:03s remains)
INFO - root - 2017-12-07 20:56:22.479614: step 48200, loss = 21.67, batch loss = 21.59 (8.6 examples/sec; 0.927 sec/batch; 73h:12m:13s remains)
2017-12-07 20:56:23.460079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3408241 -4.3502069 -4.3595843 -4.3702 -4.3762026 -4.3874593 -4.3957915 -4.3906651 -4.3808403 -4.3794951 -4.3891568 -4.3979077 -4.3875375 -4.356401 -4.3123856][-4.3342314 -4.365325 -4.400135 -4.4359918 -4.4574919 -4.4723296 -4.4729862 -4.4542189 -4.4367719 -4.437396 -4.4565873 -4.4731607 -4.4605875 -4.4155126 -4.3535681][-4.324049 -4.3788681 -4.4477243 -4.5197196 -4.5665951 -4.5850196 -4.5638843 -4.5121655 -4.468771 -4.4589596 -4.4845514 -4.5169177 -4.5180364 -4.4780641 -4.4124045][-4.3343744 -4.4046659 -4.5017715 -4.6069064 -4.6767364 -4.6896744 -4.6293693 -4.5264692 -4.4432373 -4.4152184 -4.4475532 -4.5020332 -4.5277896 -4.5086527 -4.452642][-4.3645077 -4.4382544 -4.5495205 -4.6740847 -4.7509527 -4.7362967 -4.6170015 -4.4525223 -4.3291049 -4.2902131 -4.3395629 -4.4253564 -4.4856935 -4.4990177 -4.4633141][-4.3893647 -4.4589019 -4.5725069 -4.697535 -4.7562017 -4.6880326 -4.4952092 -4.271461 -4.12148 -4.0895739 -4.1664 -4.2902403 -4.395628 -4.4542465 -4.4528041][-4.3919263 -4.4612985 -4.5711904 -4.6792932 -4.6976886 -4.5606027 -4.2944136 -4.0242562 -3.8612695 -3.8446493 -3.9510779 -4.1156826 -4.2746086 -4.38523 -4.4235287][-4.3946228 -4.4726624 -4.5770631 -4.6585755 -4.6279936 -4.4280562 -4.1091695 -3.813062 -3.6446295 -3.6404452 -3.7714376 -3.973397 -4.1786213 -4.3265386 -4.3888345][-4.41814 -4.5098443 -4.6084652 -4.6633353 -4.5932369 -4.3602738 -4.0301752 -3.7378261 -3.5712564 -3.5687933 -3.7081034 -3.9299014 -4.1575818 -4.3138137 -4.3743148][-4.4373727 -4.5359354 -4.6314478 -4.674057 -4.5927105 -4.3701534 -4.0755172 -3.8187613 -3.6667922 -3.6631515 -3.7958179 -4.0111117 -4.2255745 -4.3583055 -4.3955779][-4.4389958 -4.5345731 -4.6264582 -4.6739006 -4.6143837 -4.4401364 -4.2113457 -4.0091047 -3.886951 -3.889015 -4.0055885 -4.1866074 -4.3539743 -4.4397988 -4.4469643][-4.4574804 -4.5426288 -4.6228709 -4.6752572 -4.6488495 -4.5345716 -4.3747211 -4.2271295 -4.1405869 -4.1521664 -4.2454634 -4.36981 -4.46678 -4.4983091 -4.4827924][-4.5115366 -4.5810556 -4.6394935 -4.6810622 -4.6741896 -4.6072669 -4.5045452 -4.4044013 -4.3519444 -4.369041 -4.4274025 -4.4840922 -4.5090237 -4.4968019 -4.4722872][-4.5627623 -4.6245193 -4.6607194 -4.6782603 -4.6703587 -4.63178 -4.5769815 -4.5226345 -4.4962859 -4.5011306 -4.5093808 -4.5034165 -4.4807105 -4.4521914 -4.4356132][-4.5802617 -4.6468096 -4.6721044 -4.6634164 -4.640707 -4.6119809 -4.5917335 -4.5770636 -4.5641341 -4.5389709 -4.49189 -4.4428225 -4.4091969 -4.3964796 -4.4047413]]...]
INFO - root - 2017-12-07 20:56:32.874549: step 48210, loss = 21.33, batch loss = 21.25 (8.3 examples/sec; 0.969 sec/batch; 76h:33m:17s remains)
INFO - root - 2017-12-07 20:56:42.383660: step 48220, loss = 21.28, batch loss = 21.19 (8.5 examples/sec; 0.940 sec/batch; 74h:11m:45s remains)
INFO - root - 2017-12-07 20:56:51.807864: step 48230, loss = 21.25, batch loss = 21.17 (9.1 examples/sec; 0.881 sec/batch; 69h:33m:20s remains)
INFO - root - 2017-12-07 20:57:01.191370: step 48240, loss = 21.06, batch loss = 20.98 (8.6 examples/sec; 0.932 sec/batch; 73h:34m:31s remains)
INFO - root - 2017-12-07 20:57:10.524391: step 48250, loss = 21.16, batch loss = 21.08 (9.1 examples/sec; 0.882 sec/batch; 69h:38m:58s remains)
INFO - root - 2017-12-07 20:57:19.963489: step 48260, loss = 21.17, batch loss = 21.08 (8.9 examples/sec; 0.903 sec/batch; 71h:17m:45s remains)
INFO - root - 2017-12-07 20:57:29.523410: step 48270, loss = 21.47, batch loss = 21.39 (8.3 examples/sec; 0.959 sec/batch; 75h:42m:57s remains)
INFO - root - 2017-12-07 20:57:39.013564: step 48280, loss = 21.50, batch loss = 21.41 (8.7 examples/sec; 0.922 sec/batch; 72h:48m:19s remains)
INFO - root - 2017-12-07 20:57:48.163696: step 48290, loss = 21.57, batch loss = 21.49 (9.3 examples/sec; 0.864 sec/batch; 68h:11m:16s remains)
INFO - root - 2017-12-07 20:57:57.484414: step 48300, loss = 21.51, batch loss = 21.42 (8.8 examples/sec; 0.913 sec/batch; 72h:03m:32s remains)
2017-12-07 20:57:58.472790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4141893 -4.4496441 -4.460619 -4.430563 -4.3788943 -4.3364649 -4.3307419 -4.3647075 -4.4025049 -4.4104152 -4.3929329 -4.3728418 -4.3745871 -4.4073625 -4.4615927][-4.4399271 -4.4852047 -4.5017633 -4.4669218 -4.4018393 -4.3502364 -4.3538384 -4.4109173 -4.4537568 -4.4330726 -4.370472 -4.3136764 -4.3083868 -4.3626866 -4.4509482][-4.4434419 -4.4818821 -4.4888773 -4.4412174 -4.3655391 -4.3136711 -4.3297873 -4.4037671 -4.4448409 -4.401361 -4.3099451 -4.2339272 -4.232542 -4.3117065 -4.4350686][-4.4483223 -4.4688106 -4.4497609 -4.3745356 -4.279562 -4.2211766 -4.238049 -4.3170629 -4.36241 -4.3216395 -4.2341108 -4.1588483 -4.1582503 -4.241014 -4.3770847][-4.4629602 -4.4671855 -4.4147582 -4.298285 -4.1711597 -4.0958047 -4.1040707 -4.1857572 -4.246274 -4.2319055 -4.166224 -4.0971775 -4.0828834 -4.1367135 -4.2485218][-4.4676256 -4.4608569 -4.3752356 -4.2134509 -4.0483475 -3.9478862 -3.9402862 -4.017046 -4.0914593 -4.107698 -4.066072 -4.0002623 -3.9649427 -3.98432 -4.06528][-4.4490881 -4.4315424 -4.3152542 -4.1128187 -3.9178081 -3.8018734 -3.7827559 -3.8507421 -3.9282429 -3.9648426 -3.947063 -3.8927555 -3.8509791 -3.8544288 -3.9170568][-4.4332986 -4.4048266 -4.2623858 -4.0306311 -3.8216472 -3.7140992 -3.7062597 -3.768039 -3.8329327 -3.8694658 -3.870965 -3.8414178 -3.8209968 -3.8421617 -3.9109874][-4.4510345 -4.4159074 -4.2587676 -4.0113072 -3.7981892 -3.7092073 -3.7272682 -3.787137 -3.8288088 -3.8428628 -3.8478804 -3.845392 -3.8643355 -3.9312127 -4.0346017][-4.5048046 -4.4737225 -4.3226514 -4.0805759 -3.8742998 -3.802176 -3.8404009 -3.9085488 -3.9389539 -3.9301524 -3.9233871 -3.92983 -3.9747055 -4.0741272 -4.2054343][-4.5343733 -4.5258527 -4.4112244 -4.2136278 -4.0430708 -3.988312 -4.0338273 -4.10852 -4.1408486 -4.122467 -4.1027074 -4.1028733 -4.145216 -4.2389216 -4.3593159][-4.4983749 -4.5155635 -4.4576631 -4.338695 -4.2325182 -4.20118 -4.2377906 -4.2970757 -4.3189311 -4.2975569 -4.2738309 -4.2758055 -4.3163309 -4.3924837 -4.4822321][-4.4108715 -4.4424787 -4.4383779 -4.4013734 -4.365252 -4.3611364 -4.3890572 -4.4309926 -4.4464912 -4.4321356 -4.416914 -4.4280324 -4.466959 -4.5188394 -4.5690713][-4.307302 -4.3344183 -4.3585782 -4.3758817 -4.3909516 -4.4098725 -4.438683 -4.4772034 -4.4970117 -4.4941254 -4.4882684 -4.5001054 -4.5267863 -4.5499353 -4.5634861][-4.2301917 -4.2455573 -4.2724361 -4.3046074 -4.3346043 -4.3588986 -4.3831182 -4.4120741 -4.4297514 -4.431653 -4.4308767 -4.4410129 -4.4566422 -4.4636574 -4.4605861]]...]
INFO - root - 2017-12-07 20:58:07.889289: step 48310, loss = 21.34, batch loss = 21.26 (9.1 examples/sec; 0.876 sec/batch; 69h:07m:26s remains)
INFO - root - 2017-12-07 20:58:17.239407: step 48320, loss = 21.69, batch loss = 21.61 (8.5 examples/sec; 0.943 sec/batch; 74h:27m:50s remains)
INFO - root - 2017-12-07 20:58:26.677769: step 48330, loss = 21.62, batch loss = 21.54 (8.1 examples/sec; 0.989 sec/batch; 78h:05m:30s remains)
INFO - root - 2017-12-07 20:58:36.095037: step 48340, loss = 21.91, batch loss = 21.83 (8.0 examples/sec; 1.000 sec/batch; 78h:56m:34s remains)
INFO - root - 2017-12-07 20:58:45.483525: step 48350, loss = 22.04, batch loss = 21.96 (8.3 examples/sec; 0.959 sec/batch; 75h:43m:44s remains)
INFO - root - 2017-12-07 20:58:54.887994: step 48360, loss = 21.52, batch loss = 21.44 (9.0 examples/sec; 0.892 sec/batch; 70h:25m:37s remains)
INFO - root - 2017-12-07 20:59:04.320179: step 48370, loss = 21.38, batch loss = 21.29 (8.1 examples/sec; 0.986 sec/batch; 77h:51m:30s remains)
INFO - root - 2017-12-07 20:59:13.736880: step 48380, loss = 21.45, batch loss = 21.37 (8.3 examples/sec; 0.964 sec/batch; 76h:05m:25s remains)
INFO - root - 2017-12-07 20:59:23.080073: step 48390, loss = 21.36, batch loss = 21.28 (8.6 examples/sec; 0.932 sec/batch; 73h:34m:25s remains)
INFO - root - 2017-12-07 20:59:32.502569: step 48400, loss = 21.33, batch loss = 21.25 (9.4 examples/sec; 0.847 sec/batch; 66h:51m:05s remains)
2017-12-07 20:59:33.465277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4388318 -4.5345955 -4.6210561 -4.6059132 -4.4709625 -4.3073559 -4.1985164 -4.1755619 -4.21335 -4.2894683 -4.3490539 -4.4091458 -4.4537635 -4.4201131 -4.3884149][-4.4400525 -4.5279355 -4.601222 -4.569665 -4.4188404 -4.2447453 -4.1353927 -4.1282206 -4.1881161 -4.2839255 -4.3686171 -4.4408541 -4.4901581 -4.4649291 -4.4336681][-4.4369388 -4.5211396 -4.585053 -4.538847 -4.3722692 -4.1865077 -4.0722508 -4.0698657 -4.1377 -4.2420807 -4.3390479 -4.4187212 -4.4777946 -4.4752545 -4.4593258][-4.4341149 -4.5157571 -4.5732145 -4.5155449 -4.3343067 -4.1346312 -4.012351 -4.00869 -4.0754457 -4.1767921 -4.2707634 -4.3440766 -4.4081359 -4.4338207 -4.4446316][-4.4301009 -4.5107055 -4.5638757 -4.4983845 -4.3056288 -4.0935621 -3.966841 -3.9668674 -4.03974 -4.1379113 -4.2188683 -4.2728033 -4.3272629 -4.3670845 -4.401546][-4.4274483 -4.5066729 -4.5578895 -4.489017 -4.2899971 -4.0701151 -3.9407825 -3.948349 -4.0332818 -4.1337867 -4.2082939 -4.2497549 -4.2912436 -4.3295307 -4.3719583][-4.427032 -4.5050373 -4.5569763 -4.4895916 -4.2910094 -4.0685115 -3.9355166 -3.9460959 -4.0375504 -4.1387553 -4.2147341 -4.2568226 -4.2914109 -4.32116 -4.3604712][-4.4276438 -4.5071855 -4.5618596 -4.4988704 -4.3051271 -4.0855374 -3.9528794 -3.9658725 -4.0587378 -4.157557 -4.2335868 -4.2794895 -4.3132925 -4.3381071 -4.3758945][-4.4302926 -4.5113258 -4.5670686 -4.5049667 -4.3122339 -4.0946374 -3.9652665 -3.9839487 -4.0812221 -4.1830215 -4.2646141 -4.3205695 -4.3612189 -4.3857374 -4.4212017][-4.432898 -4.5140285 -4.5665669 -4.4978576 -4.2975111 -4.0747051 -3.944366 -3.9673407 -4.0750947 -4.191782 -4.2903252 -4.3644419 -4.4176426 -4.44164 -4.467515][-4.4343929 -4.5128493 -4.5586343 -4.4795122 -4.2705884 -4.0449967 -3.9166803 -3.9461579 -4.0667868 -4.200449 -4.3137026 -4.4004498 -4.4601903 -4.4789057 -4.4906855][-4.4323716 -4.5075426 -4.5482955 -4.4676762 -4.2666078 -4.0571623 -3.9411998 -3.9714248 -4.0856481 -4.2116156 -4.3172026 -4.4025068 -4.4643764 -4.4827046 -4.4884896][-4.4274383 -4.5014739 -4.5462856 -4.4825034 -4.3088617 -4.1227098 -4.0076985 -4.0088992 -4.0834002 -4.180234 -4.27299 -4.3630939 -4.435379 -4.4609871 -4.4685879][-4.4220872 -4.4983954 -4.5592866 -4.5299778 -4.398602 -4.2376738 -4.1113362 -4.0577755 -4.0713816 -4.1372232 -4.2312083 -4.3402615 -4.4268312 -4.45687 -4.4639883][-4.4169579 -4.4987607 -4.5844436 -4.6047339 -4.5336514 -4.4149203 -4.28939 -4.182868 -4.1270223 -4.1514511 -4.2376857 -4.3565273 -4.4490929 -4.4770131 -4.4801464]]...]
INFO - root - 2017-12-07 20:59:42.915363: step 48410, loss = 21.61, batch loss = 21.53 (7.9 examples/sec; 1.014 sec/batch; 80h:02m:20s remains)
INFO - root - 2017-12-07 20:59:52.256772: step 48420, loss = 21.49, batch loss = 21.41 (8.5 examples/sec; 0.939 sec/batch; 74h:04m:18s remains)
INFO - root - 2017-12-07 21:00:01.640293: step 48430, loss = 21.77, batch loss = 21.68 (8.8 examples/sec; 0.910 sec/batch; 71h:49m:21s remains)
INFO - root - 2017-12-07 21:00:10.928598: step 48440, loss = 21.38, batch loss = 21.30 (8.7 examples/sec; 0.922 sec/batch; 72h:43m:14s remains)
INFO - root - 2017-12-07 21:00:20.340958: step 48450, loss = 21.56, batch loss = 21.48 (8.8 examples/sec; 0.914 sec/batch; 72h:04m:58s remains)
INFO - root - 2017-12-07 21:00:29.668637: step 48460, loss = 21.35, batch loss = 21.27 (8.6 examples/sec; 0.930 sec/batch; 73h:24m:21s remains)
INFO - root - 2017-12-07 21:00:39.076122: step 48470, loss = 21.24, batch loss = 21.15 (8.5 examples/sec; 0.946 sec/batch; 74h:37m:20s remains)
INFO - root - 2017-12-07 21:00:48.554610: step 48480, loss = 21.64, batch loss = 21.56 (8.2 examples/sec; 0.977 sec/batch; 77h:03m:38s remains)
INFO - root - 2017-12-07 21:00:57.849097: step 48490, loss = 20.90, batch loss = 20.82 (7.9 examples/sec; 1.018 sec/batch; 80h:18m:07s remains)
INFO - root - 2017-12-07 21:01:07.157974: step 48500, loss = 21.35, batch loss = 21.26 (8.2 examples/sec; 0.973 sec/batch; 76h:44m:24s remains)
2017-12-07 21:01:08.118494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4222889 -4.4524217 -4.4867754 -4.5166192 -4.5405741 -4.5555849 -4.5555911 -4.542984 -4.5301 -4.5266829 -4.5397263 -4.5627651 -4.577219 -4.5708046 -4.5402136][-4.48078 -4.5378919 -4.5901384 -4.6275539 -4.6553249 -4.667922 -4.6566858 -4.6320472 -4.614408 -4.6212111 -4.659677 -4.7073665 -4.7241125 -4.70037 -4.6426973][-4.5383744 -4.6199269 -4.6749744 -4.6977167 -4.7083778 -4.6979032 -4.6598563 -4.6171618 -4.6028996 -4.63889 -4.7252669 -4.8095922 -4.8278265 -4.7851567 -4.7016344][-4.572865 -4.6708217 -4.7170353 -4.71121 -4.6851187 -4.625083 -4.5362258 -4.4628716 -4.4586749 -4.5450563 -4.6987557 -4.8286715 -4.8513 -4.7946172 -4.6920395][-4.5705829 -4.6780915 -4.7172017 -4.6871357 -4.6182642 -4.4964328 -4.343761 -4.228466 -4.2310243 -4.3648405 -4.5759254 -4.747086 -4.7877412 -4.7392373 -4.6332335][-4.5442181 -4.649334 -4.6790571 -4.6271944 -4.5167456 -4.3432035 -4.1390839 -3.9825239 -3.9765587 -4.1217875 -4.3484039 -4.5470924 -4.6293178 -4.6265855 -4.5489345][-4.5174546 -4.599812 -4.6055422 -4.5255384 -4.3810654 -4.1837368 -3.9641747 -3.7893975 -3.770164 -3.8890526 -4.0905175 -4.3035479 -4.4429536 -4.5061083 -4.4760051][-4.503922 -4.5558362 -4.5337758 -4.42934 -4.26188 -4.0669684 -3.8733211 -3.7187376 -3.694092 -3.7706561 -3.9277055 -4.1384091 -4.3192983 -4.4345975 -4.4453754][-4.5133176 -4.5473008 -4.5137239 -4.4040871 -4.2363558 -4.0621319 -3.919096 -3.8124404 -3.7910285 -3.8305495 -3.9446301 -4.1374264 -4.3251586 -4.4504814 -4.4690938][-4.5385809 -4.5806451 -4.5615253 -4.4704304 -4.3247805 -4.1850414 -4.1008449 -4.0502458 -4.0387716 -4.0581522 -4.1421041 -4.3061948 -4.4623938 -4.5463266 -4.5268221][-4.5593629 -4.6196861 -4.6183877 -4.5487375 -4.4310837 -4.3294673 -4.29781 -4.2928619 -4.2936773 -4.3086858 -4.374609 -4.5045252 -4.6128678 -4.6362839 -4.5639572][-4.55418 -4.6210537 -4.6313806 -4.5808816 -4.4937358 -4.4287581 -4.4336419 -4.4601026 -4.4733281 -4.4865017 -4.5339632 -4.6237993 -4.6860418 -4.6617537 -4.5567851][-4.5147061 -4.5743475 -4.5924497 -4.5661645 -4.51721 -4.4883389 -4.513268 -4.5542526 -4.5725231 -4.5781651 -4.6023231 -4.6512337 -4.6744137 -4.6256075 -4.518352][-4.4626679 -4.5047245 -4.5240626 -4.5205379 -4.5067835 -4.5042038 -4.5303741 -4.5653629 -4.57898 -4.5759554 -4.5816078 -4.6009712 -4.6019769 -4.5525379 -4.4681735][-4.4147577 -4.4369845 -4.4491029 -4.454329 -4.4582009 -4.4650421 -4.4806309 -4.4986663 -4.5035715 -4.4976883 -4.4976273 -4.506155 -4.5032029 -4.4710927 -4.4209037]]...]
INFO - root - 2017-12-07 21:01:17.572950: step 48510, loss = 21.41, batch loss = 21.32 (8.7 examples/sec; 0.923 sec/batch; 72h:50m:51s remains)
INFO - root - 2017-12-07 21:01:27.068148: step 48520, loss = 21.69, batch loss = 21.60 (8.6 examples/sec; 0.932 sec/batch; 73h:31m:56s remains)
INFO - root - 2017-12-07 21:01:36.469467: step 48530, loss = 21.54, batch loss = 21.45 (8.2 examples/sec; 0.975 sec/batch; 76h:56m:00s remains)
INFO - root - 2017-12-07 21:01:45.885818: step 48540, loss = 21.56, batch loss = 21.47 (8.6 examples/sec; 0.934 sec/batch; 73h:42m:06s remains)
INFO - root - 2017-12-07 21:01:55.174511: step 48550, loss = 21.52, batch loss = 21.44 (8.9 examples/sec; 0.901 sec/batch; 71h:02m:56s remains)
INFO - root - 2017-12-07 21:02:04.627049: step 48560, loss = 21.29, batch loss = 21.21 (8.6 examples/sec; 0.931 sec/batch; 73h:25m:26s remains)
INFO - root - 2017-12-07 21:02:13.983563: step 48570, loss = 21.63, batch loss = 21.55 (8.4 examples/sec; 0.957 sec/batch; 75h:26m:25s remains)
INFO - root - 2017-12-07 21:02:23.323020: step 48580, loss = 21.39, batch loss = 21.31 (8.8 examples/sec; 0.909 sec/batch; 71h:40m:02s remains)
INFO - root - 2017-12-07 21:02:32.771149: step 48590, loss = 21.29, batch loss = 21.21 (8.6 examples/sec; 0.931 sec/batch; 73h:24m:55s remains)
INFO - root - 2017-12-07 21:02:42.154737: step 48600, loss = 21.41, batch loss = 21.33 (8.3 examples/sec; 0.967 sec/batch; 76h:17m:20s remains)
2017-12-07 21:02:43.149692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5040298 -4.4826956 -4.4645414 -4.4596214 -4.4737091 -4.4991426 -4.5261602 -4.5428362 -4.539598 -4.5213213 -4.4984164 -4.4798865 -4.4701791 -4.4672747 -4.4652276][-4.60429 -4.5761547 -4.5463758 -4.5324559 -4.5401039 -4.5596228 -4.5828147 -4.602118 -4.6044688 -4.5907207 -4.5696282 -4.5501142 -4.54142 -4.5422611 -4.542695][-4.672895 -4.6366496 -4.593915 -4.5645285 -4.5501761 -4.5388427 -4.5315661 -4.5388217 -4.5501733 -4.5596633 -4.5635471 -4.559659 -4.5631204 -4.5802946 -4.5968103][-4.6728153 -4.6249528 -4.5720415 -4.5306349 -4.4898663 -4.4282818 -4.3620253 -4.3367772 -4.3579636 -4.4086819 -4.45983 -4.4879913 -4.5133266 -4.5562067 -4.601862][-4.6018958 -4.542625 -4.4882054 -4.4444261 -4.3821459 -4.2668371 -4.1343927 -4.0723066 -4.1072688 -4.2046576 -4.3052797 -4.3633757 -4.4046497 -4.4664044 -4.5454874][-4.4785209 -4.4029589 -4.3537564 -4.3189955 -4.2502141 -4.1030312 -3.92893 -3.8486307 -3.9008384 -4.0309048 -4.1617737 -4.2374187 -4.2842197 -4.3519936 -4.4603467][-4.3549643 -4.2724333 -4.2387743 -4.2206244 -4.1595654 -4.0120773 -3.8347149 -3.7560511 -3.8161864 -3.9479661 -4.0788608 -4.1623778 -4.2120113 -4.2779403 -4.3999705][-4.2851915 -4.2238297 -4.2092724 -4.1951108 -4.1347609 -4.0031381 -3.8518734 -3.7936094 -3.8563089 -3.9683726 -4.0819416 -4.16981 -4.2207956 -4.2771735 -4.3908825][-4.2684875 -4.2409539 -4.2387571 -4.2181978 -4.1569867 -4.0558977 -3.9562902 -3.9383802 -4.010282 -4.1020746 -4.196991 -4.2806625 -4.3223734 -4.3583045 -4.4429393][-4.2823882 -4.2781038 -4.2805324 -4.2609782 -4.2120895 -4.1508055 -4.1105518 -4.1353621 -4.2229533 -4.3076925 -4.3863678 -4.44737 -4.458251 -4.4600658 -4.5034838][-4.2828913 -4.2775126 -4.2676382 -4.2464705 -4.211833 -4.1862183 -4.1962347 -4.2656927 -4.3851123 -4.4791274 -4.539947 -4.563992 -4.5351009 -4.50885 -4.5277734][-4.2560148 -4.2209363 -4.1809688 -4.155839 -4.1425924 -4.1497779 -4.1978941 -4.3054214 -4.4522905 -4.5509863 -4.5846882 -4.5630965 -4.4991622 -4.4712739 -4.5062075][-4.2183042 -4.1468711 -4.0749092 -4.0506167 -4.068244 -4.1122375 -4.1896791 -4.31771 -4.4618268 -4.5344572 -4.5273561 -4.4657378 -4.3793788 -4.3673759 -4.4406953][-4.1756325 -4.0838037 -3.9882128 -3.9555194 -3.9898343 -4.0611 -4.1655297 -4.3086143 -4.4329815 -4.4709177 -4.435904 -4.357585 -4.2688713 -4.277236 -4.3810339][-4.1387887 -4.0439892 -3.9394188 -3.8906412 -3.918581 -3.9985971 -4.1183877 -4.2696114 -4.3778491 -4.4028087 -4.3730793 -4.3059206 -4.2337003 -4.2601061 -4.3745146]]...]
INFO - root - 2017-12-07 21:02:52.497185: step 48610, loss = 21.88, batch loss = 21.80 (8.2 examples/sec; 0.970 sec/batch; 76h:28m:53s remains)
INFO - root - 2017-12-07 21:03:02.025300: step 48620, loss = 21.25, batch loss = 21.17 (8.3 examples/sec; 0.963 sec/batch; 75h:57m:36s remains)
INFO - root - 2017-12-07 21:03:11.393944: step 48630, loss = 21.22, batch loss = 21.14 (8.5 examples/sec; 0.943 sec/batch; 74h:19m:49s remains)
INFO - root - 2017-12-07 21:03:20.818855: step 48640, loss = 21.40, batch loss = 21.32 (8.7 examples/sec; 0.923 sec/batch; 72h:48m:30s remains)
INFO - root - 2017-12-07 21:03:30.194925: step 48650, loss = 21.62, batch loss = 21.54 (8.4 examples/sec; 0.955 sec/batch; 75h:19m:58s remains)
INFO - root - 2017-12-07 21:03:39.684671: step 48660, loss = 21.61, batch loss = 21.52 (7.7 examples/sec; 1.035 sec/batch; 81h:34m:00s remains)
INFO - root - 2017-12-07 21:03:49.006347: step 48670, loss = 21.23, batch loss = 21.15 (8.9 examples/sec; 0.901 sec/batch; 71h:03m:07s remains)
INFO - root - 2017-12-07 21:03:58.506184: step 48680, loss = 21.54, batch loss = 21.46 (8.6 examples/sec; 0.932 sec/batch; 73h:27m:09s remains)
INFO - root - 2017-12-07 21:04:07.926738: step 48690, loss = 21.12, batch loss = 21.04 (8.7 examples/sec; 0.921 sec/batch; 72h:35m:34s remains)
INFO - root - 2017-12-07 21:04:17.125410: step 48700, loss = 21.90, batch loss = 21.82 (8.3 examples/sec; 0.966 sec/batch; 76h:09m:01s remains)
2017-12-07 21:04:18.053389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4810781 -4.5639429 -4.5967975 -4.5597553 -4.4807429 -4.3988476 -4.3622551 -4.3678575 -4.371913 -4.3609405 -4.3594937 -4.3725557 -4.3932452 -4.4197373 -4.4406614][-4.5080428 -4.5836453 -4.5875516 -4.5084271 -4.3938365 -4.2949262 -4.2573771 -4.2711349 -4.2849035 -4.2980161 -4.3329253 -4.3763161 -4.4127469 -4.4466949 -4.4746685][-4.5209737 -4.5846596 -4.5525646 -4.4239435 -4.2660232 -4.141221 -4.0966983 -4.1176372 -4.1541729 -4.2092524 -4.2880116 -4.362535 -4.4100175 -4.4462667 -4.4780474][-4.5297203 -4.5805397 -4.5146165 -4.3419042 -4.1466689 -3.9999201 -3.9518948 -3.9876387 -4.0605907 -4.16305 -4.2760515 -4.3659463 -4.4066343 -4.4311113 -4.4604535][-4.5386748 -4.5778756 -4.4888716 -4.2921224 -4.0772085 -3.9209125 -3.8777723 -3.93076 -4.0363255 -4.1684256 -4.2976866 -4.3884749 -4.4125862 -4.4179349 -4.438931][-4.5467834 -4.5781808 -4.4794288 -4.2777762 -4.0597992 -3.9063694 -3.8695436 -3.9358926 -4.05641 -4.19621 -4.3273163 -4.4141188 -4.4257097 -4.4150839 -4.4250426][-4.5495381 -4.5769844 -4.4735246 -4.2696028 -4.0531926 -3.9085679 -3.8802729 -3.95715 -4.0825577 -4.219943 -4.345366 -4.4275622 -4.4380846 -4.4227924 -4.4229693][-4.5462427 -4.5731158 -4.4658241 -4.2554049 -4.0391622 -3.9048414 -3.8887949 -3.9767163 -4.1020412 -4.2292233 -4.3408308 -4.4160795 -4.4327493 -4.4235029 -4.4213176][-4.5438638 -4.573 -4.4645481 -4.2513032 -4.0392771 -3.91786 -3.9166989 -4.0150495 -4.1384826 -4.2479944 -4.335228 -4.3943038 -4.4111228 -4.4077439 -4.4090319][-4.5493779 -4.5786591 -4.47334 -4.26479 -4.0615582 -3.9545827 -3.968796 -4.0780325 -4.1960931 -4.278306 -4.3317389 -4.3655624 -4.3728056 -4.3722196 -4.3806248][-4.5513468 -4.5765276 -4.4759707 -4.2749243 -4.0790138 -3.9789765 -3.9973502 -4.107235 -4.2143154 -4.2744455 -4.3060613 -4.3257227 -4.3272505 -4.3292656 -4.3436446][-4.5389304 -4.5573254 -4.4569545 -4.2556543 -4.0551333 -3.9443564 -3.9464331 -4.0413146 -4.1395693 -4.2040668 -4.2490387 -4.2834768 -4.2975903 -4.3038259 -4.3149571][-4.5201116 -4.5292888 -4.4216614 -4.2078657 -3.9892011 -3.8525763 -3.8256745 -3.8977051 -3.9978735 -4.09614 -4.1852884 -4.2591381 -4.3003278 -4.3079867 -4.300498][-4.5085921 -4.5097589 -4.3936477 -4.16561 -3.9266233 -3.761277 -3.7035265 -3.7492471 -3.8555479 -3.9982777 -4.1404567 -4.26149 -4.3301325 -4.3302536 -4.2907934][-4.5072937 -4.5065026 -4.3942351 -4.1689415 -3.9243414 -3.7431087 -3.6612959 -3.6829627 -3.7951887 -3.9740179 -4.156776 -4.3105469 -4.3885264 -4.3659163 -4.2860756]]...]
INFO - root - 2017-12-07 21:04:27.461334: step 48710, loss = 21.35, batch loss = 21.27 (8.3 examples/sec; 0.963 sec/batch; 75h:54m:04s remains)
INFO - root - 2017-12-07 21:04:36.868641: step 48720, loss = 21.75, batch loss = 21.66 (8.5 examples/sec; 0.938 sec/batch; 73h:54m:32s remains)
INFO - root - 2017-12-07 21:04:46.155370: step 48730, loss = 21.44, batch loss = 21.36 (8.2 examples/sec; 0.973 sec/batch; 76h:43m:37s remains)
INFO - root - 2017-12-07 21:04:55.770974: step 48740, loss = 21.60, batch loss = 21.52 (8.4 examples/sec; 0.951 sec/batch; 74h:59m:42s remains)
INFO - root - 2017-12-07 21:05:05.065658: step 48750, loss = 21.74, batch loss = 21.66 (9.0 examples/sec; 0.892 sec/batch; 70h:20m:11s remains)
INFO - root - 2017-12-07 21:05:14.578922: step 48760, loss = 21.62, batch loss = 21.54 (8.6 examples/sec; 0.932 sec/batch; 73h:29m:19s remains)
INFO - root - 2017-12-07 21:05:23.870780: step 48770, loss = 21.49, batch loss = 21.41 (8.6 examples/sec; 0.932 sec/batch; 73h:25m:57s remains)
INFO - root - 2017-12-07 21:05:33.129007: step 48780, loss = 21.42, batch loss = 21.33 (8.7 examples/sec; 0.921 sec/batch; 72h:35m:42s remains)
INFO - root - 2017-12-07 21:05:42.680522: step 48790, loss = 21.09, batch loss = 21.01 (8.4 examples/sec; 0.956 sec/batch; 75h:20m:46s remains)
INFO - root - 2017-12-07 21:05:51.976517: step 48800, loss = 21.80, batch loss = 21.72 (8.4 examples/sec; 0.949 sec/batch; 74h:47m:53s remains)
2017-12-07 21:05:53.043010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3387709 -4.4372525 -4.5534954 -4.6502371 -4.706955 -4.6821237 -4.5575137 -4.364377 -4.1440158 -3.9533739 -3.8632281 -3.9020903 -4.0416179 -4.2335234 -4.4062452][-4.346447 -4.4788566 -4.6110339 -4.705442 -4.7553663 -4.7453637 -4.6622615 -4.5272851 -4.3728709 -4.2434292 -4.1856627 -4.2197361 -4.319829 -4.4446421 -4.5411005][-4.3550072 -4.5088544 -4.6439023 -4.7274466 -4.7683096 -4.7705083 -4.7229891 -4.6392441 -4.5519228 -4.4925313 -4.4805617 -4.5212526 -4.5869865 -4.6405821 -4.6495395][-4.3716278 -4.5313549 -4.6562824 -4.7153959 -4.7221642 -4.6958308 -4.6399536 -4.5789595 -4.5549603 -4.5750585 -4.6283231 -4.6963568 -4.7440739 -4.738997 -4.6769881][-4.4025917 -4.5458188 -4.6424179 -4.6578507 -4.5997839 -4.4977546 -4.3804245 -4.3096347 -4.3381643 -4.4412727 -4.5703173 -4.6830077 -4.7383657 -4.7092733 -4.6175227][-4.4413409 -4.5515547 -4.6097894 -4.5710893 -4.4379086 -4.2441831 -4.0451889 -3.9430134 -3.9975483 -4.1611309 -4.3611617 -4.5321431 -4.6183405 -4.5983634 -4.5137539][-4.4677787 -4.5473404 -4.5684772 -4.4762034 -4.2779226 -4.0114336 -3.7507939 -3.6208777 -3.6824281 -3.874877 -4.1227789 -4.3460655 -4.4689722 -4.4738145 -4.4139986][-4.4744105 -4.5442696 -4.5479107 -4.423965 -4.190115 -3.8948197 -3.619591 -3.4872255 -3.5508821 -3.7480969 -4.0082741 -4.2516317 -4.3880873 -4.4032092 -4.358891][-4.46399 -4.5505571 -4.5696516 -4.4543142 -4.2304258 -3.953512 -3.7049174 -3.5935028 -3.6601629 -3.8445997 -4.0832348 -4.3049445 -4.4180546 -4.4145403 -4.361104][-4.4372811 -4.5530329 -4.6082444 -4.5313377 -4.3482037 -4.1162248 -3.9120255 -3.8263078 -3.8935494 -4.0563755 -4.2572355 -4.4335837 -4.5015159 -4.4630713 -4.3839931][-4.4013405 -4.5448871 -4.639997 -4.6155357 -4.4906659 -4.3181505 -4.1663504 -4.1080437 -4.1700916 -4.3002973 -4.4470878 -4.5560279 -4.5642066 -4.4849725 -4.3823123][-4.3628969 -4.5151062 -4.6395931 -4.6690273 -4.61017 -4.505435 -4.4111996 -4.3836679 -4.44132 -4.5326838 -4.6096773 -4.6334372 -4.5722127 -4.4555116 -4.342401][-4.3354921 -4.4667149 -4.5918593 -4.6569905 -4.6554637 -4.6125031 -4.5691118 -4.565414 -4.6128893 -4.66778 -4.6826358 -4.6358018 -4.526104 -4.3929338 -4.2855306][-4.3252859 -4.4102626 -4.5052295 -4.5774751 -4.6137505 -4.6205254 -4.6165438 -4.6242676 -4.6525178 -4.6727705 -4.6475487 -4.5693269 -4.4505424 -4.3294878 -4.2412477][-4.3109784 -4.3450265 -4.394608 -4.4471121 -4.4939938 -4.5296359 -4.55306 -4.568893 -4.5812149 -4.5766311 -4.5374947 -4.463903 -4.3699203 -4.2825947 -4.2212253]]...]
INFO - root - 2017-12-07 21:06:02.472095: step 48810, loss = 21.58, batch loss = 21.49 (8.3 examples/sec; 0.966 sec/batch; 76h:05m:29s remains)
INFO - root - 2017-12-07 21:06:11.918876: step 48820, loss = 21.39, batch loss = 21.31 (8.8 examples/sec; 0.913 sec/batch; 71h:55m:44s remains)
INFO - root - 2017-12-07 21:06:21.248588: step 48830, loss = 21.50, batch loss = 21.41 (8.9 examples/sec; 0.900 sec/batch; 70h:55m:13s remains)
INFO - root - 2017-12-07 21:06:30.426330: step 48840, loss = 21.36, batch loss = 21.28 (8.9 examples/sec; 0.902 sec/batch; 71h:04m:25s remains)
INFO - root - 2017-12-07 21:06:39.962658: step 48850, loss = 21.60, batch loss = 21.51 (8.6 examples/sec; 0.935 sec/batch; 73h:39m:30s remains)
INFO - root - 2017-12-07 21:06:49.346192: step 48860, loss = 21.45, batch loss = 21.37 (8.2 examples/sec; 0.980 sec/batch; 77h:14m:00s remains)
INFO - root - 2017-12-07 21:06:58.709530: step 48870, loss = 21.34, batch loss = 21.25 (8.7 examples/sec; 0.923 sec/batch; 72h:43m:44s remains)
INFO - root - 2017-12-07 21:07:08.224166: step 48880, loss = 21.44, batch loss = 21.36 (9.0 examples/sec; 0.891 sec/batch; 70h:09m:48s remains)
INFO - root - 2017-12-07 21:07:17.651870: step 48890, loss = 21.42, batch loss = 21.33 (8.7 examples/sec; 0.915 sec/batch; 72h:04m:08s remains)
INFO - root - 2017-12-07 21:07:26.913030: step 48900, loss = 21.76, batch loss = 21.68 (8.7 examples/sec; 0.919 sec/batch; 72h:24m:57s remains)
2017-12-07 21:07:27.871486: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1688938 -4.2339926 -4.3661904 -4.5025606 -4.5958157 -4.6247172 -4.5968165 -4.5555315 -4.4837337 -4.3591561 -4.2174191 -4.1379628 -4.1535921 -4.2268453 -4.3115888][-4.1914015 -4.283875 -4.4104075 -4.5100756 -4.5598531 -4.5654335 -4.5523787 -4.55336 -4.5522265 -4.517046 -4.4484253 -4.3909478 -4.3729124 -4.3852744 -4.4073858][-4.1830325 -4.2769265 -4.4064021 -4.4989634 -4.5243979 -4.4981556 -4.4652357 -4.4701152 -4.5151153 -4.5698776 -4.5981727 -4.595542 -4.5747652 -4.5496354 -4.5190477][-4.1961875 -4.2646875 -4.3776813 -4.4584894 -4.4547019 -4.3767405 -4.2883182 -4.2626667 -4.3320384 -4.46707 -4.5977192 -4.6722608 -4.6814318 -4.6530156 -4.6008511][-4.2138629 -4.2454996 -4.3201241 -4.3694119 -4.3244352 -4.1811008 -4.0193133 -3.9523048 -4.0450187 -4.260489 -4.4918246 -4.6419473 -4.6779356 -4.6458855 -4.5871773][-4.2298083 -4.2287822 -4.2543521 -4.256773 -4.1630106 -3.953213 -3.7191594 -3.6118731 -3.7259517 -4.0134816 -4.3340549 -4.5479155 -4.5983219 -4.5534825 -4.4975014][-4.2808146 -4.2625704 -4.2419739 -4.1919241 -4.046495 -3.776727 -3.4783883 -3.3262212 -3.4366181 -3.7570078 -4.1288476 -4.3877597 -4.4581795 -4.4208813 -4.3943205][-4.3561773 -4.3419461 -4.2988887 -4.2129688 -4.0321236 -3.7259884 -3.3905807 -3.2079191 -3.2962794 -3.6033232 -3.9727669 -4.2371197 -4.317492 -4.2983713 -4.3090396][-4.4372253 -4.4320617 -4.3819895 -4.2842231 -4.1013088 -3.8035171 -3.4790487 -3.2966659 -3.3645248 -3.6346633 -3.9667926 -4.2055936 -4.283133 -4.2826533 -4.3177419][-4.5046897 -4.4881611 -4.4332275 -4.3477979 -4.2009807 -3.9578094 -3.686245 -3.5298033 -3.5823278 -3.8069575 -4.0871558 -4.290072 -4.3601813 -4.367589 -4.3957143][-4.536293 -4.487277 -4.4311166 -4.3816519 -4.3015733 -4.1431203 -3.9507208 -3.8394566 -3.882828 -4.0531425 -4.2599168 -4.4054031 -4.4532943 -4.4497232 -4.4459052][-4.4991474 -4.4171152 -4.3645725 -4.3553448 -4.3444405 -4.2735076 -4.1649785 -4.1023073 -4.1353693 -4.24229 -4.3611569 -4.43937 -4.4630694 -4.4536428 -4.4317055][-4.4108763 -4.3284173 -4.2906027 -4.3120594 -4.352632 -4.3524642 -4.3139157 -4.283433 -4.2901273 -4.3225522 -4.3536716 -4.3730922 -4.3849025 -4.3863935 -4.3736582][-4.3460708 -4.29802 -4.2734485 -4.2961416 -4.3496237 -4.3880439 -4.3946838 -4.3812962 -4.3560066 -4.3189964 -4.2788548 -4.2610674 -4.2783065 -4.3075752 -4.3223677][-4.3410044 -4.3405704 -4.3199844 -4.3129311 -4.3361769 -4.371933 -4.3949304 -4.3903551 -4.3457751 -4.2645059 -4.1801329 -4.1451077 -4.1766572 -4.2404895 -4.2913675]]...]
INFO - root - 2017-12-07 21:07:37.196296: step 48910, loss = 21.78, batch loss = 21.69 (8.3 examples/sec; 0.963 sec/batch; 75h:51m:25s remains)
INFO - root - 2017-12-07 21:07:46.581998: step 48920, loss = 21.80, batch loss = 21.72 (8.9 examples/sec; 0.900 sec/batch; 70h:55m:39s remains)
INFO - root - 2017-12-07 21:07:55.930974: step 48930, loss = 22.03, batch loss = 21.95 (8.0 examples/sec; 0.998 sec/batch; 78h:34m:23s remains)
INFO - root - 2017-12-07 21:08:05.231814: step 48940, loss = 21.22, batch loss = 21.14 (8.7 examples/sec; 0.916 sec/batch; 72h:07m:22s remains)
INFO - root - 2017-12-07 21:08:14.571452: step 48950, loss = 21.11, batch loss = 21.02 (8.4 examples/sec; 0.957 sec/batch; 75h:24m:58s remains)
INFO - root - 2017-12-07 21:08:24.030306: step 48960, loss = 21.75, batch loss = 21.67 (8.8 examples/sec; 0.907 sec/batch; 71h:26m:59s remains)
INFO - root - 2017-12-07 21:08:33.456520: step 48970, loss = 21.74, batch loss = 21.65 (8.8 examples/sec; 0.913 sec/batch; 71h:55m:25s remains)
INFO - root - 2017-12-07 21:08:42.839084: step 48980, loss = 20.89, batch loss = 20.81 (8.8 examples/sec; 0.908 sec/batch; 71h:32m:05s remains)
INFO - root - 2017-12-07 21:08:52.174570: step 48990, loss = 21.97, batch loss = 21.89 (8.9 examples/sec; 0.895 sec/batch; 70h:31m:19s remains)
INFO - root - 2017-12-07 21:09:01.683868: step 49000, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.934 sec/batch; 73h:31m:19s remains)
2017-12-07 21:09:02.621981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2745242 -4.2611432 -4.2361593 -4.2140236 -4.2017975 -4.1947823 -4.1830225 -4.1722827 -4.1680546 -4.1781082 -4.217875 -4.2882547 -4.3903942 -4.4784565 -4.5049639][-4.3035793 -4.2684021 -4.2297239 -4.1994843 -4.18239 -4.1802826 -4.1830568 -4.1926236 -4.2139006 -4.2523117 -4.3142362 -4.3854842 -4.473341 -4.5339541 -4.5248423][-4.3164759 -4.2778025 -4.2420363 -4.2147789 -4.1955438 -4.1887913 -4.193162 -4.2149825 -4.262361 -4.3297348 -4.4096236 -4.4762073 -4.5398459 -4.5636849 -4.5217671][-4.331059 -4.2965417 -4.2696166 -4.2505331 -4.2311106 -4.2112575 -4.1969104 -4.2092304 -4.2658663 -4.3513436 -4.445169 -4.5174594 -4.5698972 -4.5751009 -4.5234919][-4.32921 -4.3009133 -4.2809844 -4.2675552 -4.2416959 -4.2007661 -4.1600032 -4.1637688 -4.236937 -4.3460011 -4.4528942 -4.5329247 -4.5744891 -4.5608082 -4.5009923][-4.3159847 -4.2725673 -4.2389226 -4.222466 -4.1933632 -4.1392221 -4.0839911 -4.0932627 -4.1957536 -4.332603 -4.4493971 -4.5292335 -4.5486488 -4.5036049 -4.4230909][-4.2964025 -4.2232389 -4.1591444 -4.1198311 -4.0746288 -4.0013142 -3.9295993 -3.9429975 -4.0778542 -4.2513289 -4.3833981 -4.4618564 -4.4635892 -4.398675 -4.3033314][-4.2746811 -4.1870546 -4.0940261 -4.0225096 -3.954133 -3.8664508 -3.7842398 -3.7984998 -3.9570379 -4.1554909 -4.2936683 -4.3649206 -4.359025 -4.2903223 -4.1815052][-4.2596025 -4.1689062 -4.0596457 -3.9726369 -3.913779 -3.854506 -3.7992823 -3.8232532 -3.9716539 -4.143187 -4.2472644 -4.2916479 -4.274684 -4.2010522 -4.0777445][-4.2468657 -4.1506863 -4.0339203 -3.9439969 -3.9100008 -3.9015703 -3.9003017 -3.95444 -4.0790324 -4.1954069 -4.2484879 -4.2621074 -4.2387643 -4.1674562 -4.042881][-4.27892 -4.1863184 -4.0724568 -3.9859688 -3.9738591 -4.00372 -4.0435014 -4.1242857 -4.226337 -4.29445 -4.3185115 -4.31775 -4.2958813 -4.2345362 -4.1130328][-4.3879914 -4.305419 -4.2123814 -4.1470342 -4.1552172 -4.2020197 -4.2539611 -4.3337235 -4.4042587 -4.4328146 -4.4386787 -4.4263477 -4.4024115 -4.3512135 -4.2396245][-4.5089712 -4.44229 -4.3738465 -4.3272886 -4.33839 -4.3775158 -4.4252043 -4.4992847 -4.5527287 -4.5640192 -4.5537004 -4.5209346 -4.4891171 -4.45725 -4.3847117][-4.5942154 -4.5528831 -4.513804 -4.4857264 -4.4920182 -4.5164146 -4.5556607 -4.6183777 -4.6617246 -4.6716232 -4.6580863 -4.6193304 -4.5863171 -4.5694757 -4.5321231][-4.6091738 -4.5933142 -4.5836277 -4.5793014 -4.5879345 -4.6028376 -4.6306438 -4.6720843 -4.6968408 -4.7018604 -4.6891184 -4.6564107 -4.628489 -4.6179218 -4.59713]]...]
INFO - root - 2017-12-07 21:09:12.037016: step 49010, loss = 21.71, batch loss = 21.63 (8.2 examples/sec; 0.970 sec/batch; 76h:24m:32s remains)
INFO - root - 2017-12-07 21:09:21.339043: step 49020, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.954 sec/batch; 75h:07m:42s remains)
INFO - root - 2017-12-07 21:09:30.854820: step 49030, loss = 20.92, batch loss = 20.84 (8.0 examples/sec; 1.001 sec/batch; 78h:48m:14s remains)
INFO - root - 2017-12-07 21:09:40.276360: step 49040, loss = 21.91, batch loss = 21.83 (8.7 examples/sec; 0.916 sec/batch; 72h:07m:54s remains)
INFO - root - 2017-12-07 21:09:49.723904: step 49050, loss = 21.32, batch loss = 21.24 (8.2 examples/sec; 0.972 sec/batch; 76h:32m:17s remains)
INFO - root - 2017-12-07 21:09:59.122476: step 49060, loss = 21.57, batch loss = 21.48 (8.2 examples/sec; 0.975 sec/batch; 76h:46m:36s remains)
INFO - root - 2017-12-07 21:10:08.514140: step 49070, loss = 21.24, batch loss = 21.15 (8.6 examples/sec; 0.929 sec/batch; 73h:08m:39s remains)
INFO - root - 2017-12-07 21:10:17.819081: step 49080, loss = 20.93, batch loss = 20.85 (8.4 examples/sec; 0.956 sec/batch; 75h:14m:13s remains)
INFO - root - 2017-12-07 21:10:27.253757: step 49090, loss = 21.34, batch loss = 21.25 (8.4 examples/sec; 0.948 sec/batch; 74h:38m:40s remains)
INFO - root - 2017-12-07 21:10:36.721144: step 49100, loss = 21.24, batch loss = 21.15 (8.6 examples/sec; 0.932 sec/batch; 73h:24m:10s remains)
2017-12-07 21:10:37.699881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4648724 -4.5754304 -4.6376052 -4.6562233 -4.6520886 -4.6500206 -4.6081605 -4.5361834 -4.5091238 -4.5088334 -4.5202742 -4.5369043 -4.5464959 -4.5517154 -4.5471125][-4.4489427 -4.5472236 -4.5884037 -4.5806713 -4.5502658 -4.532732 -4.4943395 -4.4461465 -4.4576936 -4.5015979 -4.5398192 -4.5649705 -4.5704675 -4.5708461 -4.5552874][-4.4477763 -4.5185461 -4.5162158 -4.4598012 -4.3831882 -4.3366842 -4.3048491 -4.294848 -4.3622351 -4.4624095 -4.5338192 -4.5708256 -4.5831594 -4.5887351 -4.5694094][-4.4677706 -4.5145354 -4.4727211 -4.3694377 -4.2503996 -4.1786633 -4.1505365 -4.1659636 -4.2634997 -4.3814669 -4.4478679 -4.4694672 -4.4822888 -4.5084267 -4.5125179][-4.4859562 -4.5160713 -4.4455113 -4.30651 -4.1622066 -4.0785861 -4.0470219 -4.0614529 -4.1539435 -4.2526765 -4.284204 -4.2710395 -4.2769709 -4.3351426 -4.3856583][-4.49254 -4.5078545 -4.410696 -4.2421818 -4.0869188 -4.0043292 -3.9703114 -3.9746864 -4.0489135 -4.1216969 -4.123867 -4.0790043 -4.0682917 -4.1479249 -4.2473607][-4.4966626 -4.5016923 -4.383347 -4.1885805 -4.01695 -3.9294858 -3.8938789 -3.8927627 -3.9511065 -4.0098763 -4.0060644 -3.9535983 -3.9343884 -4.0217581 -4.1552505][-4.4918556 -4.4993157 -4.3693533 -4.14653 -3.9371984 -3.8176415 -3.772584 -3.7717762 -3.83106 -3.9110644 -3.9501452 -3.9427433 -3.9504776 -4.0344944 -4.1641464][-4.4990878 -4.5206461 -4.3950405 -4.1576252 -3.9118814 -3.7485909 -3.6817067 -3.6819685 -3.7590346 -3.8862309 -3.9916468 -4.0554605 -4.1031461 -4.1695437 -4.2595897][-4.5371866 -4.5830212 -4.4877 -4.2771816 -4.0330143 -3.8394308 -3.7401636 -3.7237544 -3.8074708 -3.9626369 -4.110404 -4.2297912 -4.313952 -4.3669486 -4.4132109][-4.5835848 -4.6484709 -4.6001296 -4.4600372 -4.267406 -4.0818214 -3.9640684 -3.9296284 -4.0060954 -4.1545796 -4.3006763 -4.4284616 -4.5157895 -4.5531645 -4.5657144][-4.6101203 -4.6844692 -4.6797514 -4.617382 -4.5016847 -4.3636909 -4.2652345 -4.2350698 -4.2987266 -4.4134846 -4.516109 -4.5948009 -4.6358442 -4.6384468 -4.6259017][-4.595006 -4.6634636 -4.6791577 -4.6666236 -4.6191483 -4.5479541 -4.4962988 -4.4838004 -4.5349979 -4.6177359 -4.6730509 -4.68753 -4.668345 -4.6341057 -4.6103024][-4.5578623 -4.6125836 -4.61776 -4.6074309 -4.5794244 -4.5438075 -4.5283546 -4.5369053 -4.5939293 -4.6731806 -4.7068229 -4.6821017 -4.6276331 -4.5734239 -4.5468907][-4.5471058 -4.6012111 -4.5911226 -4.5489831 -4.4784803 -4.4164805 -4.4006367 -4.422677 -4.5019097 -4.601522 -4.6343551 -4.594296 -4.529995 -4.4734955 -4.4490142]]...]
INFO - root - 2017-12-07 21:10:46.841357: step 49110, loss = 21.27, batch loss = 21.19 (8.9 examples/sec; 0.904 sec/batch; 71h:09m:29s remains)
INFO - root - 2017-12-07 21:10:56.166949: step 49120, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.956 sec/batch; 75h:15m:35s remains)
INFO - root - 2017-12-07 21:11:05.764307: step 49130, loss = 21.84, batch loss = 21.76 (8.2 examples/sec; 0.974 sec/batch; 76h:40m:44s remains)
INFO - root - 2017-12-07 21:11:15.105666: step 49140, loss = 21.42, batch loss = 21.34 (8.1 examples/sec; 0.985 sec/batch; 77h:30m:12s remains)
INFO - root - 2017-12-07 21:11:24.594850: step 49150, loss = 21.47, batch loss = 21.39 (7.7 examples/sec; 1.045 sec/batch; 82h:15m:04s remains)
INFO - root - 2017-12-07 21:11:33.921721: step 49160, loss = 21.18, batch loss = 21.10 (8.1 examples/sec; 0.990 sec/batch; 77h:53m:58s remains)
INFO - root - 2017-12-07 21:11:43.273363: step 49170, loss = 21.42, batch loss = 21.34 (8.1 examples/sec; 0.992 sec/batch; 78h:06m:13s remains)
INFO - root - 2017-12-07 21:11:52.587954: step 49180, loss = 21.42, batch loss = 21.34 (8.4 examples/sec; 0.950 sec/batch; 74h:46m:31s remains)
INFO - root - 2017-12-07 21:12:01.831709: step 49190, loss = 21.73, batch loss = 21.65 (8.6 examples/sec; 0.927 sec/batch; 72h:58m:09s remains)
INFO - root - 2017-12-07 21:12:11.200037: step 49200, loss = 21.84, batch loss = 21.76 (8.4 examples/sec; 0.951 sec/batch; 74h:49m:01s remains)
2017-12-07 21:12:12.177513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3530927 -4.4048405 -4.4512153 -4.4755354 -4.4756956 -4.4632893 -4.4449744 -4.4271832 -4.4180613 -4.4158058 -4.410677 -4.3935318 -4.3675985 -4.3412704 -4.3158741][-4.3861475 -4.4547567 -4.51224 -4.5326405 -4.5239196 -4.5032029 -4.4790745 -4.4615026 -4.4610877 -4.4749403 -4.4835882 -4.4677844 -4.4300714 -4.3850141 -4.3428216][-4.398715 -4.4782724 -4.5390258 -4.5478735 -4.5222449 -4.4872055 -4.4519043 -4.429265 -4.4378004 -4.4742761 -4.5073252 -4.5028343 -4.4607615 -4.40334 -4.3499246][-4.4065351 -4.4894247 -4.5442171 -4.5322762 -4.4792061 -4.4185967 -4.3628383 -4.3293667 -4.3477035 -4.4136243 -4.4804678 -4.4981055 -4.4589858 -4.3954363 -4.3431773][-4.4171939 -4.49866 -4.5399451 -4.4992833 -4.4082665 -4.3103352 -4.2289352 -4.1917243 -4.2368617 -4.3500037 -4.4616594 -4.5052276 -4.4640822 -4.385376 -4.33243][-4.4194818 -4.4980054 -4.5219569 -4.4467649 -4.30727 -4.16038 -4.047749 -4.0107327 -4.0943961 -4.2673759 -4.4335966 -4.5139952 -4.4801264 -4.3859425 -4.3256903][-4.4121976 -4.4914374 -4.5022254 -4.393342 -4.2022457 -4.0029335 -3.8535223 -3.8041332 -3.9076951 -4.1235085 -4.3380823 -4.4701886 -4.47717 -4.3971786 -4.3361588][-4.40352 -4.4869061 -4.4923391 -4.3599458 -4.1265044 -3.8841538 -3.7011218 -3.6296732 -3.730216 -3.9585958 -4.1997628 -4.3796239 -4.4493341 -4.4174223 -4.36942][-4.4018097 -4.4915261 -4.504334 -4.3732014 -4.1291256 -3.8772817 -3.6917577 -3.6173162 -3.7048936 -3.911016 -4.1374445 -4.3245225 -4.433702 -4.4475522 -4.4131794][-4.4144163 -4.5098853 -4.53602 -4.4309726 -4.2191768 -4.0042877 -3.8572967 -3.8052356 -3.8783453 -4.0395541 -4.2183242 -4.3704877 -4.4675345 -4.4844723 -4.4433818][-4.4471455 -4.5479937 -4.587132 -4.5148797 -4.3520379 -4.1885247 -4.0916982 -4.0713091 -4.1379547 -4.2621174 -4.393733 -4.4924407 -4.5337372 -4.5072265 -4.439209][-4.4888334 -4.590693 -4.6371045 -4.5923615 -4.4705014 -4.3444767 -4.2842526 -4.2964597 -4.3692513 -4.4718814 -4.563405 -4.6049252 -4.5784249 -4.4972048 -4.4044065][-4.5190291 -4.6094232 -4.6536088 -4.6318264 -4.544744 -4.4404607 -4.3945403 -4.42695 -4.5127878 -4.6034236 -4.6591849 -4.6487927 -4.5719252 -4.4597282 -4.361094][-4.5340614 -4.5963874 -4.6245036 -4.6165485 -4.5601139 -4.4757519 -4.4339781 -4.4729776 -4.5659018 -4.6452727 -4.6697087 -4.6237173 -4.5274835 -4.4178309 -4.3314261][-4.5413575 -4.5640521 -4.5551577 -4.5332966 -4.488019 -4.4247656 -4.40058 -4.4535313 -4.5502214 -4.61989 -4.6236796 -4.560257 -4.4686084 -4.3817525 -4.31631]]...]
INFO - root - 2017-12-07 21:12:21.563157: step 49210, loss = 21.60, batch loss = 21.51 (7.7 examples/sec; 1.043 sec/batch; 82h:03m:15s remains)
INFO - root - 2017-12-07 21:12:31.027187: step 49220, loss = 21.67, batch loss = 21.59 (8.6 examples/sec; 0.931 sec/batch; 73h:15m:03s remains)
INFO - root - 2017-12-07 21:12:40.435198: step 49230, loss = 21.10, batch loss = 21.02 (8.6 examples/sec; 0.930 sec/batch; 73h:12m:39s remains)
INFO - root - 2017-12-07 21:12:49.997144: step 49240, loss = 21.26, batch loss = 21.17 (8.7 examples/sec; 0.922 sec/batch; 72h:31m:21s remains)
INFO - root - 2017-12-07 21:12:59.340950: step 49250, loss = 21.59, batch loss = 21.51 (8.6 examples/sec; 0.930 sec/batch; 73h:12m:19s remains)
INFO - root - 2017-12-07 21:13:08.779961: step 49260, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.934 sec/batch; 73h:27m:08s remains)
INFO - root - 2017-12-07 21:13:18.311711: step 49270, loss = 22.07, batch loss = 21.99 (8.9 examples/sec; 0.899 sec/batch; 70h:42m:06s remains)
INFO - root - 2017-12-07 21:13:27.657986: step 49280, loss = 21.39, batch loss = 21.31 (9.1 examples/sec; 0.878 sec/batch; 69h:03m:17s remains)
INFO - root - 2017-12-07 21:13:37.119367: step 49290, loss = 21.59, batch loss = 21.50 (8.3 examples/sec; 0.959 sec/batch; 75h:27m:00s remains)
INFO - root - 2017-12-07 21:13:46.652299: step 49300, loss = 21.37, batch loss = 21.29 (8.1 examples/sec; 0.991 sec/batch; 77h:55m:28s remains)
2017-12-07 21:13:47.616181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2380848 -4.2356076 -4.2304378 -4.2806592 -4.4104934 -4.5214705 -4.5492945 -4.5743728 -4.6493931 -4.7254348 -4.7895737 -4.8364058 -4.8166656 -4.6537595 -4.3195572][-4.2462697 -4.2731328 -4.2995114 -4.3651419 -4.4721894 -4.5308232 -4.5137086 -4.5155807 -4.58126 -4.68016 -4.8015718 -4.8921771 -4.8615632 -4.6455474 -4.25811][-4.25997 -4.3028135 -4.3522062 -4.4273129 -4.5038757 -4.5013933 -4.4291945 -4.3932538 -4.4302621 -4.5385051 -4.7169437 -4.8722496 -4.8776517 -4.67444 -4.3063612][-4.2889471 -4.3316917 -4.3955903 -4.4814177 -4.5335379 -4.4760151 -4.3542156 -4.2861366 -4.292685 -4.3945441 -4.6076 -4.81428 -4.8586607 -4.6938334 -4.3953128][-4.3224754 -4.3595762 -4.4313297 -4.5171423 -4.53585 -4.4197259 -4.2500925 -4.1643033 -4.1669059 -4.2725806 -4.4980459 -4.7215996 -4.7862568 -4.6676459 -4.4649396][-4.3654857 -4.4008303 -4.4650507 -4.5172 -4.4696383 -4.2818785 -4.0630579 -3.9644258 -3.9895358 -4.1225891 -4.3518519 -4.5716186 -4.6582513 -4.6087642 -4.5145049][-4.3916011 -4.4392142 -4.4977236 -4.5104327 -4.400785 -4.1589503 -3.9001846 -3.7728343 -3.804163 -3.9565377 -4.1808271 -4.39402 -4.5174623 -4.5586934 -4.5678735][-4.3945656 -4.4655142 -4.5348125 -4.5393205 -4.4145041 -4.1611586 -3.883153 -3.7222116 -3.7339754 -3.8774471 -4.07872 -4.2736435 -4.4240513 -4.5316811 -4.6008329][-4.3876786 -4.4712777 -4.5583739 -4.5912519 -4.5082288 -4.2997494 -4.046577 -3.8790655 -3.8652248 -3.9711084 -4.1168957 -4.259686 -4.3921804 -4.510232 -4.5847645][-4.3541617 -4.4245687 -4.516716 -4.5876222 -4.5762186 -4.4557753 -4.276897 -4.1439724 -4.1201425 -4.1809406 -4.2553616 -4.3269792 -4.4125171 -4.5019574 -4.5469432][-4.2968278 -4.33791 -4.4144964 -4.5016642 -4.5399723 -4.4973612 -4.4005837 -4.3230276 -4.3127608 -4.3481627 -4.3749824 -4.4032431 -4.4613643 -4.5303011 -4.5487309][-4.2531967 -4.2741108 -4.3318353 -4.41052 -4.4559155 -4.4470983 -4.4015265 -4.3699703 -4.38273 -4.4171472 -4.4386396 -4.4707632 -4.5332913 -4.5939198 -4.588912][-4.2400265 -4.2560692 -4.3065739 -4.3723192 -4.4034662 -4.394248 -4.3658857 -4.3573618 -4.3883629 -4.437253 -4.4843683 -4.5460591 -4.618484 -4.6615934 -4.6236892][-4.2321424 -4.2446547 -4.2950225 -4.3566294 -4.3834581 -4.3762383 -4.3553419 -4.3525257 -4.3872094 -4.4472165 -4.5222368 -4.6097879 -4.6839418 -4.7044063 -4.6382804][-4.2198291 -4.2198391 -4.26257 -4.3234744 -4.3665156 -4.3864245 -4.3855238 -4.3832831 -4.4077988 -4.4641871 -4.5449805 -4.6296682 -4.683321 -4.6790328 -4.6046162]]...]
INFO - root - 2017-12-07 21:13:56.914464: step 49310, loss = 21.61, batch loss = 21.53 (8.7 examples/sec; 0.918 sec/batch; 72h:12m:59s remains)
INFO - root - 2017-12-07 21:14:06.419166: step 49320, loss = 21.26, batch loss = 21.18 (8.4 examples/sec; 0.954 sec/batch; 75h:00m:13s remains)
INFO - root - 2017-12-07 21:14:15.807936: step 49330, loss = 21.45, batch loss = 21.36 (8.1 examples/sec; 0.989 sec/batch; 77h:49m:00s remains)
INFO - root - 2017-12-07 21:14:25.240678: step 49340, loss = 21.25, batch loss = 21.16 (8.3 examples/sec; 0.959 sec/batch; 75h:27m:10s remains)
INFO - root - 2017-12-07 21:14:34.673755: step 49350, loss = 21.14, batch loss = 21.06 (8.2 examples/sec; 0.977 sec/batch; 76h:49m:52s remains)
INFO - root - 2017-12-07 21:14:43.850964: step 49360, loss = 21.69, batch loss = 21.61 (7.9 examples/sec; 1.011 sec/batch; 79h:32m:03s remains)
INFO - root - 2017-12-07 21:14:53.331608: step 49370, loss = 21.59, batch loss = 21.50 (8.2 examples/sec; 0.978 sec/batch; 76h:55m:39s remains)
INFO - root - 2017-12-07 21:15:02.726027: step 49380, loss = 21.51, batch loss = 21.43 (8.0 examples/sec; 0.996 sec/batch; 78h:21m:16s remains)
INFO - root - 2017-12-07 21:15:12.000115: step 49390, loss = 20.93, batch loss = 20.85 (9.0 examples/sec; 0.890 sec/batch; 70h:00m:02s remains)
INFO - root - 2017-12-07 21:15:21.416079: step 49400, loss = 21.63, batch loss = 21.55 (8.5 examples/sec; 0.941 sec/batch; 74h:01m:45s remains)
2017-12-07 21:15:22.357183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4161277 -4.4478693 -4.475698 -4.4951763 -4.5070271 -4.5094695 -4.5034289 -4.4946284 -4.4894223 -4.4923749 -4.5027466 -4.511642 -4.5100174 -4.4927244 -4.459312][-4.4961171 -4.562243 -4.611547 -4.6410108 -4.6539068 -4.6449904 -4.622541 -4.6109242 -4.6218138 -4.6466317 -4.6724205 -4.6875849 -4.6799192 -4.643918 -4.5813346][-4.5802617 -4.6837125 -4.7500196 -4.778583 -4.775547 -4.7259645 -4.6541195 -4.6249042 -4.6656141 -4.7350092 -4.7955327 -4.8319325 -4.830555 -4.7865648 -4.7031946][-4.6433458 -4.7719417 -4.8398795 -4.8510733 -4.8110266 -4.6924114 -4.5380383 -4.4698248 -4.5498257 -4.686017 -4.7967634 -4.86263 -4.8790445 -4.8466458 -4.7678685][-4.6638241 -4.7869263 -4.8315721 -4.8085885 -4.721632 -4.5306439 -4.2908959 -4.1774783 -4.2992473 -4.5106163 -4.6732087 -4.7651649 -4.80026 -4.7953672 -4.7481127][-4.6386089 -4.7284374 -4.7288451 -4.6631174 -4.5332742 -4.2904305 -3.9870267 -3.8260546 -3.969166 -4.2397971 -4.4507179 -4.5729642 -4.6302676 -4.6605263 -4.6588721][-4.6058459 -4.6504526 -4.6030064 -4.4987135 -4.33375 -4.05944 -3.7200489 -3.5174453 -3.6571879 -3.9671166 -4.2300434 -4.3992128 -4.484283 -4.5373082 -4.5597105][-4.60406 -4.623735 -4.5535445 -4.4252505 -4.2338181 -3.9368253 -3.5843718 -3.3570352 -3.4768291 -3.7982023 -4.1056538 -4.3325667 -4.4554114 -4.519681 -4.5346785][-4.653553 -4.6959715 -4.6476026 -4.524312 -4.3241792 -4.0195551 -3.6772404 -3.4493489 -3.5398252 -3.8393586 -4.1576948 -4.417933 -4.5642958 -4.6213107 -4.6025043][-4.7129316 -4.8026915 -4.8032665 -4.7116795 -4.5326018 -4.2578878 -3.9653654 -3.7713408 -3.841608 -4.0962858 -4.3803053 -4.6182 -4.7452054 -4.768611 -4.7032557][-4.7299638 -4.8576841 -4.9040823 -4.8562117 -4.7295175 -4.5299153 -4.323154 -4.1815205 -4.22709 -4.4135356 -4.6259637 -4.8002834 -4.8808894 -4.8648167 -4.7644072][-4.6908727 -4.829864 -4.9056644 -4.902421 -4.8408666 -4.7327108 -4.6177197 -4.5277505 -4.54245 -4.6483397 -4.7744327 -4.8717861 -4.8986535 -4.8503094 -4.7362385][-4.5969086 -4.7133965 -4.7905359 -4.8113675 -4.7952042 -4.7559853 -4.709671 -4.6649337 -4.6642957 -4.7109871 -4.7665167 -4.7988348 -4.7829204 -4.7203822 -4.6195688][-4.4905419 -4.5624547 -4.6144357 -4.6320572 -4.62921 -4.6199489 -4.6102552 -4.5968604 -4.5927792 -4.6056838 -4.6208472 -4.6207581 -4.594161 -4.5443826 -4.47719][-4.4071097 -4.4430346 -4.4692721 -4.47826 -4.4779806 -4.475966 -4.47423 -4.4684234 -4.462172 -4.460803 -4.4603958 -4.4528685 -4.4336576 -4.4044604 -4.3686976]]...]
INFO - root - 2017-12-07 21:15:31.730774: step 49410, loss = 21.57, batch loss = 21.49 (8.7 examples/sec; 0.916 sec/batch; 72h:01m:15s remains)
INFO - root - 2017-12-07 21:15:41.058996: step 49420, loss = 21.80, batch loss = 21.72 (8.7 examples/sec; 0.920 sec/batch; 72h:22m:00s remains)
INFO - root - 2017-12-07 21:15:50.589274: step 49430, loss = 21.53, batch loss = 21.45 (8.3 examples/sec; 0.962 sec/batch; 75h:36m:36s remains)
INFO - root - 2017-12-07 21:15:59.979835: step 49440, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.928 sec/batch; 72h:58m:09s remains)
INFO - root - 2017-12-07 21:16:09.382133: step 49450, loss = 21.74, batch loss = 21.66 (8.5 examples/sec; 0.941 sec/batch; 74h:00m:51s remains)
INFO - root - 2017-12-07 21:16:18.856708: step 49460, loss = 21.52, batch loss = 21.44 (8.9 examples/sec; 0.896 sec/batch; 70h:24m:33s remains)
INFO - root - 2017-12-07 21:16:28.303759: step 49470, loss = 21.36, batch loss = 21.27 (8.6 examples/sec; 0.930 sec/batch; 73h:07m:55s remains)
INFO - root - 2017-12-07 21:16:37.866035: step 49480, loss = 21.30, batch loss = 21.22 (8.3 examples/sec; 0.961 sec/batch; 75h:31m:37s remains)
INFO - root - 2017-12-07 21:16:47.265740: step 49490, loss = 21.34, batch loss = 21.25 (8.1 examples/sec; 0.992 sec/batch; 77h:57m:44s remains)
INFO - root - 2017-12-07 21:16:56.649395: step 49500, loss = 21.27, batch loss = 21.19 (8.2 examples/sec; 0.976 sec/batch; 76h:44m:56s remains)
2017-12-07 21:16:57.565474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3566175 -4.3958564 -4.4474959 -4.4900427 -4.507905 -4.5083013 -4.5051579 -4.480134 -4.42054 -4.3562264 -4.3180556 -4.315629 -4.3221817 -4.3076124 -4.2728577][-4.3983727 -4.439044 -4.4918365 -4.5423183 -4.5688462 -4.5715094 -4.5600281 -4.52356 -4.464416 -4.4106503 -4.3950148 -4.4151678 -4.4297342 -4.40213 -4.3395996][-4.4507446 -4.4919953 -4.5405622 -4.5907936 -4.618609 -4.6161418 -4.5881953 -4.5376425 -4.4871554 -4.4631734 -4.4905481 -4.5407219 -4.5521808 -4.4946966 -4.3997955][-4.525907 -4.5586362 -4.590065 -4.6260095 -4.6414089 -4.6156797 -4.553164 -4.4817739 -4.4501109 -4.4767065 -4.5658221 -4.6559854 -4.66948 -4.5910339 -4.4776139][-4.5882378 -4.6055307 -4.616375 -4.6313667 -4.6263862 -4.5667434 -4.4571047 -4.3532887 -4.329031 -4.4005384 -4.5446458 -4.6756363 -4.708034 -4.6402493 -4.5422034][-4.5849257 -4.5802431 -4.5675635 -4.555481 -4.5261941 -4.4304848 -4.2691813 -4.1235809 -4.0979514 -4.2083821 -4.4057078 -4.5856137 -4.6548057 -4.6310763 -4.581162][-4.50524 -4.4751434 -4.4371209 -4.3908319 -4.3272 -4.18822 -3.9723713 -3.7880325 -3.7723508 -3.9324391 -4.1941981 -4.4373689 -4.5605364 -4.5976439 -4.6066194][-4.36771 -4.3315597 -4.2898517 -4.2272649 -4.1423559 -3.974333 -3.7213717 -3.5124941 -3.5103366 -3.709538 -4.0206318 -4.315042 -4.482923 -4.5684052 -4.6224241][-4.2485585 -4.2355304 -4.2124796 -4.1556129 -4.0763226 -3.9133394 -3.6637192 -3.4670799 -3.4877744 -3.7120705 -4.04469 -4.3499742 -4.520709 -4.6035218 -4.6501055][-4.1872282 -4.2102089 -4.216722 -4.1810446 -4.1278849 -4.0027242 -3.8006454 -3.6534784 -3.6974044 -3.9178367 -4.2277069 -4.4930344 -4.6203284 -4.6545768 -4.6520858][-4.1978779 -4.2609406 -4.3060589 -4.3111238 -4.3039074 -4.2397089 -4.1090064 -4.0180626 -4.0619283 -4.23124 -4.4580622 -4.6280527 -4.676847 -4.6456347 -4.5883489][-4.2640257 -4.3500061 -4.42502 -4.4706039 -4.504775 -4.4928923 -4.4266458 -4.38144 -4.4103708 -4.5132418 -4.638165 -4.6975541 -4.66372 -4.5747952 -4.4778304][-4.3270669 -4.3950067 -4.4684138 -4.5328755 -4.5904813 -4.60943 -4.5840635 -4.5606794 -4.5650434 -4.6054778 -4.6466961 -4.6256142 -4.5447206 -4.4293828 -4.3216772][-4.4000926 -4.4334574 -4.4790287 -4.5353613 -4.5915442 -4.6160355 -4.6022663 -4.5722523 -4.5403066 -4.5258946 -4.5136142 -4.4567661 -4.3719773 -4.2681489 -4.1780062][-4.47518 -4.4932876 -4.5140266 -4.544764 -4.5765228 -4.5857511 -4.5647192 -4.5178976 -4.4560781 -4.4070678 -4.3733778 -4.3234577 -4.2702179 -4.2024026 -4.1405182]]...]
INFO - root - 2017-12-07 21:17:06.824600: step 49510, loss = 21.31, batch loss = 21.22 (8.8 examples/sec; 0.906 sec/batch; 71h:11m:12s remains)
INFO - root - 2017-12-07 21:17:16.261246: step 49520, loss = 21.43, batch loss = 21.34 (8.5 examples/sec; 0.946 sec/batch; 74h:23m:31s remains)
INFO - root - 2017-12-07 21:17:25.739395: step 49530, loss = 21.81, batch loss = 21.73 (10.1 examples/sec; 0.793 sec/batch; 62h:21m:32s remains)
INFO - root - 2017-12-07 21:17:35.149584: step 49540, loss = 21.34, batch loss = 21.26 (8.6 examples/sec; 0.930 sec/batch; 73h:03m:32s remains)
INFO - root - 2017-12-07 21:17:44.544664: step 49550, loss = 21.38, batch loss = 21.30 (8.5 examples/sec; 0.936 sec/batch; 73h:32m:52s remains)
INFO - root - 2017-12-07 21:17:53.976369: step 49560, loss = 21.82, batch loss = 21.74 (8.1 examples/sec; 0.985 sec/batch; 77h:26m:17s remains)
INFO - root - 2017-12-07 21:18:03.368232: step 49570, loss = 21.25, batch loss = 21.16 (8.3 examples/sec; 0.967 sec/batch; 76h:01m:26s remains)
INFO - root - 2017-12-07 21:18:12.824291: step 49580, loss = 21.19, batch loss = 21.10 (8.8 examples/sec; 0.908 sec/batch; 71h:22m:37s remains)
INFO - root - 2017-12-07 21:18:22.294499: step 49590, loss = 21.59, batch loss = 21.51 (8.8 examples/sec; 0.914 sec/batch; 71h:50m:58s remains)
INFO - root - 2017-12-07 21:18:31.787975: step 49600, loss = 21.67, batch loss = 21.59 (8.2 examples/sec; 0.982 sec/batch; 77h:07m:47s remains)
2017-12-07 21:18:32.799804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.402442 -4.3875141 -4.3674026 -4.3489127 -4.3497939 -4.3687334 -4.3799934 -4.3518567 -4.2859664 -4.2228742 -4.2074246 -4.2608604 -4.3385305 -4.4096012 -4.4681306][-4.3351221 -4.3063521 -4.2865181 -4.2828279 -4.3096576 -4.3539515 -4.3802781 -4.3519678 -4.2693806 -4.1826792 -4.1485991 -4.2051563 -4.3030553 -4.3999119 -4.4790697][-4.2824645 -4.2421446 -4.226347 -4.2419505 -4.2930436 -4.3533516 -4.3898683 -4.3697786 -4.2896957 -4.1972003 -4.1452894 -4.1810675 -4.2698126 -4.3705521 -4.462687][-4.2299066 -4.1758213 -4.1632524 -4.1942453 -4.2575665 -4.3168507 -4.3534946 -4.3505063 -4.3033104 -4.2435336 -4.1996741 -4.2124286 -4.2705626 -4.3478479 -4.4297485][-4.1672049 -4.1030655 -4.0946255 -4.1385741 -4.2011542 -4.2408743 -4.2644734 -4.2798748 -4.2826147 -4.2839231 -4.275867 -4.2770662 -4.2994747 -4.3363214 -4.3895125][-4.1257048 -4.0560284 -4.0464168 -4.0848203 -4.1229234 -4.1218739 -4.11975 -4.1495233 -4.2011728 -4.2686577 -4.3112736 -4.3182163 -4.3151803 -4.315815 -4.337709][-4.1274886 -4.0529704 -4.02926 -4.0392175 -4.0312939 -3.9746242 -3.937012 -3.9705098 -4.0515442 -4.167789 -4.2610674 -4.2891393 -4.2805576 -4.2633181 -4.2668166][-4.2055058 -4.1231003 -4.0769973 -4.0509863 -4.001956 -3.9054155 -3.8420689 -3.8707914 -3.9519386 -4.0764427 -4.1894555 -4.2285671 -4.223022 -4.2044649 -4.2051291][-4.3075294 -4.2118487 -4.1428385 -4.0920358 -4.0284309 -3.9288542 -3.870018 -3.9010351 -3.9709413 -4.0756431 -4.1750836 -4.2058649 -4.2005935 -4.186388 -4.1847553][-4.3828082 -4.2858753 -4.2060351 -4.14589 -4.0856876 -4.00388 -3.9627895 -3.9978733 -4.0581961 -4.1372519 -4.2103057 -4.2273536 -4.2229872 -4.2159147 -4.211442][-4.4464216 -4.3711286 -4.3025165 -4.2511148 -4.2085571 -4.1549349 -4.1302462 -4.1636686 -4.2109118 -4.2563243 -4.2886677 -4.2831087 -4.2731552 -4.2687321 -4.2613492][-4.5073195 -4.4600458 -4.4102092 -4.3745317 -4.3546786 -4.3306026 -4.3179259 -4.3437996 -4.3746824 -4.3867621 -4.3771625 -4.3473105 -4.3240523 -4.3132381 -4.3013825][-4.5461507 -4.5210166 -4.4887848 -4.4660716 -4.4638376 -4.4613562 -4.4542027 -4.4679375 -4.4826307 -4.4712386 -4.4353232 -4.3911123 -4.35692 -4.3387485 -4.324831][-4.5393839 -4.5307212 -4.5171 -4.5092988 -4.5220628 -4.5372314 -4.5370908 -4.5408993 -4.54155 -4.5157113 -4.4689531 -4.4222646 -4.3856797 -4.3677998 -4.3602052][-4.4937735 -4.4899397 -4.4873981 -4.489131 -4.5085664 -4.5314112 -4.5378132 -4.538229 -4.534153 -4.5094466 -4.4706531 -4.435626 -4.4085112 -4.3983665 -4.400476]]...]
INFO - root - 2017-12-07 21:18:42.034022: step 49610, loss = 21.64, batch loss = 21.55 (8.5 examples/sec; 0.944 sec/batch; 74h:09m:35s remains)
INFO - root - 2017-12-07 21:18:51.375189: step 49620, loss = 21.40, batch loss = 21.31 (8.5 examples/sec; 0.945 sec/batch; 74h:16m:06s remains)
INFO - root - 2017-12-07 21:19:00.817714: step 49630, loss = 21.39, batch loss = 21.31 (9.1 examples/sec; 0.881 sec/batch; 69h:12m:34s remains)
INFO - root - 2017-12-07 21:19:10.150325: step 49640, loss = 21.80, batch loss = 21.72 (9.1 examples/sec; 0.877 sec/batch; 68h:54m:00s remains)
INFO - root - 2017-12-07 21:19:19.567676: step 49650, loss = 21.83, batch loss = 21.75 (8.4 examples/sec; 0.953 sec/batch; 74h:50m:16s remains)
INFO - root - 2017-12-07 21:19:28.941365: step 49660, loss = 21.36, batch loss = 21.28 (9.0 examples/sec; 0.888 sec/batch; 69h:45m:27s remains)
INFO - root - 2017-12-07 21:19:38.255554: step 49670, loss = 21.24, batch loss = 21.16 (8.4 examples/sec; 0.949 sec/batch; 74h:31m:14s remains)
INFO - root - 2017-12-07 21:19:47.776169: step 49680, loss = 21.26, batch loss = 21.18 (8.4 examples/sec; 0.954 sec/batch; 74h:54m:42s remains)
INFO - root - 2017-12-07 21:19:57.295573: step 49690, loss = 21.56, batch loss = 21.48 (7.9 examples/sec; 1.007 sec/batch; 79h:08m:12s remains)
INFO - root - 2017-12-07 21:20:06.670995: step 49700, loss = 21.70, batch loss = 21.62 (8.5 examples/sec; 0.942 sec/batch; 74h:00m:22s remains)
2017-12-07 21:20:07.704354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1801395 -4.1739416 -4.2090855 -4.2430825 -4.2366948 -4.2056084 -4.1945534 -4.2219949 -4.2547989 -4.2399592 -4.1764059 -4.1104355 -4.0836058 -4.1042752 -4.1416922][-4.1349721 -4.1321216 -4.173028 -4.21461 -4.2190032 -4.1983547 -4.1882963 -4.2126145 -4.2515554 -4.258832 -4.219698 -4.1693888 -4.1427665 -4.1464391 -4.1642094][-4.1050053 -4.1172757 -4.1679459 -4.2120008 -4.2232156 -4.2117491 -4.2002654 -4.2112393 -4.236465 -4.2455955 -4.2227507 -4.1909924 -4.1721153 -4.1674652 -4.176518][-4.0717144 -4.1087236 -4.1693664 -4.213131 -4.23 -4.2277212 -4.2170358 -4.2132535 -4.2149591 -4.2096462 -4.1921697 -4.1781178 -4.1698451 -4.1636829 -4.1693683][-4.05265 -4.108377 -4.1692433 -4.2067022 -4.2217655 -4.2232151 -4.2152147 -4.2041559 -4.1880479 -4.1673551 -4.1512008 -4.1487803 -4.1474123 -4.1381035 -4.1361885][-4.0719509 -4.1287985 -4.1750073 -4.1967344 -4.1996045 -4.1930833 -4.1843958 -4.1751223 -4.1530638 -4.1230483 -4.1082063 -4.1133857 -4.1153731 -4.1037755 -4.0934825][-4.1099548 -4.1591835 -4.1876 -4.1909442 -4.1734953 -4.1464477 -4.1265068 -4.1156988 -4.0916419 -4.0597 -4.05062 -4.0668335 -4.0751128 -4.0655694 -4.0529737][-4.1262488 -4.1650519 -4.1806264 -4.1689448 -4.1305251 -4.0816708 -4.0480766 -4.03341 -4.0101142 -3.9841852 -3.9903138 -4.0228329 -4.0399632 -4.035573 -4.0266886][-4.1040053 -4.12649 -4.13694 -4.1276841 -4.0900416 -4.0409245 -4.0081029 -3.9932663 -3.9704051 -3.9535208 -3.9760513 -4.0208712 -4.04028 -4.0369158 -4.0302057][-4.0643311 -4.0752664 -4.092041 -4.1001515 -4.0828943 -4.0502615 -4.0283818 -4.0145607 -3.9906664 -3.9816117 -4.0115609 -4.0539694 -4.0620418 -4.0504208 -4.0403972][-4.0473223 -4.0554276 -4.0712509 -4.0847359 -4.0801773 -4.0626287 -4.0527921 -4.0413237 -4.0181475 -4.0130172 -4.0400186 -4.0688014 -4.0594554 -4.0372987 -4.0267162][-4.0464973 -4.0523911 -4.0586576 -4.0636368 -4.0599637 -4.0503845 -4.0482068 -4.0362549 -4.0157881 -4.0161276 -4.0420532 -4.0619674 -4.0438905 -4.0183239 -4.0122867][-4.0488491 -4.0566912 -4.058948 -4.0572743 -4.0516224 -4.0454059 -4.0404377 -4.0196981 -4.00152 -4.0112176 -4.0466886 -4.0716496 -4.0587621 -4.0381422 -4.0372095][-4.0637879 -4.0808544 -4.0884018 -4.0871234 -4.0833683 -4.0798306 -4.0668993 -4.0313797 -4.0080113 -4.0216446 -4.0648794 -4.0983448 -4.1006236 -4.0966182 -4.1082392][-4.0787516 -4.1075158 -4.1247873 -4.1299634 -4.1342163 -4.1357646 -4.1184955 -4.0729284 -4.0423288 -4.054225 -4.0989084 -4.1407151 -4.1629438 -4.1842489 -4.2132225]]...]
INFO - root - 2017-12-07 21:20:17.027941: step 49710, loss = 21.44, batch loss = 21.36 (8.2 examples/sec; 0.981 sec/batch; 77h:01m:47s remains)
INFO - root - 2017-12-07 21:20:26.286938: step 49720, loss = 21.67, batch loss = 21.59 (8.1 examples/sec; 0.982 sec/batch; 77h:07m:41s remains)
INFO - root - 2017-12-07 21:20:35.665019: step 49730, loss = 21.43, batch loss = 21.34 (8.5 examples/sec; 0.939 sec/batch; 73h:47m:27s remains)
INFO - root - 2017-12-07 21:20:44.907509: step 49740, loss = 21.61, batch loss = 21.52 (8.7 examples/sec; 0.920 sec/batch; 72h:17m:53s remains)
INFO - root - 2017-12-07 21:20:54.316476: step 49750, loss = 21.04, batch loss = 20.95 (8.7 examples/sec; 0.925 sec/batch; 72h:38m:18s remains)
INFO - root - 2017-12-07 21:21:03.807136: step 49760, loss = 21.10, batch loss = 21.02 (8.0 examples/sec; 0.999 sec/batch; 78h:27m:26s remains)
INFO - root - 2017-12-07 21:21:13.256856: step 49770, loss = 21.08, batch loss = 21.00 (8.2 examples/sec; 0.974 sec/batch; 76h:30m:09s remains)
INFO - root - 2017-12-07 21:21:22.515814: step 49780, loss = 20.88, batch loss = 20.80 (9.2 examples/sec; 0.871 sec/batch; 68h:22m:57s remains)
INFO - root - 2017-12-07 21:21:31.965296: step 49790, loss = 21.42, batch loss = 21.33 (8.7 examples/sec; 0.922 sec/batch; 72h:24m:19s remains)
INFO - root - 2017-12-07 21:21:41.537228: step 49800, loss = 21.02, batch loss = 20.93 (8.5 examples/sec; 0.946 sec/batch; 74h:18m:56s remains)
2017-12-07 21:21:42.668936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4978948 -4.5639811 -4.636251 -4.6854463 -4.6818967 -4.6331053 -4.5660968 -4.50389 -4.4686203 -4.4798303 -4.5299258 -4.5778203 -4.5867586 -4.5516276 -4.4976783][-4.5473943 -4.6363511 -4.7200217 -4.7644148 -4.7451167 -4.6763177 -4.5998793 -4.5435767 -4.5208163 -4.5369315 -4.5917845 -4.6563344 -4.694634 -4.6828456 -4.6336746][-4.5775185 -4.6688967 -4.7303867 -4.727674 -4.6521645 -4.5354719 -4.4349613 -4.3826385 -4.3784151 -4.4115324 -4.4879875 -4.5958014 -4.696126 -4.7379975 -4.7149572][-4.5456805 -4.6164255 -4.6359329 -4.5720468 -4.4311724 -4.2605991 -4.1319065 -4.0840712 -4.1021748 -4.15632 -4.2553086 -4.4048662 -4.564157 -4.6591392 -4.6688519][-4.4411325 -4.4860787 -4.4759684 -4.3758855 -4.1901274 -3.9717693 -3.8079233 -3.753751 -3.7891104 -3.8673637 -3.9916253 -4.1732907 -4.3656192 -4.4865074 -4.515378][-4.3082876 -4.3404851 -4.3335614 -4.2367282 -4.0352249 -3.7819552 -3.5784616 -3.4995344 -3.5349951 -3.6361961 -3.7947085 -4.0044417 -4.204453 -4.3203912 -4.345602][-4.2000914 -4.24647 -4.278079 -4.2220511 -4.0416961 -3.7828867 -3.5523157 -3.4367259 -3.4471264 -3.550107 -3.7262836 -3.9406297 -4.1196346 -4.2086205 -4.2148123][-4.1486969 -4.2258391 -4.3050046 -4.3026438 -4.171402 -3.9446514 -3.7196059 -3.5819867 -3.5626829 -3.6469982 -3.8072102 -3.9866412 -4.1183624 -4.1701941 -4.15334][-4.1616225 -4.2632318 -4.3695054 -4.4039359 -4.3245549 -4.1539779 -3.9689765 -3.84314 -3.8159218 -3.8839297 -4.0107307 -4.13182 -4.1995945 -4.2055573 -4.160964][-4.2260957 -4.3408546 -4.4494524 -4.4934783 -4.44846 -4.3344069 -4.2059593 -4.1162853 -4.100018 -4.1576424 -4.2519779 -4.3208203 -4.3316545 -4.2914791 -4.2167292][-4.2974582 -4.4135733 -4.5067005 -4.5400858 -4.5088248 -4.4355416 -4.3577852 -4.3050632 -4.2995028 -4.3472104 -4.4209776 -4.4645824 -4.4487062 -4.3826013 -4.2909603][-4.3126707 -4.4182129 -4.4915886 -4.5128551 -4.4885311 -4.4417543 -4.3961377 -4.3625975 -4.3546929 -4.3883376 -4.45025 -4.4904017 -4.4738536 -4.407896 -4.3240042][-4.268775 -4.350913 -4.4055591 -4.4235988 -4.4127851 -4.3883953 -4.3601494 -4.3298559 -4.3109012 -4.3264985 -4.3732533 -4.4091845 -4.3993192 -4.3502293 -4.2957687][-4.2166381 -4.2650023 -4.2994194 -4.3160324 -4.3188105 -4.3100281 -4.2883739 -4.25463 -4.22732 -4.2303133 -4.2616444 -4.2878041 -4.2821422 -4.2538438 -4.2331233][-4.1948347 -4.2149038 -4.2298293 -4.2417126 -4.2507224 -4.247591 -4.2249513 -4.1873627 -4.1569567 -4.1530156 -4.170723 -4.1866779 -4.1850905 -4.1755724 -4.1819086]]...]
INFO - root - 2017-12-07 21:21:52.056657: step 49810, loss = 21.22, batch loss = 21.13 (8.5 examples/sec; 0.945 sec/batch; 74h:12m:24s remains)
INFO - root - 2017-12-07 21:22:01.597236: step 49820, loss = 21.52, batch loss = 21.43 (8.3 examples/sec; 0.962 sec/batch; 75h:31m:18s remains)
INFO - root - 2017-12-07 21:22:10.927505: step 49830, loss = 21.59, batch loss = 21.50 (9.4 examples/sec; 0.851 sec/batch; 66h:50m:25s remains)
INFO - root - 2017-12-07 21:22:20.476537: step 49840, loss = 21.53, batch loss = 21.45 (8.6 examples/sec; 0.934 sec/batch; 73h:18m:41s remains)
INFO - root - 2017-12-07 21:22:29.787071: step 49850, loss = 21.77, batch loss = 21.68 (8.5 examples/sec; 0.937 sec/batch; 73h:32m:38s remains)
INFO - root - 2017-12-07 21:22:39.139182: step 49860, loss = 21.27, batch loss = 21.19 (8.3 examples/sec; 0.969 sec/batch; 76h:03m:07s remains)
INFO - root - 2017-12-07 21:22:48.553360: step 49870, loss = 21.87, batch loss = 21.79 (8.6 examples/sec; 0.931 sec/batch; 73h:06m:07s remains)
INFO - root - 2017-12-07 21:22:57.841704: step 49880, loss = 21.09, batch loss = 21.01 (9.1 examples/sec; 0.882 sec/batch; 69h:14m:45s remains)
INFO - root - 2017-12-07 21:23:07.189789: step 49890, loss = 21.41, batch loss = 21.33 (9.0 examples/sec; 0.894 sec/batch; 70h:09m:38s remains)
INFO - root - 2017-12-07 21:23:16.659961: step 49900, loss = 21.51, batch loss = 21.43 (8.7 examples/sec; 0.920 sec/batch; 72h:14m:02s remains)
2017-12-07 21:23:17.581343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5801744 -4.5815368 -4.6141095 -4.6246819 -4.607018 -4.586441 -4.5995922 -4.637908 -4.6590285 -4.6487985 -4.5821528 -4.47428 -4.4232779 -4.4582043 -4.5337763][-4.5140796 -4.4766541 -4.5058527 -4.5376463 -4.5261779 -4.48602 -4.4681 -4.4923506 -4.5363259 -4.5547209 -4.4956164 -4.3684416 -4.2831788 -4.2962132 -4.3653626][-4.4369488 -4.3581295 -4.3682575 -4.4093304 -4.4023781 -4.3411336 -4.2781496 -4.2730803 -4.3467131 -4.4339738 -4.4373412 -4.3364706 -4.2304568 -4.2040281 -4.2385783][-4.4100223 -4.3098803 -4.2967844 -4.3210258 -4.2937431 -4.1963468 -4.0765653 -4.0324173 -4.13682 -4.3113403 -4.413341 -4.3733082 -4.2624016 -4.1918316 -4.1836748][-4.4539156 -4.3546824 -4.3214335 -4.3104763 -4.238656 -4.0883784 -3.9050472 -3.8182683 -3.9420388 -4.18506 -4.37875 -4.4116554 -4.3126454 -4.2055206 -4.1524773][-4.5236788 -4.4459691 -4.3983293 -4.3433418 -4.2160292 -4.0112014 -3.7828069 -3.6737406 -3.8068423 -4.076499 -4.3157492 -4.4027243 -4.3258491 -4.2003727 -4.1129384][-4.515214 -4.4610786 -4.4127975 -4.3358889 -4.1814156 -3.95186 -3.7132604 -3.6069374 -3.7444093 -4.0122604 -4.2607284 -4.3791995 -4.329299 -4.2071505 -4.10912][-4.3951993 -4.3548789 -4.3299856 -4.2841921 -4.1638207 -3.9648726 -3.7561986 -3.6651886 -3.7861648 -4.0288448 -4.2623844 -4.3870826 -4.3533807 -4.2380095 -4.1403818][-4.2505493 -4.2102089 -4.2155828 -4.2338529 -4.1947412 -4.0728087 -3.9269776 -3.8593407 -3.9428 -4.127718 -4.3127656 -4.4143553 -4.3830438 -4.2744012 -4.1851][-4.1832142 -4.1410971 -4.1603823 -4.2281909 -4.2608008 -4.2110167 -4.1283946 -4.0896807 -4.145174 -4.2678981 -4.3913455 -4.4600005 -4.4356918 -4.3465238 -4.2673893][-4.2164135 -4.1698856 -4.1811547 -4.2626886 -4.3277082 -4.3104148 -4.2580681 -4.2403727 -4.2866154 -4.3724136 -4.4580564 -4.5150213 -4.5100007 -4.44088 -4.3553386][-4.2931056 -4.2506123 -4.2447367 -4.31563 -4.3839679 -4.3679285 -4.3125434 -4.2922173 -4.3345428 -4.4153738 -4.5037971 -4.5741835 -4.5835218 -4.5116577 -4.3968759][-4.397078 -4.3573575 -4.3346353 -4.3824487 -4.4343324 -4.4029179 -4.3217268 -4.2773767 -4.3135633 -4.4092607 -4.5231509 -4.6103034 -4.6109209 -4.5105963 -4.362721][-4.4985042 -4.4657655 -4.4273682 -4.4416103 -4.4701366 -4.4230661 -4.3178797 -4.2438326 -4.2629414 -4.362833 -4.4891582 -4.5744772 -4.5454807 -4.4079466 -4.2363076][-4.5359907 -4.513772 -4.474247 -4.4719572 -4.4956312 -4.4505453 -4.3401723 -4.2458997 -4.237473 -4.3157039 -4.4304361 -4.5000787 -4.4440155 -4.2862329 -4.1105032]]...]
INFO - root - 2017-12-07 21:23:27.019658: step 49910, loss = 21.48, batch loss = 21.39 (8.4 examples/sec; 0.949 sec/batch; 74h:31m:44s remains)
INFO - root - 2017-12-07 21:23:36.469251: step 49920, loss = 21.53, batch loss = 21.45 (8.3 examples/sec; 0.963 sec/batch; 75h:37m:23s remains)
INFO - root - 2017-12-07 21:23:45.788652: step 49930, loss = 21.04, batch loss = 20.95 (8.5 examples/sec; 0.945 sec/batch; 74h:09m:07s remains)
INFO - root - 2017-12-07 21:23:55.199652: step 49940, loss = 21.19, batch loss = 21.11 (9.1 examples/sec; 0.882 sec/batch; 69h:13m:12s remains)
INFO - root - 2017-12-07 21:24:04.587657: step 49950, loss = 21.56, batch loss = 21.47 (9.1 examples/sec; 0.877 sec/batch; 68h:49m:36s remains)
INFO - root - 2017-12-07 21:24:13.972416: step 49960, loss = 21.89, batch loss = 21.80 (8.5 examples/sec; 0.940 sec/batch; 73h:45m:31s remains)
INFO - root - 2017-12-07 21:24:23.387881: step 49970, loss = 21.40, batch loss = 21.31 (8.5 examples/sec; 0.937 sec/batch; 73h:32m:14s remains)
INFO - root - 2017-12-07 21:24:32.720535: step 49980, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.970 sec/batch; 76h:05m:28s remains)
INFO - root - 2017-12-07 21:24:42.083934: step 49990, loss = 21.30, batch loss = 21.22 (9.0 examples/sec; 0.889 sec/batch; 69h:45m:08s remains)
INFO - root - 2017-12-07 21:24:51.543120: step 50000, loss = 21.31, batch loss = 21.23 (8.8 examples/sec; 0.908 sec/batch; 71h:13m:22s remains)
2017-12-07 21:24:52.503942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5127482 -4.6074238 -4.6638622 -4.6746397 -4.6439571 -4.6053786 -4.6101065 -4.6588092 -4.7065091 -4.7298388 -4.74187 -4.7418818 -4.711741 -4.63908 -4.5382724][-4.5635071 -4.6700749 -4.7249961 -4.7200885 -4.6526976 -4.5697589 -4.5538325 -4.620122 -4.6952934 -4.7228122 -4.7148428 -4.6858811 -4.6432242 -4.5755506 -4.4816279][-4.6068716 -4.7054868 -4.7441306 -4.70939 -4.5851035 -4.429811 -4.3701067 -4.4494576 -4.5647278 -4.6260095 -4.6291971 -4.5910969 -4.5443282 -4.4857836 -4.4011521][-4.6391416 -4.7115297 -4.7150993 -4.632638 -4.4443612 -4.2143974 -4.1059856 -4.1958137 -4.3636594 -4.4868536 -4.5428977 -4.5298243 -4.4881344 -4.4332452 -4.3488531][-4.6924524 -4.7389135 -4.69367 -4.546782 -4.2974968 -4.0101652 -3.8608375 -3.9619873 -4.1789522 -4.366087 -4.4868603 -4.5187345 -4.4920049 -4.4423261 -4.3567014][-4.7429595 -4.7551227 -4.6454053 -4.4209027 -4.1160936 -3.7973185 -3.6293221 -3.7504008 -4.0054026 -4.2275238 -4.388063 -4.4594045 -4.4563279 -4.4332304 -4.3741589][-4.7355614 -4.71125 -4.5471296 -4.25246 -3.8938375 -3.5547814 -3.39366 -3.558888 -3.8585575 -4.0981646 -4.2635527 -4.3346615 -4.3340139 -4.3359728 -4.3206491][-4.6987877 -4.6776843 -4.5220842 -4.2293038 -3.8692026 -3.5488148 -3.4135766 -3.6042128 -3.9104674 -4.1165519 -4.2250934 -4.2534156 -4.2310033 -4.2352676 -4.2479405][-4.7077656 -4.7335925 -4.6373715 -4.4052911 -4.0945697 -3.8264618 -3.7245307 -3.9032702 -4.173594 -4.3158789 -4.3477955 -4.332375 -4.2903228 -4.2780247 -4.2835321][-4.7289338 -4.7902145 -4.7376256 -4.5614815 -4.2999244 -4.0652275 -3.9644504 -4.0937467 -4.3170247 -4.4286346 -4.438014 -4.4379587 -4.4204588 -4.4055195 -4.3925276][-4.7247343 -4.8134046 -4.8078074 -4.7035742 -4.5081167 -4.307271 -4.1955013 -4.2444615 -4.3801022 -4.4517407 -4.4715643 -4.5278826 -4.5596242 -4.5491729 -4.50637][-4.6869812 -4.7872667 -4.8208714 -4.7889881 -4.6805978 -4.54738 -4.4626646 -4.4583941 -4.5030046 -4.5207648 -4.5397425 -4.6235085 -4.6732779 -4.6446776 -4.5620451][-4.6311731 -4.7182331 -4.7593341 -4.7610531 -4.7130585 -4.6440558 -4.6090117 -4.6052456 -4.6107788 -4.6000667 -4.6028852 -4.6534657 -4.6700587 -4.6153283 -4.527935][-4.5945053 -4.6650195 -4.7017913 -4.7113972 -4.6953397 -4.6746221 -4.6826005 -4.6989813 -4.7005811 -4.6860623 -4.6661625 -4.6556621 -4.6196404 -4.550313 -4.4886465][-4.5393343 -4.592257 -4.6211905 -4.6314368 -4.6331048 -4.6397781 -4.6612339 -4.6772428 -4.6755815 -4.66573 -4.6444273 -4.6102633 -4.5584383 -4.4981704 -4.4641728]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 21:25:02.796146: step 50010, loss = 21.52, batch loss = 21.44 (8.7 examples/sec; 0.918 sec/batch; 72h:01m:35s remains)
INFO - root - 2017-12-07 21:25:12.220973: step 50020, loss = 21.34, batch loss = 21.25 (8.5 examples/sec; 0.942 sec/batch; 73h:55m:36s remains)
INFO - root - 2017-12-07 21:25:21.575743: step 50030, loss = 21.22, batch loss = 21.14 (8.6 examples/sec; 0.927 sec/batch; 72h:45m:42s remains)
INFO - root - 2017-12-07 21:25:30.944154: step 50040, loss = 21.56, batch loss = 21.48 (8.7 examples/sec; 0.916 sec/batch; 71h:51m:06s remains)
INFO - root - 2017-12-07 21:25:40.331260: step 50050, loss = 21.32, batch loss = 21.24 (8.7 examples/sec; 0.922 sec/batch; 72h:20m:28s remains)
INFO - root - 2017-12-07 21:25:49.698066: step 50060, loss = 21.44, batch loss = 21.36 (8.4 examples/sec; 0.956 sec/batch; 74h:59m:24s remains)
INFO - root - 2017-12-07 21:25:59.003690: step 50070, loss = 21.53, batch loss = 21.45 (8.3 examples/sec; 0.960 sec/batch; 75h:19m:05s remains)
INFO - root - 2017-12-07 21:26:08.408266: step 50080, loss = 21.01, batch loss = 20.93 (8.2 examples/sec; 0.971 sec/batch; 76h:11m:38s remains)
INFO - root - 2017-12-07 21:26:17.876549: step 50090, loss = 21.45, batch loss = 21.36 (8.1 examples/sec; 0.987 sec/batch; 77h:25m:27s remains)
INFO - root - 2017-12-07 21:26:27.313668: step 50100, loss = 21.28, batch loss = 21.20 (8.6 examples/sec; 0.927 sec/batch; 72h:43m:11s remains)
2017-12-07 21:26:28.291359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1777344 -4.1713552 -4.2355762 -4.3179049 -4.3507161 -4.3003283 -4.1971946 -4.11449 -4.1003261 -4.15161 -4.2158275 -4.27131 -4.3277245 -4.3677268 -4.3766742][-4.1902938 -4.2021422 -4.2742634 -4.3455548 -4.3564382 -4.2867341 -4.1785 -4.1011357 -4.0986829 -4.1571426 -4.2178264 -4.2601128 -4.3037643 -4.3452468 -4.3743787][-4.1894064 -4.2081718 -4.2705712 -4.3161764 -4.3063626 -4.23339 -4.1385489 -4.081656 -4.0914164 -4.1499119 -4.2076554 -4.2445059 -4.2801747 -4.3203049 -4.3566532][-4.1890092 -4.1982455 -4.234323 -4.2511125 -4.2281065 -4.1606932 -4.082273 -4.0405159 -4.0512362 -4.1024971 -4.162199 -4.2112169 -4.2580876 -4.3031564 -4.3366513][-4.17975 -4.1783543 -4.1968231 -4.19847 -4.17424 -4.1113205 -4.0371165 -3.9922855 -3.988905 -4.0234184 -4.0812593 -4.1462278 -4.2184424 -4.2831612 -4.3262348][-4.1741695 -4.1704345 -4.1880631 -4.1918945 -4.1740851 -4.1122828 -4.0266781 -3.9603534 -3.935673 -3.9557273 -4.0147414 -4.1013947 -4.2074051 -4.2985778 -4.3503237][-4.2033229 -4.2095561 -4.2321773 -4.23666 -4.2147932 -4.1399379 -4.0284042 -3.935302 -3.8986084 -3.917001 -3.9835389 -4.0917292 -4.2261982 -4.3400064 -4.3997841][-4.20668 -4.2419715 -4.2779493 -4.2802968 -4.2411222 -4.1458726 -4.0176697 -3.9193909 -3.8959551 -3.9283686 -4.0004067 -4.1144266 -4.2586188 -4.3801913 -4.4421511][-4.1666961 -4.2337303 -4.2809095 -4.2727141 -4.2094965 -4.1013303 -3.9775019 -3.9086254 -3.9321938 -3.999114 -4.0780754 -4.1815667 -4.3131123 -4.4229097 -4.4727254][-4.1385064 -4.237319 -4.2862148 -4.2546697 -4.1638484 -4.0444794 -3.9350762 -3.9016819 -3.97157 -4.0752292 -4.1665053 -4.2601185 -4.370667 -4.4651766 -4.5104251][-4.1191411 -4.2393565 -4.2922363 -4.2506289 -4.1557479 -4.0493631 -3.969399 -3.9664476 -4.0556655 -4.168848 -4.2659636 -4.3528433 -4.4373918 -4.5074158 -4.5464153][-4.0910244 -4.2265773 -4.2952013 -4.2672439 -4.198319 -4.1319804 -4.0926809 -4.1131024 -4.1968536 -4.2931085 -4.3815575 -4.4582763 -4.5129867 -4.5511904 -4.5731845][-4.1322985 -4.2799616 -4.3591232 -4.3447242 -4.2959595 -4.2557082 -4.2380314 -4.26681 -4.3403878 -4.4187164 -4.4935102 -4.5526552 -4.5708365 -4.5702815 -4.5655894][-4.2549458 -4.3942666 -4.4627318 -4.4438686 -4.401958 -4.3778419 -4.3712134 -4.398632 -4.4567633 -4.5145831 -4.5698905 -4.6033697 -4.5878391 -4.5536551 -4.5223231][-4.4025497 -4.5123954 -4.5555668 -4.5227027 -4.4810987 -4.4682755 -4.4717989 -4.4927397 -4.5241561 -4.5492358 -4.5748191 -4.5830884 -4.5513282 -4.503983 -4.4609876]]...]
INFO - root - 2017-12-07 21:26:37.686824: step 50110, loss = 21.22, batch loss = 21.13 (8.0 examples/sec; 0.998 sec/batch; 78h:16m:50s remains)
INFO - root - 2017-12-07 21:26:47.047547: step 50120, loss = 21.55, batch loss = 21.47 (8.2 examples/sec; 0.972 sec/batch; 76h:14m:34s remains)
INFO - root - 2017-12-07 21:26:56.455814: step 50130, loss = 21.80, batch loss = 21.72 (8.6 examples/sec; 0.935 sec/batch; 73h:18m:02s remains)
INFO - root - 2017-12-07 21:27:05.871950: step 50140, loss = 21.77, batch loss = 21.69 (8.9 examples/sec; 0.903 sec/batch; 70h:49m:24s remains)
INFO - root - 2017-12-07 21:27:15.325492: step 50150, loss = 21.13, batch loss = 21.04 (8.9 examples/sec; 0.895 sec/batch; 70h:10m:20s remains)
INFO - root - 2017-12-07 21:27:24.586646: step 50160, loss = 21.22, batch loss = 21.14 (8.0 examples/sec; 0.997 sec/batch; 78h:13m:08s remains)
INFO - root - 2017-12-07 21:27:33.908789: step 50170, loss = 21.45, batch loss = 21.36 (9.0 examples/sec; 0.893 sec/batch; 70h:04m:10s remains)
INFO - root - 2017-12-07 21:27:43.269850: step 50180, loss = 21.66, batch loss = 21.57 (9.3 examples/sec; 0.856 sec/batch; 67h:08m:20s remains)
INFO - root - 2017-12-07 21:27:52.653874: step 50190, loss = 21.49, batch loss = 21.41 (8.8 examples/sec; 0.911 sec/batch; 71h:27m:20s remains)
INFO - root - 2017-12-07 21:28:02.108131: step 50200, loss = 21.89, batch loss = 21.80 (8.5 examples/sec; 0.937 sec/batch; 73h:27m:32s remains)
2017-12-07 21:28:03.122554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4862514 -4.56899 -4.6208196 -4.6295166 -4.5868163 -4.4898233 -4.3899026 -4.3388 -4.3414083 -4.4376717 -4.5724816 -4.6145315 -4.5811009 -4.5558224 -4.5317545][-4.5041471 -4.5953803 -4.648385 -4.6540561 -4.6036892 -4.49311 -4.380815 -4.3213677 -4.3254914 -4.4385185 -4.5963683 -4.6504836 -4.6123829 -4.5777006 -4.5463057][-4.5168757 -4.61673 -4.6688237 -4.66786 -4.6014543 -4.4743452 -4.3510418 -4.2881737 -4.3011074 -4.4305329 -4.60623 -4.6850224 -4.6622376 -4.6163983 -4.5625405][-4.5347834 -4.6374307 -4.6785769 -4.6560383 -4.5545611 -4.4022317 -4.2665396 -4.2006311 -4.2273393 -4.3666778 -4.5500507 -4.6647029 -4.6846013 -4.6429996 -4.5696878][-4.5557594 -4.6560078 -4.6756215 -4.61502 -4.4652562 -4.2902217 -4.1592617 -4.109199 -4.1578727 -4.2965331 -4.4621339 -4.5875816 -4.646522 -4.6217933 -4.5442424][-4.5671368 -4.6514006 -4.6380782 -4.526485 -4.3250403 -4.1286016 -4.00475 -3.9750118 -4.0566516 -4.2101631 -4.3631473 -4.4904432 -4.5743542 -4.5648723 -4.4911451][-4.5619755 -4.6155553 -4.5598316 -4.3951845 -4.1488824 -3.9318559 -3.7972164 -3.7687769 -3.8852053 -4.0798192 -4.2537031 -4.40189 -4.5118885 -4.515461 -4.44791][-4.5488782 -4.5777955 -4.4889641 -4.2879329 -4.0108457 -3.7712584 -3.6116948 -3.5736065 -3.7196817 -3.9622982 -4.1782026 -4.3562474 -4.4836268 -4.4966812 -4.4403477][-4.5548534 -4.5744972 -4.4821682 -4.2837281 -4.0093217 -3.7590678 -3.5772028 -3.5343494 -3.7010989 -3.9725165 -4.2131085 -4.3947058 -4.5088696 -4.5157514 -4.4666705][-4.5873871 -4.6174016 -4.5526109 -4.4024248 -4.1806126 -3.9474764 -3.7508304 -3.69549 -3.8543382 -4.1180038 -4.352674 -4.5083838 -4.583364 -4.5641789 -4.5045562][-4.6254039 -4.6740432 -4.6418262 -4.5494661 -4.3992648 -4.2008376 -3.9974768 -3.9211781 -4.0466361 -4.2759604 -4.4846764 -4.6113958 -4.6508465 -4.608253 -4.5295372][-4.640718 -4.7053227 -4.7004719 -4.6581678 -4.5742588 -4.4243221 -4.2426052 -4.1618109 -4.2560897 -4.4436884 -4.6094689 -4.694653 -4.6980267 -4.6340861 -4.5338387][-4.6254878 -4.6981125 -4.7116385 -4.70351 -4.6724825 -4.5880938 -4.4713035 -4.4191813 -4.4931059 -4.6370544 -4.7510767 -4.7809258 -4.73928 -4.649951 -4.5298963][-4.5876327 -4.6571565 -4.6786737 -4.682548 -4.6794605 -4.6556959 -4.6183524 -4.6128936 -4.6728377 -4.7669597 -4.8259678 -4.8123035 -4.7427626 -4.6385059 -4.5118103][-4.518713 -4.5830722 -4.612577 -4.6214385 -4.629683 -4.6411285 -4.6564803 -4.6805878 -4.7205791 -4.7650714 -4.78001 -4.7438374 -4.6701918 -4.5720181 -4.4612894]]...]
INFO - root - 2017-12-07 21:28:12.552590: step 50210, loss = 21.45, batch loss = 21.36 (9.2 examples/sec; 0.871 sec/batch; 68h:18m:11s remains)
INFO - root - 2017-12-07 21:28:21.896700: step 50220, loss = 21.45, batch loss = 21.37 (8.6 examples/sec; 0.925 sec/batch; 72h:33m:58s remains)
INFO - root - 2017-12-07 21:28:31.272438: step 50230, loss = 21.16, batch loss = 21.08 (9.4 examples/sec; 0.851 sec/batch; 66h:45m:40s remains)
INFO - root - 2017-12-07 21:28:40.562961: step 50240, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.930 sec/batch; 72h:53m:35s remains)
INFO - root - 2017-12-07 21:28:49.903059: step 50250, loss = 21.44, batch loss = 21.36 (8.5 examples/sec; 0.936 sec/batch; 73h:22m:00s remains)
INFO - root - 2017-12-07 21:28:59.264576: step 50260, loss = 21.39, batch loss = 21.31 (8.6 examples/sec; 0.931 sec/batch; 72h:58m:29s remains)
INFO - root - 2017-12-07 21:29:08.747756: step 50270, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.933 sec/batch; 73h:08m:33s remains)
INFO - root - 2017-12-07 21:29:18.186258: step 50280, loss = 20.89, batch loss = 20.81 (8.5 examples/sec; 0.945 sec/batch; 74h:03m:30s remains)
INFO - root - 2017-12-07 21:29:27.515313: step 50290, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.966 sec/batch; 75h:45m:49s remains)
INFO - root - 2017-12-07 21:29:36.899559: step 50300, loss = 21.55, batch loss = 21.47 (8.9 examples/sec; 0.895 sec/batch; 70h:09m:26s remains)
2017-12-07 21:29:37.987439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6065669 -4.6613922 -4.6908283 -4.6953082 -4.6964121 -4.7047319 -4.705617 -4.7066088 -4.7091446 -4.7136431 -4.7360544 -4.8056126 -4.8978968 -4.9310493 -4.8737969][-4.4781919 -4.5620556 -4.6112132 -4.61846 -4.6141548 -4.6109962 -4.5991597 -4.5928221 -4.5987506 -4.6198354 -4.6635365 -4.74161 -4.8326483 -4.8653669 -4.8107624][-4.3564291 -4.4773264 -4.552134 -4.5635338 -4.5423403 -4.5096455 -4.467802 -4.4410048 -4.4472895 -4.4892797 -4.5599642 -4.6445622 -4.7215128 -4.7453456 -4.7037182][-4.3120332 -4.4512105 -4.5374541 -4.5475903 -4.4990506 -4.415112 -4.3196363 -4.26396 -4.2769389 -4.3434019 -4.4332504 -4.51368 -4.5703106 -4.5910969 -4.5772786][-4.2948747 -4.4132004 -4.4886513 -4.4941239 -4.4198976 -4.2712679 -4.1013446 -4.0168934 -4.0571795 -4.1649075 -4.2730646 -4.3504615 -4.4020839 -4.4405322 -4.4685268][-4.24242 -4.3115892 -4.3743706 -4.3837795 -4.2853336 -4.061862 -3.8097968 -3.709971 -3.8101947 -3.9903648 -4.1334343 -4.2192044 -4.2747288 -4.3310204 -4.3887258][-4.1478157 -4.1686916 -4.2205119 -4.2324357 -4.1091347 -3.8230844 -3.51663 -3.4287853 -3.6088696 -3.8722839 -4.0593095 -4.1568551 -4.2082424 -4.2675686 -4.332727][-4.0852604 -4.0637627 -4.1025834 -4.1144505 -3.9786057 -3.6764705 -3.37216 -3.3194213 -3.5588634 -3.8696835 -4.0774016 -4.1678123 -4.2000995 -4.2532711 -4.3155947][-4.1102147 -4.0744324 -4.1118879 -4.1353645 -4.0212584 -3.7634027 -3.5143404 -3.4958024 -3.7344604 -4.022975 -4.2024021 -4.25881 -4.2617955 -4.3013511 -4.3477297][-4.1796627 -4.1647949 -4.2203608 -4.2686739 -4.200357 -4.0131016 -3.8308775 -3.8266723 -4.0195532 -4.2409067 -4.3626876 -4.3747721 -4.3556294 -4.3844557 -4.4157524][-4.2559237 -4.2853827 -4.3749456 -4.4515929 -4.4278097 -4.299438 -4.1630263 -4.1470532 -4.2768445 -4.4306588 -4.5014052 -4.4831276 -4.4587941 -4.4857216 -4.5053034][-4.3478041 -4.4045043 -4.509244 -4.5939078 -4.5955768 -4.5076447 -4.3995142 -4.368372 -4.4488573 -4.56376 -4.6165824 -4.601934 -4.5899005 -4.6147261 -4.6109419][-4.4198213 -4.4687181 -4.5493379 -4.6131792 -4.6203165 -4.562973 -4.4847269 -4.4558434 -4.5122371 -4.6115422 -4.6750832 -4.6904054 -4.7017131 -4.7186828 -4.6861219][-4.4421663 -4.46742 -4.5096269 -4.5464816 -4.5582542 -4.53468 -4.4948726 -4.48042 -4.5211906 -4.5988407 -4.6651821 -4.7067957 -4.7340317 -4.7360244 -4.6835489][-4.4412203 -4.4477777 -4.4632707 -4.4813819 -4.4936953 -4.49206 -4.4804549 -4.4771214 -4.4996214 -4.5444837 -4.5942736 -4.6460819 -4.6868062 -4.6886091 -4.6409855]]...]
INFO - root - 2017-12-07 21:29:47.309783: step 50310, loss = 21.12, batch loss = 21.03 (8.2 examples/sec; 0.979 sec/batch; 76h:43m:35s remains)
INFO - root - 2017-12-07 21:29:56.627319: step 50320, loss = 21.69, batch loss = 21.61 (8.3 examples/sec; 0.967 sec/batch; 75h:48m:05s remains)
INFO - root - 2017-12-07 21:30:05.973190: step 50330, loss = 21.70, batch loss = 21.62 (8.7 examples/sec; 0.923 sec/batch; 72h:21m:30s remains)
INFO - root - 2017-12-07 21:30:15.288426: step 50340, loss = 21.40, batch loss = 21.31 (8.6 examples/sec; 0.933 sec/batch; 73h:05m:30s remains)
INFO - root - 2017-12-07 21:30:24.687319: step 50350, loss = 21.09, batch loss = 21.01 (8.5 examples/sec; 0.943 sec/batch; 73h:53m:13s remains)
INFO - root - 2017-12-07 21:30:34.084100: step 50360, loss = 21.58, batch loss = 21.50 (8.4 examples/sec; 0.957 sec/batch; 74h:59m:47s remains)
INFO - root - 2017-12-07 21:30:43.424294: step 50370, loss = 21.02, batch loss = 20.93 (8.7 examples/sec; 0.918 sec/batch; 71h:55m:49s remains)
INFO - root - 2017-12-07 21:30:52.969517: step 50380, loss = 21.40, batch loss = 21.32 (9.0 examples/sec; 0.885 sec/batch; 69h:22m:07s remains)
INFO - root - 2017-12-07 21:31:02.332782: step 50390, loss = 21.40, batch loss = 21.32 (8.9 examples/sec; 0.899 sec/batch; 70h:25m:57s remains)
INFO - root - 2017-12-07 21:31:11.611681: step 50400, loss = 21.59, batch loss = 21.50 (8.4 examples/sec; 0.956 sec/batch; 74h:53m:36s remains)
2017-12-07 21:31:12.538130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.267632 -4.2678332 -4.255476 -4.2476063 -4.29276 -4.3592534 -4.3751078 -4.3735447 -4.385129 -4.430717 -4.4969831 -4.5654979 -4.6170673 -4.5607524 -4.3572483][-4.3063211 -4.3255115 -4.3210335 -4.3079286 -4.33974 -4.3927903 -4.4036922 -4.395288 -4.3953495 -4.4228354 -4.4750285 -4.5318089 -4.5731306 -4.5207672 -4.3384175][-4.3888803 -4.4431729 -4.4716816 -4.4648204 -4.4655623 -4.4795485 -4.4790921 -4.477427 -4.4763141 -4.4774837 -4.50006 -4.5433078 -4.57935 -4.5393009 -4.3950033][-4.455442 -4.523068 -4.5711689 -4.5588093 -4.5129871 -4.4754305 -4.4670186 -4.4979925 -4.5225267 -4.5119224 -4.5080342 -4.5371141 -4.5700827 -4.5511942 -4.455934][-4.4813766 -4.5089154 -4.5161772 -4.4594488 -4.3495784 -4.2481275 -4.2307682 -4.3219104 -4.4146762 -4.436667 -4.4391565 -4.4741406 -4.5121136 -4.5131445 -4.4566264][-4.4412313 -4.394722 -4.3223214 -4.1968613 -4.0172706 -3.8469625 -3.8148172 -3.9748843 -4.160121 -4.2570028 -4.3024464 -4.3633738 -4.4126797 -4.4180808 -4.3698611][-4.3633428 -4.2498612 -4.11043 -3.932821 -3.7058091 -3.4808891 -3.4354205 -3.6489263 -3.9147027 -4.0954008 -4.1983166 -4.2907958 -4.3494434 -4.3395724 -4.2629776][-4.33257 -4.1900573 -4.0213885 -3.8253865 -3.5902295 -3.3534145 -3.3128967 -3.5449448 -3.8377821 -4.0636258 -4.2038426 -4.3163915 -4.3779645 -4.3486109 -4.2364349][-4.3921471 -4.2626081 -4.0957413 -3.9059129 -3.7010765 -3.5129948 -3.5058246 -3.715117 -3.9636962 -4.1639743 -4.2963977 -4.4083118 -4.4673409 -4.4300394 -4.3060513][-4.5094166 -4.4022341 -4.2441778 -4.0732431 -3.9234748 -3.8241944 -3.8714719 -4.0384984 -4.2017455 -4.3229957 -4.4075842 -4.4944 -4.5395908 -4.5067267 -4.4067183][-4.6196756 -4.5369358 -4.399147 -4.2644963 -4.1807828 -4.1698465 -4.2573357 -4.3735342 -4.4450245 -4.4759612 -4.4993339 -4.5460854 -4.5656309 -4.5347171 -4.4704685][-4.6638246 -4.6090684 -4.5062666 -4.4198623 -4.3951755 -4.4389377 -4.5320921 -4.5974951 -4.6046677 -4.5821452 -4.5708661 -4.5888004 -4.5839019 -4.5473113 -4.5084772][-4.6342568 -4.6024704 -4.5337629 -4.4870276 -4.4931493 -4.5479312 -4.6166487 -4.6389937 -4.6147194 -4.5811644 -4.5712509 -4.5874691 -4.5769529 -4.5393891 -4.517231][-4.5262141 -4.5143552 -4.4780278 -4.4574494 -4.4690952 -4.5059266 -4.5403852 -4.53588 -4.5051141 -4.4831986 -4.490665 -4.5195637 -4.519619 -4.4910674 -4.4799662][-4.3685679 -4.3725882 -4.3654103 -4.3639603 -4.3727641 -4.3875694 -4.3980484 -4.3872123 -4.3679013 -4.3639426 -4.3858867 -4.4222665 -4.4319968 -4.4138036 -4.4074383]]...]
INFO - root - 2017-12-07 21:31:21.921258: step 50410, loss = 21.17, batch loss = 21.09 (9.1 examples/sec; 0.875 sec/batch; 68h:32m:44s remains)
INFO - root - 2017-12-07 21:31:31.393870: step 50420, loss = 21.29, batch loss = 21.21 (8.9 examples/sec; 0.898 sec/batch; 70h:22m:40s remains)
INFO - root - 2017-12-07 21:31:40.928802: step 50430, loss = 21.31, batch loss = 21.23 (8.1 examples/sec; 0.989 sec/batch; 77h:30m:31s remains)
INFO - root - 2017-12-07 21:31:50.089818: step 50440, loss = 21.89, batch loss = 21.81 (8.2 examples/sec; 0.971 sec/batch; 76h:04m:20s remains)
INFO - root - 2017-12-07 21:31:59.553014: step 50450, loss = 21.20, batch loss = 21.11 (7.7 examples/sec; 1.034 sec/batch; 81h:01m:33s remains)
INFO - root - 2017-12-07 21:32:08.945445: step 50460, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.941 sec/batch; 73h:43m:56s remains)
INFO - root - 2017-12-07 21:32:18.370410: step 50470, loss = 21.51, batch loss = 21.43 (8.5 examples/sec; 0.939 sec/batch; 73h:31m:32s remains)
INFO - root - 2017-12-07 21:32:27.735972: step 50480, loss = 21.17, batch loss = 21.09 (8.3 examples/sec; 0.963 sec/batch; 75h:28m:26s remains)
INFO - root - 2017-12-07 21:32:37.098809: step 50490, loss = 21.51, batch loss = 21.43 (8.2 examples/sec; 0.980 sec/batch; 76h:47m:05s remains)
INFO - root - 2017-12-07 21:32:46.536074: step 50500, loss = 21.72, batch loss = 21.64 (8.7 examples/sec; 0.923 sec/batch; 72h:20m:04s remains)
2017-12-07 21:32:47.429903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5721383 -4.561388 -4.5086551 -4.4176292 -4.3287268 -4.2803307 -4.2968884 -4.3516636 -4.3906336 -4.4118991 -4.4294949 -4.4421315 -4.440999 -4.42869 -4.41235][-4.6089897 -4.603116 -4.5637445 -4.4898543 -4.4075236 -4.3556633 -4.3533564 -4.3861957 -4.4207296 -4.4454174 -4.4598331 -4.4603724 -4.4445391 -4.4226565 -4.4023838][-4.5987639 -4.59704 -4.570209 -4.5156455 -4.4490314 -4.4004717 -4.3857141 -4.4012604 -4.4316006 -4.456223 -4.4640961 -4.4546275 -4.433095 -4.4124165 -4.3976817][-4.5627856 -4.566196 -4.5497689 -4.5093665 -4.4563718 -4.4138637 -4.3922749 -4.3964291 -4.4249034 -4.4534082 -4.4601059 -4.4448819 -4.4200597 -4.4007821 -4.3910041][-4.4951234 -4.5097528 -4.5045538 -4.4710226 -4.4279337 -4.3936634 -4.373342 -4.3738174 -4.40526 -4.4435859 -4.4582739 -4.4441414 -4.4158435 -4.3918071 -4.3785529][-4.4172754 -4.4411864 -4.435132 -4.3910279 -4.3496714 -4.3286037 -4.32206 -4.3280144 -4.3628078 -4.413384 -4.4434381 -4.4376392 -4.4105887 -4.3852072 -4.3706098][-4.36679 -4.3909674 -4.3725257 -4.3112726 -4.2680988 -4.2583194 -4.2626867 -4.2690587 -4.3010416 -4.3613043 -4.4081082 -4.4154334 -4.3964062 -4.3805733 -4.3726721][-4.3518662 -4.3726726 -4.3448224 -4.2759094 -4.2297287 -4.2170177 -4.2133355 -4.2103047 -4.2364631 -4.3020406 -4.3585691 -4.3746448 -4.3642507 -4.3640003 -4.3716307][-4.3730912 -4.395534 -4.366879 -4.300734 -4.2473931 -4.2141433 -4.1819315 -4.1597629 -4.178165 -4.2408166 -4.2960896 -4.3174195 -4.3198042 -4.3379135 -4.362998][-4.4070382 -4.4385991 -4.4225359 -4.3715653 -4.3147063 -4.2550755 -4.1862078 -4.1382289 -4.1390195 -4.179625 -4.2186813 -4.242137 -4.2638178 -4.3003263 -4.3369875][-4.4124894 -4.4550452 -4.4600458 -4.4337764 -4.385355 -4.3128929 -4.2224817 -4.1527514 -4.125824 -4.1291885 -4.1435895 -4.1690607 -4.211359 -4.2596459 -4.2979307][-4.3819203 -4.4249821 -4.4480119 -4.4453864 -4.4120893 -4.3490934 -4.269196 -4.2022161 -4.1581883 -4.1260481 -4.1114621 -4.1293511 -4.1811714 -4.2332282 -4.2654839][-4.3153262 -4.3541446 -4.394453 -4.4148812 -4.4009738 -4.3587542 -4.3074327 -4.2610731 -4.2173634 -4.1735463 -4.1447816 -4.1522059 -4.1987429 -4.2430778 -4.2618604][-4.2486334 -4.2808018 -4.3333626 -4.3702364 -4.3661084 -4.336 -4.3082829 -4.283659 -4.2526288 -4.2205777 -4.2016716 -4.2149539 -4.260972 -4.30001 -4.3069019][-4.2099295 -4.2328496 -4.2867289 -4.3273239 -4.3237772 -4.3003774 -4.2881618 -4.2806439 -4.2652187 -4.2517405 -4.2527065 -4.2806354 -4.3316231 -4.3711972 -4.3769913]]...]
INFO - root - 2017-12-07 21:32:56.853304: step 50510, loss = 21.40, batch loss = 21.32 (8.3 examples/sec; 0.959 sec/batch; 75h:06m:49s remains)
INFO - root - 2017-12-07 21:33:06.265096: step 50520, loss = 21.34, batch loss = 21.25 (8.4 examples/sec; 0.948 sec/batch; 74h:17m:07s remains)
INFO - root - 2017-12-07 21:33:15.760774: step 50530, loss = 21.29, batch loss = 21.21 (8.7 examples/sec; 0.917 sec/batch; 71h:49m:29s remains)
INFO - root - 2017-12-07 21:33:24.987410: step 50540, loss = 21.52, batch loss = 21.43 (10.3 examples/sec; 0.776 sec/batch; 60h:47m:55s remains)
INFO - root - 2017-12-07 21:33:34.340169: step 50550, loss = 22.03, batch loss = 21.95 (9.0 examples/sec; 0.891 sec/batch; 69h:44m:47s remains)
INFO - root - 2017-12-07 21:33:43.849412: step 50560, loss = 21.38, batch loss = 21.30 (8.2 examples/sec; 0.975 sec/batch; 76h:23m:12s remains)
INFO - root - 2017-12-07 21:33:53.200224: step 50570, loss = 21.43, batch loss = 21.35 (7.6 examples/sec; 1.048 sec/batch; 82h:06m:07s remains)
INFO - root - 2017-12-07 21:34:02.466145: step 50580, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.950 sec/batch; 74h:22m:21s remains)
INFO - root - 2017-12-07 21:34:11.729088: step 50590, loss = 21.66, batch loss = 21.58 (8.1 examples/sec; 0.985 sec/batch; 77h:07m:09s remains)
INFO - root - 2017-12-07 21:34:21.065056: step 50600, loss = 21.84, batch loss = 21.76 (7.8 examples/sec; 1.020 sec/batch; 79h:51m:28s remains)
2017-12-07 21:34:22.070491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6432896 -4.6934705 -4.7544351 -4.8231268 -4.8554745 -4.8011189 -4.6643677 -4.5356503 -4.5108309 -4.5718703 -4.6507292 -4.705575 -4.7404027 -4.7583494 -4.7482481][-4.749362 -4.8015695 -4.8490868 -4.8921285 -4.8910403 -4.7971144 -4.6278119 -4.4935284 -4.4801908 -4.5469141 -4.6298289 -4.70145 -4.7541432 -4.7882929 -4.7956104][-4.8186703 -4.8567472 -4.8712888 -4.8757987 -4.8400736 -4.7186823 -4.5393968 -4.4235516 -4.4440293 -4.5355654 -4.6258759 -4.6982207 -4.7454586 -4.7781987 -4.7994661][-4.8043046 -4.8332133 -4.827517 -4.805655 -4.7325044 -4.5680795 -4.3636193 -4.262609 -4.3286748 -4.4699159 -4.5863528 -4.6524839 -4.6793771 -4.7053089 -4.7490015][-4.7690015 -4.7915258 -4.7777019 -4.7403569 -4.6210647 -4.388845 -4.1331105 -4.0324006 -4.1509938 -4.3560605 -4.5153461 -4.5874853 -4.60553 -4.6297126 -4.690156][-4.7526708 -4.7587757 -4.7290306 -4.669292 -4.4991994 -4.1911054 -3.8737607 -3.7697761 -3.9429505 -4.2105927 -4.4193497 -4.5232148 -4.5633526 -4.59876 -4.658112][-4.7653389 -4.7388921 -4.6717582 -4.5739579 -4.3562856 -3.9880159 -3.6211183 -3.5130033 -3.7276716 -4.044281 -4.3036275 -4.4674392 -4.5608411 -4.6150093 -4.6541495][-4.7836843 -4.7242928 -4.6194377 -4.484551 -4.2362719 -3.8479052 -3.4727411 -3.3724718 -3.6022818 -3.9353352 -4.2255373 -4.4384365 -4.5744448 -4.6367145 -4.6480389][-4.7917433 -4.7277441 -4.614152 -4.4651861 -4.2197514 -3.8698647 -3.5493968 -3.4668293 -3.6597297 -3.9543982 -4.2266359 -4.43753 -4.5750785 -4.6360912 -4.6355543][-4.8052053 -4.757822 -4.6683288 -4.5374522 -4.3274236 -4.0535908 -3.8179817 -3.7495213 -3.8719134 -4.0901046 -4.3076854 -4.4779577 -4.5887256 -4.6485834 -4.6607041][-4.8087187 -4.7794542 -4.7252622 -4.6310539 -4.4736881 -4.2865686 -4.1397452 -4.0928788 -4.1571331 -4.2948594 -4.4430661 -4.5545411 -4.6294336 -4.6941705 -4.7330303][-4.8131533 -4.7761436 -4.7357483 -4.6729774 -4.5698681 -4.4593916 -4.38876 -4.3715644 -4.406981 -4.4815359 -4.5618744 -4.6240439 -4.6797261 -4.7543592 -4.814158][-4.8380342 -4.7806282 -4.7251248 -4.6693239 -4.6029282 -4.544136 -4.52046 -4.5269275 -4.5530605 -4.5912118 -4.6265063 -4.6594696 -4.7022943 -4.7744536 -4.8339734][-4.8831367 -4.8220749 -4.7483006 -4.6861291 -4.6400928 -4.6084905 -4.5973973 -4.5984931 -4.6102185 -4.6276574 -4.6391926 -4.6503167 -4.6717978 -4.71707 -4.753993][-4.8861065 -4.8441124 -4.7752681 -4.7150679 -4.6822166 -4.6673183 -4.6532145 -4.6299715 -4.6185579 -4.6223974 -4.6209774 -4.6124406 -4.600719 -4.603878 -4.6074681]]...]
INFO - root - 2017-12-07 21:34:31.432769: step 50610, loss = 21.21, batch loss = 21.13 (8.8 examples/sec; 0.914 sec/batch; 71h:31m:58s remains)
INFO - root - 2017-12-07 21:34:40.815793: step 50620, loss = 21.23, batch loss = 21.15 (9.0 examples/sec; 0.888 sec/batch; 69h:30m:40s remains)
INFO - root - 2017-12-07 21:34:50.256325: step 50630, loss = 21.09, batch loss = 21.00 (8.3 examples/sec; 0.966 sec/batch; 75h:39m:16s remains)
INFO - root - 2017-12-07 21:34:59.672171: step 50640, loss = 22.07, batch loss = 21.99 (8.5 examples/sec; 0.937 sec/batch; 73h:22m:29s remains)
INFO - root - 2017-12-07 21:35:08.933768: step 50650, loss = 21.50, batch loss = 21.42 (8.6 examples/sec; 0.931 sec/batch; 72h:53m:27s remains)
INFO - root - 2017-12-07 21:35:18.305665: step 50660, loss = 21.75, batch loss = 21.67 (8.8 examples/sec; 0.909 sec/batch; 71h:11m:57s remains)
INFO - root - 2017-12-07 21:35:27.596105: step 50670, loss = 21.23, batch loss = 21.14 (8.6 examples/sec; 0.930 sec/batch; 72h:49m:24s remains)
INFO - root - 2017-12-07 21:35:37.013476: step 50680, loss = 21.10, batch loss = 21.01 (8.4 examples/sec; 0.954 sec/batch; 74h:42m:53s remains)
INFO - root - 2017-12-07 21:35:46.466392: step 50690, loss = 20.90, batch loss = 20.81 (9.1 examples/sec; 0.879 sec/batch; 68h:49m:35s remains)
INFO - root - 2017-12-07 21:35:55.878486: step 50700, loss = 21.50, batch loss = 21.42 (8.6 examples/sec; 0.934 sec/batch; 73h:07m:47s remains)
2017-12-07 21:35:56.831769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4688659 -4.3690438 -4.247961 -4.1589293 -4.1032114 -4.1223655 -4.2371445 -4.3678188 -4.4334588 -4.4282718 -4.3568692 -4.2444386 -4.1493959 -4.10717 -4.1404948][-4.4373326 -4.3622227 -4.2715006 -4.21399 -4.1728477 -4.1784186 -4.2733603 -4.3893228 -4.432354 -4.4106307 -4.339643 -4.227572 -4.1264763 -4.0736055 -4.100431][-4.3834205 -4.3228927 -4.261754 -4.2424531 -4.2299085 -4.2285109 -4.2968907 -4.3932352 -4.4137321 -4.3812275 -4.3295765 -4.2482786 -4.1681776 -4.1246481 -4.1530786][-4.3151927 -4.2659225 -4.2290521 -4.2426925 -4.2634048 -4.2577319 -4.2890911 -4.3556919 -4.3617744 -4.3322177 -4.3144531 -4.282444 -4.2400756 -4.2176347 -4.2513227][-4.2598429 -4.2181792 -4.1916928 -4.2198453 -4.2605839 -4.2493992 -4.2353315 -4.263783 -4.2712355 -4.2682295 -4.2949772 -4.3094621 -4.2965922 -4.2918129 -4.3331614][-4.240417 -4.1918683 -4.1535568 -4.1699333 -4.209785 -4.1884513 -4.1273394 -4.1160879 -4.1408215 -4.1880794 -4.2665067 -4.3151903 -4.3204093 -4.3282351 -4.3731942][-4.2769804 -4.2162642 -4.1500258 -4.1312513 -4.1453776 -4.0988436 -3.987143 -3.9367082 -3.9865644 -4.0991917 -4.2331243 -4.3120217 -4.3315749 -4.3457713 -4.3800755][-4.3771968 -4.3128843 -4.2155042 -4.1458182 -4.1061864 -4.0124016 -3.8573856 -3.7819209 -3.8550055 -4.0216012 -4.2000971 -4.3047128 -4.3383183 -4.3528361 -4.3631139][-4.4803791 -4.42814 -4.3147159 -4.2012811 -4.1010642 -3.9677279 -3.8048177 -3.7320678 -3.8109021 -3.9830561 -4.1643596 -4.2773089 -4.3203382 -4.3314643 -4.3239994][-4.5297165 -4.5016236 -4.4026728 -4.275322 -4.140666 -4.0040135 -3.8826883 -3.840199 -3.9029434 -4.0262265 -4.1534438 -4.2299671 -4.2546163 -4.2556348 -4.259469][-4.5066929 -4.4990883 -4.4292603 -4.3193007 -4.1938028 -4.0992489 -4.0496535 -4.0422106 -4.0781307 -4.127924 -4.1662359 -4.1664405 -4.1393523 -4.1252346 -4.1628113][-4.4079857 -4.4051518 -4.3629823 -4.2893658 -4.2077432 -4.1789327 -4.1960649 -4.2166338 -4.2315803 -4.2197847 -4.1794586 -4.1026678 -4.0090671 -3.963551 -4.026762][-4.280695 -4.2694016 -4.2406869 -4.19865 -4.168292 -4.2061977 -4.2799816 -4.3302941 -4.34459 -4.2994852 -4.2139978 -4.086484 -3.9268544 -3.8273628 -3.879508][-4.1872997 -4.154367 -4.1145945 -4.0806365 -4.0821443 -4.1635 -4.2744932 -4.3565841 -4.3931279 -4.353322 -4.2709804 -4.1418767 -3.947917 -3.7981391 -3.8173611][-4.1670113 -4.11602 -4.0647736 -4.0206432 -4.0191145 -4.0980167 -4.203969 -4.2972054 -4.3660612 -4.3705606 -4.3371119 -4.2472434 -4.0631018 -3.9041383 -3.9010439]]...]
INFO - root - 2017-12-07 21:36:06.273008: step 50710, loss = 21.73, batch loss = 21.65 (8.6 examples/sec; 0.926 sec/batch; 72h:28m:46s remains)
INFO - root - 2017-12-07 21:36:15.589372: step 50720, loss = 21.66, batch loss = 21.58 (8.6 examples/sec; 0.930 sec/batch; 72h:48m:34s remains)
INFO - root - 2017-12-07 21:36:24.895997: step 50730, loss = 21.21, batch loss = 21.12 (8.2 examples/sec; 0.977 sec/batch; 76h:28m:36s remains)
INFO - root - 2017-12-07 21:36:34.151477: step 50740, loss = 21.41, batch loss = 21.33 (8.2 examples/sec; 0.977 sec/batch; 76h:29m:25s remains)
INFO - root - 2017-12-07 21:36:43.499416: step 50750, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.962 sec/batch; 75h:18m:08s remains)
INFO - root - 2017-12-07 21:36:52.862968: step 50760, loss = 21.36, batch loss = 21.28 (9.3 examples/sec; 0.860 sec/batch; 67h:17m:29s remains)
INFO - root - 2017-12-07 21:37:02.277954: step 50770, loss = 21.42, batch loss = 21.34 (8.5 examples/sec; 0.940 sec/batch; 73h:32m:59s remains)
INFO - root - 2017-12-07 21:37:11.687955: step 50780, loss = 21.32, batch loss = 21.24 (8.8 examples/sec; 0.908 sec/batch; 71h:04m:11s remains)
INFO - root - 2017-12-07 21:37:21.148024: step 50790, loss = 21.35, batch loss = 21.27 (8.6 examples/sec; 0.928 sec/batch; 72h:37m:27s remains)
INFO - root - 2017-12-07 21:37:30.586493: step 50800, loss = 21.17, batch loss = 21.09 (8.6 examples/sec; 0.935 sec/batch; 73h:08m:00s remains)
2017-12-07 21:37:31.526834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.76307 -4.7616224 -4.7735724 -4.8215857 -4.8610635 -4.8537645 -4.8085918 -4.7696795 -4.7655134 -4.7903404 -4.8152566 -4.8315721 -4.8342061 -4.816514 -4.7761345][-4.7177734 -4.6714745 -4.6655364 -4.753655 -4.8687077 -4.9116297 -4.87277 -4.8316641 -4.8499441 -4.9163704 -4.9660659 -4.9809017 -4.9612837 -4.9162951 -4.8513927][-4.5680366 -4.4716992 -4.4688582 -4.622725 -4.8230128 -4.9001012 -4.8367882 -4.76987 -4.8109937 -4.9395857 -5.0370903 -5.0662107 -5.0303073 -4.9528885 -4.85749][-4.34829 -4.2292867 -4.2620168 -4.4792833 -4.7129369 -4.7604017 -4.6377254 -4.5404892 -4.6180015 -4.824615 -4.9800143 -5.035727 -5.0031891 -4.9073009 -4.7880912][-4.1176271 -4.0131364 -4.0989881 -4.3449931 -4.5330262 -4.48631 -4.2840457 -4.1530294 -4.258708 -4.5276937 -4.7362666 -4.8403773 -4.8502965 -4.7742648 -4.660768][-3.9561245 -3.8904681 -4.0159111 -4.2384524 -4.3306751 -4.1749544 -3.9074798 -3.743989 -3.8438354 -4.136972 -4.394403 -4.5756531 -4.6595249 -4.6315732 -4.5582509][-3.9707994 -3.929112 -4.041223 -4.2005253 -4.2077022 -3.991744 -3.7005296 -3.5073388 -3.5698552 -3.8630626 -4.1758809 -4.4383974 -4.5707469 -4.5602689 -4.5146236][-4.1462035 -4.0824809 -4.1258912 -4.2106934 -4.1805415 -3.9715 -3.6832745 -3.4626024 -3.4941285 -3.7980032 -4.169302 -4.4661551 -4.5730762 -4.5223265 -4.4707117][-4.3301034 -4.2163777 -4.16894 -4.188189 -4.1647415 -4.0191178 -3.7809296 -3.57331 -3.6075187 -3.9235184 -4.3103604 -4.5646448 -4.5805626 -4.4602489 -4.38922][-4.4814091 -4.3299856 -4.2081089 -4.1709871 -4.1671457 -4.119101 -3.9804897 -3.8284283 -3.8662763 -4.1427422 -4.4718413 -4.6323934 -4.5486689 -4.3752384 -4.2930775][-4.61528 -4.4565721 -4.2988286 -4.222332 -4.2338066 -4.2773709 -4.245894 -4.1539536 -4.1671433 -4.3523397 -4.5767121 -4.6374164 -4.4917331 -4.3136525 -4.2497406][-4.7022324 -4.5604339 -4.4099474 -4.3342834 -4.361836 -4.4583788 -4.4970679 -4.4472771 -4.4247508 -4.510242 -4.6223273 -4.6022129 -4.4410281 -4.2988105 -4.2690506][-4.7411537 -4.6263394 -4.507338 -4.4605765 -4.5066071 -4.6093426 -4.6595716 -4.6221437 -4.5729294 -4.5842247 -4.6105671 -4.5499535 -4.4203815 -4.3361964 -4.3431735][-4.7187257 -4.6344829 -4.5548496 -4.5395775 -4.5888247 -4.659955 -4.681407 -4.6422677 -4.5929718 -4.5750504 -4.562263 -4.5075088 -4.4356756 -4.40996 -4.4438939][-4.6240568 -4.5746031 -4.53138 -4.5316429 -4.5651588 -4.5997357 -4.6011224 -4.5706277 -4.5364819 -4.5181074 -4.5043621 -4.4772687 -4.4519539 -4.4582095 -4.4968181]]...]
INFO - root - 2017-12-07 21:37:40.950581: step 50810, loss = 21.64, batch loss = 21.56 (8.0 examples/sec; 0.995 sec/batch; 77h:52m:21s remains)
INFO - root - 2017-12-07 21:37:50.460171: step 50820, loss = 21.17, batch loss = 21.09 (8.3 examples/sec; 0.962 sec/batch; 75h:15m:26s remains)
INFO - root - 2017-12-07 21:37:59.877751: step 50830, loss = 21.51, batch loss = 21.43 (9.1 examples/sec; 0.877 sec/batch; 68h:38m:18s remains)
INFO - root - 2017-12-07 21:38:09.361360: step 50840, loss = 21.28, batch loss = 21.20 (8.9 examples/sec; 0.897 sec/batch; 70h:09m:01s remains)
INFO - root - 2017-12-07 21:38:18.726261: step 50850, loss = 21.38, batch loss = 21.30 (10.1 examples/sec; 0.795 sec/batch; 62h:11m:33s remains)
INFO - root - 2017-12-07 21:38:28.266763: step 50860, loss = 21.67, batch loss = 21.58 (8.1 examples/sec; 0.989 sec/batch; 77h:21m:59s remains)
INFO - root - 2017-12-07 21:38:37.537029: step 50870, loss = 21.39, batch loss = 21.31 (8.2 examples/sec; 0.970 sec/batch; 75h:55m:05s remains)
INFO - root - 2017-12-07 21:38:46.907992: step 50880, loss = 21.28, batch loss = 21.20 (8.8 examples/sec; 0.913 sec/batch; 71h:26m:37s remains)
INFO - root - 2017-12-07 21:38:56.281085: step 50890, loss = 21.36, batch loss = 21.28 (8.7 examples/sec; 0.916 sec/batch; 71h:39m:55s remains)
INFO - root - 2017-12-07 21:39:05.675776: step 50900, loss = 21.24, batch loss = 21.16 (8.7 examples/sec; 0.918 sec/batch; 71h:48m:09s remains)
2017-12-07 21:39:06.636418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4131117 -4.2825031 -4.2293973 -4.281003 -4.4043164 -4.5052209 -4.57645 -4.5958543 -4.5235577 -4.4089427 -4.3089876 -4.2672424 -4.2738171 -4.2841697 -4.2993412][-4.4216914 -4.3098674 -4.2725139 -4.3342957 -4.4553709 -4.548533 -4.6167603 -4.6410847 -4.5701561 -4.4475603 -4.3322024 -4.28502 -4.3040266 -4.3274045 -4.3329124][-4.4597778 -4.3849621 -4.3687773 -4.4259071 -4.5062261 -4.5512533 -4.5937371 -4.6222568 -4.5729427 -4.4739547 -4.3729143 -4.3329592 -4.3600264 -4.3841186 -4.3719978][-4.4972219 -4.456542 -4.4474454 -4.4740181 -4.4802618 -4.4484138 -4.4492049 -4.4849424 -4.4787021 -4.4370522 -4.3844681 -4.3673477 -4.3989921 -4.4146228 -4.3884549][-4.4978161 -4.4788885 -4.4592371 -4.4309835 -4.3362956 -4.2050886 -4.1520357 -4.1987214 -4.25707 -4.3083091 -4.3352995 -4.35532 -4.3971772 -4.4099746 -4.3814545][-4.4555445 -4.4370241 -4.3950448 -4.3067694 -4.1165361 -3.8902955 -3.7832813 -3.8398218 -3.9708402 -4.1330457 -4.2492037 -4.3084974 -4.3662276 -4.3859215 -4.36667][-4.3858571 -4.3417439 -4.2757716 -4.1588445 -3.9260974 -3.6473205 -3.494396 -3.5418758 -3.7230666 -3.9728162 -4.1607008 -4.25167 -4.327179 -4.3592315 -4.3477][-4.315537 -4.2370057 -4.1606145 -4.0609646 -3.8611364 -3.5926137 -3.4078698 -3.41474 -3.5928082 -3.870527 -4.09452 -4.209878 -4.297051 -4.3366175 -4.322535][-4.2951097 -4.1901488 -4.1170774 -4.0621929 -3.9468579 -3.7446172 -3.563525 -3.5288084 -3.6614652 -3.9004459 -4.1127043 -4.2362313 -4.3227615 -4.3574815 -4.3315268][-4.3301682 -4.2194095 -4.1585197 -4.1487417 -4.1260247 -4.011436 -3.8639059 -3.8103333 -3.8850682 -4.0480623 -4.21675 -4.3345761 -4.4110661 -4.4339828 -4.3968697][-4.3523984 -4.25087 -4.2164683 -4.251442 -4.3085179 -4.2705469 -4.1618347 -4.1066217 -4.1321745 -4.2177806 -4.334065 -4.4357891 -4.5018539 -4.5210543 -4.4853315][-4.3482184 -4.2531109 -4.2397084 -4.302639 -4.4079905 -4.4233804 -4.3494015 -4.3008595 -4.2923183 -4.317975 -4.3855772 -4.4653707 -4.5250297 -4.5531912 -4.5333223][-4.3558416 -4.2698874 -4.2565265 -4.3112288 -4.4167595 -4.45333 -4.4046793 -4.3620219 -4.3356705 -4.3303356 -4.3652306 -4.4258785 -4.4825854 -4.5217009 -4.5196996][-4.3942866 -4.3162217 -4.2828012 -4.3003845 -4.3739109 -4.4104595 -4.38006 -4.3438926 -4.3151093 -4.3053184 -4.3280139 -4.375957 -4.4275236 -4.4658132 -4.4714484][-4.416429 -4.3369961 -4.2772074 -4.2597604 -4.3066711 -4.3476338 -4.3399639 -4.3198462 -4.3053789 -4.3103442 -4.3359227 -4.3763556 -4.4186726 -4.4451132 -4.448667]]...]
INFO - root - 2017-12-07 21:39:16.031877: step 50910, loss = 21.11, batch loss = 21.03 (10.3 examples/sec; 0.774 sec/batch; 60h:32m:09s remains)
INFO - root - 2017-12-07 21:39:25.454865: step 50920, loss = 21.23, batch loss = 21.15 (8.4 examples/sec; 0.953 sec/batch; 74h:30m:11s remains)
INFO - root - 2017-12-07 21:39:34.927070: step 50930, loss = 21.63, batch loss = 21.55 (8.3 examples/sec; 0.960 sec/batch; 75h:06m:54s remains)
INFO - root - 2017-12-07 21:39:44.298493: step 50940, loss = 21.55, batch loss = 21.46 (9.0 examples/sec; 0.889 sec/batch; 69h:33m:53s remains)
INFO - root - 2017-12-07 21:39:53.658384: step 50950, loss = 21.73, batch loss = 21.64 (8.9 examples/sec; 0.903 sec/batch; 70h:35m:36s remains)
INFO - root - 2017-12-07 21:40:03.123982: step 50960, loss = 22.00, batch loss = 21.92 (8.8 examples/sec; 0.907 sec/batch; 70h:55m:59s remains)
INFO - root - 2017-12-07 21:40:12.467734: step 50970, loss = 21.62, batch loss = 21.54 (8.4 examples/sec; 0.947 sec/batch; 74h:04m:16s remains)
INFO - root - 2017-12-07 21:40:21.917494: step 50980, loss = 21.16, batch loss = 21.08 (8.5 examples/sec; 0.937 sec/batch; 73h:16m:16s remains)
INFO - root - 2017-12-07 21:40:31.185851: step 50990, loss = 21.21, batch loss = 21.13 (8.4 examples/sec; 0.955 sec/batch; 74h:39m:10s remains)
INFO - root - 2017-12-07 21:40:40.494826: step 51000, loss = 21.36, batch loss = 21.28 (8.7 examples/sec; 0.925 sec/batch; 72h:18m:52s remains)
2017-12-07 21:40:41.462476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4678831 -4.4806724 -4.4972277 -4.5302663 -4.5714045 -4.5901465 -4.5821848 -4.5784388 -4.6124926 -4.6732974 -4.721806 -4.7334261 -4.6990843 -4.6208658 -4.5162139][-4.5305691 -4.5410104 -4.5527296 -4.5863314 -4.6269779 -4.6354318 -4.6097522 -4.6005621 -4.6619515 -4.7641544 -4.8435082 -4.8590851 -4.8054295 -4.6981735 -4.5645528][-4.5681939 -4.5711417 -4.5736027 -4.6027522 -4.6349616 -4.6204944 -4.5598364 -4.5304022 -4.6146388 -4.7611876 -4.8840642 -4.9237781 -4.8661413 -4.7393594 -4.5871382][-4.6071162 -4.5952592 -4.5753632 -4.5830183 -4.5832415 -4.5198278 -4.3983812 -4.3236003 -4.4104795 -4.5917363 -4.7634859 -4.8592591 -4.8356767 -4.7200074 -4.5718784][-4.6245441 -4.5951552 -4.5522428 -4.5359769 -4.4944077 -4.37132 -4.1745429 -4.0245595 -4.0776143 -4.2661347 -4.4761724 -4.6501904 -4.7080622 -4.6478009 -4.5329895][-4.5760379 -4.533083 -4.4801345 -4.4557934 -4.3839607 -4.2108383 -3.9429119 -3.7044263 -3.7014914 -3.8858433 -4.1244726 -4.3793964 -4.5432215 -4.565155 -4.4971189][-4.4925189 -4.4419122 -4.4040341 -4.3969507 -4.3175373 -4.1113119 -3.7978637 -3.5102832 -3.4889653 -3.6967266 -3.9634144 -4.2580404 -4.4806204 -4.5432916 -4.4892821][-4.3814931 -4.3485651 -4.3541503 -4.387537 -4.332778 -4.1375046 -3.8376584 -3.5732355 -3.5827241 -3.8293855 -4.1118574 -4.3844671 -4.5752969 -4.6008797 -4.5094891][-4.2853847 -4.290216 -4.3451838 -4.4190555 -4.4075675 -4.2590795 -4.0237923 -3.8332405 -3.87977 -4.1281404 -4.3830519 -4.5842619 -4.6924353 -4.6556768 -4.5220594][-4.2890677 -4.3060617 -4.3829331 -4.4692426 -4.4785209 -4.3681312 -4.1963482 -4.0796404 -4.1496387 -4.3712916 -4.5795622 -4.7095284 -4.7446108 -4.6671138 -4.5180631][-4.37241 -4.3561163 -4.4109139 -4.4832249 -4.4959793 -4.4187984 -4.3009019 -4.2307153 -4.2978387 -4.4781981 -4.6471763 -4.7370071 -4.7347908 -4.6461887 -4.5069838][-4.4709516 -4.4198375 -4.4392276 -4.4904141 -4.5076241 -4.4724193 -4.4120331 -4.3712988 -4.41192 -4.5281768 -4.6520886 -4.7165976 -4.6984344 -4.6133232 -4.4945297][-4.5533419 -4.49675 -4.4887462 -4.514482 -4.5309868 -4.5341945 -4.530458 -4.5224724 -4.5434585 -4.600894 -4.6717091 -4.7067938 -4.6737309 -4.5895662 -4.4855256][-4.5840816 -4.5426245 -4.521615 -4.5233879 -4.5286665 -4.5453706 -4.5744386 -4.5971885 -4.6182308 -4.6446767 -4.6726518 -4.6757703 -4.6306806 -4.550561 -4.4606867][-4.5555539 -4.5367508 -4.52254 -4.517662 -4.5159221 -4.5275555 -4.5564041 -4.5852509 -4.6069207 -4.6197782 -4.6205215 -4.5989194 -4.547739 -4.4792919 -4.4118743]]...]
INFO - root - 2017-12-07 21:40:50.794744: step 51010, loss = 21.66, batch loss = 21.57 (8.0 examples/sec; 1.002 sec/batch; 78h:21m:34s remains)
INFO - root - 2017-12-07 21:41:00.306904: step 51020, loss = 21.23, batch loss = 21.15 (8.0 examples/sec; 1.006 sec/batch; 78h:37m:49s remains)
INFO - root - 2017-12-07 21:41:09.657344: step 51030, loss = 21.10, batch loss = 21.02 (8.7 examples/sec; 0.918 sec/batch; 71h:45m:30s remains)
INFO - root - 2017-12-07 21:41:19.020433: step 51040, loss = 21.24, batch loss = 21.16 (8.3 examples/sec; 0.963 sec/batch; 75h:17m:16s remains)
INFO - root - 2017-12-07 21:41:28.466744: step 51050, loss = 21.79, batch loss = 21.70 (8.6 examples/sec; 0.927 sec/batch; 72h:29m:44s remains)
INFO - root - 2017-12-07 21:41:37.767326: step 51060, loss = 21.29, batch loss = 21.21 (8.2 examples/sec; 0.975 sec/batch; 76h:14m:07s remains)
INFO - root - 2017-12-07 21:41:47.248876: step 51070, loss = 21.41, batch loss = 21.33 (8.4 examples/sec; 0.947 sec/batch; 74h:03m:34s remains)
INFO - root - 2017-12-07 21:41:56.551778: step 51080, loss = 21.10, batch loss = 21.01 (9.2 examples/sec; 0.868 sec/batch; 67h:49m:28s remains)
INFO - root - 2017-12-07 21:42:05.924911: step 51090, loss = 21.48, batch loss = 21.40 (9.0 examples/sec; 0.888 sec/batch; 69h:26m:37s remains)
INFO - root - 2017-12-07 21:42:15.282826: step 51100, loss = 21.63, batch loss = 21.55 (8.5 examples/sec; 0.943 sec/batch; 73h:41m:01s remains)
2017-12-07 21:42:16.231472: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5375562 -4.5291519 -4.4198456 -4.2592015 -4.1421814 -4.1022568 -4.1361618 -4.2179117 -4.3042803 -4.3616652 -4.3889647 -4.3927426 -4.3928156 -4.3688269 -4.3134627][-4.5453925 -4.541193 -4.4456582 -4.3130426 -4.2195959 -4.1847959 -4.2081637 -4.2660179 -4.3231654 -4.3708649 -4.405292 -4.4140363 -4.4096966 -4.3836608 -4.334538][-4.5105977 -4.507184 -4.4327245 -4.3354807 -4.2684035 -4.2483869 -4.2739644 -4.3105474 -4.3300991 -4.3609395 -4.4065566 -4.431859 -4.4290586 -4.3986063 -4.3490663][-4.4487519 -4.4313545 -4.3695483 -4.30447 -4.26825 -4.2712975 -4.2964787 -4.2982078 -4.2723565 -4.2868128 -4.3485107 -4.3949122 -4.3996696 -4.3727212 -4.3328738][-4.3648806 -4.32376 -4.27272 -4.2418694 -4.2365332 -4.2510452 -4.2529445 -4.2065411 -4.1515617 -4.1740842 -4.2631927 -4.3287754 -4.340085 -4.3196621 -4.2932997][-4.2897763 -4.2460179 -4.2221503 -4.218123 -4.2169762 -4.2009554 -4.1397219 -4.0398178 -3.9846568 -4.0444465 -4.1649556 -4.2431936 -4.2606492 -4.2501011 -4.2376623][-4.252492 -4.2377892 -4.2502794 -4.2562757 -4.2267442 -4.14834 -4.0101848 -3.8634679 -3.8188281 -3.914912 -4.0593839 -4.1464691 -4.1727095 -4.1746454 -4.1724744][-4.2558541 -4.2873735 -4.339191 -4.3500428 -4.2884412 -4.160594 -3.9786522 -3.8139193 -3.7759309 -3.8790712 -4.0214419 -4.1086216 -4.1456785 -4.1573811 -4.1564889][-4.2916446 -4.3678436 -4.4552894 -4.4697537 -4.3907704 -4.2502527 -4.0730982 -3.9221916 -3.8783207 -3.9436579 -4.044816 -4.1147442 -4.1595082 -4.185194 -4.1889625][-4.3577137 -4.4532866 -4.5562091 -4.5751286 -4.5055962 -4.3906584 -4.2474189 -4.1178179 -4.052732 -4.0548639 -4.0908465 -4.1264167 -4.175456 -4.2254753 -4.2487326][-4.4534578 -4.536067 -4.6319757 -4.6602311 -4.6254282 -4.5574346 -4.4524188 -4.3395977 -4.2546926 -4.2073426 -4.1899614 -4.1914282 -4.2385349 -4.3123546 -4.3588147][-4.4883661 -4.5452328 -4.6262827 -4.6686568 -4.6736164 -4.6520205 -4.584856 -4.4903622 -4.3987117 -4.3324924 -4.2929664 -4.2741961 -4.3115773 -4.3938451 -4.4621739][-4.4523139 -4.4818616 -4.5439835 -4.5950522 -4.627399 -4.6395035 -4.6062012 -4.5335741 -4.4494271 -4.3879223 -4.3515048 -4.3280487 -4.349997 -4.4213452 -4.49677][-4.4365292 -4.4470091 -4.4900966 -4.5406547 -4.58546 -4.6131649 -4.6019087 -4.5520158 -4.487617 -4.4449439 -4.4244118 -4.4060364 -4.4084039 -4.4469867 -4.5030351][-4.4622426 -4.4623938 -4.4870186 -4.5274272 -4.5711842 -4.6020684 -4.60159 -4.568881 -4.5250258 -4.5003991 -4.4913459 -4.474031 -4.45851 -4.4672084 -4.5017304]]...]
INFO - root - 2017-12-07 21:42:25.672734: step 51110, loss = 21.66, batch loss = 21.58 (8.7 examples/sec; 0.917 sec/batch; 71h:39m:09s remains)
INFO - root - 2017-12-07 21:42:35.076853: step 51120, loss = 21.50, batch loss = 21.42 (8.6 examples/sec; 0.928 sec/batch; 72h:31m:09s remains)
INFO - root - 2017-12-07 21:42:44.525878: step 51130, loss = 21.51, batch loss = 21.43 (8.7 examples/sec; 0.915 sec/batch; 71h:29m:40s remains)
INFO - root - 2017-12-07 21:42:53.976772: step 51140, loss = 21.11, batch loss = 21.02 (8.6 examples/sec; 0.932 sec/batch; 72h:50m:46s remains)
INFO - root - 2017-12-07 21:43:03.509475: step 51150, loss = 21.79, batch loss = 21.71 (8.2 examples/sec; 0.971 sec/batch; 75h:54m:00s remains)
INFO - root - 2017-12-07 21:43:13.009795: step 51160, loss = 21.56, batch loss = 21.48 (7.8 examples/sec; 1.024 sec/batch; 80h:01m:12s remains)
INFO - root - 2017-12-07 21:43:22.577381: step 51170, loss = 21.37, batch loss = 21.28 (8.6 examples/sec; 0.932 sec/batch; 72h:51m:09s remains)
INFO - root - 2017-12-07 21:43:32.084729: step 51180, loss = 21.96, batch loss = 21.88 (8.9 examples/sec; 0.903 sec/batch; 70h:32m:36s remains)
INFO - root - 2017-12-07 21:43:41.187290: step 51190, loss = 21.35, batch loss = 21.26 (8.7 examples/sec; 0.925 sec/batch; 72h:15m:35s remains)
INFO - root - 2017-12-07 21:43:50.577093: step 51200, loss = 21.64, batch loss = 21.56 (8.5 examples/sec; 0.939 sec/batch; 73h:21m:59s remains)
2017-12-07 21:43:51.488062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.557435 -4.5718713 -4.5770535 -4.5736217 -4.5638847 -4.553669 -4.5499878 -4.5518103 -4.5536513 -4.5532637 -4.5502739 -4.5427146 -4.5318656 -4.5188565 -4.5052071][-4.6425281 -4.6585526 -4.6607218 -4.6542907 -4.6394877 -4.6219525 -4.611465 -4.6129103 -4.6244597 -4.6396027 -4.6487193 -4.645123 -4.6308594 -4.6123567 -4.5947647][-4.6863031 -4.7021852 -4.699141 -4.6849384 -4.656652 -4.6202426 -4.5913386 -4.5896072 -4.6201272 -4.665091 -4.6946368 -4.6932235 -4.67052 -4.6448336 -4.6259203][-4.6989708 -4.7135081 -4.7034183 -4.6766591 -4.6258841 -4.5589318 -4.5010009 -4.4942913 -4.5536318 -4.6420569 -4.6994185 -4.699738 -4.6667261 -4.63506 -4.6200323][-4.6688852 -4.6833267 -4.6626539 -4.6127315 -4.5263815 -4.4187584 -4.328104 -4.3175855 -4.4112616 -4.5514278 -4.6462336 -4.6594772 -4.6260853 -4.5943103 -4.5853639][-4.6063652 -4.6218271 -4.5911727 -4.5150957 -4.3889651 -4.2358541 -4.1076045 -4.0830493 -4.1997981 -4.390676 -4.5377235 -4.5836906 -4.5629377 -4.5321479 -4.5207243][-4.5348115 -4.5507264 -4.51433 -4.42191 -4.2662525 -4.07201 -3.9004312 -3.8457844 -3.9650548 -4.1952248 -4.4024706 -4.4962344 -4.492404 -4.4501905 -4.4157839][-4.4591827 -4.4772234 -4.4420538 -4.3452229 -4.1759377 -3.9548826 -3.7528563 -3.6738498 -3.7902231 -4.043447 -4.2945995 -4.4230428 -4.4177203 -4.341712 -4.2677903][-4.407824 -4.43029 -4.4073639 -4.3205462 -4.158411 -3.9410779 -3.7436023 -3.6664686 -3.7795172 -4.0254879 -4.2772436 -4.4046617 -4.3764291 -4.2601728 -4.1538825][-4.3600845 -4.3894844 -4.3972912 -4.3429303 -4.2146025 -4.0340886 -3.8726385 -3.8131809 -3.9103012 -4.1110411 -4.3209472 -4.4216895 -4.3727865 -4.2384248 -4.1220713][-4.2583952 -4.2903957 -4.3354473 -4.3234897 -4.2420678 -4.1149244 -4.0021605 -3.963212 -4.0361748 -4.1792426 -4.3367381 -4.4177728 -4.3781438 -4.2620716 -4.1503296][-4.118989 -4.1395211 -4.207253 -4.23548 -4.2004356 -4.1246548 -4.0524564 -4.0274787 -4.0835395 -4.1923957 -4.3207879 -4.409214 -4.4134121 -4.3433914 -4.2373352][-4.0039754 -4.0033212 -4.072269 -4.1312122 -4.143414 -4.1145844 -4.0705771 -4.0499129 -4.0971942 -4.1934276 -4.3097143 -4.4148545 -4.4723969 -4.4587865 -4.3678012][-3.9529791 -3.9283266 -3.978452 -4.0525336 -4.1125312 -4.14146 -4.1356964 -4.1198521 -4.1480217 -4.219913 -4.3133035 -4.4195175 -4.5152097 -4.5540152 -4.4981017][-4.0144105 -3.965745 -3.9737477 -4.0333524 -4.1218529 -4.2054634 -4.2455082 -4.2424126 -4.2476687 -4.2842288 -4.3431206 -4.4326687 -4.542819 -4.6179471 -4.605041]]...]
INFO - root - 2017-12-07 21:44:00.850162: step 51210, loss = 21.17, batch loss = 21.09 (8.5 examples/sec; 0.940 sec/batch; 73h:27m:20s remains)
INFO - root - 2017-12-07 21:44:10.178591: step 51220, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.952 sec/batch; 74h:21m:44s remains)
INFO - root - 2017-12-07 21:44:19.511812: step 51230, loss = 21.69, batch loss = 21.61 (7.8 examples/sec; 1.029 sec/batch; 80h:21m:54s remains)
INFO - root - 2017-12-07 21:44:28.870548: step 51240, loss = 21.31, batch loss = 21.23 (8.5 examples/sec; 0.936 sec/batch; 73h:09m:10s remains)
INFO - root - 2017-12-07 21:44:38.412531: step 51250, loss = 21.64, batch loss = 21.56 (8.6 examples/sec; 0.929 sec/batch; 72h:34m:29s remains)
INFO - root - 2017-12-07 21:44:47.664374: step 51260, loss = 21.36, batch loss = 21.27 (9.6 examples/sec; 0.832 sec/batch; 65h:01m:03s remains)
INFO - root - 2017-12-07 21:44:57.163742: step 51270, loss = 21.32, batch loss = 21.23 (8.4 examples/sec; 0.956 sec/batch; 74h:39m:38s remains)
INFO - root - 2017-12-07 21:45:06.457098: step 51280, loss = 21.43, batch loss = 21.34 (8.4 examples/sec; 0.948 sec/batch; 74h:02m:01s remains)
INFO - root - 2017-12-07 21:45:15.775780: step 51290, loss = 21.74, batch loss = 21.66 (9.2 examples/sec; 0.870 sec/batch; 67h:56m:55s remains)
INFO - root - 2017-12-07 21:45:25.219327: step 51300, loss = 21.48, batch loss = 21.40 (9.0 examples/sec; 0.891 sec/batch; 69h:37m:59s remains)
2017-12-07 21:45:26.221549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4708157 -4.5414214 -4.6006808 -4.6365342 -4.6461444 -4.6344595 -4.6136956 -4.5969124 -4.5953035 -4.606689 -4.6169081 -4.6147866 -4.5815849 -4.5167284 -4.4434662][-4.5538545 -4.6574812 -4.7425041 -4.8013344 -4.8239722 -4.7993155 -4.7404079 -4.6897907 -4.6840339 -4.7219014 -4.7688394 -4.7895584 -4.7491236 -4.6470461 -4.5213847][-4.6021338 -4.7218189 -4.8191619 -4.897296 -4.9294114 -4.8757796 -4.757298 -4.6604424 -4.6505508 -4.7349319 -4.8461809 -4.9116411 -4.8796711 -4.7527003 -4.5850062][-4.6034937 -4.7185397 -4.8078547 -4.8807883 -4.8873806 -4.7720942 -4.5856447 -4.4576592 -4.4618959 -4.6068797 -4.7988238 -4.9245238 -4.9185591 -4.7884378 -4.603085][-4.5856934 -4.6816483 -4.7487617 -4.7885938 -4.7327623 -4.5347686 -4.2863841 -4.1494169 -4.183413 -4.3811908 -4.6471071 -4.8384624 -4.8709249 -4.7572126 -4.5796366][-4.5731792 -4.6383376 -4.6700335 -4.6605153 -4.533392 -4.2597017 -3.9589136 -3.812295 -3.8759782 -4.1070843 -4.4244318 -4.681839 -4.7630248 -4.6814704 -4.5346475][-4.5673904 -4.5905247 -4.5765953 -4.5181565 -4.3429456 -4.030376 -3.6987329 -3.5329018 -3.6057372 -3.8464203 -4.1823597 -4.4879479 -4.6206732 -4.5892658 -4.4917607][-4.5732 -4.5562205 -4.4999671 -4.4130864 -4.2390156 -3.9538698 -3.6380072 -3.4566741 -3.522938 -3.7656612 -4.0969114 -4.4094353 -4.5587468 -4.5549779 -4.4851642][-4.5707765 -4.5284014 -4.4556956 -4.3755198 -4.2466731 -4.0434308 -3.7945333 -3.6242933 -3.690769 -3.9493761 -4.267487 -4.5288358 -4.6306252 -4.6019611 -4.5107388][-4.5802493 -4.5460234 -4.4906144 -4.4329581 -4.3443336 -4.2201052 -4.0522428 -3.9085896 -3.9796569 -4.2563543 -4.5437145 -4.71945 -4.7465143 -4.6665516 -4.5295148][-4.6209784 -4.6328611 -4.6131196 -4.5624771 -4.4684896 -4.3726969 -4.2582965 -4.1395025 -4.2115808 -4.4932675 -4.7410169 -4.8375754 -4.8070121 -4.6880212 -4.5189023][-4.6369581 -4.6909037 -4.6964607 -4.6338463 -4.5072575 -4.4064302 -4.3279152 -4.2420163 -4.3119941 -4.5740876 -4.7828484 -4.8332281 -4.7819643 -4.6562052 -4.488553][-4.5765224 -4.6351566 -4.6477909 -4.584219 -4.4577475 -4.3754821 -4.3386946 -4.2916117 -4.3469205 -4.5554824 -4.7192721 -4.7509532 -4.7059422 -4.5987892 -4.4580131][-4.4889555 -4.5270061 -4.5416594 -4.506844 -4.4295959 -4.3933311 -4.3972039 -4.38046 -4.4106078 -4.5449581 -4.6553645 -4.672761 -4.6309652 -4.5415835 -4.4316983][-4.4236026 -4.4363332 -4.44586 -4.4394083 -4.414928 -4.4170208 -4.4413385 -4.4416671 -4.4509635 -4.5151124 -4.5710769 -4.5748954 -4.5384455 -4.471715 -4.3968511]]...]
INFO - root - 2017-12-07 21:45:35.576962: step 51310, loss = 21.25, batch loss = 21.16 (8.4 examples/sec; 0.948 sec/batch; 74h:01m:17s remains)
INFO - root - 2017-12-07 21:45:44.995139: step 51320, loss = 21.67, batch loss = 21.59 (9.2 examples/sec; 0.868 sec/batch; 67h:46m:52s remains)
INFO - root - 2017-12-07 21:45:54.488777: step 51330, loss = 21.39, batch loss = 21.31 (8.9 examples/sec; 0.900 sec/batch; 70h:19m:48s remains)
INFO - root - 2017-12-07 21:46:03.968202: step 51340, loss = 21.28, batch loss = 21.20 (8.1 examples/sec; 0.982 sec/batch; 76h:40m:09s remains)
INFO - root - 2017-12-07 21:46:13.450227: step 51350, loss = 21.45, batch loss = 21.37 (7.7 examples/sec; 1.045 sec/batch; 81h:37m:57s remains)
INFO - root - 2017-12-07 21:46:22.912073: step 51360, loss = 21.21, batch loss = 21.12 (7.9 examples/sec; 1.009 sec/batch; 78h:48m:30s remains)
INFO - root - 2017-12-07 21:46:32.362543: step 51370, loss = 20.93, batch loss = 20.85 (8.8 examples/sec; 0.909 sec/batch; 70h:57m:32s remains)
INFO - root - 2017-12-07 21:46:41.958203: step 51380, loss = 21.66, batch loss = 21.57 (8.4 examples/sec; 0.948 sec/batch; 74h:03m:12s remains)
INFO - root - 2017-12-07 21:46:51.398419: step 51390, loss = 21.34, batch loss = 21.26 (8.6 examples/sec; 0.934 sec/batch; 72h:54m:27s remains)
INFO - root - 2017-12-07 21:47:00.886896: step 51400, loss = 21.41, batch loss = 21.32 (8.7 examples/sec; 0.923 sec/batch; 72h:06m:05s remains)
2017-12-07 21:47:01.901149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.345758 -4.3637791 -4.3848381 -4.4038973 -4.4187322 -4.430479 -4.4380145 -4.4482455 -4.4610171 -4.4753942 -4.49043 -4.4978037 -4.4893351 -4.4635253 -4.4252748][-4.3700576 -4.402698 -4.4360127 -4.4630022 -4.4806151 -4.4874115 -4.4870844 -4.494843 -4.5128226 -4.5411215 -4.5771489 -4.6017661 -4.5964766 -4.5612025 -4.501647][-4.3624682 -4.4075251 -4.4501824 -4.4837546 -4.5008616 -4.491375 -4.46683 -4.4585872 -4.4715295 -4.5136924 -4.5820661 -4.6377311 -4.6496453 -4.6198421 -4.5544057][-4.3296762 -4.3840966 -4.430397 -4.4665971 -4.477706 -4.4379611 -4.3720284 -4.334866 -4.3323512 -4.3859224 -4.4930382 -4.5889864 -4.62753 -4.6161795 -4.5618215][-4.28571 -4.3473306 -4.390698 -4.4212551 -4.4185686 -4.3418131 -4.2338586 -4.1696153 -4.146987 -4.2037516 -4.3417583 -4.4763293 -4.54345 -4.5571928 -4.5258336][-4.2456651 -4.3043652 -4.3340125 -4.3473582 -4.3257608 -4.2176447 -4.0802169 -4.00021 -3.9618194 -4.0124846 -4.1662889 -4.3318396 -4.4281688 -4.4734206 -4.4739103][-4.2346849 -4.2843037 -4.2966781 -4.2903166 -4.2551327 -4.1324978 -3.9819584 -3.8945503 -3.8479924 -3.8896556 -4.0424385 -4.2226934 -4.3381586 -4.4096193 -4.4393058][-4.2564206 -4.29982 -4.3056479 -4.2912159 -4.2565193 -4.142745 -4.0016346 -3.9199622 -3.8773179 -3.9090807 -4.0413561 -4.208384 -4.3192391 -4.3951468 -4.4364033][-4.2800274 -4.3226337 -4.3287516 -4.3103681 -4.2756748 -4.1800194 -4.0678439 -4.0128603 -3.9952424 -4.0264564 -4.1296382 -4.263411 -4.3503828 -4.4116969 -4.4478617][-4.2935963 -4.3380756 -4.3483186 -4.3304844 -4.2962351 -4.2212024 -4.1464143 -4.1324387 -4.1526365 -4.1931033 -4.2668629 -4.3576307 -4.4083238 -4.4408507 -4.4585419][-4.2938676 -4.3368125 -4.3509736 -4.3376365 -4.3068414 -4.2526727 -4.2137632 -4.2369695 -4.2889972 -4.3371997 -4.3864655 -4.4380031 -4.4561911 -4.4617186 -4.4578419][-4.283391 -4.3172112 -4.3283768 -4.3155818 -4.287694 -4.2516789 -4.2416639 -4.2879887 -4.35309 -4.4008079 -4.433413 -4.4583321 -4.4589024 -4.4502559 -4.4343219][-4.2843666 -4.3030114 -4.30356 -4.2869253 -4.2627139 -4.2418671 -4.2500253 -4.3012395 -4.3614173 -4.4025297 -4.4234872 -4.4315939 -4.424006 -4.4103494 -4.3915462][-4.2964864 -4.30139 -4.2952223 -4.2806096 -4.2654281 -4.2580729 -4.2712493 -4.3093977 -4.3501186 -4.3788924 -4.3911743 -4.3904524 -4.3807087 -4.3668118 -4.349515][-4.3127584 -4.3135238 -4.3084254 -4.3002687 -4.293169 -4.2910957 -4.2988353 -4.3167338 -4.3352728 -4.3496571 -4.3561573 -4.3540931 -4.3467383 -4.3360524 -4.3229995]]...]
INFO - root - 2017-12-07 21:47:11.451891: step 51410, loss = 21.34, batch loss = 21.25 (8.8 examples/sec; 0.907 sec/batch; 70h:48m:31s remains)
INFO - root - 2017-12-07 21:47:20.922312: step 51420, loss = 21.59, batch loss = 21.51 (8.7 examples/sec; 0.922 sec/batch; 72h:00m:34s remains)
INFO - root - 2017-12-07 21:47:30.192581: step 51430, loss = 21.49, batch loss = 21.41 (9.0 examples/sec; 0.894 sec/batch; 69h:46m:17s remains)
INFO - root - 2017-12-07 21:47:39.807958: step 51440, loss = 21.59, batch loss = 21.51 (7.9 examples/sec; 1.008 sec/batch; 78h:42m:17s remains)
INFO - root - 2017-12-07 21:47:49.387900: step 51450, loss = 21.30, batch loss = 21.21 (8.0 examples/sec; 0.996 sec/batch; 77h:46m:12s remains)
INFO - root - 2017-12-07 21:47:58.689575: step 51460, loss = 21.45, batch loss = 21.37 (8.4 examples/sec; 0.950 sec/batch; 74h:09m:37s remains)
INFO - root - 2017-12-07 21:48:08.013056: step 51470, loss = 21.78, batch loss = 21.70 (8.5 examples/sec; 0.942 sec/batch; 73h:31m:32s remains)
INFO - root - 2017-12-07 21:48:17.423584: step 51480, loss = 21.57, batch loss = 21.49 (8.6 examples/sec; 0.932 sec/batch; 72h:43m:52s remains)
INFO - root - 2017-12-07 21:48:26.902746: step 51490, loss = 21.17, batch loss = 21.08 (8.4 examples/sec; 0.951 sec/batch; 74h:11m:49s remains)
INFO - root - 2017-12-07 21:48:36.181418: step 51500, loss = 21.24, batch loss = 21.15 (8.3 examples/sec; 0.969 sec/batch; 75h:37m:01s remains)
2017-12-07 21:48:37.080896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1795526 -4.2062874 -4.2473049 -4.266027 -4.2371793 -4.1990008 -4.2073593 -4.2640285 -4.320539 -4.3571124 -4.3658037 -4.3499551 -4.3347869 -4.2956691 -4.2345958][-4.1134777 -4.1319952 -4.1626291 -4.1794567 -4.1441574 -4.1071486 -4.1349869 -4.2059903 -4.2800579 -4.3399272 -4.3737612 -4.3860955 -4.3989105 -4.3800783 -4.3269167][-4.1241035 -4.1046286 -4.0822854 -4.059227 -3.9991655 -3.9555509 -4.002347 -4.101809 -4.2095394 -4.3081627 -4.3795543 -4.4266868 -4.4700303 -4.4766049 -4.4420013][-4.1640759 -4.0983372 -4.0100846 -3.9312408 -3.8361707 -3.7692962 -3.81794 -3.9447429 -4.0911093 -4.2349629 -4.3509011 -4.4343324 -4.5013413 -4.5237613 -4.5044165][-4.227818 -4.1218834 -3.9826498 -3.8540344 -3.7240353 -3.6238954 -3.6533189 -3.7841854 -3.9444633 -4.1101494 -4.2565126 -4.3697524 -4.4524322 -4.4840517 -4.4831119][-4.3022037 -4.1875958 -4.0404615 -3.8948069 -3.7452581 -3.6130855 -3.6034904 -3.7008817 -3.834245 -3.9837792 -4.1254373 -4.2390428 -4.3180504 -4.3529058 -4.3797131][-4.3675942 -4.2883277 -4.1837296 -4.0639133 -3.9187884 -3.7637894 -3.7047088 -3.739877 -3.8065257 -3.9053514 -4.0125976 -4.0983505 -4.1532135 -4.1807165 -4.2306809][-4.4197516 -4.4041433 -4.3675146 -4.2949867 -4.1739407 -4.0144229 -3.9153695 -3.8893337 -3.8872151 -3.9332023 -4.0031557 -4.0526876 -4.0752516 -4.085597 -4.13641][-4.4480877 -4.4935246 -4.5223045 -4.5011549 -4.4227114 -4.2903166 -4.1800232 -4.1125784 -4.0611982 -4.0662708 -4.1056542 -4.1277528 -4.1287646 -4.1233969 -4.1583347][-4.4456196 -4.529623 -4.6021128 -4.6273642 -4.6012845 -4.5156808 -4.4199085 -4.33735 -4.2663369 -4.2489939 -4.2644114 -4.2670784 -4.256948 -4.2428565 -4.2547946][-4.4068327 -4.498126 -4.5861826 -4.6379595 -4.6518707 -4.6098 -4.5392575 -4.4737382 -4.4242854 -4.4160018 -4.4241767 -4.4189138 -4.4022989 -4.3794785 -4.3689466][-4.3413625 -4.409287 -4.48525 -4.5427146 -4.576858 -4.5634828 -4.5229559 -4.4970179 -4.4914651 -4.5041261 -4.5131979 -4.5066457 -4.4919581 -4.4691777 -4.4481206][-4.2735567 -4.3174744 -4.3732185 -4.4157844 -4.4410887 -4.4349489 -4.4225664 -4.4418888 -4.4775219 -4.5075927 -4.5191283 -4.5110674 -4.4957752 -4.47608 -4.4575887][-4.2388535 -4.2729692 -4.3013391 -4.3072467 -4.3028154 -4.2961068 -4.3129859 -4.370676 -4.4305215 -4.4657087 -4.4769068 -4.4689384 -4.4508343 -4.4317312 -4.41989][-4.2356086 -4.2631145 -4.2544265 -4.2170243 -4.1899419 -4.1989169 -4.2483778 -4.3296437 -4.3972578 -4.4268093 -4.4265409 -4.413322 -4.3929377 -4.3751593 -4.3699145]]...]
INFO - root - 2017-12-07 21:48:46.437424: step 51510, loss = 21.24, batch loss = 21.16 (8.6 examples/sec; 0.936 sec/batch; 73h:01m:26s remains)
INFO - root - 2017-12-07 21:48:55.826137: step 51520, loss = 21.81, batch loss = 21.72 (8.3 examples/sec; 0.960 sec/batch; 74h:53m:34s remains)
INFO - root - 2017-12-07 21:49:05.051832: step 51530, loss = 21.28, batch loss = 21.19 (8.5 examples/sec; 0.943 sec/batch; 73h:36m:46s remains)
INFO - root - 2017-12-07 21:49:14.526881: step 51540, loss = 21.23, batch loss = 21.14 (8.8 examples/sec; 0.905 sec/batch; 70h:36m:31s remains)
INFO - root - 2017-12-07 21:49:24.119415: step 51550, loss = 21.31, batch loss = 21.23 (8.3 examples/sec; 0.963 sec/batch; 75h:07m:28s remains)
INFO - root - 2017-12-07 21:49:33.483939: step 51560, loss = 21.73, batch loss = 21.65 (9.0 examples/sec; 0.892 sec/batch; 69h:36m:00s remains)
INFO - root - 2017-12-07 21:49:42.876219: step 51570, loss = 21.67, batch loss = 21.59 (8.7 examples/sec; 0.921 sec/batch; 71h:51m:49s remains)
INFO - root - 2017-12-07 21:49:52.291717: step 51580, loss = 21.46, batch loss = 21.38 (8.4 examples/sec; 0.951 sec/batch; 74h:13m:37s remains)
INFO - root - 2017-12-07 21:50:01.684615: step 51590, loss = 21.25, batch loss = 21.17 (8.3 examples/sec; 0.968 sec/batch; 75h:34m:06s remains)
INFO - root - 2017-12-07 21:50:11.024294: step 51600, loss = 21.32, batch loss = 21.24 (9.0 examples/sec; 0.885 sec/batch; 69h:04m:23s remains)
2017-12-07 21:50:11.918069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.50333 -4.6088552 -4.6833119 -4.7101631 -4.678597 -4.6014385 -4.5292878 -4.5012383 -4.5211015 -4.5454922 -4.53528 -4.4998107 -4.4771509 -4.4750409 -4.4679937][-4.4393234 -4.5660954 -4.682579 -4.7501473 -4.7310858 -4.6328344 -4.5322018 -4.4900012 -4.5129886 -4.5490036 -4.5454044 -4.5019183 -4.4586487 -4.4297466 -4.3934832][-4.3524232 -4.4616227 -4.6015468 -4.710125 -4.7056532 -4.5786619 -4.430974 -4.3634291 -4.3916292 -4.4616451 -4.50416 -4.4984555 -4.4668202 -4.4206219 -4.3509674][-4.317502 -4.3811932 -4.5152054 -4.643291 -4.6447186 -4.48484 -4.2828717 -4.1843019 -4.2266793 -4.3507776 -4.4677835 -4.534853 -4.5425396 -4.4935021 -4.3935561][-4.3511043 -4.3693452 -4.4672766 -4.5730429 -4.5576291 -4.3685131 -4.1166945 -3.9822416 -4.0399404 -4.2263923 -4.4274554 -4.5749364 -4.6291437 -4.5876923 -4.476449][-4.4174 -4.4067807 -4.4478059 -4.4901819 -4.4340272 -4.2204757 -3.9322629 -3.7632167 -3.8343494 -4.0781994 -4.34235 -4.5351286 -4.6099229 -4.5872808 -4.507936][-4.508893 -4.4858685 -4.4671125 -4.4300518 -4.3191209 -4.0802155 -3.7628014 -3.5618072 -3.6399353 -3.9221363 -4.214757 -4.4068341 -4.4738126 -4.4742022 -4.4574466][-4.6292019 -4.6064863 -4.5526614 -4.4628444 -4.3100767 -4.0483608 -3.7092752 -3.4764125 -3.5374181 -3.8207541 -4.1187739 -4.3034759 -4.3604918 -4.3754239 -4.4073114][-4.7261267 -4.7158632 -4.66993 -4.5880442 -4.443707 -4.1900339 -3.8529229 -3.5924511 -3.5940194 -3.8268864 -4.104897 -4.2861166 -4.341414 -4.357852 -4.4048223][-4.762485 -4.7595353 -4.7434154 -4.7145982 -4.6314068 -4.4335566 -4.1360407 -3.8609571 -3.779793 -3.923676 -4.153873 -4.32417 -4.3816929 -4.3924532 -4.4269][-4.7150946 -4.7163496 -4.7287512 -4.7560492 -4.7497888 -4.6419277 -4.4204187 -4.1640887 -4.0216284 -4.072823 -4.2367196 -4.3804092 -4.4354806 -4.4357977 -4.4379134][-4.6151118 -4.6209888 -4.649332 -4.70874 -4.7632146 -4.746079 -4.61453 -4.4108205 -4.2586021 -4.2485638 -4.346468 -4.4455738 -4.481298 -4.4592505 -4.4181571][-4.51294 -4.5302386 -4.56887 -4.63177 -4.7127905 -4.7593508 -4.7075219 -4.5724339 -4.4538269 -4.431179 -4.4876232 -4.5423937 -4.5449862 -4.4893732 -4.4088941][-4.3920851 -4.4379592 -4.5043292 -4.5695148 -4.6533613 -4.7297845 -4.7297659 -4.6529331 -4.5805774 -4.5750918 -4.6194816 -4.64964 -4.626924 -4.5460734 -4.4461322][-4.249824 -4.3223934 -4.4273343 -4.5120311 -4.6001453 -4.6925111 -4.7227654 -4.6781797 -4.6295605 -4.6340661 -4.6776128 -4.7019119 -4.6770935 -4.5990496 -4.5063267]]...]
INFO - root - 2017-12-07 21:50:21.304847: step 51610, loss = 21.89, batch loss = 21.81 (9.2 examples/sec; 0.873 sec/batch; 68h:05m:13s remains)
INFO - root - 2017-12-07 21:50:30.831234: step 51620, loss = 21.74, batch loss = 21.66 (9.0 examples/sec; 0.892 sec/batch; 69h:36m:18s remains)
INFO - root - 2017-12-07 21:50:40.356075: step 51630, loss = 21.57, batch loss = 21.49 (8.6 examples/sec; 0.929 sec/batch; 72h:28m:32s remains)
INFO - root - 2017-12-07 21:50:49.832506: step 51640, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.957 sec/batch; 74h:39m:16s remains)
INFO - root - 2017-12-07 21:50:59.296927: step 51650, loss = 21.64, batch loss = 21.55 (8.4 examples/sec; 0.958 sec/batch; 74h:43m:21s remains)
INFO - root - 2017-12-07 21:51:08.773337: step 51660, loss = 21.46, batch loss = 21.37 (8.7 examples/sec; 0.924 sec/batch; 72h:04m:32s remains)
INFO - root - 2017-12-07 21:51:18.003353: step 51670, loss = 21.60, batch loss = 21.52 (9.5 examples/sec; 0.841 sec/batch; 65h:36m:11s remains)
INFO - root - 2017-12-07 21:51:27.405547: step 51680, loss = 21.97, batch loss = 21.89 (8.0 examples/sec; 0.997 sec/batch; 77h:44m:29s remains)
INFO - root - 2017-12-07 21:51:36.789972: step 51690, loss = 21.35, batch loss = 21.26 (8.9 examples/sec; 0.901 sec/batch; 70h:19m:04s remains)
INFO - root - 2017-12-07 21:51:46.167479: step 51700, loss = 21.28, batch loss = 21.20 (8.6 examples/sec; 0.932 sec/batch; 72h:40m:07s remains)
2017-12-07 21:51:47.131416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4826794 -4.5156393 -4.5215526 -4.5138316 -4.4885311 -4.4410405 -4.42028 -4.4135051 -4.3809462 -4.3548784 -4.3448629 -4.3163505 -4.2878003 -4.3007541 -4.3858376][-4.5623369 -4.5707264 -4.5499587 -4.5116282 -4.4670057 -4.409164 -4.3644781 -4.334589 -4.302197 -4.3008666 -4.3284407 -4.3287344 -4.3102789 -4.3246336 -4.4140172][-4.5494347 -4.5434341 -4.5230947 -4.4962707 -4.4690518 -4.4166822 -4.3504558 -4.2933145 -4.2591076 -4.2779541 -4.3381591 -4.3683996 -4.3688483 -4.3886452 -4.474442][-4.5044036 -4.4777274 -4.4570875 -4.4482837 -4.4450674 -4.3933067 -4.2955012 -4.2072005 -4.1665125 -4.1970215 -4.2807226 -4.3455353 -4.3848443 -4.4307165 -4.5227847][-4.4733024 -4.4287109 -4.3975968 -4.38482 -4.375844 -4.2908087 -4.1325197 -3.998523 -3.9466858 -3.9892869 -4.0930014 -4.1942263 -4.284914 -4.3757071 -4.4910464][-4.4351325 -4.3951268 -4.3628926 -4.3377981 -4.2995682 -4.1580195 -3.9201832 -3.72303 -3.6518478 -3.7147408 -3.8476789 -3.9843583 -4.1187325 -4.2488842 -4.390265][-4.4136691 -4.3908167 -4.3732119 -4.345962 -4.2814331 -4.096673 -3.8016458 -3.5497081 -3.4570625 -3.5437202 -3.7096152 -3.872227 -4.0261464 -4.1688895 -4.316299][-4.4335709 -4.422267 -4.4220214 -4.4076185 -4.34533 -4.1743679 -3.9004498 -3.6483929 -3.551506 -3.6426404 -3.8009353 -3.9499598 -4.0880079 -4.2106605 -4.3367319][-4.444375 -4.4400916 -4.4525104 -4.4555674 -4.4214873 -4.3181443 -4.139461 -3.9518814 -3.8725176 -3.9346104 -4.0284443 -4.1189222 -4.2195697 -4.3141589 -4.4147096][-4.4456882 -4.4438767 -4.4601641 -4.4736123 -4.4682546 -4.437696 -4.3656416 -4.2618365 -4.2100606 -4.231308 -4.241713 -4.2576475 -4.3184795 -4.3939524 -4.4817233][-4.4713449 -4.475925 -4.4912028 -4.505527 -4.5162473 -4.5310473 -4.5235214 -4.4750061 -4.4370527 -4.420804 -4.3735762 -4.3411579 -4.3776889 -4.4455118 -4.5271816][-4.5340018 -4.5485468 -4.5664139 -4.5800691 -4.59627 -4.6240406 -4.6357317 -4.6079774 -4.5654545 -4.5188622 -4.4468808 -4.4040718 -4.4407539 -4.5123725 -4.5865464][-4.6015983 -4.6253285 -4.6453018 -4.6594658 -4.6797853 -4.7106876 -4.7322826 -4.7249155 -4.6873884 -4.62305 -4.5378733 -4.49645 -4.5414939 -4.6212525 -4.683486][-4.6337843 -4.6541867 -4.666441 -4.6766305 -4.6976967 -4.7254944 -4.748127 -4.7557974 -4.7341461 -4.67658 -4.6001639 -4.5718427 -4.6284652 -4.7185068 -4.7747865][-4.6038256 -4.6118488 -4.6104531 -4.6140456 -4.6352811 -4.6586041 -4.6721449 -4.6752548 -4.6602912 -4.6236038 -4.5794473 -4.5791755 -4.6494164 -4.7454271 -4.8013]]...]
INFO - root - 2017-12-07 21:51:56.707801: step 51710, loss = 21.36, batch loss = 21.28 (8.2 examples/sec; 0.979 sec/batch; 76h:22m:57s remains)
INFO - root - 2017-12-07 21:52:06.217682: step 51720, loss = 21.54, batch loss = 21.46 (8.3 examples/sec; 0.970 sec/batch; 75h:37m:02s remains)
INFO - root - 2017-12-07 21:52:15.676118: step 51730, loss = 21.47, batch loss = 21.39 (8.4 examples/sec; 0.950 sec/batch; 74h:07m:39s remains)
INFO - root - 2017-12-07 21:52:25.126230: step 51740, loss = 21.61, batch loss = 21.53 (9.2 examples/sec; 0.867 sec/batch; 67h:38m:38s remains)
INFO - root - 2017-12-07 21:52:34.516219: step 51750, loss = 21.80, batch loss = 21.72 (8.7 examples/sec; 0.916 sec/batch; 71h:24m:20s remains)
INFO - root - 2017-12-07 21:52:43.944216: step 51760, loss = 21.40, batch loss = 21.32 (8.2 examples/sec; 0.972 sec/batch; 75h:48m:10s remains)
INFO - root - 2017-12-07 21:52:53.422581: step 51770, loss = 21.29, batch loss = 21.21 (7.9 examples/sec; 1.008 sec/batch; 78h:37m:07s remains)
INFO - root - 2017-12-07 21:53:02.420029: step 51780, loss = 21.48, batch loss = 21.40 (8.0 examples/sec; 1.001 sec/batch; 78h:03m:38s remains)
INFO - root - 2017-12-07 21:53:11.789350: step 51790, loss = 21.36, batch loss = 21.28 (8.2 examples/sec; 0.980 sec/batch; 76h:24m:36s remains)
INFO - root - 2017-12-07 21:53:21.382065: step 51800, loss = 21.70, batch loss = 21.61 (8.6 examples/sec; 0.932 sec/batch; 72h:39m:26s remains)
2017-12-07 21:53:22.258583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3963737 -4.4135385 -4.4493775 -4.4673104 -4.4424095 -4.3978219 -4.3665595 -4.3556542 -4.3540421 -4.3549929 -4.3490305 -4.3308764 -4.30575 -4.2889795 -4.2889652][-4.4145622 -4.4484177 -4.4965014 -4.516705 -4.4911594 -4.4459271 -4.4178038 -4.4150043 -4.4210711 -4.422164 -4.408299 -4.3743854 -4.3300228 -4.2982197 -4.2948942][-4.4325027 -4.4684005 -4.5084147 -4.5182638 -4.4928541 -4.4541368 -4.4314103 -4.4350839 -4.45147 -4.4620552 -4.4519768 -4.4121075 -4.3571754 -4.3175535 -4.3136864][-4.4440823 -4.4646163 -4.4824014 -4.4782052 -4.4583688 -4.4324527 -4.4168243 -4.4252157 -4.4488664 -4.4682302 -4.4683032 -4.4336624 -4.3825774 -4.3494105 -4.3488226][-4.4408145 -4.4408269 -4.4357409 -4.4204426 -4.401113 -4.3744512 -4.3529625 -4.3575087 -4.38179 -4.4092531 -4.427238 -4.4138131 -4.3881445 -4.3799062 -4.3896418][-4.4112897 -4.3866658 -4.3605208 -4.3320479 -4.2996407 -4.2484765 -4.2036242 -4.1987586 -4.2259626 -4.269063 -4.313889 -4.3327875 -4.3435364 -4.371911 -4.4026418][-4.3696151 -4.3165765 -4.2679625 -4.2258248 -4.1731524 -4.0862494 -4.010004 -4.0008984 -4.0470066 -4.1250906 -4.2080936 -4.2597818 -4.2970037 -4.3474517 -4.3913145][-4.304255 -4.22956 -4.1712408 -4.1284924 -4.0685782 -3.9541397 -3.8432155 -3.8224773 -3.8908718 -4.0138664 -4.1450853 -4.2358351 -4.2953887 -4.3486872 -4.3820562][-4.2112727 -4.1267676 -4.0739665 -4.0486279 -4.01182 -3.906738 -3.7805767 -3.7400937 -3.8124542 -3.9582489 -4.1188183 -4.2426085 -4.3257694 -4.3780284 -4.3906722][-4.1146135 -4.0273552 -3.9819062 -3.9808524 -3.9921825 -3.9440908 -3.8528843 -3.8063328 -3.8534544 -3.9766052 -4.1281009 -4.2653804 -4.3701334 -4.4272156 -4.4248848][-4.0619469 -3.9783211 -3.9392531 -3.9536593 -3.9995012 -4.0071831 -3.9646316 -3.9266047 -3.9460878 -4.0292354 -4.1582561 -4.2951517 -4.40484 -4.45779 -4.4437666][-4.1053605 -4.0246954 -3.9768405 -3.9773302 -4.0161643 -4.0466485 -4.0446825 -4.0368643 -4.0629396 -4.1335015 -4.2474008 -4.3678012 -4.4486985 -4.4649377 -4.4239364][-4.2286859 -4.14441 -4.0746098 -4.040133 -4.046051 -4.0805244 -4.1206193 -4.1624918 -4.2271385 -4.3102431 -4.4081759 -4.49271 -4.5183077 -4.4781084 -4.4017653][-4.367187 -4.2801504 -4.196085 -4.1367278 -4.1190667 -4.1581664 -4.229445 -4.3059783 -4.396574 -4.4855647 -4.5597687 -4.600647 -4.570931 -4.4813972 -4.3796992][-4.4764571 -4.3879147 -4.3016171 -4.2356305 -4.2094941 -4.2506742 -4.3315482 -4.41184 -4.4993486 -4.5792794 -4.6310668 -4.638453 -4.5736423 -4.4602251 -4.354383]]...]
INFO - root - 2017-12-07 21:53:31.625831: step 51810, loss = 21.82, batch loss = 21.73 (9.0 examples/sec; 0.891 sec/batch; 69h:27m:50s remains)
INFO - root - 2017-12-07 21:53:41.104391: step 51820, loss = 21.45, batch loss = 21.37 (8.5 examples/sec; 0.939 sec/batch; 73h:10m:31s remains)
INFO - root - 2017-12-07 21:53:50.556916: step 51830, loss = 21.50, batch loss = 21.41 (8.5 examples/sec; 0.939 sec/batch; 73h:14m:32s remains)
INFO - root - 2017-12-07 21:53:59.909448: step 51840, loss = 21.49, batch loss = 21.40 (8.5 examples/sec; 0.942 sec/batch; 73h:28m:36s remains)
INFO - root - 2017-12-07 21:54:09.289740: step 51850, loss = 21.57, batch loss = 21.48 (8.4 examples/sec; 0.947 sec/batch; 73h:51m:03s remains)
INFO - root - 2017-12-07 21:54:18.830442: step 51860, loss = 21.02, batch loss = 20.94 (8.3 examples/sec; 0.964 sec/batch; 75h:08m:31s remains)
INFO - root - 2017-12-07 21:54:28.375949: step 51870, loss = 21.62, batch loss = 21.54 (8.6 examples/sec; 0.930 sec/batch; 72h:30m:16s remains)
INFO - root - 2017-12-07 21:54:37.717007: step 51880, loss = 21.38, batch loss = 21.30 (8.4 examples/sec; 0.949 sec/batch; 74h:00m:03s remains)
INFO - root - 2017-12-07 21:54:47.203660: step 51890, loss = 21.48, batch loss = 21.39 (8.2 examples/sec; 0.972 sec/batch; 75h:47m:07s remains)
INFO - root - 2017-12-07 21:54:56.700854: step 51900, loss = 21.40, batch loss = 21.32 (8.3 examples/sec; 0.969 sec/batch; 75h:32m:56s remains)
2017-12-07 21:54:57.661643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.661551 -4.6926384 -4.7038488 -4.685061 -4.6451097 -4.5462685 -4.4035993 -4.3472757 -4.4293628 -4.5566711 -4.6476483 -4.678215 -4.6424947 -4.5560102 -4.4531808][-4.6013403 -4.6381211 -4.663744 -4.6569839 -4.6270032 -4.5318828 -4.3884697 -4.3436661 -4.4512544 -4.6006455 -4.699357 -4.7352042 -4.7048635 -4.6143446 -4.4934206][-4.5646925 -4.5811558 -4.602778 -4.6117954 -4.6076136 -4.5364213 -4.403861 -4.3638091 -4.4801397 -4.635067 -4.7362714 -4.7817979 -4.7651892 -4.67814 -4.543324][-4.5343747 -4.5224876 -4.5283208 -4.5519137 -4.5788374 -4.5342803 -4.4122868 -4.3730445 -4.4931641 -4.6507173 -4.7599287 -4.8229346 -4.8231072 -4.7407408 -4.594192][-4.4385805 -4.4175725 -4.4180994 -4.458147 -4.5101871 -4.4825373 -4.3701797 -4.3399553 -4.4726067 -4.6400156 -4.7630982 -4.8420858 -4.8538613 -4.7757535 -4.6239715][-4.3188043 -4.2917585 -4.2838817 -4.3185606 -4.36702 -4.3271942 -4.2036514 -4.1725321 -4.3186035 -4.5097485 -4.6637721 -4.7718282 -4.8079219 -4.7509809 -4.6140471][-4.2222071 -4.1890211 -4.1713872 -4.1867943 -4.2101812 -4.1423025 -3.9909632 -3.9415517 -4.0853848 -4.296855 -4.4934683 -4.6490765 -4.7288084 -4.7058744 -4.5911827][-4.1487408 -4.1130347 -4.0930872 -4.0908442 -4.0868258 -4.000536 -3.8407645 -3.7853 -3.91813 -4.1301069 -4.3494868 -4.5395517 -4.659771 -4.6700931 -4.5743518][-4.21468 -4.1793494 -4.14386 -4.1055694 -4.0547442 -3.9378953 -3.7795711 -3.7327573 -3.857589 -4.0555425 -4.2663851 -4.4587 -4.5967693 -4.6303468 -4.5533628][-4.3339581 -4.3124022 -4.2707267 -4.2107582 -4.1295276 -3.9969945 -3.8632033 -3.8518636 -3.9904351 -4.1843429 -4.3680263 -4.5173693 -4.6178169 -4.6271548 -4.5421166][-4.427958 -4.410655 -4.3665466 -4.2985415 -4.2040744 -4.072535 -3.9740086 -4.0055122 -4.170805 -4.3758116 -4.5466118 -4.6524773 -4.6939507 -4.654367 -4.5435309][-4.58237 -4.5541754 -4.5069337 -4.4390655 -4.3437481 -4.221189 -4.1419911 -4.1808362 -4.3311152 -4.5122204 -4.6544056 -4.7263002 -4.7306828 -4.6646705 -4.5394158][-4.6908531 -4.6716371 -4.6459279 -4.6077437 -4.5483189 -4.4625249 -4.4015312 -4.4281015 -4.5331516 -4.6578364 -4.7531939 -4.791203 -4.7670045 -4.6792388 -4.5399952][-4.6703234 -4.670289 -4.6740823 -4.6755686 -4.663754 -4.619493 -4.5682082 -4.5686784 -4.62582 -4.7053881 -4.7763128 -4.8066077 -4.7757406 -4.6796827 -4.5341048][-4.603723 -4.6149244 -4.6305685 -4.6470609 -4.6557565 -4.6299725 -4.5810733 -4.5626273 -4.5916252 -4.6474714 -4.7097158 -4.7450657 -4.7221069 -4.6367717 -4.5056038]]...]
INFO - root - 2017-12-07 21:55:07.097093: step 51910, loss = 21.34, batch loss = 21.26 (8.4 examples/sec; 0.953 sec/batch; 74h:17m:03s remains)
INFO - root - 2017-12-07 21:55:16.597189: step 51920, loss = 21.09, batch loss = 21.00 (8.3 examples/sec; 0.958 sec/batch; 74h:41m:37s remains)
INFO - root - 2017-12-07 21:55:26.065869: step 51930, loss = 21.36, batch loss = 21.27 (9.0 examples/sec; 0.889 sec/batch; 69h:15m:41s remains)
INFO - root - 2017-12-07 21:55:35.569233: step 51940, loss = 21.56, batch loss = 21.48 (8.9 examples/sec; 0.899 sec/batch; 70h:03m:31s remains)
INFO - root - 2017-12-07 21:55:44.799673: step 51950, loss = 21.40, batch loss = 21.31 (8.4 examples/sec; 0.956 sec/batch; 74h:30m:57s remains)
INFO - root - 2017-12-07 21:55:54.226941: step 51960, loss = 21.30, batch loss = 21.21 (8.5 examples/sec; 0.942 sec/batch; 73h:26m:17s remains)
INFO - root - 2017-12-07 21:56:03.604510: step 51970, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.951 sec/batch; 74h:07m:16s remains)
INFO - root - 2017-12-07 21:56:12.752560: step 51980, loss = 21.67, batch loss = 21.59 (8.5 examples/sec; 0.937 sec/batch; 72h:58m:29s remains)
INFO - root - 2017-12-07 21:56:22.116684: step 51990, loss = 21.08, batch loss = 21.00 (9.0 examples/sec; 0.891 sec/batch; 69h:26m:10s remains)
INFO - root - 2017-12-07 21:56:31.438990: step 52000, loss = 20.99, batch loss = 20.91 (8.4 examples/sec; 0.949 sec/batch; 73h:54m:36s remains)
2017-12-07 21:56:32.399857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4185028 -4.4406781 -4.4676919 -4.4877834 -4.4942837 -4.4888883 -4.4838576 -4.49033 -4.5082159 -4.5322127 -4.5579462 -4.5754032 -4.5723109 -4.545826 -4.5021892][-4.4879923 -4.5222163 -4.5518022 -4.5626407 -4.5503473 -4.5293145 -4.521472 -4.5368819 -4.5713449 -4.6137991 -4.658493 -4.69159 -4.6960239 -4.6663976 -4.6063843][-4.5317225 -4.5694995 -4.591476 -4.58604 -4.5552936 -4.5202432 -4.5082726 -4.5272741 -4.5735216 -4.6408644 -4.7152042 -4.7683835 -4.7794771 -4.7470417 -4.6767011][-4.5089231 -4.5381584 -4.5467167 -4.5268054 -4.4816947 -4.4290867 -4.3938384 -4.384038 -4.4198952 -4.5215769 -4.6553431 -4.7563343 -4.792294 -4.7635345 -4.6831408][-4.4533434 -4.4631996 -4.4541669 -4.4171963 -4.3490672 -4.2589145 -4.1670094 -4.0886889 -4.0811734 -4.2095132 -4.4218183 -4.6062717 -4.7109885 -4.7168365 -4.635675][-4.4263458 -4.4176049 -4.3960204 -4.3402934 -4.23465 -4.0848985 -3.9157116 -3.7589419 -3.698478 -3.8330424 -4.1071477 -4.377861 -4.5737047 -4.6429048 -4.5832038][-4.4231071 -4.4152746 -4.40587 -4.353786 -4.225451 -4.0241394 -3.7877321 -3.5717921 -3.4715307 -3.5935092 -3.890058 -4.2103181 -4.4685483 -4.5934839 -4.5660462][-4.4297233 -4.4500947 -4.4781508 -4.458837 -4.345942 -4.1373925 -3.8759813 -3.6348147 -3.5066559 -3.5919023 -3.8621855 -4.1760015 -4.4371715 -4.5800261 -4.5749931][-4.4362783 -4.5039129 -4.5812292 -4.6071219 -4.5367355 -4.3624387 -4.127255 -3.9039354 -3.7702792 -3.8109121 -4.0200043 -4.2779074 -4.4855046 -4.597909 -4.58642][-4.3911042 -4.4989252 -4.6279473 -4.7082334 -4.6963396 -4.5815187 -4.4026246 -4.2313371 -4.1217442 -4.1330013 -4.2740459 -4.4543557 -4.5841951 -4.6341915 -4.5880852][-4.2511005 -4.3556223 -4.5240803 -4.6692972 -4.7371464 -4.700809 -4.5894356 -4.4751949 -4.4024539 -4.40603 -4.4940519 -4.6025038 -4.6652632 -4.6584024 -4.5780268][-4.0052867 -4.0640154 -4.2502685 -4.4623871 -4.6284285 -4.6923914 -4.65264 -4.58393 -4.5487022 -4.567327 -4.6287146 -4.6858883 -4.701448 -4.6565495 -4.5557981][-3.7362452 -3.7436731 -3.9281063 -4.1863141 -4.4394908 -4.6025414 -4.6273232 -4.5917611 -4.5879755 -4.6307449 -4.6868243 -4.7144604 -4.7010226 -4.6388712 -4.5367146][-3.6082063 -3.5783222 -3.7451994 -4.0168762 -4.3176312 -4.546885 -4.6203861 -4.6099777 -4.6239562 -4.6768312 -4.7276835 -4.7384744 -4.7076588 -4.6368737 -4.5421443][-3.6992965 -3.6413617 -3.7662406 -4.0068345 -4.2912169 -4.5243435 -4.6194139 -4.6290879 -4.6439261 -4.6883073 -4.7412519 -4.7633705 -4.7412305 -4.6726971 -4.5818686]]...]
INFO - root - 2017-12-07 21:56:41.820886: step 52010, loss = 21.29, batch loss = 21.21 (8.3 examples/sec; 0.958 sec/batch; 74h:39m:40s remains)
INFO - root - 2017-12-07 21:56:51.262229: step 52020, loss = 21.77, batch loss = 21.69 (8.4 examples/sec; 0.952 sec/batch; 74h:10m:53s remains)
INFO - root - 2017-12-07 21:57:00.700560: step 52030, loss = 21.36, batch loss = 21.28 (8.2 examples/sec; 0.978 sec/batch; 76h:11m:32s remains)
INFO - root - 2017-12-07 21:57:10.037888: step 52040, loss = 21.43, batch loss = 21.34 (8.3 examples/sec; 0.964 sec/batch; 75h:07m:38s remains)
INFO - root - 2017-12-07 21:57:19.435609: step 52050, loss = 22.00, batch loss = 21.91 (8.6 examples/sec; 0.927 sec/batch; 72h:14m:36s remains)
INFO - root - 2017-12-07 21:57:28.787017: step 52060, loss = 21.44, batch loss = 21.35 (9.2 examples/sec; 0.867 sec/batch; 67h:31m:56s remains)
INFO - root - 2017-12-07 21:57:38.156022: step 52070, loss = 21.63, batch loss = 21.54 (8.6 examples/sec; 0.934 sec/batch; 72h:47m:26s remains)
INFO - root - 2017-12-07 21:57:47.567507: step 52080, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.943 sec/batch; 73h:28m:13s remains)
INFO - root - 2017-12-07 21:57:56.808250: step 52090, loss = 21.28, batch loss = 21.20 (8.1 examples/sec; 0.987 sec/batch; 76h:54m:11s remains)
INFO - root - 2017-12-07 21:58:06.134251: step 52100, loss = 21.55, batch loss = 21.46 (8.6 examples/sec; 0.925 sec/batch; 72h:03m:20s remains)
2017-12-07 21:58:07.032675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1753621 -4.2497563 -4.3516579 -4.4430175 -4.517786 -4.5465126 -4.521976 -4.4810266 -4.444447 -4.4313688 -4.4508152 -4.43925 -4.4218307 -4.4021559 -4.3194742][-4.13746 -4.232873 -4.3409042 -4.4149413 -4.4652119 -4.4780474 -4.4569926 -4.4405589 -4.4442821 -4.4736037 -4.5225949 -4.5250726 -4.5012412 -4.4651175 -4.3762693][-4.1772037 -4.2788682 -4.3712978 -4.4108777 -4.4276648 -4.423048 -4.4052429 -4.4158773 -4.45557 -4.5199361 -4.59128 -4.6081529 -4.5857735 -4.5455079 -4.4702854][-4.2852135 -4.3772588 -4.428 -4.4121852 -4.3814979 -4.3490987 -4.3293514 -4.3637004 -4.4334245 -4.5251555 -4.6145468 -4.6450381 -4.6300011 -4.5969725 -4.5434656][-4.4341927 -4.5018678 -4.5009952 -4.4250007 -4.33703 -4.2568812 -4.21116 -4.2494893 -4.3415661 -4.4617977 -4.5799994 -4.6392856 -4.6422181 -4.6182804 -4.5742621][-4.5817885 -4.6246481 -4.5832877 -4.4600711 -4.3200517 -4.1795945 -4.0791621 -4.0873475 -4.1784034 -4.3181272 -4.4719577 -4.5797257 -4.617856 -4.6077666 -4.562995][-4.6823173 -4.7119708 -4.6503115 -4.5044093 -4.3288975 -4.13312 -3.9758255 -3.9437907 -4.021153 -4.170248 -4.3519907 -4.508142 -4.5916486 -4.6022773 -4.5468903][-4.7335224 -4.7608619 -4.6857619 -4.5258126 -4.33053 -4.0966873 -3.8983321 -3.8324163 -3.8955405 -4.0563045 -4.2598658 -4.445508 -4.5625324 -4.5883579 -4.52401][-4.7507119 -4.7752328 -4.679224 -4.4991112 -4.29149 -4.0443511 -3.831213 -3.7420449 -3.7852206 -3.9446883 -4.1605158 -4.369154 -4.5233173 -4.5774031 -4.5278497][-4.7360096 -4.7542543 -4.6366091 -4.4365587 -4.2223868 -3.9795673 -3.7728965 -3.6816649 -3.7192953 -3.8788514 -4.096971 -4.3138795 -4.4934654 -4.5788255 -4.5606847][-4.7071877 -4.7236266 -4.6033678 -4.4061742 -4.2027287 -3.9777226 -3.7841589 -3.7008755 -3.7445657 -3.9066629 -4.1234622 -4.3295302 -4.507719 -4.610508 -4.6207771][-4.689055 -4.7173524 -4.6219673 -4.4544282 -4.2768388 -4.0731335 -3.8847775 -3.7978644 -3.8363276 -3.9949515 -4.21237 -4.4045639 -4.56261 -4.6666017 -4.6959338][-4.6781564 -4.7292929 -4.6822624 -4.5695534 -4.4325852 -4.2551565 -4.0733829 -3.9775 -3.9987855 -4.1433835 -4.3536916 -4.5225773 -4.6403527 -4.7199712 -4.74629][-4.6588845 -4.7321367 -4.7397685 -4.6933274 -4.6063828 -4.46283 -4.2921152 -4.1859541 -4.1863656 -4.3093472 -4.49968 -4.6377926 -4.7107396 -4.7534904 -4.765079][-4.6294694 -4.7111864 -4.7559905 -4.762805 -4.7234111 -4.6183991 -4.470993 -4.3659582 -4.353704 -4.4518857 -4.605926 -4.7112446 -4.7548728 -4.7690096 -4.7652364]]...]
INFO - root - 2017-12-07 21:58:16.433607: step 52110, loss = 21.41, batch loss = 21.33 (8.7 examples/sec; 0.921 sec/batch; 71h:43m:48s remains)
INFO - root - 2017-12-07 21:58:25.659712: step 52120, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.950 sec/batch; 74h:00m:11s remains)
INFO - root - 2017-12-07 21:58:35.043534: step 52130, loss = 21.27, batch loss = 21.19 (8.2 examples/sec; 0.981 sec/batch; 76h:22m:40s remains)
INFO - root - 2017-12-07 21:58:44.506576: step 52140, loss = 21.57, batch loss = 21.49 (8.1 examples/sec; 0.991 sec/batch; 77h:12m:52s remains)
INFO - root - 2017-12-07 21:58:53.711229: step 52150, loss = 21.70, batch loss = 21.62 (8.8 examples/sec; 0.912 sec/batch; 71h:01m:15s remains)
INFO - root - 2017-12-07 21:59:03.176114: step 52160, loss = 21.34, batch loss = 21.25 (8.4 examples/sec; 0.958 sec/batch; 74h:33m:55s remains)
INFO - root - 2017-12-07 21:59:12.690699: step 52170, loss = 21.70, batch loss = 21.62 (8.2 examples/sec; 0.970 sec/batch; 75h:31m:14s remains)
INFO - root - 2017-12-07 21:59:22.043067: step 52180, loss = 21.59, batch loss = 21.51 (8.7 examples/sec; 0.920 sec/batch; 71h:36m:48s remains)
INFO - root - 2017-12-07 21:59:31.300217: step 52190, loss = 21.67, batch loss = 21.59 (8.7 examples/sec; 0.919 sec/batch; 71h:34m:09s remains)
INFO - root - 2017-12-07 21:59:40.855230: step 52200, loss = 21.06, batch loss = 20.98 (8.4 examples/sec; 0.953 sec/batch; 74h:13m:43s remains)
2017-12-07 21:59:41.808199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6124244 -4.6372924 -4.6732464 -4.7139058 -4.70943 -4.6453972 -4.584455 -4.5714526 -4.5870247 -4.6008496 -4.5867591 -4.5520473 -4.5225387 -4.5303435 -4.594727][-4.4841385 -4.5278378 -4.5806093 -4.632216 -4.6309285 -4.5563684 -4.485899 -4.47631 -4.5041723 -4.5366178 -4.5511961 -4.5586033 -4.5689459 -4.6009288 -4.67481][-4.3476834 -4.4101729 -4.4767895 -4.5274324 -4.5229836 -4.4516721 -4.3878913 -4.3883891 -4.4235263 -4.4637251 -4.4950981 -4.5280666 -4.56656 -4.6207237 -4.7026062][-4.2537847 -4.318243 -4.3873129 -4.4178958 -4.3923039 -4.3270278 -4.2815185 -4.2996731 -4.3441415 -4.3888555 -4.4254632 -4.4607415 -4.4986091 -4.55744 -4.6377306][-4.186326 -4.2447147 -4.3126626 -4.3215895 -4.2685061 -4.2014904 -4.1710343 -4.206099 -4.2700534 -4.3370333 -4.3865576 -4.4155622 -4.4325013 -4.472527 -4.5390339][-4.126668 -4.1896639 -4.2649961 -4.264853 -4.1852326 -4.091568 -4.040154 -4.0666432 -4.1495914 -4.2581906 -4.3390093 -4.3755846 -4.3853045 -4.4095583 -4.4622879][-4.0663218 -4.1595564 -4.2587414 -4.2667379 -4.1682277 -4.0301323 -3.9156971 -3.8894525 -3.9696929 -4.1205688 -4.2428236 -4.3065109 -4.334156 -4.359333 -4.4024639][-4.0593572 -4.1778836 -4.2968974 -4.3212752 -4.219245 -4.0425591 -3.8560917 -3.751229 -3.7995505 -3.9727612 -4.1292529 -4.2113447 -4.2488213 -4.2747107 -4.314333][-4.1154828 -4.2282448 -4.3478942 -4.3894811 -4.3040323 -4.1250048 -3.9110429 -3.7668843 -3.7830412 -3.9443867 -4.1035819 -4.1741757 -4.1864614 -4.19485 -4.2350574][-4.2003517 -4.2879949 -4.3934536 -4.4438772 -4.381846 -4.2290196 -4.0437803 -3.9224625 -3.9363751 -4.0732517 -4.2111478 -4.247941 -4.2078433 -4.1793981 -4.2188225][-4.3004069 -4.3565412 -4.4439092 -4.4912734 -4.4444723 -4.3236685 -4.190618 -4.1263981 -4.1612539 -4.2729077 -4.3763151 -4.3824077 -4.3048897 -4.2471542 -4.2800426][-4.4006743 -4.43966 -4.5157266 -4.556808 -4.5187788 -4.4255171 -4.3354053 -4.3117738 -4.3592534 -4.4425583 -4.5115838 -4.5060692 -4.4298768 -4.369101 -4.3903003][-4.477519 -4.5130572 -4.5820785 -4.6187921 -4.592989 -4.5269928 -4.4667816 -4.45808 -4.4989357 -4.5559559 -4.5994549 -4.5942044 -4.5402141 -4.4912372 -4.4966593][-4.488626 -4.525629 -4.5885248 -4.6279454 -4.6248722 -4.5954323 -4.5660009 -4.56298 -4.5877228 -4.622335 -4.6472735 -4.6398787 -4.6013165 -4.5624418 -4.5522919][-4.4320235 -4.4628243 -4.5127549 -4.5528111 -4.5716667 -4.5767426 -4.5758367 -4.5793877 -4.5913286 -4.6067853 -4.6149435 -4.6016788 -4.5704193 -4.5382509 -4.5198512]]...]
INFO - root - 2017-12-07 21:59:51.326518: step 52210, loss = 21.54, batch loss = 21.46 (7.9 examples/sec; 1.016 sec/batch; 79h:05m:48s remains)
INFO - root - 2017-12-07 22:00:00.684827: step 52220, loss = 21.49, batch loss = 21.40 (8.5 examples/sec; 0.945 sec/batch; 73h:35m:29s remains)
INFO - root - 2017-12-07 22:00:10.107654: step 52230, loss = 21.25, batch loss = 21.17 (8.6 examples/sec; 0.930 sec/batch; 72h:21m:56s remains)
INFO - root - 2017-12-07 22:00:19.484623: step 52240, loss = 21.41, batch loss = 21.32 (8.2 examples/sec; 0.980 sec/batch; 76h:19m:40s remains)
INFO - root - 2017-12-07 22:00:28.757519: step 52250, loss = 21.57, batch loss = 21.49 (8.7 examples/sec; 0.922 sec/batch; 71h:48m:02s remains)
INFO - root - 2017-12-07 22:00:38.234125: step 52260, loss = 21.18, batch loss = 21.10 (8.4 examples/sec; 0.949 sec/batch; 73h:52m:42s remains)
INFO - root - 2017-12-07 22:00:47.736286: step 52270, loss = 21.90, batch loss = 21.82 (9.0 examples/sec; 0.893 sec/batch; 69h:31m:22s remains)
INFO - root - 2017-12-07 22:00:57.136203: step 52280, loss = 21.77, batch loss = 21.68 (8.7 examples/sec; 0.915 sec/batch; 71h:15m:38s remains)
INFO - root - 2017-12-07 22:01:06.479530: step 52290, loss = 21.73, batch loss = 21.64 (8.4 examples/sec; 0.947 sec/batch; 73h:43m:41s remains)
INFO - root - 2017-12-07 22:01:15.716587: step 52300, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.949 sec/batch; 73h:49m:40s remains)
2017-12-07 22:01:16.610922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5783224 -4.5993681 -4.6311693 -4.680728 -4.7373896 -4.75329 -4.7007055 -4.625947 -4.6044984 -4.6455159 -4.7002659 -4.7188067 -4.731957 -4.7479596 -4.7378693][-4.6061072 -4.6389318 -4.6855531 -4.746161 -4.799643 -4.7881231 -4.692152 -4.5902867 -4.5707955 -4.6392126 -4.7309828 -4.7747912 -4.8034115 -4.8260455 -4.8163843][-4.6024837 -4.638597 -4.6838655 -4.7292938 -4.7552533 -4.7074323 -4.581152 -4.471806 -4.4624543 -4.5446248 -4.654355 -4.7181692 -4.7620673 -4.7919903 -4.7923012][-4.5709896 -4.61951 -4.6465187 -4.6435156 -4.6187153 -4.5404844 -4.412817 -4.3189335 -4.3189073 -4.3950176 -4.4971633 -4.5679722 -4.6230416 -4.6613293 -4.6779103][-4.5020857 -4.586482 -4.5942879 -4.521503 -4.4304376 -4.3248773 -4.2129889 -4.1397972 -4.1472135 -4.2116671 -4.2994027 -4.3800364 -4.4551783 -4.5102315 -4.5419946][-4.3808575 -4.523654 -4.5350146 -4.4098892 -4.26525 -4.1378951 -4.0323896 -3.9597981 -3.9605846 -4.0216389 -4.1192541 -4.2346015 -4.3426757 -4.4118705 -4.44602][-4.2581758 -4.4634662 -4.5052872 -4.3634133 -4.1866822 -4.0302024 -3.899385 -3.7953866 -3.7787933 -3.8509922 -3.9800155 -4.1451426 -4.2857733 -4.3620729 -4.3933091][-4.1921043 -4.4346652 -4.5058928 -4.3730354 -4.18643 -4.0087981 -3.8476794 -3.7055969 -3.668366 -3.7480474 -3.9052567 -4.1087971 -4.278161 -4.367187 -4.4037352][-4.1749196 -4.4177995 -4.5103016 -4.411777 -4.2501283 -4.0827031 -3.9190409 -3.7613668 -3.7052996 -3.7742996 -3.9325938 -4.1439128 -4.3230333 -4.4155612 -4.4539423][-4.21223 -4.4291825 -4.5307007 -4.4738107 -4.3530807 -4.2105789 -4.059886 -3.9020774 -3.8326952 -3.8873291 -4.03084 -4.2265725 -4.3928728 -4.4696732 -4.5011196][-4.342958 -4.5100451 -4.5989051 -4.5714793 -4.4889841 -4.3780127 -4.2520785 -4.1134276 -4.0465841 -4.095696 -4.223134 -4.39208 -4.5316982 -4.5805454 -4.587667][-4.5109854 -4.6214318 -4.6852045 -4.6811967 -4.6420431 -4.5770535 -4.4921336 -4.3874936 -4.3337388 -4.374105 -4.4776587 -4.6065826 -4.7066517 -4.722784 -4.6946363][-4.5908093 -4.6610456 -4.7097759 -4.7276096 -4.7253304 -4.6988497 -4.6485205 -4.5761065 -4.5339336 -4.5611296 -4.6347585 -4.7247987 -4.7954464 -4.7979021 -4.748776][-4.5341182 -4.585835 -4.6325164 -4.6684489 -4.691782 -4.6923 -4.6714797 -4.6321716 -4.6062684 -4.6221719 -4.6679621 -4.7209921 -4.7619495 -4.7534657 -4.6968269][-4.393909 -4.4352565 -4.4817109 -4.5277338 -4.5642447 -4.5822759 -4.5855222 -4.576241 -4.5685053 -4.5786977 -4.600965 -4.6198115 -4.6275153 -4.6057062 -4.5537696]]...]
INFO - root - 2017-12-07 22:01:25.937377: step 52310, loss = 21.49, batch loss = 21.40 (9.0 examples/sec; 0.887 sec/batch; 69h:02m:21s remains)
INFO - root - 2017-12-07 22:01:35.466301: step 52320, loss = 21.57, batch loss = 21.49 (8.2 examples/sec; 0.976 sec/batch; 75h:59m:13s remains)
INFO - root - 2017-12-07 22:01:44.811233: step 52330, loss = 21.58, batch loss = 21.50 (8.2 examples/sec; 0.971 sec/batch; 75h:35m:41s remains)
INFO - root - 2017-12-07 22:01:54.268942: step 52340, loss = 21.39, batch loss = 21.31 (7.8 examples/sec; 1.025 sec/batch; 79h:46m:51s remains)
INFO - root - 2017-12-07 22:02:03.532530: step 52350, loss = 21.50, batch loss = 21.41 (8.3 examples/sec; 0.962 sec/batch; 74h:50m:18s remains)
INFO - root - 2017-12-07 22:02:12.896641: step 52360, loss = 21.54, batch loss = 21.45 (8.2 examples/sec; 0.979 sec/batch; 76h:08m:45s remains)
INFO - root - 2017-12-07 22:02:22.340563: step 52370, loss = 20.92, batch loss = 20.84 (8.4 examples/sec; 0.956 sec/batch; 74h:22m:27s remains)
INFO - root - 2017-12-07 22:02:31.880900: step 52380, loss = 21.18, batch loss = 21.09 (8.6 examples/sec; 0.926 sec/batch; 72h:04m:46s remains)
INFO - root - 2017-12-07 22:02:41.123536: step 52390, loss = 21.38, batch loss = 21.30 (9.8 examples/sec; 0.816 sec/batch; 63h:29m:50s remains)
INFO - root - 2017-12-07 22:02:50.523117: step 52400, loss = 21.47, batch loss = 21.39 (8.5 examples/sec; 0.946 sec/batch; 73h:36m:19s remains)
2017-12-07 22:02:51.408912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2494421 -4.3071527 -4.3529143 -4.3786182 -4.4066949 -4.4501481 -4.5040617 -4.55237 -4.5879841 -4.60159 -4.5863247 -4.5393529 -4.4551983 -4.3185415 -4.1647167][-4.3111258 -4.3809032 -4.4250493 -4.4448462 -4.4688473 -4.512012 -4.5702071 -4.6145182 -4.6224651 -4.5912566 -4.5405951 -4.4800711 -4.4103909 -4.2997122 -4.161622][-4.3997769 -4.4672561 -4.4911175 -4.4824018 -4.4833369 -4.5136256 -4.5769768 -4.6290131 -4.6277905 -4.5718765 -4.4966264 -4.420929 -4.3626819 -4.2921729 -4.2066913][-4.4740181 -4.5257955 -4.5163741 -4.4701095 -4.439033 -4.445837 -4.5060983 -4.570631 -4.5850987 -4.5404181 -4.4601192 -4.3635249 -4.29434 -4.2482166 -4.2187924][-4.5160046 -4.5449948 -4.5047054 -4.4263034 -4.36301 -4.3323107 -4.3644047 -4.4262352 -4.4715495 -4.475091 -4.4183598 -4.3110356 -4.2183833 -4.1691585 -4.162951][-4.5087838 -4.5183082 -4.4581971 -4.3609514 -4.2705674 -4.191587 -4.160337 -4.1847739 -4.2572055 -4.3312197 -4.3336277 -4.2503977 -4.15549 -4.102809 -4.0964222][-4.4559813 -4.4620285 -4.396884 -4.288703 -4.1724076 -4.0398412 -3.9250085 -3.8848877 -3.9671478 -4.1063428 -4.1824331 -4.153326 -4.0890231 -4.057682 -4.0602331][-4.3797741 -4.4142051 -4.367651 -4.2570953 -4.118782 -3.937964 -3.7416291 -3.6284747 -3.7026563 -3.8892169 -4.0361705 -4.0686436 -4.0458674 -4.0427179 -4.0589008][-4.3180494 -4.4016533 -4.39345 -4.2989597 -4.156065 -3.953531 -3.7133098 -3.5554998 -3.6145184 -3.8197691 -4.00953 -4.0891228 -4.0970845 -4.0992785 -4.1082144][-4.3166771 -4.437861 -4.464725 -4.3936796 -4.2730083 -4.09329 -3.8692765 -3.7138062 -3.7543569 -3.9358079 -4.1179657 -4.215138 -4.23969 -4.2302923 -4.2091813][-4.3721876 -4.5040584 -4.5475774 -4.502234 -4.4234247 -4.302031 -4.1396608 -4.0170574 -4.0337973 -4.1551723 -4.2908316 -4.3857393 -4.4277582 -4.4175797 -4.3725958][-4.4594154 -4.5750666 -4.6166725 -4.59098 -4.5504031 -4.4952774 -4.4119473 -4.3375235 -4.3349767 -4.3935323 -4.4760995 -4.5569186 -4.6112604 -4.6172938 -4.5826254][-4.5504241 -4.6403818 -4.6743727 -4.6579881 -4.6328111 -4.6141815 -4.5875716 -4.5535069 -4.5435061 -4.5602646 -4.6044822 -4.6635804 -4.712079 -4.7293653 -4.7202835][-4.60621 -4.67805 -4.709713 -4.6976442 -4.6673522 -4.6463151 -4.6314344 -4.6097655 -4.5907679 -4.5822124 -4.6019588 -4.6418858 -4.679616 -4.7001967 -4.7109933][-4.5943847 -4.6545806 -4.6808496 -4.6629691 -4.6149473 -4.5713816 -4.5415859 -4.5088253 -4.4711156 -4.4400172 -4.439003 -4.463963 -4.4919319 -4.5081882 -4.5259547]]...]
INFO - root - 2017-12-07 22:03:00.741209: step 52410, loss = 21.50, batch loss = 21.42 (8.6 examples/sec; 0.930 sec/batch; 72h:22m:03s remains)
INFO - root - 2017-12-07 22:03:10.036965: step 52420, loss = 21.46, batch loss = 21.37 (8.8 examples/sec; 0.910 sec/batch; 70h:47m:14s remains)
INFO - root - 2017-12-07 22:03:19.290494: step 52430, loss = 21.33, batch loss = 21.25 (8.2 examples/sec; 0.972 sec/batch; 75h:35m:41s remains)
INFO - root - 2017-12-07 22:03:28.636070: step 52440, loss = 21.58, batch loss = 21.50 (8.4 examples/sec; 0.952 sec/batch; 74h:03m:25s remains)
INFO - root - 2017-12-07 22:03:38.018136: step 52450, loss = 21.02, batch loss = 20.94 (8.8 examples/sec; 0.904 sec/batch; 70h:21m:36s remains)
INFO - root - 2017-12-07 22:03:47.306532: step 52460, loss = 21.13, batch loss = 21.05 (9.0 examples/sec; 0.888 sec/batch; 69h:05m:46s remains)
INFO - root - 2017-12-07 22:03:56.452341: step 52470, loss = 21.26, batch loss = 21.18 (8.6 examples/sec; 0.929 sec/batch; 72h:15m:33s remains)
INFO - root - 2017-12-07 22:04:05.872756: step 52480, loss = 21.57, batch loss = 21.49 (8.6 examples/sec; 0.932 sec/batch; 72h:31m:19s remains)
INFO - root - 2017-12-07 22:04:15.290746: step 52490, loss = 21.41, batch loss = 21.32 (8.2 examples/sec; 0.974 sec/batch; 75h:45m:12s remains)
INFO - root - 2017-12-07 22:04:24.485087: step 52500, loss = 21.48, batch loss = 21.40 (8.6 examples/sec; 0.933 sec/batch; 72h:33m:47s remains)
2017-12-07 22:04:25.478157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3742418 -4.3923945 -4.3748035 -4.3318329 -4.2900076 -4.2646179 -4.2514081 -4.254868 -4.2868834 -4.3511481 -4.4268742 -4.4585562 -4.4191618 -4.3591027 -4.3172016][-4.4643035 -4.4750056 -4.4366078 -4.3666167 -4.292223 -4.2382832 -4.2130117 -4.2236691 -4.2737317 -4.3557091 -4.4417892 -4.4685378 -4.4092569 -4.3253608 -4.2690277][-4.5236883 -4.5200415 -4.4646873 -4.3811836 -4.2907205 -4.2214108 -4.1901178 -4.2063408 -4.26899 -4.3543367 -4.4303665 -4.4336572 -4.3426619 -4.2343531 -4.1760268][-4.5317178 -4.5093155 -4.4392333 -4.3508973 -4.2590976 -4.1873 -4.1559706 -4.17842 -4.2540188 -4.3443522 -4.4065242 -4.3801985 -4.2529168 -4.1198821 -4.0634985][-4.4847922 -4.4462228 -4.35909 -4.2633562 -4.1729221 -4.106564 -4.0820246 -4.114532 -4.2053385 -4.3042789 -4.3565135 -4.310863 -4.16775 -4.0307355 -3.9888175][-4.4241095 -4.368783 -4.2643113 -4.1571951 -4.0651689 -4.0050888 -3.9901259 -4.0355086 -4.1424341 -4.2517781 -4.3041687 -4.2616029 -4.1356483 -4.0250916 -4.0109682][-4.3979673 -4.3278708 -4.2048755 -4.0789094 -3.9781742 -3.9207525 -3.9151609 -3.9717071 -4.0896072 -4.2101321 -4.2782793 -4.2648258 -4.1813593 -4.1112118 -4.1189394][-4.402987 -4.3208132 -4.1791115 -4.0315824 -3.9182096 -3.8611495 -3.8605285 -3.9170675 -4.032763 -4.1598654 -4.253365 -4.2872581 -4.2595758 -4.2322044 -4.2520795][-4.4251108 -4.3322029 -4.1754937 -4.0122862 -3.8902745 -3.8327315 -3.8322263 -3.8836358 -3.9940612 -4.12774 -4.2480416 -4.3280821 -4.3499851 -4.3536277 -4.37162][-4.4553604 -4.364295 -4.20522 -4.0382109 -3.9130144 -3.8538039 -3.851455 -3.9013324 -4.0121994 -4.1556644 -4.2943988 -4.3986917 -4.4439206 -4.4530931 -4.4532084][-4.4837255 -4.4060497 -4.2629576 -4.1124425 -3.9957495 -3.9356241 -3.9270823 -3.9715238 -4.0804734 -4.2245178 -4.3608475 -4.458601 -4.496623 -4.4929061 -4.47399][-4.51389 -4.4564719 -4.343236 -4.2249527 -4.1285329 -4.0694284 -4.0507584 -4.083581 -4.1791959 -4.3056364 -4.4158049 -4.4815478 -4.49276 -4.4704695 -4.4401183][-4.5404863 -4.5038762 -4.4254932 -4.3447261 -4.2753797 -4.2243757 -4.2005095 -4.2212911 -4.2955132 -4.3900766 -4.46001 -4.4840941 -4.4657316 -4.4283996 -4.39383][-4.5253286 -4.5048537 -4.4606261 -4.4168515 -4.3779025 -4.3439593 -4.3245354 -4.3375125 -4.3881979 -4.4475532 -4.4792614 -4.4719338 -4.4370646 -4.3957314 -4.3635817][-4.4682503 -4.4614406 -4.4434505 -4.4249024 -4.4076061 -4.3910246 -4.3828087 -4.3941746 -4.42548 -4.4556131 -4.4611 -4.4396172 -4.4039025 -4.369112 -4.3446093]]...]
INFO - root - 2017-12-07 22:04:34.887219: step 52510, loss = 21.50, batch loss = 21.41 (8.1 examples/sec; 0.984 sec/batch; 76h:29m:52s remains)
INFO - root - 2017-12-07 22:04:44.144433: step 52520, loss = 21.48, batch loss = 21.39 (8.5 examples/sec; 0.941 sec/batch; 73h:12m:43s remains)
INFO - root - 2017-12-07 22:04:53.532497: step 52530, loss = 21.42, batch loss = 21.34 (8.8 examples/sec; 0.907 sec/batch; 70h:32m:29s remains)
INFO - root - 2017-12-07 22:05:02.900243: step 52540, loss = 21.33, batch loss = 21.25 (9.2 examples/sec; 0.873 sec/batch; 67h:53m:03s remains)
INFO - root - 2017-12-07 22:05:12.232713: step 52550, loss = 21.25, batch loss = 21.17 (8.7 examples/sec; 0.917 sec/batch; 71h:16m:37s remains)
INFO - root - 2017-12-07 22:05:21.591534: step 52560, loss = 21.60, batch loss = 21.52 (8.4 examples/sec; 0.953 sec/batch; 74h:04m:22s remains)
INFO - root - 2017-12-07 22:05:30.904166: step 52570, loss = 21.38, batch loss = 21.30 (8.7 examples/sec; 0.920 sec/batch; 71h:33m:02s remains)
INFO - root - 2017-12-07 22:05:40.323121: step 52580, loss = 21.52, batch loss = 21.44 (8.6 examples/sec; 0.933 sec/batch; 72h:32m:27s remains)
INFO - root - 2017-12-07 22:05:49.737684: step 52590, loss = 21.97, batch loss = 21.88 (8.2 examples/sec; 0.973 sec/batch; 75h:39m:46s remains)
INFO - root - 2017-12-07 22:05:59.010735: step 52600, loss = 21.35, batch loss = 21.27 (8.4 examples/sec; 0.956 sec/batch; 74h:21m:39s remains)
2017-12-07 22:05:59.898211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4263058 -4.3697581 -4.2825232 -4.2022071 -4.1639433 -4.1743064 -4.1965842 -4.202877 -4.2085285 -4.2261524 -4.238174 -4.2479753 -4.27378 -4.3134027 -4.3489494][-4.4396443 -4.3679161 -4.268683 -4.1888227 -4.1574183 -4.1708546 -4.1930242 -4.2040668 -4.2211194 -4.2464323 -4.2571912 -4.2560749 -4.2696714 -4.2994981 -4.3290792][-4.4404874 -4.365303 -4.2656045 -4.1896648 -4.1622262 -4.1734252 -4.193728 -4.2162786 -4.2565804 -4.3008943 -4.3149133 -4.3000193 -4.2906609 -4.295908 -4.3079705][-4.432241 -4.3607306 -4.2658491 -4.190001 -4.1557527 -4.1530676 -4.1630235 -4.1957846 -4.2672882 -4.3456774 -4.3788586 -4.3615804 -4.3321714 -4.3086944 -4.2944269][-4.4206405 -4.35475 -4.2672195 -4.18874 -4.1368122 -4.1061516 -4.0924058 -4.1220288 -4.2182159 -4.3363471 -4.406177 -4.4080696 -4.374361 -4.3294234 -4.2884755][-4.4077444 -4.347158 -4.2680984 -4.1860075 -4.1120157 -4.04924 -4.0069518 -4.0247316 -4.1329432 -4.2820058 -4.3902135 -4.4222107 -4.399415 -4.3471684 -4.2878389][-4.3895912 -4.3330145 -4.2625203 -4.1791754 -4.08736 -4.0000525 -3.9403327 -3.9517958 -4.0619245 -4.2206779 -4.3469372 -4.3998775 -4.3933125 -4.3484454 -4.2870584][-4.3633537 -4.3048534 -4.2407055 -4.1640425 -4.0685081 -3.9688382 -3.9004662 -3.9099946 -4.0152245 -4.1668267 -4.291482 -4.354208 -4.3660522 -4.33985 -4.2889204][-4.3357177 -4.2697372 -4.2106252 -4.1515369 -4.0715461 -3.9743049 -3.8987386 -3.8969185 -3.9828076 -4.1139989 -4.2293997 -4.3017149 -4.3391237 -4.3394222 -4.3047848][-4.3218527 -4.2449708 -4.18878 -4.1524315 -4.1025925 -4.0232458 -3.9452832 -3.9223504 -3.9749818 -4.0744004 -4.1750293 -4.2588043 -4.3284397 -4.3617568 -4.343729][-4.3168626 -4.2311382 -4.1717415 -4.1498432 -4.1313767 -4.0840778 -4.0226178 -3.9930923 -4.0180078 -4.084095 -4.1648369 -4.2535415 -4.3501782 -4.4123445 -4.4034767][-4.3175898 -4.2212753 -4.1497421 -4.1272092 -4.1313787 -4.1240759 -4.1018691 -4.0937538 -4.1116185 -4.1515708 -4.2079635 -4.29203 -4.4021587 -4.4776273 -4.4649873][-4.318954 -4.210155 -4.1218629 -4.0891967 -4.1077528 -4.1389585 -4.163342 -4.1894956 -4.2151222 -4.238235 -4.2689543 -4.3355589 -4.4380283 -4.5076814 -4.4856687][-4.3171048 -4.201108 -4.098022 -4.0544233 -4.0821829 -4.1446147 -4.2074327 -4.2598953 -4.2907777 -4.2974086 -4.2997742 -4.3353577 -4.4088569 -4.4593306 -4.43511][-4.3131413 -4.1978064 -4.0879841 -4.0378356 -4.0707507 -4.153451 -4.2400436 -4.3012891 -4.3241243 -4.3096275 -4.2802768 -4.2754626 -4.3072782 -4.3361616 -4.3244972]]...]
INFO - root - 2017-12-07 22:06:09.208632: step 52610, loss = 21.65, batch loss = 21.57 (8.7 examples/sec; 0.922 sec/batch; 71h:41m:37s remains)
INFO - root - 2017-12-07 22:06:18.609690: step 52620, loss = 21.47, batch loss = 21.38 (8.2 examples/sec; 0.981 sec/batch; 76h:14m:01s remains)
INFO - root - 2017-12-07 22:06:27.983808: step 52630, loss = 21.45, batch loss = 21.37 (8.1 examples/sec; 0.989 sec/batch; 76h:52m:55s remains)
INFO - root - 2017-12-07 22:06:37.071077: step 52640, loss = 21.27, batch loss = 21.18 (8.8 examples/sec; 0.913 sec/batch; 70h:58m:01s remains)
INFO - root - 2017-12-07 22:06:46.406869: step 52650, loss = 21.56, batch loss = 21.47 (8.6 examples/sec; 0.926 sec/batch; 71h:58m:51s remains)
INFO - root - 2017-12-07 22:06:55.771703: step 52660, loss = 21.80, batch loss = 21.72 (8.5 examples/sec; 0.943 sec/batch; 73h:19m:34s remains)
INFO - root - 2017-12-07 22:07:05.095661: step 52670, loss = 21.26, batch loss = 21.18 (8.5 examples/sec; 0.946 sec/batch; 73h:33m:00s remains)
INFO - root - 2017-12-07 22:07:14.524492: step 52680, loss = 21.70, batch loss = 21.62 (8.8 examples/sec; 0.910 sec/batch; 70h:44m:58s remains)
INFO - root - 2017-12-07 22:07:23.801103: step 52690, loss = 21.39, batch loss = 21.31 (8.6 examples/sec; 0.928 sec/batch; 72h:08m:21s remains)
INFO - root - 2017-12-07 22:07:33.040303: step 52700, loss = 21.57, batch loss = 21.48 (8.5 examples/sec; 0.936 sec/batch; 72h:46m:50s remains)
2017-12-07 22:07:33.979308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2968078 -4.2196188 -4.1796412 -4.2081447 -4.2784009 -4.3577514 -4.4091768 -4.4586725 -4.5309558 -4.5866585 -4.5841169 -4.5191517 -4.4152346 -4.3213148 -4.26761][-4.3770547 -4.3245196 -4.3116894 -4.3562889 -4.4224277 -4.4694552 -4.4673576 -4.4678335 -4.5199351 -4.5855465 -4.61711 -4.5928969 -4.5202761 -4.4395633 -4.3784709][-4.4548597 -4.4378605 -4.4435148 -4.4768715 -4.5052361 -4.4908342 -4.4233685 -4.3762302 -4.4109526 -4.4949484 -4.5740137 -4.6075559 -4.5857239 -4.5331016 -4.47218][-4.5091934 -4.5218649 -4.5271955 -4.5278182 -4.5008388 -4.4221048 -4.3041053 -4.2273421 -4.2528353 -4.3538489 -4.4699464 -4.5528064 -4.583066 -4.5661907 -4.5181875][-4.5229506 -4.5460987 -4.5344825 -4.4976611 -4.421864 -4.298542 -4.1593051 -4.0783162 -4.10726 -4.2224121 -4.3546276 -4.4583216 -4.5189 -4.5290871 -4.5008607][-4.499805 -4.5132222 -4.4762087 -4.4045525 -4.2872539 -4.1339169 -3.9951684 -3.9326873 -3.9826512 -4.1172876 -4.255703 -4.3599973 -4.4298639 -4.45448 -4.4484549][-4.4579344 -4.450479 -4.3921227 -4.2949448 -4.1393952 -3.959677 -3.8277903 -3.7899997 -3.8661714 -4.0237174 -4.1705766 -4.2736039 -4.3456545 -4.3797922 -4.3973207][-4.4034448 -4.3768225 -4.3146138 -4.2071075 -4.0233912 -3.8259861 -3.7056668 -3.6898708 -3.7837868 -3.9556658 -4.1051517 -4.2004008 -4.2637038 -4.2997236 -4.3357043][-4.3441105 -4.3159909 -4.2790866 -4.19105 -4.0065508 -3.8081336 -3.7007856 -3.6954577 -3.7909617 -3.9601002 -4.098455 -4.1722674 -4.2104034 -4.2317348 -4.2687216][-4.3037834 -4.2911181 -4.3001194 -4.2608666 -4.1132183 -3.9391844 -3.8440018 -3.8313973 -3.90728 -4.0560169 -4.1776986 -4.2321448 -4.2451315 -4.2410541 -4.2543335][-4.2997932 -4.302547 -4.3441105 -4.3513837 -4.2566934 -4.1234775 -4.0427389 -4.0165834 -4.0608449 -4.1793942 -4.2885127 -4.3392224 -4.3421268 -4.3193359 -4.3028283][-4.3237295 -4.3352995 -4.3876591 -4.4236965 -4.382937 -4.3031731 -4.2478237 -4.2149959 -4.2266479 -4.3051896 -4.3924327 -4.4385266 -4.4363647 -4.4041052 -4.3685703][-4.3567791 -4.3747811 -4.4274487 -4.4766793 -4.4767165 -4.4464021 -4.4227948 -4.3970771 -4.3883638 -4.4265738 -4.4802632 -4.5072074 -4.4936395 -4.4537668 -4.4092689][-4.375998 -4.4019036 -4.4536715 -4.5055852 -4.5257769 -4.5235686 -4.518106 -4.5009408 -4.4838095 -4.4928436 -4.5150037 -4.5186453 -4.4924459 -4.4492383 -4.4063621][-4.3672104 -4.4000769 -4.4540567 -4.5081859 -4.537869 -4.5478845 -4.5481811 -4.5352569 -4.5158777 -4.5084939 -4.5081291 -4.4932513 -4.4593749 -4.4186463 -4.3839989]]...]
INFO - root - 2017-12-07 22:07:43.419533: step 52710, loss = 21.22, batch loss = 21.13 (7.8 examples/sec; 1.021 sec/batch; 79h:21m:57s remains)
INFO - root - 2017-12-07 22:07:52.833457: step 52720, loss = 21.62, batch loss = 21.53 (8.4 examples/sec; 0.955 sec/batch; 74h:12m:52s remains)
INFO - root - 2017-12-07 22:08:02.241695: step 52730, loss = 20.93, batch loss = 20.85 (8.2 examples/sec; 0.975 sec/batch; 75h:44m:29s remains)
INFO - root - 2017-12-07 22:08:11.636507: step 52740, loss = 21.54, batch loss = 21.46 (8.3 examples/sec; 0.965 sec/batch; 74h:58m:00s remains)
INFO - root - 2017-12-07 22:08:20.981904: step 52750, loss = 21.79, batch loss = 21.71 (8.2 examples/sec; 0.978 sec/batch; 75h:58m:24s remains)
INFO - root - 2017-12-07 22:08:30.288004: step 52760, loss = 21.97, batch loss = 21.89 (8.4 examples/sec; 0.957 sec/batch; 74h:23m:09s remains)
INFO - root - 2017-12-07 22:08:39.528561: step 52770, loss = 21.27, batch loss = 21.18 (8.5 examples/sec; 0.936 sec/batch; 72h:44m:36s remains)
INFO - root - 2017-12-07 22:08:48.980387: step 52780, loss = 21.56, batch loss = 21.48 (8.8 examples/sec; 0.910 sec/batch; 70h:41m:12s remains)
INFO - root - 2017-12-07 22:08:58.360322: step 52790, loss = 21.35, batch loss = 21.27 (8.9 examples/sec; 0.896 sec/batch; 69h:39m:12s remains)
INFO - root - 2017-12-07 22:09:07.751684: step 52800, loss = 21.23, batch loss = 21.14 (8.8 examples/sec; 0.907 sec/batch; 70h:27m:37s remains)
2017-12-07 22:09:08.769454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4678097 -4.5128536 -4.5348186 -4.5565839 -4.5871773 -4.6031508 -4.5823393 -4.5448084 -4.5260229 -4.5346127 -4.56348 -4.5973449 -4.6074991 -4.5925727 -4.554513][-4.5229836 -4.5886707 -4.6265612 -4.664391 -4.7124262 -4.7327938 -4.692183 -4.620554 -4.5797114 -4.5956249 -4.6565051 -4.7189069 -4.7212992 -4.6737871 -4.5993776][-4.55056 -4.6284747 -4.6784644 -4.7242861 -4.7726455 -4.7774444 -4.7089934 -4.6053848 -4.5437322 -4.5622053 -4.6481843 -4.7362189 -4.7296462 -4.6530142 -4.5538197][-4.5452704 -4.6186423 -4.6578236 -4.6871934 -4.7143407 -4.6960974 -4.6068034 -4.4910231 -4.4268694 -4.4504681 -4.5509076 -4.6500878 -4.6355019 -4.5427661 -4.4431667][-4.5274196 -4.5801773 -4.5790582 -4.5515609 -4.522305 -4.4724746 -4.3831582 -4.2953339 -4.2677288 -4.31825 -4.4279823 -4.5168247 -4.4878483 -4.388927 -4.3130307][-4.5168614 -4.555737 -4.5125561 -4.4074688 -4.2846441 -4.1642413 -4.0617428 -4.0182128 -4.0629935 -4.1736851 -4.3035135 -4.3753924 -4.3353763 -4.2429852 -4.2007227][-4.516407 -4.5587158 -4.499876 -4.3406534 -4.1288962 -3.9168007 -3.7643361 -3.7265887 -3.8245897 -3.9999948 -4.1663589 -4.2440562 -4.2141581 -4.1373377 -4.1135268][-4.508925 -4.5646377 -4.5253224 -4.3743844 -4.1453342 -3.8901086 -3.682838 -3.5953784 -3.6724646 -3.8602881 -4.0503922 -4.1476312 -4.1407628 -4.0791273 -4.0468245][-4.4832211 -4.5481296 -4.5408053 -4.4401989 -4.2634172 -4.0429149 -3.8387182 -3.7172039 -3.7350163 -3.8710389 -4.0423646 -4.1561236 -4.1796546 -4.1320052 -4.0668116][-4.4668336 -4.5391793 -4.55877 -4.5026245 -4.3786197 -4.2104807 -4.0471048 -3.9383028 -3.9312258 -4.0231209 -4.1674328 -4.2879758 -4.3325291 -4.2898178 -4.1852493][-4.4694567 -4.5548453 -4.5998635 -4.5762868 -4.4848161 -4.3523774 -4.2212415 -4.1315603 -4.120008 -4.18811 -4.3114595 -4.4297361 -4.4790854 -4.4329629 -4.3044786][-4.4708986 -4.5692525 -4.6373615 -4.6420207 -4.5775204 -4.4750528 -4.3690782 -4.2889872 -4.2648225 -4.301199 -4.3863597 -4.4786181 -4.5203719 -4.478488 -4.3580618][-4.4564981 -4.5588884 -4.6418819 -4.6688905 -4.6304603 -4.5605555 -4.4865074 -4.4272342 -4.4005432 -4.41134 -4.4543824 -4.5070114 -4.5322218 -4.4986639 -4.4001932][-4.4230394 -4.5150161 -4.598793 -4.6431212 -4.6355495 -4.60373 -4.5660563 -4.5351739 -4.5202565 -4.5264611 -4.5502782 -4.5742416 -4.5818954 -4.5545211 -4.4802771][-4.3829713 -4.4523993 -4.5250058 -4.5809188 -4.6031737 -4.602941 -4.5902166 -4.5782208 -4.576036 -4.5903792 -4.6135387 -4.62757 -4.6272321 -4.6101642 -4.5644617]]...]
INFO - root - 2017-12-07 22:09:17.848523: step 52810, loss = 21.42, batch loss = 21.33 (10.6 examples/sec; 0.753 sec/batch; 58h:31m:02s remains)
INFO - root - 2017-12-07 22:09:27.195796: step 52820, loss = 21.72, batch loss = 21.64 (8.9 examples/sec; 0.899 sec/batch; 69h:50m:04s remains)
INFO - root - 2017-12-07 22:09:36.585632: step 52830, loss = 21.35, batch loss = 21.26 (8.8 examples/sec; 0.909 sec/batch; 70h:35m:42s remains)
INFO - root - 2017-12-07 22:09:45.988938: step 52840, loss = 21.47, batch loss = 21.38 (8.3 examples/sec; 0.961 sec/batch; 74h:41m:23s remains)
INFO - root - 2017-12-07 22:09:55.391518: step 52850, loss = 21.75, batch loss = 21.67 (7.9 examples/sec; 1.017 sec/batch; 79h:01m:01s remains)
INFO - root - 2017-12-07 22:10:04.790707: step 52860, loss = 21.08, batch loss = 21.00 (8.2 examples/sec; 0.979 sec/batch; 76h:01m:35s remains)
INFO - root - 2017-12-07 22:10:14.253722: step 52870, loss = 21.06, batch loss = 20.98 (8.4 examples/sec; 0.949 sec/batch; 73h:44m:58s remains)
INFO - root - 2017-12-07 22:10:23.625948: step 52880, loss = 21.43, batch loss = 21.35 (9.2 examples/sec; 0.872 sec/batch; 67h:42m:14s remains)
INFO - root - 2017-12-07 22:10:33.035755: step 52890, loss = 21.54, batch loss = 21.46 (8.6 examples/sec; 0.935 sec/batch; 72h:38m:58s remains)
INFO - root - 2017-12-07 22:10:42.329401: step 52900, loss = 21.48, batch loss = 21.40 (8.9 examples/sec; 0.901 sec/batch; 69h:59m:45s remains)
2017-12-07 22:10:43.282777: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4387331 -4.4963708 -4.5578794 -4.5959935 -4.5982227 -4.5743279 -4.5442004 -4.5350924 -4.5618844 -4.6080308 -4.63284 -4.6216216 -4.5902061 -4.5477638 -4.4953055][-4.4553838 -4.5216994 -4.5827889 -4.6063066 -4.5862865 -4.5372648 -4.4802594 -4.4550323 -4.4833093 -4.5528054 -4.6165667 -4.6316986 -4.599391 -4.5418434 -4.475462][-4.4621515 -4.5276289 -4.5767312 -4.5754757 -4.5289822 -4.4581294 -4.378706 -4.3359828 -4.3610764 -4.4516544 -4.5575213 -4.6079597 -4.587553 -4.5305567 -4.4632964][-4.4614005 -4.5177503 -4.5450463 -4.5095944 -4.4333024 -4.3469987 -4.2602539 -4.2097435 -4.2335334 -4.3383408 -4.4748011 -4.5565243 -4.5577116 -4.5199022 -4.4652281][-4.4589882 -4.5068946 -4.51106 -4.4431257 -4.3423095 -4.24741 -4.1615024 -4.1076875 -4.125349 -4.22752 -4.3708935 -4.4693694 -4.4971952 -4.4933281 -4.4620833][-4.4597335 -4.4995241 -4.4807811 -4.3845263 -4.2652664 -4.1649537 -4.0779448 -4.0174189 -4.0170879 -4.0959768 -4.22694 -4.3308682 -4.3877497 -4.4259 -4.4254713][-4.4617767 -4.4929018 -4.4532285 -4.3319092 -4.1957197 -4.0866055 -3.9958496 -3.9325719 -3.9208839 -3.9845715 -4.1069975 -4.2175655 -4.3002357 -4.368227 -4.3863878][-4.462019 -4.4922476 -4.4489589 -4.3258696 -4.1907535 -4.0769796 -3.9780817 -3.9136479 -3.9055629 -3.9723258 -4.097373 -4.2203083 -4.3218441 -4.3944783 -4.399765][-4.4614177 -4.4959974 -4.4637752 -4.3593192 -4.2393909 -4.1233635 -4.0116577 -3.9432161 -3.942142 -4.0158348 -4.1438632 -4.2814431 -4.3999577 -4.4625659 -4.4401722][-4.458478 -4.5002394 -4.4846158 -4.40344 -4.2999196 -4.182024 -4.0618515 -3.9914596 -3.9944975 -4.069119 -4.1936045 -4.3397489 -4.4672685 -4.5129223 -4.4614148][-4.4587617 -4.5106635 -4.5159359 -4.4632235 -4.3808951 -4.2722783 -4.1565819 -4.0860438 -4.0845447 -4.1479011 -4.2578917 -4.3968248 -4.5140386 -4.5355635 -4.4621878][-4.4593863 -4.5191522 -4.5423684 -4.5133724 -4.4482756 -4.353024 -4.2491732 -4.1801004 -4.171339 -4.2215042 -4.3162589 -4.4393096 -4.5348449 -4.5398293 -4.4667258][-4.4543133 -4.5142322 -4.5480337 -4.53615 -4.484364 -4.4046168 -4.3203011 -4.2615047 -4.2552814 -4.3053837 -4.3953428 -4.4963732 -4.5608134 -4.5563006 -4.500505][-4.4411335 -4.4922862 -4.5290151 -4.530407 -4.4968233 -4.4436784 -4.3923259 -4.3588977 -4.3657675 -4.4199624 -4.502656 -4.5734863 -4.6018229 -4.5869894 -4.547543][-4.4225645 -4.4587545 -4.487709 -4.491858 -4.4715762 -4.4447107 -4.4260798 -4.4192524 -4.4384952 -4.4917431 -4.5576634 -4.598444 -4.6048808 -4.591238 -4.5695233]]...]
INFO - root - 2017-12-07 22:10:52.623975: step 52910, loss = 21.61, batch loss = 21.52 (8.4 examples/sec; 0.949 sec/batch; 73h:40m:34s remains)
INFO - root - 2017-12-07 22:11:02.070733: step 52920, loss = 21.29, batch loss = 21.21 (8.3 examples/sec; 0.970 sec/batch; 75h:18m:02s remains)
INFO - root - 2017-12-07 22:11:11.539445: step 52930, loss = 21.32, batch loss = 21.24 (8.2 examples/sec; 0.978 sec/batch; 75h:54m:45s remains)
INFO - root - 2017-12-07 22:11:20.844832: step 52940, loss = 22.20, batch loss = 22.11 (8.6 examples/sec; 0.928 sec/batch; 72h:04m:46s remains)
INFO - root - 2017-12-07 22:11:30.220487: step 52950, loss = 21.73, batch loss = 21.65 (8.8 examples/sec; 0.910 sec/batch; 70h:40m:26s remains)
INFO - root - 2017-12-07 22:11:39.640506: step 52960, loss = 21.67, batch loss = 21.59 (8.5 examples/sec; 0.946 sec/batch; 73h:25m:48s remains)
INFO - root - 2017-12-07 22:11:49.203387: step 52970, loss = 21.16, batch loss = 21.08 (8.0 examples/sec; 0.995 sec/batch; 77h:17m:31s remains)
INFO - root - 2017-12-07 22:11:58.600902: step 52980, loss = 21.78, batch loss = 21.70 (8.0 examples/sec; 1.006 sec/batch; 78h:06m:41s remains)
INFO - root - 2017-12-07 22:12:07.894909: step 52990, loss = 21.54, batch loss = 21.46 (7.9 examples/sec; 1.011 sec/batch; 78h:28m:36s remains)
INFO - root - 2017-12-07 22:12:17.320844: step 53000, loss = 21.98, batch loss = 21.90 (8.2 examples/sec; 0.975 sec/batch; 75h:43m:19s remains)
2017-12-07 22:12:18.267170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4141431 -4.4241138 -4.4377489 -4.4506054 -4.4617224 -4.4774418 -4.4990811 -4.5143313 -4.5154119 -4.5107174 -4.5104294 -4.5190248 -4.5336971 -4.5444303 -4.5406971][-4.42926 -4.44838 -4.4700012 -4.4915814 -4.5105443 -4.5314112 -4.5544195 -4.5637836 -4.5525122 -4.5405121 -4.5436463 -4.5601578 -4.5793767 -4.5907025 -4.58572][-4.449924 -4.4736414 -4.4980631 -4.5265026 -4.5515304 -4.57355 -4.58925 -4.5813632 -4.54327 -4.5133352 -4.5223255 -4.5555296 -4.5863452 -4.6007767 -4.597506][-4.4603443 -4.4820395 -4.5033126 -4.5334482 -4.5593491 -4.5775194 -4.5787649 -4.5431828 -4.467154 -4.4132118 -4.4328885 -4.4957728 -4.552465 -4.5789614 -4.5814905][-4.4294519 -4.4370332 -4.4527125 -4.482389 -4.5080357 -4.5235705 -4.5117269 -4.4425063 -4.318543 -4.2312927 -4.2591248 -4.36248 -4.4639606 -4.5227771 -4.5452018][-4.3586431 -4.3464351 -4.359776 -4.3935318 -4.419466 -4.4330568 -4.4125223 -4.3119183 -4.1392593 -4.0177426 -4.050765 -4.195096 -4.3448162 -4.4450021 -4.4972386][-4.2692094 -4.2485495 -4.274353 -4.3181286 -4.3382568 -4.3411775 -4.3124137 -4.1934419 -3.9936669 -3.8551655 -3.896951 -4.0701842 -4.2497139 -4.3764219 -4.4506683][-4.2047019 -4.18685 -4.2301455 -4.2806988 -4.2851615 -4.2681026 -4.2364783 -4.1263208 -3.9440901 -3.8248639 -3.8839278 -4.0638189 -4.2367611 -4.3543873 -4.425827][-4.2223167 -4.2157063 -4.2679095 -4.3067665 -4.2841325 -4.246634 -4.2239485 -4.1483574 -4.0227184 -3.9570568 -4.0427961 -4.2107849 -4.3437963 -4.4144573 -4.4483986][-4.3253117 -4.3237062 -4.3606415 -4.3640532 -4.306704 -4.2523346 -4.2415519 -4.203835 -4.1398354 -4.1293473 -4.2391238 -4.3906016 -4.48161 -4.5053167 -4.4963565][-4.4523225 -4.4442983 -4.4477978 -4.4113336 -4.32967 -4.2716126 -4.2738981 -4.2648034 -4.2409039 -4.2576938 -4.3651385 -4.4887958 -4.5480385 -4.54811 -4.5209532][-4.53803 -4.5290742 -4.508214 -4.4522171 -4.3706965 -4.3266773 -4.3438663 -4.3564405 -4.3541203 -4.3741226 -4.453506 -4.534709 -4.5623484 -4.5468616 -4.5167322][-4.5205822 -4.5225129 -4.5035405 -4.4575205 -4.398767 -4.3761611 -4.4025059 -4.4280276 -4.4387412 -4.4530826 -4.4971495 -4.5382843 -4.5433488 -4.521749 -4.4963956][-4.4118948 -4.4265237 -4.4255347 -4.40709 -4.3827782 -4.3822551 -4.4107637 -4.438561 -4.4540825 -4.4619575 -4.479176 -4.4949865 -4.4938416 -4.4804811 -4.467988][-4.3254285 -4.3448148 -4.3599348 -4.3639107 -4.3638673 -4.3747406 -4.3961473 -4.4155269 -4.4282289 -4.4351082 -4.4432764 -4.4511051 -4.4537797 -4.4520226 -4.4502907]]...]
INFO - root - 2017-12-07 22:12:27.630192: step 53010, loss = 21.48, batch loss = 21.40 (8.6 examples/sec; 0.927 sec/batch; 71h:57m:29s remains)
INFO - root - 2017-12-07 22:12:37.034199: step 53020, loss = 21.60, batch loss = 21.52 (8.6 examples/sec; 0.926 sec/batch; 71h:53m:46s remains)
INFO - root - 2017-12-07 22:12:46.389710: step 53030, loss = 21.06, batch loss = 20.98 (8.4 examples/sec; 0.948 sec/batch; 73h:35m:02s remains)
INFO - root - 2017-12-07 22:12:55.790896: step 53040, loss = 21.38, batch loss = 21.30 (8.6 examples/sec; 0.927 sec/batch; 71h:58m:47s remains)
INFO - root - 2017-12-07 22:13:05.235747: step 53050, loss = 21.74, batch loss = 21.65 (8.7 examples/sec; 0.922 sec/batch; 71h:35m:58s remains)
INFO - root - 2017-12-07 22:13:14.536767: step 53060, loss = 22.30, batch loss = 22.22 (8.6 examples/sec; 0.930 sec/batch; 72h:10m:39s remains)
INFO - root - 2017-12-07 22:13:23.926284: step 53070, loss = 21.15, batch loss = 21.07 (8.6 examples/sec; 0.926 sec/batch; 71h:54m:21s remains)
INFO - root - 2017-12-07 22:13:33.258895: step 53080, loss = 21.06, batch loss = 20.98 (9.1 examples/sec; 0.879 sec/batch; 68h:13m:49s remains)
INFO - root - 2017-12-07 22:13:42.690075: step 53090, loss = 21.40, batch loss = 21.31 (8.8 examples/sec; 0.912 sec/batch; 70h:45m:00s remains)
INFO - root - 2017-12-07 22:13:52.082802: step 53100, loss = 21.06, batch loss = 20.98 (8.7 examples/sec; 0.916 sec/batch; 71h:03m:19s remains)
2017-12-07 22:13:53.031329: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5772214 -4.614984 -4.6333137 -4.6096539 -4.5564232 -4.4990034 -4.4939075 -4.5139136 -4.5068188 -4.5091558 -4.5277257 -4.5378914 -4.5477338 -4.5561838 -4.5553331][-4.6087751 -4.6222544 -4.6318817 -4.6063938 -4.5371046 -4.457314 -4.4470954 -4.4803858 -4.4966183 -4.5227356 -4.5502477 -4.5606265 -4.5769944 -4.5995679 -4.6123676][-4.6095967 -4.6100788 -4.6256309 -4.6070719 -4.5286074 -4.4334331 -4.4110389 -4.4421024 -4.4758391 -4.5363765 -4.5923557 -4.6115136 -4.6323967 -4.6610465 -4.6772614][-4.5761032 -4.572413 -4.6011672 -4.5878272 -4.4996376 -4.3894362 -4.3474236 -4.3656635 -4.4117074 -4.5073328 -4.5926909 -4.6203942 -4.6475 -4.6860337 -4.7092361][-4.4984159 -4.502924 -4.54591 -4.5331268 -4.4294434 -4.2963295 -4.2256808 -4.2258916 -4.279952 -4.4033823 -4.5090146 -4.5407219 -4.5760365 -4.6343303 -4.6815138][-4.3976588 -4.4137788 -4.4720111 -4.4638233 -4.349823 -4.1898541 -4.0774851 -4.0453873 -4.1011724 -4.2467031 -4.3692541 -4.4105754 -4.4574313 -4.5394473 -4.6182871][-4.2916794 -4.3073168 -4.3653989 -4.3580093 -4.2426357 -4.0704913 -3.9262085 -3.8609071 -3.9131694 -4.0698667 -4.2045383 -4.2590771 -4.3172903 -4.4148936 -4.5186229][-4.2125645 -4.2173243 -4.269505 -4.2644262 -4.1666446 -4.0192184 -3.8795457 -3.7932618 -3.8227234 -3.9619832 -4.0891004 -4.1512365 -4.2165661 -4.3184447 -4.4283037][-4.2166815 -4.2147346 -4.2653203 -4.2716889 -4.2082009 -4.1160049 -4.0193229 -3.9366412 -3.9352927 -4.0290937 -4.1247454 -4.1798363 -4.24071 -4.3293772 -4.4194942][-4.2627339 -4.2502613 -4.2930303 -4.3095956 -4.2798858 -4.2425761 -4.2009997 -4.1437759 -4.126102 -4.176795 -4.2372384 -4.2796364 -4.3310552 -4.4015541 -4.4702554][-4.3522353 -4.3372931 -4.37202 -4.3936458 -4.3855944 -4.3804502 -4.374104 -4.3445411 -4.3275728 -4.348887 -4.377883 -4.4038849 -4.4417367 -4.4957013 -4.5455794][-4.4638524 -4.4567971 -4.4843063 -4.5059242 -4.5080156 -4.5139484 -4.5205512 -4.5093246 -4.4984822 -4.5037684 -4.5106478 -4.5196209 -4.5404863 -4.5735097 -4.5991449][-4.5073543 -4.5045991 -4.5182705 -4.5293827 -4.5325122 -4.5398617 -4.5492606 -4.5488338 -4.5455217 -4.5452609 -4.5412498 -4.5372543 -4.5410819 -4.5539894 -4.5597076][-4.4701586 -4.4719896 -4.4777117 -4.4812722 -4.4822044 -4.4858894 -4.4918284 -4.4940095 -4.4935384 -4.4917006 -4.4858513 -4.477983 -4.4714637 -4.46838 -4.4614377][-4.3961439 -4.3990583 -4.4023371 -4.4036918 -4.4038553 -4.4048347 -4.4067874 -4.407414 -4.4069142 -4.4058337 -4.40296 -4.3978195 -4.3903513 -4.3821149 -4.3724465]]...]
INFO - root - 2017-12-07 22:14:02.235940: step 53110, loss = 22.08, batch loss = 22.00 (10.1 examples/sec; 0.789 sec/batch; 61h:11m:44s remains)
INFO - root - 2017-12-07 22:14:11.453999: step 53120, loss = 21.04, batch loss = 20.95 (8.5 examples/sec; 0.942 sec/batch; 73h:04m:52s remains)
INFO - root - 2017-12-07 22:14:20.871884: step 53130, loss = 20.85, batch loss = 20.76 (8.2 examples/sec; 0.977 sec/batch; 75h:49m:55s remains)
INFO - root - 2017-12-07 22:14:30.287410: step 53140, loss = 21.62, batch loss = 21.53 (8.8 examples/sec; 0.914 sec/batch; 70h:54m:41s remains)
INFO - root - 2017-12-07 22:14:39.640455: step 53150, loss = 21.05, batch loss = 20.96 (8.5 examples/sec; 0.938 sec/batch; 72h:47m:24s remains)
INFO - root - 2017-12-07 22:14:48.888976: step 53160, loss = 21.21, batch loss = 21.13 (8.9 examples/sec; 0.903 sec/batch; 70h:03m:11s remains)
INFO - root - 2017-12-07 22:14:58.303785: step 53170, loss = 21.20, batch loss = 21.12 (8.7 examples/sec; 0.916 sec/batch; 71h:03m:21s remains)
INFO - root - 2017-12-07 22:15:07.697178: step 53180, loss = 21.47, batch loss = 21.39 (8.6 examples/sec; 0.926 sec/batch; 71h:49m:09s remains)
INFO - root - 2017-12-07 22:15:17.052026: step 53190, loss = 21.22, batch loss = 21.13 (8.2 examples/sec; 0.979 sec/batch; 75h:59m:41s remains)
INFO - root - 2017-12-07 22:15:26.421734: step 53200, loss = 21.59, batch loss = 21.51 (8.2 examples/sec; 0.970 sec/batch; 75h:14m:29s remains)
2017-12-07 22:15:27.366982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4149261 -4.3993568 -4.3448381 -4.3089514 -4.34501 -4.4446864 -4.5384817 -4.5652146 -4.4999185 -4.3680739 -4.2160759 -4.09414 -3.9962926 -3.9161279 -3.8771253][-4.3720884 -4.3588562 -4.2986989 -4.2546806 -4.2898936 -4.3982825 -4.5045214 -4.5467706 -4.5061049 -4.4015579 -4.2778282 -4.1684709 -4.0711575 -4.0084476 -4.0023251][-4.3034053 -4.3024917 -4.243032 -4.1918087 -4.2172718 -4.3122191 -4.409008 -4.4512072 -4.4240055 -4.3460455 -4.2596216 -4.1873503 -4.1289034 -4.1245475 -4.1884589][-4.2484617 -4.2621369 -4.2012615 -4.1384335 -4.1429191 -4.1998425 -4.2642126 -4.2927852 -4.2810273 -4.239521 -4.20358 -4.1843023 -4.1796875 -4.2473421 -4.384975][-4.2093992 -4.240634 -4.1826916 -4.1076994 -4.0778832 -4.0772862 -4.0981927 -4.1265006 -4.160007 -4.18743 -4.2194996 -4.2521105 -4.2893434 -4.3945103 -4.5537114][-4.2173495 -4.2738171 -4.2205043 -4.1263118 -4.0400457 -3.9587953 -3.922816 -3.9596028 -4.0628996 -4.1806283 -4.2868552 -4.3728065 -4.4460392 -4.556067 -4.6787124][-4.3009276 -4.36977 -4.3218188 -4.2067442 -4.0491424 -3.8677888 -3.7565851 -3.783006 -3.94213 -4.1437616 -4.3259029 -4.4766383 -4.5972733 -4.6985335 -4.7529135][-4.4414206 -4.5107193 -4.4737892 -4.3451943 -4.1162791 -3.8309319 -3.6324272 -3.6180377 -3.7902293 -4.038002 -4.2738485 -4.4834228 -4.6474333 -4.7390151 -4.7358122][-4.6094637 -4.6724339 -4.6433887 -4.5078168 -4.2232847 -3.8591609 -3.5920608 -3.5281169 -3.6784546 -3.9222665 -4.16529 -4.3910646 -4.5715647 -4.6616321 -4.6452284][-4.7389727 -4.7862668 -4.7656178 -4.6355314 -4.3370428 -3.9520314 -3.6623843 -3.5660756 -3.6816301 -3.8889971 -4.105463 -4.3123441 -4.4875097 -4.5834036 -4.5837817][-4.7466445 -4.7939868 -4.7976651 -4.6999145 -4.4373384 -4.0932817 -3.831784 -3.736815 -3.833045 -4.0080976 -4.1901374 -4.3601708 -4.5076451 -4.5951953 -4.60718][-4.670393 -4.7286768 -4.7659731 -4.7180457 -4.5275321 -4.2671919 -4.0674381 -4.0027571 -4.1009007 -4.2550607 -4.3979363 -4.5077424 -4.5912013 -4.6381788 -4.6459775][-4.5618057 -4.6299367 -4.698976 -4.7044506 -4.5952997 -4.4261546 -4.2925777 -4.2639947 -4.3671403 -4.5020318 -4.5961151 -4.6296687 -4.6287374 -4.6209016 -4.6219997][-4.4571362 -4.5341315 -4.62535 -4.6734123 -4.6337862 -4.5399175 -4.4571757 -4.450017 -4.541007 -4.6456447 -4.6877913 -4.6489406 -4.5702929 -4.5162139 -4.5148487][-4.3896756 -4.4696136 -4.565321 -4.6319265 -4.6366816 -4.59573 -4.5511384 -4.5549722 -4.6198659 -4.6822662 -4.6724529 -4.5733852 -4.4354978 -4.3461084 -4.3422041]]...]
INFO - root - 2017-12-07 22:15:36.605170: step 53210, loss = 21.60, batch loss = 21.52 (8.7 examples/sec; 0.914 sec/batch; 70h:56m:50s remains)
INFO - root - 2017-12-07 22:15:45.899506: step 53220, loss = 21.75, batch loss = 21.67 (8.7 examples/sec; 0.917 sec/batch; 71h:07m:54s remains)
INFO - root - 2017-12-07 22:15:55.283943: step 53230, loss = 21.51, batch loss = 21.43 (8.4 examples/sec; 0.951 sec/batch; 73h:48m:21s remains)
INFO - root - 2017-12-07 22:16:04.728567: step 53240, loss = 20.99, batch loss = 20.91 (8.6 examples/sec; 0.935 sec/batch; 72h:31m:35s remains)
INFO - root - 2017-12-07 22:16:14.083240: step 53250, loss = 21.30, batch loss = 21.21 (8.8 examples/sec; 0.907 sec/batch; 70h:20m:56s remains)
INFO - root - 2017-12-07 22:16:23.547906: step 53260, loss = 21.50, batch loss = 21.42 (8.7 examples/sec; 0.916 sec/batch; 71h:04m:18s remains)
INFO - root - 2017-12-07 22:16:32.997884: step 53270, loss = 21.79, batch loss = 21.71 (8.4 examples/sec; 0.953 sec/batch; 73h:53m:00s remains)
INFO - root - 2017-12-07 22:16:42.510742: step 53280, loss = 21.65, batch loss = 21.57 (8.1 examples/sec; 0.993 sec/batch; 77h:02m:20s remains)
INFO - root - 2017-12-07 22:16:51.889030: step 53290, loss = 21.31, batch loss = 21.23 (8.0 examples/sec; 1.002 sec/batch; 77h:41m:57s remains)
INFO - root - 2017-12-07 22:17:01.354847: step 53300, loss = 21.31, batch loss = 21.23 (8.6 examples/sec; 0.926 sec/batch; 71h:48m:29s remains)
2017-12-07 22:17:02.404559: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3690052 -4.3663368 -4.3668122 -4.3664918 -4.3660231 -4.3656321 -4.36544 -4.3659554 -4.3674617 -4.3698955 -4.3713117 -4.3706012 -4.3677535 -4.3658495 -4.3656354][-4.3824606 -4.3830929 -4.3834348 -4.380928 -4.3763456 -4.3714857 -4.368495 -4.3690624 -4.3735828 -4.3795352 -4.3824573 -4.3793383 -4.37174 -4.3654895 -4.3656635][-4.3976674 -4.401268 -4.4015121 -4.3957043 -4.3851595 -4.3748903 -4.3704548 -4.37455 -4.3848948 -4.3942838 -4.3957253 -4.387167 -4.3736181 -4.3648529 -4.3673077][-4.4140673 -4.4172158 -4.4149117 -4.4037905 -4.3848448 -4.368928 -4.3675351 -4.3820519 -4.4013796 -4.4113007 -4.4057512 -4.3888769 -4.3724327 -4.3672848 -4.37584][-4.4112167 -4.4005857 -4.3853531 -4.3609881 -4.3293676 -4.3082261 -4.3142123 -4.3447375 -4.3748226 -4.3813181 -4.3615947 -4.3327456 -4.317111 -4.3240256 -4.3483849][-4.3719573 -4.3342829 -4.2933183 -4.2448072 -4.1953144 -4.1699595 -4.1864314 -4.2356143 -4.2753863 -4.2731857 -4.2350783 -4.1980796 -4.1976824 -4.2382231 -4.2989426][-4.3370428 -4.2691917 -4.199327 -4.1221361 -4.0512104 -4.0207386 -4.0463233 -4.1106033 -4.1553092 -4.1426716 -4.0913982 -4.0579467 -4.0865445 -4.1726618 -4.276794][-4.3174119 -4.2393417 -4.1572642 -4.0641284 -3.9770885 -3.9394383 -3.9661031 -4.0320535 -4.0718474 -4.0509286 -3.9957626 -3.9696894 -4.0136585 -4.1212211 -4.2484541][-4.3086505 -4.2419581 -4.1726093 -4.08757 -3.9989872 -3.9547544 -3.9741888 -4.0294085 -4.0561075 -4.0298615 -3.9789395 -3.9589453 -4.0008717 -4.1046815 -4.234695][-4.2949252 -4.2629218 -4.2290845 -4.1748538 -4.1034641 -4.0598674 -4.0689392 -4.1053362 -4.1171594 -4.0906167 -4.0479565 -4.0301828 -4.0606556 -4.1469717 -4.2654324][-4.263185 -4.273747 -4.2805662 -4.2667131 -4.2286687 -4.1973996 -4.197875 -4.2140131 -4.2128253 -4.19103 -4.1639061 -4.15491 -4.179379 -4.2454572 -4.33968][-4.2297616 -4.2693539 -4.3113284 -4.3380141 -4.3384757 -4.3243237 -4.3176527 -4.3192582 -4.3142967 -4.3028755 -4.2928395 -4.2952766 -4.3204203 -4.37187 -4.4403439][-4.1901994 -4.2528758 -4.3238912 -4.3827562 -4.4085917 -4.3991113 -4.3808522 -4.3742685 -4.3756132 -4.3778396 -4.379992 -4.3891945 -4.4172344 -4.4614768 -4.5064483][-4.14803 -4.228898 -4.3215027 -4.3979754 -4.4310188 -4.4149666 -4.386416 -4.3770752 -4.384582 -4.3951797 -4.4025922 -4.4145517 -4.4430928 -4.4764614 -4.4941735][-4.1339583 -4.2250481 -4.3286347 -4.4094071 -4.4398203 -4.4193692 -4.3886561 -4.3765435 -4.3770475 -4.3802261 -4.3851924 -4.4013515 -4.4318562 -4.4543839 -4.4471388]]...]
INFO - root - 2017-12-07 22:17:11.918704: step 53310, loss = 21.57, batch loss = 21.48 (8.2 examples/sec; 0.974 sec/batch; 75h:32m:03s remains)
INFO - root - 2017-12-07 22:17:21.263011: step 53320, loss = 21.68, batch loss = 21.60 (8.0 examples/sec; 0.995 sec/batch; 77h:09m:49s remains)
INFO - root - 2017-12-07 22:17:30.525206: step 53330, loss = 21.65, batch loss = 21.56 (8.7 examples/sec; 0.922 sec/batch; 71h:29m:46s remains)
INFO - root - 2017-12-07 22:17:40.082493: step 53340, loss = 21.33, batch loss = 21.25 (8.8 examples/sec; 0.912 sec/batch; 70h:42m:47s remains)
INFO - root - 2017-12-07 22:17:49.377895: step 53350, loss = 21.53, batch loss = 21.44 (8.8 examples/sec; 0.911 sec/batch; 70h:37m:20s remains)
INFO - root - 2017-12-07 22:17:58.720344: step 53360, loss = 21.68, batch loss = 21.60 (8.9 examples/sec; 0.904 sec/batch; 70h:05m:06s remains)
INFO - root - 2017-12-07 22:18:08.203791: step 53370, loss = 21.89, batch loss = 21.80 (9.2 examples/sec; 0.872 sec/batch; 67h:37m:27s remains)
INFO - root - 2017-12-07 22:18:17.553052: step 53380, loss = 21.74, batch loss = 21.66 (9.1 examples/sec; 0.878 sec/batch; 68h:04m:08s remains)
INFO - root - 2017-12-07 22:18:26.986406: step 53390, loss = 21.16, batch loss = 21.08 (8.6 examples/sec; 0.929 sec/batch; 71h:59m:17s remains)
INFO - root - 2017-12-07 22:18:36.388672: step 53400, loss = 21.24, batch loss = 21.16 (8.1 examples/sec; 0.982 sec/batch; 76h:09m:48s remains)
2017-12-07 22:18:37.411762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5334544 -4.516089 -4.520802 -4.5305181 -4.5534315 -4.5618234 -4.5219059 -4.4860139 -4.4860883 -4.4904861 -4.4382386 -4.3452024 -4.3149152 -4.3504891 -4.3724108][-4.5284638 -4.5154452 -4.5304947 -4.5578666 -4.5907259 -4.5927491 -4.5360479 -4.4877605 -4.4869537 -4.4987888 -4.4501572 -4.3630328 -4.3427391 -4.3827991 -4.4111271][-4.5030904 -4.4936118 -4.5225849 -4.563302 -4.5880651 -4.564177 -4.4916849 -4.43887 -4.4407372 -4.4709029 -4.4417224 -4.3759813 -4.3708444 -4.41475 -4.4445615][-4.470901 -4.4544153 -4.4861183 -4.5233941 -4.5188 -4.4577661 -4.373045 -4.3211021 -4.3297887 -4.38497 -4.38082 -4.3368282 -4.3489065 -4.4062004 -4.4460998][-4.4446387 -4.4184442 -4.4450822 -4.4712191 -4.4332495 -4.3324661 -4.2225585 -4.1580062 -4.1797514 -4.2714262 -4.3007112 -4.2716951 -4.285665 -4.3446765 -4.3899775][-4.4411726 -4.4101224 -4.4299507 -4.438026 -4.3596735 -4.2057281 -4.0473504 -3.9658325 -4.0322552 -4.193707 -4.2668819 -4.2402868 -4.2271538 -4.2459159 -4.2709379][-4.455997 -4.4238663 -4.4269996 -4.4024282 -4.274745 -4.0658569 -3.8638146 -3.7893043 -3.93104 -4.1727138 -4.2831917 -4.2522864 -4.1884913 -4.1326203 -4.1147819][-4.4710636 -4.4358196 -4.4153042 -4.3598843 -4.20202 -3.9711361 -3.7647533 -3.7276623 -3.937927 -4.2153969 -4.3212214 -4.2698793 -4.1639361 -4.0567093 -4.0134535][-4.4643626 -4.4197845 -4.3850446 -4.3267794 -4.1832566 -3.9807491 -3.8181551 -3.8289618 -4.0577922 -4.2977285 -4.3553195 -4.2822323 -4.1694641 -4.0644293 -4.0321336][-4.4388037 -4.3913536 -4.3641052 -4.3315153 -4.2306237 -4.0839691 -3.9813585 -4.0195346 -4.2083731 -4.3614588 -4.3660851 -4.2952523 -4.2137713 -4.146812 -4.1405654][-4.4325557 -4.3972011 -4.3869233 -4.3741646 -4.3029547 -4.1989646 -4.1365376 -4.1679811 -4.2774544 -4.34087 -4.3183808 -4.2824845 -4.254365 -4.23311 -4.2509027][-4.4331141 -4.4067802 -4.3969474 -4.3814759 -4.3208289 -4.2451534 -4.2036343 -4.2127929 -4.2545919 -4.2667017 -4.2493625 -4.2524204 -4.2648954 -4.2785234 -4.3186522][-4.428452 -4.4046483 -4.3783164 -4.3386259 -4.2727194 -4.217299 -4.195137 -4.1965752 -4.2118206 -4.2162352 -4.2178516 -4.2392321 -4.2598081 -4.2816839 -4.3288617][-4.4194307 -4.40193 -4.3626742 -4.300518 -4.2276034 -4.1887684 -4.1909389 -4.2075467 -4.2285585 -4.2431989 -4.25514 -4.2710581 -4.2719188 -4.2762051 -4.3124647][-4.4020576 -4.3925333 -4.3534036 -4.2912455 -4.230238 -4.2142267 -4.2397141 -4.2739954 -4.3023343 -4.3206744 -4.3324461 -4.3361635 -4.3191609 -4.3075895 -4.3295493]]...]
INFO - root - 2017-12-07 22:18:46.826703: step 53410, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.963 sec/batch; 74h:38m:07s remains)
INFO - root - 2017-12-07 22:18:56.301237: step 53420, loss = 21.46, batch loss = 21.38 (8.2 examples/sec; 0.970 sec/batch; 75h:12m:58s remains)
INFO - root - 2017-12-07 22:19:05.884335: step 53430, loss = 21.16, batch loss = 21.07 (7.8 examples/sec; 1.026 sec/batch; 79h:31m:16s remains)
INFO - root - 2017-12-07 22:19:15.357550: step 53440, loss = 21.56, batch loss = 21.47 (7.7 examples/sec; 1.033 sec/batch; 80h:02m:38s remains)
INFO - root - 2017-12-07 22:19:24.620578: step 53450, loss = 21.56, batch loss = 21.48 (8.4 examples/sec; 0.953 sec/batch; 73h:51m:44s remains)
INFO - root - 2017-12-07 22:19:34.048361: step 53460, loss = 21.32, batch loss = 21.24 (8.3 examples/sec; 0.962 sec/batch; 74h:33m:48s remains)
INFO - root - 2017-12-07 22:19:43.601858: step 53470, loss = 21.58, batch loss = 21.49 (8.3 examples/sec; 0.966 sec/batch; 74h:54m:26s remains)
INFO - root - 2017-12-07 22:19:52.862754: step 53480, loss = 21.52, batch loss = 21.43 (8.8 examples/sec; 0.907 sec/batch; 70h:17m:14s remains)
INFO - root - 2017-12-07 22:20:02.275054: step 53490, loss = 21.36, batch loss = 21.27 (9.0 examples/sec; 0.889 sec/batch; 68h:52m:41s remains)
INFO - root - 2017-12-07 22:20:11.687023: step 53500, loss = 21.35, batch loss = 21.26 (9.0 examples/sec; 0.891 sec/batch; 69h:03m:19s remains)
2017-12-07 22:20:12.686726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3864412 -4.3827024 -4.3819551 -4.3841143 -4.3904386 -4.399972 -4.4106245 -4.4199119 -4.4265938 -4.4320416 -4.4379811 -4.4479289 -4.4638233 -4.4820871 -4.4942441][-4.4616938 -4.4588294 -4.4582286 -4.4615293 -4.4717135 -4.4872093 -4.5040016 -4.5173969 -4.52865 -4.5394731 -4.5511394 -4.5661058 -4.5877438 -4.6132298 -4.6272516][-4.5610218 -4.5649052 -4.5636225 -4.5650764 -4.5778441 -4.5977778 -4.6167321 -4.6294742 -4.6447854 -4.6648388 -4.6833248 -4.6998076 -4.7146835 -4.7328925 -4.7401857][-4.6147871 -4.6201491 -4.6132431 -4.6075888 -4.6215749 -4.6439686 -4.6557188 -4.6560378 -4.6673923 -4.6947675 -4.720201 -4.7416706 -4.7486129 -4.759347 -4.759758][-4.5168014 -4.5075479 -4.4848895 -4.4643927 -4.4794188 -4.5011783 -4.4917588 -4.4649334 -4.4636497 -4.5058556 -4.5608912 -4.6209784 -4.6513848 -4.67712 -4.6766829][-4.304996 -4.2751245 -4.2397842 -4.2088928 -4.2306666 -4.2509904 -4.2164164 -4.1624484 -4.1573734 -4.2341676 -4.3444929 -4.4708209 -4.5454178 -4.5933552 -4.5873065][-4.1570973 -4.1101027 -4.0687027 -4.032455 -4.0610394 -4.0791039 -4.0270648 -3.9586449 -3.9572821 -4.0655122 -4.2106404 -4.3762879 -4.4825497 -4.5455456 -4.5378432][-4.1401238 -4.0698042 -4.0181422 -3.977978 -4.0045662 -4.0093012 -3.943342 -3.868403 -3.8702135 -3.9916098 -4.1443005 -4.313653 -4.4314656 -4.5083985 -4.5161939][-4.219635 -4.134038 -4.0712638 -4.0308146 -4.0547967 -4.05525 -3.9933205 -3.9273224 -3.9348724 -4.048542 -4.1849117 -4.3283677 -4.4359317 -4.5145793 -4.5359178][-4.3424106 -4.2716966 -4.2214012 -4.20258 -4.2427344 -4.2628155 -4.2272019 -4.1821742 -4.1917853 -4.277739 -4.3738194 -4.46433 -4.5352616 -4.5890603 -4.6033015][-4.4387422 -4.4071684 -4.3908253 -4.4016175 -4.4554915 -4.4923239 -4.4858546 -4.4661527 -4.478344 -4.5319195 -4.5824251 -4.6172709 -4.6423182 -4.6608262 -4.6585312][-4.4946618 -4.4891148 -4.49274 -4.5115304 -4.5561833 -4.5909 -4.6017933 -4.60133 -4.6164622 -4.6509733 -4.6736112 -4.6783657 -4.6761308 -4.6739583 -4.6634135][-4.4927621 -4.4862261 -4.4913893 -4.5063429 -4.5333028 -4.5539179 -4.5643744 -4.5704546 -4.5846467 -4.6058674 -4.6145663 -4.6102371 -4.6045332 -4.6030068 -4.5978866][-4.4093356 -4.3932309 -4.3928113 -4.4048443 -4.4230194 -4.4346466 -4.4409671 -4.4490647 -4.4622626 -4.4736819 -4.4741211 -4.4687319 -4.4687057 -4.4782186 -4.4850421][-4.3332462 -4.3131094 -4.3081512 -4.3171053 -4.3312087 -4.3406773 -4.3466096 -4.3568921 -4.3703103 -4.3762226 -4.3703413 -4.3630166 -4.3656535 -4.3799915 -4.3944411]]...]
INFO - root - 2017-12-07 22:20:21.974913: step 53510, loss = 21.81, batch loss = 21.73 (8.5 examples/sec; 0.939 sec/batch; 72h:46m:33s remains)
INFO - root - 2017-12-07 22:20:31.358416: step 53520, loss = 21.48, batch loss = 21.40 (8.8 examples/sec; 0.909 sec/batch; 70h:27m:59s remains)
INFO - root - 2017-12-07 22:20:40.711924: step 53530, loss = 21.65, batch loss = 21.57 (8.3 examples/sec; 0.965 sec/batch; 74h:47m:53s remains)
INFO - root - 2017-12-07 22:20:50.120164: step 53540, loss = 21.82, batch loss = 21.74 (8.3 examples/sec; 0.969 sec/batch; 75h:06m:35s remains)
INFO - root - 2017-12-07 22:20:59.630088: step 53550, loss = 21.60, batch loss = 21.52 (8.2 examples/sec; 0.976 sec/batch; 75h:35m:30s remains)
INFO - root - 2017-12-07 22:21:09.091178: step 53560, loss = 21.49, batch loss = 21.41 (8.0 examples/sec; 1.002 sec/batch; 77h:37m:07s remains)
INFO - root - 2017-12-07 22:21:18.372506: step 53570, loss = 22.07, batch loss = 21.99 (8.5 examples/sec; 0.945 sec/batch; 73h:13m:57s remains)
INFO - root - 2017-12-07 22:21:27.801531: step 53580, loss = 21.47, batch loss = 21.38 (8.6 examples/sec; 0.928 sec/batch; 71h:55m:44s remains)
INFO - root - 2017-12-07 22:21:37.202110: step 53590, loss = 21.31, batch loss = 21.23 (8.4 examples/sec; 0.951 sec/batch; 73h:41m:46s remains)
INFO - root - 2017-12-07 22:21:46.610352: step 53600, loss = 21.12, batch loss = 21.04 (8.3 examples/sec; 0.966 sec/batch; 74h:52m:20s remains)
2017-12-07 22:21:47.515391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.41289 -4.4223576 -4.4558258 -4.4825993 -4.50977 -4.5549927 -4.5941525 -4.6191053 -4.6404319 -4.6474257 -4.6226969 -4.556088 -4.4619451 -4.3688893 -4.3252482][-4.4401116 -4.4833436 -4.5495586 -4.5982413 -4.6275735 -4.658071 -4.6851115 -4.711833 -4.7571559 -4.7944846 -4.791501 -4.7366114 -4.6419888 -4.538126 -4.4767723][-4.4210882 -4.4901586 -4.5882473 -4.6534896 -4.6617236 -4.6352291 -4.6141825 -4.640254 -4.7313008 -4.8273115 -4.8723879 -4.84762 -4.7638593 -4.6616364 -4.5928316][-4.4056683 -4.4861455 -4.597157 -4.6510544 -4.6055441 -4.4834652 -4.390698 -4.4305754 -4.5915022 -4.75772 -4.8532476 -4.8589029 -4.7872758 -4.6872554 -4.6141367][-4.4392772 -4.5190072 -4.608151 -4.60062 -4.4654822 -4.2308044 -4.0639095 -4.1399117 -4.3898339 -4.6238265 -4.7584782 -4.7836146 -4.7207942 -4.6156688 -4.5313134][-4.483882 -4.5593534 -4.5998616 -4.5014138 -4.2697639 -3.9371424 -3.7010627 -3.8165593 -4.1488395 -4.4389024 -4.609849 -4.653738 -4.6003132 -4.48284 -4.3810964][-4.5066004 -4.5829444 -4.5720506 -4.3917618 -4.0885348 -3.6949246 -3.4031863 -3.5355675 -3.91949 -4.250237 -4.4582777 -4.5182042 -4.4714465 -4.3391275 -4.2198296][-4.511735 -4.5917578 -4.5589728 -4.3526158 -4.0349121 -3.6302397 -3.3079672 -3.4253423 -3.8147588 -4.1616654 -4.3911934 -4.448132 -4.3928161 -4.2485113 -4.1187415][-4.5269246 -4.6127696 -4.5986128 -4.4350262 -4.1678228 -3.8069525 -3.498723 -3.5822194 -3.9288347 -4.2458792 -4.4474854 -4.46925 -4.3921061 -4.2501211 -4.1240921][-4.5685968 -4.6504116 -4.6644077 -4.5637512 -4.3680606 -4.0794964 -3.8248858 -3.8870549 -4.169487 -4.4190197 -4.5539083 -4.5346403 -4.4486032 -4.3347669 -4.2313776][-4.6114535 -4.6800175 -4.7047977 -4.6425562 -4.5031519 -4.2915258 -4.11153 -4.1650229 -4.3771687 -4.5475011 -4.6175351 -4.5829678 -4.5159779 -4.447701 -4.3796158][-4.6127591 -4.6735005 -4.6969566 -4.6478658 -4.5469584 -4.4120903 -4.3110251 -4.3582931 -4.5013561 -4.6035166 -4.633348 -4.6080832 -4.5674372 -4.5294809 -4.4838567][-4.5766196 -4.6333971 -4.6495132 -4.6043153 -4.5322919 -4.4623041 -4.4221792 -4.458693 -4.5446415 -4.6038566 -4.6199989 -4.6111708 -4.5891175 -4.5584178 -4.5151329][-4.5300059 -4.5783358 -4.5806284 -4.5361247 -4.4874883 -4.462379 -4.4566793 -4.475275 -4.51769 -4.5550694 -4.5723295 -4.577179 -4.5651579 -4.533392 -4.4865127][-4.5050974 -4.5385218 -4.5231724 -4.4775929 -4.4432869 -4.4341378 -4.4316745 -4.4249277 -4.4364643 -4.4673162 -4.492002 -4.5056968 -4.499754 -4.4682441 -4.420465]]...]
INFO - root - 2017-12-07 22:21:56.990849: step 53610, loss = 20.92, batch loss = 20.84 (8.3 examples/sec; 0.966 sec/batch; 74h:49m:57s remains)
INFO - root - 2017-12-07 22:22:06.395838: step 53620, loss = 21.45, batch loss = 21.36 (8.4 examples/sec; 0.948 sec/batch; 73h:24m:44s remains)
INFO - root - 2017-12-07 22:22:15.667794: step 53630, loss = 21.75, batch loss = 21.67 (8.4 examples/sec; 0.947 sec/batch; 73h:23m:11s remains)
INFO - root - 2017-12-07 22:22:24.989337: step 53640, loss = 21.28, batch loss = 21.20 (8.8 examples/sec; 0.908 sec/batch; 70h:21m:42s remains)
INFO - root - 2017-12-07 22:22:34.355520: step 53650, loss = 20.99, batch loss = 20.91 (8.7 examples/sec; 0.916 sec/batch; 70h:56m:24s remains)
INFO - root - 2017-12-07 22:22:43.624959: step 53660, loss = 21.39, batch loss = 21.30 (8.6 examples/sec; 0.932 sec/batch; 72h:12m:12s remains)
INFO - root - 2017-12-07 22:22:52.881350: step 53670, loss = 21.26, batch loss = 21.17 (8.5 examples/sec; 0.945 sec/batch; 73h:13m:03s remains)
INFO - root - 2017-12-07 22:23:02.107375: step 53680, loss = 21.32, batch loss = 21.23 (8.7 examples/sec; 0.917 sec/batch; 70h:59m:39s remains)
INFO - root - 2017-12-07 22:23:11.412578: step 53690, loss = 21.61, batch loss = 21.53 (9.4 examples/sec; 0.852 sec/batch; 66h:01m:22s remains)
INFO - root - 2017-12-07 22:23:20.698011: step 53700, loss = 21.89, batch loss = 21.81 (9.2 examples/sec; 0.871 sec/batch; 67h:29m:08s remains)
2017-12-07 22:23:21.622622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5268397 -4.4101329 -4.2450209 -4.1318564 -4.1344514 -4.2330317 -4.3722072 -4.4867177 -4.5211654 -4.4601321 -4.3790259 -4.3446631 -4.3763781 -4.4581513 -4.5394793][-4.5037169 -4.426981 -4.3086047 -4.2306108 -4.2449551 -4.3336782 -4.4350247 -4.4914188 -4.4712315 -4.37702 -4.2948956 -4.2883234 -4.3699307 -4.503037 -4.6130581][-4.4023814 -4.3722224 -4.3135629 -4.27892 -4.2999392 -4.3678131 -4.4305654 -4.4429736 -4.390893 -4.2944741 -4.2349205 -4.2654777 -4.384696 -4.5384331 -4.6419129][-4.2796645 -4.2975154 -4.2995238 -4.3037338 -4.3233747 -4.361239 -4.3864841 -4.3697996 -4.3042712 -4.2263823 -4.2049828 -4.2703972 -4.3920231 -4.5101986 -4.55418][-4.1798582 -4.2314286 -4.266726 -4.2839394 -4.2935724 -4.30566 -4.3086686 -4.2832527 -4.227632 -4.1876397 -4.2106743 -4.2995043 -4.3916035 -4.4370456 -4.3948908][-4.1335249 -4.2014637 -4.2380333 -4.2386146 -4.2197719 -4.2017508 -4.1929216 -4.1789856 -4.1630263 -4.1795936 -4.2496758 -4.3490038 -4.3989868 -4.37575 -4.2644792][-4.1212039 -4.1894255 -4.2129083 -4.1858168 -4.1310921 -4.0792794 -4.0626974 -4.0696607 -4.1011205 -4.1672063 -4.2689605 -4.364089 -4.3850513 -4.339211 -4.2157421][-4.1326661 -4.1932015 -4.20088 -4.1534791 -4.074986 -4.0034328 -3.9877424 -4.0148373 -4.0780563 -4.1615233 -4.2607369 -4.3373475 -4.3456187 -4.3164983 -4.2285318][-4.1521 -4.2097783 -4.2235613 -4.1905341 -4.1257463 -4.0621734 -4.0537295 -4.0882649 -4.1498389 -4.2110076 -4.2752848 -4.3204784 -4.3237715 -4.31416 -4.2635665][-4.1793232 -4.24565 -4.2824903 -4.2863741 -4.2625194 -4.2318306 -4.2378025 -4.2673321 -4.3013592 -4.3217039 -4.342999 -4.3589344 -4.3571095 -4.3513427 -4.3102026][-4.2320371 -4.3129978 -4.3691077 -4.4037595 -4.4187675 -4.4241934 -4.44257 -4.4586372 -4.4537835 -4.4318309 -4.4171095 -4.4140062 -4.4094744 -4.3978591 -4.3425512][-4.2764778 -4.3617353 -4.42092 -4.4701357 -4.5116415 -4.5438094 -4.5615373 -4.5466313 -4.4923983 -4.4289989 -4.3901134 -4.3846793 -4.3925042 -4.3907933 -4.3366709][-4.2960558 -4.355998 -4.39826 -4.4488344 -4.5056868 -4.5514407 -4.5593648 -4.5103016 -4.4153476 -4.3303647 -4.2920609 -4.3025322 -4.330193 -4.3451061 -4.3071928][-4.2688527 -4.2787118 -4.2926941 -4.33936 -4.4070468 -4.4625983 -4.4669366 -4.4003587 -4.2911706 -4.2096648 -4.1911926 -4.2226181 -4.2640204 -4.2920132 -4.2763][-4.2004166 -4.1688504 -4.1692042 -4.2171831 -4.289474 -4.3467555 -4.3539033 -4.2943234 -4.202445 -4.14354 -4.1441693 -4.183475 -4.2204561 -4.2481537 -4.2432742]]...]
INFO - root - 2017-12-07 22:23:31.042000: step 53710, loss = 21.05, batch loss = 20.96 (8.5 examples/sec; 0.944 sec/batch; 73h:06m:04s remains)
INFO - root - 2017-12-07 22:23:40.448170: step 53720, loss = 21.67, batch loss = 21.59 (8.8 examples/sec; 0.905 sec/batch; 70h:07m:06s remains)
INFO - root - 2017-12-07 22:23:49.620977: step 53730, loss = 21.43, batch loss = 21.34 (8.7 examples/sec; 0.923 sec/batch; 71h:28m:20s remains)
INFO - root - 2017-12-07 22:23:59.080904: step 53740, loss = 21.72, batch loss = 21.64 (8.4 examples/sec; 0.953 sec/batch; 73h:49m:55s remains)
INFO - root - 2017-12-07 22:24:08.596617: step 53750, loss = 21.63, batch loss = 21.55 (8.2 examples/sec; 0.974 sec/batch; 75h:23m:02s remains)
INFO - root - 2017-12-07 22:24:17.915536: step 53760, loss = 21.72, batch loss = 21.64 (8.7 examples/sec; 0.917 sec/batch; 71h:00m:43s remains)
INFO - root - 2017-12-07 22:24:27.380941: step 53770, loss = 21.44, batch loss = 21.35 (8.2 examples/sec; 0.979 sec/batch; 75h:46m:25s remains)
INFO - root - 2017-12-07 22:24:36.883117: step 53780, loss = 22.04, batch loss = 21.96 (8.3 examples/sec; 0.969 sec/batch; 75h:03m:01s remains)
INFO - root - 2017-12-07 22:24:46.398243: step 53790, loss = 21.62, batch loss = 21.53 (8.4 examples/sec; 0.956 sec/batch; 74h:00m:14s remains)
INFO - root - 2017-12-07 22:24:55.737711: step 53800, loss = 21.34, batch loss = 21.26 (8.7 examples/sec; 0.918 sec/batch; 71h:02m:14s remains)
2017-12-07 22:24:56.619651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4397817 -4.48702 -4.5655742 -4.6222382 -4.6232338 -4.5769434 -4.5576625 -4.5877414 -4.6169806 -4.61952 -4.553503 -4.4187212 -4.3220758 -4.3588614 -4.484664][-4.5082226 -4.5933843 -4.7146187 -4.7795806 -4.7467136 -4.6575818 -4.6126475 -4.6337781 -4.6680846 -4.6758413 -4.6240282 -4.5097156 -4.4155374 -4.4336433 -4.5305181][-4.5511746 -4.6582127 -4.7894754 -4.8234205 -4.7223773 -4.5695415 -4.4914875 -4.5095062 -4.5666895 -4.609107 -4.6060071 -4.5569091 -4.505414 -4.5195904 -4.5857911][-4.5922194 -4.7056494 -4.8155646 -4.7859569 -4.594739 -4.3708653 -4.2585506 -4.2640343 -4.3368092 -4.4187937 -4.4719825 -4.4977055 -4.50722 -4.541275 -4.6035347][-4.6279521 -4.7275739 -4.7985177 -4.7061052 -4.4445362 -4.1751022 -4.0420403 -4.0247469 -4.0870609 -4.1798062 -4.2629838 -4.3403945 -4.4010258 -4.4637828 -4.5492005][-4.6456518 -4.7162361 -4.7479453 -4.6150594 -4.3253193 -4.0465269 -3.9092402 -3.8759742 -3.9127953 -3.9814315 -4.05539 -4.1499829 -4.2365079 -4.3143649 -4.421761][-4.6541977 -4.6913633 -4.692378 -4.546598 -4.2633805 -3.9974918 -3.8610251 -3.818234 -3.8297992 -3.8608396 -3.9088941 -4.0108986 -4.1257048 -4.2198453 -4.3363714][-4.6625915 -4.6775908 -4.66724 -4.5395617 -4.2941995 -4.0536823 -3.9171686 -3.8648438 -3.8491051 -3.8359153 -3.8453794 -3.9516068 -4.1093097 -4.2419896 -4.3691711][-4.6677251 -4.6732097 -4.670157 -4.5891438 -4.4151006 -4.2272196 -4.1058626 -4.0476952 -4.00446 -3.9459267 -3.907244 -3.9856479 -4.1557755 -4.3181839 -4.4577703][-4.6600137 -4.6685886 -4.6857634 -4.6638279 -4.5748348 -4.4562883 -4.3679585 -4.3130345 -4.2558141 -4.1726909 -4.0950303 -4.1201324 -4.25161 -4.3994331 -4.5326233][-4.6344852 -4.6559005 -4.6995516 -4.729619 -4.710948 -4.6538348 -4.5961933 -4.5499778 -4.4981728 -4.4218903 -4.3339834 -4.3131952 -4.3781004 -4.4687157 -4.5653954][-4.6104574 -4.6313772 -4.6831956 -4.7401423 -4.7651615 -4.7539263 -4.7221451 -4.6889772 -4.6571336 -4.610796 -4.5411906 -4.5005555 -4.5062585 -4.52861 -4.5735884][-4.6069989 -4.60743 -4.644712 -4.7009473 -4.7406416 -4.7513537 -4.7361269 -4.7162523 -4.7065392 -4.6932378 -4.6540623 -4.6186337 -4.5968838 -4.5770912 -4.5808148][-4.6135063 -4.5898438 -4.6038809 -4.6460128 -4.6822104 -4.6959066 -4.6852674 -4.6703777 -4.6707397 -4.6727095 -4.6551709 -4.6341581 -4.6133451 -4.5882235 -4.5808482][-4.5952377 -4.5597949 -4.5586443 -4.5894327 -4.618104 -4.6244335 -4.6077228 -4.58816 -4.5851803 -4.5863314 -4.5769825 -4.5653858 -4.5536847 -4.5412011 -4.5456161]]...]
INFO - root - 2017-12-07 22:25:06.117293: step 53810, loss = 21.29, batch loss = 21.20 (8.2 examples/sec; 0.974 sec/batch; 75h:25m:04s remains)
INFO - root - 2017-12-07 22:25:15.472179: step 53820, loss = 21.24, batch loss = 21.15 (8.4 examples/sec; 0.950 sec/batch; 73h:30m:32s remains)
INFO - root - 2017-12-07 22:25:24.787434: step 53830, loss = 21.42, batch loss = 21.33 (8.5 examples/sec; 0.942 sec/batch; 72h:54m:11s remains)
INFO - root - 2017-12-07 22:25:34.171975: step 53840, loss = 21.25, batch loss = 21.17 (8.3 examples/sec; 0.964 sec/batch; 74h:37m:33s remains)
INFO - root - 2017-12-07 22:25:43.621636: step 53850, loss = 21.42, batch loss = 21.33 (8.4 examples/sec; 0.955 sec/batch; 73h:55m:16s remains)
INFO - root - 2017-12-07 22:25:53.057860: step 53860, loss = 21.44, batch loss = 21.35 (8.7 examples/sec; 0.921 sec/batch; 71h:18m:04s remains)
INFO - root - 2017-12-07 22:26:02.475571: step 53870, loss = 21.95, batch loss = 21.87 (8.5 examples/sec; 0.940 sec/batch; 72h:46m:00s remains)
INFO - root - 2017-12-07 22:26:11.995214: step 53880, loss = 21.52, batch loss = 21.44 (8.8 examples/sec; 0.905 sec/batch; 70h:01m:48s remains)
INFO - root - 2017-12-07 22:26:21.559176: step 53890, loss = 21.26, batch loss = 21.18 (8.2 examples/sec; 0.978 sec/batch; 75h:39m:08s remains)
INFO - root - 2017-12-07 22:26:30.980300: step 53900, loss = 21.31, batch loss = 21.23 (8.1 examples/sec; 0.984 sec/batch; 76h:09m:41s remains)
2017-12-07 22:26:31.937243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5777578 -4.5991449 -4.6142507 -4.6159067 -4.6063066 -4.5923905 -4.5779881 -4.566052 -4.560699 -4.5658641 -4.5766568 -4.5836067 -4.5808668 -4.5723352 -4.56507][-4.5254807 -4.5647035 -4.5944872 -4.5996847 -4.5794072 -4.5419774 -4.502089 -4.4763966 -4.4713039 -4.4865861 -4.50583 -4.5144758 -4.5079679 -4.4956665 -4.4870691][-4.4497228 -4.4980345 -4.5367012 -4.5406194 -4.5049734 -4.4376416 -4.3649554 -4.3223281 -4.3193941 -4.3478727 -4.3794107 -4.3958507 -4.395731 -4.3880496 -4.3826618][-4.3644657 -4.4034581 -4.4376316 -4.4296861 -4.3745365 -4.2790713 -4.1779022 -4.1214156 -4.1221828 -4.1683121 -4.2238245 -4.2665272 -4.2918773 -4.299027 -4.2985015][-4.2598753 -4.2808452 -4.2919931 -4.254456 -4.1721292 -4.0559149 -3.9417241 -3.8824434 -3.8896282 -3.9554133 -4.04445 -4.1332126 -4.2072306 -4.2449732 -4.2574673][-4.1360836 -4.1424422 -4.1186132 -4.0369196 -3.9201455 -3.7871151 -3.669678 -3.6113899 -3.6220713 -3.7023017 -3.8253467 -3.9696684 -4.1039252 -4.181931 -4.2112741][-4.0354481 -4.0349393 -3.9818425 -3.8631127 -3.7195365 -3.5755174 -3.4547729 -3.3892872 -3.3895552 -3.4651194 -3.6043687 -3.7904942 -3.974884 -4.0892615 -4.1343284][-4.0230689 -4.0236387 -3.9617584 -3.8397782 -3.6974139 -3.5557442 -3.4321098 -3.3522305 -3.329309 -3.3760853 -3.4973114 -3.6809559 -3.8706007 -3.9925671 -4.0401211][-4.1035194 -4.1062164 -4.06629 -3.9885905 -3.8876381 -3.7688324 -3.6481178 -3.5560582 -3.509414 -3.5201149 -3.6010866 -3.7408762 -3.8884873 -3.9799802 -4.0070162][-4.2258968 -4.2274718 -4.2236013 -4.2151346 -4.1788464 -4.1007776 -3.9980807 -3.9101162 -3.8561642 -3.8429976 -3.8827548 -3.9639809 -4.047102 -4.0872717 -4.0780416][-4.2809429 -4.2735066 -4.3018427 -4.3589249 -4.386138 -4.3503242 -4.2725496 -4.2022991 -4.1616173 -4.1486583 -4.165278 -4.2015123 -4.2332339 -4.2322845 -4.1902461][-4.2503204 -4.21521 -4.248558 -4.3388333 -4.4070029 -4.4034524 -4.3520947 -4.310473 -4.301003 -4.3090224 -4.3211312 -4.3319149 -4.3323035 -4.3083611 -4.249033][-4.2008228 -4.1284218 -4.132906 -4.2117481 -4.2863574 -4.2978649 -4.267602 -4.2553072 -4.281589 -4.3174181 -4.3340716 -4.3296318 -4.3098483 -4.2774057 -4.2215185][-4.2051983 -4.1001744 -4.0560122 -4.0898213 -4.1413622 -4.15554 -4.14744 -4.1662087 -4.2239628 -4.2786765 -4.2936206 -4.2729955 -4.2407031 -4.2180119 -4.1921444][-4.2813873 -4.1635594 -4.0777626 -4.0600982 -4.0766692 -4.0869188 -4.0988731 -4.1426659 -4.2156868 -4.270185 -4.2742028 -4.2425027 -4.2113595 -4.2142639 -4.2335539]]...]
INFO - root - 2017-12-07 22:26:41.328217: step 53910, loss = 21.24, batch loss = 21.15 (8.4 examples/sec; 0.953 sec/batch; 73h:44m:00s remains)
INFO - root - 2017-12-07 22:26:50.689130: step 53920, loss = 21.48, batch loss = 21.40 (8.5 examples/sec; 0.940 sec/batch; 72h:45m:46s remains)
INFO - root - 2017-12-07 22:27:00.088849: step 53930, loss = 20.96, batch loss = 20.87 (9.6 examples/sec; 0.832 sec/batch; 64h:20m:49s remains)
INFO - root - 2017-12-07 22:27:09.424855: step 53940, loss = 21.28, batch loss = 21.20 (9.0 examples/sec; 0.893 sec/batch; 69h:05m:22s remains)
INFO - root - 2017-12-07 22:27:18.850399: step 53950, loss = 21.49, batch loss = 21.41 (8.4 examples/sec; 0.955 sec/batch; 73h:54m:39s remains)
INFO - root - 2017-12-07 22:27:28.301028: step 53960, loss = 21.27, batch loss = 21.19 (8.3 examples/sec; 0.964 sec/batch; 74h:34m:10s remains)
INFO - root - 2017-12-07 22:27:37.663483: step 53970, loss = 21.24, batch loss = 21.15 (8.7 examples/sec; 0.922 sec/batch; 71h:20m:39s remains)
INFO - root - 2017-12-07 22:27:47.020797: step 53980, loss = 21.57, batch loss = 21.48 (8.5 examples/sec; 0.946 sec/batch; 73h:11m:43s remains)
INFO - root - 2017-12-07 22:27:56.381065: step 53990, loss = 21.11, batch loss = 21.02 (8.5 examples/sec; 0.939 sec/batch; 72h:39m:07s remains)
INFO - root - 2017-12-07 22:28:05.735599: step 54000, loss = 21.17, batch loss = 21.08 (8.8 examples/sec; 0.910 sec/batch; 70h:24m:26s remains)
2017-12-07 22:28:06.755158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2697906 -4.2704449 -4.2809219 -4.28816 -4.289772 -4.2877793 -4.2855906 -4.2862597 -4.2895918 -4.294116 -4.2990618 -4.3020096 -4.2998004 -4.2933984 -4.2808862][-4.2783327 -4.2853022 -4.3038054 -4.3167863 -4.3202467 -4.3166046 -4.3110285 -4.308527 -4.30961 -4.3121867 -4.3148489 -4.3146987 -4.3093805 -4.2994618 -4.283493][-4.3022604 -4.3158469 -4.3407364 -4.3568039 -4.3586221 -4.3494635 -4.3380952 -4.3328662 -4.3345404 -4.3395944 -4.3434997 -4.3415589 -4.33141 -4.3148284 -4.291769][-4.3356977 -4.3513141 -4.3757787 -4.3883114 -4.3832574 -4.3653383 -4.347836 -4.341814 -4.3492379 -4.3640065 -4.376544 -4.3791084 -4.3680654 -4.345963 -4.3145127][-4.3613296 -4.3723903 -4.3880949 -4.3888702 -4.3699584 -4.3387032 -4.31246 -4.3050156 -4.3211555 -4.35362 -4.3865886 -4.4061947 -4.4055843 -4.3869276 -4.3531137][-4.3633351 -4.3672595 -4.3734212 -4.3621817 -4.3271089 -4.277802 -4.2372704 -4.2237225 -4.2469215 -4.2984853 -4.3563128 -4.3989115 -4.416203 -4.4103208 -4.3838072][-4.34079 -4.3426008 -4.3484049 -4.3337927 -4.2855115 -4.214108 -4.1510468 -4.1232181 -4.1469369 -4.2124982 -4.28984 -4.349535 -4.3804641 -4.3876238 -4.3724642][-4.3136473 -4.3186774 -4.3306794 -4.3177333 -4.2584872 -4.1638856 -4.0740232 -4.0277596 -4.0472903 -4.1207252 -4.2100172 -4.2760706 -4.3090487 -4.3196344 -4.3102331][-4.2978497 -4.306468 -4.32482 -4.3149209 -4.25084 -4.1452761 -4.0419626 -3.9848409 -3.9984927 -4.0712814 -4.161653 -4.2233667 -4.247344 -4.2499814 -4.2356329][-4.2939081 -4.3062229 -4.3316436 -4.3276472 -4.270123 -4.1739545 -4.0794277 -4.026022 -4.0345325 -4.095376 -4.1707706 -4.2184877 -4.2288351 -4.219202 -4.1948195][-4.2856741 -4.3070621 -4.3465047 -4.3555617 -4.31247 -4.2363176 -4.1626382 -4.1229253 -4.1297984 -4.1717091 -4.2226076 -4.2553668 -4.2589383 -4.244401 -4.2136321][-4.25434 -4.2904854 -4.3551326 -4.3888531 -4.3673053 -4.3109813 -4.256536 -4.2304397 -4.237 -4.2595143 -4.2839074 -4.3037138 -4.3092041 -4.3014059 -4.2751017][-4.2105594 -4.2597146 -4.3518872 -4.4156079 -4.4192543 -4.3788166 -4.334434 -4.3142619 -4.3167233 -4.32169 -4.3249893 -4.3371525 -4.3510056 -4.3589745 -4.3476973][-4.1883097 -4.2408075 -4.3450246 -4.4271936 -4.4488997 -4.4177995 -4.3737826 -4.3483973 -4.34026 -4.3320589 -4.3272295 -4.3415961 -4.368957 -4.3972669 -4.4069557][-4.1873932 -4.2354245 -4.3303342 -4.4090557 -4.43276 -4.4037366 -4.3577671 -4.3245955 -4.3045459 -4.2896218 -4.2885456 -4.3129177 -4.3550124 -4.4011602 -4.4294844]]...]
INFO - root - 2017-12-07 22:28:16.275578: step 54010, loss = 21.54, batch loss = 21.46 (8.6 examples/sec; 0.930 sec/batch; 71h:54m:24s remains)
INFO - root - 2017-12-07 22:28:25.443428: step 54020, loss = 21.44, batch loss = 21.35 (9.0 examples/sec; 0.885 sec/batch; 68h:26m:19s remains)
INFO - root - 2017-12-07 22:28:34.809468: step 54030, loss = 21.31, batch loss = 21.22 (8.3 examples/sec; 0.959 sec/batch; 74h:12m:03s remains)
INFO - root - 2017-12-07 22:28:44.085991: step 54040, loss = 21.51, batch loss = 21.43 (8.2 examples/sec; 0.981 sec/batch; 75h:52m:31s remains)
INFO - root - 2017-12-07 22:28:53.512027: step 54050, loss = 21.41, batch loss = 21.33 (8.9 examples/sec; 0.899 sec/batch; 69h:34m:24s remains)
INFO - root - 2017-12-07 22:29:02.946241: step 54060, loss = 21.15, batch loss = 21.07 (8.8 examples/sec; 0.906 sec/batch; 70h:05m:03s remains)
INFO - root - 2017-12-07 22:29:12.315851: step 54070, loss = 21.24, batch loss = 21.16 (8.8 examples/sec; 0.913 sec/batch; 70h:34m:35s remains)
INFO - root - 2017-12-07 22:29:21.810831: step 54080, loss = 21.71, batch loss = 21.63 (8.6 examples/sec; 0.926 sec/batch; 71h:36m:07s remains)
INFO - root - 2017-12-07 22:29:31.134115: step 54090, loss = 21.65, batch loss = 21.56 (8.5 examples/sec; 0.947 sec/batch; 73h:12m:51s remains)
INFO - root - 2017-12-07 22:29:40.432633: step 54100, loss = 21.21, batch loss = 21.12 (8.6 examples/sec; 0.925 sec/batch; 71h:34m:18s remains)
2017-12-07 22:29:41.448233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3475018 -4.2964816 -4.2542129 -4.2278485 -4.2282867 -4.24343 -4.2662549 -4.2928276 -4.324791 -4.3630085 -4.390554 -4.3891582 -4.3518386 -4.2937613 -4.2672367][-4.3462811 -4.309597 -4.2796693 -4.2600369 -4.2756038 -4.30809 -4.3465767 -4.3855157 -4.4157376 -4.4378462 -4.443686 -4.4227772 -4.3755536 -4.3243165 -4.3132329][-4.347033 -4.3067136 -4.2730412 -4.2596464 -4.2964048 -4.3498716 -4.40154 -4.4433489 -4.4635272 -4.4684339 -4.4581985 -4.4293756 -4.3894057 -4.3621883 -4.375886][-4.3550329 -4.3003964 -4.2535863 -4.2370844 -4.2772536 -4.3292551 -4.3775334 -4.4109635 -4.4171524 -4.405818 -4.38368 -4.3582921 -4.342555 -4.3526683 -4.3979816][-4.3674655 -4.3002477 -4.238152 -4.2022352 -4.2124748 -4.2298627 -4.2584028 -4.2851458 -4.2916384 -4.2801046 -4.2573466 -4.2454643 -4.2585583 -4.2978768 -4.3596158][-4.3925104 -4.3152184 -4.2343121 -4.1636949 -4.1191235 -4.0840669 -4.0919704 -4.12825 -4.1584463 -4.1637893 -4.1465259 -4.1450434 -4.1702313 -4.2102962 -4.2628422][-4.4114137 -4.3321066 -4.2427726 -4.1416464 -4.0421371 -3.9618685 -3.9591131 -4.0124917 -4.0699797 -4.087574 -4.0677342 -4.0673442 -4.0948009 -4.1284685 -4.1671629][-4.4021168 -4.3411937 -4.2616029 -4.1425848 -4.0022521 -3.9017413 -3.9043713 -3.97426 -4.0508838 -4.0743403 -4.049377 -4.0470023 -4.0739584 -4.1008587 -4.1235323][-4.3512297 -4.3138227 -4.2579718 -4.141727 -3.9934974 -3.9126024 -3.938308 -4.0222483 -4.1081386 -4.1355419 -4.1088519 -4.0979419 -4.1121612 -4.1239548 -4.1282659][-4.2582688 -4.2444181 -4.2288451 -4.1453066 -4.0305152 -3.9972351 -4.049819 -4.1381645 -4.2216678 -4.248239 -4.220521 -4.2015505 -4.2030134 -4.1989641 -4.1842055][-4.188467 -4.1969142 -4.2139421 -4.1608458 -4.084794 -4.0934787 -4.1676641 -4.2591 -4.3370433 -4.361495 -4.33682 -4.3165441 -4.3105903 -4.2936344 -4.2570124][-4.1640544 -4.183486 -4.2129626 -4.1758885 -4.1295862 -4.1654372 -4.253448 -4.3485179 -4.4203606 -4.4390845 -4.4125924 -4.3862815 -4.3720717 -4.3505011 -4.3080835][-4.1558747 -4.1882544 -4.2295494 -4.2033896 -4.1723719 -4.2085819 -4.2873836 -4.3755341 -4.4370794 -4.4464622 -4.4168878 -4.3831048 -4.360786 -4.3445754 -4.3155026][-4.1783109 -4.2190976 -4.2643452 -4.2408075 -4.2127647 -4.2359047 -4.2923312 -4.3692474 -4.42217 -4.4205289 -4.3819113 -4.3344936 -4.2964563 -4.2818356 -4.2684827][-4.2285547 -4.2641425 -4.3039126 -4.2846317 -4.2690473 -4.288414 -4.3229775 -4.3832645 -4.4257946 -4.4126158 -4.3609223 -4.2979345 -4.2425766 -4.2250004 -4.223208]]...]
INFO - root - 2017-12-07 22:29:50.818712: step 54110, loss = 21.38, batch loss = 21.30 (8.1 examples/sec; 0.987 sec/batch; 76h:19m:37s remains)
INFO - root - 2017-12-07 22:30:00.194240: step 54120, loss = 21.23, batch loss = 21.15 (8.8 examples/sec; 0.910 sec/batch; 70h:20m:08s remains)
INFO - root - 2017-12-07 22:30:09.641850: step 54130, loss = 21.22, batch loss = 21.14 (8.7 examples/sec; 0.920 sec/batch; 71h:07m:38s remains)
INFO - root - 2017-12-07 22:30:18.785060: step 54140, loss = 21.28, batch loss = 21.20 (8.6 examples/sec; 0.931 sec/batch; 72h:00m:37s remains)
INFO - root - 2017-12-07 22:30:28.215152: step 54150, loss = 21.36, batch loss = 21.28 (8.5 examples/sec; 0.941 sec/batch; 72h:44m:05s remains)
INFO - root - 2017-12-07 22:30:37.546571: step 54160, loss = 21.53, batch loss = 21.45 (8.3 examples/sec; 0.958 sec/batch; 74h:04m:53s remains)
INFO - root - 2017-12-07 22:30:46.840581: step 54170, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.948 sec/batch; 73h:19m:40s remains)
INFO - root - 2017-12-07 22:30:56.217059: step 54180, loss = 21.62, batch loss = 21.54 (8.3 examples/sec; 0.962 sec/batch; 74h:22m:55s remains)
INFO - root - 2017-12-07 22:31:05.628592: step 54190, loss = 21.37, batch loss = 21.28 (8.7 examples/sec; 0.919 sec/batch; 71h:03m:44s remains)
INFO - root - 2017-12-07 22:31:14.911297: step 54200, loss = 21.39, batch loss = 21.30 (9.1 examples/sec; 0.880 sec/batch; 68h:02m:41s remains)
2017-12-07 22:31:15.840380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.529027 -4.5122342 -4.4618568 -4.3933558 -4.291995 -4.1670222 -4.0623159 -4.0666947 -4.160912 -4.2358255 -4.2418089 -4.211987 -4.1572995 -4.0921903 -4.0789766][-4.5342875 -4.517405 -4.4561458 -4.3771005 -4.2716522 -4.1477633 -4.0308294 -4.0115137 -4.0986438 -4.1897011 -4.22671 -4.2098513 -4.1619291 -4.0969157 -4.0704927][-4.5407176 -4.5213556 -4.4455972 -4.3511825 -4.2478814 -4.1398845 -4.0193853 -3.9763515 -4.0608807 -4.1692944 -4.2279892 -4.2201047 -4.1780186 -4.1221361 -4.0910063][-4.5498514 -4.5264091 -4.4358397 -4.3261151 -4.2251129 -4.1396546 -4.0199776 -3.9529765 -4.0398412 -4.1720562 -4.2483668 -4.250607 -4.2197719 -4.17748 -4.1458058][-4.5554247 -4.5235014 -4.41463 -4.2883 -4.1879048 -4.1251435 -4.0137053 -3.9273539 -4.0129671 -4.1668067 -4.2557378 -4.2709002 -4.2642226 -4.2460079 -4.229166][-4.55597 -4.5125208 -4.38439 -4.2443027 -4.1451721 -4.0950294 -3.9873934 -3.8843155 -3.9665089 -4.1343722 -4.2335663 -4.2640004 -4.2885695 -4.3076906 -4.3224444][-4.5533714 -4.500587 -4.35916 -4.2129846 -4.1117878 -4.0569363 -3.9451299 -3.8390374 -3.9236159 -4.0907068 -4.1838746 -4.2150731 -4.2614059 -4.3209867 -4.3768754][-4.5472441 -4.4925237 -4.3540063 -4.2199712 -4.125073 -4.0631161 -3.9569657 -3.8742537 -3.961674 -4.1068621 -4.1758094 -4.187139 -4.2278652 -4.2997117 -4.3737283][-4.5430455 -4.4910269 -4.3685479 -4.2593923 -4.1806874 -4.1215324 -4.0382853 -3.9928923 -4.0763073 -4.186842 -4.2231646 -4.2050972 -4.2185483 -4.2671041 -4.3265262][-4.54295 -4.4887166 -4.3802981 -4.2920513 -4.2300649 -4.1864061 -4.1406221 -4.1341281 -4.2078047 -4.2817807 -4.2850442 -4.2348166 -4.2150726 -4.2276063 -4.2576017][-4.5405431 -4.4798746 -4.37773 -4.3005867 -4.2555437 -4.2392197 -4.2333889 -4.2517333 -4.3078732 -4.3491006 -4.3289995 -4.2622366 -4.2229629 -4.2073779 -4.20802][-4.5351763 -4.4686179 -4.3650231 -4.2856541 -4.2451921 -4.2468281 -4.2678194 -4.2978163 -4.3384461 -4.3581181 -4.3293772 -4.2658281 -4.2275929 -4.2047482 -4.1925211][-4.5311627 -4.4626112 -4.3541489 -4.2652559 -4.2156587 -4.2182384 -4.2491775 -4.2834868 -4.3163505 -4.3276148 -4.3054056 -4.2605319 -4.2386627 -4.2299981 -4.2237639][-4.5317483 -4.4673576 -4.3582139 -4.2653351 -4.2067132 -4.2018051 -4.228673 -4.2636256 -4.3005238 -4.3212175 -4.3185921 -4.2968864 -4.2928205 -4.3025403 -4.3051023][-4.5400167 -4.4866657 -4.38567 -4.2994833 -4.237515 -4.22073 -4.2320704 -4.2589183 -4.3039794 -4.3434167 -4.3623562 -4.353673 -4.3569407 -4.37718 -4.3869081]]...]
INFO - root - 2017-12-07 22:31:25.249448: step 54210, loss = 21.21, batch loss = 21.13 (8.5 examples/sec; 0.945 sec/batch; 73h:04m:47s remains)
INFO - root - 2017-12-07 22:31:34.668504: step 54220, loss = 21.82, batch loss = 21.74 (8.0 examples/sec; 1.002 sec/batch; 77h:25m:15s remains)
INFO - root - 2017-12-07 22:31:44.007822: step 54230, loss = 21.75, batch loss = 21.67 (8.6 examples/sec; 0.928 sec/batch; 71h:43m:35s remains)
INFO - root - 2017-12-07 22:31:53.344331: step 54240, loss = 21.43, batch loss = 21.35 (8.7 examples/sec; 0.917 sec/batch; 70h:53m:13s remains)
INFO - root - 2017-12-07 22:32:02.589021: step 54250, loss = 21.50, batch loss = 21.42 (8.7 examples/sec; 0.916 sec/batch; 70h:49m:09s remains)
INFO - root - 2017-12-07 22:32:12.035198: step 54260, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.926 sec/batch; 71h:35m:41s remains)
INFO - root - 2017-12-07 22:32:21.494297: step 54270, loss = 21.62, batch loss = 21.54 (8.4 examples/sec; 0.953 sec/batch; 73h:39m:09s remains)
INFO - root - 2017-12-07 22:32:30.894826: step 54280, loss = 21.19, batch loss = 21.11 (8.7 examples/sec; 0.921 sec/batch; 71h:12m:01s remains)
INFO - root - 2017-12-07 22:32:40.177361: step 54290, loss = 21.30, batch loss = 21.21 (8.5 examples/sec; 0.942 sec/batch; 72h:49m:49s remains)
INFO - root - 2017-12-07 22:32:49.540290: step 54300, loss = 21.62, batch loss = 21.54 (8.3 examples/sec; 0.969 sec/batch; 74h:52m:12s remains)
2017-12-07 22:32:50.557852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2291474 -4.2353811 -4.2140603 -4.2064753 -4.2145905 -4.2475009 -4.3134756 -4.3930964 -4.4838362 -4.547492 -4.5687613 -4.56877 -4.5529046 -4.5193686 -4.4778152][-4.2127833 -4.2472758 -4.2595325 -4.2814331 -4.3101935 -4.3417749 -4.381897 -4.4223056 -4.4801059 -4.5340624 -4.5672235 -4.5832014 -4.565228 -4.5075336 -4.4346094][-4.2467771 -4.3003907 -4.330997 -4.3669786 -4.400476 -4.4165034 -4.4216247 -4.4172997 -4.4391904 -4.4800224 -4.5248251 -4.5624366 -4.5515728 -4.4779282 -4.3749547][-4.3281412 -4.3838053 -4.4118752 -4.4450603 -4.4701109 -4.4649081 -4.4394526 -4.4038854 -4.40446 -4.4362903 -4.48557 -4.5341973 -4.5247989 -4.4327421 -4.2965121][-4.4219565 -4.4641342 -4.4699731 -4.4804349 -4.4843035 -4.4602256 -4.41953 -4.3819036 -4.3927364 -4.4319611 -4.4805379 -4.5203915 -4.4958773 -4.37943 -4.2105117][-4.4639192 -4.4859018 -4.45941 -4.4305286 -4.3960605 -4.3460565 -4.3006115 -4.2874641 -4.3393636 -4.41273 -4.4746146 -4.5086451 -4.4723468 -4.3426986 -4.1577969][-4.4149241 -4.425375 -4.3822174 -4.320303 -4.2394028 -4.1468058 -4.0809417 -4.08226 -4.1735888 -4.3000712 -4.40844 -4.4757442 -4.465137 -4.35478 -4.182611][-4.3412704 -4.344913 -4.295486 -4.207809 -4.0787296 -3.9304779 -3.8241405 -3.8155661 -3.9318514 -4.1212053 -4.3018928 -4.4322205 -4.4760642 -4.412879 -4.2859988][-4.3407784 -4.3349676 -4.2768073 -4.1695123 -4.0088539 -3.8247225 -3.6846955 -3.6519537 -3.7673423 -3.9933193 -4.2236509 -4.3933172 -4.4769049 -4.4642167 -4.3985882][-4.3865104 -4.3763275 -4.3235354 -4.2303314 -4.0885468 -3.9218948 -3.783319 -3.7276883 -3.8116124 -4.0173645 -4.2326641 -4.3816514 -4.4583917 -4.4709392 -4.4548893][-4.3917766 -4.3713908 -4.3297791 -4.2721996 -4.1813397 -4.0646825 -3.9548554 -3.8967643 -3.9562178 -4.123198 -4.29227 -4.3883266 -4.4267588 -4.4395509 -4.4542546][-4.3649759 -4.3194504 -4.2643843 -4.2185154 -4.1657047 -4.1004682 -4.040349 -4.014329 -4.0789967 -4.21724 -4.334764 -4.3761954 -4.3780122 -4.3927507 -4.4335709][-4.3248148 -4.2569308 -4.1781468 -4.1223483 -4.0927539 -4.079288 -4.082129 -4.10651 -4.1864219 -4.2953854 -4.3594069 -4.3582373 -4.3435664 -4.3692346 -4.4332194][-4.288888 -4.2150621 -4.11882 -4.0417047 -4.0140047 -4.03793 -4.0975742 -4.1712685 -4.2655768 -4.350677 -4.3778887 -4.3596506 -4.35076 -4.3879557 -4.4611759][-4.2949266 -4.2330856 -4.1350904 -4.0460391 -4.0127153 -4.0534072 -4.1460209 -4.2443919 -4.3373675 -4.4002595 -4.4038186 -4.3817763 -4.3826818 -4.4238572 -4.4901633]]...]
INFO - root - 2017-12-07 22:33:00.019258: step 54310, loss = 21.16, batch loss = 21.08 (8.4 examples/sec; 0.950 sec/batch; 73h:22m:50s remains)
INFO - root - 2017-12-07 22:33:09.588658: step 54320, loss = 21.38, batch loss = 21.30 (8.0 examples/sec; 0.999 sec/batch; 77h:10m:11s remains)
INFO - root - 2017-12-07 22:33:18.945074: step 54330, loss = 21.04, batch loss = 20.95 (8.4 examples/sec; 0.951 sec/batch; 73h:27m:18s remains)
INFO - root - 2017-12-07 22:33:28.163004: step 54340, loss = 21.59, batch loss = 21.51 (8.4 examples/sec; 0.948 sec/batch; 73h:15m:20s remains)
INFO - root - 2017-12-07 22:33:37.464030: step 54350, loss = 21.50, batch loss = 21.42 (8.3 examples/sec; 0.969 sec/batch; 74h:52m:59s remains)
INFO - root - 2017-12-07 22:33:46.806329: step 54360, loss = 21.03, batch loss = 20.95 (9.0 examples/sec; 0.893 sec/batch; 68h:59m:47s remains)
INFO - root - 2017-12-07 22:33:56.019476: step 54370, loss = 21.42, batch loss = 21.33 (8.8 examples/sec; 0.911 sec/batch; 70h:20m:38s remains)
INFO - root - 2017-12-07 22:34:05.307011: step 54380, loss = 21.28, batch loss = 21.20 (8.4 examples/sec; 0.957 sec/batch; 73h:57m:47s remains)
INFO - root - 2017-12-07 22:34:14.623952: step 54390, loss = 21.53, batch loss = 21.45 (9.0 examples/sec; 0.894 sec/batch; 69h:01m:40s remains)
INFO - root - 2017-12-07 22:34:24.035432: step 54400, loss = 21.52, batch loss = 21.44 (8.8 examples/sec; 0.907 sec/batch; 70h:04m:09s remains)
2017-12-07 22:34:24.941668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1979308 -4.238359 -4.3274522 -4.424943 -4.4498739 -4.3886614 -4.3147554 -4.3111682 -4.4059658 -4.5507755 -4.6662006 -4.699122 -4.6559391 -4.536212 -4.371335][-4.1993837 -4.2367845 -4.3205652 -4.4121723 -4.4384432 -4.3841395 -4.32282 -4.3280344 -4.4121003 -4.54735 -4.6595063 -4.6935048 -4.6569972 -4.5415144 -4.3725929][-4.1995435 -4.2416706 -4.3271403 -4.4166088 -4.4354568 -4.3633351 -4.284667 -4.2773309 -4.3454242 -4.4739933 -4.5981617 -4.6508479 -4.6393414 -4.5473223 -4.3898306][-4.2105565 -4.262248 -4.350287 -4.4322238 -4.4349422 -4.3371773 -4.2358232 -4.2152038 -4.2741532 -4.4010124 -4.5323944 -4.5918784 -4.5946984 -4.5263672 -4.397028][-4.2361393 -4.2960033 -4.3755674 -4.4314423 -4.4066525 -4.2916574 -4.1891408 -4.1800032 -4.2489424 -4.3754511 -4.4962378 -4.54146 -4.5454845 -4.4991808 -4.4106278][-4.2735066 -4.3321838 -4.3831906 -4.3851681 -4.3125634 -4.1860256 -4.1084661 -4.1431222 -4.2480168 -4.3786507 -4.4782515 -4.5016484 -4.4977593 -4.4723067 -4.4316268][-4.3121624 -4.3638659 -4.3712058 -4.2936583 -4.1452489 -3.9901874 -3.9314387 -4.0150204 -4.1716814 -4.3254638 -4.4259591 -4.4488611 -4.4420381 -4.4358807 -4.4409094][-4.3403192 -4.3851161 -4.35113 -4.1966867 -3.9668233 -3.7611442 -3.6846068 -3.7815475 -3.9725788 -4.1590176 -4.2915707 -4.3460174 -4.3526387 -4.3664589 -4.4142485][-4.3574944 -4.4027109 -4.3543057 -4.1624303 -3.8794165 -3.6172688 -3.4810781 -3.5352578 -3.7200875 -3.93713 -4.1229882 -4.2293487 -4.2618022 -4.2908683 -4.3681364][-4.3686056 -4.4291162 -4.4038978 -4.2304797 -3.9442358 -3.6504278 -3.4541163 -3.4395971 -3.5886509 -3.8209648 -4.0568252 -4.2098985 -4.2593479 -4.2832403 -4.3587804][-4.3770571 -4.4648132 -4.4888258 -4.3769069 -4.1374526 -3.8573432 -3.6339519 -3.5676389 -3.6765852 -3.9044857 -4.1605854 -4.3323817 -4.378099 -4.3744488 -4.4159727][-4.3859019 -4.505321 -4.5877533 -4.552711 -4.3827958 -4.1465034 -3.9328709 -3.8478324 -3.9277582 -4.12883 -4.362009 -4.5135174 -4.5385356 -4.5042157 -4.5021873][-4.3919969 -4.5370708 -4.6665273 -4.693306 -4.584609 -4.3975811 -4.2148066 -4.1350651 -4.1938033 -4.3488879 -4.5215716 -4.620502 -4.6148806 -4.5611687 -4.529664][-4.3870616 -4.5421963 -4.6941333 -4.7578 -4.6983185 -4.5624671 -4.4203286 -4.3539548 -4.3890562 -4.4863305 -4.5822473 -4.6141496 -4.5743351 -4.5091786 -4.462028][-4.3654637 -4.5143175 -4.6678534 -4.7513394 -4.7343655 -4.6476307 -4.5439329 -4.4849687 -4.4888115 -4.5266433 -4.5524468 -4.5271015 -4.4593139 -4.3885164 -4.3393278]]...]
INFO - root - 2017-12-07 22:34:34.306189: step 54410, loss = 21.65, batch loss = 21.57 (8.4 examples/sec; 0.955 sec/batch; 73h:47m:39s remains)
INFO - root - 2017-12-07 22:34:43.645086: step 54420, loss = 21.54, batch loss = 21.45 (8.6 examples/sec; 0.931 sec/batch; 71h:55m:56s remains)
INFO - root - 2017-12-07 22:34:53.086544: step 54430, loss = 21.74, batch loss = 21.65 (8.8 examples/sec; 0.909 sec/batch; 70h:11m:31s remains)
INFO - root - 2017-12-07 22:35:02.579362: step 54440, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.943 sec/batch; 72h:52m:22s remains)
INFO - root - 2017-12-07 22:35:11.803476: step 54450, loss = 21.11, batch loss = 21.03 (8.1 examples/sec; 0.985 sec/batch; 76h:04m:06s remains)
INFO - root - 2017-12-07 22:35:21.165550: step 54460, loss = 21.38, batch loss = 21.30 (8.1 examples/sec; 0.989 sec/batch; 76h:24m:15s remains)
INFO - root - 2017-12-07 22:35:30.569469: step 54470, loss = 21.46, batch loss = 21.38 (8.0 examples/sec; 0.996 sec/batch; 76h:57m:06s remains)
INFO - root - 2017-12-07 22:35:40.035734: step 54480, loss = 21.68, batch loss = 21.60 (8.1 examples/sec; 0.984 sec/batch; 75h:59m:04s remains)
INFO - root - 2017-12-07 22:35:49.443953: step 54490, loss = 22.16, batch loss = 22.07 (8.1 examples/sec; 0.989 sec/batch; 76h:23m:28s remains)
INFO - root - 2017-12-07 22:35:58.790568: step 54500, loss = 21.54, batch loss = 21.46 (8.4 examples/sec; 0.958 sec/batch; 73h:58m:21s remains)
2017-12-07 22:35:59.806502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4031396 -4.4162412 -4.3503561 -4.2320862 -4.13031 -4.0950656 -4.1619196 -4.2995253 -4.4357481 -4.5393772 -4.59523 -4.58218 -4.502358 -4.4005151 -4.3483062][-4.3657126 -4.3870249 -4.3299718 -4.2184229 -4.1306305 -4.1144781 -4.1864738 -4.310627 -4.4197917 -4.4930115 -4.5289607 -4.499887 -4.3997407 -4.2832351 -4.2331257][-4.3205624 -4.3513265 -4.316987 -4.2403994 -4.1950536 -4.2147055 -4.2880726 -4.3746881 -4.4335675 -4.4606657 -4.4665337 -4.4230609 -4.3182178 -4.2062325 -4.1680293][-4.29605 -4.3194222 -4.3024817 -4.2640514 -4.2618351 -4.3036742 -4.3570533 -4.3928409 -4.4055157 -4.4056859 -4.3987327 -4.3536687 -4.2588019 -4.1647506 -4.1416483][-4.3422709 -4.345613 -4.3283377 -4.3020577 -4.3037424 -4.3193674 -4.3182106 -4.2978439 -4.2866321 -4.2966776 -4.3112063 -4.2935457 -4.2348852 -4.1732078 -4.1688251][-4.4286623 -4.4030519 -4.3618159 -4.3147612 -4.2789984 -4.2280025 -4.1519232 -4.0835233 -4.0752592 -4.1256514 -4.1949596 -4.2403121 -4.2481651 -4.2394223 -4.2579265][-4.5328326 -4.47268 -4.3920636 -4.3037567 -4.2086368 -4.0715327 -3.9091375 -3.7965653 -3.8079443 -3.9189756 -4.0632858 -4.1888242 -4.2763195 -4.329143 -4.3710527][-4.6288648 -4.5533175 -4.4420323 -4.3173604 -4.1702709 -3.9591 -3.7215521 -3.574003 -3.60713 -3.7777824 -3.9909866 -4.1774035 -4.3200417 -4.4131622 -4.4667721][-4.6880074 -4.6230412 -4.5057039 -4.3728709 -4.20865 -3.965652 -3.6958563 -3.5409827 -3.5914795 -3.7934086 -4.034708 -4.23409 -4.3773961 -4.4681053 -4.5128307][-4.6780033 -4.6371279 -4.53959 -4.42846 -4.2830892 -4.0525112 -3.7974434 -3.6609952 -3.7224956 -3.9196551 -4.1396656 -4.3095717 -4.4221611 -4.4858327 -4.5084066][-4.581418 -4.5660462 -4.5022936 -4.430738 -4.3244233 -4.1394243 -3.9357128 -3.8389893 -3.911046 -4.0821929 -4.2490215 -4.3657918 -4.436552 -4.4722481 -4.4767518][-4.4612055 -4.4632444 -4.4322915 -4.4008574 -4.328898 -4.1916842 -4.0538735 -4.0130053 -4.1014991 -4.242785 -4.3484678 -4.4023452 -4.4293885 -4.4431934 -4.4411945][-4.3927484 -4.3969865 -4.3779736 -4.3720446 -4.3253474 -4.2298212 -4.15385 -4.1630259 -4.2645535 -4.3836074 -4.4421668 -4.4447918 -4.436626 -4.4326539 -4.4247394][-4.3824897 -4.3886528 -4.3683968 -4.3701835 -4.3406591 -4.2790275 -4.2474985 -4.2836404 -4.3809891 -4.4800735 -4.5135365 -4.4871373 -4.4527154 -4.4290161 -4.407999][-4.420423 -4.4310336 -4.4082494 -4.4054909 -4.3828092 -4.3421421 -4.3298817 -4.3621244 -4.432507 -4.5052123 -4.5267715 -4.4968309 -4.4573488 -4.4251695 -4.3944378]]...]
INFO - root - 2017-12-07 22:36:09.194470: step 54510, loss = 21.31, batch loss = 21.23 (8.4 examples/sec; 0.951 sec/batch; 73h:25m:37s remains)
INFO - root - 2017-12-07 22:36:18.587234: step 54520, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.951 sec/batch; 73h:23m:41s remains)
INFO - root - 2017-12-07 22:36:27.983491: step 54530, loss = 21.36, batch loss = 21.27 (8.2 examples/sec; 0.981 sec/batch; 75h:42m:43s remains)
INFO - root - 2017-12-07 22:36:37.211579: step 54540, loss = 21.13, batch loss = 21.05 (8.7 examples/sec; 0.919 sec/batch; 70h:57m:48s remains)
INFO - root - 2017-12-07 22:36:46.559813: step 54550, loss = 21.12, batch loss = 21.04 (8.7 examples/sec; 0.922 sec/batch; 71h:10m:47s remains)
INFO - root - 2017-12-07 22:36:55.905316: step 54560, loss = 21.29, batch loss = 21.21 (9.0 examples/sec; 0.889 sec/batch; 68h:39m:05s remains)
INFO - root - 2017-12-07 22:37:05.287352: step 54570, loss = 21.28, batch loss = 21.20 (8.7 examples/sec; 0.922 sec/batch; 71h:12m:53s remains)
INFO - root - 2017-12-07 22:37:14.573427: step 54580, loss = 21.84, batch loss = 21.76 (8.8 examples/sec; 0.911 sec/batch; 70h:20m:24s remains)
INFO - root - 2017-12-07 22:37:23.973698: step 54590, loss = 21.70, batch loss = 21.62 (8.7 examples/sec; 0.919 sec/batch; 70h:55m:46s remains)
INFO - root - 2017-12-07 22:37:33.218290: step 54600, loss = 21.66, batch loss = 21.58 (9.3 examples/sec; 0.860 sec/batch; 66h:25m:13s remains)
2017-12-07 22:37:34.196711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5805292 -4.6491575 -4.686748 -4.6862669 -4.6504979 -4.5895896 -4.5174589 -4.4676976 -4.4868913 -4.5755062 -4.679678 -4.7511911 -4.7535672 -4.6681304 -4.5354786][-4.6020746 -4.7080379 -4.7516136 -4.7384386 -4.6807823 -4.5962772 -4.5096812 -4.4689317 -4.5135913 -4.6333938 -4.7597294 -4.8381977 -4.8342342 -4.7327485 -4.5795326][-4.5785604 -4.721684 -4.7738581 -4.7523842 -4.6793165 -4.5790691 -4.4873233 -4.4638324 -4.5322137 -4.6750016 -4.8144317 -4.8927703 -4.8842754 -4.7794127 -4.6169305][-4.5512266 -4.7141347 -4.7600446 -4.7191033 -4.6302838 -4.5155129 -4.4203825 -4.4126558 -4.4987712 -4.6553535 -4.8005338 -4.8786559 -4.8783665 -4.79344 -4.6395984][-4.5376635 -4.6780682 -4.679769 -4.59748 -4.4903913 -4.3681159 -4.2783089 -4.2886934 -4.3922238 -4.55827 -4.7041321 -4.7881174 -4.811666 -4.7608066 -4.6305556][-4.4987011 -4.5877562 -4.5209293 -4.3816094 -4.2420449 -4.1031685 -4.0154872 -4.043992 -4.1748319 -4.3659139 -4.5325189 -4.6481433 -4.7120733 -4.6958876 -4.5910268][-4.4362583 -4.4839134 -4.3657794 -4.1725698 -3.9826131 -3.8037386 -3.7024076 -3.7444339 -3.9121633 -4.1448517 -4.3548269 -4.52253 -4.6306553 -4.6374593 -4.5452194][-4.406652 -4.4533758 -4.32153 -4.0975132 -3.852509 -3.6236267 -3.5079961 -3.5653884 -3.7625704 -4.0258913 -4.2635169 -4.4630766 -4.5989037 -4.6153569 -4.5208573][-4.4449811 -4.5087295 -4.3910909 -4.1690636 -3.8958428 -3.6412029 -3.5327432 -3.6089332 -3.8053613 -4.0577374 -4.2859087 -4.4793453 -4.616015 -4.6285553 -4.5242968][-4.52106 -4.5972581 -4.506825 -4.3188462 -4.059967 -3.8056419 -3.7053146 -3.7858224 -3.9611063 -4.1855903 -4.389183 -4.55326 -4.6652179 -4.6587367 -4.5401545][-4.5854645 -4.66289 -4.6038642 -4.4639468 -4.2477689 -4.0160661 -3.9186373 -3.9873295 -4.1378803 -4.3325634 -4.5068212 -4.6325188 -4.7052636 -4.672811 -4.5429134][-4.64363 -4.7098904 -4.6805592 -4.5908675 -4.4349666 -4.2589231 -4.1829486 -4.2392783 -4.361104 -4.5090113 -4.6303387 -4.6995168 -4.7213616 -4.6614642 -4.5270677][-4.6974812 -4.7456942 -4.7386312 -4.6938992 -4.60168 -4.4960732 -4.4546556 -4.4964838 -4.5769215 -4.66224 -4.7195749 -4.7358589 -4.7160416 -4.6332521 -4.4986405][-4.693409 -4.7252994 -4.7324209 -4.7213936 -4.6814051 -4.6324644 -4.6135483 -4.6350412 -4.672677 -4.7044435 -4.7182107 -4.7083535 -4.6665621 -4.5723505 -4.4473648][-4.58596 -4.607738 -4.6220746 -4.63313 -4.6290107 -4.6149321 -4.6074672 -4.613739 -4.6245942 -4.6294937 -4.62529 -4.6059031 -4.5575285 -4.4715147 -4.3748841]]...]
INFO - root - 2017-12-07 22:37:43.587614: step 54610, loss = 21.89, batch loss = 21.81 (8.6 examples/sec; 0.927 sec/batch; 71h:34m:50s remains)
INFO - root - 2017-12-07 22:37:52.954666: step 54620, loss = 21.77, batch loss = 21.69 (8.4 examples/sec; 0.949 sec/batch; 73h:13m:59s remains)
INFO - root - 2017-12-07 22:38:02.285337: step 54630, loss = 21.40, batch loss = 21.31 (9.1 examples/sec; 0.883 sec/batch; 68h:11m:17s remains)
INFO - root - 2017-12-07 22:38:11.562791: step 54640, loss = 21.38, batch loss = 21.30 (8.5 examples/sec; 0.940 sec/batch; 72h:34m:25s remains)
INFO - root - 2017-12-07 22:38:20.785034: step 54650, loss = 21.45, batch loss = 21.37 (9.6 examples/sec; 0.834 sec/batch; 64h:20m:02s remains)
INFO - root - 2017-12-07 22:38:30.194321: step 54660, loss = 21.58, batch loss = 21.50 (8.6 examples/sec; 0.933 sec/batch; 72h:00m:22s remains)
INFO - root - 2017-12-07 22:38:39.546701: step 54670, loss = 21.31, batch loss = 21.23 (8.2 examples/sec; 0.973 sec/batch; 75h:05m:54s remains)
INFO - root - 2017-12-07 22:38:48.866882: step 54680, loss = 21.54, batch loss = 21.46 (8.3 examples/sec; 0.969 sec/batch; 74h:44m:49s remains)
INFO - root - 2017-12-07 22:38:58.377605: step 54690, loss = 21.15, batch loss = 21.07 (8.1 examples/sec; 0.986 sec/batch; 76h:07m:28s remains)
INFO - root - 2017-12-07 22:39:07.798092: step 54700, loss = 21.22, batch loss = 21.13 (8.5 examples/sec; 0.942 sec/batch; 72h:42m:51s remains)
2017-12-07 22:39:08.899772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2660928 -4.3309011 -4.4151282 -4.4521356 -4.4245739 -4.3799028 -4.3404546 -4.31521 -4.3188062 -4.3309307 -4.301795 -4.2458692 -4.2190032 -4.2135849 -4.1879082][-4.3199182 -4.3568983 -4.4298472 -4.4869766 -4.496582 -4.480051 -4.4583068 -4.4410286 -4.4379239 -4.4376192 -4.3988004 -4.34114 -4.2992196 -4.26707 -4.2214794][-4.3484674 -4.3685503 -4.435463 -4.5068355 -4.5441985 -4.5464578 -4.5310092 -4.5123167 -4.5020919 -4.5015965 -4.4774094 -4.4347391 -4.3897557 -4.3422818 -4.2844543][-4.3871822 -4.3991556 -4.4499121 -4.5093646 -4.5388207 -4.5153689 -4.467741 -4.4312358 -4.4296064 -4.4696088 -4.5049744 -4.5079465 -4.4769454 -4.4250536 -4.3565078][-4.4458394 -4.4593692 -4.4790258 -4.4901056 -4.4672208 -4.3710914 -4.2513409 -4.1785264 -4.1955156 -4.3067269 -4.4329119 -4.5087929 -4.5159373 -4.47833 -4.4084234][-4.48452 -4.5063262 -4.5002193 -4.4584312 -4.3760386 -4.2078495 -4.0191827 -3.9032559 -3.9242988 -4.0933862 -4.2969012 -4.4413195 -4.4958448 -4.4884605 -4.4357061][-4.4950461 -4.5236669 -4.5060077 -4.4348292 -4.3154488 -4.1071153 -3.8800461 -3.7223325 -3.7225208 -3.9164252 -4.1617432 -4.3423486 -4.4281406 -4.4534669 -4.4290543][-4.4946866 -4.5213518 -4.4950919 -4.4069395 -4.2692938 -4.0539603 -3.8229458 -3.6364858 -3.6084762 -3.8074279 -4.0710511 -4.2657852 -4.3670397 -4.4128637 -4.4094338][-4.504251 -4.5205135 -4.4868946 -4.3955054 -4.2655725 -4.0819325 -3.8915906 -3.7239246 -3.6846793 -3.8504641 -4.0798683 -4.2516003 -4.3419161 -4.3843207 -4.3843861][-4.5511088 -4.5445871 -4.5063009 -4.4343743 -4.3424869 -4.2203693 -4.1063385 -3.9981961 -3.9469237 -4.0192666 -4.1429415 -4.2462711 -4.303102 -4.3298235 -4.3288431][-4.6009192 -4.5605497 -4.5125933 -4.46117 -4.4010539 -4.3292246 -4.2858396 -4.2413712 -4.1864758 -4.1662931 -4.176661 -4.2059155 -4.2315679 -4.2473841 -4.2535734][-4.6008968 -4.5284128 -4.4635053 -4.4111376 -4.3617043 -4.32607 -4.3414745 -4.3534331 -4.3139682 -4.2446971 -4.1816931 -4.1579394 -4.1604776 -4.1738133 -4.1961842][-4.5475616 -4.4516029 -4.3644986 -4.2991872 -4.2545843 -4.2566228 -4.3261371 -4.3879957 -4.3809848 -4.3106918 -4.2216458 -4.1691427 -4.1585317 -4.1751795 -4.2080235][-4.4788218 -4.3725724 -4.2683105 -4.1905909 -4.1522861 -4.1850219 -4.2908931 -4.3857937 -4.4143138 -4.3710728 -4.292902 -4.2405887 -4.2346478 -4.2547941 -4.2804179][-4.418087 -4.3269129 -4.2273421 -4.1546383 -4.1294875 -4.1782837 -4.2911997 -4.391459 -4.4379191 -4.4138241 -4.3474064 -4.2992325 -4.3014326 -4.3278837 -4.3442535]]...]
INFO - root - 2017-12-07 22:39:18.206423: step 54710, loss = 21.88, batch loss = 21.80 (9.4 examples/sec; 0.850 sec/batch; 65h:35m:49s remains)
INFO - root - 2017-12-07 22:39:27.529807: step 54720, loss = 21.38, batch loss = 21.30 (8.3 examples/sec; 0.967 sec/batch; 74h:38m:43s remains)
INFO - root - 2017-12-07 22:39:36.799452: step 54730, loss = 21.73, batch loss = 21.65 (8.6 examples/sec; 0.930 sec/batch; 71h:43m:08s remains)
INFO - root - 2017-12-07 22:39:46.224015: step 54740, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.930 sec/batch; 71h:45m:12s remains)
INFO - root - 2017-12-07 22:39:55.638992: step 54750, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.941 sec/batch; 72h:35m:30s remains)
INFO - root - 2017-12-07 22:40:04.971821: step 54760, loss = 21.65, batch loss = 21.57 (8.5 examples/sec; 0.943 sec/batch; 72h:43m:42s remains)
INFO - root - 2017-12-07 22:40:14.401335: step 54770, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.953 sec/batch; 73h:31m:15s remains)
INFO - root - 2017-12-07 22:40:23.825413: step 54780, loss = 21.36, batch loss = 21.28 (8.5 examples/sec; 0.936 sec/batch; 72h:13m:26s remains)
INFO - root - 2017-12-07 22:40:33.163167: step 54790, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.933 sec/batch; 71h:59m:42s remains)
INFO - root - 2017-12-07 22:40:42.612371: step 54800, loss = 21.30, batch loss = 21.22 (8.3 examples/sec; 0.967 sec/batch; 74h:34m:19s remains)
2017-12-07 22:40:43.666013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4427056 -4.4973044 -4.5221524 -4.5245447 -4.496706 -4.4467082 -4.384902 -4.3346205 -4.3131504 -4.3224893 -4.3756862 -4.4485097 -4.4996152 -4.4988723 -4.4563532][-4.4370418 -4.4987926 -4.5320067 -4.5406356 -4.510592 -4.4424319 -4.3622036 -4.3120489 -4.3148632 -4.3505716 -4.4077845 -4.4609375 -4.4880357 -4.4777417 -4.4438496][-4.4345913 -4.4983883 -4.5366292 -4.5434985 -4.4985518 -4.4040914 -4.3080893 -4.2649779 -4.294045 -4.352057 -4.400939 -4.4226308 -4.4265938 -4.4227324 -4.4244208][-4.4264812 -4.4871526 -4.5325832 -4.5320916 -4.4588919 -4.3305779 -4.2186422 -4.1811929 -4.2324481 -4.311832 -4.355742 -4.348134 -4.325387 -4.3167458 -4.3400011][-4.4181781 -4.4771786 -4.528481 -4.5152926 -4.402945 -4.2305942 -4.0951614 -4.0615525 -4.1392746 -4.2411256 -4.2871747 -4.2692776 -4.2327652 -4.2081451 -4.2224612][-4.4181519 -4.4638829 -4.5134664 -4.4938922 -4.3572841 -4.1504669 -3.9851978 -3.9473727 -4.0461817 -4.1553288 -4.1991687 -4.1965575 -4.1784105 -4.149209 -4.139071][-4.4380445 -4.4641161 -4.4994073 -4.4709988 -4.32582 -4.0986285 -3.899261 -3.8468938 -3.9539394 -4.0594978 -4.1081686 -4.1431627 -4.16494 -4.14287 -4.1127357][-4.4498711 -4.4628882 -4.4745793 -4.4266152 -4.2723269 -4.0370669 -3.8197515 -3.7646751 -3.8842988 -3.9977787 -4.0679641 -4.143187 -4.1946354 -4.1796708 -4.1459417][-4.4212279 -4.4405503 -4.4449267 -4.3939443 -4.2549748 -4.0451074 -3.8488488 -3.8033006 -3.9161305 -4.024889 -4.1066842 -4.1941619 -4.252409 -4.2449613 -4.2188711][-4.3749027 -4.4053221 -4.4260859 -4.4020624 -4.3125081 -4.1649446 -4.0231042 -3.9871397 -4.0608425 -4.1325488 -4.193871 -4.2655373 -4.319582 -4.3274603 -4.3159337][-4.344852 -4.3801465 -4.4178157 -4.4239388 -4.3857574 -4.3028221 -4.21681 -4.1871204 -4.214911 -4.2422314 -4.2710876 -4.3230119 -4.3803582 -4.4160371 -4.423758][-4.3583179 -4.3879147 -4.4213991 -4.4320412 -4.4126167 -4.3689146 -4.320231 -4.2949886 -4.2948895 -4.29605 -4.3043203 -4.34003 -4.4014153 -4.4646292 -4.4996409][-4.4068289 -4.4172583 -4.4281912 -4.4181457 -4.3854513 -4.35122 -4.3254771 -4.3147945 -4.3141708 -4.3124189 -4.3116794 -4.3287239 -4.3791385 -4.4517789 -4.5126271][-4.4557395 -4.4492369 -4.4419446 -4.4160709 -4.3677707 -4.3308926 -4.3164945 -4.3181043 -4.321682 -4.3211555 -4.3154287 -4.3127623 -4.3395538 -4.4026146 -4.4736271][-4.456563 -4.45261 -4.4559526 -4.4462228 -4.4093852 -4.3753128 -4.3587775 -4.3550858 -4.3479257 -4.3359423 -4.3253431 -4.3123388 -4.3218565 -4.3739772 -4.4421077]]...]
INFO - root - 2017-12-07 22:40:53.006344: step 54810, loss = 21.39, batch loss = 21.30 (8.2 examples/sec; 0.974 sec/batch; 75h:08m:23s remains)
INFO - root - 2017-12-07 22:41:02.284756: step 54820, loss = 21.77, batch loss = 21.68 (8.8 examples/sec; 0.914 sec/batch; 70h:29m:51s remains)
INFO - root - 2017-12-07 22:41:11.721840: step 54830, loss = 21.73, batch loss = 21.64 (8.9 examples/sec; 0.898 sec/batch; 69h:16m:03s remains)
INFO - root - 2017-12-07 22:41:20.999532: step 54840, loss = 21.50, batch loss = 21.42 (8.9 examples/sec; 0.901 sec/batch; 69h:28m:30s remains)
INFO - root - 2017-12-07 22:41:30.526275: step 54850, loss = 21.08, batch loss = 21.00 (8.5 examples/sec; 0.942 sec/batch; 72h:38m:27s remains)
INFO - root - 2017-12-07 22:41:39.797231: step 54860, loss = 21.22, batch loss = 21.14 (8.6 examples/sec; 0.930 sec/batch; 71h:45m:27s remains)
INFO - root - 2017-12-07 22:41:49.147933: step 54870, loss = 21.59, batch loss = 21.50 (8.5 examples/sec; 0.939 sec/batch; 72h:25m:20s remains)
INFO - root - 2017-12-07 22:41:58.650970: step 54880, loss = 21.39, batch loss = 21.30 (8.1 examples/sec; 0.991 sec/batch; 76h:23m:23s remains)
INFO - root - 2017-12-07 22:42:07.904787: step 54890, loss = 21.91, batch loss = 21.83 (8.3 examples/sec; 0.969 sec/batch; 74h:44m:31s remains)
INFO - root - 2017-12-07 22:42:17.440857: step 54900, loss = 21.66, batch loss = 21.58 (8.5 examples/sec; 0.938 sec/batch; 72h:21m:34s remains)
2017-12-07 22:42:18.364307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5845342 -4.5121417 -4.4766388 -4.5297484 -4.6469622 -4.703712 -4.6129088 -4.486599 -4.420136 -4.4266758 -4.4810781 -4.4963489 -4.448267 -4.4158244 -4.4894547][-4.6116352 -4.5435104 -4.5247679 -4.5924482 -4.6921024 -4.6990957 -4.5718575 -4.4368839 -4.3649406 -4.3608975 -4.403286 -4.4275312 -4.4191723 -4.4356217 -4.5248408][-4.6153717 -4.5711017 -4.5733056 -4.638588 -4.6993017 -4.6538696 -4.5083466 -4.3863211 -4.3177114 -4.2947974 -4.3145704 -4.349885 -4.39159 -4.4510369 -4.5196505][-4.5818877 -4.5707955 -4.5884266 -4.62201 -4.6199703 -4.5140452 -4.3555732 -4.2594395 -4.2116084 -4.1840463 -4.19075 -4.2436595 -4.3379459 -4.4271092 -4.4487481][-4.4834762 -4.5148492 -4.5446634 -4.5249767 -4.4430676 -4.2747664 -4.1086974 -4.0565495 -4.0648551 -4.0616574 -4.0709782 -4.1412287 -4.2732735 -4.3697462 -4.3331714][-4.3457179 -4.4177952 -4.4586945 -4.3785191 -4.206161 -3.9710169 -3.8007407 -3.81396 -3.9108319 -3.9629097 -3.9937642 -4.0832553 -4.234261 -4.3159013 -4.2290292][-4.2128372 -4.31188 -4.3571076 -4.2312784 -3.9885514 -3.7082882 -3.5528488 -3.6457253 -3.8389282 -3.9517186 -4.0128736 -4.1231375 -4.2840142 -4.35081 -4.241396][-4.1275177 -4.2421393 -4.2946334 -4.1664271 -3.9133511 -3.6460559 -3.5389745 -3.6891522 -3.9219568 -4.0550175 -4.1285233 -4.2458878 -4.4038 -4.4650249 -4.3702316][-4.0936465 -4.2162013 -4.2789969 -4.1864047 -3.9805543 -3.777379 -3.7310641 -3.8981135 -4.1095243 -4.2239418 -4.293263 -4.4004393 -4.5369973 -4.5977249 -4.5421023][-4.0899282 -4.2061129 -4.276422 -4.2408395 -4.1220522 -4.0134134 -4.0283828 -4.1865282 -4.3461604 -4.4287629 -4.48675 -4.5687375 -4.6680164 -4.7201014 -4.6972504][-4.1069813 -4.2070951 -4.2754941 -4.2980609 -4.2838054 -4.2818685 -4.344408 -4.4695792 -4.5676928 -4.6128831 -4.6511936 -4.7009821 -4.7549858 -4.7818842 -4.7634673][-4.1402068 -4.2181764 -4.2755823 -4.3371763 -4.40715 -4.48247 -4.5573182 -4.6295018 -4.6657858 -4.6755896 -4.6915989 -4.7101989 -4.7240767 -4.7240658 -4.7004867][-4.2376356 -4.3007083 -4.345634 -4.4140019 -4.5061817 -4.5889726 -4.6286993 -4.634068 -4.6150756 -4.5964413 -4.5934405 -4.5909791 -4.5833778 -4.5722651 -4.5534558][-4.3872752 -4.4504867 -4.4890494 -4.5379052 -4.5928078 -4.6227493 -4.5999489 -4.5450764 -4.486825 -4.4482412 -4.4299612 -4.4150057 -4.4032655 -4.4008551 -4.4013915][-4.54284 -4.6010437 -4.6251922 -4.6386838 -4.6330628 -4.5926929 -4.5107365 -4.4171095 -4.3434286 -4.300498 -4.2796249 -4.2676253 -4.2700992 -4.2916408 -4.3179722]]...]
INFO - root - 2017-12-07 22:42:27.861926: step 54910, loss = 21.49, batch loss = 21.41 (8.4 examples/sec; 0.950 sec/batch; 73h:14m:23s remains)
INFO - root - 2017-12-07 22:42:37.443844: step 54920, loss = 21.68, batch loss = 21.60 (8.0 examples/sec; 1.001 sec/batch; 77h:11m:08s remains)
INFO - root - 2017-12-07 22:42:46.974901: step 54930, loss = 21.49, batch loss = 21.41 (8.0 examples/sec; 1.002 sec/batch; 77h:16m:29s remains)
INFO - root - 2017-12-07 22:42:56.416335: step 54940, loss = 21.81, batch loss = 21.73 (8.7 examples/sec; 0.920 sec/batch; 70h:56m:10s remains)
INFO - root - 2017-12-07 22:43:05.922871: step 54950, loss = 21.35, batch loss = 21.27 (9.0 examples/sec; 0.884 sec/batch; 68h:10m:09s remains)
INFO - root - 2017-12-07 22:43:15.212313: step 54960, loss = 21.77, batch loss = 21.69 (8.8 examples/sec; 0.908 sec/batch; 70h:00m:41s remains)
INFO - root - 2017-12-07 22:43:24.683226: step 54970, loss = 21.46, batch loss = 21.37 (8.6 examples/sec; 0.931 sec/batch; 71h:44m:47s remains)
INFO - root - 2017-12-07 22:43:34.015969: step 54980, loss = 21.36, batch loss = 21.28 (8.2 examples/sec; 0.979 sec/batch; 75h:28m:40s remains)
INFO - root - 2017-12-07 22:43:43.393152: step 54990, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.948 sec/batch; 73h:06m:23s remains)
INFO - root - 2017-12-07 22:43:52.808527: step 55000, loss = 21.27, batch loss = 21.19 (8.6 examples/sec; 0.925 sec/batch; 71h:20m:13s remains)
2017-12-07 22:43:53.764295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.48377 -4.5111032 -4.4817891 -4.4015856 -4.2925758 -4.1912193 -4.1268888 -4.0997009 -4.1107717 -4.1663313 -4.2317038 -4.2712545 -4.3051581 -4.334815 -4.3602414][-4.4938059 -4.5238185 -4.4914355 -4.3921366 -4.2571654 -4.13702 -4.0600362 -4.0280514 -4.0359859 -4.0855947 -4.1499715 -4.1923561 -4.2264285 -4.2498546 -4.2704344][-4.509294 -4.54239 -4.5115933 -4.4020748 -4.2451897 -4.1065927 -4.0172477 -3.9775081 -3.9785147 -4.017529 -4.0719967 -4.1141448 -4.1525826 -4.1734557 -4.1893864][-4.5240955 -4.56097 -4.535161 -4.4256363 -4.2556238 -4.101985 -4.0013018 -3.9496861 -3.9391685 -3.9669929 -4.0112128 -4.056304 -4.1031857 -4.1267705 -4.1418934][-4.5204358 -4.5690322 -4.5568652 -4.4584064 -4.2851887 -4.1166973 -3.9989724 -3.9286783 -3.9047527 -3.9295185 -3.979938 -4.0476537 -4.1184893 -4.1543279 -4.1679988][-4.4843736 -4.550755 -4.5588174 -4.47996 -4.3116231 -4.1256 -3.9804564 -3.8839445 -3.8466043 -3.8812714 -3.95981 -4.0642467 -4.1667466 -4.2193904 -4.2346253][-4.4349241 -4.5196013 -4.548234 -4.4904943 -4.3315578 -4.1316915 -3.956167 -3.8290844 -3.7789142 -3.8301547 -3.9453099 -4.0848918 -4.2125826 -4.2863936 -4.3143768][-4.4053855 -4.5015521 -4.5417905 -4.4959173 -4.3496742 -4.15129 -3.9649353 -3.8281548 -3.7767837 -3.8356533 -3.9660769 -4.1156597 -4.2515879 -4.3448534 -4.390944][-4.4035506 -4.4952817 -4.5326152 -4.4941406 -4.3738532 -4.2014713 -4.0294294 -3.8997636 -3.8504915 -3.9005008 -4.017736 -4.1567788 -4.2909017 -4.4009171 -4.4672213][-4.4147143 -4.4776978 -4.499404 -4.4723797 -4.3922591 -4.2631507 -4.1141849 -3.9889233 -3.9316797 -3.9634175 -4.0621262 -4.1928616 -4.3333869 -4.4661551 -4.5580187][-4.4147921 -4.4369526 -4.4395928 -4.4294257 -4.3981867 -4.3269453 -4.2180805 -4.1081848 -4.0494156 -4.0708861 -4.1532469 -4.2668233 -4.3971734 -4.5277457 -4.6223736][-4.3895416 -4.3814316 -4.3725314 -4.383348 -4.3985023 -4.3853173 -4.3245492 -4.2389879 -4.1839523 -4.1983452 -4.2676163 -4.3624425 -4.4718304 -4.580277 -4.6557107][-4.3304739 -4.3130612 -4.3094568 -4.3432269 -4.3927288 -4.422996 -4.4061685 -4.3548584 -4.3189216 -4.3386388 -4.4000258 -4.4752269 -4.5533247 -4.6243057 -4.6651912][-4.2837234 -4.2714057 -4.2761183 -4.3223572 -4.3881879 -4.4446445 -4.4649215 -4.4527273 -4.4478221 -4.4814897 -4.5354037 -4.5825272 -4.6166339 -4.6377678 -4.6353579][-4.2973251 -4.2945209 -4.295763 -4.3325071 -4.3929524 -4.4601936 -4.5095148 -4.5325756 -4.5522332 -4.5865083 -4.6196995 -4.6328034 -4.6239457 -4.6019173 -4.5680337]]...]
INFO - root - 2017-12-07 22:44:03.121530: step 55010, loss = 21.66, batch loss = 21.57 (8.5 examples/sec; 0.941 sec/batch; 72h:34m:16s remains)
INFO - root - 2017-12-07 22:44:12.521926: step 55020, loss = 20.91, batch loss = 20.82 (8.4 examples/sec; 0.952 sec/batch; 73h:24m:08s remains)
INFO - root - 2017-12-07 22:44:21.945275: step 55030, loss = 21.51, batch loss = 21.43 (8.0 examples/sec; 1.005 sec/batch; 77h:26m:48s remains)
INFO - root - 2017-12-07 22:44:31.474689: step 55040, loss = 21.53, batch loss = 21.44 (8.6 examples/sec; 0.930 sec/batch; 71h:39m:12s remains)
INFO - root - 2017-12-07 22:44:40.984541: step 55050, loss = 21.54, batch loss = 21.45 (7.8 examples/sec; 1.021 sec/batch; 78h:41m:21s remains)
INFO - root - 2017-12-07 22:44:50.013698: step 55060, loss = 21.51, batch loss = 21.43 (9.0 examples/sec; 0.884 sec/batch; 68h:08m:20s remains)
INFO - root - 2017-12-07 22:44:59.197965: step 55070, loss = 21.36, batch loss = 21.28 (8.9 examples/sec; 0.904 sec/batch; 69h:38m:19s remains)
INFO - root - 2017-12-07 22:45:08.521594: step 55080, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.944 sec/batch; 72h:45m:24s remains)
INFO - root - 2017-12-07 22:45:17.916673: step 55090, loss = 21.19, batch loss = 21.11 (8.9 examples/sec; 0.900 sec/batch; 69h:20m:34s remains)
INFO - root - 2017-12-07 22:45:27.243249: step 55100, loss = 21.08, batch loss = 21.00 (8.7 examples/sec; 0.922 sec/batch; 71h:02m:41s remains)
2017-12-07 22:45:28.222066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4109511 -4.3933392 -4.3830409 -4.3783889 -4.3823032 -4.3946385 -4.410007 -4.4239068 -4.433497 -4.437798 -4.4403629 -4.4459171 -4.4619946 -4.4972839 -4.5664754][-4.3947887 -4.3897347 -4.3950882 -4.4079704 -4.4280725 -4.4508629 -4.4699235 -4.4821033 -4.489069 -4.4963946 -4.5068755 -4.5168133 -4.5205588 -4.522028 -4.5418324][-4.4036446 -4.4070549 -4.4285464 -4.4617577 -4.4965277 -4.51867 -4.5193424 -4.5067558 -4.5009155 -4.5186009 -4.5555964 -4.5909939 -4.5995169 -4.5748196 -4.5440984][-4.419807 -4.4370866 -4.4769526 -4.5234132 -4.5500975 -4.531745 -4.4673157 -4.3977056 -4.3769274 -4.4283872 -4.5252771 -4.6167679 -4.65568 -4.6263928 -4.5617228][-4.44049 -4.4713321 -4.5213423 -4.5581293 -4.5372443 -4.4292622 -4.2575908 -4.1160154 -4.0969191 -4.2134795 -4.3981786 -4.5654969 -4.65219 -4.6416578 -4.5703959][-4.4582367 -4.4970994 -4.5413504 -4.5403757 -4.4372606 -4.2101345 -3.9208159 -3.7196279 -3.7249193 -3.9232774 -4.2025242 -4.4456348 -4.5842385 -4.6093035 -4.5556726][-4.4686437 -4.508554 -4.5363622 -4.4868441 -4.3005304 -3.9715743 -3.601393 -3.3761582 -3.4170291 -3.6823978 -4.0308523 -4.32662 -4.5031552 -4.5623546 -4.5318036][-4.4730468 -4.5119863 -4.5276489 -4.4484215 -4.2209039 -3.8568621 -3.4791553 -3.2725568 -3.3372836 -3.6199687 -3.9839935 -4.2920833 -4.4779391 -4.5485044 -4.524271][-4.4743419 -4.5165076 -4.5378036 -4.4675627 -4.2598038 -3.9344676 -3.6090677 -3.4378338 -3.4960704 -3.74482 -4.0758424 -4.360682 -4.5280023 -4.581408 -4.5411067][-4.4718356 -4.5232162 -4.56646 -4.5388861 -4.3971715 -4.155643 -3.9073536 -3.7719605 -3.8112612 -4.0058775 -4.2749329 -4.5048008 -4.6256733 -4.6372786 -4.5674582][-4.4642282 -4.52313 -4.5911913 -4.6169467 -4.5579824 -4.4163213 -4.2548914 -4.1634893 -4.1908917 -4.3270063 -4.5117636 -4.6571097 -4.7092533 -4.6703482 -4.5757046][-4.4528432 -4.5108676 -4.5906711 -4.6540217 -4.663414 -4.6100631 -4.5284104 -4.4804425 -4.5000463 -4.5791206 -4.67579 -4.7336483 -4.7216854 -4.6481891 -4.5497117][-4.43873 -4.4869571 -4.559495 -4.6320963 -4.6764469 -4.6797452 -4.6569138 -4.6423826 -4.6539187 -4.6842594 -4.7099018 -4.7033706 -4.6545258 -4.5762391 -4.4975882][-4.4273996 -4.4581895 -4.5091677 -4.5648456 -4.6081605 -4.6302943 -4.6365266 -4.6397367 -4.6437683 -4.6433659 -4.6298361 -4.595583 -4.5436687 -4.4865856 -4.4411325][-4.4197845 -4.4338393 -4.4594636 -4.4869103 -4.5083623 -4.5210156 -4.527391 -4.53027 -4.5286455 -4.5200138 -4.50232 -4.4758716 -4.446126 -4.4205079 -4.40497]]...]
INFO - root - 2017-12-07 22:45:37.575899: step 55110, loss = 21.53, batch loss = 21.45 (8.9 examples/sec; 0.903 sec/batch; 69h:35m:18s remains)
INFO - root - 2017-12-07 22:45:47.143400: step 55120, loss = 21.17, batch loss = 21.08 (7.8 examples/sec; 1.022 sec/batch; 78h:46m:14s remains)
INFO - root - 2017-12-07 22:45:56.747361: step 55130, loss = 21.43, batch loss = 21.34 (8.3 examples/sec; 0.968 sec/batch; 74h:33m:27s remains)
INFO - root - 2017-12-07 22:46:06.130808: step 55140, loss = 21.51, batch loss = 21.43 (8.8 examples/sec; 0.911 sec/batch; 70h:11m:59s remains)
INFO - root - 2017-12-07 22:46:15.463560: step 55150, loss = 21.46, batch loss = 21.38 (9.3 examples/sec; 0.863 sec/batch; 66h:27m:54s remains)
INFO - root - 2017-12-07 22:46:24.968387: step 55160, loss = 21.41, batch loss = 21.33 (8.4 examples/sec; 0.948 sec/batch; 73h:03m:07s remains)
INFO - root - 2017-12-07 22:46:34.250646: step 55170, loss = 21.64, batch loss = 21.55 (8.3 examples/sec; 0.962 sec/batch; 74h:07m:02s remains)
INFO - root - 2017-12-07 22:46:43.535309: step 55180, loss = 21.11, batch loss = 21.03 (8.2 examples/sec; 0.975 sec/batch; 75h:04m:59s remains)
INFO - root - 2017-12-07 22:46:52.845724: step 55190, loss = 21.59, batch loss = 21.51 (8.9 examples/sec; 0.898 sec/batch; 69h:12m:24s remains)
INFO - root - 2017-12-07 22:47:02.367337: step 55200, loss = 21.41, batch loss = 21.33 (8.9 examples/sec; 0.896 sec/batch; 69h:00m:51s remains)
2017-12-07 22:47:03.342120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3584452 -4.4090147 -4.4141316 -4.4051571 -4.4450526 -4.5005083 -4.4860396 -4.413321 -4.3789372 -4.43581 -4.5321693 -4.5700865 -4.5038042 -4.3889351 -4.3224931][-4.4536233 -4.4949384 -4.475287 -4.4129748 -4.3769708 -4.3665657 -4.3196006 -4.2517772 -4.2624917 -4.3751211 -4.514842 -4.5959759 -4.5764294 -4.4928207 -4.4213018][-4.5524216 -4.57571 -4.5318384 -4.4270635 -4.3179421 -4.2306228 -4.142601 -4.0829382 -4.1472907 -4.3217731 -4.5090618 -4.63443 -4.6604815 -4.6009641 -4.5190763][-4.5968556 -4.610961 -4.5687623 -4.4646425 -4.3204823 -4.1785164 -4.0580277 -4.0027695 -4.1017981 -4.3142786 -4.5286422 -4.6739364 -4.7185869 -4.6593151 -4.5566416][-4.6047225 -4.6222014 -4.5979738 -4.5157132 -4.3635006 -4.1918073 -4.053318 -4.0008106 -4.1100984 -4.3344655 -4.5598288 -4.7070384 -4.7414556 -4.6605372 -4.5313683][-4.6045585 -4.6247468 -4.6134629 -4.5417681 -4.378768 -4.1870351 -4.0378785 -3.9868677 -4.0888438 -4.3101287 -4.5441551 -4.7005396 -4.7304487 -4.6332836 -4.48676][-4.6129932 -4.6275787 -4.6101861 -4.5260735 -4.3386793 -4.1223907 -3.9606225 -3.9128072 -4.0146766 -4.2358241 -4.4789081 -4.648602 -4.6849241 -4.5887728 -4.4450469][-4.6393619 -4.6477337 -4.6179543 -4.511271 -4.29465 -4.0504761 -3.8724287 -3.8327599 -3.9456224 -4.1699433 -4.4117379 -4.5830941 -4.624722 -4.5487709 -4.433537][-4.6584406 -4.6738439 -4.6418443 -4.5281844 -4.3041062 -4.0412822 -3.8392692 -3.7916849 -3.9091418 -4.1327929 -4.3660393 -4.5273066 -4.5750966 -4.5319052 -4.461091][-4.6623116 -4.6937337 -4.6761651 -4.5819087 -4.3808675 -4.1217546 -3.8996518 -3.8300884 -3.9349861 -4.1486626 -4.3663197 -4.5145144 -4.569056 -4.5591598 -4.5264311][-4.65746 -4.70115 -4.7011614 -4.6396227 -4.479146 -4.2465334 -4.0239391 -3.9409819 -4.0313907 -4.23238 -4.4322252 -4.5638909 -4.6166053 -4.6210804 -4.6072688][-4.6226563 -4.6730227 -4.6934247 -4.672533 -4.5673776 -4.3899956 -4.20216 -4.1242194 -4.195303 -4.366745 -4.53394 -4.6343775 -4.6649656 -4.6594987 -4.6461978][-4.59157 -4.6470389 -4.687592 -4.7047729 -4.6560526 -4.5442033 -4.4106774 -4.3478127 -4.3902946 -4.5109749 -4.6305127 -4.6952076 -4.7024741 -4.6820254 -4.66031][-4.6031637 -4.6659937 -4.719593 -4.7577262 -4.7470584 -4.6829705 -4.5930185 -4.5347919 -4.5345116 -4.5900826 -4.6594553 -4.7060146 -4.7161674 -4.69881 -4.672462][-4.6265512 -4.6972375 -4.754776 -4.7962441 -4.8008685 -4.7569218 -4.6761079 -4.5893354 -4.5224671 -4.5030994 -4.5318246 -4.5889015 -4.6393685 -4.6578736 -4.6461034]]...]
INFO - root - 2017-12-07 22:47:12.777246: step 55210, loss = 21.42, batch loss = 21.34 (8.0 examples/sec; 1.002 sec/batch; 77h:12m:41s remains)
INFO - root - 2017-12-07 22:47:22.164422: step 55220, loss = 21.89, batch loss = 21.81 (8.0 examples/sec; 1.005 sec/batch; 77h:24m:18s remains)
INFO - root - 2017-12-07 22:47:31.535929: step 55230, loss = 22.01, batch loss = 21.92 (8.2 examples/sec; 0.970 sec/batch; 74h:41m:35s remains)
INFO - root - 2017-12-07 22:47:40.982476: step 55240, loss = 21.25, batch loss = 21.17 (8.6 examples/sec; 0.931 sec/batch; 71h:43m:52s remains)
INFO - root - 2017-12-07 22:47:50.470233: step 55250, loss = 21.55, batch loss = 21.47 (8.6 examples/sec; 0.933 sec/batch; 71h:53m:26s remains)
INFO - root - 2017-12-07 22:47:59.824373: step 55260, loss = 21.58, batch loss = 21.50 (8.4 examples/sec; 0.952 sec/batch; 73h:20m:45s remains)
INFO - root - 2017-12-07 22:48:09.103535: step 55270, loss = 21.48, batch loss = 21.40 (8.3 examples/sec; 0.964 sec/batch; 74h:16m:23s remains)
INFO - root - 2017-12-07 22:48:18.542678: step 55280, loss = 21.21, batch loss = 21.13 (8.9 examples/sec; 0.900 sec/batch; 69h:17m:17s remains)
INFO - root - 2017-12-07 22:48:27.962881: step 55290, loss = 21.39, batch loss = 21.30 (9.4 examples/sec; 0.855 sec/batch; 65h:51m:05s remains)
INFO - root - 2017-12-07 22:48:37.414614: step 55300, loss = 21.35, batch loss = 21.27 (8.7 examples/sec; 0.924 sec/batch; 71h:06m:41s remains)
2017-12-07 22:48:38.355128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3322716 -4.3085814 -4.2877431 -4.2914972 -4.2969265 -4.3422222 -4.4441676 -4.5265732 -4.5300641 -4.4941025 -4.4676251 -4.469141 -4.484622 -4.4955931 -4.4956851][-4.4055886 -4.3972616 -4.3833051 -4.3885789 -4.3989048 -4.4393835 -4.5126262 -4.547008 -4.5086884 -4.4581203 -4.4379292 -4.4629588 -4.5083661 -4.5466228 -4.5666842][-4.423346 -4.4306917 -4.4314613 -4.4445748 -4.4659824 -4.506259 -4.5496039 -4.5356979 -4.4583683 -4.3868613 -4.3632159 -4.4017706 -4.4698544 -4.5280623 -4.5613084][-4.3360229 -4.3474526 -4.3589888 -4.383214 -4.4223862 -4.4731412 -4.4976816 -4.4497871 -4.3474541 -4.2617226 -4.2372484 -4.2897844 -4.3789787 -4.4536376 -4.4949355][-4.1963415 -4.1982965 -4.2098427 -4.2387066 -4.2893734 -4.3430934 -4.3435407 -4.2631359 -4.1445651 -4.0600224 -4.05214 -4.1334596 -4.25751 -4.3621092 -4.4181356][-4.0675292 -4.0576272 -4.0635815 -4.0939794 -4.1483293 -4.1928897 -4.1544666 -4.0313048 -3.9015009 -3.8410254 -3.8711782 -3.9955187 -4.1641631 -4.3042183 -4.373795][-4.0100846 -3.9980769 -3.9991786 -4.0256662 -4.0694561 -4.0871482 -3.9937282 -3.8157358 -3.6728587 -3.648206 -3.7380486 -3.9221475 -4.137599 -4.2978697 -4.3623][-4.0392718 -4.0278335 -4.038744 -4.0743237 -4.1110067 -4.10128 -3.9656453 -3.7481139 -3.5920634 -3.5885406 -3.7210081 -3.9418747 -4.1627741 -4.2948132 -4.3209128][-4.1146226 -4.0974574 -4.121736 -4.1738915 -4.2162347 -4.2048869 -4.0804496 -3.8849478 -3.7500517 -3.7565212 -3.8896499 -4.0822964 -4.2316713 -4.2692156 -4.2201366][-4.2536979 -4.241334 -4.2717981 -4.3221011 -4.354188 -4.33965 -4.2481327 -4.1093321 -4.0172205 -4.0326486 -4.1431303 -4.2705255 -4.3157024 -4.2395024 -4.1111817][-4.3961768 -4.3971572 -4.4309173 -4.4733648 -4.4949226 -4.4822788 -4.4229536 -4.3355112 -4.2777009 -4.2916369 -4.3694983 -4.4330697 -4.4013028 -4.2557807 -4.0821581][-4.4620023 -4.4774747 -4.5095148 -4.5392985 -4.5529084 -4.5441365 -4.5100222 -4.4582014 -4.42216 -4.4355149 -4.4950233 -4.5282526 -4.4770112 -4.3295431 -4.1632609][-4.4558082 -4.47928 -4.5057616 -4.5230403 -4.5297313 -4.5235 -4.5024753 -4.4675074 -4.4430346 -4.4601521 -4.5191655 -4.552319 -4.5192428 -4.4138503 -4.289535][-4.391293 -4.4114866 -4.4316726 -4.442616 -4.448245 -4.4468517 -4.435751 -4.4145527 -4.4050488 -4.4344397 -4.4992533 -4.5381327 -4.52712 -4.4689274 -4.3948693][-4.3253274 -4.3358564 -4.3470387 -4.350472 -4.3511915 -4.3483019 -4.3414364 -4.3331418 -4.3438191 -4.3923545 -4.4680438 -4.5158834 -4.5205669 -4.4957309 -4.4611669]]...]
INFO - root - 2017-12-07 22:48:47.794260: step 55310, loss = 21.54, batch loss = 21.46 (8.9 examples/sec; 0.897 sec/batch; 69h:03m:10s remains)
INFO - root - 2017-12-07 22:48:57.197794: step 55320, loss = 21.67, batch loss = 21.58 (8.8 examples/sec; 0.907 sec/batch; 69h:48m:35s remains)
INFO - root - 2017-12-07 22:49:06.675750: step 55330, loss = 21.46, batch loss = 21.38 (8.2 examples/sec; 0.973 sec/batch; 74h:56m:31s remains)
INFO - root - 2017-12-07 22:49:16.286926: step 55340, loss = 21.27, batch loss = 21.19 (7.7 examples/sec; 1.036 sec/batch; 79h:45m:25s remains)
INFO - root - 2017-12-07 22:49:25.805434: step 55350, loss = 21.24, batch loss = 21.15 (8.0 examples/sec; 1.006 sec/batch; 77h:27m:42s remains)
INFO - root - 2017-12-07 22:49:35.190776: step 55360, loss = 21.54, batch loss = 21.46 (8.2 examples/sec; 0.973 sec/batch; 74h:54m:16s remains)
INFO - root - 2017-12-07 22:49:44.475518: step 55370, loss = 21.45, batch loss = 21.37 (8.6 examples/sec; 0.935 sec/batch; 71h:59m:07s remains)
INFO - root - 2017-12-07 22:49:53.904370: step 55380, loss = 21.51, batch loss = 21.42 (8.7 examples/sec; 0.920 sec/batch; 70h:49m:21s remains)
INFO - root - 2017-12-07 22:50:03.395754: step 55390, loss = 21.57, batch loss = 21.49 (8.3 examples/sec; 0.965 sec/batch; 74h:19m:09s remains)
INFO - root - 2017-12-07 22:50:12.753529: step 55400, loss = 21.60, batch loss = 21.52 (8.5 examples/sec; 0.940 sec/batch; 72h:19m:05s remains)
2017-12-07 22:50:13.620661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2813869 -4.3968711 -4.4978509 -4.5700278 -4.5918417 -4.5497456 -4.4734821 -4.4225841 -4.3789654 -4.2934394 -4.1882706 -4.0811424 -3.9844518 -3.9850559 -4.1083612][-4.27797 -4.4064026 -4.523016 -4.6024303 -4.62075 -4.573647 -4.5019131 -4.4627566 -4.4381108 -4.3700633 -4.2763591 -4.1933756 -4.1358829 -4.1610069 -4.2730994][-4.2944226 -4.4246526 -4.5351472 -4.5960059 -4.5850925 -4.5179095 -4.4517484 -4.4332881 -4.4402013 -4.4155517 -4.3638844 -4.3240981 -4.3095107 -4.3452525 -4.42403][-4.2937832 -4.4053264 -4.4919224 -4.5188813 -4.4653668 -4.3618703 -4.2820234 -4.2760863 -4.3276806 -4.379518 -4.4057407 -4.4180341 -4.4243875 -4.4398332 -4.4716229][-4.281548 -4.3598075 -4.4163055 -4.4104347 -4.3119979 -4.1553135 -4.0278249 -4.0065069 -4.1080351 -4.2677665 -4.4012351 -4.4706459 -4.4754496 -4.4476323 -4.4322405][-4.2613549 -4.3195438 -4.3556833 -4.3242364 -4.1840229 -3.9662471 -3.7690887 -3.7094326 -3.8568356 -4.12993 -4.370182 -4.48681 -4.4755931 -4.3975716 -4.3419347][-4.2607384 -4.3195992 -4.3510189 -4.3040118 -4.1304026 -3.8570189 -3.5969791 -3.4999752 -3.680759 -4.0328245 -4.3347049 -4.4623489 -4.4219146 -4.3002906 -4.2134218][-4.3035259 -4.3762894 -4.418807 -4.3728237 -4.1847558 -3.8875093 -3.609868 -3.5105863 -3.6982334 -4.0518122 -4.3300529 -4.4137807 -4.3301697 -4.1821284 -4.0817184][-4.3806343 -4.4634075 -4.518899 -4.4874678 -4.3153553 -4.0425124 -3.8053498 -3.7402687 -3.9066339 -4.1865911 -4.3650174 -4.3593178 -4.22435 -4.0669708 -3.9740772][-4.4817739 -4.5548406 -4.6091733 -4.588408 -4.4475784 -4.2320113 -4.0672283 -4.043745 -4.1682844 -4.3454657 -4.4154243 -4.3271904 -4.1516643 -3.9826612 -3.8885636][-4.561832 -4.615912 -4.6555805 -4.6390381 -4.5374861 -4.3985295 -4.3118596 -4.3138456 -4.381494 -4.4678931 -4.478682 -4.3627357 -4.1712074 -3.9798579 -3.8614316][-4.5692511 -4.6078792 -4.6348057 -4.6269808 -4.573226 -4.5146885 -4.4946618 -4.5033307 -4.5145187 -4.5443158 -4.55457 -4.4634318 -4.2721334 -4.0557232 -3.9112144][-4.4995422 -4.52967 -4.5588098 -4.5733991 -4.5714574 -4.5801396 -4.6065788 -4.6136527 -4.5820346 -4.5791368 -4.6090345 -4.5620432 -4.3868694 -4.1644936 -4.0183392][-4.3960452 -4.4186959 -4.4512453 -4.4849348 -4.5224819 -4.5764661 -4.6355481 -4.6517267 -4.6072478 -4.5853634 -4.6202364 -4.6057124 -4.467977 -4.2832584 -4.1751261][-4.3107066 -4.322742 -4.3447123 -4.3732347 -4.4164677 -4.4845586 -4.5651379 -4.6100273 -4.5932207 -4.5775013 -4.6068549 -4.6099334 -4.5267892 -4.411777 -4.3567772]]...]
INFO - root - 2017-12-07 22:50:22.989448: step 55410, loss = 21.64, batch loss = 21.56 (8.7 examples/sec; 0.917 sec/batch; 70h:36m:39s remains)
INFO - root - 2017-12-07 22:50:32.311432: step 55420, loss = 21.21, batch loss = 21.13 (8.1 examples/sec; 0.988 sec/batch; 76h:03m:02s remains)
INFO - root - 2017-12-07 22:50:41.744206: step 55430, loss = 21.53, batch loss = 21.44 (8.8 examples/sec; 0.906 sec/batch; 69h:45m:19s remains)
INFO - root - 2017-12-07 22:50:51.112686: step 55440, loss = 21.53, batch loss = 21.44 (8.9 examples/sec; 0.897 sec/batch; 69h:00m:00s remains)
INFO - root - 2017-12-07 22:51:00.575139: step 55450, loss = 21.41, batch loss = 21.32 (8.8 examples/sec; 0.912 sec/batch; 70h:10m:25s remains)
INFO - root - 2017-12-07 22:51:09.979415: step 55460, loss = 21.48, batch loss = 21.40 (8.3 examples/sec; 0.965 sec/batch; 74h:16m:29s remains)
INFO - root - 2017-12-07 22:51:19.224793: step 55470, loss = 21.44, batch loss = 21.36 (8.2 examples/sec; 0.977 sec/batch; 75h:09m:04s remains)
INFO - root - 2017-12-07 22:51:28.700577: step 55480, loss = 21.15, batch loss = 21.07 (8.1 examples/sec; 0.993 sec/batch; 76h:23m:36s remains)
INFO - root - 2017-12-07 22:51:38.183851: step 55490, loss = 21.39, batch loss = 21.31 (8.3 examples/sec; 0.959 sec/batch; 73h:45m:44s remains)
INFO - root - 2017-12-07 22:51:47.566477: step 55500, loss = 21.71, batch loss = 21.63 (8.6 examples/sec; 0.930 sec/batch; 71h:31m:35s remains)
2017-12-07 22:51:48.473418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5806479 -4.5914536 -4.6749887 -4.7623477 -4.7727814 -4.7087193 -4.6170764 -4.5510569 -4.5502906 -4.5854053 -4.610321 -4.6368041 -4.668632 -4.6777186 -4.6761594][-4.6088934 -4.6331611 -4.7372503 -4.8377981 -4.8437748 -4.7656474 -4.6587372 -4.5881929 -4.598496 -4.6433139 -4.6633062 -4.6726542 -4.6726637 -4.6309938 -4.6062427][-4.6440239 -4.6774278 -4.7764697 -4.8538451 -4.8220448 -4.7139816 -4.5887408 -4.5173731 -4.5553842 -4.6302385 -4.662806 -4.6660032 -4.6411848 -4.558373 -4.5227504][-4.6460609 -4.6917963 -4.7702937 -4.7904105 -4.6939216 -4.5498829 -4.4019079 -4.3260884 -4.4002028 -4.5197639 -4.5829592 -4.601809 -4.579638 -4.4868751 -4.4624104][-4.6041508 -4.6868429 -4.7397504 -4.680522 -4.513144 -4.3332438 -4.1505566 -4.0573 -4.1720138 -4.3524528 -4.4591341 -4.5048051 -4.501102 -4.4177413 -4.4105911][-4.5804358 -4.6945014 -4.705369 -4.5462532 -4.2985659 -4.0738993 -3.8497427 -3.7457769 -3.9241316 -4.190218 -4.35302 -4.4348674 -4.4603319 -4.3966103 -4.4051113][-4.579855 -4.7043624 -4.67708 -4.4434948 -4.1351857 -3.865572 -3.6053655 -3.5113633 -3.7661452 -4.1032424 -4.2943764 -4.3907967 -4.4424138 -4.4108844 -4.4379358][-4.6207504 -4.7461181 -4.7212505 -4.4897118 -4.179306 -3.8926716 -3.6226211 -3.5433946 -3.8209317 -4.159585 -4.3253069 -4.3975925 -4.4480977 -4.4353938 -4.4697547][-4.6933455 -4.8081293 -4.8049383 -4.6266031 -4.3608065 -4.0932875 -3.8483369 -3.7865045 -4.0259485 -4.3048525 -4.4218445 -4.4654403 -4.5036492 -4.5000839 -4.5339212][-4.7734671 -4.8659921 -4.8797865 -4.7724771 -4.5863938 -4.3827629 -4.2045403 -4.1630368 -4.3303647 -4.519875 -4.5915484 -4.616961 -4.640224 -4.6298594 -4.6456246][-4.8705249 -4.9353271 -4.949924 -4.8959026 -4.7899842 -4.6686034 -4.56929 -4.5513721 -4.6497512 -4.7569265 -4.7914119 -4.7960987 -4.7967072 -4.7757139 -4.7747588][-4.9060864 -4.9386563 -4.9380665 -4.9092035 -4.8586893 -4.7998729 -4.7471 -4.7286091 -4.7716846 -4.8263555 -4.8451982 -4.842968 -4.8357406 -4.8197689 -4.8211675][-4.8310943 -4.8364768 -4.8247838 -4.8140645 -4.8092041 -4.7996054 -4.7658644 -4.7264266 -4.7245631 -4.7496014 -4.7658377 -4.7648048 -4.7543216 -4.7398262 -4.7391739][-4.7112775 -4.7084255 -4.6980252 -4.7010946 -4.7216082 -4.7303896 -4.6910071 -4.625164 -4.60005 -4.6300554 -4.6728892 -4.692112 -4.6824584 -4.6566448 -4.6354403][-4.5641427 -4.5646176 -4.5609026 -4.5777259 -4.6056809 -4.6021428 -4.5389314 -4.4552283 -4.4341292 -4.501121 -4.5951052 -4.648984 -4.6426811 -4.5977616 -4.5471349]]...]
INFO - root - 2017-12-07 22:51:57.901946: step 55510, loss = 21.41, batch loss = 21.33 (9.6 examples/sec; 0.836 sec/batch; 64h:18m:01s remains)
INFO - root - 2017-12-07 22:52:07.201109: step 55520, loss = 21.60, batch loss = 21.52 (9.0 examples/sec; 0.894 sec/batch; 68h:46m:16s remains)
INFO - root - 2017-12-07 22:52:16.660304: step 55530, loss = 21.43, batch loss = 21.35 (8.0 examples/sec; 0.995 sec/batch; 76h:34m:21s remains)
INFO - root - 2017-12-07 22:52:25.760331: step 55540, loss = 21.54, batch loss = 21.45 (8.7 examples/sec; 0.919 sec/batch; 70h:43m:04s remains)
INFO - root - 2017-12-07 22:52:34.979553: step 55550, loss = 21.92, batch loss = 21.84 (8.6 examples/sec; 0.931 sec/batch; 71h:38m:30s remains)
INFO - root - 2017-12-07 22:52:44.478799: step 55560, loss = 21.39, batch loss = 21.31 (7.9 examples/sec; 1.009 sec/batch; 77h:36m:38s remains)
INFO - root - 2017-12-07 22:52:53.856106: step 55570, loss = 21.31, batch loss = 21.23 (8.1 examples/sec; 0.982 sec/batch; 75h:33m:28s remains)
INFO - root - 2017-12-07 22:53:02.890745: step 55580, loss = 21.16, batch loss = 21.08 (8.2 examples/sec; 0.977 sec/batch; 75h:09m:46s remains)
INFO - root - 2017-12-07 22:53:12.236793: step 55590, loss = 21.61, batch loss = 21.53 (8.4 examples/sec; 0.950 sec/batch; 73h:04m:31s remains)
INFO - root - 2017-12-07 22:53:21.619375: step 55600, loss = 21.16, batch loss = 21.07 (8.5 examples/sec; 0.937 sec/batch; 72h:02m:28s remains)
2017-12-07 22:53:22.559100: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2154503 -4.2632465 -4.3138847 -4.3331442 -4.3334336 -4.2929797 -4.2430658 -4.2447171 -4.2941771 -4.3483958 -4.3771615 -4.4003682 -4.4075069 -4.3644261 -4.2765145][-4.1267638 -4.142879 -4.1806531 -4.1930819 -4.209373 -4.211494 -4.1950841 -4.2192631 -4.2918692 -4.3603864 -4.3922734 -4.4151864 -4.4203997 -4.3707161 -4.2676458][-4.0256839 -4.0331445 -4.0710516 -4.0775828 -4.1032119 -4.1406388 -4.1589293 -4.2100649 -4.3067636 -4.3806567 -4.4051471 -4.4179931 -4.4132719 -4.3527017 -4.240428][-4.0068135 -4.0289907 -4.0802937 -4.0844231 -4.0999475 -4.133656 -4.1499505 -4.1934485 -4.2793589 -4.33323 -4.3445439 -4.3587441 -4.3620768 -4.3068476 -4.1959939][-4.0730777 -4.11558 -4.1805172 -4.1817236 -4.176703 -4.1808934 -4.1590643 -4.1537976 -4.1864157 -4.2002549 -4.2040071 -4.2408719 -4.2837195 -4.262701 -4.1687422][-4.1665692 -4.2221479 -4.293335 -4.2932768 -4.2685313 -4.2385197 -4.1687932 -4.0997677 -4.0664263 -4.0463653 -4.0573211 -4.1324039 -4.2307286 -4.2579236 -4.1906981][-4.2496376 -4.3177619 -4.3871832 -4.3719144 -4.3065624 -4.2254763 -4.1166306 -4.0123978 -3.9507813 -3.934433 -3.9689589 -4.0744076 -4.2102776 -4.2767582 -4.238615][-4.3009844 -4.3758278 -4.4312639 -4.3787003 -4.2555108 -4.1225944 -4.0048084 -3.9210081 -3.8879035 -3.9127486 -3.97814 -4.08906 -4.2271738 -4.3063517 -4.2892876][-4.3349204 -4.4088869 -4.4407587 -4.3448243 -4.16993 -4.00844 -3.9183264 -3.8931348 -3.9124746 -3.9777079 -4.055985 -4.1419916 -4.2483282 -4.3151755 -4.3093562][-4.3400884 -4.4016452 -4.4140582 -4.3019094 -4.1173964 -3.9660897 -3.9225321 -3.9550195 -4.0111518 -4.0835924 -4.1409745 -4.1792927 -4.2352452 -4.2772193 -4.2768073][-4.3117247 -4.3624444 -4.3712721 -4.272336 -4.1153936 -4.0070224 -4.0110035 -4.0785966 -4.1400909 -4.1874971 -4.2035522 -4.1928487 -4.2026196 -4.2202253 -4.2184548][-4.2789488 -4.3138156 -4.318646 -4.2389646 -4.1279535 -4.0863662 -4.1422982 -4.2249475 -4.2675338 -4.2720509 -4.2467947 -4.2096639 -4.1999903 -4.209506 -4.20938][-4.2950807 -4.3056912 -4.3009763 -4.2333937 -4.1633344 -4.1817245 -4.2750244 -4.355464 -4.3712573 -4.3429012 -4.2969108 -4.2590127 -4.2513008 -4.261384 -4.2631049][-4.3322067 -4.3279343 -4.3185134 -4.2634215 -4.2269855 -4.2777238 -4.369771 -4.4274874 -4.4228234 -4.3835311 -4.342598 -4.3216467 -4.3254952 -4.3325157 -4.3261781][-4.3693666 -4.3614745 -4.3590107 -4.3255439 -4.3174167 -4.3716688 -4.4335876 -4.4596605 -4.4468431 -4.4164538 -4.3952961 -4.3940029 -4.4030638 -4.3978767 -4.3775253]]...]
INFO - root - 2017-12-07 22:53:32.027627: step 55610, loss = 22.10, batch loss = 22.02 (8.6 examples/sec; 0.933 sec/batch; 71h:47m:35s remains)
INFO - root - 2017-12-07 22:53:41.480243: step 55620, loss = 21.36, batch loss = 21.27 (8.5 examples/sec; 0.939 sec/batch; 72h:11m:02s remains)
INFO - root - 2017-12-07 22:53:50.940595: step 55630, loss = 21.55, batch loss = 21.46 (8.9 examples/sec; 0.894 sec/batch; 68h:44m:58s remains)
INFO - root - 2017-12-07 22:54:00.295499: step 55640, loss = 21.06, batch loss = 20.98 (8.6 examples/sec; 0.933 sec/batch; 71h:45m:20s remains)
INFO - root - 2017-12-07 22:54:09.645444: step 55650, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.927 sec/batch; 71h:15m:04s remains)
INFO - root - 2017-12-07 22:54:18.939480: step 55660, loss = 21.41, batch loss = 21.33 (9.1 examples/sec; 0.883 sec/batch; 67h:54m:34s remains)
INFO - root - 2017-12-07 22:54:28.325708: step 55670, loss = 21.52, batch loss = 21.44 (9.5 examples/sec; 0.843 sec/batch; 64h:47m:49s remains)
INFO - root - 2017-12-07 22:54:37.767963: step 55680, loss = 21.53, batch loss = 21.44 (8.5 examples/sec; 0.937 sec/batch; 72h:03m:57s remains)
INFO - root - 2017-12-07 22:54:47.211358: step 55690, loss = 21.03, batch loss = 20.95 (8.2 examples/sec; 0.972 sec/batch; 74h:45m:01s remains)
INFO - root - 2017-12-07 22:54:56.702077: step 55700, loss = 21.24, batch loss = 21.15 (8.0 examples/sec; 0.997 sec/batch; 76h:40m:27s remains)
2017-12-07 22:54:57.784151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.418016 -4.4010897 -4.3997116 -4.4278793 -4.467628 -4.4967442 -4.5269628 -4.5643487 -4.5744109 -4.5228324 -4.4329448 -4.3425794 -4.2865028 -4.2734818 -4.3058491][-4.487926 -4.4766669 -4.4846492 -4.5188847 -4.5418105 -4.5237589 -4.4891415 -4.4741645 -4.46255 -4.4073133 -4.3200135 -4.2347169 -4.1958013 -4.2026424 -4.2402005][-4.5407448 -4.5274334 -4.5311189 -4.5650411 -4.5774212 -4.5210762 -4.4236627 -4.3554344 -4.3275762 -4.2801337 -4.1998062 -4.1172719 -4.0933194 -4.124361 -4.1699615][-4.4819522 -4.484087 -4.4989042 -4.5373993 -4.540091 -4.4529414 -4.3136206 -4.2194281 -4.2056808 -4.1948137 -4.1351776 -4.0551291 -4.0409937 -4.098453 -4.1517024][-4.3713579 -4.3970065 -4.4226356 -4.4501839 -4.4230924 -4.3005896 -4.1445527 -4.0615726 -4.0926604 -4.1473179 -4.1352081 -4.0752816 -4.0751987 -4.1595564 -4.2250443][-4.2816696 -4.3200622 -4.3479624 -4.35571 -4.2861671 -4.1175208 -3.9469228 -3.8831162 -3.9585025 -4.0769238 -4.1231928 -4.0989132 -4.1168962 -4.223742 -4.3128967][-4.2503133 -4.2786164 -4.2883868 -4.2578092 -4.1327758 -3.9135642 -3.7279034 -3.6838005 -3.7988939 -3.9784632 -4.0913787 -4.1173377 -4.1555123 -4.27114 -4.3748522][-4.2937036 -4.2876778 -4.2589989 -4.1844826 -4.0201211 -3.78105 -3.5952082 -3.5570512 -3.6803532 -3.886781 -4.0467172 -4.1227708 -4.1865516 -4.3046322 -4.4135094][-4.38888 -4.3592839 -4.3086262 -4.221745 -4.0679941 -3.8586192 -3.691766 -3.6313932 -3.6996427 -3.8579843 -4.0110335 -4.1140594 -4.2070522 -4.338376 -4.454145][-4.4670095 -4.4434161 -4.4007549 -4.3321996 -4.2175193 -4.0597243 -3.9260192 -3.8580575 -3.8772655 -3.9753604 -4.0912809 -4.1833019 -4.2765179 -4.4048738 -4.5169072][-4.4945807 -4.4872813 -4.4598932 -4.4169979 -4.3470936 -4.2393003 -4.1391282 -4.0798531 -4.0813427 -4.1430664 -4.2170467 -4.2704873 -4.3345504 -4.4471784 -4.557611][-4.4537783 -4.46211 -4.4529424 -4.4429135 -4.4219041 -4.3618331 -4.2901554 -4.2411323 -4.2301016 -4.252172 -4.2683682 -4.2580347 -4.2749949 -4.3713074 -4.4920683][-4.3627634 -4.3793449 -4.3900261 -4.4124222 -4.432466 -4.4107437 -4.3636856 -4.3241272 -4.297893 -4.274467 -4.2254748 -4.148356 -4.124176 -4.214438 -4.3560495][-4.2738781 -4.2797117 -4.2984428 -4.3336277 -4.3638406 -4.353663 -4.3225145 -4.2953305 -4.2640944 -4.2099628 -4.1196318 -4.0124216 -3.9874179 -4.1068826 -4.2908597][-4.222085 -4.2165976 -4.2423162 -4.278914 -4.2971272 -4.2834911 -4.2648864 -4.2512 -4.2192483 -4.14489 -4.036665 -3.9344432 -3.9348412 -4.091198 -4.31305]]...]
INFO - root - 2017-12-07 22:55:07.096558: step 55710, loss = 21.81, batch loss = 21.73 (9.5 examples/sec; 0.841 sec/batch; 64h:40m:34s remains)
INFO - root - 2017-12-07 22:55:16.519269: step 55720, loss = 21.31, batch loss = 21.23 (8.9 examples/sec; 0.895 sec/batch; 68h:50m:08s remains)
INFO - root - 2017-12-07 22:55:26.083232: step 55730, loss = 21.95, batch loss = 21.87 (7.3 examples/sec; 1.096 sec/batch; 84h:13m:59s remains)
INFO - root - 2017-12-07 22:55:35.462905: step 55740, loss = 21.43, batch loss = 21.34 (8.4 examples/sec; 0.958 sec/batch; 73h:38m:14s remains)
INFO - root - 2017-12-07 22:55:44.622713: step 55750, loss = 21.50, batch loss = 21.42 (8.9 examples/sec; 0.899 sec/batch; 69h:05m:21s remains)
INFO - root - 2017-12-07 22:55:53.984267: step 55760, loss = 21.80, batch loss = 21.72 (8.7 examples/sec; 0.916 sec/batch; 70h:25m:02s remains)
INFO - root - 2017-12-07 22:56:03.236574: step 55770, loss = 21.65, batch loss = 21.56 (8.2 examples/sec; 0.971 sec/batch; 74h:36m:09s remains)
INFO - root - 2017-12-07 22:56:12.601536: step 55780, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.929 sec/batch; 71h:22m:22s remains)
INFO - root - 2017-12-07 22:56:21.871354: step 55790, loss = 21.42, batch loss = 21.34 (8.9 examples/sec; 0.903 sec/batch; 69h:24m:23s remains)
INFO - root - 2017-12-07 22:56:31.345332: step 55800, loss = 21.68, batch loss = 21.59 (8.3 examples/sec; 0.968 sec/batch; 74h:22m:27s remains)
2017-12-07 22:56:32.348097: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3327756 -4.3107095 -4.2261114 -4.168612 -4.2018232 -4.2892852 -4.3409014 -4.3504915 -4.376286 -4.3996134 -4.3800836 -4.3163257 -4.1836452 -4.0234718 -3.9469106][-4.2857451 -4.2487688 -4.1713662 -4.1434894 -4.2126904 -4.3311 -4.3981485 -4.4041076 -4.4118466 -4.4080577 -4.3664551 -4.2935228 -4.1703568 -4.0343876 -3.974709][-4.3134928 -4.2696996 -4.1979966 -4.1733522 -4.23681 -4.3524961 -4.4322157 -4.45782 -4.4722691 -4.4583306 -4.4005308 -4.3254466 -4.2322 -4.1430407 -4.1148872][-4.4197154 -4.368031 -4.2775974 -4.2167406 -4.228066 -4.3080635 -4.3912406 -4.4475451 -4.488029 -4.4857016 -4.4334297 -4.3813372 -4.3492727 -4.3290672 -4.3392215][-4.5693707 -4.5003757 -4.3773718 -4.262619 -4.1997876 -4.2198076 -4.2851644 -4.3586097 -4.4234071 -4.4434943 -4.4187255 -4.4097519 -4.4448032 -4.4924579 -4.539711][-4.6766572 -4.5871511 -4.4495325 -4.3068314 -4.1957712 -4.1594143 -4.1831179 -4.2324834 -4.282568 -4.3048763 -4.3074918 -4.3390069 -4.4206014 -4.5116067 -4.5884819][-4.6626916 -4.563025 -4.4406214 -4.3111391 -4.1971226 -4.1335816 -4.1068654 -4.1014152 -4.1103573 -4.1208682 -4.1521683 -4.2222409 -4.3264813 -4.4282174 -4.5125804][-4.5213804 -4.4204626 -4.3299823 -4.2409596 -4.1626892 -4.10785 -4.0446458 -3.9896653 -3.9644065 -3.9705524 -4.0362153 -4.1430964 -4.2525883 -4.3390942 -4.4053974][-4.3481436 -4.2515588 -4.1903405 -4.1394114 -4.1073151 -4.076499 -3.9985952 -3.9293342 -3.9057381 -3.9256184 -4.0183225 -4.1381335 -4.2301178 -4.2846651 -4.31914][-4.2437572 -4.1370964 -4.0831571 -4.0516586 -4.0510893 -4.0417624 -3.9744818 -3.9331496 -3.9514542 -4.0011277 -4.1009769 -4.2020154 -4.26076 -4.2833924 -4.2888575][-4.2634797 -4.1281471 -4.056149 -4.0219355 -4.0275521 -4.0223937 -3.9690769 -3.9684396 -4.0383153 -4.1197495 -4.2135539 -4.2857323 -4.3167324 -4.3202343 -4.3103395][-4.3176384 -4.1645508 -4.0825467 -4.0513005 -4.0630012 -4.0582628 -4.010149 -4.0275788 -4.1178226 -4.207479 -4.281146 -4.328485 -4.3523941 -4.3576503 -4.34888][-4.311317 -4.1729565 -4.1108985 -4.099782 -4.1244078 -4.1283603 -4.0822997 -4.0881996 -4.15564 -4.216949 -4.2556806 -4.2854867 -4.3194332 -4.3426728 -4.3434625][-4.2940412 -4.2002668 -4.1724296 -4.178349 -4.1974692 -4.196063 -4.1423306 -4.1204853 -4.1458035 -4.1603732 -4.1676078 -4.1957607 -4.2524419 -4.2987618 -4.3064404][-4.2965646 -4.2467527 -4.2426753 -4.2460022 -4.2389941 -4.2168012 -4.15504 -4.1181974 -4.1207952 -4.10829 -4.1049166 -4.1419468 -4.2194963 -4.284297 -4.2931614]]...]
INFO - root - 2017-12-07 22:56:41.718737: step 55810, loss = 21.39, batch loss = 21.31 (8.2 examples/sec; 0.973 sec/batch; 74h:48m:50s remains)
INFO - root - 2017-12-07 22:56:51.065711: step 55820, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.929 sec/batch; 71h:23m:08s remains)
INFO - root - 2017-12-07 22:57:00.475829: step 55830, loss = 21.94, batch loss = 21.86 (8.4 examples/sec; 0.952 sec/batch; 73h:07m:33s remains)
INFO - root - 2017-12-07 22:57:09.821577: step 55840, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.950 sec/batch; 73h:02m:29s remains)
INFO - root - 2017-12-07 22:57:19.367713: step 55850, loss = 21.30, batch loss = 21.22 (8.6 examples/sec; 0.931 sec/batch; 71h:32m:53s remains)
INFO - root - 2017-12-07 22:57:28.818978: step 55860, loss = 20.98, batch loss = 20.89 (8.7 examples/sec; 0.914 sec/batch; 70h:15m:57s remains)
INFO - root - 2017-12-07 22:57:38.300221: step 55870, loss = 21.45, batch loss = 21.36 (8.6 examples/sec; 0.936 sec/batch; 71h:53m:54s remains)
INFO - root - 2017-12-07 22:57:47.773836: step 55880, loss = 21.37, batch loss = 21.29 (8.5 examples/sec; 0.937 sec/batch; 71h:58m:20s remains)
INFO - root - 2017-12-07 22:57:57.267678: step 55890, loss = 21.40, batch loss = 21.32 (8.8 examples/sec; 0.908 sec/batch; 69h:43m:54s remains)
INFO - root - 2017-12-07 22:58:06.767492: step 55900, loss = 21.61, batch loss = 21.53 (8.5 examples/sec; 0.938 sec/batch; 72h:04m:37s remains)
2017-12-07 22:58:07.803554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1552281 -4.3431807 -4.5107689 -4.6247492 -4.6635427 -4.6318717 -4.5619097 -4.4928842 -4.441792 -4.4112015 -4.393343 -4.3639369 -4.31222 -4.2500372 -4.1992087][-4.3875923 -4.545671 -4.6763783 -4.7446704 -4.731226 -4.6545033 -4.561368 -4.49204 -4.4605846 -4.4576173 -4.4625931 -4.44901 -4.40228 -4.3382649 -4.2845254][-4.5715861 -4.6959753 -4.7798791 -4.7876511 -4.7138462 -4.5975242 -4.4946966 -4.4392257 -4.4365869 -4.4682188 -4.5046406 -4.5141473 -4.4772825 -4.4140587 -4.3592119][-4.649621 -4.7372885 -4.7730613 -4.7245951 -4.6028762 -4.4653 -4.3656945 -4.3266439 -4.3503261 -4.4178095 -4.49546 -4.5430288 -4.5289211 -4.4728141 -4.4156647][-4.6145577 -4.6654639 -4.6596274 -4.5740867 -4.4304638 -4.2872305 -4.1877842 -4.1523662 -4.19217 -4.2922392 -4.4174314 -4.517303 -4.5420108 -4.5049472 -4.4486413][-4.5225224 -4.5470772 -4.5194364 -4.4212966 -4.2705979 -4.1152048 -3.995976 -3.947403 -3.9967217 -4.1253953 -4.2933731 -4.4421849 -4.5083885 -4.4984431 -4.4520693][-4.4440346 -4.4588819 -4.4216847 -4.3096914 -4.1357837 -3.9476407 -3.7994337 -3.7494044 -3.8266182 -3.9904568 -4.1899753 -4.3670449 -4.4570966 -4.4627414 -4.4246664][-4.4150448 -4.4323192 -4.3910942 -4.2623625 -4.057497 -3.8305149 -3.6550398 -3.6128979 -3.7286384 -3.9297774 -4.1469212 -4.3251734 -4.4131737 -4.4161983 -4.3777704][-4.4200377 -4.4490762 -4.4209175 -4.2973661 -4.0884738 -3.8498943 -3.6663859 -3.6283903 -3.7578351 -3.9712665 -4.1868315 -4.3470721 -4.414 -4.397419 -4.3425965][-4.4280052 -4.4695549 -4.4662166 -4.373373 -4.197392 -3.9868574 -3.8237958 -3.7938209 -3.9144633 -4.1124949 -4.3076549 -4.439826 -4.4765978 -4.427844 -4.3420658][-4.4205709 -4.4732757 -4.498261 -4.4519725 -4.3345494 -4.1819048 -4.0584984 -4.0363374 -4.1362147 -4.3027587 -4.4645972 -4.5597506 -4.5578585 -4.4714975 -4.35501][-4.3949504 -4.4526315 -4.5032263 -4.5080447 -4.4590092 -4.3731179 -4.2969856 -4.28787 -4.3686986 -4.5007586 -4.6164351 -4.6591535 -4.607511 -4.4864864 -4.3595481][-4.3581038 -4.4057641 -4.4629159 -4.5011635 -4.507731 -4.4853415 -4.4616046 -4.4780045 -4.5536838 -4.6554766 -4.7213764 -4.7045517 -4.602457 -4.4604778 -4.34437][-4.3231826 -4.3513308 -4.395021 -4.43884 -4.4713163 -4.4891958 -4.506784 -4.5499125 -4.6287632 -4.7091484 -4.7345505 -4.6704731 -4.534461 -4.3897829 -4.2943835][-4.3019824 -4.3140917 -4.3387642 -4.3698783 -4.4001622 -4.4282966 -4.463922 -4.5204539 -4.5974007 -4.6603775 -4.6582532 -4.5679922 -4.4185596 -4.2771125 -4.1913824]]...]
INFO - root - 2017-12-07 22:58:17.173291: step 55910, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.951 sec/batch; 73h:05m:32s remains)
INFO - root - 2017-12-07 22:58:26.482483: step 55920, loss = 21.11, batch loss = 21.03 (8.4 examples/sec; 0.957 sec/batch; 73h:29m:59s remains)
INFO - root - 2017-12-07 22:58:35.923379: step 55930, loss = 21.36, batch loss = 21.28 (8.3 examples/sec; 0.961 sec/batch; 73h:51m:44s remains)
INFO - root - 2017-12-07 22:58:45.313925: step 55940, loss = 21.55, batch loss = 21.47 (8.7 examples/sec; 0.920 sec/batch; 70h:39m:01s remains)
INFO - root - 2017-12-07 22:58:54.708245: step 55950, loss = 21.39, batch loss = 21.31 (9.0 examples/sec; 0.889 sec/batch; 68h:16m:51s remains)
INFO - root - 2017-12-07 22:59:04.053900: step 55960, loss = 21.69, batch loss = 21.61 (8.8 examples/sec; 0.907 sec/batch; 69h:39m:32s remains)
INFO - root - 2017-12-07 22:59:13.553487: step 55970, loss = 21.31, batch loss = 21.23 (8.5 examples/sec; 0.939 sec/batch; 72h:06m:31s remains)
INFO - root - 2017-12-07 22:59:22.911454: step 55980, loss = 21.49, batch loss = 21.41 (8.1 examples/sec; 0.985 sec/batch; 75h:39m:30s remains)
INFO - root - 2017-12-07 22:59:32.155728: step 55990, loss = 21.51, batch loss = 21.43 (8.1 examples/sec; 0.984 sec/batch; 75h:36m:33s remains)
INFO - root - 2017-12-07 22:59:41.589052: step 56000, loss = 21.22, batch loss = 21.13 (9.0 examples/sec; 0.889 sec/batch; 68h:17m:43s remains)
2017-12-07 22:59:42.566296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2229352 -4.2714338 -4.3361526 -4.4188309 -4.5071592 -4.5519753 -4.5447793 -4.5151162 -4.5000782 -4.5262194 -4.5578313 -4.5429916 -4.4823976 -4.4222441 -4.3949847][-4.2544556 -4.3355389 -4.4155064 -4.4887247 -4.543839 -4.5522337 -4.5218353 -4.49554 -4.5078082 -4.5623827 -4.6175585 -4.6210036 -4.5770822 -4.520617 -4.4600472][-4.298317 -4.4179993 -4.5153022 -4.5740566 -4.5754457 -4.5183325 -4.4443984 -4.4188623 -4.4717188 -4.569447 -4.6490335 -4.6689563 -4.6420794 -4.5967221 -4.508913][-4.333715 -4.4716377 -4.5705304 -4.6154284 -4.5760427 -4.4608955 -4.3404202 -4.3025112 -4.3861041 -4.5248442 -4.6285529 -4.6695871 -4.6648836 -4.6361604 -4.5387955][-4.3609 -4.4954944 -4.5766659 -4.6016684 -4.5351605 -4.3782797 -4.215076 -4.1482625 -4.237762 -4.402987 -4.5308061 -4.6053419 -4.6333776 -4.6272354 -4.53931][-4.39946 -4.5221276 -4.5706816 -4.5577803 -4.4556923 -4.2629471 -4.0683904 -3.9780347 -4.0667486 -4.2486124 -4.3952417 -4.500999 -4.5636525 -4.5833306 -4.518568][-4.4538121 -4.5611243 -4.5610585 -4.4850221 -4.330564 -4.110673 -3.9162464 -3.8385005 -3.9439721 -4.1414509 -4.2965651 -4.4171376 -4.5075521 -4.554966 -4.5224524][-4.501595 -4.5824909 -4.5229931 -4.3727703 -4.1670012 -3.9436243 -3.7897887 -3.7645071 -3.9036987 -4.1142626 -4.2718887 -4.3962379 -4.5025396 -4.5679178 -4.5571432][-4.5211997 -4.5666013 -4.4533238 -4.2448645 -4.0073075 -3.7977042 -3.6992815 -3.7433226 -3.9272683 -4.1526508 -4.3129086 -4.4334917 -4.5365477 -4.5970721 -4.5801759][-4.5260725 -4.5440059 -4.4026923 -4.16826 -3.9189785 -3.7190733 -3.6594667 -3.7644048 -3.9938326 -4.2322378 -4.3821292 -4.4782214 -4.5572147 -4.5977345 -4.5608158][-4.5319777 -4.553174 -4.4286065 -4.208045 -3.9636056 -3.7606719 -3.7091463 -3.8407867 -4.0878539 -4.3146405 -4.4308429 -4.48594 -4.5357776 -4.559494 -4.5116382][-4.5434861 -4.5957036 -4.5203037 -4.3408618 -4.1167479 -3.9127595 -3.8479319 -3.962774 -4.1829915 -4.369905 -4.4452925 -4.4652224 -4.4954214 -4.5124412 -4.4661975][-4.5448842 -4.6306782 -4.611793 -4.4851551 -4.2980957 -4.113709 -4.0409961 -4.1180654 -4.2814174 -4.413506 -4.4543138 -4.4551148 -4.4792295 -4.4988837 -4.467845][-4.5182085 -4.6216812 -4.6539955 -4.5926032 -4.4671216 -4.3338037 -4.2716942 -4.3063526 -4.3952017 -4.4627824 -4.4807677 -4.4882679 -4.5262618 -4.560051 -4.5519457][-4.4693718 -4.5678773 -4.6300435 -4.626193 -4.5693188 -4.4973378 -4.4534044 -4.4522924 -4.4767661 -4.4957776 -4.5079603 -4.5377355 -4.5980167 -4.64838 -4.66024]]...]
INFO - root - 2017-12-07 22:59:51.899511: step 56010, loss = 21.40, batch loss = 21.32 (8.2 examples/sec; 0.974 sec/batch; 74h:50m:17s remains)
INFO - root - 2017-12-07 23:00:01.183911: step 56020, loss = 21.13, batch loss = 21.04 (9.3 examples/sec; 0.863 sec/batch; 66h:17m:24s remains)
INFO - root - 2017-12-07 23:00:10.483698: step 56030, loss = 21.36, batch loss = 21.27 (9.0 examples/sec; 0.886 sec/batch; 68h:00m:37s remains)
INFO - root - 2017-12-07 23:00:19.974501: step 56040, loss = 21.74, batch loss = 21.66 (7.6 examples/sec; 1.054 sec/batch; 80h:56m:32s remains)
INFO - root - 2017-12-07 23:00:29.269612: step 56050, loss = 21.58, batch loss = 21.49 (8.2 examples/sec; 0.978 sec/batch; 75h:06m:02s remains)
INFO - root - 2017-12-07 23:00:38.712096: step 56060, loss = 21.59, batch loss = 21.51 (8.6 examples/sec; 0.936 sec/batch; 71h:50m:51s remains)
INFO - root - 2017-12-07 23:00:48.080989: step 56070, loss = 21.40, batch loss = 21.31 (8.4 examples/sec; 0.952 sec/batch; 73h:06m:04s remains)
INFO - root - 2017-12-07 23:00:57.573048: step 56080, loss = 21.40, batch loss = 21.32 (8.6 examples/sec; 0.926 sec/batch; 71h:03m:51s remains)
INFO - root - 2017-12-07 23:01:07.056421: step 56090, loss = 21.61, batch loss = 21.53 (8.0 examples/sec; 0.999 sec/batch; 76h:40m:43s remains)
INFO - root - 2017-12-07 23:01:16.334147: step 56100, loss = 21.52, batch loss = 21.44 (8.5 examples/sec; 0.946 sec/batch; 72h:35m:40s remains)
2017-12-07 23:01:17.333769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3089094 -4.3214469 -4.39211 -4.4853516 -4.51219 -4.4487157 -4.3638873 -4.3488245 -4.3690124 -4.3608603 -4.3463249 -4.3523784 -4.3732519 -4.37884 -4.3333297][-4.3965917 -4.4494619 -4.5297389 -4.6019473 -4.6086273 -4.5395265 -4.4443817 -4.4254084 -4.474144 -4.5108652 -4.5303688 -4.5468407 -4.5524664 -4.5343928 -4.4737606][-4.49688 -4.5617185 -4.6240115 -4.6532879 -4.622467 -4.5257277 -4.3996105 -4.3665648 -4.4504075 -4.5528216 -4.6286392 -4.67603 -4.6906877 -4.6664777 -4.5938306][-4.5526872 -4.6187267 -4.6561427 -4.6405993 -4.5627646 -4.4128723 -4.2262335 -4.1565094 -4.2661862 -4.4392791 -4.5854878 -4.6860461 -4.7362771 -4.7244234 -4.6444159][-4.5496316 -4.6160493 -4.6305666 -4.5713859 -4.442102 -4.2293668 -3.9713266 -3.8510704 -3.975636 -4.2213159 -4.4536829 -4.6271887 -4.7271895 -4.7305589 -4.6354661][-4.5150733 -4.5782075 -4.5749168 -4.4790521 -4.3046427 -4.0390539 -3.7157693 -3.5347848 -3.6464539 -3.9389496 -4.2498808 -4.5013714 -4.6568513 -4.6811576 -4.5755677][-4.4555078 -4.5148182 -4.5071983 -4.4021506 -4.2147937 -3.9342418 -3.5830369 -3.3543262 -3.4309492 -3.725807 -4.0702662 -4.3676252 -4.5596547 -4.6053696 -4.5054913][-4.4011922 -4.4653196 -4.4680986 -4.378582 -4.2086725 -3.9579422 -3.6427174 -3.4197397 -3.4687233 -3.7367265 -4.06987 -4.3645306 -4.54948 -4.5831804 -4.4742947][-4.3968863 -4.4633346 -4.4812489 -4.4234209 -4.2894421 -4.0896807 -3.8435962 -3.6602383 -3.6883404 -3.9128504 -4.2034721 -4.4577823 -4.6087923 -4.6144037 -4.4905443][-4.4345675 -4.4961147 -4.5249186 -4.5079513 -4.4295382 -4.2882018 -4.1114216 -3.9693091 -3.9699936 -4.1375303 -4.3705363 -4.5644217 -4.6728921 -4.6614375 -4.536829][-4.4850283 -4.5298219 -4.5611534 -4.5844026 -4.5708327 -4.496563 -4.38429 -4.2798986 -4.255496 -4.3680038 -4.5472922 -4.6821795 -4.7406812 -4.70725 -4.5824742][-4.5220361 -4.54527 -4.5692925 -4.6168947 -4.6547327 -4.6368518 -4.567625 -4.4764948 -4.4196687 -4.4789553 -4.6152325 -4.7148013 -4.7458925 -4.7021694 -4.5832586][-4.4987 -4.50432 -4.5149369 -4.5608478 -4.6130905 -4.6179357 -4.5657525 -4.4777412 -4.4033237 -4.4224677 -4.5237179 -4.6132603 -4.6479053 -4.6154246 -4.5166082][-4.4168468 -4.4061804 -4.4067092 -4.4440951 -4.4933343 -4.5059733 -4.4664693 -4.3901081 -4.3225756 -4.3278546 -4.403502 -4.48204 -4.5211005 -4.5028338 -4.4292288][-4.3216929 -4.3049455 -4.3030663 -4.3362637 -4.3814363 -4.3985009 -4.3739643 -4.3204656 -4.276124 -4.2823071 -4.3333139 -4.3911424 -4.4244485 -4.4152985 -4.3676696]]...]
INFO - root - 2017-12-07 23:01:26.652046: step 56110, loss = 21.19, batch loss = 21.11 (8.6 examples/sec; 0.926 sec/batch; 71h:05m:00s remains)
INFO - root - 2017-12-07 23:01:36.142374: step 56120, loss = 20.99, batch loss = 20.91 (8.3 examples/sec; 0.961 sec/batch; 73h:45m:21s remains)
INFO - root - 2017-12-07 23:01:45.565912: step 56130, loss = 21.06, batch loss = 20.97 (8.6 examples/sec; 0.931 sec/batch; 71h:28m:52s remains)
INFO - root - 2017-12-07 23:01:54.865218: step 56140, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.932 sec/batch; 71h:32m:57s remains)
INFO - root - 2017-12-07 23:02:04.203187: step 56150, loss = 21.70, batch loss = 21.62 (8.7 examples/sec; 0.919 sec/batch; 70h:32m:54s remains)
INFO - root - 2017-12-07 23:02:13.694154: step 56160, loss = 21.71, batch loss = 21.63 (8.8 examples/sec; 0.910 sec/batch; 69h:50m:35s remains)
INFO - root - 2017-12-07 23:02:23.134025: step 56170, loss = 21.64, batch loss = 21.56 (8.5 examples/sec; 0.940 sec/batch; 72h:09m:41s remains)
INFO - root - 2017-12-07 23:02:32.635359: step 56180, loss = 21.16, batch loss = 21.07 (8.6 examples/sec; 0.936 sec/batch; 71h:48m:35s remains)
INFO - root - 2017-12-07 23:02:41.978446: step 56190, loss = 21.66, batch loss = 21.58 (8.6 examples/sec; 0.935 sec/batch; 71h:44m:17s remains)
INFO - root - 2017-12-07 23:02:51.325831: step 56200, loss = 21.20, batch loss = 21.12 (8.0 examples/sec; 0.994 sec/batch; 76h:19m:26s remains)
2017-12-07 23:02:52.281995: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4929514 -4.5014882 -4.5078063 -4.5113277 -4.5113063 -4.5088248 -4.5061722 -4.5065627 -4.5095754 -4.5124769 -4.5127406 -4.5097494 -4.5048962 -4.5005679 -4.499505][-4.5063882 -4.518466 -4.5242982 -4.522449 -4.5171084 -4.5140977 -4.5162816 -4.5262032 -4.539463 -4.5489283 -4.5523105 -4.549334 -4.5412788 -4.5327592 -4.528296][-4.5022879 -4.5163178 -4.5206017 -4.5103526 -4.4961429 -4.4890966 -4.4933996 -4.5109677 -4.532125 -4.5473633 -4.5567164 -4.5575428 -4.551836 -4.5421743 -4.5315671][-4.4575281 -4.4696217 -4.4762425 -4.4612584 -4.4378924 -4.4192843 -4.4128594 -4.427321 -4.444169 -4.4547229 -4.4634104 -4.4647613 -4.4646807 -4.4581628 -4.4424634][-4.3735733 -4.38074 -4.393105 -4.3780003 -4.3444533 -4.3034596 -4.2757888 -4.283165 -4.2949662 -4.3021908 -4.3095455 -4.3108211 -4.3194189 -4.3153491 -4.2935867][-4.29115 -4.2941213 -4.3082075 -4.28825 -4.23531 -4.1610246 -4.1141081 -4.1276822 -4.153502 -4.1756635 -4.190578 -4.1966963 -4.2144608 -4.2078781 -4.1766744][-4.2573543 -4.2625885 -4.2732692 -4.2390289 -4.1569424 -4.04324 -3.9823372 -4.0106721 -4.059535 -4.0987549 -4.1153688 -4.1228375 -4.14484 -4.1370382 -4.1036134][-4.2886558 -4.2977939 -4.2993016 -4.250134 -4.1454124 -4.0033855 -3.9329665 -3.9640062 -4.0199738 -4.0584149 -4.0659738 -4.0718794 -4.0956855 -4.0990043 -4.0791521][-4.3394747 -4.3454251 -4.3414106 -4.2971625 -4.20087 -4.0637059 -3.9914682 -4.0148215 -4.0671215 -4.0971637 -4.0974174 -4.1032553 -4.1268935 -4.1450987 -4.1428413][-4.34143 -4.3374634 -4.3372812 -4.3176308 -4.2611127 -4.1642952 -4.1113505 -4.1350479 -4.1821241 -4.2077341 -4.2096004 -4.213367 -4.2271461 -4.2432208 -4.2439041][-4.3050756 -4.2984767 -4.3110251 -4.3198118 -4.3094568 -4.2615743 -4.2347727 -4.263833 -4.3017755 -4.3223343 -4.3234177 -4.3121057 -4.3076434 -4.308774 -4.3043795][-4.2722316 -4.273272 -4.29656 -4.3160591 -4.3303361 -4.310585 -4.3029709 -4.3335276 -4.3598108 -4.3750362 -4.3718219 -4.335381 -4.3108869 -4.2947583 -4.2850256][-4.2549257 -4.2599111 -4.2820048 -4.2933545 -4.3018675 -4.2809253 -4.2748094 -4.3000703 -4.31331 -4.325386 -4.3194141 -4.2663012 -4.2364068 -4.2147365 -4.2006569][-4.2492304 -4.2525797 -4.2665339 -4.2667861 -4.2605796 -4.2270575 -4.214592 -4.2307763 -4.2325597 -4.2449169 -4.2375612 -4.1842747 -4.1610427 -4.1356783 -4.1115117][-4.243751 -4.2467914 -4.2609892 -4.2663689 -4.2597184 -4.222106 -4.2061 -4.2161679 -4.2112923 -4.2177052 -4.2021246 -4.1538687 -4.1384368 -4.1119766 -4.08007]]...]
INFO - root - 2017-12-07 23:03:01.699409: step 56210, loss = 21.26, batch loss = 21.18 (8.3 examples/sec; 0.961 sec/batch; 73h:44m:13s remains)
INFO - root - 2017-12-07 23:03:11.039869: step 56220, loss = 21.63, batch loss = 21.55 (8.6 examples/sec; 0.926 sec/batch; 71h:03m:13s remains)
INFO - root - 2017-12-07 23:03:20.521252: step 56230, loss = 21.26, batch loss = 21.18 (7.8 examples/sec; 1.021 sec/batch; 78h:21m:48s remains)
INFO - root - 2017-12-07 23:03:29.892714: step 56240, loss = 21.26, batch loss = 21.17 (8.5 examples/sec; 0.941 sec/batch; 72h:11m:17s remains)
INFO - root - 2017-12-07 23:03:39.319991: step 56250, loss = 21.55, batch loss = 21.46 (8.3 examples/sec; 0.966 sec/batch; 74h:08m:57s remains)
INFO - root - 2017-12-07 23:03:48.764619: step 56260, loss = 21.73, batch loss = 21.64 (8.6 examples/sec; 0.927 sec/batch; 71h:07m:27s remains)
INFO - root - 2017-12-07 23:03:58.089509: step 56270, loss = 21.46, batch loss = 21.38 (8.4 examples/sec; 0.948 sec/batch; 72h:43m:09s remains)
INFO - root - 2017-12-07 23:04:07.489754: step 56280, loss = 21.39, batch loss = 21.30 (8.3 examples/sec; 0.964 sec/batch; 73h:56m:58s remains)
INFO - root - 2017-12-07 23:04:16.846557: step 56290, loss = 21.49, batch loss = 21.40 (8.4 examples/sec; 0.955 sec/batch; 73h:18m:28s remains)
INFO - root - 2017-12-07 23:04:26.232224: step 56300, loss = 21.38, batch loss = 21.30 (8.5 examples/sec; 0.944 sec/batch; 72h:25m:21s remains)
2017-12-07 23:04:27.170626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4145422 -4.4859924 -4.5577941 -4.616055 -4.6602287 -4.6683474 -4.652236 -4.6367884 -4.6145864 -4.5917006 -4.5848403 -4.5892634 -4.5996823 -4.6096277 -4.6224937][-4.4364738 -4.5073452 -4.5778341 -4.6366429 -4.678597 -4.6760612 -4.6495905 -4.6307664 -4.6111536 -4.585865 -4.5708423 -4.5741177 -4.5874629 -4.6022124 -4.6289239][-4.5094733 -4.5859361 -4.6499491 -4.6783347 -4.6702142 -4.616569 -4.5615439 -4.54565 -4.5605974 -4.5779843 -4.5916085 -4.6115727 -4.6254225 -4.6316786 -4.6509953][-4.5995669 -4.6878629 -4.7391663 -4.7100072 -4.6122432 -4.4764471 -4.3738046 -4.3627844 -4.4357457 -4.5248237 -4.5969925 -4.6510458 -4.6708274 -4.6624479 -4.6617975][-4.6734295 -4.7770205 -4.8091946 -4.7073236 -4.5037384 -4.2706594 -4.1085544 -4.1066122 -4.250083 -4.4228296 -4.5678263 -4.6673746 -4.6963787 -4.6714292 -4.6479592][-4.7123785 -4.817802 -4.813971 -4.6337256 -4.3330479 -4.0145111 -3.80263 -3.8241546 -4.0457315 -4.3028345 -4.520617 -4.6658192 -4.7068791 -4.6775589 -4.6407852][-4.6861463 -4.7701025 -4.7203274 -4.4855332 -4.1350307 -3.7804859 -3.5549941 -3.615706 -3.9075117 -4.2319927 -4.497695 -4.6611404 -4.70182 -4.68028 -4.6470985][-4.6311502 -4.6918635 -4.6145411 -4.3676038 -4.0214739 -3.677989 -3.461519 -3.5487204 -3.8726554 -4.2265868 -4.5096059 -4.6648822 -4.694417 -4.6843076 -4.6632724][-4.57417 -4.6231208 -4.5460215 -4.32105 -4.0107203 -3.697082 -3.497282 -3.5886741 -3.9068072 -4.2541752 -4.5283532 -4.663835 -4.6821828 -4.6849713 -4.6797814][-4.5335841 -4.5796127 -4.5161977 -4.3210568 -4.0480089 -3.7669928 -3.5951076 -3.6912036 -3.9913912 -4.3091068 -4.5511346 -4.6641417 -4.6806879 -4.6975455 -4.7054954][-4.5516315 -4.5929632 -4.5407763 -4.3670311 -4.12374 -3.8807368 -3.7516975 -3.8622019 -4.1342359 -4.3972349 -4.58405 -4.6679077 -4.6865454 -4.7131343 -4.7278557][-4.62378 -4.6516762 -4.5981646 -4.438201 -4.2283869 -4.03769 -3.9652979 -4.0926113 -4.324934 -4.5155129 -4.6282024 -4.6729126 -4.684948 -4.7088528 -4.7225614][-4.7104549 -4.7285862 -4.668612 -4.5265131 -4.3623438 -4.2319431 -4.2107244 -4.3386126 -4.5147514 -4.6255293 -4.6686993 -4.6796236 -4.6842608 -4.702949 -4.7155528][-4.7709241 -4.7952738 -4.7392073 -4.6286182 -4.5199761 -4.4419608 -4.4449954 -4.5416508 -4.6452217 -4.6849685 -4.6847968 -4.6828475 -4.6869245 -4.7013206 -4.7085485][-4.7550488 -4.8017926 -4.7656808 -4.6945758 -4.6387644 -4.6008444 -4.6095557 -4.6616468 -4.6987934 -4.6953506 -4.6857634 -4.6894727 -4.6911826 -4.6930251 -4.6847491]]...]
INFO - root - 2017-12-07 23:04:36.681656: step 56310, loss = 21.40, batch loss = 21.32 (8.0 examples/sec; 1.005 sec/batch; 77h:06m:56s remains)
INFO - root - 2017-12-07 23:04:46.039503: step 56320, loss = 21.16, batch loss = 21.08 (8.5 examples/sec; 0.941 sec/batch; 72h:10m:23s remains)
INFO - root - 2017-12-07 23:04:55.404163: step 56330, loss = 21.63, batch loss = 21.55 (8.4 examples/sec; 0.949 sec/batch; 72h:49m:27s remains)
INFO - root - 2017-12-07 23:05:04.830962: step 56340, loss = 21.52, batch loss = 21.44 (8.9 examples/sec; 0.900 sec/batch; 69h:03m:39s remains)
INFO - root - 2017-12-07 23:05:14.216264: step 56350, loss = 21.45, batch loss = 21.37 (8.6 examples/sec; 0.931 sec/batch; 71h:22m:41s remains)
INFO - root - 2017-12-07 23:05:23.758276: step 56360, loss = 21.14, batch loss = 21.05 (8.4 examples/sec; 0.948 sec/batch; 72h:43m:23s remains)
INFO - root - 2017-12-07 23:05:33.126979: step 56370, loss = 21.25, batch loss = 21.17 (7.9 examples/sec; 1.016 sec/batch; 77h:57m:36s remains)
INFO - root - 2017-12-07 23:05:42.536177: step 56380, loss = 21.61, batch loss = 21.53 (8.1 examples/sec; 0.986 sec/batch; 75h:35m:37s remains)
INFO - root - 2017-12-07 23:05:51.964183: step 56390, loss = 21.26, batch loss = 21.18 (8.4 examples/sec; 0.948 sec/batch; 72h:41m:48s remains)
INFO - root - 2017-12-07 23:06:01.301027: step 56400, loss = 21.54, batch loss = 21.46 (8.7 examples/sec; 0.925 sec/batch; 70h:54m:40s remains)
2017-12-07 23:06:02.217562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3693247 -4.4077482 -4.4645371 -4.522573 -4.5623922 -4.5686264 -4.55556 -4.5436797 -4.5378432 -4.5299368 -4.5059667 -4.47301 -4.4304767 -4.3871584 -4.3626261][-4.38571 -4.428771 -4.4951091 -4.5572629 -4.5925331 -4.58535 -4.5546551 -4.5325174 -4.5275521 -4.5349393 -4.535511 -4.5227571 -4.4904604 -4.4379611 -4.3922181][-4.4099245 -4.4568281 -4.5257387 -4.5773058 -4.5900745 -4.5556436 -4.4954362 -4.4559517 -4.4496303 -4.4735489 -4.5092287 -4.5281239 -4.5192413 -4.4722276 -4.410171][-4.4314713 -4.4818211 -4.544383 -4.5713849 -4.5456915 -4.4687457 -4.3676419 -4.3100877 -4.3167481 -4.3719373 -4.45629 -4.5172267 -4.5356841 -4.5007849 -4.4315233][-4.4350514 -4.4857445 -4.5314136 -4.525609 -4.4532809 -4.3259239 -4.1773782 -4.1022897 -4.1353016 -4.237422 -4.3792195 -4.491075 -4.5381188 -4.5196095 -4.4591255][-4.4367585 -4.4875069 -4.513773 -4.4756384 -4.3570333 -4.1790662 -3.982461 -3.884249 -3.9363461 -4.0772057 -4.2593551 -4.4158506 -4.4983659 -4.5094528 -4.4763479][-4.450531 -4.5033126 -4.5132847 -4.4494143 -4.2951641 -4.0791993 -3.8476355 -3.7294409 -3.7941141 -3.9614878 -4.1566916 -4.3293858 -4.43469 -4.473073 -4.4673696][-4.4785395 -4.534574 -4.5372996 -4.4662838 -4.3077016 -4.0861254 -3.8485153 -3.7206011 -3.7840586 -3.9548881 -4.1358638 -4.2900925 -4.3892713 -4.4354 -4.4421754][-4.4982634 -4.5536876 -4.5588279 -4.5092154 -4.3913913 -4.2155042 -4.0174689 -3.903425 -3.9518964 -4.0899253 -4.2256579 -4.3371563 -4.4104018 -4.4468374 -4.4523058][-4.4941192 -4.5435119 -4.5521317 -4.5366979 -4.4821963 -4.3775043 -4.2462406 -4.1623411 -4.1872296 -4.2682071 -4.3469057 -4.4146819 -4.4600239 -4.4820447 -4.4843478][-4.4682541 -4.5075011 -4.520155 -4.534236 -4.5327067 -4.4857321 -4.4117117 -4.3605676 -4.3702297 -4.4055767 -4.4418178 -4.4767838 -4.4965639 -4.5018644 -4.500237][-4.4235582 -4.451932 -4.4664245 -4.4933505 -4.5126848 -4.4842277 -4.430892 -4.4020548 -4.4132829 -4.4349384 -4.459516 -4.4886789 -4.5081854 -4.5084825 -4.4976387][-4.3985925 -4.4134965 -4.4157896 -4.4240346 -4.4278069 -4.386075 -4.31629 -4.2790666 -4.287251 -4.3102155 -4.3457675 -4.3984694 -4.4524961 -4.4803734 -4.4714718][-4.39706 -4.406333 -4.3877845 -4.356874 -4.3274775 -4.2720981 -4.1917944 -4.1396523 -4.1379766 -4.1620946 -4.2104716 -4.2871604 -4.3770518 -4.43865 -4.4352021][-4.4002953 -4.4132414 -4.3790264 -4.3102484 -4.2475471 -4.1854711 -4.1098604 -4.0505295 -4.0440636 -4.0777 -4.1353416 -4.2180338 -4.3190603 -4.4018049 -4.4115]]...]
INFO - root - 2017-12-07 23:06:11.635420: step 56410, loss = 21.59, batch loss = 21.51 (8.7 examples/sec; 0.920 sec/batch; 70h:34m:01s remains)
INFO - root - 2017-12-07 23:06:21.068813: step 56420, loss = 21.51, batch loss = 21.43 (8.4 examples/sec; 0.952 sec/batch; 72h:58m:46s remains)
INFO - root - 2017-12-07 23:06:30.426560: step 56430, loss = 21.75, batch loss = 21.67 (8.5 examples/sec; 0.940 sec/batch; 72h:06m:39s remains)
INFO - root - 2017-12-07 23:06:39.575562: step 56440, loss = 21.91, batch loss = 21.83 (8.4 examples/sec; 0.953 sec/batch; 73h:04m:07s remains)
INFO - root - 2017-12-07 23:06:48.954695: step 56450, loss = 21.31, batch loss = 21.23 (8.5 examples/sec; 0.937 sec/batch; 71h:52m:07s remains)
INFO - root - 2017-12-07 23:06:58.397861: step 56460, loss = 21.25, batch loss = 21.16 (8.6 examples/sec; 0.932 sec/batch; 71h:26m:08s remains)
INFO - root - 2017-12-07 23:07:07.790617: step 56470, loss = 21.47, batch loss = 21.39 (8.4 examples/sec; 0.952 sec/batch; 72h:59m:25s remains)
INFO - root - 2017-12-07 23:07:17.308728: step 56480, loss = 21.38, batch loss = 21.30 (8.7 examples/sec; 0.916 sec/batch; 70h:14m:37s remains)
INFO - root - 2017-12-07 23:07:26.780375: step 56490, loss = 21.28, batch loss = 21.20 (8.9 examples/sec; 0.902 sec/batch; 69h:07m:32s remains)
INFO - root - 2017-12-07 23:07:36.005069: step 56500, loss = 21.60, batch loss = 21.51 (9.8 examples/sec; 0.816 sec/batch; 62h:32m:45s remains)
2017-12-07 23:07:37.070035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4029164 -4.3718948 -4.3863039 -4.4331503 -4.4758568 -4.4919777 -4.4966631 -4.5005727 -4.5060396 -4.5057721 -4.4782891 -4.4244347 -4.3607135 -4.3055105 -4.268455][-4.3326545 -4.3021164 -4.3304305 -4.3925486 -4.4374576 -4.4401817 -4.4262919 -4.4159527 -4.415895 -4.4139423 -4.386467 -4.3365049 -4.2804146 -4.2381091 -4.215065][-4.2887082 -4.2694778 -4.3074727 -4.3667464 -4.393352 -4.3694921 -4.3320251 -4.3118725 -4.3176 -4.3238006 -4.3030682 -4.2626028 -4.221539 -4.1999235 -4.1965828][-4.26769 -4.26003 -4.2931066 -4.3259768 -4.3127809 -4.2496028 -4.1876755 -4.1692467 -4.2021708 -4.2392073 -4.2422838 -4.2195921 -4.1979418 -4.2000175 -4.2177567][-4.2617521 -4.2532659 -4.2676225 -4.2629919 -4.2045836 -4.1012049 -4.0176859 -4.0090733 -4.0841184 -4.1722054 -4.2160559 -4.2176037 -4.2109342 -4.2254381 -4.2534342][-4.262537 -4.2396526 -4.2304883 -4.1944342 -4.1022763 -3.9668865 -3.8658526 -3.8702097 -3.9882812 -4.133008 -4.2230115 -4.2472095 -4.2456932 -4.2582536 -4.2844028][-4.2650394 -4.227047 -4.2022624 -4.1511188 -4.0392709 -3.8776371 -3.7543659 -3.7567368 -3.8991082 -4.0860152 -4.2159691 -4.2631545 -4.2677636 -4.2757516 -4.296762][-4.2843652 -4.2407241 -4.2133169 -4.1637611 -4.0508547 -3.8767464 -3.7301478 -3.7106583 -3.8414333 -4.03416 -4.1813068 -4.2443829 -4.257987 -4.2660151 -4.2835107][-4.3116536 -4.2792397 -4.2653823 -4.23598 -4.1462231 -3.986551 -3.8328178 -3.782697 -3.8684702 -4.0230122 -4.1530967 -4.2159829 -4.2357068 -4.2461042 -4.2610674][-4.3161154 -4.3057203 -4.3167553 -4.3193207 -4.271318 -4.1516972 -4.015419 -3.944629 -3.9744506 -4.0640731 -4.15106 -4.1981854 -4.2184067 -4.2316027 -4.2446442][-4.2571483 -4.270606 -4.3098288 -4.3481603 -4.3482571 -4.281455 -4.1838474 -4.1125674 -4.0977254 -4.1256428 -4.166358 -4.194613 -4.2133732 -4.2284255 -4.2391953][-4.1424117 -4.1610184 -4.2181044 -4.28373 -4.323782 -4.3076444 -4.2599316 -4.2121825 -4.182641 -4.1748428 -4.1862087 -4.2042794 -4.2236485 -4.2396464 -4.2474556][-4.0282097 -4.0367422 -4.0951281 -4.1709938 -4.2301435 -4.2441988 -4.2378955 -4.2252541 -4.2101378 -4.1996613 -4.2039518 -4.2208843 -4.24267 -4.2587824 -4.2627487][-3.9751821 -3.9693441 -4.0192003 -4.0921607 -4.151813 -4.1736321 -4.1860847 -4.1979461 -4.2056022 -4.2104726 -4.2222018 -4.2420011 -4.2632842 -4.276361 -4.2764006][-4.0210729 -4.0064564 -4.0434618 -4.1019211 -4.1484222 -4.165287 -4.1802249 -4.2004795 -4.2211804 -4.2390852 -4.2589011 -4.2812181 -4.3010983 -4.3122158 -4.3121715]]...]
INFO - root - 2017-12-07 23:07:46.265274: step 56510, loss = 21.21, batch loss = 21.13 (8.6 examples/sec; 0.929 sec/batch; 71h:11m:37s remains)
INFO - root - 2017-12-07 23:07:55.667257: step 56520, loss = 21.60, batch loss = 21.52 (8.8 examples/sec; 0.907 sec/batch; 69h:34m:05s remains)
INFO - root - 2017-12-07 23:08:05.229142: step 56530, loss = 21.50, batch loss = 21.41 (8.5 examples/sec; 0.937 sec/batch; 71h:49m:11s remains)
INFO - root - 2017-12-07 23:08:14.658669: step 56540, loss = 21.70, batch loss = 21.62 (8.8 examples/sec; 0.907 sec/batch; 69h:32m:39s remains)
INFO - root - 2017-12-07 23:08:24.113393: step 56550, loss = 21.29, batch loss = 21.21 (8.4 examples/sec; 0.952 sec/batch; 72h:56m:55s remains)
INFO - root - 2017-12-07 23:08:33.687144: step 56560, loss = 21.36, batch loss = 21.27 (8.1 examples/sec; 0.986 sec/batch; 75h:36m:15s remains)
INFO - root - 2017-12-07 23:08:43.155957: step 56570, loss = 21.56, batch loss = 21.48 (8.3 examples/sec; 0.963 sec/batch; 73h:46m:57s remains)
INFO - root - 2017-12-07 23:08:52.570396: step 56580, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.947 sec/batch; 72h:34m:50s remains)
INFO - root - 2017-12-07 23:09:01.863237: step 56590, loss = 21.37, batch loss = 21.29 (8.3 examples/sec; 0.958 sec/batch; 73h:26m:07s remains)
INFO - root - 2017-12-07 23:09:11.291181: step 56600, loss = 21.97, batch loss = 21.88 (8.7 examples/sec; 0.917 sec/batch; 70h:17m:51s remains)
2017-12-07 23:09:12.195174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4494219 -4.4785604 -4.4957633 -4.49946 -4.4980164 -4.4993453 -4.5097466 -4.5245647 -4.5257297 -4.5022016 -4.4769082 -4.4792933 -4.5084567 -4.5474787 -4.5682278][-4.4947133 -4.5407248 -4.5710998 -4.57983 -4.5737529 -4.5639138 -4.566237 -4.5880885 -4.6038685 -4.5884881 -4.5697737 -4.5811548 -4.6129718 -4.643672 -4.646986][-4.5505381 -4.6107664 -4.6428771 -4.640141 -4.6126885 -4.57251 -4.5477347 -4.562705 -4.5903244 -4.5932918 -4.5964537 -4.630816 -4.6762104 -4.7090654 -4.7080436][-4.6186056 -4.678196 -4.6891012 -4.6521482 -4.5890255 -4.5132465 -4.4557786 -4.45461 -4.4868541 -4.5064888 -4.5345349 -4.5978322 -4.6678224 -4.7183213 -4.7303252][-4.6752911 -4.7051468 -4.6627903 -4.5699048 -4.4655404 -4.3632216 -4.2876387 -4.2764444 -4.3114471 -4.3428273 -4.3880911 -4.4751987 -4.5763116 -4.6592021 -4.7018943][-4.6841474 -4.6561627 -4.53963 -4.3884182 -4.2527862 -4.1427274 -4.0693512 -4.0614386 -4.0993423 -4.1373558 -4.1870904 -4.2848024 -4.4166145 -4.5413604 -4.6261835][-4.6412177 -4.5534363 -4.3678851 -4.1664553 -4.0019174 -3.8867614 -3.8239646 -3.8351507 -3.8912761 -3.9434214 -3.9969354 -4.0940995 -4.2436767 -4.4011922 -4.52132][-4.5953994 -4.4850526 -4.2622261 -4.0192 -3.8165207 -3.6792724 -3.6117415 -3.6381114 -3.7220149 -3.7975893 -3.8641355 -3.9620478 -4.1169033 -4.2924023 -4.4344225][-4.5995078 -4.5332313 -4.3350239 -4.0785365 -3.8343265 -3.6555839 -3.5676694 -3.5957465 -3.700973 -3.7955508 -3.8738997 -3.9756413 -4.1300468 -4.3068027 -4.446125][-4.6383343 -4.6579027 -4.5311875 -4.296864 -4.0318604 -3.8232679 -3.7278543 -3.763726 -3.88024 -3.9733448 -4.0454855 -4.1474376 -4.298039 -4.4603171 -4.5696516][-4.66435 -4.7578707 -4.7076049 -4.5186543 -4.2606769 -4.0444264 -3.9542761 -4.0010834 -4.1185203 -4.1886287 -4.223937 -4.3026838 -4.440434 -4.5905476 -4.6803069][-4.6621332 -4.7994094 -4.821146 -4.7046061 -4.494401 -4.2996216 -4.2187982 -4.265358 -4.3664069 -4.4020982 -4.3885942 -4.4264979 -4.5371771 -4.6671948 -4.73886][-4.6394777 -4.794673 -4.8744588 -4.8378291 -4.6991024 -4.5435781 -4.4727306 -4.5094709 -4.5884223 -4.6030588 -4.5631738 -4.5679488 -4.6413813 -4.7314267 -4.767272][-4.5891786 -4.72847 -4.8253393 -4.8378177 -4.7634225 -4.662159 -4.6152487 -4.64628 -4.7041044 -4.7079492 -4.6636467 -4.6507521 -4.6925855 -4.7457027 -4.7501016][-4.5132833 -4.6010537 -4.6680369 -4.6881418 -4.6580181 -4.6124978 -4.597435 -4.6243596 -4.6583643 -4.6529617 -4.6186938 -4.6092815 -4.6385446 -4.6744719 -4.6704888]]...]
INFO - root - 2017-12-07 23:09:21.317759: step 56610, loss = 21.08, batch loss = 21.00 (10.2 examples/sec; 0.781 sec/batch; 59h:51m:52s remains)
INFO - root - 2017-12-07 23:09:30.698933: step 56620, loss = 21.27, batch loss = 21.19 (9.1 examples/sec; 0.879 sec/batch; 67h:20m:15s remains)
INFO - root - 2017-12-07 23:09:40.126535: step 56630, loss = 21.21, batch loss = 21.13 (8.3 examples/sec; 0.964 sec/batch; 73h:51m:16s remains)
INFO - root - 2017-12-07 23:09:49.505622: step 56640, loss = 21.78, batch loss = 21.69 (8.3 examples/sec; 0.961 sec/batch; 73h:37m:03s remains)
INFO - root - 2017-12-07 23:09:58.964002: step 56650, loss = 21.70, batch loss = 21.62 (8.5 examples/sec; 0.938 sec/batch; 71h:50m:09s remains)
INFO - root - 2017-12-07 23:10:08.363860: step 56660, loss = 21.38, batch loss = 21.29 (8.5 examples/sec; 0.943 sec/batch; 72h:13m:52s remains)
INFO - root - 2017-12-07 23:10:17.747634: step 56670, loss = 21.87, batch loss = 21.79 (8.7 examples/sec; 0.918 sec/batch; 70h:18m:29s remains)
INFO - root - 2017-12-07 23:10:27.053972: step 56680, loss = 21.70, batch loss = 21.61 (9.2 examples/sec; 0.867 sec/batch; 66h:26m:34s remains)
INFO - root - 2017-12-07 23:10:36.503419: step 56690, loss = 21.62, batch loss = 21.54 (8.7 examples/sec; 0.917 sec/batch; 70h:13m:57s remains)
INFO - root - 2017-12-07 23:10:45.783391: step 56700, loss = 21.36, batch loss = 21.27 (8.9 examples/sec; 0.899 sec/batch; 68h:52m:33s remains)
2017-12-07 23:10:46.726107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5974317 -4.5478997 -4.535388 -4.5988855 -4.6890035 -4.7008786 -4.6194968 -4.5324044 -4.5066881 -4.561717 -4.6516309 -4.7153206 -4.7506661 -4.773922 -4.76108][-4.6244769 -4.5756283 -4.5755491 -4.6619735 -4.76379 -4.7719965 -4.68063 -4.5797544 -4.5459118 -4.6066456 -4.7052479 -4.7611976 -4.7773156 -4.7838635 -4.7735429][-4.6740341 -4.6305456 -4.643908 -4.7398524 -4.824501 -4.8027782 -4.69608 -4.5948682 -4.5663896 -4.6290936 -4.7224808 -4.7636142 -4.7499437 -4.7156568 -4.6811156][-4.7028704 -4.6726766 -4.6931281 -4.7780209 -4.8259025 -4.7578497 -4.6305528 -4.5423856 -4.5395665 -4.6114473 -4.69712 -4.7252336 -4.67755 -4.58398 -4.50221][-4.6108894 -4.60745 -4.6298156 -4.6853747 -4.6797309 -4.5492029 -4.3851047 -4.3133826 -4.3618526 -4.4768953 -4.58545 -4.6251917 -4.5651627 -4.4335151 -4.3268008][-4.4244418 -4.4763131 -4.5034261 -4.519722 -4.4472418 -4.2374249 -4.0156131 -3.9542117 -4.062561 -4.2321539 -4.3797469 -4.4423728 -4.38497 -4.2552671 -4.17466][-4.2511363 -4.3652258 -4.4034147 -4.3866897 -4.2518954 -3.9631433 -3.6752939 -3.6106381 -3.7634008 -3.9699237 -4.1479697 -4.2293968 -4.1810126 -4.0830636 -4.06304][-4.1527719 -4.3014097 -4.3491797 -4.3292727 -4.1774654 -3.8586206 -3.5329161 -3.4472961 -3.60117 -3.8048716 -3.9812007 -4.0644593 -4.0222559 -3.9614265 -4.007443][-4.1654921 -4.3058472 -4.3553939 -4.3608561 -4.255456 -3.9896865 -3.6949148 -3.58639 -3.681742 -3.8256516 -3.9540865 -4.0114617 -3.9679766 -3.9308107 -4.015305][-4.3013325 -4.3944616 -4.428041 -4.45595 -4.4142971 -4.2475934 -4.04133 -3.9361656 -3.9586785 -4.0236936 -4.0836563 -4.0975595 -4.0476294 -4.0208435 -4.1051288][-4.4772081 -4.5220027 -4.5371766 -4.5703592 -4.5676 -4.4876761 -4.376842 -4.2981415 -4.2727523 -4.2784104 -4.2840023 -4.2628303 -4.2152586 -4.1984038 -4.2639036][-4.5953813 -4.6089368 -4.6100664 -4.6345778 -4.6434469 -4.6140704 -4.5682054 -4.519403 -4.4780774 -4.4549718 -4.4309006 -4.3934231 -4.3618078 -4.3658886 -4.4188814][-4.5875039 -4.5784197 -4.5633454 -4.5709152 -4.577734 -4.5737615 -4.5626345 -4.5384479 -4.5073547 -4.4826384 -4.4494338 -4.4120884 -4.4028721 -4.4408493 -4.5056725][-4.4536114 -4.43668 -4.4130154 -4.4079013 -4.4142032 -4.4256959 -4.4329367 -4.424974 -4.4062867 -4.3829 -4.3458915 -4.3179398 -4.3362422 -4.4125152 -4.5018663][-4.2918468 -4.2833614 -4.2639337 -4.2572451 -4.2692494 -4.2954555 -4.3169155 -4.320014 -4.3059473 -4.2772031 -4.23884 -4.2263818 -4.2686763 -4.3644361 -4.4580021]]...]
INFO - root - 2017-12-07 23:10:56.025468: step 56710, loss = 21.38, batch loss = 21.30 (8.6 examples/sec; 0.929 sec/batch; 71h:08m:50s remains)
INFO - root - 2017-12-07 23:11:05.333907: step 56720, loss = 21.63, batch loss = 21.54 (8.1 examples/sec; 0.987 sec/batch; 75h:34m:32s remains)
INFO - root - 2017-12-07 23:11:14.771960: step 56730, loss = 21.30, batch loss = 21.22 (8.9 examples/sec; 0.894 sec/batch; 68h:28m:56s remains)
INFO - root - 2017-12-07 23:11:24.200607: step 56740, loss = 21.39, batch loss = 21.31 (8.2 examples/sec; 0.975 sec/batch; 74h:39m:00s remains)
INFO - root - 2017-12-07 23:11:33.587681: step 56750, loss = 21.71, batch loss = 21.62 (8.2 examples/sec; 0.978 sec/batch; 74h:55m:46s remains)
INFO - root - 2017-12-07 23:11:43.036200: step 56760, loss = 21.12, batch loss = 21.04 (8.7 examples/sec; 0.925 sec/batch; 70h:49m:52s remains)
INFO - root - 2017-12-07 23:11:52.540235: step 56770, loss = 21.42, batch loss = 21.34 (8.8 examples/sec; 0.912 sec/batch; 69h:49m:39s remains)
INFO - root - 2017-12-07 23:12:02.028169: step 56780, loss = 21.30, batch loss = 21.22 (8.8 examples/sec; 0.910 sec/batch; 69h:43m:28s remains)
INFO - root - 2017-12-07 23:12:11.445860: step 56790, loss = 21.07, batch loss = 20.99 (8.1 examples/sec; 0.987 sec/batch; 75h:33m:41s remains)
INFO - root - 2017-12-07 23:12:20.858737: step 56800, loss = 21.35, batch loss = 21.26 (8.2 examples/sec; 0.970 sec/batch; 74h:16m:17s remains)
2017-12-07 23:12:21.838611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6289721 -4.6409807 -4.6114507 -4.5686913 -4.5237045 -4.4791775 -4.4813604 -4.5218453 -4.5978537 -4.6568279 -4.6508284 -4.6059012 -4.5519738 -4.53196 -4.542367][-4.6225924 -4.622191 -4.58138 -4.5230989 -4.4737158 -4.4304562 -4.440908 -4.4913235 -4.5855069 -4.6645975 -4.6668038 -4.6201992 -4.5626197 -4.5493064 -4.5723152][-4.5604792 -4.5495977 -4.5118027 -4.451077 -4.4027128 -4.3646293 -4.3773093 -4.4356861 -4.541914 -4.6400161 -4.65725 -4.6173868 -4.5613165 -4.5536718 -4.5855141][-4.4873743 -4.4580064 -4.4277782 -4.3736815 -4.3305397 -4.3015962 -4.3154325 -4.376019 -4.4763303 -4.5741048 -4.6041489 -4.5745115 -4.5248146 -4.528337 -4.5768137][-4.4197359 -4.3697786 -4.3439727 -4.3009462 -4.2615085 -4.2394805 -4.2482419 -4.299345 -4.3771334 -4.4506407 -4.4822083 -4.4606895 -4.4213433 -4.447619 -4.5315127][-4.3769464 -4.3110733 -4.2894526 -4.2555938 -4.2110553 -4.1765947 -4.1629925 -4.1933861 -4.2488074 -4.3000703 -4.3313036 -4.3219934 -4.296277 -4.3414321 -4.4595647][-4.3793416 -4.3011146 -4.2785287 -4.2436342 -4.1871724 -4.1288853 -4.0808911 -4.0816712 -4.1219568 -4.1655936 -4.2016511 -4.2085209 -4.1974592 -4.2428217 -4.3724046][-4.416625 -4.333766 -4.3121352 -4.2760611 -4.2163534 -4.1481919 -4.0718684 -4.0380697 -4.0610304 -4.0989347 -4.1309376 -4.1447253 -4.1489148 -4.1876945 -4.3049188][-4.44346 -4.363687 -4.3456392 -4.3166623 -4.2774091 -4.2281651 -4.1494756 -4.0948358 -4.0996561 -4.129796 -4.1526341 -4.16547 -4.1844692 -4.2206926 -4.3115559][-4.4595966 -4.3961892 -4.3819718 -4.3616204 -4.3576126 -4.35115 -4.2953038 -4.23556 -4.2146688 -4.2244973 -4.2334471 -4.2418137 -4.2809224 -4.3324666 -4.4036813][-4.4612536 -4.4260654 -4.4181924 -4.4043026 -4.4317904 -4.46984 -4.4462357 -4.3868523 -4.3384523 -4.3242149 -4.32602 -4.3416615 -4.4052372 -4.4854674 -4.554708][-4.4409194 -4.4252372 -4.421453 -4.41159 -4.4580426 -4.5266852 -4.536581 -4.4913912 -4.4330373 -4.4124813 -4.4210548 -4.4466739 -4.5222816 -4.6194377 -4.6923852][-4.4222 -4.4118371 -4.4081779 -4.4029961 -4.4541907 -4.5329003 -4.5699611 -4.5514708 -4.5030904 -4.4870324 -4.4977078 -4.5192404 -4.5839181 -4.671905 -4.7380953][-4.4155822 -4.4109755 -4.4102468 -4.4090767 -4.4502449 -4.5154133 -4.5590143 -4.5611634 -4.5315981 -4.5230155 -4.5304837 -4.5365272 -4.573329 -4.6327929 -4.6774869][-4.4090042 -4.4151006 -4.4219418 -4.4243407 -4.4515166 -4.4942508 -4.5301256 -4.5408635 -4.5261636 -4.5219469 -4.5251956 -4.5182204 -4.5298667 -4.5602026 -4.582201]]...]
INFO - root - 2017-12-07 23:12:31.236107: step 56810, loss = 21.26, batch loss = 21.17 (8.3 examples/sec; 0.960 sec/batch; 73h:31m:54s remains)
INFO - root - 2017-12-07 23:12:40.757070: step 56820, loss = 21.21, batch loss = 21.12 (8.0 examples/sec; 1.003 sec/batch; 76h:47m:57s remains)
INFO - root - 2017-12-07 23:12:49.991818: step 56830, loss = 21.32, batch loss = 21.24 (8.3 examples/sec; 0.967 sec/batch; 74h:00m:36s remains)
INFO - root - 2017-12-07 23:12:59.471699: step 56840, loss = 21.62, batch loss = 21.54 (8.1 examples/sec; 0.990 sec/batch; 75h:48m:32s remains)
INFO - root - 2017-12-07 23:13:08.752511: step 56850, loss = 21.81, batch loss = 21.73 (9.3 examples/sec; 0.863 sec/batch; 66h:03m:11s remains)
INFO - root - 2017-12-07 23:13:18.157714: step 56860, loss = 21.49, batch loss = 21.40 (9.0 examples/sec; 0.890 sec/batch; 68h:07m:02s remains)
INFO - root - 2017-12-07 23:13:27.607157: step 56870, loss = 21.17, batch loss = 21.09 (8.3 examples/sec; 0.959 sec/batch; 73h:23m:50s remains)
INFO - root - 2017-12-07 23:13:36.935405: step 56880, loss = 21.35, batch loss = 21.27 (8.7 examples/sec; 0.915 sec/batch; 70h:03m:58s remains)
INFO - root - 2017-12-07 23:13:46.378392: step 56890, loss = 21.38, batch loss = 21.30 (9.0 examples/sec; 0.893 sec/batch; 68h:21m:38s remains)
INFO - root - 2017-12-07 23:13:55.974713: step 56900, loss = 21.46, batch loss = 21.38 (8.4 examples/sec; 0.949 sec/batch; 72h:38m:58s remains)
2017-12-07 23:13:56.920978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2121911 -4.1527667 -4.1091948 -4.1162496 -4.1907635 -4.3012714 -4.3842573 -4.4160147 -4.417264 -4.4153113 -4.4304609 -4.4569845 -4.4683366 -4.4516335 -4.4121361][-4.2662654 -4.2072654 -4.1623058 -4.1685266 -4.2379718 -4.33336 -4.39781 -4.4198122 -4.429462 -4.4520793 -4.49034 -4.5244946 -4.5328979 -4.5093637 -4.4541225][-4.3694115 -4.305346 -4.2521286 -4.2482367 -4.2981982 -4.3540092 -4.3714337 -4.3673325 -4.3872771 -4.4476819 -4.5225477 -4.5695844 -4.5770826 -4.5538297 -4.4917254][-4.4901266 -4.4202218 -4.3523684 -4.3306046 -4.3505254 -4.3495088 -4.3057928 -4.2747288 -4.3195534 -4.4327421 -4.5496831 -4.6055226 -4.6042237 -4.5781169 -4.5153279][-4.5563569 -4.4933648 -4.4176021 -4.3784184 -4.365499 -4.30168 -4.1921368 -4.1381116 -4.2166119 -4.3889461 -4.5567751 -4.63726 -4.6355948 -4.60204 -4.5333858][-4.5212774 -4.4777932 -4.4150505 -4.3697882 -4.3237705 -4.1950722 -4.0174747 -3.9347243 -4.0320826 -4.247108 -4.4705386 -4.6127734 -4.6547637 -4.6334014 -4.5572963][-4.4069648 -4.393507 -4.3660994 -4.3293772 -4.2517414 -4.0640588 -3.82725 -3.71451 -3.8114612 -4.0441542 -4.3093786 -4.5263605 -4.64407 -4.6587181 -4.5819821][-4.279243 -4.2959294 -4.3157506 -4.3014421 -4.2012277 -3.9723587 -3.6937313 -3.5521104 -3.6425526 -3.8853498 -4.1793814 -4.4544334 -4.637753 -4.6850352 -4.6065245][-4.1960855 -4.233654 -4.2940488 -4.3087435 -4.2060761 -3.9719388 -3.6902628 -3.5383079 -3.6204247 -3.8616581 -4.1605926 -4.4538913 -4.6613369 -4.7184134 -4.6319385][-4.2146344 -4.2672868 -4.3450966 -4.3848968 -4.3072071 -4.1094666 -3.8658278 -3.7261395 -3.7935123 -4.0090256 -4.2710547 -4.5241618 -4.6989512 -4.7380705 -4.6421671][-4.2952876 -4.3507829 -4.4347954 -4.4965849 -4.4625411 -4.3227191 -4.1268187 -3.9934433 -4.0309062 -4.2064743 -4.4168377 -4.6042662 -4.71768 -4.72189 -4.6208425][-4.3577847 -4.3944888 -4.4715252 -4.5421863 -4.5415568 -4.4525027 -4.2992392 -4.1733379 -4.1895318 -4.3375163 -4.5131307 -4.6490831 -4.7075257 -4.676487 -4.573555][-4.3978109 -4.3881087 -4.4355311 -4.4945874 -4.5119104 -4.4633331 -4.3575058 -4.2624717 -4.2852626 -4.4221959 -4.5745192 -4.6717215 -4.6892915 -4.6278095 -4.5227342][-4.3803349 -4.3414612 -4.3621244 -4.4078 -4.4353981 -4.4190097 -4.359273 -4.3061337 -4.3478394 -4.4784021 -4.6074262 -4.6775789 -4.67351 -4.5964913 -4.4905963][-4.354804 -4.3263493 -4.3437152 -4.3852983 -4.4176273 -4.41516 -4.3768139 -4.3443546 -4.3866711 -4.4958591 -4.6013064 -4.6590967 -4.65226 -4.57827 -4.477726]]...]
INFO - root - 2017-12-07 23:14:06.292742: step 56910, loss = 21.21, batch loss = 21.13 (9.1 examples/sec; 0.880 sec/batch; 67h:20m:15s remains)
INFO - root - 2017-12-07 23:14:15.807302: step 56920, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.955 sec/batch; 73h:07m:00s remains)
INFO - root - 2017-12-07 23:14:25.165134: step 56930, loss = 21.46, batch loss = 21.38 (8.5 examples/sec; 0.940 sec/batch; 71h:55m:01s remains)
INFO - root - 2017-12-07 23:14:34.626686: step 56940, loss = 21.44, batch loss = 21.36 (8.3 examples/sec; 0.961 sec/batch; 73h:35m:28s remains)
INFO - root - 2017-12-07 23:14:44.046510: step 56950, loss = 21.76, batch loss = 21.68 (8.0 examples/sec; 1.000 sec/batch; 76h:34m:40s remains)
INFO - root - 2017-12-07 23:14:53.377220: step 56960, loss = 21.68, batch loss = 21.59 (8.6 examples/sec; 0.926 sec/batch; 70h:52m:58s remains)
INFO - root - 2017-12-07 23:15:02.812101: step 56970, loss = 21.66, batch loss = 21.58 (7.8 examples/sec; 1.029 sec/batch; 78h:46m:26s remains)
INFO - root - 2017-12-07 23:15:12.212061: step 56980, loss = 21.29, batch loss = 21.21 (7.9 examples/sec; 1.013 sec/batch; 77h:31m:08s remains)
INFO - root - 2017-12-07 23:15:21.440356: step 56990, loss = 21.59, batch loss = 21.51 (8.2 examples/sec; 0.977 sec/batch; 74h:47m:44s remains)
INFO - root - 2017-12-07 23:15:30.924811: step 57000, loss = 21.38, batch loss = 21.29 (8.4 examples/sec; 0.948 sec/batch; 72h:33m:27s remains)
2017-12-07 23:15:31.908253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.57934 -4.5830865 -4.5665135 -4.5197325 -4.4534936 -4.3920193 -4.3602624 -4.376061 -4.4376907 -4.48946 -4.4606156 -4.373106 -4.2987838 -4.2713933 -4.2878695][-4.6100621 -4.5775661 -4.5126 -4.4218736 -4.3303862 -4.2563968 -4.2162514 -4.239994 -4.3371706 -4.4422884 -4.4682436 -4.4202538 -4.353591 -4.3098655 -4.3116035][-4.6179566 -4.5642152 -4.4642825 -4.334568 -4.2079315 -4.1019554 -4.0366569 -4.0595007 -4.1850524 -4.3371572 -4.4233732 -4.428216 -4.3876915 -4.350584 -4.3483019][-4.6370292 -4.5816965 -4.4588313 -4.2870517 -4.1043711 -3.9448457 -3.8471069 -3.8725185 -4.0285192 -4.2225065 -4.3581724 -4.4069896 -4.3913803 -4.371985 -4.3822122][-4.646461 -4.6165805 -4.4930305 -4.2937651 -4.062892 -3.8556747 -3.7340724 -3.7684553 -3.9535496 -4.1734419 -4.333549 -4.3982863 -4.388062 -4.3780656 -4.4033594][-4.6316981 -4.6390219 -4.5338035 -4.3364291 -4.0987606 -3.8844557 -3.763917 -3.803391 -3.9892364 -4.1961851 -4.3453932 -4.4020376 -4.3866057 -4.3789 -4.4141674][-4.6299448 -4.6652212 -4.5799441 -4.399035 -4.179924 -3.989048 -3.8872271 -3.9249637 -4.0821462 -4.2440386 -4.3596725 -4.400455 -4.382803 -4.376482 -4.4109206][-4.63013 -4.6819968 -4.6161332 -4.4543829 -4.2556739 -4.0897126 -4.0038929 -4.0310607 -4.1472564 -4.2616677 -4.3509855 -4.3879671 -4.3803234 -4.3788705 -4.4024062][-4.599648 -4.6476121 -4.5910735 -4.4417253 -4.2598009 -4.1157527 -4.0447059 -4.0633512 -4.1484475 -4.2381368 -4.327116 -4.3807168 -4.3914919 -4.3914032 -4.3961821][-4.5538845 -4.56935 -4.502152 -4.354569 -4.1906819 -4.0771408 -4.0354919 -4.0657029 -4.144413 -4.2337503 -4.3345213 -4.4083343 -4.4363503 -4.4340997 -4.4191194][-4.5053706 -4.4914923 -4.4071064 -4.2547359 -4.109304 -4.0304379 -4.0266519 -4.0827632 -4.16858 -4.2630095 -4.3655181 -4.4434395 -4.47576 -4.4668412 -4.4378037][-4.455668 -4.4282894 -4.3301506 -4.1742659 -4.0459347 -3.9960148 -4.0227251 -4.0977936 -4.1852121 -4.274261 -4.3638382 -4.4325709 -4.4599857 -4.4441442 -4.4076643][-4.4324179 -4.4038582 -4.2926822 -4.1340551 -4.0173693 -3.9815969 -4.0252848 -4.1095281 -4.1892834 -4.2651958 -4.3384624 -4.39486 -4.4157887 -4.3940296 -4.3543296][-4.441258 -4.4160657 -4.3001914 -4.1490111 -4.04122 -4.0064936 -4.05769 -4.1428266 -4.2126064 -4.2740817 -4.3320546 -4.3763518 -4.3887782 -4.3594089 -4.3181643][-4.4853373 -4.45583 -4.3449817 -4.2150078 -4.1236043 -4.0936146 -4.14812 -4.2259817 -4.2820182 -4.3242469 -4.3666515 -4.3970222 -4.3957071 -4.35499 -4.3124409]]...]
INFO - root - 2017-12-07 23:15:41.451225: step 57010, loss = 21.18, batch loss = 21.10 (7.7 examples/sec; 1.038 sec/batch; 79h:27m:18s remains)
INFO - root - 2017-12-07 23:15:50.924444: step 57020, loss = 21.56, batch loss = 21.48 (7.6 examples/sec; 1.047 sec/batch; 80h:09m:15s remains)
INFO - root - 2017-12-07 23:16:00.384837: step 57030, loss = 21.82, batch loss = 21.74 (8.3 examples/sec; 0.962 sec/batch; 73h:35m:18s remains)
INFO - root - 2017-12-07 23:16:09.736635: step 57040, loss = 21.72, batch loss = 21.63 (8.9 examples/sec; 0.899 sec/batch; 68h:46m:39s remains)
INFO - root - 2017-12-07 23:16:19.120541: step 57050, loss = 21.27, batch loss = 21.19 (7.9 examples/sec; 1.007 sec/batch; 77h:05m:04s remains)
INFO - root - 2017-12-07 23:16:28.623178: step 57060, loss = 21.65, batch loss = 21.56 (7.9 examples/sec; 1.008 sec/batch; 77h:05m:54s remains)
INFO - root - 2017-12-07 23:16:38.064273: step 57070, loss = 21.50, batch loss = 21.41 (8.9 examples/sec; 0.903 sec/batch; 69h:05m:42s remains)
INFO - root - 2017-12-07 23:16:47.467218: step 57080, loss = 21.56, batch loss = 21.48 (9.1 examples/sec; 0.881 sec/batch; 67h:22m:18s remains)
INFO - root - 2017-12-07 23:16:56.866317: step 57090, loss = 21.33, batch loss = 21.24 (8.7 examples/sec; 0.915 sec/batch; 69h:58m:30s remains)
INFO - root - 2017-12-07 23:17:06.368664: step 57100, loss = 21.37, batch loss = 21.28 (8.7 examples/sec; 0.917 sec/batch; 70h:07m:16s remains)
2017-12-07 23:17:07.300313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3479033 -4.3709841 -4.3914566 -4.4177427 -4.4216285 -4.3846288 -4.3114281 -4.2523937 -4.2626505 -4.3059411 -4.3347898 -4.3393836 -4.330596 -4.3543425 -4.4168816][-4.2441974 -4.29046 -4.34641 -4.4118042 -4.444057 -4.3974948 -4.2779245 -4.166317 -4.1563935 -4.220767 -4.2953572 -4.3444982 -4.3512664 -4.3612757 -4.4034719][-4.1945195 -4.2664804 -4.3499193 -4.4391823 -4.4784436 -4.4068809 -4.2321148 -4.0584478 -4.0164366 -4.0954828 -4.2175541 -4.3262172 -4.3684845 -4.3759546 -4.3991079][-4.2143493 -4.3036571 -4.3933692 -4.4826312 -4.5119195 -4.4115639 -4.1921325 -3.9748502 -3.9157693 -4.0062747 -4.1601849 -4.3113108 -4.3873897 -4.4023886 -4.4125504][-4.2932448 -4.3825822 -4.4594316 -4.5312824 -4.5330744 -4.3990192 -4.1484132 -3.9134748 -3.8579264 -3.9669232 -4.1403513 -4.31216 -4.4086566 -4.431973 -4.434227][-4.391542 -4.4809232 -4.5415249 -4.5752959 -4.5222526 -4.3453684 -4.0826259 -3.8565121 -3.8211555 -3.9585369 -4.1562414 -4.3376808 -4.4373107 -4.458209 -4.4554038][-4.454423 -4.5459404 -4.5882497 -4.5698066 -4.4428287 -4.2140985 -3.9428561 -3.7326472 -3.7282274 -3.9099069 -4.1494555 -4.3455138 -4.445981 -4.4670472 -4.4671249][-4.4687476 -4.5544271 -4.5771551 -4.5154328 -4.3333721 -4.05999 -3.7641678 -3.5498064 -3.5759823 -3.8162842 -4.10668 -4.319417 -4.4188523 -4.4436879 -4.4563508][-4.4545388 -4.5291052 -4.5309544 -4.4533277 -4.2725811 -4.0175862 -3.7452335 -3.5516186 -3.5888681 -3.8349531 -4.1253648 -4.3233218 -4.4024925 -4.4185233 -4.436542][-4.3973761 -4.4577084 -4.4473782 -4.38586 -4.2613149 -4.0896039 -3.8961377 -3.7522354 -3.7770207 -3.9747558 -4.2159534 -4.3742938 -4.4203739 -4.4152579 -4.4260936][-4.3136344 -4.3637857 -4.3605347 -4.3352423 -4.280735 -4.1942434 -4.0734978 -3.972198 -3.9851818 -4.1263056 -4.3069739 -4.4216948 -4.4424 -4.4242706 -4.4276152][-4.2229447 -4.2700858 -4.2829428 -4.2949471 -4.3034415 -4.2861719 -4.2263651 -4.1586747 -4.1617966 -4.2507434 -4.3699532 -4.4488673 -4.4598846 -4.4452376 -4.4495854][-4.1716676 -4.2224064 -4.2449484 -4.2776327 -4.3261738 -4.3610077 -4.3542929 -4.31689 -4.3084922 -4.3478909 -4.4112291 -4.4593644 -4.4727349 -4.4752593 -4.4874415][-4.18565 -4.2370181 -4.2600055 -4.2988353 -4.3708897 -4.4402061 -4.4720707 -4.4576378 -4.4351172 -4.4251523 -4.4377084 -4.4585376 -4.4761267 -4.49728 -4.5177584][-4.2159309 -4.262629 -4.2936964 -4.3490572 -4.4433317 -4.5291128 -4.5661874 -4.5418596 -4.4942551 -4.4485383 -4.4284496 -4.4364581 -4.4631538 -4.5034428 -4.5309076]]...]
INFO - root - 2017-12-07 23:17:16.868281: step 57110, loss = 21.65, batch loss = 21.57 (9.0 examples/sec; 0.892 sec/batch; 68h:14m:34s remains)
INFO - root - 2017-12-07 23:17:26.360968: step 57120, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.960 sec/batch; 73h:27m:51s remains)
INFO - root - 2017-12-07 23:17:35.751813: step 57130, loss = 21.58, batch loss = 21.50 (8.1 examples/sec; 0.989 sec/batch; 75h:36m:48s remains)
INFO - root - 2017-12-07 23:17:45.104722: step 57140, loss = 21.36, batch loss = 21.28 (8.0 examples/sec; 1.003 sec/batch; 76h:42m:51s remains)
INFO - root - 2017-12-07 23:17:54.567952: step 57150, loss = 20.98, batch loss = 20.89 (8.4 examples/sec; 0.952 sec/batch; 72h:46m:56s remains)
INFO - root - 2017-12-07 23:18:04.005565: step 57160, loss = 21.53, batch loss = 21.44 (8.2 examples/sec; 0.972 sec/batch; 74h:22m:35s remains)
INFO - root - 2017-12-07 23:18:13.314176: step 57170, loss = 21.83, batch loss = 21.74 (8.6 examples/sec; 0.929 sec/batch; 71h:01m:57s remains)
INFO - root - 2017-12-07 23:18:22.768493: step 57180, loss = 21.03, batch loss = 20.95 (8.7 examples/sec; 0.916 sec/batch; 70h:05m:29s remains)
INFO - root - 2017-12-07 23:18:32.220725: step 57190, loss = 21.46, batch loss = 21.37 (8.9 examples/sec; 0.899 sec/batch; 68h:44m:44s remains)
INFO - root - 2017-12-07 23:18:41.643796: step 57200, loss = 21.08, batch loss = 21.00 (8.9 examples/sec; 0.904 sec/batch; 69h:05m:55s remains)
2017-12-07 23:18:42.574417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1155219 -4.1285768 -4.2704082 -4.4079661 -4.4439034 -4.4093823 -4.4011292 -4.4309731 -4.4530888 -4.4137182 -4.3474574 -4.341455 -4.3845954 -4.4001579 -4.3651128][-4.1081319 -4.146554 -4.3100462 -4.4621286 -4.4969082 -4.4443192 -4.4070554 -4.4241791 -4.4599466 -4.4327736 -4.3614955 -4.3304334 -4.3536062 -4.3770976 -4.3584194][-4.1270175 -4.1972909 -4.379035 -4.5237603 -4.5222774 -4.4172907 -4.3310571 -4.3386579 -4.4087844 -4.4277983 -4.385942 -4.3531108 -4.3660169 -4.4005384 -4.3988104][-4.16168 -4.2653604 -4.4565868 -4.5714569 -4.499701 -4.311799 -4.168314 -4.1754351 -4.2991142 -4.3909235 -4.4031858 -4.3876481 -4.3930335 -4.4256992 -4.4284592][-4.2043934 -4.329422 -4.5035839 -4.5568976 -4.3911114 -4.1174421 -3.9382308 -3.9724174 -4.1669159 -4.342865 -4.4188304 -4.4304876 -4.4265556 -4.4355655 -4.4217205][-4.2428675 -4.369473 -4.4895878 -4.451046 -4.1809716 -3.8357534 -3.6526611 -3.7449193 -4.0246706 -4.2847004 -4.4199791 -4.4575224 -4.4401078 -4.40715 -4.3620248][-4.3333187 -4.4464579 -4.4913325 -4.3606296 -4.0085578 -3.6094604 -3.4186625 -3.5472832 -3.8859408 -4.2033129 -4.3803191 -4.4390025 -4.4175725 -4.3577714 -4.3044939][-4.4821439 -4.5635438 -4.5427909 -4.370811 -4.0068946 -3.5915914 -3.3728666 -3.4832404 -3.8230491 -4.1449633 -4.3302779 -4.4048495 -4.3971357 -4.3382735 -4.2990294][-4.6253905 -4.6753278 -4.6258006 -4.4810233 -4.1851597 -3.8056331 -3.5640519 -3.6244926 -3.9208074 -4.20543 -4.3682346 -4.4422884 -4.4458532 -4.3916907 -4.3560295][-4.7396936 -4.7676258 -4.7195454 -4.6281929 -4.4269094 -4.1216021 -3.8939817 -3.9215117 -4.1654243 -4.3956347 -4.5198731 -4.5743871 -4.5736184 -4.5152416 -4.46373][-4.77683 -4.7999887 -4.7719846 -4.7306609 -4.6153 -4.403038 -4.2258072 -4.2387452 -4.4202437 -4.5890884 -4.6754651 -4.7075944 -4.6941133 -4.6275826 -4.5535016][-4.7249317 -4.7602758 -4.7598772 -4.7549062 -4.70722 -4.5894032 -4.4773984 -4.4783874 -4.5881782 -4.692203 -4.7438569 -4.7573409 -4.7318783 -4.6561017 -4.5616198][-4.6535339 -4.6908083 -4.6999946 -4.7059736 -4.6963248 -4.6500616 -4.5972805 -4.59356 -4.6441255 -4.6903219 -4.7100172 -4.70719 -4.6743951 -4.5992575 -4.5041337][-4.5815797 -4.6091752 -4.608778 -4.6021419 -4.5991321 -4.59039 -4.5801258 -4.5867629 -4.6114373 -4.6261039 -4.6231775 -4.6056213 -4.5675097 -4.5018783 -4.4247646][-4.5012665 -4.5130243 -4.496685 -4.4702573 -4.4571929 -4.4607434 -4.4744973 -4.4948816 -4.5159769 -4.5223145 -4.5112391 -4.4875779 -4.4495268 -4.3978481 -4.3464856]]...]
INFO - root - 2017-12-07 23:18:51.962497: step 57210, loss = 22.05, batch loss = 21.96 (8.9 examples/sec; 0.897 sec/batch; 68h:36m:03s remains)
INFO - root - 2017-12-07 23:19:01.214750: step 57220, loss = 21.41, batch loss = 21.32 (9.0 examples/sec; 0.886 sec/batch; 67h:45m:48s remains)
INFO - root - 2017-12-07 23:19:10.702885: step 57230, loss = 21.63, batch loss = 21.54 (8.5 examples/sec; 0.939 sec/batch; 71h:48m:24s remains)
INFO - root - 2017-12-07 23:19:20.114115: step 57240, loss = 21.74, batch loss = 21.66 (8.5 examples/sec; 0.945 sec/batch; 72h:13m:49s remains)
INFO - root - 2017-12-07 23:19:29.540644: step 57250, loss = 21.45, batch loss = 21.37 (8.2 examples/sec; 0.980 sec/batch; 74h:55m:14s remains)
INFO - root - 2017-12-07 23:19:39.012351: step 57260, loss = 21.32, batch loss = 21.23 (7.8 examples/sec; 1.025 sec/batch; 78h:23m:22s remains)
INFO - root - 2017-12-07 23:19:48.444114: step 57270, loss = 21.34, batch loss = 21.26 (8.4 examples/sec; 0.953 sec/batch; 72h:53m:48s remains)
INFO - root - 2017-12-07 23:19:57.905325: step 57280, loss = 21.52, batch loss = 21.44 (8.5 examples/sec; 0.946 sec/batch; 72h:19m:34s remains)
INFO - root - 2017-12-07 23:20:07.225706: step 57290, loss = 21.17, batch loss = 21.09 (8.8 examples/sec; 0.910 sec/batch; 69h:31m:57s remains)
INFO - root - 2017-12-07 23:20:16.694288: step 57300, loss = 21.55, batch loss = 21.47 (8.3 examples/sec; 0.962 sec/batch; 73h:34m:30s remains)
2017-12-07 23:20:17.678483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7667885 -4.8097816 -4.8349667 -4.8281679 -4.8016257 -4.7913427 -4.7907448 -4.7965183 -4.8068109 -4.7837896 -4.7330122 -4.7067251 -4.7186465 -4.735692 -4.7384787][-4.7803907 -4.8207684 -4.8380642 -4.8177996 -4.76907 -4.7413378 -4.7395139 -4.7541957 -4.7803721 -4.7677097 -4.7167416 -4.6883192 -4.710145 -4.7530928 -4.7818575][-4.6884074 -4.715126 -4.7143464 -4.6810322 -4.6167293 -4.5834146 -4.5910859 -4.6135373 -4.6487551 -4.6430645 -4.59777 -4.5677562 -4.5953712 -4.6685786 -4.7410278][-4.6041794 -4.6159282 -4.5959163 -4.5545521 -4.4904332 -4.4675546 -4.4920216 -4.5161533 -4.5422454 -4.5211082 -4.4635625 -4.4190683 -4.43915 -4.5300403 -4.6373234][-4.51936 -4.5370231 -4.5137134 -4.4715261 -4.4072995 -4.3789997 -4.401053 -4.4243851 -4.4484215 -4.4251475 -4.3674192 -4.3194189 -4.3312278 -4.4250121 -4.5436273][-4.4278007 -4.4469042 -4.4180841 -4.3597574 -4.2694235 -4.1997724 -4.188138 -4.2209249 -4.2735615 -4.2877803 -4.2607775 -4.2218566 -4.2229843 -4.31419 -4.4441762][-4.3707786 -4.3749738 -4.3240509 -4.2389431 -4.1147995 -3.9838133 -3.9196558 -3.9662187 -4.0560708 -4.1122823 -4.1144357 -4.0782828 -4.0624413 -4.145196 -4.294735][-4.3894372 -4.3700595 -4.2962904 -4.2023721 -4.0761838 -3.9170935 -3.8164215 -3.8644772 -3.9717779 -4.0345407 -4.0270061 -3.9754066 -3.9451532 -4.0177956 -4.1805868][-4.4286609 -4.4027352 -4.3305707 -4.2571888 -4.1718855 -4.0450778 -3.9543378 -3.9959071 -4.09514 -4.1403227 -4.1051426 -4.03393 -4.001894 -4.0639858 -4.2055979][-4.4436827 -4.4216518 -4.3670244 -4.3203616 -4.2871742 -4.2259426 -4.1701403 -4.2037396 -4.2818952 -4.3170385 -4.2811542 -4.2181859 -4.1996412 -4.2491488 -4.3478251][-4.4311142 -4.4222383 -4.3985953 -4.3790846 -4.382453 -4.3801479 -4.3581257 -4.372241 -4.4204979 -4.4499993 -4.4409347 -4.4103928 -4.4100733 -4.4461937 -4.50498][-4.4046288 -4.4111667 -4.4196272 -4.424058 -4.43928 -4.4664316 -4.4759421 -4.4816236 -4.5006709 -4.517643 -4.5350723 -4.5435748 -4.5612364 -4.5858994 -4.6145349][-4.4080682 -4.4266124 -4.4537153 -4.4686203 -4.4735894 -4.4934077 -4.5135655 -4.5204468 -4.5248275 -4.5244126 -4.5432353 -4.570858 -4.5991349 -4.6214609 -4.6384063][-4.4334064 -4.4589353 -4.4939947 -4.5146055 -4.5108438 -4.5130315 -4.5236263 -4.5262227 -4.5221882 -4.5102692 -4.513082 -4.5297122 -4.550128 -4.5679464 -4.5796757][-4.42958 -4.4546614 -4.4866714 -4.5064993 -4.5021057 -4.4938445 -4.491055 -4.4850173 -4.4774151 -4.4664574 -4.4625812 -4.4633894 -4.4675908 -4.47405 -4.4765296]]...]
INFO - root - 2017-12-07 23:20:27.255056: step 57310, loss = 21.55, batch loss = 21.46 (8.5 examples/sec; 0.944 sec/batch; 72h:09m:10s remains)
INFO - root - 2017-12-07 23:20:36.742670: step 57320, loss = 21.48, batch loss = 21.40 (9.0 examples/sec; 0.890 sec/batch; 68h:00m:56s remains)
INFO - root - 2017-12-07 23:20:46.002208: step 57330, loss = 21.51, batch loss = 21.43 (9.2 examples/sec; 0.869 sec/batch; 66h:26m:53s remains)
INFO - root - 2017-12-07 23:20:55.530566: step 57340, loss = 21.31, batch loss = 21.23 (8.4 examples/sec; 0.953 sec/batch; 72h:48m:21s remains)
INFO - root - 2017-12-07 23:21:05.057078: step 57350, loss = 21.47, batch loss = 21.39 (8.1 examples/sec; 0.983 sec/batch; 75h:07m:42s remains)
INFO - root - 2017-12-07 23:21:14.524397: step 57360, loss = 21.66, batch loss = 21.57 (7.7 examples/sec; 1.034 sec/batch; 78h:59m:49s remains)
INFO - root - 2017-12-07 23:21:23.995698: step 57370, loss = 21.52, batch loss = 21.43 (8.4 examples/sec; 0.953 sec/batch; 72h:48m:45s remains)
INFO - root - 2017-12-07 23:21:33.437439: step 57380, loss = 21.41, batch loss = 21.33 (8.3 examples/sec; 0.960 sec/batch; 73h:21m:58s remains)
INFO - root - 2017-12-07 23:21:42.820968: step 57390, loss = 21.55, batch loss = 21.47 (8.1 examples/sec; 0.986 sec/batch; 75h:18m:54s remains)
INFO - root - 2017-12-07 23:21:52.230211: step 57400, loss = 21.53, batch loss = 21.44 (8.5 examples/sec; 0.937 sec/batch; 71h:35m:08s remains)
2017-12-07 23:21:53.114292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2300286 -4.2436018 -4.3038311 -4.3862176 -4.4336247 -4.4139094 -4.370295 -4.3703909 -4.3822212 -4.3721318 -4.3362103 -4.2971044 -4.2787738 -4.2906804 -4.3225856][-4.2340178 -4.2498031 -4.3138065 -4.391696 -4.411212 -4.3458819 -4.2594671 -4.2376056 -4.2486997 -4.2556815 -4.2465653 -4.2205658 -4.1910763 -4.1930714 -4.2240405][-4.23596 -4.2545056 -4.3178849 -4.3851357 -4.3715706 -4.2574759 -4.1298628 -4.0929341 -4.1273656 -4.1848197 -4.2262173 -4.2224431 -4.1869421 -4.1791511 -4.1991563][-4.2389126 -4.2595353 -4.3209467 -4.3793283 -4.3424177 -4.1931958 -4.0329785 -3.9824426 -4.0462017 -4.1621504 -4.2533746 -4.2684321 -4.233017 -4.2244844 -4.2359791][-4.2408233 -4.2624722 -4.3213592 -4.3750687 -4.3301468 -4.1671767 -3.9863536 -3.9129612 -3.9806356 -4.1262197 -4.2372756 -4.2580948 -4.2321997 -4.2396073 -4.2638335][-4.2414842 -4.2621469 -4.3169932 -4.364902 -4.3157372 -4.1462693 -3.9461699 -3.8346145 -3.87151 -4.0135059 -4.1309896 -4.1646037 -4.1675563 -4.2041006 -4.250628][-4.2416811 -4.2618132 -4.3132429 -4.3538289 -4.2979674 -4.1180038 -3.8888559 -3.7219195 -3.7055624 -3.8323894 -3.9735451 -4.0532789 -4.1042552 -4.1710367 -4.231391][-4.2425289 -4.2651944 -4.3172255 -4.3562007 -4.3005972 -4.1148357 -3.8635724 -3.6485786 -3.5780311 -3.6838379 -3.8533692 -3.9879735 -4.0842524 -4.1688266 -4.228898][-4.243763 -4.2721939 -4.3292704 -4.3767366 -4.3378906 -4.170115 -3.9308469 -3.7117362 -3.6154127 -3.6881835 -3.8533747 -3.9986432 -4.0963922 -4.1708617 -4.2213635][-4.2450094 -4.2796793 -4.3427644 -4.402709 -4.3905988 -4.2625542 -4.0684857 -3.8867884 -3.7959352 -3.8374863 -3.9650891 -4.0686016 -4.1147442 -4.1486549 -4.1868286][-4.2454967 -4.282177 -4.3454 -4.4085937 -4.415184 -4.3263388 -4.1870508 -4.0535889 -3.980134 -3.9981923 -4.080956 -4.1312842 -4.1217651 -4.1165566 -4.1488714][-4.2441597 -4.2765532 -4.3313794 -4.384213 -4.3931589 -4.3286815 -4.2314029 -4.1362724 -4.0804777 -4.0849996 -4.1348624 -4.1538124 -4.1170053 -4.0954723 -4.1308622][-4.2415128 -4.2645369 -4.3076181 -4.3459945 -4.3500805 -4.2971425 -4.2215829 -4.1465287 -4.1044426 -4.1071558 -4.1391678 -4.1438613 -4.1016555 -4.0828333 -4.126709][-4.2383265 -4.25112 -4.2855163 -4.3173614 -4.3233 -4.2785892 -4.2137585 -4.146853 -4.1107697 -4.109767 -4.1230006 -4.1110249 -4.0659823 -4.0532823 -4.1042891][-4.2340574 -4.2397494 -4.2673073 -4.2990332 -4.3154173 -4.2883935 -4.2412782 -4.1859884 -4.1480393 -4.129663 -4.1103454 -4.0743036 -4.0257926 -4.015182 -4.0638456]]...]
INFO - root - 2017-12-07 23:22:02.562760: step 57410, loss = 21.35, batch loss = 21.27 (8.5 examples/sec; 0.946 sec/batch; 72h:19m:02s remains)
INFO - root - 2017-12-07 23:22:12.021875: step 57420, loss = 21.55, batch loss = 21.47 (8.0 examples/sec; 1.001 sec/batch; 76h:28m:33s remains)
INFO - root - 2017-12-07 23:22:21.205691: step 57430, loss = 20.87, batch loss = 20.79 (8.6 examples/sec; 0.933 sec/batch; 71h:17m:30s remains)
INFO - root - 2017-12-07 23:22:30.549142: step 57440, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.951 sec/batch; 72h:37m:51s remains)
INFO - root - 2017-12-07 23:22:39.968050: step 57450, loss = 21.89, batch loss = 21.81 (9.0 examples/sec; 0.889 sec/batch; 67h:53m:08s remains)
INFO - root - 2017-12-07 23:22:49.528636: step 57460, loss = 21.41, batch loss = 21.32 (8.3 examples/sec; 0.962 sec/batch; 73h:32m:04s remains)
INFO - root - 2017-12-07 23:22:58.932688: step 57470, loss = 21.10, batch loss = 21.02 (8.4 examples/sec; 0.947 sec/batch; 72h:22m:10s remains)
INFO - root - 2017-12-07 23:23:08.217092: step 57480, loss = 21.59, batch loss = 21.51 (8.9 examples/sec; 0.902 sec/batch; 68h:53m:10s remains)
INFO - root - 2017-12-07 23:23:17.565270: step 57490, loss = 21.35, batch loss = 21.27 (9.2 examples/sec; 0.868 sec/batch; 66h:18m:33s remains)
INFO - root - 2017-12-07 23:23:27.018179: step 57500, loss = 21.50, batch loss = 21.41 (9.1 examples/sec; 0.878 sec/batch; 67h:05m:15s remains)
2017-12-07 23:23:27.942731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7071996 -4.7395992 -4.7399793 -4.7040439 -4.6405272 -4.5776572 -4.548522 -4.5698037 -4.654129 -4.7492414 -4.793879 -4.7898717 -4.7617173 -4.7022533 -4.6270924][-4.7048512 -4.7516851 -4.7518754 -4.6956296 -4.6095362 -4.5385118 -4.5269618 -4.5788875 -4.6871395 -4.7870421 -4.8207345 -4.8061762 -4.774313 -4.7161059 -4.6382546][-4.6919703 -4.7445807 -4.7332125 -4.6561813 -4.5636678 -4.5020423 -4.5148191 -4.5892687 -4.7035284 -4.795238 -4.81941 -4.7998152 -4.7691274 -4.7210312 -4.6515012][-4.6834888 -4.7239666 -4.681848 -4.5761962 -4.4728613 -4.4192944 -4.4468775 -4.5335608 -4.6432028 -4.7239985 -4.7531796 -4.7511277 -4.7445273 -4.7252154 -4.6807008][-4.6818075 -4.6892982 -4.5982723 -4.4549336 -4.32588 -4.2545667 -4.2617283 -4.3374195 -4.4423838 -4.5274692 -4.5867429 -4.632247 -4.6819649 -4.7132912 -4.706594][-4.6697645 -4.6363206 -4.4949203 -4.3172464 -4.1582346 -4.0458627 -4.0018568 -4.0484848 -4.1592817 -4.2728391 -4.3792648 -4.485652 -4.5975485 -4.677598 -4.7037535][-4.6535778 -4.5959554 -4.428175 -4.2350826 -4.0563288 -3.9042106 -3.8090734 -3.8276815 -3.9568231 -4.10547 -4.2467837 -4.3839846 -4.5197897 -4.6183615 -4.6551104][-4.6526251 -4.6004767 -4.4420505 -4.2587085 -4.0840206 -3.9205027 -3.8052649 -3.8163853 -3.9662213 -4.14163 -4.293961 -4.4127865 -4.5086441 -4.5713887 -4.5810919][-4.6707516 -4.6504369 -4.5320311 -4.3833089 -4.2389679 -4.098053 -3.998122 -4.014801 -4.1717191 -4.3565054 -4.4978704 -4.5583467 -4.563221 -4.5437956 -4.4978194][-4.7034044 -4.7248731 -4.6569705 -4.5472 -4.4318776 -4.3157868 -4.2360253 -4.2500772 -4.3839068 -4.5487013 -4.6615806 -4.6637936 -4.5839305 -4.4824958 -4.3856][-4.7223725 -4.7791991 -4.7553082 -4.6737761 -4.5721993 -4.4718289 -4.4128156 -4.4211769 -4.5159669 -4.6440892 -4.7209859 -4.6828556 -4.55635 -4.405426 -4.2797527][-4.7058768 -4.7865415 -4.7950935 -4.7341743 -4.641448 -4.5577993 -4.5188603 -4.5199375 -4.5783567 -4.6734409 -4.7254105 -4.674232 -4.5437717 -4.3880534 -4.2573848][-4.648901 -4.7240849 -4.741437 -4.6918325 -4.612658 -4.5518541 -4.532321 -4.5311165 -4.570787 -4.6518908 -4.7018809 -4.6592426 -4.5478725 -4.421505 -4.3175683][-4.5608163 -4.6074743 -4.617888 -4.5792694 -4.5293646 -4.5044169 -4.5065837 -4.5109978 -4.5427046 -4.6103215 -4.6627278 -4.6380959 -4.5578432 -4.477272 -4.41997][-4.4819045 -4.5125093 -4.5212479 -4.4984541 -4.4775853 -4.4767532 -4.4836698 -4.4822326 -4.4997063 -4.546783 -4.5990434 -4.59702 -4.5414848 -4.4871383 -4.4516487]]...]
INFO - root - 2017-12-07 23:23:37.284410: step 57510, loss = 21.44, batch loss = 21.36 (8.8 examples/sec; 0.912 sec/batch; 69h:39m:40s remains)
INFO - root - 2017-12-07 23:23:46.672528: step 57520, loss = 21.81, batch loss = 21.73 (9.1 examples/sec; 0.881 sec/batch; 67h:18m:25s remains)
INFO - root - 2017-12-07 23:23:55.920849: step 57530, loss = 21.73, batch loss = 21.65 (9.0 examples/sec; 0.886 sec/batch; 67h:38m:36s remains)
INFO - root - 2017-12-07 23:24:05.309444: step 57540, loss = 21.03, batch loss = 20.95 (8.7 examples/sec; 0.922 sec/batch; 70h:25m:11s remains)
INFO - root - 2017-12-07 23:24:14.797031: step 57550, loss = 21.75, batch loss = 21.66 (8.3 examples/sec; 0.966 sec/batch; 73h:48m:48s remains)
INFO - root - 2017-12-07 23:24:24.144332: step 57560, loss = 21.58, batch loss = 21.49 (8.5 examples/sec; 0.943 sec/batch; 72h:02m:50s remains)
INFO - root - 2017-12-07 23:24:33.599240: step 57570, loss = 21.24, batch loss = 21.16 (8.2 examples/sec; 0.970 sec/batch; 74h:04m:59s remains)
INFO - root - 2017-12-07 23:24:43.055923: step 57580, loss = 21.23, batch loss = 21.15 (8.6 examples/sec; 0.933 sec/batch; 71h:14m:29s remains)
INFO - root - 2017-12-07 23:24:52.445398: step 57590, loss = 21.68, batch loss = 21.60 (8.3 examples/sec; 0.960 sec/batch; 73h:17m:04s remains)
INFO - root - 2017-12-07 23:25:01.927753: step 57600, loss = 21.82, batch loss = 21.73 (8.2 examples/sec; 0.975 sec/batch; 74h:28m:40s remains)
2017-12-07 23:25:02.883554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4835296 -4.3424587 -4.2364 -4.2221479 -4.2427969 -4.2239814 -4.1829119 -4.1685805 -4.2307048 -4.3864274 -4.5697522 -4.7170954 -4.7742543 -4.7220774 -4.5820427][-4.5140829 -4.3867831 -4.2860117 -4.2560887 -4.2538033 -4.2145548 -4.1519437 -4.1237397 -4.1835313 -4.3351192 -4.5090051 -4.650125 -4.7061052 -4.6407361 -4.4790716][-4.5473523 -4.4458747 -4.3534107 -4.2956352 -4.2545218 -4.1849647 -4.1028528 -4.0647345 -4.1193681 -4.2584109 -4.41701 -4.5517035 -4.6042128 -4.5297012 -4.3643379][-4.5791678 -4.4968953 -4.3999081 -4.3090429 -4.2273707 -4.1260424 -4.0309758 -3.9861939 -4.0304561 -4.1543159 -4.30187 -4.4378939 -4.4883957 -4.4128737 -4.26679][-4.5917535 -4.52232 -4.41763 -4.3035207 -4.197906 -4.0811944 -3.9845808 -3.9386339 -3.9702473 -4.0750327 -4.209713 -4.3446131 -4.394536 -4.3288417 -4.2126336][-4.5840464 -4.5268693 -4.4234595 -4.3028769 -4.1895375 -4.0700808 -3.976759 -3.933732 -3.960279 -4.0522184 -4.1735029 -4.2950964 -4.3343773 -4.2740006 -4.1818342][-4.5583482 -4.51045 -4.41626 -4.2977476 -4.1779757 -4.0538197 -3.9619613 -3.9318666 -3.9758618 -4.08142 -4.20683 -4.3131804 -4.3320408 -4.265902 -4.1892042][-4.5277033 -4.4852977 -4.4016466 -4.2804875 -4.1385741 -3.9940553 -3.8964949 -3.8851261 -3.9679441 -4.116519 -4.2752986 -4.3852863 -4.390408 -4.3148546 -4.2438617][-4.4781437 -4.4426656 -4.3734632 -4.2505984 -4.0809011 -3.9068735 -3.8003318 -3.8069983 -3.92875 -4.1223187 -4.322649 -4.4523396 -4.4608388 -4.38502 -4.3131552][-4.4070206 -4.3828492 -4.3397613 -4.2306113 -4.05093 -3.8593783 -3.7481163 -3.7689517 -3.9127624 -4.1230192 -4.3379679 -4.4777584 -4.4956889 -4.4266791 -4.3512554][-4.3313355 -4.3189092 -4.3030787 -4.2221055 -4.0639486 -3.8867555 -3.7869964 -3.8202848 -3.9643495 -4.1549873 -4.3476453 -4.4728465 -4.4906321 -4.4241643 -4.3413105][-4.27893 -4.2714562 -4.2759643 -4.2309909 -4.1187468 -3.9835794 -3.9070258 -3.9401059 -4.0592957 -4.2057076 -4.3559337 -4.4543166 -4.4645967 -4.3951797 -4.30724][-4.2824945 -4.2815475 -4.2975206 -4.2745609 -4.2003746 -4.1042547 -4.0469561 -4.06929 -4.1541252 -4.2573552 -4.36819 -4.4424424 -4.4463372 -4.3772988 -4.2938595][-4.3228331 -4.3235836 -4.3300633 -4.3031826 -4.2490911 -4.1900206 -4.1583376 -4.1741133 -4.2283163 -4.2936349 -4.3695884 -4.4260039 -4.4293308 -4.3707552 -4.3047428][-4.3302603 -4.3197742 -4.3073211 -4.2751074 -4.2478051 -4.2362714 -4.2391429 -4.2485361 -4.264061 -4.2843118 -4.3246989 -4.3691463 -4.3807311 -4.3450384 -4.3046322]]...]
INFO - root - 2017-12-07 23:25:12.315977: step 57610, loss = 21.09, batch loss = 21.01 (8.8 examples/sec; 0.908 sec/batch; 69h:20m:02s remains)
INFO - root - 2017-12-07 23:25:21.866990: step 57620, loss = 21.92, batch loss = 21.83 (8.3 examples/sec; 0.966 sec/batch; 73h:46m:37s remains)
INFO - root - 2017-12-07 23:25:31.203607: step 57630, loss = 21.86, batch loss = 21.77 (8.5 examples/sec; 0.941 sec/batch; 71h:52m:39s remains)
INFO - root - 2017-12-07 23:25:40.459846: step 57640, loss = 21.44, batch loss = 21.35 (8.7 examples/sec; 0.921 sec/batch; 70h:19m:25s remains)
INFO - root - 2017-12-07 23:25:49.631055: step 57650, loss = 21.43, batch loss = 21.35 (9.2 examples/sec; 0.866 sec/batch; 66h:08m:22s remains)
INFO - root - 2017-12-07 23:25:59.040732: step 57660, loss = 21.27, batch loss = 21.18 (8.5 examples/sec; 0.945 sec/batch; 72h:11m:00s remains)
INFO - root - 2017-12-07 23:26:08.502129: step 57670, loss = 21.10, batch loss = 21.02 (8.4 examples/sec; 0.954 sec/batch; 72h:48m:33s remains)
INFO - root - 2017-12-07 23:26:17.803551: step 57680, loss = 21.14, batch loss = 21.05 (8.9 examples/sec; 0.903 sec/batch; 68h:57m:51s remains)
INFO - root - 2017-12-07 23:26:27.317189: step 57690, loss = 21.25, batch loss = 21.17 (8.3 examples/sec; 0.967 sec/batch; 73h:47m:26s remains)
INFO - root - 2017-12-07 23:26:36.631964: step 57700, loss = 21.28, batch loss = 21.20 (8.3 examples/sec; 0.965 sec/batch; 73h:39m:26s remains)
2017-12-07 23:26:37.629438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3435135 -4.3724589 -4.4331284 -4.5172796 -4.5840368 -4.595789 -4.5464559 -4.4721293 -4.4242973 -4.4277105 -4.4669046 -4.4834652 -4.4587607 -4.4087849 -4.3603325][-4.4363141 -4.4681287 -4.5211287 -4.5755606 -4.5843058 -4.5234766 -4.4074292 -4.2848005 -4.2218876 -4.249464 -4.3225746 -4.3495426 -4.3090286 -4.2335787 -4.17123][-4.5335155 -4.5582862 -4.5799522 -4.5718284 -4.4982924 -4.3604569 -4.1955032 -4.0515885 -3.9913659 -4.0448637 -4.1487293 -4.1931844 -4.1555238 -4.0728683 -3.99881][-4.5984411 -4.6143117 -4.599998 -4.524559 -4.3788452 -4.1931214 -4.0206962 -3.8916461 -3.8479173 -3.9181995 -4.0371256 -4.0996494 -4.0796642 -4.0028987 -3.9158731][-4.6082006 -4.6162906 -4.5755239 -4.4564047 -4.2788019 -4.0936184 -3.9509141 -3.8561859 -3.8314922 -3.9054787 -4.0247879 -4.0977535 -4.0929303 -4.0238085 -3.9307542][-4.5708961 -4.5729551 -4.5205364 -4.3901472 -4.21827 -4.0599523 -3.950222 -3.8788252 -3.86809 -3.9499745 -4.0738339 -4.1519375 -4.1457348 -4.07217 -3.9791713][-4.5010071 -4.5004563 -4.4540009 -4.3360195 -4.1789365 -4.0309076 -3.920069 -3.8482146 -3.8564491 -3.9736607 -4.1329937 -4.2224636 -4.1994095 -4.1030169 -4.0077653][-4.412385 -4.4213529 -4.4027667 -4.3156409 -4.1656165 -3.9926822 -3.8443735 -3.7539077 -3.7805939 -3.9433005 -4.1511421 -4.2568607 -4.2115831 -4.0829806 -3.9828768][-4.3135648 -4.3564515 -4.3937488 -4.3519406 -4.204432 -3.9881141 -3.7908034 -3.6782312 -3.7164028 -3.9153392 -4.1626592 -4.277946 -4.2077861 -4.0488119 -3.9438143][-4.23235 -4.3193121 -4.416461 -4.4241605 -4.2862897 -4.0415277 -3.8145576 -3.6940958 -3.7442312 -3.9658139 -4.2313042 -4.3425441 -4.2486091 -4.0658393 -3.9529703][-4.2014914 -4.30913 -4.4398785 -4.4875646 -4.3756666 -4.1369395 -3.9141088 -3.8057818 -3.870086 -4.0962152 -4.35216 -4.4445772 -4.332974 -4.1400857 -4.0203524][-4.2416334 -4.3358107 -4.4613357 -4.5250483 -4.4426837 -4.2368803 -4.045043 -3.9620931 -4.0402346 -4.2560906 -4.4817195 -4.5492067 -4.4362245 -4.2517185 -4.1327186][-4.3448238 -4.4058371 -4.4977965 -4.5537138 -4.4966083 -4.3396683 -4.1933222 -4.1380625 -4.2175322 -4.4048967 -4.5863314 -4.6341538 -4.537209 -4.3788261 -4.2705913][-4.466958 -4.4982138 -4.5543818 -4.5929146 -4.5555282 -4.4507203 -4.3510895 -4.3160496 -4.38033 -4.5216165 -4.6546836 -4.6908083 -4.6222286 -4.50173 -4.412077][-4.554997 -4.5715413 -4.6035066 -4.6258268 -4.604219 -4.5438709 -4.4815388 -4.4556975 -4.4919138 -4.5814705 -4.6730194 -4.7082977 -4.6778603 -4.6041255 -4.5411787]]...]
INFO - root - 2017-12-07 23:26:47.172529: step 57710, loss = 21.48, batch loss = 21.40 (8.2 examples/sec; 0.972 sec/batch; 74h:11m:39s remains)
INFO - root - 2017-12-07 23:26:56.493730: step 57720, loss = 21.38, batch loss = 21.30 (9.0 examples/sec; 0.887 sec/batch; 67h:40m:11s remains)
INFO - root - 2017-12-07 23:27:05.885952: step 57730, loss = 21.30, batch loss = 21.21 (8.2 examples/sec; 0.970 sec/batch; 74h:01m:56s remains)
INFO - root - 2017-12-07 23:27:15.261480: step 57740, loss = 21.42, batch loss = 21.34 (8.7 examples/sec; 0.915 sec/batch; 69h:47m:54s remains)
INFO - root - 2017-12-07 23:27:24.722555: step 57750, loss = 21.42, batch loss = 21.34 (8.7 examples/sec; 0.915 sec/batch; 69h:50m:43s remains)
INFO - root - 2017-12-07 23:27:33.937630: step 57760, loss = 21.30, batch loss = 21.22 (8.8 examples/sec; 0.905 sec/batch; 69h:04m:24s remains)
INFO - root - 2017-12-07 23:27:43.367365: step 57770, loss = 21.78, batch loss = 21.70 (8.7 examples/sec; 0.919 sec/batch; 70h:07m:24s remains)
INFO - root - 2017-12-07 23:27:52.994961: step 57780, loss = 21.17, batch loss = 21.08 (8.3 examples/sec; 0.968 sec/batch; 73h:54m:09s remains)
INFO - root - 2017-12-07 23:28:02.392391: step 57790, loss = 21.72, batch loss = 21.64 (8.7 examples/sec; 0.921 sec/batch; 70h:17m:36s remains)
INFO - root - 2017-12-07 23:28:11.693455: step 57800, loss = 21.67, batch loss = 21.59 (8.6 examples/sec; 0.925 sec/batch; 70h:35m:43s remains)
2017-12-07 23:28:12.668770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5480638 -4.5808139 -4.62613 -4.6574664 -4.64057 -4.5767431 -4.5195155 -4.5035605 -4.4935303 -4.4713078 -4.445044 -4.4063029 -4.3485227 -4.2815928 -4.2259617][-4.6113663 -4.65978 -4.7265167 -4.7709651 -4.7531686 -4.6752205 -4.6181817 -4.6369915 -4.6702633 -4.6714282 -4.6457357 -4.5821228 -4.4807625 -4.3756366 -4.3025937][-4.6673651 -4.7283854 -4.8085051 -4.8558493 -4.8269196 -4.7316852 -4.6762152 -4.7318668 -4.8140421 -4.846436 -4.8252158 -4.7383356 -4.591608 -4.4424162 -4.3470035][-4.7084031 -4.7733941 -4.8505135 -4.8804426 -4.8183923 -4.6874051 -4.6181588 -4.6927543 -4.8199329 -4.8955631 -4.8987908 -4.8130083 -4.6460428 -4.4700556 -4.3554363][-4.7229347 -4.7734332 -4.8301287 -4.8235216 -4.7083182 -4.5214357 -4.4160962 -4.4903316 -4.6553006 -4.7881794 -4.8456063 -4.8002748 -4.6611862 -4.499908 -4.3872733][-4.6883049 -4.7158422 -4.7371764 -4.673183 -4.482203 -4.2163415 -4.0596795 -4.13534 -4.3543649 -4.5705833 -4.7127032 -4.7408872 -4.661644 -4.5419011 -4.448276][-4.6214 -4.6238785 -4.6075416 -4.4890718 -4.2193232 -3.8636973 -3.644912 -3.7270634 -4.0223532 -4.3436747 -4.5824161 -4.6940536 -4.684432 -4.6149125 -4.544672][-4.5589314 -4.5444608 -4.5083208 -4.3709455 -4.0655394 -3.6555138 -3.3850741 -3.4623079 -3.8165178 -4.223855 -4.5332685 -4.7011652 -4.7369533 -4.6997795 -4.6433249][-4.5217514 -4.5089483 -4.4869318 -4.3801789 -4.1099486 -3.7242744 -3.4526258 -3.5144494 -3.8669691 -4.2844114 -4.5934896 -4.752933 -4.7793059 -4.7396755 -4.6855831][-4.5222735 -4.5229993 -4.5320134 -4.4823565 -4.2898188 -3.9871624 -3.7648938 -3.8088036 -4.0971766 -4.4371467 -4.6749649 -4.7803783 -4.7740159 -4.7222376 -4.673295][-4.5589609 -4.5793066 -4.6185083 -4.6150866 -4.5004234 -4.2957473 -4.1406217 -4.1699829 -4.36926 -4.5942464 -4.7334266 -4.7735672 -4.7352543 -4.6706953 -4.6242876][-4.6160483 -4.6536427 -4.7073832 -4.728117 -4.6711297 -4.55385 -4.4682436 -4.4942026 -4.6149182 -4.7352996 -4.78708 -4.7711043 -4.7079844 -4.6332712 -4.5803022][-4.64955 -4.6835175 -4.7266603 -4.7481318 -4.7251234 -4.6716404 -4.6385365 -4.6635942 -4.7299185 -4.7823033 -4.7848063 -4.7448077 -4.6818104 -4.6185212 -4.5711946][-4.6017609 -4.6282659 -4.656384 -4.6690879 -4.6590047 -4.6376987 -4.6310949 -4.6537948 -4.6937056 -4.7221484 -4.7207069 -4.69203 -4.6488233 -4.6056271 -4.5701466][-4.4504075 -4.476963 -4.503727 -4.5147185 -4.5071335 -4.49352 -4.4929228 -4.5134788 -4.5478525 -4.5805521 -4.5991969 -4.5995345 -4.5855117 -4.5663085 -4.5475569]]...]
INFO - root - 2017-12-07 23:28:21.893111: step 57810, loss = 21.75, batch loss = 21.67 (8.5 examples/sec; 0.943 sec/batch; 71h:59m:02s remains)
INFO - root - 2017-12-07 23:28:31.239796: step 57820, loss = 21.44, batch loss = 21.36 (8.1 examples/sec; 0.991 sec/batch; 75h:38m:18s remains)
INFO - root - 2017-12-07 23:28:40.737086: step 57830, loss = 21.81, batch loss = 21.73 (8.3 examples/sec; 0.968 sec/batch; 73h:51m:51s remains)
INFO - root - 2017-12-07 23:28:50.106436: step 57840, loss = 21.63, batch loss = 21.55 (8.1 examples/sec; 0.986 sec/batch; 75h:12m:19s remains)
INFO - root - 2017-12-07 23:28:59.416739: step 57850, loss = 21.53, batch loss = 21.45 (8.8 examples/sec; 0.914 sec/batch; 69h:42m:39s remains)
INFO - root - 2017-12-07 23:29:08.962950: step 57860, loss = 20.87, batch loss = 20.79 (8.2 examples/sec; 0.974 sec/batch; 74h:16m:04s remains)
INFO - root - 2017-12-07 23:29:18.314842: step 57870, loss = 21.51, batch loss = 21.43 (8.5 examples/sec; 0.939 sec/batch; 71h:36m:30s remains)
INFO - root - 2017-12-07 23:29:27.757381: step 57880, loss = 21.63, batch loss = 21.55 (8.5 examples/sec; 0.936 sec/batch; 71h:25m:33s remains)
INFO - root - 2017-12-07 23:29:37.180740: step 57890, loss = 21.58, batch loss = 21.50 (8.7 examples/sec; 0.919 sec/batch; 70h:06m:40s remains)
INFO - root - 2017-12-07 23:29:46.609853: step 57900, loss = 21.67, batch loss = 21.59 (8.7 examples/sec; 0.919 sec/batch; 70h:03m:57s remains)
2017-12-07 23:29:47.510358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6067491 -4.7318339 -4.8691859 -4.9439754 -4.9251957 -4.86426 -4.8148842 -4.7994957 -4.8373041 -4.9276729 -5.0259805 -5.0600982 -5.0122604 -4.9169173 -4.82336][-4.5776563 -4.7395048 -4.8931532 -4.9381576 -4.8499174 -4.723825 -4.6397724 -4.63738 -4.7188663 -4.8518472 -4.9984379 -5.0889778 -5.0729628 -4.9765997 -4.8792324][-4.5396109 -4.7091942 -4.8442163 -4.8280206 -4.645062 -4.4350338 -4.3130865 -4.336391 -4.4782429 -4.6570959 -4.847476 -5.0096097 -5.0552626 -4.97823 -4.8804383][-4.4908724 -4.6377649 -4.7225418 -4.633153 -4.3690519 -4.0871 -3.93054 -3.9738128 -4.1791043 -4.4172616 -4.6545734 -4.8734665 -4.9665618 -4.8981733 -4.7871246][-4.4642196 -4.574235 -4.600316 -4.4493704 -4.1279178 -3.7882121 -3.5896926 -3.6390278 -3.9129763 -4.2384152 -4.5283604 -4.7633567 -4.849288 -4.7559867 -4.61847][-4.4736829 -4.5421314 -4.5190749 -4.3355427 -3.9945421 -3.6306777 -3.4051185 -3.450547 -3.7660725 -4.1608262 -4.4798741 -4.685832 -4.725862 -4.598227 -4.4441919][-4.432086 -4.4753852 -4.44721 -4.2826262 -3.9781904 -3.6478131 -3.438123 -3.4804792 -3.7819529 -4.1706009 -4.4587703 -4.6032691 -4.5900912 -4.443964 -4.2912068][-4.2932954 -4.3484092 -4.3773203 -4.2937312 -4.0783224 -3.8256121 -3.6560431 -3.685169 -3.9180691 -4.2176375 -4.4176655 -4.490149 -4.4387655 -4.2997694 -4.1685243][-4.0793905 -4.1757717 -4.30266 -4.3387804 -4.2363443 -4.0699573 -3.9314492 -3.9343247 -4.0818348 -4.2595711 -4.35057 -4.3549275 -4.2854834 -4.176487 -4.086885][-3.8733761 -4.0142112 -4.2321134 -4.3697162 -4.3547812 -4.2485371 -4.1204224 -4.0928593 -4.1686983 -4.2452435 -4.2594118 -4.2362261 -4.1804032 -4.1187115 -4.0744348][-3.7734466 -3.9297078 -4.1737704 -4.3498812 -4.375658 -4.3034487 -4.1784735 -4.1287379 -4.1635327 -4.19191 -4.1836772 -4.176156 -4.1648293 -4.156074 -4.1477232][-3.8297551 -3.9510272 -4.1507263 -4.3070688 -4.3493142 -4.3143497 -4.2132382 -4.1589541 -4.1730928 -4.1868396 -4.1901808 -4.2118125 -4.2443509 -4.2705398 -4.2724872][-4.046937 -4.1134 -4.2341127 -4.3393788 -4.3880281 -4.3966813 -4.3442907 -4.304965 -4.3055377 -4.3104696 -4.3219891 -4.3562531 -4.4077959 -4.438859 -4.4259806][-4.334053 -4.3586159 -4.4073577 -4.4563189 -4.4968581 -4.531981 -4.5275569 -4.5162334 -4.5131111 -4.5072613 -4.5089827 -4.530436 -4.5737467 -4.5943589 -4.5692048][-4.5278726 -4.5449066 -4.5624 -4.574728 -4.5923672 -4.6168861 -4.6277571 -4.6318097 -4.630002 -4.61989 -4.6132374 -4.6212606 -4.6462955 -4.6543727 -4.631011]]...]
INFO - root - 2017-12-07 23:29:56.752140: step 57910, loss = 21.41, batch loss = 21.32 (9.2 examples/sec; 0.872 sec/batch; 66h:32m:55s remains)
INFO - root - 2017-12-07 23:30:06.135563: step 57920, loss = 22.04, batch loss = 21.96 (9.3 examples/sec; 0.863 sec/batch; 65h:49m:51s remains)
INFO - root - 2017-12-07 23:30:15.675015: step 57930, loss = 21.26, batch loss = 21.18 (8.5 examples/sec; 0.942 sec/batch; 71h:48m:28s remains)
INFO - root - 2017-12-07 23:30:25.020770: step 57940, loss = 21.25, batch loss = 21.17 (8.1 examples/sec; 0.987 sec/batch; 75h:14m:36s remains)
INFO - root - 2017-12-07 23:30:34.417655: step 57950, loss = 21.31, batch loss = 21.23 (7.8 examples/sec; 1.031 sec/batch; 78h:39m:19s remains)
INFO - root - 2017-12-07 23:30:43.709678: step 57960, loss = 21.58, batch loss = 21.50 (8.7 examples/sec; 0.916 sec/batch; 69h:52m:16s remains)
INFO - root - 2017-12-07 23:30:53.129256: step 57970, loss = 21.18, batch loss = 21.10 (8.8 examples/sec; 0.907 sec/batch; 69h:11m:22s remains)
INFO - root - 2017-12-07 23:31:02.537779: step 57980, loss = 21.51, batch loss = 21.43 (8.7 examples/sec; 0.917 sec/batch; 69h:56m:36s remains)
INFO - root - 2017-12-07 23:31:12.063228: step 57990, loss = 21.13, batch loss = 21.05 (8.0 examples/sec; 1.006 sec/batch; 76h:40m:54s remains)
INFO - root - 2017-12-07 23:31:21.272476: step 58000, loss = 21.03, batch loss = 20.94 (8.5 examples/sec; 0.936 sec/batch; 71h:21m:21s remains)
2017-12-07 23:31:22.244370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4477024 -4.4389014 -4.4092617 -4.3802724 -4.3550897 -4.3393741 -4.3422165 -4.367703 -4.3946018 -4.4001679 -4.3852491 -4.368093 -4.3581076 -4.3539591 -4.3539529][-4.5003271 -4.5074396 -4.4915795 -4.4682832 -4.424787 -4.3724675 -4.3447781 -4.3662281 -4.4112377 -4.4360776 -4.4274549 -4.4091554 -4.395895 -4.3846273 -4.3747225][-4.5157576 -4.5539074 -4.5719228 -4.5680785 -4.5074434 -4.4071541 -4.3342509 -4.3372955 -4.3921595 -4.4438262 -4.4584465 -4.4546509 -4.4480939 -4.4341679 -4.4152727][-4.4740286 -4.5265336 -4.5598569 -4.560452 -4.4879351 -4.3649287 -4.2724671 -4.2722378 -4.3378849 -4.4146404 -4.4584384 -4.4787955 -4.487102 -4.4779973 -4.4568186][-4.3724918 -4.4112539 -4.4256587 -4.4040141 -4.3274355 -4.2280383 -4.1684866 -4.1966686 -4.2784352 -4.3628206 -4.4177189 -4.4573965 -4.4859104 -4.4899158 -4.4763427][-4.250845 -4.2548513 -4.2346773 -4.1792107 -4.0980139 -4.0430832 -4.0421696 -4.1084905 -4.1978521 -4.2681088 -4.3107009 -4.361217 -4.4159966 -4.4439721 -4.4495206][-4.1861868 -4.1457162 -4.0881433 -3.9985366 -3.9067903 -3.8845708 -3.9314127 -4.0169606 -4.097199 -4.1452274 -4.1739068 -4.2346559 -4.3156123 -4.3699512 -4.3974862][-4.2135324 -4.1366134 -4.0520096 -3.9498894 -3.8601463 -3.8503871 -3.906096 -3.9684448 -4.0153232 -4.0551834 -4.102941 -4.1924696 -4.2927003 -4.3555784 -4.3845034][-4.255033 -4.1937089 -4.1251907 -4.0497017 -3.9812744 -3.9631689 -3.9775631 -3.9742179 -3.9717579 -4.0213375 -4.1191392 -4.2480145 -4.3558974 -4.4137211 -4.4280396][-4.2053981 -4.2044325 -4.19964 -4.1732926 -4.1292319 -4.0892973 -4.0463204 -3.9751289 -3.9348361 -4.0064178 -4.1520519 -4.2969937 -4.396482 -4.45418 -4.4678388][-4.0560451 -4.1159472 -4.1767392 -4.19274 -4.1639981 -4.1099091 -4.0429411 -3.951138 -3.9096892 -4.0079508 -4.1735687 -4.3016019 -4.3802485 -4.442893 -4.472362][-3.9070354 -3.9991472 -4.100791 -4.1452532 -4.1298308 -4.0857816 -4.0412841 -3.9820521 -3.9681494 -4.0813866 -4.2369776 -4.3295379 -4.3794484 -4.4376054 -4.4787679][-3.8718596 -3.9474413 -4.0520244 -4.1077571 -4.1099849 -4.0975909 -4.1088505 -4.1102123 -4.1245666 -4.2201219 -4.3342881 -4.3863516 -4.412673 -4.4647021 -4.5097165][-3.9778762 -4.0085683 -4.0849805 -4.1295462 -4.1412263 -4.1612391 -4.2268705 -4.2810884 -4.2973542 -4.3287039 -4.36507 -4.3752766 -4.3932004 -4.4514704 -4.5113134][-4.139915 -4.1316223 -4.1748438 -4.2018189 -4.2170715 -4.2582245 -4.3503566 -4.4197164 -4.4052124 -4.3559885 -4.3174505 -4.2973623 -4.3142219 -4.375205 -4.451344]]...]
INFO - root - 2017-12-07 23:31:31.564581: step 58010, loss = 21.24, batch loss = 21.15 (8.3 examples/sec; 0.964 sec/batch; 73h:31m:15s remains)
INFO - root - 2017-12-07 23:31:40.949428: step 58020, loss = 21.45, batch loss = 21.37 (8.0 examples/sec; 1.004 sec/batch; 76h:30m:43s remains)
INFO - root - 2017-12-07 23:31:50.430544: step 58030, loss = 21.15, batch loss = 21.07 (8.1 examples/sec; 0.989 sec/batch; 75h:22m:35s remains)
INFO - root - 2017-12-07 23:31:59.795551: step 58040, loss = 21.42, batch loss = 21.34 (8.8 examples/sec; 0.913 sec/batch; 69h:34m:37s remains)
INFO - root - 2017-12-07 23:32:09.046597: step 58050, loss = 21.49, batch loss = 21.40 (8.6 examples/sec; 0.927 sec/batch; 70h:39m:27s remains)
INFO - root - 2017-12-07 23:32:18.530752: step 58060, loss = 21.68, batch loss = 21.59 (8.5 examples/sec; 0.945 sec/batch; 72h:03m:06s remains)
INFO - root - 2017-12-07 23:32:28.081164: step 58070, loss = 21.27, batch loss = 21.19 (8.1 examples/sec; 0.984 sec/batch; 75h:02m:25s remains)
INFO - root - 2017-12-07 23:32:37.428176: step 58080, loss = 21.82, batch loss = 21.74 (9.2 examples/sec; 0.870 sec/batch; 66h:21m:00s remains)
INFO - root - 2017-12-07 23:32:46.813414: step 58090, loss = 21.66, batch loss = 21.58 (8.8 examples/sec; 0.906 sec/batch; 69h:03m:00s remains)
INFO - root - 2017-12-07 23:32:56.203503: step 58100, loss = 21.44, batch loss = 21.36 (9.0 examples/sec; 0.887 sec/batch; 67h:36m:47s remains)
2017-12-07 23:32:57.196683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8716736 -5.0041413 -5.0296626 -4.9944191 -4.9280229 -4.8201103 -4.6566229 -4.4640288 -4.3455253 -4.3844218 -4.4959569 -4.5776534 -4.6161065 -4.625701 -4.6287208][-4.664762 -4.8491764 -4.930131 -4.9310293 -4.8691125 -4.7402949 -4.5537748 -4.3415456 -4.2152472 -4.2634645 -4.37908 -4.4466367 -4.451725 -4.4238372 -4.40066][-4.5038428 -4.6936741 -4.804316 -4.8387651 -4.79893 -4.6712408 -4.4800406 -4.2648873 -4.1421437 -4.1887245 -4.2962704 -4.3484225 -4.3207736 -4.2534637 -4.193819][-4.4247165 -4.5844226 -4.6855779 -4.7240019 -4.6948185 -4.5774193 -4.4032826 -4.2165709 -4.1274452 -4.1866469 -4.2917891 -4.3332 -4.2748275 -4.1739211 -4.0858808][-4.3595886 -4.4686151 -4.5413985 -4.5590148 -4.5160575 -4.4020457 -4.2536616 -4.1134295 -4.0798922 -4.1726875 -4.2980251 -4.3542418 -4.29211 -4.1807876 -4.08578][-4.2487307 -4.3056068 -4.3590479 -4.3689165 -4.3191872 -4.21014 -4.0792108 -3.9714777 -3.9819603 -4.1073956 -4.2603326 -4.3429484 -4.2994661 -4.1991658 -4.1178303][-4.0892487 -4.1123581 -4.1749029 -4.2070093 -4.1765218 -4.0838017 -3.9594681 -3.8605568 -3.8869839 -4.0252767 -4.187489 -4.2797461 -4.2484884 -4.1598878 -4.1020584][-3.9714925 -3.9769545 -4.0513358 -4.1051297 -4.0992284 -4.0346508 -3.9271662 -3.8429945 -3.8772144 -3.9999804 -4.1270618 -4.1853952 -4.1548834 -4.1021409 -4.097878][-3.9888446 -3.99821 -4.0788083 -4.1364479 -4.1327996 -4.0789413 -3.9826987 -3.9184785 -3.9631643 -4.0621676 -4.1341991 -4.1402125 -4.1174893 -4.1360092 -4.2135582][-4.088738 -4.1161065 -4.2054257 -4.2720437 -4.2783527 -4.2395868 -4.1541996 -4.1051054 -4.1483765 -4.2133827 -4.227478 -4.1861863 -4.1726294 -4.2537694 -4.3957429][-4.2608776 -4.2736945 -4.3370214 -4.3995061 -4.4258518 -4.4191341 -4.3668833 -4.3468814 -4.3910904 -4.42539 -4.3966455 -4.3248577 -4.3146887 -4.4204226 -4.57779][-4.49594 -4.4863358 -4.50538 -4.533463 -4.5490832 -4.547121 -4.5182371 -4.5219307 -4.5696878 -4.5958109 -4.5613422 -4.4884014 -4.4799728 -4.578135 -4.7106538][-4.5968738 -4.5952463 -4.6008396 -4.6095753 -4.612751 -4.6055117 -4.584631 -4.5895162 -4.623436 -4.6403136 -4.6130157 -4.5525289 -4.5394273 -4.60807 -4.6991811][-4.5470533 -4.5585303 -4.5707636 -4.5828857 -4.5913181 -4.5923915 -4.5849705 -4.5889134 -4.6041088 -4.6047268 -4.576314 -4.5204997 -4.4886646 -4.5094485 -4.5506797][-4.4487634 -4.4621344 -4.4776421 -4.4900575 -4.4978628 -4.5009437 -4.501544 -4.50593 -4.5096731 -4.5000491 -4.470048 -4.4218688 -4.3815918 -4.3691344 -4.374979]]...]
INFO - root - 2017-12-07 23:33:06.549377: step 58110, loss = 21.46, batch loss = 21.38 (8.3 examples/sec; 0.964 sec/batch; 73h:30m:36s remains)
INFO - root - 2017-12-07 23:33:16.092633: step 58120, loss = 21.01, batch loss = 20.92 (8.7 examples/sec; 0.924 sec/batch; 70h:23m:17s remains)
INFO - root - 2017-12-07 23:33:25.558418: step 58130, loss = 21.71, batch loss = 21.62 (8.5 examples/sec; 0.942 sec/batch; 71h:48m:21s remains)
INFO - root - 2017-12-07 23:33:35.049621: step 58140, loss = 21.85, batch loss = 21.76 (8.0 examples/sec; 1.004 sec/batch; 76h:30m:57s remains)
INFO - root - 2017-12-07 23:33:44.237678: step 58150, loss = 21.40, batch loss = 21.32 (8.8 examples/sec; 0.910 sec/batch; 69h:18m:46s remains)
INFO - root - 2017-12-07 23:33:53.577213: step 58160, loss = 21.27, batch loss = 21.19 (8.0 examples/sec; 0.996 sec/batch; 75h:52m:11s remains)
INFO - root - 2017-12-07 23:34:02.749147: step 58170, loss = 21.77, batch loss = 21.68 (8.9 examples/sec; 0.894 sec/batch; 68h:09m:20s remains)
INFO - root - 2017-12-07 23:34:12.291295: step 58180, loss = 21.10, batch loss = 21.01 (8.7 examples/sec; 0.915 sec/batch; 69h:43m:45s remains)
INFO - root - 2017-12-07 23:34:21.571792: step 58190, loss = 21.32, batch loss = 21.23 (8.8 examples/sec; 0.910 sec/batch; 69h:18m:46s remains)
INFO - root - 2017-12-07 23:34:31.032610: step 58200, loss = 21.17, batch loss = 21.08 (8.1 examples/sec; 0.989 sec/batch; 75h:22m:37s remains)
2017-12-07 23:34:31.910060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5362244 -4.6058335 -4.6520925 -4.6583552 -4.6454268 -4.6449666 -4.6567693 -4.6535416 -4.6387959 -4.6471295 -4.6870213 -4.7339082 -4.7558808 -4.7427359 -4.7000155][-4.5971851 -4.6618004 -4.6947637 -4.6794367 -4.6435061 -4.6324306 -4.6451983 -4.63978 -4.6177716 -4.6280384 -4.6963773 -4.7883234 -4.841073 -4.8162117 -4.7245555][-4.6048284 -4.644568 -4.6549468 -4.6196785 -4.5632181 -4.5367122 -4.5418024 -4.5348005 -4.514318 -4.5349216 -4.63447 -4.7664866 -4.8370781 -4.7836294 -4.62488][-4.5546603 -4.5586143 -4.5489659 -4.5087647 -4.4422517 -4.3976283 -4.3879662 -4.3812208 -4.3718395 -4.4092298 -4.5281754 -4.6757646 -4.7411385 -4.6513195 -4.4337449][-4.4651418 -4.4192462 -4.3867111 -4.3537469 -4.2968149 -4.2429266 -4.2177534 -4.2142434 -4.2239127 -4.2838225 -4.4115024 -4.5518718 -4.605402 -4.4962478 -4.2535172][-4.3791714 -4.2670546 -4.1971946 -4.1696897 -4.1328516 -4.07813 -4.0346966 -4.0312371 -4.0670843 -4.1608529 -4.2999396 -4.432611 -4.4859886 -4.3856039 -4.1476245][-4.3311343 -4.1606221 -4.04299 -4.0073543 -3.9896772 -3.9445734 -3.8899655 -3.8838065 -3.9410329 -4.061173 -4.2075009 -4.3400416 -4.4080873 -4.3375163 -4.1219263][-4.3168354 -4.1185865 -3.9775012 -3.9463513 -3.9590745 -3.9408092 -3.8917329 -3.8807664 -3.9371028 -4.049386 -4.1759195 -4.2983494 -4.3792377 -4.3490281 -4.183835][-4.3075533 -4.1197181 -3.9932179 -3.97978 -4.0154376 -4.0214477 -3.988831 -3.9731994 -4.0090437 -4.0894551 -4.1837707 -4.2826958 -4.361238 -4.367969 -4.275424][-4.2952728 -4.1475167 -4.0609975 -4.061327 -4.0915556 -4.0953536 -4.0728631 -4.0588541 -4.0743246 -4.126101 -4.1982045 -4.2736568 -4.3386889 -4.3697271 -4.3450384][-4.2751608 -4.1795464 -4.1470184 -4.16665 -4.1850142 -4.17694 -4.1588273 -4.1480651 -4.1480551 -4.1777172 -4.2350097 -4.2950206 -4.350081 -4.3961248 -4.4104195][-4.2698531 -4.2282305 -4.2510095 -4.3000426 -4.3192472 -4.3019943 -4.28187 -4.2700028 -4.2623491 -4.2773962 -4.3132558 -4.3555727 -4.4070096 -4.4642115 -4.4894028][-4.275188 -4.2799745 -4.3463664 -4.4226723 -4.4567504 -4.4469075 -4.4269981 -4.4151411 -4.4020529 -4.3977308 -4.3996119 -4.4173388 -4.4673615 -4.5335145 -4.5579667][-4.27649 -4.3160429 -4.3997374 -4.48275 -4.5310831 -4.5431533 -4.540349 -4.5348978 -4.5167203 -4.4860606 -4.4482212 -4.4379163 -4.4803815 -4.5483451 -4.5757761][-4.291357 -4.3470588 -4.4178839 -4.4789019 -4.5228214 -4.5527148 -4.5725489 -4.5798874 -4.5624762 -4.5174608 -4.4576283 -4.4254684 -4.4500456 -4.5048919 -4.5328269]]...]
INFO - root - 2017-12-07 23:34:41.332196: step 58210, loss = 21.73, batch loss = 21.65 (8.3 examples/sec; 0.963 sec/batch; 73h:21m:42s remains)
INFO - root - 2017-12-07 23:34:50.831941: step 58220, loss = 21.72, batch loss = 21.64 (7.8 examples/sec; 1.021 sec/batch; 77h:46m:26s remains)
INFO - root - 2017-12-07 23:35:00.164804: step 58230, loss = 21.57, batch loss = 21.48 (8.6 examples/sec; 0.930 sec/batch; 70h:52m:35s remains)
INFO - root - 2017-12-07 23:35:09.480855: step 58240, loss = 21.08, batch loss = 21.00 (8.8 examples/sec; 0.906 sec/batch; 69h:03m:04s remains)
INFO - root - 2017-12-07 23:35:18.797458: step 58250, loss = 21.26, batch loss = 21.18 (9.4 examples/sec; 0.853 sec/batch; 65h:01m:08s remains)
INFO - root - 2017-12-07 23:35:28.213303: step 58260, loss = 21.37, batch loss = 21.28 (9.0 examples/sec; 0.891 sec/batch; 67h:54m:00s remains)
INFO - root - 2017-12-07 23:35:37.677578: step 58270, loss = 21.56, batch loss = 21.48 (8.2 examples/sec; 0.970 sec/batch; 73h:53m:13s remains)
INFO - root - 2017-12-07 23:35:47.166162: step 58280, loss = 21.70, batch loss = 21.62 (8.1 examples/sec; 0.983 sec/batch; 74h:51m:45s remains)
INFO - root - 2017-12-07 23:35:56.508824: step 58290, loss = 21.60, batch loss = 21.51 (8.6 examples/sec; 0.931 sec/batch; 70h:55m:04s remains)
INFO - root - 2017-12-07 23:36:06.046907: step 58300, loss = 21.42, batch loss = 21.34 (8.7 examples/sec; 0.923 sec/batch; 70h:16m:01s remains)
2017-12-07 23:36:06.981319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5710354 -4.5818872 -4.541152 -4.4603305 -4.4182243 -4.4654918 -4.59538 -4.6940384 -4.6840181 -4.6135049 -4.546041 -4.5312738 -4.5809131 -4.6599455 -4.7155547][-4.6094618 -4.6332655 -4.6133766 -4.5612698 -4.5383244 -4.5898509 -4.7138467 -4.797163 -4.7705665 -4.6917648 -4.627285 -4.6145253 -4.65532 -4.7130113 -4.7466145][-4.6367145 -4.6845574 -4.6961517 -4.676538 -4.6605554 -4.6785488 -4.7356443 -4.7588873 -4.7071919 -4.6390491 -4.6084714 -4.6266575 -4.6755104 -4.7234149 -4.7438912][-4.6497388 -4.7109208 -4.7392936 -4.7284327 -4.6911106 -4.6434226 -4.5967865 -4.5437932 -4.4792695 -4.4471393 -4.4740067 -4.5466213 -4.6254396 -4.6828313 -4.7056942][-4.6686082 -4.7169132 -4.7197413 -4.6719642 -4.5871515 -4.4688339 -4.3291683 -4.2164025 -4.1627507 -4.1891408 -4.2856684 -4.418611 -4.5345821 -4.6129808 -4.6483922][-4.6730318 -4.6786604 -4.6208696 -4.50901 -4.3768225 -4.2130342 -4.0192332 -3.8750534 -3.8502314 -3.947443 -4.111814 -4.2925482 -4.4344873 -4.5334673 -4.5882926][-4.6420488 -4.5922804 -4.4746313 -4.3144417 -4.1551609 -3.9772773 -3.7719421 -3.6284471 -3.6464968 -3.8125379 -4.0267253 -4.2261477 -4.3695364 -4.4757991 -4.5466857][-4.5952821 -4.510951 -4.3677688 -4.1906686 -4.0261121 -3.8569453 -3.6803808 -3.5674992 -3.6232581 -3.8268909 -4.0562749 -4.2393827 -4.3608608 -4.4598737 -4.5369444][-4.58002 -4.4966025 -4.3593922 -4.1890521 -4.03355 -3.893003 -3.7708116 -3.6994767 -3.7639241 -3.952142 -4.1534662 -4.3006787 -4.4057121 -4.503839 -4.5815659][-4.5970697 -4.5414119 -4.4273515 -4.2755375 -4.1336131 -4.0277247 -3.9585736 -3.9135683 -3.9523563 -4.085207 -4.2305551 -4.3394046 -4.4480362 -4.56432 -4.6475186][-4.6067333 -4.578485 -4.4952192 -4.3747044 -4.2598429 -4.1869488 -4.1485758 -4.1054034 -4.1008191 -4.1752253 -4.271677 -4.3546143 -4.4687324 -4.5981674 -4.6855454][-4.5730252 -4.5706134 -4.531518 -4.4652605 -4.40068 -4.366909 -4.3428035 -4.2846222 -4.23829 -4.2680745 -4.3361616 -4.4018564 -4.50179 -4.6145458 -4.6879554][-4.5060997 -4.5277772 -4.5347528 -4.528182 -4.5201035 -4.5251803 -4.5135961 -4.4507446 -4.385323 -4.3863363 -4.4303427 -4.4737515 -4.5372539 -4.6071615 -4.6474705][-4.43738 -4.468925 -4.496026 -4.5182705 -4.5385423 -4.5620375 -4.5619879 -4.5134211 -4.4542794 -4.4394464 -4.4563174 -4.4728737 -4.4972372 -4.5251584 -4.5371733][-4.3745651 -4.4031734 -4.4299617 -4.4543352 -4.4740477 -4.4889879 -4.4863844 -4.4534068 -4.4106317 -4.3895559 -4.3859081 -4.3823056 -4.3811917 -4.3838854 -4.3838205]]...]
INFO - root - 2017-12-07 23:36:16.387587: step 58310, loss = 21.11, batch loss = 21.03 (8.4 examples/sec; 0.958 sec/batch; 72h:58m:11s remains)
INFO - root - 2017-12-07 23:36:25.769117: step 58320, loss = 20.89, batch loss = 20.81 (9.4 examples/sec; 0.854 sec/batch; 65h:02m:54s remains)
INFO - root - 2017-12-07 23:36:35.154279: step 58330, loss = 21.45, batch loss = 21.36 (8.8 examples/sec; 0.905 sec/batch; 68h:56m:55s remains)
INFO - root - 2017-12-07 23:36:44.469560: step 58340, loss = 21.21, batch loss = 21.13 (8.5 examples/sec; 0.937 sec/batch; 71h:21m:52s remains)
INFO - root - 2017-12-07 23:36:53.909405: step 58350, loss = 21.27, batch loss = 21.19 (8.7 examples/sec; 0.916 sec/batch; 69h:47m:26s remains)
INFO - root - 2017-12-07 23:37:03.140750: step 58360, loss = 21.32, batch loss = 21.24 (9.0 examples/sec; 0.890 sec/batch; 67h:46m:04s remains)
INFO - root - 2017-12-07 23:37:12.591880: step 58370, loss = 21.53, batch loss = 21.44 (8.4 examples/sec; 0.951 sec/batch; 72h:23m:22s remains)
INFO - root - 2017-12-07 23:37:22.071385: step 58380, loss = 21.74, batch loss = 21.65 (8.2 examples/sec; 0.971 sec/batch; 73h:54m:58s remains)
INFO - root - 2017-12-07 23:37:31.582284: step 58390, loss = 21.69, batch loss = 21.61 (8.1 examples/sec; 0.989 sec/batch; 75h:17m:36s remains)
INFO - root - 2017-12-07 23:37:40.979886: step 58400, loss = 21.77, batch loss = 21.69 (8.6 examples/sec; 0.928 sec/batch; 70h:41m:27s remains)
2017-12-07 23:37:41.908414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4082222 -4.4755049 -4.51477 -4.5070548 -4.4740167 -4.4611588 -4.5303059 -4.6368074 -4.662796 -4.5865688 -4.469893 -4.3639755 -4.2924719 -4.2658148 -4.2746944][-4.399961 -4.4833908 -4.5193024 -4.4987297 -4.4476228 -4.4142251 -4.4752731 -4.5929818 -4.63327 -4.558 -4.4299145 -4.3115726 -4.2350836 -4.2114758 -4.2261953][-4.415657 -4.505496 -4.522296 -4.4717045 -4.3929653 -4.3419552 -4.3969936 -4.526835 -4.5927515 -4.547636 -4.4486427 -4.3471541 -4.2773986 -4.2555804 -4.2704964][-4.445024 -4.5174537 -4.4974608 -4.4010973 -4.2872419 -4.2254539 -4.2874727 -4.4315963 -4.5213518 -4.512836 -4.4621263 -4.4042625 -4.3634462 -4.3567457 -4.3799167][-4.443316 -4.48806 -4.4204078 -4.2705112 -4.1171694 -4.0467162 -4.1191192 -4.2718396 -4.3691969 -4.3810511 -4.3719311 -4.3663616 -4.3736973 -4.4051495 -4.4578662][-4.4065123 -4.4418936 -4.3426733 -4.1480956 -3.952642 -3.8638961 -3.9330063 -4.078423 -4.1670995 -4.1836319 -4.2010632 -4.2393713 -4.3009648 -4.38372 -4.4791737][-4.3493648 -4.4054012 -4.3074589 -4.0921707 -3.8626654 -3.7475286 -3.8029871 -3.9357786 -4.0136976 -4.0297408 -4.057375 -4.1256804 -4.2322435 -4.35961 -4.4830976][-4.2907958 -4.370995 -4.2955713 -4.0938816 -3.8609037 -3.7323096 -3.770319 -3.8884234 -3.9610136 -3.9773607 -4.0017843 -4.0766268 -4.2029862 -4.348455 -4.4725523][-4.2713289 -4.36197 -4.3189917 -4.1628475 -3.9640236 -3.8429461 -3.8628581 -3.9588332 -4.0209312 -4.0308037 -4.0438681 -4.1130991 -4.2437987 -4.38511 -4.4876461][-4.2899828 -4.3726211 -4.3580632 -4.2620978 -4.1234078 -4.0297813 -4.0393553 -4.1130614 -4.161314 -4.163085 -4.1678023 -4.23427 -4.36394 -4.4908 -4.5600743][-4.3621187 -4.4229131 -4.4230309 -4.3767958 -4.2971964 -4.2364774 -4.240716 -4.2958632 -4.3332148 -4.3332343 -4.3374491 -4.3969693 -4.5092015 -4.6085119 -4.6432214][-4.4829617 -4.5191593 -4.5233 -4.5084867 -4.4724817 -4.438746 -4.4372883 -4.4703526 -4.49666 -4.5009875 -4.5082464 -4.5520067 -4.6277323 -4.6868219 -4.6902342][-4.5554395 -4.5728507 -4.5775385 -4.5797076 -4.5723314 -4.5608397 -4.5594449 -4.5743437 -4.5885367 -4.5946608 -4.60303 -4.6299157 -4.668623 -4.6917424 -4.6770959][-4.5504241 -4.5568128 -4.5595722 -4.56492 -4.5680065 -4.5679283 -4.5686088 -4.5743904 -4.5797877 -4.5831451 -4.5874238 -4.5992665 -4.6144643 -4.6191397 -4.6025624][-4.4735 -4.4756632 -4.476862 -4.4799204 -4.4826622 -4.4842863 -4.4852791 -4.4871006 -4.4886708 -4.4903069 -4.4928451 -4.4978209 -4.5019708 -4.4993653 -4.4857788]]...]
INFO - root - 2017-12-07 23:37:51.362476: step 58410, loss = 21.22, batch loss = 21.14 (8.3 examples/sec; 0.967 sec/batch; 73h:39m:33s remains)
INFO - root - 2017-12-07 23:38:00.807287: step 58420, loss = 21.40, batch loss = 21.32 (8.1 examples/sec; 0.990 sec/batch; 75h:20m:16s remains)
INFO - root - 2017-12-07 23:38:10.254693: step 58430, loss = 21.30, batch loss = 21.22 (8.5 examples/sec; 0.944 sec/batch; 71h:52m:34s remains)
INFO - root - 2017-12-07 23:38:19.470019: step 58440, loss = 21.50, batch loss = 21.41 (8.7 examples/sec; 0.923 sec/batch; 70h:14m:37s remains)
INFO - root - 2017-12-07 23:38:28.953434: step 58450, loss = 21.53, batch loss = 21.45 (8.3 examples/sec; 0.958 sec/batch; 72h:56m:16s remains)
INFO - root - 2017-12-07 23:38:38.357688: step 58460, loss = 21.30, batch loss = 21.22 (8.5 examples/sec; 0.941 sec/batch; 71h:39m:36s remains)
INFO - root - 2017-12-07 23:38:47.775907: step 58470, loss = 21.74, batch loss = 21.66 (8.6 examples/sec; 0.925 sec/batch; 70h:26m:49s remains)
INFO - root - 2017-12-07 23:38:57.240424: step 58480, loss = 21.60, batch loss = 21.52 (8.6 examples/sec; 0.928 sec/batch; 70h:35m:55s remains)
INFO - root - 2017-12-07 23:39:06.815000: step 58490, loss = 21.67, batch loss = 21.59 (8.6 examples/sec; 0.930 sec/batch; 70h:45m:56s remains)
INFO - root - 2017-12-07 23:39:16.311211: step 58500, loss = 21.54, batch loss = 21.46 (8.2 examples/sec; 0.971 sec/batch; 73h:52m:15s remains)
2017-12-07 23:39:17.318843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5369167 -4.5518012 -4.5645695 -4.57662 -4.5817766 -4.5868649 -4.5896659 -4.5805826 -4.566608 -4.5553122 -4.5556273 -4.5659719 -4.5779791 -4.5831337 -4.5686231][-4.626543 -4.6407776 -4.6499634 -4.6592512 -4.6565552 -4.6523018 -4.6495757 -4.6298566 -4.5992742 -4.576982 -4.5781107 -4.5979114 -4.6201406 -4.6361775 -4.6277819][-4.6783967 -4.6900306 -4.6940675 -4.6975679 -4.6771955 -4.6560888 -4.6459513 -4.6165471 -4.5774422 -4.5583243 -4.5709205 -4.6076932 -4.6468444 -4.6776824 -4.6785531][-4.6932769 -4.7047696 -4.7017393 -4.6927438 -4.64659 -4.603878 -4.5886345 -4.5551257 -4.5192032 -4.5144272 -4.5355172 -4.586165 -4.6412797 -4.6853323 -4.6984587][-4.6665769 -4.6830683 -4.6763344 -4.65496 -4.5855064 -4.5229006 -4.5003228 -4.4589949 -4.4231367 -4.4238591 -4.4475856 -4.5104804 -4.5892615 -4.652822 -4.6818652][-4.6006985 -4.62871 -4.6304622 -4.6010485 -4.5040112 -4.4051824 -4.3498878 -4.2815504 -4.2362504 -4.2387571 -4.2725258 -4.3650885 -4.4907556 -4.5905733 -4.6437325][-4.5345531 -4.5646977 -4.5671062 -4.5224948 -4.4016871 -4.2740245 -4.1884766 -4.0992794 -4.0539031 -4.0712419 -4.131938 -4.2589741 -4.4259648 -4.5522184 -4.6194868][-4.4870353 -4.5015674 -4.4893332 -4.4247236 -4.2954974 -4.1600409 -4.0596218 -3.962554 -3.9295349 -3.977231 -4.0721445 -4.2227697 -4.4005284 -4.528245 -4.5993476][-4.4938469 -4.4906287 -4.4520822 -4.3609819 -4.2278852 -4.1012659 -4.003757 -3.9125228 -3.8985169 -3.9803917 -4.1109095 -4.2782598 -4.446403 -4.5507145 -4.6006088][-4.5244226 -4.5152049 -4.4634547 -4.3660021 -4.2492933 -4.1533775 -4.0817466 -4.0141358 -4.016767 -4.1071873 -4.2378631 -4.3871388 -4.5181003 -4.5850172 -4.6099224][-4.5419765 -4.5343118 -4.4913764 -4.41586 -4.3330464 -4.2721372 -4.2257919 -4.1753917 -4.1785097 -4.2500153 -4.3556304 -4.4694958 -4.55925 -4.599761 -4.6124892][-4.5430861 -4.5393834 -4.5114131 -4.4649 -4.4202795 -4.3931746 -4.3688426 -4.333252 -4.3340859 -4.3867507 -4.4689927 -4.5519452 -4.6098585 -4.6318159 -4.6287851][-4.512064 -4.5082521 -4.4913054 -4.4702911 -4.4591107 -4.46419 -4.4671688 -4.4560704 -4.4636073 -4.5027404 -4.558949 -4.6102886 -4.6405325 -4.6442237 -4.6232395][-4.4567823 -4.4543114 -4.447094 -4.4423623 -4.4478331 -4.4644551 -4.4770131 -4.4766173 -4.4822884 -4.5046635 -4.5372324 -4.5683641 -4.5872545 -4.589211 -4.5687037][-4.4009595 -4.3998218 -4.3993487 -4.4028525 -4.4134455 -4.4314842 -4.4471912 -4.4546771 -4.460783 -4.4704361 -4.4826236 -4.4947128 -4.5033369 -4.5051174 -4.4916611]]...]
INFO - root - 2017-12-07 23:39:26.609199: step 58510, loss = 21.41, batch loss = 21.33 (8.3 examples/sec; 0.963 sec/batch; 73h:19m:40s remains)
INFO - root - 2017-12-07 23:39:36.043091: step 58520, loss = 21.43, batch loss = 21.35 (8.7 examples/sec; 0.918 sec/batch; 69h:50m:12s remains)
INFO - root - 2017-12-07 23:39:45.474157: step 58530, loss = 21.51, batch loss = 21.43 (9.0 examples/sec; 0.886 sec/batch; 67h:24m:59s remains)
INFO - root - 2017-12-07 23:39:54.939802: step 58540, loss = 21.49, batch loss = 21.40 (8.7 examples/sec; 0.916 sec/batch; 69h:41m:25s remains)
INFO - root - 2017-12-07 23:40:04.424340: step 58550, loss = 21.28, batch loss = 21.20 (8.7 examples/sec; 0.919 sec/batch; 69h:56m:41s remains)
INFO - root - 2017-12-07 23:40:13.810276: step 58560, loss = 21.14, batch loss = 21.06 (8.3 examples/sec; 0.964 sec/batch; 73h:19m:23s remains)
INFO - root - 2017-12-07 23:40:23.288777: step 58570, loss = 21.65, batch loss = 21.57 (8.1 examples/sec; 0.985 sec/batch; 74h:57m:39s remains)
INFO - root - 2017-12-07 23:40:32.719158: step 58580, loss = 21.83, batch loss = 21.75 (8.6 examples/sec; 0.927 sec/batch; 70h:34m:00s remains)
INFO - root - 2017-12-07 23:40:41.941469: step 58590, loss = 21.27, batch loss = 21.18 (8.7 examples/sec; 0.922 sec/batch; 70h:10m:06s remains)
INFO - root - 2017-12-07 23:40:51.231877: step 58600, loss = 21.28, batch loss = 21.20 (8.7 examples/sec; 0.925 sec/batch; 70h:21m:24s remains)
2017-12-07 23:40:52.240035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5827308 -4.7066483 -4.7654567 -4.7412786 -4.6685762 -4.5898218 -4.5195055 -4.4587293 -4.4349284 -4.4537821 -4.4948449 -4.5517359 -4.5992122 -4.6157789 -4.585721][-4.6530871 -4.7743244 -4.8191571 -4.7704229 -4.6598811 -4.5522175 -4.4715252 -4.4209938 -4.4182615 -4.4474235 -4.4882655 -4.5473881 -4.5982842 -4.6163096 -4.5842576][-4.6635885 -4.7799282 -4.8120079 -4.7441916 -4.5999503 -4.4577513 -4.3606505 -4.32725 -4.3609266 -4.41269 -4.4548554 -4.5094571 -4.5564084 -4.5739832 -4.5466094][-4.649786 -4.7459 -4.7516294 -4.6527338 -4.4733047 -4.2939219 -4.178544 -4.1660614 -4.2458191 -4.3297009 -4.3780251 -4.4268327 -4.4727631 -4.499516 -4.4890318][-4.59875 -4.6630826 -4.6403933 -4.5188761 -4.3242679 -4.1233859 -3.9923961 -3.9872327 -4.1011734 -4.2173433 -4.2835984 -4.3374023 -4.3927026 -4.4368424 -4.4455934][-4.5150685 -4.5498748 -4.5154223 -4.4003887 -4.2174439 -4.0060754 -3.8453593 -3.8244333 -3.9509976 -4.0982075 -4.2011852 -4.2832575 -4.3604069 -4.42047 -4.4377918][-4.4544606 -4.4687428 -4.436717 -4.3456631 -4.1845546 -3.9647055 -3.7747626 -3.7375648 -3.8721445 -4.0413008 -4.1750278 -4.2850876 -4.3799014 -4.4451246 -4.4574637][-4.3735175 -4.38481 -4.3735213 -4.3148532 -4.1837668 -3.9766543 -3.7895041 -3.7672718 -3.9143329 -4.0821924 -4.2117434 -4.3195429 -4.4094148 -4.4664493 -4.4682984][-4.2554574 -4.2781625 -4.28818 -4.2537804 -4.1545877 -3.9902675 -3.8482625 -3.8646607 -4.0208783 -4.16784 -4.2672977 -4.3527136 -4.4292841 -4.4765539 -4.4692011][-4.1742291 -4.2039676 -4.216567 -4.1838007 -4.0951347 -3.9678419 -3.880929 -3.9364662 -4.0972185 -4.2213907 -4.2987871 -4.3768167 -4.4554372 -4.502841 -4.4879665][-4.1756711 -4.1972671 -4.1918564 -4.1387005 -4.0304523 -3.9108629 -3.8608124 -3.9469635 -4.1127095 -4.2301373 -4.3100543 -4.4052691 -4.50205 -4.5562654 -4.5338788][-4.2385349 -4.2480183 -4.2212205 -4.1462035 -4.0166473 -3.8954268 -3.867146 -3.9712505 -4.1397896 -4.2611003 -4.3561454 -4.4720521 -4.5811815 -4.6339889 -4.6006937][-4.3469009 -4.3566771 -4.3260779 -4.2497897 -4.1240582 -4.0081844 -3.9859791 -4.0809917 -4.2293725 -4.3429117 -4.4404268 -4.5599451 -4.6624708 -4.7025924 -4.6594052][-4.4677172 -4.4892712 -4.4673567 -4.4076366 -4.30791 -4.2111354 -4.1860881 -4.2509942 -4.358294 -4.4464097 -4.5294704 -4.6319509 -4.7136788 -4.7361226 -4.6875086][-4.5875669 -4.6170468 -4.6057119 -4.5675097 -4.5026655 -4.4344997 -4.4109311 -4.4425664 -4.5033989 -4.5568352 -4.6125441 -4.682302 -4.7319913 -4.7326593 -4.6791945]]...]
INFO - root - 2017-12-07 23:41:01.601392: step 58610, loss = 21.69, batch loss = 21.61 (8.4 examples/sec; 0.947 sec/batch; 72h:02m:09s remains)
INFO - root - 2017-12-07 23:41:11.197064: step 58620, loss = 21.49, batch loss = 21.41 (8.2 examples/sec; 0.979 sec/batch; 74h:28m:13s remains)
INFO - root - 2017-12-07 23:41:20.597777: step 58630, loss = 21.45, batch loss = 21.36 (8.4 examples/sec; 0.952 sec/batch; 72h:23m:40s remains)
INFO - root - 2017-12-07 23:41:29.998937: step 58640, loss = 21.17, batch loss = 21.08 (8.5 examples/sec; 0.942 sec/batch; 71h:39m:02s remains)
INFO - root - 2017-12-07 23:41:39.404859: step 58650, loss = 21.42, batch loss = 21.34 (8.9 examples/sec; 0.898 sec/batch; 68h:19m:02s remains)
INFO - root - 2017-12-07 23:41:48.665797: step 58660, loss = 21.30, batch loss = 21.21 (9.3 examples/sec; 0.859 sec/batch; 65h:21m:15s remains)
INFO - root - 2017-12-07 23:41:58.162729: step 58670, loss = 21.49, batch loss = 21.41 (9.1 examples/sec; 0.884 sec/batch; 67h:13m:52s remains)
INFO - root - 2017-12-07 23:42:07.543949: step 58680, loss = 21.56, batch loss = 21.48 (8.1 examples/sec; 0.985 sec/batch; 74h:55m:09s remains)
INFO - root - 2017-12-07 23:42:16.830130: step 58690, loss = 21.25, batch loss = 21.17 (8.3 examples/sec; 0.963 sec/batch; 73h:14m:50s remains)
INFO - root - 2017-12-07 23:42:26.162295: step 58700, loss = 21.42, batch loss = 21.34 (8.7 examples/sec; 0.920 sec/batch; 69h:58m:49s remains)
2017-12-07 23:42:27.161926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3630581 -4.3499684 -4.3458414 -4.356863 -4.3745022 -4.3728342 -4.3523703 -4.3166533 -4.2877421 -4.277318 -4.2777843 -4.2869673 -4.2931952 -4.3079734 -4.3250027][-4.3367891 -4.3142753 -4.3083663 -4.3388281 -4.3807616 -4.3945847 -4.3733582 -4.326334 -4.2879386 -4.2790122 -4.2922668 -4.3164306 -4.340683 -4.3651032 -4.387157][-4.295568 -4.2626829 -4.25673 -4.3080783 -4.3741865 -4.4049358 -4.387249 -4.3347354 -4.2966685 -4.305625 -4.352416 -4.4062881 -4.4424195 -4.4603276 -4.4701295][-4.2750955 -4.2399769 -4.2341056 -4.2902536 -4.3527346 -4.3751192 -4.34441 -4.277863 -4.2339416 -4.2536983 -4.3269358 -4.4072151 -4.4540582 -4.465167 -4.4660149][-4.2786517 -4.2453103 -4.2403479 -4.2883992 -4.325716 -4.3164873 -4.2554779 -4.16929 -4.1247964 -4.154398 -4.2376876 -4.3282366 -4.3809366 -4.3898292 -4.3860936][-4.2699924 -4.2491641 -4.25152 -4.2947187 -4.3069067 -4.2589164 -4.1565118 -4.0498519 -4.0177579 -4.0728889 -4.17002 -4.2678041 -4.3232346 -4.3324933 -4.323842][-4.2732468 -4.269599 -4.276576 -4.3108339 -4.2933412 -4.1960354 -4.0400972 -3.9080498 -3.8909616 -3.9792938 -4.1012607 -4.2162309 -4.2837892 -4.3029509 -4.2948456][-4.3026705 -4.3083162 -4.3195786 -4.3433876 -4.29976 -4.1570864 -3.9542747 -3.7953012 -3.7828567 -3.8842161 -4.0181384 -4.1480546 -4.2362638 -4.2775555 -4.284234][-4.3120141 -4.3254766 -4.3505392 -4.3870826 -4.35231 -4.2127862 -4.0066857 -3.8354206 -3.8099678 -3.8930092 -4.0073452 -4.1193457 -4.1977344 -4.2396584 -4.259409][-4.2992506 -4.31269 -4.3485928 -4.4034085 -4.3992944 -4.3022361 -4.1414008 -3.9894552 -3.948684 -3.994611 -4.062675 -4.1208611 -4.1480813 -4.1582565 -4.1801524][-4.2904038 -4.2929277 -4.3269963 -4.388968 -4.4178081 -4.3730836 -4.2620535 -4.1302223 -4.077178 -4.0934272 -4.1234655 -4.1439395 -4.13749 -4.1182432 -4.1260324][-4.27028 -4.275423 -4.3164935 -4.392014 -4.4549122 -4.4518776 -4.3747549 -4.2495666 -4.179152 -4.172637 -4.1806159 -4.1938734 -4.1950774 -4.1724777 -4.1616082][-4.2445207 -4.280757 -4.3398972 -4.4210849 -4.4927855 -4.50701 -4.449326 -4.3290977 -4.2481856 -4.2259421 -4.2242942 -4.2498116 -4.2771292 -4.2669468 -4.2459731][-4.2342658 -4.292645 -4.3656836 -4.4478674 -4.5122819 -4.5283952 -4.4862828 -4.3802481 -4.2968645 -4.261796 -4.2601171 -4.3085423 -4.3646903 -4.36775 -4.3364873][-4.2451487 -4.3072019 -4.3877945 -4.4717293 -4.5273066 -4.5470161 -4.5316772 -4.4610043 -4.3857317 -4.3376951 -4.3276415 -4.3819566 -4.44951 -4.4535174 -4.4102707]]...]
INFO - root - 2017-12-07 23:42:36.796392: step 58710, loss = 21.47, batch loss = 21.39 (7.9 examples/sec; 1.018 sec/batch; 77h:24m:40s remains)
INFO - root - 2017-12-07 23:42:46.222979: step 58720, loss = 21.56, batch loss = 21.48 (8.1 examples/sec; 0.988 sec/batch; 75h:09m:06s remains)
INFO - root - 2017-12-07 23:42:55.555067: step 58730, loss = 21.26, batch loss = 21.18 (8.3 examples/sec; 0.964 sec/batch; 73h:18m:09s remains)
INFO - root - 2017-12-07 23:43:05.210958: step 58740, loss = 21.25, batch loss = 21.17 (8.5 examples/sec; 0.939 sec/batch; 71h:24m:26s remains)
INFO - root - 2017-12-07 23:43:14.679813: step 58750, loss = 21.37, batch loss = 21.29 (8.6 examples/sec; 0.925 sec/batch; 70h:20m:36s remains)
INFO - root - 2017-12-07 23:43:24.163923: step 58760, loss = 21.58, batch loss = 21.50 (7.9 examples/sec; 1.015 sec/batch; 77h:08m:44s remains)
INFO - root - 2017-12-07 23:43:33.606141: step 58770, loss = 20.92, batch loss = 20.84 (8.4 examples/sec; 0.955 sec/batch; 72h:34m:50s remains)
INFO - root - 2017-12-07 23:43:43.158912: step 58780, loss = 21.13, batch loss = 21.05 (8.8 examples/sec; 0.909 sec/batch; 69h:08m:05s remains)
INFO - root - 2017-12-07 23:43:52.550861: step 58790, loss = 21.51, batch loss = 21.42 (9.1 examples/sec; 0.877 sec/batch; 66h:40m:48s remains)
INFO - root - 2017-12-07 23:44:02.018739: step 58800, loss = 21.93, batch loss = 21.85 (8.6 examples/sec; 0.926 sec/batch; 70h:25m:41s remains)
2017-12-07 23:44:03.017383: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6835613 -4.7067766 -4.7403741 -4.7681503 -4.7846 -4.7890387 -4.787611 -4.7832966 -4.7663727 -4.7131515 -4.644062 -4.6114635 -4.6258321 -4.6541228 -4.6810961][-4.6844139 -4.6970406 -4.7261152 -4.7579203 -4.7865453 -4.8064952 -4.8189569 -4.827105 -4.8162432 -4.7527313 -4.6664419 -4.6282239 -4.6527596 -4.7025194 -4.7436814][-4.6876335 -4.6803541 -4.6950612 -4.7243176 -4.7618904 -4.7959495 -4.8256097 -4.8541231 -4.8626304 -4.8022194 -4.705904 -4.6633945 -4.6987162 -4.7678552 -4.8153543][-4.6513467 -4.621336 -4.6189113 -4.6372614 -4.6674147 -4.6979251 -4.7315922 -4.7782688 -4.8132858 -4.7755184 -4.69113 -4.6544666 -4.699461 -4.7826271 -4.8347578][-4.5501366 -4.5060987 -4.4888391 -4.48768 -4.4925995 -4.4959683 -4.5103712 -4.5606003 -4.6258821 -4.6364355 -4.5966368 -4.5826254 -4.6315675 -4.7140517 -4.7684488][-4.4004164 -4.353261 -4.3295765 -4.3166924 -4.2951803 -4.2543468 -4.2208176 -4.2504706 -4.3403287 -4.4124513 -4.439568 -4.4664683 -4.5229454 -4.5997663 -4.6604185][-4.2288837 -4.1890979 -4.1741943 -4.1681333 -4.1321578 -4.0396061 -3.9397049 -3.9272947 -4.0279417 -4.1509724 -4.23763 -4.3044686 -4.3787045 -4.4653773 -4.545867][-4.0855613 -4.0516419 -4.045867 -4.0540743 -4.0213957 -3.9003837 -3.7543635 -3.703999 -3.8043427 -3.95436 -4.0730162 -4.1598339 -4.2415504 -4.3408346 -4.447433][-4.0221734 -3.9912038 -3.9883363 -4.0137234 -4.0011497 -3.8925943 -3.750139 -3.679038 -3.7552705 -3.8910389 -4.0055413 -4.0932541 -4.17372 -4.277082 -4.3982553][-4.0612164 -4.0399933 -4.0427675 -4.0836363 -4.1016221 -4.0379753 -3.9371204 -3.8658376 -3.8966396 -3.9856658 -4.0743208 -4.1553569 -4.2308545 -4.3237562 -4.429338][-4.1871686 -4.1750903 -4.1817608 -4.2309451 -4.2736826 -4.2560458 -4.1995416 -4.1375842 -4.1336312 -4.1801071 -4.2410932 -4.3084106 -4.3701415 -4.4393559 -4.5125346][-4.3772111 -4.372046 -4.3745775 -4.4151831 -4.463532 -4.4725809 -4.4451909 -4.3962021 -4.3747129 -4.3901763 -4.4249945 -4.4706535 -4.5084238 -4.5490389 -4.593245][-4.5343919 -4.5317621 -4.5233369 -4.547071 -4.5907927 -4.6131845 -4.6069565 -4.5787177 -4.555861 -4.5459795 -4.5517559 -4.5732908 -4.5909448 -4.6150923 -4.6499062][-4.5944562 -4.5829167 -4.5660262 -4.5794225 -4.62017 -4.6513662 -4.6623378 -4.6556406 -4.6368275 -4.6038404 -4.5811329 -4.5856853 -4.6006689 -4.631875 -4.6760979][-4.5877357 -4.5647817 -4.5434318 -4.5536227 -4.5902538 -4.6185141 -4.6324625 -4.6364779 -4.6216717 -4.5769176 -4.5361719 -4.5323534 -4.5582957 -4.6086779 -4.6677113]]...]
INFO - root - 2017-12-07 23:44:12.568443: step 58810, loss = 21.49, batch loss = 21.40 (8.7 examples/sec; 0.915 sec/batch; 69h:32m:37s remains)
INFO - root - 2017-12-07 23:44:22.001056: step 58820, loss = 21.29, batch loss = 21.21 (8.7 examples/sec; 0.918 sec/batch; 69h:48m:26s remains)
INFO - root - 2017-12-07 23:44:31.393280: step 58830, loss = 21.60, batch loss = 21.52 (8.5 examples/sec; 0.946 sec/batch; 71h:56m:23s remains)
INFO - root - 2017-12-07 23:44:40.812826: step 58840, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.945 sec/batch; 71h:50m:14s remains)
INFO - root - 2017-12-07 23:44:50.348364: step 58850, loss = 21.54, batch loss = 21.45 (8.0 examples/sec; 1.001 sec/batch; 76h:04m:18s remains)
INFO - root - 2017-12-07 23:44:59.495706: step 58860, loss = 21.43, batch loss = 21.34 (8.0 examples/sec; 0.996 sec/batch; 75h:41m:04s remains)
INFO - root - 2017-12-07 23:45:08.828430: step 58870, loss = 21.54, batch loss = 21.46 (8.8 examples/sec; 0.913 sec/batch; 69h:22m:59s remains)
INFO - root - 2017-12-07 23:45:18.202782: step 58880, loss = 21.42, batch loss = 21.34 (8.6 examples/sec; 0.934 sec/batch; 70h:57m:05s remains)
INFO - root - 2017-12-07 23:45:27.608382: step 58890, loss = 21.55, batch loss = 21.47 (8.6 examples/sec; 0.931 sec/batch; 70h:46m:59s remains)
INFO - root - 2017-12-07 23:45:37.006772: step 58900, loss = 21.37, batch loss = 21.29 (8.1 examples/sec; 0.983 sec/batch; 74h:40m:19s remains)
2017-12-07 23:45:38.019336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.327209 -4.3392467 -4.4066362 -4.4676242 -4.4399419 -4.3415332 -4.2574849 -4.2369208 -4.2596126 -4.3237982 -4.4051108 -4.4462423 -4.4451661 -4.4500136 -4.4813514][-4.332737 -4.3447456 -4.42404 -4.5192022 -4.5102053 -4.3842144 -4.2530084 -4.1971769 -4.1918521 -4.2218504 -4.2685442 -4.2880573 -4.2871642 -4.3197346 -4.4004221][-4.3742256 -4.37579 -4.4503293 -4.5605745 -4.5629511 -4.4187975 -4.2554789 -4.1758356 -4.1537223 -4.1601129 -4.1753893 -4.1634169 -4.1479096 -4.1971717 -4.3157296][-4.3665609 -4.3718472 -4.4459767 -4.5572205 -4.5664434 -4.4285111 -4.2694626 -4.1884909 -4.159965 -4.1521854 -4.1511011 -4.1250253 -4.1065607 -4.1689448 -4.3064814][-4.2876711 -4.3140616 -4.3989682 -4.4995103 -4.5015216 -4.3778558 -4.2462521 -4.1853256 -4.1683025 -4.1658492 -4.167316 -4.1467657 -4.1375675 -4.2046962 -4.3329277][-4.1962538 -4.2601914 -4.3659673 -4.4487314 -4.4206247 -4.2842741 -4.1565733 -4.1088066 -4.1248665 -4.1705422 -4.2154436 -4.2324142 -4.2432733 -4.3017058 -4.3945456][-4.1740127 -4.277576 -4.3940725 -4.4487262 -4.3662009 -4.1727653 -3.9964108 -3.9367442 -4.001205 -4.1351013 -4.2640367 -4.3441238 -4.3856058 -4.434103 -4.4927511][-4.2180624 -4.3453374 -4.459096 -4.4852777 -4.3439107 -4.0675 -3.8065467 -3.7102795 -3.8209321 -4.0560536 -4.2766762 -4.4113493 -4.4707084 -4.510438 -4.5555358][-4.2788439 -4.3986673 -4.496973 -4.5095024 -4.3433027 -4.0217485 -3.7055368 -3.5754523 -3.7087169 -4.0111036 -4.2863975 -4.4367127 -4.484396 -4.5043888 -4.536232][-4.3754363 -4.4600906 -4.5259118 -4.5290751 -4.376112 -4.0771251 -3.7732394 -3.6364148 -3.7593889 -4.0643697 -4.3406105 -4.4762273 -4.4957447 -4.478652 -4.4666371][-4.5225096 -4.567801 -4.5985818 -4.5915961 -4.4730787 -4.2418065 -3.9983063 -3.8738444 -3.9584422 -4.2085118 -4.4416995 -4.5486526 -4.5426645 -4.4840322 -4.4093471][-4.6451187 -4.6666179 -4.6796212 -4.6738009 -4.6018496 -4.4542642 -4.2862449 -4.1844506 -4.2240729 -4.3953223 -4.5623827 -4.6340547 -4.6095161 -4.5208597 -4.3983965][-4.6686244 -4.6869459 -4.6980305 -4.6946774 -4.6562514 -4.5769167 -4.4787617 -4.40938 -4.4255052 -4.5321922 -4.6378875 -4.6769829 -4.6386261 -4.5362473 -4.3990383][-4.576817 -4.6028886 -4.6192703 -4.6176176 -4.5956106 -4.5559363 -4.5058317 -4.4674954 -4.4790587 -4.5457711 -4.610301 -4.6285267 -4.5865769 -4.4918423 -4.3711276][-4.4296269 -4.4568815 -4.4806237 -4.4876809 -4.4803371 -4.4643035 -4.4430642 -4.4273777 -4.4379764 -4.4755297 -4.5096083 -4.5157671 -4.4807339 -4.408987 -4.3200064]]...]
INFO - root - 2017-12-07 23:45:47.406689: step 58910, loss = 21.33, batch loss = 21.25 (8.4 examples/sec; 0.954 sec/batch; 72h:31m:10s remains)
INFO - root - 2017-12-07 23:45:56.918301: step 58920, loss = 21.41, batch loss = 21.32 (8.2 examples/sec; 0.971 sec/batch; 73h:45m:53s remains)
INFO - root - 2017-12-07 23:46:06.316469: step 58930, loss = 21.66, batch loss = 21.58 (8.0 examples/sec; 0.997 sec/batch; 75h:47m:48s remains)
INFO - root - 2017-12-07 23:46:15.590857: step 58940, loss = 21.28, batch loss = 21.20 (8.4 examples/sec; 0.953 sec/batch; 72h:25m:22s remains)
INFO - root - 2017-12-07 23:46:25.076099: step 58950, loss = 21.03, batch loss = 20.95 (8.3 examples/sec; 0.962 sec/batch; 73h:06m:43s remains)
INFO - root - 2017-12-07 23:46:34.500168: step 58960, loss = 20.87, batch loss = 20.78 (8.3 examples/sec; 0.961 sec/batch; 73h:02m:04s remains)
INFO - root - 2017-12-07 23:46:43.947490: step 58970, loss = 21.24, batch loss = 21.16 (8.1 examples/sec; 0.990 sec/batch; 75h:11m:29s remains)
INFO - root - 2017-12-07 23:46:53.293815: step 58980, loss = 21.33, batch loss = 21.25 (8.2 examples/sec; 0.972 sec/batch; 73h:52m:18s remains)
INFO - root - 2017-12-07 23:47:02.769298: step 58990, loss = 21.36, batch loss = 21.28 (9.6 examples/sec; 0.834 sec/batch; 63h:21m:57s remains)
INFO - root - 2017-12-07 23:47:12.107578: step 59000, loss = 21.43, batch loss = 21.35 (9.4 examples/sec; 0.851 sec/batch; 64h:37m:38s remains)
2017-12-07 23:47:13.256936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2016854 -4.2121658 -4.2634559 -4.3237977 -4.3346539 -4.3209767 -4.3327513 -4.3726711 -4.3982725 -4.4003458 -4.3597093 -4.2582984 -4.1715331 -4.1141481 -4.1251836][-4.2176375 -4.2356415 -4.2897038 -4.3606563 -4.3795648 -4.3525982 -4.3444605 -4.3721781 -4.39268 -4.3973646 -4.3599372 -4.249217 -4.1598969 -4.0986242 -4.1081529][-4.2236457 -4.252614 -4.30764 -4.3797355 -4.4020729 -4.3616309 -4.3295984 -4.3328443 -4.336956 -4.3394451 -4.3029852 -4.1973443 -4.1277213 -4.0730438 -4.0709643][-4.2140193 -4.2469168 -4.2970552 -4.36218 -4.386117 -4.3386607 -4.2864437 -4.2586422 -4.2383018 -4.2333345 -4.2005363 -4.1190348 -4.0896125 -4.0603123 -4.0494947][-4.1879296 -4.2153873 -4.257021 -4.3094311 -4.3307996 -4.2837815 -4.2188768 -4.165164 -4.12623 -4.1189604 -4.1019597 -4.06274 -4.0754132 -4.0761075 -4.0655961][-4.1646295 -4.1774559 -4.2017193 -4.2289658 -4.2340989 -4.1872239 -4.1193519 -4.0559964 -4.0194988 -4.0215535 -4.0306039 -4.0381346 -4.0853662 -4.1178761 -4.1240439][-4.1592197 -4.1461706 -4.1433606 -4.1351852 -4.1148295 -4.0690012 -4.0114784 -3.9617925 -3.9502726 -3.9742739 -4.0145268 -4.0583849 -4.1229029 -4.1775208 -4.2051349][-4.170764 -4.1282706 -4.0956984 -4.0585518 -4.019897 -3.9802752 -3.9428782 -3.9160316 -3.9324224 -3.9827995 -4.0501165 -4.1129074 -4.178894 -4.2439842 -4.28736][-4.1974239 -4.1318469 -4.076148 -4.0292673 -3.9898562 -3.9652593 -3.954417 -3.9470696 -3.974112 -4.0373611 -4.1152186 -4.1763005 -4.2278953 -4.2868848 -4.3318381][-4.2267394 -4.1451893 -4.0774269 -4.0398369 -4.0160069 -4.0130424 -4.0309196 -4.0383925 -4.0626283 -4.1206827 -4.184864 -4.2220364 -4.2453823 -4.2791944 -4.3099856][-4.265398 -4.1792688 -4.1038513 -4.0743909 -4.0654473 -4.0828314 -4.125515 -4.1454639 -4.1624746 -4.2001863 -4.2320261 -4.2342997 -4.2233443 -4.2233386 -4.2345085][-4.3229079 -4.235672 -4.1487818 -4.1066561 -4.0947857 -4.1249566 -4.1857319 -4.2186565 -4.230196 -4.2410049 -4.2351031 -4.2058353 -4.1691709 -4.1443949 -4.1453576][-4.3639688 -4.27512 -4.180038 -4.1151586 -4.0895767 -4.1232376 -4.1913548 -4.2327089 -4.2428021 -4.232347 -4.1982956 -4.1485929 -4.0975642 -4.0653291 -4.0699468][-4.3747869 -4.2887845 -4.190618 -4.1090846 -4.0737772 -4.1029654 -4.1684408 -4.2111907 -4.2210264 -4.2005739 -4.1546926 -4.09774 -4.0406432 -4.0115151 -4.0252547][-4.358067 -4.2826262 -4.18859 -4.1038346 -4.0682826 -4.0952954 -4.1585541 -4.2015605 -4.208807 -4.1788111 -4.1240263 -4.0648012 -4.0063539 -3.9906898 -4.0197191]]...]
INFO - root - 2017-12-07 23:47:22.677892: step 59010, loss = 21.69, batch loss = 21.61 (8.6 examples/sec; 0.930 sec/batch; 70h:40m:06s remains)
INFO - root - 2017-12-07 23:47:32.158388: step 59020, loss = 21.65, batch loss = 21.57 (8.4 examples/sec; 0.948 sec/batch; 72h:01m:25s remains)
INFO - root - 2017-12-07 23:47:41.662577: step 59030, loss = 21.45, batch loss = 21.36 (8.4 examples/sec; 0.957 sec/batch; 72h:41m:05s remains)
INFO - root - 2017-12-07 23:47:50.955322: step 59040, loss = 21.22, batch loss = 21.14 (8.4 examples/sec; 0.950 sec/batch; 72h:09m:57s remains)
INFO - root - 2017-12-07 23:48:00.506122: step 59050, loss = 21.22, batch loss = 21.13 (8.1 examples/sec; 0.982 sec/batch; 74h:37m:31s remains)
INFO - root - 2017-12-07 23:48:09.908942: step 59060, loss = 21.41, batch loss = 21.33 (8.5 examples/sec; 0.943 sec/batch; 71h:35m:27s remains)
INFO - root - 2017-12-07 23:48:19.135942: step 59070, loss = 21.29, batch loss = 21.21 (9.4 examples/sec; 0.849 sec/batch; 64h:27m:00s remains)
INFO - root - 2017-12-07 23:48:28.622933: step 59080, loss = 21.34, batch loss = 21.26 (8.7 examples/sec; 0.923 sec/batch; 70h:05m:13s remains)
INFO - root - 2017-12-07 23:48:37.931090: step 59090, loss = 21.39, batch loss = 21.31 (8.6 examples/sec; 0.926 sec/batch; 70h:21m:23s remains)
INFO - root - 2017-12-07 23:48:47.398938: step 59100, loss = 21.73, batch loss = 21.65 (8.4 examples/sec; 0.950 sec/batch; 72h:06m:59s remains)
2017-12-07 23:48:48.355169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4088516 -4.4264827 -4.456738 -4.4769888 -4.4816914 -4.4772358 -4.4773498 -4.4887476 -4.5056205 -4.5201454 -4.5240083 -4.510633 -4.4815679 -4.4446979 -4.4119515][-4.4651837 -4.4918156 -4.5238638 -4.5379081 -4.5260715 -4.5022779 -4.488749 -4.4955077 -4.5239625 -4.5609937 -4.5914941 -4.601027 -4.5765266 -4.5242743 -4.4638062][-4.517395 -4.539866 -4.5626788 -4.5653214 -4.5376778 -4.4940419 -4.4601245 -4.4528379 -4.4852805 -4.5385318 -4.5952764 -4.6383576 -4.6379189 -4.5919647 -4.5200462][-4.5190535 -4.5358715 -4.5491772 -4.5384722 -4.4879966 -4.4116082 -4.3443975 -4.3182993 -4.3590941 -4.4322062 -4.5160155 -4.5979056 -4.6322665 -4.6118841 -4.5514107][-4.4605603 -4.4845624 -4.4920263 -4.4564948 -4.3580403 -4.2218533 -4.1053219 -4.0607176 -4.1241856 -4.2369633 -4.3635926 -4.4903135 -4.5607505 -4.5721531 -4.5372515][-4.389812 -4.4425206 -4.4526157 -4.3820128 -4.2146134 -3.9984527 -3.8162923 -3.7440507 -3.8299348 -3.9956458 -4.1815181 -4.3601537 -4.4662867 -4.5038075 -4.4919186][-4.3452182 -4.4483504 -4.4855428 -4.3986535 -4.1764402 -3.8909042 -3.6475079 -3.5411234 -3.629437 -3.828136 -4.0589285 -4.2753382 -4.4052634 -4.4530916 -4.4475622][-4.3158727 -4.4743004 -4.5605803 -4.4953623 -4.2685428 -3.9633026 -3.697221 -3.5746324 -3.6481669 -3.841795 -4.0731258 -4.2843351 -4.4079766 -4.444356 -4.4261808][-4.2868075 -4.4729128 -4.5963936 -4.573504 -4.3948479 -4.135077 -3.903563 -3.796319 -3.8541365 -4.0176144 -4.20853 -4.3704 -4.4536409 -4.4580483 -4.4160752][-4.2891684 -4.4584336 -4.5846205 -4.5949016 -4.4823809 -4.3023529 -4.1391873 -4.0635133 -4.1058311 -4.2266512 -4.3586369 -4.4552946 -4.4869485 -4.4573646 -4.3968554][-4.3240685 -4.4571309 -4.5638418 -4.5924659 -4.5360169 -4.4294314 -4.3258471 -4.26951 -4.2858248 -4.35711 -4.4345007 -4.4817891 -4.4790697 -4.4325423 -4.3687282][-4.3483582 -4.4484434 -4.5406785 -4.5845752 -4.5709968 -4.5145388 -4.4434566 -4.3872051 -4.3705792 -4.3954406 -4.4316945 -4.4497695 -4.4325747 -4.384922 -4.3296432][-4.3288779 -4.4086919 -4.4953117 -4.5515985 -4.5633669 -4.5348363 -4.479 -4.4185982 -4.3757215 -4.3633056 -4.3669314 -4.3658071 -4.3441248 -4.3036709 -4.2625909][-4.275219 -4.340939 -4.4168134 -4.4686208 -4.4815392 -4.460393 -4.41294 -4.3560662 -4.306242 -4.2764864 -4.2623172 -4.2525864 -4.2352824 -4.2087235 -4.1846938][-4.2297277 -4.2751961 -4.3314137 -4.3659592 -4.3656173 -4.339407 -4.296176 -4.2497678 -4.2100477 -4.1845989 -4.172029 -4.1665983 -4.1596065 -4.1478419 -4.13868]]...]
INFO - root - 2017-12-07 23:48:57.805816: step 59110, loss = 21.59, batch loss = 21.51 (8.8 examples/sec; 0.905 sec/batch; 68h:45m:47s remains)
INFO - root - 2017-12-07 23:49:07.192347: step 59120, loss = 22.18, batch loss = 22.10 (8.2 examples/sec; 0.974 sec/batch; 73h:59m:07s remains)
INFO - root - 2017-12-07 23:49:16.604750: step 59130, loss = 21.57, batch loss = 21.48 (8.4 examples/sec; 0.949 sec/batch; 72h:03m:48s remains)
INFO - root - 2017-12-07 23:49:26.065623: step 59140, loss = 21.35, batch loss = 21.26 (8.7 examples/sec; 0.915 sec/batch; 69h:26m:53s remains)
INFO - root - 2017-12-07 23:49:35.458035: step 59150, loss = 21.45, batch loss = 21.36 (8.8 examples/sec; 0.914 sec/batch; 69h:25m:03s remains)
INFO - root - 2017-12-07 23:49:44.768382: step 59160, loss = 21.61, batch loss = 21.53 (8.9 examples/sec; 0.902 sec/batch; 68h:27m:25s remains)
INFO - root - 2017-12-07 23:49:54.092779: step 59170, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.929 sec/batch; 70h:33m:38s remains)
INFO - root - 2017-12-07 23:50:03.505592: step 59180, loss = 21.23, batch loss = 21.15 (8.1 examples/sec; 0.992 sec/batch; 75h:18m:44s remains)
INFO - root - 2017-12-07 23:50:12.957489: step 59190, loss = 21.08, batch loss = 21.00 (8.1 examples/sec; 0.991 sec/batch; 75h:12m:32s remains)
INFO - root - 2017-12-07 23:50:22.199245: step 59200, loss = 21.43, batch loss = 21.35 (8.0 examples/sec; 1.000 sec/batch; 75h:53m:10s remains)
2017-12-07 23:50:23.172856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3863535 -4.3755779 -4.3309469 -4.2609005 -4.2053814 -4.2187357 -4.2986097 -4.3756218 -4.4175272 -4.4294105 -4.4356747 -4.4403758 -4.4007864 -4.3177691 -4.2695951][-4.4017444 -4.4002743 -4.3531628 -4.2729144 -4.2025495 -4.2030549 -4.27584 -4.3456697 -4.391994 -4.41921 -4.4508834 -4.4765539 -4.4309034 -4.3183784 -4.2455778][-4.4132876 -4.4341784 -4.4091043 -4.3479514 -4.278749 -4.2468653 -4.26695 -4.2955885 -4.3403921 -4.3964539 -4.4659534 -4.5203848 -4.4821939 -4.3535562 -4.2438231][-4.4222732 -4.4801283 -4.499753 -4.4720945 -4.3956313 -4.2994528 -4.2293158 -4.2002831 -4.2512794 -4.3497891 -4.4581432 -4.5379605 -4.5186009 -4.3990483 -4.2658615][-4.4067392 -4.5122938 -4.5816674 -4.5726719 -4.4600816 -4.2777677 -4.1106839 -4.0334949 -4.0995765 -4.2371855 -4.3702011 -4.4616051 -4.4689016 -4.393662 -4.2825441][-4.3502855 -4.4977622 -4.5978026 -4.576386 -4.4070592 -4.145267 -3.9069855 -3.8032684 -3.8897858 -4.06032 -4.211813 -4.3135128 -4.3564763 -4.3507023 -4.3039832][-4.2777696 -4.4491673 -4.5565329 -4.5045185 -4.2837763 -3.9710054 -3.6919336 -3.57721 -3.6857176 -3.8916745 -4.0745811 -4.2002187 -4.2803888 -4.3369541 -4.3537483][-4.2120781 -4.3827853 -4.480866 -4.4075561 -4.1684422 -3.8476717 -3.5608199 -3.4457107 -3.568717 -3.8031201 -4.0213537 -4.1752505 -4.2779665 -4.3583961 -4.3991075][-4.2048283 -4.3476205 -4.4215107 -4.3370304 -4.1099415 -3.8182998 -3.5572972 -3.4558637 -3.5815556 -3.8210144 -4.0541787 -4.2248416 -4.3348622 -4.405623 -4.4321437][-4.2780213 -4.3821397 -4.4242973 -4.3352737 -4.1329646 -3.8872423 -3.675842 -3.6046789 -3.7224767 -3.9337473 -4.1419711 -4.3002739 -4.4020791 -4.4511695 -4.4540024][-4.369154 -4.4396186 -4.4599261 -4.3818016 -4.2138977 -4.0172205 -3.8607187 -3.8205576 -3.9179296 -4.0737319 -4.2201257 -4.3342452 -4.415112 -4.4510965 -4.4460196][-4.4479203 -4.4968019 -4.50714 -4.4451728 -4.3120284 -4.1592288 -4.0509715 -4.0360022 -4.1069217 -4.1974392 -4.268364 -4.3271966 -4.3832951 -4.4151769 -4.4155364][-4.5221748 -4.5565486 -4.5566769 -4.5026579 -4.3965626 -4.281394 -4.2095709 -4.2059445 -4.2445416 -4.2700005 -4.2671652 -4.26785 -4.29226 -4.3210764 -4.3382215][-4.5643983 -4.5786777 -4.5620246 -4.5115867 -4.4339595 -4.3541026 -4.3043723 -4.2918139 -4.2848544 -4.2459593 -4.1789527 -4.1303191 -4.1306143 -4.1655931 -4.2167907][-4.5661235 -4.5631924 -4.5293903 -4.4737787 -4.4094329 -4.3472557 -4.3016891 -4.2688661 -4.2178512 -4.1306643 -4.0353823 -3.9844332 -4.0030203 -4.0690422 -4.1613183]]...]
INFO - root - 2017-12-07 23:50:32.565989: step 59210, loss = 21.47, batch loss = 21.38 (8.2 examples/sec; 0.970 sec/batch; 73h:38m:40s remains)
INFO - root - 2017-12-07 23:50:42.107470: step 59220, loss = 21.14, batch loss = 21.06 (9.2 examples/sec; 0.874 sec/batch; 66h:21m:02s remains)
INFO - root - 2017-12-07 23:50:51.603225: step 59230, loss = 21.26, batch loss = 21.18 (8.6 examples/sec; 0.929 sec/batch; 70h:33m:05s remains)
INFO - root - 2017-12-07 23:51:01.117111: step 59240, loss = 21.47, batch loss = 21.39 (8.8 examples/sec; 0.914 sec/batch; 69h:22m:44s remains)
INFO - root - 2017-12-07 23:51:10.583925: step 59250, loss = 21.56, batch loss = 21.48 (8.4 examples/sec; 0.952 sec/batch; 72h:14m:35s remains)
INFO - root - 2017-12-07 23:51:20.070293: step 59260, loss = 21.33, batch loss = 21.24 (8.2 examples/sec; 0.973 sec/batch; 73h:52m:51s remains)
INFO - root - 2017-12-07 23:51:29.452173: step 59270, loss = 21.99, batch loss = 21.90 (8.4 examples/sec; 0.954 sec/batch; 72h:26m:28s remains)
INFO - root - 2017-12-07 23:51:38.877010: step 59280, loss = 21.42, batch loss = 21.34 (8.9 examples/sec; 0.897 sec/batch; 68h:04m:12s remains)
INFO - root - 2017-12-07 23:51:48.326040: step 59290, loss = 21.47, batch loss = 21.39 (8.6 examples/sec; 0.935 sec/batch; 70h:55m:36s remains)
INFO - root - 2017-12-07 23:51:57.889366: step 59300, loss = 21.47, batch loss = 21.39 (8.3 examples/sec; 0.968 sec/batch; 73h:27m:02s remains)
2017-12-07 23:51:58.949142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3079977 -4.35812 -4.3588133 -4.354826 -4.3873653 -4.4588938 -4.5386534 -4.603303 -4.6390667 -4.6232424 -4.5950518 -4.6023178 -4.6412916 -4.6668653 -4.6401234][-4.3523307 -4.3717341 -4.3555279 -4.3671236 -4.4429216 -4.5648842 -4.6674809 -4.7116075 -4.6986718 -4.6311073 -4.5731072 -4.5882235 -4.66328 -4.7221742 -4.6988921][-4.4255724 -4.4381952 -4.4254208 -4.4484768 -4.5249805 -4.6195455 -4.665072 -4.6374254 -4.5695686 -4.4827628 -4.4344964 -4.4760981 -4.587182 -4.6796989 -4.674737][-4.5340862 -4.5600882 -4.5605049 -4.5776186 -4.6001472 -4.5879278 -4.5026112 -4.3778448 -4.2886949 -4.251112 -4.2611189 -4.3292136 -4.4439178 -4.5454946 -4.5668297][-4.6502056 -4.6875911 -4.6882877 -4.6758556 -4.6158137 -4.4674411 -4.2332425 -4.0240264 -3.9588645 -4.0285606 -4.1292424 -4.2171574 -4.3054051 -4.3870964 -4.4271741][-4.7172976 -4.7385902 -4.7152166 -4.6596203 -4.529882 -4.28039 -3.9376125 -3.6739478 -3.6556108 -3.8465858 -4.0494733 -4.1676083 -4.23272 -4.2840376 -4.3232961][-4.7011218 -4.6901364 -4.6278148 -4.5316968 -4.3685308 -4.08436 -3.7010527 -3.4262848 -3.455862 -3.7372451 -4.0231318 -4.1827083 -4.2516308 -4.2814155 -4.2983861][-4.6135144 -4.5802641 -4.4961438 -4.3903003 -4.24138 -3.9860644 -3.6325643 -3.3860693 -3.4348705 -3.730237 -4.0400505 -4.233182 -4.3333154 -4.3696561 -4.3658652][-4.5099211 -4.4822488 -4.406662 -4.3235869 -4.2193336 -4.0279894 -3.7488549 -3.5524831 -3.5917397 -3.8356359 -4.1113696 -4.3159022 -4.4478736 -4.5018559 -4.4841666][-4.4521861 -4.4442821 -4.3867588 -4.3311086 -4.2737374 -4.1495533 -3.9595845 -3.8276558 -3.8627784 -4.0482435 -4.2739816 -4.467731 -4.6014223 -4.6440187 -4.5951562][-4.4492345 -4.4579468 -4.4126883 -4.3766494 -4.3560324 -4.2928104 -4.1888814 -4.1250734 -4.1721673 -4.3151188 -4.4883981 -4.6458216 -4.7421679 -4.7375913 -4.6416588][-4.48555 -4.5011697 -4.458519 -4.4294543 -4.4321008 -4.4215212 -4.3872037 -4.3748875 -4.4289603 -4.5335131 -4.6518803 -4.7555561 -4.7931757 -4.7306037 -4.5937009][-4.5291109 -4.5442796 -4.5002232 -4.4710159 -4.48548 -4.5154753 -4.5344796 -4.5510955 -4.5918183 -4.65041 -4.7088761 -4.7479591 -4.723002 -4.6130142 -4.4622421][-4.5659924 -4.5811768 -4.5415168 -4.5091233 -4.5196195 -4.5641508 -4.6069784 -4.6260605 -4.6369529 -4.6477156 -4.6518764 -4.6353388 -4.5641465 -4.43567 -4.3093486][-4.5907445 -4.6102233 -4.5789671 -4.5379424 -4.5274229 -4.5533233 -4.5841947 -4.5860248 -4.5680976 -4.5499253 -4.529129 -4.4923716 -4.4122524 -4.2955112 -4.2088985]]...]
INFO - root - 2017-12-07 23:52:08.353987: step 59310, loss = 21.22, batch loss = 21.14 (8.4 examples/sec; 0.957 sec/batch; 72h:36m:38s remains)
INFO - root - 2017-12-07 23:52:17.789235: step 59320, loss = 21.47, batch loss = 21.38 (8.7 examples/sec; 0.923 sec/batch; 70h:04m:08s remains)
INFO - root - 2017-12-07 23:52:27.194729: step 59330, loss = 21.43, batch loss = 21.34 (9.0 examples/sec; 0.893 sec/batch; 67h:44m:50s remains)
INFO - root - 2017-12-07 23:52:36.562326: step 59340, loss = 21.48, batch loss = 21.39 (8.7 examples/sec; 0.915 sec/batch; 69h:23m:57s remains)
INFO - root - 2017-12-07 23:52:45.967703: step 59350, loss = 21.42, batch loss = 21.34 (8.9 examples/sec; 0.895 sec/batch; 67h:53m:56s remains)
INFO - root - 2017-12-07 23:52:55.455498: step 59360, loss = 21.71, batch loss = 21.63 (8.8 examples/sec; 0.912 sec/batch; 69h:13m:46s remains)
INFO - root - 2017-12-07 23:53:04.868022: step 59370, loss = 21.54, batch loss = 21.46 (9.0 examples/sec; 0.888 sec/batch; 67h:23m:28s remains)
INFO - root - 2017-12-07 23:53:13.931944: step 59380, loss = 21.59, batch loss = 21.51 (8.6 examples/sec; 0.927 sec/batch; 70h:21m:02s remains)
INFO - root - 2017-12-07 23:53:23.422947: step 59390, loss = 21.65, batch loss = 21.57 (8.5 examples/sec; 0.944 sec/batch; 71h:36m:28s remains)
INFO - root - 2017-12-07 23:53:32.835177: step 59400, loss = 21.65, batch loss = 21.56 (8.5 examples/sec; 0.940 sec/batch; 71h:19m:27s remains)
2017-12-07 23:53:33.801012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3970027 -4.3918033 -4.3814807 -4.3698654 -4.3615584 -4.3599644 -4.3629308 -4.3670154 -4.3684816 -4.3654771 -4.3602562 -4.3549385 -4.3492537 -4.3425136 -4.3362236][-4.3674955 -4.3612185 -4.3445163 -4.3262677 -4.3154926 -4.3144288 -4.3178897 -4.3196907 -4.317338 -4.314436 -4.3135376 -4.3135157 -4.3107924 -4.3027234 -4.2935133][-4.3347416 -4.3265004 -4.299633 -4.27212 -4.258029 -4.2554417 -4.2561879 -4.2537794 -4.2489977 -4.2497106 -4.2563734 -4.2651477 -4.27079 -4.2693462 -4.2640214][-4.3073955 -4.2930489 -4.2485271 -4.2051191 -4.1805348 -4.1690054 -4.1632905 -4.1575804 -4.1547179 -4.1634126 -4.1790323 -4.200212 -4.2227793 -4.240253 -4.2499061][-4.2739635 -4.2456489 -4.1769609 -4.1126995 -4.0718522 -4.0466995 -4.0340567 -4.0272841 -4.02683 -4.0394278 -4.0577393 -4.0916519 -4.1394715 -4.1893315 -4.2262907][-4.2227316 -4.1791883 -4.0914736 -4.0128527 -3.9603384 -3.9250128 -3.9083822 -3.9008729 -3.8975997 -3.9054716 -3.9177983 -3.9594016 -4.03065 -4.1127152 -4.17678][-4.1769919 -4.1258841 -4.03479 -3.9558074 -3.9020045 -3.8644772 -3.8454576 -3.8338258 -3.8206458 -3.8134089 -3.8102951 -3.8492606 -3.9317713 -4.0332804 -4.1144772][-4.1588669 -4.1135521 -4.0384269 -3.9764075 -3.9346881 -3.9034424 -3.8823774 -3.8642504 -3.8403082 -3.8189039 -3.8027673 -3.8325384 -3.9093356 -4.0071573 -4.08484][-4.1764388 -4.1401658 -4.08817 -4.0493984 -4.0261278 -4.0080824 -3.9917905 -3.974328 -3.9495611 -3.9239793 -3.9028087 -3.9194722 -3.9758995 -4.0497632 -4.105351][-4.2119575 -4.1858087 -4.1562524 -4.1375704 -4.1310697 -4.1286755 -4.1250625 -4.1183963 -4.1032848 -4.082953 -4.063796 -4.0683932 -4.0988054 -4.1387148 -4.1637087][-4.2374358 -4.2195563 -4.206039 -4.2010212 -4.2060127 -4.2160997 -4.2262783 -4.2332797 -4.2309561 -4.2197623 -4.2066035 -4.2045445 -4.2147465 -4.2275743 -4.229701][-4.2470722 -4.2331681 -4.2272944 -4.2280436 -4.2360439 -4.2496514 -4.2654567 -4.2799149 -4.2870736 -4.2855272 -4.2807159 -4.279263 -4.2814207 -4.282218 -4.2750916][-4.2458062 -4.2350283 -4.2334566 -4.2365127 -4.2428722 -4.2526712 -4.2648625 -4.2782073 -4.2887011 -4.2931347 -4.2942405 -4.2950397 -4.2958474 -4.2944665 -4.2877545][-4.2564964 -4.24952 -4.2505212 -4.2546535 -4.2583666 -4.2611628 -4.2640085 -4.2693739 -4.2759585 -4.2801704 -4.282114 -4.2829885 -4.2835245 -4.2830396 -4.2800612][-4.2927413 -4.2915754 -4.2948203 -4.2997503 -4.3019619 -4.2996716 -4.2951031 -4.2937512 -4.2960277 -4.2986226 -4.2992315 -4.2973614 -4.2940984 -4.2908688 -4.2881727]]...]
INFO - root - 2017-12-07 23:53:43.148923: step 59410, loss = 21.73, batch loss = 21.64 (8.8 examples/sec; 0.911 sec/batch; 69h:05m:45s remains)
INFO - root - 2017-12-07 23:53:52.557447: step 59420, loss = 20.80, batch loss = 20.72 (8.7 examples/sec; 0.917 sec/batch; 69h:35m:44s remains)
INFO - root - 2017-12-07 23:54:02.012459: step 59430, loss = 21.36, batch loss = 21.27 (8.7 examples/sec; 0.922 sec/batch; 69h:55m:29s remains)
INFO - root - 2017-12-07 23:54:11.268457: step 59440, loss = 21.60, batch loss = 21.52 (8.4 examples/sec; 0.951 sec/batch; 72h:09m:21s remains)
INFO - root - 2017-12-07 23:54:20.651026: step 59450, loss = 21.49, batch loss = 21.41 (7.9 examples/sec; 1.014 sec/batch; 76h:56m:24s remains)
INFO - root - 2017-12-07 23:54:30.200053: step 59460, loss = 21.15, batch loss = 21.06 (8.6 examples/sec; 0.927 sec/batch; 70h:17m:15s remains)
INFO - root - 2017-12-07 23:54:39.818195: step 59470, loss = 21.44, batch loss = 21.36 (8.4 examples/sec; 0.953 sec/batch; 72h:17m:50s remains)
INFO - root - 2017-12-07 23:54:49.194886: step 59480, loss = 21.28, batch loss = 21.20 (9.0 examples/sec; 0.890 sec/batch; 67h:29m:33s remains)
INFO - root - 2017-12-07 23:54:58.539399: step 59490, loss = 21.55, batch loss = 21.47 (8.2 examples/sec; 0.971 sec/batch; 73h:36m:18s remains)
INFO - root - 2017-12-07 23:55:08.101638: step 59500, loss = 21.49, batch loss = 21.40 (7.9 examples/sec; 1.008 sec/batch; 76h:25m:46s remains)
2017-12-07 23:55:09.033896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4109921 -4.5278349 -4.6268868 -4.6818395 -4.6921864 -4.6476126 -4.5500174 -4.47387 -4.4833345 -4.5536852 -4.5809083 -4.5248737 -4.4197693 -4.3140559 -4.2960291][-4.4112029 -4.535574 -4.6254768 -4.6527276 -4.632863 -4.5613313 -4.43922 -4.3500996 -4.3699427 -4.4757876 -4.5367661 -4.4911628 -4.3731775 -4.2424531 -4.2143874][-4.406178 -4.5315652 -4.6045356 -4.592484 -4.5327821 -4.4373207 -4.3007622 -4.2032266 -4.2268291 -4.3527389 -4.44197 -4.4167218 -4.3088732 -4.1880178 -4.1705742][-4.41941 -4.5437326 -4.5959549 -4.5385456 -4.4297523 -4.3030386 -4.1565776 -4.0613179 -4.0922847 -4.2332191 -4.3489442 -4.3527856 -4.2761936 -4.1975193 -4.2126369][-4.4432917 -4.5605054 -4.5917749 -4.4959702 -4.3388638 -4.1750097 -4.0231209 -3.9471171 -4.00063 -4.1610703 -4.3011651 -4.3280015 -4.2776151 -4.2400742 -4.2818904][-4.4637709 -4.570353 -4.5861206 -4.473331 -4.2881932 -4.0957718 -3.939187 -3.8822329 -3.951828 -4.1187105 -4.2698517 -4.3113313 -4.284265 -4.2739062 -4.3238306][-4.4717422 -4.5638061 -4.5680323 -4.4596825 -4.2770057 -4.0801868 -3.927846 -3.8816733 -3.9518898 -4.1130075 -4.2693052 -4.3284764 -4.3234062 -4.327703 -4.37082][-4.4673686 -4.5414805 -4.5283 -4.4197435 -4.2487559 -4.0686016 -3.9337842 -3.8993447 -3.9656408 -4.113821 -4.2682552 -4.3360853 -4.3365765 -4.3402452 -4.3746][-4.465209 -4.5314264 -4.5102363 -4.4010768 -4.2403522 -4.07787 -3.9554389 -3.9240384 -3.9843984 -4.1178188 -4.2602916 -4.3261046 -4.3271847 -4.330236 -4.3630738][-4.4678164 -4.5413074 -4.532815 -4.4364395 -4.2905936 -4.1390033 -4.0090032 -3.9600697 -4.0092096 -4.1325717 -4.2667594 -4.3341427 -4.3486476 -4.3625875 -4.390223][-4.4725347 -4.5655189 -4.5856705 -4.5126467 -4.381825 -4.2303276 -4.0771928 -3.9984982 -4.030901 -4.1421685 -4.261867 -4.3285661 -4.3609591 -4.3906851 -4.4110026][-4.4841814 -4.6040611 -4.6610069 -4.6206131 -4.5074148 -4.3543487 -4.1801848 -4.0717134 -4.077158 -4.1612787 -4.25583 -4.317533 -4.3635535 -4.401505 -4.4083595][-4.5002117 -4.6404347 -4.7277393 -4.717031 -4.6217136 -4.4748406 -4.299994 -4.1821003 -4.1676965 -4.22665 -4.300817 -4.3560491 -4.3992395 -4.424129 -4.4073071][-4.5090661 -4.6538353 -4.7542682 -4.7634931 -4.6886783 -4.5604725 -4.4116912 -4.3142209 -4.301589 -4.3489952 -4.4031844 -4.4347315 -4.4504948 -4.4450889 -4.4074473][-4.5034442 -4.6367416 -4.7408333 -4.7711592 -4.7267442 -4.6335559 -4.5285654 -4.4675179 -4.4655132 -4.4967728 -4.516037 -4.5060616 -4.4824195 -4.4500332 -4.4047933]]...]
INFO - root - 2017-12-07 23:55:18.574258: step 59510, loss = 21.60, batch loss = 21.51 (8.4 examples/sec; 0.952 sec/batch; 72h:12m:15s remains)
INFO - root - 2017-12-07 23:55:27.995348: step 59520, loss = 21.59, batch loss = 21.51 (8.0 examples/sec; 1.000 sec/batch; 75h:48m:44s remains)
INFO - root - 2017-12-07 23:55:37.423995: step 59530, loss = 21.10, batch loss = 21.02 (8.8 examples/sec; 0.906 sec/batch; 68h:40m:58s remains)
INFO - root - 2017-12-07 23:55:46.746254: step 59540, loss = 21.38, batch loss = 21.29 (9.3 examples/sec; 0.859 sec/batch; 65h:06m:45s remains)
INFO - root - 2017-12-07 23:55:56.017147: step 59550, loss = 21.31, batch loss = 21.22 (8.9 examples/sec; 0.895 sec/batch; 67h:51m:15s remains)
INFO - root - 2017-12-07 23:56:05.517842: step 59560, loss = 21.43, batch loss = 21.35 (8.5 examples/sec; 0.946 sec/batch; 71h:41m:09s remains)
INFO - root - 2017-12-07 23:56:14.900325: step 59570, loss = 21.24, batch loss = 21.16 (8.1 examples/sec; 0.988 sec/batch; 74h:54m:20s remains)
INFO - root - 2017-12-07 23:56:24.243751: step 59580, loss = 21.41, batch loss = 21.32 (8.7 examples/sec; 0.922 sec/batch; 69h:53m:52s remains)
INFO - root - 2017-12-07 23:56:33.629700: step 59590, loss = 21.52, batch loss = 21.43 (8.9 examples/sec; 0.898 sec/batch; 68h:03m:57s remains)
INFO - root - 2017-12-07 23:56:43.102690: step 59600, loss = 21.00, batch loss = 20.92 (8.5 examples/sec; 0.943 sec/batch; 71h:27m:50s remains)
2017-12-07 23:56:44.034053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5396752 -4.5553012 -4.5566726 -4.5444312 -4.5261903 -4.5125794 -4.4999409 -4.4865322 -4.4734097 -4.4726772 -4.489697 -4.51341 -4.5261388 -4.5155907 -4.4786491][-4.6596918 -4.6813269 -4.6672421 -4.6296248 -4.5923076 -4.5651054 -4.5426731 -4.526587 -4.5233665 -4.544848 -4.5894885 -4.6324024 -4.6498475 -4.6281524 -4.5597658][-4.7837029 -4.8023376 -4.7586312 -4.677804 -4.6075106 -4.5527334 -4.5035529 -4.4702539 -4.4710116 -4.5243583 -4.6172738 -4.6991587 -4.7384849 -4.7230597 -4.6370978][-4.8440933 -4.847497 -4.7709312 -4.6519284 -4.5578184 -4.4802465 -4.4003959 -4.3388104 -4.330729 -4.4136138 -4.5591855 -4.68375 -4.7550163 -4.7636309 -4.6824064][-4.7656951 -4.74996 -4.6548481 -4.5227332 -4.4246902 -4.3396225 -4.2372346 -4.1366692 -4.1036983 -4.2103567 -4.4028444 -4.5665307 -4.6802716 -4.7307467 -4.6782537][-4.5879788 -4.5650573 -4.4657006 -4.3325443 -4.236372 -4.14743 -4.0230961 -3.8702929 -3.8014517 -3.9262304 -4.159153 -4.368268 -4.5423493 -4.6504359 -4.6419539][-4.4354968 -4.4143476 -4.3173995 -4.1760345 -4.0571594 -3.9510643 -3.8085878 -3.6169434 -3.5283706 -3.6752975 -3.939945 -4.1975245 -4.4313431 -4.5887313 -4.6158504][-4.4259157 -4.40993 -4.3082519 -4.1493559 -3.9998593 -3.8775082 -3.7266526 -3.5270243 -3.4543064 -3.6245289 -3.9022136 -4.18452 -4.4392176 -4.60334 -4.6314535][-4.5541544 -4.5377445 -4.4353814 -4.2753229 -4.1217489 -4.0044923 -3.862042 -3.6882763 -3.6519976 -3.8251953 -4.0878644 -4.3583069 -4.5784655 -4.6939216 -4.6843743][-4.7078004 -4.6968803 -4.6110806 -4.4761972 -4.3539762 -4.2690945 -4.1416535 -3.9843953 -3.9642057 -4.1222854 -4.3581829 -4.598763 -4.7592268 -4.8044634 -4.7432604][-4.8223424 -4.8171582 -4.7468038 -4.6368012 -4.5611343 -4.523726 -4.4231281 -4.2793245 -4.2470512 -4.3652983 -4.5605707 -4.7623281 -4.8686061 -4.8630919 -4.7686429][-4.8737631 -4.8713427 -4.8107166 -4.72073 -4.6812878 -4.6812053 -4.613524 -4.4936795 -4.4415407 -4.50552 -4.6504831 -4.8078337 -4.8756495 -4.8497257 -4.7486386][-4.8543615 -4.8486786 -4.794394 -4.7248454 -4.7083063 -4.7246447 -4.6863861 -4.6002846 -4.5401087 -4.5577536 -4.651247 -4.7635689 -4.8088846 -4.7838421 -4.6954041][-4.7697692 -4.7635479 -4.7227554 -4.6775327 -4.6749039 -4.6911106 -4.6708493 -4.618227 -4.5675249 -4.5602045 -4.609149 -4.6782546 -4.7088604 -4.6922517 -4.6248531][-4.6573811 -4.6544743 -4.6336765 -4.6147933 -4.6185527 -4.6272717 -4.6175389 -4.5918331 -4.561398 -4.5494285 -4.5676546 -4.5996671 -4.6154213 -4.6017451 -4.5520363]]...]
INFO - root - 2017-12-07 23:56:53.420232: step 59610, loss = 21.46, batch loss = 21.37 (8.2 examples/sec; 0.980 sec/batch; 74h:17m:56s remains)
INFO - root - 2017-12-07 23:57:02.794561: step 59620, loss = 21.81, batch loss = 21.73 (8.0 examples/sec; 0.996 sec/batch; 75h:30m:42s remains)
INFO - root - 2017-12-07 23:57:12.204573: step 59630, loss = 21.32, batch loss = 21.24 (7.9 examples/sec; 1.010 sec/batch; 76h:32m:13s remains)
INFO - root - 2017-12-07 23:57:21.642528: step 59640, loss = 21.30, batch loss = 21.22 (8.4 examples/sec; 0.952 sec/batch; 72h:10m:06s remains)
INFO - root - 2017-12-07 23:57:31.159988: step 59650, loss = 21.56, batch loss = 21.47 (8.9 examples/sec; 0.899 sec/batch; 68h:09m:20s remains)
INFO - root - 2017-12-07 23:57:40.588752: step 59660, loss = 21.57, batch loss = 21.49 (8.6 examples/sec; 0.934 sec/batch; 70h:45m:36s remains)
INFO - root - 2017-12-07 23:57:49.995961: step 59670, loss = 21.72, batch loss = 21.64 (8.5 examples/sec; 0.938 sec/batch; 71h:07m:24s remains)
INFO - root - 2017-12-07 23:57:59.522257: step 59680, loss = 21.47, batch loss = 21.38 (8.5 examples/sec; 0.938 sec/batch; 71h:04m:34s remains)
INFO - root - 2017-12-07 23:58:08.860121: step 59690, loss = 21.17, batch loss = 21.09 (8.7 examples/sec; 0.916 sec/batch; 69h:23m:33s remains)
INFO - root - 2017-12-07 23:58:18.309064: step 59700, loss = 21.67, batch loss = 21.59 (8.8 examples/sec; 0.912 sec/batch; 69h:05m:01s remains)
2017-12-07 23:58:19.279811: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4945321 -4.577107 -4.6175814 -4.6371837 -4.64987 -4.6365285 -4.55916 -4.4174628 -4.3077083 -4.2867336 -4.3177695 -4.3241196 -4.2436962 -4.123105 -4.0700107][-4.6252918 -4.6899986 -4.69875 -4.682838 -4.6679487 -4.6472578 -4.5851259 -4.4843659 -4.4283075 -4.4495988 -4.5105772 -4.5474563 -4.4930973 -4.3572164 -4.2344317][-4.6955662 -4.7575707 -4.749814 -4.7060881 -4.6619878 -4.6236806 -4.5611315 -4.490346 -4.4857254 -4.55291 -4.6377869 -4.6895285 -4.6567044 -4.5297771 -4.3733025][-4.6733928 -4.7468882 -4.7415018 -4.6777673 -4.59364 -4.509284 -4.4131451 -4.3452072 -4.3834457 -4.5105491 -4.6357503 -4.70806 -4.6964774 -4.5946226 -4.4413][-4.5993247 -4.6747718 -4.66486 -4.5786076 -4.4517217 -4.3164253 -4.178288 -4.093946 -4.1550269 -4.3341775 -4.5080972 -4.6103559 -4.6198759 -4.541944 -4.4126873][-4.4787121 -4.5270009 -4.5036716 -4.4068866 -4.2613511 -4.0953279 -3.9216208 -3.8081365 -3.8698919 -4.0851541 -4.3097548 -4.456099 -4.4960408 -4.4402747 -4.3470621][-4.3699532 -4.3841615 -4.3478131 -4.2492146 -4.0997591 -3.9207261 -3.7282715 -3.5937574 -3.649292 -3.8794835 -4.1391044 -4.3254924 -4.3965454 -4.364522 -4.3058839][-4.3506827 -4.3389482 -4.2889924 -4.1864123 -4.0390835 -3.8685734 -3.6864009 -3.5503559 -3.5887632 -3.7920763 -4.0426869 -4.2450752 -4.3492107 -4.3571877 -4.3346291][-4.3702116 -4.3482542 -4.2960634 -4.2002535 -4.0699277 -3.9256737 -3.7723026 -3.652797 -3.6657245 -3.8151469 -4.0293155 -4.2284141 -4.3607488 -4.4132285 -4.4179873][-4.3861747 -4.3734875 -4.331531 -4.2537484 -4.1535835 -4.0457821 -3.9279244 -3.83058 -3.8236897 -3.925334 -4.1001248 -4.2830954 -4.4234881 -4.4941092 -4.4999433][-4.3918772 -4.3915982 -4.3563209 -4.2947392 -4.2303 -4.1688309 -4.0967574 -4.0334353 -4.0273733 -4.1010375 -4.2353396 -4.3822846 -4.4981813 -4.5546756 -4.5483856][-4.37435 -4.3818679 -4.3491716 -4.3046331 -4.2783685 -4.2663093 -4.2439938 -4.2182021 -4.2239218 -4.2825418 -4.3802571 -4.4834695 -4.5526538 -4.57111 -4.5510345][-4.3488712 -4.35925 -4.3272333 -4.2918763 -4.2863364 -4.3037195 -4.3165178 -4.3224072 -4.3464484 -4.4060559 -4.4882441 -4.5658531 -4.5987592 -4.5829792 -4.5474191][-4.3268638 -4.3264513 -4.2901793 -4.2566562 -4.2573462 -4.2874808 -4.3190217 -4.342773 -4.3787022 -4.4414458 -4.521904 -4.5938115 -4.6183748 -4.5951042 -4.5536609][-4.3167653 -4.2991018 -4.2588673 -4.2294836 -4.2359581 -4.2701321 -4.3005037 -4.3161559 -4.3424892 -4.4004879 -4.4808621 -4.5557041 -4.5881591 -4.57318 -4.5333295]]...]
INFO - root - 2017-12-07 23:58:28.866300: step 59710, loss = 21.54, batch loss = 21.46 (8.2 examples/sec; 0.977 sec/batch; 74h:03m:25s remains)
INFO - root - 2017-12-07 23:58:38.080554: step 59720, loss = 21.58, batch loss = 21.50 (8.5 examples/sec; 0.937 sec/batch; 71h:00m:03s remains)
INFO - root - 2017-12-07 23:58:47.554276: step 59730, loss = 21.65, batch loss = 21.56 (8.3 examples/sec; 0.963 sec/batch; 72h:57m:10s remains)
INFO - root - 2017-12-07 23:58:57.099618: step 59740, loss = 21.57, batch loss = 21.49 (7.8 examples/sec; 1.020 sec/batch; 77h:15m:10s remains)
INFO - root - 2017-12-07 23:59:06.447946: step 59750, loss = 21.84, batch loss = 21.76 (8.2 examples/sec; 0.973 sec/batch; 73h:43m:31s remains)
INFO - root - 2017-12-07 23:59:15.852976: step 59760, loss = 21.40, batch loss = 21.32 (8.4 examples/sec; 0.957 sec/batch; 72h:28m:29s remains)
INFO - root - 2017-12-07 23:59:25.298228: step 59770, loss = 21.38, batch loss = 21.29 (8.2 examples/sec; 0.975 sec/batch; 73h:53m:35s remains)
INFO - root - 2017-12-07 23:59:34.664305: step 59780, loss = 21.61, batch loss = 21.53 (8.2 examples/sec; 0.975 sec/batch; 73h:52m:03s remains)
INFO - root - 2017-12-07 23:59:44.072332: step 59790, loss = 21.58, batch loss = 21.50 (8.4 examples/sec; 0.957 sec/batch; 72h:29m:16s remains)
INFO - root - 2017-12-07 23:59:53.613821: step 59800, loss = 21.51, batch loss = 21.43 (8.1 examples/sec; 0.982 sec/batch; 74h:23m:25s remains)
2017-12-07 23:59:54.578952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5024729 -4.5216756 -4.5290956 -4.526226 -4.5283813 -4.538085 -4.5355411 -4.5219564 -4.5230904 -4.5452881 -4.5724549 -4.5878377 -4.5822005 -4.54829 -4.4882212][-4.5634036 -4.5778804 -4.5791593 -4.5704913 -4.5673857 -4.5724478 -4.5519185 -4.5173674 -4.5206695 -4.5678096 -4.6234736 -4.65454 -4.6531458 -4.6147614 -4.5388727][-4.5812588 -4.5736308 -4.5639019 -4.55389 -4.5496087 -4.5460343 -4.497138 -4.4334459 -4.443325 -4.5286317 -4.6287627 -4.6865478 -4.6969028 -4.66257 -4.5812192][-4.5560856 -4.5186758 -4.4987679 -4.4977474 -4.5021658 -4.4955893 -4.4212575 -4.3331337 -4.3521662 -4.4774985 -4.6227975 -4.7035584 -4.7198181 -4.6887493 -4.6078796][-4.5134234 -4.4526711 -4.4218836 -4.4284472 -4.4426417 -4.4318075 -4.3318734 -4.2188754 -4.2374835 -4.389462 -4.5677238 -4.6664848 -4.6915979 -4.6721978 -4.6028013][-4.4848042 -4.4163089 -4.3749723 -4.3797359 -4.3895655 -4.3562961 -4.21384 -4.06029 -4.0687833 -4.2383442 -4.4440207 -4.5711579 -4.6191773 -4.6218252 -4.5715747][-4.4842668 -4.4111824 -4.3517618 -4.3360109 -4.3266964 -4.2534971 -4.0448928 -3.8353109 -3.8442647 -4.0452967 -4.293303 -4.4692311 -4.5522671 -4.574842 -4.5350385][-4.497324 -4.4132242 -4.3323345 -4.2900105 -4.251358 -4.1305523 -3.8506389 -3.59236 -3.623868 -3.8835912 -4.1913333 -4.4230008 -4.5381145 -4.5618038 -4.5124903][-4.5128894 -4.4214888 -4.3360195 -4.2893667 -4.2381158 -4.0984206 -3.7999783 -3.5408888 -3.5984545 -3.8966596 -4.2289739 -4.4744625 -4.5877833 -4.5896592 -4.516264][-4.5418448 -4.4567151 -4.3912082 -4.3651729 -4.3252778 -4.2052903 -3.9536417 -3.744772 -3.8074615 -4.0819407 -4.3726454 -4.5685425 -4.6451654 -4.624145 -4.53636][-4.5504708 -4.4848995 -4.4522052 -4.4573312 -4.4353046 -4.3419604 -4.1606627 -4.0158615 -4.0674934 -4.2798204 -4.4853029 -4.6024933 -4.6391749 -4.6184568 -4.5463018][-4.5332527 -4.4972124 -4.5015845 -4.5365252 -4.5334172 -4.4673014 -4.351697 -4.2597814 -4.2927032 -4.4339433 -4.5552187 -4.6031437 -4.612236 -4.6005549 -4.5519838][-4.5105696 -4.50477 -4.5339694 -4.5805273 -4.5865321 -4.5436549 -4.4779978 -4.4238124 -4.4389663 -4.5234022 -4.585464 -4.5920463 -4.5861673 -4.5782604 -4.5464706][-4.4636793 -4.4777713 -4.51154 -4.5496697 -4.5564666 -4.5358195 -4.5095491 -4.4869008 -4.4953375 -4.54052 -4.5654182 -4.5551958 -4.54368 -4.5352879 -4.5127144][-4.3970475 -4.4147925 -4.4405556 -4.4654021 -4.4741459 -4.4726291 -4.47257 -4.472291 -4.4790463 -4.4979644 -4.503521 -4.4923773 -4.4820714 -4.4716058 -4.4529853]]...]
INFO - root - 2017-12-08 00:00:04.098144: step 59810, loss = 21.23, batch loss = 21.15 (8.5 examples/sec; 0.945 sec/batch; 71h:35m:46s remains)
INFO - root - 2017-12-08 00:00:13.458349: step 59820, loss = 21.66, batch loss = 21.57 (8.9 examples/sec; 0.897 sec/batch; 67h:54m:58s remains)
INFO - root - 2017-12-08 00:00:22.797016: step 59830, loss = 21.63, batch loss = 21.55 (8.4 examples/sec; 0.951 sec/batch; 72h:03m:28s remains)
INFO - root - 2017-12-08 00:00:32.020751: step 59840, loss = 21.17, batch loss = 21.09 (8.8 examples/sec; 0.908 sec/batch; 68h:46m:32s remains)
INFO - root - 2017-12-08 00:00:41.448370: step 59850, loss = 21.54, batch loss = 21.46 (8.9 examples/sec; 0.896 sec/batch; 67h:50m:35s remains)
INFO - root - 2017-12-08 00:00:50.735040: step 59860, loss = 21.99, batch loss = 21.91 (8.5 examples/sec; 0.939 sec/batch; 71h:07m:03s remains)
INFO - root - 2017-12-08 00:01:00.077944: step 59870, loss = 21.23, batch loss = 21.15 (8.1 examples/sec; 0.982 sec/batch; 74h:23m:51s remains)
INFO - root - 2017-12-08 00:01:09.429476: step 59880, loss = 21.59, batch loss = 21.51 (8.1 examples/sec; 0.987 sec/batch; 74h:45m:58s remains)
INFO - root - 2017-12-08 00:01:18.569408: step 59890, loss = 21.57, batch loss = 21.49 (10.5 examples/sec; 0.758 sec/batch; 57h:25m:36s remains)
INFO - root - 2017-12-08 00:01:27.859274: step 59900, loss = 21.03, batch loss = 20.95 (9.0 examples/sec; 0.887 sec/batch; 67h:08m:50s remains)
2017-12-08 00:01:28.780795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3598771 -4.3865948 -4.4286819 -4.4829755 -4.5424676 -4.5964837 -4.6319447 -4.6378703 -4.6186905 -4.5784678 -4.5359383 -4.5139155 -4.502676 -4.478354 -4.4536052][-4.4001446 -4.4449072 -4.4895792 -4.5230336 -4.5452905 -4.5635781 -4.580133 -4.5863705 -4.5778885 -4.5431437 -4.4956293 -4.4661784 -4.45563 -4.437572 -4.4143095][-4.4869504 -4.5487628 -4.5783567 -4.5640321 -4.5210752 -4.4887872 -4.4903769 -4.5112352 -4.5275216 -4.5097485 -4.467062 -4.4394116 -4.431869 -4.4154739 -4.3837705][-4.5767808 -4.6451187 -4.6517458 -4.5863438 -4.4799867 -4.4050312 -4.4106803 -4.4698529 -4.5236931 -4.5282187 -4.491272 -4.4604373 -4.4507437 -4.4289374 -4.381156][-4.6098561 -4.666986 -4.6477 -4.54318 -4.3903866 -4.2825937 -4.2996011 -4.4075456 -4.5125432 -4.555563 -4.5310435 -4.4907508 -4.4712296 -4.442296 -4.3853388][-4.5947051 -4.6288114 -4.5820694 -4.4464674 -4.2509685 -4.0987616 -4.0994077 -4.2317595 -4.3950233 -4.5115185 -4.5345387 -4.5038214 -4.4829707 -4.4554915 -4.4035435][-4.574347 -4.5827818 -4.5069385 -4.3368244 -4.092514 -3.8825788 -3.8332551 -3.9491854 -4.1543384 -4.362175 -4.4726257 -4.4937224 -4.5004287 -4.4877977 -4.4523363][-4.582201 -4.5705309 -4.4683228 -4.2621422 -3.9699121 -3.7065079 -3.6012948 -3.6750288 -3.8916349 -4.1707878 -4.3752322 -4.4757171 -4.5312076 -4.5374575 -4.5159216][-4.6144609 -4.5984125 -4.4875703 -4.2598033 -3.9400725 -3.6473603 -3.5042307 -3.5383811 -3.7436886 -4.0512815 -4.31379 -4.475256 -4.5664496 -4.5804615 -4.5634313][-4.6368384 -4.6367011 -4.5416756 -4.326303 -4.02061 -3.7369478 -3.5832672 -3.5924692 -3.772213 -4.0591254 -4.31806 -4.4881811 -4.5811863 -4.5932522 -4.5830193][-4.6305037 -4.6541967 -4.5893769 -4.4146919 -4.1675172 -3.9391384 -3.8095982 -3.812439 -3.9569058 -4.1871324 -4.3924527 -4.5221143 -4.5831966 -4.5849547 -4.583643][-4.5954318 -4.6399112 -4.6070752 -4.4805608 -4.3093071 -4.1609488 -4.0809164 -4.0955386 -4.2070827 -4.3685422 -4.5021987 -4.5711412 -4.5840545 -4.5634346 -4.557723][-4.5463061 -4.6091428 -4.6020932 -4.5149813 -4.4092751 -4.3364496 -4.3096128 -4.3409333 -4.4222956 -4.5223141 -4.5921993 -4.6025696 -4.5626774 -4.507935 -4.4823165][-4.491148 -4.5568514 -4.5661974 -4.5129828 -4.46192 -4.4504223 -4.4668169 -4.50919 -4.5659461 -4.6207361 -4.6423345 -4.6058836 -4.5214014 -4.4351387 -4.388669][-4.429235 -4.4763637 -4.4925156 -4.469779 -4.4631357 -4.4961452 -4.540844 -4.5872316 -4.625905 -4.6500978 -4.6392145 -4.575757 -4.4704027 -4.3722854 -4.3188939]]...]
INFO - root - 2017-12-08 00:01:38.059723: step 59910, loss = 21.20, batch loss = 21.12 (8.6 examples/sec; 0.929 sec/batch; 70h:18m:36s remains)
INFO - root - 2017-12-08 00:01:47.433270: step 59920, loss = 21.15, batch loss = 21.07 (8.7 examples/sec; 0.917 sec/batch; 69h:27m:21s remains)
INFO - root - 2017-12-08 00:01:56.691680: step 59930, loss = 21.43, batch loss = 21.35 (8.9 examples/sec; 0.902 sec/batch; 68h:18m:29s remains)
INFO - root - 2017-12-08 00:02:06.147532: step 59940, loss = 21.59, batch loss = 21.51 (8.4 examples/sec; 0.954 sec/batch; 72h:13m:39s remains)
INFO - root - 2017-12-08 00:02:15.632961: step 59950, loss = 20.97, batch loss = 20.89 (8.2 examples/sec; 0.976 sec/batch; 73h:53m:29s remains)
INFO - root - 2017-12-08 00:02:25.016720: step 59960, loss = 21.49, batch loss = 21.41 (8.0 examples/sec; 0.997 sec/batch; 75h:28m:09s remains)
INFO - root - 2017-12-08 00:02:34.306286: step 59970, loss = 21.27, batch loss = 21.18 (8.0 examples/sec; 0.997 sec/batch; 75h:27m:23s remains)
INFO - root - 2017-12-08 00:02:43.690699: step 59980, loss = 21.40, batch loss = 21.32 (8.5 examples/sec; 0.937 sec/batch; 70h:56m:23s remains)
INFO - root - 2017-12-08 00:02:53.014825: step 59990, loss = 21.46, batch loss = 21.38 (8.5 examples/sec; 0.937 sec/batch; 70h:54m:59s remains)
INFO - root - 2017-12-08 00:03:02.341772: step 60000, loss = 21.54, batch loss = 21.46 (8.2 examples/sec; 0.978 sec/batch; 74h:00m:46s remains)
2017-12-08 00:03:03.248102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6226449 -4.6404543 -4.6720796 -4.6897025 -4.6798282 -4.6592994 -4.6483793 -4.6632342 -4.6931372 -4.7051616 -4.6790385 -4.631547 -4.5996752 -4.6000352 -4.6173263][-4.6872921 -4.6954327 -4.7230878 -4.738615 -4.7221036 -4.6872921 -4.6629381 -4.676054 -4.7183409 -4.7430887 -4.7179275 -4.6667247 -4.6472378 -4.6734228 -4.7023611][-4.6740675 -4.6683207 -4.68749 -4.6961923 -4.6649094 -4.6106853 -4.5736232 -4.5881705 -4.6451817 -4.6900597 -4.6774859 -4.6284685 -4.6216693 -4.676724 -4.7248816][-4.6012044 -4.5850425 -4.5966382 -4.5862565 -4.5225878 -4.4449673 -4.4010968 -4.4164166 -4.4836912 -4.5544438 -4.5720348 -4.5380211 -4.538075 -4.6086774 -4.6795859][-4.506628 -4.4839449 -4.4773622 -4.4286108 -4.320087 -4.2193551 -4.1724706 -4.1857243 -4.259232 -4.3611646 -4.425282 -4.4240856 -4.4308238 -4.50048 -4.5881786][-4.4411964 -4.3885746 -4.3417444 -4.2481041 -4.1031542 -3.991076 -3.9485979 -3.9671178 -4.0518184 -4.1830697 -4.2895837 -4.324523 -4.3392005 -4.3941789 -4.4800429][-4.4115996 -4.3027596 -4.2106814 -4.0993009 -3.9490533 -3.8314087 -3.7883372 -3.8153877 -3.9118114 -4.0629754 -4.2006049 -4.2663207 -4.2884 -4.3195977 -4.3830476][-4.3893108 -4.2367153 -4.1289411 -4.0402851 -3.9128013 -3.7789192 -3.7065694 -3.7224989 -3.8218024 -3.9867773 -4.1534834 -4.2459145 -4.2693367 -4.2720904 -4.3049684][-4.3687758 -4.2153745 -4.1287036 -4.0833921 -3.9831941 -3.8238783 -3.7003624 -3.6807246 -3.7679718 -3.9430156 -4.1360278 -4.2469926 -4.2631168 -4.2428923 -4.2558866][-4.3422184 -4.2242069 -4.1808619 -4.1772861 -4.0990477 -3.9225736 -3.7552338 -3.6920397 -3.756459 -3.9345407 -4.1417594 -4.2594619 -4.2711043 -4.2495737 -4.264915][-4.3197269 -4.23844 -4.2342873 -4.2568626 -4.187407 -4.0075965 -3.8259361 -3.7419949 -3.7951121 -3.9715686 -4.1735115 -4.2849894 -4.2997093 -4.3004866 -4.3394356][-4.312994 -4.2484617 -4.2742991 -4.3175449 -4.2568622 -4.0912719 -3.9299533 -3.8613079 -3.9195082 -4.0835495 -4.2544727 -4.3392158 -4.3552051 -4.3867664 -4.454999][-4.3092446 -4.2537432 -4.3078423 -4.3751683 -4.3314471 -4.1978817 -4.0826879 -4.0508938 -4.1149135 -4.2497168 -4.3729672 -4.4193878 -4.4293013 -4.4840927 -4.575038][-4.3223176 -4.2845569 -4.358458 -4.439507 -4.418118 -4.3291183 -4.2645378 -4.2645025 -4.3219652 -4.4141479 -4.4856 -4.4996581 -4.5065079 -4.5729012 -4.6682734][-4.3786511 -4.3732934 -4.4511566 -4.5281029 -4.5291128 -4.4856906 -4.458683 -4.46985 -4.5068626 -4.5539889 -4.5810461 -4.5759668 -4.58293 -4.6452289 -4.7229414]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lastaddlayer-adm-0.001-l2loss/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 00:03:13.223430: step 60010, loss = 21.76, batch loss = 21.67 (9.1 examples/sec; 0.884 sec/batch; 66h:54m:34s remains)
INFO - root - 2017-12-08 00:03:22.571696: step 60020, loss = 21.54, batch loss = 21.45 (8.8 examples/sec; 0.912 sec/batch; 69h:03m:23s remains)
INFO - root - 2017-12-08 00:03:31.863169: step 60030, loss = 21.54, batch loss = 21.46 (8.8 examples/sec; 0.907 sec/batch; 68h:38m:01s remains)
INFO - root - 2017-12-08 00:03:41.301046: step 60040, loss = 21.66, batch loss = 21.58 (8.3 examples/sec; 0.962 sec/batch; 72h:46m:56s remains)
INFO - root - 2017-12-08 00:03:50.716843: step 60050, loss = 21.13, batch loss = 21.05 (8.8 examples/sec; 0.911 sec/batch; 68h:54m:42s remains)
INFO - root - 2017-12-08 00:04:00.035112: step 60060, loss = 21.71, batch loss = 21.63 (8.4 examples/sec; 0.955 sec/batch; 72h:18m:18s remains)
INFO - root - 2017-12-08 00:04:09.384980: step 60070, loss = 21.67, batch loss = 21.59 (8.3 examples/sec; 0.962 sec/batch; 72h:47m:27s remains)
INFO - root - 2017-12-08 00:04:18.865240: step 60080, loss = 21.53, batch loss = 21.45 (7.8 examples/sec; 1.021 sec/batch; 77h:15m:15s remains)
INFO - root - 2017-12-08 00:04:28.151722: step 60090, loss = 21.49, batch loss = 21.41 (8.6 examples/sec; 0.934 sec/batch; 70h:41m:22s remains)
INFO - root - 2017-12-08 00:04:37.332960: step 60100, loss = 21.77, batch loss = 21.69 (8.7 examples/sec; 0.925 sec/batch; 69h:58m:46s remains)
2017-12-08 00:04:38.285986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4797792 -4.5228276 -4.504766 -4.4537182 -4.3984385 -4.3585048 -4.3558359 -4.3713436 -4.3953409 -4.41732 -4.4484429 -4.5143876 -4.5933876 -4.62275 -4.5936074][-4.3789845 -4.4259739 -4.4231515 -4.3911557 -4.3506465 -4.3150983 -4.3084126 -4.3172832 -4.3326392 -4.3527322 -4.3928909 -4.4812288 -4.5916405 -4.6360779 -4.6091862][-4.3050833 -4.3502603 -4.3668113 -4.3617263 -4.3371086 -4.305891 -4.3032613 -4.3139424 -4.3171821 -4.32254 -4.3536406 -4.4483743 -4.5765815 -4.6312222 -4.6099224][-4.2582922 -4.2969475 -4.3227496 -4.3303814 -4.3116393 -4.2864795 -4.305048 -4.338932 -4.3457789 -4.341465 -4.3578615 -4.442955 -4.5698953 -4.6244426 -4.6055474][-4.2568593 -4.2832866 -4.2967219 -4.2804031 -4.231976 -4.1909986 -4.2223725 -4.2828803 -4.3103337 -4.316504 -4.3360558 -4.4229803 -4.5538626 -4.6133404 -4.603076][-4.2687263 -4.2762995 -4.2573032 -4.192462 -4.087275 -4.0076442 -4.0336618 -4.1123562 -4.1691308 -4.2090583 -4.261641 -4.3731108 -4.5180316 -4.5875263 -4.5924735][-4.2810907 -4.2731705 -4.2170339 -4.0977259 -3.9344814 -3.8163304 -3.8318534 -3.9194391 -3.9961052 -4.06835 -4.1565733 -4.2970958 -4.4584446 -4.5385461 -4.5585518][-4.2903414 -4.2745566 -4.2059393 -4.0658946 -3.8700581 -3.722528 -3.7185204 -3.7917733 -3.8624003 -3.9439657 -4.0494223 -4.2082362 -4.3819742 -4.4724293 -4.5059185][-4.3414712 -4.3536453 -4.3204746 -4.2026396 -4.00251 -3.8334429 -3.797214 -3.8302169 -3.8690953 -3.929775 -4.0235777 -4.1848264 -4.3616109 -4.4579148 -4.4912724][-4.4444857 -4.4957862 -4.5113454 -4.429533 -4.2416759 -4.0616641 -3.9942858 -3.9883938 -3.9978814 -4.0253248 -4.0886707 -4.2303839 -4.393363 -4.488193 -4.515533][-4.531054 -4.5959606 -4.6372509 -4.58249 -4.4159622 -4.2439828 -4.1666126 -4.1515574 -4.1624379 -4.1750126 -4.2156572 -4.32586 -4.4576173 -4.5407457 -4.5612631][-4.5840926 -4.6466026 -4.693141 -4.6581345 -4.5254655 -4.3839355 -4.323452 -4.329546 -4.3611331 -4.3730326 -4.396131 -4.4652667 -4.5498471 -4.6049643 -4.6123147][-4.5981 -4.649653 -4.6904082 -4.6740041 -4.589026 -4.498291 -4.4690204 -4.498703 -4.5419431 -4.55439 -4.5612297 -4.5859613 -4.6186371 -4.638731 -4.6275015][-4.5867205 -4.6303644 -4.6625409 -4.6608505 -4.6200361 -4.5752907 -4.5653458 -4.5930214 -4.6240034 -4.6291962 -4.6236668 -4.6205854 -4.6196809 -4.6145544 -4.5891318][-4.5336685 -4.564158 -4.5839534 -4.5857038 -4.5691051 -4.5502748 -4.5454979 -4.55713 -4.5678205 -4.5653334 -4.5567536 -4.5479178 -4.5387769 -4.5267339 -4.5026369]]...]
INFO - root - 2017-12-08 00:04:47.620896: step 60110, loss = 21.71, batch loss = 21.62 (8.8 examples/sec; 0.911 sec/batch; 68h:57m:55s remains)
INFO - root - 2017-12-08 00:04:57.117336: step 60120, loss = 21.22, batch loss = 21.14 (8.6 examples/sec; 0.925 sec/batch; 69h:58m:54s remains)
INFO - root - 2017-12-08 00:05:06.435693: step 60130, loss = 21.34, batch loss = 21.26 (8.5 examples/sec; 0.943 sec/batch; 71h:20m:44s remains)
INFO - root - 2017-12-08 00:05:15.819364: step 60140, loss = 21.35, batch loss = 21.26 (8.4 examples/sec; 0.951 sec/batch; 71h:57m:17s remains)
INFO - root - 2017-12-08 00:05:25.253531: step 60150, loss = 21.72, batch loss = 21.63 (8.3 examples/sec; 0.962 sec/batch; 72h:44m:35s remains)
INFO - root - 2017-12-08 00:05:34.630352: step 60160, loss = 21.34, batch loss = 21.26 (8.1 examples/sec; 0.985 sec/batch; 74h:28m:47s remains)
INFO - root - 2017-12-08 00:05:44.013783: step 60170, loss = 21.72, batch loss = 21.64 (8.7 examples/sec; 0.920 sec/batch; 69h:34m:15s remains)
INFO - root - 2017-12-08 00:05:53.358768: step 60180, loss = 21.35, batch loss = 21.26 (8.6 examples/sec; 0.935 sec/batch; 70h:43m:49s remains)
INFO - root - 2017-12-08 00:06:02.812752: step 60190, loss = 21.42, batch loss = 21.33 (8.8 examples/sec; 0.905 sec/batch; 68h:29m:31s remains)
INFO - root - 2017-12-08 00:06:12.264653: step 60200, loss = 21.70, batch loss = 21.61 (8.7 examples/sec; 0.917 sec/batch; 69h:22m:54s remains)
2017-12-08 00:06:13.112954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.528192 -4.5551562 -4.5960493 -4.6361952 -4.6430788 -4.623414 -4.5964947 -4.57059 -4.554431 -4.5605278 -4.5825834 -4.5839958 -4.5322208 -4.4410481 -4.3576279][-4.6010218 -4.6226492 -4.6826611 -4.7489047 -4.7550082 -4.7105656 -4.6565881 -4.6205363 -4.616148 -4.6469936 -4.6950717 -4.71838 -4.6583962 -4.5265479 -4.400702][-4.61068 -4.6190395 -4.6925235 -4.7788143 -4.7737374 -4.6889772 -4.6032743 -4.5732775 -4.6100645 -4.6904311 -4.7812958 -4.8394823 -4.7812924 -4.6131306 -4.4428449][-4.5465746 -4.5447512 -4.6219797 -4.71589 -4.6922803 -4.5635657 -4.4465151 -4.4298773 -4.5189371 -4.6526456 -4.789782 -4.8930984 -4.8565664 -4.6742005 -4.47189][-4.4441309 -4.4415417 -4.5102878 -4.5923233 -4.54847 -4.3886137 -4.245266 -4.2317824 -4.3590269 -4.5305891 -4.7056746 -4.8527093 -4.8526034 -4.6865954 -4.4789257][-4.3416367 -4.3511539 -4.40521 -4.4540753 -4.3798923 -4.1951923 -4.0236988 -3.9997091 -4.1468349 -4.3499956 -4.5624862 -4.7471662 -4.7882829 -4.6591249 -4.4684134][-4.2503166 -4.2917151 -4.3419418 -4.3434644 -4.2189722 -4.0005069 -3.7929823 -3.7456665 -3.8982282 -4.1322379 -4.3840466 -4.603848 -4.6898565 -4.6098342 -4.4515285][-4.183989 -4.2812676 -4.3571095 -4.3254404 -4.1459408 -3.8825312 -3.6410806 -3.573977 -3.7239172 -3.9716513 -4.2396936 -4.4747009 -4.595396 -4.5627813 -4.4377027][-4.1488152 -4.3090825 -4.4386721 -4.4139538 -4.2077436 -3.908643 -3.6481926 -3.5757389 -3.7112558 -3.9440548 -4.1915884 -4.410439 -4.5397043 -4.5351019 -4.4311671][-4.1610837 -4.3537574 -4.5288992 -4.5454726 -4.3598924 -4.0540309 -3.785013 -3.7054203 -3.8210242 -4.0333891 -4.2480836 -4.4329696 -4.5428457 -4.5358319 -4.4335279][-4.2646489 -4.4322968 -4.6005659 -4.6514754 -4.5157909 -4.2457385 -3.9952323 -3.9116118 -4.0098495 -4.2080693 -4.4004459 -4.5480523 -4.6146936 -4.5713978 -4.4475236][-4.4308357 -4.5357132 -4.6545949 -4.7130814 -4.6319094 -4.4295292 -4.2269711 -4.1512995 -4.2333837 -4.4139032 -4.5862451 -4.6979837 -4.7142091 -4.6216254 -4.4672513][-4.5596409 -4.6116014 -4.67729 -4.7264123 -4.6883612 -4.5620017 -4.4271531 -4.3714657 -4.4331388 -4.5780044 -4.7204046 -4.8004436 -4.7799649 -4.6531076 -4.4809542][-4.6074662 -4.6238046 -4.6498632 -4.6835995 -4.6783705 -4.624815 -4.5619912 -4.536819 -4.5786676 -4.6740327 -4.7722883 -4.8195968 -4.77829 -4.6443305 -4.4765925][-4.5940871 -4.5821009 -4.5748177 -4.5870457 -4.6018119 -4.6051474 -4.6004076 -4.6029 -4.6316729 -4.6832409 -4.7364693 -4.7532525 -4.7040253 -4.5853748 -4.4432817]]...]
INFO - root - 2017-12-08 00:06:22.519968: step 60210, loss = 21.51, batch loss = 21.43 (8.2 examples/sec; 0.975 sec/batch; 73h:43m:56s remains)
INFO - root - 2017-12-08 00:06:31.954273: step 60220, loss = 21.38, batch loss = 21.30 (8.5 examples/sec; 0.945 sec/batch; 71h:27m:45s remains)
INFO - root - 2017-12-08 00:06:41.438395: step 60230, loss = 21.46, batch loss = 21.38 (8.0 examples/sec; 0.998 sec/batch; 75h:28m:33s remains)
INFO - root - 2017-12-08 00:06:50.608283: step 60240, loss = 21.70, batch loss = 21.61 (8.9 examples/sec; 0.898 sec/batch; 67h:55m:19s remains)
INFO - root - 2017-12-08 00:07:00.006274: step 60250, loss = 21.30, batch loss = 21.22 (8.3 examples/sec; 0.959 sec/batch; 72h:33m:26s remains)
INFO - root - 2017-12-08 00:07:09.523294: step 60260, loss = 21.47, batch loss = 21.38 (8.5 examples/sec; 0.939 sec/batch; 71h:02m:20s remains)
INFO - root - 2017-12-08 00:07:19.032730: step 60270, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.947 sec/batch; 71h:34m:44s remains)
INFO - root - 2017-12-08 00:07:28.411520: step 60280, loss = 21.74, batch loss = 21.65 (8.6 examples/sec; 0.929 sec/batch; 70h:16m:05s remains)
INFO - root - 2017-12-08 00:07:37.813128: step 60290, loss = 21.43, batch loss = 21.35 (8.5 examples/sec; 0.947 sec/batch; 71h:35m:08s remains)
INFO - root - 2017-12-08 00:07:47.102251: step 60300, loss = 21.39, batch loss = 21.30 (8.7 examples/sec; 0.917 sec/batch; 69h:18m:00s remains)
2017-12-08 00:07:48.189085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6182094 -4.6884036 -4.6546364 -4.5979743 -4.6015649 -4.641428 -4.6769648 -4.6691184 -4.6550126 -4.6857104 -4.7339244 -4.7558312 -4.734446 -4.6723151 -4.5933046][-4.6823268 -4.7243891 -4.6544914 -4.5781674 -4.5807781 -4.6331325 -4.6783624 -4.6688719 -4.6567793 -4.7030015 -4.7694311 -4.8034625 -4.7839932 -4.7208576 -4.6371217][-4.709568 -4.7038341 -4.5945983 -4.5036268 -4.51076 -4.5762429 -4.6239376 -4.606895 -4.5962863 -4.6593943 -4.7472448 -4.7958879 -4.782773 -4.7243495 -4.6440177][-4.678771 -4.6296549 -4.491076 -4.3898659 -4.397182 -4.4644513 -4.5019722 -4.4743977 -4.464993 -4.5475488 -4.66728 -4.7372074 -4.7365451 -4.6908035 -4.6202412][-4.5957747 -4.5279784 -4.3752232 -4.2593746 -4.2506657 -4.2924337 -4.2973838 -4.2526851 -4.25257 -4.3637872 -4.5280089 -4.632163 -4.6570792 -4.6369734 -4.5867529][-4.5153308 -4.4512143 -4.2919755 -4.1521387 -4.1015515 -4.0863094 -4.0377178 -3.965919 -3.9857607 -4.1388016 -4.3528132 -4.5000787 -4.5610204 -4.5784025 -4.559454][-4.4730272 -4.4245467 -4.2755413 -4.1226277 -4.02834 -3.9506223 -3.8316393 -3.7165971 -3.7538915 -3.9449174 -4.1960235 -4.3857875 -4.4823637 -4.532548 -4.5394235][-4.471385 -4.4625916 -4.3458323 -4.1959357 -4.0702572 -3.9412038 -3.7578411 -3.6037934 -3.6539464 -3.870259 -4.1416645 -4.3617086 -4.478971 -4.5353308 -4.5388322][-4.5264683 -4.5654354 -4.4866347 -4.3426003 -4.195755 -4.0382862 -3.8332772 -3.6805608 -3.7401557 -3.9602747 -4.232841 -4.4652491 -4.5835876 -4.6171765 -4.5826635][-4.6145363 -4.6858773 -4.6368442 -4.5028439 -4.3516359 -4.1970415 -4.0163321 -3.8931637 -3.9479649 -4.1428523 -4.3912196 -4.6063075 -4.7102127 -4.717587 -4.6425357][-4.6853366 -4.76953 -4.7473135 -4.6405382 -4.5087175 -4.3788862 -4.243597 -4.1545258 -4.1865845 -4.3310871 -4.526978 -4.6945872 -4.7698541 -4.7602811 -4.6684556][-4.6980958 -4.7725563 -4.7756295 -4.7163205 -4.6328807 -4.5508041 -4.4722109 -4.4155984 -4.4179544 -4.4998474 -4.6271152 -4.7313228 -4.7685323 -4.7421331 -4.6482587][-4.6418266 -4.68984 -4.7066483 -4.6938953 -4.6649809 -4.6335168 -4.6050525 -4.5759029 -4.5600281 -4.5901365 -4.6558924 -4.7081409 -4.7157459 -4.6753025 -4.5870919][-4.5510311 -4.5714211 -4.5867481 -4.5975633 -4.6024861 -4.6030955 -4.6027541 -4.592083 -4.5758095 -4.5791917 -4.6026239 -4.6189971 -4.609479 -4.566072 -4.494215][-4.4413996 -4.4446812 -4.4540558 -4.4677691 -4.4827232 -4.4966955 -4.5082564 -4.510006 -4.5028067 -4.499001 -4.5011868 -4.496727 -4.4780459 -4.441092 -4.39214]]...]
INFO - root - 2017-12-08 00:07:57.483989: step 60310, loss = 21.48, batch loss = 21.40 (8.1 examples/sec; 0.986 sec/batch; 74h:31m:37s remains)
INFO - root - 2017-12-08 00:08:06.904546: step 60320, loss = 21.57, batch loss = 21.49 (8.4 examples/sec; 0.957 sec/batch; 72h:20m:50s remains)
INFO - root - 2017-12-08 00:08:16.322960: step 60330, loss = 21.79, batch loss = 21.71 (8.4 examples/sec; 0.950 sec/batch; 71h:49m:40s remains)
INFO - root - 2017-12-08 00:08:25.776313: step 60340, loss = 21.63, batch loss = 21.55 (8.3 examples/sec; 0.961 sec/batch; 72h:38m:57s remains)
INFO - root - 2017-12-08 00:08:35.226877: step 60350, loss = 21.89, batch loss = 21.81 (8.1 examples/sec; 0.984 sec/batch; 74h:21m:12s remains)
INFO - root - 2017-12-08 00:08:44.660718: step 60360, loss = 21.70, batch loss = 21.62 (8.6 examples/sec; 0.935 sec/batch; 70h:41m:46s remains)
INFO - root - 2017-12-08 00:08:54.128189: step 60370, loss = 21.52, batch loss = 21.43 (8.3 examples/sec; 0.961 sec/batch; 72h:40m:47s remains)
INFO - root - 2017-12-08 00:09:03.298461: step 60380, loss = 21.82, batch loss = 21.73 (8.5 examples/sec; 0.938 sec/batch; 70h:53m:40s remains)
INFO - root - 2017-12-08 00:09:12.679508: step 60390, loss = 21.38, batch loss = 21.30 (8.6 examples/sec; 0.930 sec/batch; 70h:19m:37s remains)
INFO - root - 2017-12-08 00:09:21.993288: step 60400, loss = 20.98, batch loss = 20.90 (8.7 examples/sec; 0.918 sec/batch; 69h:25m:23s remains)
2017-12-08 00:09:22.973253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3024344 -4.1338816 -4.0430951 -4.0987282 -4.2446351 -4.37431 -4.4198132 -4.3877459 -4.3151355 -4.2408457 -4.1968441 -4.1839342 -4.2011619 -4.2153764 -4.2536397][-4.2967792 -4.1681352 -4.1071324 -4.1607733 -4.2962384 -4.4197907 -4.451817 -4.396522 -4.3041248 -4.228385 -4.1932206 -4.1874528 -4.1968689 -4.2045612 -4.2435408][-4.3798118 -4.288331 -4.24202 -4.2727828 -4.3665152 -4.4555821 -4.4646735 -4.3943214 -4.3041472 -4.2533445 -4.24355 -4.2508926 -4.2533607 -4.2492976 -4.2907181][-4.490777 -4.4313169 -4.3897724 -4.3942766 -4.4401946 -4.4785385 -4.4486494 -4.3584061 -4.2783041 -4.2673655 -4.2968554 -4.3229742 -4.324636 -4.3108745 -4.3545108][-4.5864577 -4.5544777 -4.5143209 -4.48897 -4.4801021 -4.453197 -4.3719292 -4.2599945 -4.1980033 -4.2388139 -4.3203831 -4.3746114 -4.3756943 -4.3414555 -4.3688545][-4.6116776 -4.6129923 -4.5746894 -4.5103121 -4.4324675 -4.3353825 -4.2116785 -4.0929756 -4.0621667 -4.1587529 -4.2918506 -4.379169 -4.3913913 -4.3464422 -4.3531146][-4.5787106 -4.60811 -4.5658779 -4.4552412 -4.3031039 -4.13996 -3.9881573 -3.871861 -3.8694038 -4.0100718 -4.18005 -4.3019996 -4.347487 -4.3252277 -4.3349328][-4.5638542 -4.599947 -4.5414953 -4.3969517 -4.190804 -3.9802077 -3.8074145 -3.6777742 -3.6726952 -3.8273878 -4.0148172 -4.1679811 -4.2600074 -4.2863555 -4.3209515][-4.5534906 -4.5746913 -4.5051856 -4.3664551 -4.1645679 -3.9531167 -3.7823303 -3.6417508 -3.6171794 -3.7529557 -3.929914 -4.0906277 -4.2077541 -4.2692757 -4.3246517][-4.5078206 -4.4987245 -4.4277835 -4.3303351 -4.1860533 -4.0242462 -3.8884344 -3.7597239 -3.7232594 -3.825423 -3.9754946 -4.12225 -4.2344251 -4.2991686 -4.3535132][-4.4389019 -4.4050484 -4.3392925 -4.2880092 -4.2129169 -4.1126008 -4.0220542 -3.9209342 -3.8822646 -3.9522159 -4.0718379 -4.1942797 -4.2864981 -4.3401337 -4.3813891][-4.3821325 -4.3366203 -4.2769685 -4.2554932 -4.2300296 -4.1778283 -4.1231537 -4.0560813 -4.0291348 -4.0756564 -4.166297 -4.259593 -4.3297181 -4.3698354 -4.389605][-4.325686 -4.2777286 -4.2268672 -4.224227 -4.2356119 -4.2252769 -4.2089834 -4.18332 -4.1721749 -4.1912351 -4.2415152 -4.2958455 -4.3422613 -4.3667226 -4.3607626][-4.2734504 -4.2181268 -4.1698074 -4.185401 -4.2419682 -4.2892413 -4.3224158 -4.3347511 -4.3283415 -4.3066082 -4.29655 -4.2961721 -4.3071022 -4.3102727 -4.2937779][-4.2692761 -4.1974182 -4.1381369 -4.1573339 -4.2460852 -4.3425703 -4.4120059 -4.4372907 -4.4193506 -4.3600807 -4.2978506 -4.2541027 -4.2383223 -4.2376719 -4.2408485]]...]
INFO - root - 2017-12-08 00:09:32.115472: step 60410, loss = 21.06, batch loss = 20.97 (8.2 examples/sec; 0.979 sec/batch; 74h:01m:12s remains)
INFO - root - 2017-12-08 00:09:41.483394: step 60420, loss = 21.50, batch loss = 21.41 (8.3 examples/sec; 0.965 sec/batch; 72h:56m:13s remains)
INFO - root - 2017-12-08 00:09:51.074061: step 60430, loss = 21.68, batch loss = 21.60 (8.3 examples/sec; 0.966 sec/batch; 73h:01m:25s remains)
INFO - root - 2017-12-08 00:10:00.482156: step 60440, loss = 21.79, batch loss = 21.71 (8.8 examples/sec; 0.909 sec/batch; 68h:39m:31s remains)
INFO - root - 2017-12-08 00:10:09.815222: step 60450, loss = 21.30, batch loss = 21.22 (9.0 examples/sec; 0.892 sec/batch; 67h:24m:26s remains)
INFO - root - 2017-12-08 00:10:19.385364: step 60460, loss = 21.28, batch loss = 21.20 (8.3 examples/sec; 0.967 sec/batch; 73h:02m:18s remains)
INFO - root - 2017-12-08 00:10:28.981285: step 60470, loss = 21.23, batch loss = 21.15 (8.5 examples/sec; 0.942 sec/batch; 71h:10m:42s remains)
INFO - root - 2017-12-08 00:10:38.397707: step 60480, loss = 21.47, batch loss = 21.39 (8.7 examples/sec; 0.920 sec/batch; 69h:33m:12s remains)
INFO - root - 2017-12-08 00:10:47.821253: step 60490, loss = 21.25, batch loss = 21.17 (8.7 examples/sec; 0.922 sec/batch; 69h:39m:05s remains)
INFO - root - 2017-12-08 00:10:57.327685: step 60500, loss = 21.50, batch loss = 21.41 (8.2 examples/sec; 0.973 sec/batch; 73h:32m:34s remains)
2017-12-08 00:10:58.256268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9526334 -4.9644842 -4.8741751 -4.7349586 -4.6099744 -4.5347323 -4.5032983 -4.4932775 -4.5254197 -4.5972371 -4.6799288 -4.7671027 -4.8197875 -4.7980933 -4.726788][-4.949954 -4.9527926 -4.8395529 -4.681047 -4.5464373 -4.4787292 -4.4598293 -4.447443 -4.4692063 -4.5418062 -4.64296 -4.7598629 -4.8346734 -4.8202343 -4.7430882][-4.9477139 -4.9355307 -4.7904844 -4.6004567 -4.4442348 -4.3749928 -4.3705087 -4.3664865 -4.3876505 -4.4670606 -4.5922394 -4.739255 -4.8370528 -4.8332844 -4.7555041][-4.9260535 -4.8850923 -4.7001176 -4.4714317 -4.2863674 -4.1970716 -4.1926732 -4.2028146 -4.2445045 -4.3506269 -4.5099845 -4.6897497 -4.8138814 -4.8306713 -4.7644582][-4.867878 -4.8004212 -4.581563 -4.317873 -4.1048565 -3.9900157 -3.9761486 -4.0064487 -4.0849371 -4.2265582 -4.4122624 -4.6110754 -4.754384 -4.7951217 -4.7486038][-4.8122544 -4.7321935 -4.4950438 -4.2037382 -3.9623961 -3.8177714 -3.7908435 -3.8450119 -3.9713645 -4.153182 -4.3470435 -4.5330172 -4.6656089 -4.7128739 -4.682373][-4.76733 -4.7005734 -4.4741931 -4.1742573 -3.906925 -3.7323151 -3.6961608 -3.7800341 -3.9591272 -4.1730204 -4.3526506 -4.4930787 -4.5791864 -4.6060405 -4.5791469][-4.7145982 -4.6961217 -4.5165296 -4.2377892 -3.9629314 -3.7784047 -3.7456841 -3.8511536 -4.0521793 -4.2634768 -4.4105573 -4.4950771 -4.5237532 -4.5161166 -4.4833][-4.659019 -4.6983204 -4.5823236 -4.3481631 -4.0996 -3.9359548 -3.911859 -4.0078139 -4.1740317 -4.3377376 -4.4390531 -4.481391 -4.4737759 -4.4431257 -4.4091468][-4.6333365 -4.7031188 -4.6343679 -4.4486823 -4.25152 -4.1309042 -4.1105824 -4.1618514 -4.247191 -4.335597 -4.3947167 -4.4200969 -4.4093442 -4.3775263 -4.3506975][-4.6441727 -4.7042513 -4.6517591 -4.5079994 -4.3696041 -4.2961292 -4.2682929 -4.25787 -4.2570395 -4.2840476 -4.3284869 -4.3697238 -4.3833447 -4.3635 -4.3404484][-4.6493979 -4.675559 -4.6313863 -4.5391355 -4.4628935 -4.4280353 -4.3880072 -4.3252058 -4.2655444 -4.2626467 -4.3155932 -4.3848782 -4.4257197 -4.4152732 -4.3883691][-4.6044712 -4.6044393 -4.5921574 -4.5732617 -4.5632057 -4.556931 -4.5061116 -4.4124589 -4.3285661 -4.322844 -4.3911219 -4.4763141 -4.528192 -4.5174904 -4.48291][-4.487505 -4.4861751 -4.5376949 -4.6153455 -4.6718121 -4.6804471 -4.6199403 -4.5201645 -4.445899 -4.4539042 -4.5273914 -4.6048384 -4.644537 -4.627914 -4.5893044][-4.3750253 -4.3902874 -4.4988585 -4.6472569 -4.7482905 -4.7657528 -4.7100635 -4.6305046 -4.5858278 -4.606204 -4.6630349 -4.7105017 -4.7250113 -4.6999955 -4.6604419]]...]
INFO - root - 2017-12-08 00:11:07.548849: step 60510, loss = 21.56, batch loss = 21.47 (8.2 examples/sec; 0.972 sec/batch; 73h:27m:06s remains)
INFO - root - 2017-12-08 00:11:17.146292: step 60520, loss = 21.73, batch loss = 21.65 (8.1 examples/sec; 0.989 sec/batch; 74h:44m:24s remains)
INFO - root - 2017-12-08 00:11:26.519628: step 60530, loss = 20.99, batch loss = 20.91 (8.0 examples/sec; 1.004 sec/batch; 75h:48m:45s remains)
INFO - root - 2017-12-08 00:11:35.983546: step 60540, loss = 21.58, batch loss = 21.50 (7.9 examples/sec; 1.018 sec/batch; 76h:53m:01s remains)
INFO - root - 2017-12-08 00:11:45.521893: step 60550, loss = 21.10, batch loss = 21.02 (8.3 examples/sec; 0.968 sec/batch; 73h:08m:33s remains)
INFO - root - 2017-12-08 00:11:54.810397: step 60560, loss = 21.17, batch loss = 21.09 (9.2 examples/sec; 0.874 sec/batch; 66h:00m:49s remains)
INFO - root - 2017-12-08 00:12:04.207972: step 60570, loss = 21.27, batch loss = 21.19 (8.1 examples/sec; 0.988 sec/batch; 74h:35m:35s remains)
INFO - root - 2017-12-08 00:12:13.727076: step 60580, loss = 21.67, batch loss = 21.58 (8.1 examples/sec; 0.989 sec/batch; 74h:43m:57s remains)
INFO - root - 2017-12-08 00:12:22.908487: step 60590, loss = 21.43, batch loss = 21.35 (8.3 examples/sec; 0.960 sec/batch; 72h:31m:03s remains)
INFO - root - 2017-12-08 00:12:32.268044: step 60600, loss = 21.50, batch loss = 21.42 (8.8 examples/sec; 0.907 sec/batch; 68h:32m:18s remains)
2017-12-08 00:12:33.253916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3690472 -4.3431544 -4.3023624 -4.2785478 -4.2925229 -4.3326473 -4.3901305 -4.46588 -4.5483794 -4.6166544 -4.6407876 -4.60326 -4.5531821 -4.5506773 -4.5598283][-4.3363147 -4.3492713 -4.3405833 -4.3216124 -4.314405 -4.3259363 -4.3646207 -4.435708 -4.5142369 -4.5823426 -4.6164718 -4.594717 -4.557333 -4.5540314 -4.5563574][-4.3149953 -4.3674955 -4.3998747 -4.3951354 -4.3680544 -4.3452754 -4.3626213 -4.4315696 -4.5127449 -4.5768366 -4.609149 -4.5962372 -4.574367 -4.5793009 -4.5816851][-4.3136845 -4.3839393 -4.4462643 -4.4586511 -4.4186988 -4.3614321 -4.3497334 -4.4092221 -4.4883609 -4.5450807 -4.5678163 -4.5557084 -4.5483575 -4.5678382 -4.5779653][-4.2920513 -4.3510184 -4.4224811 -4.4495764 -4.4063416 -4.3205347 -4.274343 -4.3184538 -4.3983784 -4.458714 -4.4818892 -4.4783025 -4.4929228 -4.53366 -4.5579929][-4.2277889 -4.25761 -4.3210888 -4.3532872 -4.3057275 -4.1971622 -4.1221051 -4.1566334 -4.2537994 -4.3403225 -4.3863158 -4.4085116 -4.4514341 -4.5109391 -4.5465436][-4.158217 -4.1598182 -4.2089152 -4.2357516 -4.1800375 -4.0553913 -3.9613607 -3.9952962 -4.1197991 -4.2443933 -4.3254724 -4.3830905 -4.4492145 -4.5162673 -4.5540915][-4.0931153 -4.0811558 -4.1200461 -4.1413355 -4.0852442 -3.9629955 -3.8689854 -3.9065564 -4.0478411 -4.1978745 -4.3106923 -4.4016409 -4.4832349 -4.5493569 -4.5803313][-4.0489521 -4.042902 -4.082613 -4.1063571 -4.0598774 -3.9568915 -3.8768051 -3.908042 -4.0358105 -4.1840591 -4.3145041 -4.4327607 -4.5285974 -4.5917521 -4.6124196][-4.0596561 -4.0651875 -4.1117945 -4.1444292 -4.117219 -4.0454812 -3.9842043 -3.9923782 -4.0747848 -4.1911607 -4.3186655 -4.4496202 -4.555366 -4.619617 -4.6342988][-4.1027303 -4.1131926 -4.1608863 -4.2021747 -4.2011385 -4.1650748 -4.1215458 -4.1067429 -4.1396804 -4.2140827 -4.3216009 -4.44935 -4.5603294 -4.6287832 -4.6424532][-4.1699324 -4.1806254 -4.2163053 -4.2499132 -4.2584033 -4.241889 -4.2095723 -4.1859746 -4.1917048 -4.2337184 -4.3155103 -4.432416 -4.5473304 -4.6216011 -4.6376023][-4.2729578 -4.2823558 -4.298233 -4.312655 -4.3172159 -4.3046961 -4.2743506 -4.2505913 -4.244688 -4.2602353 -4.3109131 -4.4089007 -4.5235615 -4.6024742 -4.6215014][-4.3760409 -4.3824744 -4.3841457 -4.3861904 -4.3897796 -4.377986 -4.3465557 -4.3234 -4.3110924 -4.3025155 -4.3170938 -4.3905916 -4.49848 -4.5799351 -4.60348][-4.4944983 -4.4898777 -4.47853 -4.4659405 -4.4567609 -4.4327326 -4.3900332 -4.3592081 -4.3377762 -4.3061566 -4.2882276 -4.3374519 -4.4370446 -4.5249419 -4.5610542]]...]
INFO - root - 2017-12-08 00:12:42.746167: step 60610, loss = 21.24, batch loss = 21.16 (8.5 examples/sec; 0.946 sec/batch; 71h:28m:11s remains)
INFO - root - 2017-12-08 00:12:51.899222: step 60620, loss = 21.49, batch loss = 21.41 (8.7 examples/sec; 0.923 sec/batch; 69h:44m:29s remains)
INFO - root - 2017-12-08 00:13:01.188795: step 60630, loss = 21.30, batch loss = 21.22 (9.0 examples/sec; 0.889 sec/batch; 67h:06m:37s remains)
INFO - root - 2017-12-08 00:13:10.608484: step 60640, loss = 21.85, batch loss = 21.77 (8.8 examples/sec; 0.910 sec/batch; 68h:43m:38s remains)
INFO - root - 2017-12-08 00:13:20.023351: step 60650, loss = 20.96, batch loss = 20.88 (8.5 examples/sec; 0.943 sec/batch; 71h:12m:54s remains)
INFO - root - 2017-12-08 00:13:29.461805: step 60660, loss = 21.59, batch loss = 21.50 (8.3 examples/sec; 0.963 sec/batch; 72h:41m:35s remains)
INFO - root - 2017-12-08 00:13:38.837562: step 60670, loss = 21.48, batch loss = 21.40 (7.8 examples/sec; 1.025 sec/batch; 77h:24m:20s remains)
INFO - root - 2017-12-08 00:13:48.199378: step 60680, loss = 21.23, batch loss = 21.14 (8.6 examples/sec; 0.929 sec/batch; 70h:07m:03s remains)
INFO - root - 2017-12-08 00:13:57.523231: step 60690, loss = 21.29, batch loss = 21.20 (8.5 examples/sec; 0.947 sec/batch; 71h:28m:22s remains)
INFO - root - 2017-12-08 00:14:07.093490: step 60700, loss = 21.47, batch loss = 21.39 (8.2 examples/sec; 0.975 sec/batch; 73h:34m:39s remains)
2017-12-08 00:14:08.108496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4695816 -4.4655566 -4.4626355 -4.4593973 -4.4586725 -4.4615116 -4.4658823 -4.469862 -4.4747863 -4.4810181 -4.4861016 -4.4847465 -4.475234 -4.4591103 -4.4410305][-4.5551839 -4.5472412 -4.5405684 -4.5341525 -4.5318675 -4.5357056 -4.5444336 -4.5549054 -4.5646834 -4.5729036 -4.5754137 -4.5666094 -4.5477562 -4.5246038 -4.5016589][-4.5835319 -4.564858 -4.5564828 -4.552022 -4.5491347 -4.5508752 -4.5607142 -4.5747223 -4.584414 -4.5883379 -4.5834522 -4.5666766 -4.5432072 -4.5190749 -4.496942][-4.5239477 -4.4875174 -4.4802618 -4.4841452 -4.483973 -4.4818211 -4.4877276 -4.4979019 -4.4991679 -4.4927273 -4.4798822 -4.460897 -4.4413657 -4.4231486 -4.4065065][-4.3979578 -4.3411574 -4.3329973 -4.3473835 -4.3564596 -4.3562031 -4.3585763 -4.3606272 -4.3486681 -4.3317256 -4.3172264 -4.30607 -4.29818 -4.2886834 -4.2777243][-4.2409596 -4.173069 -4.1586294 -4.174274 -4.1919804 -4.200429 -4.2044921 -4.1994171 -4.1767521 -4.1559033 -4.1487894 -4.1546874 -4.1653013 -4.1678696 -4.1645355][-4.1138577 -4.0459185 -4.0101347 -4.003489 -4.0153432 -4.0337038 -4.048068 -4.0489612 -4.0331621 -4.0232387 -4.032969 -4.0594134 -4.0883408 -4.1045966 -4.1136575][-4.0688143 -3.9965076 -3.9251924 -3.8791769 -3.8719265 -3.8951142 -3.9242127 -3.945961 -3.9601181 -3.9813232 -4.0166965 -4.0603824 -4.0987797 -4.1218743 -4.1400323][-4.099977 -4.0252709 -3.9249685 -3.8462076 -3.8221245 -3.8473592 -3.8904362 -3.9366467 -3.9852393 -4.0397916 -4.0986876 -4.1526046 -4.1892381 -4.2064667 -4.2201624][-4.1487293 -4.0758758 -3.9669678 -3.8810165 -3.8566828 -3.8867736 -3.9379296 -3.996104 -4.0618653 -4.1351757 -4.208498 -4.2661595 -4.2955313 -4.2992487 -4.2974496][-4.1558762 -4.0946503 -4.0060992 -3.9428091 -3.9362411 -3.9761672 -4.03031 -4.0846653 -4.1435146 -4.2134471 -4.285913 -4.3398356 -4.3610625 -4.352078 -4.3338957][-4.1444325 -4.107214 -4.0544028 -4.0224333 -4.0340033 -4.0806961 -4.13223 -4.1714969 -4.20745 -4.2558088 -4.3109245 -4.35156 -4.3638735 -4.3474188 -4.3196745][-4.1568542 -4.1468115 -4.1248894 -4.1124988 -4.129559 -4.1763086 -4.223805 -4.248971 -4.2631326 -4.2840385 -4.3111224 -4.3298192 -4.3311291 -4.311408 -4.28274][-4.18924 -4.2070489 -4.2105222 -4.2065468 -4.2180319 -4.2573133 -4.2982097 -4.3153095 -4.3167658 -4.3154106 -4.3150549 -4.311523 -4.3015771 -4.2797379 -4.25336][-4.2133842 -4.25787 -4.2814617 -4.283761 -4.2901168 -4.3207231 -4.3553586 -4.3681087 -4.3610082 -4.342433 -4.3208127 -4.3009205 -4.2836852 -4.2617679 -4.23957]]...]
INFO - root - 2017-12-08 00:14:17.382354: step 60710, loss = 21.42, batch loss = 21.33 (9.3 examples/sec; 0.862 sec/batch; 65h:02m:46s remains)
INFO - root - 2017-12-08 00:14:26.724981: step 60720, loss = 20.90, batch loss = 20.82 (8.6 examples/sec; 0.934 sec/batch; 70h:30m:32s remains)
INFO - root - 2017-12-08 00:14:36.200541: step 60730, loss = 21.07, batch loss = 20.99 (8.4 examples/sec; 0.947 sec/batch; 71h:29m:40s remains)
INFO - root - 2017-12-08 00:14:45.558010: step 60740, loss = 21.65, batch loss = 21.57 (8.3 examples/sec; 0.966 sec/batch; 72h:55m:09s remains)
INFO - root - 2017-12-08 00:14:54.880851: step 60750, loss = 21.26, batch loss = 21.18 (9.0 examples/sec; 0.893 sec/batch; 67h:24m:17s remains)
INFO - root - 2017-12-08 00:15:04.054473: step 60760, loss = 21.54, batch loss = 21.46 (9.1 examples/sec; 0.880 sec/batch; 66h:26m:51s remains)
INFO - root - 2017-12-08 00:15:13.490337: step 60770, loss = 21.21, batch loss = 21.13 (8.7 examples/sec; 0.916 sec/batch; 69h:07m:47s remains)
INFO - root - 2017-12-08 00:15:22.956422: step 60780, loss = 21.68, batch loss = 21.60 (8.3 examples/sec; 0.966 sec/batch; 72h:53m:20s remains)
INFO - root - 2017-12-08 00:15:32.383740: step 60790, loss = 21.30, batch loss = 21.22 (8.2 examples/sec; 0.978 sec/batch; 73h:48m:23s remains)
INFO - root - 2017-12-08 00:15:41.815949: step 60800, loss = 21.49, batch loss = 21.40 (8.0 examples/sec; 0.999 sec/batch; 75h:21m:57s remains)
2017-12-08 00:15:42.715343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3369184 -4.3353419 -4.3347955 -4.3321404 -4.3351135 -4.3454676 -4.3577247 -4.3655705 -4.3599472 -4.3408647 -4.3166 -4.2968583 -4.2843332 -4.282908 -4.2888036][-4.4422212 -4.4457388 -4.4456234 -4.4411788 -4.4431868 -4.45264 -4.4653463 -4.4773011 -4.4736919 -4.44557 -4.4034414 -4.3655124 -4.3432603 -4.3444614 -4.361496][-4.5468469 -4.5540915 -4.553658 -4.54715 -4.5472341 -4.5510311 -4.5574546 -4.570734 -4.5748239 -4.5488696 -4.49909 -4.4514 -4.4280081 -4.4393234 -4.4722128][-4.59808 -4.5979557 -4.5899668 -4.5794396 -4.5785389 -4.57638 -4.5732646 -4.58267 -4.6004238 -4.600965 -4.5737729 -4.5421085 -4.5339518 -4.5598717 -4.6053977][-4.5859303 -4.5626817 -4.5329785 -4.5108962 -4.5086465 -4.5056839 -4.4945807 -4.4889903 -4.5176516 -4.5735941 -4.6148405 -4.641643 -4.6710815 -4.7081113 -4.7496858][-4.5286946 -4.4716425 -4.4119277 -4.3724594 -4.3659306 -4.3669486 -4.3457704 -4.3016791 -4.32268 -4.4474187 -4.593286 -4.7166891 -4.8011189 -4.841135 -4.8606749][-4.4782491 -4.3928547 -4.3011174 -4.2310748 -4.1962357 -4.1820021 -4.129477 -4.0163975 -4.0119085 -4.2046428 -4.4602432 -4.6912513 -4.8349032 -4.8792667 -4.881361][-4.486989 -4.4030504 -4.3018794 -4.1983461 -4.10847 -4.0396252 -3.9138675 -3.7092919 -3.6744409 -3.9209642 -4.2577147 -4.5655551 -4.7503657 -4.8031449 -4.8067803][-4.5369663 -4.4843569 -4.4008856 -4.2810969 -4.1439991 -4.0118251 -3.7999809 -3.5117736 -3.4502349 -3.7150908 -4.0816879 -4.4149165 -4.6121888 -4.67367 -4.6915269][-4.5879788 -4.5807729 -4.5334463 -4.4291639 -4.2849464 -4.1243796 -3.8689759 -3.5502632 -3.4727921 -3.7059443 -4.0371294 -4.3414135 -4.5236216 -4.5870514 -4.6144519][-4.6192846 -4.648324 -4.6403832 -4.5802913 -4.4806371 -4.3481021 -4.1187339 -3.8336504 -3.7498009 -3.919306 -4.178741 -4.4215779 -4.5695524 -4.6203566 -4.6360679][-4.6215615 -4.6604066 -4.6732693 -4.6595292 -4.6260071 -4.5601892 -4.4133406 -4.2083368 -4.1292772 -4.2320995 -4.4136353 -4.5839138 -4.6864872 -4.7176933 -4.7143903][-4.5932355 -4.6220946 -4.6356182 -4.6502752 -4.6649075 -4.6586637 -4.5963707 -4.4775429 -4.412765 -4.4652867 -4.5857153 -4.6959338 -4.7582092 -4.7756104 -4.7680869][-4.5254178 -4.5374231 -4.5407362 -4.5645909 -4.6032195 -4.63248 -4.6266146 -4.5728846 -4.5292277 -4.556273 -4.6347885 -4.7015071 -4.7338018 -4.743938 -4.7452631][-4.42776 -4.4289412 -4.4247503 -4.4455585 -4.4864469 -4.52663 -4.5458131 -4.52923 -4.5058613 -4.5205674 -4.5669584 -4.6031628 -4.6176972 -4.6259975 -4.6410336]]...]
INFO - root - 2017-12-08 00:15:52.102772: step 60810, loss = 21.62, batch loss = 21.54 (8.3 examples/sec; 0.967 sec/batch; 72h:57m:06s remains)
INFO - root - 2017-12-08 00:16:01.405732: step 60820, loss = 21.24, batch loss = 21.15 (9.0 examples/sec; 0.891 sec/batch; 67h:14m:18s remains)
INFO - root - 2017-12-08 00:16:10.895837: step 60830, loss = 21.45, batch loss = 21.37 (8.7 examples/sec; 0.916 sec/batch; 69h:07m:09s remains)
INFO - root - 2017-12-08 00:16:20.227775: step 60840, loss = 21.89, batch loss = 21.80 (9.1 examples/sec; 0.878 sec/batch; 66h:14m:48s remains)
INFO - root - 2017-12-08 00:16:29.555946: step 60850, loss = 21.16, batch loss = 21.07 (8.7 examples/sec; 0.923 sec/batch; 69h:40m:57s remains)
INFO - root - 2017-12-08 00:16:38.764846: step 60860, loss = 21.18, batch loss = 21.10 (8.0 examples/sec; 0.996 sec/batch; 75h:10m:41s remains)
INFO - root - 2017-12-08 00:16:47.908503: step 60870, loss = 21.29, batch loss = 21.21 (8.3 examples/sec; 0.960 sec/batch; 72h:26m:29s remains)
INFO - root - 2017-12-08 00:16:57.259737: step 60880, loss = 21.52, batch loss = 21.43 (8.3 examples/sec; 0.964 sec/batch; 72h:43m:14s remains)
INFO - root - 2017-12-08 00:17:06.606251: step 60890, loss = 21.51, batch loss = 21.43 (8.1 examples/sec; 0.989 sec/batch; 74h:37m:44s remains)
INFO - root - 2017-12-08 00:17:15.974337: step 60900, loss = 21.54, batch loss = 21.46 (8.8 examples/sec; 0.913 sec/batch; 68h:52m:20s remains)
2017-12-08 00:17:16.859322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3992405 -4.4629016 -4.530726 -4.5739465 -4.647624 -4.7459812 -4.8005052 -4.8348622 -4.8545656 -4.8421068 -4.7745991 -4.6758995 -4.5952172 -4.5362554 -4.5023012][-4.3824673 -4.4457536 -4.5310464 -4.5917163 -4.6681209 -4.7601357 -4.8088369 -4.8396645 -4.8721952 -4.8855171 -4.8360119 -4.72297 -4.6038346 -4.5182652 -4.4786639][-4.4263496 -4.4947376 -4.5957847 -4.6557341 -4.6879325 -4.7022815 -4.6770954 -4.6728287 -4.7348161 -4.816617 -4.8322082 -4.7431269 -4.6050963 -4.4908347 -4.4383693][-4.5057945 -4.5934129 -4.7088761 -4.7420597 -4.6890664 -4.5776582 -4.4346771 -4.378531 -4.4829984 -4.6574407 -4.7620678 -4.7305322 -4.607079 -4.4838047 -4.4216013][-4.5659494 -4.6813517 -4.8008347 -4.7918763 -4.6481895 -4.4115376 -4.1449161 -4.0346823 -4.1837735 -4.4441819 -4.6300983 -4.6709123 -4.5977287 -4.4960842 -4.4366918][-4.5717773 -4.7104969 -4.8193126 -4.768774 -4.5526018 -4.2174215 -3.8420386 -3.6805773 -3.8701386 -4.2005348 -4.4502015 -4.5562253 -4.5380373 -4.47013 -4.4293733][-4.5484591 -4.6998377 -4.7907386 -4.7071667 -4.4362941 -4.0291533 -3.5696034 -3.3609807 -3.5867329 -3.9788198 -4.2824416 -4.4350157 -4.4463339 -4.3933759 -4.3735209][-4.5382948 -4.6942239 -4.7715955 -4.66801 -4.3651533 -3.9236786 -3.4336333 -3.2034135 -3.4602952 -3.897105 -4.233654 -4.3961215 -4.3881235 -4.30969 -4.2959743][-4.533618 -4.7013869 -4.7917418 -4.7067037 -4.4164476 -3.9904079 -3.5258727 -3.3009019 -3.5661097 -4.0144529 -4.3528142 -4.4888339 -4.4202747 -4.2804289 -4.2461009][-4.5152826 -4.7001758 -4.82461 -4.7945261 -4.5639296 -4.197979 -3.795907 -3.5963206 -3.8423896 -4.2617559 -4.5643415 -4.6499224 -4.5204768 -4.3216882 -4.249095][-4.5067306 -4.6963348 -4.8412771 -4.8609366 -4.7027469 -4.4213939 -4.1117034 -3.963851 -4.1737432 -4.5210981 -4.75161 -4.778255 -4.6148567 -4.3951397 -4.2922726][-4.51671 -4.6847548 -4.8177061 -4.8566566 -4.7604952 -4.5756488 -4.3762975 -4.2968092 -4.4598093 -4.705595 -4.8460636 -4.8200855 -4.6533303 -4.4509993 -4.3415551][-4.5201063 -4.65278 -4.7553258 -4.78823 -4.7383051 -4.6434669 -4.5499477 -4.5291009 -4.6362672 -4.7734594 -4.8302059 -4.7722826 -4.6265407 -4.4674115 -4.3762631][-4.5055 -4.5993776 -4.6694827 -4.6895895 -4.6676679 -4.6327209 -4.6055655 -4.6137581 -4.6720281 -4.7305584 -4.7363052 -4.6726842 -4.5608287 -4.4502044 -4.3874083][-4.4814315 -4.5367794 -4.5796285 -4.59176 -4.5840726 -4.5702882 -4.5593295 -4.5670829 -4.5960264 -4.6227202 -4.6211038 -4.5801473 -4.5102839 -4.4427791 -4.4042377]]...]
INFO - root - 2017-12-08 00:17:26.203063: step 60910, loss = 21.32, batch loss = 21.23 (9.1 examples/sec; 0.880 sec/batch; 66h:22m:15s remains)
INFO - root - 2017-12-08 00:17:35.484427: step 60920, loss = 21.40, batch loss = 21.32 (9.3 examples/sec; 0.860 sec/batch; 64h:54m:50s remains)
INFO - root - 2017-12-08 00:17:44.695247: step 60930, loss = 21.50, batch loss = 21.42 (8.3 examples/sec; 0.960 sec/batch; 72h:24m:41s remains)
INFO - root - 2017-12-08 00:17:54.115430: step 60940, loss = 21.58, batch loss = 21.50 (8.1 examples/sec; 0.991 sec/batch; 74h:46m:57s remains)
INFO - root - 2017-12-08 00:18:03.509504: step 60950, loss = 21.84, batch loss = 21.76 (9.0 examples/sec; 0.893 sec/batch; 67h:21m:52s remains)
INFO - root - 2017-12-08 00:18:12.906233: step 60960, loss = 21.76, batch loss = 21.68 (8.8 examples/sec; 0.909 sec/batch; 68h:33m:53s remains)
INFO - root - 2017-12-08 00:18:22.255847: step 60970, loss = 21.47, batch loss = 21.38 (8.9 examples/sec; 0.900 sec/batch; 67h:51m:58s remains)
INFO - root - 2017-12-08 00:18:31.583433: step 60980, loss = 21.34, batch loss = 21.25 (8.8 examples/sec; 0.906 sec/batch; 68h:19m:54s remains)
INFO - root - 2017-12-08 00:18:40.904402: step 60990, loss = 21.53, batch loss = 21.45 (8.7 examples/sec; 0.923 sec/batch; 69h:36m:49s remains)
INFO - root - 2017-12-08 00:18:50.281993: step 61000, loss = 21.81, batch loss = 21.73 (8.8 examples/sec; 0.911 sec/batch; 68h:41m:00s remains)
2017-12-08 00:18:51.203197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3572793 -4.2734294 -4.1622066 -4.0723333 -4.0333924 -4.0582328 -4.1466231 -4.2375031 -4.3065138 -4.3325338 -4.3240032 -4.3123245 -4.3077025 -4.3058896 -4.3065581][-4.4237337 -4.3561926 -4.2603374 -4.1822796 -4.1459126 -4.1631789 -4.2331367 -4.3081384 -4.3649259 -4.3828878 -4.3695164 -4.3498654 -4.3319335 -4.315258 -4.3072596][-4.4829025 -4.4473081 -4.3785548 -4.3142 -4.2721381 -4.2672315 -4.306879 -4.3632579 -4.4141221 -4.4348536 -4.4264727 -4.404182 -4.3737721 -4.3422909 -4.3267469][-4.4947591 -4.4845929 -4.4334283 -4.3691754 -4.3100286 -4.2766314 -4.2879229 -4.3366404 -4.4023962 -4.4512539 -4.4691577 -4.4584832 -4.4232569 -4.3815026 -4.3606586][-4.4425735 -4.4336014 -4.3743386 -4.28981 -4.2048821 -4.146575 -4.1411943 -4.1970258 -4.2960491 -4.3942981 -4.4592929 -4.4788671 -4.4528694 -4.4091868 -4.3848629][-4.3619409 -4.335763 -4.2510357 -4.1360192 -4.0247951 -3.9492757 -3.9371345 -4.0044479 -4.1368642 -4.28353 -4.3964062 -4.4521041 -4.4437466 -4.4052372 -4.3791595][-4.3089261 -4.2653303 -4.1615744 -4.0246158 -3.8958266 -3.8094537 -3.7910938 -3.8605547 -4.0076013 -4.1785545 -4.3167858 -4.3956275 -4.4031448 -4.3746023 -4.35196][-4.2936029 -4.2474475 -4.14454 -4.0050097 -3.8719592 -3.7802444 -3.7533855 -3.8126974 -3.9518626 -4.1165738 -4.2501578 -4.3305616 -4.3470192 -4.33243 -4.3216767][-4.294837 -4.2584586 -4.172646 -4.0489221 -3.9277573 -3.8438151 -3.8177249 -3.8668411 -3.9838061 -4.1178017 -4.2215972 -4.2827921 -4.2984805 -4.2970214 -4.3026342][-4.2955055 -4.2729378 -4.2144113 -4.124537 -4.0366406 -3.9800353 -3.9678807 -4.0085244 -4.0914311 -4.1776266 -4.2377262 -4.2698298 -4.2789297 -4.286911 -4.3063045][-4.2912312 -4.2823868 -4.2539635 -4.2060933 -4.1610966 -4.1384249 -4.1412621 -4.1695566 -4.212729 -4.2491031 -4.2698197 -4.2793436 -4.2843056 -4.2970386 -4.3219376][-4.2836986 -4.2856588 -4.2832055 -4.2719817 -4.2630067 -4.26496 -4.2749929 -4.2894855 -4.3003058 -4.3004293 -4.2954817 -4.2930374 -4.2967329 -4.309032 -4.3297043][-4.2746558 -4.2833943 -4.2980609 -4.31116 -4.3243971 -4.3377914 -4.3462024 -4.3473372 -4.3374128 -4.3185067 -4.3023672 -4.2967358 -4.3014493 -4.3121338 -4.3246307][-4.2640495 -4.2731123 -4.291975 -4.3120079 -4.3298283 -4.3416333 -4.3434653 -4.3359985 -4.3196268 -4.2997589 -4.2858434 -4.2826772 -4.288271 -4.2966576 -4.3022847][-4.2521505 -4.2541442 -4.2651415 -4.2771306 -4.2872877 -4.2927175 -4.2917519 -4.2858896 -4.2762365 -4.2661457 -4.2601023 -4.2598996 -4.2637024 -4.2678838 -4.26907]]...]
INFO - root - 2017-12-08 00:19:00.590760: step 61010, loss = 21.02, batch loss = 20.94 (8.3 examples/sec; 0.968 sec/batch; 73h:02m:08s remains)
INFO - root - 2017-12-08 00:19:09.912701: step 61020, loss = 21.54, batch loss = 21.45 (8.3 examples/sec; 0.958 sec/batch; 72h:15m:16s remains)
INFO - root - 2017-12-08 00:19:19.100416: step 61030, loss = 21.59, batch loss = 21.51 (9.3 examples/sec; 0.864 sec/batch; 65h:09m:41s remains)
INFO - root - 2017-12-08 00:19:28.485980: step 61040, loss = 21.78, batch loss = 21.70 (9.1 examples/sec; 0.882 sec/batch; 66h:30m:24s remains)
INFO - root - 2017-12-08 00:19:37.793757: step 61050, loss = 21.19, batch loss = 21.10 (8.5 examples/sec; 0.942 sec/batch; 71h:01m:38s remains)
INFO - root - 2017-12-08 00:19:47.325796: step 61060, loss = 21.62, batch loss = 21.53 (8.5 examples/sec; 0.943 sec/batch; 71h:05m:15s remains)
INFO - root - 2017-12-08 00:19:56.574573: step 61070, loss = 21.66, batch loss = 21.58 (8.8 examples/sec; 0.906 sec/batch; 68h:19m:22s remains)
INFO - root - 2017-12-08 00:20:05.891004: step 61080, loss = 21.36, batch loss = 21.27 (8.5 examples/sec; 0.937 sec/batch; 70h:37m:36s remains)
INFO - root - 2017-12-08 00:20:15.272001: step 61090, loss = 21.57, batch loss = 21.48 (8.6 examples/sec; 0.932 sec/batch; 70h:14m:22s remains)
INFO - root - 2017-12-08 00:20:24.439842: step 61100, loss = 21.60, batch loss = 21.52 (9.0 examples/sec; 0.887 sec/batch; 66h:50m:10s remains)
2017-12-08 00:20:25.468744: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3901172 -4.3773069 -4.3552809 -4.3249273 -4.2972565 -4.2800803 -4.2683325 -4.2572122 -4.2450881 -4.2326016 -4.2235665 -4.2153673 -4.20438 -4.1852689 -4.1544833][-4.4067807 -4.40111 -4.3918986 -4.3774781 -4.3636346 -4.3525162 -4.3401475 -4.3239708 -4.3023868 -4.2780614 -4.2582583 -4.241446 -4.2228818 -4.1960258 -4.1562014][-4.4285765 -4.4434986 -4.4433117 -4.4310851 -4.418685 -4.410831 -4.4045682 -4.3956003 -4.3787584 -4.35407 -4.3285742 -4.3034058 -4.2764916 -4.2425189 -4.1969018][-4.4721088 -4.4945183 -4.4814491 -4.4491878 -4.4210129 -4.4058805 -4.4067121 -4.4196544 -4.4323053 -4.434525 -4.4232349 -4.3999686 -4.370481 -4.3345857 -4.2904367][-4.4898553 -4.4992719 -4.4577951 -4.3942175 -4.33878 -4.3012161 -4.2974691 -4.3346128 -4.3960972 -4.455596 -4.4873991 -4.4857264 -4.4656563 -4.4352603 -4.3982277][-4.4371071 -4.429657 -4.3658171 -4.279355 -4.1945186 -4.1184564 -4.0927582 -4.1439261 -4.2552943 -4.3833547 -4.4709668 -4.4990339 -4.4920096 -4.4685268 -4.4397721][-4.3325663 -4.3207922 -4.248426 -4.1496568 -4.0347042 -3.9157717 -3.8632598 -3.9211016 -4.0704851 -4.2493172 -4.3740869 -4.4176865 -4.4170418 -4.4014368 -4.3838463][-4.2235541 -4.2174096 -4.14419 -4.0381417 -3.8978136 -3.7441037 -3.6747131 -3.7435136 -3.9192681 -4.1194715 -4.2475886 -4.2858953 -4.2838368 -4.2758651 -4.2671027][-4.1704187 -4.1795163 -4.1094594 -3.9998839 -3.8428009 -3.6654887 -3.5823057 -3.6550181 -3.8411827 -4.0382071 -4.1526265 -4.1814756 -4.1765 -4.1715951 -4.1607356][-4.1735697 -4.2114968 -4.1631908 -4.0688648 -3.9197459 -3.742296 -3.6494722 -3.7050493 -3.8670678 -4.0306292 -4.1228948 -4.1515012 -4.1501203 -4.1441379 -4.12249][-4.2232332 -4.2929649 -4.2878542 -4.2378497 -4.1311436 -3.9865994 -3.8972869 -3.9237676 -4.0348549 -4.1435394 -4.2056003 -4.2287726 -4.22143 -4.1993394 -4.1550932][-4.3426785 -4.4307504 -4.4655766 -4.4591665 -4.3944383 -4.2843008 -4.1995807 -4.1954088 -4.2569027 -4.3237886 -4.369688 -4.3901615 -4.3709769 -4.3261428 -4.2627397][-4.4722486 -4.5586152 -4.6075716 -4.6119471 -4.5555534 -4.4533997 -4.3646851 -4.3368196 -4.370707 -4.4347959 -4.5029054 -4.5477262 -4.5361042 -4.4811506 -4.4061613][-4.5454187 -4.61329 -4.649 -4.6303825 -4.5492334 -4.4308858 -4.3274841 -4.2778363 -4.2981553 -4.3860059 -4.5090547 -4.6076031 -4.6254826 -4.5737877 -4.4911304][-4.5283165 -4.569633 -4.5801735 -4.5288243 -4.4134774 -4.2686615 -4.14146 -4.0619884 -4.0648313 -4.1773233 -4.3593841 -4.5181556 -4.5749545 -4.5361319 -4.4537086]]...]
INFO - root - 2017-12-08 00:20:34.924013: step 61110, loss = 21.48, batch loss = 21.40 (8.3 examples/sec; 0.959 sec/batch; 72h:19m:28s remains)
INFO - root - 2017-12-08 00:20:44.315938: step 61120, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.951 sec/batch; 71h:39m:46s remains)
INFO - root - 2017-12-08 00:20:53.501035: step 61130, loss = 21.30, batch loss = 21.21 (8.6 examples/sec; 0.932 sec/batch; 70h:14m:41s remains)
INFO - root - 2017-12-08 00:21:02.880772: step 61140, loss = 21.71, batch loss = 21.62 (8.5 examples/sec; 0.941 sec/batch; 70h:54m:20s remains)
INFO - root - 2017-12-08 00:21:12.142254: step 61150, loss = 21.58, batch loss = 21.50 (9.2 examples/sec; 0.868 sec/batch; 65h:27m:02s remains)
INFO - root - 2017-12-08 00:21:21.397481: step 61160, loss = 21.73, batch loss = 21.64 (8.6 examples/sec; 0.934 sec/batch; 70h:25m:32s remains)
INFO - root - 2017-12-08 00:21:30.828075: step 61170, loss = 21.62, batch loss = 21.53 (8.3 examples/sec; 0.965 sec/batch; 72h:42m:28s remains)
INFO - root - 2017-12-08 00:21:40.145346: step 61180, loss = 21.22, batch loss = 21.14 (8.4 examples/sec; 0.952 sec/batch; 71h:45m:01s remains)
INFO - root - 2017-12-08 00:21:49.328986: step 61190, loss = 21.62, batch loss = 21.54 (8.8 examples/sec; 0.912 sec/batch; 68h:46m:02s remains)
INFO - root - 2017-12-08 00:21:58.651121: step 61200, loss = 21.29, batch loss = 21.21 (9.1 examples/sec; 0.878 sec/batch; 66h:11m:05s remains)
2017-12-08 00:21:59.588997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3592234 -4.4653058 -4.5334487 -4.5699043 -4.5981069 -4.6232147 -4.632762 -4.6337762 -4.6357145 -4.6316667 -4.6119041 -4.5874133 -4.5412774 -4.4672008 -4.3984566][-4.3063984 -4.481195 -4.6130438 -4.6946149 -4.7441483 -4.7742043 -4.77237 -4.7551394 -4.759119 -4.7778921 -4.7780328 -4.7379808 -4.6435914 -4.5205283 -4.4272442][-4.3179393 -4.5331326 -4.7162328 -4.8287311 -4.8681469 -4.8514576 -4.787746 -4.7321444 -4.7480507 -4.8257418 -4.8927803 -4.8704882 -4.7407908 -4.5679684 -4.4445286][-4.3794622 -4.5842371 -4.7783403 -4.89273 -4.8902841 -4.7811184 -4.6124673 -4.49106 -4.4977593 -4.6264033 -4.7796893 -4.8281603 -4.7262325 -4.553412 -4.4283829][-4.4967403 -4.644249 -4.8031211 -4.8897185 -4.8348641 -4.6341143 -4.3756218 -4.1983933 -4.1748147 -4.3100109 -4.5171442 -4.6452208 -4.6174827 -4.49575 -4.3986068][-4.6220303 -4.6957374 -4.7939878 -4.8366303 -4.7313371 -4.4624205 -4.1528549 -3.9566116 -3.9312251 -4.0739427 -4.305233 -4.4827433 -4.5242682 -4.4646277 -4.4004221][-4.7156992 -4.7226248 -4.7460423 -4.7287197 -4.5764627 -4.2569785 -3.9048567 -3.6968482 -3.7038906 -3.9028678 -4.1813326 -4.40207 -4.49892 -4.4952316 -4.4586821][-4.7448597 -4.7022152 -4.6452513 -4.5530367 -4.3600812 -4.0241804 -3.6528254 -3.4297853 -3.4600608 -3.7134435 -4.0406027 -4.3079515 -4.4613733 -4.5178256 -4.5194287][-4.6845112 -4.6223116 -4.5163126 -4.3796725 -4.1838861 -3.8852971 -3.5445251 -3.3257565 -3.3447628 -3.5810149 -3.8922009 -4.169734 -4.3661995 -4.4783573 -4.5205607][-4.5909162 -4.5350695 -4.4435744 -4.3404665 -4.2081552 -3.9883585 -3.71427 -3.5144763 -3.4911435 -3.6468568 -3.8800023 -4.1167221 -4.305274 -4.4244065 -4.4729009][-4.5321388 -4.47377 -4.4146838 -4.3839006 -4.3509016 -4.236104 -4.0534439 -3.8886926 -3.8247347 -3.8978906 -4.0526309 -4.2231956 -4.3461661 -4.405674 -4.4143987][-4.4831457 -4.3889565 -4.3255696 -4.3381662 -4.3896022 -4.3860469 -4.3201966 -4.2237763 -4.1589389 -4.1988707 -4.3203964 -4.440351 -4.4894652 -4.474822 -4.4367561][-4.3840046 -4.2441111 -4.1560225 -4.1809669 -4.2832122 -4.3715067 -4.4177542 -4.4110608 -4.3893557 -4.4342341 -4.5380082 -4.6167054 -4.6205583 -4.5757647 -4.5328794][-4.2483168 -4.0742679 -3.969928 -4.0016828 -4.1358104 -4.2896748 -4.411922 -4.4746928 -4.4968696 -4.5435033 -4.6082444 -4.63615 -4.6146355 -4.5751557 -4.5555873][-4.199975 -4.0028944 -3.883286 -3.9133351 -4.0596652 -4.2458258 -4.405817 -4.5063829 -4.5484362 -4.5717654 -4.5732164 -4.5438342 -4.5021253 -4.4696579 -4.4607844]]...]
INFO - root - 2017-12-08 00:22:08.937303: step 61210, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.934 sec/batch; 70h:24m:42s remains)
INFO - root - 2017-12-08 00:22:18.410733: step 61220, loss = 21.50, batch loss = 21.42 (8.7 examples/sec; 0.921 sec/batch; 69h:23m:59s remains)
INFO - root - 2017-12-08 00:22:27.739453: step 61230, loss = 21.41, batch loss = 21.33 (9.3 examples/sec; 0.858 sec/batch; 64h:40m:16s remains)
INFO - root - 2017-12-08 00:22:37.092557: step 61240, loss = 21.29, batch loss = 21.21 (8.9 examples/sec; 0.898 sec/batch; 67h:40m:40s remains)
INFO - root - 2017-12-08 00:22:46.648079: step 61250, loss = 21.40, batch loss = 21.32 (8.1 examples/sec; 0.985 sec/batch; 74h:12m:36s remains)
INFO - root - 2017-12-08 00:22:56.136140: step 61260, loss = 21.50, batch loss = 21.41 (8.0 examples/sec; 1.001 sec/batch; 75h:24m:03s remains)
INFO - root - 2017-12-08 00:23:05.524043: step 61270, loss = 21.38, batch loss = 21.30 (8.0 examples/sec; 1.006 sec/batch; 75h:47m:19s remains)
INFO - root - 2017-12-08 00:23:14.734841: step 61280, loss = 21.36, batch loss = 21.28 (8.3 examples/sec; 0.966 sec/batch; 72h:46m:41s remains)
INFO - root - 2017-12-08 00:23:24.138388: step 61290, loss = 21.28, batch loss = 21.20 (8.4 examples/sec; 0.949 sec/batch; 71h:30m:37s remains)
INFO - root - 2017-12-08 00:23:33.500709: step 61300, loss = 21.48, batch loss = 21.40 (8.3 examples/sec; 0.961 sec/batch; 72h:24m:57s remains)
2017-12-08 00:23:34.475736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3501382 -4.2685189 -4.2217441 -4.2197809 -4.2534938 -4.2886486 -4.3166723 -4.352622 -4.3751812 -4.3669758 -4.3449087 -4.3521667 -4.3977561 -4.4441404 -4.4708509][-4.3868723 -4.2747355 -4.2170682 -4.227437 -4.2914672 -4.353869 -4.3967075 -4.4396772 -4.4475217 -4.4004169 -4.3353891 -4.317781 -4.3595939 -4.4121137 -4.4514995][-4.402925 -4.2777672 -4.2244387 -4.2531805 -4.3408256 -4.4161725 -4.4539208 -4.484189 -4.4703674 -4.3941751 -4.3006883 -4.264914 -4.30476 -4.3665686 -4.4233012][-4.394032 -4.2877231 -4.2576437 -4.3041205 -4.3946137 -4.4537778 -4.461184 -4.4606686 -4.4273171 -4.3467426 -4.2557549 -4.2196236 -4.2595134 -4.3265347 -4.3948083][-4.3625503 -4.2917986 -4.2802658 -4.3194966 -4.3783832 -4.3941631 -4.3607407 -4.3378043 -4.3158565 -4.2763519 -4.2268333 -4.2083616 -4.2418084 -4.2974634 -4.3632178][-4.3112383 -4.26285 -4.2513738 -4.2579627 -4.2603636 -4.2184873 -4.1416559 -4.1076851 -4.1255178 -4.1666632 -4.1956959 -4.2166615 -4.2469249 -4.2876325 -4.3438854][-4.2796917 -4.2393823 -4.2228761 -4.2041044 -4.1670246 -4.0824533 -3.9617209 -3.8988974 -3.9310346 -4.0331683 -4.1355848 -4.198988 -4.2334256 -4.2701421 -4.3246202][-4.23924 -4.2026639 -4.1923008 -4.17853 -4.1376014 -4.0361266 -3.8804717 -3.776381 -3.7899828 -3.9093246 -4.0486445 -4.1370177 -4.1735153 -4.2136621 -4.276691][-4.1791606 -4.1547179 -4.1720991 -4.1940565 -4.1749959 -4.0723639 -3.8974705 -3.7665288 -3.7633297 -3.8794777 -4.0233727 -4.1120787 -4.1403222 -4.1822033 -4.252152][-4.1713357 -4.15895 -4.2017765 -4.2611537 -4.2668629 -4.1708946 -3.9959035 -3.8650777 -3.8640761 -3.974297 -4.101831 -4.1702285 -4.1831307 -4.2234278 -4.2939606][-4.2575359 -4.2420959 -4.2806277 -4.3433046 -4.3542295 -4.2642031 -4.0984306 -3.9779954 -3.9840569 -4.0913486 -4.2039971 -4.2588291 -4.268075 -4.31049 -4.3765063][-4.3733048 -4.35185 -4.3719115 -4.4146242 -4.4150071 -4.3266973 -4.1714945 -4.0603905 -4.0686393 -4.1684008 -4.2690444 -4.3188791 -4.3351521 -4.3838491 -4.4475513][-4.4653535 -4.4507036 -4.4605622 -4.4824281 -4.473206 -4.3954215 -4.2653985 -4.1744051 -4.1850643 -4.2689281 -4.3494706 -4.3880329 -4.4065042 -4.4520831 -4.5070114][-4.5215273 -4.5217957 -4.5355864 -4.5526137 -4.549036 -4.4978781 -4.4099321 -4.3499603 -4.3598552 -4.4189367 -4.4721069 -4.4964776 -4.512444 -4.5451121 -4.5792856][-4.5225959 -4.5365109 -4.5596404 -4.5824957 -4.5926309 -4.5742464 -4.5319314 -4.5018935 -4.5094256 -4.5445576 -4.577929 -4.5950022 -4.6066194 -4.621511 -4.6295447]]...]
INFO - root - 2017-12-08 00:23:43.875717: step 61310, loss = 21.50, batch loss = 21.42 (9.2 examples/sec; 0.871 sec/batch; 65h:38m:05s remains)
INFO - root - 2017-12-08 00:23:53.409041: step 61320, loss = 21.47, batch loss = 21.39 (8.5 examples/sec; 0.941 sec/batch; 70h:52m:43s remains)
INFO - root - 2017-12-08 00:24:02.852609: step 61330, loss = 21.69, batch loss = 21.61 (8.0 examples/sec; 0.997 sec/batch; 75h:03m:48s remains)
INFO - root - 2017-12-08 00:24:11.913693: step 61340, loss = 21.30, batch loss = 21.22 (8.3 examples/sec; 0.960 sec/batch; 72h:16m:25s remains)
INFO - root - 2017-12-08 00:24:21.202690: step 61350, loss = 21.14, batch loss = 21.05 (8.2 examples/sec; 0.980 sec/batch; 73h:50m:08s remains)
INFO - root - 2017-12-08 00:24:30.610445: step 61360, loss = 21.32, batch loss = 21.24 (8.4 examples/sec; 0.952 sec/batch; 71h:42m:20s remains)
INFO - root - 2017-12-08 00:24:39.944204: step 61370, loss = 21.28, batch loss = 21.20 (8.6 examples/sec; 0.926 sec/batch; 69h:42m:59s remains)
INFO - root - 2017-12-08 00:24:49.474413: step 61380, loss = 21.49, batch loss = 21.40 (8.0 examples/sec; 0.998 sec/batch; 75h:08m:22s remains)
INFO - root - 2017-12-08 00:24:58.861261: step 61390, loss = 22.02, batch loss = 21.93 (8.2 examples/sec; 0.979 sec/batch; 73h:42m:34s remains)
INFO - root - 2017-12-08 00:25:08.184828: step 61400, loss = 21.80, batch loss = 21.71 (8.9 examples/sec; 0.901 sec/batch; 67h:53m:11s remains)
2017-12-08 00:25:09.118970: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3873096 -4.4193335 -4.4133167 -4.3914394 -4.3685455 -4.3449092 -4.3190765 -4.2972116 -4.284296 -4.2910347 -4.3141589 -4.3371577 -4.3464441 -4.3471251 -4.3425369][-4.4481487 -4.4810462 -4.4677215 -4.4312315 -4.3879957 -4.3479433 -4.3182216 -4.303709 -4.3025889 -4.3195405 -4.3487768 -4.3732419 -4.3797808 -4.3766561 -4.3676491][-4.4704924 -4.5151291 -4.5090742 -4.47317 -4.4223332 -4.3735914 -4.3441963 -4.3405561 -4.3543277 -4.38277 -4.4173985 -4.4449453 -4.4527655 -4.4486661 -4.4357944][-4.4284487 -4.5067835 -4.5222898 -4.4932466 -4.4384575 -4.3824215 -4.3540034 -4.362308 -4.3930845 -4.4372039 -4.4796572 -4.5129485 -4.5271387 -4.5255427 -4.5095572][-4.3366323 -4.4610353 -4.4996181 -4.4676781 -4.3951292 -4.3182869 -4.2814703 -4.2985454 -4.3531985 -4.4265323 -4.4893675 -4.5358353 -4.5598221 -4.5617046 -4.5438733][-4.2517176 -4.4009604 -4.4415832 -4.3871408 -4.2788587 -4.1643982 -4.1039128 -4.1236548 -4.2088394 -4.3257432 -4.4267254 -4.4954991 -4.5266285 -4.52079 -4.4902234][-4.2213883 -4.350575 -4.3683944 -4.2832022 -4.1386452 -3.9833133 -3.8860509 -3.89535 -4.0051055 -4.1667047 -4.3139205 -4.4094219 -4.4411144 -4.4132781 -4.3520007][-4.2631011 -4.3418059 -4.3269053 -4.2205667 -4.0593996 -3.8820915 -3.7511439 -3.7374468 -3.8497884 -4.0321422 -4.20845 -4.3200855 -4.3455992 -4.294373 -4.1985412][-4.3476481 -4.3870087 -4.3541522 -4.249949 -4.1005259 -3.9333355 -3.7931523 -3.7566104 -3.8442302 -4.0083985 -4.1757693 -4.2757316 -4.286644 -4.2205987 -4.1030483][-4.413476 -4.4527879 -4.4363556 -4.3617506 -4.2458239 -4.1112304 -3.9870875 -3.940768 -3.9959762 -4.11818 -4.2418647 -4.3018446 -4.2895374 -4.2215815 -4.1061444][-4.4214921 -4.4864225 -4.5112052 -4.488935 -4.4239297 -4.3350873 -4.2441187 -4.20114 -4.2254105 -4.2972074 -4.3641915 -4.3789153 -4.3496232 -4.2933059 -4.2008066][-4.3798771 -4.472537 -4.5302086 -4.5512614 -4.5324459 -4.4880939 -4.4374089 -4.4079137 -4.4103117 -4.4410763 -4.466114 -4.4592853 -4.4351048 -4.4013505 -4.3399248][-4.3280067 -4.4329376 -4.4995027 -4.5353031 -4.5387268 -4.5228477 -4.5059295 -4.4938278 -4.4876127 -4.4974155 -4.5062761 -4.5022607 -4.4950457 -4.4820404 -4.4467397][-4.2876477 -4.388854 -4.4482474 -4.4807458 -4.489995 -4.4887629 -4.4935546 -4.4964781 -4.4912605 -4.4944773 -4.5006232 -4.5043879 -4.50849 -4.5075274 -4.4889965][-4.3032694 -4.3874531 -4.4335356 -4.4566984 -4.4634838 -4.4647942 -4.4750118 -4.4823527 -4.4787011 -4.4802752 -4.4860706 -4.4944253 -4.5033708 -4.5102191 -4.5074797]]...]
INFO - root - 2017-12-08 00:25:18.500958: step 61410, loss = 21.67, batch loss = 21.59 (8.2 examples/sec; 0.972 sec/batch; 73h:12m:20s remains)
INFO - root - 2017-12-08 00:25:27.903148: step 61420, loss = 21.59, batch loss = 21.50 (8.4 examples/sec; 0.953 sec/batch; 71h:45m:40s remains)
INFO - root - 2017-12-08 00:25:37.292612: step 61430, loss = 21.78, batch loss = 21.70 (8.9 examples/sec; 0.896 sec/batch; 67h:25m:55s remains)
INFO - root - 2017-12-08 00:25:46.585097: step 61440, loss = 21.50, batch loss = 21.42 (8.8 examples/sec; 0.905 sec/batch; 68h:10m:44s remains)
INFO - root - 2017-12-08 00:25:55.915014: step 61450, loss = 21.30, batch loss = 21.22 (8.7 examples/sec; 0.925 sec/batch; 69h:37m:25s remains)
INFO - root - 2017-12-08 00:26:05.478547: step 61460, loss = 21.46, batch loss = 21.38 (7.7 examples/sec; 1.037 sec/batch; 78h:05m:39s remains)
INFO - root - 2017-12-08 00:26:14.817769: step 61470, loss = 21.69, batch loss = 21.61 (8.4 examples/sec; 0.954 sec/batch; 71h:47m:31s remains)
INFO - root - 2017-12-08 00:26:24.130411: step 61480, loss = 21.16, batch loss = 21.08 (8.1 examples/sec; 0.987 sec/batch; 74h:20m:01s remains)
INFO - root - 2017-12-08 00:26:33.496999: step 61490, loss = 21.25, batch loss = 21.16 (8.1 examples/sec; 0.992 sec/batch; 74h:41m:31s remains)
INFO - root - 2017-12-08 00:26:42.977232: step 61500, loss = 21.30, batch loss = 21.22 (8.7 examples/sec; 0.916 sec/batch; 68h:55m:03s remains)
2017-12-08 00:26:43.914167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4201937 -4.480742 -4.5310698 -4.5545526 -4.5539713 -4.5520105 -4.5604858 -4.5684066 -4.5589471 -4.53299 -4.5093665 -4.499094 -4.4949079 -4.4762521 -4.4367394][-4.4810858 -4.5625119 -4.6156774 -4.6199832 -4.5862436 -4.5558085 -4.5543518 -4.571969 -4.5778852 -4.5597062 -4.5409636 -4.5425935 -4.5529175 -4.5421643 -4.5010104][-4.5297546 -4.6103978 -4.6433649 -4.6091261 -4.5272779 -4.4557271 -4.4334507 -4.4582062 -4.4915385 -4.5013638 -4.5014567 -4.5198283 -4.5480604 -4.5536165 -4.5248947][-4.5477304 -4.6017461 -4.5909476 -4.502037 -4.3659344 -4.2530861 -4.2084846 -4.2404318 -4.3122921 -4.3718271 -4.4098878 -4.4530573 -4.501195 -4.5265393 -4.5175571][-4.5342846 -4.5470576 -4.4825888 -4.3369584 -4.1501803 -3.9981818 -3.9298668 -3.9675391 -4.0824718 -4.2045774 -4.29793 -4.3782535 -4.4546442 -4.5047154 -4.513217][-4.4987617 -4.4795113 -4.3783121 -4.19624 -3.9724784 -3.7829928 -3.6869831 -3.7269564 -3.8806067 -4.0577321 -4.201211 -4.3159194 -4.4222178 -4.496944 -4.5107365][-4.466804 -4.4405203 -4.3376508 -4.1534467 -3.9151168 -3.6939361 -3.5670757 -3.6037054 -3.779757 -3.9854989 -4.150497 -4.2775264 -4.399426 -4.4840469 -4.48235][-4.4532404 -4.4461432 -4.3699174 -4.2146044 -3.9913127 -3.7616839 -3.6167183 -3.6430643 -3.8119459 -4.0070882 -4.1580095 -4.2697287 -4.3799915 -4.4468451 -4.4071393][-4.4580045 -4.4773622 -4.43902 -4.3316426 -4.1550393 -3.9546647 -3.8209095 -3.8361073 -3.9714339 -4.123703 -4.2324119 -4.3077245 -4.3812318 -4.4046397 -4.31355][-4.4613295 -4.5020576 -4.4986277 -4.4417181 -4.3265996 -4.1839843 -4.0873017 -4.0988445 -4.1953163 -4.296319 -4.3546443 -4.3871975 -4.41751 -4.3964896 -4.2638121][-4.46521 -4.5153584 -4.5337262 -4.5127029 -4.4485207 -4.363483 -4.3101678 -4.3260183 -4.3925862 -4.4516473 -4.4699445 -4.4713097 -4.4765015 -4.4399805 -4.3025274][-4.4716263 -4.5189505 -4.54517 -4.5453839 -4.5179219 -4.4799404 -4.4628739 -4.4824295 -4.5240054 -4.5508213 -4.5444026 -4.5325594 -4.5341024 -4.509336 -4.4004912][-4.46293 -4.500195 -4.5260344 -4.5395784 -4.5398259 -4.5354991 -4.5396194 -4.5558419 -4.5760026 -4.582768 -4.5709796 -4.5651445 -4.5788326 -4.5784683 -4.5153489][-4.4384789 -4.4600968 -4.4783578 -4.4940839 -4.5060921 -4.5157657 -4.5257268 -4.5361671 -4.5456219 -4.5500927 -4.550787 -4.5631065 -4.5929933 -4.6154246 -4.5962443][-4.4022865 -4.4121256 -4.4219003 -4.4336209 -4.4456034 -4.455977 -4.4642625 -4.4710064 -4.4783082 -4.4871926 -4.4995279 -4.5226 -4.5570498 -4.5873809 -4.5935574]]...]
INFO - root - 2017-12-08 00:26:53.416493: step 61510, loss = 21.31, batch loss = 21.23 (9.3 examples/sec; 0.858 sec/batch; 64h:34m:12s remains)
INFO - root - 2017-12-08 00:27:02.837803: step 61520, loss = 21.42, batch loss = 21.34 (9.0 examples/sec; 0.885 sec/batch; 66h:37m:44s remains)
INFO - root - 2017-12-08 00:27:12.211893: step 61530, loss = 21.91, batch loss = 21.83 (8.3 examples/sec; 0.965 sec/batch; 72h:38m:49s remains)
INFO - root - 2017-12-08 00:27:21.339028: step 61540, loss = 21.38, batch loss = 21.30 (8.9 examples/sec; 0.897 sec/batch; 67h:32m:47s remains)
INFO - root - 2017-12-08 00:27:30.855430: step 61550, loss = 21.70, batch loss = 21.62 (8.7 examples/sec; 0.922 sec/batch; 69h:24m:39s remains)
INFO - root - 2017-12-08 00:27:40.305114: step 61560, loss = 21.44, batch loss = 21.36 (8.5 examples/sec; 0.943 sec/batch; 70h:58m:20s remains)
INFO - root - 2017-12-08 00:27:49.718827: step 61570, loss = 21.47, batch loss = 21.38 (8.3 examples/sec; 0.965 sec/batch; 72h:39m:28s remains)
INFO - root - 2017-12-08 00:27:58.992441: step 61580, loss = 21.55, batch loss = 21.46 (8.7 examples/sec; 0.919 sec/batch; 69h:11m:47s remains)
INFO - root - 2017-12-08 00:28:08.671498: step 61590, loss = 21.01, batch loss = 20.93 (8.4 examples/sec; 0.953 sec/batch; 71h:43m:00s remains)
INFO - root - 2017-12-08 00:28:18.022538: step 61600, loss = 21.51, batch loss = 21.43 (8.3 examples/sec; 0.969 sec/batch; 72h:56m:05s remains)
2017-12-08 00:28:19.035165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7533231 -4.7907672 -4.8340235 -4.8699985 -4.8776197 -4.8475885 -4.7658477 -4.62305 -4.4372993 -4.2665071 -4.1662979 -4.1759129 -4.2906671 -4.4430952 -4.5642781][-4.852149 -4.9149261 -4.9696083 -4.9869657 -4.9433241 -4.8436222 -4.7215462 -4.6040483 -4.4996705 -4.4079089 -4.3293176 -4.2910361 -4.3293061 -4.4273105 -4.5391188][-4.885993 -4.9473763 -4.9863491 -4.9752493 -4.8951035 -4.7681837 -4.653657 -4.5964603 -4.5915694 -4.589994 -4.53051 -4.4428926 -4.4093733 -4.4466686 -4.5270829][-4.8601322 -4.894784 -4.9006019 -4.8577743 -4.7514415 -4.6100192 -4.4981146 -4.4758554 -4.5527182 -4.6506042 -4.6525574 -4.5803518 -4.5247903 -4.5108047 -4.5349097][-4.8079305 -4.8033013 -4.7690067 -4.6869617 -4.5421839 -4.3588305 -4.2031717 -4.1600614 -4.2805457 -4.4732513 -4.5804687 -4.6024718 -4.5939078 -4.5675454 -4.5485377][-4.7687254 -4.7274017 -4.652544 -4.5265179 -4.326663 -4.0767913 -3.845315 -3.746954 -3.8755174 -4.134007 -4.3432531 -4.4825363 -4.5539904 -4.5567136 -4.5358281][-4.7482753 -4.7032127 -4.6127338 -4.4546409 -4.2039886 -3.8926497 -3.588623 -3.4258838 -3.5352726 -3.8136024 -4.0879192 -4.3164172 -4.4533153 -4.4929104 -4.4930334][-4.7346926 -4.7261014 -4.6635733 -4.5087528 -4.2443027 -3.9196982 -3.6020551 -3.4194803 -3.5027685 -3.75419 -4.0330119 -4.2827396 -4.4270263 -4.4689975 -4.4737945][-4.7082229 -4.7533531 -4.7499876 -4.6344872 -4.3963294 -4.1048059 -3.8319232 -3.6793141 -3.7406893 -3.9425731 -4.1833243 -4.3999386 -4.5081987 -4.5203447 -4.5052686][-4.6570973 -4.7433949 -4.799551 -4.7354312 -4.5375357 -4.2851491 -4.0658474 -3.9584002 -4.019558 -4.1923833 -4.3962913 -4.56565 -4.6255913 -4.5977044 -4.5535192][-4.5677934 -4.6823759 -4.7770448 -4.7605844 -4.6132412 -4.4087396 -4.2469997 -4.1924911 -4.2722125 -4.4322348 -4.5971637 -4.7073865 -4.7146034 -4.6504288 -4.5833678][-4.481657 -4.6206455 -4.7313161 -4.7408133 -4.6379972 -4.4855819 -4.3798594 -4.371419 -4.4633517 -4.5989847 -4.7107315 -4.7551036 -4.7181792 -4.6369829 -4.5693893][-4.4466152 -4.5897393 -4.6862025 -4.6944895 -4.61655 -4.5052528 -4.4427676 -4.4665051 -4.5558162 -4.651371 -4.6999769 -4.68435 -4.6202388 -4.5466871 -4.5023346][-4.4586797 -4.56717 -4.6167979 -4.6023188 -4.537982 -4.4593053 -4.4277139 -4.4706826 -4.5515733 -4.6084304 -4.604815 -4.548295 -4.4770679 -4.4289193 -4.4187288][-4.5217776 -4.5606484 -4.5392632 -4.4839625 -4.4163971 -4.3498273 -4.3239074 -4.3639326 -4.431447 -4.4684596 -4.4487948 -4.3937421 -4.34708 -4.33831 -4.3651352]]...]
INFO - root - 2017-12-08 00:28:28.488372: step 61610, loss = 21.29, batch loss = 21.21 (8.3 examples/sec; 0.969 sec/batch; 72h:56m:56s remains)
INFO - root - 2017-12-08 00:28:37.786807: step 61620, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.957 sec/batch; 72h:00m:43s remains)
INFO - root - 2017-12-08 00:28:47.078198: step 61630, loss = 21.35, batch loss = 21.27 (9.0 examples/sec; 0.887 sec/batch; 66h:44m:05s remains)
INFO - root - 2017-12-08 00:28:56.480756: step 61640, loss = 21.49, batch loss = 21.40 (9.8 examples/sec; 0.820 sec/batch; 61h:40m:13s remains)
INFO - root - 2017-12-08 00:29:05.794021: step 61650, loss = 21.74, batch loss = 21.66 (8.6 examples/sec; 0.929 sec/batch; 69h:51m:27s remains)
INFO - root - 2017-12-08 00:29:15.302248: step 61660, loss = 21.33, batch loss = 21.25 (8.2 examples/sec; 0.970 sec/batch; 72h:58m:16s remains)
INFO - root - 2017-12-08 00:29:24.670989: step 61670, loss = 21.44, batch loss = 21.36 (8.3 examples/sec; 0.963 sec/batch; 72h:24m:38s remains)
INFO - root - 2017-12-08 00:29:34.087448: step 61680, loss = 21.43, batch loss = 21.34 (8.4 examples/sec; 0.949 sec/batch; 71h:21m:57s remains)
INFO - root - 2017-12-08 00:29:43.492169: step 61690, loss = 21.60, batch loss = 21.51 (8.6 examples/sec; 0.933 sec/batch; 70h:08m:50s remains)
INFO - root - 2017-12-08 00:29:52.911469: step 61700, loss = 21.29, batch loss = 21.21 (8.3 examples/sec; 0.969 sec/batch; 72h:51m:55s remains)
2017-12-08 00:29:53.860145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4074373 -4.3141313 -4.2381191 -4.1917815 -4.1299176 -4.0368447 -4.0154147 -4.1241512 -4.2592139 -4.3278623 -4.3214412 -4.2804909 -4.2399883 -4.2179995 -4.2128634][-4.3533263 -4.2518964 -4.1781378 -4.1455884 -4.0981588 -4.0104461 -3.9756269 -4.0636272 -4.1832352 -4.2477212 -4.2455606 -4.205368 -4.1596651 -4.1262031 -4.1134067][-4.3119488 -4.2204914 -4.1530046 -4.1244349 -4.0841846 -4.0086846 -3.971364 -4.0313654 -4.1287313 -4.1936445 -4.2030387 -4.169899 -4.1148682 -4.049613 -3.9986844][-4.2880306 -4.2092338 -4.144021 -4.1054831 -4.0632849 -4.0042548 -3.9754028 -4.0157428 -4.1029034 -4.1849637 -4.2227449 -4.2126236 -4.15868 -4.063076 -3.9594998][-4.2468815 -4.1902223 -4.1354971 -4.091423 -4.0428042 -3.9874384 -3.9570668 -3.9890199 -4.0913353 -4.2153916 -4.2987733 -4.3229222 -4.2878294 -4.1896496 -4.0577469][-4.2138824 -4.2071528 -4.1883636 -4.154398 -4.0936766 -4.0115833 -3.9459233 -3.9573374 -4.0809197 -4.2588353 -4.4008207 -4.4698443 -4.4722357 -4.4090829 -4.2979784][-4.2244592 -4.2751184 -4.3074903 -4.2966676 -4.22441 -4.093111 -3.9560719 -3.9209588 -4.0438004 -4.25883 -4.4504361 -4.5599213 -4.6018815 -4.5934596 -4.5428414][-4.2649775 -4.3393941 -4.4030833 -4.4146767 -4.3452721 -4.1841373 -3.9924428 -3.911757 -4.0048757 -4.2118 -4.4133663 -4.538599 -4.6026692 -4.6343746 -4.6390166][-4.289742 -4.3560753 -4.420651 -4.4457679 -4.3975554 -4.2493033 -4.0526309 -3.9516795 -4.0034738 -4.16621 -4.3414679 -4.4607182 -4.5299668 -4.5716677 -4.5954642][-4.2854757 -4.3379569 -4.3935223 -4.4274879 -4.4027362 -4.285255 -4.1184926 -4.0222173 -4.0408807 -4.1486316 -4.2826557 -4.3875475 -4.4514713 -4.4821143 -4.4946032][-4.2855468 -4.3299518 -4.3842454 -4.4223762 -4.405345 -4.3044167 -4.1631017 -4.0752292 -4.0743213 -4.1389604 -4.2351546 -4.3258014 -4.3791518 -4.3863673 -4.3712621][-4.29272 -4.3360014 -4.3979416 -4.442318 -4.4196148 -4.3117 -4.1744595 -4.0906515 -4.0901704 -4.1443987 -4.2224946 -4.297832 -4.3329344 -4.3142653 -4.2722621][-4.3024321 -4.3476629 -4.421279 -4.4757957 -4.4445744 -4.3195806 -4.1676264 -4.0794311 -4.092473 -4.1650953 -4.248363 -4.3165412 -4.3360786 -4.3035712 -4.25479][-4.349638 -4.3888378 -4.45848 -4.5110164 -4.4711666 -4.33391 -4.1731625 -4.0883546 -4.1219096 -4.220695 -4.3183031 -4.3814116 -4.3842854 -4.3413439 -4.3001461][-4.4297833 -4.44903 -4.4966288 -4.5376625 -4.4979911 -4.36757 -4.2179804 -4.1496305 -4.2011442 -4.3140717 -4.4148188 -4.4639053 -4.446836 -4.3920074 -4.3560586]]...]
INFO - root - 2017-12-08 00:30:03.353430: step 61710, loss = 21.69, batch loss = 21.61 (8.0 examples/sec; 1.001 sec/batch; 75h:18m:22s remains)
INFO - root - 2017-12-08 00:30:12.709052: step 61720, loss = 21.27, batch loss = 21.19 (8.4 examples/sec; 0.950 sec/batch; 71h:28m:41s remains)
INFO - root - 2017-12-08 00:30:22.035661: step 61730, loss = 21.65, batch loss = 21.57 (8.3 examples/sec; 0.961 sec/batch; 72h:15m:55s remains)
INFO - root - 2017-12-08 00:30:31.429874: step 61740, loss = 21.31, batch loss = 21.22 (8.1 examples/sec; 0.992 sec/batch; 74h:36m:56s remains)
INFO - root - 2017-12-08 00:30:40.591201: step 61750, loss = 21.02, batch loss = 20.94 (9.1 examples/sec; 0.883 sec/batch; 66h:23m:34s remains)
INFO - root - 2017-12-08 00:30:49.911417: step 61760, loss = 21.61, batch loss = 21.53 (8.5 examples/sec; 0.944 sec/batch; 71h:01m:42s remains)
INFO - root - 2017-12-08 00:30:59.345844: step 61770, loss = 21.33, batch loss = 21.24 (8.3 examples/sec; 0.960 sec/batch; 72h:11m:26s remains)
INFO - root - 2017-12-08 00:31:08.834926: step 61780, loss = 21.63, batch loss = 21.54 (8.8 examples/sec; 0.914 sec/batch; 68h:44m:49s remains)
INFO - root - 2017-12-08 00:31:18.149162: step 61790, loss = 21.62, batch loss = 21.54 (11.2 examples/sec; 0.713 sec/batch; 53h:35m:36s remains)
INFO - root - 2017-12-08 00:31:27.601721: step 61800, loss = 21.33, batch loss = 21.25 (8.3 examples/sec; 0.962 sec/batch; 72h:18m:25s remains)
2017-12-08 00:31:28.561856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4541917 -4.4864726 -4.3740916 -4.200067 -4.0768838 -4.0666747 -4.1612692 -4.2781796 -4.4170232 -4.5422907 -4.5121918 -4.3496819 -4.20925 -4.1785445 -4.2591729][-4.45274 -4.5095854 -4.4361048 -4.2972507 -4.2037158 -4.2038727 -4.2794313 -4.3522716 -4.4401579 -4.5310664 -4.4827809 -4.2997532 -4.1482277 -4.1142926 -4.1845627][-4.4954085 -4.5854654 -4.5515609 -4.4439282 -4.3526955 -4.3192067 -4.3343353 -4.3370457 -4.3637505 -4.4267769 -4.3839192 -4.2135086 -4.0663881 -4.0358982 -4.0955005][-4.5469494 -4.6648412 -4.6570311 -4.5613894 -4.4441333 -4.3442073 -4.2825446 -4.2271895 -4.2241077 -4.2854924 -4.2750859 -4.1384306 -4.0014553 -3.9627898 -3.9975893][-4.5673604 -4.6945448 -4.6910834 -4.5850163 -4.424222 -4.2551088 -4.1363358 -4.0618706 -4.0793791 -4.1772022 -4.2123652 -4.1173534 -3.9891906 -3.9257257 -3.9247868][-4.5552411 -4.6667027 -4.6424685 -4.50313 -4.2906694 -4.0639157 -3.9147582 -3.8616881 -3.9434333 -4.1097112 -4.2018991 -4.1485977 -4.0296144 -3.9477496 -3.9231191][-4.5361919 -4.6206651 -4.562058 -4.38123 -4.1191421 -3.8460488 -3.6822634 -3.6586654 -3.8085113 -4.0586777 -4.2227869 -4.2195783 -4.1158242 -4.028542 -4.0007029][-4.532907 -4.5938172 -4.5027423 -4.2865324 -3.9818175 -3.6695392 -3.4895446 -3.4810367 -3.6800976 -4.0153055 -4.2727118 -4.3389516 -4.2651615 -4.1879959 -4.1665783][-4.5471015 -4.6065426 -4.507319 -4.2812691 -3.95735 -3.6175313 -3.4146178 -3.397747 -3.610415 -3.9975343 -4.33083 -4.4640245 -4.4299941 -4.3719697 -4.3598256][-4.5721321 -4.6631317 -4.5984 -4.4077978 -4.10877 -3.7746644 -3.5606647 -3.5191717 -3.6993282 -4.0609779 -4.3875961 -4.531569 -4.5147 -4.4702048 -4.464272][-4.604764 -4.7477989 -4.7448144 -4.6212921 -4.38336 -4.0866609 -3.8841252 -3.827764 -3.9542499 -4.2278056 -4.46578 -4.5537281 -4.50902 -4.438942 -4.4139986][-4.6228619 -4.8109727 -4.8717418 -4.8172555 -4.6446004 -4.3956518 -4.2150187 -4.1579461 -4.2419438 -4.4255414 -4.5607867 -4.5712242 -4.4751172 -4.3531437 -4.2889609][-4.5947728 -4.8085132 -4.9186683 -4.9178567 -4.7980213 -4.5919948 -4.4307036 -4.3659186 -4.4143648 -4.53678 -4.6080484 -4.5774503 -4.4584012 -4.3083315 -4.219996][-4.5165896 -4.7249694 -4.8592787 -4.895575 -4.8241115 -4.6690583 -4.5296955 -4.4492683 -4.4613943 -4.5444579 -4.591207 -4.5573969 -4.4511094 -4.3159976 -4.2359767][-4.4061189 -4.5828176 -4.7225733 -4.7901282 -4.7716541 -4.68352 -4.5872087 -4.5099378 -4.4932604 -4.5383286 -4.566957 -4.5434861 -4.4691057 -4.3766489 -4.3239751]]...]
INFO - root - 2017-12-08 00:31:38.045001: step 61810, loss = 21.30, batch loss = 21.22 (8.6 examples/sec; 0.931 sec/batch; 69h:58m:10s remains)
INFO - root - 2017-12-08 00:31:47.544659: step 61820, loss = 21.59, batch loss = 21.51 (8.5 examples/sec; 0.936 sec/batch; 70h:22m:33s remains)
INFO - root - 2017-12-08 00:31:56.777505: step 61830, loss = 21.89, batch loss = 21.81 (9.3 examples/sec; 0.860 sec/batch; 64h:37m:54s remains)
INFO - root - 2017-12-08 00:32:06.088180: step 61840, loss = 21.35, batch loss = 21.27 (8.5 examples/sec; 0.941 sec/batch; 70h:43m:57s remains)
INFO - root - 2017-12-08 00:32:15.504574: step 61850, loss = 21.10, batch loss = 21.01 (8.2 examples/sec; 0.971 sec/batch; 73h:01m:54s remains)
INFO - root - 2017-12-08 00:32:24.903141: step 61860, loss = 21.77, batch loss = 21.69 (8.5 examples/sec; 0.936 sec/batch; 70h:21m:46s remains)
INFO - root - 2017-12-08 00:32:34.256897: step 61870, loss = 21.26, batch loss = 21.18 (8.7 examples/sec; 0.915 sec/batch; 68h:45m:12s remains)
INFO - root - 2017-12-08 00:32:43.716621: step 61880, loss = 21.77, batch loss = 21.69 (8.9 examples/sec; 0.900 sec/batch; 67h:37m:06s remains)
INFO - root - 2017-12-08 00:32:53.207845: step 61890, loss = 21.37, batch loss = 21.29 (8.5 examples/sec; 0.936 sec/batch; 70h:20m:43s remains)
INFO - root - 2017-12-08 00:33:02.630665: step 61900, loss = 21.63, batch loss = 21.55 (8.5 examples/sec; 0.939 sec/batch; 70h:36m:44s remains)
2017-12-08 00:33:03.591017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6632652 -4.7336774 -4.7559447 -4.7237158 -4.6165528 -4.4746432 -4.3616505 -4.3098254 -4.3347883 -4.3877797 -4.4442372 -4.5063043 -4.5397162 -4.5142875 -4.4454012][-4.6421609 -4.7288179 -4.7705035 -4.7314849 -4.5930238 -4.4158297 -4.2685537 -4.1998868 -4.2305303 -4.3029847 -4.3830886 -4.4700465 -4.5216484 -4.5088234 -4.447372][-4.625473 -4.7110524 -4.7688146 -4.7385449 -4.5929093 -4.3895612 -4.1975141 -4.0871611 -4.0967436 -4.1740146 -4.2782784 -4.3997836 -4.4852223 -4.4961977 -4.4466934][-4.6478915 -4.707902 -4.7605748 -4.7342939 -4.5856819 -4.3550253 -4.1154766 -3.9608934 -3.9483016 -4.0313754 -4.1605644 -4.3160915 -4.4385996 -4.4780273 -4.4426026][-4.7009139 -4.7235713 -4.7452965 -4.7015448 -4.5370188 -4.2765856 -4.0058241 -3.8383398 -3.8370543 -3.9473636 -4.1009555 -4.272593 -4.4127922 -4.4674416 -4.4395328][-4.7572289 -4.7398157 -4.7170935 -4.6391449 -4.4539824 -4.1810813 -3.9152908 -3.7735734 -3.8092387 -3.9541395 -4.1220222 -4.289896 -4.4239564 -4.4751339 -4.4440351][-4.7770905 -4.7257628 -4.6653857 -4.5591168 -4.3725719 -4.1221585 -3.8914895 -3.7815804 -3.8398569 -3.9969506 -4.1649928 -4.3268228 -4.4537563 -4.4980607 -4.45818][-4.7082539 -4.6376925 -4.5664868 -4.465044 -4.3198018 -4.13358 -3.9558055 -3.8628986 -3.9084229 -4.0463943 -4.204793 -4.3662992 -4.4928274 -4.5298138 -4.4771919][-4.5576982 -4.4930558 -4.4413409 -4.3793516 -4.3048425 -4.1989789 -4.0743632 -3.9843259 -4.0016913 -4.113647 -4.2625728 -4.4232554 -4.5423651 -4.56198 -4.4921851][-4.4255972 -4.3820853 -4.354413 -4.3293209 -4.3060412 -4.2522616 -4.1652312 -4.0827007 -4.0907993 -4.1959043 -4.340054 -4.4885464 -4.5839529 -4.5774741 -4.4922657][-4.3943367 -4.3738065 -4.3564134 -4.3364663 -4.3118582 -4.2535353 -4.1735249 -4.1077495 -4.1357741 -4.2588897 -4.4054155 -4.5359893 -4.5991364 -4.5670824 -4.475791][-4.4280415 -4.4270749 -4.411912 -4.3791995 -4.3164868 -4.218677 -4.1255641 -4.0774536 -4.1361237 -4.2815409 -4.4327245 -4.5495672 -4.58634 -4.5388126 -4.451293][-4.464108 -4.4693928 -4.4549017 -4.4180889 -4.330307 -4.2059588 -4.1031175 -4.0660739 -4.142303 -4.2915044 -4.4359713 -4.541296 -4.56337 -4.5124693 -4.4329033][-4.4747176 -4.4603057 -4.4448743 -4.4195766 -4.3402739 -4.2247844 -4.1291881 -4.0953884 -4.1676569 -4.2973886 -4.4204497 -4.514617 -4.5381765 -4.4970284 -4.4265652][-4.4779334 -4.4418235 -4.4187713 -4.4008965 -4.3376007 -4.2411323 -4.1538634 -4.1136594 -4.1654859 -4.2658792 -4.36998 -4.4647779 -4.5059218 -4.4846296 -4.4244528]]...]
INFO - root - 2017-12-08 00:33:13.080339: step 61910, loss = 21.40, batch loss = 21.32 (8.2 examples/sec; 0.980 sec/batch; 73h:37m:32s remains)
INFO - root - 2017-12-08 00:33:22.528859: step 61920, loss = 21.40, batch loss = 21.31 (9.1 examples/sec; 0.878 sec/batch; 66h:00m:46s remains)
INFO - root - 2017-12-08 00:33:31.987775: step 61930, loss = 21.28, batch loss = 21.20 (8.3 examples/sec; 0.960 sec/batch; 72h:07m:58s remains)
INFO - root - 2017-12-08 00:33:41.388003: step 61940, loss = 21.70, batch loss = 21.62 (8.3 examples/sec; 0.961 sec/batch; 72h:15m:30s remains)
INFO - root - 2017-12-08 00:33:50.755644: step 61950, loss = 21.28, batch loss = 21.19 (8.2 examples/sec; 0.975 sec/batch; 73h:18m:32s remains)
INFO - root - 2017-12-08 00:34:00.005610: step 61960, loss = 21.36, batch loss = 21.28 (9.4 examples/sec; 0.847 sec/batch; 63h:38m:18s remains)
INFO - root - 2017-12-08 00:34:09.231487: step 61970, loss = 21.57, batch loss = 21.49 (8.9 examples/sec; 0.894 sec/batch; 67h:13m:00s remains)
INFO - root - 2017-12-08 00:34:18.729037: step 61980, loss = 21.43, batch loss = 21.35 (8.4 examples/sec; 0.951 sec/batch; 71h:29m:57s remains)
INFO - root - 2017-12-08 00:34:28.103226: step 61990, loss = 21.37, batch loss = 21.29 (8.8 examples/sec; 0.909 sec/batch; 68h:16m:59s remains)
INFO - root - 2017-12-08 00:34:37.506637: step 62000, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.953 sec/batch; 71h:35m:58s remains)
2017-12-08 00:34:38.521235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4254994 -4.3717051 -4.3747892 -4.4170055 -4.4569473 -4.4337044 -4.3595843 -4.3191605 -4.3641338 -4.4740014 -4.5822477 -4.6246734 -4.60572 -4.5578232 -4.4934816][-4.3939257 -4.3470068 -4.3593383 -4.4046121 -4.4390588 -4.4066062 -4.3125687 -4.2594357 -4.3160753 -4.45653 -4.60058 -4.6670914 -4.6511059 -4.5904884 -4.5121827][-4.3491559 -4.3195672 -4.340044 -4.3859153 -4.4112539 -4.3645182 -4.243197 -4.16672 -4.2223215 -4.3833928 -4.568152 -4.67791 -4.6820197 -4.6149697 -4.5260544][-4.2944174 -4.2858539 -4.316885 -4.3630242 -4.3775916 -4.31434 -4.1641216 -4.0608463 -4.1066232 -4.2743697 -4.4906797 -4.6458764 -4.6828375 -4.621501 -4.5313754][-4.24839 -4.2537551 -4.2874117 -4.330348 -4.3361306 -4.2630534 -4.0998273 -3.9819713 -4.0094552 -4.1636205 -4.3917284 -4.5822287 -4.657094 -4.6145267 -4.5322905][-4.2375364 -4.2458754 -4.26959 -4.2956405 -4.2847247 -4.2004485 -4.0395174 -3.9248757 -3.9357929 -4.0691814 -4.29504 -4.5068493 -4.616024 -4.6008806 -4.5324082][-4.269105 -4.27689 -4.2864408 -4.2862992 -4.2460766 -4.1363382 -3.9780991 -3.8776164 -3.8845403 -4.0063105 -4.2235718 -4.4411259 -4.5714173 -4.5836015 -4.5312171][-4.323061 -4.335865 -4.340147 -4.3257995 -4.2607079 -4.1231127 -3.9643035 -3.8750823 -3.8826535 -3.9902608 -4.1874833 -4.3950753 -4.533514 -4.5676651 -4.5286217][-4.3799615 -4.407084 -4.4209237 -4.409318 -4.3359866 -4.1849689 -4.0235529 -3.9319263 -3.9295654 -4.0170836 -4.1905017 -4.3812723 -4.5159321 -4.5596075 -4.5265446][-4.4142876 -4.4555216 -4.483695 -4.4842229 -4.4157834 -4.2657557 -4.1076765 -4.0147033 -4.0006428 -4.068912 -4.222002 -4.3922505 -4.5125413 -4.553277 -4.5219111][-4.4185576 -4.4561148 -4.4928493 -4.5046868 -4.4503036 -4.319313 -4.18223 -4.0938759 -4.0620489 -4.1080585 -4.2424731 -4.3953538 -4.502306 -4.5398865 -4.5139003][-4.3961616 -4.4092588 -4.4393468 -4.4621549 -4.4367118 -4.3473511 -4.244606 -4.157526 -4.1020832 -4.1242566 -4.2403431 -4.3812294 -4.4836588 -4.5229716 -4.504539][-4.3544006 -4.3383093 -4.3575144 -4.3908625 -4.3944387 -4.3491426 -4.2750096 -4.1913004 -4.1296453 -4.1395197 -4.2385564 -4.3683667 -4.4715214 -4.5131574 -4.4985108][-4.3091354 -4.266799 -4.2732549 -4.3088865 -4.3271675 -4.3138165 -4.2649012 -4.194335 -4.1471057 -4.1610012 -4.2415876 -4.3555036 -4.460485 -4.5077071 -4.4967756][-4.2787547 -4.218204 -4.2106471 -4.2494326 -4.2750983 -4.2869411 -4.2660241 -4.2064967 -4.16418 -4.1815076 -4.2487564 -4.3478065 -4.450119 -4.5029311 -4.4967208]]...]
INFO - root - 2017-12-08 00:34:48.126761: step 62010, loss = 21.57, batch loss = 21.49 (8.7 examples/sec; 0.922 sec/batch; 69h:17m:05s remains)
INFO - root - 2017-12-08 00:34:57.433205: step 62020, loss = 21.14, batch loss = 21.06 (7.9 examples/sec; 1.007 sec/batch; 75h:40m:22s remains)
INFO - root - 2017-12-08 00:35:06.895590: step 62030, loss = 21.28, batch loss = 21.20 (8.0 examples/sec; 0.997 sec/batch; 74h:53m:06s remains)
INFO - root - 2017-12-08 00:35:16.476620: step 62040, loss = 21.41, batch loss = 21.32 (8.5 examples/sec; 0.941 sec/batch; 70h:40m:18s remains)
INFO - root - 2017-12-08 00:35:26.029375: step 62050, loss = 21.60, batch loss = 21.52 (8.9 examples/sec; 0.904 sec/batch; 67h:53m:34s remains)
INFO - root - 2017-12-08 00:35:35.157261: step 62060, loss = 20.96, batch loss = 20.88 (8.7 examples/sec; 0.923 sec/batch; 69h:21m:03s remains)
INFO - root - 2017-12-08 00:35:44.475493: step 62070, loss = 21.32, batch loss = 21.23 (8.5 examples/sec; 0.939 sec/batch; 70h:33m:14s remains)
INFO - root - 2017-12-08 00:35:53.963085: step 62080, loss = 21.61, batch loss = 21.53 (8.8 examples/sec; 0.905 sec/batch; 67h:59m:34s remains)
INFO - root - 2017-12-08 00:36:03.292306: step 62090, loss = 21.80, batch loss = 21.72 (9.3 examples/sec; 0.865 sec/batch; 64h:56m:47s remains)
INFO - root - 2017-12-08 00:36:12.657929: step 62100, loss = 21.83, batch loss = 21.75 (8.5 examples/sec; 0.936 sec/batch; 70h:17m:53s remains)
2017-12-08 00:36:13.716707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3857269 -4.3924842 -4.3994927 -4.4016171 -4.4020743 -4.4014888 -4.4006691 -4.4005418 -4.4018145 -4.403336 -4.4026928 -4.402534 -4.405807 -4.4104233 -4.4117742][-4.4583516 -4.471684 -4.4797215 -4.4841824 -4.4889383 -4.4907722 -4.4875927 -4.4817305 -4.4770174 -4.4712858 -4.4631076 -4.46077 -4.4703717 -4.4861193 -4.4953952][-4.5261393 -4.5379658 -4.5361624 -4.5312338 -4.5325971 -4.5342674 -4.529952 -4.5186987 -4.5064921 -4.4938359 -4.478148 -4.4732385 -4.4901581 -4.5195484 -4.5422444][-4.5751505 -4.5747194 -4.5471535 -4.5128 -4.4941015 -4.4913173 -4.4935179 -4.4866014 -4.4741721 -4.4620557 -4.4437609 -4.43088 -4.4403825 -4.4689069 -4.5009713][-4.57854 -4.5604854 -4.5041323 -4.4344654 -4.3865252 -4.3765965 -4.3942938 -4.4060373 -4.4016681 -4.3944678 -4.3771687 -4.3469853 -4.3297095 -4.3365068 -4.3690033][-4.5197935 -4.4842072 -4.413816 -4.32731 -4.2590027 -4.2433405 -4.2777944 -4.312923 -4.3137431 -4.30478 -4.2883978 -4.2448373 -4.20362 -4.1855092 -4.2098384][-4.4106517 -4.3606272 -4.2994194 -4.2282686 -4.1621156 -4.1450377 -4.1897078 -4.2410192 -4.2377448 -4.2120414 -4.1891947 -4.1433649 -4.10118 -4.0779681 -4.0936661][-4.2771916 -4.2214432 -4.1866384 -4.1575971 -4.1202154 -4.1117482 -4.15337 -4.2055264 -4.1911325 -4.143158 -4.1058764 -4.0583105 -4.0278478 -4.0211444 -4.0453544][-4.1559439 -4.115519 -4.1204681 -4.1409116 -4.1413569 -4.1415896 -4.1619859 -4.1927934 -4.1699424 -4.12111 -4.082015 -4.0373774 -4.0168228 -4.0302086 -4.0753827][-4.0684223 -4.0707026 -4.1256552 -4.1868982 -4.2185245 -4.2270455 -4.2231545 -4.2244654 -4.2004104 -4.1748133 -4.1560054 -4.1245689 -4.1079407 -4.1256385 -4.18265][-4.0302958 -4.0753913 -4.1741347 -4.2701044 -4.327662 -4.3450441 -4.3230972 -4.2979083 -4.2738748 -4.2732239 -4.2809639 -4.2701235 -4.2613735 -4.2742276 -4.32665][-4.0423045 -4.10408 -4.2280159 -4.3530722 -4.4319425 -4.4510942 -4.4138613 -4.3747888 -4.3617644 -4.3833303 -4.409626 -4.4101653 -4.4005923 -4.4027796 -4.439271][-4.1281948 -4.1765156 -4.2969513 -4.429688 -4.5124936 -4.5184722 -4.4619455 -4.4179473 -4.4292164 -4.478446 -4.5116982 -4.502439 -4.4753819 -4.4591804 -4.4764085][-4.2409863 -4.2735844 -4.3749785 -4.4933214 -4.5626616 -4.5480132 -4.4649687 -4.4034142 -4.4250917 -4.498405 -4.5420814 -4.5266361 -4.4896016 -4.4649858 -4.47011][-4.3024092 -4.3348336 -4.4199133 -4.51498 -4.5655422 -4.53788 -4.4365883 -4.3456564 -4.3453951 -4.41928 -4.480206 -4.4876084 -4.4693704 -4.4543471 -4.4559379]]...]
INFO - root - 2017-12-08 00:36:23.071272: step 62110, loss = 21.32, batch loss = 21.24 (8.6 examples/sec; 0.930 sec/batch; 69h:53m:14s remains)
INFO - root - 2017-12-08 00:36:32.402560: step 62120, loss = 21.95, batch loss = 21.87 (9.0 examples/sec; 0.890 sec/batch; 66h:52m:36s remains)
INFO - root - 2017-12-08 00:36:41.743588: step 62130, loss = 21.11, batch loss = 21.03 (8.7 examples/sec; 0.922 sec/batch; 69h:12m:31s remains)
INFO - root - 2017-12-08 00:36:51.089020: step 62140, loss = 21.19, batch loss = 21.11 (8.3 examples/sec; 0.963 sec/batch; 72h:21m:06s remains)
INFO - root - 2017-12-08 00:37:00.671448: step 62150, loss = 21.38, batch loss = 21.29 (8.0 examples/sec; 1.002 sec/batch; 75h:15m:49s remains)
INFO - root - 2017-12-08 00:37:09.944543: step 62160, loss = 21.13, batch loss = 21.05 (8.4 examples/sec; 0.957 sec/batch; 71h:51m:52s remains)
INFO - root - 2017-12-08 00:37:19.411453: step 62170, loss = 21.68, batch loss = 21.60 (8.6 examples/sec; 0.936 sec/batch; 70h:15m:23s remains)
INFO - root - 2017-12-08 00:37:28.773859: step 62180, loss = 21.34, batch loss = 21.25 (8.7 examples/sec; 0.924 sec/batch; 69h:23m:36s remains)
INFO - root - 2017-12-08 00:37:38.178709: step 62190, loss = 21.18, batch loss = 21.09 (8.1 examples/sec; 0.989 sec/batch; 74h:16m:19s remains)
INFO - root - 2017-12-08 00:37:47.566881: step 62200, loss = 21.35, batch loss = 21.26 (8.8 examples/sec; 0.913 sec/batch; 68h:32m:10s remains)
2017-12-08 00:37:48.528122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4652596 -4.4464879 -4.3989124 -4.3481922 -4.3153834 -4.3149309 -4.3477249 -4.3907251 -4.4169226 -4.4259152 -4.4330864 -4.4433904 -4.4487405 -4.4433632 -4.4268193][-4.4525461 -4.4658437 -4.4486418 -4.42296 -4.4053903 -4.4081316 -4.4288945 -4.4478641 -4.4552064 -4.4645452 -4.4860997 -4.5117054 -4.5229936 -4.5092454 -4.4759459][-4.4329891 -4.4638963 -4.4719057 -4.4756265 -4.481904 -4.4865909 -4.4786053 -4.4519725 -4.4270487 -4.438776 -4.4861031 -4.538105 -4.5577273 -4.53128 -4.4758158][-4.43059 -4.4629483 -4.4810791 -4.502533 -4.5207348 -4.5065651 -4.4430742 -4.3519797 -4.2939529 -4.322392 -4.4127917 -4.5042629 -4.5379338 -4.4968858 -4.41798][-4.4285 -4.4537811 -4.470849 -4.4954505 -4.505157 -4.4469986 -4.3067427 -4.1484509 -4.0746393 -4.1421165 -4.2876592 -4.4213519 -4.4740729 -4.4246268 -4.3325777][-4.4094305 -4.4288673 -4.4422488 -4.4617624 -4.4482174 -4.3328543 -4.1152625 -3.9049449 -3.8385608 -3.9585586 -4.1544461 -4.3230948 -4.4033 -4.3647141 -4.2789621][-4.382175 -4.3996882 -4.4119458 -4.4241347 -4.3822575 -4.2117639 -3.9355094 -3.6954327 -3.6471741 -3.8107131 -4.0356622 -4.223846 -4.3376665 -4.3358464 -4.283011][-4.3916736 -4.4107547 -4.4226093 -4.4258895 -4.3593922 -4.1547246 -3.8583372 -3.6211505 -3.5925155 -3.7742491 -3.9952955 -4.17448 -4.3050704 -4.3460846 -4.3376617][-4.4473886 -4.4749136 -4.4938006 -4.4973512 -4.4226127 -4.2137475 -3.9343319 -3.7237308 -3.7047155 -3.8724439 -4.0658278 -4.2197261 -4.3434024 -4.4052095 -4.421474][-4.5147853 -4.5566335 -4.5946388 -4.6167512 -4.5583243 -4.3715219 -4.1264806 -3.9429009 -3.9152277 -4.0458 -4.2104259 -4.3454785 -4.4480972 -4.4963264 -4.49844][-4.5483818 -4.6035862 -4.6695 -4.7276683 -4.7091489 -4.566267 -4.3640356 -4.2042069 -4.1605811 -4.2484145 -4.3855844 -4.5041504 -4.5729566 -4.5781207 -4.5383773][-4.5172334 -4.5794458 -4.6678991 -4.7632952 -4.7958064 -4.713593 -4.5656281 -4.4365005 -4.3883247 -4.4405012 -4.543983 -4.6327925 -4.656846 -4.612607 -4.5339928][-4.4454651 -4.5025196 -4.5942984 -4.7057533 -4.777627 -4.7596865 -4.6775689 -4.5928183 -4.5568352 -4.5852261 -4.647676 -4.6908669 -4.6703134 -4.594089 -4.5016451][-4.3865047 -4.4295745 -4.5025706 -4.5974174 -4.6757193 -4.6998181 -4.6760292 -4.6403971 -4.6280766 -4.6467237 -4.6746855 -4.675633 -4.6260815 -4.5436211 -4.4615364][-4.3573995 -4.3860693 -4.4308639 -4.489779 -4.5453691 -4.5784369 -4.5868754 -4.5854864 -4.5917797 -4.6067677 -4.6150193 -4.5959115 -4.5446396 -4.4789329 -4.4210238]]...]
INFO - root - 2017-12-08 00:37:57.955084: step 62210, loss = 21.40, batch loss = 21.32 (8.6 examples/sec; 0.930 sec/batch; 69h:49m:14s remains)
INFO - root - 2017-12-08 00:38:07.378009: step 62220, loss = 21.64, batch loss = 21.56 (7.9 examples/sec; 1.014 sec/batch; 76h:07m:11s remains)
INFO - root - 2017-12-08 00:38:16.786635: step 62230, loss = 21.29, batch loss = 21.21 (8.7 examples/sec; 0.918 sec/batch; 68h:52m:53s remains)
INFO - root - 2017-12-08 00:38:26.192354: step 62240, loss = 21.51, batch loss = 21.43 (8.4 examples/sec; 0.947 sec/batch; 71h:06m:13s remains)
INFO - root - 2017-12-08 00:38:35.492336: step 62250, loss = 21.60, batch loss = 21.52 (9.0 examples/sec; 0.893 sec/batch; 67h:03m:52s remains)
INFO - root - 2017-12-08 00:38:44.795558: step 62260, loss = 21.64, batch loss = 21.55 (9.0 examples/sec; 0.891 sec/batch; 66h:53m:32s remains)
INFO - root - 2017-12-08 00:38:54.196325: step 62270, loss = 21.67, batch loss = 21.59 (8.7 examples/sec; 0.921 sec/batch; 69h:06m:37s remains)
INFO - root - 2017-12-08 00:39:03.619451: step 62280, loss = 21.43, batch loss = 21.35 (8.4 examples/sec; 0.950 sec/batch; 71h:16m:58s remains)
INFO - root - 2017-12-08 00:39:13.062130: step 62290, loss = 21.66, batch loss = 21.57 (8.2 examples/sec; 0.970 sec/batch; 72h:49m:25s remains)
INFO - root - 2017-12-08 00:39:22.544015: step 62300, loss = 21.86, batch loss = 21.78 (7.9 examples/sec; 1.008 sec/batch; 75h:40m:30s remains)
2017-12-08 00:39:23.520583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3827786 -4.4560094 -4.5437284 -4.5885434 -4.5689416 -4.523232 -4.4818954 -4.4688153 -4.4820433 -4.489264 -4.4671597 -4.4093051 -4.3248491 -4.2525277 -4.2283478][-4.4609461 -4.5189686 -4.5830755 -4.5912256 -4.5345449 -4.4693766 -4.4252119 -4.4215922 -4.4552765 -4.483233 -4.479085 -4.4394975 -4.3639731 -4.2866058 -4.2447867][-4.50545 -4.5426178 -4.5778036 -4.551712 -4.473196 -4.4106121 -4.3835077 -4.3929935 -4.4381022 -4.4720244 -4.4721384 -4.4455233 -4.3864713 -4.3252192 -4.2935038][-4.4635367 -4.4870572 -4.5028262 -4.4569807 -4.368577 -4.3106132 -4.2953973 -4.3090448 -4.3625321 -4.4129248 -4.4275441 -4.4175072 -4.3790112 -4.3452053 -4.3430338][-4.4312811 -4.4432397 -4.43822 -4.3736458 -4.2707019 -4.2013884 -4.1754065 -4.1776476 -4.2353907 -4.3133631 -4.358439 -4.371685 -4.3609533 -4.3644223 -4.4005866][-4.4711227 -4.4909735 -4.4774914 -4.3927579 -4.2540188 -4.1313787 -4.0456777 -4.0056596 -4.055079 -4.164793 -4.25871 -4.3112617 -4.3444171 -4.3946424 -4.4680858][-4.5441079 -4.5906196 -4.592257 -4.4974432 -4.3105755 -4.0960336 -3.897465 -3.77638 -3.7973506 -3.937058 -4.0940104 -4.205267 -4.2949915 -4.393743 -4.4994211][-4.6066833 -4.676537 -4.7012382 -4.6078258 -4.3889189 -4.1010671 -3.8061941 -3.6141067 -3.6017509 -3.7500782 -3.9446065 -4.09512 -4.2215781 -4.3493948 -4.4723911][-4.5957956 -4.6853781 -4.7358112 -4.6577611 -4.4399276 -4.1371603 -3.8197336 -3.6099491 -3.5781021 -3.7119756 -3.9051371 -4.0635414 -4.1973782 -4.3242807 -4.4373145][-4.5020542 -4.6081581 -4.694191 -4.6611223 -4.4844389 -4.2122507 -3.9213259 -3.72012 -3.6718633 -3.7822552 -3.9670084 -4.1372924 -4.2745247 -4.3823023 -4.4590707][-4.3955483 -4.5103588 -4.6304026 -4.6596041 -4.5481558 -4.3230963 -4.0678263 -3.8777483 -3.8187952 -3.9147518 -4.1043444 -4.2950439 -4.4296975 -4.5029559 -4.5236492][-4.3192387 -4.4339862 -4.5685687 -4.6477923 -4.600915 -4.4318519 -4.2275105 -4.0765638 -4.040524 -4.1399746 -4.3207517 -4.4920573 -4.5828247 -4.5967464 -4.5559106][-4.29072 -4.3982067 -4.525681 -4.6293526 -4.6379347 -4.5348315 -4.3961949 -4.3060503 -4.3163519 -4.4244952 -4.568481 -4.6703053 -4.6801786 -4.6213617 -4.529047][-4.3038397 -4.4001403 -4.5095296 -4.6151581 -4.6634746 -4.6269865 -4.555037 -4.5195079 -4.5628829 -4.6640663 -4.7560539 -4.7783709 -4.7118731 -4.5949225 -4.4690123][-4.3675189 -4.45687 -4.5472307 -4.6377106 -4.6971731 -4.6992054 -4.6644664 -4.646163 -4.6815066 -4.7487583 -4.7890124 -4.7610912 -4.6649928 -4.5348573 -4.4043055]]...]
INFO - root - 2017-12-08 00:39:32.592145: step 62310, loss = 22.04, batch loss = 21.96 (8.5 examples/sec; 0.938 sec/batch; 70h:24m:57s remains)
INFO - root - 2017-12-08 00:39:41.926661: step 62320, loss = 21.46, batch loss = 21.38 (8.9 examples/sec; 0.901 sec/batch; 67h:36m:36s remains)
INFO - root - 2017-12-08 00:39:51.305268: step 62330, loss = 21.50, batch loss = 21.41 (9.0 examples/sec; 0.886 sec/batch; 66h:31m:32s remains)
INFO - root - 2017-12-08 00:40:00.781880: step 62340, loss = 21.26, batch loss = 21.18 (8.5 examples/sec; 0.946 sec/batch; 70h:57m:58s remains)
INFO - root - 2017-12-08 00:40:10.138597: step 62350, loss = 21.89, batch loss = 21.81 (8.1 examples/sec; 0.994 sec/batch; 74h:33m:33s remains)
INFO - root - 2017-12-08 00:40:19.430994: step 62360, loss = 21.30, batch loss = 21.22 (8.2 examples/sec; 0.977 sec/batch; 73h:17m:41s remains)
INFO - root - 2017-12-08 00:40:28.847360: step 62370, loss = 21.45, batch loss = 21.37 (8.6 examples/sec; 0.927 sec/batch; 69h:34m:19s remains)
INFO - root - 2017-12-08 00:40:38.260280: step 62380, loss = 21.49, batch loss = 21.41 (8.5 examples/sec; 0.944 sec/batch; 70h:48m:43s remains)
INFO - root - 2017-12-08 00:40:47.616644: step 62390, loss = 21.23, batch loss = 21.15 (8.1 examples/sec; 0.986 sec/batch; 74h:00m:24s remains)
INFO - root - 2017-12-08 00:40:57.054511: step 62400, loss = 21.62, batch loss = 21.54 (8.5 examples/sec; 0.943 sec/batch; 70h:46m:35s remains)
2017-12-08 00:40:58.053509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.363759 -4.4365363 -4.5108981 -4.5364389 -4.5215111 -4.4992352 -4.4992189 -4.4992075 -4.4741917 -4.4363637 -4.4096217 -4.3702607 -4.32775 -4.3181229 -4.3274751][-4.3723078 -4.4457498 -4.5129514 -4.524601 -4.4954677 -4.4777174 -4.5015683 -4.5216217 -4.4830103 -4.4076605 -4.3561339 -4.3237433 -4.2986259 -4.3143773 -4.34988][-4.4615498 -4.5268383 -4.5704894 -4.5455847 -4.4722962 -4.4245648 -4.4445267 -4.4635143 -4.4110518 -4.3263822 -4.2884526 -4.2921004 -4.2941632 -4.3202553 -4.3573093][-4.53654 -4.5966959 -4.6117516 -4.5458579 -4.4209595 -4.323535 -4.3142686 -4.3179445 -4.2643595 -4.2101216 -4.2221842 -4.2756538 -4.3059297 -4.3297157 -4.3534694][-4.5430312 -4.5938711 -4.57187 -4.4684463 -4.3071194 -4.16032 -4.1114564 -4.1059089 -4.0769663 -4.0865602 -4.1694479 -4.2716045 -4.3282514 -4.355144 -4.3708873][-4.4781179 -4.5207491 -4.4612164 -4.3318143 -4.1625571 -3.9853652 -3.8962147 -3.8869529 -3.9025347 -3.998347 -4.1562867 -4.2883253 -4.3559804 -4.3866673 -4.4077926][-4.3753548 -4.4232979 -4.3442364 -4.2055707 -4.0486908 -3.8580937 -3.7325623 -3.71736 -3.7786589 -3.9521847 -4.1677194 -4.3048835 -4.36248 -4.3938069 -4.4264069][-4.2932715 -4.3550153 -4.2787113 -4.1457787 -4.0112934 -3.8213916 -3.665977 -3.6388898 -3.7389042 -3.9684367 -4.2090797 -4.3338237 -4.3750896 -4.4016008 -4.4390306][-4.2576971 -4.3261576 -4.2763062 -4.1738377 -4.0711808 -3.8970063 -3.729625 -3.687588 -3.7993968 -4.0389447 -4.2653913 -4.370142 -4.4037356 -4.4288712 -4.4593883][-4.2542162 -4.3191328 -4.3106947 -4.2651873 -4.2065349 -4.0620975 -3.9042287 -3.8447406 -3.9305232 -4.1250081 -4.3039756 -4.3912244 -4.4228768 -4.4434557 -4.4579854][-4.2839975 -4.3482447 -4.3861427 -4.404326 -4.3861504 -4.2657804 -4.1220446 -4.0426211 -4.0729408 -4.1935668 -4.3208385 -4.4013619 -4.4292836 -4.4363909 -4.4342122][-4.3417091 -4.4157839 -4.4891424 -4.553328 -4.5583491 -4.4551811 -4.3314481 -4.2420559 -4.21148 -4.2529488 -4.335722 -4.4058766 -4.4155664 -4.39925 -4.3920913][-4.4163666 -4.4916654 -4.5712419 -4.6496677 -4.6612606 -4.5732722 -4.4715762 -4.3790665 -4.3021393 -4.2866373 -4.3410015 -4.4009757 -4.3910551 -4.3591433 -4.3640327][-4.50889 -4.5612822 -4.6152248 -4.6763973 -4.6823382 -4.6106815 -4.5341592 -4.4514503 -4.3567066 -4.3166986 -4.3594646 -4.408639 -4.388978 -4.359303 -4.3867073][-4.6043129 -4.6216888 -4.6382694 -4.6726766 -4.676579 -4.629878 -4.5813236 -4.5179505 -4.4316053 -4.3906059 -4.423986 -4.4581866 -4.4381709 -4.4213452 -4.464859]]...]
INFO - root - 2017-12-08 00:41:07.448834: step 62410, loss = 21.27, batch loss = 21.19 (8.3 examples/sec; 0.965 sec/batch; 72h:21m:46s remains)
INFO - root - 2017-12-08 00:41:16.970372: step 62420, loss = 21.57, batch loss = 21.49 (8.2 examples/sec; 0.974 sec/batch; 73h:04m:30s remains)
INFO - root - 2017-12-08 00:41:26.245085: step 62430, loss = 21.12, batch loss = 21.04 (8.6 examples/sec; 0.926 sec/batch; 69h:27m:27s remains)
INFO - root - 2017-12-08 00:41:35.656448: step 62440, loss = 21.53, batch loss = 21.45 (8.7 examples/sec; 0.920 sec/batch; 69h:01m:21s remains)
INFO - root - 2017-12-08 00:41:45.123268: step 62450, loss = 21.44, batch loss = 21.35 (8.9 examples/sec; 0.894 sec/batch; 67h:04m:04s remains)
INFO - root - 2017-12-08 00:41:54.600573: step 62460, loss = 21.67, batch loss = 21.59 (8.4 examples/sec; 0.948 sec/batch; 71h:06m:44s remains)
INFO - root - 2017-12-08 00:42:03.953417: step 62470, loss = 21.42, batch loss = 21.33 (8.2 examples/sec; 0.974 sec/batch; 73h:01m:41s remains)
INFO - root - 2017-12-08 00:42:13.194041: step 62480, loss = 21.42, batch loss = 21.33 (9.3 examples/sec; 0.857 sec/batch; 64h:17m:58s remains)
INFO - root - 2017-12-08 00:42:22.634694: step 62490, loss = 21.40, batch loss = 21.32 (8.1 examples/sec; 0.989 sec/batch; 74h:12m:42s remains)
INFO - root - 2017-12-08 00:42:32.102888: step 62500, loss = 21.57, batch loss = 21.49 (9.0 examples/sec; 0.885 sec/batch; 66h:24m:36s remains)
2017-12-08 00:42:33.049929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5676918 -4.502924 -4.4966092 -4.5498195 -4.6209526 -4.6764226 -4.6914692 -4.6648374 -4.6161466 -4.5704403 -4.528028 -4.5191789 -4.5448937 -4.5853786 -4.64869][-4.5520945 -4.4484529 -4.4216528 -4.4686608 -4.538682 -4.5990562 -4.6195178 -4.5835428 -4.5232639 -4.4916983 -4.4698443 -4.4870558 -4.528451 -4.5744305 -4.6599808][-4.4626765 -4.3467059 -4.3274345 -4.3777814 -4.439528 -4.4901514 -4.5046458 -4.4512138 -4.3754134 -4.3630605 -4.3740444 -4.4256835 -4.4845338 -4.5319228 -4.6377058][-4.3746004 -4.2674551 -4.2776556 -4.3408227 -4.3867607 -4.412405 -4.4020619 -4.3266392 -4.2451067 -4.259048 -4.3091083 -4.3925519 -4.4646311 -4.509933 -4.6193485][-4.3254004 -4.2268333 -4.255939 -4.3242407 -4.3510466 -4.3413305 -4.281786 -4.1707368 -4.08977 -4.140419 -4.2405248 -4.357038 -4.4420071 -4.4962449 -4.6055279][-4.2794003 -4.1636157 -4.1821904 -4.2514472 -4.2780423 -4.2463579 -4.124115 -3.9582336 -3.8763061 -3.9724934 -4.1334138 -4.2829556 -4.3781776 -4.4513006 -4.5676684][-4.2154632 -4.0870504 -4.0968466 -4.1816759 -4.2296891 -4.1862369 -3.9945092 -3.7677703 -3.6950285 -3.8496747 -4.0754871 -4.243485 -4.322938 -4.3957024 -4.5014539][-4.1648531 -4.0414143 -4.0580931 -4.1642828 -4.2372661 -4.1822743 -3.9433091 -3.6800666 -3.6266422 -3.823585 -4.0849228 -4.2396955 -4.2731576 -4.32242 -4.4018297][-4.1981554 -4.078577 -4.0928836 -4.20085 -4.2938862 -4.2589283 -4.0388308 -3.7909014 -3.7563932 -3.9457469 -4.1798668 -4.2840133 -4.2630839 -4.2887182 -4.3501153][-4.3110075 -4.187335 -4.1860423 -4.2723293 -4.3792768 -4.3915009 -4.2416711 -4.0396662 -3.9987516 -4.1302862 -4.2939982 -4.3402295 -4.2918816 -4.3220077 -4.3947935][-4.429513 -4.3035131 -4.2796445 -4.339385 -4.45502 -4.5192838 -4.4463487 -4.2968125 -4.2416239 -4.3131852 -4.411797 -4.4152513 -4.35724 -4.3947811 -4.4855027][-4.5270376 -4.4257746 -4.4013577 -4.4450345 -4.5530767 -4.6346955 -4.6054487 -4.5013204 -4.4484639 -4.4889126 -4.5478754 -4.5273972 -4.4606977 -4.4803958 -4.5606546][-4.6000414 -4.5497422 -4.5437765 -4.5779133 -4.6575274 -4.7172422 -4.69487 -4.6182723 -4.5734806 -4.5985274 -4.6369276 -4.6150556 -4.553556 -4.5476389 -4.5923252][-4.6072164 -4.5986052 -4.608273 -4.6349869 -4.682723 -4.710402 -4.6860323 -4.6293297 -4.5915637 -4.5991735 -4.6186152 -4.6049151 -4.5638952 -4.547648 -4.561389][-4.5476871 -4.5545111 -4.5681171 -4.586339 -4.6088948 -4.6140089 -4.5931087 -4.5565557 -4.5282435 -4.5235519 -4.5281262 -4.5207572 -4.50014 -4.4882965 -4.4894962]]...]
INFO - root - 2017-12-08 00:42:42.414896: step 62510, loss = 21.30, batch loss = 21.21 (8.9 examples/sec; 0.896 sec/batch; 67h:12m:46s remains)
INFO - root - 2017-12-08 00:42:51.888296: step 62520, loss = 21.23, batch loss = 21.15 (8.8 examples/sec; 0.908 sec/batch; 68h:04m:39s remains)
INFO - root - 2017-12-08 00:43:01.313715: step 62530, loss = 21.29, batch loss = 21.21 (8.6 examples/sec; 0.929 sec/batch; 69h:40m:30s remains)
INFO - root - 2017-12-08 00:43:10.840633: step 62540, loss = 21.72, batch loss = 21.64 (8.3 examples/sec; 0.965 sec/batch; 72h:22m:35s remains)
INFO - root - 2017-12-08 00:43:20.370928: step 62550, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.971 sec/batch; 72h:46m:37s remains)
INFO - root - 2017-12-08 00:43:29.725031: step 62560, loss = 21.82, batch loss = 21.74 (8.5 examples/sec; 0.942 sec/batch; 70h:38m:05s remains)
INFO - root - 2017-12-08 00:43:38.955749: step 62570, loss = 21.46, batch loss = 21.38 (8.8 examples/sec; 0.905 sec/batch; 67h:49m:48s remains)
INFO - root - 2017-12-08 00:43:48.403734: step 62580, loss = 21.24, batch loss = 21.16 (8.8 examples/sec; 0.909 sec/batch; 68h:11m:27s remains)
INFO - root - 2017-12-08 00:43:57.803130: step 62590, loss = 21.99, batch loss = 21.91 (8.7 examples/sec; 0.920 sec/batch; 68h:57m:14s remains)
INFO - root - 2017-12-08 00:44:07.205600: step 62600, loss = 20.92, batch loss = 20.83 (8.5 examples/sec; 0.936 sec/batch; 70h:12m:33s remains)
2017-12-08 00:44:08.279707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3024025 -4.3655105 -4.4240985 -4.4480686 -4.4596853 -4.4796839 -4.4897809 -4.4731517 -4.4293 -4.3772516 -4.3291988 -4.3004637 -4.3068342 -4.3324866 -4.3408375][-4.3835473 -4.422214 -4.4626489 -4.47673 -4.4646869 -4.4388142 -4.4097962 -4.3925118 -4.3918028 -4.41054 -4.42636 -4.4181485 -4.3918452 -4.3575931 -4.3177528][-4.4632764 -4.4680753 -4.47664 -4.4704542 -4.4369717 -4.3754058 -4.308002 -4.2737956 -4.2969122 -4.3765845 -4.4589243 -4.4834256 -4.4437242 -4.37687 -4.316494][-4.4990573 -4.4834585 -4.4666748 -4.4384174 -4.379786 -4.2770696 -4.1596165 -4.0965595 -4.1388626 -4.2740593 -4.4166145 -4.4803309 -4.4543939 -4.3897548 -4.3325958][-4.4769168 -4.4608541 -4.4394488 -4.3996048 -4.3124819 -4.1578717 -3.9831805 -3.8943713 -3.9646804 -4.1548495 -4.344553 -4.4400043 -4.4358716 -4.3841572 -4.3292632][-4.405189 -4.3959074 -4.3851018 -4.3414855 -4.2215476 -4.0079908 -3.7802279 -3.6814804 -3.7949185 -4.0420389 -4.2710528 -4.3909068 -4.4053588 -4.3604627 -4.2981949][-4.3258576 -4.319283 -4.322331 -4.2832656 -4.1417508 -3.8834393 -3.6188395 -3.5161235 -3.6623158 -3.9461904 -4.1992755 -4.3378344 -4.3704562 -4.3361053 -4.270988][-4.2880559 -4.2804413 -4.2955995 -4.2730074 -4.1362677 -3.8652794 -3.5860827 -3.4743304 -3.6177337 -3.8994188 -4.1572742 -4.3140235 -4.3690333 -4.3519311 -4.2950006][-4.3226056 -4.3152466 -4.338129 -4.3340325 -4.2164826 -3.9546473 -3.6724505 -3.5436797 -3.6578672 -3.9155493 -4.1718774 -4.3478045 -4.4211922 -4.4179721 -4.3762808][-4.4016685 -4.3956027 -4.4219184 -4.4340792 -4.3428354 -4.106885 -3.8356676 -3.6942811 -3.7767251 -4.0078382 -4.2579947 -4.4329414 -4.4951057 -4.4840622 -4.4536109][-4.4633336 -4.4569082 -4.4874949 -4.5209842 -4.4705276 -4.2883453 -4.0579104 -3.9225776 -3.9762573 -4.1714587 -4.3967624 -4.540669 -4.5619731 -4.5221891 -4.4922476][-4.4583206 -4.4551039 -4.502562 -4.5726967 -4.5835171 -4.4829607 -4.3166776 -4.1941977 -4.2086906 -4.3485317 -4.5245137 -4.6208868 -4.5973248 -4.5259762 -4.487608][-4.3856621 -4.3852096 -4.4558134 -4.56804 -4.6444378 -4.6274843 -4.5281773 -4.4228115 -4.4038744 -4.486515 -4.6032763 -4.649754 -4.595046 -4.5072789 -4.4611831][-4.2795191 -4.2740335 -4.3575544 -4.4954104 -4.6163235 -4.6616426 -4.6217151 -4.5470037 -4.5170231 -4.5545297 -4.6105309 -4.612463 -4.5456572 -4.4618073 -4.4134593][-4.1770363 -4.1549468 -4.2297 -4.3644733 -4.4974623 -4.5777683 -4.588129 -4.5563188 -4.5326295 -4.533339 -4.5325384 -4.50112 -4.4390082 -4.3719883 -4.3244486]]...]
INFO - root - 2017-12-08 00:44:17.532114: step 62610, loss = 21.40, batch loss = 21.32 (8.8 examples/sec; 0.911 sec/batch; 68h:17m:13s remains)
INFO - root - 2017-12-08 00:44:27.053492: step 62620, loss = 21.40, batch loss = 21.32 (8.6 examples/sec; 0.927 sec/batch; 69h:30m:04s remains)
INFO - root - 2017-12-08 00:44:36.472538: step 62630, loss = 21.42, batch loss = 21.34 (8.8 examples/sec; 0.912 sec/batch; 68h:21m:50s remains)
INFO - root - 2017-12-08 00:44:45.869037: step 62640, loss = 21.90, batch loss = 21.81 (8.4 examples/sec; 0.947 sec/batch; 70h:59m:26s remains)
INFO - root - 2017-12-08 00:44:55.211954: step 62650, loss = 21.72, batch loss = 21.64 (8.7 examples/sec; 0.917 sec/batch; 68h:43m:44s remains)
INFO - root - 2017-12-08 00:45:04.571752: step 62660, loss = 21.34, batch loss = 21.26 (8.4 examples/sec; 0.956 sec/batch; 71h:40m:40s remains)
INFO - root - 2017-12-08 00:45:13.897531: step 62670, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.942 sec/batch; 70h:36m:31s remains)
INFO - root - 2017-12-08 00:45:23.380659: step 62680, loss = 22.06, batch loss = 21.97 (8.3 examples/sec; 0.968 sec/batch; 72h:34m:23s remains)
INFO - root - 2017-12-08 00:45:32.937277: step 62690, loss = 21.69, batch loss = 21.61 (8.2 examples/sec; 0.975 sec/batch; 73h:04m:05s remains)
INFO - root - 2017-12-08 00:45:42.423581: step 62700, loss = 21.03, batch loss = 20.94 (8.7 examples/sec; 0.917 sec/batch; 68h:44m:46s remains)
2017-12-08 00:45:43.426745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4218593 -4.1839662 -4.1093225 -4.232296 -4.3936844 -4.5430908 -4.6715975 -4.7302485 -4.7163219 -4.7040009 -4.7387609 -4.8050861 -4.8202038 -4.6870847 -4.5124044][-4.3987613 -4.1849961 -4.1397271 -4.287096 -4.4563708 -4.5986104 -4.7020693 -4.7291713 -4.7066169 -4.7135839 -4.7682734 -4.8429842 -4.8423376 -4.6529818 -4.4150138][-4.4044695 -4.259408 -4.2579923 -4.4074373 -4.5537734 -4.6582112 -4.7178664 -4.7140989 -4.6915712 -4.7164431 -4.7800441 -4.855402 -4.8415961 -4.61672 -4.3355985][-4.4715385 -4.4047318 -4.4308028 -4.5365992 -4.6026011 -4.61997 -4.6168132 -4.5966172 -4.594986 -4.64302 -4.7173076 -4.8017306 -4.7934561 -4.5687823 -4.2847991][-4.579772 -4.562243 -4.5685892 -4.57389 -4.505743 -4.3955765 -4.3158388 -4.2992196 -4.3544683 -4.4576254 -4.5738635 -4.6974688 -4.7274432 -4.548048 -4.3161778][-4.6313868 -4.6386528 -4.5974631 -4.4710846 -4.2388391 -3.9793961 -3.8142891 -3.8132317 -3.966557 -4.1885624 -4.3987722 -4.5889125 -4.6759648 -4.5682144 -4.4248829][-4.6196594 -4.6549072 -4.5802984 -4.3417883 -3.96152 -3.5677631 -3.3218617 -3.3394966 -3.6131022 -3.9824464 -4.29241 -4.5265088 -4.65575 -4.6278038 -4.5862217][-4.6236248 -4.7070327 -4.6421032 -4.361907 -3.9208384 -3.4598968 -3.1530092 -3.1737401 -3.5390055 -4.0191364 -4.3774304 -4.5905743 -4.7007594 -4.7096229 -4.7278976][-4.6543045 -4.7901425 -4.7687097 -4.5205483 -4.113131 -3.6761327 -3.3665009 -3.3806572 -3.7639306 -4.2616992 -4.5979595 -4.7433758 -4.78441 -4.76784 -4.7693219][-4.6995769 -4.8595209 -4.8765435 -4.6944942 -4.3755474 -4.0268288 -3.7724493 -3.7830048 -4.1005445 -4.5078964 -4.7737722 -4.8634162 -4.8586435 -4.8169594 -4.7831459][-4.7264595 -4.8914003 -4.9412823 -4.8371706 -4.6250863 -4.3914609 -4.2289796 -4.2470469 -4.4539189 -4.7066765 -4.8660975 -4.9045939 -4.8754535 -4.8230133 -4.7698488][-4.7016177 -4.8593669 -4.928781 -4.8864217 -4.7660713 -4.6385517 -4.5715919 -4.6072669 -4.7157 -4.82058 -4.868917 -4.85424 -4.8034573 -4.739255 -4.6685338][-4.6402431 -4.7672696 -4.8320446 -4.8251858 -4.7745538 -4.7279577 -4.7208614 -4.7542596 -4.7937317 -4.8089929 -4.7952509 -4.7569389 -4.70151 -4.6343203 -4.5584021][-4.60625 -4.6924338 -4.7401223 -4.7498364 -4.7436714 -4.7440872 -4.7582068 -4.7738504 -4.773087 -4.7538013 -4.7252607 -4.6911063 -4.6474752 -4.5940657 -4.5363145][-4.5882878 -4.6347961 -4.6599479 -4.6680951 -4.6740737 -4.6867452 -4.7014356 -4.7063508 -4.6948414 -4.6742449 -4.6532445 -4.6326942 -4.6082225 -4.5798497 -4.5525074]]...]
INFO - root - 2017-12-08 00:45:52.879770: step 62710, loss = 21.24, batch loss = 21.16 (8.5 examples/sec; 0.936 sec/batch; 70h:09m:31s remains)
INFO - root - 2017-12-08 00:46:02.504929: step 62720, loss = 21.49, batch loss = 21.40 (8.0 examples/sec; 1.001 sec/batch; 75h:00m:24s remains)
INFO - root - 2017-12-08 00:46:11.945711: step 62730, loss = 21.37, batch loss = 21.29 (8.5 examples/sec; 0.941 sec/batch; 70h:31m:28s remains)
INFO - root - 2017-12-08 00:46:21.397328: step 62740, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.954 sec/batch; 71h:30m:11s remains)
INFO - root - 2017-12-08 00:46:30.734771: step 62750, loss = 21.28, batch loss = 21.20 (8.9 examples/sec; 0.894 sec/batch; 66h:58m:55s remains)
INFO - root - 2017-12-08 00:46:40.103885: step 62760, loss = 21.30, batch loss = 21.22 (9.0 examples/sec; 0.887 sec/batch; 66h:27m:32s remains)
INFO - root - 2017-12-08 00:46:49.403466: step 62770, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.931 sec/batch; 69h:43m:28s remains)
INFO - root - 2017-12-08 00:46:58.728512: step 62780, loss = 21.01, batch loss = 20.93 (8.6 examples/sec; 0.928 sec/batch; 69h:32m:04s remains)
INFO - root - 2017-12-08 00:47:08.033599: step 62790, loss = 21.57, batch loss = 21.49 (9.1 examples/sec; 0.882 sec/batch; 66h:06m:00s remains)
INFO - root - 2017-12-08 00:47:17.359327: step 62800, loss = 21.56, batch loss = 21.48 (8.2 examples/sec; 0.970 sec/batch; 72h:41m:36s remains)
2017-12-08 00:47:18.286472: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3440075 -4.3422155 -4.3401046 -4.335567 -4.3306227 -4.3285723 -4.3287306 -4.3290167 -4.3274775 -4.3236737 -4.3183794 -4.3118544 -4.3047237 -4.2961636 -4.284359][-4.4203005 -4.4288383 -4.433516 -4.4317675 -4.4266191 -4.4219437 -4.4198623 -4.4185629 -4.4159174 -4.4113388 -4.4055643 -4.39902 -4.3917 -4.3808575 -4.3595343][-4.4937725 -4.5120049 -4.5194349 -4.5154004 -4.5035257 -4.4899468 -4.4798241 -4.4721494 -4.46577 -4.4612832 -4.459075 -4.4584336 -4.4572315 -4.447783 -4.417017][-4.5418696 -4.5568104 -4.554914 -4.537456 -4.510572 -4.483767 -4.4630485 -4.4458666 -4.4327793 -4.4302759 -4.4409304 -4.4606104 -4.4797359 -4.4807811 -4.445436][-4.492424 -4.4824476 -4.4534116 -4.4101028 -4.3631177 -4.3251543 -4.298398 -4.2736344 -4.2534208 -4.2578645 -4.2986951 -4.3628383 -4.4223566 -4.4452791 -4.4124246][-4.3580875 -4.3096366 -4.2439089 -4.1704631 -4.1040158 -4.06007 -4.0328403 -4.0028019 -3.9737949 -3.9871469 -4.0722442 -4.20094 -4.3156772 -4.368834 -4.3452725][-4.2134242 -4.1301527 -4.035584 -3.9417384 -3.8639445 -3.8189864 -3.7930219 -3.7562151 -3.7152524 -3.7362654 -3.8694353 -4.0652065 -4.2323947 -4.31334 -4.301446][-4.1254206 -4.0250216 -3.920928 -3.8215263 -3.7390809 -3.6928909 -3.6639247 -3.6168036 -3.5603006 -3.5813749 -3.7459416 -3.9853249 -4.183866 -4.2808952 -4.2781758][-4.1332092 -4.0380888 -3.9482763 -3.8622787 -3.7842264 -3.7352915 -3.7009385 -3.64125 -3.5654914 -3.5711699 -3.7277005 -3.9626114 -4.1553545 -4.2501049 -4.2502036][-4.216331 -4.1414456 -4.0821862 -4.0277233 -3.9706008 -3.9290326 -3.8955657 -3.8332005 -3.751451 -3.7407374 -3.856256 -4.0369678 -4.1806264 -4.24267 -4.2272234][-4.29215 -4.24132 -4.2163091 -4.2042222 -4.1853347 -4.1647363 -4.1418781 -4.094274 -4.0316062 -4.0187945 -4.087368 -4.1942768 -4.2693381 -4.282012 -4.2384591][-4.307322 -4.2764158 -4.2796936 -4.3048954 -4.3220482 -4.3230643 -4.3166695 -4.2996955 -4.275249 -4.273098 -4.3053417 -4.3513436 -4.3734617 -4.35024 -4.2858329][-4.2535148 -4.2328644 -4.2538986 -4.2980161 -4.32861 -4.3365197 -4.3451152 -4.3671279 -4.3871183 -4.4000654 -4.4116149 -4.4231133 -4.4241834 -4.3940139 -4.3273745][-4.1678715 -4.1518121 -4.1769266 -4.2166553 -4.232779 -4.232851 -4.2591152 -4.3252006 -4.3855619 -4.4064445 -4.3945475 -4.3804779 -4.3810477 -4.3674173 -4.3160224][-4.1026 -4.09181 -4.11339 -4.1343727 -4.1221614 -4.110539 -4.1572485 -4.2642746 -4.3560104 -4.3736877 -4.331676 -4.2931147 -4.2972031 -4.3045368 -4.2709007]]...]
INFO - root - 2017-12-08 00:47:27.488521: step 62810, loss = 21.32, batch loss = 21.24 (9.0 examples/sec; 0.887 sec/batch; 66h:28m:28s remains)
INFO - root - 2017-12-08 00:47:36.969718: step 62820, loss = 21.37, batch loss = 21.28 (8.6 examples/sec; 0.925 sec/batch; 69h:18m:10s remains)
INFO - root - 2017-12-08 00:47:46.128564: step 62830, loss = 21.25, batch loss = 21.16 (8.3 examples/sec; 0.960 sec/batch; 71h:56m:17s remains)
INFO - root - 2017-12-08 00:47:55.358604: step 62840, loss = 21.61, batch loss = 21.52 (8.8 examples/sec; 0.912 sec/batch; 68h:19m:28s remains)
INFO - root - 2017-12-08 00:48:04.723364: step 62850, loss = 22.03, batch loss = 21.94 (9.3 examples/sec; 0.856 sec/batch; 64h:08m:39s remains)
INFO - root - 2017-12-08 00:48:14.039525: step 62860, loss = 21.56, batch loss = 21.47 (8.9 examples/sec; 0.902 sec/batch; 67h:34m:37s remains)
INFO - root - 2017-12-08 00:48:23.489359: step 62870, loss = 21.65, batch loss = 21.57 (8.3 examples/sec; 0.967 sec/batch; 72h:27m:20s remains)
INFO - root - 2017-12-08 00:48:32.724338: step 62880, loss = 21.59, batch loss = 21.51 (8.4 examples/sec; 0.950 sec/batch; 71h:10m:39s remains)
INFO - root - 2017-12-08 00:48:42.181756: step 62890, loss = 21.34, batch loss = 21.26 (8.7 examples/sec; 0.919 sec/batch; 68h:48m:56s remains)
INFO - root - 2017-12-08 00:48:51.553562: step 62900, loss = 21.63, batch loss = 21.54 (8.5 examples/sec; 0.942 sec/batch; 70h:30m:53s remains)
2017-12-08 00:48:52.484422: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6131454 -4.606627 -4.6119132 -4.6729083 -4.7524238 -4.7728624 -4.7023015 -4.6249619 -4.6544342 -4.7804284 -4.8898187 -4.9226937 -4.8740191 -4.7505679 -4.5929966][-4.6707458 -4.6555524 -4.6499815 -4.7244596 -4.8160272 -4.809123 -4.6733823 -4.5395694 -4.5571094 -4.701138 -4.8405995 -4.9003134 -4.8696547 -4.7583284 -4.6073465][-4.6406188 -4.6176767 -4.615664 -4.7162108 -4.8122258 -4.7488542 -4.5153017 -4.3057256 -4.3051438 -4.4803452 -4.68309 -4.8045368 -4.8143034 -4.7285161 -4.5973959][-4.5694427 -4.5450287 -4.5554061 -4.6723957 -4.7455835 -4.6042533 -4.2802844 -4.0171781 -4.0082889 -4.2096024 -4.4629922 -4.6399851 -4.6946282 -4.6462097 -4.5524592][-4.517478 -4.499815 -4.5092368 -4.600409 -4.6046023 -4.3774452 -4.0088172 -3.7604208 -3.7960961 -4.0320482 -4.3046765 -4.502471 -4.582509 -4.5627327 -4.4997187][-4.5031018 -4.4974194 -4.4944277 -4.5280852 -4.4499726 -4.1672134 -3.8088107 -3.6174054 -3.7118645 -3.974966 -4.2440205 -4.4435806 -4.5425534 -4.5368414 -4.4769645][-4.507833 -4.5219884 -4.5006542 -4.461195 -4.3117895 -4.0209794 -3.7226045 -3.5988975 -3.7194467 -3.9722424 -4.2202287 -4.4252524 -4.5543323 -4.5619664 -4.4882784][-4.5268154 -4.5733638 -4.5469766 -4.4438257 -4.2311521 -3.944629 -3.7132342 -3.6494176 -3.773376 -3.9987283 -4.2330728 -4.449019 -4.600554 -4.6122251 -4.5174232][-4.5531769 -4.6273303 -4.6171155 -4.4927549 -4.25337 -3.96839 -3.7589107 -3.7031155 -3.8042374 -4.0117736 -4.2648664 -4.5056205 -4.6611052 -4.6577282 -4.5406208][-4.5442295 -4.6138039 -4.6128445 -4.5034127 -4.2968965 -4.0452514 -3.8403466 -3.7600133 -3.8272929 -4.0300369 -4.316402 -4.577394 -4.712101 -4.6760507 -4.5403528][-4.5005679 -4.5398312 -4.530293 -4.4500608 -4.3190923 -4.1451268 -3.9751208 -3.8910708 -3.9409361 -4.1406169 -4.4329586 -4.668592 -4.7442746 -4.6600771 -4.5109196][-4.46753 -4.4854674 -4.4715004 -4.4245434 -4.3669691 -4.2713628 -4.1502209 -4.0857 -4.1374831 -4.3237596 -4.5768638 -4.7387323 -4.73391 -4.6082892 -4.4625559][-4.4555783 -4.4656148 -4.4556189 -4.4393077 -4.4331636 -4.3920679 -4.31312 -4.2700477 -4.3215566 -4.4788203 -4.6683555 -4.7469258 -4.6797948 -4.5417957 -4.4185977][-4.4399123 -4.4514914 -4.4513183 -4.4611597 -4.4884319 -4.485476 -4.4428229 -4.4151587 -4.4506021 -4.5624714 -4.6840763 -4.6995826 -4.6065397 -4.4840608 -4.3914037][-4.4119921 -4.4310236 -4.444663 -4.4731994 -4.5154238 -4.5301609 -4.5101643 -4.4893856 -4.5024123 -4.5673785 -4.6329026 -4.6145687 -4.5269017 -4.4353414 -4.3730454]]...]
INFO - root - 2017-12-08 00:49:01.834705: step 62910, loss = 21.44, batch loss = 21.36 (8.2 examples/sec; 0.977 sec/batch; 73h:09m:00s remains)
INFO - root - 2017-12-08 00:49:11.172169: step 62920, loss = 21.51, batch loss = 21.43 (8.2 examples/sec; 0.976 sec/batch; 73h:03m:04s remains)
INFO - root - 2017-12-08 00:49:20.647396: step 62930, loss = 21.80, batch loss = 21.71 (8.4 examples/sec; 0.956 sec/batch; 71h:35m:49s remains)
INFO - root - 2017-12-08 00:49:30.265600: step 62940, loss = 21.30, batch loss = 21.22 (8.0 examples/sec; 0.996 sec/batch; 74h:36m:34s remains)
INFO - root - 2017-12-08 00:49:39.724394: step 62950, loss = 21.34, batch loss = 21.25 (8.2 examples/sec; 0.980 sec/batch; 73h:21m:14s remains)
INFO - root - 2017-12-08 00:49:49.061093: step 62960, loss = 21.38, batch loss = 21.30 (8.8 examples/sec; 0.914 sec/batch; 68h:23m:51s remains)
INFO - root - 2017-12-08 00:49:58.423543: step 62970, loss = 21.55, batch loss = 21.47 (8.7 examples/sec; 0.917 sec/batch; 68h:39m:05s remains)
INFO - root - 2017-12-08 00:50:07.666935: step 62980, loss = 21.57, batch loss = 21.48 (9.0 examples/sec; 0.888 sec/batch; 66h:29m:12s remains)
INFO - root - 2017-12-08 00:50:17.093990: step 62990, loss = 22.09, batch loss = 22.01 (8.4 examples/sec; 0.951 sec/batch; 71h:09m:49s remains)
INFO - root - 2017-12-08 00:50:26.363520: step 63000, loss = 21.94, batch loss = 21.86 (8.5 examples/sec; 0.942 sec/batch; 70h:32m:31s remains)
2017-12-08 00:50:27.278474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2544436 -4.1732564 -4.0416622 -3.9600677 -3.9789109 -4.0645466 -4.1257844 -4.1193147 -4.0428448 -3.944838 -3.9345191 -4.0603528 -4.25737 -4.397625 -4.43614][-4.2321472 -4.1294265 -3.9995677 -3.9377732 -3.976634 -4.0759163 -4.1627316 -4.1844921 -4.133235 -4.057559 -4.0581079 -4.1726851 -4.3380971 -4.4456186 -4.4624796][-4.2000809 -4.0747008 -3.9574318 -3.9300556 -3.9928927 -4.1028576 -4.2151732 -4.2771511 -4.2674508 -4.2240858 -4.2351742 -4.3256197 -4.4414787 -4.5046635 -4.4932551][-4.1533947 -4.0179505 -3.9169517 -3.9149413 -3.9914548 -4.0967555 -4.215045 -4.3066397 -4.342011 -4.3406115 -4.3700151 -4.444737 -4.5221114 -4.553853 -4.5285668][-4.0934358 -3.9647126 -3.8796339 -3.8836389 -3.9438751 -4.0120125 -4.0982628 -4.1921206 -4.2613792 -4.3104124 -4.378212 -4.4696717 -4.5473237 -4.5795245 -4.5610242][-4.0580907 -3.9377985 -3.8560913 -3.8461139 -3.8684568 -3.87885 -3.9019098 -3.9602246 -4.0345078 -4.1309471 -4.2531476 -4.3917484 -4.5076737 -4.5710607 -4.5772967][-4.0904517 -3.9718015 -3.8764589 -3.8394701 -3.824127 -3.7872653 -3.7419732 -3.7329817 -3.7726972 -3.8925695 -4.0646234 -4.2555184 -4.4229975 -4.5272932 -4.5626893][-4.2018547 -4.0814719 -3.9666181 -3.9065278 -3.8747911 -3.8267283 -3.7444344 -3.6669371 -3.6439447 -3.7446434 -3.931565 -4.14821 -4.3425817 -4.460856 -4.4996128][-4.3514819 -4.233748 -4.1087236 -4.0422249 -4.0200143 -3.9940174 -3.9170344 -3.8018682 -3.7194333 -3.7640376 -3.9153318 -4.1140709 -4.2935448 -4.3846545 -4.3914609][-4.4802423 -4.3751922 -4.254931 -4.1930246 -4.1904774 -4.1980085 -4.1499748 -4.0318122 -3.9139862 -3.8950243 -3.9793324 -4.1227522 -4.2514963 -4.2941151 -4.2585478][-4.54404 -4.4620595 -4.361784 -4.3140435 -4.3289723 -4.3609848 -4.3414683 -4.244853 -4.12515 -4.0650191 -4.0870829 -4.165627 -4.2311683 -4.2219849 -4.1505394][-4.5281329 -4.4679623 -4.3980246 -4.37528 -4.4058065 -4.4523616 -4.4608741 -4.4034414 -4.30913 -4.237781 -4.222311 -4.2518477 -4.26706 -4.2228975 -4.1361][-4.4664068 -4.4167118 -4.3715544 -4.374342 -4.4174471 -4.4717665 -4.5057025 -4.4898133 -4.4322324 -4.3776388 -4.357913 -4.3671484 -4.35931 -4.308085 -4.23025][-4.3981586 -4.3560057 -4.3345079 -4.36256 -4.4142351 -4.4684968 -4.5156813 -4.52632 -4.4986887 -4.4711623 -4.4672432 -4.4745121 -4.4610395 -4.4165606 -4.3571424][-4.3607759 -4.3269434 -4.3263683 -4.3738055 -4.4275913 -4.4700747 -4.5080042 -4.518188 -4.5028152 -4.500001 -4.5191269 -4.5381093 -4.5297747 -4.4954271 -4.4536304]]...]
INFO - root - 2017-12-08 00:50:36.781276: step 63010, loss = 21.32, batch loss = 21.24 (8.7 examples/sec; 0.925 sec/batch; 69h:13m:55s remains)
INFO - root - 2017-12-08 00:50:46.346371: step 63020, loss = 21.52, batch loss = 21.44 (8.4 examples/sec; 0.950 sec/batch; 71h:04m:50s remains)
INFO - root - 2017-12-08 00:50:55.730455: step 63030, loss = 21.17, batch loss = 21.09 (7.8 examples/sec; 1.020 sec/batch; 76h:20m:22s remains)
INFO - root - 2017-12-08 00:51:05.085707: step 63040, loss = 21.56, batch loss = 21.48 (8.1 examples/sec; 0.985 sec/batch; 73h:45m:04s remains)
INFO - root - 2017-12-08 00:51:14.358998: step 63050, loss = 21.38, batch loss = 21.30 (8.2 examples/sec; 0.979 sec/batch; 73h:18m:19s remains)
INFO - root - 2017-12-08 00:51:23.821449: step 63060, loss = 21.22, batch loss = 21.14 (8.6 examples/sec; 0.931 sec/batch; 69h:39m:48s remains)
INFO - root - 2017-12-08 00:51:33.306330: step 63070, loss = 21.25, batch loss = 21.17 (8.0 examples/sec; 0.995 sec/batch; 74h:27m:06s remains)
INFO - root - 2017-12-08 00:51:42.548746: step 63080, loss = 21.57, batch loss = 21.48 (8.8 examples/sec; 0.906 sec/batch; 67h:49m:27s remains)
INFO - root - 2017-12-08 00:51:52.002989: step 63090, loss = 21.05, batch loss = 20.96 (8.8 examples/sec; 0.912 sec/batch; 68h:14m:47s remains)
INFO - root - 2017-12-08 00:52:01.453498: step 63100, loss = 21.40, batch loss = 21.32 (8.5 examples/sec; 0.940 sec/batch; 70h:21m:09s remains)
2017-12-08 00:52:02.509420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4500461 -4.4810576 -4.47977 -4.4407773 -4.4706697 -4.5311966 -4.5055847 -4.4347219 -4.4400272 -4.5361891 -4.6019483 -4.6204658 -4.6266785 -4.6296029 -4.5825462][-4.2601914 -4.2895665 -4.3239784 -4.3274221 -4.3767576 -4.4404354 -4.4105277 -4.3308864 -4.3402853 -4.4522066 -4.5215492 -4.5356574 -4.5405841 -4.5615997 -4.5449262][-4.130969 -4.1753669 -4.2489061 -4.2938862 -4.3457675 -4.3819737 -4.3294654 -4.2392116 -4.2538 -4.3816609 -4.4637747 -4.4843855 -4.4905176 -4.5224681 -4.5259533][-4.1509609 -4.2162189 -4.3068423 -4.3658838 -4.38802 -4.357223 -4.2584405 -4.1593404 -4.1845756 -4.3305712 -4.4366331 -4.4779773 -4.4963017 -4.5343337 -4.5439119][-4.2992191 -4.3901954 -4.465951 -4.4859152 -4.4292459 -4.2925739 -4.137289 -4.04819 -4.1072478 -4.2778683 -4.410841 -4.4778233 -4.5170984 -4.5620413 -4.5692544][-4.4130492 -4.5227656 -4.5718746 -4.5173821 -4.3468566 -4.089982 -3.8808403 -3.8295841 -3.9645076 -4.1917605 -4.365056 -4.4637537 -4.5280719 -4.5769606 -4.5752573][-4.3900232 -4.4898257 -4.5165448 -4.3957429 -4.1270051 -3.7787831 -3.5279844 -3.5244548 -3.7659981 -4.082305 -4.3104286 -4.4415922 -4.5275483 -4.5743709 -4.5573444][-4.3032637 -4.3666248 -4.3780389 -4.2411003 -3.9499395 -3.5810387 -3.3118005 -3.3357887 -3.6563091 -4.0410528 -4.3069596 -4.4511671 -4.5415325 -4.5741682 -4.533226][-4.2891269 -4.325449 -4.3466592 -4.2512984 -4.0155315 -3.6982961 -3.4435472 -3.452976 -3.7692454 -4.1448903 -4.3940177 -4.5198069 -4.5951571 -4.6031852 -4.5293617][-4.4179463 -4.4478865 -4.487606 -4.4464235 -4.2843065 -4.0371323 -3.8119502 -3.7936363 -4.0512052 -4.3626337 -4.557106 -4.6403108 -4.6791391 -4.6503639 -4.5429573][-4.6005912 -4.6286421 -4.6646361 -4.6444387 -4.5351467 -4.3581033 -4.182199 -4.1534505 -4.3408012 -4.5691962 -4.6994872 -4.7344456 -4.7297297 -4.6696281 -4.5453577][-4.6665492 -4.6922855 -4.7116528 -4.697247 -4.6377254 -4.5390582 -4.4315171 -4.4079032 -4.5272532 -4.6745625 -4.7504673 -4.7514629 -4.7200909 -4.6453485 -4.5225043][-4.5978694 -4.6301966 -4.6455226 -4.6404648 -4.626224 -4.5974479 -4.5516071 -4.5325904 -4.5916576 -4.6711268 -4.7097058 -4.6963997 -4.6588206 -4.5868311 -4.4783053][-4.476922 -4.5160131 -4.5404263 -4.55056 -4.5657535 -4.577024 -4.5671654 -4.5478268 -4.5615549 -4.5935326 -4.6112356 -4.6003141 -4.5703611 -4.5091896 -4.421227][-4.3785839 -4.4110746 -4.4404631 -4.4601421 -4.482101 -4.5001674 -4.4972658 -4.4721951 -4.4638381 -4.4795032 -4.4999232 -4.5049877 -4.4867945 -4.4358983 -4.3671441]]...]
INFO - root - 2017-12-08 00:52:11.852017: step 63110, loss = 21.57, batch loss = 21.49 (8.5 examples/sec; 0.939 sec/batch; 70h:15m:59s remains)
INFO - root - 2017-12-08 00:52:21.196767: step 63120, loss = 21.65, batch loss = 21.56 (8.6 examples/sec; 0.927 sec/batch; 69h:22m:13s remains)
INFO - root - 2017-12-08 00:52:30.528964: step 63130, loss = 21.30, batch loss = 21.21 (8.6 examples/sec; 0.926 sec/batch; 69h:19m:30s remains)
INFO - root - 2017-12-08 00:52:39.951162: step 63140, loss = 21.75, batch loss = 21.67 (9.0 examples/sec; 0.890 sec/batch; 66h:35m:19s remains)
INFO - root - 2017-12-08 00:52:49.308589: step 63150, loss = 21.39, batch loss = 21.31 (8.5 examples/sec; 0.937 sec/batch; 70h:05m:46s remains)
INFO - root - 2017-12-08 00:52:58.856317: step 63160, loss = 21.72, batch loss = 21.63 (8.3 examples/sec; 0.962 sec/batch; 71h:58m:48s remains)
INFO - root - 2017-12-08 00:53:08.192948: step 63170, loss = 21.21, batch loss = 21.13 (9.5 examples/sec; 0.845 sec/batch; 63h:13m:29s remains)
INFO - root - 2017-12-08 00:53:17.574906: step 63180, loss = 21.79, batch loss = 21.71 (8.5 examples/sec; 0.943 sec/batch; 70h:32m:06s remains)
INFO - root - 2017-12-08 00:53:26.808674: step 63190, loss = 21.17, batch loss = 21.09 (8.2 examples/sec; 0.973 sec/batch; 72h:47m:55s remains)
INFO - root - 2017-12-08 00:53:36.286504: step 63200, loss = 21.39, batch loss = 21.31 (8.4 examples/sec; 0.951 sec/batch; 71h:10m:26s remains)
2017-12-08 00:53:37.267749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3881011 -4.392158 -4.4024272 -4.4158206 -4.4264889 -4.4314394 -4.4301825 -4.4198089 -4.3970947 -4.3574309 -4.3090711 -4.2924495 -4.3003035 -4.2968636 -4.2954345][-4.4353609 -4.4348617 -4.4389234 -4.4532008 -4.46971 -4.4816909 -4.4853759 -4.4750786 -4.4466949 -4.390872 -4.320169 -4.2736492 -4.249032 -4.2310953 -4.2339191][-4.4552722 -4.450644 -4.453846 -4.4735117 -4.4916039 -4.4928679 -4.4786129 -4.4499269 -4.4082985 -4.3466763 -4.2797642 -4.2354984 -4.2119441 -4.208456 -4.231586][-4.4195805 -4.410131 -4.4149771 -4.438549 -4.4487262 -4.4219255 -4.3707972 -4.3126631 -4.2568183 -4.208075 -4.1785226 -4.1694293 -4.1774044 -4.2135339 -4.2651796][-4.3422971 -4.3253827 -4.3294468 -4.3518744 -4.3501868 -4.2938776 -4.20772 -4.1239533 -4.0584874 -4.0345449 -4.0566521 -4.0929971 -4.1343708 -4.2011933 -4.2649732][-4.2464986 -4.2222619 -4.2259722 -4.24555 -4.2348933 -4.1598439 -4.0546422 -3.9572659 -3.8874531 -3.8845234 -3.946326 -4.0150728 -4.0723267 -4.1417208 -4.1931105][-4.1461992 -4.1291237 -4.1434412 -4.1647258 -4.1501303 -4.0692978 -3.9626276 -3.8650885 -3.8014007 -3.8128867 -3.8876243 -3.9637649 -4.0195389 -4.0740256 -4.1051412][-4.0834422 -4.0956049 -4.1322765 -4.1583543 -4.1439781 -4.0712652 -3.9829872 -3.9028103 -3.8520331 -3.863622 -3.9212763 -3.9829447 -4.0300245 -4.0705295 -4.0877376][-4.1065578 -4.1510272 -4.2061863 -4.2326217 -4.2174768 -4.1617851 -4.105001 -4.0551386 -4.0246663 -4.0315881 -4.0643368 -4.1086249 -4.1480103 -4.1779957 -4.1835842][-4.1969481 -4.25738 -4.3156886 -4.3334517 -4.3152704 -4.2789207 -4.2546067 -4.2389321 -4.2353082 -4.2482729 -4.2748346 -4.3167505 -4.3536181 -4.3727503 -4.360323][-4.276 -4.3369761 -4.3853145 -4.3887143 -4.3677359 -4.3499842 -4.3529024 -4.368525 -4.3913088 -4.4195266 -4.4556065 -4.5016904 -4.5323591 -4.53462 -4.4981227][-4.3030376 -4.3575611 -4.3901935 -4.3772554 -4.3535571 -4.3533783 -4.3793941 -4.4184384 -4.4578943 -4.4927664 -4.5300875 -4.5669179 -4.5811849 -4.5691605 -4.5218906][-4.2946844 -4.3407431 -4.3572664 -4.3302746 -4.306941 -4.3202548 -4.3627443 -4.4161792 -4.4585958 -4.4844928 -4.5089626 -4.5264282 -4.5236549 -4.5048442 -4.461957][-4.2948132 -4.33252 -4.3334675 -4.2948704 -4.2721415 -4.293766 -4.3454266 -4.4048662 -4.4430103 -4.4564953 -4.4635382 -4.4578753 -4.4359465 -4.4087148 -4.3679929][-4.313107 -4.3443708 -4.3340635 -4.2869439 -4.263134 -4.28887 -4.3450751 -4.4050112 -4.4362707 -4.4380612 -4.4240146 -4.3879905 -4.3382621 -4.2929759 -4.2535143]]...]
INFO - root - 2017-12-08 00:53:46.568012: step 63210, loss = 21.48, batch loss = 21.39 (9.5 examples/sec; 0.843 sec/batch; 63h:03m:06s remains)
INFO - root - 2017-12-08 00:53:55.925566: step 63220, loss = 21.16, batch loss = 21.07 (8.2 examples/sec; 0.972 sec/batch; 72h:43m:46s remains)
INFO - root - 2017-12-08 00:54:05.305015: step 63230, loss = 20.84, batch loss = 20.76 (8.5 examples/sec; 0.942 sec/batch; 70h:25m:26s remains)
INFO - root - 2017-12-08 00:54:14.633546: step 63240, loss = 21.51, batch loss = 21.43 (8.4 examples/sec; 0.952 sec/batch; 71h:11m:27s remains)
INFO - root - 2017-12-08 00:54:24.069334: step 63250, loss = 21.25, batch loss = 21.17 (8.3 examples/sec; 0.963 sec/batch; 72h:01m:25s remains)
INFO - root - 2017-12-08 00:54:33.527212: step 63260, loss = 21.39, batch loss = 21.31 (8.7 examples/sec; 0.922 sec/batch; 68h:56m:00s remains)
INFO - root - 2017-12-08 00:54:42.917616: step 63270, loss = 21.61, batch loss = 21.52 (8.7 examples/sec; 0.915 sec/batch; 68h:24m:01s remains)
INFO - root - 2017-12-08 00:54:52.281097: step 63280, loss = 21.79, batch loss = 21.71 (8.2 examples/sec; 0.974 sec/batch; 72h:48m:46s remains)
INFO - root - 2017-12-08 00:55:01.565940: step 63290, loss = 21.23, batch loss = 21.15 (8.3 examples/sec; 0.968 sec/batch; 72h:22m:16s remains)
INFO - root - 2017-12-08 00:55:10.849913: step 63300, loss = 21.20, batch loss = 21.11 (8.5 examples/sec; 0.942 sec/batch; 70h:25m:29s remains)
2017-12-08 00:55:11.794426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2266693 -4.1970897 -4.2388239 -4.3196726 -4.4035692 -4.4379778 -4.4323626 -4.4289227 -4.4178729 -4.4024405 -4.4318323 -4.5309062 -4.6519685 -4.705781 -4.6859012][-4.264245 -4.2441211 -4.2959466 -4.3735476 -4.4325061 -4.4307885 -4.3940988 -4.3767023 -4.3648281 -4.3543015 -4.3896937 -4.4930673 -4.6172214 -4.6766233 -4.6718807][-4.3291674 -4.32132 -4.3728275 -4.4318371 -4.4493456 -4.4049954 -4.3428497 -4.3202171 -4.3186278 -4.3273754 -4.3755183 -4.4716477 -4.5765166 -4.6316781 -4.6426349][-4.4249043 -4.424499 -4.457386 -4.4763875 -4.4357224 -4.3367739 -4.2425952 -4.2123485 -4.2299604 -4.2691221 -4.3327017 -4.418859 -4.5087218 -4.5719824 -4.6104937][-4.4981604 -4.4959311 -4.4932442 -4.4602623 -4.3606839 -4.2069931 -4.0715175 -4.0298867 -4.0815005 -4.1613846 -4.2384338 -4.3150287 -4.3996925 -4.487247 -4.5691962][-4.5208268 -4.5097795 -4.4656067 -4.3821 -4.2401309 -4.0457458 -3.8673024 -3.8124807 -3.9057522 -4.0392127 -4.136148 -4.2088752 -4.2945943 -4.4058046 -4.5227122][-4.5035286 -4.5027566 -4.4322453 -4.3061404 -4.1339717 -3.9152441 -3.70934 -3.6465974 -3.7724135 -3.9536085 -4.08241 -4.1709027 -4.2673726 -4.3884449 -4.504344][-4.4946437 -4.5330467 -4.4667768 -4.3113184 -4.1156297 -3.8914976 -3.6850617 -3.6240325 -3.7606645 -3.9626327 -4.1206636 -4.2380719 -4.3511033 -4.4629374 -4.536582][-4.4915118 -4.5841961 -4.5555158 -4.40486 -4.2112808 -4.0104218 -3.8376637 -3.7886052 -3.9007487 -4.072619 -4.2282786 -4.3605137 -4.4771304 -4.5601616 -4.5682745][-4.4646788 -4.5973263 -4.6252384 -4.5240827 -4.3717241 -4.2134809 -4.0759182 -4.018353 -4.0603781 -4.1543455 -4.2726145 -4.4042506 -4.5247064 -4.5921626 -4.5534139][-4.4281688 -4.5724425 -4.6501317 -4.6194038 -4.5233788 -4.402492 -4.27648 -4.1786432 -4.1206555 -4.1112595 -4.1731629 -4.2949448 -4.4293313 -4.5145812 -4.4850988][-4.38166 -4.5118117 -4.623559 -4.6603622 -4.6198182 -4.5270624 -4.3988223 -4.2519107 -4.1078725 -4.0149012 -4.0255184 -4.1287794 -4.2705889 -4.3846869 -4.4029789][-4.2796364 -4.3826451 -4.5072336 -4.5902863 -4.6032891 -4.551136 -4.4413075 -4.2853174 -4.1133347 -3.9904935 -3.9737847 -4.0506248 -4.1741338 -4.2976604 -4.3624277][-4.1473594 -4.2231393 -4.3402729 -4.4405642 -4.4907913 -4.4839888 -4.4202881 -4.3119273 -4.1789861 -4.0803361 -4.065733 -4.1219692 -4.2184744 -4.3320594 -4.4178524][-4.056674 -4.1127 -4.2108006 -4.3067732 -4.3806143 -4.4184966 -4.4188313 -4.3844748 -4.3149719 -4.2546487 -4.2479196 -4.291688 -4.3644934 -4.4526491 -4.5242591]]...]
INFO - root - 2017-12-08 00:55:21.292366: step 63310, loss = 21.27, batch loss = 21.19 (7.9 examples/sec; 1.009 sec/batch; 75h:27m:54s remains)
INFO - root - 2017-12-08 00:55:30.740018: step 63320, loss = 21.08, batch loss = 20.99 (8.3 examples/sec; 0.960 sec/batch; 71h:49m:06s remains)
INFO - root - 2017-12-08 00:55:40.177844: step 63330, loss = 21.44, batch loss = 21.36 (9.2 examples/sec; 0.868 sec/batch; 64h:52m:50s remains)
INFO - root - 2017-12-08 00:55:49.471858: step 63340, loss = 21.83, batch loss = 21.74 (8.8 examples/sec; 0.907 sec/batch; 67h:49m:33s remains)
INFO - root - 2017-12-08 00:55:58.792042: step 63350, loss = 21.46, batch loss = 21.37 (8.9 examples/sec; 0.902 sec/batch; 67h:26m:57s remains)
INFO - root - 2017-12-08 00:56:08.392951: step 63360, loss = 21.40, batch loss = 21.32 (8.2 examples/sec; 0.973 sec/batch; 72h:46m:15s remains)
INFO - root - 2017-12-08 00:56:17.673308: step 63370, loss = 21.08, batch loss = 21.00 (7.9 examples/sec; 1.013 sec/batch; 75h:42m:01s remains)
INFO - root - 2017-12-08 00:56:26.920745: step 63380, loss = 21.38, batch loss = 21.29 (8.4 examples/sec; 0.957 sec/batch; 71h:30m:51s remains)
INFO - root - 2017-12-08 00:56:36.063425: step 63390, loss = 21.50, batch loss = 21.41 (9.3 examples/sec; 0.864 sec/batch; 64h:33m:38s remains)
INFO - root - 2017-12-08 00:56:45.560266: step 63400, loss = 21.80, batch loss = 21.72 (8.2 examples/sec; 0.980 sec/batch; 73h:13m:10s remains)
2017-12-08 00:56:46.469736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5703721 -4.5726094 -4.59298 -4.6303239 -4.6517224 -4.6407647 -4.6031537 -4.5658417 -4.5611167 -4.6017551 -4.6662993 -4.7023497 -4.6745968 -4.6023092 -4.544342][-4.5833535 -4.603281 -4.6441808 -4.7046161 -4.7504921 -4.7643991 -4.7514095 -4.731245 -4.7269654 -4.7566614 -4.8008604 -4.8146696 -4.7657323 -4.6720076 -4.586226][-4.6080813 -4.6538906 -4.7135167 -4.7753363 -4.807734 -4.8057866 -4.7842078 -4.763742 -4.7648582 -4.8013287 -4.8508773 -4.8676014 -4.8182397 -4.7157221 -4.607687][-4.66932 -4.733583 -4.7911205 -4.8189006 -4.7938156 -4.7256026 -4.6460395 -4.5921445 -4.6024785 -4.6860271 -4.7894382 -4.8461103 -4.8216257 -4.7263346 -4.6074367][-4.74654 -4.7995391 -4.8158832 -4.7730012 -4.6629624 -4.4976797 -4.3313446 -4.2370367 -4.2883778 -4.4786959 -4.6832261 -4.7998457 -4.8071332 -4.7210965 -4.5950871][-4.7933064 -4.8122149 -4.7628212 -4.6437082 -4.46033 -4.2033892 -3.9492867 -3.825932 -3.9473288 -4.2750397 -4.5935721 -4.7630253 -4.7942429 -4.7175689 -4.5882778][-4.7780347 -4.7542906 -4.636332 -4.4583097 -4.2318735 -3.9092398 -3.5829587 -3.4393866 -3.6227472 -4.0625567 -4.4700928 -4.6835203 -4.7432466 -4.6916122 -4.5798264][-4.7125864 -4.6563859 -4.4867625 -4.2748661 -4.0331154 -3.6849742 -3.3294034 -3.186975 -3.4100263 -3.9040284 -4.3539162 -4.5973635 -4.6845422 -4.6559896 -4.5681844][-4.6438351 -4.5808587 -4.3952565 -4.183877 -3.9646754 -3.6553123 -3.3445868 -3.2381134 -3.4640942 -3.923553 -4.3363123 -4.5644689 -4.6481652 -4.6223845 -4.5507631][-4.592092 -4.5527959 -4.394402 -4.2159653 -4.0458035 -3.8128488 -3.5835023 -3.519366 -3.7104163 -4.0639267 -4.3719149 -4.5425234 -4.5969505 -4.5682516 -4.5161576][-4.547399 -4.5488567 -4.4521918 -4.33102 -4.2178025 -4.064239 -3.9106169 -3.8746748 -4.012248 -4.2422085 -4.4291658 -4.5237885 -4.5394049 -4.5113878 -4.4781804][-4.5152516 -4.5526037 -4.5234423 -4.4702449 -4.4168887 -4.3308754 -4.2296987 -4.198988 -4.2734017 -4.3858838 -4.4662147 -4.49624 -4.4901185 -4.4773765 -4.46407][-4.5010977 -4.5458183 -4.5582719 -4.5576291 -4.55074 -4.5102153 -4.4375582 -4.394897 -4.405602 -4.4238176 -4.4284697 -4.4283967 -4.4307051 -4.4475503 -4.4570074][-4.4967775 -4.5268245 -4.5448136 -4.5645494 -4.5820017 -4.5728669 -4.5247617 -4.4768057 -4.4470453 -4.408061 -4.3720322 -4.3615623 -4.3763881 -4.4109831 -4.4311228][-4.4906631 -4.4979768 -4.5029826 -4.524334 -4.5545487 -4.5695634 -4.5460677 -4.5050855 -4.462152 -4.4015679 -4.3498759 -4.3369 -4.3537383 -4.3837147 -4.3964086]]...]
INFO - root - 2017-12-08 00:56:55.773667: step 63410, loss = 21.08, batch loss = 20.99 (8.7 examples/sec; 0.922 sec/batch; 68h:54m:07s remains)
INFO - root - 2017-12-08 00:57:05.221840: step 63420, loss = 21.39, batch loss = 21.30 (8.6 examples/sec; 0.936 sec/batch; 69h:55m:40s remains)
INFO - root - 2017-12-08 00:57:14.684986: step 63430, loss = 21.69, batch loss = 21.61 (8.5 examples/sec; 0.941 sec/batch; 70h:18m:01s remains)
INFO - root - 2017-12-08 00:57:24.033386: step 63440, loss = 21.44, batch loss = 21.35 (8.4 examples/sec; 0.948 sec/batch; 70h:52m:26s remains)
INFO - root - 2017-12-08 00:57:33.364694: step 63450, loss = 21.52, batch loss = 21.44 (8.6 examples/sec; 0.929 sec/batch; 69h:24m:13s remains)
INFO - root - 2017-12-08 00:57:42.796347: step 63460, loss = 21.44, batch loss = 21.36 (8.8 examples/sec; 0.914 sec/batch; 68h:16m:57s remains)
INFO - root - 2017-12-08 00:57:52.103288: step 63470, loss = 21.33, batch loss = 21.25 (9.3 examples/sec; 0.859 sec/batch; 64h:09m:46s remains)
INFO - root - 2017-12-08 00:58:01.491450: step 63480, loss = 21.42, batch loss = 21.34 (8.6 examples/sec; 0.934 sec/batch; 69h:46m:49s remains)
INFO - root - 2017-12-08 00:58:10.765689: step 63490, loss = 21.42, batch loss = 21.34 (9.1 examples/sec; 0.875 sec/batch; 65h:21m:52s remains)
INFO - root - 2017-12-08 00:58:20.095167: step 63500, loss = 21.99, batch loss = 21.90 (8.2 examples/sec; 0.973 sec/batch; 72h:42m:46s remains)
2017-12-08 00:58:21.217381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.349823 -4.3165717 -4.3083529 -4.3144479 -4.3261642 -4.3417678 -4.3962879 -4.4756622 -4.5351133 -4.5245976 -4.4619074 -4.407002 -4.3772707 -4.3639956 -4.3327723][-4.387464 -4.3383393 -4.3210826 -4.3180256 -4.3239031 -4.3308687 -4.3619852 -4.4203072 -4.4788408 -4.4707675 -4.4032869 -4.3338962 -4.286931 -4.2641416 -4.2208061][-4.377037 -4.3192511 -4.3022957 -4.305151 -4.3119621 -4.3054466 -4.2969165 -4.3169365 -4.3692665 -4.3842559 -4.3421149 -4.2737422 -4.2085404 -4.1639614 -4.1008949][-4.3169193 -4.2622457 -4.2549214 -4.2724423 -4.2864656 -4.2644496 -4.2110195 -4.1897616 -4.2353482 -4.2896461 -4.2944098 -4.2390175 -4.160418 -4.096417 -4.0271974][-4.2694645 -4.2296495 -4.2292819 -4.2519951 -4.2598748 -4.2099929 -4.112803 -4.0631814 -4.1144171 -4.2167854 -4.2743359 -4.247076 -4.1761389 -4.1092029 -4.0459785][-4.2822552 -4.2752595 -4.2823687 -4.2905731 -4.2634325 -4.1671939 -4.0349288 -3.9790213 -4.0466743 -4.1834908 -4.2774343 -4.2829633 -4.2365084 -4.1785016 -4.1267734][-4.3172169 -4.3456631 -4.363337 -4.3473744 -4.2723255 -4.1350665 -3.9951091 -3.9583962 -4.0389223 -4.1771636 -4.2775459 -4.307477 -4.2893667 -4.2477946 -4.2126737][-4.3375454 -4.3844652 -4.3973894 -4.3567848 -4.2474613 -4.1010838 -3.9900091 -3.9948986 -4.0811572 -4.1918683 -4.2714992 -4.3080878 -4.3085542 -4.2819462 -4.26019][-4.33163 -4.3686342 -4.3634582 -4.3078451 -4.1903214 -4.0621963 -3.9974077 -4.0502539 -4.1421227 -4.2197285 -4.2686925 -4.2925563 -4.2965393 -4.281507 -4.2707][-4.2804427 -4.2927847 -4.2713933 -4.2130342 -4.103375 -3.9940023 -3.9608409 -4.055697 -4.1651549 -4.230093 -4.2583842 -4.2663007 -4.268436 -4.2646594 -4.2678065][-4.2048683 -4.19775 -4.1811457 -4.1299772 -4.0290236 -3.931109 -3.9112613 -4.0325994 -4.1656561 -4.2317405 -4.2480717 -4.2432265 -4.2437592 -4.2534266 -4.2738109][-4.1458135 -4.1278019 -4.1363215 -4.1049666 -4.0168443 -3.9273539 -3.9075334 -4.0326414 -4.18388 -4.2565155 -4.264112 -4.2455096 -4.2436376 -4.2647076 -4.3006215][-4.1364036 -4.1143236 -4.1394854 -4.1194644 -4.03327 -3.9521701 -3.9404275 -4.0657954 -4.2324281 -4.3126082 -4.3109474 -4.2736511 -4.2629766 -4.2892189 -4.3363523][-4.1718135 -4.1455832 -4.1614871 -4.1363664 -4.0513124 -3.9887781 -4.0050163 -4.1426592 -4.319397 -4.4013472 -4.3847384 -4.3233981 -4.2947445 -4.3166866 -4.3668461][-4.2090883 -4.1697416 -4.1628585 -4.1315417 -4.0658021 -4.0412755 -4.0988359 -4.2509069 -4.4180474 -4.4833469 -4.4481182 -4.3648825 -4.317306 -4.3294487 -4.376883]]...]
INFO - root - 2017-12-08 00:58:30.629845: step 63510, loss = 21.56, batch loss = 21.47 (8.8 examples/sec; 0.912 sec/batch; 68h:09m:51s remains)
INFO - root - 2017-12-08 00:58:39.906807: step 63520, loss = 21.52, batch loss = 21.44 (9.1 examples/sec; 0.880 sec/batch; 65h:43m:06s remains)
INFO - root - 2017-12-08 00:58:49.368487: step 63530, loss = 21.50, batch loss = 21.42 (8.7 examples/sec; 0.922 sec/batch; 68h:55m:10s remains)
INFO - root - 2017-12-08 00:58:58.763898: step 63540, loss = 21.22, batch loss = 21.14 (8.5 examples/sec; 0.937 sec/batch; 70h:01m:32s remains)
INFO - root - 2017-12-08 00:59:08.222183: step 63550, loss = 22.07, batch loss = 21.99 (8.7 examples/sec; 0.921 sec/batch; 68h:46m:39s remains)
INFO - root - 2017-12-08 00:59:17.597448: step 63560, loss = 21.29, batch loss = 21.21 (8.9 examples/sec; 0.902 sec/batch; 67h:25m:00s remains)
INFO - root - 2017-12-08 00:59:26.886867: step 63570, loss = 21.52, batch loss = 21.43 (8.5 examples/sec; 0.941 sec/batch; 70h:18m:41s remains)
INFO - root - 2017-12-08 00:59:36.287954: step 63580, loss = 21.71, batch loss = 21.63 (8.1 examples/sec; 0.986 sec/batch; 73h:38m:51s remains)
INFO - root - 2017-12-08 00:59:45.602323: step 63590, loss = 21.06, batch loss = 20.98 (8.8 examples/sec; 0.912 sec/batch; 68h:07m:35s remains)
INFO - root - 2017-12-08 00:59:54.883709: step 63600, loss = 21.17, batch loss = 21.09 (9.4 examples/sec; 0.851 sec/batch; 63h:32m:43s remains)
2017-12-08 00:59:55.945254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4295034 -4.4982252 -4.5324788 -4.4819207 -4.3545957 -4.2064567 -4.1136975 -4.097014 -4.1315436 -4.1831112 -4.2216973 -4.246933 -4.2791557 -4.341207 -4.4278584][-4.4188147 -4.5023794 -4.5359616 -4.4700665 -4.3262711 -4.1738038 -4.0885587 -4.0978689 -4.1668205 -4.2350192 -4.2619476 -4.2587681 -4.2666969 -4.3176479 -4.40259][-4.4163346 -4.4945517 -4.5126853 -4.428772 -4.2769461 -4.1310511 -4.0606718 -4.0995045 -4.2026076 -4.2862325 -4.3004069 -4.2683244 -4.2502966 -4.2871623 -4.3679852][-4.4043069 -4.4802375 -4.4908872 -4.4024453 -4.2547855 -4.1188583 -4.0566006 -4.1113362 -4.2304311 -4.3157248 -4.3141227 -4.258574 -4.2268391 -4.2582326 -4.3367791][-4.3759551 -4.4620175 -4.4811096 -4.3990607 -4.252419 -4.1146345 -4.0455441 -4.09676 -4.2099133 -4.2869358 -4.2789893 -4.2189469 -4.1957374 -4.2401795 -4.3244147][-4.3246088 -4.4321413 -4.473804 -4.4014049 -4.2454028 -4.0860915 -3.9934025 -4.0313616 -4.135488 -4.2142215 -4.2201447 -4.1762652 -4.1754823 -4.2402816 -4.3294287][-4.2597961 -4.3876648 -4.4524555 -4.3865056 -4.2139039 -4.0178452 -3.8922417 -3.9204712 -4.0336823 -4.1383133 -4.1808023 -4.1679277 -4.1881289 -4.2624426 -4.3477535][-4.2199225 -4.3649058 -4.4421453 -4.3763523 -4.1891837 -3.9597919 -3.8060598 -3.8266296 -3.956285 -4.0976229 -4.1864095 -4.2099409 -4.2425075 -4.3073454 -4.3731213][-4.2386918 -4.3976784 -4.4694996 -4.3985062 -4.2139082 -3.9809651 -3.8201289 -3.8249152 -3.9543715 -4.1184473 -4.2438207 -4.2990379 -4.3374376 -4.3789086 -4.4080153][-4.3037438 -4.4593067 -4.5126495 -4.4367909 -4.2713642 -4.0650353 -3.9190388 -3.9072659 -4.021513 -4.1861868 -4.3275337 -4.4008718 -4.4412775 -4.4600487 -4.45037][-4.3654628 -4.5034866 -4.5432048 -4.4756947 -4.3401146 -4.1713305 -4.0463495 -4.0199876 -4.1092205 -4.248939 -4.376297 -4.445313 -4.4819436 -4.4901705 -4.4583211][-4.3865371 -4.4975724 -4.5327082 -4.48636 -4.3889117 -4.26283 -4.1566892 -4.114511 -4.168664 -4.2599282 -4.3466115 -4.3976574 -4.4329767 -4.4479818 -4.412889][-4.3459558 -4.414897 -4.4409986 -4.4183059 -4.3675408 -4.2940083 -4.2118988 -4.1625671 -4.1826873 -4.2188945 -4.2575717 -4.2917118 -4.3335695 -4.3639164 -4.3351989][-4.2631249 -4.2890978 -4.3026781 -4.2972555 -4.2894239 -4.2684555 -4.2180362 -4.1792889 -4.1783733 -4.1694856 -4.1647806 -4.1820021 -4.2299433 -4.2686834 -4.2422509][-4.2004018 -4.2076659 -4.2096267 -4.2054486 -4.2190213 -4.2319655 -4.2109838 -4.193625 -4.1906319 -4.1617494 -4.1315608 -4.1313262 -4.1713161 -4.197175 -4.16242]]...]
INFO - root - 2017-12-08 01:00:05.321933: step 63610, loss = 21.39, batch loss = 21.30 (8.2 examples/sec; 0.980 sec/batch; 73h:10m:12s remains)
INFO - root - 2017-12-08 01:00:14.699876: step 63620, loss = 21.52, batch loss = 21.44 (8.2 examples/sec; 0.976 sec/batch; 72h:52m:13s remains)
INFO - root - 2017-12-08 01:00:23.987365: step 63630, loss = 21.32, batch loss = 21.23 (8.4 examples/sec; 0.950 sec/batch; 70h:55m:06s remains)
INFO - root - 2017-12-08 01:00:33.326120: step 63640, loss = 21.69, batch loss = 21.60 (8.1 examples/sec; 0.984 sec/batch; 73h:29m:44s remains)
INFO - root - 2017-12-08 01:00:42.726544: step 63650, loss = 21.44, batch loss = 21.36 (8.6 examples/sec; 0.929 sec/batch; 69h:24m:15s remains)
INFO - root - 2017-12-08 01:00:52.041353: step 63660, loss = 21.25, batch loss = 21.16 (9.4 examples/sec; 0.848 sec/batch; 63h:18m:58s remains)
INFO - root - 2017-12-08 01:01:01.362823: step 63670, loss = 21.53, batch loss = 21.45 (9.5 examples/sec; 0.841 sec/batch; 62h:46m:17s remains)
INFO - root - 2017-12-08 01:01:10.821810: step 63680, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.928 sec/batch; 69h:18m:11s remains)
INFO - root - 2017-12-08 01:01:20.029903: step 63690, loss = 21.53, batch loss = 21.45 (8.2 examples/sec; 0.973 sec/batch; 72h:40m:44s remains)
INFO - root - 2017-12-08 01:01:29.430093: step 63700, loss = 21.50, batch loss = 21.42 (8.2 examples/sec; 0.978 sec/batch; 73h:02m:19s remains)
2017-12-08 01:01:30.319441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5164671 -4.4773655 -4.4269609 -4.3673158 -4.2950745 -4.2240047 -4.1578484 -4.1512189 -4.2051649 -4.2748661 -4.3525949 -4.4072561 -4.4053588 -4.3727517 -4.3159509][-4.5758023 -4.5710416 -4.534688 -4.45621 -4.35377 -4.258194 -4.1781111 -4.1670156 -4.220293 -4.2745953 -4.3141732 -4.3232579 -4.2860708 -4.2345281 -4.1684341][-4.6024761 -4.6447477 -4.6333747 -4.544435 -4.4134178 -4.2896204 -4.2034278 -4.1986852 -4.2469859 -4.2725763 -4.2659707 -4.2371144 -4.1953015 -4.1740294 -4.1465454][-4.5837502 -4.657783 -4.665926 -4.5791311 -4.431797 -4.2817497 -4.1899052 -4.2016745 -4.2580972 -4.2740827 -4.25387 -4.2215867 -4.1982603 -4.2225914 -4.2478566][-4.5423875 -4.625309 -4.6400266 -4.5604486 -4.4067912 -4.2358942 -4.1333766 -4.1584177 -4.2346244 -4.2653089 -4.2602439 -4.2462044 -4.2453737 -4.2952209 -4.3499556][-4.5128927 -4.592442 -4.6086464 -4.5331597 -4.3692503 -4.1701093 -4.0361505 -4.046947 -4.1393375 -4.2053857 -4.2367077 -4.2565966 -4.2825747 -4.3399372 -4.3936133][-4.4924374 -4.5610452 -4.5705657 -4.4840479 -4.2948108 -4.0544481 -3.8741791 -3.8615923 -3.985003 -4.1173615 -4.2108483 -4.2752786 -4.3225236 -4.3703842 -4.3951211][-4.4706535 -4.5242682 -4.51971 -4.4151726 -4.2046971 -3.9428949 -3.7444458 -3.7328475 -3.9033434 -4.1057072 -4.2486053 -4.3309345 -4.3740182 -4.4034438 -4.4013109][-4.4662704 -4.5178747 -4.5062461 -4.3933578 -4.1857634 -3.9457958 -3.7751074 -3.7857306 -3.9826617 -4.2041092 -4.3414831 -4.4010711 -4.4241047 -4.4446788 -4.4377813][-4.4515624 -4.5067325 -4.4980278 -4.4036956 -4.2351971 -4.0544457 -3.9410939 -3.9739776 -4.1543117 -4.3412838 -4.4356236 -4.4541893 -4.4504852 -4.4565806 -4.4446907][-4.4002538 -4.4565415 -4.4694371 -4.4196086 -4.3096285 -4.1935091 -4.1367245 -4.1855383 -4.3290768 -4.4602032 -4.5016193 -4.4798341 -4.446939 -4.426312 -4.3936405][-4.3426056 -4.3960352 -4.4284563 -4.4155126 -4.3530579 -4.2862191 -4.2704048 -4.3261757 -4.4303279 -4.5117893 -4.5171051 -4.4750209 -4.4311342 -4.3996906 -4.3572779][-4.3412933 -4.3782287 -4.4044337 -4.4014497 -4.3699627 -4.3399205 -4.3456936 -4.3918495 -4.4541278 -4.4953761 -4.4813089 -4.4358144 -4.3965516 -4.372437 -4.3389726][-4.38365 -4.4004769 -4.401618 -4.39085 -4.37721 -4.3721209 -4.3818684 -4.4002862 -4.420516 -4.4361 -4.4245772 -4.3949704 -4.3699007 -4.3546767 -4.3297849][-4.3876314 -4.3836756 -4.3617778 -4.345789 -4.3503518 -4.3671379 -4.3739195 -4.3582311 -4.341104 -4.3456059 -4.35641 -4.3574514 -4.3507652 -4.3393412 -4.3116908]]...]
INFO - root - 2017-12-08 01:01:39.694648: step 63710, loss = 21.63, batch loss = 21.55 (8.6 examples/sec; 0.925 sec/batch; 69h:04m:09s remains)
INFO - root - 2017-12-08 01:01:49.137712: step 63720, loss = 21.85, batch loss = 21.76 (7.9 examples/sec; 1.014 sec/batch; 75h:44m:07s remains)
INFO - root - 2017-12-08 01:01:58.504430: step 63730, loss = 21.47, batch loss = 21.39 (8.0 examples/sec; 1.001 sec/batch; 74h:44m:30s remains)
INFO - root - 2017-12-08 01:02:07.929349: step 63740, loss = 21.29, batch loss = 21.21 (9.0 examples/sec; 0.887 sec/batch; 66h:12m:34s remains)
INFO - root - 2017-12-08 01:02:17.265797: step 63750, loss = 21.78, batch loss = 21.69 (8.8 examples/sec; 0.910 sec/batch; 67h:57m:22s remains)
INFO - root - 2017-12-08 01:02:26.567565: step 63760, loss = 21.48, batch loss = 21.39 (8.2 examples/sec; 0.972 sec/batch; 72h:31m:24s remains)
INFO - root - 2017-12-08 01:02:35.840261: step 63770, loss = 21.74, batch loss = 21.65 (8.5 examples/sec; 0.937 sec/batch; 69h:54m:38s remains)
INFO - root - 2017-12-08 01:02:45.263286: step 63780, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.955 sec/batch; 71h:17m:42s remains)
INFO - root - 2017-12-08 01:02:54.662625: step 63790, loss = 21.65, batch loss = 21.57 (8.3 examples/sec; 0.969 sec/batch; 72h:21m:08s remains)
INFO - root - 2017-12-08 01:03:03.958502: step 63800, loss = 21.62, batch loss = 21.53 (8.3 examples/sec; 0.962 sec/batch; 71h:46m:48s remains)
2017-12-08 01:03:05.019867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3652387 -4.24987 -4.1499877 -4.136271 -4.1854563 -4.2246151 -4.2494459 -4.274076 -4.2867365 -4.2964392 -4.313489 -4.3336277 -4.35277 -4.3751674 -4.3990717][-4.3586049 -4.2912216 -4.240932 -4.2492051 -4.28503 -4.3025985 -4.3226333 -4.352808 -4.368968 -4.38318 -4.4041109 -4.4268312 -4.4429541 -4.4616804 -4.485353][-4.3747988 -4.3529673 -4.3435435 -4.3635826 -4.3767428 -4.3661785 -4.3802185 -4.4196305 -4.4496179 -4.4793248 -4.5127935 -4.5379906 -4.5438266 -4.5544133 -4.5760169][-4.3893628 -4.3945413 -4.3967896 -4.3994932 -4.3756385 -4.3271928 -4.3235159 -4.3712845 -4.428946 -4.4933209 -4.5582719 -4.592196 -4.583324 -4.5845466 -4.6068397][-4.3955879 -4.3946295 -4.3630381 -4.3141837 -4.2464886 -4.1579261 -4.1256146 -4.1789103 -4.2773361 -4.3977737 -4.5178556 -4.5730996 -4.5446277 -4.5282969 -4.5437551][-4.4113622 -4.3767419 -4.2706823 -4.141674 -4.0239716 -3.8937535 -3.819273 -3.8734815 -4.0252075 -4.21625 -4.41028 -4.5020332 -4.4521432 -4.40773 -4.4033055][-4.4632554 -4.4045157 -4.2319832 -4.0270257 -3.8639352 -3.6911492 -3.5531664 -3.5834479 -3.7736974 -4.0225906 -4.276402 -4.4124794 -4.3673429 -4.3106861 -4.2946506][-4.5318151 -4.4826889 -4.2985387 -4.0714231 -3.8990042 -3.7105079 -3.5173769 -3.4942808 -3.6779275 -3.9346759 -4.1967678 -4.3567104 -4.3397408 -4.289927 -4.272913][-4.5667224 -4.5537038 -4.4154854 -4.2238727 -4.0758348 -3.9064503 -3.7056007 -3.6411936 -3.7777882 -3.9885466 -4.2007942 -4.3503571 -4.3693595 -4.337739 -4.32167][-4.5544848 -4.5781927 -4.5163641 -4.3995709 -4.2967749 -4.1687865 -4.0011253 -3.9216723 -3.9980974 -4.1369123 -4.2787113 -4.4015045 -4.451149 -4.4421272 -4.4285169][-4.5008283 -4.539845 -4.5390177 -4.4996986 -4.4552493 -4.3905635 -4.292233 -4.2355161 -4.271884 -4.3489208 -4.4293308 -4.5164456 -4.5685759 -4.5688705 -4.5533142][-4.4191351 -4.4511962 -4.474987 -4.4836068 -4.4880743 -4.4872417 -4.4718118 -4.4700131 -4.501482 -4.5426326 -4.5783467 -4.6264296 -4.6627083 -4.6612062 -4.63622][-4.323462 -4.3420649 -4.3674655 -4.396287 -4.427382 -4.465004 -4.5039172 -4.5468225 -4.5894647 -4.61817 -4.6318083 -4.6494861 -4.6638803 -4.6537328 -4.6179714][-4.2321973 -4.230628 -4.246501 -4.2817912 -4.3251634 -4.3755341 -4.4338441 -4.4950004 -4.5426812 -4.5657864 -4.568254 -4.565136 -4.5587997 -4.5391445 -4.4996319][-4.1702366 -4.1482396 -4.1534 -4.1889763 -4.23338 -4.2780766 -4.3299046 -4.3849463 -4.4236107 -4.4367542 -4.4305692 -4.4160204 -4.3988142 -4.3781176 -4.3492551]]...]
INFO - root - 2017-12-08 01:03:14.281833: step 63810, loss = 21.08, batch loss = 21.00 (9.0 examples/sec; 0.884 sec/batch; 65h:59m:49s remains)
INFO - root - 2017-12-08 01:03:23.828987: step 63820, loss = 20.99, batch loss = 20.91 (8.3 examples/sec; 0.965 sec/batch; 72h:02m:30s remains)
INFO - root - 2017-12-08 01:03:33.272948: step 63830, loss = 21.01, batch loss = 20.93 (8.4 examples/sec; 0.958 sec/batch; 71h:28m:33s remains)
INFO - root - 2017-12-08 01:03:42.711979: step 63840, loss = 21.52, batch loss = 21.44 (8.5 examples/sec; 0.936 sec/batch; 69h:50m:55s remains)
INFO - root - 2017-12-08 01:03:52.095109: step 63850, loss = 21.49, batch loss = 21.41 (9.3 examples/sec; 0.863 sec/batch; 64h:25m:20s remains)
INFO - root - 2017-12-08 01:04:01.350237: step 63860, loss = 21.57, batch loss = 21.49 (9.5 examples/sec; 0.845 sec/batch; 63h:03m:05s remains)
INFO - root - 2017-12-08 01:04:10.666416: step 63870, loss = 21.35, batch loss = 21.26 (8.3 examples/sec; 0.967 sec/batch; 72h:10m:22s remains)
INFO - root - 2017-12-08 01:04:20.080154: step 63880, loss = 21.20, batch loss = 21.11 (8.3 examples/sec; 0.959 sec/batch; 71h:31m:31s remains)
INFO - root - 2017-12-08 01:04:29.477682: step 63890, loss = 21.37, batch loss = 21.28 (8.7 examples/sec; 0.921 sec/batch; 68h:44m:22s remains)
INFO - root - 2017-12-08 01:04:38.853696: step 63900, loss = 21.27, batch loss = 21.19 (8.9 examples/sec; 0.903 sec/batch; 67h:24m:04s remains)
2017-12-08 01:04:39.822056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4590445 -4.5267792 -4.5167856 -4.4434228 -4.3761234 -4.3645072 -4.3262672 -4.2367625 -4.1650162 -4.1081109 -4.1021643 -4.1329017 -4.1492596 -4.1499481 -4.1434011][-4.4635386 -4.5328736 -4.5191045 -4.4357371 -4.3611856 -4.341989 -4.290154 -4.1813245 -4.0928683 -4.0281124 -4.0331917 -4.0846543 -4.1162167 -4.1143827 -4.0939536][-4.4642997 -4.534471 -4.5169878 -4.4286175 -4.3496666 -4.3262305 -4.2699914 -4.1553144 -4.0636306 -3.9935892 -3.9950087 -4.05166 -4.0958395 -4.1009083 -4.0750504][-4.4673438 -4.5398145 -4.5235157 -4.4359012 -4.3525524 -4.3201752 -4.2526555 -4.1282458 -4.0358582 -3.9660678 -3.9628673 -4.0221834 -4.0901165 -4.1285281 -4.128231][-4.474175 -4.5477581 -4.530694 -4.4354777 -4.3268046 -4.2565117 -4.1522107 -4.0101786 -3.9370432 -3.9058483 -3.932543 -4.0149503 -4.1175971 -4.2054229 -4.2528281][-4.4808612 -4.5500031 -4.5182514 -4.3886471 -4.2165661 -4.0707393 -3.9093328 -3.7609675 -3.7489479 -3.81259 -3.9179571 -4.0481157 -4.1813293 -4.3061514 -4.394218][-4.4843669 -4.5445051 -4.4900937 -4.3068123 -4.0480742 -3.8101537 -3.5898237 -3.4554677 -3.5374157 -3.7288296 -3.9384539 -4.1199384 -4.2611108 -4.3876834 -4.4839492][-4.4824181 -4.5423074 -4.4858861 -4.2769651 -3.9645336 -3.6641166 -3.4059181 -3.2987893 -3.4614282 -3.7470646 -4.0219026 -4.2177477 -4.3326492 -4.42587 -4.495985][-4.4787269 -4.5565968 -4.5394616 -4.3652797 -4.0700765 -3.7614737 -3.4896984 -3.4005079 -3.5894501 -3.8889718 -4.158288 -4.323247 -4.3914323 -4.4359837 -4.4569416][-4.4754767 -4.58576 -4.6335793 -4.5326581 -4.3001127 -4.0182209 -3.7451491 -3.6603777 -3.8324442 -4.0942349 -4.3192048 -4.4342985 -4.459403 -4.4554744 -4.4098716][-4.4690266 -4.6117716 -4.7197523 -4.69048 -4.5274868 -4.2886391 -4.0372858 -3.9624343 -4.10296 -4.3119397 -4.4857006 -4.5560102 -4.5485816 -4.4956717 -4.3736987][-4.4477835 -4.6079106 -4.750988 -4.7727842 -4.6686039 -4.4838967 -4.2852535 -4.2393789 -4.35543 -4.5137606 -4.6341934 -4.659276 -4.6169081 -4.5160136 -4.3324533][-4.398798 -4.5517759 -4.6974807 -4.7429986 -4.6836443 -4.5655589 -4.4516158 -4.45725 -4.56287 -4.6749191 -4.7347093 -4.7073669 -4.6232162 -4.48388 -4.2762403][-4.3198915 -4.4368787 -4.5564032 -4.6071773 -4.5863647 -4.5434365 -4.5232735 -4.5782695 -4.6737227 -4.7383876 -4.738862 -4.6674271 -4.5548816 -4.4020567 -4.2089758][-4.226644 -4.2873054 -4.3647017 -4.4105892 -4.4236264 -4.4453387 -4.4939737 -4.5748267 -4.6509614 -4.6737971 -4.6366816 -4.5543771 -4.4477468 -4.3104634 -4.1506581]]...]
INFO - root - 2017-12-08 01:04:49.276737: step 63910, loss = 21.34, batch loss = 21.26 (8.1 examples/sec; 0.992 sec/batch; 74h:01m:27s remains)
INFO - root - 2017-12-08 01:04:58.616104: step 63920, loss = 21.62, batch loss = 21.53 (7.7 examples/sec; 1.032 sec/batch; 77h:00m:48s remains)
INFO - root - 2017-12-08 01:05:07.936025: step 63930, loss = 21.08, batch loss = 21.00 (8.5 examples/sec; 0.946 sec/batch; 70h:34m:40s remains)
INFO - root - 2017-12-08 01:05:17.463163: step 63940, loss = 21.37, batch loss = 21.29 (8.0 examples/sec; 1.002 sec/batch; 74h:46m:03s remains)
INFO - root - 2017-12-08 01:05:26.934999: step 63950, loss = 21.14, batch loss = 21.06 (8.1 examples/sec; 0.982 sec/batch; 73h:16m:05s remains)
INFO - root - 2017-12-08 01:05:36.378517: step 63960, loss = 21.23, batch loss = 21.15 (8.1 examples/sec; 0.986 sec/batch; 73h:33m:25s remains)
INFO - root - 2017-12-08 01:05:45.826278: step 63970, loss = 21.35, batch loss = 21.26 (9.3 examples/sec; 0.862 sec/batch; 64h:15m:39s remains)
INFO - root - 2017-12-08 01:05:55.259765: step 63980, loss = 21.13, batch loss = 21.05 (8.9 examples/sec; 0.897 sec/batch; 66h:55m:33s remains)
INFO - root - 2017-12-08 01:06:04.812073: step 63990, loss = 21.34, batch loss = 21.26 (8.7 examples/sec; 0.917 sec/batch; 68h:22m:40s remains)
INFO - root - 2017-12-08 01:06:14.133162: step 64000, loss = 21.47, batch loss = 21.39 (8.5 examples/sec; 0.940 sec/batch; 70h:05m:52s remains)
2017-12-08 01:06:15.097536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.236598 -4.2517266 -4.264883 -4.277287 -4.2963309 -4.325067 -4.3503842 -4.3523307 -4.3287597 -4.2920008 -4.258697 -4.2366705 -4.2220316 -4.2094488 -4.2026091][-4.2665124 -4.2775989 -4.2914548 -4.3081608 -4.3289838 -4.35438 -4.3774614 -4.3873272 -4.3856421 -4.3772817 -4.3657222 -4.3492789 -4.322021 -4.2851915 -4.2508907][-4.3000011 -4.3193989 -4.3468771 -4.3749371 -4.3925061 -4.3991723 -4.4023576 -4.409647 -4.4294095 -4.4583311 -4.48098 -4.4799533 -4.4451013 -4.382184 -4.31378][-4.3370714 -4.3749366 -4.4221039 -4.4576783 -4.4606066 -4.4348912 -4.4067993 -4.4029493 -4.4402561 -4.50756 -4.5695224 -4.5942616 -4.5666165 -4.491549 -4.3968711][-4.3642263 -4.4184976 -4.4791036 -4.5116038 -4.4901819 -4.4267311 -4.3598332 -4.3341241 -4.378891 -4.4773412 -4.5802088 -4.6415367 -4.6421795 -4.5841074 -4.4914093][-4.37051 -4.4286518 -4.4862971 -4.5013938 -4.4473009 -4.3387818 -4.2229729 -4.1635151 -4.2068973 -4.3372879 -4.4890895 -4.5992441 -4.6426706 -4.6227245 -4.5600848][-4.357995 -4.4071212 -4.4476056 -4.4341078 -4.3390994 -4.1793618 -4.0065904 -3.9028683 -3.9365895 -4.100625 -4.3120151 -4.4825068 -4.5705423 -4.5837164 -4.5523453][-4.3422022 -4.382473 -4.4079523 -4.3701658 -4.2376394 -4.02372 -3.7868848 -3.6310451 -3.648973 -3.8446095 -4.1196322 -4.3510633 -4.471487 -4.4886451 -4.4548144][-4.3378158 -4.378715 -4.4056826 -4.3631878 -4.2140713 -3.9672873 -3.6865306 -3.4912281 -3.4892712 -3.6960638 -4.0084457 -4.2778649 -4.4060483 -4.3903456 -4.3073807][-4.3417721 -4.3941793 -4.4395132 -4.4167361 -4.2877274 -4.0557737 -3.7811549 -3.5772493 -3.5493546 -3.7288022 -4.024406 -4.28306 -4.3896065 -4.3206897 -4.1696091][-4.346982 -4.4144578 -4.4858518 -4.4974365 -4.4128218 -4.2306414 -4.0043426 -3.8228152 -3.7755244 -3.905051 -4.1418819 -4.3486533 -4.4092865 -4.2889061 -4.0855813][-4.3452282 -4.4233704 -4.5171266 -4.5649538 -4.5298157 -4.4086385 -4.2467351 -4.11044 -4.0658016 -4.1522536 -4.3199625 -4.4582305 -4.4642439 -4.3083673 -4.0852795][-4.3355536 -4.4117527 -4.5155606 -4.5923896 -4.6035457 -4.5443206 -4.4502344 -4.3709574 -4.3468928 -4.40556 -4.5126486 -4.587883 -4.5523138 -4.3874803 -4.1783233][-4.3197751 -4.3816862 -4.4777966 -4.5671368 -4.6119046 -4.6046724 -4.5692887 -4.5394244 -4.5383291 -4.5806155 -4.645596 -4.6823058 -4.6363425 -4.4978933 -4.3343592][-4.3028131 -4.3445115 -4.4173732 -4.4973683 -4.55425 -4.5782213 -4.5804496 -4.5800567 -4.591248 -4.621449 -4.6619711 -4.6855416 -4.6606402 -4.580369 -4.4869547]]...]
INFO - root - 2017-12-08 01:06:24.514661: step 64010, loss = 21.39, batch loss = 21.31 (8.7 examples/sec; 0.922 sec/batch; 68h:47m:23s remains)
INFO - root - 2017-12-08 01:06:33.991522: step 64020, loss = 21.28, batch loss = 21.19 (8.3 examples/sec; 0.964 sec/batch; 71h:52m:35s remains)
INFO - root - 2017-12-08 01:06:43.448132: step 64030, loss = 22.04, batch loss = 21.96 (8.2 examples/sec; 0.981 sec/batch; 73h:10m:19s remains)
INFO - root - 2017-12-08 01:06:52.688155: step 64040, loss = 21.49, batch loss = 21.40 (8.1 examples/sec; 0.985 sec/batch; 73h:27m:47s remains)
INFO - root - 2017-12-08 01:07:02.043881: step 64050, loss = 21.37, batch loss = 21.29 (8.1 examples/sec; 0.982 sec/batch; 73h:14m:31s remains)
INFO - root - 2017-12-08 01:07:11.517960: step 64060, loss = 21.33, batch loss = 21.25 (8.7 examples/sec; 0.920 sec/batch; 68h:34m:24s remains)
INFO - root - 2017-12-08 01:07:20.898759: step 64070, loss = 21.12, batch loss = 21.04 (8.4 examples/sec; 0.951 sec/batch; 70h:54m:46s remains)
INFO - root - 2017-12-08 01:07:30.293184: step 64080, loss = 21.50, batch loss = 21.42 (8.0 examples/sec; 1.002 sec/batch; 74h:40m:34s remains)
INFO - root - 2017-12-08 01:07:39.549757: step 64090, loss = 21.63, batch loss = 21.55 (8.2 examples/sec; 0.970 sec/batch; 72h:20m:24s remains)
INFO - root - 2017-12-08 01:07:48.741024: step 64100, loss = 21.49, batch loss = 21.41 (8.9 examples/sec; 0.900 sec/batch; 67h:05m:03s remains)
2017-12-08 01:07:49.761942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4490185 -4.4966931 -4.5132856 -4.4834747 -4.4421158 -4.4324279 -4.4341974 -4.4664831 -4.5453882 -4.6299896 -4.6609306 -4.6179247 -4.5389452 -4.4677248 -4.4617004][-4.4733834 -4.5221419 -4.529285 -4.4952722 -4.4499063 -4.4229541 -4.4163647 -4.4535284 -4.5452828 -4.6358662 -4.6556444 -4.6019087 -4.5194821 -4.4496021 -4.4410458][-4.5077262 -4.5568509 -4.55656 -4.5216813 -4.4783816 -4.4401407 -4.4278603 -4.461194 -4.5447497 -4.6275921 -4.64149 -4.591918 -4.520556 -4.4521594 -4.4347205][-4.5262756 -4.5737419 -4.5609756 -4.5151577 -4.4721532 -4.4325919 -4.4205713 -4.4504023 -4.5175338 -4.5886493 -4.6106186 -4.5856471 -4.5358291 -4.4645591 -4.4235778][-4.4993806 -4.5425735 -4.5187368 -4.4520082 -4.3928652 -4.3444142 -4.3261518 -4.3522487 -4.4078608 -4.4834514 -4.5353279 -4.5487814 -4.5228891 -4.447978 -4.3783107][-4.4256411 -4.452589 -4.4087524 -4.3130136 -4.2254519 -4.1583328 -4.1212134 -4.13621 -4.189651 -4.2928114 -4.3999109 -4.4717193 -4.4871106 -4.4253168 -4.3390856][-4.3612256 -4.365706 -4.2996588 -4.1803117 -4.0620251 -3.9630761 -3.88798 -3.8713882 -3.9185252 -4.0534534 -4.2225094 -4.3564959 -4.4153576 -4.3729253 -4.2831874][-4.3777504 -4.3603678 -4.2808347 -4.1553278 -4.0161161 -3.8839712 -3.7706356 -3.7128394 -3.7336991 -3.8650384 -4.0601435 -4.2279673 -4.3098707 -4.2822666 -4.2052631][-4.4438238 -4.4027495 -4.3135657 -4.1967506 -4.0635576 -3.9345081 -3.8192644 -3.738739 -3.7219949 -3.8097954 -3.9838216 -4.15245 -4.2371869 -4.2188358 -4.1627097][-4.4943371 -4.44026 -4.356811 -4.2700028 -4.1725159 -4.074513 -3.9812965 -3.8976891 -3.8521402 -3.8873532 -4.0129933 -4.156795 -4.2315984 -4.2130294 -4.1646552][-4.5051389 -4.4560966 -4.4052925 -4.3676004 -4.31363 -4.2466445 -4.1758642 -4.1052117 -4.0532408 -4.0505075 -4.1206822 -4.224472 -4.2876511 -4.2746692 -4.2333717][-4.4335423 -4.4075894 -4.4130869 -4.4373975 -4.4274907 -4.3833685 -4.3331575 -4.2931867 -4.2638888 -4.2526078 -4.2811217 -4.3402114 -4.3868113 -4.3796253 -4.3493633][-4.3104768 -4.313416 -4.3729258 -4.4490733 -4.4753 -4.449492 -4.4182482 -4.4109764 -4.4171276 -4.4205556 -4.4332862 -4.4582648 -4.4785719 -4.4626031 -4.4344063][-4.2441735 -4.2623386 -4.34232 -4.4369612 -4.48164 -4.4688139 -4.4466472 -4.4517756 -4.4735522 -4.4890885 -4.5033674 -4.515656 -4.5187287 -4.4950218 -4.4660473][-4.2895579 -4.306716 -4.3672423 -4.4394736 -4.4761882 -4.4639564 -4.4394984 -4.4347453 -4.4468679 -4.4618206 -4.4840078 -4.5031862 -4.5075192 -4.4843764 -4.4499149]]...]
INFO - root - 2017-12-08 01:07:59.144167: step 64110, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.949 sec/batch; 70h:43m:23s remains)
INFO - root - 2017-12-08 01:08:08.551875: step 64120, loss = 21.73, batch loss = 21.65 (8.2 examples/sec; 0.973 sec/batch; 72h:34m:18s remains)
INFO - root - 2017-12-08 01:08:17.845425: step 64130, loss = 21.65, batch loss = 21.57 (9.4 examples/sec; 0.852 sec/batch; 63h:29m:17s remains)
INFO - root - 2017-12-08 01:08:27.165739: step 64140, loss = 21.95, batch loss = 21.87 (9.0 examples/sec; 0.885 sec/batch; 66h:00m:04s remains)
INFO - root - 2017-12-08 01:08:36.537825: step 64150, loss = 21.75, batch loss = 21.67 (8.4 examples/sec; 0.951 sec/batch; 70h:52m:47s remains)
INFO - root - 2017-12-08 01:08:46.093565: step 64160, loss = 21.32, batch loss = 21.24 (8.1 examples/sec; 0.986 sec/batch; 73h:28m:13s remains)
INFO - root - 2017-12-08 01:08:55.531383: step 64170, loss = 21.35, batch loss = 21.26 (8.0 examples/sec; 0.997 sec/batch; 74h:17m:57s remains)
INFO - root - 2017-12-08 01:09:04.883703: step 64180, loss = 21.35, batch loss = 21.26 (8.3 examples/sec; 0.965 sec/batch; 71h:55m:13s remains)
INFO - root - 2017-12-08 01:09:14.223475: step 64190, loss = 21.51, batch loss = 21.42 (8.7 examples/sec; 0.923 sec/batch; 68h:45m:30s remains)
INFO - root - 2017-12-08 01:09:23.490744: step 64200, loss = 21.48, batch loss = 21.40 (8.7 examples/sec; 0.922 sec/batch; 68h:41m:23s remains)
2017-12-08 01:09:24.418633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4605536 -4.648304 -4.7711387 -4.7528806 -4.6256943 -4.4586563 -4.3489928 -4.407577 -4.6314111 -4.8329554 -4.8738 -4.7938237 -4.6682096 -4.5400319 -4.4579091][-4.4554377 -4.6169691 -4.7266092 -4.70948 -4.5886321 -4.4384232 -4.3618712 -4.4318662 -4.6205463 -4.7854533 -4.8158026 -4.7324414 -4.6096606 -4.4801416 -4.3736725][-4.4641528 -4.5963182 -4.6840277 -4.670619 -4.5673981 -4.437561 -4.3742962 -4.4243517 -4.5532064 -4.6724324 -4.7117224 -4.6600714 -4.5612707 -4.4340763 -4.2911091][-4.4723711 -4.5730476 -4.6298714 -4.6090035 -4.5162716 -4.389924 -4.3041148 -4.2974339 -4.3569026 -4.4515328 -4.5332217 -4.5518441 -4.502521 -4.3898592 -4.219986][-4.4616094 -4.5334373 -4.5658889 -4.5391607 -4.4406133 -4.2832556 -4.1375961 -4.0487804 -4.0380812 -4.1152534 -4.2453394 -4.3579421 -4.3965058 -4.3408561 -4.1895509][-4.4366426 -4.4908323 -4.5100603 -4.4783926 -4.3636036 -4.1666083 -3.9690583 -3.8240948 -3.7614131 -3.8090725 -3.9499078 -4.1288891 -4.2603765 -4.292654 -4.1970339][-4.4066572 -4.4515457 -4.4631457 -4.4275241 -4.3038278 -4.0907016 -3.8826931 -3.7324221 -3.6597505 -3.6884747 -3.8136573 -4.0108929 -4.1936064 -4.2967567 -4.2625976][-4.3814487 -4.42255 -4.4261732 -4.3822465 -4.2574143 -4.0573878 -3.8805163 -3.7704504 -3.7324502 -3.7752163 -3.8849516 -4.0617924 -4.2376013 -4.3558364 -4.3507619][-4.3791723 -4.4284196 -4.4377928 -4.3976254 -4.2879815 -4.1228433 -3.9925766 -3.9330544 -3.93806 -3.9957702 -4.0790486 -4.2090864 -4.3442731 -4.4329553 -4.4181533][-4.393456 -4.4651446 -4.5048084 -4.4932289 -4.4132352 -4.2848659 -4.1861892 -4.1564116 -4.1763792 -4.2150702 -4.2507153 -4.3280873 -4.4273028 -4.4888363 -4.4598665][-4.3980808 -4.4881163 -4.5616407 -4.5876069 -4.5434 -4.4469 -4.3638368 -4.3422461 -4.3603239 -4.3739071 -4.3679714 -4.3999624 -4.4722581 -4.5211887 -4.49071][-4.3902736 -4.482482 -4.5747647 -4.6315274 -4.6241689 -4.5620074 -4.4957151 -4.4779396 -4.4932804 -4.49655 -4.4727883 -4.4736309 -4.5243607 -4.569191 -4.5490322][-4.3717413 -4.4501824 -4.5368533 -4.6022849 -4.6211147 -4.5939364 -4.552835 -4.5399318 -4.5512519 -4.55617 -4.539866 -4.5357084 -4.575202 -4.6202331 -4.6184092][-4.3492708 -4.4015751 -4.4628654 -4.5149822 -4.540626 -4.5368714 -4.5189662 -4.5108156 -4.5170479 -4.5263014 -4.5275474 -4.5352192 -4.571135 -4.6136875 -4.6273479][-4.3288026 -4.3517 -4.3811479 -4.4076819 -4.4238067 -4.4274936 -4.4232955 -4.4201622 -4.4235897 -4.4317532 -4.4398088 -4.4535708 -4.4827023 -4.5175567 -4.5394235]]...]
INFO - root - 2017-12-08 01:09:33.517878: step 64210, loss = 21.64, batch loss = 21.55 (9.3 examples/sec; 0.862 sec/batch; 64h:15m:08s remains)
INFO - root - 2017-12-08 01:09:42.822272: step 64220, loss = 21.13, batch loss = 21.05 (8.6 examples/sec; 0.935 sec/batch; 69h:41m:16s remains)
INFO - root - 2017-12-08 01:09:52.231548: step 64230, loss = 21.55, batch loss = 21.47 (8.3 examples/sec; 0.965 sec/batch; 71h:55m:40s remains)
INFO - root - 2017-12-08 01:10:01.377047: step 64240, loss = 21.45, batch loss = 21.37 (9.0 examples/sec; 0.889 sec/batch; 66h:15m:40s remains)
INFO - root - 2017-12-08 01:10:10.422635: step 64250, loss = 21.57, batch loss = 21.48 (8.9 examples/sec; 0.898 sec/batch; 66h:53m:21s remains)
INFO - root - 2017-12-08 01:10:19.855903: step 64260, loss = 21.41, batch loss = 21.32 (8.4 examples/sec; 0.957 sec/batch; 71h:19m:33s remains)
INFO - root - 2017-12-08 01:10:29.226604: step 64270, loss = 21.00, batch loss = 20.92 (8.1 examples/sec; 0.991 sec/batch; 73h:50m:42s remains)
INFO - root - 2017-12-08 01:10:38.601552: step 64280, loss = 21.62, batch loss = 21.53 (8.0 examples/sec; 0.998 sec/batch; 74h:23m:26s remains)
INFO - root - 2017-12-08 01:10:47.954392: step 64290, loss = 21.26, batch loss = 21.18 (8.5 examples/sec; 0.940 sec/batch; 70h:02m:34s remains)
INFO - root - 2017-12-08 01:10:57.313912: step 64300, loss = 21.36, batch loss = 21.28 (8.3 examples/sec; 0.961 sec/batch; 71h:37m:38s remains)
2017-12-08 01:10:58.174432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5517354 -4.5499558 -4.57496 -4.6150537 -4.6440506 -4.644104 -4.6132827 -4.5598655 -4.5131249 -4.5278335 -4.5866785 -4.6437321 -4.6818638 -4.6823153 -4.6437092][-4.4268022 -4.4456668 -4.4942622 -4.5610352 -4.6174893 -4.629622 -4.5994339 -4.5372248 -4.4782739 -4.4865632 -4.5500526 -4.6141162 -4.6536479 -4.6586628 -4.6326337][-4.2905693 -4.3406649 -4.4087143 -4.4967995 -4.5746856 -4.5948462 -4.5608082 -4.4876204 -4.4253755 -4.4393625 -4.5119252 -4.5870972 -4.6283426 -4.6360784 -4.6206422][-4.1611528 -4.2449327 -4.321353 -4.4150395 -4.4973717 -4.5152731 -4.4696817 -4.3917804 -4.3470697 -4.3840203 -4.4707317 -4.5622067 -4.6129789 -4.6259651 -4.6183267][-4.056654 -4.1503773 -4.2104831 -4.2823315 -4.3462296 -4.3494658 -4.2867393 -4.2136173 -4.2046256 -4.2699766 -4.3719525 -4.4881816 -4.5654159 -4.6020966 -4.6084023][-4.0435081 -4.1140103 -4.1276121 -4.1394639 -4.1528807 -4.1214466 -4.0300493 -3.9593706 -3.9924448 -4.0865636 -4.20419 -4.3519683 -4.4754963 -4.5599236 -4.593122][-4.0862083 -4.1314783 -4.104353 -4.0545096 -4.008 -3.9336514 -3.8040614 -3.7154164 -3.7704682 -3.8767376 -3.9977438 -4.16593 -4.3400235 -4.4876103 -4.5606728][-4.16808 -4.1931705 -4.1524587 -4.0733318 -3.990875 -3.8931446 -3.744091 -3.6365545 -3.6888356 -3.7795064 -3.8755655 -4.0320854 -4.227529 -4.4150262 -4.5130258][-4.2727427 -4.2858324 -4.2550397 -4.1872044 -4.1067586 -4.0190449 -3.8907607 -3.7933979 -3.8332236 -3.8931963 -3.9514842 -4.0782828 -4.2616639 -4.4354496 -4.5126171][-4.3712969 -4.3800788 -4.3694587 -4.3362422 -4.283567 -4.2180495 -4.1207366 -4.04427 -4.0661445 -4.0956292 -4.1254406 -4.2296319 -4.3891888 -4.5187831 -4.5541997][-4.4577355 -4.4660726 -4.4703 -4.4694867 -4.4506845 -4.4126782 -4.3507314 -4.2981043 -4.3045421 -4.3122535 -4.3257394 -4.4114518 -4.5371761 -4.612175 -4.6063895][-4.5298305 -4.54086 -4.5505052 -4.5627303 -4.5618954 -4.5445037 -4.5124354 -4.4810553 -4.4801092 -4.4831324 -4.4983921 -4.5751123 -4.671536 -4.7033091 -4.6654844][-4.5782104 -4.5894423 -4.5987868 -4.6114931 -4.6174526 -4.6144791 -4.6034083 -4.5876069 -4.5873442 -4.5954661 -4.6161022 -4.67796 -4.7425451 -4.7475305 -4.69741][-4.596169 -4.6061544 -4.6126552 -4.6214914 -4.6292548 -4.6364107 -4.6404963 -4.6365714 -4.6391082 -4.6518207 -4.6758118 -4.720685 -4.7577095 -4.7481866 -4.695806][-4.5783658 -4.5841966 -4.5870686 -4.5906243 -4.5951977 -4.6019721 -4.6082883 -4.6091681 -4.6132731 -4.6270742 -4.6523852 -4.6878457 -4.7130818 -4.7038288 -4.6585636]]...]
INFO - root - 2017-12-08 01:11:07.526740: step 64310, loss = 21.26, batch loss = 21.17 (8.9 examples/sec; 0.897 sec/batch; 66h:49m:48s remains)
INFO - root - 2017-12-08 01:11:16.769584: step 64320, loss = 21.73, batch loss = 21.65 (8.9 examples/sec; 0.898 sec/batch; 66h:52m:04s remains)
INFO - root - 2017-12-08 01:11:26.188480: step 64330, loss = 21.28, batch loss = 21.20 (8.6 examples/sec; 0.926 sec/batch; 68h:57m:13s remains)
INFO - root - 2017-12-08 01:11:35.601366: step 64340, loss = 21.11, batch loss = 21.03 (8.0 examples/sec; 0.995 sec/batch; 74h:08m:18s remains)
INFO - root - 2017-12-08 01:11:44.959909: step 64350, loss = 21.19, batch loss = 21.11 (8.3 examples/sec; 0.966 sec/batch; 71h:58m:08s remains)
INFO - root - 2017-12-08 01:11:54.359791: step 64360, loss = 21.20, batch loss = 21.12 (8.4 examples/sec; 0.954 sec/batch; 71h:01m:43s remains)
INFO - root - 2017-12-08 01:12:03.559311: step 64370, loss = 21.54, batch loss = 21.46 (8.9 examples/sec; 0.901 sec/batch; 67h:06m:58s remains)
INFO - root - 2017-12-08 01:12:12.844007: step 64380, loss = 21.54, batch loss = 21.45 (8.8 examples/sec; 0.910 sec/batch; 67h:46m:02s remains)
INFO - root - 2017-12-08 01:12:22.253149: step 64390, loss = 21.32, batch loss = 21.23 (8.2 examples/sec; 0.971 sec/batch; 72h:20m:22s remains)
INFO - root - 2017-12-08 01:12:31.612537: step 64400, loss = 21.25, batch loss = 21.17 (8.6 examples/sec; 0.929 sec/batch; 69h:13m:00s remains)
2017-12-08 01:12:32.662119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3725305 -4.32904 -4.3069134 -4.3020144 -4.2945046 -4.3148623 -4.3940787 -4.483376 -4.5289621 -4.5397558 -4.5245094 -4.4798689 -4.4077234 -4.339303 -4.3081861][-4.3879495 -4.3607249 -4.3565855 -4.3637829 -4.3569269 -4.3700051 -4.4396381 -4.5164504 -4.5489359 -4.55394 -4.5509953 -4.530149 -4.4725761 -4.4007354 -4.3623338][-4.414721 -4.4031963 -4.4173155 -4.437511 -4.43685 -4.4462085 -4.4951735 -4.5379448 -4.5450706 -4.5452652 -4.5632677 -4.5793343 -4.5500154 -4.4822106 -4.4337325][-4.4537458 -4.4488344 -4.4693465 -4.49153 -4.4877877 -4.4783034 -4.4869113 -4.4787917 -4.4597263 -4.4672956 -4.5155234 -4.5745516 -4.5853515 -4.5397682 -4.4935813][-4.4945393 -4.4858284 -4.498632 -4.5090175 -4.4883213 -4.4393029 -4.3877778 -4.3189244 -4.27834 -4.3100395 -4.4022861 -4.5070391 -4.5588894 -4.5429416 -4.511735][-4.5094657 -4.4819927 -4.4717417 -4.4638772 -4.4284086 -4.3424006 -4.2296729 -4.1047668 -4.0589113 -4.1434717 -4.2999921 -4.4540362 -4.5347781 -4.5331969 -4.5095568][-4.5002427 -4.4445076 -4.4046283 -4.3802767 -4.3329597 -4.2127619 -4.045074 -3.877213 -3.8397231 -3.9824991 -4.199995 -4.4003515 -4.5098205 -4.5191646 -4.4932384][-4.4986057 -4.4159169 -4.3518333 -4.3163114 -4.2535505 -4.1004467 -3.8973131 -3.7152967 -3.6911285 -3.8565412 -4.0924878 -4.3208 -4.4641795 -4.4892421 -4.4532471][-4.5079384 -4.4070263 -4.3321137 -4.2917409 -4.2124743 -4.04412 -3.8487251 -3.6983261 -3.6923437 -3.840816 -4.0533643 -4.2813878 -4.4427361 -4.4712896 -4.4130516][-4.5212779 -4.4147348 -4.3416786 -4.3013577 -4.2096868 -4.0444279 -3.8785441 -3.7750781 -3.7897913 -3.914206 -4.0955124 -4.299777 -4.4486208 -4.461195 -4.3725843][-4.5341754 -4.4267755 -4.3486061 -4.2938809 -4.1844087 -4.0212603 -3.8878703 -3.8421578 -3.9009225 -4.0283556 -4.187284 -4.3494263 -4.4604244 -4.4439974 -4.3255367][-4.5489483 -4.4311256 -4.3219709 -4.2267466 -4.0898404 -3.9325867 -3.8441308 -3.8716259 -4.000607 -4.1574607 -4.2970123 -4.4053092 -4.4652996 -4.4166031 -4.2734571][-4.5965538 -4.4752584 -4.3295536 -4.1899929 -4.0359879 -3.9011767 -3.8625622 -3.9394875 -4.1004748 -4.2553415 -4.3564482 -4.4049225 -4.4166765 -4.3481941 -4.2003684][-4.657402 -4.5573392 -4.4047165 -4.2516546 -4.110374 -4.0156441 -4.01742 -4.0982904 -4.2269344 -4.3331261 -4.3812976 -4.3836555 -4.36502 -4.290628 -4.1498728][-4.6865516 -4.6239867 -4.5001812 -4.37069 -4.2633142 -4.2080908 -4.2303529 -4.2844625 -4.3509879 -4.3985767 -4.4112177 -4.4028769 -4.3814125 -4.3102283 -4.1793222]]...]
INFO - root - 2017-12-08 01:12:41.981054: step 64410, loss = 21.61, batch loss = 21.52 (8.3 examples/sec; 0.965 sec/batch; 71h:52m:16s remains)
INFO - root - 2017-12-08 01:12:51.218459: step 64420, loss = 20.96, batch loss = 20.88 (8.7 examples/sec; 0.922 sec/batch; 68h:41m:36s remains)
INFO - root - 2017-12-08 01:13:00.507924: step 64430, loss = 21.15, batch loss = 21.07 (8.8 examples/sec; 0.909 sec/batch; 67h:42m:02s remains)
INFO - root - 2017-12-08 01:13:09.928166: step 64440, loss = 21.70, batch loss = 21.62 (9.4 examples/sec; 0.850 sec/batch; 63h:15m:36s remains)
INFO - root - 2017-12-08 01:13:19.428997: step 64450, loss = 21.63, batch loss = 21.55 (9.2 examples/sec; 0.873 sec/batch; 65h:01m:55s remains)
INFO - root - 2017-12-08 01:13:28.872897: step 64460, loss = 21.31, batch loss = 21.23 (8.6 examples/sec; 0.932 sec/batch; 69h:22m:18s remains)
INFO - root - 2017-12-08 01:13:38.202924: step 64470, loss = 21.83, batch loss = 21.75 (8.3 examples/sec; 0.960 sec/batch; 71h:26m:33s remains)
INFO - root - 2017-12-08 01:13:47.607531: step 64480, loss = 21.70, batch loss = 21.62 (8.2 examples/sec; 0.970 sec/batch; 72h:15m:05s remains)
INFO - root - 2017-12-08 01:13:56.855188: step 64490, loss = 21.35, batch loss = 21.27 (9.2 examples/sec; 0.865 sec/batch; 64h:23m:13s remains)
INFO - root - 2017-12-08 01:14:06.262705: step 64500, loss = 21.57, batch loss = 21.48 (8.3 examples/sec; 0.959 sec/batch; 71h:21m:39s remains)
2017-12-08 01:14:07.151180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3247766 -4.3220234 -4.3098383 -4.3037648 -4.3008561 -4.294054 -4.2500491 -4.2086573 -4.2077522 -4.2321205 -4.2802973 -4.3283739 -4.3300433 -4.2390432 -4.1172185][-4.359127 -4.3930211 -4.3937068 -4.3753581 -4.3554268 -4.3261137 -4.2592454 -4.1979837 -4.1883831 -4.2208776 -4.2935638 -4.3718238 -4.3932576 -4.2988567 -4.163352][-4.4429483 -4.5165753 -4.5354524 -4.5107107 -4.4775739 -4.42716 -4.34834 -4.2867584 -4.2848182 -4.3280292 -4.4110518 -4.4993019 -4.5222116 -4.4231439 -4.2833109][-4.5429559 -4.6464806 -4.67929 -4.6518116 -4.6055059 -4.5273533 -4.428896 -4.3591738 -4.3659735 -4.425663 -4.5181537 -4.6139021 -4.6420088 -4.5574722 -4.4298482][-4.5978427 -4.70235 -4.7323508 -4.7012677 -4.6412997 -4.5211196 -4.3708086 -4.2582827 -4.2750368 -4.3845239 -4.5218625 -4.6515222 -4.7008991 -4.6433196 -4.5297828][-4.574595 -4.6512623 -4.6629324 -4.6287355 -4.5574055 -4.3803024 -4.13467 -3.9422705 -3.988162 -4.1959357 -4.4187717 -4.6025729 -4.680253 -4.6484609 -4.5425167][-4.5279307 -4.5814958 -4.5783167 -4.5403762 -4.4485765 -4.1946774 -3.8194282 -3.5305989 -3.6262646 -3.9581017 -4.279377 -4.524878 -4.6370144 -4.62334 -4.5102973][-4.5232949 -4.5825372 -4.5778651 -4.5311685 -4.4055042 -4.0793886 -3.5953937 -3.2415671 -3.3893905 -3.8144593 -4.2001595 -4.4814858 -4.6151366 -4.6098542 -4.4842734][-4.555274 -4.6372833 -4.6445537 -4.5939341 -4.4469032 -4.103549 -3.6073747 -3.2696261 -3.4408569 -3.8663926 -4.2335606 -4.4919686 -4.6163826 -4.6135478 -4.4905834][-4.5764132 -4.6744909 -4.6996136 -4.6583571 -4.5195346 -4.2254105 -3.8184302 -3.5613587 -3.7100394 -4.0483136 -4.3400826 -4.5433154 -4.6418824 -4.6386714 -4.5278482][-4.5576673 -4.6420746 -4.6764255 -4.658689 -4.5590496 -4.3556066 -4.0863867 -3.9306698 -4.03962 -4.2722387 -4.4794264 -4.6204925 -4.6866832 -4.6763492 -4.5765595][-4.49093 -4.5325236 -4.570971 -4.5947132 -4.5618854 -4.456851 -4.3061538 -4.226553 -4.297318 -4.4435759 -4.5804033 -4.6690712 -4.7093544 -4.6944551 -4.608161][-4.3973479 -4.3966002 -4.44074 -4.5061307 -4.5317206 -4.4927993 -4.4061327 -4.3620377 -4.4092512 -4.508636 -4.6056175 -4.6609826 -4.6838069 -4.6676321 -4.5966368][-4.3073826 -4.2771506 -4.3251758 -4.4108768 -4.4634681 -4.4521842 -4.3910947 -4.3599834 -4.3958764 -4.4736314 -4.5547371 -4.5999694 -4.6167779 -4.6022477 -4.5468512][-4.2482357 -4.2054863 -4.2515545 -4.3343911 -4.3881822 -4.3853869 -4.3386054 -4.3122654 -4.3313608 -4.3815951 -4.444808 -4.4869461 -4.5061955 -4.4996953 -4.463449]]...]
INFO - root - 2017-12-08 01:14:16.610649: step 64510, loss = 21.28, batch loss = 21.20 (8.2 examples/sec; 0.974 sec/batch; 72h:28m:16s remains)
INFO - root - 2017-12-08 01:14:25.994527: step 64520, loss = 21.38, batch loss = 21.30 (8.8 examples/sec; 0.911 sec/batch; 67h:47m:11s remains)
INFO - root - 2017-12-08 01:14:35.486801: step 64530, loss = 21.44, batch loss = 21.36 (8.5 examples/sec; 0.945 sec/batch; 70h:20m:34s remains)
INFO - root - 2017-12-08 01:14:44.799477: step 64540, loss = 21.50, batch loss = 21.42 (8.4 examples/sec; 0.949 sec/batch; 70h:36m:48s remains)
INFO - root - 2017-12-08 01:14:54.216228: step 64550, loss = 21.18, batch loss = 21.10 (8.0 examples/sec; 1.000 sec/batch; 74h:28m:03s remains)
INFO - root - 2017-12-08 01:15:03.368582: step 64560, loss = 21.64, batch loss = 21.56 (8.4 examples/sec; 0.951 sec/batch; 70h:48m:42s remains)
INFO - root - 2017-12-08 01:15:12.660700: step 64570, loss = 22.17, batch loss = 22.09 (8.7 examples/sec; 0.918 sec/batch; 68h:17m:40s remains)
INFO - root - 2017-12-08 01:15:21.997569: step 64580, loss = 21.46, batch loss = 21.38 (9.1 examples/sec; 0.877 sec/batch; 65h:16m:54s remains)
INFO - root - 2017-12-08 01:15:31.371865: step 64590, loss = 21.40, batch loss = 21.32 (8.7 examples/sec; 0.918 sec/batch; 68h:20m:11s remains)
INFO - root - 2017-12-08 01:15:40.783294: step 64600, loss = 21.26, batch loss = 21.18 (8.7 examples/sec; 0.916 sec/batch; 68h:09m:50s remains)
2017-12-08 01:15:41.769823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.489738 -4.5774584 -4.6128516 -4.6049414 -4.545785 -4.4607692 -4.3722391 -4.2794681 -4.281601 -4.3931108 -4.4653444 -4.4815 -4.4979448 -4.5331917 -4.5776873][-4.4814992 -4.575058 -4.6200862 -4.6193738 -4.5766611 -4.5142851 -4.4346209 -4.3385825 -4.338655 -4.4383631 -4.5136771 -4.5421033 -4.5515161 -4.5691934 -4.6063981][-4.4740648 -4.5977879 -4.6632848 -4.6625171 -4.6240392 -4.5790954 -4.5081134 -4.4130354 -4.41212 -4.4903493 -4.5506148 -4.5744643 -4.571291 -4.5689158 -4.5910807][-4.4788289 -4.6372061 -4.7165556 -4.7013087 -4.6472063 -4.5938568 -4.5110912 -4.3969893 -4.3811727 -4.4463954 -4.5080085 -4.5510454 -4.5660777 -4.5560603 -4.5490727][-4.4837894 -4.6586204 -4.7294621 -4.6880188 -4.6056485 -4.5166874 -4.3832178 -4.2125816 -4.1694155 -4.2475896 -4.3435969 -4.4390082 -4.4956036 -4.4767895 -4.4168043][-4.481504 -4.6519346 -4.7011957 -4.6374803 -4.5323367 -4.3966932 -4.1823611 -3.9320369 -3.8717725 -3.993824 -4.1412573 -4.2883835 -4.3815141 -4.3430996 -4.21557][-4.4755516 -4.6342964 -4.6648273 -4.5866785 -4.4533224 -4.2548904 -3.942657 -3.6123064 -3.5618443 -3.7586455 -3.9679863 -4.1520147 -4.2584 -4.1949387 -4.0200315][-4.4690752 -4.6152005 -4.6317306 -4.53434 -4.3551836 -4.0825295 -3.6953273 -3.3386292 -3.344604 -3.6292939 -3.8896916 -4.075284 -4.1502838 -4.0544205 -3.8742452][-4.4797463 -4.6201558 -4.6355987 -4.5312943 -4.326859 -4.0256343 -3.6484315 -3.354183 -3.4225931 -3.7247591 -3.9698775 -4.102951 -4.0977407 -3.9634063 -3.8182321][-4.5152183 -4.6590209 -4.68541 -4.5963454 -4.4014883 -4.1304097 -3.8393846 -3.6646719 -3.7731731 -4.021811 -4.1999249 -4.2571964 -4.1666293 -4.0037475 -3.9061279][-4.5466962 -4.6904373 -4.7233472 -4.650641 -4.4800887 -4.2600331 -4.0651374 -3.9937835 -4.1273613 -4.3218632 -4.4338112 -4.4275494 -4.28275 -4.1127586 -4.0598111][-4.5549922 -4.6998706 -4.7359366 -4.67048 -4.5139184 -4.3248472 -4.1804032 -4.1513829 -4.2831178 -4.4424968 -4.5038433 -4.4653511 -4.3314881 -4.2141056 -4.2191563][-4.5459228 -4.6952085 -4.7348328 -4.6673403 -4.506361 -4.3167491 -4.1778784 -4.1486063 -4.2781577 -4.44416 -4.4939456 -4.4555116 -4.3786564 -4.3417826 -4.3851223][-4.5295 -4.6845241 -4.7403584 -4.68931 -4.537426 -4.3463168 -4.1937704 -4.1478567 -4.278451 -4.4661822 -4.5273657 -4.4933376 -4.4547958 -4.4565139 -4.4939413][-4.5249829 -4.6891093 -4.7770882 -4.76867 -4.6481004 -4.4633908 -4.2894206 -4.2066956 -4.3013363 -4.4751153 -4.5470815 -4.5344329 -4.5241714 -4.5360417 -4.5555067]]...]
INFO - root - 2017-12-08 01:15:51.089242: step 64610, loss = 21.41, batch loss = 21.33 (8.9 examples/sec; 0.902 sec/batch; 67h:07m:06s remains)
INFO - root - 2017-12-08 01:16:00.312446: step 64620, loss = 21.24, batch loss = 21.16 (9.6 examples/sec; 0.837 sec/batch; 62h:15m:04s remains)
INFO - root - 2017-12-08 01:16:09.793775: step 64630, loss = 21.83, batch loss = 21.74 (8.5 examples/sec; 0.946 sec/batch; 70h:21m:36s remains)
INFO - root - 2017-12-08 01:16:19.217735: step 64640, loss = 21.50, batch loss = 21.42 (8.8 examples/sec; 0.909 sec/batch; 67h:38m:14s remains)
INFO - root - 2017-12-08 01:16:28.696535: step 64650, loss = 21.76, batch loss = 21.68 (8.0 examples/sec; 0.995 sec/batch; 74h:01m:20s remains)
INFO - root - 2017-12-08 01:16:38.124912: step 64660, loss = 21.37, batch loss = 21.28 (8.9 examples/sec; 0.901 sec/batch; 67h:03m:16s remains)
INFO - root - 2017-12-08 01:16:47.573962: step 64670, loss = 21.43, batch loss = 21.35 (8.0 examples/sec; 1.006 sec/batch; 74h:48m:50s remains)
INFO - root - 2017-12-08 01:16:56.912964: step 64680, loss = 21.43, batch loss = 21.34 (8.2 examples/sec; 0.978 sec/batch; 72h:44m:13s remains)
INFO - root - 2017-12-08 01:17:06.305891: step 64690, loss = 21.59, batch loss = 21.51 (8.7 examples/sec; 0.921 sec/batch; 68h:32m:11s remains)
INFO - root - 2017-12-08 01:17:15.598223: step 64700, loss = 21.65, batch loss = 21.57 (8.8 examples/sec; 0.911 sec/batch; 67h:47m:05s remains)
2017-12-08 01:17:16.557705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.55663 -4.5531373 -4.5439949 -4.5221391 -4.4777813 -4.42165 -4.3838964 -4.3848844 -4.4218564 -4.46733 -4.517745 -4.5606089 -4.5464706 -4.462687 -4.3957486][-4.5627565 -4.5553255 -4.5315571 -4.4903216 -4.4344254 -4.3821197 -4.3515563 -4.3537812 -4.3892159 -4.4305587 -4.4784374 -4.52991 -4.5323296 -4.4801407 -4.4457345][-4.6159673 -4.5976982 -4.5494266 -4.473875 -4.39628 -4.339427 -4.3154926 -4.32568 -4.3642449 -4.4146008 -4.4646988 -4.5092692 -4.5125947 -4.4785495 -4.4574881][-4.6177187 -4.6107349 -4.5581455 -4.4665208 -4.37526 -4.3121958 -4.2936606 -4.3117766 -4.3553133 -4.4152737 -4.4608393 -4.4818907 -4.4650617 -4.4286256 -4.4061794][-4.5436878 -4.5721593 -4.5400505 -4.4599009 -4.3708458 -4.29494 -4.268425 -4.281074 -4.3235426 -4.3905282 -4.4299569 -4.4274883 -4.3874326 -4.3442626 -4.3305984][-4.4479532 -4.5023127 -4.4901752 -4.4185638 -4.3192153 -4.2103624 -4.15079 -4.1522446 -4.2124348 -4.308044 -4.358161 -4.3433113 -4.2832966 -4.2369733 -4.2511663][-4.3224716 -4.392941 -4.3989477 -4.331636 -4.2145863 -4.0618486 -3.9443104 -3.9196441 -4.0077114 -4.1462164 -4.2246246 -4.2178388 -4.1521583 -4.105401 -4.1388326][-4.2271271 -4.3085222 -4.3306289 -4.2764421 -4.1573491 -3.9749484 -3.8011358 -3.7420325 -3.8470755 -4.0169225 -4.1222744 -4.1369023 -4.0865498 -4.0426545 -4.0787759][-4.2612667 -4.331336 -4.3516693 -4.3179965 -4.2293577 -4.0733976 -3.9035387 -3.835484 -3.9281273 -4.0769162 -4.17022 -4.1928864 -4.1652079 -4.133287 -4.1662626][-4.3372793 -4.3800936 -4.3918872 -4.386785 -4.3532891 -4.2710166 -4.157548 -4.0969462 -4.146946 -4.240634 -4.2988386 -4.3168211 -4.3050289 -4.2826676 -4.3051076][-4.4216857 -4.4404688 -4.4390769 -4.440258 -4.43582 -4.4194393 -4.3744822 -4.3323765 -4.3495374 -4.4025121 -4.4380822 -4.44789 -4.4312944 -4.4008031 -4.4027233][-4.5338964 -4.5410433 -4.524838 -4.5145574 -4.5076771 -4.5237 -4.5349917 -4.524014 -4.5344744 -4.5675168 -4.5859485 -4.57477 -4.527565 -4.4704204 -4.4535179][-4.6094408 -4.612669 -4.5886908 -4.5706387 -4.5553346 -4.5804272 -4.6243958 -4.648128 -4.6686997 -4.6875949 -4.6842051 -4.6416426 -4.5616188 -4.4831724 -4.4526892][-4.615222 -4.6114635 -4.5825725 -4.563283 -4.5434084 -4.5616875 -4.61106 -4.6619315 -4.6989288 -4.7087121 -4.6844406 -4.6139407 -4.5162039 -4.4249907 -4.3745961][-4.5793705 -4.5608521 -4.5252819 -4.5050936 -4.48696 -4.4953709 -4.5344863 -4.5944247 -4.6412125 -4.647604 -4.6077547 -4.5225153 -4.4249763 -4.3362184 -4.2740173]]...]
INFO - root - 2017-12-08 01:17:25.917190: step 64710, loss = 21.82, batch loss = 21.73 (8.3 examples/sec; 0.959 sec/batch; 71h:22m:16s remains)
INFO - root - 2017-12-08 01:17:35.407939: step 64720, loss = 21.87, batch loss = 21.78 (8.9 examples/sec; 0.900 sec/batch; 66h:57m:20s remains)
INFO - root - 2017-12-08 01:17:44.229558: step 64730, loss = 21.74, batch loss = 21.66 (8.5 examples/sec; 0.940 sec/batch; 69h:53m:26s remains)
INFO - root - 2017-12-08 01:17:53.616522: step 64740, loss = 21.85, batch loss = 21.77 (8.2 examples/sec; 0.972 sec/batch; 72h:17m:35s remains)
INFO - root - 2017-12-08 01:18:02.958604: step 64750, loss = 21.04, batch loss = 20.95 (8.3 examples/sec; 0.967 sec/batch; 71h:53m:50s remains)
INFO - root - 2017-12-08 01:18:12.195453: step 64760, loss = 21.57, batch loss = 21.49 (8.6 examples/sec; 0.927 sec/batch; 68h:56m:34s remains)
INFO - root - 2017-12-08 01:18:21.569340: step 64770, loss = 21.69, batch loss = 21.61 (8.4 examples/sec; 0.954 sec/batch; 70h:54m:45s remains)
INFO - root - 2017-12-08 01:18:31.100050: step 64780, loss = 21.54, batch loss = 21.46 (8.4 examples/sec; 0.947 sec/batch; 70h:27m:09s remains)
INFO - root - 2017-12-08 01:18:40.514366: step 64790, loss = 21.62, batch loss = 21.53 (9.0 examples/sec; 0.891 sec/batch; 66h:15m:25s remains)
INFO - root - 2017-12-08 01:18:49.922872: step 64800, loss = 21.13, batch loss = 21.04 (9.1 examples/sec; 0.881 sec/batch; 65h:28m:50s remains)
2017-12-08 01:18:50.875866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8252954 -4.8421159 -4.878139 -4.9091835 -4.9005814 -4.8594804 -4.8256068 -4.8228183 -4.8411193 -4.8260064 -4.7740045 -4.7316065 -4.6917105 -4.62874 -4.60415][-4.706068 -4.7298231 -4.7834678 -4.8243837 -4.815218 -4.769136 -4.7355528 -4.7479911 -4.7917557 -4.7999487 -4.7520523 -4.69615 -4.6309919 -4.5459857 -4.5184631][-4.5380673 -4.560122 -4.621182 -4.6661954 -4.6565256 -4.6085644 -4.577539 -4.6052637 -4.6756964 -4.7212086 -4.6967015 -4.6356006 -4.5454454 -4.4413028 -4.413579][-4.3671641 -4.3854737 -4.4440012 -4.4875274 -4.4754128 -4.4193974 -4.3782473 -4.4031434 -4.4839511 -4.5612745 -4.5684972 -4.5233817 -4.4343681 -4.3344617 -4.3207169][-4.2281485 -4.240706 -4.2887897 -4.3228745 -4.3028941 -4.2260284 -4.1536617 -4.1484356 -4.2155375 -4.31286 -4.3610115 -4.3594747 -4.3100634 -4.2455373 -4.2628474][-4.1184387 -4.1341934 -4.1692863 -4.1867762 -4.1581645 -4.0624762 -3.957088 -3.9098849 -3.9441135 -4.0403333 -4.1139445 -4.1559229 -4.1596446 -4.1471491 -4.2075696][-4.07531 -4.1058497 -4.1342974 -4.1375551 -4.1005912 -3.9946752 -3.8718786 -3.7924469 -3.7979472 -3.8859205 -3.9684551 -4.035532 -4.0765219 -4.0967665 -4.1805358][-4.1257319 -4.1901946 -4.2323718 -4.2307186 -4.179615 -4.0641804 -3.9364102 -3.8414385 -3.8323951 -3.9184632 -4.0109849 -4.0969706 -4.1596928 -4.1835561 -4.2517042][-4.2393961 -4.3376436 -4.4042983 -4.4082155 -4.3461428 -4.223814 -4.0998807 -4.00275 -3.9877336 -4.0694041 -4.1678839 -4.2683105 -4.3427129 -4.3596153 -4.3966317][-4.3760204 -4.480351 -4.5609751 -4.576849 -4.5173059 -4.4008455 -4.2863417 -4.1947527 -4.1763477 -4.2447553 -4.3367796 -4.4408226 -4.5196161 -4.5346556 -4.5525312][-4.5154972 -4.597219 -4.6673408 -4.6855617 -4.6377521 -4.5398126 -4.4415593 -4.3623929 -4.3442645 -4.3986363 -4.4788446 -4.5750017 -4.6502404 -4.6651168 -4.6730585][-4.6124687 -4.6599717 -4.7053852 -4.7183905 -4.6876183 -4.6217895 -4.551403 -4.4924917 -4.4780579 -4.5160394 -4.577023 -4.6537247 -4.7150307 -4.7268772 -4.7280641][-4.6208305 -4.6376395 -4.6607084 -4.6682334 -4.6577854 -4.633122 -4.6014438 -4.5708919 -4.5646882 -4.589746 -4.63118 -4.6857448 -4.7279053 -4.7301574 -4.7192292][-4.5348306 -4.5341272 -4.5424633 -4.5433655 -4.5431824 -4.5448322 -4.5415525 -4.5353117 -4.5418611 -4.565289 -4.5994158 -4.6400123 -4.6668506 -4.661695 -4.6397252][-4.3939929 -4.3873997 -4.3891644 -4.3866348 -4.38764 -4.3953147 -4.40154 -4.4064012 -4.4171147 -4.4360361 -4.4610934 -4.4890676 -4.50715 -4.5061293 -4.4907765]]...]
INFO - root - 2017-12-08 01:19:00.289501: step 64810, loss = 21.75, batch loss = 21.67 (8.2 examples/sec; 0.975 sec/batch; 72h:30m:38s remains)
INFO - root - 2017-12-08 01:19:09.700612: step 64820, loss = 21.50, batch loss = 21.42 (8.9 examples/sec; 0.902 sec/batch; 67h:04m:31s remains)
INFO - root - 2017-12-08 01:19:18.854271: step 64830, loss = 21.52, batch loss = 21.44 (9.1 examples/sec; 0.883 sec/batch; 65h:40m:56s remains)
INFO - root - 2017-12-08 01:19:28.145543: step 64840, loss = 21.11, batch loss = 21.03 (8.8 examples/sec; 0.906 sec/batch; 67h:22m:14s remains)
INFO - root - 2017-12-08 01:19:37.533314: step 64850, loss = 21.17, batch loss = 21.09 (8.3 examples/sec; 0.962 sec/batch; 71h:29m:41s remains)
INFO - root - 2017-12-08 01:19:46.843926: step 64860, loss = 21.30, batch loss = 21.21 (8.0 examples/sec; 0.996 sec/batch; 74h:04m:08s remains)
INFO - root - 2017-12-08 01:19:56.148258: step 64870, loss = 21.31, batch loss = 21.23 (8.5 examples/sec; 0.945 sec/batch; 70h:16m:09s remains)
INFO - root - 2017-12-08 01:20:05.714975: step 64880, loss = 21.38, batch loss = 21.30 (7.7 examples/sec; 1.038 sec/batch; 77h:10m:50s remains)
INFO - root - 2017-12-08 01:20:15.115766: step 64890, loss = 21.46, batch loss = 21.37 (8.4 examples/sec; 0.958 sec/batch; 71h:11m:29s remains)
INFO - root - 2017-12-08 01:20:24.362173: step 64900, loss = 21.39, batch loss = 21.31 (8.3 examples/sec; 0.958 sec/batch; 71h:14m:35s remains)
2017-12-08 01:20:25.260775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4317622 -4.4208593 -4.3925939 -4.3732066 -4.3729959 -4.384213 -4.40245 -4.4217663 -4.4350052 -4.4388189 -4.4294162 -4.402029 -4.3476763 -4.26041 -4.1847067][-4.481792 -4.4856148 -4.4543238 -4.4088273 -4.3779454 -4.3802881 -4.4178271 -4.4694953 -4.5077009 -4.5129724 -4.4792285 -4.4223504 -4.35369 -4.2639551 -4.1809659][-4.5141048 -4.5222788 -4.4838924 -4.4125805 -4.3481321 -4.3345151 -4.3840971 -4.4661956 -4.5350733 -4.5601726 -4.5322137 -4.4684377 -4.3917074 -4.3047004 -4.2266278][-4.5222692 -4.5117092 -4.4517503 -4.351799 -4.2536883 -4.2163563 -4.2661786 -4.3691654 -4.4668193 -4.5222464 -4.5252671 -4.4840236 -4.4203053 -4.3530436 -4.301147][-4.4981446 -4.463635 -4.3823605 -4.2538929 -4.11961 -4.0494838 -4.0876122 -4.2043257 -4.3270569 -4.4109888 -4.4506574 -4.4452562 -4.4096255 -4.3742008 -4.3573418][-4.4584804 -4.4144316 -4.3206215 -4.1618247 -3.9772949 -3.852747 -3.8602083 -3.9851453 -4.1394777 -4.2598286 -4.3370852 -4.3671308 -4.3599467 -4.35204 -4.3623981][-4.4151216 -4.3830853 -4.2905083 -4.1082954 -3.8734336 -3.682972 -3.6369357 -3.7500026 -3.9319012 -4.0959611 -4.2110991 -4.2684522 -4.2777476 -4.2791562 -4.2980165][-4.3851824 -4.3886681 -4.3207278 -4.1453238 -3.8887992 -3.6435549 -3.5301623 -3.6050665 -3.7987666 -3.9998348 -4.1419158 -4.2108111 -4.2240138 -4.2194977 -4.2304821][-4.3856111 -4.4272909 -4.3970928 -4.2605433 -4.0270057 -3.7718306 -3.6140747 -3.636066 -3.807375 -4.018486 -4.1694036 -4.2362933 -4.2477617 -4.2335787 -4.2270608][-4.4190183 -4.4699564 -4.4661703 -4.3849435 -4.2157707 -4.0040541 -3.8415267 -3.8149264 -3.9369874 -4.1254811 -4.2666087 -4.3262162 -4.3377295 -4.3224587 -4.3034735][-4.463336 -4.4949913 -4.5020571 -4.4804788 -4.3992286 -4.2623286 -4.1253848 -4.0664949 -4.1331673 -4.2796717 -4.398469 -4.4477868 -4.4594564 -4.4483528 -4.4248829][-4.4826279 -4.4824157 -4.4902349 -4.5218177 -4.5229421 -4.4605169 -4.36395 -4.29892 -4.3267369 -4.4344296 -4.5338964 -4.577949 -4.58535 -4.56807 -4.5382357][-4.4622197 -4.4283562 -4.4362493 -4.5058126 -4.5622888 -4.5486565 -4.4850545 -4.4328084 -4.4499288 -4.5400071 -4.632525 -4.6747327 -4.6707344 -4.6369829 -4.6013327][-4.4134111 -4.358242 -4.3667665 -4.4550576 -4.5299954 -4.5309572 -4.483655 -4.4481535 -4.4738626 -4.5670891 -4.6612849 -4.7013412 -4.6854234 -4.6393728 -4.6069303][-4.3502274 -4.2846413 -4.2968388 -4.3929119 -4.4658952 -4.4594603 -4.4074521 -4.3728023 -4.4030986 -4.5011 -4.5964179 -4.6358466 -4.6236863 -4.5862737 -4.5684795]]...]
INFO - root - 2017-12-08 01:20:34.599202: step 64910, loss = 21.27, batch loss = 21.18 (9.2 examples/sec; 0.866 sec/batch; 64h:24m:25s remains)
INFO - root - 2017-12-08 01:20:44.049807: step 64920, loss = 21.63, batch loss = 21.54 (8.4 examples/sec; 0.952 sec/batch; 70h:44m:50s remains)
INFO - root - 2017-12-08 01:20:53.375664: step 64930, loss = 21.45, batch loss = 21.37 (7.9 examples/sec; 1.019 sec/batch; 75h:42m:07s remains)
INFO - root - 2017-12-08 01:21:02.607970: step 64940, loss = 21.52, batch loss = 21.44 (8.5 examples/sec; 0.942 sec/batch; 70h:01m:34s remains)
INFO - root - 2017-12-08 01:21:12.066113: step 64950, loss = 21.68, batch loss = 21.60 (9.2 examples/sec; 0.867 sec/batch; 64h:27m:27s remains)
INFO - root - 2017-12-08 01:21:21.420548: step 64960, loss = 21.65, batch loss = 21.57 (8.9 examples/sec; 0.896 sec/batch; 66h:35m:42s remains)
INFO - root - 2017-12-08 01:21:30.819994: step 64970, loss = 21.33, batch loss = 21.25 (8.5 examples/sec; 0.946 sec/batch; 70h:15m:53s remains)
INFO - root - 2017-12-08 01:21:40.253471: step 64980, loss = 21.03, batch loss = 20.95 (8.5 examples/sec; 0.943 sec/batch; 70h:03m:22s remains)
INFO - root - 2017-12-08 01:21:49.740555: step 64990, loss = 21.49, batch loss = 21.40 (8.0 examples/sec; 0.998 sec/batch; 74h:09m:37s remains)
INFO - root - 2017-12-08 01:21:59.210367: step 65000, loss = 21.79, batch loss = 21.71 (7.9 examples/sec; 1.012 sec/batch; 75h:11m:53s remains)
2017-12-08 01:22:00.131636: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2735953 -4.2464371 -4.23064 -4.2042789 -4.2231421 -4.28105 -4.30044 -4.2675133 -4.265655 -4.3261223 -4.3519211 -4.2764711 -4.1771321 -4.1686258 -4.2565608][-4.4417467 -4.4203796 -4.3965349 -4.3652215 -4.3766456 -4.4264765 -4.44584 -4.4274731 -4.4371958 -4.4726582 -4.4631763 -4.3811922 -4.2913461 -4.2883096 -4.3742013][-4.6561074 -4.6347876 -4.5983324 -4.5558071 -4.5484667 -4.5800037 -4.5972118 -4.5891461 -4.5976844 -4.6071672 -4.5813746 -4.5057926 -4.4159145 -4.3936 -4.4550538][-4.7824235 -4.7488637 -4.6938133 -4.6265616 -4.5792136 -4.5737581 -4.5769787 -4.5716758 -4.5881057 -4.6137624 -4.6283112 -4.5989442 -4.5249777 -4.4762821 -4.4918189][-4.7549114 -4.7097516 -4.6398888 -4.5365133 -4.43338 -4.3775239 -4.3545275 -4.3468585 -4.3844876 -4.4623947 -4.5645428 -4.6210442 -4.5931258 -4.5411577 -4.5160155][-4.6126137 -4.5651894 -4.4876113 -4.34721 -4.18885 -4.0807886 -4.0240583 -4.0130682 -4.0843039 -4.231317 -4.4269834 -4.5763979 -4.6163058 -4.5963717 -4.557384][-4.4574609 -4.4141841 -4.33574 -4.1716738 -3.9761581 -3.8235188 -3.7259917 -3.7000709 -3.7966135 -4.0008688 -4.263979 -4.4822149 -4.5850558 -4.6050758 -4.5649719][-4.3622565 -4.3339386 -4.2647285 -4.0966291 -3.8911228 -3.7227163 -3.5988655 -3.5448954 -3.6334758 -3.859596 -4.1519213 -4.4041562 -4.5457234 -4.5819559 -4.5253496][-4.3496132 -4.3457441 -4.2984662 -4.1487083 -3.9605069 -3.81399 -3.6999927 -3.6221619 -3.6646636 -3.8590517 -4.1372957 -4.3863535 -4.5329914 -4.56076 -4.4792337][-4.4022207 -4.4194107 -4.3929334 -4.2768884 -4.1369047 -4.0501194 -3.983242 -3.8979917 -3.8735218 -3.9839091 -4.1923594 -4.3911133 -4.508697 -4.5186639 -4.4264054][-4.4843869 -4.5064363 -4.4865656 -4.4089255 -4.3325529 -4.3159976 -4.3085303 -4.2443957 -4.175899 -4.1937122 -4.30126 -4.4164138 -4.482883 -4.4747391 -4.3905239][-4.5732875 -4.5876951 -4.5643926 -4.5142469 -4.4822154 -4.5056009 -4.5357757 -4.5084343 -4.4417639 -4.4074087 -4.4314494 -4.4693894 -4.4857497 -4.4641623 -4.4020858][-4.6627231 -4.6591487 -4.6288023 -4.5937829 -4.5816121 -4.6125708 -4.6551285 -4.6616392 -4.624083 -4.58095 -4.5581927 -4.5434537 -4.5254436 -4.499692 -4.4671426][-4.682044 -4.6664238 -4.636097 -4.6152644 -4.6108017 -4.6338887 -4.6750875 -4.7028656 -4.699729 -4.6763229 -4.6425042 -4.603303 -4.565907 -4.5418224 -4.5353312][-4.6332493 -4.6149082 -4.5866356 -4.5732508 -4.568377 -4.5816212 -4.616477 -4.6469917 -4.6615334 -4.6594796 -4.6337538 -4.593545 -4.5531392 -4.5366483 -4.5504985]]...]
INFO - root - 2017-12-08 01:22:09.499760: step 65010, loss = 21.58, batch loss = 21.50 (8.7 examples/sec; 0.920 sec/batch; 68h:22m:05s remains)
INFO - root - 2017-12-08 01:22:18.843853: step 65020, loss = 21.53, batch loss = 21.44 (9.5 examples/sec; 0.842 sec/batch; 62h:32m:19s remains)
INFO - root - 2017-12-08 01:22:28.242284: step 65030, loss = 20.98, batch loss = 20.90 (8.7 examples/sec; 0.918 sec/batch; 68h:14m:02s remains)
INFO - root - 2017-12-08 01:22:37.482615: step 65040, loss = 21.68, batch loss = 21.60 (8.7 examples/sec; 0.918 sec/batch; 68h:10m:26s remains)
INFO - root - 2017-12-08 01:22:46.868659: step 65050, loss = 21.27, batch loss = 21.19 (8.6 examples/sec; 0.929 sec/batch; 69h:00m:12s remains)
INFO - root - 2017-12-08 01:22:56.359741: step 65060, loss = 21.07, batch loss = 20.98 (8.4 examples/sec; 0.948 sec/batch; 70h:24m:30s remains)
INFO - root - 2017-12-08 01:23:05.713493: step 65070, loss = 21.18, batch loss = 21.10 (8.9 examples/sec; 0.897 sec/batch; 66h:38m:49s remains)
INFO - root - 2017-12-08 01:23:14.923949: step 65080, loss = 21.52, batch loss = 21.44 (8.3 examples/sec; 0.968 sec/batch; 71h:54m:15s remains)
INFO - root - 2017-12-08 01:23:24.266162: step 65090, loss = 21.25, batch loss = 21.16 (8.5 examples/sec; 0.938 sec/batch; 69h:38m:35s remains)
INFO - root - 2017-12-08 01:23:33.695277: step 65100, loss = 21.29, batch loss = 21.21 (8.6 examples/sec; 0.926 sec/batch; 68h:45m:08s remains)
2017-12-08 01:23:34.623899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3999276 -4.4471397 -4.5131054 -4.579689 -4.6198683 -4.6226859 -4.5954461 -4.5678244 -4.5568395 -4.5594225 -4.5665865 -4.5658722 -4.5634947 -4.5706768 -4.5913305][-4.4276533 -4.4987955 -4.5873981 -4.6615219 -4.6852379 -4.6620293 -4.6061091 -4.5568323 -4.54292 -4.5499582 -4.5598712 -4.5669646 -4.5812392 -4.61328 -4.6601539][-4.4395189 -4.5226297 -4.6140037 -4.6726365 -4.6627717 -4.6090918 -4.5283346 -4.4554887 -4.4405637 -4.4661713 -4.4901662 -4.5127816 -4.54895 -4.6007142 -4.6619692][-4.4320855 -4.5171881 -4.6027484 -4.6399112 -4.5912294 -4.5018535 -4.393858 -4.2896237 -4.2733417 -4.3346391 -4.3908172 -4.4383574 -4.4959154 -4.5449886 -4.5872478][-4.4203968 -4.5128713 -4.6010785 -4.6244545 -4.5332403 -4.3870945 -4.2307591 -4.0881062 -4.0766964 -4.1865859 -4.2730274 -4.333365 -4.3965406 -4.4205661 -4.4173818][-4.4275904 -4.5368848 -4.6368594 -4.6532068 -4.5208383 -4.2987809 -4.0705967 -3.8926528 -3.8992162 -4.0513778 -4.1543078 -4.2097392 -4.2597346 -4.2496219 -4.1841111][-4.4398918 -4.5694351 -4.6820617 -4.6922388 -4.5240664 -4.2247462 -3.9225192 -3.7203484 -3.7515416 -3.9355137 -4.0557594 -4.1094513 -4.1426692 -4.10156 -3.9790642][-4.4443321 -4.5847793 -4.6994548 -4.6936865 -4.4919186 -4.1308613 -3.7726433 -3.5656657 -3.6303477 -3.8504176 -4.0068688 -4.0775075 -4.1105905 -4.0564446 -3.8952918][-4.4439478 -4.5852113 -4.6900988 -4.6653156 -4.444778 -4.0615687 -3.6862943 -3.4939778 -3.593312 -3.8543689 -4.0516605 -4.14653 -4.1859565 -4.1326284 -3.9628172][-4.4506888 -4.5904031 -4.6870885 -4.6510291 -4.4308658 -4.0613751 -3.7092297 -3.5525119 -3.6888051 -3.9819667 -4.1987758 -4.2954636 -4.3285851 -4.2801595 -4.132647][-4.4622831 -4.59681 -4.6876788 -4.6529284 -4.4519186 -4.1228952 -3.8249116 -3.7178624 -3.881062 -4.174644 -4.3711228 -4.4399753 -4.4508963 -4.4118075 -4.3144927][-4.466589 -4.5893216 -4.6744895 -4.6550622 -4.5006995 -4.2447491 -4.0227232 -3.9586759 -4.1117859 -4.3578744 -4.5040321 -4.5375509 -4.5285926 -4.5026989 -4.4592028][-4.4599471 -4.5714326 -4.6561584 -4.6649032 -4.5793023 -4.420269 -4.281692 -4.2473712 -4.3622642 -4.5327973 -4.6203451 -4.6313133 -4.6184492 -4.6064839 -4.5971217][-4.4481063 -4.5485458 -4.6356196 -4.6742835 -4.6525769 -4.5795293 -4.507153 -4.4886546 -4.5558763 -4.6504984 -4.6923685 -4.6976023 -4.6944351 -4.6953392 -4.7026877][-4.4251003 -4.5041065 -4.5805306 -4.6279769 -4.6393967 -4.6206284 -4.5929947 -4.5822787 -4.6092024 -4.6471729 -4.6634135 -4.6705885 -4.6763949 -4.6821847 -4.690361]]...]
INFO - root - 2017-12-08 01:23:43.951735: step 65110, loss = 21.45, batch loss = 21.37 (8.1 examples/sec; 0.993 sec/batch; 73h:45m:30s remains)
INFO - root - 2017-12-08 01:23:53.395433: step 65120, loss = 21.54, batch loss = 21.45 (7.9 examples/sec; 1.012 sec/batch; 75h:07m:38s remains)
INFO - root - 2017-12-08 01:24:02.753450: step 65130, loss = 21.90, batch loss = 21.81 (8.9 examples/sec; 0.900 sec/batch; 66h:52m:08s remains)
INFO - root - 2017-12-08 01:24:12.019398: step 65140, loss = 21.33, batch loss = 21.24 (8.1 examples/sec; 0.985 sec/batch; 73h:10m:13s remains)
INFO - root - 2017-12-08 01:24:21.469995: step 65150, loss = 21.36, batch loss = 21.28 (8.4 examples/sec; 0.954 sec/batch; 70h:49m:52s remains)
INFO - root - 2017-12-08 01:24:30.932826: step 65160, loss = 21.21, batch loss = 21.13 (8.5 examples/sec; 0.945 sec/batch; 70h:08m:57s remains)
INFO - root - 2017-12-08 01:24:40.493463: step 65170, loss = 21.31, batch loss = 21.22 (8.6 examples/sec; 0.934 sec/batch; 69h:23m:01s remains)
INFO - root - 2017-12-08 01:24:49.886238: step 65180, loss = 21.34, batch loss = 21.25 (8.8 examples/sec; 0.904 sec/batch; 67h:07m:25s remains)
INFO - root - 2017-12-08 01:24:59.215803: step 65190, loss = 21.54, batch loss = 21.46 (8.6 examples/sec; 0.931 sec/batch; 69h:09m:07s remains)
INFO - root - 2017-12-08 01:25:08.575843: step 65200, loss = 21.15, batch loss = 21.07 (8.3 examples/sec; 0.958 sec/batch; 71h:09m:05s remains)
2017-12-08 01:25:09.500818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3504853 -4.326045 -4.3763604 -4.4708982 -4.5603914 -4.6083794 -4.61228 -4.5879784 -4.5458827 -4.5061293 -4.4957619 -4.5123787 -4.5162721 -4.4845085 -4.4280515][-4.2132053 -4.1801295 -4.2532592 -4.3892674 -4.5161457 -4.5819349 -4.5925775 -4.5830488 -4.5686283 -4.5628037 -4.5735803 -4.5864754 -4.57071 -4.51818 -4.4499731][-4.0246377 -3.9869361 -4.0825381 -4.2598124 -4.4231424 -4.5032787 -4.5118756 -4.5064244 -4.5168128 -4.5541334 -4.6044168 -4.6313992 -4.6066403 -4.5407038 -4.4652581][-3.8256388 -3.7654841 -3.8668528 -4.0757923 -4.2759609 -4.3776121 -4.3866024 -4.3777933 -4.3998361 -4.4709125 -4.5630388 -4.6184621 -4.6036625 -4.53982 -4.4659286][-3.6893353 -3.5967019 -3.681421 -3.8956571 -4.1146865 -4.2299724 -4.2367759 -4.2226233 -4.24544 -4.3306017 -4.453352 -4.5436854 -4.5582829 -4.5169 -4.4558954][-3.7176228 -3.6035933 -3.6529729 -3.831599 -4.0250988 -4.1175208 -4.1049838 -4.0815973 -4.1005578 -4.1888776 -4.33 -4.4479885 -4.49475 -4.4831743 -4.4407592][-3.9092221 -3.7878535 -3.7927763 -3.9031827 -4.0274754 -4.0592551 -4.0099416 -3.9717088 -3.9855614 -4.0766168 -4.2325916 -4.3702626 -4.4393687 -4.4488244 -4.4233651][-4.1651406 -4.0591331 -4.0285268 -4.0641832 -4.1050067 -4.0674114 -3.982758 -3.9314165 -3.9383001 -4.0320339 -4.1979985 -4.3447189 -4.417304 -4.4256725 -4.4059696][-4.3908176 -4.3213191 -4.2783036 -4.2591648 -4.2332649 -4.149508 -4.0488696 -3.995949 -3.9999111 -4.0893269 -4.2490687 -4.3874884 -4.4432211 -4.4279828 -4.3971276][-4.5233955 -4.504621 -4.47327 -4.42795 -4.3645296 -4.2690921 -4.1800313 -4.143888 -4.1571689 -4.2366419 -4.3704247 -4.4813437 -4.5069246 -4.4593196 -4.4055][-4.5594025 -4.5892673 -4.5814738 -4.5360904 -4.4649878 -4.3825884 -4.3202109 -4.3076453 -4.33223 -4.3965368 -4.4928527 -4.5698624 -4.5697937 -4.5005484 -4.426971][-4.5167646 -4.5763116 -4.5934005 -4.5673628 -4.5136781 -4.4600277 -4.427897 -4.4314361 -4.4567714 -4.4995985 -4.5585837 -4.60868 -4.6003785 -4.5318146 -4.4510984][-4.4403038 -4.50285 -4.5314136 -4.5261602 -4.4987144 -4.4743686 -4.4664083 -4.4770465 -4.4933057 -4.51471 -4.5476985 -4.5855322 -4.5848012 -4.531189 -4.4531212][-4.375176 -4.414854 -4.4359093 -4.439157 -4.4296112 -4.4216232 -4.4231305 -4.4324155 -4.4398084 -4.4506869 -4.4752822 -4.5141811 -4.5260649 -4.488111 -4.4162307][-4.3703589 -4.3803129 -4.3833404 -4.3815536 -4.3766112 -4.3731489 -4.3747206 -4.3801579 -4.3833733 -4.3915977 -4.4143052 -4.4532804 -4.4715629 -4.4449506 -4.3850703]]...]
INFO - root - 2017-12-08 01:25:18.770861: step 65210, loss = 21.03, batch loss = 20.95 (8.9 examples/sec; 0.900 sec/batch; 66h:47m:35s remains)
INFO - root - 2017-12-08 01:25:28.278532: step 65220, loss = 21.34, batch loss = 21.25 (8.5 examples/sec; 0.940 sec/batch; 69h:46m:25s remains)
INFO - root - 2017-12-08 01:25:37.599532: step 65230, loss = 21.60, batch loss = 21.52 (8.3 examples/sec; 0.967 sec/batch; 71h:49m:00s remains)
INFO - root - 2017-12-08 01:25:46.883777: step 65240, loss = 21.38, batch loss = 21.30 (8.1 examples/sec; 0.985 sec/batch; 73h:08m:25s remains)
INFO - root - 2017-12-08 01:25:56.299836: step 65250, loss = 21.61, batch loss = 21.53 (8.6 examples/sec; 0.931 sec/batch; 69h:05m:19s remains)
INFO - root - 2017-12-08 01:26:05.690422: step 65260, loss = 21.43, batch loss = 21.35 (8.7 examples/sec; 0.917 sec/batch; 68h:02m:12s remains)
INFO - root - 2017-12-08 01:26:15.029788: step 65270, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.954 sec/batch; 70h:46m:59s remains)
INFO - root - 2017-12-08 01:26:24.411566: step 65280, loss = 21.48, batch loss = 21.40 (8.3 examples/sec; 0.961 sec/batch; 71h:19m:42s remains)
INFO - root - 2017-12-08 01:26:33.689093: step 65290, loss = 20.97, batch loss = 20.89 (8.4 examples/sec; 0.952 sec/batch; 70h:41m:44s remains)
INFO - root - 2017-12-08 01:26:43.043054: step 65300, loss = 21.38, batch loss = 21.30 (8.4 examples/sec; 0.955 sec/batch; 70h:51m:24s remains)
2017-12-08 01:26:44.040727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4275746 -4.440865 -4.4484344 -4.4556475 -4.4668117 -4.4788857 -4.4928966 -4.5051322 -4.5093808 -4.5123663 -4.5254378 -4.5484982 -4.5678768 -4.566586 -4.5435205][-4.4609518 -4.4838467 -4.4963012 -4.506598 -4.513411 -4.514822 -4.5208569 -4.5300527 -4.5325928 -4.5375113 -4.5593982 -4.5984321 -4.6351137 -4.6402678 -4.6097083][-4.479188 -4.5160174 -4.5386271 -4.5548329 -4.5550117 -4.5364733 -4.5242872 -4.5268974 -4.5304556 -4.5429354 -4.5737095 -4.6195054 -4.6649847 -4.6708794 -4.6332493][-4.5016928 -4.5586047 -4.5969262 -4.6194768 -4.6052794 -4.5513048 -4.5040536 -4.4916797 -4.5000782 -4.5318913 -4.5790205 -4.6244693 -4.6652193 -4.6645026 -4.620255][-4.5303979 -4.6091819 -4.6621904 -4.6772947 -4.6238909 -4.5028539 -4.3976936 -4.3630757 -4.3887653 -4.4618292 -4.5419679 -4.5914936 -4.6225605 -4.6164646 -4.5739226][-4.5541816 -4.652473 -4.7127633 -4.696238 -4.5621982 -4.3339252 -4.1586704 -4.1255169 -4.2055955 -4.3387642 -4.449163 -4.500957 -4.5296764 -4.5331993 -4.5074592][-4.5582318 -4.6649523 -4.715795 -4.6377287 -4.3905511 -4.03957 -3.8139338 -3.8272271 -3.9927788 -4.1816921 -4.3018537 -4.3534374 -4.3966594 -4.4325609 -4.4417076][-4.5398207 -4.6541629 -4.689847 -4.5493412 -4.20566 -3.7684059 -3.5271883 -3.602529 -3.8485 -4.0806727 -4.20659 -4.2602687 -4.3212166 -4.386003 -4.4205728][-4.5338955 -4.6696568 -4.7125444 -4.5506406 -4.1774459 -3.7296267 -3.5041966 -3.6159039 -3.891289 -4.1318369 -4.2565846 -4.3081937 -4.3741779 -4.445858 -4.4785461][-4.5398855 -4.7007437 -4.7667465 -4.6263204 -4.2934389 -3.9072466 -3.7205076 -3.8330104 -4.0770221 -4.2766018 -4.3741088 -4.406364 -4.4623919 -4.5261784 -4.55052][-4.5332346 -4.7020226 -4.7886128 -4.6972857 -4.450768 -4.1695752 -4.0391903 -4.1309175 -4.3028541 -4.4215083 -4.4679785 -4.4752846 -4.5164933 -4.5685868 -4.587461][-4.516789 -4.6708455 -4.7681189 -4.7344828 -4.5869946 -4.415802 -4.339292 -4.3965592 -4.4813604 -4.5147653 -4.5184255 -4.5208845 -4.5621567 -4.6078019 -4.6154552][-4.4955883 -4.6164093 -4.7019286 -4.7017412 -4.6247497 -4.53154 -4.49059 -4.517437 -4.541079 -4.529861 -4.5239286 -4.5451779 -4.5974679 -4.6383281 -4.6314754][-4.4841986 -4.5690446 -4.6306162 -4.64034 -4.605814 -4.5629234 -4.5420523 -4.5441909 -4.533535 -4.5097365 -4.5090537 -4.5430365 -4.5950427 -4.6238704 -4.60573][-4.4716539 -4.52995 -4.5710773 -4.5806575 -4.5681548 -4.5512123 -4.5356216 -4.5171165 -4.4874272 -4.4608774 -4.4626722 -4.4923897 -4.5281863 -4.5425386 -4.5238614]]...]
INFO - root - 2017-12-08 01:26:53.497960: step 65310, loss = 21.89, batch loss = 21.81 (8.5 examples/sec; 0.945 sec/batch; 70h:06m:57s remains)
INFO - root - 2017-12-08 01:27:02.907873: step 65320, loss = 21.53, batch loss = 21.45 (8.1 examples/sec; 0.993 sec/batch; 73h:41m:29s remains)
INFO - root - 2017-12-08 01:27:12.351641: step 65330, loss = 21.60, batch loss = 21.51 (7.9 examples/sec; 1.017 sec/batch; 75h:30m:11s remains)
INFO - root - 2017-12-08 01:27:21.788618: step 65340, loss = 21.69, batch loss = 21.60 (9.7 examples/sec; 0.828 sec/batch; 61h:24m:42s remains)
INFO - root - 2017-12-08 01:27:31.002995: step 65350, loss = 21.48, batch loss = 21.40 (8.1 examples/sec; 0.992 sec/batch; 73h:36m:42s remains)
INFO - root - 2017-12-08 01:27:40.473269: step 65360, loss = 22.07, batch loss = 21.98 (8.5 examples/sec; 0.941 sec/batch; 69h:48m:36s remains)
INFO - root - 2017-12-08 01:27:49.851919: step 65370, loss = 21.79, batch loss = 21.71 (8.7 examples/sec; 0.918 sec/batch; 68h:08m:57s remains)
INFO - root - 2017-12-08 01:27:59.215697: step 65380, loss = 21.19, batch loss = 21.11 (8.2 examples/sec; 0.979 sec/batch; 72h:37m:57s remains)
INFO - root - 2017-12-08 01:28:08.713070: step 65390, loss = 20.94, batch loss = 20.86 (8.8 examples/sec; 0.914 sec/batch; 67h:49m:51s remains)
INFO - root - 2017-12-08 01:28:18.319874: step 65400, loss = 21.53, batch loss = 21.44 (8.3 examples/sec; 0.963 sec/batch; 71h:26m:59s remains)
2017-12-08 01:28:19.412677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3512726 -4.3379531 -4.3088012 -4.2705374 -4.2540183 -4.2711248 -4.2850389 -4.2637281 -4.2129388 -4.1735091 -4.1638427 -4.2128229 -4.3062344 -4.3665404 -4.35873][-4.3835435 -4.342525 -4.3145967 -4.2984424 -4.3038568 -4.329577 -4.329361 -4.2821784 -4.2184987 -4.1812057 -4.1833844 -4.2488232 -4.3461761 -4.3982325 -4.37333][-4.4338436 -4.3807468 -4.356391 -4.3491721 -4.3531179 -4.3620939 -4.3335919 -4.2648458 -4.2019219 -4.1802011 -4.2027526 -4.2788825 -4.3648119 -4.3974996 -4.3582306][-4.4922895 -4.4403324 -4.4164119 -4.3985758 -4.3706274 -4.3356066 -4.2718587 -4.19652 -4.1561828 -4.16838 -4.2271852 -4.3211241 -4.3930621 -4.3986273 -4.335216][-4.5249023 -4.4796619 -4.4565954 -4.4240174 -4.3558326 -4.2722178 -4.1796241 -4.10629 -4.0883551 -4.1251192 -4.208313 -4.3131142 -4.3796387 -4.3781056 -4.3113079][-4.5290513 -4.4808679 -4.450491 -4.3992524 -4.2913685 -4.16172 -4.0472474 -3.9831359 -3.9908276 -4.0492506 -4.1425333 -4.2428885 -4.3055458 -4.3135667 -4.2692709][-4.4968181 -4.4318061 -4.3906145 -4.3301525 -4.2008948 -4.043705 -3.9183021 -3.8644094 -3.8970442 -3.9743013 -4.0718975 -4.1635008 -4.2252121 -4.24668 -4.2260857][-4.4354663 -4.3598523 -4.3277488 -4.2851086 -4.1646895 -4.0080533 -3.8804817 -3.8225465 -3.8517265 -3.9217749 -4.0089993 -4.0915146 -4.15855 -4.1979942 -4.1994753][-4.3769188 -4.3189049 -4.3220053 -4.3148618 -4.2173295 -4.075016 -3.9520724 -3.8855019 -3.8973098 -3.9440372 -4.0073605 -4.0730753 -4.1337409 -4.1785312 -4.1933289][-4.35014 -4.3280125 -4.3693237 -4.39482 -4.3256626 -4.2085533 -4.1009011 -4.032774 -4.0268993 -4.0489044 -4.0856676 -4.129941 -4.1754756 -4.2153349 -4.2300816][-4.3781805 -4.3838577 -4.4424553 -4.4829574 -4.4395132 -4.3528767 -4.2673044 -4.2048922 -4.1865668 -4.190691 -4.2100372 -4.2380257 -4.2716031 -4.3028479 -4.3030748][-4.4272766 -4.4380126 -4.485198 -4.5179124 -4.4925132 -4.4379697 -4.3844466 -4.3462677 -4.3363566 -4.338903 -4.3473449 -4.3595943 -4.3809071 -4.3979812 -4.3741574][-4.4380107 -4.4479232 -4.4753256 -4.4933739 -4.480648 -4.4585905 -4.4447379 -4.4412346 -4.4483247 -4.4517283 -4.4481463 -4.4450569 -4.4563875 -4.4633965 -4.4266133][-4.421494 -4.4388585 -4.4556551 -4.4616218 -4.453896 -4.4524717 -4.4669685 -4.4874892 -4.5044265 -4.5042577 -4.4878826 -4.4734569 -4.4787526 -4.4847589 -4.4535537][-4.3988357 -4.4168639 -4.4220657 -4.4168606 -4.4096756 -4.4179711 -4.4445148 -4.4729662 -4.49107 -4.4896035 -4.4718218 -4.4577937 -4.4643011 -4.4754124 -4.4599266]]...]
INFO - root - 2017-12-08 01:28:28.837772: step 65410, loss = 21.69, batch loss = 21.61 (8.8 examples/sec; 0.905 sec/batch; 67h:07m:53s remains)
INFO - root - 2017-12-08 01:28:38.131902: step 65420, loss = 21.51, batch loss = 21.43 (8.4 examples/sec; 0.956 sec/batch; 70h:54m:19s remains)
INFO - root - 2017-12-08 01:28:47.730199: step 65430, loss = 21.09, batch loss = 21.01 (8.4 examples/sec; 0.947 sec/batch; 70h:15m:35s remains)
INFO - root - 2017-12-08 01:28:57.145649: step 65440, loss = 21.74, batch loss = 21.66 (7.8 examples/sec; 1.020 sec/batch; 75h:38m:58s remains)
INFO - root - 2017-12-08 01:29:06.496936: step 65450, loss = 21.78, batch loss = 21.70 (8.0 examples/sec; 0.995 sec/batch; 73h:47m:56s remains)
INFO - root - 2017-12-08 01:29:15.947957: step 65460, loss = 21.20, batch loss = 21.12 (7.9 examples/sec; 1.017 sec/batch; 75h:28m:01s remains)
INFO - root - 2017-12-08 01:29:25.370603: step 65470, loss = 21.21, batch loss = 21.12 (8.4 examples/sec; 0.957 sec/batch; 70h:58m:09s remains)
INFO - root - 2017-12-08 01:29:34.904008: step 65480, loss = 21.00, batch loss = 20.92 (8.5 examples/sec; 0.942 sec/batch; 69h:53m:54s remains)
INFO - root - 2017-12-08 01:29:44.368174: step 65490, loss = 21.18, batch loss = 21.10 (8.2 examples/sec; 0.973 sec/batch; 72h:08m:19s remains)
INFO - root - 2017-12-08 01:29:53.684644: step 65500, loss = 21.54, batch loss = 21.46 (8.3 examples/sec; 0.968 sec/batch; 71h:47m:00s remains)
2017-12-08 01:29:54.604781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2587976 -4.2992854 -4.3447824 -4.3445311 -4.3146229 -4.2892685 -4.2881851 -4.3022442 -4.2797709 -4.2265577 -4.1687775 -4.1375265 -4.173192 -4.2528062 -4.3185797][-4.2687798 -4.310667 -4.3467779 -4.3275847 -4.2797594 -4.2450733 -4.2436562 -4.2510395 -4.2186279 -4.1526985 -4.0819626 -4.0472317 -4.09255 -4.18173 -4.2452121][-4.2813187 -4.3268757 -4.3556437 -4.3113418 -4.2341514 -4.1813664 -4.1829224 -4.2085185 -4.2006679 -4.1552739 -4.095149 -4.0619478 -4.1004281 -4.178463 -4.2299447][-4.2943821 -4.3426037 -4.3584704 -4.2826633 -4.1705828 -4.0976043 -4.11092 -4.1805186 -4.2290111 -4.2298608 -4.1967273 -4.1667085 -4.1830025 -4.2315226 -4.2653546][-4.308403 -4.3523417 -4.3459916 -4.2345843 -4.0830731 -3.982729 -3.9985647 -4.1142812 -4.2322497 -4.29246 -4.2960882 -4.2728481 -4.2674217 -4.2888737 -4.3121762][-4.3214245 -4.3612871 -4.3349371 -4.1952338 -4.0080857 -3.8673244 -3.8525503 -3.9787364 -4.1391172 -4.2428269 -4.278347 -4.2688069 -4.2630887 -4.2849388 -4.3174462][-4.3386645 -4.3888907 -4.3700657 -4.2341642 -4.0366158 -3.8604989 -3.7908437 -3.8790779 -4.0319548 -4.14099 -4.1807942 -4.1718497 -4.1719174 -4.210721 -4.2616048][-4.3654737 -4.4415517 -4.4588032 -4.3579412 -4.1799092 -3.9898636 -3.8678598 -3.8990207 -4.016943 -4.1076207 -4.1283793 -4.0953512 -4.0815225 -4.1226058 -4.1778131][-4.3914123 -4.4969654 -4.5548258 -4.4962373 -4.35139 -4.1644549 -4.0069442 -3.9886322 -4.07259 -4.150497 -4.1588492 -4.10422 -4.0686035 -4.0958266 -4.1412048][-4.406539 -4.5297 -4.6153531 -4.5899305 -4.4790463 -4.3104854 -4.1396661 -4.08527 -4.1366849 -4.2041116 -4.2094197 -4.1506639 -4.1117568 -4.126267 -4.1537986][-4.4037781 -4.5274649 -4.62548 -4.6251664 -4.5495973 -4.4147525 -4.25746 -4.1818142 -4.1985092 -4.2447124 -4.2420554 -4.1910367 -4.1690631 -4.1804266 -4.18853][-4.377955 -4.489234 -4.5905271 -4.6141987 -4.5781364 -4.4910946 -4.3677931 -4.2843962 -4.2637329 -4.2776322 -4.2647953 -4.2315912 -4.2364473 -4.2547641 -4.2551894][-4.3271017 -4.410955 -4.5037956 -4.5493646 -4.5573683 -4.5298162 -4.4630346 -4.3971887 -4.3510389 -4.327848 -4.3000388 -4.2820578 -4.3072376 -4.330585 -4.3281655][-4.2641 -4.3065791 -4.37407 -4.4259286 -4.469286 -4.5000691 -4.5021377 -4.4824858 -4.4394932 -4.393085 -4.3522921 -4.3465796 -4.3859167 -4.4124336 -4.4054532][-4.2183447 -4.2257652 -4.2637315 -4.3028107 -4.3518715 -4.4055371 -4.4484258 -4.4753609 -4.4645786 -4.4332466 -4.4049363 -4.4188852 -4.4748955 -4.5054779 -4.4861035]]...]
INFO - root - 2017-12-08 01:30:03.989799: step 65510, loss = 21.46, batch loss = 21.37 (8.7 examples/sec; 0.915 sec/batch; 67h:51m:33s remains)
INFO - root - 2017-12-08 01:30:13.432941: step 65520, loss = 21.45, batch loss = 21.37 (8.2 examples/sec; 0.981 sec/batch; 72h:46m:49s remains)
INFO - root - 2017-12-08 01:30:22.884413: step 65530, loss = 22.10, batch loss = 22.02 (8.7 examples/sec; 0.919 sec/batch; 68h:10m:26s remains)
INFO - root - 2017-12-08 01:30:32.191169: step 65540, loss = 21.32, batch loss = 21.24 (9.5 examples/sec; 0.841 sec/batch; 62h:21m:06s remains)
INFO - root - 2017-12-08 01:30:41.596998: step 65550, loss = 21.89, batch loss = 21.81 (8.9 examples/sec; 0.902 sec/batch; 66h:53m:45s remains)
INFO - root - 2017-12-08 01:30:51.014962: step 65560, loss = 21.63, batch loss = 21.55 (8.2 examples/sec; 0.977 sec/batch; 72h:27m:24s remains)
INFO - root - 2017-12-08 01:31:00.449689: step 65570, loss = 20.99, batch loss = 20.91 (8.1 examples/sec; 0.985 sec/batch; 73h:01m:35s remains)
INFO - root - 2017-12-08 01:31:09.977102: step 65580, loss = 21.26, batch loss = 21.18 (7.9 examples/sec; 1.010 sec/batch; 74h:51m:39s remains)
INFO - root - 2017-12-08 01:31:19.100939: step 65590, loss = 21.45, batch loss = 21.37 (8.7 examples/sec; 0.920 sec/batch; 68h:11m:25s remains)
INFO - root - 2017-12-08 01:31:28.523777: step 65600, loss = 21.42, batch loss = 21.34 (8.5 examples/sec; 0.945 sec/batch; 70h:03m:00s remains)
2017-12-08 01:31:29.428180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2812681 -4.2461538 -4.1897154 -4.1216908 -4.0501919 -3.9779785 -3.9055874 -3.8611026 -3.8853264 -3.99333 -4.1641107 -4.3273854 -4.4239116 -4.4385138 -4.4077525][-4.2767138 -4.2210855 -4.1669784 -4.1280293 -4.1071577 -4.086925 -4.0385351 -3.9789495 -3.9705989 -4.0540714 -4.2127247 -4.375196 -4.476666 -4.4913354 -4.4517279][-4.2683291 -4.1984649 -4.145793 -4.1237507 -4.13594 -4.1577773 -4.1431684 -4.095643 -4.0810456 -4.1482663 -4.2869592 -4.4376187 -4.5348964 -4.545783 -4.495873][-4.2658277 -4.1978612 -4.1524057 -4.1367531 -4.1547756 -4.1867948 -4.1892185 -4.15336 -4.1392627 -4.1973195 -4.3216538 -4.4616122 -4.5552063 -4.565331 -4.5111437][-4.2740979 -4.2357769 -4.2084632 -4.187459 -4.1788325 -4.1817641 -4.1633778 -4.1098876 -4.0773034 -4.1247888 -4.2518373 -4.4021 -4.5108843 -4.5389771 -4.4983354][-4.29003 -4.2976418 -4.29318 -4.257442 -4.2041926 -4.1540675 -4.088788 -3.9962206 -3.934443 -3.9720318 -4.1061573 -4.2717557 -4.4081283 -4.4746456 -4.4691095][-4.3044505 -4.3514771 -4.36396 -4.3157606 -4.2261858 -4.1290708 -4.0207586 -3.8949964 -3.8143299 -3.8459558 -3.9789152 -4.1461492 -4.30022 -4.4052167 -4.4381981][-4.3033261 -4.3716226 -4.3913369 -4.3395023 -4.2401724 -4.1259155 -4.0045791 -3.8812597 -3.8095477 -3.8338013 -3.9442298 -4.0941472 -4.251204 -4.3798885 -4.4377537][-4.2952228 -4.3627543 -4.377212 -4.3266926 -4.2390127 -4.1424961 -4.0468554 -3.9605398 -3.9123094 -3.9180136 -3.9851153 -4.1030521 -4.2562437 -4.4024739 -4.479785][-4.2982645 -4.3495908 -4.3486342 -4.2962146 -4.2255321 -4.1600742 -4.1058483 -4.069284 -4.0469823 -4.0315857 -4.0535388 -4.1362395 -4.2768955 -4.4325838 -4.5291433][-4.3234859 -4.3537869 -4.3317909 -4.2718344 -4.21272 -4.1702957 -4.151741 -4.1629229 -4.1744523 -4.1599517 -4.1545353 -4.1978631 -4.3076754 -4.4538341 -4.5584254][-4.3649716 -4.37583 -4.3335261 -4.265831 -4.2102237 -4.1790366 -4.183569 -4.2287116 -4.2733812 -4.2747855 -4.2521162 -4.2459369 -4.3019643 -4.4153628 -4.517642][-4.4002318 -4.3984213 -4.3490171 -4.2852116 -4.2349725 -4.2073374 -4.2188373 -4.2740254 -4.3280554 -4.3266473 -4.2640243 -4.1893396 -4.182622 -4.2614803 -4.3684912][-4.41098 -4.4084005 -4.369791 -4.3239989 -4.2857385 -4.2602596 -4.2676749 -4.3099747 -4.3404293 -4.302618 -4.1808958 -4.0415339 -3.990124 -4.0501318 -4.1673956][-4.4001617 -4.4023762 -4.38408 -4.3628774 -4.3396354 -4.3173051 -4.3181167 -4.336072 -4.3260827 -4.2400093 -4.0694885 -3.8950286 -3.8244915 -3.8763707 -4.0030251]]...]
INFO - root - 2017-12-08 01:31:38.747614: step 65610, loss = 21.28, batch loss = 21.20 (9.3 examples/sec; 0.865 sec/batch; 64h:06m:07s remains)
INFO - root - 2017-12-08 01:31:48.180826: step 65620, loss = 20.95, batch loss = 20.87 (8.5 examples/sec; 0.943 sec/batch; 69h:55m:26s remains)
INFO - root - 2017-12-08 01:31:57.716271: step 65630, loss = 21.54, batch loss = 21.46 (7.9 examples/sec; 1.006 sec/batch; 74h:36m:14s remains)
INFO - root - 2017-12-08 01:32:07.213004: step 65640, loss = 21.66, batch loss = 21.57 (8.0 examples/sec; 0.997 sec/batch; 73h:55m:25s remains)
INFO - root - 2017-12-08 01:32:16.460967: step 65650, loss = 21.72, batch loss = 21.63 (9.0 examples/sec; 0.891 sec/batch; 66h:03m:37s remains)
INFO - root - 2017-12-08 01:32:25.897256: step 65660, loss = 21.29, batch loss = 21.21 (8.6 examples/sec; 0.931 sec/batch; 68h:58m:14s remains)
INFO - root - 2017-12-08 01:32:35.278439: step 65670, loss = 21.72, batch loss = 21.64 (8.7 examples/sec; 0.917 sec/batch; 67h:58m:58s remains)
INFO - root - 2017-12-08 01:32:44.626555: step 65680, loss = 21.05, batch loss = 20.97 (8.7 examples/sec; 0.920 sec/batch; 68h:10m:58s remains)
INFO - root - 2017-12-08 01:32:54.059354: step 65690, loss = 21.51, batch loss = 21.43 (8.8 examples/sec; 0.908 sec/batch; 67h:15m:45s remains)
INFO - root - 2017-12-08 01:33:03.455474: step 65700, loss = 21.23, batch loss = 21.15 (8.7 examples/sec; 0.915 sec/batch; 67h:50m:42s remains)
2017-12-08 01:33:04.444851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2525539 -4.2536807 -4.2465553 -4.2418613 -4.2583752 -4.2969642 -4.3491731 -4.3961182 -4.4103746 -4.3915577 -4.3572426 -4.3200412 -4.3216038 -4.3652797 -4.4261994][-4.2685885 -4.2668743 -4.2615376 -4.2605562 -4.2805595 -4.3155847 -4.3613343 -4.39934 -4.3979611 -4.3670506 -4.3220711 -4.2760973 -4.2745614 -4.3207164 -4.3845372][-4.2651219 -4.2630277 -4.2643552 -4.2679157 -4.2820354 -4.295578 -4.3154192 -4.3397489 -4.3296165 -4.2938457 -4.2456169 -4.1975584 -4.2020683 -4.2643614 -4.3394003][-4.2468786 -4.2508359 -4.2626491 -4.26878 -4.2655697 -4.2378817 -4.2149653 -4.2294722 -4.2331691 -4.2152634 -4.1762466 -4.1282196 -4.1359887 -4.212461 -4.3000445][-4.2247758 -4.2464828 -4.272192 -4.2760053 -4.2491827 -4.180038 -4.1176004 -4.1283255 -4.1620512 -4.1826072 -4.1729856 -4.1336818 -4.1368518 -4.2084265 -4.2936468][-4.1876183 -4.2378836 -4.2843804 -4.2864127 -4.2413793 -4.1467447 -4.062705 -4.074883 -4.1364775 -4.1901116 -4.2107959 -4.1856337 -4.184875 -4.2451687 -4.3247948][-4.1587677 -4.2348571 -4.3008437 -4.3017106 -4.2447209 -4.13945 -4.0495629 -4.0650325 -4.1459389 -4.2198167 -4.2629614 -4.2543344 -4.2526097 -4.3025646 -4.3762178][-4.1948814 -4.2842326 -4.3488727 -4.3363466 -4.2671356 -4.161356 -4.07865 -4.0993266 -4.1896281 -4.2694058 -4.3173032 -4.3138881 -4.3040633 -4.334218 -4.3966732][-4.3050928 -4.3872352 -4.4219594 -4.3844 -4.3143568 -4.22679 -4.1565042 -4.1679263 -4.2424974 -4.3070116 -4.3379669 -4.3240285 -4.3000207 -4.308475 -4.3586636][-4.4112349 -4.4728036 -4.4795012 -4.4335089 -4.3776131 -4.3133168 -4.2522922 -4.2445207 -4.2814903 -4.3130569 -4.3178015 -4.2883463 -4.2560887 -4.2542796 -4.2999711][-4.4373522 -4.47902 -4.4849939 -4.4619775 -4.4319611 -4.3875656 -4.3357205 -4.3096766 -4.3014035 -4.2927041 -4.2716413 -4.2337093 -4.209269 -4.2149553 -4.2655959][-4.4159594 -4.4307427 -4.4421606 -4.44999 -4.4433889 -4.4109159 -4.3696694 -4.3388977 -4.2990704 -4.25556 -4.2157369 -4.1811204 -4.1810246 -4.2082429 -4.2665911][-4.40422 -4.3910737 -4.3884368 -4.4017935 -4.4049268 -4.3821521 -4.3546367 -4.3323741 -4.277194 -4.2056646 -4.1533732 -4.1360126 -4.1731124 -4.2298055 -4.2947049][-4.4133077 -4.3778491 -4.3476973 -4.3430767 -4.3461132 -4.3349857 -4.3202748 -4.3060856 -4.2476959 -4.161727 -4.1065292 -4.1141315 -4.1918521 -4.2812982 -4.3503623][-4.4166365 -4.3734832 -4.3254633 -4.302207 -4.2995481 -4.2973042 -4.2853169 -4.2680154 -4.2180138 -4.1474419 -4.1159539 -4.1502814 -4.2503347 -4.3548117 -4.4168129]]...]
INFO - root - 2017-12-08 01:33:13.884917: step 65710, loss = 21.53, batch loss = 21.44 (8.3 examples/sec; 0.963 sec/batch; 71h:21m:23s remains)
INFO - root - 2017-12-08 01:33:23.395151: step 65720, loss = 21.47, batch loss = 21.39 (8.3 examples/sec; 0.967 sec/batch; 71h:39m:39s remains)
INFO - root - 2017-12-08 01:33:32.836299: step 65730, loss = 21.64, batch loss = 21.56 (9.0 examples/sec; 0.891 sec/batch; 65h:59m:20s remains)
INFO - root - 2017-12-08 01:33:42.183485: step 65740, loss = 21.18, batch loss = 21.10 (8.6 examples/sec; 0.929 sec/batch; 68h:52m:27s remains)
INFO - root - 2017-12-08 01:33:51.740756: step 65750, loss = 21.40, batch loss = 21.32 (8.4 examples/sec; 0.950 sec/batch; 70h:24m:26s remains)
INFO - root - 2017-12-08 01:34:01.092946: step 65760, loss = 21.41, batch loss = 21.33 (8.2 examples/sec; 0.973 sec/batch; 72h:05m:06s remains)
INFO - root - 2017-12-08 01:34:10.436007: step 65770, loss = 21.41, batch loss = 21.33 (8.2 examples/sec; 0.970 sec/batch; 71h:54m:18s remains)
INFO - root - 2017-12-08 01:34:19.880770: step 65780, loss = 21.43, batch loss = 21.35 (8.4 examples/sec; 0.953 sec/batch; 70h:36m:02s remains)
INFO - root - 2017-12-08 01:34:29.090953: step 65790, loss = 21.57, batch loss = 21.49 (8.2 examples/sec; 0.977 sec/batch; 72h:24m:15s remains)
INFO - root - 2017-12-08 01:34:38.545252: step 65800, loss = 21.25, batch loss = 21.17 (8.4 examples/sec; 0.956 sec/batch; 70h:47m:15s remains)
2017-12-08 01:34:39.446383: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2725792 -4.2661757 -4.264658 -4.2622962 -4.2637677 -4.2711463 -4.2905073 -4.3229008 -4.3586941 -4.3832493 -4.3933811 -4.3961682 -4.4028664 -4.4172788 -4.4284825][-4.3000231 -4.2943339 -4.2923007 -4.2910628 -4.2968445 -4.3140864 -4.347702 -4.3938594 -4.4390874 -4.4627209 -4.4651518 -4.4620652 -4.4679379 -4.4866939 -4.5050793][-4.3472471 -4.3436613 -4.3381853 -4.3335176 -4.3414459 -4.3670363 -4.4101377 -4.4623919 -4.5042844 -4.5136223 -4.5035906 -4.4999914 -4.50895 -4.5268369 -4.5395212][-4.4108915 -4.4046235 -4.3870521 -4.3680062 -4.3664026 -4.3906369 -4.4329343 -4.476644 -4.496953 -4.4766703 -4.4536471 -4.4630075 -4.4937634 -4.5211267 -4.5224519][-4.4696112 -4.4651709 -4.4369445 -4.3954206 -4.367065 -4.3700714 -4.3916264 -4.4058027 -4.386138 -4.3306093 -4.2986536 -4.3312354 -4.4059963 -4.4672327 -4.4682984][-4.4834032 -4.4808669 -4.44135 -4.3658042 -4.2894859 -4.2506318 -4.2431812 -4.2276378 -4.174262 -4.0999446 -4.0666108 -4.1153011 -4.2343063 -4.3516526 -4.3786616][-4.44513 -4.4494171 -4.4028869 -4.28534 -4.1431594 -4.0427337 -4.0019326 -3.9702811 -3.9109859 -3.8514125 -3.8345068 -3.8888531 -4.032618 -4.2014027 -4.2673278][-4.3965907 -4.4237881 -4.3840961 -4.2313609 -4.0228024 -3.8567779 -3.7876253 -3.7657678 -3.7422252 -3.7413771 -3.7681878 -3.8186088 -3.9471288 -4.1197867 -4.1979609][-4.3518004 -4.3945484 -4.354969 -4.1770191 -3.92555 -3.7138267 -3.6321557 -3.6468325 -3.6971667 -3.7934403 -3.8997428 -3.9607668 -4.0453215 -4.1690726 -4.2188077][-4.3350611 -4.376523 -4.3310723 -4.1501474 -3.8952692 -3.6719739 -3.5887628 -3.6364708 -3.7523468 -3.9284906 -4.1065621 -4.1918254 -4.23838 -4.297936 -4.3057065][-4.3623257 -4.3963966 -4.358685 -4.2164435 -4.0113816 -3.8216243 -3.7504213 -3.8112063 -3.9458692 -4.1325583 -4.3211555 -4.4131727 -4.4338994 -4.4420953 -4.4185152][-4.394073 -4.4210491 -4.4036188 -4.3229828 -4.1996074 -4.080111 -4.0395374 -4.0980597 -4.2079611 -4.3432684 -4.4836526 -4.5582738 -4.567019 -4.5494261 -4.51372][-4.4002824 -4.4224544 -4.4270787 -4.401526 -4.3500824 -4.2985711 -4.2923236 -4.3439593 -4.4109669 -4.4698052 -4.5327787 -4.5745363 -4.582171 -4.5681372 -4.5411315][-4.3648639 -4.3855958 -4.4036403 -4.408567 -4.3977828 -4.3868804 -4.3995275 -4.4406848 -4.4738431 -4.4796476 -4.4862728 -4.5037036 -4.5174408 -4.520257 -4.5112982][-4.2929988 -4.3127337 -4.3365197 -4.352334 -4.3563185 -4.3579144 -4.3691897 -4.3928814 -4.4031296 -4.3867378 -4.3716183 -4.3804488 -4.4017234 -4.4215808 -4.4311919]]...]
INFO - root - 2017-12-08 01:34:48.909523: step 65810, loss = 21.31, batch loss = 21.22 (9.3 examples/sec; 0.856 sec/batch; 63h:25m:54s remains)
INFO - root - 2017-12-08 01:34:58.213874: step 65820, loss = 21.11, batch loss = 21.02 (9.2 examples/sec; 0.865 sec/batch; 64h:05m:06s remains)
INFO - root - 2017-12-08 01:35:07.618844: step 65830, loss = 21.44, batch loss = 21.36 (8.3 examples/sec; 0.965 sec/batch; 71h:30m:28s remains)
INFO - root - 2017-12-08 01:35:17.080387: step 65840, loss = 21.31, batch loss = 21.23 (8.4 examples/sec; 0.952 sec/batch; 70h:30m:30s remains)
INFO - root - 2017-12-08 01:35:26.598766: step 65850, loss = 21.42, batch loss = 21.33 (8.5 examples/sec; 0.937 sec/batch; 69h:22m:47s remains)
INFO - root - 2017-12-08 01:35:35.944498: step 65860, loss = 21.57, batch loss = 21.49 (8.5 examples/sec; 0.943 sec/batch; 69h:50m:58s remains)
INFO - root - 2017-12-08 01:35:45.329136: step 65870, loss = 21.31, batch loss = 21.23 (8.4 examples/sec; 0.955 sec/batch; 70h:43m:24s remains)
INFO - root - 2017-12-08 01:35:54.930475: step 65880, loss = 22.12, batch loss = 22.03 (8.5 examples/sec; 0.940 sec/batch; 69h:37m:28s remains)
INFO - root - 2017-12-08 01:36:04.294707: step 65890, loss = 21.32, batch loss = 21.24 (8.7 examples/sec; 0.921 sec/batch; 68h:10m:36s remains)
INFO - root - 2017-12-08 01:36:13.665738: step 65900, loss = 21.36, batch loss = 21.28 (8.6 examples/sec; 0.932 sec/batch; 68h:59m:56s remains)
2017-12-08 01:36:14.718446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4188538 -4.4366527 -4.4238343 -4.4076695 -4.3753824 -4.3250551 -4.2968388 -4.3130474 -4.3625412 -4.398109 -4.3763657 -4.3141317 -4.259376 -4.2381053 -4.223146][-4.4396229 -4.4695721 -4.4627 -4.4517961 -4.4239168 -4.3736253 -4.322938 -4.3107758 -4.3497825 -4.3892727 -4.3957696 -4.3704948 -4.3297887 -4.301857 -4.2770791][-4.468142 -4.5014343 -4.4986773 -4.49022 -4.4538722 -4.38622 -4.2994561 -4.242569 -4.2593074 -4.3027658 -4.3368163 -4.3476048 -4.3286867 -4.3059216 -4.2806435][-4.4878774 -4.52082 -4.5246968 -4.5125933 -4.4547811 -4.3611374 -4.2401528 -4.1437798 -4.1359386 -4.1740932 -4.2148538 -4.2439342 -4.2490568 -4.2465863 -4.2356286][-4.4751515 -4.4952021 -4.49431 -4.4646025 -4.3785939 -4.2692876 -4.1407056 -4.0391049 -4.0319834 -4.0698891 -4.1045108 -4.1371703 -4.1616988 -4.1808577 -4.1825914][-4.4658036 -4.4567118 -4.4279408 -4.3599272 -4.2427235 -4.1264019 -4.0147691 -3.9512026 -3.9851203 -4.0419745 -4.0689964 -4.0923748 -4.1325622 -4.1804972 -4.206706][-4.4802504 -4.4465089 -4.386548 -4.2791433 -4.1361494 -4.0119972 -3.9239337 -3.9215789 -4.01692 -4.10065 -4.1226983 -4.1330252 -4.1818671 -4.2531443 -4.3029184][-4.4887729 -4.4412374 -4.3557611 -4.2230921 -4.0776949 -3.9620359 -3.901829 -3.958554 -4.1081309 -4.21889 -4.2461209 -4.2440262 -4.2808366 -4.338367 -4.3743544][-4.4732037 -4.4122381 -4.3116097 -4.1788745 -4.0579615 -3.9688191 -3.9392705 -4.0355797 -4.2092624 -4.3316689 -4.369956 -4.3620205 -4.369029 -4.383709 -4.3768125][-4.4135752 -4.3432946 -4.2512817 -4.1516614 -4.070437 -4.0065827 -3.9948571 -4.1044531 -4.2692943 -4.3796048 -4.4209256 -4.4084125 -4.3863254 -4.360805 -4.3149557][-4.3414149 -4.2738481 -4.2069688 -4.1478 -4.0957232 -4.0408831 -4.0315614 -4.1367164 -4.2768574 -4.36463 -4.4004393 -4.3810344 -4.3374267 -4.2903147 -4.2344103][-4.3240495 -4.2784753 -4.2358623 -4.1979828 -4.1531754 -4.0960212 -4.081202 -4.1667519 -4.2735925 -4.3367805 -4.3602867 -4.3319392 -4.2815623 -4.2445345 -4.2135286][-4.3788705 -4.3642278 -4.3369837 -4.3003159 -4.2521005 -4.1942334 -4.1715031 -4.2255764 -4.2946472 -4.3345833 -4.3436608 -4.3073821 -4.2625451 -4.2561865 -4.2663894][-4.4577909 -4.4680929 -4.445765 -4.3976021 -4.3409271 -4.2900996 -4.2704158 -4.3055677 -4.3554397 -4.3826861 -4.3775978 -4.3326006 -4.2963347 -4.3179345 -4.3587246][-4.48535 -4.5062971 -4.4898791 -4.4440122 -4.3966045 -4.367918 -4.364624 -4.3936515 -4.4289994 -4.44032 -4.4181833 -4.3689866 -4.3445468 -4.3840847 -4.4391751]]...]
INFO - root - 2017-12-08 01:36:24.189837: step 65910, loss = 21.50, batch loss = 21.42 (8.3 examples/sec; 0.967 sec/batch; 71h:34m:28s remains)
INFO - root - 2017-12-08 01:36:33.577081: step 65920, loss = 21.38, batch loss = 21.30 (8.4 examples/sec; 0.954 sec/batch; 70h:36m:54s remains)
INFO - root - 2017-12-08 01:36:43.007339: step 65930, loss = 21.63, batch loss = 21.55 (8.6 examples/sec; 0.933 sec/batch; 69h:06m:22s remains)
INFO - root - 2017-12-08 01:36:52.303195: step 65940, loss = 21.24, batch loss = 21.16 (8.8 examples/sec; 0.905 sec/batch; 67h:01m:53s remains)
INFO - root - 2017-12-08 01:37:01.623149: step 65950, loss = 21.35, batch loss = 21.27 (9.0 examples/sec; 0.887 sec/batch; 65h:38m:30s remains)
INFO - root - 2017-12-08 01:37:10.978230: step 65960, loss = 21.58, batch loss = 21.50 (8.1 examples/sec; 0.985 sec/batch; 72h:57m:03s remains)
INFO - root - 2017-12-08 01:37:20.288768: step 65970, loss = 21.58, batch loss = 21.49 (8.3 examples/sec; 0.959 sec/batch; 71h:01m:53s remains)
INFO - root - 2017-12-08 01:37:29.704199: step 65980, loss = 21.35, batch loss = 21.26 (8.8 examples/sec; 0.909 sec/batch; 67h:16m:20s remains)
INFO - root - 2017-12-08 01:37:39.015817: step 65990, loss = 21.75, batch loss = 21.66 (8.8 examples/sec; 0.913 sec/batch; 67h:34m:10s remains)
INFO - root - 2017-12-08 01:37:48.417497: step 66000, loss = 21.76, batch loss = 21.67 (8.8 examples/sec; 0.913 sec/batch; 67h:36m:02s remains)
2017-12-08 01:37:49.368611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3184924 -4.3285632 -4.3309693 -4.344502 -4.3534994 -4.33822 -4.29607 -4.2846446 -4.3314185 -4.4172373 -4.5025573 -4.5404387 -4.5037193 -4.4333663 -4.3800383][-4.2651978 -4.2561731 -4.2614827 -4.2832727 -4.2874589 -4.2535038 -4.1920152 -4.1748691 -4.2288227 -4.3304648 -4.4409266 -4.5102186 -4.5006232 -4.4429631 -4.3914351][-4.2431688 -4.2260814 -4.2370243 -4.2599912 -4.2499113 -4.1921329 -4.1187949 -4.1033187 -4.16429 -4.2793412 -4.4095964 -4.5010605 -4.509244 -4.4571195 -4.402791][-4.304553 -4.2953105 -4.3083177 -4.3138175 -4.2702084 -4.175396 -4.0819578 -4.0588493 -4.12132 -4.2497172 -4.4026604 -4.5144553 -4.5315728 -4.474009 -4.4102039][-4.4230132 -4.4342322 -4.44244 -4.4144473 -4.3251657 -4.1886106 -4.0667462 -4.0228176 -4.0800991 -4.2207484 -4.4013886 -4.5369668 -4.5596051 -4.4900208 -4.4130578][-4.534966 -4.5602851 -4.5541992 -4.4870906 -4.3561873 -4.1926966 -4.0545673 -3.9929469 -4.0440526 -4.1946683 -4.399415 -4.5587025 -4.5872283 -4.5054431 -4.4150395][-4.6012335 -4.6225734 -4.5940437 -4.4891939 -4.3232317 -4.1434774 -3.9985979 -3.9284675 -3.9765656 -4.1386204 -4.3667121 -4.553638 -4.5984535 -4.514575 -4.4158669][-4.6192632 -4.6214004 -4.5726547 -4.4428444 -4.25186 -4.0565929 -3.9091606 -3.839766 -3.8908777 -4.065608 -4.3133435 -4.5249658 -4.5907192 -4.5140672 -4.4138427][-4.5843544 -4.5676327 -4.512177 -4.3789134 -4.1778531 -3.9758387 -3.8411536 -3.7954769 -3.8656607 -4.0482197 -4.2939448 -4.5054135 -4.5765228 -4.50603 -4.4091878][-4.5132465 -4.4913 -4.4524264 -4.3424153 -4.1474304 -3.9427831 -3.823071 -3.8046036 -3.8947468 -4.07825 -4.309381 -4.5044093 -4.56714 -4.4983115 -4.4069028][-4.463376 -4.4496627 -4.4412251 -4.3721266 -4.2038159 -4.0097942 -3.9003506 -3.8936417 -3.9873626 -4.157146 -4.3605065 -4.5295572 -4.5750713 -4.5014505 -4.4130569][-4.45444 -4.4554715 -4.4754844 -4.4462314 -4.3170652 -4.1527643 -4.0607052 -4.0622258 -4.1496372 -4.2891889 -4.4476595 -4.5768504 -4.5969133 -4.515265 -4.4261947][-4.4665861 -4.4879589 -4.5273352 -4.5261393 -4.4362473 -4.3107591 -4.2420421 -4.2466373 -4.3163419 -4.4181404 -4.5316329 -4.6212511 -4.61625 -4.5273867 -4.4371057][-4.50807 -4.5460968 -4.5963817 -4.6062036 -4.5415735 -4.44903 -4.402359 -4.4094543 -4.4590521 -4.5286036 -4.6062541 -4.6583824 -4.6266003 -4.5293059 -4.4385972][-4.5450778 -4.6004944 -4.6553326 -4.6644864 -4.6132336 -4.5468645 -4.5187263 -4.5288606 -4.5621037 -4.6080656 -4.6600394 -4.6790891 -4.6221104 -4.517561 -4.4292493]]...]
INFO - root - 2017-12-08 01:37:58.924466: step 66010, loss = 21.56, batch loss = 21.48 (7.9 examples/sec; 1.016 sec/batch; 75h:13m:11s remains)
INFO - root - 2017-12-08 01:38:08.291694: step 66020, loss = 21.56, batch loss = 21.47 (8.4 examples/sec; 0.953 sec/batch; 70h:32m:12s remains)
INFO - root - 2017-12-08 01:38:17.691588: step 66030, loss = 21.57, batch loss = 21.48 (8.2 examples/sec; 0.980 sec/batch; 72h:31m:21s remains)
INFO - root - 2017-12-08 01:38:26.970849: step 66040, loss = 21.46, batch loss = 21.38 (8.5 examples/sec; 0.937 sec/batch; 69h:22m:45s remains)
INFO - root - 2017-12-08 01:38:36.416805: step 66050, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.928 sec/batch; 68h:39m:25s remains)
INFO - root - 2017-12-08 01:38:45.676235: step 66060, loss = 21.28, batch loss = 21.20 (8.8 examples/sec; 0.906 sec/batch; 67h:02m:23s remains)
INFO - root - 2017-12-08 01:38:55.213478: step 66070, loss = 21.55, batch loss = 21.47 (8.4 examples/sec; 0.947 sec/batch; 70h:04m:17s remains)
INFO - root - 2017-12-08 01:39:04.485965: step 66080, loss = 21.67, batch loss = 21.59 (8.7 examples/sec; 0.916 sec/batch; 67h:46m:06s remains)
INFO - root - 2017-12-08 01:39:13.985402: step 66090, loss = 21.87, batch loss = 21.79 (8.9 examples/sec; 0.896 sec/batch; 66h:17m:02s remains)
INFO - root - 2017-12-08 01:39:23.455752: step 66100, loss = 21.60, batch loss = 21.52 (8.8 examples/sec; 0.911 sec/batch; 67h:23m:13s remains)
2017-12-08 01:39:24.462289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2556539 -4.2233233 -4.2284102 -4.2753639 -4.34201 -4.3916578 -4.403862 -4.40045 -4.4095592 -4.4337711 -4.4602051 -4.4723206 -4.4592519 -4.4262247 -4.3854561][-4.2927213 -4.2876487 -4.3118763 -4.3672166 -4.429338 -4.4606357 -4.444221 -4.4156151 -4.4168096 -4.44969 -4.4942856 -4.5274506 -4.5255113 -4.4902282 -4.4418917][-4.3223138 -4.3410525 -4.3818312 -4.436769 -4.4724293 -4.457334 -4.3981104 -4.34706 -4.3504796 -4.3989277 -4.4652696 -4.525856 -4.5462914 -4.5193081 -4.4724197][-4.3669076 -4.4041967 -4.4491477 -4.47627 -4.4453254 -4.3512216 -4.2422986 -4.1853294 -4.2118406 -4.2874384 -4.3774471 -4.4675817 -4.5168476 -4.5067744 -4.4700313][-4.4226604 -4.4596572 -4.4810166 -4.4455 -4.3213115 -4.1450944 -4.0054417 -3.9679193 -4.0379586 -4.1496034 -4.2636371 -4.3807869 -4.4572806 -4.4663258 -4.4416671][-4.4560695 -4.464005 -4.4359746 -4.3248591 -4.1201358 -3.894598 -3.7528715 -3.743269 -3.8520517 -3.9937484 -4.1326065 -4.2787805 -4.3771191 -4.3988619 -4.3775892][-4.4196815 -4.3850021 -4.3077664 -4.1452451 -3.9090705 -3.685185 -3.5670071 -3.5782738 -3.7007041 -3.8531156 -4.0124345 -4.1861429 -4.2952352 -4.3145332 -4.2804837][-4.3058357 -4.2452393 -4.15181 -3.9913659 -3.7900407 -3.6220496 -3.5480244 -3.5683498 -3.6723237 -3.809536 -3.9770865 -4.1645942 -4.2648978 -4.257081 -4.189424][-4.201942 -4.145545 -4.0684996 -3.9486337 -3.8201466 -3.7291653 -3.6992035 -3.7150846 -3.7823653 -3.8919325 -4.0525103 -4.2268414 -4.2923427 -4.235508 -4.1283817][-4.1753855 -4.1359768 -4.0841656 -4.0117035 -3.9540043 -3.9305208 -3.9356856 -3.9493754 -3.9865398 -4.070137 -4.2076826 -4.3409185 -4.3560286 -4.2520967 -4.1236629][-4.2069049 -4.1877208 -4.1610723 -4.13274 -4.1337676 -4.164506 -4.1991773 -4.2155948 -4.2301836 -4.2828159 -4.3792472 -4.4558969 -4.424489 -4.296555 -4.1722622][-4.2620344 -4.2708917 -4.270246 -4.2744427 -4.3084607 -4.3623261 -4.4026365 -4.4052653 -4.3880172 -4.4013705 -4.4542294 -4.4897585 -4.4430728 -4.3319087 -4.2463923][-4.3025255 -4.3479996 -4.3712397 -4.3910451 -4.4296956 -4.4751215 -4.494741 -4.4662814 -4.41049 -4.387063 -4.4134707 -4.4412856 -4.4190159 -4.3605485 -4.3344717][-4.3222985 -4.3904567 -4.4222989 -4.440064 -4.4673791 -4.4934587 -4.4880247 -4.4301343 -4.3462052 -4.3017077 -4.3201733 -4.3581071 -4.3705306 -4.3657117 -4.3887515][-4.3242164 -4.3850465 -4.4044347 -4.4094834 -4.4249458 -4.4404206 -4.4250455 -4.3589425 -4.2709718 -4.2238708 -4.2389174 -4.2767735 -4.303196 -4.3269577 -4.3761139]]...]
INFO - root - 2017-12-08 01:39:33.671483: step 66110, loss = 21.23, batch loss = 21.15 (8.7 examples/sec; 0.918 sec/batch; 67h:55m:03s remains)
INFO - root - 2017-12-08 01:39:43.059811: step 66120, loss = 21.49, batch loss = 21.41 (8.8 examples/sec; 0.906 sec/batch; 67h:03m:31s remains)
INFO - root - 2017-12-08 01:39:52.313446: step 66130, loss = 21.51, batch loss = 21.42 (9.3 examples/sec; 0.856 sec/batch; 63h:20m:11s remains)
INFO - root - 2017-12-08 01:40:01.724607: step 66140, loss = 21.79, batch loss = 21.70 (8.6 examples/sec; 0.927 sec/batch; 68h:35m:29s remains)
INFO - root - 2017-12-08 01:40:11.340990: step 66150, loss = 21.57, batch loss = 21.49 (8.2 examples/sec; 0.979 sec/batch; 72h:24m:44s remains)
INFO - root - 2017-12-08 01:40:20.750086: step 66160, loss = 21.58, batch loss = 21.50 (8.2 examples/sec; 0.976 sec/batch; 72h:10m:30s remains)
INFO - root - 2017-12-08 01:40:30.048514: step 66170, loss = 21.25, batch loss = 21.17 (7.9 examples/sec; 1.017 sec/batch; 75h:12m:05s remains)
INFO - root - 2017-12-08 01:40:39.504037: step 66180, loss = 21.64, batch loss = 21.55 (8.8 examples/sec; 0.908 sec/batch; 67h:09m:27s remains)
INFO - root - 2017-12-08 01:40:48.858973: step 66190, loss = 21.71, batch loss = 21.63 (8.6 examples/sec; 0.929 sec/batch; 68h:42m:56s remains)
INFO - root - 2017-12-08 01:40:58.441403: step 66200, loss = 21.41, batch loss = 21.33 (8.1 examples/sec; 0.994 sec/batch; 73h:29m:51s remains)
2017-12-08 01:40:59.329933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2464824 -4.3496733 -4.4820218 -4.5787086 -4.5958195 -4.5515075 -4.4999743 -4.4698048 -4.471868 -4.517581 -4.561902 -4.5720882 -4.5476861 -4.5209017 -4.5321746][-4.2874727 -4.4107547 -4.5653872 -4.678741 -4.702868 -4.6612768 -4.6087236 -4.567544 -4.553669 -4.5753808 -4.5894551 -4.577291 -4.5452428 -4.5118141 -4.5156336][-4.342782 -4.4776063 -4.6381226 -4.7545681 -4.7824779 -4.7395153 -4.6771951 -4.6258421 -4.6099591 -4.6255627 -4.61907 -4.5811772 -4.5277352 -4.4716692 -4.4551754][-4.3993168 -4.530169 -4.6850324 -4.7965126 -4.8153143 -4.7487226 -4.6544633 -4.581471 -4.5661626 -4.5896716 -4.5893216 -4.5513158 -4.4875712 -4.4128723 -4.3774395][-4.4647131 -4.5787096 -4.7179279 -4.8119583 -4.7949324 -4.6700006 -4.5182123 -4.4183397 -4.4110532 -4.4624944 -4.5056877 -4.5105386 -4.4620204 -4.3823781 -4.3364882][-4.5214314 -4.6068707 -4.7111754 -4.76122 -4.6786966 -4.4745927 -4.263186 -4.1499767 -4.1705279 -4.2750263 -4.3854885 -4.45623 -4.4467039 -4.3819113 -4.3409438][-4.5680394 -4.6240931 -4.6846805 -4.6779256 -4.524653 -4.2497597 -3.9903138 -3.8674235 -3.9128718 -4.0710607 -4.2406864 -4.3673606 -4.4087181 -4.38414 -4.3711348][-4.5889468 -4.6317587 -4.6580296 -4.6034021 -4.399714 -4.0833864 -3.7933495 -3.6576514 -3.715529 -3.9099145 -4.1185918 -4.2800474 -4.366003 -4.3884997 -4.4132628][-4.5789108 -4.6237726 -4.6393561 -4.5662436 -4.3606186 -4.0598207 -3.782048 -3.6482229 -3.7054842 -3.9022839 -4.1106586 -4.266386 -4.3549991 -4.3840146 -4.41754][-4.533124 -4.5848031 -4.6127124 -4.5621 -4.4075208 -4.1810212 -3.9619465 -3.8462715 -3.8824153 -4.0367379 -4.1996093 -4.3121848 -4.3582067 -4.3463964 -4.3545561][-4.5045133 -4.56212 -4.6056175 -4.5890059 -4.4959216 -4.3475308 -4.1935196 -4.0998039 -4.1080875 -4.1962147 -4.2861414 -4.3316846 -4.3153763 -4.2541189 -4.2370749][-4.5118814 -4.5658703 -4.6112456 -4.6172385 -4.5668454 -4.4651432 -4.3550172 -4.2913184 -4.289587 -4.324019 -4.3428249 -4.3179374 -4.2456131 -4.1531458 -4.1267815][-4.5409694 -4.5797343 -4.6062355 -4.6143093 -4.5863652 -4.5057878 -4.4150124 -4.3803449 -4.3920875 -4.4044533 -4.3808475 -4.311224 -4.2102852 -4.1167116 -4.097167][-4.5630631 -4.5864606 -4.5934768 -4.5926671 -4.5758677 -4.5026031 -4.4057469 -4.3763509 -4.3986444 -4.412766 -4.386004 -4.3130283 -4.2186456 -4.1540742 -4.1514626][-4.5588818 -4.5723281 -4.567275 -4.552546 -4.5334611 -4.4653115 -4.3659306 -4.3304076 -4.3543959 -4.3781333 -4.3665333 -4.31302 -4.2448478 -4.2185559 -4.2352257]]...]
INFO - root - 2017-12-08 01:41:08.607117: step 66210, loss = 21.43, batch loss = 21.35 (8.6 examples/sec; 0.931 sec/batch; 68h:51m:56s remains)
INFO - root - 2017-12-08 01:41:18.145252: step 66220, loss = 21.99, batch loss = 21.91 (8.4 examples/sec; 0.947 sec/batch; 70h:04m:23s remains)
INFO - root - 2017-12-08 01:41:27.701859: step 66230, loss = 21.46, batch loss = 21.37 (8.0 examples/sec; 0.994 sec/batch; 73h:33m:25s remains)
INFO - root - 2017-12-08 01:41:37.140880: step 66240, loss = 21.41, batch loss = 21.33 (8.3 examples/sec; 0.965 sec/batch; 71h:24m:23s remains)
INFO - root - 2017-12-08 01:41:46.444021: step 66250, loss = 21.70, batch loss = 21.62 (8.3 examples/sec; 0.968 sec/batch; 71h:36m:35s remains)
INFO - root - 2017-12-08 01:41:55.897052: step 66260, loss = 21.81, batch loss = 21.73 (8.1 examples/sec; 0.989 sec/batch; 73h:06m:41s remains)
INFO - root - 2017-12-08 01:42:05.322924: step 66270, loss = 21.39, batch loss = 21.31 (8.1 examples/sec; 0.984 sec/batch; 72h:44m:40s remains)
INFO - root - 2017-12-08 01:42:14.551669: step 66280, loss = 21.55, batch loss = 21.47 (9.9 examples/sec; 0.805 sec/batch; 59h:29m:55s remains)
INFO - root - 2017-12-08 01:42:23.953020: step 66290, loss = 21.23, batch loss = 21.15 (9.0 examples/sec; 0.892 sec/batch; 65h:58m:02s remains)
INFO - root - 2017-12-08 01:42:33.257290: step 66300, loss = 21.36, batch loss = 21.28 (8.5 examples/sec; 0.936 sec/batch; 69h:11m:41s remains)
2017-12-08 01:42:34.165719: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3001313 -4.3049397 -4.3102098 -4.3222432 -4.345614 -4.3726916 -4.3919649 -4.4138474 -4.4571285 -4.5037026 -4.5320964 -4.5306177 -4.493041 -4.4269361 -4.3477597][-4.3886976 -4.4017491 -4.4134164 -4.4352551 -4.468071 -4.4924941 -4.4938784 -4.5001531 -4.5419235 -4.5916224 -4.6264138 -4.6362071 -4.6054254 -4.5282674 -4.4149981][-4.4534178 -4.4757004 -4.4972391 -4.5326209 -4.5743437 -4.5838909 -4.5479603 -4.52481 -4.5651693 -4.6247005 -4.6811976 -4.723012 -4.7201905 -4.6469646 -4.5026846][-4.5044804 -4.5442891 -4.5817041 -4.6299653 -4.6703906 -4.646071 -4.5482755 -4.4752345 -4.5086627 -4.5829597 -4.6747828 -4.7665205 -4.8065071 -4.7490692 -4.58462][-4.5361462 -4.6005363 -4.654315 -4.7043514 -4.7228937 -4.6465778 -4.4715819 -4.3398705 -4.3702621 -4.4653563 -4.5989151 -4.752388 -4.8487158 -4.8172121 -4.6449194][-4.5240612 -4.6087065 -4.673666 -4.7101107 -4.68193 -4.5351415 -4.2815986 -4.1139159 -4.1714034 -4.2972765 -4.4624333 -4.6717672 -4.8254404 -4.8269882 -4.6580863][-4.4680052 -4.55975 -4.6252255 -4.63443 -4.5389524 -4.2998033 -3.9697461 -3.8178506 -3.9618821 -4.14062 -4.3245597 -4.5698977 -4.7640142 -4.7904916 -4.6243343][-4.3480582 -4.4274387 -4.47282 -4.4424362 -4.27485 -3.9373274 -3.5512226 -3.4793072 -3.771894 -4.040319 -4.2542129 -4.5264325 -4.74007 -4.767592 -4.585474][-4.2015724 -4.2723584 -4.302321 -4.2432141 -4.0210786 -3.601346 -3.1878452 -3.2169616 -3.6585751 -4.0340528 -4.2996464 -4.5892873 -4.7906895 -4.78846 -4.5723834][-4.1388054 -4.2310944 -4.2799649 -4.22947 -3.9929695 -3.5365264 -3.1251504 -3.2114964 -3.7265677 -4.1764474 -4.4897032 -4.7700663 -4.9163618 -4.8531036 -4.595696][-4.2022548 -4.3285475 -4.4143834 -4.3963656 -4.182055 -3.7512767 -3.3838108 -3.4749985 -3.9607668 -4.4178 -4.7431164 -4.9822674 -5.0570951 -4.9317636 -4.6467013][-4.3290462 -4.475831 -4.5934472 -4.6116667 -4.444654 -4.098227 -3.816113 -3.8908877 -4.2773514 -4.6633768 -4.9382753 -5.1037955 -5.1134434 -4.9599104 -4.6861563][-4.4560022 -4.6017504 -4.7364163 -4.7884622 -4.6863413 -4.4542737 -4.2742639 -4.326827 -4.5861535 -4.8499961 -5.0247078 -5.1014986 -5.0657811 -4.9186525 -4.6860719][-4.5412436 -4.6591825 -4.7777848 -4.8394527 -4.796329 -4.6745439 -4.5828681 -4.607584 -4.7411184 -4.8805633 -4.9652739 -4.9868155 -4.9411993 -4.8214989 -4.6368527][-4.5631089 -4.645113 -4.7240124 -4.7666245 -4.752439 -4.7033195 -4.6671467 -4.6701646 -4.7151227 -4.7665424 -4.7979755 -4.80391 -4.7715545 -4.6838551 -4.5452347]]...]
INFO - root - 2017-12-08 01:42:43.547926: step 66310, loss = 21.41, batch loss = 21.33 (8.6 examples/sec; 0.935 sec/batch; 69h:07m:16s remains)
INFO - root - 2017-12-08 01:42:52.970562: step 66320, loss = 21.32, batch loss = 21.24 (8.5 examples/sec; 0.938 sec/batch; 69h:19m:45s remains)
INFO - root - 2017-12-08 01:43:02.315729: step 66330, loss = 21.75, batch loss = 21.66 (8.4 examples/sec; 0.956 sec/batch; 70h:40m:41s remains)
INFO - root - 2017-12-08 01:43:11.740815: step 66340, loss = 21.58, batch loss = 21.50 (8.7 examples/sec; 0.924 sec/batch; 68h:17m:36s remains)
INFO - root - 2017-12-08 01:43:21.063130: step 66350, loss = 21.10, batch loss = 21.02 (8.9 examples/sec; 0.901 sec/batch; 66h:38m:36s remains)
INFO - root - 2017-12-08 01:43:30.443442: step 66360, loss = 21.47, batch loss = 21.38 (8.8 examples/sec; 0.913 sec/batch; 67h:28m:25s remains)
INFO - root - 2017-12-08 01:43:39.923166: step 66370, loss = 21.57, batch loss = 21.49 (8.5 examples/sec; 0.937 sec/batch; 69h:15m:06s remains)
INFO - root - 2017-12-08 01:43:49.369854: step 66380, loss = 21.69, batch loss = 21.61 (8.1 examples/sec; 0.990 sec/batch; 73h:10m:27s remains)
INFO - root - 2017-12-08 01:43:58.739382: step 66390, loss = 21.43, batch loss = 21.35 (8.1 examples/sec; 0.982 sec/batch; 72h:36m:55s remains)
INFO - root - 2017-12-08 01:44:08.268657: step 66400, loss = 21.24, batch loss = 21.15 (8.3 examples/sec; 0.959 sec/batch; 70h:52m:38s remains)
2017-12-08 01:44:09.209347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2593517 -4.2621956 -4.307343 -4.3747106 -4.4171648 -4.4175458 -4.4150128 -4.4103351 -4.3882618 -4.3814721 -4.3997374 -4.4159279 -4.4080725 -4.3822584 -4.3566504][-4.1481142 -4.1481414 -4.2060394 -4.3031464 -4.382627 -4.4098473 -4.4198341 -4.4245839 -4.4173207 -4.4270453 -4.4499154 -4.4595704 -4.4454851 -4.4133162 -4.3768535][-4.0359883 -4.0405159 -4.1158805 -4.2382612 -4.3444223 -4.3874531 -4.3948293 -4.3933129 -4.3981953 -4.432116 -4.4681435 -4.4759622 -4.459404 -4.4281631 -4.3896031][-3.9442825 -3.9546602 -4.0441318 -4.1785369 -4.2933192 -4.3385034 -4.3334746 -4.3170533 -4.3280177 -4.3828139 -4.4306684 -4.4350467 -4.4142175 -4.3852873 -4.3551722][-3.8822074 -3.90585 -4.0133233 -4.1495748 -4.2522855 -4.2812872 -4.2534876 -4.2206655 -4.23536 -4.3091531 -4.3731818 -4.3830924 -4.3603754 -4.329915 -4.3092031][-3.8582115 -3.9057431 -4.0348005 -4.1575723 -4.2133207 -4.1889772 -4.1239944 -4.0766759 -4.1022525 -4.2021484 -4.29828 -4.3331003 -4.3194685 -4.2862968 -4.2694359][-3.8944736 -3.947084 -4.0750422 -4.1674104 -4.1572013 -4.0625234 -3.9611528 -3.910975 -3.9554603 -4.084455 -4.2197657 -4.2904248 -4.2929096 -4.2575865 -4.2408018][-3.9954486 -4.0324988 -4.1359262 -4.1952167 -4.1370039 -3.9965661 -3.8837249 -3.845053 -3.9052298 -4.0453525 -4.1939445 -4.2758961 -4.28432 -4.2528048 -4.2438731][-4.1229472 -4.1361117 -4.2044735 -4.2320833 -4.1535759 -4.0121779 -3.9145839 -3.8873725 -3.9485846 -4.0842009 -4.2262964 -4.2996249 -4.3033757 -4.2768111 -4.2777271][-4.242 -4.2284274 -4.2645569 -4.2716031 -4.2007809 -4.0920272 -4.0215855 -3.9990339 -4.0476613 -4.1679335 -4.2925439 -4.3514962 -4.3469462 -4.3185444 -4.3174348][-4.3302956 -4.2932773 -4.3067541 -4.3126526 -4.2703657 -4.2082152 -4.1649857 -4.1394687 -4.1642137 -4.2539148 -4.3501191 -4.3990021 -4.3969307 -4.3746581 -4.3706408][-4.3631415 -4.3078146 -4.31108 -4.33141 -4.3189378 -4.2916355 -4.2655358 -4.2367496 -4.2411232 -4.301002 -4.3744063 -4.4264097 -4.4439754 -4.4412894 -4.4415174][-4.3605146 -4.2910624 -4.2898078 -4.3256516 -4.3326778 -4.3236918 -4.3082414 -4.2781162 -4.2653055 -4.2956977 -4.3507624 -4.4125981 -4.4545007 -4.4742117 -4.4823284][-4.3486042 -4.2745852 -4.2688241 -4.3106918 -4.3263731 -4.324676 -4.3128891 -4.2754822 -4.2414341 -4.2399588 -4.2736778 -4.335536 -4.3949251 -4.4421568 -4.4740796][-4.3273411 -4.2636976 -4.2559094 -4.2923069 -4.3054442 -4.3020978 -4.2884994 -4.2414026 -4.18797 -4.1607676 -4.1747904 -4.2266569 -4.2909007 -4.3609428 -4.4242773]]...]
INFO - root - 2017-12-08 01:44:18.531955: step 66410, loss = 21.54, batch loss = 21.45 (8.5 examples/sec; 0.943 sec/batch; 69h:42m:36s remains)
INFO - root - 2017-12-08 01:44:27.970579: step 66420, loss = 21.99, batch loss = 21.91 (8.9 examples/sec; 0.902 sec/batch; 66h:38m:24s remains)
INFO - root - 2017-12-08 01:44:37.429208: step 66430, loss = 21.60, batch loss = 21.52 (8.7 examples/sec; 0.922 sec/batch; 68h:08m:48s remains)
INFO - root - 2017-12-08 01:44:46.717963: step 66440, loss = 21.69, batch loss = 21.61 (8.3 examples/sec; 0.967 sec/batch; 71h:27m:32s remains)
INFO - root - 2017-12-08 01:44:56.125979: step 66450, loss = 21.42, batch loss = 21.34 (8.5 examples/sec; 0.938 sec/batch; 69h:19m:34s remains)
INFO - root - 2017-12-08 01:45:05.304079: step 66460, loss = 21.71, batch loss = 21.63 (8.6 examples/sec; 0.929 sec/batch; 68h:38m:16s remains)
INFO - root - 2017-12-08 01:45:14.718429: step 66470, loss = 21.53, batch loss = 21.44 (8.4 examples/sec; 0.954 sec/batch; 70h:31m:22s remains)
INFO - root - 2017-12-08 01:45:24.004520: step 66480, loss = 21.67, batch loss = 21.58 (8.3 examples/sec; 0.962 sec/batch; 71h:06m:17s remains)
INFO - root - 2017-12-08 01:45:33.408635: step 66490, loss = 21.12, batch loss = 21.04 (8.4 examples/sec; 0.952 sec/batch; 70h:19m:46s remains)
INFO - root - 2017-12-08 01:45:42.880733: step 66500, loss = 21.68, batch loss = 21.60 (8.6 examples/sec; 0.936 sec/batch; 69h:07m:33s remains)
2017-12-08 01:45:43.805683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5692463 -4.6924372 -4.7578526 -4.7401867 -4.6618171 -4.5867658 -4.5454345 -4.5042148 -4.4742789 -4.4885383 -4.5235858 -4.5210772 -4.4981389 -4.533999 -4.6016059][-4.552649 -4.6424212 -4.6913595 -4.6681514 -4.5973845 -4.5402756 -4.5107265 -4.4688263 -4.4335952 -4.4530473 -4.5179925 -4.5572696 -4.5513349 -4.5669131 -4.5971093][-4.5478735 -4.6034732 -4.6244593 -4.5846696 -4.5164223 -4.4774861 -4.4607234 -4.425169 -4.3859463 -4.39727 -4.4699874 -4.5336366 -4.5376649 -4.5358248 -4.5373235][-4.5584064 -4.5844574 -4.5732365 -4.5133958 -4.4449911 -4.4118848 -4.3903255 -4.35163 -4.2926159 -4.2703161 -4.3243246 -4.401938 -4.4441075 -4.4677496 -4.4712939][-4.5601473 -4.5598435 -4.5181866 -4.4370322 -4.3638577 -4.3161836 -4.26983 -4.2166381 -4.1320415 -4.0746694 -4.1114635 -4.2129478 -4.3203936 -4.4038444 -4.4364161][-4.5172749 -4.4754739 -4.3937469 -4.2874789 -4.209332 -4.1501784 -4.0892715 -4.031445 -3.9399836 -3.876487 -3.9165154 -4.043067 -4.2037554 -4.332993 -4.3894539][-4.4370804 -4.3441806 -4.2193274 -4.092907 -4.012392 -3.9525721 -3.9020007 -3.86405 -3.8072772 -3.7899294 -3.8589685 -3.9967391 -4.1646037 -4.2909718 -4.3445506][-4.3770046 -4.2517128 -4.1000695 -3.9652147 -3.8777785 -3.8213522 -3.7958207 -3.7936778 -3.8034482 -3.8655396 -3.9749615 -4.1067214 -4.2333369 -4.3125777 -4.3448167][-4.3902721 -4.2672262 -4.1195164 -3.9972126 -3.908499 -3.8582847 -3.8562148 -3.8792026 -3.9388978 -4.0581388 -4.1881237 -4.2949071 -4.3606024 -4.3821015 -4.3916206][-4.4657011 -4.3890295 -4.2936716 -4.2148337 -4.1385541 -4.0883427 -4.0838366 -4.1038322 -4.1720977 -4.2951517 -4.4043736 -4.4649358 -4.4704285 -4.4537187 -4.4579124][-4.537138 -4.5256443 -4.4949136 -4.4623537 -4.4063196 -4.3575726 -4.3442755 -4.3527064 -4.4036751 -4.4938602 -4.5585141 -4.5688152 -4.5316296 -4.4999142 -4.5071535][-4.5630136 -4.5976262 -4.6120811 -4.6146207 -4.586637 -4.5567164 -4.5529189 -4.56072 -4.58718 -4.6302671 -4.6422586 -4.6070127 -4.5472422 -4.5159721 -4.527617][-4.5597715 -4.6189394 -4.6568923 -4.6759558 -4.6692333 -4.6611538 -4.6726866 -4.682004 -4.68482 -4.6828022 -4.6497917 -4.5859137 -4.5225945 -4.4993124 -4.5143514][-4.533287 -4.59212 -4.6299777 -4.6464987 -4.64666 -4.6485062 -4.6613245 -4.6641936 -4.6494584 -4.6213894 -4.5719957 -4.5098228 -4.4600706 -4.4440975 -4.4556866][-4.4686723 -4.5110183 -4.5395975 -4.5521665 -4.5545583 -4.5553503 -4.556952 -4.5474896 -4.522306 -4.4875207 -4.4455214 -4.4035716 -4.3740029 -4.3647342 -4.370719]]...]
INFO - root - 2017-12-08 01:45:53.232177: step 66510, loss = 21.77, batch loss = 21.69 (8.6 examples/sec; 0.931 sec/batch; 68h:46m:53s remains)
INFO - root - 2017-12-08 01:46:02.682664: step 66520, loss = 21.52, batch loss = 21.44 (8.1 examples/sec; 0.990 sec/batch; 73h:08m:47s remains)
INFO - root - 2017-12-08 01:46:12.079775: step 66530, loss = 21.41, batch loss = 21.33 (8.7 examples/sec; 0.920 sec/batch; 68h:00m:20s remains)
INFO - root - 2017-12-08 01:46:21.475993: step 66540, loss = 21.65, batch loss = 21.57 (8.5 examples/sec; 0.946 sec/batch; 69h:53m:19s remains)
INFO - root - 2017-12-08 01:46:30.770223: step 66550, loss = 21.68, batch loss = 21.59 (8.9 examples/sec; 0.899 sec/batch; 66h:26m:22s remains)
INFO - root - 2017-12-08 01:46:40.196680: step 66560, loss = 21.64, batch loss = 21.56 (8.2 examples/sec; 0.978 sec/batch; 72h:16m:46s remains)
INFO - root - 2017-12-08 01:46:49.733413: step 66570, loss = 21.71, batch loss = 21.63 (8.6 examples/sec; 0.928 sec/batch; 68h:34m:04s remains)
INFO - root - 2017-12-08 01:46:59.051066: step 66580, loss = 21.19, batch loss = 21.10 (8.5 examples/sec; 0.943 sec/batch; 69h:40m:38s remains)
INFO - root - 2017-12-08 01:47:08.510756: step 66590, loss = 21.73, batch loss = 21.64 (8.2 examples/sec; 0.972 sec/batch; 71h:49m:53s remains)
INFO - root - 2017-12-08 01:47:17.909721: step 66600, loss = 21.62, batch loss = 21.54 (8.5 examples/sec; 0.945 sec/batch; 69h:49m:57s remains)
2017-12-08 01:47:18.820842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0820589 -4.0680971 -4.2082233 -4.39946 -4.5442882 -4.568018 -4.5091543 -4.4913597 -4.5572443 -4.6454296 -4.6857481 -4.6380424 -4.525991 -4.4290056 -4.3810506][-4.0145617 -4.0331964 -4.2116432 -4.4268932 -4.5517969 -4.5167351 -4.3872261 -4.3239846 -4.3986907 -4.544261 -4.6572757 -4.6418409 -4.5006337 -4.3577213 -4.2932148][-4.0148964 -4.0658174 -4.26513 -4.47526 -4.5425067 -4.4131465 -4.1934209 -4.0866151 -4.1871142 -4.4087133 -4.6095924 -4.63272 -4.4662867 -4.2815614 -4.2020588][-4.069644 -4.1335392 -4.32808 -4.510273 -4.5067573 -4.2870889 -3.9890859 -3.8490436 -3.9737825 -4.2608442 -4.5388212 -4.5994806 -4.4273634 -4.223052 -4.1357627][-4.1236782 -4.1849904 -4.3590808 -4.5058475 -4.4507151 -4.1799951 -3.8432672 -3.6912291 -3.8314514 -4.1499648 -4.4645009 -4.55168 -4.3958054 -4.1962767 -4.1054049][-4.1456046 -4.1971817 -4.3471704 -4.4667234 -4.3905749 -4.1124105 -3.7777772 -3.635319 -3.7863417 -4.1061769 -4.4153233 -4.5121875 -4.3851423 -4.20236 -4.1058264][-4.1452656 -4.1801939 -4.3045478 -4.40776 -4.3383069 -4.0893431 -3.7889862 -3.6693711 -3.822484 -4.1178856 -4.3891263 -4.4770217 -4.3795495 -4.2206435 -4.1225619][-4.1274862 -4.145287 -4.24742 -4.3466768 -4.30694 -4.115109 -3.8707857 -3.7699254 -3.8972502 -4.1362991 -4.3472013 -4.4203219 -4.3541603 -4.22619 -4.1396384][-4.1152434 -4.1219368 -4.2094588 -4.3153219 -4.314703 -4.1880522 -3.9993234 -3.8982618 -3.9704089 -4.1364751 -4.2906156 -4.3572164 -4.3234105 -4.2305584 -4.16463][-4.1177645 -4.1225958 -4.2018933 -4.3117857 -4.3385115 -4.2560482 -4.1026988 -3.989661 -4.0092421 -4.1230187 -4.24569 -4.3138151 -4.3065386 -4.2463818 -4.2043595][-4.1337595 -4.140316 -4.2095885 -4.3143291 -4.3514738 -4.2910204 -4.1561842 -4.0341043 -4.0214109 -4.109055 -4.2182913 -4.2873716 -4.2954187 -4.2632055 -4.2453375][-4.1547174 -4.1599288 -4.2156591 -4.3128042 -4.3567677 -4.3100548 -4.1894 -4.0698934 -4.0471745 -4.1273108 -4.2337751 -4.304143 -4.3197036 -4.3015542 -4.2901368][-4.1674309 -4.1626635 -4.2009544 -4.290225 -4.3425956 -4.3119087 -4.2115684 -4.1070294 -4.0830646 -4.1577 -4.2627449 -4.3335776 -4.3519588 -4.3358593 -4.31602][-4.1736689 -4.1514354 -4.1695848 -4.2489848 -4.3103747 -4.30221 -4.2298241 -4.1431441 -4.1098485 -4.1634541 -4.2571378 -4.3267984 -4.3510275 -4.3419 -4.3204737][-4.1876216 -4.1459136 -4.1442709 -4.212472 -4.2824922 -4.2995639 -4.2600517 -4.1915197 -4.1412325 -4.1610613 -4.2364988 -4.3041625 -4.3357773 -4.3363814 -4.32105]]...]
INFO - root - 2017-12-08 01:47:28.249942: step 66610, loss = 21.81, batch loss = 21.73 (8.5 examples/sec; 0.938 sec/batch; 69h:16m:25s remains)
INFO - root - 2017-12-08 01:47:37.612112: step 66620, loss = 21.52, batch loss = 21.43 (8.5 examples/sec; 0.939 sec/batch; 69h:20m:04s remains)
INFO - root - 2017-12-08 01:47:46.969805: step 66630, loss = 21.98, batch loss = 21.90 (8.5 examples/sec; 0.942 sec/batch; 69h:34m:27s remains)
INFO - root - 2017-12-08 01:47:56.285999: step 66640, loss = 21.24, batch loss = 21.16 (8.2 examples/sec; 0.979 sec/batch; 72h:17m:07s remains)
INFO - root - 2017-12-08 01:48:05.705094: step 66650, loss = 21.43, batch loss = 21.34 (8.5 examples/sec; 0.944 sec/batch; 69h:43m:45s remains)
INFO - root - 2017-12-08 01:48:15.027271: step 66660, loss = 21.42, batch loss = 21.34 (8.9 examples/sec; 0.898 sec/batch; 66h:17m:32s remains)
INFO - root - 2017-12-08 01:48:24.262656: step 66670, loss = 21.21, batch loss = 21.12 (8.3 examples/sec; 0.964 sec/batch; 71h:12m:59s remains)
INFO - root - 2017-12-08 01:48:33.665223: step 66680, loss = 21.59, batch loss = 21.51 (8.6 examples/sec; 0.927 sec/batch; 68h:27m:08s remains)
INFO - root - 2017-12-08 01:48:43.018031: step 66690, loss = 21.28, batch loss = 21.20 (8.1 examples/sec; 0.984 sec/batch; 72h:38m:36s remains)
INFO - root - 2017-12-08 01:48:52.373280: step 66700, loss = 21.49, batch loss = 21.40 (8.8 examples/sec; 0.905 sec/batch; 66h:47m:54s remains)
2017-12-08 01:48:53.315934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2171555 -4.2259603 -4.2509804 -4.2808347 -4.2855024 -4.2628055 -4.2276063 -4.1742086 -4.1405082 -4.1398792 -4.1195951 -4.0646029 -3.9941151 -3.9219248 -3.8813837][-4.3006206 -4.2934284 -4.3057261 -4.3213 -4.3140435 -4.2785645 -4.2331467 -4.1765184 -4.1367693 -4.1341128 -4.1201239 -4.0794888 -4.0269589 -3.9586773 -3.9146318][-4.3561997 -4.3281612 -4.3175645 -4.3152266 -4.2997413 -4.2597647 -4.2127829 -4.1631474 -4.1273646 -4.1288033 -4.1379366 -4.1323872 -4.115808 -4.06993 -4.0301232][-4.3389282 -4.3015413 -4.2831078 -4.2880726 -4.2858858 -4.256041 -4.2128034 -4.1644192 -4.1266403 -4.1205945 -4.1405439 -4.1638985 -4.1818576 -4.1618352 -4.1340723][-4.2566977 -4.2117348 -4.1936803 -4.2155375 -4.2393117 -4.2340579 -4.2020063 -4.1516423 -4.1076193 -4.0903673 -4.1134386 -4.1549058 -4.1945143 -4.1940565 -4.1798253][-4.1454206 -4.100636 -4.0883026 -4.1235242 -4.1668682 -4.1802864 -4.1556158 -4.1058292 -4.0624194 -4.043746 -4.0764327 -4.135026 -4.1888084 -4.2051868 -4.1981554][-4.0838661 -4.0474849 -4.04686 -4.0886855 -4.1382818 -4.150197 -4.1176062 -4.0679522 -4.0246034 -4.0107522 -4.0592651 -4.1345515 -4.1961312 -4.2261658 -4.218215][-4.121635 -4.082284 -4.0805054 -4.1179934 -4.166646 -4.1720457 -4.1263862 -4.0708513 -4.0202842 -4.0112362 -4.0790558 -4.1678638 -4.2332826 -4.2679782 -4.2513046][-4.2494092 -4.2053671 -4.1844254 -4.1978207 -4.23163 -4.2291193 -4.17739 -4.1218209 -4.0734062 -4.076076 -4.1595135 -4.252018 -4.3088083 -4.3299313 -4.2936664][-4.3869195 -4.3482351 -4.3096867 -4.2942743 -4.3110781 -4.3045387 -4.2570043 -4.2137337 -4.1788082 -4.1880136 -4.2621017 -4.332572 -4.3654342 -4.3643336 -4.3110762][-4.4502096 -4.4236979 -4.379509 -4.355484 -4.3738112 -4.3754764 -4.3443975 -4.3240504 -4.3014178 -4.2987809 -4.3370361 -4.3630824 -4.3661671 -4.3522778 -4.3024535][-4.4038825 -4.389605 -4.3504472 -4.3445435 -4.38683 -4.4054866 -4.3954291 -4.3947697 -4.3758483 -4.3578691 -4.3649254 -4.35925 -4.3496838 -4.3437686 -4.316853][-4.3092971 -4.2924385 -4.2430382 -4.2480841 -4.306468 -4.3431387 -4.3633876 -4.3869257 -4.3761597 -4.3590345 -4.3570175 -4.3410149 -4.3310246 -4.3392396 -4.334796][-4.1940789 -4.1588116 -4.0932364 -4.0988116 -4.1643982 -4.2221932 -4.2833185 -4.3354368 -4.3392053 -4.3327007 -4.3309455 -4.3109236 -4.3015842 -4.3153996 -4.3217854][-4.1151743 -4.0668983 -3.9950287 -3.9950581 -4.0553637 -4.1250191 -4.2134786 -4.28832 -4.3108578 -4.3130937 -4.3050084 -4.27835 -4.2671294 -4.2752562 -4.2800622]]...]
INFO - root - 2017-12-08 01:49:02.694658: step 66710, loss = 21.30, batch loss = 21.22 (8.0 examples/sec; 1.000 sec/batch; 73h:48m:10s remains)
INFO - root - 2017-12-08 01:49:12.052239: step 66720, loss = 21.94, batch loss = 21.86 (8.1 examples/sec; 0.987 sec/batch; 72h:50m:07s remains)
INFO - root - 2017-12-08 01:49:21.401244: step 66730, loss = 21.31, batch loss = 21.23 (8.7 examples/sec; 0.916 sec/batch; 67h:38m:22s remains)
INFO - root - 2017-12-08 01:49:30.891853: step 66740, loss = 21.43, batch loss = 21.35 (8.5 examples/sec; 0.943 sec/batch; 69h:37m:08s remains)
INFO - root - 2017-12-08 01:49:40.387735: step 66750, loss = 21.47, batch loss = 21.39 (8.7 examples/sec; 0.920 sec/batch; 67h:54m:01s remains)
INFO - root - 2017-12-08 01:49:49.696532: step 66760, loss = 21.54, batch loss = 21.46 (8.5 examples/sec; 0.945 sec/batch; 69h:46m:47s remains)
INFO - root - 2017-12-08 01:49:58.998189: step 66770, loss = 21.43, batch loss = 21.35 (9.3 examples/sec; 0.857 sec/batch; 63h:14m:48s remains)
INFO - root - 2017-12-08 01:50:08.277231: step 66780, loss = 21.94, batch loss = 21.85 (10.2 examples/sec; 0.782 sec/batch; 57h:43m:49s remains)
INFO - root - 2017-12-08 01:50:17.654898: step 66790, loss = 21.67, batch loss = 21.59 (8.6 examples/sec; 0.931 sec/batch; 68h:43m:48s remains)
INFO - root - 2017-12-08 01:50:26.942666: step 66800, loss = 21.29, batch loss = 21.21 (9.0 examples/sec; 0.887 sec/batch; 65h:29m:06s remains)
2017-12-08 01:50:27.842818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6642766 -4.7243094 -4.7561159 -4.7457037 -4.6936188 -4.6307549 -4.5937352 -4.5914159 -4.6076465 -4.6228189 -4.622582 -4.603457 -4.5721831 -4.5425453 -4.5250587][-4.6368542 -4.7112341 -4.7548666 -4.7501616 -4.699595 -4.6438527 -4.6204443 -4.6319485 -4.653481 -4.6628966 -4.6505218 -4.615232 -4.5598583 -4.4981647 -4.4485264][-4.58286 -4.6726484 -4.7302146 -4.736608 -4.6972227 -4.6543088 -4.6463408 -4.6757021 -4.7122345 -4.7307787 -4.7227497 -4.682385 -4.6064377 -4.5075927 -4.4158297][-4.5577221 -4.666605 -4.7397017 -4.7527423 -4.7068295 -4.6446567 -4.6141181 -4.6363945 -4.6941185 -4.7532129 -4.7844605 -4.7680683 -4.6949787 -4.5777764 -4.4571471][-4.5676818 -4.6911511 -4.7703695 -4.7738085 -4.692709 -4.5636797 -4.456512 -4.4388328 -4.5261273 -4.6592493 -4.76144 -4.7943926 -4.7496724 -4.6427627 -4.5227966][-4.5844421 -4.7011175 -4.7640843 -4.7417154 -4.6095977 -4.3904285 -4.1814084 -4.1107874 -4.2381587 -4.4665203 -4.6538472 -4.7442269 -4.7377615 -4.6566267 -4.5553603][-4.5829229 -4.67154 -4.70047 -4.6451745 -4.4675636 -4.1719003 -3.8814523 -3.7776041 -3.9527216 -4.2718105 -4.5344605 -4.6712956 -4.6951609 -4.6343851 -4.5498462][-4.577981 -4.6314969 -4.6181479 -4.5197444 -4.2987146 -3.9504747 -3.6192591 -3.5116782 -3.7295289 -4.114624 -4.4369183 -4.6153164 -4.6680098 -4.6223965 -4.5421119][-4.5896711 -4.6173029 -4.5691514 -4.4299469 -4.1745925 -3.8043387 -3.4711413 -3.3719912 -3.5971103 -3.9937167 -4.3402061 -4.5550451 -4.6488733 -4.6294589 -4.5489011][-4.6115589 -4.6340408 -4.5788074 -4.4313145 -4.1818523 -3.8415718 -3.5482762 -3.4604695 -3.6434851 -3.9800014 -4.2975659 -4.5212975 -4.6439714 -4.6460514 -4.5602622][-4.6257491 -4.6600461 -4.6263337 -4.5129747 -4.317977 -4.0568581 -3.836194 -3.7647219 -3.8816357 -4.1176124 -4.3632779 -4.5560756 -4.6723871 -4.6714725 -4.5695515][-4.6156478 -4.6641917 -4.6645823 -4.6093473 -4.4986281 -4.3444819 -4.2166762 -4.1744251 -4.2301817 -4.3539639 -4.4989486 -4.6243534 -4.7000127 -4.6785645 -4.5631242][-4.5655165 -4.6204381 -4.6507025 -4.6500912 -4.6171236 -4.5565491 -4.5088215 -4.4944649 -4.5097613 -4.547102 -4.5958691 -4.6449819 -4.6719704 -4.6338372 -4.5255809][-4.473587 -4.5253816 -4.56783 -4.5974431 -4.6109495 -4.6053033 -4.5973411 -4.5888262 -4.5798082 -4.5743403 -4.5713496 -4.5763535 -4.5757446 -4.53962 -4.4602718][-4.3734889 -4.4121161 -4.4501653 -4.4887 -4.5217929 -4.5395546 -4.5412335 -4.5222173 -4.4974513 -4.4802694 -4.4692216 -4.4705849 -4.4670892 -4.4411368 -4.3935561]]...]
INFO - root - 2017-12-08 01:50:37.245984: step 66810, loss = 21.84, batch loss = 21.76 (9.0 examples/sec; 0.886 sec/batch; 65h:24m:34s remains)
INFO - root - 2017-12-08 01:50:46.664035: step 66820, loss = 21.46, batch loss = 21.38 (9.0 examples/sec; 0.887 sec/batch; 65h:27m:36s remains)
INFO - root - 2017-12-08 01:50:56.083022: step 66830, loss = 21.40, batch loss = 21.31 (8.2 examples/sec; 0.980 sec/batch; 72h:17m:22s remains)
INFO - root - 2017-12-08 01:51:05.476013: step 66840, loss = 21.80, batch loss = 21.72 (8.1 examples/sec; 0.989 sec/batch; 72h:56m:49s remains)
INFO - root - 2017-12-08 01:51:14.864692: step 66850, loss = 20.95, batch loss = 20.87 (8.1 examples/sec; 0.983 sec/batch; 72h:30m:12s remains)
INFO - root - 2017-12-08 01:51:24.339499: step 66860, loss = 21.06, batch loss = 20.98 (8.6 examples/sec; 0.929 sec/batch; 68h:31m:46s remains)
INFO - root - 2017-12-08 01:51:33.850653: step 66870, loss = 20.82, batch loss = 20.74 (8.6 examples/sec; 0.931 sec/batch; 68h:41m:07s remains)
INFO - root - 2017-12-08 01:51:43.201281: step 66880, loss = 21.38, batch loss = 21.30 (8.5 examples/sec; 0.937 sec/batch; 69h:07m:32s remains)
INFO - root - 2017-12-08 01:51:52.580560: step 66890, loss = 21.15, batch loss = 21.07 (8.9 examples/sec; 0.900 sec/batch; 66h:24m:07s remains)
INFO - root - 2017-12-08 01:52:01.918775: step 66900, loss = 21.28, batch loss = 21.20 (9.1 examples/sec; 0.883 sec/batch; 65h:09m:19s remains)
2017-12-08 01:52:02.908101: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5031953 -4.5434623 -4.5655084 -4.5356712 -4.4267249 -4.2639647 -4.0512075 -3.8952546 -3.9126332 -4.0435009 -4.2079988 -4.3263216 -4.3711615 -4.3846331 -4.4190807][-4.5222416 -4.5720549 -4.5948515 -4.5556774 -4.4399562 -4.2760296 -4.0500736 -3.8670688 -3.8638811 -3.9955454 -4.1721959 -4.3063421 -4.365181 -4.3825865 -4.4083915][-4.5310864 -4.5792913 -4.5953007 -4.5528693 -4.4468212 -4.3058534 -4.1104369 -3.9373424 -3.9102428 -4.0139041 -4.1745291 -4.3121619 -4.38873 -4.4153018 -4.4271641][-4.5331168 -4.5741587 -4.5771785 -4.5274763 -4.4251151 -4.3119273 -4.1718273 -4.0418067 -4.0066905 -4.0764122 -4.2035074 -4.3286242 -4.4159775 -4.4565887 -4.4638486][-4.5188084 -4.5464363 -4.5319614 -4.464788 -4.3439527 -4.2372317 -4.1475725 -4.0798764 -4.0666971 -4.117321 -4.209507 -4.3142242 -4.4034419 -4.4590397 -4.477159][-4.4895525 -4.5052757 -4.4694772 -4.3738527 -4.2175531 -4.08124 -4.0090628 -4.0029316 -4.04052 -4.1041017 -4.1828265 -4.2719412 -4.3528237 -4.4127851 -4.4476409][-4.4483724 -4.4621568 -4.4156055 -4.3031015 -4.1197033 -3.940268 -3.8557067 -3.8919232 -3.9871006 -4.0839095 -4.169868 -4.2483768 -4.3093987 -4.3523602 -4.3914771][-4.4102888 -4.4358191 -4.4013057 -4.2996111 -4.1225762 -3.9254472 -3.81453 -3.8585284 -3.9846549 -4.1078486 -4.2053461 -4.27337 -4.3061104 -4.3135519 -4.3300843][-4.382195 -4.4255157 -4.421267 -4.3520603 -4.2096477 -4.0305767 -3.9011831 -3.9129944 -4.0300326 -4.1644163 -4.2735105 -4.3380365 -4.3455286 -4.3130631 -4.2924609][-4.3565664 -4.4149942 -4.4447336 -4.4125767 -4.3115611 -4.1701908 -4.03702 -3.9999602 -4.0833817 -4.2192974 -4.3394566 -4.4091659 -4.4053836 -4.3445964 -4.2904468][-4.3269134 -4.3909431 -4.4461479 -4.4473696 -4.38122 -4.2816253 -4.1664176 -4.0957646 -4.1396656 -4.2586908 -4.3732891 -4.4443827 -4.4420366 -4.3779368 -4.3122325][-4.3091435 -4.3635035 -4.4277692 -4.4553065 -4.4109788 -4.3455868 -4.2649226 -4.1889734 -4.2022433 -4.2899203 -4.3816495 -4.4467115 -4.4522681 -4.3997178 -4.3328614][-4.3195 -4.3520231 -4.40563 -4.4426837 -4.40544 -4.3605375 -4.3182087 -4.2598944 -4.2583447 -4.313333 -4.3722119 -4.4198771 -4.4262819 -4.383872 -4.3162246][-4.3495183 -4.3545766 -4.3796329 -4.4077392 -4.3709955 -4.3351846 -4.3243318 -4.2970824 -4.298418 -4.329586 -4.3559933 -4.3795233 -4.3750129 -4.3345389 -4.2716107][-4.380517 -4.3644505 -4.3573165 -4.3653059 -4.3261981 -4.2934775 -4.3001223 -4.298264 -4.3081264 -4.3288326 -4.3387365 -4.3503046 -4.340764 -4.29988 -4.2446523]]...]
INFO - root - 2017-12-08 01:52:12.522266: step 66910, loss = 22.05, batch loss = 21.97 (8.1 examples/sec; 0.982 sec/batch; 72h:25m:12s remains)
INFO - root - 2017-12-08 01:52:22.055344: step 66920, loss = 21.94, batch loss = 21.86 (8.6 examples/sec; 0.927 sec/batch; 68h:23m:30s remains)
INFO - root - 2017-12-08 01:52:31.276917: step 66930, loss = 21.59, batch loss = 21.51 (9.2 examples/sec; 0.867 sec/batch; 63h:59m:06s remains)
INFO - root - 2017-12-08 01:52:40.649794: step 66940, loss = 21.51, batch loss = 21.42 (8.9 examples/sec; 0.901 sec/batch; 66h:28m:57s remains)
INFO - root - 2017-12-08 01:52:49.967874: step 66950, loss = 21.32, batch loss = 21.24 (8.6 examples/sec; 0.928 sec/batch; 68h:26m:29s remains)
INFO - root - 2017-12-08 01:52:59.494307: step 66960, loss = 21.64, batch loss = 21.56 (8.2 examples/sec; 0.975 sec/batch; 71h:54m:21s remains)
INFO - root - 2017-12-08 01:53:08.873848: step 66970, loss = 21.44, batch loss = 21.36 (8.4 examples/sec; 0.949 sec/batch; 70h:01m:26s remains)
INFO - root - 2017-12-08 01:53:18.072675: step 66980, loss = 21.81, batch loss = 21.72 (8.1 examples/sec; 0.985 sec/batch; 72h:41m:00s remains)
INFO - root - 2017-12-08 01:53:27.321053: step 66990, loss = 21.54, batch loss = 21.46 (8.0 examples/sec; 1.004 sec/batch; 74h:01m:43s remains)
INFO - root - 2017-12-08 01:53:36.711055: step 67000, loss = 21.33, batch loss = 21.25 (8.3 examples/sec; 0.960 sec/batch; 70h:47m:43s remains)
2017-12-08 01:53:37.769699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3905907 -4.4388127 -4.508615 -4.5671248 -4.6028776 -4.5899258 -4.5363894 -4.4815021 -4.4394026 -4.4235806 -4.4372668 -4.4636207 -4.4931011 -4.5051532 -4.4912839][-4.4083004 -4.4566488 -4.5176082 -4.5704288 -4.6045547 -4.6003017 -4.5693932 -4.5394697 -4.5161581 -4.5049267 -4.5051641 -4.515306 -4.5379939 -4.5529404 -4.5485406][-4.4143806 -4.4610672 -4.5165372 -4.5634327 -4.5918608 -4.5841584 -4.5571756 -4.5416193 -4.5397639 -4.5435128 -4.5429158 -4.5434594 -4.5536456 -4.5611849 -4.5557442][-4.4093156 -4.4512067 -4.50296 -4.5437417 -4.5620189 -4.5376983 -4.4938211 -4.4734659 -4.4819069 -4.5034404 -4.515873 -4.5184951 -4.5195746 -4.5182767 -4.5080113][-4.3993683 -4.436553 -4.4832563 -4.5159612 -4.522275 -4.4795661 -4.4145851 -4.3820596 -4.390099 -4.4231105 -4.4540567 -4.4692416 -4.4740396 -4.4782858 -4.4763618][-4.390831 -4.4217405 -4.4558973 -4.4671149 -4.4444866 -4.3737864 -4.2896676 -4.25454 -4.2704039 -4.3237147 -4.3857136 -4.4288797 -4.452435 -4.4751492 -4.4936128][-4.3825922 -4.4036655 -4.4167695 -4.3933845 -4.32608 -4.21756 -4.115634 -4.0877538 -4.125824 -4.2130542 -4.3144703 -4.3906965 -4.4361234 -4.476862 -4.5138063][-4.3750315 -4.3873467 -4.3842564 -4.3323865 -4.2258949 -4.0797276 -3.9529569 -3.9211257 -3.9761195 -4.0952678 -4.2312984 -4.3361731 -4.3978357 -4.4445677 -4.4872227][-4.3719258 -4.3823848 -4.3810759 -4.3270688 -4.211555 -4.0481677 -3.8956366 -3.8416202 -3.8924158 -4.0241919 -4.1775279 -4.2945037 -4.3595567 -4.3965573 -4.4276767][-4.3735671 -4.3922768 -4.4114189 -4.3857417 -4.3006091 -4.1564045 -3.9957142 -3.9105213 -3.9278214 -4.0342727 -4.1721926 -4.27577 -4.3295875 -4.3534732 -4.3729258][-4.377327 -4.4052267 -4.4467044 -4.4578075 -4.4185085 -4.3177781 -4.174727 -4.0717115 -4.0496268 -4.1082377 -4.2043557 -4.2733335 -4.3039627 -4.3160677 -4.3314829][-4.3838196 -4.4172325 -4.4713511 -4.5083237 -4.5069604 -4.4540424 -4.3520131 -4.2614536 -4.2218509 -4.2395344 -4.2876039 -4.3138485 -4.3165917 -4.3146648 -4.3241954][-4.3947248 -4.4305849 -4.4877934 -4.5382662 -4.5628433 -4.5511274 -4.4991283 -4.4427438 -4.40988 -4.4054818 -4.4141769 -4.4055152 -4.3884726 -4.3743792 -4.37228][-4.4060349 -4.44593 -4.5049491 -4.5629864 -4.6034746 -4.61997 -4.609055 -4.5906062 -4.5799403 -4.5759706 -4.565793 -4.537735 -4.5099988 -4.4854069 -4.4673138][-4.4113131 -4.45893 -4.5231948 -4.5886073 -4.6392794 -4.6681938 -4.6748872 -4.6783209 -4.6893759 -4.7003627 -4.6954494 -4.6700835 -4.640543 -4.6045032 -4.5651054]]...]
INFO - root - 2017-12-08 01:53:47.080979: step 67010, loss = 21.41, batch loss = 21.32 (9.3 examples/sec; 0.864 sec/batch; 63h:41m:41s remains)
INFO - root - 2017-12-08 01:53:56.544345: step 67020, loss = 21.49, batch loss = 21.40 (8.5 examples/sec; 0.940 sec/batch; 69h:17m:43s remains)
INFO - root - 2017-12-08 01:54:06.058703: step 67030, loss = 20.93, batch loss = 20.84 (8.3 examples/sec; 0.965 sec/batch; 71h:11m:07s remains)
INFO - root - 2017-12-08 01:54:15.447627: step 67040, loss = 21.73, batch loss = 21.65 (8.4 examples/sec; 0.950 sec/batch; 70h:02m:01s remains)
INFO - root - 2017-12-08 01:54:24.952903: step 67050, loss = 21.35, batch loss = 21.26 (8.4 examples/sec; 0.956 sec/batch; 70h:28m:58s remains)
INFO - root - 2017-12-08 01:54:34.431894: step 67060, loss = 21.27, batch loss = 21.19 (8.5 examples/sec; 0.938 sec/batch; 69h:11m:29s remains)
INFO - root - 2017-12-08 01:54:43.804669: step 67070, loss = 21.30, batch loss = 21.22 (8.4 examples/sec; 0.949 sec/batch; 69h:59m:02s remains)
INFO - root - 2017-12-08 01:54:53.283460: step 67080, loss = 21.41, batch loss = 21.33 (8.7 examples/sec; 0.916 sec/batch; 67h:32m:50s remains)
INFO - root - 2017-12-08 01:55:02.653737: step 67090, loss = 21.34, batch loss = 21.26 (8.7 examples/sec; 0.921 sec/batch; 67h:53m:01s remains)
INFO - root - 2017-12-08 01:55:11.998087: step 67100, loss = 21.55, batch loss = 21.46 (8.4 examples/sec; 0.958 sec/batch; 70h:36m:55s remains)
2017-12-08 01:55:12.931043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5917249 -4.4565525 -4.3359036 -4.2506638 -4.2378464 -4.3452616 -4.545855 -4.7201457 -4.7927384 -4.6895447 -4.4769635 -4.27076 -4.1229234 -4.0040197 -3.9238105][-4.6071815 -4.420085 -4.2534676 -4.1494708 -4.1366692 -4.2473745 -4.4449224 -4.6137848 -4.7022605 -4.6515722 -4.4873047 -4.2849474 -4.1062841 -3.9396696 -3.8096397][-4.5769076 -4.3909793 -4.2360363 -4.1660943 -4.1694293 -4.2569003 -4.4197474 -4.5728159 -4.6746221 -4.6870713 -4.5969834 -4.4182205 -4.2084622 -3.9904017 -3.8012834][-4.571332 -4.4486823 -4.3469143 -4.3199987 -4.3114114 -4.3196568 -4.3981061 -4.5189409 -4.6335177 -4.7036233 -4.695631 -4.5679054 -4.3636308 -4.1354032 -3.9260869][-4.5805469 -4.5226316 -4.4479785 -4.4193826 -4.3718824 -4.2918983 -4.273982 -4.3574071 -4.4863658 -4.606914 -4.6813078 -4.6403389 -4.4970098 -4.3105717 -4.1353397][-4.550334 -4.5520015 -4.4882636 -4.4249687 -4.3273969 -4.172431 -4.0595 -4.0849228 -4.2109833 -4.3733683 -4.5295634 -4.5944591 -4.5429406 -4.4256716 -4.3121042][-4.5067167 -4.5722761 -4.5250173 -4.4092135 -4.2369509 -4.0084929 -3.8126445 -3.7701604 -3.8783164 -4.0790381 -4.3147349 -4.4795113 -4.5144892 -4.4673724 -4.416142][-4.437356 -4.5651979 -4.5552406 -4.4098516 -4.1759949 -3.8951516 -3.6460292 -3.5496688 -3.6345167 -3.860991 -4.1572828 -4.3899059 -4.4804106 -4.4814496 -4.4776669][-4.3456573 -4.5263076 -4.5800705 -4.4608555 -4.2229218 -3.9349015 -3.666677 -3.534411 -3.591958 -3.8170738 -4.1349497 -4.3872967 -4.4843011 -4.487618 -4.4919167][-4.3003049 -4.497252 -4.5891519 -4.5144119 -4.3209248 -4.0825176 -3.8494549 -3.7199812 -3.7562952 -3.9474752 -4.2305412 -4.4526062 -4.5239911 -4.50657 -4.4948745][-4.3184724 -4.4846077 -4.5766635 -4.5417295 -4.4159093 -4.2590179 -4.0963945 -3.9972672 -4.0165191 -4.1492238 -4.356688 -4.524549 -4.5731373 -4.5456877 -4.5160928][-4.3955421 -4.5186644 -4.593163 -4.5846734 -4.5187011 -4.4322262 -4.3298631 -4.2528977 -4.2485423 -4.3253388 -4.4606657 -4.5752368 -4.6039095 -4.5680637 -4.5183516][-4.5021634 -4.578217 -4.62659 -4.6336718 -4.6157923 -4.585742 -4.5334539 -4.4735827 -4.4437385 -4.4695435 -4.5407777 -4.6034617 -4.6072626 -4.561626 -4.5007191][-4.543407 -4.5842152 -4.6213961 -4.6487865 -4.6668181 -4.6733689 -4.6590672 -4.6230164 -4.5837955 -4.5684061 -4.5821409 -4.5969081 -4.5795574 -4.5312452 -4.4735155][-4.5009236 -4.5251141 -4.5609612 -4.6026969 -4.6386604 -4.6582956 -4.6591692 -4.6401882 -4.6072383 -4.5763755 -4.5563149 -4.5392146 -4.5084338 -4.4630461 -4.4148927]]...]
INFO - root - 2017-12-08 01:55:22.258160: step 67110, loss = 21.28, batch loss = 21.20 (8.1 examples/sec; 0.987 sec/batch; 72h:44m:04s remains)
INFO - root - 2017-12-08 01:55:31.722463: step 67120, loss = 21.21, batch loss = 21.12 (8.6 examples/sec; 0.936 sec/batch; 68h:58m:28s remains)
INFO - root - 2017-12-08 01:55:41.122552: step 67130, loss = 21.48, batch loss = 21.40 (9.6 examples/sec; 0.829 sec/batch; 61h:06m:54s remains)
INFO - root - 2017-12-08 01:55:50.563400: step 67140, loss = 21.46, batch loss = 21.38 (8.9 examples/sec; 0.898 sec/batch; 66h:11m:50s remains)
INFO - root - 2017-12-08 01:55:59.560743: step 67150, loss = 21.41, batch loss = 21.33 (8.5 examples/sec; 0.943 sec/batch; 69h:32m:07s remains)
INFO - root - 2017-12-08 01:56:09.051675: step 67160, loss = 21.66, batch loss = 21.58 (8.4 examples/sec; 0.952 sec/batch; 70h:10m:07s remains)
INFO - root - 2017-12-08 01:56:18.377073: step 67170, loss = 21.50, batch loss = 21.41 (8.4 examples/sec; 0.957 sec/batch; 70h:31m:42s remains)
INFO - root - 2017-12-08 01:56:27.658672: step 67180, loss = 21.30, batch loss = 21.21 (8.3 examples/sec; 0.967 sec/batch; 71h:15m:31s remains)
INFO - root - 2017-12-08 01:56:36.398546: step 67190, loss = 21.21, batch loss = 21.12 (8.1 examples/sec; 0.984 sec/batch; 72h:32m:19s remains)
