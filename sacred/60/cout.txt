INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "60"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 07:49:41.442476: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:49:41.442507: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:49:41.442513: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:49:41.442517: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:49:41.442521: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:49:42.002176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-05 07:49:42.002214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 07:49:42.002221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 07:49:42.002340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 07:49:45.694343: step 0, loss = 2.03, batch loss = 1.97 (3.1 examples/sec; 2.562 sec/batch; 236h:36m:04s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 07:49:48.752729: step 10, loss = 2.04, batch loss = 1.98 (36.9 examples/sec; 0.217 sec/batch; 20h:02m:50s remains)
INFO - root - 2017-12-05 07:49:50.948428: step 20, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.212 sec/batch; 19h:34m:04s remains)
INFO - root - 2017-12-05 07:49:53.149829: step 30, loss = 2.03, batch loss = 1.98 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:57s remains)
INFO - root - 2017-12-05 07:49:55.400368: step 40, loss = 2.02, batch loss = 1.96 (37.9 examples/sec; 0.211 sec/batch; 19h:29m:39s remains)
INFO - root - 2017-12-05 07:49:57.524129: step 50, loss = 2.03, batch loss = 1.97 (37.2 examples/sec; 0.215 sec/batch; 19h:52m:22s remains)
INFO - root - 2017-12-05 07:49:59.659468: step 60, loss = 2.00, batch loss = 1.94 (38.0 examples/sec; 0.211 sec/batch; 19h:27m:55s remains)
INFO - root - 2017-12-05 07:50:01.764396: step 70, loss = 2.00, batch loss = 1.94 (38.5 examples/sec; 0.208 sec/batch; 19h:11m:33s remains)
INFO - root - 2017-12-05 07:50:03.896667: step 80, loss = 2.04, batch loss = 1.98 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:16s remains)
INFO - root - 2017-12-05 07:50:06.058385: step 90, loss = 2.01, batch loss = 1.95 (33.3 examples/sec; 0.240 sec/batch; 22h:09m:23s remains)
INFO - root - 2017-12-05 07:50:08.204853: step 100, loss = 2.01, batch loss = 1.95 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:22s remains)
INFO - root - 2017-12-05 07:50:10.435543: step 110, loss = 1.99, batch loss = 1.93 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:56s remains)
INFO - root - 2017-12-05 07:50:12.617891: step 120, loss = 2.03, batch loss = 1.97 (37.9 examples/sec; 0.211 sec/batch; 19h:30m:27s remains)
INFO - root - 2017-12-05 07:50:14.775087: step 130, loss = 2.02, batch loss = 1.96 (37.0 examples/sec; 0.216 sec/batch; 19h:58m:42s remains)
INFO - root - 2017-12-05 07:50:16.925132: step 140, loss = 1.99, batch loss = 1.93 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:37s remains)
INFO - root - 2017-12-05 07:50:19.067488: step 150, loss = 1.97, batch loss = 1.91 (38.2 examples/sec; 0.209 sec/batch; 19h:20m:23s remains)
INFO - root - 2017-12-05 07:50:21.235823: step 160, loss = 2.01, batch loss = 1.95 (36.4 examples/sec; 0.220 sec/batch; 20h:18m:54s remains)
INFO - root - 2017-12-05 07:50:23.365912: step 170, loss = 1.98, batch loss = 1.92 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:32s remains)
INFO - root - 2017-12-05 07:50:25.509668: step 180, loss = 1.95, batch loss = 1.90 (36.8 examples/sec; 0.217 sec/batch; 20h:03m:10s remains)
INFO - root - 2017-12-05 07:50:27.656625: step 190, loss = 1.90, batch loss = 1.84 (37.5 examples/sec; 0.213 sec/batch; 19h:42m:28s remains)
INFO - root - 2017-12-05 07:50:29.800209: step 200, loss = 1.92, batch loss = 1.86 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:21s remains)
INFO - root - 2017-12-05 07:50:32.000880: step 210, loss = 1.94, batch loss = 1.88 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:47s remains)
INFO - root - 2017-12-05 07:50:34.135347: step 220, loss = 1.97, batch loss = 1.91 (38.1 examples/sec; 0.210 sec/batch; 19h:23m:08s remains)
INFO - root - 2017-12-05 07:50:36.305735: step 230, loss = 1.94, batch loss = 1.88 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:10s remains)
INFO - root - 2017-12-05 07:50:38.452386: step 240, loss = 1.96, batch loss = 1.90 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:13s remains)
INFO - root - 2017-12-05 07:50:40.606832: step 250, loss = 1.91, batch loss = 1.85 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:05s remains)
INFO - root - 2017-12-05 07:50:42.763788: step 260, loss = 1.91, batch loss = 1.86 (37.3 examples/sec; 0.214 sec/batch; 19h:47m:31s remains)
INFO - root - 2017-12-05 07:50:44.897519: step 270, loss = 1.89, batch loss = 1.84 (38.3 examples/sec; 0.209 sec/batch; 19h:18m:04s remains)
INFO - root - 2017-12-05 07:50:47.042491: step 280, loss = 1.84, batch loss = 1.79 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:19s remains)
INFO - root - 2017-12-05 07:50:49.192588: step 290, loss = 1.79, batch loss = 1.73 (38.9 examples/sec; 0.206 sec/batch; 18h:59m:00s remains)
INFO - root - 2017-12-05 07:50:51.337930: step 300, loss = 1.85, batch loss = 1.79 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:20s remains)
INFO - root - 2017-12-05 07:50:53.552595: step 310, loss = 1.87, batch loss = 1.82 (36.3 examples/sec; 0.221 sec/batch; 20h:21m:34s remains)
INFO - root - 2017-12-05 07:50:55.727383: step 320, loss = 1.82, batch loss = 1.76 (36.6 examples/sec; 0.218 sec/batch; 20h:09m:05s remains)
INFO - root - 2017-12-05 07:50:57.878123: step 330, loss = 1.77, batch loss = 1.71 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:39s remains)
INFO - root - 2017-12-05 07:51:00.015158: step 340, loss = 1.76, batch loss = 1.70 (38.1 examples/sec; 0.210 sec/batch; 19h:23m:22s remains)
INFO - root - 2017-12-05 07:51:02.188843: step 350, loss = 1.82, batch loss = 1.76 (38.1 examples/sec; 0.210 sec/batch; 19h:21m:04s remains)
INFO - root - 2017-12-05 07:51:04.339584: step 360, loss = 1.81, batch loss = 1.75 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:21s remains)
INFO - root - 2017-12-05 07:51:06.471963: step 370, loss = 1.86, batch loss = 1.80 (38.2 examples/sec; 0.209 sec/batch; 19h:18m:00s remains)
INFO - root - 2017-12-05 07:51:08.626189: step 380, loss = 1.84, batch loss = 1.78 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:11s remains)
INFO - root - 2017-12-05 07:51:10.759886: step 390, loss = 1.70, batch loss = 1.64 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:53s remains)
INFO - root - 2017-12-05 07:51:12.906078: step 400, loss = 1.64, batch loss = 1.58 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:38s remains)
INFO - root - 2017-12-05 07:51:15.148321: step 410, loss = 1.71, batch loss = 1.65 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:58s remains)
INFO - root - 2017-12-05 07:51:17.300854: step 420, loss = 1.77, batch loss = 1.71 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:28s remains)
INFO - root - 2017-12-05 07:51:19.431070: step 430, loss = 1.71, batch loss = 1.65 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:06s remains)
INFO - root - 2017-12-05 07:51:21.587095: step 440, loss = 1.73, batch loss = 1.67 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:24s remains)
INFO - root - 2017-12-05 07:51:23.747033: step 450, loss = 1.77, batch loss = 1.71 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:57s remains)
INFO - root - 2017-12-05 07:51:25.948312: step 460, loss = 1.49, batch loss = 1.43 (37.3 examples/sec; 0.214 sec/batch; 19h:46m:35s remains)
INFO - root - 2017-12-05 07:51:28.122625: step 470, loss = 1.65, batch loss = 1.60 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:52s remains)
INFO - root - 2017-12-05 07:51:30.294922: step 480, loss = 1.71, batch loss = 1.65 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:58s remains)
INFO - root - 2017-12-05 07:51:32.454805: step 490, loss = 1.65, batch loss = 1.59 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:44s remains)
INFO - root - 2017-12-05 07:51:34.623124: step 500, loss = 1.71, batch loss = 1.65 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:25s remains)
INFO - root - 2017-12-05 07:51:36.854596: step 510, loss = 1.61, batch loss = 1.55 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:38s remains)
INFO - root - 2017-12-05 07:51:38.999318: step 520, loss = 1.61, batch loss = 1.55 (38.0 examples/sec; 0.210 sec/batch; 19h:24m:08s remains)
INFO - root - 2017-12-05 07:51:41.170217: step 530, loss = 1.50, batch loss = 1.44 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:45s remains)
INFO - root - 2017-12-05 07:51:43.331308: step 540, loss = 1.59, batch loss = 1.53 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:46s remains)
INFO - root - 2017-12-05 07:51:45.494626: step 550, loss = 1.46, batch loss = 1.40 (36.3 examples/sec; 0.221 sec/batch; 20h:20m:03s remains)
INFO - root - 2017-12-05 07:51:47.651792: step 560, loss = 1.57, batch loss = 1.51 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:27s remains)
INFO - root - 2017-12-05 07:51:49.797086: step 570, loss = 1.55, batch loss = 1.50 (37.8 examples/sec; 0.211 sec/batch; 19h:29m:44s remains)
INFO - root - 2017-12-05 07:51:51.974425: step 580, loss = 1.43, batch loss = 1.37 (37.5 examples/sec; 0.214 sec/batch; 19h:41m:05s remains)
INFO - root - 2017-12-05 07:51:54.133383: step 590, loss = 1.52, batch loss = 1.46 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:01s remains)
INFO - root - 2017-12-05 07:51:56.303443: step 600, loss = 1.55, batch loss = 1.49 (37.1 examples/sec; 0.216 sec/batch; 19h:52m:48s remains)
INFO - root - 2017-12-05 07:51:58.515597: step 610, loss = 1.67, batch loss = 1.61 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:08s remains)
INFO - root - 2017-12-05 07:52:00.666959: step 620, loss = 1.47, batch loss = 1.41 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:17s remains)
INFO - root - 2017-12-05 07:52:02.846568: step 630, loss = 1.50, batch loss = 1.44 (36.5 examples/sec; 0.219 sec/batch; 20h:13m:38s remains)
INFO - root - 2017-12-05 07:52:04.987768: step 640, loss = 1.41, batch loss = 1.36 (36.8 examples/sec; 0.218 sec/batch; 20h:03m:17s remains)
INFO - root - 2017-12-05 07:52:07.161463: step 650, loss = 1.44, batch loss = 1.38 (36.1 examples/sec; 0.221 sec/batch; 20h:24m:43s remains)
INFO - root - 2017-12-05 07:52:09.340269: step 660, loss = 1.36, batch loss = 1.30 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:53s remains)
INFO - root - 2017-12-05 07:52:11.491568: step 670, loss = 1.35, batch loss = 1.29 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:34s remains)
INFO - root - 2017-12-05 07:52:13.666882: step 680, loss = 1.50, batch loss = 1.44 (37.5 examples/sec; 0.214 sec/batch; 19h:40m:47s remains)
INFO - root - 2017-12-05 07:52:15.872880: step 690, loss = 1.35, batch loss = 1.29 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:16s remains)
INFO - root - 2017-12-05 07:52:18.056427: step 700, loss = 1.28, batch loss = 1.23 (36.6 examples/sec; 0.218 sec/batch; 20h:07m:47s remains)
INFO - root - 2017-12-05 07:52:20.300269: step 710, loss = 1.48, batch loss = 1.42 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:23s remains)
INFO - root - 2017-12-05 07:52:22.492878: step 720, loss = 1.44, batch loss = 1.38 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:12s remains)
INFO - root - 2017-12-05 07:52:24.683150: step 730, loss = 1.56, batch loss = 1.50 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:14s remains)
INFO - root - 2017-12-05 07:52:26.847416: step 740, loss = 1.38, batch loss = 1.32 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:44s remains)
INFO - root - 2017-12-05 07:52:29.024914: step 750, loss = 1.46, batch loss = 1.40 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:44s remains)
INFO - root - 2017-12-05 07:52:31.241190: step 760, loss = 1.57, batch loss = 1.51 (36.3 examples/sec; 0.220 sec/batch; 20h:17m:08s remains)
INFO - root - 2017-12-05 07:52:33.414317: step 770, loss = 1.42, batch loss = 1.36 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:49s remains)
INFO - root - 2017-12-05 07:52:35.565710: step 780, loss = 1.40, batch loss = 1.34 (36.6 examples/sec; 0.218 sec/batch; 20h:08m:00s remains)
INFO - root - 2017-12-05 07:52:37.731697: step 790, loss = 1.60, batch loss = 1.54 (37.5 examples/sec; 0.213 sec/batch; 19h:38m:13s remains)
INFO - root - 2017-12-05 07:52:39.926114: step 800, loss = 1.39, batch loss = 1.33 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:11s remains)
INFO - root - 2017-12-05 07:52:42.156170: step 810, loss = 1.35, batch loss = 1.29 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:18s remains)
INFO - root - 2017-12-05 07:52:44.319071: step 820, loss = 1.53, batch loss = 1.47 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:50s remains)
INFO - root - 2017-12-05 07:52:46.464571: step 830, loss = 1.49, batch loss = 1.43 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:28s remains)
INFO - root - 2017-12-05 07:52:48.636446: step 840, loss = 1.40, batch loss = 1.34 (36.8 examples/sec; 0.218 sec/batch; 20h:02m:54s remains)
INFO - root - 2017-12-05 07:52:50.799976: step 850, loss = 1.45, batch loss = 1.39 (35.2 examples/sec; 0.227 sec/batch; 20h:56m:36s remains)
INFO - root - 2017-12-05 07:52:52.980324: step 860, loss = 1.44, batch loss = 1.39 (35.1 examples/sec; 0.228 sec/batch; 21h:00m:09s remains)
INFO - root - 2017-12-05 07:52:55.163399: step 870, loss = 1.57, batch loss = 1.51 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:20s remains)
INFO - root - 2017-12-05 07:52:57.338349: step 880, loss = 1.71, batch loss = 1.65 (35.2 examples/sec; 0.227 sec/batch; 20h:54m:23s remains)
INFO - root - 2017-12-05 07:52:59.511315: step 890, loss = 1.42, batch loss = 1.36 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:21s remains)
INFO - root - 2017-12-05 07:53:01.665747: step 900, loss = 1.41, batch loss = 1.35 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:29s remains)
INFO - root - 2017-12-05 07:53:03.880848: step 910, loss = 1.69, batch loss = 1.63 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:35s remains)
INFO - root - 2017-12-05 07:53:06.040309: step 920, loss = 1.69, batch loss = 1.63 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:15s remains)
INFO - root - 2017-12-05 07:53:08.193327: step 930, loss = 1.54, batch loss = 1.48 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:43s remains)
INFO - root - 2017-12-05 07:53:10.410293: step 940, loss = 1.46, batch loss = 1.40 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:41s remains)
INFO - root - 2017-12-05 07:53:12.571542: step 950, loss = 1.51, batch loss = 1.46 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:20s remains)
INFO - root - 2017-12-05 07:53:14.722060: step 960, loss = 1.65, batch loss = 1.59 (37.8 examples/sec; 0.211 sec/batch; 19h:28m:06s remains)
INFO - root - 2017-12-05 07:53:16.895701: step 970, loss = 1.57, batch loss = 1.51 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:32s remains)
INFO - root - 2017-12-05 07:53:19.092719: step 980, loss = 1.82, batch loss = 1.76 (37.5 examples/sec; 0.214 sec/batch; 19h:40m:01s remains)
INFO - root - 2017-12-05 07:53:21.264259: step 990, loss = 1.57, batch loss = 1.51 (36.2 examples/sec; 0.221 sec/batch; 20h:20m:09s remains)
INFO - root - 2017-12-05 07:53:23.424675: step 1000, loss = 1.55, batch loss = 1.49 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:51s remains)
INFO - root - 2017-12-05 07:53:25.757629: step 1010, loss = 1.47, batch loss = 1.41 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:40s remains)
INFO - root - 2017-12-05 07:53:27.938811: step 1020, loss = 1.51, batch loss = 1.45 (36.6 examples/sec; 0.219 sec/batch; 20h:08m:27s remains)
INFO - root - 2017-12-05 07:53:30.101972: step 1030, loss = 1.61, batch loss = 1.56 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:05s remains)
INFO - root - 2017-12-05 07:53:32.264370: step 1040, loss = 1.66, batch loss = 1.60 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:07s remains)
INFO - root - 2017-12-05 07:53:34.407077: step 1050, loss = 1.79, batch loss = 1.73 (37.3 examples/sec; 0.215 sec/batch; 19h:45m:29s remains)
INFO - root - 2017-12-05 07:53:36.563397: step 1060, loss = 1.64, batch loss = 1.58 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:40s remains)
INFO - root - 2017-12-05 07:53:38.754105: step 1070, loss = 1.74, batch loss = 1.68 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:22s remains)
