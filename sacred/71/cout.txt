INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "71"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 10:21:39.239721: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:21:39.239769: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:21:39.239775: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:21:39.239779: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:21:39.239783: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:21:39.829391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-05 10:21:39.829431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 10:21:39.829438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 10:21:39.829446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 10:21:42.883768: step 0, loss = 2.03, batch loss = 1.97 (3.4 examples/sec; 2.351 sec/batch; 217h:08m:14s remains)
2017-12-05 10:21:43.588219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3331933 -4.3318019 -4.3317528 -4.3317 -4.3295326 -4.3214293 -4.3082857 -4.2892895 -4.2688313 -4.2578382 -4.2585726 -4.2679262 -4.2873669 -4.3119473 -4.3317113][-4.3332195 -4.3318491 -4.3320379 -4.3304758 -4.3246579 -4.3084488 -4.2840347 -4.2534056 -4.2249608 -4.2127461 -4.2181363 -4.2352543 -4.2644267 -4.2973995 -4.3219266][-4.3285847 -4.3278403 -4.3280034 -4.3237038 -4.3087316 -4.2783771 -4.2365093 -4.1896181 -4.15102 -4.1390519 -4.153317 -4.1860366 -4.2314496 -4.27611 -4.3080492][-4.3171282 -4.3184209 -4.3193717 -4.3123507 -4.28298 -4.2332845 -4.1685452 -4.0966344 -4.0420589 -4.0304236 -4.0600076 -4.1146145 -4.1847472 -4.2481503 -4.2920041][-4.3000531 -4.3040843 -4.3073764 -4.2990627 -4.2588854 -4.194077 -4.1092072 -4.0069914 -3.9277191 -3.9185 -3.9690986 -4.0459743 -4.1411657 -4.2227263 -4.2781157][-4.2812881 -4.2882886 -4.2926321 -4.2808924 -4.2308626 -4.15072 -4.0393076 -3.9001589 -3.787677 -3.7910492 -3.8741815 -3.9779 -4.0982881 -4.1967731 -4.26482][-4.261662 -4.2696548 -4.2721529 -4.2540684 -4.19573 -4.1037025 -3.9665775 -3.7877908 -3.6468596 -3.6828482 -3.8111906 -3.9421115 -4.0762057 -4.1819715 -4.2564645][-4.2476029 -4.2516413 -4.2482662 -4.2232842 -4.1629457 -4.0676918 -3.9199419 -3.7254469 -3.5930383 -3.6768584 -3.8307967 -3.9647329 -4.0895433 -4.1886773 -4.2609429][-4.2382784 -4.2341232 -4.2239661 -4.1973238 -4.1499863 -4.076663 -3.9597507 -3.808538 -3.7324963 -3.8223505 -3.9426665 -4.0393848 -4.1300683 -4.2094083 -4.2720876][-4.2306433 -4.2179503 -4.203742 -4.18169 -4.1534319 -4.1094866 -4.0367317 -3.943495 -3.9102173 -3.9756243 -4.0514245 -4.10835 -4.1674676 -4.2291055 -4.283329][-4.2240348 -4.2072396 -4.1922026 -4.1753826 -4.160924 -4.1396933 -4.1007342 -4.0482221 -4.0346608 -4.0760689 -4.1196752 -4.1489367 -4.1876349 -4.2407722 -4.2918887][-4.2196083 -4.2025571 -4.1885562 -4.1773505 -4.1696329 -4.1606731 -4.1403551 -4.1120982 -4.1085768 -4.1329284 -4.156415 -4.1712971 -4.2024827 -4.2530918 -4.3008471][-4.2244492 -4.212781 -4.2039242 -4.1986113 -4.1962051 -4.1942472 -4.1833243 -4.1658278 -4.1627517 -4.1685243 -4.1772051 -4.1849818 -4.214922 -4.2656507 -4.3096151][-4.2400432 -4.2391295 -4.2389874 -4.2393737 -4.2391129 -4.2369866 -4.2251234 -4.2048059 -4.1946468 -4.1868267 -4.1841226 -4.188663 -4.2200165 -4.2717757 -4.3140235][-4.246747 -4.2526107 -4.2611094 -4.2675405 -4.2675648 -4.2621293 -4.2443981 -4.2166662 -4.1968942 -4.1818762 -4.1755986 -4.1832056 -4.218442 -4.2720838 -4.3140054]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 10:21:46.824230: step 10, loss = 2.06, batch loss = 2.00 (39.0 examples/sec; 0.205 sec/batch; 18h:56m:18s remains)
INFO - root - 2017-12-05 10:21:48.905123: step 20, loss = 2.05, batch loss = 1.99 (38.9 examples/sec; 0.205 sec/batch; 18h:58m:28s remains)
INFO - root - 2017-12-05 10:21:50.994485: step 30, loss = 2.05, batch loss = 1.99 (38.0 examples/sec; 0.210 sec/batch; 19h:26m:07s remains)
INFO - root - 2017-12-05 10:21:53.061548: step 40, loss = 2.08, batch loss = 2.02 (38.2 examples/sec; 0.209 sec/batch; 19h:19m:44s remains)
INFO - root - 2017-12-05 10:21:55.148548: step 50, loss = 2.07, batch loss = 2.01 (39.7 examples/sec; 0.202 sec/batch; 18h:37m:53s remains)
INFO - root - 2017-12-05 10:21:57.233898: step 60, loss = 2.10, batch loss = 2.04 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:21s remains)
INFO - root - 2017-12-05 10:21:59.332747: step 70, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:42m:30s remains)
INFO - root - 2017-12-05 10:22:01.432129: step 80, loss = 2.06, batch loss = 2.00 (38.8 examples/sec; 0.206 sec/batch; 19h:02m:41s remains)
INFO - root - 2017-12-05 10:22:03.516302: step 90, loss = 2.09, batch loss = 2.03 (38.2 examples/sec; 0.209 sec/batch; 19h:20m:12s remains)
INFO - root - 2017-12-05 10:22:05.625456: step 100, loss = 2.06, batch loss = 2.00 (39.0 examples/sec; 0.205 sec/batch; 18h:57m:33s remains)
2017-12-05 10:22:05.896200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.274303 -4.2605243 -4.2501965 -4.2345924 -4.2244186 -4.227951 -4.2433472 -4.257946 -4.2645297 -4.2557554 -4.2272253 -4.1770687 -4.1132951 -4.0495954 -3.9972167][-4.2904096 -4.2689409 -4.2460413 -4.2216554 -4.2110243 -4.2173386 -4.2346678 -4.2490582 -4.250349 -4.23357 -4.1979723 -4.1420426 -4.0768223 -4.0165977 -3.9741549][-4.2898412 -4.2713733 -4.2474728 -4.2202997 -4.2089152 -4.2129269 -4.2231193 -4.2283678 -4.2216697 -4.2000237 -4.1628985 -4.1165438 -4.072969 -4.0431471 -4.0333266][-4.277494 -4.2711506 -4.2551608 -4.2303352 -4.2150726 -4.2091503 -4.2026763 -4.190444 -4.1731625 -4.1504707 -4.1225328 -4.1021881 -4.0976892 -4.1102242 -4.1301346][-4.2690496 -4.2728148 -4.265101 -4.2437143 -4.2193136 -4.1939445 -4.1616311 -4.127439 -4.1022243 -4.0854321 -4.0799379 -4.0957985 -4.1303248 -4.1731386 -4.207624][-4.26721 -4.2728176 -4.2680359 -4.24606 -4.2089062 -4.1565576 -4.0945015 -4.0397744 -4.0122843 -4.014679 -4.0413933 -4.0884633 -4.1485829 -4.2087216 -4.2480855][-4.25643 -4.2561846 -4.2488494 -4.2253132 -4.180851 -4.10916 -4.0233765 -3.9528065 -3.9309049 -3.9660497 -4.03133 -4.100956 -4.1711216 -4.2336354 -4.2728524][-4.2326803 -4.2227554 -4.2129188 -4.1964226 -4.1599107 -4.09098 -4.0055261 -3.9381862 -3.9271009 -3.9865685 -4.07435 -4.1480742 -4.2098212 -4.2587891 -4.2895603][-4.1966214 -4.1760559 -4.1659737 -4.1641617 -4.152153 -4.1152897 -4.067029 -4.0342388 -4.0381289 -4.0847545 -4.152791 -4.2059956 -4.2449293 -4.2715778 -4.2864017][-4.148982 -4.12343 -4.1211762 -4.1447358 -4.1658163 -4.1693335 -4.1611528 -4.1562958 -4.1678205 -4.1943097 -4.2315226 -4.2573419 -4.2711453 -4.2730927 -4.2706928][-4.0910931 -4.0682688 -4.0832076 -4.1357908 -4.1873288 -4.2215614 -4.236269 -4.2433639 -4.2554107 -4.2693276 -4.2828727 -4.2850709 -4.27612 -4.2611766 -4.2476339][-4.0323076 -4.022572 -4.0586004 -4.13278 -4.2036939 -4.2509079 -4.2727184 -4.28305 -4.297091 -4.3067951 -4.3109283 -4.2962928 -4.2673526 -4.2382326 -4.2168517][-4.0049925 -4.0105152 -4.0611463 -4.1383247 -4.2105527 -4.2549133 -4.2736411 -4.2852826 -4.3023477 -4.3131309 -4.3120489 -4.2871461 -4.2469044 -4.211319 -4.1854687][-4.0327482 -4.0498567 -4.0980883 -4.1603808 -4.2181745 -4.2523937 -4.264369 -4.2749853 -4.2895327 -4.2964182 -4.2895441 -4.262866 -4.2250514 -4.1943045 -4.1727009][-4.1029034 -4.1248226 -4.1606727 -4.1999393 -4.236824 -4.2584457 -4.2641864 -4.270246 -4.2761269 -4.2758026 -4.2656517 -4.24517 -4.2187862 -4.2024345 -4.1942239]]...]
INFO - root - 2017-12-05 10:22:07.973579: step 110, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:24m:43s remains)
INFO - root - 2017-12-05 10:22:10.135868: step 120, loss = 2.04, batch loss = 1.98 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:33s remains)
INFO - root - 2017-12-05 10:22:12.262576: step 130, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:16s remains)
INFO - root - 2017-12-05 10:22:14.367049: step 140, loss = 2.05, batch loss = 2.00 (38.6 examples/sec; 0.208 sec/batch; 19h:09m:28s remains)
INFO - root - 2017-12-05 10:22:16.474079: step 150, loss = 2.11, batch loss = 2.05 (39.2 examples/sec; 0.204 sec/batch; 18h:50m:42s remains)
INFO - root - 2017-12-05 10:22:18.618555: step 160, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:39m:50s remains)
INFO - root - 2017-12-05 10:22:20.825989: step 170, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:21m:41s remains)
INFO - root - 2017-12-05 10:22:22.999377: step 180, loss = 2.06, batch loss = 2.01 (38.2 examples/sec; 0.209 sec/batch; 19h:18m:36s remains)
INFO - root - 2017-12-05 10:22:25.205892: step 190, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:45s remains)
INFO - root - 2017-12-05 10:22:27.374305: step 200, loss = 2.06, batch loss = 2.00 (38.5 examples/sec; 0.208 sec/batch; 19h:12m:16s remains)
2017-12-05 10:22:27.651040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.304667 -4.2717967 -4.2450933 -4.2337604 -4.2404027 -4.2460861 -4.2466717 -4.2480235 -4.2494574 -4.2461958 -4.2445703 -4.2536182 -4.259974 -4.2537875 -4.2377186][-4.2963324 -4.2679954 -4.2520218 -4.2532558 -4.2639837 -4.273839 -4.2729445 -4.26621 -4.2585912 -4.24772 -4.2381034 -4.2378788 -4.2350721 -4.2236962 -4.2062635][-4.2792797 -4.2590513 -4.2540584 -4.2641349 -4.2783275 -4.291604 -4.28936 -4.2770715 -4.2653193 -4.2505569 -4.2361073 -4.2282124 -4.2180285 -4.2040896 -4.1858549][-4.2542806 -4.2422996 -4.2453451 -4.2551003 -4.2710109 -4.2864022 -4.2856092 -4.27455 -4.2646542 -4.2543707 -4.2427974 -4.2359986 -4.2279873 -4.2162266 -4.195869][-4.2380304 -4.2288313 -4.2297778 -4.2301855 -4.2417889 -4.2519965 -4.2503228 -4.2478495 -4.251544 -4.2539921 -4.2520046 -4.2508349 -4.2496467 -4.2437634 -4.2203536][-4.2335076 -4.2206283 -4.2052422 -4.1877856 -4.1760449 -4.1605082 -4.1496668 -4.1665621 -4.2064724 -4.2378082 -4.2544446 -4.2666368 -4.27407 -4.2744017 -4.247848][-4.2483692 -4.2265677 -4.191081 -4.1446218 -4.0873103 -4.0164366 -3.9778252 -4.0306048 -4.1278882 -4.2031021 -4.2508383 -4.2799439 -4.2952967 -4.2966247 -4.2678928][-4.2725329 -4.2454348 -4.1940064 -4.1146884 -4.0021138 -3.8584204 -3.7748895 -3.8720751 -4.0320849 -4.1525517 -4.2324028 -4.2783589 -4.3024464 -4.300313 -4.2652044][-4.2908974 -4.2621756 -4.202651 -4.1105733 -3.9822083 -3.826787 -3.7366467 -3.8382514 -4.0041733 -4.1363282 -4.2303486 -4.2821956 -4.3085532 -4.2985349 -4.2502952][-4.3035131 -4.27555 -4.2170677 -4.13427 -4.032527 -3.9298215 -3.8795519 -3.9425535 -4.0579653 -4.1642222 -4.2414079 -4.2828069 -4.3001571 -4.2787929 -4.2192211][-4.3061237 -4.2817035 -4.2336578 -4.1700268 -4.1025753 -4.0537963 -4.038157 -4.0745254 -4.1427541 -4.2185016 -4.2717118 -4.2921395 -4.2906203 -4.2533588 -4.1865473][-4.3044343 -4.2865663 -4.2586975 -4.2222219 -4.1836357 -4.1676679 -4.16987 -4.191359 -4.2305131 -4.2788367 -4.308218 -4.3065715 -4.2853107 -4.2320833 -4.1566739][-4.3024654 -4.289474 -4.2813549 -4.2728596 -4.2612252 -4.2615485 -4.2699113 -4.2816563 -4.3017139 -4.3263097 -4.3335428 -4.3143067 -4.2795277 -4.21933 -4.1449394][-4.3019791 -4.2897644 -4.2913604 -4.3043752 -4.3153777 -4.3275647 -4.3358364 -4.3380451 -4.3427896 -4.3487153 -4.341331 -4.3180208 -4.2828865 -4.2274446 -4.1656466][-4.3041706 -4.2906957 -4.2897134 -4.3053112 -4.3239775 -4.3410745 -4.3507881 -4.3511729 -4.3490243 -4.3455734 -4.3336749 -4.3157253 -4.2873011 -4.2421479 -4.1994834]]...]
INFO - root - 2017-12-05 10:22:29.827682: step 210, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:37s remains)
INFO - root - 2017-12-05 10:22:31.964573: step 220, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:14m:24s remains)
INFO - root - 2017-12-05 10:22:34.090199: step 230, loss = 2.06, batch loss = 2.00 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:21s remains)
INFO - root - 2017-12-05 10:22:36.204400: step 240, loss = 2.04, batch loss = 1.99 (38.5 examples/sec; 0.208 sec/batch; 19h:09m:39s remains)
INFO - root - 2017-12-05 10:22:38.358291: step 250, loss = 2.06, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:20s remains)
INFO - root - 2017-12-05 10:22:40.486280: step 260, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:37s remains)
INFO - root - 2017-12-05 10:22:42.629176: step 270, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:50s remains)
INFO - root - 2017-12-05 10:22:44.745902: step 280, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:41s remains)
INFO - root - 2017-12-05 10:22:46.873000: step 290, loss = 2.07, batch loss = 2.02 (37.8 examples/sec; 0.211 sec/batch; 19h:30m:29s remains)
INFO - root - 2017-12-05 10:22:48.981297: step 300, loss = 2.08, batch loss = 2.02 (38.5 examples/sec; 0.208 sec/batch; 19h:09m:13s remains)
2017-12-05 10:22:49.282242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0685592 -4.05848 -4.066638 -4.0891371 -4.1243711 -4.1644449 -4.1930895 -4.1918664 -4.1656442 -4.1310072 -4.1110773 -4.1138368 -4.1230836 -4.1220546 -4.1198244][-4.14713 -4.1296992 -4.115067 -4.1103654 -4.1248317 -4.1611667 -4.19745 -4.2054205 -4.1906209 -4.171093 -4.1645422 -4.1689396 -4.1693668 -4.1557279 -4.1403251][-4.2155304 -4.1990581 -4.1689558 -4.1363993 -4.1293921 -4.1597 -4.1989 -4.2134037 -4.2112184 -4.2091146 -4.2112141 -4.2129059 -4.2045393 -4.1793947 -4.1488829][-4.2463145 -4.2327313 -4.1964531 -4.1469717 -4.1236115 -4.1474452 -4.1872592 -4.2043757 -4.2133412 -4.2244625 -4.2281737 -4.2233891 -4.20795 -4.1766376 -4.136385][-4.2504344 -4.2374864 -4.199717 -4.1432405 -4.1064754 -4.1178107 -4.1485038 -4.1654921 -4.1828732 -4.2037697 -4.2095566 -4.1941867 -4.1709771 -4.140069 -4.1026316][-4.2301917 -4.2214522 -4.1898861 -4.140255 -4.1030531 -4.1016955 -4.1172991 -4.130239 -4.1503015 -4.1706047 -4.1771 -4.1566124 -4.1267962 -4.0940647 -4.0632625][-4.1980634 -4.1935 -4.1713195 -4.1380768 -4.1131196 -4.1072664 -4.1116319 -4.1218081 -4.137907 -4.1477685 -4.1450219 -4.1189537 -4.0825238 -4.0441365 -4.0191183][-4.1719685 -4.1645403 -4.144846 -4.1264825 -4.1164718 -4.1135678 -4.1119084 -4.1199155 -4.133275 -4.1321888 -4.1174927 -4.0859547 -4.0439372 -3.9965982 -3.9733982][-4.1620517 -4.1525655 -4.1378279 -4.1309581 -4.1289535 -4.1241384 -4.1154342 -4.1187763 -4.1294217 -4.1217575 -4.103261 -4.0718732 -4.02963 -3.9812009 -3.9650607][-4.1602206 -4.1609383 -4.1629982 -4.1671505 -4.16998 -4.1650267 -4.1507468 -4.1452503 -4.145185 -4.1317806 -4.1144657 -4.0917783 -4.0578318 -4.0175772 -4.0103979][-4.1549611 -4.1726408 -4.189095 -4.2004862 -4.2071428 -4.2037644 -4.186542 -4.1740689 -4.1632342 -4.1473446 -4.1340265 -4.1251006 -4.1061974 -4.0794897 -4.0790777][-4.1414819 -4.1738205 -4.197526 -4.2118893 -4.222784 -4.2227216 -4.2055469 -4.1888413 -4.1737061 -4.1554875 -4.1448112 -4.1534858 -4.1532907 -4.1415057 -4.1441507][-4.1261964 -4.1687078 -4.1971626 -4.2140756 -4.2292366 -4.2312088 -4.2159848 -4.1998386 -4.1815538 -4.1595078 -4.1494842 -4.16901 -4.1812496 -4.1793723 -4.184957][-4.114922 -4.1516938 -4.1760902 -4.194787 -4.2138238 -4.2212634 -4.2158642 -4.2057605 -4.1861844 -4.1621261 -4.1537604 -4.1765785 -4.1939993 -4.1956844 -4.2006149][-4.1352644 -4.1529903 -4.16596 -4.1817584 -4.2003622 -4.211278 -4.2127275 -4.2090554 -4.19083 -4.1669512 -4.1582355 -4.1798868 -4.1971416 -4.1976032 -4.1998391]]...]
INFO - root - 2017-12-05 10:22:51.400329: step 310, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:09s remains)
INFO - root - 2017-12-05 10:22:53.516714: step 320, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.213 sec/batch; 19h:41m:56s remains)
INFO - root - 2017-12-05 10:22:55.640670: step 330, loss = 2.07, batch loss = 2.02 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:22s remains)
INFO - root - 2017-12-05 10:22:57.777869: step 340, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:38s remains)
INFO - root - 2017-12-05 10:22:59.886598: step 350, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:34s remains)
INFO - root - 2017-12-05 10:23:02.017926: step 360, loss = 2.09, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:39s remains)
INFO - root - 2017-12-05 10:23:04.169429: step 370, loss = 2.07, batch loss = 2.01 (37.9 examples/sec; 0.211 sec/batch; 19h:29m:02s remains)
INFO - root - 2017-12-05 10:23:06.323991: step 380, loss = 2.05, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:20s remains)
INFO - root - 2017-12-05 10:23:08.466804: step 390, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 20h:44m:09s remains)
INFO - root - 2017-12-05 10:23:10.602213: step 400, loss = 2.06, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:29m:30s remains)
2017-12-05 10:23:10.904664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16669 -4.1774597 -4.1880431 -4.1911039 -4.187501 -4.17611 -4.1848345 -4.1926322 -4.2012773 -4.2309241 -4.2631741 -4.2845378 -4.2952771 -4.29941 -4.2978845][-4.1355343 -4.165833 -4.1904578 -4.197648 -4.1891127 -4.1730347 -4.1714139 -4.1763444 -4.1876874 -4.2190852 -4.2524748 -4.2783709 -4.2931366 -4.2990746 -4.2960458][-4.1209474 -4.159544 -4.1903324 -4.201066 -4.1875219 -4.1629877 -4.142333 -4.1396637 -4.1624131 -4.20344 -4.2405334 -4.2702026 -4.2866464 -4.2921162 -4.2869873][-4.1600194 -4.1899114 -4.2019596 -4.1964397 -4.1696239 -4.1262097 -4.0748358 -4.0673962 -4.1196461 -4.1802044 -4.224999 -4.2568178 -4.2737851 -4.2753019 -4.2675619][-4.2146087 -4.2261124 -4.2076654 -4.1760054 -4.131598 -4.0559244 -3.9541235 -3.9491491 -4.052042 -4.1416316 -4.1904349 -4.2217383 -4.2378206 -4.2348905 -4.2224112][-4.2352982 -4.2321596 -4.1892371 -4.134757 -4.0718517 -3.9591684 -3.8013458 -3.7996407 -3.9547 -4.0732493 -4.1272984 -4.1597424 -4.1773548 -4.1698875 -4.1522012][-4.221982 -4.2172661 -4.1669822 -4.1035256 -4.0361271 -3.9126906 -3.7349284 -3.7218428 -3.8806219 -4.0021987 -4.060297 -4.0997095 -4.1184649 -4.1060143 -4.0819941][-4.2200785 -4.2185955 -4.174881 -4.1185126 -4.0648532 -3.9744136 -3.8454947 -3.8297787 -3.9293423 -4.0087552 -4.0517774 -4.0975451 -4.1168332 -4.0969853 -4.0652924][-4.2465825 -4.2459536 -4.2124028 -4.1654177 -4.1238079 -4.0692053 -4.0008907 -3.9932034 -4.0392303 -4.0762868 -4.1042919 -4.1509652 -4.1722517 -4.15545 -4.1286392][-4.2845564 -4.2832971 -4.2607145 -4.2259936 -4.1901984 -4.156435 -4.126574 -4.1211796 -4.1365871 -4.1523881 -4.1732883 -4.2163296 -4.2386489 -4.2307615 -4.2150078][-4.3095069 -4.3062553 -4.2933974 -4.2723131 -4.2411542 -4.2162013 -4.2038279 -4.1953335 -4.1947808 -4.2014995 -4.2195086 -4.2537632 -4.2737784 -4.2721119 -4.2634997][-4.3050461 -4.3021884 -4.2966824 -4.2843289 -4.256391 -4.2329235 -4.2245336 -4.2116346 -4.2023249 -4.2007957 -4.2138968 -4.2426896 -4.2630959 -4.2655754 -4.2597227][-4.271853 -4.2745752 -4.2772775 -4.2727814 -4.2511191 -4.23122 -4.2239089 -4.2081804 -4.1887627 -4.174859 -4.1793985 -4.2039318 -4.22114 -4.2220159 -4.2177162][-4.2234759 -4.2356815 -4.2480555 -4.2531085 -4.2413106 -4.2278891 -4.2221365 -4.2083864 -4.1846075 -4.1584997 -4.1520233 -4.1680083 -4.1770811 -4.1709609 -4.1634264][-4.1843214 -4.2009349 -4.2182627 -4.2332067 -4.2328677 -4.2267394 -4.2240272 -4.215858 -4.19475 -4.1673422 -4.1550689 -4.1595869 -4.1582522 -4.1460915 -4.1338844]]...]
INFO - root - 2017-12-05 10:23:13.047573: step 410, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:54s remains)
INFO - root - 2017-12-05 10:23:15.183005: step 420, loss = 2.05, batch loss = 1.99 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:17s remains)
INFO - root - 2017-12-05 10:23:17.333542: step 430, loss = 2.08, batch loss = 2.03 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:05s remains)
INFO - root - 2017-12-05 10:23:19.498214: step 440, loss = 2.07, batch loss = 2.01 (38.7 examples/sec; 0.207 sec/batch; 19h:04m:56s remains)
INFO - root - 2017-12-05 10:23:21.668443: step 450, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:59s remains)
INFO - root - 2017-12-05 10:23:23.824374: step 460, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:46m:06s remains)
INFO - root - 2017-12-05 10:23:25.983668: step 470, loss = 2.06, batch loss = 2.01 (36.1 examples/sec; 0.221 sec/batch; 20h:25m:34s remains)
INFO - root - 2017-12-05 10:23:28.121087: step 480, loss = 2.06, batch loss = 2.00 (38.0 examples/sec; 0.211 sec/batch; 19h:26m:04s remains)
INFO - root - 2017-12-05 10:23:30.293013: step 490, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.218 sec/batch; 20h:04m:02s remains)
INFO - root - 2017-12-05 10:23:32.436554: step 500, loss = 2.06, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:49s remains)
2017-12-05 10:23:32.717234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1514411 -4.1780233 -4.2149076 -4.2546759 -4.2851095 -4.2750769 -4.2339005 -4.2012773 -4.1888919 -4.1953621 -4.21771 -4.2380805 -4.2477875 -4.2424316 -4.2048154][-4.1356406 -4.1749263 -4.223156 -4.2663088 -4.2937784 -4.2748289 -4.223249 -4.1812248 -4.16352 -4.1744576 -4.2033505 -4.2273736 -4.240531 -4.2415652 -4.211997][-4.1341228 -4.1885252 -4.2450967 -4.2843966 -4.3000383 -4.2713461 -4.2123251 -4.1592174 -4.1354551 -4.1522322 -4.193295 -4.2274127 -4.2490177 -4.2601442 -4.2435513][-4.1616216 -4.2193923 -4.2679415 -4.2934141 -4.292686 -4.25484 -4.1887984 -4.123436 -4.0978332 -4.1258054 -4.183413 -4.2314239 -4.262363 -4.283093 -4.282248][-4.2049661 -4.2496147 -4.280283 -4.2863522 -4.266531 -4.217876 -4.1417432 -4.0649509 -4.0501266 -4.0980711 -4.1727257 -4.2316394 -4.2676749 -4.2928953 -4.302022][-4.2416487 -4.2585478 -4.2669463 -4.25595 -4.2206383 -4.1572862 -4.0588064 -3.9695489 -3.9867876 -4.0713148 -4.1670303 -4.2332 -4.2690187 -4.2897096 -4.2975087][-4.2577119 -4.2509203 -4.2419877 -4.2196627 -4.1717386 -4.0879717 -3.9585936 -3.8592529 -3.926883 -4.0547748 -4.1648965 -4.2314706 -4.2624116 -4.27376 -4.27588][-4.2428493 -4.2209334 -4.2054567 -4.1863103 -4.1369019 -4.0426569 -3.905375 -3.8255355 -3.932446 -4.0735297 -4.1771336 -4.2336636 -4.25557 -4.2572207 -4.2507672][-4.2078419 -4.1811895 -4.1739259 -4.1680536 -4.1273255 -4.0462637 -3.9444883 -3.9143181 -4.0147319 -4.1283584 -4.207365 -4.244976 -4.253345 -4.246068 -4.2347856][-4.1839952 -4.16331 -4.1656837 -4.1693964 -4.1385565 -4.0824127 -4.02468 -4.0269842 -4.1042447 -4.1815596 -4.2325411 -4.2535634 -4.2540359 -4.2463484 -4.2359328][-4.1712561 -4.1598425 -4.1697345 -4.1797872 -4.1619616 -4.1293831 -4.0997691 -4.1080089 -4.1592345 -4.2084408 -4.2413588 -4.2555375 -4.2562575 -4.2532144 -4.2478819][-4.1710639 -4.1635985 -4.1749368 -4.1885018 -4.1850224 -4.1717772 -4.1563096 -4.1585627 -4.1881795 -4.2171035 -4.2386341 -4.2526712 -4.2597494 -4.2631555 -4.2631626][-4.1914053 -4.1829262 -4.1872311 -4.2005491 -4.2083898 -4.2070026 -4.1977968 -4.1949511 -4.2105041 -4.2302151 -4.2460651 -4.2611012 -4.2719173 -4.2767839 -4.2779984][-4.2385235 -4.2276888 -4.2256088 -4.2350588 -4.2458959 -4.2478895 -4.238152 -4.2338729 -4.244216 -4.2583318 -4.2695227 -4.2825441 -4.2918639 -4.2949324 -4.295752][-4.2858825 -4.2750306 -4.2713661 -4.2746348 -4.2813821 -4.2801604 -4.2700853 -4.2670283 -4.2763362 -4.2863889 -4.2932529 -4.3011241 -4.305512 -4.3059378 -4.307272]]...]
INFO - root - 2017-12-05 10:23:34.870419: step 510, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:52s remains)
INFO - root - 2017-12-05 10:23:37.034851: step 520, loss = 2.07, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:52m:54s remains)
INFO - root - 2017-12-05 10:23:39.181943: step 530, loss = 2.04, batch loss = 1.98 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:56s remains)
INFO - root - 2017-12-05 10:23:41.351437: step 540, loss = 2.06, batch loss = 2.01 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:13s remains)
INFO - root - 2017-12-05 10:23:43.505985: step 550, loss = 2.07, batch loss = 2.02 (37.5 examples/sec; 0.214 sec/batch; 19h:41m:19s remains)
INFO - root - 2017-12-05 10:23:45.637502: step 560, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:08s remains)
INFO - root - 2017-12-05 10:23:47.787670: step 570, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.212 sec/batch; 19h:30m:28s remains)
INFO - root - 2017-12-05 10:23:49.937130: step 580, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:52s remains)
INFO - root - 2017-12-05 10:23:52.071530: step 590, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:18s remains)
INFO - root - 2017-12-05 10:23:54.226774: step 600, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.226 sec/batch; 20h:48m:08s remains)
2017-12-05 10:23:54.495942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1786466 -4.1662955 -4.1602964 -4.1635261 -4.1662197 -4.172802 -4.19748 -4.2336717 -4.257369 -4.2729158 -4.280333 -4.2628975 -4.2238889 -4.1964483 -4.1953506][-4.1864586 -4.1761241 -4.1666193 -4.1600018 -4.1560392 -4.1599803 -4.1876554 -4.226913 -4.2522449 -4.2677479 -4.2775702 -4.2678514 -4.241816 -4.2232084 -4.2244339][-4.1848125 -4.1791153 -4.1772466 -4.1743279 -4.1727757 -4.175137 -4.2004032 -4.2351589 -4.2571177 -4.26749 -4.271677 -4.263298 -4.2457752 -4.2375131 -4.2455559][-4.17363 -4.1726241 -4.1813054 -4.185041 -4.1836743 -4.1820674 -4.2034073 -4.2300072 -4.244813 -4.2489305 -4.2482514 -4.2411704 -4.2291269 -4.2283192 -4.2442732][-4.1614032 -4.1582446 -4.1711817 -4.1756396 -4.1669412 -4.155726 -4.1711283 -4.1904573 -4.2010074 -4.2059617 -4.2107472 -4.2094889 -4.2005072 -4.2028131 -4.2208958][-4.1450396 -4.1417265 -4.1554108 -4.1577911 -4.1370959 -4.1172705 -4.127852 -4.1445007 -4.1568213 -4.1707363 -4.1843219 -4.1846647 -4.1741471 -4.1695223 -4.1809812][-4.1262608 -4.123281 -4.1277218 -4.1170506 -4.0784545 -4.0474038 -4.0574489 -4.0859275 -4.1156831 -4.1444969 -4.1627727 -4.1565838 -4.1355395 -4.119967 -4.12043][-4.1236749 -4.1162686 -4.0987487 -4.0617876 -4.0017262 -3.9575877 -3.9666123 -4.0149713 -4.0657458 -4.1007123 -4.1156454 -4.1012578 -4.06679 -4.0415268 -4.0406761][-4.1186953 -4.1078396 -4.0759315 -4.0173063 -3.9420867 -3.8910208 -3.8973403 -3.9509833 -4.0024214 -4.0289044 -4.0362778 -4.0189781 -3.9812934 -3.961482 -3.9795797][-4.1187086 -4.1072989 -4.0749917 -4.0138779 -3.9428103 -3.8940763 -3.8940132 -3.930773 -3.96025 -3.9683337 -3.9648247 -3.9449399 -3.9119606 -3.9128366 -3.9600067][-4.1418743 -4.1358576 -4.1156416 -4.0705309 -4.0177064 -3.978724 -3.9686816 -3.9820387 -3.9881232 -3.9811456 -3.9683094 -3.945498 -3.918283 -3.9337525 -3.9922745][-4.1767492 -4.1742454 -4.1644731 -4.1368852 -4.1070147 -4.0859118 -4.077414 -4.0785332 -4.0734382 -4.0583858 -4.0416093 -4.019352 -3.9985611 -4.0143371 -4.0593367][-4.1970983 -4.1913652 -4.1884928 -4.1772919 -4.1677547 -4.1669416 -4.168385 -4.1687393 -4.1629739 -4.1503811 -4.1362576 -4.119081 -4.1028185 -4.1099749 -4.1352954][-4.2005668 -4.1897688 -4.1897531 -4.1922092 -4.2006516 -4.2149944 -4.2237763 -4.226285 -4.2233205 -4.215034 -4.2059445 -4.1951208 -4.1834993 -4.1849604 -4.194675][-4.1982136 -4.1848106 -4.1828074 -4.1923523 -4.211719 -4.2342348 -4.2464461 -4.2501593 -4.24889 -4.2428327 -4.2362356 -4.2304444 -4.2225413 -4.2222509 -4.2254581]]...]
INFO - root - 2017-12-05 10:23:56.632320: step 610, loss = 2.09, batch loss = 2.03 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:34s remains)
INFO - root - 2017-12-05 10:23:58.773843: step 620, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:42s remains)
INFO - root - 2017-12-05 10:24:01.000426: step 630, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:29s remains)
INFO - root - 2017-12-05 10:24:03.165423: step 640, loss = 2.07, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:11s remains)
INFO - root - 2017-12-05 10:24:05.281034: step 650, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:47s remains)
INFO - root - 2017-12-05 10:24:07.445976: step 660, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:56s remains)
INFO - root - 2017-12-05 10:24:09.626607: step 670, loss = 2.09, batch loss = 2.03 (34.8 examples/sec; 0.230 sec/batch; 21h:10m:45s remains)
INFO - root - 2017-12-05 10:24:11.748310: step 680, loss = 2.08, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:57s remains)
INFO - root - 2017-12-05 10:24:13.899528: step 690, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:36s remains)
INFO - root - 2017-12-05 10:24:16.042907: step 700, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:46m:26s remains)
2017-12-05 10:24:16.323535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2209787 -4.215157 -4.2106233 -4.2054386 -4.2010512 -4.1972766 -4.1927443 -4.1878648 -4.1850247 -4.1849241 -4.1851325 -4.1800046 -4.1684594 -4.153307 -4.1447921][-4.2377892 -4.2306218 -4.2249866 -4.2184396 -4.2120986 -4.20555 -4.1972046 -4.1878366 -4.1812081 -4.1782951 -4.1761737 -4.1695609 -4.1593275 -4.1462574 -4.1394157][-4.2516432 -4.2444906 -4.2381034 -4.2325149 -4.228632 -4.2241359 -4.2157168 -4.2058535 -4.1989341 -4.19593 -4.1933 -4.1868024 -4.1789246 -4.1668959 -4.1592493][-4.2396374 -4.2304783 -4.2215939 -4.2175879 -4.2185364 -4.2195792 -4.2150908 -4.2096348 -4.208426 -4.2129717 -4.2165146 -4.21423 -4.2087855 -4.1955156 -4.1841459][-4.2098536 -4.1941419 -4.1791506 -4.1749024 -4.179945 -4.1847258 -4.1820335 -4.1798873 -4.1878295 -4.2053275 -4.2222733 -4.2298536 -4.2305646 -4.2182617 -4.2043438][-4.1775322 -4.153801 -4.1335106 -4.1288557 -4.135015 -4.1377444 -4.1299891 -4.1256204 -4.1395841 -4.1700878 -4.202 -4.2235394 -4.2348371 -4.2287788 -4.2176313][-4.1633449 -4.1391859 -4.1197438 -4.115654 -4.1193194 -4.1138549 -4.0953436 -4.0830545 -4.0970626 -4.13329 -4.17325 -4.2048192 -4.2249112 -4.225687 -4.2187972][-4.1649637 -4.1502385 -4.139535 -4.1398296 -4.1426697 -4.1338582 -4.1115413 -4.0943284 -4.1020956 -4.1307139 -4.1653643 -4.1945724 -4.2143412 -4.2160611 -4.2109518][-4.1779962 -4.171607 -4.1675696 -4.1699266 -4.1725368 -4.1654205 -4.1495519 -4.1393147 -4.1470532 -4.1676483 -4.1925678 -4.2117252 -4.2215037 -4.2155209 -4.2064152][-4.1992393 -4.1952825 -4.1902695 -4.1886077 -4.1860714 -4.177731 -4.1666617 -4.1642823 -4.1765103 -4.1964178 -4.2189674 -4.2334185 -4.2354889 -4.2219267 -4.2069216][-4.22464 -4.2185545 -4.2095289 -4.2019053 -4.193193 -4.1799331 -4.1681185 -4.1685452 -4.1823463 -4.2024212 -4.2245793 -4.2384911 -4.23937 -4.2234793 -4.2060466][-4.2418232 -4.2344146 -4.2251058 -4.2170663 -4.2091289 -4.1965404 -4.1844931 -4.18296 -4.1925669 -4.2079668 -4.2255988 -4.2359858 -4.235291 -4.2208138 -4.2071095][-4.2420888 -4.2349 -4.2279506 -4.223433 -4.2200484 -4.2135096 -4.2061834 -4.2044983 -4.2096772 -4.2181478 -4.2288671 -4.2338057 -4.2294264 -4.2159166 -4.206748][-4.2368126 -4.2297549 -4.2235174 -4.2201319 -4.218926 -4.2176561 -4.2169371 -4.21787 -4.2216978 -4.2258267 -4.2308116 -4.2322536 -4.2262154 -4.2142029 -4.2076507][-4.2263713 -4.2193265 -4.21268 -4.2094092 -4.2095122 -4.211133 -4.213799 -4.2167511 -4.21997 -4.2220488 -4.2241344 -4.2245336 -4.2193 -4.2098322 -4.2055016]]...]
INFO - root - 2017-12-05 10:24:18.457572: step 710, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.215 sec/batch; 19h:50m:55s remains)
INFO - root - 2017-12-05 10:24:20.605040: step 720, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:21s remains)
INFO - root - 2017-12-05 10:24:22.760883: step 730, loss = 2.07, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:56s remains)
INFO - root - 2017-12-05 10:24:24.906843: step 740, loss = 2.09, batch loss = 2.03 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:29s remains)
INFO - root - 2017-12-05 10:24:27.076637: step 750, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.214 sec/batch; 19h:45m:12s remains)
INFO - root - 2017-12-05 10:24:29.230782: step 760, loss = 2.09, batch loss = 2.03 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:44s remains)
INFO - root - 2017-12-05 10:24:31.386798: step 770, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:59m:17s remains)
INFO - root - 2017-12-05 10:24:33.520930: step 780, loss = 2.08, batch loss = 2.02 (38.4 examples/sec; 0.208 sec/batch; 19h:10m:29s remains)
INFO - root - 2017-12-05 10:24:35.677092: step 790, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:38m:20s remains)
INFO - root - 2017-12-05 10:24:37.819939: step 800, loss = 2.09, batch loss = 2.03 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:45s remains)
2017-12-05 10:24:38.099313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26339 -4.2563348 -4.2524204 -4.2361917 -4.2081327 -4.1824331 -4.1689663 -4.1702867 -4.1777477 -4.1892529 -4.1994405 -4.1984582 -4.18501 -4.1709328 -4.1585393][-4.27484 -4.2676311 -4.2636514 -4.2479711 -4.2191548 -4.1886868 -4.1670995 -4.1583104 -4.15914 -4.1665468 -4.1746426 -4.173945 -4.1630096 -4.1530347 -4.14586][-4.26517 -4.2572007 -4.2529097 -4.2375507 -4.209218 -4.1778984 -4.1521931 -4.1380134 -4.1343112 -4.1394081 -4.1481824 -4.1491613 -4.1414347 -4.1383042 -4.13454][-4.2287145 -4.2180686 -4.2113657 -4.1960907 -4.1701708 -4.1423922 -4.1193533 -4.1063414 -4.1039472 -4.1118755 -4.122982 -4.1246724 -4.1218147 -4.1258955 -4.1259193][-4.180748 -4.1679516 -4.1602755 -4.1466446 -4.1242285 -4.1008658 -4.0820618 -4.0728397 -4.0761514 -4.0898027 -4.1020985 -4.103189 -4.106451 -4.1193619 -4.1264815][-4.1309705 -4.116982 -4.1069479 -4.09215 -4.0699506 -4.0461926 -4.025116 -4.0188379 -4.0339522 -4.0621347 -4.0806656 -4.085474 -4.0948186 -4.1112294 -4.119698][-4.1010065 -4.0854363 -4.0680046 -4.0444908 -4.0121861 -3.9731359 -3.9351068 -3.9268723 -3.9598405 -4.0099683 -4.0418077 -4.0535121 -4.0620394 -4.0731578 -4.0791163][-4.0994678 -4.0873442 -4.066371 -4.0345688 -3.9911904 -3.9382992 -3.8840737 -3.8671441 -3.9065087 -3.9679708 -4.0064592 -4.0188341 -4.021307 -4.0245109 -4.0235524][-4.1241493 -4.1168013 -4.098042 -4.0674009 -4.030292 -3.9889631 -3.9470663 -3.9303553 -3.9551272 -4.0007052 -4.0285063 -4.0334554 -4.0287504 -4.0240974 -4.0158219][-4.166172 -4.1601028 -4.1444879 -4.1199551 -4.0942545 -4.0678306 -4.0394764 -4.0249615 -4.0365396 -4.06329 -4.0799422 -4.0789528 -4.0688143 -4.0603905 -4.051405][-4.22156 -4.2159052 -4.2042074 -4.1848845 -4.1662836 -4.1480489 -4.1261272 -4.1125317 -4.116384 -4.1300378 -4.1392193 -4.1350546 -4.1192341 -4.1071239 -4.1000409][-4.2747245 -4.2699442 -4.2618809 -4.2489729 -4.2341151 -4.2191772 -4.2015381 -4.190371 -4.1917548 -4.19952 -4.204999 -4.20109 -4.185061 -4.1726003 -4.1637297][-4.3034143 -4.3016524 -4.2971449 -4.2899752 -4.2805748 -4.271708 -4.2608862 -4.2545395 -4.2549233 -4.2575507 -4.2591867 -4.2568464 -4.2467222 -4.2382197 -4.2292676][-4.3096676 -4.3096633 -4.3067646 -4.302227 -4.2968292 -4.292964 -4.2885809 -4.2865548 -4.286684 -4.2858257 -4.2838211 -4.2806435 -4.2747846 -4.2687078 -4.2601972][-4.3102212 -4.3106737 -4.308383 -4.3048916 -4.3021235 -4.3010364 -4.2997856 -4.29889 -4.2982221 -4.2949648 -4.2891269 -4.2825546 -4.2758527 -4.2687473 -4.258625]]...]
INFO - root - 2017-12-05 10:24:40.292351: step 810, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 20h:47m:42s remains)
INFO - root - 2017-12-05 10:24:42.452350: step 820, loss = 2.03, batch loss = 1.97 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:58s remains)
INFO - root - 2017-12-05 10:24:44.604008: step 830, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:56s remains)
INFO - root - 2017-12-05 10:24:46.751841: step 840, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 20h:24m:50s remains)
INFO - root - 2017-12-05 10:24:48.903756: step 850, loss = 2.06, batch loss = 2.01 (38.0 examples/sec; 0.210 sec/batch; 19h:22m:59s remains)
INFO - root - 2017-12-05 10:24:51.060522: step 860, loss = 2.09, batch loss = 2.03 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:49s remains)
INFO - root - 2017-12-05 10:24:53.211069: step 870, loss = 2.06, batch loss = 2.01 (37.5 examples/sec; 0.214 sec/batch; 19h:40m:42s remains)
INFO - root - 2017-12-05 10:24:55.398327: step 880, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:22s remains)
INFO - root - 2017-12-05 10:24:57.571287: step 890, loss = 2.09, batch loss = 2.04 (36.6 examples/sec; 0.219 sec/batch; 20h:08m:15s remains)
INFO - root - 2017-12-05 10:24:59.724977: step 900, loss = 2.05, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:34s remains)
2017-12-05 10:25:00.039980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.131216 -4.1226764 -4.1178384 -4.1003509 -4.0865531 -4.1064787 -4.1221724 -4.1262503 -4.1283116 -4.1306238 -4.12984 -4.1341991 -4.1124706 -4.0921774 -4.0975928][-4.10592 -4.1148162 -4.1212144 -4.1075783 -4.086822 -4.0921726 -4.1057315 -4.1179619 -4.1298642 -4.1366343 -4.1312261 -4.1291885 -4.1052427 -4.0880365 -4.0938025][-4.0767145 -4.115108 -4.138773 -4.1263947 -4.0963235 -4.0882339 -4.0971441 -4.1148248 -4.1343031 -4.1492743 -4.1446462 -4.1363444 -4.1097693 -4.0886469 -4.0951877][-4.0400324 -4.1107659 -4.157558 -4.1487508 -4.1095066 -4.0827394 -4.0807195 -4.0923114 -4.1138368 -4.138855 -4.1434336 -4.1412191 -4.1220107 -4.0959392 -4.1025715][-4.0580425 -4.1282024 -4.1798148 -4.17254 -4.1258783 -4.076035 -4.046464 -4.043437 -4.0692911 -4.1074777 -4.1216393 -4.1317987 -4.1283956 -4.1066494 -4.1093063][-4.1274648 -4.1716208 -4.2052503 -4.1873879 -4.1265078 -4.0607557 -4.0043864 -3.9831495 -4.0208874 -4.0783768 -4.1076117 -4.1247392 -4.1291137 -4.1076436 -4.1013522][-4.1837497 -4.1970592 -4.2074084 -4.1746488 -4.1015038 -4.020721 -3.9374807 -3.9083309 -3.9682696 -4.0471649 -4.0953703 -4.1116614 -4.1079826 -4.0772023 -4.0711184][-4.2054758 -4.1917429 -4.1815104 -4.135767 -4.0453463 -3.9498696 -3.8508623 -3.8320913 -3.9315319 -4.0372286 -4.0955305 -4.1064148 -4.0839648 -4.0384932 -4.0281186][-4.2097483 -4.1832147 -4.153965 -4.0962439 -4.0091968 -3.9335406 -3.8681767 -3.8706412 -3.980406 -4.0844936 -4.1408353 -4.1476803 -4.1158838 -4.0583339 -4.0373254][-4.2110553 -4.1914396 -4.1637115 -4.1118207 -4.0464754 -4.0061426 -3.9827511 -4.0005608 -4.0841312 -4.1592641 -4.1957145 -4.1984763 -4.1705933 -4.1160278 -4.0948629][-4.21911 -4.2184625 -4.2060914 -4.172771 -4.1288891 -4.1072536 -4.1023254 -4.1133137 -4.1605091 -4.2021585 -4.2144427 -4.2134061 -4.2001019 -4.1576724 -4.1391616][-4.220788 -4.2319326 -4.2328153 -4.2164335 -4.1908383 -4.1839318 -4.1864 -4.18727 -4.2017813 -4.2170949 -4.2130995 -4.2087321 -4.2036848 -4.1678858 -4.1462631][-4.2182884 -4.23179 -4.2384272 -4.2307906 -4.214437 -4.2105904 -4.2147946 -4.2112484 -4.2113461 -4.2159553 -4.20504 -4.1948929 -4.1899524 -4.1571212 -4.1311326][-4.2234893 -4.226995 -4.2326593 -4.2298923 -4.2181053 -4.2124181 -4.2162971 -4.2172642 -4.2176075 -4.2195444 -4.2137966 -4.2070832 -4.2025871 -4.1700263 -4.1401024][-4.2377477 -4.2342024 -4.2359366 -4.235765 -4.2318931 -4.2299643 -4.2360067 -4.23948 -4.2383056 -4.2346988 -4.2323127 -4.2314153 -4.2285109 -4.1962676 -4.1695905]]...]
INFO - root - 2017-12-05 10:25:02.183822: step 910, loss = 2.08, batch loss = 2.03 (38.8 examples/sec; 0.206 sec/batch; 18h:58m:41s remains)
INFO - root - 2017-12-05 10:25:04.320941: step 920, loss = 2.09, batch loss = 2.04 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:20s remains)
INFO - root - 2017-12-05 10:25:06.462709: step 930, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:41s remains)
INFO - root - 2017-12-05 10:25:08.620081: step 940, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 20h:25m:12s remains)
INFO - root - 2017-12-05 10:25:10.776533: step 950, loss = 2.11, batch loss = 2.05 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:32s remains)
INFO - root - 2017-12-05 10:25:12.947614: step 960, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:26s remains)
INFO - root - 2017-12-05 10:25:15.098433: step 970, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:41s remains)
INFO - root - 2017-12-05 10:25:17.246878: step 980, loss = 2.07, batch loss = 2.02 (36.6 examples/sec; 0.218 sec/batch; 20h:06m:28s remains)
INFO - root - 2017-12-05 10:25:19.399650: step 990, loss = 2.04, batch loss = 1.99 (38.1 examples/sec; 0.210 sec/batch; 19h:21m:00s remains)
INFO - root - 2017-12-05 10:25:21.554412: step 1000, loss = 2.03, batch loss = 1.98 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:31s remains)
2017-12-05 10:25:21.809621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1085973 -4.114192 -4.1359844 -4.1627426 -4.1750135 -4.1707983 -4.144608 -4.11215 -4.1086273 -4.121583 -4.1444063 -4.1742907 -4.2093282 -4.2447639 -4.2775311][-4.102963 -4.1121688 -4.135159 -4.1597366 -4.1713834 -4.1665592 -4.1379323 -4.1032424 -4.1037269 -4.1242247 -4.1541486 -4.1881256 -4.2221932 -4.255765 -4.2830486][-4.088419 -4.1025133 -4.1262164 -4.1488843 -4.1613421 -4.1560087 -4.12619 -4.0892525 -4.0903692 -4.1147413 -4.1499968 -4.185503 -4.2196131 -4.2533627 -4.280159][-4.0956655 -4.1066179 -4.1208706 -4.135159 -4.1417518 -4.1334348 -4.1025925 -4.06522 -4.0666823 -4.0953913 -4.1336842 -4.1710229 -4.2110052 -4.2488141 -4.2782559][-4.1181579 -4.119113 -4.1207519 -4.1221995 -4.1157207 -4.0961976 -4.0568433 -4.0153008 -4.0227385 -4.065455 -4.1106896 -4.1550627 -4.2036052 -4.2466526 -4.2791529][-4.14826 -4.1363668 -4.1232352 -4.1111684 -4.0887303 -4.0533986 -3.9997127 -3.9494433 -3.968564 -4.0336242 -4.0936971 -4.1464996 -4.2006407 -4.2479658 -4.282311][-4.1591282 -4.1379356 -4.116961 -4.0981874 -4.0680251 -4.0239234 -3.9574647 -3.8980892 -3.9292943 -4.0165009 -4.0917196 -4.1520357 -4.2067285 -4.2540054 -4.288116][-4.1628103 -4.1396918 -4.1190052 -4.1020608 -4.0765624 -4.0391 -3.9783463 -3.9216733 -3.9531987 -4.0368571 -4.1068268 -4.1643639 -4.21525 -4.2602196 -4.2932005][-4.1615815 -4.1375694 -4.1202416 -4.112493 -4.1012492 -4.0818329 -4.0421443 -3.9977412 -4.0155449 -4.0731583 -4.1238265 -4.1708412 -4.2160697 -4.2583995 -4.292767][-4.1596937 -4.1381559 -4.1260376 -4.1263447 -4.1267085 -4.12156 -4.0987029 -4.0612183 -4.06246 -4.0968361 -4.1316724 -4.1689873 -4.2112947 -4.2530603 -4.2901058][-4.1604738 -4.1419134 -4.13158 -4.1343379 -4.1404209 -4.1456742 -4.1377182 -4.1073065 -4.0992775 -4.1199651 -4.1438427 -4.1751862 -4.21591 -4.2567329 -4.2927][-4.1684065 -4.151825 -4.1428814 -4.1483107 -4.1586103 -4.1687241 -4.1684866 -4.14326 -4.1328025 -4.1456418 -4.1622362 -4.1910477 -4.2313523 -4.2706995 -4.3026][-4.1932468 -4.1791959 -4.1735458 -4.1814632 -4.1919127 -4.200511 -4.1992178 -4.1751947 -4.16441 -4.1738105 -4.1894403 -4.2180262 -4.2567296 -4.2922578 -4.3181896][-4.2305341 -4.2211666 -4.2182441 -4.2244883 -4.2313247 -4.2352791 -4.23008 -4.20842 -4.200213 -4.2097788 -4.2268648 -4.252553 -4.284493 -4.3133268 -4.3323412][-4.2636318 -4.2576342 -4.2562218 -4.2600608 -4.2643318 -4.2661815 -4.2613759 -4.2456036 -4.2396502 -4.247581 -4.2625589 -4.2825265 -4.3057413 -4.3267937 -4.3399167]]...]
INFO - root - 2017-12-05 10:25:23.988280: step 1010, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:14s remains)
INFO - root - 2017-12-05 10:25:26.154430: step 1020, loss = 2.05, batch loss = 1.99 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:56s remains)
INFO - root - 2017-12-05 10:25:28.308873: step 1030, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.221 sec/batch; 20h:18m:52s remains)
INFO - root - 2017-12-05 10:25:30.502608: step 1040, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.218 sec/batch; 20h:02m:16s remains)
INFO - root - 2017-12-05 10:25:32.661374: step 1050, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:28s remains)
INFO - root - 2017-12-05 10:25:34.809387: step 1060, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 20h:07m:03s remains)
INFO - root - 2017-12-05 10:25:36.952500: step 1070, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:17s remains)
INFO - root - 2017-12-05 10:25:39.139143: step 1080, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.230 sec/batch; 21h:11m:44s remains)
INFO - root - 2017-12-05 10:25:41.322356: step 1090, loss = 2.04, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:14s remains)
INFO - root - 2017-12-05 10:25:43.500693: step 1100, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.214 sec/batch; 19h:39m:26s remains)
2017-12-05 10:25:43.795417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1783886 -4.1379652 -4.097887 -4.0834737 -4.0931244 -4.1237764 -4.1712203 -4.2100792 -4.2341013 -4.2458434 -4.2560587 -4.2612939 -4.2740989 -4.2823668 -4.2842784][-4.1809788 -4.1419835 -4.0894217 -4.0404534 -4.01749 -4.0445013 -4.11178 -4.1722007 -4.208015 -4.2289977 -4.2454019 -4.2548132 -4.2693048 -4.281323 -4.2889276][-4.1913457 -4.1551442 -4.1000948 -4.0332355 -3.9821465 -3.9951313 -4.0656185 -4.1359992 -4.1805115 -4.208642 -4.2308288 -4.2473893 -4.2653704 -4.2785611 -4.2891111][-4.2068753 -4.1795864 -4.1372 -4.0796361 -4.0223851 -4.0143061 -4.0623198 -4.1217661 -4.1651559 -4.1930509 -4.2197161 -4.2413659 -4.2622728 -4.277204 -4.2889023][-4.1986489 -4.1765633 -4.1416464 -4.0991783 -4.0519066 -4.032207 -4.0554128 -4.0996485 -4.1430984 -4.1739798 -4.2050443 -4.2314248 -4.2546844 -4.2744155 -4.2885623][-4.1737366 -4.1520491 -4.1173553 -4.082231 -4.0434551 -4.0159359 -4.0137439 -4.0399208 -4.0856609 -4.1286221 -4.1721773 -4.2078252 -4.2380919 -4.2662873 -4.2857666][-4.1596618 -4.1367979 -4.1023364 -4.0696626 -4.0331011 -3.9955134 -3.9701853 -3.9758029 -4.0207739 -4.0770903 -4.1302638 -4.1743979 -4.214076 -4.2515697 -4.2800608][-4.18145 -4.1547327 -4.119792 -4.0883446 -4.0551682 -4.0173154 -3.9850585 -3.9762211 -4.0077424 -4.0617065 -4.1145 -4.1609693 -4.2049284 -4.2467728 -4.2801094][-4.2179441 -4.1937203 -4.1594934 -4.1251922 -4.0921717 -4.060606 -4.0347013 -4.0210786 -4.0367117 -4.07693 -4.1242061 -4.1682115 -4.213531 -4.2568059 -4.2904968][-4.2312765 -4.2167482 -4.1903076 -4.156487 -4.1210203 -4.08744 -4.0637307 -4.0508304 -4.0560718 -4.0839653 -4.1292686 -4.1744881 -4.22017 -4.2644162 -4.2981348][-4.2226362 -4.221642 -4.2102079 -4.1874132 -4.1554942 -4.1209769 -4.0944424 -4.0761724 -4.068315 -4.0847545 -4.1255302 -4.1711874 -4.2173514 -4.2618766 -4.2965913][-4.2241817 -4.2322435 -4.2340913 -4.2239485 -4.2014303 -4.1715779 -4.1431804 -4.1159215 -4.09427 -4.09676 -4.128067 -4.1692753 -4.2131639 -4.2550206 -4.2889814][-4.2455339 -4.2523174 -4.2560654 -4.2533712 -4.2424297 -4.22325 -4.1986885 -4.1678009 -4.1383533 -4.12975 -4.1481218 -4.1794014 -4.2165937 -4.2522092 -4.2828183][-4.2637343 -4.2651205 -4.2663612 -4.2664185 -4.2636495 -4.2550411 -4.2385321 -4.2086153 -4.1762753 -4.1595669 -4.1672678 -4.1889133 -4.2184949 -4.2483335 -4.2751417][-4.2657471 -4.2633195 -4.262073 -4.2616692 -4.2609096 -4.2575326 -4.2462649 -4.2193294 -4.1894183 -4.1713986 -4.1721516 -4.1866283 -4.21227 -4.238328 -4.2630844]]...]
INFO - root - 2017-12-05 10:25:45.929850: step 1110, loss = 2.04, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:10s remains)
INFO - root - 2017-12-05 10:25:48.103055: step 1120, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:45m:15s remains)
INFO - root - 2017-12-05 10:25:50.264307: step 1130, loss = 2.06, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:08s remains)
INFO - root - 2017-12-05 10:25:52.401996: step 1140, loss = 2.08, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:14s remains)
INFO - root - 2017-12-05 10:25:54.555676: step 1150, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:47s remains)
INFO - root - 2017-12-05 10:25:56.697779: step 1160, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:10s remains)
INFO - root - 2017-12-05 10:25:58.860286: step 1170, loss = 2.07, batch loss = 2.02 (36.6 examples/sec; 0.218 sec/batch; 20h:06m:26s remains)
INFO - root - 2017-12-05 10:26:01.018974: step 1180, loss = 2.05, batch loss = 1.99 (37.7 examples/sec; 0.212 sec/batch; 19h:31m:22s remains)
INFO - root - 2017-12-05 10:26:03.200242: step 1190, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.215 sec/batch; 19h:49m:35s remains)
INFO - root - 2017-12-05 10:26:05.354520: step 1200, loss = 2.05, batch loss = 1.99 (36.0 examples/sec; 0.222 sec/batch; 20h:25m:21s remains)
2017-12-05 10:26:05.647525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2464552 -4.2330127 -4.2251854 -4.2225466 -4.2279105 -4.2400618 -4.2532516 -4.2733073 -4.2766604 -4.2563033 -4.2268977 -4.2172971 -4.2367439 -4.2634549 -4.2825627][-4.2327113 -4.2180319 -4.1990094 -4.1805573 -4.1751304 -4.1840138 -4.2028103 -4.23634 -4.251502 -4.2398691 -4.2201433 -4.2127419 -4.2281032 -4.2463312 -4.2573047][-4.20023 -4.1851373 -4.1545849 -4.116415 -4.0964508 -4.10383 -4.1317878 -4.178555 -4.2122197 -4.2242041 -4.2265606 -4.2277412 -4.238009 -4.2434235 -4.2392826][-4.1589727 -4.1470041 -4.1098428 -4.0502048 -4.0137115 -4.0220876 -4.0517411 -4.092248 -4.1361055 -4.1812224 -4.2170386 -4.2363644 -4.2518287 -4.2537713 -4.2387104][-4.1266656 -4.1217885 -4.0796585 -4.0002842 -3.9476411 -3.9470997 -3.9520478 -3.951669 -3.9928443 -4.0805364 -4.1591535 -4.2099886 -4.2458525 -4.2582951 -4.2469053][-4.1156654 -4.1243868 -4.0860658 -3.9998426 -3.9306614 -3.8963633 -3.8393564 -3.764461 -3.7940698 -3.9399612 -4.0704417 -4.1560063 -4.2144403 -4.2425752 -4.2442994][-4.1229739 -4.1488419 -4.1314068 -4.0596132 -3.9799252 -3.8962994 -3.7584751 -3.5906494 -3.5970101 -3.8026247 -3.9757068 -4.0847549 -4.1612592 -4.2057934 -4.223527][-4.1179452 -4.1688604 -4.1846166 -4.1492019 -4.0833969 -3.9815392 -3.8080473 -3.5943058 -3.5764937 -3.7731817 -3.9329836 -4.0325232 -4.1085672 -4.1635447 -4.1944561][-4.1124773 -4.179966 -4.224102 -4.2291694 -4.1976342 -4.1253223 -3.9863851 -3.81078 -3.7776043 -3.8875327 -3.9817166 -4.0396109 -4.0930271 -4.14547 -4.1846919][-4.1211557 -4.1871009 -4.2371459 -4.2643852 -4.2623992 -4.2304592 -4.1470537 -4.0302606 -3.9983013 -4.0446477 -4.0831742 -4.1043072 -4.1339417 -4.1764174 -4.2111087][-4.1554723 -4.204288 -4.2374415 -4.2667546 -4.2775817 -4.2688284 -4.2244096 -4.1545315 -4.135644 -4.1601987 -4.1771641 -4.1873169 -4.2039862 -4.2269788 -4.2451487][-4.1868067 -4.211689 -4.227879 -4.2529516 -4.2685952 -4.2738652 -4.2516146 -4.2064643 -4.1937304 -4.2108283 -4.2264304 -4.2354374 -4.2426438 -4.2463121 -4.2471471][-4.1957011 -4.2006369 -4.2035356 -4.2266026 -4.2457209 -4.2546358 -4.2403255 -4.201973 -4.1882548 -4.20142 -4.2210979 -4.2329059 -4.2341681 -4.2263045 -4.2155614][-4.1936684 -4.1840024 -4.176157 -4.191884 -4.2115107 -4.2234974 -4.2135849 -4.1792431 -4.1605225 -4.157959 -4.1666927 -4.1754375 -4.1716046 -4.1561279 -4.1481862][-4.2022796 -4.1838527 -4.16533 -4.1665196 -4.1801271 -4.1958742 -4.1911678 -4.1638265 -4.1390491 -4.1151137 -4.0975375 -4.0852604 -4.071259 -4.0649829 -4.080296]]...]
INFO - root - 2017-12-05 10:26:07.810216: step 1210, loss = 2.07, batch loss = 2.01 (38.0 examples/sec; 0.211 sec/batch; 19h:23m:06s remains)
INFO - root - 2017-12-05 10:26:09.985458: step 1220, loss = 2.06, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:04s remains)
INFO - root - 2017-12-05 10:26:12.150322: step 1230, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.216 sec/batch; 19h:51m:54s remains)
INFO - root - 2017-12-05 10:26:14.292488: step 1240, loss = 2.07, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:49s remains)
INFO - root - 2017-12-05 10:26:16.439701: step 1250, loss = 2.09, batch loss = 2.03 (36.8 examples/sec; 0.217 sec/batch; 19h:59m:39s remains)
INFO - root - 2017-12-05 10:26:18.618252: step 1260, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.214 sec/batch; 19h:43m:03s remains)
INFO - root - 2017-12-05 10:26:20.804178: step 1270, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:55s remains)
INFO - root - 2017-12-05 10:26:22.964369: step 1280, loss = 2.07, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:03m:36s remains)
INFO - root - 2017-12-05 10:26:25.124666: step 1290, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:41s remains)
INFO - root - 2017-12-05 10:26:27.309775: step 1300, loss = 2.07, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:46m:16s remains)
2017-12-05 10:26:27.569195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2626796 -4.2591839 -4.2325726 -4.2028766 -4.1746492 -4.1586046 -4.1645985 -4.1920819 -4.2162623 -4.2205086 -4.2140474 -4.2074971 -4.2086287 -4.2199445 -4.2383289][-4.2666526 -4.2595606 -4.2289443 -4.1868758 -4.1404939 -4.1097131 -4.1092687 -4.1387095 -4.1728916 -4.1925588 -4.1975613 -4.1940861 -4.1934338 -4.1969376 -4.2066917][-4.2370858 -4.2286739 -4.1999331 -4.1522779 -4.0950627 -4.0558147 -4.0542812 -4.0871739 -4.1310472 -4.1641817 -4.175693 -4.1710424 -4.166966 -4.1626906 -4.1626768][-4.1884027 -4.1788464 -4.1548347 -4.1088195 -4.0530849 -4.0131416 -4.0129333 -4.0510731 -4.09986 -4.1377506 -4.1526628 -4.1457882 -4.1377525 -4.1307669 -4.1251922][-4.143 -4.1351542 -4.1174569 -4.0754857 -4.0222273 -3.9735053 -3.9645109 -4.0098448 -4.0683174 -4.1140471 -4.1365232 -4.1341314 -4.1262279 -4.1207714 -4.1121531][-4.1154852 -4.1083121 -4.0956063 -4.0543838 -3.9964643 -3.9306238 -3.9014974 -3.9569652 -4.038137 -4.1025634 -4.141552 -4.1485229 -4.1419387 -4.13866 -4.1265173][-4.1095648 -4.1020851 -4.0903015 -4.0465312 -3.9828277 -3.907733 -3.8602183 -3.9213223 -4.0208158 -4.1032076 -4.1549411 -4.1705155 -4.1708269 -4.1734829 -4.1597853][-4.1089625 -4.1023674 -4.093163 -4.0542836 -4.0015345 -3.9443643 -3.9047689 -3.9548352 -4.0435944 -4.1209593 -4.1715622 -4.1890187 -4.1986394 -4.2103176 -4.1998491][-4.0991793 -4.0919991 -4.0942097 -4.0714917 -4.0392942 -4.0095778 -3.9879789 -4.0201521 -4.0821085 -4.1403656 -4.1841826 -4.2052236 -4.2215858 -4.2372189 -4.2294512][-4.091608 -4.0883484 -4.1017537 -4.09562 -4.0745978 -4.0593705 -4.0475669 -4.065104 -4.1029086 -4.1456828 -4.1838388 -4.2055869 -4.2250905 -4.2399449 -4.2339592][-4.104166 -4.1109457 -4.129354 -4.12898 -4.110096 -4.0960073 -4.0839963 -4.0906639 -4.1153927 -4.1480808 -4.1787128 -4.1969151 -4.2142491 -4.2194557 -4.207809][-4.1321473 -4.1436453 -4.1585512 -4.1568871 -4.1395559 -4.1279793 -4.1151204 -4.1159792 -4.1335816 -4.1595564 -4.1833954 -4.1959887 -4.2066441 -4.1976409 -4.1741662][-4.1578 -4.1715827 -4.1862469 -4.1844125 -4.1727962 -4.1641631 -4.150732 -4.145669 -4.1498532 -4.1637759 -4.1800518 -4.1867867 -4.1929951 -4.182003 -4.1566558][-4.1776328 -4.1927333 -4.2129207 -4.215785 -4.2084546 -4.2009583 -4.1884441 -4.1768 -4.164794 -4.1626525 -4.1639609 -4.1617489 -4.1689315 -4.1692595 -4.1599913][-4.1909957 -4.2058105 -4.23206 -4.2438512 -4.2413926 -4.2346187 -4.2217679 -4.2062273 -4.1880527 -4.1752591 -4.1623483 -4.1509809 -4.1572189 -4.167068 -4.1692877]]...]
INFO - root - 2017-12-05 10:26:29.738819: step 1310, loss = 2.08, batch loss = 2.02 (38.0 examples/sec; 0.211 sec/batch; 19h:22m:51s remains)
INFO - root - 2017-12-05 10:26:31.902666: step 1320, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.217 sec/batch; 19h:55m:01s remains)
INFO - root - 2017-12-05 10:26:34.080263: step 1330, loss = 2.08, batch loss = 2.02 (36.6 examples/sec; 0.219 sec/batch; 20h:06m:25s remains)
INFO - root - 2017-12-05 10:26:36.271463: step 1340, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:31s remains)
INFO - root - 2017-12-05 10:26:38.428306: step 1350, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:29s remains)
INFO - root - 2017-12-05 10:26:40.614311: step 1360, loss = 2.05, batch loss = 2.00 (38.0 examples/sec; 0.210 sec/batch; 19h:21m:42s remains)
INFO - root - 2017-12-05 10:26:42.775684: step 1370, loss = 2.05, batch loss = 1.99 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:11s remains)
INFO - root - 2017-12-05 10:26:44.953871: step 1380, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:13s remains)
INFO - root - 2017-12-05 10:26:47.088726: step 1390, loss = 2.04, batch loss = 1.99 (36.6 examples/sec; 0.219 sec/batch; 20h:06m:13s remains)
INFO - root - 2017-12-05 10:26:49.242538: step 1400, loss = 2.04, batch loss = 1.98 (38.5 examples/sec; 0.208 sec/batch; 19h:06m:56s remains)
2017-12-05 10:26:49.515228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1052461 -4.0837774 -4.0647554 -4.0764971 -4.1004496 -4.1119251 -4.1062784 -4.0850024 -4.0948725 -4.1429935 -4.1839852 -4.1862631 -4.1700134 -4.157917 -4.1513515][-4.1290221 -4.1094346 -4.0844445 -4.0942955 -4.1159935 -4.1154976 -4.0917344 -4.0592556 -4.0785527 -4.1415544 -4.1936483 -4.2074375 -4.2065392 -4.2091613 -4.2071815][-4.1485467 -4.1290059 -4.0980968 -4.1009407 -4.1149592 -4.1034436 -4.0596633 -4.0257983 -4.0646992 -4.1368914 -4.1911497 -4.2167711 -4.2343311 -4.24988 -4.2480507][-4.1464596 -4.1271696 -4.0935249 -4.0916286 -4.0946655 -4.0635056 -3.9953184 -3.9711964 -4.0426049 -4.1256485 -4.1801553 -4.2197428 -4.2552495 -4.2766452 -4.2695212][-4.1402335 -4.13334 -4.1037612 -4.0915265 -4.0741005 -4.0164676 -3.9202378 -3.9057157 -4.012424 -4.1127372 -4.1680269 -4.2200069 -4.2689562 -4.2941136 -4.277195][-4.1441636 -4.1598063 -4.1343293 -4.1115789 -4.0758133 -3.9969714 -3.88127 -3.8642106 -3.9884026 -4.0990944 -4.158741 -4.2219667 -4.2782879 -4.2993617 -4.2707224][-4.1580658 -4.190515 -4.1705184 -4.1409359 -4.0963573 -4.0110927 -3.8992529 -3.8718028 -3.9750261 -4.0761671 -4.1383429 -4.213079 -4.2742887 -4.2876644 -4.2501936][-4.1912084 -4.2267766 -4.2120047 -4.178771 -4.1352348 -4.0607238 -3.968538 -3.9306123 -3.9928706 -4.070632 -4.1288376 -4.2044969 -4.2636042 -4.2683907 -4.22755][-4.2334166 -4.2635894 -4.2525749 -4.2217164 -4.1870732 -4.1343942 -4.0601306 -4.016118 -4.0463734 -4.1023092 -4.1521006 -4.2138085 -4.2607155 -4.2604623 -4.2202353][-4.2615561 -4.2830791 -4.2754245 -4.2491636 -4.2256737 -4.1934938 -4.1328807 -4.0876441 -4.1023335 -4.1449513 -4.1837282 -4.2258306 -4.259089 -4.2571263 -4.2221465][-4.2773571 -4.2942605 -4.2875609 -4.2635188 -4.2457118 -4.22696 -4.1827989 -4.1446581 -4.151423 -4.1856465 -4.2168164 -4.2399025 -4.2616444 -4.2615528 -4.2335443][-4.2876678 -4.30125 -4.2967229 -4.2735562 -4.2555752 -4.2420287 -4.2135797 -4.186697 -4.1910267 -4.2190714 -4.2430239 -4.2559876 -4.272718 -4.2775111 -4.2575231][-4.2978 -4.3048158 -4.2996507 -4.2792883 -4.2605095 -4.2493219 -4.2322578 -4.2163568 -4.2220631 -4.2444835 -4.2641692 -4.2741184 -4.28633 -4.292798 -4.279562][-4.304718 -4.3060813 -4.3005419 -4.2861333 -4.2691445 -4.25811 -4.2483773 -4.2416239 -4.2476277 -4.2645435 -4.2809038 -4.2889009 -4.2947598 -4.2987947 -4.2907963][-4.3034511 -4.3018517 -4.2972188 -4.2890253 -4.2776837 -4.2699704 -4.264925 -4.2632322 -4.26913 -4.2807779 -4.2916493 -4.2981467 -4.2985768 -4.2984457 -4.2928848]]...]
INFO - root - 2017-12-05 10:26:51.698806: step 1410, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:36m:58s remains)
INFO - root - 2017-12-05 10:26:53.881670: step 1420, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:45m:52s remains)
INFO - root - 2017-12-05 10:26:56.066195: step 1430, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:46m:23s remains)
INFO - root - 2017-12-05 10:26:58.251048: step 1440, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 20h:06m:29s remains)
INFO - root - 2017-12-05 10:27:00.430145: step 1450, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:05s remains)
INFO - root - 2017-12-05 10:27:02.601105: step 1460, loss = 2.10, batch loss = 2.04 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:51s remains)
INFO - root - 2017-12-05 10:27:04.773833: step 1470, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:46s remains)
INFO - root - 2017-12-05 10:27:06.927319: step 1480, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:49m:31s remains)
INFO - root - 2017-12-05 10:27:09.071636: step 1490, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:51s remains)
INFO - root - 2017-12-05 10:27:11.241494: step 1500, loss = 2.09, batch loss = 2.03 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:19s remains)
2017-12-05 10:27:11.537416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2875338 -4.2868419 -4.2907214 -4.3011894 -4.3131695 -4.3131571 -4.291976 -4.2548032 -4.2243729 -4.2160864 -4.2255812 -4.2348156 -4.2287045 -4.2063155 -4.1944456][-4.2920094 -4.2942667 -4.2953806 -4.2996793 -4.3041306 -4.3004642 -4.28465 -4.2569189 -4.2331023 -4.22535 -4.2315159 -4.2317305 -4.2119727 -4.1795568 -4.1649][-4.2824507 -4.28477 -4.2801986 -4.2747383 -4.2738223 -4.2731576 -4.2640333 -4.2449884 -4.2263513 -4.2217779 -4.2305937 -4.2297206 -4.2004404 -4.1551313 -4.1328015][-4.2702012 -4.2710943 -4.2615891 -4.250864 -4.24877 -4.2520432 -4.2464156 -4.2307048 -4.21572 -4.2145028 -4.2254329 -4.2245874 -4.1864133 -4.1295352 -4.1028113][-4.2642179 -4.2631946 -4.2510319 -4.2370033 -4.2327123 -4.2368546 -4.2359 -4.2256784 -4.2168565 -4.2195592 -4.2282429 -4.220386 -4.174396 -4.1137161 -4.0924964][-4.266736 -4.2593479 -4.2432957 -4.2258444 -4.2177529 -4.218225 -4.2171383 -4.2133574 -4.21693 -4.227881 -4.2339687 -4.2150145 -4.16334 -4.1114988 -4.1055079][-4.2726951 -4.2581663 -4.236115 -4.2125244 -4.1983619 -4.1885724 -4.1782446 -4.1797175 -4.1998115 -4.2208352 -4.2245674 -4.2013478 -4.1586123 -4.1272726 -4.1354747][-4.2821884 -4.2653694 -4.2419429 -4.2138934 -4.1878896 -4.1585193 -4.1352973 -4.1422238 -4.1768789 -4.202991 -4.2057366 -4.1864829 -4.1608844 -4.1490421 -4.1637468][-4.2936058 -4.2795043 -4.259129 -4.2317467 -4.1964526 -4.1520982 -4.1221113 -4.133832 -4.169488 -4.1912904 -4.1938915 -4.1843009 -4.1743979 -4.174036 -4.1887269][-4.2943888 -4.2848396 -4.2698851 -4.2459216 -4.2087522 -4.1604576 -4.1307564 -4.1377697 -4.1618085 -4.1790328 -4.1894846 -4.1924648 -4.1950812 -4.1988864 -4.2059488][-4.2766123 -4.273108 -4.2648282 -4.2443557 -4.2052636 -4.1604505 -4.1327596 -4.1336179 -4.1489792 -4.1660881 -4.184104 -4.19758 -4.2038631 -4.2040668 -4.2032924][-4.2534761 -4.2547197 -4.2521415 -4.2338772 -4.1948371 -4.1544075 -4.1290894 -4.1281962 -4.142355 -4.161478 -4.1839743 -4.1996484 -4.2019196 -4.198019 -4.1928096][-4.228507 -4.2355862 -4.2354507 -4.2192221 -4.1858931 -4.1555338 -4.1337767 -4.1299591 -4.1446509 -4.1666832 -4.1872177 -4.198143 -4.1966209 -4.193572 -4.1919241][-4.2048082 -4.2165747 -4.21801 -4.2057428 -4.1833186 -4.165904 -4.1505594 -4.1474738 -4.1570444 -4.1730814 -4.1910763 -4.1999 -4.1973033 -4.2002964 -4.2047768][-4.1813836 -4.1961775 -4.1966276 -4.1882682 -4.1784477 -4.1716371 -4.1657162 -4.1644473 -4.1668739 -4.1766253 -4.1906295 -4.1970949 -4.1952705 -4.2044272 -4.2085576]]...]
INFO - root - 2017-12-05 10:27:13.698046: step 1510, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:41s remains)
INFO - root - 2017-12-05 10:27:15.872186: step 1520, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:39m:20s remains)
INFO - root - 2017-12-05 10:27:18.033146: step 1530, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:53s remains)
INFO - root - 2017-12-05 10:27:20.215933: step 1540, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:02s remains)
INFO - root - 2017-12-05 10:27:22.383181: step 1550, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.222 sec/batch; 20h:22m:54s remains)
INFO - root - 2017-12-05 10:27:24.561922: step 1560, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.215 sec/batch; 19h:48m:13s remains)
INFO - root - 2017-12-05 10:27:26.740705: step 1570, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:45m:26s remains)
INFO - root - 2017-12-05 10:27:28.883616: step 1580, loss = 2.06, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:08s remains)
INFO - root - 2017-12-05 10:27:31.072625: step 1590, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:53s remains)
INFO - root - 2017-12-05 10:27:33.229691: step 1600, loss = 2.06, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:18m:18s remains)
2017-12-05 10:27:33.533614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3328538 -4.3329144 -4.3333793 -4.3302565 -4.3223691 -4.31354 -4.3045363 -4.30195 -4.3064809 -4.3137579 -4.3228617 -4.3326325 -4.3386011 -4.33978 -4.3381262][-4.315578 -4.315917 -4.3211207 -4.3201962 -4.312325 -4.3030567 -4.2932673 -4.2902632 -4.2966595 -4.3077822 -4.3203759 -4.3297939 -4.33132 -4.3253675 -4.3187771][-4.2980528 -4.2985249 -4.3068891 -4.3063045 -4.2971392 -4.2880492 -4.2796445 -4.2781692 -4.2886515 -4.3031745 -4.3171492 -4.3238454 -4.3190975 -4.3044152 -4.2902751][-4.2830224 -4.2832212 -4.2905936 -4.2842407 -4.2692218 -4.2581739 -4.2504511 -4.252871 -4.2736907 -4.2984362 -4.3174877 -4.3245692 -4.3151011 -4.2956471 -4.2774568][-4.2687511 -4.2627912 -4.2641053 -4.2495785 -4.2252736 -4.2073112 -4.19508 -4.1992364 -4.2335057 -4.2747045 -4.3054481 -4.317656 -4.3098173 -4.2913456 -4.2716236][-4.2591295 -4.2424417 -4.2286177 -4.202776 -4.1680264 -4.1390595 -4.1151695 -4.1187897 -4.170506 -4.2287211 -4.2694283 -4.2908106 -4.2921247 -4.2823963 -4.26695][-4.258852 -4.232203 -4.2031555 -4.158639 -4.10116 -4.0431857 -3.9920568 -3.99336 -4.0710688 -4.1534629 -4.2134228 -4.2528191 -4.2689204 -4.2692509 -4.258821][-4.2614179 -4.2318473 -4.1970859 -4.1430721 -4.0659819 -3.972888 -3.8834362 -3.8739765 -3.96748 -4.0702715 -4.1518707 -4.2122254 -4.2441955 -4.2554007 -4.2527976][-4.26984 -4.2512655 -4.2320871 -4.1951876 -4.1326241 -4.0482168 -3.961576 -3.9406679 -4.00279 -4.078846 -4.1484113 -4.2048817 -4.23648 -4.2489696 -4.2508988][-4.2679477 -4.2606034 -4.2578893 -4.2383113 -4.1918149 -4.1281285 -4.0654044 -4.0464211 -4.0749884 -4.1175313 -4.1641259 -4.205677 -4.2310238 -4.2416334 -4.2454195][-4.2504473 -4.2485628 -4.2560706 -4.2480707 -4.2117748 -4.1658592 -4.1255488 -4.11253 -4.1280169 -4.1538868 -4.183444 -4.2095423 -4.2293468 -4.2383966 -4.2422895][-4.2320213 -4.2310243 -4.2426143 -4.242547 -4.21939 -4.1920891 -4.1701975 -4.1633768 -4.1758947 -4.1948171 -4.2141495 -4.2296643 -4.2428789 -4.2504945 -4.2527552][-4.2189689 -4.2146282 -4.2224932 -4.2225575 -4.2087951 -4.195085 -4.18464 -4.1823764 -4.19561 -4.2123594 -4.2293539 -4.2449737 -4.2584853 -4.2670293 -4.2672706][-4.2178469 -4.2083564 -4.2110257 -4.2099175 -4.2024207 -4.1984229 -4.1946545 -4.1959252 -4.210526 -4.2268319 -4.2444963 -4.261034 -4.2749853 -4.2809615 -4.2771583][-4.2321353 -4.2185616 -4.2157822 -4.2137413 -4.2109251 -4.209897 -4.2075052 -4.2092576 -4.2217259 -4.2356243 -4.2512279 -4.2669277 -4.280551 -4.2866697 -4.2837524]]...]
INFO - root - 2017-12-05 10:27:35.687878: step 1610, loss = 2.06, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:35m:44s remains)
INFO - root - 2017-12-05 10:27:37.850028: step 1620, loss = 2.05, batch loss = 1.99 (37.0 examples/sec; 0.216 sec/batch; 19h:50m:46s remains)
INFO - root - 2017-12-05 10:27:40.009806: step 1630, loss = 2.07, batch loss = 2.01 (37.9 examples/sec; 0.211 sec/batch; 19h:24m:04s remains)
INFO - root - 2017-12-05 10:27:42.187762: step 1640, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:22s remains)
INFO - root - 2017-12-05 10:27:44.348229: step 1650, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:33s remains)
INFO - root - 2017-12-05 10:27:46.499097: step 1660, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:07m:09s remains)
INFO - root - 2017-12-05 10:27:48.641308: step 1670, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:31m:24s remains)
INFO - root - 2017-12-05 10:27:50.802229: step 1680, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:36s remains)
INFO - root - 2017-12-05 10:27:52.976907: step 1690, loss = 2.08, batch loss = 2.02 (38.0 examples/sec; 0.210 sec/batch; 19h:19m:28s remains)
INFO - root - 2017-12-05 10:27:55.158202: step 1700, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:22s remains)
2017-12-05 10:27:55.422352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1429205 -4.0884571 -4.0840235 -4.13797 -4.2130914 -4.27555 -4.3094645 -4.3142419 -4.3083611 -4.3018513 -4.3020811 -4.3092103 -4.3129187 -4.2980075 -4.2750187][-4.094161 -4.0159745 -4.0101814 -4.0892663 -4.1898823 -4.2616339 -4.289113 -4.28594 -4.2810473 -4.2847257 -4.29302 -4.3068166 -4.3120351 -4.2940207 -4.2704525][-4.0818348 -4.0044365 -4.0045695 -4.0902791 -4.1936922 -4.2580724 -4.2678332 -4.2500863 -4.2445631 -4.2577457 -4.2799673 -4.3078709 -4.3193932 -4.2976394 -4.267838][-4.1266165 -4.0779243 -4.0900245 -4.1557951 -4.2325959 -4.2731895 -4.2505789 -4.2060065 -4.1967258 -4.2194123 -4.2598 -4.3032384 -4.3210211 -4.296051 -4.2645946][-4.1749153 -4.1667709 -4.193397 -4.2316947 -4.2687025 -4.2652316 -4.19967 -4.1238918 -4.1185055 -4.1666107 -4.2314353 -4.2881641 -4.3083291 -4.284718 -4.2586641][-4.21005 -4.2264156 -4.2562084 -4.26528 -4.2592192 -4.19963 -4.073318 -3.9609141 -3.9791081 -4.0786676 -4.1842484 -4.2556524 -4.2781358 -4.2586222 -4.2392664][-4.2349896 -4.26209 -4.2850857 -4.2720704 -4.2290306 -4.1152649 -3.9238095 -3.7783303 -3.8385606 -3.9962361 -4.1363564 -4.22123 -4.2480073 -4.2325549 -4.2164135][-4.2671976 -4.2920523 -4.3032775 -4.2755089 -4.2166386 -4.093585 -3.9124041 -3.7991581 -3.8777115 -4.0276046 -4.1496563 -4.2195673 -4.2360468 -4.2142887 -4.1988153][-4.2904744 -4.3058772 -4.3057613 -4.2755036 -4.2262959 -4.1365037 -4.0273905 -3.9811671 -4.0449739 -4.1451745 -4.2214522 -4.2551875 -4.2459435 -4.2116117 -4.1961613][-4.2973475 -4.3065753 -4.3043718 -4.2798128 -4.2481132 -4.1940107 -4.1401644 -4.1309896 -4.1789222 -4.2440567 -4.2869864 -4.29125 -4.2616749 -4.2201138 -4.2063766][-4.2998319 -4.3089013 -4.3074784 -4.2901936 -4.2739978 -4.243032 -4.213161 -4.2129459 -4.2461653 -4.2892132 -4.3142724 -4.3081288 -4.2769828 -4.2414222 -4.2297425][-4.3065763 -4.31097 -4.3093305 -4.3004069 -4.2944665 -4.27579 -4.255311 -4.2546873 -4.2752056 -4.3013196 -4.3154736 -4.3079948 -4.2879772 -4.266531 -4.2567821][-4.3144197 -4.314126 -4.3088231 -4.3024068 -4.3014135 -4.29482 -4.2879291 -4.2882218 -4.2973843 -4.3070168 -4.3102155 -4.3028874 -4.2923431 -4.2812419 -4.274827][-4.3237319 -4.32038 -4.3139486 -4.30971 -4.3107839 -4.3116703 -4.31403 -4.3168683 -4.3203592 -4.3194404 -4.3144569 -4.3070674 -4.3011713 -4.2954731 -4.2923875][-4.3258171 -4.3230963 -4.3200865 -4.3178859 -4.317771 -4.3191071 -4.3230743 -4.32661 -4.327486 -4.323401 -4.319037 -4.3159409 -4.3148561 -4.31455 -4.3137331]]...]
INFO - root - 2017-12-05 10:27:57.598948: step 1710, loss = 2.11, batch loss = 2.05 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:57s remains)
INFO - root - 2017-12-05 10:27:59.788056: step 1720, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:23s remains)
INFO - root - 2017-12-05 10:28:01.983318: step 1730, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:45m:02s remains)
INFO - root - 2017-12-05 10:28:04.222986: step 1740, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.218 sec/batch; 20h:03m:40s remains)
INFO - root - 2017-12-05 10:28:06.369364: step 1750, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:14s remains)
INFO - root - 2017-12-05 10:28:08.511314: step 1760, loss = 2.08, batch loss = 2.03 (37.1 examples/sec; 0.216 sec/batch; 19h:48m:43s remains)
INFO - root - 2017-12-05 10:28:10.666928: step 1770, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:06s remains)
INFO - root - 2017-12-05 10:28:12.828484: step 1780, loss = 2.10, batch loss = 2.04 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:16s remains)
INFO - root - 2017-12-05 10:28:14.998414: step 1790, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:18s remains)
INFO - root - 2017-12-05 10:28:17.180863: step 1800, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:17m:17s remains)
2017-12-05 10:28:17.469716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2044954 -4.2336578 -4.2391238 -4.2192554 -4.1945763 -4.1772957 -4.1344414 -4.0645041 -4.0567122 -4.1048179 -4.1640749 -4.2023606 -4.2140937 -4.2123561 -4.2181931][-4.2231927 -4.2479053 -4.2516394 -4.2358103 -4.2142367 -4.1956997 -4.1475573 -4.0724382 -4.0719485 -4.1304917 -4.1935034 -4.2291112 -4.2348251 -4.2271476 -4.2306824][-4.2371397 -4.2472939 -4.2457123 -4.2363329 -4.2243695 -4.2064266 -4.1550994 -4.0824385 -4.093648 -4.1631627 -4.2222362 -4.2517776 -4.249845 -4.2375951 -4.2375627][-4.2379031 -4.2326474 -4.2260156 -4.2218385 -4.2167397 -4.1979961 -4.1439519 -4.0759954 -4.0980997 -4.1756992 -4.22909 -4.2553973 -4.2505417 -4.2343903 -4.2322822][-4.2280126 -4.2101808 -4.1968164 -4.1927714 -4.191823 -4.1732569 -4.1165795 -4.0463672 -4.0760846 -4.1628981 -4.2136774 -4.2397184 -4.2334352 -4.214839 -4.2087793][-4.2027011 -4.1736298 -4.1554084 -4.1546674 -4.1566825 -4.1332345 -4.0654631 -3.9816971 -4.0200739 -4.126791 -4.1861267 -4.2126679 -4.2060785 -4.1857824 -4.1760483][-4.1574845 -4.1311283 -4.1178989 -4.1225405 -4.1270289 -4.0895944 -3.9955606 -3.8843107 -3.9355783 -4.0743613 -4.1510644 -4.1788344 -4.168498 -4.1474366 -4.1416245][-4.1085939 -4.1015844 -4.1058064 -4.1149378 -4.1129589 -4.0562577 -3.9327321 -3.7949734 -3.8653879 -4.0368891 -4.1339087 -4.1643243 -4.1510386 -4.1298718 -4.1278625][-4.0844088 -4.099431 -4.1230335 -4.1327162 -4.1198516 -4.059474 -3.9401808 -3.8178916 -3.9015899 -4.0682054 -4.1620979 -4.189486 -4.1730542 -4.1482935 -4.1445336][-4.1013365 -4.1240082 -4.1553044 -4.1684871 -4.1541061 -4.1110287 -4.0300746 -3.9521046 -4.0184569 -4.1390934 -4.2047405 -4.2215004 -4.1993017 -4.1688948 -4.165832][-4.1442232 -4.1645603 -4.1937389 -4.2098889 -4.1999421 -4.1743584 -4.128273 -4.0823255 -4.1220746 -4.1945777 -4.2322249 -4.2401686 -4.2145782 -4.1819782 -4.1814179][-4.188282 -4.1947975 -4.2173448 -4.2348566 -4.228528 -4.2123942 -4.1883655 -4.157896 -4.1748953 -4.2176781 -4.2383342 -4.2383623 -4.2109036 -4.1768541 -4.1785622][-4.2104363 -4.1998687 -4.2144513 -4.2332964 -4.2278004 -4.2144985 -4.2044787 -4.1836581 -4.1857429 -4.21379 -4.2259288 -4.2212839 -4.1903582 -4.1510749 -4.1548629][-4.210989 -4.181705 -4.1858182 -4.205472 -4.1998591 -4.183044 -4.1808043 -4.1682181 -4.1661181 -4.1928463 -4.2053466 -4.1992364 -4.1631804 -4.1208715 -4.1292391][-4.2090764 -4.1677551 -4.1606245 -4.1792254 -4.1714811 -4.146821 -4.1496859 -4.1489868 -4.1529255 -4.1865468 -4.2029347 -4.1939788 -4.1522169 -4.1118588 -4.1223397]]...]
INFO - root - 2017-12-05 10:28:19.621850: step 1810, loss = 2.04, batch loss = 1.98 (37.8 examples/sec; 0.212 sec/batch; 19h:27m:16s remains)
INFO - root - 2017-12-05 10:28:21.833166: step 1820, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:07m:38s remains)
INFO - root - 2017-12-05 10:28:24.005407: step 1830, loss = 2.04, batch loss = 1.98 (34.9 examples/sec; 0.229 sec/batch; 21h:03m:19s remains)
INFO - root - 2017-12-05 10:28:26.173457: step 1840, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:04s remains)
INFO - root - 2017-12-05 10:28:28.347009: step 1850, loss = 2.06, batch loss = 2.01 (35.5 examples/sec; 0.226 sec/batch; 20h:43m:35s remains)
INFO - root - 2017-12-05 10:28:30.509195: step 1860, loss = 2.01, batch loss = 1.96 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:25s remains)
INFO - root - 2017-12-05 10:28:32.663533: step 1870, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:05s remains)
INFO - root - 2017-12-05 10:28:34.810447: step 1880, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:47m:44s remains)
INFO - root - 2017-12-05 10:28:37.003171: step 1890, loss = 2.08, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:34s remains)
INFO - root - 2017-12-05 10:28:39.159959: step 1900, loss = 2.06, batch loss = 2.00 (38.0 examples/sec; 0.211 sec/batch; 19h:20m:46s remains)
2017-12-05 10:28:39.460118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.161243 -4.1308465 -4.1538963 -4.1902113 -4.2116776 -4.221962 -4.2335935 -4.2537961 -4.2724361 -4.2758789 -4.2623048 -4.2447095 -4.2166777 -4.2138243 -4.2183805][-4.1281118 -4.0778785 -4.0898733 -4.1353478 -4.1760759 -4.1963964 -4.2017164 -4.2205958 -4.2443838 -4.2514572 -4.2455 -4.2410846 -4.2150521 -4.2059064 -4.2021608][-4.1131625 -4.0512877 -4.0418835 -4.0773516 -4.1294003 -4.1636672 -4.1740756 -4.1949563 -4.2211852 -4.2317157 -4.2357583 -4.2442956 -4.2251854 -4.2105322 -4.1984568][-4.1148276 -4.0506549 -4.0222569 -4.0392041 -4.0828333 -4.1215405 -4.1455994 -4.1763906 -4.2106786 -4.2274809 -4.2348762 -4.2460976 -4.2325006 -4.2148118 -4.1940651][-4.1227975 -4.0649691 -4.0273137 -4.0256319 -4.0480666 -4.0745878 -4.1013045 -4.144587 -4.1890812 -4.215188 -4.2250767 -4.2311587 -4.2151136 -4.1977086 -4.1784897][-4.1214166 -4.0726 -4.0356688 -4.0162354 -4.0191436 -4.0274863 -4.0489178 -4.0917864 -4.1410565 -4.16985 -4.1786146 -4.1869378 -4.1774483 -4.1716313 -4.1607656][-4.09902 -4.0615959 -4.032464 -4.0098209 -3.9979014 -3.9923966 -4.0037246 -4.0388408 -4.0889635 -4.1185408 -4.1237326 -4.1351438 -4.1400146 -4.1540956 -4.1501646][-4.0670323 -4.03843 -4.0165095 -3.9969437 -3.9772348 -3.9605856 -3.960752 -3.9864912 -4.0327344 -4.0634904 -4.0757709 -4.0974364 -4.1166248 -4.1404643 -4.1427574][-4.0469704 -4.0217409 -4.0007391 -3.9814432 -3.963851 -3.9443352 -3.9370878 -3.951015 -3.9881337 -4.0221291 -4.0469079 -4.0809808 -4.1068306 -4.1327171 -4.1407948][-4.0535 -4.0312409 -4.0077929 -3.9862843 -3.9709337 -3.9551013 -3.9463882 -3.9517155 -3.9764535 -4.0050216 -4.0325 -4.0715108 -4.1000805 -4.1274352 -4.1388965][-4.1047077 -4.0837889 -4.0607104 -4.041019 -4.028707 -4.0195589 -4.0148473 -4.0160012 -4.0268435 -4.0412116 -4.0559807 -4.0887747 -4.1162353 -4.1405888 -4.1499028][-4.1910667 -4.174746 -4.1545143 -4.13783 -4.12752 -4.1209793 -4.1175561 -4.117063 -4.12027 -4.1223364 -4.1219006 -4.1388292 -4.1572542 -4.17515 -4.1801834][-4.2744484 -4.2666011 -4.2531667 -4.2416215 -4.2342319 -4.2285538 -4.2248797 -4.2236762 -4.2251396 -4.2240591 -4.2155032 -4.2187638 -4.223906 -4.231729 -4.2310443][-4.3305411 -4.329452 -4.3223085 -4.3164105 -4.3134265 -4.3107743 -4.3085766 -4.3077197 -4.3087826 -4.3073139 -4.2973862 -4.292892 -4.2901273 -4.2900181 -4.28735][-4.3505187 -4.3540154 -4.3513503 -4.3497033 -4.3500595 -4.3506236 -4.35181 -4.3532515 -4.3545232 -4.3536415 -4.3454814 -4.3382692 -4.3330197 -4.3308783 -4.3285108]]...]
INFO - root - 2017-12-05 10:28:41.621484: step 1910, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:28s remains)
INFO - root - 2017-12-05 10:28:43.792384: step 1920, loss = 2.07, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:53s remains)
INFO - root - 2017-12-05 10:28:46.006476: step 1930, loss = 2.08, batch loss = 2.02 (36.6 examples/sec; 0.219 sec/batch; 20h:05m:44s remains)
INFO - root - 2017-12-05 10:28:48.213425: step 1940, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.224 sec/batch; 20h:36m:22s remains)
INFO - root - 2017-12-05 10:28:50.385523: step 1950, loss = 2.08, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:45m:56s remains)
INFO - root - 2017-12-05 10:28:52.572563: step 1960, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.228 sec/batch; 20h:57m:26s remains)
INFO - root - 2017-12-05 10:28:54.751665: step 1970, loss = 2.09, batch loss = 2.03 (36.4 examples/sec; 0.220 sec/batch; 20h:10m:13s remains)
INFO - root - 2017-12-05 10:28:56.922429: step 1980, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:48s remains)
INFO - root - 2017-12-05 10:28:59.072021: step 1990, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:58s remains)
INFO - root - 2017-12-05 10:29:01.234810: step 2000, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:18m:49s remains)
2017-12-05 10:29:01.531934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2397366 -4.2302713 -4.2361507 -4.24581 -4.2452974 -4.2408085 -4.24541 -4.2431812 -4.2357955 -4.2353992 -4.2381554 -4.2443428 -4.2468085 -4.2423615 -4.2412071][-4.2287488 -4.2161388 -4.2229738 -4.2369666 -4.236413 -4.2265239 -4.2313843 -4.2340431 -4.2267556 -4.21898 -4.2121453 -4.2116151 -4.2111826 -4.2103133 -4.2162189][-4.2240448 -4.213449 -4.2193561 -4.232461 -4.2248816 -4.2036672 -4.206378 -4.2181334 -4.2210364 -4.2116714 -4.1949167 -4.1814981 -4.1696711 -4.1653967 -4.1732349][-4.21345 -4.2097955 -4.2155023 -4.2275233 -4.2127633 -4.1817684 -4.176868 -4.1958532 -4.2140312 -4.2120066 -4.1905861 -4.1639104 -4.1375041 -4.1245055 -4.129828][-4.1902027 -4.1888666 -4.1939163 -4.2049708 -4.1889706 -4.15257 -4.1334972 -4.1525707 -4.1859989 -4.1971569 -4.1816077 -4.1535583 -4.1221128 -4.10438 -4.1086168][-4.1657977 -4.1589856 -4.1611128 -4.1713338 -4.1568656 -4.1091528 -4.0636811 -4.0706215 -4.1237979 -4.1637759 -4.1696792 -4.150579 -4.1217008 -4.1054287 -4.1140909][-4.1733637 -4.1507945 -4.1388946 -4.1422906 -4.1254277 -4.0594196 -3.97608 -3.9604998 -4.0383015 -4.1188436 -4.1548638 -4.1501584 -4.1268969 -4.1148396 -4.1290154][-4.1906176 -4.1559 -4.1294942 -4.1227908 -4.1031208 -4.0273757 -3.9177501 -3.8771353 -3.9625971 -4.0689468 -4.1284194 -4.139235 -4.1223874 -4.1141672 -4.1337824][-4.1994567 -4.1654596 -4.1377983 -4.1321621 -4.1229429 -4.067131 -3.9748414 -3.9272642 -3.9793477 -4.0619192 -4.1150761 -4.1305742 -4.1176357 -4.1110544 -4.1298213][-4.1969991 -4.1767235 -4.1646805 -4.1733465 -4.183053 -4.1559892 -4.0948915 -4.0506129 -4.0596795 -4.0981274 -4.128798 -4.1406541 -4.1329279 -4.1263223 -4.1395826][-4.1744423 -4.1693931 -4.175736 -4.1988797 -4.224443 -4.2169204 -4.1806183 -4.1430759 -4.1307859 -4.142879 -4.1596255 -4.1685514 -4.1663156 -4.159687 -4.1644506][-4.159029 -4.1614933 -4.1765661 -4.2029824 -4.2351303 -4.2416935 -4.223877 -4.1966329 -4.1811252 -4.1836672 -4.1925936 -4.1999702 -4.2000089 -4.1916294 -4.1889472][-4.1695852 -4.16867 -4.1813488 -4.2033334 -4.2333555 -4.2483425 -4.2412524 -4.2228818 -4.2101235 -4.2092085 -4.213161 -4.2194362 -4.2209911 -4.2146444 -4.21107][-4.18852 -4.18146 -4.1857247 -4.1989493 -4.2219195 -4.2399011 -4.2419391 -4.2329364 -4.2240825 -4.2223554 -4.2235479 -4.2284036 -4.2320738 -4.2310605 -4.2308836][-4.2091312 -4.1988697 -4.1950397 -4.1993527 -4.2139468 -4.2310419 -4.2402029 -4.2400293 -4.2368784 -4.2346697 -4.2332482 -4.2370353 -4.2421083 -4.2442102 -4.2451949]]...]
INFO - root - 2017-12-05 10:29:03.691011: step 2010, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:43m:18s remains)
INFO - root - 2017-12-05 10:29:05.879871: step 2020, loss = 2.09, batch loss = 2.03 (37.1 examples/sec; 0.216 sec/batch; 19h:48m:18s remains)
INFO - root - 2017-12-05 10:29:08.039171: step 2030, loss = 2.05, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:08s remains)
INFO - root - 2017-12-05 10:29:10.183811: step 2040, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:50m:10s remains)
INFO - root - 2017-12-05 10:29:12.390840: step 2050, loss = 2.08, batch loss = 2.02 (36.6 examples/sec; 0.219 sec/batch; 20h:03m:28s remains)
INFO - root - 2017-12-05 10:29:14.554626: step 2060, loss = 2.05, batch loss = 1.99 (38.0 examples/sec; 0.211 sec/batch; 19h:20m:25s remains)
INFO - root - 2017-12-05 10:29:16.704329: step 2070, loss = 2.08, batch loss = 2.03 (35.8 examples/sec; 0.223 sec/batch; 20h:30m:30s remains)
INFO - root - 2017-12-05 10:29:18.873699: step 2080, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:34s remains)
INFO - root - 2017-12-05 10:29:21.013823: step 2090, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.214 sec/batch; 19h:41m:05s remains)
INFO - root - 2017-12-05 10:29:23.198836: step 2100, loss = 2.07, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:16m:46s remains)
2017-12-05 10:29:23.491118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1536965 -4.154315 -4.164001 -4.1852164 -4.20055 -4.1970243 -4.1987977 -4.2077565 -4.2274537 -4.2591262 -4.2928777 -4.3169665 -4.3287778 -4.3246875 -4.3019533][-4.1604509 -4.1656156 -4.1782656 -4.19724 -4.2102342 -4.2117605 -4.2162037 -4.229084 -4.2545118 -4.290288 -4.3229461 -4.3421969 -4.3524685 -4.3432379 -4.3154373][-4.1734004 -4.1718254 -4.1822858 -4.1956029 -4.2021585 -4.20105 -4.2030435 -4.216476 -4.2474003 -4.291121 -4.3253822 -4.3447609 -4.3582768 -4.3473244 -4.3165083][-4.1869421 -4.1763363 -4.1841526 -4.1960912 -4.1925826 -4.1785321 -4.1708465 -4.1779423 -4.2129979 -4.2660136 -4.3087339 -4.3314724 -4.3484406 -4.3380122 -4.3108435][-4.19919 -4.1860657 -4.1899791 -4.1943941 -4.172224 -4.1263394 -4.0951557 -4.0999608 -4.1488066 -4.2162285 -4.2730174 -4.3056083 -4.329494 -4.326427 -4.3073235][-4.2174263 -4.2087965 -4.2069764 -4.1990867 -4.15335 -4.065897 -3.9959235 -3.9944575 -4.0641384 -4.1541376 -4.2280684 -4.2773275 -4.3109736 -4.3178434 -4.3097773][-4.2351637 -4.2386494 -4.2371864 -4.2195396 -4.1624804 -4.0571742 -3.9567554 -3.934715 -4.0094352 -4.1133008 -4.19718 -4.2586412 -4.2981296 -4.3105764 -4.3075585][-4.2427154 -4.2568073 -4.2569141 -4.23828 -4.1834974 -4.0863347 -3.9870057 -3.9558909 -4.0120487 -4.1039867 -4.1840978 -4.2515225 -4.29157 -4.3007731 -4.2944531][-4.23702 -4.2576761 -4.2599025 -4.2449775 -4.1989436 -4.11698 -4.0306325 -4.0040989 -4.045167 -4.1188407 -4.1879516 -4.2504773 -4.288425 -4.2915654 -4.2795048][-4.2217546 -4.2448664 -4.250422 -4.2415037 -4.2092257 -4.1424017 -4.0693274 -4.0470843 -4.0810132 -4.1418233 -4.1979518 -4.2497253 -4.2829132 -4.2836447 -4.2699394][-4.2239408 -4.2403769 -4.2418652 -4.2367949 -4.2155304 -4.1666412 -4.1094503 -4.0960727 -4.1306314 -4.1818171 -4.2233481 -4.2566557 -4.2759409 -4.275105 -4.2668033][-4.249897 -4.2608981 -4.2545948 -4.2449975 -4.2265058 -4.1921806 -4.1530156 -4.1502109 -4.1826925 -4.2234979 -4.2477818 -4.2601876 -4.25918 -4.2535272 -4.2541428][-4.278234 -4.2911744 -4.285955 -4.2715459 -4.2525144 -4.2276835 -4.2042127 -4.2058191 -4.2316952 -4.2621193 -4.2713766 -4.2598648 -4.2299309 -4.209538 -4.2147169][-4.289731 -4.3029532 -4.3029938 -4.2940459 -4.282639 -4.26884 -4.255816 -4.2583737 -4.2762227 -4.2940049 -4.2906451 -4.2620473 -4.2082982 -4.1705742 -4.1734524][-4.2769766 -4.2904849 -4.2956562 -4.2991939 -4.29916 -4.2950859 -4.2920523 -4.2975812 -4.3113518 -4.3200383 -4.3070092 -4.2691097 -4.2110453 -4.1671219 -4.16399]]...]
INFO - root - 2017-12-05 10:29:25.659161: step 2110, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:12m:35s remains)
INFO - root - 2017-12-05 10:29:27.857900: step 2120, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:17m:29s remains)
INFO - root - 2017-12-05 10:29:30.042546: step 2130, loss = 2.04, batch loss = 1.98 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:40s remains)
INFO - root - 2017-12-05 10:29:32.215320: step 2140, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:14s remains)
INFO - root - 2017-12-05 10:29:34.391467: step 2150, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:34m:42s remains)
INFO - root - 2017-12-05 10:29:36.566450: step 2160, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.223 sec/batch; 20h:25m:08s remains)
INFO - root - 2017-12-05 10:29:38.712270: step 2170, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:30m:32s remains)
INFO - root - 2017-12-05 10:29:40.886316: step 2180, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:16m:44s remains)
INFO - root - 2017-12-05 10:29:43.034454: step 2190, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.219 sec/batch; 20h:08m:16s remains)
INFO - root - 2017-12-05 10:29:45.183731: step 2200, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.214 sec/batch; 19h:40m:34s remains)
2017-12-05 10:29:45.473544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2677522 -4.2709045 -4.28053 -4.2733369 -4.2502475 -4.2138848 -4.1691871 -4.1288486 -4.1287813 -4.1520567 -4.166749 -4.1900458 -4.2240944 -4.2468214 -4.2567472][-4.2594032 -4.2653446 -4.2788734 -4.2774005 -4.2536387 -4.2110314 -4.1600885 -4.1143947 -4.113904 -4.1481562 -4.1771111 -4.2125578 -4.2561264 -4.2812495 -4.2868853][-4.2503762 -4.2621074 -4.2807021 -4.2812114 -4.2510881 -4.1982088 -4.1367445 -4.0908728 -4.0988464 -4.1456609 -4.1874852 -4.2320237 -4.2804494 -4.3102007 -4.3113008][-4.2485785 -4.2663937 -4.2887883 -4.2863393 -4.2441988 -4.1738949 -4.0946231 -4.0452108 -4.0695457 -4.137733 -4.1969223 -4.2521629 -4.3052087 -4.3368239 -4.3327818][-4.2557888 -4.2776542 -4.2996531 -4.2904644 -4.2325239 -4.1409035 -4.038918 -3.9823387 -4.02162 -4.1164093 -4.2007546 -4.2694817 -4.3266521 -4.3565769 -4.3433304][-4.2678771 -4.2925606 -4.3115335 -4.2938151 -4.2212906 -4.1065054 -3.9783716 -3.9092669 -3.9575634 -4.081821 -4.1963782 -4.2829 -4.3426533 -4.3635931 -4.3381662][-4.2796936 -4.3072233 -4.3227983 -4.297554 -4.2150016 -4.0813336 -3.9249225 -3.8335552 -3.8827186 -4.0325775 -4.1766796 -4.2801871 -4.3418326 -4.3555632 -4.3237448][-4.2952828 -4.3205152 -4.3293028 -4.2996268 -4.216197 -4.0771832 -3.9061465 -3.7911804 -3.82579 -3.9816546 -4.1419606 -4.2599511 -4.3281097 -4.3385468 -4.3016667][-4.3182459 -4.3362465 -4.3374834 -4.3057594 -4.2294536 -4.1026874 -3.9351354 -3.8085268 -3.8204856 -3.9590049 -4.1161847 -4.2388616 -4.312191 -4.3244772 -4.2849422][-4.3411989 -4.3515162 -4.347815 -4.31649 -4.2510424 -4.1453433 -3.9996324 -3.8799133 -3.8735762 -3.98225 -4.1229753 -4.2354846 -4.3031516 -4.3159871 -4.2805872][-4.3528337 -4.3585577 -4.3532257 -4.3253884 -4.2742891 -4.193634 -4.0810537 -3.9820871 -3.9652803 -4.039207 -4.1488686 -4.241147 -4.2964787 -4.3093834 -4.2821136][-4.3534269 -4.3543806 -4.3471565 -4.3247123 -4.2876959 -4.2312107 -4.1542306 -4.0850749 -4.0674434 -4.1107559 -4.1874208 -4.2551374 -4.29625 -4.3060093 -4.283751][-4.345118 -4.34074 -4.33124 -4.3148708 -4.28982 -4.25168 -4.2019663 -4.1613045 -4.150815 -4.1775188 -4.2304072 -4.2757988 -4.3030019 -4.3074374 -4.2880769][-4.3356833 -4.3258495 -4.3128114 -4.2998524 -4.283093 -4.2577376 -4.2252741 -4.2021704 -4.2000465 -4.2233491 -4.2623768 -4.2918692 -4.3083739 -4.308969 -4.2921133][-4.3279591 -4.3146458 -4.2992954 -4.28753 -4.2766337 -4.2601156 -4.2386851 -4.2225008 -4.2213063 -4.2401657 -4.2692285 -4.2900286 -4.3018274 -4.3053584 -4.2963834]]...]
INFO - root - 2017-12-05 10:29:47.615489: step 2210, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.211 sec/batch; 19h:24m:13s remains)
INFO - root - 2017-12-05 10:29:49.799729: step 2220, loss = 2.04, batch loss = 1.98 (37.0 examples/sec; 0.216 sec/batch; 19h:50m:51s remains)
INFO - root - 2017-12-05 10:29:51.969601: step 2230, loss = 2.09, batch loss = 2.03 (37.8 examples/sec; 0.212 sec/batch; 19h:26m:16s remains)
INFO - root - 2017-12-05 10:29:54.115345: step 2240, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:40m:16s remains)
INFO - root - 2017-12-05 10:29:56.250898: step 2250, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:32s remains)
INFO - root - 2017-12-05 10:29:58.408910: step 2260, loss = 2.04, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:11m:36s remains)
INFO - root - 2017-12-05 10:30:00.571426: step 2270, loss = 2.07, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:19s remains)
INFO - root - 2017-12-05 10:30:02.770426: step 2280, loss = 2.08, batch loss = 2.02 (38.0 examples/sec; 0.211 sec/batch; 19h:19m:53s remains)
INFO - root - 2017-12-05 10:30:04.940393: step 2290, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:53m:20s remains)
INFO - root - 2017-12-05 10:30:07.115647: step 2300, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:13m:14s remains)
2017-12-05 10:30:07.422903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1902676 -4.1153703 -4.0758615 -4.0782528 -4.1148748 -4.1651897 -4.1935406 -4.18778 -4.1764927 -4.1684933 -4.1576529 -4.1538072 -4.1606822 -4.1738782 -4.204617][-4.17981 -4.1180873 -4.0868144 -4.0915356 -4.1193347 -4.1594152 -4.1962266 -4.205215 -4.2041297 -4.1979685 -4.1780329 -4.1559577 -4.1493964 -4.157619 -4.1937852][-4.1719861 -4.1228943 -4.1013045 -4.1068444 -4.1222954 -4.1453195 -4.1806412 -4.2009425 -4.2099576 -4.2067175 -4.1859512 -4.1572752 -4.1435585 -4.1422849 -4.1739569][-4.1729879 -4.1291928 -4.1125212 -4.1189108 -4.1270723 -4.1437235 -4.1750188 -4.1954446 -4.2078748 -4.2031746 -4.1834464 -4.1612349 -4.149034 -4.1422691 -4.1602979][-4.1851211 -4.1415591 -4.1236539 -4.1233163 -4.1278076 -4.1431379 -4.173213 -4.1916924 -4.1959248 -4.1877565 -4.1746693 -4.1654887 -4.1579156 -4.1431928 -4.1452408][-4.2047949 -4.1611938 -4.14147 -4.1315646 -4.1288934 -4.1408253 -4.1665554 -4.1826591 -4.1800961 -4.1715927 -4.1648021 -4.165669 -4.1641369 -4.149334 -4.1425657][-4.2250686 -4.1831279 -4.1601038 -4.1417832 -4.1322222 -4.1381369 -4.1555815 -4.1677094 -4.1653929 -4.1635318 -4.1687231 -4.1777415 -4.1778336 -4.1682858 -4.1598644][-4.2362924 -4.1947217 -4.1689095 -4.1469669 -4.13211 -4.1312571 -4.139349 -4.1437855 -4.1438031 -4.1560559 -4.1778903 -4.195406 -4.1970425 -4.1901336 -4.1815591][-4.2386036 -4.2001252 -4.1776338 -4.1576366 -4.136981 -4.1252403 -4.1211963 -4.1214933 -4.1260056 -4.1505585 -4.186594 -4.2113209 -4.2139788 -4.2044325 -4.1925621][-4.2382812 -4.2076254 -4.1929359 -4.181366 -4.1583395 -4.1340485 -4.1171489 -4.111444 -4.1173668 -4.1511297 -4.1959949 -4.225595 -4.2279873 -4.2127476 -4.1938329][-4.2365055 -4.209928 -4.2026649 -4.2001009 -4.1830239 -4.1570883 -4.1355233 -4.1211166 -4.1195493 -4.1523633 -4.1974711 -4.2235122 -4.2224593 -4.2027712 -4.1763496][-4.23937 -4.2091832 -4.2009277 -4.2029581 -4.1969509 -4.1835885 -4.1681256 -4.1484528 -4.1380482 -4.1551332 -4.187798 -4.2052827 -4.200541 -4.1783533 -4.1446767][-4.246841 -4.2118268 -4.1997762 -4.1989655 -4.1982961 -4.1988397 -4.1895523 -4.1681576 -4.152494 -4.1517873 -4.1664166 -4.175972 -4.1733956 -4.1545715 -4.1176128][-4.2538157 -4.2168818 -4.1988468 -4.1947579 -4.1988997 -4.2056403 -4.1951 -4.1682258 -4.148489 -4.134613 -4.1355758 -4.1437206 -4.1482854 -4.1392198 -4.1115518][-4.2559137 -4.2245936 -4.206881 -4.2014384 -4.205214 -4.2123246 -4.2019477 -4.1734915 -4.1503963 -4.132575 -4.1288128 -4.1362429 -4.1426177 -4.1421747 -4.1285114]]...]
INFO - root - 2017-12-05 10:30:09.565260: step 2310, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:28s remains)
INFO - root - 2017-12-05 10:30:11.728495: step 2320, loss = 2.05, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:03s remains)
INFO - root - 2017-12-05 10:30:13.887159: step 2330, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:44s remains)
INFO - root - 2017-12-05 10:30:16.050614: step 2340, loss = 2.07, batch loss = 2.01 (38.0 examples/sec; 0.211 sec/batch; 19h:19m:42s remains)
INFO - root - 2017-12-05 10:30:18.190892: step 2350, loss = 2.07, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:50m:56s remains)
INFO - root - 2017-12-05 10:30:20.361510: step 2360, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:34m:24s remains)
INFO - root - 2017-12-05 10:30:22.519649: step 2370, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:47m:42s remains)
INFO - root - 2017-12-05 10:30:24.652043: step 2380, loss = 2.05, batch loss = 1.99 (38.0 examples/sec; 0.210 sec/batch; 19h:17m:59s remains)
INFO - root - 2017-12-05 10:30:26.822225: step 2390, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:05s remains)
INFO - root - 2017-12-05 10:30:28.968480: step 2400, loss = 2.04, batch loss = 1.98 (36.7 examples/sec; 0.218 sec/batch; 19h:58m:11s remains)
2017-12-05 10:30:29.255325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0927734 -4.116117 -4.1603613 -4.197288 -4.2181787 -4.2256866 -4.2223406 -4.218173 -4.217586 -4.216979 -4.2175007 -4.219451 -4.2202888 -4.2290425 -4.2469873][-4.0908365 -4.1114626 -4.1556082 -4.1976266 -4.22531 -4.24185 -4.2417469 -4.2347751 -4.2293377 -4.2224026 -4.2196822 -4.2252975 -4.2351885 -4.2461133 -4.260468][-4.1144075 -4.1294913 -4.1709757 -4.2122369 -4.2404227 -4.2546287 -4.2499895 -4.2393384 -4.2317266 -4.2236648 -4.222548 -4.2296267 -4.2377005 -4.2419305 -4.2486658][-4.141902 -4.1564112 -4.194005 -4.2255111 -4.243083 -4.2448845 -4.228848 -4.2121944 -4.2034731 -4.2013655 -4.2150545 -4.2302017 -4.2331848 -4.2285833 -4.22273][-4.1513963 -4.1678815 -4.1994934 -4.2180386 -4.2160916 -4.1949658 -4.1591196 -4.1339469 -4.1348205 -4.1526947 -4.1900249 -4.2225714 -4.2300224 -4.2207484 -4.2033997][-4.13481 -4.1528683 -4.180511 -4.1851478 -4.1575251 -4.1028562 -4.04055 -4.0099773 -4.0351954 -4.08873 -4.1573153 -4.2091703 -4.2295203 -4.2242541 -4.200995][-4.1161184 -4.1314044 -4.1525497 -4.1443839 -4.0897346 -4.0011721 -3.912899 -3.8827078 -3.9385157 -4.0296769 -4.1260128 -4.1944604 -4.229125 -4.2308135 -4.2078824][-4.1242709 -4.1362925 -4.1514654 -4.1369562 -4.072299 -3.976779 -3.8894262 -3.8677986 -3.9281442 -4.0208154 -4.1182051 -4.1892338 -4.2316031 -4.2402182 -4.2191653][-4.1559553 -4.1644006 -4.1747437 -4.1633053 -4.113821 -4.0474172 -3.9949329 -3.9834437 -4.0169592 -4.0741978 -4.1426597 -4.19759 -4.237114 -4.2497778 -4.2315307][-4.1962414 -4.1949244 -4.1966691 -4.1913514 -4.1671438 -4.1388812 -4.1222153 -4.1156936 -4.1223683 -4.1416583 -4.1761384 -4.2098279 -4.2388039 -4.2511 -4.2398586][-4.2242713 -4.2133546 -4.2077188 -4.20901 -4.2054939 -4.2036967 -4.2071323 -4.2009592 -4.191648 -4.1848211 -4.1923733 -4.208312 -4.2259736 -4.2369919 -4.2362][-4.2366734 -4.2253542 -4.219718 -4.2262564 -4.233201 -4.2415423 -4.2454076 -4.2326112 -4.2122874 -4.1894121 -4.1781788 -4.1802411 -4.1899061 -4.2027493 -4.2145972][-4.2272124 -4.2222595 -4.2237597 -4.23478 -4.2442 -4.252471 -4.250361 -4.2280035 -4.1968584 -4.1629763 -4.1382303 -4.1298184 -4.1345181 -4.1527939 -4.1782289][-4.193584 -4.1958971 -4.2059588 -4.2202225 -4.2280216 -4.2328906 -4.2268076 -4.2004676 -4.1663628 -4.13116 -4.1021285 -4.0886993 -4.0923548 -4.1165791 -4.1538792][-4.1490531 -4.1544638 -4.1710067 -4.1853828 -4.1877971 -4.186862 -4.1786771 -4.1561575 -4.1327767 -4.1140928 -4.0973883 -4.0877519 -4.091383 -4.11839 -4.1613054]]...]
INFO - root - 2017-12-05 10:30:31.390148: step 2410, loss = 2.06, batch loss = 2.00 (38.7 examples/sec; 0.207 sec/batch; 18h:58m:41s remains)
INFO - root - 2017-12-05 10:30:33.543890: step 2420, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:42m:05s remains)
INFO - root - 2017-12-05 10:30:35.711094: step 2430, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 20h:36m:50s remains)
INFO - root - 2017-12-05 10:30:37.859361: step 2440, loss = 2.04, batch loss = 1.98 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:04s remains)
INFO - root - 2017-12-05 10:30:40.022169: step 2450, loss = 2.04, batch loss = 1.98 (35.8 examples/sec; 0.223 sec/batch; 20h:27m:45s remains)
INFO - root - 2017-12-05 10:30:42.199999: step 2460, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 20h:03m:14s remains)
INFO - root - 2017-12-05 10:30:44.342016: step 2470, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.218 sec/batch; 20h:01m:49s remains)
INFO - root - 2017-12-05 10:30:46.512664: step 2480, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:51m:35s remains)
INFO - root - 2017-12-05 10:30:48.657918: step 2490, loss = 2.06, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:15m:39s remains)
INFO - root - 2017-12-05 10:30:50.821171: step 2500, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:19s remains)
2017-12-05 10:30:51.086576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3007951 -4.2986016 -4.2942514 -4.2874832 -4.2805238 -4.2739725 -4.2726555 -4.2836466 -4.2995372 -4.3080359 -4.3067069 -4.298739 -4.2923279 -4.2918482 -4.2956834][-4.2912903 -4.2861385 -4.2760983 -4.2653303 -4.254313 -4.2430167 -4.2387781 -4.2526226 -4.275322 -4.2909741 -4.2955346 -4.2886462 -4.2798619 -4.2740388 -4.2741404][-4.265296 -4.2549067 -4.2364216 -4.2216911 -4.2106152 -4.1970348 -4.1923442 -4.212564 -4.2426109 -4.2646294 -4.2772012 -4.2770157 -4.2698641 -4.2601285 -4.2578387][-4.2415628 -4.2227426 -4.1957226 -4.1794839 -4.166162 -4.1507187 -4.1430516 -4.1666493 -4.2019076 -4.2238789 -4.2417789 -4.2466564 -4.2421432 -4.2326841 -4.2304945][-4.2308097 -4.20304 -4.168458 -4.1488814 -4.1288023 -4.1049995 -4.0882149 -4.10964 -4.1524644 -4.1792326 -4.1998105 -4.2078543 -4.2113762 -4.2111068 -4.2121611][-4.2166529 -4.1842728 -4.1450524 -4.1127629 -4.079854 -4.0395069 -4.0013504 -4.0096526 -4.0683036 -4.1180577 -4.15255 -4.168654 -4.1831312 -4.19673 -4.2077365][-4.2068119 -4.1786466 -4.1414137 -4.0901937 -4.033102 -3.9652576 -3.8940997 -3.8755517 -3.9484286 -4.0329432 -4.0932708 -4.1250696 -4.1455479 -4.1656761 -4.1879072][-4.2146416 -4.1876087 -4.1502147 -4.0844326 -4.0112829 -3.9412909 -3.8636642 -3.834208 -3.9037254 -3.9918818 -4.0501337 -4.0818858 -4.1079488 -4.1348929 -4.1620345][-4.2245359 -4.2025037 -4.1644573 -4.0979328 -4.0252986 -3.9712834 -3.9156113 -3.9015713 -3.9566116 -4.0191908 -4.05643 -4.0703611 -4.0862012 -4.110775 -4.1365118][-4.2165585 -4.2073092 -4.1805964 -4.1316557 -4.072926 -4.0285063 -3.9843667 -3.9679449 -4.0003309 -4.0386963 -4.063746 -4.0728846 -4.0809751 -4.0968528 -4.1195931][-4.1945271 -4.1890163 -4.18048 -4.1548929 -4.1172695 -4.0834289 -4.0487194 -4.0305309 -4.0416613 -4.0590754 -4.0700383 -4.0772533 -4.0888767 -4.1043444 -4.1243477][-4.1966357 -4.1920824 -4.1940222 -4.1772676 -4.15635 -4.1362796 -4.1144228 -4.099328 -4.1042027 -4.1086059 -4.1044521 -4.1087966 -4.1269317 -4.139955 -4.1550589][-4.2097669 -4.2105746 -4.2180867 -4.2057762 -4.1931982 -4.1834149 -4.1703877 -4.1588092 -4.1652241 -4.1644645 -4.1537628 -4.154387 -4.1713428 -4.1826358 -4.1966419][-4.2365212 -4.2401853 -4.25048 -4.2429729 -4.2349291 -4.2338147 -4.2280483 -4.2230854 -4.2336593 -4.2331252 -4.2227454 -4.2225718 -4.2331681 -4.2400026 -4.2487068][-4.2713432 -4.2766585 -4.2863517 -4.2844229 -4.2825608 -4.2844005 -4.2824063 -4.2816863 -4.2904434 -4.2902856 -4.2842426 -4.2848406 -4.2910905 -4.2947674 -4.2996349]]...]
INFO - root - 2017-12-05 10:30:53.253986: step 2510, loss = 2.04, batch loss = 1.98 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:07s remains)
INFO - root - 2017-12-05 10:30:55.426326: step 2520, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:55m:08s remains)
INFO - root - 2017-12-05 10:30:57.598794: step 2530, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:27m:28s remains)
INFO - root - 2017-12-05 10:30:59.744557: step 2540, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.215 sec/batch; 19h:40m:28s remains)
INFO - root - 2017-12-05 10:31:01.918151: step 2550, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 20h:33m:05s remains)
INFO - root - 2017-12-05 10:31:04.079784: step 2560, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:41m:52s remains)
INFO - root - 2017-12-05 10:31:06.226297: step 2570, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:41m:45s remains)
INFO - root - 2017-12-05 10:31:08.407477: step 2580, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:59m:44s remains)
INFO - root - 2017-12-05 10:31:10.588444: step 2590, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.215 sec/batch; 19h:44m:51s remains)
INFO - root - 2017-12-05 10:31:12.746519: step 2600, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 20h:31m:55s remains)
2017-12-05 10:31:13.039113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.236536 -4.2377014 -4.2297392 -4.2204041 -4.2186141 -4.2217021 -4.2237873 -4.2267823 -4.2311993 -4.2372031 -4.2484522 -4.2583613 -4.2635365 -4.2711453 -4.2753925][-4.2173052 -4.2159977 -4.1996312 -4.1798468 -4.1685061 -4.1646237 -4.1632543 -4.16789 -4.17471 -4.1835146 -4.2010555 -4.2202229 -4.23481 -4.2471337 -4.2510934][-4.1883059 -4.1837888 -4.1622577 -4.1353683 -4.1188459 -4.1117134 -4.1056948 -4.1083922 -4.1165786 -4.1270804 -4.1485057 -4.1716247 -4.1879859 -4.2008629 -4.2021914][-4.1514263 -4.1470976 -4.1286588 -4.1075521 -4.0981007 -4.0954747 -4.0878282 -4.0862203 -4.0935264 -4.1040454 -4.1226268 -4.1429172 -4.1532049 -4.1608725 -4.1578684][-4.1249704 -4.1197309 -4.1097322 -4.1013579 -4.100019 -4.1016464 -4.0927315 -4.0844417 -4.093514 -4.1086926 -4.128252 -4.144383 -4.1437511 -4.13864 -4.1234922][-4.1163735 -4.1081505 -4.1036844 -4.102509 -4.0982504 -4.089437 -4.067903 -4.0445123 -4.0548916 -4.0845814 -4.1156888 -4.1360884 -4.1323667 -4.1161404 -4.0942802][-4.1155052 -4.101716 -4.0917778 -4.0846958 -4.0669732 -4.0350375 -3.9915156 -3.942199 -3.9546804 -4.0127335 -4.0702581 -4.1112256 -4.1227632 -4.1157942 -4.1031861][-4.1080017 -4.0916805 -4.0776606 -4.064899 -4.0372477 -3.9897087 -3.9324548 -3.8642883 -3.8768449 -3.9575524 -4.0338659 -4.0925226 -4.1250777 -4.1328478 -4.1338177][-4.1039209 -4.0934196 -4.0862646 -4.0830112 -4.06626 -4.0333743 -4.0000362 -3.95809 -3.9639282 -4.0136962 -4.0634265 -4.1059203 -4.13833 -4.1486773 -4.1535048][-4.12361 -4.1189804 -4.1177282 -4.1221476 -4.11976 -4.1083093 -4.1006408 -4.0861969 -4.0870481 -4.1069403 -4.1222577 -4.1359568 -4.1515961 -4.1529694 -4.1511703][-4.1672673 -4.16396 -4.1581573 -4.1564894 -4.1559863 -4.15235 -4.1536312 -4.153091 -4.1538987 -4.1640663 -4.1628156 -4.1567173 -4.1537633 -4.1419258 -4.1304965][-4.2032137 -4.19725 -4.1826196 -4.1678529 -4.1567793 -4.1478262 -4.1433396 -4.1446967 -4.1502938 -4.164957 -4.1624832 -4.1524453 -4.1442552 -4.1298742 -4.1156411][-4.21336 -4.2048984 -4.1867261 -4.165699 -4.1467066 -4.1281238 -4.1103053 -4.1067777 -4.1158376 -4.1374078 -4.1465783 -4.1497979 -4.1519275 -4.146173 -4.135675][-4.2005048 -4.18844 -4.1718917 -4.1555181 -4.1402073 -4.1252956 -4.1038504 -4.0979018 -4.1079535 -4.1302185 -4.1464171 -4.1644211 -4.1772394 -4.1808281 -4.1771855][-4.1738586 -4.1524363 -4.1359668 -4.1257505 -4.1215 -4.1209512 -4.1072359 -4.1032772 -4.1140494 -4.1333942 -4.153038 -4.1820188 -4.2033396 -4.2137208 -4.2158031]]...]
INFO - root - 2017-12-05 10:31:15.181121: step 2610, loss = 2.08, batch loss = 2.02 (38.0 examples/sec; 0.211 sec/batch; 19h:18m:49s remains)
INFO - root - 2017-12-05 10:31:17.343414: step 2620, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:30m:55s remains)
INFO - root - 2017-12-05 10:31:19.506190: step 2630, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 20h:07m:56s remains)
INFO - root - 2017-12-05 10:31:21.711715: step 2640, loss = 2.06, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:04m:38s remains)
INFO - root - 2017-12-05 10:31:23.885458: step 2650, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:54s remains)
INFO - root - 2017-12-05 10:31:26.083039: step 2660, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:34m:35s remains)
INFO - root - 2017-12-05 10:31:28.277448: step 2670, loss = 2.10, batch loss = 2.05 (35.7 examples/sec; 0.224 sec/batch; 20h:32m:16s remains)
INFO - root - 2017-12-05 10:31:30.464199: step 2680, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:38m:16s remains)
INFO - root - 2017-12-05 10:31:32.616368: step 2690, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.221 sec/batch; 20h:16m:29s remains)
INFO - root - 2017-12-05 10:31:34.792607: step 2700, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:51m:22s remains)
2017-12-05 10:31:35.071888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1974869 -4.2141433 -4.2424397 -4.27341 -4.291791 -4.2973695 -4.299345 -4.3031807 -4.3060689 -4.3041153 -4.3005662 -4.2981906 -4.2976632 -4.2979393 -4.297729][-4.2076025 -4.2238631 -4.2454906 -4.2705307 -4.2856445 -4.291409 -4.2951965 -4.2998967 -4.3027163 -4.300982 -4.2974768 -4.2953763 -4.2960048 -4.2969975 -4.2971983][-4.2393713 -4.2514243 -4.2612319 -4.2702026 -4.2741637 -4.2722812 -4.2725968 -4.2776527 -4.2815657 -4.2830062 -4.2843356 -4.2867746 -4.2912407 -4.2938223 -4.2931871][-4.2603922 -4.2668624 -4.2655497 -4.260767 -4.2545605 -4.2465596 -4.2423162 -4.2480173 -4.2548947 -4.2610178 -4.26867 -4.2769303 -4.2854137 -4.2888613 -4.285748][-4.2435713 -4.2432957 -4.2344732 -4.22106 -4.2086129 -4.1991057 -4.1949553 -4.2039981 -4.2172732 -4.2278218 -4.2387619 -4.251266 -4.2646317 -4.2683005 -4.2598119][-4.196764 -4.1913605 -4.1786628 -4.1628571 -4.1499944 -4.1426477 -4.1416535 -4.1566052 -4.1800208 -4.1995869 -4.2179461 -4.2362518 -4.2529416 -4.2537575 -4.2362928][-4.1392794 -4.1284838 -4.1089292 -4.0863857 -4.0708795 -4.0617986 -4.0597305 -4.0762553 -4.1109705 -4.1480737 -4.1800714 -4.2052636 -4.2236762 -4.2218013 -4.1987963][-4.0718584 -4.0534563 -4.0240755 -3.9957922 -3.9773192 -3.9618981 -3.9483137 -3.9592838 -4.0067945 -4.0670252 -4.1166706 -4.1505451 -4.16844 -4.1623011 -4.1403255][-4.034287 -4.0071926 -3.9789295 -3.9568117 -3.9425497 -3.9223762 -3.8956478 -3.8960948 -3.9466515 -4.0188575 -4.0785427 -4.1175542 -4.1343036 -4.1248856 -4.1032925][-4.077632 -4.0554323 -4.041646 -4.0372462 -4.0370221 -4.0265822 -4.0064344 -4.0053596 -4.0407271 -4.0946107 -4.1383514 -4.1661143 -4.1727285 -4.1583333 -4.1337686][-4.1542807 -4.1385822 -4.1277943 -4.1255584 -4.1304069 -4.1277103 -4.1172352 -4.1197019 -4.1438608 -4.1784635 -4.2049723 -4.2213297 -4.2232223 -4.2134633 -4.1930013][-4.2031913 -4.1928034 -4.1820211 -4.1739225 -4.1761065 -4.1764407 -4.17031 -4.1733446 -4.1869788 -4.2049694 -4.2153816 -4.2196012 -4.2178817 -4.2144947 -4.2022324][-4.1983452 -4.191031 -4.1774788 -4.1621447 -4.1604476 -4.1611977 -4.1562123 -4.1579571 -4.1652632 -4.1746693 -4.17655 -4.1743903 -4.1750932 -4.1790667 -4.17628][-4.1818929 -4.1703763 -4.149363 -4.1296759 -4.126121 -4.1282744 -4.1270552 -4.1291561 -4.1332626 -4.1391659 -4.1409726 -4.1418147 -4.1490021 -4.16167 -4.1703744][-4.183816 -4.166256 -4.1406889 -4.120574 -4.1175036 -4.124166 -4.128675 -4.1317616 -4.134645 -4.1407666 -4.1459236 -4.150847 -4.1610441 -4.1774068 -4.1913447]]...]
INFO - root - 2017-12-05 10:31:37.228094: step 2710, loss = 2.10, batch loss = 2.04 (37.8 examples/sec; 0.211 sec/batch; 19h:21m:53s remains)
INFO - root - 2017-12-05 10:31:39.389757: step 2720, loss = 2.07, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:32s remains)
INFO - root - 2017-12-05 10:31:41.531250: step 2730, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:30m:38s remains)
INFO - root - 2017-12-05 10:31:43.679445: step 2740, loss = 2.09, batch loss = 2.03 (36.3 examples/sec; 0.221 sec/batch; 20h:12m:50s remains)
INFO - root - 2017-12-05 10:31:45.829174: step 2750, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:26m:02s remains)
INFO - root - 2017-12-05 10:31:48.009706: step 2760, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:05m:35s remains)
INFO - root - 2017-12-05 10:31:50.178181: step 2770, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 21h:15m:08s remains)
INFO - root - 2017-12-05 10:31:52.362068: step 2780, loss = 2.04, batch loss = 1.99 (37.9 examples/sec; 0.211 sec/batch; 19h:20m:02s remains)
INFO - root - 2017-12-05 10:31:54.544458: step 2790, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.212 sec/batch; 19h:23m:51s remains)
INFO - root - 2017-12-05 10:31:56.695575: step 2800, loss = 2.06, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:11m:27s remains)
2017-12-05 10:31:56.987698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1648455 -4.1703439 -4.1743217 -4.1766162 -4.1839442 -4.1739626 -4.1215463 -4.0610151 -4.0453272 -4.0851879 -4.1525984 -4.2202168 -4.26968 -4.3073564 -4.3313718][-4.1276321 -4.1228747 -4.12273 -4.12594 -4.1375632 -4.1330209 -4.0844235 -4.0259151 -4.0135159 -4.0607171 -4.1381006 -4.2107654 -4.2627912 -4.299757 -4.3238716][-4.1303678 -4.1104946 -4.0969334 -4.0930576 -4.1057849 -4.1093879 -4.073257 -4.0214763 -4.0107374 -4.0639811 -4.1422853 -4.2096944 -4.2577219 -4.29304 -4.3178062][-4.1646008 -4.1343122 -4.1057348 -4.08357 -4.085556 -4.0907621 -4.0698462 -4.0326505 -4.0264926 -4.0867157 -4.1627984 -4.2208929 -4.2628007 -4.2966461 -4.320148][-4.1979136 -4.1621175 -4.1228604 -4.0872378 -4.067678 -4.0613031 -4.046803 -4.0187044 -4.0168772 -4.0851488 -4.1698337 -4.2255774 -4.2648888 -4.3001547 -4.3241906][-4.2206893 -4.1824021 -4.1321516 -4.0779662 -4.0312939 -4.0042877 -3.9847565 -3.9608178 -3.9652896 -4.0477529 -4.1483788 -4.2120337 -4.2544475 -4.294034 -4.32234][-4.227653 -4.1885729 -4.1297536 -4.0488434 -3.9728653 -3.9374161 -3.9190285 -3.8940792 -3.9058394 -4.0053015 -4.1230536 -4.1962619 -4.2438903 -4.2879038 -4.3200774][-4.2248578 -4.1851768 -4.1211863 -4.0252185 -3.9261765 -3.8862724 -3.8708668 -3.849695 -3.8677862 -3.9789164 -4.106349 -4.1877789 -4.2408643 -4.2854733 -4.3195968][-4.2186885 -4.1793265 -4.1204891 -4.039238 -3.9472392 -3.9120822 -3.8928223 -3.8713589 -3.8830502 -3.9845169 -4.1074686 -4.1899405 -4.2450147 -4.2886453 -4.3216376][-4.2057562 -4.1691208 -4.1293159 -4.0911579 -4.0463266 -4.0201769 -3.9868679 -3.9517648 -3.9429083 -4.0158834 -4.1233325 -4.1974015 -4.2464347 -4.2868505 -4.3194394][-4.184864 -4.1497593 -4.1232719 -4.1193366 -4.111599 -4.0979862 -4.0674906 -4.0280476 -4.00921 -4.0619078 -4.1479774 -4.2094488 -4.2502093 -4.2863564 -4.3178306][-4.1582851 -4.1219358 -4.1046629 -4.1114397 -4.1179843 -4.11255 -4.0874968 -4.0594339 -4.0505271 -4.0958014 -4.1663246 -4.2169752 -4.2535591 -4.2902689 -4.3201056][-4.1510344 -4.1142368 -4.1045427 -4.1171141 -4.1210408 -4.1101265 -4.0870938 -4.07139 -4.0738187 -4.1155424 -4.1756673 -4.2194324 -4.2556181 -4.295228 -4.3244486][-4.161952 -4.1278739 -4.1252546 -4.1412411 -4.1401277 -4.1229463 -4.0984373 -4.0872755 -4.0964026 -4.1355352 -4.1861725 -4.2223091 -4.2563744 -4.2992358 -4.3285661][-4.1705594 -4.12997 -4.1232018 -4.1400614 -4.1445413 -4.1323109 -4.1118755 -4.1066709 -4.1206865 -4.1591582 -4.2003322 -4.2287211 -4.2577024 -4.3006611 -4.3301859]]...]
INFO - root - 2017-12-05 10:31:59.138072: step 2810, loss = 2.07, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 20h:13m:58s remains)
INFO - root - 2017-12-05 10:32:01.301735: step 2820, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:09m:30s remains)
INFO - root - 2017-12-05 10:32:03.456641: step 2830, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 20h:03m:07s remains)
INFO - root - 2017-12-05 10:32:05.633589: step 2840, loss = 2.04, batch loss = 1.98 (36.8 examples/sec; 0.217 sec/batch; 19h:54m:09s remains)
INFO - root - 2017-12-05 10:32:07.783172: step 2850, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:41m:29s remains)
INFO - root - 2017-12-05 10:32:09.964424: step 2860, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 20h:20m:59s remains)
INFO - root - 2017-12-05 10:32:12.160859: step 2870, loss = 2.09, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 19h:57m:22s remains)
INFO - root - 2017-12-05 10:32:14.340651: step 2880, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:32m:30s remains)
INFO - root - 2017-12-05 10:32:16.517688: step 2890, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:33m:59s remains)
INFO - root - 2017-12-05 10:32:18.697412: step 2900, loss = 2.08, batch loss = 2.02 (38.1 examples/sec; 0.210 sec/batch; 19h:13m:25s remains)
2017-12-05 10:32:18.974849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2960224 -4.3072748 -4.3123078 -4.3112335 -4.3050485 -4.2875733 -4.2531047 -4.2223797 -4.2084045 -4.22029 -4.249455 -4.27377 -4.2814856 -4.2705197 -4.2528048][-4.3016834 -4.3081026 -4.3023734 -4.2934995 -4.2875638 -4.2762947 -4.2487879 -4.2220578 -4.2096729 -4.2267036 -4.2631736 -4.2964339 -4.3099637 -4.3016777 -4.2821393][-4.3000526 -4.300878 -4.2833891 -4.2678738 -4.2670083 -4.2711568 -4.2572107 -4.2354121 -4.2196016 -4.2333369 -4.2708616 -4.30684 -4.3219438 -4.3157458 -4.2973218][-4.3001757 -4.2952037 -4.2717333 -4.2556977 -4.2622728 -4.2798858 -4.2799344 -4.2636218 -4.243207 -4.247097 -4.2761574 -4.30393 -4.3138328 -4.3067565 -4.2891307][-4.3045888 -4.296947 -4.2733784 -4.2589517 -4.2680106 -4.2881083 -4.2944713 -4.2839627 -4.2632933 -4.2586417 -4.2755656 -4.29184 -4.2925076 -4.2802224 -4.2628031][-4.3025455 -4.2996297 -4.2826457 -4.2726464 -4.2800817 -4.2947569 -4.3007054 -4.2942057 -4.2769346 -4.2665114 -4.2718444 -4.2767458 -4.268764 -4.2513423 -4.2321596][-4.2981553 -4.3031054 -4.2952256 -4.2911634 -4.2975206 -4.3049569 -4.3083115 -4.3046608 -4.291995 -4.2815208 -4.2817888 -4.2808995 -4.2667608 -4.2437625 -4.2199187][-4.2900033 -4.3028359 -4.3031235 -4.3015766 -4.3026328 -4.302268 -4.3032227 -4.3016653 -4.2957368 -4.2925529 -4.2971506 -4.2991824 -4.2859263 -4.262085 -4.2355556][-4.27418 -4.2980194 -4.3059692 -4.3017383 -4.2908983 -4.2805367 -4.2794104 -4.2802849 -4.2784548 -4.2798824 -4.2891307 -4.2965646 -4.2884569 -4.272296 -4.2520289][-4.2497592 -4.2881021 -4.3037224 -4.29487 -4.2717996 -4.25266 -4.2510653 -4.2537427 -4.2535381 -4.2552714 -4.2623177 -4.2696033 -4.2677007 -4.2631469 -4.2560506][-4.2267523 -4.27591 -4.2972574 -4.2854471 -4.2546248 -4.2308321 -4.2308283 -4.2343454 -4.2309775 -4.2263865 -4.2258816 -4.2296381 -4.2307992 -4.2353859 -4.2421117][-4.2294784 -4.2803516 -4.2997952 -4.2868075 -4.2545748 -4.2293792 -4.229187 -4.2325153 -4.2244158 -4.2120085 -4.2001462 -4.1934023 -4.1898012 -4.1992731 -4.2172565][-4.2566142 -4.295588 -4.3053088 -4.2896409 -4.2614388 -4.2412958 -4.2435536 -4.2461839 -4.2367339 -4.2207003 -4.1994433 -4.1778808 -4.1642208 -4.17402 -4.1943932][-4.2814031 -4.300818 -4.2988982 -4.2820344 -4.2648206 -4.2557921 -4.2607822 -4.2636251 -4.2568574 -4.2438054 -4.2208719 -4.1903577 -4.1710596 -4.1782284 -4.1900969][-4.2819738 -4.285749 -4.2782907 -4.2670274 -4.2638068 -4.2649956 -4.2709036 -4.2745395 -4.2709866 -4.2629504 -4.24308 -4.2129192 -4.1955943 -4.1997027 -4.1988196]]...]
INFO - root - 2017-12-05 10:32:21.153150: step 2910, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:51m:19s remains)
INFO - root - 2017-12-05 10:32:23.307878: step 2920, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.221 sec/batch; 20h:12m:02s remains)
INFO - root - 2017-12-05 10:32:25.488218: step 2930, loss = 2.10, batch loss = 2.04 (37.8 examples/sec; 0.212 sec/batch; 19h:23m:24s remains)
INFO - root - 2017-12-05 10:32:27.635049: step 2940, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.214 sec/batch; 19h:36m:51s remains)
INFO - root - 2017-12-05 10:32:29.779106: step 2950, loss = 2.10, batch loss = 2.04 (37.2 examples/sec; 0.215 sec/batch; 19h:41m:16s remains)
INFO - root - 2017-12-05 10:32:31.980230: step 2960, loss = 2.10, batch loss = 2.04 (37.1 examples/sec; 0.216 sec/batch; 19h:44m:13s remains)
INFO - root - 2017-12-05 10:32:34.161300: step 2970, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:30m:13s remains)
INFO - root - 2017-12-05 10:32:36.346435: step 2980, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:42m:41s remains)
INFO - root - 2017-12-05 10:32:38.524689: step 2990, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.218 sec/batch; 19h:54m:37s remains)
INFO - root - 2017-12-05 10:32:40.716127: step 3000, loss = 2.07, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 20h:23m:01s remains)
2017-12-05 10:32:40.990903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1689234 -4.2067666 -4.2464871 -4.2658906 -4.2473087 -4.197186 -4.1467624 -4.1242733 -4.1418886 -4.1798348 -4.20194 -4.2150006 -4.2236924 -4.2135248 -4.1980572][-4.1618633 -4.1958318 -4.2345328 -4.2557015 -4.2392344 -4.1901236 -4.1388092 -4.11443 -4.1339154 -4.1765494 -4.197053 -4.2046623 -4.209661 -4.2007704 -4.1910777][-4.1520724 -4.1781507 -4.2154307 -4.24079 -4.2332835 -4.1950321 -4.1587024 -4.1461844 -4.1697311 -4.2089052 -4.2235012 -4.2242451 -4.2230511 -4.2131462 -4.2064729][-4.139617 -4.1497812 -4.1747279 -4.1958437 -4.1934028 -4.1684089 -4.1509256 -4.1538363 -4.1854305 -4.2292323 -4.2474918 -4.2499948 -4.2453489 -4.2366772 -4.2354484][-4.1210132 -4.1112242 -4.11599 -4.1296439 -4.131732 -4.110661 -4.0945725 -4.1004887 -4.1403236 -4.1928926 -4.2214284 -4.2306004 -4.2281227 -4.2269773 -4.23397][-4.0939384 -4.0598145 -4.04007 -4.0470266 -4.052763 -4.0257754 -3.988992 -3.9808152 -4.0372343 -4.1152282 -4.1628871 -4.1786194 -4.1776342 -4.1858 -4.1994166][-4.0818667 -4.0182815 -3.9652667 -3.9582634 -3.9583673 -3.9167449 -3.8417335 -3.8068435 -3.8924496 -4.00781 -4.0753503 -4.0927958 -4.0940747 -4.1120663 -4.1377373][-4.1148152 -4.0388021 -3.972703 -3.9512405 -3.9449792 -3.8911638 -3.7828248 -3.7233126 -3.8318503 -3.9690945 -4.0441666 -4.0510693 -4.0413904 -4.0543871 -4.0803933][-4.1804075 -4.1207385 -4.0664616 -4.0414066 -4.0369282 -4.0052586 -3.934751 -3.9024768 -3.9759703 -4.07258 -4.12327 -4.1105728 -4.0769854 -4.0676727 -4.0770388][-4.2404184 -4.2038054 -4.1703472 -4.1513934 -4.1500435 -4.1345992 -4.1015334 -4.0862846 -4.1234565 -4.1735334 -4.1967173 -4.1745167 -4.1357236 -4.11936 -4.1185713][-4.2733192 -4.2546778 -4.2389374 -4.2294178 -4.2279515 -4.2175517 -4.1998816 -4.1887813 -4.1989918 -4.213325 -4.208077 -4.1743274 -4.1362548 -4.1237755 -4.1154304][-4.2876129 -4.2781124 -4.27207 -4.2682652 -4.268187 -4.2616744 -4.2482023 -4.2342296 -4.2292624 -4.2230268 -4.1955457 -4.1451077 -4.1021233 -4.0867529 -4.0681052][-4.3001108 -4.2950006 -4.2926006 -4.2920532 -4.2928557 -4.2878251 -4.2740455 -4.2560086 -4.2427278 -4.2237787 -4.1799273 -4.1187696 -4.0726786 -4.0547881 -4.0339017][-4.3094997 -4.3051257 -4.3032713 -4.3039179 -4.3067975 -4.3052764 -4.2950978 -4.27573 -4.2566009 -4.2318339 -4.1879034 -4.1318083 -4.0886555 -4.0717168 -4.0544033][-4.3242288 -4.3204365 -4.3177943 -4.3176508 -4.318368 -4.3161383 -4.3084917 -4.2940588 -4.2751031 -4.2508759 -4.2163329 -4.178 -4.1452875 -4.1297874 -4.1148772]]...]
INFO - root - 2017-12-05 10:32:43.139075: step 3010, loss = 2.05, batch loss = 2.00 (38.3 examples/sec; 0.209 sec/batch; 19h:06m:59s remains)
INFO - root - 2017-12-05 10:32:45.333653: step 3020, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.221 sec/batch; 20h:11m:21s remains)
INFO - root - 2017-12-05 10:32:47.495193: step 3030, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:48m:15s remains)
INFO - root - 2017-12-05 10:32:49.656586: step 3040, loss = 2.05, batch loss = 1.99 (36.6 examples/sec; 0.218 sec/batch; 19h:59m:37s remains)
INFO - root - 2017-12-05 10:32:51.869526: step 3050, loss = 2.05, batch loss = 1.99 (34.3 examples/sec; 0.233 sec/batch; 21h:21m:12s remains)
INFO - root - 2017-12-05 10:32:54.022006: step 3060, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:22m:01s remains)
INFO - root - 2017-12-05 10:32:56.190772: step 3070, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.218 sec/batch; 19h:59m:34s remains)
INFO - root - 2017-12-05 10:32:58.327891: step 3080, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:25m:46s remains)
INFO - root - 2017-12-05 10:33:00.488405: step 3090, loss = 2.08, batch loss = 2.03 (35.2 examples/sec; 0.228 sec/batch; 20h:49m:17s remains)
INFO - root - 2017-12-05 10:33:02.703018: step 3100, loss = 2.06, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 20h:46m:00s remains)
2017-12-05 10:33:03.024937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2982922 -4.2956605 -4.2986951 -4.2976704 -4.2953205 -4.29355 -4.291533 -4.2916622 -4.2909865 -4.2883625 -4.284225 -4.281498 -4.2806125 -4.2816648 -4.2851472][-4.3266921 -4.3262067 -4.330133 -4.3282027 -4.3244066 -4.3190103 -4.3126087 -4.3095484 -4.3031282 -4.2942958 -4.2846055 -4.2761378 -4.2700267 -4.2668662 -4.2668633][-4.3482809 -4.3516088 -4.3545 -4.3486948 -4.3403115 -4.3285408 -4.3149872 -4.3066306 -4.2947445 -4.2827191 -4.2696285 -4.2564588 -4.2464375 -4.2381725 -4.231586][-4.3500414 -4.3570175 -4.3571458 -4.3460531 -4.3290434 -4.3087516 -4.2894969 -4.2760792 -4.2644243 -4.2573671 -4.2463417 -4.2281423 -4.2121906 -4.1986928 -4.1852345][-4.3302221 -4.3398323 -4.3356838 -4.3200517 -4.2964268 -4.2700281 -4.2451954 -4.22381 -4.2124891 -4.2149687 -4.2076592 -4.1791673 -4.1529889 -4.1362972 -4.1192636][-4.29393 -4.3029795 -4.293067 -4.2697558 -4.2411108 -4.2074394 -4.1704493 -4.133862 -4.1210852 -4.1368141 -4.1362457 -4.1056333 -4.0778584 -4.0692706 -4.0611377][-4.26516 -4.2677627 -4.2440119 -4.2036171 -4.1630692 -4.1169844 -4.058836 -3.9938836 -3.9785316 -4.0177417 -4.0372934 -4.0173826 -4.0035653 -4.0216112 -4.0418234][-4.2590032 -4.2493963 -4.2077575 -4.1456943 -4.0859413 -4.0259237 -3.9465318 -3.8467426 -3.8303404 -3.9084287 -3.96218 -3.9710305 -3.9917564 -4.0447702 -4.094512][-4.2692161 -4.2495108 -4.1980271 -4.1228237 -4.0501928 -3.9858966 -3.9026246 -3.7917593 -3.7766588 -3.8755856 -3.9494145 -3.9837344 -4.0331273 -4.1061444 -4.1684551][-4.2815852 -4.2642903 -4.2196627 -4.1536274 -4.0889263 -4.0336852 -3.9698982 -3.8910351 -3.8803122 -3.9498267 -4.0091891 -4.0462084 -4.0971 -4.1655083 -4.2230458][-4.3020854 -4.292685 -4.2618265 -4.2162714 -4.1700845 -4.1296949 -4.0868759 -4.0399709 -4.0303907 -4.0655017 -4.1028867 -4.1317196 -4.16943 -4.2196054 -4.26317][-4.3190789 -4.316915 -4.2987189 -4.2708759 -4.2423797 -4.2168818 -4.1914945 -4.1659503 -4.1590662 -4.1752157 -4.19699 -4.2141795 -4.2338724 -4.2616906 -4.2882533][-4.3244758 -4.326776 -4.3171973 -4.3020053 -4.2882462 -4.2771363 -4.2657514 -4.2556291 -4.2543936 -4.2627382 -4.2735853 -4.2788382 -4.2830267 -4.2938375 -4.3080149][-4.322051 -4.3254719 -4.32119 -4.3145366 -4.3104453 -4.3081045 -4.3066068 -4.3054891 -4.3096313 -4.315464 -4.3200116 -4.3178353 -4.3141532 -4.31736 -4.3259873][-4.3147435 -4.3186393 -4.31745 -4.3165622 -4.3183279 -4.3202653 -4.3240328 -4.3271761 -4.3336077 -4.3387189 -4.3411674 -4.336895 -4.3315706 -4.3329172 -4.3388424]]...]
INFO - root - 2017-12-05 10:33:05.218532: step 3110, loss = 2.04, batch loss = 1.98 (35.5 examples/sec; 0.225 sec/batch; 20h:37m:14s remains)
INFO - root - 2017-12-05 10:33:07.375818: step 3120, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:36m:37s remains)
INFO - root - 2017-12-05 10:33:10.048161: step 3130, loss = 2.08, batch loss = 2.02 (19.2 examples/sec; 0.416 sec/batch; 38h:03m:38s remains)
INFO - root - 2017-12-05 10:33:14.043847: step 3140, loss = 2.07, batch loss = 2.02 (18.6 examples/sec; 0.431 sec/batch; 39h:24m:47s remains)
INFO - root - 2017-12-05 10:33:18.378718: step 3150, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.459 sec/batch; 41h:58m:34s remains)
INFO - root - 2017-12-05 10:33:22.793625: step 3160, loss = 2.05, batch loss = 1.99 (18.3 examples/sec; 0.438 sec/batch; 40h:06m:02s remains)
INFO - root - 2017-12-05 10:33:27.120264: step 3170, loss = 2.05, batch loss = 2.00 (17.7 examples/sec; 0.452 sec/batch; 41h:18m:17s remains)
INFO - root - 2017-12-05 10:33:31.615390: step 3180, loss = 2.09, batch loss = 2.03 (17.8 examples/sec; 0.448 sec/batch; 41h:01m:09s remains)
INFO - root - 2017-12-05 10:33:36.076318: step 3190, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.450 sec/batch; 41h:09m:08s remains)
INFO - root - 2017-12-05 10:33:40.622219: step 3200, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.466 sec/batch; 42h:35m:39s remains)
2017-12-05 10:33:41.106716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3145533 -4.3169966 -4.3118377 -4.2977304 -4.2853117 -4.2815466 -4.2794237 -4.2732744 -4.2664 -4.2644954 -4.2706914 -4.2826505 -4.2918811 -4.29589 -4.2937155][-4.3023958 -4.305716 -4.2974119 -4.2778363 -4.2634764 -4.2580285 -4.2531447 -4.24684 -4.2387943 -4.2360997 -4.2429709 -4.2550521 -4.2631197 -4.2636933 -4.25148][-4.2845464 -4.2841573 -4.2713466 -4.24838 -4.234262 -4.2259374 -4.2165551 -4.2090716 -4.2009649 -4.1990871 -4.2058582 -4.2147379 -4.220768 -4.2173471 -4.1950355][-4.2634258 -4.2565413 -4.23475 -4.204812 -4.1872416 -4.1746445 -4.169209 -4.1705966 -4.1710339 -4.1706781 -4.1757941 -4.182755 -4.1887431 -4.1848731 -4.1580739][-4.24614 -4.2315984 -4.2054238 -4.1706882 -4.1373734 -4.1048141 -4.0938339 -4.1076956 -4.1237211 -4.1303897 -4.13535 -4.1480222 -4.1657548 -4.1752858 -4.1584978][-4.2348809 -4.2114506 -4.1832185 -4.1418715 -4.08713 -4.020721 -3.9911439 -4.0110488 -4.0427837 -4.0700378 -4.0918922 -4.1227379 -4.1575322 -4.1817818 -4.1790309][-4.232945 -4.2024941 -4.1700287 -4.1220875 -4.0444913 -3.9446139 -3.8939295 -3.9159517 -3.9622035 -4.016324 -4.0693216 -4.1226535 -4.1685209 -4.1973171 -4.2031088][-4.231194 -4.1977096 -4.1656466 -4.1198134 -4.0287228 -3.9086881 -3.850162 -3.8815482 -3.9423726 -4.0072379 -4.0682907 -4.1248941 -4.1628828 -4.1850548 -4.192873][-4.2291603 -4.1960511 -4.1701288 -4.1344 -4.0505285 -3.9418454 -3.8934264 -3.9297731 -3.9920354 -4.0449896 -4.0911307 -4.1296787 -4.1512423 -4.1612463 -4.1658773][-4.2323971 -4.2046065 -4.1869516 -4.1633644 -4.1012816 -4.0170627 -3.9721727 -3.9943695 -4.04003 -4.071898 -4.1032767 -4.1302633 -4.1404085 -4.1380038 -4.1340523][-4.2331567 -4.217483 -4.2109194 -4.1983972 -4.1536765 -4.08804 -4.0419321 -4.049624 -4.0788927 -4.0937314 -4.1097345 -4.1286693 -4.1334457 -4.122499 -4.11248][-4.2304134 -4.2231045 -4.226521 -4.2235928 -4.1938472 -4.1464629 -4.1016545 -4.0944519 -4.1102767 -4.1130505 -4.1188521 -4.1343455 -4.1361279 -4.1235118 -4.1096334][-4.2325773 -4.229599 -4.2383685 -4.2410803 -4.2225671 -4.19293 -4.1616011 -4.1517963 -4.1594129 -4.1575623 -4.1570191 -4.16449 -4.1600285 -4.1438427 -4.1313767][-4.2495146 -4.2456632 -4.254179 -4.2631841 -4.2574434 -4.2425442 -4.2232375 -4.212585 -4.2129669 -4.2101469 -4.2098618 -4.2135606 -4.20878 -4.1959434 -4.1890364][-4.2751689 -4.270751 -4.2771573 -4.2868009 -4.2869005 -4.2799888 -4.2666535 -4.2567158 -4.2549005 -4.2520146 -4.2556772 -4.2633076 -4.2654238 -4.26099 -4.2613463]]...]
INFO - root - 2017-12-05 10:33:45.616135: step 3210, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.461 sec/batch; 42h:09m:09s remains)
INFO - root - 2017-12-05 10:33:50.100368: step 3220, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.451 sec/batch; 41h:15m:34s remains)
INFO - root - 2017-12-05 10:33:54.574201: step 3230, loss = 2.06, batch loss = 2.00 (18.8 examples/sec; 0.427 sec/batch; 39h:01m:13s remains)
INFO - root - 2017-12-05 10:33:58.907971: step 3240, loss = 2.06, batch loss = 2.01 (17.8 examples/sec; 0.449 sec/batch; 41h:02m:34s remains)
INFO - root - 2017-12-05 10:34:03.345746: step 3250, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.459 sec/batch; 42h:00m:15s remains)
INFO - root - 2017-12-05 10:34:07.899124: step 3260, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.448 sec/batch; 40h:59m:16s remains)
INFO - root - 2017-12-05 10:34:12.440323: step 3270, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.454 sec/batch; 41h:33m:49s remains)
INFO - root - 2017-12-05 10:34:16.961455: step 3280, loss = 2.05, batch loss = 1.99 (18.7 examples/sec; 0.428 sec/batch; 39h:10m:35s remains)
INFO - root - 2017-12-05 10:34:21.502279: step 3290, loss = 2.04, batch loss = 1.99 (17.3 examples/sec; 0.461 sec/batch; 42h:12m:08s remains)
INFO - root - 2017-12-05 10:34:25.908214: step 3300, loss = 2.05, batch loss = 1.99 (19.2 examples/sec; 0.417 sec/batch; 38h:05m:35s remains)
2017-12-05 10:34:26.333595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1803327 -4.187047 -4.2224069 -4.2592711 -4.2887883 -4.3125682 -4.3264828 -4.3270969 -4.3157558 -4.289885 -4.2661257 -4.2428 -4.22159 -4.2194328 -4.243124][-4.1678953 -4.1887493 -4.229857 -4.268137 -4.2949629 -4.3165951 -4.3332257 -4.3416376 -4.3405633 -4.3291802 -4.3183532 -4.3049774 -4.2890458 -4.2780776 -4.2745814][-4.1532159 -4.1866965 -4.2314205 -4.2670956 -4.2879806 -4.3030891 -4.3150024 -4.3213835 -4.3234506 -4.3238869 -4.3269262 -4.3283653 -4.3245711 -4.3143883 -4.2938905][-4.17167 -4.2034545 -4.2376719 -4.2592072 -4.2672415 -4.2717471 -4.275157 -4.2763705 -4.2783685 -4.2884674 -4.304224 -4.3221831 -4.3332891 -4.3318372 -4.3109365][-4.2091851 -4.2244616 -4.23941 -4.2429028 -4.2321777 -4.2179885 -4.2049947 -4.1940036 -4.1920137 -4.2104821 -4.2410254 -4.2768717 -4.308507 -4.3268528 -4.3207059][-4.2416735 -4.2408576 -4.2363482 -4.2222095 -4.1912293 -4.1548853 -4.1192894 -4.0894494 -4.0805626 -4.1113319 -4.159905 -4.213192 -4.264914 -4.3021212 -4.3119812][-4.259129 -4.2545748 -4.2376485 -4.2060885 -4.1551933 -4.0949306 -4.0322123 -3.9788656 -3.9660046 -4.0171242 -4.0870357 -4.155498 -4.2219014 -4.2702575 -4.287128][-4.270412 -4.2707953 -4.2498932 -4.204855 -4.1380396 -4.057888 -3.9648767 -3.8812256 -3.8627315 -3.9353633 -4.02662 -4.1085925 -4.1856394 -4.23747 -4.2586117][-4.288547 -4.2946382 -4.2743607 -4.2254314 -4.1543317 -4.0663047 -3.9581256 -3.8583765 -3.8398709 -3.9208984 -4.0187249 -4.1034241 -4.179884 -4.2285495 -4.2514629][-4.3132234 -4.3211164 -4.3023186 -4.2553988 -4.1904011 -4.1149931 -4.0259776 -3.9497268 -3.9425592 -4.0075545 -4.0816789 -4.1464725 -4.20664 -4.2475262 -4.272305][-4.3315439 -4.3353167 -4.3149018 -4.2714529 -4.2196631 -4.1685591 -4.1151528 -4.076942 -4.08212 -4.1230912 -4.1625457 -4.1999912 -4.2399139 -4.2735429 -4.3012047][-4.3245726 -4.3208323 -4.2989182 -4.2630816 -4.231741 -4.2083898 -4.1889391 -4.1812644 -4.1930766 -4.2113352 -4.2199044 -4.2297783 -4.2488079 -4.2737675 -4.3048296][-4.3063445 -4.2950635 -4.2721562 -4.245667 -4.2340736 -4.233562 -4.2368116 -4.2501459 -4.2650442 -4.2662253 -4.2505059 -4.2340269 -4.23152 -4.244936 -4.2775955][-4.28874 -4.2716608 -4.2445569 -4.2222872 -4.224575 -4.2399511 -4.2570233 -4.2812653 -4.2974095 -4.2894745 -4.2590361 -4.2249589 -4.2048235 -4.2077165 -4.240119][-4.2719131 -4.2549105 -4.2286811 -4.2111478 -4.222671 -4.24664 -4.268095 -4.2929382 -4.3090186 -4.3016896 -4.269599 -4.2282557 -4.1975074 -4.1902127 -4.2172918]]...]
INFO - root - 2017-12-05 10:34:30.816371: step 3310, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.443 sec/batch; 40h:32m:24s remains)
INFO - root - 2017-12-05 10:34:35.323551: step 3320, loss = 2.10, batch loss = 2.04 (17.4 examples/sec; 0.459 sec/batch; 41h:58m:12s remains)
INFO - root - 2017-12-05 10:34:39.541081: step 3330, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.441 sec/batch; 40h:17m:32s remains)
INFO - root - 2017-12-05 10:34:44.060000: step 3340, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.456 sec/batch; 41h:41m:42s remains)
INFO - root - 2017-12-05 10:34:48.477568: step 3350, loss = 2.09, batch loss = 2.03 (17.2 examples/sec; 0.465 sec/batch; 42h:30m:49s remains)
INFO - root - 2017-12-05 10:34:52.999366: step 3360, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.463 sec/batch; 42h:18m:37s remains)
INFO - root - 2017-12-05 10:34:57.533964: step 3370, loss = 2.04, batch loss = 1.98 (17.6 examples/sec; 0.454 sec/batch; 41h:33m:00s remains)
INFO - root - 2017-12-05 10:35:02.035747: step 3380, loss = 2.05, batch loss = 1.99 (18.8 examples/sec; 0.425 sec/batch; 38h:53m:35s remains)
INFO - root - 2017-12-05 10:35:06.449840: step 3390, loss = 2.08, batch loss = 2.02 (19.3 examples/sec; 0.415 sec/batch; 37h:56m:04s remains)
INFO - root - 2017-12-05 10:35:10.975625: step 3400, loss = 2.05, batch loss = 1.99 (18.4 examples/sec; 0.436 sec/batch; 39h:48m:50s remains)
2017-12-05 10:35:11.444217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2524247 -4.256835 -4.2810369 -4.2937202 -4.2989912 -4.3018851 -4.2952771 -4.2809038 -4.2706075 -4.2635865 -4.2629209 -4.268321 -4.2681212 -4.2495494 -4.2102666][-4.23019 -4.2298503 -4.2540908 -4.2673092 -4.2735639 -4.274188 -4.2623072 -4.2418661 -4.2274408 -4.2175589 -4.218224 -4.2287912 -4.2316923 -4.210453 -4.1605844][-4.1872067 -4.1768804 -4.2044554 -4.2235088 -4.2320428 -4.2296376 -4.2096753 -4.1813755 -4.1623492 -4.1529126 -4.1634851 -4.1885996 -4.2032585 -4.1870675 -4.1386738][-4.1307416 -4.1051793 -4.1394558 -4.1713219 -4.1819935 -4.17488 -4.1436663 -4.1059804 -4.0857897 -4.0873303 -4.1188931 -4.1618237 -4.1888814 -4.1816072 -4.1418195][-4.0808783 -4.0444465 -4.0894575 -4.1331553 -4.1439319 -4.1251421 -4.0729723 -4.0178895 -4.0027943 -4.0301771 -4.0895052 -4.1474624 -4.18149 -4.1854124 -4.1606965][-4.0694108 -4.0367174 -4.0867524 -4.12479 -4.1206322 -4.0805759 -3.9978154 -3.9172072 -3.9155536 -3.9821739 -4.0691428 -4.1346335 -4.1708455 -4.184494 -4.1778064][-4.1058879 -4.0890546 -4.1303248 -4.1462131 -4.1156931 -4.0471363 -3.9339409 -3.8326404 -3.853621 -3.959965 -4.0629668 -4.1289849 -4.1649365 -4.1810203 -4.1855197][-4.1575832 -4.1537189 -4.1820354 -4.1781697 -4.1282248 -4.0402875 -3.9144974 -3.8209803 -3.8669648 -3.9887655 -4.0856833 -4.1436896 -4.1769609 -4.1919746 -4.1950355][-4.1974468 -4.19674 -4.2164445 -4.2081823 -4.1603327 -4.0787258 -3.9775894 -3.9268663 -3.9775295 -4.0732169 -4.1402059 -4.1822829 -4.2080264 -4.2184582 -4.2128959][-4.2310925 -4.2280569 -4.2408738 -4.2362828 -4.2018809 -4.1419821 -4.0828242 -4.0705047 -4.1119161 -4.1703777 -4.2072158 -4.2312131 -4.2477708 -4.2504206 -4.2365956][-4.273931 -4.2668109 -4.2718329 -4.2666492 -4.2469831 -4.2111745 -4.1811557 -4.184257 -4.2105865 -4.2444491 -4.2659616 -4.2771835 -4.2817025 -4.2734833 -4.2571359][-4.3081765 -4.2998633 -4.2952671 -4.2844467 -4.2738032 -4.2542248 -4.2413526 -4.2470064 -4.2638731 -4.2875161 -4.3065805 -4.3138161 -4.3077784 -4.2914839 -4.2748532][-4.3241878 -4.3175812 -4.3077874 -4.2950397 -4.286932 -4.2778358 -4.2745023 -4.2818804 -4.29575 -4.3143353 -4.330821 -4.3347321 -4.3242593 -4.3064094 -4.2917738][-4.3133793 -4.3093166 -4.3022332 -4.2937646 -4.2882361 -4.2830806 -4.2844682 -4.2959867 -4.3115215 -4.3265944 -4.3356094 -4.3330293 -4.3201356 -4.3040509 -4.2909689][-4.2940507 -4.2914095 -4.2897344 -4.2893677 -4.2883554 -4.2839675 -4.2843761 -4.2957306 -4.3086877 -4.3179493 -4.3194547 -4.3135633 -4.3024521 -4.2913432 -4.2822576]]...]
INFO - root - 2017-12-05 10:35:15.982200: step 3410, loss = 2.04, batch loss = 1.98 (17.8 examples/sec; 0.449 sec/batch; 41h:01m:06s remains)
INFO - root - 2017-12-05 10:35:20.271193: step 3420, loss = 2.06, batch loss = 2.00 (17.8 examples/sec; 0.449 sec/batch; 41h:04m:46s remains)
INFO - root - 2017-12-05 10:35:24.702947: step 3430, loss = 2.09, batch loss = 2.03 (17.4 examples/sec; 0.459 sec/batch; 41h:59m:07s remains)
INFO - root - 2017-12-05 10:35:29.256403: step 3440, loss = 2.05, batch loss = 1.99 (18.1 examples/sec; 0.442 sec/batch; 40h:22m:37s remains)
INFO - root - 2017-12-05 10:35:33.766016: step 3450, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.427 sec/batch; 39h:00m:32s remains)
INFO - root - 2017-12-05 10:35:38.246238: step 3460, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 40h:05m:28s remains)
INFO - root - 2017-12-05 10:35:42.884823: step 3470, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.459 sec/batch; 41h:54m:39s remains)
INFO - root - 2017-12-05 10:35:47.369126: step 3480, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.454 sec/batch; 41h:28m:19s remains)
INFO - root - 2017-12-05 10:35:51.838395: step 3490, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.464 sec/batch; 42h:23m:45s remains)
INFO - root - 2017-12-05 10:35:56.298142: step 3500, loss = 2.04, batch loss = 1.98 (17.2 examples/sec; 0.464 sec/batch; 42h:25m:40s remains)
2017-12-05 10:35:56.762834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3374763 -4.3405008 -4.3426909 -4.3432822 -4.3415833 -4.3394327 -4.3396106 -4.3410163 -4.3420548 -4.3452249 -4.3490262 -4.3527379 -4.3544025 -4.352685 -4.3491178][-4.3347139 -4.3349524 -4.3346066 -4.3334322 -4.331296 -4.3271127 -4.325984 -4.3268795 -4.3297825 -4.335608 -4.3409576 -4.3449588 -4.3469481 -4.3435779 -4.3383555][-4.326508 -4.3235922 -4.3201575 -4.3167949 -4.3144245 -4.3098607 -4.3075428 -4.3060055 -4.3086052 -4.3142109 -4.3168569 -4.3182883 -4.3205323 -4.3193746 -4.3174639][-4.3219094 -4.3165121 -4.3084908 -4.3002014 -4.2955461 -4.2885942 -4.2833753 -4.2766752 -4.2744684 -4.2758136 -4.2747612 -4.2737103 -4.2770591 -4.280776 -4.2857046][-4.3159471 -4.309001 -4.2958326 -4.2814465 -4.2717562 -4.2615223 -4.2526846 -4.2404947 -4.23263 -4.2294474 -4.2276154 -4.2300935 -4.2367473 -4.2435188 -4.2534847][-4.3054295 -4.2983451 -4.2825594 -4.2619572 -4.2457681 -4.2319489 -4.2215338 -4.2094316 -4.1992769 -4.1920357 -4.1897087 -4.1986794 -4.2113252 -4.2208548 -4.2325559][-4.2961988 -4.2896547 -4.2726607 -4.2506561 -4.2302642 -4.20862 -4.193284 -4.18439 -4.17919 -4.1701412 -4.1637783 -4.1757855 -4.1935949 -4.2062111 -4.2191429][-4.2938557 -4.2879744 -4.2722082 -4.2522855 -4.2284265 -4.1929603 -4.1668081 -4.1606231 -4.1617975 -4.1600404 -4.1576447 -4.1733942 -4.1952815 -4.2082996 -4.2193532][-4.2924075 -4.288991 -4.2780633 -4.2586389 -4.227232 -4.1767683 -4.1414361 -4.1366072 -4.1476493 -4.1608791 -4.1749134 -4.1944528 -4.2139091 -4.2218957 -4.2273679][-4.2875361 -4.2906408 -4.2847772 -4.2617965 -4.2189627 -4.1618567 -4.1278811 -4.1298218 -4.1534972 -4.1781683 -4.2009177 -4.2202897 -4.2354031 -4.2371597 -4.2365894][-4.2870393 -4.2973356 -4.2981772 -4.2763162 -4.2326059 -4.1790195 -4.1496124 -4.1578083 -4.185853 -4.209682 -4.2280807 -4.2414756 -4.2517939 -4.2517214 -4.2477126][-4.2981949 -4.314775 -4.3229804 -4.310277 -4.2784314 -4.2359381 -4.2078562 -4.2119207 -4.2311325 -4.2500205 -4.2614441 -4.2651892 -4.2674627 -4.262012 -4.2548151][-4.3023362 -4.3222008 -4.3344522 -4.3321109 -4.3145118 -4.2864523 -4.2619081 -4.2606859 -4.2701941 -4.2813535 -4.2860279 -4.2842283 -4.2779932 -4.2670665 -4.2546654][-4.29314 -4.3105812 -4.3210697 -4.3235307 -4.3163323 -4.2995853 -4.2813745 -4.2747612 -4.2801514 -4.288187 -4.2897129 -4.2858219 -4.2744827 -4.260314 -4.2461295][-4.2752662 -4.2864881 -4.291471 -4.2929506 -4.2896833 -4.2785997 -4.2651768 -4.2561717 -4.2606044 -4.2682486 -4.2700119 -4.2670641 -4.2552209 -4.2425952 -4.2323222]]...]
INFO - root - 2017-12-05 10:36:01.074473: step 3510, loss = 2.08, batch loss = 2.03 (17.5 examples/sec; 0.458 sec/batch; 41h:49m:28s remains)
INFO - root - 2017-12-05 10:36:05.553311: step 3520, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.453 sec/batch; 41h:25m:32s remains)
INFO - root - 2017-12-05 10:36:10.053002: step 3530, loss = 2.06, batch loss = 2.00 (18.3 examples/sec; 0.438 sec/batch; 40h:02m:24s remains)
INFO - root - 2017-12-05 10:36:14.637752: step 3540, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.438 sec/batch; 39h:59m:13s remains)
INFO - root - 2017-12-05 10:36:19.185403: step 3550, loss = 2.05, batch loss = 1.99 (17.5 examples/sec; 0.458 sec/batch; 41h:50m:26s remains)
INFO - root - 2017-12-05 10:36:23.667962: step 3560, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 39h:57m:14s remains)
INFO - root - 2017-12-05 10:36:28.221862: step 3570, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.447 sec/batch; 40h:53m:03s remains)
INFO - root - 2017-12-05 10:36:32.752947: step 3580, loss = 2.05, batch loss = 1.99 (17.7 examples/sec; 0.452 sec/batch; 41h:16m:01s remains)
INFO - root - 2017-12-05 10:36:37.263927: step 3590, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.451 sec/batch; 41h:10m:27s remains)
INFO - root - 2017-12-05 10:36:41.634357: step 3600, loss = 2.06, batch loss = 2.00 (31.5 examples/sec; 0.254 sec/batch; 23h:11m:13s remains)
2017-12-05 10:36:42.108462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2889724 -4.2946949 -4.2972965 -4.2886095 -4.2647724 -4.2263036 -4.1847973 -4.1540127 -4.1417141 -4.1420264 -4.159627 -4.2073526 -4.2562551 -4.2875671 -4.3032765][-4.31316 -4.3205175 -4.3216567 -4.3110356 -4.2874613 -4.2501931 -4.2131867 -4.1818914 -4.1580758 -4.1505489 -4.1685343 -4.216135 -4.2622008 -4.2938542 -4.3086185][-4.3106594 -4.3211818 -4.3215342 -4.3122911 -4.2925348 -4.2599788 -4.2285814 -4.2010841 -4.1777639 -4.174902 -4.1956329 -4.2322531 -4.2637644 -4.2842288 -4.2949481][-4.2862945 -4.3008423 -4.300838 -4.2878065 -4.2686787 -4.2369876 -4.2101054 -4.1907244 -4.1781578 -4.186161 -4.2082014 -4.2333775 -4.250227 -4.2570987 -4.2649584][-4.2577257 -4.2687869 -4.2603335 -4.2367272 -4.2124114 -4.181757 -4.1588616 -4.1536179 -4.1537066 -4.1675563 -4.1862469 -4.204535 -4.2103577 -4.2067847 -4.2209563][-4.2423396 -4.2425375 -4.2177615 -4.1763334 -4.1411791 -4.1037993 -4.0840092 -4.0926566 -4.1032925 -4.1218233 -4.1454954 -4.1649818 -4.1617861 -4.1506786 -4.1724982][-4.2335835 -4.219048 -4.1759877 -4.1200857 -4.0731039 -4.0284481 -4.0014739 -4.0168514 -4.0376177 -4.0705433 -4.1074448 -4.1276507 -4.1202374 -4.1079721 -4.1384993][-4.2316947 -4.2063718 -4.1614647 -4.1095715 -4.06694 -4.0246143 -3.9893727 -3.9994826 -4.0204806 -4.0587587 -4.1011071 -4.1136422 -4.0966992 -4.0824423 -4.1148062][-4.241415 -4.2195077 -4.1852684 -4.1509356 -4.1271152 -4.1012936 -4.0746994 -4.0797439 -4.0940657 -4.119812 -4.1504455 -4.1495466 -4.1168118 -4.0976481 -4.1260695][-4.2510219 -4.2371888 -4.220253 -4.207346 -4.2016921 -4.1932039 -4.17689 -4.1769724 -4.184752 -4.1985292 -4.2189016 -4.2105708 -4.1746554 -4.1611652 -4.1857228][-4.2643 -4.257905 -4.2536445 -4.2538481 -4.2581692 -4.260428 -4.2512565 -4.2500095 -4.2513342 -4.2578926 -4.2703419 -4.2604451 -4.2324543 -4.2303925 -4.2537518][-4.2782149 -4.2730622 -4.2738881 -4.2806444 -4.2916908 -4.3012872 -4.2980661 -4.2958179 -4.2907658 -4.2893658 -4.2934942 -4.28707 -4.273716 -4.2809591 -4.303463][-4.2854 -4.2765913 -4.2780533 -4.2864833 -4.2997994 -4.3120408 -4.3107963 -4.3028727 -4.2920923 -4.2899175 -4.2948389 -4.2960353 -4.2960415 -4.3101063 -4.33227][-4.2839155 -4.2694573 -4.2690005 -4.276659 -4.2901506 -4.3023777 -4.3019996 -4.2931314 -4.2828984 -4.2859464 -4.2967267 -4.3054008 -4.3119211 -4.3283038 -4.3481989][-4.2876353 -4.2687931 -4.2625346 -4.2640867 -4.2704659 -4.2761326 -4.2757826 -4.2695909 -4.2679605 -4.2786632 -4.2989907 -4.3139067 -4.3243952 -4.3406196 -4.3584905]]...]
INFO - root - 2017-12-05 10:36:46.580713: step 3610, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.451 sec/batch; 41h:13m:43s remains)
INFO - root - 2017-12-05 10:36:51.048221: step 3620, loss = 2.05, batch loss = 1.99 (18.0 examples/sec; 0.445 sec/batch; 40h:40m:08s remains)
INFO - root - 2017-12-05 10:36:55.659924: step 3630, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.462 sec/batch; 42h:09m:40s remains)
INFO - root - 2017-12-05 10:37:00.163688: step 3640, loss = 2.06, batch loss = 2.00 (17.8 examples/sec; 0.449 sec/batch; 41h:01m:17s remains)
INFO - root - 2017-12-05 10:37:04.737228: step 3650, loss = 2.08, batch loss = 2.02 (17.5 examples/sec; 0.457 sec/batch; 41h:42m:30s remains)
INFO - root - 2017-12-05 10:37:09.257124: step 3660, loss = 2.10, batch loss = 2.04 (17.4 examples/sec; 0.460 sec/batch; 41h:58m:30s remains)
INFO - root - 2017-12-05 10:37:13.931856: step 3670, loss = 2.05, batch loss = 1.99 (16.2 examples/sec; 0.493 sec/batch; 45h:01m:48s remains)
INFO - root - 2017-12-05 10:37:20.650440: step 3680, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:57m:05s remains)
INFO - root - 2017-12-05 10:37:27.341144: step 3690, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 60h:56m:11s remains)
INFO - root - 2017-12-05 10:37:33.994243: step 3700, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 67h:23m:07s remains)
2017-12-05 10:37:34.559831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2087708 -4.2154937 -4.2263484 -4.2397079 -4.2508245 -4.2623577 -4.270967 -4.2614965 -4.2463784 -4.2212796 -4.214849 -4.2313018 -4.2658877 -4.2973261 -4.3201213][-4.1660681 -4.1788311 -4.19709 -4.2185678 -4.2347536 -4.2452593 -4.2493191 -4.2306066 -4.2091627 -4.1791477 -4.1765175 -4.1976719 -4.236969 -4.2741385 -4.3038044][-4.1794043 -4.1907086 -4.2067461 -4.2266974 -4.2403927 -4.2450786 -4.2338758 -4.2002215 -4.1767178 -4.1540265 -4.1617455 -4.186646 -4.2265048 -4.2599478 -4.2874422][-4.1909304 -4.1993871 -4.2131133 -4.2315078 -4.2403631 -4.2275844 -4.1844831 -4.1272073 -4.1074576 -4.1104288 -4.146317 -4.1876779 -4.2338438 -4.2615633 -4.28127][-4.1951203 -4.1959157 -4.2047286 -4.2186718 -4.2184324 -4.1835151 -4.106534 -4.0202103 -4.0128927 -4.0587559 -4.1314278 -4.190908 -4.2442579 -4.2701216 -4.283823][-4.2053742 -4.1948605 -4.1910977 -4.1898527 -4.1713653 -4.1034193 -3.9788761 -3.8584757 -3.8809056 -3.9907074 -4.105618 -4.1842704 -4.2466674 -4.2758336 -4.289916][-4.2236018 -4.2047591 -4.1875429 -4.1650858 -4.1203303 -4.0113363 -3.8245115 -3.6492634 -3.7162528 -3.90624 -4.0702577 -4.1727567 -4.2450023 -4.2790666 -4.2957735][-4.2584238 -4.2403617 -4.2156253 -4.178442 -4.1142855 -3.9858997 -3.77476 -3.5856812 -3.6816883 -3.9010773 -4.073113 -4.1782112 -4.2467389 -4.2775016 -4.2921534][-4.2839351 -4.263721 -4.2377071 -4.20199 -4.1430717 -4.0396161 -3.8850863 -3.7746332 -3.8624299 -4.0184379 -4.1424956 -4.2164683 -4.2562609 -4.2689676 -4.2798057][-4.2951093 -4.27211 -4.2454987 -4.2135067 -4.1688023 -4.1046643 -4.0207558 -3.9791932 -4.0544686 -4.1533427 -4.2267809 -4.2662163 -4.27548 -4.2681847 -4.2698159][-4.3023849 -4.277338 -4.2494636 -4.2194948 -4.185802 -4.1501894 -4.1155457 -4.1165357 -4.1791654 -4.2446589 -4.2857695 -4.3020267 -4.2948432 -4.2760081 -4.2705526][-4.3019633 -4.2798948 -4.255939 -4.232502 -4.2112255 -4.1938686 -4.1836128 -4.1974277 -4.2405572 -4.280838 -4.305192 -4.3128119 -4.3011532 -4.2809935 -4.2769413][-4.29209 -4.2764921 -4.257822 -4.2421412 -4.2336578 -4.2280903 -4.2244129 -4.2363358 -4.265141 -4.2902 -4.3079348 -4.3122964 -4.3018389 -4.2864714 -4.2860675][-4.2793121 -4.2699323 -4.257092 -4.2475362 -4.2482066 -4.2452207 -4.2406688 -4.2509074 -4.2755189 -4.2970552 -4.312407 -4.3170314 -4.3118992 -4.2999368 -4.29953][-4.2701473 -4.2666082 -4.2625895 -4.2594948 -4.2581568 -4.2491984 -4.2375426 -4.2470732 -4.2741766 -4.2960682 -4.3097358 -4.318274 -4.3177295 -4.30936 -4.3091164]]...]
INFO - root - 2017-12-05 10:37:41.325192: step 3710, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 61h:44m:51s remains)
INFO - root - 2017-12-05 10:37:48.161279: step 3720, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 60h:18m:24s remains)
INFO - root - 2017-12-05 10:37:55.017275: step 3730, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 61h:49m:30s remains)
INFO - root - 2017-12-05 10:38:01.850546: step 3740, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 63h:50m:25s remains)
INFO - root - 2017-12-05 10:38:08.747648: step 3750, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.687 sec/batch; 62h:45m:44s remains)
INFO - root - 2017-12-05 10:38:15.562864: step 3760, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 65h:16m:23s remains)
INFO - root - 2017-12-05 10:38:22.218349: step 3770, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 56h:42m:27s remains)
INFO - root - 2017-12-05 10:38:29.016439: step 3780, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 60h:45m:27s remains)
INFO - root - 2017-12-05 10:38:35.772085: step 3790, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 63h:20m:08s remains)
INFO - root - 2017-12-05 10:38:42.610868: step 3800, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 62h:04m:33s remains)
2017-12-05 10:38:43.147611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2027869 -4.1324549 -4.0832162 -4.0626736 -4.1108141 -4.1865454 -4.2203107 -4.22285 -4.2139487 -4.2046041 -4.2018127 -4.2002025 -4.1941676 -4.20777 -4.2386041][-4.2116828 -4.1547303 -4.1177788 -4.1114917 -4.1617737 -4.218308 -4.2382984 -4.2351694 -4.2223563 -4.2136312 -4.2289267 -4.2513733 -4.2597542 -4.2660007 -4.2722373][-4.2438445 -4.2038822 -4.1799722 -4.1830974 -4.2219796 -4.2520447 -4.2527456 -4.2330217 -4.20794 -4.2014041 -4.2302628 -4.2678466 -4.286015 -4.2904806 -4.286644][-4.2810903 -4.2582922 -4.2455955 -4.248239 -4.267621 -4.2711549 -4.2422519 -4.1918988 -4.1529322 -4.1625218 -4.2140942 -4.2641191 -4.2866712 -4.2897382 -4.2826695][-4.30762 -4.3024235 -4.2997775 -4.2960415 -4.2871513 -4.260344 -4.1860409 -4.0918045 -4.050735 -4.0995026 -4.1809292 -4.2431407 -4.272481 -4.2814159 -4.278542][-4.3183622 -4.324482 -4.3239355 -4.3046064 -4.2670436 -4.2010341 -4.0664358 -3.923223 -3.9140208 -4.0282445 -4.136199 -4.2102757 -4.253036 -4.276865 -4.2840486][-4.3242025 -4.3337364 -4.3255982 -4.2891316 -4.2229052 -4.105535 -3.9020784 -3.7169032 -3.7862186 -3.9733863 -4.1046596 -4.1881452 -4.2426672 -4.27684 -4.2909727][-4.3240442 -4.3349433 -4.3196588 -4.2682815 -4.1775179 -4.0236363 -3.7912803 -3.62568 -3.7735524 -3.9817169 -4.1101151 -4.1912503 -4.2488279 -4.2861619 -4.3019423][-4.3186393 -4.3243475 -4.30013 -4.2390513 -4.1425843 -4.001616 -3.8227706 -3.7520761 -3.8970294 -4.0555096 -4.1520348 -4.2215462 -4.2723351 -4.3018694 -4.3115482][-4.3141513 -4.3146524 -4.2868018 -4.2263594 -4.1450849 -4.0428276 -3.9440663 -3.9447544 -4.0521588 -4.1528344 -4.2177992 -4.2698793 -4.304718 -4.3179355 -4.3196487][-4.313221 -4.3117146 -4.2865281 -4.2394633 -4.1847463 -4.1251979 -4.0850506 -4.1105938 -4.1800728 -4.2394075 -4.2816205 -4.3152652 -4.3312531 -4.3310823 -4.326416][-4.3161917 -4.3146167 -4.2955632 -4.2693944 -4.2454529 -4.2226119 -4.2149796 -4.237637 -4.2755332 -4.3076172 -4.3302622 -4.34414 -4.3468204 -4.3382449 -4.3308439][-4.3224206 -4.3230915 -4.3143272 -4.3078737 -4.3058848 -4.3021183 -4.3040819 -4.3148627 -4.3317013 -4.3454356 -4.3537793 -4.3536191 -4.3488917 -4.3379855 -4.331471][-4.325448 -4.3291793 -4.3300672 -4.3340559 -4.340476 -4.3438325 -4.3455038 -4.3455458 -4.3483453 -4.3506432 -4.3500462 -4.3445315 -4.3376517 -4.3297629 -4.3255062][-4.3209734 -4.3241978 -4.3268175 -4.3307171 -4.3363566 -4.3406968 -4.3420134 -4.3393865 -4.33628 -4.3329172 -4.3292689 -4.32548 -4.3216329 -4.31857 -4.3166885]]...]
INFO - root - 2017-12-05 10:38:49.958073: step 3810, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 64h:11m:55s remains)
INFO - root - 2017-12-05 10:38:56.768352: step 3820, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 63h:29m:03s remains)
INFO - root - 2017-12-05 10:39:03.555856: step 3830, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 62h:47m:08s remains)
INFO - root - 2017-12-05 10:39:10.472483: step 3840, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.706 sec/batch; 64h:27m:28s remains)
INFO - root - 2017-12-05 10:39:17.198079: step 3850, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.669 sec/batch; 61h:02m:00s remains)
INFO - root - 2017-12-05 10:39:24.133733: step 3860, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 64h:53m:14s remains)
INFO - root - 2017-12-05 10:39:30.733868: step 3870, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 61h:35m:46s remains)
INFO - root - 2017-12-05 10:39:37.290262: step 3880, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 62h:28m:16s remains)
INFO - root - 2017-12-05 10:39:44.168520: step 3890, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 62h:50m:50s remains)
INFO - root - 2017-12-05 10:39:50.991918: step 3900, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 64h:28m:06s remains)
2017-12-05 10:39:51.536847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2918029 -4.2864227 -4.2869172 -4.29024 -4.2956576 -4.3008914 -4.30498 -4.3040614 -4.2957497 -4.2824922 -4.2678971 -4.2594833 -4.2687078 -4.2932539 -4.3181515][-4.2968106 -4.2939162 -4.2957921 -4.2986503 -4.3016448 -4.3033881 -4.3053174 -4.3061419 -4.3033895 -4.2992835 -4.29469 -4.2909827 -4.297801 -4.3158469 -4.3336124][-4.3066287 -4.3052011 -4.306035 -4.3036003 -4.2983732 -4.2909951 -4.2858152 -4.2838387 -4.2828622 -4.2846718 -4.2878256 -4.2897406 -4.2977066 -4.3146877 -4.3326273][-4.3173189 -4.3158736 -4.3131657 -4.3029037 -4.28551 -4.263505 -4.2465892 -4.2368331 -4.2348504 -4.2402792 -4.2485466 -4.2541704 -4.264791 -4.2854996 -4.3101711][-4.3236861 -4.3216324 -4.3132362 -4.2923622 -4.2590265 -4.2182508 -4.1868653 -4.1708035 -4.1702604 -4.1770244 -4.1847262 -4.19003 -4.2052526 -4.236412 -4.2747641][-4.3119807 -4.3105111 -4.2988415 -4.267035 -4.2141471 -4.1513925 -4.1041059 -4.0849562 -4.0865526 -4.0961733 -4.103466 -4.112011 -4.139617 -4.18823 -4.2439823][-4.27616 -4.2788854 -4.26709 -4.2264357 -4.1579409 -4.0783372 -4.0191808 -3.9962862 -4.0032454 -4.0236549 -4.0420928 -4.0659547 -4.1131487 -4.1785579 -4.2436075][-4.22401 -4.235878 -4.2293997 -4.1854286 -4.1098185 -4.0225725 -3.9569433 -3.933883 -3.9527972 -3.9953439 -4.041482 -4.0886865 -4.1492062 -4.2159157 -4.2741604][-4.1774864 -4.2028685 -4.2084556 -4.1730404 -4.1067834 -4.0286212 -3.9697304 -3.9543712 -3.9863446 -4.0465641 -4.1071558 -4.1612649 -4.2178087 -4.2708025 -4.31283][-4.1664667 -4.2050486 -4.2234015 -4.2034583 -4.1578693 -4.1000156 -4.0583911 -4.0535297 -4.0870714 -4.1410794 -4.1919513 -4.2359266 -4.2778711 -4.3138785 -4.339344][-4.2003751 -4.2454667 -4.2705631 -4.2648091 -4.2385674 -4.1983027 -4.1699009 -4.1671515 -4.1894565 -4.2248592 -4.2601357 -4.29091 -4.316226 -4.3362222 -4.3487668][-4.256619 -4.2944775 -4.3180289 -4.3201513 -4.3068633 -4.2808704 -4.2606215 -4.2565861 -4.2684112 -4.2890849 -4.3107042 -4.32775 -4.3384428 -4.3449969 -4.3479543][-4.3027382 -4.3283305 -4.3471603 -4.3526177 -4.3464837 -4.3301978 -4.3157349 -4.3098192 -4.3144417 -4.3251076 -4.336339 -4.3426504 -4.343019 -4.3410192 -4.3389573][-4.3266392 -4.3398056 -4.3522224 -4.3574944 -4.3552513 -4.3453593 -4.3346057 -4.3277164 -4.3283072 -4.3334317 -4.33918 -4.3409419 -4.3368444 -4.331521 -4.3296862][-4.3341589 -4.3384089 -4.3436933 -4.3447618 -4.3418212 -4.33474 -4.3260541 -4.3195562 -4.3202848 -4.3254585 -4.3305497 -4.3314686 -4.3276587 -4.3238044 -4.3244748]]...]
INFO - root - 2017-12-05 10:39:58.377012: step 3910, loss = 2.09, batch loss = 2.04 (11.4 examples/sec; 0.703 sec/batch; 64h:09m:02s remains)
INFO - root - 2017-12-05 10:40:05.224591: step 3920, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 65h:13m:05s remains)
INFO - root - 2017-12-05 10:40:12.097606: step 3930, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.717 sec/batch; 65h:25m:05s remains)
INFO - root - 2017-12-05 10:40:19.001149: step 3940, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 59h:08m:08s remains)
INFO - root - 2017-12-05 10:40:25.859175: step 3950, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 61h:57m:26s remains)
INFO - root - 2017-12-05 10:40:32.643311: step 3960, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 61h:57m:58s remains)
INFO - root - 2017-12-05 10:40:39.462314: step 3970, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 64h:40m:58s remains)
INFO - root - 2017-12-05 10:40:46.240035: step 3980, loss = 2.11, batch loss = 2.05 (11.4 examples/sec; 0.699 sec/batch; 63h:47m:26s remains)
INFO - root - 2017-12-05 10:40:53.012500: step 3990, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.694 sec/batch; 63h:20m:13s remains)
INFO - root - 2017-12-05 10:40:59.769177: step 4000, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 63h:34m:39s remains)
2017-12-05 10:41:00.349418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2002115 -4.2176828 -4.2466435 -4.262548 -4.2666569 -4.2692575 -4.2709479 -4.27285 -4.2773781 -4.2865534 -4.2955637 -4.2922239 -4.2835035 -4.276916 -4.2753167][-4.1880364 -4.2061396 -4.2391057 -4.2584829 -4.2595434 -4.2582231 -4.2564244 -4.258276 -4.2663131 -4.2761416 -4.2845321 -4.2833405 -4.274117 -4.265162 -4.2607284][-4.180891 -4.1913872 -4.2184777 -4.2348385 -4.230947 -4.2221317 -4.2176762 -4.2270508 -4.2485547 -4.2685156 -4.280365 -4.2810087 -4.2707157 -4.2566628 -4.2463722][-4.1668391 -4.1669211 -4.1857891 -4.1990371 -4.193542 -4.1824656 -4.1776876 -4.1932049 -4.2279816 -4.2601509 -4.2766194 -4.2764955 -4.2641063 -4.2473793 -4.2304134][-4.1538949 -4.145833 -4.1606741 -4.1738515 -4.1709123 -4.1606894 -4.1546664 -4.17264 -4.213696 -4.2508869 -4.2682467 -4.2666259 -4.253499 -4.2356334 -4.2158527][-4.1615124 -4.1477952 -4.1565223 -4.1677089 -4.1643376 -4.1492791 -4.134017 -4.1460686 -4.1868057 -4.2282591 -4.2498589 -4.25267 -4.2458262 -4.2322268 -4.2141333][-4.1873059 -4.1717286 -4.1717558 -4.1747713 -4.1620569 -4.1307869 -4.0957565 -4.0933089 -4.1362672 -4.1906404 -4.2255235 -4.2435074 -4.248271 -4.2416296 -4.2268147][-4.2083058 -4.1974 -4.1938796 -4.1881843 -4.1624475 -4.1120138 -4.05255 -4.0287437 -4.0788832 -4.1538782 -4.2106509 -4.245163 -4.2587667 -4.2576346 -4.2460089][-4.2197914 -4.216681 -4.2120409 -4.20215 -4.1737189 -4.1185713 -4.0460715 -4.0030866 -4.0498347 -4.1348495 -4.2042332 -4.2467318 -4.2648082 -4.2678747 -4.2599998][-4.2307014 -4.23284 -4.2249808 -4.2117472 -4.1908326 -4.1485715 -4.0855455 -4.0420036 -4.0698981 -4.1402073 -4.201755 -4.2423792 -4.2618475 -4.2695317 -4.2681][-4.2406836 -4.2439823 -4.238657 -4.2277222 -4.2134876 -4.1830521 -4.1348653 -4.0974298 -4.1083665 -4.1584773 -4.2059193 -4.2413797 -4.2614574 -4.2718649 -4.2740831][-4.2579093 -4.259831 -4.2574954 -4.2545 -4.2479777 -4.2241359 -4.1851954 -4.1524715 -4.1522403 -4.1872334 -4.2242055 -4.2523251 -4.2685094 -4.279192 -4.28077][-4.276134 -4.275743 -4.2737212 -4.2747188 -4.2722473 -4.2549973 -4.2268796 -4.2033958 -4.2003565 -4.2256393 -4.2517271 -4.2664838 -4.2744942 -4.2830706 -4.2817497][-4.28543 -4.28415 -4.2813106 -4.277873 -4.2725143 -4.2615004 -4.2482681 -4.2376842 -4.2367129 -4.2542076 -4.2711496 -4.2768321 -4.2788587 -4.2821279 -4.2788525][-4.27774 -4.2770543 -4.272048 -4.2598929 -4.2464275 -4.2388239 -4.2393122 -4.241045 -4.2429423 -4.2577085 -4.2736015 -4.2800961 -4.2811503 -4.2810073 -4.2765341]]...]
INFO - root - 2017-12-05 10:41:07.168068: step 4010, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 62h:23m:13s remains)
INFO - root - 2017-12-05 10:41:14.040681: step 4020, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.683 sec/batch; 62h:20m:57s remains)
INFO - root - 2017-12-05 10:41:20.891758: step 4030, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 59h:28m:40s remains)
INFO - root - 2017-12-05 10:41:27.775391: step 4040, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.684 sec/batch; 62h:24m:05s remains)
INFO - root - 2017-12-05 10:41:34.555107: step 4050, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 61h:25m:08s remains)
INFO - root - 2017-12-05 10:41:41.325811: step 4060, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.716 sec/batch; 65h:19m:03s remains)
INFO - root - 2017-12-05 10:41:48.103155: step 4070, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 64h:08m:20s remains)
INFO - root - 2017-12-05 10:41:55.016894: step 4080, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 64h:00m:40s remains)
INFO - root - 2017-12-05 10:42:01.970847: step 4090, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 65h:36m:23s remains)
INFO - root - 2017-12-05 10:42:08.697545: step 4100, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 61h:51m:10s remains)
2017-12-05 10:42:09.277283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3290539 -4.3296938 -4.3250608 -4.3248191 -4.32941 -4.3298683 -4.3278308 -4.3318858 -4.3378482 -4.3356705 -4.32901 -4.3215251 -4.3161564 -4.3130383 -4.3142829][-4.3080282 -4.3037257 -4.3011227 -4.3072658 -4.3143048 -4.3162127 -4.3177695 -4.3273635 -4.3344603 -4.3265977 -4.3115535 -4.2955179 -4.2882204 -4.2872739 -4.2906146][-4.2678261 -4.2607355 -4.2640891 -4.2771258 -4.2849946 -4.2888718 -4.2956653 -4.3090372 -4.3138323 -4.3004813 -4.27942 -4.2598777 -4.2561274 -4.2612109 -4.2688389][-4.2256866 -4.2243 -4.2359161 -4.253098 -4.2596917 -4.2615538 -4.2682652 -4.2794719 -4.2795272 -4.2628417 -4.2435136 -4.2313986 -4.2352338 -4.2465196 -4.2530236][-4.2062678 -4.2123537 -4.2281275 -4.2415881 -4.2411518 -4.2327876 -4.2267337 -4.2316179 -4.2305813 -4.2178755 -4.2108173 -4.2152939 -4.2261691 -4.2364454 -4.2360325][-4.2099476 -4.2171741 -4.2322335 -4.2392011 -4.2240214 -4.1928611 -4.1672974 -4.1685839 -4.1778712 -4.1849165 -4.2014413 -4.2188053 -4.2269716 -4.2284665 -4.2196178][-4.2190437 -4.2148662 -4.2164764 -4.2084455 -4.1707163 -4.1109395 -4.0629697 -4.0749612 -4.1175241 -4.1620588 -4.2032785 -4.2264009 -4.2277026 -4.2170782 -4.198051][-4.2218537 -4.2045455 -4.1912031 -4.1640921 -4.099854 -4.0101151 -3.9409411 -3.9813008 -4.0745416 -4.1551404 -4.208611 -4.2289352 -4.2207417 -4.1997528 -4.1755147][-4.2146292 -4.196002 -4.1799459 -4.1473722 -4.0815935 -3.9957561 -3.938976 -3.9991271 -4.1070848 -4.1880784 -4.2291417 -4.2316484 -4.20745 -4.1795239 -4.1589661][-4.2085843 -4.1978788 -4.1858249 -4.162559 -4.1227822 -4.0782046 -4.0607462 -4.1123252 -4.1835356 -4.2292047 -4.2419982 -4.2199249 -4.1826382 -4.1605306 -4.1577911][-4.207448 -4.2087793 -4.2020283 -4.1902928 -4.1725273 -4.156004 -4.16025 -4.19738 -4.2313757 -4.243012 -4.2320123 -4.193296 -4.1590381 -4.1566133 -4.1771073][-4.212204 -4.2217793 -4.2182379 -4.211195 -4.2013659 -4.196959 -4.2080097 -4.2313027 -4.23847 -4.2285719 -4.2049994 -4.1649375 -4.145782 -4.1643653 -4.2023697][-4.2180562 -4.2284713 -4.2256103 -4.2187114 -4.2117977 -4.2150888 -4.228271 -4.2418532 -4.2386432 -4.2201738 -4.1908994 -4.1583681 -4.1553946 -4.1848183 -4.2286253][-4.1939459 -4.2032204 -4.2034826 -4.1984644 -4.1954021 -4.2027125 -4.2175264 -4.2253175 -4.2208972 -4.2046928 -4.1812057 -4.1616335 -4.1667566 -4.195488 -4.237752][-4.1420121 -4.1503129 -4.1576872 -4.1579695 -4.1585312 -4.17061 -4.1887922 -4.194602 -4.1930566 -4.1860819 -4.175168 -4.1669025 -4.1720457 -4.1911931 -4.2265105]]...]
INFO - root - 2017-12-05 10:42:16.108544: step 4110, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 62h:27m:59s remains)
INFO - root - 2017-12-05 10:42:22.934933: step 4120, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 60h:26m:56s remains)
INFO - root - 2017-12-05 10:42:29.807090: step 4130, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.681 sec/batch; 62h:04m:38s remains)
INFO - root - 2017-12-05 10:42:36.589218: step 4140, loss = 2.03, batch loss = 1.97 (11.3 examples/sec; 0.710 sec/batch; 64h:47m:11s remains)
INFO - root - 2017-12-05 10:42:43.348170: step 4150, loss = 2.04, batch loss = 1.98 (13.1 examples/sec; 0.611 sec/batch; 55h:41m:46s remains)
INFO - root - 2017-12-05 10:42:50.116093: step 4160, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 65h:42m:59s remains)
INFO - root - 2017-12-05 10:42:57.106902: step 4170, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.676 sec/batch; 61h:38m:58s remains)
INFO - root - 2017-12-05 10:43:03.958455: step 4180, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.675 sec/batch; 61h:34m:19s remains)
INFO - root - 2017-12-05 10:43:10.784510: step 4190, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 60h:17m:12s remains)
INFO - root - 2017-12-05 10:43:17.603779: step 4200, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 65h:27m:49s remains)
2017-12-05 10:43:18.166821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1676474 -4.1901865 -4.2095733 -4.21522 -4.2150321 -4.197217 -4.1645436 -4.1276851 -4.1240339 -4.1378193 -4.1230907 -4.088129 -4.0802608 -4.1053939 -4.1506529][-4.1776791 -4.2042689 -4.2283139 -4.2345243 -4.2271233 -4.1991277 -4.1632633 -4.131001 -4.1300917 -4.1354403 -4.1156731 -4.0758386 -4.0586057 -4.07081 -4.1108527][-4.19518 -4.2213526 -4.2453904 -4.2515254 -4.2363667 -4.1964226 -4.1516423 -4.1201582 -4.1240082 -4.1296873 -4.1105733 -4.0738664 -4.0560303 -4.0605307 -4.0880284][-4.224288 -4.2443023 -4.2637882 -4.26762 -4.2415695 -4.1808305 -4.111165 -4.0643039 -4.0745878 -4.0982676 -4.0968175 -4.0746636 -4.0656209 -4.0695596 -4.0904465][-4.2569304 -4.2703533 -4.2828722 -4.2783661 -4.2379808 -4.1571746 -4.0591264 -3.9905243 -3.9995701 -4.044457 -4.0712271 -4.0763478 -4.0826578 -4.0898247 -4.1095529][-4.2840204 -4.2927418 -4.2987294 -4.2852221 -4.2340417 -4.1392493 -4.0237379 -3.9364982 -3.9309113 -3.9824622 -4.0360837 -4.07356 -4.0991077 -4.11612 -4.1402078][-4.2944951 -4.3002472 -4.3003435 -4.279933 -4.2288179 -4.1391573 -4.0301142 -3.9368839 -3.9051542 -3.941941 -4.0038652 -4.0642242 -4.1084428 -4.1400304 -4.1706529][-4.3003917 -4.3072376 -4.30763 -4.284812 -4.2389073 -4.1686053 -4.0824089 -3.9960873 -3.9414084 -3.9468625 -3.9975829 -4.0674577 -4.1261325 -4.1712708 -4.2090464][-4.3033729 -4.3098903 -4.3138027 -4.2949319 -4.2585297 -4.20818 -4.144805 -4.0683484 -4.0056906 -3.9853585 -4.0136771 -4.0796418 -4.1451268 -4.1982765 -4.2385716][-4.3000708 -4.3034477 -4.3101964 -4.3011608 -4.2788529 -4.24727 -4.2001758 -4.1348276 -4.073873 -4.0392551 -4.047915 -4.1007676 -4.1644526 -4.2209558 -4.2604122][-4.3028159 -4.3032756 -4.3119469 -4.3136206 -4.30584 -4.2908177 -4.2594514 -4.2095785 -4.156023 -4.1163769 -4.1086974 -4.1435008 -4.1979847 -4.2516046 -4.2871][-4.3143721 -4.3135419 -4.3215632 -4.3276873 -4.3269587 -4.3213167 -4.3030396 -4.2680278 -4.2260432 -4.1895661 -4.1744823 -4.1946917 -4.2387218 -4.2837358 -4.3131213][-4.322042 -4.3214459 -4.3277473 -4.3337326 -4.3346968 -4.3337893 -4.3247795 -4.30271 -4.2740545 -4.2438436 -4.2278743 -4.2372909 -4.2664776 -4.2996445 -4.3234777][-4.3241282 -4.3232718 -4.3278904 -4.3335176 -4.3353076 -4.336658 -4.3349991 -4.3230247 -4.3047056 -4.28203 -4.2677841 -4.2698803 -4.2859697 -4.3068924 -4.3235722][-4.3242397 -4.3226089 -4.3247466 -4.3285451 -4.3305559 -4.332561 -4.333673 -4.3279605 -4.3163037 -4.3016191 -4.2928905 -4.2934055 -4.3026109 -4.3138647 -4.3233471]]...]
INFO - root - 2017-12-05 10:43:25.038785: step 4210, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 63h:22m:03s remains)
INFO - root - 2017-12-05 10:43:31.846280: step 4220, loss = 2.10, batch loss = 2.04 (11.3 examples/sec; 0.709 sec/batch; 64h:37m:45s remains)
INFO - root - 2017-12-05 10:43:38.856200: step 4230, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 57h:12m:27s remains)
INFO - root - 2017-12-05 10:43:48.161907: step 4240, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 80h:46m:34s remains)
INFO - root - 2017-12-05 10:43:57.316514: step 4250, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 80h:06m:20s remains)
INFO - root - 2017-12-05 10:44:06.373098: step 4260, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 79h:35m:23s remains)
INFO - root - 2017-12-05 10:44:15.497558: step 4270, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 80h:47m:34s remains)
INFO - root - 2017-12-05 10:44:24.614250: step 4280, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.952 sec/batch; 86h:48m:19s remains)
INFO - root - 2017-12-05 10:44:33.758879: step 4290, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:01m:36s remains)
INFO - root - 2017-12-05 10:44:42.920636: step 4300, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.894 sec/batch; 81h:32m:53s remains)
2017-12-05 10:44:43.651424: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1738305 -4.1760006 -4.1747494 -4.1671753 -4.1524391 -4.1332841 -4.1184745 -4.11841 -4.1329122 -4.1509366 -4.1651092 -4.1732802 -4.1738391 -4.1681128 -4.1597013][-4.1596217 -4.1656594 -4.1650786 -4.15354 -4.1291652 -4.0990186 -4.0789523 -4.0828404 -4.1062145 -4.1308031 -4.1510444 -4.1671729 -4.1718254 -4.1654205 -4.154973][-4.1462164 -4.1541634 -4.1520715 -4.1325312 -4.0942893 -4.0524983 -4.030736 -4.0433455 -4.0776262 -4.1102161 -4.137569 -4.1603017 -4.1674576 -4.1590157 -4.1446486][-4.1391144 -4.1467319 -4.1404328 -4.1104445 -4.0574574 -4.0043488 -3.980547 -4.0022106 -4.0492187 -4.0928364 -4.1289206 -4.1551623 -4.16144 -4.1496153 -4.1288886][-4.1400843 -4.142602 -4.1289887 -4.0900483 -4.0256433 -3.9648886 -3.9389267 -3.9706812 -4.0323558 -4.08756 -4.1308079 -4.1579733 -4.1610203 -4.1423669 -4.1104846][-4.1502409 -4.142489 -4.1187406 -4.0706158 -3.9972506 -3.9324396 -3.9058633 -3.9508159 -4.0286484 -4.09362 -4.1418853 -4.1694651 -4.1681724 -4.138483 -4.0911942][-4.1609674 -4.1430631 -4.1104994 -4.05339 -3.9722464 -3.90258 -3.8753071 -3.9349878 -4.0283442 -4.1018548 -4.1537457 -4.1824775 -4.1783161 -4.138145 -4.0759687][-4.1632419 -4.1389933 -4.1041913 -4.0446982 -3.9606218 -3.8849549 -3.8564065 -3.9283164 -4.0329847 -4.113512 -4.1672072 -4.1974421 -4.1924262 -4.1469383 -4.0745912][-4.1536608 -4.1270375 -4.0992517 -4.0528522 -3.983819 -3.9140468 -3.8885961 -3.9582305 -4.0599294 -4.1372871 -4.186502 -4.2153864 -4.2112617 -4.1672707 -4.0942712][-4.1380854 -4.1096992 -4.0932107 -4.0681896 -4.0273023 -3.9780121 -3.9648724 -4.0246329 -4.1089039 -4.1708808 -4.2061281 -4.2282062 -4.2239356 -4.1871319 -4.1241412][-4.1222982 -4.0979323 -4.0955753 -4.0950007 -4.0836792 -4.0551782 -4.0494208 -4.0942216 -4.156364 -4.1997528 -4.2206874 -4.2347374 -4.2303662 -4.2024565 -4.1498575][-4.1061912 -4.0915504 -4.1051 -4.126081 -4.1352811 -4.1211934 -4.1176715 -4.1482282 -4.1912422 -4.2216663 -4.2333627 -4.2404566 -4.2352061 -4.2097082 -4.1594114][-4.0913067 -4.0884147 -4.1147771 -4.1481161 -4.1676674 -4.1640811 -4.1637521 -4.1843371 -4.2144718 -4.2381053 -4.2479739 -4.2514358 -4.2416754 -4.2113256 -4.1573882][-4.0746584 -4.0829811 -4.1164761 -4.1540484 -4.1764431 -4.1797724 -4.1828923 -4.1982369 -4.2222509 -4.2435527 -4.2546506 -4.2558584 -4.2420506 -4.2083111 -4.156693][-4.0625305 -4.0807662 -4.1155043 -4.1496615 -4.166976 -4.1740746 -4.1786356 -4.1868119 -4.2068381 -4.2294841 -4.2432623 -4.2449236 -4.2287574 -4.1954875 -4.1543579]]...]
INFO - root - 2017-12-05 10:44:52.689982: step 4310, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 83h:16m:28s remains)
INFO - root - 2017-12-05 10:45:01.540425: step 4320, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 82h:45m:42s remains)
INFO - root - 2017-12-05 10:45:10.660496: step 4330, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 86h:22m:38s remains)
INFO - root - 2017-12-05 10:45:19.425041: step 4340, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 75h:08m:24s remains)
INFO - root - 2017-12-05 10:45:28.546836: step 4350, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 81h:49m:58s remains)
INFO - root - 2017-12-05 10:45:37.675491: step 4360, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.890 sec/batch; 81h:07m:10s remains)
INFO - root - 2017-12-05 10:45:46.898671: step 4370, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 82h:19m:14s remains)
INFO - root - 2017-12-05 10:45:55.956880: step 4380, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 80h:18m:38s remains)
INFO - root - 2017-12-05 10:46:04.960592: step 4390, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 83h:36m:21s remains)
INFO - root - 2017-12-05 10:46:13.674639: step 4400, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 77h:45m:54s remains)
2017-12-05 10:46:14.410330: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2672629 -4.2453084 -4.2376823 -4.2431517 -4.251502 -4.2574992 -4.2553754 -4.2499142 -4.2466326 -4.248035 -4.2543597 -4.2512684 -4.24556 -4.2303858 -4.2110057][-4.2582855 -4.2387776 -4.2340713 -4.2415109 -4.2482185 -4.2498779 -4.2428856 -4.2351942 -4.2359314 -4.2382913 -4.246727 -4.2452793 -4.238934 -4.2195835 -4.2002583][-4.2507176 -4.2365646 -4.2355347 -4.2420635 -4.2438493 -4.2398391 -4.2282948 -4.2190228 -4.2239685 -4.2301311 -4.2437153 -4.2471423 -4.2416754 -4.2222424 -4.20369][-4.2426348 -4.2362003 -4.2378154 -4.2382274 -4.2309389 -4.2183537 -4.2002749 -4.1877737 -4.1954794 -4.2096558 -4.2306962 -4.2421122 -4.2432041 -4.2310057 -4.2180591][-4.2204618 -4.2245607 -4.2284169 -4.2222667 -4.2072744 -4.18802 -4.1627913 -4.1430764 -4.1533093 -4.17763 -4.2066941 -4.2263184 -4.2370248 -4.2391915 -4.2356515][-4.1903186 -4.2037659 -4.2084532 -4.1979795 -4.1775055 -4.1505909 -4.1169648 -4.0891695 -4.1021304 -4.1401777 -4.1825566 -4.2099333 -4.2286277 -4.2406311 -4.2435517][-4.1711154 -4.1830373 -4.1849594 -4.1727905 -4.1519542 -4.11908 -4.0744376 -4.0361867 -4.0495443 -4.1029606 -4.1612558 -4.1969018 -4.2204413 -4.2360597 -4.2433758][-4.1576624 -4.1669459 -4.1678276 -4.157186 -4.1370907 -4.1031642 -4.0503473 -3.9997969 -4.0091753 -4.0747786 -4.145843 -4.1882524 -4.2127066 -4.2292175 -4.2391686][-4.1408329 -4.1538033 -4.1599121 -4.1545386 -4.1390028 -4.1108313 -4.0572562 -3.994415 -3.9920378 -4.0560451 -4.1319523 -4.1790657 -4.2044988 -4.2226048 -4.2334781][-4.1258297 -4.1459994 -4.1592 -4.1589322 -4.1508956 -4.1309824 -4.0810766 -4.0101047 -3.9926126 -4.0472012 -4.1203561 -4.169951 -4.1978173 -4.2159719 -4.2259903][-4.1244764 -4.1431732 -4.1570826 -4.1622925 -4.1611633 -4.146183 -4.0988646 -4.0261278 -3.9994135 -4.0437179 -4.1108065 -4.1606712 -4.1882882 -4.2042465 -4.2168489][-4.1193895 -4.1279483 -4.1386228 -4.1486893 -4.1550212 -4.1463213 -4.1023197 -4.0316014 -4.0005288 -4.0379496 -4.1004767 -4.1490493 -4.1763158 -4.1904492 -4.2083325][-4.1134219 -4.1064863 -4.1104555 -4.1236267 -4.1402755 -4.1444097 -4.1107311 -4.0472422 -4.0142045 -4.0447726 -4.1026387 -4.1498461 -4.1777396 -4.1922317 -4.2100987][-4.1121941 -4.0923905 -4.0877295 -4.1002584 -4.1220775 -4.1389947 -4.1217146 -4.0703468 -4.036253 -4.060637 -4.11483 -4.1635604 -4.1954069 -4.2092257 -4.2205052][-4.1254129 -4.0991 -4.0891991 -4.0977907 -4.1185794 -4.1381702 -4.1312232 -4.0877266 -4.0537348 -4.0717983 -4.1250353 -4.1786551 -4.2142153 -4.2250519 -4.2298894]]...]
INFO - root - 2017-12-05 10:46:23.477682: step 4410, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 81h:38m:44s remains)
INFO - root - 2017-12-05 10:46:32.589382: step 4420, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 82h:20m:24s remains)
INFO - root - 2017-12-05 10:46:41.538544: step 4430, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 79h:43m:05s remains)
INFO - root - 2017-12-05 10:46:50.377148: step 4440, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 83h:31m:05s remains)
INFO - root - 2017-12-05 10:46:59.528701: step 4450, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 75h:58m:36s remains)
INFO - root - 2017-12-05 10:47:08.744282: step 4460, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 86h:22m:29s remains)
INFO - root - 2017-12-05 10:47:17.915241: step 4470, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 83h:33m:03s remains)
INFO - root - 2017-12-05 10:47:27.102268: step 4480, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 86h:00m:50s remains)
INFO - root - 2017-12-05 10:47:36.030957: step 4490, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 82h:50m:02s remains)
INFO - root - 2017-12-05 10:47:50.712879: step 4500, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 85h:07m:22s remains)
2017-12-05 10:47:51.410090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3272219 -4.3224373 -4.3179474 -4.3050127 -4.2868218 -4.2661033 -4.2407141 -4.2309256 -4.2416625 -4.2466984 -4.2546754 -4.2746115 -4.2927456 -4.302959 -4.301599][-4.3331232 -4.3269367 -4.3180842 -4.2975178 -4.2693439 -4.233737 -4.1900787 -4.1778502 -4.2058287 -4.2241459 -4.2388592 -4.2675076 -4.2932997 -4.3056779 -4.3041334][-4.3329992 -4.3255548 -4.3141937 -4.2848816 -4.2440667 -4.189641 -4.1301026 -4.1247191 -4.1788926 -4.213316 -4.2336683 -4.2644711 -4.2940855 -4.3109035 -4.3085704][-4.324913 -4.3149047 -4.299017 -4.257688 -4.2034097 -4.1293592 -4.0595012 -4.0699682 -4.1506553 -4.1987681 -4.2225585 -4.2562575 -4.2884216 -4.3095222 -4.3082581][-4.3108225 -4.2969036 -4.2752948 -4.2219963 -4.1534882 -4.0573568 -3.9786258 -4.0141869 -4.1172819 -4.1777239 -4.210474 -4.2515178 -4.2871237 -4.3092122 -4.3077793][-4.2975473 -4.2825809 -4.2527847 -4.1927743 -4.1070333 -3.9719481 -3.8721476 -3.9435377 -4.0741382 -4.1492968 -4.1951156 -4.2445517 -4.2873659 -4.3120742 -4.3118153][-4.2946014 -4.2831678 -4.2517347 -4.1923947 -4.0956111 -3.9275503 -3.7995715 -3.9012239 -4.0558 -4.1397753 -4.1917753 -4.2463946 -4.2937455 -4.3188586 -4.3198113][-4.2967143 -4.293282 -4.2649555 -4.214149 -4.1292129 -3.9787824 -3.858973 -3.9440894 -4.0819635 -4.1584549 -4.2082214 -4.2615924 -4.3020687 -4.3218136 -4.3226056][-4.2920318 -4.29062 -4.2701902 -4.2334604 -4.1720028 -4.0717936 -4.0003009 -4.052577 -4.1431546 -4.1946149 -4.23149 -4.2740431 -4.3029823 -4.315166 -4.3168097][-4.2803721 -4.2746115 -4.2611551 -4.2388535 -4.1969576 -4.1360636 -4.1011639 -4.1379495 -4.1967773 -4.230763 -4.2536564 -4.2791123 -4.2977257 -4.3026714 -4.3033967][-4.2619619 -4.2543874 -4.2463856 -4.2320442 -4.2024293 -4.16607 -4.1490831 -4.1801147 -4.2261882 -4.2548184 -4.2673712 -4.2747946 -4.281425 -4.28111 -4.28237][-4.2434254 -4.2417326 -4.2387309 -4.2266951 -4.2077241 -4.1897516 -4.1825433 -4.2022705 -4.2321095 -4.2527447 -4.2605071 -4.2581539 -4.2588096 -4.2558031 -4.2559152][-4.229032 -4.2345266 -4.235858 -4.2259793 -4.216548 -4.2120566 -4.21349 -4.2221847 -4.2342472 -4.2434964 -4.2474651 -4.2414227 -4.2380948 -4.2347307 -4.2334204][-4.2317662 -4.2428851 -4.2429633 -4.2348871 -4.2307878 -4.2308121 -4.2342248 -4.2375007 -4.2388077 -4.238636 -4.2400031 -4.2321014 -4.2258434 -4.2182317 -4.213717][-4.2500615 -4.2646027 -4.2641854 -4.2562656 -4.2518897 -4.2481542 -4.2455411 -4.2417893 -4.2379704 -4.2356396 -4.2363296 -4.2299514 -4.2215023 -4.2124863 -4.2085457]]...]
INFO - root - 2017-12-05 10:48:00.444724: step 4510, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.922 sec/batch; 83h:59m:32s remains)
INFO - root - 2017-12-05 10:48:09.445960: step 4520, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 80h:55m:33s remains)
INFO - root - 2017-12-05 10:48:18.486112: step 4530, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 83h:51m:55s remains)
INFO - root - 2017-12-05 10:48:27.564390: step 4540, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 83h:22m:05s remains)
INFO - root - 2017-12-05 10:48:36.771685: step 4550, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 82h:04m:16s remains)
INFO - root - 2017-12-05 10:48:46.043285: step 4560, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 85h:58m:16s remains)
INFO - root - 2017-12-05 10:48:55.283006: step 4570, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 85h:02m:39s remains)
INFO - root - 2017-12-05 10:49:04.854792: step 4580, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 88h:41m:14s remains)
INFO - root - 2017-12-05 10:49:14.136620: step 4590, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 83h:25m:46s remains)
INFO - root - 2017-12-05 10:49:23.298657: step 4600, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 88h:10m:41s remains)
2017-12-05 10:49:24.027494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2958851 -4.293983 -4.2945518 -4.295671 -4.2975616 -4.2993226 -4.3004551 -4.3016148 -4.3023968 -4.29738 -4.291079 -4.2886767 -4.2856212 -4.2803807 -4.2739944][-4.2987356 -4.2948561 -4.2948961 -4.2948041 -4.2969179 -4.2994533 -4.3016748 -4.3049393 -4.3077745 -4.3021984 -4.2947783 -4.2897267 -4.2847943 -4.2781787 -4.2705517][-4.2821178 -4.274766 -4.2728553 -4.2714291 -4.2732353 -4.2754574 -4.2786732 -4.2843456 -4.29184 -4.2909927 -4.287889 -4.2850738 -4.2833204 -4.2797103 -4.275764][-4.2564173 -4.2435212 -4.2376609 -4.2332869 -4.2327843 -4.234488 -4.2393789 -4.2485843 -4.2617593 -4.2678008 -4.2719593 -4.2753358 -4.2798781 -4.2822456 -4.2843666][-4.2376695 -4.2179832 -4.2067828 -4.1977615 -4.1937895 -4.1917148 -4.1966238 -4.20782 -4.2259521 -4.2410603 -4.2532253 -4.2638054 -4.2744193 -4.2807269 -4.2846808][-4.19873 -4.1728907 -4.1540623 -4.1382794 -4.1300111 -4.1214094 -4.1200175 -4.1293287 -4.1503267 -4.174984 -4.1992378 -4.2210622 -4.2395248 -4.2530518 -4.262784][-4.1484561 -4.1164122 -4.0893636 -4.0668092 -4.0515571 -4.0314584 -4.0155005 -4.0157185 -4.0367169 -4.0696521 -4.1071081 -4.1442375 -4.1758976 -4.2000928 -4.2195764][-4.140058 -4.1101384 -4.0822916 -4.0568347 -4.0364532 -4.0053835 -3.9715657 -3.95463 -3.9632707 -3.9922376 -4.0329595 -4.0810018 -4.1253018 -4.1590586 -4.1848311][-4.1814942 -4.1605964 -4.1409307 -4.1217227 -4.105979 -4.0816169 -4.0528717 -4.0316133 -4.0244746 -4.0353603 -4.06148 -4.0995984 -4.1373439 -4.1669054 -4.1898537][-4.2377138 -4.2272773 -4.2162609 -4.2043624 -4.1975303 -4.1851931 -4.1682458 -4.1510644 -4.1392145 -4.1420226 -4.15702 -4.1820564 -4.2069893 -4.224021 -4.2359123][-4.2756052 -4.2729049 -4.267921 -4.261375 -4.2604656 -4.2564592 -4.2467961 -4.2356524 -4.2256951 -4.2260141 -4.2353296 -4.2517381 -4.2678442 -4.276433 -4.2786708][-4.2885547 -4.2876272 -4.2865443 -4.2844415 -4.2895851 -4.2947855 -4.2936144 -4.2891235 -4.2826042 -4.2802896 -4.2814684 -4.2875667 -4.2930775 -4.2932835 -4.2890558][-4.2781329 -4.278441 -4.2803111 -4.2819886 -4.2912755 -4.3018451 -4.3052182 -4.3058848 -4.3032227 -4.2952614 -4.2867455 -4.2834916 -4.2820916 -4.2790732 -4.2746077][-4.2526708 -4.2555056 -4.2589436 -4.2614732 -4.2690763 -4.2778897 -4.2819915 -4.2858381 -4.28635 -4.27288 -4.2570434 -4.2498789 -4.2473788 -4.2454476 -4.2443094][-4.2272987 -4.2295351 -4.2323112 -4.2348208 -4.2394781 -4.244102 -4.2465234 -4.250721 -4.2530046 -4.2372117 -4.217762 -4.2085729 -4.208035 -4.2113829 -4.2187481]]...]
INFO - root - 2017-12-05 10:49:33.237240: step 4610, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 82h:53m:27s remains)
INFO - root - 2017-12-05 10:49:42.672977: step 4620, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 86h:01m:05s remains)
INFO - root - 2017-12-05 10:49:51.887711: step 4630, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 83h:02m:32s remains)
INFO - root - 2017-12-05 10:50:01.099150: step 4640, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 83h:22m:20s remains)
INFO - root - 2017-12-05 10:50:10.379097: step 4650, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.978 sec/batch; 89h:05m:44s remains)
INFO - root - 2017-12-05 10:50:19.819475: step 4660, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.877 sec/batch; 79h:52m:06s remains)
INFO - root - 2017-12-05 10:50:29.208602: step 4670, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.868 sec/batch; 79h:02m:44s remains)
INFO - root - 2017-12-05 10:50:38.506737: step 4680, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.993 sec/batch; 90h:25m:43s remains)
INFO - root - 2017-12-05 10:50:47.819775: step 4690, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.914 sec/batch; 83h:15m:12s remains)
INFO - root - 2017-12-05 10:50:56.991306: step 4700, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 82h:26m:52s remains)
2017-12-05 10:50:57.761455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2647915 -4.25781 -4.2683907 -4.2834859 -4.2842364 -4.2698245 -4.2584643 -4.2608714 -4.2697864 -4.2804565 -4.2784505 -4.2513237 -4.2127705 -4.2057433 -4.2310276][-4.253932 -4.2566476 -4.278913 -4.3017216 -4.3013349 -4.2793264 -4.2612548 -4.2603664 -4.2655749 -4.2738853 -4.2776108 -4.2585249 -4.2247109 -4.2208881 -4.247211][-4.2205982 -4.2340183 -4.2694087 -4.3040857 -4.3097162 -4.2817812 -4.2536688 -4.2486815 -4.2554545 -4.2638831 -4.2678771 -4.2521005 -4.2248731 -4.2229176 -4.250134][-4.1748776 -4.1977763 -4.2483392 -4.297163 -4.3105822 -4.2770824 -4.2370324 -4.2287683 -4.243722 -4.2565451 -4.256237 -4.2372561 -4.2188773 -4.2249537 -4.2511611][-4.1603637 -4.1835537 -4.2395229 -4.2934942 -4.3032632 -4.252768 -4.1890368 -4.1767058 -4.2115574 -4.2402258 -4.2422256 -4.2211809 -4.2095685 -4.223918 -4.2548671][-4.1708112 -4.1889 -4.2446747 -4.2889342 -4.2749782 -4.1922197 -4.0962663 -4.0896273 -4.1553984 -4.2057858 -4.2138424 -4.1911488 -4.1824064 -4.203083 -4.2464252][-4.1883941 -4.2029653 -4.249104 -4.2723784 -4.2282767 -4.105237 -3.9791162 -3.9857914 -4.0904021 -4.1603637 -4.1709456 -4.1415811 -4.1311851 -4.1590075 -4.2169528][-4.2081304 -4.2243953 -4.261219 -4.2663198 -4.20214 -4.0591955 -3.9242108 -3.9396129 -4.0643139 -4.1413383 -4.1435795 -4.1004152 -4.0891938 -4.1284633 -4.1993136][-4.2363482 -4.2551374 -4.28401 -4.2796903 -4.21493 -4.0844541 -3.9665396 -3.9785852 -4.0918241 -4.1624527 -4.1558833 -4.1023483 -4.0912971 -4.1356273 -4.2094359][-4.2767138 -4.2925062 -4.3114848 -4.29921 -4.2406907 -4.136054 -4.0386467 -4.0416269 -4.132946 -4.1932859 -4.1823525 -4.1264024 -4.1140294 -4.15923 -4.2328539][-4.3030472 -4.3141279 -4.32628 -4.3120751 -4.2641282 -4.184936 -4.1094222 -4.1045723 -4.1748137 -4.2252464 -4.2114239 -4.1590219 -4.1416893 -4.1823893 -4.253284][-4.3096948 -4.3180966 -4.32564 -4.3146324 -4.2784638 -4.2237983 -4.1731625 -4.1653843 -4.2135944 -4.2536435 -4.2434382 -4.1980515 -4.1746321 -4.206223 -4.2701893][-4.3038931 -4.3120279 -4.3191624 -4.3134732 -4.2889171 -4.2540107 -4.2248311 -4.2193933 -4.2510643 -4.2780676 -4.2703805 -4.2347136 -4.2121572 -4.236444 -4.28882][-4.3044844 -4.3143554 -4.3214688 -4.3191605 -4.3045292 -4.2824693 -4.2668886 -4.2657623 -4.2861629 -4.2998075 -4.2910128 -4.2645411 -4.2512054 -4.2724438 -4.3096752][-4.3120537 -4.3221149 -4.3292689 -4.3309484 -4.3246188 -4.309979 -4.3007355 -4.3016157 -4.3141685 -4.319068 -4.3088522 -4.2888784 -4.2842574 -4.30087 -4.3235044]]...]
INFO - root - 2017-12-05 10:51:07.064253: step 4710, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 81h:40m:09s remains)
INFO - root - 2017-12-05 10:51:16.623427: step 4720, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.957 sec/batch; 87h:10m:06s remains)
INFO - root - 2017-12-05 10:51:25.835232: step 4730, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 82h:14m:18s remains)
INFO - root - 2017-12-05 10:51:35.283535: step 4740, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 81h:58m:20s remains)
INFO - root - 2017-12-05 10:51:44.674257: step 4750, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 82h:03m:51s remains)
INFO - root - 2017-12-05 10:51:53.963480: step 4760, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 81h:14m:41s remains)
INFO - root - 2017-12-05 10:52:03.289388: step 4770, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 83h:23m:42s remains)
INFO - root - 2017-12-05 10:52:12.772552: step 4780, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 81h:54m:29s remains)
INFO - root - 2017-12-05 10:52:22.083145: step 4790, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 81h:02m:03s remains)
INFO - root - 2017-12-05 10:52:31.492516: step 4800, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.938 sec/batch; 85h:21m:23s remains)
2017-12-05 10:52:32.233029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1856327 -4.1894855 -4.1970086 -4.2044449 -4.2084193 -4.2082243 -4.2022266 -4.2016954 -4.2095962 -4.2194633 -4.2226062 -4.2189679 -4.2203298 -4.232964 -4.2531009][-4.1972833 -4.1969051 -4.1996179 -4.2040648 -4.2100945 -4.2156773 -4.2176805 -4.2216315 -4.2299938 -4.2387319 -4.2409759 -4.2385244 -4.243331 -4.2551603 -4.2700753][-4.2179627 -4.2134686 -4.2130871 -4.2157054 -4.2220654 -4.2288475 -4.23224 -4.2349644 -4.2411489 -4.2468805 -4.2445841 -4.239656 -4.2442279 -4.2553596 -4.2666936][-4.2379904 -4.2321448 -4.2309213 -4.2321157 -4.2367592 -4.2394338 -4.2369933 -4.2322288 -4.2330446 -4.2345986 -4.2278738 -4.21798 -4.2189178 -4.2284946 -4.2383513][-4.2431765 -4.2378049 -4.2351022 -4.2348838 -4.2371168 -4.2314715 -4.2140694 -4.1947823 -4.1887827 -4.1899319 -4.1843376 -4.1734376 -4.1724663 -4.1823797 -4.19413][-4.222744 -4.21642 -4.2091866 -4.2070155 -4.206984 -4.190249 -4.1499362 -4.1124778 -4.1064615 -4.1194153 -4.1267056 -4.1226978 -4.1260848 -4.13915 -4.1536031][-4.1820049 -4.1764622 -4.1661296 -4.1615729 -4.1585665 -4.1269712 -4.0571327 -3.9975102 -4.0009575 -4.0440011 -4.0759478 -4.0869 -4.1008549 -4.1177897 -4.1328993][-4.1342778 -4.1323881 -4.1235743 -4.1186714 -4.1147742 -4.0712395 -3.9770007 -3.8986797 -3.9184906 -3.9950843 -4.0514026 -4.07731 -4.0989532 -4.1170349 -4.1316128][-4.0954294 -4.0974569 -4.09246 -4.0900264 -4.096941 -4.069201 -3.9892349 -3.9256537 -3.9582405 -4.0388207 -4.0941176 -4.1173267 -4.1321907 -4.1413608 -4.1516433][-4.0684047 -4.0685816 -4.0661306 -4.07337 -4.1007142 -4.1037412 -4.0642772 -4.0338612 -4.0657883 -4.1262913 -4.1677308 -4.1817646 -4.1845522 -4.182075 -4.18551][-4.0596972 -4.0508366 -4.0489349 -4.0695739 -4.1157985 -4.1422682 -4.1354866 -4.1272731 -4.1511059 -4.1922636 -4.2242179 -4.2370949 -4.2375326 -4.2317843 -4.2315664][-4.069562 -4.0534744 -4.054852 -4.088007 -4.1427312 -4.1768742 -4.1851788 -4.1872711 -4.2039604 -4.232285 -4.2577519 -4.2722769 -4.2765164 -4.275146 -4.2733588][-4.0987182 -4.0811348 -4.0893497 -4.12698 -4.1745515 -4.2041159 -4.2165527 -4.2239313 -4.2374358 -4.2567291 -4.275588 -4.2899485 -4.2975779 -4.2988753 -4.2959104][-4.1438208 -4.1295061 -4.1382213 -4.1667671 -4.1975565 -4.2184234 -4.2310128 -4.2421989 -4.2545528 -4.2675891 -4.2804017 -4.2924852 -4.3010092 -4.3025918 -4.2971816][-4.1742291 -4.1647081 -4.1717029 -4.1906261 -4.2093773 -4.223568 -4.2357 -4.2471418 -4.254756 -4.2570772 -4.2594562 -4.2682023 -4.2765288 -4.2763119 -4.266964]]...]
INFO - root - 2017-12-05 10:52:41.580740: step 4810, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.944 sec/batch; 85h:58m:09s remains)
INFO - root - 2017-12-05 10:52:50.862686: step 4820, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 83h:06m:16s remains)
INFO - root - 2017-12-05 10:53:00.030470: step 4830, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 82h:24m:57s remains)
INFO - root - 2017-12-05 10:53:09.410762: step 4840, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 84h:53m:00s remains)
INFO - root - 2017-12-05 10:53:18.869378: step 4850, loss = 2.05, batch loss = 2.00 (8.0 examples/sec; 1.000 sec/batch; 91h:00m:47s remains)
INFO - root - 2017-12-05 10:53:28.226972: step 4860, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 85h:25m:43s remains)
INFO - root - 2017-12-05 10:53:37.459677: step 4870, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 88h:35m:09s remains)
INFO - root - 2017-12-05 10:53:46.616092: step 4880, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.838 sec/batch; 76h:18m:06s remains)
INFO - root - 2017-12-05 10:53:56.149695: step 4890, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 85h:37m:07s remains)
INFO - root - 2017-12-05 10:54:05.487740: step 4900, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 87h:19m:52s remains)
2017-12-05 10:54:06.191900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2172594 -4.20268 -4.1798592 -4.1568894 -4.1404672 -4.1409988 -4.1541452 -4.1686578 -4.1893606 -4.2078443 -4.2114234 -4.2032366 -4.1980033 -4.1998115 -4.2029324][-4.2434211 -4.2326522 -4.212781 -4.1958575 -4.1896777 -4.1960115 -4.2089739 -4.2248554 -4.24845 -4.2671256 -4.2723432 -4.2654748 -4.2559724 -4.2487159 -4.2412772][-4.2634773 -4.2590022 -4.2464886 -4.2349567 -4.2332969 -4.2376504 -4.2453523 -4.2569461 -4.2796659 -4.3005362 -4.3096595 -4.3066158 -4.2981472 -4.2867632 -4.2712][-4.2617588 -4.2583194 -4.2480865 -4.2372546 -4.2361703 -4.2383251 -4.2405915 -4.2498484 -4.2718267 -4.29318 -4.3040814 -4.3044581 -4.3003469 -4.2905169 -4.2727427][-4.2407045 -4.2328672 -4.2189984 -4.2060461 -4.203373 -4.1989188 -4.1937146 -4.1982346 -4.2150083 -4.2329907 -4.2460475 -4.2523885 -4.2571626 -4.2563848 -4.2465572][-4.2110877 -4.1976995 -4.1760082 -4.1591353 -4.1551027 -4.1446061 -4.1326351 -4.1327524 -4.1414413 -4.1518326 -4.1641116 -4.1780109 -4.19666 -4.2156773 -4.2260075][-4.178194 -4.1593032 -4.1336923 -4.1172767 -4.112112 -4.0977626 -4.0841522 -4.0858574 -4.0877814 -4.0838909 -4.0908937 -4.1135716 -4.1442504 -4.1791205 -4.2067494][-4.168406 -4.1450777 -4.1190462 -4.1032267 -4.0955935 -4.0794287 -4.0649476 -4.064889 -4.0611124 -4.0411973 -4.0399709 -4.0714526 -4.1093974 -4.1493969 -4.1847863][-4.1868458 -4.1623511 -4.138793 -4.1205816 -4.1089816 -4.0966434 -4.0904365 -4.0919275 -4.0842919 -4.0586133 -4.0489912 -4.0730124 -4.1020823 -4.1375117 -4.176774][-4.2142811 -4.1921821 -4.1719832 -4.1530919 -4.1428323 -4.14161 -4.1520143 -4.164989 -4.1640539 -4.1451244 -4.1295953 -4.1355081 -4.1455393 -4.1667652 -4.1961317][-4.2222667 -4.1983509 -4.1768951 -4.1585526 -4.1542377 -4.1653581 -4.1914058 -4.2212334 -4.2368631 -4.2314687 -4.217123 -4.2124372 -4.2080059 -4.2147765 -4.2271786][-4.2147155 -4.183506 -4.1528296 -4.1255369 -4.1186075 -4.1363339 -4.1760988 -4.2242732 -4.2562847 -4.26602 -4.261179 -4.2578115 -4.2503266 -4.2493358 -4.2479482][-4.2199368 -4.1821508 -4.1370878 -4.0899568 -4.0675368 -4.0818911 -4.1284094 -4.1859531 -4.2281122 -4.2488532 -4.2551064 -4.2604342 -4.259069 -4.2572627 -4.2489333][-4.2388921 -4.1997371 -4.1454363 -4.0786624 -4.033473 -4.0301862 -4.062418 -4.1103945 -4.1537323 -4.183125 -4.2034273 -4.2228708 -4.2334642 -4.23731 -4.2309022][-4.2703948 -4.23646 -4.1835313 -4.1120996 -4.0523386 -4.0237956 -4.0270982 -4.0495548 -4.0771356 -4.1005516 -4.1285477 -4.1625433 -4.1867375 -4.2006359 -4.2052298]]...]
INFO - root - 2017-12-05 10:54:15.479968: step 4910, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 88h:48m:50s remains)
INFO - root - 2017-12-05 10:54:24.700486: step 4920, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 85h:11m:03s remains)
INFO - root - 2017-12-05 10:54:34.096791: step 4930, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 87h:01m:14s remains)
INFO - root - 2017-12-05 10:54:43.677176: step 4940, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.003 sec/batch; 91h:16m:22s remains)
INFO - root - 2017-12-05 10:54:52.881531: step 4950, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 86h:04m:03s remains)
INFO - root - 2017-12-05 10:55:02.239615: step 4960, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 88h:06m:07s remains)
INFO - root - 2017-12-05 10:55:11.546587: step 4970, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 85h:49m:23s remains)
INFO - root - 2017-12-05 10:55:20.714459: step 4980, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.899 sec/batch; 81h:49m:22s remains)
INFO - root - 2017-12-05 10:55:30.225369: step 4990, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 87h:32m:09s remains)
INFO - root - 2017-12-05 10:55:39.527518: step 5000, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 84h:35m:30s remains)
2017-12-05 10:55:40.296021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1902366 -4.1979651 -4.1883922 -4.1480312 -4.1149817 -4.10441 -4.1180305 -4.1503186 -4.1777916 -4.2044697 -4.2197528 -4.2330179 -4.2496209 -4.2561817 -4.2642069][-4.1842809 -4.1972718 -4.1952581 -4.1610951 -4.1271996 -4.1197004 -4.130312 -4.1517062 -4.1682172 -4.18914 -4.2064214 -4.221621 -4.2380242 -4.24712 -4.2627048][-4.1819549 -4.20162 -4.1958809 -4.1578369 -4.1246195 -4.1172171 -4.1284332 -4.1473351 -4.1598821 -4.1795993 -4.1975164 -4.2094092 -4.221365 -4.2370505 -4.2606931][-4.1724062 -4.1993408 -4.1948056 -4.1623163 -4.1330423 -4.1169782 -4.1168337 -4.1333971 -4.1419139 -4.1576362 -4.1787095 -4.1908317 -4.1984882 -4.2218108 -4.2556214][-4.1668267 -4.2033105 -4.2071548 -4.1812897 -4.148653 -4.1100416 -4.0915384 -4.1128759 -4.125473 -4.1397915 -4.1627736 -4.1797647 -4.189723 -4.2161655 -4.2514286][-4.17148 -4.2124906 -4.2158852 -4.1827292 -4.1284857 -4.0488505 -4.0037465 -4.0442853 -4.0843363 -4.1122561 -4.1400256 -4.16465 -4.1873879 -4.2193675 -4.2493258][-4.1721187 -4.2073531 -4.2047019 -4.1557751 -4.069643 -3.9353647 -3.8582764 -3.9346395 -4.0266819 -4.0774908 -4.1028409 -4.1263185 -4.16159 -4.20063 -4.2301178][-4.1760221 -4.2005067 -4.1925468 -4.1322584 -4.0258989 -3.8599603 -3.7635565 -3.8736873 -4.0038581 -4.0664382 -4.0803962 -4.0905523 -4.1264677 -4.1677976 -4.199955][-4.2013421 -4.2139769 -4.198101 -4.1381497 -4.0492291 -3.9192538 -3.8493407 -3.9543905 -4.0674558 -4.1110315 -4.1031675 -4.0900221 -4.1137896 -4.1421247 -4.1678643][-4.2401953 -4.2443752 -4.2259297 -4.176486 -4.1152487 -4.0368223 -4.0012174 -4.0690947 -4.1382675 -4.1532731 -4.1292095 -4.1067753 -4.1236668 -4.1415958 -4.1545534][-4.2641635 -4.2653356 -4.2525373 -4.217442 -4.1769757 -4.1314597 -4.1118293 -4.1463561 -4.1800385 -4.1793666 -4.1479645 -4.1224322 -4.1334181 -4.1494007 -4.1521206][-4.2721758 -4.2752752 -4.2687149 -4.2453237 -4.2185707 -4.1947236 -4.1825786 -4.1954341 -4.2072916 -4.1988711 -4.1731405 -4.14908 -4.1479216 -4.1549692 -4.1491055][-4.2829423 -4.2850528 -4.2829003 -4.2667303 -4.249588 -4.2395411 -4.2318068 -4.2337465 -4.235467 -4.2270412 -4.2079735 -4.1832438 -4.1690578 -4.1651869 -4.15674][-4.2987657 -4.2993865 -4.2961063 -4.2820954 -4.2700653 -4.2642918 -4.2588539 -4.2571383 -4.2567239 -4.2521167 -4.240222 -4.219584 -4.20159 -4.1937103 -4.1896739][-4.3142133 -4.3131609 -4.3085155 -4.2962966 -4.2853203 -4.2806983 -4.2761073 -4.2739263 -4.2731366 -4.2738919 -4.2690372 -4.2545047 -4.2396207 -4.2319942 -4.2310057]]...]
INFO - root - 2017-12-05 10:55:49.495557: step 5010, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 83h:08m:57s remains)
INFO - root - 2017-12-05 10:55:58.641964: step 5020, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 85h:14m:35s remains)
INFO - root - 2017-12-05 10:56:07.984984: step 5030, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 83h:23m:04s remains)
INFO - root - 2017-12-05 10:56:17.446525: step 5040, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 84h:33m:23s remains)
INFO - root - 2017-12-05 10:56:26.679840: step 5050, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 86h:49m:21s remains)
INFO - root - 2017-12-05 10:56:35.958947: step 5060, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.931 sec/batch; 84h:42m:48s remains)
INFO - root - 2017-12-05 10:56:45.302119: step 5070, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 88h:02m:15s remains)
INFO - root - 2017-12-05 10:56:54.517566: step 5080, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 86h:26m:37s remains)
INFO - root - 2017-12-05 10:57:03.967999: step 5090, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 79h:21m:49s remains)
INFO - root - 2017-12-05 10:57:13.194697: step 5100, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 86h:09m:29s remains)
2017-12-05 10:57:13.935833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2558241 -4.2636828 -4.2761078 -4.2788324 -4.272634 -4.2575684 -4.2420535 -4.2239919 -4.2032604 -4.17641 -4.158885 -4.1512847 -4.1493282 -4.1566491 -4.1792769][-4.2520719 -4.2614145 -4.2724762 -4.2742677 -4.2724466 -4.2669396 -4.2589669 -4.2334886 -4.1987638 -4.1594734 -4.1319795 -4.1238661 -4.1296334 -4.1499643 -4.18289][-4.2442694 -4.253428 -4.2605162 -4.2617612 -4.2654386 -4.2674222 -4.2622147 -4.2305951 -4.1860871 -4.139657 -4.1071963 -4.1021643 -4.1182981 -4.1530714 -4.1945744][-4.2337751 -4.2454357 -4.25466 -4.2574782 -4.2623544 -4.263473 -4.2539835 -4.2190814 -4.1745167 -4.1292872 -4.0969768 -4.0951247 -4.1192317 -4.1633835 -4.20826][-4.2257953 -4.2390628 -4.2499566 -4.2555618 -4.2593007 -4.2541037 -4.2350206 -4.1995912 -4.1618652 -4.1240873 -4.0995612 -4.1005774 -4.1285424 -4.1727381 -4.2126093][-4.2121625 -4.2247963 -4.2386208 -4.2484016 -4.2472224 -4.2294283 -4.198225 -4.1606889 -4.1344767 -4.1116362 -4.1029463 -4.1078672 -4.1347923 -4.1718278 -4.2050996][-4.2010713 -4.2175541 -4.2295771 -4.2309732 -4.2124472 -4.17798 -4.1342497 -4.0910864 -4.0779133 -4.0781832 -4.0890322 -4.1067796 -4.1348791 -4.1636548 -4.1896477][-4.2001886 -4.2211862 -4.2237482 -4.207274 -4.16687 -4.1116295 -4.046267 -3.9892516 -3.9908466 -4.0234118 -4.0631905 -4.1001568 -4.1323767 -4.1542182 -4.1698256][-4.2067757 -4.2264132 -4.2177596 -4.1837239 -4.121634 -4.0414128 -3.950068 -3.8890324 -3.9150412 -3.9820805 -4.0436568 -4.0922709 -4.1249948 -4.1398115 -4.1450043][-4.2165451 -4.229063 -4.2067161 -4.1580548 -4.0828395 -3.9910417 -3.9013636 -3.8685243 -3.9225075 -3.9983597 -4.0556459 -4.1019073 -4.1291623 -4.1375489 -4.1359863][-4.2256284 -4.2280445 -4.1936584 -4.1387782 -4.0658526 -3.9869492 -3.933888 -3.9464521 -4.0101018 -4.0681486 -4.1061087 -4.1381483 -4.1563282 -4.1598625 -4.1562524][-4.2430615 -4.2310209 -4.18705 -4.1306481 -4.0686159 -4.0163336 -4.0034862 -4.0441642 -4.1074657 -4.1516681 -4.1745319 -4.1926279 -4.2025719 -4.203742 -4.2020936][-4.2661638 -4.237401 -4.1842265 -4.1216192 -4.064229 -4.034194 -4.0520816 -4.1053538 -4.1620274 -4.1980681 -4.2157364 -4.2296209 -4.2377715 -4.2423663 -4.2456317][-4.2795811 -4.2378006 -4.1727486 -4.098135 -4.0407991 -4.0310674 -4.07293 -4.1326504 -4.1813459 -4.2111297 -4.2279253 -4.2419806 -4.2517452 -4.2604618 -4.2686348][-4.2735996 -4.22033 -4.1419897 -4.0583677 -4.0052896 -4.0125475 -4.0733953 -4.13816 -4.1827607 -4.2079916 -4.2240968 -4.2399631 -4.2527957 -4.2645235 -4.2765303]]...]
INFO - root - 2017-12-05 10:57:23.227473: step 5110, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 85h:24m:34s remains)
INFO - root - 2017-12-05 10:57:32.482992: step 5120, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 90h:42m:47s remains)
INFO - root - 2017-12-05 10:57:41.943077: step 5130, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 89h:33m:22s remains)
INFO - root - 2017-12-05 10:57:51.273084: step 5140, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 87h:43m:22s remains)
INFO - root - 2017-12-05 10:58:00.756548: step 5150, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 84h:50m:15s remains)
INFO - root - 2017-12-05 10:58:10.166956: step 5160, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 84h:59m:44s remains)
INFO - root - 2017-12-05 10:58:19.552989: step 5170, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 79h:15m:52s remains)
INFO - root - 2017-12-05 10:58:28.910283: step 5180, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 83h:38m:54s remains)
INFO - root - 2017-12-05 10:58:38.183601: step 5190, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.999 sec/batch; 90h:51m:35s remains)
INFO - root - 2017-12-05 10:58:47.434219: step 5200, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 87h:48m:14s remains)
2017-12-05 10:58:48.143494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2915635 -4.2834082 -4.2761517 -4.2719398 -4.2724633 -4.2753663 -4.2774196 -4.2777209 -4.2775803 -4.2779174 -4.2795272 -4.2813163 -4.2809515 -4.2780347 -4.275022][-4.2948036 -4.2897344 -4.2805891 -4.2700028 -4.2654409 -4.2672439 -4.2718925 -4.2760119 -4.2777696 -4.2773409 -4.2766743 -4.2768383 -4.276412 -4.2749095 -4.2736712][-4.2837415 -4.2865252 -4.2781224 -4.2605872 -4.2475681 -4.2457924 -4.2549672 -4.2681088 -4.2783413 -4.28212 -4.2804985 -4.2782426 -4.276154 -4.2744913 -4.2744327][-4.2524676 -4.2665305 -4.2594419 -4.2336493 -4.2097497 -4.2028317 -4.2167473 -4.2429457 -4.26846 -4.2839317 -4.2880449 -4.2857428 -4.2810721 -4.2769146 -4.2763476][-4.2059121 -4.2345791 -4.227695 -4.1889553 -4.1485109 -4.1333342 -4.1533003 -4.1959472 -4.2421436 -4.2758756 -4.2919226 -4.2934661 -4.288548 -4.2814116 -4.2786112][-4.1554203 -4.2028284 -4.1976357 -4.1436167 -4.0768294 -4.0416594 -4.0643706 -4.1269727 -4.1972547 -4.2530036 -4.2864203 -4.2964759 -4.2927494 -4.2839646 -4.2796335][-4.1102252 -4.1788416 -4.1821222 -4.1182141 -4.0205755 -3.9489408 -3.9580615 -4.0377369 -4.1342525 -4.2143478 -4.2685356 -4.291728 -4.291913 -4.2823372 -4.2776985][-4.0973563 -4.1730103 -4.1858521 -4.1266093 -4.01336 -3.8992181 -3.8693285 -3.9486427 -4.0644674 -4.1656737 -4.2392116 -4.2788506 -4.2867928 -4.277915 -4.2731853][-4.1395059 -4.1944709 -4.2071657 -4.1602497 -4.0627108 -3.9495764 -3.8874602 -3.9284644 -4.0300317 -4.1317668 -4.2131529 -4.265203 -4.2835383 -4.2772708 -4.2700262][-4.2063432 -4.231607 -4.2323518 -4.1940928 -4.1284776 -4.0516648 -4.00222 -4.0100522 -4.067193 -4.1395864 -4.2087197 -4.2620349 -4.2879786 -4.2851748 -4.2740512][-4.2555571 -4.2591581 -4.2461863 -4.2133956 -4.1749678 -4.1340766 -4.1047344 -4.1038947 -4.1293025 -4.1688104 -4.2165222 -4.263978 -4.2946239 -4.2976274 -4.2854824][-4.2731977 -4.2627525 -4.2380624 -4.2089424 -4.188858 -4.1693583 -4.1526442 -4.1489491 -4.15691 -4.1764641 -4.2133522 -4.258513 -4.292881 -4.3040957 -4.2965269][-4.2699513 -4.2459474 -4.2124248 -4.1866951 -4.1768789 -4.1663179 -4.1504207 -4.1365819 -4.1263642 -4.1347628 -4.1759229 -4.2321482 -4.2769094 -4.2998223 -4.3005695][-4.2576866 -4.2232227 -4.186532 -4.1657252 -4.1620717 -4.1541872 -4.1297832 -4.0930281 -4.0549779 -4.0497584 -4.0994787 -4.1736641 -4.2380433 -4.2793427 -4.2945237][-4.2385473 -4.202776 -4.17356 -4.1628728 -4.1668649 -4.1625662 -4.1284385 -4.0641108 -3.9857535 -3.948494 -3.9978471 -4.0919447 -4.1784177 -4.2421465 -4.2772145]]...]
INFO - root - 2017-12-05 10:58:57.610687: step 5210, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 82h:26m:46s remains)
INFO - root - 2017-12-05 10:59:06.709207: step 5220, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 79h:47m:51s remains)
INFO - root - 2017-12-05 10:59:15.836973: step 5230, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 86h:33m:15s remains)
INFO - root - 2017-12-05 10:59:24.994970: step 5240, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 89h:55m:05s remains)
INFO - root - 2017-12-05 10:59:34.334765: step 5250, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 86h:20m:54s remains)
INFO - root - 2017-12-05 10:59:43.769254: step 5260, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 84h:58m:00s remains)
INFO - root - 2017-12-05 10:59:53.119823: step 5270, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 85h:43m:28s remains)
INFO - root - 2017-12-05 11:00:02.115419: step 5280, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 84h:53m:18s remains)
INFO - root - 2017-12-05 11:00:11.265981: step 5290, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 80h:24m:53s remains)
INFO - root - 2017-12-05 11:00:20.673829: step 5300, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 86h:22m:23s remains)
2017-12-05 11:00:21.527880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2873774 -4.2921152 -4.3014212 -4.3053408 -4.2968879 -4.2747965 -4.2604403 -4.2507844 -4.2446008 -4.2430315 -4.2400489 -4.2465134 -4.2641773 -4.2808905 -4.295743][-4.2579541 -4.2639866 -4.2752295 -4.279192 -4.2680593 -4.2392945 -4.22026 -4.2097197 -4.2077422 -4.208252 -4.201869 -4.2068305 -4.2273264 -4.2478781 -4.2658019][-4.209775 -4.21701 -4.2303061 -4.2363305 -4.2232304 -4.1888633 -4.1635251 -4.1543059 -4.1622434 -4.1697545 -4.1621618 -4.1655803 -4.1858025 -4.2062283 -4.2237163][-4.15792 -4.1638222 -4.1793242 -4.1903419 -4.1785665 -4.1382661 -4.1035056 -4.0930533 -4.110321 -4.1300225 -4.1255889 -4.1274729 -4.1477995 -4.1686907 -4.1845231][-4.1175728 -4.1209841 -4.1357617 -4.1505017 -4.140645 -4.0946822 -4.0492477 -4.0358448 -4.0603957 -4.0935593 -4.0993352 -4.1045122 -4.1241326 -4.1447959 -4.1595097][-4.0928688 -4.0976906 -4.1125741 -4.1281118 -4.114892 -4.0587187 -3.9988611 -3.9804044 -4.0109706 -4.0587406 -4.0805464 -4.0936885 -4.1122637 -4.1316309 -4.1439261][-4.0938883 -4.105978 -4.1248684 -4.1378756 -4.1154723 -4.0451283 -3.970149 -3.9450626 -3.9800951 -4.0386815 -4.0776968 -4.0990653 -4.1175814 -4.1345091 -4.1440377][-4.10563 -4.1281567 -4.149899 -4.1602516 -4.1335015 -4.0614281 -3.9871001 -3.9649246 -3.9996812 -4.0531464 -4.0948567 -4.1158342 -4.1302085 -4.1458378 -4.1590371][-4.1045575 -4.1334767 -4.1570835 -4.1664491 -4.1432619 -4.0846119 -4.0258713 -4.0143213 -4.0427661 -4.0803013 -4.1104956 -4.1226921 -4.1302285 -4.1439 -4.1617417][-4.1012592 -4.130856 -4.1556711 -4.1665659 -4.15013 -4.1081333 -4.0675559 -4.0613947 -4.0799775 -4.1028724 -4.1201878 -4.12572 -4.1285992 -4.142025 -4.1652055][-4.0995383 -4.1259408 -4.1497784 -4.161612 -4.15273 -4.1257057 -4.1012397 -4.1004238 -4.1138611 -4.1263566 -4.1341977 -4.1361904 -4.1389632 -4.152657 -4.1763096][-4.0985751 -4.1195416 -4.141222 -4.1547627 -4.1541538 -4.1387038 -4.1270781 -4.1323628 -4.145257 -4.1512108 -4.1515784 -4.1509604 -4.1539564 -4.1690893 -4.1904349][-4.1047821 -4.1214046 -4.1379819 -4.1496358 -4.1515961 -4.1424308 -4.1391764 -4.15055 -4.1656256 -4.1702418 -4.1671791 -4.1650405 -4.1688657 -4.18352 -4.2018542][-4.1135473 -4.1291208 -4.1419659 -4.1509066 -4.1543612 -4.1508265 -4.152905 -4.1659074 -4.1808162 -4.1856794 -4.1825795 -4.18158 -4.186142 -4.1996346 -4.2152929][-4.1281352 -4.1437178 -4.1535177 -4.15956 -4.16417 -4.1647286 -4.1677094 -4.17715 -4.1896515 -4.19622 -4.19509 -4.1944923 -4.20076 -4.2143011 -4.2273345]]...]
INFO - root - 2017-12-05 11:00:30.837235: step 5310, loss = 2.12, batch loss = 2.06 (8.6 examples/sec; 0.934 sec/batch; 84h:53m:30s remains)
INFO - root - 2017-12-05 11:00:40.155778: step 5320, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 84h:52m:44s remains)
INFO - root - 2017-12-05 11:00:49.411635: step 5330, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 88h:38m:41s remains)
INFO - root - 2017-12-05 11:00:58.679160: step 5340, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 86h:06m:08s remains)
INFO - root - 2017-12-05 11:01:07.972249: step 5350, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 84h:19m:44s remains)
INFO - root - 2017-12-05 11:01:17.187772: step 5360, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 83h:11m:46s remains)
INFO - root - 2017-12-05 11:01:26.558060: step 5370, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 83h:35m:05s remains)
INFO - root - 2017-12-05 11:01:35.856275: step 5380, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 82h:18m:40s remains)
INFO - root - 2017-12-05 11:01:45.253375: step 5390, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 82h:49m:14s remains)
INFO - root - 2017-12-05 11:01:54.606040: step 5400, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.914 sec/batch; 83h:04m:59s remains)
2017-12-05 11:01:55.345117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3303318 -4.3261023 -4.305861 -4.2848034 -4.2606626 -4.2312107 -4.2169442 -4.2180715 -4.2171965 -4.2133188 -4.2157793 -4.2163739 -4.222054 -4.21486 -4.2043867][-4.3245912 -4.3190241 -4.2987328 -4.2776976 -4.2492452 -4.211308 -4.1970296 -4.1991277 -4.1993275 -4.1973195 -4.2011051 -4.2020617 -4.2044563 -4.1871309 -4.1766829][-4.3196278 -4.3159313 -4.2975841 -4.2732749 -4.2345319 -4.1827407 -4.1648784 -4.1674705 -4.1703062 -4.1703353 -4.1759543 -4.1780562 -4.1771407 -4.1569715 -4.1473041][-4.3137727 -4.3128738 -4.2978349 -4.2705297 -4.2248645 -4.1641669 -4.1405425 -4.1429648 -4.1440597 -4.1420808 -4.1537509 -4.1645021 -4.1623516 -4.1411972 -4.1360235][-4.3077931 -4.3042383 -4.2858181 -4.2537808 -4.2029753 -4.130065 -4.0959496 -4.0947118 -4.0890131 -4.0811934 -4.1066422 -4.1378593 -4.1427674 -4.1231232 -4.1225863][-4.2970481 -4.2843094 -4.2538061 -4.2098336 -4.149334 -4.0673571 -4.0269594 -4.0212021 -4.0071492 -3.9974148 -4.0441818 -4.1052136 -4.1284857 -4.1225977 -4.12924][-4.286015 -4.2542672 -4.2021909 -4.1425843 -4.0709071 -3.9804997 -3.937139 -3.9393086 -3.9318767 -3.9381177 -4.0072565 -4.0892262 -4.1301723 -4.144434 -4.161211][-4.2709641 -4.2225642 -4.1564012 -4.0891366 -4.0200191 -3.9374356 -3.9014757 -3.9252572 -3.9456127 -3.9762537 -4.0471907 -4.1189146 -4.1578665 -4.1795554 -4.1987782][-4.2477617 -4.1996851 -4.1438856 -4.0974617 -4.0560679 -3.9981358 -3.9802012 -4.0191545 -4.0537963 -4.08876 -4.1374774 -4.1811495 -4.2047234 -4.2213354 -4.2355185][-4.2355123 -4.2075825 -4.1812854 -4.1625943 -4.14469 -4.1063852 -4.1007724 -4.1347971 -4.1623259 -4.1869397 -4.2121811 -4.2318997 -4.2417445 -4.2475238 -4.2500658][-4.239409 -4.2306252 -4.2226596 -4.2187181 -4.2179723 -4.2000017 -4.2002554 -4.2204247 -4.2367206 -4.251152 -4.2615047 -4.269629 -4.2726 -4.2666922 -4.2569995][-4.2497039 -4.2483425 -4.2466393 -4.2521915 -4.2637744 -4.2598395 -4.2622538 -4.27132 -4.280961 -4.2898979 -4.2940903 -4.2930479 -4.2865357 -4.2658229 -4.2416654][-4.2562928 -4.2559066 -4.2580376 -4.2681355 -4.28232 -4.2792439 -4.2772322 -4.2822266 -4.2916436 -4.2967839 -4.2931156 -4.2842669 -4.268929 -4.2338428 -4.1970015][-4.2585111 -4.2535124 -4.2532678 -4.2588768 -4.2657585 -4.2585425 -4.2535033 -4.2574239 -4.2682266 -4.2750554 -4.2693753 -4.2562842 -4.2338276 -4.1890121 -4.1394873][-4.2490206 -4.2400627 -4.2375 -4.2379351 -4.2363009 -4.2240262 -4.2184381 -4.2273393 -4.2460332 -4.2578335 -4.2503557 -4.2287421 -4.19703 -4.145689 -4.0803776]]...]
INFO - root - 2017-12-05 11:02:04.592187: step 5410, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 83h:53m:46s remains)
INFO - root - 2017-12-05 11:02:13.664891: step 5420, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.748 sec/batch; 67h:57m:32s remains)
INFO - root - 2017-12-05 11:02:22.676157: step 5430, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 86h:55m:33s remains)
INFO - root - 2017-12-05 11:02:31.904697: step 5440, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 85h:55m:35s remains)
INFO - root - 2017-12-05 11:02:41.031791: step 5450, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 80h:42m:06s remains)
INFO - root - 2017-12-05 11:02:50.340218: step 5460, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 86h:06m:37s remains)
INFO - root - 2017-12-05 11:02:59.739497: step 5470, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 86h:22m:35s remains)
INFO - root - 2017-12-05 11:03:09.250927: step 5480, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 85h:04m:18s remains)
INFO - root - 2017-12-05 11:03:18.574532: step 5490, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 84h:36m:02s remains)
INFO - root - 2017-12-05 11:03:27.952560: step 5500, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 83h:29m:43s remains)
2017-12-05 11:03:28.688684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2011209 -4.2039781 -4.1989503 -4.1809359 -4.175931 -4.1772184 -4.1756692 -4.1745105 -4.1804714 -4.1981373 -4.2061982 -4.2054753 -4.2136636 -4.22268 -4.2194562][-4.1843305 -4.1893816 -4.1862564 -4.17153 -4.1741652 -4.1794581 -4.1737013 -4.1704025 -4.1727915 -4.19057 -4.1993771 -4.1941438 -4.1986489 -4.2103367 -4.211895][-4.1610436 -4.1685309 -4.1688457 -4.1620808 -4.1661286 -4.1716228 -4.1702685 -4.1731939 -4.1813335 -4.1966071 -4.2037516 -4.1909614 -4.1885667 -4.2010126 -4.2096272][-4.1469646 -4.1530032 -4.1556158 -4.1573191 -4.1585493 -4.1590433 -4.1602731 -4.1680765 -4.1809316 -4.1912217 -4.1953936 -4.1781507 -4.1676641 -4.1802144 -4.1974726][-4.1283035 -4.1324496 -4.1349216 -4.1396532 -4.1395717 -4.1343155 -4.131712 -4.1423235 -4.1597695 -4.1705322 -4.1721859 -4.1528673 -4.1314597 -4.1393452 -4.1573892][-4.1108952 -4.1004076 -4.0908189 -4.0957417 -4.0874419 -4.0639553 -4.0350304 -4.0421352 -4.0823488 -4.1122246 -4.12742 -4.1147604 -4.0907927 -4.0968108 -4.110455][-4.0832834 -4.0597868 -4.0395117 -4.033843 -4.0121031 -3.9530299 -3.8640742 -3.8433118 -3.9321654 -4.0145044 -4.0622873 -4.0650721 -4.0534363 -4.0718493 -4.0922279][-4.0569987 -4.0304427 -4.0041413 -3.9845204 -3.9554353 -3.8759484 -3.730355 -3.6481543 -3.777303 -3.9166834 -3.9905877 -4.006278 -4.0147166 -4.057754 -4.0910654][-4.0549054 -4.0399742 -4.0302386 -4.0158014 -3.997509 -3.9440017 -3.8291781 -3.7366486 -3.8207567 -3.9488173 -4.0128665 -4.0308924 -4.0434165 -4.0880537 -4.1255913][-4.065475 -4.0589023 -4.0667553 -4.0710363 -4.0741482 -4.0591264 -4.0054545 -3.947629 -3.9780569 -4.0570426 -4.0992355 -4.1123323 -4.1217108 -4.1534438 -4.172811][-4.0785685 -4.0795946 -4.0990252 -4.1158743 -4.1340427 -4.1445985 -4.13014 -4.1015496 -4.1040125 -4.1383829 -4.1592183 -4.15915 -4.1643519 -4.1879535 -4.1964755][-4.10446 -4.1152611 -4.1436353 -4.1646628 -4.1848469 -4.1986694 -4.200901 -4.1881456 -4.1769218 -4.1827059 -4.1807079 -4.1564288 -4.1491089 -4.1724854 -4.187871][-4.1283574 -4.1404734 -4.1712465 -4.1872916 -4.2029696 -4.2126384 -4.2168202 -4.2120109 -4.1974583 -4.1860948 -4.1663232 -4.1200809 -4.0993762 -4.1274219 -4.1589127][-4.1453147 -4.1562719 -4.1813869 -4.1934595 -4.2034011 -4.21044 -4.2152667 -4.2145581 -4.2040343 -4.1887884 -4.165978 -4.124249 -4.1075807 -4.1416698 -4.180409][-4.1779957 -4.1803446 -4.1925731 -4.1989121 -4.2061496 -4.2134981 -4.2193713 -4.2212334 -4.2166305 -4.208838 -4.1945219 -4.1698875 -4.1662421 -4.1976666 -4.2309818]]...]
INFO - root - 2017-12-05 11:03:37.968686: step 5510, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 86h:24m:03s remains)
INFO - root - 2017-12-05 11:03:47.221586: step 5520, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 76h:21m:12s remains)
INFO - root - 2017-12-05 11:03:56.477582: step 5530, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 81h:18m:58s remains)
INFO - root - 2017-12-05 11:04:05.695236: step 5540, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 87h:55m:05s remains)
INFO - root - 2017-12-05 11:04:14.911846: step 5550, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 89h:28m:45s remains)
INFO - root - 2017-12-05 11:04:24.237190: step 5560, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 89h:07m:59s remains)
INFO - root - 2017-12-05 11:04:33.639210: step 5570, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 89h:01m:27s remains)
INFO - root - 2017-12-05 11:04:43.110337: step 5580, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 88h:43m:22s remains)
INFO - root - 2017-12-05 11:04:52.388042: step 5590, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 87h:25m:14s remains)
INFO - root - 2017-12-05 11:05:01.608715: step 5600, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 87h:23m:21s remains)
2017-12-05 11:05:02.314824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.220365 -4.2032247 -4.1979365 -4.194777 -4.1970696 -4.2103591 -4.2200804 -4.2128115 -4.1874232 -4.1570945 -4.1310515 -4.1206751 -4.1315322 -4.1394024 -4.1477346][-4.1786418 -4.154892 -4.1470528 -4.1423311 -4.1464462 -4.16541 -4.1844754 -4.1902423 -4.1774507 -4.154573 -4.131166 -4.1222377 -4.1340609 -4.1426334 -4.154521][-4.1546192 -4.1308928 -4.1213341 -4.1203218 -4.127224 -4.1449409 -4.1633239 -4.1724682 -4.1702757 -4.1616759 -4.14609 -4.1339645 -4.1377363 -4.146512 -4.1588249][-4.1527834 -4.1308031 -4.1179581 -4.1156178 -4.1223078 -4.1368084 -4.1528277 -4.1626959 -4.1674404 -4.166997 -4.1545715 -4.1347671 -4.1225076 -4.1267152 -4.1374907][-4.1385918 -4.1099825 -4.0902328 -4.0831709 -4.0910416 -4.1077938 -4.1228843 -4.13365 -4.1403265 -4.1412444 -4.1289716 -4.1060805 -4.0844955 -4.0835185 -4.0932164][-4.11817 -4.0776448 -4.04812 -4.0386038 -4.0497904 -4.0687346 -4.0760951 -4.0759287 -4.0732412 -4.0745158 -4.06724 -4.04927 -4.0277557 -4.0254951 -4.0350718][-4.1024566 -4.054647 -4.0200515 -4.0088768 -4.0121894 -4.0125375 -3.9915061 -3.9677722 -3.9619112 -3.9786322 -3.9917824 -3.9926515 -3.982193 -3.9789939 -3.9867315][-4.1082473 -4.0696354 -4.03965 -4.0186687 -3.9953494 -3.9639063 -3.9124553 -3.8705454 -3.8707304 -3.9087 -3.94993 -3.9750571 -3.9780273 -3.9738574 -3.97937][-4.1396422 -4.1119947 -4.084209 -4.0511179 -4.0090609 -3.9607434 -3.9018097 -3.8648255 -3.8744287 -3.9248543 -3.9772007 -4.0096602 -4.0165734 -4.0115862 -4.0175977][-4.1765852 -4.1535153 -4.1281695 -4.0968165 -4.06337 -4.0288105 -3.9903407 -3.96869 -3.9778948 -4.0152435 -4.0537581 -4.0788164 -4.0881853 -4.0886917 -4.1005177][-4.2084465 -4.1936536 -4.1818814 -4.1684327 -4.1563115 -4.1403689 -4.1191306 -4.105412 -4.1073766 -4.123414 -4.1439877 -4.1607742 -4.1715126 -4.1782236 -4.1909928][-4.2312059 -4.2249122 -4.2243338 -4.225687 -4.228909 -4.2281551 -4.2181859 -4.207552 -4.2050824 -4.20889 -4.2169533 -4.226685 -4.235167 -4.2423525 -4.2504826][-4.2490206 -4.246294 -4.2508731 -4.2595205 -4.2696881 -4.2778549 -4.2752819 -4.2680573 -4.2645516 -4.2641912 -4.2655044 -4.267879 -4.2695231 -4.2720966 -4.2749476][-4.2729807 -4.2695251 -4.2720771 -4.2784834 -4.286972 -4.2950864 -4.2967286 -4.2951627 -4.2947407 -4.2948871 -4.2945609 -4.2922592 -4.2882905 -4.286478 -4.28667][-4.3009381 -4.295516 -4.2932487 -4.2938795 -4.2966609 -4.3015337 -4.3057704 -4.3094206 -4.312367 -4.3138971 -4.3136282 -4.3107214 -4.3065872 -4.304378 -4.3046751]]...]
INFO - root - 2017-12-05 11:05:11.690677: step 5610, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 84h:34m:11s remains)
INFO - root - 2017-12-05 11:05:21.099208: step 5620, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 83h:23m:50s remains)
INFO - root - 2017-12-05 11:05:30.518532: step 5630, loss = 2.10, batch loss = 2.05 (8.6 examples/sec; 0.929 sec/batch; 84h:19m:03s remains)
INFO - root - 2017-12-05 11:05:39.839172: step 5640, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 88h:08m:49s remains)
INFO - root - 2017-12-05 11:05:49.260380: step 5650, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 88h:11m:07s remains)
INFO - root - 2017-12-05 11:05:58.714598: step 5660, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 86h:18m:18s remains)
INFO - root - 2017-12-05 11:06:08.173161: step 5670, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 83h:59m:21s remains)
INFO - root - 2017-12-05 11:06:17.461052: step 5680, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 84h:09m:40s remains)
INFO - root - 2017-12-05 11:06:26.895594: step 5690, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 82h:59m:01s remains)
INFO - root - 2017-12-05 11:06:35.986412: step 5700, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 82h:08m:36s remains)
2017-12-05 11:06:36.691384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24276 -4.258779 -4.2668996 -4.2692122 -4.2727613 -4.2799439 -4.2845607 -4.2801633 -4.2621617 -4.2357607 -4.2211609 -4.2210259 -4.221231 -4.212266 -4.1916485][-4.2696252 -4.2875371 -4.2942 -4.2954984 -4.297864 -4.305397 -4.3120284 -4.3103085 -4.2961955 -4.2706943 -4.247057 -4.2307091 -4.2201476 -4.2046857 -4.1767683][-4.2758923 -4.293592 -4.2979851 -4.2963767 -4.294405 -4.2974854 -4.3034463 -4.3056355 -4.301939 -4.2867618 -4.2635045 -4.2366819 -4.2161679 -4.1926937 -4.1566339][-4.2729506 -4.2855091 -4.2844367 -4.2775335 -4.2668743 -4.2579932 -4.257947 -4.2653632 -4.27619 -4.2775726 -4.2645144 -4.2428756 -4.2227864 -4.1980653 -4.1650577][-4.2673345 -4.2693057 -4.2599359 -4.2419233 -4.2152147 -4.1878886 -4.1763544 -4.1889291 -4.2146559 -4.2336068 -4.23793 -4.2327414 -4.2223172 -4.2094741 -4.1927862][-4.2417502 -4.2365837 -4.2158556 -4.1819453 -4.1330423 -4.0795579 -4.0510035 -4.0712852 -4.1203251 -4.1622052 -4.18413 -4.1934462 -4.19277 -4.1931715 -4.19629][-4.1868558 -4.1765728 -4.1407633 -4.0834341 -4.0018897 -3.9106309 -3.8582048 -3.8878567 -3.9703963 -4.0444236 -4.087184 -4.1121492 -4.1281953 -4.147428 -4.1684108][-4.1353827 -4.1176906 -4.0689192 -3.9917581 -3.8792238 -3.7484362 -3.6603498 -3.6859057 -3.7903502 -3.8884568 -3.949348 -3.9910998 -4.0302391 -4.0742736 -4.1121559][-4.1267715 -4.1159163 -4.0778012 -4.0129895 -3.9068179 -3.7769642 -3.6788497 -3.6779737 -3.7540491 -3.8286109 -3.8736718 -3.9103212 -3.9552705 -4.0108633 -4.0571032][-4.1483006 -4.1586766 -4.1474528 -4.1149611 -4.0506525 -3.9656665 -3.8961711 -3.8780963 -3.9023201 -3.9260201 -3.9334424 -3.9439492 -3.9729819 -4.01793 -4.0568981][-4.1699433 -4.1893487 -4.1899467 -4.1767273 -4.1486158 -4.1106925 -4.0795741 -4.0698795 -4.0744143 -4.0714779 -4.057343 -4.0490623 -4.0615749 -4.085928 -4.1041017][-4.2008262 -4.2144608 -4.2099886 -4.2011542 -4.1927838 -4.1814108 -4.1730928 -4.1794839 -4.1903739 -4.1889348 -4.1762938 -4.1679335 -4.1732707 -4.1796894 -4.1788654][-4.2333741 -4.23158 -4.21721 -4.206913 -4.2063227 -4.204658 -4.2030635 -4.2131958 -4.2285089 -4.2330317 -4.229054 -4.2320075 -4.2419333 -4.2439938 -4.2367673][-4.2591543 -4.2452326 -4.2242103 -4.2108173 -4.208818 -4.2078667 -4.2067518 -4.2141523 -4.2273979 -4.2303414 -4.2273788 -4.237009 -4.2545824 -4.2621331 -4.2580814][-4.2823596 -4.2637939 -4.2365103 -4.2154212 -4.2068806 -4.1979036 -4.189445 -4.1919923 -4.198278 -4.1928844 -4.1852012 -4.1990719 -4.2250657 -4.2395477 -4.2410417]]...]
INFO - root - 2017-12-05 11:06:45.998962: step 5710, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 82h:53m:26s remains)
INFO - root - 2017-12-05 11:06:55.432918: step 5720, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 83h:39m:59s remains)
INFO - root - 2017-12-05 11:07:04.914774: step 5730, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 85h:59m:50s remains)
INFO - root - 2017-12-05 11:07:14.267862: step 5740, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 76h:26m:27s remains)
INFO - root - 2017-12-05 11:07:23.654092: step 5750, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 82h:39m:44s remains)
INFO - root - 2017-12-05 11:07:32.982010: step 5760, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.946 sec/batch; 85h:49m:31s remains)
INFO - root - 2017-12-05 11:07:42.323712: step 5770, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 81h:35m:35s remains)
INFO - root - 2017-12-05 11:07:51.790219: step 5780, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 85h:15m:41s remains)
INFO - root - 2017-12-05 11:08:01.019695: step 5790, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 78h:23m:56s remains)
INFO - root - 2017-12-05 11:08:10.151458: step 5800, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 85h:27m:02s remains)
2017-12-05 11:08:10.892805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2189751 -4.2186995 -4.2386909 -4.2567463 -4.2642779 -4.2607269 -4.2465296 -4.2415929 -4.2502675 -4.2645597 -4.2821403 -4.3000412 -4.3121805 -4.3139863 -4.3047872][-4.2168069 -4.2190576 -4.2408981 -4.262198 -4.2733135 -4.2663307 -4.2461839 -4.2384658 -4.2479939 -4.2668624 -4.2879982 -4.3066177 -4.3170757 -4.3162031 -4.3024106][-4.2164736 -4.2213945 -4.2470083 -4.2725382 -4.2865405 -4.2810011 -4.2634645 -4.2556124 -4.2627792 -4.2754602 -4.2897425 -4.3035836 -4.3129988 -4.3153749 -4.3055897][-4.2147551 -4.2189541 -4.2481585 -4.2771044 -4.2906246 -4.2888255 -4.2771187 -4.2719812 -4.2807126 -4.2894106 -4.294168 -4.298996 -4.303977 -4.3073654 -4.3012843][-4.2293148 -4.2331128 -4.2552705 -4.2761459 -4.2850389 -4.2783642 -4.2638159 -4.2625489 -4.2807508 -4.29488 -4.2974553 -4.2969952 -4.2959752 -4.2958188 -4.290936][-4.2537532 -4.2544584 -4.2613373 -4.2667723 -4.2617416 -4.2390752 -4.2070656 -4.2019806 -4.2336507 -4.2664733 -4.2840757 -4.2890959 -4.2841034 -4.2780137 -4.2714443][-4.2643294 -4.2627673 -4.2578855 -4.2467585 -4.21809 -4.1670771 -4.1044693 -4.0814266 -4.1262555 -4.1919889 -4.2368155 -4.2545004 -4.24876 -4.2386632 -4.2340994][-4.2614403 -4.262423 -4.2542958 -4.229816 -4.1750073 -4.0910683 -3.9931464 -3.9441524 -3.997508 -4.098453 -4.1750641 -4.2074933 -4.205893 -4.1968822 -4.1957808][-4.2667112 -4.2722921 -4.2658939 -4.2379985 -4.1756315 -4.0819774 -3.972295 -3.9011495 -3.9400668 -4.0474734 -4.135262 -4.1722188 -4.1741638 -4.1691322 -4.1713409][-4.2780085 -4.2900581 -4.289248 -4.2688589 -4.2207608 -4.1526432 -4.0804167 -4.0322504 -4.0443673 -4.1018724 -4.1544228 -4.1738586 -4.168076 -4.1603241 -4.162014][-4.2742581 -4.291256 -4.2994828 -4.2925911 -4.2675467 -4.2325315 -4.1996579 -4.1767073 -4.1729293 -4.1900043 -4.2099872 -4.2126932 -4.1908946 -4.1665053 -4.1559463][-4.265727 -4.2837129 -4.2963467 -4.2980938 -4.2864223 -4.2710633 -4.2632532 -4.2584014 -4.2522445 -4.2515435 -4.2558985 -4.2535758 -4.228447 -4.1964769 -4.1759877][-4.2610593 -4.2797275 -4.2922006 -4.2938929 -4.2837515 -4.2748008 -4.2766476 -4.2776446 -4.27558 -4.2753563 -4.2778873 -4.2753482 -4.2550025 -4.2303309 -4.2143087][-4.2580957 -4.2782626 -4.289886 -4.2876039 -4.2778225 -4.2689972 -4.2669721 -4.2650528 -4.266798 -4.2714586 -4.281549 -4.2858472 -4.2760868 -4.2642212 -4.2569623][-4.2581882 -4.2776613 -4.2841172 -4.2800097 -4.2718573 -4.26257 -4.258276 -4.2550139 -4.2540135 -4.2567091 -4.2708449 -4.2823992 -4.2843013 -4.2844415 -4.2860904]]...]
INFO - root - 2017-12-05 11:08:20.162492: step 5810, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 86h:45m:20s remains)
INFO - root - 2017-12-05 11:08:29.574787: step 5820, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 89h:10m:08s remains)
INFO - root - 2017-12-05 11:08:38.932661: step 5830, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 81h:37m:24s remains)
INFO - root - 2017-12-05 11:08:48.421864: step 5840, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 87h:15m:42s remains)
INFO - root - 2017-12-05 11:08:57.848160: step 5850, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 82h:59m:51s remains)
INFO - root - 2017-12-05 11:09:07.365778: step 5860, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 86h:11m:43s remains)
INFO - root - 2017-12-05 11:09:17.010404: step 5870, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 86h:07m:02s remains)
INFO - root - 2017-12-05 11:09:26.203671: step 5880, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 82h:25m:08s remains)
INFO - root - 2017-12-05 11:09:35.569105: step 5890, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 84h:49m:03s remains)
INFO - root - 2017-12-05 11:09:44.942271: step 5900, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 83h:28m:13s remains)
2017-12-05 11:09:45.672049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.19162 -4.1905422 -4.200489 -4.215044 -4.2212381 -4.2178917 -4.216002 -4.2181745 -4.2222753 -4.229434 -4.2404017 -4.2506108 -4.2559805 -4.2615457 -4.2656946][-4.1747108 -4.1699448 -4.1789956 -4.1913896 -4.1954207 -4.1897163 -4.1876974 -4.1954932 -4.2065072 -4.2200418 -4.234798 -4.2465057 -4.2499647 -4.251627 -4.2517104][-4.1658154 -4.1602459 -4.1696 -4.1802568 -4.180182 -4.1705108 -4.167944 -4.18009 -4.197577 -4.2179394 -4.2366505 -4.2487874 -4.250001 -4.2463512 -4.2394619][-4.156002 -4.1554418 -4.1726141 -4.18682 -4.1846938 -4.1700377 -4.1636782 -4.1765981 -4.1966643 -4.2215767 -4.2442417 -4.2586026 -4.2595873 -4.2513485 -4.2383471][-4.1426682 -4.1474757 -4.1769524 -4.1991773 -4.1974044 -4.1769271 -4.1653237 -4.1772556 -4.1988626 -4.2260456 -4.2504978 -4.2670794 -4.2686415 -4.2571325 -4.2395697][-4.1223097 -4.1325917 -4.1729317 -4.2008829 -4.1980729 -4.172852 -4.15884 -4.1705341 -4.1932111 -4.2207909 -4.2436843 -4.2601013 -4.2636123 -4.2507777 -4.2293658][-4.1032357 -4.1210361 -4.1705055 -4.2017283 -4.1980619 -4.1710963 -4.1586666 -4.1708622 -4.19292 -4.2183771 -4.2393861 -4.2560258 -4.26126 -4.2482157 -4.2251086][-4.0961018 -4.1232858 -4.1775756 -4.2067623 -4.197711 -4.16763 -4.1555281 -4.1655116 -4.1872511 -4.2136664 -4.2360096 -4.2547417 -4.2641549 -4.2542415 -4.2325764][-4.0949759 -4.1291633 -4.1854606 -4.2106104 -4.195025 -4.1612039 -4.1494031 -4.1597223 -4.1845546 -4.2131357 -4.2349515 -4.2546477 -4.2664413 -4.2577362 -4.23826][-4.1115332 -4.1466708 -4.1995435 -4.2199779 -4.2014408 -4.167942 -4.16119 -4.1751027 -4.2007093 -4.2251906 -4.2416363 -4.2576942 -4.2678967 -4.2585993 -4.2409534][-4.1444283 -4.174509 -4.2175455 -4.2315969 -4.210556 -4.1795397 -4.1783423 -4.1970682 -4.2230453 -4.2426491 -4.2548113 -4.26695 -4.2750235 -4.2653732 -4.2486439][-4.1792006 -4.20266 -4.2342792 -4.2420955 -4.2211695 -4.1956859 -4.1982293 -4.21765 -4.2418013 -4.2572775 -4.2668281 -4.2768774 -4.28355 -4.2749243 -4.2602563][-4.2112384 -4.2271614 -4.2499452 -4.2561116 -4.2405143 -4.2234216 -4.2271762 -4.2432928 -4.2618375 -4.2723551 -4.2794447 -4.2874112 -4.2935162 -4.28737 -4.2767987][-4.2478032 -4.2559595 -4.271441 -4.2780581 -4.2705321 -4.2612095 -4.2647738 -4.2759256 -4.2874079 -4.2934165 -4.297996 -4.3033113 -4.3070912 -4.3035951 -4.2983971][-4.2842832 -4.2870712 -4.2971373 -4.3039045 -4.3025923 -4.29908 -4.3016167 -4.3075423 -4.3135886 -4.3163991 -4.31889 -4.32147 -4.3228083 -4.32038 -4.3180747]]...]
INFO - root - 2017-12-05 11:09:54.915156: step 5910, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 85h:17m:34s remains)
INFO - root - 2017-12-05 11:10:04.652023: step 5920, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 85h:44m:17s remains)
INFO - root - 2017-12-05 11:10:14.067132: step 5930, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 79h:07m:44s remains)
INFO - root - 2017-12-05 11:10:23.409777: step 5940, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 90h:00m:34s remains)
INFO - root - 2017-12-05 11:10:32.913302: step 5950, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 86h:56m:05s remains)
INFO - root - 2017-12-05 11:10:42.440150: step 5960, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 89h:26m:52s remains)
INFO - root - 2017-12-05 11:10:51.603776: step 5970, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 83h:08m:06s remains)
INFO - root - 2017-12-05 11:11:00.845495: step 5980, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 82h:59m:18s remains)
INFO - root - 2017-12-05 11:11:10.489424: step 5990, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 85h:37m:53s remains)
INFO - root - 2017-12-05 11:11:19.812238: step 6000, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.936 sec/batch; 84h:50m:49s remains)
2017-12-05 11:11:20.580332: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3523216 -4.3515062 -4.35194 -4.3526187 -4.3521113 -4.3457947 -4.3325367 -4.3176775 -4.3079271 -4.3079767 -4.3174024 -4.3320312 -4.3438931 -4.3506379 -4.3518143][-4.3569546 -4.3572168 -4.3581939 -4.3578534 -4.3531084 -4.3368173 -4.3096824 -4.2828336 -4.2670836 -4.2676754 -4.2833142 -4.3114123 -4.3358107 -4.3503528 -4.3539596][-4.3646393 -4.3668447 -4.3676176 -4.361372 -4.3450623 -4.3118548 -4.2653146 -4.2221613 -4.2014947 -4.2080703 -4.2363634 -4.2818666 -4.320467 -4.3430905 -4.3507123][-4.3695936 -4.3733225 -4.3712559 -4.3547974 -4.32403 -4.2713003 -4.202775 -4.1430354 -4.1216636 -4.1424785 -4.1908174 -4.2552209 -4.3056192 -4.3346391 -4.3455229][-4.3710189 -4.3741012 -4.3668284 -4.3390694 -4.2929473 -4.2196774 -4.1302586 -4.0603056 -4.0497575 -4.0963411 -4.1680059 -4.245564 -4.302485 -4.3330731 -4.3429255][-4.3732395 -4.3731642 -4.3572731 -4.314146 -4.2477765 -4.152441 -4.04498 -3.9729092 -3.9896832 -4.0697451 -4.1637645 -4.2503219 -4.308394 -4.3366165 -4.3443074][-4.3680506 -4.3644981 -4.3393745 -4.277669 -4.1914024 -4.0760212 -3.9491594 -3.8843036 -3.9381757 -4.0511575 -4.1646557 -4.2564144 -4.3142233 -4.3415904 -4.3480668][-4.3482671 -4.3436875 -4.3116536 -4.2374487 -4.1382227 -4.0080066 -3.8686786 -3.8261318 -3.9180853 -4.049593 -4.1702309 -4.2621765 -4.3201818 -4.3475037 -4.351716][-4.3156137 -4.3137836 -4.2787957 -4.1986914 -4.0941415 -3.9624655 -3.834465 -3.8272865 -3.9489925 -4.0843511 -4.196147 -4.2768 -4.3285356 -4.3511667 -4.3517556][-4.2844267 -4.2908483 -4.2603703 -4.1832089 -4.0845904 -3.9722123 -3.8812666 -3.9022379 -4.0247307 -4.1444297 -4.2366447 -4.300993 -4.3405743 -4.3538375 -4.3494854][-4.2674351 -4.2884603 -4.268919 -4.2026811 -4.1212196 -4.0433583 -3.9914339 -4.0213871 -4.1182294 -4.2100821 -4.2816687 -4.32972 -4.3536477 -4.3569527 -4.348166][-4.2661128 -4.2993751 -4.2935433 -4.2472792 -4.18953 -4.1409326 -4.1137047 -4.1398916 -4.206387 -4.271255 -4.3231649 -4.3564677 -4.3675094 -4.3627133 -4.3503652][-4.270144 -4.3122544 -4.3200145 -4.2982411 -4.2628956 -4.2326074 -4.2144451 -4.2282615 -4.2670302 -4.3103628 -4.3482308 -4.3719769 -4.3775425 -4.3695593 -4.3553624][-4.2731671 -4.3232193 -4.3436337 -4.3365393 -4.3097529 -4.28529 -4.2667737 -4.2691402 -4.2896824 -4.3179312 -4.349194 -4.3705783 -4.3772531 -4.3705244 -4.3578172][-4.2851467 -4.3399324 -4.3639069 -4.3550944 -4.3235388 -4.2919564 -4.2653923 -4.2578011 -4.2697172 -4.2925005 -4.3262415 -4.3532348 -4.3658118 -4.3638844 -4.3559227]]...]
INFO - root - 2017-12-05 11:11:30.101868: step 6010, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 85h:30m:58s remains)
INFO - root - 2017-12-05 11:11:39.523682: step 6020, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 85h:13m:53s remains)
INFO - root - 2017-12-05 11:11:48.950109: step 6030, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 83h:09m:33s remains)
INFO - root - 2017-12-05 11:11:58.440964: step 6040, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.959 sec/batch; 87h:00m:08s remains)
INFO - root - 2017-12-05 11:12:07.768843: step 6050, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 87h:11m:32s remains)
INFO - root - 2017-12-05 11:12:17.189299: step 6060, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 82h:19m:43s remains)
INFO - root - 2017-12-05 11:12:26.427134: step 6070, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 84h:36m:16s remains)
INFO - root - 2017-12-05 11:12:35.794080: step 6080, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 86h:03m:35s remains)
INFO - root - 2017-12-05 11:12:45.206815: step 6090, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 84h:44m:25s remains)
INFO - root - 2017-12-05 11:12:54.653302: step 6100, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 81h:20m:16s remains)
2017-12-05 11:12:55.503621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3198256 -4.3258481 -4.3318348 -4.3326197 -4.324327 -4.3118172 -4.3032331 -4.301024 -4.3013468 -4.3089857 -4.3207517 -4.3288655 -4.3278313 -4.3201566 -4.3127995][-4.3092284 -4.3120694 -4.3165336 -4.3171725 -4.306488 -4.2876778 -4.2753444 -4.2750387 -4.2762742 -4.2838902 -4.2991953 -4.3149056 -4.3154168 -4.3042893 -4.2988591][-4.3040361 -4.3016839 -4.3032012 -4.303442 -4.288785 -4.2633429 -4.2462144 -4.2486329 -4.2527862 -4.2596812 -4.2781019 -4.3015418 -4.3049755 -4.2902241 -4.284646][-4.2973495 -4.2884622 -4.284843 -4.2839785 -4.2679343 -4.2376528 -4.2136917 -4.2164474 -4.22397 -4.2337246 -4.25208 -4.27987 -4.2844296 -4.2671647 -4.2615161][-4.286634 -4.2715096 -4.2643404 -4.2657137 -4.254683 -4.2241259 -4.193151 -4.1916976 -4.2038937 -4.218174 -4.2293625 -4.2511053 -4.2500424 -4.2280178 -4.2222013][-4.2823882 -4.2643337 -4.2541056 -4.2537441 -4.24356 -4.2138166 -4.1798186 -4.1786332 -4.1974411 -4.2189016 -4.2280436 -4.2388067 -4.2271342 -4.1965971 -4.1886663][-4.2801685 -4.2600436 -4.2462363 -4.2393126 -4.2249966 -4.1944642 -4.16256 -4.1609774 -4.1808763 -4.2073183 -4.2256942 -4.2373905 -4.2223439 -4.19004 -4.1787891][-4.2797427 -4.257513 -4.2400694 -4.2269435 -4.2070069 -4.1749797 -4.1440854 -4.1388273 -4.1542645 -4.1882076 -4.2220559 -4.2398429 -4.2277303 -4.2032 -4.19417][-4.2826624 -4.2639894 -4.2464786 -4.2299933 -4.2053924 -4.1723003 -4.141984 -4.1304479 -4.1389995 -4.17501 -4.2159557 -4.2366829 -4.23076 -4.2205086 -4.2189469][-4.2875838 -4.2748823 -4.2604418 -4.2435651 -4.2172279 -4.1816936 -4.1484318 -4.1333227 -4.1400838 -4.17113 -4.2067504 -4.2250156 -4.2256021 -4.2242513 -4.2258348][-4.2866983 -4.2767763 -4.2632041 -4.2463307 -4.2216611 -4.1858721 -4.1501207 -4.1353588 -4.1444607 -4.1730094 -4.203022 -4.2210822 -4.2265034 -4.2288961 -4.2293129][-4.2838054 -4.2727203 -4.2587605 -4.2432976 -4.2248564 -4.1960554 -4.1669397 -4.1563654 -4.1683788 -4.1944351 -4.2170982 -4.2305365 -4.2362647 -4.2387877 -4.2393379][-4.2832875 -4.2729206 -4.2628136 -4.2532735 -4.2467351 -4.2317314 -4.2137647 -4.2071419 -4.2167044 -4.2340217 -4.2467937 -4.2529716 -4.2537584 -4.2540607 -4.2554679][-4.2843428 -4.2763233 -4.271997 -4.2694192 -4.2714052 -4.267818 -4.2603602 -4.2576523 -4.2627912 -4.2706141 -4.2756934 -4.2763858 -4.2751579 -4.2748666 -4.2766271][-4.2862382 -4.2788548 -4.2764964 -4.2765937 -4.2813406 -4.2840867 -4.2843084 -4.2852654 -4.2880092 -4.2903128 -4.2906985 -4.2893033 -4.2878561 -4.2877631 -4.2888083]]...]
INFO - root - 2017-12-05 11:13:04.735758: step 6110, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 77h:53m:27s remains)
INFO - root - 2017-12-05 11:13:14.154976: step 6120, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 87h:30m:06s remains)
INFO - root - 2017-12-05 11:13:23.520664: step 6130, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 83h:25m:16s remains)
INFO - root - 2017-12-05 11:13:32.928166: step 6140, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 85h:41m:12s remains)
INFO - root - 2017-12-05 11:13:42.310196: step 6150, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 82h:23m:21s remains)
INFO - root - 2017-12-05 11:13:51.615367: step 6160, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 82h:40m:33s remains)
INFO - root - 2017-12-05 11:14:00.997686: step 6170, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 88h:46m:09s remains)
INFO - root - 2017-12-05 11:14:10.186406: step 6180, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 87h:46m:01s remains)
INFO - root - 2017-12-05 11:14:19.469935: step 6190, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 89h:49m:34s remains)
INFO - root - 2017-12-05 11:14:28.734090: step 6200, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 87h:17m:30s remains)
2017-12-05 11:14:29.474683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20391 -4.1933556 -4.2042832 -4.2228723 -4.2254357 -4.205256 -4.1644716 -4.1169863 -4.0760121 -4.0702448 -4.1067104 -4.15268 -4.1938224 -4.2317805 -4.2805691][-4.2095909 -4.2073841 -4.2257872 -4.2444129 -4.2426472 -4.212985 -4.1633911 -4.1151156 -4.080061 -4.0737023 -4.1072683 -4.1596527 -4.2113118 -4.2604265 -4.3106914][-4.2273326 -4.2366571 -4.25653 -4.2712531 -4.2609148 -4.21976 -4.1602697 -4.1073995 -4.0764165 -4.0708418 -4.1060276 -4.1671686 -4.2286654 -4.2821364 -4.3255019][-4.250834 -4.2682638 -4.28765 -4.2926178 -4.2667823 -4.2121196 -4.1404247 -4.0785017 -4.05414 -4.0622144 -4.110682 -4.1821532 -4.252346 -4.3041191 -4.3317356][-4.2731934 -4.2941532 -4.3092756 -4.3014 -4.2580805 -4.1871309 -4.0982947 -4.026897 -4.0186467 -4.0555434 -4.1252794 -4.2066493 -4.2798476 -4.3218775 -4.3265185][-4.291285 -4.309701 -4.3162565 -4.2942452 -4.23305 -4.1381373 -4.0307837 -3.9640167 -3.9904947 -4.0653276 -4.1544113 -4.23883 -4.3027396 -4.3265328 -4.3074737][-4.2848215 -4.2971487 -4.2950554 -4.2596564 -4.178535 -4.0618629 -3.9479704 -3.9094658 -3.9812779 -4.0851045 -4.183207 -4.2634816 -4.3126521 -4.3173518 -4.27847][-4.2562809 -4.2651482 -4.2587652 -4.2124815 -4.1186976 -3.997982 -3.9033513 -3.9117794 -4.0118675 -4.1176672 -4.2079554 -4.2768993 -4.3110075 -4.2991028 -4.2448835][-4.2232323 -4.2348218 -4.2270317 -4.1751661 -4.0781741 -3.971473 -3.9192562 -3.9707079 -4.0683465 -4.1578441 -4.235064 -4.2891035 -4.3065081 -4.2799115 -4.2169061][-4.1907225 -4.2076979 -4.2004418 -4.1496987 -4.05922 -3.9784198 -3.9662943 -4.0345192 -4.1175957 -4.1907043 -4.2577133 -4.29844 -4.2980103 -4.2613478 -4.1950378][-4.1646223 -4.182056 -4.1706786 -4.1244321 -4.0530205 -4.0054755 -4.02223 -4.0922532 -4.1600604 -4.2157826 -4.2706833 -4.3000693 -4.2865934 -4.2403684 -4.1724496][-4.1537943 -4.1647277 -4.1474509 -4.1099834 -4.0657625 -4.0451865 -4.0751081 -4.1393137 -4.1943564 -4.2384229 -4.2822156 -4.2999949 -4.2744236 -4.2182469 -4.1459479][-4.1605196 -4.1627359 -4.1421285 -4.116745 -4.0958161 -4.0899124 -4.1189928 -4.1725621 -4.2208414 -4.2602696 -4.2922192 -4.2956591 -4.2602015 -4.1943722 -4.117826][-4.1775041 -4.1740732 -4.1532555 -4.1387653 -4.134985 -4.1365023 -4.1596456 -4.2000885 -4.2378316 -4.2678704 -4.2857938 -4.2797413 -4.23881 -4.1711841 -4.0968609][-4.202487 -4.2038722 -4.1872139 -4.1761618 -4.1746526 -4.1755209 -4.1903334 -4.2155771 -4.2397118 -4.2581172 -4.2649908 -4.2518463 -4.211113 -4.1484365 -4.0886078]]...]
INFO - root - 2017-12-05 11:14:38.730631: step 6210, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 81h:21m:39s remains)
INFO - root - 2017-12-05 11:14:48.064972: step 6220, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 83h:55m:18s remains)
INFO - root - 2017-12-05 11:14:57.444404: step 6230, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 82h:34m:04s remains)
INFO - root - 2017-12-05 11:15:06.872712: step 6240, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 85h:11m:54s remains)
INFO - root - 2017-12-05 11:15:16.075733: step 6250, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 83h:50m:22s remains)
INFO - root - 2017-12-05 11:15:25.456670: step 6260, loss = 2.11, batch loss = 2.05 (8.9 examples/sec; 0.899 sec/batch; 81h:27m:32s remains)
INFO - root - 2017-12-05 11:15:34.913170: step 6270, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 84h:24m:30s remains)
INFO - root - 2017-12-05 11:15:44.399093: step 6280, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 81h:08m:42s remains)
INFO - root - 2017-12-05 11:15:53.764923: step 6290, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 78h:54m:40s remains)
INFO - root - 2017-12-05 11:16:03.232817: step 6300, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 85h:11m:41s remains)
2017-12-05 11:16:03.966629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1787291 -4.1959105 -4.212781 -4.2227163 -4.2271862 -4.2366734 -4.2487307 -4.2486424 -4.2366467 -4.2208104 -4.216567 -4.2206907 -4.2208095 -4.2135839 -4.2057724][-4.2237935 -4.2479129 -4.2662435 -4.2730646 -4.2703762 -4.268374 -4.2653909 -4.2512884 -4.2342019 -4.2228074 -4.2209368 -4.2202697 -4.2145462 -4.2099323 -4.2102833][-4.2647324 -4.29338 -4.3136969 -4.3164144 -4.3041019 -4.2835417 -4.2550435 -4.2215023 -4.2027984 -4.204 -4.2110653 -4.2070179 -4.1965747 -4.1940403 -4.1992536][-4.2643404 -4.2933788 -4.3103948 -4.3081355 -4.2870636 -4.2439446 -4.1832938 -4.1289444 -4.1190176 -4.1460886 -4.1695232 -4.1655178 -4.154386 -4.1578636 -4.168273][-4.2310019 -4.2526193 -4.2599206 -4.2514591 -4.2199755 -4.1494827 -4.0491576 -3.9739344 -3.9883637 -4.0523977 -4.0992541 -4.10437 -4.1026726 -4.1187549 -4.138545][-4.1870856 -4.1967821 -4.1961803 -4.1843963 -4.1450315 -4.0448079 -3.8973675 -3.7997425 -3.8567383 -3.9695761 -4.0352902 -4.0516205 -4.0632248 -4.0912871 -4.1204629][-4.1640258 -4.161499 -4.1497087 -4.1302242 -4.0837784 -3.9670248 -3.7943215 -3.6904984 -3.7939148 -3.9439454 -4.0160661 -4.0326 -4.0475526 -4.0784421 -4.1096745][-4.1651864 -4.1534977 -4.1374145 -4.1187263 -4.0759153 -3.9744768 -3.8351984 -3.7650411 -3.86162 -3.9903555 -4.0438 -4.0495734 -4.0592036 -4.0861645 -4.1150823][-4.1868849 -4.1751904 -4.1641526 -4.154264 -4.1235876 -4.0517187 -3.964175 -3.9270749 -3.9852982 -4.0637407 -4.0912242 -4.0903606 -4.0957804 -4.1173782 -4.1422791][-4.2173162 -4.2108064 -4.2065282 -4.2025676 -4.1838574 -4.1377811 -4.0869408 -4.068891 -4.094974 -4.1305528 -4.1421342 -4.1422243 -4.147697 -4.1632223 -4.1812][-4.2505293 -4.2489471 -4.2489147 -4.2462072 -4.23515 -4.2066064 -4.1793933 -4.1707578 -4.17682 -4.1863475 -4.1889315 -4.1917715 -4.1984291 -4.2078195 -4.2164693][-4.2784753 -4.2818179 -4.2839766 -4.2827039 -4.2768674 -4.2621727 -4.2494035 -4.2439671 -4.2397656 -4.2357292 -4.2312546 -4.2329321 -4.2369585 -4.2358088 -4.2322445][-4.2947564 -4.30117 -4.3038549 -4.3031139 -4.2998142 -4.2934842 -4.2877769 -4.2828135 -4.273385 -4.2647839 -4.25813 -4.2568045 -4.25248 -4.2390971 -4.2236061][-4.3107386 -4.3191071 -4.3212647 -4.3185439 -4.3142309 -4.3110981 -4.3083949 -4.3027153 -4.2907729 -4.2803698 -4.2732763 -4.2657919 -4.2491636 -4.2248816 -4.199275][-4.3221941 -4.3301511 -4.3315816 -4.328114 -4.3231611 -4.3216481 -4.320312 -4.3152647 -4.3060207 -4.2978516 -4.2912378 -4.2777524 -4.2488694 -4.2146797 -4.1797829]]...]
INFO - root - 2017-12-05 11:16:13.471481: step 6310, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 78h:33m:42s remains)
INFO - root - 2017-12-05 11:16:22.828254: step 6320, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 84h:20m:21s remains)
INFO - root - 2017-12-05 11:16:32.484255: step 6330, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.971 sec/batch; 87h:58m:48s remains)
INFO - root - 2017-12-05 11:16:41.776701: step 6340, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 85h:49m:32s remains)
INFO - root - 2017-12-05 11:16:51.276834: step 6350, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 85h:30m:02s remains)
INFO - root - 2017-12-05 11:17:00.642296: step 6360, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 85h:02m:31s remains)
INFO - root - 2017-12-05 11:17:09.960520: step 6370, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 86h:38m:14s remains)
INFO - root - 2017-12-05 11:17:19.429475: step 6380, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 84h:41m:01s remains)
INFO - root - 2017-12-05 11:17:28.721642: step 6390, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 87h:42m:14s remains)
INFO - root - 2017-12-05 11:17:38.012551: step 6400, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 85h:36m:49s remains)
2017-12-05 11:17:38.812797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1938143 -4.1975484 -4.2089953 -4.2138195 -4.2087741 -4.1980038 -4.1887727 -4.1808386 -4.1760578 -4.1792808 -4.1856718 -4.19104 -4.1899047 -4.1773415 -4.1607943][-4.2113056 -4.2135692 -4.2199893 -4.217248 -4.2056723 -4.1915631 -4.1827288 -4.1798315 -4.1812115 -4.1872482 -4.1906118 -4.1921721 -4.19556 -4.1891656 -4.1783013][-4.2209029 -4.21678 -4.2160192 -4.2083979 -4.1937819 -4.1769762 -4.1679425 -4.17046 -4.1787667 -4.190021 -4.194931 -4.19796 -4.2043986 -4.2010517 -4.1913276][-4.2147059 -4.207315 -4.2060404 -4.1995349 -4.1841855 -4.1642971 -4.1523771 -4.1554742 -4.1680903 -4.1833076 -4.1932578 -4.1972728 -4.2003756 -4.1946692 -4.1832352][-4.2065525 -4.1986928 -4.1971645 -4.192306 -4.177609 -4.1535654 -4.1343145 -4.1319718 -4.1463 -4.1692224 -4.1843786 -4.1844745 -4.175909 -4.164969 -4.1533775][-4.1997628 -4.19237 -4.1885171 -4.1854258 -4.1708794 -4.14082 -4.1122174 -4.1002288 -4.115344 -4.146101 -4.16526 -4.161819 -4.1445332 -4.1302485 -4.1208959][-4.1784096 -4.1742091 -4.1738534 -4.1748238 -4.1617732 -4.1290331 -4.0902185 -4.0662165 -4.082 -4.1192756 -4.142662 -4.1405053 -4.1253848 -4.115952 -4.1151175][-4.1531315 -4.1537404 -4.15699 -4.1594076 -4.1494966 -4.1176791 -4.0718961 -4.0382843 -4.0523367 -4.0910749 -4.119812 -4.1312251 -4.1349874 -4.141037 -4.1528258][-4.1370378 -4.1385469 -4.1399236 -4.1404552 -4.1354108 -4.108984 -4.063067 -4.0266438 -4.0379229 -4.0763359 -4.10798 -4.1295233 -4.1537347 -4.1798778 -4.202538][-4.1454182 -4.1445684 -4.1397448 -4.1361418 -4.1363955 -4.120822 -4.0855246 -4.053658 -4.0574765 -4.0850334 -4.1107521 -4.1352577 -4.1690187 -4.2070646 -4.2335191][-4.1698756 -4.163229 -4.1474648 -4.1383061 -4.1447692 -4.1473341 -4.1303039 -4.1067576 -4.0997853 -4.1125889 -4.1312814 -4.15523 -4.1885982 -4.2225308 -4.241888][-4.1917706 -4.1837997 -4.16183 -4.1489763 -4.1546354 -4.1692214 -4.170558 -4.1590943 -4.1498623 -4.1503091 -4.1584263 -4.1752214 -4.1945772 -4.2106242 -4.2172766][-4.2135048 -4.2073364 -4.1875091 -4.17387 -4.1747952 -4.1879821 -4.1984892 -4.1981664 -4.1938272 -4.1925411 -4.1935883 -4.1966758 -4.195159 -4.19182 -4.1890635][-4.2373052 -4.2303362 -4.2144127 -4.2022481 -4.198298 -4.20443 -4.2162962 -4.2218285 -4.2220297 -4.2231569 -4.220387 -4.2109723 -4.196331 -4.1833081 -4.1774721][-4.2621078 -4.2542725 -4.23975 -4.22749 -4.2194767 -4.2195716 -4.2290106 -4.2365761 -4.2412977 -4.2452884 -4.2417769 -4.2261181 -4.2086058 -4.1984863 -4.1965747]]...]
INFO - root - 2017-12-05 11:17:48.362240: step 6410, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 84h:48m:14s remains)
INFO - root - 2017-12-05 11:17:57.703801: step 6420, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 86h:36m:41s remains)
INFO - root - 2017-12-05 11:18:06.911938: step 6430, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 87h:37m:48s remains)
INFO - root - 2017-12-05 11:18:16.248333: step 6440, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 86h:48m:13s remains)
INFO - root - 2017-12-05 11:18:25.861506: step 6450, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.978 sec/batch; 88h:33m:14s remains)
INFO - root - 2017-12-05 11:18:35.322711: step 6460, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.996 sec/batch; 90h:10m:25s remains)
INFO - root - 2017-12-05 11:18:44.455588: step 6470, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 85h:26m:52s remains)
INFO - root - 2017-12-05 11:18:53.785325: step 6480, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 83h:42m:22s remains)
INFO - root - 2017-12-05 11:19:03.189138: step 6490, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 81h:47m:32s remains)
INFO - root - 2017-12-05 11:19:12.596527: step 6500, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 84h:34m:03s remains)
2017-12-05 11:19:13.316838: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2786036 -4.2257066 -4.16904 -4.1346831 -4.1066122 -4.1033778 -4.1333079 -4.1730037 -4.201849 -4.2182031 -4.2222853 -4.2074008 -4.1849494 -4.1686764 -4.1568327][-4.2601357 -4.1927371 -4.1250648 -4.0844736 -4.0468349 -4.0476747 -4.0927873 -4.141573 -4.1794553 -4.2001343 -4.1990013 -4.1744666 -4.1403866 -4.1146874 -4.0958118][-4.2401361 -4.1575661 -4.0795422 -4.0310955 -3.9857433 -3.9954252 -4.0569448 -4.1105781 -4.1519127 -4.1737013 -4.1695843 -4.1411519 -4.1002345 -4.0635591 -4.0364151][-4.2180777 -4.1243491 -4.0423918 -3.9894757 -3.9413059 -3.9594574 -4.029532 -4.0837588 -4.1208777 -4.1376343 -4.1333261 -4.1074114 -4.0643606 -4.023385 -3.9959807][-4.1970959 -4.0962725 -4.01238 -3.9575846 -3.9104862 -3.9309056 -3.9985638 -4.0474038 -4.0813527 -4.0972953 -4.0977573 -4.080616 -4.0411634 -4.0034914 -3.9786024][-4.1813874 -4.0741162 -3.9840333 -3.9232526 -3.8762143 -3.8929915 -3.9467971 -3.9839911 -4.0145111 -4.0368285 -4.0536065 -4.0538177 -4.0287495 -3.9958508 -3.9695168][-4.1703763 -4.0576859 -3.9604626 -3.8911016 -3.8407874 -3.84962 -3.8870349 -3.9154677 -3.9458704 -3.978327 -4.01115 -4.0287995 -4.0183125 -3.9863517 -3.9561331][-4.1646729 -4.0502195 -3.9457858 -3.8676553 -3.814501 -3.8145912 -3.8398118 -3.869662 -3.9050856 -3.9401453 -3.9816363 -4.0126705 -4.0155892 -3.9845829 -3.9497933][-4.1655326 -4.0556335 -3.9517605 -3.8752255 -3.8237405 -3.818696 -3.8393559 -3.8754992 -3.9112113 -3.9418855 -3.9872546 -4.0263381 -4.03626 -4.0030804 -3.9619069][-4.1698713 -4.0703931 -3.9777474 -3.914094 -3.8718815 -3.8660722 -3.8861809 -3.9225602 -3.950438 -3.9786942 -4.0268641 -4.0683036 -4.0751972 -4.0411229 -3.9988334][-4.1781716 -4.0946908 -4.0203118 -3.9701772 -3.9357572 -3.9330957 -3.9572487 -3.9932282 -4.0147305 -4.0395904 -4.0807323 -4.1170259 -4.1180348 -4.0856237 -4.0489564][-4.1937752 -4.1286511 -4.0737634 -4.0368376 -4.0100136 -4.0129728 -4.0384221 -4.07003 -4.08812 -4.1096411 -4.1418724 -4.1675506 -4.1621323 -4.1327386 -4.1039262][-4.2192016 -4.1714754 -4.1339684 -4.1117439 -4.0967612 -4.102509 -4.1270647 -4.1519623 -4.1648111 -4.1792569 -4.201407 -4.2165613 -4.2084837 -4.185163 -4.1641769][-4.2531 -4.2209177 -4.196907 -4.1856356 -4.1794429 -4.1873446 -4.2081342 -4.2252674 -4.2330675 -4.2395496 -4.2516389 -4.2588291 -4.2519126 -4.2363291 -4.2226195][-4.2864432 -4.26601 -4.250165 -4.2428074 -4.2397318 -4.24739 -4.2637854 -4.2757225 -4.2802253 -4.2830663 -4.289156 -4.292717 -4.2900119 -4.2836785 -4.2765055]]...]
INFO - root - 2017-12-05 11:19:22.796262: step 6510, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 85h:41m:44s remains)
INFO - root - 2017-12-05 11:19:32.035459: step 6520, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 0.786 sec/batch; 71h:10m:26s remains)
INFO - root - 2017-12-05 11:19:41.506925: step 6530, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 84h:10m:34s remains)
INFO - root - 2017-12-05 11:19:50.983717: step 6540, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.951 sec/batch; 86h:04m:04s remains)
INFO - root - 2017-12-05 11:20:00.574618: step 6550, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 89h:56m:25s remains)
INFO - root - 2017-12-05 11:20:10.091033: step 6560, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 90h:16m:18s remains)
INFO - root - 2017-12-05 11:20:19.538807: step 6570, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.980 sec/batch; 88h:42m:17s remains)
INFO - root - 2017-12-05 11:20:28.893208: step 6580, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 89h:13m:21s remains)
INFO - root - 2017-12-05 11:20:38.329135: step 6590, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 87h:39m:51s remains)
INFO - root - 2017-12-05 11:20:47.531166: step 6600, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 83h:21m:47s remains)
2017-12-05 11:20:48.324687: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3306375 -4.3121705 -4.2748771 -4.2308736 -4.1941366 -4.1664195 -4.1425009 -4.1138897 -4.0854149 -4.0721083 -4.0782957 -4.0948124 -4.1183028 -4.1505523 -4.165307][-4.3360634 -4.317575 -4.2814322 -4.2395577 -4.2026978 -4.1721468 -4.1494765 -4.1266985 -4.1022058 -4.0917892 -4.0964317 -4.1132188 -4.1419196 -4.171844 -4.1806645][-4.3344522 -4.313961 -4.2787623 -4.2395816 -4.2030606 -4.1742573 -4.1521754 -4.1331477 -4.117372 -4.1128159 -4.1154709 -4.1253633 -4.1484475 -4.1711984 -4.17645][-4.3325386 -4.3087697 -4.2681656 -4.22535 -4.1879616 -4.1642647 -4.1403637 -4.1221042 -4.1138358 -4.1175289 -4.1212378 -4.1232071 -4.1348639 -4.1557336 -4.1677403][-4.3279076 -4.2987251 -4.249548 -4.2028651 -4.1711173 -4.1517138 -4.121563 -4.0955191 -4.0895524 -4.1025796 -4.1148462 -4.1175756 -4.121964 -4.1431012 -4.1633759][-4.3184495 -4.2813349 -4.2212591 -4.1692982 -4.1377449 -4.1127877 -4.0697117 -4.0357227 -4.0410533 -4.0703483 -4.0960264 -4.1058726 -4.1096692 -4.1295571 -4.1526456][-4.30791 -4.2630234 -4.1930938 -4.1330481 -4.0899806 -4.0542831 -4.0018725 -3.9659696 -3.9839597 -4.0280409 -4.0648789 -4.0815711 -4.0886512 -4.1112819 -4.1364722][-4.3010883 -4.2535605 -4.1842237 -4.12411 -4.0765839 -4.034595 -3.977833 -3.9389987 -3.955807 -4.001956 -4.0468464 -4.0708356 -4.0781159 -4.0921969 -4.1098952][-4.3010168 -4.2587771 -4.2041149 -4.1597238 -4.1200833 -4.0823154 -4.0241122 -3.9775879 -3.9814475 -4.0203533 -4.0663347 -4.0922294 -4.09532 -4.0924497 -4.0913539][-4.3040929 -4.2679715 -4.223774 -4.1898971 -4.15805 -4.128654 -4.079504 -4.0359187 -4.0343652 -4.0670733 -4.107059 -4.1264577 -4.1200542 -4.1011944 -4.0841827][-4.3007908 -4.2670822 -4.2292895 -4.20173 -4.1779661 -4.158844 -4.1269226 -4.0963964 -4.0915923 -4.111495 -4.1388307 -4.1488304 -4.13494 -4.1081142 -4.0857592][-4.2971854 -4.2666636 -4.2359262 -4.2160482 -4.202023 -4.1915846 -4.1746035 -4.1556749 -4.1490359 -4.1535559 -4.1625605 -4.1601086 -4.1397204 -4.1079059 -4.0867858][-4.3015542 -4.27302 -4.243063 -4.2220078 -4.2103672 -4.2054505 -4.1958971 -4.1839161 -4.178967 -4.1784029 -4.1782808 -4.1705008 -4.1534629 -4.127666 -4.1086912][-4.3134956 -4.2896457 -4.2634616 -4.2445893 -4.2378736 -4.2382865 -4.2351017 -4.2264404 -4.2232428 -4.2210932 -4.2164474 -4.2065473 -4.1921821 -4.1698403 -4.1511965][-4.3254037 -4.3080883 -4.2876768 -4.2750955 -4.2741022 -4.2799239 -4.2786994 -4.2701783 -4.2658515 -4.2623496 -4.2567534 -4.2481775 -4.2398458 -4.2248049 -4.2101393]]...]
INFO - root - 2017-12-05 11:20:57.547109: step 6610, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 85h:18m:40s remains)
INFO - root - 2017-12-05 11:21:06.949658: step 6620, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.952 sec/batch; 86h:12m:49s remains)
INFO - root - 2017-12-05 11:21:16.460667: step 6630, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 84h:48m:47s remains)
INFO - root - 2017-12-05 11:21:25.911744: step 6640, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 85h:07m:48s remains)
INFO - root - 2017-12-05 11:21:35.206537: step 6650, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 77h:17m:49s remains)
INFO - root - 2017-12-05 11:21:44.590655: step 6660, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 87h:32m:16s remains)
INFO - root - 2017-12-05 11:21:54.155634: step 6670, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 84h:55m:24s remains)
INFO - root - 2017-12-05 11:22:03.523613: step 6680, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 84h:52m:48s remains)
INFO - root - 2017-12-05 11:22:12.834098: step 6690, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 81h:23m:49s remains)
INFO - root - 2017-12-05 11:22:22.315239: step 6700, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 86h:27m:58s remains)
2017-12-05 11:22:23.131023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.146194 -4.1694655 -4.187531 -4.1735654 -4.1327868 -4.0814891 -4.0347743 -4.0158863 -4.0477171 -4.1057935 -4.1651497 -4.20115 -4.2231355 -4.2319274 -4.2164307][-4.1393981 -4.1598425 -4.1854095 -4.187149 -4.1560621 -4.103693 -4.0436707 -4.009335 -4.0277114 -4.0743203 -4.1271787 -4.16602 -4.1937776 -4.2106714 -4.2078986][-4.1494818 -4.166316 -4.1904597 -4.1989532 -4.1760111 -4.12767 -4.0639691 -4.0202889 -4.0226393 -4.0515652 -4.0901957 -4.12554 -4.1521115 -4.1759377 -4.1864834][-4.1511683 -4.1788 -4.2072945 -4.2161374 -4.192904 -4.1481085 -4.0926747 -4.0498676 -4.0412951 -4.0543103 -4.0768991 -4.1012392 -4.1232529 -4.1497369 -4.1689672][-4.1270595 -4.1653452 -4.204607 -4.2204261 -4.1952553 -4.1488404 -4.0998082 -4.0646648 -4.0614324 -4.0759244 -4.0903649 -4.1092014 -4.1272354 -4.1464052 -4.1631141][-4.087244 -4.1267467 -4.1749258 -4.1972027 -4.1714091 -4.1189728 -4.0616517 -4.0255947 -4.0329323 -4.0678425 -4.0981283 -4.1274648 -4.1455579 -4.1589904 -4.1707911][-4.0463915 -4.0803647 -4.13688 -4.165123 -4.14257 -4.0822463 -4.0072536 -3.955349 -3.9629574 -4.0219684 -4.0836987 -4.13707 -4.1639605 -4.1783648 -4.1903939][-4.0187645 -4.0452819 -4.0965352 -4.1247668 -4.1065717 -4.0473394 -3.9725754 -3.9206276 -3.9291 -3.9906352 -4.0640464 -4.1298637 -4.1627688 -4.1765809 -4.18816][-4.0074334 -4.0241661 -4.0626483 -4.086483 -4.0752177 -4.0275211 -3.9734452 -3.9369836 -3.9452677 -3.9924505 -4.052928 -4.1123366 -4.1410804 -4.1500134 -4.1599879][-4.0168772 -4.0325952 -4.0647125 -4.0818648 -4.0753322 -4.0449967 -4.0126185 -3.9938054 -4.0025015 -4.0353394 -4.0809979 -4.1244106 -4.1404333 -4.1368103 -4.1407475][-4.0318284 -4.0570097 -4.0916023 -4.1070971 -4.1012487 -4.0806561 -4.0622597 -4.0532513 -4.0638285 -4.0919313 -4.1306152 -4.1630135 -4.169518 -4.1548476 -4.1498528][-4.0623803 -4.0958624 -4.13289 -4.1454854 -4.1396604 -4.1250596 -4.1138325 -4.1121078 -4.1274385 -4.1536326 -4.1888976 -4.21461 -4.215261 -4.1942334 -4.1802773][-4.128078 -4.1577492 -4.1856766 -4.1943192 -4.1882944 -4.1794453 -4.1723337 -4.1728458 -4.18875 -4.21337 -4.2434611 -4.2625756 -4.2611737 -4.2419534 -4.2236371][-4.2027826 -4.2201939 -4.2358918 -4.2434931 -4.2420883 -4.2377486 -4.2317295 -4.2308712 -4.2422948 -4.2624493 -4.2860985 -4.2999768 -4.3000126 -4.286489 -4.2697797][-4.2610731 -4.2705116 -4.2805061 -4.2883897 -4.2900019 -4.2852788 -4.2783828 -4.27633 -4.2830625 -4.2966447 -4.3133173 -4.3243551 -4.3263116 -4.3182673 -4.3067408]]...]
INFO - root - 2017-12-05 11:22:32.437354: step 6710, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 84h:14m:41s remains)
INFO - root - 2017-12-05 11:22:41.795757: step 6720, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 83h:29m:50s remains)
INFO - root - 2017-12-05 11:22:51.154770: step 6730, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 84h:20m:18s remains)
INFO - root - 2017-12-05 11:23:00.699721: step 6740, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 87h:22m:10s remains)
INFO - root - 2017-12-05 11:23:10.181062: step 6750, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 88h:27m:57s remains)
INFO - root - 2017-12-05 11:23:19.683055: step 6760, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.994 sec/batch; 89h:55m:57s remains)
INFO - root - 2017-12-05 11:23:29.233907: step 6770, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 81h:42m:55s remains)
INFO - root - 2017-12-05 11:23:38.602610: step 6780, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 84h:39m:39s remains)
INFO - root - 2017-12-05 11:23:48.156300: step 6790, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 88h:39m:43s remains)
INFO - root - 2017-12-05 11:23:57.620965: step 6800, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.007 sec/batch; 91h:08m:38s remains)
2017-12-05 11:23:58.379443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2872419 -4.2597976 -4.2300758 -4.2162118 -4.2226152 -4.2501478 -4.2683125 -4.27192 -4.2661972 -4.2600627 -4.2621427 -4.2536325 -4.244 -4.2377782 -4.2200122][-4.2837067 -4.2515373 -4.2142458 -4.190268 -4.1890512 -4.2128711 -4.2336812 -4.2404327 -4.2408247 -4.2409878 -4.2475057 -4.2372227 -4.2264581 -4.2225447 -4.2053185][-4.2830019 -4.2490931 -4.2068868 -4.1742215 -4.1652908 -4.1830139 -4.1977534 -4.19696 -4.1963019 -4.2024364 -4.215064 -4.2101822 -4.201324 -4.1970286 -4.1752682][-4.2835894 -4.24891 -4.2053771 -4.1662717 -4.147285 -4.1525912 -4.156652 -4.1475744 -4.1433048 -4.1545186 -4.1765671 -4.1758718 -4.1688056 -4.1614728 -4.1332569][-4.2873235 -4.2528005 -4.2105036 -4.1688633 -4.138576 -4.1315184 -4.1295447 -4.1183777 -4.1124969 -4.1263494 -4.1525574 -4.1539745 -4.1491961 -4.1417308 -4.1137638][-4.2903533 -4.2570958 -4.2149439 -4.1755004 -4.1457095 -4.1379008 -4.1353726 -4.1258984 -4.1185246 -4.1309614 -4.15506 -4.1594057 -4.1611772 -4.1560268 -4.1306071][-4.284862 -4.2479515 -4.2007027 -4.159606 -4.1349125 -4.1284494 -4.1255836 -4.120635 -4.1169691 -4.1312952 -4.1556931 -4.1682558 -4.1767364 -4.1756673 -4.1552906][-4.27203 -4.2270641 -4.1703234 -4.1266723 -4.1079764 -4.1025953 -4.09364 -4.0884748 -4.0836763 -4.1014028 -4.12987 -4.1498084 -4.16099 -4.1652288 -4.1562214][-4.2542715 -4.1988773 -4.1340384 -4.0860972 -4.0664396 -4.0593905 -4.04311 -4.0307512 -4.0165472 -4.0375695 -4.0744905 -4.1035171 -4.1226749 -4.1349115 -4.1391358][-4.2356668 -4.1705441 -4.0985126 -4.044342 -4.0197229 -4.0139084 -4.0026255 -3.9875016 -3.9689026 -3.9940956 -4.0386419 -4.0749626 -4.1020241 -4.1224294 -4.13637][-4.2317152 -4.1668682 -4.09616 -4.0414963 -4.0171008 -4.0153503 -4.0159378 -4.0094752 -4.0004206 -4.0228057 -4.0611148 -4.0941954 -4.1218128 -4.1460052 -4.1661363][-4.2504878 -4.1978831 -4.1394472 -4.0950723 -4.0758424 -4.0795321 -4.0878348 -4.0908475 -4.0877752 -4.1012731 -4.1286144 -4.1542959 -4.1779637 -4.2003665 -4.21825][-4.2824945 -4.2466259 -4.2066355 -4.1790247 -4.1703386 -4.1785827 -4.1876378 -4.194056 -4.1924353 -4.1943855 -4.2074766 -4.2221222 -4.2359056 -4.2495136 -4.2606406][-4.311204 -4.288691 -4.2620749 -4.2459154 -4.2421875 -4.2499914 -4.2577467 -4.2629967 -4.2605524 -4.2571139 -4.2622046 -4.2693238 -4.2771382 -4.2860641 -4.2936096][-4.3257041 -4.3123412 -4.2947526 -4.284441 -4.2840004 -4.2905297 -4.2970438 -4.300868 -4.2981467 -4.2934523 -4.2936082 -4.29665 -4.3012967 -4.30823 -4.3150387]]...]
INFO - root - 2017-12-05 11:24:07.573670: step 6810, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 88h:55m:54s remains)
INFO - root - 2017-12-05 11:24:16.743838: step 6820, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 87h:04m:48s remains)
INFO - root - 2017-12-05 11:24:26.199538: step 6830, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 82h:14m:48s remains)
INFO - root - 2017-12-05 11:24:35.591315: step 6840, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 85h:50m:13s remains)
INFO - root - 2017-12-05 11:24:44.958573: step 6850, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 81h:46m:40s remains)
INFO - root - 2017-12-05 11:24:54.304127: step 6860, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 83h:39m:25s remains)
INFO - root - 2017-12-05 11:25:03.628688: step 6870, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 83h:37m:47s remains)
INFO - root - 2017-12-05 11:25:13.055241: step 6880, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 81h:34m:10s remains)
INFO - root - 2017-12-05 11:25:22.447593: step 6890, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 79h:02m:04s remains)
INFO - root - 2017-12-05 11:25:31.762761: step 6900, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 83h:05m:14s remains)
2017-12-05 11:25:32.550286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1661615 -4.1394329 -4.1379204 -4.156424 -4.1714811 -4.1753483 -4.1604457 -4.14661 -4.1405005 -4.1402626 -4.1625605 -4.1899962 -4.2031889 -4.2077875 -4.2195654][-4.187192 -4.17027 -4.1701894 -4.18482 -4.2002268 -4.206562 -4.1902308 -4.174612 -4.1621671 -4.1566038 -4.1782207 -4.2121506 -4.2323813 -4.2371192 -4.2429571][-4.1928468 -4.1860237 -4.1894479 -4.2007847 -4.2180176 -4.2271848 -4.2119436 -4.195456 -4.1788216 -4.1710806 -4.1950436 -4.2347169 -4.256587 -4.2579226 -4.2571416][-4.1813807 -4.18178 -4.1876693 -4.2020359 -4.2219748 -4.2273498 -4.2072129 -4.1853004 -4.16704 -4.1641212 -4.1942129 -4.2385044 -4.2630677 -4.2652674 -4.260541][-4.1664906 -4.174675 -4.1837955 -4.2004333 -4.2151232 -4.2021871 -4.1629324 -4.12969 -4.1179781 -4.1304421 -4.17494 -4.2274733 -4.2550344 -4.2588282 -4.2481222][-4.1583357 -4.174583 -4.1849394 -4.1974578 -4.1918077 -4.1466823 -4.0746241 -4.0258346 -4.0336618 -4.0790639 -4.1481962 -4.2106013 -4.2417 -4.2430282 -4.2250628][-4.1534982 -4.17071 -4.1742444 -4.1747522 -4.1413302 -4.0521154 -3.9278569 -3.8693163 -3.9273691 -4.0271268 -4.1206217 -4.1916571 -4.2237349 -4.2214141 -4.1994872][-4.1590495 -4.164134 -4.150804 -4.1324825 -4.0649648 -3.920671 -3.7365127 -3.6945574 -3.8363295 -3.992136 -4.1051221 -4.1824217 -4.2160168 -4.209681 -4.1787682][-4.1710639 -4.1586761 -4.1316714 -4.0900064 -3.998271 -3.8274498 -3.6402183 -3.6554441 -3.8434348 -4.0128865 -4.1232643 -4.1947374 -4.2257619 -4.2139921 -4.1746969][-4.1786618 -4.1574326 -4.1243796 -4.0722113 -3.9870515 -3.860275 -3.7670503 -3.8209999 -3.9645414 -4.0905023 -4.1676292 -4.217907 -4.24099 -4.2277389 -4.1885395][-4.1731091 -4.1505051 -4.1149073 -4.0668931 -4.011601 -3.9507191 -3.9354744 -3.9930234 -4.0776238 -4.1501865 -4.1953192 -4.2265086 -4.2420506 -4.23453 -4.2053089][-4.1514134 -4.1270008 -4.0919838 -4.0614872 -4.0464396 -4.0348854 -4.0509191 -4.0963173 -4.1372657 -4.1717706 -4.1983528 -4.22012 -4.2323408 -4.2370591 -4.2273383][-4.1290846 -4.1105523 -4.0819535 -4.069798 -4.0894222 -4.1097026 -4.1316433 -4.1621065 -4.1728635 -4.1764531 -4.1901708 -4.2082138 -4.2205524 -4.2358403 -4.2431641][-4.1213622 -4.1141038 -4.0875397 -4.0841 -4.1235013 -4.162045 -4.1876826 -4.2062373 -4.1965852 -4.1754165 -4.1699176 -4.1842222 -4.2040076 -4.2293286 -4.24879][-4.1378651 -4.1322222 -4.1010938 -4.0937848 -4.1345787 -4.1824293 -4.2115507 -4.2224116 -4.2054024 -4.1737428 -4.1589584 -4.1717896 -4.1946344 -4.2212186 -4.2439694]]...]
INFO - root - 2017-12-05 11:25:41.977687: step 6910, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 84h:04m:15s remains)
INFO - root - 2017-12-05 11:25:51.268809: step 6920, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 82h:00m:18s remains)
INFO - root - 2017-12-05 11:26:00.633929: step 6930, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 84h:11m:21s remains)
INFO - root - 2017-12-05 11:26:10.263242: step 6940, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 87h:03m:46s remains)
INFO - root - 2017-12-05 11:26:19.616350: step 6950, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 87h:36m:57s remains)
INFO - root - 2017-12-05 11:26:29.041888: step 6960, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 84h:55m:11s remains)
INFO - root - 2017-12-05 11:26:38.329004: step 6970, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 79h:45m:41s remains)
INFO - root - 2017-12-05 11:26:47.505354: step 6980, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 75h:21m:43s remains)
INFO - root - 2017-12-05 11:26:56.742630: step 6990, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 77h:47m:36s remains)
INFO - root - 2017-12-05 11:27:06.161608: step 7000, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 79h:37m:18s remains)
2017-12-05 11:27:06.898091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1472845 -4.1355453 -4.1150951 -4.1026611 -4.0974565 -4.1125765 -4.1313696 -4.1422939 -4.1315022 -4.1098166 -4.0961123 -4.1000929 -4.0994067 -4.0978551 -4.1081123][-4.1281171 -4.1142416 -4.0975761 -4.0890555 -4.0879197 -4.1069078 -4.1233482 -4.1338973 -4.1288376 -4.1181517 -4.1063619 -4.1037889 -4.0969462 -4.0875826 -4.0918941][-4.1206665 -4.1067648 -4.096487 -4.0862384 -4.0792537 -4.0917149 -4.1042404 -4.1136918 -4.1196795 -4.1244354 -4.1227694 -4.1088076 -4.0854478 -4.0700278 -4.0724096][-4.119246 -4.1183619 -4.1164947 -4.1043406 -4.0893888 -4.0853157 -4.0782881 -4.0785289 -4.096539 -4.1219134 -4.1296716 -4.1080785 -4.0765858 -4.070673 -4.0767088][-4.11421 -4.1212015 -4.1241016 -4.1142516 -4.0981951 -4.080246 -4.0470972 -4.0358663 -4.0707626 -4.11933 -4.1437187 -4.129 -4.1070356 -4.1116934 -4.1181006][-4.0979323 -4.1132588 -4.1244321 -4.1209412 -4.1033974 -4.0640836 -3.9910402 -3.9628451 -4.0262508 -4.1077986 -4.1521115 -4.149025 -4.1369843 -4.1433382 -4.1458526][-4.054215 -4.0802097 -4.1051855 -4.105999 -4.0790472 -4.0142159 -3.905287 -3.87573 -3.9869983 -4.1002216 -4.153563 -4.1586847 -4.15407 -4.1558208 -4.1504769][-4.0073476 -4.0405359 -4.0748553 -4.0794306 -4.0496206 -3.981879 -3.8809018 -3.8823414 -4.0064397 -4.1086073 -4.1468019 -4.1522508 -4.1503358 -4.1502404 -4.1472659][-3.997278 -4.0388894 -4.0766964 -4.0827174 -4.062139 -4.0191016 -3.9640191 -3.9853389 -4.068614 -4.1254086 -4.1389174 -4.1351619 -4.1264353 -4.1249137 -4.1310673][-4.0308638 -4.0692492 -4.0958481 -4.097506 -4.0924253 -4.0727148 -4.0487342 -4.0677619 -4.1156764 -4.1406088 -4.1391253 -4.1323271 -4.1214504 -4.1120539 -4.112421][-4.0755591 -4.109221 -4.1231527 -4.1211538 -4.1228147 -4.1117644 -4.0963411 -4.105207 -4.13114 -4.1422138 -4.1414738 -4.1405058 -4.1307926 -4.10966 -4.0980668][-4.1214533 -4.1493387 -4.1537395 -4.1475272 -4.1484971 -4.1352935 -4.1122241 -4.1126161 -4.1257124 -4.1348295 -4.1417384 -4.1512694 -4.1389456 -4.1134453 -4.099493][-4.1460428 -4.1630669 -4.1594529 -4.1517496 -4.1565566 -4.1428452 -4.1189389 -4.1197839 -4.1323705 -4.1397438 -4.1503358 -4.1618748 -4.1468964 -4.1193972 -4.1152997][-4.1421857 -4.1491513 -4.1425548 -4.1394477 -4.150938 -4.1429911 -4.1241517 -4.1242409 -4.134212 -4.1382871 -4.1445818 -4.1493883 -4.1319284 -4.1115575 -4.120348][-4.1292844 -4.1308355 -4.1273384 -4.1361961 -4.1525044 -4.1457386 -4.1233053 -4.1129947 -4.1166148 -4.1187773 -4.1264677 -4.1325297 -4.1164227 -4.0995665 -4.1120586]]...]
INFO - root - 2017-12-05 11:27:16.406483: step 7010, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 84h:11m:29s remains)
INFO - root - 2017-12-05 11:27:25.733323: step 7020, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 82h:15m:36s remains)
INFO - root - 2017-12-05 11:27:35.076215: step 7030, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 85h:38m:56s remains)
INFO - root - 2017-12-05 11:27:44.513567: step 7040, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 82h:13m:42s remains)
INFO - root - 2017-12-05 11:27:53.746134: step 7050, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 78h:33m:53s remains)
INFO - root - 2017-12-05 11:28:02.979425: step 7060, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 83h:49m:11s remains)
INFO - root - 2017-12-05 11:28:12.193896: step 7070, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 87h:58m:48s remains)
INFO - root - 2017-12-05 11:28:21.670209: step 7080, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 84h:00m:38s remains)
INFO - root - 2017-12-05 11:28:31.049977: step 7090, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 86h:39m:42s remains)
INFO - root - 2017-12-05 11:28:40.591333: step 7100, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 85h:43m:54s remains)
2017-12-05 11:28:41.352960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3018427 -4.3047032 -4.3017898 -4.291369 -4.2719173 -4.2485151 -4.2267776 -4.2129664 -4.217082 -4.2318921 -4.25044 -4.2672539 -4.2717462 -4.2602377 -4.2403579][-4.3043046 -4.3075471 -4.3021483 -4.2826972 -4.2503085 -4.2120886 -4.1786847 -4.16493 -4.1785245 -4.2072206 -4.2373466 -4.2608337 -4.2704697 -4.2623277 -4.2417359][-4.2973628 -4.2993011 -4.2898374 -4.2599268 -4.21288 -4.1587 -4.113018 -4.1021228 -4.1300049 -4.1740713 -4.2177329 -4.2473731 -4.2620225 -4.2583423 -4.2403312][-4.2934785 -4.2929282 -4.2778277 -4.2358437 -4.1720476 -4.0993414 -4.040277 -4.0335855 -4.0763569 -4.1373563 -4.1950269 -4.2340956 -4.2542148 -4.2527108 -4.2364287][-4.3007259 -4.2958255 -4.2722015 -4.2181716 -4.1389809 -4.048111 -3.9775832 -3.9739871 -4.0280724 -4.1051207 -4.1778517 -4.2245545 -4.2495618 -4.2476897 -4.2334123][-4.31133 -4.299726 -4.2683935 -4.2042737 -4.1129122 -4.0120244 -3.9410884 -3.935096 -3.9897044 -4.073782 -4.152348 -4.207469 -4.23915 -4.2450986 -4.2373509][-4.3047214 -4.2860742 -4.2483754 -4.1761737 -4.081655 -3.9810359 -3.9131746 -3.9015965 -3.9488955 -4.0318255 -4.1072197 -4.167408 -4.2134013 -4.2358608 -4.2438059][-4.2945 -4.2675428 -4.2241597 -4.1493235 -4.0582581 -3.9669051 -3.9036515 -3.8849754 -3.923708 -4.0008636 -4.0713 -4.1344032 -4.1935596 -4.2297597 -4.2497911][-4.2882681 -4.2547035 -4.2029037 -4.131176 -4.0590963 -3.9911661 -3.9377096 -3.910336 -3.9352245 -4.0030384 -4.0693521 -4.1326957 -4.1965585 -4.2358632 -4.2571959][-4.27787 -4.2395353 -4.1819472 -4.1181669 -4.0731978 -4.0374007 -4.0023232 -3.9687495 -3.9744151 -4.0242677 -4.0858793 -4.1517377 -4.2117682 -4.2426934 -4.2480922][-4.2674804 -4.2314744 -4.1808076 -4.1324582 -4.1095762 -4.0963254 -4.0783186 -4.0486608 -4.0437713 -4.0758786 -4.1267333 -4.1840954 -4.22841 -4.2409987 -4.2204976][-4.2770352 -4.2488379 -4.2104678 -4.1805367 -4.1694412 -4.1667709 -4.1627097 -4.1457529 -4.1377673 -4.153419 -4.1858187 -4.223145 -4.2437367 -4.2354293 -4.1956906][-4.2881641 -4.2717671 -4.2474709 -4.2310514 -4.2268705 -4.2299967 -4.233902 -4.2265038 -4.2182708 -4.2234244 -4.23919 -4.2588577 -4.2586846 -4.2331448 -4.1828866][-4.2947245 -4.2908263 -4.2823114 -4.2764096 -4.2762771 -4.2813129 -4.2855148 -4.2811394 -4.2740412 -4.2745576 -4.2797446 -4.2840877 -4.2723718 -4.2400107 -4.1910787][-4.2990112 -4.3001046 -4.3014121 -4.3005886 -4.3004937 -4.3044581 -4.3067861 -4.3044276 -4.29918 -4.2971687 -4.295722 -4.29363 -4.2826838 -4.2569814 -4.2205243]]...]
INFO - root - 2017-12-05 11:28:50.881712: step 7110, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.948 sec/batch; 85h:42m:47s remains)
INFO - root - 2017-12-05 11:28:59.872867: step 7120, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.958 sec/batch; 86h:35m:32s remains)
INFO - root - 2017-12-05 11:29:09.284695: step 7130, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 85h:26m:25s remains)
INFO - root - 2017-12-05 11:29:18.692522: step 7140, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 88h:26m:38s remains)
INFO - root - 2017-12-05 11:29:28.253456: step 7150, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 89h:30m:58s remains)
INFO - root - 2017-12-05 11:29:37.212907: step 7160, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 77h:36m:16s remains)
INFO - root - 2017-12-05 11:29:46.535453: step 7170, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 83h:26m:57s remains)
INFO - root - 2017-12-05 11:29:56.173231: step 7180, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 89h:07m:42s remains)
INFO - root - 2017-12-05 11:30:05.544667: step 7190, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 87h:49m:31s remains)
INFO - root - 2017-12-05 11:30:15.063380: step 7200, loss = 2.05, batch loss = 2.00 (7.8 examples/sec; 1.023 sec/batch; 92h:26m:04s remains)
2017-12-05 11:30:15.833096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3361187 -4.3294606 -4.3223386 -4.315237 -4.3078833 -4.3004689 -4.2955694 -4.2925329 -4.293879 -4.3008194 -4.3055024 -4.3116679 -4.3204694 -4.3261681 -4.3318286][-4.3236241 -4.3095074 -4.2944293 -4.2805724 -4.2679076 -4.2562933 -4.2457123 -4.2396197 -4.2424431 -4.2550454 -4.2631879 -4.27243 -4.2896128 -4.3037763 -4.3151827][-4.2953982 -4.2688665 -4.2435107 -4.2198629 -4.1984782 -4.180068 -4.1629319 -4.1568179 -4.1642141 -4.1822481 -4.1977315 -4.2126389 -4.2393346 -4.2652025 -4.2877045][-4.2597947 -4.2190146 -4.1810803 -4.147572 -4.11809 -4.0891428 -4.0649281 -4.0676336 -4.083591 -4.1048212 -4.1282325 -4.1483426 -4.1810989 -4.2168922 -4.2517996][-4.2309518 -4.1767192 -4.1272445 -4.0835729 -4.0437646 -3.999351 -3.9679658 -3.9881811 -4.013957 -4.0381937 -4.0698247 -4.0969219 -4.1346517 -4.1783919 -4.2212224][-4.2064471 -4.1375647 -4.0723829 -4.0160184 -3.964833 -3.9045253 -3.8672752 -3.9026463 -3.9389451 -3.966573 -4.006403 -4.0448542 -4.0917044 -4.1421919 -4.1908207][-4.1916018 -4.1117787 -4.0318742 -3.9644575 -3.9105043 -3.845861 -3.8049707 -3.8410668 -3.8768644 -3.9138951 -3.9617329 -4.0092092 -4.0647545 -4.1201835 -4.1735277][-4.1923952 -4.1162028 -4.0401897 -3.98145 -3.9405787 -3.8891592 -3.8500948 -3.8724456 -3.8957186 -3.9308553 -3.9740677 -4.0166082 -4.0693564 -4.1225324 -4.1762013][-4.2085881 -4.1435232 -4.0847487 -4.0470896 -4.0223875 -3.9876466 -3.9563785 -3.9684699 -3.977299 -4.0057249 -4.0373755 -4.0700669 -4.1118646 -4.1553345 -4.204947][-4.2319975 -4.1812115 -4.1417875 -4.1211367 -4.1073928 -4.0860009 -4.0716405 -4.0825357 -4.0825915 -4.1006508 -4.1235123 -4.1488256 -4.17787 -4.2067823 -4.2455835][-4.2627664 -4.2271695 -4.2021585 -4.1915483 -4.182179 -4.1710081 -4.1749225 -4.1906013 -4.188477 -4.1974578 -4.2168617 -4.2397308 -4.2572427 -4.2706556 -4.2933712][-4.2961712 -4.2791672 -4.2669964 -4.2632432 -4.2560124 -4.2493277 -4.2627559 -4.2784944 -4.272018 -4.2737889 -4.2918396 -4.314343 -4.3245525 -4.3274064 -4.3354621][-4.3180246 -4.3144875 -4.3131237 -4.3137574 -4.3099208 -4.3073688 -4.3205643 -4.330801 -4.32292 -4.3220377 -4.3380017 -4.3570752 -4.36126 -4.357903 -4.358357][-4.329391 -4.3316026 -4.3345804 -4.3375454 -4.3359761 -4.336122 -4.3448396 -4.347548 -4.3403864 -4.3391991 -4.3505845 -4.36371 -4.3662205 -4.3634486 -4.3623934][-4.3384562 -4.3415709 -4.3457294 -4.3489652 -4.3488722 -4.3484192 -4.350554 -4.349328 -4.3443146 -4.3432708 -4.349823 -4.3572259 -4.3600545 -4.3588767 -4.3576031]]...]
INFO - root - 2017-12-05 11:30:25.216448: step 7210, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 87h:47m:35s remains)
INFO - root - 2017-12-05 11:30:34.398568: step 7220, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 83h:46m:21s remains)
INFO - root - 2017-12-05 11:30:43.967800: step 7230, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 87h:20m:43s remains)
INFO - root - 2017-12-05 11:30:53.521193: step 7240, loss = 2.03, batch loss = 1.97 (8.1 examples/sec; 0.990 sec/batch; 89h:29m:14s remains)
INFO - root - 2017-12-05 11:31:02.627547: step 7250, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 81h:02m:28s remains)
INFO - root - 2017-12-05 11:31:11.942431: step 7260, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 86h:29m:40s remains)
INFO - root - 2017-12-05 11:31:21.304484: step 7270, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 87h:37m:54s remains)
INFO - root - 2017-12-05 11:31:30.830167: step 7280, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 85h:42m:39s remains)
INFO - root - 2017-12-05 11:31:40.155779: step 7290, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 78h:34m:35s remains)
INFO - root - 2017-12-05 11:31:49.366040: step 7300, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 86h:52m:53s remains)
2017-12-05 11:31:50.132404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2865248 -4.2764759 -4.2814727 -4.2930436 -4.299839 -4.2983956 -4.2886539 -4.2870016 -4.2916236 -4.2964292 -4.2972746 -4.2898655 -4.2814445 -4.2799969 -4.2865262][-4.2351413 -4.2166762 -4.2211571 -4.23773 -4.2484431 -4.2465587 -4.2351007 -4.2331185 -4.2373872 -4.2402821 -4.2400093 -4.2334585 -4.2285271 -4.2338037 -4.2456131][-4.1952615 -4.1717896 -4.1731153 -4.1928596 -4.2090898 -4.2092838 -4.2001376 -4.2004719 -4.203454 -4.2016411 -4.1997719 -4.1971769 -4.1995063 -4.2104545 -4.2258577][-4.1827641 -4.1644855 -4.1656938 -4.1823049 -4.1952066 -4.1915584 -4.18309 -4.1843867 -4.1887255 -4.1889257 -4.192987 -4.1992397 -4.2094669 -4.2192488 -4.2286553][-4.1913323 -4.1797967 -4.1830192 -4.1912475 -4.190093 -4.1758404 -4.1564465 -4.1453424 -4.1477814 -4.1664591 -4.191957 -4.2109389 -4.2247787 -4.2257795 -4.2246871][-4.2068944 -4.2008138 -4.2060523 -4.2086034 -4.1955376 -4.1669812 -4.1189947 -4.0719752 -4.06334 -4.1173148 -4.1810985 -4.21516 -4.2294283 -4.2262673 -4.2187834][-4.2197051 -4.2171879 -4.2222948 -4.2237654 -4.2053471 -4.1618018 -4.0815063 -3.9865646 -3.9583235 -4.0537639 -4.1586318 -4.2071257 -4.2238879 -4.2258115 -4.2220092][-4.2224789 -4.2192707 -4.2243037 -4.2290058 -4.2174859 -4.1736941 -4.0858746 -3.9788361 -3.9411676 -4.0411134 -4.1524014 -4.2036719 -4.2177348 -4.2251205 -4.2277455][-4.217957 -4.2204323 -4.2320728 -4.2451444 -4.2451658 -4.215786 -4.1534934 -4.0842295 -4.0634117 -4.1188622 -4.1888275 -4.2219315 -4.2255197 -4.229784 -4.2319331][-4.2074761 -4.2131238 -4.2284722 -4.2488294 -4.2587285 -4.248847 -4.220386 -4.18969 -4.181644 -4.2066097 -4.2366295 -4.2437291 -4.233614 -4.2254419 -4.2202687][-4.2039819 -4.204145 -4.2185907 -4.2410355 -4.254714 -4.2553926 -4.2507095 -4.248569 -4.2493362 -4.2596726 -4.269177 -4.2673054 -4.2550912 -4.2391849 -4.2238865][-4.2150383 -4.2082376 -4.2175555 -4.2366157 -4.249877 -4.2531085 -4.2568808 -4.264008 -4.27162 -4.2802191 -4.2843747 -4.2863669 -4.2834225 -4.2719884 -4.2554297][-4.246181 -4.240417 -4.2462296 -4.2564282 -4.2595673 -4.2600551 -4.264524 -4.2727604 -4.2839236 -4.2952871 -4.3032203 -4.3089013 -4.3137493 -4.3077579 -4.2933025][-4.2730627 -4.2735415 -4.2800331 -4.2853856 -4.2824802 -4.2805262 -4.2808247 -4.2857018 -4.2968469 -4.30945 -4.3186951 -4.3224292 -4.3254824 -4.3225422 -4.3111076][-4.2867332 -4.2903686 -4.2975121 -4.3002782 -4.2975454 -4.2970753 -4.2963438 -4.2955718 -4.3032117 -4.315834 -4.3237009 -4.3257866 -4.3250833 -4.3239036 -4.315598]]...]
INFO - root - 2017-12-05 11:31:59.839688: step 7310, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 86h:19m:49s remains)
INFO - root - 2017-12-05 11:32:09.167324: step 7320, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 84h:18m:08s remains)
INFO - root - 2017-12-05 11:32:18.657800: step 7330, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.991 sec/batch; 89h:28m:30s remains)
INFO - root - 2017-12-05 11:32:27.965187: step 7340, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 84h:54m:44s remains)
INFO - root - 2017-12-05 11:32:37.204521: step 7350, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 84h:04m:57s remains)
INFO - root - 2017-12-05 11:32:46.403186: step 7360, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 83h:09m:25s remains)
INFO - root - 2017-12-05 11:32:55.828716: step 7370, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 83h:59m:04s remains)
INFO - root - 2017-12-05 11:33:05.195736: step 7380, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 83h:40m:53s remains)
INFO - root - 2017-12-05 11:33:14.591336: step 7390, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 83h:11m:36s remains)
INFO - root - 2017-12-05 11:33:23.850216: step 7400, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 80h:29m:27s remains)
2017-12-05 11:33:24.656159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3105688 -4.3127732 -4.30206 -4.2878218 -4.270947 -4.2390313 -4.2077374 -4.1953378 -4.18687 -4.1814804 -4.193625 -4.2097735 -4.2112479 -4.1936421 -4.169611][-4.2914944 -4.2987118 -4.3019552 -4.3043981 -4.3026128 -4.2861176 -4.267807 -4.2534323 -4.2385173 -4.229569 -4.2389169 -4.2466955 -4.2334304 -4.20674 -4.1745834][-4.23806 -4.2463837 -4.2594776 -4.2760692 -4.2863212 -4.2877989 -4.2893834 -4.2867117 -4.2785563 -4.2728572 -4.2785497 -4.2800527 -4.2608848 -4.233778 -4.2002916][-4.1719069 -4.1814914 -4.1977892 -4.2200704 -4.2370181 -4.2492785 -4.2633119 -4.2746062 -4.2829576 -4.2911205 -4.3004074 -4.3023515 -4.2871323 -4.2651029 -4.2394552][-4.1242375 -4.1339359 -4.1498365 -4.1695728 -4.1814609 -4.1839705 -4.1831517 -4.1929746 -4.21787 -4.2525792 -4.2799077 -4.2952118 -4.2969494 -4.2870193 -4.2768784][-4.1215625 -4.1310863 -4.1439486 -4.1529207 -4.1452389 -4.1137676 -4.0665321 -4.0446258 -4.0854015 -4.161129 -4.2222419 -4.2588096 -4.2829642 -4.2907343 -4.2972627][-4.1624074 -4.1694913 -4.1763606 -4.1678219 -4.1310816 -4.0557122 -3.9449544 -3.8709316 -3.928278 -4.0526018 -4.148181 -4.2052784 -4.2470217 -4.2712584 -4.2922411][-4.2141824 -4.2171712 -4.2208581 -4.2041216 -4.153635 -4.0602365 -3.9209545 -3.8106661 -3.8631454 -3.993387 -4.0922089 -4.1524277 -4.1999612 -4.2337966 -4.2599998][-4.246871 -4.2502327 -4.2585764 -4.2455454 -4.2063026 -4.1354322 -4.0336137 -3.9515958 -3.9583454 -4.0228491 -4.0806723 -4.1219177 -4.1599007 -4.1924715 -4.2158771][-4.2534709 -4.2547212 -4.2686057 -4.26994 -4.2498107 -4.2102833 -4.1537867 -4.0992923 -4.0777721 -4.0837312 -4.0948186 -4.1085162 -4.1276784 -4.1505785 -4.1676335][-4.2294068 -4.2313571 -4.2495971 -4.2651896 -4.2658706 -4.2510591 -4.22388 -4.1889563 -4.162322 -4.1458249 -4.131309 -4.122725 -4.1231928 -4.1282325 -4.1261644][-4.1952543 -4.1869054 -4.20281 -4.2274327 -4.2441278 -4.2488041 -4.2435455 -4.2290363 -4.214983 -4.2039328 -4.1887755 -4.1727681 -4.1555009 -4.1386323 -4.1133757][-4.1768174 -4.1492405 -4.1531053 -4.1765232 -4.2016616 -4.2234144 -4.235425 -4.2425709 -4.2455859 -4.2484555 -4.2420168 -4.2273617 -4.204638 -4.1733704 -4.1374607][-4.1836524 -4.1475539 -4.137331 -4.147377 -4.1718535 -4.2024364 -4.2273426 -4.2501507 -4.2677116 -4.2788758 -4.2780514 -4.266027 -4.24375 -4.21397 -4.182065][-4.2152224 -4.1837626 -4.1587358 -4.1505651 -4.166285 -4.1967611 -4.2272897 -4.2564936 -4.2792993 -4.2929268 -4.2973151 -4.2913947 -4.2750082 -4.2537856 -4.2330179]]...]
INFO - root - 2017-12-05 11:33:34.174928: step 7410, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 88h:42m:01s remains)
INFO - root - 2017-12-05 11:33:43.509747: step 7420, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 84h:03m:38s remains)
INFO - root - 2017-12-05 11:33:52.770354: step 7430, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 76h:29m:58s remains)
INFO - root - 2017-12-05 11:34:02.234527: step 7440, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 86h:13m:51s remains)
INFO - root - 2017-12-05 11:34:11.385658: step 7450, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.828 sec/batch; 74h:46m:51s remains)
INFO - root - 2017-12-05 11:34:20.898661: step 7460, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 79h:51m:33s remains)
INFO - root - 2017-12-05 11:34:30.302454: step 7470, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 84h:58m:30s remains)
INFO - root - 2017-12-05 11:34:39.639536: step 7480, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 84h:49m:11s remains)
INFO - root - 2017-12-05 11:34:48.965689: step 7490, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 82h:09m:26s remains)
INFO - root - 2017-12-05 11:34:58.377374: step 7500, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 83h:43m:27s remains)
2017-12-05 11:34:59.212323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2262158 -4.2169271 -4.2049303 -4.1816449 -4.1551933 -4.1440611 -4.1609817 -4.2134848 -4.2619562 -4.290206 -4.3017015 -4.2988396 -4.2831416 -4.2548232 -4.2107997][-4.1922126 -4.1833091 -4.1660867 -4.1278677 -4.0886397 -4.0738516 -4.10488 -4.1843548 -4.2485585 -4.2837224 -4.3023767 -4.301331 -4.2847624 -4.2549396 -4.2081604][-4.1498604 -4.1501923 -4.137208 -4.0926094 -4.0415378 -4.022347 -4.0605617 -4.159214 -4.2395797 -4.2773829 -4.2947326 -4.2904224 -4.2753754 -4.2534761 -4.2150249][-4.1229057 -4.13868 -4.1390944 -4.0974007 -4.0431428 -4.0217924 -4.0518956 -4.1472883 -4.230433 -4.2654591 -4.2790279 -4.27151 -4.2612934 -4.250484 -4.223691][-4.1381254 -4.1597433 -4.1684713 -4.1346321 -4.08383 -4.0617552 -4.0751672 -4.1421032 -4.2096319 -4.2380085 -4.2456574 -4.2380333 -4.2305312 -4.2354145 -4.2221646][-4.174829 -4.19556 -4.2061863 -4.1756997 -4.1271753 -4.1007233 -4.0942149 -4.1281114 -4.1773534 -4.2039146 -4.2082891 -4.2049022 -4.2065835 -4.2206879 -4.2162247][-4.1901956 -4.2060351 -4.2151022 -4.1871414 -4.1369057 -4.0949097 -4.0702467 -4.0835843 -4.1278358 -4.1647968 -4.1767712 -4.1791949 -4.1884623 -4.2055688 -4.2057967][-4.1706185 -4.1813006 -4.1878786 -4.1576519 -4.1043634 -4.0433888 -4.0016322 -4.0127144 -4.0711894 -4.1292677 -4.1508474 -4.1609306 -4.1785703 -4.1964955 -4.1984234][-4.1197748 -4.1313882 -4.1456971 -4.1241632 -4.0742011 -3.9995356 -3.9454331 -3.9658108 -4.0495262 -4.1302161 -4.1606064 -4.17326 -4.1886983 -4.1975846 -4.19483][-4.1018105 -4.1225538 -4.154954 -4.1442127 -4.0923719 -4.014442 -3.9657669 -3.9925487 -4.08104 -4.1610851 -4.1924114 -4.1996593 -4.2053871 -4.2062531 -4.1954412][-4.1251 -4.1479163 -4.1873994 -4.1823912 -4.1355157 -4.0737557 -4.0419922 -4.068161 -4.1361923 -4.1970453 -4.2181635 -4.2138047 -4.2088518 -4.2105474 -4.2042227][-4.1694784 -4.1857028 -4.2151718 -4.2050238 -4.1663637 -4.1305537 -4.1196404 -4.1449118 -4.1927991 -4.2259922 -4.23065 -4.2141509 -4.2028713 -4.2092819 -4.2131815][-4.2222366 -4.230978 -4.2455273 -4.2263441 -4.1959782 -4.1744952 -4.1760492 -4.2011528 -4.23787 -4.2529154 -4.2426219 -4.2234383 -4.2099576 -4.2218151 -4.2348242][-4.2701521 -4.2733541 -4.2725616 -4.2541575 -4.2311945 -4.2122073 -4.2160907 -4.2350345 -4.2613697 -4.2706547 -4.2595091 -4.241961 -4.2276092 -4.2406807 -4.2568049][-4.2973723 -4.2998123 -4.2965593 -4.2805357 -4.2626867 -4.2472396 -4.2478118 -4.2578416 -4.2743983 -4.2848611 -4.2813811 -4.2665658 -4.25446 -4.2661285 -4.2810707]]...]
INFO - root - 2017-12-05 11:35:08.641377: step 7510, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 85h:21m:57s remains)
INFO - root - 2017-12-05 11:35:17.849901: step 7520, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 82h:55m:11s remains)
INFO - root - 2017-12-05 11:35:27.081180: step 7530, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 85h:53m:09s remains)
INFO - root - 2017-12-05 11:35:36.614743: step 7540, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 83h:09m:31s remains)
INFO - root - 2017-12-05 11:35:45.991816: step 7550, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 83h:16m:57s remains)
INFO - root - 2017-12-05 11:35:55.394989: step 7560, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 84h:14m:10s remains)
INFO - root - 2017-12-05 11:36:04.692131: step 7570, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.881 sec/batch; 79h:30m:12s remains)
INFO - root - 2017-12-05 11:36:14.012674: step 7580, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.828 sec/batch; 74h:44m:07s remains)
INFO - root - 2017-12-05 11:36:23.230246: step 7590, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 83h:15m:33s remains)
INFO - root - 2017-12-05 11:36:32.686081: step 7600, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 76h:04m:04s remains)
2017-12-05 11:36:33.434454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3163495 -4.3071389 -4.294044 -4.28613 -4.2840047 -4.28085 -4.277422 -4.2772617 -4.2861538 -4.29194 -4.2898173 -4.2928524 -4.3053865 -4.3176727 -4.3198977][-4.2963614 -4.2842064 -4.2660651 -4.2535958 -4.244504 -4.2331738 -4.2218766 -4.2176557 -4.2317452 -4.2469172 -4.2517924 -4.2603526 -4.2791462 -4.29521 -4.2951951][-4.2754269 -4.2619705 -4.2410069 -4.2258549 -4.2083354 -4.1856947 -4.1629677 -4.1547689 -4.1728449 -4.1937933 -4.2070608 -4.2209334 -4.2445149 -4.2635884 -4.2628646][-4.2576976 -4.2436762 -4.2222137 -4.2038817 -4.177702 -4.1426172 -4.1112056 -4.102984 -4.1253715 -4.1483421 -4.1615162 -4.1733394 -4.1952629 -4.2174258 -4.223381][-4.2432008 -4.2267575 -4.2035093 -4.179853 -4.1440139 -4.0946856 -4.0533953 -4.0505195 -4.0827923 -4.1076813 -4.1158023 -4.1173825 -4.1315832 -4.15338 -4.1729774][-4.2361159 -4.2159843 -4.1888113 -4.1573834 -4.1068959 -4.0338244 -3.9753959 -3.9873705 -4.0426579 -4.0756617 -4.0830355 -4.0729141 -4.0739098 -4.0961733 -4.1326852][-4.2359242 -4.2138662 -4.1864009 -4.1514783 -4.0862408 -3.9806881 -3.8942976 -3.920619 -4.0037465 -4.0510182 -4.0613914 -4.0461659 -4.0376439 -4.0599513 -4.1104479][-4.2420678 -4.222208 -4.2008247 -4.171278 -4.1070795 -3.9936917 -3.8923149 -3.9117637 -3.9961338 -4.0456924 -4.0575962 -4.0437841 -4.0323372 -4.0498776 -4.1002917][-4.2515821 -4.2377963 -4.2248797 -4.2054567 -4.1585426 -4.0734596 -3.9919693 -3.9896684 -4.0395179 -4.06806 -4.0766931 -4.0745234 -4.0728059 -4.0835509 -4.1144395][-4.2587142 -4.2492204 -4.240715 -4.2260108 -4.1924815 -4.138607 -4.0860443 -4.0764203 -4.0952048 -4.1017952 -4.1059608 -4.119019 -4.1296706 -4.1369276 -4.1487947][-4.2597122 -4.2501926 -4.24323 -4.2287264 -4.200686 -4.1673265 -4.1389403 -4.1342983 -4.1419725 -4.1374254 -4.1370506 -4.1546764 -4.171556 -4.1792927 -4.1839809][-4.2542067 -4.2410455 -4.2318931 -4.2157073 -4.1907992 -4.17193 -4.1672893 -4.1766443 -4.1830235 -4.1735444 -4.1688313 -4.1821389 -4.1983438 -4.207139 -4.2125435][-4.2495718 -4.2321491 -4.2161775 -4.1950932 -4.16988 -4.1588812 -4.1675234 -4.1906471 -4.2059855 -4.1995063 -4.1935277 -4.2007565 -4.2126989 -4.2205281 -4.2232423][-4.250133 -4.2300596 -4.2081017 -4.1805515 -4.1497955 -4.1394496 -4.1505642 -4.1809559 -4.2069178 -4.2089453 -4.2023983 -4.2029428 -4.2102466 -4.2168694 -4.2180882][-4.2545781 -4.2364 -4.2134767 -4.1826859 -4.1492968 -4.1354971 -4.1435752 -4.172431 -4.2016516 -4.2105546 -4.2060442 -4.2038097 -4.2089329 -4.217432 -4.2194085]]...]
INFO - root - 2017-12-05 11:36:42.927180: step 7610, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.946 sec/batch; 85h:20m:20s remains)
INFO - root - 2017-12-05 11:36:51.969983: step 7620, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 89h:02m:34s remains)
INFO - root - 2017-12-05 11:37:01.203524: step 7630, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 88h:44m:27s remains)
INFO - root - 2017-12-05 11:37:10.866545: step 7640, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 87h:13m:22s remains)
INFO - root - 2017-12-05 11:37:20.185684: step 7650, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 86h:04m:07s remains)
INFO - root - 2017-12-05 11:37:29.409028: step 7660, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 85h:10m:39s remains)
INFO - root - 2017-12-05 11:37:38.733043: step 7670, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 87h:40m:56s remains)
INFO - root - 2017-12-05 11:37:48.304169: step 7680, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.009 sec/batch; 91h:01m:18s remains)
INFO - root - 2017-12-05 11:37:57.579593: step 7690, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 82h:52m:58s remains)
INFO - root - 2017-12-05 11:38:06.867530: step 7700, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 78h:47m:48s remains)
2017-12-05 11:38:07.595488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2832308 -4.28065 -4.2779021 -4.2789559 -4.2847037 -4.2917666 -4.2925935 -4.2861824 -4.2655478 -4.2365708 -4.2123542 -4.2079453 -4.2256927 -4.2548485 -4.2792454][-4.2715158 -4.2734604 -4.2694411 -4.2693195 -4.2737851 -4.2777214 -4.2758579 -4.2706728 -4.2500939 -4.2180123 -4.1897678 -4.1814637 -4.1990728 -4.2324123 -4.2662306][-4.2462149 -4.2534704 -4.2479944 -4.2460546 -4.2469764 -4.2457886 -4.2410994 -4.2400985 -4.2272453 -4.2017021 -4.1754661 -4.163496 -4.177578 -4.210351 -4.2489705][-4.2140241 -4.2232647 -4.216567 -4.2134681 -4.2101774 -4.197051 -4.1884727 -4.1904187 -4.1843991 -4.170577 -4.152091 -4.1407933 -4.1525283 -4.1855373 -4.226254][-4.1822729 -4.1945167 -4.1897082 -4.1859555 -4.1745591 -4.1448359 -4.1234975 -4.1189079 -4.11587 -4.1150494 -4.1129355 -4.114212 -4.1299877 -4.1653051 -4.2063661][-4.1534739 -4.1685672 -4.1709638 -4.16504 -4.1421003 -4.0909681 -4.0427504 -4.019897 -4.0190649 -4.0365372 -4.062283 -4.0891652 -4.1221604 -4.1643696 -4.2046089][-4.1246376 -4.1432877 -4.1556387 -4.1456919 -4.10678 -4.0275793 -3.9408991 -3.8957593 -3.9045656 -3.95089 -4.0125709 -4.0734587 -4.1304646 -4.18093 -4.2191949][-4.1081347 -4.1246223 -4.1402831 -4.1310992 -4.084259 -3.9861991 -3.8760653 -3.8233895 -3.8540454 -3.9267926 -4.0107288 -4.0864835 -4.1527042 -4.2020555 -4.2348237][-4.1124234 -4.1191077 -4.1354055 -4.1375074 -4.1075559 -4.02812 -3.9375489 -3.9067292 -3.9480529 -4.0101728 -4.0693669 -4.121613 -4.1731997 -4.21369 -4.2402163][-4.1175976 -4.125041 -4.151587 -4.1701012 -4.1667876 -4.1191339 -4.0594673 -4.0447335 -4.0700636 -4.0982637 -4.1231294 -4.1504531 -4.1866608 -4.2188549 -4.241518][-4.1251841 -4.137465 -4.1758137 -4.2052765 -4.2148561 -4.1878219 -4.1429071 -4.1305766 -4.1421556 -4.1490207 -4.1540918 -4.1719956 -4.2016788 -4.2291951 -4.2469058][-4.136867 -4.15166 -4.1930408 -4.22527 -4.2345791 -4.2178726 -4.1841106 -4.1736622 -4.1816139 -4.186039 -4.1884675 -4.2016153 -4.2223558 -4.243154 -4.2555761][-4.1459265 -4.1612883 -4.2030363 -4.2320433 -4.2368178 -4.2251635 -4.2013259 -4.1946778 -4.2034554 -4.2149267 -4.2219791 -4.2308564 -4.2427363 -4.2547359 -4.2611585][-4.1446915 -4.1598263 -4.204783 -4.2321129 -4.2326021 -4.2226143 -4.204608 -4.2032657 -4.2166829 -4.2331734 -4.2452 -4.2517719 -4.2562442 -4.2604375 -4.2624507][-4.1300478 -4.145402 -4.1934423 -4.2265711 -4.2301254 -4.2224212 -4.2101836 -4.2128239 -4.2254114 -4.2382455 -4.2498727 -4.2565408 -4.2588015 -4.2625113 -4.2658143]]...]
INFO - root - 2017-12-05 11:38:16.826913: step 7710, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 81h:59m:26s remains)
INFO - root - 2017-12-05 11:38:26.182143: step 7720, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 85h:17m:40s remains)
INFO - root - 2017-12-05 11:38:35.554464: step 7730, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 86h:33m:09s remains)
INFO - root - 2017-12-05 11:38:44.856646: step 7740, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.934 sec/batch; 84h:15m:55s remains)
INFO - root - 2017-12-05 11:38:54.030945: step 7750, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 81h:51m:50s remains)
INFO - root - 2017-12-05 11:39:03.429187: step 7760, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 86h:57m:39s remains)
INFO - root - 2017-12-05 11:39:12.707355: step 7770, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 85h:19m:08s remains)
INFO - root - 2017-12-05 11:39:21.973305: step 7780, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 85h:40m:47s remains)
INFO - root - 2017-12-05 11:39:31.106335: step 7790, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 87h:48m:59s remains)
INFO - root - 2017-12-05 11:39:40.477361: step 7800, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 90h:48m:01s remains)
2017-12-05 11:39:41.301021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2750492 -4.2607212 -4.2537618 -4.2457991 -4.2386312 -4.2391768 -4.2411427 -4.2399211 -4.2388468 -4.237927 -4.2394304 -4.25237 -4.2710056 -4.2848096 -4.2896986][-4.23923 -4.22531 -4.2250834 -4.2222304 -4.217567 -4.2203822 -4.2252131 -4.2238526 -4.2216806 -4.2195435 -4.2195811 -4.230401 -4.2497687 -4.2637706 -4.268043][-4.19776 -4.1862926 -4.197557 -4.2069554 -4.2088346 -4.2131982 -4.2143941 -4.212009 -4.2120471 -4.2108831 -4.2089224 -4.2164574 -4.2347159 -4.2484937 -4.25403][-4.1696334 -4.1595688 -4.1781378 -4.191462 -4.1884913 -4.1811428 -4.16727 -4.1600585 -4.1748958 -4.1896615 -4.1949725 -4.2055855 -4.2225008 -4.23383 -4.2417059][-4.1731634 -4.1640525 -4.1806297 -4.1886063 -4.1750822 -4.1523995 -4.1176944 -4.1063476 -4.1407008 -4.1774006 -4.1944752 -4.208765 -4.2198691 -4.226614 -4.23768][-4.1805782 -4.1731663 -4.1858616 -4.1869607 -4.1627245 -4.1239305 -4.0727577 -4.0534387 -4.1068659 -4.1645103 -4.1954546 -4.2149658 -4.2253008 -4.2304397 -4.2434573][-4.1806059 -4.1708078 -4.1782351 -4.171 -4.130096 -4.0677524 -3.986201 -3.9409165 -4.0129538 -4.1075139 -4.1626878 -4.1954141 -4.212997 -4.2250609 -4.2432427][-4.18277 -4.1771851 -4.1750097 -4.1482959 -4.0788336 -3.9832864 -3.8556092 -3.7656853 -3.8611584 -4.0035305 -4.0926967 -4.1441879 -4.1722283 -4.189805 -4.2103057][-4.2015629 -4.2026949 -4.1941671 -4.1556282 -4.0766888 -3.9711123 -3.8350625 -3.7346821 -3.8261008 -3.9781868 -4.0724263 -4.1242256 -4.1523018 -4.1676679 -4.1808305][-4.2349253 -4.2417521 -4.2354436 -4.2023273 -4.1408019 -4.0655437 -3.9762185 -3.913233 -3.9645514 -4.0737429 -4.1462078 -4.1793776 -4.197875 -4.2055321 -4.2078376][-4.2604527 -4.270751 -4.2683487 -4.2456627 -4.203793 -4.1535616 -4.1002364 -4.0600753 -4.0838752 -4.1540613 -4.2093124 -4.2331219 -4.2475977 -4.253262 -4.2532578][-4.272162 -4.2863355 -4.2896523 -4.2747455 -4.2457213 -4.2110906 -4.1748352 -4.1486096 -4.1576576 -4.2009525 -4.240006 -4.2594457 -4.2714896 -4.278419 -4.2809138][-4.2811985 -4.30116 -4.3097205 -4.2984796 -4.2742047 -4.24732 -4.2234674 -4.2102828 -4.2179623 -4.2430921 -4.2661033 -4.2779655 -4.2839117 -4.2867126 -4.2856579][-4.285449 -4.3064594 -4.3173075 -4.3085051 -4.2886028 -4.271101 -4.2602787 -4.2604823 -4.2689276 -4.2822542 -4.2926211 -4.2972555 -4.2979827 -4.2975297 -4.2932959][-4.28381 -4.3013549 -4.3133664 -4.30868 -4.2952919 -4.2863889 -4.2847347 -4.2912045 -4.2986312 -4.3047247 -4.3091011 -4.3118725 -4.313086 -4.3130774 -4.3105669]]...]
INFO - root - 2017-12-05 11:39:50.584605: step 7810, loss = 2.04, batch loss = 1.99 (8.1 examples/sec; 0.991 sec/batch; 89h:23m:38s remains)
INFO - root - 2017-12-05 11:39:59.836620: step 7820, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 85h:19m:10s remains)
INFO - root - 2017-12-05 11:40:09.023904: step 7830, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 77h:10m:17s remains)
INFO - root - 2017-12-05 11:40:18.341005: step 7840, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 82h:05m:41s remains)
INFO - root - 2017-12-05 11:40:27.705572: step 7850, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 85h:27m:38s remains)
INFO - root - 2017-12-05 11:40:36.994111: step 7860, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.912 sec/batch; 82h:13m:04s remains)
INFO - root - 2017-12-05 11:40:46.452872: step 7870, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 82h:03m:03s remains)
INFO - root - 2017-12-05 11:40:55.881188: step 7880, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 79h:20m:16s remains)
INFO - root - 2017-12-05 11:41:05.055953: step 7890, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 87h:52m:55s remains)
INFO - root - 2017-12-05 11:41:14.338268: step 7900, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 83h:11m:11s remains)
2017-12-05 11:41:15.116425: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3205462 -4.3142323 -4.3107 -4.3068304 -4.30422 -4.3019733 -4.3012786 -4.3029237 -4.3032446 -4.3044634 -4.3108535 -4.316453 -4.3182416 -4.3156109 -4.3145189][-4.3030891 -4.2948923 -4.2921066 -4.2879868 -4.2823472 -4.2765474 -4.2700195 -4.2677927 -4.2616 -4.2601981 -4.2691984 -4.2773671 -4.2793918 -4.2753367 -4.2754011][-4.2813087 -4.2706208 -4.2691817 -4.265563 -4.2565131 -4.24634 -4.2324619 -4.2211785 -4.205442 -4.2039 -4.2191663 -4.2320857 -4.2356486 -4.2320266 -4.2349644][-4.2449913 -4.228332 -4.2240982 -4.2216525 -4.2091565 -4.1933336 -4.1690674 -4.1440458 -4.1237683 -4.128551 -4.1566305 -4.1815205 -4.1951709 -4.1958733 -4.2022495][-4.1928749 -4.1671481 -4.1582441 -4.15576 -4.1388884 -4.1151195 -4.0798221 -4.0411372 -4.02547 -4.0437818 -4.08701 -4.1284819 -4.1548357 -4.1632066 -4.1737971][-4.1424465 -4.1064291 -4.0836148 -4.0765591 -4.051919 -4.0158057 -3.9630997 -3.9111097 -3.9223821 -3.9768207 -4.0378742 -4.0870585 -4.1144819 -4.1288023 -4.1475658][-4.1113148 -4.0623941 -4.0219784 -4.0013394 -3.9567783 -3.902781 -3.8237133 -3.7462647 -3.7963572 -3.9018271 -3.9847982 -4.0410237 -4.0639796 -4.0795784 -4.1110582][-4.1182809 -4.0649385 -4.0116339 -3.9771538 -3.9193916 -3.8481343 -3.735095 -3.6188204 -3.7000399 -3.8440492 -3.9446781 -4.0001612 -4.0127926 -4.0264287 -4.0666533][-4.1451116 -4.1041059 -4.0579271 -4.0223041 -3.9736557 -3.9152074 -3.8289073 -3.7455359 -3.8018959 -3.9110894 -3.9909189 -4.0261908 -4.0193968 -4.0144897 -4.0420637][-4.1537433 -4.1264534 -4.0974016 -4.0697489 -4.038517 -4.0013504 -3.9547906 -3.916594 -3.9499509 -4.01287 -4.0614715 -4.0790958 -4.0629644 -4.0442748 -4.0543504][-4.1667924 -4.1453462 -4.128861 -4.1106339 -4.0925522 -4.0740242 -4.0509706 -4.0335946 -4.0481257 -4.0807757 -4.1114888 -4.1196222 -4.1055446 -4.0888062 -4.0913815][-4.1964941 -4.179862 -4.1695995 -4.1584239 -4.1507258 -4.1448469 -4.1341257 -4.1262255 -4.1300483 -4.1432228 -4.1567373 -4.1600685 -4.1546106 -4.1457162 -4.1471086][-4.2296462 -4.2193818 -4.2116256 -4.2071686 -4.2050786 -4.2029972 -4.1990614 -4.19761 -4.199717 -4.2041984 -4.2098942 -4.2107277 -4.2085261 -4.20611 -4.2100163][-4.2577925 -4.2533374 -4.2501178 -4.2499623 -4.2508116 -4.2509546 -4.2507334 -4.2535105 -4.2575188 -4.2603431 -4.2632208 -4.2642555 -4.2640595 -4.2646265 -4.2682166][-4.2870607 -4.2839289 -4.2815547 -4.2821293 -4.2840843 -4.2856832 -4.2871752 -4.2901192 -4.2930741 -4.2954946 -4.2987452 -4.3008542 -4.3023243 -4.3043995 -4.3074765]]...]
INFO - root - 2017-12-05 11:41:24.383901: step 7910, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 82h:38m:43s remains)
INFO - root - 2017-12-05 11:41:33.876215: step 7920, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 78h:54m:37s remains)
INFO - root - 2017-12-05 11:41:43.328223: step 7930, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 87h:43m:08s remains)
INFO - root - 2017-12-05 11:41:52.335720: step 7940, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 84h:20m:04s remains)
INFO - root - 2017-12-05 11:42:01.670326: step 7950, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 86h:06m:05s remains)
INFO - root - 2017-12-05 11:42:11.053044: step 7960, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 85h:14m:38s remains)
INFO - root - 2017-12-05 11:42:20.356183: step 7970, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 83h:36m:36s remains)
INFO - root - 2017-12-05 11:42:29.708480: step 7980, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 82h:16m:44s remains)
INFO - root - 2017-12-05 11:42:38.975351: step 7990, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 87h:38m:20s remains)
INFO - root - 2017-12-05 11:42:48.243065: step 8000, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.902 sec/batch; 81h:20m:06s remains)
2017-12-05 11:42:48.987229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1974993 -4.2440906 -4.2763681 -4.2919416 -4.2986894 -4.3047442 -4.3098903 -4.3038239 -4.2793045 -4.2379045 -4.1891193 -4.1320562 -4.0841608 -4.0462427 -4.0106373][-4.14721 -4.2156057 -4.2651091 -4.2909856 -4.303082 -4.312705 -4.3177185 -4.3085203 -4.2800593 -4.2338266 -4.1796346 -4.1202393 -4.0740089 -4.0463915 -4.0228338][-4.1027308 -4.1826224 -4.2453361 -4.2799606 -4.2986422 -4.3133116 -4.3178215 -4.3058505 -4.2745638 -4.2309875 -4.18169 -4.1310205 -4.0937572 -4.0765514 -4.0693173][-4.0668635 -4.15033 -4.2205176 -4.2614765 -4.283237 -4.2953649 -4.2948375 -4.276052 -4.240952 -4.2042794 -4.1720963 -4.1445704 -4.1245222 -4.1187973 -4.1249561][-4.0371852 -4.1205626 -4.1929855 -4.2380462 -4.2587414 -4.2646813 -4.2564321 -4.2279015 -4.1872144 -4.1597862 -4.1568108 -4.1591582 -4.1552548 -4.1549673 -4.1610932][-4.0171742 -4.1003346 -4.1737504 -4.2227359 -4.2436481 -4.2403507 -4.220932 -4.1824331 -4.1361561 -4.1178975 -4.1399326 -4.1669221 -4.1764021 -4.1775784 -4.1780066][-4.0193062 -4.101243 -4.1743731 -4.224524 -4.2438836 -4.2310457 -4.204721 -4.1605587 -4.1096892 -4.0964513 -4.1355371 -4.1775246 -4.197042 -4.1966987 -4.1902165][-4.04055 -4.1168361 -4.1841984 -4.2327147 -4.2507324 -4.236445 -4.2147574 -4.176609 -4.1284857 -4.1152735 -4.157187 -4.198441 -4.2165108 -4.2131681 -4.2009659][-4.0787573 -4.140986 -4.1984196 -4.244029 -4.2644715 -4.2540259 -4.2430978 -4.2194529 -4.1801224 -4.1671472 -4.1959457 -4.2225456 -4.2342257 -4.2272825 -4.2090278][-4.1247315 -4.164537 -4.2086868 -4.2520533 -4.2739844 -4.2680645 -4.2654166 -4.25613 -4.2313 -4.2222757 -4.2392168 -4.2547908 -4.2574682 -4.2462916 -4.2200379][-4.173975 -4.1933241 -4.2228966 -4.2615123 -4.2827606 -4.2848768 -4.2893982 -4.288322 -4.2731581 -4.2683582 -4.2798924 -4.2876372 -4.2826972 -4.2660294 -4.2371097][-4.2237086 -4.2286234 -4.2470741 -4.2779026 -4.2982092 -4.3087411 -4.3191552 -4.3220773 -4.3098955 -4.302999 -4.3071146 -4.3096867 -4.30182 -4.2856116 -4.2629385][-4.2665653 -4.2665472 -4.2789707 -4.2998734 -4.3164964 -4.3286309 -4.3394794 -4.3446679 -4.3364673 -4.3289337 -4.3265486 -4.3234949 -4.3158154 -4.3039451 -4.2893472][-4.2929134 -4.291923 -4.30083 -4.3147292 -4.3260641 -4.3342762 -4.3419943 -4.3485546 -4.3446703 -4.3392463 -4.3359308 -4.3293014 -4.3217564 -4.3129592 -4.3042746][-4.3031392 -4.3007913 -4.3057623 -4.3150992 -4.3209705 -4.3239746 -4.3281302 -4.3335109 -4.3316751 -4.329175 -4.3290462 -4.3268828 -4.3253746 -4.3215055 -4.3160434]]...]
INFO - root - 2017-12-05 11:42:58.261571: step 8010, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 85h:01m:21s remains)
INFO - root - 2017-12-05 11:43:07.635777: step 8020, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 79h:13m:14s remains)
INFO - root - 2017-12-05 11:43:17.103856: step 8030, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 84h:41m:59s remains)
INFO - root - 2017-12-05 11:43:26.566067: step 8040, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 86h:31m:32s remains)
INFO - root - 2017-12-05 11:43:35.926068: step 8050, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 86h:01m:06s remains)
INFO - root - 2017-12-05 11:43:45.387724: step 8060, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 87h:37m:36s remains)
INFO - root - 2017-12-05 11:43:54.652610: step 8070, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 84h:09m:31s remains)
INFO - root - 2017-12-05 11:44:03.846319: step 8080, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.825 sec/batch; 74h:21m:59s remains)
INFO - root - 2017-12-05 11:44:13.089966: step 8090, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 82h:38m:05s remains)
INFO - root - 2017-12-05 11:44:22.502763: step 8100, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 86h:38m:21s remains)
2017-12-05 11:44:23.239612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2234349 -4.2306991 -4.2423172 -4.2441354 -4.2427173 -4.2466512 -4.244545 -4.23395 -4.2220583 -4.1953964 -4.1635227 -4.1469154 -4.166141 -4.2020845 -4.2162232][-4.2408628 -4.2435737 -4.2465482 -4.2405567 -4.2302513 -4.2310276 -4.23895 -4.2385869 -4.2305779 -4.206985 -4.17143 -4.1471004 -4.1652555 -4.2004771 -4.2172728][-4.2578993 -4.2572989 -4.2484317 -4.23073 -4.211266 -4.2030396 -4.2175503 -4.229198 -4.2324843 -4.2176914 -4.1838293 -4.1612587 -4.1795473 -4.2099371 -4.2239408][-4.2670627 -4.2636395 -4.2480788 -4.2208796 -4.1917605 -4.1802797 -4.1984673 -4.2185664 -4.2294884 -4.2218924 -4.1948967 -4.1794415 -4.1987967 -4.2235465 -4.2299423][-4.2663198 -4.2586112 -4.237308 -4.2049608 -4.1678357 -4.1517792 -4.1706414 -4.1966982 -4.212863 -4.2138762 -4.1986351 -4.19437 -4.2125764 -4.2272754 -4.2248325][-4.2570772 -4.2481585 -4.22529 -4.1915393 -4.1439385 -4.1189604 -4.139389 -4.1706357 -4.191102 -4.196579 -4.1915274 -4.1961064 -4.2136521 -4.2253637 -4.2207274][-4.2499037 -4.2416725 -4.2182193 -4.18245 -4.1222506 -4.085629 -4.111917 -4.1519556 -4.1738424 -4.1745849 -4.1716876 -4.1795621 -4.1970687 -4.2138133 -4.2165747][-4.239202 -4.2328205 -4.208693 -4.169857 -4.1017685 -4.0598536 -4.0952044 -4.1508517 -4.1781707 -4.1710858 -4.1596432 -4.1631775 -4.1799669 -4.1950817 -4.2070413][-4.2216268 -4.2177267 -4.1968956 -4.1558595 -4.0870762 -4.0482321 -4.0897403 -4.1574068 -4.1928182 -4.1872673 -4.1678758 -4.1561685 -4.1597342 -4.1698928 -4.1846347][-4.22019 -4.2191877 -4.1997685 -4.1559887 -4.0872641 -4.0497208 -4.0865846 -4.1542888 -4.1953158 -4.1991973 -4.1818061 -4.1542816 -4.1349707 -4.1335878 -4.1511264][-4.2278166 -4.2312145 -4.2129426 -4.1695294 -4.099123 -4.0558925 -4.08118 -4.1446409 -4.1922817 -4.2126684 -4.2057371 -4.17419 -4.1319337 -4.1114378 -4.1248379][-4.227788 -4.242238 -4.231967 -4.1937189 -4.1276178 -4.076561 -4.0876379 -4.1421394 -4.1936731 -4.2278662 -4.233623 -4.20861 -4.1548629 -4.115438 -4.1138353][-4.2135839 -4.238832 -4.2445455 -4.2229 -4.1650424 -4.1062613 -4.0972586 -4.1369891 -4.1829338 -4.220984 -4.2399554 -4.2270961 -4.1768951 -4.1267328 -4.1091681][-4.1919045 -4.2274232 -4.2475867 -4.2450018 -4.2029471 -4.1415668 -4.1082683 -4.1271839 -4.163733 -4.2011933 -4.2304988 -4.2289143 -4.184866 -4.1276722 -4.0983977][-4.1633453 -4.2088614 -4.2463593 -4.2604327 -4.2396731 -4.1857853 -4.1408029 -4.1405253 -4.1676054 -4.2006044 -4.2264228 -4.2288375 -4.1906133 -4.1309609 -4.0861416]]...]
INFO - root - 2017-12-05 11:44:32.731925: step 8110, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.894 sec/batch; 80h:35m:58s remains)
INFO - root - 2017-12-05 11:44:42.061972: step 8120, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 79h:22m:15s remains)
INFO - root - 2017-12-05 11:44:51.409737: step 8130, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 84h:23m:11s remains)
INFO - root - 2017-12-05 11:45:00.816239: step 8140, loss = 2.02, batch loss = 1.97 (8.3 examples/sec; 0.962 sec/batch; 86h:41m:12s remains)
INFO - root - 2017-12-05 11:45:10.385087: step 8150, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.015 sec/batch; 91h:27m:21s remains)
INFO - root - 2017-12-05 11:45:19.721915: step 8160, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 87h:46m:05s remains)
INFO - root - 2017-12-05 11:45:28.975599: step 8170, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 87h:26m:39s remains)
INFO - root - 2017-12-05 11:45:38.297361: step 8180, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 87h:22m:28s remains)
INFO - root - 2017-12-05 11:45:47.816144: step 8190, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 87h:52m:00s remains)
INFO - root - 2017-12-05 11:45:57.158535: step 8200, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 86h:55m:21s remains)
2017-12-05 11:45:57.951770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3111949 -4.2687535 -4.208189 -4.14632 -4.1099138 -4.1061544 -4.1320291 -4.1636825 -4.1779542 -4.1847253 -4.2026196 -4.2292795 -4.2627845 -4.2956553 -4.3187327][-4.2869697 -4.2456589 -4.197041 -4.1506696 -4.1223907 -4.1152692 -4.132514 -4.1564217 -4.162559 -4.1649957 -4.1793528 -4.203166 -4.2393379 -4.2779703 -4.3099885][-4.2516036 -4.2189412 -4.1910319 -4.1626997 -4.1403027 -4.12706 -4.1301217 -4.1405025 -4.1377287 -4.1412392 -4.156497 -4.1789021 -4.21733 -4.2606096 -4.2982802][-4.2189941 -4.1974883 -4.1894584 -4.1777487 -4.1593962 -4.133647 -4.119566 -4.1134925 -4.0968795 -4.0995903 -4.11704 -4.1428747 -4.1913819 -4.2445474 -4.2892323][-4.1884055 -4.1689062 -4.1716137 -4.1763206 -4.16905 -4.1408434 -4.1109276 -4.086298 -4.0557213 -4.0497193 -4.0635147 -4.0964894 -4.1606803 -4.2308707 -4.285605][-4.1395278 -4.1137071 -4.1151967 -4.1348672 -4.1497846 -4.1457634 -4.1217422 -4.0892019 -4.0463624 -4.0233779 -4.0294876 -4.0649176 -4.139255 -4.2187791 -4.2798305][-4.1001482 -4.0741625 -4.0684576 -4.0898705 -4.1236386 -4.1465383 -4.1398444 -4.1033287 -4.042501 -4.0018973 -4.0006871 -4.0324969 -4.1062489 -4.1909552 -4.2605062][-4.0988245 -4.0726304 -4.0609012 -4.0730233 -4.1124187 -4.15244 -4.1498685 -4.1016254 -4.0224118 -3.9681034 -3.9668593 -4.0055881 -4.0745 -4.1605072 -4.2376294][-4.1231203 -4.0965114 -4.0749431 -4.0708036 -4.1049423 -4.1511192 -4.1529474 -4.102272 -4.0105863 -3.9498498 -3.9545629 -4.0042338 -4.0686922 -4.151413 -4.2257357][-4.1657481 -4.1494961 -4.1235538 -4.1060824 -4.1238866 -4.1615748 -4.1725469 -4.1323409 -4.053225 -3.9968216 -3.9958253 -4.041925 -4.0939779 -4.1610837 -4.22367][-4.2241335 -4.2194281 -4.198195 -4.1777334 -4.1769824 -4.1947641 -4.2081151 -4.1912651 -4.1440959 -4.098206 -4.0819178 -4.1098919 -4.1443324 -4.1880088 -4.2338562][-4.2728672 -4.2760143 -4.2649088 -4.2507477 -4.2433233 -4.2465196 -4.2542624 -4.254333 -4.2315526 -4.1935983 -4.1666512 -4.173842 -4.1859055 -4.2130904 -4.2479625][-4.2949209 -4.3035159 -4.2996464 -4.290051 -4.2829509 -4.2846122 -4.2862272 -4.2893829 -4.2750835 -4.2442365 -4.21426 -4.2095661 -4.2135315 -4.2335238 -4.2637177][-4.2930894 -4.3027592 -4.3039718 -4.2977414 -4.291779 -4.2941489 -4.2920952 -4.2943864 -4.2855725 -4.2624073 -4.2371526 -4.2316356 -4.2383695 -4.2540054 -4.2786756][-4.29333 -4.29862 -4.3021502 -4.29984 -4.2957621 -4.2961588 -4.2939243 -4.2958288 -4.2922115 -4.2782168 -4.2627511 -4.25924 -4.2655587 -4.2766795 -4.2951522]]...]
INFO - root - 2017-12-05 11:46:07.203157: step 8210, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 83h:55m:29s remains)
INFO - root - 2017-12-05 11:46:16.550769: step 8220, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.958 sec/batch; 86h:19m:11s remains)
INFO - root - 2017-12-05 11:46:25.848902: step 8230, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 82h:12m:28s remains)
INFO - root - 2017-12-05 11:46:35.335746: step 8240, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 86h:35m:34s remains)
INFO - root - 2017-12-05 11:46:44.734274: step 8250, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 81h:39m:03s remains)
INFO - root - 2017-12-05 11:46:54.076181: step 8260, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 85h:31m:18s remains)
INFO - root - 2017-12-05 11:47:03.291017: step 8270, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 82h:29m:37s remains)
INFO - root - 2017-12-05 11:47:12.781926: step 8280, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 83h:17m:06s remains)
INFO - root - 2017-12-05 11:47:22.132052: step 8290, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 80h:52m:33s remains)
INFO - root - 2017-12-05 11:47:31.497406: step 8300, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 82h:02m:31s remains)
2017-12-05 11:47:32.301402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2422228 -4.2566948 -4.2664819 -4.2693114 -4.266665 -4.2560487 -4.2431865 -4.236589 -4.2344761 -4.2373 -4.2468071 -4.250186 -4.2383442 -4.2230773 -4.2180362][-4.2380924 -4.2541585 -4.26835 -4.2741389 -4.2752318 -4.2701635 -4.2593656 -4.2453346 -4.2315292 -4.2286835 -4.2393584 -4.2470284 -4.2417393 -4.2303634 -4.2243266][-4.2364955 -4.2467184 -4.255568 -4.2582512 -4.261189 -4.2617941 -4.254889 -4.2359371 -4.2152147 -4.2108126 -4.2247305 -4.2368717 -4.2377892 -4.2314143 -4.2258334][-4.2403283 -4.24201 -4.2415609 -4.23689 -4.2376437 -4.2388778 -4.2316256 -4.21183 -4.1921377 -4.1930604 -4.2136259 -4.2317715 -4.2408519 -4.2411771 -4.2365131][-4.2433963 -4.2404604 -4.2337475 -4.2224069 -4.21464 -4.20559 -4.1885023 -4.1651063 -4.1491127 -4.161706 -4.1953368 -4.22378 -4.242743 -4.2521811 -4.2493033][-4.2490697 -4.2436943 -4.2353058 -4.2219276 -4.2049274 -4.1782241 -4.1396294 -4.0994258 -4.081862 -4.1112542 -4.1667142 -4.2071543 -4.2351336 -4.2533174 -4.2532525][-4.2520146 -4.2471089 -4.2412758 -4.2314034 -4.2061687 -4.1609659 -4.0976062 -4.0312014 -4.0054045 -4.0510864 -4.1257372 -4.1764908 -4.2107849 -4.2358027 -4.237215][-4.2500839 -4.2456865 -4.2433748 -4.2387514 -4.2102866 -4.154398 -4.0753212 -3.99266 -3.9621859 -4.0157642 -4.0929956 -4.1442747 -4.18112 -4.2095342 -4.2156167][-4.2727408 -4.2689204 -4.2684231 -4.2680178 -4.2438025 -4.1912889 -4.1142211 -4.0356331 -4.0134625 -4.0587459 -4.1196156 -4.158926 -4.1884656 -4.2132096 -4.2222519][-4.29574 -4.2920723 -4.2926087 -4.29629 -4.2822714 -4.2423139 -4.1784945 -4.1171761 -4.1075029 -4.1404243 -4.1779103 -4.1997409 -4.2182169 -4.2366219 -4.2454505][-4.3096409 -4.3049974 -4.3073897 -4.31377 -4.3084049 -4.2802896 -4.2301745 -4.1857104 -4.1852336 -4.2081318 -4.2275853 -4.2363009 -4.244523 -4.2555828 -4.2619119][-4.3176632 -4.3127251 -4.3147206 -4.3175716 -4.3097486 -4.2837243 -4.2459326 -4.2208567 -4.2291145 -4.2497978 -4.2638988 -4.2661362 -4.26659 -4.2707634 -4.2759981][-4.31213 -4.3110576 -4.313282 -4.311048 -4.296185 -4.2673383 -4.2357373 -4.2230711 -4.2376833 -4.2608852 -4.2780252 -4.2825208 -4.2821426 -4.2844658 -4.2907181][-4.3060956 -4.31006 -4.3135428 -4.3082476 -4.2904625 -4.2600384 -4.230824 -4.2237854 -4.2382007 -4.2600427 -4.2802382 -4.290576 -4.2940049 -4.2964454 -4.3014021][-4.3010063 -4.31032 -4.3165503 -4.3111525 -4.2940812 -4.2640538 -4.236259 -4.2303605 -4.2401648 -4.2575369 -4.2786226 -4.293221 -4.29804 -4.2983346 -4.2989073]]...]
INFO - root - 2017-12-05 11:47:41.487071: step 8310, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 82h:33m:01s remains)
INFO - root - 2017-12-05 11:47:50.927811: step 8320, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 84h:41m:58s remains)
INFO - root - 2017-12-05 11:48:00.282707: step 8330, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 76h:30m:58s remains)
INFO - root - 2017-12-05 11:48:09.517988: step 8340, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 85h:05m:07s remains)
INFO - root - 2017-12-05 11:48:18.757983: step 8350, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 87h:32m:42s remains)
INFO - root - 2017-12-05 11:48:28.201227: step 8360, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 85h:05m:18s remains)
INFO - root - 2017-12-05 11:48:37.600206: step 8370, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 86h:32m:30s remains)
INFO - root - 2017-12-05 11:48:46.939142: step 8380, loss = 2.06, batch loss = 2.01 (7.9 examples/sec; 1.010 sec/batch; 90h:53m:44s remains)
INFO - root - 2017-12-05 11:48:56.417899: step 8390, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 83h:17m:07s remains)
INFO - root - 2017-12-05 11:49:05.644349: step 8400, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 88h:13m:31s remains)
2017-12-05 11:49:06.392826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1767521 -4.1768961 -4.1869783 -4.1979384 -4.2068954 -4.2093878 -4.2029071 -4.1807365 -4.1710043 -4.1618271 -4.148396 -4.1391692 -4.1185966 -4.1182942 -4.1443319][-4.1616354 -4.164535 -4.175983 -4.1899157 -4.204021 -4.205749 -4.1969409 -4.1828542 -4.1865792 -4.1898732 -4.1838307 -4.1690788 -4.1399045 -4.1353803 -4.1528115][-4.1648183 -4.1649981 -4.1745181 -4.1925473 -4.2061453 -4.2037272 -4.1907058 -4.1756039 -4.1831131 -4.1886716 -4.181304 -4.1602511 -4.136713 -4.1404819 -4.1595845][-4.1857295 -4.1802573 -4.1879258 -4.2090917 -4.2176471 -4.2111511 -4.1983132 -4.180778 -4.1808095 -4.1813345 -4.1677694 -4.1442461 -4.1306109 -4.1441464 -4.1694016][-4.197258 -4.1904254 -4.1996088 -4.2235966 -4.2264042 -4.2133508 -4.198246 -4.174715 -4.1710825 -4.1729174 -4.1614537 -4.1419806 -4.13967 -4.1596074 -4.185822][-4.1773086 -4.1696253 -4.1776977 -4.1973605 -4.1911211 -4.1732025 -4.1508684 -4.1165061 -4.1161966 -4.13601 -4.1377249 -4.1378441 -4.1535091 -4.1772079 -4.2042103][-4.1337619 -4.1198092 -4.1230268 -4.1354284 -4.1177726 -4.0888748 -4.0518179 -3.9902256 -3.990483 -4.0441484 -4.0818162 -4.1149578 -4.1486731 -4.1751227 -4.2058597][-4.1410508 -4.1187158 -4.1145225 -4.1163397 -4.085393 -4.04362 -3.9853978 -3.8766143 -3.8505235 -3.9234977 -3.9979403 -4.0636568 -4.114222 -4.1430955 -4.17893][-4.1859736 -4.1691866 -4.1705942 -4.1790624 -4.156486 -4.12994 -4.0837383 -3.9714577 -3.9137378 -3.9559355 -4.015871 -4.0704226 -4.1090789 -4.1294804 -4.1647062][-4.1997614 -4.1928334 -4.2050333 -4.2229862 -4.2184563 -4.2098947 -4.1859703 -4.1004915 -4.0503879 -4.0794086 -4.1149745 -4.1378856 -4.1474109 -4.1543822 -4.1816216][-4.197876 -4.1946654 -4.2115088 -4.2320185 -4.2335424 -4.2292948 -4.21023 -4.1440158 -4.1075273 -4.1362848 -4.1629333 -4.1726871 -4.1718249 -4.1739926 -4.1974916][-4.1952486 -4.1949987 -4.2102232 -4.2288017 -4.2291188 -4.2250428 -4.2070041 -4.1504688 -4.1203628 -4.141643 -4.1613913 -4.1710396 -4.1694455 -4.17358 -4.1961989][-4.1920919 -4.1947913 -4.2078576 -4.2270203 -4.2319374 -4.2293277 -4.215456 -4.1671739 -4.1397572 -4.1486278 -4.1568203 -4.1627641 -4.1634722 -4.1691642 -4.1945066][-4.1849055 -4.1893325 -4.2028885 -4.224679 -4.2316027 -4.2288857 -4.2225461 -4.1909695 -4.1692371 -4.167882 -4.1685562 -4.1746821 -4.1766825 -4.178956 -4.2019553][-4.1736321 -4.176435 -4.18939 -4.2078066 -4.2091484 -4.2020926 -4.2003922 -4.1842871 -4.1710267 -4.1686687 -4.1728172 -4.1833615 -4.1869836 -4.1912436 -4.2154493]]...]
INFO - root - 2017-12-05 11:49:15.728784: step 8410, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 83h:43m:48s remains)
INFO - root - 2017-12-05 11:49:25.129926: step 8420, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.927 sec/batch; 83h:24m:52s remains)
INFO - root - 2017-12-05 11:49:34.291291: step 8430, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 77h:16m:52s remains)
INFO - root - 2017-12-05 11:49:43.415579: step 8440, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 81h:09m:30s remains)
INFO - root - 2017-12-05 11:49:52.860702: step 8450, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.946 sec/batch; 85h:10m:33s remains)
INFO - root - 2017-12-05 11:50:02.116993: step 8460, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 85h:24m:25s remains)
INFO - root - 2017-12-05 11:50:11.462298: step 8470, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 84h:18m:57s remains)
INFO - root - 2017-12-05 11:50:20.834606: step 8480, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 84h:24m:27s remains)
INFO - root - 2017-12-05 11:50:30.389187: step 8490, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 82h:32m:54s remains)
INFO - root - 2017-12-05 11:50:39.712229: step 8500, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 82h:17m:53s remains)
2017-12-05 11:50:40.450025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2059135 -4.2013578 -4.2041087 -4.1925311 -4.1768832 -4.1736975 -4.179781 -4.2035804 -4.2335892 -4.2504935 -4.2555866 -4.2530794 -4.2406321 -4.2158122 -4.1849518][-4.1868362 -4.1843762 -4.1909 -4.1851158 -4.1784577 -4.17785 -4.18331 -4.2018805 -4.2253056 -4.2326241 -4.2266855 -4.21684 -4.2002597 -4.1791973 -4.1568375][-4.1842794 -4.1796632 -4.1814289 -4.1731315 -4.1683331 -4.1687775 -4.1721883 -4.189002 -4.2135878 -4.2190781 -4.20602 -4.1858153 -4.1630859 -4.1439638 -4.1315074][-4.192874 -4.182301 -4.1729765 -4.1521416 -4.1314764 -4.1198311 -4.1205397 -4.1438317 -4.17536 -4.1881576 -4.1774306 -4.15512 -4.1313696 -4.1135015 -4.11066][-4.1998191 -4.1794181 -4.1537519 -4.1135588 -4.0701561 -4.040596 -4.0392017 -4.0712142 -4.1168776 -4.144486 -4.1427326 -4.1254773 -4.1052194 -4.0900488 -4.0929995][-4.2008014 -4.1679406 -4.124239 -4.0644917 -3.9984462 -3.9523718 -3.9448559 -3.9829111 -4.0457211 -4.0937066 -4.1087837 -4.1034565 -4.0936732 -4.0867219 -4.0923738][-4.198195 -4.1513305 -4.0897026 -4.0163918 -3.9423816 -3.8940377 -3.8858957 -3.9254084 -4.0013943 -4.0666385 -4.0945482 -4.0973153 -4.0960736 -4.0965714 -4.1023188][-4.1931667 -4.134192 -4.0637164 -3.9907386 -3.9243042 -3.8908579 -3.894474 -3.9373765 -4.0167842 -4.0875816 -4.1155148 -4.1152678 -4.1127839 -4.1164889 -4.119082][-4.2037511 -4.142869 -4.07911 -4.0200577 -3.9735518 -3.961834 -3.9766695 -4.0177526 -4.0881572 -4.1459074 -4.1606865 -4.1472044 -4.1368151 -4.1369867 -4.1363869][-4.2207351 -4.1678677 -4.1190119 -4.0779171 -4.0532317 -4.0579305 -4.0710912 -4.1016011 -4.1554832 -4.1958055 -4.1884394 -4.1551576 -4.1322188 -4.1249089 -4.1244874][-4.2381 -4.199554 -4.1663103 -4.1384077 -4.1256371 -4.1342583 -4.1429296 -4.161346 -4.1966958 -4.214941 -4.1822319 -4.1288805 -4.0957842 -4.088007 -4.0960541][-4.2509403 -4.2242732 -4.2000093 -4.1763592 -4.1647949 -4.1679735 -4.1701136 -4.1793408 -4.1951852 -4.1921124 -4.143115 -4.0845737 -4.05594 -4.0603619 -4.0837884][-4.2621312 -4.2409296 -4.215281 -4.1862092 -4.1671643 -4.1590576 -4.1552243 -4.1599607 -4.1626325 -4.1450572 -4.0937281 -4.0429068 -4.0289278 -4.0512819 -4.0869064][-4.2744784 -4.2571025 -4.22776 -4.1899886 -4.1590023 -4.1381063 -4.1249433 -4.1287909 -4.1308451 -4.11209 -4.068634 -4.0314493 -4.034987 -4.0744381 -4.1160293][-4.280715 -4.2691441 -4.2411275 -4.1967874 -4.1528106 -4.1188464 -4.1006 -4.1108489 -4.1225991 -4.1095529 -4.0744362 -4.0502367 -4.0691233 -4.1190748 -4.1634445]]...]
INFO - root - 2017-12-05 11:50:49.795361: step 8510, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.919 sec/batch; 82h:44m:53s remains)
INFO - root - 2017-12-05 11:50:59.037166: step 8520, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 86h:56m:04s remains)
INFO - root - 2017-12-05 11:51:08.275063: step 8530, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 82h:08m:27s remains)
INFO - root - 2017-12-05 11:51:17.641830: step 8540, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 82h:55m:57s remains)
INFO - root - 2017-12-05 11:51:27.061651: step 8550, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 81h:39m:45s remains)
INFO - root - 2017-12-05 11:51:36.351182: step 8560, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 80h:14m:36s remains)
INFO - root - 2017-12-05 11:51:45.496660: step 8570, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 84h:53m:12s remains)
INFO - root - 2017-12-05 11:51:54.843824: step 8580, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 89h:13m:39s remains)
INFO - root - 2017-12-05 11:52:04.096636: step 8590, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 80h:56m:58s remains)
INFO - root - 2017-12-05 11:52:13.636978: step 8600, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 83h:18m:20s remains)
2017-12-05 11:52:14.456240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3022828 -4.3031855 -4.3021493 -4.3016977 -4.301384 -4.3010774 -4.3018365 -4.3012962 -4.2993283 -4.2957516 -4.2906132 -4.2854114 -4.2839217 -4.2842607 -4.2861609][-4.2926693 -4.2914748 -4.29032 -4.2888966 -4.2856727 -4.2824693 -4.2811813 -4.2798285 -4.2795315 -4.2787018 -4.27241 -4.265666 -4.2634182 -4.2652841 -4.2704387][-4.2750745 -4.2718692 -4.271347 -4.2686329 -4.26108 -4.25458 -4.2501678 -4.2483 -4.2527518 -4.2558846 -4.2494826 -4.238975 -4.2324119 -4.2347546 -4.2412324][-4.2509727 -4.2467942 -4.2452688 -4.24108 -4.232264 -4.2253447 -4.2178593 -4.2133245 -4.2209682 -4.2287064 -4.2239876 -4.209384 -4.1981683 -4.1976619 -4.2034011][-4.2185855 -4.2103019 -4.2053981 -4.1954312 -4.1820006 -4.1752405 -4.1669412 -4.1648664 -4.1820059 -4.1944704 -4.1879277 -4.1707935 -4.1550436 -4.1498756 -4.1558967][-4.1648903 -4.1462064 -4.1338711 -4.1140308 -4.0921617 -4.0829892 -4.0717773 -4.0754147 -4.1133952 -4.1405334 -4.1447988 -4.1335168 -4.1123996 -4.0922251 -4.0888247][-4.090127 -4.0535955 -4.0260954 -3.9881487 -3.9534252 -3.9367011 -3.9121518 -3.9140751 -3.9748697 -4.0244908 -4.0563197 -4.0679927 -4.0414281 -4.0041165 -3.9880416][-4.0380597 -3.9840269 -3.9393122 -3.8856127 -3.8379641 -3.8059864 -3.7595425 -3.7448378 -3.8188434 -3.8954554 -3.9620855 -3.9978142 -3.9704871 -3.9375367 -3.9223435][-4.0742702 -4.0241632 -3.9811842 -3.9288006 -3.88367 -3.8504543 -3.8014474 -3.774446 -3.8295565 -3.8973789 -3.9705651 -4.0116191 -3.9853954 -3.9641933 -3.9587483][-4.1579041 -4.1260352 -4.0949268 -4.0543041 -4.0214324 -4.0001483 -3.9701309 -3.9465983 -3.9731255 -4.0113497 -4.0668607 -4.1012373 -4.0810308 -4.0676861 -4.0674348][-4.2221661 -4.2022481 -4.1792703 -4.1503572 -4.131865 -4.1253586 -4.117558 -4.1023231 -4.1127439 -4.1292949 -4.1626344 -4.1848598 -4.1686759 -4.1608067 -4.1615896][-4.2589965 -4.248569 -4.2312493 -4.2084022 -4.1994195 -4.20529 -4.2133737 -4.2117105 -4.2175531 -4.2228227 -4.2383957 -4.2443204 -4.2267442 -4.2185555 -4.2182732][-4.2835069 -4.27624 -4.2619319 -4.2463422 -4.245265 -4.2573853 -4.2728896 -4.2789154 -4.2800989 -4.2783952 -4.2817116 -4.2780714 -4.2605233 -4.2465248 -4.2387834][-4.2943926 -4.2895837 -4.2783046 -4.2676911 -4.2703986 -4.2858448 -4.3023467 -4.3080049 -4.3046651 -4.299695 -4.2977419 -4.2887115 -4.2671852 -4.2467833 -4.2304749][-4.2985482 -4.2940097 -4.2846661 -4.277102 -4.2808156 -4.2931509 -4.3053465 -4.3089008 -4.3032675 -4.2959909 -4.2909031 -4.2791624 -4.2582054 -4.2365513 -4.2159281]]...]
INFO - root - 2017-12-05 11:52:23.649247: step 8610, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 85h:35m:18s remains)
INFO - root - 2017-12-05 11:52:32.970840: step 8620, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 83h:39m:54s remains)
INFO - root - 2017-12-05 11:52:42.250702: step 8630, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.980 sec/batch; 88h:07m:23s remains)
INFO - root - 2017-12-05 11:52:51.525165: step 8640, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.978 sec/batch; 87h:56m:54s remains)
INFO - root - 2017-12-05 11:53:01.045736: step 8650, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 83h:15m:59s remains)
INFO - root - 2017-12-05 11:53:10.238748: step 8660, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 83h:06m:20s remains)
INFO - root - 2017-12-05 11:53:19.603157: step 8670, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 84h:40m:51s remains)
INFO - root - 2017-12-05 11:53:28.987637: step 8680, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 81h:25m:37s remains)
INFO - root - 2017-12-05 11:53:38.456414: step 8690, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.936 sec/batch; 84h:09m:29s remains)
INFO - root - 2017-12-05 11:53:47.695085: step 8700, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.953 sec/batch; 85h:45m:00s remains)
2017-12-05 11:53:48.428630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2460818 -4.24705 -4.2488732 -4.2491708 -4.2469168 -4.2447281 -4.2452092 -4.2465963 -4.2478747 -4.2496972 -4.2518382 -4.2531824 -4.2535977 -4.2542315 -4.2542744][-4.2325721 -4.2341762 -4.2351966 -4.2344384 -4.2306547 -4.2283626 -4.2300086 -4.2324252 -4.2355189 -4.2400103 -4.2427721 -4.2441015 -4.246377 -4.2489409 -4.2497873][-4.21525 -4.2167349 -4.2166157 -4.2130527 -4.2062607 -4.2044377 -4.2075472 -4.2121925 -4.2176232 -4.2241058 -4.2265916 -4.2287893 -4.2341471 -4.2412367 -4.2452192][-4.1964946 -4.19573 -4.1923575 -4.1834159 -4.1728 -4.1709747 -4.1734271 -4.1779766 -4.187098 -4.1980686 -4.2029562 -4.2085438 -4.2197871 -4.2316937 -4.2383413][-4.1816463 -4.1778731 -4.170063 -4.1527448 -4.1337061 -4.1248288 -4.1184344 -4.1185026 -4.1331558 -4.1536584 -4.1688132 -4.1851435 -4.206141 -4.2244916 -4.2347479][-4.1739383 -4.1673021 -4.155292 -4.1296844 -4.0993857 -4.073833 -4.0490541 -4.0432439 -4.0635219 -4.0917192 -4.1173244 -4.1474442 -4.1793451 -4.2060761 -4.222012][-4.1760697 -4.1616297 -4.1412206 -4.108634 -4.0678468 -4.0177412 -3.9640322 -3.9516146 -3.9820623 -4.0180855 -4.0561466 -4.0975494 -4.1369438 -4.1719851 -4.1978884][-4.1813393 -4.1596713 -4.1292219 -4.0923791 -4.0462933 -3.9724858 -3.8876824 -3.8660696 -3.9049346 -3.9524469 -4.0031004 -4.0516162 -4.0931816 -4.1331873 -4.1692386][-4.1964688 -4.1699033 -4.13618 -4.1014423 -4.0606375 -3.9923408 -3.9111316 -3.8837714 -3.91103 -3.9492803 -3.9970417 -4.0404029 -4.0754805 -4.1126103 -4.1514344][-4.2109451 -4.188684 -4.1636763 -4.1409378 -4.112421 -4.0655189 -4.0127311 -3.9925065 -3.9969449 -4.00931 -4.0383978 -4.0645957 -4.0830793 -4.1098452 -4.1458254][-4.2274508 -4.2154093 -4.203033 -4.1897078 -4.1677861 -4.1375241 -4.1079636 -4.0961041 -4.0849781 -4.0734615 -4.0751386 -4.0774403 -4.0823917 -4.1014061 -4.1343856][-4.2429891 -4.2376604 -4.2338514 -4.2225413 -4.2008066 -4.1767917 -4.1618843 -4.1530647 -4.1344409 -4.1119041 -4.0937982 -4.0801044 -4.0821047 -4.1007643 -4.1297355][-4.2488379 -4.2468204 -4.2437243 -4.2313094 -4.2092071 -4.1852942 -4.1709738 -4.160429 -4.1432147 -4.1244135 -4.1067119 -4.0923924 -4.0904465 -4.1053972 -4.1276679][-4.2367764 -4.2339077 -4.229073 -4.213047 -4.1913524 -4.17076 -4.1549978 -4.1386447 -4.1247797 -4.1140785 -4.1051369 -4.0974207 -4.0926747 -4.0975084 -4.1104822][-4.2075243 -4.2024822 -4.1962509 -4.1803918 -4.1627941 -4.1491084 -4.1362362 -4.1202435 -4.1035514 -4.0921135 -4.0897422 -4.0899506 -4.0925751 -4.0987806 -4.1000123]]...]
INFO - root - 2017-12-05 11:53:57.716615: step 8710, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 86h:00m:53s remains)
INFO - root - 2017-12-05 11:54:07.057100: step 8720, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 82h:43m:47s remains)
INFO - root - 2017-12-05 11:54:16.475663: step 8730, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 77h:39m:30s remains)
INFO - root - 2017-12-05 11:54:25.872960: step 8740, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 80h:08m:25s remains)
INFO - root - 2017-12-05 11:54:35.210900: step 8750, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 83h:40m:20s remains)
INFO - root - 2017-12-05 11:54:44.720197: step 8760, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 85h:34m:54s remains)
INFO - root - 2017-12-05 11:54:53.985724: step 8770, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 79h:40m:08s remains)
INFO - root - 2017-12-05 11:55:03.530787: step 8780, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 86h:35m:34s remains)
INFO - root - 2017-12-05 11:55:12.886665: step 8790, loss = 2.02, batch loss = 1.97 (8.4 examples/sec; 0.953 sec/batch; 85h:39m:21s remains)
INFO - root - 2017-12-05 11:55:22.220594: step 8800, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 83h:19m:41s remains)
2017-12-05 11:55:22.972338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3304796 -4.3323359 -4.3362589 -4.3410249 -4.34176 -4.3394332 -4.3366556 -4.3335762 -4.3316913 -4.3321896 -4.333529 -4.3338265 -4.335288 -4.3379164 -4.3412318][-4.3083296 -4.3069134 -4.3114243 -4.3179507 -4.3207922 -4.3195467 -4.3188934 -4.3167624 -4.3146753 -4.3164554 -4.3179426 -4.3176489 -4.3188238 -4.32336 -4.3279405][-4.2841477 -4.275322 -4.2782068 -4.2825732 -4.2849774 -4.2863517 -4.2913709 -4.2962914 -4.2985268 -4.3033433 -4.3040938 -4.2998009 -4.2988739 -4.3025594 -4.3084321][-4.2418957 -4.2276497 -4.2295833 -4.2319064 -4.2324829 -4.2310729 -4.2371616 -4.2502632 -4.2613845 -4.2714143 -4.2739782 -4.2667546 -4.2620292 -4.2641139 -4.2703238][-4.1821766 -4.1628203 -4.1629505 -4.1621809 -4.1598577 -4.1546211 -4.1577835 -4.1765795 -4.198956 -4.2163663 -4.2227325 -4.2159266 -4.2068596 -4.2057457 -4.2120156][-4.131321 -4.1103373 -4.1025143 -4.0929971 -4.0857205 -4.0709863 -4.0599079 -4.0748768 -4.1076846 -4.1355753 -4.1537066 -4.1574154 -4.1519003 -4.1503587 -4.1574984][-4.1070976 -4.0920868 -4.0742946 -4.0502353 -4.0289259 -3.9988642 -3.9676924 -3.9682944 -4.003963 -4.0456042 -4.0903449 -4.1189861 -4.1267157 -4.1281033 -4.1374512][-4.1240759 -4.11374 -4.0866137 -4.0498896 -4.01452 -3.9780369 -3.9422407 -3.9337039 -3.9585819 -3.9980156 -4.0621052 -4.1131907 -4.1315236 -4.1325979 -4.1399155][-4.1662312 -4.1661158 -4.1408687 -4.1034923 -4.0627913 -4.0285072 -4.0009389 -3.9936032 -4.0015073 -4.021275 -4.0844593 -4.1417179 -4.1612587 -4.161109 -4.162015][-4.1970325 -4.2141109 -4.2064319 -4.1817312 -4.1481304 -4.1140232 -4.0866213 -4.0817928 -4.0827513 -4.088202 -4.1281271 -4.1721487 -4.1891451 -4.1865573 -4.1810627][-4.2015033 -4.2306437 -4.2445269 -4.2370219 -4.216784 -4.1920118 -4.1702409 -4.1631665 -4.1594791 -4.1606264 -4.1788955 -4.1982446 -4.2014513 -4.1905866 -4.1793041][-4.1820588 -4.2132993 -4.2419758 -4.251328 -4.2447968 -4.2344275 -4.2242169 -4.2151365 -4.208035 -4.2099361 -4.2160006 -4.2162786 -4.2061934 -4.1855412 -4.1707926][-4.1424518 -4.1713896 -4.2082458 -4.2300305 -4.2376742 -4.2383661 -4.2372489 -4.2294893 -4.219183 -4.2174754 -4.2196317 -4.2177558 -4.204607 -4.1794147 -4.1598067][-4.1094632 -4.130126 -4.1633511 -4.1886287 -4.2062778 -4.2191515 -4.2259564 -4.2239814 -4.2174082 -4.2111945 -4.2047491 -4.1986904 -4.1878181 -4.16496 -4.1478643][-4.1109142 -4.1214113 -4.1471786 -4.1694236 -4.1886311 -4.2052984 -4.2168813 -4.2193756 -4.2188592 -4.2135935 -4.2036781 -4.1924105 -4.1806293 -4.1632648 -4.1553087]]...]
INFO - root - 2017-12-05 11:55:32.251940: step 8810, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 75h:51m:32s remains)
INFO - root - 2017-12-05 11:55:41.714568: step 8820, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 88h:33m:13s remains)
INFO - root - 2017-12-05 11:55:51.283597: step 8830, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 88h:24m:57s remains)
INFO - root - 2017-12-05 11:56:00.556759: step 8840, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 86h:07m:21s remains)
INFO - root - 2017-12-05 11:56:10.093029: step 8850, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.988 sec/batch; 88h:49m:31s remains)
INFO - root - 2017-12-05 11:56:19.573015: step 8860, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 86h:18m:52s remains)
INFO - root - 2017-12-05 11:56:29.021962: step 8870, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 86h:04m:16s remains)
INFO - root - 2017-12-05 11:56:38.628624: step 8880, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 86h:55m:24s remains)
INFO - root - 2017-12-05 11:56:48.033266: step 8890, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 85h:03m:34s remains)
INFO - root - 2017-12-05 11:56:57.517706: step 8900, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.958 sec/batch; 86h:09m:02s remains)
2017-12-05 11:56:58.285846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1929383 -4.1872988 -4.1973696 -4.2038364 -4.2022748 -4.1955967 -4.1988339 -4.2075477 -4.2177367 -4.2238574 -4.2178311 -4.2032933 -4.1803885 -4.155519 -4.142942][-4.1515541 -4.1528144 -4.1769843 -4.2018881 -4.210835 -4.2074189 -4.20963 -4.2132959 -4.2222719 -4.2357192 -4.2274523 -4.1965923 -4.1584554 -4.1227579 -4.1020036][-4.1111021 -4.1332169 -4.1764841 -4.2132878 -4.2245569 -4.2176471 -4.2125239 -4.2091608 -4.209476 -4.2161484 -4.1994514 -4.1539278 -4.1009278 -4.0625639 -4.0440359][-4.0595417 -4.1121664 -4.1787772 -4.22152 -4.2246675 -4.2049809 -4.1881814 -4.1774693 -4.1678791 -4.1625609 -4.147038 -4.1025372 -4.0510621 -4.0253639 -4.0195332][-4.0357847 -4.1110792 -4.1859241 -4.2192688 -4.2064495 -4.1664581 -4.134819 -4.1218987 -4.1083641 -4.1026783 -4.1001081 -4.0753326 -4.0449405 -4.0407863 -4.0519977][-4.1075892 -4.1722984 -4.2199717 -4.2227407 -4.1847048 -4.1175036 -4.0627637 -4.0515046 -4.0508451 -4.0599051 -4.078104 -4.0820041 -4.0807958 -4.0900254 -4.1052456][-4.1891422 -4.2232776 -4.2299304 -4.19848 -4.134191 -4.0400743 -3.9605117 -3.9655702 -4.0005631 -4.0380354 -4.0802412 -4.1135478 -4.1329942 -4.1428294 -4.1455669][-4.2081933 -4.2126064 -4.183342 -4.1178312 -4.029439 -3.91178 -3.8086271 -3.8481729 -3.936202 -4.0078259 -4.0745878 -4.1285267 -4.1574497 -4.158639 -4.1434097][-4.2006865 -4.17586 -4.1114225 -4.0166111 -3.9161303 -3.7943888 -3.6865582 -3.7639258 -3.8959973 -3.9852233 -4.058073 -4.1204557 -4.1504993 -4.1393809 -4.1117988][-4.1845212 -4.1397986 -4.0626082 -3.9665496 -3.8867083 -3.8090184 -3.7500172 -3.8298492 -3.94634 -4.0184441 -4.0716333 -4.1272855 -4.1547894 -4.1361208 -4.1102524][-4.1680021 -4.1181045 -4.0428867 -3.9628012 -3.9147146 -3.8853712 -3.8713691 -3.9325931 -4.0148931 -4.0662665 -4.0982056 -4.1409388 -4.1644878 -4.1479707 -4.1317286][-4.1650758 -4.1140823 -4.0460005 -3.9881074 -3.968843 -3.9733303 -3.9828115 -4.0263925 -4.079493 -4.1123147 -4.129457 -4.1596627 -4.1791449 -4.1659126 -4.1583734][-4.1957588 -4.1556687 -4.1069193 -4.073781 -4.0716424 -4.08671 -4.1020889 -4.12786 -4.1550493 -4.170886 -4.1797352 -4.1984167 -4.2130895 -4.20763 -4.2073655][-4.247551 -4.2235107 -4.1963463 -4.1823044 -4.1848116 -4.1974087 -4.2109962 -4.22243 -4.2312942 -4.2391453 -4.2441115 -4.2537766 -4.2627068 -4.2631025 -4.2664089][-4.292985 -4.2807984 -4.2690473 -4.2658477 -4.2704835 -4.2796392 -4.2901073 -4.2945862 -4.2958994 -4.2993593 -4.3014183 -4.30615 -4.3106327 -4.3105526 -4.3123527]]...]
INFO - root - 2017-12-05 11:57:07.748870: step 8910, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 83h:14m:18s remains)
INFO - root - 2017-12-05 11:57:17.111273: step 8920, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 84h:00m:54s remains)
INFO - root - 2017-12-05 11:57:26.461625: step 8930, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 84h:40m:42s remains)
INFO - root - 2017-12-05 11:57:35.843205: step 8940, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 84h:02m:44s remains)
INFO - root - 2017-12-05 11:57:45.187969: step 8950, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 84h:37m:25s remains)
INFO - root - 2017-12-05 11:57:54.605844: step 8960, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 80h:50m:35s remains)
INFO - root - 2017-12-05 11:58:04.135822: step 8970, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 87h:13m:41s remains)
INFO - root - 2017-12-05 11:58:13.467618: step 8980, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.814 sec/batch; 73h:10m:37s remains)
INFO - root - 2017-12-05 11:58:22.883029: step 8990, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 83h:34m:29s remains)
INFO - root - 2017-12-05 11:58:32.321331: step 9000, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 88h:17m:59s remains)
2017-12-05 11:58:33.090713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1598158 -4.1467662 -4.1368866 -4.1343756 -4.1307039 -4.13184 -4.1315317 -4.126296 -4.125474 -4.1324091 -4.1458535 -4.1543021 -4.1523561 -4.1346364 -4.111836][-4.1438766 -4.1269493 -4.1153183 -4.1140518 -4.1110363 -4.1082373 -4.1006713 -4.089705 -4.0923538 -4.1073794 -4.129508 -4.1451817 -4.1473403 -4.1304579 -4.1050062][-4.1277246 -4.1092086 -4.098691 -4.1018367 -4.10283 -4.0994816 -4.0891166 -4.0769749 -4.084352 -4.1060338 -4.1334271 -4.1502047 -4.1529942 -4.1353621 -4.1067238][-4.1053486 -4.0919919 -4.0852313 -4.09165 -4.0937934 -4.0882649 -4.0750642 -4.064589 -4.0805655 -4.1106472 -4.142982 -4.1611423 -4.1646848 -4.1504264 -4.1249008][-4.068017 -4.0641308 -4.0592227 -4.0599113 -4.0544481 -4.0410118 -4.0215263 -4.0133252 -4.0416722 -4.0859594 -4.1292787 -4.155355 -4.1652155 -4.1599193 -4.145916][-4.0232553 -4.023591 -4.0164609 -4.005527 -3.9874251 -3.962214 -3.9300053 -3.9209647 -3.9631035 -4.0264034 -4.0881076 -4.1290464 -4.1500192 -4.1528115 -4.1492515][-4.0226073 -4.0180373 -4.0069089 -3.9905477 -3.9649718 -3.9318714 -3.8873038 -3.8681479 -3.9071343 -3.9747245 -4.045208 -4.0971475 -4.1256995 -4.1306152 -4.1291494][-4.0605674 -4.0538716 -4.0471926 -4.0387197 -4.023025 -4.0035667 -3.9716399 -3.9501605 -3.9688425 -4.0127082 -4.0617733 -4.0985532 -4.1188521 -4.11884 -4.1098228][-4.1054668 -4.1011667 -4.1019397 -4.1017294 -4.0970111 -4.091351 -4.0800548 -4.0716605 -4.0815849 -4.1040282 -4.1277418 -4.1446552 -4.1554179 -4.15234 -4.1370745][-4.149014 -4.1458449 -4.1522903 -4.157618 -4.1570635 -4.1569853 -4.1568847 -4.157795 -4.1651697 -4.1747189 -4.183753 -4.1899738 -4.1960011 -4.1944003 -4.1819668][-4.17875 -4.1761904 -4.1836443 -4.1905041 -4.1918564 -4.1951203 -4.2007451 -4.206614 -4.2125735 -4.2160459 -4.2198272 -4.2238011 -4.2273889 -4.2258787 -4.218524][-4.20214 -4.1978321 -4.2005486 -4.2015944 -4.1985679 -4.2028885 -4.21355 -4.222497 -4.2275558 -4.2290487 -4.2324696 -4.2370853 -4.2403965 -4.2404575 -4.2379622][-4.22589 -4.2214565 -4.221221 -4.2158561 -4.20693 -4.2089696 -4.2184381 -4.224144 -4.2255344 -4.2246757 -4.2259235 -4.2301807 -4.2335181 -4.2354245 -4.2364616][-4.2299142 -4.2269254 -4.2287993 -4.22487 -4.2184896 -4.2219982 -4.23134 -4.2371516 -4.2386012 -4.2369413 -4.2345567 -4.234611 -4.2349739 -4.2369218 -4.2386241][-4.2192478 -4.2180767 -4.2215476 -4.2196846 -4.2165561 -4.2207403 -4.2297792 -4.2373095 -4.241569 -4.2432232 -4.2429123 -4.2439232 -4.2464991 -4.2506595 -4.2539053]]...]
INFO - root - 2017-12-05 11:58:42.402648: step 9010, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 86h:13m:09s remains)
INFO - root - 2017-12-05 11:58:51.809308: step 9020, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 82h:09m:00s remains)
INFO - root - 2017-12-05 11:59:01.177920: step 9030, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.024 sec/batch; 92h:02m:24s remains)
INFO - root - 2017-12-05 11:59:10.664385: step 9040, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 86h:30m:52s remains)
INFO - root - 2017-12-05 11:59:20.132579: step 9050, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 87h:03m:23s remains)
INFO - root - 2017-12-05 11:59:29.501069: step 9060, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 86h:55m:55s remains)
INFO - root - 2017-12-05 11:59:38.883936: step 9070, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 88h:32m:56s remains)
INFO - root - 2017-12-05 11:59:48.125373: step 9080, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 89h:14m:37s remains)
INFO - root - 2017-12-05 11:59:57.475698: step 9090, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 79h:39m:24s remains)
INFO - root - 2017-12-05 12:00:07.017890: step 9100, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.018 sec/batch; 91h:27m:00s remains)
2017-12-05 12:00:07.789746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3283582 -4.3033934 -4.2606997 -4.2200389 -4.2089806 -4.2125187 -4.2106943 -4.2132668 -4.2210512 -4.2228742 -4.2314258 -4.2456388 -4.2659369 -4.2654471 -4.2441325][-4.3264785 -4.300736 -4.2586737 -4.2205567 -4.2093954 -4.2160144 -4.2185965 -4.2216868 -4.2261558 -4.2213326 -4.2220554 -4.2359529 -4.2599068 -4.2653751 -4.2473454][-4.3256655 -4.2968249 -4.2532845 -4.217793 -4.2078829 -4.2147965 -4.222548 -4.2338405 -4.2403536 -4.2331228 -4.2210875 -4.2254138 -4.2442732 -4.25203 -4.2418351][-4.3318348 -4.3011789 -4.2539997 -4.2119818 -4.19394 -4.1944251 -4.2053242 -4.2299895 -4.2450976 -4.2426705 -4.226347 -4.2188621 -4.2261505 -4.23059 -4.227139][-4.341547 -4.3097711 -4.2575259 -4.2013288 -4.1649508 -4.1518822 -4.1606445 -4.1975317 -4.2298074 -4.2418509 -4.2326012 -4.2199783 -4.2161903 -4.2111783 -4.2113323][-4.3501916 -4.318574 -4.2616515 -4.1888866 -4.1249485 -4.0843854 -4.0795126 -4.1269073 -4.1879144 -4.2280436 -4.2393689 -4.2316418 -4.2188272 -4.203908 -4.19932][-4.3552971 -4.3254886 -4.2676034 -4.1836772 -4.0924292 -4.0140495 -3.9783897 -4.0304089 -4.123776 -4.1984634 -4.2401586 -4.2480822 -4.2355886 -4.2160034 -4.2026877][-4.357296 -4.3324704 -4.2779765 -4.1928368 -4.0868382 -3.9774537 -3.897357 -3.9354265 -4.0522938 -4.1549921 -4.2257094 -4.2565761 -4.2585182 -4.2437415 -4.2264767][-4.3591423 -4.3390541 -4.2921357 -4.2166338 -4.1198115 -4.0130792 -3.9214921 -3.9369607 -4.0348182 -4.12803 -4.204453 -4.2499304 -4.2683029 -4.2643337 -4.2472577][-4.361557 -4.3453436 -4.3065948 -4.2457857 -4.1733608 -4.1002989 -4.0394664 -4.0421958 -4.087491 -4.1349263 -4.1899981 -4.2352242 -4.2601719 -4.2649255 -4.2547436][-4.36137 -4.3461809 -4.3108521 -4.2582612 -4.2043481 -4.1626348 -4.1353536 -4.1403403 -4.1520286 -4.1637192 -4.1961308 -4.2322059 -4.2553649 -4.2635674 -4.2578058][-4.3581634 -4.3423586 -4.3066354 -4.2549357 -4.2100873 -4.1863732 -4.178288 -4.1870337 -4.1917577 -4.1959944 -4.2143173 -4.2381859 -4.2536893 -4.256115 -4.2509136][-4.3529506 -4.3343897 -4.2963066 -4.243804 -4.2024574 -4.1824131 -4.177475 -4.1885905 -4.2006145 -4.2156968 -4.2302785 -4.24272 -4.2474933 -4.240798 -4.2272644][-4.3496609 -4.3282948 -4.2873044 -4.233881 -4.1923318 -4.1695352 -4.1617465 -4.179996 -4.2059565 -4.2290287 -4.2401781 -4.2451282 -4.2429252 -4.2283282 -4.2052612][-4.3485122 -4.3266268 -4.2864013 -4.2378578 -4.2004447 -4.1771245 -4.1690927 -4.1929259 -4.2247663 -4.2449808 -4.2506776 -4.2507567 -4.2436008 -4.2269478 -4.2006521]]...]
INFO - root - 2017-12-05 12:00:17.423464: step 9110, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 82h:53m:46s remains)
INFO - root - 2017-12-05 12:00:26.692982: step 9120, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 80h:59m:07s remains)
INFO - root - 2017-12-05 12:00:36.235906: step 9130, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.014 sec/batch; 91h:07m:28s remains)
INFO - root - 2017-12-05 12:00:45.796840: step 9140, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 84h:52m:30s remains)
INFO - root - 2017-12-05 12:00:55.003927: step 9150, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.988 sec/batch; 88h:45m:13s remains)
INFO - root - 2017-12-05 12:01:04.454144: step 9160, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.965 sec/batch; 86h:42m:44s remains)
INFO - root - 2017-12-05 12:01:13.884885: step 9170, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 83h:35m:13s remains)
INFO - root - 2017-12-05 12:01:23.179734: step 9180, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.001 sec/batch; 89h:55m:28s remains)
INFO - root - 2017-12-05 12:01:32.771281: step 9190, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 82h:28m:55s remains)
INFO - root - 2017-12-05 12:01:42.200756: step 9200, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 85h:40m:25s remains)
2017-12-05 12:01:42.961893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3262815 -4.3165555 -4.3054218 -4.2910032 -4.2728124 -4.2619576 -4.2622008 -4.2709403 -4.2777882 -4.280293 -4.2870183 -4.2963166 -4.2992525 -4.3025384 -4.3040261][-4.3222 -4.3084879 -4.2853451 -4.2551756 -4.224678 -4.2062554 -4.2069082 -4.2235236 -4.2377081 -4.245327 -4.256608 -4.2690539 -4.2720108 -4.2736764 -4.27199][-4.3228455 -4.3035884 -4.2640834 -4.2122059 -4.166357 -4.1371026 -4.129158 -4.1523247 -4.1848974 -4.2063885 -4.224793 -4.2389417 -4.242291 -4.24346 -4.2394409][-4.3249679 -4.29956 -4.2450366 -4.176712 -4.12019 -4.0780892 -4.0549374 -4.0836229 -4.1323304 -4.1703415 -4.1980805 -4.21349 -4.2171063 -4.2172327 -4.2110968][-4.3284926 -4.3005409 -4.2354531 -4.1592393 -4.093327 -4.0272722 -3.9752641 -4.008759 -4.0788074 -4.1339059 -4.1718307 -4.1874518 -4.1865478 -4.1833916 -4.177248][-4.3304749 -4.3041797 -4.2356462 -4.1555243 -4.077899 -3.982482 -3.8912537 -3.9185386 -4.0147433 -4.0866261 -4.1380324 -4.1558695 -4.1541572 -4.1490169 -4.1478586][-4.3283691 -4.3010416 -4.2297812 -4.14241 -4.0512114 -3.9260321 -3.7875249 -3.7825236 -3.9098537 -4.01815 -4.0917563 -4.119523 -4.1204524 -4.1193452 -4.126677][-4.3215971 -4.2920728 -4.2197046 -4.1263013 -4.0185413 -3.8660672 -3.6819344 -3.6358411 -3.8017108 -3.9578679 -4.0529795 -4.0913672 -4.0969982 -4.1015368 -4.1156931][-4.3121147 -4.2782712 -4.2026091 -4.1051984 -3.9896252 -3.8375669 -3.6724348 -3.6427369 -3.8107018 -3.9634583 -4.051559 -4.0865064 -4.0884676 -4.0944963 -4.1125917][-4.300384 -4.2675748 -4.196764 -4.1100588 -4.0142274 -3.8972979 -3.7973723 -3.8069847 -3.9259353 -4.0273619 -4.0856977 -4.1056561 -4.1024137 -4.1033611 -4.1197906][-4.2948174 -4.2683277 -4.2076139 -4.1420441 -4.0727057 -3.9869618 -3.9292233 -3.9512289 -4.02496 -4.0780859 -4.1100316 -4.1256685 -4.1279883 -4.1286311 -4.1429291][-4.2935305 -4.2754812 -4.2345119 -4.186923 -4.1321578 -4.0680404 -4.0328393 -4.0591125 -4.1027403 -4.1220708 -4.1353874 -4.1501369 -4.1640816 -4.1692119 -4.1794243][-4.2992921 -4.287065 -4.2574239 -4.218574 -4.1757607 -4.1366878 -4.12087 -4.1451344 -4.1673789 -4.16759 -4.1667166 -4.178947 -4.20079 -4.2096395 -4.2167854][-4.3143125 -4.3056965 -4.2837853 -4.2553105 -4.2297359 -4.2114172 -4.2090178 -4.22312 -4.2288146 -4.2218804 -4.2176385 -4.2270246 -4.2480421 -4.2547317 -4.2573638][-4.32939 -4.3237939 -4.3102794 -4.2926292 -4.2766404 -4.2695074 -4.2761226 -4.2864351 -4.2883081 -4.2823267 -4.2773528 -4.2797365 -4.2928905 -4.29607 -4.2924123]]...]
INFO - root - 2017-12-05 12:01:52.406799: step 9210, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.830 sec/batch; 74h:31m:20s remains)
INFO - root - 2017-12-05 12:02:01.906858: step 9220, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 80h:49m:37s remains)
INFO - root - 2017-12-05 12:02:11.350964: step 9230, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 85h:01m:44s remains)
INFO - root - 2017-12-05 12:02:20.786782: step 9240, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 85h:35m:03s remains)
INFO - root - 2017-12-05 12:02:30.241800: step 9250, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 79h:56m:09s remains)
INFO - root - 2017-12-05 12:02:39.606553: step 9260, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 83h:44m:48s remains)
INFO - root - 2017-12-05 12:02:48.993937: step 9270, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.980 sec/batch; 88h:01m:00s remains)
INFO - root - 2017-12-05 12:02:58.224986: step 9280, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 87h:24m:44s remains)
INFO - root - 2017-12-05 12:03:07.709755: step 9290, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 88h:12m:48s remains)
INFO - root - 2017-12-05 12:03:17.022030: step 9300, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 88h:57m:36s remains)
2017-12-05 12:03:17.768624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1737413 -4.1516504 -4.1337585 -4.1092587 -4.0930481 -4.0962915 -4.1073852 -4.1278281 -4.1607537 -4.1876836 -4.1969647 -4.1913605 -4.1740828 -4.1540513 -4.151329][-4.133194 -4.1069431 -4.0886979 -4.0625648 -4.04652 -4.0565586 -4.0725727 -4.0949559 -4.12744 -4.15354 -4.1653337 -4.16535 -4.1554327 -4.1437092 -4.1491456][-4.10371 -4.0773687 -4.0608373 -4.0389538 -4.0297894 -4.04501 -4.0620713 -4.0803332 -4.1088834 -4.135139 -4.1502795 -4.1555037 -4.1516223 -4.1459212 -4.1533217][-4.0921559 -4.070713 -4.0576715 -4.0428166 -4.0392566 -4.0527325 -4.0651922 -4.0750461 -4.0998816 -4.1287656 -4.1473527 -4.1561203 -4.1581421 -4.1550894 -4.1607113][-4.0814304 -4.0614867 -4.0475211 -4.0357375 -4.0320759 -4.0401087 -4.0476942 -4.0520449 -4.0761166 -4.110127 -4.1325741 -4.14548 -4.1532125 -4.1536908 -4.1596003][-4.0631423 -4.0362272 -4.013875 -3.9977086 -3.9899802 -3.9969037 -4.0101471 -4.0214272 -4.05169 -4.0924568 -4.1206512 -4.1373277 -4.1475296 -4.1492047 -4.1565361][-4.0404553 -4.0096626 -3.9799848 -3.9554491 -3.9433153 -3.9521539 -3.9760563 -4.0016236 -4.0403924 -4.0847988 -4.1159954 -4.1324024 -4.1399641 -4.1414056 -4.1517572][-4.0300617 -4.0068746 -3.9827585 -3.957794 -3.9432838 -3.9485002 -3.9749804 -4.0091977 -4.0535545 -4.0987115 -4.1298871 -4.1438723 -4.1464133 -4.1455345 -4.1556396][-4.047626 -4.0311947 -4.0142384 -3.9920371 -3.9748576 -3.9752936 -3.9979107 -4.0333371 -4.0793338 -4.1252241 -4.1580572 -4.1711888 -4.1687326 -4.1625471 -4.1674008][-4.069808 -4.0566697 -4.041049 -4.0191903 -4.0009251 -4.0005636 -4.0212426 -4.0553856 -4.0994935 -4.1449404 -4.1797285 -4.1937218 -4.1883159 -4.1778727 -4.1791778][-4.0824566 -4.0750442 -4.0636 -4.0454216 -4.0279326 -4.0261512 -4.041822 -4.0710592 -4.1104736 -4.1538982 -4.1894908 -4.2051954 -4.1994429 -4.1873674 -4.1872187][-4.0789576 -4.0742388 -4.0670543 -4.0561471 -4.0443864 -4.0421472 -4.049624 -4.0687027 -4.1015072 -4.1418424 -4.1789083 -4.1982875 -4.1949887 -4.1846938 -4.1862626][-4.0731564 -4.0648179 -4.056118 -4.04675 -4.0381117 -4.0338073 -4.0318995 -4.0386839 -4.0638409 -4.1009064 -4.1399503 -4.1674404 -4.1725197 -4.1682286 -4.1748748][-4.0749931 -4.0662227 -4.0562458 -4.0454855 -4.036715 -4.032722 -4.0281367 -4.0282493 -4.0474234 -4.0799484 -4.1172423 -4.1483464 -4.1573787 -4.1551118 -4.1625357][-4.0841331 -4.0792975 -4.07094 -4.0625968 -4.0577779 -4.0595155 -4.0607753 -4.0600004 -4.0754266 -4.1054878 -4.1382685 -4.1634793 -4.1666942 -4.1581869 -4.1589041]]...]
INFO - root - 2017-12-05 12:03:27.133886: step 9310, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 87h:55m:52s remains)
INFO - root - 2017-12-05 12:03:36.599365: step 9320, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 86h:00m:52s remains)
INFO - root - 2017-12-05 12:03:45.989442: step 9330, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 87h:40m:37s remains)
INFO - root - 2017-12-05 12:03:55.266664: step 9340, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 89h:28m:25s remains)
INFO - root - 2017-12-05 12:04:04.350412: step 9350, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 88h:44m:51s remains)
INFO - root - 2017-12-05 12:04:13.833807: step 9360, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 84h:42m:06s remains)
INFO - root - 2017-12-05 12:04:23.007717: step 9370, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.878 sec/batch; 78h:46m:12s remains)
INFO - root - 2017-12-05 12:04:32.290170: step 9380, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 81h:40m:21s remains)
INFO - root - 2017-12-05 12:04:41.692566: step 9390, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 83h:20m:09s remains)
INFO - root - 2017-12-05 12:04:51.035281: step 9400, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 87h:37m:13s remains)
2017-12-05 12:04:51.795528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2105641 -4.2098112 -4.2134027 -4.2306423 -4.2456112 -4.2434549 -4.24104 -4.2482147 -4.2446885 -4.2087188 -4.144402 -4.0960641 -4.1105881 -4.1466665 -4.1716857][-4.2182093 -4.2168441 -4.2313571 -4.2560673 -4.2653232 -4.2552834 -4.2510891 -4.2620864 -4.2658014 -4.2299809 -4.1595831 -4.1024542 -4.1122708 -4.1484895 -4.1708083][-4.2379751 -4.234499 -4.256721 -4.2775517 -4.2749672 -4.257648 -4.2566004 -4.2682238 -4.2727165 -4.2359772 -4.162364 -4.1012588 -4.1132536 -4.1524172 -4.1736989][-4.2569547 -4.249145 -4.2656665 -4.2747455 -4.262188 -4.2458487 -4.2520776 -4.261272 -4.2656312 -4.2323656 -4.1624489 -4.1050305 -4.1254511 -4.1641951 -4.18067][-4.2664227 -4.2541618 -4.2559581 -4.2471657 -4.2287583 -4.2217488 -4.2351127 -4.2421188 -4.2412186 -4.2080851 -4.144001 -4.0963974 -4.1261158 -4.16499 -4.1775017][-4.2719564 -4.2552919 -4.2385836 -4.2063408 -4.1757236 -4.1773014 -4.1972566 -4.19919 -4.1838994 -4.1511254 -4.0998917 -4.0679288 -4.105382 -4.1424375 -4.1520734][-4.2393928 -4.2141838 -4.17974 -4.1186638 -4.0651784 -4.0722017 -4.0994964 -4.091948 -4.0564075 -4.0240936 -3.9899578 -3.984786 -4.0386634 -4.0868125 -4.104084][-4.1906252 -4.1625271 -4.1194735 -4.0398836 -3.9726248 -3.981493 -4.015955 -4.0044889 -3.9508839 -3.920507 -3.9035227 -3.913147 -3.9756637 -4.0352993 -4.0641971][-4.1495585 -4.1257014 -4.0898848 -4.0131054 -3.9469693 -3.9597926 -3.999619 -3.9899988 -3.9322958 -3.9163551 -3.9236999 -3.9389155 -3.99171 -4.0445776 -4.0755272][-4.1327915 -4.1201663 -4.0981431 -4.0369711 -3.977241 -3.9864943 -4.0257821 -4.014432 -3.956929 -3.957449 -3.9930637 -4.0206537 -4.06786 -4.1148415 -4.1409469][-4.1450915 -4.1428771 -4.1293468 -4.0914478 -4.048399 -4.0465784 -4.0698624 -4.0531049 -4.0063081 -4.0181427 -4.06685 -4.1040463 -4.1489782 -4.1946812 -4.2201114][-4.1761718 -4.1808176 -4.181581 -4.1692953 -4.14764 -4.140101 -4.1456051 -4.1252046 -4.0918088 -4.1048307 -4.1433406 -4.1759591 -4.2155037 -4.2542057 -4.2756448][-4.2148466 -4.2217 -4.2315073 -4.2351732 -4.2290792 -4.224822 -4.2274041 -4.2101107 -4.1863909 -4.1894 -4.2036648 -4.2214189 -4.2514925 -4.2802129 -4.2947578][-4.2620449 -4.2642393 -4.2703032 -4.2766166 -4.2764 -4.273397 -4.2756133 -4.2682853 -4.2537565 -4.2459645 -4.2370214 -4.2375736 -4.2579417 -4.2773342 -4.2873082][-4.2979565 -4.2929025 -4.2925 -4.2942786 -4.2926645 -4.2899256 -4.2900872 -4.2850747 -4.273222 -4.2556834 -4.2268062 -4.2142973 -4.23432 -4.2542114 -4.2680817]]...]
INFO - root - 2017-12-05 12:05:01.122765: step 9410, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 82h:23m:03s remains)
INFO - root - 2017-12-05 12:05:10.407048: step 9420, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 78h:38m:06s remains)
INFO - root - 2017-12-05 12:05:19.587287: step 9430, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 85h:06m:04s remains)
INFO - root - 2017-12-05 12:05:28.780922: step 9440, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 87h:54m:39s remains)
INFO - root - 2017-12-05 12:05:38.275921: step 9450, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 84h:36m:16s remains)
INFO - root - 2017-12-05 12:05:47.624052: step 9460, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 84h:17m:15s remains)
INFO - root - 2017-12-05 12:05:57.099489: step 9470, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 81h:55m:07s remains)
INFO - root - 2017-12-05 12:06:06.542570: step 9480, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.947 sec/batch; 84h:56m:29s remains)
INFO - root - 2017-12-05 12:06:16.018753: step 9490, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 83h:32m:43s remains)
INFO - root - 2017-12-05 12:06:25.247541: step 9500, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 86h:54m:51s remains)
2017-12-05 12:06:26.014550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1775794 -4.2100129 -4.249299 -4.2577395 -4.2108765 -4.1267104 -4.0566187 -4.0384951 -4.096395 -4.1857495 -4.2360663 -4.2537279 -4.26279 -4.2684817 -4.2618508][-4.1959782 -4.2262969 -4.263792 -4.2680583 -4.2115197 -4.1156273 -4.0352616 -4.0163789 -4.0824747 -4.1858559 -4.2483644 -4.2686052 -4.2782307 -4.2836633 -4.2762866][-4.2124276 -4.2379155 -4.2673683 -4.2647986 -4.2062 -4.1107979 -4.0350652 -4.0114546 -4.0661969 -4.1670151 -4.2367029 -4.2674689 -4.2869658 -4.2971787 -4.2919111][-4.23578 -4.2479496 -4.2630305 -4.2524724 -4.1935663 -4.1080537 -4.0413632 -4.0117774 -4.0525866 -4.1390963 -4.2040777 -4.2436161 -4.2779965 -4.2973042 -4.2997241][-4.2490811 -4.2472658 -4.2453046 -4.2219868 -4.1617818 -4.0918536 -4.036746 -4.0086951 -4.0405312 -4.106864 -4.1604509 -4.2038412 -4.2505503 -4.2820559 -4.2932844][-4.2311716 -4.2183013 -4.2006707 -4.1605062 -4.0961919 -4.0444221 -4.0095944 -3.9922907 -4.0218978 -4.0720911 -4.1126618 -4.1592131 -4.2120342 -4.2487264 -4.2685418][-4.1965151 -4.1772432 -4.1447268 -4.086081 -4.0169172 -3.9815245 -3.9660783 -3.9565284 -3.9778137 -4.0196247 -4.0628123 -4.1180253 -4.1734376 -4.212306 -4.23929][-4.1678586 -4.1528788 -4.1218777 -4.0642185 -4.002264 -3.9824595 -3.9814904 -3.9707885 -3.9759552 -4.0050826 -4.0521326 -4.1139526 -4.1692634 -4.2047634 -4.23316][-4.1792316 -4.17121 -4.15418 -4.11005 -4.057394 -4.0389462 -4.0401697 -4.0278478 -4.0277252 -4.0503407 -4.0981321 -4.1587629 -4.2076969 -4.2329268 -4.2508044][-4.2152591 -4.2138577 -4.2102642 -4.184083 -4.1413708 -4.1225367 -4.1233053 -4.1145778 -4.1127496 -4.1290984 -4.1687574 -4.2207222 -4.2604408 -4.2777228 -4.28521][-4.2536364 -4.2566223 -4.2608337 -4.245635 -4.2133884 -4.20084 -4.21007 -4.2064114 -4.19959 -4.20863 -4.2331967 -4.2697015 -4.2959933 -4.307291 -4.3100986][-4.2746458 -4.2808843 -4.2874165 -4.2782512 -4.2554617 -4.2481065 -4.2629976 -4.2673073 -4.2611122 -4.2620773 -4.275157 -4.2959833 -4.3109803 -4.3162413 -4.315918][-4.2769556 -4.2827125 -4.2917094 -4.290626 -4.2783642 -4.2755146 -4.2886429 -4.294776 -4.2877522 -4.2853847 -4.2951841 -4.3057904 -4.3120618 -4.3141375 -4.3132677][-4.280941 -4.2818179 -4.2887359 -4.28883 -4.2793465 -4.276135 -4.2855926 -4.290802 -4.28879 -4.2872157 -4.2955861 -4.3047476 -4.310462 -4.3140678 -4.3147726][-4.2914834 -4.290473 -4.2933359 -4.2915764 -4.2836528 -4.2792406 -4.2822781 -4.2856274 -4.2874937 -4.2904348 -4.2967725 -4.3030787 -4.3085256 -4.313633 -4.3153739]]...]
INFO - root - 2017-12-05 12:06:35.561224: step 9510, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 85h:11m:56s remains)
INFO - root - 2017-12-05 12:06:44.817253: step 9520, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 88h:05m:52s remains)
INFO - root - 2017-12-05 12:06:54.129952: step 9530, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 86h:34m:48s remains)
INFO - root - 2017-12-05 12:07:03.503465: step 9540, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 88h:02m:34s remains)
INFO - root - 2017-12-05 12:07:12.919024: step 9550, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 84h:03m:38s remains)
INFO - root - 2017-12-05 12:07:22.370259: step 9560, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 88h:11m:04s remains)
INFO - root - 2017-12-05 12:07:31.894969: step 9570, loss = 2.02, batch loss = 1.96 (8.3 examples/sec; 0.965 sec/batch; 86h:34m:58s remains)
INFO - root - 2017-12-05 12:07:41.377121: step 9580, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 86h:17m:36s remains)
INFO - root - 2017-12-05 12:07:50.694369: step 9590, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 77h:41m:13s remains)
INFO - root - 2017-12-05 12:08:00.068899: step 9600, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 87h:50m:25s remains)
2017-12-05 12:08:00.809183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3113317 -4.3099961 -4.3124189 -4.3083906 -4.2959785 -4.2728462 -4.2420812 -4.2174234 -4.2047715 -4.20075 -4.1949234 -4.1780038 -4.1547012 -4.1033444 -4.0426354][-4.2906251 -4.2901292 -4.2986503 -4.2980919 -4.2860003 -4.2616186 -4.2299252 -4.2066936 -4.2000446 -4.2011404 -4.1981096 -4.1824884 -4.1651649 -4.1144004 -4.0482645][-4.2653718 -4.2684555 -4.2829986 -4.2870274 -4.275969 -4.2505546 -4.22172 -4.2032743 -4.2050891 -4.2132006 -4.2147808 -4.2091041 -4.2059054 -4.1721368 -4.1217413][-4.2285609 -4.2357712 -4.2567439 -4.2671466 -4.2612762 -4.2397194 -4.2190509 -4.2080412 -4.2173543 -4.2311134 -4.23652 -4.2380905 -4.2454772 -4.2303305 -4.2004266][-4.182282 -4.1917458 -4.219738 -4.2398396 -4.2424774 -4.225987 -4.2139997 -4.2093596 -4.2204618 -4.2361779 -4.2443848 -4.2512159 -4.2649961 -4.2658629 -4.2547312][-4.1290684 -4.1428738 -4.1776943 -4.2062 -4.2172489 -4.2042451 -4.1940389 -4.19077 -4.202673 -4.2217526 -4.2365656 -4.2493553 -4.2686677 -4.2807479 -4.2805533][-4.0957 -4.1127276 -4.144423 -4.1696868 -4.1824222 -4.1731567 -4.1627369 -4.1590633 -4.1706643 -4.193172 -4.2155271 -4.2334085 -4.2586708 -4.2770734 -4.2833261][-4.1067047 -4.117465 -4.1304975 -4.1366153 -4.1370482 -4.1209488 -4.1050534 -4.0981412 -4.109622 -4.1385093 -4.1691947 -4.1974459 -4.2350111 -4.2627487 -4.2744431][-4.1287832 -4.126955 -4.1157393 -4.0927134 -4.0672412 -4.0312648 -4.00248 -3.9927466 -4.005127 -4.0489206 -4.0988569 -4.1446652 -4.1994429 -4.2396865 -4.259984][-4.1489334 -4.1318321 -4.0998492 -4.0561209 -4.0083141 -3.9614604 -3.9314725 -3.92622 -3.9407458 -3.9950979 -4.0606031 -4.1129003 -4.1697049 -4.2144895 -4.244144][-4.162147 -4.1372004 -4.0992117 -4.0540638 -4.0098448 -3.9777193 -3.9620466 -3.965627 -3.9773118 -4.02309 -4.0826826 -4.1268635 -4.1681089 -4.2058964 -4.2350307][-4.16998 -4.1468225 -4.1166162 -4.0840292 -4.0588975 -4.05043 -4.0464325 -4.0531168 -4.0620389 -4.0923591 -4.1395521 -4.1729088 -4.199204 -4.2240095 -4.2447662][-4.1844625 -4.1637836 -4.14698 -4.1317673 -4.1239629 -4.1332016 -4.1418881 -4.1500673 -4.1578465 -4.1766915 -4.2093081 -4.2322912 -4.2469754 -4.26078 -4.27085][-4.2261658 -4.2091904 -4.2043438 -4.2047825 -4.2080603 -4.2237883 -4.2372441 -4.24377 -4.2490821 -4.2604375 -4.2784743 -4.290041 -4.295506 -4.2994905 -4.2974391][-4.275641 -4.2667875 -4.2705388 -4.2777891 -4.2838144 -4.2973561 -4.3076921 -4.3097892 -4.3103189 -4.3135023 -4.3209758 -4.3243284 -4.3248196 -4.3215466 -4.311686]]...]
INFO - root - 2017-12-05 12:08:10.348094: step 9610, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 84h:16m:42s remains)
INFO - root - 2017-12-05 12:08:19.529782: step 9620, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 88h:08m:41s remains)
INFO - root - 2017-12-05 12:08:29.137555: step 9630, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 83h:22m:38s remains)
INFO - root - 2017-12-05 12:08:38.601700: step 9640, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 86h:49m:35s remains)
INFO - root - 2017-12-05 12:08:47.930670: step 9650, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 83h:29m:39s remains)
INFO - root - 2017-12-05 12:08:57.362329: step 9660, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 84h:38m:00s remains)
INFO - root - 2017-12-05 12:09:06.671804: step 9670, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.826 sec/batch; 74h:05m:38s remains)
INFO - root - 2017-12-05 12:09:16.240282: step 9680, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 85h:59m:33s remains)
INFO - root - 2017-12-05 12:09:25.634209: step 9690, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 83h:33m:04s remains)
INFO - root - 2017-12-05 12:09:35.006945: step 9700, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 85h:46m:09s remains)
2017-12-05 12:09:35.807958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2486606 -4.2642846 -4.2689257 -4.2704787 -4.2721362 -4.2708817 -4.2770734 -4.26976 -4.2550688 -4.2547679 -4.2624073 -4.2773008 -4.3009057 -4.3150096 -4.3143573][-4.2134471 -4.2386284 -4.2416153 -4.23905 -4.23523 -4.2300634 -4.23343 -4.2168655 -4.1948366 -4.2001886 -4.21991 -4.2492146 -4.2863874 -4.3074117 -4.3094378][-4.1996417 -4.2288704 -4.2254586 -4.2135592 -4.1999207 -4.1851177 -4.1765695 -4.1389 -4.1035404 -4.1203928 -4.15821 -4.2064862 -4.2568383 -4.2872524 -4.297019][-4.1913266 -4.22107 -4.21125 -4.1876025 -4.1634822 -4.1369 -4.1140046 -4.047317 -3.9904423 -4.0227566 -4.0826869 -4.1511097 -4.2178707 -4.2607508 -4.2822433][-4.1907229 -4.2207503 -4.2090158 -4.1759176 -4.1412888 -4.1039977 -4.0579824 -3.9509416 -3.865298 -3.9296877 -4.023396 -4.1070261 -4.1862469 -4.2392559 -4.2703161][-4.2103572 -4.2317691 -4.2178888 -4.1808605 -4.1383882 -4.0854912 -4.0049615 -3.8387611 -3.7164764 -3.8371828 -3.9839199 -4.0829597 -4.170783 -4.2296286 -4.2633305][-4.2203221 -4.2315125 -4.2236958 -4.1963387 -4.1535559 -4.0816 -3.9551914 -3.7285404 -3.5757356 -3.7593648 -3.9568014 -4.0708003 -4.1639495 -4.2287149 -4.2642951][-4.2246261 -4.2285113 -4.2273731 -4.2166247 -4.1808167 -4.1050673 -3.9757586 -3.7739539 -3.6582923 -3.8252141 -3.9998319 -4.0931511 -4.1738291 -4.2354922 -4.26976][-4.2303133 -4.2283893 -4.2308035 -4.2311 -4.2054691 -4.1365161 -4.0225229 -3.8653076 -3.7933292 -3.925384 -4.0576944 -4.1279888 -4.1909304 -4.2447486 -4.2748504][-4.2218823 -4.2130337 -4.2193985 -4.230516 -4.2205868 -4.1655679 -4.0713391 -3.94748 -3.9036264 -4.0112834 -4.1098056 -4.1636848 -4.214798 -4.2600093 -4.2816191][-4.21041 -4.1926293 -4.2066855 -4.2325754 -4.2371669 -4.2035456 -4.1374936 -4.0457115 -4.0164757 -4.0948329 -4.1628561 -4.2041664 -4.2440543 -4.2791548 -4.2911234][-4.2125025 -4.1917977 -4.2119503 -4.2467113 -4.2608933 -4.248909 -4.210958 -4.1474586 -4.1245232 -4.1731071 -4.2151828 -4.2453108 -4.2761669 -4.3002348 -4.30301][-4.2140918 -4.1941657 -4.2184758 -4.2566042 -4.2757697 -4.2796636 -4.261982 -4.2170854 -4.1964045 -4.2238536 -4.2507467 -4.2727151 -4.29317 -4.3100977 -4.3083925][-4.2112417 -4.1983747 -4.2264318 -4.2658687 -4.2897449 -4.3013573 -4.2927022 -4.2586827 -4.2389736 -4.2543731 -4.2729554 -4.287456 -4.2987785 -4.3095255 -4.3058043][-4.2388978 -4.2323613 -4.2562675 -4.2874203 -4.3084574 -4.3196735 -4.3137617 -4.2879133 -4.2691603 -4.275692 -4.285491 -4.2902641 -4.2964983 -4.3028464 -4.2984486]]...]
INFO - root - 2017-12-05 12:09:45.149923: step 9710, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 88h:03m:44s remains)
INFO - root - 2017-12-05 12:09:54.592623: step 9720, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 87h:35m:39s remains)
INFO - root - 2017-12-05 12:10:03.865047: step 9730, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 85h:59m:01s remains)
INFO - root - 2017-12-05 12:10:13.122141: step 9740, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 81h:39m:11s remains)
INFO - root - 2017-12-05 12:10:22.700420: step 9750, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 88h:04m:23s remains)
INFO - root - 2017-12-05 12:10:31.969447: step 9760, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.947 sec/batch; 84h:55m:58s remains)
INFO - root - 2017-12-05 12:10:41.463287: step 9770, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.016 sec/batch; 91h:05m:10s remains)
INFO - root - 2017-12-05 12:10:50.765972: step 9780, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.953 sec/batch; 85h:24m:20s remains)
INFO - root - 2017-12-05 12:10:59.834158: step 9790, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.921 sec/batch; 82h:33m:13s remains)
INFO - root - 2017-12-05 12:11:08.989383: step 9800, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 79h:27m:27s remains)
2017-12-05 12:11:09.750504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2982378 -4.2953863 -4.2917471 -4.2874627 -4.2851672 -4.286272 -4.2903748 -4.2962451 -4.3017497 -4.3052573 -4.3000674 -4.2905631 -4.2871494 -4.2807837 -4.2723966][-4.28239 -4.2753773 -4.2672582 -4.258534 -4.2537603 -4.2559981 -4.2646079 -4.27681 -4.2894158 -4.2986417 -4.2950163 -4.2854128 -4.2783761 -4.2657347 -4.2520914][-4.2747231 -4.2654371 -4.2526765 -4.2384157 -4.2300563 -4.2330732 -4.2461681 -4.2639737 -4.2830043 -4.2976823 -4.2981029 -4.2916064 -4.2843251 -4.2693219 -4.2513514][-4.2654734 -4.2546258 -4.2361321 -4.2139664 -4.1999 -4.20175 -4.2163935 -4.2365751 -4.259872 -4.2795725 -4.2862573 -4.2861028 -4.2835307 -4.2713642 -4.2509055][-4.2334208 -4.2190309 -4.1930532 -4.1609373 -4.1373777 -4.133935 -4.1464257 -4.165884 -4.1904559 -4.2152944 -4.2326126 -4.2447429 -4.2528653 -4.2499337 -4.2367706][-4.1867085 -4.1707821 -4.140862 -4.1003532 -4.0648847 -4.04974 -4.0511904 -4.0613236 -4.078742 -4.1055183 -4.1365075 -4.1661906 -4.1891227 -4.1990414 -4.1993651][-4.1932211 -4.181829 -4.158051 -4.1205292 -4.0814514 -4.0545735 -4.0385966 -4.0308042 -4.0338769 -4.0556245 -4.0919709 -4.127954 -4.1549778 -4.1689267 -4.1773767][-4.2519183 -4.24392 -4.2303739 -4.2077017 -4.1826143 -4.1621604 -4.14193 -4.1240768 -4.1148443 -4.1203485 -4.139719 -4.1630459 -4.1803331 -4.18531 -4.1899505][-4.2792482 -4.2752719 -4.2694345 -4.259819 -4.2494802 -4.240067 -4.2249923 -4.2064967 -4.1919384 -4.1837416 -4.1854963 -4.1945057 -4.203548 -4.2007895 -4.1992149][-4.2727327 -4.2719841 -4.2714667 -4.2690096 -4.2656379 -4.2632604 -4.2543454 -4.2398391 -4.226439 -4.2135935 -4.2063789 -4.20529 -4.2085605 -4.2028666 -4.198688][-4.2733 -4.2715011 -4.2699909 -4.2663145 -4.2619462 -4.2597609 -4.2547708 -4.2460766 -4.2388167 -4.2312093 -4.225194 -4.2212524 -4.21914 -4.2099071 -4.2039413][-4.2843614 -4.2797127 -4.2742929 -4.2646894 -4.2534194 -4.2459626 -4.241333 -4.23649 -4.2350459 -4.2349057 -4.2344379 -4.2303934 -4.22269 -4.2101021 -4.2045727][-4.3011408 -4.2936187 -4.28509 -4.2711864 -4.2531133 -4.2381349 -4.22845 -4.2239976 -4.2256341 -4.2302909 -4.2333336 -4.2295537 -4.218049 -4.2035913 -4.1998][-4.3211756 -4.3157554 -4.308176 -4.2935367 -4.2712417 -4.2488279 -4.2314391 -4.2224784 -4.2222452 -4.227675 -4.2325449 -4.2287889 -4.2140346 -4.1971669 -4.1940074][-4.3322606 -4.3314881 -4.3292966 -4.320457 -4.3020282 -4.2790012 -4.2571049 -4.2416272 -4.2363434 -4.2381887 -4.2415528 -4.2367444 -4.2210636 -4.2037897 -4.2008986]]...]
INFO - root - 2017-12-05 12:11:19.207203: step 9810, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 84h:51m:23s remains)
INFO - root - 2017-12-05 12:11:28.359433: step 9820, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 76h:19m:20s remains)
INFO - root - 2017-12-05 12:11:37.910888: step 9830, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 83h:56m:14s remains)
INFO - root - 2017-12-05 12:11:47.396010: step 9840, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 83h:14m:28s remains)
INFO - root - 2017-12-05 12:11:56.937234: step 9850, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 84h:53m:07s remains)
INFO - root - 2017-12-05 12:12:06.341714: step 9860, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 84h:05m:26s remains)
INFO - root - 2017-12-05 12:12:15.776611: step 9870, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.921 sec/batch; 82h:33m:41s remains)
INFO - root - 2017-12-05 12:12:25.192417: step 9880, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 83h:05m:05s remains)
INFO - root - 2017-12-05 12:12:34.371160: step 9890, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 73h:41m:52s remains)
INFO - root - 2017-12-05 12:12:43.672296: step 9900, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 73h:30m:22s remains)
2017-12-05 12:12:44.438674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2322283 -4.2404509 -4.2544293 -4.2711325 -4.2860494 -4.29519 -4.2938185 -4.2839117 -4.2756705 -4.2764945 -4.2818284 -4.287869 -4.2880087 -4.2803826 -4.27077][-4.1816831 -4.1893859 -4.2064776 -4.229867 -4.2512021 -4.2628193 -4.2566624 -4.2381992 -4.2225976 -4.220192 -4.2265406 -4.2369628 -4.2423096 -4.2360768 -4.2268877][-4.1365414 -4.1450858 -4.16255 -4.1872396 -4.2106705 -4.2224097 -4.2123919 -4.1871862 -4.1637077 -4.1547484 -4.1595783 -4.1719503 -4.1829071 -4.1807194 -4.1743565][-4.1189351 -4.1247568 -4.1379437 -4.1598468 -4.1820707 -4.1928821 -4.1806183 -4.1517463 -4.1218681 -4.1062207 -4.1101594 -4.1254797 -4.1401234 -4.1425071 -4.1395683][-4.1412597 -4.1358166 -4.135756 -4.1512141 -4.1692343 -4.1787324 -4.1715713 -4.1476035 -4.11191 -4.0890374 -4.0888252 -4.1029096 -4.1185741 -4.1243157 -4.1237073][-4.17926 -4.1623511 -4.1457553 -4.146553 -4.1538358 -4.157578 -4.1586161 -4.1479511 -4.1175275 -4.0948052 -4.0920911 -4.1019268 -4.1126127 -4.1162643 -4.113893][-4.2160149 -4.1905823 -4.1603818 -4.1413655 -4.1271019 -4.1191616 -4.1245241 -4.1280212 -4.1147418 -4.1032052 -4.1043134 -4.1101761 -4.1140637 -4.1109424 -4.101716][-4.2472129 -4.2173285 -4.1752963 -4.1345682 -4.0984974 -4.079195 -4.0864358 -4.1028123 -4.10582 -4.1052709 -4.1117897 -4.1173272 -4.1155987 -4.107193 -4.0922537][-4.2618222 -4.2326059 -4.1902037 -4.14517 -4.1020832 -4.0779715 -4.0857019 -4.1040015 -4.1081758 -4.1051245 -4.1092515 -4.1168103 -4.1195521 -4.1157646 -4.1007586][-4.242703 -4.2159743 -4.18439 -4.1547971 -4.1231313 -4.1083875 -4.1206975 -4.1346927 -4.1268325 -4.1074028 -4.0960307 -4.10223 -4.1176982 -4.1254911 -4.1169767][-4.200768 -4.1783981 -4.1616893 -4.1522202 -4.1402745 -4.1421404 -4.1643081 -4.1770668 -4.1600771 -4.1230845 -4.0917082 -4.0931668 -4.1193719 -4.1361527 -4.1319427][-4.1728878 -4.1515942 -4.1386909 -4.1377559 -4.1399632 -4.1558533 -4.1855178 -4.1976213 -4.1772628 -4.133975 -4.0964489 -4.0947046 -4.1261654 -4.1475654 -4.1426153][-4.1705737 -4.1488361 -4.1312962 -4.1255445 -4.1264763 -4.1440115 -4.1732717 -4.1901326 -4.17627 -4.1411686 -4.1096978 -4.1107016 -4.1430931 -4.1629844 -4.1551342][-4.1856322 -4.162519 -4.1437612 -4.132709 -4.1290159 -4.1397147 -4.16329 -4.1867924 -4.1869082 -4.166832 -4.1435676 -4.1427846 -4.1639142 -4.1752443 -4.16524][-4.1918292 -4.1691055 -4.1522193 -4.14054 -4.1311126 -4.1329656 -4.1497211 -4.1772566 -4.1912422 -4.1839976 -4.1670818 -4.1628551 -4.173636 -4.1776767 -4.16857]]...]
INFO - root - 2017-12-05 12:12:53.905495: step 9910, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 83h:53m:30s remains)
INFO - root - 2017-12-05 12:13:03.255470: step 9920, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 84h:18m:51s remains)
INFO - root - 2017-12-05 12:13:12.742092: step 9930, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 82h:55m:38s remains)
INFO - root - 2017-12-05 12:13:22.138191: step 9940, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.005 sec/batch; 90h:05m:03s remains)
INFO - root - 2017-12-05 12:13:31.262836: step 9950, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 82h:13m:21s remains)
INFO - root - 2017-12-05 12:13:40.851048: step 9960, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.994 sec/batch; 89h:03m:54s remains)
INFO - root - 2017-12-05 12:13:50.260093: step 9970, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.992 sec/batch; 88h:51m:29s remains)
INFO - root - 2017-12-05 12:13:59.703691: step 9980, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.984 sec/batch; 88h:09m:17s remains)
INFO - root - 2017-12-05 12:14:09.076946: step 9990, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 86h:28m:18s remains)
INFO - root - 2017-12-05 12:14:18.469312: step 10000, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 86h:04m:33s remains)
2017-12-05 12:14:19.255673: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2132931 -4.2230153 -4.2385516 -4.2505188 -4.2525568 -4.2466178 -4.2312727 -4.215888 -4.2077184 -4.2048244 -4.2072616 -4.2196064 -4.2394266 -4.2555313 -4.2644777][-4.2287297 -4.2440834 -4.2629485 -4.275167 -4.2776322 -4.271008 -4.2561288 -4.2443886 -4.2413921 -4.2419572 -4.2467318 -4.2531805 -4.2622747 -4.2731004 -4.2806458][-4.2430954 -4.262105 -4.2806568 -4.2902732 -4.2880316 -4.2776532 -4.2637849 -4.2575355 -4.2599058 -4.2669411 -4.2731366 -4.2778049 -4.2828932 -4.2917519 -4.2963881][-4.2489834 -4.2660575 -4.2821236 -4.2860394 -4.2731066 -4.2566543 -4.2400374 -4.2295637 -4.2331691 -4.2532272 -4.2710314 -4.2849603 -4.2908888 -4.2994051 -4.2998872][-4.23695 -4.2454262 -4.2542906 -4.246501 -4.2179494 -4.1869559 -4.1559477 -4.1394558 -4.1532664 -4.1923051 -4.2304344 -4.2572241 -4.2694025 -4.282589 -4.2842383][-4.2204151 -4.2164874 -4.2091036 -4.1840014 -4.1369219 -4.080905 -4.0217218 -4.0003924 -4.0442734 -4.1156392 -4.1760073 -4.2175913 -4.2375956 -4.2609048 -4.2661762][-4.2119188 -4.1901116 -4.1540122 -4.1006932 -4.0286589 -3.9357109 -3.8299146 -3.8110518 -3.9136288 -4.0285282 -4.1059294 -4.1607056 -4.1977735 -4.2364178 -4.2507863][-4.212728 -4.1783113 -4.1186872 -4.041019 -3.9452434 -3.821382 -3.6845756 -3.6852517 -3.8339045 -3.9632597 -4.0373077 -4.0989742 -4.1560607 -4.2112274 -4.2380404][-4.2251911 -4.2033281 -4.1544051 -4.0855246 -3.9965446 -3.8932483 -3.8059928 -3.8247344 -3.9283032 -4.0020962 -4.033453 -4.0743947 -4.1323109 -4.1906633 -4.2214594][-4.2202415 -4.223588 -4.2084856 -4.174262 -4.1178226 -4.0564842 -4.0139661 -4.0262671 -4.068778 -4.0873365 -4.0847597 -4.1037283 -4.1515279 -4.2023015 -4.2279277][-4.1980858 -4.2224784 -4.2352562 -4.2291479 -4.2011404 -4.1655388 -4.1446075 -4.1476417 -4.1550527 -4.1502695 -4.1443887 -4.1616631 -4.2057052 -4.2478938 -4.2655277][-4.1793571 -4.2179847 -4.2408848 -4.241591 -4.2207804 -4.1926723 -4.1780887 -4.1762743 -4.1702433 -4.1594167 -4.1623163 -4.1901441 -4.23965 -4.283421 -4.2972679][-4.1877103 -4.2253618 -4.2410522 -4.2362475 -4.2132759 -4.1819639 -4.1617351 -4.158567 -4.1515751 -4.1454549 -4.1573744 -4.1890044 -4.2390313 -4.2851462 -4.3018041][-4.2004457 -4.2321281 -4.2415886 -4.2293463 -4.2031417 -4.1680946 -4.1452236 -4.1416984 -4.1399155 -4.1385651 -4.1527853 -4.1817861 -4.2245593 -4.2671061 -4.2902083][-4.2029214 -4.23294 -4.2420883 -4.2272663 -4.2016411 -4.166863 -4.1429834 -4.1384335 -4.1371932 -4.1326652 -4.1398382 -4.1635008 -4.2025266 -4.241364 -4.2708406]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 12:14:29.289352: step 10010, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.999 sec/batch; 89h:31m:19s remains)
INFO - root - 2017-12-05 12:14:38.521243: step 10020, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 89h:32m:02s remains)
INFO - root - 2017-12-05 12:14:48.110992: step 10030, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 85h:26m:03s remains)
INFO - root - 2017-12-05 12:14:57.403679: step 10040, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 82h:04m:38s remains)
INFO - root - 2017-12-05 12:15:06.805575: step 10050, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 81h:31m:37s remains)
INFO - root - 2017-12-05 12:15:16.015629: step 10060, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 78h:29m:31s remains)
INFO - root - 2017-12-05 12:15:25.382344: step 10070, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.883 sec/batch; 79h:02m:40s remains)
INFO - root - 2017-12-05 12:15:34.951084: step 10080, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 85h:30m:36s remains)
INFO - root - 2017-12-05 12:15:44.366290: step 10090, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.959 sec/batch; 85h:54m:21s remains)
INFO - root - 2017-12-05 12:15:53.718269: step 10100, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.988 sec/batch; 88h:27m:00s remains)
2017-12-05 12:15:54.545063: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2173619 -4.2268367 -4.2609658 -4.2987347 -4.3209729 -4.3335152 -4.3331804 -4.3234496 -4.3099422 -4.2998242 -4.30265 -4.3154421 -4.3259025 -4.3279004 -4.3236761][-4.2048464 -4.2060146 -4.2343731 -4.2719588 -4.2983861 -4.3170977 -4.3219771 -4.3121214 -4.2946906 -4.2795954 -4.2791882 -4.2918129 -4.3050776 -4.31149 -4.3118663][-4.1945329 -4.1837497 -4.1994696 -4.2300539 -4.2579751 -4.2825627 -4.2944388 -4.2890997 -4.274158 -4.2609472 -4.2589421 -4.2679844 -4.2805724 -4.2916646 -4.2989211][-4.1823311 -4.1592488 -4.1587877 -4.1751423 -4.1960835 -4.21728 -4.2329674 -4.2379732 -4.2353187 -4.2332759 -4.2353606 -4.242619 -4.2529378 -4.2674851 -4.2804675][-4.1667337 -4.1322188 -4.1160979 -4.1149793 -4.1162796 -4.1204662 -4.1345763 -4.1535826 -4.1743293 -4.1939244 -4.2098689 -4.2235217 -4.2377224 -4.2536297 -4.2660637][-4.1583734 -4.1103225 -4.077611 -4.0561953 -4.027566 -4.0014672 -4.00443 -4.0414863 -4.0935183 -4.141995 -4.1834087 -4.2154574 -4.2371712 -4.2498178 -4.2561507][-4.162044 -4.1036758 -4.0571318 -4.0154119 -3.9527946 -3.883579 -3.8670104 -3.9256957 -4.0154047 -4.0957437 -4.1643267 -4.2173533 -4.2471275 -4.2549567 -4.2551923][-4.1728172 -4.1128411 -4.0618939 -4.0081739 -3.9191177 -3.8110161 -3.7728946 -3.8436623 -3.9572239 -4.0588083 -4.1457586 -4.212461 -4.248939 -4.2559266 -4.2556534][-4.1774426 -4.1286969 -4.0878482 -4.0426922 -3.9641328 -3.8625298 -3.8172669 -3.8641534 -3.9595129 -4.0525122 -4.1369662 -4.2037482 -4.2391396 -4.2452788 -4.2465143][-4.1618218 -4.1336436 -4.1164646 -4.0975304 -4.0536489 -3.988822 -3.9527245 -3.9690554 -4.026094 -4.0903273 -4.1501079 -4.19952 -4.2220926 -4.2220888 -4.2232366][-4.1389828 -4.1312475 -4.140573 -4.150362 -4.1377997 -4.1073217 -4.0844364 -4.0841756 -4.1109681 -4.1419396 -4.1683178 -4.1917419 -4.1961594 -4.1860008 -4.1813416][-4.1274114 -4.1316371 -4.1602926 -4.19088 -4.1963782 -4.1868272 -4.175046 -4.1686378 -4.1750116 -4.1813664 -4.1844172 -4.1851096 -4.1719403 -4.14976 -4.1363835][-4.1410294 -4.1526356 -4.1908216 -4.2295637 -4.2411447 -4.2360706 -4.223928 -4.210268 -4.202858 -4.195363 -4.1832914 -4.1724792 -4.1543789 -4.1305757 -4.1139936][-4.18901 -4.2025175 -4.237946 -4.2715168 -4.2760019 -4.262907 -4.24212 -4.2171283 -4.1953754 -4.1760879 -4.1577449 -4.1427755 -4.1312513 -4.1220803 -4.1184964][-4.2451344 -4.2565145 -4.2841887 -4.3061013 -4.3013482 -4.2833104 -4.2573466 -4.222538 -4.1853371 -4.1562371 -4.1354423 -4.1216345 -4.1214042 -4.132267 -4.1476836]]...]
INFO - root - 2017-12-05 12:16:04.124981: step 10110, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 83h:45m:11s remains)
INFO - root - 2017-12-05 12:16:13.571325: step 10120, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 76h:41m:42s remains)
INFO - root - 2017-12-05 12:16:22.989440: step 10130, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 83h:39m:01s remains)
INFO - root - 2017-12-05 12:16:32.512110: step 10140, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 79h:46m:03s remains)
INFO - root - 2017-12-05 12:16:41.911048: step 10150, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 84h:11m:08s remains)
INFO - root - 2017-12-05 12:16:51.215529: step 10160, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 81h:42m:02s remains)
INFO - root - 2017-12-05 12:17:00.502925: step 10170, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 81h:29m:53s remains)
INFO - root - 2017-12-05 12:17:09.939311: step 10180, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 83h:03m:45s remains)
INFO - root - 2017-12-05 12:17:19.392894: step 10190, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 86h:58m:14s remains)
INFO - root - 2017-12-05 12:17:28.824033: step 10200, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 78h:16m:10s remains)
2017-12-05 12:17:29.597942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2868958 -4.2825885 -4.2810335 -4.2835927 -4.2925029 -4.3005605 -4.3073115 -4.3139472 -4.3196268 -4.3234811 -4.3270574 -4.3333216 -4.3382459 -4.3391294 -4.3350577][-4.3033242 -4.3051219 -4.3085361 -4.31233 -4.3183064 -4.3213305 -4.3217816 -4.319694 -4.3176274 -4.31806 -4.3210225 -4.3269954 -4.3325009 -4.33428 -4.33027][-4.3159184 -4.3193769 -4.3216815 -4.3239613 -4.3267565 -4.3240938 -4.315876 -4.3048239 -4.2972689 -4.296793 -4.3016076 -4.3088155 -4.3156419 -4.3195977 -4.3157473][-4.3170829 -4.3213782 -4.323245 -4.3241167 -4.3225203 -4.3128314 -4.2941451 -4.2725072 -4.2593961 -4.2622762 -4.2746563 -4.28549 -4.2933378 -4.2972107 -4.2909107][-4.3063493 -4.3143392 -4.3177366 -4.3170176 -4.30952 -4.2905869 -4.2586374 -4.2218132 -4.2007475 -4.2103343 -4.2367234 -4.2559385 -4.2653804 -4.2661333 -4.2564821][-4.2937546 -4.3053856 -4.307374 -4.2996478 -4.2818637 -4.249382 -4.2003021 -4.1446652 -4.1198215 -4.1454415 -4.1935096 -4.2283649 -4.2425094 -4.2409387 -4.2277508][-4.2807975 -4.2927294 -4.2883544 -4.2681284 -4.2349057 -4.1854587 -4.1168227 -4.0436482 -4.0247579 -4.077323 -4.1541486 -4.2100239 -4.23201 -4.2288165 -4.2111783][-4.2694921 -4.2787342 -4.2659812 -4.2326341 -4.1827369 -4.1193423 -4.038517 -3.9598966 -3.9599514 -4.0410085 -4.1392488 -4.2081404 -4.2317462 -4.2230449 -4.1997018][-4.2646432 -4.2683558 -4.2483549 -4.2062831 -4.1493578 -4.0878744 -4.0188341 -3.9605567 -3.9824789 -4.0697761 -4.1639161 -4.2254066 -4.2398014 -4.2202373 -4.1935434][-4.2771 -4.2742548 -4.2509651 -4.20907 -4.1584558 -4.1148367 -4.0749578 -4.0497141 -4.0773749 -4.1432791 -4.2098365 -4.2512374 -4.2497606 -4.2200613 -4.1960926][-4.2996378 -4.2930121 -4.2733331 -4.2404814 -4.2019572 -4.1752648 -4.1596131 -4.1545405 -4.1753721 -4.2142844 -4.250452 -4.2717142 -4.2609825 -4.2315536 -4.2139187][-4.3160958 -4.3095064 -4.2983484 -4.2769794 -4.2527356 -4.2381172 -4.2350812 -4.2367253 -4.2476616 -4.2672729 -4.2846289 -4.2935462 -4.2828889 -4.2627072 -4.2527637][-4.3209629 -4.3176818 -4.3143535 -4.3027592 -4.2895837 -4.2816739 -4.2808614 -4.2823057 -4.2868233 -4.2985625 -4.3098459 -4.3164916 -4.3125267 -4.3050108 -4.2991214][-4.3178782 -4.3169174 -4.3156796 -4.3073931 -4.2986794 -4.2942615 -4.294405 -4.2963672 -4.2983923 -4.3069792 -4.317553 -4.3281732 -4.3324046 -4.3339763 -4.3317347][-4.3065982 -4.3050637 -4.2999711 -4.2886391 -4.2770977 -4.27122 -4.2718267 -4.2746105 -4.2779121 -4.28714 -4.2997942 -4.3168569 -4.3270111 -4.3328919 -4.3344197]]...]
INFO - root - 2017-12-05 12:17:38.975576: step 10210, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 75h:17m:35s remains)
INFO - root - 2017-12-05 12:17:48.418763: step 10220, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.937 sec/batch; 83h:53m:11s remains)
INFO - root - 2017-12-05 12:17:57.855249: step 10230, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.943 sec/batch; 84h:24m:58s remains)
INFO - root - 2017-12-05 12:18:07.319541: step 10240, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 83h:44m:23s remains)
INFO - root - 2017-12-05 12:18:16.744912: step 10250, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 83h:38m:24s remains)
INFO - root - 2017-12-05 12:18:26.011277: step 10260, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 83h:18m:30s remains)
INFO - root - 2017-12-05 12:18:35.154146: step 10270, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 83h:50m:06s remains)
INFO - root - 2017-12-05 12:18:44.587745: step 10280, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 82h:34m:49s remains)
INFO - root - 2017-12-05 12:18:53.888261: step 10290, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 83h:17m:06s remains)
INFO - root - 2017-12-05 12:19:03.267154: step 10300, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 80h:40m:19s remains)
2017-12-05 12:19:04.050705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3527894 -4.3573551 -4.3552575 -4.3441916 -4.328054 -4.3103671 -4.2983184 -4.2996469 -4.3106332 -4.3230767 -4.3326216 -4.3372383 -4.3396912 -4.3402672 -4.3411789][-4.3586864 -4.3632541 -4.3592653 -4.3438106 -4.3207979 -4.2937741 -4.2717843 -4.2675061 -4.2815762 -4.3030124 -4.3208075 -4.3323221 -4.3399019 -4.3439841 -4.345809][-4.3602948 -4.3645587 -4.3592653 -4.3403358 -4.3101182 -4.2698579 -4.2298765 -4.2118177 -4.2244987 -4.2563267 -4.2882457 -4.3131242 -4.3305173 -4.3400121 -4.3437109][-4.3593445 -4.364614 -4.3604021 -4.3391366 -4.2997465 -4.2421818 -4.1779418 -4.137394 -4.1420312 -4.1819949 -4.2313242 -4.2753315 -4.3074889 -4.3261967 -4.334702][-4.3598337 -4.3667717 -4.3642349 -4.3392634 -4.2872915 -4.2062979 -4.1119442 -4.0446081 -4.0381207 -4.0844655 -4.15307 -4.2217336 -4.2754545 -4.3076267 -4.323894][-4.3565378 -4.3654189 -4.364059 -4.3352795 -4.2696309 -4.1628084 -4.0349908 -3.9401903 -3.9209237 -3.9756186 -4.0675244 -4.1645422 -4.2395811 -4.2858958 -4.3119307][-4.3376393 -4.3489528 -4.3487873 -4.3176312 -4.2443342 -4.1179094 -3.9618804 -3.8390131 -3.8057718 -3.8746243 -3.9973826 -4.1237988 -4.21862 -4.2749653 -4.3063025][-4.3001828 -4.3135133 -4.3161583 -4.2879248 -4.2197304 -4.0975509 -3.944102 -3.8171153 -3.7746596 -3.843051 -3.9775536 -4.1173759 -4.2208042 -4.2805281 -4.3124604][-4.2362676 -4.256093 -4.2693319 -4.2573271 -4.2120376 -4.1218882 -4.007328 -3.9065592 -3.8614316 -3.9074874 -4.022779 -4.1501722 -4.2441182 -4.2978415 -4.323854][-4.1679645 -4.2024708 -4.233674 -4.24519 -4.2262096 -4.16997 -4.09447 -4.0216904 -3.9823344 -4.0102825 -4.0972853 -4.1994572 -4.2755213 -4.3165755 -4.3346963][-4.144763 -4.1947856 -4.2382336 -4.2624712 -4.2560649 -4.2184458 -4.1670523 -4.1163635 -4.0897551 -4.1101546 -4.1741548 -4.2497325 -4.3046441 -4.3305373 -4.3405266][-4.1840954 -4.235549 -4.2774215 -4.2994347 -4.2961254 -4.2689538 -4.2349319 -4.204309 -4.1891584 -4.2015057 -4.2435288 -4.2919035 -4.3261023 -4.340313 -4.3441596][-4.2520337 -4.2916031 -4.3216429 -4.3356624 -4.3331079 -4.316968 -4.2958817 -4.276875 -4.2668653 -4.2707367 -4.294559 -4.3223429 -4.3397694 -4.34572 -4.3464708][-4.3095651 -4.3326817 -4.3474059 -4.3527794 -4.350215 -4.3417668 -4.33056 -4.3199811 -4.3131042 -4.3138409 -4.3262253 -4.3395748 -4.3457355 -4.3467774 -4.3465843][-4.3387542 -4.34806 -4.3535852 -4.3560691 -4.3549919 -4.3520417 -4.3469343 -4.3403945 -4.3345156 -4.3326144 -4.3373094 -4.3423514 -4.3442564 -4.3445368 -4.344861]]...]
INFO - root - 2017-12-05 12:19:13.427668: step 10310, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 84h:27m:19s remains)
INFO - root - 2017-12-05 12:19:22.880403: step 10320, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 84h:02m:16s remains)
INFO - root - 2017-12-05 12:19:32.050835: step 10330, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 86h:17m:06s remains)
INFO - root - 2017-12-05 12:19:41.588797: step 10340, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 87h:17m:08s remains)
INFO - root - 2017-12-05 12:19:51.009195: step 10350, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 85h:40m:17s remains)
INFO - root - 2017-12-05 12:20:00.482470: step 10360, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 85h:00m:39s remains)
INFO - root - 2017-12-05 12:20:09.859615: step 10370, loss = 2.04, batch loss = 1.98 (7.8 examples/sec; 1.032 sec/batch; 92h:19m:52s remains)
INFO - root - 2017-12-05 12:20:19.475713: step 10380, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 84h:30m:39s remains)
INFO - root - 2017-12-05 12:20:28.855906: step 10390, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 87h:43m:09s remains)
INFO - root - 2017-12-05 12:20:38.289664: step 10400, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 83h:53m:47s remains)
2017-12-05 12:20:39.075414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3411965 -4.3441534 -4.3428159 -4.3325462 -4.3102889 -4.2829914 -4.26173 -4.2582979 -4.2772574 -4.3059664 -4.3269143 -4.3387914 -4.3428707 -4.3424568 -4.3410673][-4.3434873 -4.3462892 -4.342659 -4.3281183 -4.301477 -4.2700305 -4.247129 -4.2456346 -4.2691083 -4.3018503 -4.3260264 -4.3406796 -4.3453546 -4.3441834 -4.3419924][-4.345643 -4.3473072 -4.3395805 -4.3194947 -4.2876692 -4.2504654 -4.2238913 -4.2226858 -4.25018 -4.2878671 -4.3167892 -4.3360219 -4.3443022 -4.3447318 -4.3429341][-4.3425393 -4.3419924 -4.330049 -4.3048825 -4.26571 -4.2214532 -4.1921067 -4.1934185 -4.2264938 -4.2701273 -4.3050938 -4.3294282 -4.3418274 -4.3451138 -4.3449965][-4.3240681 -4.3205791 -4.302629 -4.2676978 -4.2177114 -4.1678438 -4.1387029 -4.1445866 -4.1832933 -4.233964 -4.2780495 -4.3109255 -4.3318911 -4.3419371 -4.345767][-4.2846723 -4.2760763 -4.2494383 -4.201932 -4.1415806 -4.089592 -4.0651965 -4.0751777 -4.1151352 -4.1693726 -4.2240877 -4.2705417 -4.305141 -4.3271027 -4.3390808][-4.2316504 -4.2155185 -4.1784763 -4.119421 -4.054985 -4.0092998 -3.9942055 -4.0074139 -4.0410113 -4.0881343 -4.1504073 -4.2137156 -4.2661119 -4.3033948 -4.3255816][-4.1884937 -4.1641183 -4.1199346 -4.0566707 -3.9957602 -3.9610989 -3.9565306 -3.9700012 -3.9901242 -4.0202684 -4.0814848 -4.1562967 -4.225203 -4.2778869 -4.31123][-4.1714554 -4.1424623 -4.0984397 -4.042881 -3.9933522 -3.9688444 -3.9706104 -3.9790671 -3.9833417 -3.9966993 -4.050951 -4.1261339 -4.2011542 -4.2618794 -4.3021131][-4.1753836 -4.1455388 -4.1071587 -4.0653796 -4.030221 -4.0149336 -4.0180173 -4.0193543 -4.011529 -4.0138922 -4.060308 -4.1271734 -4.1992488 -4.2606034 -4.3018203][-4.1961951 -4.1694093 -4.1399112 -4.1113482 -4.08934 -4.0816412 -4.0849886 -4.0810246 -4.0661507 -4.0618587 -4.096313 -4.1503425 -4.2126317 -4.2685404 -4.3057585][-4.2300611 -4.2118006 -4.1940894 -4.1781054 -4.167201 -4.1653562 -4.1684732 -4.1627727 -4.1462212 -4.1378736 -4.1588683 -4.1973162 -4.2440529 -4.286046 -4.31324][-4.26055 -4.2503343 -4.24226 -4.2372146 -4.2360563 -4.2392592 -4.243154 -4.2386317 -4.2254505 -4.2163277 -4.2265396 -4.2508488 -4.2810736 -4.3071761 -4.3221045][-4.2853737 -4.2796297 -4.2761965 -4.2770448 -4.2810497 -4.2864194 -4.2900519 -4.2876682 -4.2793531 -4.2715678 -4.27462 -4.2888589 -4.3073831 -4.3213034 -4.3269534][-4.3060255 -4.3021579 -4.2998991 -4.301168 -4.3048048 -4.3084321 -4.3104634 -4.30916 -4.3047156 -4.3002834 -4.3008347 -4.3082633 -4.3183284 -4.3248363 -4.3251343]]...]
INFO - root - 2017-12-05 12:20:48.396997: step 10410, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 74h:44m:28s remains)
INFO - root - 2017-12-05 12:20:57.765860: step 10420, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 82h:12m:17s remains)
INFO - root - 2017-12-05 12:21:07.137145: step 10430, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 84h:19m:27s remains)
INFO - root - 2017-12-05 12:21:16.413803: step 10440, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 82h:00m:01s remains)
INFO - root - 2017-12-05 12:21:25.795123: step 10450, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 86h:16m:20s remains)
INFO - root - 2017-12-05 12:21:35.092706: step 10460, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.933 sec/batch; 83h:27m:23s remains)
INFO - root - 2017-12-05 12:21:44.547567: step 10470, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 86h:18m:30s remains)
INFO - root - 2017-12-05 12:21:53.905869: step 10480, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 78h:46m:42s remains)
INFO - root - 2017-12-05 12:22:03.243377: step 10490, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 81h:10m:53s remains)
INFO - root - 2017-12-05 12:22:12.637548: step 10500, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 84h:56m:54s remains)
2017-12-05 12:22:13.389330: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0147047 -4.0279646 -4.0717425 -4.111268 -4.1214867 -4.0944815 -4.0687251 -4.0795164 -4.11717 -4.165184 -4.2046342 -4.2262583 -4.2355604 -4.242897 -4.2622313][-3.9849606 -3.9988592 -4.0557823 -4.1102371 -4.1361251 -4.1181254 -4.0864263 -4.0889959 -4.1268525 -4.1759982 -4.2126617 -4.2318931 -4.240449 -4.2489457 -4.2673149][-3.9831984 -3.9974155 -4.0592518 -4.1187954 -4.1508789 -4.1382837 -4.1084743 -4.1063466 -4.1411161 -4.1891618 -4.2216597 -4.2353144 -4.2425785 -4.2533937 -4.2725453][-4.0104604 -4.0152674 -4.0681858 -4.118948 -4.1395178 -4.12533 -4.1063471 -4.1122928 -4.1456704 -4.1900487 -4.2211556 -4.2345281 -4.241622 -4.25416 -4.2754416][-4.053463 -4.0522461 -4.0868926 -4.114315 -4.1119471 -4.091238 -4.080965 -4.1024857 -4.1421504 -4.1840878 -4.2142038 -4.2308187 -4.2399688 -4.2536 -4.2755604][-4.0983782 -4.1029654 -4.1183968 -4.1170545 -4.0931225 -4.0628219 -4.0588255 -4.092463 -4.1321797 -4.1685543 -4.1968527 -4.2192397 -4.2335491 -4.2504487 -4.2743597][-4.1347489 -4.14505 -4.1436477 -4.1214213 -4.0835137 -4.0506744 -4.0561132 -4.0918169 -4.1218696 -4.14674 -4.1730709 -4.20012 -4.2191439 -4.2415552 -4.2708941][-4.1678333 -4.1767788 -4.1618247 -4.1277833 -4.0913553 -4.0687828 -4.080584 -4.1074095 -4.1254325 -4.1380625 -4.1585054 -4.1831913 -4.2032146 -4.2297511 -4.2654762][-4.2002392 -4.2041922 -4.1819177 -4.1440072 -4.1138587 -4.1040993 -4.118525 -4.137126 -4.1434369 -4.1474652 -4.1583562 -4.1736007 -4.1889329 -4.21798 -4.2594352][-4.2136812 -4.208087 -4.1860256 -4.15383 -4.1332493 -4.1320004 -4.1463614 -4.1588125 -4.1599646 -4.1620312 -4.1641631 -4.1657043 -4.1753469 -4.2043166 -4.2503977][-4.1883955 -4.180203 -4.1672096 -4.1509395 -4.1423965 -4.1480327 -4.1628404 -4.1738791 -4.1682296 -4.1627183 -4.1579838 -4.1524048 -4.1591582 -4.1880846 -4.2377043][-4.1419163 -4.1309214 -4.1264038 -4.1314921 -4.1401367 -4.1542878 -4.1699228 -4.1790328 -4.1681738 -4.1525774 -4.1395779 -4.1334167 -4.1443996 -4.1768875 -4.2262635][-4.0981016 -4.0779905 -4.0805 -4.1063609 -4.1319141 -4.1530786 -4.16875 -4.1777558 -4.1643753 -4.1378713 -4.1170015 -4.1151257 -4.1340632 -4.1713166 -4.2202992][-4.0848608 -4.0572324 -4.0629387 -4.097157 -4.1249352 -4.1399007 -4.1490984 -4.16042 -4.1531367 -4.1281586 -4.1102753 -4.1132092 -4.1352992 -4.1723342 -4.2177591][-4.0960751 -4.0682626 -4.0721397 -4.099041 -4.1129866 -4.1125469 -4.119545 -4.1418939 -4.1475763 -4.1328635 -4.1235847 -4.1296797 -4.1472826 -4.1771975 -4.2168541]]...]
INFO - root - 2017-12-05 12:22:22.905349: step 10510, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 82h:09m:52s remains)
INFO - root - 2017-12-05 12:22:32.138380: step 10520, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 82h:38m:49s remains)
INFO - root - 2017-12-05 12:22:41.278983: step 10530, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 78h:59m:05s remains)
INFO - root - 2017-12-05 12:22:50.810629: step 10540, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.973 sec/batch; 87h:02m:04s remains)
INFO - root - 2017-12-05 12:23:00.068387: step 10550, loss = 2.05, batch loss = 1.99 (7.8 examples/sec; 1.024 sec/batch; 91h:35m:49s remains)
INFO - root - 2017-12-05 12:23:09.215765: step 10560, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 82h:55m:59s remains)
INFO - root - 2017-12-05 12:23:18.557517: step 10570, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 82h:44m:04s remains)
INFO - root - 2017-12-05 12:23:27.970174: step 10580, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 83h:45m:45s remains)
INFO - root - 2017-12-05 12:23:37.420752: step 10590, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 87h:53m:31s remains)
INFO - root - 2017-12-05 12:23:46.905012: step 10600, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.991 sec/batch; 88h:36m:31s remains)
2017-12-05 12:23:47.689476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1596866 -4.0809412 -4.0129585 -3.9856961 -4.0464783 -4.1118226 -4.1312032 -4.124299 -4.1145239 -4.1208754 -4.1514077 -4.1725616 -4.1820741 -4.1883841 -4.1834512][-4.1595268 -4.0911632 -4.0360327 -4.0149069 -4.0597024 -4.0982461 -4.0972552 -4.0857658 -4.07745 -4.0953703 -4.14279 -4.1759906 -4.1933122 -4.2055583 -4.2110538][-4.1741872 -4.1245651 -4.09011 -4.08019 -4.1071897 -4.1211677 -4.10494 -4.090023 -4.0864496 -4.1125221 -4.1658697 -4.2034473 -4.2211924 -4.2290778 -4.2334619][-4.2069063 -4.1736774 -4.1554222 -4.1531844 -4.16618 -4.1634912 -4.1481042 -4.140646 -4.142241 -4.1641994 -4.2043839 -4.2334352 -4.2439847 -4.2458873 -4.2468429][-4.2349772 -4.206697 -4.1938834 -4.1954322 -4.2006168 -4.1943884 -4.1911168 -4.2031612 -4.2203217 -4.2329059 -4.2423344 -4.2462416 -4.2421265 -4.2395563 -4.2430711][-4.2513409 -4.2189994 -4.19919 -4.1935797 -4.188621 -4.1784973 -4.1806421 -4.2055025 -4.239306 -4.2555809 -4.2458696 -4.2277141 -4.2144732 -4.2133923 -4.2276454][-4.2450132 -4.199152 -4.1650887 -4.1433544 -4.121408 -4.1015368 -4.0995545 -4.1290541 -4.1782174 -4.2018189 -4.1919217 -4.1771326 -4.1747971 -4.1909981 -4.2218771][-4.21484 -4.1563406 -4.1098785 -4.0718946 -4.0357156 -4.00641 -3.9971697 -4.0229363 -4.0804954 -4.1088619 -4.1044645 -4.1105957 -4.1390676 -4.1825666 -4.2300324][-4.1814981 -4.1144271 -4.0658765 -4.0258608 -3.9875569 -3.9546168 -3.9329741 -3.9432192 -3.9982545 -4.0252166 -4.0246921 -4.0579987 -4.1233668 -4.1894131 -4.2418585][-4.1695571 -4.1000252 -4.0532184 -4.0166092 -3.9847691 -3.9605694 -3.940872 -3.9416025 -3.9805517 -3.9906607 -3.9927468 -4.0539956 -4.1496739 -4.2207685 -4.2586141][-4.1825557 -4.1223278 -4.0856371 -4.05907 -4.0428562 -4.0359349 -4.0332756 -4.0313435 -4.0468478 -4.0408263 -4.0402079 -4.1051474 -4.2006 -4.2590508 -4.2695017][-4.2032795 -4.1591058 -4.1387544 -4.1286712 -4.1263585 -4.1319141 -4.1373768 -4.133688 -4.1427717 -4.1443954 -4.1488352 -4.1915035 -4.2537022 -4.2816367 -4.2651834][-4.2181473 -4.1857009 -4.1778431 -4.1843615 -4.1936154 -4.2010765 -4.2035861 -4.1989012 -4.208384 -4.2191048 -4.2312241 -4.2566152 -4.2844768 -4.2856493 -4.25275][-4.2298846 -4.2031665 -4.2013617 -4.2162471 -4.2317762 -4.23925 -4.2387743 -4.2343125 -4.2429571 -4.25628 -4.27165 -4.2892919 -4.296617 -4.2823339 -4.2452054][-4.2335596 -4.2091274 -4.210393 -4.228591 -4.2476468 -4.2539935 -4.2535877 -4.2532663 -4.2627816 -4.276228 -4.2875781 -4.2991452 -4.2991819 -4.2808032 -4.24675]]...]
INFO - root - 2017-12-05 12:23:56.987960: step 10610, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 84h:02m:31s remains)
INFO - root - 2017-12-05 12:24:06.197879: step 10620, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.002 sec/batch; 89h:37m:32s remains)
INFO - root - 2017-12-05 12:24:15.525205: step 10630, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 85h:11m:31s remains)
INFO - root - 2017-12-05 12:24:25.080222: step 10640, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 81h:25m:47s remains)
INFO - root - 2017-12-05 12:24:34.433578: step 10650, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 82h:35m:59s remains)
INFO - root - 2017-12-05 12:24:43.749340: step 10660, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 82h:12m:44s remains)
INFO - root - 2017-12-05 12:24:53.119834: step 10670, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 82h:29m:42s remains)
INFO - root - 2017-12-05 12:25:02.556414: step 10680, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 82h:16m:46s remains)
INFO - root - 2017-12-05 12:25:12.092468: step 10690, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 85h:42m:29s remains)
INFO - root - 2017-12-05 12:25:21.490023: step 10700, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 83h:33m:28s remains)
2017-12-05 12:25:22.230219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.180223 -4.1987524 -4.2218385 -4.2430253 -4.2587347 -4.2697158 -4.2777157 -4.2805843 -4.2767282 -4.2699575 -4.2627387 -4.2590818 -4.257237 -4.2560239 -4.2552228][-4.1945591 -4.2128129 -4.2349463 -4.25479 -4.2683082 -4.2766867 -4.2822862 -4.2839956 -4.2797375 -4.2723188 -4.2637858 -4.2583981 -4.2557468 -4.2548642 -4.2560563][-4.2207122 -4.2358794 -4.2529969 -4.2670927 -4.2746973 -4.2786083 -4.2812753 -4.2813077 -4.2767034 -4.2688231 -4.2598844 -4.25407 -4.2521329 -4.2526889 -4.2559395][-4.2434545 -4.2527022 -4.2620392 -4.2679949 -4.269454 -4.2697043 -4.2698627 -4.2692819 -4.2658553 -4.2595954 -4.2519364 -4.246943 -4.2459612 -4.2476983 -4.2523565][-4.2576666 -4.2587881 -4.2586713 -4.2556663 -4.2512603 -4.2486305 -4.2479706 -4.2489138 -4.2495432 -4.2484589 -4.2449818 -4.2427945 -4.243701 -4.2464061 -4.251524][-4.2660847 -4.2594709 -4.2506042 -4.2395024 -4.23002 -4.2253361 -4.2254586 -4.2299528 -4.2365184 -4.2421255 -4.2446909 -4.2462292 -4.2493029 -4.2529664 -4.2576704][-4.2700739 -4.2599006 -4.2467575 -4.2322507 -4.2208862 -4.2159019 -4.2178273 -4.2257028 -4.2361536 -4.2456141 -4.2514238 -4.2550173 -4.2587938 -4.2627454 -4.2673273][-4.2732234 -4.263629 -4.25057 -4.2364345 -4.2260518 -4.2223082 -4.2256384 -4.234201 -4.2438717 -4.25204 -4.2568474 -4.2599964 -4.2633123 -4.2669463 -4.2719259][-4.2747822 -4.2670403 -4.2561579 -4.2443104 -4.2362881 -4.2343178 -4.2380013 -4.2446094 -4.2506061 -4.2550707 -4.2572875 -4.2588744 -4.2611303 -4.2637343 -4.2683115][-4.2754946 -4.2698746 -4.2619867 -4.2534261 -4.2478423 -4.2470303 -4.2499409 -4.2537413 -4.2562437 -4.2578425 -4.2581024 -4.2582088 -4.2591009 -4.2602839 -4.2636695][-4.27794 -4.274127 -4.268816 -4.263051 -4.2593565 -4.2590704 -4.26085 -4.2625442 -4.2630186 -4.2631664 -4.2626081 -4.2619939 -4.2620735 -4.2625394 -4.2648058][-4.2816453 -4.2792439 -4.2759995 -4.2723026 -4.269773 -4.2693744 -4.270164 -4.2707129 -4.2704434 -4.2701292 -4.2695842 -4.2688775 -4.268579 -4.2687249 -4.2702971][-4.2844853 -4.2826452 -4.2807722 -4.2786088 -4.2768464 -4.2763543 -4.2766414 -4.2768359 -4.276619 -4.2764568 -4.2763872 -4.2759686 -4.2754922 -4.2753429 -4.2758932][-4.2848978 -4.2833281 -4.2819943 -4.2805495 -4.2792425 -4.2787333 -4.2788186 -4.2788639 -4.2786179 -4.2787547 -4.2791386 -4.2790942 -4.2786736 -4.2783442 -4.27805][-4.2847528 -4.28352 -4.2826014 -4.2816792 -4.2807436 -4.2802052 -4.2799711 -4.2794785 -4.2785368 -4.2780533 -4.2779522 -4.2775412 -4.2766705 -4.275939 -4.2752848]]...]
INFO - root - 2017-12-05 12:25:31.334967: step 10710, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.935 sec/batch; 83h:35m:33s remains)
INFO - root - 2017-12-05 12:25:40.796502: step 10720, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 84h:06m:10s remains)
INFO - root - 2017-12-05 12:25:50.290907: step 10730, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 83h:40m:48s remains)
INFO - root - 2017-12-05 12:25:59.759896: step 10740, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 82h:47m:15s remains)
INFO - root - 2017-12-05 12:26:09.065271: step 10750, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 80h:25m:25s remains)
INFO - root - 2017-12-05 12:26:18.418721: step 10760, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 82h:02m:22s remains)
INFO - root - 2017-12-05 12:26:27.772788: step 10770, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 78h:36m:31s remains)
INFO - root - 2017-12-05 12:26:37.101547: step 10780, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 82h:41m:18s remains)
INFO - root - 2017-12-05 12:26:46.386721: step 10790, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 81h:09m:04s remains)
INFO - root - 2017-12-05 12:26:55.644368: step 10800, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 85h:11m:17s remains)
2017-12-05 12:26:56.434568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3202147 -4.3143339 -4.3087378 -4.3054204 -4.300313 -4.2892537 -4.2770133 -4.2649765 -4.2594261 -4.2611861 -4.2628779 -4.2565823 -4.2426023 -4.228159 -4.2028379][-4.3149047 -4.3069363 -4.2982688 -4.288929 -4.2768331 -4.2607932 -4.2460923 -4.234313 -4.2334332 -4.2433372 -4.2515893 -4.2473545 -4.2312784 -4.2086072 -4.168932][-4.3057647 -4.2940841 -4.28164 -4.267704 -4.2540588 -4.2391133 -4.2263012 -4.2178535 -4.2206149 -4.2362347 -4.2492948 -4.2453613 -4.2282944 -4.1995707 -4.153347][-4.2920189 -4.276751 -4.2624955 -4.248404 -4.23795 -4.2215786 -4.2016139 -4.1899967 -4.1920433 -4.2101421 -4.2249432 -4.2280197 -4.2221122 -4.205112 -4.1749425][-4.2784667 -4.264585 -4.2482338 -4.2290664 -4.2114367 -4.1791263 -4.1373987 -4.1186914 -4.125721 -4.1500998 -4.1728606 -4.1896992 -4.1995082 -4.2062054 -4.1998339][-4.2759762 -4.2636018 -4.2411761 -4.2112775 -4.1776376 -4.1200089 -4.044723 -4.0135794 -4.0311103 -4.0713468 -4.1065335 -4.135818 -4.1589813 -4.184917 -4.1970696][-4.283577 -4.2696481 -4.2365613 -4.188684 -4.1324034 -4.0482769 -3.93745 -3.8894534 -3.9306211 -3.9990385 -4.0475397 -4.0849228 -4.1195579 -4.1646738 -4.1938925][-4.2880888 -4.2736011 -4.232265 -4.168047 -4.0935378 -4.0017757 -3.8846562 -3.8386827 -3.912046 -3.9982045 -4.0385032 -4.0698819 -4.1090455 -4.16368 -4.2052207][-4.2811651 -4.2694488 -4.2282114 -4.16233 -4.0904646 -4.025547 -3.9587002 -3.9438357 -4.0028257 -4.0638757 -4.0765619 -4.0947576 -4.1310816 -4.1849632 -4.2276149][-4.2539763 -4.2511649 -4.2241015 -4.1754513 -4.1236029 -4.09834 -4.0856256 -4.0921712 -4.12004 -4.142313 -4.1326585 -4.1436129 -4.1739469 -4.2164378 -4.2475471][-4.2034278 -4.2132978 -4.2116194 -4.1924167 -4.1677771 -4.1707578 -4.1870823 -4.2006459 -4.2086196 -4.203042 -4.1801777 -4.1843529 -4.2065792 -4.2360258 -4.2561622][-4.1570683 -4.1843405 -4.2034397 -4.20412 -4.1972957 -4.2071409 -4.2261004 -4.2389884 -4.2400188 -4.2268748 -4.2033811 -4.2019272 -4.2125144 -4.2343464 -4.2508559][-4.1374373 -4.1769729 -4.1981382 -4.2017946 -4.1997705 -4.2057652 -4.2194409 -4.2330174 -4.2366886 -4.2285366 -4.2106361 -4.2048798 -4.2085857 -4.2300234 -4.2490482][-4.1470304 -4.1771569 -4.1911111 -4.1917033 -4.1872039 -4.1866078 -4.1981559 -4.2155747 -4.224236 -4.2240634 -4.2166691 -4.216774 -4.2233882 -4.2501116 -4.2749872][-4.1613574 -4.1738319 -4.1822133 -4.1807847 -4.1718922 -4.1700144 -4.1869297 -4.210155 -4.2265778 -4.2392583 -4.2440104 -4.2511997 -4.262589 -4.2877121 -4.309257]]...]
INFO - root - 2017-12-05 12:27:05.842383: step 10810, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 81h:10m:57s remains)
INFO - root - 2017-12-05 12:27:15.201048: step 10820, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 82h:41m:40s remains)
INFO - root - 2017-12-05 12:27:24.677698: step 10830, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.924 sec/batch; 82h:36m:07s remains)
INFO - root - 2017-12-05 12:27:33.959024: step 10840, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 80h:41m:08s remains)
INFO - root - 2017-12-05 12:27:43.189467: step 10850, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.938 sec/batch; 83h:50m:02s remains)
INFO - root - 2017-12-05 12:27:52.640058: step 10860, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 85h:08m:32s remains)
INFO - root - 2017-12-05 12:28:01.884388: step 10870, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 81h:19m:03s remains)
INFO - root - 2017-12-05 12:28:11.226292: step 10880, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 83h:43m:50s remains)
INFO - root - 2017-12-05 12:28:20.520029: step 10890, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 79h:04m:52s remains)
INFO - root - 2017-12-05 12:28:29.862612: step 10900, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 84h:43m:19s remains)
2017-12-05 12:28:30.588141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1863675 -4.2204938 -4.2536206 -4.267065 -4.2571659 -4.208 -4.126317 -4.0872564 -4.1139703 -4.1714883 -4.2199993 -4.2600694 -4.2853074 -4.2793016 -4.2380776][-4.1877146 -4.2216434 -4.2567387 -4.272419 -4.2621555 -4.2083316 -4.1201949 -4.0778031 -4.107554 -4.1704683 -4.2214861 -4.261889 -4.2853479 -4.2737589 -4.2276545][-4.1914349 -4.2271962 -4.26383 -4.2811246 -4.2668195 -4.2023158 -4.1081285 -4.0648985 -4.1001725 -4.1693878 -4.2238665 -4.26381 -4.2837181 -4.26463 -4.2114062][-4.20336 -4.2373505 -4.2712474 -4.2855139 -4.2639375 -4.1877604 -4.0911913 -4.0519142 -4.0956531 -4.1696806 -4.2255034 -4.26357 -4.2806239 -4.2544155 -4.1930785][-4.2119217 -4.2441268 -4.2754245 -4.2837954 -4.2529931 -4.1684256 -4.07606 -4.04673 -4.0992455 -4.174881 -4.2287111 -4.2632685 -4.2773132 -4.2441759 -4.1738491][-4.2100444 -4.2426381 -4.2740192 -4.2779365 -4.2386179 -4.1512222 -4.069 -4.0525312 -4.1107011 -4.1844821 -4.2334304 -4.2636952 -4.2737451 -4.2332225 -4.1544604][-4.2015562 -4.2362738 -4.2687879 -4.26921 -4.2232356 -4.1364613 -4.0657353 -4.0597296 -4.1199627 -4.1919374 -4.2374058 -4.2656808 -4.2706914 -4.22242 -4.1365495][-4.1976328 -4.2329807 -4.2647448 -4.25889 -4.205986 -4.1215014 -4.06019 -4.0605378 -4.1193304 -4.1917796 -4.238802 -4.2676873 -4.2661629 -4.2110806 -4.12413][-4.2054067 -4.23576 -4.2617483 -4.2469659 -4.1875544 -4.1079817 -4.0541434 -4.0576615 -4.1132789 -4.1870232 -4.2378974 -4.268259 -4.2602024 -4.2001433 -4.1190195][-4.213428 -4.2353554 -4.2537789 -4.230938 -4.1667933 -4.0969253 -4.04993 -4.0530982 -4.1070037 -4.1807346 -4.2345934 -4.2658291 -4.2528558 -4.1912422 -4.1195168][-4.2162137 -4.2310324 -4.2437758 -4.2139735 -4.1472607 -4.0884666 -4.0481315 -4.0468979 -4.1003132 -4.1732378 -4.2287183 -4.2603903 -4.2453284 -4.186851 -4.123672][-4.2155132 -4.228209 -4.23705 -4.2007246 -4.1328745 -4.0817618 -4.0453348 -4.0398889 -4.0948386 -4.1651831 -4.2204819 -4.251451 -4.2374783 -4.1845946 -4.1284709][-4.2195678 -4.232563 -4.2369237 -4.1947718 -4.1251478 -4.0762 -4.03867 -4.0294161 -4.0884056 -4.1581321 -4.2134161 -4.2436905 -4.2319326 -4.1838293 -4.1339993][-4.2289529 -4.2425704 -4.241231 -4.195581 -4.1255097 -4.0758615 -4.0339189 -4.019989 -4.0829582 -4.1541939 -4.2107449 -4.2403212 -4.23003 -4.1847553 -4.1413269][-4.2382116 -4.254056 -4.2477107 -4.2025089 -4.1360059 -4.0851359 -4.0374436 -4.0187736 -4.0834446 -4.1561489 -4.2130609 -4.2415013 -4.2312803 -4.1889629 -4.1531081]]...]
INFO - root - 2017-12-05 12:28:39.842152: step 10910, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 77h:55m:23s remains)
INFO - root - 2017-12-05 12:28:49.131643: step 10920, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 85h:27m:13s remains)
INFO - root - 2017-12-05 12:28:58.498166: step 10930, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 83h:03m:54s remains)
INFO - root - 2017-12-05 12:29:07.861399: step 10940, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 83h:19m:46s remains)
INFO - root - 2017-12-05 12:29:17.113330: step 10950, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 84h:00m:46s remains)
INFO - root - 2017-12-05 12:29:26.511622: step 10960, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 84h:20m:54s remains)
INFO - root - 2017-12-05 12:29:35.818680: step 10970, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 81h:24m:47s remains)
INFO - root - 2017-12-05 12:29:44.925805: step 10980, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 73h:40m:51s remains)
INFO - root - 2017-12-05 12:29:54.374666: step 10990, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 83h:05m:58s remains)
INFO - root - 2017-12-05 12:30:03.516123: step 11000, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 84h:12m:51s remains)
2017-12-05 12:30:04.309540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3486052 -4.3442726 -4.33716 -4.3304257 -4.329133 -4.3344069 -4.3391709 -4.3412476 -4.3408051 -4.3401427 -4.337676 -4.3348637 -4.3327065 -4.3318391 -4.3330331][-4.3421521 -4.3316655 -4.3203015 -4.312542 -4.3130226 -4.3214579 -4.3279419 -4.3292918 -4.3260694 -4.3218703 -4.3163624 -4.3137336 -4.3135734 -4.314002 -4.3168864][-4.3280582 -4.3113861 -4.29795 -4.2924376 -4.2959242 -4.3061686 -4.3118215 -4.3101068 -4.3029785 -4.2947669 -4.2870569 -4.2840457 -4.2882395 -4.294857 -4.3027248][-4.3033714 -4.2779932 -4.2641678 -4.2632618 -4.2698131 -4.2775369 -4.2800636 -4.2747164 -4.2656703 -4.2581291 -4.2530117 -4.2471695 -4.2509847 -4.2650485 -4.2824664][-4.2682371 -4.2297 -4.210371 -4.2127833 -4.2224793 -4.2247925 -4.2191863 -4.2080398 -4.2068286 -4.2133193 -4.2169766 -4.210433 -4.2123609 -4.2293863 -4.2519207][-4.2332621 -4.1808028 -4.1513295 -4.1541624 -4.1630015 -4.1535273 -4.1292915 -4.1032681 -4.1210971 -4.1603518 -4.1843395 -4.184463 -4.1870036 -4.2021136 -4.223053][-4.2091575 -4.149189 -4.1105967 -4.1039262 -4.0988607 -4.065299 -4.00941 -3.9554448 -3.9999261 -4.088747 -4.1491117 -4.1687207 -4.1748323 -4.1850681 -4.1971908][-4.2006927 -4.1412134 -4.099925 -4.0796905 -4.0558882 -3.9984207 -3.9092591 -3.8284931 -3.8993769 -4.0214052 -4.1131182 -4.1540933 -4.1675892 -4.1761217 -4.179122][-4.2037621 -4.1509247 -4.1178842 -4.0953255 -4.0620737 -4.001792 -3.9117868 -3.8337343 -3.9028952 -4.0091667 -4.0991669 -4.1489968 -4.1641064 -4.1755319 -4.1773229][-4.2124152 -4.1694 -4.1476822 -4.1318607 -4.1056395 -4.066597 -4.0080404 -3.95928 -4.0060935 -4.0682635 -4.1258116 -4.1612196 -4.1672745 -4.176228 -4.1823115][-4.2272058 -4.1957688 -4.1844311 -4.1774054 -4.1625843 -4.1440582 -4.1149459 -4.090344 -4.1193466 -4.1501713 -4.1776891 -4.1922297 -4.1855459 -4.1896739 -4.1967888][-4.2488217 -4.2278676 -4.2246194 -4.2241035 -4.2196727 -4.2167869 -4.2077594 -4.200325 -4.2192454 -4.2347322 -4.2412686 -4.2388506 -4.2252212 -4.2255855 -4.2329497][-4.2747574 -4.2618685 -4.2627907 -4.2662048 -4.2673774 -4.2719569 -4.2755079 -4.2791309 -4.2940078 -4.30406 -4.3018179 -4.2910786 -4.2756276 -4.2720051 -4.2758403][-4.2952461 -4.2861023 -4.2879148 -4.2935123 -4.2975187 -4.3038025 -4.3109946 -4.3193655 -4.3308878 -4.3367825 -4.3314223 -4.3192878 -4.3059292 -4.3004827 -4.3026757][-4.3106 -4.3023129 -4.3034406 -4.3094568 -4.3153739 -4.3218684 -4.3278513 -4.3342352 -4.3409433 -4.3431835 -4.3390245 -4.3300929 -4.3208156 -4.316361 -4.3179908]]...]
INFO - root - 2017-12-05 12:30:13.585236: step 11010, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 87h:17m:09s remains)
INFO - root - 2017-12-05 12:30:22.904516: step 11020, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 82h:19m:58s remains)
INFO - root - 2017-12-05 12:30:32.323673: step 11030, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 86h:10m:21s remains)
INFO - root - 2017-12-05 12:30:41.788154: step 11040, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 80h:15m:09s remains)
INFO - root - 2017-12-05 12:30:51.114491: step 11050, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 77h:56m:38s remains)
INFO - root - 2017-12-05 12:31:00.592799: step 11060, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 86h:59m:54s remains)
INFO - root - 2017-12-05 12:31:09.799190: step 11070, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 81h:02m:56s remains)
INFO - root - 2017-12-05 12:31:19.151665: step 11080, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.959 sec/batch; 85h:35m:24s remains)
INFO - root - 2017-12-05 12:31:28.678811: step 11090, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 81h:53m:33s remains)
INFO - root - 2017-12-05 12:31:38.012304: step 11100, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 85h:50m:23s remains)
2017-12-05 12:31:38.782731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22664 -4.2257357 -4.2277226 -4.2300868 -4.2278175 -4.21975 -4.2092156 -4.1972 -4.188333 -4.1841755 -4.1853352 -4.1842489 -4.1773491 -4.178175 -4.1936793][-4.239594 -4.2434731 -4.2498207 -4.2566643 -4.2551646 -4.2424746 -4.2210307 -4.1976786 -4.1832685 -4.1796484 -4.1873627 -4.19504 -4.1935477 -4.1968055 -4.2126517][-4.2497368 -4.2552295 -4.2621112 -4.26572 -4.2542753 -4.2282305 -4.190484 -4.1564059 -4.1432543 -4.151082 -4.1714911 -4.1912961 -4.2016406 -4.212265 -4.2296677][-4.2508893 -4.2534981 -4.25554 -4.2471929 -4.2188087 -4.1738267 -4.1168122 -4.0740228 -4.0696378 -4.0971165 -4.1386132 -4.178278 -4.204524 -4.2212234 -4.2349353][-4.2512846 -4.2532487 -4.2491827 -4.2240615 -4.1709557 -4.0999651 -4.0157404 -3.9576714 -3.9636424 -4.0174623 -4.0874543 -4.1524158 -4.1983523 -4.2210121 -4.2290645][-4.25369 -4.2589626 -4.2510157 -4.2080092 -4.1278839 -4.0234084 -3.9067779 -3.8278768 -3.8409097 -3.91815 -4.0168076 -4.1066494 -4.1709914 -4.2012877 -4.206275][-4.255022 -4.264894 -4.2558341 -4.1993494 -4.0976343 -3.9718173 -3.8387625 -3.75051 -3.7651296 -3.8547904 -3.9672382 -4.0709305 -4.1439238 -4.174614 -4.1749849][-4.2693005 -4.2858148 -4.2774572 -4.2135653 -4.1085567 -3.9899802 -3.8725958 -3.7930572 -3.7981422 -3.8699894 -3.967452 -4.0633674 -4.1315646 -4.1515975 -4.137825][-4.293448 -4.3075075 -4.2948794 -4.2315717 -4.1390448 -4.0478764 -3.9761162 -3.9285705 -3.9235871 -3.959801 -4.0200992 -4.0890379 -4.1354785 -4.1357751 -4.1037025][-4.3072276 -4.3142877 -4.2944055 -4.2391338 -4.1682682 -4.1087179 -4.08327 -4.0755444 -4.07273 -4.0827951 -4.1104054 -4.1464982 -4.1660323 -4.148458 -4.1022263][-4.2961173 -4.2978721 -4.2749734 -4.2286181 -4.1773772 -4.1407971 -4.1464882 -4.17123 -4.1839166 -4.1877227 -4.1962447 -4.2085786 -4.2039003 -4.1756124 -4.1249518][-4.2661881 -4.2664957 -4.2465291 -4.2108688 -4.17499 -4.1546521 -4.1733456 -4.2103291 -4.2337604 -4.2413731 -4.2438731 -4.2407188 -4.219244 -4.1868782 -4.1459608][-4.2493238 -4.2473903 -4.2308393 -4.2017341 -4.170105 -4.1547256 -4.1711793 -4.2032557 -4.2328882 -4.2513852 -4.2548814 -4.246768 -4.2187438 -4.185802 -4.1595116][-4.2621121 -4.2574472 -4.2398233 -4.2118888 -4.1793547 -4.161891 -4.170321 -4.1913023 -4.219295 -4.2447538 -4.2514367 -4.239697 -4.2066355 -4.1748576 -4.1625519][-4.2854319 -4.2774949 -4.2595263 -4.2336192 -4.2053742 -4.1866746 -4.1878705 -4.1998024 -4.2244296 -4.2524886 -4.2589941 -4.2383084 -4.1976323 -4.1660514 -4.163475]]...]
INFO - root - 2017-12-05 12:31:48.260955: step 11110, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.992 sec/batch; 88h:31m:43s remains)
INFO - root - 2017-12-05 12:31:57.283906: step 11120, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 81h:26m:18s remains)
INFO - root - 2017-12-05 12:32:06.637866: step 11130, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 83h:30m:05s remains)
INFO - root - 2017-12-05 12:32:16.101788: step 11140, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 85h:14m:13s remains)
INFO - root - 2017-12-05 12:32:25.408057: step 11150, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 84h:49m:45s remains)
INFO - root - 2017-12-05 12:32:34.597122: step 11160, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 82h:00m:49s remains)
INFO - root - 2017-12-05 12:32:43.783161: step 11170, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 80h:08m:42s remains)
INFO - root - 2017-12-05 12:32:53.118236: step 11180, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 83h:25m:40s remains)
INFO - root - 2017-12-05 12:33:02.319491: step 11190, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 79h:24m:12s remains)
INFO - root - 2017-12-05 12:33:11.695526: step 11200, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 87h:30m:01s remains)
2017-12-05 12:33:12.493937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2672563 -4.278172 -4.2822504 -4.2781029 -4.2782335 -4.2832184 -4.2942848 -4.30332 -4.3054156 -4.2971659 -4.2801914 -4.2619815 -4.2510009 -4.2504888 -4.2565494][-4.2504849 -4.2621512 -4.268218 -4.2635007 -4.2616582 -4.2659287 -4.2806878 -4.295229 -4.3028569 -4.2974133 -4.2817206 -4.2618513 -4.2484689 -4.2478018 -4.257][-4.2381277 -4.2494669 -4.2545328 -4.2434721 -4.234519 -4.2338 -4.2497334 -4.2701869 -4.2842073 -4.2840543 -4.27416 -4.2621584 -4.2550035 -4.2580643 -4.2692842][-4.2417669 -4.2497983 -4.2474804 -4.2238684 -4.2029643 -4.1944947 -4.2096877 -4.2376881 -4.2616224 -4.2700686 -4.2681966 -4.2660189 -4.269424 -4.2783451 -4.2912865][-4.2598433 -4.2669978 -4.2536411 -4.2131152 -4.1743979 -4.1494427 -4.1534028 -4.1832247 -4.2223711 -4.2481432 -4.2613506 -4.2698131 -4.2830329 -4.2988987 -4.3134561][-4.2792454 -4.2861013 -4.2665176 -4.2135229 -4.1548734 -4.1058607 -4.0771265 -4.09206 -4.1482639 -4.2024608 -4.2387123 -4.2616158 -4.2826862 -4.3033319 -4.3199329][-4.2897625 -4.2979484 -4.2783861 -4.2208214 -4.1469393 -4.0691671 -3.9912953 -3.9690726 -4.0421543 -4.1348367 -4.2002921 -4.2391539 -4.2656322 -4.2895827 -4.3088441][-4.2953215 -4.306921 -4.2934127 -4.2417378 -4.1615014 -4.0627818 -3.943254 -3.8751035 -3.9501557 -4.0737391 -4.1651835 -4.2164607 -4.2420068 -4.2637677 -4.2845969][-4.2978969 -4.3156872 -4.31461 -4.2766762 -4.2054315 -4.113328 -4.0014114 -3.9258542 -3.9709172 -4.0777707 -4.1616259 -4.2040591 -4.2164717 -4.2256866 -4.2478747][-4.3003035 -4.3256788 -4.3370895 -4.3138175 -4.2607975 -4.1927319 -4.1161094 -4.0634351 -4.0796633 -4.1395054 -4.1864781 -4.196805 -4.1761951 -4.1578584 -4.1816444][-4.2983623 -4.32932 -4.350071 -4.3372827 -4.2989931 -4.2529964 -4.2067122 -4.176713 -4.1811509 -4.202322 -4.2085772 -4.1784191 -4.1059628 -4.0427852 -4.0700665][-4.2954082 -4.327621 -4.3508644 -4.3425055 -4.3143597 -4.2836576 -4.2571769 -4.2439661 -4.2463393 -4.2472315 -4.2254677 -4.1625247 -4.0411038 -3.9242566 -3.9482081][-4.2944083 -4.3244743 -4.3462706 -4.3416343 -4.3237381 -4.3038211 -4.28957 -4.2863522 -4.2911797 -4.2870855 -4.2561603 -4.1852546 -4.0604062 -3.9370604 -3.9364231][-4.2940044 -4.3215761 -4.3416872 -4.340704 -4.3316865 -4.3208561 -4.316083 -4.3172321 -4.3236609 -4.3207164 -4.2914586 -4.232379 -4.1419148 -4.0586157 -4.0448284][-4.2928252 -4.3179011 -4.3364754 -4.3370643 -4.3322086 -4.3262205 -4.3268871 -4.3318372 -4.3411479 -4.3409681 -4.3156152 -4.2717447 -4.2173715 -4.1747613 -4.1625233]]...]
INFO - root - 2017-12-05 12:33:21.845698: step 11210, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 86h:45m:21s remains)
INFO - root - 2017-12-05 12:33:31.096333: step 11220, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 86h:34m:12s remains)
INFO - root - 2017-12-05 12:33:40.584266: step 11230, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.015 sec/batch; 90h:37m:01s remains)
INFO - root - 2017-12-05 12:33:50.044002: step 11240, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 86h:14m:13s remains)
INFO - root - 2017-12-05 12:33:59.266241: step 11250, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.876 sec/batch; 78h:08m:30s remains)
INFO - root - 2017-12-05 12:34:08.458951: step 11260, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 83h:54m:12s remains)
INFO - root - 2017-12-05 12:34:17.991473: step 11270, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 85h:41m:11s remains)
INFO - root - 2017-12-05 12:34:27.356265: step 11280, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 83h:33m:17s remains)
INFO - root - 2017-12-05 12:34:36.708824: step 11290, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 82h:15m:35s remains)
INFO - root - 2017-12-05 12:34:46.260153: step 11300, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 84h:01m:35s remains)
2017-12-05 12:34:47.088671: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1779671 -4.1878529 -4.1913266 -4.1723394 -4.1531849 -4.1482053 -4.1531258 -4.1550765 -4.1571379 -4.163012 -4.1586337 -4.1490426 -4.1460185 -4.1578569 -4.1808887][-4.1750345 -4.1871424 -4.1952524 -4.1747689 -4.1439457 -4.1289787 -4.1340613 -4.1428142 -4.148612 -4.1528258 -4.1452293 -4.1330543 -4.1335745 -4.1536021 -4.1802778][-4.1660933 -4.1795697 -4.1937308 -4.1793246 -4.1445255 -4.116858 -4.1113172 -4.1196361 -4.1335769 -4.1356068 -4.1207252 -4.1021709 -4.1086774 -4.1381607 -4.1725597][-4.1451058 -4.16102 -4.1814041 -4.1777735 -4.1447191 -4.1028156 -4.0798306 -4.089632 -4.12222 -4.1324449 -4.1128478 -4.090065 -4.0992136 -4.1311388 -4.1654553][-4.1253667 -4.1484418 -4.1755495 -4.1769195 -4.1360269 -4.07866 -4.0424814 -4.0630336 -4.1224127 -4.1512804 -4.1335959 -4.1038055 -4.1060648 -4.1261215 -4.1540132][-4.133461 -4.1537633 -4.176621 -4.1700883 -4.110507 -4.0303841 -3.9818873 -4.0216761 -4.1163235 -4.169992 -4.1550269 -4.1159468 -4.1071367 -4.1136222 -4.1364756][-4.1585908 -4.1678119 -4.1761289 -4.1531167 -4.0700808 -3.9520919 -3.8811395 -3.9436154 -4.0783672 -4.1581068 -4.1534567 -4.1213536 -4.1124816 -4.1130462 -4.1307163][-4.1873541 -4.1880813 -4.1812949 -4.1400738 -4.0416088 -3.8947921 -3.7998595 -3.8821437 -4.0437274 -4.1381168 -4.14655 -4.1297865 -4.1297846 -4.13149 -4.1431675][-4.2076359 -4.2068973 -4.1941843 -4.1513815 -4.0657616 -3.9374769 -3.8551054 -3.920553 -4.0576096 -4.1422706 -4.1492085 -4.1362357 -4.1424484 -4.1505237 -4.1621184][-4.2180309 -4.2165689 -4.2108078 -4.1847153 -4.130394 -4.0427985 -3.9866498 -4.0216012 -4.105473 -4.1631255 -4.16621 -4.1496859 -4.1504517 -4.1604786 -4.1735296][-4.2087069 -4.2045078 -4.2102489 -4.2073488 -4.1826253 -4.1279292 -4.0844421 -4.0961061 -4.1433673 -4.1822166 -4.1924853 -4.1792808 -4.1739073 -4.1773367 -4.1843596][-4.1858525 -4.1716347 -4.1832342 -4.1979642 -4.1965723 -4.1669278 -4.1314611 -4.1285672 -4.1577721 -4.189837 -4.2055192 -4.1980743 -4.1874318 -4.1811266 -4.1791739][-4.1648197 -4.1453624 -4.1564331 -4.1794076 -4.1905046 -4.172636 -4.1418552 -4.1326227 -4.15361 -4.1817832 -4.2006464 -4.1985526 -4.18202 -4.1653357 -4.1534438][-4.1632323 -4.1422668 -4.14771 -4.1707907 -4.183733 -4.1668139 -4.1371136 -4.129035 -4.1490507 -4.1742554 -4.1926928 -4.19437 -4.1772194 -4.1551685 -4.1369214][-4.1835852 -4.1678085 -4.1694436 -4.1847458 -4.189568 -4.1694608 -4.14166 -4.1377268 -4.1608229 -4.1847959 -4.2012591 -4.2014818 -4.1849351 -4.1631956 -4.1430473]]...]
INFO - root - 2017-12-05 12:34:56.439297: step 11310, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.936 sec/batch; 83h:27m:56s remains)
INFO - root - 2017-12-05 12:35:05.830767: step 11320, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 83h:47m:22s remains)
INFO - root - 2017-12-05 12:35:15.147685: step 11330, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 81h:16m:50s remains)
INFO - root - 2017-12-05 12:35:24.629364: step 11340, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 84h:08m:25s remains)
INFO - root - 2017-12-05 12:35:33.800660: step 11350, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 82h:21m:02s remains)
INFO - root - 2017-12-05 12:35:43.347542: step 11360, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 82h:58m:56s remains)
INFO - root - 2017-12-05 12:35:52.885378: step 11370, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 83h:44m:05s remains)
INFO - root - 2017-12-05 12:36:02.236493: step 11380, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 81h:28m:34s remains)
INFO - root - 2017-12-05 12:36:11.863978: step 11390, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 86h:13m:16s remains)
INFO - root - 2017-12-05 12:36:21.284990: step 11400, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 87h:10m:16s remains)
2017-12-05 12:36:22.082679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.311656 -4.3088231 -4.3072805 -4.3059945 -4.3081374 -4.3102074 -4.304462 -4.296474 -4.2879019 -4.2838731 -4.2871633 -4.3002257 -4.3168902 -4.3297048 -4.3352776][-4.2867203 -4.2822328 -4.2799797 -4.2780533 -4.2807198 -4.2814765 -4.2726917 -4.2611275 -4.2520185 -4.2527966 -4.2603946 -4.2799482 -4.3050776 -4.3245277 -4.33167][-4.2565303 -4.2501597 -4.2459702 -4.2430062 -4.2460141 -4.247982 -4.2344022 -4.2154703 -4.2046819 -4.2135606 -4.2276173 -4.2521305 -4.2835226 -4.3092942 -4.3188758][-4.22104 -4.2110815 -4.2039957 -4.195951 -4.1928086 -4.1939745 -4.1783218 -4.1528177 -4.1384606 -4.1586447 -4.1835165 -4.2144341 -4.2496519 -4.2787557 -4.2916536][-4.1879587 -4.174 -4.1599512 -4.141993 -4.1218843 -4.1085272 -4.0837274 -4.049829 -4.039412 -4.0821815 -4.1276059 -4.1695662 -4.2052555 -4.2334113 -4.2492466][-4.165184 -4.1492934 -4.1291871 -4.0999174 -4.0564284 -4.0090446 -3.9507418 -3.8920109 -3.8881326 -3.9737349 -4.055943 -4.1163692 -4.1553741 -4.1876116 -4.2103357][-4.162611 -4.1417003 -4.1180506 -4.0866828 -4.0317769 -3.9536304 -3.8414698 -3.7276206 -3.7163575 -3.8547752 -3.9850094 -4.07223 -4.1228046 -4.1658049 -4.1980796][-4.1607418 -4.1279249 -4.1045332 -4.0873528 -4.0443335 -3.9629955 -3.8237729 -3.668777 -3.6406727 -3.8027475 -3.9577665 -4.06268 -4.1223726 -4.1733894 -4.2093115][-4.1592631 -4.1236887 -4.1082191 -4.1090226 -4.0916719 -4.0357985 -3.9242761 -3.7959962 -3.7650571 -3.8808064 -4.0042653 -4.0930147 -4.1469736 -4.1988759 -4.231101][-4.1633606 -4.1296964 -4.1230054 -4.136488 -4.1429291 -4.112041 -4.0366616 -3.9465086 -3.9214835 -3.9922152 -4.0754561 -4.1383967 -4.1809034 -4.224359 -4.2494059][-4.169518 -4.1426339 -4.1442013 -4.1648788 -4.1847978 -4.1759934 -4.1310849 -4.0717177 -4.055738 -4.0976539 -4.1528158 -4.1928706 -4.2206345 -4.2517767 -4.2682786][-4.1993175 -4.1770806 -4.1800265 -4.1970572 -4.2181177 -4.2234049 -4.202704 -4.1681538 -4.1595016 -4.18265 -4.2177157 -4.2413964 -4.2567139 -4.2780995 -4.2867131][-4.2434897 -4.2280097 -4.2292852 -4.2365346 -4.2490392 -4.2567897 -4.2493005 -4.2304521 -4.228415 -4.2438374 -4.2663374 -4.2783413 -4.2857904 -4.2984967 -4.300107][-4.2755365 -4.2651334 -4.2638149 -4.2662916 -4.2698746 -4.2735615 -4.2714572 -4.2631841 -4.2649951 -4.2755413 -4.2894087 -4.2954669 -4.2989039 -4.3060641 -4.3076935][-4.2906923 -4.2817125 -4.2786856 -4.2786093 -4.2790294 -4.280364 -4.2826238 -4.2812157 -4.2823639 -4.288209 -4.2972507 -4.30274 -4.3065734 -4.3130279 -4.3164067]]...]
INFO - root - 2017-12-05 12:36:31.565129: step 11410, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 85h:12m:19s remains)
INFO - root - 2017-12-05 12:36:40.843720: step 11420, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 82h:46m:54s remains)
INFO - root - 2017-12-05 12:36:50.350911: step 11430, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 86h:20m:53s remains)
INFO - root - 2017-12-05 12:36:59.738153: step 11440, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 87h:28m:11s remains)
INFO - root - 2017-12-05 12:37:09.029149: step 11450, loss = 2.03, batch loss = 1.98 (9.1 examples/sec; 0.876 sec/batch; 78h:05m:20s remains)
INFO - root - 2017-12-05 12:37:18.473574: step 11460, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 83h:02m:07s remains)
INFO - root - 2017-12-05 12:37:27.986414: step 11470, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 87h:45m:41s remains)
INFO - root - 2017-12-05 12:37:37.482785: step 11480, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 86h:29m:24s remains)
INFO - root - 2017-12-05 12:37:46.847671: step 11490, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 86h:46m:03s remains)
INFO - root - 2017-12-05 12:37:56.292619: step 11500, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 78h:24m:08s remains)
2017-12-05 12:37:56.996417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24786 -4.2360997 -4.2220569 -4.1927104 -4.1525626 -4.1333561 -4.1428919 -4.160234 -4.1837769 -4.2107487 -4.2250414 -4.236917 -4.2615952 -4.2795873 -4.2895846][-4.230649 -4.2214894 -4.2140427 -4.1846547 -4.1314759 -4.0979271 -4.1052608 -4.1320443 -4.1664257 -4.2058225 -4.225266 -4.2393541 -4.2640305 -4.2829356 -4.2931604][-4.211081 -4.2086558 -4.2127457 -4.1865396 -4.1248908 -4.0763006 -4.0786324 -4.1094046 -4.1457162 -4.1911688 -4.219348 -4.2379484 -4.2631884 -4.285933 -4.2990885][-4.1927695 -4.1953506 -4.2087955 -4.1841931 -4.1170282 -4.0553761 -4.0515585 -4.0877361 -4.1248136 -4.1701541 -4.2068229 -4.233335 -4.2597871 -4.2866497 -4.3045211][-4.1825252 -4.1871357 -4.2075868 -4.1842384 -4.1110473 -4.0360656 -4.0281038 -4.0781884 -4.12445 -4.1671968 -4.2022672 -4.230813 -4.2552066 -4.2835407 -4.3063264][-4.162961 -4.1690149 -4.1943307 -4.17442 -4.0937333 -3.9991775 -3.9818816 -4.0503159 -4.1160412 -4.1631184 -4.1985855 -4.2277007 -4.2486181 -4.2755332 -4.3029189][-4.1211214 -4.1390381 -4.170929 -4.1561222 -4.0653276 -3.940093 -3.9014366 -3.9874678 -4.0830712 -4.1439133 -4.1891732 -4.228426 -4.2497425 -4.2712388 -4.298614][-4.0554585 -4.0995097 -4.1413708 -4.1333289 -4.0348487 -3.8667073 -3.7846527 -3.8861966 -4.0237627 -4.1165419 -4.1834574 -4.2353177 -4.2570682 -4.2719464 -4.2945151][-3.9841597 -4.0533261 -4.1083412 -4.1169038 -4.0264859 -3.8247776 -3.6775372 -3.7791872 -3.9604871 -4.0882783 -4.1759315 -4.2371984 -4.2615614 -4.2721205 -4.2905645][-3.9615836 -4.0399323 -4.102232 -4.1273723 -4.0617113 -3.8783629 -3.7055531 -3.7721381 -3.9527309 -4.08654 -4.1759686 -4.23422 -4.2619271 -4.2757831 -4.2913861][-4.01388 -4.0769186 -4.1293149 -4.1586165 -4.1255031 -4.0020456 -3.8597221 -3.874943 -4.00212 -4.1115041 -4.1878829 -4.2351131 -4.2648697 -4.2843752 -4.2974935][-4.0895033 -4.1384521 -4.1748867 -4.1976509 -4.1904273 -4.1231074 -4.0225892 -4.0070796 -4.0787783 -4.16094 -4.2207303 -4.2533092 -4.2777629 -4.2983437 -4.3061972][-4.1687207 -4.2005243 -4.2225847 -4.2415605 -4.2485075 -4.2164116 -4.1506896 -4.1224837 -4.1589561 -4.2156372 -4.257926 -4.2787971 -4.2938352 -4.3085847 -4.3109827][-4.2291861 -4.2468305 -4.26295 -4.2795205 -4.2925329 -4.2810473 -4.2401266 -4.2080874 -4.220325 -4.2581415 -4.2885485 -4.3020105 -4.3099694 -4.31845 -4.3160248][-4.2691364 -4.2799883 -4.2920556 -4.3051257 -4.315556 -4.3124456 -4.2885289 -4.2608576 -4.2619653 -4.2870274 -4.3078852 -4.316556 -4.3208203 -4.3236632 -4.3185844]]...]
INFO - root - 2017-12-05 12:38:06.351291: step 11510, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 80h:09m:57s remains)
INFO - root - 2017-12-05 12:38:15.625615: step 11520, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 83h:05m:16s remains)
INFO - root - 2017-12-05 12:38:24.780503: step 11530, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.964 sec/batch; 85h:55m:05s remains)
INFO - root - 2017-12-05 12:38:34.068447: step 11540, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 76h:56m:07s remains)
INFO - root - 2017-12-05 12:38:43.316192: step 11550, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 81h:39m:51s remains)
INFO - root - 2017-12-05 12:38:52.594696: step 11560, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 79h:54m:50s remains)
INFO - root - 2017-12-05 12:39:02.135455: step 11570, loss = 2.04, batch loss = 1.98 (8.0 examples/sec; 1.003 sec/batch; 89h:23m:14s remains)
INFO - root - 2017-12-05 12:39:11.395591: step 11580, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 83h:32m:16s remains)
INFO - root - 2017-12-05 12:39:20.670242: step 11590, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 82h:32m:05s remains)
INFO - root - 2017-12-05 12:39:30.152777: step 11600, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 87h:05m:15s remains)
2017-12-05 12:39:30.952746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3033876 -4.3007321 -4.2968431 -4.2964673 -4.2978444 -4.3025804 -4.3101845 -4.3149261 -4.3165255 -4.3198447 -4.3256044 -4.3275394 -4.327002 -4.3266287 -4.3279352][-4.2653217 -4.2594457 -4.2544613 -4.2537589 -4.2517948 -4.2562003 -4.2664366 -4.2735052 -4.2771406 -4.2837949 -4.2935023 -4.3004942 -4.3050265 -4.3094134 -4.3144255][-4.2138581 -4.1977978 -4.1925931 -4.1953754 -4.1977448 -4.2042918 -4.2159228 -4.2250404 -4.2302594 -4.2413235 -4.2524953 -4.2633381 -4.2724471 -4.27973 -4.2897005][-4.1637731 -4.1375074 -4.1265903 -4.1308789 -4.137495 -4.1421752 -4.1525869 -4.1632767 -4.1713858 -4.1897221 -4.2064896 -4.2235074 -4.2369757 -4.2445931 -4.258347][-4.1289039 -4.0967188 -4.0781789 -4.0765972 -4.0768762 -4.070569 -4.0747023 -4.0826387 -4.0942473 -4.1252408 -4.153451 -4.1786194 -4.1984696 -4.2103405 -4.2285933][-4.1081467 -4.0710421 -4.0450726 -4.0287881 -4.0130267 -3.990226 -3.9760704 -3.9690375 -3.983711 -4.0323553 -4.0794721 -4.1211419 -4.1585846 -4.1828542 -4.2076945][-4.1118722 -4.0716591 -4.0409894 -4.0133533 -3.9795105 -3.9341567 -3.8877649 -3.8517039 -3.86763 -3.9332972 -4.0031509 -4.0691891 -4.1261163 -4.1621952 -4.1948051][-4.137743 -4.1044927 -4.0826335 -4.0567865 -4.0181203 -3.9619498 -3.8926036 -3.8347588 -3.843857 -3.9049919 -3.9746916 -4.0471606 -4.1121678 -4.1571183 -4.1955271][-4.1607656 -4.136797 -4.1261082 -4.1129332 -4.0819092 -4.0294752 -3.9618282 -3.9050286 -3.9039721 -3.9438174 -3.9971619 -4.0581894 -4.11993 -4.1672235 -4.2040911][-4.1822109 -4.1600394 -4.1541657 -4.1500077 -4.1291752 -4.0893 -4.0343409 -3.9847417 -3.975224 -3.9971836 -4.0340524 -4.0848541 -4.1386533 -4.1814232 -4.2156692][-4.2085872 -4.187665 -4.1800828 -4.1763911 -4.1616173 -4.1347804 -4.0950623 -4.0595036 -4.0499649 -4.0619984 -4.0859604 -4.1239991 -4.1646886 -4.1990886 -4.2303872][-4.245604 -4.2312608 -4.2236991 -4.2199197 -4.2088737 -4.1913838 -4.1625881 -4.1361961 -4.1265478 -4.1314874 -4.1473389 -4.1729808 -4.2019362 -4.2299414 -4.25754][-4.2876978 -4.282022 -4.278574 -4.2760067 -4.2682409 -4.2574177 -4.2373791 -4.2168074 -4.2070546 -4.2095551 -4.2212124 -4.236659 -4.2543235 -4.2732234 -4.2948108][-4.3198047 -4.3206391 -4.3195696 -4.3170905 -4.3106332 -4.3040175 -4.2924137 -4.2806206 -4.2768955 -4.2813044 -4.2898793 -4.2974577 -4.30595 -4.315351 -4.3295193][-4.3358927 -4.3383436 -4.3380804 -4.3361211 -4.3319254 -4.3293948 -4.3246803 -4.3200617 -4.3202653 -4.3245597 -4.3295026 -4.3328171 -4.3359041 -4.3404236 -4.3486729]]...]
INFO - root - 2017-12-05 12:39:40.388997: step 11610, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 83h:38m:13s remains)
INFO - root - 2017-12-05 12:39:49.585658: step 11620, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 86h:46m:41s remains)
INFO - root - 2017-12-05 12:39:58.999548: step 11630, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 87h:13m:58s remains)
INFO - root - 2017-12-05 12:40:08.291762: step 11640, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 81h:18m:50s remains)
INFO - root - 2017-12-05 12:40:17.531795: step 11650, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 82h:32m:45s remains)
INFO - root - 2017-12-05 12:40:27.023998: step 11660, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 82h:57m:16s remains)
INFO - root - 2017-12-05 12:40:36.439018: step 11670, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.908 sec/batch; 80h:54m:47s remains)
INFO - root - 2017-12-05 12:40:45.761692: step 11680, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 82h:33m:07s remains)
INFO - root - 2017-12-05 12:40:55.351168: step 11690, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 86h:17m:17s remains)
INFO - root - 2017-12-05 12:41:04.631687: step 11700, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 84h:08m:15s remains)
2017-12-05 12:41:05.410057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.225697 -4.199079 -4.2020979 -4.2275681 -4.2557917 -4.27365 -4.2836962 -4.2899742 -4.299582 -4.3088808 -4.3064585 -4.2836518 -4.2571511 -4.2397656 -4.2376442][-4.2702336 -4.2510176 -4.248498 -4.2616963 -4.2783637 -4.28815 -4.2930613 -4.2987528 -4.3090124 -4.3201008 -4.3266625 -4.3166819 -4.2948465 -4.2756925 -4.2683163][-4.2894158 -4.2797518 -4.2768669 -4.2821355 -4.2861934 -4.2831287 -4.28014 -4.2834063 -4.2952356 -4.3092241 -4.3240714 -4.3299494 -4.3214149 -4.3076582 -4.298965][-4.2847424 -4.2787127 -4.2737408 -4.2780557 -4.2752819 -4.26038 -4.2467442 -4.2459583 -4.260653 -4.2799845 -4.2996516 -4.3170428 -4.3254519 -4.3255553 -4.3190794][-4.2402854 -4.2368522 -4.228909 -4.235785 -4.2300014 -4.1978822 -4.167295 -4.1633039 -4.1908369 -4.2250113 -4.2535372 -4.2770653 -4.2952156 -4.309196 -4.3100061][-4.1483932 -4.1531272 -4.1473904 -4.1616817 -4.1546874 -4.1006384 -4.0394087 -4.0202484 -4.0662861 -4.128654 -4.1753664 -4.2118788 -4.2419481 -4.2701178 -4.2820206][-4.0772576 -4.0888925 -4.0833626 -4.0896287 -4.0762587 -4.0045357 -3.9060464 -3.8501287 -3.9092412 -4.01258 -4.0891147 -4.1454763 -4.1899424 -4.2273664 -4.2488356][-4.1178875 -4.1258984 -4.1161423 -4.1063671 -4.0858974 -4.0172777 -3.9135785 -3.8298936 -3.8650005 -3.9679 -4.0564427 -4.1230912 -4.1722236 -4.2070208 -4.2253671][-4.20815 -4.2080145 -4.200336 -4.1912451 -4.1782308 -4.137116 -4.0664754 -3.9981568 -3.9961936 -4.0480032 -4.1053905 -4.1562867 -4.1960144 -4.2205667 -4.2285485][-4.2698336 -4.2629333 -4.2589369 -4.2544084 -4.2473035 -4.2255526 -4.1850138 -4.1426105 -4.1328039 -4.1507554 -4.1743588 -4.2008281 -4.2238216 -4.2366953 -4.2392812][-4.2910547 -4.2848392 -4.2827 -4.2821555 -4.2763238 -4.2587571 -4.2309 -4.2087603 -4.2077765 -4.2184529 -4.2262974 -4.2332568 -4.2380753 -4.23826 -4.2383466][-4.2787471 -4.2757053 -4.2759609 -4.2772508 -4.2703753 -4.2523956 -4.2289567 -4.2164178 -4.2295508 -4.2511287 -4.2596068 -4.2550993 -4.2441759 -4.2346883 -4.2331753][-4.2614079 -4.2584319 -4.2635946 -4.2682366 -4.2596269 -4.2416172 -4.2200227 -4.2069535 -4.2230248 -4.2556019 -4.2748656 -4.269918 -4.2502208 -4.230885 -4.2249851][-4.2595539 -4.2511334 -4.2580791 -4.2676387 -4.2633529 -4.2485027 -4.2289963 -4.2121286 -4.221961 -4.255909 -4.2809067 -4.2786789 -4.2563186 -4.2251797 -4.2087221][-4.2679467 -4.2577195 -4.265933 -4.2808046 -4.2835135 -4.274302 -4.2588911 -4.2418933 -4.2443733 -4.2692609 -4.2898884 -4.2867446 -4.2607422 -4.2203441 -4.1945982]]...]
INFO - root - 2017-12-05 12:41:14.664021: step 11710, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 82h:19m:26s remains)
INFO - root - 2017-12-05 12:41:23.870779: step 11720, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 84h:50m:21s remains)
INFO - root - 2017-12-05 12:41:33.319585: step 11730, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 81h:07m:08s remains)
INFO - root - 2017-12-05 12:41:42.636653: step 11740, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 80h:23m:27s remains)
INFO - root - 2017-12-05 12:41:52.013070: step 11750, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 82h:45m:02s remains)
INFO - root - 2017-12-05 12:42:01.400916: step 11760, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 81h:01m:32s remains)
INFO - root - 2017-12-05 12:42:10.792959: step 11770, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 82h:38m:56s remains)
INFO - root - 2017-12-05 12:42:20.260510: step 11780, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.936 sec/batch; 83h:21m:21s remains)
INFO - root - 2017-12-05 12:42:29.744927: step 11790, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 81h:29m:33s remains)
INFO - root - 2017-12-05 12:42:39.153226: step 11800, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 79h:08m:19s remains)
2017-12-05 12:42:39.895360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2029505 -4.1908226 -4.213378 -4.2321892 -4.231009 -4.2276888 -4.2237153 -4.2218623 -4.2211046 -4.2277222 -4.2426667 -4.2543416 -4.2555394 -4.2550354 -4.2619963][-4.1802406 -4.1740985 -4.2012849 -4.2207694 -4.222477 -4.2239895 -4.2254386 -4.2271485 -4.2260456 -4.2298317 -4.2375617 -4.2430644 -4.2407179 -4.2385235 -4.241713][-4.1531396 -4.1467872 -4.1760631 -4.1934376 -4.1946893 -4.198873 -4.2099643 -4.2238312 -4.2311864 -4.2381425 -4.2464981 -4.2528467 -4.2511592 -4.2446413 -4.2409081][-4.1280313 -4.1046581 -4.1225276 -4.13401 -4.134572 -4.1417332 -4.1599035 -4.1874747 -4.209362 -4.2259555 -4.2412043 -4.2543664 -4.2559481 -4.2478795 -4.2430239][-4.1054173 -4.0657721 -4.0685706 -4.0727515 -4.0714269 -4.0806794 -4.1008291 -4.1374278 -4.1738219 -4.1992598 -4.2198005 -4.2390404 -4.2452431 -4.2424173 -4.2427545][-4.0879955 -4.0478559 -4.0440073 -4.0420284 -4.0365334 -4.0402274 -4.0504913 -4.0802627 -4.1230259 -4.1572242 -4.1831913 -4.2086124 -4.2206049 -4.2280426 -4.2382789][-4.0733333 -4.0427208 -4.040813 -4.0420322 -4.0423861 -4.0444241 -4.0417094 -4.0566454 -4.091969 -4.1249394 -4.1476746 -4.1715536 -4.1879692 -4.2054248 -4.2248592][-4.0463042 -4.025805 -4.0262165 -4.0335412 -4.0393734 -4.0401993 -4.037045 -4.0509486 -4.0828142 -4.1090856 -4.1222916 -4.1353054 -4.1508245 -4.1727643 -4.1998696][-4.0228395 -4.0066166 -4.0055652 -4.0154057 -4.0212092 -4.0216174 -4.0273242 -4.0470653 -4.0785136 -4.1031179 -4.1114974 -4.1174359 -4.12886 -4.14767 -4.1755581][-4.0170469 -4.0065155 -4.01566 -4.0285454 -4.0319591 -4.0314302 -4.0400915 -4.0586944 -4.0842237 -4.1082573 -4.118371 -4.123992 -4.1333141 -4.14823 -4.1732769][-4.0362377 -4.0298944 -4.0465417 -4.0622959 -4.0644183 -4.0643053 -4.0737028 -4.0873127 -4.1029277 -4.1202097 -4.13198 -4.1384029 -4.1455703 -4.1574874 -4.1812854][-4.0650182 -4.057899 -4.0700603 -4.0830708 -4.0863295 -4.0911651 -4.1024542 -4.1173935 -4.1275916 -4.1355414 -4.1455832 -4.1526012 -4.1572294 -4.1663861 -4.1900678][-4.10901 -4.1036711 -4.1098995 -4.1171837 -4.1181149 -4.1234527 -4.1359572 -4.1536894 -4.1624427 -4.1660471 -4.1735773 -4.1756954 -4.1744094 -4.180203 -4.2006145][-4.1651869 -4.16283 -4.1667938 -4.1703515 -4.170259 -4.1740494 -4.1833324 -4.195755 -4.2026086 -4.2059703 -4.211823 -4.2105546 -4.2052207 -4.2075882 -4.2217064][-4.2171936 -4.2161446 -4.2181263 -4.2204056 -4.2201509 -4.2211685 -4.2252803 -4.2319779 -4.2376108 -4.2431097 -4.2494769 -4.2496805 -4.2459702 -4.2457247 -4.2534518]]...]
INFO - root - 2017-12-05 12:42:49.279043: step 11810, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 87h:55m:58s remains)
INFO - root - 2017-12-05 12:42:58.620910: step 11820, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 81h:56m:50s remains)
INFO - root - 2017-12-05 12:43:08.040787: step 11830, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 81h:47m:59s remains)
INFO - root - 2017-12-05 12:43:17.414788: step 11840, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 76h:09m:27s remains)
INFO - root - 2017-12-05 12:43:26.815476: step 11850, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 79h:30m:26s remains)
INFO - root - 2017-12-05 12:43:36.477772: step 11860, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.971 sec/batch; 86h:30m:33s remains)
INFO - root - 2017-12-05 12:43:45.795376: step 11870, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.936 sec/batch; 83h:20m:47s remains)
INFO - root - 2017-12-05 12:43:55.229087: step 11880, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 84h:03m:40s remains)
INFO - root - 2017-12-05 12:44:04.760920: step 11890, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 79h:42m:17s remains)
INFO - root - 2017-12-05 12:44:14.058101: step 11900, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 81h:11m:15s remains)
2017-12-05 12:44:14.805998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2974348 -4.2888288 -4.2815213 -4.2775731 -4.269105 -4.2560821 -4.2466645 -4.2269826 -4.2020011 -4.18298 -4.1761775 -4.1850948 -4.2141938 -4.24834 -4.275156][-4.3103666 -4.3018332 -4.2913465 -4.2798033 -4.2619605 -4.2375112 -4.2243748 -4.2055855 -4.1882906 -4.1842074 -4.1873465 -4.198348 -4.2276673 -4.2623496 -4.2818952][-4.2955632 -4.2898426 -4.2835732 -4.2686672 -4.24089 -4.203485 -4.1769538 -4.1471033 -4.1319594 -4.1467175 -4.1671662 -4.18922 -4.2223272 -4.2549305 -4.2700357][-4.2702794 -4.2630587 -4.2535582 -4.2330537 -4.1964121 -4.1491776 -4.1102729 -4.0773888 -4.0748215 -4.1086016 -4.1453247 -4.1760268 -4.2104774 -4.2392807 -4.2490907][-4.2447624 -4.2300034 -4.2078047 -4.1720057 -4.1186266 -4.0567856 -4.0058365 -3.981786 -4.0123816 -4.0710354 -4.12272 -4.1611543 -4.1969376 -4.2219214 -4.22724][-4.2365494 -4.2181187 -4.18181 -4.126905 -4.04939 -3.9610968 -3.8810272 -3.8564568 -3.9306276 -4.0225248 -4.0920525 -4.1430531 -4.1834908 -4.2089982 -4.2142191][-4.2289863 -4.2134256 -4.1725092 -4.1062055 -4.01816 -3.9156802 -3.8136194 -3.7855849 -3.8904412 -4.0046225 -4.0784397 -4.1310368 -4.1675825 -4.1973844 -4.2110138][-4.225585 -4.2159119 -4.1841574 -4.1235223 -4.0464473 -3.9582644 -3.8625009 -3.8316653 -3.9326346 -4.031323 -4.0875025 -4.1280932 -4.1573973 -4.1923118 -4.2172375][-4.2436218 -4.2428613 -4.2254486 -4.1812649 -4.1238465 -4.057611 -3.9735649 -3.932085 -4.004055 -4.0729313 -4.1067605 -4.1353869 -4.1591878 -4.1963124 -4.2282271][-4.2649 -4.2720995 -4.2672281 -4.2409911 -4.2020726 -4.1536655 -4.0813179 -4.0362387 -4.0876565 -4.1334562 -4.1505632 -4.1695013 -4.1880779 -4.2195816 -4.2483406][-4.2810555 -4.2896261 -4.2922959 -4.2803359 -4.2580776 -4.2242012 -4.16543 -4.1312251 -4.17472 -4.2109408 -4.2167759 -4.226181 -4.2364497 -4.2545133 -4.2715273][-4.3010712 -4.3022184 -4.302783 -4.2975879 -4.2851753 -4.2644944 -4.2227011 -4.2023287 -4.2398038 -4.2675891 -4.2674608 -4.2700615 -4.2731161 -4.2776637 -4.2840848][-4.3152838 -4.3100433 -4.31068 -4.3121042 -4.3079777 -4.2974844 -4.2720723 -4.2599049 -4.2833805 -4.2973032 -4.2916069 -4.2911282 -4.2916093 -4.2884846 -4.2892089][-4.3194256 -4.3120723 -4.3129897 -4.3193913 -4.3224077 -4.3201208 -4.310185 -4.30476 -4.3140388 -4.3111782 -4.2973032 -4.2926517 -4.2949772 -4.2911577 -4.2901235][-4.3222809 -4.3176084 -4.3173881 -4.3225303 -4.3268747 -4.326283 -4.3193097 -4.3136063 -4.31278 -4.2979741 -4.2793975 -4.2739215 -4.2821655 -4.2839222 -4.2855463]]...]
INFO - root - 2017-12-05 12:44:24.103195: step 11910, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 81h:21m:08s remains)
INFO - root - 2017-12-05 12:44:33.283655: step 11920, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 79h:49m:36s remains)
INFO - root - 2017-12-05 12:44:42.648769: step 11930, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.914 sec/batch; 81h:26m:00s remains)
INFO - root - 2017-12-05 12:44:52.385165: step 11940, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 86h:34m:01s remains)
INFO - root - 2017-12-05 12:45:01.832291: step 11950, loss = 2.03, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 79h:04m:23s remains)
INFO - root - 2017-12-05 12:45:11.170580: step 11960, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 84h:31m:11s remains)
INFO - root - 2017-12-05 12:45:20.826304: step 11970, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.005 sec/batch; 89h:29m:22s remains)
INFO - root - 2017-12-05 12:45:30.371376: step 11980, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 80h:41m:43s remains)
INFO - root - 2017-12-05 12:45:39.674768: step 11990, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 80h:39m:44s remains)
INFO - root - 2017-12-05 12:45:49.253150: step 12000, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 84h:43m:33s remains)
2017-12-05 12:45:49.972154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2405405 -4.2368259 -4.2287469 -4.2233038 -4.2299323 -4.2504568 -4.2666245 -4.2634077 -4.25347 -4.2493663 -4.2529812 -4.2570848 -4.2487636 -4.2346954 -4.2207837][-4.2246346 -4.2186875 -4.210114 -4.2063293 -4.2161164 -4.236444 -4.2513123 -4.2439747 -4.2289491 -4.2222662 -4.2256088 -4.2310123 -4.2264843 -4.2182641 -4.1997228][-4.214128 -4.2077918 -4.1972623 -4.1928363 -4.202774 -4.2221584 -4.2350492 -4.2256513 -4.2025223 -4.1915412 -4.1943011 -4.2020469 -4.209094 -4.2047195 -4.1765833][-4.2142529 -4.2074819 -4.1969714 -4.1942215 -4.2067204 -4.2241225 -4.2296462 -4.2096319 -4.1731548 -4.1576147 -4.1648817 -4.1811152 -4.19816 -4.1952705 -4.1693573][-4.2157478 -4.20737 -4.2006965 -4.2040114 -4.2164459 -4.2259417 -4.2158194 -4.1761189 -4.1234264 -4.1092792 -4.1301689 -4.1570115 -4.1790986 -4.1793537 -4.1657534][-4.2192841 -4.2096281 -4.207171 -4.2146797 -4.2214227 -4.2165861 -4.1810689 -4.1058369 -4.0339355 -4.0405827 -4.0938058 -4.1391997 -4.1629782 -4.1691375 -4.1680841][-4.232152 -4.22517 -4.2203579 -4.2218351 -4.21695 -4.1934271 -4.1231174 -3.9986694 -3.9102221 -3.962976 -4.0661263 -4.1353064 -4.1643014 -4.1736183 -4.1797571][-4.2513151 -4.2445526 -4.2314715 -4.2212615 -4.2004313 -4.1520238 -4.0523109 -3.8972344 -3.8129022 -3.9220276 -4.0669961 -4.1535645 -4.1849494 -4.190938 -4.1888804][-4.263155 -4.2547703 -4.23573 -4.2134352 -4.1737108 -4.1105475 -4.0148134 -3.8908145 -3.8502336 -3.9713755 -4.1106095 -4.1888456 -4.2085695 -4.2041512 -4.1881719][-4.2537241 -4.2464132 -4.2277174 -4.1986465 -4.1538815 -4.1015439 -4.0440016 -3.986289 -3.9874735 -4.0782523 -4.1788368 -4.2310934 -4.2302966 -4.2105169 -4.177979][-4.2238564 -4.2196574 -4.2067947 -4.1851015 -4.155045 -4.133018 -4.11843 -4.1071563 -4.1237564 -4.1785922 -4.2395253 -4.2666497 -4.24773 -4.2119532 -4.1708493][-4.1787653 -4.17784 -4.1743 -4.17322 -4.1733022 -4.1789212 -4.1862025 -4.1941857 -4.2146826 -4.247395 -4.2788062 -4.2833462 -4.2540336 -4.2132797 -4.17465][-4.1325536 -4.1407981 -4.152174 -4.1750684 -4.1981339 -4.2112827 -4.2207665 -4.2366014 -4.2570148 -4.2711477 -4.2749457 -4.2614427 -4.2310939 -4.1979203 -4.1696739][-4.1087756 -4.1313214 -4.1592436 -4.1917019 -4.2171702 -4.2260685 -4.2315412 -4.2449741 -4.2572355 -4.2521005 -4.231678 -4.2101884 -4.192224 -4.1769772 -4.1625781][-4.1318145 -4.1609855 -4.1908183 -4.2152925 -4.2287793 -4.2283516 -4.226336 -4.2307682 -4.231144 -4.2121167 -4.1808653 -4.1617708 -4.1611524 -4.1637373 -4.1592445]]...]
INFO - root - 2017-12-05 12:45:59.203868: step 12010, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 84h:01m:08s remains)
INFO - root - 2017-12-05 12:46:08.534809: step 12020, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 85h:15m:29s remains)
INFO - root - 2017-12-05 12:46:18.050100: step 12030, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.949 sec/batch; 84h:26m:07s remains)
INFO - root - 2017-12-05 12:46:27.523309: step 12040, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.956 sec/batch; 85h:04m:14s remains)
INFO - root - 2017-12-05 12:46:36.956021: step 12050, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 82h:32m:25s remains)
INFO - root - 2017-12-05 12:46:46.350121: step 12060, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 82h:06m:51s remains)
INFO - root - 2017-12-05 12:46:55.753938: step 12070, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 1.003 sec/batch; 89h:14m:35s remains)
INFO - root - 2017-12-05 12:47:04.831638: step 12080, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 86h:45m:36s remains)
INFO - root - 2017-12-05 12:47:14.353419: step 12090, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.865 sec/batch; 77h:00m:42s remains)
INFO - root - 2017-12-05 12:47:23.676910: step 12100, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 85h:06m:20s remains)
2017-12-05 12:47:24.412755: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2227111 -4.2175021 -4.2181978 -4.2138386 -4.1957383 -4.1757212 -4.1676655 -4.1829581 -4.2099261 -4.2301154 -4.2429652 -4.2454071 -4.2456484 -4.2525468 -4.2635808][-4.2060556 -4.2062106 -4.2119069 -4.2114673 -4.1987305 -4.1857409 -4.18404 -4.2012792 -4.2287674 -4.2448673 -4.2482862 -4.2427831 -4.2359176 -4.2396727 -4.2471123][-4.1950822 -4.1995745 -4.208282 -4.2102432 -4.2033634 -4.193275 -4.1929636 -4.2126 -4.2443352 -4.264009 -4.2652335 -4.2563262 -4.2412515 -4.2345324 -4.231216][-4.2053533 -4.2082458 -4.2157416 -4.2169709 -4.2093234 -4.1923671 -4.1787152 -4.1928034 -4.2310262 -4.2625542 -4.2750435 -4.2702 -4.253417 -4.2398915 -4.2277913][-4.2343006 -4.2322717 -4.2358685 -4.2352309 -4.2225695 -4.1926885 -4.153285 -4.1400023 -4.1723938 -4.22464 -4.2615261 -4.2717037 -4.2622814 -4.249835 -4.2377377][-4.2618313 -4.2595181 -4.2598357 -4.2561817 -4.2361531 -4.1926875 -4.1263385 -4.0729308 -4.0808916 -4.14799 -4.2169194 -4.2529755 -4.2611337 -4.2601767 -4.2542992][-4.2810936 -4.285708 -4.2877917 -4.2821469 -4.2571163 -4.2063251 -4.1211572 -4.02776 -3.9915421 -4.0531683 -4.148416 -4.2124357 -4.2448721 -4.2612133 -4.2649155][-4.2860618 -4.302247 -4.3113122 -4.3061247 -4.28209 -4.238759 -4.1611633 -4.0533891 -3.9730747 -3.993154 -4.0772533 -4.15633 -4.2103095 -4.2445927 -4.2624049][-4.27904 -4.3030009 -4.3195872 -4.3206606 -4.3055329 -4.2773786 -4.224422 -4.1393003 -4.0537205 -4.025609 -4.0576053 -4.1150041 -4.1710162 -4.2128015 -4.240562][-4.2593145 -4.2854171 -4.3075924 -4.3202395 -4.3201308 -4.3062468 -4.2733665 -4.2165756 -4.1528273 -4.116488 -4.1152306 -4.1358171 -4.1650052 -4.1909 -4.2156157][-4.2383957 -4.2637477 -4.2863212 -4.3044395 -4.316576 -4.3146448 -4.293344 -4.2527914 -4.2105412 -4.1846848 -4.181601 -4.1889238 -4.1984382 -4.2040524 -4.2105293][-4.2324672 -4.2544985 -4.2701082 -4.2850943 -4.2989693 -4.3015394 -4.283463 -4.2505965 -4.219861 -4.2042851 -4.20979 -4.2247124 -4.2358718 -4.234714 -4.2269282][-4.2452269 -4.2591457 -4.2653227 -4.27134 -4.2775126 -4.2751775 -4.2572808 -4.2276821 -4.1985497 -4.184814 -4.1969576 -4.224741 -4.246387 -4.2485471 -4.2408853][-4.2657442 -4.2690649 -4.267787 -4.2684274 -4.2670236 -4.2586527 -4.2373843 -4.2063127 -4.1738334 -4.1602106 -4.1768756 -4.21156 -4.2379642 -4.24285 -4.2379436][-4.2838993 -4.2745132 -4.2675729 -4.2673674 -4.2649035 -4.2560091 -4.2337294 -4.2014151 -4.1700654 -4.1581244 -4.1775761 -4.2082391 -4.2273498 -4.2282815 -4.2255974]]...]
INFO - root - 2017-12-05 12:47:33.556608: step 12110, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 85h:52m:38s remains)
INFO - root - 2017-12-05 12:47:43.016395: step 12120, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 85h:55m:37s remains)
INFO - root - 2017-12-05 12:47:52.726406: step 12130, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 85h:56m:14s remains)
INFO - root - 2017-12-05 12:48:01.979994: step 12140, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 78h:36m:31s remains)
INFO - root - 2017-12-05 12:48:11.324605: step 12150, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 83h:20m:54s remains)
INFO - root - 2017-12-05 12:48:20.641898: step 12160, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 78h:14m:37s remains)
INFO - root - 2017-12-05 12:48:29.893081: step 12170, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 83h:03m:23s remains)
INFO - root - 2017-12-05 12:48:39.354607: step 12180, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 86h:31m:52s remains)
INFO - root - 2017-12-05 12:48:48.530985: step 12190, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.941 sec/batch; 83h:42m:16s remains)
INFO - root - 2017-12-05 12:48:58.036431: step 12200, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 86h:10m:16s remains)
2017-12-05 12:48:58.775396: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2305546 -4.2355986 -4.2248411 -4.2013216 -4.1687126 -4.15172 -4.1770086 -4.1988211 -4.2037964 -4.2013392 -4.1911325 -4.1866632 -4.1969934 -4.2162318 -4.233727][-4.229907 -4.2401042 -4.2272577 -4.2025857 -4.1719217 -4.1523957 -4.1683807 -4.184391 -4.1864438 -4.1785336 -4.1640873 -4.15969 -4.1719747 -4.1941543 -4.2148876][-4.2282767 -4.236361 -4.2191138 -4.1981544 -4.1730514 -4.1562281 -4.1596928 -4.1694746 -4.1746979 -4.1677127 -4.1533313 -4.1493216 -4.1590233 -4.1799712 -4.19987][-4.2238245 -4.2251496 -4.2025671 -4.185431 -4.165576 -4.1530671 -4.1480341 -4.15407 -4.1679406 -4.1678562 -4.1583848 -4.153801 -4.1549516 -4.1705041 -4.1868944][-4.2132621 -4.2129831 -4.1896224 -4.1733823 -4.1533508 -4.1431112 -4.1288538 -4.1338186 -4.1547647 -4.1638274 -4.1656308 -4.1630554 -4.1547856 -4.164968 -4.1802583][-4.2012448 -4.2070956 -4.1893873 -4.1721363 -4.1438813 -4.1251407 -4.0963464 -4.09736 -4.1257768 -4.1454272 -4.1634841 -4.1690979 -4.1580687 -4.1654844 -4.1820035][-4.1864061 -4.203867 -4.195899 -4.1738539 -4.132081 -4.0973969 -4.0503817 -4.0454965 -4.0824003 -4.1168923 -4.1548486 -4.1762977 -4.1733813 -4.182786 -4.1991282][-4.1687093 -4.2006903 -4.2038918 -4.18106 -4.1299963 -4.080523 -4.0189333 -4.0068135 -4.0533071 -4.10293 -4.1556835 -4.1916137 -4.2002449 -4.2150645 -4.2300282][-4.1506438 -4.1956625 -4.2099648 -4.1937203 -4.1437159 -4.0924172 -4.03174 -4.0172787 -4.064733 -4.1199813 -4.1739483 -4.212791 -4.2281551 -4.245501 -4.259306][-4.1405044 -4.1910768 -4.2136788 -4.208344 -4.1710153 -4.1318655 -4.0870972 -4.073843 -4.11058 -4.1564741 -4.200243 -4.2333941 -4.2492037 -4.2667179 -4.2807922][-4.1431122 -4.1915512 -4.2173281 -4.2219925 -4.1998782 -4.1749721 -4.1446142 -4.1340718 -4.1587009 -4.1917539 -4.2234464 -4.2503991 -4.2669888 -4.2848291 -4.2973042][-4.1615062 -4.2020717 -4.2271681 -4.2381353 -4.2265935 -4.2102442 -4.18909 -4.1793289 -4.196856 -4.221477 -4.2454262 -4.2690306 -4.2866182 -4.3047881 -4.3140454][-4.198667 -4.2298732 -4.2505822 -4.2641091 -4.2587886 -4.2481833 -4.23347 -4.2250681 -4.2366624 -4.2530584 -4.2706914 -4.2895579 -4.3063254 -4.3220892 -4.3271408][-4.2363739 -4.2599034 -4.2759356 -4.287941 -4.2851343 -4.278553 -4.2690659 -4.263535 -4.2714663 -4.2831693 -4.2944808 -4.3085027 -4.3221126 -4.3329659 -4.3339591][-4.2599573 -4.2788658 -4.2932081 -4.3037839 -4.302547 -4.2982717 -4.2939372 -4.2915344 -4.2981644 -4.3067527 -4.3144245 -4.3243213 -4.3334055 -4.33894 -4.3362031]]...]
INFO - root - 2017-12-05 12:49:08.288129: step 12210, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.983 sec/batch; 87h:29m:05s remains)
INFO - root - 2017-12-05 12:49:17.614249: step 12220, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 75h:39m:31s remains)
INFO - root - 2017-12-05 12:49:26.951337: step 12230, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 81h:49m:30s remains)
INFO - root - 2017-12-05 12:49:36.613397: step 12240, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 81h:31m:27s remains)
INFO - root - 2017-12-05 12:49:45.902743: step 12250, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.980 sec/batch; 87h:12m:57s remains)
INFO - root - 2017-12-05 12:49:54.953602: step 12260, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 83h:35m:38s remains)
INFO - root - 2017-12-05 12:50:04.247533: step 12270, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.004 sec/batch; 89h:17m:05s remains)
INFO - root - 2017-12-05 12:50:13.847482: step 12280, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 81h:49m:06s remains)
INFO - root - 2017-12-05 12:50:23.330219: step 12290, loss = 2.03, batch loss = 1.98 (8.5 examples/sec; 0.942 sec/batch; 83h:46m:20s remains)
INFO - root - 2017-12-05 12:50:32.700069: step 12300, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 82h:06m:34s remains)
2017-12-05 12:50:33.441342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2704883 -4.2639322 -4.2594066 -4.2632103 -4.2587037 -4.2442636 -4.2340927 -4.2378454 -4.2474179 -4.2504897 -4.2518535 -4.2535214 -4.2514195 -4.2579532 -4.2703013][-4.2596979 -4.2544165 -4.248271 -4.2489624 -4.2446771 -4.231482 -4.2216091 -4.228498 -4.2397237 -4.2417779 -4.2424831 -4.2441397 -4.2446523 -4.2558231 -4.2731047][-4.2588353 -4.2548766 -4.2430568 -4.2369332 -4.2295685 -4.2192349 -4.2124538 -4.2219229 -4.2334666 -4.2331052 -4.2320342 -4.2360053 -4.2394447 -4.2501655 -4.2682261][-4.2530551 -4.2487025 -4.2339668 -4.2212191 -4.2120519 -4.2036104 -4.1992745 -4.2103682 -4.2237577 -4.2234011 -4.2201004 -4.2241688 -4.2293615 -4.2365556 -4.2504587][-4.2310791 -4.2345576 -4.2267995 -4.2143397 -4.2034945 -4.1904097 -4.17874 -4.1825471 -4.1949453 -4.1947803 -4.187429 -4.1909075 -4.1983843 -4.2032709 -4.2093639][-4.2039866 -4.2144127 -4.212338 -4.2033706 -4.190526 -4.1685762 -4.1433082 -4.1362495 -4.1470151 -4.1531482 -4.1464815 -4.1460867 -4.1507168 -4.1509476 -4.1490459][-4.1828103 -4.1968546 -4.1916842 -4.1764927 -4.1575494 -4.1267085 -4.0875168 -4.0671759 -4.0774822 -4.0955739 -4.0949535 -4.0935593 -4.1011891 -4.1013885 -4.0928369][-4.1768641 -4.1829295 -4.1671062 -4.1394334 -4.10898 -4.071589 -4.027844 -4.00499 -4.0243258 -4.0572629 -4.0663242 -4.0665646 -4.073267 -4.0728116 -4.0641589][-4.1741261 -4.1699123 -4.1519074 -4.1261353 -4.1006632 -4.0735149 -4.0429134 -4.0313139 -4.0552378 -4.0895185 -4.1029673 -4.1037426 -4.1047392 -4.0997634 -4.0957122][-4.1898146 -4.1801991 -4.1691885 -4.156692 -4.1434617 -4.1318321 -4.1152015 -4.1107135 -4.1283808 -4.1496406 -4.1588693 -4.1567116 -4.1506352 -4.1410327 -4.1409345][-4.19828 -4.1884694 -4.1843939 -4.1817975 -4.1805887 -4.1792059 -4.1695991 -4.1660914 -4.1775351 -4.1894269 -4.1937218 -4.1895394 -4.1809196 -4.16998 -4.1732168][-4.2143631 -4.2063689 -4.206171 -4.2114258 -4.2182746 -4.2221465 -4.2159915 -4.21125 -4.2160497 -4.2218108 -4.2222204 -4.2177992 -4.2097292 -4.1998839 -4.2026482][-4.2384396 -4.2332196 -4.2350926 -4.241322 -4.2465792 -4.2493896 -4.2455373 -4.2398887 -4.2380295 -4.2396736 -4.2411537 -4.2393494 -4.2335296 -4.2251291 -4.22516][-4.2668786 -4.2627139 -4.2633629 -4.2665672 -4.2675624 -4.267786 -4.2662559 -4.2625012 -4.2589321 -4.2592344 -4.2607031 -4.2602615 -4.25576 -4.2497411 -4.2480621][-4.2907281 -4.2894974 -4.2900867 -4.2895112 -4.2861309 -4.2824039 -4.2795687 -4.2759485 -4.2710977 -4.2685742 -4.2693849 -4.2698927 -4.268425 -4.2667432 -4.2663488]]...]
INFO - root - 2017-12-05 12:50:42.857039: step 12310, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 86h:13m:10s remains)
INFO - root - 2017-12-05 12:50:52.412026: step 12320, loss = 2.05, batch loss = 1.99 (7.7 examples/sec; 1.040 sec/batch; 92h:27m:10s remains)
INFO - root - 2017-12-05 12:51:01.735476: step 12330, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 82h:10m:47s remains)
INFO - root - 2017-12-05 12:51:11.121691: step 12340, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 83h:06m:32s remains)
INFO - root - 2017-12-05 12:51:20.464073: step 12350, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 87h:20m:34s remains)
INFO - root - 2017-12-05 12:51:29.829602: step 12360, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.936 sec/batch; 83h:15m:29s remains)
INFO - root - 2017-12-05 12:51:39.320653: step 12370, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 84h:19m:11s remains)
INFO - root - 2017-12-05 12:51:48.601840: step 12380, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 81h:15m:03s remains)
INFO - root - 2017-12-05 12:51:58.105139: step 12390, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 84h:29m:48s remains)
INFO - root - 2017-12-05 12:52:07.509995: step 12400, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 85h:29m:25s remains)
2017-12-05 12:52:08.287908: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33156 -4.3248734 -4.3168 -4.3080859 -4.3057604 -4.3118181 -4.3207765 -4.3226957 -4.3189983 -4.3167562 -4.3136339 -4.3099461 -4.3048611 -4.301878 -4.304749][-4.3262444 -4.3133535 -4.3010511 -4.2934031 -4.2925491 -4.2973323 -4.3042502 -4.2990456 -4.2879806 -4.2817879 -4.2788835 -4.2754421 -4.2728548 -4.2740746 -4.2844353][-4.31018 -4.2863879 -4.2654514 -4.25594 -4.2555456 -4.2549109 -4.2538209 -4.242897 -4.2297292 -4.2275167 -4.2309113 -4.2261057 -4.2211781 -4.2278342 -4.2480216][-4.2807603 -4.2417932 -4.208353 -4.1964622 -4.1998744 -4.1925397 -4.1775875 -4.153203 -4.1447663 -4.1604128 -4.17813 -4.1762686 -4.1672525 -4.1736264 -4.2007604][-4.2468448 -4.1946797 -4.149262 -4.1296468 -4.1290817 -4.1072822 -4.064671 -4.0182843 -4.0290875 -4.0837636 -4.1269884 -4.1329646 -4.1226649 -4.12665 -4.1528521][-4.2157125 -4.1556697 -4.098917 -4.0642047 -4.0481429 -4.002564 -3.9184897 -3.841172 -3.8834364 -3.995054 -4.0752115 -4.0949764 -4.0896077 -4.0886545 -4.1077995][-4.1991839 -4.1383247 -4.0797119 -4.0376239 -4.0062361 -3.9417591 -3.8392816 -3.7549422 -3.8100395 -3.9412885 -4.0428061 -4.0776639 -4.0814223 -4.0780754 -4.0902753][-4.1999578 -4.1472425 -4.0991573 -4.06286 -4.0302982 -3.970294 -3.8838513 -3.8227093 -3.8603299 -3.9593105 -4.0409365 -4.0735283 -4.084466 -4.0848556 -4.097497][-4.2036042 -4.1636181 -4.1311841 -4.1082678 -4.0871296 -4.0443273 -3.9843841 -3.9464159 -3.9641223 -4.0218134 -4.0662723 -4.0789413 -4.0805149 -4.0834346 -4.1025081][-4.2079253 -4.1772132 -4.1578512 -4.1498451 -4.1445408 -4.1245322 -4.0907907 -4.0724025 -4.0816884 -4.108151 -4.1225891 -4.1149659 -4.1003065 -4.1001029 -4.1206841][-4.2231383 -4.1993065 -4.1879497 -4.1887665 -4.194663 -4.1944857 -4.1793313 -4.1714711 -4.1774011 -4.1866918 -4.1850166 -4.1687956 -4.1488113 -4.1459212 -4.1630111][-4.2520151 -4.2346778 -4.2275896 -4.2332726 -4.2451358 -4.2533822 -4.248642 -4.24445 -4.2500072 -4.2521443 -4.2466474 -4.2316337 -4.2144895 -4.2088242 -4.2202697][-4.2753797 -4.2625833 -4.2601585 -4.2692385 -4.2816358 -4.2902007 -4.2881284 -4.2864995 -4.2917452 -4.2943029 -4.2903848 -4.2785769 -4.2647319 -4.2581463 -4.263411][-4.2855053 -4.2753587 -4.275197 -4.2836938 -4.29404 -4.3013 -4.3026657 -4.3043041 -4.3091779 -4.3130379 -4.3113413 -4.3034897 -4.294734 -4.2902694 -4.290854][-4.2988114 -4.2906747 -4.289228 -4.2933149 -4.2993431 -4.3052573 -4.3092332 -4.31316 -4.3170028 -4.3195391 -4.3189425 -4.3146434 -4.3098435 -4.3071375 -4.30622]]...]
INFO - root - 2017-12-05 12:52:17.744851: step 12410, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 78h:13m:02s remains)
INFO - root - 2017-12-05 12:52:27.265162: step 12420, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.977 sec/batch; 86h:53m:53s remains)
INFO - root - 2017-12-05 12:52:36.682996: step 12430, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 85h:20m:42s remains)
INFO - root - 2017-12-05 12:52:45.803679: step 12440, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 83h:06m:26s remains)
INFO - root - 2017-12-05 12:52:55.202296: step 12450, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 85h:20m:18s remains)
INFO - root - 2017-12-05 12:53:04.686604: step 12460, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 77h:06m:21s remains)
INFO - root - 2017-12-05 12:53:14.036570: step 12470, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 81h:13m:43s remains)
INFO - root - 2017-12-05 12:53:23.475489: step 12480, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 83h:08m:07s remains)
INFO - root - 2017-12-05 12:53:32.908252: step 12490, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 85h:30m:57s remains)
INFO - root - 2017-12-05 12:53:42.209145: step 12500, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 84h:05m:58s remains)
2017-12-05 12:53:42.929355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1232758 -4.1478362 -4.1904407 -4.2165842 -4.2242856 -4.2140665 -4.1881485 -4.1695919 -4.1765194 -4.1998386 -4.2194228 -4.2349167 -4.2565269 -4.2577457 -4.242312][-4.1028142 -4.1234407 -4.1622419 -4.1888456 -4.2069902 -4.204639 -4.1874275 -4.174511 -4.1855068 -4.2060194 -4.2188659 -4.2269464 -4.238349 -4.2334843 -4.2170234][-4.1132517 -4.1196046 -4.1470337 -4.1718516 -4.1985517 -4.2106156 -4.2023892 -4.1914153 -4.202599 -4.2225046 -4.2302179 -4.2309179 -4.2344937 -4.2243476 -4.2056317][-4.1564584 -4.1488118 -4.1599693 -4.1757293 -4.1987133 -4.2173686 -4.2137213 -4.2018642 -4.2103791 -4.2347145 -4.2461171 -4.2440343 -4.242239 -4.2305613 -4.2091284][-4.2057152 -4.1947417 -4.19009 -4.1924615 -4.2009559 -4.208324 -4.197258 -4.1747861 -4.1757669 -4.2105041 -4.2363453 -4.2450619 -4.2457771 -4.237576 -4.2171087][-4.2401094 -4.2304692 -4.2203 -4.2144227 -4.2041311 -4.180058 -4.1423364 -4.0929189 -4.0844836 -4.1415229 -4.1991496 -4.2325354 -4.2485752 -4.2494836 -4.2351747][-4.2371335 -4.2233834 -4.2109666 -4.2021933 -4.1776156 -4.1208963 -4.0441322 -3.9561777 -3.9382005 -4.0367193 -4.1427741 -4.2120142 -4.253808 -4.2679644 -4.2584639][-4.2056842 -4.1875291 -4.1737638 -4.161067 -4.1278167 -4.0584068 -3.9557643 -3.8348799 -3.8063438 -3.9431047 -4.0880508 -4.1838632 -4.242753 -4.263833 -4.2577934][-4.1823783 -4.1644373 -4.1520176 -4.1404924 -4.1142774 -4.0597644 -3.9680202 -3.8507879 -3.8134484 -3.9365973 -4.0797095 -4.1726069 -4.2273917 -4.2450309 -4.2429619][-4.1918654 -4.1787319 -4.170002 -4.1649413 -4.1561041 -4.1272221 -4.0695953 -3.990087 -3.955085 -4.0275035 -4.1249733 -4.1882691 -4.2233906 -4.2298293 -4.2242031][-4.2224188 -4.2153883 -4.2097816 -4.2109065 -4.2165117 -4.2051086 -4.1734381 -4.124938 -4.0941372 -4.1261172 -4.1786337 -4.2115383 -4.2280078 -4.2215304 -4.20611][-4.2522221 -4.2453561 -4.237504 -4.2387 -4.2497621 -4.2466993 -4.2254486 -4.1941814 -4.1716933 -4.18864 -4.2176547 -4.2290993 -4.2324748 -4.2146935 -4.188365][-4.2745194 -4.263546 -4.2503881 -4.2461095 -4.2540708 -4.2513371 -4.2303429 -4.2098379 -4.2016129 -4.2192183 -4.2373662 -4.2381115 -4.233892 -4.2103953 -4.1805015][-4.2851939 -4.2718477 -4.255785 -4.2431731 -4.2414255 -4.2385244 -4.2217855 -4.20849 -4.2109742 -4.2321596 -4.24836 -4.2473946 -4.2411466 -4.2177429 -4.1910639][-4.2892804 -4.2748055 -4.2577 -4.2394285 -4.2290597 -4.2266479 -4.2177811 -4.20933 -4.2156487 -4.2363882 -4.2514472 -4.2518239 -4.2459278 -4.2279329 -4.2080727]]...]
INFO - root - 2017-12-05 12:53:52.361616: step 12510, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 80h:59m:30s remains)
INFO - root - 2017-12-05 12:54:01.801182: step 12520, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 82h:28m:52s remains)
INFO - root - 2017-12-05 12:54:11.043584: step 12530, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 84h:10m:00s remains)
INFO - root - 2017-12-05 12:54:20.413159: step 12540, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 75h:31m:23s remains)
INFO - root - 2017-12-05 12:54:29.754269: step 12550, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 83h:26m:12s remains)
INFO - root - 2017-12-05 12:54:39.408231: step 12560, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 83h:23m:30s remains)
INFO - root - 2017-12-05 12:54:48.801644: step 12570, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 75h:36m:22s remains)
INFO - root - 2017-12-05 12:54:58.269801: step 12580, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.895 sec/batch; 79h:31m:22s remains)
INFO - root - 2017-12-05 12:55:07.459008: step 12590, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 86h:06m:17s remains)
INFO - root - 2017-12-05 12:55:16.879267: step 12600, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 86h:45m:23s remains)
2017-12-05 12:55:17.674696: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1729259 -4.1237731 -4.0958533 -4.121223 -4.1421032 -4.1200652 -4.0568151 -4.0027614 -4.0187173 -4.0605021 -4.0596881 -4.04571 -4.0477281 -4.0821886 -4.114902][-4.1704025 -4.1287117 -4.1049418 -4.1256618 -4.1405587 -4.1166515 -4.0521164 -3.9951394 -4.0117149 -4.0619531 -4.0715761 -4.0692492 -4.0740762 -4.1050582 -4.1347055][-4.1596665 -4.1262908 -4.1104465 -4.1289926 -4.1382861 -4.1097755 -4.0430884 -3.9837036 -3.997858 -4.0489845 -4.0690289 -4.0788293 -4.0827227 -4.1097975 -4.1341815][-4.132205 -4.1038485 -4.096056 -4.1127863 -4.1157775 -4.0840611 -4.0168285 -3.9611166 -3.9799135 -4.033668 -4.0616779 -4.0755324 -4.07628 -4.0994148 -4.1204042][-4.1019964 -4.0778179 -4.0705748 -4.0849481 -4.0872808 -4.0551734 -3.9876575 -3.9313421 -3.9628546 -4.0318518 -4.070178 -4.075665 -4.0638766 -4.080554 -4.1009154][-4.0843625 -4.0583606 -4.0407395 -4.0519505 -4.0581222 -4.0204186 -3.9370713 -3.8612351 -3.9175825 -4.0212321 -4.07045 -4.0663366 -4.0411592 -4.0534363 -4.0804167][-4.06262 -4.0321159 -4.0051665 -4.0128865 -4.0193577 -3.9622719 -3.8393338 -3.7229745 -3.8227801 -3.9756367 -4.0346746 -4.0161004 -3.980767 -4.0033712 -4.0446858][-4.0452666 -4.0126662 -3.9799163 -3.9827363 -3.9868677 -3.9148622 -3.7527738 -3.6034906 -3.7442236 -3.9282734 -3.9867482 -3.9499555 -3.9100478 -3.9507468 -4.0102644][-4.0618739 -4.0340147 -4.0051193 -4.0105982 -4.0204639 -3.9586415 -3.8189557 -3.7060931 -3.8151202 -3.9598796 -3.9972396 -3.9556441 -3.9214232 -3.9738503 -4.03691][-4.0952826 -4.0670776 -4.0405593 -4.0533886 -4.0720048 -4.027595 -3.9305742 -3.8599885 -3.9271231 -4.0258374 -4.0408516 -4.0039835 -3.9777293 -4.0315628 -4.0931587][-4.1173782 -4.0902114 -4.0627193 -4.078433 -4.1018586 -4.0722666 -4.0072837 -3.9591105 -4.0030313 -4.0758691 -4.085196 -4.05446 -4.0309191 -4.0789738 -4.1351705][-4.1263995 -4.1067815 -4.085958 -4.1049728 -4.126667 -4.1056595 -4.0563183 -4.0134091 -4.0436487 -4.0996294 -4.1092148 -4.0824842 -4.0615921 -4.1013021 -4.1472406][-4.1266928 -4.1186762 -4.1075644 -4.1239991 -4.1423931 -4.1247149 -4.0841818 -4.0459652 -4.0680742 -4.1113367 -4.1214828 -4.1005473 -4.0847178 -4.112155 -4.1481571][-4.1402764 -4.1397433 -4.1310248 -4.1427979 -4.1633983 -4.15449 -4.1243181 -4.0956841 -4.1132112 -4.1475582 -4.156085 -4.1417809 -4.1285076 -4.1456718 -4.1742578][-4.1763234 -4.1771436 -4.1699185 -4.1769676 -4.1939421 -4.190897 -4.1710429 -4.1568294 -4.1762166 -4.2068024 -4.2159939 -4.2068977 -4.1968355 -4.2032828 -4.2206545]]...]
INFO - root - 2017-12-05 12:55:27.011477: step 12610, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.011 sec/batch; 89h:49m:38s remains)
INFO - root - 2017-12-05 12:55:36.136858: step 12620, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 86h:43m:54s remains)
INFO - root - 2017-12-05 12:55:45.770412: step 12630, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 85h:49m:58s remains)
INFO - root - 2017-12-05 12:55:55.222586: step 12640, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.929 sec/batch; 82h:33m:24s remains)
INFO - root - 2017-12-05 12:56:04.580976: step 12650, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 86h:59m:09s remains)
INFO - root - 2017-12-05 12:56:13.978159: step 12660, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.894 sec/batch; 79h:24m:12s remains)
INFO - root - 2017-12-05 12:56:23.456352: step 12670, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 85h:26m:20s remains)
INFO - root - 2017-12-05 12:56:32.827891: step 12680, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 88h:36m:04s remains)
INFO - root - 2017-12-05 12:56:42.283219: step 12690, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 88h:45m:26s remains)
INFO - root - 2017-12-05 12:56:51.603292: step 12700, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 86h:19m:10s remains)
2017-12-05 12:56:52.373868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1578255 -4.1502481 -4.1494341 -4.162189 -4.19293 -4.22951 -4.264884 -4.26633 -4.2412338 -4.2128634 -4.1948667 -4.204452 -4.2417245 -4.275547 -4.2894473][-4.164412 -4.1540189 -4.1415229 -4.1413655 -4.1716361 -4.2153111 -4.2630539 -4.2751842 -4.2585025 -4.2426467 -4.2326193 -4.2387018 -4.2690372 -4.2971873 -4.3070383][-4.1753712 -4.1629767 -4.1404762 -4.1271133 -4.1459084 -4.1842651 -4.2355652 -4.2629051 -4.2620888 -4.2622528 -4.2623234 -4.2682457 -4.2905364 -4.3103232 -4.3090243][-4.1937437 -4.1863546 -4.1587892 -4.1330495 -4.1296625 -4.1473584 -4.1916471 -4.2343154 -4.2518311 -4.2656512 -4.2768164 -4.2860732 -4.3037138 -4.3172069 -4.3099823][-4.2144523 -4.2149963 -4.1884336 -4.1507874 -4.1173825 -4.1008162 -4.1295586 -4.1845279 -4.2248116 -4.2560315 -4.2803903 -4.2954736 -4.3118138 -4.3190069 -4.3095827][-4.2201815 -4.2346945 -4.2190704 -4.1733007 -4.1071162 -4.0450788 -4.0442867 -4.1139932 -4.1841164 -4.2376022 -4.2759705 -4.2961969 -4.3109875 -4.314424 -4.3068256][-4.2199655 -4.244863 -4.2385879 -4.1852484 -4.09143 -3.9830329 -3.9425449 -4.0246506 -4.1264696 -4.1997743 -4.248157 -4.2723742 -4.2849927 -4.2916045 -4.2923074][-4.2146478 -4.243721 -4.2423463 -4.1870217 -4.0825434 -3.9512193 -3.8811207 -3.9656408 -4.0848236 -4.1650019 -4.2124271 -4.2310805 -4.2355795 -4.2477493 -4.2622242][-4.1952262 -4.2304716 -4.2378364 -4.1891069 -4.0954876 -3.9796154 -3.9135702 -3.9780169 -4.0869823 -4.1614184 -4.198544 -4.200469 -4.1885891 -4.2014809 -4.2287207][-4.1820292 -4.219698 -4.2306895 -4.190114 -4.1145325 -4.0265889 -3.9779196 -4.0205326 -4.1062903 -4.16667 -4.1893044 -4.1749635 -4.1491423 -4.1617465 -4.1981444][-4.1763926 -4.2169 -4.229651 -4.1957035 -4.1351528 -4.0719862 -4.041852 -4.0695162 -4.131484 -4.1727557 -4.1790228 -4.1524391 -4.1173725 -4.1218338 -4.1597304][-4.1785 -4.2225647 -4.237926 -4.2098923 -4.1590753 -4.1128225 -4.094502 -4.1125755 -4.1545005 -4.1796088 -4.1726823 -4.1416159 -4.1028318 -4.095993 -4.1283708][-4.1835084 -4.2252903 -4.2422819 -4.2218332 -4.1789517 -4.1423373 -4.1289697 -4.1399188 -4.1692324 -4.18869 -4.184042 -4.1573248 -4.1176262 -4.0977459 -4.1147146][-4.1923141 -4.2295055 -4.2489204 -4.2372532 -4.1996703 -4.1666117 -4.1513295 -4.1552935 -4.1770272 -4.2013702 -4.207253 -4.1872778 -4.1516137 -4.1257091 -4.1271172][-4.2120867 -4.24358 -4.2620234 -4.2548504 -4.2193789 -4.1852155 -4.1634946 -4.1608849 -4.1815972 -4.2132092 -4.2287326 -4.2123427 -4.1777244 -4.1487379 -4.1416874]]...]
INFO - root - 2017-12-05 12:57:01.524545: step 12710, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 77h:28m:07s remains)
INFO - root - 2017-12-05 12:57:11.014162: step 12720, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 80h:11m:34s remains)
INFO - root - 2017-12-05 12:57:20.444057: step 12730, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 79h:32m:28s remains)
INFO - root - 2017-12-05 12:57:29.761240: step 12740, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 83h:37m:23s remains)
INFO - root - 2017-12-05 12:57:39.270089: step 12750, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 86h:57m:19s remains)
INFO - root - 2017-12-05 12:57:48.620271: step 12760, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 74h:50m:11s remains)
INFO - root - 2017-12-05 12:57:58.122631: step 12770, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.982 sec/batch; 87h:10m:15s remains)
INFO - root - 2017-12-05 12:58:07.576046: step 12780, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 78h:29m:57s remains)
INFO - root - 2017-12-05 12:58:16.954087: step 12790, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 82h:27m:04s remains)
INFO - root - 2017-12-05 12:58:26.100388: step 12800, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.850 sec/batch; 75h:26m:48s remains)
2017-12-05 12:58:26.719913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2224884 -4.2308073 -4.2401714 -4.2454357 -4.2521687 -4.2619338 -4.2671456 -4.2648044 -4.2588625 -4.244422 -4.229979 -4.2227955 -4.2200332 -4.2120342 -4.1971021][-4.2049813 -4.2194762 -4.2313905 -4.2320051 -4.2296576 -4.2378254 -4.2442422 -4.2396584 -4.2345543 -4.2276669 -4.2216954 -4.2194996 -4.2137403 -4.1949749 -4.1703987][-4.19748 -4.2151518 -4.2243524 -4.21653 -4.2061481 -4.2085967 -4.2124615 -4.2053733 -4.2005272 -4.2018766 -4.2092242 -4.2185016 -4.2138743 -4.1879182 -4.1584868][-4.2040882 -4.2212882 -4.222681 -4.20531 -4.1874452 -4.1816607 -4.1783295 -4.1636419 -4.1586285 -4.1707349 -4.19371 -4.2161703 -4.2181172 -4.187942 -4.1577663][-4.2201796 -4.2301617 -4.2218919 -4.1969528 -4.1706085 -4.1578665 -4.1399636 -4.1077533 -4.0968204 -4.1282625 -4.1746926 -4.2122393 -4.2238011 -4.190269 -4.1580315][-4.2377133 -4.2379203 -4.2220325 -4.1897783 -4.1527867 -4.1297889 -4.0936027 -4.0323071 -4.0102844 -4.0654798 -4.1414366 -4.195776 -4.2157035 -4.1828475 -4.1526308][-4.2447462 -4.2383 -4.2190847 -4.1837759 -4.1385412 -4.1043096 -4.051168 -3.9646838 -3.928309 -4.0023007 -4.1036839 -4.1707959 -4.1968837 -4.1664486 -4.1421485][-4.2474627 -4.2395511 -4.2202034 -4.1877909 -4.1464505 -4.1114111 -4.0529413 -3.9567761 -3.9095051 -3.9843888 -4.0907812 -4.157248 -4.1827216 -4.1550465 -4.1367717][-4.2519207 -4.2454133 -4.22934 -4.2050133 -4.1734357 -4.144516 -4.0951366 -4.0157032 -3.974108 -4.0265894 -4.1102 -4.1645479 -4.1831326 -4.1584086 -4.147068][-4.2544122 -4.2488227 -4.2350621 -4.2176919 -4.1978364 -4.1768522 -4.1421356 -4.0858903 -4.0510015 -4.077467 -4.1315813 -4.1721382 -4.1863751 -4.1676087 -4.1615157][-4.2487397 -4.2431841 -4.2281256 -4.2123866 -4.1984811 -4.1847525 -4.1627417 -4.12556 -4.0977244 -4.1065989 -4.1386232 -4.1687627 -4.1796861 -4.1667528 -4.16627][-4.2393079 -4.2341862 -4.2166433 -4.2016706 -4.1927223 -4.1821761 -4.1674409 -4.1433907 -4.124465 -4.1274014 -4.1475396 -4.1688323 -4.1774487 -4.1687107 -4.170351][-4.2348061 -4.230165 -4.2126403 -4.2002792 -4.1956167 -4.18919 -4.1793747 -4.1628761 -4.1508536 -4.1521072 -4.16573 -4.1791735 -4.185873 -4.1793046 -4.1797438][-4.2403169 -4.2355795 -4.2203383 -4.2099795 -4.2084246 -4.2062283 -4.2011166 -4.1907344 -4.1839833 -4.1846209 -4.1919036 -4.198277 -4.2030053 -4.1982675 -4.1955914][-4.2530127 -4.2489719 -4.2363472 -4.227519 -4.22707 -4.22737 -4.2250104 -4.218751 -4.216217 -4.2185893 -4.2224021 -4.2246795 -4.2281914 -4.2253695 -4.2211795]]...]
INFO - root - 2017-12-05 12:58:36.125495: step 12810, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 78h:11m:06s remains)
INFO - root - 2017-12-05 12:58:45.814854: step 12820, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 82h:29m:19s remains)
INFO - root - 2017-12-05 12:58:55.241023: step 12830, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 86h:15m:54s remains)
INFO - root - 2017-12-05 12:59:04.570464: step 12840, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 89h:22m:29s remains)
INFO - root - 2017-12-05 12:59:14.104639: step 12850, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 86h:47m:20s remains)
INFO - root - 2017-12-05 12:59:23.570159: step 12860, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 86h:32m:28s remains)
INFO - root - 2017-12-05 12:59:32.923152: step 12870, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 88h:11m:02s remains)
INFO - root - 2017-12-05 12:59:42.526479: step 12880, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 86h:13m:26s remains)
INFO - root - 2017-12-05 12:59:51.935629: step 12890, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.962 sec/batch; 85h:24m:18s remains)
INFO - root - 2017-12-05 13:00:00.891446: step 12900, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 84h:17m:13s remains)
2017-12-05 13:00:01.632532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2652497 -4.2789154 -4.2925553 -4.2982645 -4.2970309 -4.2972393 -4.3008261 -4.3094397 -4.31665 -4.3194132 -4.314281 -4.2970796 -4.2682881 -4.2413807 -4.2322574][-4.2409511 -4.2620587 -4.2847428 -4.2931423 -4.2882962 -4.28353 -4.2825379 -4.2869773 -4.2974 -4.3051934 -4.3014297 -4.27721 -4.2353988 -4.1985 -4.1896958][-4.2114 -4.237144 -4.268065 -4.2821126 -4.2751679 -4.2641325 -4.2536793 -4.2462521 -4.2541928 -4.2704387 -4.2754307 -4.2540188 -4.2077107 -4.1675706 -4.1593022][-4.1847253 -4.2105174 -4.2452774 -4.2613893 -4.2493954 -4.226264 -4.2004671 -4.178195 -4.1895671 -4.2233043 -4.2441945 -4.232892 -4.1930137 -4.1599536 -4.1535215][-4.1674495 -4.1905837 -4.2229095 -4.234993 -4.2147727 -4.1757712 -4.1254597 -4.0824008 -4.1046906 -4.1669211 -4.2102475 -4.2132888 -4.1851826 -4.1619887 -4.1567459][-4.15952 -4.1812606 -4.2093277 -4.2154579 -4.1869164 -4.1266947 -4.0420341 -3.9694452 -4.0063953 -4.1015024 -4.1713705 -4.1938229 -4.1803045 -4.1667581 -4.1612697][-4.1605692 -4.1827064 -4.2120509 -4.2183924 -4.1853976 -4.1097088 -3.9960921 -3.8922238 -3.9381819 -4.0541229 -4.1400814 -4.1810503 -4.1813273 -4.1724429 -4.1642666][-4.1636295 -4.1877375 -4.2248764 -4.2385941 -4.208601 -4.1334372 -4.0169535 -3.9103284 -3.9487002 -4.0554423 -4.1367674 -4.1822052 -4.1903796 -4.1872087 -4.1790385][-4.165493 -4.1890478 -4.2278514 -4.2514162 -4.2346473 -4.1764722 -4.0858607 -4.00697 -4.0305486 -4.10535 -4.1653256 -4.2003326 -4.2103119 -4.2092428 -4.2001848][-4.1723504 -4.1906691 -4.2212439 -4.2446795 -4.2414513 -4.2079382 -4.153976 -4.1067319 -4.1179819 -4.1613989 -4.1967754 -4.2209153 -4.2316141 -4.2314234 -4.2224388][-4.18366 -4.1941218 -4.2076778 -4.2182736 -4.2216425 -4.2093768 -4.1865888 -4.1627502 -4.16774 -4.1909165 -4.2078013 -4.2183781 -4.2243061 -4.223814 -4.2189436][-4.1920371 -4.1947722 -4.195117 -4.195344 -4.2004371 -4.2012081 -4.1994219 -4.1920934 -4.1917939 -4.1984081 -4.2008677 -4.2030449 -4.2075524 -4.210814 -4.2108431][-4.1920934 -4.1898484 -4.1919894 -4.1970391 -4.2044115 -4.2105565 -4.2169514 -4.2175355 -4.2142649 -4.2061167 -4.1961966 -4.1950197 -4.2000589 -4.20587 -4.2091012][-4.1808605 -4.1735554 -4.1872149 -4.2086239 -4.2215071 -4.2269168 -4.2321014 -4.236146 -4.2324343 -4.2165427 -4.2011695 -4.198123 -4.2065606 -4.2164469 -4.2226343][-4.1681 -4.1519194 -4.1684742 -4.2025852 -4.2248755 -4.2309523 -4.2312522 -4.2371716 -4.2364211 -4.2227211 -4.2086673 -4.2066379 -4.2167263 -4.2277555 -4.2353582]]...]
INFO - root - 2017-12-05 13:00:11.036817: step 12910, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 85h:41m:04s remains)
INFO - root - 2017-12-05 13:00:20.512104: step 12920, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 85h:14m:21s remains)
INFO - root - 2017-12-05 13:00:29.805760: step 12930, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 77h:32m:34s remains)
INFO - root - 2017-12-05 13:00:39.370220: step 12940, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.982 sec/batch; 87h:09m:01s remains)
INFO - root - 2017-12-05 13:00:48.751397: step 12950, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.994 sec/batch; 88h:13m:23s remains)
INFO - root - 2017-12-05 13:00:58.337052: step 12960, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.998 sec/batch; 88h:32m:36s remains)
INFO - root - 2017-12-05 13:01:07.813535: step 12970, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 84h:28m:41s remains)
INFO - root - 2017-12-05 13:01:17.133475: step 12980, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 81h:45m:22s remains)
INFO - root - 2017-12-05 13:01:26.038668: step 12990, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 84h:50m:20s remains)
INFO - root - 2017-12-05 13:01:35.297446: step 13000, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 86h:16m:00s remains)
2017-12-05 13:01:35.996195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3130946 -4.3124676 -4.3077221 -4.3038979 -4.3015914 -4.2999153 -4.2975216 -4.2959819 -4.2964072 -4.2970724 -4.2973318 -4.2978573 -4.2998385 -4.3031416 -4.3057017][-4.3093209 -4.3078928 -4.30283 -4.2989078 -4.2962565 -4.29405 -4.29073 -4.2881641 -4.287396 -4.2871237 -4.2869215 -4.2890286 -4.2939849 -4.3004704 -4.3054571][-4.3082948 -4.3057442 -4.299655 -4.29323 -4.2864223 -4.27932 -4.2710314 -4.2650785 -4.2630973 -4.2633233 -4.26512 -4.2707429 -4.280767 -4.2933187 -4.3033552][-4.3057919 -4.3003745 -4.2907658 -4.2770596 -4.258996 -4.23971 -4.2212911 -4.2094259 -4.2072892 -4.2121391 -4.2217288 -4.2357178 -4.255034 -4.2777562 -4.2962132][-4.3014288 -4.2903919 -4.2731009 -4.2460012 -4.2082186 -4.1673241 -4.1323533 -4.1134863 -4.1133318 -4.1286106 -4.1540837 -4.183876 -4.2179518 -4.25454 -4.2835259][-4.2896662 -4.2747951 -4.2489181 -4.2044826 -4.1410446 -4.0726037 -4.0186596 -3.9971929 -4.004396 -4.0361891 -4.0847855 -4.1355724 -4.1864038 -4.2353959 -4.2726245][-4.2604413 -4.2489276 -4.2243433 -4.1737657 -4.0965409 -4.009429 -3.9423668 -3.9237971 -3.9414282 -3.9897447 -4.06058 -4.128262 -4.1869044 -4.2372694 -4.2720642][-4.2229381 -4.22276 -4.2114596 -4.1756868 -4.1148829 -4.03989 -3.9802465 -3.96696 -3.9871118 -4.0359983 -4.1076617 -4.1728015 -4.2232027 -4.2607903 -4.2820387][-4.1734724 -4.1903872 -4.2027159 -4.1991143 -4.1748362 -4.1326213 -4.0936308 -4.0840855 -4.0989571 -4.134902 -4.1875205 -4.2340064 -4.2666464 -4.2868867 -4.294034][-4.1266465 -4.1564341 -4.190527 -4.21832 -4.2312589 -4.221921 -4.2035103 -4.1955643 -4.2023392 -4.2229681 -4.254035 -4.2813697 -4.29829 -4.3044724 -4.3022413][-4.1024609 -4.1357112 -4.1784811 -4.2231979 -4.2611375 -4.277956 -4.2772379 -4.2721949 -4.2734127 -4.2815847 -4.2964449 -4.3097806 -4.3165588 -4.3141055 -4.3059278][-4.096384 -4.1207366 -4.1593461 -4.2079105 -4.2588949 -4.2964439 -4.312531 -4.3136668 -4.311789 -4.3094168 -4.312808 -4.3166914 -4.317791 -4.31202 -4.3011637][-4.0966249 -4.1088943 -4.1381116 -4.1831021 -4.2375689 -4.2876968 -4.3148966 -4.3192863 -4.3121977 -4.2992578 -4.2937517 -4.2933307 -4.2939444 -4.2918625 -4.283597][-4.1016369 -4.107 -4.1298308 -4.16878 -4.2203083 -4.270925 -4.2970724 -4.2934203 -4.2730536 -4.2477503 -4.2360787 -4.2368803 -4.2446594 -4.2542281 -4.2526255][-4.1218162 -4.1239614 -4.1431775 -4.1745915 -4.2193689 -4.2602019 -4.2717376 -4.2489457 -4.207324 -4.1666865 -4.1502647 -4.1558094 -4.175437 -4.1991506 -4.203464]]...]
INFO - root - 2017-12-05 13:01:45.528006: step 13010, loss = 2.04, batch loss = 1.99 (8.0 examples/sec; 1.000 sec/batch; 88h:42m:16s remains)
INFO - root - 2017-12-05 13:01:54.822573: step 13020, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 85h:12m:05s remains)
INFO - root - 2017-12-05 13:02:03.963178: step 13030, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.882 sec/batch; 78h:14m:44s remains)
INFO - root - 2017-12-05 13:02:13.556947: step 13040, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 87h:14m:39s remains)
INFO - root - 2017-12-05 13:02:22.927899: step 13050, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 83h:06m:26s remains)
INFO - root - 2017-12-05 13:02:32.148498: step 13060, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 81h:56m:40s remains)
INFO - root - 2017-12-05 13:02:41.550381: step 13070, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 81h:17m:31s remains)
INFO - root - 2017-12-05 13:02:50.591872: step 13080, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 81h:52m:18s remains)
INFO - root - 2017-12-05 13:02:59.689439: step 13090, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.967 sec/batch; 85h:46m:25s remains)
INFO - root - 2017-12-05 13:03:08.951296: step 13100, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 85h:36m:51s remains)
2017-12-05 13:03:09.701152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3431864 -4.3322115 -4.3193316 -4.308454 -4.3035626 -4.30144 -4.2950726 -4.2908235 -4.2878513 -4.2829905 -4.2768269 -4.2763996 -4.2788739 -4.2820415 -4.2853522][-4.3283172 -4.3111119 -4.292335 -4.2741394 -4.2641048 -4.258327 -4.2467828 -4.2404509 -4.2414937 -4.2414737 -4.2353 -4.234879 -4.2409563 -4.2471452 -4.2486668][-4.3059592 -4.2786665 -4.2505889 -4.2200356 -4.1991763 -4.1827703 -4.1627536 -4.1529694 -4.162734 -4.1729822 -4.1702957 -4.1726952 -4.1860533 -4.1979914 -4.20043][-4.2762074 -4.2380195 -4.1973944 -4.1534772 -4.1151733 -4.0836897 -4.0564861 -4.0486388 -4.062561 -4.0769024 -4.0777874 -4.0823956 -4.1027231 -4.1193271 -4.1288848][-4.2457604 -4.1988282 -4.1451173 -4.0856004 -4.0302839 -3.9855158 -3.9513056 -3.9481304 -3.9639351 -3.98035 -3.9810252 -3.9901562 -4.018229 -4.0436044 -4.064764][-4.2152214 -4.15864 -4.0914226 -4.0133748 -3.946533 -3.8924954 -3.845448 -3.8373938 -3.8572016 -3.8817565 -3.8906014 -3.9160128 -3.962395 -3.997556 -4.022676][-4.192831 -4.1251016 -4.0409861 -3.9495497 -3.8741336 -3.8076296 -3.7411392 -3.7208772 -3.758168 -3.8080587 -3.8375342 -3.88304 -3.9458103 -3.9884846 -4.016757][-4.1803985 -4.1039844 -4.0070653 -3.9159303 -3.842402 -3.7778029 -3.7145271 -3.7027845 -3.7646224 -3.8391716 -3.8856027 -3.9307244 -3.9887223 -4.0295982 -4.0493331][-4.1777353 -4.1038842 -4.0144162 -3.9418869 -3.890811 -3.8550606 -3.823431 -3.8258734 -3.8818021 -3.9464774 -3.9876344 -4.0179586 -4.066011 -4.1045117 -4.1202416][-4.183074 -4.1236625 -4.057363 -4.0091386 -3.9790189 -3.9630866 -3.9498854 -3.9526136 -3.9864962 -4.025753 -4.0534897 -4.0751276 -4.1248941 -4.1704597 -4.1929541][-4.2016997 -4.161603 -4.12015 -4.094389 -4.0795288 -4.0680623 -4.0601654 -4.0566258 -4.0740733 -4.0972123 -4.1161404 -4.13455 -4.1808896 -4.2252517 -4.2483888][-4.232523 -4.2095423 -4.1856818 -4.1691704 -4.1559091 -4.1445818 -4.1396332 -4.1365004 -4.1482587 -4.167541 -4.1835742 -4.2017307 -4.2342463 -4.2643552 -4.2794905][-4.2567782 -4.2399325 -4.2210751 -4.2069316 -4.1935978 -4.19039 -4.1962972 -4.2070355 -4.2256708 -4.2416549 -4.2513623 -4.2619705 -4.2775536 -4.2904553 -4.2965536][-4.2742739 -4.2560821 -4.2392192 -4.2328234 -4.229063 -4.2355871 -4.2487779 -4.2661324 -4.2834749 -4.294065 -4.2961755 -4.2966948 -4.3007174 -4.3057375 -4.3100729][-4.2947969 -4.2766213 -4.2618566 -4.2580504 -4.2587013 -4.2673225 -4.2800827 -4.2947917 -4.308167 -4.3164091 -4.3171997 -4.3155613 -4.3158402 -4.3199673 -4.3249784]]...]
INFO - root - 2017-12-05 13:03:19.024936: step 13110, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 85h:59m:06s remains)
INFO - root - 2017-12-05 13:03:28.208601: step 13120, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 82h:15m:19s remains)
INFO - root - 2017-12-05 13:03:37.874998: step 13130, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 86h:12m:58s remains)
INFO - root - 2017-12-05 13:03:47.220741: step 13140, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.946 sec/batch; 83h:55m:21s remains)
INFO - root - 2017-12-05 13:03:56.469260: step 13150, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 83h:04m:13s remains)
INFO - root - 2017-12-05 13:04:05.890969: step 13160, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 86h:46m:40s remains)
INFO - root - 2017-12-05 13:04:15.098648: step 13170, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.951 sec/batch; 84h:23m:28s remains)
INFO - root - 2017-12-05 13:04:24.404116: step 13180, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 83h:22m:54s remains)
INFO - root - 2017-12-05 13:04:33.800146: step 13190, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.928 sec/batch; 82h:19m:18s remains)
INFO - root - 2017-12-05 13:04:43.213562: step 13200, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 86h:14m:33s remains)
2017-12-05 13:04:43.967153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2957778 -4.2937946 -4.2839942 -4.2702851 -4.2587228 -4.2514377 -4.24941 -4.2580123 -4.2713356 -4.2810473 -4.2856832 -4.2871161 -4.2794123 -4.26776 -4.2570753][-4.2842712 -4.2835712 -4.2767143 -4.2644415 -4.25103 -4.23829 -4.231317 -4.2397723 -4.2571983 -4.2710834 -4.2815661 -4.2877331 -4.2814126 -4.2688341 -4.2548447][-4.273839 -4.2749414 -4.2719717 -4.2580218 -4.2343073 -4.2080145 -4.1893439 -4.1960821 -4.2215815 -4.2444582 -4.2658019 -4.2813668 -4.2820749 -4.2734723 -4.2584538][-4.2677794 -4.2706175 -4.2715397 -4.2528839 -4.2158113 -4.1730642 -4.1397734 -4.1451163 -4.1802459 -4.2130923 -4.2418694 -4.2637482 -4.2723851 -4.2704325 -4.25836][-4.262332 -4.2654033 -4.2691255 -4.244874 -4.1946888 -4.1310644 -4.0768747 -4.0805383 -4.1288595 -4.17348 -4.2087631 -4.2371426 -4.25564 -4.2597647 -4.25496][-4.2576556 -4.2567377 -4.2593102 -4.2251825 -4.1614156 -4.06784 -3.9838428 -3.9912643 -4.0623145 -4.1297178 -4.1798782 -4.2195168 -4.2486587 -4.2591219 -4.263566][-4.2594261 -4.2524176 -4.2464905 -4.1973791 -4.1112804 -3.9752383 -3.8505802 -3.8787096 -3.9880745 -4.0874457 -4.1573339 -4.2114425 -4.2498684 -4.2673135 -4.2808943][-4.2666912 -4.2523632 -4.2372727 -4.1756406 -4.067163 -3.8914278 -3.72979 -3.7880592 -3.9375691 -4.0623693 -4.1492591 -4.2110457 -4.253232 -4.278131 -4.296937][-4.2778883 -4.2580705 -4.241035 -4.18231 -4.0809188 -3.9156036 -3.768373 -3.8408384 -3.9851727 -4.0927563 -4.17243 -4.2268739 -4.2663641 -4.2945566 -4.3122339][-4.2857528 -4.2649155 -4.2515316 -4.2044668 -4.12625 -4.0025206 -3.8996632 -3.9618485 -4.0751133 -4.1518717 -4.2100854 -4.2514458 -4.2854137 -4.3125491 -4.3255782][-4.2882123 -4.2692614 -4.2596736 -4.22336 -4.1630268 -4.0786223 -4.0190954 -4.0676942 -4.1504807 -4.2026191 -4.2432017 -4.2725921 -4.301919 -4.3246961 -4.3307028][-4.2979 -4.2812419 -4.2727532 -4.244319 -4.203063 -4.1544781 -4.1297259 -4.1672626 -4.2199807 -4.25284 -4.2762194 -4.29169 -4.311523 -4.327981 -4.3294325][-4.3059154 -4.2929549 -4.2886033 -4.2693729 -4.2417355 -4.218811 -4.2195282 -4.24942 -4.2799482 -4.2977824 -4.3062906 -4.3096991 -4.3194957 -4.3291874 -4.3289547][-4.3145742 -4.3027248 -4.3045673 -4.2994232 -4.2846575 -4.277349 -4.29207 -4.3158331 -4.3311133 -4.3350816 -4.3310561 -4.32573 -4.329514 -4.3329172 -4.3307586][-4.3234129 -4.3117437 -4.3144832 -4.3176827 -4.3127193 -4.3142352 -4.3333359 -4.35141 -4.3596711 -4.3576384 -4.3469648 -4.33647 -4.336647 -4.3356891 -4.3300982]]...]
INFO - root - 2017-12-05 13:04:53.350393: step 13210, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 81h:35m:51s remains)
INFO - root - 2017-12-05 13:05:02.559848: step 13220, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.904 sec/batch; 80h:10m:59s remains)
INFO - root - 2017-12-05 13:05:11.916993: step 13230, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.914 sec/batch; 81h:04m:14s remains)
INFO - root - 2017-12-05 13:05:21.529557: step 13240, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.958 sec/batch; 84h:59m:44s remains)
INFO - root - 2017-12-05 13:05:30.913975: step 13250, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 83h:24m:05s remains)
INFO - root - 2017-12-05 13:05:40.191730: step 13260, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 83h:38m:55s remains)
INFO - root - 2017-12-05 13:05:49.756002: step 13270, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 82h:06m:01s remains)
INFO - root - 2017-12-05 13:05:58.888875: step 13280, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 79h:47m:10s remains)
INFO - root - 2017-12-05 13:06:08.385323: step 13290, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 85h:06m:24s remains)
INFO - root - 2017-12-05 13:06:17.673995: step 13300, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 74h:40m:33s remains)
2017-12-05 13:06:18.424870: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.216938 -4.2134371 -4.214314 -4.2061973 -4.1951122 -4.1753259 -4.1330853 -4.060288 -3.9897709 -3.9587877 -3.9831507 -4.0302687 -4.0655541 -4.0645027 -4.0231953][-4.2511458 -4.2499094 -4.2479858 -4.2363791 -4.2198763 -4.2007847 -4.1683121 -4.108315 -4.0525479 -4.0287523 -4.0392604 -4.0637507 -4.081634 -4.0778928 -4.0371][-4.2596197 -4.2583103 -4.2512422 -4.2365732 -4.2157354 -4.1989021 -4.1793556 -4.1391845 -4.1032352 -4.0888233 -4.0854011 -4.0874629 -4.0893497 -4.086493 -4.0559039][-4.24055 -4.2399521 -4.228919 -4.2084551 -4.182703 -4.1679492 -4.1573443 -4.1275663 -4.106874 -4.0988507 -4.0875449 -4.0810823 -4.081234 -4.0825553 -4.0671563][-4.2251 -4.2249303 -4.2156067 -4.1882458 -4.151844 -4.1314511 -4.1170545 -4.0889153 -4.0725284 -4.065814 -4.0569706 -4.05964 -4.0724936 -4.0825367 -4.0765491][-4.2211514 -4.2196937 -4.2084155 -4.1729007 -4.1280932 -4.1007166 -4.0785007 -4.0483842 -4.0294127 -4.0177975 -4.0246658 -4.0516429 -4.0820355 -4.10024 -4.0982666][-4.2014947 -4.1962142 -4.1804948 -4.13448 -4.0848093 -4.0614905 -4.0444217 -4.0174303 -3.9955716 -3.9788234 -3.9998562 -4.0433173 -4.0807967 -4.1029048 -4.1057005][-4.1688957 -4.1539469 -4.1316528 -4.081449 -4.0364141 -4.0269 -4.0269561 -4.0087976 -3.9896646 -3.973804 -3.9956462 -4.0274706 -4.0549731 -4.07612 -4.089478][-4.1456332 -4.1256742 -4.1001191 -4.0540051 -4.0224452 -4.026298 -4.0348482 -4.0283017 -4.0165038 -4.011044 -4.0215874 -4.0273037 -4.0346684 -4.0504169 -4.0707312][-4.152503 -4.134964 -4.1073546 -4.0650411 -4.0444655 -4.0586734 -4.0709448 -4.072413 -4.0720396 -4.0770478 -4.0761037 -4.0559311 -4.03796 -4.0373068 -4.0537105][-4.1612296 -4.1450338 -4.1150403 -4.0810709 -4.0732064 -4.0876789 -4.0941215 -4.0967255 -4.1069036 -4.1219587 -4.1172419 -4.0823278 -4.0505214 -4.0391288 -4.0507078][-4.1680136 -4.1519828 -4.1217408 -4.0921988 -4.0855489 -4.0887733 -4.0872059 -4.0942388 -4.1160088 -4.1369939 -4.1320624 -4.0991387 -4.0680776 -4.0521212 -4.0589228][-4.1848869 -4.1660795 -4.1332936 -4.1032634 -4.0847454 -4.0750074 -4.0703435 -4.0814943 -4.1142058 -4.1393466 -4.13638 -4.11287 -4.0916376 -4.0801668 -4.0839691][-4.1910505 -4.1730309 -4.1439042 -4.1146197 -4.0865693 -4.0681024 -4.0614367 -4.0755997 -4.1111712 -4.1349854 -4.1360016 -4.1226873 -4.1104016 -4.1044779 -4.1085262][-4.1816883 -4.1655273 -4.143116 -4.1187663 -4.0896244 -4.067462 -4.0620785 -4.0788388 -4.1120076 -4.1345234 -4.1412926 -4.1374946 -4.1322756 -4.131496 -4.1365895]]...]
INFO - root - 2017-12-05 13:06:27.785980: step 13310, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 82h:01m:33s remains)
INFO - root - 2017-12-05 13:06:37.162873: step 13320, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 83h:53m:59s remains)
INFO - root - 2017-12-05 13:06:46.418330: step 13330, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 81h:18m:31s remains)
INFO - root - 2017-12-05 13:06:55.939767: step 13340, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 77h:29m:34s remains)
INFO - root - 2017-12-05 13:07:05.073671: step 13350, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 81h:29m:13s remains)
INFO - root - 2017-12-05 13:07:14.451398: step 13360, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 76h:17m:10s remains)
INFO - root - 2017-12-05 13:07:23.832548: step 13370, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.766 sec/batch; 67h:55m:57s remains)
INFO - root - 2017-12-05 13:07:33.140583: step 13380, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 81h:16m:08s remains)
INFO - root - 2017-12-05 13:07:42.422159: step 13390, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 84h:18m:43s remains)
INFO - root - 2017-12-05 13:07:51.626126: step 13400, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 84h:28m:33s remains)
2017-12-05 13:07:52.441059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3578968 -4.3658857 -4.3589368 -4.3358812 -4.2998476 -4.2561269 -4.2033997 -4.16477 -4.1314235 -4.11365 -4.1225548 -4.1494532 -4.1860819 -4.233304 -4.2883692][-4.3602281 -4.3658576 -4.3527212 -4.3235373 -4.2789412 -4.2271123 -4.1741219 -4.1396132 -4.1153779 -4.1164789 -4.1480551 -4.1815362 -4.2173452 -4.263844 -4.3134971][-4.3635731 -4.3656669 -4.3474431 -4.31171 -4.25766 -4.1984 -4.1496682 -4.1233058 -4.1115227 -4.1251035 -4.1668739 -4.2030449 -4.24212 -4.291481 -4.33585][-4.3651123 -4.3639703 -4.3408437 -4.2985992 -4.2329597 -4.1664205 -4.1202741 -4.099515 -4.100759 -4.1291895 -4.1745963 -4.2113996 -4.2570176 -4.3087664 -4.3462539][-4.3643808 -4.3604012 -4.333045 -4.2825751 -4.2019706 -4.1221561 -4.0677667 -4.0416026 -4.0513887 -4.1017385 -4.1577 -4.198246 -4.2553711 -4.3121495 -4.3434563][-4.3615251 -4.3540969 -4.3208971 -4.2573538 -4.1582103 -4.0550365 -3.9783764 -3.9432552 -3.9741662 -4.0609717 -4.1364379 -4.1863432 -4.2521448 -4.309093 -4.3382339][-4.3590317 -4.3477721 -4.3088098 -4.2328653 -4.1170263 -3.9893951 -3.8893952 -3.8501563 -3.9123104 -4.038013 -4.1317296 -4.1918879 -4.2529783 -4.3040571 -4.3330455][-4.360095 -4.3477793 -4.3074989 -4.2248349 -4.1034365 -3.9674594 -3.8577812 -3.8228874 -3.9050853 -4.0457568 -4.14505 -4.2068825 -4.2621288 -4.3070879 -4.3343124][-4.3625278 -4.3518314 -4.3145123 -4.234992 -4.1175981 -3.9837518 -3.8780956 -3.8555133 -3.9431047 -4.074996 -4.1685472 -4.2280121 -4.2779956 -4.3170614 -4.3407488][-4.3636851 -4.3560929 -4.32567 -4.2579632 -4.1534696 -4.0315609 -3.93993 -3.9303894 -4.0086694 -4.1164131 -4.1988578 -4.2573423 -4.3029819 -4.3338642 -4.351419][-4.3650079 -4.3617845 -4.3398695 -4.288867 -4.2039857 -4.10096 -4.0297356 -4.0293221 -4.0883365 -4.1667571 -4.2375784 -4.2942834 -4.3345914 -4.3574266 -4.3677325][-4.3654661 -4.364099 -4.3484893 -4.3109632 -4.2455955 -4.1654782 -4.1156082 -4.1228538 -4.1704373 -4.229537 -4.2856283 -4.3301406 -4.3613143 -4.3762121 -4.3800454][-4.3622828 -4.3604245 -4.3471036 -4.3178663 -4.2688155 -4.2119241 -4.1818171 -4.1976633 -4.2425241 -4.2897725 -4.3287759 -4.3578343 -4.3770223 -4.38468 -4.3837471][-4.3585911 -4.3559542 -4.345367 -4.32374 -4.2880182 -4.2494879 -4.2351723 -4.25575 -4.2957578 -4.3312707 -4.3560925 -4.3727579 -4.3824024 -4.3837609 -4.3795][-4.3577647 -4.3564653 -4.3501182 -4.3369784 -4.3135262 -4.28779 -4.2786098 -4.2960963 -4.3264608 -4.3524618 -4.3685923 -4.3787351 -4.3825469 -4.3795972 -4.3738647]]...]
INFO - root - 2017-12-05 13:08:01.821600: step 13410, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 80h:32m:13s remains)
INFO - root - 2017-12-05 13:08:11.456253: step 13420, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 82h:53m:56s remains)
INFO - root - 2017-12-05 13:08:20.514943: step 13430, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 82h:25m:23s remains)
INFO - root - 2017-12-05 13:08:29.751785: step 13440, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 78h:49m:34s remains)
INFO - root - 2017-12-05 13:08:39.198088: step 13450, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 85h:07m:14s remains)
INFO - root - 2017-12-05 13:08:48.581855: step 13460, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 79h:33m:38s remains)
INFO - root - 2017-12-05 13:08:57.861230: step 13470, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 85h:22m:00s remains)
INFO - root - 2017-12-05 13:09:07.392377: step 13480, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 86h:24m:29s remains)
INFO - root - 2017-12-05 13:09:16.786100: step 13490, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 84h:25m:10s remains)
INFO - root - 2017-12-05 13:09:26.213067: step 13500, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.938 sec/batch; 83h:04m:53s remains)
2017-12-05 13:09:26.992924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2904944 -4.2939587 -4.29815 -4.2985263 -4.2970853 -4.2964163 -4.2929311 -4.2796054 -4.2546487 -4.2303166 -4.2140765 -4.2039409 -4.197823 -4.2020464 -4.2115269][-4.2926226 -4.2942629 -4.2984357 -4.2961807 -4.2905412 -4.2866669 -4.2793455 -4.2613506 -4.2306089 -4.2059431 -4.1961455 -4.1945043 -4.1947727 -4.2070112 -4.2211428][-4.2945089 -4.2932825 -4.2934575 -4.2855291 -4.2737856 -4.2632222 -4.2502747 -4.2297397 -4.19948 -4.1793537 -4.1800785 -4.1907077 -4.2007246 -4.2194424 -4.2354789][-4.2991095 -4.2968965 -4.2905602 -4.27422 -4.2521634 -4.2319131 -4.2117805 -4.1890912 -4.1613922 -4.1478906 -4.1607571 -4.1850052 -4.2075224 -4.2317362 -4.248374][-4.2966785 -4.2915158 -4.2760811 -4.2492628 -4.2168326 -4.1856604 -4.1558595 -4.1254654 -4.0995355 -4.0982351 -4.1255293 -4.1631474 -4.197957 -4.2325892 -4.2549229][-4.2822256 -4.2725444 -4.2482352 -4.2133503 -4.1741629 -4.1318097 -4.0878968 -4.0422029 -4.0168476 -4.0353942 -4.084115 -4.1364589 -4.1833825 -4.2289476 -4.258316][-4.2650728 -4.2520432 -4.2232008 -4.1854196 -4.1438766 -4.0937881 -4.0343618 -3.9727759 -3.9470339 -3.9867399 -4.0555782 -4.1215262 -4.1788731 -4.2310328 -4.2636619][-4.2551775 -4.2410612 -4.2125773 -4.1741939 -4.1337633 -4.0858779 -4.0266323 -3.9646912 -3.9396157 -3.9884677 -4.0643921 -4.133007 -4.1899877 -4.2397237 -4.26945][-4.2433596 -4.22917 -4.202878 -4.1655951 -4.1297126 -4.0948639 -4.0546174 -4.0135279 -3.9969759 -4.0373287 -4.1014919 -4.1600294 -4.2089705 -4.2507138 -4.2746787][-4.233974 -4.2215905 -4.1981516 -4.162961 -4.1323891 -4.111506 -4.0925169 -4.074779 -4.0684614 -4.0989761 -4.1474857 -4.1927943 -4.2309036 -4.2647619 -4.2834845][-4.2375236 -4.2296839 -4.2100086 -4.1764741 -4.1497626 -4.1403537 -4.1337504 -4.1264529 -4.1261168 -4.1511469 -4.1877222 -4.22147 -4.24841 -4.2746692 -4.288619][-4.2380762 -4.2316356 -4.2159524 -4.1877646 -4.1671023 -4.1651 -4.162817 -4.1574979 -4.1589723 -4.1816568 -4.210824 -4.2349925 -4.2522616 -4.2716646 -4.2818532][-4.2251749 -4.217423 -4.2046905 -4.18338 -4.1686068 -4.1694746 -4.1678853 -4.1647782 -4.1676469 -4.1877623 -4.2122159 -4.2315273 -4.2430024 -4.2571754 -4.265666][-4.2079806 -4.1997185 -4.1871409 -4.1698947 -4.1603303 -4.1621547 -4.162127 -4.16212 -4.1668692 -4.1834435 -4.2033939 -4.2191238 -4.2279916 -4.2394028 -4.2486634][-4.1995316 -4.191339 -4.1793141 -4.1658173 -4.1586604 -4.1601772 -4.1634636 -4.1672721 -4.1725907 -4.1847992 -4.1994467 -4.2115684 -4.2191157 -4.2293754 -4.2408261]]...]
INFO - root - 2017-12-05 13:09:36.513826: step 13510, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 87h:23m:26s remains)
INFO - root - 2017-12-05 13:09:45.681881: step 13520, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 81h:49m:11s remains)
INFO - root - 2017-12-05 13:09:55.187972: step 13530, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 78h:41m:01s remains)
INFO - root - 2017-12-05 13:10:04.279494: step 13540, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 87h:59m:33s remains)
INFO - root - 2017-12-05 13:10:13.499303: step 13550, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 86h:46m:57s remains)
INFO - root - 2017-12-05 13:10:23.069924: step 13560, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.984 sec/batch; 87h:11m:49s remains)
INFO - root - 2017-12-05 13:10:32.315969: step 13570, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.951 sec/batch; 84h:15m:02s remains)
INFO - root - 2017-12-05 13:10:41.741857: step 13580, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.936 sec/batch; 82h:53m:12s remains)
INFO - root - 2017-12-05 13:10:51.140594: step 13590, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 81h:17m:01s remains)
INFO - root - 2017-12-05 13:11:00.423857: step 13600, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 83h:54m:01s remains)
2017-12-05 13:11:01.159740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2276282 -4.219522 -4.1878924 -4.1508722 -4.1255469 -4.1039519 -4.1071091 -4.1045122 -4.052896 -4.0146055 -4.0310802 -4.0625482 -4.0831213 -4.0990715 -4.101871][-4.233954 -4.222734 -4.1881905 -4.1436148 -4.1140351 -4.090899 -4.0886073 -4.0913343 -4.0638471 -4.0435853 -4.0594511 -4.0710173 -4.0645089 -4.0636177 -4.071579][-4.230052 -4.2153335 -4.181149 -4.1277804 -4.0877614 -4.0589218 -4.0564141 -4.0673661 -4.0711241 -4.0734463 -4.0883074 -4.0815077 -4.0479431 -4.0328007 -4.0488768][-4.2196 -4.2024827 -4.1665654 -4.1064224 -4.0545812 -4.0192862 -4.0193467 -4.0471387 -4.0789838 -4.1013637 -4.1159124 -4.0976367 -4.0527725 -4.0329947 -4.052876][-4.2166996 -4.1973133 -4.1608648 -4.0998435 -4.0362992 -3.9896538 -3.9871476 -4.031333 -4.0854969 -4.1200523 -4.1363368 -4.1177754 -4.0709925 -4.0454059 -4.0623703][-4.218739 -4.1974125 -4.1620216 -4.1016903 -4.0247793 -3.9536755 -3.9364593 -3.9998431 -4.0780926 -4.1258216 -4.147748 -4.1396337 -4.0955429 -4.0621943 -4.0721235][-4.213944 -4.19347 -4.1594229 -4.0963216 -4.0009665 -3.8951948 -3.8486795 -3.9360793 -4.0550132 -4.1248083 -4.155993 -4.1564789 -4.1243677 -4.0909595 -4.0961022][-4.2080832 -4.1920433 -4.1552181 -4.0843105 -3.9748793 -3.8356862 -3.7578764 -3.8785095 -4.0373917 -4.1228018 -4.1573625 -4.1648879 -4.1474519 -4.1249151 -4.1301103][-4.2059836 -4.1896439 -4.1472621 -4.0757 -3.9731438 -3.8467646 -3.7851782 -3.9014161 -4.0443811 -4.1178441 -4.152113 -4.1712666 -4.1766357 -4.1674137 -4.1664858][-4.1962361 -4.172286 -4.1301241 -4.0710425 -4.0006547 -3.9319127 -3.9167256 -3.9912145 -4.0727558 -4.1165075 -4.1469169 -4.1785278 -4.2065349 -4.2093878 -4.2012191][-4.1773663 -4.1436615 -4.1066666 -4.0668173 -4.0304537 -4.0089622 -4.0182772 -4.0604596 -4.0969443 -4.1232138 -4.1451492 -4.1816797 -4.2177091 -4.2293577 -4.215723][-4.1602073 -4.123745 -4.0963354 -4.0749855 -4.059454 -4.0588269 -4.0727916 -4.098093 -4.1166563 -4.1363668 -4.1545062 -4.1844182 -4.2125 -4.2225347 -4.2036014][-4.1513848 -4.1205778 -4.1003146 -4.0921645 -4.086956 -4.094296 -4.1082916 -4.1211147 -4.1303406 -4.146471 -4.1615129 -4.1791863 -4.195415 -4.2012386 -4.1787615][-4.1377521 -4.1179833 -4.1035967 -4.1048408 -4.1088891 -4.1182575 -4.1323762 -4.1368084 -4.1412692 -4.1541252 -4.16545 -4.1729012 -4.1767178 -4.1778588 -4.1555295][-4.1228848 -4.113512 -4.1066375 -4.110846 -4.118938 -4.1277061 -4.1434979 -4.1483784 -4.1539164 -4.16461 -4.1721473 -4.1697946 -4.1637311 -4.1581984 -4.137887]]...]
INFO - root - 2017-12-05 13:11:10.384006: step 13610, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 84h:26m:17s remains)
INFO - root - 2017-12-05 13:11:19.716951: step 13620, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 83h:55m:34s remains)
INFO - root - 2017-12-05 13:11:28.952026: step 13630, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 82h:37m:37s remains)
INFO - root - 2017-12-05 13:11:38.458530: step 13640, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 84h:39m:45s remains)
INFO - root - 2017-12-05 13:11:47.951724: step 13650, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 85h:12m:50s remains)
INFO - root - 2017-12-05 13:11:57.480794: step 13660, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 83h:12m:35s remains)
INFO - root - 2017-12-05 13:12:06.667252: step 13670, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 85h:20m:31s remains)
INFO - root - 2017-12-05 13:12:15.899233: step 13680, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 88h:17m:30s remains)
INFO - root - 2017-12-05 13:12:25.130130: step 13690, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 83h:57m:10s remains)
INFO - root - 2017-12-05 13:12:34.582521: step 13700, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 88h:34m:44s remains)
2017-12-05 13:12:35.318872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3595386 -4.3566256 -4.3500295 -4.342155 -4.3341517 -4.3282504 -4.3255157 -4.3246279 -4.3247275 -4.3276229 -4.3352032 -4.3448915 -4.3522296 -4.3564792 -4.3573265][-4.357286 -4.3515582 -4.3410788 -4.3286324 -4.3169637 -4.3072357 -4.2997489 -4.2943869 -4.2911925 -4.2948437 -4.3084378 -4.324821 -4.3376613 -4.3453851 -4.3476896][-4.349884 -4.3397379 -4.3231072 -4.3060751 -4.2933049 -4.2830729 -4.27225 -4.2634377 -4.2587848 -4.2644529 -4.2815013 -4.3014297 -4.3182487 -4.3293943 -4.333364][-4.33378 -4.3137732 -4.2857742 -4.2601757 -4.2432609 -4.2308273 -4.2165141 -4.20794 -4.209806 -4.2267227 -4.25204 -4.2784281 -4.3008819 -4.3168955 -4.3244281][-4.3112903 -4.275876 -4.2307072 -4.1904678 -4.1604214 -4.1349754 -4.10674 -4.0994349 -4.1211538 -4.1631289 -4.2069955 -4.2472172 -4.28005 -4.3032861 -4.3149776][-4.2949176 -4.2464862 -4.1834993 -4.126555 -4.0761104 -4.026721 -3.9734373 -3.9614816 -4.0101466 -4.0851903 -4.1525788 -4.2072759 -4.2509317 -4.28137 -4.2961488][-4.2919779 -4.2392039 -4.1693339 -4.1061988 -4.0434794 -3.9744306 -3.8974693 -3.8733187 -3.9339991 -4.0279884 -4.10911 -4.1690841 -4.2170634 -4.2517357 -4.2702212][-4.2968903 -4.2488384 -4.1858854 -4.1319246 -4.0779285 -4.0165539 -3.9465997 -3.9213274 -3.9695256 -4.0472822 -4.1159654 -4.1641636 -4.2048683 -4.2350717 -4.251986][-4.3050308 -4.2667894 -4.2189436 -4.1813669 -4.14506 -4.1044927 -4.0578737 -4.0448866 -4.0810647 -4.1344047 -4.1801534 -4.2083683 -4.2330985 -4.2507696 -4.259][-4.3127804 -4.2853417 -4.2542195 -4.2323356 -4.2126312 -4.1895795 -4.1622691 -4.160234 -4.1886954 -4.2243638 -4.252111 -4.2658033 -4.2760005 -4.2808881 -4.2808723][-4.314404 -4.2946324 -4.2747374 -4.2629948 -4.2542248 -4.2449074 -4.2338724 -4.2409415 -4.2633915 -4.2864747 -4.2991796 -4.3023686 -4.3038816 -4.3026962 -4.300005][-4.3113222 -4.2957783 -4.2826948 -4.2784691 -4.2796183 -4.279768 -4.2788882 -4.2877927 -4.3024974 -4.3136024 -4.3159189 -4.3120427 -4.3085546 -4.3045912 -4.3032484][-4.3062596 -4.29251 -4.2818818 -4.2809582 -4.2863708 -4.2908516 -4.2964373 -4.3064976 -4.31446 -4.3151274 -4.3098688 -4.3014736 -4.2942567 -4.288918 -4.2913737][-4.3045969 -4.2929745 -4.2842612 -4.2839274 -4.2898602 -4.2951212 -4.3026247 -4.3119559 -4.3147006 -4.3070331 -4.2942023 -4.2819052 -4.2722678 -4.2673569 -4.2743173][-4.3071117 -4.2965717 -4.2895222 -4.2886672 -4.292892 -4.2966437 -4.30402 -4.3126721 -4.3130841 -4.2999845 -4.2791934 -4.2615123 -4.2498288 -4.2460761 -4.2566257]]...]
INFO - root - 2017-12-05 13:12:44.604369: step 13710, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.948 sec/batch; 83h:57m:04s remains)
INFO - root - 2017-12-05 13:12:53.775041: step 13720, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.976 sec/batch; 86h:23m:13s remains)
INFO - root - 2017-12-05 13:13:03.123302: step 13730, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 85h:14m:52s remains)
INFO - root - 2017-12-05 13:13:12.536706: step 13740, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 83h:14m:58s remains)
INFO - root - 2017-12-05 13:13:21.713744: step 13750, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 77h:49m:44s remains)
INFO - root - 2017-12-05 13:13:30.865591: step 13760, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 76h:12m:20s remains)
INFO - root - 2017-12-05 13:13:40.408099: step 13770, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 84h:55m:12s remains)
INFO - root - 2017-12-05 13:13:49.853830: step 13780, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 82h:42m:48s remains)
INFO - root - 2017-12-05 13:13:59.128899: step 13790, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.904 sec/batch; 79h:59m:16s remains)
INFO - root - 2017-12-05 13:14:08.343771: step 13800, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 84h:34m:00s remains)
2017-12-05 13:14:09.076964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2374811 -4.2309766 -4.2398539 -4.2507315 -4.2439833 -4.2302446 -4.22327 -4.2224317 -4.2358975 -4.2608137 -4.2887545 -4.2992191 -4.2972484 -4.2838593 -4.2684326][-4.2117 -4.1921372 -4.2018852 -4.2224054 -4.2240953 -4.208662 -4.1874495 -4.1711459 -4.1795297 -4.21629 -4.2616119 -4.2748384 -4.2720828 -4.2627697 -4.2527938][-4.1861553 -4.1599865 -4.1685 -4.1888142 -4.1963029 -4.1832509 -4.1477904 -4.1083803 -4.0987139 -4.1484418 -4.2184114 -4.243289 -4.2446623 -4.2438984 -4.24171][-4.1661634 -4.13923 -4.1455626 -4.1556158 -4.1599922 -4.1458721 -4.0963774 -4.0252128 -3.9945681 -4.0610228 -4.1636343 -4.204071 -4.214325 -4.2232223 -4.228653][-4.1528063 -4.1236157 -4.126719 -4.1292582 -4.1290855 -4.1151462 -4.0531254 -3.9322548 -3.867291 -3.9587395 -4.0994329 -4.1629004 -4.1871438 -4.2060413 -4.2164989][-4.1571555 -4.1284776 -4.1220984 -4.1130123 -4.1099467 -4.0982265 -4.0211482 -3.8524585 -3.7447596 -3.8609552 -4.0425868 -4.1345739 -4.1791768 -4.2047057 -4.20951][-4.1885643 -4.167531 -4.1477952 -4.1160336 -4.099915 -4.090755 -4.0096788 -3.826776 -3.697535 -3.8129747 -4.0119076 -4.1249824 -4.1824484 -4.206924 -4.2044635][-4.2247844 -4.2125597 -4.1842537 -4.1322179 -4.1002975 -4.0983906 -4.03654 -3.8877163 -3.7858796 -3.8707685 -4.0298343 -4.1306028 -4.18605 -4.2006273 -4.1948385][-4.2484283 -4.2436891 -4.2186813 -4.1585207 -4.1164103 -4.1251574 -4.0949731 -3.9988637 -3.94102 -3.991699 -4.0861893 -4.1533689 -4.1882663 -4.1912589 -4.1860571][-4.2608228 -4.2654972 -4.2458057 -4.1901889 -4.1478481 -4.1634316 -4.167017 -4.1242723 -4.0934997 -4.1126738 -4.1516066 -4.1794977 -4.1837978 -4.1776109 -4.1748366][-4.2691393 -4.2798963 -4.270577 -4.2362814 -4.2028952 -4.2166753 -4.2385983 -4.2316275 -4.2145939 -4.209837 -4.2174983 -4.213881 -4.1920047 -4.1771922 -4.1848483][-4.2855067 -4.2937469 -4.2896543 -4.2741604 -4.2571874 -4.2649989 -4.2846718 -4.2933903 -4.2824845 -4.2716579 -4.26329 -4.2389278 -4.2008271 -4.1819897 -4.2001543][-4.297792 -4.3021684 -4.298769 -4.28911 -4.2808738 -4.2849789 -4.3001504 -4.3166013 -4.3138232 -4.3044167 -4.2902908 -4.2586794 -4.219574 -4.2061052 -4.2308745][-4.2948842 -4.2924666 -4.2872667 -4.281961 -4.2821546 -4.2915812 -4.3079062 -4.3289251 -4.3363628 -4.3338046 -4.3167911 -4.2860527 -4.2568269 -4.2499 -4.27134][-4.286305 -4.2742996 -4.2657213 -4.2671847 -4.2783351 -4.2962346 -4.3156285 -4.334754 -4.3465209 -4.3509383 -4.3383789 -4.3098645 -4.286458 -4.2778621 -4.2871532]]...]
INFO - root - 2017-12-05 13:14:18.459251: step 13810, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 84h:52m:35s remains)
INFO - root - 2017-12-05 13:14:27.822814: step 13820, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 78h:56m:48s remains)
INFO - root - 2017-12-05 13:14:37.154825: step 13830, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 82h:56m:44s remains)
INFO - root - 2017-12-05 13:14:46.369033: step 13840, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 83h:30m:24s remains)
INFO - root - 2017-12-05 13:14:55.710067: step 13850, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 84h:33m:15s remains)
INFO - root - 2017-12-05 13:15:05.089764: step 13860, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 83h:48m:15s remains)
INFO - root - 2017-12-05 13:15:14.492339: step 13870, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 83h:55m:15s remains)
INFO - root - 2017-12-05 13:15:23.983232: step 13880, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 86h:02m:38s remains)
INFO - root - 2017-12-05 13:15:33.330643: step 13890, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 76h:51m:27s remains)
INFO - root - 2017-12-05 13:15:42.514021: step 13900, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 81h:59m:35s remains)
2017-12-05 13:15:43.243479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.220561 -4.2360644 -4.2476749 -4.2421637 -4.2338676 -4.2232971 -4.2148318 -4.2101398 -4.1920414 -4.1675367 -4.160625 -4.1758742 -4.193655 -4.2076836 -4.2226133][-4.2206521 -4.2402329 -4.2544546 -4.2538757 -4.2489 -4.237886 -4.225687 -4.2127075 -4.1839361 -4.153707 -4.1522226 -4.1789932 -4.2113509 -4.2413511 -4.2629356][-4.2239256 -4.2510948 -4.266839 -4.2691231 -4.2626243 -4.2474952 -4.2289481 -4.206708 -4.1730127 -4.1471143 -4.1545186 -4.1873174 -4.2216187 -4.25529 -4.2778497][-4.2216878 -4.2544141 -4.2691875 -4.2702007 -4.2588634 -4.238296 -4.21458 -4.1907663 -4.1636786 -4.1496906 -4.1659732 -4.1980147 -4.2237697 -4.2453318 -4.2603407][-4.2287498 -4.2600107 -4.2709088 -4.2670088 -4.2447996 -4.2122765 -4.1826835 -4.1635165 -4.1491866 -4.1504006 -4.1754217 -4.2079434 -4.2258897 -4.2294745 -4.2329473][-4.2359772 -4.2600064 -4.2685108 -4.2618442 -4.2285333 -4.1824393 -4.1480985 -4.1354156 -4.1320438 -4.1459384 -4.1768827 -4.2083855 -4.2180057 -4.2080822 -4.2068882][-4.2414265 -4.26116 -4.2693925 -4.258574 -4.2153521 -4.1588225 -4.1181073 -4.1036267 -4.1043911 -4.1269994 -4.1616597 -4.188364 -4.1903214 -4.1780591 -4.1849313][-4.2399154 -4.2593775 -4.2683964 -4.2532749 -4.2027354 -4.1387725 -4.0916095 -4.0752711 -4.0773478 -4.1034737 -4.1428676 -4.1643596 -4.1589389 -4.1470313 -4.1651421][-4.2337804 -4.2532187 -4.2637277 -4.2481837 -4.1955695 -4.1275344 -4.0788822 -4.0661321 -4.0709939 -4.1001244 -4.1406403 -4.1563468 -4.1422038 -4.1285949 -4.1479506][-4.2331257 -4.2499127 -4.2600636 -4.2461472 -4.1984315 -4.1356688 -4.0909848 -4.0833344 -4.0928421 -4.1192536 -4.1548886 -4.1678548 -4.1555266 -4.1401291 -4.1478844][-4.2362409 -4.2469726 -4.2526832 -4.23792 -4.2003965 -4.154933 -4.1245985 -4.1228938 -4.1350765 -4.1563644 -4.1848974 -4.1937079 -4.183836 -4.1680202 -4.1630621][-4.2412019 -4.242486 -4.2386036 -4.2197847 -4.1921062 -4.1654615 -4.1526065 -4.1604013 -4.1788263 -4.1986527 -4.215807 -4.2147403 -4.1991386 -4.1828089 -4.172452][-4.242981 -4.2350531 -4.2220683 -4.2018752 -4.1819906 -4.1685066 -4.1653852 -4.179388 -4.20296 -4.2241297 -4.2342067 -4.2240896 -4.2003422 -4.179626 -4.1701612][-4.242033 -4.2285318 -4.210835 -4.19304 -4.18304 -4.1808338 -4.1847196 -4.2019167 -4.2263308 -4.2463484 -4.25264 -4.2381911 -4.209024 -4.1795 -4.1668043][-4.243403 -4.2307887 -4.2148576 -4.2019525 -4.2003312 -4.2082744 -4.2184463 -4.2344742 -4.2529812 -4.2661715 -4.2683144 -4.2533331 -4.2247305 -4.191093 -4.1736822]]...]
INFO - root - 2017-12-05 13:15:52.724654: step 13910, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 82h:02m:13s remains)
INFO - root - 2017-12-05 13:16:02.153710: step 13920, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 86h:31m:28s remains)
INFO - root - 2017-12-05 13:16:11.691499: step 13930, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.924 sec/batch; 81h:46m:40s remains)
INFO - root - 2017-12-05 13:16:21.030183: step 13940, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 85h:03m:44s remains)
INFO - root - 2017-12-05 13:16:30.484108: step 13950, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 79h:57m:59s remains)
INFO - root - 2017-12-05 13:16:39.602045: step 13960, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 86h:24m:54s remains)
INFO - root - 2017-12-05 13:16:48.860271: step 13970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 76h:00m:18s remains)
INFO - root - 2017-12-05 13:16:58.181635: step 13980, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 84h:35m:18s remains)
INFO - root - 2017-12-05 13:17:07.421177: step 13990, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 85h:55m:31s remains)
INFO - root - 2017-12-05 13:17:16.890938: step 14000, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 81h:37m:59s remains)
2017-12-05 13:17:17.655649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2234464 -4.2098169 -4.2118759 -4.223208 -4.2301974 -4.2271509 -4.2295489 -4.2382841 -4.2556548 -4.2656093 -4.2661309 -4.2670703 -4.2702761 -4.2739143 -4.2758365][-4.2261248 -4.2150068 -4.2211194 -4.23594 -4.2476873 -4.2473354 -4.2491531 -4.2540712 -4.271697 -4.2817893 -4.2805753 -4.2795539 -4.2829456 -4.287714 -4.2901287][-4.22936 -4.2203393 -4.2300396 -4.2471137 -4.2606883 -4.2592735 -4.2567496 -4.2577033 -4.2746992 -4.2828031 -4.2758389 -4.2693181 -4.2723465 -4.2795162 -4.2829909][-4.23594 -4.2296157 -4.2396212 -4.252255 -4.2586083 -4.249475 -4.2376065 -4.2305989 -4.2428045 -4.2436318 -4.229322 -4.2184534 -4.2229991 -4.2342949 -4.2381654][-4.2382088 -4.2327662 -4.2418547 -4.2465081 -4.2382941 -4.2115774 -4.1769428 -4.147912 -4.1512942 -4.1493797 -4.1372776 -4.1315093 -4.1412311 -4.1587863 -4.1651855][-4.2355971 -4.2250457 -4.2233067 -4.2108369 -4.1737642 -4.1135011 -4.0381737 -3.9730823 -3.9776211 -4.000762 -4.0200734 -4.0424738 -4.070169 -4.1005912 -4.114388][-4.2323666 -4.2140317 -4.196559 -4.1638026 -4.099618 -4.0091977 -3.8999979 -3.8084645 -3.8365493 -3.9054651 -3.967483 -4.0191317 -4.0583692 -4.0920649 -4.1072025][-4.2406797 -4.2240934 -4.208981 -4.1842322 -4.1351156 -4.0631189 -3.9789915 -3.9111407 -3.9345667 -3.9935529 -4.0494752 -4.0923772 -4.1220355 -4.1464438 -4.1540346][-4.25487 -4.2443061 -4.2409391 -4.2368884 -4.2126918 -4.1679268 -4.11947 -4.0807228 -4.0908542 -4.1181774 -4.1462688 -4.1677923 -4.1827359 -4.1972842 -4.1997132][-4.2598548 -4.24806 -4.2458453 -4.2492628 -4.2386732 -4.2117076 -4.1894765 -4.1757154 -4.1827831 -4.1910911 -4.1969671 -4.2004919 -4.2039623 -4.2094359 -4.2074986][-4.252914 -4.2356892 -4.2309628 -4.236547 -4.2366529 -4.2248478 -4.21931 -4.2253213 -4.2368937 -4.2379794 -4.230741 -4.222724 -4.2178397 -4.2130461 -4.2028174][-4.2436805 -4.2261629 -4.2233224 -4.2300892 -4.23563 -4.2309752 -4.2310972 -4.2432265 -4.2568851 -4.255486 -4.2466245 -4.2371674 -4.2306595 -4.2240477 -4.2141976][-4.237854 -4.2224483 -4.2230577 -4.2298155 -4.2357845 -4.229701 -4.224874 -4.231205 -4.2433419 -4.2440867 -4.2400355 -4.2376266 -4.2373581 -4.2371707 -4.2355194][-4.236464 -4.2188864 -4.2172866 -4.2208323 -4.2231312 -4.2145638 -4.208528 -4.2150235 -4.2314463 -4.2396531 -4.2432423 -4.2475471 -4.2528367 -4.2573528 -4.2602859][-4.2342105 -4.2114606 -4.2051578 -4.2050734 -4.20546 -4.1985011 -4.1957493 -4.2074933 -4.2328758 -4.2498255 -4.2600756 -4.2689843 -4.2763038 -4.2813416 -4.2845879]]...]
INFO - root - 2017-12-05 13:17:27.070991: step 14010, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.965 sec/batch; 85h:24m:57s remains)
INFO - root - 2017-12-05 13:17:36.335116: step 14020, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 77h:50m:20s remains)
INFO - root - 2017-12-05 13:17:45.600792: step 14030, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 85h:11m:48s remains)
INFO - root - 2017-12-05 13:17:54.965677: step 14040, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.992 sec/batch; 87h:43m:28s remains)
INFO - root - 2017-12-05 13:18:04.351910: step 14050, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 85h:33m:32s remains)
INFO - root - 2017-12-05 13:18:13.876763: step 14060, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 85h:51m:12s remains)
INFO - root - 2017-12-05 13:18:23.109944: step 14070, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 81h:32m:32s remains)
INFO - root - 2017-12-05 13:18:32.018479: step 14080, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 85h:15m:15s remains)
INFO - root - 2017-12-05 13:18:41.423854: step 14090, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 83h:37m:13s remains)
INFO - root - 2017-12-05 13:18:50.798130: step 14100, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 77h:31m:26s remains)
2017-12-05 13:18:51.601823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2314143 -4.2278228 -4.2252836 -4.2235503 -4.225647 -4.2252083 -4.2210331 -4.2137475 -4.2044554 -4.2001948 -4.2014766 -4.2141881 -4.2328076 -4.2467179 -4.2595816][-4.1940227 -4.1847434 -4.182148 -4.184114 -4.1902223 -4.1918845 -4.1871119 -4.1789732 -4.1689458 -4.1656075 -4.168725 -4.1844316 -4.2062182 -4.2224402 -4.2386022][-4.1515026 -4.1385593 -4.1378188 -4.1451626 -4.1552815 -4.1586332 -4.1565647 -4.1523213 -4.1475983 -4.1503739 -4.1573324 -4.17008 -4.18745 -4.2017579 -4.2202921][-4.1071439 -4.0902438 -4.0880308 -4.095048 -4.1059537 -4.109024 -4.1077585 -4.1099539 -4.11369 -4.125998 -4.1401596 -4.1556678 -4.1726179 -4.1855869 -4.2044487][-4.077188 -4.0535383 -4.0439858 -4.045002 -4.051568 -4.0496187 -4.0441494 -4.0470781 -4.0578232 -4.0823817 -4.1098218 -4.1349258 -4.1584744 -4.17523 -4.1952677][-4.0652089 -4.0322418 -4.010242 -4.0006571 -3.9974406 -3.9850283 -3.9671946 -3.9600735 -3.9755201 -4.0180306 -4.0673385 -4.1083589 -4.1423726 -4.1669397 -4.1918888][-4.0373397 -3.9991868 -3.9655273 -3.9388978 -3.9158568 -3.8893967 -3.8565316 -3.8327432 -3.8538642 -3.9237738 -4.0014911 -4.0605559 -4.1097565 -4.1488204 -4.1859379][-4.0275788 -3.9991603 -3.9679649 -3.9277785 -3.8880804 -3.85335 -3.8127143 -3.7813315 -3.8084161 -3.8894186 -3.9720943 -4.0321188 -4.0885315 -4.1384492 -4.1803727][-4.0598774 -4.0507116 -4.033669 -3.9988065 -3.9643867 -3.9386635 -3.9070728 -3.8821583 -3.905457 -3.9654524 -4.0208216 -4.0600858 -4.1060729 -4.1508651 -4.184752][-4.1253486 -4.1312675 -4.1289821 -4.1081672 -4.0875959 -4.0712714 -4.0465345 -4.0287466 -4.0424609 -4.076632 -4.1031895 -4.12095 -4.1527338 -4.1857071 -4.2072024][-4.1950512 -4.2078943 -4.2164693 -4.2083063 -4.1990185 -4.1887565 -4.1685 -4.1561193 -4.1635528 -4.1799703 -4.1908107 -4.1963406 -4.2150517 -4.2334843 -4.2440367][-4.2511454 -4.2619028 -4.2742524 -4.2728162 -4.2682142 -4.2619729 -4.2482076 -4.2404575 -4.2430182 -4.25057 -4.2572427 -4.258388 -4.2664962 -4.2751522 -4.2792325][-4.2668877 -4.2725978 -4.2812767 -4.28119 -4.2797918 -4.2785826 -4.2716494 -4.2661104 -4.2663865 -4.2723961 -4.2805843 -4.2831573 -4.2882862 -4.2952037 -4.2988372][-4.2533436 -4.2565069 -4.2618461 -4.2631655 -4.2638121 -4.2646747 -4.261456 -4.2565327 -4.2535505 -4.2583642 -4.2683415 -4.2746563 -4.2829576 -4.2935953 -4.3017545][-4.2557564 -4.2598295 -4.2637348 -4.2654243 -4.2668753 -4.2674274 -4.2646623 -4.2604561 -4.2569551 -4.2596488 -4.2675028 -4.2741933 -4.28241 -4.293376 -4.3033328]]...]
INFO - root - 2017-12-05 13:19:00.779368: step 14110, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 78h:57m:39s remains)
INFO - root - 2017-12-05 13:19:10.052327: step 14120, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 83h:13m:33s remains)
INFO - root - 2017-12-05 13:19:19.413593: step 14130, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 84h:10m:51s remains)
INFO - root - 2017-12-05 13:19:28.930561: step 14140, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.002 sec/batch; 88h:37m:09s remains)
INFO - root - 2017-12-05 13:19:38.286600: step 14150, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 84h:58m:14s remains)
INFO - root - 2017-12-05 13:19:47.595630: step 14160, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 83h:06m:26s remains)
INFO - root - 2017-12-05 13:19:56.838981: step 14170, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 80h:14m:53s remains)
INFO - root - 2017-12-05 13:20:06.259181: step 14180, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 82h:00m:22s remains)
INFO - root - 2017-12-05 13:20:15.765779: step 14190, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 81h:04m:11s remains)
INFO - root - 2017-12-05 13:20:25.298359: step 14200, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 85h:50m:22s remains)
2017-12-05 13:20:26.086839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2955828 -4.2887273 -4.285593 -4.2870631 -4.2860613 -4.2833133 -4.281271 -4.2875347 -4.2952528 -4.2975073 -4.2959871 -4.2945151 -4.2947354 -4.2959275 -4.2966094][-4.2888708 -4.2838225 -4.2804008 -4.2787685 -4.2713227 -4.2623262 -4.2573762 -4.2686453 -4.287549 -4.2984595 -4.3010244 -4.3000579 -4.297925 -4.29708 -4.2981148][-4.2868233 -4.2844272 -4.2760372 -4.2622008 -4.2423515 -4.2211828 -4.2090812 -4.2249784 -4.2592621 -4.2856221 -4.2976723 -4.3046465 -4.3055387 -4.3033857 -4.3033819][-4.2903695 -4.2897358 -4.2752976 -4.2459598 -4.2081828 -4.1688366 -4.1410069 -4.1571784 -4.2107911 -4.2558069 -4.27951 -4.2969403 -4.3063254 -4.3075233 -4.3079238][-4.2976837 -4.297483 -4.2786951 -4.236589 -4.1801581 -4.1146541 -4.0616 -4.0715322 -4.1448779 -4.2110515 -4.2480636 -4.2756548 -4.2945576 -4.3020673 -4.3060946][-4.3042517 -4.3066082 -4.2869816 -4.2414732 -4.1736264 -4.0804505 -3.98988 -3.9755628 -4.0604739 -4.1494875 -4.2040582 -4.2416673 -4.26921 -4.2844167 -4.2945943][-4.3066506 -4.3119659 -4.2942171 -4.2553473 -4.1963825 -4.098196 -3.9700274 -3.9034171 -3.9717021 -4.0730453 -4.1457744 -4.1966324 -4.2286062 -4.249052 -4.26572][-4.3039775 -4.3091922 -4.2919559 -4.2619038 -4.2209768 -4.1439705 -4.0173154 -3.9160779 -3.9385862 -4.01806 -4.08667 -4.1410594 -4.17687 -4.1964521 -4.2127342][-4.2994528 -4.3014612 -4.2831793 -4.2585387 -4.2317472 -4.1858087 -4.093821 -4.0080247 -4.004858 -4.0364757 -4.0746803 -4.1132994 -4.1397367 -4.15238 -4.1611357][-4.2962885 -4.2969909 -4.279954 -4.2580762 -4.2345982 -4.2066703 -4.1522961 -4.0995865 -4.0981216 -4.1062994 -4.1166105 -4.1361961 -4.145278 -4.1410103 -4.130599][-4.2937608 -4.2962494 -4.2858281 -4.2676687 -4.2454896 -4.2224188 -4.1876159 -4.1626763 -4.1684337 -4.1690574 -4.1658306 -4.177587 -4.1788878 -4.1629348 -4.1336508][-4.2918358 -4.294126 -4.2912993 -4.2822537 -4.2659554 -4.2429924 -4.2143607 -4.1995573 -4.2059593 -4.2033591 -4.1953425 -4.20397 -4.2069335 -4.1890512 -4.1589174][-4.2905307 -4.2892623 -4.2898121 -4.2885041 -4.2810559 -4.26263 -4.23737 -4.2223597 -4.2235045 -4.2179589 -4.2088714 -4.2162724 -4.2225451 -4.2109752 -4.1891208][-4.2938852 -4.2883821 -4.2887087 -4.2903757 -4.2903152 -4.2784457 -4.2594967 -4.2446423 -4.2414908 -4.2353268 -4.22461 -4.2281213 -4.2347751 -4.2275076 -4.2128539][-4.3056011 -4.29969 -4.2980709 -4.2977319 -4.2994418 -4.2937732 -4.2825308 -4.2736726 -4.2715683 -4.2664704 -4.2581387 -4.2599564 -4.2655406 -4.2606854 -4.2486644]]...]
INFO - root - 2017-12-05 13:20:35.579194: step 14210, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 84h:54m:11s remains)
INFO - root - 2017-12-05 13:20:45.168863: step 14220, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.984 sec/batch; 86h:58m:41s remains)
INFO - root - 2017-12-05 13:20:54.760495: step 14230, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.934 sec/batch; 82h:35m:40s remains)
INFO - root - 2017-12-05 13:21:04.150847: step 14240, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 84h:50m:31s remains)
INFO - root - 2017-12-05 13:21:13.676431: step 14250, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 81h:06m:13s remains)
INFO - root - 2017-12-05 13:21:22.635151: step 14260, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 74h:12m:51s remains)
INFO - root - 2017-12-05 13:21:31.747838: step 14270, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.977 sec/batch; 86h:21m:50s remains)
INFO - root - 2017-12-05 13:21:41.126907: step 14280, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 85h:07m:59s remains)
INFO - root - 2017-12-05 13:21:50.811540: step 14290, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 84h:57m:10s remains)
INFO - root - 2017-12-05 13:22:00.015076: step 14300, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 82h:48m:32s remains)
2017-12-05 13:22:00.824129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28871 -4.2776251 -4.2807412 -4.2856717 -4.2809381 -4.2786126 -4.2815204 -4.2873225 -4.2837496 -4.2713189 -4.2557116 -4.2423449 -4.2315531 -4.229075 -4.2263041][-4.298553 -4.288671 -4.2888093 -4.287622 -4.2787547 -4.2743292 -4.2730212 -4.27418 -4.2670031 -4.254 -4.2432222 -4.233171 -4.2172537 -4.2088375 -4.2025394][-4.3042021 -4.2927036 -4.28575 -4.274332 -4.2568865 -4.2487993 -4.2439542 -4.2414927 -4.2336926 -4.2233782 -4.2226281 -4.214962 -4.1963806 -4.1879277 -4.1821985][-4.3055034 -4.2914929 -4.27806 -4.2597356 -4.235641 -4.2196684 -4.2049646 -4.1942225 -4.1880331 -4.1883874 -4.2022772 -4.2042813 -4.1937537 -4.1920094 -4.1904678][-4.3010969 -4.2843504 -4.2658057 -4.243103 -4.2156148 -4.1872468 -4.1445785 -4.10736 -4.1061149 -4.1362691 -4.1808686 -4.2063837 -4.2121744 -4.2198625 -4.2205849][-4.2953086 -4.2788758 -4.2588167 -4.2309451 -4.1961851 -4.1456184 -4.0543013 -3.9731541 -3.9869587 -4.0722156 -4.1630378 -4.2159667 -4.2402978 -4.2521853 -4.2479367][-4.2940755 -4.2807016 -4.2609506 -4.2313595 -4.1881089 -4.1080084 -3.9572558 -3.8215942 -3.8557158 -4.0046563 -4.1426806 -4.218204 -4.257616 -4.2745452 -4.2664933][-4.2979455 -4.2875223 -4.2721186 -4.248673 -4.2047095 -4.1126165 -3.9432216 -3.7835803 -3.8232269 -3.9927092 -4.1408587 -4.2181296 -4.2607946 -4.2744946 -4.2609949][-4.30245 -4.2931037 -4.2825046 -4.2661633 -4.233376 -4.1613197 -4.0368114 -3.9188285 -3.9423079 -4.0628166 -4.1736717 -4.2296357 -4.2599158 -4.2617321 -4.2406869][-4.3045549 -4.2923713 -4.2799134 -4.264698 -4.2404184 -4.1989341 -4.1294856 -4.0583224 -4.0675874 -4.1348724 -4.2026992 -4.2418733 -4.2646236 -4.2611241 -4.2353477][-4.3030267 -4.2845106 -4.2633829 -4.2426252 -4.2236547 -4.203414 -4.1695352 -4.1282377 -4.1340017 -4.1781 -4.2221994 -4.2586808 -4.2811446 -4.2740288 -4.2418809][-4.3026915 -4.2809925 -4.2531714 -4.2249885 -4.2069812 -4.1972561 -4.177958 -4.1513734 -4.1559014 -4.1908178 -4.2270851 -4.2639303 -4.2865124 -4.2757344 -4.2389922][-4.3032284 -4.2843165 -4.2590079 -4.2296271 -4.2095551 -4.1994495 -4.1801267 -4.1569386 -4.1604877 -4.1921372 -4.2272186 -4.2616658 -4.2809625 -4.268424 -4.2359462][-4.3039742 -4.2906661 -4.2733812 -4.2486725 -4.225172 -4.2061167 -4.17888 -4.1557188 -4.1603379 -4.1926093 -4.2271161 -4.2550349 -4.268539 -4.2586236 -4.2388725][-4.2995806 -4.2891669 -4.2789984 -4.2588725 -4.2342167 -4.2088385 -4.1774125 -4.1577611 -4.169528 -4.2043848 -4.2367735 -4.2565813 -4.2638893 -4.2554574 -4.2433796]]...]
INFO - root - 2017-12-05 13:22:10.260173: step 14310, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.914 sec/batch; 80h:48m:36s remains)
INFO - root - 2017-12-05 13:22:19.674425: step 14320, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 82h:46m:52s remains)
INFO - root - 2017-12-05 13:22:29.092151: step 14330, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 80h:59m:46s remains)
INFO - root - 2017-12-05 13:22:38.632887: step 14340, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 83h:04m:13s remains)
INFO - root - 2017-12-05 13:22:48.083042: step 14350, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 85h:55m:36s remains)
INFO - root - 2017-12-05 13:22:57.392235: step 14360, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 85h:34m:19s remains)
INFO - root - 2017-12-05 13:23:06.845255: step 14370, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 86h:48m:47s remains)
INFO - root - 2017-12-05 13:23:16.372393: step 14380, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 83h:12m:03s remains)
INFO - root - 2017-12-05 13:23:25.531841: step 14390, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 87h:39m:14s remains)
INFO - root - 2017-12-05 13:23:34.893975: step 14400, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 85h:48m:08s remains)
2017-12-05 13:23:35.641266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3127894 -4.2893128 -4.2669835 -4.2472057 -4.2267251 -4.211524 -4.2197928 -4.2362585 -4.2541804 -4.2666163 -4.27546 -4.2821779 -4.2812705 -4.2757807 -4.2661781][-4.3202391 -4.2927928 -4.266366 -4.2401042 -4.20873 -4.1838856 -4.1893578 -4.2068563 -4.2326827 -4.255827 -4.2737441 -4.2840576 -4.2820473 -4.2726097 -4.2548785][-4.3138204 -4.281579 -4.2510958 -4.2210183 -4.1837707 -4.1521235 -4.1483107 -4.1627412 -4.201055 -4.2387619 -4.2639675 -4.272337 -4.2668786 -4.2538395 -4.2285471][-4.309844 -4.2738953 -4.2426023 -4.2128367 -4.173357 -4.1370544 -4.1217713 -4.1300097 -4.18212 -4.2317662 -4.2550035 -4.2518296 -4.2385416 -4.2222247 -4.192297][-4.3124795 -4.27456 -4.2426138 -4.2133374 -4.1741014 -4.1314278 -4.0929003 -4.084115 -4.1488118 -4.2118583 -4.2351046 -4.2243252 -4.2020183 -4.1799645 -4.1524072][-4.307539 -4.2675753 -4.2294054 -4.1893158 -4.1403122 -4.0774527 -4.0005813 -3.9695697 -4.055295 -4.1435237 -4.1785088 -4.1729026 -4.1494732 -4.1267853 -4.1034102][-4.2920675 -4.2453761 -4.1980639 -4.1434884 -4.0795293 -3.9891984 -3.8632517 -3.8068526 -3.9150374 -4.0355716 -4.0958014 -4.1089358 -4.09468 -4.07725 -4.0584912][-4.2860813 -4.2406735 -4.1988225 -4.1520548 -4.0972266 -4.0153022 -3.8902171 -3.8333747 -3.9234829 -4.0315127 -4.09371 -4.11262 -4.1001115 -4.0814829 -4.0604253][-4.2960062 -4.2605314 -4.2293334 -4.1990581 -4.1649079 -4.111598 -4.0224495 -3.9847341 -4.0407276 -4.1124926 -4.1597958 -4.1709886 -4.1523767 -4.1281209 -4.1006355][-4.3038783 -4.2752314 -4.2502403 -4.22723 -4.2042184 -4.1694026 -4.1065826 -4.0836058 -4.1181045 -4.1644173 -4.1988993 -4.2079773 -4.1913137 -4.1662135 -4.1363592][-4.3035684 -4.275722 -4.2499695 -4.2276564 -4.2084827 -4.1812897 -4.1349936 -4.1208496 -4.1447659 -4.1781864 -4.2012405 -4.2091069 -4.1992745 -4.1780353 -4.1525669][-4.301692 -4.2705503 -4.2407765 -4.2169476 -4.2016249 -4.1830812 -4.1540689 -4.1481223 -4.1670547 -4.1927328 -4.2095652 -4.2154694 -4.2083721 -4.1919088 -4.1730328][-4.295444 -4.2630773 -4.2346749 -4.2172 -4.2111683 -4.2079935 -4.1997385 -4.2022524 -4.2152658 -4.2314405 -4.2382722 -4.23657 -4.22587 -4.2110033 -4.1970348][-4.28712 -4.25364 -4.2264762 -4.2126255 -4.2122841 -4.2216668 -4.229672 -4.23841 -4.2468596 -4.2546825 -4.2548184 -4.249361 -4.2396579 -4.2294 -4.2190013][-4.2885289 -4.2555737 -4.2276235 -4.2121639 -4.2100649 -4.2217793 -4.2363806 -4.2468467 -4.2532363 -4.2563114 -4.2552252 -4.2518563 -4.247467 -4.2434082 -4.2374496]]...]
INFO - root - 2017-12-05 13:23:45.177664: step 14410, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 86h:18m:56s remains)
INFO - root - 2017-12-05 13:23:54.479365: step 14420, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 83h:31m:58s remains)
INFO - root - 2017-12-05 13:24:04.038382: step 14430, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 86h:14m:34s remains)
INFO - root - 2017-12-05 13:24:13.371266: step 14440, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 77h:47m:17s remains)
INFO - root - 2017-12-05 13:24:22.843956: step 14450, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 84h:14m:07s remains)
INFO - root - 2017-12-05 13:24:32.189401: step 14460, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.015 sec/batch; 89h:39m:23s remains)
INFO - root - 2017-12-05 13:24:41.751338: step 14470, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 87h:10m:23s remains)
INFO - root - 2017-12-05 13:24:51.249008: step 14480, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.015 sec/batch; 89h:41m:27s remains)
INFO - root - 2017-12-05 13:25:00.713599: step 14490, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.006 sec/batch; 88h:50m:33s remains)
INFO - root - 2017-12-05 13:25:10.248861: step 14500, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 77h:43m:49s remains)
2017-12-05 13:25:11.028616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2467852 -4.249589 -4.2545495 -4.2530169 -4.2464633 -4.2413359 -4.2437778 -4.2577224 -4.2712712 -4.2824407 -4.2857456 -4.2862067 -4.2913947 -4.2945261 -4.2961373][-4.2462177 -4.25577 -4.2691221 -4.2758007 -4.2746367 -4.2710614 -4.2708664 -4.2806783 -4.2931213 -4.3035994 -4.3094773 -4.3125029 -4.3181949 -4.3196573 -4.3124704][-4.2433977 -4.2555809 -4.273653 -4.2858691 -4.2908473 -4.2877026 -4.2807126 -4.2795706 -4.2853775 -4.292531 -4.3003287 -4.3073525 -4.3133636 -4.3167906 -4.3087211][-4.2482886 -4.2561531 -4.2714996 -4.2842674 -4.291491 -4.2823734 -4.2589812 -4.2378025 -4.2314339 -4.2389417 -4.2538238 -4.2689447 -4.2778435 -4.2851052 -4.282023][-4.2567196 -4.2553091 -4.2629805 -4.27544 -4.2801671 -4.2582197 -4.2096429 -4.1605096 -4.1430836 -4.1567192 -4.1853547 -4.2112474 -4.2262163 -4.2344346 -4.2326217][-4.2539873 -4.240654 -4.237854 -4.2453971 -4.2417946 -4.2011142 -4.1260881 -4.05397 -4.0381341 -4.0675678 -4.11225 -4.1493397 -4.1709676 -4.1778359 -4.1716623][-4.2162771 -4.1899157 -4.1764565 -4.1755667 -4.1601014 -4.1010017 -4.0051756 -3.9265356 -3.9318106 -3.9900022 -4.0524797 -4.1005392 -4.1304092 -4.136456 -4.1250868][-4.1353197 -4.1006413 -4.0845175 -4.07971 -4.0571213 -3.9925334 -3.9010458 -3.8437138 -3.8750687 -3.9471178 -4.0101905 -4.0602283 -4.0962992 -4.1037273 -4.0893059][-4.0621772 -4.0335517 -4.0271087 -4.0273619 -4.0090151 -3.9564495 -3.8935862 -3.8676736 -3.9066081 -3.9674921 -4.014792 -4.0550337 -4.0882339 -4.0927153 -4.0730987][-4.0450096 -4.0387 -4.0507507 -4.0592189 -4.0477819 -4.011529 -3.9765942 -3.9715872 -4.0034466 -4.0439768 -4.0704761 -4.0963879 -4.119771 -4.1173935 -4.09368][-4.1045442 -4.1163015 -4.1368942 -4.1465564 -4.1381803 -4.1150374 -4.1000195 -4.1067877 -4.1310086 -4.155293 -4.1671443 -4.1831021 -4.197197 -4.187798 -4.1606946][-4.2010736 -4.2189302 -4.2383018 -4.2447429 -4.2376719 -4.2257094 -4.2241235 -4.2359781 -4.2541037 -4.2690392 -4.2733879 -4.2811165 -4.2860661 -4.2711387 -4.2430625][-4.2809777 -4.2949929 -4.3080459 -4.3123045 -4.30932 -4.3074465 -4.314003 -4.3262062 -4.3391285 -4.3470321 -4.3455877 -4.3441072 -4.3405566 -4.3261957 -4.3040495][-4.33023 -4.3381152 -4.3446779 -4.3468742 -4.3470106 -4.3498058 -4.3574333 -4.3669372 -4.3748283 -4.3780336 -4.3747048 -4.3692436 -4.3623066 -4.3517203 -4.3384418][-4.3508053 -4.3545771 -4.3567491 -4.3577709 -4.3583336 -4.3605027 -4.3657193 -4.3714786 -4.3757567 -4.37701 -4.3748646 -4.3703136 -4.3645182 -4.3587289 -4.3529644]]...]
INFO - root - 2017-12-05 13:25:20.523889: step 14510, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 82h:59m:02s remains)
INFO - root - 2017-12-05 13:25:29.980397: step 14520, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 87h:18m:59s remains)
INFO - root - 2017-12-05 13:25:39.159118: step 14530, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 76h:10m:03s remains)
INFO - root - 2017-12-05 13:25:48.461131: step 14540, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 83h:50m:53s remains)
INFO - root - 2017-12-05 13:25:57.856156: step 14550, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 82h:09m:15s remains)
INFO - root - 2017-12-05 13:26:07.307895: step 14560, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 77h:43m:39s remains)
INFO - root - 2017-12-05 13:26:16.745790: step 14570, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 77h:00m:11s remains)
INFO - root - 2017-12-05 13:26:26.160343: step 14580, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 81h:52m:15s remains)
INFO - root - 2017-12-05 13:26:35.686494: step 14590, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.016 sec/batch; 89h:42m:31s remains)
INFO - root - 2017-12-05 13:26:45.277834: step 14600, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 81h:31m:53s remains)
2017-12-05 13:26:46.061058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.229795 -4.1925621 -4.1735029 -4.1686039 -4.18301 -4.21815 -4.2560964 -4.28974 -4.3151841 -4.3281784 -4.3355813 -4.34011 -4.3421073 -4.3330135 -4.29821][-4.2718792 -4.2296309 -4.2012649 -4.1851516 -4.1866817 -4.2133489 -4.2454109 -4.2756805 -4.3017359 -4.3175144 -4.3298178 -4.3386664 -4.3441353 -4.341959 -4.32175][-4.3081737 -4.2669673 -4.2305894 -4.2027082 -4.1929746 -4.2090855 -4.2345715 -4.2608929 -4.2846107 -4.3032694 -4.3196497 -4.3322978 -4.3389649 -4.3409572 -4.3345532][-4.3291554 -4.2909079 -4.2487087 -4.2132521 -4.1941094 -4.1997304 -4.2164025 -4.2394848 -4.2651124 -4.287941 -4.308351 -4.323525 -4.3312812 -4.3359404 -4.33976][-4.3352246 -4.3033967 -4.262218 -4.2214603 -4.1888652 -4.1766315 -4.181211 -4.2072854 -4.2404761 -4.2699485 -4.291697 -4.3075843 -4.316833 -4.3272734 -4.3398447][-4.3334069 -4.3047352 -4.26126 -4.2091088 -4.159111 -4.1239386 -4.1117668 -4.1448646 -4.1987276 -4.2420683 -4.266253 -4.2824197 -4.2938061 -4.3117337 -4.333425][-4.3244419 -4.2889881 -4.2336535 -4.1653619 -4.1019812 -4.047431 -4.0139689 -4.050972 -4.1313481 -4.1923733 -4.2213764 -4.2393122 -4.2547541 -4.2828574 -4.3169975][-4.3110528 -4.263308 -4.1922145 -4.1107988 -4.0397282 -3.9732985 -3.9139748 -3.9433875 -4.0434904 -4.12004 -4.1558805 -4.1810393 -4.2076659 -4.2478628 -4.2932253][-4.3013954 -4.2415562 -4.1591325 -4.0714126 -4.0015273 -3.9344556 -3.8617547 -3.8692675 -3.9659102 -4.0440993 -4.0805964 -4.1160851 -4.157824 -4.2115192 -4.2663951][-4.3006597 -4.2337127 -4.1467237 -4.0588036 -3.992404 -3.936146 -3.8727593 -3.8578537 -3.9180567 -3.9721391 -3.9976087 -4.0402384 -4.0992641 -4.1684761 -4.2350507][-4.308753 -4.2429743 -4.1589069 -4.0731897 -4.005115 -3.9540262 -3.9056883 -3.8869696 -3.9112313 -3.9276142 -3.9329147 -3.9733953 -4.042809 -4.1248574 -4.2030492][-4.3252153 -4.2711229 -4.199194 -4.1238809 -4.0595326 -4.0091286 -3.9683812 -3.9475958 -3.9440715 -3.9262977 -3.9068289 -3.9348712 -4.0041089 -4.0900087 -4.1736765][-4.3447437 -4.3079748 -4.2549906 -4.1985145 -4.1485786 -4.1064038 -4.0711946 -4.0444584 -4.0202589 -3.9793508 -3.9384878 -3.9448211 -4.0016003 -4.0823531 -4.1640191][-4.3604326 -4.3413339 -4.3093615 -4.2739768 -4.2420158 -4.2140017 -4.1884403 -4.1618705 -4.1304741 -4.0859165 -4.0411992 -4.0313754 -4.0684724 -4.1327438 -4.1995258][-4.3684654 -4.3607883 -4.3455076 -4.3277574 -4.3115458 -4.2974682 -4.28461 -4.2672596 -4.2411356 -4.2044492 -4.1692829 -4.1557856 -4.1769042 -4.2221155 -4.2674732]]...]
INFO - root - 2017-12-05 13:26:55.391373: step 14610, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 80h:55m:06s remains)
INFO - root - 2017-12-05 13:27:04.926757: step 14620, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.011 sec/batch; 89h:16m:21s remains)
INFO - root - 2017-12-05 13:27:14.259863: step 14630, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 84h:22m:21s remains)
INFO - root - 2017-12-05 13:27:23.611892: step 14640, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 80h:47m:27s remains)
INFO - root - 2017-12-05 13:27:32.947618: step 14650, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 77h:06m:35s remains)
INFO - root - 2017-12-05 13:27:42.447131: step 14660, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 81h:39m:59s remains)
INFO - root - 2017-12-05 13:27:51.760758: step 14670, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 82h:51m:38s remains)
INFO - root - 2017-12-05 13:28:01.269824: step 14680, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 86h:22m:56s remains)
INFO - root - 2017-12-05 13:28:10.849028: step 14690, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 82h:00m:50s remains)
INFO - root - 2017-12-05 13:28:20.196193: step 14700, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 83h:49m:29s remains)
2017-12-05 13:28:20.982608: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2984776 -4.301125 -4.3112454 -4.3200965 -4.3229961 -4.3207006 -4.3247428 -4.3256168 -4.3173852 -4.3091526 -4.3046212 -4.30891 -4.3118682 -4.3160877 -4.3197875][-4.2726927 -4.2703629 -4.2789679 -4.2879629 -4.2924695 -4.2929564 -4.2974167 -4.2957125 -4.2865043 -4.2768559 -4.2711458 -4.2719707 -4.2735887 -4.2765665 -4.2786055][-4.213357 -4.2042537 -4.2115932 -4.221848 -4.2303672 -4.23297 -4.23566 -4.2321115 -4.2205648 -4.2097087 -4.2014604 -4.1952391 -4.1908627 -4.1913042 -4.1936474][-4.1405573 -4.1184545 -4.115119 -4.12342 -4.1365733 -4.1446109 -4.1489153 -4.1451025 -4.1313925 -4.1170158 -4.1021624 -4.08669 -4.0739727 -4.0694966 -4.0686426][-4.0579004 -4.0156507 -3.9952805 -3.9965441 -4.00937 -4.0194917 -4.0248375 -4.0218511 -4.00913 -3.9951656 -3.9783826 -3.9574263 -3.9386187 -3.9301519 -3.9251847][-4.023953 -3.9674804 -3.9330363 -3.9250681 -3.929112 -3.9314828 -3.9325547 -3.9308987 -3.926887 -3.9247546 -3.9179847 -3.9018455 -3.8858576 -3.8786535 -3.8735938][-4.0788283 -4.02868 -3.9920843 -3.9782979 -3.971036 -3.9594405 -3.9476156 -3.9453695 -3.9557483 -3.9720373 -3.9850171 -3.9865451 -3.9822316 -3.9796734 -3.9761629][-4.1506438 -4.1138616 -4.0842414 -4.0705156 -4.0590434 -4.0386038 -4.0157895 -4.0098524 -4.0268874 -4.0532537 -4.0802016 -4.0989895 -4.1057081 -4.1079726 -4.1055322][-4.1873069 -4.1576734 -4.1332154 -4.123683 -4.1161809 -4.0986652 -4.0762296 -4.0682707 -4.0843897 -4.1108947 -4.1401386 -4.1635919 -4.1730533 -4.176239 -4.1719003][-4.1936169 -4.1642513 -4.1406097 -4.1347313 -4.1335325 -4.1235967 -4.10756 -4.1000957 -4.1096435 -4.129252 -4.1530905 -4.1731839 -4.1804581 -4.1832223 -4.1807094][-4.1940036 -4.1624017 -4.1372471 -4.1343441 -4.1392126 -4.1372643 -4.1299791 -4.12362 -4.1217527 -4.1280613 -4.14238 -4.1567688 -4.1630096 -4.1661978 -4.1655393][-4.2000566 -4.1666451 -4.1381726 -4.1358881 -4.14558 -4.150466 -4.1527576 -4.1492181 -4.1373339 -4.1306348 -4.1344233 -4.1416569 -4.14544 -4.148839 -4.1498742][-4.2169991 -4.1828456 -4.1526208 -4.1493707 -4.1573181 -4.1610637 -4.1656823 -4.1616583 -4.1436839 -4.1277142 -4.123189 -4.122447 -4.1228981 -4.1268315 -4.12867][-4.2322116 -4.1994543 -4.1706662 -4.1658888 -4.1688704 -4.1700139 -4.1735315 -4.1692944 -4.1506767 -4.1341929 -4.1266093 -4.1203837 -4.1175909 -4.1210055 -4.1235733][-4.2402468 -4.2072754 -4.1799574 -4.1733913 -4.1735649 -4.1738224 -4.1778789 -4.1773553 -4.1649728 -4.1519275 -4.144383 -4.1365457 -4.1323948 -4.134624 -4.1399331]]...]
INFO - root - 2017-12-05 13:28:30.400134: step 14710, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 83h:39m:45s remains)
INFO - root - 2017-12-05 13:28:39.693032: step 14720, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 81h:14m:05s remains)
INFO - root - 2017-12-05 13:28:48.982769: step 14730, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 77h:16m:29s remains)
INFO - root - 2017-12-05 13:28:58.411064: step 14740, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.930 sec/batch; 82h:02m:43s remains)
INFO - root - 2017-12-05 13:29:07.850299: step 14750, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 80h:54m:06s remains)
INFO - root - 2017-12-05 13:29:17.389067: step 14760, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 81h:41m:03s remains)
INFO - root - 2017-12-05 13:29:26.776621: step 14770, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 80h:19m:21s remains)
INFO - root - 2017-12-05 13:29:36.129673: step 14780, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 83h:12m:21s remains)
INFO - root - 2017-12-05 13:29:45.486655: step 14790, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 80h:51m:47s remains)
INFO - root - 2017-12-05 13:29:54.797835: step 14800, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 79h:33m:58s remains)
2017-12-05 13:29:55.580003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3170662 -4.3104577 -4.3020291 -4.2968621 -4.2960615 -4.2971425 -4.2924285 -4.2785678 -4.2611923 -4.252727 -4.2526979 -4.2580161 -4.2669868 -4.2751255 -4.2863774][-4.32335 -4.3151093 -4.3074622 -4.304924 -4.3079014 -4.3132868 -4.3127112 -4.3032894 -4.2890882 -4.27883 -4.2747388 -4.2779508 -4.2862086 -4.29273 -4.3010406][-4.3158984 -4.3062673 -4.2984595 -4.2951674 -4.2984624 -4.3066545 -4.3099051 -4.3027172 -4.2895226 -4.2801714 -4.2761121 -4.2795839 -4.2883124 -4.2961745 -4.303833][-4.2919993 -4.2842741 -4.2754784 -4.26804 -4.2679644 -4.2747221 -4.2764235 -4.2665405 -4.2525582 -4.2455449 -4.2457452 -4.2537661 -4.2659149 -4.27851 -4.2896819][-4.2463703 -4.245 -4.2379813 -4.2271585 -4.2210937 -4.221416 -4.2153006 -4.1975074 -4.1784282 -4.1738949 -4.1821423 -4.2004447 -4.2202611 -4.2401896 -4.259089][-4.1833892 -4.1898117 -4.1858435 -4.1711321 -4.1603265 -4.1537223 -4.1368036 -4.1064482 -4.0768595 -4.075645 -4.0986037 -4.1327391 -4.1638227 -4.1931067 -4.2207317][-4.1196456 -4.12657 -4.1207814 -4.0993915 -4.0840631 -4.0728054 -4.0511165 -4.01228 -3.9735842 -3.978003 -4.0158148 -4.0649176 -4.1082926 -4.1469393 -4.1828732][-4.0643573 -4.0608091 -4.0451517 -4.017406 -4.0014491 -3.9931223 -3.9778328 -3.9427526 -3.905875 -3.9190102 -3.9642591 -4.0183134 -4.0677781 -4.1125565 -4.1551418][-4.0547543 -4.042357 -4.0223746 -3.9973617 -3.9855731 -3.9822705 -3.9774628 -3.95711 -3.9329765 -3.9498675 -3.9892867 -4.03473 -4.0784492 -4.1192818 -4.1584721][-4.1025934 -4.0895791 -4.0736628 -4.0575814 -4.0511746 -4.0525064 -4.0560455 -4.0497165 -4.0370784 -4.0501409 -4.0773015 -4.10843 -4.1391854 -4.16787 -4.1963687][-4.1700716 -4.1618528 -4.1533055 -4.1469221 -4.1441832 -4.1470933 -4.15394 -4.1547351 -4.1496043 -4.1571555 -4.1735783 -4.1937108 -4.2133746 -4.230619 -4.247117][-4.2368703 -4.2335396 -4.2308688 -4.2291718 -4.2274489 -4.2285604 -4.2340536 -4.2365723 -4.2360778 -4.2409296 -4.2510386 -4.2647123 -4.2775941 -4.2861919 -4.2923307][-4.2682047 -4.2668986 -4.2670412 -4.2668948 -4.2666249 -4.2677183 -4.2716351 -4.2738829 -4.2756119 -4.2799244 -4.28774 -4.299037 -4.310113 -4.3159728 -4.3178444][-4.2726636 -4.2718973 -4.2732406 -4.2745819 -4.2760663 -4.2779374 -4.280879 -4.2826104 -4.2854786 -4.2903056 -4.2974849 -4.307837 -4.3199134 -4.3260427 -4.3272681][-4.2647982 -4.2652578 -4.2676311 -4.2692761 -4.2714458 -4.2744112 -4.2765989 -4.2769513 -4.2789364 -4.28287 -4.28875 -4.2991776 -4.3132563 -4.3213849 -4.3240252]]...]
INFO - root - 2017-12-05 13:30:04.902216: step 14810, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 82h:04m:09s remains)
INFO - root - 2017-12-05 13:30:14.662496: step 14820, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.998 sec/batch; 88h:03m:21s remains)
INFO - root - 2017-12-05 13:30:24.028638: step 14830, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 83h:39m:49s remains)
INFO - root - 2017-12-05 13:30:33.533978: step 14840, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 80h:47m:38s remains)
INFO - root - 2017-12-05 13:30:42.845491: step 14850, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 79h:51m:38s remains)
INFO - root - 2017-12-05 13:30:52.439754: step 14860, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.007 sec/batch; 88h:52m:23s remains)
INFO - root - 2017-12-05 13:31:01.763343: step 14870, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 81h:59m:58s remains)
INFO - root - 2017-12-05 13:31:11.177255: step 14880, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.954 sec/batch; 84h:09m:39s remains)
INFO - root - 2017-12-05 13:31:20.692764: step 14890, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 87h:26m:38s remains)
INFO - root - 2017-12-05 13:31:29.976025: step 14900, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 85h:28m:33s remains)
2017-12-05 13:31:30.731887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25105 -4.2160654 -4.169651 -4.1560555 -4.1776037 -4.2122021 -4.2368503 -4.207159 -4.150816 -4.1342816 -4.1592927 -4.1758757 -4.1763325 -4.1856122 -4.2081218][-4.2328558 -4.1941123 -4.1479168 -4.1486073 -4.1873488 -4.2302222 -4.2580633 -4.2306614 -4.169498 -4.1428137 -4.1581635 -4.1752844 -4.1769161 -4.1856165 -4.2042594][-4.1964741 -4.1535931 -4.1106443 -4.12653 -4.1814933 -4.2285819 -4.2529039 -4.2355566 -4.1934948 -4.1764159 -4.1776915 -4.1858473 -4.1881723 -4.1954436 -4.2067752][-4.164259 -4.121026 -4.0831828 -4.1046114 -4.1600652 -4.198679 -4.2164054 -4.2166619 -4.2045031 -4.2059979 -4.2040653 -4.20075 -4.2033319 -4.2110577 -4.2148151][-4.1557922 -4.1141424 -4.0797796 -4.0853853 -4.1088958 -4.1218777 -4.1335812 -4.1554 -4.1835957 -4.2108912 -4.2177563 -4.2122025 -4.2139215 -4.2244678 -4.2267523][-4.182548 -4.1489034 -4.109385 -4.0789208 -4.0436158 -4.004611 -4.00094 -4.0521193 -4.1311765 -4.1962996 -4.2212033 -4.2222276 -4.2283835 -4.2467284 -4.2508049][-4.2314105 -4.2072725 -4.1675792 -4.1110582 -4.0238142 -3.9178958 -3.877172 -3.9544516 -4.0814018 -4.17717 -4.2195058 -4.2279224 -4.2430143 -4.2691936 -4.2772756][-4.2710214 -4.2576013 -4.224864 -4.1691689 -4.0702929 -3.9336972 -3.8496475 -3.9118483 -4.0499411 -4.1554174 -4.2057304 -4.2212687 -4.2412882 -4.2714219 -4.2895174][-4.2836924 -4.2791314 -4.260148 -4.2242651 -4.1495185 -4.0288024 -3.9194393 -3.9242096 -4.030745 -4.1324019 -4.1863656 -4.2050128 -4.2258821 -4.2565308 -4.2854471][-4.2816472 -4.2853394 -4.2788548 -4.2585225 -4.2099071 -4.1201305 -4.0114717 -3.9671121 -4.0242682 -4.1126981 -4.17042 -4.1925488 -4.2122822 -4.23599 -4.2680492][-4.2698903 -4.2785335 -4.2819767 -4.2727327 -4.2437315 -4.1845179 -4.0951405 -4.0340643 -4.0474749 -4.1101556 -4.1642818 -4.1879778 -4.1999784 -4.2136545 -4.2423534][-4.2541008 -4.2606196 -4.2696438 -4.2688355 -4.2533345 -4.2175589 -4.15467 -4.0991526 -4.0889988 -4.1238194 -4.1675329 -4.1878829 -4.1944962 -4.1993837 -4.2174687][-4.2332945 -4.2301588 -4.2340193 -4.2361064 -4.2278595 -4.2138357 -4.1803288 -4.1428838 -4.129272 -4.1482639 -4.1826057 -4.1982684 -4.1964231 -4.1912928 -4.1971612][-4.2256193 -4.2086315 -4.1963191 -4.1893473 -4.1788516 -4.1761565 -4.1681232 -4.1557627 -4.1523638 -4.167829 -4.1990995 -4.2093611 -4.1965623 -4.1783242 -4.1737008][-4.2328634 -4.2105389 -4.1818776 -4.1575785 -4.1377306 -4.1350889 -4.1373982 -4.1405149 -4.1507168 -4.1709771 -4.2026443 -4.2113285 -4.1944261 -4.168045 -4.1516771]]...]
INFO - root - 2017-12-05 13:31:40.115413: step 14910, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 86h:10m:01s remains)
INFO - root - 2017-12-05 13:31:49.618381: step 14920, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.983 sec/batch; 86h:41m:11s remains)
INFO - root - 2017-12-05 13:31:58.836929: step 14930, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 85h:05m:52s remains)
INFO - root - 2017-12-05 13:32:08.286150: step 14940, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 82h:18m:17s remains)
INFO - root - 2017-12-05 13:32:17.592433: step 14950, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 82h:49m:31s remains)
INFO - root - 2017-12-05 13:32:27.143518: step 14960, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 82h:04m:13s remains)
INFO - root - 2017-12-05 13:32:36.567289: step 14970, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 86h:08m:28s remains)
INFO - root - 2017-12-05 13:32:46.075612: step 14980, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 83h:30m:39s remains)
INFO - root - 2017-12-05 13:32:55.299558: step 14990, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 84h:11m:09s remains)
INFO - root - 2017-12-05 13:33:04.704943: step 15000, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 84h:57m:21s remains)
2017-12-05 13:33:05.428230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2440639 -4.23799 -4.2373228 -4.2378192 -4.2349854 -4.2297211 -4.2237678 -4.2203026 -4.2209277 -4.2207952 -4.2131186 -4.1989236 -4.187273 -4.1940918 -4.2113733][-4.2520475 -4.2474484 -4.2493968 -4.2522211 -4.2506695 -4.2463589 -4.2432032 -4.2417073 -4.2397614 -4.2340083 -4.2183032 -4.1965046 -4.1793437 -4.1836767 -4.2004557][-4.2430644 -4.2381582 -4.2409654 -4.2433047 -4.2399344 -4.2349973 -4.2346392 -4.2361732 -4.2362876 -4.2305713 -4.2132168 -4.1885347 -4.1682138 -4.1686411 -4.18349][-4.2160974 -4.2129455 -4.2161503 -4.2178802 -4.2131052 -4.2068243 -4.2085519 -4.2145605 -4.2192578 -4.2202525 -4.2101274 -4.1913333 -4.1724129 -4.1693907 -4.1802921][-4.1933708 -4.1892514 -4.1893816 -4.1875954 -4.1777191 -4.1658907 -4.1660604 -4.1742477 -4.1866183 -4.2008009 -4.2042046 -4.1968722 -4.1855087 -4.1825833 -4.1883788][-4.1829262 -4.174789 -4.1679254 -4.1578546 -4.137239 -4.1110024 -4.0969706 -4.09733 -4.1211724 -4.1556444 -4.1758924 -4.1811285 -4.17807 -4.1789794 -4.1881032][-4.1760955 -4.1657448 -4.153563 -4.1349821 -4.0980096 -4.0441689 -3.9945235 -3.9740381 -4.0141931 -4.08079 -4.127378 -4.1469574 -4.1509809 -4.154943 -4.166431][-4.1747303 -4.1628265 -4.1459455 -4.1202307 -4.0684671 -3.9850929 -3.8971598 -3.8574193 -3.916748 -4.0152936 -4.0869379 -4.1181011 -4.1248131 -4.1258435 -4.1365538][-4.1847887 -4.174408 -4.1594324 -4.1378388 -4.0909367 -4.0088563 -3.9248431 -3.8926809 -3.9446409 -4.0344753 -4.1025162 -4.1280475 -4.12701 -4.1187325 -4.1270943][-4.2024546 -4.1934071 -4.1873965 -4.181118 -4.1549773 -4.1003766 -4.0431976 -4.0218453 -4.0505977 -4.1072063 -4.1526685 -4.1661725 -4.1576467 -4.1447887 -4.1513772][-4.2218623 -4.2116957 -4.211997 -4.2201452 -4.2165737 -4.1903124 -4.1562004 -4.1393929 -4.1472774 -4.1748762 -4.2017989 -4.2097874 -4.2048225 -4.195673 -4.2010965][-4.2304945 -4.219038 -4.2239418 -4.2422609 -4.2534733 -4.24576 -4.2274904 -4.2164421 -4.2159686 -4.2245836 -4.2364545 -4.2422357 -4.2453551 -4.2442217 -4.2490611][-4.2209873 -4.2097526 -4.2167335 -4.2396145 -4.2596407 -4.2640724 -4.2577662 -4.2521529 -4.2478595 -4.2459435 -4.2487893 -4.2553029 -4.2656088 -4.2701397 -4.2727532][-4.1969066 -4.1911144 -4.2015686 -4.2272005 -4.2517843 -4.2632136 -4.2637844 -4.2588181 -4.2511525 -4.2455263 -4.2474976 -4.2568517 -4.270421 -4.27662 -4.2754431][-4.1646376 -4.1646986 -4.1796274 -4.2066669 -4.2335362 -4.2481685 -4.2516551 -4.2475486 -4.2396235 -4.2357764 -4.2396631 -4.2503457 -4.2640781 -4.2708187 -4.2693596]]...]
INFO - root - 2017-12-05 13:33:14.883471: step 15010, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 83h:47m:30s remains)
INFO - root - 2017-12-05 13:33:24.177307: step 15020, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 82h:17m:48s remains)
INFO - root - 2017-12-05 13:33:33.439830: step 15030, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 79h:42m:50s remains)
INFO - root - 2017-12-05 13:33:42.838355: step 15040, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.856 sec/batch; 75h:26m:28s remains)
INFO - root - 2017-12-05 13:33:52.311888: step 15050, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 82h:23m:27s remains)
INFO - root - 2017-12-05 13:34:01.708110: step 15060, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 81h:02m:49s remains)
INFO - root - 2017-12-05 13:34:11.216977: step 15070, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.967 sec/batch; 85h:14m:50s remains)
INFO - root - 2017-12-05 13:34:20.453516: step 15080, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 79h:58m:45s remains)
INFO - root - 2017-12-05 13:34:29.798463: step 15090, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 79h:52m:56s remains)
INFO - root - 2017-12-05 13:34:39.215482: step 15100, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 82h:15m:52s remains)
2017-12-05 13:34:39.959650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3239083 -4.3080254 -4.291779 -4.2763405 -4.2659268 -4.2593031 -4.2534986 -4.2470055 -4.2560549 -4.2749548 -4.2840815 -4.281682 -4.2808967 -4.2828026 -4.2828922][-4.3131027 -4.2907825 -4.2726765 -4.2537351 -4.2345648 -4.2169189 -4.2064624 -4.1982121 -4.213264 -4.2410593 -4.2583456 -4.2562456 -4.2490263 -4.248929 -4.2503166][-4.306675 -4.2812076 -4.2649097 -4.2462931 -4.21647 -4.1783061 -4.1534438 -4.1494064 -4.1774158 -4.2171693 -4.2450056 -4.2480679 -4.2352691 -4.2311277 -4.2335429][-4.2998228 -4.2738781 -4.258316 -4.2409511 -4.2058911 -4.1480565 -4.0988517 -4.0998745 -4.1477284 -4.1991577 -4.23534 -4.2448769 -4.2302866 -4.2221766 -4.2251406][-4.2845316 -4.2594051 -4.2421341 -4.2222371 -4.1836753 -4.1081314 -4.0270257 -4.0252051 -4.0974874 -4.1659 -4.2131724 -4.23355 -4.2257919 -4.2163696 -4.21798][-4.2641697 -4.2398958 -4.2234635 -4.20129 -4.1557775 -4.0669985 -3.9613457 -3.9438756 -4.0325322 -4.123281 -4.184402 -4.2140856 -4.2170978 -4.2134933 -4.2135029][-4.2449164 -4.2229891 -4.2074008 -4.1794367 -4.1233983 -4.0212831 -3.8983862 -3.8608885 -3.9558218 -4.0697861 -4.1498389 -4.1874514 -4.2006903 -4.2076902 -4.2090907][-4.2287035 -4.2119751 -4.1980948 -4.1660547 -4.0999866 -3.9884148 -3.8603725 -3.8165555 -3.9027674 -4.0200405 -4.1076541 -4.1535206 -4.1815405 -4.1989846 -4.2055798][-4.218472 -4.2054224 -4.1959224 -4.1667156 -4.1101589 -4.0140557 -3.9106121 -3.8782542 -3.9349046 -4.0182481 -4.0886745 -4.1336522 -4.1683774 -4.1893768 -4.1989269][-4.2195139 -4.21007 -4.20347 -4.18067 -4.1383004 -4.0727849 -4.0089741 -3.9989474 -4.030992 -4.072185 -4.1155415 -4.14591 -4.17496 -4.1931982 -4.2003512][-4.2259135 -4.2234044 -4.2217441 -4.2054343 -4.1724782 -4.1276841 -4.0915942 -4.1012011 -4.1209936 -4.14371 -4.170372 -4.184967 -4.19936 -4.2110476 -4.2152424][-4.2407322 -4.2440395 -4.2475643 -4.2410927 -4.2182007 -4.1823316 -4.1555748 -4.1690316 -4.1880918 -4.2062588 -4.2250733 -4.2314749 -4.2345834 -4.2396483 -4.2405996][-4.2602506 -4.269053 -4.2746153 -4.2760639 -4.259778 -4.2267084 -4.2055864 -4.2226443 -4.2414756 -4.253634 -4.2653885 -4.2693381 -4.2723656 -4.2750654 -4.2732239][-4.2749896 -4.2875304 -4.2979155 -4.3029122 -4.2919607 -4.266274 -4.2534423 -4.2702889 -4.28278 -4.2869749 -4.2925196 -4.2985377 -4.3038883 -4.3069563 -4.3036528][-4.2852588 -4.2972913 -4.3084006 -4.3147216 -4.3114271 -4.300282 -4.2986841 -4.3115191 -4.3147197 -4.3135276 -4.3157897 -4.3215961 -4.3252611 -4.3263474 -4.322947]]...]
INFO - root - 2017-12-05 13:34:49.231556: step 15110, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 76h:03m:13s remains)
INFO - root - 2017-12-05 13:34:58.579753: step 15120, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.962 sec/batch; 84h:48m:15s remains)
INFO - root - 2017-12-05 13:35:07.983334: step 15130, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 85h:58m:59s remains)
INFO - root - 2017-12-05 13:35:17.389123: step 15140, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 83h:23m:54s remains)
INFO - root - 2017-12-05 13:35:26.858055: step 15150, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 81h:43m:33s remains)
INFO - root - 2017-12-05 13:35:36.307941: step 15160, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 82h:37m:03s remains)
INFO - root - 2017-12-05 13:35:45.633221: step 15170, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.825 sec/batch; 72h:42m:36s remains)
INFO - root - 2017-12-05 13:35:55.071368: step 15180, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.908 sec/batch; 80h:00m:25s remains)
INFO - root - 2017-12-05 13:36:04.373308: step 15190, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 86h:45m:10s remains)
INFO - root - 2017-12-05 13:36:13.879818: step 15200, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.938 sec/batch; 82h:39m:49s remains)
2017-12-05 13:36:14.617097: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2415619 -4.2479057 -4.244123 -4.2331452 -4.2282004 -4.2330418 -4.2372189 -4.2396469 -4.2282834 -4.2281971 -4.2427421 -4.26354 -4.2830219 -4.2988453 -4.3001809][-4.249764 -4.2607889 -4.2589889 -4.246264 -4.2360978 -4.2342653 -4.2337613 -4.2343163 -4.2275252 -4.2319789 -4.251802 -4.2714429 -4.2853355 -4.2955856 -4.2972183][-4.245122 -4.2622766 -4.2663851 -4.2556138 -4.2418203 -4.2326083 -4.2238722 -4.2198539 -4.2137933 -4.2184997 -4.2379637 -4.2561221 -4.2669215 -4.2798343 -4.2878494][-4.2233143 -4.24414 -4.2573576 -4.252841 -4.2384286 -4.2230062 -4.2064638 -4.1947775 -4.1849694 -4.1855769 -4.2044988 -4.2222257 -4.2344289 -4.2553663 -4.2729287][-4.1979485 -4.2168975 -4.2357345 -4.2369666 -4.2264891 -4.2081528 -4.1849971 -4.1614714 -4.14082 -4.1318398 -4.1476507 -4.1675715 -4.19242 -4.2285023 -4.2599049][-4.172214 -4.1863804 -4.2061858 -4.2110481 -4.2030516 -4.1832552 -4.1549349 -4.1193504 -4.0897098 -4.0732431 -4.0835748 -4.1117444 -4.1603665 -4.2154531 -4.2577038][-4.1447697 -4.156229 -4.1783786 -4.1902933 -4.1846104 -4.1675062 -4.1335549 -4.0853782 -4.0463309 -4.0262132 -4.0399704 -4.0884981 -4.1590548 -4.2248044 -4.2677355][-4.123014 -4.1323366 -4.156178 -4.17303 -4.1723142 -4.1604939 -4.1237416 -4.0692797 -4.0215025 -3.9998279 -4.0244107 -4.0934315 -4.1755962 -4.2428641 -4.2807903][-4.1152353 -4.11759 -4.1399951 -4.1583509 -4.1612291 -4.1486154 -4.1117516 -4.058888 -4.0119309 -4.0017424 -4.0424294 -4.1216965 -4.2012591 -4.2599411 -4.2906351][-4.1292782 -4.1215224 -4.1417108 -4.1576853 -4.1577673 -4.1412878 -4.1085567 -4.0687861 -4.0413852 -4.0569973 -4.1052494 -4.1745753 -4.2348623 -4.2774415 -4.2990112][-4.1557221 -4.143477 -4.1635628 -4.1739373 -4.1694193 -4.1518817 -4.12342 -4.1010084 -4.0990524 -4.130353 -4.1747847 -4.2246957 -4.2639813 -4.2930884 -4.3070359][-4.1911526 -4.1794491 -4.1942105 -4.1959438 -4.1873789 -4.1723933 -4.1535707 -4.1477046 -4.1582203 -4.1920614 -4.2294803 -4.2604237 -4.2874427 -4.3103127 -4.3186407][-4.2398558 -4.2298164 -4.2356095 -4.2270441 -4.2170343 -4.207294 -4.1980257 -4.2011652 -4.2136388 -4.2436013 -4.2727742 -4.2916708 -4.3115234 -4.3281608 -4.3299413][-4.2828183 -4.2723851 -4.2697096 -4.2562432 -4.245645 -4.2412086 -4.2405519 -4.2470932 -4.2598176 -4.2834311 -4.3065443 -4.3203664 -4.3330011 -4.3413439 -4.3368688][-4.3073826 -4.3011885 -4.2954822 -4.2802362 -4.2676229 -4.2623763 -4.2641468 -4.2726479 -4.287744 -4.3097858 -4.3302975 -4.3415685 -4.3476715 -4.3479633 -4.3389745]]...]
INFO - root - 2017-12-05 13:36:24.023923: step 15210, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 81h:21m:02s remains)
INFO - root - 2017-12-05 13:36:33.384360: step 15220, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 80h:38m:45s remains)
INFO - root - 2017-12-05 13:36:42.881822: step 15230, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 85h:25m:42s remains)
INFO - root - 2017-12-05 13:36:52.231756: step 15240, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 81h:21m:16s remains)
INFO - root - 2017-12-05 13:37:01.674342: step 15250, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 81h:42m:56s remains)
INFO - root - 2017-12-05 13:37:10.935258: step 15260, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 81h:53m:37s remains)
INFO - root - 2017-12-05 13:37:20.218282: step 15270, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 81h:14m:30s remains)
INFO - root - 2017-12-05 13:37:29.715594: step 15280, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 83h:26m:11s remains)
INFO - root - 2017-12-05 13:37:38.987934: step 15290, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 83h:14m:18s remains)
INFO - root - 2017-12-05 13:37:48.369228: step 15300, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 85h:13m:57s remains)
2017-12-05 13:37:49.107363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2143092 -4.2157669 -4.2346416 -4.254168 -4.2613311 -4.241693 -4.2000675 -4.1645689 -4.1502423 -4.1372123 -4.1042771 -4.0858884 -4.1142197 -4.1513987 -4.1542277][-4.2067366 -4.2198119 -4.2367072 -4.2423129 -4.2387562 -4.2088737 -4.1656289 -4.1388516 -4.1334014 -4.12709 -4.1024466 -4.09864 -4.1417971 -4.1940241 -4.2080016][-4.2034163 -4.2190089 -4.2289948 -4.2206368 -4.2131071 -4.1842418 -4.1503506 -4.1327715 -4.1338353 -4.1281881 -4.1060514 -4.1039944 -4.1447549 -4.199182 -4.2244725][-4.1919608 -4.202311 -4.207325 -4.2010164 -4.2005253 -4.1811886 -4.1565146 -4.1472077 -4.1491418 -4.1404634 -4.1189046 -4.1119614 -4.1429887 -4.1904144 -4.2234983][-4.1619358 -4.1668034 -4.1688762 -4.1709228 -4.1808834 -4.1672316 -4.1472149 -4.1468534 -4.1553755 -4.1536193 -4.13386 -4.1232228 -4.1433449 -4.1840606 -4.2257261][-4.1182027 -4.1198149 -4.1194324 -4.1297503 -4.1424828 -4.1269627 -4.1058745 -4.1102228 -4.130836 -4.143744 -4.1370764 -4.1323485 -4.1465726 -4.183713 -4.2294135][-4.0890951 -4.0880723 -4.0810027 -4.0906687 -4.0935721 -4.0668716 -4.0379677 -4.0454397 -4.0816607 -4.1209917 -4.1419435 -4.1466374 -4.1556334 -4.18499 -4.2260485][-4.0764666 -4.0690351 -4.0502939 -4.0502348 -4.0445204 -4.0127926 -3.9765925 -3.9819396 -4.0260577 -4.0876579 -4.1359558 -4.1535397 -4.1542234 -4.1718092 -4.2067308][-4.0720057 -4.0528774 -4.019352 -4.0104957 -4.0007253 -3.9724233 -3.9432542 -3.9515829 -4.0000958 -4.0697761 -4.1294861 -4.1528668 -4.14634 -4.153687 -4.1820173][-4.08128 -4.0547671 -4.0119328 -3.9979689 -3.9896786 -3.968915 -3.9551835 -3.9732046 -4.0237861 -4.0882721 -4.1369424 -4.1492386 -4.1355519 -4.1354618 -4.157692][-4.1257024 -4.1005735 -4.0617266 -4.0503912 -4.0455837 -4.031383 -4.0280633 -4.0497255 -4.0879822 -4.1332126 -4.1573505 -4.14925 -4.1264172 -4.121738 -4.1377912][-4.1819582 -4.1676536 -4.145628 -4.1454821 -4.1462331 -4.1377964 -4.1379642 -4.1530824 -4.1719642 -4.191144 -4.1934137 -4.1669641 -4.1386924 -4.1348391 -4.1469064][-4.2191715 -4.2177367 -4.2116346 -4.219553 -4.2270856 -4.2254744 -4.2261105 -4.2349281 -4.2405229 -4.2421756 -4.2337804 -4.2050023 -4.1808276 -4.1798186 -4.190999][-4.2262449 -4.2318392 -4.236022 -4.248435 -4.2600317 -4.2632995 -4.2658005 -4.2740631 -4.2780266 -4.2767954 -4.2679987 -4.2450371 -4.229526 -4.2314639 -4.2404108][-4.2172318 -4.2264576 -4.2362804 -4.2515774 -4.2645907 -4.2719 -4.2780428 -4.2867146 -4.2932963 -4.2927961 -4.2846656 -4.2696795 -4.2623634 -4.2648864 -4.2712855]]...]
INFO - root - 2017-12-05 13:37:58.408492: step 15310, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 77h:01m:41s remains)
INFO - root - 2017-12-05 13:38:07.645545: step 15320, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 83h:06m:45s remains)
INFO - root - 2017-12-05 13:38:16.964124: step 15330, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.955 sec/batch; 84h:07m:48s remains)
INFO - root - 2017-12-05 13:38:26.440092: step 15340, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.993 sec/batch; 87h:30m:37s remains)
INFO - root - 2017-12-05 13:38:35.761947: step 15350, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.016 sec/batch; 89h:28m:27s remains)
INFO - root - 2017-12-05 13:38:45.060173: step 15360, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 80h:49m:32s remains)
INFO - root - 2017-12-05 13:38:54.334420: step 15370, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 76h:42m:26s remains)
INFO - root - 2017-12-05 13:39:03.615037: step 15380, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 78h:51m:43s remains)
INFO - root - 2017-12-05 13:39:12.930702: step 15390, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 80h:33m:26s remains)
INFO - root - 2017-12-05 13:39:22.304725: step 15400, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 82h:48m:27s remains)
2017-12-05 13:39:23.045600: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3104148 -4.3114057 -4.3112612 -4.31114 -4.3096466 -4.3058219 -4.3004074 -4.2964134 -4.29566 -4.2978959 -4.3018165 -4.3051753 -4.3074136 -4.3080173 -4.3071432][-4.2956786 -4.2959018 -4.2966938 -4.2976522 -4.295258 -4.2886734 -4.28024 -4.2749419 -4.2748575 -4.2785659 -4.2838569 -4.2881947 -4.2921572 -4.2947721 -4.295321][-4.2725849 -4.2736154 -4.2755337 -4.2767038 -4.2720523 -4.2600546 -4.24592 -4.2369075 -4.2366047 -4.2424059 -4.2518449 -4.2601724 -4.2669969 -4.272357 -4.2756829][-4.2362103 -4.2372441 -4.2392912 -4.2394161 -4.2314425 -4.2108283 -4.1866837 -4.1710939 -4.1708145 -4.182725 -4.2009635 -4.2175169 -4.2293019 -4.2382822 -4.2445769][-4.1956272 -4.1934633 -4.1923213 -4.1894827 -4.1751809 -4.1429954 -4.1053486 -4.0821295 -4.086503 -4.1133723 -4.1452007 -4.16952 -4.1863732 -4.1996317 -4.2086468][-4.1649332 -4.1548066 -4.1450725 -4.1352916 -4.1120148 -4.06504 -4.01072 -3.9836915 -4.0038877 -4.0523324 -4.0984025 -4.1300697 -4.1513224 -4.1670713 -4.1774864][-4.1474991 -4.1250672 -4.1032534 -4.0849948 -4.053009 -3.9922986 -3.9246984 -3.9037347 -3.9488037 -4.017983 -4.0732069 -4.1076932 -4.1295648 -4.1459889 -4.156641][-4.1445255 -4.1120996 -4.0818787 -4.062345 -4.0335064 -3.9751151 -3.9095039 -3.8981187 -3.9542816 -4.0263257 -4.0795517 -4.1086226 -4.1253781 -4.1397481 -4.1482215][-4.1572475 -4.1234007 -4.0938258 -4.0827065 -4.07174 -4.0395904 -4.0021386 -3.9986861 -4.0378246 -4.0840077 -4.1161566 -4.1295533 -4.1356516 -4.1444082 -4.1504192][-4.1736951 -4.145484 -4.1245518 -4.1265163 -4.1335039 -4.12475 -4.1100769 -4.1097603 -4.129159 -4.1473141 -4.156023 -4.15455 -4.1495204 -4.1491013 -4.15064][-4.1944594 -4.1749244 -4.1641817 -4.1762767 -4.1921287 -4.1937842 -4.1906648 -4.1904025 -4.194665 -4.1962948 -4.1926112 -4.1822677 -4.169282 -4.1612787 -4.1610484][-4.2084517 -4.2020922 -4.1992488 -4.2135682 -4.2301455 -4.2357283 -4.2387996 -4.2400703 -4.2370987 -4.2321935 -4.2227364 -4.208982 -4.1937132 -4.1832938 -4.1820459][-4.2175865 -4.2178545 -4.2151027 -4.2231507 -4.2368765 -4.2462063 -4.2543221 -4.2574277 -4.2554016 -4.2490172 -4.2376361 -4.2251639 -4.212523 -4.2028451 -4.1998339][-4.221312 -4.2218013 -4.2125921 -4.211112 -4.2232275 -4.2366586 -4.2490067 -4.2561741 -4.2569971 -4.2483954 -4.2382622 -4.2322984 -4.2276268 -4.2218428 -4.2183747][-4.2121406 -4.2139115 -4.2009387 -4.1927404 -4.2037168 -4.2202148 -4.2352514 -4.2451491 -4.2472086 -4.2389908 -4.2313023 -4.2322054 -4.2387471 -4.2392693 -4.2354164]]...]
INFO - root - 2017-12-05 13:39:32.356311: step 15410, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.902 sec/batch; 79h:26m:41s remains)
INFO - root - 2017-12-05 13:39:41.639057: step 15420, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 81h:21m:04s remains)
INFO - root - 2017-12-05 13:39:51.001818: step 15430, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 83h:28m:30s remains)
INFO - root - 2017-12-05 13:40:00.284310: step 15440, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 81h:10m:51s remains)
INFO - root - 2017-12-05 13:40:09.662846: step 15450, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 81h:19m:29s remains)
INFO - root - 2017-12-05 13:40:18.777570: step 15460, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 76h:31m:10s remains)
INFO - root - 2017-12-05 13:40:28.214559: step 15470, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.016 sec/batch; 89h:30m:03s remains)
INFO - root - 2017-12-05 13:40:37.744874: step 15480, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 81h:31m:38s remains)
INFO - root - 2017-12-05 13:40:47.041305: step 15490, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.918 sec/batch; 80h:49m:08s remains)
INFO - root - 2017-12-05 13:40:56.311820: step 15500, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 84h:19m:18s remains)
2017-12-05 13:40:57.062031: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1273184 -4.1294255 -4.1343956 -4.1345987 -4.1507845 -4.1628766 -4.1723595 -4.2021708 -4.2168651 -4.22129 -4.240376 -4.2443504 -4.2364955 -4.2284985 -4.2188921][-4.0995836 -4.1027164 -4.1190624 -4.1398196 -4.1752844 -4.20093 -4.2178411 -4.2455435 -4.2532887 -4.2416911 -4.2481489 -4.2520761 -4.24195 -4.2290158 -4.2189789][-4.1085987 -4.1214175 -4.1521959 -4.1889076 -4.2292633 -4.2568674 -4.267972 -4.2846937 -4.2814794 -4.2572193 -4.2533603 -4.2580171 -4.2493858 -4.2336149 -4.22572][-4.162879 -4.1886568 -4.2239947 -4.2583785 -4.2840714 -4.2976689 -4.2917871 -4.2827077 -4.2645411 -4.2394905 -4.235137 -4.2428536 -4.237133 -4.2221751 -4.2178473][-4.23737 -4.2618904 -4.2866321 -4.3031573 -4.3044376 -4.2926407 -4.2574549 -4.212883 -4.1847491 -4.1800327 -4.1871219 -4.2024035 -4.2056217 -4.1986361 -4.2026572][-4.2888403 -4.2983923 -4.3031864 -4.304975 -4.2855306 -4.243319 -4.1673775 -4.0765777 -4.0456872 -4.0756779 -4.1049719 -4.137898 -4.1667376 -4.1779356 -4.1999598][-4.3175859 -4.3133168 -4.3013005 -4.2829957 -4.2416048 -4.1663013 -4.0434475 -3.9016433 -3.8752766 -3.9593437 -4.02316 -4.0794106 -4.1341472 -4.1698036 -4.214644][-4.329958 -4.3161235 -4.2894158 -4.2476768 -4.1835103 -4.0922713 -3.963444 -3.8267999 -3.8217878 -3.9347816 -4.0153952 -4.0863829 -4.1541829 -4.2016492 -4.2500229][-4.3338976 -4.3118448 -4.2763133 -4.2228265 -4.155654 -4.0809383 -3.9952209 -3.918365 -3.9320621 -4.0156283 -4.0807638 -4.151546 -4.2188435 -4.2643914 -4.3018517][-4.3328176 -4.3028154 -4.2663584 -4.2218027 -4.1707368 -4.1218963 -4.0796738 -4.0542107 -4.0844631 -4.1390305 -4.18149 -4.235002 -4.2868171 -4.3186293 -4.3368397][-4.324903 -4.2875276 -4.2470617 -4.2095652 -4.1750712 -4.1495533 -4.1408119 -4.1532178 -4.1977692 -4.235569 -4.254931 -4.2805705 -4.3110409 -4.3294168 -4.3378882][-4.3017521 -4.2560534 -4.2045641 -4.160172 -4.1362848 -4.1358743 -4.1559391 -4.1966424 -4.248529 -4.2769227 -4.2847819 -4.2913008 -4.30367 -4.311749 -4.312767][-4.2630658 -4.208148 -4.1435118 -4.0878377 -4.0733886 -4.1030178 -4.1525722 -4.2142549 -4.2681646 -4.2880688 -4.2852111 -4.2788806 -4.275136 -4.2742682 -4.2713246][-4.2202287 -4.1618681 -4.0981383 -4.0452957 -4.0432463 -4.0952311 -4.158123 -4.2168016 -4.2560959 -4.2643714 -4.2529778 -4.2376089 -4.2189 -4.2117505 -4.2137666][-4.2082086 -4.1647825 -4.1189489 -4.0793037 -4.0842385 -4.1357956 -4.19144 -4.2281585 -4.2414274 -4.2342234 -4.2149687 -4.190814 -4.1633782 -4.1566224 -4.1708422]]...]
INFO - root - 2017-12-05 13:41:06.402339: step 15510, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.938 sec/batch; 82h:34m:10s remains)
INFO - root - 2017-12-05 13:41:15.651417: step 15520, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 80h:46m:17s remains)
INFO - root - 2017-12-05 13:41:24.880291: step 15530, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 80h:13m:41s remains)
INFO - root - 2017-12-05 13:41:34.162757: step 15540, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 86h:30m:25s remains)
INFO - root - 2017-12-05 13:41:43.449617: step 15550, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 80h:11m:19s remains)
INFO - root - 2017-12-05 13:41:52.946004: step 15560, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 83h:14m:57s remains)
INFO - root - 2017-12-05 13:42:02.339470: step 15570, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 84h:15m:47s remains)
INFO - root - 2017-12-05 13:42:11.688507: step 15580, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.927 sec/batch; 81h:37m:15s remains)
INFO - root - 2017-12-05 13:42:21.125771: step 15590, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 82h:37m:53s remains)
INFO - root - 2017-12-05 13:42:30.388186: step 15600, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 79h:11m:17s remains)
2017-12-05 13:42:31.107475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.180954 -4.2127008 -4.242857 -4.2558641 -4.2419996 -4.2109642 -4.18649 -4.1793003 -4.1741862 -4.1673455 -4.1749496 -4.1921592 -4.2093134 -4.2241406 -4.2260103][-4.18389 -4.2083697 -4.2334347 -4.2402468 -4.222105 -4.1859784 -4.1580787 -4.1554737 -4.1639295 -4.1754131 -4.1947079 -4.2150745 -4.2287092 -4.2394481 -4.2394261][-4.1968021 -4.2039165 -4.2104049 -4.2021317 -4.1736717 -4.1348143 -4.1151671 -4.1315508 -4.1654015 -4.1953964 -4.2195258 -4.2352257 -4.24399 -4.2537169 -4.2532721][-4.2053 -4.1943941 -4.1776581 -4.1479659 -4.1088753 -4.0722942 -4.0734963 -4.1162605 -4.1734142 -4.215333 -4.2373738 -4.2457056 -4.2486095 -4.2528973 -4.2486982][-4.2034688 -4.1821084 -4.14278 -4.0917878 -4.0453467 -4.0207891 -4.0493402 -4.1145077 -4.1813478 -4.2280016 -4.2500477 -4.2504873 -4.2413878 -4.2319412 -4.2223034][-4.1860843 -4.1556029 -4.1016593 -4.0417747 -4.000133 -3.9935136 -4.0412817 -4.1132908 -4.1824007 -4.2325931 -4.2573004 -4.2500896 -4.2257347 -4.2023411 -4.189014][-4.1524358 -4.1110058 -4.0555334 -4.0026026 -3.9792943 -3.9892516 -4.0359974 -4.1007438 -4.1686029 -4.2235284 -4.2490153 -4.2332959 -4.1946316 -4.1614375 -4.1484995][-4.1145277 -4.067596 -4.02036 -3.9862325 -3.9834552 -3.9995279 -4.0329938 -4.0825353 -4.1457591 -4.20067 -4.2166872 -4.1886806 -4.1409364 -4.1064072 -4.0990362][-4.1022005 -4.0577488 -4.0235972 -4.0094748 -4.0162845 -4.0325437 -4.0556068 -4.089303 -4.1386142 -4.1839409 -4.1839371 -4.1398783 -4.0853868 -4.0525346 -4.0534558][-4.125164 -4.0881929 -4.0686469 -4.0657034 -4.0726194 -4.0853462 -4.1011434 -4.1211061 -4.1525908 -4.1813712 -4.1657786 -4.1142168 -4.061049 -4.0277095 -4.0255284][-4.1678743 -4.1431794 -4.1348171 -4.1347857 -4.1381893 -4.1451049 -4.1518955 -4.1615882 -4.1769176 -4.1873231 -4.1633015 -4.116642 -4.0728493 -4.037189 -4.0212297][-4.2106271 -4.20279 -4.2026076 -4.203548 -4.20436 -4.2055607 -4.2057052 -4.2079158 -4.2106271 -4.2079206 -4.1854091 -4.1508617 -4.1200604 -4.0871606 -4.0620775][-4.2558632 -4.25385 -4.2523255 -4.247457 -4.2435927 -4.2460461 -4.2480793 -4.2472734 -4.2443976 -4.2375646 -4.2215595 -4.1983008 -4.1769233 -4.1520042 -4.1268511][-4.2835116 -4.28068 -4.276329 -4.2685294 -4.2647285 -4.2712035 -4.2760191 -4.2745914 -4.269238 -4.2610354 -4.24829 -4.229269 -4.2120762 -4.1932864 -4.1736803][-4.283206 -4.2788396 -4.2766814 -4.2741656 -4.2752171 -4.2839451 -4.2883954 -4.2847261 -4.2763987 -4.2687111 -4.2586889 -4.2429686 -4.2263241 -4.2086353 -4.1911435]]...]
INFO - root - 2017-12-05 13:42:40.460348: step 15610, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 80h:44m:44s remains)
INFO - root - 2017-12-05 13:42:49.813074: step 15620, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 79h:08m:28s remains)
INFO - root - 2017-12-05 13:42:58.972766: step 15630, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 82h:46m:29s remains)
INFO - root - 2017-12-05 13:43:08.305120: step 15640, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 78h:42m:39s remains)
INFO - root - 2017-12-05 13:43:17.525374: step 15650, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 83h:10m:26s remains)
INFO - root - 2017-12-05 13:43:27.031394: step 15660, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 86h:00m:23s remains)
INFO - root - 2017-12-05 13:43:36.374064: step 15670, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 83h:28m:47s remains)
INFO - root - 2017-12-05 13:43:45.723211: step 15680, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 82h:34m:11s remains)
INFO - root - 2017-12-05 13:43:55.091348: step 15690, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 81h:09m:32s remains)
INFO - root - 2017-12-05 13:44:04.277118: step 15700, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 85h:09m:35s remains)
2017-12-05 13:44:05.047197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3355126 -4.3330364 -4.3315887 -4.32858 -4.3250747 -4.3191323 -4.3091826 -4.2965264 -4.288753 -4.2859535 -4.2859945 -4.2883697 -4.2923584 -4.2984204 -4.3081875][-4.3330917 -4.3312397 -4.3289275 -4.3206468 -4.309329 -4.295207 -4.2796297 -4.2624879 -4.2547164 -4.2533927 -4.2531805 -4.25614 -4.2626848 -4.2721205 -4.2866516][-4.3262167 -4.3208556 -4.3141961 -4.299017 -4.2779489 -4.2545528 -4.2353477 -4.2145672 -4.2103338 -4.2145605 -4.2144775 -4.216485 -4.2246251 -4.237205 -4.2569981][-4.31961 -4.3057485 -4.2897778 -4.2670879 -4.2373247 -4.20277 -4.1704974 -4.1378727 -4.1405234 -4.1633196 -4.1751218 -4.1828938 -4.1942639 -4.2055635 -4.2256446][-4.3094749 -4.2891617 -4.26423 -4.2336535 -4.1905622 -4.1366692 -4.0727954 -4.0100131 -4.0236382 -4.0829268 -4.1257248 -4.15092 -4.1704516 -4.1822591 -4.1989117][-4.2907867 -4.26296 -4.2303543 -4.1917076 -4.1320224 -4.0486016 -3.9347968 -3.8202181 -3.8448172 -3.9550581 -4.040843 -4.0924945 -4.1291733 -4.1513066 -4.1697364][-4.263855 -4.2271743 -4.1867905 -4.140542 -4.0697627 -3.965606 -3.8094606 -3.6400962 -3.6636081 -3.8209491 -3.947757 -4.0253911 -4.0808706 -4.1185741 -4.1451459][-4.236383 -4.1910763 -4.1487832 -4.1062508 -4.0506535 -3.9657469 -3.8239732 -3.6613283 -3.6634488 -3.8005233 -3.917387 -3.9958901 -4.060039 -4.1084533 -4.1428447][-4.2134542 -4.1611333 -4.1186042 -4.0872846 -4.0638709 -4.025001 -3.9412324 -3.8357451 -3.8209326 -3.8955736 -3.9680369 -4.021904 -4.0770025 -4.1263456 -4.163146][-4.1983671 -4.1434751 -4.1015759 -4.0849519 -4.0896583 -4.0883684 -4.0538254 -3.9978356 -3.9836764 -4.0175104 -4.0503421 -4.0766983 -4.1138792 -4.1563225 -4.1928048][-4.1959891 -4.1428318 -4.104301 -4.0983248 -4.1171432 -4.1351695 -4.1309905 -4.105876 -4.0967269 -4.1117849 -4.1263123 -4.1392651 -4.1627688 -4.194428 -4.2276592][-4.2184463 -4.1757126 -4.1491923 -4.1496658 -4.1681418 -4.1849051 -4.1902957 -4.1816378 -4.1779957 -4.1877456 -4.1992226 -4.2101545 -4.2252793 -4.2461157 -4.2714314][-4.2593007 -4.2311368 -4.2176757 -4.2235928 -4.2367377 -4.2475967 -4.2508092 -4.2467122 -4.244669 -4.2497091 -4.2579861 -4.2675552 -4.2788858 -4.2924633 -4.309988][-4.2921624 -4.2752347 -4.2682109 -4.2760458 -4.2870665 -4.2951927 -4.2972913 -4.296567 -4.2947574 -4.2965198 -4.3002968 -4.3060551 -4.3152575 -4.325151 -4.3356819][-4.3141146 -4.306715 -4.3039351 -4.310205 -4.3157029 -4.3189926 -4.3201914 -4.3211265 -4.320467 -4.3211789 -4.3234386 -4.3273659 -4.3334417 -4.3400416 -4.3458614]]...]
INFO - root - 2017-12-05 13:44:14.222151: step 15710, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 81h:26m:39s remains)
INFO - root - 2017-12-05 13:44:23.364382: step 15720, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 74h:11m:55s remains)
INFO - root - 2017-12-05 13:44:32.694512: step 15730, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 80h:53m:12s remains)
INFO - root - 2017-12-05 13:44:42.046811: step 15740, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 78h:29m:18s remains)
INFO - root - 2017-12-05 13:44:51.392446: step 15750, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 77h:48m:03s remains)
INFO - root - 2017-12-05 13:45:00.804299: step 15760, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 79h:00m:47s remains)
INFO - root - 2017-12-05 13:45:10.241713: step 15770, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 80h:19m:00s remains)
INFO - root - 2017-12-05 13:45:19.533958: step 15780, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 80h:00m:33s remains)
INFO - root - 2017-12-05 13:45:28.778416: step 15790, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.914 sec/batch; 80h:26m:08s remains)
INFO - root - 2017-12-05 13:45:38.082717: step 15800, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 83h:03m:25s remains)
2017-12-05 13:45:38.917137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1334677 -4.1383896 -4.1510205 -4.1787982 -4.2123332 -4.2422614 -4.2640214 -4.2717752 -4.2619762 -4.2389145 -4.2055182 -4.1683373 -4.1459551 -4.1524129 -4.1748352][-4.1518092 -4.1516805 -4.1593904 -4.1841378 -4.2200851 -4.2559218 -4.28024 -4.2827106 -4.2631817 -4.2346034 -4.2007351 -4.1680121 -4.154356 -4.1692014 -4.1969795][-4.170927 -4.1615515 -4.1631007 -4.1845584 -4.2187366 -4.2487884 -4.26289 -4.2549891 -4.2303867 -4.204391 -4.179378 -4.1608162 -4.1589131 -4.1806955 -4.210875][-4.194623 -4.183907 -4.1836572 -4.199265 -4.2187657 -4.2282577 -4.2251015 -4.2096648 -4.188251 -4.1727729 -4.1649237 -4.1664748 -4.1764669 -4.1999927 -4.2239771][-4.22143 -4.2154446 -4.2183156 -4.2224164 -4.2177792 -4.2021427 -4.1802125 -4.1588845 -4.140378 -4.1376877 -4.1532593 -4.1744676 -4.1931629 -4.2123995 -4.2232537][-4.2359009 -4.2353821 -4.2386312 -4.2312808 -4.2041869 -4.1650076 -4.1229978 -4.0887 -4.0755186 -4.0981789 -4.1422458 -4.1808319 -4.2061019 -4.219646 -4.2174644][-4.2186694 -4.2200942 -4.2213531 -4.2065377 -4.1664863 -4.1113935 -4.0508342 -4.0034285 -4.0057817 -4.062809 -4.1331472 -4.1866503 -4.2181506 -4.2273903 -4.2148][-4.1860027 -4.1847668 -4.1824679 -4.1647487 -4.120779 -4.0602956 -3.9938028 -3.9507051 -3.9807942 -4.0646577 -4.1492009 -4.2074761 -4.2359142 -4.2351432 -4.2113919][-4.1554003 -4.1426582 -4.1324835 -4.1130781 -4.0726161 -4.0229158 -3.978735 -3.9660881 -4.0150766 -4.0976658 -4.1725922 -4.2194209 -4.2367 -4.2278776 -4.2007914][-4.1381869 -4.1081805 -4.084929 -4.0625052 -4.031352 -4.0025153 -3.991549 -4.0074697 -4.0579252 -4.1224074 -4.1773028 -4.2089448 -4.2155914 -4.2028618 -4.1781611][-4.1410193 -4.1045275 -4.0758233 -4.0535455 -4.0308952 -4.0154777 -4.018806 -4.0408831 -4.0823822 -4.1300569 -4.1678648 -4.1879482 -4.1876035 -4.1749206 -4.1551442][-4.1514788 -4.1245561 -4.1039243 -4.0895119 -4.0774522 -4.0714884 -4.0777693 -4.0917492 -4.1165614 -4.1448236 -4.1671357 -4.1760764 -4.1678233 -4.1507115 -4.12929][-4.1641674 -4.153779 -4.1426816 -4.1312647 -4.1224227 -4.1202869 -4.12605 -4.1357374 -4.1519127 -4.1706996 -4.1823807 -4.1822934 -4.1658168 -4.1411629 -4.1137571][-4.1891932 -4.191566 -4.1819849 -4.1652932 -4.1526294 -4.148386 -4.1512637 -4.1589727 -4.17331 -4.1877422 -4.192801 -4.1856923 -4.1665516 -4.1424289 -4.117506][-4.2198353 -4.2219067 -4.2073112 -4.1873226 -4.1742759 -4.169363 -4.170125 -4.1761937 -4.1870456 -4.1939106 -4.1899457 -4.1758885 -4.157691 -4.1412992 -4.1253891]]...]
INFO - root - 2017-12-05 13:45:48.038548: step 15810, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 80h:21m:01s remains)
INFO - root - 2017-12-05 13:45:57.397492: step 15820, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 81h:07m:47s remains)
INFO - root - 2017-12-05 13:46:06.866134: step 15830, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 85h:25m:08s remains)
INFO - root - 2017-12-05 13:46:16.212175: step 15840, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 82h:23m:11s remains)
INFO - root - 2017-12-05 13:46:25.673359: step 15850, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.947 sec/batch; 83h:17m:54s remains)
INFO - root - 2017-12-05 13:46:35.027089: step 15860, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 81h:20m:08s remains)
INFO - root - 2017-12-05 13:46:44.441089: step 15870, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 82h:03m:58s remains)
INFO - root - 2017-12-05 13:46:53.789467: step 15880, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 82h:36m:11s remains)
INFO - root - 2017-12-05 13:47:03.181425: step 15890, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 83h:20m:17s remains)
INFO - root - 2017-12-05 13:47:12.180128: step 15900, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 78h:52m:24s remains)
2017-12-05 13:47:12.922229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.325613 -4.3264079 -4.3209386 -4.3138013 -4.3138504 -4.3220754 -4.3303657 -4.3325768 -4.3261089 -4.3138452 -4.2995057 -4.2892375 -4.287046 -4.2928839 -4.3045359][-4.30436 -4.305686 -4.3003116 -4.2931814 -4.2936444 -4.3058472 -4.3183484 -4.3247123 -4.3186855 -4.3007746 -4.2766557 -4.259666 -4.2559614 -4.2649651 -4.2824736][-4.2685165 -4.2681689 -4.2630935 -4.2568331 -4.2591696 -4.2745852 -4.2904153 -4.3026571 -4.2991128 -4.2792768 -4.2514086 -4.2334647 -4.2312098 -4.24354 -4.2650623][-4.2235847 -4.2158422 -4.2056713 -4.1998997 -4.2086558 -4.2300706 -4.2472816 -4.2618623 -4.2619433 -4.2478971 -4.2265315 -4.2150211 -4.2186193 -4.2362614 -4.25967][-4.1903248 -4.1707964 -4.1504984 -4.1396437 -4.1494112 -4.1687312 -4.18158 -4.1970234 -4.2091389 -4.214119 -4.2129607 -4.2148261 -4.2260356 -4.2458649 -4.2642488][-4.1808891 -4.1540642 -4.1243262 -4.1038451 -4.0999503 -4.0951109 -4.0857105 -4.0918989 -4.1253238 -4.1619158 -4.1891646 -4.2093425 -4.2289691 -4.2478561 -4.259119][-4.1852007 -4.1619072 -4.1300154 -4.0991197 -4.0701156 -4.0224338 -3.9683466 -3.9557292 -4.0185657 -4.096323 -4.1489553 -4.1806068 -4.2034478 -4.222084 -4.2324834][-4.1922483 -4.1855588 -4.1643324 -4.1275091 -4.0730796 -3.9856884 -3.8881526 -3.8511419 -3.9354372 -4.038115 -4.0980062 -4.1262441 -4.1472611 -4.1720281 -4.19504][-4.1911578 -4.2091141 -4.2060418 -4.1767449 -4.1177435 -4.0276704 -3.9390278 -3.90729 -3.9686155 -4.0371943 -4.0661359 -4.0709071 -4.082469 -4.1131415 -4.1525192][-4.1868811 -4.2208066 -4.2324986 -4.217185 -4.1729646 -4.1095815 -4.0556655 -4.0389137 -4.0647755 -4.0807533 -4.0704708 -4.0483313 -4.0487051 -4.0816226 -4.1266351][-4.1928983 -4.2289033 -4.2481303 -4.2470083 -4.2241831 -4.1913972 -4.1645951 -4.152483 -4.1536732 -4.1418495 -4.1145964 -4.084147 -4.0796494 -4.1069217 -4.1440096][-4.2148528 -4.2419443 -4.2647114 -4.2742872 -4.2692494 -4.2564292 -4.2421937 -4.2281637 -4.2179437 -4.2016969 -4.180532 -4.1605148 -4.1583996 -4.1756344 -4.1965432][-4.2475839 -4.26455 -4.286808 -4.3029075 -4.3051825 -4.3005142 -4.2927346 -4.28039 -4.270083 -4.2598963 -4.2514954 -4.2428231 -4.24409 -4.2536464 -4.2623935][-4.2844515 -4.2949777 -4.3121037 -4.3268914 -4.3314691 -4.3297663 -4.3237782 -4.3142133 -4.3088441 -4.307653 -4.3094897 -4.3095603 -4.313201 -4.3181744 -4.3187718][-4.3168097 -4.3254642 -4.3370314 -4.3462348 -4.34819 -4.3459797 -4.3406858 -4.3329778 -4.3313394 -4.3355746 -4.3420224 -4.3458958 -4.3500633 -4.3511152 -4.3463573]]...]
INFO - root - 2017-12-05 13:47:22.148837: step 15910, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 81h:40m:13s remains)
INFO - root - 2017-12-05 13:47:31.514712: step 15920, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 80h:27m:17s remains)
INFO - root - 2017-12-05 13:47:40.914341: step 15930, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 83h:00m:26s remains)
INFO - root - 2017-12-05 13:47:50.300200: step 15940, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 79h:17m:40s remains)
INFO - root - 2017-12-05 13:47:59.536497: step 15950, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 79h:12m:32s remains)
INFO - root - 2017-12-05 13:48:08.879509: step 15960, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 82h:49m:03s remains)
INFO - root - 2017-12-05 13:48:18.110564: step 15970, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 82h:08m:49s remains)
INFO - root - 2017-12-05 13:48:27.472753: step 15980, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 79h:24m:12s remains)
INFO - root - 2017-12-05 13:48:36.686753: step 15990, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 81h:02m:26s remains)
INFO - root - 2017-12-05 13:48:45.868279: step 16000, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 80h:31m:10s remains)
2017-12-05 13:48:46.596386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2296371 -4.2374287 -4.2557764 -4.2696915 -4.2710223 -4.262949 -4.2600055 -4.2689433 -4.2789268 -4.2778897 -4.2498159 -4.2242775 -4.202002 -4.1907783 -4.1893063][-4.2131162 -4.2206154 -4.2442617 -4.2684722 -4.2773495 -4.2695804 -4.2651582 -4.2720942 -4.2808065 -4.281743 -4.257359 -4.233707 -4.2206197 -4.2212138 -4.2246962][-4.19112 -4.2016869 -4.2325635 -4.2633452 -4.275351 -4.2692175 -4.2669592 -4.2747569 -4.2837105 -4.2842641 -4.2554622 -4.2280784 -4.2205219 -4.2311363 -4.2410488][-4.1932578 -4.2106333 -4.24606 -4.27769 -4.2837815 -4.2755113 -4.2663903 -4.2697282 -4.2807021 -4.2814527 -4.2548323 -4.2234755 -4.21031 -4.2183204 -4.231204][-4.2211356 -4.2401161 -4.2755761 -4.3060341 -4.3044353 -4.2827468 -4.2590308 -4.2489095 -4.2516265 -4.252862 -4.237195 -4.2137117 -4.1919904 -4.1865258 -4.1989517][-4.22805 -4.252985 -4.2892504 -4.3115687 -4.294096 -4.2456875 -4.1928535 -4.1619978 -4.1651263 -4.1814275 -4.1889682 -4.1850882 -4.1687307 -4.1527352 -4.1579881][-4.2056861 -4.2318134 -4.2682838 -4.2775288 -4.2404461 -4.1571083 -4.0622282 -4.0058761 -4.0236273 -4.0755587 -4.1125193 -4.1255412 -4.1184692 -4.1017714 -4.1025805][-4.1282067 -4.1558704 -4.2021937 -4.2095776 -4.1556435 -4.0478735 -3.9203684 -3.8375773 -3.8727946 -3.9684372 -4.0362287 -4.0643907 -4.06154 -4.0439854 -4.0443563][-3.9989829 -4.0309248 -4.0999484 -4.1237864 -4.0760241 -3.970562 -3.8452153 -3.772886 -3.8289411 -3.9382045 -4.010057 -4.0407963 -4.0376225 -4.0201721 -4.0218449][-3.8809369 -3.9128127 -4.0012159 -4.0535746 -4.03603 -3.9677229 -3.8861604 -3.8530552 -3.9122877 -3.9914923 -4.0405145 -4.0644488 -4.0621705 -4.04971 -4.0525827][-3.8950613 -3.9162951 -3.9930167 -4.0566454 -4.0696845 -4.0377374 -3.9957724 -3.9914122 -4.0416608 -4.093749 -4.1233339 -4.1397552 -4.136992 -4.125761 -4.1301546][-4.0184512 -4.0290174 -4.0692911 -4.1086612 -4.1251745 -4.1137662 -4.0973897 -4.1114845 -4.1600814 -4.2019849 -4.2199016 -4.2239766 -4.2123556 -4.2002406 -4.2064295][-4.1317534 -4.1368976 -4.1459045 -4.1510873 -4.1560712 -4.1570163 -4.1586556 -4.1850772 -4.2329555 -4.2688928 -4.27662 -4.2701435 -4.253984 -4.2402563 -4.243639][-4.1918769 -4.1912389 -4.1845512 -4.1717587 -4.1715088 -4.1839929 -4.1989346 -4.2250943 -4.2613029 -4.286118 -4.2876444 -4.2808642 -4.2685137 -4.2554049 -4.2529721][-4.2107382 -4.2066913 -4.2014513 -4.1947937 -4.20228 -4.222261 -4.239234 -4.2556953 -4.2724929 -4.2802129 -4.2772932 -4.2732906 -4.2655578 -4.2535167 -4.2454257]]...]
INFO - root - 2017-12-05 13:48:56.093554: step 16010, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 83h:01m:52s remains)
INFO - root - 2017-12-05 13:49:05.528997: step 16020, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 84h:03m:39s remains)
INFO - root - 2017-12-05 13:49:15.113024: step 16030, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 85h:12m:29s remains)
INFO - root - 2017-12-05 13:49:24.620688: step 16040, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 83h:41m:43s remains)
INFO - root - 2017-12-05 13:49:34.021663: step 16050, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 82h:08m:33s remains)
INFO - root - 2017-12-05 13:49:43.504624: step 16060, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 80h:30m:43s remains)
INFO - root - 2017-12-05 13:49:52.739492: step 16070, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 80h:58m:12s remains)
INFO - root - 2017-12-05 13:50:01.931489: step 16080, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 82h:36m:01s remains)
INFO - root - 2017-12-05 13:50:11.165758: step 16090, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 74h:02m:56s remains)
INFO - root - 2017-12-05 13:50:20.488470: step 16100, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 79h:22m:12s remains)
2017-12-05 13:50:21.217639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28323 -4.2901788 -4.2928448 -4.295578 -4.2985225 -4.3005133 -4.30214 -4.3032932 -4.303031 -4.3015838 -4.2995996 -4.2965484 -4.2956486 -4.3011355 -4.3127494][-4.2624669 -4.2673821 -4.2688508 -4.2728424 -4.2795606 -4.2848458 -4.2885823 -4.291944 -4.2939944 -4.2941918 -4.2932014 -4.2885585 -4.2845907 -4.2878542 -4.3001308][-4.2398925 -4.2382812 -4.2331729 -4.2329826 -4.2385793 -4.2426839 -4.24485 -4.2505264 -4.2584386 -4.2653346 -4.2702365 -4.267592 -4.2607484 -4.2610607 -4.2742672][-4.2041974 -4.1915011 -4.1759171 -4.1674447 -4.1671729 -4.1630039 -4.1573014 -4.1650944 -4.18412 -4.2037029 -4.2192264 -4.2230768 -4.2163339 -4.2165179 -4.23357][-4.1581807 -4.1332159 -4.10447 -4.0847731 -4.0738807 -4.05347 -4.0287504 -4.0354919 -4.07208 -4.1124587 -4.1465335 -4.1646848 -4.164957 -4.1671405 -4.1884847][-4.126164 -4.0902863 -4.0470634 -4.013485 -3.9899626 -3.9474218 -3.8910844 -3.8914988 -3.9528053 -4.0168319 -4.0692163 -4.1021938 -4.115675 -4.1252279 -4.1523747][-4.1389513 -4.1010404 -4.0507894 -4.0048389 -3.965821 -3.9019289 -3.8139129 -3.8024433 -3.8823195 -3.9629066 -4.0219378 -4.0615993 -4.0888791 -4.1067424 -4.138113][-4.179935 -4.153543 -4.1139107 -4.070466 -4.0257487 -3.9593058 -3.8711014 -3.8495109 -3.9133568 -3.9779866 -4.0233073 -4.0597854 -4.0950422 -4.118957 -4.1501532][-4.2167211 -4.2083149 -4.1882639 -4.1596942 -4.1242909 -4.0717549 -4.0081563 -3.9892738 -4.0226941 -4.0542011 -4.0750241 -4.1002789 -4.1311693 -4.1536131 -4.1802711][-4.2268672 -4.2366281 -4.2369618 -4.2273679 -4.2097692 -4.1769958 -4.1386743 -4.1248364 -4.1352139 -4.1429319 -4.1452522 -4.1565075 -4.1772532 -4.1943779 -4.2139039][-4.205843 -4.2297945 -4.2463636 -4.2565074 -4.2614264 -4.2505474 -4.2330842 -4.224947 -4.2232356 -4.2171755 -4.209362 -4.2076735 -4.21564 -4.2250986 -4.2382779][-4.1706161 -4.2013483 -4.2261076 -4.251421 -4.2765322 -4.2851472 -4.2832785 -4.2804213 -4.2749844 -4.2631941 -4.2498479 -4.2392082 -4.2370639 -4.2389655 -4.2477293][-4.1681409 -4.1926312 -4.213028 -4.2421522 -4.2780657 -4.2975521 -4.3052039 -4.3063712 -4.3021374 -4.2914457 -4.2768497 -4.2614751 -4.2524023 -4.2496753 -4.2558618][-4.2177815 -4.2312078 -4.2413092 -4.2632041 -4.2941327 -4.3123384 -4.3198047 -4.3215394 -4.3195972 -4.31356 -4.30441 -4.2927728 -4.28394 -4.279603 -4.2826638][-4.2764821 -4.2844687 -4.2891421 -4.3019052 -4.3203745 -4.3304267 -4.3328152 -4.3327513 -4.331655 -4.3290963 -4.3255844 -4.3202806 -4.3153415 -4.3118143 -4.3125196]]...]
INFO - root - 2017-12-05 13:50:30.463662: step 16110, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.994 sec/batch; 87h:23m:12s remains)
INFO - root - 2017-12-05 13:50:39.906531: step 16120, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.974 sec/batch; 85h:33m:41s remains)
INFO - root - 2017-12-05 13:50:49.157642: step 16130, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 82h:04m:12s remains)
INFO - root - 2017-12-05 13:50:58.398906: step 16140, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.960 sec/batch; 84h:22m:50s remains)
INFO - root - 2017-12-05 13:51:07.430902: step 16150, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 79h:32m:11s remains)
INFO - root - 2017-12-05 13:51:16.694521: step 16160, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 80h:34m:30s remains)
INFO - root - 2017-12-05 13:51:26.058645: step 16170, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 79h:34m:55s remains)
INFO - root - 2017-12-05 13:51:35.386355: step 16180, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 84h:37m:44s remains)
INFO - root - 2017-12-05 13:51:44.634783: step 16190, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 81h:29m:25s remains)
INFO - root - 2017-12-05 13:51:53.975516: step 16200, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 83h:30m:13s remains)
2017-12-05 13:51:54.806087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3026838 -4.2932291 -4.288877 -4.2934165 -4.2923274 -4.2846575 -4.2721319 -4.2576871 -4.253901 -4.2578392 -4.2555833 -4.2522135 -4.2487545 -4.2511306 -4.2584171][-4.2983055 -4.292213 -4.2879066 -4.2866907 -4.2779655 -4.2577062 -4.2259874 -4.2040086 -4.2023482 -4.21218 -4.21406 -4.2249007 -4.2452593 -4.2610354 -4.2712317][-4.2997327 -4.2948117 -4.2855825 -4.2737784 -4.2484789 -4.2077665 -4.1491795 -4.1139131 -4.1207852 -4.1475625 -4.162262 -4.1904893 -4.2311773 -4.2571793 -4.2693386][-4.302423 -4.2942543 -4.2788663 -4.2571979 -4.2191248 -4.1629777 -4.0859246 -4.03478 -4.0396891 -4.0767574 -4.1072626 -4.1496773 -4.202323 -4.2361779 -4.2530437][-4.293745 -4.2875166 -4.2703681 -4.242907 -4.1987276 -4.1400971 -4.064539 -4.0041008 -3.9891181 -4.0200157 -4.0630417 -4.1168861 -4.1727762 -4.2102013 -4.2301984][-4.2842093 -4.2808743 -4.2627296 -4.229825 -4.1842 -4.1308002 -4.0655446 -4.0030837 -3.9680383 -3.995892 -4.0512 -4.1100097 -4.1570921 -4.1910381 -4.213223][-4.2847333 -4.2851219 -4.2687583 -4.2359886 -4.1959906 -4.1503954 -4.0944214 -4.03444 -3.9938474 -4.0187263 -4.071682 -4.1175394 -4.146585 -4.1744642 -4.194231][-4.2869239 -4.2918444 -4.2856793 -4.2636428 -4.2318048 -4.1931133 -4.1507759 -4.1127124 -4.08273 -4.0986729 -4.1279807 -4.1458263 -4.1468511 -4.1627736 -4.1757431][-4.2889118 -4.2967324 -4.3002467 -4.2896528 -4.2685628 -4.2379742 -4.2080226 -4.1895342 -4.1765046 -4.1853671 -4.1921806 -4.1884284 -4.1717525 -4.1753044 -4.178524][-4.2913089 -4.3007984 -4.3094859 -4.3049073 -4.2935309 -4.2716489 -4.2478395 -4.24017 -4.234735 -4.2396994 -4.2398567 -4.2315488 -4.2108474 -4.206708 -4.200285][-4.2892694 -4.3000178 -4.3102531 -4.3107028 -4.30727 -4.2958832 -4.2821364 -4.2765527 -4.2693806 -4.2694407 -4.2698822 -4.2629161 -4.2436876 -4.237174 -4.2257953][-4.2777677 -4.2923908 -4.305438 -4.3112974 -4.3145752 -4.3122363 -4.3047504 -4.2976508 -4.2884912 -4.2860994 -4.2857795 -4.2789488 -4.2611179 -4.2557654 -4.2451696][-4.2685952 -4.2862787 -4.2970829 -4.3035064 -4.3093662 -4.311636 -4.3085165 -4.3031235 -4.2965627 -4.2973166 -4.2990227 -4.2959137 -4.28057 -4.27536 -4.2704406][-4.2668228 -4.2826996 -4.2891417 -4.2949033 -4.3037462 -4.3099103 -4.3112206 -4.3079467 -4.3043246 -4.3055844 -4.3074827 -4.3032718 -4.2873397 -4.2810936 -4.2830677][-4.2630615 -4.2746282 -4.2786779 -4.2824783 -4.2915373 -4.3005586 -4.3039732 -4.3037691 -4.303278 -4.3028946 -4.301496 -4.29097 -4.2702746 -4.2604303 -4.266974]]...]
INFO - root - 2017-12-05 13:52:04.121113: step 16210, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 87h:43m:41s remains)
INFO - root - 2017-12-05 13:52:13.597554: step 16220, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 83h:43m:06s remains)
INFO - root - 2017-12-05 13:52:23.142707: step 16230, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.965 sec/batch; 84h:46m:27s remains)
INFO - root - 2017-12-05 13:52:32.128570: step 16240, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 77h:18m:30s remains)
INFO - root - 2017-12-05 13:52:41.613705: step 16250, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 82h:45m:05s remains)
INFO - root - 2017-12-05 13:52:51.121567: step 16260, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 79h:13m:44s remains)
INFO - root - 2017-12-05 13:53:00.473920: step 16270, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.936 sec/batch; 82h:10m:36s remains)
INFO - root - 2017-12-05 13:53:09.736227: step 16280, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.984 sec/batch; 86h:24m:15s remains)
INFO - root - 2017-12-05 13:53:19.285503: step 16290, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 85h:59m:12s remains)
INFO - root - 2017-12-05 13:53:28.722071: step 16300, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 82h:12m:51s remains)
2017-12-05 13:53:29.501527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3143139 -4.3086777 -4.3047738 -4.300077 -4.294416 -4.2924943 -4.294981 -4.3023481 -4.3121805 -4.3171906 -4.3123608 -4.2994981 -4.2840252 -4.276351 -4.280889][-4.2944684 -4.2856746 -4.2818055 -4.2761059 -4.2670479 -4.2599964 -4.2579327 -4.2660646 -4.2814732 -4.2924385 -4.2881546 -4.2683544 -4.2444105 -4.2338114 -4.2439251][-4.2653265 -4.2566004 -4.2575488 -4.2545252 -4.2401361 -4.2217045 -4.2094793 -4.2180858 -4.2413769 -4.2600951 -4.2571163 -4.2304673 -4.19662 -4.1809764 -4.1937318][-4.2420435 -4.2387877 -4.2493873 -4.251266 -4.2297578 -4.1934881 -4.1644979 -4.1723189 -4.207788 -4.2383351 -4.2421365 -4.2145753 -4.1698647 -4.1409397 -4.1497612][-4.2284851 -4.2292647 -4.2468991 -4.2517533 -4.222218 -4.1653342 -4.1137834 -4.1187763 -4.17145 -4.2219415 -4.2394876 -4.2217784 -4.1747732 -4.1303473 -4.1266818][-4.2164598 -4.2213712 -4.2418995 -4.2441549 -4.2040663 -4.1245346 -4.0462742 -4.0431585 -4.1157365 -4.1946054 -4.2334156 -4.2347608 -4.19748 -4.1450505 -4.1270413][-4.1951361 -4.20431 -4.2239008 -4.2222786 -4.1704426 -4.0679235 -3.9641194 -3.9495859 -4.0413709 -4.151948 -4.2195568 -4.242599 -4.2189794 -4.1617041 -4.128067][-4.1662993 -4.1793385 -4.1986151 -4.1954536 -4.1359653 -4.0204124 -3.9011538 -3.8746586 -3.9723682 -4.1031275 -4.1993275 -4.2467909 -4.2379651 -4.1825166 -4.1340847][-4.13665 -4.153667 -4.1810284 -4.188458 -4.1413236 -4.0387726 -3.9274244 -3.8908446 -3.961587 -4.0821304 -4.1858153 -4.248148 -4.2518058 -4.2023964 -4.1418748][-4.1215334 -4.138443 -4.1736274 -4.196279 -4.1770949 -4.1092687 -4.0282512 -3.9900272 -4.0201244 -4.1030927 -4.1898966 -4.2489448 -4.255167 -4.2102871 -4.1451426][-4.1308975 -4.1444168 -4.180192 -4.2098641 -4.2136292 -4.1799264 -4.1314898 -4.0966034 -4.0992942 -4.147429 -4.2098727 -4.2578745 -4.2634211 -4.2246723 -4.1643906][-4.1687975 -4.1786113 -4.20991 -4.2431026 -4.2587829 -4.2513819 -4.2274437 -4.1963587 -4.1871433 -4.2105083 -4.2497125 -4.2857842 -4.2926774 -4.2618876 -4.2114673][-4.2273345 -4.2330017 -4.2583466 -4.289135 -4.30901 -4.3142624 -4.3055696 -4.2830377 -4.2711682 -4.278718 -4.3001623 -4.3241239 -4.3297129 -4.3057222 -4.2656531][-4.27996 -4.2825255 -4.2992067 -4.323 -4.3390303 -4.34534 -4.3442354 -4.3319569 -4.3226051 -4.3208585 -4.330153 -4.3445039 -4.3478222 -4.3303847 -4.301743][-4.3062477 -4.3081808 -4.317523 -4.3312573 -4.3391442 -4.3425188 -4.34419 -4.3405938 -4.3356404 -4.3321648 -4.3352108 -4.3417683 -4.3419051 -4.3296328 -4.3118496]]...]
INFO - root - 2017-12-05 13:53:38.849737: step 16310, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 84h:38m:09s remains)
INFO - root - 2017-12-05 13:53:48.432077: step 16320, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 83h:31m:07s remains)
INFO - root - 2017-12-05 13:53:57.854233: step 16330, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 81h:42m:05s remains)
INFO - root - 2017-12-05 13:54:07.179847: step 16340, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 86h:30m:58s remains)
INFO - root - 2017-12-05 13:54:16.684107: step 16350, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 86h:46m:14s remains)
INFO - root - 2017-12-05 13:54:25.966832: step 16360, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.925 sec/batch; 81h:11m:39s remains)
INFO - root - 2017-12-05 13:54:35.634602: step 16370, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 83h:34m:12s remains)
INFO - root - 2017-12-05 13:54:45.011784: step 16380, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 73h:14m:59s remains)
INFO - root - 2017-12-05 13:54:54.387842: step 16390, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 78h:10m:58s remains)
INFO - root - 2017-12-05 13:55:03.796080: step 16400, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 81h:52m:25s remains)
2017-12-05 13:55:04.567546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.262496 -4.2654829 -4.2682467 -4.2694139 -4.2693691 -4.2690039 -4.268559 -4.2672362 -4.2651172 -4.2627282 -4.260993 -4.2596784 -4.2590041 -4.2585535 -4.2578282][-4.2587481 -4.26454 -4.2684765 -4.2695394 -4.2681279 -4.26617 -4.2644506 -4.2623396 -4.2597504 -4.2569041 -4.2547679 -4.2530947 -4.2518091 -4.2502894 -4.2479811][-4.2709742 -4.2758555 -4.2779288 -4.2773795 -4.2744961 -4.2709818 -4.2678385 -4.2653604 -4.2638206 -4.2623591 -4.2616348 -4.2608252 -4.2596817 -4.2578506 -4.2545643][-4.2812443 -4.2827954 -4.2825246 -4.2801042 -4.275712 -4.2709394 -4.2663465 -4.2632484 -4.2629666 -4.2633471 -4.2643781 -4.2650747 -4.265008 -4.2636976 -4.2602897][-4.2782054 -4.2770462 -4.2743831 -4.2694221 -4.2612929 -4.2531 -4.24578 -4.241621 -4.2413411 -4.2422714 -4.2437835 -4.2451115 -4.2459755 -4.2455258 -4.2430372][-4.2662206 -4.2620316 -4.2556248 -4.2451243 -4.23011 -4.2157474 -4.204237 -4.1970859 -4.1946039 -4.1942329 -4.1949978 -4.1963778 -4.1979432 -4.1984954 -4.1976981][-4.2510467 -4.2433496 -4.2318459 -4.2146807 -4.1929941 -4.1729207 -4.157517 -4.1469502 -4.1411119 -4.1378856 -4.137032 -4.13814 -4.1402555 -4.1421757 -4.1437774][-4.2303939 -4.222321 -4.2093091 -4.1904063 -4.1670141 -4.1453948 -4.1283112 -4.1150193 -4.1065907 -4.1019254 -4.1011052 -4.1035223 -4.1070862 -4.11082 -4.114944][-4.2207341 -4.2136149 -4.20287 -4.18831 -4.1705217 -4.1530871 -4.138732 -4.1271381 -4.1195874 -4.1156092 -4.1149759 -4.1176214 -4.1214428 -4.1259217 -4.1311169][-4.236115 -4.2333732 -4.2270617 -4.2174082 -4.2053952 -4.19265 -4.1820431 -4.1737237 -4.1679106 -4.1642661 -4.1630988 -4.1647649 -4.1679287 -4.1718817 -4.1757593][-4.2599115 -4.2605658 -4.2585425 -4.2536864 -4.2464194 -4.2377954 -4.2301393 -4.2240772 -4.2196651 -4.2165742 -4.21499 -4.2152214 -4.2167335 -4.2191634 -4.2209754][-4.2595024 -4.2612557 -4.26232 -4.2619262 -4.2587767 -4.253334 -4.24847 -4.2448115 -4.2419596 -4.239604 -4.2378535 -4.23705 -4.2373433 -4.2392583 -4.241128][-4.2410855 -4.24381 -4.2474766 -4.2504768 -4.25077 -4.24873 -4.2472458 -4.2464437 -4.2462749 -4.2458816 -4.2450809 -4.2443414 -4.245007 -4.2475462 -4.2498856][-4.2301049 -4.231874 -4.2359819 -4.2407861 -4.2440987 -4.2461033 -4.2493811 -4.2533979 -4.2574511 -4.2597284 -4.2599735 -4.2595334 -4.2606659 -4.2628083 -4.2634768][-4.2327867 -4.2319055 -4.2335052 -4.2371187 -4.2408352 -4.2450447 -4.2518811 -4.2597256 -4.2669806 -4.2713413 -4.2729349 -4.2732 -4.274394 -4.2755961 -4.2747006]]...]
INFO - root - 2017-12-05 13:55:13.993746: step 16410, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 85h:16m:28s remains)
INFO - root - 2017-12-05 13:55:23.373814: step 16420, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 80h:07m:39s remains)
INFO - root - 2017-12-05 13:55:32.590853: step 16430, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.943 sec/batch; 82h:48m:59s remains)
INFO - root - 2017-12-05 13:55:42.033248: step 16440, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 84h:51m:16s remains)
INFO - root - 2017-12-05 13:55:51.363750: step 16450, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 83h:47m:32s remains)
INFO - root - 2017-12-05 13:56:00.857718: step 16460, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 85h:15m:03s remains)
INFO - root - 2017-12-05 13:56:10.106830: step 16470, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 83h:50m:54s remains)
INFO - root - 2017-12-05 13:56:19.564295: step 16480, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 78h:57m:06s remains)
INFO - root - 2017-12-05 13:56:28.584276: step 16490, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 82h:03m:27s remains)
INFO - root - 2017-12-05 13:56:38.020604: step 16500, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 84h:53m:28s remains)
2017-12-05 13:56:38.898563: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1954403 -4.197144 -4.1857162 -4.1607056 -4.1265273 -4.1027894 -4.1162105 -4.1411142 -4.1614752 -4.1681385 -4.1761785 -4.1803441 -4.1648359 -4.1498938 -4.158453][-4.179667 -4.1749077 -4.1629772 -4.1415496 -4.1140728 -4.1004138 -4.1216273 -4.1501808 -4.1741247 -4.1899862 -4.1965756 -4.1946783 -4.1761675 -4.1641588 -4.1760488][-4.1563835 -4.1415215 -4.1308365 -4.1153054 -4.0937557 -4.08796 -4.1168146 -4.1499481 -4.1803007 -4.2051463 -4.2123218 -4.2058945 -4.1824765 -4.1671319 -4.1753287][-4.1427021 -4.1215291 -4.1163268 -4.1099434 -4.0943575 -4.0898004 -4.1174455 -4.1531591 -4.1919136 -4.2235575 -4.2311325 -4.2187037 -4.1873341 -4.1643119 -4.1662889][-4.1386747 -4.121006 -4.122138 -4.1218019 -4.1094074 -4.1025276 -4.1193452 -4.1537957 -4.201232 -4.2377434 -4.241797 -4.224555 -4.1907392 -4.1659245 -4.1649122][-4.141264 -4.1297779 -4.1340241 -4.1342564 -4.1172986 -4.1021571 -4.102479 -4.1313295 -4.1823792 -4.2200871 -4.2249584 -4.2076745 -4.1767364 -4.1563253 -4.155055][-4.150672 -4.150806 -4.1587429 -4.152761 -4.1252284 -4.0932755 -4.0727582 -4.09122 -4.1416278 -4.180572 -4.1893573 -4.1782393 -4.1539636 -4.1358871 -4.1329365][-4.1518617 -4.1659331 -4.1793575 -4.1681094 -4.128643 -4.0771189 -4.03519 -4.0431361 -4.0947809 -4.1422114 -4.1604338 -4.1579285 -4.1370358 -4.117197 -4.1127958][-4.143425 -4.1684651 -4.1885309 -4.1751757 -4.1236095 -4.05218 -3.9895957 -3.9871798 -4.0444169 -4.1062717 -4.1382494 -4.1394 -4.1123953 -4.0891514 -4.0873351][-4.1263466 -4.1566181 -4.1752577 -4.1575747 -4.0965018 -4.0096235 -3.9324806 -3.9246407 -3.9894533 -4.0674772 -4.1122408 -4.1147494 -4.0815153 -4.0544252 -4.0597568][-4.114614 -4.1426325 -4.1571937 -4.1335835 -4.0664253 -3.9714313 -3.8946047 -3.8956583 -3.9705 -4.0554214 -4.1040659 -4.1048732 -4.0682225 -4.0403113 -4.0498967][-4.1220269 -4.146152 -4.1589484 -4.1322494 -4.0681515 -3.9807546 -3.9245577 -3.944356 -4.0224543 -4.096839 -4.1335273 -4.1254597 -4.0827703 -4.0516882 -4.0588346][-4.1491714 -4.1702337 -4.1806922 -4.1556382 -4.0990338 -4.0273051 -3.9912331 -4.0163975 -4.0846949 -4.1417642 -4.1632938 -4.1461444 -4.1039028 -4.0750737 -4.0785728][-4.1772408 -4.197124 -4.2060266 -4.1873837 -4.1402245 -4.0813313 -4.0546007 -4.0757465 -4.1256418 -4.1657476 -4.1754985 -4.1536775 -4.1167159 -4.09431 -4.1022863][-4.2003903 -4.2136512 -4.220799 -4.209888 -4.1770139 -4.1337481 -4.1128883 -4.1254387 -4.1557436 -4.179822 -4.1791353 -4.1542687 -4.1213088 -4.1055465 -4.1168051]]...]
INFO - root - 2017-12-05 13:56:48.196989: step 16510, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 76h:35m:08s remains)
INFO - root - 2017-12-05 13:56:57.356661: step 16520, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 74h:39m:02s remains)
INFO - root - 2017-12-05 13:57:06.835815: step 16530, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 82h:21m:37s remains)
INFO - root - 2017-12-05 13:57:15.951559: step 16540, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 77h:07m:31s remains)
INFO - root - 2017-12-05 13:57:25.160647: step 16550, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 84h:01m:27s remains)
INFO - root - 2017-12-05 13:57:34.351201: step 16560, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 80h:24m:30s remains)
INFO - root - 2017-12-05 13:57:43.970665: step 16570, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.974 sec/batch; 85h:29m:09s remains)
INFO - root - 2017-12-05 13:57:53.243508: step 16580, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 87h:30m:00s remains)
INFO - root - 2017-12-05 13:58:02.546753: step 16590, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.886 sec/batch; 77h:46m:18s remains)
INFO - root - 2017-12-05 13:58:11.998092: step 16600, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 80h:21m:11s remains)
2017-12-05 13:58:12.778937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.101018 -4.0983658 -4.0813332 -4.0529246 -4.0772729 -4.1328354 -4.1563759 -4.1575584 -4.1582627 -4.1534238 -4.1423635 -4.1509233 -4.1612511 -4.1659112 -4.1724863][-4.1322327 -4.1334376 -4.1156168 -4.0882273 -4.107295 -4.152441 -4.1728163 -4.17917 -4.1804996 -4.1725526 -4.1613817 -4.1783991 -4.1927948 -4.1850972 -4.1787815][-4.1523814 -4.151999 -4.1357131 -4.1147432 -4.1316552 -4.1656489 -4.1758857 -4.1877356 -4.1967158 -4.1895671 -4.1848469 -4.2054319 -4.2176666 -4.1987648 -4.1832209][-4.1561637 -4.1524148 -4.1382041 -4.1203313 -4.135098 -4.1567626 -4.1540136 -4.1704354 -4.1894445 -4.1886349 -4.1905479 -4.2051482 -4.2066369 -4.1810312 -4.1623011][-4.1534119 -4.1419568 -4.1221156 -4.1016188 -4.1125813 -4.1252985 -4.1191235 -4.1417022 -4.1638513 -4.16366 -4.1757827 -4.1933918 -4.1888404 -4.1574345 -4.1346602][-4.13685 -4.1172662 -4.0891809 -4.0643415 -4.064261 -4.0512471 -4.0281973 -4.0667787 -4.1067677 -4.119153 -4.1447692 -4.1748667 -4.1799078 -4.1555605 -4.1396918][-4.1044693 -4.0718126 -4.0328169 -3.9947958 -3.9714632 -3.9046235 -3.840019 -3.9202709 -4.0186033 -4.0659609 -4.1117983 -4.1619172 -4.1869097 -4.1835093 -4.1828036][-4.0950756 -4.0468192 -3.9947507 -3.9405093 -3.8803215 -3.7436478 -3.6222229 -3.7614634 -3.9383507 -4.0279379 -4.0909204 -4.1589332 -4.201333 -4.2150831 -4.2231832][-4.100194 -4.0540342 -4.0031934 -3.9558682 -3.9047894 -3.7863104 -3.6787806 -3.7938633 -3.9609442 -4.0505943 -4.1091118 -4.1802692 -4.2303357 -4.2494254 -4.2539654][-4.1053257 -4.0716028 -4.0322571 -4.0063095 -3.9902928 -3.9376342 -3.8848882 -3.9473279 -4.0523233 -4.1134038 -4.1518416 -4.2064905 -4.2499566 -4.2643857 -4.258884][-4.0954785 -4.0798068 -4.0538521 -4.0483208 -4.0607667 -4.0479259 -4.0274224 -4.0643077 -4.1283064 -4.1628885 -4.1896815 -4.2311378 -4.2636032 -4.2748537 -4.2659225][-4.0827241 -4.081902 -4.0711207 -4.0796494 -4.1128941 -4.1194935 -4.1146808 -4.1447873 -4.1796851 -4.1978111 -4.2170138 -4.2453208 -4.2624249 -4.2667813 -4.2625275][-4.0816159 -4.0913429 -4.0944209 -4.1094928 -4.1538172 -4.1758161 -4.1745753 -4.1891208 -4.20767 -4.2172351 -4.2315354 -4.249331 -4.2511983 -4.2486048 -4.2511277][-4.1082635 -4.1246524 -4.1355186 -4.1462903 -4.1810217 -4.2092524 -4.2079148 -4.2100863 -4.2195358 -4.223083 -4.2299814 -4.2362881 -4.2289481 -4.2236643 -4.2329683][-4.1601219 -4.1781764 -4.1840611 -4.1843038 -4.2026777 -4.2270069 -4.22678 -4.222661 -4.2250781 -4.2253332 -4.2254891 -4.2262397 -4.218451 -4.21261 -4.22386]]...]
INFO - root - 2017-12-05 13:58:22.330398: step 16610, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 82h:00m:34s remains)
INFO - root - 2017-12-05 13:58:31.660299: step 16620, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.002 sec/batch; 87h:52m:49s remains)
INFO - root - 2017-12-05 13:58:41.070759: step 16630, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 74h:20m:15s remains)
INFO - root - 2017-12-05 13:58:50.538881: step 16640, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 84h:32m:57s remains)
INFO - root - 2017-12-05 13:58:59.787491: step 16650, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 82h:52m:38s remains)
INFO - root - 2017-12-05 13:59:09.035078: step 16660, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 78h:48m:25s remains)
INFO - root - 2017-12-05 13:59:18.366070: step 16670, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 84h:59m:39s remains)
INFO - root - 2017-12-05 13:59:27.744619: step 16680, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 82h:30m:17s remains)
INFO - root - 2017-12-05 13:59:37.184515: step 16690, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.952 sec/batch; 83h:29m:16s remains)
INFO - root - 2017-12-05 13:59:46.406165: step 16700, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.958 sec/batch; 84h:04m:00s remains)
2017-12-05 13:59:47.123436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2087426 -4.2032642 -4.2082853 -4.2126579 -4.2079053 -4.1872129 -4.15856 -4.1548638 -4.1738191 -4.1820335 -4.1815805 -4.16335 -4.1328788 -4.102118 -4.0852919][-4.1805372 -4.1732793 -4.1816115 -4.19026 -4.1811152 -4.1458879 -4.0986929 -4.0948949 -4.1328549 -4.1596885 -4.1720533 -4.1568642 -4.1143956 -4.0736465 -4.052599][-4.1476088 -4.1438365 -4.1571693 -4.1686196 -4.1497641 -4.0953517 -4.0244241 -4.0151839 -4.0742664 -4.1185188 -4.1419687 -4.131567 -4.0799046 -4.0320196 -4.0041509][-4.131197 -4.1265574 -4.140512 -4.1532483 -4.1250978 -4.0507717 -3.951616 -3.9367335 -4.0212221 -4.0802994 -4.1168747 -4.1166296 -4.0614309 -4.0043559 -3.9691689][-4.1212664 -4.112361 -4.1223788 -4.1335173 -4.1005859 -4.0055747 -3.8778496 -3.8642864 -3.9801683 -4.054523 -4.1018724 -4.1112576 -4.0592442 -4.0007658 -3.9621716][-4.0976686 -4.0875783 -4.0959606 -4.1060281 -4.0740304 -3.9628222 -3.8079247 -3.7944756 -3.9390914 -4.0267572 -4.0789585 -4.0946636 -4.050693 -3.9993348 -3.9658408][-4.0634212 -4.0526137 -4.0644736 -4.0769711 -4.0505137 -3.9334645 -3.761375 -3.742486 -3.9035468 -3.9994993 -4.0532117 -4.0709805 -4.0310369 -3.9872699 -3.9651864][-4.0658569 -4.0538573 -4.0646596 -4.0814438 -4.0683322 -3.9671066 -3.8112187 -3.7876065 -3.9414766 -4.0324368 -4.0764804 -4.0858712 -4.0412478 -3.9971151 -3.9818552][-4.1090078 -4.1000104 -4.1083961 -4.1248622 -4.1210489 -4.0458145 -3.925442 -3.9006617 -4.024713 -4.0939956 -4.1187487 -4.1144705 -4.0700569 -4.0302219 -4.0178857][-4.1585774 -4.1541238 -4.1602364 -4.1676035 -4.1642036 -4.1130142 -4.0271411 -4.00193 -4.0921288 -4.1395159 -4.150825 -4.140625 -4.1068573 -4.0801363 -4.0685244][-4.2098317 -4.2055478 -4.2069287 -4.2032714 -4.1965246 -4.1651106 -4.1104732 -4.0873866 -4.1487913 -4.1878281 -4.1946754 -4.1856284 -4.1661344 -4.1521454 -4.1396141][-4.2448707 -4.235702 -4.2324195 -4.2248735 -4.2173033 -4.2006836 -4.1672735 -4.151381 -4.1938396 -4.2246118 -4.2320662 -4.2274375 -4.2161717 -4.2078071 -4.19824][-4.2558422 -4.2446747 -4.2415414 -4.2336845 -4.2250276 -4.2188916 -4.2017231 -4.19074 -4.2181258 -4.2380891 -4.2460566 -4.2442117 -4.2393756 -4.2347984 -4.2281957][-4.2572317 -4.2500124 -4.25021 -4.2452755 -4.2371321 -4.2328997 -4.2227006 -4.2160044 -4.231719 -4.2397175 -4.2425013 -4.2426429 -4.2414923 -4.2421608 -4.2382827][-4.2650962 -4.262991 -4.2648139 -4.2620497 -4.2548776 -4.2507238 -4.2434888 -4.240047 -4.2496443 -4.253212 -4.2510247 -4.2508779 -4.2533827 -4.2565637 -4.2548714]]...]
INFO - root - 2017-12-05 13:59:56.592639: step 16710, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 87h:23m:41s remains)
INFO - root - 2017-12-05 14:00:05.532655: step 16720, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 76h:11m:31s remains)
INFO - root - 2017-12-05 14:00:14.890712: step 16730, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 81h:37m:55s remains)
INFO - root - 2017-12-05 14:00:24.282853: step 16740, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.883 sec/batch; 77h:26m:39s remains)
INFO - root - 2017-12-05 14:00:33.801382: step 16750, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 82h:56m:26s remains)
INFO - root - 2017-12-05 14:00:43.144844: step 16760, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 79h:04m:16s remains)
INFO - root - 2017-12-05 14:00:52.374230: step 16770, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 81h:44m:21s remains)
INFO - root - 2017-12-05 14:01:01.837101: step 16780, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 82h:34m:42s remains)
INFO - root - 2017-12-05 14:01:11.341760: step 16790, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.916 sec/batch; 80h:18m:11s remains)
INFO - root - 2017-12-05 14:01:20.635341: step 16800, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.901 sec/batch; 79h:00m:09s remains)
2017-12-05 14:01:21.482463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2703481 -4.2615581 -4.2558718 -4.2488427 -4.2251453 -4.1860976 -4.1593742 -4.1621404 -4.1872606 -4.2049046 -4.2194386 -4.2341776 -4.2455153 -4.2490797 -4.2481585][-4.2642875 -4.2531433 -4.2482615 -4.23851 -4.2010865 -4.1476588 -4.1187267 -4.131804 -4.1754332 -4.2033634 -4.2204185 -4.2313333 -4.2385015 -4.2401152 -4.2385168][-4.2471271 -4.2326884 -4.2332439 -4.230052 -4.1924872 -4.1322174 -4.1039643 -4.1277237 -4.183538 -4.2167788 -4.2337818 -4.2395515 -4.2412739 -4.2363105 -4.2335443][-4.2178273 -4.1996531 -4.2081866 -4.2144132 -4.1831574 -4.1242766 -4.0976067 -4.1333456 -4.1938367 -4.2230005 -4.236001 -4.241158 -4.2435813 -4.2331614 -4.2290869][-4.1806488 -4.1604829 -4.175066 -4.184597 -4.1531768 -4.0898366 -4.0651989 -4.1144848 -4.1801891 -4.2031369 -4.2069626 -4.2123089 -4.2168608 -4.2068858 -4.2076535][-4.1440134 -4.1215277 -4.1353469 -4.1364193 -4.0879617 -3.9965067 -3.9583116 -4.0272956 -4.114593 -4.149354 -4.1593618 -4.1706419 -4.1797223 -4.1717539 -4.1747327][-4.12051 -4.0929074 -4.1018043 -4.0885448 -4.0083804 -3.8674448 -3.7980895 -3.8973131 -4.0291305 -4.0957122 -4.1231856 -4.1459122 -4.1609416 -4.1561484 -4.1590686][-4.121964 -4.0965824 -4.1046133 -4.0807915 -3.9835415 -3.82094 -3.7411897 -3.8674145 -4.0194926 -4.0968027 -4.1263809 -4.1481442 -4.1707535 -4.1728854 -4.1772246][-4.1543627 -4.1413307 -4.1505189 -4.1271429 -4.0468369 -3.925992 -3.88762 -3.9920321 -4.1038666 -4.1550879 -4.1686964 -4.1791577 -4.1963019 -4.2027411 -4.2107363][-4.1857023 -4.1869268 -4.1995096 -4.1845927 -4.1293683 -4.0583286 -4.05015 -4.122427 -4.1939497 -4.2220278 -4.2211504 -4.224442 -4.2355256 -4.238996 -4.2453232][-4.2137384 -4.22465 -4.2402158 -4.23324 -4.1934171 -4.1470757 -4.1475778 -4.1941848 -4.2407942 -4.2622013 -4.2625928 -4.2673244 -4.2744021 -4.27483 -4.2785726][-4.2422996 -4.2588882 -4.2758136 -4.2693024 -4.2382288 -4.2027745 -4.2015705 -4.2291737 -4.2620349 -4.2823548 -4.2868996 -4.2920589 -4.2973161 -4.2981935 -4.3018079][-4.26296 -4.2789769 -4.2933288 -4.287972 -4.2641296 -4.2362022 -4.2329593 -4.2481894 -4.2725906 -4.2930908 -4.3001151 -4.302516 -4.303524 -4.3020954 -4.3056312][-4.268177 -4.2802 -4.2921238 -4.2897868 -4.2732239 -4.2548327 -4.2514434 -4.2601304 -4.2781687 -4.2962 -4.3056455 -4.3083711 -4.3060923 -4.3016477 -4.3037181][-4.2731032 -4.2781096 -4.286036 -4.2864952 -4.2758312 -4.2629075 -4.2616539 -4.2699704 -4.2837777 -4.2964325 -4.3053331 -4.310194 -4.3092852 -4.3064814 -4.3074408]]...]
INFO - root - 2017-12-05 14:01:30.824941: step 16810, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 83h:53m:15s remains)
INFO - root - 2017-12-05 14:01:40.076655: step 16820, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 82h:58m:59s remains)
INFO - root - 2017-12-05 14:01:49.403219: step 16830, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 82h:26m:09s remains)
INFO - root - 2017-12-05 14:01:58.879187: step 16840, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 80h:02m:28s remains)
INFO - root - 2017-12-05 14:02:08.294790: step 16850, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 82h:33m:52s remains)
INFO - root - 2017-12-05 14:02:17.709622: step 16860, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 81h:07m:47s remains)
INFO - root - 2017-12-05 14:02:26.982755: step 16870, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.875 sec/batch; 76h:45m:10s remains)
INFO - root - 2017-12-05 14:02:36.544686: step 16880, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 78h:42m:17s remains)
INFO - root - 2017-12-05 14:02:45.842210: step 16890, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 80h:34m:03s remains)
INFO - root - 2017-12-05 14:02:55.315708: step 16900, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.829 sec/batch; 72h:40m:54s remains)
2017-12-05 14:02:56.139220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.222754 -4.2133818 -4.2158012 -4.2355132 -4.2564631 -4.2660842 -4.2673078 -4.2643948 -4.261755 -4.2580175 -4.2530794 -4.2402029 -4.2299256 -4.2321181 -4.2444181][-4.2938204 -4.2894888 -4.2913985 -4.3044639 -4.3166304 -4.3218102 -4.3228569 -4.3216481 -4.3190665 -4.3129168 -4.303576 -4.2915225 -4.2866421 -4.2934213 -4.3045044][-4.3363709 -4.3344989 -4.3350663 -4.3412995 -4.344687 -4.3431149 -4.3393078 -4.3349538 -4.3289533 -4.3182473 -4.3033404 -4.2913222 -4.29152 -4.3053885 -4.319448][-4.350709 -4.3481159 -4.3471122 -4.3474789 -4.3424077 -4.3321843 -4.3231368 -4.3156376 -4.3052492 -4.2895775 -4.2669168 -4.24987 -4.2503285 -4.2674065 -4.2856092][-4.3450251 -4.3404684 -4.3369637 -4.3315449 -4.3167949 -4.296237 -4.2793326 -4.2662988 -4.2512803 -4.2321539 -4.2073026 -4.1899247 -4.1882491 -4.2014856 -4.2204204][-4.3299603 -4.3203583 -4.3132677 -4.3032851 -4.2797527 -4.2479143 -4.2196803 -4.2009249 -4.186233 -4.1713367 -4.1528897 -4.1444716 -4.1422205 -4.1436892 -4.1551375][-4.310276 -4.2940726 -4.2835298 -4.27081 -4.243535 -4.2048111 -4.1668844 -4.1442604 -4.1368279 -4.1321921 -4.1236238 -4.1206393 -4.1143112 -4.1056457 -4.1117511][-4.2912192 -4.2703638 -4.2585225 -4.2450314 -4.2176828 -4.1793509 -4.142487 -4.1224918 -4.1249084 -4.1324515 -4.1329556 -4.1315889 -4.1219597 -4.1081319 -4.1149573][-4.2768955 -4.2581787 -4.2499228 -4.23739 -4.2122307 -4.1778393 -4.1478038 -4.1366134 -4.150847 -4.1700988 -4.1750417 -4.1711922 -4.1595426 -4.1430211 -4.150332][-4.2684612 -4.2606983 -4.2600312 -4.2492356 -4.2272696 -4.1998577 -4.1798697 -4.1785922 -4.2017345 -4.2241597 -4.2271147 -4.2189283 -4.2015424 -4.1802392 -4.18642][-4.2606163 -4.272027 -4.2821503 -4.2757969 -4.260128 -4.2393594 -4.227489 -4.2338414 -4.2583041 -4.27449 -4.268878 -4.2521954 -4.2282677 -4.20189 -4.2086678][-4.251976 -4.2827964 -4.3048306 -4.3074193 -4.3006239 -4.2856469 -4.2796297 -4.2896428 -4.3092008 -4.3159356 -4.3005643 -4.2727156 -4.239707 -4.2087088 -4.2182317][-4.2506037 -4.2903709 -4.3193331 -4.3288074 -4.330687 -4.3231506 -4.3235559 -4.3365684 -4.3477 -4.3470869 -4.3266406 -4.291008 -4.2460337 -4.208034 -4.2187791][-4.2604041 -4.2961712 -4.3215628 -4.332808 -4.3400574 -4.33973 -4.3464031 -4.3629246 -4.3674126 -4.3648849 -4.3471589 -4.3100371 -4.259171 -4.21717 -4.2269926][-4.2733893 -4.298533 -4.3146415 -4.3253622 -4.3350053 -4.3397202 -4.3504119 -4.3671865 -4.3690853 -4.3680005 -4.3558111 -4.3234324 -4.2743874 -4.233829 -4.2411251]]...]
INFO - root - 2017-12-05 14:03:05.625503: step 16910, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.967 sec/batch; 84h:45m:07s remains)
INFO - root - 2017-12-05 14:03:14.859490: step 16920, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 83h:22m:44s remains)
INFO - root - 2017-12-05 14:03:24.343940: step 16930, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 80h:36m:11s remains)
INFO - root - 2017-12-05 14:03:33.675839: step 16940, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 81h:47m:45s remains)
INFO - root - 2017-12-05 14:03:43.131555: step 16950, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 82h:39m:00s remains)
INFO - root - 2017-12-05 14:03:52.622246: step 16960, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 81h:45m:49s remains)
INFO - root - 2017-12-05 14:04:01.871282: step 16970, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 78h:06m:55s remains)
INFO - root - 2017-12-05 14:04:11.193053: step 16980, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 81h:12m:06s remains)
INFO - root - 2017-12-05 14:04:20.683880: step 16990, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 78h:41m:52s remains)
INFO - root - 2017-12-05 14:04:29.901944: step 17000, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 79h:43m:35s remains)
2017-12-05 14:04:30.756298: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.282701 -4.27583 -4.2700443 -4.2643127 -4.2628951 -4.2661729 -4.2705193 -4.2734542 -4.2783165 -4.2838111 -4.2877684 -4.2949948 -4.3040686 -4.3125353 -4.318965][-4.2889018 -4.2790928 -4.2699914 -4.2582331 -4.2514944 -4.2531691 -4.2565827 -4.2593155 -4.265686 -4.2749648 -4.2811804 -4.2881756 -4.2956619 -4.3040686 -4.3107042][-4.2929492 -4.2790074 -4.2635937 -4.2450328 -4.2338514 -4.2343426 -4.2370481 -4.2418509 -4.254477 -4.269763 -4.2789469 -4.2866445 -4.2930036 -4.3001585 -4.3063164][-4.2919621 -4.2720966 -4.2456284 -4.2162814 -4.2000217 -4.2035813 -4.2082386 -4.2174006 -4.2388391 -4.2612557 -4.2742095 -4.2838726 -4.2931714 -4.3022394 -4.3072691][-4.2801609 -4.2497239 -4.2076297 -4.1637583 -4.1387463 -4.1411705 -4.1497917 -4.1708541 -4.211442 -4.2460189 -4.2629824 -4.2742491 -4.2882333 -4.3022552 -4.3063741][-4.2450371 -4.2033515 -4.1458354 -4.0845594 -4.0411091 -4.0285907 -4.0282826 -4.059824 -4.1327157 -4.1941814 -4.2256732 -4.2471867 -4.2708316 -4.2918134 -4.2973447][-4.1937556 -4.1481185 -4.0799494 -4.0027103 -3.930567 -3.8826036 -3.8467522 -3.8756032 -3.986455 -4.0864649 -4.1456103 -4.1880078 -4.227685 -4.2600579 -4.2747383][-4.1463075 -4.1014938 -4.0345149 -3.9482231 -3.8542695 -3.7607479 -3.6686349 -3.6760433 -3.8180296 -3.9550285 -4.0449128 -4.1094942 -4.1660795 -4.2127585 -4.2441273][-4.1350651 -4.0942969 -4.0423656 -3.968883 -3.8772676 -3.7709713 -3.6528649 -3.6241496 -3.7478883 -3.8880296 -3.9956856 -4.0764756 -4.1455307 -4.2010522 -4.2398787][-4.1688309 -4.1362805 -4.0994911 -4.0491023 -3.982512 -3.8982089 -3.8015769 -3.7565818 -3.8308167 -3.9378037 -4.0350323 -4.109416 -4.1723685 -4.2216949 -4.2569156][-4.2229624 -4.194706 -4.1684022 -4.136477 -4.0917568 -4.0317464 -3.9593267 -3.9158673 -3.9549119 -4.0284023 -4.1097956 -4.1740208 -4.222887 -4.2584515 -4.2837811][-4.2836337 -4.2620406 -4.2444172 -4.22514 -4.1959553 -4.1539922 -4.0997992 -4.0628643 -4.0808692 -4.1280065 -4.1915746 -4.2404313 -4.2742372 -4.2971878 -4.3117323][-4.3261037 -4.3152833 -4.3057203 -4.2955151 -4.2775359 -4.2501779 -4.21247 -4.1847668 -4.19158 -4.21979 -4.2638311 -4.2957067 -4.3141255 -4.3258753 -4.3338742][-4.3296938 -4.3258858 -4.3220773 -4.3187566 -4.3125496 -4.3002319 -4.280396 -4.2656236 -4.2712088 -4.2884622 -4.3147316 -4.3330641 -4.3416247 -4.347281 -4.3508487][-4.3190236 -4.3163347 -4.3135262 -4.314024 -4.3152323 -4.3136282 -4.3097348 -4.307961 -4.3131285 -4.3234444 -4.3376722 -4.3466506 -4.35118 -4.3545303 -4.3559389]]...]
INFO - root - 2017-12-05 14:04:40.296413: step 17010, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 83h:34m:39s remains)
INFO - root - 2017-12-05 14:04:49.526669: step 17020, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 84h:29m:01s remains)
INFO - root - 2017-12-05 14:04:58.918655: step 17030, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 85h:04m:16s remains)
INFO - root - 2017-12-05 14:05:08.281663: step 17040, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 81h:25m:24s remains)
INFO - root - 2017-12-05 14:05:17.739370: step 17050, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 74h:11m:00s remains)
INFO - root - 2017-12-05 14:05:27.214924: step 17060, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 77h:48m:36s remains)
INFO - root - 2017-12-05 14:05:36.399774: step 17070, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 79h:32m:00s remains)
INFO - root - 2017-12-05 14:05:45.817556: step 17080, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 78h:38m:42s remains)
INFO - root - 2017-12-05 14:05:55.040025: step 17090, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 82h:56m:21s remains)
INFO - root - 2017-12-05 14:06:04.455410: step 17100, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 82h:08m:31s remains)
2017-12-05 14:06:05.190753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23902 -4.2404461 -4.2422118 -4.2435851 -4.2450991 -4.2458668 -4.2469893 -4.2482204 -4.2486825 -4.2486858 -4.2489738 -4.2496095 -4.2512197 -4.2518144 -4.2471867][-4.2372937 -4.2401066 -4.2421551 -4.2436919 -4.2469583 -4.2499571 -4.2533283 -4.2554226 -4.2554703 -4.2537889 -4.252111 -4.2523708 -4.2548423 -4.2568536 -4.2546167][-4.2401023 -4.2402635 -4.2393551 -4.2388754 -4.2414293 -4.24428 -4.247859 -4.2502556 -4.2513981 -4.2506008 -4.2503519 -4.2538137 -4.2603517 -4.2657142 -4.2669654][-4.2364783 -4.2320108 -4.2276168 -4.2242036 -4.2234292 -4.2218952 -4.2226224 -4.2243829 -4.2279553 -4.2314253 -4.2354555 -4.2445374 -4.255712 -4.2635021 -4.2658186][-4.2191896 -4.2077322 -4.1976495 -4.1897993 -4.18328 -4.1757216 -4.1736288 -4.1754723 -4.1831245 -4.1917729 -4.2013783 -4.2161269 -4.2311044 -4.2396646 -4.2416358][-4.17575 -4.1597333 -4.1456113 -4.1319904 -4.1166778 -4.1015282 -4.095994 -4.0996151 -4.1117048 -4.1239138 -4.1358066 -4.15374 -4.1717024 -4.1829071 -4.1873603][-4.1133118 -4.1008563 -4.0894737 -4.0723605 -4.0464735 -4.0203075 -4.0067234 -4.0098963 -4.0297213 -4.0484562 -4.0610423 -4.0772 -4.096746 -4.1124291 -4.122087][-4.0776539 -4.0681734 -4.058444 -4.0403986 -4.0101 -3.9790154 -3.9596615 -3.9635661 -3.9924572 -4.01955 -4.0323 -4.0421929 -4.0587068 -4.0774093 -4.0923758][-4.1003509 -4.0899496 -4.0781097 -4.0622554 -4.039783 -4.0173659 -4.0009742 -4.0039597 -4.0308642 -4.0546751 -4.0607176 -4.0577979 -4.0629354 -4.0769048 -4.0918269][-4.1596718 -4.1493759 -4.1369705 -4.1238589 -4.1111388 -4.1014156 -4.0922337 -4.0928679 -4.1067038 -4.1154156 -4.1078792 -4.0908523 -4.0811958 -4.0823603 -4.0910282][-4.2142692 -4.2062368 -4.1953106 -4.1848087 -4.1780257 -4.1753659 -4.1711364 -4.1695004 -4.1723938 -4.1691422 -4.1534562 -4.1297212 -4.1109533 -4.101378 -4.100347][-4.2419653 -4.2351022 -4.2263474 -4.2176547 -4.2116995 -4.209734 -4.2068934 -4.2050171 -4.2035861 -4.1974831 -4.1829252 -4.1627626 -4.1437006 -4.1285081 -4.1170735][-4.2307272 -4.2252154 -4.216867 -4.2072988 -4.1994524 -4.1958613 -4.1933722 -4.1937022 -4.1950655 -4.1955767 -4.1894493 -4.1762762 -4.1598258 -4.1410341 -4.1233535][-4.1821804 -4.17945 -4.174736 -4.1658955 -4.155962 -4.1501274 -4.1471438 -4.1497555 -4.1566782 -4.1662583 -4.1701593 -4.1647058 -4.1528168 -4.1349039 -4.1170216][-4.1424894 -4.1433859 -4.1441355 -4.137918 -4.1277943 -4.1211786 -4.1197867 -4.1253166 -4.1364331 -4.151196 -4.1626096 -4.1661654 -4.1620455 -4.1502748 -4.1374617]]...]
INFO - root - 2017-12-05 14:06:14.516715: step 17110, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 75h:18m:45s remains)
INFO - root - 2017-12-05 14:06:23.829055: step 17120, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 85h:25m:03s remains)
INFO - root - 2017-12-05 14:06:32.942472: step 17130, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 78h:22m:18s remains)
INFO - root - 2017-12-05 14:06:42.386560: step 17140, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 86h:20m:43s remains)
INFO - root - 2017-12-05 14:06:51.814110: step 17150, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 82h:18m:44s remains)
INFO - root - 2017-12-05 14:07:01.131761: step 17160, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 85h:06m:36s remains)
INFO - root - 2017-12-05 14:07:10.623670: step 17170, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 82h:10m:33s remains)
INFO - root - 2017-12-05 14:07:19.729031: step 17180, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 80h:48m:33s remains)
INFO - root - 2017-12-05 14:07:29.299809: step 17190, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 85h:30m:17s remains)
INFO - root - 2017-12-05 14:07:38.677782: step 17200, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 88h:05m:57s remains)
2017-12-05 14:07:39.629250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2762551 -4.26845 -4.2691746 -4.2768984 -4.2845206 -4.2853436 -4.2792935 -4.2715654 -4.2683778 -4.2715712 -4.2757325 -4.2748184 -4.26618 -4.2530713 -4.2384439][-4.2882242 -4.2800169 -4.27481 -4.2761631 -4.2803226 -4.2812262 -4.277288 -4.2712941 -4.2693486 -4.2743936 -4.2794023 -4.2794466 -4.273263 -4.2632012 -4.2515244][-4.284061 -4.2787004 -4.2674909 -4.259131 -4.257308 -4.259294 -4.2601423 -4.2606406 -4.2642283 -4.2720566 -4.2778907 -4.2786984 -4.2744241 -4.2670441 -4.2591772][-4.2488289 -4.2443485 -4.2268109 -4.2086921 -4.201694 -4.2081146 -4.2183523 -4.2302852 -4.2422295 -4.255156 -4.2648726 -4.2689676 -4.2651372 -4.2587242 -4.2537184][-4.1902242 -4.1839957 -4.1629128 -4.1369233 -4.1251783 -4.135848 -4.1509361 -4.1679955 -4.1856484 -4.2045126 -4.2222476 -4.2345667 -4.236208 -4.2349954 -4.2342319][-4.1481485 -4.1446862 -4.122313 -4.0910039 -4.0726352 -4.0815215 -4.0942678 -4.10488 -4.1189709 -4.1381249 -4.1615429 -4.1817579 -4.1897745 -4.197432 -4.206902][-4.14862 -4.1527038 -4.1337276 -4.1029181 -4.0804477 -4.0815544 -4.0837293 -4.0808024 -4.0860224 -4.1015649 -4.1235085 -4.1450706 -4.156868 -4.1704078 -4.1887317][-4.1836681 -4.194603 -4.1823745 -4.158854 -4.1368885 -4.1294427 -4.1206923 -4.1041112 -4.1002741 -4.1107645 -4.1267939 -4.1449008 -4.1594419 -4.1753035 -4.1959724][-4.228735 -4.2404237 -4.2347431 -4.2207246 -4.20601 -4.1978831 -4.1849604 -4.16212 -4.1504097 -4.15291 -4.1589718 -4.171742 -4.1871352 -4.2037559 -4.2238975][-4.272778 -4.2782359 -4.2740035 -4.2665906 -4.2592587 -4.2563653 -4.2490587 -4.230258 -4.216794 -4.2111669 -4.2080994 -4.2153258 -4.2274842 -4.2401361 -4.2554426][-4.3139448 -4.3103285 -4.3027596 -4.2988148 -4.2972007 -4.2996712 -4.2991147 -4.2878504 -4.2759285 -4.2675352 -4.2607846 -4.2608519 -4.2662125 -4.2727017 -4.2804661][-4.3399639 -4.3308454 -4.3201928 -4.3161168 -4.3172445 -4.3244576 -4.3296142 -4.3259015 -4.3185205 -4.3126054 -4.308023 -4.3027463 -4.2982249 -4.2962513 -4.2957039][-4.3393188 -4.3303313 -4.3187046 -4.3140106 -4.3175569 -4.3279052 -4.3367767 -4.3374987 -4.3323865 -4.3282309 -4.32606 -4.3204904 -4.3114872 -4.3054876 -4.3005004][-4.3211117 -4.3181891 -4.3080606 -4.30306 -4.3069119 -4.3179445 -4.3279343 -4.3290377 -4.3226633 -4.3173418 -4.3156743 -4.3107142 -4.3022089 -4.2955675 -4.2920475][-4.300601 -4.3037939 -4.2975588 -4.2935543 -4.2975206 -4.3087044 -4.3202257 -4.3209162 -4.3121648 -4.3022814 -4.2982373 -4.2943754 -4.2867103 -4.27976 -4.2782574]]...]
INFO - root - 2017-12-05 14:07:49.026603: step 17210, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.992 sec/batch; 86h:53m:36s remains)
INFO - root - 2017-12-05 14:07:58.531045: step 17220, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 82h:05m:59s remains)
INFO - root - 2017-12-05 14:08:07.964336: step 17230, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 87h:28m:00s remains)
INFO - root - 2017-12-05 14:08:17.409714: step 17240, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.921 sec/batch; 80h:38m:56s remains)
INFO - root - 2017-12-05 14:08:26.829327: step 17250, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 81h:57m:48s remains)
INFO - root - 2017-12-05 14:08:36.145973: step 17260, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 80h:27m:08s remains)
INFO - root - 2017-12-05 14:08:45.412278: step 17270, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 86h:19m:01s remains)
INFO - root - 2017-12-05 14:08:54.745702: step 17280, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 84h:30m:10s remains)
INFO - root - 2017-12-05 14:09:04.191009: step 17290, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 83h:41m:42s remains)
INFO - root - 2017-12-05 14:09:13.674200: step 17300, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 86h:51m:36s remains)
2017-12-05 14:09:14.435161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2573695 -4.2498341 -4.2428026 -4.2325811 -4.2262516 -4.2352009 -4.2542214 -4.2654028 -4.2666812 -4.268424 -4.2642455 -4.2559452 -4.253643 -4.2543755 -4.2441239][-4.26204 -4.2510047 -4.2414107 -4.2294426 -4.21988 -4.225862 -4.2440333 -4.2547922 -4.252594 -4.249896 -4.2411146 -4.227747 -4.2199254 -4.2177143 -4.2059064][-4.2604394 -4.2459421 -4.2315292 -4.2144413 -4.1979804 -4.1970024 -4.2124114 -4.2273083 -4.2301116 -4.2282362 -4.2177062 -4.2010016 -4.1890106 -4.1839585 -4.1732306][-4.2503643 -4.2348385 -4.2182455 -4.2010098 -4.1817684 -4.1716723 -4.1815457 -4.1989522 -4.2057514 -4.2058954 -4.1956487 -4.1792016 -4.1667171 -4.1580639 -4.1484475][-4.2218661 -4.2062645 -4.1884513 -4.178061 -4.162744 -4.1501555 -4.1557565 -4.1723218 -4.1817389 -4.1867537 -4.1822934 -4.1706104 -4.159925 -4.1493568 -4.1416874][-4.1882005 -4.1623926 -4.13482 -4.1225414 -4.1087708 -4.0976839 -4.1036119 -4.1233435 -4.1384006 -4.1508422 -4.1558313 -4.1549044 -4.1530304 -4.1497307 -4.1493125][-4.163353 -4.1247959 -4.0816741 -4.0570865 -4.0388346 -4.02725 -4.0309296 -4.0498371 -4.0726643 -4.0983181 -4.1183925 -4.1353273 -4.1505256 -4.1617465 -4.1724954][-4.1512108 -4.1050234 -4.0492945 -4.0115175 -3.9885559 -3.9787996 -3.9798868 -3.9952631 -4.0268917 -4.0702457 -4.1094961 -4.1470885 -4.1796951 -4.2048287 -4.224329][-4.15514 -4.1122875 -4.0585513 -4.018683 -3.99512 -3.99029 -3.9930506 -4.0048156 -4.0330005 -4.0785217 -4.1256037 -4.1725597 -4.2139816 -4.2448392 -4.2675409][-4.1693439 -4.1422844 -4.1034865 -4.0737 -4.0596294 -4.0602384 -4.06553 -4.0736632 -4.0884633 -4.1181135 -4.1552534 -4.1959462 -4.2323065 -4.2585416 -4.2791815][-4.1841078 -4.16905 -4.1455679 -4.1309681 -4.1302233 -4.1322913 -4.140635 -4.1483116 -4.1513934 -4.1619558 -4.1806078 -4.2044415 -4.2290258 -4.2471309 -4.2639942][-4.1905427 -4.1839347 -4.1702018 -4.1679168 -4.1768179 -4.1805525 -4.1919971 -4.2021451 -4.199996 -4.196527 -4.1961317 -4.2004461 -4.2097063 -4.2157712 -4.2236018][-4.1708841 -4.1774545 -4.176662 -4.1812429 -4.1954579 -4.2029114 -4.2177172 -4.2312064 -4.230104 -4.2213836 -4.2110753 -4.2033095 -4.1997695 -4.193758 -4.1912603][-4.1380568 -4.1566567 -4.1628346 -4.1688528 -4.1853409 -4.19526 -4.2122154 -4.2280993 -4.2307911 -4.22353 -4.2114053 -4.1996183 -4.1892014 -4.1743908 -4.1621432][-4.0935416 -4.1167188 -4.127387 -4.13482 -4.1522689 -4.1683464 -4.1883631 -4.2047591 -4.2096891 -4.2028251 -4.1889753 -4.1746316 -4.1604877 -4.1435452 -4.1292157]]...]
INFO - root - 2017-12-05 14:09:24.068659: step 17310, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 85h:48m:50s remains)
INFO - root - 2017-12-05 14:09:33.554164: step 17320, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.997 sec/batch; 87h:19m:26s remains)
INFO - root - 2017-12-05 14:09:42.755415: step 17330, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.981 sec/batch; 85h:53m:39s remains)
INFO - root - 2017-12-05 14:09:52.192941: step 17340, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.993 sec/batch; 86h:55m:34s remains)
INFO - root - 2017-12-05 14:10:01.716428: step 17350, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 85h:17m:13s remains)
INFO - root - 2017-12-05 14:10:10.870441: step 17360, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 84h:07m:58s remains)
INFO - root - 2017-12-05 14:10:20.291049: step 17370, loss = 2.04, batch loss = 1.98 (7.9 examples/sec; 1.010 sec/batch; 88h:27m:08s remains)
INFO - root - 2017-12-05 14:10:29.678196: step 17380, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 85h:12m:28s remains)
INFO - root - 2017-12-05 14:10:39.138852: step 17390, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.994 sec/batch; 87h:00m:31s remains)
INFO - root - 2017-12-05 14:10:48.680662: step 17400, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 84h:00m:25s remains)
2017-12-05 14:10:49.447262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2958107 -4.289485 -4.2847958 -4.2803011 -4.27369 -4.2695508 -4.2704439 -4.2662005 -4.261313 -4.2594757 -4.2580366 -4.2606177 -4.270278 -4.2782307 -4.285378][-4.2723713 -4.2616324 -4.2534838 -4.2489467 -4.2414174 -4.236455 -4.2343388 -4.2242308 -4.2139225 -4.2120814 -4.2143464 -4.2198696 -4.2319446 -4.2402821 -4.2474542][-4.246583 -4.2311845 -4.2186522 -4.2145929 -4.210804 -4.2086921 -4.2022176 -4.1838179 -4.1710262 -4.1763453 -4.1888118 -4.1963553 -4.2039309 -4.2051978 -4.2079091][-4.2178922 -4.1968131 -4.179769 -4.1762476 -4.1808953 -4.1815224 -4.168303 -4.1416593 -4.1302862 -4.1449742 -4.1660113 -4.1763706 -4.1786108 -4.1717048 -4.1698904][-4.1924005 -4.1637354 -4.1440706 -4.144063 -4.1547346 -4.153646 -4.1263123 -4.0885129 -4.0772777 -4.1005187 -4.1316237 -4.1518307 -4.15493 -4.1449494 -4.1403942][-4.1780462 -4.1433692 -4.1256065 -4.1323614 -4.1436458 -4.1336093 -4.0838966 -4.0254917 -4.0151839 -4.0561914 -4.1053381 -4.1392345 -4.1463275 -4.1325212 -4.1251616][-4.1718564 -4.13468 -4.1200061 -4.1309872 -4.1374149 -4.1091685 -4.0255852 -3.9311044 -3.9213068 -4.0012908 -4.0835319 -4.1338887 -4.144033 -4.1255932 -4.1165609][-4.1676769 -4.1300521 -4.1138706 -4.1206975 -4.1137867 -4.0669236 -3.9420958 -3.7947974 -3.795974 -3.9336457 -4.054225 -4.1148672 -4.1234679 -4.0982766 -4.0927029][-4.160254 -4.1191549 -4.1002903 -4.1038232 -4.0925174 -4.037641 -3.9043469 -3.7546139 -3.7759809 -3.9287405 -4.0520782 -4.1044788 -4.1045756 -4.0765362 -4.0746012][-4.1607656 -4.1187506 -4.1008325 -4.1057944 -4.1004477 -4.055491 -3.9611483 -3.8689559 -3.886018 -3.993428 -4.0813813 -4.1111569 -4.1013875 -4.076838 -4.0825887][-4.1768408 -4.1373949 -4.1219974 -4.1261344 -4.1205878 -4.0874052 -4.033236 -3.985575 -3.9903054 -4.0505304 -4.1032424 -4.1170254 -4.1060481 -4.0909128 -4.1046476][-4.211009 -4.1734304 -4.15571 -4.1540608 -4.1476569 -4.1251931 -4.0988555 -4.0786886 -4.0809565 -4.1110072 -4.1377726 -4.1407266 -4.1290555 -4.121007 -4.1375904][-4.250833 -4.2212834 -4.2031775 -4.1952825 -4.1879892 -4.1737652 -4.162869 -4.1595182 -4.1652532 -4.1799846 -4.1920457 -4.189527 -4.179008 -4.1748247 -4.1864][-4.2833738 -4.2640328 -4.2492476 -4.2420554 -4.2375946 -4.2306871 -4.2270503 -4.229507 -4.2332244 -4.2391415 -4.2439756 -4.2424135 -4.2375317 -4.236773 -4.2435465][-4.3062367 -4.2935624 -4.2834573 -4.2803812 -4.2812123 -4.2808394 -4.2819576 -4.2857127 -4.2866759 -4.28765 -4.2897744 -4.2896619 -4.2879934 -4.2879305 -4.291594]]...]
INFO - root - 2017-12-05 14:10:58.893525: step 17410, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 82h:08m:00s remains)
INFO - root - 2017-12-05 14:11:08.337197: step 17420, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 80h:43m:14s remains)
INFO - root - 2017-12-05 14:11:17.773382: step 17430, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 85h:58m:26s remains)
INFO - root - 2017-12-05 14:11:27.229723: step 17440, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 80h:08m:37s remains)
INFO - root - 2017-12-05 14:11:36.347934: step 17450, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 82h:08m:17s remains)
INFO - root - 2017-12-05 14:11:45.715545: step 17460, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 74h:39m:00s remains)
INFO - root - 2017-12-05 14:11:55.122688: step 17470, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 82h:24m:42s remains)
INFO - root - 2017-12-05 14:12:04.619041: step 17480, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 83h:26m:56s remains)
INFO - root - 2017-12-05 14:12:14.145792: step 17490, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 87h:44m:26s remains)
INFO - root - 2017-12-05 14:12:23.531543: step 17500, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 83h:07m:54s remains)
2017-12-05 14:12:24.295291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2474804 -4.2390246 -4.2398028 -4.250958 -4.2557826 -4.2566972 -4.2623181 -4.2713394 -4.2837152 -4.2904348 -4.296432 -4.3017693 -4.3037629 -4.3043323 -4.3028145][-4.2082953 -4.2015166 -4.2048044 -4.2187824 -4.2236681 -4.2237144 -4.2275629 -4.235827 -4.2546582 -4.268064 -4.2779164 -4.2849326 -4.2860007 -4.2851834 -4.2822695][-4.1737146 -4.171473 -4.1745954 -4.1856303 -4.1862922 -4.1834292 -4.180656 -4.185348 -4.213161 -4.2401643 -4.2603946 -4.2718177 -4.2732186 -4.2685323 -4.2588134][-4.1432438 -4.150928 -4.1533222 -4.1599379 -4.1534467 -4.13878 -4.1126714 -4.1017694 -4.1461983 -4.197701 -4.2358189 -4.2543578 -4.2558675 -4.2427707 -4.2199397][-4.1029978 -4.1244354 -4.1310282 -4.1361623 -4.1207929 -4.0814743 -4.0077906 -3.9705572 -4.0423269 -4.1306844 -4.1918778 -4.2175 -4.2191067 -4.1974287 -4.1541157][-4.0627127 -4.0983434 -4.1156569 -4.1239495 -4.0985789 -4.0249877 -3.8842597 -3.7995269 -3.9045756 -4.0416222 -4.1290884 -4.1654296 -4.1702394 -4.1435084 -4.0807462][-4.0544987 -4.0898294 -4.1072874 -4.1186805 -4.0933547 -4.0009913 -3.8156705 -3.6790226 -3.7955494 -3.9611392 -4.0622554 -4.1083784 -4.1231685 -4.1011763 -4.0317621][-4.0893421 -4.1114116 -4.1213226 -4.1407642 -4.1316867 -4.057518 -3.9015045 -3.7694778 -3.834456 -3.9636831 -4.0444174 -4.089561 -4.1170015 -4.1135955 -4.0613527][-4.134376 -4.1409464 -4.1448312 -4.1733451 -4.1815271 -4.1387677 -4.0450339 -3.949959 -3.9650271 -4.0342207 -4.0816684 -4.1154304 -4.1482453 -4.1634331 -4.1341982][-4.1733012 -4.1645164 -4.1623263 -4.1929016 -4.2107272 -4.1924062 -4.1480093 -4.0913005 -4.0822568 -4.1069479 -4.1280932 -4.150671 -4.1807961 -4.2046576 -4.1935997][-4.1880713 -4.1723166 -4.1705914 -4.1998019 -4.221231 -4.215414 -4.1959772 -4.164619 -4.1491323 -4.1522985 -4.1606951 -4.1751728 -4.1981134 -4.2209034 -4.2191505][-4.20768 -4.1908774 -4.1901846 -4.2125936 -4.2313395 -4.2312641 -4.2233233 -4.2079935 -4.1951203 -4.1922722 -4.1968913 -4.2069354 -4.2197566 -4.2344604 -4.235393][-4.247004 -4.234684 -4.2349167 -4.2483525 -4.259892 -4.2618136 -4.2600918 -4.2552733 -4.2484541 -4.2462687 -4.2483974 -4.252212 -4.2556119 -4.2631531 -4.2647014][-4.2942185 -4.288516 -4.2888241 -4.2960534 -4.300981 -4.30172 -4.3010492 -4.2995453 -4.2986274 -4.2985239 -4.2990203 -4.2978787 -4.2956824 -4.298708 -4.2998075][-4.3241925 -4.3213921 -4.3211517 -4.3239446 -4.3257074 -4.3258758 -4.3257179 -4.3252454 -4.3259907 -4.3265452 -4.3263216 -4.3243337 -4.3218641 -4.3229403 -4.3233738]]...]
INFO - root - 2017-12-05 14:12:33.701719: step 17510, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 78h:55m:19s remains)
INFO - root - 2017-12-05 14:12:43.157709: step 17520, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.950 sec/batch; 83h:05m:17s remains)
INFO - root - 2017-12-05 14:12:52.542777: step 17530, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 81h:55m:11s remains)
INFO - root - 2017-12-05 14:13:01.839923: step 17540, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 81h:12m:58s remains)
INFO - root - 2017-12-05 14:13:11.140297: step 17550, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 81h:21m:30s remains)
INFO - root - 2017-12-05 14:13:20.474732: step 17560, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 81h:16m:39s remains)
INFO - root - 2017-12-05 14:13:29.818298: step 17570, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 79h:19m:31s remains)
INFO - root - 2017-12-05 14:13:39.348685: step 17580, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 84h:13m:03s remains)
INFO - root - 2017-12-05 14:13:48.808458: step 17590, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 81h:53m:18s remains)
INFO - root - 2017-12-05 14:13:57.848649: step 17600, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 82h:06m:42s remains)
2017-12-05 14:13:58.658580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22493 -4.2319169 -4.2432213 -4.252275 -4.2590103 -4.2634921 -4.2654362 -4.2632256 -4.262002 -4.2623773 -4.2640738 -4.2680345 -4.273859 -4.2767105 -4.2756572][-4.2036805 -4.2150135 -4.2327151 -4.24697 -4.2552338 -4.2583151 -4.26047 -4.2610731 -4.2630062 -4.2649655 -4.2653804 -4.2669711 -4.26824 -4.2695308 -4.2700386][-4.2090621 -4.224226 -4.2429233 -4.2567081 -4.2610431 -4.261045 -4.2629609 -4.2655115 -4.2699623 -4.2742362 -4.2750721 -4.2735152 -4.2692962 -4.2664227 -4.2667174][-4.2332664 -4.2478685 -4.2613835 -4.2675014 -4.2639122 -4.2589188 -4.2561178 -4.2576275 -4.2617488 -4.2681994 -4.2709684 -4.2669787 -4.25802 -4.2534742 -4.2558932][-4.2722254 -4.2815275 -4.2846656 -4.2792459 -4.2668848 -4.2555346 -4.2427549 -4.2342782 -4.2349606 -4.2453785 -4.2516637 -4.2481341 -4.2365689 -4.2315779 -4.2358313][-4.2864966 -4.2843251 -4.2733631 -4.2534895 -4.228137 -4.2054548 -4.1741357 -4.147697 -4.15043 -4.1745505 -4.1937647 -4.1987805 -4.1887574 -4.1851144 -4.19158][-4.2563205 -4.2364845 -4.2059851 -4.166573 -4.1213303 -4.0727205 -4.0104184 -3.9650929 -3.9857147 -4.04421 -4.0909176 -4.1125345 -4.1058221 -4.101491 -4.1108255][-4.1967235 -4.1622882 -4.1147046 -4.0576782 -3.9876776 -3.9071424 -3.8149314 -3.7557564 -3.8068981 -3.9047415 -3.9747748 -4.0097437 -4.0082626 -4.0096436 -4.0279894][-4.1507649 -4.1139655 -4.0667768 -4.0149817 -3.9524682 -3.8835227 -3.8126526 -3.7741444 -3.8334522 -3.9210382 -3.9723337 -3.9965055 -3.9915171 -3.9930089 -4.0163932][-4.14735 -4.12152 -4.091907 -4.0652733 -4.0347567 -4.005661 -3.9788673 -3.96529 -4.0065765 -4.0582 -4.0804358 -4.08562 -4.072504 -4.0693274 -4.0863547][-4.1860271 -4.1749806 -4.1641426 -4.1580834 -4.1509771 -4.1467237 -4.1433454 -4.1398792 -4.1628423 -4.1882977 -4.1962523 -4.1940536 -4.1821132 -4.1778364 -4.1860919][-4.234581 -4.2329316 -4.2334633 -4.237114 -4.2408137 -4.2459674 -4.2500257 -4.2490721 -4.2596006 -4.2708006 -4.2747946 -4.2734127 -4.2665963 -4.2618132 -4.2610269][-4.2663579 -4.2655506 -4.2674718 -4.2736149 -4.2798357 -4.2848973 -4.2878375 -4.2868457 -4.2908831 -4.2956634 -4.2984838 -4.2987094 -4.2942972 -4.2893229 -4.2840347][-4.2789359 -4.2785015 -4.2809868 -4.2861519 -4.2901845 -4.2928333 -4.2934337 -4.2913394 -4.2912884 -4.2928138 -4.2945852 -4.294867 -4.2926674 -4.2905092 -4.2877207][-4.2622013 -4.2630711 -4.2634435 -4.26535 -4.2663689 -4.266633 -4.2661629 -4.26471 -4.2647395 -4.2662749 -4.2682176 -4.2693262 -4.2693686 -4.2700086 -4.2707391]]...]
INFO - root - 2017-12-05 14:14:08.052534: step 17610, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 82h:18m:39s remains)
INFO - root - 2017-12-05 14:14:17.449883: step 17620, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 83h:25m:14s remains)
INFO - root - 2017-12-05 14:14:26.754845: step 17630, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.898 sec/batch; 78h:33m:47s remains)
INFO - root - 2017-12-05 14:14:36.188109: step 17640, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 84h:24m:47s remains)
INFO - root - 2017-12-05 14:14:45.533710: step 17650, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 77h:46m:28s remains)
INFO - root - 2017-12-05 14:14:54.968117: step 17660, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 79h:53m:58s remains)
INFO - root - 2017-12-05 14:15:04.226665: step 17670, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 75h:58m:45s remains)
INFO - root - 2017-12-05 14:15:13.519391: step 17680, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 81h:38m:18s remains)
INFO - root - 2017-12-05 14:15:23.022667: step 17690, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 85h:44m:35s remains)
INFO - root - 2017-12-05 14:15:32.383036: step 17700, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 82h:33m:23s remains)
2017-12-05 14:15:33.107207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2702456 -4.2689686 -4.2607131 -4.242197 -4.2293181 -4.2294741 -4.2403908 -4.2639494 -4.2771525 -4.2661533 -4.2462244 -4.2372651 -4.2552905 -4.2780743 -4.28246][-4.2538543 -4.2504854 -4.240355 -4.221118 -4.2101245 -4.215323 -4.2266865 -4.2447209 -4.2476678 -4.2334704 -4.2219911 -4.22197 -4.2472734 -4.2753506 -4.2839713][-4.2332458 -4.2288947 -4.2170954 -4.1977587 -4.1925082 -4.2051582 -4.21849 -4.2280345 -4.2208185 -4.2108474 -4.2123957 -4.2261434 -4.2554817 -4.2819819 -4.287056][-4.2068419 -4.2069364 -4.1935968 -4.1703558 -4.1641784 -4.18023 -4.1942992 -4.19717 -4.1889281 -4.1882744 -4.2035232 -4.230144 -4.2610989 -4.2822337 -4.281157][-4.192028 -4.1969752 -4.1813126 -4.1559205 -4.1500349 -4.16472 -4.1755629 -4.1740575 -4.171196 -4.17968 -4.2043667 -4.2364841 -4.26799 -4.281631 -4.2719274][-4.2054396 -4.2078066 -4.1879158 -4.1607528 -4.1528578 -4.1688504 -4.1816325 -4.1811008 -4.1846209 -4.1986508 -4.2163377 -4.2359676 -4.2606421 -4.2741575 -4.2673149][-4.2110624 -4.201983 -4.1785541 -4.1532078 -4.1423554 -4.1554708 -4.1710334 -4.1790733 -4.1905808 -4.2062664 -4.212935 -4.215559 -4.2345657 -4.2589774 -4.2623806][-4.1943173 -4.1797323 -4.1609306 -4.1397657 -4.1257939 -4.1323504 -4.1466379 -4.1620646 -4.1827807 -4.1994548 -4.2012424 -4.1972356 -4.2172756 -4.2464733 -4.2514744][-4.1661968 -4.1534839 -4.1525645 -4.1454711 -4.1330619 -4.1347289 -4.1452909 -4.1643414 -4.1857662 -4.1989775 -4.1976981 -4.197783 -4.2173233 -4.2445059 -4.2494555][-4.1555014 -4.1451192 -4.1562262 -4.1620488 -4.1587815 -4.1609488 -4.1650791 -4.178205 -4.1937532 -4.2006359 -4.1976523 -4.1986637 -4.2138453 -4.2363133 -4.2452989][-4.1686916 -4.158144 -4.1669345 -4.1747918 -4.1779747 -4.1809716 -4.1802115 -4.1864982 -4.1978321 -4.2038555 -4.198782 -4.1962905 -4.2043886 -4.2239566 -4.2358313][-4.1866817 -4.1839561 -4.186348 -4.1864181 -4.1863012 -4.1904421 -4.1900892 -4.1911154 -4.19988 -4.2125511 -4.210875 -4.2045884 -4.1991286 -4.2090273 -4.2183442][-4.1901026 -4.2014346 -4.2030139 -4.1951795 -4.1909909 -4.1945467 -4.1969228 -4.1943116 -4.1975608 -4.2112336 -4.2146926 -4.2050724 -4.1857476 -4.1785355 -4.1803451][-4.1801438 -4.2041211 -4.2076344 -4.1993175 -4.1979437 -4.2045569 -4.2077613 -4.2004471 -4.1928921 -4.1967297 -4.2011657 -4.1923585 -4.1675038 -4.1470408 -4.1391068][-4.1644473 -4.192256 -4.1971726 -4.187542 -4.1911969 -4.2045922 -4.2092276 -4.1999078 -4.1842103 -4.175416 -4.176158 -4.17078 -4.1518407 -4.1303835 -4.1171818]]...]
INFO - root - 2017-12-05 14:15:42.555466: step 17710, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 75h:38m:40s remains)
INFO - root - 2017-12-05 14:15:51.749047: step 17720, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 87h:47m:41s remains)
INFO - root - 2017-12-05 14:16:01.138481: step 17730, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 84h:37m:05s remains)
INFO - root - 2017-12-05 14:16:10.719950: step 17740, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 78h:32m:52s remains)
INFO - root - 2017-12-05 14:16:19.943532: step 17750, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 83h:59m:02s remains)
INFO - root - 2017-12-05 14:16:29.306340: step 17760, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 77h:30m:50s remains)
INFO - root - 2017-12-05 14:16:38.728192: step 17770, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 83h:34m:17s remains)
INFO - root - 2017-12-05 14:16:48.211261: step 17780, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 85h:13m:49s remains)
INFO - root - 2017-12-05 14:16:57.465591: step 17790, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 81h:00m:52s remains)
INFO - root - 2017-12-05 14:17:06.781112: step 17800, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 82h:33m:56s remains)
2017-12-05 14:17:07.586276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2777963 -4.2458382 -4.2225747 -4.217917 -4.2264705 -4.2371397 -4.2445283 -4.2507119 -4.2563815 -4.2611938 -4.2670879 -4.2683072 -4.2604513 -4.2521353 -4.2522426][-4.2719855 -4.2395539 -4.2216444 -4.2249103 -4.2353458 -4.240706 -4.2394896 -4.2381096 -4.2383943 -4.2405381 -4.2432737 -4.2393875 -4.2277784 -4.221509 -4.2280984][-4.2719 -4.2441621 -4.2346468 -4.2417641 -4.2476125 -4.2399974 -4.2237611 -4.2107668 -4.204895 -4.2063956 -4.2100229 -4.205092 -4.1969843 -4.1994157 -4.2160592][-4.2769985 -4.2566938 -4.2525926 -4.2570543 -4.2529688 -4.2276292 -4.191412 -4.1632304 -4.1533456 -4.1635842 -4.1763825 -4.1785841 -4.180871 -4.1949492 -4.2204647][-4.2878871 -4.2724113 -4.2686896 -4.2664557 -4.2484684 -4.2025328 -4.1444321 -4.1010289 -4.0974722 -4.1286936 -4.157135 -4.17166 -4.1856713 -4.2086415 -4.235898][-4.2947164 -4.2800422 -4.2732072 -4.2615786 -4.2261891 -4.1597939 -4.0817108 -4.0335493 -4.05729 -4.1179333 -4.1602335 -4.1815233 -4.1989808 -4.221458 -4.2436414][-4.2890992 -4.272161 -4.2609067 -4.2383275 -4.1871967 -4.1077957 -4.0249548 -3.9978657 -4.0590615 -4.1357112 -4.1775169 -4.193573 -4.2041459 -4.2191706 -4.235785][-4.2805943 -4.2606549 -4.2423139 -4.2101936 -4.152328 -4.0794244 -4.0253377 -4.0367522 -4.1060562 -4.1689353 -4.1943 -4.19651 -4.1950979 -4.2038636 -4.2210803][-4.2799578 -4.2560382 -4.2309008 -4.194314 -4.1399808 -4.088932 -4.0728049 -4.1038551 -4.1568484 -4.1947031 -4.2026434 -4.1926904 -4.180397 -4.1850829 -4.205668][-4.2791529 -4.24962 -4.2175884 -4.1804976 -4.1382031 -4.1129074 -4.1211214 -4.1538095 -4.1884241 -4.2049155 -4.2000632 -4.18364 -4.1666822 -4.1690383 -4.1937366][-4.271379 -4.2357683 -4.199235 -4.1641345 -4.133769 -4.1267586 -4.1462665 -4.1771812 -4.1999922 -4.2029653 -4.1899447 -4.1705813 -4.1538477 -4.156796 -4.182631][-4.2510304 -4.2147341 -4.179419 -4.1473966 -4.1234059 -4.1226377 -4.14481 -4.1741295 -4.1924319 -4.1894994 -4.1730452 -4.1544938 -4.1421351 -4.1463308 -4.1678977][-4.2319212 -4.19868 -4.1674623 -4.1377482 -4.1138988 -4.1111584 -4.1307287 -4.1570964 -4.1730337 -4.1696997 -4.1559186 -4.1426215 -4.13415 -4.1378555 -4.1537576][-4.2204628 -4.1909 -4.1630397 -4.1351595 -4.1134286 -4.1099844 -4.1241655 -4.1452861 -4.1591039 -4.157033 -4.1477313 -4.1402144 -4.1357551 -4.1397657 -4.1518807][-4.2210364 -4.1950412 -4.171721 -4.1484818 -4.1319318 -4.1285982 -4.1360159 -4.151052 -4.1622376 -4.1603665 -4.1542149 -4.1507006 -4.1496606 -4.1545634 -4.1632276]]...]
INFO - root - 2017-12-05 14:17:16.914516: step 17810, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 81h:27m:21s remains)
INFO - root - 2017-12-05 14:17:26.170720: step 17820, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 77h:34m:23s remains)
INFO - root - 2017-12-05 14:17:35.364146: step 17830, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 76h:54m:45s remains)
INFO - root - 2017-12-05 14:17:44.752632: step 17840, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 76h:31m:30s remains)
INFO - root - 2017-12-05 14:17:54.157977: step 17850, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 80h:35m:15s remains)
INFO - root - 2017-12-05 14:18:03.576591: step 17860, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 80h:28m:08s remains)
INFO - root - 2017-12-05 14:18:13.026483: step 17870, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 82h:44m:49s remains)
INFO - root - 2017-12-05 14:18:22.489125: step 17880, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 82h:37m:41s remains)
INFO - root - 2017-12-05 14:18:31.961451: step 17890, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 76h:48m:28s remains)
INFO - root - 2017-12-05 14:18:41.064912: step 17900, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 84h:23m:53s remains)
2017-12-05 14:18:41.797496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2014894 -4.2077951 -4.2285838 -4.2447429 -4.2437344 -4.229897 -4.2143855 -4.1959791 -4.183599 -4.1636915 -4.1440024 -4.1476536 -4.1703568 -4.1958227 -4.2098436][-4.2078147 -4.2080517 -4.22481 -4.2431865 -4.2431974 -4.2278018 -4.2081895 -4.18929 -4.1806822 -4.1678581 -4.1551924 -4.1579638 -4.1783848 -4.20374 -4.2152305][-4.2285528 -4.2224317 -4.2316904 -4.2438054 -4.2415361 -4.2284565 -4.2147841 -4.2050028 -4.2039609 -4.20097 -4.1943955 -4.1938486 -4.2074742 -4.2256751 -4.2281079][-4.2594981 -4.2549849 -4.2558327 -4.254755 -4.2447996 -4.2311234 -4.221127 -4.218648 -4.2241654 -4.231566 -4.2314768 -4.2277689 -4.2360158 -4.2473259 -4.246284][-4.2597013 -4.2622933 -4.2622604 -4.2525558 -4.2360988 -4.2217922 -4.2118697 -4.211309 -4.2211514 -4.2344842 -4.2408834 -4.2405653 -4.2490621 -4.2584748 -4.2573013][-4.2146773 -4.2287726 -4.2371769 -4.2264519 -4.2088304 -4.2000928 -4.1942368 -4.1945438 -4.203835 -4.2195969 -4.2295876 -4.2331491 -4.2425513 -4.2534194 -4.2569427][-4.14723 -4.1776242 -4.1975622 -4.1886678 -4.1750584 -4.1729527 -4.1746626 -4.1803274 -4.1922283 -4.2077551 -4.2202306 -4.2281866 -4.2350225 -4.2447572 -4.2540283][-4.1068759 -4.1446252 -4.1662703 -4.1527996 -4.1383328 -4.1430173 -4.1532774 -4.1684151 -4.1874332 -4.19921 -4.2092357 -4.2196503 -4.2255592 -4.2328286 -4.2459078][-4.1218615 -4.1471596 -4.1571646 -4.139225 -4.1239157 -4.1306343 -4.1414647 -4.1578593 -4.1737995 -4.1769943 -4.1816344 -4.192071 -4.197402 -4.2071738 -4.2260342][-4.1592188 -4.1711044 -4.1748013 -4.1624174 -4.1547422 -4.1599951 -4.1634154 -4.170681 -4.1771021 -4.1666932 -4.1559329 -4.1542358 -4.1561422 -4.1685305 -4.1936231][-4.1672468 -4.1749778 -4.1781578 -4.1764636 -4.1792192 -4.1857476 -4.1834068 -4.1822281 -4.1802554 -4.1583271 -4.1285539 -4.1057615 -4.1006484 -4.1143 -4.1439381][-4.1444426 -4.1583204 -4.1676388 -4.1753531 -4.1840339 -4.1884031 -4.1822538 -4.1773968 -4.1705842 -4.1418881 -4.0984025 -4.063343 -4.0535626 -4.0680728 -4.101079][-4.1673045 -4.1828957 -4.1928482 -4.1977372 -4.2007656 -4.1981764 -4.1888752 -4.1834216 -4.17831 -4.1511607 -4.1063743 -4.0691566 -4.0593162 -4.072547 -4.1003036][-4.2086058 -4.2202845 -4.2282586 -4.2307034 -4.23048 -4.2243485 -4.2152371 -4.2108059 -4.2079096 -4.185854 -4.1446705 -4.111557 -4.1043792 -4.1156178 -4.1333303][-4.230576 -4.237401 -4.2418165 -4.2438989 -4.2424583 -4.2366366 -4.2305746 -4.2295136 -4.2297635 -4.2133527 -4.1780424 -4.1512971 -4.1483774 -4.1595931 -4.1721773]]...]
INFO - root - 2017-12-05 14:18:51.037165: step 17910, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 84h:30m:49s remains)
INFO - root - 2017-12-05 14:19:00.419961: step 17920, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 80h:36m:58s remains)
INFO - root - 2017-12-05 14:19:09.687958: step 17930, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.979 sec/batch; 85h:30m:45s remains)
INFO - root - 2017-12-05 14:19:18.957549: step 17940, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 83h:59m:36s remains)
INFO - root - 2017-12-05 14:19:28.433219: step 17950, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 77h:06m:39s remains)
INFO - root - 2017-12-05 14:19:37.829337: step 17960, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 84h:09m:55s remains)
INFO - root - 2017-12-05 14:19:47.089676: step 17970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 74h:49m:59s remains)
INFO - root - 2017-12-05 14:19:56.228199: step 17980, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 76h:53m:40s remains)
INFO - root - 2017-12-05 14:20:05.592019: step 17990, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 80h:52m:08s remains)
INFO - root - 2017-12-05 14:20:14.888830: step 18000, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.990 sec/batch; 86h:27m:30s remains)
2017-12-05 14:20:15.646014: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3403592 -4.3340068 -4.3243752 -4.3137488 -4.3029051 -4.2899318 -4.2815948 -4.2826076 -4.2889214 -4.29953 -4.3077579 -4.3098469 -4.3082623 -4.3014674 -4.28806][-4.3371778 -4.3275657 -4.3130703 -4.298512 -4.2837276 -4.26951 -4.2633533 -4.2659717 -4.2756991 -4.2923231 -4.3084216 -4.3144259 -4.3134451 -4.3046756 -4.2892475][-4.3343768 -4.321259 -4.3016791 -4.2840514 -4.2644634 -4.247519 -4.242135 -4.2461934 -4.2610297 -4.2791681 -4.2973084 -4.3055396 -4.3049045 -4.2961631 -4.2832375][-4.3337035 -4.3189116 -4.2963276 -4.2766843 -4.2539282 -4.2334905 -4.2252078 -4.228879 -4.2466297 -4.2650518 -4.2804604 -4.2893219 -4.2908735 -4.2842975 -4.2771316][-4.3339863 -4.3209343 -4.2990074 -4.2795272 -4.2540064 -4.2293248 -4.2160921 -4.2176623 -4.233953 -4.2511249 -4.2662086 -4.279366 -4.2881389 -4.2861857 -4.2839341][-4.3338747 -4.3242836 -4.3080192 -4.2904816 -4.2611365 -4.230834 -4.2112942 -4.2048354 -4.2130013 -4.2304859 -4.2520227 -4.2743979 -4.2921705 -4.2995944 -4.302331][-4.3318558 -4.3248363 -4.3145642 -4.2976623 -4.2637906 -4.2271929 -4.1991944 -4.183414 -4.1831689 -4.2019615 -4.2332568 -4.2677193 -4.29544 -4.3092442 -4.3146982][-4.3291531 -4.320816 -4.310267 -4.2889948 -4.2481866 -4.2039833 -4.1620111 -4.1362357 -4.1323118 -4.1584239 -4.2046113 -4.254138 -4.2932363 -4.3125496 -4.3188891][-4.3277483 -4.3167729 -4.3012261 -4.2739511 -4.2242517 -4.166255 -4.1040621 -4.0650821 -4.063272 -4.1042275 -4.1669917 -4.23096 -4.2765093 -4.3001366 -4.30568][-4.3278809 -4.3162794 -4.2979555 -4.2684031 -4.211143 -4.1373858 -4.0555468 -4.0038433 -4.0024171 -4.0561986 -4.1338496 -4.2074208 -4.2553754 -4.2787132 -4.2832093][-4.3283024 -4.3175464 -4.2997546 -4.2727027 -4.2176065 -4.1444073 -4.059792 -3.9996157 -3.9912839 -4.0485206 -4.1306791 -4.2009978 -4.2444153 -4.2610006 -4.2592626][-4.3286657 -4.3177476 -4.3012352 -4.278327 -4.2364421 -4.1806216 -4.1135378 -4.0575314 -4.0439944 -4.092989 -4.1650944 -4.2215662 -4.254159 -4.2655234 -4.2596912][-4.3275418 -4.3140979 -4.2979403 -4.2810621 -4.2523408 -4.215569 -4.1729755 -4.1344137 -4.1228142 -4.1604309 -4.2169037 -4.2580066 -4.2777824 -4.285357 -4.2787528][-4.3260303 -4.3095627 -4.2941866 -4.2815986 -4.2628069 -4.2407875 -4.2218032 -4.2050343 -4.2025795 -4.2315092 -4.2735238 -4.3008132 -4.3093209 -4.3114166 -4.3044162][-4.3259254 -4.3056293 -4.2896404 -4.279923 -4.2696452 -4.2609615 -4.2576957 -4.2572131 -4.26619 -4.2916017 -4.3213568 -4.3368788 -4.3354645 -4.3294215 -4.3198304]]...]
INFO - root - 2017-12-05 14:20:25.006842: step 18010, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 81h:35m:48s remains)
INFO - root - 2017-12-05 14:20:34.400636: step 18020, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 81h:12m:08s remains)
INFO - root - 2017-12-05 14:20:43.819892: step 18030, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 81h:44m:42s remains)
INFO - root - 2017-12-05 14:20:53.163267: step 18040, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.916 sec/batch; 80h:00m:20s remains)
INFO - root - 2017-12-05 14:21:02.368545: step 18050, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 80h:33m:20s remains)
INFO - root - 2017-12-05 14:21:11.718425: step 18060, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.979 sec/batch; 85h:28m:41s remains)
INFO - root - 2017-12-05 14:21:20.822124: step 18070, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 76h:14m:14s remains)
INFO - root - 2017-12-05 14:21:30.024934: step 18080, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 80h:27m:40s remains)
INFO - root - 2017-12-05 14:21:39.398233: step 18090, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 83h:36m:17s remains)
INFO - root - 2017-12-05 14:21:48.708935: step 18100, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 79h:47m:36s remains)
2017-12-05 14:21:49.490256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2921329 -4.2863646 -4.2818308 -4.2821093 -4.2827163 -4.2806644 -4.27628 -4.274642 -4.2800274 -4.2813163 -4.2740726 -4.2667928 -4.264596 -4.2658548 -4.27006][-4.2475262 -4.2405267 -4.2353764 -4.2332077 -4.2309995 -4.2288365 -4.2228131 -4.2189927 -4.2270088 -4.2323904 -4.2247291 -4.2139897 -4.2072167 -4.2061052 -4.2092829][-4.2138662 -4.2075 -4.202539 -4.1977067 -4.1949444 -4.1942954 -4.1841264 -4.1729126 -4.1770139 -4.186945 -4.1845913 -4.1772313 -4.1715479 -4.1708174 -4.1741338][-4.1980748 -4.194232 -4.191627 -4.18822 -4.1903915 -4.1917586 -4.176414 -4.1548891 -4.1535516 -4.1665449 -4.1686816 -4.16704 -4.1672482 -4.1708784 -4.178421][-4.196857 -4.1922655 -4.1868782 -4.1830826 -4.1867247 -4.1871915 -4.1709995 -4.1458349 -4.1437197 -4.1546283 -4.1517968 -4.14793 -4.1534328 -4.1657033 -4.1814537][-4.2062392 -4.200695 -4.1865273 -4.174952 -4.1730604 -4.1712685 -4.1549106 -4.1298785 -4.1235623 -4.1236567 -4.1175823 -4.1145358 -4.1209326 -4.1372652 -4.1553364][-4.1879449 -4.1836629 -4.1657209 -4.1459923 -4.1343355 -4.1265697 -4.1074672 -4.0804615 -4.0667162 -4.05892 -4.0595565 -4.0664034 -4.0766516 -4.0891604 -4.1001644][-4.1552014 -4.1485133 -4.1320887 -4.1114821 -4.0914755 -4.0743742 -4.0507541 -4.0242419 -4.0054 -3.9973001 -4.0124593 -4.0357265 -4.0550246 -4.0645766 -4.0651312][-4.1475315 -4.1371861 -4.1231837 -4.1076994 -4.0887671 -4.0739942 -4.0524735 -4.0292153 -4.0105462 -4.0055981 -4.0291872 -4.0597749 -4.084589 -4.0955582 -4.0943823][-4.1662407 -4.1564007 -4.1450129 -4.1367035 -4.126596 -4.1206098 -4.1069546 -4.0890756 -4.0728683 -4.0675249 -4.0821018 -4.1069345 -4.1310115 -4.1408281 -4.1394253][-4.1940861 -4.1898465 -4.1855016 -4.1864495 -4.1857553 -4.1873493 -4.1784582 -4.1630149 -4.1495986 -4.1409888 -4.1433911 -4.1563611 -4.17318 -4.1758456 -4.1713796][-4.1966963 -4.2000837 -4.2051048 -4.2139831 -4.2205849 -4.2291913 -4.2291117 -4.2222619 -4.2132897 -4.2027712 -4.1951694 -4.1953511 -4.1976905 -4.1886125 -4.1774011][-4.1733627 -4.1835952 -4.1966805 -4.2138371 -4.2290177 -4.2430782 -4.2499833 -4.2509012 -4.2449503 -4.2313619 -4.2166772 -4.2078328 -4.2021856 -4.1884584 -4.1746283][-4.1513491 -4.1626534 -4.1759658 -4.19129 -4.2043924 -4.2175951 -4.2273016 -4.2317333 -4.229629 -4.219501 -4.2089443 -4.2007022 -4.1921377 -4.1755028 -4.1610837][-4.1636343 -4.1737614 -4.179183 -4.1861339 -4.1924033 -4.1990638 -4.2032495 -4.2077327 -4.2121181 -4.21053 -4.2076445 -4.2068815 -4.2016339 -4.1849775 -4.170526]]...]
INFO - root - 2017-12-05 14:21:58.666711: step 18110, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 79h:03m:00s remains)
INFO - root - 2017-12-05 14:22:08.078897: step 18120, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 83h:26m:11s remains)
INFO - root - 2017-12-05 14:22:17.498094: step 18130, loss = 2.01, batch loss = 1.95 (9.4 examples/sec; 0.847 sec/batch; 73h:58m:51s remains)
INFO - root - 2017-12-05 14:22:26.767089: step 18140, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 83h:46m:51s remains)
INFO - root - 2017-12-05 14:22:36.069584: step 18150, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 86h:15m:11s remains)
INFO - root - 2017-12-05 14:22:45.525498: step 18160, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 84h:35m:33s remains)
INFO - root - 2017-12-05 14:22:55.014803: step 18170, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 79h:49m:22s remains)
INFO - root - 2017-12-05 14:23:04.221549: step 18180, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.822 sec/batch; 71h:46m:05s remains)
INFO - root - 2017-12-05 14:23:13.484768: step 18190, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 86h:29m:36s remains)
INFO - root - 2017-12-05 14:23:22.988845: step 18200, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 82h:09m:52s remains)
2017-12-05 14:23:23.829494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1292357 -4.1347947 -4.1532164 -4.1668081 -4.1679358 -4.158978 -4.1649709 -4.1699495 -4.1464977 -4.1151762 -4.0882955 -4.0639577 -4.0495353 -4.0416417 -4.0386591][-4.1258521 -4.1369705 -4.1601262 -4.1741052 -4.168951 -4.1572647 -4.1633034 -4.1668868 -4.1462922 -4.1130729 -4.0798726 -4.0562019 -4.0446148 -4.0400643 -4.0447478][-4.11779 -4.1270118 -4.147181 -4.1548252 -4.143301 -4.1265187 -4.1303697 -4.1350994 -4.1273623 -4.1003633 -4.0625744 -4.0406842 -4.0367341 -4.0402341 -4.0537124][-4.1121483 -4.1194348 -4.1330824 -4.1289625 -4.10465 -4.07643 -4.0730214 -4.0772042 -4.0848436 -4.07036 -4.03633 -4.0215197 -4.0247421 -4.0344834 -4.0542536][-4.1215429 -4.1269941 -4.1333957 -4.1184149 -4.0775971 -4.0303359 -4.0099578 -4.0074816 -4.0316181 -4.0322294 -4.0091472 -4.0083771 -4.0195856 -4.0337691 -4.0541253][-4.1301613 -4.1304111 -4.1342058 -4.1152658 -4.0630093 -3.9970658 -3.9561617 -3.9421704 -3.9821963 -4.0024772 -3.9992707 -4.0112987 -4.019649 -4.0267758 -4.0454574][-4.1311507 -4.1217852 -4.11862 -4.1020956 -4.0523272 -3.9795809 -3.9211102 -3.8972378 -3.9518986 -3.9866488 -3.9981172 -4.0195422 -4.0218358 -4.0208416 -4.0358987][-4.1309381 -4.1129789 -4.1032934 -4.0941978 -4.0595222 -3.9993165 -3.9466083 -3.9290562 -3.9810238 -4.00531 -4.0147476 -4.0339274 -4.0301347 -4.0283475 -4.03898][-4.1455522 -4.1227603 -4.1069679 -4.1052089 -4.0862513 -4.0477319 -4.0157013 -4.0113373 -4.0434833 -4.046701 -4.0485234 -4.0627923 -4.0568714 -4.0554943 -4.0636377][-4.1477118 -4.1299257 -4.1128945 -4.114502 -4.1076279 -4.0903311 -4.077177 -4.0826383 -4.1018138 -4.0962982 -4.0957036 -4.1043692 -4.0965548 -4.0933948 -4.10055][-4.1426668 -4.1407118 -4.1307 -4.134336 -4.1386819 -4.1356969 -4.1327777 -4.1366353 -4.1473436 -4.1465864 -4.152132 -4.159585 -4.1510248 -4.1446567 -4.14762][-4.1545367 -4.1609464 -4.1578264 -4.1610928 -4.1656594 -4.1650281 -4.1664715 -4.1730328 -4.1828637 -4.1906676 -4.2039542 -4.2123528 -4.2054362 -4.19972 -4.2004261][-4.1727853 -4.1812811 -4.1802979 -4.1766071 -4.1724882 -4.1688952 -4.173306 -4.1825166 -4.191802 -4.2057905 -4.2256889 -4.236238 -4.2295237 -4.222806 -4.2227044][-4.1948104 -4.2044187 -4.2029605 -4.19362 -4.182128 -4.1737661 -4.17514 -4.1840992 -4.19484 -4.2121911 -4.2354355 -4.2479234 -4.2414875 -4.2325048 -4.2313447][-4.2136621 -4.2168427 -4.2127 -4.2056456 -4.195725 -4.1901875 -4.1912565 -4.1997166 -4.21275 -4.2291679 -4.2479367 -4.2587953 -4.2559977 -4.24956 -4.2475033]]...]
INFO - root - 2017-12-05 14:23:33.293908: step 18210, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 86h:28m:20s remains)
INFO - root - 2017-12-05 14:23:42.643870: step 18220, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 79h:55m:54s remains)
INFO - root - 2017-12-05 14:23:51.902098: step 18230, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.861 sec/batch; 75h:10m:33s remains)
INFO - root - 2017-12-05 14:24:01.340304: step 18240, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.936 sec/batch; 81h:40m:38s remains)
INFO - root - 2017-12-05 14:24:10.753715: step 18250, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 81h:02m:31s remains)
INFO - root - 2017-12-05 14:24:20.216630: step 18260, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 83h:56m:41s remains)
INFO - root - 2017-12-05 14:24:29.483138: step 18270, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 80h:39m:38s remains)
INFO - root - 2017-12-05 14:24:38.843923: step 18280, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 83h:57m:38s remains)
INFO - root - 2017-12-05 14:24:48.251934: step 18290, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 84h:22m:17s remains)
INFO - root - 2017-12-05 14:24:57.714054: step 18300, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 85h:40m:57s remains)
2017-12-05 14:24:58.440353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1726618 -4.1920347 -4.2174091 -4.2458882 -4.2698722 -4.2811608 -4.2771053 -4.2664347 -4.260807 -4.2636771 -4.27044 -4.2804322 -4.2927332 -4.3064642 -4.3202252][-4.2034016 -4.2300434 -4.2540879 -4.2713909 -4.2794447 -4.2715888 -4.2496638 -4.2284541 -4.2242713 -4.2372761 -4.2563214 -4.2770796 -4.2936382 -4.3077254 -4.3208466][-4.2259092 -4.2564821 -4.2769928 -4.2825837 -4.2728362 -4.2422347 -4.1969848 -4.1610155 -4.1582813 -4.1848192 -4.2222662 -4.2600508 -4.2879148 -4.3071671 -4.3223963][-4.2410321 -4.2736235 -4.2902813 -4.2873292 -4.2623043 -4.2100987 -4.1402206 -4.0869646 -4.0842962 -4.1248889 -4.1817985 -4.2378707 -4.2782397 -4.304646 -4.3240638][-4.2507734 -4.2848949 -4.2990608 -4.2889881 -4.2515335 -4.1804867 -4.0870442 -4.0130987 -4.0084882 -4.0642447 -4.1410327 -4.2147131 -4.2668157 -4.2987304 -4.3210988][-4.2514081 -4.2833853 -4.2955894 -4.2794991 -4.2313957 -4.14606 -4.0321603 -3.9337826 -3.921325 -3.9960763 -4.0974865 -4.1908336 -4.2552204 -4.2915769 -4.3146119][-4.2425117 -4.2688479 -4.277575 -4.2539377 -4.1957579 -4.1009283 -3.9726961 -3.8466463 -3.8206062 -3.9199338 -4.0538549 -4.1684651 -4.2454252 -4.2860909 -4.3102288][-4.2293139 -4.2495112 -4.2556329 -4.2290111 -4.1691141 -4.0727787 -3.9389837 -3.792274 -3.7471519 -3.8633976 -4.0212955 -4.1495709 -4.234787 -4.2793574 -4.3048525][-4.2243319 -4.2415938 -4.2501426 -4.2301893 -4.1824746 -4.1024904 -3.9837742 -3.8470514 -3.7938385 -3.8901985 -4.0307331 -4.1478305 -4.2285419 -4.2726173 -4.2965088][-4.2222891 -4.2404866 -4.2551246 -4.2487488 -4.2226863 -4.1728187 -4.0913062 -3.9940341 -3.9490457 -4.0033159 -4.0945225 -4.1728778 -4.231812 -4.2671256 -4.2879305][-4.2063646 -4.2264767 -4.246068 -4.2514944 -4.2457275 -4.2242146 -4.1788554 -4.1205859 -4.0886297 -4.1099458 -4.1531057 -4.191093 -4.2244377 -4.2483783 -4.2660513][-4.1740103 -4.18974 -4.2101817 -4.2238955 -4.236907 -4.239851 -4.2227492 -4.1937881 -4.1738596 -4.1735382 -4.1771326 -4.1783366 -4.1880488 -4.2020659 -4.216536][-4.1313314 -4.140749 -4.1607018 -4.1797428 -4.2048345 -4.2263479 -4.2287154 -4.2165804 -4.2029858 -4.1895871 -4.1663051 -4.1393695 -4.1312118 -4.1382761 -4.1517968][-4.107357 -4.117331 -4.136425 -4.1542597 -4.18426 -4.2165637 -4.231648 -4.2296658 -4.2172217 -4.193121 -4.1535921 -4.11378 -4.1007004 -4.107975 -4.1205559][-4.1289124 -4.1410956 -4.1586833 -4.1712804 -4.1984038 -4.2317142 -4.249948 -4.2505941 -4.2387824 -4.2152042 -4.1777043 -4.13969 -4.1296768 -4.1384926 -4.1467566]]...]
INFO - root - 2017-12-05 14:25:07.853061: step 18310, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 81h:31m:39s remains)
INFO - root - 2017-12-05 14:25:17.335695: step 18320, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 85h:28m:47s remains)
INFO - root - 2017-12-05 14:25:26.735411: step 18330, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 82h:24m:41s remains)
INFO - root - 2017-12-05 14:25:35.936092: step 18340, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 79h:30m:36s remains)
INFO - root - 2017-12-05 14:25:45.274120: step 18350, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 82h:01m:49s remains)
INFO - root - 2017-12-05 14:25:54.599046: step 18360, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 75h:28m:52s remains)
INFO - root - 2017-12-05 14:26:03.866090: step 18370, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 82h:33m:28s remains)
INFO - root - 2017-12-05 14:26:13.330765: step 18380, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 82h:20m:55s remains)
INFO - root - 2017-12-05 14:26:22.732147: step 18390, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 77h:27m:49s remains)
INFO - root - 2017-12-05 14:26:32.188080: step 18400, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 84h:25m:21s remains)
2017-12-05 14:26:32.977531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2157454 -4.1989646 -4.182332 -4.1690907 -4.1684079 -4.1854019 -4.2008042 -4.2074947 -4.2092085 -4.2216325 -4.2393456 -4.2490983 -4.2490258 -4.2533865 -4.2641253][-4.2584643 -4.2432833 -4.2232323 -4.2047358 -4.2031507 -4.2198315 -4.237052 -4.2510972 -4.2604289 -4.2692337 -4.273972 -4.2663 -4.2512026 -4.2489357 -4.2566514][-4.2779841 -4.2745528 -4.2615142 -4.2491493 -4.2444272 -4.2515264 -4.2606535 -4.2725372 -4.2810011 -4.28205 -4.274322 -4.2523432 -4.2303247 -4.2276168 -4.239922][-4.2702026 -4.2796636 -4.2763009 -4.2717614 -4.2675824 -4.2636428 -4.2619128 -4.2647061 -4.268959 -4.2628727 -4.2456446 -4.2199445 -4.20194 -4.2071066 -4.2289581][-4.240416 -4.2525439 -4.2579679 -4.2599649 -4.2561903 -4.2484283 -4.2402296 -4.2351213 -4.2316222 -4.2195883 -4.1988544 -4.1782608 -4.1720138 -4.1918783 -4.22566][-4.2041593 -4.2199802 -4.2290549 -4.2307553 -4.2246761 -4.2183065 -4.206861 -4.1954908 -4.188241 -4.17399 -4.1569839 -4.1462259 -4.1580977 -4.194572 -4.2326436][-4.1727338 -4.1950917 -4.2070303 -4.2016406 -4.1905384 -4.181788 -4.1649556 -4.1438646 -4.1327381 -4.124527 -4.1226 -4.1325154 -4.1651573 -4.2098007 -4.2442212][-4.1572862 -4.1785822 -4.1818285 -4.1626968 -4.1387544 -4.122344 -4.1027389 -4.0789781 -4.0643392 -4.06266 -4.0819392 -4.1213484 -4.1730623 -4.2209983 -4.2518134][-4.1545653 -4.16286 -4.1509228 -4.1160669 -4.077455 -4.0518956 -4.0328946 -4.0171971 -4.0061359 -4.0105138 -4.0487466 -4.11022 -4.1730347 -4.2222395 -4.2523789][-4.1553421 -4.1579113 -4.139822 -4.0953565 -4.0471926 -4.01928 -4.012229 -4.0104332 -4.0067425 -4.0199862 -4.0655661 -4.1233206 -4.1789751 -4.2241549 -4.2541685][-4.1640663 -4.1705775 -4.1530666 -4.109786 -4.0666857 -4.0488939 -4.0574245 -4.0697021 -4.0733171 -4.0900478 -4.126359 -4.1639023 -4.1981134 -4.2328315 -4.2604485][-4.1748314 -4.1861744 -4.1759934 -4.1456909 -4.1187825 -4.1183476 -4.1391692 -4.1551476 -4.1592708 -4.1705251 -4.1890273 -4.2027349 -4.2188125 -4.2459464 -4.2711897][-4.1916032 -4.2023587 -4.2008624 -4.1880965 -4.1790047 -4.1893015 -4.2128687 -4.2252955 -4.2253633 -4.2263961 -4.2293177 -4.229352 -4.2384868 -4.2624364 -4.2842021][-4.2290277 -4.2354665 -4.2382913 -4.2380972 -4.2408848 -4.2527385 -4.2675829 -4.2718511 -4.2651186 -4.2577286 -4.2539015 -4.2488971 -4.2551918 -4.277523 -4.2976041][-4.2777686 -4.2770662 -4.2781858 -4.2817383 -4.2868738 -4.2953043 -4.3020635 -4.2996078 -4.2876825 -4.2776589 -4.2732835 -4.266325 -4.2694335 -4.2897773 -4.3086677]]...]
INFO - root - 2017-12-05 14:26:42.264676: step 18410, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.966 sec/batch; 84h:18m:19s remains)
INFO - root - 2017-12-05 14:26:51.548336: step 18420, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.988 sec/batch; 86h:10m:42s remains)
INFO - root - 2017-12-05 14:27:01.061308: step 18430, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 80h:21m:55s remains)
INFO - root - 2017-12-05 14:27:10.442267: step 18440, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 84h:03m:38s remains)
INFO - root - 2017-12-05 14:27:19.703136: step 18450, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 80h:30m:00s remains)
INFO - root - 2017-12-05 14:27:29.145037: step 18460, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.994 sec/batch; 86h:44m:53s remains)
INFO - root - 2017-12-05 14:27:38.438629: step 18470, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 81h:53m:54s remains)
INFO - root - 2017-12-05 14:27:47.767292: step 18480, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.920 sec/batch; 80h:17m:23s remains)
INFO - root - 2017-12-05 14:27:57.215312: step 18490, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 83h:10m:51s remains)
INFO - root - 2017-12-05 14:28:06.851124: step 18500, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 82h:54m:05s remains)
2017-12-05 14:28:07.588692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2638278 -4.2675381 -4.2700272 -4.2667441 -4.2603903 -4.2558746 -4.2548213 -4.2530932 -4.24708 -4.2387943 -4.2356133 -4.2426338 -4.2550092 -4.2674565 -4.2809958][-4.2310061 -4.2434392 -4.2491608 -4.2502108 -4.2497606 -4.2491374 -4.2484951 -4.2485647 -4.2455816 -4.238997 -4.2360024 -4.2420363 -4.2524624 -4.2615304 -4.2724037][-4.188561 -4.2119651 -4.221539 -4.2217669 -4.2237816 -4.2269859 -4.228271 -4.2322307 -4.238553 -4.2406149 -4.2436171 -4.2508397 -4.2595 -4.2639341 -4.2701192][-4.1408873 -4.17136 -4.18736 -4.1894188 -4.1903076 -4.1920252 -4.191709 -4.2007031 -4.2191272 -4.2340031 -4.2475882 -4.25969 -4.269516 -4.2725382 -4.2743697][-4.1091251 -4.135623 -4.1535912 -4.1576352 -4.1564136 -4.1498523 -4.1397886 -4.149858 -4.1804624 -4.21004 -4.2356606 -4.2558293 -4.2724633 -4.2792821 -4.2814517][-4.1103859 -4.1184769 -4.1245737 -4.1193676 -4.1081891 -4.089097 -4.0653858 -4.067956 -4.1101146 -4.1592278 -4.2000327 -4.2317576 -4.2578769 -4.27258 -4.2795773][-4.1246681 -4.1167045 -4.1060619 -4.0842485 -4.05804 -4.0258522 -3.984746 -3.9689651 -4.0144949 -4.081933 -4.1382051 -4.1840849 -4.2239013 -4.2488623 -4.2639303][-4.1358433 -4.1196508 -4.1014624 -4.0756922 -4.0463324 -4.008513 -3.9525926 -3.9083495 -3.9398077 -4.0109577 -4.0758796 -4.1317692 -4.1823931 -4.217113 -4.2401009][-4.1454992 -4.130096 -4.1147738 -4.0971003 -4.0781908 -4.0514426 -4.0021081 -3.9462786 -3.9506254 -4.001143 -4.0559883 -4.1091981 -4.1609507 -4.199172 -4.2263923][-4.1601105 -4.1528211 -4.1472244 -4.1430535 -4.1378131 -4.1256809 -4.0948753 -4.049767 -4.03867 -4.0637221 -4.09632 -4.1345186 -4.1776013 -4.2101607 -4.2330656][-4.1867914 -4.1854553 -4.1863151 -4.189857 -4.1930609 -4.1914039 -4.1768088 -4.1482382 -4.135941 -4.1475058 -4.1643391 -4.1869135 -4.2156019 -4.2365117 -4.2508254][-4.2224088 -4.2216516 -4.2225418 -4.2260637 -4.23085 -4.2333632 -4.2296019 -4.2163415 -4.2094398 -4.2161789 -4.225791 -4.2382054 -4.2541571 -4.2658277 -4.2727003][-4.2563667 -4.2535591 -4.2521386 -4.2528338 -4.2555776 -4.2583842 -4.2587185 -4.2545514 -4.2528348 -4.2576303 -4.2638288 -4.2704716 -4.2778444 -4.2837038 -4.2869887][-4.2821026 -4.2777553 -4.27491 -4.2733278 -4.2733817 -4.2743874 -4.2749586 -4.2745366 -4.275434 -4.2791991 -4.283195 -4.2864413 -4.2902765 -4.29399 -4.2967415][-4.2977138 -4.293726 -4.2910671 -4.2892113 -4.2882118 -4.2881184 -4.288166 -4.2880182 -4.2885594 -4.2902818 -4.2924876 -4.2950315 -4.2986364 -4.3022122 -4.3051519]]...]
INFO - root - 2017-12-05 14:28:16.926969: step 18510, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 82h:51m:25s remains)
INFO - root - 2017-12-05 14:28:26.045756: step 18520, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 73h:45m:08s remains)
INFO - root - 2017-12-05 14:28:35.508524: step 18530, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 84h:32m:58s remains)
INFO - root - 2017-12-05 14:28:44.817153: step 18540, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 84h:01m:40s remains)
INFO - root - 2017-12-05 14:28:54.106266: step 18550, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 82h:41m:54s remains)
INFO - root - 2017-12-05 14:29:03.662312: step 18560, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.978 sec/batch; 85h:19m:28s remains)
INFO - root - 2017-12-05 14:29:13.012024: step 18570, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 84h:47m:47s remains)
INFO - root - 2017-12-05 14:29:22.339366: step 18580, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 79h:54m:48s remains)
INFO - root - 2017-12-05 14:29:31.802547: step 18590, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 78h:34m:10s remains)
INFO - root - 2017-12-05 14:29:41.195281: step 18600, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 81h:31m:14s remains)
2017-12-05 14:29:41.945444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2000823 -4.199616 -4.213974 -4.2234311 -4.2122908 -4.2007923 -4.1950231 -4.1858306 -4.1604409 -4.1300468 -4.122653 -4.1420908 -4.1771016 -4.2127237 -4.2547641][-4.1968632 -4.1926017 -4.205379 -4.2226315 -4.2190042 -4.2106676 -4.203629 -4.1894245 -4.1632118 -4.1306305 -4.1250305 -4.1457915 -4.1748624 -4.2010832 -4.2388492][-4.188765 -4.1893463 -4.2043843 -4.2244868 -4.2256289 -4.2205296 -4.2139635 -4.2041388 -4.1856751 -4.1594429 -4.148191 -4.1567755 -4.1706829 -4.1837583 -4.2208128][-4.181797 -4.1957331 -4.2157669 -4.2339706 -4.2360735 -4.2266879 -4.2166157 -4.2091141 -4.2035685 -4.1942191 -4.1794972 -4.1744289 -4.1719308 -4.177495 -4.2157154][-4.1852193 -4.2075033 -4.2280226 -4.2429485 -4.2396603 -4.2198005 -4.1945152 -4.170989 -4.1794009 -4.2024713 -4.2015948 -4.1943927 -4.1854353 -4.189394 -4.2247415][-4.1974754 -4.2210464 -4.2425976 -4.2536979 -4.2381678 -4.2008772 -4.1426315 -4.0814319 -4.0993347 -4.164547 -4.1916585 -4.1975212 -4.1911297 -4.1991267 -4.2345829][-4.2103152 -4.2272453 -4.2447863 -4.2505941 -4.2241449 -4.1668425 -4.0705357 -3.9583902 -3.9863422 -4.0981789 -4.1574697 -4.1767788 -4.1791253 -4.195632 -4.2295113][-4.2128682 -4.2194777 -4.2285533 -4.2315826 -4.2075281 -4.1400781 -4.0150785 -3.861968 -3.904187 -4.0434589 -4.1177363 -4.1491022 -4.1615357 -4.18581 -4.2176428][-4.2173347 -4.2140665 -4.2128782 -4.215199 -4.2044578 -4.1505 -4.0390396 -3.9100974 -3.9451029 -4.0475559 -4.1015425 -4.1306334 -4.14655 -4.1732192 -4.2024145][-4.2221637 -4.2123432 -4.2079163 -4.2145019 -4.2177224 -4.1868467 -4.1107769 -4.0291548 -4.0479026 -4.0984335 -4.1191187 -4.1331754 -4.1421084 -4.1650195 -4.19093][-4.2237725 -4.212676 -4.2117143 -4.2246428 -4.2368054 -4.2195406 -4.1707869 -4.1206708 -4.1311364 -4.153017 -4.1557264 -4.1555676 -4.1513267 -4.1645794 -4.1894231][-4.2377663 -4.2258973 -4.2270551 -4.2418661 -4.2584424 -4.2494092 -4.2162466 -4.1866212 -4.191833 -4.1988554 -4.1949973 -4.1919746 -4.1819558 -4.1864405 -4.2089496][-4.2603321 -4.2478004 -4.2484536 -4.2617612 -4.275034 -4.2705841 -4.244957 -4.230072 -4.2311797 -4.2287712 -4.2271628 -4.2308655 -4.2233572 -4.2252564 -4.24436][-4.2790575 -4.2658815 -4.2648339 -4.2752147 -4.2848144 -4.2794724 -4.2616053 -4.257565 -4.2570138 -4.25309 -4.2570481 -4.2675133 -4.2651234 -4.2657046 -4.2788296][-4.2869043 -4.2738857 -4.271594 -4.2804484 -4.2884946 -4.2828312 -4.2724338 -4.2722721 -4.2685523 -4.265584 -4.2704062 -4.282567 -4.2826271 -4.2837229 -4.2928319]]...]
INFO - root - 2017-12-05 14:29:51.491520: step 18610, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 83h:54m:26s remains)
INFO - root - 2017-12-05 14:30:00.704934: step 18620, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 82h:09m:11s remains)
INFO - root - 2017-12-05 14:30:10.062166: step 18630, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 86h:35m:21s remains)
INFO - root - 2017-12-05 14:30:19.206908: step 18640, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 78h:52m:49s remains)
INFO - root - 2017-12-05 14:30:28.515526: step 18650, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 84h:20m:34s remains)
INFO - root - 2017-12-05 14:30:37.965442: step 18660, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 82h:11m:18s remains)
INFO - root - 2017-12-05 14:30:47.353742: step 18670, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 78h:31m:01s remains)
INFO - root - 2017-12-05 14:30:56.698708: step 18680, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 82h:35m:03s remains)
INFO - root - 2017-12-05 14:31:06.049743: step 18690, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 78h:38m:05s remains)
INFO - root - 2017-12-05 14:31:15.320595: step 18700, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 83h:16m:26s remains)
2017-12-05 14:31:16.155275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2523894 -4.2246227 -4.2028122 -4.1642756 -4.1119561 -4.1185608 -4.1620569 -4.2055545 -4.2372832 -4.2437344 -4.2340131 -4.2184625 -4.1967058 -4.17433 -4.1750183][-4.2417579 -4.2102213 -4.1859136 -4.1424713 -4.0871463 -4.0839329 -4.1205444 -4.1616354 -4.1978474 -4.2167888 -4.2239308 -4.2226787 -4.2099261 -4.1901307 -4.1855054][-4.2358007 -4.2059546 -4.1788177 -4.1322627 -4.0713754 -4.0522532 -4.0727253 -4.1093211 -4.1524835 -4.18659 -4.2113729 -4.2250352 -4.2211518 -4.2039847 -4.1923409][-4.2363362 -4.2121072 -4.1884041 -4.1478362 -4.0873904 -4.0462379 -4.0392389 -4.0634365 -4.111021 -4.15771 -4.1987777 -4.2267761 -4.2331948 -4.2195892 -4.2039971][-4.2391844 -4.2170773 -4.20108 -4.1764016 -4.1252646 -4.0692153 -4.0337496 -4.0390267 -4.0827417 -4.1336603 -4.1784506 -4.2163944 -4.2355671 -4.2318573 -4.2199807][-4.2420549 -4.2158914 -4.2003412 -4.1849413 -4.1474147 -4.0944672 -4.045773 -4.0389371 -4.0786486 -4.1249142 -4.1599097 -4.1954627 -4.2232294 -4.2323766 -4.2317328][-4.2477317 -4.2184873 -4.1968827 -4.1762419 -4.1448951 -4.1022005 -4.0623989 -4.0534215 -4.0818853 -4.1089396 -4.123672 -4.1500397 -4.1880746 -4.2179861 -4.2334981][-4.2547193 -4.2301364 -4.2133231 -4.1938882 -4.1659141 -4.1300125 -4.0986037 -4.0809889 -4.0795031 -4.0717607 -4.0559154 -4.0703855 -4.1239872 -4.1813393 -4.2181535][-4.256999 -4.2426286 -4.2358289 -4.2303958 -4.2148604 -4.1924238 -4.163507 -4.1258426 -4.085113 -4.038754 -3.9907818 -3.9910078 -4.0548377 -4.1348405 -4.1895108][-4.265038 -4.2574387 -4.2572694 -4.2623491 -4.2637758 -4.2563424 -4.2311225 -4.1830292 -4.1214757 -4.0557961 -3.994931 -3.98487 -4.0424514 -4.1235261 -4.1790485][-4.2782 -4.2752972 -4.2797523 -4.2896929 -4.2988648 -4.2996597 -4.2789016 -4.2339921 -4.1761584 -4.117218 -4.0674725 -4.0565262 -4.0973272 -4.1581969 -4.1956482][-4.2897172 -4.2880659 -4.2923951 -4.3001518 -4.3108959 -4.3151331 -4.2992015 -4.2641668 -4.2210793 -4.1780138 -4.1449842 -4.1399183 -4.168715 -4.2071176 -4.2245378][-4.3001194 -4.2980032 -4.2993321 -4.3015714 -4.3094335 -4.3159056 -4.3065362 -4.2836752 -4.2578111 -4.2306309 -4.2115641 -4.2111797 -4.2284026 -4.2482882 -4.2525697][-4.305428 -4.3044734 -4.3027935 -4.3013053 -4.3070168 -4.3151989 -4.3105311 -4.3001957 -4.289804 -4.2752891 -4.2625251 -4.2586079 -4.265038 -4.2706919 -4.2666125][-4.297164 -4.2997494 -4.2992086 -4.2969065 -4.2997866 -4.3057432 -4.3053765 -4.303081 -4.3015475 -4.294857 -4.2851324 -4.2791305 -4.2793245 -4.2779703 -4.2705317]]...]
INFO - root - 2017-12-05 14:31:25.356237: step 18710, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 77h:21m:01s remains)
INFO - root - 2017-12-05 14:31:34.551850: step 18720, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 80h:06m:36s remains)
INFO - root - 2017-12-05 14:31:43.805802: step 18730, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 80h:27m:30s remains)
INFO - root - 2017-12-05 14:31:52.826881: step 18740, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.957 sec/batch; 83h:26m:53s remains)
INFO - root - 2017-12-05 14:32:02.181147: step 18750, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 82h:25m:36s remains)
INFO - root - 2017-12-05 14:32:11.506661: step 18760, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 84h:23m:48s remains)
INFO - root - 2017-12-05 14:32:20.970594: step 18770, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 80h:41m:19s remains)
INFO - root - 2017-12-05 14:32:30.234251: step 18780, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 75h:32m:43s remains)
INFO - root - 2017-12-05 14:32:39.630117: step 18790, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 82h:59m:26s remains)
INFO - root - 2017-12-05 14:32:49.096795: step 18800, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.936 sec/batch; 81h:31m:38s remains)
2017-12-05 14:32:49.948430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2798591 -4.26941 -4.2567954 -4.2407794 -4.2270312 -4.216692 -4.21098 -4.1906323 -4.1651106 -4.1562438 -4.154748 -4.1565633 -4.1695762 -4.1919017 -4.1782246][-4.26825 -4.2603931 -4.2510529 -4.2330503 -4.2131543 -4.1954012 -4.1821151 -4.1591177 -4.1308208 -4.1159253 -4.11334 -4.1136069 -4.1257062 -4.1555133 -4.159605][-4.2637215 -4.2589617 -4.2565184 -4.2372212 -4.210319 -4.183341 -4.1577644 -4.1246696 -4.0973072 -4.0886807 -4.0911784 -4.0901442 -4.0940924 -4.1246276 -4.1434569][-4.2611318 -4.2583995 -4.2604074 -4.2472692 -4.2214708 -4.1967368 -4.1687617 -4.1337843 -4.1096158 -4.1092978 -4.113852 -4.1036158 -4.09504 -4.1197071 -4.1439838][-4.2496705 -4.244381 -4.2474093 -4.2386627 -4.2138162 -4.1900697 -4.1624022 -4.1345806 -4.122654 -4.1381645 -4.1500154 -4.1341662 -4.1186719 -4.1320195 -4.1496916][-4.2131987 -4.2016153 -4.20635 -4.2101259 -4.1880984 -4.1530104 -4.1048288 -4.0655437 -4.0699015 -4.1147127 -4.14441 -4.1337118 -4.1167574 -4.1211123 -4.1307449][-4.1605177 -4.1401181 -4.1370044 -4.144803 -4.1260858 -4.0719819 -3.9852524 -3.9115598 -3.937866 -4.025281 -4.0849104 -4.0958195 -4.0908632 -4.0932341 -4.0967288][-4.1177521 -4.083003 -4.0677071 -4.0709872 -4.0503316 -3.9764712 -3.8457336 -3.7280989 -3.7880037 -3.9308436 -4.0238428 -4.0578 -4.0642395 -4.0674262 -4.0699697][-4.0997162 -4.0598307 -4.04523 -4.0555062 -4.0507746 -3.9848678 -3.8585243 -3.7387314 -3.8022351 -3.9458582 -4.0358129 -4.0747552 -4.0785651 -4.0717783 -4.0680342][-4.1047196 -4.0736609 -4.0723376 -4.1006374 -4.1215925 -4.0889292 -4.0101776 -3.9287755 -3.9562125 -4.0456614 -4.10329 -4.128696 -4.119194 -4.0979652 -4.079658][-4.1138711 -4.0962114 -4.1067505 -4.1457815 -4.1818004 -4.1741452 -4.1296387 -4.0716357 -4.0761719 -4.1274705 -4.165103 -4.1793189 -4.1629734 -4.1336408 -4.1000423][-4.112834 -4.1014547 -4.1158023 -4.1574378 -4.1994529 -4.2044182 -4.1773033 -4.1385007 -4.1365557 -4.1672292 -4.1936674 -4.2063727 -4.1948152 -4.1629581 -4.118865][-4.1266484 -4.116116 -4.1265159 -4.1618848 -4.2018638 -4.2122259 -4.1968913 -4.1733732 -4.1702409 -4.1920457 -4.2128706 -4.2236247 -4.2169566 -4.1852932 -4.136198][-4.1665416 -4.154623 -4.15988 -4.1800661 -4.20664 -4.2156439 -4.2097759 -4.198843 -4.19835 -4.21054 -4.2226934 -4.2309761 -4.2265043 -4.1964355 -4.1500134][-4.2208929 -4.2072673 -4.2070274 -4.2152047 -4.2285323 -4.2325406 -4.2308125 -4.2281218 -4.2296882 -4.234787 -4.2379422 -4.2397051 -4.2336893 -4.2073808 -4.1696157]]...]
INFO - root - 2017-12-05 14:32:59.107702: step 18810, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 84h:48m:44s remains)
INFO - root - 2017-12-05 14:33:08.352324: step 18820, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 83h:35m:04s remains)
INFO - root - 2017-12-05 14:33:17.930442: step 18830, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 85h:57m:19s remains)
INFO - root - 2017-12-05 14:33:27.376968: step 18840, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 84h:36m:14s remains)
INFO - root - 2017-12-05 14:33:36.552716: step 18850, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 79h:23m:16s remains)
INFO - root - 2017-12-05 14:33:45.756141: step 18860, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 78h:25m:17s remains)
INFO - root - 2017-12-05 14:33:55.084437: step 18870, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.982 sec/batch; 85h:30m:40s remains)
INFO - root - 2017-12-05 14:34:04.626022: step 18880, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 82h:07m:17s remains)
INFO - root - 2017-12-05 14:34:14.019436: step 18890, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 79h:06m:12s remains)
INFO - root - 2017-12-05 14:34:23.399666: step 18900, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 82h:58m:57s remains)
2017-12-05 14:34:24.164341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.297215 -4.3001494 -4.308147 -4.31304 -4.3099194 -4.2939281 -4.2782559 -4.2647681 -4.2629743 -4.2739439 -4.2844167 -4.2930751 -4.295599 -4.2949567 -4.2917228][-4.2675514 -4.275816 -4.2955384 -4.3091364 -4.305881 -4.2834964 -4.2582192 -4.2353058 -4.2312307 -4.2481127 -4.2674117 -4.2812204 -4.2823682 -4.2817945 -4.2790518][-4.2369995 -4.2520528 -4.2823877 -4.3013983 -4.298593 -4.2736378 -4.2424717 -4.2098045 -4.2004571 -4.2199178 -4.2455578 -4.2617726 -4.2601943 -4.2603006 -4.2606931][-4.2042656 -4.2203188 -4.2580872 -4.2863569 -4.285428 -4.2575107 -4.2174706 -4.1750121 -4.1640763 -4.1872025 -4.2137971 -4.2310376 -4.2329044 -4.2393689 -4.2492175][-4.1699166 -4.1792841 -4.2189484 -4.2540803 -4.2505059 -4.2156887 -4.1658459 -4.1220889 -4.1238513 -4.1547627 -4.1847363 -4.2133431 -4.2252626 -4.2393737 -4.2552652][-4.1432362 -4.135046 -4.1652164 -4.1987557 -4.1876564 -4.1424966 -4.088017 -4.0625019 -4.0895553 -4.1322708 -4.1681633 -4.2030392 -4.2237177 -4.2466674 -4.2641115][-4.1389523 -4.11015 -4.1162615 -4.1358428 -4.1155906 -4.0538564 -3.981916 -3.97777 -4.0469942 -4.1172042 -4.1642532 -4.2024484 -4.2290778 -4.24979 -4.2625995][-4.1581645 -4.1206717 -4.1093521 -4.106154 -4.0630236 -3.9623604 -3.8452089 -3.8615477 -3.9984291 -4.1184454 -4.1830316 -4.2228308 -4.2484579 -4.2565451 -4.2589135][-4.1773539 -4.1447659 -4.1309338 -4.1158247 -4.0648289 -3.9546654 -3.8230889 -3.8418722 -3.998122 -4.1357374 -4.2087088 -4.2485785 -4.269547 -4.2639732 -4.2583485][-4.1837177 -4.1565537 -4.1417384 -4.1253014 -4.0913815 -4.0242929 -3.9447167 -3.9531043 -4.0573211 -4.1651173 -4.2293019 -4.2666912 -4.2844 -4.2721152 -4.2644682][-4.1774664 -4.1570129 -4.1459513 -4.1323218 -4.1121068 -4.0805411 -4.0454617 -4.0548568 -4.1180534 -4.1862626 -4.2292986 -4.261116 -4.2781887 -4.2696609 -4.2687721][-4.1719151 -4.160418 -4.1624346 -4.1544828 -4.1396074 -4.1255803 -4.1157632 -4.1345463 -4.17609 -4.2160969 -4.23642 -4.2571888 -4.271584 -4.2692046 -4.2745595][-4.1757431 -4.1739073 -4.1881003 -4.1916375 -4.1849546 -4.1813636 -4.1813006 -4.1977072 -4.2223563 -4.2383275 -4.2424488 -4.256475 -4.26873 -4.269671 -4.2822666][-4.1855316 -4.1905127 -4.2122636 -4.2268686 -4.233829 -4.2393713 -4.242661 -4.2487316 -4.2570019 -4.2542443 -4.2476811 -4.255662 -4.2671394 -4.2723994 -4.2922535][-4.2064133 -4.2086282 -4.2290125 -4.2495232 -4.2654281 -4.277689 -4.2803512 -4.2805057 -4.2806234 -4.2701983 -4.2555838 -4.2575917 -4.267405 -4.277658 -4.3007097]]...]
INFO - root - 2017-12-05 14:34:33.387633: step 18910, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 82h:55m:44s remains)
INFO - root - 2017-12-05 14:34:42.810942: step 18920, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 86h:29m:42s remains)
INFO - root - 2017-12-05 14:34:52.199076: step 18930, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 84h:42m:04s remains)
INFO - root - 2017-12-05 14:35:01.667378: step 18940, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 83h:16m:06s remains)
INFO - root - 2017-12-05 14:35:11.197518: step 18950, loss = 2.02, batch loss = 1.96 (8.6 examples/sec; 0.929 sec/batch; 80h:57m:03s remains)
INFO - root - 2017-12-05 14:35:20.705899: step 18960, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 82h:29m:02s remains)
INFO - root - 2017-12-05 14:35:30.148689: step 18970, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 82h:51m:07s remains)
INFO - root - 2017-12-05 14:35:39.544244: step 18980, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 83h:39m:06s remains)
INFO - root - 2017-12-05 14:35:48.653619: step 18990, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 78h:10m:10s remains)
INFO - root - 2017-12-05 14:35:57.922405: step 19000, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 77h:27m:09s remains)
2017-12-05 14:35:58.689178: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2873483 -4.2774177 -4.25567 -4.2214112 -4.1819305 -4.1511693 -4.1228652 -4.1106887 -4.1269712 -4.15681 -4.1800585 -4.2030339 -4.218298 -4.2313 -4.2458653][-4.2849879 -4.2705145 -4.2471457 -4.2102003 -4.1692338 -4.1363835 -4.1054907 -4.0875034 -4.0976043 -4.1225338 -4.1456852 -4.1738114 -4.2004433 -4.2266388 -4.2479391][-4.2957397 -4.2817802 -4.2562056 -4.2175355 -4.1751919 -4.140615 -4.110754 -4.085423 -4.0847964 -4.1007538 -4.1252847 -4.1595545 -4.1940012 -4.2268167 -4.251142][-4.3061924 -4.2961493 -4.2700949 -4.232861 -4.1922708 -4.157814 -4.1219511 -4.0822954 -4.0716863 -4.082047 -4.1103277 -4.1528325 -4.1900792 -4.2219868 -4.2462339][-4.3124003 -4.3051405 -4.2804489 -4.2452469 -4.2040749 -4.1606755 -4.1132941 -4.0583062 -4.0412593 -4.0549803 -4.09508 -4.1498413 -4.18978 -4.2175174 -4.2403069][-4.3114772 -4.3018732 -4.2738795 -4.2328329 -4.1827235 -4.1244693 -4.0580978 -3.988435 -3.978483 -4.01825 -4.0815692 -4.1506925 -4.1941643 -4.2183971 -4.2383142][-4.3050466 -4.2908931 -4.2566023 -4.2063961 -4.1417875 -4.0597258 -3.9633751 -3.8735876 -3.8937359 -3.9866402 -4.0813341 -4.1585765 -4.2003818 -4.2191315 -4.2359858][-4.2973146 -4.2781439 -4.2334895 -4.172368 -4.09327 -3.9870546 -3.854661 -3.7491293 -3.8224478 -3.9751332 -4.0938759 -4.169785 -4.2027845 -4.2161875 -4.2315464][-4.2926106 -4.2679253 -4.2163725 -4.1492186 -4.0661564 -3.9592857 -3.8342068 -3.7650323 -3.869941 -4.0238061 -4.1285028 -4.1871634 -4.2082515 -4.2181988 -4.2337084][-4.2888927 -4.2577944 -4.2096028 -4.1523356 -4.08677 -4.0153956 -3.9440157 -3.9221983 -4.0014644 -4.1039605 -4.1705379 -4.2053447 -4.2193265 -4.2303438 -4.2457681][-4.2805929 -4.2456188 -4.2050242 -4.1672316 -4.1287313 -4.09277 -4.0582457 -4.0507903 -4.1024113 -4.1653004 -4.2053885 -4.2264104 -4.2381124 -4.2494469 -4.2628479][-4.2699666 -4.2334728 -4.1997261 -4.176311 -4.1571956 -4.1427779 -4.1221466 -4.1179018 -4.1535707 -4.1972942 -4.2264485 -4.2422214 -4.2535877 -4.2639947 -4.2737546][-4.26486 -4.2258906 -4.1931248 -4.1750116 -4.1667295 -4.1630859 -4.1509733 -4.1498389 -4.1773715 -4.2120042 -4.236692 -4.2513595 -4.26197 -4.2711511 -4.2787504][-4.2640042 -4.2227211 -4.1888628 -4.1719089 -4.1670752 -4.1692314 -4.1665444 -4.1721153 -4.1972547 -4.2280092 -4.2497978 -4.2627 -4.2725511 -4.279532 -4.2863717][-4.2673512 -4.2266417 -4.1940188 -4.178041 -4.1754751 -4.1847005 -4.1911941 -4.2014313 -4.2219477 -4.2460613 -4.2633343 -4.2748656 -4.2839088 -4.2913022 -4.2986436]]...]
INFO - root - 2017-12-05 14:36:07.805803: step 19010, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 81h:02m:52s remains)
INFO - root - 2017-12-05 14:36:17.293557: step 19020, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 85h:08m:27s remains)
INFO - root - 2017-12-05 14:36:26.817748: step 19030, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 85h:17m:17s remains)
INFO - root - 2017-12-05 14:36:36.067871: step 19040, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 83h:18m:01s remains)
INFO - root - 2017-12-05 14:36:45.557766: step 19050, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 81h:18m:00s remains)
INFO - root - 2017-12-05 14:36:54.905454: step 19060, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 82h:22m:30s remains)
INFO - root - 2017-12-05 14:37:04.343360: step 19070, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 83h:46m:56s remains)
INFO - root - 2017-12-05 14:37:13.668726: step 19080, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.809 sec/batch; 70h:23m:47s remains)
INFO - root - 2017-12-05 14:37:22.767820: step 19090, loss = 2.06, batch loss = 2.01 (10.3 examples/sec; 0.774 sec/batch; 67h:21m:19s remains)
INFO - root - 2017-12-05 14:37:32.141058: step 19100, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 84h:41m:36s remains)
2017-12-05 14:37:32.907666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2456622 -4.2602525 -4.2786069 -4.2939663 -4.298974 -4.3025579 -4.3106794 -4.3134866 -4.3048081 -4.2869263 -4.2760453 -4.2581224 -4.215837 -4.1771574 -4.1699076][-4.2139497 -4.232739 -4.2516975 -4.2657332 -4.2684283 -4.2763453 -4.2877584 -4.2930322 -4.2895737 -4.278173 -4.2764373 -4.2670221 -4.2283893 -4.1854019 -4.1668181][-4.1788149 -4.2030787 -4.2282534 -4.2424512 -4.2333426 -4.2347054 -4.2479997 -4.2620811 -4.270649 -4.2733188 -4.2812638 -4.2794614 -4.247623 -4.2107081 -4.1910305][-4.1498151 -4.1797466 -4.2126913 -4.2300911 -4.2052188 -4.1906548 -4.2010756 -4.22819 -4.2517228 -4.2669139 -4.2804422 -4.2804656 -4.2602191 -4.2363777 -4.2246652][-4.1547003 -4.1835532 -4.2103772 -4.21858 -4.1796803 -4.1390095 -4.1233196 -4.1575093 -4.2041245 -4.2415 -4.2691669 -4.2717981 -4.2581549 -4.2454367 -4.2415252][-4.1749792 -4.198204 -4.2056327 -4.2012711 -4.1570354 -4.0902042 -4.0278592 -4.0437155 -4.1175938 -4.1906304 -4.2386236 -4.2487464 -4.2370658 -4.2275972 -4.2250605][-4.1816916 -4.1993127 -4.19508 -4.1850424 -4.1439877 -4.0600405 -3.9385393 -3.9035769 -4.0024686 -4.1208568 -4.1883926 -4.2068667 -4.1979194 -4.1881571 -4.1871991][-4.16416 -4.1813354 -4.1783972 -4.1744046 -4.1466146 -4.0601869 -3.8963764 -3.7939954 -3.9015961 -4.0577078 -4.1389008 -4.1558952 -4.147336 -4.141346 -4.1479173][-4.1740265 -4.1909003 -4.1926279 -4.1961565 -4.1803503 -4.115118 -3.9758465 -3.8509593 -3.9167087 -4.0540729 -4.1242905 -4.1308103 -4.1188521 -4.1176481 -4.1368604][-4.19436 -4.2112465 -4.2196875 -4.2272387 -4.2228436 -4.1854458 -4.0985756 -4.0027261 -4.0145831 -4.1028214 -4.1483765 -4.1409917 -4.125833 -4.1264715 -4.1502252][-4.2225862 -4.2301764 -4.2373457 -4.2441025 -4.2490964 -4.2354097 -4.1896386 -4.1321173 -4.1224074 -4.1696382 -4.1915112 -4.1756072 -4.1617618 -4.1638885 -4.1866889][-4.2579603 -4.2552037 -4.2553048 -4.2608037 -4.270329 -4.2700572 -4.2479577 -4.2178831 -4.20687 -4.2303967 -4.2409744 -4.2235079 -4.2100019 -4.2155194 -4.2354646][-4.276721 -4.2734747 -4.2730689 -4.2794118 -4.289062 -4.2950559 -4.2876544 -4.2749453 -4.2681384 -4.2773447 -4.2805748 -4.2695584 -4.2601347 -4.2648573 -4.2763057][-4.2914457 -4.290566 -4.29095 -4.2952538 -4.302402 -4.3089795 -4.3091364 -4.3067479 -4.3052258 -4.3088546 -4.3070655 -4.2972198 -4.2897406 -4.292099 -4.2996521][-4.30283 -4.3041582 -4.3051229 -4.3070922 -4.309114 -4.3113251 -4.311573 -4.3114696 -4.3117561 -4.3132133 -4.3117218 -4.3048682 -4.2974267 -4.2969184 -4.3014331]]...]
INFO - root - 2017-12-05 14:37:42.213671: step 19110, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 79h:26m:12s remains)
INFO - root - 2017-12-05 14:37:51.495249: step 19120, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 82h:01m:25s remains)
INFO - root - 2017-12-05 14:38:00.905871: step 19130, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 81h:14m:12s remains)
INFO - root - 2017-12-05 14:38:10.125134: step 19140, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.959 sec/batch; 83h:28m:59s remains)
INFO - root - 2017-12-05 14:38:19.554439: step 19150, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 81h:22m:30s remains)
INFO - root - 2017-12-05 14:38:28.701774: step 19160, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 83h:47m:39s remains)
INFO - root - 2017-12-05 14:38:37.886759: step 19170, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 78h:48m:20s remains)
INFO - root - 2017-12-05 14:38:47.196948: step 19180, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.936 sec/batch; 81h:29m:32s remains)
INFO - root - 2017-12-05 14:38:56.415613: step 19190, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 82h:49m:35s remains)
INFO - root - 2017-12-05 14:39:05.760447: step 19200, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.943 sec/batch; 82h:04m:38s remains)
2017-12-05 14:39:06.471807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3134327 -4.3070197 -4.3013048 -4.2992234 -4.298872 -4.2970462 -4.2935286 -4.2899375 -4.2863164 -4.2866387 -4.296227 -4.3117909 -4.3237529 -4.3312669 -4.337184][-4.3011789 -4.2926378 -4.284852 -4.2850981 -4.2894068 -4.2906122 -4.2850132 -4.2786012 -4.2737947 -4.2737989 -4.2837696 -4.3010087 -4.316855 -4.3254495 -4.3308568][-4.291307 -4.2824235 -4.2748046 -4.2783561 -4.2888069 -4.2959909 -4.2901006 -4.2803822 -4.2766409 -4.2801108 -4.2908564 -4.3049154 -4.3196554 -4.3259258 -4.3288164][-4.2778893 -4.2677059 -4.2582297 -4.2621183 -4.2769127 -4.2891345 -4.2823915 -4.2706819 -4.2712536 -4.2799683 -4.2939482 -4.3067718 -4.3176508 -4.3205581 -4.3200908][-4.2542028 -4.2370586 -4.2198882 -4.2168 -4.2319231 -4.2508287 -4.2443495 -4.2289281 -4.2296457 -4.2394538 -4.2606177 -4.2796125 -4.2912221 -4.29648 -4.2995143][-4.2217231 -4.1894107 -4.1545596 -4.1353569 -4.1436887 -4.1667 -4.1579776 -4.1323853 -4.1232471 -4.1343446 -4.1698208 -4.2081771 -4.2337217 -4.2508383 -4.2659593][-4.1946559 -4.1437654 -4.0853944 -4.0441523 -4.0384054 -4.0554857 -4.043004 -4.0045795 -3.9821634 -3.9932244 -4.05149 -4.1230288 -4.1711683 -4.2060351 -4.2364974][-4.1953669 -4.13685 -4.0707722 -4.0222735 -4.0078773 -4.0162759 -4.0032954 -3.9727867 -3.956583 -3.9730098 -4.0366974 -4.1165123 -4.1706762 -4.2063847 -4.2386789][-4.2152791 -4.1672044 -4.1172743 -4.0856638 -4.0795197 -4.0870748 -4.0790772 -4.0590248 -4.0503531 -4.0664692 -4.1159096 -4.1820097 -4.2293611 -4.2537322 -4.2733479][-4.2433982 -4.2105751 -4.1824059 -4.1734333 -4.1796179 -4.1910758 -4.1882992 -4.1748285 -4.1666703 -4.1751728 -4.2018909 -4.2441349 -4.2781477 -4.2931614 -4.3034024][-4.2707047 -4.2517509 -4.2423606 -4.2477074 -4.2607684 -4.2719245 -4.2716451 -4.262341 -4.25336 -4.25526 -4.2673254 -4.2889848 -4.3089471 -4.3171473 -4.3222332][-4.2993493 -4.2881417 -4.2866197 -4.2940826 -4.3037734 -4.3111939 -4.3126321 -4.3083005 -4.3024416 -4.300571 -4.3058972 -4.31776 -4.3292079 -4.3324542 -4.3346806][-4.3238039 -4.3173652 -4.3180928 -4.3228393 -4.3282938 -4.3329077 -4.3328681 -4.3285313 -4.3251004 -4.3234911 -4.3274622 -4.3344412 -4.34101 -4.3431697 -4.3447456][-4.3443079 -4.339747 -4.3391333 -4.3412118 -4.3444471 -4.3470817 -4.3452692 -4.3399162 -4.3357434 -4.3351626 -4.3380551 -4.3418674 -4.346158 -4.3490891 -4.3512325][-4.3557987 -4.3521123 -4.3494883 -4.3486476 -4.3496842 -4.3503752 -4.3482738 -4.34438 -4.3416224 -4.3414021 -4.3425989 -4.3450828 -4.3490911 -4.3527603 -4.3550858]]...]
INFO - root - 2017-12-05 14:39:15.833370: step 19210, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 81h:15m:28s remains)
INFO - root - 2017-12-05 14:39:25.334491: step 19220, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 82h:31m:43s remains)
INFO - root - 2017-12-05 14:39:34.479348: step 19230, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.943 sec/batch; 82h:00m:59s remains)
INFO - root - 2017-12-05 14:39:43.793910: step 19240, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 81h:46m:54s remains)
INFO - root - 2017-12-05 14:39:53.145622: step 19250, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.922 sec/batch; 80h:12m:38s remains)
INFO - root - 2017-12-05 14:40:02.519083: step 19260, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 80h:35m:53s remains)
INFO - root - 2017-12-05 14:40:11.718612: step 19270, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 78h:06m:07s remains)
INFO - root - 2017-12-05 14:40:20.774673: step 19280, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 79h:22m:05s remains)
INFO - root - 2017-12-05 14:40:29.980369: step 19290, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 78h:53m:49s remains)
INFO - root - 2017-12-05 14:40:39.214490: step 19300, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 81h:02m:45s remains)
2017-12-05 14:40:40.015569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2670588 -4.2787266 -4.2819896 -4.2768083 -4.2671828 -4.2600551 -4.25793 -4.2600017 -4.2648931 -4.2694106 -4.2706838 -4.2677984 -4.2629647 -4.2588072 -4.2559295][-4.2508469 -4.2764316 -4.287158 -4.282649 -4.268609 -4.2569342 -4.2524395 -4.2532625 -4.2608285 -4.2703681 -4.2757568 -4.2736692 -4.2677851 -4.2610965 -4.2541895][-4.2180982 -4.2556453 -4.2735605 -4.27038 -4.2533989 -4.2361112 -4.2242403 -4.219965 -4.2294569 -4.2469668 -4.2611704 -4.2654376 -4.2636023 -4.2575221 -4.2485723][-4.185184 -4.2293549 -4.2531967 -4.2508984 -4.2273288 -4.1974225 -4.1707063 -4.156137 -4.1683497 -4.1996851 -4.2284679 -4.2440543 -4.2503572 -4.2472067 -4.2377477][-4.1806006 -4.2221565 -4.2451153 -4.2394338 -4.2038445 -4.1539741 -4.1040125 -4.076046 -4.0940323 -4.1447196 -4.1918945 -4.2208209 -4.2364311 -4.239161 -4.2332554][-4.1944089 -4.2266092 -4.2431893 -4.2323675 -4.1841693 -4.1115513 -4.0332093 -3.9907029 -4.0185328 -4.0893517 -4.1556926 -4.1992793 -4.2247076 -4.234097 -4.2328854][-4.1977625 -4.2240295 -4.2386475 -4.2248778 -4.1697035 -4.0803709 -3.98244 -3.935297 -3.9741879 -4.0565891 -4.1283174 -4.1758208 -4.2023754 -4.21156 -4.2100525][-4.1716366 -4.2018051 -4.224853 -4.2164588 -4.1633496 -4.0749779 -3.9796777 -3.9418917 -3.9843235 -4.0588088 -4.1152287 -4.1515851 -4.1700892 -4.1748118 -4.1713185][-4.1468024 -4.1838775 -4.215971 -4.2150726 -4.1687074 -4.094471 -4.0189862 -3.9924855 -4.0250883 -4.0755057 -4.1068854 -4.1261182 -4.1358991 -4.1387138 -4.1350617][-4.1533928 -4.1894512 -4.2217813 -4.2263803 -4.1936479 -4.1422229 -4.08932 -4.066587 -4.081409 -4.105525 -4.1153674 -4.1215806 -4.1258736 -4.129559 -4.124352][-4.1791272 -4.2049942 -4.2295079 -4.23517 -4.2165728 -4.1863003 -4.1497207 -4.1269464 -4.12709 -4.1339612 -4.1350608 -4.1374159 -4.1427269 -4.1504869 -4.1466823][-4.2065935 -4.2208424 -4.2359333 -4.2391019 -4.22934 -4.2114058 -4.1855049 -4.1647115 -4.1609287 -4.162395 -4.1631217 -4.1659231 -4.1732745 -4.1836085 -4.1836767][-4.2297635 -4.2392445 -4.2471647 -4.2473173 -4.2400451 -4.226501 -4.204936 -4.1841559 -4.1798854 -4.1826973 -4.1869187 -4.1924138 -4.2017069 -4.2146707 -4.2214904][-4.2520509 -4.262157 -4.2679305 -4.2691956 -4.2642641 -4.2510405 -4.2284803 -4.2078266 -4.2016945 -4.2044253 -4.2087984 -4.2135363 -4.2225828 -4.2368121 -4.2487617][-4.2716208 -4.2836442 -4.2909164 -4.2951446 -4.2936859 -4.2824941 -4.2613912 -4.2412615 -4.2318053 -4.2302237 -4.2284536 -4.2275434 -4.2332621 -4.2470593 -4.2595272]]...]
INFO - root - 2017-12-05 14:40:49.527449: step 19310, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 86h:10m:53s remains)
INFO - root - 2017-12-05 14:40:58.931350: step 19320, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 80h:44m:21s remains)
INFO - root - 2017-12-05 14:41:08.102322: step 19330, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 75h:29m:17s remains)
INFO - root - 2017-12-05 14:41:17.504847: step 19340, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 84h:18m:48s remains)
INFO - root - 2017-12-05 14:41:27.016798: step 19350, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 80h:45m:02s remains)
INFO - root - 2017-12-05 14:41:36.277056: step 19360, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 79h:10m:37s remains)
INFO - root - 2017-12-05 14:41:45.457254: step 19370, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 86h:41m:33s remains)
INFO - root - 2017-12-05 14:41:54.832204: step 19380, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 81h:40m:50s remains)
INFO - root - 2017-12-05 14:42:04.104565: step 19390, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 85h:00m:29s remains)
INFO - root - 2017-12-05 14:42:13.228964: step 19400, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.917 sec/batch; 79h:45m:17s remains)
2017-12-05 14:42:14.001458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.220643 -4.233891 -4.253294 -4.2547784 -4.2441158 -4.2385278 -4.2320495 -4.2190409 -4.2116084 -4.2060709 -4.1925654 -4.1800041 -4.1743555 -4.1777935 -4.1889553][-4.2088804 -4.2331882 -4.256104 -4.2573967 -4.2461529 -4.2363191 -4.2292566 -4.2198858 -4.2146215 -4.210403 -4.2052174 -4.1971054 -4.1869459 -4.1795678 -4.1788168][-4.1906 -4.2277908 -4.2561746 -4.2583919 -4.2454352 -4.230195 -4.21882 -4.2097712 -4.2081151 -4.2090678 -4.2128396 -4.2123766 -4.20335 -4.1870022 -4.1697426][-4.179544 -4.215519 -4.2450142 -4.2500925 -4.2401276 -4.223393 -4.2064424 -4.1971354 -4.1968184 -4.20176 -4.2161212 -4.23154 -4.2379093 -4.2263336 -4.2027626][-4.2028642 -4.2181644 -4.2308617 -4.2294903 -4.2161918 -4.1945357 -4.1755223 -4.1712542 -4.1746697 -4.1829915 -4.2062783 -4.2366509 -4.2612433 -4.2631669 -4.2472296][-4.2368741 -4.2292843 -4.2215714 -4.2096319 -4.1889033 -4.1601753 -4.1378675 -4.1340165 -4.1399136 -4.1535287 -4.1835275 -4.2251263 -4.2644067 -4.2821879 -4.281095][-4.25825 -4.2381825 -4.2182908 -4.197257 -4.1698689 -4.1326737 -4.098722 -4.086894 -4.0912542 -4.1110845 -4.1484175 -4.2004271 -4.252449 -4.2848716 -4.2955394][-4.2642765 -4.2390823 -4.2153759 -4.1897531 -4.1549077 -4.1084995 -4.05549 -4.0249605 -4.0284958 -4.0581713 -4.1055918 -4.1648726 -4.2242775 -4.2664227 -4.2840023][-4.2616839 -4.2367625 -4.216939 -4.1955166 -4.1616855 -4.1121011 -4.0443339 -3.9930253 -3.9875007 -4.0222721 -4.0773354 -4.1411815 -4.2001967 -4.2409549 -4.2578411][-4.2708855 -4.2476563 -4.233902 -4.2222366 -4.2013807 -4.16673 -4.1115594 -4.06217 -4.0467186 -4.0679216 -4.1118517 -4.16628 -4.2163124 -4.246829 -4.256794][-4.2922792 -4.2723193 -4.2649307 -4.2626705 -4.2546148 -4.2360287 -4.2016068 -4.1668334 -4.152411 -4.1612062 -4.18976 -4.2302542 -4.2683411 -4.2878995 -4.289588][-4.3132491 -4.2976608 -4.293426 -4.2957921 -4.2934327 -4.2849956 -4.2674675 -4.2479367 -4.2374334 -4.2392917 -4.2565465 -4.2835155 -4.3090329 -4.3196807 -4.3167324][-4.3279705 -4.3171535 -4.3139868 -4.3165407 -4.3161993 -4.3120036 -4.3035359 -4.295712 -4.2889862 -4.2868567 -4.2957382 -4.3110528 -4.3262787 -4.3322358 -4.3310261][-4.3375912 -4.3313122 -4.3291125 -4.3300581 -4.3299093 -4.3276305 -4.32346 -4.320487 -4.3167205 -4.3139229 -4.3178306 -4.3264117 -4.3352156 -4.3388748 -4.3397822][-4.3390155 -4.3361287 -4.3350344 -4.3344059 -4.3336735 -4.3324866 -4.3307734 -4.3305588 -4.3295217 -4.3281851 -4.3307633 -4.3359756 -4.3405733 -4.3423305 -4.3433084]]...]
INFO - root - 2017-12-05 14:42:23.472681: step 19410, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 82h:33m:27s remains)
INFO - root - 2017-12-05 14:42:32.905129: step 19420, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 81h:59m:19s remains)
INFO - root - 2017-12-05 14:42:42.290150: step 19430, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 82h:35m:16s remains)
INFO - root - 2017-12-05 14:42:51.716947: step 19440, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 84h:23m:48s remains)
INFO - root - 2017-12-05 14:43:01.131226: step 19450, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 80h:08m:37s remains)
INFO - root - 2017-12-05 14:43:10.481152: step 19460, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 81h:01m:34s remains)
INFO - root - 2017-12-05 14:43:19.916673: step 19470, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 80h:05m:28s remains)
INFO - root - 2017-12-05 14:43:29.198422: step 19480, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 73h:00m:45s remains)
INFO - root - 2017-12-05 14:43:38.643944: step 19490, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 78h:25m:06s remains)
INFO - root - 2017-12-05 14:43:47.860843: step 19500, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 83h:41m:20s remains)
2017-12-05 14:43:48.605297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1563959 -4.1166053 -4.0808454 -4.0544639 -4.0608215 -4.1107893 -4.1818166 -4.2456112 -4.2945371 -4.3261065 -4.3428783 -4.347712 -4.3475976 -4.3487453 -4.3525276][-4.1909881 -4.1372447 -4.0739336 -4.0143104 -4.003633 -4.056138 -4.131752 -4.1987438 -4.254282 -4.2968836 -4.3268275 -4.3426 -4.3490319 -4.3525925 -4.3561273][-4.2225094 -4.1659379 -4.0907478 -4.0151248 -3.9933538 -4.0378566 -4.1019249 -4.1589947 -4.2128148 -4.2619972 -4.3028169 -4.3301826 -4.3463421 -4.3544478 -4.3578811][-4.2408204 -4.1931453 -4.128809 -4.0635862 -4.0382714 -4.06087 -4.0951939 -4.1317439 -4.1751051 -4.2264566 -4.2756257 -4.3131847 -4.3389487 -4.3526921 -4.3583007][-4.2546782 -4.226346 -4.182085 -4.1320972 -4.1000419 -4.0941567 -4.09367 -4.10212 -4.1299849 -4.1818075 -4.2404132 -4.2901268 -4.3271303 -4.3487606 -4.3584557][-4.2819262 -4.2629566 -4.2280917 -4.18639 -4.1485038 -4.1170974 -4.0809665 -4.0554838 -4.0670819 -4.1239672 -4.1931057 -4.2527928 -4.29836 -4.3299141 -4.3488226][-4.289012 -4.2727146 -4.2492514 -4.2215362 -4.1825504 -4.1315789 -4.062252 -4.0051265 -4.007062 -4.0688086 -4.1392713 -4.1987872 -4.2488174 -4.2916212 -4.324554][-4.2779918 -4.2696819 -4.2587185 -4.2411661 -4.2034435 -4.1416512 -4.0544815 -3.9796782 -3.9784648 -4.0391111 -4.0979972 -4.1451516 -4.1929851 -4.244833 -4.2913613][-4.2616019 -4.258667 -4.2590246 -4.2469358 -4.2101345 -4.1499014 -4.0762448 -4.0170331 -4.0125222 -4.0463858 -4.0704679 -4.0950222 -4.136023 -4.1920452 -4.24868][-4.2420254 -4.24251 -4.2481446 -4.2410345 -4.2147942 -4.1743412 -4.1306157 -4.0928431 -4.0697327 -4.05377 -4.0297971 -4.0293121 -4.07 -4.131381 -4.1925507][-4.2351952 -4.2452497 -4.2559571 -4.2532482 -4.2361245 -4.2078567 -4.1750169 -4.1415796 -4.0994616 -4.0430589 -3.9766119 -3.9572053 -4.0070181 -4.0788684 -4.1443782][-4.2498555 -4.256587 -4.261735 -4.2572947 -4.2456059 -4.2265062 -4.2021756 -4.1721411 -4.1284018 -4.0678859 -4.0026035 -3.9823153 -4.0196481 -4.0719872 -4.1238222][-4.254838 -4.2565241 -4.2556 -4.2497444 -4.2443414 -4.2388883 -4.2307281 -4.2152743 -4.1896582 -4.1526642 -4.1121893 -4.0911379 -4.09512 -4.1082759 -4.1315513][-4.25299 -4.2598648 -4.2640615 -4.2648511 -4.2684426 -4.2732329 -4.2761536 -4.272059 -4.2574224 -4.2320228 -4.2035661 -4.1804333 -4.1628017 -4.1464453 -4.1421046][-4.2690148 -4.2822194 -4.2906532 -4.2945642 -4.29965 -4.3049221 -4.3083639 -4.3060284 -4.2953753 -4.277391 -4.2562332 -4.2330346 -4.2071276 -4.17916 -4.1622987]]...]
INFO - root - 2017-12-05 14:43:58.082972: step 19510, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.017 sec/batch; 88h:24m:52s remains)
INFO - root - 2017-12-05 14:44:07.372031: step 19520, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 75h:30m:06s remains)
INFO - root - 2017-12-05 14:44:16.704010: step 19530, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 81h:00m:35s remains)
INFO - root - 2017-12-05 14:44:25.870734: step 19540, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 80h:58m:45s remains)
INFO - root - 2017-12-05 14:44:35.037184: step 19550, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 82h:25m:06s remains)
INFO - root - 2017-12-05 14:44:44.344612: step 19560, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.988 sec/batch; 85h:55m:14s remains)
INFO - root - 2017-12-05 14:44:53.573999: step 19570, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 78h:59m:24s remains)
INFO - root - 2017-12-05 14:45:02.848549: step 19580, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 81h:33m:15s remains)
INFO - root - 2017-12-05 14:45:12.207393: step 19590, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 77h:06m:37s remains)
INFO - root - 2017-12-05 14:45:21.787032: step 19600, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.970 sec/batch; 84h:16m:34s remains)
2017-12-05 14:45:22.550201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3341055 -4.3312421 -4.3290038 -4.3316469 -4.3378677 -4.3321867 -4.3124928 -4.2818737 -4.2466273 -4.2170277 -4.203177 -4.2076912 -4.2302289 -4.254262 -4.273191][-4.3309636 -4.322145 -4.3154259 -4.3165617 -4.3228335 -4.3167381 -4.2948866 -4.2562151 -4.2153306 -4.1831737 -4.1694245 -4.1717234 -4.1917505 -4.2179055 -4.2417884][-4.3167257 -4.3017788 -4.2892613 -4.2863994 -4.29149 -4.286962 -4.2672048 -4.2226005 -4.1759686 -4.147172 -4.1419826 -4.1485033 -4.1647749 -4.18916 -4.2150083][-4.2862144 -4.2659535 -4.2475119 -4.241179 -4.2454419 -4.2434273 -4.23055 -4.1901078 -4.1437278 -4.1264067 -4.1351433 -4.1459942 -4.1530857 -4.1675038 -4.1879568][-4.2429328 -4.2139716 -4.1851425 -4.1749482 -4.1788497 -4.1801481 -4.1737647 -4.1398149 -4.0962791 -4.1008158 -4.1315851 -4.1497517 -4.1505017 -4.1516008 -4.1615362][-4.1993823 -4.1572418 -4.1107049 -4.0879197 -4.0887046 -4.09248 -4.08855 -4.0509248 -4.0081315 -4.044981 -4.1125293 -4.1468377 -4.1515331 -4.1503267 -4.153944][-4.1660089 -4.1143093 -4.049788 -4.0049396 -3.9914412 -3.9842036 -3.9685588 -3.9137621 -3.8727391 -3.9564948 -4.0724735 -4.1310954 -4.1492872 -4.1596055 -4.162859][-4.1500897 -4.0963378 -4.0221438 -3.9575984 -3.92074 -3.8897645 -3.8454258 -3.7606709 -3.7198572 -3.8482795 -4.00528 -4.0922289 -4.1304173 -4.1554074 -4.16455][-4.1587553 -4.1119809 -4.04541 -3.9821069 -3.9377623 -3.8894467 -3.8181086 -3.7087197 -3.662786 -3.7859554 -3.9421489 -4.0425158 -4.0950527 -4.1311164 -4.1519823][-4.1939197 -4.15968 -4.1106205 -4.0613875 -4.0209002 -3.9691231 -3.8877878 -3.7753711 -3.7157867 -3.787744 -3.9059632 -3.9999247 -4.0586977 -4.1016297 -4.1384826][-4.2483268 -4.2257633 -4.1943703 -4.1618366 -4.12954 -4.0812755 -4.0055308 -3.9054947 -3.8359275 -3.853344 -3.9293973 -4.0073915 -4.0644155 -4.1091642 -4.1579671][-4.3050346 -4.2892733 -4.2698193 -4.25136 -4.2282004 -4.1902728 -4.1316824 -4.0596161 -4.0021324 -3.9968326 -4.0373554 -4.0945835 -4.1431003 -4.1805296 -4.2218342][-4.3418984 -4.3327842 -4.3231864 -4.3148785 -4.3017282 -4.2754436 -4.2351642 -4.190033 -4.1552906 -4.153059 -4.1794124 -4.2188721 -4.2528687 -4.2759008 -4.2980552][-4.3566785 -4.3522987 -4.3476071 -4.3438592 -4.3368597 -4.3209352 -4.2953691 -4.2707524 -4.2568965 -4.2612696 -4.280726 -4.3070941 -4.3279557 -4.3393984 -4.3464913][-4.354713 -4.3535743 -4.3516293 -4.3493028 -4.3448763 -4.3358965 -4.3204479 -4.3064342 -4.3019891 -4.3091736 -4.3232226 -4.3388987 -4.350956 -4.3552666 -4.3555336]]...]
INFO - root - 2017-12-05 14:45:31.717845: step 19610, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 81h:42m:58s remains)
INFO - root - 2017-12-05 14:45:41.113846: step 19620, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 80h:18m:41s remains)
INFO - root - 2017-12-05 14:45:50.579549: step 19630, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 86h:58m:05s remains)
INFO - root - 2017-12-05 14:45:59.836123: step 19640, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.986 sec/batch; 85h:41m:08s remains)
INFO - root - 2017-12-05 14:46:09.403801: step 19650, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.958 sec/batch; 83h:13m:37s remains)
INFO - root - 2017-12-05 14:46:18.975946: step 19660, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.912 sec/batch; 79h:12m:54s remains)
INFO - root - 2017-12-05 14:46:28.149067: step 19670, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 81h:39m:43s remains)
INFO - root - 2017-12-05 14:46:37.564553: step 19680, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.982 sec/batch; 85h:21m:22s remains)
INFO - root - 2017-12-05 14:46:46.971598: step 19690, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 76h:51m:34s remains)
INFO - root - 2017-12-05 14:46:56.291766: step 19700, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 86h:03m:53s remains)
2017-12-05 14:46:57.005386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2960148 -4.2856426 -4.2617564 -4.2271786 -4.1947947 -4.1788297 -4.1827278 -4.1923804 -4.1853185 -4.1581488 -4.1401119 -4.156507 -4.1889176 -4.2061281 -4.2177806][-4.3024797 -4.2971253 -4.2801552 -4.2497964 -4.2159743 -4.1905494 -4.1803513 -4.1776724 -4.1628385 -4.1325579 -4.1189704 -4.1444392 -4.1875582 -4.2119355 -4.2290239][-4.30695 -4.3082552 -4.2962441 -4.264936 -4.2261348 -4.1911721 -4.1690178 -4.1553879 -4.1362934 -4.1102972 -4.099226 -4.1267567 -4.1760912 -4.2116556 -4.2350669][-4.3076892 -4.3153353 -4.3051519 -4.2712488 -4.2293453 -4.1891422 -4.1560516 -4.1359005 -4.1187716 -4.1020637 -4.0974579 -4.1225429 -4.1719575 -4.2139206 -4.2417083][-4.3030868 -4.3138947 -4.3038082 -4.2692165 -4.2268124 -4.1833868 -4.1460671 -4.1274776 -4.1207814 -4.1169963 -4.1223016 -4.1437469 -4.1822138 -4.219018 -4.2472553][-4.2930522 -4.3001113 -4.2853813 -4.2501678 -4.2028995 -4.1555247 -4.1232967 -4.1134753 -4.1211882 -4.1325622 -4.1519775 -4.1743093 -4.200325 -4.2311368 -4.2587934][-4.2781429 -4.2760468 -4.2548227 -4.2139964 -4.1609287 -4.113162 -4.0857668 -4.08878 -4.1122231 -4.1366577 -4.1697893 -4.1990829 -4.2225833 -4.2513423 -4.276094][-4.2586961 -4.2465525 -4.2170763 -4.1709151 -4.1176453 -4.0730577 -4.0519609 -4.0703897 -4.11208 -4.1479897 -4.1859565 -4.2213874 -4.2467527 -4.271471 -4.2888827][-4.2373853 -4.2177491 -4.1837411 -4.1410556 -4.0946732 -4.0572329 -4.0444288 -4.0767121 -4.1272459 -4.1667366 -4.2064385 -4.2433171 -4.2675481 -4.2854133 -4.2937288][-4.2227135 -4.2013011 -4.1683059 -4.1331515 -4.0976682 -4.0688896 -4.06565 -4.1020312 -4.1509767 -4.1902952 -4.2302771 -4.2630296 -4.2793255 -4.2853065 -4.2861524][-4.2221675 -4.2050738 -4.1773825 -4.1463265 -4.1179514 -4.09702 -4.0969887 -4.1306124 -4.1741261 -4.214592 -4.2502184 -4.2726035 -4.2788544 -4.2756357 -4.2717948][-4.2299824 -4.2187967 -4.1986842 -4.1733928 -4.1486692 -4.1309857 -4.1293478 -4.1558933 -4.1958141 -4.235414 -4.264235 -4.27566 -4.27465 -4.2678022 -4.2632608][-4.2386794 -4.2319932 -4.2201629 -4.2036624 -4.1851344 -4.171464 -4.1677403 -4.1845803 -4.2155294 -4.2477794 -4.2673063 -4.2728815 -4.2715406 -4.2657423 -4.2621689][-4.24434 -4.2388449 -4.2337337 -4.2276897 -4.2191839 -4.2125325 -4.2102923 -4.2168179 -4.2333198 -4.2531242 -4.2642198 -4.2664633 -4.266902 -4.2652512 -4.2642717][-4.2474246 -4.2408161 -4.2360535 -4.2357969 -4.2378869 -4.2422347 -4.24502 -4.2447948 -4.247581 -4.2541347 -4.2584229 -4.2601681 -4.2622514 -4.2627015 -4.2644544]]...]
INFO - root - 2017-12-05 14:47:06.589247: step 19710, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 83h:19m:54s remains)
INFO - root - 2017-12-05 14:47:15.802595: step 19720, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 82h:06m:55s remains)
INFO - root - 2017-12-05 14:47:25.077783: step 19730, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 80h:11m:51s remains)
INFO - root - 2017-12-05 14:47:34.513902: step 19740, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 78h:46m:05s remains)
INFO - root - 2017-12-05 14:47:43.913316: step 19750, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 80h:13m:43s remains)
INFO - root - 2017-12-05 14:47:53.171240: step 19760, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 77h:18m:13s remains)
INFO - root - 2017-12-05 14:48:02.614749: step 19770, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 74h:29m:03s remains)
INFO - root - 2017-12-05 14:48:12.106695: step 19780, loss = 2.12, batch loss = 2.06 (8.7 examples/sec; 0.920 sec/batch; 79h:55m:25s remains)
INFO - root - 2017-12-05 14:48:21.483069: step 19790, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 79h:35m:17s remains)
INFO - root - 2017-12-05 14:48:30.986110: step 19800, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.965 sec/batch; 83h:50m:08s remains)
2017-12-05 14:48:31.737836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3282938 -4.3247852 -4.3090496 -4.2684259 -4.2141814 -4.1529603 -4.0810447 -4.0745816 -4.122097 -4.1639347 -4.1868982 -4.2127252 -4.2470903 -4.2787185 -4.2951965][-4.3315353 -4.33143 -4.3105083 -4.2553811 -4.1851711 -4.1029406 -4.0160818 -4.0109277 -4.0659375 -4.1141772 -4.1467175 -4.1846905 -4.2279906 -4.2641783 -4.2819724][-4.3249068 -4.3250036 -4.3004475 -4.2378912 -4.1566811 -4.0620885 -3.9691062 -3.9520254 -3.9947357 -4.0442252 -4.0880356 -4.1345844 -4.1871715 -4.2327585 -4.2560911][-4.3182611 -4.314363 -4.2866611 -4.2232337 -4.1390238 -4.0390134 -3.9440496 -3.9091468 -3.9201207 -3.9622912 -4.0227 -4.084496 -4.1483278 -4.2021275 -4.2342024][-4.3101935 -4.3009677 -4.2664928 -4.1994977 -4.1125584 -4.0098305 -3.9220057 -3.8802195 -3.8796115 -3.9310205 -4.00879 -4.0805373 -4.1471982 -4.1976142 -4.2313423][-4.3023396 -4.2840242 -4.2445993 -4.1695256 -4.0718164 -3.9607081 -3.8754213 -3.8428302 -3.8648756 -3.9448829 -4.0451822 -4.1223493 -4.1827927 -4.2223687 -4.2497897][-4.2969809 -4.2684021 -4.22051 -4.1327367 -4.0224791 -3.9129851 -3.84067 -3.8352702 -3.8916736 -3.9949172 -4.1027737 -4.18039 -4.2348228 -4.2656722 -4.2803063][-4.2804747 -4.2504921 -4.2045069 -4.1216731 -4.0231934 -3.9383078 -3.8935595 -3.9147382 -3.9786129 -4.0725436 -4.1675372 -4.2352242 -4.2816415 -4.3012776 -4.3055463][-4.2555447 -4.2324724 -4.2018738 -4.1472836 -4.0774198 -4.0233827 -4.0053287 -4.0337973 -4.0825682 -4.1474128 -4.2205629 -4.2750916 -4.3109207 -4.3214579 -4.3173103][-4.2336755 -4.2249479 -4.2117672 -4.1883483 -4.1530142 -4.1275234 -4.1233811 -4.1457567 -4.1741843 -4.2134948 -4.2673287 -4.307157 -4.3323555 -4.332747 -4.3159971][-4.2244868 -4.2280006 -4.2281637 -4.2269735 -4.2224441 -4.2150135 -4.21173 -4.2228427 -4.2380433 -4.2630925 -4.2989917 -4.3260727 -4.339458 -4.3290377 -4.3043585][-4.2305937 -4.2392292 -4.2454443 -4.2556462 -4.2703233 -4.2723246 -4.2708645 -4.2798028 -4.288403 -4.3020492 -4.3188987 -4.3303795 -4.3333192 -4.3177128 -4.2977958][-4.2594457 -4.2669611 -4.2734203 -4.2871637 -4.3072743 -4.3115387 -4.3103271 -4.318213 -4.32208 -4.3244438 -4.326407 -4.328186 -4.325448 -4.3116651 -4.3007693][-4.2940187 -4.2966461 -4.3003259 -4.3122082 -4.327805 -4.3316441 -4.3302245 -4.3349872 -4.3357515 -4.3307047 -4.3264837 -4.3250775 -4.3197212 -4.3104563 -4.3086162][-4.3142128 -4.3169065 -4.3181982 -4.323657 -4.3310142 -4.3312788 -4.3287859 -4.33106 -4.3300571 -4.3243074 -4.3206835 -4.3188748 -4.3125696 -4.3048215 -4.3075833]]...]
INFO - root - 2017-12-05 14:48:41.068405: step 19810, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 80h:39m:31s remains)
INFO - root - 2017-12-05 14:48:50.413635: step 19820, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 82h:25m:56s remains)
INFO - root - 2017-12-05 14:48:59.759568: step 19830, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 83h:33m:16s remains)
INFO - root - 2017-12-05 14:49:09.334584: step 19840, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 82h:21m:22s remains)
INFO - root - 2017-12-05 14:49:18.778866: step 19850, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 82h:21m:54s remains)
INFO - root - 2017-12-05 14:49:28.271043: step 19860, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.015 sec/batch; 88h:11m:00s remains)
INFO - root - 2017-12-05 14:49:37.728056: step 19870, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 85h:08m:15s remains)
INFO - root - 2017-12-05 14:49:47.190608: step 19880, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 81h:51m:52s remains)
INFO - root - 2017-12-05 14:49:56.413501: step 19890, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.938 sec/batch; 81h:26m:37s remains)
INFO - root - 2017-12-05 14:50:05.564501: step 19900, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 81h:24m:39s remains)
2017-12-05 14:50:06.424303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1987958 -4.1475067 -4.1081567 -4.1084595 -4.1347761 -4.1671305 -4.1869717 -4.195127 -4.1962609 -4.1963253 -4.2007818 -4.2029943 -4.1984677 -4.1891642 -4.1873164][-4.1974568 -4.15211 -4.1291008 -4.1360416 -4.157393 -4.1761394 -4.1842012 -4.1880074 -4.188211 -4.1857543 -4.1862755 -4.19384 -4.1995225 -4.1971912 -4.2023396][-4.204711 -4.1688666 -4.155458 -4.1580019 -4.1711082 -4.1828651 -4.1840677 -4.1806016 -4.1788177 -4.1780868 -4.1725626 -4.1748428 -4.1815639 -4.1805129 -4.1893344][-4.2186279 -4.1940002 -4.1833777 -4.1776843 -4.1794362 -4.1861939 -4.1845326 -4.16921 -4.152545 -4.14983 -4.138401 -4.1283517 -4.128695 -4.1315026 -4.1447287][-4.2343321 -4.2142816 -4.2006869 -4.1878057 -4.180377 -4.1802073 -4.1745687 -4.1503425 -4.1176586 -4.1053839 -4.08742 -4.0650015 -4.060729 -4.0727358 -4.093019][-4.23501 -4.213407 -4.1961203 -4.1826243 -4.1667404 -4.1530876 -4.1374259 -4.1076283 -4.0665851 -4.0460258 -4.0269232 -4.0053988 -4.0044889 -4.02213 -4.0441594][-4.2198095 -4.1960783 -4.1784282 -4.165833 -4.1410317 -4.1124206 -4.0841541 -4.0473027 -4.0061669 -3.9869306 -3.9826088 -3.9791858 -3.9895968 -4.0093093 -4.0302815][-4.2060442 -4.1790991 -4.1539068 -4.1342373 -4.1028605 -4.0701208 -4.0353527 -3.9997497 -3.9724894 -3.9700046 -3.9850695 -4.0022836 -4.0220656 -4.0387182 -4.0545359][-4.1932545 -4.1609178 -4.1276283 -4.1040068 -4.0757685 -4.0496626 -4.0187459 -3.9964073 -3.9915292 -4.006402 -4.0322227 -4.0546961 -4.0720415 -4.0803771 -4.0889139][-4.17231 -4.136631 -4.1071897 -4.0934792 -4.0777912 -4.0649061 -4.0484147 -4.042819 -4.0479755 -4.0603871 -4.07921 -4.0925198 -4.099463 -4.0988731 -4.1017394][-4.1507478 -4.1190577 -4.1024389 -4.1034012 -4.098259 -4.0949907 -4.0875 -4.09126 -4.0968838 -4.0962958 -4.0996561 -4.102201 -4.1031871 -4.0964389 -4.0951247][-4.1466246 -4.125145 -4.1199517 -4.1311827 -4.1330471 -4.1320477 -4.124927 -4.1304212 -4.1334743 -4.123477 -4.1158543 -4.1106987 -4.10671 -4.0995631 -4.0991888][-4.1751595 -4.1631165 -4.1656914 -4.178854 -4.1847506 -4.1831732 -4.1736379 -4.1744881 -4.177228 -4.1680884 -4.1577106 -4.1491556 -4.1428814 -4.1383281 -4.1390476][-4.2290955 -4.2262907 -4.2316566 -4.240952 -4.2465687 -4.24536 -4.2365818 -4.2338672 -4.2365642 -4.229311 -4.2201357 -4.2110658 -4.2029743 -4.199224 -4.2012439][-4.2787347 -4.2810817 -4.285872 -4.2901416 -4.2934704 -4.2921052 -4.2851982 -4.2820015 -4.2835155 -4.2791157 -4.2724676 -4.2657876 -4.2611051 -4.258441 -4.2602425]]...]
INFO - root - 2017-12-05 14:50:15.969180: step 19910, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 77h:04m:49s remains)
INFO - root - 2017-12-05 14:50:25.568553: step 19920, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 80h:56m:15s remains)
INFO - root - 2017-12-05 14:50:34.884143: step 19930, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.969 sec/batch; 84h:07m:55s remains)
INFO - root - 2017-12-05 14:50:44.406537: step 19940, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 78h:11m:20s remains)
INFO - root - 2017-12-05 14:50:53.712529: step 19950, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 80h:08m:59s remains)
INFO - root - 2017-12-05 14:51:02.849780: step 19960, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 82h:15m:49s remains)
INFO - root - 2017-12-05 14:51:12.194130: step 19970, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 81h:37m:28s remains)
INFO - root - 2017-12-05 14:51:21.772381: step 19980, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.910 sec/batch; 79h:01m:52s remains)
INFO - root - 2017-12-05 14:51:31.045069: step 19990, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.974 sec/batch; 84h:32m:00s remains)
INFO - root - 2017-12-05 14:51:40.559811: step 20000, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 80h:00m:11s remains)
2017-12-05 14:51:41.294465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3179917 -4.2987647 -4.281239 -4.2726393 -4.2756085 -4.2923594 -4.3155036 -4.333436 -4.3402677 -4.3352122 -4.3241682 -4.3110414 -4.3027692 -4.3037286 -4.3127871][-4.3148489 -4.2848582 -4.2540321 -4.2314692 -4.2292204 -4.25539 -4.2954154 -4.3279071 -4.3427863 -4.3415308 -4.33035 -4.3134327 -4.2990766 -4.2959452 -4.3049917][-4.3086295 -4.2668958 -4.2183118 -4.1783924 -4.1676965 -4.2009425 -4.2585955 -4.3097243 -4.3376756 -4.3442245 -4.3355708 -4.3157582 -4.294764 -4.2853208 -4.292048][-4.3030128 -4.2517619 -4.1862025 -4.1265488 -4.1012959 -4.1314754 -4.2002573 -4.2695007 -4.3155684 -4.338151 -4.33957 -4.3230958 -4.2959414 -4.2764306 -4.2769327][-4.2991524 -4.2413254 -4.1627116 -4.0839667 -4.0374827 -4.0478272 -4.1095939 -4.1886325 -4.2559233 -4.3036995 -4.3279028 -4.3253703 -4.2996283 -4.2725616 -4.2643995][-4.297718 -4.2386122 -4.1508007 -4.0506425 -3.9729557 -3.9499393 -3.9896908 -4.06898 -4.1572533 -4.2350736 -4.2892547 -4.311851 -4.3002529 -4.2746735 -4.2609706][-4.2990837 -4.2462573 -4.1586518 -4.0396361 -3.9168448 -3.8426509 -3.8493166 -3.9318316 -4.0488062 -4.1541705 -4.2316504 -4.2799106 -4.2931223 -4.2807531 -4.2673626][-4.3034039 -4.2648678 -4.1891069 -4.0695081 -3.9235115 -3.8010955 -3.7650256 -3.8411098 -3.9748843 -4.09383 -4.180995 -4.2416773 -4.2752752 -4.280313 -4.2747736][-4.3085485 -4.288301 -4.2371426 -4.1417136 -4.0118027 -3.8870182 -3.8250365 -3.8678236 -3.9691911 -4.0689764 -4.1461949 -4.2069345 -4.2499857 -4.2698817 -4.2745972][-4.3078685 -4.2993574 -4.275434 -4.2150993 -4.1213284 -4.02224 -3.9598663 -3.9650509 -4.0117 -4.0673561 -4.121191 -4.1755757 -4.2222004 -4.251894 -4.2660618][-4.2963085 -4.2952609 -4.2893529 -4.2598 -4.1994042 -4.126811 -4.0686455 -4.0494747 -4.060998 -4.0838323 -4.1139617 -4.1563148 -4.1997075 -4.2324586 -4.2530389][-4.280973 -4.2858515 -4.2903442 -4.2793894 -4.2415624 -4.1854148 -4.1310863 -4.1009283 -4.0984559 -4.1104126 -4.125494 -4.1520023 -4.1864328 -4.2172456 -4.24179][-4.2684946 -4.2787318 -4.290772 -4.2918625 -4.2692666 -4.2244258 -4.1737947 -4.1402297 -4.1345763 -4.1432738 -4.1515493 -4.1634159 -4.1843486 -4.20948 -4.2344408][-4.2631478 -4.2742963 -4.2894263 -4.29939 -4.2901931 -4.2591953 -4.2172179 -4.1853685 -4.1758976 -4.180017 -4.1850667 -4.1891179 -4.197114 -4.212997 -4.2345977][-4.2581296 -4.2706151 -4.2853589 -4.298183 -4.2996316 -4.2852216 -4.2585182 -4.2338834 -4.2215023 -4.2205749 -4.2223129 -4.2220793 -4.2228708 -4.230772 -4.2459521]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 14:51:51.707631: step 20010, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.975 sec/batch; 84h:39m:40s remains)
INFO - root - 2017-12-05 14:52:01.179868: step 20020, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.945 sec/batch; 82h:02m:33s remains)
INFO - root - 2017-12-05 14:52:10.622056: step 20030, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 84h:15m:47s remains)
INFO - root - 2017-12-05 14:52:19.937159: step 20040, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 73h:37m:00s remains)
INFO - root - 2017-12-05 14:52:29.365624: step 20050, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 80h:47m:23s remains)
INFO - root - 2017-12-05 14:52:38.776195: step 20060, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 82h:51m:21s remains)
INFO - root - 2017-12-05 14:52:48.320955: step 20070, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 85h:32m:24s remains)
INFO - root - 2017-12-05 14:52:57.421532: step 20080, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 82h:26m:13s remains)
INFO - root - 2017-12-05 14:53:06.712021: step 20090, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 71h:53m:34s remains)
INFO - root - 2017-12-05 14:53:16.012401: step 20100, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 78h:28m:50s remains)
2017-12-05 14:53:16.761190: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1861639 -4.1814651 -4.184763 -4.1931381 -4.2002516 -4.197804 -4.1856079 -4.1742196 -4.1766191 -4.1913519 -4.1963034 -4.1962686 -4.1999512 -4.2055597 -4.2281852][-4.1745858 -4.1725416 -4.1808081 -4.1974735 -4.2130561 -4.216188 -4.2064652 -4.195158 -4.1963606 -4.2045078 -4.1989689 -4.1892495 -4.190691 -4.1979156 -4.2197075][-4.1762233 -4.1746197 -4.1887674 -4.2118692 -4.2314591 -4.2385192 -4.2352896 -4.2285442 -4.230505 -4.2288604 -4.2097344 -4.191401 -4.1930861 -4.2021604 -4.2214313][-4.1766076 -4.1723928 -4.1855311 -4.2133722 -4.2369432 -4.2486558 -4.2507768 -4.2502489 -4.2580209 -4.2556615 -4.2302628 -4.2084479 -4.2072077 -4.212585 -4.2284713][-4.1505222 -4.1385841 -4.145545 -4.1764541 -4.2063589 -4.2203908 -4.2231712 -4.2308407 -4.24944 -4.2574959 -4.2407885 -4.226191 -4.2238979 -4.2271261 -4.2394252][-4.1031866 -4.0762687 -4.0673776 -4.0947642 -4.1290669 -4.1492 -4.1573381 -4.1745429 -4.2038789 -4.2260509 -4.2280812 -4.2284961 -4.2322674 -4.2405248 -4.2514877][-4.0298176 -3.9784505 -3.9559155 -3.9827583 -4.0211449 -4.0452724 -4.0590396 -4.0875154 -4.1335773 -4.1745758 -4.1964025 -4.2088151 -4.2201166 -4.236413 -4.2526884][-4.0162983 -3.9491224 -3.9161744 -3.9335713 -3.9615841 -3.9730582 -3.9811943 -4.0141745 -4.0707145 -4.1215167 -4.1570606 -4.1799927 -4.1984019 -4.2221136 -4.24591][-4.0954161 -4.0474229 -4.020493 -4.0189285 -4.0217609 -4.0131187 -4.0122266 -4.0390887 -4.0852036 -4.1291842 -4.1588535 -4.1762228 -4.1894622 -4.2114272 -4.2379112][-4.161705 -4.1371775 -4.1287446 -4.1243229 -4.1178989 -4.0986371 -4.0920758 -4.1115232 -4.1426749 -4.1712523 -4.186933 -4.193655 -4.2008176 -4.2189536 -4.2412658][-4.1944618 -4.1924019 -4.193893 -4.1911149 -4.1834106 -4.1634378 -4.1538935 -4.16815 -4.1886334 -4.2049875 -4.2083817 -4.2096996 -4.2167773 -4.2336311 -4.2512727][-4.1745019 -4.1863732 -4.1957803 -4.1969724 -4.1905723 -4.174315 -4.1688533 -4.1863403 -4.2053809 -4.2187281 -4.2158709 -4.2130079 -4.2203693 -4.2375422 -4.2558179][-4.1431203 -4.150156 -4.1531596 -4.1560941 -4.1561403 -4.1446147 -4.1426549 -4.1654654 -4.1934304 -4.2159343 -4.2193189 -4.2191129 -4.224287 -4.2399087 -4.2573972][-4.1362023 -4.1291094 -4.11753 -4.1139927 -4.114182 -4.1055794 -4.104672 -4.13003 -4.171402 -4.207809 -4.2222424 -4.2274666 -4.2331705 -4.2469268 -4.2594614][-4.1451716 -4.1304412 -4.1031613 -4.0872912 -4.0864496 -4.0840683 -4.0879631 -4.1089444 -4.1506605 -4.1947541 -4.2180023 -4.229857 -4.23904 -4.2506533 -4.2573929]]...]
INFO - root - 2017-12-05 14:53:26.166798: step 20110, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 83h:59m:09s remains)
INFO - root - 2017-12-05 14:53:35.766127: step 20120, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 85h:07m:50s remains)
INFO - root - 2017-12-05 14:53:45.134705: step 20130, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 85h:12m:48s remains)
INFO - root - 2017-12-05 14:53:54.460913: step 20140, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 84h:19m:23s remains)
INFO - root - 2017-12-05 14:54:03.920921: step 20150, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 81h:11m:45s remains)
INFO - root - 2017-12-05 14:54:13.459450: step 20160, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.994 sec/batch; 86h:16m:15s remains)
INFO - root - 2017-12-05 14:54:22.609995: step 20170, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 79h:31m:37s remains)
INFO - root - 2017-12-05 14:54:32.175577: step 20180, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 84h:24m:53s remains)
INFO - root - 2017-12-05 14:54:41.693411: step 20190, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 85h:44m:00s remains)
INFO - root - 2017-12-05 14:54:51.149188: step 20200, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 84h:21m:11s remains)
2017-12-05 14:54:51.902210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2241631 -4.2376933 -4.257298 -4.268311 -4.2676511 -4.2617016 -4.2500815 -4.2336035 -4.2222905 -4.2210069 -4.2229137 -4.2229767 -4.22325 -4.22315 -4.221261][-4.2503996 -4.2610412 -4.2718029 -4.2738709 -4.2648077 -4.2481585 -4.2256031 -4.2019753 -4.1895823 -4.1931729 -4.1990881 -4.2015581 -4.2043719 -4.2061391 -4.20593][-4.2772388 -4.2827268 -4.285171 -4.277739 -4.2605853 -4.2368731 -4.2099342 -4.1870842 -4.178441 -4.1846275 -4.1888905 -4.1879339 -4.1888514 -4.1902118 -4.19177][-4.2820244 -4.28184 -4.2780108 -4.2662635 -4.2480335 -4.2259064 -4.2028761 -4.1865244 -4.1860375 -4.1953492 -4.1984978 -4.1948514 -4.1925144 -4.1912889 -4.1918678][-4.2497382 -4.2475848 -4.2432532 -4.2328238 -4.216156 -4.1985188 -4.1818409 -4.1729965 -4.182961 -4.1990414 -4.2034626 -4.2010775 -4.1997857 -4.2004213 -4.2008038][-4.1927547 -4.186903 -4.1829762 -4.174386 -4.1602221 -4.1452808 -4.1320543 -4.1298933 -4.1502504 -4.1764164 -4.186554 -4.1861892 -4.1892676 -4.1959372 -4.1991339][-4.1391263 -4.1248269 -4.11644 -4.104496 -4.0865273 -4.0687723 -4.0537896 -4.0542836 -4.0851927 -4.1238842 -4.1431561 -4.1470289 -4.1541176 -4.1674533 -4.1779552][-4.1105409 -4.0843077 -4.0652566 -4.0378251 -4.0009341 -3.9655092 -3.9384372 -3.9398048 -3.9907391 -4.0524158 -4.0831556 -4.091928 -4.1044159 -4.126946 -4.1494789][-4.1257429 -4.0935507 -4.0647388 -4.01806 -3.9566884 -3.8944285 -3.8412611 -3.8305309 -3.8957143 -3.9797907 -4.0223804 -4.03691 -4.0547242 -4.0846448 -4.1170025][-4.1664815 -4.1365738 -4.108489 -4.0611849 -4.0011444 -3.9406207 -3.884321 -3.8555317 -3.8878198 -3.9475543 -3.9799316 -3.993365 -4.0138893 -4.0478354 -4.08605][-4.2082615 -4.1884885 -4.1704793 -4.1344872 -4.0888405 -4.0422258 -3.9946566 -3.9576342 -3.9577127 -3.9804623 -3.9878721 -3.9877791 -4.0027328 -4.0338354 -4.0715084][-4.2503109 -4.2407255 -4.2311025 -4.207727 -4.1772847 -4.1428547 -4.1017566 -4.0613036 -4.0421095 -4.0399041 -4.0306411 -4.0204191 -4.0267096 -4.0497932 -4.0802045][-4.287878 -4.2826052 -4.2746463 -4.2568941 -4.2381072 -4.220006 -4.194973 -4.1636438 -4.140564 -4.1272764 -4.1114974 -4.09661 -4.0956006 -4.1094489 -4.129878][-4.3158288 -4.3110723 -4.3029451 -4.2887759 -4.277823 -4.2704077 -4.2594619 -4.2431393 -4.2290196 -4.2193332 -4.2063308 -4.1920524 -4.1861734 -4.1898685 -4.1992393][-4.3325558 -4.3277526 -4.3210244 -4.3114161 -4.3046904 -4.30075 -4.2950163 -4.2847414 -4.2758117 -4.2710557 -4.2645264 -4.2545567 -4.2472529 -4.2445722 -4.2454586]]...]
INFO - root - 2017-12-05 14:55:01.203312: step 20210, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 82h:44m:23s remains)
INFO - root - 2017-12-05 14:55:10.821920: step 20220, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 80h:57m:03s remains)
INFO - root - 2017-12-05 14:55:20.271098: step 20230, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 81h:22m:47s remains)
INFO - root - 2017-12-05 14:55:29.499529: step 20240, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 85h:49m:31s remains)
INFO - root - 2017-12-05 14:55:38.653543: step 20250, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 79h:05m:48s remains)
INFO - root - 2017-12-05 14:55:48.042906: step 20260, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 80h:58m:20s remains)
INFO - root - 2017-12-05 14:55:57.336931: step 20270, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 79h:14m:56s remains)
INFO - root - 2017-12-05 14:56:06.714099: step 20280, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 80h:24m:43s remains)
INFO - root - 2017-12-05 14:56:16.417727: step 20290, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 85h:09m:43s remains)
INFO - root - 2017-12-05 14:56:25.995452: step 20300, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.982 sec/batch; 85h:11m:01s remains)
2017-12-05 14:56:26.723695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3142262 -4.300108 -4.2907476 -4.2887139 -4.2948933 -4.3028126 -4.3119183 -4.3144321 -4.3150067 -4.3151684 -4.3097363 -4.298521 -4.2789164 -4.2600112 -4.2564569][-4.3157206 -4.2990322 -4.2880874 -4.2840481 -4.2868834 -4.2950125 -4.3070812 -4.3110433 -4.3128653 -4.3137336 -4.3082995 -4.2969689 -4.2766128 -4.2572269 -4.2517462][-4.3062034 -4.286593 -4.2740107 -4.268662 -4.2694159 -4.2774625 -4.2911263 -4.2997785 -4.3071451 -4.3099227 -4.3016548 -4.2873673 -4.2627616 -4.240521 -4.2339206][-4.2941556 -4.271956 -4.2542934 -4.2398629 -4.2346764 -4.2455282 -4.265914 -4.2848697 -4.3014822 -4.3101606 -4.3016357 -4.2835484 -4.25392 -4.2247119 -4.211926][-4.2830396 -4.2596073 -4.2348089 -4.2050676 -4.1829424 -4.189033 -4.2182074 -4.2557611 -4.2871294 -4.3071251 -4.3087144 -4.2918539 -4.2591038 -4.2212267 -4.1986961][-4.2779832 -4.2544026 -4.2229571 -4.178586 -4.1368651 -4.1250882 -4.1511478 -4.204329 -4.256249 -4.2922425 -4.3079643 -4.3022885 -4.2776346 -4.2382717 -4.2077308][-4.282269 -4.2594318 -4.2240124 -4.1691284 -4.1060033 -4.0644436 -4.0682449 -4.1166673 -4.1878452 -4.2490993 -4.2829251 -4.2923179 -4.2861695 -4.2626987 -4.2375531][-4.290453 -4.269639 -4.2369843 -4.1824532 -4.1085205 -4.0345526 -3.9886556 -3.9976678 -4.0726933 -4.1676292 -4.233604 -4.2684646 -4.2840285 -4.2827806 -4.2706709][-4.295701 -4.2810163 -4.259521 -4.2221856 -4.1593232 -4.0779715 -3.9923353 -3.93584 -3.9730492 -4.0723858 -4.1611977 -4.2239738 -4.2650185 -4.2870545 -4.2915812][-4.2898836 -4.2816877 -4.2720203 -4.2560406 -4.220932 -4.1668105 -4.09753 -4.0303526 -4.0143695 -4.0543032 -4.113122 -4.1755495 -4.2282052 -4.2678518 -4.2931647][-4.2698164 -4.2646389 -4.2644958 -4.2623205 -4.249917 -4.2289381 -4.1973009 -4.1589317 -4.13109 -4.1253676 -4.13404 -4.1611056 -4.1953535 -4.233614 -4.2736483][-4.2502031 -4.24364 -4.2397642 -4.2396731 -4.2388477 -4.2414956 -4.2419019 -4.2342715 -4.221478 -4.2091966 -4.1977663 -4.1885614 -4.1830158 -4.1990805 -4.2373905][-4.2377381 -4.2269559 -4.2102509 -4.2019134 -4.2020645 -4.2148991 -4.2338619 -4.250577 -4.2567086 -4.2521625 -4.2434816 -4.2286029 -4.2040739 -4.1942129 -4.211154][-4.2135921 -4.2074695 -4.1874652 -4.1682682 -4.1566586 -4.1614861 -4.182682 -4.2152958 -4.2393222 -4.2462864 -4.246336 -4.2399807 -4.22169 -4.20351 -4.1984644][-4.1740212 -4.1794615 -4.1667385 -4.1482744 -4.13113 -4.122901 -4.1309066 -4.1604891 -4.1918063 -4.2087917 -4.217792 -4.2230043 -4.2197361 -4.20814 -4.1955686]]...]
INFO - root - 2017-12-05 14:56:36.152076: step 20310, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 83h:12m:56s remains)
INFO - root - 2017-12-05 14:56:45.914053: step 20320, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 86h:23m:49s remains)
INFO - root - 2017-12-05 14:56:55.255053: step 20330, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 82h:18m:41s remains)
INFO - root - 2017-12-05 14:57:04.587199: step 20340, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.007 sec/batch; 87h:21m:35s remains)
INFO - root - 2017-12-05 14:57:13.785997: step 20350, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 79h:26m:07s remains)
INFO - root - 2017-12-05 14:57:23.285411: step 20360, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.930 sec/batch; 80h:40m:30s remains)
INFO - root - 2017-12-05 14:57:32.462007: step 20370, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 84h:43m:06s remains)
INFO - root - 2017-12-05 14:57:41.884580: step 20380, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 82h:20m:47s remains)
INFO - root - 2017-12-05 14:57:51.282384: step 20390, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 80h:19m:15s remains)
INFO - root - 2017-12-05 14:58:00.684478: step 20400, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 82h:17m:46s remains)
2017-12-05 14:58:01.491458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1481438 -4.1613445 -4.1737404 -4.1738062 -4.1681466 -4.1651082 -4.1673193 -4.16818 -4.1687613 -4.1758318 -4.1937079 -4.2145348 -4.2299438 -4.2441106 -4.2465277][-4.1143436 -4.1203723 -4.133862 -4.1336012 -4.1273108 -4.1277022 -4.1332021 -4.1355939 -4.1331944 -4.1315017 -4.1389813 -4.1541982 -4.174994 -4.2040734 -4.2289491][-4.0985155 -4.0996 -4.1177349 -4.124383 -4.1211557 -4.1239071 -4.1295257 -4.1288228 -4.1181564 -4.1036534 -4.0961661 -4.100563 -4.1195617 -4.154995 -4.1980705][-4.1081886 -4.1081915 -4.1299968 -4.144701 -4.148736 -4.1526284 -4.1515126 -4.138124 -4.1132321 -4.086102 -4.0708737 -4.0757785 -4.0990586 -4.1390471 -4.1904016][-4.1489453 -4.1453052 -4.1653228 -4.1834726 -4.1901236 -4.1881714 -4.1736293 -4.1423044 -4.0996222 -4.0599151 -4.0449533 -4.065361 -4.1067133 -4.1567745 -4.2118397][-4.21628 -4.2110324 -4.2251463 -4.2386069 -4.2391186 -4.2247667 -4.1936259 -4.1428657 -4.0835752 -4.0360007 -4.0295959 -4.0737057 -4.1357489 -4.1935382 -4.2447062][-4.2814674 -4.2776556 -4.2869091 -4.2944713 -4.28731 -4.2608271 -4.2160907 -4.1559329 -4.0929394 -4.0481095 -4.0530443 -4.1107645 -4.1806345 -4.2363977 -4.2767453][-4.3226309 -4.3226147 -4.328608 -4.3305349 -4.3186169 -4.2886863 -4.243576 -4.1879396 -4.1340685 -4.0985742 -4.1106381 -4.1680713 -4.2305083 -4.2750506 -4.300868][-4.3280506 -4.3326859 -4.3388472 -4.3405414 -4.3314009 -4.3085213 -4.2743616 -4.2339964 -4.1963668 -4.1712604 -4.1798763 -4.22122 -4.265624 -4.2950015 -4.3080668][-4.3009882 -4.3135862 -4.32546 -4.333252 -4.3329597 -4.32272 -4.3048344 -4.282784 -4.2603593 -4.2416439 -4.2401757 -4.2595878 -4.2828822 -4.2971816 -4.3021569][-4.26591 -4.2876225 -4.30599 -4.3191552 -4.325613 -4.3253269 -4.3201594 -4.3120074 -4.3009882 -4.2868962 -4.277564 -4.2802281 -4.2874694 -4.2913337 -4.29267][-4.2427731 -4.2685881 -4.289784 -4.3043885 -4.3120537 -4.3154883 -4.3164086 -4.3156643 -4.3113885 -4.3016443 -4.291698 -4.2878284 -4.28703 -4.2858763 -4.2863154][-4.2397666 -4.2646008 -4.2839417 -4.29539 -4.2999253 -4.3007917 -4.3001194 -4.2988567 -4.2956285 -4.2892976 -4.2832294 -4.2807727 -4.2792468 -4.2794881 -4.283072][-4.25183 -4.272326 -4.2877765 -4.2948704 -4.2946844 -4.290524 -4.284337 -4.2782259 -4.2727919 -4.268187 -4.2659812 -4.2667847 -4.2681456 -4.2724986 -4.2804303][-4.2720556 -4.285955 -4.2967176 -4.3008614 -4.2985377 -4.2918749 -4.281508 -4.2698097 -4.2599163 -4.2536683 -4.2525773 -4.2555432 -4.2596226 -4.2673368 -4.2789803]]...]
INFO - root - 2017-12-05 14:58:10.972399: step 20410, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 81h:17m:03s remains)
INFO - root - 2017-12-05 14:58:20.574099: step 20420, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.937 sec/batch; 81h:12m:50s remains)
INFO - root - 2017-12-05 14:58:30.220054: step 20430, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 85h:45m:07s remains)
INFO - root - 2017-12-05 14:58:39.607976: step 20440, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 83h:28m:48s remains)
INFO - root - 2017-12-05 14:58:48.989371: step 20450, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 82h:44m:58s remains)
INFO - root - 2017-12-05 14:58:58.292221: step 20460, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 78h:04m:14s remains)
INFO - root - 2017-12-05 14:59:07.662986: step 20470, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 79h:35m:20s remains)
INFO - root - 2017-12-05 14:59:17.153569: step 20480, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 83h:07m:04s remains)
INFO - root - 2017-12-05 14:59:26.614247: step 20490, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 84h:21m:06s remains)
INFO - root - 2017-12-05 14:59:36.041656: step 20500, loss = 2.04, batch loss = 1.98 (8.0 examples/sec; 0.996 sec/batch; 86h:18m:56s remains)
2017-12-05 14:59:36.843827: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1429 -4.1477113 -4.15402 -4.1808004 -4.2107205 -4.2231231 -4.2125697 -4.1958361 -4.1740942 -4.1859035 -4.1958513 -4.1988668 -4.200058 -4.1964703 -4.2007632][-4.1249175 -4.1289577 -4.1390905 -4.1775208 -4.2108045 -4.2234735 -4.2201419 -4.21522 -4.2007003 -4.2093005 -4.2102647 -4.2003207 -4.193162 -4.1843505 -4.1882992][-4.1397595 -4.1360297 -4.1453447 -4.1812248 -4.2041392 -4.2019563 -4.1922269 -4.1893854 -4.1855803 -4.192843 -4.1889977 -4.1685853 -4.155745 -4.1501846 -4.1630011][-4.1695809 -4.1627069 -4.1685209 -4.1895509 -4.1932864 -4.173665 -4.1516294 -4.1486816 -4.1591439 -4.1731319 -4.164268 -4.1345129 -4.11103 -4.1091037 -4.1328049][-4.1746197 -4.173707 -4.1808505 -4.190896 -4.1747546 -4.1362967 -4.0937619 -4.0840826 -4.1130614 -4.1435413 -4.143806 -4.1160817 -4.087606 -4.085094 -4.1113687][-4.1618276 -4.1691136 -4.1806498 -4.1798968 -4.1475983 -4.0878749 -4.0043521 -3.9611044 -4.0083966 -4.0712934 -4.0982127 -4.0907354 -4.0687494 -4.0678744 -4.0898643][-4.1616225 -4.1755733 -4.1836495 -4.1729293 -4.1246133 -4.0340967 -3.8964171 -3.7947407 -3.8593965 -3.9723165 -4.0411758 -4.0584326 -4.0450974 -4.0416322 -4.0553117][-4.1821728 -4.1976237 -4.2012739 -4.1841474 -4.13754 -4.0461612 -3.8909466 -3.7449198 -3.7831404 -3.9049561 -3.9968374 -4.0314856 -4.0265346 -4.0207195 -4.0304189][-4.1922431 -4.2072263 -4.2143564 -4.2053714 -4.1803188 -4.1236639 -4.0171323 -3.8998017 -3.8829033 -3.9426463 -4.010262 -4.0423269 -4.0372686 -4.029613 -4.034678][-4.18576 -4.1982503 -4.2116508 -4.2176495 -4.217464 -4.1926732 -4.1305709 -4.0482965 -4.0031166 -4.0120125 -4.0508761 -4.07751 -4.0773029 -4.0714235 -4.0703082][-4.176815 -4.1816487 -4.2006679 -4.2206159 -4.2376142 -4.2378325 -4.2121949 -4.1664672 -4.1186552 -4.098177 -4.1102657 -4.12487 -4.1287041 -4.123621 -4.1168165][-4.1731935 -4.1644473 -4.181272 -4.2082314 -4.23905 -4.2559967 -4.2512493 -4.230557 -4.1970572 -4.1704431 -4.1712933 -4.1832671 -4.1927276 -4.1876063 -4.177][-4.1776662 -4.1550331 -4.1623263 -4.1906886 -4.2294297 -4.2548184 -4.2545815 -4.2459645 -4.2326617 -4.2160273 -4.2179914 -4.232882 -4.2495112 -4.2491078 -4.2368298][-4.1935859 -4.16319 -4.1598754 -4.1777449 -4.2094059 -4.2321482 -4.2325816 -4.23474 -4.2357025 -4.231638 -4.2395034 -4.2562051 -4.2751393 -4.2811179 -4.2733674][-4.2172461 -4.1956697 -4.18473 -4.1828408 -4.1932096 -4.1988893 -4.1924815 -4.2021031 -4.2133603 -4.22042 -4.23256 -4.2510486 -4.2729292 -4.2839646 -4.2830896]]...]
INFO - root - 2017-12-05 14:59:45.974834: step 20510, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.927 sec/batch; 80h:20m:45s remains)
INFO - root - 2017-12-05 14:59:55.454808: step 20520, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 84h:15m:37s remains)
INFO - root - 2017-12-05 15:00:04.947817: step 20530, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.933 sec/batch; 80h:51m:55s remains)
INFO - root - 2017-12-05 15:00:13.974608: step 20540, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 80h:46m:55s remains)
INFO - root - 2017-12-05 15:00:23.203351: step 20550, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.942 sec/batch; 81h:36m:20s remains)
INFO - root - 2017-12-05 15:00:32.537099: step 20560, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 76h:55m:06s remains)
INFO - root - 2017-12-05 15:00:42.061105: step 20570, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 79h:33m:36s remains)
INFO - root - 2017-12-05 15:00:51.464629: step 20580, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 81h:21m:39s remains)
INFO - root - 2017-12-05 15:01:00.880791: step 20590, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 78h:16m:31s remains)
INFO - root - 2017-12-05 15:01:10.395177: step 20600, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 84h:48m:40s remains)
2017-12-05 15:01:11.191544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2507224 -4.240891 -4.2477117 -4.2584887 -4.2629876 -4.256741 -4.2434978 -4.2377229 -4.2486739 -4.2664266 -4.2803726 -4.2758317 -4.2449403 -4.2105336 -4.2040548][-4.2391958 -4.2252455 -4.2370195 -4.2609344 -4.2777328 -4.2705212 -4.2456656 -4.2278352 -4.2343578 -4.2545424 -4.2727718 -4.2705579 -4.2389507 -4.2042561 -4.1986613][-4.1918879 -4.1737618 -4.1962 -4.2420807 -4.2786098 -4.2755837 -4.2415962 -4.2117524 -4.2125726 -4.2393494 -4.2689123 -4.2734203 -4.2424779 -4.2065163 -4.196537][-4.1101704 -4.0870519 -4.125494 -4.1988382 -4.2538342 -4.2543716 -4.2114191 -4.1707373 -4.1708736 -4.210947 -4.2572427 -4.2719579 -4.245585 -4.2079778 -4.1900921][-4.0036 -3.9802012 -4.0425706 -4.1418772 -4.2079673 -4.2044864 -4.1478906 -4.0930581 -4.0970016 -4.1554146 -4.2203469 -4.2512188 -4.2362452 -4.2030606 -4.1793537][-3.9302652 -3.917913 -3.9998546 -4.1067257 -4.1615434 -4.1357346 -4.0547929 -3.9845805 -3.9991236 -4.0809026 -4.1696081 -4.2210875 -4.2232251 -4.1981792 -4.1716285][-3.9666903 -3.9762142 -4.0474763 -4.1218376 -4.138895 -4.0827265 -3.9800179 -3.9063404 -3.9378743 -4.0376019 -4.1400638 -4.2031479 -4.2179232 -4.1996074 -4.1732287][-4.0748262 -4.0919237 -4.1324868 -4.1598139 -4.1419473 -4.0767288 -3.9888527 -3.941489 -3.976315 -4.06129 -4.1493454 -4.2077227 -4.2269588 -4.2135534 -4.1863904][-4.1559825 -4.1665697 -4.1821051 -4.1816778 -4.1525 -4.105372 -4.0587416 -4.0418057 -4.0665812 -4.1207132 -4.1811132 -4.2249517 -4.2408166 -4.230732 -4.2028913][-4.189436 -4.1908574 -4.1898489 -4.1752954 -4.1466761 -4.1214986 -4.1069207 -4.1123981 -4.13381 -4.1688352 -4.2095156 -4.2410526 -4.2517648 -4.2401519 -4.2115812][-4.20411 -4.1969433 -4.1830053 -4.1584158 -4.1303668 -4.1178994 -4.121347 -4.1401219 -4.165184 -4.1971426 -4.2303824 -4.2549973 -4.2600965 -4.2426267 -4.2114959][-4.2185726 -4.2025213 -4.1787157 -4.1509762 -4.1274285 -4.1220641 -4.1305442 -4.1524425 -4.1791224 -4.2104292 -4.2414651 -4.2624431 -4.2647715 -4.2451224 -4.213079][-4.23112 -4.2134132 -4.1902294 -4.1659226 -4.1484108 -4.146235 -4.1532674 -4.1700888 -4.1931472 -4.2212539 -4.2506971 -4.2710738 -4.273447 -4.255219 -4.2246037][-4.2369676 -4.2226028 -4.2077303 -4.1930332 -4.1841073 -4.1851559 -4.1908917 -4.2033658 -4.2212744 -4.2427688 -4.2659168 -4.2828889 -4.28517 -4.2688632 -4.2385836][-4.2340431 -4.2241554 -4.2203755 -4.2203355 -4.2225819 -4.226584 -4.2311139 -4.2394648 -4.2507806 -4.262898 -4.2755222 -4.2852859 -4.2872887 -4.2740908 -4.2447805]]...]
INFO - root - 2017-12-05 15:01:20.669945: step 20610, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 83h:45m:14s remains)
INFO - root - 2017-12-05 15:01:30.027865: step 20620, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 83h:15m:45s remains)
INFO - root - 2017-12-05 15:01:39.385986: step 20630, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 82h:15m:20s remains)
INFO - root - 2017-12-05 15:01:48.871029: step 20640, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.964 sec/batch; 83h:30m:22s remains)
INFO - root - 2017-12-05 15:01:58.379496: step 20650, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 79h:27m:20s remains)
INFO - root - 2017-12-05 15:02:07.593202: step 20660, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 79h:02m:59s remains)
INFO - root - 2017-12-05 15:02:17.154466: step 20670, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 82h:07m:20s remains)
INFO - root - 2017-12-05 15:02:26.612011: step 20680, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 85h:04m:24s remains)
INFO - root - 2017-12-05 15:02:36.068832: step 20690, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 86h:35m:56s remains)
INFO - root - 2017-12-05 15:02:45.554178: step 20700, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.016 sec/batch; 87h:59m:57s remains)
2017-12-05 15:02:46.346912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2610226 -4.239222 -4.2292094 -4.23531 -4.2595048 -4.293294 -4.3204665 -4.3341508 -4.3366466 -4.331481 -4.3230963 -4.3217263 -4.3301244 -4.3403563 -4.3405981][-4.2814322 -4.2553415 -4.2376204 -4.235847 -4.2526245 -4.2822833 -4.3096671 -4.3247623 -4.3290277 -4.3274097 -4.3246837 -4.3241668 -4.3305659 -4.3418303 -4.345346][-4.286097 -4.2457895 -4.2118835 -4.196939 -4.2059965 -4.2353859 -4.2672524 -4.2873378 -4.2943974 -4.2995653 -4.3076997 -4.3139067 -4.3208265 -4.3328815 -4.3388252][-4.2571216 -4.1939692 -4.1362596 -4.1016817 -4.1040053 -4.1412964 -4.1879787 -4.2205682 -4.2355042 -4.249424 -4.2686315 -4.2847013 -4.2958288 -4.3094878 -4.3180189][-4.2060118 -4.1158319 -4.028038 -3.9652357 -3.9556835 -4.0037704 -4.072443 -4.1266246 -4.1565514 -4.1837072 -4.2182074 -4.2494516 -4.2702661 -4.2874217 -4.2973628][-4.1634903 -4.0493817 -3.9316759 -3.8395462 -3.8122666 -3.861582 -3.9484398 -4.022542 -4.0679669 -4.1130748 -4.1652217 -4.2132521 -4.2475715 -4.2701225 -4.2828493][-4.1498642 -4.0288386 -3.8966165 -3.7860994 -3.7365599 -3.7720726 -3.8594487 -3.9445877 -4.0012093 -4.0621872 -4.1308379 -4.1900096 -4.2354259 -4.2643604 -4.2811232][-4.176228 -4.0726671 -3.9544959 -3.8496008 -3.7867537 -3.7896986 -3.8488059 -3.9264734 -3.9839849 -4.0515356 -4.1259041 -4.1900806 -4.2409024 -4.2743268 -4.29398][-4.2334628 -4.1591363 -4.067636 -3.9807372 -3.9187205 -3.8957047 -3.9245443 -3.9822779 -4.0318184 -4.0909796 -4.1581097 -4.2183194 -4.2669024 -4.2977839 -4.3136315][-4.2944121 -4.242877 -4.1731915 -4.104723 -4.0546603 -4.0269146 -4.0377316 -4.0754185 -4.1112576 -4.1562948 -4.2076826 -4.2563715 -4.2955132 -4.320447 -4.330965][-4.3311548 -4.2976408 -4.2448769 -4.1911054 -4.1553092 -4.1376624 -4.1442947 -4.1649709 -4.1886988 -4.2192345 -4.2512703 -4.2831912 -4.3085108 -4.324573 -4.32743][-4.3375716 -4.3176804 -4.2798405 -4.2386475 -4.2148166 -4.2086606 -4.2173891 -4.2289891 -4.2407918 -4.2581291 -4.2753839 -4.2907891 -4.3016472 -4.307682 -4.3056407][-4.3275957 -4.3163981 -4.293952 -4.2679276 -4.2562089 -4.2584662 -4.2683153 -4.2735734 -4.2757158 -4.2822013 -4.2899804 -4.2947259 -4.2952352 -4.2925587 -4.2839828][-4.3067021 -4.3004341 -4.2929387 -4.2841043 -4.2840614 -4.2921286 -4.3027382 -4.3060288 -4.3038888 -4.3043108 -4.3055038 -4.3035326 -4.2972417 -4.2859406 -4.2677865][-4.2821479 -4.2766628 -4.28127 -4.2879086 -4.2974129 -4.30982 -4.3224607 -4.327549 -4.3229942 -4.3191004 -4.3168044 -4.3126917 -4.3012061 -4.2812605 -4.2533545]]...]
INFO - root - 2017-12-05 15:02:55.785731: step 20710, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 77h:58m:03s remains)
INFO - root - 2017-12-05 15:03:05.097290: step 20720, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 84h:22m:25s remains)
INFO - root - 2017-12-05 15:03:14.352939: step 20730, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 86h:27m:51s remains)
INFO - root - 2017-12-05 15:03:23.564985: step 20740, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 80h:55m:46s remains)
INFO - root - 2017-12-05 15:03:33.189022: step 20750, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 85h:07m:52s remains)
INFO - root - 2017-12-05 15:03:42.340326: step 20760, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 82h:06m:38s remains)
INFO - root - 2017-12-05 15:03:51.614827: step 20770, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 74h:27m:48s remains)
INFO - root - 2017-12-05 15:04:01.002631: step 20780, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 80h:19m:31s remains)
INFO - root - 2017-12-05 15:04:10.523089: step 20790, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 82h:34m:46s remains)
INFO - root - 2017-12-05 15:04:19.995280: step 20800, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 83h:44m:16s remains)
2017-12-05 15:04:20.727866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2905169 -4.2753549 -4.2694106 -4.2607617 -4.2580705 -4.2590461 -4.2559466 -4.2544608 -4.2656054 -4.2862792 -4.2992597 -4.290514 -4.2659369 -4.2310882 -4.20191][-4.28526 -4.2707043 -4.2614746 -4.2527332 -4.2540317 -4.2613249 -4.2638764 -4.2643661 -4.2767339 -4.2995477 -4.3106394 -4.2957573 -4.2639651 -4.2233491 -4.192471][-4.2679462 -4.2520342 -4.2438941 -4.2414412 -4.2465482 -4.253778 -4.2530785 -4.2478728 -4.25852 -4.2847209 -4.3018165 -4.2900076 -4.2557273 -4.2149706 -4.1865172][-4.2436118 -4.2256021 -4.2184186 -4.2225947 -4.2282524 -4.2294331 -4.222362 -4.2126141 -4.2208843 -4.252264 -4.2783809 -4.2722139 -4.2418318 -4.2043109 -4.1769753][-4.2149811 -4.1983213 -4.1950688 -4.2012143 -4.2006884 -4.18925 -4.1711659 -4.156867 -4.1647515 -4.2034864 -4.239944 -4.2473469 -4.2292843 -4.199451 -4.1759467][-4.1965752 -4.178937 -4.175396 -4.1807542 -4.1741986 -4.1495214 -4.1144991 -4.087996 -4.0954866 -4.1442833 -4.1931939 -4.2182407 -4.218853 -4.2024403 -4.1891127][-4.1802878 -4.161 -4.1552305 -4.1605597 -4.1550241 -4.1134119 -4.0504441 -4.0024281 -4.0109406 -4.0733862 -4.1378474 -4.1840396 -4.2084165 -4.2134085 -4.2160978][-4.1467237 -4.1332946 -4.1358576 -4.1478324 -4.1411681 -4.0825648 -3.9883471 -3.9150157 -3.9249706 -4.0015759 -4.0818219 -4.1451311 -4.1877747 -4.2128615 -4.231739][-4.0930281 -4.0950236 -4.1146379 -4.1379852 -4.134778 -4.07261 -3.9700859 -3.8947458 -3.9058371 -3.9785459 -4.0621529 -4.1341977 -4.1847987 -4.2199578 -4.2490025][-4.0442977 -4.0655947 -4.102632 -4.1354446 -4.1387005 -4.0860639 -4.000484 -3.9431632 -3.9576073 -4.0141716 -4.085124 -4.150269 -4.1944771 -4.2302227 -4.266314][-4.0455408 -4.0751047 -4.1187558 -4.1546068 -4.1607757 -4.1210113 -4.0550632 -4.0150595 -4.030683 -4.0707235 -4.1203928 -4.1659641 -4.1975956 -4.230988 -4.272594][-4.0873108 -4.1198459 -4.16198 -4.1947837 -4.2033238 -4.1738567 -4.1257377 -4.1023297 -4.116231 -4.1371474 -4.1618271 -4.1847758 -4.2033281 -4.231338 -4.2726603][-4.1455207 -4.1755962 -4.2087955 -4.2334995 -4.2391877 -4.2159944 -4.1813054 -4.1671052 -4.1724162 -4.1753049 -4.181356 -4.1878719 -4.1997766 -4.2258754 -4.2641792][-4.20491 -4.2276545 -4.2448087 -4.2560759 -4.2550654 -4.2341027 -4.210124 -4.1973939 -4.1872458 -4.1760297 -4.1723094 -4.1723161 -4.1841979 -4.2119989 -4.2476144][-4.2443843 -4.2603951 -4.2673035 -4.2650933 -4.2551527 -4.2311711 -4.2122016 -4.200356 -4.1816916 -4.1639166 -4.158226 -4.1594934 -4.1748514 -4.2028012 -4.2361908]]...]
INFO - root - 2017-12-05 15:04:30.120284: step 20810, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 83h:08m:36s remains)
INFO - root - 2017-12-05 15:04:39.531781: step 20820, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 73h:58m:32s remains)
INFO - root - 2017-12-05 15:04:48.909169: step 20830, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 81h:22m:38s remains)
INFO - root - 2017-12-05 15:04:58.243269: step 20840, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 83h:09m:09s remains)
INFO - root - 2017-12-05 15:05:07.612685: step 20850, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.924 sec/batch; 80h:01m:46s remains)
INFO - root - 2017-12-05 15:05:16.980124: step 20860, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 80h:35m:59s remains)
INFO - root - 2017-12-05 15:05:26.456693: step 20870, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 79h:26m:24s remains)
INFO - root - 2017-12-05 15:05:35.824310: step 20880, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 79h:18m:56s remains)
INFO - root - 2017-12-05 15:05:45.163894: step 20890, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.909 sec/batch; 78h:42m:11s remains)
INFO - root - 2017-12-05 15:05:54.525613: step 20900, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 79h:51m:01s remains)
2017-12-05 15:05:55.266935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1663356 -4.2244864 -4.2686305 -4.2915087 -4.3012881 -4.2981353 -4.2884526 -4.2731586 -4.2636008 -4.2562757 -4.251894 -4.2514172 -4.255806 -4.2602654 -4.2600579][-4.0997548 -4.1808457 -4.2439628 -4.2826715 -4.3036485 -4.3088422 -4.3041787 -4.2952547 -4.2910438 -4.2860975 -4.280407 -4.2737608 -4.2689452 -4.266129 -4.2636676][-4.0743761 -4.1646771 -4.2328758 -4.2744102 -4.2950349 -4.2973528 -4.2918119 -4.2880797 -4.2937727 -4.2989874 -4.2992988 -4.2917762 -4.2791338 -4.2690077 -4.2615461][-4.12128 -4.196085 -4.2492733 -4.277267 -4.284039 -4.2740588 -4.2623644 -4.2601109 -4.2731814 -4.2887678 -4.296721 -4.2929969 -4.2781372 -4.2618809 -4.2500658][-4.2001514 -4.250072 -4.2772489 -4.2823339 -4.2668548 -4.2396684 -4.2185903 -4.2190404 -4.2394714 -4.2656155 -4.2833257 -4.2871137 -4.2740541 -4.2544169 -4.2400126][-4.2605715 -4.2867866 -4.2881069 -4.2675433 -4.2285533 -4.1837749 -4.1541605 -4.1597762 -4.1917925 -4.2304397 -4.2589664 -4.2730732 -4.2660217 -4.2499051 -4.2366328][-4.2913923 -4.2944312 -4.2694292 -4.2196121 -4.1538625 -4.0910563 -4.054491 -4.0670834 -4.1181364 -4.1789351 -4.2265339 -4.2539077 -4.2568831 -4.2513008 -4.243495][-4.3024087 -4.2855673 -4.2365193 -4.1570477 -4.0644016 -3.9802198 -3.9283934 -3.9392409 -4.0117168 -4.1008496 -4.1727676 -4.2220607 -4.2452226 -4.2565513 -4.2582703][-4.3126516 -4.2850823 -4.2228737 -4.1273632 -4.016161 -3.9080119 -3.82929 -3.8240798 -3.9097228 -4.0217161 -4.1141381 -4.1828003 -4.2262907 -4.2538862 -4.2650228][-4.3294048 -4.3057075 -4.2525234 -4.1690946 -4.0653834 -3.9550345 -3.8611045 -3.8260722 -3.8902791 -3.9987152 -4.0970931 -4.17434 -4.2274408 -4.2602525 -4.2729383][-4.3441715 -4.3321061 -4.2997527 -4.2455015 -4.1723404 -4.088819 -4.0118427 -3.9636769 -3.9869096 -4.06248 -4.1454244 -4.2138157 -4.2606158 -4.2865186 -4.2924476][-4.3506551 -4.3476415 -4.3334279 -4.306674 -4.2655735 -4.2128582 -4.1596622 -4.1165543 -4.1163926 -4.162394 -4.2240777 -4.2741919 -4.3062329 -4.3195734 -4.317205][-4.3469286 -4.3477859 -4.3451724 -4.3388577 -4.3232808 -4.2939544 -4.2573857 -4.2250853 -4.2203612 -4.2487731 -4.2911587 -4.3237123 -4.3407512 -4.3431077 -4.3355618][-4.3407955 -4.3411889 -4.3426523 -4.3461251 -4.3451324 -4.331892 -4.3107162 -4.2902341 -4.2868891 -4.3051639 -4.3316779 -4.34913 -4.3546715 -4.3508835 -4.3415847][-4.3366132 -4.3358741 -4.33743 -4.3425 -4.3461823 -4.3431249 -4.3343372 -4.3246374 -4.3233495 -4.3346119 -4.3487053 -4.3562708 -4.3557787 -4.3494368 -4.3406272]]...]
INFO - root - 2017-12-05 15:06:04.696507: step 20910, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.940 sec/batch; 81h:22m:21s remains)
INFO - root - 2017-12-05 15:06:13.990922: step 20920, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 79h:21m:45s remains)
INFO - root - 2017-12-05 15:06:23.363761: step 20930, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 84h:39m:12s remains)
INFO - root - 2017-12-05 15:06:32.858985: step 20940, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 80h:54m:18s remains)
INFO - root - 2017-12-05 15:06:42.253673: step 20950, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 84h:16m:22s remains)
INFO - root - 2017-12-05 15:06:51.330943: step 20960, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 82h:42m:00s remains)
INFO - root - 2017-12-05 15:07:00.762368: step 20970, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.977 sec/batch; 84h:31m:40s remains)
INFO - root - 2017-12-05 15:07:10.124179: step 20980, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 85h:29m:06s remains)
INFO - root - 2017-12-05 15:07:19.746557: step 20990, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 86h:03m:24s remains)
INFO - root - 2017-12-05 15:07:29.320734: step 21000, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 78h:32m:28s remains)
2017-12-05 15:07:30.124534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2672329 -4.2637649 -4.265924 -4.2710896 -4.2847857 -4.2916107 -4.2925825 -4.2929459 -4.2911954 -4.285079 -4.2799215 -4.280931 -4.2858429 -4.29597 -4.3056092][-4.2252374 -4.2229605 -4.22838 -4.2345524 -4.2518497 -4.2632289 -4.2644196 -4.2622027 -4.2566223 -4.2453074 -4.2363234 -4.2334161 -4.2398262 -4.2576637 -4.2738366][-4.1835942 -4.1846271 -4.1925073 -4.1973619 -4.2139564 -4.2290668 -4.2332821 -4.2319412 -4.2303076 -4.2223387 -4.2095 -4.19877 -4.2032962 -4.2253661 -4.2450376][-4.159133 -4.1635695 -4.1762671 -4.1762381 -4.1858425 -4.1959968 -4.1942091 -4.1911764 -4.1962771 -4.1969705 -4.1879625 -4.1762652 -4.1805539 -4.202436 -4.2224865][-4.1472859 -4.1559515 -4.1719971 -4.1700959 -4.1699719 -4.1673417 -4.1515851 -4.1438565 -4.1556973 -4.1674633 -4.169909 -4.1681118 -4.1767406 -4.1935287 -4.2111621][-4.1297393 -4.1441813 -4.163507 -4.1612949 -4.1499543 -4.1240168 -4.0856729 -4.0719132 -4.095252 -4.130918 -4.1539416 -4.1682611 -4.1836972 -4.1941142 -4.2058773][-4.0992646 -4.1177306 -4.1416349 -4.1417475 -4.1140924 -4.0523357 -3.9806364 -3.9583123 -4.0027738 -4.0697637 -4.1167579 -4.1516256 -4.1783047 -4.1919875 -4.2045474][-4.0762539 -4.0969896 -4.1241922 -4.1190734 -4.0673752 -3.9650147 -3.8533888 -3.8219209 -3.8896492 -3.9813316 -4.0460057 -4.1058159 -4.1530862 -4.1792908 -4.2006173][-4.0578084 -4.084002 -4.1139641 -4.108017 -4.0548425 -3.9539108 -3.8413038 -3.8062205 -3.86583 -3.9437757 -4.0006418 -4.0651407 -4.1191311 -4.156765 -4.1909108][-4.0487309 -4.0805244 -4.1093512 -4.1047654 -4.0710092 -4.0135441 -3.9442842 -3.9210968 -3.9523978 -3.9840422 -4.0037985 -4.045917 -4.0911665 -4.1339412 -4.178442][-4.05634 -4.0827775 -4.1059856 -4.1102948 -4.10215 -4.0809627 -4.0497632 -4.0387883 -4.0507331 -4.0483975 -4.0358891 -4.0511274 -4.0835371 -4.1240368 -4.172493][-4.0893726 -4.1087766 -4.12719 -4.1367049 -4.1408186 -4.1381221 -4.1284018 -4.1257305 -4.1290317 -4.1121325 -4.0831442 -4.0821075 -4.1043639 -4.1400156 -4.1862006][-4.1611552 -4.1725545 -4.1839857 -4.1913986 -4.1978464 -4.2002487 -4.1956434 -4.1954775 -4.1954861 -4.1770563 -4.1479487 -4.140759 -4.1574793 -4.188014 -4.2276978][-4.2423258 -4.2460961 -4.2525482 -4.258513 -4.2643323 -4.2655706 -4.2634196 -4.2641511 -4.2618551 -4.2449369 -4.2201648 -4.2127595 -4.2272754 -4.2512803 -4.2801762][-4.2979097 -4.2945127 -4.2975874 -4.3045049 -4.3095183 -4.309422 -4.3084679 -4.3097939 -4.3053651 -4.29284 -4.2771811 -4.2719421 -4.2812009 -4.2954135 -4.31502]]...]
INFO - root - 2017-12-05 15:07:39.439593: step 21010, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 79h:07m:16s remains)
INFO - root - 2017-12-05 15:07:48.767026: step 21020, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 84h:55m:02s remains)
INFO - root - 2017-12-05 15:07:58.197618: step 21030, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 81h:40m:54s remains)
INFO - root - 2017-12-05 15:08:07.645899: step 21040, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 84h:25m:48s remains)
INFO - root - 2017-12-05 15:08:16.905201: step 21050, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.995 sec/batch; 86h:04m:30s remains)
INFO - root - 2017-12-05 15:08:26.302897: step 21060, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 83h:56m:20s remains)
INFO - root - 2017-12-05 15:08:35.604188: step 21070, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 81h:32m:24s remains)
INFO - root - 2017-12-05 15:08:45.101593: step 21080, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 85h:20m:14s remains)
INFO - root - 2017-12-05 15:08:54.502665: step 21090, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.970 sec/batch; 83h:51m:53s remains)
INFO - root - 2017-12-05 15:09:03.904346: step 21100, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 81h:29m:47s remains)
2017-12-05 15:09:04.673542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3023992 -4.3052506 -4.3104234 -4.3124423 -4.3100991 -4.3018923 -4.28785 -4.271019 -4.2569451 -4.2504478 -4.2521105 -4.2549939 -4.2539396 -4.2523203 -4.2559118][-4.2677436 -4.2729859 -4.283433 -4.2889805 -4.2853708 -4.2727308 -4.2533541 -4.232789 -4.2151041 -4.206881 -4.2105093 -4.2128267 -4.2063441 -4.1998973 -4.20238][-4.2287436 -4.2366772 -4.2506523 -4.2570767 -4.2493315 -4.2330709 -4.2137966 -4.1962509 -4.1796303 -4.1748028 -4.1846628 -4.1884074 -4.1752772 -4.1609082 -4.1573434][-4.1895704 -4.2027373 -4.2203412 -4.2267842 -4.2168565 -4.19842 -4.18151 -4.1681271 -4.1533823 -4.1530113 -4.170084 -4.1740708 -4.1533713 -4.131464 -4.1225805][-4.1549783 -4.1733823 -4.196012 -4.203495 -4.1933389 -4.1744 -4.1581192 -4.145659 -4.1315632 -4.1393385 -4.1654496 -4.1698723 -4.1408095 -4.1117444 -4.1003251][-4.1226678 -4.1463494 -4.1749096 -4.1850138 -4.1724405 -4.147922 -4.1247845 -4.1024861 -4.0851974 -4.1094737 -4.150352 -4.1585455 -4.1278496 -4.0981059 -4.0873508][-4.0966854 -4.1253166 -4.1568451 -4.1629114 -4.1361265 -4.0941148 -4.0495195 -4.0003133 -3.9751542 -4.0253377 -4.0894279 -4.109015 -4.0855355 -4.0629053 -4.0615554][-4.0837536 -4.115325 -4.1401019 -4.131618 -4.0839567 -4.0149918 -3.9377437 -3.8518662 -3.8171263 -3.8987851 -3.9860089 -4.0177422 -4.0031252 -3.9921837 -4.0115786][-4.0966606 -4.1247654 -4.1402988 -4.1173058 -4.0568914 -3.9698212 -3.8691759 -3.7593317 -3.7214918 -3.825696 -3.9215961 -3.9546905 -3.9454837 -3.9472113 -3.9817107][-4.1365695 -4.1563163 -4.1646957 -4.1400008 -4.0871291 -4.0099864 -3.9212441 -3.829808 -3.8059785 -3.8943872 -3.9654529 -3.9856067 -3.9758441 -3.9842358 -4.0192842][-4.17637 -4.1906161 -4.1993594 -4.1860008 -4.1534691 -4.1024265 -4.0410118 -3.9793749 -3.9691498 -4.0259609 -4.0646386 -4.0722656 -4.0615692 -4.0670471 -4.088758][-4.2025785 -4.2150736 -4.2268744 -4.2266269 -4.2119012 -4.1826591 -4.1427565 -4.1020441 -4.0970545 -4.1267519 -4.1424279 -4.142796 -4.1315479 -4.1329427 -4.1428528][-4.2140388 -4.223722 -4.2343187 -4.2396088 -4.2362347 -4.22102 -4.1948256 -4.1704807 -4.1704116 -4.1861138 -4.1922426 -4.1883554 -4.1770563 -4.1735888 -4.1753407][-4.2080331 -4.214365 -4.2226706 -4.2296882 -4.2316184 -4.2255373 -4.2116618 -4.2001395 -4.2028222 -4.2124181 -4.2156191 -4.2086153 -4.1963887 -4.1889243 -4.1844807][-4.208004 -4.2127085 -4.2197208 -4.2266889 -4.231051 -4.2301431 -4.2244005 -4.2199636 -4.2228794 -4.2281165 -4.2283411 -4.2196722 -4.2082314 -4.1997542 -4.193759]]...]
INFO - root - 2017-12-05 15:09:14.274291: step 21110, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.936 sec/batch; 80h:55m:12s remains)
INFO - root - 2017-12-05 15:09:23.519512: step 21120, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 81h:33m:21s remains)
INFO - root - 2017-12-05 15:09:32.912507: step 21130, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 86h:18m:31s remains)
INFO - root - 2017-12-05 15:09:42.240512: step 21140, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 77h:03m:33s remains)
INFO - root - 2017-12-05 15:09:51.485684: step 21150, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 85h:10m:18s remains)
INFO - root - 2017-12-05 15:10:00.873813: step 21160, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 86h:02m:11s remains)
INFO - root - 2017-12-05 15:10:10.293261: step 21170, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 86h:17m:29s remains)
INFO - root - 2017-12-05 15:10:19.628505: step 21180, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 85h:14m:53s remains)
INFO - root - 2017-12-05 15:10:28.684578: step 21190, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 81h:35m:56s remains)
INFO - root - 2017-12-05 15:10:38.177870: step 21200, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 82h:16m:18s remains)
2017-12-05 15:10:38.957045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1525121 -4.1268811 -4.1120543 -4.1127033 -4.1383634 -4.1639724 -4.1733909 -4.1773806 -4.1779008 -4.181282 -4.1988683 -4.2253847 -4.2423 -4.2418256 -4.2356415][-4.1358728 -4.1216183 -4.1188641 -4.1257071 -4.1545935 -4.18194 -4.1918945 -4.1966233 -4.1949425 -4.19584 -4.2130489 -4.2361836 -4.2501559 -4.2466135 -4.2354488][-4.1315961 -4.1372738 -4.1473985 -4.1551843 -4.1748896 -4.1881142 -4.18527 -4.1810331 -4.1800117 -4.1870108 -4.2066746 -4.2257295 -4.2343421 -4.2301927 -4.2175031][-4.1434608 -4.1727991 -4.1929259 -4.1946774 -4.1935911 -4.179409 -4.1476793 -4.1241436 -4.1246958 -4.151825 -4.1856203 -4.2076974 -4.2075009 -4.1967034 -4.1767297][-4.1748934 -4.2155356 -4.2345338 -4.2228193 -4.1916642 -4.1409106 -4.0736241 -4.0344019 -4.0458436 -4.1059175 -4.1663685 -4.2002039 -4.1959004 -4.1801047 -4.1480947][-4.2276864 -4.262146 -4.2684436 -4.2360005 -4.1668096 -4.0715008 -3.9640481 -3.9163547 -3.9533861 -4.0576029 -4.15458 -4.2032571 -4.1963921 -4.1736889 -4.1279559][-4.2756572 -4.3003287 -4.2965541 -4.2466168 -4.1385489 -3.9955132 -3.8487847 -3.7971091 -3.8637993 -4.0054789 -4.1333928 -4.1978478 -4.1895003 -4.1586518 -4.1015973][-4.305057 -4.3227315 -4.3162603 -4.2637753 -4.1434 -3.9876225 -3.8374848 -3.792361 -3.8665218 -4.0034046 -4.1279612 -4.1966524 -4.1898851 -4.1498318 -4.08856][-4.3160753 -4.3304071 -4.3261309 -4.2870092 -4.1897626 -4.0679169 -3.9645019 -3.9433808 -4.00011 -4.0886288 -4.1699648 -4.2203574 -4.2125988 -4.1667657 -4.1061435][-4.3125639 -4.3244624 -4.3220196 -4.2947149 -4.2258773 -4.1415648 -4.085392 -4.0909977 -4.1285181 -4.1673203 -4.2039866 -4.2316141 -4.2199659 -4.1736255 -4.1143742][-4.2996163 -4.3079782 -4.3043418 -4.2791657 -4.2247853 -4.1648626 -4.13716 -4.1620035 -4.1939049 -4.2089658 -4.2191916 -4.2293768 -4.2129469 -4.1660419 -4.10415][-4.2940683 -4.2999616 -4.2897305 -4.2612648 -4.2147579 -4.1717286 -4.1611857 -4.1952696 -4.2244797 -4.2305269 -4.2296619 -4.2284288 -4.206409 -4.1587844 -4.0963044][-4.3043847 -4.3071876 -4.2897835 -4.2596745 -4.2229009 -4.1953859 -4.1965146 -4.2311807 -4.2555876 -4.2531567 -4.244153 -4.2303891 -4.2023826 -4.1601639 -4.107254][-4.3120213 -4.3098845 -4.2872233 -4.2613535 -4.2380919 -4.2262955 -4.2370729 -4.2698784 -4.2883735 -4.2823777 -4.2713103 -4.2501678 -4.2183909 -4.17588 -4.1301575][-4.3016162 -4.295691 -4.2724714 -4.2530155 -4.2434049 -4.2458472 -4.2639675 -4.2942371 -4.3106155 -4.307076 -4.2991395 -4.2744775 -4.235003 -4.1818166 -4.1314111]]...]
INFO - root - 2017-12-05 15:10:48.338225: step 21210, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.955 sec/batch; 82h:35m:12s remains)
INFO - root - 2017-12-05 15:10:57.976290: step 21220, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.951 sec/batch; 82h:15m:25s remains)
INFO - root - 2017-12-05 15:11:07.341365: step 21230, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 81h:37m:30s remains)
INFO - root - 2017-12-05 15:11:16.706536: step 21240, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 76h:25m:23s remains)
INFO - root - 2017-12-05 15:11:26.231409: step 21250, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 82h:51m:07s remains)
INFO - root - 2017-12-05 15:11:35.636418: step 21260, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 76h:02m:47s remains)
INFO - root - 2017-12-05 15:11:44.972373: step 21270, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 83h:37m:07s remains)
INFO - root - 2017-12-05 15:11:54.254014: step 21280, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 84h:02m:26s remains)
INFO - root - 2017-12-05 15:12:03.681431: step 21290, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 85h:44m:20s remains)
INFO - root - 2017-12-05 15:12:13.271957: step 21300, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 80h:59m:35s remains)
2017-12-05 15:12:14.002662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3316069 -4.323092 -4.3188534 -4.3156009 -4.3117309 -4.31192 -4.3204603 -4.3263831 -4.3283148 -4.33145 -4.3364816 -4.3411098 -4.3453035 -4.3499279 -4.35447][-4.30317 -4.2903166 -4.2848907 -4.2763715 -4.26298 -4.2570539 -4.2695203 -4.2791724 -4.2796121 -4.2829261 -4.2918587 -4.3016686 -4.3097515 -4.318625 -4.3303423][-4.2737794 -4.2585626 -4.2539496 -4.2410879 -4.2165117 -4.2016225 -4.2164774 -4.2291641 -4.2267866 -4.2302918 -4.2440591 -4.2578053 -4.2669687 -4.2766786 -4.2927852][-4.24981 -4.2300806 -4.2229772 -4.2053 -4.1690292 -4.147059 -4.165966 -4.1801815 -4.17649 -4.1819296 -4.202806 -4.2203856 -4.2283983 -4.2374692 -4.2570553][-4.2421279 -4.2146854 -4.1978292 -4.1696396 -4.1182861 -4.0862269 -4.109973 -4.13034 -4.1287723 -4.1368003 -4.1643319 -4.1856675 -4.1947279 -4.2050142 -4.2294345][-4.2429118 -4.2104807 -4.1821218 -4.1427293 -4.0751257 -4.0265694 -4.0443678 -4.0682912 -4.0745435 -4.0934443 -4.1306534 -4.158958 -4.1734762 -4.1885209 -4.2190309][-4.2474403 -4.2146544 -4.1777425 -4.1276417 -4.0429559 -3.9685431 -3.9620929 -3.9855084 -4.0087824 -4.0508456 -4.1057663 -4.1460485 -4.1664314 -4.1825938 -4.2144804][-4.2561183 -4.2291746 -4.1930051 -4.1404467 -4.0513482 -3.9590886 -3.916414 -3.9278026 -3.9646749 -4.0259161 -4.1001339 -4.15418 -4.178462 -4.1920085 -4.2166433][-4.2558737 -4.2358766 -4.2110863 -4.1684518 -4.0989895 -4.0286331 -3.9747529 -3.9543376 -3.9746456 -4.0322547 -4.1128798 -4.1695075 -4.1932888 -4.2062931 -4.2264614][-4.2345004 -4.2186017 -4.2052531 -4.1756482 -4.1299953 -4.0919313 -4.0572405 -4.0254316 -4.023612 -4.0652971 -4.1361837 -4.1839466 -4.2024703 -4.2124534 -4.2328892][-4.2027292 -4.1844263 -4.1800251 -4.16615 -4.1420064 -4.1298165 -4.1156678 -4.0886869 -4.0787454 -4.1110845 -4.170094 -4.212132 -4.2281933 -4.2329879 -4.2490897][-4.1798358 -4.1588383 -4.1594696 -4.1581903 -4.146801 -4.1510444 -4.1530404 -4.1363134 -4.1248074 -4.1496515 -4.19881 -4.2349262 -4.2474046 -4.2492175 -4.2642775][-4.1825643 -4.16656 -4.1716518 -4.1793957 -4.1767488 -4.1888437 -4.1991186 -4.1888471 -4.1803517 -4.2005825 -4.2399082 -4.2651324 -4.2708321 -4.2711415 -4.2852936][-4.2130208 -4.2071137 -4.21695 -4.2268362 -4.2257075 -4.2376232 -4.2505622 -4.245275 -4.2392259 -4.2540188 -4.2850952 -4.3022661 -4.3027687 -4.3023696 -4.3141766][-4.2620745 -4.2649088 -4.2767296 -4.2837586 -4.2796636 -4.286375 -4.2956052 -4.2886248 -4.28102 -4.28895 -4.3110156 -4.3218384 -4.3232379 -4.3281937 -4.3398228]]...]
INFO - root - 2017-12-05 15:12:23.250671: step 21310, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 80h:36m:12s remains)
INFO - root - 2017-12-05 15:12:32.484570: step 21320, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.898 sec/batch; 77h:39m:30s remains)
INFO - root - 2017-12-05 15:12:41.557711: step 21330, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.951 sec/batch; 82h:09m:34s remains)
INFO - root - 2017-12-05 15:12:50.868948: step 21340, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 85h:20m:44s remains)
INFO - root - 2017-12-05 15:13:00.365016: step 21350, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 84h:37m:52s remains)
INFO - root - 2017-12-05 15:13:09.440342: step 21360, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.984 sec/batch; 85h:02m:14s remains)
INFO - root - 2017-12-05 15:13:18.811070: step 21370, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.002 sec/batch; 86h:33m:38s remains)
INFO - root - 2017-12-05 15:13:28.366061: step 21380, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 84h:26m:41s remains)
INFO - root - 2017-12-05 15:13:37.670421: step 21390, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 84h:06m:17s remains)
INFO - root - 2017-12-05 15:13:47.186172: step 21400, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 86h:16m:50s remains)
2017-12-05 15:13:47.972655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2534237 -4.2415166 -4.2442417 -4.2504086 -4.2615051 -4.2700448 -4.2745376 -4.278214 -4.2867546 -4.2877512 -4.2702889 -4.2348838 -4.198297 -4.1853819 -4.2032504][-4.2358575 -4.2237372 -4.229104 -4.2382293 -4.2512231 -4.2588882 -4.2627845 -4.26462 -4.2720771 -4.27705 -4.2725892 -4.252913 -4.2240515 -4.2068949 -4.21701][-4.2058172 -4.18468 -4.1844087 -4.1952429 -4.2167 -4.2328949 -4.2433696 -4.2492495 -4.2561131 -4.2660732 -4.2776341 -4.2796159 -4.2635126 -4.2447963 -4.2429433][-4.18053 -4.1420164 -4.1221066 -4.1238708 -4.1481123 -4.1788521 -4.2067747 -4.2278337 -4.2430015 -4.2587047 -4.280097 -4.2999468 -4.3002944 -4.2831273 -4.2677541][-4.17539 -4.1262407 -4.0842233 -4.0613284 -4.0659885 -4.0933537 -4.1368251 -4.1831112 -4.2198806 -4.2470794 -4.2740135 -4.3030086 -4.3159256 -4.3041744 -4.2817225][-4.1878872 -4.1371531 -4.0889549 -4.0498562 -4.0232854 -4.0114136 -4.0358372 -4.0980153 -4.1684752 -4.221015 -4.2568169 -4.2897229 -4.3087115 -4.3024755 -4.2788382][-4.2206397 -4.174634 -4.1303 -4.0885673 -4.04171 -3.9865508 -3.9621747 -4.0087891 -4.1008081 -4.1780362 -4.2277083 -4.26731 -4.289258 -4.2886753 -4.2669716][-4.2544408 -4.2172618 -4.1793218 -4.1441483 -4.0992746 -4.0310221 -3.9776187 -3.9971313 -4.0761104 -4.1511688 -4.2059984 -4.2483749 -4.2716708 -4.2728744 -4.25551][-4.262732 -4.2377672 -4.208909 -4.1812696 -4.1530538 -4.1046734 -4.0564 -4.05553 -4.1048217 -4.1583629 -4.1992674 -4.2316818 -4.2505617 -4.2543187 -4.2442279][-4.2557659 -4.2367496 -4.2172837 -4.19797 -4.1882315 -4.1669574 -4.1379032 -4.1280112 -4.1467581 -4.1756988 -4.2011938 -4.2155752 -4.2233248 -4.2271848 -4.2273893][-4.2406993 -4.2257648 -4.2118645 -4.2011127 -4.2048807 -4.2023406 -4.1869617 -4.17584 -4.1784678 -4.1878529 -4.2001038 -4.2067719 -4.2101431 -4.2110829 -4.2110052][-4.24021 -4.2250819 -4.210577 -4.200563 -4.2061749 -4.2114239 -4.2052546 -4.1970091 -4.1916022 -4.1896858 -4.192265 -4.1982985 -4.2097135 -4.2141104 -4.2099319][-4.2593374 -4.247467 -4.2303662 -4.2161589 -4.2171612 -4.2268353 -4.2267456 -4.218545 -4.2070994 -4.1943364 -4.1867204 -4.1927452 -4.2123866 -4.2264338 -4.2265277][-4.2820044 -4.2757993 -4.2634926 -4.2503123 -4.2469788 -4.2541761 -4.2569656 -4.2480426 -4.2298489 -4.2070203 -4.1865177 -4.1847911 -4.204721 -4.2243781 -4.23049][-4.2917962 -4.2899747 -4.2829108 -4.2746029 -4.2722926 -4.2781558 -4.2815332 -4.27269 -4.2523508 -4.2243481 -4.1987681 -4.1845164 -4.1890464 -4.1990104 -4.2066278]]...]
INFO - root - 2017-12-05 15:13:57.393092: step 21410, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 84h:18m:52s remains)
INFO - root - 2017-12-05 15:14:06.890154: step 21420, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.009 sec/batch; 87h:10m:17s remains)
INFO - root - 2017-12-05 15:14:16.223037: step 21430, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 79h:49m:53s remains)
INFO - root - 2017-12-05 15:14:25.479348: step 21440, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 82h:22m:07s remains)
INFO - root - 2017-12-05 15:14:35.034438: step 21450, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 85h:05m:37s remains)
INFO - root - 2017-12-05 15:14:44.457848: step 21460, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.010 sec/batch; 87h:16m:54s remains)
INFO - root - 2017-12-05 15:14:53.948741: step 21470, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.934 sec/batch; 80h:40m:37s remains)
INFO - root - 2017-12-05 15:15:03.449891: step 21480, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 83h:44m:38s remains)
INFO - root - 2017-12-05 15:15:12.822784: step 21490, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 78h:41m:07s remains)
INFO - root - 2017-12-05 15:15:22.276350: step 21500, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 80h:47m:04s remains)
2017-12-05 15:15:23.003801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1895208 -4.1919985 -4.1960764 -4.2000036 -4.206161 -4.2000742 -4.1945777 -4.1956525 -4.200458 -4.2096257 -4.2191648 -4.2301679 -4.2375264 -4.2372365 -4.2286716][-4.21005 -4.2151246 -4.2181406 -4.2250609 -4.2351727 -4.2311172 -4.2251267 -4.2239561 -4.2238317 -4.2271008 -4.2291722 -4.2321739 -4.2328 -4.2272005 -4.2189384][-4.2474546 -4.2548676 -4.2577434 -4.2667866 -4.279707 -4.2758536 -4.2652893 -4.2569194 -4.2497706 -4.2439785 -4.2359958 -4.2281823 -4.2222085 -4.2159257 -4.2135277][-4.279458 -4.2928247 -4.3012781 -4.3110266 -4.3170691 -4.3036494 -4.2831817 -4.2661142 -4.25371 -4.2435918 -4.2326393 -4.2218728 -4.2129045 -4.207305 -4.2102513][-4.2941146 -4.3079381 -4.3145013 -4.3180504 -4.3110075 -4.2862263 -4.2563996 -4.2336287 -4.2201138 -4.2114573 -4.2067146 -4.2031136 -4.1973572 -4.19238 -4.1985388][-4.2830515 -4.2869458 -4.2830825 -4.2769136 -4.2556276 -4.216516 -4.1761503 -4.1496797 -4.1422729 -4.1466885 -4.157618 -4.167819 -4.1712322 -4.170557 -4.1738796][-4.2464619 -4.2301083 -4.2092681 -4.1877995 -4.155293 -4.1058412 -4.0606632 -4.0380826 -4.0501971 -4.0797758 -4.1122923 -4.1397347 -4.1525278 -4.154881 -4.1537962][-4.1869116 -4.154604 -4.1211548 -4.0887332 -4.0466318 -3.9856739 -3.9366424 -3.9217908 -3.9593985 -4.019 -4.0732369 -4.115098 -4.1376691 -4.14338 -4.1413355][-4.14286 -4.1064415 -4.065764 -4.0161562 -3.9530087 -3.8763041 -3.8281953 -3.8363612 -3.9101846 -3.9957378 -4.0608683 -4.1064739 -4.1319318 -4.1394839 -4.1383147][-4.1429372 -4.10889 -4.0595007 -3.9871721 -3.9009116 -3.8244984 -3.802937 -3.8411121 -3.928607 -4.0128288 -4.0731931 -4.1150517 -4.1376014 -4.1431413 -4.1400909][-4.1541615 -4.1218839 -4.0686345 -3.9911468 -3.9114807 -3.8652174 -3.8719735 -3.916328 -3.9859762 -4.0491686 -4.0953088 -4.1266971 -4.1431742 -4.1462693 -4.1399856][-4.1697721 -4.1470327 -4.1046891 -4.0450554 -3.9904606 -3.9625649 -3.973614 -4.0083408 -4.0582209 -4.1004667 -4.1287575 -4.1454864 -4.1514273 -4.1508641 -4.1436939][-4.2004633 -4.1896529 -4.1588864 -4.1161 -4.0814252 -4.0625596 -4.0735354 -4.1031079 -4.1419849 -4.1664209 -4.1740127 -4.1725373 -4.16485 -4.1586609 -4.1508217][-4.2289577 -4.2265553 -4.2072 -4.1779337 -4.1547818 -4.1405392 -4.1495 -4.1744585 -4.202723 -4.2120881 -4.2040796 -4.191308 -4.1806316 -4.1725984 -4.16228][-4.2331681 -4.235569 -4.2243056 -4.2048774 -4.1905422 -4.1799121 -4.1873322 -4.2055936 -4.2230649 -4.2225938 -4.2071218 -4.1929212 -4.1860089 -4.1800985 -4.1680326]]...]
INFO - root - 2017-12-05 15:15:32.400416: step 21510, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 83h:31m:52s remains)
INFO - root - 2017-12-05 15:15:41.648485: step 21520, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 79h:50m:57s remains)
INFO - root - 2017-12-05 15:15:50.941868: step 21530, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 81h:47m:52s remains)
INFO - root - 2017-12-05 15:15:59.913403: step 21540, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 84h:00m:06s remains)
INFO - root - 2017-12-05 15:16:09.214456: step 21550, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 73h:14m:44s remains)
INFO - root - 2017-12-05 15:16:18.713630: step 21560, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 83h:28m:41s remains)
INFO - root - 2017-12-05 15:16:28.176024: step 21570, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 82h:27m:47s remains)
INFO - root - 2017-12-05 15:16:37.444311: step 21580, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.978 sec/batch; 84h:25m:53s remains)
INFO - root - 2017-12-05 15:16:46.821301: step 21590, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 82h:44m:44s remains)
INFO - root - 2017-12-05 15:16:56.199760: step 21600, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 78h:46m:41s remains)
2017-12-05 15:16:56.927395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.217124 -4.223546 -4.2294025 -4.2254963 -4.2152591 -4.2077966 -4.20799 -4.2074265 -4.2040958 -4.2049913 -4.2120547 -4.218616 -4.2295465 -4.24624 -4.2660885][-4.209538 -4.2138872 -4.2154584 -4.2068605 -4.1924305 -4.1833968 -4.1837525 -4.1853561 -4.1866355 -4.1947665 -4.2101383 -4.2197242 -4.2291365 -4.2434249 -4.2635059][-4.1635375 -4.1614504 -4.156642 -4.1424694 -4.1247931 -4.1172967 -4.1256371 -4.1374626 -4.1504016 -4.1704206 -4.1970148 -4.213048 -4.2237706 -4.2368431 -4.2577658][-4.1061339 -4.1007423 -4.09324 -4.0767822 -4.0574164 -4.0541282 -4.0739689 -4.0987535 -4.1236854 -4.1529961 -4.1857686 -4.207047 -4.2196779 -4.2328868 -4.2542467][-4.0495949 -4.0455389 -4.0407653 -4.0263357 -4.0076332 -4.0085673 -4.0377464 -4.0735903 -4.1091695 -4.1458197 -4.1815977 -4.2042732 -4.2188249 -4.2330289 -4.2543459][-4.0167584 -4.0146461 -4.0121222 -3.9969294 -3.9758224 -3.9756792 -4.0069928 -4.0463362 -4.0873728 -4.1316719 -4.1721673 -4.199101 -4.2180142 -4.2347183 -4.2560444][-4.0093083 -4.009696 -4.0072021 -3.9896793 -3.9630642 -3.9562976 -3.9818046 -4.0180268 -4.0603642 -4.1093464 -4.1550441 -4.1881351 -4.2131371 -4.2338948 -4.2570615][-4.0050063 -4.0052481 -4.0021477 -3.9821615 -3.9519253 -3.9402826 -3.9589846 -3.9914827 -4.0341392 -4.0858564 -4.1347604 -4.1722107 -4.2032671 -4.2291546 -4.2558503][-4.0104766 -4.0084143 -4.0031433 -3.9821782 -3.9547963 -3.9447212 -3.9572461 -3.9849107 -4.0264645 -4.0779004 -4.12658 -4.1638989 -4.196702 -4.225472 -4.2549725][-4.0365968 -4.0372329 -4.03626 -4.0225382 -4.0002241 -3.9870946 -3.9897921 -4.00694 -4.0394487 -4.0832553 -4.12618 -4.1603537 -4.1934996 -4.2245331 -4.2555337][-4.0746889 -4.0754066 -4.0774617 -4.0721169 -4.0581179 -4.0464311 -4.0454445 -4.0552616 -4.0776758 -4.1102972 -4.1423516 -4.1695108 -4.1995754 -4.2295456 -4.2586489][-4.1328354 -4.1322036 -4.1342826 -4.1335073 -4.1274624 -4.1216736 -4.1222172 -4.1284084 -4.1408052 -4.1602354 -4.1780233 -4.1939716 -4.2165289 -4.2412348 -4.2650347][-4.1831355 -4.1853886 -4.1871824 -4.1871405 -4.1849222 -4.183207 -4.18573 -4.1906576 -4.1975603 -4.2079268 -4.2155066 -4.222446 -4.2374158 -4.2554078 -4.2728667][-4.182025 -4.1861577 -4.1877046 -4.1860905 -4.1831493 -4.1849375 -4.1930766 -4.2036786 -4.2152352 -4.22766 -4.236711 -4.2435341 -4.2553515 -4.268167 -4.2802567][-4.1453443 -4.1478395 -4.1478477 -4.1430531 -4.1370983 -4.1397228 -4.1527295 -4.1709151 -4.1912084 -4.2133636 -4.2323155 -4.2468815 -4.2616634 -4.2742505 -4.2847257]]...]
INFO - root - 2017-12-05 15:17:06.110186: step 21610, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 78h:15m:22s remains)
INFO - root - 2017-12-05 15:17:15.449172: step 21620, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 79h:39m:58s remains)
INFO - root - 2017-12-05 15:17:24.636059: step 21630, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 80h:17m:02s remains)
INFO - root - 2017-12-05 15:17:33.953514: step 21640, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.827 sec/batch; 71h:23m:29s remains)
INFO - root - 2017-12-05 15:17:43.315150: step 21650, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.931 sec/batch; 80h:23m:13s remains)
INFO - root - 2017-12-05 15:17:52.760707: step 21660, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 80h:18m:21s remains)
INFO - root - 2017-12-05 15:18:02.197089: step 21670, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 83h:33m:40s remains)
INFO - root - 2017-12-05 15:18:11.547028: step 21680, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 78h:07m:37s remains)
INFO - root - 2017-12-05 15:18:20.953105: step 21690, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 78h:21m:39s remains)
INFO - root - 2017-12-05 15:18:30.536979: step 21700, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.969 sec/batch; 83h:42m:00s remains)
2017-12-05 15:18:31.298772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2213745 -4.225225 -4.2331691 -4.2364888 -4.2275019 -4.2109303 -4.181726 -4.1574311 -4.1593423 -4.1783404 -4.18586 -4.1906667 -4.1996732 -4.2115083 -4.2301254][-4.2045665 -4.2072444 -4.214426 -4.2157803 -4.2036338 -4.1805043 -4.1463828 -4.1177454 -4.1240778 -4.1502619 -4.16433 -4.1695294 -4.1803284 -4.1949534 -4.2110186][-4.1900926 -4.1924286 -4.1973605 -4.194603 -4.175652 -4.1549716 -4.1163144 -4.0819368 -4.0902066 -4.1202369 -4.1413269 -4.1559105 -4.1724825 -4.1904254 -4.2023511][-4.1841817 -4.1933517 -4.198936 -4.187768 -4.168303 -4.1506925 -4.1095881 -4.0730124 -4.0846434 -4.1200609 -4.1451654 -4.1665282 -4.1852956 -4.2013164 -4.2089891][-4.17712 -4.18739 -4.18678 -4.1670132 -4.1492996 -4.1275835 -4.0764642 -4.0428848 -4.0690203 -4.1189122 -4.1528435 -4.1762681 -4.1953321 -4.2108626 -4.2184138][-4.1727824 -4.1749053 -4.1688452 -4.1384387 -4.1087575 -4.063262 -3.9867606 -3.9525208 -4.0084963 -4.0889454 -4.1396575 -4.1711121 -4.1958952 -4.2139435 -4.2232981][-4.1747589 -4.1704803 -4.1592388 -4.1183815 -4.0577631 -3.963553 -3.841496 -3.8066678 -3.912631 -4.0373125 -4.1141448 -4.162291 -4.2009916 -4.223568 -4.2338309][-4.1903672 -4.1819739 -4.1667404 -4.1203933 -4.040832 -3.9127607 -3.7560148 -3.7270901 -3.8675611 -4.0130253 -4.1064315 -4.1690083 -4.2166328 -4.2399879 -4.2494121][-4.209271 -4.2025304 -4.1947083 -4.1574869 -4.0882874 -3.9751961 -3.8379071 -3.8099604 -3.9286847 -4.0605187 -4.146821 -4.2036314 -4.2430634 -4.2571092 -4.2630911][-4.2269917 -4.224586 -4.2276211 -4.2082696 -4.161201 -4.0786057 -3.9725981 -3.9394574 -4.0153012 -4.1205163 -4.1933985 -4.2400556 -4.2670393 -4.2696919 -4.2711415][-4.2447081 -4.2480712 -4.2592874 -4.2463665 -4.2122946 -4.1468925 -4.0594077 -4.0166249 -4.0604057 -4.1419787 -4.2068357 -4.2543039 -4.2789288 -4.2754374 -4.2722235][-4.25555 -4.2619395 -4.2728863 -4.2630262 -4.2309284 -4.1749964 -4.0969925 -4.0532436 -4.0778437 -4.1396465 -4.2012358 -4.2556438 -4.285913 -4.2802896 -4.2765455][-4.2537365 -4.2572913 -4.2639685 -4.2577767 -4.2307754 -4.1864781 -4.1259127 -4.0914416 -4.1086917 -4.154376 -4.2093143 -4.2623758 -4.292532 -4.2867165 -4.2830882][-4.251349 -4.25217 -4.2574487 -4.2525668 -4.2327065 -4.202836 -4.1631675 -4.1427212 -4.1563129 -4.1884012 -4.2318497 -4.2765617 -4.3005805 -4.2949104 -4.29073][-4.2463112 -4.2468061 -4.2532144 -4.2484231 -4.2354727 -4.2161765 -4.1927729 -4.1805663 -4.1919518 -4.2165847 -4.2499075 -4.2871895 -4.3051887 -4.300106 -4.2963524]]...]
INFO - root - 2017-12-05 15:18:40.656650: step 21710, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 83h:08m:44s remains)
INFO - root - 2017-12-05 15:18:49.912419: step 21720, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 77h:45m:46s remains)
INFO - root - 2017-12-05 15:18:59.294152: step 21730, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 81h:59m:44s remains)
INFO - root - 2017-12-05 15:19:08.670310: step 21740, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.946 sec/batch; 81h:38m:54s remains)
INFO - root - 2017-12-05 15:19:18.212589: step 21750, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 85h:07m:59s remains)
INFO - root - 2017-12-05 15:19:27.902280: step 21760, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 84h:45m:02s remains)
INFO - root - 2017-12-05 15:19:37.363199: step 21770, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 82h:15m:52s remains)
INFO - root - 2017-12-05 15:19:46.877443: step 21780, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 83h:30m:02s remains)
INFO - root - 2017-12-05 15:19:56.369103: step 21790, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.942 sec/batch; 81h:16m:35s remains)
INFO - root - 2017-12-05 15:20:05.644774: step 21800, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.924 sec/batch; 79h:43m:53s remains)
2017-12-05 15:20:06.365579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1695209 -4.1695452 -4.1647544 -4.1517215 -4.1193519 -4.0890017 -4.0824103 -4.10428 -4.1467862 -4.1845589 -4.2219296 -4.2449932 -4.2525034 -4.2598619 -4.2646465][-4.1335359 -4.1406493 -4.144228 -4.1402555 -4.1134663 -4.0865312 -4.083755 -4.1092162 -4.1538658 -4.1973534 -4.2397285 -4.2660255 -4.2755952 -4.2867913 -4.2922068][-4.0920334 -4.1082883 -4.1212397 -4.123261 -4.1027508 -4.0857935 -4.0929589 -4.1252561 -4.1731272 -4.216701 -4.2529826 -4.275229 -4.2832193 -4.2920604 -4.295383][-4.0639749 -4.0886378 -4.112164 -4.114645 -4.095633 -4.0890417 -4.1051021 -4.1456442 -4.1964903 -4.2373233 -4.27048 -4.2872047 -4.2905293 -4.2899232 -4.2865376][-4.0754213 -4.1044712 -4.1328988 -4.1299391 -4.1056585 -4.0991988 -4.1144662 -4.1549473 -4.2073069 -4.2504263 -4.2814741 -4.2970281 -4.2970476 -4.2858391 -4.2723465][-4.1099949 -4.1358905 -4.1633887 -4.1545811 -4.1245089 -4.1084795 -4.112164 -4.1466284 -4.2026768 -4.2486076 -4.2787967 -4.2935047 -4.2928896 -4.2759418 -4.2564626][-4.1518536 -4.1621037 -4.1769104 -4.1646628 -4.1345582 -4.1079788 -4.1034293 -4.1353154 -4.18918 -4.2351689 -4.2646317 -4.2788587 -4.2764621 -4.2560415 -4.2379656][-4.1838231 -4.170743 -4.1623287 -4.1408067 -4.1111684 -4.0858459 -4.0858459 -4.1178422 -4.1622305 -4.2017722 -4.2299175 -4.2428589 -4.238801 -4.221839 -4.2150531][-4.1886635 -4.1569581 -4.1263 -4.0936279 -4.0676017 -4.0563 -4.0740418 -4.1085114 -4.14197 -4.172576 -4.195168 -4.2037406 -4.1997266 -4.1930146 -4.1986852][-4.1723037 -4.1357613 -4.0993738 -4.0662317 -4.0482721 -4.0527196 -4.0857596 -4.1239691 -4.154192 -4.1768909 -4.1904321 -4.1933045 -4.1882253 -4.1860704 -4.1955981][-4.1467605 -4.1209517 -4.0996323 -4.0836296 -4.0782423 -4.0931721 -4.13072 -4.1695323 -4.1997819 -4.2192273 -4.2280569 -4.2270703 -4.2205968 -4.2147565 -4.21585][-4.11499 -4.1058927 -4.1090136 -4.12007 -4.1338787 -4.1545057 -4.1900754 -4.2275391 -4.2576237 -4.277802 -4.2883782 -4.28802 -4.277709 -4.2621741 -4.2479076][-4.0933914 -4.0958061 -4.1162305 -4.1477242 -4.1781545 -4.2079058 -4.2416716 -4.2721357 -4.2980866 -4.3175411 -4.3294463 -4.3303289 -4.319993 -4.3004508 -4.277288][-4.0944347 -4.1064906 -4.1371245 -4.1772504 -4.2140636 -4.2462368 -4.2731528 -4.292438 -4.310421 -4.3260612 -4.3356309 -4.3385377 -4.334177 -4.3206387 -4.3005962][-4.1126008 -4.1300831 -4.1610789 -4.20176 -4.2381773 -4.2658291 -4.2822013 -4.288136 -4.2948365 -4.3049192 -4.3121738 -4.3176012 -4.3206396 -4.31734 -4.3068228]]...]
INFO - root - 2017-12-05 15:20:15.871806: step 21810, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 84h:13m:46s remains)
INFO - root - 2017-12-05 15:20:25.429423: step 21820, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 77h:46m:19s remains)
INFO - root - 2017-12-05 15:20:34.553029: step 21830, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 82h:58m:02s remains)
INFO - root - 2017-12-05 15:20:44.083355: step 21840, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 86h:49m:01s remains)
INFO - root - 2017-12-05 15:20:53.484037: step 21850, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.951 sec/batch; 82h:06m:01s remains)
INFO - root - 2017-12-05 15:21:03.012547: step 21860, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.015 sec/batch; 87h:32m:46s remains)
INFO - root - 2017-12-05 15:21:12.355102: step 21870, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 84h:31m:08s remains)
INFO - root - 2017-12-05 15:21:21.925287: step 21880, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 81h:52m:05s remains)
INFO - root - 2017-12-05 15:21:31.389677: step 21890, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 81h:41m:29s remains)
INFO - root - 2017-12-05 15:21:40.675670: step 21900, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 84h:11m:29s remains)
2017-12-05 15:21:41.461961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3318844 -4.3359261 -4.338017 -4.3375425 -4.3343477 -4.3307924 -4.3282795 -4.3288531 -4.3294339 -4.3275967 -4.3234649 -4.31869 -4.3116751 -4.3062692 -4.302444][-4.3336291 -4.3336005 -4.3294015 -4.3213248 -4.3103838 -4.3016019 -4.2976732 -4.3019156 -4.3114362 -4.3189683 -4.3208594 -4.3184943 -4.3118038 -4.3050528 -4.2987185][-4.3302078 -4.3233018 -4.3098927 -4.2912483 -4.2691665 -4.2510362 -4.24527 -4.2540545 -4.2745452 -4.2959118 -4.3075023 -4.310215 -4.3050795 -4.2978973 -4.2898602][-4.327981 -4.3135428 -4.2891169 -4.2586403 -4.2242107 -4.1951222 -4.1859975 -4.1984205 -4.2295012 -4.2648921 -4.2854395 -4.2928267 -4.2903633 -4.2841458 -4.2765245][-4.3215361 -4.3028684 -4.2722435 -4.2330365 -4.1873112 -4.14628 -4.13154 -4.1469731 -4.1877179 -4.2356954 -4.2625103 -4.2710214 -4.2704558 -4.2662039 -4.2611647][-4.3108163 -4.294672 -4.2662416 -4.2263355 -4.177319 -4.1296062 -4.1081791 -4.1215115 -4.1634789 -4.213274 -4.2405949 -4.2467232 -4.2442884 -4.2394943 -4.23762][-4.3011336 -4.2911973 -4.2699652 -4.2362971 -4.1904597 -4.1414208 -4.11355 -4.1189084 -4.1496511 -4.1877842 -4.2084322 -4.211916 -4.2085295 -4.20476 -4.2082753][-4.271431 -4.2682486 -4.2590456 -4.2392817 -4.2037749 -4.1587372 -4.1276307 -4.1225076 -4.1365309 -4.1559787 -4.1669359 -4.1714025 -4.1740866 -4.1761017 -4.1862922][-4.2354417 -4.2389636 -4.2405562 -4.2334051 -4.2085533 -4.1698236 -4.1374331 -4.124423 -4.1246462 -4.1293197 -4.1324048 -4.1402326 -4.1508484 -4.1601019 -4.1765127][-4.222362 -4.2304249 -4.2373714 -4.2357745 -4.215476 -4.1784024 -4.141058 -4.1190748 -4.1122432 -4.1126623 -4.115263 -4.1296234 -4.1485329 -4.1634603 -4.1817303][-4.2353597 -4.2448506 -4.2535419 -4.2533412 -4.2343879 -4.198647 -4.1597848 -4.1344261 -4.1264296 -4.127841 -4.134203 -4.1538754 -4.1755047 -4.1911545 -4.2061343][-4.2702279 -4.2775846 -4.284368 -4.2857175 -4.2729187 -4.2462792 -4.2155948 -4.1950922 -4.1902575 -4.1929188 -4.1992321 -4.2149329 -4.2285638 -4.2360277 -4.2424135][-4.3071766 -4.3119326 -4.3165364 -4.3188214 -4.3134689 -4.2996054 -4.2804766 -4.2659044 -4.2623143 -4.2637668 -4.2663345 -4.2724481 -4.2741871 -4.2721009 -4.2706323][-4.3294163 -4.3310781 -4.3327909 -4.3349152 -4.3344235 -4.3297629 -4.32101 -4.3121748 -4.308701 -4.3088384 -4.30799 -4.3054833 -4.2979708 -4.2902632 -4.2844591][-4.3316717 -4.3316464 -4.3317437 -4.3334441 -4.3349938 -4.3346033 -4.3321824 -4.3279872 -4.3250809 -4.3241282 -4.3202152 -4.3127837 -4.3014078 -4.2927113 -4.2868581]]...]
INFO - root - 2017-12-05 15:21:50.911301: step 21910, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 82h:22m:56s remains)
INFO - root - 2017-12-05 15:21:59.869062: step 21920, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 72h:23m:55s remains)
INFO - root - 2017-12-05 15:22:09.233536: step 21930, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 80h:54m:47s remains)
INFO - root - 2017-12-05 15:22:18.616025: step 21940, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.884 sec/batch; 76h:16m:06s remains)
INFO - root - 2017-12-05 15:22:28.134027: step 21950, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 82h:41m:18s remains)
INFO - root - 2017-12-05 15:22:37.632778: step 21960, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 82h:49m:08s remains)
INFO - root - 2017-12-05 15:22:47.029685: step 21970, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 81h:27m:22s remains)
INFO - root - 2017-12-05 15:22:56.509671: step 21980, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 80h:37m:46s remains)
INFO - root - 2017-12-05 15:23:05.652826: step 21990, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 75h:47m:26s remains)
INFO - root - 2017-12-05 15:23:15.047112: step 22000, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.936 sec/batch; 80h:41m:28s remains)
2017-12-05 15:23:15.759620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2762976 -4.2534075 -4.2270384 -4.2056541 -4.1997848 -4.1993065 -4.1780782 -4.1547551 -4.1512036 -4.1566672 -4.1498713 -4.1381907 -4.1384287 -4.1436043 -4.1466455][-4.278213 -4.2589579 -4.2345419 -4.2173438 -4.2158327 -4.216732 -4.1955042 -4.1631274 -4.1406403 -4.131237 -4.1239815 -4.1240611 -4.1397681 -4.1505456 -4.150476][-4.2790918 -4.261404 -4.240756 -4.2290916 -4.2291293 -4.2263436 -4.205512 -4.1685605 -4.1330733 -4.11035 -4.097126 -4.1079855 -4.1393785 -4.1597657 -4.1602645][-4.2815423 -4.2622366 -4.2412672 -4.2304063 -4.2231774 -4.2075834 -4.1777544 -4.1382022 -4.112267 -4.1022868 -4.0907049 -4.1006279 -4.1347451 -4.1597295 -4.16455][-4.2818389 -4.2597318 -4.2328167 -4.2141881 -4.1905622 -4.1512036 -4.0989027 -4.05513 -4.0556922 -4.0812144 -4.0906415 -4.1042347 -4.133419 -4.1560745 -4.1613774][-4.2765446 -4.2507248 -4.216382 -4.1853209 -4.1449575 -4.0803823 -3.9937387 -3.9347363 -3.9689298 -4.0431013 -4.086689 -4.1123686 -4.1343246 -4.150176 -4.1565981][-4.2673144 -4.2369542 -4.1975713 -4.1562753 -4.1044378 -4.0203037 -3.9020472 -3.8205595 -3.8833513 -4.0010591 -4.0743432 -4.1125669 -4.1281815 -4.135891 -4.1437016][-4.257628 -4.2238946 -4.1809649 -4.1346455 -4.083158 -4.0025883 -3.8901167 -3.8164818 -3.8814809 -3.9974852 -4.0717921 -4.1063647 -4.1138425 -4.114368 -4.1186032][-4.2494044 -4.2139907 -4.1717472 -4.1297865 -4.095027 -4.0455351 -3.9829736 -3.9397125 -3.9783771 -4.0510373 -4.1026998 -4.1255741 -4.1234603 -4.1159616 -4.1095862][-4.2471991 -4.2093377 -4.1694117 -4.1343594 -4.1170416 -4.1003885 -4.0806961 -4.0629382 -4.08231 -4.1212525 -4.1532893 -4.1709714 -4.1679096 -4.153666 -4.138052][-4.2547617 -4.2185359 -4.1818442 -4.1551819 -4.1478076 -4.1511641 -4.1547794 -4.1496081 -4.15994 -4.1810369 -4.2015109 -4.2172484 -4.2143741 -4.1973777 -4.1798382][-4.2707362 -4.2411795 -4.211935 -4.1917739 -4.1835313 -4.1900549 -4.2015834 -4.2029133 -4.2128134 -4.2296906 -4.2457466 -4.2575111 -4.2531986 -4.2382541 -4.2236428][-4.2900987 -4.2681737 -4.2469277 -4.2321386 -4.2248325 -4.2294273 -4.2391486 -4.2427964 -4.2522006 -4.2673588 -4.282073 -4.2917724 -4.2900476 -4.2794237 -4.2675786][-4.306828 -4.2896266 -4.2733727 -4.2635403 -4.2630367 -4.2700019 -4.2800689 -4.284441 -4.2906179 -4.3010273 -4.3108683 -4.3166542 -4.3163385 -4.3125238 -4.3055224][-4.3195457 -4.304419 -4.291429 -4.2869673 -4.2924929 -4.3025208 -4.3107738 -4.3128676 -4.314292 -4.3180285 -4.3227525 -4.3263183 -4.3276439 -4.3266778 -4.3234639]]...]
INFO - root - 2017-12-05 15:23:25.171636: step 22010, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 83h:21m:21s remains)
INFO - root - 2017-12-05 15:23:34.762216: step 22020, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 82h:46m:57s remains)
INFO - root - 2017-12-05 15:23:44.217568: step 22030, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 82h:45m:23s remains)
INFO - root - 2017-12-05 15:23:53.934749: step 22040, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.952 sec/batch; 82h:04m:17s remains)
INFO - root - 2017-12-05 15:24:03.429691: step 22050, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 78h:30m:02s remains)
INFO - root - 2017-12-05 15:24:12.744153: step 22060, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 78h:38m:45s remains)
INFO - root - 2017-12-05 15:24:21.883537: step 22070, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 85h:20m:51s remains)
INFO - root - 2017-12-05 15:24:31.260364: step 22080, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 83h:19m:51s remains)
INFO - root - 2017-12-05 15:24:40.612312: step 22090, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 82h:08m:26s remains)
INFO - root - 2017-12-05 15:24:49.788198: step 22100, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 80h:27m:18s remains)
2017-12-05 15:24:50.535483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2974243 -4.2768979 -4.2556071 -4.240634 -4.2378097 -4.24258 -4.2436709 -4.2271562 -4.19948 -4.1804442 -4.1863461 -4.2032056 -4.23127 -4.2685547 -4.2966466][-4.2812262 -4.2612796 -4.2419567 -4.2341752 -4.2389622 -4.2501755 -4.2486868 -4.2196574 -4.1823125 -4.1632466 -4.1792631 -4.2020741 -4.2325273 -4.2691536 -4.2970839][-4.23462 -4.2196164 -4.2096615 -4.2100091 -4.2242765 -4.2389078 -4.2340045 -4.20099 -4.165277 -4.157074 -4.1880345 -4.2155442 -4.2451181 -4.2760348 -4.3008466][-4.1838326 -4.1791015 -4.1794167 -4.1855884 -4.1967092 -4.2030592 -4.1921616 -4.1629524 -4.1443219 -4.1578121 -4.201714 -4.2348795 -4.2624397 -4.2869291 -4.305][-4.1427832 -4.1525064 -4.162837 -4.1685662 -4.1671414 -4.1559281 -4.1381125 -4.1127429 -4.1124816 -4.1462326 -4.2059741 -4.2499485 -4.2777252 -4.2974439 -4.3107929][-4.1090841 -4.1366792 -4.1605878 -4.1680779 -4.1571312 -4.1244259 -4.0866718 -4.0594873 -4.0718794 -4.1232209 -4.1990361 -4.251523 -4.2830534 -4.3045444 -4.3166108][-4.0948577 -4.1344833 -4.1771536 -4.1884384 -4.1691728 -4.1136489 -4.0464067 -4.0088563 -4.0274205 -4.0922484 -4.1821103 -4.244463 -4.2840052 -4.3094196 -4.3209109][-4.1183448 -4.1564651 -4.2066493 -4.2178645 -4.1859407 -4.1150427 -4.0320029 -3.9877853 -4.007338 -4.0776114 -4.1693721 -4.2344065 -4.2802529 -4.3070288 -4.3199134][-4.1783247 -4.204926 -4.2434869 -4.2518759 -4.2166643 -4.1476278 -4.07416 -4.0335464 -4.0460048 -4.1031508 -4.1779585 -4.2356262 -4.2786593 -4.3047509 -4.3181291][-4.2274656 -4.2451982 -4.2692728 -4.2780113 -4.2552156 -4.2078195 -4.1569366 -4.1260581 -4.129333 -4.1643648 -4.2156711 -4.2595034 -4.2934966 -4.3139267 -4.3234086][-4.2560081 -4.2729182 -4.2880492 -4.2948222 -4.2830148 -4.2560844 -4.2213058 -4.1998072 -4.2009697 -4.223978 -4.2611456 -4.2952175 -4.3204589 -4.3336577 -4.3354959][-4.2820287 -4.297914 -4.3066883 -4.3063087 -4.293016 -4.2785454 -4.2556548 -4.2428141 -4.249536 -4.2710133 -4.3019834 -4.3287015 -4.3470974 -4.3528466 -4.3463583][-4.3014922 -4.3181925 -4.3225818 -4.316 -4.2968111 -4.2832255 -4.269032 -4.26392 -4.2753582 -4.2961984 -4.3220158 -4.3440695 -4.3569679 -4.3568039 -4.3466225][-4.310885 -4.3257513 -4.3271971 -4.3188024 -4.3017159 -4.28814 -4.2795224 -4.2801962 -4.2904696 -4.3050928 -4.32354 -4.3401151 -4.3486557 -4.3474193 -4.3399997][-4.3066392 -4.317832 -4.320642 -4.3176937 -4.3106771 -4.3035836 -4.2974167 -4.2954812 -4.299 -4.30524 -4.3161731 -4.3270688 -4.3339348 -4.3357668 -4.333673]]...]
INFO - root - 2017-12-05 15:24:59.857388: step 22110, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 83h:18m:24s remains)
INFO - root - 2017-12-05 15:25:09.086178: step 22120, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 78h:11m:21s remains)
INFO - root - 2017-12-05 15:25:18.732195: step 22130, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 84h:55m:26s remains)
INFO - root - 2017-12-05 15:25:27.982106: step 22140, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 81h:11m:06s remains)
INFO - root - 2017-12-05 15:25:37.249301: step 22150, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 79h:55m:33s remains)
INFO - root - 2017-12-05 15:25:46.575466: step 22160, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 79h:27m:12s remains)
INFO - root - 2017-12-05 15:25:55.918927: step 22170, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 74h:52m:57s remains)
INFO - root - 2017-12-05 15:26:05.365342: step 22180, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 79h:41m:35s remains)
INFO - root - 2017-12-05 15:26:14.635980: step 22190, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 78h:16m:37s remains)
INFO - root - 2017-12-05 15:26:24.078118: step 22200, loss = 2.03, batch loss = 1.98 (8.5 examples/sec; 0.943 sec/batch; 81h:15m:44s remains)
2017-12-05 15:26:24.814318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2464972 -4.2375107 -4.2325978 -4.2340188 -4.2455535 -4.2569051 -4.2632141 -4.2638078 -4.2619233 -4.2549605 -4.2488108 -4.2341595 -4.2190776 -4.2053223 -4.1734743][-4.237071 -4.2209516 -4.2108264 -4.2097945 -4.223156 -4.2387419 -4.2507596 -4.2552161 -4.2544923 -4.2480416 -4.2397733 -4.2182755 -4.199296 -4.1889577 -4.1698904][-4.2171011 -4.2035861 -4.1927128 -4.1897888 -4.2045379 -4.223639 -4.2403245 -4.2491717 -4.2524304 -4.24653 -4.2355695 -4.2102423 -4.189249 -4.1806726 -4.1726766][-4.1951346 -4.1882415 -4.1845717 -4.18508 -4.1955028 -4.211082 -4.2261 -4.2366271 -4.2397761 -4.2336488 -4.2213254 -4.1965141 -4.175878 -4.1690578 -4.1676044][-4.1780128 -4.1744413 -4.1780052 -4.1849947 -4.1895723 -4.1949873 -4.1978035 -4.2020674 -4.2071705 -4.2033796 -4.1949158 -4.1765904 -4.16033 -4.1577272 -4.1630235][-4.1479044 -4.1476521 -4.1555972 -4.156601 -4.1481371 -4.1404371 -4.1304684 -4.12897 -4.1447639 -4.1554666 -4.1557136 -4.1404538 -4.12217 -4.119524 -4.1255984][-4.1074257 -4.1024027 -4.0993128 -4.083179 -4.0594239 -4.0440879 -4.0386629 -4.054059 -4.0895615 -4.1112967 -4.108736 -4.0880294 -4.0687785 -4.063684 -4.06178][-4.0659094 -4.0577888 -4.0400529 -4.0021148 -3.9607766 -3.944833 -3.9601829 -4.0037131 -4.056951 -4.0880342 -4.0883126 -4.069335 -4.0532379 -4.0463729 -4.0295143][-4.0719204 -4.0762568 -4.0651841 -4.0277772 -3.9885688 -3.9776754 -3.9998398 -4.0426984 -4.0925093 -4.126996 -4.1362667 -4.124589 -4.1083817 -4.0954857 -4.0657005][-4.1185775 -4.1346707 -4.1385021 -4.12243 -4.1024871 -4.0921373 -4.0988784 -4.1215153 -4.1520085 -4.1759987 -4.1851816 -4.1808386 -4.1697369 -4.1576171 -4.1299367][-4.1608095 -4.1758513 -4.1874585 -4.1877403 -4.1859903 -4.1839466 -4.181746 -4.1870542 -4.1988039 -4.2096691 -4.2166028 -4.2167239 -4.2108126 -4.2025986 -4.1865129][-4.1971054 -4.2032061 -4.2133183 -4.2193255 -4.2246571 -4.2265 -4.2228789 -4.2179847 -4.2175517 -4.218524 -4.2210155 -4.2236757 -4.2233362 -4.2187462 -4.2097077][-4.2398739 -4.2389059 -4.2426853 -4.2437468 -4.2453351 -4.2467284 -4.2449455 -4.2409964 -4.2348943 -4.2299657 -4.2270279 -4.2284422 -4.2306333 -4.2268343 -4.2186346][-4.28432 -4.2837462 -4.2851973 -4.2831039 -4.27975 -4.2771964 -4.2735682 -4.2703037 -4.2660308 -4.2607083 -4.2558217 -4.2545152 -4.2544413 -4.2518163 -4.244565][-4.309844 -4.3115244 -4.3129983 -4.3093309 -4.3030739 -4.2985115 -4.2963648 -4.2959919 -4.2945266 -4.2908311 -4.2875218 -4.2854853 -4.28356 -4.2821479 -4.2774973]]...]
INFO - root - 2017-12-05 15:26:34.363348: step 22210, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 78h:31m:15s remains)
INFO - root - 2017-12-05 15:26:43.686287: step 22220, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 79h:50m:44s remains)
INFO - root - 2017-12-05 15:26:53.292125: step 22230, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 80h:30m:14s remains)
INFO - root - 2017-12-05 15:27:02.681431: step 22240, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 79h:36m:30s remains)
INFO - root - 2017-12-05 15:27:11.952498: step 22250, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 76h:31m:10s remains)
INFO - root - 2017-12-05 15:27:21.476976: step 22260, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 75h:56m:02s remains)
INFO - root - 2017-12-05 15:27:30.904197: step 22270, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 81h:12m:19s remains)
INFO - root - 2017-12-05 15:27:40.173431: step 22280, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 77h:25m:07s remains)
INFO - root - 2017-12-05 15:27:49.613829: step 22290, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 76h:40m:50s remains)
INFO - root - 2017-12-05 15:27:58.948506: step 22300, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 81h:30m:08s remains)
2017-12-05 15:27:59.756698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1968312 -4.1969843 -4.2268848 -4.2593427 -4.2720785 -4.2608247 -4.2345748 -4.2001915 -4.1738586 -4.1697216 -4.1835461 -4.2077112 -4.2395916 -4.2689371 -4.2884312][-4.2108693 -4.2120748 -4.2441788 -4.2799549 -4.2940593 -4.2780008 -4.2408009 -4.1924572 -4.1581731 -4.1581979 -4.1884594 -4.2309713 -4.2735419 -4.3074279 -4.3250737][-4.2168097 -4.2156239 -4.2466254 -4.2823062 -4.2954979 -4.2718048 -4.2219539 -4.1596632 -4.1194482 -4.1266422 -4.1715307 -4.23183 -4.2862 -4.3252668 -4.3436842][-4.2191405 -4.2180324 -4.2491655 -4.2813354 -4.2871704 -4.2500849 -4.1825104 -4.1077404 -4.0686884 -4.0910778 -4.1534381 -4.2291951 -4.2941613 -4.3365464 -4.3548617][-4.2339773 -4.2345991 -4.264132 -4.2859578 -4.2764816 -4.2185473 -4.12625 -4.0350676 -4.0005155 -4.0446277 -4.1275864 -4.21849 -4.2933512 -4.3408141 -4.360846][-4.2590313 -4.2632565 -4.2872257 -4.2962408 -4.2671652 -4.1832132 -4.0603766 -3.9508233 -3.9262078 -3.9954016 -4.0992155 -4.2009449 -4.2818375 -4.3327136 -4.3565321][-4.2867651 -4.2952666 -4.3118806 -4.3086643 -4.2617192 -4.15486 -4.0087848 -3.8915658 -3.8845732 -3.9777069 -4.0923972 -4.1944833 -4.2729912 -4.3208137 -4.3455696][-4.3076386 -4.3152189 -4.324501 -4.3131461 -4.2567186 -4.1422296 -3.9960549 -3.8962016 -3.9121809 -4.0139441 -4.1219964 -4.2075577 -4.2714224 -4.31069 -4.3316803][-4.3049045 -4.3091869 -4.3138475 -4.3016863 -4.2474027 -4.1421456 -4.0173225 -3.9507697 -3.9827733 -4.0741596 -4.1619787 -4.22448 -4.26936 -4.2977843 -4.3136907][-4.2832007 -4.2826533 -4.2891912 -4.2842913 -4.2432923 -4.1615467 -4.0663328 -4.0263529 -4.0625358 -4.1369371 -4.2004 -4.238811 -4.2628584 -4.2781444 -4.2894158][-4.2500038 -4.2469506 -4.2599711 -4.2655783 -4.2436638 -4.190671 -4.1229639 -4.0967813 -4.1288872 -4.1850166 -4.2272959 -4.2447424 -4.24581 -4.2445149 -4.2509317][-4.2130904 -4.207468 -4.2260261 -4.2425742 -4.2382283 -4.2080474 -4.1602006 -4.142694 -4.1723566 -4.2153239 -4.2396107 -4.2371278 -4.2146769 -4.1972637 -4.2023754][-4.1905336 -4.1841106 -4.2049422 -4.2307124 -4.2387271 -4.2195034 -4.1811543 -4.1673918 -4.1939912 -4.2280283 -4.2396336 -4.2203155 -4.1802182 -4.1543546 -4.1656671][-4.1871018 -4.1837182 -4.2082424 -4.2380323 -4.2483344 -4.2304707 -4.1962447 -4.183444 -4.203124 -4.2269373 -4.228158 -4.1973591 -4.1514645 -4.1299543 -4.1518383][-4.1954823 -4.1992455 -4.2289295 -4.2604046 -4.2654309 -4.2385564 -4.1981921 -4.1787853 -4.1903558 -4.2052531 -4.1992416 -4.16619 -4.1267757 -4.1184254 -4.1485281]]...]
INFO - root - 2017-12-05 15:28:09.006397: step 22310, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 79h:47m:29s remains)
INFO - root - 2017-12-05 15:28:18.263877: step 22320, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 81h:55m:12s remains)
INFO - root - 2017-12-05 15:28:27.684186: step 22330, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 80h:54m:43s remains)
INFO - root - 2017-12-05 15:28:36.882925: step 22340, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.820 sec/batch; 70h:40m:38s remains)
INFO - root - 2017-12-05 15:28:46.217093: step 22350, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 76h:11m:49s remains)
INFO - root - 2017-12-05 15:28:55.411213: step 22360, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 82h:33m:07s remains)
INFO - root - 2017-12-05 15:29:04.674977: step 22370, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 83h:06m:45s remains)
INFO - root - 2017-12-05 15:29:13.952916: step 22380, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.894 sec/batch; 77h:01m:54s remains)
INFO - root - 2017-12-05 15:29:23.340200: step 22390, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.993 sec/batch; 85h:31m:04s remains)
INFO - root - 2017-12-05 15:29:32.608931: step 22400, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 80h:47m:32s remains)
2017-12-05 15:29:33.394215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2719007 -4.26598 -4.2621307 -4.2614822 -4.2625108 -4.2612228 -4.2577786 -4.2526131 -4.2411904 -4.2311816 -4.2281537 -4.2282939 -4.2273912 -4.2225442 -4.2160611][-4.2549195 -4.2488403 -4.2469354 -4.2472491 -4.2457542 -4.2376771 -4.2256169 -4.2138615 -4.1946373 -4.1770735 -4.1707625 -4.1730714 -4.174633 -4.171854 -4.1683292][-4.2426848 -4.2348609 -4.2274022 -4.2178 -4.2056141 -4.1875458 -4.16611 -4.1466284 -4.1199121 -4.0974989 -4.0912247 -4.10039 -4.1059208 -4.1069098 -4.1142559][-4.2283354 -4.2148347 -4.1944613 -4.1673708 -4.1391482 -4.1058679 -4.0702543 -4.0462608 -4.0214052 -4.0018349 -4.0024796 -4.0244904 -4.0384278 -4.053616 -4.0804634][-4.2024355 -4.1736274 -4.1353192 -4.08914 -4.0440097 -3.9971864 -3.959991 -3.949261 -3.9463758 -3.9410615 -3.9533849 -3.9849448 -4.0107913 -4.0450306 -4.0890441][-4.1501765 -4.1001673 -4.0427642 -3.9818637 -3.9266212 -3.8795319 -3.8591712 -3.8858037 -3.918205 -3.9369917 -3.9624913 -3.9960654 -4.0298319 -4.0775919 -4.126924][-4.062912 -3.9855821 -3.9100375 -3.8500392 -3.8128595 -3.7888284 -3.8044653 -3.8779244 -3.9408946 -3.9784122 -4.0134869 -4.0471458 -4.0808492 -4.1310029 -4.1747141][-3.9452724 -3.8515558 -3.7738371 -3.7399459 -3.7548614 -3.7803802 -3.8404818 -3.9395592 -4.011241 -4.05188 -4.0875692 -4.1190071 -4.1494246 -4.1921034 -4.2240357][-3.8332527 -3.7604866 -3.7272449 -3.751091 -3.8168516 -3.8805814 -3.9518976 -4.0419068 -4.1028781 -4.1336517 -4.1638985 -4.187676 -4.2099023 -4.2408247 -4.2612119][-3.7692761 -3.75803 -3.7960503 -3.8601885 -3.9339023 -3.9985633 -4.0562916 -4.1248989 -4.1720848 -4.195107 -4.2172914 -4.2336135 -4.2449613 -4.2667155 -4.28424][-3.7819571 -3.8257875 -3.9064569 -3.9746416 -4.0252423 -4.0662341 -4.1064553 -4.1574688 -4.1978159 -4.2207842 -4.2431169 -4.2551603 -4.2605429 -4.2760735 -4.2952433][-3.8660789 -3.92594 -4.0011325 -4.0529819 -4.0827584 -4.1040263 -4.133801 -4.1725907 -4.2062588 -4.2301259 -4.2500219 -4.2593546 -4.2649775 -4.2803025 -4.3028994][-3.9760873 -4.0237408 -4.0700274 -4.0982394 -4.1200581 -4.1403971 -4.1663132 -4.1963215 -4.2218575 -4.2416267 -4.2552571 -4.262084 -4.2729316 -4.290472 -4.3113074][-4.0711255 -4.103075 -4.1226916 -4.1331592 -4.1510582 -4.17418 -4.1998186 -4.2226954 -4.2391438 -4.2559452 -4.2671156 -4.2758961 -4.2930217 -4.3094258 -4.3250461][-4.141118 -4.15812 -4.16395 -4.1670423 -4.186039 -4.2120981 -4.2355151 -4.2516155 -4.2624216 -4.27737 -4.2896218 -4.3009953 -4.3176312 -4.3308764 -4.3399396]]...]
INFO - root - 2017-12-05 15:29:42.799798: step 22410, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 80h:47m:19s remains)
INFO - root - 2017-12-05 15:29:52.218256: step 22420, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 82h:24m:56s remains)
INFO - root - 2017-12-05 15:30:01.402812: step 22430, loss = 2.02, batch loss = 1.96 (8.5 examples/sec; 0.941 sec/batch; 81h:05m:09s remains)
INFO - root - 2017-12-05 15:30:10.779144: step 22440, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 82h:59m:24s remains)
INFO - root - 2017-12-05 15:30:20.127128: step 22450, loss = 2.03, batch loss = 1.97 (8.3 examples/sec; 0.960 sec/batch; 82h:42m:10s remains)
INFO - root - 2017-12-05 15:30:29.653758: step 22460, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 86h:12m:30s remains)
INFO - root - 2017-12-05 15:30:39.023777: step 22470, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 71h:49m:09s remains)
INFO - root - 2017-12-05 15:30:48.378360: step 22480, loss = 2.12, batch loss = 2.06 (8.6 examples/sec; 0.932 sec/batch; 80h:15m:59s remains)
INFO - root - 2017-12-05 15:30:57.456658: step 22490, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 81h:49m:57s remains)
INFO - root - 2017-12-05 15:31:06.606305: step 22500, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 76h:11m:42s remains)
2017-12-05 15:31:07.391457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2925992 -4.2916231 -4.2877517 -4.28159 -4.277225 -4.2773948 -4.2830453 -4.291954 -4.2964149 -4.2902694 -4.2744145 -4.2558832 -4.2452974 -4.2430573 -4.2425008][-4.2931447 -4.2921591 -4.2867441 -4.2765269 -4.2668095 -4.2632089 -4.2668204 -4.2765088 -4.2810245 -4.2752857 -4.2622681 -4.2486048 -4.2440238 -4.2464576 -4.2494693][-4.294982 -4.2939925 -4.2873 -4.2727156 -4.2565675 -4.2472992 -4.2467756 -4.2528014 -4.25243 -4.2458792 -4.2386165 -4.2346807 -4.240005 -4.2474146 -4.2517705][-4.3012156 -4.3012571 -4.2946644 -4.2773571 -4.2560496 -4.2411427 -4.2344818 -4.233778 -4.2257195 -4.2152553 -4.2091289 -4.2093821 -4.2202821 -4.233253 -4.2410836][-4.304934 -4.3065286 -4.3008866 -4.2828088 -4.2585764 -4.2388759 -4.22525 -4.2154365 -4.1992049 -4.1817913 -4.1710267 -4.1706018 -4.1845236 -4.2049308 -4.2190075][-4.3060284 -4.3092208 -4.3045444 -4.2873139 -4.2634482 -4.2428341 -4.2247305 -4.2059951 -4.182313 -4.1561828 -4.1358356 -4.128324 -4.1420221 -4.1694813 -4.1905961][-4.30417 -4.3081231 -4.3034053 -4.2877445 -4.2666755 -4.2482896 -4.2293234 -4.2043476 -4.1739631 -4.1390653 -4.107789 -4.0908222 -4.1047473 -4.1391745 -4.1659813][-4.3000503 -4.3047366 -4.2994504 -4.2846327 -4.2648916 -4.2467394 -4.226954 -4.1970706 -4.1593342 -4.11723 -4.0779891 -4.0566363 -4.0762057 -4.1183524 -4.1480255][-4.294126 -4.2996058 -4.2923312 -4.2765293 -4.2559862 -4.2356977 -4.214807 -4.1838303 -4.1414218 -4.0944915 -4.0506473 -4.0288386 -4.0549226 -4.1012125 -4.1304832][-4.2882776 -4.2949486 -4.2859087 -4.2683883 -4.2459087 -4.2233353 -4.2034149 -4.174479 -4.1324706 -4.0837035 -4.0375142 -4.0150695 -4.0431981 -4.0887151 -4.1152][-4.277801 -4.2870545 -4.2791367 -4.2601604 -4.2352509 -4.2098451 -4.192275 -4.1710281 -4.1333718 -4.0876493 -4.0430064 -4.0172024 -4.0393419 -4.0793567 -4.1033912][-4.2636275 -4.2757845 -4.2706943 -4.2517314 -4.2257953 -4.2005486 -4.18714 -4.1737185 -4.143621 -4.1082482 -4.0714159 -4.04518 -4.0562444 -4.0857916 -4.1027608][-4.2491961 -4.2611074 -4.2594342 -4.2432647 -4.219862 -4.1987586 -4.1931791 -4.1934576 -4.1800709 -4.1591868 -4.1298833 -4.1003437 -4.09377 -4.1045833 -4.1054459][-4.2304034 -4.2393551 -4.2413778 -4.2321324 -4.2145662 -4.1990371 -4.2010241 -4.2119107 -4.2121911 -4.2027717 -4.1797719 -4.1458249 -4.1227255 -4.11756 -4.1061811][-4.2154241 -4.2164702 -4.2199082 -4.2201252 -4.213336 -4.2059641 -4.2110596 -4.2238345 -4.2297277 -4.2275338 -4.2100234 -4.1764054 -4.143095 -4.1291451 -4.1140156]]...]
INFO - root - 2017-12-05 15:31:16.752112: step 22510, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 80h:36m:09s remains)
INFO - root - 2017-12-05 15:31:26.098586: step 22520, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 80h:01m:55s remains)
INFO - root - 2017-12-05 15:31:35.501116: step 22530, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 78h:43m:53s remains)
INFO - root - 2017-12-05 15:31:44.782581: step 22540, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 79h:48m:28s remains)
INFO - root - 2017-12-05 15:31:54.118129: step 22550, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 79h:02m:46s remains)
INFO - root - 2017-12-05 15:32:03.310256: step 22560, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 79h:23m:36s remains)
INFO - root - 2017-12-05 15:32:12.583036: step 22570, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.945 sec/batch; 81h:21m:41s remains)
INFO - root - 2017-12-05 15:32:22.097490: step 22580, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 81h:14m:29s remains)
INFO - root - 2017-12-05 15:32:31.453636: step 22590, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 82h:55m:16s remains)
INFO - root - 2017-12-05 15:32:40.854492: step 22600, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.958 sec/batch; 82h:27m:39s remains)
2017-12-05 15:32:41.539016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2852721 -4.265873 -4.2523532 -4.2448897 -4.2466278 -4.2524757 -4.2579908 -4.2632542 -4.263742 -4.2638979 -4.2668662 -4.2742581 -4.2802238 -4.2845316 -4.2882071][-4.2611856 -4.2326269 -4.2132425 -4.2014513 -4.2061052 -4.2150598 -4.218863 -4.2239361 -4.2287126 -4.2356434 -4.2427106 -4.2547021 -4.2652593 -4.2694869 -4.2685671][-4.236022 -4.1984916 -4.1719227 -4.1535335 -4.1569595 -4.166225 -4.1621761 -4.1642733 -4.1765342 -4.1921511 -4.2045112 -4.2180324 -4.2366066 -4.24423 -4.2411537][-4.2215753 -4.1772056 -4.1433649 -4.11703 -4.1123695 -4.1139035 -4.0975208 -4.0964832 -4.1233554 -4.1484823 -4.1651974 -4.1749258 -4.1979356 -4.212709 -4.2079735][-4.2100549 -4.1601591 -4.1221533 -4.0929494 -4.0791621 -4.060225 -4.0148878 -3.9986863 -4.0417285 -4.0897803 -4.1129889 -4.1190581 -4.1451111 -4.1711664 -4.1733632][-4.1888084 -4.12922 -4.0840878 -4.052412 -4.0281143 -3.9831467 -3.9002247 -3.8580592 -3.9370608 -4.0225739 -4.0558147 -4.0581126 -4.09002 -4.1308608 -4.1449938][-4.159802 -4.0815735 -4.0190444 -3.979851 -3.9460123 -3.8708239 -3.722141 -3.6281621 -3.7742956 -3.9202971 -3.9695213 -3.9768445 -4.0205231 -4.08006 -4.115171][-4.1402454 -4.0528755 -3.975771 -3.9325867 -3.9022377 -3.816155 -3.6205497 -3.4916053 -3.6904917 -3.8695619 -3.9292283 -3.9454956 -3.9953582 -4.0582371 -4.1028056][-4.1539497 -4.0894737 -4.0361748 -4.0119314 -4.0021949 -3.9448457 -3.80214 -3.7219 -3.8523097 -3.9760408 -4.0126224 -4.0245571 -4.0595531 -4.0988479 -4.1359758][-4.1745296 -4.133389 -4.1059155 -4.0919166 -4.0891671 -4.0545363 -3.9719439 -3.9375165 -4.0073829 -4.0737085 -4.0922842 -4.0972137 -4.1134386 -4.1337223 -4.1647558][-4.200819 -4.1754832 -4.1578355 -4.1452632 -4.1342106 -4.1010795 -4.0511656 -4.0464015 -4.0876832 -4.1187854 -4.1312833 -4.137146 -4.14786 -4.16459 -4.1919489][-4.23757 -4.224678 -4.2144918 -4.2042317 -4.1886225 -4.1581411 -4.12916 -4.1389909 -4.1680427 -4.1808052 -4.1858768 -4.1917181 -4.20437 -4.2191072 -4.235734][-4.2783952 -4.2751722 -4.2747536 -4.2740374 -4.2668009 -4.2491422 -4.2342539 -4.2449551 -4.2617025 -4.2650361 -4.2663283 -4.2716374 -4.2808852 -4.28856 -4.2962627][-4.3082838 -4.3081551 -4.3109317 -4.3148313 -4.3155584 -4.3103909 -4.3061562 -4.3139162 -4.3237586 -4.3233218 -4.3219361 -4.3220639 -4.3253479 -4.32703 -4.3303423][-4.3233762 -4.3229647 -4.3244371 -4.3264155 -4.3263474 -4.3254008 -4.3258824 -4.3301492 -4.3341675 -4.3342824 -4.334199 -4.3344474 -4.3342719 -4.3343463 -4.3359108]]...]
INFO - root - 2017-12-05 15:32:50.958878: step 22610, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 81h:10m:04s remains)
INFO - root - 2017-12-05 15:33:00.348126: step 22620, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 82h:44m:21s remains)
INFO - root - 2017-12-05 15:33:09.547864: step 22630, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 77h:53m:12s remains)
INFO - root - 2017-12-05 15:33:18.993651: step 22640, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 82h:00m:03s remains)
INFO - root - 2017-12-05 15:33:28.325618: step 22650, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 79h:58m:24s remains)
INFO - root - 2017-12-05 15:33:37.664484: step 22660, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 76h:21m:14s remains)
INFO - root - 2017-12-05 15:33:47.161691: step 22670, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 81h:32m:36s remains)
INFO - root - 2017-12-05 15:33:56.525646: step 22680, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 82h:48m:59s remains)
INFO - root - 2017-12-05 15:34:05.895571: step 22690, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 82h:17m:57s remains)
INFO - root - 2017-12-05 15:34:15.047459: step 22700, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.991 sec/batch; 85h:17m:01s remains)
2017-12-05 15:34:15.864506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3025103 -4.2884159 -4.2677288 -4.2412724 -4.216711 -4.2007627 -4.1886091 -4.1818752 -4.1823664 -4.1888804 -4.2004824 -4.2149234 -4.2215753 -4.2213955 -4.2174916][-4.2924552 -4.2742748 -4.2477975 -4.2118268 -4.1789775 -4.1568408 -4.1402483 -4.1309915 -4.1352124 -4.1453838 -4.1582575 -4.1783457 -4.1842775 -4.1736975 -4.1584663][-4.2828755 -4.257453 -4.2218819 -4.1735034 -4.12765 -4.0929327 -4.0693917 -4.0668316 -4.0874395 -4.1111627 -4.1338162 -4.1607265 -4.165163 -4.1416979 -4.1090508][-4.2729244 -4.2399993 -4.1934309 -4.1288257 -4.0633841 -4.0082388 -3.9702659 -3.9754524 -4.0269756 -4.0782356 -4.1171184 -4.1506381 -4.1554904 -4.1270409 -4.0829258][-4.2652321 -4.2251072 -4.1674509 -4.085669 -3.9992411 -3.9216151 -3.858403 -3.8584085 -3.9448943 -4.0371513 -4.0973058 -4.1370177 -4.1454563 -4.1214004 -4.074801][-4.26282 -4.2180347 -4.1496511 -4.0548205 -3.9515252 -3.8534565 -3.7523069 -3.7196922 -3.8274283 -3.9587777 -4.0365191 -4.0829468 -4.1030803 -4.0958161 -4.0571737][-4.2655091 -4.2198172 -4.1458845 -4.0438285 -3.9360421 -3.82975 -3.6943071 -3.6094744 -3.7120683 -3.8697925 -3.9610708 -4.0117292 -4.0418572 -4.0509553 -4.0323639][-4.2693963 -4.2295976 -4.1630821 -4.0689411 -3.973871 -3.8862484 -3.7724845 -3.6842372 -3.7462611 -3.8749878 -3.9527788 -3.9914145 -4.0133491 -4.0280042 -4.0291252][-4.2746873 -4.2449145 -4.1934686 -4.1201615 -4.0532622 -3.9998021 -3.93291 -3.8791211 -3.8935235 -3.9548504 -4.0029831 -4.0326734 -4.0427103 -4.0487766 -4.0573816][-4.2818465 -4.2605104 -4.2228351 -4.1708803 -4.1306391 -4.1034389 -4.0724783 -4.0454021 -4.0336509 -4.0383449 -4.0600243 -4.0861959 -4.0937881 -4.094398 -4.1026134][-4.2933664 -4.2787561 -4.2520032 -4.2171221 -4.1931825 -4.1790018 -4.1637063 -4.1485677 -4.1299295 -4.1140904 -4.1202931 -4.1421833 -4.1512961 -4.1507487 -4.1564][-4.3040228 -4.2922912 -4.2705431 -4.2479367 -4.2319427 -4.2243719 -4.2189331 -4.2147341 -4.2048025 -4.1890507 -4.1847572 -4.19734 -4.2070518 -4.207345 -4.2032409][-4.3098741 -4.3005543 -4.2847767 -4.270978 -4.26076 -4.2564025 -4.2553177 -4.2569523 -4.2555971 -4.2466717 -4.2390351 -4.2426825 -4.2510633 -4.2531023 -4.2483282][-4.3175049 -4.3113875 -4.301105 -4.2924891 -4.2856283 -4.2814026 -4.2811437 -4.2847018 -4.2866549 -4.2821479 -4.2758794 -4.2777281 -4.2844005 -4.2888761 -4.28769][-4.3210335 -4.3174629 -4.3111048 -4.3069286 -4.3037167 -4.3015513 -4.3017035 -4.30473 -4.3066287 -4.3050151 -4.3026228 -4.3036089 -4.3074989 -4.3109083 -4.3115311]]...]
INFO - root - 2017-12-05 15:34:25.204438: step 22710, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 79h:36m:02s remains)
INFO - root - 2017-12-05 15:34:34.417259: step 22720, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 80h:31m:44s remains)
INFO - root - 2017-12-05 15:34:43.680250: step 22730, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 82h:45m:53s remains)
INFO - root - 2017-12-05 15:34:52.959753: step 22740, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 81h:52m:59s remains)
INFO - root - 2017-12-05 15:35:02.103472: step 22750, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 82h:50m:05s remains)
INFO - root - 2017-12-05 15:35:11.430726: step 22760, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.947 sec/batch; 81h:29m:07s remains)
INFO - root - 2017-12-05 15:35:20.857225: step 22770, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 80h:33m:37s remains)
INFO - root - 2017-12-05 15:35:30.301159: step 22780, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 81h:07m:12s remains)
INFO - root - 2017-12-05 15:35:39.620875: step 22790, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 82h:16m:02s remains)
INFO - root - 2017-12-05 15:35:48.984177: step 22800, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 84h:12m:10s remains)
2017-12-05 15:35:49.741321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2895036 -4.3019018 -4.3159676 -4.3193092 -4.3042636 -4.280664 -4.2670012 -4.268445 -4.2813115 -4.2963824 -4.3112922 -4.3173628 -4.3178086 -4.3165264 -4.3064685][-4.2851357 -4.2990513 -4.3156714 -4.3159933 -4.2926874 -4.2604914 -4.243175 -4.2482271 -4.2696581 -4.292078 -4.308423 -4.3132324 -4.3121943 -4.3121257 -4.3044562][-4.29088 -4.3002362 -4.3063345 -4.2935028 -4.2568927 -4.2146754 -4.1902127 -4.1968541 -4.2284994 -4.2624116 -4.2846441 -4.2921367 -4.2940412 -4.2982941 -4.2961607][-4.3019543 -4.302495 -4.2863131 -4.2503214 -4.1933131 -4.1348648 -4.1025743 -4.1145573 -4.1638241 -4.215414 -4.249783 -4.2655306 -4.2735491 -4.2817197 -4.2867656][-4.2993684 -4.2869625 -4.2465215 -4.1843877 -4.1013002 -4.01948 -3.9786251 -4.0019636 -4.0736508 -4.1453042 -4.2000828 -4.22967 -4.2461195 -4.2627678 -4.2779608][-4.2796879 -4.2548866 -4.193212 -4.1102285 -4.0045762 -3.900104 -3.8537297 -3.8945658 -3.9898 -4.0790362 -4.1503992 -4.1893826 -4.211627 -4.2371454 -4.266098][-4.2525373 -4.2152338 -4.1433516 -4.0568843 -3.9476886 -3.8350325 -3.7846789 -3.8343229 -3.9395919 -4.0388088 -4.1181793 -4.1584296 -4.1805754 -4.2098851 -4.2465677][-4.2321 -4.1911573 -4.124155 -4.0516763 -3.9589651 -3.859941 -3.8092813 -3.8445148 -3.9340231 -4.027822 -4.1069164 -4.1468368 -4.1695089 -4.1999907 -4.2334843][-4.229671 -4.1976008 -4.1495886 -4.1011138 -4.0393353 -3.9676955 -3.9218183 -3.9335501 -3.9937904 -4.0679822 -4.1315227 -4.16606 -4.1884661 -4.2116351 -4.2334661][-4.2466974 -4.2277489 -4.2005048 -4.1777897 -4.1491537 -4.105433 -4.0673332 -4.0657587 -4.1009679 -4.1479797 -4.1872749 -4.2092729 -4.2218747 -4.2312818 -4.2386427][-4.2641993 -4.254765 -4.2428169 -4.2388096 -4.2332096 -4.2132154 -4.1885004 -4.1829138 -4.1978145 -4.2170362 -4.2303734 -4.2409344 -4.2464857 -4.2456851 -4.2460556][-4.2713666 -4.2698011 -4.2669129 -4.2710047 -4.2728267 -4.2679935 -4.2585216 -4.25368 -4.2556496 -4.2576013 -4.2533908 -4.2551069 -4.2568369 -4.2508082 -4.248323][-4.272397 -4.2736926 -4.272891 -4.2750216 -4.277503 -4.2830534 -4.2863927 -4.2871842 -4.28583 -4.2803788 -4.2659388 -4.2618623 -4.2617874 -4.2521033 -4.2487974][-4.2777362 -4.2751536 -4.2688565 -4.2639484 -4.2650666 -4.279633 -4.2933965 -4.3020949 -4.3014779 -4.2902188 -4.2719746 -4.266139 -4.2647362 -4.2538161 -4.2511377][-4.2817645 -4.2720494 -4.2576079 -4.2478285 -4.2510457 -4.271287 -4.292459 -4.3069258 -4.3060384 -4.289012 -4.2665043 -4.2578182 -4.2548676 -4.245183 -4.2467813]]...]
INFO - root - 2017-12-05 15:35:59.130102: step 22810, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 83h:19m:44s remains)
INFO - root - 2017-12-05 15:36:08.620050: step 22820, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 82h:22m:17s remains)
INFO - root - 2017-12-05 15:36:18.243299: step 22830, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.003 sec/batch; 86h:16m:34s remains)
INFO - root - 2017-12-05 15:36:27.602467: step 22840, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 81h:21m:58s remains)
INFO - root - 2017-12-05 15:36:36.982973: step 22850, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 74h:49m:27s remains)
INFO - root - 2017-12-05 15:36:46.199824: step 22860, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 79h:29m:04s remains)
INFO - root - 2017-12-05 15:36:55.868957: step 22870, loss = 2.03, batch loss = 1.97 (8.2 examples/sec; 0.971 sec/batch; 83h:33m:25s remains)
INFO - root - 2017-12-05 15:37:05.167713: step 22880, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 83h:47m:20s remains)
INFO - root - 2017-12-05 15:37:14.235235: step 22890, loss = 2.12, batch loss = 2.06 (8.6 examples/sec; 0.925 sec/batch; 79h:34m:30s remains)
INFO - root - 2017-12-05 15:37:23.619370: step 22900, loss = 2.03, batch loss = 1.97 (8.1 examples/sec; 0.983 sec/batch; 84h:29m:48s remains)
2017-12-05 15:37:24.384131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3129582 -4.3098612 -4.3062119 -4.3001137 -4.2921863 -4.2846975 -4.2789378 -4.2745075 -4.2731576 -4.2755275 -4.2790456 -4.2819791 -4.2866149 -4.2916484 -4.2963591][-4.31222 -4.3048444 -4.2967343 -4.2849288 -4.2724156 -4.2622986 -4.2533417 -4.2458558 -4.2438049 -4.249548 -4.2576637 -4.2642875 -4.2730994 -4.2811856 -4.288208][-4.2955122 -4.2801991 -4.2653985 -4.24616 -4.2279162 -4.214767 -4.20447 -4.19907 -4.2031369 -4.215239 -4.22858 -4.2400694 -4.2531428 -4.2638788 -4.2724886][-4.2553596 -4.2346559 -4.2139268 -4.1874018 -4.1623306 -4.1437888 -4.1300211 -4.1269827 -4.1435218 -4.1690145 -4.1909032 -4.2079558 -4.2248878 -4.23645 -4.2445655][-4.1902909 -4.16718 -4.1437993 -4.1131759 -4.0813112 -4.052855 -4.0272684 -4.021647 -4.0540257 -4.0987062 -4.1360555 -4.1644497 -4.185524 -4.19681 -4.202446][-4.1077871 -4.0840058 -4.0593948 -4.0253139 -3.9869106 -3.9415209 -3.8877718 -3.8662233 -3.9155524 -3.9866138 -4.0489874 -4.1005249 -4.1312609 -4.1422348 -4.1479793][-4.0267754 -4.003716 -3.9808245 -3.9497459 -3.9125214 -3.8559728 -3.7759051 -3.7361393 -3.7939305 -3.8814478 -3.9616275 -4.0306096 -4.0686164 -4.0815144 -4.0894375][-4.0020328 -3.9861705 -3.9694686 -3.9501791 -3.9344242 -3.9020505 -3.8422616 -3.805676 -3.8378015 -3.893919 -3.9579158 -4.0216107 -4.0556183 -4.0677176 -4.0744376][-4.0497241 -4.0450029 -4.0402765 -4.0352569 -4.0382624 -4.0330973 -4.0066595 -3.9828472 -3.9847305 -4.0013323 -4.0369754 -4.0813355 -4.1037984 -4.1109018 -4.114059][-4.1393509 -4.1423817 -4.146419 -4.1468806 -4.1505489 -4.1525645 -4.1435227 -4.1271439 -4.1161737 -4.1145411 -4.1310616 -4.157227 -4.1726246 -4.1775918 -4.1791468][-4.209507 -4.21406 -4.2209873 -4.2218795 -4.2213058 -4.222044 -4.2185407 -4.2080641 -4.1982627 -4.1949749 -4.2021408 -4.2160764 -4.2248259 -4.2263269 -4.2263064][-4.2339711 -4.2376285 -4.2425852 -4.2426047 -4.2402 -4.2395124 -4.2383308 -4.2322521 -4.22744 -4.2278867 -4.2309813 -4.2358093 -4.2381158 -4.2357039 -4.2339678][-4.205369 -4.2070508 -4.2093005 -4.2085557 -4.2071567 -4.2086492 -4.210341 -4.2088928 -4.2090945 -4.2106328 -4.2113638 -4.2122717 -4.2109537 -4.2057409 -4.2043772][-4.1865611 -4.1858358 -4.1849856 -4.1831179 -4.1820126 -4.1823483 -4.182725 -4.1821961 -4.1832767 -4.1841879 -4.1845617 -4.1854906 -4.1849756 -4.1817808 -4.1821752][-4.214962 -4.2131419 -4.2106404 -4.2085395 -4.2082038 -4.2086272 -4.2088571 -4.2091742 -4.2098742 -4.2088966 -4.2076178 -4.2072878 -4.2065477 -4.2049885 -4.2060156]]...]
INFO - root - 2017-12-05 15:37:33.884157: step 22910, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 83h:10m:23s remains)
INFO - root - 2017-12-05 15:37:43.371368: step 22920, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 85h:59m:28s remains)
INFO - root - 2017-12-05 15:37:52.947990: step 22930, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.029 sec/batch; 88h:31m:02s remains)
INFO - root - 2017-12-05 15:38:02.376826: step 22940, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 83h:34m:23s remains)
INFO - root - 2017-12-05 15:38:11.662779: step 22950, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 85h:07m:44s remains)
INFO - root - 2017-12-05 15:38:21.111674: step 22960, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.973 sec/batch; 83h:40m:27s remains)
INFO - root - 2017-12-05 15:38:30.609295: step 22970, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.884 sec/batch; 75h:59m:46s remains)
INFO - root - 2017-12-05 15:38:39.876818: step 22980, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 81h:49m:48s remains)
INFO - root - 2017-12-05 15:38:48.772758: step 22990, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 56h:08m:00s remains)
INFO - root - 2017-12-05 15:38:58.190139: step 23000, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 79h:38m:42s remains)
2017-12-05 15:38:58.908991: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2062697 -4.215971 -4.2073722 -4.1739006 -4.1296463 -4.1021519 -4.1031342 -4.113688 -4.12235 -4.1383071 -4.1603613 -4.1620283 -4.1483049 -4.1428046 -4.1460123][-4.2225432 -4.231318 -4.2264132 -4.1947517 -4.1507764 -4.1142654 -4.1019192 -4.1171494 -4.1460571 -4.1777267 -4.2031765 -4.2047176 -4.1882548 -4.164999 -4.1501594][-4.2340612 -4.2460661 -4.2469296 -4.2200046 -4.178371 -4.1402388 -4.1259656 -4.1433506 -4.1770387 -4.210309 -4.2351923 -4.24159 -4.2276716 -4.1921573 -4.1598988][-4.2293167 -4.2458673 -4.2493691 -4.2287869 -4.1936693 -4.1592345 -4.1439729 -4.157774 -4.1845384 -4.214982 -4.2405686 -4.2554126 -4.2506423 -4.2208962 -4.1828103][-4.223948 -4.2396817 -4.2385044 -4.2199569 -4.1885872 -4.1580181 -4.140811 -4.1465735 -4.1645622 -4.1911731 -4.2216496 -4.249742 -4.2560058 -4.2407107 -4.2126684][-4.2167091 -4.2246032 -4.2164 -4.1986465 -4.1742363 -4.151794 -4.1388888 -4.1383748 -4.1455507 -4.1656513 -4.197556 -4.2314172 -4.2420397 -4.2389503 -4.2286334][-4.213028 -4.2059727 -4.1918178 -4.1747885 -4.1580939 -4.1439 -4.1374989 -4.1361856 -4.1413875 -4.1567621 -4.1861091 -4.2142305 -4.2216854 -4.2248359 -4.2265782][-4.1958461 -4.1786346 -4.1635776 -4.150147 -4.1358242 -4.1246719 -4.124557 -4.1300697 -4.1390042 -4.1516027 -4.1707873 -4.1837106 -4.1863351 -4.192925 -4.1989021][-4.1918635 -4.1670837 -4.1473703 -4.1341672 -4.1150179 -4.0946188 -4.0987325 -4.1197219 -4.1453333 -4.16157 -4.1741524 -4.1760864 -4.1746716 -4.1791215 -4.1829734][-4.1853704 -4.15687 -4.1372013 -4.1224232 -4.0998783 -4.0766516 -4.0825214 -4.1193414 -4.1651492 -4.1933279 -4.2088218 -4.2092075 -4.2096281 -4.2091246 -4.2051125][-4.1598959 -4.1351562 -4.1316152 -4.1320543 -4.11922 -4.0971951 -4.0980535 -4.1351538 -4.1856613 -4.2192168 -4.2388697 -4.2453394 -4.2516785 -4.2527819 -4.2442765][-4.1608038 -4.1351485 -4.1419535 -4.16244 -4.171175 -4.1585536 -4.1556616 -4.1756735 -4.2049651 -4.2246585 -4.2402515 -4.2531862 -4.2672892 -4.2740927 -4.2693276][-4.1479115 -4.119637 -4.1278133 -4.1619177 -4.1888065 -4.1904964 -4.1907272 -4.1966743 -4.2079649 -4.214994 -4.2226334 -4.2325006 -4.2488861 -4.2606468 -4.2662292][-4.1269026 -4.0940161 -4.0960674 -4.1312971 -4.1653323 -4.1773748 -4.1834674 -4.1874371 -4.1973886 -4.2054191 -4.2114067 -4.2150469 -4.2271147 -4.2452826 -4.262496][-4.0823541 -4.0504513 -4.0570979 -4.1027513 -4.1506443 -4.1731124 -4.1804371 -4.1796679 -4.1874228 -4.1958494 -4.2016287 -4.2018466 -4.2089725 -4.2261271 -4.2468076]]...]
INFO - root - 2017-12-05 15:39:08.165400: step 23010, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 82h:42m:21s remains)
INFO - root - 2017-12-05 15:39:17.496946: step 23020, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 83h:58m:50s remains)
INFO - root - 2017-12-05 15:39:27.047495: step 23030, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 84h:07m:58s remains)
INFO - root - 2017-12-05 15:39:36.579807: step 23040, loss = 2.03, batch loss = 1.98 (8.4 examples/sec; 0.955 sec/batch; 82h:03m:05s remains)
INFO - root - 2017-12-05 15:39:45.951168: step 23050, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 82h:59m:36s remains)
INFO - root - 2017-12-05 15:39:55.170598: step 23060, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 83h:40m:33s remains)
INFO - root - 2017-12-05 15:40:04.739596: step 23070, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 81h:38m:50s remains)
INFO - root - 2017-12-05 15:40:14.227470: step 23080, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 74h:10m:04s remains)
INFO - root - 2017-12-05 15:40:23.387808: step 23090, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 80h:16m:55s remains)
INFO - root - 2017-12-05 15:40:33.043389: step 23100, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 84h:01m:16s remains)
2017-12-05 15:40:33.809423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2089143 -4.2227297 -4.2470317 -4.2763176 -4.2997642 -4.3099174 -4.301477 -4.2749925 -4.2465568 -4.2453232 -4.2731 -4.3150086 -4.3465128 -4.3578434 -4.35762][-4.2282434 -4.2374239 -4.2574706 -4.2831731 -4.3060231 -4.3166132 -4.3098569 -4.28555 -4.2563143 -4.2530146 -4.2754388 -4.3129926 -4.3410115 -4.3500581 -4.3517294][-4.263895 -4.26507 -4.2788258 -4.2993789 -4.32023 -4.3314252 -4.3248768 -4.3003783 -4.2652273 -4.2532058 -4.2639604 -4.2936392 -4.3193212 -4.3314228 -4.3407989][-4.2953281 -4.2932506 -4.3024874 -4.317946 -4.3350921 -4.3427444 -4.3285103 -4.2959914 -4.25025 -4.2266383 -4.225461 -4.2482495 -4.2769303 -4.3001966 -4.3224049][-4.3032823 -4.3030457 -4.3115692 -4.3212557 -4.3281741 -4.3231359 -4.29434 -4.2430563 -4.182241 -4.1533661 -4.1524777 -4.1806254 -4.2229919 -4.2659049 -4.3043885][-4.2414284 -4.2498817 -4.2660041 -4.2777958 -4.2753806 -4.25378 -4.205687 -4.130558 -4.0574121 -4.0367 -4.0555592 -4.1086736 -4.1748252 -4.2407646 -4.2926269][-4.1218925 -4.1477685 -4.181046 -4.2005887 -4.1886325 -4.1480184 -4.0790839 -3.9800208 -3.9046583 -3.9121416 -3.9704778 -4.0599957 -4.1509986 -4.2312713 -4.2885537][-3.9926429 -4.051033 -4.1042976 -4.1288681 -4.1061878 -4.0509706 -3.9665215 -3.848875 -3.7821085 -3.8347335 -3.9384866 -4.0578604 -4.1622338 -4.2436113 -4.2992096][-3.9162571 -4.0024281 -4.0686622 -4.0919785 -4.0605822 -4.0008616 -3.9208074 -3.8117659 -3.7722964 -3.8580709 -3.9797864 -4.1020513 -4.1996493 -4.2721291 -4.3199048][-3.9573436 -4.0416808 -4.0999994 -4.1144571 -4.0786338 -4.027679 -3.9713364 -3.8992817 -3.884882 -3.9620876 -4.0642295 -4.16622 -4.2457294 -4.3037186 -4.3408675][-4.08284 -4.1362395 -4.16957 -4.1732941 -4.1447768 -4.1140928 -4.0853109 -4.0477185 -4.04538 -4.0957851 -4.1641269 -4.2349086 -4.2921619 -4.334352 -4.3609419][-4.212111 -4.235229 -4.2464051 -4.2427058 -4.2261786 -4.215066 -4.2065763 -4.1913767 -4.1930408 -4.2187061 -4.2555141 -4.2966347 -4.3325844 -4.3622136 -4.3799567][-4.291636 -4.2998791 -4.3019304 -4.2963305 -4.286984 -4.2858868 -4.2884431 -4.28551 -4.287457 -4.2978477 -4.3171091 -4.34109 -4.3629169 -4.3816876 -4.3910379][-4.328558 -4.332695 -4.3335938 -4.3296866 -4.3249564 -4.3267412 -4.3311415 -4.3324776 -4.334136 -4.339313 -4.3497405 -4.3629313 -4.3742213 -4.3839965 -4.3883352][-4.3493414 -4.3515511 -4.35267 -4.3506169 -4.3477349 -4.3479981 -4.3504996 -4.3531837 -4.3547277 -4.3575225 -4.3625388 -4.3689103 -4.3739152 -4.3782353 -4.3805132]]...]
INFO - root - 2017-12-05 15:40:43.191630: step 23110, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 77h:03m:36s remains)
INFO - root - 2017-12-05 15:40:52.543270: step 23120, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 82h:50m:23s remains)
INFO - root - 2017-12-05 15:41:02.076228: step 23130, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 81h:28m:03s remains)
INFO - root - 2017-12-05 15:41:11.441649: step 23140, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 79h:27m:09s remains)
INFO - root - 2017-12-05 15:41:20.842732: step 23150, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 83h:06m:12s remains)
INFO - root - 2017-12-05 15:41:30.004444: step 23160, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 83h:57m:11s remains)
INFO - root - 2017-12-05 15:41:39.374854: step 23170, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.959 sec/batch; 82h:22m:38s remains)
INFO - root - 2017-12-05 15:41:48.798771: step 23180, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 80h:26m:28s remains)
INFO - root - 2017-12-05 15:41:58.134639: step 23190, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.997 sec/batch; 85h:41m:12s remains)
INFO - root - 2017-12-05 15:42:07.609668: step 23200, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 79h:05m:31s remains)
2017-12-05 15:42:08.421801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2472963 -4.2487273 -4.245275 -4.2377763 -4.2327161 -4.2384806 -4.2529335 -4.2622337 -4.2606454 -4.2492175 -4.2375088 -4.2323084 -4.2314053 -4.2406592 -4.2614212][-4.262825 -4.2664967 -4.2645049 -4.2575526 -4.2505188 -4.2514238 -4.2588005 -4.2641325 -4.263689 -4.2579565 -4.2565656 -4.2606897 -4.2600436 -4.2626805 -4.2746816][-4.2806463 -4.2815537 -4.2754803 -4.2641182 -4.2481632 -4.2350273 -4.2289338 -4.2246022 -4.2254467 -4.2314844 -4.2430129 -4.2584786 -4.2636395 -4.2675257 -4.2762504][-4.2838535 -4.2747517 -4.2618203 -4.2449436 -4.2206287 -4.1920962 -4.1657729 -4.1421385 -4.1350327 -4.1528831 -4.180799 -4.2100558 -4.2265568 -4.2395606 -4.2531996][-4.2636571 -4.2412977 -4.2203703 -4.1974225 -4.1659346 -4.1249738 -4.0748439 -4.0215859 -3.9968612 -4.02019 -4.0697579 -4.1242957 -4.1632481 -4.1900177 -4.2117004][-4.2241135 -4.1915927 -4.16685 -4.1410284 -4.1060405 -4.0550184 -3.9800725 -3.895997 -3.846622 -3.8690925 -3.941932 -4.0323253 -4.1016312 -4.146255 -4.1779189][-4.170948 -4.1328306 -4.1097608 -4.0893846 -4.0599751 -4.0045753 -3.9198945 -3.825824 -3.7666836 -3.7939491 -3.8829019 -3.9964502 -4.0836215 -4.1388617 -4.1742363][-4.1321626 -4.0933065 -4.0710974 -4.0614209 -4.0442185 -3.9968624 -3.92705 -3.8514121 -3.8092356 -3.8457322 -3.9357042 -4.0450807 -4.1289253 -4.179718 -4.2082124][-4.1334138 -4.098506 -4.0767593 -4.0727916 -4.0656228 -4.0314856 -3.9864378 -3.9434056 -3.9300923 -3.9693336 -4.045485 -4.1307273 -4.1946969 -4.2341409 -4.2547169][-4.1710114 -4.1439743 -4.1264524 -4.1236677 -4.1197996 -4.0965834 -4.07168 -4.058784 -4.0660534 -4.099473 -4.1527944 -4.2119761 -4.2571135 -4.2835383 -4.2961946][-4.2359071 -4.2182312 -4.2033296 -4.1982036 -4.1905618 -4.1726813 -4.1597204 -4.1645455 -4.1792574 -4.2029629 -4.2362504 -4.2730079 -4.2990518 -4.3124685 -4.3180327][-4.2928886 -4.2842631 -4.2701459 -4.2576303 -4.2432885 -4.2286897 -4.2237997 -4.2360172 -4.2509246 -4.2653074 -4.2818542 -4.2982469 -4.308639 -4.313252 -4.3177543][-4.3244905 -4.3228607 -4.3100147 -4.2923675 -4.2732339 -4.2599778 -4.260541 -4.2757821 -4.2910089 -4.2980289 -4.3020124 -4.3049273 -4.3044386 -4.3023529 -4.3080554][-4.3315463 -4.3344026 -4.3238406 -4.3019757 -4.2772279 -4.26465 -4.2699504 -4.2870321 -4.3014512 -4.3051267 -4.3040905 -4.3005962 -4.2939334 -4.2883849 -4.2961473][-4.3123975 -4.3127632 -4.300055 -4.2753577 -4.2495756 -4.2423511 -4.255435 -4.2760334 -4.2920761 -4.296885 -4.2949324 -4.290277 -4.2821236 -4.2762194 -4.2849035]]...]
INFO - root - 2017-12-05 15:42:17.917518: step 23210, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 80h:32m:29s remains)
INFO - root - 2017-12-05 15:42:27.271261: step 23220, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.936 sec/batch; 80h:22m:48s remains)
INFO - root - 2017-12-05 15:42:36.645866: step 23230, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 79h:13m:54s remains)
INFO - root - 2017-12-05 15:42:46.112770: step 23240, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 81h:14m:31s remains)
INFO - root - 2017-12-05 15:42:55.535792: step 23250, loss = 2.02, batch loss = 1.96 (8.6 examples/sec; 0.931 sec/batch; 79h:58m:56s remains)
INFO - root - 2017-12-05 15:43:04.917665: step 23260, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 81h:24m:14s remains)
INFO - root - 2017-12-05 15:43:14.169906: step 23270, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 82h:17m:36s remains)
INFO - root - 2017-12-05 15:43:23.576525: step 23280, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 82h:10m:29s remains)
INFO - root - 2017-12-05 15:43:32.814768: step 23290, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 79h:01m:33s remains)
INFO - root - 2017-12-05 15:43:42.165857: step 23300, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 79h:49m:32s remains)
2017-12-05 15:43:42.893955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2947941 -4.2861238 -4.2806854 -4.2815228 -4.2836118 -4.2843108 -4.2884407 -4.2929473 -4.2919879 -4.2892137 -4.2874446 -4.2900667 -4.2950621 -4.3004069 -4.3071871][-4.278574 -4.2624507 -4.2515254 -4.2511883 -4.2564845 -4.2601991 -4.2662978 -4.275465 -4.2745104 -4.2687483 -4.2597442 -4.2604437 -4.2690892 -4.2806697 -4.2944717][-4.256289 -4.2282529 -4.2091241 -4.2057166 -4.2155175 -4.2239094 -4.2303166 -4.2417212 -4.2408662 -4.2312355 -4.212595 -4.2090526 -4.2215919 -4.23963 -4.2613115][-4.2263007 -4.1788006 -4.1459389 -4.1421752 -4.1606359 -4.178112 -4.1859779 -4.1954393 -4.1932826 -4.1846333 -4.1601286 -4.1525016 -4.1691513 -4.1907749 -4.2197189][-4.19287 -4.1233873 -4.074255 -4.065722 -4.09411 -4.1216307 -4.131927 -4.1403441 -4.1339936 -4.1249595 -4.0950069 -4.0880408 -4.1167359 -4.1465411 -4.1824536][-4.1490774 -4.0689216 -4.0078487 -3.9928637 -4.0225372 -4.0497093 -4.0570803 -4.0542326 -4.0336375 -4.0231581 -3.9999828 -4.0064826 -4.0573192 -4.1053867 -4.1537561][-4.111095 -4.0368414 -3.9737127 -3.9438429 -3.9560103 -3.9618616 -3.9542329 -3.9331968 -3.8977027 -3.8961148 -3.9043059 -3.9455793 -4.0208387 -4.0823331 -4.1401982][-4.1022959 -4.0449243 -3.9900839 -3.9476879 -3.939944 -3.9203446 -3.8915567 -3.8494778 -3.7973828 -3.8035419 -3.8449731 -3.9196768 -4.0064468 -4.072855 -4.1369538][-4.12663 -4.090621 -4.0543551 -4.0150113 -4.0036068 -3.979285 -3.9535015 -3.9169934 -3.866178 -3.863924 -3.8949842 -3.956553 -4.0210514 -4.0714455 -4.1333218][-4.1738472 -4.154573 -4.1354413 -4.1093626 -4.0923352 -4.0686536 -4.0529819 -4.0235057 -3.9826305 -3.9717281 -3.9816976 -4.0196905 -4.0591011 -4.0912933 -4.1454678][-4.229959 -4.2209463 -4.2111769 -4.1989026 -4.1782532 -4.1561041 -4.144074 -4.1155472 -4.0819287 -4.0706253 -4.06724 -4.086504 -4.110558 -4.1338396 -4.1800852][-4.2669687 -4.2628145 -4.2601728 -4.2556753 -4.2362332 -4.2202263 -4.2120485 -4.1896729 -4.1669292 -4.1581421 -4.1485038 -4.1602869 -4.1753259 -4.1911893 -4.22405][-4.2872233 -4.2852125 -4.2838807 -4.2831306 -4.2733784 -4.2682843 -4.2683868 -4.2564168 -4.2425404 -4.2339544 -4.2237754 -4.2285786 -4.2350745 -4.2445655 -4.262557][-4.295536 -4.2940912 -4.2929139 -4.294313 -4.2919445 -4.2942829 -4.30072 -4.2969303 -4.2902732 -4.2833538 -4.2756133 -4.277185 -4.2772274 -4.2791848 -4.287148][-4.3048873 -4.304801 -4.3040395 -4.3040638 -4.3027277 -4.3042269 -4.3082862 -4.3101139 -4.3101654 -4.3073111 -4.3027091 -4.3017454 -4.299 -4.2982974 -4.3027344]]...]
INFO - root - 2017-12-05 15:43:52.280002: step 23310, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 82h:43m:55s remains)
INFO - root - 2017-12-05 15:44:01.719984: step 23320, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.937 sec/batch; 80h:29m:21s remains)
INFO - root - 2017-12-05 15:44:11.126719: step 23330, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 80h:37m:33s remains)
INFO - root - 2017-12-05 15:44:20.612532: step 23340, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 81h:31m:53s remains)
INFO - root - 2017-12-05 15:44:30.172784: step 23350, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 79h:02m:12s remains)
INFO - root - 2017-12-05 15:44:39.575417: step 23360, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 82h:50m:58s remains)
INFO - root - 2017-12-05 15:44:49.039746: step 23370, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 78h:25m:22s remains)
INFO - root - 2017-12-05 15:44:58.106445: step 23380, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 79h:46m:46s remains)
INFO - root - 2017-12-05 15:45:07.469033: step 23390, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 81h:33m:45s remains)
INFO - root - 2017-12-05 15:45:16.882369: step 23400, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 80h:14m:20s remains)
2017-12-05 15:45:17.610449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.290432 -4.2653885 -4.2380424 -4.2221513 -4.2232022 -4.2291331 -4.2366886 -4.236105 -4.2289524 -4.2201357 -4.2179351 -4.2244539 -4.2344785 -4.2506962 -4.2748089][-4.2981973 -4.2763362 -4.2557354 -4.2406354 -4.2347269 -4.2350931 -4.24079 -4.2453866 -4.2489285 -4.2518821 -4.253376 -4.2559395 -4.2569108 -4.2656193 -4.2854095][-4.2997055 -4.2781258 -4.2575631 -4.2374539 -4.2175322 -4.2054667 -4.2064652 -4.216639 -4.2323017 -4.2523541 -4.26298 -4.2692962 -4.2707682 -4.2767105 -4.2933064][-4.2891474 -4.2633214 -4.2374649 -4.2111278 -4.1766434 -4.1472268 -4.1421437 -4.1627669 -4.1962862 -4.2339711 -4.2558265 -4.2709246 -4.27962 -4.2863359 -4.2999158][-4.2711973 -4.2369289 -4.2007284 -4.1677718 -4.1192069 -4.0680289 -4.0538349 -4.0907969 -4.1502447 -4.2088437 -4.2414632 -4.2637014 -4.2783146 -4.2896161 -4.3024726][-4.2436109 -4.2010636 -4.154839 -4.1127048 -4.05159 -3.9747663 -3.9392691 -3.9900446 -4.0773864 -4.1606045 -4.2104697 -4.2412605 -4.2619886 -4.2803435 -4.2962203][-4.2202539 -4.1764326 -4.1277561 -4.0764513 -4.003511 -3.9050648 -3.8361368 -3.8915427 -3.998075 -4.1006761 -4.1672478 -4.2098479 -4.2405438 -4.2644711 -4.283915][-4.2220335 -4.1828938 -4.1360559 -4.0838079 -4.0151505 -3.9279675 -3.8598263 -3.8992877 -3.9827294 -4.068584 -4.1365895 -4.1863837 -4.2251139 -4.2524734 -4.2739968][-4.2484355 -4.2155542 -4.1723824 -4.1288595 -4.0844316 -4.0388737 -4.0022178 -4.0154333 -4.0486364 -4.0936995 -4.143445 -4.1889567 -4.2269049 -4.2528577 -4.2733135][-4.2726173 -4.2472706 -4.2105083 -4.1776381 -4.1576204 -4.1435013 -4.1250272 -4.1187358 -4.1169038 -4.1326432 -4.1686053 -4.21038 -4.243381 -4.2640705 -4.281292][-4.2775855 -4.2562876 -4.229444 -4.2105012 -4.2048526 -4.2031832 -4.19378 -4.178647 -4.158854 -4.1614304 -4.1927495 -4.2327356 -4.2614346 -4.2776203 -4.2918739][-4.2700167 -4.2489667 -4.231935 -4.2241817 -4.2285867 -4.2336283 -4.2303734 -4.2118988 -4.1860056 -4.1829629 -4.2095413 -4.2458334 -4.2713647 -4.2871275 -4.2995815][-4.2486315 -4.224843 -4.2166791 -4.2174163 -4.2249541 -4.2337117 -4.2319946 -4.2162514 -4.1955891 -4.1912689 -4.212513 -4.2468681 -4.2715163 -4.2883253 -4.3006225][-4.2174668 -4.1905732 -4.1868777 -4.1943121 -4.2040296 -4.2140059 -4.2157216 -4.2031193 -4.1855941 -4.1791897 -4.1929235 -4.2244182 -4.2514396 -4.27344 -4.2901974][-4.186625 -4.1501641 -4.1446605 -4.1550136 -4.1724162 -4.1894298 -4.1975155 -4.1882305 -4.1714163 -4.1642156 -4.16904 -4.1906757 -4.2190919 -4.2484 -4.2737312]]...]
INFO - root - 2017-12-05 15:45:27.054497: step 23410, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 79h:29m:01s remains)
INFO - root - 2017-12-05 15:45:36.390477: step 23420, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 76h:51m:34s remains)
INFO - root - 2017-12-05 15:45:45.779834: step 23430, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 77h:02m:13s remains)
INFO - root - 2017-12-05 15:45:55.302950: step 23440, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 83h:25m:48s remains)
INFO - root - 2017-12-05 15:46:04.759860: step 23450, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 80h:46m:04s remains)
INFO - root - 2017-12-05 15:46:14.180290: step 23460, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 78h:55m:39s remains)
INFO - root - 2017-12-05 15:46:23.615256: step 23470, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 80h:30m:42s remains)
INFO - root - 2017-12-05 15:46:33.015689: step 23480, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 83h:19m:37s remains)
INFO - root - 2017-12-05 15:46:42.436028: step 23490, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 75h:47m:22s remains)
INFO - root - 2017-12-05 15:46:51.653416: step 23500, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 78h:20m:03s remains)
2017-12-05 15:46:52.418241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22248 -4.2319722 -4.2308884 -4.22182 -4.2028112 -4.1971421 -4.196002 -4.1966805 -4.1880531 -4.1687112 -4.165257 -4.1776528 -4.1861911 -4.1933494 -4.2166519][-4.1980205 -4.2126403 -4.2119441 -4.2019243 -4.178648 -4.1729221 -4.1741204 -4.1785846 -4.168262 -4.1446514 -4.1334934 -4.1480575 -4.1640053 -4.1729445 -4.1955][-4.1763334 -4.1895061 -4.183125 -4.1731362 -4.15339 -4.15137 -4.155375 -4.1680145 -4.1653147 -4.1436019 -4.1237092 -4.133769 -4.1539416 -4.1647296 -4.1826577][-4.1628795 -4.1703691 -4.1588259 -4.143724 -4.1235695 -4.1178293 -4.1177459 -4.1415143 -4.1626534 -4.1547446 -4.1248693 -4.1204572 -4.1370449 -4.1525888 -4.1689534][-4.184691 -4.1851997 -4.17169 -4.1514921 -4.1219459 -4.095252 -4.0692172 -4.09262 -4.1498375 -4.1688175 -4.1391778 -4.1201153 -4.1263547 -4.143784 -4.1609049][-4.2111545 -4.2081985 -4.19527 -4.172369 -4.1328926 -4.0792942 -4.0073357 -4.0073915 -4.101315 -4.1664562 -4.156796 -4.1340528 -4.130116 -4.142849 -4.1586695][-4.1965275 -4.1993051 -4.1953964 -4.1778183 -4.1388512 -4.0669641 -3.9444361 -3.8981442 -4.0172186 -4.1329169 -4.1587205 -4.1473846 -4.14025 -4.1488624 -4.1631322][-4.1721344 -4.1773906 -4.1831517 -4.1750665 -4.1500473 -4.0919247 -3.9649525 -3.8882589 -3.9913774 -4.1225023 -4.1690521 -4.168293 -4.162097 -4.1672926 -4.1782255][-4.1695642 -4.1732287 -4.1863871 -4.1884685 -4.1865544 -4.1629682 -4.0773067 -4.011786 -4.0635576 -4.1573324 -4.1944623 -4.1947522 -4.1894622 -4.1922011 -4.1998272][-4.1875982 -4.1880374 -4.2018013 -4.2055092 -4.2129583 -4.2143784 -4.1718283 -4.1354904 -4.1530919 -4.2028303 -4.2183895 -4.2163572 -4.2136316 -4.2164598 -4.2222056][-4.2040834 -4.199831 -4.2133527 -4.2181716 -4.2269869 -4.2406807 -4.22654 -4.2130985 -4.214016 -4.2312274 -4.2280169 -4.2247667 -4.2290945 -4.2357035 -4.2438741][-4.2205539 -4.2137976 -4.2261791 -4.2335696 -4.24137 -4.2560229 -4.2552567 -4.251956 -4.2471919 -4.2479963 -4.2355461 -4.2324462 -4.2397571 -4.2468057 -4.2577567][-4.2503648 -4.2468667 -4.2545009 -4.2618942 -4.2647467 -4.2721057 -4.2736769 -4.2731009 -4.2699695 -4.266552 -4.2534614 -4.2524323 -4.25732 -4.2607818 -4.2717934][-4.274662 -4.2753687 -4.2820396 -4.2862449 -4.2845416 -4.2862754 -4.2874022 -4.2883053 -4.2880158 -4.2865582 -4.2783327 -4.2787013 -4.2805443 -4.2789321 -4.2865224][-4.285584 -4.28794 -4.2940493 -4.2971716 -4.2930155 -4.291563 -4.2932596 -4.2969108 -4.2987828 -4.2973995 -4.2949848 -4.2986155 -4.2995033 -4.2960129 -4.3005276]]...]
INFO - root - 2017-12-05 15:47:01.897172: step 23510, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 80h:13m:39s remains)
INFO - root - 2017-12-05 15:47:11.258789: step 23520, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 77h:26m:23s remains)
INFO - root - 2017-12-05 15:47:20.553929: step 23530, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.008 sec/batch; 86h:29m:13s remains)
INFO - root - 2017-12-05 15:47:29.928235: step 23540, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 81h:13m:01s remains)
INFO - root - 2017-12-05 15:47:39.236107: step 23550, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 83h:46m:40s remains)
INFO - root - 2017-12-05 15:47:48.643118: step 23560, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 80h:40m:45s remains)
INFO - root - 2017-12-05 15:47:57.996816: step 23570, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 81h:16m:55s remains)
INFO - root - 2017-12-05 15:48:07.088175: step 23580, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 80h:06m:55s remains)
INFO - root - 2017-12-05 15:48:16.472893: step 23590, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 84h:38m:48s remains)
INFO - root - 2017-12-05 15:48:25.975913: step 23600, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 74h:22m:20s remains)
2017-12-05 15:48:26.801407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2457538 -4.24364 -4.2439685 -4.2471466 -4.2515435 -4.253891 -4.2528348 -4.24604 -4.2289672 -4.2119408 -4.2021556 -4.1952138 -4.1965976 -4.2064891 -4.2109904][-4.2471581 -4.2425385 -4.2403874 -4.2434883 -4.2477207 -4.2498035 -4.2478967 -4.23712 -4.2144222 -4.196506 -4.189621 -4.1883097 -4.1964011 -4.2132716 -4.2189012][-4.24998 -4.23978 -4.2320504 -4.2323494 -4.2349253 -4.2358179 -4.2314491 -4.2176743 -4.1938949 -4.179553 -4.1800957 -4.1881986 -4.2049913 -4.22455 -4.2298088][-4.2515039 -4.2372184 -4.2239718 -4.21766 -4.21445 -4.2081938 -4.1988115 -4.1836591 -4.1626434 -4.1564069 -4.1665034 -4.1837888 -4.2062292 -4.2265787 -4.2320251][-4.2472148 -4.2304807 -4.2122197 -4.1963935 -4.1824288 -4.1667585 -4.1486988 -4.1289129 -4.1114545 -4.1157832 -4.1371603 -4.1641526 -4.1952391 -4.2209845 -4.2296247][-4.2413707 -4.2215953 -4.1963391 -4.1692648 -4.1422877 -4.1128526 -4.0771341 -4.0453386 -4.0331931 -4.05473 -4.0924983 -4.1333432 -4.1778388 -4.2138448 -4.2278666][-4.2381105 -4.2132154 -4.1792469 -4.1414175 -4.1020813 -4.055398 -3.997117 -3.948071 -3.9431565 -3.989289 -4.0489712 -4.1061993 -4.1641765 -4.2099385 -4.2284875][-4.23667 -4.2101226 -4.1740913 -4.1333542 -4.0887222 -4.0330229 -3.9641159 -3.9070072 -3.905838 -3.9677417 -4.040885 -4.103878 -4.1640143 -4.2110586 -4.2298017][-4.238924 -4.2141318 -4.1800036 -4.1410184 -4.0997944 -4.0520492 -3.9985793 -3.9542842 -3.9532645 -4.0089555 -4.0756865 -4.131074 -4.183238 -4.22468 -4.239171][-4.24593 -4.2240348 -4.193017 -4.1555591 -4.1214638 -4.0879869 -4.0572462 -4.0326934 -4.0316119 -4.0741787 -4.1272721 -4.17113 -4.2134252 -4.2467556 -4.2557817][-4.2540655 -4.236743 -4.2119918 -4.1785378 -4.1500082 -4.1276083 -4.1102424 -4.0980773 -4.0988126 -4.1304927 -4.1706386 -4.2029977 -4.23598 -4.2630773 -4.26759][-4.2511983 -4.2379413 -4.2204638 -4.1942248 -4.1716051 -4.1561613 -4.1449661 -4.1367888 -4.1388474 -4.1636863 -4.1941776 -4.2183495 -4.2443638 -4.2666759 -4.2701626][-4.2424278 -4.2301373 -4.2173228 -4.1979327 -4.1818786 -4.1706357 -4.1611395 -4.1553593 -4.1581178 -4.1769571 -4.2002897 -4.2189918 -4.2401686 -4.2584338 -4.2628455][-4.2303982 -4.2186203 -4.2077427 -4.1924739 -4.1814013 -4.1722374 -4.1634917 -4.1595287 -4.1621814 -4.1771016 -4.1961641 -4.21207 -4.230454 -4.2463579 -4.2524371][-4.2216439 -4.2111187 -4.2024932 -4.1916437 -4.1835079 -4.1767926 -4.1707854 -4.1687822 -4.1713343 -4.1828971 -4.1975574 -4.2104564 -4.2256651 -4.2394366 -4.2456751]]...]
INFO - root - 2017-12-05 15:48:36.314309: step 23610, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 85h:30m:06s remains)
INFO - root - 2017-12-05 15:48:45.538907: step 23620, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 82h:16m:36s remains)
INFO - root - 2017-12-05 15:48:54.906979: step 23630, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 83h:45m:55s remains)
INFO - root - 2017-12-05 15:49:04.414540: step 23640, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 81h:38m:55s remains)
INFO - root - 2017-12-05 15:49:13.775938: step 23650, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 82h:16m:12s remains)
INFO - root - 2017-12-05 15:49:23.059744: step 23660, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.003 sec/batch; 86h:00m:31s remains)
INFO - root - 2017-12-05 15:49:32.245982: step 23670, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 0.799 sec/batch; 68h:32m:30s remains)
INFO - root - 2017-12-05 15:49:41.669251: step 23680, loss = 2.03, batch loss = 1.97 (8.3 examples/sec; 0.968 sec/batch; 83h:04m:03s remains)
INFO - root - 2017-12-05 15:49:51.086589: step 23690, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 83h:50m:17s remains)
INFO - root - 2017-12-05 15:50:00.582910: step 23700, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 81h:26m:05s remains)
2017-12-05 15:50:01.338936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1409473 -4.1250606 -4.0993485 -4.0707803 -4.0655122 -4.0932412 -4.1403565 -4.1855717 -4.2193022 -4.2276778 -4.2132564 -4.1814613 -4.1439228 -4.1147289 -4.1085963][-4.1640711 -4.1512213 -4.1250448 -4.095767 -4.0871921 -4.1068134 -4.1436267 -4.1790648 -4.2035928 -4.2078614 -4.1959867 -4.1738667 -4.1552858 -4.145236 -4.1483073][-4.1945338 -4.1864023 -4.1604657 -4.1274 -4.1103859 -4.1162195 -4.1379242 -4.1611915 -4.1776848 -4.1800466 -4.1729059 -4.162065 -4.1583991 -4.1646004 -4.179069][-4.2279677 -4.2219748 -4.1956534 -4.1570883 -4.1277261 -4.1158247 -4.1220708 -4.1346779 -4.143374 -4.1449852 -4.1443477 -4.1441255 -4.1502795 -4.1637788 -4.1829643][-4.2544522 -4.2450538 -4.2114015 -4.1617765 -4.1218753 -4.1015463 -4.1018715 -4.1058593 -4.1045418 -4.1056685 -4.1103282 -4.1178503 -4.1299152 -4.1473103 -4.1690249][-4.2607083 -4.2380013 -4.1892605 -4.1241665 -4.0738068 -4.0508242 -4.0513387 -4.053834 -4.0516362 -4.0567255 -4.0700045 -4.0909076 -4.1185808 -4.1463666 -4.1698895][-4.2410903 -4.2073994 -4.1487575 -4.0767779 -4.0226994 -3.9988379 -3.9973927 -4.0001922 -3.9957628 -3.9979513 -4.0216374 -4.0648422 -4.1128621 -4.150095 -4.1741848][-4.2149582 -4.1778545 -4.1192102 -4.0486264 -3.9943264 -3.9671259 -3.9578817 -3.952106 -3.9306431 -3.9122694 -3.9448483 -4.0149751 -4.0835652 -4.1281796 -4.1534524][-4.1943817 -4.1589117 -4.1068869 -4.0444322 -3.9980798 -3.9754224 -3.9646969 -3.9509642 -3.9158864 -3.8809915 -3.9112291 -3.9861991 -4.05651 -4.1010723 -4.1276894][-4.1809068 -4.1500878 -4.1070881 -4.0593143 -4.0315194 -4.0249634 -4.0218682 -4.0111046 -3.9817853 -3.9487977 -3.9630282 -4.0137844 -4.063674 -4.098083 -4.1199093][-4.1712379 -4.1460171 -4.1136079 -4.0806675 -4.0673203 -4.072772 -4.0775743 -4.070991 -4.0430179 -4.0121431 -4.015718 -4.0473013 -4.081367 -4.1086817 -4.1316648][-4.1716924 -4.1551476 -4.1341081 -4.1127439 -4.1056666 -4.1091056 -4.1099329 -4.1017804 -4.0732102 -4.0447721 -4.0474367 -4.0751767 -4.1035314 -4.1284571 -4.1495037][-4.1768785 -4.1740108 -4.168222 -4.1595922 -4.155601 -4.1505475 -4.1396456 -4.1225882 -4.09089 -4.0631914 -4.0687261 -4.0967851 -4.1238356 -4.1523972 -4.1731358][-4.1869516 -4.1949725 -4.2027555 -4.2045732 -4.2022095 -4.1880736 -4.1635652 -4.1340122 -4.095922 -4.0709062 -4.0793896 -4.1059604 -4.1348433 -4.1685057 -4.1927047][-4.2068467 -4.2197642 -4.2344713 -4.2434015 -4.2426724 -4.2233696 -4.1915636 -4.1551461 -4.1157947 -4.09324 -4.0995431 -4.1243954 -4.15506 -4.1861572 -4.2039895]]...]
INFO - root - 2017-12-05 15:50:10.672526: step 23710, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 84h:40m:52s remains)
INFO - root - 2017-12-05 15:50:19.993376: step 23720, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 73h:46m:08s remains)
INFO - root - 2017-12-05 15:50:29.484551: step 23730, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.965 sec/batch; 82h:48m:00s remains)
INFO - root - 2017-12-05 15:50:38.842217: step 23740, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 81h:08m:29s remains)
INFO - root - 2017-12-05 15:50:48.248302: step 23750, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 80h:28m:54s remains)
INFO - root - 2017-12-05 15:50:57.683583: step 23760, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 71h:36m:12s remains)
INFO - root - 2017-12-05 15:51:06.849906: step 23770, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 80h:15m:28s remains)
INFO - root - 2017-12-05 15:51:16.317190: step 23780, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 82h:31m:08s remains)
INFO - root - 2017-12-05 15:51:25.802717: step 23790, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 79h:00m:14s remains)
INFO - root - 2017-12-05 15:51:35.306252: step 23800, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 79h:19m:27s remains)
2017-12-05 15:51:36.063041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2547269 -4.2435594 -4.2354822 -4.2268939 -4.2159276 -4.2106414 -4.2107139 -4.2144842 -4.217649 -4.219933 -4.2211995 -4.2167735 -4.2059112 -4.1989255 -4.1985831][-4.2486978 -4.237196 -4.2241364 -4.2078638 -4.1883907 -4.1770854 -4.174614 -4.1841612 -4.1982527 -4.2112923 -4.2211294 -4.2258034 -4.2235088 -4.219934 -4.2197971][-4.2364726 -4.2275648 -4.2116761 -4.1886497 -4.1615229 -4.1420722 -4.1354971 -4.1498165 -4.1756749 -4.2014871 -4.2233267 -4.239327 -4.2470765 -4.2478476 -4.2463551][-4.2276621 -4.2221341 -4.2078938 -4.1826758 -4.1511931 -4.1243186 -4.1122494 -4.1269951 -4.159534 -4.1967435 -4.2299037 -4.2569122 -4.2734957 -4.2796516 -4.2762585][-4.2090735 -4.2103934 -4.2013431 -4.1761727 -4.1416917 -4.108357 -4.0905657 -4.1015272 -4.1364632 -4.1828332 -4.2236805 -4.2563171 -4.2754936 -4.2844238 -4.2800922][-4.1661415 -4.1801009 -4.1812916 -4.1607585 -4.1244082 -4.0847678 -4.0618863 -4.0650873 -4.0970907 -4.14646 -4.1935678 -4.229188 -4.2476063 -4.2558613 -4.2542043][-4.092207 -4.1199603 -4.1376171 -4.1325259 -4.1027203 -4.0612922 -4.0365376 -4.0324364 -4.0594277 -4.1051817 -4.1536331 -4.1908288 -4.2078671 -4.2156663 -4.2195597][-4.0404773 -4.0744624 -4.1059632 -4.1142387 -4.0976057 -4.0651531 -4.0426054 -4.0337782 -4.0547194 -4.0954261 -4.1408238 -4.1754231 -4.1922283 -4.2019057 -4.2118812][-4.061944 -4.08916 -4.1238732 -4.138371 -4.13186 -4.1096511 -4.0897751 -4.0783038 -4.092988 -4.124383 -4.1593432 -4.1857858 -4.1979742 -4.2063642 -4.2191992][-4.1298032 -4.1442003 -4.1719942 -4.1873355 -4.1904855 -4.1785021 -4.1608019 -4.1461897 -4.1529784 -4.1714387 -4.1916828 -4.2054496 -4.2100158 -4.21446 -4.2256036][-4.1997461 -4.2015004 -4.2166624 -4.2275381 -4.2381439 -4.2377081 -4.22371 -4.2074466 -4.2059474 -4.2111816 -4.2189894 -4.2225332 -4.2210846 -4.2231059 -4.228159][-4.2509937 -4.2448716 -4.2497582 -4.2549844 -4.2652025 -4.2704916 -4.2637525 -4.2516336 -4.2464247 -4.2427025 -4.2425108 -4.2396755 -4.2332668 -4.2308712 -4.227869][-4.2792368 -4.2738419 -4.2757277 -4.27944 -4.2878423 -4.2970376 -4.2981315 -4.2923617 -4.2866211 -4.2773948 -4.2686424 -4.2577395 -4.2427249 -4.2318339 -4.2228036][-4.2706265 -4.2747 -4.285049 -4.2971683 -4.3099484 -4.3225513 -4.3280225 -4.3252616 -4.3165135 -4.3034644 -4.2914491 -4.2787423 -4.262289 -4.2486019 -4.2384534][-4.2414036 -4.2562518 -4.2787209 -4.300931 -4.3196855 -4.3345971 -4.339901 -4.3376203 -4.3273964 -4.314146 -4.3034811 -4.294425 -4.2821941 -4.2676992 -4.2563205]]...]
INFO - root - 2017-12-05 15:51:45.409952: step 23810, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 79h:14m:03s remains)
INFO - root - 2017-12-05 15:51:54.839193: step 23820, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 72h:18m:33s remains)
INFO - root - 2017-12-05 15:52:04.144068: step 23830, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 77h:24m:18s remains)
INFO - root - 2017-12-05 15:52:13.692956: step 23840, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 81h:01m:21s remains)
INFO - root - 2017-12-05 15:52:22.849484: step 23850, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 79h:08m:40s remains)
INFO - root - 2017-12-05 15:52:32.263129: step 23860, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 79h:29m:50s remains)
INFO - root - 2017-12-05 15:52:41.789268: step 23870, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 82h:17m:33s remains)
INFO - root - 2017-12-05 15:52:51.082068: step 23880, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 82h:24m:06s remains)
INFO - root - 2017-12-05 15:53:00.365028: step 23890, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 80h:00m:52s remains)
INFO - root - 2017-12-05 15:53:09.578909: step 23900, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 80h:03m:12s remains)
2017-12-05 15:53:10.407686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2862277 -4.28227 -4.2749972 -4.2693653 -4.27089 -4.2753582 -4.2792645 -4.2773147 -4.27068 -4.2679977 -4.2707372 -4.276247 -4.2835407 -4.2925143 -4.2979684][-4.2487979 -4.2381945 -4.2231374 -4.2171879 -4.223444 -4.231389 -4.2399211 -4.2381177 -4.2239676 -4.2231746 -4.2380838 -4.2572618 -4.2724109 -4.286447 -4.2942281][-4.214354 -4.19453 -4.1694069 -4.1591568 -4.1676326 -4.1757894 -4.1805854 -4.163425 -4.1257849 -4.1256857 -4.1631026 -4.2087545 -4.2422776 -4.2674909 -4.2818594][-4.2001567 -4.1737432 -4.1435056 -4.1304665 -4.1414604 -4.1471677 -4.1361322 -4.0820789 -4.0043545 -4.00565 -4.0734973 -4.1477909 -4.2028909 -4.2431078 -4.266151][-4.1891742 -4.1581011 -4.1281271 -4.1172791 -4.1332927 -4.1402016 -4.1215653 -4.0427594 -3.9367509 -3.9403639 -4.0326247 -4.1203756 -4.1830091 -4.2319803 -4.260396][-4.1697621 -4.1290255 -4.0992508 -4.0921321 -4.1108375 -4.1214733 -4.1060367 -4.0271788 -3.9124072 -3.915792 -4.0289268 -4.1204653 -4.1829109 -4.2337928 -4.2597704][-4.1448913 -4.0979276 -4.0720472 -4.0695615 -4.0799322 -4.0755773 -4.044435 -3.9487426 -3.8080034 -3.8162498 -3.9712255 -4.0839186 -4.1598377 -4.2237606 -4.2547326][-4.1400251 -4.0955415 -4.0759912 -4.0777516 -4.0743055 -4.0414362 -3.9737344 -3.8367362 -3.6585429 -3.6822803 -3.888051 -4.0342274 -4.1294613 -4.2054086 -4.2438321][-4.1607332 -4.1234293 -4.1123791 -4.120553 -4.1153688 -4.0686126 -3.9822075 -3.8377738 -3.6711373 -3.7072458 -3.9064612 -4.0434904 -4.1301856 -4.2000852 -4.2355351][-4.1840358 -4.1561217 -4.1557808 -4.1763744 -4.1791015 -4.1387072 -4.0689306 -3.9641576 -3.8547597 -3.8819323 -4.0243063 -4.1171112 -4.175755 -4.2251964 -4.2505693][-4.2010584 -4.1779714 -4.1777205 -4.2007308 -4.2097669 -4.1798682 -4.1264992 -4.0530252 -3.9809289 -3.9954691 -4.0992627 -4.1713905 -4.2179427 -4.2569623 -4.2740645][-4.2183642 -4.2050438 -4.2025757 -4.2171535 -4.2202849 -4.1906409 -4.1421962 -4.0816269 -4.0175881 -4.0204368 -4.1081691 -4.1814418 -4.23177 -4.2686591 -4.2843561][-4.2405276 -4.2368855 -4.2371869 -4.2458844 -4.2431812 -4.2161312 -4.1780119 -4.1304455 -4.0758314 -4.067811 -4.1318841 -4.1935225 -4.237258 -4.2706561 -4.2872486][-4.2683306 -4.2683921 -4.2706246 -4.2733426 -4.2691393 -4.2508068 -4.2270656 -4.1921616 -4.1458721 -4.1314826 -4.169179 -4.2093363 -4.2421179 -4.2725124 -4.2906117][-4.2938309 -4.2940679 -4.2958 -4.2926469 -4.2864728 -4.2744141 -4.2596226 -4.2322598 -4.1916823 -4.1741595 -4.1954184 -4.2216697 -4.2498178 -4.2783604 -4.2958832]]...]
INFO - root - 2017-12-05 15:53:19.729378: step 23910, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 80h:55m:44s remains)
INFO - root - 2017-12-05 15:53:28.955130: step 23920, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 81h:01m:13s remains)
INFO - root - 2017-12-05 15:53:38.353879: step 23930, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.887 sec/batch; 76h:02m:29s remains)
INFO - root - 2017-12-05 15:53:47.699050: step 23940, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.900 sec/batch; 77h:09m:03s remains)
INFO - root - 2017-12-05 15:53:57.077537: step 23950, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 81h:05m:21s remains)
INFO - root - 2017-12-05 15:54:06.532869: step 23960, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 82h:57m:17s remains)
INFO - root - 2017-12-05 15:54:15.799872: step 23970, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 82h:18m:37s remains)
INFO - root - 2017-12-05 15:54:25.088013: step 23980, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 84h:52m:08s remains)
INFO - root - 2017-12-05 15:54:34.552281: step 23990, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 73h:44m:52s remains)
INFO - root - 2017-12-05 15:54:44.000270: step 24000, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 80h:58m:39s remains)
2017-12-05 15:54:44.735900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2055187 -4.1965513 -4.1926937 -4.1956048 -4.2012162 -4.2039728 -4.1893806 -4.1613579 -4.1377439 -4.1412416 -4.1631207 -4.200583 -4.2332983 -4.2411432 -4.2051244][-4.2094893 -4.1930852 -4.1946115 -4.199295 -4.1975627 -4.1892352 -4.1713529 -4.1422167 -4.1254225 -4.1416893 -4.1713438 -4.2065983 -4.2319818 -4.2293248 -4.1929026][-4.1969357 -4.1729779 -4.1791949 -4.1861219 -4.1778913 -4.1614218 -4.144073 -4.1182556 -4.1063504 -4.1313095 -4.1698685 -4.205277 -4.2258358 -4.2229891 -4.1920781][-4.1642556 -4.1366186 -4.1520863 -4.1705704 -4.163548 -4.1396775 -4.1147728 -4.0854492 -4.07829 -4.1150131 -4.1624942 -4.2006459 -4.2196937 -4.2185831 -4.1932573][-4.1302795 -4.102531 -4.1315641 -4.1704245 -4.1729188 -4.1382918 -4.0918956 -4.0454044 -4.0432286 -4.1006327 -4.1613808 -4.2018991 -4.2191739 -4.2189856 -4.1963363][-4.1024361 -4.0778933 -4.1170917 -4.1733789 -4.1843119 -4.1350121 -4.0529742 -3.976418 -3.991652 -4.0845666 -4.165061 -4.20748 -4.2225261 -4.2229481 -4.2048635][-4.1091084 -4.0922194 -4.1326256 -4.1871371 -4.19131 -4.1240368 -4.0126457 -3.9125781 -3.9561365 -4.0779219 -4.1688828 -4.2070169 -4.2156754 -4.2165003 -4.2025084][-4.133667 -4.1350636 -4.1767793 -4.2172308 -4.2097836 -4.1416883 -4.0395093 -3.9556143 -4.0016866 -4.1031661 -4.1749024 -4.1988406 -4.2028465 -4.2095633 -4.2027321][-4.1495523 -4.1684241 -4.2104611 -4.2389207 -4.2286315 -4.1758332 -4.1043825 -4.0518651 -4.07926 -4.1365242 -4.1785278 -4.1918063 -4.1977243 -4.2117133 -4.2084122][-4.1600533 -4.1875086 -4.2285991 -4.2561545 -4.2482128 -4.2070146 -4.1536775 -4.1180992 -4.1305161 -4.1571841 -4.1765308 -4.1857 -4.198 -4.21317 -4.2045703][-4.1821184 -4.2093368 -4.2447329 -4.268671 -4.2667856 -4.2328629 -4.1880407 -4.16033 -4.1649032 -4.1774335 -4.18565 -4.1933451 -4.20604 -4.2158356 -4.2007427][-4.2086163 -4.2312465 -4.2575884 -4.27388 -4.2720919 -4.2439036 -4.2094355 -4.1917367 -4.1981483 -4.208334 -4.2114158 -4.2150912 -4.2206912 -4.2231908 -4.2050104][-4.2227688 -4.2399621 -4.2580895 -4.268312 -4.2653403 -4.2430878 -4.2182689 -4.2113214 -4.2215433 -4.233037 -4.2347832 -4.234973 -4.2339659 -4.2319989 -4.217485][-4.2392445 -4.2464247 -4.2549529 -4.2594647 -4.2540274 -4.2402091 -4.2290416 -4.2303534 -4.2412167 -4.2532444 -4.2565441 -4.2548032 -4.25192 -4.2489262 -4.23991][-4.2768054 -4.2769356 -4.27851 -4.2790022 -4.2735085 -4.2673941 -4.2677484 -4.2754602 -4.2847204 -4.2925353 -4.2940712 -4.2911363 -4.287756 -4.2843237 -4.2777243]]...]
INFO - root - 2017-12-05 15:54:54.173919: step 24010, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 74h:47m:15s remains)
INFO - root - 2017-12-05 15:55:03.556435: step 24020, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.954 sec/batch; 81h:45m:00s remains)
INFO - root - 2017-12-05 15:55:13.030637: step 24030, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 85h:33m:46s remains)
INFO - root - 2017-12-05 15:55:22.332435: step 24040, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 84h:19m:54s remains)
INFO - root - 2017-12-05 15:55:31.632324: step 24050, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 81h:10m:09s remains)
INFO - root - 2017-12-05 15:55:41.237382: step 24060, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 81h:01m:59s remains)
INFO - root - 2017-12-05 15:55:50.608906: step 24070, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 82h:00m:42s remains)
INFO - root - 2017-12-05 15:56:00.054008: step 24080, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 83h:27m:46s remains)
INFO - root - 2017-12-05 15:56:09.290826: step 24090, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 82h:02m:39s remains)
INFO - root - 2017-12-05 15:56:18.655629: step 24100, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 82h:09m:53s remains)
2017-12-05 15:56:19.425943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3136497 -4.3091207 -4.3067093 -4.30502 -4.31028 -4.3146167 -4.3079777 -4.2922049 -4.2852569 -4.2919283 -4.3060117 -4.3203149 -4.3298793 -4.3331556 -4.3248296][-4.3168454 -4.3159366 -4.3156009 -4.3145633 -4.3160043 -4.3101559 -4.2886314 -4.2590022 -4.2448106 -4.251883 -4.2705364 -4.29413 -4.3150864 -4.3266406 -4.3237538][-4.3141284 -4.3177004 -4.3191013 -4.3190613 -4.3165455 -4.2994528 -4.2603774 -4.2144337 -4.1915812 -4.1993833 -4.2248154 -4.2608871 -4.2973094 -4.3184805 -4.3199034][-4.30302 -4.3135548 -4.3182106 -4.3183942 -4.3097243 -4.2804914 -4.2238503 -4.1608171 -4.1283894 -4.1355615 -4.164968 -4.2134995 -4.2685971 -4.3027658 -4.3115869][-4.2796025 -4.3002691 -4.3090687 -4.3086753 -4.2943721 -4.2548604 -4.1841769 -4.1082277 -4.0685039 -4.0712452 -4.1012959 -4.1603379 -4.2321382 -4.27848 -4.2952943][-4.2406821 -4.2704768 -4.2836351 -4.283782 -4.2676625 -4.224297 -4.1505837 -4.0730581 -4.0329938 -4.0312486 -4.0584 -4.1184316 -4.1930757 -4.2415562 -4.2637806][-4.2093248 -4.239459 -4.2533269 -4.2558255 -4.2443156 -4.2075644 -4.1453714 -4.0784965 -4.0441256 -4.0392938 -4.0584059 -4.1045609 -4.1605625 -4.19663 -4.2180433][-4.2073207 -4.2331514 -4.2466431 -4.2524548 -4.2491746 -4.2249846 -4.1780443 -4.1249433 -4.0955977 -4.0853381 -4.0928354 -4.1168613 -4.1432781 -4.1601276 -4.1761813][-4.2364626 -4.2562628 -4.2667537 -4.2715106 -4.2719588 -4.2559824 -4.2213674 -4.18112 -4.1555929 -4.1431379 -4.1428704 -4.14803 -4.1490183 -4.1494594 -4.1597595][-4.2684312 -4.2810445 -4.2863407 -4.2867341 -4.2867689 -4.2766929 -4.254035 -4.2265854 -4.2083468 -4.2002358 -4.2006531 -4.1984034 -4.1867452 -4.1777477 -4.1816607][-4.2886624 -4.2970366 -4.2983766 -4.2953973 -4.2946506 -4.2889915 -4.2759185 -4.2595954 -4.2500191 -4.2481608 -4.2509856 -4.2500396 -4.2403307 -4.231565 -4.2311521][-4.299705 -4.3065195 -4.3067231 -4.3040161 -4.3043628 -4.3029637 -4.2967372 -4.2874222 -4.2818732 -4.2810273 -4.2842345 -4.2862706 -4.2837219 -4.2805414 -4.28031][-4.3085546 -4.3153119 -4.3160787 -4.3150992 -4.3170352 -4.3188624 -4.3166847 -4.3104577 -4.3049288 -4.3012714 -4.3013391 -4.3045235 -4.3078451 -4.3105187 -4.3133059][-4.316751 -4.32232 -4.3237166 -4.3246131 -4.328136 -4.3308554 -4.328928 -4.3223667 -4.3146911 -4.3077383 -4.3046889 -4.3071833 -4.3118629 -4.3177171 -4.3243356][-4.3190627 -4.3229275 -4.32435 -4.3261333 -4.3294368 -4.330833 -4.3271713 -4.3190303 -4.3100376 -4.3019 -4.29785 -4.2992692 -4.3035297 -4.3104529 -4.3195934]]...]
INFO - root - 2017-12-05 15:56:28.401233: step 24110, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 79h:58m:17s remains)
INFO - root - 2017-12-05 15:56:37.797839: step 24120, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 81h:15m:42s remains)
INFO - root - 2017-12-05 15:56:47.013737: step 24130, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 73h:29m:03s remains)
INFO - root - 2017-12-05 15:56:56.299042: step 24140, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 78h:43m:32s remains)
INFO - root - 2017-12-05 15:57:05.585810: step 24150, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 77h:30m:21s remains)
INFO - root - 2017-12-05 15:57:14.697858: step 24160, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 82h:23m:10s remains)
INFO - root - 2017-12-05 15:57:24.179134: step 24170, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 78h:11m:13s remains)
INFO - root - 2017-12-05 15:57:33.440756: step 24180, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 72h:24m:14s remains)
INFO - root - 2017-12-05 15:57:42.801958: step 24190, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 82h:25m:03s remains)
INFO - root - 2017-12-05 15:57:51.953796: step 24200, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 76h:29m:10s remains)
2017-12-05 15:57:52.698034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2002029 -4.1882205 -4.1998696 -4.2325392 -4.2701626 -4.301362 -4.3184109 -4.3202763 -4.3172626 -4.3174515 -4.3103023 -4.2967429 -4.2796173 -4.2751975 -4.2833061][-4.1835446 -4.1818919 -4.2028708 -4.2353425 -4.26821 -4.2967663 -4.3105063 -4.3096256 -4.3039293 -4.3015394 -4.2881265 -4.2672715 -4.2441106 -4.2396803 -4.2465758][-4.2076797 -4.2140532 -4.23432 -4.2517781 -4.2639656 -4.2776556 -4.2837725 -4.2809215 -4.2739077 -4.2689409 -4.2499385 -4.2257991 -4.20574 -4.2112823 -4.2217832][-4.23663 -4.2424145 -4.253696 -4.2479491 -4.2293797 -4.2220688 -4.2193913 -4.217783 -4.214448 -4.211369 -4.1950521 -4.1784573 -4.1750913 -4.1982384 -4.217999][-4.2429013 -4.2350616 -4.2309217 -4.19816 -4.146131 -4.1164055 -4.1067924 -4.1134109 -4.1245923 -4.1341114 -4.1305065 -4.1307735 -4.1468434 -4.1882172 -4.2201767][-4.2238908 -4.1939654 -4.1715221 -4.112915 -4.03355 -3.9890451 -3.9793091 -4.0016208 -4.0296259 -4.055841 -4.0738683 -4.09993 -4.13498 -4.1870065 -4.2253165][-4.1881309 -4.1426468 -4.1096067 -4.0416908 -3.9621077 -3.9236479 -3.9203928 -3.9478641 -3.9814472 -4.0205626 -4.0610828 -4.1084957 -4.152782 -4.2039089 -4.2404141][-4.158205 -4.117815 -4.095284 -4.0463057 -3.9952612 -3.9798565 -3.9791861 -3.9930532 -4.0169368 -4.0561638 -4.1021819 -4.1502228 -4.1905479 -4.2339039 -4.2646642][-4.1527672 -4.1328974 -4.1309304 -4.111825 -4.0925007 -4.093575 -4.0893507 -4.0884843 -4.0986166 -4.1259794 -4.1627979 -4.201282 -4.2295938 -4.260673 -4.2849922][-4.1728373 -4.1706367 -4.1834722 -4.18544 -4.183342 -4.1874619 -4.1786122 -4.170835 -4.1733847 -4.1890473 -4.2122273 -4.2385058 -4.2547016 -4.273757 -4.29086][-4.1967444 -4.2021146 -4.2210212 -4.2312641 -4.2338686 -4.2355537 -4.2273903 -4.2220578 -4.2247224 -4.2341046 -4.2447486 -4.2568808 -4.2622533 -4.2722111 -4.2839975][-4.1906471 -4.1956444 -4.2175994 -4.2325892 -4.235929 -4.2356181 -4.2311063 -4.2325191 -4.2379045 -4.243103 -4.2440748 -4.2436666 -4.2407732 -4.2466626 -4.2552719][-4.1627431 -4.1606131 -4.1821017 -4.1978836 -4.2035928 -4.2070675 -4.2096143 -4.2177696 -4.2256465 -4.2291217 -4.2248535 -4.2182612 -4.209445 -4.2138238 -4.2233658][-4.1424932 -4.1332426 -4.1493106 -4.1634259 -4.1737509 -4.1843596 -4.1936727 -4.203126 -4.2092819 -4.2102256 -4.2062354 -4.1984205 -4.1870446 -4.1928706 -4.2096615][-4.1504164 -4.1377344 -4.1483631 -4.1626992 -4.1787934 -4.1931825 -4.2013764 -4.2054868 -4.2071218 -4.2066603 -4.2039313 -4.1961837 -4.1827769 -4.1921759 -4.217545]]...]
INFO - root - 2017-12-05 15:58:02.333209: step 24210, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 80h:17m:37s remains)
INFO - root - 2017-12-05 15:58:11.575574: step 24220, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 80h:35m:05s remains)
INFO - root - 2017-12-05 15:58:21.041400: step 24230, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 84h:12m:22s remains)
INFO - root - 2017-12-05 15:58:30.497933: step 24240, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 82h:28m:12s remains)
INFO - root - 2017-12-05 15:58:39.855184: step 24250, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 78h:50m:44s remains)
INFO - root - 2017-12-05 15:58:49.148401: step 24260, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 82h:38m:40s remains)
INFO - root - 2017-12-05 15:58:58.583188: step 24270, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 84h:11m:45s remains)
INFO - root - 2017-12-05 15:59:07.970507: step 24280, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 80h:26m:12s remains)
INFO - root - 2017-12-05 15:59:17.248072: step 24290, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 76h:24m:09s remains)
INFO - root - 2017-12-05 15:59:26.674186: step 24300, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 80h:48m:50s remains)
2017-12-05 15:59:27.429256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2353363 -4.2073193 -4.1782784 -4.14533 -4.1155109 -4.1032367 -4.1241646 -4.154274 -4.1840472 -4.2031593 -4.1944351 -4.1672716 -4.14046 -4.1296315 -4.1423688][-4.2446103 -4.2188549 -4.1894064 -4.1525397 -4.1233635 -4.1156354 -4.1382608 -4.1667948 -4.1931143 -4.2100797 -4.20175 -4.17484 -4.151123 -4.1417966 -4.1481123][-4.2566223 -4.2379322 -4.2139864 -4.1811833 -4.160758 -4.1590896 -4.1794796 -4.1958961 -4.2068963 -4.2120366 -4.198966 -4.1778212 -4.1683917 -4.1702929 -4.1735773][-4.2569227 -4.2428508 -4.2269177 -4.2026262 -4.1903377 -4.1950459 -4.2129545 -4.2208924 -4.2137318 -4.1997175 -4.1814017 -4.169591 -4.178308 -4.1955204 -4.2018132][-4.2416549 -4.228559 -4.2195096 -4.2001834 -4.1908588 -4.1968 -4.2145939 -4.2187104 -4.202086 -4.17365 -4.1463943 -4.1379237 -4.1607223 -4.1876597 -4.1967158][-4.2153273 -4.1919856 -4.1778622 -4.1529479 -4.1385436 -4.1476641 -4.1781116 -4.1854091 -4.1667318 -4.1351852 -4.1069694 -4.0949507 -4.1136804 -4.1393218 -4.1523252][-4.1927094 -4.1550212 -4.1297579 -4.1011872 -4.0802922 -4.0838261 -4.1191235 -4.1288328 -4.1138568 -4.0894918 -4.0744758 -4.0653033 -4.0642266 -4.0722504 -4.0815549][-4.1919866 -4.1414595 -4.1062584 -4.0677257 -4.0321226 -4.0190835 -4.0450449 -4.055409 -4.0484667 -4.0430818 -4.0477376 -4.0489788 -4.040606 -4.0313044 -4.02684][-4.1927166 -4.1340566 -4.0912876 -4.0445638 -4.0000067 -3.9751737 -3.9884119 -3.9962764 -4.0001364 -4.0171895 -4.0449047 -4.0646558 -4.0651178 -4.0611053 -4.0552039][-4.1891685 -4.1344585 -4.0993237 -4.0648155 -4.0282836 -4.0028572 -4.0102162 -4.01638 -4.0262156 -4.0565705 -4.0988 -4.128922 -4.1353717 -4.1366253 -4.1329803][-4.1982722 -4.1595106 -4.1426477 -4.125936 -4.1051459 -4.0891805 -4.0980072 -4.105195 -4.1160822 -4.1457529 -4.1835494 -4.2106824 -4.2178788 -4.2185569 -4.2147255][-4.2163649 -4.1974354 -4.1989918 -4.1976838 -4.1897936 -4.1802211 -4.187664 -4.1941094 -4.2034421 -4.2227173 -4.244019 -4.2588339 -4.2625947 -4.26247 -4.2594261][-4.2308393 -4.2254071 -4.2396779 -4.2497606 -4.2518787 -4.2437863 -4.241765 -4.2413216 -4.24756 -4.2613111 -4.2721353 -4.2773132 -4.27843 -4.2788324 -4.2809777][-4.2423315 -4.2413964 -4.2580762 -4.2742505 -4.2839341 -4.2802768 -4.2744336 -4.2707434 -4.2761717 -4.2881117 -4.2960715 -4.2975149 -4.2955933 -4.2943015 -4.2968884][-4.2520442 -4.2516332 -4.2653933 -4.2810407 -4.29319 -4.2944331 -4.2913933 -4.289052 -4.29331 -4.3026638 -4.3071966 -4.3056788 -4.301405 -4.2975483 -4.2971435]]...]
INFO - root - 2017-12-05 15:59:36.852049: step 24310, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.965 sec/batch; 82h:37m:53s remains)
INFO - root - 2017-12-05 15:59:46.273878: step 24320, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 81h:16m:32s remains)
INFO - root - 2017-12-05 15:59:55.884455: step 24330, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 82h:55m:41s remains)
INFO - root - 2017-12-05 16:00:05.252717: step 24340, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.903 sec/batch; 77h:19m:15s remains)
INFO - root - 2017-12-05 16:00:14.411326: step 24350, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 80h:34m:10s remains)
INFO - root - 2017-12-05 16:00:23.793721: step 24360, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.982 sec/batch; 84h:01m:04s remains)
INFO - root - 2017-12-05 16:00:33.174362: step 24370, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 80h:50m:16s remains)
INFO - root - 2017-12-05 16:00:42.446196: step 24380, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 81h:06m:16s remains)
INFO - root - 2017-12-05 16:00:51.885494: step 24390, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 82h:38m:41s remains)
INFO - root - 2017-12-05 16:01:01.282531: step 24400, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 84h:29m:11s remains)
2017-12-05 16:01:02.055072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2663569 -4.261447 -4.2743721 -4.295393 -4.3124413 -4.3215914 -4.3223114 -4.3121181 -4.3113003 -4.3153996 -4.3149757 -4.3171964 -4.309515 -4.2959781 -4.2743983][-4.2287073 -4.2265158 -4.2450132 -4.2706842 -4.2928748 -4.3074422 -4.3083591 -4.2953553 -4.2959814 -4.3014727 -4.2991161 -4.2972975 -4.2830744 -4.2606091 -4.2290359][-4.2247558 -4.226562 -4.2476435 -4.27055 -4.2889528 -4.2997479 -4.2964859 -4.2795095 -4.2806249 -4.2860913 -4.2805319 -4.2757788 -4.257597 -4.2320795 -4.1983891][-4.255496 -4.2587605 -4.2773089 -4.289979 -4.2954292 -4.2926874 -4.2783995 -4.2552614 -4.2529645 -4.2551646 -4.248693 -4.2447448 -4.2299089 -4.2103319 -4.1842084][-4.281137 -4.283535 -4.294035 -4.2929797 -4.279582 -4.2558146 -4.2274861 -4.20253 -4.2033629 -4.2035933 -4.2011347 -4.2055626 -4.2021646 -4.194519 -4.1820693][-4.2767134 -4.2692227 -4.26592 -4.2452621 -4.2079415 -4.1580758 -4.109735 -4.080965 -4.0956159 -4.1131015 -4.1316509 -4.1589866 -4.178184 -4.1891594 -4.1894703][-4.2537322 -4.2350855 -4.215219 -4.1733932 -4.1080303 -4.026011 -3.9462466 -3.9034224 -3.9368277 -3.9858422 -4.0416818 -4.0998731 -4.1393857 -4.1682291 -4.1817036][-4.2423558 -4.2212019 -4.1978607 -4.15157 -4.0752716 -3.9773216 -3.8829961 -3.8296828 -3.8721924 -3.9453218 -4.0240946 -4.0954914 -4.1368089 -4.1623955 -4.1738515][-4.2298923 -4.2164531 -4.2093816 -4.186821 -4.135798 -4.0682898 -4.00963 -3.9766166 -4.0063877 -4.0633669 -4.12083 -4.1652021 -4.1802897 -4.1805949 -4.1730576][-4.1940789 -4.1872363 -4.2021222 -4.2117748 -4.1953363 -4.1652675 -4.1406264 -4.1276207 -4.1483088 -4.1841526 -4.2097654 -4.2197247 -4.205132 -4.1844435 -4.1588869][-4.1582174 -4.1524887 -4.1824384 -4.217557 -4.2315764 -4.2282934 -4.2195897 -4.2120891 -4.221858 -4.2378783 -4.2401266 -4.2329698 -4.2044454 -4.1733365 -4.1408129][-4.1500373 -4.1480284 -4.1836262 -4.2260661 -4.2556381 -4.2673912 -4.2638927 -4.2551708 -4.2542253 -4.2550769 -4.242928 -4.2261777 -4.1924515 -4.1606555 -4.1322951][-4.1904936 -4.1928563 -4.2262354 -4.2621832 -4.2910075 -4.303514 -4.2976489 -4.2860227 -4.2795534 -4.2731009 -4.256022 -4.237134 -4.2073593 -4.1847839 -4.1693058][-4.2721786 -4.2756357 -4.2986207 -4.3209181 -4.3379755 -4.3407373 -4.3288736 -4.3166828 -4.3119941 -4.30871 -4.2961555 -4.2827029 -4.2629375 -4.2489119 -4.2446847][-4.3408432 -4.3413806 -4.3542361 -4.3666062 -4.3741641 -4.3715949 -4.3568964 -4.3441625 -4.3409982 -4.342762 -4.3385544 -4.3322272 -4.3211851 -4.312798 -4.3133068]]...]
INFO - root - 2017-12-05 16:01:11.273012: step 24410, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 76h:22m:05s remains)
INFO - root - 2017-12-05 16:01:20.638143: step 24420, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 81h:37m:54s remains)
INFO - root - 2017-12-05 16:01:30.253181: step 24430, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.995 sec/batch; 85h:09m:59s remains)
INFO - root - 2017-12-05 16:01:39.716774: step 24440, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 80h:43m:22s remains)
INFO - root - 2017-12-05 16:01:49.279529: step 24450, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 82h:08m:16s remains)
INFO - root - 2017-12-05 16:01:58.642301: step 24460, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 78h:50m:37s remains)
INFO - root - 2017-12-05 16:02:07.727298: step 24470, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 84h:18m:26s remains)
INFO - root - 2017-12-05 16:02:17.190721: step 24480, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 83h:12m:38s remains)
INFO - root - 2017-12-05 16:02:26.639438: step 24490, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 80h:51m:12s remains)
INFO - root - 2017-12-05 16:02:35.914240: step 24500, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 79h:27m:13s remains)
2017-12-05 16:02:36.645640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2550449 -4.268867 -4.2840176 -4.2926283 -4.2909441 -4.2848773 -4.2874455 -4.2988224 -4.3130851 -4.3227415 -4.3192258 -4.30244 -4.278266 -4.25992 -4.2501626][-4.2213974 -4.2432623 -4.2637453 -4.2768159 -4.2811022 -4.2775264 -4.2783518 -4.2905989 -4.3101978 -4.3227544 -4.3211718 -4.3026485 -4.2749047 -4.2501364 -4.2335167][-4.1947651 -4.2244644 -4.2504978 -4.2615862 -4.2614393 -4.2494764 -4.2419305 -4.25421 -4.2825766 -4.3046427 -4.3121405 -4.3032103 -4.2819319 -4.2585788 -4.236536][-4.1938 -4.2242508 -4.2485657 -4.2515821 -4.235095 -4.2007637 -4.1745996 -4.1860976 -4.2292824 -4.2662044 -4.2872305 -4.2955694 -4.2872114 -4.27099 -4.2515283][-4.1972122 -4.2230849 -4.2411833 -4.2315993 -4.1918211 -4.1240892 -4.068532 -4.0823689 -4.1495948 -4.2094431 -4.251174 -4.2771797 -4.2837944 -4.2757263 -4.262702][-4.2069259 -4.2297392 -4.2392378 -4.2180996 -4.1565909 -4.05752 -3.9732835 -3.9836426 -4.0661693 -4.1459417 -4.2066727 -4.24858 -4.270216 -4.2734485 -4.2691464][-4.2149739 -4.23804 -4.2380695 -4.2083325 -4.1407013 -4.0396037 -3.958998 -3.9665987 -4.03991 -4.1138759 -4.1741934 -4.2238131 -4.2567482 -4.2737904 -4.2785387][-4.2228651 -4.2490568 -4.2434926 -4.2103534 -4.1490054 -4.071219 -4.0229282 -4.0342188 -4.0833869 -4.1317372 -4.1748753 -4.2178512 -4.2516351 -4.2748513 -4.2883096][-4.2368994 -4.2636447 -4.2588191 -4.2348104 -4.1875529 -4.1384621 -4.1186495 -4.1323195 -4.1607404 -4.1835322 -4.2067227 -4.2313695 -4.2542353 -4.2726369 -4.2883449][-4.2569418 -4.2778082 -4.2755938 -4.2636361 -4.2334075 -4.2068038 -4.2063947 -4.2215815 -4.2381468 -4.2458444 -4.2498646 -4.2556672 -4.2599835 -4.2649803 -4.2771049][-4.2788415 -4.2927904 -4.28776 -4.2776027 -4.2595029 -4.2488651 -4.2605715 -4.27953 -4.2921953 -4.2926092 -4.2800236 -4.2693777 -4.2634263 -4.256381 -4.263133][-4.2993307 -4.3030758 -4.2917495 -4.2782421 -4.2677851 -4.2718744 -4.2911587 -4.3067427 -4.31487 -4.309936 -4.2903833 -4.2715559 -4.2630005 -4.2548923 -4.2594681][-4.3171091 -4.3128042 -4.2949252 -4.2774334 -4.2718973 -4.2850947 -4.3036251 -4.3121243 -4.3149714 -4.3094821 -4.2936473 -4.2765279 -4.2683473 -4.2633476 -4.2678723][-4.3232918 -4.3144903 -4.2983651 -4.28179 -4.2764034 -4.2909937 -4.3070254 -4.3113275 -4.3100915 -4.3035221 -4.2940574 -4.2811432 -4.2730665 -4.2700377 -4.2761788][-4.3159394 -4.3060722 -4.295258 -4.2838717 -4.2809625 -4.2928095 -4.3046117 -4.3060722 -4.3017063 -4.2958007 -4.2893567 -4.2773905 -4.2686148 -4.2682505 -4.2794871]]...]
INFO - root - 2017-12-05 16:02:45.938616: step 24510, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 79h:02m:48s remains)
INFO - root - 2017-12-05 16:02:55.293190: step 24520, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 79h:39m:23s remains)
INFO - root - 2017-12-05 16:03:04.775107: step 24530, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.929 sec/batch; 79h:27m:01s remains)
INFO - root - 2017-12-05 16:03:14.063094: step 24540, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 79h:14m:32s remains)
INFO - root - 2017-12-05 16:03:23.173567: step 24550, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.009 sec/batch; 86h:20m:14s remains)
INFO - root - 2017-12-05 16:03:32.404895: step 24560, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 83h:09m:37s remains)
INFO - root - 2017-12-05 16:03:41.799313: step 24570, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 82h:55m:15s remains)
INFO - root - 2017-12-05 16:03:50.997215: step 24580, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 73h:44m:01s remains)
INFO - root - 2017-12-05 16:04:00.432511: step 24590, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 82h:47m:34s remains)
INFO - root - 2017-12-05 16:04:09.786359: step 24600, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 83h:44m:11s remains)
2017-12-05 16:04:10.535653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3104458 -4.2996306 -4.2890573 -4.2805963 -4.279953 -4.2838397 -4.2850151 -4.2864265 -4.296185 -4.3037491 -4.3036375 -4.2999668 -4.2932262 -4.282907 -4.2750125][-4.30008 -4.2817478 -4.2624083 -4.2451806 -4.2365694 -4.2343421 -4.2342257 -4.2340765 -4.2493057 -4.270535 -4.2820621 -4.2887068 -4.2842641 -4.2680964 -4.2494488][-4.2902894 -4.2623558 -4.2305388 -4.1973209 -4.1761642 -4.1619053 -4.152391 -4.1498966 -4.1711812 -4.2089243 -4.2383747 -4.2583737 -4.2606907 -4.2425885 -4.2156177][-4.2832966 -4.2478414 -4.2041607 -4.1548185 -4.1209145 -4.0925531 -4.0637527 -4.0486259 -4.0766978 -4.1361017 -4.1873851 -4.2166791 -4.2222095 -4.2072706 -4.1785941][-4.2796345 -4.2419434 -4.1900063 -4.1272368 -4.0787916 -4.0290918 -3.9681344 -3.9290822 -3.9666679 -4.052381 -4.1240044 -4.158329 -4.171205 -4.1675673 -4.1472616][-4.2775717 -4.2426825 -4.1869407 -4.1110277 -4.0446496 -3.9675827 -3.8640506 -3.7986875 -3.8574934 -3.9748073 -4.0654716 -4.0982847 -4.1082726 -4.1163125 -4.1110449][-4.2773366 -4.2495537 -4.1970353 -4.1162858 -4.0354109 -3.9334829 -3.8000057 -3.7248049 -3.8075066 -3.9460249 -4.0391731 -4.0531464 -4.0414057 -4.0567393 -4.0736308][-4.277885 -4.2596831 -4.2163234 -4.1444139 -4.0701962 -3.9786489 -3.8668885 -3.8121674 -3.8849077 -3.9975462 -4.0552459 -4.0290546 -3.9818075 -3.9912381 -4.0223951][-4.2793961 -4.2677627 -4.2376471 -4.186563 -4.1348653 -4.0728 -4.0058341 -3.9757206 -4.0154085 -4.0786462 -4.0897713 -4.0303087 -3.9595053 -3.9549148 -3.9816449][-4.2807865 -4.2711253 -4.2523828 -4.2219377 -4.19405 -4.1609249 -4.1271491 -4.1060123 -4.11442 -4.1398759 -4.12972 -4.0680242 -4.00519 -3.9905727 -4.0013247][-4.2805853 -4.2681575 -4.2558293 -4.2397761 -4.2313724 -4.2217689 -4.2089629 -4.1911759 -4.1805754 -4.186779 -4.1784892 -4.144443 -4.11465 -4.1002312 -4.0914][-4.2804828 -4.262948 -4.2500696 -4.243752 -4.2491083 -4.2566605 -4.259295 -4.2458858 -4.2303185 -4.2304554 -4.2298164 -4.2191997 -4.2133226 -4.2005019 -4.1816154][-4.2806964 -4.2572503 -4.2410378 -4.2407947 -4.2544427 -4.2737169 -4.2842207 -4.2736397 -4.2576413 -4.2568083 -4.2585912 -4.2603273 -4.2649431 -4.257062 -4.2414804][-4.2837496 -4.2569494 -4.2386255 -4.2403212 -4.2580671 -4.281599 -4.2943845 -4.285028 -4.27318 -4.2753239 -4.27719 -4.2823224 -4.2913809 -4.2901568 -4.2838178][-4.2920551 -4.2672234 -4.2490821 -4.2504516 -4.2669964 -4.2867441 -4.2972894 -4.290441 -4.2835674 -4.2867303 -4.2879677 -4.2921195 -4.3003206 -4.3006778 -4.2984838]]...]
INFO - root - 2017-12-05 16:04:19.915804: step 24610, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 81h:52m:02s remains)
INFO - root - 2017-12-05 16:04:29.170315: step 24620, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 82h:58m:46s remains)
INFO - root - 2017-12-05 16:04:38.413995: step 24630, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.986 sec/batch; 84h:19m:46s remains)
INFO - root - 2017-12-05 16:04:47.532888: step 24640, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 81h:17m:13s remains)
INFO - root - 2017-12-05 16:04:56.827853: step 24650, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 75h:23m:17s remains)
INFO - root - 2017-12-05 16:05:05.965524: step 24660, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 76h:47m:44s remains)
INFO - root - 2017-12-05 16:05:15.206395: step 24670, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.946 sec/batch; 80h:55m:19s remains)
INFO - root - 2017-12-05 16:05:24.854461: step 24680, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 80h:22m:25s remains)
INFO - root - 2017-12-05 16:05:34.142101: step 24690, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 75h:14m:42s remains)
INFO - root - 2017-12-05 16:05:43.411591: step 24700, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 82h:51m:59s remains)
2017-12-05 16:05:44.170750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.285851 -4.3012109 -4.3106637 -4.3033061 -4.2853584 -4.2619486 -4.2351103 -4.2167773 -4.2123284 -4.2129688 -4.2160864 -4.2180352 -4.2108922 -4.193862 -4.1771297][-4.2869997 -4.29971 -4.3070583 -4.2997546 -4.2848234 -4.2657046 -4.2412596 -4.2249327 -4.2208142 -4.2218547 -4.2242084 -4.2245369 -4.2171764 -4.1989064 -4.18379][-4.2797847 -4.285841 -4.2875547 -4.2793388 -4.2679763 -4.2543559 -4.2341623 -4.2207627 -4.21675 -4.2165093 -4.2186885 -4.2210841 -4.21709 -4.2036405 -4.1959629][-4.2690539 -4.2675405 -4.2650189 -4.2564783 -4.2484045 -4.2397265 -4.2233768 -4.2121749 -4.2088861 -4.2086234 -4.2115 -4.2176719 -4.2195992 -4.2133889 -4.211513][-4.2479181 -4.2390165 -4.235044 -4.228847 -4.2230792 -4.216516 -4.2008772 -4.1898971 -4.1897826 -4.1927037 -4.1986613 -4.2090182 -4.2176781 -4.2180567 -4.2179861][-4.223588 -4.211154 -4.2092395 -4.2062054 -4.1997952 -4.1899967 -4.1668582 -4.14878 -4.1514168 -4.159596 -4.1714478 -4.1870093 -4.200037 -4.20218 -4.1999512][-4.1890588 -4.1731591 -4.1704435 -4.16514 -4.1518459 -4.1310849 -4.0932789 -4.0618343 -4.0682192 -4.0891562 -4.1131582 -4.1380529 -4.1567883 -4.1592889 -4.1534805][-4.1621904 -4.1458664 -4.14172 -4.1321416 -4.1074505 -4.0706868 -4.0154605 -3.9717317 -3.9836888 -4.021091 -4.0585976 -4.0920105 -4.1148071 -4.1161709 -4.1086392][-4.1761389 -4.1691051 -4.1698642 -4.1641464 -4.1381612 -4.0990047 -4.0460811 -4.0101461 -4.0251918 -4.0638361 -4.0987444 -4.1255097 -4.1398354 -4.134336 -4.1236963][-4.2180896 -4.2214608 -4.2282519 -4.2270932 -4.2065392 -4.1760283 -4.1375551 -4.1178827 -4.1325588 -4.1612425 -4.1828089 -4.1948304 -4.1965594 -4.1857934 -4.1751065][-4.2625256 -4.273881 -4.2851424 -4.28517 -4.2691336 -4.246254 -4.2207642 -4.2131672 -4.2248416 -4.2431269 -4.2535853 -4.2544365 -4.2472181 -4.2343445 -4.2248888][-4.2924113 -4.3045979 -4.3144569 -4.3129334 -4.3009162 -4.2853351 -4.2710028 -4.2702532 -4.2782235 -4.2876887 -4.2906666 -4.2863083 -4.2769895 -4.2656593 -4.2584524][-4.307766 -4.3160005 -4.3224397 -4.321805 -4.3157043 -4.3083539 -4.3030624 -4.30516 -4.30924 -4.3125615 -4.3114138 -4.3059921 -4.2982554 -4.2912283 -4.2872353][-4.3149486 -4.3189149 -4.3230925 -4.325078 -4.3248692 -4.3233466 -4.321981 -4.3226943 -4.3230729 -4.322957 -4.3211751 -4.3177352 -4.3138852 -4.3109388 -4.3097706][-4.3178525 -4.3186927 -4.3202839 -4.3226223 -4.3245058 -4.3250513 -4.3247833 -4.3240747 -4.3227477 -4.32217 -4.3215957 -4.3206954 -4.3199124 -4.31973 -4.319942]]...]
INFO - root - 2017-12-05 16:05:53.682164: step 24710, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.973 sec/batch; 83h:10m:22s remains)
INFO - root - 2017-12-05 16:06:02.733643: step 24720, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 77h:17m:43s remains)
INFO - root - 2017-12-05 16:06:12.010212: step 24730, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.909 sec/batch; 77h:43m:40s remains)
INFO - root - 2017-12-05 16:06:21.266128: step 24740, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 74h:54m:32s remains)
INFO - root - 2017-12-05 16:06:30.441361: step 24750, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 74h:54m:16s remains)
INFO - root - 2017-12-05 16:06:39.809817: step 24760, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.940 sec/batch; 80h:20m:25s remains)
INFO - root - 2017-12-05 16:06:49.188742: step 24770, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.014 sec/batch; 86h:42m:50s remains)
INFO - root - 2017-12-05 16:06:58.472600: step 24780, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 82h:23m:20s remains)
INFO - root - 2017-12-05 16:07:07.785379: step 24790, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 81h:36m:46s remains)
INFO - root - 2017-12-05 16:07:17.127038: step 24800, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 77h:09m:28s remains)
2017-12-05 16:07:17.916499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2646914 -4.2509937 -4.2399945 -4.2462683 -4.2546859 -4.2583632 -4.2608624 -4.2653904 -4.265501 -4.2588496 -4.2468772 -4.2370167 -4.2339649 -4.2400932 -4.2425537][-4.2468824 -4.2295284 -4.2146125 -4.2218914 -4.231607 -4.2350025 -4.2388654 -4.2450976 -4.2504435 -4.2494774 -4.2405591 -4.2288427 -4.2210116 -4.2259369 -4.2319989][-4.2366796 -4.2198324 -4.2048841 -4.2116609 -4.2159157 -4.212563 -4.2127118 -4.2183776 -4.2281594 -4.2334704 -4.2319913 -4.223598 -4.2122846 -4.2122946 -4.2180738][-4.2322268 -4.2160721 -4.1991267 -4.2008662 -4.1997471 -4.1910815 -4.1888032 -4.1938152 -4.2026081 -4.209384 -4.2135677 -4.2086306 -4.1987948 -4.19735 -4.2018738][-4.2154384 -4.2011814 -4.1830893 -4.1765084 -4.1680241 -4.1536212 -4.1530495 -4.1641359 -4.1711464 -4.1740937 -4.178277 -4.1737165 -4.1658235 -4.1679406 -4.1796875][-4.1893072 -4.1734476 -4.1522393 -4.1330032 -4.1122966 -4.0933022 -4.0985389 -4.1169872 -4.1285248 -4.1365027 -4.1400504 -4.1286821 -4.1142306 -4.1180973 -4.1423135][-4.1829371 -4.1615148 -4.133286 -4.0997968 -4.0652418 -4.042028 -4.0525475 -4.0730634 -4.0900922 -4.1063261 -4.108016 -4.0871444 -4.0594578 -4.0580769 -4.0879846][-4.1893992 -4.167985 -4.1323357 -4.0908132 -4.0495772 -4.0237679 -4.0337553 -4.0528126 -4.07144 -4.0903521 -4.0885496 -4.0675831 -4.0339346 -4.0282555 -4.0583339][-4.2044439 -4.1875153 -4.1471105 -4.103364 -4.0653644 -4.0377741 -4.0405393 -4.0557981 -4.0766511 -4.0989933 -4.1039729 -4.0921702 -4.0575371 -4.045332 -4.0656004][-4.2213845 -4.2072968 -4.1666594 -4.1264791 -4.0956321 -4.0716944 -4.0704913 -4.0786233 -4.0987649 -4.1269264 -4.14377 -4.1379285 -4.1019683 -4.0826144 -4.0902128][-4.2256832 -4.2174263 -4.1838756 -4.1538177 -4.129364 -4.1122289 -4.109386 -4.1087627 -4.1207633 -4.1512671 -4.1793256 -4.1778259 -4.14405 -4.1261797 -4.1293211][-4.20152 -4.20399 -4.181632 -4.1601119 -4.1413808 -4.1369023 -4.1396365 -4.1332793 -4.1299925 -4.1502004 -4.1835742 -4.1917815 -4.1716943 -4.1628389 -4.1679382][-4.1640935 -4.1748347 -4.1619439 -4.1479268 -4.1390195 -4.1484947 -4.1567268 -4.1479769 -4.1274753 -4.1291966 -4.156249 -4.1764336 -4.1837649 -4.1906219 -4.1986189][-4.1567807 -4.1664619 -4.1508241 -4.1333852 -4.1310625 -4.1505685 -4.1680245 -4.1645756 -4.1362329 -4.1179891 -4.1316357 -4.1586914 -4.1876559 -4.21072 -4.2220817][-4.1738038 -4.1779509 -4.1563187 -4.1342978 -4.1323085 -4.1511765 -4.17652 -4.1837521 -4.15589 -4.1211495 -4.1172543 -4.1396513 -4.1739831 -4.2067633 -4.2254224]]...]
INFO - root - 2017-12-05 16:07:27.191074: step 24810, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 76h:18m:10s remains)
INFO - root - 2017-12-05 16:07:36.415688: step 24820, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 82h:30m:38s remains)
INFO - root - 2017-12-05 16:07:45.593398: step 24830, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 76h:21m:17s remains)
INFO - root - 2017-12-05 16:07:54.698168: step 24840, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 71h:40m:43s remains)
INFO - root - 2017-12-05 16:08:04.021929: step 24850, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 82h:37m:53s remains)
INFO - root - 2017-12-05 16:08:13.483295: step 24860, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 76h:11m:46s remains)
INFO - root - 2017-12-05 16:08:22.864487: step 24870, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 79h:43m:44s remains)
INFO - root - 2017-12-05 16:08:32.390749: step 24880, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 81h:24m:41s remains)
INFO - root - 2017-12-05 16:08:41.875977: step 24890, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 80h:58m:57s remains)
INFO - root - 2017-12-05 16:08:51.242681: step 24900, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 79h:20m:38s remains)
2017-12-05 16:08:51.969140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2861567 -4.2876892 -4.2709002 -4.2420478 -4.2176189 -4.1993575 -4.1930432 -4.1974587 -4.1950455 -4.1866717 -4.1843653 -4.1964631 -4.2135735 -4.229013 -4.2431517][-4.287869 -4.2871933 -4.2663927 -4.2306776 -4.2021346 -4.1793451 -4.1738605 -4.1860838 -4.1876373 -4.1774774 -4.1716471 -4.187017 -4.20866 -4.2272153 -4.2375064][-4.2839718 -4.2811794 -4.2570181 -4.2184582 -4.1869941 -4.1610231 -4.1554456 -4.1789713 -4.1911783 -4.1850715 -4.1764488 -4.1892643 -4.2116408 -4.2299223 -4.2330742][-4.2856021 -4.2791243 -4.2483525 -4.202002 -4.1620851 -4.1283336 -4.1187968 -4.1494923 -4.1778412 -4.185751 -4.181879 -4.1889892 -4.2059636 -4.2235241 -4.2263627][-4.2919416 -4.2851868 -4.2540689 -4.2038279 -4.1555357 -4.1132298 -4.0926442 -4.1160784 -4.1528034 -4.1752753 -4.18045 -4.1840749 -4.1955347 -4.211493 -4.2151861][-4.3004036 -4.2973704 -4.272964 -4.2221861 -4.1690316 -4.1205225 -4.0863962 -4.0906539 -4.1238375 -4.1496882 -4.1625323 -4.1748824 -4.1886082 -4.2028332 -4.2079711][-4.3042574 -4.3048377 -4.2863641 -4.237793 -4.1804495 -4.124176 -4.0736709 -4.055728 -4.0805082 -4.1096778 -4.1331663 -4.1612525 -4.1841192 -4.2012424 -4.2098618][-4.3008595 -4.3021269 -4.2859898 -4.2450447 -4.1920218 -4.1324754 -4.0697708 -4.0349851 -4.0543923 -4.0883079 -4.1174774 -4.1554871 -4.1828265 -4.1991882 -4.2091351][-4.2895923 -4.2875967 -4.2724242 -4.2432046 -4.2023711 -4.1551547 -4.0979886 -4.0611548 -4.0772443 -4.1054339 -4.1305966 -4.170198 -4.1924734 -4.2011657 -4.2066665][-4.2709756 -4.26774 -4.2564631 -4.240571 -4.2163835 -4.1894979 -4.1496277 -4.1243973 -4.1368685 -4.1528873 -4.1688104 -4.2004147 -4.2163405 -4.2190309 -4.216085][-4.2564421 -4.2552996 -4.2508941 -4.2435994 -4.2302771 -4.2189894 -4.1933918 -4.1814961 -4.1922045 -4.1992993 -4.20797 -4.227438 -4.2386136 -4.2386565 -4.2306466][-4.2519841 -4.2508712 -4.2506127 -4.2467089 -4.237 -4.233418 -4.2211175 -4.2232537 -4.2369323 -4.2422037 -4.2463593 -4.2532544 -4.25916 -4.2608 -4.2535014][-4.2543178 -4.251914 -4.2528496 -4.252872 -4.2493834 -4.2537403 -4.2531009 -4.2609735 -4.2744594 -4.2779846 -4.2779527 -4.2772479 -4.2813039 -4.2850456 -4.2806306][-4.2697577 -4.2676382 -4.2689438 -4.2704868 -4.2707391 -4.2769952 -4.2803226 -4.2899942 -4.3001523 -4.29959 -4.297349 -4.2963037 -4.2986565 -4.3001842 -4.2996783][-4.2946744 -4.2956595 -4.2969074 -4.2959518 -4.294086 -4.2967768 -4.2984829 -4.3045864 -4.310101 -4.3082862 -4.3054967 -4.3041644 -4.3051081 -4.3052864 -4.3065476]]...]
INFO - root - 2017-12-05 16:09:01.132771: step 24910, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 80h:59m:41s remains)
INFO - root - 2017-12-05 16:09:10.315450: step 24920, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.984 sec/batch; 84h:05m:35s remains)
INFO - root - 2017-12-05 16:09:19.796089: step 24930, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 80h:24m:13s remains)
INFO - root - 2017-12-05 16:09:28.969572: step 24940, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 80h:58m:37s remains)
INFO - root - 2017-12-05 16:09:38.597198: step 24950, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 84h:28m:34s remains)
INFO - root - 2017-12-05 16:09:48.175780: step 24960, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 76h:52m:56s remains)
INFO - root - 2017-12-05 16:09:57.433435: step 24970, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 81h:46m:14s remains)
INFO - root - 2017-12-05 16:10:06.832664: step 24980, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 83h:00m:31s remains)
INFO - root - 2017-12-05 16:10:16.381134: step 24990, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 83h:32m:25s remains)
INFO - root - 2017-12-05 16:10:25.373814: step 25000, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 78h:51m:31s remains)
2017-12-05 16:10:26.169223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3022194 -4.3015614 -4.2960591 -4.2936273 -4.2883067 -4.282937 -4.2563596 -4.2205529 -4.2195721 -4.2362804 -4.2428269 -4.2412105 -4.2492023 -4.2914519 -4.3261728][-4.3053589 -4.2939749 -4.2838554 -4.2771249 -4.2700515 -4.2673645 -4.244978 -4.2128949 -4.2175488 -4.2358027 -4.2429075 -4.2428994 -4.253149 -4.2950511 -4.3261609][-4.2956724 -4.2761879 -4.2593021 -4.248333 -4.2394395 -4.2382879 -4.2197504 -4.1943774 -4.211266 -4.237093 -4.2474432 -4.2506914 -4.2635207 -4.3015394 -4.328723][-4.2741594 -4.2496943 -4.2290788 -4.2152658 -4.2038217 -4.1971769 -4.1775088 -4.1586995 -4.19029 -4.231226 -4.2485318 -4.2542748 -4.2704892 -4.3069668 -4.331007][-4.2579255 -4.2323065 -4.2064595 -4.187583 -4.1716166 -4.1503582 -4.1167464 -4.0933037 -4.1407261 -4.2015505 -4.2291627 -4.2413836 -4.2658949 -4.3027654 -4.3221273][-4.236567 -4.2164707 -4.1890221 -4.1654615 -4.1433377 -4.1031861 -4.0437322 -4.0061917 -4.064446 -4.1428971 -4.18186 -4.2061481 -4.2432718 -4.2814336 -4.2956729][-4.2046132 -4.2002721 -4.1804256 -4.1529527 -4.1132431 -4.0458107 -3.9477179 -3.8869896 -3.9602067 -4.0635195 -4.1204128 -4.1581216 -4.2034416 -4.2417626 -4.252883][-4.17292 -4.184638 -4.1701083 -4.1339364 -4.07739 -3.9874909 -3.8490884 -3.756161 -3.8512039 -3.9881406 -4.0641394 -4.1075025 -4.1521783 -4.1904755 -4.2025681][-4.1574688 -4.1710529 -4.1560812 -4.12239 -4.070641 -3.9898915 -3.8609502 -3.7766297 -3.8703523 -3.9982638 -4.0660272 -4.0974259 -4.1305113 -4.1616073 -4.1710014][-4.1593232 -4.1670222 -4.1552649 -4.1375852 -4.1136384 -4.0662971 -3.9895043 -3.94149 -3.9946098 -4.0687666 -4.1042933 -4.1155691 -4.1349506 -4.1585026 -4.1644926][-4.1855273 -4.1862431 -4.1797509 -4.1796284 -4.177834 -4.1567683 -4.1190867 -4.0927343 -4.1090717 -4.1378026 -4.1459451 -4.1430893 -4.15187 -4.1653514 -4.1689425][-4.2346673 -4.2288046 -4.2225552 -4.2304158 -4.2388644 -4.2320337 -4.2131562 -4.2011 -4.2056842 -4.2152162 -4.2128177 -4.2037573 -4.2041869 -4.2078691 -4.2042451][-4.277709 -4.2644296 -4.2559724 -4.2670217 -4.2821217 -4.2849588 -4.2775068 -4.2753263 -4.2830391 -4.2889938 -4.2843971 -4.2726703 -4.2662845 -4.2640343 -4.2562737][-4.3023462 -4.2861671 -4.2793846 -4.2906876 -4.3067045 -4.3125606 -4.3099351 -4.3127427 -4.320056 -4.324976 -4.3223109 -4.3127365 -4.3054094 -4.3010082 -4.2932582][-4.3203397 -4.3063912 -4.3009019 -4.3065686 -4.3168836 -4.3209906 -4.3202906 -4.3225932 -4.3259521 -4.328546 -4.3275294 -4.3216925 -4.316874 -4.3143449 -4.309835]]...]
INFO - root - 2017-12-05 16:10:35.443684: step 25010, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 76h:03m:06s remains)
INFO - root - 2017-12-05 16:10:44.687526: step 25020, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 79h:44m:25s remains)
INFO - root - 2017-12-05 16:10:54.155590: step 25030, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 81h:14m:11s remains)
INFO - root - 2017-12-05 16:11:03.559589: step 25040, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 76h:36m:12s remains)
INFO - root - 2017-12-05 16:11:13.023153: step 25050, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 81h:01m:11s remains)
INFO - root - 2017-12-05 16:11:22.240110: step 25060, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 77h:24m:54s remains)
INFO - root - 2017-12-05 16:11:31.703253: step 25070, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 78h:19m:11s remains)
INFO - root - 2017-12-05 16:11:41.116805: step 25080, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 77h:23m:23s remains)
INFO - root - 2017-12-05 16:11:50.090747: step 25090, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 76h:55m:32s remains)
INFO - root - 2017-12-05 16:11:59.722970: step 25100, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.982 sec/batch; 83h:48m:48s remains)
2017-12-05 16:12:00.520524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3300419 -4.3192253 -4.3080769 -4.3005414 -4.2954564 -4.2885594 -4.2822332 -4.2806854 -4.2909212 -4.2995577 -4.293108 -4.2795906 -4.2725892 -4.2712178 -4.2756925][-4.3123717 -4.2997146 -4.2881517 -4.2802587 -4.27448 -4.2597709 -4.2401228 -4.2281041 -4.2409492 -4.2602453 -4.2583642 -4.241734 -4.2304392 -4.2283144 -4.2338347][-4.29098 -4.2737708 -4.2601891 -4.2523103 -4.24633 -4.2231069 -4.1888614 -4.1633978 -4.1762638 -4.2032747 -4.2075372 -4.1901994 -4.1749563 -4.171917 -4.1826921][-4.2722716 -4.2512088 -4.2361174 -4.2290721 -4.2197094 -4.1865325 -4.1395626 -4.1023369 -4.11249 -4.1446524 -4.1587243 -4.1481438 -4.1333265 -4.12835 -4.1407809][-4.2595744 -4.2338748 -4.2167535 -4.2097468 -4.1952543 -4.1532574 -4.0903316 -4.036746 -4.0449471 -4.0872903 -4.1095181 -4.1076908 -4.1021214 -4.1036253 -4.1198959][-4.2422256 -4.2117543 -4.1914778 -4.1833882 -4.1597157 -4.10403 -4.0157185 -3.9254293 -3.925086 -3.9879603 -4.0316834 -4.0536981 -4.0689516 -4.0844169 -4.105762][-4.2245522 -4.1879125 -4.1657424 -4.1540318 -4.1160307 -4.0306096 -3.9007163 -3.7522855 -3.7328017 -3.8436894 -3.9438002 -4.0113621 -4.0449305 -4.0668974 -4.0861292][-4.215724 -4.1739821 -4.1476965 -4.1246667 -4.0730433 -3.9623308 -3.806149 -3.628598 -3.6075137 -3.7694669 -3.9092183 -4.0016403 -4.0418377 -4.0614724 -4.0765386][-4.21358 -4.1694188 -4.1409364 -4.1115913 -4.0593982 -3.9639359 -3.8496923 -3.737957 -3.737107 -3.8629668 -3.9664288 -4.0341296 -4.0672021 -4.0853009 -4.0985789][-4.2133379 -4.1702652 -4.1442604 -4.1204791 -4.0868173 -4.0325136 -3.9792018 -3.927207 -3.9253483 -3.9901681 -4.0438147 -4.0821538 -4.1045532 -4.11984 -4.1323624][-4.2193213 -4.1840057 -4.1701159 -4.1560221 -4.1419988 -4.1174769 -4.0923853 -4.0616918 -4.0540509 -4.0855083 -4.1136894 -4.1372652 -4.1498494 -4.1595521 -4.1730947][-4.2332797 -4.203764 -4.2023144 -4.2043624 -4.2063413 -4.1986175 -4.1805978 -4.1544418 -4.1435194 -4.15965 -4.1741018 -4.1843681 -4.1900687 -4.2011051 -4.2174358][-4.2554398 -4.2274618 -4.2285209 -4.2383595 -4.2463455 -4.2422705 -4.229497 -4.2119007 -4.2035046 -4.2101212 -4.2152662 -4.2156677 -4.2182264 -4.2352552 -4.2538123][-4.2838426 -4.2594175 -4.2573333 -4.2672606 -4.276 -4.2754664 -4.2685447 -4.25747 -4.25018 -4.2490673 -4.2492671 -4.2489223 -4.2543077 -4.2715831 -4.2869182][-4.3136306 -4.2978077 -4.2936149 -4.2997332 -4.3075562 -4.3108459 -4.3075857 -4.3006363 -4.2931914 -4.2889981 -4.2887697 -4.2928581 -4.2984462 -4.3096938 -4.3181686]]...]
INFO - root - 2017-12-05 16:12:09.687288: step 25110, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 78h:26m:29s remains)
INFO - root - 2017-12-05 16:12:18.806238: step 25120, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 79h:16m:23s remains)
INFO - root - 2017-12-05 16:12:27.969684: step 25130, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.824 sec/batch; 70h:22m:11s remains)
INFO - root - 2017-12-05 16:12:37.084431: step 25140, loss = 2.03, batch loss = 1.98 (8.5 examples/sec; 0.943 sec/batch; 80h:28m:39s remains)
INFO - root - 2017-12-05 16:12:46.397202: step 25150, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 80h:57m:46s remains)
INFO - root - 2017-12-05 16:12:55.768383: step 25160, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 81h:15m:48s remains)
INFO - root - 2017-12-05 16:13:05.182006: step 25170, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 84h:10m:08s remains)
INFO - root - 2017-12-05 16:13:14.180124: step 25180, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 78h:10m:52s remains)
INFO - root - 2017-12-05 16:13:23.431124: step 25190, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 78h:01m:23s remains)
INFO - root - 2017-12-05 16:13:32.763772: step 25200, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 81h:12m:59s remains)
2017-12-05 16:13:33.549977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.274406 -4.2549376 -4.2350078 -4.2111406 -4.1858974 -4.15561 -4.1354284 -4.1468143 -4.1758409 -4.1893134 -4.1935096 -4.2069521 -4.2229404 -4.2256379 -4.2186708][-4.2728376 -4.25243 -4.2305775 -4.20007 -4.1632118 -4.1154003 -4.0805941 -4.0920625 -4.1336613 -4.1619954 -4.1747293 -4.1906333 -4.2045445 -4.2053576 -4.199616][-4.2739406 -4.24816 -4.2203116 -4.1844754 -4.1442256 -4.0856385 -4.0337191 -4.0340395 -4.0837722 -4.1256385 -4.1461658 -4.1711659 -4.1934052 -4.1980462 -4.193325][-4.2735953 -4.240386 -4.20789 -4.171865 -4.1386628 -4.0812645 -4.0196195 -4.0080118 -4.0563169 -4.1008291 -4.1288338 -4.1661625 -4.1996512 -4.2055159 -4.1967168][-4.2712655 -4.236867 -4.2081189 -4.1808076 -4.1581678 -4.1063066 -4.0375471 -4.0122128 -4.0521317 -4.0955968 -4.1320944 -4.1773429 -4.2108855 -4.210526 -4.1965265][-4.2660785 -4.2330322 -4.2044592 -4.1802306 -4.1592894 -4.1087914 -4.0298162 -3.9809694 -4.0181966 -4.0823932 -4.1317406 -4.1770134 -4.2022638 -4.1918869 -4.1704183][-4.257689 -4.2194905 -4.1839676 -4.1545238 -4.1244712 -4.0612597 -3.9483294 -3.8541467 -3.9057965 -4.0219226 -4.0956182 -4.14008 -4.1543379 -4.1369309 -4.1131039][-4.2550168 -4.21466 -4.1757035 -4.1403575 -4.0965214 -4.0142593 -3.8693089 -3.7420409 -3.8192985 -3.9732208 -4.0578175 -4.0945415 -4.0968232 -4.0691915 -4.0398808][-4.2671847 -4.23183 -4.193428 -4.1529708 -4.0986595 -4.0216017 -3.9070339 -3.8188033 -3.887815 -4.0030565 -4.0543485 -4.0661087 -4.0554037 -4.0184956 -3.986666][-4.2840915 -4.2563081 -4.22305 -4.1846943 -4.1351991 -4.0776196 -4.005506 -3.9600718 -4.003087 -4.0603657 -4.0735841 -4.0656648 -4.0545044 -4.0242844 -4.0019383][-4.2947683 -4.2719731 -4.24574 -4.2158456 -4.1772246 -4.1360669 -4.0903826 -4.0654497 -4.0842762 -4.1012573 -4.0932903 -4.0866218 -4.086988 -4.0744 -4.0646334][-4.2966108 -4.2762051 -4.256928 -4.2387962 -4.2131662 -4.1834545 -4.1554818 -4.1413507 -4.1440349 -4.1397319 -4.1251717 -4.1255403 -4.12886 -4.1238785 -4.121666][-4.2975 -4.2810831 -4.2699142 -4.2619848 -4.2470441 -4.2277145 -4.2124491 -4.2052488 -4.201417 -4.1881285 -4.1730528 -4.1727676 -4.1698847 -4.1576824 -4.1547165][-4.296392 -4.2836761 -4.2796369 -4.2815695 -4.2764983 -4.26529 -4.2546935 -4.2504406 -4.24628 -4.228188 -4.2116327 -4.2076526 -4.1985025 -4.1781726 -4.168323][-4.2929215 -4.2831469 -4.2829843 -4.2911458 -4.2929468 -4.2860093 -4.2746434 -4.2721872 -4.2720127 -4.255969 -4.2412248 -4.2329459 -4.2187252 -4.1970491 -4.1804352]]...]
INFO - root - 2017-12-05 16:13:42.992568: step 25210, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 78h:23m:11s remains)
INFO - root - 2017-12-05 16:13:52.261458: step 25220, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 76h:42m:03s remains)
INFO - root - 2017-12-05 16:14:01.525762: step 25230, loss = 2.03, batch loss = 1.97 (8.2 examples/sec; 0.978 sec/batch; 83h:30m:13s remains)
INFO - root - 2017-12-05 16:14:10.995966: step 25240, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 78h:44m:19s remains)
INFO - root - 2017-12-05 16:14:20.324846: step 25250, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 80h:39m:28s remains)
INFO - root - 2017-12-05 16:14:29.704071: step 25260, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 79h:00m:32s remains)
INFO - root - 2017-12-05 16:14:39.075499: step 25270, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 77h:42m:05s remains)
INFO - root - 2017-12-05 16:14:48.465419: step 25280, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 81h:32m:35s remains)
INFO - root - 2017-12-05 16:14:57.863688: step 25290, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 83h:06m:38s remains)
INFO - root - 2017-12-05 16:15:07.176799: step 25300, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 82h:39m:41s remains)
2017-12-05 16:15:07.958364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2806387 -4.2747831 -4.263463 -4.2494879 -4.2431912 -4.2491088 -4.2631011 -4.2673182 -4.2674432 -4.2708774 -4.2761197 -4.2811489 -4.286931 -4.2979407 -4.3137579][-4.2593904 -4.25301 -4.2381792 -4.2189445 -4.2106671 -4.219173 -4.237133 -4.2419806 -4.2388945 -4.2409506 -4.2445817 -4.2484865 -4.2570124 -4.2730212 -4.2948427][-4.2450972 -4.2402058 -4.2232485 -4.1986713 -4.1862955 -4.1954083 -4.2160192 -4.2198796 -4.2142172 -4.2177119 -4.2233825 -4.2274642 -4.2360835 -4.2529917 -4.27868][-4.2257133 -4.2178726 -4.1953783 -4.1633778 -4.1440921 -4.1493187 -4.1679354 -4.1706533 -4.1701365 -4.1828914 -4.1948948 -4.2020926 -4.2137918 -4.2344112 -4.2647414][-4.2052937 -4.1909561 -4.1628766 -4.124073 -4.0951867 -4.0897141 -4.0962553 -4.0938268 -4.1032577 -4.1306205 -4.1548557 -4.1700282 -4.1889338 -4.2162638 -4.2513938][-4.1877279 -4.1651549 -4.1280794 -4.0805926 -4.0388632 -4.0145683 -3.9998355 -3.9944706 -4.0221896 -4.0696135 -4.109324 -4.1373158 -4.1644683 -4.1973972 -4.236743][-4.1682196 -4.1370935 -4.0887175 -4.029582 -3.9714372 -3.9279375 -3.8913171 -3.8827457 -3.9329476 -4.0031281 -4.0628839 -4.1075439 -4.1452575 -4.1855569 -4.2286491][-4.14096 -4.1079464 -4.0606184 -4.0058942 -3.9520402 -3.9100509 -3.8720531 -3.86721 -3.9245911 -3.9960263 -4.0603189 -4.1097751 -4.1504111 -4.1912894 -4.2329268][-4.1216989 -4.0957456 -4.0608578 -4.0186033 -3.9741907 -3.9412866 -3.9129348 -3.9161272 -3.9721696 -4.0340443 -4.0896544 -4.1323867 -4.1668038 -4.2029066 -4.2410035][-4.1295052 -4.1142969 -4.09466 -4.0680051 -4.0362816 -4.0120583 -3.9907134 -3.9958858 -4.0428581 -4.0917196 -4.1348076 -4.1679287 -4.1943393 -4.2231908 -4.2552943][-4.1516633 -4.14629 -4.1416049 -4.1312675 -4.1148267 -4.1003971 -4.0860834 -4.0883756 -4.1203656 -4.15505 -4.1863523 -4.2093973 -4.226994 -4.2477031 -4.2730861][-4.1904135 -4.1899223 -4.1927032 -4.1918149 -4.1866527 -4.1792326 -4.1695004 -4.1691742 -4.1887565 -4.2129588 -4.2351465 -4.2499876 -4.2612724 -4.2746935 -4.2924957][-4.2304935 -4.2300286 -4.2329803 -4.2352848 -4.2350802 -4.2317762 -4.2258348 -4.22395 -4.2341137 -4.2504091 -4.2665691 -4.2780948 -4.2874374 -4.2974033 -4.3094869][-4.2614141 -4.2596374 -4.2608347 -4.2624674 -4.2630248 -4.2620282 -4.2595935 -4.2577958 -4.2620568 -4.2720857 -4.2840624 -4.294445 -4.3036728 -4.3128333 -4.3220868][-4.2896075 -4.2883673 -4.2888155 -4.2897978 -4.2906895 -4.2911391 -4.2903075 -4.2891121 -4.29054 -4.29603 -4.3038545 -4.3111391 -4.3177795 -4.3246822 -4.3315849]]...]
INFO - root - 2017-12-05 16:15:17.375028: step 25310, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 75h:01m:12s remains)
INFO - root - 2017-12-05 16:15:26.621833: step 25320, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 83h:27m:13s remains)
INFO - root - 2017-12-05 16:15:35.885823: step 25330, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 85h:01m:33s remains)
INFO - root - 2017-12-05 16:15:45.502626: step 25340, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 79h:55m:31s remains)
INFO - root - 2017-12-05 16:15:54.918932: step 25350, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.970 sec/batch; 82h:45m:17s remains)
INFO - root - 2017-12-05 16:16:04.155802: step 25360, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 73h:28m:41s remains)
INFO - root - 2017-12-05 16:16:13.479724: step 25370, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 81h:51m:49s remains)
INFO - root - 2017-12-05 16:16:22.796022: step 25380, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 79h:33m:32s remains)
INFO - root - 2017-12-05 16:16:31.935144: step 25390, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 73h:04m:04s remains)
INFO - root - 2017-12-05 16:16:41.106552: step 25400, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 74h:57m:55s remains)
2017-12-05 16:16:41.941705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2487755 -4.227119 -4.2114577 -4.2047815 -4.2037325 -4.1963987 -4.1738858 -4.1523137 -4.1480832 -4.1634712 -4.20385 -4.2445154 -4.2785244 -4.3028398 -4.3210025][-4.2110271 -4.1850042 -4.1671772 -4.1548338 -4.1537623 -4.1440859 -4.1075 -4.0733151 -4.0636053 -4.0850081 -4.1400795 -4.1986666 -4.2490835 -4.2846222 -4.3108263][-4.1758327 -4.1526537 -4.1360354 -4.1165314 -4.1173735 -4.1056633 -4.0536313 -4.0038614 -3.9925282 -4.0208893 -4.0837021 -4.1530294 -4.2174497 -4.2643709 -4.2986045][-4.1359124 -4.1196413 -4.1083932 -4.0810094 -4.0768328 -4.0593519 -3.991852 -3.9273336 -3.9212711 -3.9663854 -4.0369158 -4.1125593 -4.1872725 -4.2451324 -4.2882175][-4.0953355 -4.0855284 -4.07492 -4.0405827 -4.0288796 -4.0057778 -3.9279437 -3.8472278 -3.8486178 -3.9211595 -4.0040264 -4.08312 -4.163619 -4.2305088 -4.2812514][-4.0630293 -4.052527 -4.0366316 -4.0029216 -3.9858463 -3.9565263 -3.8759875 -3.7770853 -3.7806149 -3.8851211 -3.9883611 -4.0720186 -4.1554122 -4.2248931 -4.2794604][-4.0452089 -4.0344429 -4.0078168 -3.9801836 -3.9644489 -3.9306424 -3.8442326 -3.7179949 -3.7107744 -3.8454595 -3.972019 -4.0650492 -4.1512675 -4.220655 -4.2778831][-4.0518742 -4.0483675 -4.0200777 -4.0013866 -3.9944885 -3.9616876 -3.8695483 -3.7290413 -3.7075105 -3.8457563 -3.9737389 -4.0638 -4.1489778 -4.2163615 -4.2747316][-4.0997238 -4.1085892 -4.0925589 -4.081955 -4.0856886 -4.0594645 -3.9688098 -3.8420343 -3.8168786 -3.9128268 -4.01255 -4.0831013 -4.1554418 -4.2154312 -4.2723737][-4.1652842 -4.1773524 -4.1695843 -4.1613278 -4.1655941 -4.1465206 -4.06462 -3.9565501 -3.9298813 -3.9861388 -4.0589528 -4.1130891 -4.1700287 -4.2199316 -4.2724171][-4.2248983 -4.2321138 -4.2276697 -4.225728 -4.2264843 -4.2085609 -4.1365829 -4.0427194 -4.0165949 -4.0497718 -4.1036768 -4.1469073 -4.1893449 -4.2302594 -4.2767768][-4.2623606 -4.2670808 -4.2704148 -4.2742734 -4.2721686 -4.2556796 -4.1945105 -4.113555 -4.0879936 -4.1070623 -4.1457214 -4.178597 -4.2106977 -4.2457962 -4.287324][-4.2868 -4.2905169 -4.3005657 -4.3081784 -4.3041797 -4.2903848 -4.2455783 -4.1784067 -4.1507854 -4.1609049 -4.1879058 -4.2114792 -4.2375655 -4.2686052 -4.3024387][-4.3034782 -4.3075171 -4.3203025 -4.3292875 -4.3272166 -4.3190918 -4.2894931 -4.2405629 -4.2131152 -4.2171836 -4.2348514 -4.250195 -4.2705407 -4.2946391 -4.3192124][-4.3176508 -4.3226781 -4.3345723 -4.3427019 -4.3425479 -4.3384113 -4.3198442 -4.2856436 -4.2628808 -4.2630854 -4.2761483 -4.2862954 -4.2988086 -4.3152709 -4.3321056]]...]
INFO - root - 2017-12-05 16:16:51.301676: step 25410, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 84h:10m:08s remains)
INFO - root - 2017-12-05 16:17:00.480948: step 25420, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 73h:03m:25s remains)
INFO - root - 2017-12-05 16:17:09.859028: step 25430, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.978 sec/batch; 83h:24m:35s remains)
INFO - root - 2017-12-05 16:17:18.787101: step 25440, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 78h:32m:04s remains)
INFO - root - 2017-12-05 16:17:28.138459: step 25450, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.890 sec/batch; 75h:54m:27s remains)
INFO - root - 2017-12-05 16:17:37.343267: step 25460, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 79h:35m:49s remains)
INFO - root - 2017-12-05 16:17:46.776952: step 25470, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 80h:13m:09s remains)
INFO - root - 2017-12-05 16:17:55.973947: step 25480, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 79h:54m:38s remains)
INFO - root - 2017-12-05 16:18:05.210609: step 25490, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 77h:53m:50s remains)
INFO - root - 2017-12-05 16:18:14.675767: step 25500, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 81h:49m:46s remains)
2017-12-05 16:18:15.500516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2887735 -4.2819481 -4.277885 -4.2782903 -4.2799783 -4.284966 -4.2853842 -4.282227 -4.2798071 -4.2822037 -4.2869859 -4.2904835 -4.2949967 -4.3003521 -4.2969866][-4.2832565 -4.2811966 -4.2852793 -4.2929654 -4.2983294 -4.3049388 -4.3013277 -4.2930045 -4.2851419 -4.2842474 -4.2877851 -4.2911906 -4.2954731 -4.2977362 -4.288516][-4.2550731 -4.2541833 -4.2634783 -4.2747755 -4.2800341 -4.2863817 -4.2815542 -4.2694569 -4.2617111 -4.2662506 -4.2758594 -4.2837706 -4.289999 -4.2907305 -4.2788711][-4.2025433 -4.1997485 -4.2124104 -4.2274141 -4.2320704 -4.2342811 -4.2261944 -4.2128854 -4.2097569 -4.2228675 -4.2411418 -4.254293 -4.2612619 -4.2615042 -4.253334][-4.1485119 -4.1479335 -4.1618891 -4.1726022 -4.1739092 -4.1712193 -4.1578054 -4.1438828 -4.1509614 -4.1775064 -4.2074885 -4.2261796 -4.2330937 -4.2324848 -4.2274294][-4.1149969 -4.1071849 -4.1104279 -4.1035733 -4.0866718 -4.0657105 -4.0405049 -4.0300641 -4.0561404 -4.1068635 -4.1550369 -4.1871724 -4.2043028 -4.2069321 -4.2044387][-4.0901752 -4.0629072 -4.0382495 -4.0016975 -3.9559979 -3.9073017 -3.8674974 -3.8579423 -3.9027326 -3.982013 -4.0511193 -4.1019773 -4.1380777 -4.1531038 -4.1636052][-4.1035051 -4.0722647 -4.0373769 -3.9932032 -3.9427688 -3.8883867 -3.8503461 -3.841213 -3.8843782 -3.9638348 -4.031404 -4.0831022 -4.1241689 -4.1431069 -4.157733][-4.1301203 -4.1115427 -4.0889678 -4.0614734 -4.032537 -3.9997668 -3.9795475 -3.9734635 -3.9998903 -4.0520253 -4.0962353 -4.1328106 -4.1630974 -4.1738167 -4.1818576][-4.1695557 -4.1662831 -4.15972 -4.1473503 -4.1323385 -4.1146874 -4.1035833 -4.0998149 -4.1123152 -4.1412373 -4.1674018 -4.1914296 -4.2122369 -4.216516 -4.2164311][-4.2087092 -4.2161641 -4.2232842 -4.2231517 -4.214395 -4.1990108 -4.1885953 -4.184042 -4.1866627 -4.2036061 -4.222218 -4.240541 -4.2566681 -4.2592773 -4.2534404][-4.2427092 -4.2539148 -4.2670021 -4.2745471 -4.2716146 -4.2596388 -4.2517643 -4.2479043 -4.2454324 -4.254941 -4.2693629 -4.2846637 -4.295907 -4.2977242 -4.2906451][-4.2735524 -4.2835078 -4.2939291 -4.3002152 -4.29769 -4.286 -4.2776742 -4.2736058 -4.2709603 -4.2780714 -4.2899981 -4.3029127 -4.3138332 -4.3185186 -4.3129048][-4.291954 -4.2976966 -4.304028 -4.3075066 -4.3056531 -4.2968631 -4.2902632 -4.286478 -4.2838197 -4.2883949 -4.2967014 -4.3063078 -4.3150768 -4.3202944 -4.3175793][-4.3003378 -4.3046508 -4.3094463 -4.3126407 -4.31258 -4.3082337 -4.30502 -4.3031278 -4.3015547 -4.3037148 -4.3074226 -4.3115811 -4.31564 -4.317945 -4.3161449]]...]
INFO - root - 2017-12-05 16:18:25.209534: step 25510, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.971 sec/batch; 82h:45m:44s remains)
INFO - root - 2017-12-05 16:18:34.516674: step 25520, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.815 sec/batch; 69h:30m:16s remains)
INFO - root - 2017-12-05 16:18:43.788672: step 25530, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 81h:04m:13s remains)
INFO - root - 2017-12-05 16:18:53.118708: step 25540, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 80h:30m:23s remains)
INFO - root - 2017-12-05 16:19:02.452330: step 25550, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 78h:58m:47s remains)
INFO - root - 2017-12-05 16:19:12.016575: step 25560, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 80h:16m:14s remains)
INFO - root - 2017-12-05 16:19:21.361352: step 25570, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 79h:17m:31s remains)
INFO - root - 2017-12-05 16:19:30.641700: step 25580, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 78h:29m:24s remains)
INFO - root - 2017-12-05 16:19:40.138255: step 25590, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 82h:16m:18s remains)
INFO - root - 2017-12-05 16:19:49.555388: step 25600, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 80h:12m:13s remains)
2017-12-05 16:19:50.321148: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2529769 -4.226284 -4.2205973 -4.21821 -4.2073574 -4.1837873 -4.1494164 -4.1020546 -4.0745573 -4.1032834 -4.1773949 -4.2545719 -4.308413 -4.3340554 -4.3440762][-4.2367992 -4.2055831 -4.1972489 -4.1974397 -4.1919651 -4.173902 -4.1435757 -4.0927505 -4.0646648 -4.0971708 -4.1727037 -4.2515726 -4.3089633 -4.3350277 -4.3444757][-4.2317758 -4.198925 -4.1881733 -4.1869245 -4.1814475 -4.1637778 -4.1303763 -4.0718875 -4.0451016 -4.0839534 -4.1626425 -4.24473 -4.3068504 -4.3341727 -4.3428135][-4.2404313 -4.2056046 -4.1878333 -4.1801686 -4.1680765 -4.1455846 -4.1041603 -4.0400648 -4.0189233 -4.068984 -4.1546187 -4.2409034 -4.3065066 -4.3351569 -4.3424859][-4.2609553 -4.2281017 -4.2050571 -4.1880107 -4.1621685 -4.124824 -4.0704341 -4.0040412 -3.9929161 -4.0542374 -4.1463175 -4.2360935 -4.3044052 -4.3353052 -4.3428636][-4.2885485 -4.2618537 -4.238214 -4.2138186 -4.1724691 -4.1158834 -4.04674 -3.9796505 -3.9757986 -4.0435333 -4.1405559 -4.2321262 -4.3021979 -4.334569 -4.34353][-4.3138227 -4.2964106 -4.2789083 -4.2544174 -4.2050376 -4.1338887 -4.0518403 -3.9796209 -3.9707968 -4.0372138 -4.135591 -4.2284122 -4.2993493 -4.3334737 -4.3441143][-4.3308473 -4.3245063 -4.3160424 -4.2987146 -4.2532659 -4.1770639 -4.0840368 -3.9996428 -3.9727507 -4.0314913 -4.1294527 -4.2251868 -4.2973576 -4.3323693 -4.3444905][-4.3373923 -4.3386779 -4.3385043 -4.3303285 -4.2961555 -4.2246509 -4.1255503 -4.02587 -3.9755425 -4.0216112 -4.118844 -4.2195358 -4.2944117 -4.3301749 -4.3439994][-4.3371572 -4.3405652 -4.3447638 -4.3445349 -4.3226161 -4.2607417 -4.1620693 -4.0514417 -3.9811409 -4.012064 -4.1052632 -4.2104259 -4.2885957 -4.3262715 -4.3419757][-4.3341537 -4.3386593 -4.3444042 -4.3493228 -4.3376303 -4.2866354 -4.1942248 -4.0806308 -3.9976368 -4.0120525 -4.0973535 -4.2033415 -4.2832608 -4.3230939 -4.3396759][-4.3310633 -4.3361778 -4.3419237 -4.3495307 -4.3445921 -4.3048687 -4.2226348 -4.1125746 -4.0225949 -4.0209122 -4.0976067 -4.2003417 -4.2801905 -4.3222246 -4.3389163][-4.3281789 -4.3326764 -4.3370571 -4.3461242 -4.3467979 -4.3184328 -4.2476711 -4.1453991 -4.0525227 -4.0360169 -4.103085 -4.201396 -4.2801256 -4.3228335 -4.33932][-4.3292189 -4.3306293 -4.3318071 -4.3402767 -4.34623 -4.3294258 -4.2720494 -4.1830564 -4.0965691 -4.0713272 -4.1244411 -4.2117453 -4.2843 -4.3248572 -4.3411446][-4.3298507 -4.327775 -4.3260627 -4.3338156 -4.3436136 -4.336102 -4.2900581 -4.216188 -4.14332 -4.1169796 -4.1552467 -4.2264156 -4.2894344 -4.3272777 -4.3438182]]...]
INFO - root - 2017-12-05 16:19:59.581314: step 25610, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 77h:42m:22s remains)
INFO - root - 2017-12-05 16:20:08.905417: step 25620, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 77h:12m:15s remains)
INFO - root - 2017-12-05 16:20:18.235880: step 25630, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 78h:53m:19s remains)
INFO - root - 2017-12-05 16:20:27.529792: step 25640, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 79h:37m:39s remains)
INFO - root - 2017-12-05 16:20:36.736609: step 25650, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 79h:59m:37s remains)
INFO - root - 2017-12-05 16:20:46.092007: step 25660, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 81h:52m:58s remains)
INFO - root - 2017-12-05 16:20:55.583024: step 25670, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.922 sec/batch; 78h:33m:13s remains)
INFO - root - 2017-12-05 16:21:04.914253: step 25680, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 78h:23m:04s remains)
INFO - root - 2017-12-05 16:21:14.340944: step 25690, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 77h:22m:50s remains)
INFO - root - 2017-12-05 16:21:23.795164: step 25700, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 81h:28m:13s remains)
2017-12-05 16:21:24.572585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3274794 -4.313489 -4.2959671 -4.282475 -4.2616081 -4.2471004 -4.2427545 -4.2393136 -4.2312436 -4.2225122 -4.2141404 -4.2110529 -4.2167106 -4.2339745 -4.2518797][-4.3268185 -4.3112893 -4.29397 -4.2802453 -4.2624531 -4.251071 -4.2469134 -4.2431655 -4.2342696 -4.2245846 -4.2157564 -4.2150774 -4.225811 -4.2471008 -4.2642851][-4.32694 -4.3111987 -4.295002 -4.2820182 -4.2650876 -4.2516103 -4.2441349 -4.2406178 -4.2344713 -4.2259932 -4.2232265 -4.2295866 -4.2459564 -4.2676167 -4.2778049][-4.3265624 -4.3110161 -4.2945595 -4.2810445 -4.2626696 -4.2439032 -4.2318611 -4.2290173 -4.2290382 -4.2279592 -4.2361727 -4.2516518 -4.2702255 -4.2854733 -4.2825918][-4.3255792 -4.3096981 -4.2919984 -4.2751584 -4.2534289 -4.2252717 -4.2022123 -4.1959128 -4.2071133 -4.2227464 -4.2466974 -4.2709947 -4.2868023 -4.2916517 -4.2751164][-4.3246274 -4.3104753 -4.2888689 -4.2636967 -4.2335467 -4.1873035 -4.1403866 -4.1293325 -4.1597934 -4.1956677 -4.2385926 -4.2715654 -4.2850795 -4.2792239 -4.2496486][-4.32565 -4.3133526 -4.2850442 -4.2499166 -4.2079096 -4.1363096 -4.0506086 -4.0343866 -4.0931153 -4.1516829 -4.2134213 -4.2534437 -4.2632656 -4.2482429 -4.2071872][-4.3283129 -4.3169622 -4.2831254 -4.2409019 -4.1875663 -4.0860786 -3.9541154 -3.93682 -4.0322542 -4.1166191 -4.1931119 -4.2361565 -4.2379236 -4.2091403 -4.1543851][-4.3322868 -4.3212557 -4.2863569 -4.2458887 -4.1900249 -4.0774159 -3.92622 -3.9120507 -4.0235319 -4.1141896 -4.1872191 -4.2257767 -4.2213626 -4.1857347 -4.1234622][-4.3354692 -4.323482 -4.2913747 -4.25679 -4.2120152 -4.12011 -4.0035806 -3.9950395 -4.0768375 -4.143611 -4.1960921 -4.223403 -4.2138834 -4.1764059 -4.1192355][-4.3383532 -4.3250546 -4.296268 -4.2689552 -4.236805 -4.1748748 -4.1030235 -4.1005058 -4.1491942 -4.1882029 -4.217864 -4.2336 -4.2212434 -4.1843443 -4.1369953][-4.3406215 -4.3277211 -4.3048825 -4.2832117 -4.2571793 -4.2156415 -4.1751084 -4.174408 -4.201376 -4.2224641 -4.2381959 -4.247129 -4.2342496 -4.1996469 -4.159596][-4.3412523 -4.3289967 -4.3120852 -4.2934017 -4.2665768 -4.2338667 -4.2082777 -4.2061458 -4.2197351 -4.2327266 -4.2456083 -4.2549896 -4.2453403 -4.2164164 -4.1830211][-4.3417449 -4.3272328 -4.3112726 -4.2934055 -4.2656932 -4.2368255 -4.2148705 -4.2096496 -4.2181416 -4.228941 -4.2436481 -4.2544913 -4.2516341 -4.2339582 -4.2091074][-4.339107 -4.3216152 -4.302526 -4.2835536 -4.2561173 -4.2275887 -4.2069855 -4.2015295 -4.21 -4.219975 -4.2339339 -4.2463269 -4.2485089 -4.2419152 -4.2310934]]...]
INFO - root - 2017-12-05 16:21:34.050834: step 25710, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 80h:25m:18s remains)
INFO - root - 2017-12-05 16:21:43.249515: step 25720, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 78h:19m:16s remains)
INFO - root - 2017-12-05 16:21:52.360901: step 25730, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.891 sec/batch; 75h:53m:30s remains)
INFO - root - 2017-12-05 16:22:01.842423: step 25740, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 77h:45m:50s remains)
INFO - root - 2017-12-05 16:22:11.220843: step 25750, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 80h:20m:33s remains)
INFO - root - 2017-12-05 16:22:20.336742: step 25760, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 79h:15m:16s remains)
INFO - root - 2017-12-05 16:22:29.665292: step 25770, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.019 sec/batch; 86h:47m:41s remains)
INFO - root - 2017-12-05 16:22:39.121793: step 25780, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 84h:15m:28s remains)
INFO - root - 2017-12-05 16:22:48.558869: step 25790, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 82h:59m:39s remains)
INFO - root - 2017-12-05 16:22:57.975400: step 25800, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 77h:41m:43s remains)
2017-12-05 16:22:58.716165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2438459 -4.2143183 -4.1927714 -4.1827617 -4.1921468 -4.2249761 -4.2550197 -4.264431 -4.2540851 -4.2354922 -4.2217369 -4.2147508 -4.2100358 -4.2048988 -4.2008615][-4.2280679 -4.192647 -4.1678319 -4.1559997 -4.166862 -4.19988 -4.228344 -4.2325721 -4.2180934 -4.19647 -4.1818042 -4.1724558 -4.1643572 -4.156044 -4.1506424][-4.2141566 -4.1704669 -4.1392584 -4.1267471 -4.1404305 -4.1716938 -4.1951313 -4.1995926 -4.1886106 -4.1735792 -4.1651821 -4.155652 -4.1425614 -4.1272631 -4.1173854][-4.2001777 -4.1480732 -4.1096306 -4.0975528 -4.117404 -4.151166 -4.1703391 -4.171495 -4.16274 -4.1577649 -4.162519 -4.1600919 -4.1443391 -4.1166782 -4.0929694][-4.1696754 -4.1163893 -4.0835485 -4.0852656 -4.114243 -4.1447816 -4.1526165 -4.1422486 -4.1260309 -4.1274028 -4.1448808 -4.1533895 -4.1383457 -4.1000352 -4.0625954][-4.1288762 -4.08815 -4.0728827 -4.0887995 -4.1162028 -4.1339135 -4.1245861 -4.0955563 -4.0692739 -4.076797 -4.1057744 -4.1272964 -4.1148186 -4.072444 -4.0335894][-4.1111431 -4.0957689 -4.094131 -4.1099563 -4.1250753 -4.1242881 -4.0894041 -4.0313034 -3.9927318 -4.0086451 -4.0477629 -4.0797863 -4.0785866 -4.0477667 -4.0273843][-4.1190968 -4.1215334 -4.1221757 -4.1273794 -4.1262722 -4.10192 -4.0321541 -3.9289315 -3.8766081 -3.9224577 -3.9865267 -4.0382614 -4.0631785 -4.0646477 -4.0702586][-4.1081133 -4.1188836 -4.1188164 -4.1154552 -4.1017323 -4.0567904 -3.9584005 -3.828629 -3.7977924 -3.8966439 -3.9914851 -4.0617137 -4.1063547 -4.1267004 -4.1325746][-4.08733 -4.1044636 -4.1104946 -4.1087 -4.0885143 -4.0431023 -3.9626749 -3.8781552 -3.889401 -3.9803379 -4.0605659 -4.1192 -4.1535935 -4.16522 -4.1567893][-4.0974307 -4.1174936 -4.1282883 -4.1293221 -4.1102686 -4.0717425 -4.0182886 -3.9797969 -4.0084648 -4.0726895 -4.1232347 -4.153985 -4.1692533 -4.1702766 -4.1524572][-4.1267838 -4.134851 -4.131465 -4.12471 -4.1070971 -4.0793157 -4.0494246 -4.0445738 -4.0851092 -4.1362348 -4.1672173 -4.1781626 -4.1804981 -4.1748919 -4.1492486][-4.1527476 -4.1462793 -4.1271644 -4.1116829 -4.0974073 -4.0843906 -4.077879 -4.0984015 -4.1449785 -4.1887665 -4.2083149 -4.2100129 -4.2056055 -4.192656 -4.1608968][-4.1896062 -4.1785078 -4.1581149 -4.1426291 -4.1303158 -4.124105 -4.1306992 -4.1595883 -4.1998606 -4.2343073 -4.2470741 -4.2473569 -4.2443132 -4.2300649 -4.201725][-4.2361674 -4.2256975 -4.2122922 -4.2044592 -4.193903 -4.1865845 -4.1945424 -4.2218523 -4.251308 -4.2721438 -4.2787843 -4.2804461 -4.2789273 -4.2679963 -4.2479162]]...]
INFO - root - 2017-12-05 16:23:08.190825: step 25810, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 84h:07m:08s remains)
INFO - root - 2017-12-05 16:23:17.144776: step 25820, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 80h:55m:35s remains)
INFO - root - 2017-12-05 16:23:26.608859: step 25830, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 80h:53m:00s remains)
INFO - root - 2017-12-05 16:23:35.985497: step 25840, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.775 sec/batch; 66h:00m:51s remains)
INFO - root - 2017-12-05 16:23:45.271982: step 25850, loss = 2.03, batch loss = 1.98 (8.4 examples/sec; 0.948 sec/batch; 80h:46m:28s remains)
INFO - root - 2017-12-05 16:23:54.631479: step 25860, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 82h:02m:20s remains)
INFO - root - 2017-12-05 16:24:03.942429: step 25870, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 73h:59m:35s remains)
INFO - root - 2017-12-05 16:24:13.261124: step 25880, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 80h:25m:29s remains)
INFO - root - 2017-12-05 16:24:22.762563: step 25890, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 77h:09m:06s remains)
INFO - root - 2017-12-05 16:24:32.158178: step 25900, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 79h:46m:21s remains)
2017-12-05 16:24:32.936488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3242459 -4.3128076 -4.3002214 -4.29272 -4.2899928 -4.2913117 -4.2962127 -4.3022437 -4.3069372 -4.3134737 -4.3194237 -4.3219013 -4.3258338 -4.332963 -4.3414578][-4.2909474 -4.2703733 -4.2516108 -4.2392988 -4.2338986 -4.2365875 -4.2439132 -4.2549143 -4.26334 -4.2746425 -4.285553 -4.2890038 -4.2952313 -4.3069725 -4.3210082][-4.267271 -4.2379889 -4.2120118 -4.1900496 -4.1787186 -4.1819572 -4.1915445 -4.2054238 -4.213573 -4.2277765 -4.2406559 -4.2435913 -4.2528439 -4.2694259 -4.2907171][-4.2606239 -4.2282743 -4.1961865 -4.1638112 -4.1484451 -4.1523781 -4.15803 -4.1662831 -4.1682611 -4.1762061 -4.18501 -4.1863971 -4.1993308 -4.2210584 -4.2505989][-4.2686086 -4.241334 -4.2085972 -4.1718659 -4.1540785 -4.1488147 -4.1411176 -4.1353226 -4.1229696 -4.1177382 -4.1198473 -4.1169653 -4.1303844 -4.1567788 -4.1983933][-4.2771659 -4.2557416 -4.224719 -4.1856675 -4.1619439 -4.1396313 -4.1132336 -4.0922894 -4.0631104 -4.0465193 -4.0441513 -4.0360956 -4.0525036 -4.0860806 -4.1421666][-4.2716093 -4.2508464 -4.2193823 -4.1747985 -4.1401672 -4.1003795 -4.0586886 -4.0296879 -3.9868374 -3.9672582 -3.9684081 -3.9629257 -3.9897516 -4.0339408 -4.1028237][-4.2409024 -4.2136312 -4.1776705 -4.1232805 -4.0745373 -4.0204186 -3.9696059 -3.939383 -3.8927453 -3.876883 -3.8884583 -3.8951082 -3.940613 -4.0017591 -4.0837159][-4.1937709 -4.1553335 -4.110301 -4.047071 -3.9888656 -3.930439 -3.8847208 -3.8672633 -3.8353119 -3.8321812 -3.8572752 -3.8813944 -3.9431369 -4.0136023 -4.0976562][-4.1595435 -4.115901 -4.0695553 -4.014183 -3.9660966 -3.9251585 -3.9027119 -3.9009767 -3.8875086 -3.8943989 -3.9239683 -3.9555581 -4.0173922 -4.0818992 -4.1539946][-4.1886168 -4.15627 -4.1243768 -4.0885587 -4.0584068 -4.0365543 -4.0296555 -4.0300879 -4.025311 -4.0354095 -4.0611391 -4.0913491 -4.1399417 -4.1864734 -4.236618][-4.2596846 -4.2472453 -4.2321429 -4.2129664 -4.1953874 -4.183249 -4.1794977 -4.1779761 -4.176754 -4.1869745 -4.2061172 -4.2282777 -4.2589712 -4.2845116 -4.3115292][-4.3183832 -4.3177228 -4.3128176 -4.3041706 -4.2957063 -4.2889671 -4.2859082 -4.2850609 -4.2854691 -4.2933269 -4.3047738 -4.3163595 -4.3310475 -4.3410358 -4.3518596][-4.353816 -4.3554664 -4.3537912 -4.3495135 -4.3458915 -4.3422794 -4.3405 -4.3404703 -4.3411374 -4.3453579 -4.3498025 -4.3535523 -4.3591957 -4.3627362 -4.3674455][-4.3687768 -4.3693328 -4.3696432 -4.3693476 -4.369256 -4.3682084 -4.3677249 -4.3675141 -4.3670225 -4.36855 -4.3695412 -4.3698311 -4.3715272 -4.3730574 -4.3750772]]...]
INFO - root - 2017-12-05 16:24:42.215607: step 25910, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 75h:09m:11s remains)
INFO - root - 2017-12-05 16:24:51.610921: step 25920, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 82h:21m:06s remains)
INFO - root - 2017-12-05 16:25:00.808411: step 25930, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 78h:57m:24s remains)
INFO - root - 2017-12-05 16:25:10.175119: step 25940, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 83h:09m:06s remains)
INFO - root - 2017-12-05 16:25:19.566801: step 25950, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 79h:43m:29s remains)
INFO - root - 2017-12-05 16:25:28.762871: step 25960, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 78h:17m:48s remains)
INFO - root - 2017-12-05 16:25:38.079417: step 25970, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 79h:59m:44s remains)
INFO - root - 2017-12-05 16:25:47.505302: step 25980, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 80h:56m:32s remains)
INFO - root - 2017-12-05 16:25:56.758113: step 25990, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.904 sec/batch; 76h:57m:08s remains)
INFO - root - 2017-12-05 16:26:05.857386: step 26000, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 76h:05m:59s remains)
2017-12-05 16:26:06.586970: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3316836 -4.3205013 -4.3050351 -4.2833633 -4.2498422 -4.2041211 -4.1573162 -4.1262774 -4.1247754 -4.1344948 -4.1289868 -4.1046548 -4.0841322 -4.0889511 -4.1397552][-4.3306189 -4.31786 -4.2981362 -4.268198 -4.2262616 -4.1716404 -4.1211419 -4.0968018 -4.10374 -4.11882 -4.1162372 -4.0872812 -4.0593057 -4.0626874 -4.1187835][-4.3290954 -4.316257 -4.2955909 -4.2633319 -4.2197905 -4.1606116 -4.1066732 -4.0853543 -4.0990453 -4.1210079 -4.1248512 -4.0994725 -4.0679846 -4.0663266 -4.1192403][-4.3297911 -4.3181496 -4.2986364 -4.2690134 -4.2298136 -4.169569 -4.1134686 -4.0944982 -4.1126595 -4.1392527 -4.15047 -4.1323347 -4.0999017 -4.0912924 -4.1360683][-4.3306108 -4.3199773 -4.302177 -4.2749543 -4.2377934 -4.1773777 -4.1208014 -4.1092076 -4.1337614 -4.1624308 -4.1767178 -4.1646748 -4.136724 -4.1233964 -4.159626][-4.3288283 -4.3177457 -4.2993007 -4.2720861 -4.2331557 -4.16848 -4.1120944 -4.113657 -4.1480093 -4.1796694 -4.1948657 -4.1860175 -4.1638179 -4.1496925 -4.1806116][-4.3274827 -4.315331 -4.2967782 -4.2681279 -4.221427 -4.1463928 -4.0897884 -4.1029649 -4.1467385 -4.1833677 -4.2005363 -4.1933379 -4.1752243 -4.1637731 -4.1931396][-4.3276377 -4.3149571 -4.2955818 -4.2640972 -4.2103124 -4.130373 -4.0780916 -4.0952148 -4.1428261 -4.18131 -4.1959314 -4.1887069 -4.174356 -4.1694436 -4.2001858][-4.328805 -4.3161459 -4.2954226 -4.2642579 -4.212605 -4.1418796 -4.0977345 -4.1084666 -4.147222 -4.1773448 -4.1855354 -4.1793184 -4.1710334 -4.1728578 -4.2044592][-4.3303943 -4.3191123 -4.2998133 -4.272295 -4.2270455 -4.1689711 -4.1290255 -4.1278028 -4.1529865 -4.1742358 -4.1796961 -4.1792812 -4.1774445 -4.1819592 -4.211555][-4.3311892 -4.3236842 -4.3090525 -4.2871342 -4.2482986 -4.19819 -4.15869 -4.1455135 -4.1586509 -4.1748204 -4.1834364 -4.1915312 -4.1938653 -4.1993942 -4.2250066][-4.3320217 -4.3277497 -4.3183393 -4.302906 -4.2713866 -4.2277794 -4.1884112 -4.1679077 -4.1720858 -4.1842523 -4.1935058 -4.2046943 -4.2095652 -4.2161045 -4.2387447][-4.3321323 -4.3295007 -4.3246226 -4.315464 -4.2927389 -4.259182 -4.2249656 -4.2044353 -4.2028356 -4.2084384 -4.2129664 -4.2221727 -4.2271895 -4.23357 -4.2531562][-4.3346558 -4.3352456 -4.33637 -4.3345504 -4.3216429 -4.2991228 -4.2720451 -4.254909 -4.2498503 -4.2481608 -4.2465019 -4.2509408 -4.2546082 -4.2597127 -4.2748981][-4.3391671 -4.3424644 -4.3490067 -4.3535748 -4.3483458 -4.3353667 -4.3167086 -4.3050895 -4.299408 -4.293427 -4.2867966 -4.2851653 -4.2850866 -4.2878251 -4.2973518]]...]
INFO - root - 2017-12-05 16:26:15.717176: step 26010, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.980 sec/batch; 83h:24m:56s remains)
INFO - root - 2017-12-05 16:26:25.010924: step 26020, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 73h:42m:32s remains)
INFO - root - 2017-12-05 16:26:34.320549: step 26030, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 82h:55m:49s remains)
INFO - root - 2017-12-05 16:26:43.588239: step 26040, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 81h:28m:14s remains)
INFO - root - 2017-12-05 16:26:52.977523: step 26050, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 78h:57m:30s remains)
INFO - root - 2017-12-05 16:27:02.336139: step 26060, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 80h:09m:28s remains)
INFO - root - 2017-12-05 16:27:11.884955: step 26070, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 81h:21m:45s remains)
INFO - root - 2017-12-05 16:27:21.227138: step 26080, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.933 sec/batch; 79h:22m:18s remains)
INFO - root - 2017-12-05 16:27:30.582897: step 26090, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 79h:51m:04s remains)
INFO - root - 2017-12-05 16:27:40.078660: step 26100, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.000 sec/batch; 85h:05m:54s remains)
2017-12-05 16:27:40.881529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2696443 -4.2668834 -4.2715573 -4.2838969 -4.2915869 -4.2768054 -4.241971 -4.2114186 -4.20018 -4.2013845 -4.2185583 -4.24382 -4.2629037 -4.2509508 -4.2194138][-4.2681184 -4.2616916 -4.2634072 -4.2743154 -4.2782545 -4.2574835 -4.2178884 -4.1841054 -4.1738577 -4.1871834 -4.2169342 -4.2475553 -4.2633142 -4.2496133 -4.2192569][-4.26894 -4.2577219 -4.2583952 -4.266974 -4.2639527 -4.2372928 -4.1964788 -4.1580129 -4.152586 -4.1839223 -4.223083 -4.2522607 -4.2652516 -4.253016 -4.2242603][-4.2775993 -4.2650027 -4.2648888 -4.2689619 -4.2550774 -4.2222433 -4.1783986 -4.1320558 -4.1384735 -4.1899123 -4.2328205 -4.2564206 -4.27206 -4.2650595 -4.2391353][-4.2843862 -4.2711935 -4.2684622 -4.267899 -4.2440157 -4.2042212 -4.1507516 -4.0975194 -4.1182542 -4.1913848 -4.2402968 -4.260458 -4.2782497 -4.2793069 -4.2579079][-4.28513 -4.2730174 -4.2728267 -4.2654576 -4.2290955 -4.177712 -4.1062155 -4.0436411 -4.0880437 -4.1830559 -4.2405925 -4.2663178 -4.2847109 -4.28822 -4.2672234][-4.2863383 -4.276093 -4.2748437 -4.259016 -4.2077727 -4.1424012 -4.0470176 -3.981231 -4.0549636 -4.1717081 -4.2426004 -4.2788568 -4.2966137 -4.2986045 -4.2706547][-4.2934523 -4.28594 -4.2796245 -4.2562714 -4.1950293 -4.1167088 -4.0021305 -3.9403555 -4.0381708 -4.1640697 -4.246748 -4.290596 -4.3088388 -4.3079505 -4.2731438][-4.302474 -4.2992592 -4.2891016 -4.2607565 -4.196661 -4.1130886 -3.998477 -3.9533093 -4.056324 -4.170311 -4.2486725 -4.2911348 -4.3099489 -4.3062224 -4.2690573][-4.308785 -4.3071995 -4.2951016 -4.2666211 -4.209806 -4.1366849 -4.0447068 -4.0229459 -4.1118197 -4.1981707 -4.2628994 -4.2957578 -4.311008 -4.3025661 -4.2590685][-4.317101 -4.3173156 -4.3058691 -4.2803297 -4.2337136 -4.1791992 -4.1119671 -4.1051669 -4.1750131 -4.23687 -4.286314 -4.3101482 -4.316618 -4.2992187 -4.2480021][-4.3249717 -4.3275251 -4.3203435 -4.3018408 -4.26624 -4.2233014 -4.1748972 -4.1783834 -4.2343183 -4.2813282 -4.3136687 -4.3250051 -4.3230362 -4.2976708 -4.2464676][-4.3275976 -4.3318191 -4.3291688 -4.3181157 -4.2929306 -4.2586823 -4.2241917 -4.234045 -4.2796474 -4.3176656 -4.3349614 -4.3365512 -4.3287711 -4.2977896 -4.2500091][-4.325686 -4.3289227 -4.3280416 -4.3212347 -4.303493 -4.2766604 -4.252686 -4.2674556 -4.3055277 -4.33641 -4.3425035 -4.3378558 -4.3299079 -4.3006816 -4.2578468][-4.321178 -4.3231544 -4.3211689 -4.3138638 -4.2989945 -4.2785459 -4.264852 -4.2813306 -4.3123627 -4.336894 -4.3377333 -4.3332968 -4.3299723 -4.3065262 -4.2711287]]...]
INFO - root - 2017-12-05 16:27:49.934608: step 26110, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 78h:07m:46s remains)
INFO - root - 2017-12-05 16:27:59.294620: step 26120, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.912 sec/batch; 77h:35m:55s remains)
INFO - root - 2017-12-05 16:28:08.836394: step 26130, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 77h:43m:50s remains)
INFO - root - 2017-12-05 16:28:18.277599: step 26140, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 77h:24m:47s remains)
INFO - root - 2017-12-05 16:28:27.520628: step 26150, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 72h:51m:18s remains)
INFO - root - 2017-12-05 16:28:36.881993: step 26160, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 79h:51m:39s remains)
INFO - root - 2017-12-05 16:28:46.342981: step 26170, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 83h:02m:11s remains)
INFO - root - 2017-12-05 16:28:55.658494: step 26180, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 74h:39m:49s remains)
INFO - root - 2017-12-05 16:29:05.080663: step 26190, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 80h:47m:08s remains)
INFO - root - 2017-12-05 16:29:14.529075: step 26200, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.904 sec/batch; 76h:53m:55s remains)
2017-12-05 16:29:15.360186: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25649 -4.2470021 -4.2388659 -4.2391853 -4.2458434 -4.2498384 -4.2492123 -4.2463136 -4.2419558 -4.2354069 -4.23249 -4.2344985 -4.2290583 -4.2168794 -4.2107348][-4.2178907 -4.2023487 -4.1886649 -4.1851153 -4.1910949 -4.1943359 -4.191956 -4.1879325 -4.1864152 -4.1866822 -4.1905437 -4.1950536 -4.1882553 -4.17491 -4.1686373][-4.1678982 -4.1420817 -4.1210966 -4.1141477 -4.1201944 -4.1230249 -4.1179829 -4.1123362 -4.1152754 -4.1245294 -4.1369581 -4.1444888 -4.1400127 -4.1331 -4.1339588][-4.1179614 -4.0800009 -4.0551744 -4.0508232 -4.0610962 -4.065721 -4.0590816 -4.0512185 -4.058454 -4.0780168 -4.1000528 -4.1140752 -4.1184578 -4.1211386 -4.1274776][-4.0799613 -4.0359521 -4.0136995 -4.0151553 -4.0276275 -4.0285606 -4.0142307 -4.0004334 -4.0118237 -4.0448279 -4.0804081 -4.1067085 -4.1225095 -4.1319313 -4.1388931][-4.0483928 -4.0126853 -4.0010462 -4.0069561 -4.0115852 -3.9976964 -3.9687743 -3.9492011 -3.9678288 -4.0164227 -4.0675693 -4.1070523 -4.1325159 -4.1465406 -4.1535025][-4.0295558 -4.0168386 -4.0185022 -4.0238032 -4.0159764 -3.9856448 -3.9452929 -3.9262455 -3.9514496 -4.0066371 -4.06185 -4.1051931 -4.1340537 -4.1511803 -4.1591387][-4.0290756 -4.0433531 -4.0549741 -4.05697 -4.0399051 -4.0041828 -3.9653428 -3.948966 -3.9688494 -4.0109043 -4.0535293 -4.0890403 -4.1179528 -4.1400347 -4.1537256][-4.035594 -4.0698075 -4.0854435 -4.0840812 -4.0648227 -4.0336976 -4.002284 -3.9866362 -3.9950328 -4.0159073 -4.0371642 -4.0573106 -4.0845337 -4.1155524 -4.1389422][-4.0485721 -4.0882397 -4.1010017 -4.0980325 -4.0825367 -4.0599966 -4.03675 -4.0215087 -4.0184841 -4.0208106 -4.0231009 -4.0283084 -4.0539927 -4.0952663 -4.1287193][-4.0685387 -4.0987792 -4.1052432 -4.1035032 -4.0938082 -4.0773888 -4.0577555 -4.0415583 -4.0332947 -4.0273757 -4.0205026 -4.0196557 -4.046648 -4.0936594 -4.1302385][-4.0978117 -4.112824 -4.1129255 -4.1115708 -4.10608 -4.0945339 -4.0789018 -4.0636172 -4.0544195 -4.048336 -4.0415621 -4.0406189 -4.0664072 -4.1111608 -4.1425762][-4.1363587 -4.1387463 -4.1337943 -4.1326094 -4.1307154 -4.1240287 -4.1127028 -4.0992723 -4.090251 -4.0860476 -4.0822759 -4.0827847 -4.1046138 -4.1400867 -4.1605482][-4.1796513 -4.1762958 -4.1703534 -4.1694379 -4.1692243 -4.1652069 -4.1567974 -4.1464186 -4.1400461 -4.1386042 -4.1373796 -4.1388049 -4.1532707 -4.1747627 -4.1845026][-4.2210917 -4.2179241 -4.2135615 -4.2123728 -4.211904 -4.2091618 -4.2035818 -4.19696 -4.1933055 -4.1932683 -4.1929169 -4.1940732 -4.2010503 -4.2110271 -4.2152572]]...]
INFO - root - 2017-12-05 16:29:24.638903: step 26210, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.017 sec/batch; 86h:30m:08s remains)
INFO - root - 2017-12-05 16:29:33.934222: step 26220, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 75h:22m:59s remains)
INFO - root - 2017-12-05 16:29:43.343820: step 26230, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 79h:46m:49s remains)
INFO - root - 2017-12-05 16:29:52.918166: step 26240, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 84h:10m:23s remains)
INFO - root - 2017-12-05 16:30:02.255566: step 26250, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 80h:59m:55s remains)
INFO - root - 2017-12-05 16:30:11.659795: step 26260, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 81h:17m:52s remains)
INFO - root - 2017-12-05 16:30:20.853118: step 26270, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 80h:45m:11s remains)
INFO - root - 2017-12-05 16:30:30.108499: step 26280, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 80h:32m:19s remains)
INFO - root - 2017-12-05 16:30:39.433157: step 26290, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.869 sec/batch; 73h:56m:15s remains)
INFO - root - 2017-12-05 16:30:48.595770: step 26300, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 78h:16m:50s remains)
2017-12-05 16:30:49.317253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0686464 -4.0571117 -4.0661063 -4.0950527 -4.138309 -4.1743546 -4.1597824 -4.1167531 -4.0799541 -4.0412383 -4.0040817 -3.9727144 -3.9615417 -3.9960682 -4.0694547][-4.0576854 -4.0579772 -4.0734258 -4.107296 -4.1487217 -4.1726193 -4.1476665 -4.1011748 -4.0556793 -4.0045781 -3.9501328 -3.8992944 -3.879359 -3.9227319 -4.0185747][-4.0884333 -4.1007514 -4.1124759 -4.1343994 -4.1607504 -4.1641536 -4.1289396 -4.0855861 -4.0570393 -4.0242829 -3.9742267 -3.9189568 -3.8923039 -3.9335916 -4.0239706][-4.1308508 -4.1361065 -4.1324086 -4.1359272 -4.1457462 -4.1383858 -4.1083212 -4.0760207 -4.0690742 -4.0650082 -4.040894 -4.0104718 -3.9970415 -4.0345645 -4.0899758][-4.1876855 -4.1745548 -4.1490984 -4.1283188 -4.1155772 -4.0842123 -4.03698 -4.0062861 -4.0315819 -4.0709314 -4.0891418 -4.09632 -4.1077118 -4.1355982 -4.1538839][-4.244164 -4.2171836 -4.1691628 -4.1171703 -4.055963 -3.962945 -3.847909 -3.7793584 -3.8574753 -3.9764323 -4.0537052 -4.0989709 -4.1360655 -4.1607 -4.1655226][-4.2837386 -4.2501292 -4.1866393 -4.1072569 -3.9979949 -3.8416629 -3.6427116 -3.4930801 -3.6174858 -3.8294187 -3.964607 -4.0412869 -4.0948281 -4.1278954 -4.1396017][-4.3045912 -4.2710724 -4.2026105 -4.1183105 -4.0041471 -3.858829 -3.6780388 -3.5282323 -3.6259215 -3.8173466 -3.93982 -4.010282 -4.0583677 -4.096117 -4.1173396][-4.3094244 -4.2781792 -4.2196646 -4.152801 -4.0703645 -3.9863892 -3.8887961 -3.8114483 -3.8514955 -3.9432864 -4.0007458 -4.0329795 -4.0554566 -4.0838928 -4.1047435][-4.2908 -4.2677045 -4.2228451 -4.1760716 -4.1268725 -4.0931311 -4.0575585 -4.0189886 -4.0241022 -4.0530119 -4.06946 -4.0721221 -4.0791168 -4.0960932 -4.1102939][-4.2519636 -4.2399421 -4.2072544 -4.1801429 -4.1594615 -4.1527758 -4.1443114 -4.1232605 -4.1151505 -4.118885 -4.1226077 -4.1177177 -4.1182442 -4.1308222 -4.1429591][-4.2181373 -4.2198524 -4.2046247 -4.1972561 -4.1942177 -4.1964979 -4.1941366 -4.1817775 -4.1684361 -4.1625872 -4.1612997 -4.1575651 -4.1590238 -4.170259 -4.1852918][-4.2244267 -4.2325397 -4.2318416 -4.2348924 -4.2375817 -4.2409534 -4.2393479 -4.2297649 -4.213994 -4.2058024 -4.2066092 -4.2016492 -4.2004814 -4.2103105 -4.2273946][-4.2582922 -4.2653384 -4.2700772 -4.2766514 -4.2801886 -4.281539 -4.27795 -4.2711043 -4.2616587 -4.2558002 -4.2559023 -4.2495265 -4.2460356 -4.2525287 -4.2654][-4.2916722 -4.2974057 -4.3018246 -4.3075547 -4.3083968 -4.3054919 -4.2987385 -4.2915039 -4.2859635 -4.2846694 -4.2857261 -4.282392 -4.2814021 -4.2862382 -4.294054]]...]
INFO - root - 2017-12-05 16:30:58.614397: step 26310, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 73h:10m:19s remains)
INFO - root - 2017-12-05 16:31:08.051965: step 26320, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.945 sec/batch; 80h:22m:04s remains)
INFO - root - 2017-12-05 16:31:17.314726: step 26330, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 79h:14m:56s remains)
INFO - root - 2017-12-05 16:31:26.415826: step 26340, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 73h:54m:11s remains)
INFO - root - 2017-12-05 16:31:36.078866: step 26350, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 79h:54m:55s remains)
INFO - root - 2017-12-05 16:31:45.430835: step 26360, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 79h:11m:08s remains)
INFO - root - 2017-12-05 16:31:54.866055: step 26370, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 85h:31m:43s remains)
INFO - root - 2017-12-05 16:32:04.290567: step 26380, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 76h:50m:38s remains)
INFO - root - 2017-12-05 16:32:13.419265: step 26390, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 80h:48m:42s remains)
INFO - root - 2017-12-05 16:32:22.711134: step 26400, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.966 sec/batch; 82h:06m:16s remains)
2017-12-05 16:32:23.493615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2532015 -4.2710195 -4.2954488 -4.3123717 -4.3183136 -4.3183613 -4.3112235 -4.296401 -4.2799091 -4.2723312 -4.2717872 -4.2729321 -4.2599869 -4.2370749 -4.2183809][-4.257381 -4.2708 -4.2869682 -4.2984734 -4.3064194 -4.3170924 -4.3241415 -4.3227744 -4.3161068 -4.3130417 -4.3086967 -4.2966032 -4.2661076 -4.2257786 -4.1955018][-4.2579622 -4.2631631 -4.2676039 -4.2698 -4.2721128 -4.2835312 -4.3008385 -4.3158655 -4.3261757 -4.3336897 -4.3307738 -4.31026 -4.2664905 -4.2090454 -4.1610289][-4.2554889 -4.2457232 -4.2334847 -4.2210984 -4.2100906 -4.2148232 -4.2392907 -4.2719073 -4.3024549 -4.3254147 -4.331624 -4.3151164 -4.2703743 -4.2059164 -4.1431932][-4.25325 -4.2219291 -4.1830482 -4.1434107 -4.1065941 -4.0986218 -4.133358 -4.188951 -4.2428293 -4.2867785 -4.312026 -4.308742 -4.2708607 -4.2082553 -4.1415877][-4.2498574 -4.1990819 -4.1322579 -4.0584669 -3.9873624 -3.960485 -4.0065975 -4.0907092 -4.169724 -4.2324896 -4.2744164 -4.2850723 -4.2598372 -4.2115264 -4.1576929][-4.2541533 -4.1967015 -4.1149082 -4.0169511 -3.917603 -3.8724904 -3.9239056 -4.0259662 -4.1209574 -4.1937847 -4.24432 -4.2656889 -4.256062 -4.2269688 -4.1936617][-4.270009 -4.2195292 -4.1455245 -4.0560403 -3.963567 -3.9197435 -3.958813 -4.04658 -4.131063 -4.1960163 -4.2414732 -4.2636485 -4.265202 -4.2522912 -4.2335138][-4.2785773 -4.2356668 -4.1796312 -4.1178222 -4.0556498 -4.0293989 -4.0559278 -4.1170254 -4.1768317 -4.2226028 -4.2549443 -4.2740726 -4.2817307 -4.2790265 -4.2669163][-4.2793884 -4.2403531 -4.2005825 -4.1644087 -4.1336265 -4.1274052 -4.1501703 -4.190217 -4.226778 -4.2514672 -4.2691545 -4.28492 -4.2956433 -4.2956715 -4.2829485][-4.2747326 -4.2413921 -4.2135935 -4.19588 -4.1891975 -4.1998758 -4.2220054 -4.2456203 -4.2614403 -4.2676215 -4.2720089 -4.28456 -4.29644 -4.2969222 -4.2804031][-4.2624087 -4.2393394 -4.2269769 -4.2239885 -4.2315817 -4.2500544 -4.2707362 -4.2826219 -4.2817388 -4.2716203 -4.2637038 -4.2696514 -4.2783523 -4.2767463 -4.2568622][-4.2542887 -4.2454095 -4.2489662 -4.2584372 -4.2720127 -4.2880263 -4.3006368 -4.2999339 -4.2845616 -4.2623796 -4.2458239 -4.2442632 -4.2472706 -4.2422829 -4.220562][-4.256732 -4.2612529 -4.2752714 -4.2909088 -4.302711 -4.3090935 -4.308486 -4.2960386 -4.2731395 -4.2489052 -4.2328453 -4.2268896 -4.2217975 -4.2107506 -4.184844][-4.2728453 -4.2875433 -4.3050528 -4.3192592 -4.3259292 -4.32314 -4.3118939 -4.2918 -4.2682629 -4.248466 -4.2383132 -4.230432 -4.2164035 -4.1969867 -4.1651974]]...]
INFO - root - 2017-12-05 16:32:32.863526: step 26410, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 82h:30m:47s remains)
INFO - root - 2017-12-05 16:32:42.258253: step 26420, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 75h:07m:24s remains)
INFO - root - 2017-12-05 16:32:51.676505: step 26430, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 80h:47m:52s remains)
INFO - root - 2017-12-05 16:33:00.970768: step 26440, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 78h:45m:33s remains)
INFO - root - 2017-12-05 16:33:10.052368: step 26450, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.962 sec/batch; 81h:48m:14s remains)
INFO - root - 2017-12-05 16:33:19.407906: step 26460, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 76h:15m:58s remains)
INFO - root - 2017-12-05 16:33:28.714447: step 26470, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 80h:06m:34s remains)
INFO - root - 2017-12-05 16:33:37.955602: step 26480, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.934 sec/batch; 79h:23m:48s remains)
INFO - root - 2017-12-05 16:33:47.311989: step 26490, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 79h:09m:16s remains)
INFO - root - 2017-12-05 16:33:56.454980: step 26500, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 78h:04m:44s remains)
2017-12-05 16:33:57.189455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.345613 -4.3456469 -4.3440132 -4.3413935 -4.3385134 -4.3361015 -4.3337965 -4.3313208 -4.3302736 -4.3328962 -4.3370128 -4.3394971 -4.340466 -4.340776 -4.3400073][-4.3428707 -4.3410153 -4.3360848 -4.3286605 -4.3210349 -4.3144116 -4.30768 -4.3022008 -4.3014889 -4.3089228 -4.3197179 -4.3298945 -4.3359151 -4.3389349 -4.3385882][-4.3414941 -4.3390026 -4.3307619 -4.3163452 -4.3000565 -4.28475 -4.2676525 -4.2537475 -4.2502708 -4.2624397 -4.2830153 -4.3065348 -4.3241892 -4.3343534 -4.3366132][-4.3408484 -4.3378148 -4.3255692 -4.3013654 -4.2738414 -4.2461133 -4.2140841 -4.1847897 -4.1734271 -4.1891389 -4.2239776 -4.2673922 -4.303009 -4.3250566 -4.3335576][-4.3386154 -4.3322916 -4.3125515 -4.2760944 -4.2367644 -4.1962237 -4.1466169 -4.0988345 -4.0793056 -4.1023521 -4.1548462 -4.2193785 -4.2747588 -4.3111081 -4.3282852][-4.3380251 -4.328299 -4.3015337 -4.2547131 -4.2050829 -4.1536331 -4.0847564 -4.015759 -3.9901035 -4.026443 -4.0954041 -4.1757436 -4.2462745 -4.29562 -4.3218369][-4.339118 -4.325655 -4.2948723 -4.2405372 -4.1813831 -4.1184125 -4.0315442 -3.9463747 -3.9208581 -3.972692 -4.0547142 -4.143394 -4.2228746 -4.2808666 -4.3137283][-4.3403287 -4.3247471 -4.2928872 -4.2368503 -4.1725225 -4.1015105 -4.0080228 -3.9177146 -3.8923402 -3.9525595 -4.04118 -4.1305094 -4.2107735 -4.2715588 -4.3079581][-4.3383865 -4.3199143 -4.2895184 -4.2407646 -4.1831241 -4.1180158 -4.036767 -3.9559419 -3.9260023 -3.9740391 -4.053617 -4.1361175 -4.2140274 -4.2741919 -4.3101487][-4.3259239 -4.3041024 -4.2762389 -4.2403207 -4.1992598 -4.1522379 -4.0958948 -4.0373111 -4.00807 -4.0343471 -4.0935726 -4.1627593 -4.2324977 -4.288774 -4.3215947][-4.2943497 -4.2696891 -4.2485418 -4.2313404 -4.2142491 -4.1931124 -4.1644444 -4.1323457 -4.11212 -4.1241746 -4.1619277 -4.2132397 -4.2683125 -4.3119354 -4.3354354][-4.2421255 -4.2186232 -4.2113795 -4.2180009 -4.2273145 -4.2349854 -4.2332158 -4.2221193 -4.2105207 -4.2156138 -4.2370672 -4.2694921 -4.3066263 -4.333231 -4.3454032][-4.1837664 -4.1719446 -4.1864672 -4.2178726 -4.2508659 -4.2764034 -4.2869091 -4.2838874 -4.2750664 -4.2755742 -4.2893858 -4.311893 -4.3358645 -4.3493872 -4.3513494][-4.1389246 -4.1508794 -4.1904545 -4.2413359 -4.286222 -4.3158665 -4.3249087 -4.3180132 -4.3051372 -4.3000774 -4.3091474 -4.3268433 -4.3437004 -4.3521128 -4.3505106][-4.1267238 -4.1643238 -4.2203994 -4.2758136 -4.3175735 -4.3408022 -4.3417282 -4.3251376 -4.3026309 -4.2871127 -4.2893114 -4.30411 -4.32114 -4.3337812 -4.3386536]]...]
INFO - root - 2017-12-05 16:34:06.630877: step 26510, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 80h:09m:22s remains)
INFO - root - 2017-12-05 16:34:16.102852: step 26520, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 80h:19m:47s remains)
INFO - root - 2017-12-05 16:34:25.495879: step 26530, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 79h:30m:11s remains)
INFO - root - 2017-12-05 16:34:34.891606: step 26540, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 81h:23m:06s remains)
INFO - root - 2017-12-05 16:34:44.158096: step 26550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 72h:53m:02s remains)
INFO - root - 2017-12-05 16:34:53.381929: step 26560, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 71h:54m:27s remains)
INFO - root - 2017-12-05 16:35:02.658454: step 26570, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 74h:19m:21s remains)
INFO - root - 2017-12-05 16:35:12.097757: step 26580, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 81h:50m:47s remains)
INFO - root - 2017-12-05 16:35:21.416488: step 26590, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 77h:08m:13s remains)
INFO - root - 2017-12-05 16:35:30.741819: step 26600, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 80h:17m:04s remains)
2017-12-05 16:35:31.621252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3151622 -4.3068819 -4.2984576 -4.2941523 -4.2952523 -4.299149 -4.2993021 -4.2929626 -4.2839847 -4.2799964 -4.2839713 -4.2949553 -4.3059192 -4.3118305 -4.3124948][-4.3256965 -4.3130026 -4.2969933 -4.2838788 -4.2763333 -4.2743421 -4.2770286 -4.27643 -4.2713323 -4.267921 -4.269578 -4.27886 -4.292026 -4.3032403 -4.3099976][-4.3239512 -4.3060303 -4.2830968 -4.2617989 -4.2448816 -4.2332039 -4.2326217 -4.2350841 -4.2360892 -4.2370682 -4.2381315 -4.2453122 -4.257916 -4.2726769 -4.2882285][-4.312727 -4.289125 -4.2580252 -4.227663 -4.1993876 -4.1746969 -4.1662169 -4.1685872 -4.1777658 -4.187921 -4.1941218 -4.2020721 -4.2135639 -4.2283473 -4.248929][-4.2985048 -4.2668004 -4.2260008 -4.1857958 -4.1463022 -4.1038113 -4.0829282 -4.0888848 -4.1109776 -4.1352553 -4.1498332 -4.1569104 -4.1622725 -4.1746879 -4.1961613][-4.2834358 -4.2428751 -4.1923356 -4.1384 -4.080946 -4.0126863 -3.9665947 -3.975914 -4.0233989 -4.0669069 -4.0854769 -4.0863233 -4.0807943 -4.0943975 -4.1200113][-4.2613497 -4.2163715 -4.1609693 -4.0970283 -4.0209713 -3.9233868 -3.8397248 -3.8409431 -3.91757 -3.9875262 -4.0143905 -4.0064836 -3.9873364 -3.9983313 -4.0323896][-4.233819 -4.193388 -4.149395 -4.0973353 -4.030499 -3.9376817 -3.8452623 -3.8294954 -3.9062409 -3.9835427 -4.01904 -4.0129814 -3.9841161 -3.9783466 -4.0029726][-4.2090425 -4.1920166 -4.1743693 -4.1501908 -4.1099133 -4.0506034 -3.9917293 -3.9766927 -4.0225868 -4.0751424 -4.1034727 -4.1038356 -4.0828195 -4.0691633 -4.0764589][-4.1761274 -4.1871333 -4.2004862 -4.2051878 -4.194006 -4.1674519 -4.1398149 -4.1327891 -4.1519909 -4.1769514 -4.1923065 -4.1951804 -4.1836085 -4.1713309 -4.1720858][-4.1377964 -4.1653261 -4.1992626 -4.2250605 -4.2335515 -4.2294617 -4.2224803 -4.2218566 -4.2293692 -4.2391162 -4.2484126 -4.2527647 -4.2503548 -4.2439456 -4.2439666][-4.138196 -4.166831 -4.2023473 -4.2315331 -4.24693 -4.2526851 -4.2541032 -4.2561569 -4.2603784 -4.2652583 -4.2743082 -4.283793 -4.2883449 -4.2885818 -4.2893939][-4.18176 -4.2012606 -4.2233596 -4.2422071 -4.2550392 -4.2626095 -4.2671952 -4.2711387 -4.2755351 -4.2812185 -4.2900076 -4.3001857 -4.3070216 -4.3090258 -4.3090529][-4.2307825 -4.24034 -4.2490773 -4.2547479 -4.2606068 -4.2652411 -4.2697678 -4.2759528 -4.2828512 -4.290834 -4.2999024 -4.3083119 -4.3139024 -4.315392 -4.31445][-4.26932 -4.2752366 -4.2778158 -4.2769027 -4.2775912 -4.2795653 -4.2827754 -4.2892981 -4.2982922 -4.3074579 -4.3148789 -4.3193417 -4.3209615 -4.3197703 -4.3174562]]...]
INFO - root - 2017-12-05 16:35:41.243618: step 26610, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 79h:00m:39s remains)
INFO - root - 2017-12-05 16:35:50.539073: step 26620, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 77h:29m:48s remains)
INFO - root - 2017-12-05 16:35:59.955628: step 26630, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 79h:32m:01s remains)
INFO - root - 2017-12-05 16:36:09.279514: step 26640, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 78h:32m:57s remains)
INFO - root - 2017-12-05 16:36:18.737633: step 26650, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 80h:50m:00s remains)
INFO - root - 2017-12-05 16:36:28.187984: step 26660, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 79h:22m:13s remains)
INFO - root - 2017-12-05 16:36:37.504500: step 26670, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 77h:36m:56s remains)
INFO - root - 2017-12-05 16:36:46.991422: step 26680, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 76h:14m:20s remains)
INFO - root - 2017-12-05 16:36:56.219103: step 26690, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 75h:27m:09s remains)
INFO - root - 2017-12-05 16:37:05.620828: step 26700, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 79h:02m:02s remains)
2017-12-05 16:37:06.389188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3039145 -4.3096771 -4.3096461 -4.3023691 -4.2798967 -4.2559166 -4.2456422 -4.2510352 -4.2709765 -4.2924752 -4.3135247 -4.3314457 -4.3426075 -4.3464837 -4.3487611][-4.2891517 -4.298275 -4.3040643 -4.2960615 -4.2599072 -4.2178421 -4.1973591 -4.2032633 -4.2296495 -4.2614708 -4.2940168 -4.3195391 -4.336144 -4.3434629 -4.347403][-4.2732635 -4.2882442 -4.3019381 -4.2923369 -4.2409658 -4.1734877 -4.1329846 -4.136724 -4.1722169 -4.2179418 -4.2659607 -4.3029995 -4.3273892 -4.3388181 -4.34483][-4.2578897 -4.2795839 -4.2994223 -4.28757 -4.2193294 -4.1221805 -4.0539036 -4.0528431 -4.102632 -4.1659727 -4.2313447 -4.2832985 -4.3176265 -4.3332968 -4.3410034][-4.2366004 -4.2666454 -4.2908454 -4.2777715 -4.194077 -4.0660925 -3.9626503 -3.9484284 -4.0203128 -4.1082964 -4.1911607 -4.2564363 -4.30317 -4.3264956 -4.3362155][-4.2029967 -4.2462234 -4.2807112 -4.2731972 -4.183249 -4.0288625 -3.8767624 -3.8285227 -3.9196043 -4.0403094 -4.1419296 -4.2232027 -4.2844734 -4.3191781 -4.3330832][-4.1573315 -4.2214918 -4.2745018 -4.2808156 -4.1971493 -4.0310512 -3.829114 -3.7225966 -3.817915 -3.9709144 -4.0923305 -4.1896753 -4.2641535 -4.3122296 -4.3324714][-4.1004395 -4.1859188 -4.2610445 -4.2888327 -4.2269526 -4.08296 -3.8742762 -3.7255538 -3.7897434 -3.9362864 -4.0594311 -4.1628804 -4.2450585 -4.3029547 -4.33019][-4.0521441 -4.1565723 -4.249649 -4.3024898 -4.2725415 -4.1681075 -3.990351 -3.8322737 -3.8379591 -3.9397094 -4.0442796 -4.144619 -4.2289052 -4.2917252 -4.3249559][-4.042366 -4.1563134 -4.2571168 -4.3217807 -4.3145418 -4.240654 -4.0948858 -3.9445312 -3.9106326 -3.9679966 -4.0526347 -4.1454611 -4.2243543 -4.2845273 -4.3197227][-4.0758004 -4.1827421 -4.2757869 -4.3336945 -4.3367372 -4.2821708 -4.16452 -4.0350628 -3.9892077 -4.0217266 -4.0904493 -4.1715083 -4.2374158 -4.2878213 -4.3185039][-4.1313 -4.216311 -4.292028 -4.3356309 -4.3399858 -4.3038359 -4.2133384 -4.1114511 -4.0675664 -4.0855017 -4.139698 -4.2072897 -4.2592592 -4.2986512 -4.3215823][-4.1881452 -4.2458344 -4.3025208 -4.3329315 -4.3363409 -4.3152738 -4.2485685 -4.1694722 -4.1306062 -4.1381717 -4.1813951 -4.238636 -4.2812591 -4.3112783 -4.32659][-4.2309647 -4.2681751 -4.3081098 -4.3289256 -4.328229 -4.3147693 -4.2678351 -4.2073941 -4.1702266 -4.1724172 -4.210012 -4.260653 -4.2982745 -4.3222413 -4.3326626][-4.262867 -4.2843375 -4.309051 -4.3219609 -4.3194904 -4.3094354 -4.2771683 -4.233861 -4.2023335 -4.2017918 -4.2342949 -4.2793956 -4.3145351 -4.3350167 -4.3414812]]...]
INFO - root - 2017-12-05 16:37:15.748345: step 26710, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 79h:15m:37s remains)
INFO - root - 2017-12-05 16:37:25.259285: step 26720, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 80h:26m:35s remains)
INFO - root - 2017-12-05 16:37:34.445399: step 26730, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.983 sec/batch; 83h:28m:37s remains)
INFO - root - 2017-12-05 16:37:43.698570: step 26740, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 78h:35m:21s remains)
INFO - root - 2017-12-05 16:37:53.008005: step 26750, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 80h:17m:55s remains)
INFO - root - 2017-12-05 16:38:02.311402: step 26760, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 81h:06m:44s remains)
INFO - root - 2017-12-05 16:38:11.886693: step 26770, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 82h:07m:38s remains)
INFO - root - 2017-12-05 16:38:21.360196: step 26780, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 83h:40m:41s remains)
INFO - root - 2017-12-05 16:38:30.842864: step 26790, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 82h:14m:09s remains)
INFO - root - 2017-12-05 16:38:40.141309: step 26800, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 80h:30m:50s remains)
2017-12-05 16:38:40.879016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1360283 -4.1267495 -4.1090441 -4.0809236 -4.0570893 -4.060895 -4.0790067 -4.0850239 -4.0818744 -4.0865059 -4.0933876 -4.1051197 -4.1276312 -4.1704988 -4.1988153][-4.1340566 -4.1315446 -4.1298413 -4.1243625 -4.1298871 -4.1490359 -4.1612921 -4.1534858 -4.1384454 -4.1303477 -4.126976 -4.1425796 -4.1643949 -4.1997123 -4.2232819][-4.1238837 -4.1238117 -4.1271782 -4.1322608 -4.1547914 -4.1799378 -4.1891451 -4.1798167 -4.1623406 -4.1487479 -4.1401339 -4.1508622 -4.1676679 -4.1989202 -4.2246971][-4.1065149 -4.10495 -4.1059637 -4.1149106 -4.1428385 -4.1659274 -4.1644955 -4.1538219 -4.1437659 -4.1378021 -4.1308022 -4.131165 -4.139482 -4.1664109 -4.1979866][-4.0918365 -4.0957456 -4.0953159 -4.1058192 -4.1316423 -4.1392765 -4.1123943 -4.09589 -4.0977688 -4.0948262 -4.0821028 -4.0718184 -4.0808988 -4.10925 -4.1482449][-4.0811915 -4.0796518 -4.0749416 -4.079247 -4.0937414 -4.0794492 -4.0296597 -4.0077772 -4.0234437 -4.029758 -4.0135221 -3.9990375 -4.0133367 -4.0502629 -4.0944057][-4.0660172 -4.0546379 -4.0389442 -4.028244 -4.0241075 -3.9790938 -3.900408 -3.879648 -3.9346828 -3.9695582 -3.9700179 -3.9710135 -3.9925542 -4.0288167 -4.0715761][-4.0629616 -4.0513368 -4.0290618 -4.001595 -3.9569693 -3.8568559 -3.7189212 -3.706002 -3.831543 -3.9186447 -3.9556537 -4.0017371 -4.0517378 -4.0915089 -4.1197782][-4.0716252 -4.0728197 -4.064651 -4.0332222 -3.9643052 -3.8171508 -3.6133578 -3.5867336 -3.7722902 -3.898746 -3.9517851 -4.0138702 -4.0787048 -4.1265497 -4.15926][-4.1030354 -4.1084495 -4.1158352 -4.1007128 -4.0493512 -3.9332533 -3.776722 -3.7392955 -3.8491471 -3.9224684 -3.9383993 -3.9795094 -4.0355434 -4.0881777 -4.13665][-4.1421161 -4.1502528 -4.1634073 -4.1646237 -4.1401443 -4.0645413 -3.9689 -3.936049 -3.9823499 -3.9940104 -3.9541492 -3.9361897 -3.9617608 -4.0161142 -4.0808797][-4.1757612 -4.1840477 -4.1979051 -4.2088594 -4.2025824 -4.1527557 -4.09522 -4.0703292 -4.0894046 -4.0726624 -4.0019693 -3.9433875 -3.9351082 -3.9730341 -4.0329108][-4.1919975 -4.200036 -4.2168069 -4.2346458 -4.2383466 -4.2101059 -4.1738472 -4.1527615 -4.1560154 -4.1358271 -4.0662227 -4.008656 -3.9976077 -4.0172725 -4.0479126][-4.1879106 -4.1995211 -4.2133451 -4.2322931 -4.2435665 -4.2286129 -4.2098384 -4.1987505 -4.2001743 -4.1813807 -4.1255569 -4.0846014 -4.0838304 -4.1002183 -4.1134572][-4.1717229 -4.1828904 -4.1938925 -4.2112675 -4.2243891 -4.2168016 -4.2104564 -4.2133837 -4.2226262 -4.2120128 -4.1724911 -4.1433725 -4.1488695 -4.1689939 -4.1785817]]...]
INFO - root - 2017-12-05 16:38:50.282496: step 26810, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 74h:26m:33s remains)
INFO - root - 2017-12-05 16:38:59.379376: step 26820, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 71h:11m:41s remains)
INFO - root - 2017-12-05 16:39:08.760359: step 26830, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 76h:14m:48s remains)
INFO - root - 2017-12-05 16:39:18.257965: step 26840, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 81h:57m:23s remains)
INFO - root - 2017-12-05 16:39:27.805136: step 26850, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 84h:30m:24s remains)
INFO - root - 2017-12-05 16:39:37.451257: step 26860, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 84h:02m:41s remains)
INFO - root - 2017-12-05 16:39:46.855203: step 26870, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 81h:02m:16s remains)
INFO - root - 2017-12-05 16:39:56.320958: step 26880, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 77h:57m:39s remains)
INFO - root - 2017-12-05 16:40:05.700931: step 26890, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 83h:44m:34s remains)
INFO - root - 2017-12-05 16:40:15.051524: step 26900, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 78h:15m:15s remains)
2017-12-05 16:40:15.815661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2770047 -4.2786155 -4.2816191 -4.2837815 -4.2860126 -4.2884245 -4.2919307 -4.2953205 -4.2944341 -4.292954 -4.2924566 -4.2920837 -4.2930269 -4.2932467 -4.2928877][-4.260942 -4.2594271 -4.2586837 -4.25704 -4.2575169 -4.2602038 -4.2644715 -4.2673798 -4.2657175 -4.265244 -4.2655659 -4.2656784 -4.2712159 -4.2737465 -4.27298][-4.2491255 -4.2405748 -4.2330952 -4.2281585 -4.2285814 -4.2345185 -4.2427893 -4.25 -4.2532911 -4.2595081 -4.2645311 -4.2645593 -4.26877 -4.2689891 -4.2634578][-4.2454467 -4.2223921 -4.2012591 -4.1915669 -4.1948061 -4.2040439 -4.2118874 -4.22264 -4.2400861 -4.2603559 -4.2717671 -4.2708368 -4.2745585 -4.27328 -4.2614264][-4.2189136 -4.183167 -4.1492305 -4.1320739 -4.1365018 -4.1439805 -4.1396341 -4.149087 -4.191421 -4.2360659 -4.2601929 -4.2571177 -4.2539053 -4.2528825 -4.2366576][-4.1732531 -4.135016 -4.0928507 -4.0636225 -4.0588875 -4.0429544 -3.9980466 -3.9848099 -4.0589185 -4.1419177 -4.186058 -4.1895189 -4.1886539 -4.1936712 -4.1799865][-4.1368651 -4.1020908 -4.0560474 -4.016995 -3.9918389 -3.9399309 -3.8346169 -3.7674308 -3.8732984 -4.0098448 -4.0838737 -4.1042891 -4.1130419 -4.1322889 -4.1334734][-4.144877 -4.1177 -4.0783958 -4.0381985 -4.0012531 -3.9285107 -3.795157 -3.6846964 -3.7880635 -3.9570656 -4.0571551 -4.0976257 -4.1129594 -4.1405396 -4.153461][-4.2045774 -4.1894755 -4.1696243 -4.144474 -4.1226006 -4.0745058 -3.9852934 -3.9035378 -3.9537139 -4.066864 -4.1452775 -4.1806254 -4.189395 -4.2080855 -4.2137609][-4.2627282 -4.26127 -4.254108 -4.2440042 -4.2337871 -4.2046118 -4.1527786 -4.1035905 -4.1207867 -4.1766224 -4.2204742 -4.2400031 -4.2409768 -4.2506242 -4.2461514][-4.2661886 -4.274653 -4.2755866 -4.2775359 -4.2761464 -4.255372 -4.2182164 -4.1862564 -4.1871805 -4.212441 -4.2344346 -4.2445011 -4.2452011 -4.2499113 -4.2404389][-4.2367682 -4.2495871 -4.2540431 -4.2608848 -4.2650709 -4.2456479 -4.2123952 -4.1889338 -4.1844187 -4.194963 -4.208127 -4.2184553 -4.2270689 -4.2368379 -4.2334476][-4.181262 -4.2009749 -4.2049809 -4.2104726 -4.21638 -4.2014709 -4.1720018 -4.1486077 -4.1393313 -4.1435304 -4.1586475 -4.1801577 -4.2032361 -4.2268414 -4.2339945][-4.130403 -4.1583672 -4.169045 -4.1729612 -4.1754289 -4.163703 -4.1409531 -4.119966 -4.1093497 -4.1128888 -4.1288414 -4.1545343 -4.1838427 -4.213356 -4.2285109][-4.12211 -4.1540575 -4.1689644 -4.1695127 -4.1663671 -4.1547093 -4.137176 -4.1228576 -4.1144276 -4.1181211 -4.1312022 -4.1511836 -4.1770954 -4.202548 -4.2193]]...]
INFO - root - 2017-12-05 16:40:24.972105: step 26910, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 72h:41m:39s remains)
INFO - root - 2017-12-05 16:40:34.323424: step 26920, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 79h:07m:52s remains)
INFO - root - 2017-12-05 16:40:43.567401: step 26930, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.895 sec/batch; 76h:00m:18s remains)
INFO - root - 2017-12-05 16:40:52.743696: step 26940, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 80h:58m:04s remains)
INFO - root - 2017-12-05 16:41:02.077508: step 26950, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 83h:13m:53s remains)
INFO - root - 2017-12-05 16:41:11.405139: step 26960, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 75h:31m:54s remains)
INFO - root - 2017-12-05 16:41:20.779024: step 26970, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 83h:01m:13s remains)
INFO - root - 2017-12-05 16:41:30.153005: step 26980, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 75h:00m:02s remains)
INFO - root - 2017-12-05 16:41:39.570001: step 26990, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.962 sec/batch; 81h:37m:04s remains)
INFO - root - 2017-12-05 16:41:48.837620: step 27000, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 84h:00m:58s remains)
2017-12-05 16:41:49.571967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1822629 -4.1840429 -4.1671081 -4.1410565 -4.1196961 -4.1007566 -4.09176 -4.0810766 -4.0837831 -4.1022549 -4.1149573 -4.1273384 -4.1510963 -4.1855597 -4.2229581][-4.180903 -4.1851153 -4.1669636 -4.1400242 -4.1174126 -4.1003919 -4.0940542 -4.0863361 -4.0882978 -4.1035409 -4.1210017 -4.1419215 -4.1699853 -4.2021947 -4.2357764][-4.1814609 -4.1908207 -4.1790314 -4.1603818 -4.1432934 -4.1296744 -4.1257381 -4.1227031 -4.1264715 -4.139751 -4.1597676 -4.1853724 -4.2108159 -4.2339525 -4.2573652][-4.17709 -4.1905575 -4.1846509 -4.172184 -4.1569777 -4.1414456 -4.1298585 -4.1280546 -4.142416 -4.1658683 -4.1920557 -4.2165637 -4.2385607 -4.257051 -4.2716393][-4.1770329 -4.1871848 -4.1746335 -4.1524663 -4.124239 -4.0940065 -4.0672312 -4.0640483 -4.0950346 -4.1392932 -4.1776094 -4.2049713 -4.2306929 -4.251821 -4.2649684][-4.1698256 -4.1708322 -4.1428442 -4.0970592 -4.0426168 -3.9869761 -3.933125 -3.9163222 -3.9645424 -4.0426197 -4.1058536 -4.1499672 -4.1910334 -4.2231245 -4.2430925][-4.1500754 -4.13735 -4.0899472 -4.01874 -3.9387391 -3.8597836 -3.7736621 -3.7366691 -3.8088458 -3.9265254 -4.0193949 -4.08847 -4.1480179 -4.1892695 -4.2171478][-4.1341767 -4.1115 -4.0584712 -3.9859021 -3.9095256 -3.836983 -3.7522688 -3.7126026 -3.7795348 -3.8948038 -3.9889212 -4.0642204 -4.1298466 -4.1748114 -4.2074146][-4.1335692 -4.1132336 -4.0713749 -4.0217938 -3.9759507 -3.9342091 -3.8822808 -3.8579295 -3.8963852 -3.9696171 -4.0362649 -4.0967345 -4.1550164 -4.1949248 -4.2242613][-4.1666994 -4.1469488 -4.114224 -4.0838566 -4.0622478 -4.0421653 -4.0167356 -4.0077009 -4.0265779 -4.0673156 -4.1099877 -4.1552544 -4.1989088 -4.2278194 -4.2500658][-4.2251239 -4.20751 -4.1827674 -4.1623521 -4.1490235 -4.1339235 -4.12004 -4.1153483 -4.1237803 -4.1450548 -4.1741738 -4.2107625 -4.2415724 -4.25993 -4.2748094][-4.2786322 -4.2676282 -4.2511549 -4.2371206 -4.2254858 -4.2116694 -4.2017093 -4.1968865 -4.1996126 -4.2105746 -4.2283139 -4.2530856 -4.2728233 -4.2805805 -4.2872286][-4.3069267 -4.2996688 -4.2887397 -4.2810383 -4.273314 -4.26405 -4.2581491 -4.2564654 -4.2592473 -4.262578 -4.266943 -4.2784119 -4.2875319 -4.2868142 -4.2869158][-4.3098383 -4.304193 -4.2971764 -4.2935586 -4.2895722 -4.2845311 -4.2816687 -4.2839246 -4.2895594 -4.2895284 -4.2873335 -4.2885141 -4.288527 -4.283946 -4.2819624][-4.3083711 -4.3053474 -4.3005538 -4.29864 -4.2973666 -4.2937546 -4.2912235 -4.2942367 -4.2986355 -4.295301 -4.286346 -4.2786827 -4.2744603 -4.27151 -4.2723265]]...]
INFO - root - 2017-12-05 16:41:58.587122: step 27010, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 78h:13m:29s remains)
INFO - root - 2017-12-05 16:42:07.943402: step 27020, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 77h:58m:29s remains)
INFO - root - 2017-12-05 16:42:17.310931: step 27030, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.945 sec/batch; 80h:13m:29s remains)
INFO - root - 2017-12-05 16:42:26.682542: step 27040, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.937 sec/batch; 79h:27m:44s remains)
INFO - root - 2017-12-05 16:42:36.041597: step 27050, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.884 sec/batch; 75h:00m:04s remains)
INFO - root - 2017-12-05 16:42:45.474283: step 27060, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.958 sec/batch; 81h:18m:06s remains)
INFO - root - 2017-12-05 16:42:55.093795: step 27070, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 77h:48m:26s remains)
INFO - root - 2017-12-05 16:43:04.305409: step 27080, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 81h:01m:07s remains)
INFO - root - 2017-12-05 16:43:13.671176: step 27090, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 81h:32m:43s remains)
INFO - root - 2017-12-05 16:43:23.103916: step 27100, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 80h:24m:20s remains)
2017-12-05 16:43:23.880844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3066173 -4.3060293 -4.3042912 -4.3022094 -4.3013992 -4.3030643 -4.3055353 -4.3068867 -4.3061657 -4.3045797 -4.3046184 -4.306129 -4.3073983 -4.3076072 -4.3063273][-4.2931066 -4.292954 -4.2910275 -4.2872105 -4.2851944 -4.2886076 -4.2938209 -4.2973218 -4.2985015 -4.2977929 -4.2986379 -4.3016586 -4.3032823 -4.3018675 -4.2969069][-4.27752 -4.2790694 -4.2765145 -4.2671113 -4.2604408 -4.2647362 -4.2737489 -4.2796583 -4.2819834 -4.2807131 -4.2824745 -4.2878718 -4.2931232 -4.2939582 -4.2867069][-4.2719941 -4.274663 -4.2694941 -4.2530518 -4.2389951 -4.2410097 -4.251646 -4.2576022 -4.2593169 -4.2577567 -4.2604089 -4.2682729 -4.2785106 -4.2850933 -4.2806988][-4.2756119 -4.2758427 -4.2681108 -4.2476916 -4.2247953 -4.22038 -4.2312508 -4.2402606 -4.2439055 -4.2400084 -4.240406 -4.2511921 -4.269042 -4.2852769 -4.2888675][-4.2800756 -4.2772255 -4.2666903 -4.2428637 -4.2138252 -4.1999073 -4.2079024 -4.21986 -4.2243271 -4.2199988 -4.2188597 -4.2323275 -4.2574773 -4.2828426 -4.2963762][-4.2822647 -4.279767 -4.2663221 -4.2391109 -4.2077847 -4.186563 -4.1885891 -4.2009296 -4.208415 -4.2055454 -4.2032905 -4.2183871 -4.2460318 -4.2730465 -4.2920995][-4.2807984 -4.2777882 -4.2648544 -4.2390323 -4.2082248 -4.1828132 -4.178093 -4.194212 -4.2069321 -4.2040453 -4.1995158 -4.2151709 -4.2414813 -4.2644625 -4.2846236][-4.2671719 -4.264369 -4.2559366 -4.2352629 -4.2076879 -4.1877275 -4.1854749 -4.2060661 -4.2236505 -4.218791 -4.2085524 -4.218039 -4.2398739 -4.259294 -4.2784138][-4.2503095 -4.2474074 -4.244503 -4.2321267 -4.2139068 -4.2046118 -4.2061615 -4.22496 -4.2458248 -4.2455173 -4.2347469 -4.2358932 -4.2496152 -4.2643056 -4.27869][-4.24218 -4.2376184 -4.2353106 -4.2271013 -4.2198482 -4.221118 -4.2266374 -4.2382283 -4.2554221 -4.2622533 -4.2581372 -4.2553835 -4.2607336 -4.2674484 -4.2735114][-4.2469792 -4.2400885 -4.2365246 -4.2263522 -4.222199 -4.2266173 -4.2313929 -4.2412696 -4.2563968 -4.2674928 -4.2702928 -4.2663569 -4.2654204 -4.2631674 -4.2592969][-4.260551 -4.2506719 -4.2430859 -4.2306466 -4.2223334 -4.2212057 -4.2214231 -4.230515 -4.2457104 -4.2620225 -4.2724662 -4.2706971 -4.2676587 -4.2604103 -4.2501273][-4.2684836 -4.2593174 -4.2499847 -4.2357893 -4.2257738 -4.2185516 -4.213253 -4.2158985 -4.2263885 -4.2448149 -4.2625575 -4.26811 -4.272646 -4.268991 -4.2544293][-4.2656832 -4.2608876 -4.2594156 -4.25121 -4.2395539 -4.22593 -4.2134089 -4.2061567 -4.2081394 -4.2202415 -4.2384939 -4.2541709 -4.2708087 -4.2751565 -4.2616529]]...]
INFO - root - 2017-12-05 16:43:33.330860: step 27110, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.946 sec/batch; 80h:13m:31s remains)
INFO - root - 2017-12-05 16:43:42.434913: step 27120, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 75h:17m:29s remains)
INFO - root - 2017-12-05 16:43:51.996296: step 27130, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 79h:50m:15s remains)
INFO - root - 2017-12-05 16:44:01.407039: step 27140, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 79h:27m:24s remains)
INFO - root - 2017-12-05 16:44:10.699858: step 27150, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.958 sec/batch; 81h:15m:00s remains)
INFO - root - 2017-12-05 16:44:19.614167: step 27160, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 77h:15m:51s remains)
INFO - root - 2017-12-05 16:44:29.230889: step 27170, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 74h:22m:41s remains)
INFO - root - 2017-12-05 16:44:38.377627: step 27180, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.941 sec/batch; 79h:47m:57s remains)
INFO - root - 2017-12-05 16:44:47.453153: step 27190, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 70h:50m:10s remains)
INFO - root - 2017-12-05 16:44:56.790003: step 27200, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 77h:00m:23s remains)
2017-12-05 16:44:57.518621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.282207 -4.2886558 -4.2958646 -4.3028994 -4.3073754 -4.3110456 -4.3141184 -4.3159227 -4.3139806 -4.3115692 -4.311141 -4.3122888 -4.3113246 -4.3089533 -4.3026695][-4.3032641 -4.3111825 -4.3205972 -4.325552 -4.3240442 -4.3186979 -4.3132181 -4.3100729 -4.3083072 -4.3065991 -4.3069358 -4.3105273 -4.3116436 -4.3079295 -4.3003511][-4.3072767 -4.3184819 -4.3322163 -4.3390269 -4.3344584 -4.3218932 -4.307673 -4.2972355 -4.2924089 -4.2898321 -4.2925148 -4.3012342 -4.306159 -4.3032789 -4.2960334][-4.2819767 -4.2995954 -4.319273 -4.3283587 -4.3214488 -4.3040843 -4.2838035 -4.2673435 -4.2579808 -4.2542963 -4.2602563 -4.2743406 -4.2852368 -4.287549 -4.2857037][-4.2241282 -4.2467046 -4.2713046 -4.2812452 -4.2723927 -4.2514148 -4.2274637 -4.20779 -4.1959796 -4.1927061 -4.2044044 -4.2264524 -4.2454381 -4.2576022 -4.2669568][-4.1552277 -4.1784563 -4.2039342 -4.214045 -4.2048039 -4.1824007 -4.1555247 -4.1351438 -4.1233325 -4.1224532 -4.1419563 -4.1742654 -4.203299 -4.2264552 -4.2482886][-4.1128912 -4.1310639 -4.1521859 -4.1612391 -4.1530008 -4.13081 -4.1012964 -4.0784321 -4.0677004 -4.0729718 -4.1018081 -4.1432257 -4.1792226 -4.2086706 -4.2373018][-4.1183949 -4.1309605 -4.1460886 -4.1524787 -4.1434908 -4.1208 -4.0884905 -4.0621543 -4.0518627 -4.0633869 -4.0974269 -4.1407447 -4.1771626 -4.2072363 -4.2342181][-4.1597071 -4.1702995 -4.1824393 -4.1858015 -4.1741862 -4.1505 -4.1173677 -4.088975 -4.0771303 -4.0893888 -4.1207762 -4.1583509 -4.1907487 -4.2174945 -4.2376304][-4.2141304 -4.2244458 -4.2349472 -4.2352133 -4.219883 -4.1965027 -4.1665592 -4.13996 -4.1266537 -4.1349077 -4.1588411 -4.1874852 -4.2136617 -4.2350197 -4.2477689][-4.2680769 -4.2780032 -4.2874236 -4.2862196 -4.2702365 -4.2494593 -4.2254329 -4.2031674 -4.1888404 -4.1910577 -4.2053165 -4.2232246 -4.2404494 -4.2556839 -4.2633247][-4.3081169 -4.3164091 -4.3248668 -4.3247871 -4.3130455 -4.2976046 -4.2802534 -4.2630048 -4.2501407 -4.2487578 -4.2549362 -4.2619529 -4.2694383 -4.2799282 -4.2857695][-4.3295674 -4.336113 -4.3434892 -4.3458705 -4.3407078 -4.3319154 -4.3205171 -4.3079596 -4.2978191 -4.2943659 -4.2941985 -4.2937264 -4.295114 -4.3030944 -4.309207][-4.3398981 -4.344429 -4.3499479 -4.3532896 -4.3526087 -4.3486762 -4.3424397 -4.3350391 -4.328845 -4.3254218 -4.3227873 -4.3201156 -4.3200555 -4.3269138 -4.3333578][-4.3451719 -4.3479762 -4.3518028 -4.3551579 -4.3567986 -4.3561196 -4.3534966 -4.35002 -4.3468647 -4.3439755 -4.3408294 -4.3385968 -4.3394132 -4.3453 -4.3511844]]...]
INFO - root - 2017-12-05 16:45:06.732403: step 27210, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 69h:52m:35s remains)
INFO - root - 2017-12-05 16:45:15.886264: step 27220, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 81h:57m:35s remains)
INFO - root - 2017-12-05 16:45:25.407399: step 27230, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 78h:41m:11s remains)
INFO - root - 2017-12-05 16:45:34.748556: step 27240, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.919 sec/batch; 77h:55m:20s remains)
INFO - root - 2017-12-05 16:45:44.161076: step 27250, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.883 sec/batch; 74h:54m:46s remains)
INFO - root - 2017-12-05 16:45:53.705515: step 27260, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 82h:47m:41s remains)
INFO - root - 2017-12-05 16:46:03.048871: step 27270, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 80h:04m:02s remains)
INFO - root - 2017-12-05 16:46:12.178258: step 27280, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 83h:32m:50s remains)
INFO - root - 2017-12-05 16:46:21.565769: step 27290, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 82h:14m:31s remains)
INFO - root - 2017-12-05 16:46:31.055598: step 27300, loss = 2.05, batch loss = 2.00 (7.9 examples/sec; 1.013 sec/batch; 85h:53m:17s remains)
2017-12-05 16:46:31.844849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1868019 -4.2102685 -4.2292361 -4.2415867 -4.2486639 -4.2557497 -4.261179 -4.2621551 -4.2610478 -4.2572727 -4.25253 -4.2446833 -4.2321849 -4.2177777 -4.1997237][-4.2104683 -4.2304606 -4.2386646 -4.2416511 -4.2379389 -4.2389565 -4.23875 -4.2322164 -4.2247405 -4.2195067 -4.2171783 -4.2137561 -4.2057929 -4.1978292 -4.1806755][-4.2286806 -4.2424164 -4.241971 -4.2366471 -4.2264047 -4.2167606 -4.2069459 -4.1937108 -4.1830707 -4.1805696 -4.1844172 -4.1872211 -4.1876421 -4.1869349 -4.1715455][-4.2301373 -4.2383161 -4.236412 -4.2290964 -4.2126079 -4.1897383 -4.16788 -4.1487608 -4.1392288 -4.1475096 -4.1629429 -4.1716413 -4.1767812 -4.17819 -4.1608152][-4.2166948 -4.2209096 -4.22045 -4.2125473 -4.1922927 -4.1587582 -4.1227427 -4.09362 -4.0845628 -4.1067519 -4.141211 -4.1615124 -4.1704612 -4.1674829 -4.1443744][-4.2024894 -4.2067423 -4.2091279 -4.2004509 -4.171195 -4.1238203 -4.0701327 -4.026123 -4.0124817 -4.0454726 -4.0997896 -4.1387997 -4.157403 -4.1609612 -4.1432643][-4.1935759 -4.2053118 -4.2143316 -4.2059145 -4.1631794 -4.0935297 -4.0140796 -3.9472632 -3.9246726 -3.9751041 -4.0629745 -4.1318517 -4.1686606 -4.1804447 -4.170434][-4.1972184 -4.2182508 -4.2337122 -4.2275062 -4.1792474 -4.098959 -4.0000648 -3.9073229 -3.8653338 -3.929877 -4.0438776 -4.132297 -4.1806374 -4.199111 -4.1933384][-4.2142248 -4.2378864 -4.2556753 -4.2511334 -4.2090445 -4.1400032 -4.0502496 -3.9612808 -3.9087231 -3.956986 -4.0610332 -4.1435986 -4.1925187 -4.2114105 -4.2090144][-4.2349358 -4.2512517 -4.265882 -4.266016 -4.2398181 -4.1949019 -4.1393814 -4.0835943 -4.0473247 -4.0687876 -4.1273909 -4.1777287 -4.2082205 -4.2201285 -4.2248344][-4.2588291 -4.2642188 -4.2708659 -4.2732506 -4.26148 -4.2399111 -4.2143908 -4.1864762 -4.1705813 -4.1794219 -4.2015624 -4.2217183 -4.236547 -4.2436695 -4.2498212][-4.2773628 -4.2785258 -4.2789021 -4.2751741 -4.2675714 -4.26017 -4.2548165 -4.2463427 -4.2433376 -4.2518911 -4.2605543 -4.2641783 -4.2659059 -4.2642083 -4.2630277][-4.2841187 -4.28391 -4.2811174 -4.2743196 -4.2677755 -4.2665811 -4.2708149 -4.2747073 -4.2828703 -4.2931695 -4.2955804 -4.2891746 -4.2776184 -4.2633848 -4.2519116][-4.2815418 -4.2809486 -4.2780776 -4.2719359 -4.2689295 -4.2707663 -4.2773294 -4.2861071 -4.2982993 -4.3069806 -4.305933 -4.2943072 -4.2727485 -4.249258 -4.231298][-4.2850909 -4.2857289 -4.2857609 -4.2843356 -4.2839155 -4.2845554 -4.2891803 -4.2977176 -4.3076577 -4.3131185 -4.3100967 -4.2979097 -4.2764354 -4.2509308 -4.2300696]]...]
INFO - root - 2017-12-05 16:46:41.240255: step 27310, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 80h:05m:14s remains)
INFO - root - 2017-12-05 16:46:50.715813: step 27320, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 80h:16m:44s remains)
INFO - root - 2017-12-05 16:46:59.900863: step 27330, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 76h:16m:26s remains)
INFO - root - 2017-12-05 16:47:09.198791: step 27340, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 78h:53m:31s remains)
INFO - root - 2017-12-05 16:47:18.401608: step 27350, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 80h:48m:31s remains)
INFO - root - 2017-12-05 16:47:27.924557: step 27360, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 73h:34m:30s remains)
INFO - root - 2017-12-05 16:47:36.927532: step 27370, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 71h:18m:41s remains)
INFO - root - 2017-12-05 16:47:46.320411: step 27380, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 77h:00m:42s remains)
INFO - root - 2017-12-05 16:47:55.480111: step 27390, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 79h:03m:30s remains)
INFO - root - 2017-12-05 16:48:04.786205: step 27400, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 81h:02m:17s remains)
2017-12-05 16:48:05.514973: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2204151 -4.1540036 -4.0856962 -4.0459795 -4.06096 -4.1094723 -4.1712494 -4.2196078 -4.2260175 -4.1963525 -4.1548424 -4.1219859 -4.1203856 -4.1406322 -4.1464224][-4.2345533 -4.174551 -4.1050673 -4.0628834 -4.0802145 -4.1319218 -4.186161 -4.2227888 -4.2319264 -4.2150741 -4.1906896 -4.1689172 -4.164093 -4.1695018 -4.1662331][-4.2427163 -4.1970906 -4.1403794 -4.1070867 -4.123486 -4.1634235 -4.20168 -4.2255259 -4.2338109 -4.229043 -4.2213621 -4.2109432 -4.2024388 -4.1952295 -4.1887665][-4.2380352 -4.2062244 -4.1660166 -4.1426287 -4.1558 -4.1803246 -4.1989079 -4.2131815 -4.2237988 -4.2332816 -4.2392716 -4.2391057 -4.2332778 -4.22027 -4.20926][-4.2264042 -4.2063007 -4.1793685 -4.1622086 -4.1669421 -4.1680746 -4.1683006 -4.178546 -4.1983914 -4.2249546 -4.2471323 -4.25426 -4.249423 -4.23417 -4.2188635][-4.2177515 -4.2092352 -4.1909642 -4.1759892 -4.1606131 -4.1332655 -4.1215525 -4.1341763 -4.162488 -4.2039185 -4.2440791 -4.2598939 -4.2560158 -4.2417803 -4.2232685][-4.2118654 -4.2081938 -4.1968637 -4.1854625 -4.1542873 -4.1001921 -4.0734472 -4.0969377 -4.1403685 -4.190134 -4.2324104 -4.2502704 -4.2481942 -4.2362251 -4.219089][-4.212275 -4.20386 -4.194109 -4.1829572 -4.1374731 -4.0609736 -4.0288844 -4.0722318 -4.1331592 -4.1820769 -4.2133069 -4.2251444 -4.2251225 -4.2206874 -4.21469][-4.2137213 -4.2023125 -4.1920109 -4.1737914 -4.1186209 -4.0358877 -4.0101657 -4.0727983 -4.141377 -4.176887 -4.1892624 -4.1919 -4.1936932 -4.1976266 -4.2052808][-4.2145886 -4.206717 -4.201848 -4.1795354 -4.1248736 -4.0569887 -4.0369544 -4.0927258 -4.1537409 -4.1732597 -4.1691327 -4.1639738 -4.1628766 -4.1757083 -4.1927471][-4.2163563 -4.2185125 -4.2236934 -4.2049479 -4.1613278 -4.1061888 -4.0783505 -4.1059985 -4.1421657 -4.1446667 -4.13225 -4.1250577 -4.133307 -4.1615791 -4.1869707][-4.2218833 -4.2321172 -4.2430429 -4.2312155 -4.1984806 -4.1509352 -4.1127748 -4.1121979 -4.121583 -4.1053886 -4.0826168 -4.0771141 -4.105628 -4.1520047 -4.1868396][-4.2296376 -4.2437348 -4.2557077 -4.2482886 -4.2212005 -4.1808562 -4.144145 -4.1277304 -4.1158967 -4.0829005 -4.0463319 -4.0435243 -4.0843372 -4.1374989 -4.1741343][-4.2299528 -4.2429919 -4.25271 -4.2460513 -4.2214365 -4.188292 -4.1589255 -4.1436491 -4.1299753 -4.0989838 -4.0687809 -4.0681529 -4.1012506 -4.1413321 -4.1666126][-4.2285028 -4.2366872 -4.2437663 -4.2373662 -4.2143621 -4.1884437 -4.1677518 -4.1564536 -4.1486712 -4.1338863 -4.1228838 -4.1254325 -4.1420193 -4.1586347 -4.1696758]]...]
INFO - root - 2017-12-05 16:48:14.907629: step 27410, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 78h:19m:45s remains)
INFO - root - 2017-12-05 16:48:24.205582: step 27420, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 72h:09m:50s remains)
INFO - root - 2017-12-05 16:48:33.478635: step 27430, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 79h:47m:17s remains)
INFO - root - 2017-12-05 16:48:42.980525: step 27440, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 81h:38m:13s remains)
INFO - root - 2017-12-05 16:48:52.505604: step 27450, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.952 sec/batch; 80h:41m:05s remains)
INFO - root - 2017-12-05 16:49:01.810643: step 27460, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.921 sec/batch; 78h:00m:43s remains)
INFO - root - 2017-12-05 16:49:10.972965: step 27470, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.004 sec/batch; 85h:03m:56s remains)
INFO - root - 2017-12-05 16:49:20.328103: step 27480, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 80h:34m:58s remains)
INFO - root - 2017-12-05 16:49:29.643542: step 27490, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 80h:10m:01s remains)
INFO - root - 2017-12-05 16:49:38.975718: step 27500, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 75h:47m:26s remains)
2017-12-05 16:49:39.790645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1588507 -4.1550384 -4.15844 -4.1731062 -4.1996517 -4.2313175 -4.2590814 -4.270195 -4.2664261 -4.2584743 -4.2388797 -4.2120609 -4.1895423 -4.1873431 -4.1998167][-4.1672592 -4.1607943 -4.1501784 -4.1572261 -4.1846862 -4.2187643 -4.2486925 -4.2637982 -4.2660966 -4.2589221 -4.2386413 -4.2108588 -4.1936851 -4.1949973 -4.20461][-4.1762948 -4.1699729 -4.1502409 -4.1426911 -4.159071 -4.1921043 -4.2260485 -4.248414 -4.2551608 -4.2482162 -4.2279167 -4.2048006 -4.200839 -4.2088041 -4.2143693][-4.1808934 -4.1742043 -4.1517081 -4.1301541 -4.1255484 -4.1461434 -4.1807566 -4.2088137 -4.2177567 -4.2111297 -4.1956177 -4.1848049 -4.198761 -4.2125082 -4.2133093][-4.1846194 -4.1777782 -4.1548948 -4.1247907 -4.0969396 -4.08926 -4.1127143 -4.1405616 -4.1588321 -4.1625624 -4.1614971 -4.171247 -4.203815 -4.2246151 -4.2224674][-4.2027745 -4.1954823 -4.1771865 -4.1433024 -4.097363 -4.0584192 -4.0537114 -4.0731683 -4.102685 -4.1273475 -4.1455545 -4.1717992 -4.2133536 -4.23776 -4.2355952][-4.2137289 -4.207356 -4.20129 -4.1739845 -4.1218958 -4.06026 -4.0140157 -4.007308 -4.0460916 -4.1021237 -4.1451578 -4.1801124 -4.2172508 -4.2362123 -4.2325678][-4.22306 -4.2148857 -4.2170582 -4.200861 -4.1538706 -4.0863566 -4.0159831 -3.9822083 -4.0282607 -4.1123133 -4.1749363 -4.2100072 -4.2329884 -4.2415533 -4.2347813][-4.2454858 -4.2342539 -4.2420783 -4.2416115 -4.2097759 -4.1546907 -4.0890384 -4.0479856 -4.0863881 -4.1718311 -4.2361059 -4.2604532 -4.2624526 -4.2584672 -4.2496824][-4.280283 -4.26728 -4.2769694 -4.2880383 -4.2732468 -4.2357316 -4.1861539 -4.1506267 -4.1705422 -4.23405 -4.2847524 -4.2976313 -4.2841964 -4.2705331 -4.2637339][-4.3004141 -4.2895246 -4.3004179 -4.3175607 -4.31451 -4.2899761 -4.2540617 -4.2243853 -4.2293787 -4.2697563 -4.3063712 -4.3118143 -4.2938414 -4.2792215 -4.2765346][-4.3040924 -4.2949867 -4.3083735 -4.3287325 -4.3347006 -4.3200426 -4.2939262 -4.2698121 -4.2665787 -4.2887383 -4.3132744 -4.3180175 -4.3061652 -4.296916 -4.2969103][-4.3187037 -4.3071284 -4.3167729 -4.3350673 -4.343267 -4.3354888 -4.3163486 -4.2971277 -4.2906804 -4.3011179 -4.3168931 -4.3229909 -4.3187327 -4.3145037 -4.3147173][-4.3291445 -4.3181453 -4.3220892 -4.3335934 -4.3385119 -4.3334146 -4.3202043 -4.3056421 -4.2979684 -4.3005815 -4.31024 -4.3173938 -4.319078 -4.3189254 -4.3199363][-4.31791 -4.310101 -4.3112855 -4.316853 -4.3198347 -4.3176 -4.3113194 -4.3034005 -4.2972 -4.2959952 -4.3003039 -4.3059764 -4.3100715 -4.3122611 -4.3145232]]...]
INFO - root - 2017-12-05 16:49:49.146012: step 27510, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 78h:27m:24s remains)
INFO - root - 2017-12-05 16:49:58.462056: step 27520, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.928 sec/batch; 78h:38m:16s remains)
INFO - root - 2017-12-05 16:50:07.781457: step 27530, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 79h:41m:00s remains)
INFO - root - 2017-12-05 16:50:17.238624: step 27540, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 78h:11m:15s remains)
INFO - root - 2017-12-05 16:50:26.568055: step 27550, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 70h:44m:38s remains)
INFO - root - 2017-12-05 16:50:35.997249: step 27560, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 76h:56m:34s remains)
INFO - root - 2017-12-05 16:50:45.488587: step 27570, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 78h:04m:14s remains)
INFO - root - 2017-12-05 16:50:54.972058: step 27580, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 83h:19m:13s remains)
INFO - root - 2017-12-05 16:51:04.482008: step 27590, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.011 sec/batch; 85h:38m:28s remains)
INFO - root - 2017-12-05 16:51:13.624321: step 27600, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 79h:22m:49s remains)
2017-12-05 16:51:14.378336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1990466 -4.2001524 -4.195581 -4.196156 -4.2131581 -4.2295823 -4.2537155 -4.2686658 -4.2573519 -4.2366533 -4.2327962 -4.2470818 -4.2649312 -4.273068 -4.2653933][-4.1962218 -4.1914787 -4.1836071 -4.18196 -4.1981268 -4.2196269 -4.2510557 -4.2790322 -4.2872963 -4.2775693 -4.271841 -4.2761016 -4.2825022 -4.2849374 -4.2826085][-4.1859403 -4.1753011 -4.1690335 -4.1669025 -4.17924 -4.2033648 -4.2367811 -4.2669287 -4.2811909 -4.2739406 -4.260807 -4.25403 -4.2526436 -4.2573147 -4.2658715][-4.1786895 -4.1608315 -4.1557088 -4.1546221 -4.1649237 -4.1884584 -4.2170939 -4.2389655 -4.244422 -4.2258964 -4.2006469 -4.1874809 -4.1877275 -4.2013993 -4.2230339][-4.165957 -4.1427813 -4.1328812 -4.1276765 -4.1391015 -4.1670508 -4.1920309 -4.20168 -4.1907363 -4.1548905 -4.1161394 -4.1022472 -4.11288 -4.1422024 -4.1773109][-4.1504393 -4.1255469 -4.1104054 -4.1003947 -4.1163292 -4.1561322 -4.1794953 -4.1739178 -4.1427064 -4.0847025 -4.029386 -4.015924 -4.0408678 -4.08897 -4.1393762][-4.1416907 -4.1229162 -4.1076112 -4.0958867 -4.1142483 -4.1608486 -4.1790905 -4.1623654 -4.1154051 -4.0423646 -3.9761584 -3.9649386 -4.0059085 -4.0698724 -4.1307826][-4.1337833 -4.1350236 -4.1286225 -4.1215453 -4.1390839 -4.1763959 -4.1843419 -4.1614861 -4.1139421 -4.0483627 -3.9899542 -3.9858479 -4.0320783 -4.0956149 -4.1528382][-4.1368842 -4.1548076 -4.1568623 -4.1567011 -4.1709647 -4.1926584 -4.191658 -4.1734304 -4.1418095 -4.1007128 -4.0658841 -4.0692482 -4.1049247 -4.1493187 -4.1899467][-4.1745529 -4.1918426 -4.19556 -4.1969485 -4.2053814 -4.2129059 -4.2101259 -4.2062182 -4.1949658 -4.1794391 -4.1647892 -4.1680145 -4.1876407 -4.2078104 -4.2261195][-4.2216792 -4.2286391 -4.2300696 -4.2345076 -4.2444158 -4.2501855 -4.2502322 -4.2530942 -4.2522435 -4.2497888 -4.2477989 -4.2472105 -4.2500825 -4.2535143 -4.2557044][-4.2623091 -4.2619481 -4.2644863 -4.2715554 -4.2835026 -4.2922807 -4.2931805 -4.2958827 -4.2973285 -4.2965236 -4.2952757 -4.2915869 -4.2870493 -4.281939 -4.27575][-4.2943115 -4.2945247 -4.29861 -4.3030252 -4.3077426 -4.3135176 -4.3149023 -4.3169651 -4.3185697 -4.317306 -4.314702 -4.309824 -4.3042235 -4.2975378 -4.2902222][-4.31272 -4.3145561 -4.31469 -4.3131042 -4.3125186 -4.3160715 -4.3174243 -4.3195968 -4.3203073 -4.3165908 -4.3121614 -4.3082561 -4.30515 -4.3020706 -4.2998662][-4.3191862 -4.3163905 -4.3120265 -4.3060665 -4.3024006 -4.3042622 -4.3079019 -4.3111243 -4.3099504 -4.3043694 -4.3006477 -4.2999606 -4.3002033 -4.3016467 -4.3046489]]...]
INFO - root - 2017-12-05 16:51:23.769793: step 27610, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 70h:52m:24s remains)
INFO - root - 2017-12-05 16:51:33.072709: step 27620, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 75h:52m:35s remains)
INFO - root - 2017-12-05 16:51:42.370083: step 27630, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 74h:31m:18s remains)
INFO - root - 2017-12-05 16:51:51.618201: step 27640, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 76h:02m:17s remains)
INFO - root - 2017-12-05 16:52:00.931704: step 27650, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.985 sec/batch; 83h:25m:48s remains)
INFO - root - 2017-12-05 16:52:10.304658: step 27660, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 82h:31m:35s remains)
INFO - root - 2017-12-05 16:52:19.631321: step 27670, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 83h:59m:20s remains)
INFO - root - 2017-12-05 16:52:29.192574: step 27680, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 80h:28m:28s remains)
INFO - root - 2017-12-05 16:52:38.588490: step 27690, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.872 sec/batch; 73h:48m:05s remains)
INFO - root - 2017-12-05 16:52:47.879072: step 27700, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 74h:23m:48s remains)
2017-12-05 16:52:48.648202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0700307 -4.075614 -4.0997458 -4.1233563 -4.1426363 -4.1571622 -4.1636558 -4.1600647 -4.1513934 -4.1500692 -4.1540179 -4.16608 -4.1946106 -4.2294097 -4.2628574][-4.1193171 -4.1284995 -4.1534371 -4.1746821 -4.1919427 -4.206243 -4.2140622 -4.210464 -4.201622 -4.1975551 -4.1951847 -4.1986284 -4.2148876 -4.2381821 -4.2626534][-4.1555991 -4.1668072 -4.1897368 -4.2053885 -4.2167897 -4.224721 -4.2283397 -4.2220745 -4.2133961 -4.2109866 -4.2121491 -4.217958 -4.2307458 -4.2468882 -4.2635603][-4.1635971 -4.1712918 -4.1852841 -4.1920009 -4.195807 -4.1943526 -4.1902256 -4.1838245 -4.1812415 -4.1869812 -4.1976376 -4.2141795 -4.231729 -4.2479134 -4.2608838][-4.1492357 -4.1518126 -4.1576 -4.1556358 -4.146389 -4.1283474 -4.1122065 -4.1041512 -4.1105285 -4.1325035 -4.160418 -4.1904225 -4.2193666 -4.2412424 -4.2558413][-4.1209536 -4.1191874 -4.1192093 -4.1119657 -4.0900884 -4.0536976 -4.0218673 -4.0078053 -4.0212603 -4.060966 -4.1138105 -4.1649866 -4.2071977 -4.2351766 -4.2530165][-4.0902662 -4.0857649 -4.0814643 -4.0706673 -4.0421453 -3.9954071 -3.9538491 -3.9373264 -3.9555469 -4.003994 -4.0745397 -4.1443644 -4.2001348 -4.2362976 -4.258471][-4.1148839 -4.1111045 -4.1049809 -4.0919552 -4.0639534 -4.0208883 -3.9841235 -3.9724884 -3.9911859 -4.0339837 -4.09712 -4.1615114 -4.2118034 -4.2464051 -4.2695045][-4.1751904 -4.1737719 -4.1691771 -4.1584563 -4.1369534 -4.1056695 -4.0777712 -4.0666 -4.0816679 -4.1140766 -4.1584668 -4.2019153 -4.2351336 -4.2593412 -4.2765632][-4.2351956 -4.2369704 -4.2346764 -4.2279587 -4.2151661 -4.1940446 -4.1739249 -4.1609449 -4.1657453 -4.1829624 -4.2076812 -4.2298589 -4.2471581 -4.2641087 -4.2774978][-4.2708664 -4.2724609 -4.2726884 -4.270658 -4.2634964 -4.2501631 -4.2346907 -4.2177405 -4.210609 -4.2158985 -4.2281232 -4.2388134 -4.2468081 -4.259099 -4.2700348][-4.2653146 -4.26546 -4.2687197 -4.2725077 -4.2721233 -4.2658911 -4.2535005 -4.2316904 -4.2174749 -4.2155929 -4.2212543 -4.2263746 -4.2320557 -4.2416644 -4.2536969][-4.2439179 -4.243257 -4.2478008 -4.2551537 -4.2591434 -4.2579026 -4.2448149 -4.2178512 -4.1991429 -4.1956062 -4.2004795 -4.2056842 -4.212503 -4.2234454 -4.2403226][-4.219224 -4.2192678 -4.2255325 -4.2331414 -4.2368937 -4.2336354 -4.2162685 -4.1873345 -4.1694169 -4.1729164 -4.1849318 -4.1962924 -4.207458 -4.2210693 -4.2419081][-4.2196617 -4.2205081 -4.2271466 -4.2336721 -4.2362905 -4.2327394 -4.2171707 -4.1917229 -4.1770654 -4.1849351 -4.1992583 -4.2124147 -4.2245421 -4.239202 -4.2596526]]...]
INFO - root - 2017-12-05 16:52:58.002724: step 27710, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.977 sec/batch; 82h:42m:18s remains)
INFO - root - 2017-12-05 16:53:07.664480: step 27720, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 81h:16m:39s remains)
INFO - root - 2017-12-05 16:53:17.276034: step 27730, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 82h:23m:35s remains)
INFO - root - 2017-12-05 16:53:26.692917: step 27740, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.960 sec/batch; 81h:14m:46s remains)
INFO - root - 2017-12-05 16:53:36.239498: step 27750, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 83h:33m:22s remains)
INFO - root - 2017-12-05 16:53:45.525880: step 27760, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.930 sec/batch; 78h:45m:04s remains)
INFO - root - 2017-12-05 16:53:55.043109: step 27770, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 82h:10m:36s remains)
INFO - root - 2017-12-05 16:54:04.619240: step 27780, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 80h:59m:41s remains)
INFO - root - 2017-12-05 16:54:13.945191: step 27790, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.990 sec/batch; 83h:48m:06s remains)
INFO - root - 2017-12-05 16:54:23.250704: step 27800, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 77h:10m:02s remains)
2017-12-05 16:54:23.988629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.240027 -4.2427659 -4.2548108 -4.2773089 -4.2980509 -4.291719 -4.2855244 -4.2853904 -4.2929816 -4.2982121 -4.3021636 -4.3015943 -4.2881017 -4.2702537 -4.2395115][-4.204607 -4.2094922 -4.2229877 -4.2479029 -4.2725997 -4.2707548 -4.273797 -4.2847 -4.2991986 -4.3073249 -4.3103347 -4.3139815 -4.296689 -4.27317 -4.2387638][-4.151546 -4.1763682 -4.2018719 -4.2303791 -4.2524986 -4.24206 -4.2464843 -4.2639585 -4.282877 -4.2932181 -4.2965641 -4.3090439 -4.3027768 -4.279181 -4.2398567][-4.1124096 -4.1663432 -4.2092123 -4.2338538 -4.2359428 -4.199482 -4.1905503 -4.218255 -4.2456093 -4.2636518 -4.2718363 -4.2900939 -4.30133 -4.289238 -4.2470737][-4.1138206 -4.1756163 -4.2177067 -4.2234044 -4.1881118 -4.1089859 -4.0722351 -4.1145282 -4.169158 -4.2171583 -4.247468 -4.2722316 -4.2969441 -4.2980046 -4.2603726][-4.1383767 -4.18263 -4.2057977 -4.1887569 -4.1134515 -3.9805543 -3.9006698 -3.9614449 -4.0668912 -4.1608787 -4.2238207 -4.2632651 -4.2928386 -4.3020654 -4.2707305][-4.16177 -4.17915 -4.1772532 -4.1390553 -4.0358191 -3.8755608 -3.7718191 -3.8513694 -3.9952843 -4.1201534 -4.2047534 -4.2556772 -4.2859173 -4.2967587 -4.2748222][-4.1764126 -4.1829562 -4.1653194 -4.1199579 -4.0326991 -3.9096072 -3.8324175 -3.8934534 -4.0142322 -4.1263461 -4.2043152 -4.2488885 -4.2720428 -4.2776837 -4.2668943][-4.1861062 -4.1909523 -4.1722493 -4.135447 -4.0901732 -4.0330586 -3.9928439 -4.0228233 -4.094141 -4.1710672 -4.2245016 -4.2524786 -4.2616448 -4.261251 -4.26232][-4.1927929 -4.196413 -4.1818461 -4.1550713 -4.1381087 -4.1288977 -4.1213212 -4.1422276 -4.1826978 -4.2277455 -4.256711 -4.2680364 -4.262217 -4.2531276 -4.2600555][-4.2096248 -4.2098064 -4.1994352 -4.1754642 -4.1665149 -4.1826386 -4.2004752 -4.2233281 -4.2480249 -4.2719646 -4.2809472 -4.2810359 -4.2593966 -4.2386436 -4.2458482][-4.2415166 -4.2387338 -4.2255993 -4.1992707 -4.1935673 -4.2197409 -4.2457404 -4.2659063 -4.2812562 -4.2919908 -4.2865672 -4.278636 -4.2528615 -4.2247462 -4.226613][-4.2706718 -4.2700634 -4.2579432 -4.2326994 -4.23189 -4.2620521 -4.2812967 -4.2881503 -4.2947268 -4.2972927 -4.2842984 -4.2746763 -4.2552929 -4.2305107 -4.22472][-4.2890706 -4.2895112 -4.28338 -4.2659321 -4.2693648 -4.2948928 -4.3049121 -4.2991495 -4.296175 -4.294136 -4.2829986 -4.2727776 -4.2571721 -4.242559 -4.2385168][-4.2917352 -4.2963939 -4.29578 -4.2858925 -4.2907038 -4.3103242 -4.3134995 -4.300806 -4.2914829 -4.2847257 -4.272069 -4.2590866 -4.2455359 -4.2382245 -4.2403531]]...]
INFO - root - 2017-12-05 16:54:33.427054: step 27810, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 84h:30m:17s remains)
INFO - root - 2017-12-05 16:54:42.870014: step 27820, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 76h:27m:59s remains)
INFO - root - 2017-12-05 16:54:52.101951: step 27830, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 80h:31m:00s remains)
INFO - root - 2017-12-05 16:55:01.627758: step 27840, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.001 sec/batch; 84h:44m:26s remains)
INFO - root - 2017-12-05 16:55:11.089049: step 27850, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 79h:30m:46s remains)
INFO - root - 2017-12-05 16:55:20.383418: step 27860, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 71h:43m:22s remains)
INFO - root - 2017-12-05 16:55:29.781874: step 27870, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 82h:51m:23s remains)
INFO - root - 2017-12-05 16:55:39.112930: step 27880, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.941 sec/batch; 79h:39m:53s remains)
INFO - root - 2017-12-05 16:55:48.432788: step 27890, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 80h:06m:39s remains)
INFO - root - 2017-12-05 16:55:57.713515: step 27900, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 80h:56m:36s remains)
2017-12-05 16:55:58.510815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3245282 -4.3333459 -4.3406215 -4.3451495 -4.3443165 -4.3408651 -4.3301959 -4.309845 -4.2843714 -4.2664366 -4.2642174 -4.2625823 -4.2608314 -4.2597575 -4.2601933][-4.31436 -4.3178358 -4.3211293 -4.3209782 -4.3163843 -4.3136778 -4.30612 -4.2898421 -4.2703228 -4.2566075 -4.2530794 -4.2508965 -4.2482 -4.2446156 -4.243413][-4.3097515 -4.3073025 -4.30776 -4.3066769 -4.3038664 -4.3025203 -4.2994356 -4.2855549 -4.2683129 -4.2557287 -4.2508678 -4.2479615 -4.2475462 -4.2473235 -4.2457976][-4.3024216 -4.2970614 -4.2963719 -4.2979345 -4.2967429 -4.2897954 -4.28204 -4.2641988 -4.24204 -4.2327833 -4.2319708 -4.2323294 -4.2409759 -4.2535205 -4.2567086][-4.2819457 -4.2737756 -4.2710004 -4.274353 -4.2735739 -4.25957 -4.23961 -4.2095132 -4.1812897 -4.1826677 -4.1920533 -4.1993403 -4.2212057 -4.251658 -4.2691426][-4.2523069 -4.2435446 -4.2439914 -4.2490134 -4.2381277 -4.2085142 -4.1679268 -4.1139255 -4.077106 -4.1037197 -4.1383572 -4.158361 -4.19088 -4.2348795 -4.2640882][-4.2131639 -4.205934 -4.2094922 -4.2076077 -4.1803546 -4.1285372 -4.0565252 -3.9508097 -3.8923757 -3.9648991 -4.0494218 -4.0952806 -4.1446471 -4.2046175 -4.2448568][-4.1734915 -4.1603527 -4.1629629 -4.1544528 -4.1130533 -4.0365362 -3.930572 -3.7654881 -3.6756 -3.8004689 -3.9436061 -4.0234628 -4.0883141 -4.1589012 -4.2077832][-4.1766639 -4.1572971 -4.1534634 -4.1392407 -4.09867 -4.0285215 -3.932657 -3.776062 -3.6906748 -3.8081763 -3.9516129 -4.033113 -4.0920334 -4.1514978 -4.1952624][-4.21466 -4.1926661 -4.1827464 -4.1640768 -4.1362715 -4.0959306 -4.039793 -3.9384217 -3.8846228 -3.9619744 -4.0683184 -4.1290636 -4.1650634 -4.1980209 -4.2225714][-4.2540236 -4.2323527 -4.2179537 -4.197576 -4.1775994 -4.1587734 -4.137043 -4.0874896 -4.0648918 -4.1115332 -4.1794505 -4.21902 -4.234169 -4.2433562 -4.25018][-4.2797222 -4.2629929 -4.253067 -4.2384329 -4.2249475 -4.2191672 -4.2171354 -4.2013173 -4.1979651 -4.2201533 -4.2497497 -4.2692804 -4.2725472 -4.2679577 -4.2646155][-4.2812657 -4.2716131 -4.269732 -4.2676258 -4.2646756 -4.2658572 -4.2722936 -4.2719674 -4.2743258 -4.2795534 -4.2855082 -4.2934909 -4.2903342 -4.2779417 -4.2656941][-4.27904 -4.2731509 -4.2755833 -4.280942 -4.285356 -4.2907071 -4.2990851 -4.3032742 -4.3073411 -4.3076215 -4.3067832 -4.3127604 -4.30732 -4.2874527 -4.2637177][-4.2864137 -4.2837725 -4.2876711 -4.2951837 -4.3033667 -4.3105989 -4.3158569 -4.3193669 -4.3224149 -4.3209438 -4.31751 -4.3225989 -4.3173409 -4.2931781 -4.262466]]...]
INFO - root - 2017-12-05 16:56:08.113622: step 27910, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 81h:02m:03s remains)
INFO - root - 2017-12-05 16:56:17.438996: step 27920, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 76h:35m:56s remains)
INFO - root - 2017-12-05 16:56:26.782764: step 27930, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 81h:32m:40s remains)
INFO - root - 2017-12-05 16:56:35.981513: step 27940, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 80h:06m:58s remains)
INFO - root - 2017-12-05 16:56:45.422966: step 27950, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 81h:01m:59s remains)
INFO - root - 2017-12-05 16:56:54.813983: step 27960, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 78h:38m:42s remains)
INFO - root - 2017-12-05 16:57:03.935237: step 27970, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 78h:14m:44s remains)
INFO - root - 2017-12-05 16:57:13.220948: step 27980, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 78h:55m:38s remains)
INFO - root - 2017-12-05 16:57:22.573036: step 27990, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 75h:42m:14s remains)
INFO - root - 2017-12-05 16:57:32.016187: step 28000, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 79h:03m:47s remains)
2017-12-05 16:57:32.736643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1941352 -4.1899548 -4.1853828 -4.1891718 -4.1940765 -4.1996064 -4.204494 -4.1940808 -4.1654711 -4.1479006 -4.1591444 -4.1823726 -4.2055016 -4.2251616 -4.2433767][-4.1901979 -4.18914 -4.1826191 -4.1861029 -4.1873274 -4.1835151 -4.1742392 -4.1520033 -4.1221285 -4.1120496 -4.1313734 -4.1605177 -4.1984115 -4.2323332 -4.25569][-4.1990733 -4.201725 -4.1981292 -4.2016697 -4.1977625 -4.1766825 -4.144321 -4.1028733 -4.0761881 -4.0858107 -4.1226087 -4.1574678 -4.2003541 -4.2350163 -4.2511024][-4.2176247 -4.220315 -4.2190046 -4.2237678 -4.21103 -4.1713843 -4.1144962 -4.0624304 -4.054986 -4.092247 -4.1457238 -4.1850052 -4.2234545 -4.2464957 -4.2427449][-4.2382817 -4.2387319 -4.2347593 -4.2318821 -4.2049456 -4.1461635 -4.066833 -4.0137286 -4.0315952 -4.0945921 -4.1608367 -4.2104344 -4.2473845 -4.2576709 -4.2293758][-4.2392216 -4.2337294 -4.2266126 -4.2184653 -4.1800838 -4.0994668 -3.9903774 -3.9341903 -3.9887767 -4.0840592 -4.1628065 -4.2230096 -4.2606459 -4.2575808 -4.2152996][-4.2149739 -4.2017932 -4.19234 -4.1818986 -4.1390481 -4.0388355 -3.8855951 -3.8172154 -3.9214275 -4.0579319 -4.155118 -4.2235856 -4.2641592 -4.2572932 -4.2138486][-4.1753969 -4.1579618 -4.1525974 -4.1502762 -4.1134014 -4.0142622 -3.8435006 -3.7646885 -3.8944836 -4.0534544 -4.1609092 -4.2318416 -4.2734175 -4.268177 -4.2250166][-4.1591411 -4.149775 -4.1499128 -4.1508346 -4.1315451 -4.0652876 -3.9296453 -3.8618479 -3.9673212 -4.0993009 -4.1913905 -4.25057 -4.2826838 -4.2729168 -4.2275][-4.1686583 -4.1719961 -4.1720505 -4.1657906 -4.1536541 -4.1129 -4.0164289 -3.9617431 -4.0386505 -4.1440811 -4.2206473 -4.2625585 -4.2790594 -4.2612185 -4.2135491][-4.1700616 -4.1852174 -4.1872845 -4.1720281 -4.1584597 -4.127399 -4.0540648 -4.0079875 -4.0705624 -4.1649356 -4.2304425 -4.2596011 -4.2632575 -4.2360897 -4.1861839][-4.1458759 -4.1722212 -4.1800494 -4.1654482 -4.152401 -4.1290231 -4.072896 -4.031642 -4.080245 -4.1617312 -4.2176604 -4.2427897 -4.2411571 -4.2085662 -4.1568508][-4.1149116 -4.1511641 -4.1664958 -4.1589851 -4.1485486 -4.1277938 -4.0836163 -4.044642 -4.0765 -4.1437211 -4.1942015 -4.2194085 -4.2158041 -4.183116 -4.1376271][-4.1070809 -4.1434994 -4.1600437 -4.1592321 -4.1517649 -4.1373458 -4.1075068 -4.0756354 -4.0939054 -4.1459346 -4.1853552 -4.2064757 -4.2071471 -4.184185 -4.1541467][-4.1222949 -4.1431365 -4.1547937 -4.1548285 -4.1517439 -4.15147 -4.1410313 -4.1226969 -4.1340222 -4.1661453 -4.1893263 -4.2033486 -4.2097015 -4.2016048 -4.1932535]]...]
INFO - root - 2017-12-05 16:57:42.266016: step 28010, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 79h:25m:50s remains)
INFO - root - 2017-12-05 16:57:51.608099: step 28020, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 74h:10m:12s remains)
INFO - root - 2017-12-05 16:58:01.134308: step 28030, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 81h:18m:10s remains)
INFO - root - 2017-12-05 16:58:10.457051: step 28040, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 78h:52m:54s remains)
INFO - root - 2017-12-05 16:58:19.786906: step 28050, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 76h:06m:19s remains)
INFO - root - 2017-12-05 16:58:28.908669: step 28060, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 81h:03m:46s remains)
INFO - root - 2017-12-05 16:58:38.483576: step 28070, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 82h:11m:38s remains)
INFO - root - 2017-12-05 16:58:47.760781: step 28080, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 73h:47m:35s remains)
INFO - root - 2017-12-05 16:58:57.003437: step 28090, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 76h:04m:02s remains)
INFO - root - 2017-12-05 16:59:06.473240: step 28100, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 77h:11m:25s remains)
2017-12-05 16:59:07.237553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2782397 -4.271327 -4.27357 -4.2796388 -4.2847204 -4.286222 -4.2841864 -4.2807608 -4.2786865 -4.2795281 -4.2820458 -4.2831659 -4.2831087 -4.2823014 -4.2824421][-4.2608075 -4.2524281 -4.2571154 -4.2667704 -4.2742958 -4.27554 -4.2717762 -4.2653069 -4.2606106 -4.2602539 -4.2627411 -4.2640057 -4.2641168 -4.2631049 -4.2643461][-4.251657 -4.2428436 -4.2492514 -4.2611766 -4.2687459 -4.2691536 -4.2654233 -4.2578883 -4.2519288 -4.2506723 -4.2529454 -4.2537813 -4.2524896 -4.2494783 -4.2508388][-4.2438188 -4.2343941 -4.2421808 -4.2553272 -4.2626061 -4.2631555 -4.2587895 -4.2483649 -4.2398543 -4.2383256 -4.241931 -4.2442403 -4.2440434 -4.240459 -4.242568][-4.22917 -4.2180562 -4.2229776 -4.2349105 -4.24267 -4.2447224 -4.2379 -4.2197623 -4.2035308 -4.1981335 -4.2033105 -4.2113891 -4.2179441 -4.218636 -4.2248306][-4.202981 -4.1908507 -4.1925735 -4.2032871 -4.2123761 -4.2161036 -4.2053328 -4.1744256 -4.1418638 -4.1250715 -4.127821 -4.1408272 -4.1575823 -4.1689811 -4.1862092][-4.1698284 -4.1592178 -4.1617627 -4.1733394 -4.1843104 -4.1889749 -4.176466 -4.1387997 -4.0929675 -4.0612726 -4.0540581 -4.0633159 -4.084661 -4.1061983 -4.1370687][-4.1275787 -4.1177154 -4.1227179 -4.1375284 -4.1530514 -4.1618567 -4.1527057 -4.1190019 -4.0735168 -4.035296 -4.0190516 -4.0203352 -4.0366611 -4.0566845 -4.0898046][-4.0904956 -4.0793581 -4.0875578 -4.1082249 -4.1313 -4.1459017 -4.1405754 -4.1151333 -4.0799861 -4.047492 -4.0319896 -4.0304661 -4.0393448 -4.0494046 -4.0707631][-4.0788507 -4.0637317 -4.0708652 -4.093051 -4.1220608 -4.1416326 -4.1407757 -4.1203332 -4.090291 -4.0643806 -4.0543871 -4.0567207 -4.0648947 -4.0710225 -4.0826693][-4.0988955 -4.0791841 -4.0801344 -4.0963845 -4.1246729 -4.14772 -4.15116 -4.1328149 -4.1024013 -4.0787368 -4.072021 -4.0776405 -4.0880256 -4.0954132 -4.1047206][-4.1297112 -4.1099334 -4.1067772 -4.1170306 -4.140276 -4.1613879 -4.1659327 -4.1497231 -4.1211205 -4.0988808 -4.0928965 -4.1008568 -4.1133413 -4.12303 -4.1330261][-4.1514311 -4.1318178 -4.1292996 -4.1408706 -4.1643858 -4.1848183 -4.18966 -4.1755605 -4.1495128 -4.128037 -4.1232643 -4.1319165 -4.1440821 -4.1536207 -4.1639295][-4.1663322 -4.144012 -4.1417751 -4.1555347 -4.1819758 -4.2067685 -4.2173648 -4.2072926 -4.1852803 -4.16497 -4.1613417 -4.1678605 -4.1745911 -4.1805596 -4.1901789][-4.1773338 -4.1562009 -4.1498995 -4.1590424 -4.1830149 -4.2122521 -4.2300587 -4.2262969 -4.2111144 -4.1939573 -4.1909223 -4.1938987 -4.1949172 -4.1957221 -4.2026758]]...]
INFO - root - 2017-12-05 16:59:16.774396: step 28110, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 82h:11m:33s remains)
INFO - root - 2017-12-05 16:59:26.333734: step 28120, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 82h:05m:01s remains)
INFO - root - 2017-12-05 16:59:35.522384: step 28130, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.915 sec/batch; 77h:21m:17s remains)
INFO - root - 2017-12-05 16:59:45.021609: step 28140, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 81h:06m:12s remains)
INFO - root - 2017-12-05 16:59:54.328011: step 28150, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 72h:21m:58s remains)
INFO - root - 2017-12-05 17:00:03.712531: step 28160, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 78h:41m:15s remains)
INFO - root - 2017-12-05 17:00:13.297233: step 28170, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 83h:26m:23s remains)
INFO - root - 2017-12-05 17:00:22.487665: step 28180, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 77h:58m:40s remains)
INFO - root - 2017-12-05 17:00:31.881564: step 28190, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 80h:07m:59s remains)
INFO - root - 2017-12-05 17:00:41.353128: step 28200, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 72h:50m:52s remains)
2017-12-05 17:00:42.147154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.27975 -4.2730269 -4.2704282 -4.2684693 -4.2558751 -4.2381878 -4.2305808 -4.2204361 -4.2036214 -4.2070007 -4.223259 -4.229857 -4.2311978 -4.2309175 -4.220469][-4.2369232 -4.2339897 -4.2397294 -4.2461104 -4.23915 -4.2196255 -4.2084084 -4.193552 -4.1752625 -4.1841125 -4.2049732 -4.2132063 -4.2131772 -4.2115579 -4.2024632][-4.1989384 -4.2010918 -4.2153087 -4.2316403 -4.2329006 -4.2147293 -4.1982942 -4.1745067 -4.1488738 -4.1601996 -4.1864448 -4.1988983 -4.1963954 -4.19056 -4.1850839][-4.1707282 -4.1761065 -4.1913152 -4.2073369 -4.2098165 -4.1923466 -4.1721 -4.1397724 -4.1039119 -4.11456 -4.1480551 -4.1675091 -4.1690722 -4.1632853 -4.1656928][-4.1631417 -4.1699777 -4.1791158 -4.1837082 -4.17091 -4.1440582 -4.1222243 -4.0926285 -4.0593033 -4.0638251 -4.0934062 -4.1189022 -4.1322827 -4.1358237 -4.1487961][-4.1597867 -4.1642203 -4.1693931 -4.16619 -4.137074 -4.0882459 -4.0547743 -4.0319376 -4.0191703 -4.0372691 -4.0667276 -4.0878577 -4.1072373 -4.1208949 -4.1360917][-4.1482325 -4.1476073 -4.1510973 -4.1523547 -4.117928 -4.0458779 -3.9787312 -3.9389677 -3.9494271 -4.0127172 -4.0662389 -4.0900126 -4.1073856 -4.1195107 -4.13232][-4.132544 -4.1281872 -4.1351156 -4.1411452 -4.1129551 -4.0379481 -3.9394569 -3.8566875 -3.8707058 -3.9763644 -4.062788 -4.0965905 -4.1082339 -4.1113582 -4.119617][-4.1301813 -4.1265249 -4.1312022 -4.1308603 -4.1038775 -4.0423708 -3.9551759 -3.8765726 -3.8872964 -3.9863534 -4.06356 -4.0947342 -4.1053309 -4.104794 -4.1116176][-4.1239386 -4.1304488 -4.1372247 -4.1258597 -4.0893617 -4.04222 -3.9923232 -3.959965 -3.9825265 -4.0452518 -4.0900064 -4.1111789 -4.1208496 -4.1182065 -4.1265268][-4.1111832 -4.1339779 -4.1544194 -4.1454315 -4.1089182 -4.0680404 -4.0418096 -4.0420723 -4.0759144 -4.121799 -4.1459913 -4.1587348 -4.1667476 -4.1639004 -4.1724753][-4.124218 -4.1529446 -4.1859035 -4.1919303 -4.170259 -4.1370845 -4.1130342 -4.1192422 -4.1575508 -4.1978612 -4.2132812 -4.219142 -4.2234764 -4.2195964 -4.2255569][-4.1767464 -4.2013416 -4.2359381 -4.2520452 -4.2454476 -4.22334 -4.2028203 -4.2022834 -4.2294712 -4.2593517 -4.2703996 -4.27078 -4.2696528 -4.2652645 -4.2684021][-4.2493477 -4.2647586 -4.2901464 -4.3050056 -4.3030906 -4.2905836 -4.2761612 -4.2708578 -4.2819839 -4.2970328 -4.3031383 -4.3048635 -4.3044696 -4.3015523 -4.3047328][-4.3043575 -4.312839 -4.3252954 -4.3339009 -4.3339806 -4.3286252 -4.3203459 -4.3142505 -4.3164468 -4.3218622 -4.3251328 -4.32912 -4.3314524 -4.3308477 -4.3332133]]...]
INFO - root - 2017-12-05 17:00:51.482837: step 28210, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 75h:20m:52s remains)
INFO - root - 2017-12-05 17:01:00.852064: step 28220, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 79h:51m:58s remains)
INFO - root - 2017-12-05 17:01:10.452415: step 28230, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 78h:09m:38s remains)
INFO - root - 2017-12-05 17:01:19.651723: step 28240, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 81h:34m:28s remains)
INFO - root - 2017-12-05 17:01:29.046759: step 28250, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 80h:51m:23s remains)
INFO - root - 2017-12-05 17:01:38.409438: step 28260, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.888 sec/batch; 75h:01m:06s remains)
INFO - root - 2017-12-05 17:01:47.826604: step 28270, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 76h:31m:22s remains)
INFO - root - 2017-12-05 17:01:57.110498: step 28280, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 77h:03m:48s remains)
INFO - root - 2017-12-05 17:02:06.431238: step 28290, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 78h:11m:37s remains)
INFO - root - 2017-12-05 17:02:15.836252: step 28300, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 76h:33m:33s remains)
2017-12-05 17:02:16.620015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3367624 -4.3480864 -4.35005 -4.3418388 -4.3227558 -4.2964282 -4.2529011 -4.2111897 -4.1798825 -4.1679444 -4.1844916 -4.2162561 -4.2414546 -4.2390413 -4.218019][-4.3457885 -4.3525133 -4.3497348 -4.3322473 -4.3056464 -4.2767625 -4.2308645 -4.1816025 -4.1389461 -4.1228271 -4.1534305 -4.200593 -4.2295642 -4.2271976 -4.2103381][-4.346683 -4.3475647 -4.3377995 -4.310503 -4.2768083 -4.2436433 -4.1941576 -4.1354151 -4.0833464 -4.0733695 -4.1221428 -4.1781859 -4.2081146 -4.2068295 -4.1970758][-4.34628 -4.3416352 -4.3267736 -4.294198 -4.2587433 -4.2203283 -4.16091 -4.0884252 -4.0319476 -4.041431 -4.1065993 -4.1667776 -4.2020044 -4.2044291 -4.1986427][-4.3462586 -4.3396564 -4.32425 -4.2928848 -4.2545681 -4.2085352 -4.1355481 -4.0456481 -3.9794726 -4.0097585 -4.0924954 -4.1639748 -4.2137175 -4.2271438 -4.2248325][-4.3402004 -4.3334894 -4.3197269 -4.2892904 -4.2434893 -4.1821122 -4.0921793 -3.9783487 -3.8950272 -3.9510229 -4.066875 -4.1631742 -4.23115 -4.2553492 -4.2587585][-4.3306384 -4.3245387 -4.3116012 -4.2793603 -4.22305 -4.14386 -4.0342383 -3.8975115 -3.8098576 -3.9028351 -4.05295 -4.1643004 -4.2419915 -4.27503 -4.2874446][-4.3233624 -4.3179817 -4.3033309 -4.2669592 -4.2004724 -4.1126242 -3.998405 -3.8724248 -3.8209512 -3.9218061 -4.0606055 -4.1617417 -4.2363544 -4.2725496 -4.2954173][-4.3161588 -4.3130474 -4.2991886 -4.2611308 -4.1958613 -4.1161723 -4.0138731 -3.9147406 -3.8937304 -3.9764879 -4.0837607 -4.1680183 -4.2308493 -4.2648721 -4.2912226][-4.314395 -4.31342 -4.303123 -4.2678113 -4.2122025 -4.1444139 -4.0545149 -3.977741 -3.9758029 -4.0407963 -4.1234751 -4.1940126 -4.2448335 -4.2709761 -4.2925167][-4.3178644 -4.3191204 -4.31506 -4.2876549 -4.242816 -4.1877518 -4.1163211 -4.063396 -4.0718956 -4.1186342 -4.1772451 -4.2300549 -4.2670045 -4.2848573 -4.2989254][-4.318337 -4.3184843 -4.3154063 -4.2948542 -4.2617068 -4.2250481 -4.1779795 -4.1488261 -4.1585283 -4.1874495 -4.223022 -4.2541442 -4.2780852 -4.2944 -4.3037271][-4.314692 -4.3101053 -4.3062634 -4.293457 -4.2723403 -4.2505836 -4.2252951 -4.2130427 -4.2214527 -4.2349958 -4.2496905 -4.266643 -4.281919 -4.2933855 -4.2997074][-4.3146853 -4.3072205 -4.3019075 -4.2931209 -4.2781458 -4.2658386 -4.2545533 -4.2514281 -4.2566705 -4.2613549 -4.266449 -4.2767839 -4.2855692 -4.2920966 -4.2964387][-4.3198915 -4.3123503 -4.3048925 -4.2963047 -4.2853637 -4.277967 -4.2731442 -4.2728395 -4.2744589 -4.2758923 -4.2805319 -4.2888613 -4.2942863 -4.2979312 -4.3011222]]...]
INFO - root - 2017-12-05 17:02:25.872320: step 28310, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 77h:38m:58s remains)
INFO - root - 2017-12-05 17:02:35.248873: step 28320, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 76h:37m:41s remains)
INFO - root - 2017-12-05 17:02:44.515679: step 28330, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.875 sec/batch; 73h:56m:16s remains)
INFO - root - 2017-12-05 17:02:53.830217: step 28340, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 78h:00m:51s remains)
INFO - root - 2017-12-05 17:03:03.179389: step 28350, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 79h:44m:16s remains)
INFO - root - 2017-12-05 17:03:12.557942: step 28360, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 83h:25m:33s remains)
INFO - root - 2017-12-05 17:03:21.970283: step 28370, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 81h:02m:08s remains)
INFO - root - 2017-12-05 17:03:31.238572: step 28380, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 78h:37m:16s remains)
INFO - root - 2017-12-05 17:03:40.667344: step 28390, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 80h:02m:40s remains)
INFO - root - 2017-12-05 17:03:50.089565: step 28400, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 79h:33m:52s remains)
2017-12-05 17:03:50.819362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3582845 -4.3518209 -4.3389659 -4.3232088 -4.31241 -4.3137474 -4.3215313 -4.3255644 -4.323164 -4.3127089 -4.2972913 -4.2828093 -4.2736158 -4.2714005 -4.2748928][-4.3573213 -4.3502626 -4.3331327 -4.3111272 -4.2965689 -4.2991748 -4.3118286 -4.3220019 -4.3276014 -4.324584 -4.3121147 -4.2974606 -4.2844524 -4.2760658 -4.2712555][-4.3440828 -4.3326983 -4.3082952 -4.276475 -4.2543917 -4.2560987 -4.2728009 -4.2917013 -4.3084717 -4.3183832 -4.3164272 -4.3089604 -4.3001394 -4.293036 -4.285964][-4.3276124 -4.3080983 -4.2722654 -4.2281251 -4.1948018 -4.1912122 -4.20653 -4.2319341 -4.2579994 -4.2789993 -4.2901788 -4.2952003 -4.2993331 -4.3042073 -4.3057027][-4.3144956 -4.2855682 -4.238389 -4.1832666 -4.1383567 -4.122973 -4.1305852 -4.1561255 -4.186316 -4.2135425 -4.235745 -4.2541971 -4.2741537 -4.2942405 -4.3083048][-4.3061433 -4.273283 -4.2218328 -4.1609044 -4.1035624 -4.0677562 -4.0568752 -4.0741968 -4.10111 -4.1302114 -4.1604342 -4.1922584 -4.2279906 -4.262991 -4.2882805][-4.2991481 -4.2680097 -4.2194247 -4.157249 -4.0899663 -4.0362716 -4.0074177 -4.012207 -4.0329385 -4.05918 -4.0927119 -4.1339784 -4.1790347 -4.2249069 -4.2604027][-4.2964182 -4.2737932 -4.2317414 -4.1734648 -4.1059093 -4.0476737 -4.0120625 -4.0094738 -4.0281124 -4.0542831 -4.0907216 -4.1348252 -4.1762681 -4.2176485 -4.2520027][-4.3023591 -4.2916512 -4.2597566 -4.2125583 -4.1596828 -4.1126919 -4.0837069 -4.0810533 -4.0981469 -4.1196375 -4.1507287 -4.1856704 -4.2133369 -4.2391844 -4.263494][-4.3146596 -4.3158779 -4.299593 -4.2722964 -4.2432041 -4.2129498 -4.1903129 -4.1849046 -4.1923337 -4.200099 -4.2153616 -4.2345552 -4.2493114 -4.2635283 -4.2799239][-4.3258142 -4.3352165 -4.3344431 -4.3272657 -4.3188128 -4.3042817 -4.2862735 -4.2739277 -4.2658668 -4.2549586 -4.2509847 -4.2555828 -4.2640195 -4.2756333 -4.2878966][-4.3321733 -4.3435206 -4.3511415 -4.3556285 -4.3573685 -4.3522491 -4.3386116 -4.3216915 -4.3012285 -4.2759495 -4.2555485 -4.2468653 -4.252645 -4.2686992 -4.2815084][-4.336688 -4.344183 -4.3519783 -4.357697 -4.3614421 -4.3604894 -4.3524489 -4.3367243 -4.3104515 -4.2761273 -4.2439218 -4.2227459 -4.2241297 -4.2422962 -4.2571387][-4.3366194 -4.3375039 -4.3403325 -4.3425937 -4.3425012 -4.3385277 -4.3323021 -4.3208594 -4.2970467 -4.2624917 -4.2275805 -4.2016711 -4.1980214 -4.2144475 -4.229301][-4.3285527 -4.3220439 -4.3176079 -4.31353 -4.3069358 -4.2978215 -4.2896137 -4.2805018 -4.2627063 -4.2363153 -4.2096343 -4.1904478 -4.1879935 -4.2035832 -4.2175412]]...]
INFO - root - 2017-12-05 17:04:00.363142: step 28410, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 81h:30m:17s remains)
INFO - root - 2017-12-05 17:04:09.725759: step 28420, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 76h:55m:27s remains)
INFO - root - 2017-12-05 17:04:19.182178: step 28430, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 78h:50m:32s remains)
INFO - root - 2017-12-05 17:04:28.284806: step 28440, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 78h:55m:18s remains)
INFO - root - 2017-12-05 17:04:37.626914: step 28450, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 78h:34m:19s remains)
INFO - root - 2017-12-05 17:04:46.966608: step 28460, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 70h:23m:37s remains)
INFO - root - 2017-12-05 17:04:56.238521: step 28470, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.948 sec/batch; 80h:01m:23s remains)
INFO - root - 2017-12-05 17:05:05.542021: step 28480, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 81h:04m:35s remains)
INFO - root - 2017-12-05 17:05:14.784696: step 28490, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 81h:50m:29s remains)
INFO - root - 2017-12-05 17:05:24.334612: step 28500, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 80h:51m:39s remains)
2017-12-05 17:05:25.172173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3030381 -4.2969251 -4.291594 -4.2890415 -4.2917557 -4.2934275 -4.2886429 -4.2806273 -4.2721291 -4.2599077 -4.2523971 -4.2573333 -4.2628036 -4.2629118 -4.2747245][-4.2974334 -4.2868595 -4.2777662 -4.2726908 -4.2741337 -4.2729983 -4.2632804 -4.2500734 -4.2408037 -4.2264719 -4.2186379 -4.2299128 -4.2439861 -4.2479982 -4.2604513][-4.2847357 -4.2718782 -4.2623887 -4.2572083 -4.2588959 -4.2548532 -4.2363658 -4.2166834 -4.2087474 -4.1956229 -4.1874142 -4.2039795 -4.2280288 -4.2401056 -4.2554374][-4.2770996 -4.263526 -4.2491455 -4.2401705 -4.2386045 -4.2264934 -4.1924129 -4.1666837 -4.1662459 -4.1606941 -4.1513433 -4.168035 -4.1984329 -4.2215123 -4.2477808][-4.2584109 -4.2448111 -4.22603 -4.2125692 -4.206749 -4.184679 -4.1339297 -4.0981307 -4.1064024 -4.1106858 -4.1000795 -4.1121645 -4.1465125 -4.1818304 -4.2258577][-4.2314348 -4.2143207 -4.1901889 -4.1705761 -4.1575656 -4.1230116 -4.0486717 -3.9903259 -4.0007277 -4.0164046 -4.0185819 -4.0424142 -4.0887179 -4.1419454 -4.2033873][-4.2037687 -4.1701188 -4.1328769 -4.1034117 -4.0775967 -4.0257053 -3.9234562 -3.8446703 -3.8641164 -3.9136174 -3.9542131 -4.0082645 -4.0769696 -4.1432586 -4.2078528][-4.1913013 -4.1409769 -4.0878978 -4.0494213 -4.0171022 -3.9576445 -3.8460832 -3.773108 -3.815022 -3.9105809 -3.99094 -4.0608745 -4.1296043 -4.19161 -4.2458692][-4.2018466 -4.145237 -4.0917277 -4.062902 -4.0484161 -4.0143213 -3.9355927 -3.8948967 -3.9403846 -4.0262232 -4.0952511 -4.1496611 -4.201839 -4.2503757 -4.2893925][-4.2335544 -4.1877089 -4.15364 -4.1422586 -4.137908 -4.1161747 -4.0707736 -4.0576844 -4.0993767 -4.1593728 -4.2054796 -4.238451 -4.2704916 -4.3017564 -4.3240905][-4.2621465 -4.232687 -4.2244625 -4.2270637 -4.2253017 -4.2133436 -4.1943226 -4.1920056 -4.220593 -4.255774 -4.2794328 -4.2937951 -4.3086085 -4.3259363 -4.337029][-4.2816219 -4.2606325 -4.2599945 -4.2676044 -4.2719636 -4.2699294 -4.2672906 -4.2687888 -4.2825956 -4.2988071 -4.3111224 -4.3189983 -4.3265004 -4.3350711 -4.3409176][-4.2985659 -4.2826939 -4.2805119 -4.2894192 -4.2987809 -4.3061051 -4.3112588 -4.3149319 -4.3206406 -4.3264022 -4.3305736 -4.332613 -4.3349638 -4.3397923 -4.343514][-4.3166223 -4.3059316 -4.3028502 -4.309834 -4.320848 -4.3303204 -4.3356419 -4.3389421 -4.341002 -4.3410196 -4.3404546 -4.34025 -4.340189 -4.3423729 -4.3447533][-4.3358345 -4.3290758 -4.3256588 -4.3284698 -4.3348622 -4.3406243 -4.3436413 -4.3451023 -4.345881 -4.3454051 -4.3449845 -4.3452759 -4.3459187 -4.3473 -4.349122]]...]
INFO - root - 2017-12-05 17:05:34.562462: step 28510, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.958 sec/batch; 80h:52m:45s remains)
INFO - root - 2017-12-05 17:05:44.101124: step 28520, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.000 sec/batch; 84h:25m:35s remains)
INFO - root - 2017-12-05 17:05:53.531740: step 28530, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 78h:50m:22s remains)
INFO - root - 2017-12-05 17:06:02.863708: step 28540, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 75h:57m:35s remains)
INFO - root - 2017-12-05 17:06:12.196885: step 28550, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 71h:48m:06s remains)
INFO - root - 2017-12-05 17:06:21.546551: step 28560, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 74h:28m:42s remains)
INFO - root - 2017-12-05 17:06:30.767061: step 28570, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 81h:19m:47s remains)
INFO - root - 2017-12-05 17:06:39.841448: step 28580, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 80h:18m:21s remains)
INFO - root - 2017-12-05 17:06:49.307087: step 28590, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 80h:36m:56s remains)
INFO - root - 2017-12-05 17:06:58.582857: step 28600, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 79h:51m:51s remains)
2017-12-05 17:06:59.414661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2585173 -4.2544928 -4.2268653 -4.1915221 -4.1425304 -4.0907 -4.0780144 -4.1184521 -4.1714482 -4.2227149 -4.2578688 -4.2654557 -4.2605748 -4.2602391 -4.2658215][-4.2562981 -4.2551847 -4.223659 -4.182775 -4.1285868 -4.06786 -4.0477772 -4.0881658 -4.147944 -4.2116356 -4.2526445 -4.2606277 -4.2550731 -4.2528582 -4.2571259][-4.2504692 -4.2556438 -4.2287722 -4.1850367 -4.1200495 -4.0455742 -4.0149059 -4.0502076 -4.1165795 -4.19297 -4.243053 -4.25553 -4.2481475 -4.2424016 -4.2452312][-4.2518096 -4.261167 -4.2398758 -4.1919212 -4.1116362 -4.022131 -3.9810486 -4.0110221 -4.083643 -4.1674633 -4.2216067 -4.2379417 -4.2328873 -4.2287946 -4.2327633][-4.2609076 -4.2703905 -4.2504606 -4.1982746 -4.10363 -3.9952612 -3.9348583 -3.9579809 -4.0418105 -4.1351285 -4.1935592 -4.2135391 -4.2134604 -4.2125659 -4.217061][-4.2694049 -4.2779832 -4.2625508 -4.2113528 -4.1041951 -3.9701831 -3.8779557 -3.89262 -3.9945259 -4.1037617 -4.1676764 -4.1912465 -4.1958261 -4.1943812 -4.19401][-4.2837315 -4.2949653 -4.2862878 -4.2362318 -4.1222091 -3.9691772 -3.8500385 -3.8591404 -3.9756582 -4.092627 -4.1554265 -4.1769037 -4.1822267 -4.1746931 -4.1646442][-4.2987046 -4.3127394 -4.3122 -4.2662969 -4.152391 -3.9973483 -3.8726287 -3.8754175 -3.9848874 -4.0930591 -4.1486816 -4.1678181 -4.17274 -4.161294 -4.1417933][-4.2999072 -4.3142438 -4.3202462 -4.281991 -4.1774135 -4.0384851 -3.9274287 -3.9196944 -4.0041471 -4.0987144 -4.1514888 -4.1708264 -4.1741753 -4.162127 -4.14094][-4.2876544 -4.3005414 -4.3109145 -4.2848043 -4.2016644 -4.0860343 -3.9893947 -3.9654517 -4.0211015 -4.1009603 -4.1549149 -4.1790452 -4.1832509 -4.17179 -4.1541495][-4.267518 -4.2819357 -4.2963552 -4.285552 -4.2285228 -4.1347547 -4.0457411 -4.0087132 -4.0421877 -4.1034775 -4.1537385 -4.1845932 -4.1922441 -4.1836758 -4.1718845][-4.2508149 -4.2702465 -4.2885566 -4.2876067 -4.2514858 -4.1814637 -4.1068978 -4.0687432 -4.0853958 -4.1253643 -4.1656876 -4.1975074 -4.2059417 -4.2051172 -4.202157][-4.2344317 -4.2646008 -4.2905164 -4.29813 -4.2786455 -4.2303944 -4.1729045 -4.1398077 -4.1426034 -4.164217 -4.1958542 -4.2267833 -4.2386727 -4.2452812 -4.2521596][-4.2151647 -4.2590795 -4.2938304 -4.308156 -4.3010759 -4.2696309 -4.2258735 -4.1966305 -4.1905036 -4.202002 -4.2282705 -4.2607622 -4.2796664 -4.2936168 -4.3069415][-4.2058129 -4.2558217 -4.2963486 -4.315403 -4.3168459 -4.2978058 -4.2671089 -4.2431612 -4.237915 -4.2452664 -4.2653418 -4.2934952 -4.3149467 -4.3321462 -4.3443403]]...]
INFO - root - 2017-12-05 17:07:08.845664: step 28610, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 81h:22m:08s remains)
INFO - root - 2017-12-05 17:07:18.277901: step 28620, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 81h:59m:36s remains)
INFO - root - 2017-12-05 17:07:27.871793: step 28630, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 81h:19m:42s remains)
INFO - root - 2017-12-05 17:07:37.126967: step 28640, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 79h:34m:27s remains)
INFO - root - 2017-12-05 17:07:46.421883: step 28650, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 76h:46m:14s remains)
INFO - root - 2017-12-05 17:07:55.890316: step 28660, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 77h:03m:44s remains)
INFO - root - 2017-12-05 17:08:05.281781: step 28670, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 79h:21m:51s remains)
INFO - root - 2017-12-05 17:08:14.759193: step 28680, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 77h:50m:57s remains)
INFO - root - 2017-12-05 17:08:24.049420: step 28690, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 76h:33m:02s remains)
INFO - root - 2017-12-05 17:08:33.622085: step 28700, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 76h:27m:53s remains)
2017-12-05 17:08:34.418830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3281693 -4.3316107 -4.330977 -4.3251104 -4.3145 -4.3056607 -4.3037424 -4.3086257 -4.3129907 -4.3154187 -4.3156624 -4.3157511 -4.3172159 -4.3192906 -4.3201523][-4.3252668 -4.3241887 -4.31711 -4.3033829 -4.2871437 -4.2753458 -4.27526 -4.2853861 -4.2947435 -4.300375 -4.3028097 -4.3045321 -4.3083057 -4.3123183 -4.3139486][-4.3117633 -4.3017573 -4.283185 -4.2595415 -4.237093 -4.2212858 -4.2231417 -4.2391224 -4.2546105 -4.2625837 -4.2666345 -4.271688 -4.280477 -4.2889376 -4.2938237][-4.2893763 -4.2686625 -4.237361 -4.2071567 -4.18093 -4.1601925 -4.1582403 -4.1739063 -4.1929183 -4.2025061 -4.2062845 -4.2154965 -4.2333946 -4.2511463 -4.2636023][-4.2642212 -4.2368484 -4.1963968 -4.1634331 -4.1348577 -4.10696 -4.0927787 -4.1012006 -4.1241231 -4.1348891 -4.1368089 -4.1495533 -4.1781511 -4.2077994 -4.2310452][-4.2412481 -4.2115412 -4.1624103 -4.1254029 -4.0930657 -4.0505872 -4.0116 -4.0113826 -4.0490208 -4.0687451 -4.0705123 -4.08664 -4.1268964 -4.1687145 -4.2051849][-4.2243052 -4.1951194 -4.1403213 -4.0941596 -4.0553136 -3.9916375 -3.9148002 -3.9063878 -3.9722178 -4.0119042 -4.0204482 -4.0447431 -4.0958309 -4.148622 -4.1964884][-4.2109804 -4.1845374 -4.1305833 -4.0755267 -4.0270958 -3.9406977 -3.8145008 -3.7956271 -3.8983595 -3.9666448 -3.993835 -4.0355668 -4.0960903 -4.1560183 -4.2087235][-4.2033591 -4.1788845 -4.1323328 -4.0766487 -4.0263305 -3.9309654 -3.7789581 -3.7455089 -3.8646567 -3.954973 -4.00684 -4.0653639 -4.1300993 -4.1872044 -4.2337561][-4.2096548 -4.1838489 -4.1433268 -4.093811 -4.052444 -3.9763634 -3.857739 -3.8246102 -3.9164622 -4.0041928 -4.0684276 -4.13095 -4.1869488 -4.2295628 -4.2609982][-4.2255173 -4.1943073 -4.1534004 -4.1121693 -4.0825434 -4.0355515 -3.9687748 -3.9532616 -4.0141439 -4.0856447 -4.1470318 -4.1997824 -4.2393026 -4.2646213 -4.2815738][-4.2460794 -4.2120495 -4.1720395 -4.1388144 -4.1179175 -4.09292 -4.06481 -4.0678988 -4.1114469 -4.1631069 -4.2091656 -4.246326 -4.2706447 -4.2847152 -4.2930174][-4.2682447 -4.2390285 -4.205565 -4.1816154 -4.1678448 -4.1546755 -4.1452069 -4.1563139 -4.1858168 -4.215507 -4.2444959 -4.2694707 -4.2843142 -4.2928433 -4.2987928][-4.28692 -4.2690344 -4.2458906 -4.2319617 -4.2266693 -4.2205648 -4.2149105 -4.2215648 -4.2382956 -4.2521362 -4.2676506 -4.2818274 -4.2907729 -4.2974348 -4.3030167][-4.2993507 -4.292244 -4.2796631 -4.272644 -4.2730546 -4.2713342 -4.2650795 -4.2630229 -4.269845 -4.2766056 -4.2837377 -4.2901421 -4.2952228 -4.30072 -4.3060312]]...]
INFO - root - 2017-12-05 17:08:43.682187: step 28710, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 77h:13m:40s remains)
INFO - root - 2017-12-05 17:08:53.103011: step 28720, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 79h:26m:23s remains)
INFO - root - 2017-12-05 17:09:02.323234: step 28730, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 70h:10m:16s remains)
INFO - root - 2017-12-05 17:09:11.550454: step 28740, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 79h:56m:37s remains)
INFO - root - 2017-12-05 17:09:21.103532: step 28750, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.984 sec/batch; 83h:03m:43s remains)
INFO - root - 2017-12-05 17:09:30.391588: step 28760, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 76h:20m:43s remains)
INFO - root - 2017-12-05 17:09:39.841890: step 28770, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.934 sec/batch; 78h:46m:10s remains)
INFO - root - 2017-12-05 17:09:49.355064: step 28780, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 76h:43m:50s remains)
INFO - root - 2017-12-05 17:09:58.676380: step 28790, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 76h:55m:09s remains)
INFO - root - 2017-12-05 17:10:08.247920: step 28800, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 82h:33m:18s remains)
2017-12-05 17:10:09.022720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1650538 -4.1816373 -4.2192974 -4.2503843 -4.2531357 -4.2272987 -4.1826935 -4.1506157 -4.1410441 -4.1628876 -4.2076521 -4.2400756 -4.2469044 -4.23789 -4.2238417][-4.1821532 -4.181663 -4.1958094 -4.2096877 -4.2076697 -4.1877265 -4.1641097 -4.158555 -4.16423 -4.1845832 -4.2239184 -4.249 -4.2482038 -4.2295966 -4.213778][-4.1908855 -4.1819339 -4.1764431 -4.1762085 -4.1688161 -4.150671 -4.1394968 -4.1555586 -4.1725941 -4.18571 -4.2123671 -4.2272658 -4.2205811 -4.2013741 -4.18757][-4.1813617 -4.1801753 -4.1759892 -4.167388 -4.1505113 -4.1272917 -4.1167231 -4.1361 -4.157176 -4.1648693 -4.176949 -4.1832619 -4.1773844 -4.1595349 -4.1518407][-4.1557145 -4.1750875 -4.1873636 -4.1763315 -4.1496692 -4.1136084 -4.0876026 -4.0928426 -4.1167393 -4.139431 -4.1570673 -4.1610093 -4.1538696 -4.1331153 -4.1214142][-4.1308479 -4.1693993 -4.1958704 -4.1846566 -4.1516519 -4.0992374 -4.0438676 -4.0222225 -4.0532556 -4.1048651 -4.1476064 -4.1605182 -4.1565566 -4.1319556 -4.1068721][-4.1168289 -4.1581345 -4.1888576 -4.178565 -4.145484 -4.0841193 -3.9955232 -3.9271111 -3.946595 -4.0348668 -4.1163807 -4.1508489 -4.1538129 -4.1306496 -4.0957756][-4.1278105 -4.1604314 -4.18137 -4.1747022 -4.1503921 -4.1014071 -4.0027685 -3.8870544 -3.8723497 -3.9790623 -4.0889692 -4.1423759 -4.1504664 -4.1275659 -4.0877008][-4.1458087 -4.1706748 -4.1871338 -4.18782 -4.1788235 -4.1466036 -4.0673604 -3.9632587 -3.9365859 -4.0130482 -4.1072717 -4.15635 -4.1606812 -4.1387444 -4.0981135][-4.1373472 -4.1560397 -4.1760631 -4.1841531 -4.1820679 -4.1640615 -4.118021 -4.0535588 -4.0374889 -4.0808368 -4.1437941 -4.1823745 -4.188736 -4.1821051 -4.1606512][-4.1309042 -4.1422615 -4.16583 -4.1768322 -4.1769609 -4.1709127 -4.1522436 -4.1245661 -4.1179357 -4.1370106 -4.175467 -4.2056785 -4.221024 -4.2315421 -4.2272868][-4.149817 -4.15651 -4.1739125 -4.1816812 -4.185781 -4.1948385 -4.1944776 -4.1877613 -4.1847806 -4.1876721 -4.2022834 -4.2208447 -4.2392087 -4.2599735 -4.2690287][-4.1753592 -4.1783276 -4.1892581 -4.1934443 -4.2013054 -4.2160158 -4.2263975 -4.2307363 -4.2334623 -4.235168 -4.2373695 -4.247077 -4.2621059 -4.2816505 -4.2956605][-4.2051182 -4.2077413 -4.2150512 -4.2190585 -4.22641 -4.2392621 -4.2476134 -4.25226 -4.2588053 -4.2631788 -4.2608538 -4.2664905 -4.2786841 -4.2933235 -4.3064218][-4.2447381 -4.2466598 -4.2488561 -4.2496533 -4.2535591 -4.2594638 -4.2620478 -4.2651472 -4.2710943 -4.2752442 -4.2721052 -4.2721887 -4.2769122 -4.2832575 -4.2904191]]...]
INFO - root - 2017-12-05 17:10:18.382452: step 28810, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 79h:53m:06s remains)
INFO - root - 2017-12-05 17:10:27.908235: step 28820, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 82h:00m:37s remains)
INFO - root - 2017-12-05 17:10:37.219390: step 28830, loss = 2.03, batch loss = 1.98 (8.2 examples/sec; 0.973 sec/batch; 82h:04m:02s remains)
INFO - root - 2017-12-05 17:10:46.600783: step 28840, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 78h:23m:04s remains)
INFO - root - 2017-12-05 17:10:55.808900: step 28850, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.818 sec/batch; 68h:58m:58s remains)
INFO - root - 2017-12-05 17:11:05.436630: step 28860, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 80h:49m:53s remains)
INFO - root - 2017-12-05 17:11:14.621752: step 28870, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 72h:11m:36s remains)
INFO - root - 2017-12-05 17:11:24.041995: step 28880, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 78h:14m:20s remains)
INFO - root - 2017-12-05 17:11:33.441537: step 28890, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 77h:50m:01s remains)
INFO - root - 2017-12-05 17:11:42.760997: step 28900, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 79h:55m:11s remains)
2017-12-05 17:11:43.512977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2822552 -4.2680068 -4.2589526 -4.2325969 -4.1913977 -4.1566525 -4.1303744 -4.1278448 -4.1585388 -4.1895895 -4.1948643 -4.1942186 -4.2034612 -4.2207685 -4.2327647][-4.3041387 -4.2879391 -4.2704411 -4.2402296 -4.1985059 -4.1672482 -4.1338716 -4.1132107 -4.1388364 -4.177649 -4.1857705 -4.1903467 -4.20733 -4.23039 -4.2456183][-4.3222923 -4.2996655 -4.2694125 -4.2342558 -4.1965241 -4.1634946 -4.1162262 -4.0720382 -4.0952291 -4.151309 -4.1731405 -4.1869421 -4.2109571 -4.2386732 -4.2558503][-4.319345 -4.2904949 -4.2541304 -4.2185183 -4.183773 -4.1530924 -4.0967836 -4.0252705 -4.0449 -4.1209469 -4.160933 -4.1843424 -4.2130547 -4.2389145 -4.2576866][-4.29131 -4.2587519 -4.2223639 -4.1900616 -4.15857 -4.1315951 -4.0682282 -3.9684844 -3.9791245 -4.0729265 -4.127213 -4.1611433 -4.1995845 -4.2300897 -4.2515988][-4.2543035 -4.2144947 -4.1772685 -4.14306 -4.1063638 -4.0818062 -4.0096316 -3.8769534 -3.8754368 -3.9900346 -4.0640726 -4.1128411 -4.1638155 -4.2043653 -4.2341719][-4.2141933 -4.1625485 -4.1199789 -4.0779195 -4.0312595 -4.0013113 -3.9103639 -3.7351251 -3.7201872 -3.8673801 -3.9662428 -4.0316463 -4.0988226 -4.1570268 -4.200418][-4.1909351 -4.131434 -4.0835214 -4.0368133 -3.9857373 -3.9516542 -3.85854 -3.6732814 -3.6445765 -3.7918205 -3.8935747 -3.9633806 -4.0432258 -4.1180658 -4.1720157][-4.2007685 -4.1439071 -4.0981274 -4.0597014 -4.0171485 -3.9867105 -3.9112575 -3.765485 -3.7416537 -3.8486109 -3.9218011 -3.9780011 -4.0545607 -4.1276741 -4.1774144][-4.2253757 -4.1795821 -4.1413636 -4.1089292 -4.0706978 -4.0434346 -3.9874876 -3.8840127 -3.8683078 -3.9406524 -3.9892466 -4.033895 -4.1020217 -4.1643162 -4.2028852][-4.2534184 -4.219604 -4.1916881 -4.1634383 -4.1270719 -4.1053061 -4.0685115 -3.9991322 -3.9898744 -4.0384593 -4.0690475 -4.1040759 -4.1585331 -4.2061529 -4.2337861][-4.2770643 -4.2552819 -4.2387543 -4.2167773 -4.1845 -4.1681781 -4.1486115 -4.1025653 -4.0951328 -4.1261964 -4.1445408 -4.1679845 -4.208364 -4.2429171 -4.2634492][-4.2867661 -4.2752 -4.2674079 -4.2532582 -4.2308812 -4.2207732 -4.2118983 -4.1841435 -4.1780152 -4.19532 -4.2063766 -4.2200713 -4.2465529 -4.2713161 -4.2849913][-4.2888579 -4.2823014 -4.2778988 -4.2707891 -4.2584491 -4.2537179 -4.250689 -4.2363706 -4.2311149 -4.2389936 -4.2451582 -4.2531285 -4.2682409 -4.2842817 -4.2943912][-4.2941766 -4.288692 -4.2845149 -4.2813849 -4.2754455 -4.2732377 -4.2722082 -4.2661862 -4.2631216 -4.2654042 -4.2676082 -4.2716479 -4.2799325 -4.2892714 -4.2960153]]...]
INFO - root - 2017-12-05 17:11:52.938222: step 28910, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 78h:13m:48s remains)
INFO - root - 2017-12-05 17:12:02.136195: step 28920, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 75h:35m:51s remains)
INFO - root - 2017-12-05 17:12:11.487232: step 28930, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 78h:57m:05s remains)
INFO - root - 2017-12-05 17:12:20.938492: step 28940, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 78h:05m:37s remains)
INFO - root - 2017-12-05 17:12:30.145489: step 28950, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 78h:43m:27s remains)
INFO - root - 2017-12-05 17:12:39.273572: step 28960, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 80h:03m:16s remains)
INFO - root - 2017-12-05 17:12:48.648293: step 28970, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 71h:35m:09s remains)
INFO - root - 2017-12-05 17:12:58.094638: step 28980, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 83h:24m:04s remains)
INFO - root - 2017-12-05 17:13:07.430037: step 28990, loss = 2.02, batch loss = 1.96 (8.3 examples/sec; 0.962 sec/batch; 81h:04m:37s remains)
INFO - root - 2017-12-05 17:13:17.101368: step 29000, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 80h:14m:41s remains)
2017-12-05 17:13:17.919985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2940812 -4.2784715 -4.257225 -4.22769 -4.2118979 -4.2281132 -4.2559552 -4.2703257 -4.2758336 -4.2808943 -4.2787948 -4.2746954 -4.2772779 -4.2743778 -4.2660575][-4.2775288 -4.26841 -4.2493687 -4.22122 -4.2007408 -4.21115 -4.2408066 -4.2547674 -4.2539129 -4.2546363 -4.2539887 -4.2557983 -4.265542 -4.2683959 -4.261579][-4.2803192 -4.2736721 -4.2591848 -4.2334609 -4.2074203 -4.2063131 -4.2262678 -4.2388659 -4.2333832 -4.2295308 -4.230907 -4.2402854 -4.2548857 -4.2614112 -4.2614951][-4.2816453 -4.2804132 -4.2719407 -4.2488227 -4.2185545 -4.202858 -4.2106552 -4.2193179 -4.2107863 -4.2046251 -4.2090549 -4.225615 -4.2470436 -4.2596273 -4.2683372][-4.2733111 -4.2785811 -4.2724285 -4.2512774 -4.2224855 -4.2008543 -4.1970716 -4.1989541 -4.1907897 -4.1865945 -4.1994243 -4.2212729 -4.2444239 -4.2608609 -4.275743][-4.2532434 -4.2561259 -4.2457209 -4.2242193 -4.2065263 -4.19082 -4.1821909 -4.1812377 -4.173296 -4.1736474 -4.1945624 -4.2210793 -4.2425876 -4.260778 -4.2754259][-4.227869 -4.2211194 -4.2088914 -4.1961737 -4.1877375 -4.1789632 -4.1704488 -4.1702342 -4.1625018 -4.1686883 -4.1964283 -4.2281246 -4.2486744 -4.2603769 -4.2720971][-4.21726 -4.2007957 -4.1884246 -4.1845689 -4.1866779 -4.1813474 -4.1732535 -4.1734371 -4.1705813 -4.1808677 -4.2134433 -4.2496057 -4.2685819 -4.27076 -4.2722111][-4.2092872 -4.1995621 -4.1906428 -4.1914043 -4.1977487 -4.1980019 -4.1938767 -4.1915255 -4.1885962 -4.1994529 -4.2333021 -4.2720342 -4.2900629 -4.2868752 -4.2756891][-4.2048836 -4.2177358 -4.2167263 -4.2147136 -4.2168546 -4.216114 -4.2117944 -4.2072973 -4.2046976 -4.2165442 -4.2496905 -4.2850542 -4.3017783 -4.2980008 -4.2800374][-4.2202854 -4.2404346 -4.2418623 -4.2358422 -4.2310729 -4.228375 -4.2214489 -4.2139664 -4.2137232 -4.2276154 -4.2594671 -4.2889552 -4.3050642 -4.3018203 -4.2826638][-4.2363515 -4.2494841 -4.2480607 -4.2362032 -4.22651 -4.2219815 -4.2120605 -4.2054954 -4.2044239 -4.2204442 -4.25445 -4.2825189 -4.2992597 -4.2977715 -4.2849932][-4.2403121 -4.2472858 -4.2425237 -4.2302814 -4.2180929 -4.2064056 -4.1904559 -4.1849422 -4.1872873 -4.2041364 -4.2420096 -4.2740116 -4.2907882 -4.2910376 -4.2825856][-4.2274222 -4.2314386 -4.2240496 -4.2185988 -4.2114577 -4.197032 -4.18182 -4.1778212 -4.1792083 -4.1893554 -4.2273688 -4.2618828 -4.278604 -4.2801933 -4.2740426][-4.2110167 -4.2112823 -4.2026582 -4.2064323 -4.2079391 -4.196012 -4.1864004 -4.1835546 -4.1780982 -4.1773543 -4.2087874 -4.238276 -4.2540579 -4.2588215 -4.2517815]]...]
INFO - root - 2017-12-05 17:13:27.090897: step 29010, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.947 sec/batch; 79h:48m:36s remains)
INFO - root - 2017-12-05 17:13:36.288195: step 29020, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 79h:21m:22s remains)
INFO - root - 2017-12-05 17:13:45.685398: step 29030, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.021 sec/batch; 86h:06m:19s remains)
INFO - root - 2017-12-05 17:13:54.931688: step 29040, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 79h:29m:58s remains)
INFO - root - 2017-12-05 17:14:04.562047: step 29050, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 83h:25m:37s remains)
INFO - root - 2017-12-05 17:14:14.155684: step 29060, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 83h:31m:42s remains)
INFO - root - 2017-12-05 17:14:23.655958: step 29070, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 75h:18m:17s remains)
INFO - root - 2017-12-05 17:14:32.959214: step 29080, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 72h:13m:09s remains)
INFO - root - 2017-12-05 17:14:42.338420: step 29090, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.964 sec/batch; 81h:15m:58s remains)
INFO - root - 2017-12-05 17:14:51.693563: step 29100, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.931 sec/batch; 78h:29m:19s remains)
2017-12-05 17:14:52.472905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2764387 -4.2761841 -4.2751026 -4.2768435 -4.2795939 -4.2788596 -4.2762742 -4.2743511 -4.2774639 -4.2895713 -4.302927 -4.3081245 -4.3043389 -4.2985272 -4.2926922][-4.2638927 -4.2579088 -4.253078 -4.2521739 -4.2519536 -4.247561 -4.24284 -4.2423077 -4.2498074 -4.2701249 -4.2907543 -4.3011036 -4.3026462 -4.3028054 -4.2980013][-4.2451034 -4.231554 -4.2215357 -4.2159967 -4.2094064 -4.199286 -4.1933541 -4.1956682 -4.2072692 -4.2345018 -4.2615004 -4.2804947 -4.293613 -4.3050094 -4.3056765][-4.2212777 -4.2000995 -4.1836529 -4.172111 -4.1577916 -4.140367 -4.1337914 -4.13811 -4.1532836 -4.1826777 -4.2166033 -4.2484779 -4.2756271 -4.2999191 -4.3091784][-4.1992421 -4.1778293 -4.1597853 -4.14314 -4.1210847 -4.0955086 -4.0841279 -4.0826254 -4.0887303 -4.112668 -4.1564031 -4.2048864 -4.2481842 -4.2821321 -4.2984776][-4.1797757 -4.16437 -4.1493082 -4.1303787 -4.10646 -4.0758462 -4.0514975 -4.025918 -4.009151 -4.0283713 -4.0851917 -4.1529765 -4.2118187 -4.2525015 -4.2735648][-4.171958 -4.1602387 -4.1453118 -4.1253362 -4.1040106 -4.0752912 -4.0354185 -3.9695885 -3.9180288 -3.9377589 -4.0137172 -4.0993991 -4.1696744 -4.2130575 -4.2365689][-4.18409 -4.1770535 -4.1649165 -4.1478491 -4.1283588 -4.0994706 -4.0400624 -3.9350135 -3.848448 -3.8693926 -3.962117 -4.0606184 -4.1340523 -4.1776867 -4.2007561][-4.2021689 -4.2003908 -4.1962175 -4.1896896 -4.1778531 -4.1509056 -4.0900626 -3.9878256 -3.9072106 -3.9140475 -3.9806588 -4.05914 -4.1215625 -4.1640973 -4.1892052][-4.2192297 -4.2213383 -4.2232895 -4.2255869 -4.2217979 -4.2020493 -4.1530147 -4.0827327 -4.0331841 -4.0254459 -4.0506883 -4.0908976 -4.131886 -4.1701818 -4.1983428][-4.2291989 -4.2323313 -4.2374668 -4.2404814 -4.2388458 -4.2232366 -4.1894765 -4.144207 -4.1120868 -4.0936227 -4.0886493 -4.1034021 -4.1322823 -4.1702147 -4.2049856][-4.2213788 -4.2206521 -4.2236938 -4.223649 -4.2209315 -4.2125387 -4.1995721 -4.1764255 -4.1498809 -4.1234393 -4.1078978 -4.1098809 -4.1334724 -4.1739788 -4.2141213][-4.2041521 -4.1991124 -4.1955109 -4.19069 -4.19194 -4.1967812 -4.1976218 -4.1836047 -4.1615477 -4.1388392 -4.1300387 -4.13591 -4.1610274 -4.2006636 -4.2398758][-4.1984282 -4.1931148 -4.1849971 -4.1762772 -4.1793633 -4.1901865 -4.1936164 -4.1838355 -4.1692905 -4.1578112 -4.1595368 -4.1725421 -4.1999469 -4.2342787 -4.2695851][-4.1946383 -4.1910548 -4.1814404 -4.1677213 -4.1638341 -4.1667318 -4.1619167 -4.1530194 -4.1512432 -4.15239 -4.1660194 -4.1864963 -4.2157907 -4.2478271 -4.2780786]]...]
INFO - root - 2017-12-05 17:15:01.780901: step 29110, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 78h:25m:12s remains)
INFO - root - 2017-12-05 17:15:10.925654: step 29120, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 81h:55m:15s remains)
INFO - root - 2017-12-05 17:15:20.260577: step 29130, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 73h:57m:15s remains)
INFO - root - 2017-12-05 17:15:29.664976: step 29140, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.998 sec/batch; 84h:05m:15s remains)
INFO - root - 2017-12-05 17:15:38.991590: step 29150, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.958 sec/batch; 80h:44m:55s remains)
INFO - root - 2017-12-05 17:15:48.229245: step 29160, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 76h:22m:06s remains)
INFO - root - 2017-12-05 17:15:57.645418: step 29170, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 81h:31m:14s remains)
INFO - root - 2017-12-05 17:16:07.184787: step 29180, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 81h:44m:19s remains)
INFO - root - 2017-12-05 17:16:16.365868: step 29190, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.954 sec/batch; 80h:23m:21s remains)
INFO - root - 2017-12-05 17:16:25.870874: step 29200, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 81h:19m:36s remains)
2017-12-05 17:16:26.663229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3004723 -4.28249 -4.2671552 -4.2530541 -4.2327647 -4.206459 -4.1809359 -4.1501555 -4.1525121 -4.1618876 -4.1538305 -4.1454086 -4.1473308 -4.1522856 -4.1459532][-4.2713652 -4.2457681 -4.2237926 -4.2037039 -4.1817107 -4.1521392 -4.1231861 -4.0876656 -4.0902848 -4.0964828 -4.0795608 -4.0730996 -4.0930262 -4.1099453 -4.0942154][-4.246387 -4.2102575 -4.1881318 -4.1754932 -4.1659393 -4.1439753 -4.112649 -4.0696249 -4.072504 -4.0715213 -4.0426445 -4.037271 -4.0685115 -4.0908985 -4.0663376][-4.2275486 -4.1809964 -4.1608305 -4.1576324 -4.163002 -4.1532636 -4.1210208 -4.0655513 -4.0688863 -4.0730362 -4.0415049 -4.0333633 -4.0625277 -4.0839438 -4.0649948][-4.2145591 -4.1590834 -4.1303434 -4.122242 -4.1311088 -4.1275988 -4.0883889 -4.0174332 -4.0232296 -4.047586 -4.0325732 -4.0285726 -4.0535502 -4.0795636 -4.0784321][-4.2069545 -4.1424575 -4.09608 -4.0678821 -4.0649571 -4.0532002 -3.9952354 -3.8992085 -3.9115825 -3.9682512 -3.9802272 -3.9865992 -4.0112419 -4.048564 -4.0755181][-4.19795 -4.1206813 -4.0503578 -3.9946494 -3.9633346 -3.9165306 -3.8039746 -3.6591067 -3.6984117 -3.8211184 -3.8795056 -3.9153419 -3.9541814 -4.0048227 -4.0569987][-4.1782341 -4.0918097 -4.0114374 -3.9442215 -3.8886685 -3.7953947 -3.6134529 -3.4221382 -3.5123854 -3.7132244 -3.8184948 -3.8780475 -3.9248104 -3.9755135 -4.0353293][-4.1610332 -4.076076 -4.0034633 -3.9495671 -3.906956 -3.830193 -3.6842668 -3.5423377 -3.6089051 -3.766187 -3.8495924 -3.898036 -3.9392333 -3.9782648 -4.0271072][-4.1637 -4.0863838 -4.0250883 -3.9840002 -3.9660864 -3.9351835 -3.8626482 -3.7824507 -3.810755 -3.9053009 -3.9534793 -3.9783573 -4.006813 -4.0305347 -4.0624323][-4.1871266 -4.1180606 -4.0642257 -4.0305176 -4.0284142 -4.0253243 -3.99441 -3.94564 -3.9547117 -4.01892 -4.0474391 -4.0587325 -4.0790606 -4.0961609 -4.117763][-4.223361 -4.1599402 -4.1075983 -4.0758758 -4.0785213 -4.0897913 -4.0812955 -4.0564251 -4.0599608 -4.0993509 -4.1158028 -4.1249409 -4.1415029 -4.15294 -4.1673279][-4.2684083 -4.2161331 -4.1690154 -4.1397786 -4.1404328 -4.1506066 -4.1501088 -4.1375556 -4.1406 -4.1660147 -4.1780844 -4.1872511 -4.1991525 -4.2055678 -4.2107921][-4.30651 -4.2683167 -4.232964 -4.2114763 -4.2119737 -4.2201777 -4.2224436 -4.2161069 -4.2177806 -4.232964 -4.2419152 -4.2492294 -4.2574806 -4.2614374 -4.2617011][-4.334 -4.3094006 -4.2874317 -4.2748723 -4.277884 -4.2869587 -4.291605 -4.2895288 -4.2904587 -4.2965636 -4.29954 -4.303844 -4.30805 -4.3099446 -4.3088059]]...]
INFO - root - 2017-12-05 17:16:35.999338: step 29210, loss = 2.02, batch loss = 1.96 (9.3 examples/sec; 0.862 sec/batch; 72h:37m:59s remains)
INFO - root - 2017-12-05 17:16:44.860511: step 29220, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 77h:37m:48s remains)
INFO - root - 2017-12-05 17:16:54.233436: step 29230, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 79h:48m:48s remains)
INFO - root - 2017-12-05 17:17:03.702636: step 29240, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 80h:36m:38s remains)
INFO - root - 2017-12-05 17:17:13.196688: step 29250, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 81h:49m:13s remains)
INFO - root - 2017-12-05 17:17:22.495668: step 29260, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 79h:53m:27s remains)
INFO - root - 2017-12-05 17:17:31.807169: step 29270, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 79h:20m:34s remains)
INFO - root - 2017-12-05 17:17:41.155634: step 29280, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 77h:14m:45s remains)
INFO - root - 2017-12-05 17:17:50.435925: step 29290, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 75h:25m:27s remains)
INFO - root - 2017-12-05 17:17:59.792141: step 29300, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 79h:49m:30s remains)
2017-12-05 17:18:00.520985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.309742 -4.3073592 -4.3069181 -4.3073077 -4.304915 -4.3023434 -4.2942305 -4.2817616 -4.2749877 -4.2779007 -4.2846508 -4.2933869 -4.2994137 -4.3080864 -4.3207221][-4.2857075 -4.2868991 -4.2908745 -4.2934952 -4.2903895 -4.2852359 -4.2711034 -4.2512312 -4.2426982 -4.2486787 -4.2592564 -4.2704177 -4.2794724 -4.2927384 -4.3115983][-4.251018 -4.2581043 -4.2680149 -4.2731152 -4.2692752 -4.26029 -4.2391949 -4.2115121 -4.2046022 -4.2177391 -4.2329941 -4.2461419 -4.257513 -4.276134 -4.3018875][-4.2161837 -4.2309513 -4.2474227 -4.2542887 -4.24792 -4.2309422 -4.199111 -4.1638288 -4.164577 -4.1921177 -4.2151775 -4.2301049 -4.2422042 -4.2655587 -4.2970495][-4.189724 -4.211843 -4.2338009 -4.2403173 -4.2276335 -4.1951542 -4.1433697 -4.0938439 -4.1057129 -4.1525455 -4.1868696 -4.2069421 -4.2217393 -4.2535605 -4.291358][-4.1759229 -4.2020774 -4.2231069 -4.2246785 -4.2006478 -4.1481519 -4.06889 -4.0003295 -4.0254745 -4.0933638 -4.1417093 -4.17178 -4.1944828 -4.2368889 -4.2818222][-4.1732154 -4.201448 -4.2172012 -4.2089238 -4.1680365 -4.0930614 -3.988579 -3.9077995 -3.9502804 -4.0385866 -4.101799 -4.1429186 -4.1750851 -4.2267308 -4.2784848][-4.1830311 -4.2143221 -4.2232237 -4.2004013 -4.13925 -4.0477548 -3.9372692 -3.8671927 -3.9239652 -4.0207186 -4.089704 -4.1349397 -4.1722107 -4.2285109 -4.2825255][-4.2064495 -4.2395821 -4.2423425 -4.2054782 -4.1285625 -4.0366797 -3.9526339 -3.9167216 -3.9766831 -4.0642653 -4.1265669 -4.1658049 -4.1987214 -4.2489014 -4.2979088][-4.237226 -4.2744575 -4.2747536 -4.2307482 -4.1505404 -4.0747752 -4.0298476 -4.0301895 -4.0853148 -4.1522779 -4.1997709 -4.2281623 -4.2507076 -4.2861967 -4.3206544][-4.2527523 -4.2922816 -4.291769 -4.2490277 -4.1773386 -4.1244335 -4.1124744 -4.1359363 -4.1826687 -4.227056 -4.2582765 -4.2763605 -4.289587 -4.3117852 -4.3339047][-4.2536316 -4.2915449 -4.290473 -4.255506 -4.2032194 -4.1762691 -4.1862597 -4.2188339 -4.2545838 -4.2773218 -4.2921219 -4.30242 -4.3088789 -4.3214817 -4.33619][-4.255096 -4.2883091 -4.2865891 -4.2595992 -4.2274532 -4.2240257 -4.2479248 -4.27982 -4.3033252 -4.3104558 -4.3143544 -4.32004 -4.3214765 -4.3264861 -4.3349686][-4.2678776 -4.2927661 -4.2889342 -4.266942 -4.2499237 -4.2610393 -4.2903428 -4.3160796 -4.3287115 -4.3290634 -4.3291745 -4.3315573 -4.32909 -4.3291731 -4.3339505][-4.2982984 -4.3151112 -4.3099785 -4.2913208 -4.2791567 -4.2912855 -4.3162313 -4.3346467 -4.3415561 -4.3397117 -4.3382759 -4.3367887 -4.3324137 -4.3310132 -4.3347716]]...]
INFO - root - 2017-12-05 17:18:09.781835: step 29310, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 79h:10m:14s remains)
INFO - root - 2017-12-05 17:18:19.105155: step 29320, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 80h:33m:27s remains)
INFO - root - 2017-12-05 17:18:28.455922: step 29330, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 79h:17m:06s remains)
INFO - root - 2017-12-05 17:18:37.978998: step 29340, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 81h:33m:03s remains)
INFO - root - 2017-12-05 17:18:47.485995: step 29350, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.944 sec/batch; 79h:28m:14s remains)
INFO - root - 2017-12-05 17:18:56.933482: step 29360, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 77h:35m:24s remains)
INFO - root - 2017-12-05 17:19:06.322312: step 29370, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 72h:09m:51s remains)
INFO - root - 2017-12-05 17:19:15.755951: step 29380, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 81h:44m:46s remains)
INFO - root - 2017-12-05 17:19:25.046138: step 29390, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 79h:17m:13s remains)
INFO - root - 2017-12-05 17:19:34.388449: step 29400, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.941 sec/batch; 79h:13m:44s remains)
2017-12-05 17:19:35.159195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2762837 -4.2858987 -4.2991714 -4.3068771 -4.3083324 -4.3031492 -4.2859087 -4.2552423 -4.219008 -4.1878304 -4.170351 -4.1620626 -4.1597052 -4.1625242 -4.1718903][-4.2673664 -4.2746859 -4.2847471 -4.29123 -4.2928557 -4.2892442 -4.2752848 -4.2473955 -4.2147541 -4.1893721 -4.1785016 -4.1758904 -4.178618 -4.1847515 -4.1942558][-4.2193 -4.2283149 -4.2418957 -4.255394 -4.2653975 -4.2680073 -4.2617874 -4.2441306 -4.2239566 -4.2095356 -4.2047629 -4.2036104 -4.2045069 -4.2095575 -4.21669][-4.14037 -4.1521697 -4.1710405 -4.1934505 -4.2143555 -4.2277336 -4.2336955 -4.2309074 -4.2280416 -4.2291594 -4.2325463 -4.2323651 -4.230978 -4.2341733 -4.239006][-4.0649462 -4.0757442 -4.0953321 -4.1227908 -4.1480289 -4.1664824 -4.1819696 -4.1936307 -4.2063727 -4.22207 -4.2361169 -4.2421932 -4.2458034 -4.2507868 -4.2547197][-4.0401654 -4.0423093 -4.0519195 -4.0728602 -4.0915546 -4.1040668 -4.1194487 -4.1367254 -4.1566877 -4.1823759 -4.2093396 -4.2286372 -4.2427726 -4.2549458 -4.2609944][-4.0700469 -4.063467 -4.0589724 -4.0627589 -4.06781 -4.0690393 -4.0773849 -4.0876212 -4.0997829 -4.12319 -4.15674 -4.1893072 -4.2183495 -4.2426858 -4.255549][-4.1181593 -4.1095591 -4.0983815 -4.0893993 -4.0840812 -4.0776696 -4.0800834 -4.0819845 -4.0800281 -4.0920959 -4.1245065 -4.1626482 -4.2005005 -4.2331381 -4.2512569][-4.1686745 -4.1632261 -4.1542678 -4.1407008 -4.1306381 -4.1205859 -4.1169739 -4.1117582 -4.0989437 -4.1015344 -4.1300521 -4.1669602 -4.2042017 -4.2372928 -4.2554827][-4.21096 -4.2114019 -4.2115145 -4.2026505 -4.1943803 -4.1848006 -4.1787391 -4.1694746 -4.152349 -4.1489248 -4.1689844 -4.195775 -4.2225184 -4.2467351 -4.2611437][-4.2362204 -4.2421975 -4.251709 -4.2533393 -4.2510023 -4.2452669 -4.2404351 -4.2306662 -4.2149739 -4.2089643 -4.2183795 -4.2315674 -4.2439923 -4.255352 -4.2639437][-4.2460141 -4.2575669 -4.27501 -4.2856994 -4.287632 -4.2840953 -4.28055 -4.2736154 -4.2632718 -4.2572904 -4.2569437 -4.2590413 -4.2580509 -4.2558842 -4.2583585][-4.2347465 -4.2495542 -4.2706919 -4.2853131 -4.2894411 -4.2886052 -4.2902994 -4.2914095 -4.2886667 -4.2848058 -4.2794676 -4.2736592 -4.2619257 -4.249125 -4.2465711][-4.2134995 -4.229425 -4.250011 -4.2656474 -4.2721486 -4.2765036 -4.2855725 -4.293777 -4.2965922 -4.2933645 -4.2841096 -4.2715077 -4.253293 -4.2353249 -4.2304072][-4.2040663 -4.2186618 -4.2359037 -4.2487111 -4.2534871 -4.2595444 -4.2720661 -4.2841172 -4.2909718 -4.291173 -4.2820306 -4.2643952 -4.242475 -4.2226849 -4.2161665]]...]
INFO - root - 2017-12-05 17:19:44.554009: step 29410, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 75h:22m:54s remains)
INFO - root - 2017-12-05 17:19:54.086306: step 29420, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 78h:26m:03s remains)
INFO - root - 2017-12-05 17:20:03.607095: step 29430, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.993 sec/batch; 83h:35m:43s remains)
INFO - root - 2017-12-05 17:20:12.915053: step 29440, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 74h:23m:49s remains)
INFO - root - 2017-12-05 17:20:22.485196: step 29450, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 80h:08m:39s remains)
INFO - root - 2017-12-05 17:20:31.722246: step 29460, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 83h:30m:54s remains)
INFO - root - 2017-12-05 17:20:41.090039: step 29470, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.976 sec/batch; 82h:07m:16s remains)
INFO - root - 2017-12-05 17:20:50.509885: step 29480, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 80h:34m:56s remains)
INFO - root - 2017-12-05 17:20:59.731413: step 29490, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 81h:41m:25s remains)
INFO - root - 2017-12-05 17:21:09.101197: step 29500, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 72h:25m:04s remains)
2017-12-05 17:21:09.822936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2298059 -4.2252321 -4.2240572 -4.2081165 -4.2075267 -4.2167459 -4.2052855 -4.1883745 -4.1712346 -4.1737618 -4.1842489 -4.1795735 -4.1695371 -4.1577315 -4.1546345][-4.2197843 -4.2068224 -4.2016063 -4.1831207 -4.1876235 -4.2040186 -4.1961937 -4.1788816 -4.1586251 -4.1634665 -4.1771455 -4.1707444 -4.154707 -4.1385417 -4.1331182][-4.1851215 -4.1671209 -4.1592169 -4.1418514 -4.151031 -4.1714964 -4.1633272 -4.1445804 -4.1280923 -4.1403294 -4.1568589 -4.1491814 -4.1305323 -4.1123791 -4.1099463][-4.1370926 -4.116652 -4.1006103 -4.0784745 -4.0866389 -4.1122122 -4.1095147 -4.0958133 -4.0892181 -4.113471 -4.1273375 -4.11732 -4.0991387 -4.085659 -4.089745][-4.0996156 -4.0689435 -4.0410409 -4.0105357 -4.0122561 -4.0487285 -4.0652304 -4.0657063 -4.0749607 -4.1087308 -4.1193881 -4.1043367 -4.0874352 -4.0863705 -4.0988874][-4.0611506 -4.0238118 -3.9880416 -3.9454966 -3.9408808 -3.9892795 -4.0309567 -4.0501857 -4.0651774 -4.104332 -4.1141205 -4.0953226 -4.0882134 -4.099256 -4.1201077][-4.02862 -4.0009871 -3.9704034 -3.9247768 -3.9074001 -3.9382207 -3.9771957 -3.9971528 -4.0166564 -4.0670362 -4.0841351 -4.0667481 -4.0625181 -4.0867085 -4.1183019][-4.0066996 -3.9951129 -3.9752281 -3.93236 -3.8924506 -3.8775978 -3.8772879 -3.8851027 -3.9160185 -3.9839084 -4.0139937 -4.0013285 -3.9982605 -4.0323009 -4.0744309][-3.9941018 -3.989223 -3.972403 -3.9336202 -3.8800235 -3.8222945 -3.7838066 -3.7863367 -3.829479 -3.9078209 -3.9412181 -3.928961 -3.9254875 -3.9629445 -4.0138607][-4.0086064 -4.0004668 -3.9823537 -3.9486198 -3.8941522 -3.8306317 -3.7908006 -3.8028281 -3.8423128 -3.8989763 -3.9181712 -3.9066768 -3.9011061 -3.9389389 -3.9942646][-4.046977 -4.0317969 -4.0134134 -3.9896281 -3.9498677 -3.911592 -3.89071 -3.9078324 -3.9298398 -3.951164 -3.9519534 -3.9426579 -3.9411478 -3.9796536 -4.0320449][-4.1135631 -4.0992694 -4.0871954 -4.0682898 -4.0390329 -4.018723 -4.012023 -4.0263872 -4.0363631 -4.0346413 -4.0253639 -4.0187979 -4.0260262 -4.06185 -4.1048222][-4.19239 -4.1861138 -4.1814966 -4.163815 -4.1390562 -4.1285987 -4.1255074 -4.1344876 -4.1429439 -4.1355896 -4.1247253 -4.1257248 -4.1359911 -4.1589518 -4.18525][-4.2465916 -4.2473631 -4.24908 -4.2353559 -4.2179475 -4.2147884 -4.2136607 -4.2182865 -4.2266455 -4.2199359 -4.2097106 -4.211092 -4.22029 -4.2281718 -4.2399921][-4.2671456 -4.2716489 -4.27642 -4.2706375 -4.2608957 -4.2607079 -4.2611618 -4.2661018 -4.2719946 -4.2671747 -4.2573733 -4.2555094 -4.2603068 -4.2623591 -4.2685089]]...]
INFO - root - 2017-12-05 17:21:19.191448: step 29510, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 74h:33m:27s remains)
INFO - root - 2017-12-05 17:21:28.477157: step 29520, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 76h:17m:25s remains)
INFO - root - 2017-12-05 17:21:38.061867: step 29530, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 77h:08m:15s remains)
INFO - root - 2017-12-05 17:21:47.374396: step 29540, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 77h:34m:50s remains)
INFO - root - 2017-12-05 17:21:56.626480: step 29550, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 79h:59m:00s remains)
INFO - root - 2017-12-05 17:22:05.911980: step 29560, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 80h:54m:00s remains)
INFO - root - 2017-12-05 17:22:15.404894: step 29570, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.948 sec/batch; 79h:46m:18s remains)
INFO - root - 2017-12-05 17:22:24.741873: step 29580, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.978 sec/batch; 82h:17m:03s remains)
INFO - root - 2017-12-05 17:22:34.029612: step 29590, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 80h:52m:51s remains)
INFO - root - 2017-12-05 17:22:42.955589: step 29600, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 77h:12m:03s remains)
2017-12-05 17:22:43.637128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1351562 -4.1576142 -4.1769133 -4.1992817 -4.223712 -4.2350245 -4.236062 -4.2448292 -4.2583237 -4.2688885 -4.278348 -4.2871909 -4.2905383 -4.2943449 -4.3067813][-4.1251974 -4.1291738 -4.1353655 -4.1534996 -4.1791072 -4.1924248 -4.1955113 -4.2066216 -4.2238584 -4.2363114 -4.247581 -4.2609344 -4.2681127 -4.275311 -4.2915554][-4.1513114 -4.1361332 -4.1241212 -4.12695 -4.1424022 -4.1509585 -4.1526623 -4.1645737 -4.1849332 -4.1985188 -4.2094383 -4.2258286 -4.2391014 -4.2527227 -4.2738175][-4.1785421 -4.1515756 -4.1262589 -4.1125021 -4.1118579 -4.1093349 -4.1051073 -4.1161828 -4.1404719 -4.1562877 -4.1674876 -4.1866846 -4.206934 -4.2270832 -4.2506824][-4.191699 -4.1655393 -4.1369815 -4.1119089 -4.0985379 -4.0819979 -4.0630789 -4.0668359 -4.0911551 -4.1094074 -4.11998 -4.13825 -4.16404 -4.1890697 -4.2167478][-4.203135 -4.1857662 -4.1548138 -4.1206055 -4.0967298 -4.0670543 -4.0305886 -4.0143576 -4.0322604 -4.0523181 -4.0643177 -4.083385 -4.1133852 -4.14635 -4.1800189][-4.1958718 -4.1874666 -4.1575046 -4.120935 -4.0925417 -4.0567474 -4.0092621 -3.9774802 -3.989392 -4.0135288 -4.0310526 -4.0550747 -4.0906978 -4.1304121 -4.1669188][-4.187314 -4.1832085 -4.1584096 -4.1256275 -4.1013308 -4.0721431 -4.0307131 -3.9972205 -4.0038395 -4.0295835 -4.0515451 -4.0774016 -4.1117673 -4.1482844 -4.1809721][-4.2045684 -4.2018132 -4.1862054 -4.1644249 -4.1477356 -4.1286721 -4.0993118 -4.0736957 -4.0769162 -4.0966487 -4.1128335 -4.1314449 -4.1559372 -4.1830025 -4.2069445][-4.2322111 -4.2328696 -4.229404 -4.2201619 -4.2104316 -4.1994214 -4.1808357 -4.1626115 -4.1638608 -4.1761627 -4.1845112 -4.193357 -4.2078609 -4.2242427 -4.2399569][-4.2573419 -4.2630663 -4.2698975 -4.2731028 -4.2727709 -4.2694349 -4.259376 -4.248548 -4.2481885 -4.254128 -4.2562513 -4.2575722 -4.2625432 -4.268218 -4.2756271][-4.2777867 -4.2845831 -4.2951441 -4.3061357 -4.3145394 -4.3196416 -4.3181086 -4.3150191 -4.3150091 -4.3175592 -4.3177266 -4.315424 -4.3119855 -4.3073225 -4.3064666][-4.2968984 -4.3016286 -4.3086104 -4.31889 -4.3290753 -4.3364034 -4.3390932 -4.3401136 -4.3423319 -4.3450947 -4.345686 -4.3432593 -4.3365965 -4.32898 -4.3253446][-4.3172517 -4.3190804 -4.320744 -4.3247814 -4.3298864 -4.3329606 -4.3330073 -4.3331132 -4.3352189 -4.3382497 -4.3388553 -4.3367085 -4.3334689 -4.3317518 -4.3333626][-4.3366046 -4.3358455 -4.3348117 -4.3350015 -4.3353653 -4.3341875 -4.331285 -4.3289709 -4.3292503 -4.3317671 -4.3336997 -4.334075 -4.3350563 -4.3377686 -4.3422532]]...]
INFO - root - 2017-12-05 17:22:52.967724: step 29610, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 78h:25m:29s remains)
INFO - root - 2017-12-05 17:23:02.331548: step 29620, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 76h:56m:05s remains)
INFO - root - 2017-12-05 17:23:11.595144: step 29630, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 75h:53m:03s remains)
INFO - root - 2017-12-05 17:23:20.900511: step 29640, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 79h:23m:52s remains)
INFO - root - 2017-12-05 17:23:30.235154: step 29650, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 80h:51m:30s remains)
INFO - root - 2017-12-05 17:23:39.617871: step 29660, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 77h:29m:28s remains)
INFO - root - 2017-12-05 17:23:48.766146: step 29670, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 83h:01m:57s remains)
INFO - root - 2017-12-05 17:23:58.146032: step 29680, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 79h:22m:33s remains)
INFO - root - 2017-12-05 17:24:07.416019: step 29690, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 72h:27m:29s remains)
INFO - root - 2017-12-05 17:24:16.620454: step 29700, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 78h:51m:11s remains)
2017-12-05 17:24:17.398819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3547897 -4.3584275 -4.3594055 -4.3441253 -4.3083792 -4.2524905 -4.1824884 -4.1362343 -4.1225691 -4.1373339 -4.1430063 -4.127655 -4.1301303 -4.1734223 -4.2237921][-4.3551731 -4.3596811 -4.3616824 -4.3477306 -4.3096766 -4.2443309 -4.1578264 -4.1028256 -4.1046681 -4.1410475 -4.1641226 -4.1534982 -4.1513948 -4.1922412 -4.2409291][-4.3546481 -4.35924 -4.360724 -4.3483734 -4.309402 -4.2358685 -4.1337056 -4.0706487 -4.0898576 -4.1503625 -4.1890917 -4.1859417 -4.1845694 -4.22187 -4.263236][-4.3548174 -4.3584752 -4.357862 -4.3452439 -4.3049445 -4.2244072 -4.1093764 -4.0392904 -4.0749197 -4.1595125 -4.2153211 -4.218493 -4.2200351 -4.2527876 -4.284646][-4.35567 -4.3583064 -4.3558221 -4.3425608 -4.3021803 -4.2136478 -4.0866385 -4.0081224 -4.0556216 -4.1614728 -4.230772 -4.2393966 -4.2446141 -4.2721033 -4.298789][-4.3569818 -4.358923 -4.3545818 -4.3404293 -4.3004513 -4.2054996 -4.0675764 -3.9800866 -4.0312448 -4.157464 -4.2412724 -4.2565312 -4.2636247 -4.28391 -4.3048687][-4.3582363 -4.3599267 -4.353683 -4.3386321 -4.3001218 -4.2053242 -4.0640669 -3.9649005 -4.007113 -4.1434712 -4.2410216 -4.2670794 -4.2758646 -4.2865424 -4.3016977][-4.3588867 -4.3596358 -4.3518023 -4.3364191 -4.3007665 -4.21195 -4.0771608 -3.9695463 -3.9913492 -4.1235895 -4.2298741 -4.2688451 -4.28237 -4.2856317 -4.2913818][-4.3584194 -4.3577766 -4.349679 -4.3354373 -4.3033009 -4.22487 -4.1065831 -4.0014176 -3.99864 -4.1099286 -4.2100034 -4.2580605 -4.2817945 -4.2836876 -4.2828741][-4.3568196 -4.3540316 -4.3463488 -4.3362656 -4.3097663 -4.2422037 -4.1426907 -4.0531287 -4.0367155 -4.1164427 -4.1955624 -4.2411504 -4.2762446 -4.2850857 -4.282063][-4.3547297 -4.3481574 -4.3398113 -4.33259 -4.3120923 -4.2552123 -4.1747842 -4.1086903 -4.0932717 -4.14671 -4.1971221 -4.2299457 -4.2728791 -4.2928028 -4.2888322][-4.35219 -4.3423085 -4.3323 -4.3256912 -4.3100619 -4.2629952 -4.2002754 -4.157218 -4.1490984 -4.1871014 -4.2142434 -4.2306838 -4.2716794 -4.2996397 -4.296979][-4.3498187 -4.3371253 -4.3245349 -4.317008 -4.30409 -4.2655687 -4.2126575 -4.1832423 -4.1858582 -4.2191181 -4.2360072 -4.2394872 -4.2724872 -4.3020411 -4.2998223][-4.34798 -4.3320088 -4.3155766 -4.3059573 -4.2930522 -4.2600126 -4.2121754 -4.1907396 -4.2012482 -4.2347827 -4.2506666 -4.2493997 -4.2745318 -4.3031468 -4.3022165][-4.3473554 -4.3288951 -4.3083563 -4.2950397 -4.2819409 -4.2524185 -4.209322 -4.191503 -4.2039342 -4.2350473 -4.2545047 -4.25956 -4.2791891 -4.30152 -4.3013368]]...]
INFO - root - 2017-12-05 17:24:26.486051: step 29710, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 72h:20m:08s remains)
INFO - root - 2017-12-05 17:24:35.846340: step 29720, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 77h:36m:20s remains)
INFO - root - 2017-12-05 17:24:45.258546: step 29730, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 78h:09m:45s remains)
INFO - root - 2017-12-05 17:24:54.438121: step 29740, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 74h:23m:44s remains)
INFO - root - 2017-12-05 17:25:03.790676: step 29750, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 77h:57m:57s remains)
INFO - root - 2017-12-05 17:25:12.986435: step 29760, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 78h:38m:15s remains)
INFO - root - 2017-12-05 17:25:22.251686: step 29770, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 80h:10m:27s remains)
INFO - root - 2017-12-05 17:25:31.591272: step 29780, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 76h:36m:40s remains)
INFO - root - 2017-12-05 17:25:40.799481: step 29790, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.970 sec/batch; 81h:32m:29s remains)
INFO - root - 2017-12-05 17:25:50.229017: step 29800, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 73h:04m:50s remains)
2017-12-05 17:25:50.985733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3084474 -4.3023272 -4.2978077 -4.2956672 -4.2973905 -4.3008084 -4.3051391 -4.3065081 -4.3012547 -4.2894826 -4.2753897 -4.2605247 -4.247117 -4.2389622 -4.2364807][-4.2814717 -4.2728252 -4.2640018 -4.2593012 -4.2605858 -4.26657 -4.27628 -4.282454 -4.28128 -4.274333 -4.2642927 -4.2508674 -4.2367458 -4.2249818 -4.2164645][-4.2603769 -4.24926 -4.2361903 -4.2286782 -4.2270312 -4.2320504 -4.2439194 -4.2537265 -4.2566237 -4.2534132 -4.2467651 -4.2373924 -4.2249432 -4.2119346 -4.2001796][-4.2394867 -4.22473 -4.2114091 -4.2027483 -4.1988478 -4.1984034 -4.2073112 -4.2186084 -4.2287283 -4.2345963 -4.2352667 -4.2328563 -4.2262855 -4.2191248 -4.211853][-4.2278552 -4.2019005 -4.1845303 -4.175518 -4.1698852 -4.1643639 -4.1665263 -4.1787148 -4.1924753 -4.2020965 -4.2070026 -4.21248 -4.217032 -4.2239747 -4.2306643][-4.2211061 -4.1739759 -4.1417122 -4.1230011 -4.1050038 -4.0878286 -4.0876951 -4.1064367 -4.1296015 -4.14647 -4.1579843 -4.1717515 -4.18907 -4.2130513 -4.2348809][-4.2014031 -4.1270509 -4.0649338 -4.0170045 -3.970216 -3.9366 -3.9446845 -3.9839616 -4.02833 -4.0612564 -4.0849361 -4.1134496 -4.1471825 -4.1841168 -4.2142782][-4.1911058 -4.1099138 -4.0238538 -3.9416349 -3.8607609 -3.807699 -3.8251526 -3.8821692 -3.9337816 -3.9731576 -4.0105729 -4.059721 -4.1112685 -4.1575303 -4.1934547][-4.198348 -4.1374207 -4.0599618 -3.9758565 -3.890053 -3.8382313 -3.8563452 -3.9052563 -3.9370859 -3.9625084 -4.0008745 -4.06029 -4.1197448 -4.1669044 -4.1998296][-4.2040019 -4.179975 -4.1347818 -4.0798969 -4.0189204 -3.9847374 -3.9944544 -4.0191965 -4.0267997 -4.0371318 -4.0678921 -4.1190953 -4.1675673 -4.2021694 -4.2253284][-4.1985049 -4.2082896 -4.1990886 -4.1813307 -4.1548934 -4.1372833 -4.1375241 -4.1425862 -4.1387968 -4.145483 -4.1677914 -4.2030067 -4.2343497 -4.253685 -4.2667708][-4.197401 -4.228107 -4.2438426 -4.2534003 -4.253129 -4.2483749 -4.2466025 -4.2471676 -4.2452431 -4.2532368 -4.269022 -4.28652 -4.2999115 -4.304831 -4.3106713][-4.223382 -4.2566123 -4.2835765 -4.3026867 -4.3106618 -4.310421 -4.3092194 -4.3104486 -4.3124862 -4.3183379 -4.328414 -4.335732 -4.3388791 -4.3391504 -4.3425045][-4.2693396 -4.2912908 -4.317452 -4.3359084 -4.3424144 -4.3409677 -4.3399925 -4.3414884 -4.3435645 -4.3461819 -4.350522 -4.3523674 -4.351439 -4.3537784 -4.3581138][-4.3148265 -4.3253641 -4.3422656 -4.356451 -4.3612442 -4.3594351 -4.357183 -4.3568182 -4.3574061 -4.3572111 -4.3572083 -4.356111 -4.356245 -4.3602271 -4.3642478]]...]
INFO - root - 2017-12-05 17:26:00.261641: step 29810, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 79h:24m:05s remains)
INFO - root - 2017-12-05 17:26:09.580578: step 29820, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.008 sec/batch; 84h:44m:11s remains)
INFO - root - 2017-12-05 17:26:19.242764: step 29830, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 79h:55m:05s remains)
INFO - root - 2017-12-05 17:26:28.369048: step 29840, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 72h:51m:26s remains)
INFO - root - 2017-12-05 17:26:37.740662: step 29850, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 76h:13m:22s remains)
INFO - root - 2017-12-05 17:26:47.097131: step 29860, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 79h:09m:51s remains)
INFO - root - 2017-12-05 17:26:56.563780: step 29870, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 81h:18m:57s remains)
INFO - root - 2017-12-05 17:27:05.695361: step 29880, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 76h:58m:43s remains)
INFO - root - 2017-12-05 17:27:15.273418: step 29890, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 82h:26m:15s remains)
INFO - root - 2017-12-05 17:27:24.412549: step 29900, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 78h:54m:55s remains)
2017-12-05 17:27:25.217723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2669482 -4.2574863 -4.2616858 -4.265471 -4.262084 -4.244658 -4.2262197 -4.2128711 -4.2154212 -4.2309232 -4.250875 -4.2694483 -4.27569 -4.29088 -4.3013568][-4.2488427 -4.2416945 -4.2447243 -4.2515078 -4.2575121 -4.2495875 -4.2350917 -4.2171407 -4.2125258 -4.2273588 -4.245007 -4.2581015 -4.2606835 -4.2701807 -4.2705188][-4.212822 -4.2065225 -4.2128086 -4.2239871 -4.2380753 -4.2397075 -4.2353148 -4.2255564 -4.2207465 -4.2314405 -4.2415128 -4.2433362 -4.2366624 -4.2333651 -4.2223186][-4.176713 -4.1702714 -4.1828079 -4.1942892 -4.20923 -4.2129445 -4.2160392 -4.2221375 -4.2278705 -4.2370858 -4.2416906 -4.2315707 -4.2149968 -4.1964812 -4.1788549][-4.1447649 -4.1458392 -4.1731334 -4.1840038 -4.1887012 -4.1824245 -4.178062 -4.1921511 -4.2113285 -4.2259836 -4.2303724 -4.21726 -4.19056 -4.1558852 -4.127676][-4.1169171 -4.1216745 -4.156199 -4.1680584 -4.1573124 -4.1289592 -4.0995927 -4.1124568 -4.1538434 -4.1818967 -4.196579 -4.1895914 -4.15934 -4.1171117 -4.0849996][-4.0927763 -4.0790486 -4.1018286 -4.1154771 -4.0988593 -4.0380144 -3.9581146 -3.9598966 -4.0411363 -4.0953989 -4.1247377 -4.1351848 -4.1210332 -4.087141 -4.0608335][-4.0669022 -4.020915 -4.0191121 -4.0194235 -3.9935529 -3.9005044 -3.7565744 -3.7438221 -3.8909581 -3.9818716 -4.0267668 -4.0641708 -4.0796785 -4.0660491 -4.056819][-4.0793681 -4.0146737 -3.9993055 -3.9857118 -3.9509902 -3.8585794 -3.7040794 -3.6818953 -3.8494329 -3.9532857 -3.99999 -4.0493984 -4.0847869 -4.0902834 -4.0981317][-4.1432338 -4.0912752 -4.0769463 -4.0593543 -4.0283647 -3.9691875 -3.8759403 -3.8556437 -3.9580278 -4.0320659 -4.0620661 -4.1011076 -4.1364269 -4.1495466 -4.1654425][-4.2129931 -4.1868472 -4.1776114 -4.1594787 -4.1330118 -4.0996442 -4.0553951 -4.0419812 -4.0907059 -4.1365218 -4.1569619 -4.1824217 -4.2048059 -4.2135258 -4.2242866][-4.2441316 -4.2443504 -4.2449837 -4.2309647 -4.2107229 -4.192102 -4.1729279 -4.1673207 -4.189899 -4.2154989 -4.231461 -4.2451077 -4.2507915 -4.2510662 -4.2562089][-4.2447386 -4.2606668 -4.2703824 -4.264864 -4.2526526 -4.2437706 -4.2337484 -4.2327528 -4.2445326 -4.2518229 -4.2536592 -4.2607856 -4.2598205 -4.25388 -4.2530308][-4.2307067 -4.2478976 -4.2611217 -4.2633781 -4.2576809 -4.2537608 -4.2469997 -4.2463083 -4.2502093 -4.2474966 -4.2428222 -4.2457376 -4.2427626 -4.2343969 -4.2301497][-4.2331824 -4.2440395 -4.252841 -4.2562919 -4.2514896 -4.2467804 -4.2413845 -4.23984 -4.2390933 -4.2345486 -4.2289968 -4.2308679 -4.2324162 -4.2286472 -4.2257113]]...]
INFO - root - 2017-12-05 17:27:34.584254: step 29910, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.913 sec/batch; 76h:42m:35s remains)
INFO - root - 2017-12-05 17:27:43.832438: step 29920, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 78h:36m:58s remains)
INFO - root - 2017-12-05 17:27:53.265842: step 29930, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 81h:50m:11s remains)
INFO - root - 2017-12-05 17:28:02.482241: step 29940, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 72h:07m:07s remains)
INFO - root - 2017-12-05 17:28:11.810270: step 29950, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 76h:25m:04s remains)
INFO - root - 2017-12-05 17:28:21.179395: step 29960, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 74h:21m:23s remains)
INFO - root - 2017-12-05 17:28:30.595300: step 29970, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 81h:09m:49s remains)
INFO - root - 2017-12-05 17:28:39.903172: step 29980, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 77h:08m:41s remains)
INFO - root - 2017-12-05 17:28:49.400103: step 29990, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 79h:38m:57s remains)
INFO - root - 2017-12-05 17:28:58.607171: step 30000, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 78h:08m:39s remains)
2017-12-05 17:28:59.354905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2399549 -4.2175097 -4.1897044 -4.1642394 -4.1466274 -4.1369658 -4.1428452 -4.1624694 -4.188139 -4.2123694 -4.2277246 -4.2342548 -4.2381182 -4.2387071 -4.2381759][-4.218235 -4.1818709 -4.137691 -4.0977235 -4.0672112 -4.046689 -4.0515447 -4.0849962 -4.1345816 -4.1754665 -4.1984792 -4.2091284 -4.2165318 -4.2212749 -4.2230415][-4.1969271 -4.1472936 -4.0847445 -4.0278969 -3.9810514 -3.9366777 -3.9292626 -3.9833863 -4.0682225 -4.1361017 -4.1717219 -4.1861773 -4.1967211 -4.2077646 -4.2171106][-4.181324 -4.1218362 -4.0499783 -3.9825299 -3.9157214 -3.8395879 -3.812541 -3.8838985 -4.0048943 -4.0989494 -4.1493697 -4.1716022 -4.1839466 -4.2007189 -4.2162585][-4.1769876 -4.1119556 -4.0350394 -3.9545412 -3.8669932 -3.7732172 -3.7398179 -3.8154864 -3.9549203 -4.0674009 -4.1293926 -4.1604023 -4.1779528 -4.1963444 -4.212779][-4.1739902 -4.1004977 -4.0180554 -3.9318519 -3.8418736 -3.7578883 -3.7361586 -3.8052917 -3.9346478 -4.0481491 -4.1140046 -4.1506343 -4.1718411 -4.1900487 -4.203814][-4.1701279 -4.0914912 -4.0048323 -3.925612 -3.8580794 -3.8150122 -3.8178859 -3.8764327 -3.9728975 -4.0608525 -4.1175141 -4.151895 -4.170939 -4.1881928 -4.1997566][-4.1815619 -4.1082411 -4.0280614 -3.9582319 -3.9143682 -3.9059217 -3.93015 -3.9771965 -4.0434375 -4.108573 -4.1421824 -4.1621022 -4.1786995 -4.1980791 -4.210259][-4.1893616 -4.132041 -4.0653019 -4.0120711 -3.986392 -3.9907749 -4.0130372 -4.0501008 -4.1016073 -4.1563983 -4.1763735 -4.1849775 -4.2012014 -4.2234225 -4.2331495][-4.1888733 -4.1452532 -4.0911841 -4.0598993 -4.0467086 -4.0444026 -4.04774 -4.0700984 -4.1131492 -4.1656265 -4.1883864 -4.1981006 -4.2194724 -4.2420292 -4.2493868][-4.198256 -4.1628122 -4.1103172 -4.0836039 -4.0748124 -4.0542936 -4.0333352 -4.0414219 -4.0804605 -4.1407223 -4.18017 -4.2047172 -4.232801 -4.254982 -4.2624831][-4.229908 -4.2026997 -4.156436 -4.1228848 -4.1006193 -4.0616131 -4.0207043 -4.0186539 -4.0577087 -4.1250916 -4.1828961 -4.2226138 -4.2529268 -4.27417 -4.2790136][-4.2588134 -4.2402024 -4.2021465 -4.16033 -4.1198111 -4.0690923 -4.0228386 -4.0251961 -4.0741959 -4.1438804 -4.2072372 -4.251708 -4.2788095 -4.2927871 -4.2895303][-4.2708054 -4.2559967 -4.2228589 -4.1752653 -4.1222863 -4.0685649 -4.0334015 -4.0491638 -4.1048007 -4.1716108 -4.228313 -4.2685084 -4.2881341 -4.2901912 -4.2746611][-4.2687111 -4.2521653 -4.2245717 -4.1846275 -4.1344318 -4.0870652 -4.0684223 -4.0931416 -4.14341 -4.1989384 -4.2458467 -4.2784042 -4.2881331 -4.2748132 -4.2485876]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 17:29:09.277147: step 30010, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.976 sec/batch; 81h:58m:58s remains)
INFO - root - 2017-12-05 17:29:18.479133: step 30020, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 75h:45m:47s remains)
INFO - root - 2017-12-05 17:29:27.636376: step 30030, loss = 2.10, batch loss = 2.05 (9.3 examples/sec; 0.861 sec/batch; 72h:20m:29s remains)
INFO - root - 2017-12-05 17:29:36.734581: step 30040, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.828 sec/batch; 69h:36m:08s remains)
INFO - root - 2017-12-05 17:29:45.982986: step 30050, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.945 sec/batch; 79h:24m:43s remains)
INFO - root - 2017-12-05 17:29:55.360870: step 30060, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.954 sec/batch; 80h:07m:46s remains)
INFO - root - 2017-12-05 17:30:04.602203: step 30070, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 71h:13m:33s remains)
INFO - root - 2017-12-05 17:30:13.869771: step 30080, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 77h:33m:28s remains)
INFO - root - 2017-12-05 17:30:23.235861: step 30090, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 83h:55m:49s remains)
INFO - root - 2017-12-05 17:30:32.392946: step 30100, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 79h:54m:39s remains)
2017-12-05 17:30:33.110878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2595863 -4.2598648 -4.2586288 -4.2553425 -4.2539344 -4.26446 -4.281816 -4.299952 -4.3053789 -4.2967443 -4.2803807 -4.2623343 -4.249464 -4.2432961 -4.24035][-4.2358413 -4.2440634 -4.2551107 -4.2619514 -4.269732 -4.2875695 -4.3079853 -4.3253965 -4.3270254 -4.3143077 -4.2970629 -4.279263 -4.2601461 -4.2495704 -4.2478857][-4.2108669 -4.217885 -4.2313161 -4.2394252 -4.2517328 -4.2770085 -4.3063731 -4.3278279 -4.3281512 -4.3135409 -4.295682 -4.2763062 -4.2526894 -4.2359014 -4.2353249][-4.1911583 -4.179184 -4.1745868 -4.170393 -4.181612 -4.2167454 -4.2615781 -4.2950921 -4.3034625 -4.2967415 -4.2877555 -4.2718391 -4.2448564 -4.2169662 -4.2106218][-4.1873817 -4.1507192 -4.114913 -4.0847688 -4.0849414 -4.1272178 -4.1879253 -4.2363291 -4.2578626 -4.2639952 -4.2641568 -4.2561851 -4.2338877 -4.199544 -4.1815548][-4.212 -4.1650991 -4.1073589 -4.0465951 -4.0176377 -4.0486145 -4.1098795 -4.1684184 -4.2020426 -4.2198405 -4.229744 -4.2292376 -4.2145023 -4.1772385 -4.1437144][-4.2517276 -4.212707 -4.1569667 -4.0876017 -4.0364337 -4.0325704 -4.0602455 -4.1022983 -4.1349349 -4.1610928 -4.1853657 -4.1934934 -4.1837444 -4.1469069 -4.1033459][-4.2907457 -4.2671943 -4.2297516 -4.1754823 -4.1242638 -4.0951281 -4.0837307 -4.0893164 -4.1012669 -4.1246133 -4.1556635 -4.1751637 -4.1728916 -4.14124 -4.0930219][-4.3085761 -4.29781 -4.2809548 -4.2515125 -4.2138019 -4.1796875 -4.1539187 -4.1381636 -4.1314926 -4.1457295 -4.1738119 -4.1968164 -4.2012835 -4.1810021 -4.1399393][-4.2935491 -4.2881522 -4.2850008 -4.2778053 -4.258635 -4.2390556 -4.2259803 -4.2131004 -4.2004466 -4.2014179 -4.2232695 -4.242878 -4.2479758 -4.2361317 -4.2101622][-4.2645221 -4.257103 -4.2567697 -4.259336 -4.2548618 -4.2534904 -4.2596006 -4.2626271 -4.2568064 -4.2515078 -4.2647672 -4.2802377 -4.2832246 -4.2738428 -4.2585349][-4.2367835 -4.2227407 -4.2141161 -4.2113481 -4.212904 -4.2242155 -4.2454915 -4.2606964 -4.2666993 -4.2688313 -4.2782636 -4.292738 -4.2979379 -4.2939987 -4.2865906][-4.22735 -4.2043648 -4.1820464 -4.16439 -4.1610832 -4.1751494 -4.2063036 -4.2304044 -4.243803 -4.2584238 -4.2729926 -4.2829962 -4.2895393 -4.2941618 -4.2974186][-4.2376471 -4.2083039 -4.1757212 -4.1469769 -4.134397 -4.1455569 -4.180645 -4.2099738 -4.2280145 -4.2500229 -4.2679768 -4.2763019 -4.2823548 -4.2904177 -4.2987552][-4.2601318 -4.2294068 -4.1963272 -4.1669631 -4.1509023 -4.1567483 -4.1909966 -4.221756 -4.238946 -4.2549486 -4.265264 -4.2720284 -4.2787189 -4.288547 -4.2996497]]...]
INFO - root - 2017-12-05 17:30:42.318817: step 30110, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 82h:18m:22s remains)
INFO - root - 2017-12-05 17:30:51.732973: step 30120, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 80h:23m:56s remains)
INFO - root - 2017-12-05 17:31:00.676639: step 30130, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 76h:58m:08s remains)
INFO - root - 2017-12-05 17:31:09.986193: step 30140, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 80h:02m:20s remains)
INFO - root - 2017-12-05 17:31:19.454087: step 30150, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 78h:40m:38s remains)
INFO - root - 2017-12-05 17:31:28.923462: step 30160, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 80h:28m:44s remains)
INFO - root - 2017-12-05 17:31:38.277236: step 30170, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 70h:31m:41s remains)
INFO - root - 2017-12-05 17:31:47.647792: step 30180, loss = 2.10, batch loss = 2.05 (8.1 examples/sec; 0.984 sec/batch; 82h:37m:05s remains)
INFO - root - 2017-12-05 17:31:56.359685: step 30190, loss = 2.07, batch loss = 2.02 (10.7 examples/sec; 0.751 sec/batch; 63h:02m:28s remains)
INFO - root - 2017-12-05 17:32:05.962302: step 30200, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 79h:45m:49s remains)
2017-12-05 17:32:06.725086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2936935 -4.2895522 -4.2827244 -4.27332 -4.2606888 -4.2468028 -4.236907 -4.2346725 -4.2411671 -4.2463274 -4.2314477 -4.207139 -4.1885185 -4.1734371 -4.1690111][-4.2709188 -4.25921 -4.2393312 -4.217876 -4.1975303 -4.1791172 -4.1685414 -4.174264 -4.1934123 -4.207902 -4.1964879 -4.1751122 -4.1597233 -4.1457114 -4.1375356][-4.2536135 -4.2326851 -4.197197 -4.1613011 -4.1308236 -4.1065936 -4.0978756 -4.1178718 -4.1586137 -4.1900525 -4.188107 -4.1720929 -4.1579175 -4.1407032 -4.1230907][-4.2493744 -4.2206688 -4.1708302 -4.1190996 -4.0751491 -4.039825 -4.0307417 -4.0683675 -4.1393495 -4.1968946 -4.2098608 -4.1970587 -4.1780629 -4.1501446 -4.1182833][-4.2505817 -4.2185931 -4.1590343 -4.08875 -4.0180454 -3.9594474 -3.946089 -4.0098057 -4.1184835 -4.2064919 -4.2392917 -4.2325315 -4.2072277 -4.1702824 -4.1274495][-4.2494593 -4.218895 -4.1542459 -4.0611982 -3.9473028 -3.8421674 -3.8183713 -3.9195411 -4.0752769 -4.1975145 -4.25366 -4.2608037 -4.2339373 -4.1896992 -4.1403546][-4.2455583 -4.2199869 -4.153851 -4.03785 -3.8743112 -3.7064824 -3.6621008 -3.808454 -4.0148716 -4.1699123 -4.2505393 -4.2746115 -4.2479453 -4.1956062 -4.1397715][-4.236475 -4.221272 -4.1628933 -4.0396008 -3.8495796 -3.6397834 -3.5720518 -3.7454932 -3.9781191 -4.146781 -4.2409067 -4.2765307 -4.2508383 -4.1909547 -4.1308613][-4.2328959 -4.2338867 -4.1955652 -4.0924044 -3.9236469 -3.7368131 -3.6721406 -3.8171184 -4.0204787 -4.1681314 -4.24837 -4.2765479 -4.2469387 -4.1820054 -4.1220851][-4.2157707 -4.2350054 -4.2217083 -4.1518207 -4.0289297 -3.8967786 -3.8544064 -3.9562025 -4.1029406 -4.2103052 -4.26419 -4.2744007 -4.2376709 -4.1732507 -4.1171403][-4.1944404 -4.2254987 -4.2339482 -4.1940846 -4.1092954 -4.0227127 -4.0017371 -4.0708723 -4.1698775 -4.2430811 -4.2755165 -4.2712722 -4.2301393 -4.1701818 -4.1233196][-4.1984434 -4.2308774 -4.2492785 -4.2315035 -4.1752448 -4.1156964 -4.1015329 -4.1487055 -4.2175269 -4.2694812 -4.2910991 -4.2809339 -4.2399769 -4.1888838 -4.1566348][-4.22756 -4.2530637 -4.2695951 -4.2629375 -4.2280688 -4.1883106 -4.175334 -4.2056956 -4.2536669 -4.2909565 -4.305655 -4.2951622 -4.2617416 -4.2227097 -4.2042751][-4.2716808 -4.2898297 -4.3008018 -4.2995973 -4.2821336 -4.2597556 -4.2488 -4.2637 -4.2933912 -4.3164921 -4.3219538 -4.3112621 -4.2880573 -4.2632747 -4.2557197][-4.3023586 -4.3152504 -4.3222237 -4.3242416 -4.3182054 -4.3084316 -4.300899 -4.3055682 -4.321032 -4.3325434 -4.3315358 -4.3217626 -4.3078814 -4.2953396 -4.2941852]]...]
INFO - root - 2017-12-05 17:32:16.026892: step 30210, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.980 sec/batch; 82h:18m:08s remains)
INFO - root - 2017-12-05 17:32:25.385282: step 30220, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 75h:19m:07s remains)
INFO - root - 2017-12-05 17:32:35.025532: step 30230, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.970 sec/batch; 81h:24m:32s remains)
INFO - root - 2017-12-05 17:32:44.519671: step 30240, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 82h:37m:56s remains)
INFO - root - 2017-12-05 17:32:53.728092: step 30250, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 75h:41m:42s remains)
INFO - root - 2017-12-05 17:33:03.234673: step 30260, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 76h:55m:51s remains)
INFO - root - 2017-12-05 17:33:12.838186: step 30270, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 83h:02m:35s remains)
INFO - root - 2017-12-05 17:33:22.229686: step 30280, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 72h:36m:42s remains)
INFO - root - 2017-12-05 17:33:31.722467: step 30290, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 77h:12m:44s remains)
INFO - root - 2017-12-05 17:33:40.985424: step 30300, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.010 sec/batch; 84h:47m:25s remains)
2017-12-05 17:33:41.756474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.34919 -4.3484483 -4.3370476 -4.3236442 -4.3156343 -4.3145671 -4.3163352 -4.3191791 -4.3163724 -4.3127775 -4.3120837 -4.3142047 -4.3178663 -4.3242621 -4.3292565][-4.3485117 -4.3410187 -4.3178239 -4.2955751 -4.2845774 -4.2801161 -4.2770452 -4.2750854 -4.270751 -4.2688551 -4.2749858 -4.2832851 -4.290504 -4.2976522 -4.3033147][-4.3450508 -4.3284535 -4.295579 -4.2665691 -4.2543974 -4.2451582 -4.2330942 -4.2211504 -4.2117438 -4.2119765 -4.2291379 -4.2473826 -4.257884 -4.2635522 -4.26952][-4.3393054 -4.3188305 -4.2835221 -4.2510924 -4.2350888 -4.2162948 -4.1885471 -4.1564097 -4.1382914 -4.1494122 -4.1848736 -4.2140622 -4.2288923 -4.2370424 -4.2444978][-4.3392658 -4.3181143 -4.2835026 -4.24881 -4.2273932 -4.1973362 -4.1482916 -4.0962515 -4.075798 -4.1027164 -4.1568723 -4.1968303 -4.2203813 -4.2271352 -4.2283616][-4.3364739 -4.3140559 -4.2807465 -4.2462292 -4.21728 -4.1728086 -4.1066084 -4.0457907 -4.0321918 -4.0733137 -4.1329432 -4.1742592 -4.1983476 -4.2016497 -4.1980705][-4.3294253 -4.306231 -4.2740312 -4.2414989 -4.2016635 -4.1450377 -4.0753241 -4.0204163 -4.0189519 -4.0629659 -4.1149621 -4.1420026 -4.1510625 -4.1539292 -4.150322][-4.3227248 -4.298173 -4.262239 -4.2221642 -4.1734457 -4.1169415 -4.0594831 -4.0263782 -4.0352244 -4.066678 -4.0945597 -4.0997491 -4.097816 -4.1002312 -4.0945244][-4.3168464 -4.2903614 -4.2481308 -4.19927 -4.1483712 -4.1029162 -4.0678921 -4.0585012 -4.0745697 -4.089839 -4.0905085 -4.0750613 -4.0616989 -4.0577569 -4.0478249][-4.3142509 -4.2845325 -4.2386737 -4.1844816 -4.1336837 -4.0992665 -4.0839062 -4.0965085 -4.121788 -4.1280274 -4.115766 -4.0875244 -4.0585465 -4.0421429 -4.0234489][-4.3139911 -4.2826324 -4.2379923 -4.1869698 -4.1400557 -4.1156468 -4.1159544 -4.1373367 -4.1612935 -4.1635728 -4.1513224 -4.1161051 -4.0772076 -4.05153 -4.0247583][-4.3169622 -4.2859964 -4.2455425 -4.2039533 -4.1649141 -4.148036 -4.1531196 -4.171124 -4.1850233 -4.1808739 -4.1688585 -4.1362391 -4.1004305 -4.0742803 -4.0491948][-4.3230171 -4.2965736 -4.2656927 -4.2361846 -4.2095861 -4.200141 -4.2033653 -4.215354 -4.2231321 -4.21529 -4.1997957 -4.168323 -4.1388807 -4.1177239 -4.0999618][-4.3296456 -4.3113728 -4.29012 -4.2698264 -4.2544904 -4.250299 -4.2522941 -4.2585096 -4.2621856 -4.2542505 -4.2389832 -4.214107 -4.1919932 -4.176774 -4.1663184][-4.3374338 -4.3253565 -4.3106308 -4.2971339 -4.2882314 -4.2863932 -4.2859321 -4.2860346 -4.2869148 -4.2820468 -4.2727032 -4.257441 -4.2430363 -4.2330527 -4.2294211]]...]
INFO - root - 2017-12-05 17:33:51.051733: step 30310, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.976 sec/batch; 81h:55m:16s remains)
INFO - root - 2017-12-05 17:34:00.471763: step 30320, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 81h:14m:39s remains)
INFO - root - 2017-12-05 17:34:10.015884: step 30330, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.986 sec/batch; 82h:43m:57s remains)
INFO - root - 2017-12-05 17:34:19.428707: step 30340, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 83h:32m:17s remains)
INFO - root - 2017-12-05 17:34:28.432540: step 30350, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 74h:50m:15s remains)
INFO - root - 2017-12-05 17:34:37.731425: step 30360, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 80h:02m:57s remains)
INFO - root - 2017-12-05 17:34:47.353635: step 30370, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 82h:06m:31s remains)
INFO - root - 2017-12-05 17:34:56.558372: step 30380, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 79h:37m:54s remains)
INFO - root - 2017-12-05 17:35:05.890453: step 30390, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 78h:50m:00s remains)
INFO - root - 2017-12-05 17:35:15.335583: step 30400, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 82h:43m:28s remains)
2017-12-05 17:35:16.057258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2841835 -4.24379 -4.2086806 -4.1883097 -4.1980972 -4.2213659 -4.247086 -4.2666407 -4.2806792 -4.2877936 -4.2888174 -4.2832789 -4.2776566 -4.2739916 -4.2731805][-4.2823644 -4.2324972 -4.1852055 -4.1547971 -4.1589408 -4.181026 -4.20751 -4.2315097 -4.2595124 -4.2815995 -4.2939606 -4.2937131 -4.28796 -4.2814193 -4.2787323][-4.2922421 -4.2417231 -4.1907792 -4.1529527 -4.144557 -4.1554294 -4.1754861 -4.1980877 -4.234067 -4.2728696 -4.2970438 -4.3031158 -4.3012033 -4.2941084 -4.2884264][-4.3056803 -4.2580032 -4.2088656 -4.1649828 -4.137126 -4.1293635 -4.1417003 -4.1613011 -4.20149 -4.2511506 -4.2833891 -4.2934504 -4.2956533 -4.2926254 -4.2871127][-4.3187571 -4.2751784 -4.227829 -4.1771245 -4.1258578 -4.0907784 -4.0931373 -4.1171565 -4.1630974 -4.2173605 -4.2523379 -4.2629561 -4.2615538 -4.2609248 -4.2622113][-4.3238487 -4.2860923 -4.2392354 -4.1835227 -4.1182013 -4.0573945 -4.0396419 -4.0661087 -4.1218271 -4.1836891 -4.2225928 -4.2326107 -4.2243724 -4.2203975 -4.2247977][-4.3236012 -4.287724 -4.2414236 -4.1859183 -4.1215553 -4.0551457 -4.0235891 -4.0373025 -4.0892444 -4.1549039 -4.1996617 -4.2079568 -4.1930971 -4.1823473 -4.1817007][-4.3213086 -4.2816133 -4.2331867 -4.1757054 -4.1184211 -4.0593576 -4.026679 -4.0299821 -4.0655694 -4.1247425 -4.1727757 -4.1851077 -4.1665378 -4.1463933 -4.1380529][-4.321125 -4.2778983 -4.22617 -4.1664181 -4.1133394 -4.0619512 -4.0311489 -4.0262842 -4.0521731 -4.095664 -4.135263 -4.1513562 -4.1365676 -4.1106758 -4.1054282][-4.3233123 -4.283308 -4.2340961 -4.1763964 -4.123105 -4.0749083 -4.0505295 -4.0453758 -4.0658064 -4.0925055 -4.1135559 -4.1250281 -4.1086287 -4.0825415 -4.0860167][-4.3248034 -4.2931414 -4.255537 -4.2091961 -4.1602507 -4.1128063 -4.0904512 -4.0894666 -4.1081643 -4.1227541 -4.1228724 -4.1203613 -4.09829 -4.0754886 -4.0837989][-4.3238325 -4.3006063 -4.2771144 -4.2495513 -4.2189507 -4.1851516 -4.1645613 -4.1619506 -4.1704383 -4.1672144 -4.1468544 -4.1277046 -4.1048789 -4.0844412 -4.0991268][-4.3198996 -4.3004932 -4.2891369 -4.2802162 -4.2711563 -4.2562122 -4.24553 -4.2390347 -4.2315516 -4.2098465 -4.1713972 -4.1412773 -4.1179314 -4.1030097 -4.1222606][-4.3145642 -4.295248 -4.2899036 -4.2935348 -4.3017445 -4.3049889 -4.3070383 -4.3022046 -4.2819924 -4.2432623 -4.18916 -4.1495323 -4.1307235 -4.12781 -4.1512761][-4.3077831 -4.2873564 -4.283339 -4.2917728 -4.3101454 -4.3264637 -4.3389268 -4.3366337 -4.3104129 -4.2636356 -4.1985393 -4.1504407 -4.1356478 -4.1451073 -4.1746902]]...]
INFO - root - 2017-12-05 17:35:25.474500: step 30410, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.934 sec/batch; 78h:23m:55s remains)
INFO - root - 2017-12-05 17:35:34.926193: step 30420, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 81h:41m:38s remains)
INFO - root - 2017-12-05 17:35:44.581394: step 30430, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 79h:05m:57s remains)
INFO - root - 2017-12-05 17:35:53.907315: step 30440, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 77h:15m:14s remains)
INFO - root - 2017-12-05 17:36:03.376654: step 30450, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 76h:17m:38s remains)
INFO - root - 2017-12-05 17:36:12.631759: step 30460, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 80h:09m:01s remains)
INFO - root - 2017-12-05 17:36:21.474989: step 30470, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 75h:29m:43s remains)
INFO - root - 2017-12-05 17:36:30.762909: step 30480, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 77h:44m:58s remains)
INFO - root - 2017-12-05 17:36:39.986704: step 30490, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 78h:12m:30s remains)
INFO - root - 2017-12-05 17:36:49.562181: step 30500, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 82h:24m:43s remains)
2017-12-05 17:36:50.266868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2691188 -4.2672524 -4.2707639 -4.2589874 -4.2236996 -4.1589694 -4.1104493 -4.0998373 -4.1160617 -4.1417027 -4.1770549 -4.2146506 -4.2450986 -4.2627387 -4.2699041][-4.2339745 -4.2447557 -4.2570734 -4.2537556 -4.225738 -4.16362 -4.108851 -4.0856943 -4.0930519 -4.1179943 -4.1569924 -4.1933389 -4.2211084 -4.2354631 -4.2393332][-4.2122259 -4.2358823 -4.2580342 -4.2669663 -4.2533236 -4.2086954 -4.1622863 -4.1351881 -4.1333842 -4.1508851 -4.1799364 -4.200151 -4.2093987 -4.2086658 -4.2005963][-4.2100019 -4.239337 -4.263967 -4.2749786 -4.2687922 -4.243896 -4.2149734 -4.1967788 -4.1916819 -4.19905 -4.2130222 -4.2106757 -4.1934729 -4.168704 -4.1429167][-4.2083411 -4.237474 -4.2569203 -4.2589083 -4.2452793 -4.2300529 -4.2163243 -4.207408 -4.2000489 -4.1986332 -4.2038217 -4.19182 -4.1609406 -4.1169915 -4.0790076][-4.2072864 -4.230329 -4.2381597 -4.224793 -4.1898847 -4.167726 -4.1611638 -4.1584454 -4.1511879 -4.1440673 -4.1455727 -4.1418171 -4.1069078 -4.0552263 -4.0140696][-4.206254 -4.220458 -4.2172818 -4.1876044 -4.1292787 -4.0863404 -4.0810347 -4.0873785 -4.084816 -4.0764117 -4.0853872 -4.1013875 -4.0745468 -4.0222721 -3.9842103][-4.2088361 -4.212451 -4.19693 -4.1567249 -4.0896597 -4.0366497 -4.0288711 -4.0416026 -4.0413275 -4.0376773 -4.057529 -4.0938845 -4.0862656 -4.0469837 -4.0112443][-4.2132411 -4.2116575 -4.190536 -4.1495008 -4.0866427 -4.0362625 -4.01837 -4.0251927 -4.0243015 -4.03253 -4.0667257 -4.1100893 -4.1125746 -4.0893507 -4.0612078][-4.2138944 -4.2086282 -4.1887183 -4.1510706 -4.0958982 -4.0506349 -4.0213428 -4.0104289 -4.0002017 -4.0185671 -4.0670118 -4.1162219 -4.1163969 -4.10132 -4.0785627][-4.223474 -4.2154317 -4.1960182 -4.1634097 -4.1228714 -4.0878739 -4.05724 -4.033731 -4.0101323 -4.0275893 -4.0770636 -4.1233644 -4.118031 -4.1099229 -4.0952339][-4.2276235 -4.2213693 -4.208529 -4.1875882 -4.1606064 -4.132751 -4.1049714 -4.0782022 -4.0532403 -4.0672145 -4.112236 -4.1489878 -4.1445312 -4.1445622 -4.1360245][-4.2189455 -4.2219844 -4.2233434 -4.2196279 -4.2068329 -4.1882892 -4.1655359 -4.1356249 -4.1142197 -4.1245947 -4.1614246 -4.1888294 -4.19117 -4.1942177 -4.1829052][-4.2141075 -4.2279229 -4.2452455 -4.2581854 -4.2557182 -4.2447987 -4.2266145 -4.1993136 -4.1826367 -4.1887116 -4.213356 -4.2315426 -4.2411227 -4.2451763 -4.2364945][-4.2416835 -4.2568421 -4.2775373 -4.296936 -4.2999015 -4.2954955 -4.2840557 -4.2637568 -4.2512903 -4.2531495 -4.2652769 -4.2777853 -4.2923789 -4.3009839 -4.3002777]]...]
INFO - root - 2017-12-05 17:36:59.735806: step 30510, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 78h:17m:32s remains)
INFO - root - 2017-12-05 17:37:09.115897: step 30520, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 81h:41m:38s remains)
INFO - root - 2017-12-05 17:37:18.620142: step 30530, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 82h:03m:30s remains)
INFO - root - 2017-12-05 17:37:28.003336: step 30540, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 80h:32m:24s remains)
INFO - root - 2017-12-05 17:37:37.409813: step 30550, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 81h:22m:06s remains)
INFO - root - 2017-12-05 17:37:46.876972: step 30560, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 84h:02m:21s remains)
INFO - root - 2017-12-05 17:37:56.178155: step 30570, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 79h:05m:26s remains)
INFO - root - 2017-12-05 17:38:05.583526: step 30580, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 82h:01m:22s remains)
INFO - root - 2017-12-05 17:38:14.993479: step 30590, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 75h:07m:45s remains)
INFO - root - 2017-12-05 17:38:24.461116: step 30600, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.009 sec/batch; 84h:39m:01s remains)
2017-12-05 17:38:25.258090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2717838 -4.2746086 -4.284956 -4.29061 -4.2823095 -4.2744493 -4.2735043 -4.2717338 -4.2721152 -4.2700105 -4.2769723 -4.2909317 -4.2907391 -4.2821894 -4.2749081][-4.3107858 -4.3186417 -4.3295794 -4.3365836 -4.3311195 -4.3249745 -4.3230362 -4.3210092 -4.3205862 -4.31811 -4.3224888 -4.3318653 -4.3300238 -4.3207688 -4.3099585][-4.3381476 -4.3465209 -4.355392 -4.360713 -4.3545356 -4.34806 -4.345077 -4.3425007 -4.3410268 -4.3381486 -4.339 -4.3438935 -4.3411217 -4.3301892 -4.3155584][-4.3453503 -4.3521996 -4.3566995 -4.3565512 -4.3465004 -4.3360538 -4.3325882 -4.3313274 -4.3286719 -4.3242555 -4.3214474 -4.3224273 -4.3178449 -4.3040285 -4.2891536][-4.3276 -4.3290157 -4.3255134 -4.3166108 -4.2985444 -4.2792311 -4.2733545 -4.2751503 -4.2718749 -4.2651744 -4.2598262 -4.259109 -4.2548394 -4.2422552 -4.2317648][-4.2872057 -4.2764912 -4.2630072 -4.2451596 -4.2174888 -4.1831274 -4.17145 -4.1801672 -4.1812468 -4.1773214 -4.1740003 -4.1756196 -4.1759248 -4.1692066 -4.1647491][-4.2356329 -4.213603 -4.1908765 -4.1656728 -4.1274004 -4.072649 -4.0498319 -4.0708957 -4.0844874 -4.0878549 -4.0914764 -4.0977573 -4.1036224 -4.1055455 -4.1093173][-4.2069035 -4.1830831 -4.1607175 -4.1354761 -4.0948019 -4.0319672 -3.9993644 -4.0247245 -4.0477319 -4.0585051 -4.0693994 -4.0779295 -4.0791988 -4.0798769 -4.0877643][-4.2201514 -4.209228 -4.1999063 -4.1852431 -4.1564822 -4.1113772 -4.085886 -4.097959 -4.1112256 -4.1181345 -4.127759 -4.1325874 -4.1234026 -4.1145358 -4.1186419][-4.2423372 -4.244709 -4.2473717 -4.2399721 -4.221271 -4.1948252 -4.1784587 -4.1773591 -4.1792469 -4.1796036 -4.1876578 -4.1930203 -4.1829014 -4.1701217 -4.1717644][-4.2632074 -4.2711759 -4.2809305 -4.2794452 -4.2661891 -4.2505045 -4.239861 -4.2338095 -4.2319741 -4.2290549 -4.2357569 -4.2451282 -4.2402053 -4.2294774 -4.2286468][-4.2902355 -4.2997284 -4.3119836 -4.314754 -4.305685 -4.2955704 -4.2865849 -4.2804656 -4.2793603 -4.2773333 -4.2831826 -4.2941842 -4.2941236 -4.286788 -4.2843156][-4.3235011 -4.3334351 -4.3437033 -4.3484526 -4.3423872 -4.3349433 -4.3273144 -4.3222055 -4.3211842 -4.3207932 -4.3255391 -4.3355885 -4.3380642 -4.3343797 -4.3321981][-4.3453116 -4.3537326 -4.3608708 -4.3654122 -4.3622923 -4.3568935 -4.3513827 -4.3476143 -4.3460026 -4.345078 -4.3483233 -4.3567061 -4.3620529 -4.3632588 -4.3630691][-4.3511071 -4.3570027 -4.3605056 -4.3631139 -4.3624263 -4.3589187 -4.3549056 -4.3523111 -4.3512497 -4.3507943 -4.3527536 -4.3589821 -4.3650742 -4.369122 -4.3714457]]...]
INFO - root - 2017-12-05 17:38:34.356740: step 30610, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 76h:19m:37s remains)
INFO - root - 2017-12-05 17:38:43.805057: step 30620, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 74h:07m:30s remains)
INFO - root - 2017-12-05 17:38:53.198877: step 30630, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 79h:28m:16s remains)
INFO - root - 2017-12-05 17:39:02.477278: step 30640, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 79h:05m:20s remains)
INFO - root - 2017-12-05 17:39:11.924714: step 30650, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 79h:23m:53s remains)
INFO - root - 2017-12-05 17:39:21.366599: step 30660, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 82h:22m:55s remains)
INFO - root - 2017-12-05 17:39:30.620284: step 30670, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 78h:51m:28s remains)
INFO - root - 2017-12-05 17:39:39.834725: step 30680, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.960 sec/batch; 80h:31m:10s remains)
INFO - root - 2017-12-05 17:39:49.168011: step 30690, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.958 sec/batch; 80h:20m:35s remains)
INFO - root - 2017-12-05 17:39:58.804506: step 30700, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 80h:04m:14s remains)
2017-12-05 17:39:59.596953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2584581 -4.2636104 -4.2622633 -4.2725725 -4.283864 -4.2875562 -4.2801967 -4.2595434 -4.2361732 -4.2190266 -4.2000723 -4.1785836 -4.1542525 -4.1381531 -4.1432605][-4.242291 -4.2566171 -4.2664275 -4.2829108 -4.2915235 -4.2900772 -4.285522 -4.2712069 -4.2549939 -4.2385945 -4.2180166 -4.1960235 -4.17065 -4.1522765 -4.148314][-4.2541127 -4.2664804 -4.2762904 -4.2878008 -4.2883368 -4.2798514 -4.2717748 -4.2579932 -4.2453904 -4.2267337 -4.2025533 -4.1809349 -4.161644 -4.1457081 -4.1356564][-4.2816081 -4.2890916 -4.2916508 -4.2898679 -4.276741 -4.2576947 -4.2416072 -4.2251973 -4.2115846 -4.1860857 -4.157361 -4.1408434 -4.1369357 -4.132854 -4.1210027][-4.2974191 -4.30477 -4.3022623 -4.2903275 -4.2634554 -4.2302794 -4.2050524 -4.1833911 -4.1667194 -4.1352162 -4.1040964 -4.1025743 -4.1222596 -4.13269 -4.1202726][-4.29744 -4.305594 -4.299778 -4.2782688 -4.2401094 -4.1951251 -4.1621947 -4.1421614 -4.135323 -4.1135173 -4.0912347 -4.1037807 -4.1371837 -4.1514354 -4.134706][-4.2876639 -4.2937326 -4.2836771 -4.2554421 -4.2107711 -4.15854 -4.121624 -4.1096263 -4.1251535 -4.1299868 -4.1267104 -4.1441488 -4.1719666 -4.1760983 -4.1488457][-4.2720704 -4.2755275 -4.2655497 -4.2380691 -4.1942873 -4.1427622 -4.1036205 -4.0965228 -4.1322527 -4.1659765 -4.1832309 -4.2015071 -4.214262 -4.2001648 -4.1607571][-4.256052 -4.2571068 -4.2534456 -4.2361712 -4.2005353 -4.154263 -4.1132174 -4.0997162 -4.1362667 -4.1825571 -4.2112365 -4.230618 -4.2361684 -4.2118745 -4.167583][-4.241128 -4.2425056 -4.2479367 -4.2461195 -4.2228394 -4.1827679 -4.1398344 -4.1148486 -4.139091 -4.1840839 -4.2145238 -4.2343597 -4.238246 -4.2136607 -4.1775417][-4.2291131 -4.23396 -4.2503409 -4.2624307 -4.2516637 -4.2193227 -4.1772828 -4.1457076 -4.1555667 -4.1883907 -4.2137513 -4.2327304 -4.237103 -4.2199821 -4.1968374][-4.2229881 -4.2247915 -4.2461276 -4.2675371 -4.268364 -4.2470222 -4.2146816 -4.1867995 -4.1873827 -4.2057209 -4.222084 -4.2343683 -4.2386746 -4.2311144 -4.219944][-4.2135763 -4.2163477 -4.2397542 -4.2644444 -4.2702618 -4.2577863 -4.2393494 -4.2230587 -4.2207818 -4.2294211 -4.2359943 -4.24042 -4.2419019 -4.2409458 -4.230988][-4.1894121 -4.1994004 -4.22483 -4.2473421 -4.2530289 -4.244442 -4.2371254 -4.2310691 -4.2293382 -4.2312655 -4.228991 -4.2235451 -4.2190638 -4.2210622 -4.2114549][-4.1655731 -4.1854548 -4.2112865 -4.2278118 -4.229599 -4.2224183 -4.2226868 -4.2256994 -4.2259808 -4.2224541 -4.2111535 -4.1954856 -4.1836057 -4.1842 -4.1767511]]...]
INFO - root - 2017-12-05 17:40:08.740306: step 30710, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 73h:19m:00s remains)
INFO - root - 2017-12-05 17:40:18.080050: step 30720, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 76h:43m:04s remains)
INFO - root - 2017-12-05 17:40:27.416007: step 30730, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 78h:55m:30s remains)
INFO - root - 2017-12-05 17:40:36.893833: step 30740, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 79h:12m:01s remains)
INFO - root - 2017-12-05 17:40:46.341759: step 30750, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 76h:57m:48s remains)
INFO - root - 2017-12-05 17:40:55.826161: step 30760, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.963 sec/batch; 80h:42m:00s remains)
INFO - root - 2017-12-05 17:41:05.118586: step 30770, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 77h:52m:44s remains)
INFO - root - 2017-12-05 17:41:14.566426: step 30780, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 82h:25m:14s remains)
INFO - root - 2017-12-05 17:41:23.939252: step 30790, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.959 sec/batch; 80h:21m:22s remains)
INFO - root - 2017-12-05 17:41:33.514992: step 30800, loss = 2.04, batch loss = 1.99 (7.9 examples/sec; 1.011 sec/batch; 84h:45m:20s remains)
2017-12-05 17:41:34.279305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3000927 -4.2888384 -4.2818322 -4.2779646 -4.2748079 -4.2705488 -4.2610741 -4.2500482 -4.2424383 -4.241416 -4.2405071 -4.237422 -4.237359 -4.2438116 -4.2543011][-4.2828822 -4.2685165 -4.260879 -4.2551651 -4.2487793 -4.2418022 -4.2250915 -4.2063823 -4.1968937 -4.1986756 -4.2025557 -4.2019272 -4.2013245 -4.2101645 -4.2255158][-4.268178 -4.2527981 -4.2453647 -4.23499 -4.21868 -4.2038879 -4.1775737 -4.1503916 -4.1436129 -4.155045 -4.1698408 -4.1727929 -4.1720266 -4.1807804 -4.1969376][-4.2628689 -4.2513003 -4.2455454 -4.2271786 -4.1978655 -4.1703119 -4.133317 -4.1032968 -4.1037369 -4.1273003 -4.149107 -4.1538415 -4.1485925 -4.1510544 -4.1612043][-4.2598715 -4.2501597 -4.2430978 -4.2189612 -4.1810575 -4.1453681 -4.0957708 -4.0568142 -4.0682874 -4.1073685 -4.1361094 -4.1412764 -4.1315737 -4.1245813 -4.1260953][-4.2608213 -4.2519298 -4.2448907 -4.2170811 -4.1727509 -4.1253152 -4.0515938 -3.9882572 -4.0151968 -4.0784354 -4.1216888 -4.130281 -4.118742 -4.1035957 -4.1003919][-4.2613239 -4.2522354 -4.2417045 -4.2094693 -4.1533194 -4.08173 -3.9695864 -3.866992 -3.9114988 -4.0176826 -4.0854754 -4.1028118 -4.093852 -4.0759525 -4.072588][-4.2547874 -4.2443867 -4.2304516 -4.1952362 -4.1352477 -4.0521278 -3.9193773 -3.7901559 -3.8457861 -3.9824896 -4.0641208 -4.0907784 -4.0852084 -4.066668 -4.0582323][-4.238842 -4.2309947 -4.2202086 -4.1889257 -4.1419692 -4.0796437 -3.9827464 -3.8880854 -3.9281363 -4.0348959 -4.1006083 -4.1265268 -4.1194472 -4.0987949 -4.081986][-4.2293591 -4.2239189 -4.2149115 -4.1908259 -4.1592331 -4.1244946 -4.0691447 -4.016552 -4.0428939 -4.1069922 -4.1520376 -4.1698704 -4.1618214 -4.1407385 -4.1260242][-4.2392731 -4.2370982 -4.2298222 -4.2111697 -4.18821 -4.1651769 -4.1308112 -4.0989113 -4.1175909 -4.1572776 -4.18569 -4.1994896 -4.1934128 -4.1759114 -4.1685867][-4.2600417 -4.2581863 -4.2522268 -4.2380786 -4.220892 -4.2025704 -4.1772494 -4.1580329 -4.1716151 -4.199091 -4.2160983 -4.2246308 -4.2212152 -4.205637 -4.2004213][-4.2791023 -4.2777362 -4.2712407 -4.257597 -4.2452335 -4.2329979 -4.2143512 -4.2034426 -4.214725 -4.2349195 -4.2478113 -4.2539053 -4.2481174 -4.23263 -4.2269826][-4.2911406 -4.2919869 -4.2904539 -4.2806315 -4.27236 -4.2648058 -4.2557163 -4.2538352 -4.2625117 -4.2770891 -4.286921 -4.2904911 -4.2854924 -4.2725368 -4.2664356][-4.298162 -4.29939 -4.3017225 -4.2973547 -4.2930636 -4.2919316 -4.2905746 -4.2916074 -4.2956071 -4.3038564 -4.3131871 -4.316504 -4.312448 -4.3039246 -4.2983766]]...]
INFO - root - 2017-12-05 17:41:43.537954: step 30810, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 72h:55m:03s remains)
INFO - root - 2017-12-05 17:41:52.870684: step 30820, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 76h:44m:50s remains)
INFO - root - 2017-12-05 17:42:02.158358: step 30830, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 72h:33m:17s remains)
INFO - root - 2017-12-05 17:42:11.408744: step 30840, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 78h:23m:04s remains)
INFO - root - 2017-12-05 17:42:20.852978: step 30850, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 75h:59m:10s remains)
INFO - root - 2017-12-05 17:42:29.973182: step 30860, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 75h:27m:05s remains)
INFO - root - 2017-12-05 17:42:39.173815: step 30870, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 75h:45m:23s remains)
INFO - root - 2017-12-05 17:42:48.556062: step 30880, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 72h:32m:57s remains)
INFO - root - 2017-12-05 17:42:57.940198: step 30890, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 77h:49m:02s remains)
INFO - root - 2017-12-05 17:43:07.272910: step 30900, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 77h:52m:40s remains)
2017-12-05 17:43:08.042957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1969881 -4.1953259 -4.2033973 -4.2089181 -4.2118506 -4.2190232 -4.2316127 -4.2391152 -4.2310739 -4.2115383 -4.200655 -4.2025328 -4.2003918 -4.1817789 -4.157311][-4.1714172 -4.176146 -4.1916285 -4.2051921 -4.2148438 -4.2297845 -4.2453365 -4.25312 -4.2501583 -4.2362919 -4.2248049 -4.2233529 -4.2172785 -4.19613 -4.1714416][-4.1567879 -4.1766644 -4.2041941 -4.2260365 -4.2386761 -4.2503686 -4.2605562 -4.2644253 -4.2604795 -4.24982 -4.2427616 -4.2433739 -4.233305 -4.2062049 -4.1796527][-4.1679063 -4.1928077 -4.2220888 -4.244009 -4.2518454 -4.255837 -4.261096 -4.2645111 -4.2638388 -4.2602396 -4.2610064 -4.2662878 -4.254241 -4.2201567 -4.189075][-4.2027731 -4.2181473 -4.2366114 -4.2471161 -4.2407494 -4.233726 -4.2345662 -4.2371984 -4.2398186 -4.2469187 -4.2637386 -4.2798839 -4.2686014 -4.2298675 -4.1958213][-4.2347684 -4.24124 -4.2470307 -4.2426677 -4.21812 -4.1929545 -4.1802578 -4.1773491 -4.1831141 -4.2024169 -4.2365718 -4.2643781 -4.256712 -4.2183061 -4.1850724][-4.2533221 -4.2557659 -4.2548952 -4.2394404 -4.1986165 -4.1498694 -4.1149726 -4.1041021 -4.1158266 -4.1444716 -4.1843686 -4.2184563 -4.2168684 -4.184577 -4.1566219][-4.2499666 -4.2527828 -4.2536955 -4.2380848 -4.1937852 -4.1305366 -4.0763469 -4.054842 -4.0659904 -4.0939384 -4.1300831 -4.160521 -4.1642323 -4.1419311 -4.1238675][-4.2351689 -4.2421613 -4.2492681 -4.2420015 -4.2042589 -4.1381936 -4.0740252 -4.0438194 -4.0529647 -4.0782628 -4.1100359 -4.1321678 -4.1361332 -4.1244521 -4.1158876][-4.2200894 -4.2291126 -4.239151 -4.2374558 -4.2114925 -4.1600566 -4.1039271 -4.0762219 -4.0854821 -4.1099906 -4.1365523 -4.152019 -4.1529212 -4.14183 -4.1313362][-4.2145224 -4.22345 -4.2311888 -4.2285843 -4.2138281 -4.186306 -4.1556339 -4.1420393 -4.1516056 -4.1676106 -4.1858587 -4.1961188 -4.1939716 -4.1746216 -4.1529045][-4.2300873 -4.2369041 -4.2384748 -4.2294583 -4.2161527 -4.2040453 -4.19729 -4.2004714 -4.2086196 -4.2172074 -4.2315059 -4.2412028 -4.2394915 -4.2120223 -4.17655][-4.267065 -4.2678161 -4.2617893 -4.2405877 -4.2178884 -4.2097287 -4.214715 -4.2279458 -4.2392 -4.2450867 -4.2538261 -4.2614441 -4.2574968 -4.2284431 -4.1883726][-4.3125768 -4.3059893 -4.2894773 -4.2620249 -4.2322688 -4.2199006 -4.2249565 -4.2382622 -4.2501807 -4.2552576 -4.2555776 -4.2568679 -4.2507315 -4.2229657 -4.1859417][-4.3337297 -4.3211761 -4.2978606 -4.2700229 -4.2403712 -4.2237868 -4.2222342 -4.227684 -4.2358794 -4.2426653 -4.2443428 -4.2450352 -4.2394986 -4.2134838 -4.1809583]]...]
INFO - root - 2017-12-05 17:43:17.382171: step 30910, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 78h:17m:37s remains)
INFO - root - 2017-12-05 17:43:26.783015: step 30920, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.828 sec/batch; 69h:22m:49s remains)
INFO - root - 2017-12-05 17:43:36.220533: step 30930, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 78h:21m:31s remains)
INFO - root - 2017-12-05 17:43:45.357534: step 30940, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 72h:12m:51s remains)
INFO - root - 2017-12-05 17:43:54.566566: step 30950, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.017 sec/batch; 85h:12m:31s remains)
INFO - root - 2017-12-05 17:44:03.850470: step 30960, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.974 sec/batch; 81h:34m:35s remains)
INFO - root - 2017-12-05 17:44:13.336434: step 30970, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.962 sec/batch; 80h:35m:11s remains)
INFO - root - 2017-12-05 17:44:22.661416: step 30980, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 78h:05m:07s remains)
INFO - root - 2017-12-05 17:44:31.883058: step 30990, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 71h:02m:26s remains)
INFO - root - 2017-12-05 17:44:41.295341: step 31000, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 79h:09m:25s remains)
2017-12-05 17:44:42.071312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3359194 -4.3347111 -4.3335338 -4.3333821 -4.3321385 -4.3320131 -4.3308983 -4.3305526 -4.3294287 -4.3253326 -4.3215122 -4.3200593 -4.3196487 -4.3201175 -4.3203173][-4.32593 -4.3232536 -4.3230519 -4.3204722 -4.3165565 -4.3149576 -4.3115125 -4.3100648 -4.3078218 -4.3017983 -4.2967339 -4.2960997 -4.2981772 -4.3032551 -4.3074603][-4.2929888 -4.295825 -4.2973409 -4.2876968 -4.2771072 -4.267312 -4.2544 -4.2499037 -4.2518959 -4.2503486 -4.2477527 -4.2509751 -4.2608562 -4.2753673 -4.2867856][-4.2430205 -4.2586288 -4.2641916 -4.2446728 -4.21906 -4.187676 -4.152967 -4.1423569 -4.1553173 -4.1675262 -4.1741781 -4.1886506 -4.2130389 -4.2424383 -4.2643614][-4.1778436 -4.2137403 -4.2271738 -4.2008 -4.1520338 -4.0850716 -4.014482 -3.9962606 -4.0331025 -4.0701127 -4.0954494 -4.1302691 -4.17438 -4.218081 -4.2477112][-4.1007061 -4.16179 -4.1883268 -4.1606092 -4.086596 -3.974472 -3.8578577 -3.8370852 -3.9150271 -3.9936929 -4.0502872 -4.1097784 -4.1690669 -4.2173786 -4.2452593][-4.052875 -4.1361547 -4.1739569 -4.1471224 -4.0568876 -3.9128971 -3.7694292 -3.7584429 -3.8733466 -3.9874952 -4.0700035 -4.1437683 -4.203764 -4.2436 -4.2611861][-4.0722585 -4.1536427 -4.19158 -4.1705236 -4.0866404 -3.9577188 -3.8441477 -3.8483989 -3.9516616 -4.0543923 -4.1329494 -4.2018394 -4.2521257 -4.2774544 -4.282486][-4.1379976 -4.2019992 -4.2329674 -4.2181864 -4.1523581 -4.059792 -3.9926336 -4.0074668 -4.0756564 -4.1436243 -4.2004242 -4.2523727 -4.2865434 -4.2980089 -4.2949924][-4.2050581 -4.25201 -4.2767477 -4.2689848 -4.2238617 -4.1643939 -4.129662 -4.1440077 -4.1800275 -4.21743 -4.2525868 -4.2853508 -4.304728 -4.3071938 -4.3007579][-4.2540307 -4.2866769 -4.3030076 -4.2972193 -4.2681646 -4.234755 -4.2206717 -4.2312651 -4.2467017 -4.2645812 -4.2841959 -4.302927 -4.3118963 -4.30904 -4.3017974][-4.2858591 -4.3060565 -4.3133755 -4.3077388 -4.2905416 -4.274682 -4.2722936 -4.2792072 -4.284668 -4.2909751 -4.3007793 -4.3099375 -4.3126335 -4.3077726 -4.3008003][-4.3030796 -4.3131738 -4.3141303 -4.3096042 -4.30124 -4.2954245 -4.2964611 -4.2992835 -4.2998395 -4.3005571 -4.3038926 -4.3077283 -4.3073034 -4.3025503 -4.2967405][-4.3090596 -4.3132954 -4.3105025 -4.3064528 -4.3025403 -4.3005586 -4.3019433 -4.303041 -4.3023558 -4.30112 -4.3011608 -4.3019657 -4.3003221 -4.2961326 -4.291574][-4.3070483 -4.309473 -4.3055091 -4.301353 -4.2985411 -4.2972755 -4.297895 -4.2986169 -4.2986512 -4.2981143 -4.2979574 -4.2979336 -4.2954807 -4.29177 -4.2893753]]...]
INFO - root - 2017-12-05 17:44:51.337091: step 31010, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 77h:26m:16s remains)
INFO - root - 2017-12-05 17:45:00.592876: step 31020, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 76h:15m:52s remains)
INFO - root - 2017-12-05 17:45:10.055600: step 31030, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 80h:03m:49s remains)
INFO - root - 2017-12-05 17:45:19.436133: step 31040, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 78h:56m:58s remains)
INFO - root - 2017-12-05 17:45:28.611305: step 31050, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 75h:23m:00s remains)
INFO - root - 2017-12-05 17:45:37.979460: step 31060, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 80h:36m:04s remains)
INFO - root - 2017-12-05 17:45:47.237562: step 31070, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 75h:54m:10s remains)
INFO - root - 2017-12-05 17:45:56.602940: step 31080, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 81h:12m:41s remains)
INFO - root - 2017-12-05 17:46:05.668076: step 31090, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 79h:58m:42s remains)
INFO - root - 2017-12-05 17:46:15.373919: step 31100, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 82h:35m:13s remains)
2017-12-05 17:46:16.147157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21147 -4.1759224 -4.1650686 -4.1567707 -4.1332273 -4.12823 -4.1549792 -4.1946621 -4.2277637 -4.2535968 -4.2704229 -4.2783861 -4.26793 -4.2357278 -4.1929388][-4.2083278 -4.1690784 -4.1611691 -4.1616206 -4.15045 -4.1559319 -4.1843414 -4.222826 -4.2483048 -4.2614655 -4.2691393 -4.2803111 -4.2801275 -4.2565179 -4.2182732][-4.214056 -4.1763334 -4.1772466 -4.1880231 -4.1872687 -4.1974988 -4.2218046 -4.2484117 -4.2637243 -4.2698603 -4.2752481 -4.2883577 -4.2950811 -4.278646 -4.2436657][-4.214448 -4.1832476 -4.1924834 -4.2073512 -4.2061439 -4.2082534 -4.2186179 -4.227869 -4.2340569 -4.2426567 -4.2585988 -4.282824 -4.2979174 -4.2857437 -4.2530026][-4.2148561 -4.1824617 -4.1922569 -4.2048144 -4.1958003 -4.1741524 -4.1586227 -4.1463656 -4.1405263 -4.155879 -4.1955194 -4.2489948 -4.2868443 -4.2859297 -4.2604556][-4.2150664 -4.1768527 -4.1771922 -4.1791325 -4.1578388 -4.11091 -4.0655384 -4.0286384 -4.0068083 -4.038331 -4.1188288 -4.2108126 -4.2714758 -4.2853513 -4.269125][-4.209404 -4.1643424 -4.1570334 -4.1505613 -4.1143274 -4.0460033 -3.9717047 -3.909039 -3.8779955 -3.9335103 -4.0586324 -4.1847534 -4.2595205 -4.281074 -4.2673368][-4.1990056 -4.1518812 -4.1447859 -4.1400647 -4.0994549 -4.0247474 -3.9394813 -3.8658531 -3.8365445 -3.9082227 -4.0553222 -4.1931572 -4.267014 -4.2841291 -4.2657752][-4.1898513 -4.1452603 -4.1423492 -4.1477647 -4.118423 -4.0562043 -3.9863315 -3.9297683 -3.9136987 -3.9839549 -4.1150885 -4.2318287 -4.2849159 -4.2926388 -4.2732911][-4.1888132 -4.152473 -4.1549096 -4.1701627 -4.1601233 -4.1213031 -4.0787773 -4.0485468 -4.0465636 -4.1075382 -4.2083173 -4.2849703 -4.3022971 -4.2892489 -4.2644224][-4.1975985 -4.1666541 -4.1671419 -4.18868 -4.2004423 -4.186295 -4.1665463 -4.1568518 -4.1627803 -4.21273 -4.2798152 -4.3202982 -4.3105078 -4.283227 -4.2526679][-4.2110791 -4.1773911 -4.170125 -4.1913619 -4.2145247 -4.2210922 -4.2234468 -4.2302613 -4.2442384 -4.282464 -4.3215313 -4.3337445 -4.3088684 -4.2753029 -4.2462087][-4.2355886 -4.1992974 -4.1836371 -4.1983085 -4.2272339 -4.2474575 -4.2634482 -4.2799263 -4.296514 -4.3222804 -4.3360229 -4.3267632 -4.2968721 -4.2632928 -4.2385163][-4.2463832 -4.2116647 -4.1998172 -4.2151537 -4.2461357 -4.2721505 -4.2899733 -4.3059754 -4.3200421 -4.3318486 -4.328414 -4.3114877 -4.2850018 -4.2603536 -4.2377753][-4.2526836 -4.2213697 -4.2188005 -4.2409983 -4.2741723 -4.2988577 -4.3118691 -4.3208642 -4.3288293 -4.3308482 -4.321187 -4.3047051 -4.2861109 -4.2691703 -4.2452636]]...]
INFO - root - 2017-12-05 17:46:25.567177: step 31110, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 79h:06m:48s remains)
INFO - root - 2017-12-05 17:46:34.950880: step 31120, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 78h:47m:00s remains)
INFO - root - 2017-12-05 17:46:44.322224: step 31130, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 82h:01m:42s remains)
INFO - root - 2017-12-05 17:46:53.535785: step 31140, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 71h:42m:03s remains)
INFO - root - 2017-12-05 17:47:02.814409: step 31150, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 81h:54m:24s remains)
INFO - root - 2017-12-05 17:47:12.113574: step 31160, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 76h:20m:22s remains)
INFO - root - 2017-12-05 17:47:21.342187: step 31170, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 81h:38m:42s remains)
INFO - root - 2017-12-05 17:47:30.638822: step 31180, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 79h:36m:52s remains)
INFO - root - 2017-12-05 17:47:39.852308: step 31190, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 76h:03m:22s remains)
INFO - root - 2017-12-05 17:47:49.196491: step 31200, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 78h:07m:05s remains)
2017-12-05 17:47:49.929977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2980733 -4.2955589 -4.2936358 -4.2894545 -4.2805634 -4.2700057 -4.2626381 -4.2590384 -4.2612457 -4.2676921 -4.2693243 -4.2563515 -4.23296 -4.2173486 -4.2063022][-4.282146 -4.2765703 -4.2685189 -4.2551827 -4.236939 -4.2164416 -4.2048912 -4.2017 -4.2122631 -4.2342396 -4.2475543 -4.2385044 -4.2163486 -4.2026443 -4.19678][-4.2682753 -4.2588568 -4.2452888 -4.2224369 -4.1952386 -4.16525 -4.1497827 -4.14503 -4.1640472 -4.2038903 -4.2381129 -4.239522 -4.2235541 -4.2092957 -4.2032294][-4.2661729 -4.2593894 -4.2453418 -4.2144232 -4.1766939 -4.1322246 -4.1040797 -4.0876036 -4.1133947 -4.1786976 -4.2431445 -4.2643018 -4.2555923 -4.2360449 -4.2216682][-4.272871 -4.2737327 -4.2658019 -4.2291636 -4.1744175 -4.1002536 -4.0363646 -3.9967809 -4.0364404 -4.1404438 -4.2410717 -4.2886248 -4.2915006 -4.2684784 -4.2463245][-4.2748828 -4.2820315 -4.2817993 -4.2420888 -4.1666579 -4.0479712 -3.9203439 -3.8399665 -3.9078705 -4.0706425 -4.216507 -4.2935581 -4.3133521 -4.2962132 -4.269063][-4.2748528 -4.2880993 -4.2923427 -4.2499757 -4.1510825 -3.9835413 -3.7784 -3.6432197 -3.7481346 -3.9802482 -4.1735644 -4.2798834 -4.3171983 -4.3078732 -4.2776041][-4.2719908 -4.2918787 -4.2988544 -4.2543516 -4.140583 -3.9508448 -3.7035553 -3.5301805 -3.6568353 -3.931272 -4.1483393 -4.2683907 -4.3188982 -4.315165 -4.2776828][-4.2618575 -4.2922487 -4.3067336 -4.2725534 -4.1724324 -4.0078621 -3.7909522 -3.6397026 -3.74226 -3.9826272 -4.1745129 -4.2781153 -4.3217773 -4.3143325 -4.2681508][-4.2449083 -4.2847571 -4.3070345 -4.2895107 -4.2198839 -4.1041489 -3.9483593 -3.8456125 -3.910944 -4.0795345 -4.2194424 -4.2919874 -4.3169017 -4.3011241 -4.2494969][-4.222362 -4.2631073 -4.2890062 -4.2877393 -4.2471733 -4.1741667 -4.0735841 -4.0123911 -4.0528531 -4.1618176 -4.2523894 -4.2938762 -4.3004856 -4.277112 -4.2248688][-4.2204876 -4.256218 -4.2793241 -4.2844634 -4.2625451 -4.2164922 -4.1516013 -4.1151409 -4.1430721 -4.2152648 -4.2760086 -4.3003378 -4.2973485 -4.2700295 -4.2215171][-4.2432804 -4.2703261 -4.2879615 -4.2913966 -4.2784648 -4.2520638 -4.2116494 -4.1876817 -4.2079325 -4.2574492 -4.2989955 -4.3133378 -4.3074775 -4.2812152 -4.2423129][-4.2775578 -4.3000879 -4.3138204 -4.3147116 -4.3071523 -4.2926278 -4.2689672 -4.2531281 -4.2651706 -4.2969661 -4.3229795 -4.3279438 -4.3186336 -4.2977896 -4.2717686][-4.3077974 -4.3237705 -4.3329368 -4.3334551 -4.3301477 -4.3253326 -4.31549 -4.306427 -4.311213 -4.3291626 -4.3423858 -4.3389707 -4.3273821 -4.311563 -4.2969165]]...]
INFO - root - 2017-12-05 17:47:59.434327: step 31210, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 81h:18m:51s remains)
INFO - root - 2017-12-05 17:48:08.713271: step 31220, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 71h:55m:27s remains)
INFO - root - 2017-12-05 17:48:17.855157: step 31230, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 77h:37m:13s remains)
INFO - root - 2017-12-05 17:48:27.281101: step 31240, loss = 2.02, batch loss = 1.97 (8.6 examples/sec; 0.934 sec/batch; 78h:09m:59s remains)
INFO - root - 2017-12-05 17:48:36.790549: step 31250, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 79h:01m:49s remains)
INFO - root - 2017-12-05 17:48:46.168331: step 31260, loss = 2.09, batch loss = 2.04 (9.0 examples/sec; 0.885 sec/batch; 74h:03m:47s remains)
INFO - root - 2017-12-05 17:48:55.465905: step 31270, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 76h:34m:12s remains)
INFO - root - 2017-12-05 17:49:04.968460: step 31280, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 79h:46m:05s remains)
INFO - root - 2017-12-05 17:49:14.331199: step 31290, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 75h:49m:07s remains)
INFO - root - 2017-12-05 17:49:23.862917: step 31300, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 81h:39m:12s remains)
2017-12-05 17:49:24.628222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0550609 -4.0657911 -4.0625982 -4.0589614 -4.0752048 -4.1037016 -4.1226888 -4.1298747 -4.128108 -4.1169038 -4.1074452 -4.1166005 -4.1289563 -4.1246834 -4.1022868][-4.0336976 -4.0519838 -4.0652966 -4.0765405 -4.0988 -4.1233416 -4.1346889 -4.1333241 -4.1237659 -4.111969 -4.1067634 -4.12061 -4.1336684 -4.1313915 -4.1143413][-4.0376458 -4.0613823 -4.0840192 -4.1016212 -4.1251678 -4.1461363 -4.147913 -4.1334424 -4.1150503 -4.1039233 -4.1081872 -4.1323662 -4.1519713 -4.1540589 -4.1440377][-4.0644984 -4.0884223 -4.1127334 -4.1294718 -4.1462536 -4.1589055 -4.150321 -4.1220107 -4.0974417 -4.0910206 -4.1086664 -4.1472692 -4.1792707 -4.1904769 -4.1876025][-4.1126533 -4.1321254 -4.1516647 -4.1628723 -4.1674957 -4.1625338 -4.135798 -4.0930548 -4.0650139 -4.0674591 -4.0998464 -4.1524739 -4.1982121 -4.2188044 -4.2205176][-4.1610565 -4.1784592 -4.1919274 -4.1950688 -4.1853628 -4.1553478 -4.1009145 -4.0409966 -4.0108428 -4.0249138 -4.0748549 -4.1434989 -4.2036638 -4.2329545 -4.2343388][-4.2010264 -4.2147603 -4.2244334 -4.2195697 -4.1963811 -4.146163 -4.0740223 -4.0065103 -3.9779732 -4.0024376 -4.0655184 -4.1425238 -4.2086649 -4.240696 -4.2370672][-4.2267 -4.2333 -4.2368608 -4.2268724 -4.1961303 -4.1362219 -4.0628014 -4.0046124 -3.9856069 -4.0139856 -4.071692 -4.1403923 -4.20295 -4.2351289 -4.2301607][-4.2407608 -4.2391596 -4.2382431 -4.229784 -4.2024326 -4.1486745 -4.0870161 -4.0490069 -4.04313 -4.0666485 -4.1071534 -4.1573744 -4.2060885 -4.2307925 -4.2223473][-4.2435422 -4.2311735 -4.2249994 -4.2228389 -4.2082739 -4.1709824 -4.1303639 -4.1113596 -4.1141372 -4.1314192 -4.1543064 -4.1826081 -4.2100263 -4.2215939 -4.2050853][-4.23227 -4.2109289 -4.2007327 -4.2035275 -4.1987681 -4.1791639 -4.1592889 -4.1530175 -4.1575079 -4.1699114 -4.1823792 -4.1964645 -4.2064757 -4.2041645 -4.1818361][-4.208055 -4.183651 -4.1730986 -4.1780467 -4.1801653 -4.1731567 -4.1669908 -4.166719 -4.1685076 -4.1748695 -4.1801629 -4.1884789 -4.1897497 -4.1813 -4.1584349][-4.183104 -4.15932 -4.1488729 -4.1520805 -4.1538157 -4.1509352 -4.1523418 -4.1534128 -4.1509724 -4.1507726 -4.1521215 -4.1573529 -4.1545877 -4.1426206 -4.1205411][-4.1733756 -4.1541481 -4.146564 -4.1499219 -4.1539688 -4.1562562 -4.1605277 -4.1584949 -4.1516943 -4.1492229 -4.1513777 -4.1558709 -4.14925 -4.1341009 -4.1149173][-4.1979055 -4.185626 -4.1818914 -4.1865544 -4.1919956 -4.1956539 -4.1985726 -4.1932645 -4.1851082 -4.1846 -4.1885242 -4.1923218 -4.18443 -4.169951 -4.155767]]...]
INFO - root - 2017-12-05 17:49:34.020478: step 31310, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.984 sec/batch; 82h:20m:03s remains)
INFO - root - 2017-12-05 17:49:43.257867: step 31320, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 79h:55m:38s remains)
INFO - root - 2017-12-05 17:49:52.484708: step 31330, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 76h:06m:07s remains)
INFO - root - 2017-12-05 17:50:01.858166: step 31340, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 79h:28m:44s remains)
INFO - root - 2017-12-05 17:50:11.209332: step 31350, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 78h:23m:00s remains)
INFO - root - 2017-12-05 17:50:20.564867: step 31360, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 77h:49m:54s remains)
INFO - root - 2017-12-05 17:50:29.858715: step 31370, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 76h:46m:50s remains)
INFO - root - 2017-12-05 17:50:39.175655: step 31380, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 79h:57m:07s remains)
INFO - root - 2017-12-05 17:50:48.476760: step 31390, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 72h:03m:22s remains)
INFO - root - 2017-12-05 17:50:57.824720: step 31400, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 77h:10m:07s remains)
2017-12-05 17:50:58.594376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3224945 -4.3174725 -4.3216739 -4.3234119 -4.3133931 -4.2980914 -4.285265 -4.2753353 -4.2721243 -4.274344 -4.2820411 -4.297636 -4.3118296 -4.3071866 -4.2864561][-4.3270888 -4.323401 -4.3254662 -4.3245287 -4.310904 -4.2928977 -4.2768865 -4.2634463 -4.259357 -4.2615523 -4.2730546 -4.294517 -4.3150964 -4.3164616 -4.2972145][-4.3353639 -4.3334384 -4.3326883 -4.3269739 -4.3082595 -4.2859988 -4.2646971 -4.2462 -4.238306 -4.2402744 -4.2550883 -4.2824736 -4.3074036 -4.3124323 -4.29557][-4.3409133 -4.3399477 -4.3382378 -4.3281078 -4.3049855 -4.280304 -4.2545018 -4.2289996 -4.2181354 -4.2237492 -4.2420182 -4.2725453 -4.2961693 -4.2988133 -4.2810664][-4.3394632 -4.3362417 -4.32938 -4.3126845 -4.2856736 -4.2582111 -4.2259269 -4.1924438 -4.1809764 -4.1919036 -4.2141643 -4.24357 -4.2634263 -4.2665195 -4.2530265][-4.3328652 -4.3238721 -4.3095326 -4.2853885 -4.2523527 -4.2186146 -4.1750565 -4.1303248 -4.1222229 -4.1447668 -4.174438 -4.2035952 -4.221355 -4.2270265 -4.224525][-4.3234153 -4.3084159 -4.2864289 -4.2530217 -4.2074862 -4.1611342 -4.1054249 -4.0543394 -4.0583787 -4.103641 -4.1481576 -4.1821914 -4.1992483 -4.2046919 -4.2031198][-4.3112817 -4.2927432 -4.2692566 -4.2298684 -4.171175 -4.1132355 -4.0527663 -3.998919 -4.0115151 -4.0774794 -4.135025 -4.1711717 -4.1835656 -4.1817627 -4.1739888][-4.3017173 -4.2826185 -4.2615113 -4.2271719 -4.174582 -4.1180258 -4.0619683 -4.0140753 -4.0250387 -4.0869575 -4.1396275 -4.1660361 -4.1670046 -4.1532063 -4.1338658][-4.2931623 -4.2734613 -4.2552657 -4.2325726 -4.1987882 -4.155993 -4.110497 -4.0762205 -4.0804977 -4.1231346 -4.1551113 -4.1596746 -4.1439071 -4.1159425 -4.086699][-4.2830639 -4.2607679 -4.2434988 -4.2298489 -4.2108479 -4.1820364 -4.145638 -4.1178508 -4.1151834 -4.1396708 -4.1545949 -4.1433144 -4.115335 -4.0775185 -4.0463519][-4.2778716 -4.2537565 -4.2374349 -4.2297597 -4.220058 -4.2022028 -4.173666 -4.1467204 -4.1349015 -4.1429853 -4.1460152 -4.1283422 -4.0967236 -4.0591264 -4.0307741][-4.2810435 -4.2589788 -4.2480106 -4.2474046 -4.2438617 -4.2319908 -4.2082062 -4.1816607 -4.1664572 -4.1638317 -4.1580215 -4.1358986 -4.1089044 -4.0795889 -4.0542264][-4.2912345 -4.2725062 -4.2667155 -4.2694259 -4.267066 -4.2566795 -4.2359748 -4.213068 -4.1983809 -4.1927028 -4.1861944 -4.1685863 -4.1528907 -4.1371927 -4.1191339][-4.3027539 -4.2863903 -4.2811627 -4.2833004 -4.27827 -4.2664127 -4.2479839 -4.2301846 -4.2196784 -4.2159719 -4.2136602 -4.2055035 -4.20261 -4.2005129 -4.1943951]]...]
INFO - root - 2017-12-05 17:51:07.949753: step 31410, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 76h:57m:58s remains)
INFO - root - 2017-12-05 17:51:17.317657: step 31420, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 76h:55m:16s remains)
INFO - root - 2017-12-05 17:51:26.599358: step 31430, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 78h:00m:48s remains)
INFO - root - 2017-12-05 17:51:35.967112: step 31440, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 79h:58m:13s remains)
INFO - root - 2017-12-05 17:51:45.466853: step 31450, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 76h:48m:05s remains)
INFO - root - 2017-12-05 17:51:54.684695: step 31460, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.915 sec/batch; 76h:31m:12s remains)
INFO - root - 2017-12-05 17:52:03.932904: step 31470, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.948 sec/batch; 79h:18m:13s remains)
INFO - root - 2017-12-05 17:52:13.373257: step 31480, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 73h:39m:51s remains)
INFO - root - 2017-12-05 17:52:22.941126: step 31490, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 78h:23m:12s remains)
INFO - root - 2017-12-05 17:52:32.311480: step 31500, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 78h:33m:45s remains)
2017-12-05 17:52:33.089666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2312717 -4.2513452 -4.2605352 -4.2555614 -4.2354426 -4.208179 -4.1817126 -4.1738267 -4.1835141 -4.2095642 -4.2508512 -4.2787127 -4.2937846 -4.2989345 -4.2967587][-4.2758193 -4.2830315 -4.2806339 -4.2609057 -4.2239304 -4.1795654 -4.1366405 -4.1241331 -4.1442914 -4.1897793 -4.2525373 -4.2966542 -4.3189526 -4.3274069 -4.3252206][-4.3142295 -4.3124194 -4.3010635 -4.267561 -4.2132082 -4.1513357 -4.0847993 -4.0555892 -4.07929 -4.1439815 -4.2294111 -4.2907562 -4.3241196 -4.3391018 -4.3392777][-4.3427205 -4.3327603 -4.3154345 -4.272707 -4.2072182 -4.1341133 -4.0471506 -3.9958391 -4.0142713 -4.089622 -4.1897168 -4.2634149 -4.3074746 -4.3296795 -4.3320279][-4.3497953 -4.3346257 -4.3158197 -4.2699389 -4.1957607 -4.1112666 -4.0060377 -3.9357958 -3.9490964 -4.032742 -4.1443672 -4.230279 -4.2849927 -4.3123503 -4.3161349][-4.3433475 -4.326211 -4.3078809 -4.2590075 -4.178164 -4.0804577 -3.9608355 -3.8748395 -3.88508 -3.9804561 -4.1050057 -4.2027326 -4.26809 -4.301126 -4.3069634][-4.3336339 -4.3143978 -4.2963147 -4.2464809 -4.1608438 -4.0542827 -3.9256904 -3.8317986 -3.8401062 -3.9456344 -4.0842557 -4.1941113 -4.265358 -4.3007984 -4.308166][-4.3242769 -4.3025322 -4.2802272 -4.2302904 -4.1492224 -4.045198 -3.9224722 -3.8370166 -3.8448136 -3.9456491 -4.0829062 -4.1954584 -4.270051 -4.3051505 -4.314322][-4.3126311 -4.2907147 -4.2651687 -4.2186065 -4.1478724 -4.0533595 -3.9468803 -3.8812065 -3.8928144 -3.9811192 -4.1017361 -4.2069387 -4.2799196 -4.3126979 -4.3215117][-4.3028674 -4.2850065 -4.2575216 -4.2129674 -4.1524053 -4.0719047 -3.98736 -3.9437041 -3.960125 -4.0358596 -4.1361456 -4.2278285 -4.29317 -4.3216739 -4.328043][-4.304491 -4.2916961 -4.2646871 -4.2219248 -4.1699877 -4.1055222 -4.0458274 -4.0227089 -4.0444093 -4.110847 -4.1929984 -4.2674532 -4.31757 -4.3364935 -4.3376613][-4.31681 -4.3086925 -4.2867985 -4.2529798 -4.2139983 -4.1675944 -4.1291418 -4.1195669 -4.139627 -4.1907325 -4.2547488 -4.3104706 -4.3425322 -4.3502512 -4.3473721][-4.3311739 -4.327817 -4.3132415 -4.2918749 -4.2694035 -4.2408681 -4.2179642 -4.2167058 -4.2328234 -4.2668881 -4.3088536 -4.3436737 -4.3601909 -4.3599911 -4.354321][-4.3423223 -4.3422956 -4.3348789 -4.3234878 -4.313549 -4.2995505 -4.2876964 -4.2887592 -4.2998676 -4.3204551 -4.3439364 -4.3612905 -4.3674746 -4.3636885 -4.3579679][-4.3479495 -4.3492022 -4.3470306 -4.3429346 -4.3409195 -4.3372455 -4.3338952 -4.3347263 -4.3394241 -4.3485293 -4.3587165 -4.3653774 -4.3660674 -4.3624921 -4.3587656]]...]
INFO - root - 2017-12-05 17:52:42.193321: step 31510, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 75h:01m:26s remains)
INFO - root - 2017-12-05 17:52:51.564157: step 31520, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 75h:44m:58s remains)
INFO - root - 2017-12-05 17:53:01.136256: step 31530, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 79h:42m:47s remains)
INFO - root - 2017-12-05 17:53:10.620445: step 31540, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 82h:22m:18s remains)
INFO - root - 2017-12-05 17:53:20.050272: step 31550, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 77h:35m:49s remains)
INFO - root - 2017-12-05 17:53:29.289154: step 31560, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 78h:23m:59s remains)
INFO - root - 2017-12-05 17:53:38.542067: step 31570, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.934 sec/batch; 78h:03m:25s remains)
INFO - root - 2017-12-05 17:53:47.818887: step 31580, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 77h:10m:29s remains)
INFO - root - 2017-12-05 17:53:57.202989: step 31590, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 76h:56m:51s remains)
INFO - root - 2017-12-05 17:54:06.503191: step 31600, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 75h:55m:58s remains)
2017-12-05 17:54:07.223814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2604594 -4.2659168 -4.2646375 -4.2700839 -4.2836986 -4.2914286 -4.2966108 -4.3028092 -4.3010507 -4.2859631 -4.2661548 -4.2448325 -4.2313852 -4.2278934 -4.2218423][-4.2438645 -4.2508135 -4.2469821 -4.2480569 -4.2596431 -4.2706165 -4.2781181 -4.2842369 -4.2773423 -4.2487345 -4.21556 -4.1909065 -4.1818466 -4.18605 -4.1836777][-4.2166929 -4.2249126 -4.2181673 -4.2132654 -4.215806 -4.2209935 -4.2226491 -4.2206783 -4.2043438 -4.1636515 -4.12251 -4.1035585 -4.1094685 -4.1302853 -4.138916][-4.1888819 -4.1913223 -4.1783562 -4.1646848 -4.1527438 -4.1443305 -4.1371536 -4.1262016 -4.1040759 -4.0622683 -4.0259881 -4.0188956 -4.0437164 -4.0838752 -4.1074424][-4.1794415 -4.1676178 -4.1414914 -4.1103559 -4.0777583 -4.0484667 -4.0244908 -4.0045815 -3.9919763 -3.9795773 -3.9731171 -3.9899714 -4.0332732 -4.0836678 -4.1114926][-4.1787443 -4.1486621 -4.1087742 -4.0644746 -4.0191965 -3.9705989 -3.9231265 -3.8915257 -3.903451 -3.9425542 -3.9823337 -4.0267811 -4.0810604 -4.1265488 -4.1475859][-4.1737881 -4.1290989 -4.0813818 -4.0384693 -3.9995556 -3.9457185 -3.8803792 -3.8408086 -3.8758183 -3.9563344 -4.0273161 -4.0863805 -4.1418219 -4.1775274 -4.1883726][-4.1651483 -4.1159115 -4.0725369 -4.0490737 -4.0333037 -3.9924996 -3.9329684 -3.9022627 -3.9450915 -4.0267825 -4.0973353 -4.1508422 -4.1961989 -4.2235374 -4.2286491][-4.1624775 -4.1195521 -4.092627 -4.0967216 -4.1036062 -4.0808239 -4.0387969 -4.0253806 -4.0638356 -4.123178 -4.1757207 -4.2132668 -4.2440286 -4.2614279 -4.2606583][-4.1715803 -4.1466136 -4.1430082 -4.166604 -4.1844211 -4.1739216 -4.1490636 -4.144743 -4.1727085 -4.2102304 -4.2444682 -4.2672143 -4.2821565 -4.2857451 -4.2724967][-4.187829 -4.1857538 -4.2013097 -4.2298841 -4.2472668 -4.2429566 -4.2296076 -4.2296524 -4.2494421 -4.27216 -4.2955637 -4.3091884 -4.3096437 -4.2956982 -4.2661128][-4.2027273 -4.2166843 -4.2393637 -4.2636218 -4.2777715 -4.2779026 -4.2722321 -4.2754192 -4.2896996 -4.3052845 -4.3208089 -4.3261991 -4.314199 -4.2842655 -4.2406626][-4.20883 -4.2323084 -4.2566686 -4.2778072 -4.2896705 -4.2926297 -4.2913404 -4.2940807 -4.3023853 -4.3107719 -4.3175545 -4.3138161 -4.2923775 -4.2518172 -4.1987729][-4.2175956 -4.2441883 -4.2683821 -4.2864304 -4.29611 -4.2985635 -4.2968106 -4.2972083 -4.299408 -4.2998915 -4.2967362 -4.2831569 -4.2533627 -4.2075706 -4.1534557][-4.2434363 -4.263422 -4.2798724 -4.2902 -4.2934804 -4.2907405 -4.2860379 -4.2841778 -4.2822227 -4.2766619 -4.2657495 -4.2450414 -4.2112918 -4.1674871 -4.1215878]]...]
INFO - root - 2017-12-05 17:54:16.487991: step 31610, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 76h:36m:05s remains)
INFO - root - 2017-12-05 17:54:25.899115: step 31620, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 78h:52m:43s remains)
INFO - root - 2017-12-05 17:54:35.362256: step 31630, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 80h:10m:44s remains)
INFO - root - 2017-12-05 17:54:44.603512: step 31640, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 75h:40m:55s remains)
INFO - root - 2017-12-05 17:54:53.963232: step 31650, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.928 sec/batch; 77h:33m:36s remains)
INFO - root - 2017-12-05 17:55:03.258661: step 31660, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 70h:49m:05s remains)
INFO - root - 2017-12-05 17:55:12.491349: step 31670, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 71h:14m:03s remains)
INFO - root - 2017-12-05 17:55:21.864098: step 31680, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 78h:34m:55s remains)
INFO - root - 2017-12-05 17:55:31.398734: step 31690, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.930 sec/batch; 77h:44m:37s remains)
INFO - root - 2017-12-05 17:55:40.838190: step 31700, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.926 sec/batch; 77h:24m:38s remains)
2017-12-05 17:55:41.625204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1653137 -4.1602955 -4.1587868 -4.1699495 -4.1782022 -4.17785 -4.1636591 -4.1546435 -4.1581454 -4.1699634 -4.1780438 -4.184618 -4.1807156 -4.1769691 -4.1777587][-4.1388769 -4.1484623 -4.166378 -4.1877236 -4.1921973 -4.1891518 -4.1720514 -4.15741 -4.1575508 -4.1639695 -4.1630611 -4.1658998 -4.1660194 -4.1676345 -4.1750317][-4.1285591 -4.140965 -4.16567 -4.19134 -4.1901636 -4.1834812 -4.1679463 -4.15431 -4.1537642 -4.160356 -4.1573195 -4.1541429 -4.1556835 -4.1652346 -4.1788316][-4.1364164 -4.1406341 -4.1586924 -4.1800928 -4.1720467 -4.1613326 -4.1456814 -4.1343174 -4.1409521 -4.1565886 -4.1620483 -4.1615605 -4.1608391 -4.1730747 -4.1879363][-4.1398125 -4.1365256 -4.1559105 -4.1793051 -4.1735568 -4.1474485 -4.1122141 -4.0966554 -4.1143103 -4.1449494 -4.1703868 -4.1837864 -4.1810284 -4.1865635 -4.1976][-4.12666 -4.1273575 -4.1573858 -4.1871991 -4.1802535 -4.13438 -4.071485 -4.0431023 -4.0688114 -4.1144395 -4.1556716 -4.1824327 -4.1793127 -4.1804118 -4.1927047][-4.102901 -4.114006 -4.1536393 -4.1835971 -4.1669531 -4.0988846 -4.0096416 -3.961787 -3.9944403 -4.0661635 -4.1274786 -4.1635118 -4.1656418 -4.1677814 -4.1813097][-4.0980749 -4.1091728 -4.1479707 -4.1718316 -4.1420383 -4.0464392 -3.9391761 -3.887959 -3.9352975 -4.0300956 -4.1045828 -4.1474319 -4.1578803 -4.1673365 -4.1849127][-4.1232634 -4.12919 -4.1574726 -4.1709709 -4.13811 -4.0425324 -3.9488518 -3.9180522 -3.9676085 -4.0526843 -4.1157775 -4.1515956 -4.1648965 -4.1815062 -4.2026548][-4.1812744 -4.1788769 -4.1978745 -4.2072434 -4.177217 -4.1072397 -4.0476322 -4.0297923 -4.0607657 -4.1150751 -4.1516175 -4.1704659 -4.1829963 -4.2042828 -4.2247729][-4.2278233 -4.2221017 -4.2366929 -4.2427483 -4.2215285 -4.1790161 -4.1466012 -4.1368551 -4.154531 -4.1830955 -4.1927094 -4.1920943 -4.2026691 -4.22473 -4.242568][-4.24966 -4.2443728 -4.2595773 -4.2666221 -4.2531509 -4.2280126 -4.2089958 -4.2045059 -4.2172041 -4.230267 -4.2250309 -4.2138672 -4.2191181 -4.23912 -4.2567163][-4.2843819 -4.2785873 -4.2942624 -4.3031778 -4.2942061 -4.2793446 -4.2681575 -4.2651114 -4.2646289 -4.2658162 -4.2578955 -4.2471895 -4.2451363 -4.2598238 -4.2754936][-4.3212948 -4.3184896 -4.3288131 -4.3318477 -4.3233285 -4.312829 -4.3033314 -4.2990308 -4.2915459 -4.2882514 -4.2842402 -4.2772822 -4.2730727 -4.28098 -4.2908459][-4.3098917 -4.3075118 -4.3117261 -4.3113346 -4.304956 -4.2982426 -4.2933259 -4.292933 -4.287281 -4.286869 -4.289341 -4.2866082 -4.2834363 -4.2871494 -4.2933512]]...]
INFO - root - 2017-12-05 17:55:51.031765: step 31710, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 76h:57m:44s remains)
INFO - root - 2017-12-05 17:56:00.408186: step 31720, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 78h:57m:51s remains)
INFO - root - 2017-12-05 17:56:09.918631: step 31730, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 77h:20m:24s remains)
INFO - root - 2017-12-05 17:56:19.265367: step 31740, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 75h:50m:39s remains)
INFO - root - 2017-12-05 17:56:28.511289: step 31750, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 77h:41m:57s remains)
INFO - root - 2017-12-05 17:56:37.727162: step 31760, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 70h:45m:51s remains)
INFO - root - 2017-12-05 17:56:47.037993: step 31770, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 75h:43m:40s remains)
INFO - root - 2017-12-05 17:56:56.246578: step 31780, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 73h:23m:34s remains)
INFO - root - 2017-12-05 17:57:05.527362: step 31790, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.873 sec/batch; 72h:54m:22s remains)
INFO - root - 2017-12-05 17:57:15.064740: step 31800, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 80h:54m:58s remains)
2017-12-05 17:57:15.833326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3569164 -4.3564429 -4.3550019 -4.3499742 -4.3399878 -4.3299952 -4.3175397 -4.2959785 -4.2529492 -4.1831856 -4.1016026 -4.0513473 -4.0654569 -4.1305428 -4.2100592][-4.3611889 -4.3609347 -4.3574767 -4.347393 -4.3290043 -4.3086061 -4.2904735 -4.269753 -4.2359853 -4.1771793 -4.0996957 -4.0504484 -4.0634031 -4.1277 -4.2067986][-4.3646092 -4.3644314 -4.3566551 -4.339324 -4.3110771 -4.2813554 -4.2576604 -4.2427859 -4.228931 -4.2035027 -4.1634049 -4.1391692 -4.1525955 -4.199203 -4.2560287][-4.3648658 -4.3630643 -4.3490758 -4.3232522 -4.283968 -4.2458782 -4.21812 -4.2053595 -4.2030392 -4.2021017 -4.1990743 -4.2067451 -4.230948 -4.2680054 -4.3068161][-4.3532562 -4.3510895 -4.3378296 -4.3095193 -4.2618685 -4.216713 -4.1831727 -4.16268 -4.1583476 -4.1689391 -4.1900835 -4.2227335 -4.2606139 -4.3001227 -4.33356][-4.3269858 -4.3259268 -4.3160257 -4.2872982 -4.2330441 -4.1763115 -4.122931 -4.0768533 -4.0654187 -4.0939417 -4.1423717 -4.2002449 -4.2556129 -4.3019757 -4.33679][-4.2871671 -4.2926979 -4.2868214 -4.2572532 -4.1952443 -4.1225896 -4.0369186 -3.953105 -3.9311092 -3.9827456 -4.0649185 -4.151978 -4.2263288 -4.2834506 -4.3226156][-4.2485862 -4.26218 -4.2623577 -4.234128 -4.1696548 -4.0884728 -3.9823341 -3.8761978 -3.8485479 -3.9162428 -4.0173116 -4.12271 -4.20738 -4.2679477 -4.3086462][-4.235477 -4.2540584 -4.2593336 -4.2362452 -4.1788187 -4.1073737 -4.0160389 -3.92825 -3.9060249 -3.9656081 -4.0530138 -4.1496224 -4.2265296 -4.2768703 -4.3112421][-4.262754 -4.2783074 -4.2802019 -4.2589984 -4.2129974 -4.1547041 -4.0831618 -4.0202456 -4.0037565 -4.0486345 -4.1174178 -4.1990166 -4.2626972 -4.3009143 -4.3253741][-4.3045483 -4.3133149 -4.3111153 -4.2901034 -4.2515707 -4.2023849 -4.1430969 -4.0927944 -4.0786467 -4.1123824 -4.1715131 -4.2412605 -4.2931075 -4.3243752 -4.3409443][-4.3355742 -4.3386769 -4.3360906 -4.3213139 -4.2945 -4.2561903 -4.2084646 -4.1685157 -4.1574745 -4.1850929 -4.2357473 -4.2886925 -4.3257809 -4.3450284 -4.3519053][-4.3470211 -4.3472743 -4.3457346 -4.3404832 -4.3303504 -4.3112359 -4.2806273 -4.2537651 -4.2468395 -4.26541 -4.2991753 -4.3315997 -4.3513856 -4.3565478 -4.3549852][-4.3450189 -4.3435593 -4.3432293 -4.3435979 -4.3433585 -4.3376794 -4.322885 -4.3096395 -4.3065948 -4.3174992 -4.3358569 -4.3514237 -4.358726 -4.3558021 -4.3505979][-4.3366251 -4.3342981 -4.3361659 -4.3407407 -4.3467045 -4.349566 -4.3454628 -4.3410125 -4.3390713 -4.3420963 -4.3488684 -4.353416 -4.3533692 -4.3476858 -4.3414783]]...]
INFO - root - 2017-12-05 17:57:25.157122: step 31810, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.906 sec/batch; 75h:41m:15s remains)
INFO - root - 2017-12-05 17:57:34.560965: step 31820, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.019 sec/batch; 85h:06m:16s remains)
INFO - root - 2017-12-05 17:57:43.755545: step 31830, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.939 sec/batch; 78h:23m:57s remains)
INFO - root - 2017-12-05 17:57:53.172831: step 31840, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.951 sec/batch; 79h:23m:19s remains)
INFO - root - 2017-12-05 17:58:02.407925: step 31850, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 80h:36m:55s remains)
INFO - root - 2017-12-05 17:58:11.985465: step 31860, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 78h:10m:54s remains)
INFO - root - 2017-12-05 17:58:20.953612: step 31870, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 79h:13m:29s remains)
INFO - root - 2017-12-05 17:58:30.203949: step 31880, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 74h:20m:45s remains)
INFO - root - 2017-12-05 17:58:39.510192: step 31890, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 77h:20m:43s remains)
INFO - root - 2017-12-05 17:58:48.836420: step 31900, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 77h:20m:43s remains)
2017-12-05 17:58:49.593458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1815991 -4.1486368 -4.1427736 -4.1586022 -4.1985745 -4.231545 -4.2408495 -4.2362456 -4.2308354 -4.2162442 -4.199388 -4.1953969 -4.2120771 -4.2385683 -4.2582726][-4.2028041 -4.166131 -4.1532097 -4.1668196 -4.2035589 -4.2268639 -4.2276864 -4.2238984 -4.2274208 -4.2234364 -4.2157474 -4.2154307 -4.2336841 -4.2596359 -4.2769675][-4.2264662 -4.1864529 -4.1652594 -4.1711254 -4.1942425 -4.2016978 -4.1941748 -4.1981988 -4.21621 -4.2253704 -4.2285 -4.2351775 -4.2579932 -4.2833381 -4.2965913][-4.24346 -4.2001309 -4.1706266 -4.1646066 -4.1674471 -4.15518 -4.1379757 -4.1557527 -4.1945724 -4.2222137 -4.2380891 -4.2545567 -4.2813845 -4.3046503 -4.3130159][-4.2480693 -4.1995745 -4.1608896 -4.1421762 -4.1229863 -4.0877419 -4.0574479 -4.0903563 -4.1530781 -4.1995625 -4.23028 -4.259398 -4.2901683 -4.3100357 -4.3149858][-4.2395415 -4.185915 -4.1397581 -4.1086559 -4.0681725 -4.005631 -3.9601018 -4.0155864 -4.1011248 -4.1615839 -4.2050109 -4.2461095 -4.2800913 -4.2965078 -4.29898][-4.2184992 -4.1607523 -4.1108236 -4.072361 -4.0138941 -3.9251885 -3.86688 -3.9454622 -4.0450811 -4.1097708 -4.161921 -4.2160211 -4.256319 -4.2724547 -4.2748814][-4.1939678 -4.1336279 -4.0805573 -4.0379429 -3.9702671 -3.8610902 -3.7903538 -3.8805485 -3.9864824 -4.0514655 -4.1109004 -4.179956 -4.2303886 -4.2496037 -4.2526646][-4.1886773 -4.1326241 -4.0824103 -4.0386481 -3.9758117 -3.8739257 -3.8055041 -3.8805311 -3.9733944 -4.033792 -4.0908165 -4.1642728 -4.2184582 -4.2383027 -4.2408586][-4.1979923 -4.1583567 -4.1220613 -4.0842609 -4.0314341 -3.9552493 -3.9051435 -3.9509299 -4.0150509 -4.060585 -4.1112452 -4.175539 -4.2227578 -4.2404776 -4.2436247][-4.211966 -4.1884074 -4.1659737 -4.1351342 -4.0936952 -4.0442071 -4.0074472 -4.0279984 -4.0685449 -4.1036 -4.1462584 -4.1994276 -4.2361546 -4.248755 -4.2514348][-4.2310648 -4.2188516 -4.2061782 -4.185266 -4.1586208 -4.130424 -4.1051483 -4.1105461 -4.1363454 -4.1639624 -4.198771 -4.2373185 -4.2615786 -4.26836 -4.2681894][-4.2607527 -4.2572846 -4.2524347 -4.2418442 -4.2287207 -4.2136745 -4.1962061 -4.1944456 -4.2089729 -4.22816 -4.2527924 -4.2762618 -4.2892108 -4.2902584 -4.28727][-4.2942157 -4.2949266 -4.294827 -4.2912912 -4.2865915 -4.2781119 -4.2644043 -4.2580152 -4.2627339 -4.2734528 -4.2865295 -4.2969284 -4.3003235 -4.2974935 -4.292316][-4.3141804 -4.3161759 -4.317935 -4.3169265 -4.3150039 -4.3107705 -4.3015132 -4.292707 -4.289701 -4.2928553 -4.2971873 -4.2985463 -4.2951183 -4.2881083 -4.2803388]]...]
INFO - root - 2017-12-05 17:58:59.066078: step 31910, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.947 sec/batch; 79h:04m:22s remains)
INFO - root - 2017-12-05 17:59:08.439421: step 31920, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 77h:11m:43s remains)
INFO - root - 2017-12-05 17:59:17.825856: step 31930, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.982 sec/batch; 81h:59m:39s remains)
INFO - root - 2017-12-05 17:59:27.024692: step 31940, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 69h:12m:17s remains)
INFO - root - 2017-12-05 17:59:36.442511: step 31950, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.973 sec/batch; 81h:15m:04s remains)
INFO - root - 2017-12-05 17:59:45.845072: step 31960, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 79h:14m:02s remains)
INFO - root - 2017-12-05 17:59:55.123196: step 31970, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 73h:12m:47s remains)
INFO - root - 2017-12-05 18:00:04.296765: step 31980, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 81h:05m:21s remains)
INFO - root - 2017-12-05 18:00:13.527821: step 31990, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 79h:05m:03s remains)
INFO - root - 2017-12-05 18:00:23.023143: step 32000, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 75h:46m:01s remains)
2017-12-05 18:00:23.895019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3253474 -4.3184309 -4.3064027 -4.2932415 -4.2779651 -4.2572803 -4.2375021 -4.2229586 -4.2236652 -4.2232332 -4.2187071 -4.21136 -4.216157 -4.2287602 -4.2399459][-4.3227897 -4.3171792 -4.3056264 -4.2873878 -4.2631216 -4.2351871 -4.2120442 -4.1986423 -4.2090359 -4.2078552 -4.1962657 -4.1840734 -4.1934838 -4.210999 -4.2225933][-4.3181906 -4.3124051 -4.3018746 -4.2822685 -4.2491026 -4.2158923 -4.1905947 -4.1829033 -4.203126 -4.1961508 -4.17689 -4.1585007 -4.1682396 -4.1902394 -4.2051134][-4.3123326 -4.3050342 -4.2929192 -4.2709203 -4.2343659 -4.2036304 -4.1823153 -4.1816931 -4.2031636 -4.18939 -4.1633587 -4.1432118 -4.1511717 -4.172904 -4.1893215][-4.3065753 -4.2938094 -4.2731247 -4.2433214 -4.2071824 -4.1793 -4.1655068 -4.1720796 -4.1918283 -4.1700521 -4.1423349 -4.1268463 -4.13946 -4.16164 -4.1800594][-4.3057694 -4.2872009 -4.2547464 -4.2159538 -4.1768374 -4.147553 -4.1386805 -4.1499577 -4.1656046 -4.13954 -4.1132979 -4.1095414 -4.1306224 -4.1573081 -4.182344][-4.3135419 -4.2938805 -4.2537737 -4.2089925 -4.16409 -4.12906 -4.1140542 -4.1253128 -4.1399021 -4.1172638 -4.0952311 -4.1020627 -4.1325216 -4.1652555 -4.1966405][-4.3244953 -4.3094544 -4.2715368 -4.2280917 -4.1848731 -4.1463957 -4.1208725 -4.1197605 -4.1297317 -4.1170354 -4.09777 -4.1049943 -4.1416726 -4.1808529 -4.2114997][-4.3293991 -4.31705 -4.2863293 -4.2507834 -4.2171807 -4.1853433 -4.1540351 -4.1375427 -4.1370034 -4.1294093 -4.1162558 -4.1190147 -4.1529613 -4.194139 -4.2227178][-4.3243513 -4.3109589 -4.2846212 -4.257679 -4.2329459 -4.2037392 -4.1677065 -4.1385059 -4.1302586 -4.1278977 -4.1237874 -4.1289797 -4.1571331 -4.1956758 -4.2239232][-4.3174367 -4.3047891 -4.2797594 -4.2566128 -4.233418 -4.2010169 -4.1557436 -4.1117454 -4.1035371 -4.1133265 -4.1221547 -4.1334167 -4.1585155 -4.1903152 -4.2156525][-4.3136086 -4.2982578 -4.2681117 -4.2402558 -4.2146134 -4.1834841 -4.1366167 -4.0898132 -4.0895548 -4.1111803 -4.1287656 -4.142767 -4.1600242 -4.1799078 -4.1998987][-4.3098607 -4.2909689 -4.2530818 -4.2177181 -4.1914239 -4.1671906 -4.1356158 -4.1100945 -4.118515 -4.138042 -4.1509571 -4.1546264 -4.1583624 -4.1629796 -4.1742358][-4.3101621 -4.2898908 -4.2461257 -4.2032738 -4.178175 -4.1652708 -4.151576 -4.1489067 -4.1659317 -4.1805835 -4.1844831 -4.1761031 -4.166224 -4.1591444 -4.1641679][-4.311697 -4.2931314 -4.2472081 -4.1995554 -4.173223 -4.1644006 -4.1572003 -4.163578 -4.1818566 -4.193552 -4.1981783 -4.1896749 -4.1769662 -4.1661763 -4.1714606]]...]
INFO - root - 2017-12-05 18:00:33.327941: step 32010, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 78h:45m:03s remains)
INFO - root - 2017-12-05 18:00:42.910764: step 32020, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 80h:35m:51s remains)
INFO - root - 2017-12-05 18:00:52.232966: step 32030, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 80h:47m:22s remains)
INFO - root - 2017-12-05 18:01:01.639359: step 32040, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 78h:28m:09s remains)
INFO - root - 2017-12-05 18:01:10.895501: step 32050, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 75h:49m:31s remains)
INFO - root - 2017-12-05 18:01:20.243716: step 32060, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 78h:35m:09s remains)
INFO - root - 2017-12-05 18:01:29.624002: step 32070, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 79h:31m:53s remains)
INFO - root - 2017-12-05 18:01:38.966175: step 32080, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 79h:22m:35s remains)
INFO - root - 2017-12-05 18:01:48.275043: step 32090, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.010 sec/batch; 84h:18m:27s remains)
INFO - root - 2017-12-05 18:01:57.785534: step 32100, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 78h:25m:28s remains)
2017-12-05 18:01:58.477769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2577386 -4.2525325 -4.2373824 -4.2240162 -4.2235451 -4.22943 -4.2340379 -4.2368937 -4.2420192 -4.2566 -4.2774715 -4.2946815 -4.3051667 -4.3119073 -4.3130169][-4.2462287 -4.2317314 -4.20819 -4.19296 -4.1976681 -4.2098551 -4.2181978 -4.2235088 -4.2291188 -4.2424059 -4.2626572 -4.2788234 -4.2888551 -4.2963238 -4.2974396][-4.2442245 -4.2204428 -4.1897893 -4.1718206 -4.1778951 -4.1920228 -4.20533 -4.2192278 -4.2322326 -4.2469764 -4.2633533 -4.2722049 -4.2738104 -4.2749648 -4.2735853][-4.2429123 -4.2162108 -4.18253 -4.1594577 -4.1572733 -4.1672115 -4.1856647 -4.2090096 -4.2301893 -4.2485924 -4.2610569 -4.2638702 -4.2570481 -4.2494369 -4.2458453][-4.2332196 -4.2102904 -4.1762938 -4.1477938 -4.1369948 -4.1437922 -4.166409 -4.1935754 -4.2174764 -4.2375088 -4.2469482 -4.2467728 -4.2365508 -4.2241769 -4.2174258][-4.213542 -4.2024994 -4.1745586 -4.1458945 -4.1290774 -4.1302204 -4.1530547 -4.1786456 -4.2002072 -4.2171874 -4.2231212 -4.2212825 -4.2099662 -4.1960497 -4.1856918][-4.1855516 -4.1906724 -4.1762223 -4.1536956 -4.1345959 -4.1299233 -4.1471968 -4.1669 -4.1820693 -4.1946793 -4.2001405 -4.2003951 -4.1912389 -4.1766424 -4.1624169][-4.1551294 -4.1704659 -4.1698551 -4.1569576 -4.1420593 -4.1365991 -4.1467886 -4.1552291 -4.1622934 -4.1757936 -4.189209 -4.1949935 -4.1883326 -4.1724725 -4.1547546][-4.1354384 -4.1497726 -4.1572948 -4.1542439 -4.1449771 -4.1363306 -4.1356936 -4.1340051 -4.1356187 -4.1532416 -4.1821275 -4.2014637 -4.2049017 -4.193552 -4.1780376][-4.1474776 -4.152792 -4.1584864 -4.1583695 -4.1494565 -4.1322403 -4.1171961 -4.1066737 -4.1053543 -4.1268382 -4.1653786 -4.1960297 -4.2099991 -4.2093596 -4.2038097][-4.1857653 -4.1848769 -4.1836176 -4.1776357 -4.1634254 -4.13864 -4.1125565 -4.09277 -4.0856748 -4.1020947 -4.1366234 -4.168828 -4.1870751 -4.1934996 -4.2003074][-4.2132778 -4.2172656 -4.216331 -4.208457 -4.1903811 -4.1640992 -4.1373954 -4.1142507 -4.0989318 -4.0976524 -4.1097221 -4.1281171 -4.1451569 -4.1579442 -4.1746855][-4.2108088 -4.2254128 -4.2343931 -4.233727 -4.2204742 -4.200469 -4.181179 -4.1622458 -4.1451783 -4.1304855 -4.1167879 -4.1085095 -4.1117988 -4.1246719 -4.1444874][-4.1877337 -4.2094712 -4.2284832 -4.2374716 -4.233849 -4.2263021 -4.2195253 -4.2106819 -4.1977782 -4.1808829 -4.1569824 -4.1300116 -4.1154056 -4.1172643 -4.1267719][-4.157928 -4.181119 -4.2058496 -4.2204065 -4.2235031 -4.22576 -4.2317157 -4.2368641 -4.2355528 -4.226191 -4.2037816 -4.1717196 -4.14578 -4.135241 -4.131093]]...]
INFO - root - 2017-12-05 18:02:07.752452: step 32110, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 77h:00m:00s remains)
INFO - root - 2017-12-05 18:02:16.996458: step 32120, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 72h:36m:23s remains)
INFO - root - 2017-12-05 18:02:26.385969: step 32130, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 78h:47m:40s remains)
INFO - root - 2017-12-05 18:02:35.658253: step 32140, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 78h:32m:09s remains)
INFO - root - 2017-12-05 18:02:45.079353: step 32150, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 74h:35m:29s remains)
INFO - root - 2017-12-05 18:02:54.280131: step 32160, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 79h:24m:10s remains)
INFO - root - 2017-12-05 18:03:03.625400: step 32170, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 82h:50m:59s remains)
INFO - root - 2017-12-05 18:03:13.145171: step 32180, loss = 2.03, batch loss = 1.97 (8.2 examples/sec; 0.975 sec/batch; 81h:20m:05s remains)
INFO - root - 2017-12-05 18:03:22.380597: step 32190, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 78h:17m:05s remains)
INFO - root - 2017-12-05 18:03:31.532731: step 32200, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 74h:56m:07s remains)
2017-12-05 18:03:32.280120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3190765 -4.3201175 -4.3199825 -4.3184152 -4.3164034 -4.3140912 -4.3118367 -4.3105445 -4.3105035 -4.3120675 -4.3144941 -4.3177371 -4.3219147 -4.3261847 -4.3288393][-4.320147 -4.3222675 -4.32253 -4.3207808 -4.318047 -4.3147798 -4.3117085 -4.3103151 -4.3112359 -4.3146992 -4.3194981 -4.3242884 -4.3284712 -4.3316636 -4.3332295][-4.3216619 -4.3231239 -4.3212676 -4.3163614 -4.3095465 -4.3025913 -4.298049 -4.2980018 -4.3031311 -4.3123183 -4.3226233 -4.33089 -4.335187 -4.335803 -4.334374][-4.3221025 -4.3204479 -4.311903 -4.2971573 -4.2784243 -4.2611351 -4.2514462 -4.252687 -4.2652984 -4.2868333 -4.3096433 -4.3272672 -4.3362994 -4.3369493 -4.33349][-4.3162804 -4.3085608 -4.2895737 -4.2576318 -4.2170482 -4.1802845 -4.159162 -4.1607914 -4.1849527 -4.2271872 -4.2716675 -4.306107 -4.3259616 -4.3315225 -4.328639][-4.3033009 -4.2891173 -4.2592039 -4.2071424 -4.1392112 -4.0750117 -4.0347166 -4.0334115 -4.0723166 -4.141499 -4.2132034 -4.2684369 -4.3020949 -4.3158927 -4.3163514][-4.29301 -4.2757878 -4.2402062 -4.1759796 -4.086566 -3.9952669 -3.9317825 -3.9235272 -3.9751475 -4.0671711 -4.1608524 -4.2319345 -4.2753005 -4.29503 -4.2989392][-4.29473 -4.2799926 -4.2484322 -4.1896138 -4.0999775 -4.0001535 -3.9245548 -3.9108391 -3.9637003 -4.0569234 -4.1511993 -4.2215714 -4.2641773 -4.283082 -4.2872682][-4.304656 -4.2966409 -4.2768917 -4.2370305 -4.1691456 -4.0862775 -4.0189142 -4.0009933 -4.0379529 -4.1080575 -4.1804943 -4.2342534 -4.2657919 -4.2780204 -4.2796731][-4.3156724 -4.3145027 -4.3069344 -4.28754 -4.2483549 -4.1937971 -4.1455655 -4.1278448 -4.1449628 -4.1849718 -4.2272911 -4.2555552 -4.2695832 -4.2724814 -4.27027][-4.3237562 -4.3259039 -4.323473 -4.3140993 -4.2947488 -4.2655282 -4.2384171 -4.2273498 -4.2347808 -4.2547526 -4.27363 -4.2800927 -4.2773418 -4.2697377 -4.2617488][-4.3255086 -4.3242483 -4.3158231 -4.3018565 -4.2861037 -4.2702737 -4.2612166 -4.2629 -4.2730541 -4.2873759 -4.2969851 -4.29532 -4.2853727 -4.2718925 -4.2603469][-4.3182106 -4.310102 -4.2875915 -4.2556186 -4.2276268 -4.2127619 -4.2166133 -4.2362676 -4.2612991 -4.28426 -4.2978497 -4.2996168 -4.2931571 -4.2818274 -4.2712989][-4.2950978 -4.2842865 -4.2488914 -4.1952896 -4.1472821 -4.1269813 -4.1419625 -4.1811848 -4.2270141 -4.2658925 -4.2906065 -4.3016696 -4.3026061 -4.2974615 -4.2909226][-4.2651429 -4.2622204 -4.226428 -4.1644049 -4.1055136 -4.0828795 -4.1056247 -4.1573391 -4.2141166 -4.2601724 -4.2905083 -4.3069177 -4.3128853 -4.3125243 -4.3096738]]...]
INFO - root - 2017-12-05 18:03:41.623785: step 32210, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 75h:32m:32s remains)
INFO - root - 2017-12-05 18:03:50.975502: step 32220, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 76h:50m:23s remains)
INFO - root - 2017-12-05 18:04:00.297866: step 32230, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 76h:23m:27s remains)
INFO - root - 2017-12-05 18:04:09.579987: step 32240, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 76h:14m:15s remains)
INFO - root - 2017-12-05 18:04:19.079761: step 32250, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 78h:21m:33s remains)
INFO - root - 2017-12-05 18:04:28.377017: step 32260, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 78h:34m:34s remains)
INFO - root - 2017-12-05 18:04:37.636358: step 32270, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 74h:39m:30s remains)
INFO - root - 2017-12-05 18:04:46.930753: step 32280, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 81h:18m:40s remains)
INFO - root - 2017-12-05 18:04:56.281281: step 32290, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 79h:35m:58s remains)
INFO - root - 2017-12-05 18:05:05.495630: step 32300, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 74h:58m:19s remains)
2017-12-05 18:05:06.224004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.220047 -4.241776 -4.255558 -4.2578573 -4.2533226 -4.2369409 -4.2173467 -4.2021427 -4.1931028 -4.1941171 -4.211926 -4.2366095 -4.26078 -4.2768693 -4.286006][-4.2195749 -4.240654 -4.2480206 -4.2360277 -4.2145891 -4.19079 -4.1736741 -4.1658049 -4.1635351 -4.17038 -4.1925344 -4.2217417 -4.25051 -4.2708325 -4.2832375][-4.2198491 -4.2411633 -4.2418141 -4.2158213 -4.1746063 -4.1439714 -4.13094 -4.1331038 -4.1394134 -4.1520624 -4.1761031 -4.2070594 -4.2385635 -4.2626462 -4.2774105][-4.2133822 -4.2309194 -4.2242203 -4.1855 -4.129962 -4.0957022 -4.0870109 -4.0961361 -4.1101971 -4.1263876 -4.1523733 -4.1871815 -4.2225194 -4.2507219 -4.2703481][-4.2142606 -4.2184877 -4.1990218 -4.1499443 -4.0834579 -4.0485916 -4.0489464 -4.0676117 -4.0896463 -4.1106906 -4.1385536 -4.174881 -4.2122073 -4.2419853 -4.2636604][-4.2303596 -4.2093992 -4.1734791 -4.1169124 -4.0466404 -4.010447 -4.0147014 -4.0402575 -4.0701027 -4.0973587 -4.1255736 -4.1631393 -4.203701 -4.2348261 -4.2563019][-4.2456045 -4.2030563 -4.1537685 -4.0899959 -4.0200477 -3.9798717 -3.977288 -3.996819 -4.0294709 -4.0681391 -4.1053791 -4.15142 -4.2002287 -4.2351971 -4.2545791][-4.2506056 -4.19866 -4.1431236 -4.0791941 -4.0129337 -3.9652843 -3.9457018 -3.9543269 -3.9890981 -4.0412612 -4.090189 -4.146841 -4.2055163 -4.242384 -4.2541208][-4.2562294 -4.2074871 -4.1561317 -4.0994449 -4.0434365 -3.9948339 -3.9634583 -3.9671457 -4.0022097 -4.0563469 -4.1070337 -4.1645474 -4.22086 -4.2508626 -4.2497473][-4.2591138 -4.218483 -4.1775184 -4.1322961 -4.0910306 -4.0543652 -4.0296235 -4.0413013 -4.0797729 -4.1305943 -4.1739221 -4.2183685 -4.254705 -4.2640934 -4.2426667][-4.2555194 -4.2309084 -4.2068315 -4.1782923 -4.1530519 -4.133019 -4.1229963 -4.1431322 -4.1810937 -4.2250528 -4.256454 -4.280859 -4.2910552 -4.274931 -4.2320895][-4.2479005 -4.24075 -4.2348981 -4.2236571 -4.2130718 -4.2061825 -4.2053919 -4.2231116 -4.253109 -4.28507 -4.3017573 -4.3087792 -4.3013272 -4.2724185 -4.219099][-4.2525172 -4.2603564 -4.2664919 -4.2644534 -4.2594919 -4.255352 -4.2547626 -4.2634892 -4.2822 -4.3025656 -4.3099251 -4.3096137 -4.2990489 -4.2720437 -4.2225308][-4.2597194 -4.2723036 -4.2802734 -4.2787681 -4.2725616 -4.2650127 -4.2594938 -4.2604222 -4.2712069 -4.2852931 -4.2923317 -4.2950048 -4.2904563 -4.2723556 -4.2356982][-4.270689 -4.2802129 -4.2844038 -4.2782373 -4.2658081 -4.2510009 -4.2383761 -4.2316437 -4.2346754 -4.2425976 -4.2505932 -4.2580047 -4.2601671 -4.2504635 -4.229444]]...]
INFO - root - 2017-12-05 18:05:15.520518: step 32310, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 78h:24m:32s remains)
INFO - root - 2017-12-05 18:05:24.873953: step 32320, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 78h:12m:03s remains)
INFO - root - 2017-12-05 18:05:34.083190: step 32330, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 81h:35m:25s remains)
INFO - root - 2017-12-05 18:05:43.367586: step 32340, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 74h:21m:40s remains)
INFO - root - 2017-12-05 18:05:52.656499: step 32350, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 78h:01m:23s remains)
INFO - root - 2017-12-05 18:06:01.944766: step 32360, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 75h:10m:46s remains)
INFO - root - 2017-12-05 18:06:11.334595: step 32370, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 79h:19m:33s remains)
INFO - root - 2017-12-05 18:06:20.670537: step 32380, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 77h:09m:09s remains)
INFO - root - 2017-12-05 18:06:29.916121: step 32390, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 72h:50m:38s remains)
INFO - root - 2017-12-05 18:06:39.098087: step 32400, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 80h:16m:13s remains)
2017-12-05 18:06:39.872896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2231083 -4.2225394 -4.2303205 -4.246995 -4.2618957 -4.2727056 -4.2830205 -4.2961912 -4.3088012 -4.3136497 -4.3135462 -4.3139305 -4.3150311 -4.3159132 -4.3164949][-4.169085 -4.1639128 -4.1712933 -4.1950312 -4.2140718 -4.2259231 -4.2395067 -4.2628722 -4.2853823 -4.2934408 -4.2973261 -4.3021507 -4.3054252 -4.3062563 -4.3070035][-4.1464305 -4.1354322 -4.13844 -4.1606607 -4.1746678 -4.179728 -4.191401 -4.214879 -4.2362676 -4.2449784 -4.2585173 -4.2737169 -4.2832479 -4.2857537 -4.2875147][-4.1674452 -4.1540766 -4.1518035 -4.1607141 -4.1543245 -4.1445179 -4.1493182 -4.167491 -4.1844616 -4.1955447 -4.2204051 -4.2450004 -4.258009 -4.2628136 -4.2672448][-4.190856 -4.1817236 -4.1775665 -4.1698079 -4.1390057 -4.1126175 -4.10424 -4.1151075 -4.1313429 -4.1522665 -4.19149 -4.2223711 -4.2346239 -4.2398629 -4.2490044][-4.2011909 -4.2031732 -4.20272 -4.1821389 -4.1298652 -4.0810351 -4.0500588 -4.0457211 -4.0693994 -4.11043 -4.1651168 -4.1990867 -4.2102609 -4.2185664 -4.231123][-4.19937 -4.2100196 -4.2155833 -4.1900768 -4.1243291 -4.0464253 -3.968323 -3.9306698 -3.9734073 -4.0584121 -4.1316018 -4.1694784 -4.1843433 -4.1941986 -4.2066646][-4.1760139 -4.1927066 -4.2028475 -4.1767273 -4.1044812 -3.999047 -3.8740115 -3.8027859 -3.8807354 -4.0079947 -4.0932965 -4.1323304 -4.1520882 -4.1612668 -4.1739583][-4.1494803 -4.168818 -4.1814218 -4.1562209 -4.0815058 -3.9673378 -3.8403103 -3.7878141 -3.8774939 -3.9938111 -4.0650611 -4.1022348 -4.12405 -4.1314163 -4.1467228][-4.1510749 -4.1715455 -4.1825576 -4.1598721 -4.0919867 -4.0018363 -3.9298639 -3.917923 -3.9718513 -4.0385766 -4.0801516 -4.1056929 -4.1238446 -4.1339326 -4.1532135][-4.1860929 -4.2006764 -4.2094822 -4.1937728 -4.1440821 -4.0901384 -4.0671396 -4.072463 -4.0990133 -4.1330109 -4.1481366 -4.1536856 -4.1607518 -4.169281 -4.187233][-4.2237725 -4.2350121 -4.2471962 -4.2437077 -4.2155871 -4.1896534 -4.1875625 -4.1953139 -4.2094259 -4.2278609 -4.2245064 -4.2135391 -4.2043295 -4.2077289 -4.2261591][-4.255631 -4.2613282 -4.2757297 -4.2828164 -4.2728295 -4.2637777 -4.2659826 -4.2729692 -4.2853179 -4.292943 -4.2803669 -4.2587838 -4.2360458 -4.234797 -4.2562175][-4.2865787 -4.2902603 -4.3009291 -4.3099418 -4.3112817 -4.310617 -4.3107543 -4.3171258 -4.3249736 -4.3240776 -4.31295 -4.2912006 -4.2657046 -4.262351 -4.2818933][-4.3126278 -4.3153934 -4.3211164 -4.3258052 -4.3282137 -4.3270721 -4.323401 -4.328393 -4.3319407 -4.3315578 -4.32225 -4.3030453 -4.2834325 -4.2824259 -4.2995348]]...]
INFO - root - 2017-12-05 18:06:49.464373: step 32410, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 78h:31m:04s remains)
INFO - root - 2017-12-05 18:06:58.835171: step 32420, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 75h:31m:10s remains)
INFO - root - 2017-12-05 18:07:08.027316: step 32430, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 78h:44m:05s remains)
INFO - root - 2017-12-05 18:07:17.404098: step 32440, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.947 sec/batch; 78h:55m:10s remains)
INFO - root - 2017-12-05 18:07:26.529235: step 32450, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 79h:11m:43s remains)
INFO - root - 2017-12-05 18:07:35.619778: step 32460, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 77h:12m:10s remains)
INFO - root - 2017-12-05 18:07:44.997193: step 32470, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 0.786 sec/batch; 65h:29m:47s remains)
INFO - root - 2017-12-05 18:07:54.086158: step 32480, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 80h:35m:49s remains)
INFO - root - 2017-12-05 18:08:03.416945: step 32490, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 75h:34m:58s remains)
INFO - root - 2017-12-05 18:08:12.788556: step 32500, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 79h:04m:42s remains)
2017-12-05 18:08:13.601485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2051568 -4.12623 -4.0500083 -3.991399 -3.9714937 -4.001801 -4.028873 -4.0366492 -4.0329018 -4.0162225 -3.9838753 -3.9741123 -4.0024891 -4.0607648 -4.1000514][-4.2218366 -4.1536694 -4.0865388 -4.0369577 -4.0180464 -4.0366774 -4.0578327 -4.0575919 -4.040246 -4.0172224 -3.9881682 -3.9848468 -4.0143905 -4.0685177 -4.1029596][-4.2400055 -4.1848173 -4.1312037 -4.0960207 -4.0778713 -4.0856218 -4.1008348 -4.0991011 -4.0717697 -4.0419979 -4.0201278 -4.023869 -4.0502791 -4.0879464 -4.1094718][-4.2537112 -4.2059879 -4.1601043 -4.13051 -4.1084251 -4.1028862 -4.1101527 -4.1155038 -4.0923996 -4.0657973 -4.0517883 -4.0603004 -4.0821319 -4.0983214 -4.1052766][-4.2623487 -4.2161813 -4.1693344 -4.1320248 -4.10211 -4.0883975 -4.0914087 -4.09704 -4.0812507 -4.0633221 -4.0608859 -4.0694404 -4.0794067 -4.083087 -4.0877285][-4.2687497 -4.2169485 -4.1567111 -4.0982614 -4.052176 -4.033083 -4.0374703 -4.04906 -4.0517488 -4.0541167 -4.0641174 -4.0743055 -4.0738692 -4.0776873 -4.0889339][-4.263237 -4.1969562 -4.109724 -4.0231032 -3.9632761 -3.9468989 -3.9598277 -3.9839664 -4.006422 -4.0255775 -4.0406241 -4.0540104 -4.0578971 -4.069788 -4.0827732][-4.237967 -4.1484342 -4.0224457 -3.8976002 -3.8385339 -3.8429015 -3.871798 -3.9084163 -3.9386208 -3.9607947 -3.9785621 -4.0045781 -4.0257206 -4.0482321 -4.06292][-4.2152743 -4.114871 -3.9779651 -3.8489013 -3.8082113 -3.8323953 -3.8709552 -3.9163227 -3.9505315 -3.9667585 -3.9800534 -4.0049706 -4.02363 -4.0440063 -4.0605679][-4.2163715 -4.131495 -4.0332665 -3.9525034 -3.9330213 -3.952543 -3.984082 -4.0252247 -4.05694 -4.0644331 -4.0652757 -4.0752616 -4.0781221 -4.0810208 -4.0918016][-4.2364955 -4.1719079 -4.106051 -4.0572925 -4.0432906 -4.0515804 -4.0730805 -4.1056643 -4.1268549 -4.1246891 -4.114933 -4.1149282 -4.1123867 -4.1149368 -4.1286149][-4.2621846 -4.2122703 -4.1602397 -4.1220908 -4.1045833 -4.0991516 -4.1062207 -4.1313863 -4.1429334 -4.1411028 -4.13045 -4.121068 -4.1152334 -4.1240864 -4.1393361][-4.2665668 -4.2193241 -4.1675515 -4.1283436 -4.1033716 -4.0885792 -4.09003 -4.112596 -4.1230183 -4.129168 -4.1224527 -4.1085587 -4.1008377 -4.1135173 -4.1264467][-4.2481289 -4.1912394 -4.1324053 -4.0875893 -4.0595937 -4.0411959 -4.042695 -4.0655 -4.0844197 -4.1033716 -4.104887 -4.0958939 -4.0950794 -4.1115923 -4.1261363][-4.2265139 -4.1587315 -4.0932527 -4.0487442 -4.0221863 -4.0086465 -4.0142 -4.0372806 -4.0663757 -4.0869808 -4.0840158 -4.0716987 -4.0711474 -4.0942464 -4.1161251]]...]
INFO - root - 2017-12-05 18:08:22.696090: step 32510, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 71h:40m:08s remains)
INFO - root - 2017-12-05 18:08:32.052372: step 32520, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 76h:16m:17s remains)
INFO - root - 2017-12-05 18:08:41.270954: step 32530, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 77h:37m:32s remains)
INFO - root - 2017-12-05 18:08:50.522684: step 32540, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.931 sec/batch; 77h:35m:20s remains)
INFO - root - 2017-12-05 18:08:59.920285: step 32550, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 77h:45m:36s remains)
INFO - root - 2017-12-05 18:09:09.285616: step 32560, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 78h:54m:23s remains)
INFO - root - 2017-12-05 18:09:18.493310: step 32570, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 79h:22m:12s remains)
INFO - root - 2017-12-05 18:09:27.828292: step 32580, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 79h:57m:20s remains)
INFO - root - 2017-12-05 18:09:37.277305: step 32590, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 82h:03m:11s remains)
INFO - root - 2017-12-05 18:09:46.518742: step 32600, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.941 sec/batch; 78h:24m:07s remains)
2017-12-05 18:09:47.299905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2003937 -4.2236047 -4.260097 -4.2790489 -4.2847877 -4.2691107 -4.2250681 -4.1900105 -4.2002735 -4.2255363 -4.2371531 -4.235137 -4.2269917 -4.2198648 -4.2270789][-4.2066803 -4.2309761 -4.2635274 -4.2688413 -4.2641277 -4.2478046 -4.20464 -4.1715894 -4.188911 -4.2133017 -4.2228341 -4.2252688 -4.2209954 -4.2198515 -4.2335253][-4.1950669 -4.2191582 -4.2481956 -4.250484 -4.2403927 -4.2213068 -4.1845107 -4.1592469 -4.1814189 -4.2031279 -4.2138362 -4.2176008 -4.2179456 -4.2226596 -4.2423253][-4.1725731 -4.1947408 -4.2205563 -4.2283731 -4.22239 -4.2080393 -4.1851807 -4.1680059 -4.1819868 -4.1898994 -4.1987858 -4.203022 -4.2095661 -4.2268963 -4.254745][-4.1568246 -4.1713481 -4.1925511 -4.2060437 -4.2089682 -4.2000294 -4.1841521 -4.1661491 -4.1612606 -4.15408 -4.1733303 -4.1956916 -4.21636 -4.2440052 -4.2688551][-4.1452107 -4.1510034 -4.1714807 -4.1906219 -4.2008967 -4.1936278 -4.1805296 -4.15975 -4.1417875 -4.1295528 -4.1661811 -4.2118058 -4.2440286 -4.270371 -4.2812715][-4.1455374 -4.1378031 -4.151041 -4.1669879 -4.174932 -4.164505 -4.1548128 -4.1431627 -4.1286697 -4.1234813 -4.1683626 -4.2189059 -4.255404 -4.2808623 -4.2829757][-4.1638155 -4.1450214 -4.1440411 -4.1453276 -4.1370592 -4.1148219 -4.1004267 -4.1070628 -4.1066051 -4.1104355 -4.1534863 -4.1979094 -4.2357059 -4.269001 -4.2724848][-4.1818247 -4.1613531 -4.1548448 -4.1407042 -4.1125426 -4.0755191 -4.0577359 -4.0813327 -4.0941973 -4.1050272 -4.1468821 -4.1887097 -4.222919 -4.251358 -4.2530303][-4.1745114 -4.1606417 -4.1562147 -4.135951 -4.0963445 -4.053802 -4.0409489 -4.0792437 -4.1101031 -4.1294785 -4.1691556 -4.2039151 -4.2228332 -4.2331171 -4.2287536][-4.1639438 -4.1619434 -4.1638174 -4.1440916 -4.1059794 -4.070159 -4.0654407 -4.1049404 -4.1400838 -4.1593103 -4.1897354 -4.212131 -4.2140269 -4.208745 -4.2037239][-4.1822815 -4.1891117 -4.1936431 -4.1738267 -4.1426282 -4.1233826 -4.1214581 -4.1441064 -4.1731892 -4.1892509 -4.2065668 -4.2181525 -4.2127986 -4.198781 -4.191915][-4.210937 -4.2196016 -4.2244396 -4.2086539 -4.1876955 -4.1840787 -4.1857634 -4.1947627 -4.2077866 -4.2134733 -4.2197189 -4.2257676 -4.2212448 -4.207212 -4.1963768][-4.2258239 -4.2308893 -4.231595 -4.2192154 -4.2066054 -4.2115726 -4.2164087 -4.2162356 -4.212307 -4.2058349 -4.2071815 -4.2160225 -4.2183142 -4.2112579 -4.2046356][-4.2217751 -4.2294927 -4.232091 -4.2233458 -4.2135124 -4.2168055 -4.2210956 -4.2187114 -4.2088737 -4.1971478 -4.1992388 -4.2094426 -4.2161603 -4.2180834 -4.2208004]]...]
INFO - root - 2017-12-05 18:09:56.515153: step 32610, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 74h:17m:25s remains)
INFO - root - 2017-12-05 18:10:05.940911: step 32620, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.933 sec/batch; 77h:40m:50s remains)
INFO - root - 2017-12-05 18:10:15.264789: step 32630, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 78h:04m:32s remains)
INFO - root - 2017-12-05 18:10:24.514473: step 32640, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 76h:34m:00s remains)
INFO - root - 2017-12-05 18:10:33.706854: step 32650, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 82h:23m:02s remains)
INFO - root - 2017-12-05 18:10:43.043040: step 32660, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 81h:43m:15s remains)
INFO - root - 2017-12-05 18:10:52.415710: step 32670, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 81h:06m:58s remains)
INFO - root - 2017-12-05 18:11:01.953986: step 32680, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 80h:26m:14s remains)
INFO - root - 2017-12-05 18:11:11.519938: step 32690, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 77h:27m:17s remains)
INFO - root - 2017-12-05 18:11:20.868161: step 32700, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 76h:06m:26s remains)
2017-12-05 18:11:21.701002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2938085 -4.2928376 -4.2930546 -4.2930636 -4.2821078 -4.261023 -4.2421007 -4.2261105 -4.2220073 -4.2147484 -4.2159815 -4.2169037 -4.2040424 -4.1966825 -4.1982255][-4.2823315 -4.27828 -4.2762613 -4.2762809 -4.2662144 -4.2414017 -4.2193208 -4.1995029 -4.18794 -4.1720877 -4.1675787 -4.1637292 -4.1465788 -4.136889 -4.1411624][-4.2787123 -4.2755795 -4.270319 -4.2682729 -4.261065 -4.234097 -4.2026615 -4.1743531 -4.1559763 -4.1364574 -4.13042 -4.123467 -4.1040335 -4.0928669 -4.1011004][-4.28977 -4.2884245 -4.2782006 -4.2678432 -4.2518897 -4.21597 -4.1750875 -4.1392255 -4.1210489 -4.1095672 -4.1082344 -4.1028771 -4.08943 -4.082273 -4.0940118][-4.3084745 -4.3069196 -4.2892189 -4.2634997 -4.2234879 -4.1688895 -4.1168456 -4.0852075 -4.0876932 -4.0972962 -4.1051974 -4.1055636 -4.105197 -4.1117039 -4.1263494][-4.3203664 -4.3189497 -4.2949576 -4.2515988 -4.1842923 -4.0998316 -4.0219803 -3.9968088 -4.045711 -4.0933924 -4.119453 -4.1289253 -4.1397657 -4.1584897 -4.1777568][-4.3200908 -4.3210711 -4.2950621 -4.2414126 -4.1511264 -4.0354037 -3.9153795 -3.8879449 -3.9913871 -4.087184 -4.1411548 -4.1689715 -4.1902547 -4.2161922 -4.2361112][-4.3119841 -4.3134947 -4.2888865 -4.2341523 -4.1402574 -4.0173354 -3.8882642 -3.8609436 -3.9870102 -4.1035819 -4.1740465 -4.2120442 -4.2343616 -4.2561064 -4.2709842][-4.3058 -4.3035412 -4.2815723 -4.2334795 -4.1473646 -4.0440197 -3.953232 -3.9438884 -4.0500379 -4.1441951 -4.20812 -4.2439361 -4.2627306 -4.278162 -4.2892962][-4.3012614 -4.2945724 -4.276597 -4.2365589 -4.1662583 -4.0920095 -4.0410395 -4.0472054 -4.1242 -4.1923361 -4.2375865 -4.2670054 -4.2829933 -4.2928634 -4.3011017][-4.2961307 -4.2859974 -4.2711463 -4.2376404 -4.1817245 -4.13311 -4.1107478 -4.12448 -4.1783681 -4.2252073 -4.255271 -4.2808766 -4.2958803 -4.3024111 -4.3066134][-4.2923713 -4.2803626 -4.2682786 -4.2419081 -4.1954947 -4.1638646 -4.159255 -4.1760044 -4.2147508 -4.2466044 -4.267765 -4.2880287 -4.3031783 -4.3092718 -4.3111629][-4.2903414 -4.2771888 -4.2689643 -4.2514338 -4.2123141 -4.1927085 -4.1979136 -4.2094769 -4.2349114 -4.2512178 -4.2627192 -4.2795734 -4.2972984 -4.3089957 -4.3132305][-4.2899103 -4.2762775 -4.2712417 -4.261713 -4.2304668 -4.2203379 -4.2275648 -4.2297745 -4.239696 -4.2422814 -4.2439036 -4.2567821 -4.278501 -4.2974539 -4.3080297][-4.2902451 -4.2771306 -4.2754765 -4.2712059 -4.2466254 -4.2397819 -4.245172 -4.2340097 -4.2263575 -4.2113833 -4.2006578 -4.2130771 -4.2444882 -4.2755213 -4.2962956]]...]
INFO - root - 2017-12-05 18:11:30.939155: step 32710, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 74h:44m:19s remains)
INFO - root - 2017-12-05 18:11:40.126036: step 32720, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 81h:52m:18s remains)
INFO - root - 2017-12-05 18:11:49.466954: step 32730, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 77h:20m:07s remains)
INFO - root - 2017-12-05 18:11:59.026363: step 32740, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 79h:34m:46s remains)
INFO - root - 2017-12-05 18:12:08.305503: step 32750, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 80h:50m:01s remains)
INFO - root - 2017-12-05 18:12:17.585767: step 32760, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 78h:48m:17s remains)
INFO - root - 2017-12-05 18:12:26.871188: step 32770, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 81h:33m:03s remains)
INFO - root - 2017-12-05 18:12:36.113299: step 32780, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 80h:56m:07s remains)
INFO - root - 2017-12-05 18:12:45.598684: step 32790, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 80h:38m:41s remains)
INFO - root - 2017-12-05 18:12:54.902153: step 32800, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 77h:48m:20s remains)
2017-12-05 18:12:55.721058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2296987 -4.2250171 -4.20903 -4.1871843 -4.1732807 -4.1670713 -4.165482 -4.16379 -4.1544924 -4.1512313 -4.1656814 -4.194674 -4.2147393 -4.2151279 -4.2036362][-4.2184749 -4.2178836 -4.2030044 -4.1818895 -4.1730909 -4.1741056 -4.1761694 -4.1773329 -4.1711078 -4.16793 -4.1804409 -4.2018061 -4.2173939 -4.2167521 -4.2056932][-4.209897 -4.2106366 -4.1990809 -4.1833549 -4.1813884 -4.1888618 -4.1965051 -4.2050796 -4.20482 -4.1998382 -4.2046113 -4.2134409 -4.2220659 -4.2229891 -4.2153983][-4.19215 -4.1911149 -4.1814022 -4.1684222 -4.1707311 -4.1821289 -4.1954303 -4.2119217 -4.2147708 -4.2033687 -4.1974874 -4.196763 -4.2026782 -4.2087221 -4.2079291][-4.175415 -4.1714926 -4.1606278 -4.1459594 -4.1429315 -4.1487136 -4.15809 -4.1789627 -4.1861205 -4.1724439 -4.1618438 -4.1608596 -4.1697049 -4.1854496 -4.1928315][-4.1623511 -4.1616931 -4.1534052 -4.1366291 -4.1194272 -4.1070876 -4.1088166 -4.1385307 -4.15551 -4.1477675 -4.1419311 -4.1466727 -4.1583548 -4.173306 -4.179513][-4.1457067 -4.1461005 -4.1343923 -4.1046653 -4.0646782 -4.0337443 -4.041357 -4.0945797 -4.1325855 -4.1416254 -4.1493473 -4.1616545 -4.1714911 -4.1767244 -4.174088][-4.1275258 -4.1253943 -4.105442 -4.0626211 -4.0096679 -3.9726179 -3.9915824 -4.0622215 -4.1141734 -4.1380491 -4.1581197 -4.1762781 -4.1849985 -4.1821814 -4.1728606][-4.1335287 -4.1350389 -4.1205139 -4.0888519 -4.049284 -4.0189748 -4.0256004 -4.0727925 -4.1110811 -4.132596 -4.156527 -4.1785412 -4.189395 -4.1884785 -4.184526][-4.1599236 -4.166316 -4.1619048 -4.1458044 -4.1189413 -4.0890722 -4.0742397 -4.0884409 -4.1041141 -4.1168857 -4.1372423 -4.1599259 -4.1765022 -4.1869135 -4.1957293][-4.1805234 -4.1859684 -4.1844759 -4.1771526 -4.1588097 -4.1299515 -4.1037512 -4.0974255 -4.096848 -4.1004853 -4.1161084 -4.1387839 -4.1608138 -4.1795406 -4.1964908][-4.2026653 -4.2048225 -4.2045288 -4.2035141 -4.1948209 -4.1737471 -4.1494155 -4.1358509 -4.1260071 -4.1213474 -4.1291504 -4.1455364 -4.1662903 -4.1851006 -4.2018332][-4.2392926 -4.2375765 -4.2360134 -4.2391076 -4.238492 -4.2265511 -4.2102838 -4.19748 -4.1847491 -4.1747894 -4.1750479 -4.181747 -4.19412 -4.2076216 -4.2206178][-4.2759089 -4.2714667 -4.2700825 -4.2744055 -4.2768607 -4.2713451 -4.2614851 -4.2523351 -4.2408023 -4.2296882 -4.2247725 -4.224328 -4.2284026 -4.2356873 -4.2447724][-4.3050709 -4.300014 -4.2987008 -4.3010387 -4.3027954 -4.3002143 -4.2949257 -4.2900634 -4.2833052 -4.2759252 -4.2713704 -4.2690754 -4.2682986 -4.2687135 -4.2710204]]...]
INFO - root - 2017-12-05 18:13:05.155673: step 32810, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 80h:53m:19s remains)
INFO - root - 2017-12-05 18:13:14.347208: step 32820, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 80h:46m:52s remains)
INFO - root - 2017-12-05 18:13:23.816307: step 32830, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.897 sec/batch; 74h:37m:41s remains)
INFO - root - 2017-12-05 18:13:33.325138: step 32840, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.961 sec/batch; 79h:57m:22s remains)
INFO - root - 2017-12-05 18:13:42.593524: step 32850, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 76h:30m:31s remains)
INFO - root - 2017-12-05 18:13:51.986194: step 32860, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 81h:10m:01s remains)
INFO - root - 2017-12-05 18:14:01.211255: step 32870, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 77h:21m:15s remains)
INFO - root - 2017-12-05 18:14:10.394772: step 32880, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 81h:21m:32s remains)
INFO - root - 2017-12-05 18:14:19.765806: step 32890, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 78h:51m:44s remains)
INFO - root - 2017-12-05 18:14:29.130451: step 32900, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 78h:27m:30s remains)
2017-12-05 18:14:29.865585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2584734 -4.26356 -4.2764463 -4.2731161 -4.2602892 -4.25261 -4.2609911 -4.2667756 -4.2585025 -4.2447791 -4.2413588 -4.2409878 -4.2431226 -4.2493582 -4.2387638][-4.2538362 -4.2543769 -4.2695432 -4.2649355 -4.25236 -4.2417169 -4.2509575 -4.2550583 -4.2430592 -4.2249784 -4.2204704 -4.222003 -4.2224841 -4.229929 -4.2303066][-4.2324529 -4.2237844 -4.2335596 -4.226748 -4.2165689 -4.2062764 -4.2151866 -4.2161584 -4.2017436 -4.1847625 -4.1828856 -4.1854286 -4.1828313 -4.1913214 -4.1999497][-4.2004733 -4.1863866 -4.1932244 -4.1850262 -4.1787705 -4.1753206 -4.1810122 -4.1743035 -4.1551528 -4.1438122 -4.1440816 -4.1426864 -4.1353626 -4.1444988 -4.158565][-4.1583738 -4.1433473 -4.1543493 -4.15227 -4.14754 -4.1436973 -4.1391869 -4.1245189 -4.1140623 -4.120533 -4.128181 -4.1271868 -4.1192093 -4.1287336 -4.1398764][-4.1269464 -4.1088228 -4.1200633 -4.1231918 -4.1161952 -4.0963206 -4.0697737 -4.0581236 -4.0781541 -4.1142607 -4.1357112 -4.1398511 -4.1347332 -4.1408696 -4.1435132][-4.1278934 -4.1048203 -4.1099916 -4.1092539 -4.0876751 -4.0384197 -3.9773428 -3.9736555 -4.0353584 -4.1036696 -4.138103 -4.1503797 -4.1535873 -4.1615109 -4.1606979][-4.144496 -4.1233649 -4.1245174 -4.1214838 -4.0948467 -4.03567 -3.9601169 -3.9638002 -4.0388036 -4.1132655 -4.1470013 -4.1604905 -4.16667 -4.1763906 -4.1769652][-4.1759639 -4.1617341 -4.1655288 -4.1692896 -4.1574445 -4.1206388 -4.0668902 -4.0693145 -4.1149306 -4.1651554 -4.1870894 -4.1949472 -4.198638 -4.2077403 -4.2078295][-4.2033134 -4.194036 -4.2019367 -4.2138157 -4.215548 -4.1969113 -4.1654153 -4.16788 -4.191844 -4.2227254 -4.2389674 -4.2469435 -4.24785 -4.250761 -4.24652][-4.2187591 -4.2162395 -4.2276888 -4.2445216 -4.2585187 -4.2546844 -4.2376771 -4.23895 -4.2521114 -4.2726569 -4.285459 -4.2896457 -4.2870584 -4.2826591 -4.2752404][-4.22156 -4.2233944 -4.2339797 -4.25186 -4.2706671 -4.2764955 -4.271565 -4.2751474 -4.2856822 -4.2971725 -4.3037944 -4.305829 -4.3018346 -4.2924027 -4.2812328][-4.2078414 -4.207962 -4.217618 -4.239327 -4.2629743 -4.2770877 -4.2803783 -4.2857938 -4.2930961 -4.299387 -4.3034196 -4.3064108 -4.3025804 -4.2916031 -4.2803149][-4.2099309 -4.20731 -4.2119112 -4.22749 -4.2480068 -4.2646427 -4.274056 -4.2803988 -4.2839394 -4.2847681 -4.2870564 -4.2914538 -4.2912145 -4.286869 -4.2800169][-4.2228303 -4.2140675 -4.2094116 -4.2172279 -4.2297764 -4.2414951 -4.2503533 -4.2561717 -4.2586746 -4.2599869 -4.2647367 -4.2735782 -4.2796431 -4.281424 -4.277669]]...]
INFO - root - 2017-12-05 18:14:39.290237: step 32910, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 78h:45m:40s remains)
INFO - root - 2017-12-05 18:14:48.457944: step 32920, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 70h:44m:07s remains)
INFO - root - 2017-12-05 18:14:57.609135: step 32930, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 74h:09m:46s remains)
INFO - root - 2017-12-05 18:15:07.117581: step 32940, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 78h:30m:11s remains)
INFO - root - 2017-12-05 18:15:16.477433: step 32950, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 79h:18m:43s remains)
INFO - root - 2017-12-05 18:15:25.676969: step 32960, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 77h:21m:38s remains)
INFO - root - 2017-12-05 18:15:35.058318: step 32970, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 74h:38m:19s remains)
INFO - root - 2017-12-05 18:15:44.245709: step 32980, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 70h:39m:31s remains)
INFO - root - 2017-12-05 18:15:53.575726: step 32990, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 76h:15m:08s remains)
INFO - root - 2017-12-05 18:16:02.990167: step 33000, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.961 sec/batch; 79h:55m:08s remains)
2017-12-05 18:16:03.748503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2419934 -4.2221155 -4.1894097 -4.159749 -4.1433144 -4.1412387 -4.1501579 -4.1584878 -4.1618443 -4.1583886 -4.1644759 -4.190464 -4.2130427 -4.2152038 -4.2066336][-4.220881 -4.2019572 -4.1744647 -4.153677 -4.1460638 -4.150918 -4.1672745 -4.1855149 -4.1977849 -4.1953673 -4.1960273 -4.2131114 -4.2307634 -4.232461 -4.2232509][-4.1973195 -4.1815748 -4.1635075 -4.1506829 -4.1460791 -4.1519437 -4.169724 -4.1934762 -4.2112179 -4.20949 -4.2029815 -4.2105274 -4.2210236 -4.2243 -4.2203808][-4.1666884 -4.15155 -4.143043 -4.1411819 -4.13898 -4.1398234 -4.1480165 -4.1650753 -4.1801925 -4.17718 -4.1672659 -4.1692424 -4.1809869 -4.190414 -4.1959777][-4.1311636 -4.1140261 -4.1152983 -4.1243458 -4.1229873 -4.1093416 -4.0957651 -4.0995836 -4.1137443 -4.1121058 -4.1023521 -4.1098084 -4.1316261 -4.1480432 -4.1660767][-4.0981994 -4.0758114 -4.0816736 -4.0990109 -4.0975766 -4.0661354 -4.0304213 -4.0287752 -4.0532074 -4.061748 -4.0590658 -4.072865 -4.0984764 -4.1154766 -4.1405983][-4.0908065 -4.060853 -4.0607386 -4.075428 -4.0719481 -4.0345311 -3.9933958 -3.9941025 -4.0298 -4.0567465 -4.0692673 -4.0866308 -4.1083617 -4.1154251 -4.1319246][-4.1109233 -4.0688663 -4.05308 -4.0598326 -4.0589018 -4.0338831 -4.0009947 -4.0004654 -4.0406265 -4.0809011 -4.1040659 -4.1188974 -4.1351261 -4.1362257 -4.1400695][-4.1290927 -4.0815082 -4.05675 -4.0606041 -4.0678558 -4.0622835 -4.0411654 -4.034667 -4.0679188 -4.1102638 -4.1345344 -4.1403341 -4.14862 -4.1478739 -4.1482086][-4.1239772 -4.0797725 -4.0589437 -4.0670819 -4.0838742 -4.0982108 -4.0959053 -4.0864921 -4.104599 -4.1391077 -4.1553826 -4.1500626 -4.1475468 -4.1467237 -4.1513491][-4.1241469 -4.0892048 -4.0767694 -4.0872488 -4.1078615 -4.132442 -4.143846 -4.1401358 -4.1431565 -4.1607456 -4.1682034 -4.1554666 -4.14264 -4.1412816 -4.152678][-4.1486349 -4.1241474 -4.1144781 -4.1210365 -4.1366158 -4.1578693 -4.1727433 -4.1737738 -4.1699505 -4.1736026 -4.1720839 -4.1553106 -4.1370287 -4.1359754 -4.1542449][-4.1690426 -4.1554341 -4.1442542 -4.1485705 -4.1613421 -4.1798043 -4.1948423 -4.1990414 -4.193738 -4.1861477 -4.1755381 -4.1580563 -4.1406689 -4.1389813 -4.1587543][-4.1821284 -4.1734877 -4.1612029 -4.1625867 -4.1758838 -4.1965647 -4.2148776 -4.2203722 -4.208179 -4.1901174 -4.1737895 -4.1586685 -4.1458721 -4.1454649 -4.1615758][-4.1969104 -4.1894774 -4.1750593 -4.1713495 -4.182672 -4.203948 -4.2254062 -4.2318773 -4.2166529 -4.1948133 -4.17615 -4.1652565 -4.1606545 -4.1610351 -4.1688085]]...]
INFO - root - 2017-12-05 18:16:12.952853: step 33010, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 72h:23m:18s remains)
INFO - root - 2017-12-05 18:16:21.976312: step 33020, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 77h:44m:52s remains)
INFO - root - 2017-12-05 18:16:31.204662: step 33030, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 78h:37m:15s remains)
INFO - root - 2017-12-05 18:16:40.465593: step 33040, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 81h:33m:45s remains)
INFO - root - 2017-12-05 18:16:49.843449: step 33050, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 75h:55m:32s remains)
INFO - root - 2017-12-05 18:16:59.138710: step 33060, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 78h:48m:31s remains)
INFO - root - 2017-12-05 18:17:08.540260: step 33070, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 78h:59m:28s remains)
INFO - root - 2017-12-05 18:17:18.153362: step 33080, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 81h:56m:57s remains)
INFO - root - 2017-12-05 18:17:27.761728: step 33090, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 82h:13m:18s remains)
INFO - root - 2017-12-05 18:17:37.157808: step 33100, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.971 sec/batch; 80h:44m:08s remains)
2017-12-05 18:17:37.967139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2000408 -4.1797848 -4.1757317 -4.1851549 -4.1918354 -4.1993461 -4.2020307 -4.1876273 -4.1790366 -4.2070422 -4.2583876 -4.2961235 -4.3065195 -4.2910204 -4.2589703][-4.1984324 -4.1674309 -4.146102 -4.1466732 -4.1509128 -4.1530623 -4.1456547 -4.1164622 -4.1018004 -4.1409917 -4.2085543 -4.2618265 -4.2860732 -4.2810674 -4.2518253][-4.2093978 -4.1736016 -4.141942 -4.1381416 -4.1440859 -4.1379776 -4.1186438 -4.0814462 -4.0690451 -4.114531 -4.185883 -4.2423563 -4.2730374 -4.2762995 -4.2500758][-4.2265825 -4.1945767 -4.1646032 -4.1612 -4.1661391 -4.149364 -4.1191864 -4.0813351 -4.0747933 -4.1218171 -4.18749 -4.2353954 -4.2648978 -4.2738061 -4.2499766][-4.246798 -4.2217922 -4.1976395 -4.1915727 -4.1840477 -4.1532617 -4.1124482 -4.0815868 -4.0897169 -4.1393991 -4.1973953 -4.2357378 -4.2601695 -4.269392 -4.2481341][-4.2643008 -4.2404447 -4.2123222 -4.1885695 -4.1553574 -4.1029949 -4.0472789 -4.0288734 -4.068543 -4.1381125 -4.197289 -4.2336869 -4.25468 -4.2627578 -4.2433367][-4.2687235 -4.2411947 -4.2025633 -4.1556158 -4.09016 -4.0061107 -3.9315281 -3.9331503 -4.0163517 -4.1147022 -4.18214 -4.2249427 -4.2511721 -4.260119 -4.2407646][-4.2602105 -4.229588 -4.1844072 -4.1213431 -4.0332017 -3.9238195 -3.8413086 -3.8781164 -3.9998791 -4.1133313 -4.1853852 -4.2313051 -4.2615471 -4.2712235 -4.2492571][-4.2474637 -4.2170734 -4.1720467 -4.110538 -4.0296812 -3.9408915 -3.88771 -3.941376 -4.0549893 -4.1477332 -4.2056236 -4.2467346 -4.272665 -4.2758675 -4.2510042][-4.2390475 -4.212429 -4.1767726 -4.1349535 -4.0903707 -4.050827 -4.0324268 -4.0676045 -4.1316957 -4.1819844 -4.219533 -4.2526608 -4.2695813 -4.2627692 -4.2345591][-4.2295866 -4.2161531 -4.2030187 -4.1822634 -4.165761 -4.151824 -4.1397467 -4.1427565 -4.1560187 -4.1700368 -4.1959443 -4.2330241 -4.2523103 -4.2427554 -4.21373][-4.21761 -4.2220411 -4.2330294 -4.2305579 -4.2228079 -4.2052774 -4.1807642 -4.1512146 -4.1197791 -4.1054258 -4.1283078 -4.17997 -4.2149057 -4.2153931 -4.1923842][-4.1983876 -4.2193866 -4.2467971 -4.2543683 -4.2437491 -4.2194552 -4.1834569 -4.13643 -4.0825734 -4.0525293 -4.0717511 -4.13328 -4.1817203 -4.1968083 -4.1862907][-4.1688504 -4.1949677 -4.2303581 -4.2445507 -4.2399573 -4.2238064 -4.194705 -4.1561379 -4.108676 -4.0790143 -4.0931873 -4.1411347 -4.1787767 -4.1956043 -4.198832][-4.1302061 -4.1465063 -4.1774845 -4.1985559 -4.2117457 -4.2167444 -4.2143679 -4.2093811 -4.1955371 -4.1781521 -4.17981 -4.1990409 -4.210999 -4.2181921 -4.220314]]...]
INFO - root - 2017-12-05 18:17:47.090194: step 33110, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 76h:58m:07s remains)
INFO - root - 2017-12-05 18:17:56.598026: step 33120, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 81h:35m:06s remains)
INFO - root - 2017-12-05 18:18:05.998677: step 33130, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 82h:48m:29s remains)
INFO - root - 2017-12-05 18:18:15.236059: step 33140, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.977 sec/batch; 81h:16m:12s remains)
INFO - root - 2017-12-05 18:18:24.782467: step 33150, loss = 2.05, batch loss = 2.00 (7.8 examples/sec; 1.026 sec/batch; 85h:18m:20s remains)
INFO - root - 2017-12-05 18:18:34.363899: step 33160, loss = 2.05, batch loss = 1.99 (7.8 examples/sec; 1.020 sec/batch; 84h:50m:05s remains)
INFO - root - 2017-12-05 18:18:43.770484: step 33170, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 81h:45m:46s remains)
INFO - root - 2017-12-05 18:18:53.209127: step 33180, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 71h:49m:11s remains)
INFO - root - 2017-12-05 18:19:02.649867: step 33190, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 81h:07m:44s remains)
INFO - root - 2017-12-05 18:19:11.984800: step 33200, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 79h:02m:05s remains)
2017-12-05 18:19:12.747078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2811742 -4.2792721 -4.2730503 -4.2569618 -4.2271242 -4.1833048 -4.1375175 -4.106967 -4.1030464 -4.1133595 -4.1366611 -4.156764 -4.1713247 -4.1846523 -4.2167335][-4.2720971 -4.2710385 -4.26465 -4.2442842 -4.2050376 -4.1472969 -4.0979581 -4.0740943 -4.0813022 -4.1036043 -4.13395 -4.1514759 -4.1708255 -4.1919603 -4.2258925][-4.2593575 -4.2632856 -4.2577591 -4.2339268 -4.1905107 -4.1244969 -4.077755 -4.0631709 -4.0837746 -4.1164584 -4.1427164 -4.1510592 -4.1697783 -4.1957335 -4.2286296][-4.2316456 -4.2515655 -4.2550964 -4.2323914 -4.1873422 -4.1135578 -4.0592365 -4.0499892 -4.0875907 -4.1317043 -4.1527405 -4.1531954 -4.16602 -4.1923718 -4.2237744][-4.2046909 -4.2371826 -4.2510023 -4.2322731 -4.1810513 -4.0916667 -4.0098839 -3.9953969 -4.0571518 -4.1196828 -4.14573 -4.1474676 -4.160574 -4.189373 -4.2222519][-4.186656 -4.2235904 -4.2375903 -4.2110052 -4.1407204 -4.0216975 -3.892971 -3.865737 -3.9772811 -4.0771275 -4.119082 -4.1300182 -4.148437 -4.1836495 -4.2236948][-4.1498241 -4.1947026 -4.2126441 -4.1813335 -4.095377 -3.9516871 -3.7857745 -3.7448602 -3.9075487 -4.0438867 -4.0990248 -4.1222138 -4.1489115 -4.187849 -4.2319646][-4.1197166 -4.1706681 -4.1981072 -4.176259 -4.103982 -3.978883 -3.8406668 -3.8081512 -3.9501624 -4.0663257 -4.1029496 -4.119081 -4.145752 -4.1860781 -4.2304783][-4.1425481 -4.1823969 -4.205349 -4.1886272 -4.1353416 -4.0507469 -3.968559 -3.953578 -4.0435443 -4.1175938 -4.128736 -4.1304708 -4.1541629 -4.194 -4.2287478][-4.1903229 -4.2102747 -4.22492 -4.2125511 -4.173059 -4.1221437 -4.0803051 -4.0760164 -4.1261992 -4.1681952 -4.1685181 -4.1661191 -4.1855354 -4.2176404 -4.2363105][-4.2442484 -4.2485323 -4.255291 -4.2425489 -4.2079282 -4.1724916 -4.1457849 -4.1454024 -4.1723251 -4.1934094 -4.1922884 -4.1926174 -4.2113638 -4.2406387 -4.2510304][-4.2616 -4.2570791 -4.259769 -4.2453804 -4.2147617 -4.1860828 -4.1671247 -4.1732311 -4.1965051 -4.2116389 -4.2086067 -4.2094345 -4.2261081 -4.2537007 -4.2654171][-4.2611527 -4.2548404 -4.2532568 -4.2367878 -4.2140064 -4.192965 -4.183053 -4.1955104 -4.2205343 -4.2334957 -4.2265086 -4.2261305 -4.2410603 -4.2680893 -4.2824349][-4.27464 -4.2699609 -4.2631369 -4.2461414 -4.2284832 -4.2127695 -4.20904 -4.2201147 -4.242517 -4.2548709 -4.2508278 -4.2547579 -4.268621 -4.2913127 -4.3040285][-4.2918663 -4.2868533 -4.2800512 -4.2707157 -4.2612839 -4.2513332 -4.2472677 -4.2498107 -4.2608356 -4.2722006 -4.2762251 -4.2847853 -4.298234 -4.3164864 -4.3265848]]...]
INFO - root - 2017-12-05 18:19:22.213017: step 33210, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 77h:22m:16s remains)
INFO - root - 2017-12-05 18:19:31.616325: step 33220, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 76h:42m:51s remains)
INFO - root - 2017-12-05 18:19:41.116564: step 33230, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 70h:04m:50s remains)
INFO - root - 2017-12-05 18:19:50.553996: step 33240, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 76h:59m:32s remains)
INFO - root - 2017-12-05 18:20:00.058397: step 33250, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 75h:47m:02s remains)
INFO - root - 2017-12-05 18:20:09.484067: step 33260, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.935 sec/batch; 77h:41m:57s remains)
INFO - root - 2017-12-05 18:20:18.961210: step 33270, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 77h:29m:42s remains)
INFO - root - 2017-12-05 18:20:28.462850: step 33280, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 75h:09m:14s remains)
INFO - root - 2017-12-05 18:20:37.793270: step 33290, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 75h:03m:42s remains)
INFO - root - 2017-12-05 18:20:47.352591: step 33300, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 81h:00m:58s remains)
2017-12-05 18:20:48.089643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2803822 -4.2920742 -4.2849584 -4.2526975 -4.2151685 -4.1783929 -4.1347151 -4.1028466 -4.1066046 -4.1482644 -4.1992469 -4.2498217 -4.2916026 -4.3138585 -4.3266134][-4.2952442 -4.3086143 -4.3012838 -4.2645607 -4.2209558 -4.1756606 -4.1158648 -4.0710421 -4.079567 -4.1335063 -4.1892 -4.2460837 -4.2920456 -4.3134875 -4.326263][-4.2871356 -4.3034 -4.2994838 -4.2643981 -4.2186971 -4.162395 -4.08792 -4.0365863 -4.0544195 -4.1207809 -4.1806822 -4.243485 -4.2923613 -4.3129826 -4.3250461][-4.2672734 -4.2858787 -4.2833142 -4.2466316 -4.1948228 -4.124249 -4.0426683 -3.9955897 -4.0282183 -4.1060524 -4.1747551 -4.2453823 -4.2952166 -4.3140268 -4.3253231][-4.2511225 -4.2674718 -4.2605033 -4.2203174 -4.1585393 -4.0752168 -3.9962158 -3.9655414 -4.0168538 -4.1039739 -4.1818137 -4.258131 -4.3060503 -4.3211055 -4.329536][-4.2430129 -4.2489071 -4.2324309 -4.1858954 -4.1097093 -4.0136957 -3.9524179 -3.9524977 -4.0202789 -4.1093316 -4.1896014 -4.2690263 -4.3152061 -4.3294315 -4.334147][-4.2265835 -4.2215996 -4.1959543 -4.1372466 -4.0385027 -3.9247305 -3.8937607 -3.942359 -4.0321479 -4.1215849 -4.2009273 -4.2777028 -4.3217125 -4.3361273 -4.3387194][-4.2168026 -4.2046204 -4.1746716 -4.1135993 -4.0059247 -3.8870552 -3.8797622 -3.9557276 -4.0526638 -4.1367474 -4.2106767 -4.2811208 -4.3242626 -4.3406615 -4.3427262][-4.2233868 -4.2108812 -4.1822963 -4.130188 -4.0357375 -3.9354534 -3.9354441 -3.9985311 -4.0759621 -4.1439924 -4.2105579 -4.2778883 -4.3231654 -4.3415718 -4.3438778][-4.232717 -4.2238507 -4.1989627 -4.1560678 -4.0784726 -4.0002165 -4.0005288 -4.0400586 -4.093153 -4.14722 -4.2067318 -4.2728682 -4.3198438 -4.33921 -4.3421888][-4.2429657 -4.2334809 -4.2089114 -4.1713862 -4.1107 -4.0547438 -4.0551028 -4.0779204 -4.1154947 -4.1605382 -4.2132864 -4.2736888 -4.3187757 -4.3381324 -4.3414078][-4.2522736 -4.2384419 -4.2154088 -4.1852264 -4.1407967 -4.1010146 -4.0983911 -4.1095924 -4.1411591 -4.1836815 -4.2293992 -4.2807159 -4.3207083 -4.3385248 -4.3415413][-4.2643042 -4.2500105 -4.2279806 -4.205225 -4.1737041 -4.1455908 -4.1394057 -4.1416965 -4.1671538 -4.2052541 -4.2431121 -4.28678 -4.3225117 -4.3392367 -4.3419929][-4.2805572 -4.2675986 -4.2484245 -4.22842 -4.2028027 -4.1840043 -4.1772976 -4.1773682 -4.1949863 -4.2238922 -4.2539515 -4.2915964 -4.3231926 -4.339128 -4.3418546][-4.2796674 -4.2665 -4.2496357 -4.2292686 -4.2089453 -4.1965308 -4.190577 -4.1909494 -4.2047162 -4.2298217 -4.2596607 -4.2957597 -4.3241916 -4.3387337 -4.3414497]]...]
INFO - root - 2017-12-05 18:20:57.383094: step 33310, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 77h:05m:30s remains)
INFO - root - 2017-12-05 18:21:06.881184: step 33320, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 74h:10m:03s remains)
INFO - root - 2017-12-05 18:21:16.351824: step 33330, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 74h:00m:06s remains)
INFO - root - 2017-12-05 18:21:25.794960: step 33340, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.928 sec/batch; 77h:09m:23s remains)
INFO - root - 2017-12-05 18:21:35.143092: step 33350, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 77h:51m:23s remains)
INFO - root - 2017-12-05 18:21:44.589003: step 33360, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 79h:54m:03s remains)
INFO - root - 2017-12-05 18:21:54.116081: step 33370, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 81h:31m:07s remains)
INFO - root - 2017-12-05 18:22:03.371132: step 33380, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 69h:41m:08s remains)
INFO - root - 2017-12-05 18:22:12.807249: step 33390, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 77h:41m:59s remains)
INFO - root - 2017-12-05 18:22:22.089753: step 33400, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 67h:41m:06s remains)
2017-12-05 18:22:22.909017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2817283 -4.2730756 -4.271091 -4.2704816 -4.2646728 -4.2604775 -4.2621927 -4.2632585 -4.2546248 -4.2359381 -4.21519 -4.2022634 -4.2065625 -4.2205667 -4.2358489][-4.279633 -4.2707167 -4.2678432 -4.2644291 -4.256062 -4.2498422 -4.2519941 -4.2542 -4.2465725 -4.2283978 -4.2052107 -4.18827 -4.189795 -4.2056274 -4.2242641][-4.2717309 -4.2622318 -4.2596617 -4.2557039 -4.2449727 -4.234529 -4.2321987 -4.231873 -4.2265654 -4.2162051 -4.1989927 -4.1795816 -4.1761827 -4.1903548 -4.2080946][-4.2579703 -4.24492 -4.2408929 -4.236733 -4.2231574 -4.2045255 -4.1898813 -4.1804371 -4.1797481 -4.1857891 -4.1826959 -4.1651192 -4.1558843 -4.1635656 -4.1772819][-4.2395234 -4.2190833 -4.2101431 -4.2032013 -4.1843171 -4.1531897 -4.1182632 -4.0904784 -4.0927453 -4.1244721 -4.1470742 -4.141221 -4.128561 -4.1286387 -4.1398706][-4.2151771 -4.1820264 -4.1636429 -4.1539879 -4.1323524 -4.0881066 -4.0240211 -3.9662554 -3.9704583 -4.0374627 -4.0968547 -4.1142969 -4.1062465 -4.1003895 -4.108778][-4.1826282 -4.1404982 -4.1166673 -4.1076307 -4.0880022 -4.0325017 -3.9396317 -3.8563213 -3.8662596 -3.9666915 -4.0600433 -4.10397 -4.1029072 -4.0903754 -4.0916343][-4.1625485 -4.1184697 -4.0952582 -4.0904565 -4.0794573 -4.0312223 -3.9469695 -3.8802567 -3.8992319 -3.99135 -4.0821328 -4.1350918 -4.1378789 -4.1186318 -4.1112757][-4.1697259 -4.1294813 -4.10908 -4.1085911 -4.1102734 -4.0871973 -4.0423045 -4.0111175 -4.0290112 -4.0851808 -4.1471114 -4.1897082 -4.1927648 -4.1743679 -4.1644077][-4.1914048 -4.1583133 -4.143434 -4.14861 -4.1632876 -4.1658249 -4.1540542 -4.1441541 -4.1526151 -4.1752181 -4.2072592 -4.2329483 -4.2320418 -4.2169337 -4.2088618][-4.203258 -4.1728415 -4.1635485 -4.179472 -4.2083788 -4.2299004 -4.2368565 -4.2358646 -4.2342343 -4.2341013 -4.2428093 -4.24864 -4.2394967 -4.2279243 -4.2264428][-4.2025876 -4.1686091 -4.15911 -4.1836514 -4.2263212 -4.2625093 -4.2797661 -4.2832179 -4.2769623 -4.2659421 -4.2586989 -4.2475548 -4.2290378 -4.2191472 -4.2245803][-4.2047071 -4.16529 -4.1488762 -4.1716156 -4.2202659 -4.2673016 -4.292428 -4.2973089 -4.2880158 -4.273366 -4.2602382 -4.2419229 -4.2196317 -4.2112741 -4.2204065][-4.2176008 -4.1756248 -4.1499596 -4.16003 -4.199873 -4.2488766 -4.2793827 -4.2839165 -4.2738137 -4.261538 -4.2506967 -4.2328649 -4.2141643 -4.2117791 -4.225246][-4.2267585 -4.1870527 -4.1573086 -4.1530809 -4.1759706 -4.2166572 -4.2466526 -4.2519479 -4.2443185 -4.2358303 -4.2263989 -4.2106733 -4.20107 -4.209126 -4.2267485]]...]
INFO - root - 2017-12-05 18:22:32.208244: step 33410, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 77h:33m:10s remains)
INFO - root - 2017-12-05 18:22:41.443975: step 33420, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 72h:55m:45s remains)
INFO - root - 2017-12-05 18:22:50.983956: step 33430, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 75h:14m:08s remains)
INFO - root - 2017-12-05 18:23:00.329664: step 33440, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 78h:03m:34s remains)
INFO - root - 2017-12-05 18:23:09.624477: step 33450, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 81h:00m:27s remains)
INFO - root - 2017-12-05 18:23:19.186377: step 33460, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 78h:40m:31s remains)
INFO - root - 2017-12-05 18:23:28.551623: step 33470, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 77h:58m:16s remains)
INFO - root - 2017-12-05 18:23:37.755411: step 33480, loss = 2.05, batch loss = 1.99 (7.6 examples/sec; 1.054 sec/batch; 87h:31m:54s remains)
INFO - root - 2017-12-05 18:23:47.093060: step 33490, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 74h:10m:21s remains)
INFO - root - 2017-12-05 18:23:56.558550: step 33500, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 78h:45m:31s remains)
2017-12-05 18:23:57.308257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3370471 -4.3322597 -4.3189292 -4.3013639 -4.2834525 -4.2612576 -4.2370744 -4.2271347 -4.2344079 -4.2508879 -4.2630582 -4.2677493 -4.264329 -4.2588663 -4.2524509][-4.350812 -4.3461523 -4.3269076 -4.2974868 -4.2650819 -4.2312932 -4.1958156 -4.175283 -4.1790318 -4.2011056 -4.2259874 -4.2410479 -4.2454295 -4.2482591 -4.2469172][-4.3598976 -4.3531928 -4.3292756 -4.2900677 -4.2455611 -4.2008667 -4.1543264 -4.1242595 -4.1276321 -4.1569195 -4.1922235 -4.2127986 -4.2208886 -4.2276368 -4.2287307][-4.3632631 -4.3547783 -4.3280277 -4.28184 -4.2275047 -4.1722913 -4.114109 -4.0797915 -4.090518 -4.1316323 -4.1749496 -4.1925807 -4.1950178 -4.1973739 -4.1980181][-4.3626261 -4.3517323 -4.3211365 -4.26832 -4.2040286 -4.1389351 -4.070076 -4.0375195 -4.0629764 -4.121347 -4.171576 -4.1854877 -4.18134 -4.1780472 -4.1792006][-4.3579025 -4.340714 -4.3027897 -4.2414126 -4.1650229 -4.0918126 -4.0203381 -3.9976058 -4.0386147 -4.1129208 -4.1735559 -4.1905017 -4.1870575 -4.1830516 -4.1884131][-4.3491549 -4.3239141 -4.2800436 -4.2131224 -4.1293054 -4.0563498 -3.9940341 -3.9879718 -4.0426369 -4.1267548 -4.1924076 -4.2126026 -4.2113481 -4.21015 -4.2218122][-4.3412957 -4.3115106 -4.2672586 -4.2022567 -4.1216388 -4.05801 -4.0146723 -4.0290785 -4.0917673 -4.1735344 -4.2326956 -4.2497745 -4.2487826 -4.2495255 -4.2616787][-4.337945 -4.3079009 -4.2668719 -4.2093945 -4.1424079 -4.0916014 -4.066442 -4.0964937 -4.1592422 -4.2287354 -4.2745996 -4.2872953 -4.2867241 -4.2856565 -4.2927122][-4.3363552 -4.3069339 -4.2703643 -4.222887 -4.1745243 -4.1408029 -4.1274071 -4.1598835 -4.2160788 -4.2717848 -4.3030233 -4.3108177 -4.3113217 -4.3110676 -4.3149223][-4.3312054 -4.3022451 -4.2697897 -4.2324705 -4.20449 -4.1902685 -4.1881204 -4.2192535 -4.266191 -4.30672 -4.3235483 -4.3241172 -4.3232388 -4.3233519 -4.32516][-4.324614 -4.2981782 -4.2704477 -4.2439971 -4.2351213 -4.2393122 -4.2481713 -4.2752566 -4.3082094 -4.3308926 -4.3334551 -4.3262334 -4.3226972 -4.3209658 -4.3214283][-4.3187823 -4.2962785 -4.2765045 -4.2645903 -4.2717905 -4.2859936 -4.2975106 -4.3154941 -4.3316822 -4.3385754 -4.331418 -4.3224721 -4.3176455 -4.3133206 -4.3081508][-4.3150196 -4.2978082 -4.2867236 -4.2870879 -4.3036513 -4.3200412 -4.32577 -4.3310394 -4.3349061 -4.3336625 -4.3243356 -4.3163605 -4.3108406 -4.3014927 -4.2864966][-4.3141589 -4.3030572 -4.2989717 -4.3047051 -4.3222184 -4.3366752 -4.3384523 -4.3357539 -4.3324919 -4.3282142 -4.3198819 -4.3126841 -4.3041725 -4.2895112 -4.265717]]...]
INFO - root - 2017-12-05 18:24:06.521511: step 33510, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 73h:30m:42s remains)
INFO - root - 2017-12-05 18:24:15.818082: step 33520, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.861 sec/batch; 71h:31m:53s remains)
INFO - root - 2017-12-05 18:24:25.035566: step 33530, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 74h:25m:46s remains)
INFO - root - 2017-12-05 18:24:34.686527: step 33540, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 79h:23m:48s remains)
INFO - root - 2017-12-05 18:24:44.059365: step 33550, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 80h:08m:20s remains)
INFO - root - 2017-12-05 18:24:53.468727: step 33560, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 81h:28m:13s remains)
INFO - root - 2017-12-05 18:25:03.072195: step 33570, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 78h:29m:54s remains)
INFO - root - 2017-12-05 18:25:12.723603: step 33580, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.012 sec/batch; 84h:03m:09s remains)
INFO - root - 2017-12-05 18:25:22.120233: step 33590, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 82h:04m:01s remains)
INFO - root - 2017-12-05 18:25:31.430518: step 33600, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 73h:50m:45s remains)
2017-12-05 18:25:32.188098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.315177 -4.293932 -4.2721233 -4.2590208 -4.2569637 -4.2668915 -4.2802458 -4.2878165 -4.2900519 -4.2883821 -4.2879567 -4.2928357 -4.2988625 -4.3036466 -4.3064713][-4.3062778 -4.2804537 -4.2541103 -4.2376962 -4.2367487 -4.2521081 -4.2709665 -4.2843447 -4.292624 -4.2942257 -4.2909756 -4.29109 -4.2955155 -4.3006811 -4.3057418][-4.29405 -4.2600303 -4.2259011 -4.2030783 -4.1995931 -4.2161784 -4.2392731 -4.2620535 -4.28119 -4.2909765 -4.2892976 -4.2872028 -4.2903628 -4.2961855 -4.3028927][-4.2824693 -4.2396874 -4.1969161 -4.1653123 -4.1560712 -4.1697965 -4.1934295 -4.226419 -4.2614226 -4.2833157 -4.2873869 -4.2867293 -4.2882509 -4.2921677 -4.2998018][-4.2796416 -4.232161 -4.1791677 -4.1341777 -4.1100593 -4.111124 -4.1292171 -4.1664495 -4.2180614 -4.2562504 -4.270493 -4.2758284 -4.2774072 -4.2784705 -4.2867932][-4.2868843 -4.2420754 -4.1836224 -4.1234293 -4.0721869 -4.0402412 -4.0303755 -4.0578294 -4.1273761 -4.18967 -4.2227139 -4.2412963 -4.2483854 -4.2520351 -4.263792][-4.2961473 -4.2585683 -4.2047544 -4.1386003 -4.0629597 -3.9874027 -3.9246716 -3.9144044 -3.9873877 -4.07482 -4.1365681 -4.1773124 -4.1974583 -4.2091103 -4.2254128][-4.3022923 -4.2727227 -4.2308326 -4.1724615 -4.0909066 -3.9881995 -3.8719857 -3.7986763 -3.840899 -3.9338548 -4.0203137 -4.0841427 -4.1227059 -4.1454439 -4.1684146][-4.3046608 -4.2848816 -4.2592149 -4.2184834 -4.1514258 -4.0545192 -3.9257085 -3.8138402 -3.7983131 -3.8607166 -3.9448552 -4.0171175 -4.0650978 -4.0950489 -4.1229458][-4.3091431 -4.2959557 -4.2824297 -4.2593341 -4.2155666 -4.1478977 -4.0455837 -3.9392879 -3.8891072 -3.9025018 -3.9506452 -4.0037446 -4.0446119 -4.0746756 -4.1037788][-4.3203425 -4.3083038 -4.2968974 -4.2842946 -4.261868 -4.2235203 -4.1565309 -4.0741253 -4.0177779 -4.0030761 -4.0171046 -4.0453715 -4.0739717 -4.09883 -4.1226416][-4.3384686 -4.3297696 -4.3193932 -4.3110371 -4.3006439 -4.2824383 -4.2429395 -4.1871953 -4.1424189 -4.1213789 -4.1195579 -4.1325359 -4.1502228 -4.1661563 -4.1818409][-4.3528566 -4.3493123 -4.3406997 -4.3339515 -4.3284473 -4.320003 -4.2996936 -4.2687864 -4.2434964 -4.2286792 -4.2231412 -4.2288809 -4.2407656 -4.250165 -4.2587557][-4.3591251 -4.3589945 -4.3536119 -4.349472 -4.3465714 -4.34317 -4.3343139 -4.3208508 -4.310925 -4.3047609 -4.3012123 -4.3041935 -4.3125029 -4.3181911 -4.3228664][-4.3591094 -4.3605418 -4.3590641 -4.3588133 -4.359025 -4.3580747 -4.3541465 -4.3496165 -4.3479366 -4.347743 -4.3478184 -4.3502173 -4.3549662 -4.3580613 -4.3603034]]...]
INFO - root - 2017-12-05 18:25:41.832240: step 33610, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.988 sec/batch; 81h:59m:58s remains)
INFO - root - 2017-12-05 18:25:51.306767: step 33620, loss = 2.04, batch loss = 1.98 (7.9 examples/sec; 1.007 sec/batch; 83h:36m:48s remains)
INFO - root - 2017-12-05 18:26:00.881012: step 33630, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.914 sec/batch; 75h:54m:38s remains)
INFO - root - 2017-12-05 18:26:10.213392: step 33640, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 75h:26m:52s remains)
INFO - root - 2017-12-05 18:26:19.679526: step 33650, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 69h:57m:47s remains)
INFO - root - 2017-12-05 18:26:29.060058: step 33660, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 81h:13m:53s remains)
INFO - root - 2017-12-05 18:26:38.551111: step 33670, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 76h:06m:58s remains)
INFO - root - 2017-12-05 18:26:47.715556: step 33680, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 79h:21m:37s remains)
INFO - root - 2017-12-05 18:26:57.200678: step 33690, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 72h:25m:29s remains)
INFO - root - 2017-12-05 18:27:06.721257: step 33700, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 76h:48m:58s remains)
2017-12-05 18:27:07.450033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2843232 -4.2851386 -4.292778 -4.3095441 -4.3228393 -4.3187375 -4.3069882 -4.3047447 -4.3016729 -4.2899036 -4.283186 -4.2906947 -4.3074079 -4.3234596 -4.3378534][-4.242486 -4.2416239 -4.2511787 -4.2726421 -4.2868128 -4.2780266 -4.2623291 -4.2635117 -4.2629466 -4.2497368 -4.2444034 -4.2567639 -4.2766275 -4.2961631 -4.3153124][-4.1880374 -4.1893845 -4.2057061 -4.2341228 -4.244041 -4.2294912 -4.211453 -4.21806 -4.22551 -4.2153554 -4.2141771 -4.2325044 -4.2553978 -4.2773504 -4.2984686][-4.1569157 -4.1564407 -4.1755176 -4.2055726 -4.2072768 -4.1848526 -4.1615586 -4.1685715 -4.1811948 -4.1823869 -4.19041 -4.2127671 -4.2431679 -4.274003 -4.2960196][-4.164382 -4.1580086 -4.169292 -4.1858892 -4.1715188 -4.1371632 -4.1040254 -4.1009607 -4.1122928 -4.125062 -4.148201 -4.1801052 -4.2216825 -4.2645593 -4.2910867][-4.1859431 -4.1683235 -4.1652484 -4.1654916 -4.1319957 -4.0768104 -4.0234408 -4.0092998 -4.0255227 -4.0480175 -4.0827751 -4.1267986 -4.1811113 -4.2392979 -4.270205][-4.2061787 -4.1803727 -4.1617675 -4.1412749 -4.0905337 -4.0165553 -3.9426708 -3.9131002 -3.9234729 -3.9515212 -3.9994709 -4.0603828 -4.1265311 -4.1969566 -4.2353625][-4.2212114 -4.1882858 -4.1556635 -4.12287 -4.0703325 -3.9949312 -3.9160683 -3.866991 -3.8450069 -3.8572359 -3.91916 -4.0015674 -4.0758157 -4.1485043 -4.1939526][-4.2382159 -4.2048597 -4.1693606 -4.1358294 -4.0919604 -4.0312624 -3.9689202 -3.9189775 -3.8816602 -3.8758061 -3.9279985 -4.0109091 -4.0807996 -4.1414528 -4.1827111][-4.256413 -4.2299757 -4.202961 -4.1781464 -4.1466503 -4.1054339 -4.0691795 -4.0366559 -4.0063143 -3.9931269 -4.022408 -4.0814729 -4.1301451 -4.1724868 -4.2064271][-4.2736559 -4.2556639 -4.2413387 -4.2301807 -4.2127547 -4.1893625 -4.1702971 -4.1530194 -4.131043 -4.1174049 -4.1305094 -4.1657495 -4.1945186 -4.2214007 -4.2497144][-4.2888861 -4.2772779 -4.2704487 -4.26784 -4.261447 -4.2543044 -4.2481937 -4.2399035 -4.2234097 -4.2095118 -4.2129931 -4.2337885 -4.250875 -4.2680311 -4.2895012][-4.3054762 -4.2973976 -4.292635 -4.2932868 -4.2932796 -4.2919536 -4.2908244 -4.2884026 -4.2771783 -4.2654762 -4.2644 -4.2761726 -4.2885146 -4.2991838 -4.31438][-4.320919 -4.3169823 -4.3153896 -4.317102 -4.31802 -4.3175826 -4.3164988 -4.3151755 -4.3110609 -4.3058867 -4.3053541 -4.3109741 -4.3195004 -4.3266292 -4.3359346][-4.3337145 -4.3324418 -4.3325167 -4.3339357 -4.3346624 -4.334384 -4.3341832 -4.3342428 -4.3342495 -4.3347692 -4.3357849 -4.3379326 -4.3411489 -4.3448668 -4.3495579]]...]
INFO - root - 2017-12-05 18:27:16.847988: step 33710, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 79h:28m:45s remains)
INFO - root - 2017-12-05 18:27:26.297446: step 33720, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 75h:19m:53s remains)
INFO - root - 2017-12-05 18:27:35.655724: step 33730, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 77h:28m:53s remains)
INFO - root - 2017-12-05 18:27:44.929623: step 33740, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.984 sec/batch; 81h:38m:06s remains)
INFO - root - 2017-12-05 18:27:54.050761: step 33750, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.852 sec/batch; 70h:43m:59s remains)
INFO - root - 2017-12-05 18:28:03.456529: step 33760, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 77h:40m:58s remains)
INFO - root - 2017-12-05 18:28:12.915716: step 33770, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 76h:49m:54s remains)
INFO - root - 2017-12-05 18:28:22.351317: step 33780, loss = 2.06, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 84h:03m:30s remains)
INFO - root - 2017-12-05 18:28:31.803272: step 33790, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 77h:23m:45s remains)
INFO - root - 2017-12-05 18:28:41.251099: step 33800, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.973 sec/batch; 80h:44m:20s remains)
2017-12-05 18:28:42.055556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2884593 -4.2677827 -4.2497129 -4.2286983 -4.1959157 -4.1578455 -4.1445 -4.1672211 -4.1913033 -4.2129693 -4.239213 -4.2574167 -4.2598495 -4.253119 -4.2484245][-4.2956586 -4.28162 -4.2614818 -4.2311668 -4.1901364 -4.1400156 -4.11499 -4.1362634 -4.1717362 -4.2051187 -4.234374 -4.2543664 -4.2586875 -4.2553639 -4.2513952][-4.2944975 -4.2807112 -4.2574472 -4.2258244 -4.1817579 -4.1229811 -4.0880303 -4.1043029 -4.1507034 -4.1951365 -4.2295957 -4.2534709 -4.2611609 -4.2643585 -4.264029][-4.2797976 -4.2645006 -4.238966 -4.20918 -4.1692448 -4.1127682 -4.0787144 -4.0909805 -4.1358967 -4.1868572 -4.2294426 -4.2544794 -4.2609363 -4.2688265 -4.2764611][-4.2644734 -4.250536 -4.2249665 -4.1934233 -4.1553411 -4.1049333 -4.0739012 -4.0763459 -4.1061769 -4.1591492 -4.2129035 -4.2421336 -4.2491493 -4.2572684 -4.272964][-4.2584414 -4.25096 -4.2245989 -4.1870437 -4.1420593 -4.0897779 -4.0471911 -4.0162787 -4.0239654 -4.0952535 -4.1728735 -4.2163472 -4.2306046 -4.2409267 -4.26104][-4.262279 -4.2619081 -4.235209 -4.1907582 -4.1351619 -4.0711021 -3.9987218 -3.9129515 -3.8964815 -4.0084882 -4.1243567 -4.1879654 -4.2133412 -4.2293911 -4.2535439][-4.2727313 -4.2820349 -4.260828 -4.2138429 -4.1535168 -4.0803356 -3.9878223 -3.8743815 -3.8548584 -3.9879413 -4.11179 -4.1747656 -4.2010689 -4.2190032 -4.245255][-4.286694 -4.3001781 -4.2824616 -4.2322416 -4.1728582 -4.1076751 -4.0239363 -3.9335873 -3.9373994 -4.0494561 -4.1389947 -4.1802521 -4.1965156 -4.2108345 -4.2356253][-4.2926836 -4.3080978 -4.2929811 -4.2436705 -4.1856961 -4.1311321 -4.062181 -3.9995298 -4.0306215 -4.1175337 -4.1716661 -4.1957083 -4.202702 -4.2079887 -4.22953][-4.293777 -4.3128867 -4.3027611 -4.259326 -4.2038856 -4.1617436 -4.1016593 -4.049542 -4.0893135 -4.1580782 -4.1935635 -4.2131996 -4.2187424 -4.2199788 -4.2333531][-4.29777 -4.3171358 -4.3120451 -4.2768369 -4.22787 -4.1992717 -4.1501608 -4.1027679 -4.1326275 -4.1849456 -4.2100539 -4.2314339 -4.2386813 -4.2415576 -4.2483521][-4.3062859 -4.3206754 -4.3175488 -4.2918148 -4.2563491 -4.2412238 -4.2096748 -4.1705637 -4.1869011 -4.2203746 -4.2363553 -4.2539244 -4.26215 -4.2654977 -4.2684383][-4.3176718 -4.32481 -4.3229179 -4.3065004 -4.2834296 -4.2749929 -4.2576227 -4.2327628 -4.2391691 -4.2551923 -4.2642369 -4.2775664 -4.2856069 -4.287972 -4.2885561][-4.325839 -4.3277621 -4.3260727 -4.3160491 -4.3004193 -4.2916112 -4.282074 -4.2692924 -4.2695823 -4.2784109 -4.286643 -4.2973371 -4.3040748 -4.3066535 -4.307476]]...]
INFO - root - 2017-12-05 18:28:51.327387: step 33810, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.984 sec/batch; 81h:39m:07s remains)
INFO - root - 2017-12-05 18:29:00.453953: step 33820, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 71h:48m:42s remains)
INFO - root - 2017-12-05 18:29:10.004164: step 33830, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 76h:20m:19s remains)
INFO - root - 2017-12-05 18:29:19.429831: step 33840, loss = 2.04, batch loss = 1.99 (8.0 examples/sec; 0.995 sec/batch; 82h:31m:54s remains)
INFO - root - 2017-12-05 18:29:28.965696: step 33850, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 81h:24m:25s remains)
INFO - root - 2017-12-05 18:29:38.282061: step 33860, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.952 sec/batch; 78h:56m:12s remains)
INFO - root - 2017-12-05 18:29:47.707269: step 33870, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 80h:50m:48s remains)
INFO - root - 2017-12-05 18:29:57.077517: step 33880, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 78h:58m:04s remains)
INFO - root - 2017-12-05 18:30:06.325924: step 33890, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 78h:55m:53s remains)
INFO - root - 2017-12-05 18:30:15.685127: step 33900, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.917 sec/batch; 76h:03m:54s remains)
2017-12-05 18:30:16.476452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1971817 -4.2094932 -4.2250667 -4.2286348 -4.2239108 -4.2151957 -4.2098637 -4.2057509 -4.2027225 -4.2032986 -4.2073708 -4.2200565 -4.2230735 -4.2012548 -4.1658931][-4.1510167 -4.1635265 -4.18271 -4.1876111 -4.1807346 -4.1703057 -4.1660843 -4.161777 -4.1600847 -4.165545 -4.1738405 -4.1864758 -4.1954126 -4.1816063 -4.1497555][-4.124588 -4.1345754 -4.1566238 -4.1608267 -4.1518931 -4.1376381 -4.1277204 -4.1178317 -4.1159134 -4.1291809 -4.1445131 -4.1575928 -4.1737137 -4.1733084 -4.1517391][-4.1306448 -4.1394262 -4.16341 -4.1674514 -4.15603 -4.1389995 -4.1239252 -4.1075044 -4.1071777 -4.1283822 -4.1483393 -4.1600909 -4.1761165 -4.1823578 -4.1678839][-4.1466894 -4.1504531 -4.1704211 -4.1720629 -4.1547866 -4.1323338 -4.1080065 -4.0836067 -4.0891933 -4.125968 -4.1535759 -4.1641512 -4.17585 -4.1812587 -4.1676307][-4.1509018 -4.1449752 -4.1551075 -4.1508231 -4.1280279 -4.0987792 -4.0634074 -4.0309734 -4.0487309 -4.1067834 -4.1449718 -4.1556997 -4.1609454 -4.1626792 -4.1510339][-4.1517806 -4.1417818 -4.1413188 -4.12844 -4.0998392 -4.0612335 -4.0106955 -3.9634364 -3.9876723 -4.0667591 -4.1224375 -4.14127 -4.1442666 -4.146173 -4.1387191][-4.1363153 -4.1251755 -4.1180015 -4.1005077 -4.0682869 -4.0197577 -3.9497247 -3.8812523 -3.9065583 -4.0081377 -4.0860639 -4.1199379 -4.1312404 -4.1400943 -4.1381769][-4.1290803 -4.1188321 -4.1124935 -4.0964289 -4.0695109 -4.0231719 -3.9488363 -3.8754277 -3.8918214 -3.9897466 -4.0683079 -4.1094742 -4.1298938 -4.1462011 -4.1479974][-4.1481376 -4.1441989 -4.1435471 -4.1371732 -4.1259084 -4.0975204 -4.0436788 -3.9854052 -3.9794426 -4.0365572 -4.086462 -4.1164131 -4.1349154 -4.1515379 -4.1555729][-4.1807303 -4.1791272 -4.1817164 -4.1832385 -4.18363 -4.1725335 -4.1383247 -4.0932469 -4.0714631 -4.0935793 -4.1169205 -4.1375728 -4.153903 -4.1701202 -4.1761746][-4.2170291 -4.2159047 -4.2190614 -4.2228861 -4.22965 -4.2303009 -4.2116227 -4.1783495 -4.1535606 -4.1547422 -4.1591482 -4.1712308 -4.1840644 -4.1983819 -4.2051907][-4.2455168 -4.2457457 -4.2508707 -4.2560873 -4.2640986 -4.2702432 -4.2627077 -4.2424231 -4.2223196 -4.2154603 -4.2087207 -4.2105432 -4.2142568 -4.2208114 -4.2237258][-4.2740536 -4.2752767 -4.2818403 -4.2878518 -4.29552 -4.3024144 -4.3004866 -4.2894921 -4.2773404 -4.2690353 -4.2574253 -4.2494597 -4.2424631 -4.2362008 -4.2315812][-4.2981639 -4.2959018 -4.300025 -4.3028078 -4.3058491 -4.3086567 -4.3055153 -4.2981024 -4.2904172 -4.2821288 -4.2686405 -4.2566915 -4.2465525 -4.2308927 -4.2177434]]...]
INFO - root - 2017-12-05 18:30:25.620776: step 33910, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 79h:43m:05s remains)
INFO - root - 2017-12-05 18:30:34.809529: step 33920, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 71h:56m:19s remains)
INFO - root - 2017-12-05 18:30:44.009460: step 33930, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 80h:02m:50s remains)
INFO - root - 2017-12-05 18:30:53.379612: step 33940, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 76h:14m:10s remains)
INFO - root - 2017-12-05 18:31:02.552690: step 33950, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 73h:45m:39s remains)
INFO - root - 2017-12-05 18:31:11.835294: step 33960, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 70h:28m:24s remains)
INFO - root - 2017-12-05 18:31:21.281528: step 33970, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 76h:48m:32s remains)
INFO - root - 2017-12-05 18:31:30.635105: step 33980, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 76h:10m:56s remains)
INFO - root - 2017-12-05 18:31:39.927608: step 33990, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 77h:20m:22s remains)
INFO - root - 2017-12-05 18:31:49.332099: step 34000, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 75h:32m:24s remains)
2017-12-05 18:31:50.113206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2360005 -4.2225757 -4.2144675 -4.2134876 -4.2169313 -4.2244735 -4.2229557 -4.213686 -4.2041812 -4.1976657 -4.19755 -4.1938143 -4.182539 -4.1648111 -4.1533203][-4.2232785 -4.2047958 -4.1927967 -4.1871095 -4.1856461 -4.1920776 -4.1885972 -4.1770411 -4.1692553 -4.1669068 -4.1711445 -4.1689582 -4.1566448 -4.1388 -4.1303844][-4.2149882 -4.1900525 -4.1713743 -4.1582894 -4.1503811 -4.1541677 -4.1462111 -4.13154 -4.1283159 -4.1361527 -4.1493979 -4.1519513 -4.1457934 -4.1351094 -4.1325073][-4.2081609 -4.1759944 -4.1484923 -4.12659 -4.1121845 -4.1100693 -4.0919747 -4.0690522 -4.0739183 -4.0975022 -4.1239824 -4.1357889 -4.1379538 -4.1365943 -4.1378727][-4.2039251 -4.1639032 -4.1253238 -4.0922818 -4.0683064 -4.0529127 -4.0169311 -3.9814944 -4.0009565 -4.0494843 -4.0928931 -4.1165257 -4.1269503 -4.1322918 -4.1353674][-4.204 -4.1572 -4.1092534 -4.0647807 -4.0261812 -3.9867175 -3.9226782 -3.8698637 -3.9128966 -3.9986007 -4.063879 -4.09646 -4.112391 -4.1213827 -4.12722][-4.2032194 -4.1535397 -4.100791 -4.0491991 -3.9942594 -3.9268661 -3.8346086 -3.7690635 -3.8395529 -3.9594889 -4.0415831 -4.0816345 -4.0994163 -4.1066828 -4.113245][-4.1990542 -4.1510873 -4.1022868 -4.0542641 -3.9984672 -3.9281034 -3.8417268 -3.7917123 -3.8685994 -3.9810894 -4.0511074 -4.0832453 -4.0932364 -4.0938025 -4.0981665][-4.2030706 -4.1628685 -4.1249213 -4.0864954 -4.0399656 -3.9862075 -3.9299006 -3.9106383 -3.9768641 -4.0536418 -4.0950761 -4.1109157 -4.1109695 -4.107378 -4.1097836][-4.2215962 -4.1941257 -4.1700988 -4.1418109 -4.1028776 -4.0656958 -4.0377936 -4.0414567 -4.094924 -4.14055 -4.1597586 -4.1637321 -4.1569939 -4.1486936 -4.1468453][-4.2477884 -4.2305465 -4.2152734 -4.1946044 -4.1634994 -4.142108 -4.1358557 -4.1508393 -4.1903296 -4.217494 -4.2240467 -4.2218108 -4.2133522 -4.201086 -4.1955266][-4.271666 -4.2627563 -4.2519307 -4.2366452 -4.2140279 -4.2055941 -4.2129383 -4.2300487 -4.2551847 -4.2696724 -4.2710972 -4.2679033 -4.2593141 -4.2454324 -4.2377582][-4.2884345 -4.2855816 -4.2794447 -4.2698641 -4.256146 -4.2555718 -4.2662988 -4.2802949 -4.2942071 -4.3001847 -4.2993088 -4.2975707 -4.2909532 -4.2780814 -4.2671895][-4.3005328 -4.3011055 -4.29718 -4.2899561 -4.2817512 -4.2836475 -4.29312 -4.3036141 -4.3123174 -4.315774 -4.315908 -4.3157496 -4.3115392 -4.3034773 -4.2945619][-4.3040943 -4.306006 -4.3031316 -4.2982712 -4.29321 -4.2946362 -4.3008084 -4.3068819 -4.3119264 -4.3140788 -4.3138037 -4.3128381 -4.3102603 -4.3069448 -4.3029041]]...]
INFO - root - 2017-12-05 18:31:59.401346: step 34010, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.894 sec/batch; 74h:06m:48s remains)
INFO - root - 2017-12-05 18:32:08.759363: step 34020, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 78h:51m:06s remains)
INFO - root - 2017-12-05 18:32:18.290582: step 34030, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 74h:33m:42s remains)
INFO - root - 2017-12-05 18:32:27.691040: step 34040, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 81h:47m:33s remains)
INFO - root - 2017-12-05 18:32:37.040254: step 34050, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 73h:47m:02s remains)
INFO - root - 2017-12-05 18:32:46.548043: step 34060, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 81h:03m:54s remains)
INFO - root - 2017-12-05 18:32:55.885169: step 34070, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 77h:25m:22s remains)
INFO - root - 2017-12-05 18:33:05.402143: step 34080, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 73h:34m:14s remains)
INFO - root - 2017-12-05 18:33:14.760412: step 34090, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 75h:31m:55s remains)
INFO - root - 2017-12-05 18:33:24.144405: step 34100, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 71h:04m:03s remains)
2017-12-05 18:33:24.876294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3268986 -4.3282881 -4.3265743 -4.318624 -4.306787 -4.2945218 -4.2892509 -4.2892113 -4.2940283 -4.3025627 -4.3104787 -4.3149395 -4.317071 -4.31771 -4.3161035][-4.3355231 -4.3380737 -4.3376384 -4.3321567 -4.3246083 -4.3182192 -4.3166471 -4.3180523 -4.3233452 -4.3319397 -4.338429 -4.3399124 -4.3378234 -4.3334656 -4.3246655][-4.334187 -4.33624 -4.3343544 -4.3277316 -4.3200464 -4.3151522 -4.312233 -4.311708 -4.318284 -4.3310065 -4.3415971 -4.34482 -4.3419447 -4.3338032 -4.3172984][-4.3233428 -4.3222051 -4.316093 -4.3042035 -4.2917047 -4.2847943 -4.2780766 -4.2743754 -4.2817531 -4.299057 -4.315804 -4.324955 -4.325079 -4.3159361 -4.2909946][-4.3049541 -4.2985125 -4.2835574 -4.2582855 -4.2309284 -4.2110391 -4.1960454 -4.1920552 -4.2061796 -4.235218 -4.2642627 -4.2857642 -4.2933965 -4.2842426 -4.2470341][-4.2798505 -4.2654071 -4.2324233 -4.1817832 -4.1287637 -4.087357 -4.0601864 -4.0617409 -4.0948215 -4.1473956 -4.1966558 -4.2357578 -4.2551913 -4.2480388 -4.2046642][-4.2563066 -4.231019 -4.1792383 -4.1027861 -4.0196223 -3.9470029 -3.9005206 -3.9121206 -3.9745586 -4.0585089 -4.1304955 -4.1868439 -4.2174177 -4.217217 -4.1792288][-4.2392116 -4.2089882 -4.1501117 -4.0612593 -3.9589236 -3.8596752 -3.7955871 -3.8133841 -3.8962798 -4.0001321 -4.0846272 -4.1512403 -4.1907454 -4.2000289 -4.1767335][-4.2295365 -4.2042933 -4.1572175 -4.0852175 -4.0028992 -3.9220548 -3.8756328 -3.8968902 -3.9682283 -4.0541334 -4.1228242 -4.1761169 -4.2076726 -4.2167215 -4.2011805][-4.2327704 -4.2216253 -4.1980009 -4.1576872 -4.1121478 -4.0685081 -4.0490832 -4.0691156 -4.1149621 -4.1683483 -4.2085114 -4.2354 -4.2466063 -4.2420487 -4.2198544][-4.25115 -4.2513924 -4.2455144 -4.2304707 -4.212419 -4.1943107 -4.1887212 -4.2038622 -4.2299762 -4.2589583 -4.27846 -4.2878675 -4.2869229 -4.2737226 -4.247623][-4.275743 -4.2816215 -4.2860751 -4.2856727 -4.282311 -4.2754526 -4.2713547 -4.2772732 -4.2898149 -4.3050532 -4.3150764 -4.3188314 -4.3162093 -4.3038149 -4.2818437][-4.2907209 -4.2963185 -4.3040609 -4.3089609 -4.3107452 -4.3087764 -4.3052139 -4.3055754 -4.3100147 -4.3169041 -4.3218107 -4.32415 -4.3242388 -4.3177891 -4.3059959][-4.2894 -4.2921953 -4.3004885 -4.3079405 -4.3126717 -4.3138866 -4.3123684 -4.31112 -4.3110991 -4.3125119 -4.3140335 -4.3159986 -4.3184218 -4.3172541 -4.3147817][-4.27386 -4.2723508 -4.2807422 -4.2907438 -4.2992458 -4.3049378 -4.307682 -4.3084741 -4.3082733 -4.3074441 -4.3061838 -4.3052239 -4.3056726 -4.3056917 -4.308042]]...]
INFO - root - 2017-12-05 18:33:34.323270: step 34110, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 78h:56m:04s remains)
INFO - root - 2017-12-05 18:33:43.790323: step 34120, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 76h:27m:14s remains)
INFO - root - 2017-12-05 18:33:53.219719: step 34130, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 75h:57m:56s remains)
INFO - root - 2017-12-05 18:34:02.750466: step 34140, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 74h:53m:02s remains)
INFO - root - 2017-12-05 18:34:12.190282: step 34150, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 78h:01m:34s remains)
INFO - root - 2017-12-05 18:34:21.410608: step 34160, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 76h:57m:53s remains)
INFO - root - 2017-12-05 18:34:31.055463: step 34170, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 80h:09m:35s remains)
INFO - root - 2017-12-05 18:34:40.459041: step 34180, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 81h:31m:23s remains)
INFO - root - 2017-12-05 18:34:50.123285: step 34190, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 79h:30m:53s remains)
INFO - root - 2017-12-05 18:34:59.401161: step 34200, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 78h:24m:04s remains)
2017-12-05 18:35:00.168800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2182341 -4.2172532 -4.2293906 -4.248035 -4.2569609 -4.2545805 -4.2417321 -4.2243919 -4.2194252 -4.2363744 -4.2578297 -4.2710624 -4.2758269 -4.2812519 -4.2748251][-4.2200303 -4.2167149 -4.2268057 -4.2421627 -4.2520924 -4.253901 -4.2464371 -4.231205 -4.2227631 -4.2323503 -4.2535839 -4.2700839 -4.2832 -4.2888794 -4.27873][-4.2294226 -4.2288156 -4.2367949 -4.2452416 -4.2473822 -4.2433524 -4.2358994 -4.2228136 -4.2126136 -4.2119784 -4.2281556 -4.2465682 -4.2695589 -4.28259 -4.2736578][-4.2146797 -4.2204952 -4.2301464 -4.234158 -4.2263474 -4.208971 -4.1916466 -4.1763659 -4.17055 -4.1740479 -4.1929326 -4.2136049 -4.2444711 -4.2661996 -4.2646532][-4.1745605 -4.1840105 -4.1960783 -4.1976676 -4.1814046 -4.1492872 -4.1171174 -4.0961132 -4.0989156 -4.1193314 -4.1529455 -4.1808782 -4.2184353 -4.2479892 -4.2514353][-4.1385107 -4.1451507 -4.1569958 -4.1527839 -4.1290007 -4.0838408 -4.0352726 -4.004045 -4.0162 -4.0583782 -4.1111379 -4.1536269 -4.200563 -4.2372766 -4.2425766][-4.1288095 -4.1304789 -4.1388221 -4.1302094 -4.1010218 -4.0481858 -3.9843287 -3.9376988 -3.9501607 -4.010766 -4.08107 -4.1361532 -4.1863017 -4.2197852 -4.2240815][-4.1500063 -4.145278 -4.1512113 -4.1452026 -4.1202626 -4.0730219 -4.006144 -3.9520333 -3.954011 -4.0107927 -4.0778179 -4.1287584 -4.1707764 -4.1967258 -4.2024455][-4.1829786 -4.1753654 -4.1846037 -4.18788 -4.17333 -4.1376104 -4.0821023 -4.0380139 -4.0320692 -4.0669351 -4.1085443 -4.1411142 -4.1681404 -4.1864772 -4.195446][-4.1993837 -4.19305 -4.2119141 -4.2274456 -4.2266049 -4.2055669 -4.1660352 -4.1346197 -4.1247692 -4.1376028 -4.1558886 -4.1709228 -4.1841111 -4.1929035 -4.2022295][-4.2011657 -4.1914153 -4.2135658 -4.2354865 -4.2463546 -4.24291 -4.2215819 -4.2006927 -4.1929221 -4.1971769 -4.2032785 -4.2088737 -4.2131805 -4.2119474 -4.2175436][-4.1997504 -4.1840448 -4.2032561 -4.2239909 -4.2402349 -4.2495489 -4.2445245 -4.2339048 -4.2299027 -4.2328486 -4.2319975 -4.2301736 -4.2277865 -4.2199383 -4.2188568][-4.2122355 -4.1944294 -4.2053437 -4.2165146 -4.2278347 -4.2389421 -4.2446041 -4.2444143 -4.2449965 -4.2483745 -4.2440882 -4.2350636 -4.2226954 -4.2095046 -4.2044678][-4.23643 -4.2187552 -4.2202754 -4.21956 -4.2254186 -4.2357211 -4.247973 -4.2550507 -4.2609253 -4.2640367 -4.2534351 -4.2342825 -4.209815 -4.1903653 -4.182179][-4.2569938 -4.2378364 -4.2306342 -4.2228603 -4.2273307 -4.2392688 -4.2536173 -4.2647328 -4.2742853 -4.2763352 -4.2610269 -4.2353239 -4.2036438 -4.1811447 -4.170033]]...]
INFO - root - 2017-12-05 18:35:09.729262: step 34210, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 81h:20m:32s remains)
INFO - root - 2017-12-05 18:35:18.932447: step 34220, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 77h:44m:32s remains)
INFO - root - 2017-12-05 18:35:28.401382: step 34230, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 77h:29m:34s remains)
INFO - root - 2017-12-05 18:35:37.807075: step 34240, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.011 sec/batch; 83h:46m:24s remains)
INFO - root - 2017-12-05 18:35:47.235743: step 34250, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 78h:42m:52s remains)
INFO - root - 2017-12-05 18:35:56.745678: step 34260, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 81h:42m:06s remains)
INFO - root - 2017-12-05 18:36:06.222241: step 34270, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 76h:52m:12s remains)
INFO - root - 2017-12-05 18:36:15.624010: step 34280, loss = 2.05, batch loss = 2.00 (7.9 examples/sec; 1.018 sec/batch; 84h:20m:00s remains)
INFO - root - 2017-12-05 18:36:24.981957: step 34290, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 82h:29m:50s remains)
INFO - root - 2017-12-05 18:36:34.544486: step 34300, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 75h:47m:54s remains)
2017-12-05 18:36:35.272560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29047 -4.2970648 -4.2887416 -4.274509 -4.2643776 -4.2620206 -4.2704897 -4.2878923 -4.300992 -4.3026919 -4.2937751 -4.28504 -4.2842665 -4.2875042 -4.289299][-4.2858977 -4.2817125 -4.25943 -4.2316561 -4.2130742 -4.2059784 -4.2148223 -4.2433367 -4.2708406 -4.2886076 -4.2893128 -4.2820873 -4.2798147 -4.2832837 -4.287][-4.2572174 -4.2402306 -4.2035775 -4.162787 -4.135222 -4.1265588 -4.138618 -4.1772971 -4.22056 -4.2571759 -4.2734776 -4.2735605 -4.2725081 -4.2772121 -4.2835131][-4.2155261 -4.1948428 -4.159616 -4.1229768 -4.099556 -4.0936975 -4.1027951 -4.1331267 -4.176302 -4.21815 -4.2445397 -4.253778 -4.2576466 -4.2666988 -4.2787495][-4.1819291 -4.1691413 -4.1513596 -4.1313019 -4.1189246 -4.1138377 -4.1118765 -4.1210446 -4.146698 -4.1792183 -4.2073703 -4.2254572 -4.2405515 -4.2590494 -4.2769275][-4.1662564 -4.1603932 -4.1582479 -4.15143 -4.1427593 -4.1305881 -4.1147413 -4.1054487 -4.1139965 -4.13574 -4.1596932 -4.1850533 -4.2140465 -4.2420382 -4.2634864][-4.1743846 -4.1688108 -4.1712413 -4.1689954 -4.1579871 -4.1334629 -4.1068473 -4.0886974 -4.0905242 -4.1011052 -4.11464 -4.1414247 -4.1770535 -4.2102785 -4.2324252][-4.1994438 -4.1851211 -4.1845078 -4.1827807 -4.173409 -4.143796 -4.1130872 -4.0908127 -4.0852146 -4.0831037 -4.0848393 -4.1082 -4.14156 -4.1667233 -4.1775708][-4.2314024 -4.2020969 -4.1928821 -4.1915832 -4.1881747 -4.1655183 -4.1381311 -4.1180887 -4.1046143 -4.089222 -4.0790286 -4.0925341 -4.1105638 -4.1155639 -4.1072679][-4.2570581 -4.2172985 -4.2005024 -4.2012105 -4.2041926 -4.1942039 -4.1765847 -4.1617956 -4.1420836 -4.112926 -4.086833 -4.0786839 -4.071466 -4.0544429 -4.0371637][-4.2723784 -4.2279916 -4.2032332 -4.2030721 -4.2117147 -4.2140484 -4.2063231 -4.1933937 -4.17144 -4.1374693 -4.1005569 -4.0744371 -4.0436106 -4.0133963 -4.0019712][-4.2810855 -4.239193 -4.2112589 -4.2095075 -4.2226586 -4.2368293 -4.2378669 -4.2236595 -4.1986961 -4.1621871 -4.1254563 -4.0995817 -4.0700564 -4.0478797 -4.047513][-4.2883148 -4.2538018 -4.230093 -4.2296734 -4.2456374 -4.2633395 -4.2675047 -4.2554221 -4.2322087 -4.2003956 -4.1683631 -4.1456418 -4.1267509 -4.1179042 -4.1249161][-4.2940803 -4.2682977 -4.2500777 -4.2496843 -4.2606354 -4.2710724 -4.2727308 -4.2628622 -4.2494888 -4.2318563 -4.209197 -4.1894631 -4.1767082 -4.1770353 -4.1895795][-4.2934537 -4.273778 -4.2588391 -4.2558336 -4.2572508 -4.2566094 -4.2532492 -4.2474914 -4.2466 -4.2488852 -4.2446518 -4.2365766 -4.2300205 -4.2336073 -4.247334]]...]
INFO - root - 2017-12-05 18:36:44.654307: step 34310, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.956 sec/batch; 79h:11m:34s remains)
INFO - root - 2017-12-05 18:36:53.903485: step 34320, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 79h:15m:40s remains)
INFO - root - 2017-12-05 18:37:03.133893: step 34330, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 76h:09m:46s remains)
INFO - root - 2017-12-05 18:37:12.712354: step 34340, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.979 sec/batch; 81h:03m:19s remains)
INFO - root - 2017-12-05 18:37:22.088354: step 34350, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 77h:44m:04s remains)
INFO - root - 2017-12-05 18:37:31.346823: step 34360, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 71h:52m:58s remains)
INFO - root - 2017-12-05 18:37:40.489769: step 34370, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 76h:01m:31s remains)
INFO - root - 2017-12-05 18:37:49.966672: step 34380, loss = 2.10, batch loss = 2.05 (8.4 examples/sec; 0.949 sec/batch; 78h:35m:41s remains)
INFO - root - 2017-12-05 18:37:59.185418: step 34390, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 77h:09m:08s remains)
INFO - root - 2017-12-05 18:38:08.597152: step 34400, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 74h:00m:24s remains)
2017-12-05 18:38:09.436449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.220561 -4.2034631 -4.1849952 -4.1788607 -4.1890678 -4.1946282 -4.1983061 -4.2030635 -4.2072358 -4.209548 -4.2106605 -4.2101254 -4.2092328 -4.2015657 -4.1981564][-4.2398591 -4.2183151 -4.1992078 -4.1946192 -4.2068715 -4.2120261 -4.215477 -4.2198744 -4.2213798 -4.2203722 -4.219357 -4.218554 -4.2182989 -4.2152839 -4.2176995][-4.2670918 -4.2456746 -4.2270617 -4.222281 -4.233398 -4.2353687 -4.237639 -4.2421269 -4.2403007 -4.2347412 -4.2321415 -4.2309961 -4.2312074 -4.2344618 -4.2417932][-4.2841535 -4.2635612 -4.2438722 -4.2362304 -4.2442751 -4.241128 -4.2410097 -4.2443008 -4.2373605 -4.2264194 -4.2223344 -4.2226863 -4.2242308 -4.2349277 -4.2484679][-4.2949238 -4.2708378 -4.2466249 -4.2333093 -4.23415 -4.219101 -4.2100368 -4.209446 -4.2004061 -4.1894765 -4.1843281 -4.1861682 -4.1931763 -4.2143269 -4.2385149][-4.2903504 -4.2532911 -4.2111039 -4.1804738 -4.1641612 -4.1299863 -4.1122766 -4.1172066 -4.1159568 -4.1129484 -4.1130052 -4.119503 -4.1348944 -4.1698589 -4.2078576][-4.265398 -4.2016134 -4.1292195 -4.071269 -4.0298533 -3.9695539 -3.9465895 -3.9744966 -3.9973254 -4.0115275 -4.0270233 -4.0484576 -4.0767441 -4.124588 -4.1753149][-4.2227426 -4.1316867 -4.028296 -3.9411781 -3.8786645 -3.8018408 -3.7876108 -3.8510041 -3.9094772 -3.9555366 -4.0022449 -4.0440054 -4.0790577 -4.1291542 -4.181222][-4.1942754 -4.0955052 -3.9872556 -3.8996274 -3.8517809 -3.8028028 -3.8136003 -3.8920164 -3.9646842 -4.0173035 -4.0718136 -4.115777 -4.1448727 -4.1825156 -4.2201052][-4.2182817 -4.1408973 -4.0589414 -3.9973991 -3.9771712 -3.9632792 -3.9878271 -4.0548868 -4.11433 -4.1503558 -4.1867466 -4.2171693 -4.2332993 -4.2506828 -4.2657208][-4.2621408 -4.2126513 -4.1642079 -4.1306586 -4.1313243 -4.13854 -4.1645007 -4.2108126 -4.248641 -4.2652192 -4.28171 -4.2957358 -4.2986765 -4.2982073 -4.2967873][-4.2976408 -4.2710381 -4.2465587 -4.2307792 -4.2424603 -4.2561517 -4.27601 -4.3040867 -4.3245425 -4.329833 -4.3338141 -4.3372135 -4.3336453 -4.3246131 -4.3147521][-4.3092775 -4.2993035 -4.2885075 -4.2824283 -4.2978816 -4.3104467 -4.3232527 -4.3374782 -4.3476019 -4.35003 -4.3516426 -4.3485632 -4.3387918 -4.3237915 -4.3064852][-4.2960811 -4.2934275 -4.2872624 -4.2865224 -4.3028493 -4.311882 -4.3189335 -4.3263931 -4.3315349 -4.33404 -4.3349047 -4.3279986 -4.3137836 -4.2934728 -4.2720647][-4.2627268 -4.2575212 -4.2499261 -4.251915 -4.2695503 -4.2772083 -4.2822285 -4.287529 -4.2916322 -4.2936106 -4.2941689 -4.2869153 -4.2722821 -4.250731 -4.2297769]]...]
INFO - root - 2017-12-05 18:38:18.610971: step 34410, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 77h:29m:26s remains)
INFO - root - 2017-12-05 18:38:27.853971: step 34420, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 75h:54m:18s remains)
INFO - root - 2017-12-05 18:38:37.172638: step 34430, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 76h:54m:36s remains)
INFO - root - 2017-12-05 18:38:46.398811: step 34440, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 73h:03m:29s remains)
INFO - root - 2017-12-05 18:38:55.617458: step 34450, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 80h:07m:58s remains)
INFO - root - 2017-12-05 18:39:05.100721: step 34460, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.881 sec/batch; 72h:56m:09s remains)
INFO - root - 2017-12-05 18:39:14.438502: step 34470, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 79h:19m:26s remains)
INFO - root - 2017-12-05 18:39:23.572745: step 34480, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 79h:36m:34s remains)
INFO - root - 2017-12-05 18:39:32.804786: step 34490, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 78h:39m:56s remains)
INFO - root - 2017-12-05 18:39:42.203069: step 34500, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 78h:17m:24s remains)
2017-12-05 18:39:43.006421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.302897 -4.2444825 -4.1920815 -4.1533542 -4.1272917 -4.11379 -4.1173072 -4.1449561 -4.1707916 -4.1930962 -4.1981711 -4.1958289 -4.1879125 -4.1764822 -4.1792679][-4.3065224 -4.2404571 -4.1780219 -4.1335397 -4.1077929 -4.0927472 -4.0844336 -4.1047025 -4.1365147 -4.17583 -4.1992235 -4.2135391 -4.2207994 -4.2178593 -4.2173548][-4.3092184 -4.2375994 -4.165576 -4.1170721 -4.0923347 -4.0810814 -4.0673246 -4.0764832 -4.1104794 -4.158998 -4.194397 -4.21574 -4.2331562 -4.2382 -4.2418523][-4.30709 -4.2308626 -4.1488304 -4.0953727 -4.0707226 -4.0699773 -4.0651865 -4.0741172 -4.1080785 -4.1564779 -4.1950645 -4.2164 -4.2340956 -4.2428865 -4.2510643][-4.2961764 -4.2134142 -4.1206083 -4.0542207 -4.0221748 -4.0327768 -4.0489788 -4.0721893 -4.1133728 -4.1582704 -4.1947379 -4.2124181 -4.2247572 -4.2331476 -4.2452164][-4.280592 -4.1904254 -4.083611 -3.9939778 -3.9359393 -3.9452534 -3.984854 -4.0364943 -4.0996504 -4.1495242 -4.1863289 -4.2035275 -4.2101226 -4.2164888 -4.2297306][-4.2626853 -4.1656847 -4.0439954 -3.9254222 -3.8271227 -3.8188589 -3.8698082 -3.9535599 -4.0485411 -4.1127148 -4.1594439 -4.1836305 -4.1917572 -4.197361 -4.2088056][-4.2532105 -4.1556797 -4.0286613 -3.8960218 -3.7766438 -3.7576129 -3.8159046 -3.9135258 -4.0250592 -4.1007051 -4.15507 -4.1833272 -4.1894879 -4.1905255 -4.1977234][-4.265121 -4.1800542 -4.0682197 -3.9488795 -3.841306 -3.8254025 -3.882139 -3.9707961 -4.0723853 -4.1417336 -4.1902218 -4.2159028 -4.216259 -4.2100248 -4.2098227][-4.2892513 -4.2268305 -4.1416254 -4.0493636 -3.9709504 -3.9610286 -4.0060744 -4.0726805 -4.1507277 -4.2042522 -4.2414784 -4.2603817 -4.2591286 -4.2503996 -4.2436414][-4.3119659 -4.2689028 -4.2062917 -4.1425858 -4.0968623 -4.0964003 -4.1280909 -4.1704068 -4.2212782 -4.2584214 -4.2863131 -4.2992954 -4.2988396 -4.2905946 -4.2799621][-4.3232088 -4.2950945 -4.2524304 -4.2136607 -4.1946254 -4.2010159 -4.2178116 -4.2391415 -4.2676868 -4.2886868 -4.3036118 -4.3109331 -4.3118863 -4.3062005 -4.2967505][-4.3250427 -4.3080168 -4.2804041 -4.2572894 -4.2516756 -4.2596054 -4.2687359 -4.2770119 -4.288476 -4.2962561 -4.3021374 -4.3047423 -4.3061886 -4.3029437 -4.2964554][-4.3227077 -4.3123178 -4.2927442 -4.2719326 -4.2676086 -4.2752609 -4.28143 -4.2830262 -4.2852793 -4.2880077 -4.2923346 -4.2957516 -4.2986817 -4.2985349 -4.2960672][-4.3171949 -4.3078389 -4.2906475 -4.2681637 -4.2619023 -4.2685843 -4.2738667 -4.2751112 -4.2754903 -4.277699 -4.2825894 -4.2870665 -4.2906594 -4.2934947 -4.295732]]...]
INFO - root - 2017-12-05 18:39:52.339949: step 34510, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 78h:58m:48s remains)
INFO - root - 2017-12-05 18:40:01.703501: step 34520, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 78h:08m:31s remains)
INFO - root - 2017-12-05 18:40:11.000251: step 34530, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 76h:21m:54s remains)
INFO - root - 2017-12-05 18:40:20.279336: step 34540, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 77h:49m:04s remains)
INFO - root - 2017-12-05 18:40:29.776531: step 34550, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 77h:40m:34s remains)
INFO - root - 2017-12-05 18:40:39.205118: step 34560, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 76h:51m:27s remains)
INFO - root - 2017-12-05 18:40:48.442485: step 34570, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 78h:52m:05s remains)
INFO - root - 2017-12-05 18:40:57.777962: step 34580, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 79h:03m:24s remains)
INFO - root - 2017-12-05 18:41:07.176788: step 34590, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 80h:13m:11s remains)
INFO - root - 2017-12-05 18:41:16.299352: step 34600, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 82h:02m:15s remains)
2017-12-05 18:41:17.115806: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2798357 -4.2887497 -4.2959914 -4.2965059 -4.2907367 -4.28627 -4.2870936 -4.2944579 -4.3090072 -4.316802 -4.3099871 -4.2986522 -4.2882438 -4.2799287 -4.2752991][-4.2982478 -4.3103323 -4.3172083 -4.3157349 -4.3072867 -4.295156 -4.2854919 -4.2844877 -4.2922564 -4.2944407 -4.2890978 -4.2874069 -4.2821321 -4.2656608 -4.2441034][-4.3229647 -4.33375 -4.33563 -4.3285513 -4.3152471 -4.2923007 -4.2679782 -4.2563457 -4.2605357 -4.2621326 -4.2574425 -4.2624712 -4.2588873 -4.2311382 -4.1853251][-4.3437114 -4.34906 -4.3447938 -4.3301983 -4.3052797 -4.2653089 -4.2205629 -4.1926208 -4.1940417 -4.2003117 -4.2025056 -4.2198062 -4.2223706 -4.187129 -4.1186681][-4.35489 -4.3555837 -4.3420477 -4.3102946 -4.2601933 -4.1891608 -4.11279 -4.0656977 -4.0754032 -4.1063566 -4.1356521 -4.1810541 -4.1993814 -4.171216 -4.104259][-4.3551331 -4.3478742 -4.3183246 -4.2612271 -4.1809955 -4.0728211 -3.9538414 -3.8892958 -3.9288008 -4.0117073 -4.0856504 -4.1590428 -4.1952033 -4.1877036 -4.1501846][-4.337297 -4.3241086 -4.2829452 -4.2092915 -4.1069355 -3.9712844 -3.8280656 -3.7709131 -3.8569887 -3.9897704 -4.0934772 -4.1730242 -4.217463 -4.22717 -4.2169456][-4.3056183 -4.2945032 -4.2567854 -4.1848016 -4.08978 -3.9767504 -3.8655548 -3.8369386 -3.9254332 -4.0488119 -4.1445785 -4.208652 -4.2406864 -4.2483983 -4.2480645][-4.2557755 -4.2510324 -4.2333097 -4.1933117 -4.1407809 -4.0818086 -4.0194478 -4.0039825 -4.0565925 -4.1333451 -4.1955028 -4.2335954 -4.2461443 -4.2449331 -4.2510195][-4.2022142 -4.2033858 -4.2071671 -4.2028408 -4.1928139 -4.178226 -4.1476479 -4.1304827 -4.1493826 -4.1874981 -4.2237868 -4.244657 -4.2470331 -4.2466736 -4.2595072][-4.164957 -4.1611962 -4.1729774 -4.1856875 -4.2030191 -4.2188468 -4.2103581 -4.1967549 -4.2008305 -4.2229133 -4.2475481 -4.2604847 -4.2634854 -4.2665114 -4.2814522][-4.1527944 -4.138392 -4.1489377 -4.1697292 -4.2024512 -4.2392535 -4.253943 -4.2512321 -4.2522392 -4.2663321 -4.2828236 -4.29187 -4.2951045 -4.2977104 -4.30817][-4.1745791 -4.1560273 -4.1630855 -4.1872249 -4.2254395 -4.2712507 -4.2977128 -4.3027182 -4.3017931 -4.3067951 -4.3137341 -4.3170519 -4.31616 -4.31449 -4.3188004][-4.2273145 -4.2147951 -4.2171326 -4.2336192 -4.2627063 -4.2989554 -4.32428 -4.3327413 -4.3295422 -4.3252177 -4.3223157 -4.3195763 -4.3152895 -4.3089371 -4.3075466][-4.28085 -4.2753425 -4.2721715 -4.2744293 -4.2861423 -4.3063197 -4.322453 -4.327992 -4.3203111 -4.3091426 -4.3005881 -4.2977161 -4.2938542 -4.2868037 -4.28235]]...]
INFO - root - 2017-12-05 18:41:26.506714: step 34610, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 78h:55m:15s remains)
INFO - root - 2017-12-05 18:41:35.735321: step 34620, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 76h:22m:33s remains)
INFO - root - 2017-12-05 18:41:45.028934: step 34630, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 79h:03m:54s remains)
INFO - root - 2017-12-05 18:41:54.311136: step 34640, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 81h:13m:31s remains)
INFO - root - 2017-12-05 18:42:03.703027: step 34650, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 77h:22m:27s remains)
INFO - root - 2017-12-05 18:42:12.890289: step 34660, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 76h:10m:15s remains)
INFO - root - 2017-12-05 18:42:22.192252: step 34670, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 76h:43m:48s remains)
INFO - root - 2017-12-05 18:42:31.484939: step 34680, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.920 sec/batch; 76h:08m:57s remains)
INFO - root - 2017-12-05 18:42:40.901602: step 34690, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 76h:52m:56s remains)
INFO - root - 2017-12-05 18:42:50.235113: step 34700, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 77h:46m:12s remains)
2017-12-05 18:42:50.988910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3334212 -4.3320012 -4.3295527 -4.3267717 -4.3239155 -4.3223882 -4.3224583 -4.3239951 -4.3250413 -4.3249083 -4.32471 -4.3250852 -4.3263364 -4.3285508 -4.3308568][-4.3337193 -4.3318644 -4.3287725 -4.3245 -4.3195615 -4.3152075 -4.3126559 -4.312047 -4.3124523 -4.3134875 -4.3150225 -4.3162103 -4.3174047 -4.3203163 -4.3242345][-4.32678 -4.3213415 -4.3152976 -4.3095303 -4.3043833 -4.2988014 -4.2939162 -4.2903237 -4.2893524 -4.29141 -4.2951517 -4.2983904 -4.3021932 -4.3073516 -4.3132668][-4.3060713 -4.2942181 -4.28288 -4.2747121 -4.2700663 -4.2638073 -4.2568321 -4.2504487 -4.24855 -4.2512751 -4.2580581 -4.2652054 -4.2731 -4.2825904 -4.2923884][-4.2732148 -4.2529449 -4.2367859 -4.226613 -4.2213926 -4.2104053 -4.1948972 -4.1833167 -4.1840596 -4.1930594 -4.205689 -4.2177787 -4.2307968 -4.2460594 -4.2614713][-4.2347269 -4.2074051 -4.1903105 -4.1811724 -4.175065 -4.1529083 -4.1178823 -4.0916595 -4.0966229 -4.11965 -4.1428547 -4.160316 -4.1765561 -4.1954436 -4.2161865][-4.1991453 -4.1669126 -4.1512437 -4.1451421 -4.1383128 -4.1043086 -4.0467005 -4.0007839 -4.0060964 -4.0459962 -4.0852017 -4.11166 -4.1317949 -4.1530619 -4.1766205][-4.187304 -4.1543951 -4.1383862 -4.1330261 -4.12491 -4.08648 -4.02225 -3.9685626 -3.9697676 -4.0164075 -4.0676603 -4.1043606 -4.1291656 -4.1492414 -4.1684241][-4.2039876 -4.1791353 -4.162354 -4.1547575 -4.1459761 -4.1184649 -4.0788226 -4.048378 -4.049005 -4.0797071 -4.1180873 -4.1490188 -4.1683979 -4.1814561 -4.1922784][-4.2317071 -4.2196436 -4.2082028 -4.2040682 -4.2006717 -4.1881227 -4.1745477 -4.1659489 -4.1674891 -4.1792502 -4.1947765 -4.2080388 -4.2132783 -4.2159529 -4.2185893][-4.2444277 -4.2434759 -4.2407818 -4.2432165 -4.2456193 -4.2420926 -4.239161 -4.2368641 -4.2348914 -4.2339873 -4.2353473 -4.2358828 -4.23176 -4.2266808 -4.2231417][-4.2333355 -4.2378349 -4.2408466 -4.2455726 -4.2482686 -4.24829 -4.2480283 -4.2444096 -4.2373481 -4.2307367 -4.2283058 -4.226963 -4.2225308 -4.2164469 -4.2111506][-4.2107229 -4.2154975 -4.2182546 -4.2202544 -4.2200036 -4.2205186 -4.2227817 -4.2188077 -4.208127 -4.1992345 -4.1985936 -4.2005553 -4.2018471 -4.2020283 -4.2008171][-4.2000866 -4.2017579 -4.2022648 -4.2002273 -4.1964717 -4.1962461 -4.2006516 -4.1985984 -4.188652 -4.1815639 -4.1823783 -4.1859279 -4.1913075 -4.1972513 -4.2025938][-4.2106414 -4.20939 -4.2089157 -4.2040415 -4.197084 -4.1943169 -4.1977768 -4.1966796 -4.1914587 -4.1883144 -4.1897359 -4.1931415 -4.1989126 -4.20751 -4.2166815]]...]
INFO - root - 2017-12-05 18:43:00.325320: step 34710, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 79h:27m:35s remains)
INFO - root - 2017-12-05 18:43:09.728808: step 34720, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.012 sec/batch; 83h:40m:58s remains)
INFO - root - 2017-12-05 18:43:19.142761: step 34730, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 81h:23m:46s remains)
INFO - root - 2017-12-05 18:43:28.656582: step 34740, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.945 sec/batch; 78h:10m:53s remains)
INFO - root - 2017-12-05 18:43:37.958520: step 34750, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 78h:52m:18s remains)
INFO - root - 2017-12-05 18:43:47.256563: step 34760, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 77h:28m:12s remains)
INFO - root - 2017-12-05 18:43:56.474248: step 34770, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 79h:26m:46s remains)
INFO - root - 2017-12-05 18:44:06.000118: step 34780, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 76h:42m:35s remains)
INFO - root - 2017-12-05 18:44:15.287040: step 34790, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 77h:05m:02s remains)
INFO - root - 2017-12-05 18:44:24.606253: step 34800, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 78h:59m:01s remains)
2017-12-05 18:44:25.336205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1214924 -4.1144071 -4.0884843 -4.0588875 -4.0519986 -4.0803757 -4.1092486 -4.1154962 -4.1328 -4.1657329 -4.1914182 -4.1991706 -4.1950259 -4.1709552 -4.1341071][-4.1333418 -4.1313734 -4.1064425 -4.0713606 -4.0574079 -4.0849333 -4.1202946 -4.1405449 -4.1663876 -4.1939964 -4.2116117 -4.2065287 -4.1896963 -4.156024 -4.1170783][-4.1804109 -4.1758013 -4.1518044 -4.1151018 -4.0917439 -4.1060243 -4.1336675 -4.1536236 -4.18236 -4.2119312 -4.2291784 -4.2197108 -4.1961522 -4.1655645 -4.136652][-4.2174416 -4.2110023 -4.1874437 -4.1516528 -4.1209292 -4.1153951 -4.1260376 -4.1428308 -4.1773634 -4.2153587 -4.2420497 -4.239624 -4.2190566 -4.1933823 -4.167315][-4.228 -4.2193828 -4.1945353 -4.1586013 -4.1168051 -4.0877686 -4.0802894 -4.1011658 -4.1503086 -4.2027221 -4.2425313 -4.250186 -4.2355995 -4.2151613 -4.1896467][-4.2077169 -4.1953645 -4.167099 -4.1256337 -4.0662832 -4.0029922 -3.9759777 -4.0139518 -4.0925913 -4.1602354 -4.2113285 -4.2322807 -4.2313676 -4.2228427 -4.20252][-4.1622572 -4.1398368 -4.1061659 -4.0537691 -3.9609406 -3.8487234 -3.8124652 -3.8996873 -4.0188079 -4.10055 -4.1599731 -4.1950107 -4.2130313 -4.2179656 -4.2037368][-4.1322613 -4.0949316 -4.064292 -4.014564 -3.90602 -3.7774925 -3.7600541 -3.8745813 -3.9972048 -4.0740061 -4.1274524 -4.1682739 -4.1980815 -4.2089915 -4.1989269][-4.134984 -4.0931444 -4.0765152 -4.0477762 -3.966156 -3.8760045 -3.8777428 -3.9615529 -4.043746 -4.0975518 -4.1386294 -4.1769567 -4.206501 -4.2145281 -4.20345][-4.12718 -4.0868225 -4.0820041 -4.0751047 -4.0328383 -3.9845457 -3.9906764 -4.038816 -4.0857868 -4.1221914 -4.158957 -4.1949606 -4.2180171 -4.2210951 -4.2087469][-4.1141672 -4.0777264 -4.0778437 -4.0833645 -4.0658488 -4.046361 -4.0573087 -4.0890503 -4.1198616 -4.1457109 -4.1780143 -4.2083168 -4.2238889 -4.2216892 -4.2078314][-4.1299162 -4.0987182 -4.0965977 -4.1043596 -4.1000309 -4.0944028 -4.1038871 -4.1273255 -4.1512709 -4.1722426 -4.1982884 -4.2209945 -4.2292852 -4.2242842 -4.2124634][-4.166945 -4.1374426 -4.1266303 -4.1303453 -4.1336207 -4.1346264 -4.1451092 -4.1644969 -4.1847157 -4.2009449 -4.2171721 -4.2301326 -4.2328873 -4.2295551 -4.2241888][-4.2133255 -4.1858559 -4.1680517 -4.1665988 -4.1718082 -4.1770258 -4.1865807 -4.1995106 -4.213212 -4.2248664 -4.2350321 -4.241004 -4.2427468 -4.2439923 -4.2445621][-4.2498846 -4.2314105 -4.217556 -4.2137337 -4.2169886 -4.2214379 -4.2276988 -4.2329946 -4.2391286 -4.2458091 -4.2521729 -4.2563014 -4.2585597 -4.26099 -4.2631335]]...]
INFO - root - 2017-12-05 18:44:34.619124: step 34810, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 77h:29m:18s remains)
INFO - root - 2017-12-05 18:44:44.026411: step 34820, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 79h:37m:16s remains)
INFO - root - 2017-12-05 18:44:53.504817: step 34830, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 78h:13m:30s remains)
INFO - root - 2017-12-05 18:45:02.825462: step 34840, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 75h:55m:33s remains)
INFO - root - 2017-12-05 18:45:12.425010: step 34850, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 78h:12m:08s remains)
INFO - root - 2017-12-05 18:45:21.740481: step 34860, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 80h:24m:25s remains)
INFO - root - 2017-12-05 18:45:31.133033: step 34870, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 69h:39m:07s remains)
INFO - root - 2017-12-05 18:45:40.608264: step 34880, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 76h:51m:24s remains)
INFO - root - 2017-12-05 18:45:50.031191: step 34890, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 70h:04m:34s remains)
INFO - root - 2017-12-05 18:45:59.439121: step 34900, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.990 sec/batch; 81h:49m:50s remains)
2017-12-05 18:46:00.161319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2028913 -4.1562061 -4.1143613 -4.0918918 -4.1018639 -4.1383848 -4.1729951 -4.1815577 -4.1529493 -4.0995674 -4.0690546 -4.0915918 -4.1380134 -4.1895041 -4.2392869][-4.2243781 -4.1829863 -4.1383448 -4.1092649 -4.1176071 -4.15132 -4.1804013 -4.1835971 -4.1465597 -4.0809135 -4.0405717 -4.0632529 -4.1156054 -4.1780238 -4.2368197][-4.2512207 -4.2119756 -4.1614556 -4.1223965 -4.1250858 -4.1571164 -4.187036 -4.1939354 -4.1611495 -4.0989532 -4.0611882 -4.0788412 -4.1299539 -4.1964107 -4.25759][-4.2592893 -4.2199497 -4.1637311 -4.1181159 -4.1102667 -4.1349053 -4.169929 -4.1912937 -4.1787462 -4.1344423 -4.1061225 -4.115603 -4.1590261 -4.2219453 -4.279233][-4.2491755 -4.2071786 -4.14645 -4.0966225 -4.0782232 -4.094389 -4.1342092 -4.1733732 -4.1854506 -4.163662 -4.1454806 -4.1466494 -4.1750379 -4.22639 -4.2781243][-4.2438316 -4.2005153 -4.1351538 -4.0784307 -4.0494571 -4.0571465 -4.096765 -4.1479545 -4.1797752 -4.1776147 -4.166491 -4.1590595 -4.1694736 -4.2052979 -4.2497072][-4.2437906 -4.2050858 -4.1398406 -4.0789576 -4.04004 -4.0358448 -4.0713043 -4.1256385 -4.1662021 -4.174818 -4.1680756 -4.1558433 -4.1520371 -4.1717525 -4.2106533][-4.2433705 -4.2136297 -4.1609244 -4.1071348 -4.0630078 -4.043848 -4.0669093 -4.1142397 -4.1535378 -4.1661911 -4.162653 -4.1490841 -4.1339912 -4.1406245 -4.17699][-4.2424283 -4.2248425 -4.1898179 -4.1530509 -4.1164589 -4.0895605 -4.0902553 -4.1182628 -4.1473522 -4.1561341 -4.1515665 -4.1395693 -4.1209383 -4.1217837 -4.1584978][-4.2254157 -4.2191424 -4.2019877 -4.1862254 -4.1691713 -4.1470079 -4.1314096 -4.1356244 -4.1488814 -4.1510639 -4.1472397 -4.1394343 -4.1228328 -4.1220975 -4.1550422][-4.19236 -4.1915774 -4.1876974 -4.1917205 -4.1979866 -4.1879978 -4.1660571 -4.15612 -4.1615481 -4.1631541 -4.1657381 -4.1669936 -4.157445 -4.1572809 -4.1808519][-4.1494126 -4.1504145 -4.1608868 -4.1795588 -4.2026677 -4.204164 -4.1825943 -4.1687446 -4.1762571 -4.190094 -4.20668 -4.2198315 -4.2169943 -4.214726 -4.2217364][-4.1196184 -4.1181493 -4.1377726 -4.167264 -4.2002153 -4.208488 -4.1888757 -4.174612 -4.1849852 -4.2097564 -4.2379103 -4.2598677 -4.2638979 -4.2613807 -4.2552505][-4.1221948 -4.1158957 -4.1343627 -4.1642914 -4.197475 -4.2049108 -4.1864734 -4.1738634 -4.1871018 -4.2198405 -4.2538381 -4.2789493 -4.2862062 -4.2817378 -4.2662644][-4.147872 -4.1390548 -4.1499891 -4.1700425 -4.1927433 -4.191925 -4.1719995 -4.1647625 -4.1847682 -4.223279 -4.2594767 -4.2845659 -4.2912617 -4.2808275 -4.2575092]]...]
INFO - root - 2017-12-05 18:46:09.354619: step 34910, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 71h:43m:37s remains)
INFO - root - 2017-12-05 18:46:18.855112: step 34920, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 81h:14m:51s remains)
INFO - root - 2017-12-05 18:46:28.163257: step 34930, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.966 sec/batch; 79h:52m:12s remains)
INFO - root - 2017-12-05 18:46:37.432735: step 34940, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 82h:05m:27s remains)
INFO - root - 2017-12-05 18:46:46.789161: step 34950, loss = 2.09, batch loss = 2.04 (9.0 examples/sec; 0.888 sec/batch; 73h:24m:08s remains)
INFO - root - 2017-12-05 18:46:56.078024: step 34960, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 74h:08m:09s remains)
INFO - root - 2017-12-05 18:47:05.233822: step 34970, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 77h:41m:21s remains)
INFO - root - 2017-12-05 18:47:14.720403: step 34980, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 75h:50m:41s remains)
INFO - root - 2017-12-05 18:47:24.265481: step 34990, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 74h:21m:23s remains)
INFO - root - 2017-12-05 18:47:33.674833: step 35000, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 77h:11m:07s remains)
2017-12-05 18:47:34.424788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.35848 -4.3631392 -4.3650546 -4.3652287 -4.3632956 -4.3613386 -4.3582535 -4.3521681 -4.3452096 -4.3424091 -4.3462911 -4.3533154 -4.3581414 -4.355175 -4.3410568][-4.358573 -4.3636923 -4.3653908 -4.3635631 -4.3581443 -4.3533993 -4.3459325 -4.3350949 -4.3231821 -4.3170533 -4.3230815 -4.3370996 -4.3494806 -4.3492827 -4.3317451][-4.3600969 -4.363802 -4.3629251 -4.3584447 -4.3504596 -4.3419213 -4.3270926 -4.3072848 -4.28603 -4.2767024 -4.28576 -4.3087568 -4.3303733 -4.3335986 -4.3139029][-4.3616047 -4.3615389 -4.3577676 -4.3504868 -4.338654 -4.321403 -4.2935324 -4.2604542 -4.22974 -4.2185569 -4.2304335 -4.2657104 -4.2980227 -4.3066392 -4.289196][-4.3630776 -4.3608766 -4.3540306 -4.3398681 -4.3195715 -4.2892113 -4.2453818 -4.1980457 -4.1594267 -4.1521029 -4.1756935 -4.2245297 -4.2624106 -4.2792192 -4.2707534][-4.3635774 -4.3590808 -4.34543 -4.3205919 -4.288938 -4.2434773 -4.1825166 -4.1175017 -4.0740576 -4.0798306 -4.1245036 -4.1879106 -4.2351155 -4.26299 -4.2658615][-4.3570809 -4.3508096 -4.3313212 -4.2964878 -4.2514124 -4.1897922 -4.1085825 -4.022985 -3.981894 -4.0141888 -4.0870814 -4.1688647 -4.2254663 -4.2605581 -4.2718248][-4.3450947 -4.3377852 -4.3136048 -4.2690506 -4.2088122 -4.1324811 -4.0273466 -3.9230855 -3.9020162 -3.9722409 -4.0717034 -4.1640549 -4.2251911 -4.26048 -4.2703161][-4.3228459 -4.3166909 -4.2901316 -4.2354994 -4.1603394 -4.06993 -3.9422522 -3.8348842 -3.8554778 -3.9643455 -4.08094 -4.1691666 -4.2192388 -4.2430234 -4.2477808][-4.287324 -4.2879915 -4.2633195 -4.20393 -4.11978 -4.0275793 -3.9058037 -3.8220048 -3.8770773 -4.00629 -4.1236234 -4.1905751 -4.2121286 -4.21302 -4.2134924][-4.256618 -4.2685385 -4.2516584 -4.193706 -4.1139159 -4.038003 -3.9543259 -3.9107723 -3.9750459 -4.0882697 -4.1790247 -4.2141566 -4.2050238 -4.1897664 -4.1938758][-4.2419739 -4.2675982 -4.261281 -4.2131276 -4.1490188 -4.1001697 -4.0587897 -4.0481772 -4.0996685 -4.1728396 -4.2210274 -4.2241549 -4.2009678 -4.1869473 -4.202498][-4.2443194 -4.2792587 -4.2800608 -4.2438278 -4.1985707 -4.1729937 -4.1611891 -4.1692848 -4.20176 -4.2329054 -4.2396917 -4.2217364 -4.202374 -4.2044578 -4.2330661][-4.2525339 -4.2829723 -4.2842097 -4.2584214 -4.23341 -4.2311263 -4.2378211 -4.2483439 -4.2579045 -4.2525105 -4.2305284 -4.2084193 -4.2066016 -4.2301173 -4.2687736][-4.2487669 -4.2656116 -4.264667 -4.252243 -4.2503181 -4.266592 -4.2792778 -4.2793016 -4.263824 -4.2367959 -4.2083435 -4.1996727 -4.2199497 -4.2586241 -4.298665]]...]
INFO - root - 2017-12-05 18:47:43.809045: step 35010, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 74h:53m:38s remains)
INFO - root - 2017-12-05 18:47:53.103864: step 35020, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 69h:07m:09s remains)
INFO - root - 2017-12-05 18:48:02.516767: step 35030, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 79h:22m:35s remains)
INFO - root - 2017-12-05 18:48:11.989858: step 35040, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 80h:04m:25s remains)
INFO - root - 2017-12-05 18:48:21.429902: step 35050, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 76h:31m:35s remains)
INFO - root - 2017-12-05 18:48:30.524183: step 35060, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 74h:26m:14s remains)
INFO - root - 2017-12-05 18:48:39.957935: step 35070, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.990 sec/batch; 81h:47m:54s remains)
INFO - root - 2017-12-05 18:48:49.338832: step 35080, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 79h:59m:29s remains)
INFO - root - 2017-12-05 18:48:58.899982: step 35090, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 76h:54m:53s remains)
INFO - root - 2017-12-05 18:49:08.233571: step 35100, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 76h:38m:41s remains)
2017-12-05 18:49:08.969444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2630649 -4.2369218 -4.2147121 -4.2048488 -4.2029738 -4.2010927 -4.1956515 -4.1929255 -4.2001777 -4.2216411 -4.248683 -4.2644434 -4.2600102 -4.2504773 -4.2526059][-4.2781668 -4.254745 -4.2293038 -4.2145262 -4.2118425 -4.2123575 -4.2105227 -4.2093949 -4.2180166 -4.2395916 -4.2640352 -4.2776585 -4.2722425 -4.2641125 -4.2656078][-4.2861891 -4.2679152 -4.2444243 -4.231256 -4.2338624 -4.2408366 -4.2415671 -4.2394018 -4.2463951 -4.2662845 -4.2894049 -4.3005638 -4.2904058 -4.2797227 -4.2774076][-4.279644 -4.2644143 -4.244164 -4.236289 -4.2452264 -4.2559996 -4.2585707 -4.257359 -4.2623038 -4.2825646 -4.3067346 -4.3152924 -4.3009849 -4.2871108 -4.2819972][-4.2580895 -4.242188 -4.2229943 -4.2172041 -4.2252254 -4.2357211 -4.2417121 -4.2445016 -4.2499571 -4.272965 -4.3021412 -4.3136086 -4.3006997 -4.2873235 -4.2817111][-4.231771 -4.2136164 -4.1891379 -4.1768932 -4.1790304 -4.186295 -4.1966581 -4.2040377 -4.209538 -4.2361007 -4.27316 -4.2924361 -4.286099 -4.2784691 -4.2766862][-4.2060795 -4.1893167 -4.1604595 -4.1403303 -4.1350994 -4.1368165 -4.1469927 -4.15267 -4.1560583 -4.1858993 -4.2299581 -4.2588534 -4.2618322 -4.2621856 -4.26491][-4.195467 -4.1821675 -4.1523871 -4.126822 -4.1135297 -4.1065111 -4.1125298 -4.1164126 -4.1168809 -4.1440167 -4.192708 -4.2307353 -4.2446022 -4.2516541 -4.2554069][-4.2008085 -4.1944613 -4.1706905 -4.147296 -4.1314883 -4.1191368 -4.1213984 -4.1251583 -4.12399 -4.146688 -4.1948962 -4.2361946 -4.2540731 -4.2598872 -4.2590637][-4.2141075 -4.2156811 -4.2031469 -4.1884146 -4.1769304 -4.1659479 -4.16688 -4.1718297 -4.17431 -4.196012 -4.2389154 -4.27392 -4.285 -4.282352 -4.2725105][-4.2253108 -4.2334518 -4.2331657 -4.2308006 -4.2294059 -4.223608 -4.22295 -4.2260995 -4.2310276 -4.2527118 -4.2872014 -4.3117218 -4.3149652 -4.3061523 -4.290504][-4.2359238 -4.2493606 -4.2568083 -4.2616773 -4.2656622 -4.262219 -4.2589459 -4.2607141 -4.2694297 -4.290235 -4.3153892 -4.3284025 -4.3270788 -4.3189616 -4.3041086][-4.2410927 -4.2537007 -4.262404 -4.2686539 -4.27298 -4.269938 -4.2644582 -4.2646041 -4.2758436 -4.2959528 -4.3151546 -4.3218656 -4.3214645 -4.3185539 -4.3085074][-4.2391229 -4.249207 -4.2578626 -4.2641377 -4.2660952 -4.2602687 -4.2493048 -4.2455788 -4.2567616 -4.2799993 -4.300179 -4.3070726 -4.3095646 -4.31031 -4.3036923][-4.2347651 -4.2382073 -4.2426805 -4.2451792 -4.2428656 -4.2341542 -4.2174454 -4.2058725 -4.2140355 -4.2426639 -4.2677269 -4.2755241 -4.2809091 -4.2875915 -4.2863946]]...]
INFO - root - 2017-12-05 18:49:18.418753: step 35110, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.039 sec/batch; 85h:49m:50s remains)
INFO - root - 2017-12-05 18:49:28.067434: step 35120, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 77h:56m:05s remains)
INFO - root - 2017-12-05 18:49:37.579635: step 35130, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 78h:36m:40s remains)
INFO - root - 2017-12-05 18:49:46.925988: step 35140, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 79h:43m:41s remains)
INFO - root - 2017-12-05 18:49:56.217936: step 35150, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 69h:44m:42s remains)
INFO - root - 2017-12-05 18:50:05.409035: step 35160, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 78h:52m:57s remains)
INFO - root - 2017-12-05 18:50:14.695291: step 35170, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.966 sec/batch; 79h:44m:45s remains)
INFO - root - 2017-12-05 18:50:24.097293: step 35180, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 78h:26m:30s remains)
INFO - root - 2017-12-05 18:50:33.560366: step 35190, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 79h:21m:02s remains)
INFO - root - 2017-12-05 18:50:42.858725: step 35200, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 70h:22m:20s remains)
2017-12-05 18:50:43.594810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2710457 -4.2714105 -4.258811 -4.2368751 -4.2001138 -4.1585197 -4.1548386 -4.15493 -4.1257763 -4.1054206 -4.109962 -4.1253591 -4.1423645 -4.1532841 -4.1594105][-4.2791505 -4.2786751 -4.2609038 -4.2324114 -4.1930313 -4.1549373 -4.1495514 -4.1555548 -4.1376238 -4.1173415 -4.1099882 -4.1087289 -4.1152244 -4.1327152 -4.1501718][-4.2816272 -4.2792807 -4.2571321 -4.219748 -4.1738167 -4.1369853 -4.1320066 -4.149271 -4.1497865 -4.1384258 -4.1244764 -4.1048508 -4.0992064 -4.1226635 -4.1462812][-4.2803249 -4.2783079 -4.2523737 -4.208118 -4.1539764 -4.11238 -4.1074214 -4.1403913 -4.1638479 -4.1669865 -4.1503325 -4.1155505 -4.0999017 -4.1223907 -4.14925][-4.281857 -4.2823658 -4.2563438 -4.2069225 -4.143405 -4.0887465 -4.0777364 -4.1241789 -4.1692047 -4.1857038 -4.1740971 -4.13659 -4.11501 -4.1282973 -4.1509466][-4.2804189 -4.2834644 -4.2611432 -4.2085443 -4.13391 -4.0554247 -4.0307293 -4.0955715 -4.1661215 -4.2004962 -4.1977453 -4.1663675 -4.1424994 -4.1430078 -4.1566281][-4.2721491 -4.2785635 -4.259109 -4.2026711 -4.1122274 -3.9995475 -3.9535611 -4.0444283 -4.1498108 -4.2065711 -4.2167969 -4.196631 -4.1769691 -4.1703253 -4.1744623][-4.26317 -4.2690792 -4.2463737 -4.1860433 -4.0902133 -3.9642396 -3.9133048 -4.0219822 -4.1439872 -4.2115231 -4.2293468 -4.2227669 -4.211699 -4.2038889 -4.1959519][-4.2501488 -4.25332 -4.2287703 -4.1751661 -4.0964885 -4.0055532 -3.9811597 -4.065033 -4.1579323 -4.2110343 -4.2294617 -4.2356992 -4.2371922 -4.2335715 -4.2185383][-4.2325568 -4.2318454 -4.2097311 -4.1702905 -4.1176062 -4.0686731 -4.0638123 -4.1159258 -4.1723027 -4.2061129 -4.2240982 -4.2378798 -4.2475886 -4.2460551 -4.2238722][-4.2164431 -4.2120175 -4.193594 -4.1666183 -4.1396208 -4.1191683 -4.1201639 -4.1508479 -4.1850815 -4.2089629 -4.2262158 -4.2399168 -4.2491674 -4.2423429 -4.2115231][-4.2084618 -4.2039666 -4.1874771 -4.1670713 -4.1542029 -4.148644 -4.1520133 -4.1679282 -4.1891904 -4.2124438 -4.2292266 -4.2370214 -4.2407913 -4.2293205 -4.195941][-4.2035518 -4.2003312 -4.1855183 -4.17025 -4.1641226 -4.1640639 -4.1700358 -4.1805234 -4.1944957 -4.2176514 -4.2333088 -4.2344694 -4.2316079 -4.214694 -4.1788497][-4.1984391 -4.1981363 -4.1879835 -4.1757588 -4.1731815 -4.1735229 -4.1813874 -4.1881332 -4.1974068 -4.2152567 -4.2280269 -4.2323914 -4.2296543 -4.21157 -4.1761608][-4.201438 -4.2092352 -4.1988025 -4.1811724 -4.173141 -4.1707778 -4.1752114 -4.1795168 -4.1874938 -4.2054491 -4.2243762 -4.2378755 -4.2444916 -4.2329803 -4.2010264]]...]
INFO - root - 2017-12-05 18:50:52.973679: step 35210, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 78h:11m:10s remains)
INFO - root - 2017-12-05 18:51:02.576398: step 35220, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 78h:59m:15s remains)
INFO - root - 2017-12-05 18:51:11.843818: step 35230, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 78h:56m:19s remains)
INFO - root - 2017-12-05 18:51:21.218083: step 35240, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 79h:03m:19s remains)
INFO - root - 2017-12-05 18:51:30.386589: step 35250, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 69h:33m:21s remains)
INFO - root - 2017-12-05 18:51:39.758691: step 35260, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 76h:46m:22s remains)
INFO - root - 2017-12-05 18:51:49.335143: step 35270, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.988 sec/batch; 81h:36m:19s remains)
INFO - root - 2017-12-05 18:51:58.745445: step 35280, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 75h:58m:37s remains)
INFO - root - 2017-12-05 18:52:08.108299: step 35290, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 81h:08m:48s remains)
INFO - root - 2017-12-05 18:52:17.234886: step 35300, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 76h:55m:48s remains)
2017-12-05 18:52:17.944741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2971091 -4.305017 -4.3217945 -4.3420272 -4.3449888 -4.3253131 -4.2985816 -4.2737432 -4.2544646 -4.2464228 -4.2376537 -4.219593 -4.1917434 -4.1489472 -4.1013732][-4.2942734 -4.3098416 -4.3286586 -4.3437395 -4.3414841 -4.3178096 -4.2889385 -4.26923 -4.2588248 -4.2516885 -4.2371135 -4.2100296 -4.1730938 -4.1247435 -4.07289][-4.3017526 -4.3178616 -4.3278785 -4.3320937 -4.3240652 -4.2991719 -4.2735004 -4.2648072 -4.2694845 -4.2739863 -4.2630048 -4.23666 -4.2042422 -4.1689544 -4.1292171][-4.3034391 -4.3160014 -4.312109 -4.3026142 -4.2866693 -4.2535424 -4.221787 -4.2196174 -4.2454243 -4.27052 -4.2761955 -4.2675271 -4.2543077 -4.2390356 -4.2179742][-4.2991157 -4.3007531 -4.280839 -4.2567916 -4.2282419 -4.1789412 -4.1274714 -4.1201758 -4.170187 -4.2226725 -4.2460403 -4.2561164 -4.2651687 -4.2697186 -4.2652225][-4.2881742 -4.2704711 -4.2282372 -4.1788983 -4.1254787 -4.0471268 -3.9658191 -3.9490175 -4.0333734 -4.1230073 -4.1658139 -4.194365 -4.2272325 -4.2517471 -4.2585845][-4.2827392 -4.2406659 -4.1713123 -4.0923362 -4.0068121 -3.893995 -3.7759905 -3.7506373 -3.8708556 -3.9947836 -4.0605268 -4.1106119 -4.1721683 -4.2167997 -4.232398][-4.2714419 -4.219347 -4.1379704 -4.0495605 -3.9531775 -3.8351388 -3.7158985 -3.6965036 -3.8222234 -3.9493482 -4.0247555 -4.0879169 -4.1581349 -4.2082386 -4.2237515][-4.2628274 -4.2258949 -4.1608291 -4.0949636 -4.0255094 -3.9431691 -3.8644478 -3.8559213 -3.9390612 -4.0282936 -4.0878448 -4.1421161 -4.1978221 -4.2333217 -4.2373986][-4.2716312 -4.2579041 -4.2201986 -4.1817374 -4.1422052 -4.0939283 -4.0505934 -4.044929 -4.0898995 -4.1413856 -4.1805267 -4.2167139 -4.2493825 -4.2629042 -4.2567363][-4.2708983 -4.2705364 -4.2483058 -4.2261634 -4.2090874 -4.1886973 -4.170929 -4.165308 -4.188539 -4.2190695 -4.2434525 -4.2643332 -4.2735033 -4.2668791 -4.2572408][-4.2392592 -4.2414675 -4.2275982 -4.2175484 -4.22188 -4.2282553 -4.2307782 -4.2259293 -4.2357922 -4.2523732 -4.266932 -4.2755232 -4.2639136 -4.2379327 -4.2242932][-4.1902227 -4.1953154 -4.1895237 -4.1895623 -4.2088809 -4.2329559 -4.2456627 -4.2380195 -4.2328367 -4.2361979 -4.2425814 -4.2423115 -4.2187529 -4.1784782 -4.1594563][-4.153337 -4.1591582 -4.1607466 -4.1683321 -4.1930432 -4.223115 -4.2369614 -4.227387 -4.2144341 -4.2116809 -4.21417 -4.2126966 -4.186667 -4.1367617 -4.1091075][-4.1616697 -4.1656241 -4.1712646 -4.1828718 -4.2075329 -4.2352023 -4.2451625 -4.2362771 -4.227838 -4.2273445 -4.2304606 -4.2300119 -4.2057519 -4.1559758 -4.1250596]]...]
INFO - root - 2017-12-05 18:52:27.333111: step 35310, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 77h:50m:24s remains)
INFO - root - 2017-12-05 18:52:36.742285: step 35320, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 79h:16m:45s remains)
INFO - root - 2017-12-05 18:52:46.119992: step 35330, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 74h:23m:42s remains)
INFO - root - 2017-12-05 18:52:55.499337: step 35340, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 76h:36m:16s remains)
INFO - root - 2017-12-05 18:53:05.037531: step 35350, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 79h:16m:07s remains)
INFO - root - 2017-12-05 18:53:14.412116: step 35360, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 79h:28m:29s remains)
INFO - root - 2017-12-05 18:53:23.826390: step 35370, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.967 sec/batch; 79h:48m:09s remains)
INFO - root - 2017-12-05 18:53:33.205272: step 35380, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 71h:35m:18s remains)
INFO - root - 2017-12-05 18:53:42.691334: step 35390, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 76h:52m:30s remains)
INFO - root - 2017-12-05 18:53:52.017813: step 35400, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 76h:44m:46s remains)
2017-12-05 18:53:52.749017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3083563 -4.3191791 -4.3133721 -4.2905941 -4.2628813 -4.2422233 -4.2264919 -4.2172189 -4.2091637 -4.1976976 -4.1901727 -4.1917281 -4.2045503 -4.2256246 -4.2477913][-4.3068633 -4.313818 -4.3005133 -4.26959 -4.23304 -4.2032371 -4.1824622 -4.1760082 -4.1778193 -4.1709237 -4.162509 -4.1618876 -4.1721311 -4.1923475 -4.2151275][-4.3073707 -4.3108349 -4.2928028 -4.253119 -4.2034721 -4.1629419 -4.1341972 -4.1275229 -4.1399846 -4.1450968 -4.1392493 -4.138453 -4.1474991 -4.1695247 -4.192277][-4.31149 -4.3120694 -4.2903442 -4.2450013 -4.1855307 -4.1359105 -4.0980749 -4.0862246 -4.1082673 -4.1298037 -4.1339488 -4.1361589 -4.144454 -4.1609297 -4.1784687][-4.3139496 -4.3145452 -4.2960691 -4.253149 -4.18835 -4.1274929 -4.0721421 -4.0442028 -4.0735598 -4.1182523 -4.1450362 -4.1617832 -4.1717162 -4.1765771 -4.1822429][-4.3115935 -4.3140192 -4.2984619 -4.2566509 -4.1853151 -4.1019392 -4.0127592 -3.9632883 -4.0093222 -4.0867891 -4.1420908 -4.1788459 -4.1976705 -4.1991553 -4.1943116][-4.3053651 -4.3047152 -4.285202 -4.2356172 -4.1509175 -4.0409589 -3.9103858 -3.8460755 -3.9333682 -4.0491672 -4.1227989 -4.1703348 -4.19707 -4.1994324 -4.1944041][-4.2991834 -4.2936454 -4.2684984 -4.2106104 -4.1149669 -3.9958997 -3.8657875 -3.8311603 -3.9473531 -4.0635772 -4.1253533 -4.1610208 -4.183455 -4.1862855 -4.1852551][-4.2949028 -4.28706 -4.260294 -4.2028832 -4.1201158 -4.0375352 -3.9651821 -3.9628177 -4.0482664 -4.1224494 -4.1554818 -4.1682205 -4.1778526 -4.1785779 -4.1789532][-4.2949185 -4.2864552 -4.2654233 -4.2202544 -4.1660023 -4.1244688 -4.0925279 -4.0927219 -4.1377058 -4.1742153 -4.1840682 -4.1791286 -4.1775007 -4.1796875 -4.1893592][-4.2979727 -4.29139 -4.2765183 -4.2464328 -4.2140732 -4.1899424 -4.17359 -4.1726551 -4.1912923 -4.2032762 -4.2014351 -4.1913748 -4.1893921 -4.1994328 -4.21439][-4.2998481 -4.2959208 -4.2859774 -4.2674503 -4.2487655 -4.2332029 -4.2227306 -4.2218938 -4.2300682 -4.2281361 -4.220964 -4.2123566 -4.2133584 -4.2267361 -4.2414374][-4.3017273 -4.3011622 -4.2929144 -4.2810445 -4.2681279 -4.255281 -4.2472615 -4.2490654 -4.2548442 -4.2444897 -4.2357507 -4.2306471 -4.2321362 -4.2431049 -4.2552843][-4.3038836 -4.3069096 -4.2957335 -4.2826939 -4.2714052 -4.2604136 -4.2529774 -4.2565536 -4.2585258 -4.2454243 -4.2357707 -4.2327247 -4.2346191 -4.2442789 -4.2553382][-4.3047814 -4.3092608 -4.2945762 -4.2752018 -4.2605705 -4.2480164 -4.2387896 -4.2418718 -4.2441511 -4.2321634 -4.2194076 -4.2172608 -4.2245207 -4.23864 -4.2509508]]...]
INFO - root - 2017-12-05 18:54:02.100558: step 35410, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 73h:33m:15s remains)
INFO - root - 2017-12-05 18:54:11.256149: step 35420, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 78h:30m:40s remains)
INFO - root - 2017-12-05 18:54:20.479488: step 35430, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 79h:49m:55s remains)
INFO - root - 2017-12-05 18:54:30.101623: step 35440, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 81h:36m:04s remains)
INFO - root - 2017-12-05 18:54:39.445227: step 35450, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 78h:41m:36s remains)
INFO - root - 2017-12-05 18:54:48.519929: step 35460, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 76h:00m:21s remains)
INFO - root - 2017-12-05 18:54:58.200039: step 35470, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 81h:26m:35s remains)
INFO - root - 2017-12-05 18:55:07.611924: step 35480, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 82h:37m:16s remains)
INFO - root - 2017-12-05 18:55:16.792415: step 35490, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 71h:02m:21s remains)
INFO - root - 2017-12-05 18:55:26.086061: step 35500, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.941 sec/batch; 77h:39m:34s remains)
2017-12-05 18:55:26.879275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3562336 -4.3514066 -4.335588 -4.3171535 -4.3008604 -4.2815986 -4.2626629 -4.2471266 -4.2473607 -4.2639971 -4.2763872 -4.2840824 -4.289042 -4.2943597 -4.3016353][-4.3567762 -4.3501849 -4.3282385 -4.3024049 -4.2791672 -4.2510109 -4.2193685 -4.1890812 -4.1845875 -4.2076631 -4.2244506 -4.233695 -4.2379546 -4.2441993 -4.2548575][-4.3558969 -4.3476839 -4.3187342 -4.2862315 -4.2567821 -4.2201724 -4.1771932 -4.1326804 -4.121892 -4.1515794 -4.1722612 -4.1832652 -4.1844363 -4.1909556 -4.2060537][-4.3537893 -4.3435941 -4.3084283 -4.2710986 -4.2375679 -4.1943831 -4.1443634 -4.0913429 -4.074698 -4.1055121 -4.125659 -4.1317353 -4.1299233 -4.1382041 -4.1626792][-4.3478942 -4.3326693 -4.2897372 -4.2413139 -4.2001195 -4.1542425 -4.10143 -4.0443788 -4.0214405 -4.0471582 -4.0601454 -4.0613742 -4.0617437 -4.0769539 -4.1135225][-4.339818 -4.3157158 -4.2631192 -4.2038145 -4.1552377 -4.1087866 -4.0577216 -3.9980063 -3.9634101 -3.9729302 -3.974431 -3.9719934 -3.9789691 -4.0071592 -4.0569048][-4.3333888 -4.3020649 -4.2412992 -4.1743245 -4.1228986 -4.0781727 -4.0245204 -3.9498484 -3.8896298 -3.8819497 -3.8847985 -3.8854184 -3.8973541 -3.9345436 -3.9956512][-4.3277225 -4.2892761 -4.2225218 -4.1534038 -4.1016293 -4.0495262 -3.9814048 -3.8847139 -3.8032761 -3.7914233 -3.8077445 -3.8269069 -3.8499887 -3.8885992 -3.9462461][-4.3257785 -4.28328 -4.2149072 -4.1468573 -4.0937576 -4.0300317 -3.951565 -3.8503876 -3.7752721 -3.7784152 -3.8100977 -3.8382621 -3.8657851 -3.8992834 -3.9433925][-4.3316474 -4.2917271 -4.2304454 -4.1699381 -4.1230774 -4.06875 -4.0026417 -3.9273024 -3.8833835 -3.8915439 -3.9143846 -3.9396112 -3.96179 -3.9843016 -4.0128117][-4.34281 -4.3114948 -4.2645607 -4.2187948 -4.1848016 -4.1465912 -4.1053867 -4.0585628 -4.0361691 -4.0425873 -4.0530009 -4.0713434 -4.0923867 -4.1114049 -4.1266041][-4.3558106 -4.3348489 -4.3051085 -4.2771926 -4.2553997 -4.2309289 -4.2077632 -4.1793447 -4.1696877 -4.1748438 -4.1805191 -4.1971869 -4.2169328 -4.231545 -4.2373905][-4.3660789 -4.3530135 -4.3358831 -4.3202963 -4.3074942 -4.2934132 -4.2831216 -4.2703695 -4.2672029 -4.2712388 -4.2780614 -4.2928133 -4.3066592 -4.3112831 -4.3103576][-4.3725948 -4.3651686 -4.3555274 -4.3458576 -4.3364868 -4.3285956 -4.3256669 -4.3227386 -4.3251271 -4.3335061 -4.3417912 -4.3505459 -4.3551912 -4.3553228 -4.3532262][-4.3765855 -4.3733549 -4.368412 -4.362402 -4.3556032 -4.3500118 -4.3481917 -4.3486357 -4.3529906 -4.3619943 -4.3694968 -4.3741331 -4.3743181 -4.3741808 -4.3729663]]...]
INFO - root - 2017-12-05 18:55:36.379169: step 35510, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 81h:41m:10s remains)
INFO - root - 2017-12-05 18:55:45.815303: step 35520, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 79h:55m:44s remains)
INFO - root - 2017-12-05 18:55:55.222187: step 35530, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 79h:45m:07s remains)
INFO - root - 2017-12-05 18:56:04.711238: step 35540, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 71h:54m:28s remains)
INFO - root - 2017-12-05 18:56:14.123152: step 35550, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 78h:54m:30s remains)
INFO - root - 2017-12-05 18:56:23.283109: step 35560, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 74h:20m:57s remains)
INFO - root - 2017-12-05 18:56:32.685513: step 35570, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 78h:21m:27s remains)
INFO - root - 2017-12-05 18:56:42.117300: step 35580, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 78h:30m:12s remains)
INFO - root - 2017-12-05 18:56:51.425038: step 35590, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 69h:22m:31s remains)
INFO - root - 2017-12-05 18:57:00.807054: step 35600, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 75h:55m:14s remains)
2017-12-05 18:57:01.555202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3438039 -4.3402176 -4.3151026 -4.2605095 -4.1771507 -4.08922 -4.0578065 -4.0960317 -4.1631856 -4.2167077 -4.2558146 -4.290956 -4.3198943 -4.3303075 -4.3301125][-4.3469934 -4.3421 -4.3098831 -4.2434082 -4.1407375 -4.0352144 -4.0065827 -4.0691819 -4.1586046 -4.2174997 -4.2562203 -4.2936888 -4.32417 -4.3354235 -4.3339434][-4.3495364 -4.3418965 -4.3037691 -4.2278314 -4.1090231 -3.9862978 -3.9592159 -4.0461946 -4.1584044 -4.2206583 -4.2596674 -4.2987857 -4.328063 -4.3382931 -4.3352027][-4.3489523 -4.3379974 -4.2943859 -4.2112336 -4.0809364 -3.9408152 -3.9034095 -4.0057387 -4.1390233 -4.2156086 -4.2610688 -4.3030119 -4.3308697 -4.3388619 -4.3330393][-4.3450656 -4.3323631 -4.2864337 -4.1983914 -4.0649333 -3.9139509 -3.8566744 -3.9648671 -4.1137495 -4.206284 -4.2611895 -4.304132 -4.3311071 -4.3375783 -4.32789][-4.3423338 -4.3313007 -4.2864981 -4.1936517 -4.0540247 -3.8925712 -3.8130834 -3.9242506 -4.0899272 -4.1987605 -4.2614436 -4.3047433 -4.3307633 -4.3358397 -4.3214812][-4.3393807 -4.3329835 -4.2921958 -4.1953025 -4.0452142 -3.8697388 -3.771071 -3.8776541 -4.0576591 -4.17948 -4.2490816 -4.2949734 -4.323874 -4.3290586 -4.3102803][-4.3343053 -4.3320136 -4.29881 -4.2059479 -4.05542 -3.8748221 -3.7617579 -3.8479803 -4.0298209 -4.1621981 -4.2379966 -4.28484 -4.3132434 -4.3187194 -4.2960896][-4.3281641 -4.3283658 -4.3030434 -4.2254252 -4.0918059 -3.9239211 -3.8068323 -3.8566642 -4.0190916 -4.153789 -4.2329707 -4.2787642 -4.3045735 -4.3102522 -4.2860785][-4.3224053 -4.3239865 -4.3058639 -4.2481747 -4.1436076 -4.0046868 -3.8986046 -3.9150634 -4.0431566 -4.1665053 -4.2382441 -4.2785068 -4.2990913 -4.302166 -4.2778945][-4.3192983 -4.3211322 -4.3071237 -4.2645512 -4.18736 -4.0815377 -3.9955697 -3.9947653 -4.0902171 -4.1944189 -4.2560515 -4.28818 -4.302855 -4.3035684 -4.2799644][-4.3161788 -4.3180637 -4.3098183 -4.278614 -4.2221651 -4.1466789 -4.0850658 -4.0858612 -4.1590118 -4.2418804 -4.290802 -4.3103833 -4.3148432 -4.3093677 -4.2865167][-4.3112345 -4.3150463 -4.3139987 -4.2962179 -4.2579703 -4.2075744 -4.1689215 -4.172205 -4.2273135 -4.2878413 -4.3226352 -4.3330889 -4.3288484 -4.31674 -4.2933793][-4.3085485 -4.3142843 -4.3174829 -4.3091373 -4.2861595 -4.2549653 -4.2324686 -4.2370415 -4.2755203 -4.3152852 -4.3374414 -4.3426847 -4.3345842 -4.3210111 -4.301507][-4.3068271 -4.313345 -4.3188729 -4.316844 -4.3042994 -4.2858877 -4.2731833 -4.2775455 -4.3027172 -4.3271222 -4.3399568 -4.3421841 -4.3346982 -4.3225474 -4.3077936]]...]
INFO - root - 2017-12-05 18:57:11.218014: step 35610, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 75h:37m:23s remains)
INFO - root - 2017-12-05 18:57:20.766373: step 35620, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 78h:57m:02s remains)
INFO - root - 2017-12-05 18:57:30.291101: step 35630, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 78h:27m:46s remains)
INFO - root - 2017-12-05 18:57:39.510357: step 35640, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 74h:17m:43s remains)
INFO - root - 2017-12-05 18:57:49.049301: step 35650, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 79h:21m:37s remains)
INFO - root - 2017-12-05 18:57:58.212983: step 35660, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 74h:53m:33s remains)
INFO - root - 2017-12-05 18:58:07.634987: step 35670, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 75h:06m:31s remains)
INFO - root - 2017-12-05 18:58:17.069030: step 35680, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 76h:07m:17s remains)
INFO - root - 2017-12-05 18:58:26.470165: step 35690, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 78h:19m:55s remains)
INFO - root - 2017-12-05 18:58:35.984977: step 35700, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 76h:08m:22s remains)
2017-12-05 18:58:36.820344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2090311 -4.1842408 -4.1499434 -4.1369319 -4.167695 -4.208806 -4.2301207 -4.22024 -4.1856608 -4.1673355 -4.1892214 -4.23122 -4.2636595 -4.2720656 -4.2625165][-4.2125831 -4.1736407 -4.1234841 -4.1017375 -4.1346984 -4.18118 -4.2058973 -4.1972289 -4.1723876 -4.1626816 -4.1903791 -4.2339458 -4.2683654 -4.276278 -4.265532][-4.2348557 -4.1940589 -4.1445265 -4.1211033 -4.1404328 -4.1681819 -4.1775231 -4.1609793 -4.1402826 -4.1374979 -4.1712761 -4.2219191 -4.2604451 -4.268796 -4.2622738][-4.2531366 -4.2210345 -4.1851997 -4.1682296 -4.1746268 -4.1796737 -4.1641645 -4.1310425 -4.09952 -4.088975 -4.1330647 -4.1984663 -4.2420897 -4.2517614 -4.2487931][-4.2430229 -4.2289014 -4.2151861 -4.2129946 -4.2190084 -4.2062035 -4.1638656 -4.108 -4.0524039 -4.0293784 -4.0875359 -4.1688633 -4.2190118 -4.2321053 -4.232151][-4.21019 -4.21224 -4.2261138 -4.2478747 -4.2585268 -4.2328072 -4.16939 -4.087152 -4.0050511 -3.9766555 -4.0466266 -4.1393609 -4.1995344 -4.21932 -4.2230287][-4.1657472 -4.1826077 -4.2221313 -4.2645507 -4.2789359 -4.2469549 -4.1708984 -4.0691862 -3.9749877 -3.9538326 -4.0315289 -4.1272745 -4.1939549 -4.220037 -4.226615][-4.1360707 -4.1623712 -4.2153716 -4.266603 -4.2804165 -4.2415085 -4.1577773 -4.0500746 -3.9634724 -3.959439 -4.0437393 -4.1401167 -4.2088013 -4.2346263 -4.2398114][-4.1362653 -4.1668048 -4.2197204 -4.2635593 -4.2688556 -4.2243276 -4.1349931 -4.0312004 -3.9632735 -3.9779832 -4.0664158 -4.1641498 -4.2316895 -4.2552361 -4.2600241][-4.1510315 -4.1856303 -4.2328587 -4.2619977 -4.2558093 -4.2068162 -4.1183691 -4.025291 -3.9782629 -4.0055161 -4.0917511 -4.1843944 -4.2472305 -4.2692218 -4.2745528][-4.1743126 -4.2126274 -4.2508521 -4.2632976 -4.2430911 -4.192656 -4.1143327 -4.0392466 -4.0150928 -4.0497384 -4.1230035 -4.1990175 -4.252244 -4.2722893 -4.2793446][-4.1958561 -4.2323613 -4.2617831 -4.2644181 -4.2365041 -4.1882215 -4.1223311 -4.071506 -4.0707278 -4.1091928 -4.1632276 -4.2093315 -4.2425795 -4.2611532 -4.2737737][-4.2203741 -4.2516212 -4.2691536 -4.2614903 -4.2277985 -4.1786389 -4.1226969 -4.0982723 -4.1210279 -4.1660719 -4.2023773 -4.2163262 -4.2254066 -4.2380409 -4.2548938][-4.2579322 -4.280149 -4.2805767 -4.2575746 -4.2126031 -4.1555066 -4.110724 -4.1133904 -4.1534052 -4.2047758 -4.2319202 -4.2272282 -4.2170038 -4.2181625 -4.2344842][-4.2997174 -4.3102913 -4.2930479 -4.2511573 -4.1898909 -4.1266546 -4.0972328 -4.1223474 -4.1724596 -4.22709 -4.2540464 -4.2465553 -4.2304058 -4.22328 -4.231751]]...]
INFO - root - 2017-12-05 18:58:46.457921: step 35710, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 72h:49m:07s remains)
INFO - root - 2017-12-05 18:58:55.831131: step 35720, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 70h:22m:56s remains)
INFO - root - 2017-12-05 18:59:05.054707: step 35730, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 70h:56m:15s remains)
INFO - root - 2017-12-05 18:59:14.431897: step 35740, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 76h:33m:58s remains)
INFO - root - 2017-12-05 18:59:23.879007: step 35750, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 78h:42m:42s remains)
INFO - root - 2017-12-05 18:59:33.286214: step 35760, loss = 2.02, batch loss = 1.96 (8.5 examples/sec; 0.942 sec/batch; 77h:39m:59s remains)
INFO - root - 2017-12-05 18:59:42.497655: step 35770, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 77h:08m:26s remains)
INFO - root - 2017-12-05 18:59:52.015504: step 35780, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 79h:38m:56s remains)
INFO - root - 2017-12-05 19:00:01.405730: step 35790, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 78h:11m:31s remains)
INFO - root - 2017-12-05 19:00:11.053006: step 35800, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 82h:21m:29s remains)
2017-12-05 19:00:11.820020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2664285 -4.2632828 -4.2478147 -4.2248764 -4.1663718 -4.083725 -4.0294518 -4.0315132 -4.0915279 -4.1480775 -4.152782 -4.1235876 -4.0827546 -4.0675673 -4.1041327][-4.2982526 -4.2888074 -4.2660427 -4.2326527 -4.1679735 -4.0844078 -4.0258045 -4.0189562 -4.0783615 -4.1333284 -4.1380963 -4.1152153 -4.0823317 -4.0741405 -4.1143851][-4.3238225 -4.3097711 -4.2813225 -4.2382665 -4.1677217 -4.0801477 -4.014122 -4.0022383 -4.0668936 -4.1199179 -4.1267891 -4.1113524 -4.0858173 -4.0832705 -4.1251969][-4.3395114 -4.3221498 -4.28788 -4.2356763 -4.1562157 -4.0574632 -3.9761908 -3.96685 -4.0432281 -4.1015635 -4.1098037 -4.099504 -4.0690494 -4.0622244 -4.1085658][-4.3477464 -4.3253465 -4.2840037 -4.2238793 -4.1333504 -4.0168533 -3.9071977 -3.902956 -4.0027428 -4.0765696 -4.0922232 -4.07898 -4.0316038 -4.0008492 -4.0541825][-4.3448482 -4.3207283 -4.2792931 -4.2133961 -4.11214 -3.9755945 -3.8366022 -3.8410389 -3.9664562 -4.0542531 -4.077579 -4.0587773 -3.9862485 -3.9191725 -3.9760365][-4.3332329 -4.3116803 -4.2734423 -4.2067943 -4.0990639 -3.942668 -3.7767067 -3.7866669 -3.9331467 -4.0313916 -4.065752 -4.0488968 -3.9685669 -3.8754077 -3.9328561][-4.3224778 -4.3036895 -4.2688375 -4.2012777 -4.0817943 -3.8997312 -3.7045705 -3.7186735 -3.8912394 -4.0141406 -4.0706925 -4.0689745 -3.9986155 -3.9069102 -3.9593709][-4.3153749 -4.2992859 -4.26344 -4.1903329 -4.062964 -3.8649309 -3.6622791 -3.683984 -3.87963 -4.0293679 -4.1001153 -4.1127338 -4.059762 -3.9822092 -4.0204282][-4.3110924 -4.2955656 -4.2589726 -4.1892557 -4.0752931 -3.8982739 -3.7242389 -3.7497978 -3.9339614 -4.0802622 -4.146966 -4.1592369 -4.1228943 -4.0645604 -4.0840878][-4.310266 -4.2954679 -4.264873 -4.2070093 -4.1095505 -3.9613461 -3.8219805 -3.846518 -4.0047455 -4.1338782 -4.1886473 -4.1954241 -4.1742263 -4.1355004 -4.1421037][-4.3187704 -4.3036208 -4.2817268 -4.2345405 -4.1513591 -4.0292749 -3.9201851 -3.9439082 -4.0770221 -4.1851358 -4.2271647 -4.225563 -4.2105613 -4.186306 -4.1894894][-4.3394461 -4.3243442 -4.3077168 -4.2707119 -4.202527 -4.1049271 -4.0219345 -4.0459647 -4.1564832 -4.2445331 -4.2784066 -4.271997 -4.2587752 -4.2437148 -4.243762][-4.3605075 -4.3501835 -4.3384447 -4.3100138 -4.2568097 -4.182795 -4.1212568 -4.1368189 -4.2231045 -4.292974 -4.3246179 -4.3238606 -4.314147 -4.3049383 -4.3032036][-4.3632312 -4.3564777 -4.3487067 -4.3306093 -4.2908092 -4.2359581 -4.1934285 -4.2055573 -4.2710958 -4.3253808 -4.3547392 -4.3585372 -4.3542309 -4.3480263 -4.3456092]]...]
INFO - root - 2017-12-05 19:00:21.349231: step 35810, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.978 sec/batch; 80h:34m:51s remains)
INFO - root - 2017-12-05 19:00:30.658508: step 35820, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 72h:21m:45s remains)
INFO - root - 2017-12-05 19:00:40.082879: step 35830, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 78h:32m:59s remains)
INFO - root - 2017-12-05 19:00:49.420197: step 35840, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 73h:43m:10s remains)
INFO - root - 2017-12-05 19:00:58.861397: step 35850, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.992 sec/batch; 81h:43m:37s remains)
INFO - root - 2017-12-05 19:01:08.204781: step 35860, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 79h:09m:46s remains)
INFO - root - 2017-12-05 19:01:17.697858: step 35870, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.961 sec/batch; 79h:11m:12s remains)
INFO - root - 2017-12-05 19:01:27.062897: step 35880, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 74h:42m:35s remains)
INFO - root - 2017-12-05 19:01:36.568238: step 35890, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 81h:36m:51s remains)
INFO - root - 2017-12-05 19:01:46.066392: step 35900, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 78h:54m:10s remains)
2017-12-05 19:01:46.811930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.15225 -4.1691732 -4.1814075 -4.1835008 -4.1868453 -4.2018108 -4.2055793 -4.2001033 -4.1885929 -4.1962357 -4.2189627 -4.2367411 -4.2496595 -4.243751 -4.2192664][-4.1575708 -4.1667361 -4.1761093 -4.1744466 -4.1712275 -4.1770205 -4.1788173 -4.1779995 -4.1731176 -4.1851315 -4.2141886 -4.2444782 -4.2672148 -4.2631149 -4.233705][-4.1869836 -4.1794081 -4.1760926 -4.1703658 -4.1621318 -4.157959 -4.1539636 -4.1571269 -4.1591296 -4.1729736 -4.2044859 -4.2449789 -4.2793617 -4.2835655 -4.2573915][-4.2220068 -4.199995 -4.1780124 -4.1596713 -4.1446528 -4.1317096 -4.1233988 -4.128397 -4.1405792 -4.1603975 -4.197494 -4.24257 -4.28119 -4.2908192 -4.2714019][-4.2485409 -4.2180715 -4.1816635 -4.1516876 -4.1312575 -4.1115947 -4.092258 -4.0874877 -4.1002693 -4.1319213 -4.1812057 -4.2302713 -4.271009 -4.2872925 -4.2776866][-4.26637 -4.2354546 -4.19411 -4.158298 -4.1311955 -4.0985837 -4.0530648 -4.0177779 -4.0136409 -4.0632858 -4.1441941 -4.2090898 -4.2537131 -4.2740746 -4.272696][-4.2739477 -4.2510247 -4.2166805 -4.1820154 -4.1427832 -4.08823 -4.0097513 -3.9322207 -3.9034982 -3.9812059 -4.1023917 -4.1850915 -4.2350974 -4.2563753 -4.2627382][-4.2629285 -4.2556858 -4.2342081 -4.2034349 -4.1499329 -4.0793257 -3.9921656 -3.9043758 -3.8758545 -3.9646552 -4.0888004 -4.1717768 -4.2191 -4.2355232 -4.2413087][-4.2326608 -4.2403355 -4.2275343 -4.1977348 -4.1413636 -4.06941 -4.0014682 -3.9458318 -3.9325924 -3.9949024 -4.0824447 -4.1510849 -4.19582 -4.2130446 -4.2184844][-4.19816 -4.2105064 -4.2064209 -4.1833053 -4.1419621 -4.086195 -4.0407758 -4.0080671 -3.9960318 -4.0188985 -4.0610018 -4.1191688 -4.1695423 -4.1917629 -4.201313][-4.17004 -4.1816373 -4.1810617 -4.1668682 -4.1442046 -4.1111455 -4.0829396 -4.06024 -4.0437016 -4.039835 -4.0566044 -4.1087232 -4.1617427 -4.1844754 -4.1866159][-4.1610594 -4.1690931 -4.1660814 -4.15517 -4.1376443 -4.1165481 -4.1019139 -4.0889969 -4.0743828 -4.067008 -4.0807643 -4.1223011 -4.1592894 -4.1680374 -4.1564813][-4.1718497 -4.1729393 -4.1658053 -4.1497374 -4.130671 -4.114213 -4.1149378 -4.1149945 -4.1068821 -4.1044135 -4.1174574 -4.1370273 -4.14225 -4.1322412 -4.1189985][-4.1912785 -4.1839728 -4.1751447 -4.1530118 -4.128231 -4.1181021 -4.1316948 -4.1456752 -4.1501379 -4.1514988 -4.1575336 -4.1542706 -4.1344805 -4.1136065 -4.1049285][-4.206634 -4.1918397 -4.182198 -4.158896 -4.135253 -4.132061 -4.1514397 -4.17328 -4.1818628 -4.1827703 -4.18419 -4.1693659 -4.1365767 -4.1122293 -4.1079068]]...]
INFO - root - 2017-12-05 19:01:56.441151: step 35910, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.957 sec/batch; 78h:49m:17s remains)
INFO - root - 2017-12-05 19:02:05.803042: step 35920, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 77h:28m:03s remains)
INFO - root - 2017-12-05 19:02:15.206947: step 35930, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 78h:51m:04s remains)
INFO - root - 2017-12-05 19:02:24.708136: step 35940, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.914 sec/batch; 75h:16m:28s remains)
INFO - root - 2017-12-05 19:02:34.006510: step 35950, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 79h:17m:11s remains)
INFO - root - 2017-12-05 19:02:43.619262: step 35960, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 78h:33m:17s remains)
INFO - root - 2017-12-05 19:02:53.009972: step 35970, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 69h:56m:18s remains)
INFO - root - 2017-12-05 19:03:02.421504: step 35980, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 77h:10m:33s remains)
INFO - root - 2017-12-05 19:03:11.756464: step 35990, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 78h:39m:09s remains)
INFO - root - 2017-12-05 19:03:21.307161: step 36000, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 81h:35m:11s remains)
2017-12-05 19:03:22.088223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2448492 -4.2394295 -4.226747 -4.2219887 -4.2291493 -4.2335467 -4.2244296 -4.214499 -4.2104964 -4.2102442 -4.211586 -4.2244897 -4.2431893 -4.2551255 -4.2499666][-4.2554727 -4.249125 -4.2325263 -4.2200484 -4.2153149 -4.2100554 -4.1977754 -4.1944981 -4.2110052 -4.2294655 -4.2392874 -4.2508259 -4.2619061 -4.26599 -4.25604][-4.2551227 -4.2584734 -4.2417755 -4.2200289 -4.1937022 -4.1674032 -4.1431904 -4.1428146 -4.1774964 -4.2183361 -4.2437243 -4.2608166 -4.2713943 -4.2717986 -4.2615447][-4.23602 -4.2565084 -4.2458215 -4.219182 -4.1718316 -4.1180525 -4.0703311 -4.0645442 -4.1155925 -4.1818013 -4.2280655 -4.2560806 -4.2683616 -4.2683258 -4.2568631][-4.2105641 -4.2495189 -4.2489066 -4.2183185 -4.1540203 -4.0691009 -3.9842825 -3.9659307 -4.0347075 -4.130415 -4.1975989 -4.239018 -4.2569585 -4.2610064 -4.2491994][-4.1805716 -4.2405906 -4.2547097 -4.220047 -4.1353965 -4.0103149 -3.8729525 -3.8405437 -3.9429371 -4.0731297 -4.1596184 -4.2122421 -4.2408323 -4.2520256 -4.2431893][-4.1491723 -4.2252331 -4.2564206 -4.2231994 -4.1272917 -3.9732008 -3.7795303 -3.7243376 -3.8699236 -4.0273209 -4.1266351 -4.1903019 -4.2277908 -4.2452269 -4.2411036][-4.154779 -4.2279224 -4.2673287 -4.2414036 -4.1571712 -4.0199389 -3.8395312 -3.7752333 -3.8988523 -4.0281882 -4.1136618 -4.1774583 -4.21899 -4.2414079 -4.2428951][-4.201086 -4.2496505 -4.281918 -4.2695508 -4.2150936 -4.1238151 -4.0079508 -3.9504085 -4.0048075 -4.0692649 -4.1133933 -4.1579986 -4.1986423 -4.2290907 -4.2414236][-4.2241068 -4.2532043 -4.2754521 -4.275146 -4.2478776 -4.1949964 -4.1264052 -4.0769348 -4.0803828 -4.0900569 -4.0960293 -4.1206636 -4.1635847 -4.2077656 -4.2348156][-4.222724 -4.2387848 -4.2546215 -4.2582765 -4.244812 -4.2160034 -4.1734295 -4.1277356 -4.0998659 -4.0718508 -4.0554752 -4.0738106 -4.12265 -4.1781087 -4.2195311][-4.2253275 -4.2304282 -4.2377887 -4.2393174 -4.2349348 -4.222096 -4.1953325 -4.1532378 -4.1079731 -4.0557537 -4.0231309 -4.0392761 -4.087265 -4.1447086 -4.1966219][-4.2311292 -4.2318621 -4.2366238 -4.2378798 -4.2389231 -4.2409959 -4.2272744 -4.1918683 -4.1473527 -4.0902815 -4.0550022 -4.0650468 -4.0981307 -4.1431231 -4.1956992][-4.2342634 -4.2411861 -4.2486658 -4.2526989 -4.2569847 -4.2658386 -4.2611332 -4.2330284 -4.1964054 -4.1512971 -4.127449 -4.1343031 -4.1501055 -4.1772881 -4.2205105][-4.2329664 -4.2483382 -4.2610178 -4.2671332 -4.2736754 -4.286324 -4.2851462 -4.262392 -4.2325115 -4.2013121 -4.189425 -4.198894 -4.2072477 -4.2238331 -4.2563829]]...]
INFO - root - 2017-12-05 19:03:31.558505: step 36010, loss = 2.06, batch loss = 2.01 (7.9 examples/sec; 1.017 sec/batch; 83h:47m:47s remains)
INFO - root - 2017-12-05 19:03:40.956841: step 36020, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 73h:26m:57s remains)
INFO - root - 2017-12-05 19:03:50.260276: step 36030, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 78h:13m:08s remains)
INFO - root - 2017-12-05 19:03:59.779624: step 36040, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.945 sec/batch; 77h:46m:57s remains)
INFO - root - 2017-12-05 19:04:09.257887: step 36050, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 78h:10m:40s remains)
INFO - root - 2017-12-05 19:04:18.769409: step 36060, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 77h:20m:09s remains)
INFO - root - 2017-12-05 19:04:28.243231: step 36070, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 78h:44m:55s remains)
INFO - root - 2017-12-05 19:04:37.887737: step 36080, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 79h:18m:03s remains)
INFO - root - 2017-12-05 19:04:47.544707: step 36090, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.972 sec/batch; 80h:02m:02s remains)
INFO - root - 2017-12-05 19:04:56.759688: step 36100, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 71h:11m:11s remains)
2017-12-05 19:04:57.528512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2122307 -4.2254963 -4.237484 -4.2377462 -4.2335215 -4.2330751 -4.2434049 -4.25867 -4.2690535 -4.2805381 -4.2953725 -4.30481 -4.3098068 -4.3073711 -4.2954788][-4.1455464 -4.1661606 -4.1874881 -4.1912026 -4.1849828 -4.1823206 -4.1948652 -4.2172275 -4.2338405 -4.2484589 -4.2681575 -4.2848392 -4.2970352 -4.2973514 -4.2836885][-4.1146502 -4.1359644 -4.15834 -4.1617479 -4.1510448 -4.1443043 -4.1576815 -4.1842856 -4.2062092 -4.2248049 -4.2463784 -4.2658496 -4.2841859 -4.2893066 -4.2780962][-4.1105042 -4.1258922 -4.1447983 -4.1442 -4.13058 -4.1209097 -4.1298809 -4.1528654 -4.1785841 -4.2005887 -4.2238297 -4.2456989 -4.2693176 -4.2803917 -4.2747803][-4.1296167 -4.1374931 -4.15101 -4.1465826 -4.1309996 -4.1192455 -4.1194372 -4.1320033 -4.1571236 -4.183445 -4.207727 -4.2302332 -4.2562962 -4.2734103 -4.275836][-4.1673303 -4.1683354 -4.1767473 -4.1725273 -4.1543927 -4.1354651 -4.1242309 -4.1264796 -4.1506596 -4.1811919 -4.2050209 -4.2257309 -4.2533121 -4.2731953 -4.27925][-4.2172771 -4.2084804 -4.2085528 -4.2047057 -4.1839395 -4.1517944 -4.1252627 -4.1170888 -4.138505 -4.175663 -4.2045603 -4.2248707 -4.25405 -4.2759051 -4.2823315][-4.2587628 -4.240778 -4.2334805 -4.2278266 -4.2061644 -4.1655593 -4.1237679 -4.10179 -4.1187124 -4.1617155 -4.1998458 -4.22408 -4.2545009 -4.278 -4.2841988][-4.2730579 -4.2539167 -4.2457724 -4.2417665 -4.2238803 -4.18059 -4.1302967 -4.0991874 -4.1086731 -4.151854 -4.194953 -4.2228923 -4.2523446 -4.2753582 -4.2811208][-4.2689648 -4.2522922 -4.2481585 -4.2490611 -4.2397637 -4.200912 -4.14991 -4.1154308 -4.1183224 -4.1548319 -4.1927705 -4.2158976 -4.2401905 -4.2599287 -4.2652373][-4.2589893 -4.2457261 -4.24571 -4.250946 -4.2502 -4.2217026 -4.1812634 -4.1511631 -4.1492047 -4.1730247 -4.196218 -4.2052679 -4.2197828 -4.2358589 -4.242094][-4.2391181 -4.2315035 -4.2353749 -4.2424793 -4.2497663 -4.2358084 -4.2097459 -4.1832967 -4.175674 -4.186677 -4.1960387 -4.1913753 -4.1969481 -4.2117271 -4.2217379][-4.2164364 -4.2137856 -4.2202659 -4.2266459 -4.233676 -4.2297244 -4.2105908 -4.1847596 -4.1764059 -4.1843891 -4.186583 -4.1757731 -4.17872 -4.1953754 -4.2103395][-4.1967535 -4.19712 -4.2066717 -4.2129111 -4.2144918 -4.2146277 -4.1992445 -4.1734447 -4.1668482 -4.1773129 -4.177927 -4.1671214 -4.1710739 -4.1917615 -4.2110343][-4.1890364 -4.1959777 -4.2119889 -4.2163229 -4.2105212 -4.2085681 -4.1936426 -4.1663222 -4.1556249 -4.1698422 -4.1769004 -4.1713581 -4.1766014 -4.2011666 -4.2231669]]...]
INFO - root - 2017-12-05 19:05:06.924334: step 36110, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 80h:35m:03s remains)
INFO - root - 2017-12-05 19:05:16.441615: step 36120, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 79h:04m:05s remains)
INFO - root - 2017-12-05 19:05:25.711929: step 36130, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 81h:03m:00s remains)
INFO - root - 2017-12-05 19:05:35.101250: step 36140, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 80h:29m:12s remains)
INFO - root - 2017-12-05 19:05:44.699689: step 36150, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 77h:10m:52s remains)
INFO - root - 2017-12-05 19:05:53.990477: step 36160, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 78h:43m:42s remains)
INFO - root - 2017-12-05 19:06:03.272063: step 36170, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.989 sec/batch; 81h:26m:36s remains)
INFO - root - 2017-12-05 19:06:12.725007: step 36180, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 76h:54m:15s remains)
INFO - root - 2017-12-05 19:06:21.958593: step 36190, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 80h:41m:52s remains)
INFO - root - 2017-12-05 19:06:31.123918: step 36200, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 0.812 sec/batch; 66h:48m:21s remains)
2017-12-05 19:06:31.941702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2794971 -4.2860847 -4.2792096 -4.2526112 -4.2000284 -4.1003065 -3.9723902 -3.8672962 -3.8916163 -4.0032125 -4.1212654 -4.2204742 -4.2907677 -4.3298593 -4.3460107][-4.2776179 -4.2775106 -4.2651105 -4.2365923 -4.1858015 -4.0881748 -3.9594817 -3.8508735 -3.8769011 -3.9906816 -4.1116734 -4.2151203 -4.2892914 -4.3303442 -4.3471541][-4.2694063 -4.2576632 -4.2402639 -4.2140284 -4.1743183 -4.0944276 -3.9883056 -3.900723 -3.926621 -4.0275497 -4.136549 -4.2293949 -4.2968974 -4.3340826 -4.3487577][-4.2636881 -4.2425008 -4.2193656 -4.1960039 -4.1699252 -4.108572 -4.0240617 -3.9573808 -3.9827914 -4.0717649 -4.1684103 -4.2482319 -4.3060384 -4.337698 -4.3498664][-4.266861 -4.2451611 -4.2208486 -4.1979389 -4.1751046 -4.1204209 -4.0435257 -3.9829223 -4.0091071 -4.0954676 -4.1873875 -4.2607174 -4.3122725 -4.3399067 -4.3503218][-4.2706256 -4.2544842 -4.23312 -4.2090454 -4.1802115 -4.121726 -4.0407391 -3.9766679 -4.0066304 -4.0959353 -4.1893792 -4.2637224 -4.3151813 -4.341063 -4.3500028][-4.2654133 -4.2581086 -4.2442193 -4.2212682 -4.1855836 -4.1183214 -4.026073 -3.9543514 -3.9886618 -4.0833621 -4.1813 -4.2602339 -4.3150406 -4.3414326 -4.34992][-4.255724 -4.2569604 -4.2526374 -4.2346292 -4.1940732 -4.1167955 -4.0112791 -3.9276907 -3.9630985 -4.06361 -4.1690965 -4.2541351 -4.312386 -4.3407388 -4.3501382][-4.2488775 -4.2538452 -4.2561517 -4.2427454 -4.1993084 -4.1145635 -3.9981964 -3.9062352 -3.9410379 -4.0452838 -4.1572671 -4.2483759 -4.3101416 -4.3403177 -4.3507357][-4.2482953 -4.2511163 -4.2552986 -4.2455864 -4.204452 -4.1205297 -4.0038671 -3.9110541 -3.9423857 -4.0424957 -4.1542182 -4.2469649 -4.3097339 -4.339951 -4.3508258][-4.2511272 -4.251039 -4.2546844 -4.2474384 -4.212709 -4.1372623 -4.0314231 -3.947022 -3.9742825 -4.0615239 -4.1636844 -4.251976 -4.3118911 -4.3402572 -4.350534][-4.2511673 -4.2510238 -4.2528796 -4.247931 -4.223444 -4.1587029 -4.0629106 -3.9847641 -4.0077496 -4.0816512 -4.1753273 -4.2593794 -4.3161182 -4.3419204 -4.3503594][-4.2392778 -4.2392654 -4.23823 -4.2347379 -4.2193761 -4.1630797 -4.0726237 -3.9944472 -4.0136552 -4.0833983 -4.1771688 -4.262692 -4.3186388 -4.3431296 -4.3502398][-4.2173085 -4.217258 -4.2118926 -4.2062006 -4.1937714 -4.140614 -4.0490904 -3.9707522 -3.992913 -4.0698195 -4.1705165 -4.2618217 -4.3177795 -4.3420267 -4.3494248][-4.1985626 -4.1990471 -4.1924644 -4.187449 -4.1761851 -4.1246257 -4.0323277 -3.9572923 -3.985445 -4.0672622 -4.1705918 -4.26316 -4.3167715 -4.339344 -4.3470726]]...]
INFO - root - 2017-12-05 19:06:41.452016: step 36210, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 78h:07m:35s remains)
INFO - root - 2017-12-05 19:06:50.758589: step 36220, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 80h:48m:35s remains)
INFO - root - 2017-12-05 19:07:00.233623: step 36230, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 76h:18m:09s remains)
INFO - root - 2017-12-05 19:07:09.432382: step 36240, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.939 sec/batch; 77h:14m:28s remains)
INFO - root - 2017-12-05 19:07:18.862841: step 36250, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.913 sec/batch; 75h:08m:23s remains)
INFO - root - 2017-12-05 19:07:28.191248: step 36260, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 77h:33m:46s remains)
INFO - root - 2017-12-05 19:07:37.759099: step 36270, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.996 sec/batch; 81h:55m:41s remains)
INFO - root - 2017-12-05 19:07:47.165316: step 36280, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 68h:37m:32s remains)
INFO - root - 2017-12-05 19:07:56.425835: step 36290, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 73h:40m:35s remains)
INFO - root - 2017-12-05 19:08:05.810786: step 36300, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 76h:38m:27s remains)
2017-12-05 19:08:06.580762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3221478 -4.3149271 -4.3090281 -4.303298 -4.2973733 -4.293499 -4.2919354 -4.291821 -4.2933459 -4.29521 -4.2986789 -4.3017135 -4.3055472 -4.3088617 -4.3154678][-4.3042388 -4.290915 -4.2799339 -4.2703438 -4.2603669 -4.2506056 -4.2433152 -4.2394538 -4.2374482 -4.2383194 -4.2467403 -4.2587924 -4.271894 -4.2828197 -4.2969012][-4.279057 -4.2579875 -4.2413459 -4.2275152 -4.2140908 -4.1963758 -4.1770105 -4.1675119 -4.1671114 -4.1725917 -4.1893139 -4.2130036 -4.237792 -4.2582426 -4.2772732][-4.2505293 -4.2227321 -4.2021608 -4.1860147 -4.1716909 -4.1483321 -4.1180706 -4.1088872 -4.1216373 -4.1382818 -4.1593046 -4.1838989 -4.2154083 -4.2389326 -4.2577047][-4.2231507 -4.1914163 -4.1682181 -4.1503057 -4.1317167 -4.0956306 -4.0538869 -4.0522065 -4.0889173 -4.1242986 -4.1413345 -4.157434 -4.1908813 -4.2179852 -4.2348032][-4.1978173 -4.1618576 -4.1328773 -4.1084757 -4.0799465 -4.02567 -3.9683967 -3.979084 -4.0422025 -4.1008868 -4.1184459 -4.1234837 -4.1547861 -4.1841407 -4.2008791][-4.1702404 -4.1274786 -4.0937614 -4.0618138 -4.0172381 -3.9349751 -3.8556924 -3.8816235 -3.9797406 -4.0681543 -4.094902 -4.0942836 -4.1260395 -4.1587286 -4.1808577][-4.1513004 -4.1070786 -4.0722427 -4.0379581 -3.9854808 -3.8881505 -3.8006105 -3.8360589 -3.9561155 -4.0653772 -4.0975566 -4.0960522 -4.1282692 -4.1614981 -4.1834598][-4.1530972 -4.1156888 -4.0845895 -4.0574412 -4.019866 -3.9505522 -3.895015 -3.9260461 -4.0292 -4.1263943 -4.15156 -4.1497526 -4.1771297 -4.2053237 -4.2181025][-4.1694112 -4.1397963 -4.1127596 -4.0902653 -4.067555 -4.0329175 -4.0123596 -4.0394645 -4.1149025 -4.1939554 -4.2148447 -4.2143383 -4.2376852 -4.2608891 -4.2645135][-4.1949415 -4.1699386 -4.1434646 -4.121542 -4.1092587 -4.0992246 -4.1002707 -4.1269484 -4.180831 -4.2375941 -4.2522359 -4.25544 -4.279768 -4.3045959 -4.3010283][-4.2234888 -4.2010436 -4.175777 -4.1576567 -4.153316 -4.1566048 -4.1664023 -4.1862521 -4.2195973 -4.2521815 -4.2618704 -4.2710257 -4.3015203 -4.3293614 -4.3237543][-4.253788 -4.2346749 -4.2146349 -4.2029362 -4.2047834 -4.2137103 -4.2236533 -4.2325864 -4.245554 -4.2558618 -4.2593422 -4.2718868 -4.3040094 -4.3330994 -4.3313541][-4.2849607 -4.2708974 -4.2554722 -4.2462258 -4.2480397 -4.2554846 -4.2617149 -4.2625985 -4.2624922 -4.2603636 -4.2603564 -4.2730918 -4.299963 -4.3235941 -4.3269663][-4.3144274 -4.3071127 -4.2972937 -4.2901516 -4.2891121 -4.2899857 -4.2894907 -4.2856727 -4.281179 -4.2774224 -4.2771635 -4.2866931 -4.3028674 -4.3174911 -4.3232679]]...]
INFO - root - 2017-12-05 19:08:16.021200: step 36310, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 76h:59m:27s remains)
INFO - root - 2017-12-05 19:08:25.648193: step 36320, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 79h:17m:55s remains)
INFO - root - 2017-12-05 19:08:34.765978: step 36330, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 78h:06m:37s remains)
INFO - root - 2017-12-05 19:08:44.334626: step 36340, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 79h:25m:58s remains)
INFO - root - 2017-12-05 19:08:53.568148: step 36350, loss = 2.11, batch loss = 2.05 (9.4 examples/sec; 0.854 sec/batch; 70h:13m:39s remains)
INFO - root - 2017-12-05 19:09:02.987751: step 36360, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 77h:29m:58s remains)
INFO - root - 2017-12-05 19:09:12.608708: step 36370, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 80h:22m:40s remains)
INFO - root - 2017-12-05 19:09:22.068557: step 36380, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 80h:35m:17s remains)
INFO - root - 2017-12-05 19:09:31.499020: step 36390, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 80h:46m:30s remains)
INFO - root - 2017-12-05 19:09:41.037414: step 36400, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 75h:44m:42s remains)
2017-12-05 19:09:41.919293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2798924 -4.2856569 -4.2928319 -4.2956963 -4.2946029 -4.2912478 -4.2885838 -4.2888465 -4.2919722 -4.2974639 -4.3040156 -4.3134785 -4.3220062 -4.325603 -4.3161554][-4.2811565 -4.2873349 -4.293283 -4.2896867 -4.2803879 -4.2701497 -4.2645788 -4.2679157 -4.2772956 -4.285902 -4.2924538 -4.3018308 -4.3094287 -4.3144875 -4.3098636][-4.2710671 -4.2776117 -4.2799439 -4.2692003 -4.2524686 -4.2340689 -4.2263689 -4.2350755 -4.2527781 -4.2673054 -4.2750716 -4.2830906 -4.2853832 -4.2855935 -4.2827373][-4.2450786 -4.2453413 -4.2387142 -4.2198925 -4.1989818 -4.17074 -4.1595974 -4.1728749 -4.1976848 -4.2202415 -4.2369652 -4.2519951 -4.2540627 -4.247025 -4.2395625][-4.2039695 -4.1931648 -4.1711049 -4.140089 -4.1099319 -4.0677581 -4.0496745 -4.0650668 -4.0954614 -4.1286969 -4.1610289 -4.1935096 -4.201004 -4.1844945 -4.168561][-4.1640573 -4.138082 -4.0990229 -4.0572267 -4.0208817 -3.9619813 -3.9245286 -3.9323559 -3.9610698 -3.9968457 -4.04952 -4.1084914 -4.122716 -4.0948086 -4.0674925][-4.1316795 -4.094038 -4.0448971 -3.9964051 -3.9581842 -3.8861904 -3.8218498 -3.8185441 -3.8412583 -3.8687451 -3.9378693 -4.0313592 -4.0580211 -4.0136361 -3.9723179][-4.1078944 -4.0691557 -4.0274334 -3.987772 -3.960943 -3.89761 -3.8256888 -3.8118525 -3.821521 -3.838989 -3.9085531 -4.0088677 -4.035367 -3.9728334 -3.9185643][-4.1005616 -4.0832863 -4.0761328 -4.0644159 -4.0618639 -4.0368481 -3.992996 -3.971488 -3.9611955 -3.9658282 -4.0159969 -4.0860085 -4.0947695 -4.0268612 -3.9738903][-4.0847383 -4.0917225 -4.1162395 -4.1301918 -4.1418843 -4.1436443 -4.1314783 -4.1121693 -4.0959296 -4.0969448 -4.1352983 -4.1818929 -4.1870613 -4.1381383 -4.0973144][-4.0668712 -4.086545 -4.1254029 -4.1525993 -4.1683064 -4.1808143 -4.1888113 -4.179749 -4.1641579 -4.1640768 -4.191164 -4.2216916 -4.2283621 -4.2042146 -4.1794062][-4.0640807 -4.0880914 -4.1327615 -4.1629267 -4.1792197 -4.1927161 -4.2055435 -4.2030311 -4.1876769 -4.1843519 -4.1959682 -4.2090011 -4.2147231 -4.2084274 -4.1964908][-4.0967789 -4.1140084 -4.1482606 -4.1679773 -4.17989 -4.1904411 -4.1991181 -4.1965861 -4.1843944 -4.1812716 -4.1829762 -4.1862841 -4.1901641 -4.1890974 -4.1807971][-4.1566825 -4.1645021 -4.1847415 -4.1926727 -4.201654 -4.2094197 -4.2141833 -4.2090564 -4.1974483 -4.1921182 -4.1854367 -4.1814713 -4.1826973 -4.1829152 -4.1792159][-4.21503 -4.2198896 -4.2315331 -4.2338071 -4.2402658 -4.2446575 -4.2460032 -4.241293 -4.2318544 -4.2254267 -4.2140636 -4.2033734 -4.2007709 -4.2021308 -4.2007141]]...]
INFO - root - 2017-12-05 19:09:51.280325: step 36410, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 80h:50m:21s remains)
INFO - root - 2017-12-05 19:10:00.563790: step 36420, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 77h:30m:33s remains)
INFO - root - 2017-12-05 19:10:09.916530: step 36430, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 79h:23m:51s remains)
INFO - root - 2017-12-05 19:10:19.052367: step 36440, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.910 sec/batch; 74h:50m:45s remains)
INFO - root - 2017-12-05 19:10:28.348309: step 36450, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 76h:19m:18s remains)
INFO - root - 2017-12-05 19:10:37.547311: step 36460, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 69h:40m:12s remains)
INFO - root - 2017-12-05 19:10:47.007523: step 36470, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 75h:01m:29s remains)
INFO - root - 2017-12-05 19:10:56.318935: step 36480, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 74h:30m:22s remains)
INFO - root - 2017-12-05 19:11:05.798039: step 36490, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 75h:41m:42s remains)
INFO - root - 2017-12-05 19:11:15.206708: step 36500, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 78h:17m:08s remains)
2017-12-05 19:11:16.034349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2439885 -4.2565823 -4.2783837 -4.300745 -4.3164835 -4.317256 -4.2932682 -4.2587938 -4.2437372 -4.2523184 -4.2797933 -4.3072033 -4.3296404 -4.3421159 -4.34646][-4.222908 -4.2377949 -4.2635164 -4.2878652 -4.3072014 -4.3115811 -4.2904997 -4.2584057 -4.2460084 -4.2567973 -4.2845306 -4.3106189 -4.3312564 -4.342474 -4.3456235][-4.1962852 -4.2089591 -4.2352457 -4.2581682 -4.2766347 -4.2819567 -4.262435 -4.2344551 -4.2295609 -4.2492118 -4.2807951 -4.3070683 -4.3274221 -4.339973 -4.3429713][-4.160367 -4.168169 -4.1926785 -4.2155552 -4.2328181 -4.2355862 -4.2117414 -4.1837111 -4.1896138 -4.222959 -4.2601652 -4.2872462 -4.311595 -4.3293605 -4.3348742][-4.1230774 -4.1208348 -4.1398778 -4.161654 -4.1794739 -4.1773529 -4.1422834 -4.109755 -4.1291251 -4.1779919 -4.222105 -4.2537808 -4.2871065 -4.313086 -4.3228059][-4.0880337 -4.0733657 -4.08154 -4.100812 -4.120007 -4.11223 -4.06434 -4.0224252 -4.0555916 -4.125773 -4.1849785 -4.2263165 -4.2696738 -4.3010683 -4.3115125][-4.0669327 -4.0409718 -4.0360518 -4.0470009 -4.0600748 -4.0433531 -3.9861753 -3.9429619 -3.990772 -4.0817785 -4.1589956 -4.2155209 -4.2663808 -4.2991452 -4.3080211][-4.085907 -4.0572295 -4.0454006 -4.0461621 -4.0485597 -4.0245118 -3.9732828 -3.9448884 -3.9971728 -4.0888071 -4.1686726 -4.2302871 -4.2802129 -4.3097005 -4.3151903][-4.1392565 -4.1207438 -4.1131396 -4.1073418 -4.1004906 -4.0768237 -4.0448055 -4.0310516 -4.0686512 -4.1332192 -4.1940522 -4.2469831 -4.2905803 -4.3169703 -4.3209858][-4.178165 -4.1690149 -4.1649833 -4.1554303 -4.141923 -4.1235232 -4.1098633 -4.1053181 -4.1254439 -4.1635089 -4.2057505 -4.2502546 -4.2893677 -4.3146229 -4.3208041][-4.1837735 -4.1777277 -4.1732669 -4.1615767 -4.1487031 -4.1381149 -4.1335745 -4.1285372 -4.1369228 -4.1630182 -4.1994429 -4.242064 -4.2803421 -4.3050532 -4.3143172][-4.1581945 -4.1521606 -4.1452532 -4.1332831 -4.121664 -4.1146421 -4.1114244 -4.1061764 -4.1112528 -4.1348805 -4.1736169 -4.2196627 -4.2627292 -4.2917128 -4.3058147][-4.1183391 -4.111969 -4.106782 -4.0967016 -4.0852752 -4.07731 -4.0751143 -4.0729628 -4.0810795 -4.1080909 -4.1528125 -4.2047639 -4.251698 -4.284853 -4.3018718][-4.1036544 -4.0990634 -4.0998349 -4.0964236 -4.08918 -4.0827827 -4.0818515 -4.0806556 -4.0902014 -4.118053 -4.1632595 -4.2162161 -4.2612228 -4.2919388 -4.306952][-4.1468458 -4.1459718 -4.1506548 -4.1530852 -4.1520176 -4.1488028 -4.1486573 -4.1476398 -4.155973 -4.1790867 -4.215836 -4.2575583 -4.29143 -4.3120422 -4.3205462]]...]
INFO - root - 2017-12-05 19:11:25.348325: step 36510, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 71h:02m:31s remains)
INFO - root - 2017-12-05 19:11:34.903939: step 36520, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 79h:47m:45s remains)
INFO - root - 2017-12-05 19:11:44.113583: step 36530, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 73h:24m:02s remains)
INFO - root - 2017-12-05 19:11:53.542426: step 36540, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 76h:47m:35s remains)
INFO - root - 2017-12-05 19:12:02.971834: step 36550, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 77h:06m:05s remains)
INFO - root - 2017-12-05 19:12:12.411250: step 36560, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 72h:25m:59s remains)
INFO - root - 2017-12-05 19:12:21.728349: step 36570, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 74h:57m:53s remains)
INFO - root - 2017-12-05 19:12:31.101258: step 36580, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 76h:46m:00s remains)
INFO - root - 2017-12-05 19:12:40.594226: step 36590, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 79h:04m:18s remains)
INFO - root - 2017-12-05 19:12:50.039891: step 36600, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 70h:28m:11s remains)
2017-12-05 19:12:50.814462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2873526 -4.2740083 -4.2709665 -4.2761459 -4.2847204 -4.2918844 -4.2957511 -4.2980223 -4.2997012 -4.3028307 -4.3065968 -4.3092284 -4.3097072 -4.3103781 -4.3127732][-4.253643 -4.2341428 -4.2327023 -4.2435493 -4.2589822 -4.271811 -4.2772136 -4.2799082 -4.2843881 -4.2924967 -4.3005471 -4.3059692 -4.3068595 -4.3056688 -4.3063779][-4.2148075 -4.1915908 -4.1930895 -4.2105479 -4.2358 -4.2547159 -4.2585177 -4.256382 -4.2616377 -4.2768068 -4.2921014 -4.30254 -4.3045917 -4.3009162 -4.2982831][-4.18151 -4.1565847 -4.1591253 -4.1794209 -4.2081985 -4.2266488 -4.2221866 -4.2112565 -4.2177963 -4.2437897 -4.2697983 -4.2872672 -4.2914534 -4.2865191 -4.28181][-4.1643896 -4.1393027 -4.1391144 -4.1549478 -4.1762815 -4.1816974 -4.1597791 -4.1354485 -4.1436257 -4.1837258 -4.2249241 -4.2533145 -4.2639637 -4.2615137 -4.2569289][-4.1683507 -4.1435447 -4.1375093 -4.1407576 -4.1420407 -4.1187744 -4.0634432 -4.018034 -4.0325665 -4.0946565 -4.1543074 -4.1951432 -4.2150645 -4.221045 -4.2237978][-4.1844311 -4.161159 -4.1506338 -4.1402984 -4.1165738 -4.0566864 -3.9578619 -3.8847113 -3.9133711 -4.0039721 -4.0824795 -4.1362014 -4.1683946 -4.185617 -4.19628][-4.1981473 -4.1781993 -4.1707287 -4.1597815 -4.1237526 -4.0417614 -3.9164267 -3.8285532 -3.8682055 -3.9691885 -4.0468554 -4.1036644 -4.1460476 -4.1710343 -4.1849504][-4.2087426 -4.1950812 -4.1949787 -4.1929293 -4.1647325 -4.0929694 -3.9889169 -3.9226379 -3.9519536 -4.020647 -4.0675445 -4.1080046 -4.1503172 -4.1788664 -4.1925445][-4.22557 -4.218 -4.2227359 -4.2257752 -4.2078681 -4.1574416 -4.0865369 -4.0461364 -4.0677381 -4.1085653 -4.1324158 -4.15445 -4.1842771 -4.2083635 -4.216579][-4.2522354 -4.2489333 -4.2548909 -4.2578773 -4.2464342 -4.2106395 -4.1605692 -4.135015 -4.1518817 -4.1766663 -4.189023 -4.2016029 -4.2218847 -4.236701 -4.2373776][-4.2804189 -4.2798233 -4.2838473 -4.2836685 -4.2724872 -4.244741 -4.2072787 -4.1901178 -4.203793 -4.2176805 -4.2231488 -4.23313 -4.2464685 -4.2543621 -4.2506509][-4.3039026 -4.3046255 -4.3062506 -4.3032956 -4.2910647 -4.2692513 -4.2418203 -4.2281423 -4.2337642 -4.2394104 -4.243156 -4.253984 -4.2662191 -4.2725625 -4.2710028][-4.3188043 -4.3196673 -4.3207726 -4.3186107 -4.3098097 -4.2961383 -4.2797556 -4.2700162 -4.2693477 -4.2701411 -4.2723069 -4.2817445 -4.2911224 -4.2958264 -4.2971206][-4.327497 -4.32825 -4.3295708 -4.3295932 -4.3256011 -4.318346 -4.31069 -4.3057575 -4.3039494 -4.3037648 -4.3051367 -4.3089623 -4.31197 -4.312037 -4.311996]]...]
INFO - root - 2017-12-05 19:13:00.235154: step 36610, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 76h:50m:44s remains)
INFO - root - 2017-12-05 19:13:09.457726: step 36620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 70h:06m:03s remains)
INFO - root - 2017-12-05 19:13:18.804109: step 36630, loss = 2.10, batch loss = 2.05 (8.1 examples/sec; 0.982 sec/batch; 80h:43m:19s remains)
INFO - root - 2017-12-05 19:13:28.279570: step 36640, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 77h:28m:58s remains)
INFO - root - 2017-12-05 19:13:37.537186: step 36650, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 81h:06m:37s remains)
INFO - root - 2017-12-05 19:13:46.778870: step 36660, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 78h:56m:38s remains)
INFO - root - 2017-12-05 19:13:56.514915: step 36670, loss = 2.03, batch loss = 1.97 (8.1 examples/sec; 0.982 sec/batch; 80h:42m:22s remains)
INFO - root - 2017-12-05 19:14:05.915771: step 36680, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 73h:03m:38s remains)
INFO - root - 2017-12-05 19:14:15.208126: step 36690, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 78h:32m:25s remains)
INFO - root - 2017-12-05 19:14:24.464725: step 36700, loss = 2.02, batch loss = 1.96 (8.6 examples/sec; 0.932 sec/batch; 76h:33m:13s remains)
2017-12-05 19:14:25.246869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2170486 -4.2017736 -4.179503 -4.1575141 -4.1493783 -4.1611691 -4.188592 -4.2177505 -4.2422352 -4.2610211 -4.2740431 -4.27925 -4.280611 -4.2782454 -4.2700009][-4.1952267 -4.1817546 -4.1605635 -4.1408491 -4.1358113 -4.1532316 -4.1852436 -4.2140622 -4.2388411 -4.2605987 -4.2770281 -4.2836332 -4.2837772 -4.2791986 -4.2689233][-4.1971645 -4.1897383 -4.1730666 -4.1565294 -4.1547704 -4.1723943 -4.1975765 -4.2178822 -4.2390866 -4.261425 -4.2790542 -4.285265 -4.2822657 -4.273335 -4.2601376][-4.2183862 -4.2201576 -4.20944 -4.1930966 -4.1844053 -4.1888666 -4.1971879 -4.2068267 -4.2245979 -4.2472425 -4.2664509 -4.2713604 -4.2648764 -4.25105 -4.2359529][-4.2224679 -4.2264686 -4.2181621 -4.2002039 -4.1830359 -4.1729927 -4.1657057 -4.1663523 -4.1798263 -4.2036781 -4.2264953 -4.2333078 -4.2261543 -4.2100296 -4.1951661][-4.1946588 -4.1942468 -4.1855674 -4.1679668 -4.1491451 -4.1307497 -4.1140456 -4.10597 -4.1166692 -4.1434932 -4.1684723 -4.1760421 -4.1694217 -4.1550632 -4.1433296][-4.150578 -4.1453748 -4.132503 -4.1120019 -4.0896559 -4.0665183 -4.0453787 -4.0364017 -4.0541563 -4.0859137 -4.1083364 -4.1110277 -4.1024971 -4.0929117 -4.0877795][-4.104146 -4.096868 -4.0809078 -4.0570397 -4.0243506 -3.9928505 -3.9729824 -3.9754934 -4.0089722 -4.0430732 -4.0570183 -4.0508676 -4.0409226 -4.038837 -4.0445094][-4.0783963 -4.0708742 -4.0560074 -4.0295954 -3.9873788 -3.9509625 -3.9413271 -3.9628825 -4.0082769 -4.0389214 -4.0413094 -4.0256634 -4.0135155 -4.0183682 -4.0359316][-4.0879488 -4.0813332 -4.0690279 -4.0451012 -4.006813 -3.9773533 -3.979069 -4.0074639 -4.0474114 -4.0677114 -4.0626774 -4.0460472 -4.0369167 -4.0486035 -4.0739408][-4.138464 -4.1319771 -4.1249676 -4.1102843 -4.0823126 -4.0613761 -4.0648608 -4.0853138 -4.1131878 -4.1282907 -4.1265874 -4.1181979 -4.1134009 -4.1222363 -4.1412435][-4.206418 -4.1991563 -4.1959 -4.1913943 -4.1748829 -4.159018 -4.1582527 -4.1696663 -4.1900377 -4.2049551 -4.2076216 -4.2029796 -4.1959767 -4.1941004 -4.1988072][-4.2566142 -4.2487984 -4.247643 -4.2474318 -4.2384152 -4.2274194 -4.2245593 -4.2321482 -4.2478414 -4.2611356 -4.2641454 -4.2590766 -4.248836 -4.2384024 -4.2330804][-4.2704878 -4.2620945 -4.2606549 -4.2606792 -4.2558036 -4.2493191 -4.2461195 -4.2513971 -4.2654743 -4.2789536 -4.2842937 -4.2810459 -4.2704883 -4.2570748 -4.2457581][-4.2500682 -4.2427521 -4.242228 -4.2424884 -4.2405076 -4.2364397 -4.2331862 -4.23818 -4.2533717 -4.2686243 -4.2768788 -4.2772889 -4.2695665 -4.2564068 -4.2414594]]...]
INFO - root - 2017-12-05 19:14:34.532881: step 36710, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 76h:44m:44s remains)
INFO - root - 2017-12-05 19:14:43.888015: step 36720, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.961 sec/batch; 78h:55m:44s remains)
INFO - root - 2017-12-05 19:14:53.380622: step 36730, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 77h:08m:54s remains)
INFO - root - 2017-12-05 19:15:02.671229: step 36740, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 72h:24m:09s remains)
INFO - root - 2017-12-05 19:15:11.938031: step 36750, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 79h:43m:02s remains)
INFO - root - 2017-12-05 19:15:21.368768: step 36760, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 77h:52m:32s remains)
INFO - root - 2017-12-05 19:15:31.030207: step 36770, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 78h:23m:09s remains)
INFO - root - 2017-12-05 19:15:40.344730: step 36780, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 82h:00m:31s remains)
INFO - root - 2017-12-05 19:15:49.644373: step 36790, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 75h:05m:43s remains)
INFO - root - 2017-12-05 19:15:59.238521: step 36800, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 77h:09m:35s remains)
2017-12-05 19:16:00.078633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2437663 -4.2151823 -4.2046423 -4.2230806 -4.2581687 -4.2902508 -4.3033633 -4.2968507 -4.2820268 -4.2587047 -4.22876 -4.1968956 -4.1604252 -4.13584 -4.1405864][-4.2071438 -4.171483 -4.1642141 -4.1919155 -4.2346411 -4.269383 -4.2815652 -4.2754574 -4.2637787 -4.2417636 -4.2085133 -4.1686974 -4.1147885 -4.0807657 -4.0957708][-4.1713476 -4.137629 -4.1393294 -4.1753492 -4.2177091 -4.2438197 -4.2459483 -4.2371831 -4.2289538 -4.21182 -4.1795535 -4.1389561 -4.0773187 -4.0388989 -4.0640545][-4.1631474 -4.1365733 -4.1451168 -4.1776757 -4.2045569 -4.2061558 -4.1863012 -4.1736784 -4.1765976 -4.1763806 -4.1552696 -4.1244273 -4.0693169 -4.0338616 -4.0652595][-4.185297 -4.1666584 -4.1726875 -4.1810942 -4.1720228 -4.1324286 -4.0784969 -4.0622296 -4.0885177 -4.1235104 -4.1371279 -4.1376214 -4.1053019 -4.0793281 -4.108129][-4.2159114 -4.19683 -4.186965 -4.1590009 -4.10174 -4.0028076 -3.8936377 -3.8708148 -3.9488688 -4.0436954 -4.1062212 -4.1464386 -4.1453347 -4.1344233 -4.156992][-4.2340221 -4.2120085 -4.1861658 -4.1317253 -4.0394721 -3.8815141 -3.6986828 -3.6579723 -3.8068929 -3.969692 -4.0791717 -4.1505194 -4.1715307 -4.1728835 -4.1940465][-4.2326035 -4.21669 -4.19218 -4.136332 -4.0457644 -3.8824069 -3.681206 -3.6326509 -3.799638 -3.9736397 -4.0883284 -4.1619706 -4.185658 -4.191751 -4.2093611][-4.223618 -4.2249789 -4.2139263 -4.1748128 -4.1180782 -4.01009 -3.8651977 -3.8253527 -3.9399939 -4.0597763 -4.1352468 -4.1833987 -4.1926317 -4.195188 -4.2097244][-4.1894789 -4.2060189 -4.21275 -4.2021332 -4.1818023 -4.121295 -4.0333672 -4.01043 -4.081811 -4.1478753 -4.1787305 -4.1901608 -4.1759896 -4.1634512 -4.1756072][-4.1588106 -4.1845336 -4.2058539 -4.2148795 -4.2192826 -4.1947837 -4.15015 -4.1422691 -4.1776471 -4.1996093 -4.1957297 -4.1765261 -4.1402483 -4.1143713 -4.1238][-4.1654735 -4.1929255 -4.2189684 -4.2367797 -4.2524838 -4.2488661 -4.2323122 -4.2320533 -4.2407455 -4.2294726 -4.1983547 -4.159903 -4.1154032 -4.0855613 -4.0951219][-4.1883173 -4.2092991 -4.2335782 -4.2535567 -4.2755251 -4.284287 -4.2819328 -4.2832251 -4.2744946 -4.243906 -4.19839 -4.1555624 -4.1178532 -4.0948057 -4.1070595][-4.2155447 -4.2269783 -4.2435956 -4.2597528 -4.281611 -4.2967033 -4.3013654 -4.3010383 -4.2848349 -4.2523775 -4.2113075 -4.1774035 -4.1533861 -4.1379733 -4.1464133][-4.245924 -4.2502985 -4.2584119 -4.2670221 -4.2800775 -4.2930551 -4.3013415 -4.3012791 -4.2883663 -4.2673411 -4.2423763 -4.2211123 -4.2066855 -4.1942148 -4.1942282]]...]
INFO - root - 2017-12-05 19:16:09.416503: step 36810, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.938 sec/batch; 77h:00m:54s remains)
INFO - root - 2017-12-05 19:16:18.782387: step 36820, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 80h:42m:40s remains)
INFO - root - 2017-12-05 19:16:28.149451: step 36830, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 77h:42m:16s remains)
INFO - root - 2017-12-05 19:16:37.421655: step 36840, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 77h:09m:22s remains)
INFO - root - 2017-12-05 19:16:46.511884: step 36850, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 74h:12m:28s remains)
INFO - root - 2017-12-05 19:16:55.938422: step 36860, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 76h:13m:22s remains)
INFO - root - 2017-12-05 19:17:05.316474: step 36870, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 75h:08m:31s remains)
INFO - root - 2017-12-05 19:17:14.590589: step 36880, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 74h:13m:46s remains)
INFO - root - 2017-12-05 19:17:24.029841: step 36890, loss = 2.03, batch loss = 1.98 (8.0 examples/sec; 0.997 sec/batch; 81h:52m:00s remains)
INFO - root - 2017-12-05 19:17:33.464287: step 36900, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 75h:16m:13s remains)
2017-12-05 19:17:34.170011: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1168804 -4.1594944 -4.2105355 -4.2310562 -4.2292438 -4.2249088 -4.2161522 -4.197228 -4.1819086 -4.17517 -4.1718025 -4.1666288 -4.1618814 -4.1488013 -4.1369152][-4.1473093 -4.1909542 -4.2408123 -4.2607532 -4.2560983 -4.2447076 -4.2203393 -4.1850929 -4.1625447 -4.1519184 -4.1466022 -4.141623 -4.1414943 -4.1379709 -4.1295629][-4.1866288 -4.2276559 -4.2676005 -4.277123 -4.2654819 -4.2434316 -4.2085567 -4.1655254 -4.144002 -4.1320114 -4.1191487 -4.1040249 -4.0998597 -4.1103415 -4.11716][-4.2046833 -4.2407236 -4.2735119 -4.2732449 -4.2468543 -4.2100792 -4.16836 -4.1311255 -4.1266084 -4.1238303 -4.1070051 -4.0798826 -4.0672731 -4.0824475 -4.10259][-4.2092304 -4.2350183 -4.2646818 -4.259603 -4.216526 -4.1525888 -4.0924997 -4.0621729 -4.0915437 -4.1228943 -4.1261148 -4.1043983 -4.0853844 -4.089004 -4.105494][-4.1930122 -4.2078757 -4.2379518 -4.2382331 -4.1866951 -4.0906897 -3.986413 -3.9376128 -4.0046735 -4.0932117 -4.1430688 -4.1458874 -4.1324749 -4.1288733 -4.1362524][-4.157155 -4.1540475 -4.1822672 -4.2003708 -4.1533909 -4.0285816 -3.8594391 -3.7577107 -3.8606863 -4.0196915 -4.1290512 -4.1668296 -4.1725783 -4.1726241 -4.1753721][-4.1045833 -4.07503 -4.1019998 -4.1448841 -4.1344981 -4.0269732 -3.8433535 -3.7008166 -3.8048484 -3.9910519 -4.1291542 -4.1929159 -4.2162066 -4.2189198 -4.215826][-4.0517592 -4.0009317 -4.0310512 -4.1041174 -4.1494212 -4.1082411 -3.9948211 -3.8832383 -3.9304845 -4.0620985 -4.1697965 -4.2299757 -4.2566004 -4.256278 -4.246542][-4.0531588 -4.0033908 -4.0320578 -4.1091657 -4.1823754 -4.1932931 -4.1446276 -4.0728459 -4.0766382 -4.1470675 -4.217452 -4.2638354 -4.2850747 -4.2765765 -4.254209][-4.0842466 -4.0541658 -4.0741353 -4.1324797 -4.2023206 -4.2371659 -4.2300067 -4.1961122 -4.1908894 -4.228 -4.2702131 -4.2959352 -4.2988753 -4.2771139 -4.2425475][-4.1485171 -4.1389637 -4.1553297 -4.1899371 -4.2364035 -4.2674956 -4.2776051 -4.2669272 -4.2648878 -4.2903886 -4.3117518 -4.3183694 -4.3064685 -4.2746062 -4.2342606][-4.2270823 -4.2303967 -4.2461586 -4.2644281 -4.28384 -4.2957788 -4.3017783 -4.2972522 -4.2984281 -4.315505 -4.3268776 -4.325911 -4.3145409 -4.2873845 -4.2505345][-4.2803087 -4.2870855 -4.2985034 -4.3082223 -4.3128629 -4.3102694 -4.3090911 -4.3043566 -4.3046083 -4.3167005 -4.3291259 -4.3334403 -4.3309827 -4.3095117 -4.2750959][-4.2971506 -4.3030939 -4.3128309 -4.3202276 -4.321527 -4.3155127 -4.3125858 -4.3081989 -4.3090668 -4.3168449 -4.326725 -4.3316298 -4.3308744 -4.3107204 -4.2800574]]...]
INFO - root - 2017-12-05 19:17:43.570440: step 36910, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 75h:36m:31s remains)
INFO - root - 2017-12-05 19:17:52.777127: step 36920, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 76h:53m:16s remains)
INFO - root - 2017-12-05 19:18:01.927451: step 36930, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 77h:58m:23s remains)
INFO - root - 2017-12-05 19:18:11.404139: step 36940, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 76h:23m:17s remains)
INFO - root - 2017-12-05 19:18:20.784716: step 36950, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 81h:48m:01s remains)
INFO - root - 2017-12-05 19:18:30.094990: step 36960, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.014 sec/batch; 83h:13m:21s remains)
INFO - root - 2017-12-05 19:18:39.493723: step 36970, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 73h:02m:06s remains)
INFO - root - 2017-12-05 19:18:48.648235: step 36980, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.953 sec/batch; 78h:12m:36s remains)
INFO - root - 2017-12-05 19:18:58.169564: step 36990, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 78h:48m:54s remains)
INFO - root - 2017-12-05 19:19:07.469480: step 37000, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.939 sec/batch; 77h:03m:42s remains)
2017-12-05 19:19:08.205237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2961378 -4.3068404 -4.312202 -4.311398 -4.3045278 -4.2927532 -4.2832756 -4.2775726 -4.2752137 -4.2730236 -4.2657619 -4.2745986 -4.2846451 -4.2809777 -4.2676821][-4.2912912 -4.2928619 -4.2879739 -4.2828231 -4.2758293 -4.2648673 -4.2578397 -4.2543507 -4.2505946 -4.2477 -4.2348814 -4.2440782 -4.2636981 -4.2701964 -4.2623415][-4.2756162 -4.2649841 -4.245851 -4.2354412 -4.232882 -4.2266736 -4.2239437 -4.2245822 -4.2218533 -4.218791 -4.2034554 -4.2095566 -4.2332196 -4.2474627 -4.2456603][-4.2663603 -4.2454095 -4.2111921 -4.193759 -4.1977344 -4.1986403 -4.20172 -4.2065611 -4.2059197 -4.2049904 -4.1928763 -4.1928768 -4.2082834 -4.2226162 -4.224484][-4.272717 -4.2504363 -4.2103539 -4.1896091 -4.1958418 -4.1985641 -4.201849 -4.2064633 -4.207479 -4.2083354 -4.1996393 -4.189733 -4.1914878 -4.1995029 -4.2030387][-4.2903872 -4.2784519 -4.2452374 -4.2224116 -4.2182331 -4.2080941 -4.2003269 -4.1997018 -4.2021489 -4.207037 -4.204339 -4.1899562 -4.1852245 -4.1893253 -4.1909161][-4.3020172 -4.2992492 -4.2703485 -4.2412858 -4.2209682 -4.1937127 -4.1765313 -4.1730804 -4.1806779 -4.19312 -4.2030492 -4.1977668 -4.1945934 -4.1976118 -4.1945672][-4.2971539 -4.3026133 -4.2765145 -4.240087 -4.2049036 -4.16499 -4.1410751 -4.1349339 -4.147923 -4.1675482 -4.1913867 -4.2014871 -4.2072268 -4.214776 -4.2146039][-4.2716 -4.2833886 -4.2614231 -4.22304 -4.1797528 -4.13344 -4.1088305 -4.106113 -4.1268983 -4.1523423 -4.1838965 -4.2056956 -4.22072 -4.2386093 -4.2468777][-4.236547 -4.2564154 -4.2427635 -4.2090278 -4.1657577 -4.1187153 -4.0972857 -4.1040745 -4.136127 -4.1661172 -4.1956196 -4.2171884 -4.23588 -4.2566986 -4.2701287][-4.21232 -4.2396684 -4.2350793 -4.2103658 -4.1746283 -4.1357393 -4.1194253 -4.1313553 -4.16665 -4.1967168 -4.2205272 -4.2381997 -4.2552524 -4.2737527 -4.2865376][-4.2058039 -4.2371254 -4.23872 -4.223681 -4.2001472 -4.1730123 -4.1611419 -4.1732221 -4.2054162 -4.2319341 -4.2503557 -4.263103 -4.2756114 -4.2870369 -4.2929573][-4.212347 -4.2449403 -4.2512469 -4.2444968 -4.229928 -4.2117152 -4.2025852 -4.211525 -4.2363405 -4.2564993 -4.2673063 -4.2733541 -4.2789006 -4.282105 -4.2824368][-4.2221026 -4.2524405 -4.2616153 -4.2594218 -4.2477307 -4.2322993 -4.2230577 -4.2283659 -4.2426505 -4.2542791 -4.2603865 -4.262784 -4.2638083 -4.2623687 -4.25805][-4.224515 -4.24873 -4.2566371 -4.2554417 -4.2450047 -4.2316542 -4.2239995 -4.2259593 -4.2317758 -4.23732 -4.2406816 -4.2420154 -4.2411165 -4.2376657 -4.2319012]]...]
INFO - root - 2017-12-05 19:19:17.364868: step 37010, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 80h:48m:25s remains)
INFO - root - 2017-12-05 19:19:26.705735: step 37020, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 69h:16m:11s remains)
INFO - root - 2017-12-05 19:19:36.079413: step 37030, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 78h:40m:48s remains)
INFO - root - 2017-12-05 19:19:45.613651: step 37040, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 79h:58m:08s remains)
INFO - root - 2017-12-05 19:19:54.994123: step 37050, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.988 sec/batch; 81h:02m:42s remains)
INFO - root - 2017-12-05 19:20:04.385057: step 37060, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 0.995 sec/batch; 81h:37m:17s remains)
INFO - root - 2017-12-05 19:20:13.939536: step 37070, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 81h:01m:52s remains)
INFO - root - 2017-12-05 19:20:23.443742: step 37080, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 69h:24m:00s remains)
INFO - root - 2017-12-05 19:20:32.859605: step 37090, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 79h:30m:12s remains)
INFO - root - 2017-12-05 19:20:42.248374: step 37100, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 77h:01m:16s remains)
2017-12-05 19:20:43.013189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2903523 -4.2570591 -4.2233257 -4.1982756 -4.1841359 -4.1818008 -4.1608195 -4.118566 -4.08777 -4.1101623 -4.16015 -4.2069077 -4.2426529 -4.25048 -4.2547913][-4.31815 -4.2920766 -4.2671804 -4.24401 -4.2234669 -4.2054462 -4.1677876 -4.1148024 -4.0817075 -4.1081953 -4.1666622 -4.2216058 -4.2642813 -4.2766185 -4.2783241][-4.3317184 -4.3147097 -4.302773 -4.2892561 -4.2716074 -4.2483783 -4.2046828 -4.1504674 -4.11478 -4.1338234 -4.1870933 -4.2409706 -4.2826757 -4.2983561 -4.3013363][-4.32869 -4.3175492 -4.3151212 -4.3125629 -4.3051891 -4.2915244 -4.2599883 -4.2174249 -4.1837492 -4.1872663 -4.2224932 -4.2623816 -4.2937708 -4.3085403 -4.3136363][-4.2948208 -4.2719903 -4.2611179 -4.2602777 -4.2690511 -4.2814021 -4.2817659 -4.2666888 -4.2496171 -4.2469168 -4.2662091 -4.2895174 -4.3062048 -4.314599 -4.3205271][-4.2348366 -4.1820121 -4.1466618 -4.1394339 -4.1670418 -4.2156096 -4.2561975 -4.2741733 -4.2774873 -4.2796144 -4.2929349 -4.3093534 -4.3177309 -4.3230338 -4.3268523][-4.1723237 -4.0862947 -4.0204659 -4.0035973 -4.0492811 -4.1303835 -4.1991229 -4.242424 -4.2658224 -4.279717 -4.2964568 -4.313612 -4.3233786 -4.3299351 -4.3310795][-4.114759 -4.0038233 -3.9142039 -3.8831911 -3.9438767 -4.04754 -4.1296787 -4.1899986 -4.2333269 -4.2615213 -4.2852936 -4.3062 -4.3214574 -4.3313694 -4.3323135][-4.0855365 -3.9630141 -3.85331 -3.8033636 -3.8656921 -3.9828508 -4.0818915 -4.1596427 -4.2157788 -4.2516809 -4.2769313 -4.2978134 -4.3136859 -4.3242617 -4.3259377][-4.1222873 -4.009747 -3.8889849 -3.8215373 -3.8627024 -3.9658527 -4.066206 -4.1473742 -4.2048335 -4.2459621 -4.2717223 -4.2899852 -4.3048553 -4.3139768 -4.3150992][-4.2052016 -4.119266 -4.0167542 -3.9575179 -3.9761667 -4.0424 -4.1111393 -4.1690764 -4.212966 -4.2503729 -4.2765293 -4.2925253 -4.3055139 -4.3093128 -4.3071003][-4.2742209 -4.2217455 -4.1593308 -4.1302748 -4.1391187 -4.1671677 -4.1961021 -4.2220235 -4.2436786 -4.2695813 -4.2915378 -4.3049464 -4.3113108 -4.3075743 -4.3005619][-4.3061075 -4.2828879 -4.255919 -4.247858 -4.251575 -4.259007 -4.2673411 -4.2739992 -4.2795477 -4.29412 -4.3090587 -4.3170223 -4.3133583 -4.3026543 -4.2926192][-4.3095174 -4.3037109 -4.2964559 -4.2951159 -4.2953815 -4.2977319 -4.3002481 -4.300787 -4.3013506 -4.307848 -4.3149018 -4.3152547 -4.3059998 -4.2944918 -4.2863121][-4.3118405 -4.3118992 -4.3121476 -4.314352 -4.3137445 -4.3128943 -4.3132744 -4.313447 -4.3151369 -4.3176894 -4.3182116 -4.3143353 -4.3058443 -4.2980866 -4.2945518]]...]
INFO - root - 2017-12-05 19:20:52.131989: step 37110, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 79h:08m:13s remains)
INFO - root - 2017-12-05 19:21:01.771415: step 37120, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 75h:59m:34s remains)
INFO - root - 2017-12-05 19:21:11.146574: step 37130, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 77h:27m:13s remains)
INFO - root - 2017-12-05 19:21:20.381305: step 37140, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 71h:16m:40s remains)
INFO - root - 2017-12-05 19:21:29.847451: step 37150, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 78h:00m:11s remains)
INFO - root - 2017-12-05 19:21:39.268588: step 37160, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 76h:43m:26s remains)
INFO - root - 2017-12-05 19:21:48.732389: step 37170, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 75h:02m:46s remains)
INFO - root - 2017-12-05 19:21:58.096681: step 37180, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 77h:35m:35s remains)
INFO - root - 2017-12-05 19:22:07.352393: step 37190, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 77h:38m:42s remains)
INFO - root - 2017-12-05 19:22:16.465146: step 37200, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 78h:08m:08s remains)
2017-12-05 19:22:17.226182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3021588 -4.300756 -4.3036504 -4.3073668 -4.306252 -4.2952433 -4.2772064 -4.2593431 -4.2441063 -4.2309341 -4.2203431 -4.2104096 -4.2044954 -4.2024579 -4.2042451][-4.2935143 -4.2918587 -4.2959595 -4.2979712 -4.290576 -4.2675681 -4.2358284 -4.2125616 -4.201333 -4.1954513 -4.1886196 -4.1781893 -4.1695342 -4.1665344 -4.1704082][-4.2837081 -4.2831721 -4.2854209 -4.2806377 -4.263133 -4.225245 -4.1757922 -4.1467333 -4.1461258 -4.156239 -4.1591387 -4.1509509 -4.1422596 -4.1402082 -4.1424894][-4.2776747 -4.2745171 -4.2677083 -4.2531462 -4.2268758 -4.1756778 -4.110476 -4.0746732 -4.0885596 -4.1187596 -4.1369081 -4.1327629 -4.1246004 -4.123889 -4.1218128][-4.2791243 -4.2689443 -4.2467904 -4.2217741 -4.1865849 -4.1235666 -4.0431471 -3.9978626 -4.0230751 -4.0747766 -4.1140862 -4.1144824 -4.1037903 -4.1028171 -4.0979156][-4.2776551 -4.2569175 -4.2211394 -4.1852059 -4.1382565 -4.0633817 -3.9678707 -3.9139841 -3.9522915 -4.0241923 -4.0862203 -4.0999322 -4.0952063 -4.0967541 -4.09135][-4.275012 -4.2484646 -4.2068138 -4.1621313 -4.100606 -4.0115104 -3.8965333 -3.8220758 -3.8657341 -3.9651904 -4.0549631 -4.0900168 -4.0990162 -4.1081357 -4.1032863][-4.2850676 -4.2581563 -4.2158747 -4.16658 -4.0937276 -3.9878294 -3.8515859 -3.7419605 -3.7740488 -3.9011178 -4.0198979 -4.0805306 -4.1133037 -4.1347256 -4.132566][-4.2961674 -4.2702017 -4.2277803 -4.1780992 -4.1038213 -3.9931054 -3.8609262 -3.7498016 -3.7726741 -3.9037569 -4.0275803 -4.0984082 -4.1442919 -4.1721067 -4.1735072][-4.2988305 -4.2727022 -4.23044 -4.1832294 -4.1129909 -4.0137281 -3.9068587 -3.8334703 -3.8606918 -3.9647708 -4.061059 -4.1227026 -4.1658187 -4.1937037 -4.1981363][-4.2922382 -4.2679114 -4.228508 -4.1814427 -4.1144085 -4.0285234 -3.9406571 -3.8985462 -3.9339893 -4.0128555 -4.0850573 -4.1358223 -4.1739445 -4.2012129 -4.2116952][-4.2820334 -4.2622771 -4.2339525 -4.1955967 -4.1355863 -4.0585871 -3.9776714 -3.9436784 -3.9785004 -4.0390878 -4.0989571 -4.1453652 -4.1769004 -4.2002912 -4.2159576][-4.2704959 -4.2563529 -4.2420034 -4.2178354 -4.167448 -4.0973568 -4.0171742 -3.9769721 -4.00021 -4.045372 -4.0943246 -4.1326265 -4.162663 -4.1904755 -4.2084923][-4.260211 -4.2469864 -4.2401042 -4.222054 -4.1785712 -4.1145887 -4.0359864 -3.9860759 -4.0014262 -4.0361977 -4.0753512 -4.106595 -4.134747 -4.1708875 -4.195971][-4.2544842 -4.2402039 -4.2375154 -4.2245426 -4.1877303 -4.1333895 -4.0636263 -4.0113091 -4.0195675 -4.0489969 -4.0844893 -4.1112852 -4.1312289 -4.1668754 -4.1976805]]...]
INFO - root - 2017-12-05 19:22:26.408516: step 37210, loss = 2.02, batch loss = 1.97 (8.9 examples/sec; 0.899 sec/batch; 73h:44m:04s remains)
INFO - root - 2017-12-05 19:22:35.614887: step 37220, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 76h:42m:36s remains)
INFO - root - 2017-12-05 19:22:45.029084: step 37230, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 78h:01m:49s remains)
INFO - root - 2017-12-05 19:22:54.183878: step 37240, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 75h:34m:42s remains)
INFO - root - 2017-12-05 19:23:03.299939: step 37250, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 78h:34m:11s remains)
INFO - root - 2017-12-05 19:23:12.732064: step 37260, loss = 2.10, batch loss = 2.05 (8.5 examples/sec; 0.943 sec/batch; 77h:20m:15s remains)
INFO - root - 2017-12-05 19:23:21.937011: step 37270, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 80h:49m:48s remains)
INFO - root - 2017-12-05 19:23:31.437955: step 37280, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 75h:31m:21s remains)
INFO - root - 2017-12-05 19:23:40.810686: step 37290, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 73h:49m:21s remains)
INFO - root - 2017-12-05 19:23:50.255896: step 37300, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 78h:04m:36s remains)
2017-12-05 19:23:50.981381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3343558 -4.3354721 -4.3358545 -4.3361983 -4.3364244 -4.3383622 -4.3395786 -4.3397188 -4.3383565 -4.3362007 -4.3348203 -4.3338914 -4.3332615 -4.3335032 -4.3341832][-4.3402276 -4.337841 -4.3357306 -4.3342495 -4.333488 -4.3382292 -4.3415756 -4.3419156 -4.3371758 -4.332099 -4.3299484 -4.3280711 -4.3281779 -4.3292642 -4.3314905][-4.33841 -4.3288093 -4.3207335 -4.3154526 -4.3164234 -4.324779 -4.3296804 -4.3288665 -4.3182373 -4.3090005 -4.3073797 -4.306479 -4.3100691 -4.3136387 -4.3178282][-4.3210254 -4.2998214 -4.2824287 -4.2740989 -4.2779098 -4.2878537 -4.292942 -4.2909336 -4.2758093 -4.2642393 -4.2628117 -4.2644677 -4.2772489 -4.2890368 -4.297997][-4.2801781 -4.2481661 -4.2251749 -4.2134576 -4.2135062 -4.21474 -4.2133436 -4.2047548 -4.1804128 -4.1687469 -4.1717496 -4.1903853 -4.2232132 -4.2451935 -4.2618523][-4.23081 -4.1888833 -4.1648293 -4.1499786 -4.1370311 -4.1177607 -4.0983319 -4.0743361 -4.0378995 -4.0295134 -4.0536661 -4.1045294 -4.1585584 -4.1912193 -4.2176123][-4.19196 -4.1454654 -4.1288261 -4.1164656 -4.0821004 -4.0270109 -3.9831059 -3.9431179 -3.906234 -3.9132438 -3.9653673 -4.0435176 -4.1099176 -4.1527677 -4.1911077][-4.1779466 -4.131979 -4.12247 -4.1136122 -4.0575294 -3.9711308 -3.9099426 -3.8672228 -3.8468757 -3.8768454 -3.9397426 -4.0286326 -4.0998316 -4.1499968 -4.196949][-4.1926174 -4.1513257 -4.1457491 -4.1390734 -4.0802979 -3.9935522 -3.9344685 -3.8983843 -3.8895192 -3.9301536 -3.9904542 -4.0739536 -4.1400595 -4.1883688 -4.2360845][-4.2326183 -4.201108 -4.1972413 -4.1933208 -4.1508803 -4.0895042 -4.0463018 -4.0113997 -3.9984813 -4.0300469 -4.0825615 -4.15553 -4.212235 -4.2481337 -4.2867556][-4.2761216 -4.2575469 -4.2571282 -4.2536182 -4.2258821 -4.1857872 -4.1600614 -4.1296582 -4.1144538 -4.1370573 -4.1805239 -4.2402005 -4.2809567 -4.3028407 -4.3282614][-4.3085194 -4.3051138 -4.3071561 -4.2980614 -4.2712097 -4.2448497 -4.2311292 -4.2156405 -4.2128296 -4.2325048 -4.2633209 -4.3026733 -4.3261623 -4.3380971 -4.3512626][-4.3258123 -4.32933 -4.3305864 -4.3155212 -4.2907734 -4.2715535 -4.2647886 -4.2605925 -4.2716613 -4.2974334 -4.3198519 -4.33885 -4.350563 -4.3557472 -4.3603883][-4.3404703 -4.3482747 -4.3493476 -4.3311648 -4.3056259 -4.2859559 -4.2797942 -4.2810755 -4.2972188 -4.325388 -4.3458467 -4.3582926 -4.3656979 -4.3660522 -4.3646836][-4.3529592 -4.3618345 -4.361639 -4.340879 -4.3108659 -4.2886543 -4.2791872 -4.2830334 -4.3005981 -4.3257995 -4.3478642 -4.3623338 -4.3673358 -4.361908 -4.3579988]]...]
INFO - root - 2017-12-05 19:24:00.486665: step 37310, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.019 sec/batch; 83h:35m:33s remains)
INFO - root - 2017-12-05 19:24:09.751701: step 37320, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 79h:13m:51s remains)
INFO - root - 2017-12-05 19:24:19.158183: step 37330, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 78h:39m:58s remains)
INFO - root - 2017-12-05 19:24:28.559272: step 37340, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 79h:08m:40s remains)
INFO - root - 2017-12-05 19:24:37.987265: step 37350, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 77h:00m:30s remains)
INFO - root - 2017-12-05 19:24:47.117848: step 37360, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 79h:21m:42s remains)
INFO - root - 2017-12-05 19:24:56.659035: step 37370, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 78h:05m:46s remains)
INFO - root - 2017-12-05 19:25:05.810702: step 37380, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 81h:05m:07s remains)
INFO - root - 2017-12-05 19:25:14.959212: step 37390, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 71h:55m:12s remains)
INFO - root - 2017-12-05 19:25:24.279934: step 37400, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 78h:41m:54s remains)
2017-12-05 19:25:25.022225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2592072 -4.2604022 -4.2599478 -4.2691 -4.2872744 -4.3025351 -4.3109536 -4.3081818 -4.298142 -4.2845111 -4.2664838 -4.235743 -4.1974788 -4.1665545 -4.150209][-4.2889857 -4.2927666 -4.2942667 -4.3004813 -4.3125434 -4.3210573 -4.3240228 -4.3178883 -4.3038239 -4.2865868 -4.2609415 -4.2260051 -4.1855617 -4.15468 -4.1401191][-4.3010364 -4.3026934 -4.3057847 -4.3086953 -4.3129978 -4.3136654 -4.309 -4.2972026 -4.282258 -4.2686524 -4.2455554 -4.2127924 -4.1752586 -4.1470747 -4.1319408][-4.2969646 -4.2912908 -4.2903605 -4.2885122 -4.2832055 -4.2717948 -4.2537603 -4.233758 -4.2253261 -4.2265697 -4.2191076 -4.1957 -4.1672821 -4.146297 -4.1310911][-4.2683411 -4.2485933 -4.2372732 -4.23073 -4.214221 -4.1819658 -4.1417532 -4.11471 -4.1284351 -4.1598768 -4.1732759 -4.1612577 -4.1443982 -4.1382523 -4.1323514][-4.215776 -4.1752148 -4.1487308 -4.140811 -4.112752 -4.052999 -3.9733665 -3.9324188 -3.989507 -4.064105 -4.0976634 -4.0896034 -4.0740747 -4.0858579 -4.1022048][-4.1547732 -4.090054 -4.0528193 -4.0549417 -4.0256977 -3.9379272 -3.8013544 -3.7315745 -3.8505986 -3.9795911 -4.028194 -4.0067677 -3.9812129 -4.0079503 -4.0500817][-4.1051068 -4.0269165 -3.99041 -4.0088639 -3.9901342 -3.8991957 -3.7403579 -3.6591959 -3.809423 -3.9662666 -4.0202894 -3.9863515 -3.9492607 -3.971442 -4.0183291][-4.0865784 -4.0276117 -4.0156636 -4.0427127 -4.0303144 -3.9688566 -3.8640614 -3.8130326 -3.9200194 -4.046495 -4.0956907 -4.0617542 -4.0180144 -4.0147467 -4.0405412][-4.1019983 -4.0791655 -4.0893812 -4.1163912 -4.1102672 -4.0835571 -4.0333042 -4.0075207 -4.0667729 -4.1478386 -4.1848559 -4.1599936 -4.121562 -4.1009603 -4.1068597][-4.1412148 -4.1449056 -4.16108 -4.1826053 -4.1853209 -4.177628 -4.1558261 -4.143528 -4.1737747 -4.2203212 -4.2492514 -4.2372632 -4.2135081 -4.1937976 -4.1925015][-4.1955161 -4.2057467 -4.218502 -4.2346673 -4.2447667 -4.2458386 -4.2355361 -4.2322631 -4.2508621 -4.2776175 -4.294385 -4.2880874 -4.275475 -4.2639556 -4.2622404][-4.2425108 -4.2520928 -4.2605543 -4.2723775 -4.2819314 -4.28657 -4.2823348 -4.2828422 -4.2945089 -4.3096566 -4.3186994 -4.3148022 -4.3080058 -4.302597 -4.3026915][-4.2724442 -4.2804275 -4.2891631 -4.299377 -4.3075094 -4.3109617 -4.3084679 -4.3071265 -4.3120275 -4.319891 -4.3248248 -4.3211942 -4.3161454 -4.313715 -4.3166242][-4.2912593 -4.2994423 -4.3093228 -4.3188295 -4.3233643 -4.32219 -4.3164153 -4.3116779 -4.3118234 -4.3156791 -4.318872 -4.3171539 -4.3144279 -4.31472 -4.3199148]]...]
INFO - root - 2017-12-05 19:25:34.525748: step 37410, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 79h:32m:11s remains)
INFO - root - 2017-12-05 19:25:43.991293: step 37420, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.019 sec/batch; 83h:33m:40s remains)
INFO - root - 2017-12-05 19:25:53.123035: step 37430, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 80h:38m:38s remains)
INFO - root - 2017-12-05 19:26:02.288903: step 37440, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 77h:30m:16s remains)
INFO - root - 2017-12-05 19:26:11.592221: step 37450, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 79h:41m:33s remains)
INFO - root - 2017-12-05 19:26:20.810682: step 37460, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 75h:52m:44s remains)
INFO - root - 2017-12-05 19:26:30.118748: step 37470, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 74h:24m:47s remains)
INFO - root - 2017-12-05 19:26:39.472906: step 37480, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.955 sec/batch; 78h:13m:39s remains)
INFO - root - 2017-12-05 19:26:48.828181: step 37490, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 74h:58m:28s remains)
INFO - root - 2017-12-05 19:26:58.165739: step 37500, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 77h:59m:58s remains)
2017-12-05 19:26:58.965042: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2738128 -4.2732873 -4.2742667 -4.2763371 -4.275876 -4.2744274 -4.2673006 -4.2560139 -4.2440176 -4.2265806 -4.2014909 -4.1705918 -4.1482239 -4.1520991 -4.1699963][-4.2588096 -4.2526107 -4.25134 -4.2512302 -4.2499046 -4.2492337 -4.2397523 -4.2263374 -4.2092834 -4.1821842 -4.1411943 -4.086978 -4.0496745 -4.0586343 -4.0898314][-4.2444634 -4.2359705 -4.2303953 -4.2257786 -4.2219324 -4.217824 -4.2001629 -4.1798863 -4.15826 -4.1281595 -4.0785446 -4.0090151 -3.9642527 -3.9824529 -4.0288348][-4.2361526 -4.2228613 -4.2083244 -4.1966648 -4.1839809 -4.172174 -4.1439972 -4.1185961 -4.1001348 -4.0789552 -4.0373163 -3.9760494 -3.9422708 -3.9666872 -4.0162625][-4.2391572 -4.2150431 -4.1863689 -4.1658535 -4.1428442 -4.1212525 -4.0849166 -4.0596147 -4.0526547 -4.0476418 -4.0231156 -3.9809461 -3.9637957 -3.9908664 -4.0368872][-4.2346163 -4.193058 -4.1512246 -4.1277475 -4.1060762 -4.0859718 -4.0472131 -4.0218744 -4.0261922 -4.0340128 -4.0218425 -3.9945419 -3.9937181 -4.0308175 -4.0756207][-4.209208 -4.1523132 -4.1016498 -4.0822067 -4.0827208 -4.0833354 -4.0539227 -4.0293922 -4.0340881 -4.0438972 -4.0368977 -4.02224 -4.0346665 -4.0784955 -4.1211843][-4.1681466 -4.1061773 -4.0541153 -4.0472651 -4.0814924 -4.106916 -4.0930586 -4.0736318 -4.0710907 -4.0714841 -4.0674286 -4.0623579 -4.0808272 -4.1219192 -4.1566143][-4.1440291 -4.0874958 -4.0391893 -4.040916 -4.0886993 -4.1239834 -4.116919 -4.1033187 -4.1001816 -4.0990882 -4.0978837 -4.0989532 -4.1158776 -4.1452117 -4.16983][-4.1520834 -4.1112156 -4.078588 -4.0843887 -4.11671 -4.1368876 -4.1265583 -4.1162624 -4.1175995 -4.1207013 -4.1206789 -4.122416 -4.1329203 -4.149848 -4.1687021][-4.1806173 -4.1617222 -4.1405 -4.1379833 -4.151145 -4.1532464 -4.1359138 -4.1207929 -4.1201468 -4.12648 -4.1320992 -4.1373663 -4.1453514 -4.155447 -4.1735921][-4.2014046 -4.1962361 -4.1764526 -4.1646361 -4.1677451 -4.1636405 -4.14578 -4.1277738 -4.1226768 -4.131669 -4.1459212 -4.1578746 -4.1661148 -4.172801 -4.1896119][-4.183651 -4.1812363 -4.1538477 -4.1299624 -4.1268806 -4.1323662 -4.1272588 -4.1199274 -4.1245718 -4.1456566 -4.1672053 -4.1809788 -4.1859832 -4.1895785 -4.2000985][-4.1260781 -4.1055923 -4.0583229 -4.0209494 -4.02167 -4.0452719 -4.0658226 -4.0842505 -4.113688 -4.1492214 -4.1743393 -4.1864161 -4.1860409 -4.185472 -4.1891232][-4.0677185 -4.022109 -3.9533784 -3.9071217 -3.9208846 -3.9710915 -4.01887 -4.062201 -4.1096449 -4.153151 -4.1785483 -4.1844654 -4.1779652 -4.1724267 -4.1708465]]...]
INFO - root - 2017-12-05 19:27:08.341309: step 37510, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 73h:58m:26s remains)
INFO - root - 2017-12-05 19:27:17.577009: step 37520, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 74h:50m:26s remains)
INFO - root - 2017-12-05 19:27:26.897991: step 37530, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 72h:01m:27s remains)
INFO - root - 2017-12-05 19:27:36.289847: step 37540, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 77h:27m:55s remains)
INFO - root - 2017-12-05 19:27:45.688016: step 37550, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 74h:18m:28s remains)
INFO - root - 2017-12-05 19:27:55.109532: step 37560, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 75h:22m:09s remains)
INFO - root - 2017-12-05 19:28:04.308379: step 37570, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 75h:06m:39s remains)
INFO - root - 2017-12-05 19:28:13.770011: step 37580, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 78h:19m:11s remains)
INFO - root - 2017-12-05 19:28:22.843876: step 37590, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 70h:52m:21s remains)
INFO - root - 2017-12-05 19:28:32.142011: step 37600, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 74h:23m:24s remains)
2017-12-05 19:28:32.878062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2546635 -4.2800808 -4.293571 -4.2741966 -4.2278461 -4.176055 -4.1516762 -4.1602082 -4.19059 -4.2333951 -4.2677083 -4.2858582 -4.2979913 -4.3130555 -4.3297143][-4.2269878 -4.2530389 -4.2620778 -4.2328982 -4.184463 -4.1400437 -4.1310849 -4.1530151 -4.192174 -4.2351131 -4.2638025 -4.2791142 -4.2926879 -4.310689 -4.3299246][-4.2234359 -4.2426572 -4.2430968 -4.2020307 -4.1480708 -4.1078181 -4.1115317 -4.1449151 -4.1866446 -4.2272139 -4.2536407 -4.2692709 -4.2846603 -4.3069797 -4.3291674][-4.2598553 -4.2694383 -4.2604218 -4.210515 -4.1485171 -4.0980644 -4.0926385 -4.1229343 -4.1641536 -4.2053576 -4.2359653 -4.2580032 -4.2793212 -4.3062553 -4.3300385][-4.2980032 -4.3037567 -4.28923 -4.2380114 -4.170908 -4.1021514 -4.0708861 -4.0857573 -4.1243753 -4.1722412 -4.2151222 -4.247818 -4.2783675 -4.3087769 -4.3317842][-4.3163571 -4.3211579 -4.3036861 -4.2605214 -4.2000122 -4.1188469 -4.0620975 -4.0599289 -4.0932045 -4.1483665 -4.2039008 -4.2460675 -4.2825212 -4.3124633 -4.3336263][-4.3115907 -4.31754 -4.2997756 -4.2632966 -4.214684 -4.1371613 -4.0734649 -4.0624561 -4.0921159 -4.1510048 -4.2104449 -4.2543535 -4.2898178 -4.3165288 -4.3348494][-4.3038216 -4.309443 -4.2919936 -4.2607279 -4.220942 -4.1546941 -4.1011353 -4.089107 -4.114347 -4.1702576 -4.2244987 -4.2633491 -4.2944183 -4.3183041 -4.3344426][-4.2950826 -4.2976518 -4.2801828 -4.2541852 -4.2237158 -4.1688662 -4.1286573 -4.1186237 -4.1410475 -4.1920953 -4.2382817 -4.2708287 -4.2964168 -4.3179736 -4.3334918][-4.2728972 -4.2675304 -4.2480273 -4.2254324 -4.20203 -4.1604362 -4.1379333 -4.1352873 -4.1605597 -4.2077889 -4.2478523 -4.2768784 -4.2980227 -4.3176074 -4.3327584][-4.2416387 -4.2238564 -4.1956449 -4.1659508 -4.139009 -4.1110168 -4.1143694 -4.1297908 -4.16651 -4.2155032 -4.2538929 -4.2822156 -4.300034 -4.3172846 -4.3321552][-4.1947412 -4.1638622 -4.126359 -4.0846467 -4.0501065 -4.0374994 -4.0717864 -4.1141353 -4.1656022 -4.2197371 -4.2599854 -4.2876511 -4.3030272 -4.3180494 -4.3323011][-4.150898 -4.1175146 -4.0805511 -4.0344458 -3.9944868 -3.9940825 -4.0508342 -4.1130404 -4.1719804 -4.2266607 -4.2661619 -4.2915363 -4.3055658 -4.3190212 -4.3328066][-4.1382771 -4.1154065 -4.094511 -4.0564337 -4.0198135 -4.0215678 -4.0760612 -4.1359057 -4.1878514 -4.2344656 -4.2693119 -4.2910886 -4.3057418 -4.3190031 -4.3335218][-4.1567841 -4.149425 -4.1462731 -4.1177158 -4.0866942 -4.0814772 -4.1162605 -4.1625981 -4.2062092 -4.2442837 -4.2719049 -4.288506 -4.3034244 -4.3176336 -4.3342686]]...]
INFO - root - 2017-12-05 19:28:42.178282: step 37610, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 75h:40m:15s remains)
INFO - root - 2017-12-05 19:28:51.658554: step 37620, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 76h:17m:10s remains)
INFO - root - 2017-12-05 19:29:01.024542: step 37630, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 76h:02m:09s remains)
INFO - root - 2017-12-05 19:29:10.531797: step 37640, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 79h:51m:04s remains)
INFO - root - 2017-12-05 19:29:19.938917: step 37650, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 77h:04m:02s remains)
INFO - root - 2017-12-05 19:29:29.095720: step 37660, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 77h:16m:00s remains)
INFO - root - 2017-12-05 19:29:38.294153: step 37670, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 70h:41m:55s remains)
INFO - root - 2017-12-05 19:29:47.512095: step 37680, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 72h:06m:52s remains)
INFO - root - 2017-12-05 19:29:56.861553: step 37690, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 75h:28m:27s remains)
INFO - root - 2017-12-05 19:30:05.967371: step 37700, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.924 sec/batch; 75h:39m:56s remains)
2017-12-05 19:30:06.679207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2355447 -4.2294922 -4.2332811 -4.2433548 -4.2511287 -4.2553387 -4.259069 -4.2629905 -4.2657123 -4.2653646 -4.2605505 -4.25881 -4.2640777 -4.2698736 -4.2663045][-4.2380919 -4.2370224 -4.2428336 -4.2494483 -4.2523122 -4.2491255 -4.2449303 -4.2441063 -4.2480078 -4.2522249 -4.2514048 -4.2511625 -4.2565479 -4.2622752 -4.2579103][-4.2429295 -4.2430429 -4.2469192 -4.2456141 -4.238018 -4.2228312 -4.2088318 -4.2028346 -4.2054009 -4.2137203 -4.2185411 -4.2229724 -4.2314291 -4.2362337 -4.2304635][-4.2299767 -4.2238889 -4.2219911 -4.2126012 -4.1928682 -4.1646676 -4.1420145 -4.1318355 -4.1325207 -4.1402435 -4.1492896 -4.1606197 -4.1731858 -4.1772776 -4.1699371][-4.1964664 -4.1777782 -4.1672835 -4.1517906 -4.125072 -4.08803 -4.0565796 -4.0430145 -4.0410047 -4.046267 -4.0581264 -4.0779266 -4.0997286 -4.1057954 -4.096354][-4.1552863 -4.1250024 -4.1084242 -4.0910335 -4.0630445 -4.021162 -3.9812734 -3.9551427 -3.9464157 -3.9547958 -3.9753911 -4.0090847 -4.0453777 -4.0534816 -4.0357089][-4.1217237 -4.0909548 -4.0795665 -4.0671353 -4.0387168 -3.9892333 -3.9334691 -3.8853641 -3.8688598 -3.8865089 -3.920517 -3.9682081 -4.0181789 -4.0273361 -3.996706][-4.114502 -4.0935078 -4.09255 -4.0855474 -4.0557284 -4.0005245 -3.9320288 -3.8704348 -3.8469262 -3.8744512 -3.9155526 -3.9705448 -4.0253129 -4.0298691 -3.98765][-4.1326532 -4.121069 -4.1285882 -4.1271234 -4.099565 -4.0478644 -3.9815826 -3.9212763 -3.8941367 -3.9220753 -3.9658859 -4.0177083 -4.0604677 -4.0535121 -4.0053587][-4.1629186 -4.1553345 -4.1669636 -4.17192 -4.1562505 -4.1214423 -4.0703583 -4.0161171 -3.9810145 -3.998837 -4.0376434 -4.077168 -4.0911818 -4.0707755 -4.0300059][-4.1904545 -4.1853223 -4.20033 -4.2150631 -4.2141137 -4.1989536 -4.163919 -4.1126957 -4.0697188 -4.072731 -4.1027322 -4.1268682 -4.1169124 -4.0830488 -4.0473561][-4.204669 -4.2010155 -4.2186155 -4.2419434 -4.2536249 -4.2557964 -4.2346692 -4.1885233 -4.1396503 -4.1232228 -4.13503 -4.1416736 -4.1225286 -4.0862455 -4.053731][-4.2148581 -4.2093911 -4.22394 -4.2509727 -4.2721663 -4.2843733 -4.2721367 -4.2349043 -4.1868157 -4.1541033 -4.1454697 -4.138586 -4.1194916 -4.0898414 -4.0608778][-4.2268276 -4.2168126 -4.2249885 -4.247849 -4.27152 -4.285944 -4.2793555 -4.2550621 -4.2179823 -4.1849184 -4.1688957 -4.1570435 -4.1373615 -4.1165695 -4.0967226][-4.2351689 -4.222261 -4.2230105 -4.2358694 -4.2541718 -4.2668886 -4.2648597 -4.251471 -4.227704 -4.2038627 -4.1931839 -4.190589 -4.1818948 -4.170681 -4.1578135]]...]
INFO - root - 2017-12-05 19:30:15.935134: step 37710, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 73h:59m:57s remains)
INFO - root - 2017-12-05 19:30:25.099777: step 37720, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 78h:51m:35s remains)
INFO - root - 2017-12-05 19:30:34.692960: step 37730, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 78h:49m:25s remains)
INFO - root - 2017-12-05 19:30:43.987542: step 37740, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 77h:45m:06s remains)
INFO - root - 2017-12-05 19:30:53.343598: step 37750, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 77h:04m:57s remains)
INFO - root - 2017-12-05 19:31:02.865763: step 37760, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 77h:07m:32s remains)
INFO - root - 2017-12-05 19:31:12.360308: step 37770, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 78h:45m:51s remains)
INFO - root - 2017-12-05 19:31:21.824185: step 37780, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 80h:28m:53s remains)
INFO - root - 2017-12-05 19:31:31.182871: step 37790, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 80h:47m:49s remains)
INFO - root - 2017-12-05 19:31:40.633620: step 37800, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 80h:56m:55s remains)
2017-12-05 19:31:41.423543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2233334 -4.2180476 -4.2123747 -4.2092333 -4.2100029 -4.2143083 -4.2176061 -4.2220063 -4.2417111 -4.2647953 -4.2775745 -4.287406 -4.2974281 -4.3050752 -4.3112249][-4.1689177 -4.1670246 -4.1674833 -4.1699734 -4.1745386 -4.1810546 -4.1813507 -4.1813 -4.2072377 -4.2398224 -4.2589145 -4.271956 -4.283433 -4.2919006 -4.3016648][-4.1311011 -4.1340327 -4.145144 -4.1563554 -4.1647921 -4.1735272 -4.1671662 -4.1543269 -4.1765885 -4.2151227 -4.2413683 -4.2564487 -4.2690892 -4.2798419 -4.2925363][-4.1232338 -4.1269217 -4.1484108 -4.16851 -4.1773195 -4.1780696 -4.1539779 -4.1223173 -4.1354427 -4.1777425 -4.2113891 -4.2305346 -4.2471738 -4.263566 -4.2807765][-4.1280112 -4.125453 -4.1514969 -4.1789865 -4.1862292 -4.1693487 -4.1182861 -4.0621042 -4.0613384 -4.108387 -4.1573586 -4.1909161 -4.2187519 -4.2443552 -4.2677903][-4.1315088 -4.1226792 -4.1501651 -4.1812534 -4.1820054 -4.1383553 -4.04694 -3.9599152 -3.9492447 -4.01369 -4.091938 -4.1505518 -4.1949625 -4.2296734 -4.25871][-4.1349626 -4.1271467 -4.155426 -4.1818562 -4.1674924 -4.1025944 -3.9834981 -3.8797264 -3.8742442 -3.9654167 -4.0650506 -4.1356544 -4.1857805 -4.223475 -4.2549186][-4.1404138 -4.139051 -4.1673789 -4.1848383 -4.1593528 -4.0917683 -3.9753745 -3.8831701 -3.8976228 -3.9970872 -4.09585 -4.1561661 -4.1960711 -4.2274828 -4.2556982][-4.15037 -4.1579552 -4.1838989 -4.1913481 -4.1581149 -4.0937734 -3.9944198 -3.9258661 -3.9641638 -4.0629139 -4.1490703 -4.191524 -4.2144303 -4.2363963 -4.2590017][-4.1549277 -4.165853 -4.1872158 -4.1878753 -4.1539769 -4.0993896 -4.0230947 -3.9790778 -4.0270796 -4.1117611 -4.1795125 -4.209353 -4.224154 -4.2412772 -4.2600932][-4.1647248 -4.1727157 -4.1857738 -4.1808705 -4.1462 -4.10768 -4.0594225 -4.0361943 -4.0797305 -4.144062 -4.1930771 -4.215116 -4.2281322 -4.2445645 -4.261764][-4.1859007 -4.1890297 -4.1945806 -4.1859565 -4.1515627 -4.1216044 -4.0856452 -4.0730596 -4.1110592 -4.1605463 -4.1945014 -4.2110863 -4.2272043 -4.2452922 -4.2626424][-4.1989856 -4.2014475 -4.205101 -4.1972604 -4.1703019 -4.140667 -4.1004553 -4.0840073 -4.1148815 -4.1596718 -4.1880651 -4.203999 -4.2251945 -4.2456288 -4.26381][-4.1950884 -4.1978912 -4.20009 -4.1947823 -4.1746874 -4.1428614 -4.0977726 -4.0800724 -4.110795 -4.1555142 -4.1817141 -4.1998739 -4.2254128 -4.2485614 -4.2647033][-4.1847181 -4.1806412 -4.1783266 -4.1750722 -4.1607275 -4.1276822 -4.0841155 -4.0735779 -4.1122746 -4.1588182 -4.1841383 -4.2030621 -4.2286882 -4.2507606 -4.263721]]...]
INFO - root - 2017-12-05 19:31:50.914262: step 37810, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 79h:03m:51s remains)
INFO - root - 2017-12-05 19:32:00.428449: step 37820, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 77h:48m:28s remains)
INFO - root - 2017-12-05 19:32:09.947953: step 37830, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 75h:24m:41s remains)
INFO - root - 2017-12-05 19:32:19.168671: step 37840, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 73h:04m:05s remains)
INFO - root - 2017-12-05 19:32:28.803452: step 37850, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 75h:34m:16s remains)
INFO - root - 2017-12-05 19:32:38.243227: step 37860, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 75h:29m:36s remains)
INFO - root - 2017-12-05 19:32:47.503451: step 37870, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 80h:04m:36s remains)
INFO - root - 2017-12-05 19:32:56.839791: step 37880, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 81h:42m:27s remains)
INFO - root - 2017-12-05 19:33:06.226301: step 37890, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.028 sec/batch; 84h:08m:32s remains)
INFO - root - 2017-12-05 19:33:15.596971: step 37900, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.947 sec/batch; 77h:28m:08s remains)
2017-12-05 19:33:16.387160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2990265 -4.2891212 -4.2718239 -4.2458563 -4.2131805 -4.1837506 -4.1470942 -4.08665 -4.0570669 -4.0685868 -4.0984287 -4.1050158 -4.0773969 -4.0772247 -4.093648][-4.2943311 -4.2793117 -4.2552757 -4.2225108 -4.1808243 -4.1431637 -4.0983577 -4.0422788 -4.0341763 -4.0598187 -4.0950708 -4.1066713 -4.0804896 -4.0749693 -4.0855021][-4.2902994 -4.2729492 -4.2483191 -4.2140994 -4.1691418 -4.1235356 -4.0710211 -4.0185823 -4.0283871 -4.06578 -4.1021838 -4.1211157 -4.1017475 -4.0911417 -4.0922804][-4.2905173 -4.2770839 -4.2571206 -4.2271056 -4.1841803 -4.1377268 -4.0801439 -4.0310769 -4.0459256 -4.0894341 -4.1217942 -4.1399431 -4.134387 -4.1241736 -4.112555][-4.2919974 -4.2834988 -4.2695851 -4.2443013 -4.203301 -4.1572514 -4.0932469 -4.041728 -4.0561 -4.1018057 -4.1347523 -4.1512895 -4.1605997 -4.1586862 -4.1368709][-4.2941184 -4.2859759 -4.2734632 -4.2440419 -4.1970191 -4.1491079 -4.0816813 -4.015727 -4.0065212 -4.0454888 -4.0800071 -4.1060896 -4.1355605 -4.1524935 -4.1378689][-4.2938457 -4.2778578 -4.2551484 -4.2150979 -4.1578159 -4.1039944 -4.0308175 -3.9350154 -3.8725998 -3.881382 -3.9433734 -4.0075417 -4.0606079 -4.0987539 -4.1021447][-4.2853808 -4.2571788 -4.2222867 -4.171052 -4.1045933 -4.0478768 -3.968859 -3.8445349 -3.7235386 -3.693083 -3.8106089 -3.9226213 -3.9947324 -4.0469923 -4.0670376][-4.2694278 -4.2349706 -4.1989169 -4.1489573 -4.0843606 -4.0330091 -3.9591384 -3.8397019 -3.7243881 -3.7030563 -3.8284061 -3.9401405 -4.0039849 -4.0551844 -4.0853491][-4.2637291 -4.2347908 -4.2096915 -4.1753931 -4.128232 -4.08643 -4.0292568 -3.9450991 -3.8856926 -3.8956494 -3.9678578 -4.0360856 -4.085145 -4.1298513 -4.1578856][-4.2730885 -4.2583475 -4.2491879 -4.2322249 -4.2030659 -4.1689572 -4.131772 -4.078258 -4.0521708 -4.0773597 -4.111351 -4.1448159 -4.1805844 -4.2174907 -4.2323341][-4.2897 -4.2867079 -4.28752 -4.277802 -4.2568116 -4.2317166 -4.2095709 -4.1712408 -4.1589103 -4.188942 -4.2136526 -4.2316647 -4.2523589 -4.267487 -4.2606015][-4.3047814 -4.3066363 -4.3079982 -4.2995648 -4.2782235 -4.2558589 -4.2437949 -4.2177629 -4.2136912 -4.2454605 -4.2704511 -4.2808046 -4.2842412 -4.277205 -4.255456][-4.3147597 -4.3144293 -4.3099194 -4.2992406 -4.2799077 -4.2610836 -4.2521205 -4.2315164 -4.23262 -4.2616692 -4.281096 -4.2844715 -4.2782578 -4.257616 -4.2309971][-4.3176556 -4.3127646 -4.3006773 -4.2856503 -4.2647114 -4.2477288 -4.2357454 -4.2189307 -4.2234049 -4.2458892 -4.262702 -4.2678022 -4.260675 -4.2419386 -4.2206664]]...]
INFO - root - 2017-12-05 19:33:25.922738: step 37910, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 79h:52m:47s remains)
INFO - root - 2017-12-05 19:33:35.366271: step 37920, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 80h:15m:02s remains)
INFO - root - 2017-12-05 19:33:44.523407: step 37930, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 68h:48m:14s remains)
INFO - root - 2017-12-05 19:33:53.716430: step 37940, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 77h:42m:18s remains)
INFO - root - 2017-12-05 19:34:03.244422: step 37950, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 81h:27m:34s remains)
INFO - root - 2017-12-05 19:34:12.779751: step 37960, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 79h:36m:01s remains)
INFO - root - 2017-12-05 19:34:21.831554: step 37970, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 74h:16m:56s remains)
INFO - root - 2017-12-05 19:34:31.298401: step 37980, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 76h:42m:50s remains)
INFO - root - 2017-12-05 19:34:40.815483: step 37990, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 77h:47m:18s remains)
INFO - root - 2017-12-05 19:34:50.163945: step 38000, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 73h:39m:21s remains)
2017-12-05 19:34:50.971809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2883697 -4.2593241 -4.2220154 -4.1862822 -4.1620464 -4.155592 -4.1614733 -4.1675749 -4.1749425 -4.187068 -4.1980271 -4.2022958 -4.2036633 -4.2078886 -4.2117863][-4.2517648 -4.22297 -4.1874952 -4.1557579 -4.1368465 -4.1359873 -4.1490655 -4.1622977 -4.1708674 -4.1797161 -4.1889868 -4.1946492 -4.2006674 -4.2119079 -4.2219663][-4.2046213 -4.1762233 -4.1490612 -4.1328077 -4.1298242 -4.1414557 -4.1623592 -4.1794538 -4.185194 -4.1861415 -4.1886816 -4.1958275 -4.2077579 -4.2239575 -4.24004][-4.1548262 -4.1259642 -4.1079926 -4.1088028 -4.12304 -4.1498394 -4.1803651 -4.1996512 -4.201438 -4.1951017 -4.1939659 -4.2036476 -4.2221913 -4.2412276 -4.2585583][-4.1081443 -4.07626 -4.0650344 -4.0777802 -4.1063662 -4.1436305 -4.1806154 -4.1999865 -4.2000246 -4.1954737 -4.2002754 -4.2156334 -4.2368016 -4.2543459 -4.267468][-4.0800681 -4.0415854 -4.0262074 -4.0406618 -4.0728016 -4.1114035 -4.1526394 -4.1766534 -4.1834717 -4.1873946 -4.2008944 -4.2236581 -4.2468448 -4.2613015 -4.269659][-4.077847 -4.0329509 -4.0041924 -4.0073118 -4.0284858 -4.0583181 -4.09824 -4.128221 -4.1471634 -4.1597013 -4.1788411 -4.2074575 -4.2319512 -4.2464948 -4.2547255][-4.0967965 -4.0505252 -4.0152879 -4.00137 -3.9992666 -4.011724 -4.0429411 -4.0766549 -4.1067228 -4.1300206 -4.1566753 -4.1879845 -4.2128172 -4.2289968 -4.240695][-4.124042 -4.0860219 -4.0534668 -4.0277319 -4.0088878 -4.0058908 -4.0262823 -4.0561948 -4.0887566 -4.1188273 -4.14885 -4.1726723 -4.1875114 -4.2044568 -4.2230039][-4.1364112 -4.1124158 -4.0925751 -4.0709333 -4.0504947 -4.0430603 -4.054369 -4.0758762 -4.1016874 -4.1293731 -4.1520176 -4.1642857 -4.1668572 -4.1775751 -4.1960945][-4.1446261 -4.1320457 -4.1238174 -4.1125407 -4.0996122 -4.0954432 -4.1006284 -4.1102619 -4.1252465 -4.1460662 -4.1618314 -4.1699696 -4.166162 -4.1672163 -4.1798668][-4.1360941 -4.1332521 -4.1314 -4.1259632 -4.12138 -4.1218667 -4.1275039 -4.1341348 -4.1439347 -4.1580954 -4.1677914 -4.1744657 -4.1717362 -4.1699 -4.174655][-4.1227078 -4.1248779 -4.1259403 -4.1232443 -4.1230855 -4.1268892 -4.1343732 -4.144093 -4.1546669 -4.1637077 -4.1685767 -4.17258 -4.1718845 -4.1711335 -4.1711383][-4.1364322 -4.1396537 -4.1403975 -4.1374879 -4.1352797 -4.1381173 -4.1469207 -4.1572042 -4.164475 -4.168129 -4.1697655 -4.1743093 -4.1793232 -4.1801777 -4.1782312][-4.1732306 -4.1739049 -4.1728697 -4.1687155 -4.163579 -4.1643376 -4.1695075 -4.1758776 -4.1805615 -4.1845055 -4.1864524 -4.1922646 -4.2007928 -4.2027059 -4.1998453]]...]
INFO - root - 2017-12-05 19:35:00.588689: step 38010, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 79h:31m:09s remains)
INFO - root - 2017-12-05 19:35:09.956159: step 38020, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 74h:10m:50s remains)
INFO - root - 2017-12-05 19:35:19.182337: step 38030, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 76h:41m:58s remains)
INFO - root - 2017-12-05 19:35:28.736836: step 38040, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.947 sec/batch; 77h:27m:03s remains)
INFO - root - 2017-12-05 19:35:38.158393: step 38050, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 75h:13m:30s remains)
INFO - root - 2017-12-05 19:35:47.633387: step 38060, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 71h:24m:57s remains)
INFO - root - 2017-12-05 19:35:56.988777: step 38070, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 77h:54m:16s remains)
INFO - root - 2017-12-05 19:36:06.304197: step 38080, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 74h:53m:50s remains)
INFO - root - 2017-12-05 19:36:15.820416: step 38090, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 77h:16m:30s remains)
INFO - root - 2017-12-05 19:36:25.190815: step 38100, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 76h:44m:51s remains)
2017-12-05 19:36:25.966806: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3307872 -4.327343 -4.326128 -4.3265462 -4.3291984 -4.3325086 -4.3328018 -4.32817 -4.3189855 -4.308269 -4.2992773 -4.2961421 -4.295805 -4.296669 -4.2964168][-4.3032007 -4.2939897 -4.2908382 -4.2930479 -4.3012266 -4.3075032 -4.3029461 -4.2914472 -4.2751527 -4.2577457 -4.2448859 -4.2431874 -4.2449508 -4.2493258 -4.2529316][-4.2710066 -4.2558575 -4.2518005 -4.2542291 -4.2629466 -4.2686439 -4.2579203 -4.2407942 -4.2227979 -4.203064 -4.1882567 -4.1865311 -4.1920843 -4.2020288 -4.2102308][-4.2407174 -4.2226954 -4.2173948 -4.2165132 -4.2219219 -4.2221026 -4.2008014 -4.17815 -4.162653 -4.1468825 -4.1388416 -4.1447177 -4.1542983 -4.1650753 -4.1742654][-4.213026 -4.1957183 -4.1907673 -4.187768 -4.187542 -4.176228 -4.1328459 -4.0939775 -4.0838189 -4.0859833 -4.0992427 -4.1200624 -4.1317596 -4.1444411 -4.1551147][-4.18326 -4.1660542 -4.162643 -4.1592503 -4.1489987 -4.1173062 -4.0379324 -3.9724607 -3.9808414 -4.0193768 -4.0621905 -4.0943894 -4.103178 -4.1168647 -4.1306591][-4.1615686 -4.1407981 -4.133163 -4.1223135 -4.0924392 -4.026659 -3.8982663 -3.7992563 -3.8490946 -3.9454203 -4.0223913 -4.061842 -4.0637522 -4.0734649 -4.0932121][-4.165287 -4.1465611 -4.1382952 -4.1202426 -4.0751162 -3.9929309 -3.8475065 -3.7475281 -3.8226566 -3.9370761 -4.0190535 -4.0520473 -4.0497584 -4.0562806 -4.0808463][-4.1885767 -4.1745262 -4.1740904 -4.1623816 -4.1246786 -4.0586891 -3.946666 -3.88163 -3.9424405 -4.0241179 -4.0783625 -4.0950661 -4.0881448 -4.0951891 -4.1281757][-4.2061834 -4.1917739 -4.19391 -4.189817 -4.1671596 -4.1237941 -4.0427318 -4.0004549 -4.0463433 -4.1030531 -4.1375737 -4.1456223 -4.137816 -4.1464357 -4.184536][-4.2059078 -4.1920037 -4.1898 -4.1886873 -4.1786294 -4.1520371 -4.0919843 -4.0638146 -4.0973382 -4.135066 -4.15942 -4.1688285 -4.1655321 -4.1780896 -4.2167273][-4.2039714 -4.1936464 -4.1868911 -4.1823654 -4.1739278 -4.1580558 -4.1178236 -4.10128 -4.1247849 -4.1520648 -4.1713185 -4.1824269 -4.1810932 -4.1952987 -4.2306809][-4.2184043 -4.2101674 -4.2029867 -4.196857 -4.1878204 -4.1778407 -4.1528816 -4.1417694 -4.1537752 -4.1702647 -4.1853328 -4.1991286 -4.205246 -4.2220135 -4.2496982][-4.2361012 -4.2304406 -4.2273307 -4.225153 -4.2200494 -4.213829 -4.2005119 -4.1952624 -4.2010431 -4.2058239 -4.2094469 -4.2214985 -4.2334366 -4.2495012 -4.2695436][-4.2562742 -4.2498579 -4.2457623 -4.243638 -4.2386942 -4.2317891 -4.2258763 -4.2269897 -4.2333364 -4.233994 -4.2307072 -4.241334 -4.2568207 -4.2696233 -4.2822814]]...]
INFO - root - 2017-12-05 19:36:35.256554: step 38110, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 74h:33m:02s remains)
INFO - root - 2017-12-05 19:36:44.672020: step 38120, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 79h:37m:19s remains)
INFO - root - 2017-12-05 19:36:54.007554: step 38130, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 75h:52m:02s remains)
INFO - root - 2017-12-05 19:37:03.480274: step 38140, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 74h:34m:10s remains)
INFO - root - 2017-12-05 19:37:12.644177: step 38150, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.925 sec/batch; 75h:35m:43s remains)
INFO - root - 2017-12-05 19:37:21.880461: step 38160, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 81h:01m:07s remains)
INFO - root - 2017-12-05 19:37:31.173721: step 38170, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 77h:30m:38s remains)
INFO - root - 2017-12-05 19:37:40.498258: step 38180, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 73h:30m:30s remains)
INFO - root - 2017-12-05 19:37:50.137268: step 38190, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 80h:20m:31s remains)
INFO - root - 2017-12-05 19:37:59.538852: step 38200, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 77h:38m:06s remains)
2017-12-05 19:38:00.280165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3061619 -4.3095646 -4.3117871 -4.309504 -4.3056254 -4.302001 -4.30104 -4.3019285 -4.2993307 -4.2964067 -4.2898889 -4.2808528 -4.2727151 -4.2743974 -4.2853274][-4.2879372 -4.2908182 -4.2931685 -4.2912579 -4.2867069 -4.2821965 -4.2809029 -4.2829103 -4.28093 -4.2770643 -4.2693577 -4.257206 -4.2461023 -4.2469411 -4.2597351][-4.2670326 -4.267345 -4.2675385 -4.2617149 -4.2531304 -4.2464457 -4.2434435 -4.2435546 -4.2438049 -4.2425327 -4.2377248 -4.2270741 -4.2179294 -4.2207127 -4.2353473][-4.2372437 -4.232801 -4.2309184 -4.2214665 -4.2057018 -4.1889958 -4.1751056 -4.169044 -4.1753111 -4.1837492 -4.1874776 -4.1837416 -4.1793017 -4.1849036 -4.2032967][-4.2015429 -4.1912017 -4.1854067 -4.1705384 -4.1463761 -4.1154032 -4.0826836 -4.066895 -4.0829258 -4.1111045 -4.1281776 -4.1349888 -4.1330638 -4.1407957 -4.164999][-4.1706886 -4.1505876 -4.1387663 -4.1177454 -4.0821362 -4.0317364 -3.9682789 -3.9284956 -3.9522648 -4.0123835 -4.05389 -4.0749907 -4.0824871 -4.0998168 -4.1345081][-4.1604023 -4.1296639 -4.107336 -4.0768595 -4.0278239 -3.9550462 -3.8529081 -3.7714453 -3.7868633 -3.8788528 -3.9515586 -3.9930453 -4.0168509 -4.0519729 -4.1066766][-4.1746531 -4.1430492 -4.1193805 -4.0879397 -4.0374012 -3.9603021 -3.8451927 -3.7341492 -3.7179258 -3.8004024 -3.8755467 -3.9236722 -3.9597445 -4.0102119 -4.082839][-4.2085886 -4.1871071 -4.1733489 -4.1553073 -4.1231127 -4.0693717 -3.9839971 -3.8924546 -3.8531384 -3.8805773 -3.918987 -3.9518807 -3.9828331 -4.0293393 -4.0970497][-4.2403731 -4.2315335 -4.2301774 -4.226264 -4.2118349 -4.1822047 -4.1312728 -4.0729961 -4.0343647 -4.0277996 -4.034462 -4.0514679 -4.075592 -4.1089225 -4.1548476][-4.2737308 -4.2726655 -4.2769623 -4.2794418 -4.2770157 -4.2655807 -4.2423339 -4.2140265 -4.1886253 -4.1744466 -4.1668067 -4.1694632 -4.1845474 -4.2069921 -4.231916][-4.3057637 -4.3067989 -4.3107042 -4.3137069 -4.3159666 -4.3147469 -4.3062243 -4.2942376 -4.2799635 -4.2662406 -4.2564459 -4.2545028 -4.2626252 -4.2783837 -4.2918878][-4.3254046 -4.326962 -4.3304725 -4.3343425 -4.3383703 -4.3406281 -4.3385248 -4.3331742 -4.3240066 -4.3133726 -4.3045583 -4.3006077 -4.3014536 -4.3108749 -4.3193846][-4.3372321 -4.3375225 -4.3388414 -4.3411717 -4.3449764 -4.3485913 -4.3502297 -4.3496571 -4.3451881 -4.3381424 -4.3303504 -4.3249168 -4.3217196 -4.3246636 -4.3290806][-4.34441 -4.343996 -4.3436837 -4.3437891 -4.3447833 -4.3464241 -4.3482809 -4.3494334 -4.3485432 -4.3458323 -4.3425312 -4.340055 -4.3377829 -4.3380208 -4.3394742]]...]
INFO - root - 2017-12-05 19:38:09.728932: step 38210, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 78h:36m:57s remains)
INFO - root - 2017-12-05 19:38:19.167109: step 38220, loss = 2.04, batch loss = 1.98 (7.9 examples/sec; 1.008 sec/batch; 82h:24m:18s remains)
INFO - root - 2017-12-05 19:38:28.616054: step 38230, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.947 sec/batch; 77h:25m:34s remains)
INFO - root - 2017-12-05 19:38:38.148945: step 38240, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 75h:32m:27s remains)
INFO - root - 2017-12-05 19:38:47.346938: step 38250, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 73h:42m:51s remains)
INFO - root - 2017-12-05 19:38:56.805254: step 38260, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 69h:10m:38s remains)
INFO - root - 2017-12-05 19:39:06.099582: step 38270, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 77h:29m:53s remains)
INFO - root - 2017-12-05 19:39:15.476492: step 38280, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 69h:10m:42s remains)
INFO - root - 2017-12-05 19:39:25.220512: step 38290, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 73h:21m:12s remains)
INFO - root - 2017-12-05 19:39:34.592863: step 38300, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 78h:53m:22s remains)
2017-12-05 19:39:35.416929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2913022 -4.2829261 -4.2673283 -4.2459702 -4.2349019 -4.2311687 -4.2247424 -4.2190766 -4.21117 -4.1820836 -4.1421556 -4.1226792 -4.1199841 -4.1270962 -4.1363907][-4.2893438 -4.2801723 -4.2637839 -4.2425113 -4.2333894 -4.23177 -4.22685 -4.22473 -4.2262807 -4.2024288 -4.1623249 -4.1438913 -4.1431317 -4.1454725 -4.1490092][-4.2826695 -4.2721376 -4.25809 -4.2405143 -4.2338929 -4.2350006 -4.2296915 -4.2262206 -4.2311463 -4.2102985 -4.1695023 -4.1502113 -4.1516981 -4.1543021 -4.1608348][-4.2674727 -4.2573695 -4.2467117 -4.2315664 -4.2257423 -4.2239966 -4.2122498 -4.1984696 -4.1976905 -4.1830864 -4.1502981 -4.1361651 -4.1445947 -4.1555662 -4.1691446][-4.2564745 -4.2457027 -4.2345295 -4.2141328 -4.1955132 -4.1799331 -4.1549115 -4.129374 -4.1241779 -4.124156 -4.1129494 -4.1129446 -4.129693 -4.14781 -4.1691918][-4.2305431 -4.2174935 -4.20099 -4.1698804 -4.1282363 -4.0926843 -4.0556479 -4.0218248 -4.0177679 -4.0386696 -4.0625958 -4.0937643 -4.1267061 -4.1540532 -4.1794024][-4.1713586 -4.1542983 -4.1327252 -4.0952988 -4.0405374 -3.9880152 -3.9353552 -3.8935401 -3.9076843 -3.9699354 -4.0392923 -4.1035838 -4.15012 -4.1803985 -4.2039008][-4.1019926 -4.085525 -4.0672607 -4.0344615 -3.9812369 -3.9267926 -3.8755434 -3.8461173 -3.893244 -3.9861619 -4.07336 -4.1412587 -4.1838913 -4.2060118 -4.2220197][-4.0569015 -4.05246 -4.0511823 -4.0389314 -4.0107579 -3.9844363 -3.9629912 -3.9563978 -3.9991441 -4.0668635 -4.1237612 -4.1674967 -4.19481 -4.2090273 -4.2188492][-4.0679245 -4.0766559 -4.0888433 -4.0918427 -4.0796647 -4.072742 -4.0726395 -4.0758691 -4.0988321 -4.1304765 -4.1486545 -4.1626234 -4.1746821 -4.1849551 -4.1982303][-4.1008005 -4.1171679 -4.1309805 -4.1381536 -4.1339011 -4.136488 -4.1468644 -4.1534953 -4.1616707 -4.1628447 -4.1480064 -4.1371307 -4.1363726 -4.1454191 -4.1664414][-4.1196861 -4.1356816 -4.1527414 -4.164259 -4.1634183 -4.1633821 -4.1706405 -4.1750383 -4.1729431 -4.1565366 -4.1273313 -4.1106143 -4.1105905 -4.1246319 -4.1518068][-4.1449842 -4.1547937 -4.1679816 -4.1739926 -4.1685238 -4.1633582 -4.1641688 -4.1614575 -4.1515141 -4.1306715 -4.1067886 -4.1047249 -4.1173568 -4.1402092 -4.1700792][-4.1721706 -4.1720986 -4.1789522 -4.1804886 -4.1731029 -4.1635637 -4.15569 -4.1474872 -4.1372724 -4.124083 -4.1146894 -4.1248107 -4.1443868 -4.1644998 -4.1849661][-4.1814561 -4.17591 -4.1774139 -4.1792831 -4.1761761 -4.16734 -4.1580462 -4.1505551 -4.1448221 -4.1377869 -4.1339741 -4.1400833 -4.1488676 -4.1565118 -4.1660471]]...]
INFO - root - 2017-12-05 19:39:44.754302: step 38310, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 78h:18m:29s remains)
INFO - root - 2017-12-05 19:39:54.084694: step 38320, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 76h:24m:13s remains)
INFO - root - 2017-12-05 19:40:03.315457: step 38330, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 78h:30m:24s remains)
INFO - root - 2017-12-05 19:40:12.756842: step 38340, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 77h:04m:48s remains)
INFO - root - 2017-12-05 19:40:22.291293: step 38350, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 77h:08m:59s remains)
INFO - root - 2017-12-05 19:40:31.602055: step 38360, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 78h:24m:11s remains)
INFO - root - 2017-12-05 19:40:41.157474: step 38370, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 80h:37m:09s remains)
INFO - root - 2017-12-05 19:40:50.499961: step 38380, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 77h:43m:15s remains)
INFO - root - 2017-12-05 19:40:59.896182: step 38390, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 79h:01m:54s remains)
INFO - root - 2017-12-05 19:41:09.162998: step 38400, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 80h:09m:03s remains)
2017-12-05 19:41:09.932070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3176808 -4.3179569 -4.3208141 -4.3217554 -4.3173375 -4.3106637 -4.3074927 -4.3086567 -4.3112826 -4.3159013 -4.3205051 -4.3233666 -4.3246393 -4.3221769 -4.3180289][-4.297348 -4.29713 -4.3011513 -4.3008089 -4.2917943 -4.2853408 -4.2865491 -4.2909212 -4.2960043 -4.3043 -4.3129034 -4.3189106 -4.317852 -4.3083448 -4.2958937][-4.2496462 -4.2461581 -4.2497239 -4.2468638 -4.2328076 -4.229609 -4.2389965 -4.2492762 -4.2579479 -4.2727571 -4.288568 -4.3014526 -4.2996912 -4.2830906 -4.2615371][-4.1721244 -4.1697559 -4.1779838 -4.1732812 -4.1542621 -4.151865 -4.1651387 -4.1828 -4.2008071 -4.2241483 -4.2466841 -4.2645793 -4.2620711 -4.2375116 -4.2086263][-4.1053987 -4.1053123 -4.1136069 -4.102211 -4.0777726 -4.074245 -4.0853739 -4.1147795 -4.1470752 -4.1793284 -4.2050319 -4.2228732 -4.2196789 -4.1851664 -4.1463161][-4.0721927 -4.0658612 -4.0551181 -4.0203962 -3.9835255 -3.9706643 -3.9819369 -4.0348759 -4.0917382 -4.1382647 -4.1716108 -4.188324 -4.1798639 -4.1350312 -4.0876355][-4.0781384 -4.0577807 -4.0222235 -3.9606977 -3.9048758 -3.870065 -3.8729353 -3.949676 -4.0319366 -4.090333 -4.1290889 -4.1440911 -4.1290531 -4.0824285 -4.0370336][-4.1263161 -4.098094 -4.0539708 -3.9873343 -3.928791 -3.8778553 -3.8604188 -3.9334669 -4.0158939 -4.0764751 -4.1169167 -4.1331253 -4.1175036 -4.0757527 -4.0375805][-4.1761608 -4.147572 -4.1122456 -4.0680451 -4.0318708 -3.9928548 -3.9636698 -4.0037231 -4.0612507 -4.1153278 -4.1553454 -4.172833 -4.1621966 -4.1227031 -4.0858083][-4.189002 -4.1708937 -4.1520462 -4.1371131 -4.1272192 -4.1103926 -4.0835662 -4.0955005 -4.1243649 -4.1632462 -4.1940589 -4.2129431 -4.2040668 -4.1656246 -4.1290917][-4.1879358 -4.1817889 -4.1737204 -4.1771584 -4.1885486 -4.1899796 -4.1708474 -4.1692228 -4.177556 -4.1947188 -4.2180915 -4.2392883 -4.2335935 -4.1944842 -4.1541195][-4.1884651 -4.1873403 -4.1882219 -4.2039928 -4.2297029 -4.245018 -4.2332096 -4.2290707 -4.2257934 -4.2229333 -4.2383938 -4.2607269 -4.2612691 -4.2278905 -4.1914506][-4.1933908 -4.1858754 -4.1912861 -4.2149348 -4.2476015 -4.2704573 -4.2653441 -4.2635617 -4.2547708 -4.2382216 -4.2453589 -4.2647724 -4.2746949 -4.2557306 -4.2298646][-4.2138 -4.2016549 -4.2046824 -4.22412 -4.2478285 -4.2617168 -4.2584324 -4.2651143 -4.2620211 -4.2405934 -4.2342644 -4.24598 -4.2606869 -4.2575727 -4.2433634][-4.2475247 -4.2333941 -4.2329769 -4.2409329 -4.2459755 -4.2471437 -4.2456818 -4.2604761 -4.2606158 -4.2317481 -4.2051625 -4.2052445 -4.2236919 -4.2310529 -4.2232962]]...]
INFO - root - 2017-12-05 19:41:19.475625: step 38410, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.942 sec/batch; 76h:57m:03s remains)
INFO - root - 2017-12-05 19:41:28.811519: step 38420, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 75h:48m:56s remains)
INFO - root - 2017-12-05 19:41:37.958573: step 38430, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 69h:18m:46s remains)
INFO - root - 2017-12-05 19:41:47.366412: step 38440, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 74h:19m:24s remains)
INFO - root - 2017-12-05 19:41:56.781876: step 38450, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 75h:43m:59s remains)
INFO - root - 2017-12-05 19:42:06.208459: step 38460, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.944 sec/batch; 77h:08m:21s remains)
INFO - root - 2017-12-05 19:42:15.664863: step 38470, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 74h:59m:39s remains)
INFO - root - 2017-12-05 19:42:25.100228: step 38480, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.026 sec/batch; 83h:48m:20s remains)
INFO - root - 2017-12-05 19:42:34.523690: step 38490, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 75h:33m:19s remains)
INFO - root - 2017-12-05 19:42:44.068103: step 38500, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 0.995 sec/batch; 81h:17m:32s remains)
2017-12-05 19:42:44.856395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.250906 -4.2562413 -4.263485 -4.2718115 -4.2764935 -4.2730923 -4.2676229 -4.2683845 -4.2767134 -4.2815652 -4.2861748 -4.2938924 -4.3068724 -4.3195271 -4.3288593][-4.191186 -4.1968646 -4.207386 -4.2201514 -4.2250228 -4.2204618 -4.2180805 -4.2236819 -4.2366514 -4.241631 -4.2450838 -4.256834 -4.278 -4.2989397 -4.3159909][-4.1402707 -4.1367044 -4.1488123 -4.1702666 -4.17835 -4.171041 -4.1661339 -4.1753082 -4.1908131 -4.1951966 -4.1984878 -4.2124519 -4.2376 -4.265389 -4.2907515][-4.1235065 -4.1079168 -4.1127911 -4.1329074 -4.136766 -4.1215825 -4.1044 -4.1136212 -4.1369929 -4.1436324 -4.1448741 -4.1570463 -4.1844783 -4.2203975 -4.2565994][-4.1427255 -4.1209168 -4.1119084 -4.1153059 -4.098135 -4.0582337 -4.007875 -4.001687 -4.0368433 -4.0591874 -4.0673113 -4.0836325 -4.11828 -4.1676474 -4.2210922][-4.1785784 -4.1573334 -4.1400695 -4.126317 -4.0829253 -4.0065651 -3.9157605 -3.8906264 -3.9361923 -3.9779444 -3.9978187 -4.021131 -4.0655384 -4.1276379 -4.1959434][-4.2182322 -4.1982927 -4.1778502 -4.1534557 -4.0910854 -3.9920435 -3.8804781 -3.8532856 -3.9038725 -3.9528079 -3.9796596 -4.009181 -4.056674 -4.1199131 -4.186017][-4.2432432 -4.2239938 -4.2039623 -4.1750555 -4.1087475 -4.0165296 -3.920763 -3.9034438 -3.9517393 -3.9986289 -4.0282464 -4.0613995 -4.104382 -4.1542897 -4.2040482][-4.2560706 -4.2401137 -4.2251859 -4.202745 -4.1510797 -4.08605 -4.0228043 -4.0112834 -4.0415487 -4.07974 -4.1043129 -4.1299281 -4.1632795 -4.2010708 -4.236546][-4.2645459 -4.2583294 -4.2544489 -4.2413158 -4.2035747 -4.1605883 -4.118856 -4.1082435 -4.12172 -4.147151 -4.1623335 -4.1760583 -4.2000051 -4.2315946 -4.2597971][-4.2686729 -4.2697506 -4.2728906 -4.2651715 -4.2392468 -4.2093868 -4.1792059 -4.1739907 -4.181972 -4.1976795 -4.2028742 -4.2038531 -4.2218461 -4.2506056 -4.2746983][-4.2736845 -4.2761512 -4.2786183 -4.2733383 -4.253581 -4.2290206 -4.2064438 -4.2080073 -4.2159986 -4.2221603 -4.2197776 -4.2158008 -4.2319841 -4.2589746 -4.2822037][-4.27155 -4.2719364 -4.2678652 -4.2640862 -4.2512212 -4.2336063 -4.2190418 -4.2216368 -4.2290339 -4.2324309 -4.2289629 -4.2264276 -4.2409739 -4.2653646 -4.2877917][-4.2710414 -4.270226 -4.2628274 -4.2592254 -4.2498584 -4.2392249 -4.2320962 -4.23418 -4.2405958 -4.2461076 -4.2459645 -4.2464094 -4.2583075 -4.277267 -4.296804][-4.2792974 -4.2772455 -4.2709336 -4.2687941 -4.2627916 -4.2573709 -4.2570858 -4.25811 -4.2611089 -4.2677436 -4.2709985 -4.27396 -4.282937 -4.2955866 -4.3090892]]...]
INFO - root - 2017-12-05 19:42:54.229736: step 38510, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 77h:19m:25s remains)
INFO - root - 2017-12-05 19:43:03.735488: step 38520, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 76h:30m:10s remains)
INFO - root - 2017-12-05 19:43:13.158766: step 38530, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 75h:34m:28s remains)
INFO - root - 2017-12-05 19:43:22.376721: step 38540, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 75h:52m:52s remains)
INFO - root - 2017-12-05 19:43:31.702592: step 38550, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 75h:59m:16s remains)
INFO - root - 2017-12-05 19:43:41.005759: step 38560, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 73h:44m:31s remains)
INFO - root - 2017-12-05 19:43:50.398436: step 38570, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 76h:51m:35s remains)
INFO - root - 2017-12-05 19:43:59.805141: step 38580, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.935 sec/batch; 76h:20m:08s remains)
INFO - root - 2017-12-05 19:44:09.173494: step 38590, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 76h:05m:30s remains)
INFO - root - 2017-12-05 19:44:18.642454: step 38600, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 81h:20m:34s remains)
2017-12-05 19:44:19.477810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3123212 -4.3082895 -4.2801757 -4.2252841 -4.1536317 -4.0786529 -4.0126367 -3.9989705 -4.0759277 -4.1627235 -4.2082238 -4.226274 -4.2301817 -4.2278538 -4.2282958][-4.3142519 -4.3113375 -4.2849813 -4.2355261 -4.1796894 -4.1302028 -4.0900822 -4.0868855 -4.1438408 -4.2061887 -4.2401328 -4.2582397 -4.2622018 -4.2568631 -4.25079][-4.3105564 -4.3132291 -4.2957945 -4.2588091 -4.2168808 -4.1904511 -4.1776719 -4.1833067 -4.2122464 -4.2442093 -4.2664661 -4.2826443 -4.2908974 -4.2895241 -4.2802072][-4.294116 -4.3037577 -4.2954726 -4.2674379 -4.231194 -4.2149029 -4.2170191 -4.2299867 -4.244452 -4.2606549 -4.2692132 -4.2782969 -4.2902579 -4.2960887 -4.2875071][-4.2755494 -4.2895 -4.2821293 -4.252728 -4.2150588 -4.196116 -4.1967378 -4.2072654 -4.2255883 -4.2451844 -4.2509155 -4.2549567 -4.2621641 -4.2699685 -4.2633743][-4.2591982 -4.2787304 -4.2671967 -4.2264986 -4.1771584 -4.13868 -4.1157613 -4.1146512 -4.1451097 -4.1880746 -4.2107234 -4.2168679 -4.2178454 -4.2214732 -4.2180519][-4.2410007 -4.2665982 -4.25379 -4.2038164 -4.1360869 -4.06746 -4.0045223 -3.9777925 -4.0311017 -4.1184006 -4.1677008 -4.1793194 -4.1760468 -4.1755075 -4.1713462][-4.2234535 -4.2533288 -4.242929 -4.1910925 -4.1117272 -4.0260811 -3.9330273 -3.8849535 -3.957725 -4.0745835 -4.1387572 -4.1561 -4.1595922 -4.1598263 -4.1537414][-4.2216921 -4.2512627 -4.2442751 -4.1979227 -4.1258788 -4.0472813 -3.9551497 -3.9067676 -3.96775 -4.0692749 -4.1283221 -4.15423 -4.1732941 -4.182971 -4.1800723][-4.2378674 -4.2584019 -4.2534413 -4.2160873 -4.1572685 -4.0970154 -4.0267334 -3.9907913 -4.028368 -4.1004348 -4.1480646 -4.1777568 -4.204155 -4.2224636 -4.2262006][-4.2594528 -4.2677813 -4.2633057 -4.233345 -4.1848321 -4.1382942 -4.0928569 -4.0738544 -4.1035151 -4.1576462 -4.1957908 -4.2211366 -4.2453618 -4.2663493 -4.2744689][-4.2736907 -4.2777047 -4.276155 -4.2517209 -4.2109647 -4.1738381 -4.1460862 -4.1428561 -4.1774487 -4.2245245 -4.2559381 -4.2759991 -4.2928615 -4.3086982 -4.3197684][-4.2842131 -4.288835 -4.2897921 -4.2718711 -4.2396231 -4.2127762 -4.1992989 -4.2063665 -4.2411108 -4.2827334 -4.3091335 -4.3226628 -4.3283792 -4.3338408 -4.3415089][-4.29682 -4.3019409 -4.303247 -4.2903676 -4.26718 -4.2480536 -4.2418246 -4.2508655 -4.2779241 -4.3113432 -4.3331761 -4.3428741 -4.3407636 -4.3352251 -4.3324938][-4.3057742 -4.3106756 -4.3116369 -4.3026943 -4.2859635 -4.2703876 -4.2645125 -4.2699318 -4.2876611 -4.3133407 -4.3326626 -4.3407016 -4.3347106 -4.3220382 -4.3090882]]...]
INFO - root - 2017-12-05 19:44:28.923544: step 38610, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 77h:33m:29s remains)
INFO - root - 2017-12-05 19:44:38.190608: step 38620, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 75h:43m:45s remains)
INFO - root - 2017-12-05 19:44:47.676127: step 38630, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 74h:37m:00s remains)
INFO - root - 2017-12-05 19:44:57.137648: step 38640, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 76h:54m:42s remains)
INFO - root - 2017-12-05 19:45:06.340997: step 38650, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 73h:48m:52s remains)
INFO - root - 2017-12-05 19:45:15.539685: step 38660, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 79h:52m:03s remains)
INFO - root - 2017-12-05 19:45:25.080262: step 38670, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 75h:55m:21s remains)
INFO - root - 2017-12-05 19:45:34.486814: step 38680, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 77h:30m:23s remains)
INFO - root - 2017-12-05 19:45:43.783176: step 38690, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 78h:10m:28s remains)
INFO - root - 2017-12-05 19:45:53.162872: step 38700, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.970 sec/batch; 79h:09m:59s remains)
2017-12-05 19:45:53.983579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16308 -4.15419 -4.1377554 -4.1327295 -4.1513629 -4.1806812 -4.2019877 -4.2030344 -4.1856408 -4.1704941 -4.1642604 -4.156682 -4.1632285 -4.1853642 -4.2015085][-4.1745629 -4.1637774 -4.1508636 -4.1532164 -4.1737776 -4.1974478 -4.2069664 -4.1927252 -4.1684833 -4.1633353 -4.1648521 -4.1574473 -4.1612029 -4.18007 -4.1939015][-4.1959915 -4.1817303 -4.1717358 -4.1755838 -4.1913514 -4.2047958 -4.2020879 -4.1752019 -4.14781 -4.1548591 -4.1672745 -4.1642823 -4.1695123 -4.1893873 -4.2056031][-4.2207584 -4.2081485 -4.2031837 -4.2067761 -4.2143278 -4.2138963 -4.1997924 -4.1637778 -4.1358595 -4.1451554 -4.1608658 -4.1656232 -4.1801682 -4.2085714 -4.233654][-4.2418118 -4.2362914 -4.2345195 -4.2335162 -4.2295804 -4.2148547 -4.1898375 -4.1495705 -4.1248951 -4.1352921 -4.1508036 -4.1657395 -4.1933408 -4.2335153 -4.2668939][-4.2526526 -4.2515893 -4.2482624 -4.2396193 -4.2231474 -4.1974134 -4.1594267 -4.11279 -4.0925293 -4.1103888 -4.134263 -4.1634274 -4.2025042 -4.2522764 -4.2920761][-4.2526245 -4.2501583 -4.2420664 -4.225286 -4.1997771 -4.1684871 -4.1186509 -4.0631604 -4.0488257 -4.0791054 -4.1225615 -4.1675739 -4.2140622 -4.2683773 -4.310761][-4.24062 -4.232163 -4.2169552 -4.1934357 -4.1682472 -4.1387277 -4.0869856 -4.0318685 -4.0244646 -4.06542 -4.1253242 -4.1798911 -4.2301836 -4.2842784 -4.325439][-4.2268729 -4.2130904 -4.1945086 -4.1698852 -4.1464963 -4.1214776 -4.0777078 -4.0325294 -4.0296474 -4.0725951 -4.134882 -4.1915545 -4.2447009 -4.2971897 -4.3336482][-4.2267046 -4.210886 -4.1931248 -4.1699543 -4.1455231 -4.1180825 -4.0800657 -4.0453091 -4.0468783 -4.0871191 -4.1441679 -4.2003469 -4.256424 -4.3070574 -4.3376107][-4.24391 -4.2275996 -4.2100787 -4.1882076 -4.1611705 -4.1302338 -4.0980682 -4.0725756 -4.0790482 -4.1138163 -4.1617551 -4.2139697 -4.271121 -4.3169832 -4.3403859][-4.272788 -4.261981 -4.2463183 -4.2245193 -4.1965523 -4.1649175 -4.1377392 -4.1203742 -4.1274137 -4.1538467 -4.1916561 -4.2375412 -4.2907195 -4.3280535 -4.3435903][-4.2946391 -4.2930784 -4.2856841 -4.26873 -4.2432432 -4.2147012 -4.1911049 -4.1759658 -4.1787934 -4.1971545 -4.2261367 -4.2653 -4.3094893 -4.3363771 -4.3445907][-4.3084292 -4.3139 -4.3141084 -4.305079 -4.2837934 -4.2581329 -4.2357941 -4.2212477 -4.2217488 -4.2367973 -4.261764 -4.2933259 -4.3247824 -4.3421779 -4.3451419][-4.3190503 -4.3277874 -4.3330665 -4.32923 -4.3143559 -4.2930808 -4.2739921 -4.2619081 -4.2613196 -4.2743621 -4.2959986 -4.3194623 -4.3386846 -4.3476782 -4.3470731]]...]
INFO - root - 2017-12-05 19:46:03.469544: step 38710, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 75h:53m:27s remains)
INFO - root - 2017-12-05 19:46:12.942511: step 38720, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.936 sec/batch; 76h:22m:38s remains)
INFO - root - 2017-12-05 19:46:22.314104: step 38730, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.888 sec/batch; 72h:30m:00s remains)
INFO - root - 2017-12-05 19:46:31.550826: step 38740, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 74h:30m:54s remains)
INFO - root - 2017-12-05 19:46:40.867937: step 38750, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 76h:06m:23s remains)
INFO - root - 2017-12-05 19:46:50.315167: step 38760, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.958 sec/batch; 78h:08m:24s remains)
INFO - root - 2017-12-05 19:46:59.595437: step 38770, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 74h:51m:23s remains)
INFO - root - 2017-12-05 19:47:09.041712: step 38780, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 75h:55m:31s remains)
INFO - root - 2017-12-05 19:47:18.436126: step 38790, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 74h:31m:34s remains)
INFO - root - 2017-12-05 19:47:27.827337: step 38800, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 69h:16m:16s remains)
2017-12-05 19:47:28.604943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1187048 -4.154479 -4.1875296 -4.2058353 -4.2035289 -4.1851635 -4.1659889 -4.1651745 -4.1806331 -4.2065349 -4.2344794 -4.2513561 -4.2554903 -4.259758 -4.2770238][-4.1330128 -4.1817818 -4.2235193 -4.2410903 -4.2375488 -4.2207084 -4.2022915 -4.20074 -4.2141266 -4.2343564 -4.2521582 -4.2628622 -4.2649474 -4.2697997 -4.2890887][-4.1811981 -4.2263856 -4.2683258 -4.2854853 -4.2776947 -4.2575874 -4.2326636 -4.2239847 -4.2335348 -4.2519293 -4.26155 -4.2650161 -4.2646575 -4.272624 -4.2959495][-4.2379255 -4.2726908 -4.3059688 -4.3145585 -4.29799 -4.2690725 -4.2340035 -4.21761 -4.2276497 -4.2499142 -4.2611027 -4.2650104 -4.2653127 -4.275414 -4.2999063][-4.2838621 -4.3051009 -4.3225355 -4.3131456 -4.2809849 -4.2346406 -4.1855865 -4.1615772 -4.1787114 -4.213222 -4.2378664 -4.2560844 -4.2688046 -4.2809653 -4.301908][-4.3149266 -4.3203607 -4.3154464 -4.2806096 -4.2265525 -4.156487 -4.0841384 -4.0523396 -4.082674 -4.1414442 -4.1879683 -4.2270813 -4.2580862 -4.2770844 -4.2946792][-4.3309021 -4.3221259 -4.29601 -4.2399764 -4.1683254 -4.0798721 -3.9835429 -3.9437342 -3.9890625 -4.074368 -4.1446347 -4.2017212 -4.247303 -4.27256 -4.2874646][-4.3349619 -4.321238 -4.2866659 -4.2247581 -4.1521335 -4.0642843 -3.9626286 -3.9192352 -3.9671409 -4.0591893 -4.142211 -4.2087846 -4.2583203 -4.2829881 -4.2941771][-4.3402214 -4.3301754 -4.2923331 -4.2303338 -4.1670294 -4.0942783 -4.007019 -3.964761 -4.0004873 -4.080708 -4.1612029 -4.2302766 -4.2823157 -4.3063526 -4.3129954][-4.3420682 -4.3354373 -4.2968655 -4.2387719 -4.1800184 -4.12292 -4.0548348 -4.0179214 -4.0451841 -4.1096983 -4.1785851 -4.2467332 -4.3040476 -4.3334804 -4.3397055][-4.3415494 -4.3375397 -4.3036017 -4.2491956 -4.1906905 -4.1424623 -4.0850086 -4.0530362 -4.0784087 -4.1334968 -4.1907516 -4.2532296 -4.3106017 -4.3436728 -4.352993][-4.3424573 -4.3377309 -4.3096128 -4.258307 -4.1978793 -4.1470251 -4.0934296 -4.0647817 -4.0893407 -4.138494 -4.1843796 -4.2356291 -4.2872539 -4.3239017 -4.3368998][-4.3386745 -4.3316312 -4.3076153 -4.2614269 -4.2024045 -4.148901 -4.0981941 -4.06982 -4.08841 -4.1299109 -4.1642418 -4.2017894 -4.2432742 -4.2752366 -4.2898498][-4.3291168 -4.3219609 -4.3001652 -4.2618065 -4.211421 -4.16222 -4.1182461 -4.0887532 -4.0974922 -4.1247573 -4.1480737 -4.1748085 -4.2047391 -4.2282991 -4.2431431][-4.3220015 -4.3138704 -4.2933125 -4.2648597 -4.2298737 -4.1938734 -4.1621218 -4.136302 -4.13283 -4.1406074 -4.146596 -4.1565671 -4.1710992 -4.1859274 -4.2041807]]...]
INFO - root - 2017-12-05 19:47:38.129420: step 38810, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 80h:29m:31s remains)
INFO - root - 2017-12-05 19:47:47.543540: step 38820, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 77h:08m:54s remains)
INFO - root - 2017-12-05 19:47:56.782653: step 38830, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 78h:25m:22s remains)
INFO - root - 2017-12-05 19:48:05.897093: step 38840, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 77h:11m:30s remains)
INFO - root - 2017-12-05 19:48:15.365943: step 38850, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 71h:42m:12s remains)
INFO - root - 2017-12-05 19:48:24.744997: step 38860, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.951 sec/batch; 77h:32m:46s remains)
INFO - root - 2017-12-05 19:48:34.050038: step 38870, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 73h:29m:10s remains)
INFO - root - 2017-12-05 19:48:43.317716: step 38880, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 70h:03m:22s remains)
INFO - root - 2017-12-05 19:48:52.720781: step 38890, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 76h:42m:35s remains)
INFO - root - 2017-12-05 19:49:02.110086: step 38900, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 79h:16m:40s remains)
2017-12-05 19:49:02.888530: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3278022 -4.322577 -4.305017 -4.2810693 -4.271987 -4.2827873 -4.29396 -4.2997026 -4.3051033 -4.3048511 -4.3033485 -4.3080449 -4.3149939 -4.3198581 -4.3289623][-4.3165369 -4.3059325 -4.2817979 -4.2477016 -4.2337966 -4.24744 -4.2625756 -4.2765746 -4.2911553 -4.3017097 -4.306407 -4.3102 -4.3141246 -4.3166595 -4.3250504][-4.2921162 -4.2666192 -4.231719 -4.1861753 -4.1661553 -4.1802974 -4.198102 -4.2216167 -4.248611 -4.2726636 -4.2851844 -4.2892137 -4.2926092 -4.2938566 -4.3039565][-4.2716737 -4.2282457 -4.1782722 -4.1205573 -4.0949874 -4.1011653 -4.109251 -4.1327124 -4.1711297 -4.2099032 -4.2317767 -4.2406974 -4.24614 -4.248435 -4.263361][-4.2676697 -4.2123241 -4.1519489 -4.0817966 -4.0370336 -4.0135112 -3.9903603 -4.004746 -4.0563784 -4.1131592 -4.148283 -4.1665592 -4.1800365 -4.1906762 -4.2132759][-4.27004 -4.2148538 -4.153583 -4.0765743 -4.0089011 -3.9408362 -3.8665009 -3.8578084 -3.9179583 -3.9914293 -4.0435915 -4.0785489 -4.1092863 -4.135673 -4.1628485][-4.2747059 -4.235743 -4.1875777 -4.1191173 -4.0435963 -3.9490056 -3.8415749 -3.8083839 -3.8555064 -3.9210565 -3.9740167 -4.0173173 -4.060236 -4.0949464 -4.1192021][-4.2861934 -4.2660594 -4.2373624 -4.1888762 -4.1268749 -4.0481296 -3.961273 -3.9226866 -3.9363813 -3.9671266 -3.9953153 -4.0263619 -4.0605793 -4.0834007 -4.0933971][-4.3057189 -4.2993088 -4.2852225 -4.255414 -4.2152252 -4.1684084 -4.1183033 -4.0870824 -4.079607 -4.0822153 -4.087851 -4.0985956 -4.1093273 -4.1087317 -4.0979176][-4.32066 -4.3228583 -4.3181281 -4.30281 -4.2822204 -4.2593379 -4.2334762 -4.2118435 -4.1970873 -4.1847243 -4.1772027 -4.1731634 -4.1653385 -4.1488175 -4.1272278][-4.3176465 -4.3226333 -4.3224492 -4.3147645 -4.3046927 -4.2949462 -4.2836576 -4.2741575 -4.2656441 -4.2530971 -4.2407146 -4.2289786 -4.213367 -4.1913152 -4.1709256][-4.3058519 -4.3090405 -4.30433 -4.2929688 -4.2856717 -4.283462 -4.2855945 -4.2914166 -4.2957988 -4.2905154 -4.2803535 -4.2701344 -4.2554207 -4.2329555 -4.2169328][-4.2889161 -4.2906427 -4.280653 -4.2627115 -4.2505908 -4.25195 -4.2668653 -4.2869878 -4.3016958 -4.3011942 -4.29221 -4.2843246 -4.2714577 -4.2506571 -4.2399416][-4.2705684 -4.2734365 -4.2625856 -4.24265 -4.2247634 -4.222527 -4.2395668 -4.2640243 -4.2813196 -4.2823377 -4.2778826 -4.27782 -4.2695556 -4.2512159 -4.2444639][-4.2581677 -4.2637215 -4.2563581 -4.2417932 -4.2271681 -4.2223935 -4.2325578 -4.2478561 -4.2587218 -4.2602406 -4.2608027 -4.2682648 -4.267683 -4.2546716 -4.2492414]]...]
INFO - root - 2017-12-05 19:49:12.151287: step 38910, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 77h:20m:05s remains)
INFO - root - 2017-12-05 19:49:21.566216: step 38920, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 70h:13m:04s remains)
INFO - root - 2017-12-05 19:49:30.885016: step 38930, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 79h:20m:44s remains)
INFO - root - 2017-12-05 19:49:40.232903: step 38940, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 80h:40m:03s remains)
INFO - root - 2017-12-05 19:49:49.806387: step 38950, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 80h:58m:43s remains)
INFO - root - 2017-12-05 19:49:59.201133: step 38960, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 81h:45m:03s remains)
INFO - root - 2017-12-05 19:50:08.400395: step 38970, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 79h:12m:52s remains)
INFO - root - 2017-12-05 19:50:17.911518: step 38980, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 75h:37m:38s remains)
INFO - root - 2017-12-05 19:50:27.250637: step 38990, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 77h:26m:19s remains)
INFO - root - 2017-12-05 19:50:36.636027: step 39000, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 74h:06m:00s remains)
2017-12-05 19:50:37.349974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3423247 -4.3407369 -4.3340964 -4.3252883 -4.3167543 -4.3102531 -4.3081818 -4.3129125 -4.3218188 -4.3303242 -4.3353424 -4.3375497 -4.3376122 -4.3362675 -4.3342943][-4.3429494 -4.3425879 -4.3343315 -4.3202152 -4.3038173 -4.2894382 -4.2813635 -4.2840729 -4.2966056 -4.3109593 -4.3233624 -4.3328295 -4.3380742 -4.3399415 -4.3388758][-4.3224878 -4.3242874 -4.3159719 -4.2960358 -4.2712679 -4.2483768 -4.2346988 -4.2375951 -4.2549467 -4.2753668 -4.2952003 -4.3132749 -4.3274212 -4.3371587 -4.3405633][-4.2889991 -4.2911191 -4.2812686 -4.2580795 -4.2257748 -4.1919732 -4.1699409 -4.1719623 -4.1935534 -4.2181487 -4.2450504 -4.2743621 -4.3001914 -4.3208847 -4.3340235][-4.2507181 -4.2627869 -4.2590642 -4.2360492 -4.1968575 -4.1486363 -4.1093616 -4.0937705 -4.0986285 -4.1157155 -4.1517706 -4.2033095 -4.2517595 -4.2902517 -4.3178697][-4.1909032 -4.2271676 -4.2458467 -4.2406626 -4.2124195 -4.1672268 -4.1146612 -4.0630212 -4.0176911 -3.9949012 -4.0248652 -4.0980253 -4.1748834 -4.2391396 -4.2881][-4.1185951 -4.1623735 -4.2005944 -4.2182417 -4.2173071 -4.1974454 -4.1544771 -4.0852208 -4.0010633 -3.9376521 -3.94364 -4.0102291 -4.0928869 -4.1709275 -4.2388892][-4.0962009 -4.1186857 -4.1532574 -4.1799784 -4.1968927 -4.1948462 -4.1661606 -4.1097302 -4.0373092 -3.9778535 -3.9647598 -4.0006423 -4.058568 -4.126945 -4.197803][-4.13879 -4.1344948 -4.1512704 -4.1702533 -4.1803541 -4.1745787 -4.1505942 -4.1182704 -4.082284 -4.0517492 -4.0385189 -4.0474215 -4.0714903 -4.1149936 -4.1778073][-4.1766472 -4.1603642 -4.1672835 -4.1781306 -4.1814823 -4.1704679 -4.1494217 -4.133954 -4.1193237 -4.1068926 -4.0993309 -4.0997529 -4.1042051 -4.1269569 -4.1757174][-4.1879697 -4.1603456 -4.1495733 -4.1505919 -4.1594987 -4.1672549 -4.1705246 -4.1754112 -4.1730247 -4.1651192 -4.1570315 -4.1516438 -4.1478739 -4.1568322 -4.1893888][-4.2046924 -4.1606488 -4.1191754 -4.0968781 -4.1084309 -4.1407561 -4.1709518 -4.1994658 -4.2172914 -4.2200384 -4.2159929 -4.2087955 -4.1965523 -4.1938891 -4.2132378][-4.2395544 -4.1938362 -4.135179 -4.0881286 -4.083961 -4.1185384 -4.15889 -4.1987276 -4.2265997 -4.2407 -4.2476406 -4.2470984 -4.2368126 -4.2294445 -4.2392397][-4.2634425 -4.2339711 -4.188735 -4.1432252 -4.121923 -4.1301293 -4.1560659 -4.1894259 -4.2120881 -4.2270217 -4.2406034 -4.2510056 -4.2534709 -4.2527614 -4.2607374][-4.2646422 -4.2555437 -4.2324114 -4.2022691 -4.1747928 -4.1564813 -4.1468449 -4.1498928 -4.1550164 -4.1659822 -4.189281 -4.2177911 -4.2421174 -4.2588229 -4.2712412]]...]
INFO - root - 2017-12-05 19:50:46.735307: step 39010, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 72h:23m:13s remains)
INFO - root - 2017-12-05 19:50:56.179048: step 39020, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 76h:35m:08s remains)
INFO - root - 2017-12-05 19:51:05.587818: step 39030, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 78h:51m:10s remains)
INFO - root - 2017-12-05 19:51:14.949976: step 39040, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 76h:18m:39s remains)
INFO - root - 2017-12-05 19:51:24.267929: step 39050, loss = 2.03, batch loss = 1.97 (8.0 examples/sec; 0.996 sec/batch; 81h:09m:06s remains)
INFO - root - 2017-12-05 19:51:33.625467: step 39060, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.946 sec/batch; 77h:05m:36s remains)
INFO - root - 2017-12-05 19:51:43.062770: step 39070, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 76h:28m:13s remains)
INFO - root - 2017-12-05 19:51:52.536626: step 39080, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 77h:31m:41s remains)
INFO - root - 2017-12-05 19:52:01.970040: step 39090, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 80h:00m:27s remains)
INFO - root - 2017-12-05 19:52:11.413724: step 39100, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 71h:56m:44s remains)
2017-12-05 19:52:12.239384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2454052 -4.2454004 -4.2302656 -4.2145214 -4.2105751 -4.1801162 -4.1484284 -4.1881809 -4.2291336 -4.222723 -4.2202673 -4.2099357 -4.1859183 -4.1699409 -4.1781774][-4.2429781 -4.2355065 -4.2191157 -4.2031488 -4.1989326 -4.1741433 -4.1426835 -4.1764269 -4.2169623 -4.2132125 -4.2155151 -4.2086835 -4.1917496 -4.1816039 -4.1905441][-4.2459054 -4.2367272 -4.2183862 -4.1970015 -4.1854696 -4.1622081 -4.1316295 -4.1608768 -4.2070341 -4.217855 -4.2285547 -4.2257595 -4.2141423 -4.2082548 -4.2129636][-4.2570696 -4.24879 -4.2301083 -4.2058187 -4.1905923 -4.1649189 -4.1298695 -4.151093 -4.2005153 -4.2228484 -4.2395597 -4.2374115 -4.2282982 -4.2276154 -4.2292747][-4.2595344 -4.2522144 -4.2327724 -4.2051773 -4.1868234 -4.1568274 -4.1146431 -4.1262512 -4.1747651 -4.2026381 -4.2209315 -4.2181435 -4.2081528 -4.2081604 -4.2104073][-4.2485595 -4.2360816 -4.2117605 -4.1764488 -4.1510582 -4.1156936 -4.0679679 -4.07172 -4.1144896 -4.1422124 -4.1692677 -4.1758795 -4.1656137 -4.1616621 -4.1649904][-4.2231026 -4.2019739 -4.1692071 -4.1263194 -4.0927 -4.0498381 -3.993597 -3.9878268 -4.0113468 -4.0265455 -4.0698109 -4.1014695 -4.1025891 -4.1014853 -4.1120052][-4.2080092 -4.1855006 -4.1481986 -4.09936 -4.0526056 -3.99217 -3.9223986 -3.9019549 -3.8954735 -3.8858514 -3.9481647 -4.0186758 -4.048871 -4.064693 -4.0789943][-4.2184591 -4.2025533 -4.1742692 -4.1359034 -4.0916009 -4.0276427 -3.9629443 -3.9452703 -3.934444 -3.9157305 -3.9686127 -4.04529 -4.0898552 -4.1147952 -4.1250105][-4.240159 -4.233923 -4.21922 -4.195509 -4.1642056 -4.1153417 -4.0720878 -4.0711641 -4.0731316 -4.0594277 -4.0850868 -4.13325 -4.16648 -4.1875052 -4.1922941][-4.2643528 -4.268414 -4.2613363 -4.24633 -4.2260785 -4.1902771 -4.1634545 -4.1764545 -4.1851649 -4.1753578 -4.1832132 -4.2076793 -4.2261 -4.2363682 -4.2347808][-4.2800035 -4.2901735 -4.2838454 -4.2732263 -4.2609758 -4.2328796 -4.214601 -4.2343359 -4.2460527 -4.2394233 -4.2369523 -4.2438087 -4.2487125 -4.247921 -4.2380729][-4.287816 -4.2999 -4.2918177 -4.2822657 -4.2766256 -4.2559657 -4.2436604 -4.2659965 -4.2754459 -4.2676191 -4.2591515 -4.2539105 -4.2463007 -4.2337661 -4.2146153][-4.2858286 -4.2941418 -4.2862821 -4.2771697 -4.2765026 -4.2626481 -4.2531223 -4.2730308 -4.278348 -4.2679987 -4.2564845 -4.2432637 -4.2278032 -4.2109876 -4.1894531][-4.2814784 -4.2845016 -4.2761369 -4.2661057 -4.2666335 -4.2575874 -4.249475 -4.2665143 -4.27162 -4.2650671 -4.2572279 -4.2438736 -4.2255321 -4.2106137 -4.1917815]]...]
INFO - root - 2017-12-05 19:52:21.613998: step 39110, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 75h:12m:25s remains)
INFO - root - 2017-12-05 19:52:30.995519: step 39120, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 73h:10m:20s remains)
INFO - root - 2017-12-05 19:52:40.509021: step 39130, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 79h:08m:58s remains)
INFO - root - 2017-12-05 19:52:49.669590: step 39140, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 75h:31m:25s remains)
INFO - root - 2017-12-05 19:52:59.038065: step 39150, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 72h:23m:22s remains)
INFO - root - 2017-12-05 19:53:08.577625: step 39160, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 77h:05m:45s remains)
INFO - root - 2017-12-05 19:53:18.057798: step 39170, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 78h:40m:55s remains)
INFO - root - 2017-12-05 19:53:27.585741: step 39180, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 78h:37m:12s remains)
INFO - root - 2017-12-05 19:53:36.937301: step 39190, loss = 2.10, batch loss = 2.05 (8.3 examples/sec; 0.968 sec/batch; 78h:53m:15s remains)
INFO - root - 2017-12-05 19:53:45.952258: step 39200, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 68h:44m:33s remains)
2017-12-05 19:53:46.683388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2348447 -4.2329364 -4.2418771 -4.2618523 -4.2811022 -4.2881961 -4.2836967 -4.2815742 -4.2831817 -4.2946634 -4.3061638 -4.3125544 -4.3153968 -4.3040824 -4.2833757][-4.248817 -4.2410297 -4.2358541 -4.2397251 -4.2463965 -4.2484379 -4.24559 -4.2473464 -4.2547622 -4.2735496 -4.2862511 -4.2913637 -4.2946444 -4.2764626 -4.2426043][-4.2643356 -4.2508154 -4.2277102 -4.2102513 -4.2017922 -4.2018094 -4.2045217 -4.2153039 -4.2324662 -4.2589312 -4.2719536 -4.2715759 -4.2687173 -4.239953 -4.1894326][-4.2758746 -4.257473 -4.2197347 -4.1876416 -4.1678133 -4.1646562 -4.1679468 -4.1852803 -4.2141128 -4.2490377 -4.259397 -4.25201 -4.239018 -4.2032938 -4.1429014][-4.2783227 -4.2575755 -4.2107577 -4.1698742 -4.1412439 -4.1307859 -4.1236291 -4.1422586 -4.1855845 -4.2314854 -4.24416 -4.2288976 -4.2066774 -4.168642 -4.1072369][-4.2690654 -4.2487345 -4.1979346 -4.153132 -4.1189227 -4.0874476 -4.0523534 -4.0580812 -4.1205678 -4.1860437 -4.2117887 -4.2015553 -4.1821132 -4.1511717 -4.0985608][-4.2392521 -4.2174611 -4.1642742 -4.1173177 -4.0732388 -4.0063977 -3.9231548 -3.9035757 -3.9947033 -4.098393 -4.1470203 -4.1521382 -4.1437039 -4.12321 -4.0832653][-4.1857471 -4.1629224 -4.1165023 -4.0715504 -4.0143757 -3.9067423 -3.7577398 -3.6963739 -3.817091 -3.9663782 -4.046977 -4.0754423 -4.0799007 -4.06783 -4.0412979][-4.1277885 -4.1139412 -4.0861006 -4.0576143 -4.0109377 -3.9041028 -3.7382083 -3.6450043 -3.7499814 -3.8990273 -3.9868128 -4.0295386 -4.0492015 -4.0504203 -4.0388417][-4.102592 -4.107379 -4.1037073 -4.1012859 -4.0867338 -4.0325909 -3.9342005 -3.8668554 -3.9113212 -3.9917688 -4.043221 -4.073956 -4.0940552 -4.0992842 -4.0927567][-4.1415992 -4.1608391 -4.1719575 -4.1826396 -4.1870317 -4.1745639 -4.1336408 -4.095901 -4.1020703 -4.1257582 -4.1435237 -4.160778 -4.1754737 -4.1805248 -4.1759839][-4.20538 -4.2286925 -4.2423449 -4.2539811 -4.26123 -4.2650003 -4.2528973 -4.2338181 -4.2240868 -4.2220173 -4.2255516 -4.2349648 -4.2456675 -4.2507582 -4.2499175][-4.2492585 -4.2717352 -4.2863975 -4.296627 -4.3046684 -4.3128953 -4.3122191 -4.3029203 -4.2926855 -4.2843685 -4.2815895 -4.2836976 -4.2884884 -4.2922993 -4.2935963][-4.2779202 -4.2951231 -4.3068237 -4.3139963 -4.319521 -4.3272367 -4.3311815 -4.3283262 -4.3224921 -4.317656 -4.3156 -4.3140769 -4.3146796 -4.3167367 -4.3189163][-4.300992 -4.3125596 -4.3208938 -4.3257484 -4.3297772 -4.3349023 -4.3392191 -4.3402925 -4.3397212 -4.3386812 -4.3368735 -4.3340106 -4.3323565 -4.3328977 -4.3350563]]...]
INFO - root - 2017-12-05 19:53:56.064500: step 39210, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 74h:07m:11s remains)
INFO - root - 2017-12-05 19:54:05.541552: step 39220, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 77h:33m:19s remains)
INFO - root - 2017-12-05 19:54:14.837623: step 39230, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 74h:25m:09s remains)
INFO - root - 2017-12-05 19:54:24.283840: step 39240, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 70h:56m:36s remains)
INFO - root - 2017-12-05 19:54:33.848343: step 39250, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 81h:23m:55s remains)
INFO - root - 2017-12-05 19:54:43.106660: step 39260, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 75h:23m:35s remains)
INFO - root - 2017-12-05 19:54:52.674722: step 39270, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.966 sec/batch; 78h:42m:51s remains)
INFO - root - 2017-12-05 19:55:01.905891: step 39280, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.925 sec/batch; 75h:18m:05s remains)
INFO - root - 2017-12-05 19:55:11.188188: step 39290, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.886 sec/batch; 72h:07m:46s remains)
INFO - root - 2017-12-05 19:55:20.528066: step 39300, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 74h:57m:09s remains)
2017-12-05 19:55:21.270389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2074394 -4.187964 -4.1875544 -4.1955519 -4.2053347 -4.2139316 -4.2183495 -4.2218442 -4.2349815 -4.2540026 -4.2596264 -4.2497535 -4.2391829 -4.2237048 -4.2141924][-4.2160306 -4.208848 -4.216774 -4.2249122 -4.2299156 -4.2345624 -4.2339468 -4.2325997 -4.2402534 -4.254302 -4.2605243 -4.2503538 -4.2386303 -4.22471 -4.2150483][-4.2228189 -4.225071 -4.237659 -4.24086 -4.2394214 -4.2438159 -4.241518 -4.2361817 -4.2391667 -4.2484665 -4.2528868 -4.240181 -4.2280774 -4.2164164 -4.202126][-4.2261863 -4.2267847 -4.235074 -4.2335558 -4.2276721 -4.229353 -4.2247396 -4.2168155 -4.2171216 -4.2244854 -4.2275414 -4.2135391 -4.2022777 -4.1938334 -4.1839271][-4.2255907 -4.212914 -4.2107358 -4.2044687 -4.1959724 -4.1930604 -4.1829681 -4.172657 -4.1742778 -4.1849933 -4.1896248 -4.1781554 -4.1667662 -4.162137 -4.1618781][-4.2085214 -4.1830726 -4.1693411 -4.1556516 -4.1414833 -4.1290278 -4.1091323 -4.10135 -4.11412 -4.1316543 -4.1400328 -4.1336622 -4.1279092 -4.1337433 -4.1485825][-4.1846347 -4.1467 -4.125947 -4.1052823 -4.077333 -4.0433607 -4.005249 -4.0035329 -4.0413818 -4.0788369 -4.1015148 -4.1040826 -4.1107655 -4.1300082 -4.1550961][-4.1542382 -4.1041212 -4.0709176 -4.0385413 -3.999176 -3.9467204 -3.8968768 -3.9162345 -3.9915864 -4.0539594 -4.0897684 -4.1038771 -4.1255293 -4.1509237 -4.1744757][-4.1302075 -4.0776329 -4.0364022 -4.0019493 -3.9761887 -3.9464588 -3.920393 -3.9473655 -4.0213332 -4.0781384 -4.1156044 -4.1402187 -4.1664534 -4.1838522 -4.1970444][-4.1285887 -4.0867529 -4.0537972 -4.0328808 -4.0320754 -4.033884 -4.0311966 -4.045702 -4.0901737 -4.13149 -4.1688409 -4.1973438 -4.2191405 -4.222796 -4.2219691][-4.1411223 -4.1168108 -4.0971713 -4.0893908 -4.1018906 -4.1153178 -4.1162996 -4.1213803 -4.1464481 -4.1737013 -4.2019167 -4.2263751 -4.2405992 -4.23789 -4.2324095][-4.1482944 -4.1392121 -4.1323276 -4.1324711 -4.1469045 -4.1579952 -4.1561265 -4.1593733 -4.1759777 -4.1933408 -4.2082644 -4.2197938 -4.2264819 -4.2264032 -4.2286992][-4.16792 -4.1658249 -4.166389 -4.1695471 -4.1805897 -4.1902542 -4.18856 -4.1914129 -4.2029614 -4.2124348 -4.2155595 -4.2170539 -4.2207918 -4.225327 -4.23519][-4.2138906 -4.2102003 -4.2092824 -4.2134156 -4.2212977 -4.2267723 -4.2266011 -4.2306151 -4.237555 -4.2394361 -4.2324929 -4.2270131 -4.2304688 -4.237381 -4.2487693][-4.269639 -4.265821 -4.2653193 -4.2703805 -4.276897 -4.277287 -4.27388 -4.2745285 -4.2785368 -4.2782946 -4.270443 -4.2632771 -4.2647452 -4.2688327 -4.2755542]]...]
INFO - root - 2017-12-05 19:55:30.688912: step 39310, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 70h:50m:51s remains)
INFO - root - 2017-12-05 19:55:40.130552: step 39320, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 79h:07m:10s remains)
INFO - root - 2017-12-05 19:55:49.362615: step 39330, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 73h:59m:21s remains)
INFO - root - 2017-12-05 19:55:58.707547: step 39340, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 76h:14m:26s remains)
INFO - root - 2017-12-05 19:56:08.175321: step 39350, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 77h:08m:49s remains)
INFO - root - 2017-12-05 19:56:17.607933: step 39360, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 74h:09m:08s remains)
INFO - root - 2017-12-05 19:56:26.877164: step 39370, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 74h:06m:37s remains)
INFO - root - 2017-12-05 19:56:36.280744: step 39380, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.962 sec/batch; 78h:21m:21s remains)
INFO - root - 2017-12-05 19:56:45.592656: step 39390, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.948 sec/batch; 77h:10m:11s remains)
INFO - root - 2017-12-05 19:56:54.925730: step 39400, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 75h:25m:41s remains)
2017-12-05 19:56:55.713702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.322319 -4.3192472 -4.312696 -4.3033576 -4.293983 -4.2902422 -4.2888083 -4.2922025 -4.2958765 -4.2968316 -4.2976456 -4.2975249 -4.2980714 -4.29798 -4.2967491][-4.309278 -4.3034611 -4.2951961 -4.2849092 -4.27462 -4.2686987 -4.2633433 -4.2690191 -4.2783642 -4.2834034 -4.2855358 -4.2832756 -4.2817106 -4.2806411 -4.2806][-4.2946482 -4.2873731 -4.2785172 -4.2692485 -4.2578588 -4.2452912 -4.232686 -4.2411652 -4.2581954 -4.2674074 -4.2698994 -4.2650089 -4.2605338 -4.257071 -4.2567749][-4.281229 -4.271039 -4.2591949 -4.2475357 -4.2305532 -4.2048473 -4.18135 -4.1923647 -4.2193394 -4.2352443 -4.2432418 -4.2452741 -4.2433052 -4.23992 -4.2403655][-4.2640085 -4.247673 -4.2284231 -4.2090611 -4.18008 -4.1357064 -4.0971069 -4.1134496 -4.1607637 -4.188684 -4.2044978 -4.21819 -4.223135 -4.2233987 -4.2253523][-4.2426629 -4.2175808 -4.1851931 -4.1535654 -4.1094089 -4.0394731 -3.9754593 -3.9976406 -4.0754662 -4.1219325 -4.1442103 -4.1685529 -4.1815848 -4.191412 -4.1993074][-4.2210751 -4.182982 -4.1370096 -4.0941739 -4.0393887 -3.9462039 -3.8502986 -3.8764143 -3.9882138 -4.0641432 -4.1009955 -4.1319094 -4.1488252 -4.1620531 -4.1733537][-4.2075577 -4.1595936 -4.1051259 -4.05698 -4.00102 -3.9067874 -3.8009462 -3.819972 -3.943629 -4.0432 -4.0990868 -4.1324463 -4.146533 -4.1550822 -4.163784][-4.2131076 -4.16843 -4.1206493 -4.0822015 -4.0431504 -3.9745681 -3.8907151 -3.8916972 -3.9811106 -4.0716677 -4.1312914 -4.1632767 -4.1748919 -4.1796241 -4.1858563][-4.2347379 -4.2023478 -4.1716189 -4.1502314 -4.13036 -4.0916638 -4.0360065 -4.0229392 -4.0716529 -4.1334009 -4.1823678 -4.2094226 -4.2169318 -4.2191477 -4.2255583][-4.2588787 -4.2371249 -4.220777 -4.2135158 -4.207459 -4.1894813 -4.1584668 -4.1460776 -4.1719475 -4.2086921 -4.24099 -4.2588696 -4.2640147 -4.2635193 -4.2684183][-4.2839265 -4.270299 -4.2639117 -4.2659655 -4.2669516 -4.2611732 -4.2477846 -4.2424078 -4.2562594 -4.2732849 -4.2893505 -4.29982 -4.3030844 -4.2994833 -4.301167][-4.3063188 -4.2995324 -4.2991443 -4.3074088 -4.3150845 -4.31564 -4.3083529 -4.2997127 -4.2984605 -4.2995458 -4.3049655 -4.3110476 -4.3150926 -4.3147192 -4.3164482][-4.3188963 -4.3153467 -4.315876 -4.3236361 -4.3329782 -4.3363662 -4.3306689 -4.3198028 -4.3099589 -4.3039517 -4.3051643 -4.3094511 -4.31341 -4.3154526 -4.3184261][-4.3259854 -4.3232145 -4.321557 -4.3242612 -4.3290877 -4.3312397 -4.3265963 -4.3169794 -4.3077164 -4.3032341 -4.3047738 -4.3078823 -4.3104668 -4.3129158 -4.3164787]]...]
INFO - root - 2017-12-05 19:57:05.090744: step 39410, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 77h:00m:53s remains)
INFO - root - 2017-12-05 19:57:14.509847: step 39420, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 77h:15m:14s remains)
INFO - root - 2017-12-05 19:57:23.520977: step 39430, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 73h:45m:26s remains)
INFO - root - 2017-12-05 19:57:33.065358: step 39440, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 78h:47m:55s remains)
INFO - root - 2017-12-05 19:57:42.392870: step 39450, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 71h:14m:22s remains)
INFO - root - 2017-12-05 19:57:51.802208: step 39460, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.935 sec/batch; 76h:04m:36s remains)
INFO - root - 2017-12-05 19:58:01.085232: step 39470, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 78h:26m:08s remains)
INFO - root - 2017-12-05 19:58:10.256030: step 39480, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 76h:12m:28s remains)
INFO - root - 2017-12-05 19:58:19.857345: step 39490, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.006 sec/batch; 81h:54m:37s remains)
INFO - root - 2017-12-05 19:58:29.236726: step 39500, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 81h:18m:05s remains)
2017-12-05 19:58:30.030868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0921664 -4.1306043 -4.1895185 -4.2382617 -4.2725148 -4.3012486 -4.3247533 -4.3351912 -4.3310966 -4.3080926 -4.2720604 -4.2520118 -4.2523556 -4.2578783 -4.27255][-4.004302 -4.0627294 -4.150353 -4.2158346 -4.2522049 -4.2773371 -4.2971177 -4.3083649 -4.3114972 -4.2980323 -4.2713623 -4.255702 -4.256846 -4.2627745 -4.2748947][-3.9600165 -4.0244594 -4.1274881 -4.2034082 -4.23955 -4.2584434 -4.2714348 -4.2803092 -4.2871509 -4.2842836 -4.271297 -4.2639461 -4.2656908 -4.2689342 -4.27665][-3.9718707 -4.0311003 -4.1316667 -4.2012682 -4.2299891 -4.2419634 -4.251286 -4.2600594 -4.2701392 -4.2772417 -4.2763004 -4.2761674 -4.278513 -4.2785449 -4.2800989][-4.022902 -4.0684 -4.1451068 -4.1929502 -4.2066655 -4.2089324 -4.2138524 -4.2277288 -4.2475281 -4.2701674 -4.2829342 -4.2908378 -4.2959433 -4.2931247 -4.2874708][-4.091207 -4.1212068 -4.1628742 -4.1780052 -4.1666145 -4.1487522 -4.1419044 -4.1601863 -4.1967092 -4.2399883 -4.2731237 -4.2957535 -4.3077354 -4.3055687 -4.2948279][-4.1665916 -4.1847496 -4.1952105 -4.1767139 -4.1339059 -4.0896707 -4.0620928 -4.0762954 -4.1270342 -4.1918068 -4.2437115 -4.2812395 -4.3021331 -4.3051176 -4.2940631][-4.2461853 -4.2503862 -4.2325273 -4.18281 -4.1142421 -4.0501566 -4.006948 -4.0165157 -4.0763121 -4.1570048 -4.2191272 -4.2609696 -4.2875643 -4.2952409 -4.2851825][-4.3012977 -4.2926474 -4.25291 -4.1804423 -4.1002951 -4.03529 -3.9918189 -3.9974174 -4.0543613 -4.1363025 -4.1966896 -4.2367392 -4.2649345 -4.2744875 -4.2643313][-4.3162985 -4.29628 -4.2423553 -4.1604152 -4.0836163 -4.0302725 -3.999567 -4.0050516 -4.0514197 -4.1188035 -4.1673894 -4.2005448 -4.2271671 -4.2379117 -4.2295895][-4.3107328 -4.2845736 -4.2259469 -4.1473389 -4.0795879 -4.0401392 -4.0219541 -4.0290275 -4.0626922 -4.1093369 -4.1439514 -4.1704807 -4.1917543 -4.1984849 -4.1929069][-4.316577 -4.294703 -4.2451591 -4.1799679 -4.1248865 -4.0945177 -4.080904 -4.0851412 -4.1058712 -4.1315784 -4.1481113 -4.1621237 -4.1735206 -4.1707187 -4.1633749][-4.3328981 -4.3179374 -4.2823668 -4.2366753 -4.2008491 -4.1813326 -4.1711426 -4.1713419 -4.1764088 -4.178565 -4.172164 -4.1697454 -4.17163 -4.1640625 -4.1573534][-4.3412094 -4.3321013 -4.3097286 -4.2844939 -4.2689514 -4.2618093 -4.2564535 -4.2563519 -4.252171 -4.2387209 -4.2153254 -4.1995516 -4.1938977 -4.1870151 -4.185215][-4.3399668 -4.3350439 -4.3224292 -4.3108087 -4.3068752 -4.3094397 -4.3113022 -4.3118167 -4.3048592 -4.2877316 -4.2640591 -4.2459946 -4.2373371 -4.2331553 -4.2359376]]...]
INFO - root - 2017-12-05 19:58:39.377322: step 39510, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.013 sec/batch; 82h:27m:15s remains)
INFO - root - 2017-12-05 19:58:48.641631: step 39520, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 67h:49m:37s remains)
INFO - root - 2017-12-05 19:58:57.949886: step 39530, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 71h:42m:09s remains)
INFO - root - 2017-12-05 19:59:07.152021: step 39540, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 75h:05m:34s remains)
INFO - root - 2017-12-05 19:59:16.583430: step 39550, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 79h:14m:48s remains)
INFO - root - 2017-12-05 19:59:25.888521: step 39560, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 75h:01m:25s remains)
INFO - root - 2017-12-05 19:59:34.819604: step 39570, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 73h:08m:19s remains)
INFO - root - 2017-12-05 19:59:44.151156: step 39580, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 75h:35m:02s remains)
INFO - root - 2017-12-05 19:59:53.498386: step 39590, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 75h:35m:42s remains)
INFO - root - 2017-12-05 20:00:02.896326: step 39600, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.928 sec/batch; 75h:27m:52s remains)
2017-12-05 20:00:03.649807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3040204 -4.3008347 -4.2974787 -4.2927036 -4.2908688 -4.2905693 -4.295011 -4.3051424 -4.3202319 -4.3287539 -4.3319349 -4.3338332 -4.3344407 -4.3333869 -4.3306127][-4.3241348 -4.31287 -4.3000493 -4.287746 -4.2805219 -4.279213 -4.2862105 -4.300005 -4.3171444 -4.327311 -4.3319192 -4.3345351 -4.3355412 -4.3347349 -4.3322825][-4.31693 -4.2907515 -4.2648387 -4.24931 -4.2470984 -4.2542534 -4.2703161 -4.2906308 -4.310061 -4.3218122 -4.328269 -4.3318682 -4.333252 -4.3326173 -4.33028][-4.2618985 -4.2186575 -4.1878963 -4.1828527 -4.1958818 -4.2180386 -4.2459688 -4.2725673 -4.2955184 -4.3113718 -4.3210578 -4.3267045 -4.3293552 -4.3285851 -4.3251104][-4.1759453 -4.126771 -4.105782 -4.1164627 -4.1442447 -4.1785192 -4.2144012 -4.2450657 -4.2727203 -4.2944317 -4.307426 -4.3140454 -4.3177128 -4.3173771 -4.3128538][-4.1022182 -4.0694647 -4.0674272 -4.0898752 -4.123004 -4.1582074 -4.1923661 -4.2201533 -4.2475247 -4.2716789 -4.2858586 -4.2928166 -4.296711 -4.2955551 -4.2880344][-4.0655475 -4.0551629 -4.0724573 -4.1029329 -4.1364818 -4.1660109 -4.1916022 -4.2130613 -4.2362838 -4.2563367 -4.2659664 -4.2680454 -4.2669678 -4.2589345 -4.2424917][-4.0681486 -4.0726476 -4.1000438 -4.135375 -4.16838 -4.1905007 -4.2056632 -4.2209496 -4.2382283 -4.25048 -4.2508459 -4.2439866 -4.232285 -4.2098608 -4.1769471][-4.0976915 -4.1093349 -4.1367354 -4.1700988 -4.2003684 -4.2183576 -4.2287078 -4.2415252 -4.2542682 -4.2582641 -4.2484522 -4.2286654 -4.1989117 -4.1541672 -4.1027484][-4.1342325 -4.1480336 -4.1733365 -4.2040868 -4.2332335 -4.2532949 -4.2668171 -4.27903 -4.2852883 -4.278863 -4.2561984 -4.2188568 -4.1655574 -4.1010523 -4.0493164][-4.1746707 -4.1866736 -4.2103462 -4.2393842 -4.2687531 -4.288764 -4.3018894 -4.3096967 -4.3063865 -4.289093 -4.2554445 -4.2057633 -4.1418204 -4.0758162 -4.0385828][-4.2100439 -4.219337 -4.2402883 -4.2667871 -4.2928138 -4.3107843 -4.323194 -4.3272443 -4.3167024 -4.2915287 -4.2513909 -4.1983008 -4.1369 -4.0804048 -4.0561657][-4.2367644 -4.2439728 -4.2635484 -4.2871346 -4.3083162 -4.3231268 -4.3334532 -4.3339486 -4.3200254 -4.2923279 -4.2515368 -4.201612 -4.1498704 -4.1070061 -4.0940542][-4.2647543 -4.2684188 -4.2831397 -4.3004065 -4.31354 -4.3223782 -4.3283248 -4.3258781 -4.3111005 -4.2851543 -4.2503304 -4.2116718 -4.1767941 -4.1536193 -4.1537652][-4.2925858 -4.2931271 -4.3015113 -4.3115377 -4.3179412 -4.3216829 -4.3234754 -4.3193641 -4.306416 -4.286139 -4.2609696 -4.2363949 -4.2189074 -4.2134395 -4.2218146]]...]
INFO - root - 2017-12-05 20:00:12.869661: step 39610, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 76h:06m:34s remains)
INFO - root - 2017-12-05 20:00:22.049526: step 39620, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 68h:49m:32s remains)
INFO - root - 2017-12-05 20:00:31.191067: step 39630, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 78h:06m:03s remains)
INFO - root - 2017-12-05 20:00:40.701905: step 39640, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.031 sec/batch; 83h:52m:36s remains)
INFO - root - 2017-12-05 20:00:49.975365: step 39650, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 78h:39m:33s remains)
INFO - root - 2017-12-05 20:00:59.189676: step 39660, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.940 sec/batch; 76h:29m:59s remains)
INFO - root - 2017-12-05 20:01:08.508357: step 39670, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 78h:31m:32s remains)
INFO - root - 2017-12-05 20:01:17.859716: step 39680, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 75h:15m:31s remains)
INFO - root - 2017-12-05 20:01:27.061984: step 39690, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 78h:35m:07s remains)
INFO - root - 2017-12-05 20:01:36.485692: step 39700, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 80h:00m:19s remains)
2017-12-05 20:01:37.240294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3050966 -4.2967529 -4.2961817 -4.2970538 -4.2951179 -4.2866306 -4.2813268 -4.2845364 -4.2901878 -4.2973328 -4.2947531 -4.2861295 -4.2897353 -4.2984405 -4.30493][-4.2642379 -4.254324 -4.2608747 -4.2696257 -4.2719808 -4.2630005 -4.2575955 -4.2620034 -4.2673135 -4.2736192 -4.26772 -4.2523832 -4.2513275 -4.2574558 -4.261425][-4.2129498 -4.2063255 -4.2237277 -4.2435269 -4.2529793 -4.2448521 -4.2399316 -4.2441044 -4.2465272 -4.2503819 -4.239604 -4.214427 -4.2057376 -4.207788 -4.2104936][-4.1668596 -4.166153 -4.1931291 -4.2200346 -4.2298684 -4.2163172 -4.2068219 -4.2090511 -4.2091513 -4.2094049 -4.1953554 -4.1661992 -4.1559916 -4.1590643 -4.1649575][-4.1427107 -4.1459432 -4.174902 -4.1964397 -4.1960783 -4.1683693 -4.1471848 -4.1479573 -4.1519203 -4.1548471 -4.142344 -4.1183491 -4.1165071 -4.1282549 -4.141469][-4.1305928 -4.1315331 -4.1507092 -4.1546874 -4.1370649 -4.091085 -4.05535 -4.0613451 -4.0785575 -4.0944686 -4.0922422 -4.0798244 -4.0935183 -4.1194973 -4.1424165][-4.1341391 -4.1276736 -4.1297283 -4.1131072 -4.0742841 -4.0037236 -3.9524391 -3.9648156 -3.9987376 -4.0343523 -4.0509915 -4.0568123 -4.0902786 -4.1314716 -4.1606784][-4.1692214 -4.15002 -4.133337 -4.1029372 -4.0518589 -3.9653015 -3.9039395 -3.9286385 -3.980011 -4.0301433 -4.058516 -4.0762072 -4.1180263 -4.161489 -4.1876578][-4.1963377 -4.1725216 -4.149209 -4.1204605 -4.0790863 -4.0056067 -3.9533491 -3.9784911 -4.0282488 -4.0752368 -4.0997849 -4.1140227 -4.1458831 -4.1775322 -4.196208][-4.1931067 -4.1661377 -4.1450405 -4.1298618 -4.1145678 -4.077333 -4.0448852 -4.05362 -4.0815959 -4.1138821 -4.1282945 -4.1338439 -4.1503105 -4.1669369 -4.1803136][-4.1865716 -4.1539087 -4.1343331 -4.1315532 -4.1420937 -4.1358833 -4.1181607 -4.1114349 -4.120122 -4.1392903 -4.1498613 -4.1513686 -4.1526794 -4.156074 -4.165822][-4.1887026 -4.1521912 -4.129951 -4.1303573 -4.1525235 -4.1605134 -4.149313 -4.1339359 -4.1314487 -4.146481 -4.1638894 -4.1695957 -4.16486 -4.1597514 -4.1661005][-4.1775641 -4.1375041 -4.1089416 -4.1055145 -4.1282239 -4.139812 -4.1332712 -4.117857 -4.1105714 -4.1238637 -4.1493063 -4.1632881 -4.1617575 -4.157866 -4.1669741][-4.1775541 -4.13707 -4.1080437 -4.1021309 -4.1190524 -4.1286922 -4.1232667 -4.1089978 -4.102437 -4.1151872 -4.1402378 -4.1560435 -4.1595216 -4.1606216 -4.1748466][-4.2063241 -4.1772385 -4.1575112 -4.154933 -4.1641569 -4.1681924 -4.1629071 -4.1521368 -4.1471143 -4.1575351 -4.1790724 -4.1952615 -4.203793 -4.2085676 -4.2229719]]...]
INFO - root - 2017-12-05 20:01:46.733229: step 39710, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 77h:07m:58s remains)
INFO - root - 2017-12-05 20:01:56.048625: step 39720, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 78h:15m:02s remains)
INFO - root - 2017-12-05 20:02:05.316259: step 39730, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 80h:28m:35s remains)
INFO - root - 2017-12-05 20:02:14.967633: step 39740, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 78h:47m:00s remains)
INFO - root - 2017-12-05 20:02:24.113924: step 39750, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 78h:20m:37s remains)
INFO - root - 2017-12-05 20:02:33.465492: step 39760, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.984 sec/batch; 79h:58m:41s remains)
INFO - root - 2017-12-05 20:02:42.867515: step 39770, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 74h:35m:38s remains)
INFO - root - 2017-12-05 20:02:52.202731: step 39780, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 77h:33m:54s remains)
INFO - root - 2017-12-05 20:03:01.471106: step 39790, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 72h:17m:53s remains)
INFO - root - 2017-12-05 20:03:10.681003: step 39800, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 71h:07m:52s remains)
2017-12-05 20:03:11.438332: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2567768 -4.2519984 -4.2584629 -4.2847695 -4.3219404 -4.3518581 -4.3528504 -4.3226466 -4.2686148 -4.2151794 -4.1854324 -4.180377 -4.1867046 -4.2014065 -4.2184558][-4.2660646 -4.2690916 -4.2821832 -4.3125639 -4.3459744 -4.3651252 -4.3487549 -4.3019705 -4.2386222 -4.1891747 -4.1723671 -4.1791353 -4.194212 -4.2108212 -4.226841][-4.2826328 -4.2980223 -4.3204656 -4.3488913 -4.3685102 -4.36632 -4.3288293 -4.2642779 -4.19147 -4.1534019 -4.156229 -4.1776724 -4.2013211 -4.2208147 -4.2367949][-4.3147478 -4.3426557 -4.3690662 -4.3851748 -4.3816242 -4.35481 -4.2908115 -4.2018828 -4.1160207 -4.0955925 -4.1264515 -4.1716871 -4.2130527 -4.2403994 -4.2558584][-4.3444934 -4.3772268 -4.396894 -4.3903766 -4.360775 -4.3093991 -4.2221584 -4.1059175 -4.0049238 -4.0086632 -4.081542 -4.1627455 -4.2263985 -4.2589288 -4.2698393][-4.3574405 -4.38547 -4.3910413 -4.3631606 -4.3164039 -4.2481461 -4.1379609 -3.9921656 -3.8741155 -3.9107928 -4.0331416 -4.1553788 -4.2368026 -4.2659349 -4.2711396][-4.3538566 -4.3736815 -4.3630943 -4.3188457 -4.2624354 -4.1867871 -4.0629606 -3.8894832 -3.770823 -3.8531818 -4.0164814 -4.1595654 -4.2442689 -4.268681 -4.268568][-4.340395 -4.3465681 -4.322474 -4.2755961 -4.2203088 -4.15034 -4.0250936 -3.8452563 -3.755904 -3.8775215 -4.0508342 -4.1839008 -4.2563787 -4.2749634 -4.2736788][-4.3230081 -4.3211436 -4.2942886 -4.2531009 -4.2075405 -4.1502471 -4.0459657 -3.898715 -3.8528585 -3.9716833 -4.1150732 -4.2184138 -4.2725825 -4.284966 -4.2821455][-4.3130889 -4.3072476 -4.2815351 -4.2483077 -4.212173 -4.1716647 -4.0991864 -4.0010872 -3.9871008 -4.07487 -4.1755466 -4.2508931 -4.287703 -4.2902832 -4.2853136][-4.3055472 -4.3006864 -4.2779231 -4.24964 -4.2256188 -4.2008696 -4.1564512 -4.0949445 -4.0908 -4.1508694 -4.2213516 -4.27667 -4.2968159 -4.2914243 -4.2844896][-4.294425 -4.2911863 -4.2741852 -4.2558527 -4.245285 -4.2347493 -4.2083597 -4.1673374 -4.1609039 -4.197165 -4.2457376 -4.2841225 -4.2904782 -4.28179 -4.2742805][-4.2837625 -4.2814407 -4.2734132 -4.2653694 -4.2644997 -4.264389 -4.2517676 -4.22684 -4.2207413 -4.2407603 -4.2687068 -4.2891865 -4.2858872 -4.2719817 -4.26044][-4.276865 -4.2722087 -4.265 -4.2618089 -4.2682786 -4.2796655 -4.2814751 -4.2721114 -4.2686706 -4.2776065 -4.2894053 -4.2969036 -4.2913141 -4.276278 -4.2614675][-4.27996 -4.2695723 -4.254396 -4.2476635 -4.25989 -4.2824283 -4.2948732 -4.2953658 -4.2953973 -4.2995582 -4.3029966 -4.304688 -4.3017044 -4.29053 -4.2758946]]...]
INFO - root - 2017-12-05 20:03:20.758567: step 39810, loss = 2.03, batch loss = 1.98 (8.1 examples/sec; 0.986 sec/batch; 80h:11m:12s remains)
INFO - root - 2017-12-05 20:03:29.882641: step 39820, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 77h:36m:37s remains)
INFO - root - 2017-12-05 20:03:39.239847: step 39830, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 73h:32m:43s remains)
INFO - root - 2017-12-05 20:03:48.438499: step 39840, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 74h:27m:34s remains)
INFO - root - 2017-12-05 20:03:57.414260: step 39850, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 79h:30m:52s remains)
INFO - root - 2017-12-05 20:04:06.771412: step 39860, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 76h:46m:51s remains)
INFO - root - 2017-12-05 20:04:16.240956: step 39870, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 81h:50m:24s remains)
INFO - root - 2017-12-05 20:04:25.493396: step 39880, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 71h:19m:08s remains)
INFO - root - 2017-12-05 20:04:34.832910: step 39890, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 81h:01m:05s remains)
INFO - root - 2017-12-05 20:04:44.333401: step 39900, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 75h:38m:31s remains)
2017-12-05 20:04:45.070138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0569963 -4.0318336 -4.0517268 -4.1078224 -4.1668782 -4.2052 -4.2135558 -4.1950502 -4.1604972 -4.121037 -4.0944576 -4.0866547 -4.0821562 -4.1015506 -4.1454272][-4.0565634 -4.03341 -4.0553679 -4.108994 -4.1662693 -4.2006021 -4.2093716 -4.1945372 -4.1600003 -4.1183233 -4.0872288 -4.0748429 -4.0706668 -4.0948963 -4.14098][-4.0520396 -4.026866 -4.0432696 -4.0929966 -4.1475635 -4.1840234 -4.2013149 -4.1969466 -4.1693478 -4.1314425 -4.1012363 -4.0923023 -4.1032214 -4.1408434 -4.1830282][-4.0462027 -4.0193057 -4.0316491 -4.0760455 -4.1286597 -4.1699185 -4.1957593 -4.1970272 -4.1711421 -4.13524 -4.1115842 -4.1135631 -4.1447325 -4.1936502 -4.2253394][-4.0696573 -4.0468373 -4.0499263 -4.0827384 -4.1329446 -4.173974 -4.1942172 -4.1886091 -4.1604543 -4.1262612 -4.1097836 -4.1285419 -4.174417 -4.2233105 -4.2374144][-4.1059504 -4.0810809 -4.072783 -4.1004219 -4.1526151 -4.1884952 -4.1941543 -4.1721225 -4.1317992 -4.0981221 -4.0984025 -4.1316805 -4.1787457 -4.2145772 -4.21353][-4.1125546 -4.0818367 -4.0723538 -4.1087618 -4.1651182 -4.1971016 -4.1962681 -4.1614437 -4.1085982 -4.0740266 -4.0830612 -4.1187463 -4.1564302 -4.1775341 -4.1710191][-4.083878 -4.0505748 -4.0466514 -4.0942636 -4.1562591 -4.1915789 -4.1949577 -4.1547513 -4.0983372 -4.067997 -4.0849724 -4.118453 -4.1388717 -4.1391754 -4.1303415][-4.0328789 -3.9970033 -4.005899 -4.0663624 -4.1353478 -4.1727171 -4.1730809 -4.1273885 -4.0732484 -4.0581965 -4.0922804 -4.1264906 -4.1306868 -4.1161995 -4.1119757][-4.018837 -3.9804571 -3.9943323 -4.0532732 -4.1177845 -4.1484203 -4.1352615 -4.0763912 -4.025125 -4.0338635 -4.0911613 -4.1282072 -4.1241832 -4.1037211 -4.104423][-4.0703425 -4.0341024 -4.0392656 -4.077868 -4.1175127 -4.1330366 -4.1080546 -4.0399227 -3.9931693 -4.0177469 -4.0816274 -4.1178069 -4.1127677 -4.0976872 -4.108089][-4.1406269 -4.1090751 -4.1009493 -4.1153975 -4.1333523 -4.140758 -4.1186581 -4.0612273 -4.0270619 -4.05106 -4.1011791 -4.1312957 -4.1320534 -4.1265335 -4.1419516][-4.1806178 -4.1607504 -4.1511903 -4.1549506 -4.1657605 -4.1744504 -4.1608996 -4.1217275 -4.0991883 -4.1138964 -4.1446767 -4.1664553 -4.1727962 -4.17189 -4.1806107][-4.2002435 -4.1909957 -4.1883316 -4.1932087 -4.2054191 -4.2161727 -4.21057 -4.1887016 -4.1737404 -4.1775427 -4.1942368 -4.2073832 -4.2120385 -4.2081556 -4.2068305][-4.2258921 -4.2209353 -4.2211418 -4.2275152 -4.2386336 -4.2486663 -4.2468228 -4.2367234 -4.230001 -4.2300391 -4.2383561 -4.2456155 -4.245163 -4.2372928 -4.2300463]]...]
INFO - root - 2017-12-05 20:04:54.254879: step 39910, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 68h:30m:23s remains)
INFO - root - 2017-12-05 20:05:03.605286: step 39920, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 80h:38m:39s remains)
INFO - root - 2017-12-05 20:05:13.001151: step 39930, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 73h:12m:18s remains)
INFO - root - 2017-12-05 20:05:22.180775: step 39940, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 69h:42m:42s remains)
INFO - root - 2017-12-05 20:05:31.338148: step 39950, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 74h:38m:26s remains)
INFO - root - 2017-12-05 20:05:40.790739: step 39960, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 75h:09m:01s remains)
INFO - root - 2017-12-05 20:05:50.044467: step 39970, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 75h:16m:05s remains)
INFO - root - 2017-12-05 20:05:59.375006: step 39980, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 80h:09m:17s remains)
INFO - root - 2017-12-05 20:06:08.713909: step 39990, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 80h:19m:34s remains)
INFO - root - 2017-12-05 20:06:18.184981: step 40000, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 76h:37m:25s remains)
2017-12-05 20:06:18.912452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2455215 -4.2269778 -4.1694326 -4.0936666 -4.07828 -4.1287642 -4.1793041 -4.1967397 -4.1989274 -4.2030067 -4.2119627 -4.2167764 -4.2145886 -4.2042489 -4.1924853][-4.2289748 -4.2216597 -4.17767 -4.0939183 -4.0462837 -4.0885482 -4.155941 -4.1901445 -4.1999092 -4.2014542 -4.2048736 -4.2083693 -4.2084289 -4.2032294 -4.1964169][-4.2085352 -4.2123828 -4.1923 -4.1252728 -4.0551906 -4.0677538 -4.1372886 -4.1861353 -4.2016668 -4.2009835 -4.1967697 -4.1984296 -4.2013712 -4.19866 -4.196382][-4.1970768 -4.2060161 -4.2065625 -4.1652374 -4.0901637 -4.0582833 -4.1105762 -4.1737847 -4.2017555 -4.2015629 -4.1894574 -4.1873693 -4.1906819 -4.1885791 -4.1876121][-4.2013154 -4.2109237 -4.221179 -4.2004647 -4.1281319 -4.0519247 -4.0649328 -4.13722 -4.1894336 -4.2014394 -4.18815 -4.1825032 -4.184761 -4.1810994 -4.1792245][-4.2166839 -4.2234597 -4.2324119 -4.2213688 -4.1565104 -4.0513415 -4.00387 -4.0704188 -4.1560802 -4.1991553 -4.2013855 -4.1967297 -4.1947932 -4.1867766 -4.1816354][-4.2282405 -4.2315164 -4.2355852 -4.2248507 -4.1690664 -4.0533819 -3.9495962 -3.9846168 -4.100337 -4.1879544 -4.2197385 -4.2217431 -4.2139306 -4.2022066 -4.1928105][-4.2299995 -4.2311568 -4.2315879 -4.2212667 -4.1790581 -4.0801263 -3.958142 -3.9364183 -4.0486307 -4.1645155 -4.2251706 -4.2388124 -4.2310276 -4.2185421 -4.2060518][-4.2272587 -4.225801 -4.2217774 -4.2151856 -4.1949062 -4.136272 -4.0389047 -3.9753315 -4.0339804 -4.1422558 -4.2185264 -4.2463942 -4.2438259 -4.2321343 -4.215189][-4.2321 -4.22774 -4.2143817 -4.2048082 -4.2007222 -4.1831141 -4.1281166 -4.05788 -4.0519233 -4.1252637 -4.2019739 -4.244071 -4.2512527 -4.2429094 -4.2224097][-4.243361 -4.2369289 -4.2133451 -4.1909184 -4.1903973 -4.19994 -4.1859946 -4.1341767 -4.0915818 -4.1150208 -4.1773214 -4.228652 -4.2473626 -4.2455134 -4.2262225][-4.2536511 -4.2491837 -4.2219248 -4.1867743 -4.1804066 -4.2017074 -4.2157264 -4.1898112 -4.1399188 -4.1173635 -4.1516285 -4.2003946 -4.230351 -4.2373939 -4.2241859][-4.2525458 -4.2539639 -4.2335176 -4.2007446 -4.1890383 -4.2087317 -4.2306404 -4.2232265 -4.1813574 -4.1357956 -4.1363244 -4.1724439 -4.2074542 -4.2223825 -4.2154293][-4.2405725 -4.2493711 -4.2411671 -4.2198877 -4.207375 -4.219121 -4.2356076 -4.2337103 -4.206614 -4.1591682 -4.1354079 -4.15561 -4.1907215 -4.2093339 -4.2054605][-4.2285542 -4.24131 -4.2424645 -4.233243 -4.2230859 -4.2240434 -4.2269611 -4.2218752 -4.20702 -4.1727023 -4.1463552 -4.1541038 -4.184473 -4.2020679 -4.2004094]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 20:06:29.436946: step 40010, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 76h:12m:41s remains)
INFO - root - 2017-12-05 20:06:38.709506: step 40020, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 66h:38m:19s remains)
INFO - root - 2017-12-05 20:06:48.124183: step 40030, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 80h:19m:10s remains)
INFO - root - 2017-12-05 20:06:57.495115: step 40040, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 74h:56m:28s remains)
INFO - root - 2017-12-05 20:07:06.870662: step 40050, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.024 sec/batch; 83h:11m:39s remains)
INFO - root - 2017-12-05 20:07:16.266902: step 40060, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 79h:36m:58s remains)
INFO - root - 2017-12-05 20:07:25.614271: step 40070, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 74h:30m:18s remains)
INFO - root - 2017-12-05 20:07:34.947504: step 40080, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 76h:44m:40s remains)
INFO - root - 2017-12-05 20:07:44.290260: step 40090, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 76h:03m:27s remains)
INFO - root - 2017-12-05 20:07:53.598761: step 40100, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 77h:20m:05s remains)
2017-12-05 20:07:54.416609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2533565 -4.2375126 -4.2274671 -4.231894 -4.2302003 -4.2122498 -4.1954222 -4.1913161 -4.1939292 -4.1967869 -4.1859727 -4.1787314 -4.1869226 -4.2021327 -4.2178884][-4.2671366 -4.25577 -4.25087 -4.259851 -4.2568307 -4.2356791 -4.213398 -4.2024226 -4.1981649 -4.19106 -4.1684 -4.1490932 -4.1456938 -4.1524034 -4.1658926][-4.2560711 -4.2565117 -4.2635894 -4.2800694 -4.280592 -4.2618513 -4.2397079 -4.2254219 -4.2148542 -4.2003589 -4.1718774 -4.1466608 -4.1348596 -4.1332288 -4.1414084][-4.230176 -4.2413712 -4.2579556 -4.2760887 -4.2785788 -4.2644267 -4.2471337 -4.2354665 -4.22629 -4.2128167 -4.1873889 -4.1646919 -4.1543431 -4.1511025 -4.1566973][-4.1917939 -4.2072763 -4.2251573 -4.2400804 -4.2426491 -4.2339306 -4.2261066 -4.2252059 -4.226161 -4.2206779 -4.2007051 -4.1834092 -4.1799207 -4.1793237 -4.1819372][-4.1448197 -4.1598678 -4.1767735 -4.18964 -4.193121 -4.1904488 -4.1901236 -4.199616 -4.2109027 -4.2139015 -4.1975169 -4.1809955 -4.1770992 -4.1744695 -4.1737766][-4.1042852 -4.1157045 -4.1321077 -4.14431 -4.1496763 -4.1469097 -4.1443648 -4.1532254 -4.1651845 -4.1724911 -4.15893 -4.1426244 -4.1374569 -4.132822 -4.1320105][-4.080955 -4.0846229 -4.099596 -4.1075969 -4.1052861 -4.0874772 -4.0698776 -4.0696135 -4.0794148 -4.0954089 -4.0950608 -4.088717 -4.0882893 -4.0861664 -4.0888357][-4.0896115 -4.0835462 -4.0926318 -4.0923271 -4.0736361 -4.0342226 -3.997323 -3.984884 -3.991436 -4.0206623 -4.0444407 -4.0572824 -4.0678749 -4.07253 -4.0793824][-4.1359053 -4.125494 -4.1265597 -4.1173315 -4.0877523 -4.0348911 -3.9882042 -3.9686713 -3.9754741 -4.0141163 -4.0581346 -4.0887866 -4.1073689 -4.1154518 -4.1220632][-4.1926317 -4.1796246 -4.173789 -4.1603456 -4.1279025 -4.0770926 -4.0364285 -4.0227928 -4.0373182 -4.0797248 -4.1307693 -4.168385 -4.1880865 -4.1950169 -4.197762][-4.2316556 -4.219564 -4.2109361 -4.1960053 -4.1653047 -4.1258082 -4.0963283 -4.0908203 -4.1112723 -4.152267 -4.2005548 -4.2363935 -4.2535768 -4.2573977 -4.2585287][-4.2379975 -4.2277055 -4.2197151 -4.2079186 -4.1852746 -4.1592174 -4.1404681 -4.1408563 -4.1638556 -4.1999116 -4.238934 -4.2675681 -4.2807164 -4.2833114 -4.2860203][-4.2226248 -4.212461 -4.2067547 -4.1974478 -4.1822972 -4.1685557 -4.1606121 -4.1681476 -4.1926675 -4.2246661 -4.2551074 -4.2751865 -4.2837043 -4.2868977 -4.29255][-4.2144265 -4.2019906 -4.193748 -4.1816511 -4.1725545 -4.171567 -4.1761327 -4.1922188 -4.2183447 -4.2447877 -4.2661862 -4.2773294 -4.280108 -4.2818756 -4.2887077]]...]
INFO - root - 2017-12-05 20:08:03.638640: step 40110, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 74h:11m:39s remains)
INFO - root - 2017-12-05 20:08:12.663723: step 40120, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 70h:28m:03s remains)
INFO - root - 2017-12-05 20:08:22.212152: step 40130, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.996 sec/batch; 80h:51m:02s remains)
INFO - root - 2017-12-05 20:08:31.434108: step 40140, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 76h:35m:39s remains)
INFO - root - 2017-12-05 20:08:40.983650: step 40150, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 78h:06m:51s remains)
INFO - root - 2017-12-05 20:08:50.074631: step 40160, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 77h:32m:35s remains)
INFO - root - 2017-12-05 20:08:59.385667: step 40170, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 75h:44m:22s remains)
INFO - root - 2017-12-05 20:09:08.743169: step 40180, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.938 sec/batch; 76h:09m:43s remains)
INFO - root - 2017-12-05 20:09:18.411825: step 40190, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.982 sec/batch; 79h:44m:43s remains)
INFO - root - 2017-12-05 20:09:27.664434: step 40200, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 76h:23m:44s remains)
2017-12-05 20:09:28.513480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3219223 -4.31122 -4.2989826 -4.2831755 -4.2680759 -4.2658224 -4.2656374 -4.2583933 -4.2549262 -4.2669582 -4.2834282 -4.2935915 -4.3014913 -4.3046865 -4.3054657][-4.3237185 -4.3137608 -4.303194 -4.2872052 -4.269206 -4.2612343 -4.2554569 -4.2474842 -4.2478695 -4.2633872 -4.281744 -4.291007 -4.2951665 -4.2953796 -4.2945232][-4.3249769 -4.3181014 -4.3114233 -4.2960086 -4.2737279 -4.2570763 -4.2443314 -4.2346492 -4.2394066 -4.2600093 -4.28054 -4.2906704 -4.2937369 -4.2892308 -4.2843647][-4.3278069 -4.3260045 -4.3234134 -4.3094015 -4.2829981 -4.2553606 -4.2322559 -4.2174311 -4.2229867 -4.2496052 -4.2756743 -4.2894859 -4.29061 -4.2812057 -4.272017][-4.3313122 -4.333847 -4.3353987 -4.3236408 -4.2907228 -4.2467818 -4.2056751 -4.1797881 -4.1838188 -4.2203627 -4.2590284 -4.2797675 -4.280992 -4.269753 -4.2580671][-4.334866 -4.3397512 -4.3425107 -4.3301067 -4.289299 -4.2298489 -4.1659818 -4.123323 -4.1288357 -4.179553 -4.2329836 -4.2607393 -4.2663012 -4.2548075 -4.2388573][-4.3392735 -4.3444386 -4.3451028 -4.3277845 -4.2807097 -4.210742 -4.1277523 -4.0700283 -4.0795484 -4.1465359 -4.2134228 -4.2458372 -4.2546883 -4.245091 -4.2252669][-4.3429689 -4.3476396 -4.3449769 -4.3225231 -4.2707481 -4.1955462 -4.1028318 -4.0369596 -4.0496545 -4.1270876 -4.2024617 -4.2385025 -4.2510929 -4.2443748 -4.222631][-4.344553 -4.3482809 -4.3440313 -4.3195543 -4.26704 -4.1930137 -4.1017647 -4.0346794 -4.044414 -4.1201839 -4.1979213 -4.2378688 -4.2533283 -4.2481618 -4.2230649][-4.3454485 -4.3483796 -4.3443313 -4.3230362 -4.2753439 -4.2105827 -4.1314387 -4.0682583 -4.0689592 -4.1292191 -4.1958785 -4.2359982 -4.2539034 -4.2480845 -4.2180305][-4.3465357 -4.3493443 -4.3458433 -4.3287034 -4.2874002 -4.2313628 -4.1680145 -4.11684 -4.1113095 -4.15189 -4.2008085 -4.2331285 -4.2477851 -4.2418394 -4.2100439][-4.3473587 -4.3504291 -4.3481674 -4.3360424 -4.3001862 -4.2499933 -4.2018285 -4.1672916 -4.1598144 -4.1802754 -4.2106733 -4.2301788 -4.2374058 -4.2311859 -4.2018342][-4.3475118 -4.3514004 -4.351685 -4.3424144 -4.3090439 -4.2622633 -4.224123 -4.2031465 -4.1962695 -4.2029271 -4.2180681 -4.223351 -4.2182527 -4.2119737 -4.1893964][-4.3472495 -4.3522248 -4.3555727 -4.3485179 -4.3154197 -4.2716312 -4.2379789 -4.2214336 -4.2130485 -4.2117653 -4.2143941 -4.2083712 -4.1928988 -4.1873345 -4.1758208][-4.3466573 -4.3525419 -4.3588409 -4.3539209 -4.3218813 -4.2810335 -4.2487311 -4.2337036 -4.2243929 -4.2180414 -4.2107878 -4.1943331 -4.1696572 -4.1611986 -4.1549797]]...]
INFO - root - 2017-12-05 20:09:37.707701: step 40210, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.969 sec/batch; 78h:40m:12s remains)
INFO - root - 2017-12-05 20:09:46.814644: step 40220, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 76h:11m:57s remains)
INFO - root - 2017-12-05 20:09:56.211961: step 40230, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 78h:58m:35s remains)
INFO - root - 2017-12-05 20:10:05.636090: step 40240, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 77h:36m:15s remains)
INFO - root - 2017-12-05 20:10:14.961028: step 40250, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 75h:59m:49s remains)
INFO - root - 2017-12-05 20:10:24.319711: step 40260, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 75h:57m:35s remains)
INFO - root - 2017-12-05 20:10:33.654067: step 40270, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 77h:31m:55s remains)
INFO - root - 2017-12-05 20:10:42.949820: step 40280, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 75h:02m:05s remains)
INFO - root - 2017-12-05 20:10:52.207363: step 40290, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.916 sec/batch; 74h:22m:07s remains)
INFO - root - 2017-12-05 20:11:01.359920: step 40300, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 74h:16m:51s remains)
2017-12-05 20:11:02.090742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.287796 -4.2880273 -4.2862358 -4.2795486 -4.2707019 -4.2640648 -4.2528934 -4.2315927 -4.2074056 -4.2017875 -4.2181082 -4.230617 -4.2325182 -4.2353721 -4.2493744][-4.2813883 -4.2874002 -4.2862492 -4.273778 -4.2613335 -4.2548389 -4.2365794 -4.2007046 -4.1676645 -4.1669326 -4.1959486 -4.2124696 -4.2110353 -4.2099304 -4.2278395][-4.2815261 -4.294776 -4.2940392 -4.2782254 -4.2646637 -4.253293 -4.2241125 -4.1751719 -4.1363735 -4.1427774 -4.1852212 -4.2064953 -4.2016478 -4.1940389 -4.2103176][-4.2870526 -4.3029652 -4.3012161 -4.2869534 -4.2745242 -4.25698 -4.2139292 -4.1539836 -4.1128798 -4.1276722 -4.1792192 -4.2044187 -4.1977096 -4.1849203 -4.1979132][-4.2904673 -4.3058143 -4.3015909 -4.2883925 -4.2749195 -4.2410331 -4.17643 -4.1016245 -4.0624776 -4.0910864 -4.1571422 -4.1922565 -4.1888366 -4.1760564 -4.1878328][-4.2874761 -4.2988572 -4.2906365 -4.2709279 -4.2420239 -4.1822391 -4.0882878 -3.9954307 -3.9644651 -4.0206604 -4.1131206 -4.1693854 -4.1780038 -4.1664872 -4.1765175][-4.2780857 -4.2857795 -4.2694211 -4.2364383 -4.1874485 -4.1035337 -3.9775569 -3.8619633 -3.8416915 -3.9381628 -4.0656395 -4.1453929 -4.1674428 -4.1578336 -4.1667609][-4.2744927 -4.2786446 -4.25189 -4.2068896 -4.1462126 -4.0476537 -3.8988192 -3.7758777 -3.7733049 -3.9077249 -4.0637918 -4.1512036 -4.1743417 -4.1646643 -4.1737819][-4.2723227 -4.2722807 -4.238163 -4.192512 -4.1327686 -4.0317845 -3.8748851 -3.7719131 -3.81277 -3.9635532 -4.1071892 -4.1770072 -4.1899571 -4.1790433 -4.1890264][-4.2742496 -4.2719059 -4.2378616 -4.2028389 -4.1570525 -4.0656886 -3.9204788 -3.8559923 -3.9331377 -4.0598354 -4.158679 -4.2014203 -4.204886 -4.1950016 -4.2064333][-4.2812386 -4.278214 -4.2534647 -4.2340407 -4.2051134 -4.1357241 -4.0222793 -3.9921117 -4.06625 -4.1478348 -4.2020106 -4.2224736 -4.2190347 -4.2118373 -4.2230153][-4.2899828 -4.286449 -4.2717605 -4.2632976 -4.24835 -4.2044439 -4.134119 -4.1244678 -4.1757932 -4.2163239 -4.2351956 -4.2363915 -4.228188 -4.22318 -4.2368031][-4.295887 -4.2921948 -4.2830396 -4.2811332 -4.2757215 -4.2514625 -4.2174191 -4.218029 -4.2442603 -4.2552991 -4.2519283 -4.243248 -4.2352166 -4.2367611 -4.2525182][-4.3032184 -4.2988081 -4.2930212 -4.2930236 -4.2917671 -4.2795653 -4.2645307 -4.2662935 -4.2755723 -4.2716165 -4.2599258 -4.2521715 -4.2530174 -4.2618308 -4.2759008][-4.3140068 -4.3106456 -4.3061147 -4.3023052 -4.2999716 -4.29317 -4.2863464 -4.2885962 -4.2913666 -4.2835064 -4.2727213 -4.2715955 -4.2792196 -4.2893748 -4.2988467]]...]
INFO - root - 2017-12-05 20:11:11.649430: step 40310, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 74h:14m:33s remains)
INFO - root - 2017-12-05 20:11:20.954628: step 40320, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 74h:59m:03s remains)
INFO - root - 2017-12-05 20:11:30.283635: step 40330, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 75h:55m:21s remains)
INFO - root - 2017-12-05 20:11:39.475459: step 40340, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 73h:29m:39s remains)
INFO - root - 2017-12-05 20:11:48.934011: step 40350, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 74h:22m:47s remains)
INFO - root - 2017-12-05 20:11:58.337044: step 40360, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 73h:47m:04s remains)
INFO - root - 2017-12-05 20:12:07.478029: step 40370, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 73h:18m:36s remains)
INFO - root - 2017-12-05 20:12:16.929386: step 40380, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 75h:03m:18s remains)
INFO - root - 2017-12-05 20:12:26.202551: step 40390, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 74h:39m:00s remains)
INFO - root - 2017-12-05 20:12:35.181844: step 40400, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 73h:43m:02s remains)
2017-12-05 20:12:35.927632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3226428 -4.3357353 -4.3467374 -4.3494687 -4.348947 -4.3481483 -4.3459678 -4.343925 -4.3435874 -4.3451996 -4.3482914 -4.3506107 -4.3512173 -4.349771 -4.3419085][-4.3058305 -4.3160386 -4.3285136 -4.3317823 -4.3341217 -4.335372 -4.3321977 -4.3281822 -4.3272567 -4.3300214 -4.3352966 -4.3395333 -4.3400803 -4.3355126 -4.3234234][-4.2791142 -4.2779093 -4.2893648 -4.2964411 -4.3068542 -4.3141594 -4.3107772 -4.3057756 -4.3033657 -4.3068013 -4.3159204 -4.3227887 -4.3224492 -4.3144832 -4.2997847][-4.24459 -4.2237558 -4.2284737 -4.2422628 -4.2658963 -4.2833571 -4.285881 -4.2868719 -4.2884612 -4.2937231 -4.3048668 -4.3118935 -4.3110347 -4.3005838 -4.2841015][-4.2194524 -4.1804438 -4.1744046 -4.1879911 -4.2145972 -4.2350469 -4.2421575 -4.252492 -4.2691603 -4.2831454 -4.2974048 -4.3040166 -4.3028851 -4.2949352 -4.2843404][-4.210463 -4.1701345 -4.1560349 -4.1601682 -4.1711226 -4.1752696 -4.1736922 -4.1909852 -4.23039 -4.2653012 -4.2879796 -4.2964993 -4.297576 -4.2954712 -4.2959385][-4.2099013 -4.1814375 -4.1670976 -4.1542048 -4.1291275 -4.0903268 -4.0494213 -4.0577192 -4.1288347 -4.1996393 -4.2412682 -4.2620068 -4.272089 -4.2784796 -4.2873135][-4.2075849 -4.1965246 -4.1872911 -4.1561813 -4.090816 -3.9957519 -3.8877063 -3.8677988 -3.9782286 -4.0948534 -4.1630526 -4.1940041 -4.2080417 -4.2197785 -4.2359357][-4.2154226 -4.2269993 -4.2258358 -4.1930242 -4.1153145 -3.9973328 -3.8561027 -3.8122075 -3.9230895 -4.04171 -4.1050134 -4.1252322 -4.1295357 -4.1368313 -4.1585941][-4.2167687 -4.2422795 -4.252677 -4.2385874 -4.1888146 -4.1032066 -4.0041494 -3.9701574 -4.0238891 -4.0873461 -4.1125021 -4.1017952 -4.0839663 -4.0770254 -4.0996532][-4.1961908 -4.2258458 -4.2551723 -4.2657514 -4.2476087 -4.1970906 -4.1422396 -4.1201353 -4.1293235 -4.1441331 -4.1370444 -4.1099949 -4.0791497 -4.0590205 -4.0699811][-4.1581521 -4.1840267 -4.2263813 -4.2537231 -4.2545896 -4.2284937 -4.2020345 -4.1893272 -4.1775289 -4.1700735 -4.1561556 -4.1338282 -4.1032391 -4.0765805 -4.0680885][-4.1371851 -4.1451254 -4.179718 -4.2099 -4.2232175 -4.2170382 -4.2142358 -4.2133312 -4.1958766 -4.1801443 -4.17426 -4.1634989 -4.1435442 -4.1253681 -4.1092367][-4.1634817 -4.1568937 -4.174665 -4.1938281 -4.2088323 -4.2166276 -4.2313251 -4.23573 -4.2140374 -4.1971498 -4.1952009 -4.1922531 -4.1861553 -4.1835923 -4.1742849][-4.2225294 -4.2173963 -4.2256174 -4.23413 -4.2415648 -4.24931 -4.2623053 -4.2638025 -4.2418857 -4.226841 -4.2277818 -4.2304382 -4.2331362 -4.2406526 -4.2392898]]...]
INFO - root - 2017-12-05 20:12:45.283156: step 40410, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 75h:57m:40s remains)
INFO - root - 2017-12-05 20:12:54.867085: step 40420, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 78h:45m:32s remains)
INFO - root - 2017-12-05 20:13:04.162827: step 40430, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 78h:06m:50s remains)
INFO - root - 2017-12-05 20:13:13.369757: step 40440, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 75h:07m:58s remains)
INFO - root - 2017-12-05 20:13:22.580656: step 40450, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 76h:27m:40s remains)
INFO - root - 2017-12-05 20:13:32.129828: step 40460, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 75h:16m:37s remains)
INFO - root - 2017-12-05 20:13:41.475299: step 40470, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 77h:21m:15s remains)
INFO - root - 2017-12-05 20:13:50.745946: step 40480, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 76h:51m:30s remains)
INFO - root - 2017-12-05 20:14:00.152808: step 40490, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 72h:46m:12s remains)
INFO - root - 2017-12-05 20:14:09.253911: step 40500, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 71h:11m:56s remains)
2017-12-05 20:14:10.082855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0154881 -4.0382695 -4.080399 -4.1409955 -4.2055464 -4.2519169 -4.2783365 -4.2932181 -4.2943788 -4.2867527 -4.271102 -4.2551632 -4.2541671 -4.256362 -4.2569633][-4.1001115 -4.1222811 -4.1577458 -4.207665 -4.2566571 -4.2836757 -4.2931833 -4.2960315 -4.2908759 -4.2823324 -4.2756948 -4.2734237 -4.282311 -4.2885509 -4.2887144][-4.1826992 -4.1993842 -4.2265391 -4.2617745 -4.2905884 -4.2978611 -4.2934179 -4.2852035 -4.2725148 -4.2662916 -4.2700043 -4.2810426 -4.297224 -4.3042336 -4.3014288][-4.2589593 -4.2704158 -4.2862778 -4.3006444 -4.300292 -4.283052 -4.257556 -4.2314935 -4.2132554 -4.2173257 -4.2393174 -4.2649155 -4.2901497 -4.3007984 -4.3001637][-4.3039479 -4.3151803 -4.3183856 -4.30767 -4.2712731 -4.2190557 -4.1657829 -4.1235485 -4.1131983 -4.1372194 -4.1798048 -4.2212768 -4.2567029 -4.2749515 -4.2804933][-4.3016009 -4.3066444 -4.2976623 -4.2637653 -4.1920772 -4.1045456 -4.0239606 -3.9778533 -3.9945703 -4.0508194 -4.113451 -4.1648855 -4.211988 -4.2423115 -4.2553596][-4.2892485 -4.2759643 -4.2421455 -4.178196 -4.0763621 -3.9569726 -3.8581896 -3.8278508 -3.8803411 -3.9573123 -4.0289087 -4.0919051 -4.1530595 -4.195919 -4.2190137][-4.2767048 -4.2373552 -4.1738825 -4.0875115 -3.9736595 -3.8541069 -3.770308 -3.7747726 -3.8451049 -3.918947 -3.9847994 -4.0535831 -4.1211605 -4.1679959 -4.1940846][-4.2613745 -4.2103119 -4.1425538 -4.0644889 -3.9755139 -3.8950114 -3.8595953 -3.8875937 -3.9445369 -3.9945545 -4.0437551 -4.1015286 -4.1557622 -4.1922178 -4.2106662][-4.2594075 -4.2165422 -4.171237 -4.1297412 -4.0874519 -4.0551591 -4.0471444 -4.0652766 -4.0964551 -4.12393 -4.1560907 -4.1921811 -4.2208257 -4.2388096 -4.246068][-4.2647791 -4.238812 -4.2193089 -4.2094593 -4.2018442 -4.1994324 -4.1991243 -4.2061858 -4.2179494 -4.2305446 -4.2495613 -4.2684388 -4.2793689 -4.2840872 -4.283134][-4.2729173 -4.2588153 -4.2542129 -4.2602453 -4.2682776 -4.2747803 -4.2741685 -4.2747974 -4.2793717 -4.2868652 -4.2968321 -4.3057423 -4.3085494 -4.3080134 -4.3058243][-4.2826 -4.2767382 -4.2789125 -4.2886086 -4.2982545 -4.3034062 -4.3008056 -4.2966652 -4.2974725 -4.3024297 -4.3090172 -4.31384 -4.3145013 -4.3118229 -4.3083673][-4.2936807 -4.2909245 -4.2931223 -4.2982802 -4.3037481 -4.3072104 -4.3062687 -4.3034821 -4.3017116 -4.3024344 -4.3050933 -4.307086 -4.3066416 -4.30415 -4.301517][-4.3006296 -4.2984352 -4.2972841 -4.2975378 -4.298882 -4.3002014 -4.3006139 -4.3001266 -4.2989864 -4.2987733 -4.2992225 -4.2989316 -4.2974815 -4.2966719 -4.2961054]]...]
INFO - root - 2017-12-05 20:14:19.468868: step 40510, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 75h:21m:57s remains)
INFO - root - 2017-12-05 20:14:28.736254: step 40520, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 77h:00m:58s remains)
INFO - root - 2017-12-05 20:14:38.145100: step 40530, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 76h:25m:29s remains)
INFO - root - 2017-12-05 20:14:47.487800: step 40540, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 73h:59m:26s remains)
INFO - root - 2017-12-05 20:14:56.876825: step 40550, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.939 sec/batch; 76h:07m:30s remains)
INFO - root - 2017-12-05 20:15:06.139260: step 40560, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 77h:01m:26s remains)
INFO - root - 2017-12-05 20:15:15.551237: step 40570, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 75h:04m:16s remains)
INFO - root - 2017-12-05 20:15:24.759681: step 40580, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.882 sec/batch; 71h:30m:16s remains)
INFO - root - 2017-12-05 20:15:34.194782: step 40590, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 66h:34m:49s remains)
INFO - root - 2017-12-05 20:15:43.584388: step 40600, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 69h:28m:50s remains)
2017-12-05 20:15:44.400560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29691 -4.304471 -4.30184 -4.2799654 -4.2438107 -4.208313 -4.1911621 -4.1989427 -4.2173743 -4.2314 -4.2405562 -4.2394419 -4.2348051 -4.2279472 -4.221941][-4.2853651 -4.2972107 -4.2968926 -4.2791114 -4.2502146 -4.2223892 -4.21028 -4.2145052 -4.2207217 -4.2218685 -4.2208614 -4.211184 -4.2006178 -4.1918025 -4.1860456][-4.2658591 -4.2823896 -4.2863731 -4.2768388 -4.2587352 -4.240468 -4.2317586 -4.2294178 -4.22281 -4.2136245 -4.2052088 -4.1852789 -4.1630011 -4.1471329 -4.1415491][-4.2407932 -4.26235 -4.27172 -4.2709746 -4.2621555 -4.2513723 -4.2412448 -4.232008 -4.2207837 -4.2120557 -4.20237 -4.1765251 -4.1438155 -4.1201711 -4.1124368][-4.2242808 -4.2495465 -4.2650828 -4.2720251 -4.267076 -4.2570248 -4.2369275 -4.2141514 -4.2003016 -4.1959233 -4.1909695 -4.1713271 -4.139699 -4.1139255 -4.1021485][-4.2185097 -4.245275 -4.2638574 -4.2687969 -4.2561445 -4.2330508 -4.1923962 -4.1515255 -4.1371713 -4.1422877 -4.1505451 -4.1456928 -4.1273537 -4.1114941 -4.1049681][-4.2062807 -4.23248 -4.2473664 -4.240942 -4.2093863 -4.1606817 -4.0894775 -4.0306444 -4.0238094 -4.050076 -4.082674 -4.09985 -4.1021676 -4.1036468 -4.1063466][-4.1700368 -4.1940155 -4.2040324 -4.1855054 -4.1362524 -4.0645046 -3.9700162 -3.9047074 -3.9155717 -3.9724352 -4.0289745 -4.0630617 -4.0834594 -4.1033878 -4.1163945][-4.1156125 -4.1332588 -4.1394525 -4.1141419 -4.0582595 -3.9839876 -3.8986394 -3.8544197 -3.8881867 -3.9661326 -4.03177 -4.06918 -4.0969148 -4.1275721 -4.1464148][-4.0725746 -4.083313 -4.0901327 -4.0706539 -4.0319219 -3.9855957 -3.9400887 -3.9278133 -3.969445 -4.039331 -4.0892458 -4.1135612 -4.1365252 -4.1655974 -4.1847377][-4.0719109 -4.0854096 -4.1013389 -4.0992785 -4.08905 -4.0764689 -4.0635252 -4.0635338 -4.0894332 -4.1297307 -4.1547852 -4.1647158 -4.1773229 -4.1927376 -4.206749][-4.1062942 -4.1284423 -4.155035 -4.1696534 -4.1797848 -4.1853895 -4.184217 -4.1817651 -4.1874819 -4.2004323 -4.2050228 -4.2017808 -4.2016335 -4.2047415 -4.2100673][-4.164278 -4.1931505 -4.22274 -4.2424154 -4.2556443 -4.2635641 -4.2635312 -4.2548294 -4.2494159 -4.2466116 -4.2378616 -4.2245593 -4.2165303 -4.2143159 -4.21068][-4.2262249 -4.2547131 -4.2796159 -4.2957416 -4.306294 -4.3119311 -4.3104448 -4.2983112 -4.2882261 -4.2776594 -4.2652712 -4.252512 -4.2479596 -4.2465672 -4.235795][-4.2772174 -4.2996039 -4.3152094 -4.3222508 -4.3262148 -4.3300138 -4.330411 -4.3216114 -4.3134947 -4.3042226 -4.2959595 -4.2884989 -4.2922063 -4.2934184 -4.2782478]]...]
INFO - root - 2017-12-05 20:15:53.625063: step 40610, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 74h:48m:51s remains)
INFO - root - 2017-12-05 20:16:03.057457: step 40620, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 77h:51m:06s remains)
INFO - root - 2017-12-05 20:16:12.454764: step 40630, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.939 sec/batch; 76h:07m:10s remains)
INFO - root - 2017-12-05 20:16:21.842732: step 40640, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 76h:48m:03s remains)
INFO - root - 2017-12-05 20:16:31.293821: step 40650, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 75h:10m:10s remains)
INFO - root - 2017-12-05 20:16:40.515607: step 40660, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 71h:25m:22s remains)
INFO - root - 2017-12-05 20:16:49.896031: step 40670, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 76h:10m:44s remains)
INFO - root - 2017-12-05 20:16:59.029956: step 40680, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 77h:33m:59s remains)
INFO - root - 2017-12-05 20:17:08.279952: step 40690, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 73h:30m:34s remains)
INFO - root - 2017-12-05 20:17:17.513016: step 40700, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 79h:34m:18s remains)
2017-12-05 20:17:18.301069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1370268 -4.1651287 -4.1931162 -4.195919 -4.1724582 -4.144855 -4.1376028 -4.1460013 -4.1616411 -4.1674581 -4.1833086 -4.206636 -4.2284513 -4.24833 -4.2559781][-4.1600232 -4.1796021 -4.1924019 -4.1819 -4.1530156 -4.13214 -4.1362414 -4.1492763 -4.1598082 -4.1630669 -4.1801319 -4.2002063 -4.2124157 -4.225657 -4.2372842][-4.1951046 -4.1963835 -4.18942 -4.1702 -4.1421633 -4.1276193 -4.1393461 -4.1572804 -4.1686392 -4.1703963 -4.18629 -4.205029 -4.2107329 -4.2148919 -4.2255673][-4.218473 -4.204968 -4.1849232 -4.160614 -4.1331878 -4.1213155 -4.1387348 -4.1663871 -4.1900826 -4.2004371 -4.2168722 -4.2365789 -4.2340326 -4.2259264 -4.2292857][-4.2224379 -4.2022734 -4.174479 -4.1437836 -4.1131287 -4.0966821 -4.1152105 -4.1573243 -4.1999917 -4.223587 -4.2440414 -4.2633257 -4.2568846 -4.2423563 -4.2371082][-4.2195063 -4.192852 -4.1549449 -4.10863 -4.0606627 -4.0266728 -4.040442 -4.0962138 -4.1599808 -4.1990089 -4.22816 -4.2496257 -4.2504172 -4.2439618 -4.2373095][-4.2143383 -4.1831164 -4.1404533 -4.0766087 -3.9955707 -3.9262795 -3.9253457 -3.9907951 -4.0763083 -4.1355324 -4.17458 -4.1980896 -4.2090368 -4.2169776 -4.2170305][-4.1921039 -4.1655893 -4.1265931 -4.055583 -3.9455333 -3.8373668 -3.81461 -3.8788311 -3.9816356 -4.0670171 -4.1250024 -4.1547961 -4.1752162 -4.1943645 -4.2023959][-4.1804581 -4.1729569 -4.1445465 -4.0921965 -4.0002947 -3.903533 -3.8785326 -3.9172525 -3.9895778 -4.05688 -4.1120892 -4.1506824 -4.1849942 -4.2124925 -4.21977][-4.1905036 -4.2001472 -4.1849213 -4.1563783 -4.0927916 -4.0223527 -4.0060954 -4.0275493 -4.0654607 -4.098856 -4.1351571 -4.1711392 -4.2092094 -4.2392287 -4.2442718][-4.2078757 -4.2147417 -4.203742 -4.1903367 -4.1467404 -4.092289 -4.0855775 -4.1019864 -4.1223702 -4.135098 -4.1601167 -4.1955943 -4.2325454 -4.2605276 -4.263648][-4.2064543 -4.1999693 -4.186307 -4.1788049 -4.1444545 -4.1026158 -4.1041446 -4.1193352 -4.12402 -4.1183085 -4.1346931 -4.1762786 -4.2170863 -4.2445178 -4.2497439][-4.1799173 -4.1585164 -4.1409326 -4.1367421 -4.1128793 -4.08766 -4.0982709 -4.1106343 -4.1008744 -4.074162 -4.074966 -4.1219478 -4.1691146 -4.2000885 -4.2179208][-4.1541939 -4.1297879 -4.11833 -4.1213346 -4.1107931 -4.1025009 -4.1201978 -4.1309819 -4.1129279 -4.0773144 -4.0749993 -4.1235948 -4.1662779 -4.1894708 -4.2137661][-4.1482925 -4.130127 -4.1253891 -4.1311669 -4.1310277 -4.1361113 -4.1569796 -4.1680546 -4.1541286 -4.1292553 -4.1345286 -4.1767445 -4.2056255 -4.216258 -4.2367086]]...]
INFO - root - 2017-12-05 20:17:27.730873: step 40710, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.960 sec/batch; 77h:49m:06s remains)
INFO - root - 2017-12-05 20:17:37.087834: step 40720, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 78h:02m:22s remains)
INFO - root - 2017-12-05 20:17:46.530042: step 40730, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 76h:08m:23s remains)
INFO - root - 2017-12-05 20:17:55.925712: step 40740, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 79h:34m:34s remains)
INFO - root - 2017-12-05 20:18:05.141888: step 40750, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 76h:11m:27s remains)
INFO - root - 2017-12-05 20:18:14.349527: step 40760, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.957 sec/batch; 77h:34m:16s remains)
INFO - root - 2017-12-05 20:18:23.663184: step 40770, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 75h:50m:54s remains)
INFO - root - 2017-12-05 20:18:32.870569: step 40780, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 76h:34m:42s remains)
INFO - root - 2017-12-05 20:18:42.049357: step 40790, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 76h:18m:38s remains)
INFO - root - 2017-12-05 20:18:51.533829: step 40800, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 75h:41m:43s remains)
2017-12-05 20:18:52.290906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1170592 -4.152019 -4.193357 -4.2337365 -4.2630076 -4.2792797 -4.28501 -4.2816515 -4.2735605 -4.2602129 -4.2439 -4.2326365 -4.2273264 -4.2266145 -4.2315087][-4.1323361 -4.178205 -4.2242904 -4.26256 -4.2867517 -4.2980795 -4.3015542 -4.3006048 -4.2982411 -4.2899933 -4.2782149 -4.2696896 -4.2650509 -4.2640181 -4.2644062][-4.1704021 -4.2206492 -4.2624674 -4.2902126 -4.301106 -4.3020444 -4.3011346 -4.3035622 -4.3062525 -4.303576 -4.2970624 -4.2900071 -4.2856445 -4.2841659 -4.2802505][-4.2087193 -4.2577295 -4.285583 -4.2966847 -4.2899861 -4.2767997 -4.2665415 -4.2669816 -4.2745347 -4.2807593 -4.2826843 -4.2795 -4.2757277 -4.274684 -4.2666192][-4.2285662 -4.2712464 -4.2883792 -4.2855606 -4.2647533 -4.23335 -4.2061577 -4.1954556 -4.2038803 -4.220181 -4.2337294 -4.2367373 -4.234148 -4.2304273 -4.2172613][-4.2169323 -4.2509255 -4.264936 -4.256258 -4.2239985 -4.1693115 -4.1168604 -4.0888548 -4.1012135 -4.1345658 -4.163393 -4.173986 -4.1734042 -4.1714716 -4.1603723][-4.1843476 -4.2095041 -4.2191019 -4.2032175 -4.1565948 -4.0817127 -4.0079107 -3.9691353 -3.9917414 -4.0443654 -4.0879903 -4.1031842 -4.106657 -4.1123428 -4.1109438][-4.16233 -4.1804514 -4.1854715 -4.1612616 -4.0980592 -4.0085378 -3.9262135 -3.8874488 -3.9163027 -3.9757566 -4.0271268 -4.047895 -4.0591006 -4.0777221 -4.0880938][-4.172533 -4.1871762 -4.1888003 -4.1583905 -4.0922718 -4.002327 -3.9277186 -3.8966808 -3.9175029 -3.9630849 -4.0073376 -4.0267334 -4.0399928 -4.0641985 -4.0871754][-4.1945682 -4.2045412 -4.2018762 -4.1722355 -4.1187129 -4.0503178 -3.9976666 -3.97592 -3.9825454 -4.0048447 -4.0310388 -4.0415239 -4.0505733 -4.0783467 -4.1134558][-4.198801 -4.2022061 -4.1969457 -4.168602 -4.1297073 -4.09225 -4.0677967 -4.0577531 -4.059782 -4.0699558 -4.0794654 -4.0813189 -4.0888209 -4.117878 -4.1568885][-4.176249 -4.1700931 -4.1618943 -4.1347437 -4.1083093 -4.1011147 -4.1041942 -4.1098566 -4.1201434 -4.1287055 -4.1254435 -4.1172261 -4.120018 -4.1447539 -4.1820927][-4.1234465 -4.1074233 -4.1040072 -4.0935831 -4.0891285 -4.1068125 -4.1276665 -4.1414423 -4.1541095 -4.1556478 -4.1435227 -4.1321325 -4.1331625 -4.1540918 -4.1869698][-4.0794377 -4.0700397 -4.0844307 -4.0986147 -4.1094012 -4.1273451 -4.1443028 -4.1535625 -4.157876 -4.1490607 -4.1358023 -4.1319551 -4.1352324 -4.149055 -4.1687684][-4.0882363 -4.0959997 -4.1251993 -4.1459904 -4.1507311 -4.1507568 -4.1488848 -4.1459126 -4.1424394 -4.1301847 -4.1225562 -4.1295843 -4.1330757 -4.1341171 -4.1398487]]...]
INFO - root - 2017-12-05 20:19:01.661535: step 40810, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 72h:32m:40s remains)
INFO - root - 2017-12-05 20:19:11.093215: step 40820, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 75h:29m:02s remains)
INFO - root - 2017-12-05 20:19:20.572764: step 40830, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.967 sec/batch; 78h:20m:18s remains)
INFO - root - 2017-12-05 20:19:29.816914: step 40840, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 73h:54m:16s remains)
INFO - root - 2017-12-05 20:19:38.980389: step 40850, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 75h:49m:34s remains)
INFO - root - 2017-12-05 20:19:48.377516: step 40860, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 77h:16m:29s remains)
INFO - root - 2017-12-05 20:19:57.808068: step 40870, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.958 sec/batch; 77h:34m:50s remains)
INFO - root - 2017-12-05 20:20:07.160179: step 40880, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 77h:10m:04s remains)
INFO - root - 2017-12-05 20:20:16.337574: step 40890, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 77h:35m:03s remains)
INFO - root - 2017-12-05 20:20:25.779587: step 40900, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 76h:12m:15s remains)
2017-12-05 20:20:26.566947: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3334069 -4.3127675 -4.2879934 -4.2623019 -4.2358689 -4.2049923 -4.1818433 -4.1731725 -4.1772714 -4.1692047 -4.1499496 -4.1459861 -4.1760397 -4.1946645 -4.1920028][-4.3313894 -4.3108106 -4.2807574 -4.242672 -4.19997 -4.1584158 -4.1317449 -4.1296358 -4.1395221 -4.1329021 -4.1095757 -4.0967121 -4.12352 -4.1430655 -4.1413608][-4.3266497 -4.302525 -4.2594581 -4.2015743 -4.1430283 -4.0996623 -4.0804763 -4.0869584 -4.0994506 -4.0915232 -4.07042 -4.0654287 -4.0919213 -4.1145887 -4.1139293][-4.3156948 -4.2813215 -4.2224207 -4.1510572 -4.0916572 -4.0611706 -4.0558696 -4.0625906 -4.0676827 -4.0617123 -4.0575233 -4.0649929 -4.0922608 -4.120584 -4.1239171][-4.307044 -4.2633095 -4.1917987 -4.1190238 -4.0734248 -4.0591059 -4.063323 -4.0689459 -4.0669365 -4.06275 -4.0734334 -4.0894446 -4.1181369 -4.1473837 -4.1509857][-4.3030095 -4.2583532 -4.1842961 -4.1196928 -4.0810194 -4.0686908 -4.0737572 -4.082128 -4.0768867 -4.0689774 -4.0774536 -4.090754 -4.1188431 -4.1465368 -4.1561875][-4.302207 -4.2597237 -4.1875167 -4.1258173 -4.0871458 -4.0650349 -4.0635481 -4.075428 -4.0647554 -4.0537353 -4.0580735 -4.0703821 -4.0984707 -4.1294565 -4.1526914][-4.3030863 -4.2622137 -4.1935997 -4.1303897 -4.0887957 -4.0513763 -4.0298977 -4.0244026 -4.0007968 -3.9929764 -4.0015945 -4.0272942 -4.0650353 -4.1052814 -4.1403232][-4.30713 -4.2703795 -4.2063427 -4.1390066 -4.0871687 -4.0292764 -3.9720874 -3.9367266 -3.9140015 -3.932683 -3.9734397 -4.0203671 -4.069345 -4.1117306 -4.1486263][-4.3123622 -4.2831421 -4.2271218 -4.1591659 -4.0950694 -4.0110626 -3.9168732 -3.864486 -3.8727031 -3.9363854 -4.0071435 -4.060081 -4.1069722 -4.1409841 -4.1635389][-4.3218665 -4.300292 -4.2555852 -4.1958866 -4.1296358 -4.0325184 -3.927434 -3.8785667 -3.9079041 -3.9849772 -4.0558605 -4.0994711 -4.1399374 -4.1689739 -4.1833258][-4.3307858 -4.3125105 -4.2796712 -4.2352333 -4.1813369 -4.1010137 -4.0170164 -3.9766355 -3.9923258 -4.0472169 -4.099617 -4.1310453 -4.1633153 -4.190073 -4.2058687][-4.3386178 -4.3261576 -4.3037634 -4.2724671 -4.2367673 -4.1857181 -4.1302018 -4.0942369 -4.0839958 -4.1027436 -4.1316929 -4.1533642 -4.1771073 -4.2016487 -4.2221141][-4.3430166 -4.3347449 -4.3199897 -4.2972431 -4.2765827 -4.2535267 -4.222352 -4.1901293 -4.1653352 -4.1606627 -4.1710777 -4.182446 -4.1971278 -4.2191362 -4.2377167][-4.347445 -4.3420486 -4.3342967 -4.31973 -4.305964 -4.2957377 -4.2814655 -4.2603512 -4.2382131 -4.2256269 -4.2230277 -4.2252059 -4.2330704 -4.2505331 -4.2647333]]...]
INFO - root - 2017-12-05 20:20:35.817140: step 40910, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 75h:38m:48s remains)
INFO - root - 2017-12-05 20:20:45.201998: step 40920, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 77h:19m:06s remains)
INFO - root - 2017-12-05 20:20:54.494271: step 40930, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 68h:48m:11s remains)
INFO - root - 2017-12-05 20:21:03.955346: step 40940, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 76h:52m:39s remains)
INFO - root - 2017-12-05 20:21:13.351915: step 40950, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 73h:16m:01s remains)
INFO - root - 2017-12-05 20:21:22.690730: step 40960, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.924 sec/batch; 74h:49m:50s remains)
INFO - root - 2017-12-05 20:21:32.119977: step 40970, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.936 sec/batch; 75h:46m:13s remains)
INFO - root - 2017-12-05 20:21:41.171919: step 40980, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.765 sec/batch; 61h:55m:20s remains)
INFO - root - 2017-12-05 20:21:50.496940: step 40990, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 74h:04m:50s remains)
INFO - root - 2017-12-05 20:21:59.853694: step 41000, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 69h:42m:55s remains)
2017-12-05 20:22:00.609057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1712852 -4.1554527 -4.1652608 -4.1964412 -4.2106485 -4.20054 -4.1687093 -4.1165724 -4.0707755 -4.0908732 -4.17164 -4.2367668 -4.26832 -4.2901287 -4.3132057][-4.187202 -4.1678495 -4.1667519 -4.1858773 -4.1901736 -4.1740541 -4.1411014 -4.0947266 -4.0615048 -4.0920181 -4.1759958 -4.2400365 -4.2700248 -4.2894397 -4.3102651][-4.2047815 -4.1857829 -4.177918 -4.1824741 -4.1692324 -4.1421618 -4.1043468 -4.0623655 -4.0442734 -4.0862584 -4.1737523 -4.242094 -4.2754531 -4.2938547 -4.3117828][-4.2517 -4.2399778 -4.2261944 -4.2092867 -4.1679745 -4.1167421 -4.0583553 -4.0082974 -4.00713 -4.0731583 -4.1691241 -4.2411404 -4.2789469 -4.2992649 -4.3151097][-4.29929 -4.2973433 -4.2803574 -4.242548 -4.1749225 -4.0983138 -4.0159912 -3.9566693 -3.9682772 -4.0593328 -4.1620355 -4.2367187 -4.2794995 -4.3018885 -4.3154922][-4.32403 -4.3279109 -4.3080626 -4.2544088 -4.1737509 -4.084229 -3.9842963 -3.9162679 -3.9347537 -4.0431027 -4.1499443 -4.2275028 -4.2784762 -4.3039765 -4.3138108][-4.3259559 -4.3302846 -4.3083024 -4.249177 -4.1659961 -4.0716295 -3.9646366 -3.8909776 -3.9069185 -4.02152 -4.135828 -4.2204318 -4.2776709 -4.3063459 -4.31329][-4.3248844 -4.326674 -4.304348 -4.2447214 -4.1600909 -4.0659685 -3.9671068 -3.9034412 -3.9198189 -4.0289788 -4.1390824 -4.2177706 -4.2745233 -4.3055882 -4.3133583][-4.32611 -4.3246021 -4.3034477 -4.2487493 -4.1675992 -4.0773921 -3.986937 -3.9379668 -3.957665 -4.0516505 -4.1470823 -4.2168641 -4.2658472 -4.2985516 -4.3111544][-4.3235917 -4.3253613 -4.3107119 -4.2669568 -4.1976285 -4.1123157 -4.0266623 -3.9841866 -3.9968283 -4.0710244 -4.1563134 -4.2236967 -4.2637792 -4.2926221 -4.3091569][-4.3171453 -4.3253903 -4.3172188 -4.2835464 -4.2242436 -4.1480813 -4.0702138 -4.0261993 -4.027514 -4.0874128 -4.17133 -4.240984 -4.2740278 -4.29584 -4.311955][-4.3030062 -4.3198752 -4.3203769 -4.2955317 -4.2443867 -4.1736336 -4.0997849 -4.0528374 -4.04512 -4.0986571 -4.1870089 -4.2567048 -4.2845936 -4.3003116 -4.3148842][-4.2811375 -4.3088717 -4.32126 -4.3085828 -4.2662449 -4.1981392 -4.1228628 -4.0683727 -4.0499239 -4.0998654 -4.1915874 -4.2613316 -4.2870851 -4.3008337 -4.3159661][-4.264173 -4.2985525 -4.3217726 -4.3243928 -4.298945 -4.2394457 -4.1647682 -4.1018238 -4.0712838 -4.1144891 -4.1992574 -4.2624888 -4.2862058 -4.3020463 -4.3180156][-4.2457523 -4.2812405 -4.3134613 -4.3284492 -4.3202477 -4.2766781 -4.211659 -4.1500378 -4.1144633 -4.1467404 -4.2171044 -4.270823 -4.2930017 -4.310276 -4.3252168]]...]
INFO - root - 2017-12-05 20:22:09.866279: step 41010, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 75h:56m:02s remains)
INFO - root - 2017-12-05 20:22:19.103127: step 41020, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 78h:16m:50s remains)
INFO - root - 2017-12-05 20:22:28.441591: step 41030, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 79h:43m:35s remains)
INFO - root - 2017-12-05 20:22:37.687683: step 41040, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 78h:26m:59s remains)
INFO - root - 2017-12-05 20:22:47.034925: step 41050, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 78h:37m:54s remains)
INFO - root - 2017-12-05 20:22:56.460232: step 41060, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 70h:48m:55s remains)
INFO - root - 2017-12-05 20:23:05.815744: step 41070, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 74h:27m:06s remains)
INFO - root - 2017-12-05 20:23:15.221253: step 41080, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 74h:50m:37s remains)
INFO - root - 2017-12-05 20:23:24.713661: step 41090, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 78h:53m:36s remains)
INFO - root - 2017-12-05 20:23:33.932120: step 41100, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 75h:30m:33s remains)
2017-12-05 20:23:34.692886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2981439 -4.2996912 -4.2987103 -4.2887716 -4.2736211 -4.264225 -4.2637591 -4.267292 -4.2716012 -4.2748723 -4.2797084 -4.2756677 -4.2597613 -4.2320843 -4.209631][-4.3010988 -4.3063707 -4.3019834 -4.2841072 -4.2601042 -4.2367697 -4.2226458 -4.2267113 -4.2403336 -4.25046 -4.2610965 -4.2706051 -4.2737546 -4.2636194 -4.24481][-4.304822 -4.3106904 -4.2991543 -4.2624068 -4.2186761 -4.1755033 -4.1468124 -4.154181 -4.1830988 -4.2147293 -4.2425323 -4.2670059 -4.2887297 -4.2937746 -4.2820644][-4.3130374 -4.3106565 -4.2788215 -4.2135296 -4.1444569 -4.0779295 -4.0345526 -4.0439115 -4.0897422 -4.1466837 -4.1950264 -4.2346859 -4.2702942 -4.29303 -4.2981706][-4.3127761 -4.292172 -4.2321606 -4.1502385 -4.070879 -3.9927902 -3.9346321 -3.93579 -3.9883199 -4.0580544 -4.1176805 -4.1641779 -4.2086525 -4.2490292 -4.276104][-4.2958388 -4.2478089 -4.1665049 -4.0854635 -4.0201106 -3.9466391 -3.8777413 -3.8600488 -3.8923912 -3.9541121 -4.0179005 -4.0689254 -4.1228905 -4.1788788 -4.2242284][-4.2748923 -4.2120986 -4.130558 -4.0666547 -4.028244 -3.9760973 -3.9142797 -3.8825431 -3.8836761 -3.9126644 -3.9599371 -4.0035849 -4.0545731 -4.11308 -4.1608553][-4.2703094 -4.2156935 -4.1565137 -4.1187887 -4.1032705 -4.0701036 -4.0214958 -3.9871562 -3.967905 -3.9699805 -3.9932036 -4.0208511 -4.0555525 -4.0968418 -4.1314487][-4.2810397 -4.2469878 -4.2166452 -4.1999774 -4.1942115 -4.1707282 -4.1346936 -4.1054139 -4.0839243 -4.0748582 -4.07491 -4.0857964 -4.103652 -4.1237917 -4.141273][-4.2937369 -4.2780075 -4.26729 -4.2614985 -4.2568822 -4.2435994 -4.2215519 -4.2000275 -4.1825461 -4.167747 -4.1550813 -4.1518974 -4.1580896 -4.1688876 -4.1767616][-4.3027964 -4.2999234 -4.3014312 -4.3019524 -4.297997 -4.2900734 -4.2784719 -4.2653737 -4.249485 -4.2292271 -4.2094722 -4.1979847 -4.2002072 -4.2086897 -4.214252][-4.3106208 -4.3112903 -4.3158054 -4.3189545 -4.3186135 -4.3170948 -4.3144155 -4.3075538 -4.2928071 -4.2722507 -4.252933 -4.2391429 -4.2379565 -4.2416134 -4.2444296][-4.318924 -4.3196092 -4.3217392 -4.3229995 -4.3237028 -4.3251362 -4.3263617 -4.3228474 -4.3131142 -4.3002157 -4.2866888 -4.2767248 -4.2749591 -4.277411 -4.2790432][-4.3213968 -4.3211226 -4.3209696 -4.3198609 -4.3187404 -4.3189149 -4.3187504 -4.3175006 -4.3153372 -4.3125305 -4.3073525 -4.3024831 -4.3013062 -4.3019485 -4.3019142][-4.3200688 -4.318531 -4.3168697 -4.3147368 -4.3125162 -4.3109007 -4.3098168 -4.3091369 -4.3101006 -4.3115454 -4.3118839 -4.3115454 -4.3113909 -4.311471 -4.3115129]]...]
INFO - root - 2017-12-05 20:23:43.959688: step 41110, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 76h:32m:51s remains)
INFO - root - 2017-12-05 20:23:52.996538: step 41120, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 73h:10m:08s remains)
INFO - root - 2017-12-05 20:24:02.252612: step 41130, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 70h:28m:24s remains)
INFO - root - 2017-12-05 20:24:11.789686: step 41140, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 68h:48m:09s remains)
INFO - root - 2017-12-05 20:24:20.989213: step 41150, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 72h:13m:27s remains)
INFO - root - 2017-12-05 20:24:30.407553: step 41160, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 74h:33m:31s remains)
INFO - root - 2017-12-05 20:24:39.469397: step 41170, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 76h:11m:37s remains)
INFO - root - 2017-12-05 20:24:48.995013: step 41180, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 77h:49m:05s remains)
INFO - root - 2017-12-05 20:24:58.241998: step 41190, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 78h:49m:53s remains)
INFO - root - 2017-12-05 20:25:07.414960: step 41200, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 73h:58m:44s remains)
2017-12-05 20:25:08.165671: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3345156 -4.3316545 -4.3290958 -4.327714 -4.3282623 -4.3277478 -4.3282857 -4.3286982 -4.3276429 -4.3263664 -4.3264251 -4.3269753 -4.3287611 -4.3326178 -4.3390622][-4.3172164 -4.3108511 -4.3056068 -4.3018889 -4.3015151 -4.3025208 -4.306375 -4.309588 -4.3080654 -4.3052011 -4.3049941 -4.3023334 -4.3016009 -4.3079023 -4.3183727][-4.2955036 -4.2859917 -4.2788548 -4.2717919 -4.2669039 -4.2658315 -4.2721267 -4.2772694 -4.2747545 -4.2705355 -4.2703891 -4.2652178 -4.2625079 -4.2725487 -4.287106][-4.2700057 -4.2547269 -4.2419305 -4.2268286 -4.2114291 -4.2028751 -4.2093377 -4.2170396 -4.217453 -4.2169313 -4.2211623 -4.21835 -4.2162619 -4.2296791 -4.2481308][-4.2454987 -4.2201204 -4.1952996 -4.1630344 -4.1307011 -4.1101184 -4.1116 -4.1198106 -4.1310916 -4.143188 -4.1577654 -4.1616549 -4.1631169 -4.1831946 -4.2099829][-4.227036 -4.1883535 -4.1464748 -4.0900197 -4.0358047 -3.9971106 -3.9813759 -3.9846344 -4.0131803 -4.0454707 -4.0750089 -4.0906963 -4.1051531 -4.1395168 -4.1797347][-4.2217178 -4.1713023 -4.1139097 -4.0330582 -3.9585044 -3.8970177 -3.8469696 -3.8313692 -3.8782516 -3.9302046 -3.9736562 -4.0077438 -4.044714 -4.10189 -4.1594286][-4.2275624 -4.17615 -4.1163015 -4.0272593 -3.945406 -3.863101 -3.7692943 -3.7148347 -3.7638516 -3.820231 -3.8647597 -3.9133415 -3.9714189 -4.053906 -4.1323543][-4.2394834 -4.1969428 -4.149229 -4.0801239 -4.010406 -3.9250102 -3.8178515 -3.7338252 -3.7537909 -3.7871332 -3.8112793 -3.8539305 -3.9137063 -4.0060492 -4.0982862][-4.253479 -4.2249537 -4.1951628 -4.1538224 -4.1082468 -4.0412235 -3.9555919 -3.88016 -3.8656821 -3.8615131 -3.8566568 -3.8744159 -3.9175503 -4.0007372 -4.0904784][-4.2748165 -4.2583933 -4.2415552 -4.22161 -4.1997585 -4.1568575 -4.1025743 -4.0510368 -4.0225286 -3.995394 -3.9720163 -3.9674807 -3.9871776 -4.0490265 -4.1217947][-4.3104191 -4.3037529 -4.2945724 -4.2846794 -4.2773805 -4.2570095 -4.2297378 -4.2019634 -4.178371 -4.148922 -4.120491 -4.1051235 -4.1082077 -4.1461306 -4.1930428][-4.3384404 -4.3367362 -4.3321948 -4.3288493 -4.3296552 -4.3248205 -4.3137312 -4.3012867 -4.2863278 -4.2652354 -4.2428803 -4.2284536 -4.2257948 -4.2459874 -4.2708673][-4.3481226 -4.346118 -4.3421593 -4.3415227 -4.3458185 -4.3487105 -4.3477945 -4.3455796 -4.3396015 -4.32839 -4.31516 -4.3049874 -4.3014245 -4.3112583 -4.3224483][-4.3518534 -4.3489528 -4.3451214 -4.3439603 -4.34657 -4.3495975 -4.3520608 -4.3543587 -4.3542223 -4.3508606 -4.3457332 -4.3396778 -4.3355446 -4.3386836 -4.3437438]]...]
INFO - root - 2017-12-05 20:25:17.408665: step 41210, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.958 sec/batch; 77h:32m:01s remains)
INFO - root - 2017-12-05 20:25:26.920952: step 41220, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.967 sec/batch; 78h:15m:23s remains)
INFO - root - 2017-12-05 20:25:36.367111: step 41230, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 75h:03m:28s remains)
INFO - root - 2017-12-05 20:25:45.571113: step 41240, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 71h:40m:10s remains)
INFO - root - 2017-12-05 20:25:54.737579: step 41250, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 74h:43m:21s remains)
INFO - root - 2017-12-05 20:26:04.140973: step 41260, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 77h:29m:16s remains)
INFO - root - 2017-12-05 20:26:13.618276: step 41270, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 76h:23m:13s remains)
INFO - root - 2017-12-05 20:26:23.034035: step 41280, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 76h:40m:36s remains)
INFO - root - 2017-12-05 20:26:32.505091: step 41290, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 74h:22m:38s remains)
INFO - root - 2017-12-05 20:26:41.812170: step 41300, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 76h:19m:13s remains)
2017-12-05 20:26:42.636911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2018561 -4.2030396 -4.2112823 -4.2195625 -4.2348752 -4.2399645 -4.2287889 -4.2082705 -4.1796069 -4.1605392 -4.1503086 -4.1325841 -4.0989342 -4.0869403 -4.1230049][-4.2348466 -4.2438083 -4.2465763 -4.2464685 -4.255343 -4.2555847 -4.2417321 -4.2249746 -4.2002072 -4.17863 -4.1612282 -4.1333442 -4.0917292 -4.0765896 -4.1099916][-4.2410126 -4.2467422 -4.2386737 -4.2327995 -4.239697 -4.2411776 -4.2351632 -4.2323713 -4.2207851 -4.2014341 -4.182632 -4.1561394 -4.1239791 -4.1194458 -4.1474915][-4.2364631 -4.2388997 -4.221168 -4.2093134 -4.2100887 -4.2080088 -4.2061076 -4.21977 -4.223877 -4.2142363 -4.2001991 -4.1808915 -4.1649837 -4.1731267 -4.1981182][-4.2200294 -4.2311378 -4.2202544 -4.2078524 -4.2014108 -4.1900315 -4.1837077 -4.2036662 -4.214612 -4.210218 -4.1966558 -4.1814518 -4.1767492 -4.1894212 -4.2140336][-4.1963835 -4.2177114 -4.2184877 -4.2063036 -4.1884637 -4.1656394 -4.1501527 -4.1716352 -4.1906271 -4.1882172 -4.1744118 -4.1558123 -4.1486735 -4.1569448 -4.1845856][-4.183239 -4.1985097 -4.1946249 -4.1773958 -4.149816 -4.118062 -4.098743 -4.1281686 -4.1634007 -4.1694975 -4.1535254 -4.1245265 -4.1008005 -4.1017447 -4.1399579][-4.1648712 -4.15516 -4.132443 -4.109386 -4.0848994 -4.0610108 -4.0447149 -4.0798841 -4.1320257 -4.1492448 -4.130444 -4.088459 -4.0413027 -4.0372429 -4.0967026][-4.1449642 -4.1025434 -4.0543594 -4.02265 -4.0073953 -3.9996278 -3.9932613 -4.0322595 -4.0939856 -4.1144075 -4.0937362 -4.0478024 -3.9941254 -3.9922287 -4.0660791][-4.1333613 -4.0715084 -4.0093255 -3.9778798 -3.9774055 -3.9917202 -3.99813 -4.0321388 -4.0850677 -4.0975008 -4.0761414 -4.0392261 -3.9957488 -4.0004396 -4.0734291][-4.1490045 -4.0885162 -4.0348206 -4.0142732 -4.025209 -4.0518074 -4.06621 -4.0901074 -4.1181827 -4.1177473 -4.0976486 -4.0722065 -4.0466838 -4.0582013 -4.1159968][-4.1886406 -4.1463637 -4.111146 -4.1015549 -4.1146264 -4.1406255 -4.1547112 -4.1667924 -4.1721625 -4.163795 -4.1479459 -4.1335483 -4.1214571 -4.13405 -4.1718674][-4.2212992 -4.2006836 -4.1847258 -4.1821647 -4.1949615 -4.2163634 -4.22759 -4.2304168 -4.2234416 -4.2107644 -4.1986241 -4.1905084 -4.1848278 -4.1937428 -4.2165613][-4.2351208 -4.2359343 -4.2320981 -4.2312865 -4.2387619 -4.252039 -4.2577691 -4.2562408 -4.24802 -4.2385707 -4.231173 -4.226541 -4.2219262 -4.2245121 -4.23518][-4.2168603 -4.2356396 -4.2418728 -4.2422967 -4.2467532 -4.2570367 -4.2597055 -4.2571287 -4.2522659 -4.2477727 -4.2431426 -4.236721 -4.2294097 -4.2272015 -4.2312722]]...]
INFO - root - 2017-12-05 20:26:52.159071: step 41310, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 76h:55m:25s remains)
INFO - root - 2017-12-05 20:27:01.551924: step 41320, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 76h:59m:24s remains)
INFO - root - 2017-12-05 20:27:10.997864: step 41330, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 74h:53m:42s remains)
INFO - root - 2017-12-05 20:27:20.154896: step 41340, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 73h:13m:00s remains)
INFO - root - 2017-12-05 20:27:29.823491: step 41350, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.970 sec/batch; 78h:26m:09s remains)
INFO - root - 2017-12-05 20:27:39.189588: step 41360, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 75h:42m:57s remains)
INFO - root - 2017-12-05 20:27:48.431648: step 41370, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 75h:03m:36s remains)
INFO - root - 2017-12-05 20:27:57.946413: step 41380, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 76h:55m:32s remains)
INFO - root - 2017-12-05 20:28:07.523897: step 41390, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.926 sec/batch; 74h:51m:20s remains)
INFO - root - 2017-12-05 20:28:16.942904: step 41400, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 76h:54m:30s remains)
2017-12-05 20:28:17.724956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1540885 -4.20647 -4.2271662 -4.2210178 -4.1959085 -4.1609049 -4.1239944 -4.0982118 -4.1084547 -4.146904 -4.1716142 -4.173883 -4.16542 -4.1463289 -4.1219325][-4.1898937 -4.2275963 -4.2432632 -4.2358871 -4.2145433 -4.1832271 -4.1496472 -4.126224 -4.1391149 -4.17944 -4.2033076 -4.2004304 -4.1834178 -4.1502762 -4.1130877][-4.213696 -4.2440629 -4.2610254 -4.2516732 -4.2290554 -4.1955366 -4.1590571 -4.1377916 -4.1533394 -4.1907487 -4.2092595 -4.2072387 -4.1908846 -4.1543961 -4.1112366][-4.232439 -4.2586212 -4.2748365 -4.2610736 -4.2306094 -4.1869349 -4.1443653 -4.1259055 -4.1476927 -4.1839924 -4.2018795 -4.2048397 -4.1924872 -4.159936 -4.1151681][-4.2483006 -4.2690859 -4.28121 -4.2606168 -4.2132378 -4.1438413 -4.0824089 -4.0652609 -4.1059732 -4.159874 -4.1896486 -4.2034225 -4.1989737 -4.1707463 -4.1261578][-4.2657232 -4.2781744 -4.2826171 -4.2556367 -4.1890616 -4.086246 -3.9891303 -3.965543 -4.0336652 -4.1202278 -4.1762238 -4.2092872 -4.212234 -4.1865897 -4.1422973][-4.2847943 -4.2905245 -4.2861924 -4.2549033 -4.178535 -4.0549955 -3.9245684 -3.8808794 -3.9640224 -4.07971 -4.1671796 -4.2225785 -4.2325978 -4.2066779 -4.1618075][-4.3033495 -4.302475 -4.2916446 -4.2606587 -4.191947 -4.081048 -3.9519815 -3.889775 -3.9502616 -4.0591955 -4.1574636 -4.2265077 -4.2453642 -4.2259674 -4.1886334][-4.3187809 -4.3156919 -4.3038821 -4.2764874 -4.2238092 -4.1399684 -4.0330844 -3.9631617 -3.9843476 -4.0622392 -4.1510139 -4.2232246 -4.2528338 -4.2475319 -4.2243967][-4.3275337 -4.3251071 -4.3153968 -4.296154 -4.2605319 -4.2020235 -4.1189966 -4.0502892 -4.0418949 -4.0869017 -4.1587615 -4.22756 -4.2658839 -4.2750015 -4.265646][-4.3285704 -4.32739 -4.3189955 -4.3075142 -4.2859778 -4.2449112 -4.1802773 -4.1213646 -4.1016107 -4.1244226 -4.180346 -4.243166 -4.2828588 -4.2984533 -4.2976317][-4.3192558 -4.3199558 -4.3170891 -4.3117418 -4.2997942 -4.2717476 -4.2234559 -4.1764364 -4.1548486 -4.1642261 -4.2034073 -4.2540588 -4.2886691 -4.3042912 -4.3077469][-4.3089252 -4.30892 -4.3088942 -4.308167 -4.3054161 -4.2904911 -4.2586374 -4.2238193 -4.2032428 -4.2032428 -4.22534 -4.2592635 -4.2854033 -4.2975936 -4.3007231][-4.3090739 -4.3069448 -4.3073797 -4.3082952 -4.3105054 -4.3052526 -4.28721 -4.2626171 -4.2449536 -4.2409368 -4.25152 -4.2712917 -4.2873182 -4.2936144 -4.2934093][-4.3167486 -4.3142824 -4.3143568 -4.3160133 -4.3190351 -4.3172016 -4.306509 -4.2914486 -4.28045 -4.2771635 -4.2819571 -4.2909865 -4.2961545 -4.2950821 -4.2895842]]...]
INFO - root - 2017-12-05 20:28:27.149992: step 41410, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 74h:03m:43s remains)
INFO - root - 2017-12-05 20:28:36.578204: step 41420, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 75h:27m:11s remains)
INFO - root - 2017-12-05 20:28:45.951110: step 41430, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 77h:27m:33s remains)
INFO - root - 2017-12-05 20:28:55.357462: step 41440, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 78h:49m:35s remains)
INFO - root - 2017-12-05 20:29:04.844282: step 41450, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.973 sec/batch; 78h:41m:18s remains)
INFO - root - 2017-12-05 20:29:14.248927: step 41460, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 72h:53m:56s remains)
INFO - root - 2017-12-05 20:29:23.477266: step 41470, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 80h:29m:52s remains)
INFO - root - 2017-12-05 20:29:32.651848: step 41480, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 74h:12m:25s remains)
INFO - root - 2017-12-05 20:29:42.230601: step 41490, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 78h:33m:11s remains)
INFO - root - 2017-12-05 20:29:51.682024: step 41500, loss = 2.04, batch loss = 1.98 (8.0 examples/sec; 0.996 sec/batch; 80h:28m:57s remains)
2017-12-05 20:29:52.424299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1673479 -4.1760015 -4.1917763 -4.2123437 -4.2440724 -4.2733469 -4.27161 -4.2413392 -4.2152295 -4.2123766 -4.2208877 -4.2236285 -4.2129655 -4.1952605 -4.1611757][-4.1745281 -4.1852903 -4.2001276 -4.2233186 -4.2594995 -4.2857141 -4.2798762 -4.2455606 -4.2170997 -4.21691 -4.2307396 -4.2343211 -4.2234254 -4.2071352 -4.1764579][-4.1616993 -4.1806326 -4.2048521 -4.2368608 -4.2714314 -4.286027 -4.2667537 -4.2212291 -4.1872654 -4.1916666 -4.218492 -4.234972 -4.2365494 -4.2330918 -4.2151022][-4.1535873 -4.1868372 -4.22395 -4.2594266 -4.2824659 -4.2793188 -4.2424345 -4.1798844 -4.1395321 -4.1500158 -4.1944423 -4.22939 -4.249064 -4.2634931 -4.2635984][-4.1705236 -4.2122955 -4.2489209 -4.2709012 -4.2693715 -4.2436523 -4.1890039 -4.1127338 -4.0772028 -4.10575 -4.1718965 -4.2235103 -4.2568727 -4.2843156 -4.3012261][-4.2041135 -4.2310619 -4.2499709 -4.2524285 -4.228096 -4.1786795 -4.09765 -4.0096631 -3.9988494 -4.066834 -4.1572304 -4.2186027 -4.2553263 -4.2859263 -4.3098521][-4.2251058 -4.2343059 -4.2339649 -4.2178621 -4.1751485 -4.09854 -3.9816067 -3.8797755 -3.9176631 -4.0367818 -4.1473284 -4.211566 -4.2444468 -4.2692137 -4.291172][-4.216373 -4.2108731 -4.1992908 -4.1787634 -4.1289692 -4.0334229 -3.8905852 -3.784081 -3.8745265 -4.02735 -4.1421103 -4.20313 -4.2292786 -4.2465425 -4.2623563][-4.1757207 -4.1595874 -4.1542721 -4.1509304 -4.1134691 -4.0266247 -3.9082751 -3.8466003 -3.9449196 -4.0740247 -4.1634293 -4.2070436 -4.2213087 -4.2279577 -4.2360225][-4.1317506 -4.1158352 -4.1211553 -4.130796 -4.1120872 -4.0594378 -3.9987938 -3.9867895 -4.0632052 -4.1500273 -4.2058897 -4.2295694 -4.2344317 -4.2338119 -4.2356405][-4.1062269 -4.0993471 -4.1112709 -4.1289396 -4.1277003 -4.1094375 -4.0913568 -4.0995231 -4.1528482 -4.2085814 -4.2439818 -4.2568569 -4.257278 -4.2551551 -4.2561293][-4.102716 -4.1063428 -4.1249552 -4.148766 -4.1608906 -4.164526 -4.1627541 -4.1717086 -4.2060752 -4.2430153 -4.26606 -4.2725005 -4.2712655 -4.270052 -4.273952][-4.1199775 -4.12631 -4.1431904 -4.1661277 -4.1857362 -4.1980324 -4.2012353 -4.2087688 -4.2327185 -4.2596092 -4.2743869 -4.2765045 -4.2728453 -4.2719054 -4.280189][-4.1628447 -4.16885 -4.1794858 -4.1909266 -4.2062674 -4.2184343 -4.2229786 -4.2313714 -4.2510524 -4.2697258 -4.2764134 -4.2728858 -4.2644877 -4.2625589 -4.2737036][-4.2039962 -4.2138281 -4.2217751 -4.2213597 -4.2233877 -4.2263284 -4.2272387 -4.2345681 -4.2540994 -4.2683158 -4.2668657 -4.2562165 -4.2412038 -4.2386622 -4.2545557]]...]
INFO - root - 2017-12-05 20:30:01.608535: step 41510, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 75h:44m:12s remains)
INFO - root - 2017-12-05 20:30:10.836885: step 41520, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 68h:56m:10s remains)
INFO - root - 2017-12-05 20:30:20.200240: step 41530, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 73h:41m:53s remains)
INFO - root - 2017-12-05 20:30:29.479904: step 41540, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 75h:15m:01s remains)
INFO - root - 2017-12-05 20:30:38.705240: step 41550, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.973 sec/batch; 78h:37m:06s remains)
INFO - root - 2017-12-05 20:30:48.140443: step 41560, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 74h:54m:46s remains)
INFO - root - 2017-12-05 20:30:57.350241: step 41570, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 74h:07m:55s remains)
INFO - root - 2017-12-05 20:31:06.798988: step 41580, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.994 sec/batch; 80h:17m:20s remains)
INFO - root - 2017-12-05 20:31:16.354436: step 41590, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.931 sec/batch; 75h:15m:57s remains)
INFO - root - 2017-12-05 20:31:25.539718: step 41600, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.963 sec/batch; 77h:48m:08s remains)
2017-12-05 20:31:26.394200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2419119 -4.2207193 -4.2162747 -4.2277336 -4.2455792 -4.2564344 -4.2583671 -4.2525063 -4.2425876 -4.2370534 -4.2434134 -4.2606564 -4.2782121 -4.2870092 -4.2858882][-4.2178016 -4.1914525 -4.191606 -4.2144628 -4.2412372 -4.2521253 -4.247829 -4.2344146 -4.2190008 -4.2111187 -4.2182946 -4.2415442 -4.266746 -4.2836313 -4.2898426][-4.1964836 -4.1694732 -4.1767516 -4.2087617 -4.235384 -4.2360997 -4.2146192 -4.18642 -4.1654911 -4.1618567 -4.1781092 -4.2106962 -4.2451472 -4.2720461 -4.2860909][-4.1859355 -4.1642075 -4.1777115 -4.2086182 -4.2213216 -4.1988859 -4.1508508 -4.10164 -4.0797653 -4.0955963 -4.1338377 -4.1800003 -4.2234173 -4.257617 -4.2734556][-4.1773071 -4.1618724 -4.1761732 -4.195313 -4.1843023 -4.133275 -4.0558619 -3.9889185 -3.9820693 -4.0358553 -4.1041374 -4.1635051 -4.2116637 -4.2445989 -4.2544532][-4.165153 -4.1492581 -4.1591382 -4.1637259 -4.1286669 -4.0476127 -3.9362969 -3.8585062 -3.8858645 -3.9875374 -4.08771 -4.1573071 -4.2046862 -4.2297106 -4.2272191][-4.134614 -4.1143661 -4.1188354 -4.1131015 -4.0600624 -3.9487448 -3.8033113 -3.730175 -3.8057282 -3.9521356 -4.0748172 -4.1491871 -4.1924477 -4.2093563 -4.1959949][-4.0930161 -4.0718641 -4.0749526 -4.0656252 -4.002717 -3.8711021 -3.7174211 -3.6805387 -3.79293 -3.9484174 -4.0649471 -4.1325197 -4.1703291 -4.1812906 -4.1624608][-4.064652 -4.0505967 -4.057188 -4.0536542 -4.0011005 -3.8859346 -3.7716944 -3.7736688 -3.8751326 -3.9894564 -4.0716095 -4.1211081 -4.1478281 -4.1502562 -4.1278987][-4.0574765 -4.0561495 -4.07086 -4.074729 -4.0408082 -3.9630942 -3.8963645 -3.9112682 -3.9812164 -4.0497131 -4.0979228 -4.1266084 -4.1377397 -4.1296611 -4.1065159][-4.0737987 -4.0839186 -4.1036639 -4.1090579 -4.0878291 -4.0413661 -4.0073895 -4.019474 -4.06201 -4.103446 -4.1329408 -4.1487761 -4.1491027 -4.1354189 -4.1168084][-4.1221619 -4.1336823 -4.1488643 -4.1493979 -4.1351871 -4.1118231 -4.0963268 -4.1013961 -4.1250653 -4.1521034 -4.172267 -4.1830215 -4.1822791 -4.1720057 -4.1628046][-4.188561 -4.1938891 -4.2002935 -4.1968293 -4.1879735 -4.1762323 -4.1662135 -4.1650314 -4.1771336 -4.1953568 -4.212667 -4.2244496 -4.2280412 -4.2250891 -4.2240338][-4.2458329 -4.2440209 -4.2445307 -4.2406635 -4.2348981 -4.2278323 -4.2223544 -4.2213378 -4.2279506 -4.2408905 -4.2550125 -4.2668982 -4.2736459 -4.2757287 -4.2776842][-4.287498 -4.2830625 -4.2822051 -4.2793865 -4.2733135 -4.26668 -4.2630858 -4.2625418 -4.2673812 -4.2774649 -4.2889857 -4.2987466 -4.3044896 -4.3059044 -4.307178]]...]
INFO - root - 2017-12-05 20:31:35.829057: step 41610, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 78h:27m:03s remains)
INFO - root - 2017-12-05 20:31:45.014020: step 41620, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 75h:26m:44s remains)
INFO - root - 2017-12-05 20:31:54.512476: step 41630, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 76h:42m:04s remains)
INFO - root - 2017-12-05 20:32:03.901234: step 41640, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 74h:14m:02s remains)
INFO - root - 2017-12-05 20:32:13.243642: step 41650, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 77h:00m:30s remains)
INFO - root - 2017-12-05 20:32:22.674221: step 41660, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 76h:09m:12s remains)
INFO - root - 2017-12-05 20:32:31.901674: step 41670, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 70h:28m:02s remains)
INFO - root - 2017-12-05 20:32:41.157625: step 41680, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.929 sec/batch; 75h:00m:50s remains)
INFO - root - 2017-12-05 20:32:50.682252: step 41690, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 77h:42m:30s remains)
INFO - root - 2017-12-05 20:33:00.183976: step 41700, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 78h:59m:11s remains)
2017-12-05 20:33:01.012185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3390265 -4.3409309 -4.3412642 -4.34226 -4.3435044 -4.3448753 -4.3457589 -4.3461857 -4.3461103 -4.3449993 -4.3429418 -4.3404365 -4.3383522 -4.3369131 -4.3365626][-4.3410993 -4.3444648 -4.3465219 -4.348547 -4.3505759 -4.3523836 -4.3535848 -4.3541074 -4.3539248 -4.3526931 -4.3506021 -4.3483524 -4.3465691 -4.3450174 -4.3437481][-4.336679 -4.3397994 -4.3419948 -4.3435264 -4.3445525 -4.3455529 -4.3463588 -4.3471203 -4.3483868 -4.3497329 -4.3501611 -4.350307 -4.3507209 -4.3507891 -4.3502774][-4.3182683 -4.3182292 -4.3183861 -4.31828 -4.3168359 -4.3154278 -4.3146753 -4.3152223 -4.3186555 -4.3240104 -4.3287024 -4.3322248 -4.3356748 -4.3386559 -4.3410378][-4.2863836 -4.282692 -4.2812123 -4.27904 -4.2731171 -4.2665534 -4.2618561 -4.2604218 -4.265132 -4.2733979 -4.2808733 -4.28638 -4.2913265 -4.2967343 -4.30293][-4.2519422 -4.24599 -4.2418437 -4.2347722 -4.2204537 -4.2025628 -4.1883984 -4.1811032 -4.1864152 -4.1982174 -4.2085881 -4.2170448 -4.2246914 -4.2336068 -4.2433271][-4.228508 -4.2218862 -4.2158189 -4.2037735 -4.1786976 -4.1450276 -4.1146803 -4.09458 -4.094841 -4.1071324 -4.1201153 -4.1337008 -4.1471376 -4.1632218 -4.17985][-4.2277789 -4.2201214 -4.2121048 -4.1965771 -4.1638231 -4.1197753 -4.0768929 -4.0449462 -4.0361104 -4.042016 -4.0508084 -4.0641022 -4.0800753 -4.1019945 -4.1264415][-4.250936 -4.2460561 -4.2417636 -4.2319965 -4.2060885 -4.1691351 -4.1306252 -4.098207 -4.0820813 -4.0754848 -4.0714979 -4.0735817 -4.0803246 -4.0956373 -4.117867][-4.2823043 -4.2848325 -4.288322 -4.2873321 -4.2732792 -4.249949 -4.2233877 -4.1990037 -4.1821923 -4.168406 -4.1555161 -4.1465082 -4.1410646 -4.1446457 -4.1588349][-4.3017759 -4.3109527 -4.3197212 -4.3238578 -4.3183703 -4.3062806 -4.2922564 -4.2794123 -4.2688656 -4.2568116 -4.243959 -4.2326803 -4.2236066 -4.221889 -4.2297659][-4.3004646 -4.3150134 -4.3264866 -4.33155 -4.3291864 -4.32314 -4.3176303 -4.3143554 -4.3117757 -4.3071465 -4.3011379 -4.2946739 -4.2882161 -4.285718 -4.2896032][-4.278616 -4.293448 -4.3028479 -4.3048506 -4.3017354 -4.2982297 -4.298697 -4.3041191 -4.3111882 -4.31638 -4.3191504 -4.3191123 -4.3163338 -4.3130841 -4.3130856][-4.2556925 -4.2644768 -4.2679181 -4.2648368 -4.2592916 -4.2566066 -4.2608986 -4.2730551 -4.2884173 -4.302546 -4.3128624 -4.317225 -4.3153749 -4.3097563 -4.3052325][-4.259973 -4.26302 -4.2621503 -4.2562637 -4.249495 -4.246069 -4.2497325 -4.2612114 -4.2759356 -4.2902145 -4.3001137 -4.3025208 -4.2966261 -4.2855272 -4.2761173]]...]
INFO - root - 2017-12-05 20:33:10.135901: step 41710, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 74h:04m:39s remains)
INFO - root - 2017-12-05 20:33:19.454391: step 41720, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 75h:18m:12s remains)
INFO - root - 2017-12-05 20:33:28.816118: step 41730, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 72h:43m:39s remains)
INFO - root - 2017-12-05 20:33:38.304807: step 41740, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.958 sec/batch; 77h:23m:30s remains)
INFO - root - 2017-12-05 20:33:47.492021: step 41750, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 68h:40m:19s remains)
INFO - root - 2017-12-05 20:33:56.677735: step 41760, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 68h:04m:43s remains)
INFO - root - 2017-12-05 20:34:06.062730: step 41770, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.931 sec/batch; 75h:10m:18s remains)
INFO - root - 2017-12-05 20:34:15.455730: step 41780, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 76h:03m:44s remains)
INFO - root - 2017-12-05 20:34:24.877982: step 41790, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 76h:55m:01s remains)
INFO - root - 2017-12-05 20:34:34.205191: step 41800, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 72h:10m:44s remains)
2017-12-05 20:34:35.016091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2445917 -4.2551966 -4.2701859 -4.2799015 -4.2839375 -4.284595 -4.2812619 -4.2760158 -4.2727194 -4.2689548 -4.2624731 -4.2547297 -4.2394996 -4.2188325 -4.2081304][-4.2494006 -4.2603388 -4.2713008 -4.2779374 -4.2781711 -4.27255 -4.264595 -4.2556276 -4.2495131 -4.2455082 -4.245491 -4.2499919 -4.2474031 -4.2358446 -4.2263765][-4.2412853 -4.2491441 -4.2561183 -4.2645569 -4.265449 -4.2512751 -4.2343559 -4.21851 -4.2094884 -4.2061367 -4.2163043 -4.2316418 -4.2406025 -4.2403693 -4.2373958][-4.2306376 -4.2340317 -4.2344995 -4.234766 -4.2244987 -4.1929936 -4.1620874 -4.1404953 -4.133709 -4.1444058 -4.1770468 -4.207118 -4.2268763 -4.23853 -4.2447524][-4.2155209 -4.2098374 -4.19909 -4.1860714 -4.1574173 -4.1089234 -4.0617237 -4.0363812 -4.0344286 -4.0655565 -4.12389 -4.1711807 -4.2010341 -4.2229896 -4.2420473][-4.205833 -4.1926303 -4.1728005 -4.1430578 -4.0910997 -4.0264149 -3.9632149 -3.9325154 -3.9371133 -3.9902251 -4.0685072 -4.1272688 -4.1647091 -4.1955485 -4.2275729][-4.2026911 -4.1817074 -4.1540308 -4.1091232 -4.0447693 -3.9755352 -3.8999462 -3.851722 -3.8574686 -3.9253645 -4.0153232 -4.078135 -4.1248865 -4.16393 -4.2010632][-4.19782 -4.171845 -4.1435905 -4.1044545 -4.0600224 -4.0091915 -3.9391596 -3.8795567 -3.8740892 -3.9294798 -4.0021586 -4.0546002 -4.1002197 -4.138413 -4.1743369][-4.1973667 -4.1710944 -4.1527257 -4.1375132 -4.1242018 -4.0963192 -4.0412512 -3.9882865 -3.9783487 -4.0098925 -4.0493317 -4.0776138 -4.1060653 -4.1278672 -4.1561937][-4.1843014 -4.1671443 -4.1661172 -4.1718931 -4.1772127 -4.1612778 -4.1212072 -4.0882492 -4.0852737 -4.0979238 -4.1104245 -4.1185846 -4.1246028 -4.1252046 -4.1410027][-4.1470203 -4.1445055 -4.1659179 -4.1869617 -4.2009716 -4.1927109 -4.1675415 -4.1499448 -4.1504822 -4.1496835 -4.1494284 -4.1445513 -4.1297207 -4.1108389 -4.1158843][-4.1218843 -4.1313047 -4.1665993 -4.1949019 -4.2077794 -4.2012243 -4.1865 -4.1782436 -4.1818795 -4.1796765 -4.1778889 -4.1653 -4.1380405 -4.11017 -4.1105237][-4.1309137 -4.1506705 -4.190239 -4.2184458 -4.2284927 -4.2208991 -4.2099452 -4.2089963 -4.2162614 -4.2165346 -4.2119265 -4.1938496 -4.1663408 -4.1387196 -4.1338339][-4.1670918 -4.1922007 -4.2280965 -4.2496362 -4.2561965 -4.2498236 -4.2430358 -4.244359 -4.2488046 -4.2466292 -4.2380528 -4.2210612 -4.199892 -4.1769557 -4.1669993][-4.2103105 -4.2330837 -4.2575722 -4.2713475 -4.2761579 -4.2729254 -4.2713094 -4.2721014 -4.2715192 -4.2651 -4.2563071 -4.2430625 -4.2287412 -4.213037 -4.201283]]...]
INFO - root - 2017-12-05 20:34:44.273551: step 41810, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 73h:17m:03s remains)
INFO - root - 2017-12-05 20:34:53.569707: step 41820, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 79h:14m:57s remains)
INFO - root - 2017-12-05 20:35:03.014313: step 41830, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 81h:00m:15s remains)
INFO - root - 2017-12-05 20:35:12.323465: step 41840, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 76h:06m:39s remains)
INFO - root - 2017-12-05 20:35:21.674474: step 41850, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 76h:18m:37s remains)
INFO - root - 2017-12-05 20:35:31.072283: step 41860, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 76h:11m:49s remains)
INFO - root - 2017-12-05 20:35:40.483685: step 41870, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 72h:56m:51s remains)
INFO - root - 2017-12-05 20:35:49.725155: step 41880, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 79h:51m:10s remains)
INFO - root - 2017-12-05 20:35:59.086672: step 41890, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 73h:27m:37s remains)
INFO - root - 2017-12-05 20:36:08.251099: step 41900, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 70h:45m:39s remains)
2017-12-05 20:36:09.155471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2251163 -4.2006464 -4.1841397 -4.1727042 -4.155437 -4.1484623 -4.1578722 -4.14511 -4.1172938 -4.0938492 -4.1045122 -4.1399789 -4.1856146 -4.2258334 -4.2513843][-4.2142577 -4.1924996 -4.1711717 -4.1443806 -4.1155124 -4.1050076 -4.1111803 -4.0994744 -4.0884004 -4.0838537 -4.1028614 -4.1318388 -4.1687665 -4.2035971 -4.2356668][-4.209487 -4.19008 -4.1576552 -4.119051 -4.0853291 -4.0714989 -4.0761046 -4.0751681 -4.0846925 -4.0998306 -4.1232834 -4.1347718 -4.1517773 -4.1783614 -4.2082696][-4.2067475 -4.1850734 -4.1424246 -4.0946026 -4.0540862 -4.0373454 -4.0464578 -4.060771 -4.0845046 -4.1051178 -4.1279049 -4.1295109 -4.1337996 -4.1509476 -4.1718321][-4.2099466 -4.1801691 -4.12397 -4.0568585 -4.0069504 -3.991436 -4.0099173 -4.0422764 -4.0787082 -4.0994153 -4.1171627 -4.1175642 -4.1136608 -4.11797 -4.1255455][-4.2137733 -4.1792092 -4.1150374 -4.0360456 -3.9774432 -3.9619718 -3.9814904 -4.0204649 -4.0636353 -4.0841217 -4.0999832 -4.1052427 -4.0982203 -4.0912385 -4.0865092][-4.2117653 -4.1799436 -4.1289339 -4.0583444 -4.0022922 -3.9889896 -4.00346 -4.0395012 -4.0684423 -4.0777578 -4.0945635 -4.1052341 -4.0983219 -4.0807118 -4.0628481][-4.210629 -4.1842418 -4.1504774 -4.0998969 -4.0594215 -4.0533381 -4.0672159 -4.0983944 -4.1096468 -4.0996518 -4.1101384 -4.1170769 -4.0987053 -4.068893 -4.0375915][-4.2224722 -4.1920967 -4.16543 -4.1374235 -4.1184473 -4.120913 -4.136591 -4.1647716 -4.1669712 -4.1468949 -4.1507859 -4.1421466 -4.10544 -4.0637522 -4.026248][-4.2454591 -4.2134509 -4.1888642 -4.1751194 -4.1660757 -4.1690464 -4.1894054 -4.213192 -4.2116103 -4.1926932 -4.1874757 -4.1681285 -4.133575 -4.0918994 -4.0501328][-4.2611756 -4.241508 -4.2275677 -4.2183909 -4.2054834 -4.2048531 -4.2216005 -4.2359815 -4.2310643 -4.2171082 -4.2092071 -4.1895342 -4.1702948 -4.1365967 -4.0956774][-4.2589645 -4.2557473 -4.2550263 -4.2523603 -4.2420335 -4.2391267 -4.2486506 -4.2512665 -4.2436743 -4.2346106 -4.2247086 -4.20546 -4.194304 -4.1734366 -4.1407762][-4.246665 -4.2540779 -4.2646556 -4.2692523 -4.2656188 -4.2663054 -4.2721643 -4.2694297 -4.2618217 -4.2545252 -4.2447367 -4.2233014 -4.2094712 -4.1942458 -4.1727443][-4.2337537 -4.2426834 -4.2555065 -4.2641177 -4.2668791 -4.2743845 -4.2825632 -4.28384 -4.2788062 -4.2693 -4.2602048 -4.24039 -4.2220678 -4.2077465 -4.191422][-4.227253 -4.2335424 -4.2430682 -4.2507954 -4.2572346 -4.2667561 -4.2768378 -4.2827215 -4.2813406 -4.27434 -4.2663937 -4.2527342 -4.2374158 -4.2259464 -4.212256]]...]
INFO - root - 2017-12-05 20:36:18.593226: step 41910, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 75h:13m:01s remains)
INFO - root - 2017-12-05 20:36:27.851366: step 41920, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 80h:08m:15s remains)
INFO - root - 2017-12-05 20:36:37.275199: step 41930, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.993 sec/batch; 80h:10m:25s remains)
INFO - root - 2017-12-05 20:36:46.935614: step 41940, loss = 2.07, batch loss = 2.01 (6.6 examples/sec; 1.208 sec/batch; 97h:31m:21s remains)
INFO - root - 2017-12-05 20:36:56.477503: step 41950, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 75h:53m:14s remains)
INFO - root - 2017-12-05 20:37:05.717788: step 41960, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 78h:17m:15s remains)
INFO - root - 2017-12-05 20:37:15.181686: step 41970, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 77h:20m:15s remains)
INFO - root - 2017-12-05 20:37:24.620550: step 41980, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 77h:37m:12s remains)
INFO - root - 2017-12-05 20:37:33.929501: step 41990, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.013 sec/batch; 81h:44m:38s remains)
INFO - root - 2017-12-05 20:37:43.432699: step 42000, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 75h:54m:06s remains)
2017-12-05 20:37:44.194350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3493748 -4.3422179 -4.3293343 -4.3202753 -4.3154974 -4.3147659 -4.31531 -4.3169303 -4.3231869 -4.3346748 -4.3469863 -4.3566623 -4.36316 -4.3663321 -4.3669624][-4.3272572 -4.3151469 -4.2943082 -4.2785311 -4.2696619 -4.267303 -4.2659492 -4.2635112 -4.2721362 -4.2920761 -4.3140268 -4.3324556 -4.3460779 -4.3548903 -4.3591485][-4.2974086 -4.2811165 -4.2538409 -4.2336187 -4.220468 -4.2140617 -4.208478 -4.1983838 -4.205904 -4.2330484 -4.2658162 -4.2951312 -4.3201008 -4.3380327 -4.3484077][-4.2570562 -4.2416987 -4.2152252 -4.1967344 -4.1813211 -4.1672592 -4.1512966 -4.1263947 -4.1254191 -4.1548061 -4.1982603 -4.2398505 -4.2786818 -4.3097091 -4.330615][-4.2151556 -4.2126155 -4.1961355 -4.1810374 -4.1604919 -4.1349125 -4.1040583 -4.054945 -4.03711 -4.0669465 -4.1197248 -4.1732197 -4.2274122 -4.2747946 -4.3085423][-4.1794457 -4.18898 -4.18357 -4.1712346 -4.1439633 -4.1031184 -4.0512958 -3.9685321 -3.9291151 -3.9671042 -4.0392561 -4.1112146 -4.1816206 -4.2432461 -4.2872953][-4.1391 -4.1562896 -4.157917 -4.148334 -4.1195812 -4.0683661 -3.9844518 -3.8537881 -3.799073 -3.8678036 -3.9763515 -4.068778 -4.1524982 -4.2236795 -4.2737761][-4.1110063 -4.1318 -4.1354394 -4.1268988 -4.099606 -4.0411544 -3.92711 -3.7589004 -3.7048001 -3.812537 -3.9516428 -4.0582881 -4.1464376 -4.2179837 -4.2687292][-4.1142955 -4.1312108 -4.1341825 -4.13078 -4.1103392 -4.0539722 -3.9441283 -3.7915998 -3.7536385 -3.8584442 -3.9790213 -4.072082 -4.1500783 -4.2153425 -4.2652864][-4.1338086 -4.1467 -4.1505327 -4.1549578 -4.1463609 -4.1074109 -4.0320692 -3.9251845 -3.9006219 -3.9684458 -4.0412922 -4.1020222 -4.1626678 -4.2169809 -4.261507][-4.142993 -4.1565027 -4.1606708 -4.1696048 -4.1688857 -4.1455569 -4.0985451 -4.0292153 -4.0133142 -4.05017 -4.0884857 -4.1240654 -4.1721106 -4.219533 -4.2580028][-4.1543779 -4.1720986 -4.1750069 -4.1797633 -4.1781306 -4.1638389 -4.13517 -4.088088 -4.0743818 -4.0964212 -4.1184316 -4.1398177 -4.1768765 -4.219511 -4.2551103][-4.1779652 -4.1918 -4.1857734 -4.1806822 -4.1795673 -4.1736174 -4.1616378 -4.1295223 -4.1136632 -4.1281347 -4.1471806 -4.1627264 -4.1875858 -4.2217464 -4.2543583][-4.2034941 -4.2096715 -4.197989 -4.18327 -4.1758037 -4.1722379 -4.1695943 -4.1501365 -4.1390324 -4.1535835 -4.1723981 -4.1844668 -4.2035818 -4.2330503 -4.2616959][-4.2001624 -4.2081251 -4.2050848 -4.1931844 -4.1781516 -4.1677866 -4.1657171 -4.1556716 -4.1531363 -4.1713643 -4.1919904 -4.2033892 -4.22326 -4.2516456 -4.2750683]]...]
INFO - root - 2017-12-05 20:37:53.640802: step 42010, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 73h:24m:05s remains)
INFO - root - 2017-12-05 20:38:03.145683: step 42020, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 75h:20m:56s remains)
INFO - root - 2017-12-05 20:38:12.456556: step 42030, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 75h:12m:08s remains)
INFO - root - 2017-12-05 20:38:21.799828: step 42040, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 72h:03m:15s remains)
INFO - root - 2017-12-05 20:38:31.195789: step 42050, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 74h:26m:26s remains)
INFO - root - 2017-12-05 20:38:40.463317: step 42060, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 75h:03m:38s remains)
INFO - root - 2017-12-05 20:38:49.793377: step 42070, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 70h:40m:52s remains)
INFO - root - 2017-12-05 20:38:59.120406: step 42080, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.941 sec/batch; 75h:53m:03s remains)
INFO - root - 2017-12-05 20:39:08.621250: step 42090, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 77h:07m:31s remains)
INFO - root - 2017-12-05 20:39:17.971963: step 42100, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 77h:34m:47s remains)
2017-12-05 20:39:18.755156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2213106 -4.2425084 -4.2569523 -4.2552204 -4.2372408 -4.2117004 -4.1880355 -4.1651449 -4.1439667 -4.1310225 -4.1405196 -4.158011 -4.1654654 -4.1683464 -4.1788616][-4.2224283 -4.2477493 -4.2650089 -4.2616796 -4.2388058 -4.2135687 -4.1867757 -4.1588836 -4.1368876 -4.1247792 -4.1314077 -4.1461058 -4.1518378 -4.1533322 -4.1567612][-4.2066765 -4.2298265 -4.2451119 -4.2453413 -4.2207251 -4.1971126 -4.1719365 -4.14512 -4.1258507 -4.11909 -4.1272945 -4.1383228 -4.1441488 -4.1427121 -4.1347575][-4.1731915 -4.1895432 -4.2005129 -4.2035728 -4.1857638 -4.1697497 -4.1507983 -4.132884 -4.1218262 -4.121501 -4.133873 -4.1440248 -4.1508546 -4.1443191 -4.1257563][-4.1356454 -4.1438107 -4.1483283 -4.1489425 -4.1360636 -4.124784 -4.1166768 -4.114871 -4.1209044 -4.134902 -4.1535816 -4.1660352 -4.1717997 -4.160965 -4.1389441][-4.1046233 -4.0950856 -4.0848875 -4.0749474 -4.06014 -4.0562048 -4.0543823 -4.0647039 -4.0951548 -4.1284161 -4.1558509 -4.1712589 -4.1745257 -4.1648684 -4.1506448][-4.0959411 -4.0749097 -4.0478668 -4.013689 -3.9798141 -3.9634867 -3.9507089 -3.9619143 -4.024828 -4.091342 -4.1334357 -4.1469779 -4.1416407 -4.1351171 -4.1377563][-4.0978818 -4.0872884 -4.0545087 -4.00359 -3.9508238 -3.9121904 -3.8676531 -3.8668032 -3.9593978 -4.0534635 -4.100719 -4.1073565 -4.0910993 -4.0884476 -4.1095424][-4.1096115 -4.1121774 -4.0903931 -4.0516391 -4.0099969 -3.976579 -3.9355733 -3.9293952 -3.9947739 -4.0626364 -4.0936594 -4.0874705 -4.0589442 -4.0568042 -4.0884614][-4.1154943 -4.1260171 -4.1188984 -4.1043711 -4.0881209 -4.0732265 -4.0527716 -4.0481281 -4.0767069 -4.1079535 -4.1210608 -4.1065025 -4.071898 -4.0633144 -4.0901165][-4.1439362 -4.1563382 -4.15765 -4.1552229 -4.1523428 -4.1454954 -4.1363573 -4.1307325 -4.1377335 -4.148181 -4.1501565 -4.1366072 -4.1077943 -4.0921426 -4.109303][-4.16557 -4.1821322 -4.19214 -4.19764 -4.19732 -4.1905189 -4.1857767 -4.184422 -4.1856 -4.1874704 -4.1829896 -4.1695457 -4.145205 -4.1256113 -4.1336212][-4.16608 -4.1866026 -4.2060504 -4.2179008 -4.2205291 -4.2168417 -4.2156191 -4.2193713 -4.2200365 -4.2179742 -4.211328 -4.1983724 -4.1767507 -4.1600695 -4.1651216][-4.1717362 -4.1955075 -4.2206149 -4.2335992 -4.2336283 -4.2306538 -4.2323947 -4.239521 -4.2414436 -4.2371931 -4.2339034 -4.2292552 -4.2158871 -4.2013855 -4.20271][-4.1865792 -4.2103219 -4.2339492 -4.2457089 -4.2463403 -4.2445087 -4.2469821 -4.255332 -4.2583704 -4.25508 -4.2559118 -4.258769 -4.2523441 -4.2411518 -4.2385778]]...]
INFO - root - 2017-12-05 20:39:28.112284: step 42110, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.937 sec/batch; 75h:33m:31s remains)
INFO - root - 2017-12-05 20:39:37.344576: step 42120, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 78h:07m:20s remains)
INFO - root - 2017-12-05 20:39:46.421055: step 42130, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 71h:54m:27s remains)
INFO - root - 2017-12-05 20:39:55.910330: step 42140, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.973 sec/batch; 78h:27m:46s remains)
INFO - root - 2017-12-05 20:40:05.164584: step 42150, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 76h:59m:13s remains)
INFO - root - 2017-12-05 20:40:14.528798: step 42160, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 76h:16m:17s remains)
INFO - root - 2017-12-05 20:40:23.945881: step 42170, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 75h:13m:31s remains)
INFO - root - 2017-12-05 20:40:33.212691: step 42180, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.890 sec/batch; 71h:44m:00s remains)
INFO - root - 2017-12-05 20:40:42.479706: step 42190, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 73h:34m:14s remains)
INFO - root - 2017-12-05 20:40:51.796012: step 42200, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.936 sec/batch; 75h:26m:49s remains)
2017-12-05 20:40:52.580997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1703162 -4.1798344 -4.2019954 -4.20951 -4.1958551 -4.1699657 -4.1539717 -4.14253 -4.1446638 -4.1657019 -4.1888824 -4.2006426 -4.1844888 -4.165 -4.1729383][-4.216362 -4.2114415 -4.21274 -4.2089791 -4.1885118 -4.157629 -4.1340489 -4.1197324 -4.1253514 -4.1508904 -4.1780481 -4.20083 -4.19166 -4.1708317 -4.1790195][-4.2576828 -4.2369461 -4.2207165 -4.2074647 -4.1833372 -4.1551609 -4.1350088 -4.1224256 -4.1277075 -4.1488919 -4.1708164 -4.2025528 -4.2076421 -4.1878033 -4.1889877][-4.2707386 -4.2383547 -4.2127695 -4.1982713 -4.1784248 -4.1571884 -4.1428208 -4.1391358 -4.1443615 -4.155756 -4.1673422 -4.1978583 -4.2112017 -4.1905746 -4.1790428][-4.2669115 -4.2379122 -4.2130384 -4.2040987 -4.1928611 -4.1772184 -4.1595397 -4.1528831 -4.1564937 -4.1621714 -4.1632857 -4.1865859 -4.1969585 -4.1695485 -4.1410542][-4.249516 -4.236414 -4.2241888 -4.2218246 -4.2134814 -4.1962194 -4.1699553 -4.1478043 -4.1488771 -4.148241 -4.1406317 -4.1570916 -4.1615825 -4.1288619 -4.0971346][-4.2421966 -4.2360678 -4.2280645 -4.2246137 -4.215239 -4.1902089 -4.14489 -4.0964041 -4.0870433 -4.0902743 -4.0922275 -4.1173596 -4.1350532 -4.1200757 -4.1048017][-4.2343273 -4.2248707 -4.21449 -4.2032046 -4.1854887 -4.1450534 -4.0699105 -3.9900813 -3.9840877 -4.0251393 -4.0647888 -4.1189628 -4.1607323 -4.1653247 -4.1588321][-4.207231 -4.191515 -4.1728535 -4.1519294 -4.1305184 -4.0822372 -3.9953175 -3.9251478 -3.9602342 -4.0502181 -4.1176996 -4.1778493 -4.218719 -4.2200255 -4.2059374][-4.1599464 -4.1382127 -4.1204462 -4.1033993 -4.0989437 -4.0793104 -4.0315852 -4.0069184 -4.0572023 -4.139009 -4.1907372 -4.2281809 -4.2508211 -4.238328 -4.2082181][-4.1105037 -4.0985413 -4.1047354 -4.1124673 -4.1329017 -4.1434064 -4.1231475 -4.1164041 -4.1496339 -4.2006974 -4.2268958 -4.2332244 -4.2295742 -4.2035136 -4.1681724][-4.1010914 -4.1107807 -4.1391497 -4.1595287 -4.1891446 -4.2067413 -4.1888328 -4.1763763 -4.1847949 -4.2058969 -4.2102675 -4.1916237 -4.1751556 -4.15729 -4.1370854][-4.1367106 -4.1545463 -4.1815677 -4.1936831 -4.214551 -4.2232256 -4.1999412 -4.1787982 -4.1654196 -4.1634722 -4.1579871 -4.135231 -4.1248603 -4.130847 -4.1363692][-4.1781158 -4.1931195 -4.2103209 -4.2133193 -4.2202029 -4.218358 -4.1868696 -4.1563606 -4.1278729 -4.1179657 -4.1231432 -4.1188097 -4.1210852 -4.1425829 -4.1596212][-4.2012143 -4.2143679 -4.2307682 -4.2316284 -4.231329 -4.2236447 -4.1886334 -4.1520171 -4.1277328 -4.1299706 -4.1518631 -4.162508 -4.1770215 -4.20217 -4.2111654]]...]
INFO - root - 2017-12-05 20:41:01.854746: step 42210, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 70h:46m:29s remains)
INFO - root - 2017-12-05 20:41:11.353156: step 42220, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 76h:14m:25s remains)
INFO - root - 2017-12-05 20:41:20.627657: step 42230, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 79h:36m:21s remains)
INFO - root - 2017-12-05 20:41:29.992028: step 42240, loss = 2.02, batch loss = 1.96 (8.2 examples/sec; 0.979 sec/batch; 78h:54m:49s remains)
INFO - root - 2017-12-05 20:41:39.100431: step 42250, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 75h:57m:03s remains)
INFO - root - 2017-12-05 20:41:48.425533: step 42260, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 77h:43m:55s remains)
INFO - root - 2017-12-05 20:41:58.112339: step 42270, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 79h:33m:28s remains)
INFO - root - 2017-12-05 20:42:07.591834: step 42280, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 79h:29m:23s remains)
INFO - root - 2017-12-05 20:42:17.070542: step 42290, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 77h:14m:34s remains)
INFO - root - 2017-12-05 20:42:26.521925: step 42300, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 75h:48m:30s remains)
2017-12-05 20:42:27.296146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.276896 -4.2717481 -4.2648921 -4.2668719 -4.2757449 -4.27686 -4.2729969 -4.2729192 -4.2753034 -4.2759042 -4.2798629 -4.2896872 -4.2999578 -4.3029871 -4.3052378][-4.2527981 -4.2504721 -4.2485809 -4.2471867 -4.2542667 -4.2524 -4.2405958 -4.2347922 -4.2385778 -4.2441683 -4.2509842 -4.2638683 -4.2793179 -4.2841644 -4.2859445][-4.2308583 -4.2348533 -4.2412357 -4.2384081 -4.2391477 -4.2310448 -4.2094617 -4.1981115 -4.2056208 -4.2199907 -4.2350388 -4.2507629 -4.265779 -4.2695851 -4.2710447][-4.200995 -4.2115479 -4.2207723 -4.21627 -4.2104893 -4.1947818 -4.1669464 -4.1514006 -4.162941 -4.18775 -4.2153563 -4.2390404 -4.255074 -4.2594109 -4.2633066][-4.1590204 -4.1699057 -4.1804733 -4.1769733 -4.1680984 -4.1490822 -4.1211338 -4.1042161 -4.1161761 -4.1476011 -4.1845818 -4.218884 -4.2399025 -4.249012 -4.2580018][-4.1316175 -4.1324344 -4.1413107 -4.1383433 -4.1243625 -4.1005707 -4.0723305 -4.0623112 -4.079947 -4.11857 -4.1614847 -4.2041512 -4.2313757 -4.2449837 -4.25559][-4.1343102 -4.11814 -4.1178308 -4.1071496 -4.0763488 -4.0348544 -4.0051661 -4.0058169 -4.0355892 -4.0898809 -4.1423984 -4.19218 -4.226584 -4.2438464 -4.2533474][-4.165925 -4.1314259 -4.1117644 -4.0850163 -4.0364118 -3.9785831 -3.9489274 -3.9575498 -3.9979286 -4.06587 -4.1247935 -4.1775146 -4.2153597 -4.2342596 -4.2406516][-4.1724572 -4.1350784 -4.1075287 -4.0806503 -4.0368142 -3.9970791 -3.989152 -4.0070524 -4.0480003 -4.1069098 -4.1522894 -4.19081 -4.2196751 -4.2307734 -4.2309394][-4.1558456 -4.1311288 -4.1104841 -4.0917068 -4.0707736 -4.0636077 -4.0792837 -4.1006546 -4.1340528 -4.1722684 -4.1947694 -4.21512 -4.2334752 -4.2358508 -4.2291846][-4.1334476 -4.1311536 -4.1245909 -4.1183934 -4.1202903 -4.1357374 -4.1597219 -4.1732812 -4.191772 -4.2122517 -4.2213926 -4.229969 -4.2415733 -4.2404122 -4.23289][-4.1453953 -4.1516986 -4.1539426 -4.1560984 -4.1675091 -4.1892824 -4.2107258 -4.218163 -4.2281008 -4.2404733 -4.2447677 -4.2470994 -4.2532787 -4.2484756 -4.2387056][-4.2039766 -4.2053685 -4.2063541 -4.2041349 -4.2078404 -4.222434 -4.2364883 -4.2395873 -4.2481279 -4.260067 -4.266696 -4.267344 -4.2698727 -4.2637048 -4.2528248][-4.2578015 -4.25088 -4.2471004 -4.2403126 -4.2357955 -4.2430086 -4.2485394 -4.2474446 -4.2554679 -4.268075 -4.2780004 -4.2801261 -4.2810526 -4.2769442 -4.2680812][-4.2898664 -4.2800484 -4.2735744 -4.263968 -4.2550478 -4.2549896 -4.2540426 -4.252521 -4.2621145 -4.2752786 -4.2839546 -4.2825108 -4.2809749 -4.2788968 -4.2739763]]...]
INFO - root - 2017-12-05 20:42:36.499223: step 42310, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 78h:42m:13s remains)
INFO - root - 2017-12-05 20:42:45.987708: step 42320, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.908 sec/batch; 73h:12m:53s remains)
INFO - root - 2017-12-05 20:42:55.262662: step 42330, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 78h:19m:31s remains)
INFO - root - 2017-12-05 20:43:04.525578: step 42340, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 76h:47m:05s remains)
INFO - root - 2017-12-05 20:43:13.932335: step 42350, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 78h:06m:39s remains)
INFO - root - 2017-12-05 20:43:23.475628: step 42360, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 78h:13m:05s remains)
INFO - root - 2017-12-05 20:43:32.694115: step 42370, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 71h:21m:54s remains)
INFO - root - 2017-12-05 20:43:42.164448: step 42380, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 77h:03m:53s remains)
INFO - root - 2017-12-05 20:43:51.718769: step 42390, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 77h:56m:10s remains)
INFO - root - 2017-12-05 20:44:01.126227: step 42400, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 78h:17m:57s remains)
2017-12-05 20:44:01.831012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2434592 -4.264606 -4.2928081 -4.2975192 -4.284935 -4.2683296 -4.2508073 -4.24236 -4.2535591 -4.2610641 -4.2497196 -4.231369 -4.2117162 -4.1956329 -4.1956425][-4.2413516 -4.2549996 -4.2776828 -4.274693 -4.2553082 -4.2326646 -4.2169437 -4.2143993 -4.2363887 -4.2489619 -4.2428675 -4.2277174 -4.2113423 -4.1956744 -4.1881971][-4.2275581 -4.2246017 -4.2328448 -4.2221713 -4.1982307 -4.1726403 -4.1602407 -4.1730881 -4.2124815 -4.2403436 -4.2481976 -4.2428484 -4.22972 -4.2129884 -4.1959882][-4.2050366 -4.1875629 -4.1870871 -4.1785431 -4.1565914 -4.1319737 -4.11602 -4.1319108 -4.1937327 -4.2415624 -4.2618942 -4.2624712 -4.2490606 -4.2291994 -4.2033033][-4.1658907 -4.1461926 -4.1461005 -4.1455965 -4.1311178 -4.1031294 -4.0631814 -4.056304 -4.1334591 -4.2024708 -4.2314305 -4.2340326 -4.2235103 -4.2087 -4.1912651][-4.1164069 -4.109097 -4.1178756 -4.1239038 -4.1119504 -4.0655313 -3.9733596 -3.9074726 -3.9915359 -4.0905242 -4.1397991 -4.1547303 -4.1535611 -4.1514859 -4.1605778][-4.0639606 -4.0652652 -4.0772009 -4.0808778 -4.0625377 -3.9943922 -3.8540623 -3.7237248 -3.8121238 -3.9523017 -4.0342178 -4.0789509 -4.0942836 -4.0997119 -4.125329][-4.016583 -4.022449 -4.0277939 -4.0218225 -4.00367 -3.9380412 -3.8013706 -3.6672764 -3.7340071 -3.8767626 -3.9671533 -4.0310555 -4.0573 -4.0679121 -4.0952764][-4.025043 -4.0350556 -4.0348563 -4.0253329 -4.008544 -3.9635653 -3.8800235 -3.8073702 -3.8369308 -3.9236417 -3.9875035 -4.0412383 -4.0667892 -4.0748205 -4.0939603][-4.0706682 -4.0880127 -4.0911322 -4.0852156 -4.0727277 -4.0480127 -4.0142937 -3.9874985 -3.9955425 -4.0379853 -4.073432 -4.10578 -4.1211262 -4.1208124 -4.1287708][-4.1405969 -4.1595836 -4.1638932 -4.1607828 -4.1578555 -4.1454844 -4.1325355 -4.1256866 -4.1291957 -4.1534095 -4.1776958 -4.1953425 -4.1983991 -4.1910725 -4.1887727][-4.2145119 -4.2292066 -4.2318 -4.2342057 -4.2427931 -4.2351351 -4.2241879 -4.2262831 -4.2304215 -4.2454453 -4.263051 -4.277401 -4.2770696 -4.2633972 -4.2512255][-4.2856579 -4.2933078 -4.296227 -4.3013177 -4.3097429 -4.3016729 -4.2906795 -4.2885494 -4.2919941 -4.3012791 -4.3121319 -4.3250685 -4.3284326 -4.3160734 -4.3001914][-4.3240747 -4.3278909 -4.3299446 -4.3350682 -4.3401279 -4.3341389 -4.3242884 -4.3156476 -4.3150368 -4.3213758 -4.3296218 -4.3405643 -4.3474617 -4.3427029 -4.3291292][-4.327529 -4.3294 -4.3301315 -4.3320656 -4.3332906 -4.3294282 -4.3215232 -4.3135381 -4.3128705 -4.3159251 -4.3207722 -4.3277855 -4.3353739 -4.3375483 -4.3301296]]...]
INFO - root - 2017-12-05 20:44:11.115202: step 42410, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 75h:09m:24s remains)
INFO - root - 2017-12-05 20:44:20.405122: step 42420, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.941 sec/batch; 75h:48m:55s remains)
INFO - root - 2017-12-05 20:44:29.809725: step 42430, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 70h:24m:21s remains)
INFO - root - 2017-12-05 20:44:39.016249: step 42440, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 74h:31m:35s remains)
INFO - root - 2017-12-05 20:44:48.560682: step 42450, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 78h:23m:08s remains)
INFO - root - 2017-12-05 20:44:57.861741: step 42460, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 72h:02m:05s remains)
INFO - root - 2017-12-05 20:45:07.227604: step 42470, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 74h:22m:59s remains)
INFO - root - 2017-12-05 20:45:16.698539: step 42480, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 70h:27m:51s remains)
INFO - root - 2017-12-05 20:45:26.088330: step 42490, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 75h:07m:47s remains)
INFO - root - 2017-12-05 20:45:35.274007: step 42500, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 74h:09m:22s remains)
2017-12-05 20:45:36.069058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2187324 -4.2371016 -4.2611346 -4.2729139 -4.2679591 -4.2493234 -4.2337 -4.234024 -4.2404518 -4.2309637 -4.2002497 -4.1745338 -4.1786332 -4.2072225 -4.2401414][-4.2226434 -4.2336149 -4.2520866 -4.2615695 -4.2587147 -4.2390428 -4.2211356 -4.2195158 -4.2309508 -4.2290688 -4.202795 -4.1793447 -4.1850486 -4.2118254 -4.2428856][-4.2212076 -4.2223263 -4.2317367 -4.2387519 -4.2387996 -4.224822 -4.212039 -4.2134008 -4.23113 -4.2345014 -4.211072 -4.1857963 -4.1863246 -4.2085543 -4.2362995][-4.2196436 -4.2110729 -4.2114191 -4.2151513 -4.2131481 -4.2013674 -4.1947103 -4.2074194 -4.2379203 -4.2508016 -4.2341509 -4.2045708 -4.186131 -4.1896281 -4.204617][-4.232069 -4.2136903 -4.2004409 -4.1905231 -4.1761556 -4.1543155 -4.1478324 -4.1805577 -4.2313142 -4.2598104 -4.2575879 -4.2267852 -4.1876993 -4.1666512 -4.1617556][-4.2483692 -4.2212234 -4.1921358 -4.1572447 -4.1133528 -4.0652013 -4.0522714 -4.1114144 -4.1942377 -4.2457013 -4.2614689 -4.2410417 -4.2008677 -4.1656075 -4.1476121][-4.2472305 -4.2198706 -4.1833177 -4.128901 -4.0533652 -3.9648054 -3.9317775 -4.0184669 -4.1389623 -4.2183928 -4.25403 -4.2528796 -4.2302046 -4.2011585 -4.1843557][-4.2259326 -4.2069974 -4.178998 -4.1296163 -4.0504742 -3.9469938 -3.8979878 -3.9873896 -4.1168427 -4.2085447 -4.256824 -4.2710114 -4.2628236 -4.2420659 -4.2252231][-4.2106757 -4.2026081 -4.1933327 -4.1710229 -4.1215935 -4.0495315 -4.0148659 -4.06994 -4.1576471 -4.2264829 -4.2667646 -4.2807016 -4.2752523 -4.2561479 -4.2339783][-4.2217274 -4.21659 -4.2198725 -4.2199955 -4.1981592 -4.1538377 -4.1314912 -4.1579862 -4.2000113 -4.2388654 -4.267869 -4.2797356 -4.2725682 -4.2527604 -4.2283039][-4.2396364 -4.2286425 -4.2328482 -4.2414036 -4.2313361 -4.199348 -4.1827669 -4.1959319 -4.2161074 -4.2414913 -4.2652864 -4.2757874 -4.2708635 -4.254241 -4.2298269][-4.2448621 -4.2289782 -4.2339573 -4.2434654 -4.2389841 -4.2127142 -4.1961236 -4.205266 -4.2243385 -4.2504153 -4.2729115 -4.2836852 -4.2808146 -4.2663484 -4.2426472][-4.2566843 -4.2394857 -4.2384934 -4.2434254 -4.2411695 -4.2149339 -4.1939077 -4.2015376 -4.2266774 -4.2617016 -4.2875051 -4.296627 -4.2921667 -4.2750912 -4.2488441][-4.2660351 -4.2534375 -4.2488861 -4.2501945 -4.2478118 -4.22009 -4.194675 -4.1993818 -4.2260871 -4.2650456 -4.2913637 -4.2925296 -4.2802563 -4.2615166 -4.235116][-4.2684712 -4.2642961 -4.2600932 -4.2584538 -4.2564917 -4.2309494 -4.2042646 -4.2044368 -4.224515 -4.2551403 -4.2752876 -4.2695155 -4.25047 -4.2289281 -4.2075205]]...]
INFO - root - 2017-12-05 20:45:45.511309: step 42510, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.922 sec/batch; 74h:18m:26s remains)
INFO - root - 2017-12-05 20:45:54.840153: step 42520, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 74h:01m:57s remains)
INFO - root - 2017-12-05 20:46:04.167058: step 42530, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 72h:08m:27s remains)
INFO - root - 2017-12-05 20:46:13.380996: step 42540, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 69h:32m:51s remains)
INFO - root - 2017-12-05 20:46:22.637257: step 42550, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 76h:00m:02s remains)
INFO - root - 2017-12-05 20:46:32.043272: step 42560, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 78h:46m:57s remains)
INFO - root - 2017-12-05 20:46:41.527484: step 42570, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 74h:52m:11s remains)
INFO - root - 2017-12-05 20:46:50.749667: step 42580, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 74h:37m:18s remains)
INFO - root - 2017-12-05 20:47:00.147408: step 42590, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 75h:45m:04s remains)
INFO - root - 2017-12-05 20:47:09.602610: step 42600, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 78h:01m:20s remains)
2017-12-05 20:47:10.380098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1692281 -4.1695719 -4.1860967 -4.2085104 -4.2218127 -4.2206249 -4.21714 -4.2057562 -4.184382 -4.1757073 -4.1692147 -4.160255 -4.1580977 -4.1599164 -4.15421][-4.1710205 -4.1652455 -4.1713953 -4.1845531 -4.1927867 -4.1899185 -4.1875014 -4.1841354 -4.1715903 -4.1699076 -4.17298 -4.1707878 -4.1762924 -4.181725 -4.179287][-4.182673 -4.1702566 -4.1658626 -4.1669221 -4.1655984 -4.1559711 -4.1486306 -4.1494355 -4.1424532 -4.1446476 -4.1547675 -4.1632009 -4.1816139 -4.1936483 -4.194355][-4.1880822 -4.1766186 -4.1720524 -4.1683469 -4.1549644 -4.134223 -4.1160283 -4.1097741 -4.1021013 -4.1066346 -4.1219082 -4.1388674 -4.1669269 -4.1941304 -4.2043247][-4.1786952 -4.1687155 -4.1660347 -4.1630125 -4.142489 -4.1129193 -4.0866742 -4.0759788 -4.072464 -4.0850544 -4.1083412 -4.131474 -4.161798 -4.1949673 -4.2073746][-4.160398 -4.1459894 -4.1356587 -4.1281881 -4.1061158 -4.0756707 -4.0484309 -4.0397406 -4.0498781 -4.0824795 -4.1226964 -4.1538568 -4.1819077 -4.2071528 -4.207624][-4.1464596 -4.1288247 -4.1095204 -4.0926757 -4.0713549 -4.0426183 -4.0141726 -4.0112448 -4.0326295 -4.0737209 -4.1182604 -4.1536 -4.1841612 -4.2039852 -4.1979089][-4.1411729 -4.1310711 -4.1133885 -4.0926118 -4.07087 -4.0454707 -4.0247126 -4.0279012 -4.04719 -4.0746541 -4.0993195 -4.1254463 -4.1578927 -4.1763206 -4.1722746][-4.1369777 -4.1376686 -4.132298 -4.1222954 -4.1078792 -4.0875278 -4.0718465 -4.071435 -4.0770516 -4.0833521 -4.0838237 -4.0888648 -4.1093788 -4.120194 -4.117754][-4.1495509 -4.1551366 -4.1604013 -4.1620946 -4.1592107 -4.1464047 -4.1281228 -4.1125793 -4.1003432 -4.0910196 -4.0787678 -4.0695386 -4.0725636 -4.068172 -4.0592928][-4.1739168 -4.1807795 -4.1892877 -4.1934152 -4.1958032 -4.1904984 -4.1715765 -4.1427894 -4.1136055 -4.0904942 -4.0704465 -4.0590329 -4.056283 -4.0432329 -4.0252209][-4.2001495 -4.205029 -4.2096624 -4.2091117 -4.2092047 -4.2024941 -4.1824341 -4.1491323 -4.1131444 -4.0849628 -4.067987 -4.0675688 -4.0780358 -4.0778346 -4.0636091][-4.2144804 -4.215457 -4.2134938 -4.2063303 -4.1979027 -4.1839705 -4.1634049 -4.1380992 -4.1113291 -4.091558 -4.0867009 -4.1007004 -4.1283855 -4.1460252 -4.1399646][-4.213438 -4.211833 -4.2046795 -4.1924291 -4.176887 -4.1617293 -4.1492667 -4.1425 -4.1329036 -4.1227803 -4.1227555 -4.1401486 -4.168416 -4.1891012 -4.1873083][-4.2012262 -4.2006764 -4.196465 -4.1848645 -4.1691718 -4.1539321 -4.1484132 -4.1603246 -4.1671362 -4.1632748 -4.1593766 -4.1704578 -4.1915536 -4.2055316 -4.2022214]]...]
INFO - root - 2017-12-05 20:47:19.699461: step 42610, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 72h:21m:58s remains)
INFO - root - 2017-12-05 20:47:28.938054: step 42620, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 69h:52m:55s remains)
INFO - root - 2017-12-05 20:47:38.083625: step 42630, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 75h:20m:31s remains)
INFO - root - 2017-12-05 20:47:47.562047: step 42640, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 78h:34m:29s remains)
INFO - root - 2017-12-05 20:47:56.990560: step 42650, loss = 2.05, batch loss = 2.00 (8.0 examples/sec; 1.004 sec/batch; 80h:49m:16s remains)
INFO - root - 2017-12-05 20:48:06.331935: step 42660, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 79h:42m:16s remains)
INFO - root - 2017-12-05 20:48:15.855203: step 42670, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.006 sec/batch; 81h:01m:22s remains)
INFO - root - 2017-12-05 20:48:25.144689: step 42680, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 78h:01m:49s remains)
INFO - root - 2017-12-05 20:48:34.286200: step 42690, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 69h:19m:03s remains)
INFO - root - 2017-12-05 20:48:43.648984: step 42700, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 71h:56m:43s remains)
2017-12-05 20:48:44.417649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2533865 -4.2262368 -4.1845164 -4.1575766 -4.1580324 -4.1852503 -4.2168555 -4.234467 -4.2366958 -4.23037 -4.2264185 -4.2215347 -4.2048817 -4.1826048 -4.1651278][-4.24594 -4.212141 -4.166791 -4.141891 -4.1536837 -4.1940522 -4.2324843 -4.256144 -4.2585649 -4.24327 -4.2260275 -4.2137828 -4.1980205 -4.1831727 -4.1758394][-4.2255435 -4.19534 -4.1599646 -4.1428046 -4.1594653 -4.2042885 -4.2379293 -4.2591038 -4.2578821 -4.2374477 -4.216538 -4.2065172 -4.1982732 -4.1960254 -4.1945252][-4.2116547 -4.1922688 -4.1756153 -4.1683741 -4.1799297 -4.2133074 -4.2336855 -4.248539 -4.2412572 -4.2190142 -4.201611 -4.2015414 -4.2085071 -4.2177095 -4.2162066][-4.2054915 -4.1974068 -4.1944523 -4.1884704 -4.1875067 -4.2010326 -4.2089877 -4.2157521 -4.202527 -4.1805973 -4.1688027 -4.1813564 -4.2050643 -4.2173462 -4.2112308][-4.199945 -4.1962485 -4.197185 -4.1864347 -4.17235 -4.1695194 -4.1644568 -4.1547804 -4.1339507 -4.1184454 -4.1181679 -4.1488533 -4.1892452 -4.2037306 -4.19074][-4.1989331 -4.1944695 -4.1930823 -4.1753535 -4.1506028 -4.1354127 -4.1227188 -4.1032457 -4.0807333 -4.0786376 -4.0965233 -4.135848 -4.179647 -4.1926894 -4.1779432][-4.2102871 -4.2024107 -4.1961055 -4.1763067 -4.1485739 -4.1294227 -4.119318 -4.1073017 -4.098567 -4.1035838 -4.1211085 -4.1527357 -4.1828785 -4.1847959 -4.1619124][-4.2252173 -4.21347 -4.2016172 -4.1832557 -4.161272 -4.1480412 -4.1456008 -4.1488376 -4.15087 -4.1555419 -4.1623087 -4.1703343 -4.17364 -4.1599412 -4.1356215][-4.2355642 -4.21965 -4.2065597 -4.1972909 -4.1887879 -4.183497 -4.1873951 -4.1946421 -4.1966772 -4.1924047 -4.1832123 -4.1668797 -4.1462059 -4.1255512 -4.1136456][-4.2378354 -4.2219133 -4.2133131 -4.2138767 -4.2166862 -4.2202673 -4.2259502 -4.2307148 -4.2279034 -4.2158594 -4.191102 -4.1543674 -4.1232224 -4.1115804 -4.1176033][-4.22585 -4.2183423 -4.2185583 -4.2254767 -4.2358055 -4.2436204 -4.2500181 -4.2509103 -4.2399459 -4.2179475 -4.1819687 -4.1386805 -4.1114721 -4.1127062 -4.1268225][-4.21549 -4.2170219 -4.2256284 -4.2385697 -4.2502809 -4.2578182 -4.2557397 -4.2471957 -4.2295456 -4.2010808 -4.16409 -4.1274133 -4.1100407 -4.1155477 -4.1258192][-4.2210674 -4.2262254 -4.2378693 -4.2536817 -4.2631369 -4.2617359 -4.2474747 -4.2280574 -4.2048836 -4.1761675 -4.1470952 -4.1221061 -4.1136312 -4.1175308 -4.121798][-4.2303348 -4.239841 -4.2545881 -4.2695827 -4.2735019 -4.26202 -4.237865 -4.2108297 -4.1874809 -4.1682825 -4.1530228 -4.1410985 -4.1379495 -4.1397138 -4.1392088]]...]
INFO - root - 2017-12-05 20:48:53.673490: step 42710, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 76h:07m:54s remains)
INFO - root - 2017-12-05 20:49:02.999044: step 42720, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 72h:51m:00s remains)
INFO - root - 2017-12-05 20:49:12.476098: step 42730, loss = 2.03, batch loss = 1.98 (8.5 examples/sec; 0.946 sec/batch; 76h:08m:03s remains)
INFO - root - 2017-12-05 20:49:21.851684: step 42740, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 75h:59m:21s remains)
INFO - root - 2017-12-05 20:49:31.354694: step 42750, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 76h:19m:52s remains)
INFO - root - 2017-12-05 20:49:40.647539: step 42760, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 73h:17m:30s remains)
INFO - root - 2017-12-05 20:49:50.020999: step 42770, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 75h:58m:41s remains)
INFO - root - 2017-12-05 20:49:59.404634: step 42780, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 77h:50m:17s remains)
INFO - root - 2017-12-05 20:50:08.935848: step 42790, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 78h:29m:56s remains)
INFO - root - 2017-12-05 20:50:18.294580: step 42800, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 73h:33m:07s remains)
2017-12-05 20:50:18.987064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2162657 -4.221899 -4.2234483 -4.2140589 -4.1998038 -4.1802969 -4.1662703 -4.1654792 -4.170588 -4.1680293 -4.1635537 -4.1642075 -4.1681485 -4.177731 -4.1931963][-4.2358508 -4.2427673 -4.248652 -4.2437758 -4.2326264 -4.2143154 -4.1899176 -4.1657486 -4.1463165 -4.1231589 -4.1070051 -4.111659 -4.1245489 -4.1459856 -4.1748023][-4.2447939 -4.2508287 -4.2590537 -4.2578454 -4.2439332 -4.2191119 -4.178432 -4.1297297 -4.09315 -4.0570478 -4.0367241 -4.0529265 -4.080791 -4.1150885 -4.1510463][-4.2338376 -4.2392826 -4.248013 -4.2447534 -4.2196369 -4.181437 -4.1259236 -4.0634618 -4.0313044 -4.0118284 -4.0088511 -4.0458193 -4.0862303 -4.1172237 -4.1427875][-4.220943 -4.22367 -4.22781 -4.21953 -4.1853833 -4.1444016 -4.0916142 -4.0334668 -4.0189157 -4.0223379 -4.036212 -4.0804448 -4.1200218 -4.140933 -4.1515284][-4.2206292 -4.2149172 -4.211576 -4.1988773 -4.1676183 -4.1373739 -4.1023407 -4.0645127 -4.0635433 -4.0738888 -4.0867939 -4.1204147 -4.1492438 -4.1609392 -4.1631622][-4.2242661 -4.2065816 -4.1931524 -4.1789494 -4.1581335 -4.1453476 -4.134757 -4.1191068 -4.1218486 -4.1298647 -4.1383491 -4.1623635 -4.1808419 -4.185616 -4.1835866][-4.2249517 -4.1973882 -4.1796045 -4.1695848 -4.1644549 -4.1709557 -4.1776881 -4.1742382 -4.1738658 -4.1759577 -4.1796069 -4.1960278 -4.2087855 -4.2132249 -4.2113466][-4.2284226 -4.201005 -4.1897168 -4.189218 -4.1937695 -4.2080026 -4.2195024 -4.2185912 -4.2148161 -4.21549 -4.2183223 -4.230144 -4.2413945 -4.2455044 -4.242548][-4.2237153 -4.2074757 -4.2075887 -4.2146993 -4.2236204 -4.2379436 -4.2484136 -4.2489667 -4.24614 -4.2507691 -4.2550921 -4.2636142 -4.2718287 -4.2741151 -4.2689142][-4.2097216 -4.2102265 -4.2221313 -4.2368469 -4.2514977 -4.2670245 -4.2780128 -4.2819867 -4.2814345 -4.2877612 -4.2906079 -4.293036 -4.293643 -4.2929668 -4.2867279][-4.2083368 -4.2220926 -4.2430062 -4.2658019 -4.2871637 -4.3041158 -4.3146939 -4.3164392 -4.3120217 -4.3128781 -4.3117762 -4.3092442 -4.3039742 -4.3009415 -4.294292][-4.2200141 -4.2405696 -4.2648077 -4.2895975 -4.3125467 -4.3278522 -4.3349528 -4.3318081 -4.3229065 -4.3175855 -4.3143268 -4.3107123 -4.30563 -4.3023515 -4.2967949][-4.2255764 -4.2468739 -4.2712593 -4.2957673 -4.315455 -4.3276482 -4.3329296 -4.3287916 -4.3201828 -4.3131289 -4.3087554 -4.306129 -4.3016539 -4.2979827 -4.2930031][-4.2296238 -4.2460041 -4.26797 -4.2908788 -4.308434 -4.3205462 -4.3274083 -4.3266134 -4.3222408 -4.315999 -4.3106275 -4.3057003 -4.2998905 -4.2956629 -4.2905674]]...]
INFO - root - 2017-12-05 20:50:28.312562: step 42810, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 68h:39m:38s remains)
INFO - root - 2017-12-05 20:50:37.736857: step 42820, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 75h:29m:51s remains)
INFO - root - 2017-12-05 20:50:46.983166: step 42830, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.800 sec/batch; 64h:23m:48s remains)
INFO - root - 2017-12-05 20:50:56.240261: step 42840, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 77h:30m:30s remains)
INFO - root - 2017-12-05 20:51:05.335732: step 42850, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 78h:17m:22s remains)
INFO - root - 2017-12-05 20:51:14.637425: step 42860, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 75h:05m:07s remains)
INFO - root - 2017-12-05 20:51:24.242635: step 42870, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.974 sec/batch; 78h:19m:31s remains)
INFO - root - 2017-12-05 20:51:33.485442: step 42880, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 75h:58m:16s remains)
INFO - root - 2017-12-05 20:51:42.960441: step 42890, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 78h:25m:32s remains)
INFO - root - 2017-12-05 20:51:52.523991: step 42900, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 76h:28m:13s remains)
2017-12-05 20:51:53.231635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3503637 -4.3336792 -4.3215628 -4.3192153 -4.3221183 -4.3283868 -4.3350482 -4.3327026 -4.3232837 -4.3093977 -4.2969236 -4.2869415 -4.2848234 -4.2910886 -4.3012867][-4.3379884 -4.3105249 -4.2900558 -4.2852588 -4.294601 -4.3107986 -4.3185644 -4.31199 -4.2985005 -4.2831416 -4.2722135 -4.2659836 -4.2659945 -4.2765913 -4.2920632][-4.2953844 -4.2596993 -4.2401471 -4.2439981 -4.2638431 -4.2819886 -4.2804327 -4.2658415 -4.2532825 -4.2420015 -4.2337351 -4.2299619 -4.2326441 -4.2474818 -4.2668571][-4.2206836 -4.1859016 -4.1816134 -4.2034903 -4.2280149 -4.2332635 -4.2108164 -4.1861806 -4.1840229 -4.1903439 -4.1949658 -4.2007036 -4.2110558 -4.2287703 -4.2487259][-4.1267686 -4.1002169 -4.1159945 -4.1542616 -4.1752043 -4.158865 -4.1104 -4.0780649 -4.0936756 -4.1297626 -4.1595459 -4.1847849 -4.2064695 -4.2258883 -4.2415485][-4.032043 -4.0111103 -4.0417075 -4.0883293 -4.0991287 -4.0537858 -3.9669206 -3.92776 -3.9765337 -4.0544543 -4.12064 -4.1677942 -4.2006264 -4.22051 -4.2310672][-3.9858847 -3.9639688 -3.9955285 -4.0362048 -4.0263062 -3.9480026 -3.8321905 -3.8005376 -3.8819118 -3.9850397 -4.07095 -4.1315961 -4.1730056 -4.1949472 -4.2054787][-4.0215054 -3.9914162 -4.0104966 -4.03286 -4.0149121 -3.9481761 -3.8610587 -3.8396516 -3.8967724 -3.973721 -4.0480914 -4.1064167 -4.14985 -4.1737862 -4.1910119][-4.0859041 -4.0563145 -4.0693431 -4.0867548 -4.0860333 -4.0653524 -4.0246224 -3.9982104 -4.0051484 -4.0360022 -4.0844593 -4.1313105 -4.1680965 -4.1938229 -4.2197223][-4.1267376 -4.1095185 -4.1287827 -4.156652 -4.1762791 -4.1837487 -4.1687307 -4.1419816 -4.1213665 -4.1221189 -4.1511259 -4.1871543 -4.2154603 -4.2395067 -4.2658434][-4.148551 -4.1451945 -4.1743431 -4.2086864 -4.2350893 -4.2520537 -4.2514043 -4.2349072 -4.2116547 -4.1999979 -4.213388 -4.2393694 -4.2629046 -4.2838097 -4.30421][-4.1686287 -4.1718512 -4.200882 -4.2364016 -4.265204 -4.2850161 -4.2885346 -4.2779818 -4.2552447 -4.2408004 -4.2485614 -4.2690897 -4.2889714 -4.3034081 -4.3139906][-4.1952138 -4.1986136 -4.2198563 -4.2512393 -4.2795091 -4.2973127 -4.2974782 -4.2858357 -4.263916 -4.2515965 -4.2603927 -4.2787719 -4.2936935 -4.3006511 -4.3023262][-4.2289238 -4.2313738 -4.2423306 -4.2635961 -4.284986 -4.2980714 -4.2958727 -4.28592 -4.269485 -4.2628584 -4.2698112 -4.2803168 -4.2883558 -4.2907677 -4.2881293][-4.2629642 -4.2600703 -4.2587051 -4.2702518 -4.2876668 -4.2998724 -4.2999125 -4.2936554 -4.2847977 -4.2832894 -4.288106 -4.2896633 -4.2885051 -4.28541 -4.2815118]]...]
INFO - root - 2017-12-05 20:52:02.371688: step 42910, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 74h:46m:32s remains)
INFO - root - 2017-12-05 20:52:11.800458: step 42920, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 81h:30m:09s remains)
INFO - root - 2017-12-05 20:52:21.071803: step 42930, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 74h:43m:59s remains)
INFO - root - 2017-12-05 20:52:30.519221: step 42940, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 77h:46m:38s remains)
INFO - root - 2017-12-05 20:52:39.748125: step 42950, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.970 sec/batch; 78h:02m:10s remains)
INFO - root - 2017-12-05 20:52:48.996746: step 42960, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 75h:20m:24s remains)
INFO - root - 2017-12-05 20:52:58.253386: step 42970, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 71h:42m:32s remains)
INFO - root - 2017-12-05 20:53:07.441120: step 42980, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 69h:07m:09s remains)
INFO - root - 2017-12-05 20:53:16.723667: step 42990, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 78h:24m:40s remains)
INFO - root - 2017-12-05 20:53:26.044804: step 43000, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.003 sec/batch; 80h:40m:33s remains)
2017-12-05 20:53:26.782882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2203431 -4.2198868 -4.2169309 -4.2258267 -4.23333 -4.2229757 -4.180943 -4.1360168 -4.1156707 -4.1276112 -4.17218 -4.2302923 -4.2791109 -4.3103185 -4.3268075][-4.2327991 -4.2298989 -4.2317863 -4.2445483 -4.253902 -4.2445693 -4.2017927 -4.1460938 -4.1030416 -4.1015329 -4.1482491 -4.2154484 -4.2734365 -4.3100462 -4.3259287][-4.2590032 -4.2546959 -4.2555695 -4.2617717 -4.2614017 -4.244113 -4.1950026 -4.1253161 -4.0633826 -4.0597839 -4.1211672 -4.2035055 -4.2715168 -4.3117867 -4.3266058][-4.2980866 -4.2907481 -4.2826138 -4.276319 -4.2602181 -4.2310429 -4.1718435 -4.0849051 -4.0099869 -4.0139265 -4.0963812 -4.196557 -4.2713289 -4.3132491 -4.3278751][-4.3319955 -4.3232241 -4.3031015 -4.2854533 -4.2552533 -4.2106118 -4.1353116 -4.0330276 -3.9595773 -3.9822578 -4.0853443 -4.1977944 -4.2756319 -4.3154278 -4.3286428][-4.3428936 -4.3353524 -4.3070788 -4.2746472 -4.2313251 -4.1757846 -4.0857353 -3.9740157 -3.9169593 -3.9707112 -4.091507 -4.2061787 -4.2808175 -4.3164563 -4.329113][-4.3431277 -4.3375373 -4.3075695 -4.2668791 -4.2166104 -4.1565566 -4.0554247 -3.9384475 -3.8996572 -3.980464 -4.1090789 -4.2200127 -4.2876444 -4.3182235 -4.330677][-4.3363228 -4.3344054 -4.3112869 -4.27541 -4.2286158 -4.1685 -4.0614834 -3.9459162 -3.9176083 -4.0055809 -4.1265125 -4.2285371 -4.2903695 -4.319561 -4.3319044][-4.3156457 -4.3207579 -4.3110132 -4.2891431 -4.2526731 -4.1982908 -4.0960431 -3.9925528 -3.9701385 -4.0447626 -4.1463661 -4.2358241 -4.2922416 -4.3202748 -4.3325214][-4.2875166 -4.3036542 -4.3079576 -4.2996311 -4.27521 -4.2299657 -4.1400614 -4.0525217 -4.0318604 -4.0868139 -4.167325 -4.2428732 -4.2928567 -4.3193707 -4.3321128][-4.2585835 -4.2886138 -4.3082423 -4.3119268 -4.2992296 -4.2651181 -4.190145 -4.1178465 -4.0939813 -4.1313806 -4.1938977 -4.2567968 -4.2991138 -4.32214 -4.3343539][-4.2280746 -4.2759085 -4.3121934 -4.3284292 -4.3255644 -4.3004608 -4.2381454 -4.1775446 -4.1521664 -4.1770744 -4.2267561 -4.2768841 -4.3116565 -4.3314672 -4.3403168][-4.1945376 -4.2604647 -4.3117456 -4.33876 -4.34328 -4.3216963 -4.2678924 -4.2134953 -4.1909504 -4.2101746 -4.2516608 -4.2931552 -4.3248329 -4.3423381 -4.3468065][-4.1553979 -4.2383313 -4.3004417 -4.3357234 -4.3481874 -4.3324337 -4.2863412 -4.2345171 -4.2118325 -4.2253356 -4.2610373 -4.298799 -4.331131 -4.3477378 -4.3492494][-4.1071057 -4.206882 -4.2813034 -4.3250694 -4.3454261 -4.3352 -4.2955337 -4.2428279 -4.2155547 -4.2242942 -4.2592282 -4.2995577 -4.3330865 -4.3487678 -4.3494635]]...]
INFO - root - 2017-12-05 20:53:36.230476: step 43010, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 71h:29m:47s remains)
INFO - root - 2017-12-05 20:53:45.703661: step 43020, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 76h:27m:48s remains)
INFO - root - 2017-12-05 20:53:55.034556: step 43030, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 80h:13m:16s remains)
INFO - root - 2017-12-05 20:54:04.546825: step 43040, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 75h:46m:42s remains)
INFO - root - 2017-12-05 20:54:13.835439: step 43050, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 78h:42m:46s remains)
INFO - root - 2017-12-05 20:54:23.296987: step 43060, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.938 sec/batch; 75h:24m:26s remains)
INFO - root - 2017-12-05 20:54:32.751268: step 43070, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 74h:16m:13s remains)
INFO - root - 2017-12-05 20:54:41.985997: step 43080, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 75h:37m:29s remains)
INFO - root - 2017-12-05 20:54:51.433620: step 43090, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 75h:36m:08s remains)
INFO - root - 2017-12-05 20:55:00.734736: step 43100, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 73h:17m:27s remains)
2017-12-05 20:55:01.602380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1411324 -4.1241031 -4.1182551 -4.1278963 -4.16005 -4.1985955 -4.2264423 -4.2275791 -4.2189164 -4.2156124 -4.2179732 -4.2175088 -4.2276621 -4.2315292 -4.221324][-4.1798754 -4.1495543 -4.108129 -4.0768151 -4.0930963 -4.1336961 -4.1673775 -4.1749291 -4.1790161 -4.18827 -4.1996727 -4.2025747 -4.2143984 -4.2247558 -4.2181315][-4.2123442 -4.1870747 -4.1294441 -4.0589862 -4.0308576 -4.0470963 -4.0718565 -4.0847969 -4.1070609 -4.1351042 -4.1554761 -4.1727643 -4.1920629 -4.2036023 -4.1917996][-4.2209821 -4.2118368 -4.1611414 -4.0777373 -4.0045238 -3.9653959 -3.9604034 -3.9753609 -4.0242 -4.070827 -4.0994973 -4.1311398 -4.1680026 -4.1900554 -4.177165][-4.2098894 -4.2219429 -4.1873574 -4.1133394 -4.0234103 -3.9406447 -3.8852038 -3.8878405 -3.9528093 -4.0173578 -4.0531907 -4.098938 -4.1549706 -4.193593 -4.1893916][-4.1978478 -4.2208271 -4.1993766 -4.141552 -4.067956 -3.982481 -3.8999817 -3.8747971 -3.9325507 -3.9934011 -4.0331459 -4.0876532 -4.1532192 -4.2055178 -4.2117376][-4.1982079 -4.2219605 -4.2047658 -4.165205 -4.1206579 -4.0572448 -3.983741 -3.9416099 -3.9651849 -3.9942687 -4.0295053 -4.0865479 -4.1524048 -4.2093539 -4.2270079][-4.2144666 -4.2319322 -4.2128677 -4.1821971 -4.1615033 -4.1262627 -4.0789881 -4.0365338 -4.0274458 -4.0215993 -4.0481496 -4.1009641 -4.1589251 -4.2104473 -4.2345467][-4.2387104 -4.24663 -4.2274318 -4.2019076 -4.190146 -4.1744547 -4.15034 -4.1110973 -4.0828242 -4.0543942 -4.064539 -4.1051354 -4.1572566 -4.2044754 -4.2310982][-4.2511435 -4.2512522 -4.2367473 -4.21514 -4.2040796 -4.1957417 -4.182054 -4.1472163 -4.1129675 -4.0669293 -4.0521774 -4.0798421 -4.1312857 -4.182014 -4.2144608][-4.2583251 -4.25522 -4.2458844 -4.2304564 -4.2205553 -4.2121167 -4.19748 -4.1666689 -4.1325016 -4.0763206 -4.0309539 -4.0401897 -4.0922642 -4.1514721 -4.1894512][-4.2740383 -4.2718072 -4.269639 -4.258224 -4.2438931 -4.2305737 -4.21318 -4.1879921 -4.153583 -4.0901947 -4.0227509 -4.0090761 -4.0498147 -4.1150889 -4.1590009][-4.2990174 -4.2965417 -4.2970338 -4.2861977 -4.2655587 -4.2509065 -4.2360559 -4.2202148 -4.1889968 -4.118619 -4.0430603 -4.0158448 -4.0409279 -4.0970135 -4.13825][-4.3140192 -4.3098412 -4.3082595 -4.2954335 -4.2730737 -4.2609859 -4.2512889 -4.2484655 -4.22504 -4.15763 -4.0878186 -4.0654593 -4.075314 -4.1060629 -4.1311045][-4.3179317 -4.3117352 -4.3029485 -4.2836 -4.2610164 -4.2559667 -4.2542109 -4.2586036 -4.2398849 -4.182724 -4.1317248 -4.1176324 -4.1196747 -4.1266451 -4.1340508]]...]
INFO - root - 2017-12-05 20:55:10.937154: step 43110, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 74h:38m:38s remains)
INFO - root - 2017-12-05 20:55:20.078277: step 43120, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 78h:41m:27s remains)
INFO - root - 2017-12-05 20:55:29.266878: step 43130, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 74h:23m:30s remains)
INFO - root - 2017-12-05 20:55:38.686735: step 43140, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 73h:59m:47s remains)
INFO - root - 2017-12-05 20:55:47.870365: step 43150, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 70h:30m:49s remains)
INFO - root - 2017-12-05 20:55:57.282673: step 43160, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 77h:11m:55s remains)
INFO - root - 2017-12-05 20:56:06.538647: step 43170, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 77h:23m:11s remains)
INFO - root - 2017-12-05 20:56:15.834661: step 43180, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 76h:56m:18s remains)
INFO - root - 2017-12-05 20:56:25.166634: step 43190, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.985 sec/batch; 79h:11m:09s remains)
INFO - root - 2017-12-05 20:56:34.653393: step 43200, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 74h:33m:31s remains)
2017-12-05 20:56:35.433446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1577497 -4.1333594 -4.10524 -4.0829573 -4.071588 -4.0776763 -4.10205 -4.1240964 -4.1261592 -4.12403 -4.12318 -4.1165767 -4.1061993 -4.0978584 -4.0977392][-4.1558695 -4.1428542 -4.1295757 -4.1158018 -4.1033578 -4.0985126 -4.1060176 -4.1157885 -4.1080627 -4.0968404 -4.0883737 -4.0827274 -4.0814786 -4.0886955 -4.1038146][-4.146884 -4.1442232 -4.1398916 -4.1271424 -4.1104784 -4.092267 -4.0828123 -4.0839076 -4.0766358 -4.0685668 -4.0610595 -4.0558252 -4.06004 -4.0752625 -4.0990734][-4.1411915 -4.1482344 -4.1496682 -4.1324782 -4.1081753 -4.0731082 -4.0456219 -4.038744 -4.0368834 -4.0415087 -4.0440469 -4.038496 -4.038074 -4.0541883 -4.0827856][-4.1336884 -4.1528883 -4.159348 -4.1398525 -4.102828 -4.0507393 -4.0081172 -3.9944112 -3.9992409 -4.0200648 -4.0413356 -4.042345 -4.0371394 -4.0530128 -4.0876594][-4.1283846 -4.1586814 -4.1682816 -4.1432862 -4.088973 -4.0201464 -3.9659088 -3.9488113 -3.9631715 -4.0057054 -4.0520411 -4.06746 -4.0634723 -4.0787544 -4.1140776][-4.1336 -4.1720233 -4.1835327 -4.1519365 -4.0782585 -3.9889197 -3.9220476 -3.9039569 -3.9294999 -3.9950573 -4.0674133 -4.1011853 -4.106173 -4.1206059 -4.1495652][-4.1441154 -4.1878133 -4.1998777 -4.1592474 -4.0674996 -3.960041 -3.88096 -3.8614717 -3.8951044 -3.9782276 -4.0712519 -4.1212964 -4.1371098 -4.1529827 -4.1752038][-4.1534004 -4.1980462 -4.2086668 -4.1614375 -4.0609674 -3.9478512 -3.8665986 -3.8477008 -3.884908 -3.9709949 -4.06898 -4.1277847 -4.1523867 -4.1696291 -4.1856475][-4.1610146 -4.1992831 -4.2053022 -4.1564474 -4.0587606 -3.9517937 -3.878026 -3.8629622 -3.9015632 -3.9825764 -4.0738235 -4.1333585 -4.1614423 -4.1761265 -4.1838627][-4.1700559 -4.1983051 -4.2004623 -4.1584625 -4.0743251 -3.983233 -3.9232426 -3.9114375 -3.9452004 -4.013864 -4.0906534 -4.1432447 -4.1699953 -4.1808467 -4.1816616][-4.181221 -4.1985273 -4.1972275 -4.1654205 -4.1029215 -4.0345311 -3.9898798 -3.9814036 -4.0061126 -4.0571003 -4.1164308 -4.159112 -4.1801667 -4.1867313 -4.1836224][-4.1945348 -4.2016072 -4.1978321 -4.1781244 -4.1403437 -4.09717 -4.0665565 -4.0610294 -4.0756135 -4.1060486 -4.1452689 -4.1767368 -4.1901994 -4.19203 -4.1897459][-4.2128978 -4.2114115 -4.2052174 -4.1945868 -4.1783419 -4.1575336 -4.139081 -4.1366267 -4.1444807 -4.1582613 -4.1790252 -4.1980443 -4.203054 -4.1994104 -4.1976876][-4.2327185 -4.2230439 -4.213181 -4.2065964 -4.2032118 -4.199708 -4.1917877 -4.1921306 -4.1981988 -4.2017641 -4.2087479 -4.21693 -4.2158155 -4.2097683 -4.2071362]]...]
INFO - root - 2017-12-05 20:56:44.666322: step 43210, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 72h:30m:47s remains)
INFO - root - 2017-12-05 20:56:53.963148: step 43220, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.819 sec/batch; 65h:49m:21s remains)
INFO - root - 2017-12-05 20:57:03.373191: step 43230, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 77h:45m:56s remains)
INFO - root - 2017-12-05 20:57:12.836333: step 43240, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 77h:18m:23s remains)
INFO - root - 2017-12-05 20:57:22.184420: step 43250, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 76h:00m:09s remains)
INFO - root - 2017-12-05 20:57:31.575956: step 43260, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.970 sec/batch; 77h:54m:28s remains)
INFO - root - 2017-12-05 20:57:40.959659: step 43270, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 77h:14m:43s remains)
INFO - root - 2017-12-05 20:57:50.388886: step 43280, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 77h:40m:46s remains)
INFO - root - 2017-12-05 20:57:59.645194: step 43290, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 70h:48m:10s remains)
INFO - root - 2017-12-05 20:58:08.790520: step 43300, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.816 sec/batch; 65h:35m:30s remains)
2017-12-05 20:58:09.596756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2455406 -4.2493324 -4.252203 -4.2511597 -4.2473965 -4.2427087 -4.2409797 -4.2413616 -4.240664 -4.2381291 -4.2345228 -4.2321353 -4.2348514 -4.2415175 -4.2477317][-4.2527118 -4.2560554 -4.26051 -4.263525 -4.264873 -4.263917 -4.2633638 -4.2630496 -4.2613277 -4.2578883 -4.2541366 -4.2505684 -4.2502022 -4.2527142 -4.254828][-4.250011 -4.2511396 -4.25517 -4.2594428 -4.2632356 -4.2649565 -4.266283 -4.2677011 -4.2673512 -4.2656941 -4.265079 -4.2638054 -4.2630839 -4.2608008 -4.2549524][-4.2480659 -4.2497554 -4.2542734 -4.2578192 -4.2598696 -4.2589917 -4.2582126 -4.2589645 -4.26071 -4.2618809 -4.265357 -4.2676535 -4.2678881 -4.2618623 -4.2471123][-4.2398028 -4.2430553 -4.2454472 -4.2449579 -4.2399106 -4.2306643 -4.2228994 -4.2224541 -4.22995 -4.2368879 -4.2433219 -4.2472582 -4.2481685 -4.2376194 -4.2123504][-4.174685 -4.1797 -4.1794453 -4.17332 -4.1613317 -4.1419806 -4.1261721 -4.128705 -4.1465144 -4.1641183 -4.1782522 -4.1881433 -4.1913543 -4.1766295 -4.1432552][-4.0514011 -4.0592895 -4.0623775 -4.0560164 -4.0384989 -4.0062237 -3.9786968 -3.9817498 -4.0106812 -4.0438976 -4.0723877 -4.0940371 -4.1039124 -4.0897384 -4.0538816][-3.9788945 -3.9889333 -3.9978991 -3.9978356 -3.9788983 -3.9395759 -3.9055297 -3.9072065 -3.9391806 -3.9795263 -4.0156651 -4.0448747 -4.0599136 -4.0499029 -4.0196991][-4.0579815 -4.07052 -4.0828204 -4.0874572 -4.073586 -4.0419645 -4.0181756 -4.0248818 -4.0499077 -4.0762634 -4.0960178 -4.1102509 -4.1115055 -4.0951633 -4.0685024][-4.1684456 -4.1858211 -4.2002149 -4.2067285 -4.1978865 -4.1743741 -4.1584249 -4.1634312 -4.1770873 -4.1869621 -4.1891541 -4.1859646 -4.172708 -4.1493816 -4.1239114][-4.2470994 -4.2644448 -4.2767324 -4.2821455 -4.2770262 -4.2625475 -4.2529769 -4.25373 -4.2582726 -4.2587786 -4.2538023 -4.2439222 -4.226316 -4.2032819 -4.1833882][-4.2931867 -4.3045306 -4.3135653 -4.3185158 -4.3160615 -4.3074183 -4.3006449 -4.2970729 -4.294693 -4.2912288 -4.2857637 -4.2782011 -4.26596 -4.2519522 -4.243196][-4.3104224 -4.3122787 -4.316864 -4.321218 -4.3205318 -4.3168349 -4.3146186 -4.3131475 -4.3118725 -4.311172 -4.3090205 -4.3061867 -4.3012252 -4.2954082 -4.293179][-4.2910833 -4.2822089 -4.2804241 -4.2830648 -4.2850614 -4.2885604 -4.2930956 -4.2979331 -4.3026624 -4.3082967 -4.3127513 -4.3155508 -4.3165293 -4.3153019 -4.3149257][-4.2660122 -4.2466679 -4.237195 -4.2374144 -4.2437682 -4.2564683 -4.270206 -4.2822056 -4.2933655 -4.3054538 -4.3159389 -4.3232179 -4.3280783 -4.3301067 -4.3308682]]...]
INFO - root - 2017-12-05 20:58:18.793435: step 43310, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.918 sec/batch; 73h:46m:20s remains)
INFO - root - 2017-12-05 20:58:27.993248: step 43320, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 73h:07m:19s remains)
INFO - root - 2017-12-05 20:58:37.351183: step 43330, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 74h:47m:34s remains)
INFO - root - 2017-12-05 20:58:46.712166: step 43340, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 75h:08m:07s remains)
INFO - root - 2017-12-05 20:58:55.967969: step 43350, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 74h:38m:58s remains)
INFO - root - 2017-12-05 20:59:05.195100: step 43360, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 75h:02m:41s remains)
INFO - root - 2017-12-05 20:59:14.596444: step 43370, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.945 sec/batch; 75h:55m:40s remains)
INFO - root - 2017-12-05 20:59:24.014264: step 43380, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 71h:57m:55s remains)
INFO - root - 2017-12-05 20:59:33.432873: step 43390, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 74h:36m:25s remains)
INFO - root - 2017-12-05 20:59:42.631961: step 43400, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 71h:11m:14s remains)
2017-12-05 20:59:43.406015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.05669 -4.035634 -4.0831275 -4.1382833 -4.1418591 -4.1238241 -4.1074257 -4.1160331 -4.1229777 -4.1115265 -4.0959768 -4.0768385 -4.0815639 -4.1260481 -4.1485586][-4.0596004 -4.0332541 -4.0867472 -4.147378 -4.1472092 -4.12152 -4.1069269 -4.1276278 -4.1370645 -4.1142731 -4.0793734 -4.0436816 -4.057025 -4.1168575 -4.1423697][-4.0677986 -4.0397534 -4.094727 -4.1524391 -4.1430707 -4.1064248 -4.0893989 -4.1226568 -4.1425934 -4.1226964 -4.0799437 -4.0320206 -4.0506196 -4.1161056 -4.1407108][-4.0663466 -4.0396676 -4.0983357 -4.1477122 -4.1189251 -4.0601535 -4.0415878 -4.0962362 -4.1370859 -4.1318922 -4.0929027 -4.0427656 -4.0644007 -4.1242352 -4.1416292][-4.0457892 -4.0255647 -4.0876741 -4.1282911 -4.0742054 -3.9849977 -3.9687762 -4.0540972 -4.1215634 -4.1404657 -4.1125946 -4.0700459 -4.0909014 -4.13623 -4.1437187][-4.01058 -3.9986048 -4.0639849 -4.0979128 -4.0211511 -3.8983014 -3.8848433 -4.0022888 -4.0893583 -4.1299038 -4.1194949 -4.0902762 -4.1071281 -4.134038 -4.1318269][-3.9932537 -3.9835737 -4.0497737 -4.0795922 -3.9868507 -3.8324785 -3.8149786 -3.9510813 -4.0452838 -4.0919123 -4.0891047 -4.0705738 -4.0859418 -4.1045456 -4.0992918][-4.01552 -4.0017056 -4.0632687 -4.0865397 -3.9935317 -3.8365941 -3.8227634 -3.9477296 -4.0297613 -4.062789 -4.0498805 -4.0272493 -4.0428424 -4.0627103 -4.0596638][-4.0618749 -4.0412788 -4.0961828 -4.1169906 -4.0413532 -3.9197905 -3.9202259 -4.0127387 -4.0656991 -4.0762696 -4.042357 -4.011188 -4.0298834 -4.0476241 -4.0447226][-4.1107 -4.084569 -4.1315379 -4.1491323 -4.0849729 -3.9989204 -4.0186081 -4.093122 -4.1266208 -4.12189 -4.0724239 -4.0417261 -4.0632911 -4.0727496 -4.0681243][-4.1438637 -4.1180196 -4.1588016 -4.1690927 -4.1116381 -4.0498223 -4.0822973 -4.152142 -4.1831074 -4.17545 -4.1254535 -4.1026063 -4.122345 -4.121839 -4.1104412][-4.1528196 -4.1271286 -4.1662154 -4.1795073 -4.1382456 -4.0927143 -4.1238074 -4.1876817 -4.2197471 -4.2151847 -4.1736283 -4.1550241 -4.1664219 -4.1573954 -4.1344228][-4.1488853 -4.1264982 -4.1620164 -4.1772738 -4.1506729 -4.1207871 -4.1525116 -4.205626 -4.2300973 -4.2275715 -4.1933 -4.1714954 -4.1711283 -4.1542912 -4.1221752][-4.14208 -4.1271725 -4.1570292 -4.1687741 -4.1504211 -4.1355553 -4.1732421 -4.2178106 -4.2331309 -4.2242594 -4.1866331 -4.1553531 -4.1427145 -4.1243019 -4.0993257][-4.147737 -4.1416235 -4.161696 -4.1674061 -4.1539149 -4.1467943 -4.1820321 -4.2174754 -4.2238383 -4.2041063 -4.1595044 -4.1200476 -4.1021891 -4.0905371 -4.0845442]]...]
INFO - root - 2017-12-05 20:59:52.423969: step 43410, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 68h:08m:20s remains)
INFO - root - 2017-12-05 21:00:01.675057: step 43420, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 79h:27m:09s remains)
INFO - root - 2017-12-05 21:00:10.972814: step 43430, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 78h:02m:32s remains)
INFO - root - 2017-12-05 21:00:20.290355: step 43440, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 75h:54m:59s remains)
INFO - root - 2017-12-05 21:00:29.662679: step 43450, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 77h:11m:31s remains)
INFO - root - 2017-12-05 21:00:39.044844: step 43460, loss = 2.06, batch loss = 2.00 (7.6 examples/sec; 1.050 sec/batch; 84h:19m:51s remains)
INFO - root - 2017-12-05 21:00:48.473342: step 43470, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 78h:42m:14s remains)
INFO - root - 2017-12-05 21:00:57.945059: step 43480, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 72h:03m:18s remains)
INFO - root - 2017-12-05 21:01:07.369666: step 43490, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 74h:46m:51s remains)
INFO - root - 2017-12-05 21:01:16.712720: step 43500, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 73h:00m:14s remains)
2017-12-05 21:01:17.488753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3242702 -4.3136106 -4.2994671 -4.2861261 -4.2750731 -4.2686458 -4.2655468 -4.2592931 -4.2533054 -4.2566428 -4.2717371 -4.2913041 -4.308167 -4.32252 -4.3323779][-4.3171725 -4.3020573 -4.2857885 -4.2733793 -4.2626309 -4.2558656 -4.251586 -4.2415915 -4.2328138 -4.236412 -4.2592463 -4.2857008 -4.3040695 -4.3180962 -4.3280144][-4.3085308 -4.2898717 -4.2750258 -4.2640357 -4.2503295 -4.2385092 -4.2313347 -4.2191215 -4.2142124 -4.2235508 -4.2562122 -4.289803 -4.31034 -4.3221455 -4.3278637][-4.296062 -4.2722907 -4.2575836 -4.245821 -4.2292638 -4.2135615 -4.2028995 -4.190238 -4.1904631 -4.2054753 -4.2439537 -4.28538 -4.3129725 -4.3253946 -4.3275566][-4.2721281 -4.2398777 -4.2218003 -4.2113919 -4.1991005 -4.1808882 -4.1594567 -4.1357379 -4.1348953 -4.1618118 -4.2116852 -4.2591467 -4.29526 -4.3136024 -4.3173442][-4.2290916 -4.1863904 -4.162652 -4.1597834 -4.1575403 -4.1325727 -4.0833974 -4.0264311 -4.0195007 -4.0739503 -4.1493168 -4.2070656 -4.2541175 -4.286932 -4.3011374][-4.1779561 -4.1245465 -4.0956607 -4.1014733 -4.1042047 -4.0650411 -3.9679737 -3.8539643 -3.8428125 -3.9494076 -4.0700865 -4.1502948 -4.2105894 -4.2593207 -4.2877164][-4.1361041 -4.0824714 -4.0579305 -4.0652671 -4.0608082 -3.9991009 -3.8424873 -3.6568365 -3.6541235 -3.8331676 -4.0034771 -4.1079311 -4.1806812 -4.2390342 -4.2767606][-4.1313219 -4.0921659 -4.07928 -4.0799766 -4.0607548 -3.9862859 -3.8160605 -3.6269827 -3.6514707 -3.8440919 -4.0151157 -4.1165056 -4.1821384 -4.2352085 -4.2723575][-4.1660533 -4.1458459 -4.1414576 -4.1372013 -4.112402 -4.0554752 -3.9390397 -3.8291314 -3.8579602 -3.9821861 -4.1005011 -4.1735368 -4.2203274 -4.2563243 -4.2826543][-4.2189617 -4.213675 -4.2135425 -4.2082143 -4.1871853 -4.1519752 -4.0871348 -4.0326614 -4.0502486 -4.1188779 -4.1905212 -4.2370119 -4.264379 -4.2846494 -4.2994862][-4.2798409 -4.28347 -4.2827978 -4.2772555 -4.2605677 -4.2387094 -4.2013106 -4.1685033 -4.1760578 -4.21741 -4.26453 -4.2930713 -4.30658 -4.3145628 -4.3193259][-4.3230553 -4.3302045 -4.3285589 -4.3228941 -4.3128886 -4.3023663 -4.2823486 -4.2610393 -4.2634339 -4.2927585 -4.3247228 -4.3407326 -4.3450427 -4.3444142 -4.3416243][-4.3394942 -4.3439674 -4.3417029 -4.3377914 -4.3338552 -4.3329139 -4.3272371 -4.3165812 -4.3175216 -4.3354192 -4.3521042 -4.3583493 -4.3596692 -4.3566666 -4.3528824][-4.3388762 -4.3379946 -4.3334961 -4.3296113 -4.3279572 -4.3293705 -4.3291316 -4.3265982 -4.32892 -4.3352218 -4.3412337 -4.3467817 -4.3514156 -4.3527493 -4.3514624]]...]
INFO - root - 2017-12-05 21:01:26.899602: step 43510, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 74h:54m:51s remains)
INFO - root - 2017-12-05 21:01:36.105326: step 43520, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 73h:21m:13s remains)
INFO - root - 2017-12-05 21:01:45.308206: step 43530, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.776 sec/batch; 62h:16m:54s remains)
INFO - root - 2017-12-05 21:01:54.790248: step 43540, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 77h:27m:21s remains)
INFO - root - 2017-12-05 21:02:04.121439: step 43550, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 76h:30m:21s remains)
INFO - root - 2017-12-05 21:02:13.559814: step 43560, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.917 sec/batch; 73h:34m:48s remains)
INFO - root - 2017-12-05 21:02:22.861003: step 43570, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 76h:45m:51s remains)
INFO - root - 2017-12-05 21:02:32.289547: step 43580, loss = 2.02, batch loss = 1.97 (8.6 examples/sec; 0.932 sec/batch; 74h:48m:26s remains)
INFO - root - 2017-12-05 21:02:41.638533: step 43590, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.919 sec/batch; 73h:43m:52s remains)
INFO - root - 2017-12-05 21:02:50.796509: step 43600, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.979 sec/batch; 78h:33m:34s remains)
2017-12-05 21:02:51.561915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2441797 -4.2374735 -4.2317748 -4.223074 -4.2210655 -4.2271171 -4.2396679 -4.2538848 -4.2687678 -4.2779622 -4.2793193 -4.277266 -4.2734876 -4.260181 -4.23782][-4.235446 -4.2298 -4.2250843 -4.2203455 -4.224503 -4.234695 -4.2436352 -4.2537842 -4.2644744 -4.2703004 -4.2716136 -4.2721825 -4.2717438 -4.2635169 -4.2445235][-4.1956992 -4.18952 -4.19315 -4.202229 -4.2197433 -4.2367043 -4.2441311 -4.2515411 -4.2564039 -4.2547374 -4.2518754 -4.2535758 -4.2560964 -4.2522283 -4.2398744][-4.1530194 -4.1460972 -4.1613169 -4.188693 -4.2170086 -4.2345471 -4.2393265 -4.2447472 -4.2462077 -4.2409744 -4.2360926 -4.23943 -4.2456565 -4.244513 -4.2329006][-4.12607 -4.1149659 -4.1336432 -4.1672859 -4.1931405 -4.2032356 -4.2019849 -4.205 -4.2073426 -4.2049985 -4.2014527 -4.2043176 -4.21151 -4.210371 -4.19819][-4.0906806 -4.0776234 -4.0910759 -4.11182 -4.12189 -4.1165676 -4.104486 -4.1058927 -4.1142492 -4.11877 -4.1176524 -4.1172662 -4.1225109 -4.1242881 -4.1168962][-4.0511103 -4.0431204 -4.0472164 -4.0488119 -4.0399928 -4.0156026 -3.9897263 -3.9877863 -4.0025568 -4.0130911 -4.0116167 -4.0051551 -4.0064511 -4.0134711 -4.0173807][-4.0437503 -4.0402694 -4.0418816 -4.038022 -4.0248203 -4.0004129 -3.9765856 -3.9736407 -3.9899418 -4.0021429 -4.0000281 -3.9854207 -3.9752343 -3.9779804 -3.9886754][-4.0803347 -4.0782866 -4.0832229 -4.0858321 -4.0834064 -4.0754762 -4.0640693 -4.0603924 -4.0687127 -4.0748057 -4.0713778 -4.0568933 -4.0401268 -4.0347724 -4.0422273][-4.1368451 -4.1330252 -4.13633 -4.1376648 -4.1397376 -4.1440358 -4.1444516 -4.1435285 -4.1464267 -4.1485095 -4.1469131 -4.1399779 -4.1266541 -4.1159081 -4.1145473][-4.1751618 -4.1692491 -4.1694098 -4.1677136 -4.1710634 -4.1807375 -4.186584 -4.1864181 -4.1866546 -4.1878743 -4.189836 -4.1920333 -4.1874371 -4.1775155 -4.1712694][-4.2082505 -4.2026634 -4.1996646 -4.1962419 -4.196754 -4.2047019 -4.2108421 -4.2109838 -4.210907 -4.2120953 -4.2149057 -4.2211127 -4.2226849 -4.2156563 -4.2075043][-4.2306175 -4.2305207 -4.2309594 -4.2286782 -4.227612 -4.231369 -4.2358017 -4.2367721 -4.2371106 -4.2372189 -4.237525 -4.2421784 -4.2457318 -4.2415609 -4.2312584][-4.2596126 -4.2614827 -4.264297 -4.2645211 -4.264658 -4.26775 -4.2725325 -4.2744694 -4.2736573 -4.2705836 -4.267293 -4.2677979 -4.2706256 -4.2663503 -4.2526894][-4.2901211 -4.2931476 -4.2953043 -4.2957048 -4.2957997 -4.2980542 -4.301898 -4.3030033 -4.3005958 -4.2958665 -4.291378 -4.2907052 -4.2930942 -4.2878704 -4.2713027]]...]
INFO - root - 2017-12-05 21:03:00.783604: step 43610, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 76h:14m:26s remains)
INFO - root - 2017-12-05 21:03:10.240890: step 43620, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 76h:46m:14s remains)
INFO - root - 2017-12-05 21:03:19.470797: step 43630, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 77h:02m:39s remains)
INFO - root - 2017-12-05 21:03:28.982559: step 43640, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 76h:51m:04s remains)
INFO - root - 2017-12-05 21:03:38.339196: step 43650, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 73h:35m:34s remains)
INFO - root - 2017-12-05 21:03:47.689653: step 43660, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 77h:56m:19s remains)
INFO - root - 2017-12-05 21:03:56.642810: step 43670, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 72h:34m:00s remains)
INFO - root - 2017-12-05 21:04:06.277026: step 43680, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 79h:04m:54s remains)
INFO - root - 2017-12-05 21:04:15.556499: step 43690, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 68h:43m:16s remains)
INFO - root - 2017-12-05 21:04:24.865256: step 43700, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 75h:07m:43s remains)
2017-12-05 21:04:25.686024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2808628 -4.2802658 -4.2809577 -4.2831883 -4.2828746 -4.2796345 -4.2684541 -4.254847 -4.2476077 -4.2353435 -4.2166061 -4.1965175 -4.1810012 -4.1657181 -4.1503854][-4.2690582 -4.2621994 -4.2542667 -4.2528267 -4.2542586 -4.2507973 -4.2352681 -4.2129354 -4.1995163 -4.1855264 -4.1640406 -4.1432519 -4.1316319 -4.12012 -4.1075449][-4.2314119 -4.2183533 -4.2074857 -4.2010994 -4.196878 -4.1923404 -4.1778789 -4.1525149 -4.1312237 -4.1184654 -4.1101027 -4.1026454 -4.0951662 -4.0788527 -4.0667272][-4.1789379 -4.1697125 -4.1602621 -4.1501832 -4.1386805 -4.1250095 -4.1032434 -4.0742598 -4.054925 -4.0566125 -4.0734315 -4.0828567 -4.0761914 -4.0578737 -4.0439291][-4.1416321 -4.140512 -4.1303544 -4.1131134 -4.0897722 -4.0615072 -4.0162745 -3.9737105 -3.9707022 -4.0022874 -4.0442848 -4.0685043 -4.0717587 -4.0651693 -4.0563183][-4.1209092 -4.125742 -4.1106825 -4.0819697 -4.0412669 -3.9842281 -3.8955278 -3.8300138 -3.8653226 -3.9416251 -4.0078788 -4.0475068 -4.0665441 -4.0757327 -4.069304][-4.0855241 -4.0950227 -4.0765495 -4.0395641 -3.9817913 -3.8900867 -3.7486789 -3.6543489 -3.7464638 -3.8720095 -3.9486792 -3.9909933 -4.0207853 -4.0471854 -4.0458274][-4.056066 -4.0706959 -4.0512733 -4.0095944 -3.9426007 -3.8423269 -3.6994066 -3.621938 -3.7387371 -3.8589334 -3.9188731 -3.9532146 -3.988662 -4.0228233 -4.02564][-4.0813351 -4.0912271 -4.0662785 -4.0225511 -3.9709821 -3.908906 -3.8300803 -3.8018234 -3.8733766 -3.9368706 -3.9683535 -3.9978573 -4.0319271 -4.0554433 -4.0504217][-4.1415415 -4.1353784 -4.1015005 -4.0666733 -4.0421681 -4.01288 -3.9778967 -3.9768958 -4.0099511 -4.0299025 -4.04181 -4.0648069 -4.0889573 -4.1004434 -4.0869226][-4.2153735 -4.1974649 -4.1610279 -4.1347985 -4.124836 -4.1080813 -4.0865016 -4.0966783 -4.1110673 -4.1099238 -4.1133938 -4.1285563 -4.1458039 -4.1510081 -4.1390886][-4.2853894 -4.268559 -4.2324166 -4.206409 -4.2045293 -4.1983714 -4.1847749 -4.1955533 -4.2031269 -4.1963682 -4.1918526 -4.1951208 -4.2043543 -4.2120037 -4.2116027][-4.3386469 -4.3280287 -4.2981687 -4.2774119 -4.27853 -4.2764139 -4.2694669 -4.2795343 -4.2857924 -4.27989 -4.2689462 -4.26256 -4.264534 -4.2726674 -4.2786894][-4.3693261 -4.3662519 -4.3476639 -4.333961 -4.3349919 -4.3307843 -4.3261061 -4.3365993 -4.3423223 -4.3359237 -4.322474 -4.312942 -4.3090262 -4.3134308 -4.319303][-4.3624105 -4.3618531 -4.348526 -4.3382373 -4.3395476 -4.3338151 -4.3317342 -4.3432403 -4.3492851 -4.3440123 -4.3334155 -4.3242011 -4.3159127 -4.3149047 -4.318305]]...]
INFO - root - 2017-12-05 21:04:34.939781: step 43710, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.965 sec/batch; 77h:24m:45s remains)
INFO - root - 2017-12-05 21:04:44.145387: step 43720, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 73h:56m:57s remains)
INFO - root - 2017-12-05 21:04:53.604667: step 43730, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.878 sec/batch; 70h:23m:51s remains)
INFO - root - 2017-12-05 21:05:03.034697: step 43740, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 72h:19m:42s remains)
INFO - root - 2017-12-05 21:05:12.339198: step 43750, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.964 sec/batch; 77h:19m:38s remains)
INFO - root - 2017-12-05 21:05:21.766013: step 43760, loss = 2.10, batch loss = 2.05 (8.5 examples/sec; 0.942 sec/batch; 75h:31m:56s remains)
INFO - root - 2017-12-05 21:05:31.149964: step 43770, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 78h:00m:05s remains)
INFO - root - 2017-12-05 21:05:40.568054: step 43780, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 77h:15m:04s remains)
INFO - root - 2017-12-05 21:05:49.763805: step 43790, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.915 sec/batch; 73h:20m:46s remains)
INFO - root - 2017-12-05 21:05:58.917064: step 43800, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 67h:51m:18s remains)
2017-12-05 21:05:59.670776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2884803 -4.2964778 -4.3031731 -4.302237 -4.3023367 -4.3027778 -4.301991 -4.3003845 -4.2973642 -4.2948713 -4.295228 -4.2976494 -4.2961431 -4.2819037 -4.2634163][-4.3097234 -4.3168011 -4.3227797 -4.3207045 -4.3186612 -4.316124 -4.3132472 -4.3094625 -4.3028235 -4.2963014 -4.2942519 -4.2967067 -4.2944331 -4.2788944 -4.2552595][-4.3240232 -4.3295965 -4.3332334 -4.3286681 -4.3235822 -4.31775 -4.312417 -4.3090854 -4.3030944 -4.2946024 -4.2902832 -4.2939873 -4.2954416 -4.2842145 -4.2635384][-4.3326297 -4.3373241 -4.3387985 -4.3309178 -4.3214512 -4.3102889 -4.2983742 -4.2936282 -4.2863297 -4.2738733 -4.2682247 -4.273838 -4.2845488 -4.2887473 -4.277844][-4.3288207 -4.3322606 -4.3292446 -4.317832 -4.3064618 -4.2889128 -4.2673583 -4.2582493 -4.2474761 -4.2318015 -4.2268624 -4.2366457 -4.2570167 -4.2811737 -4.2840676][-4.3179412 -4.3194885 -4.3107133 -4.29674 -4.2860742 -4.2637243 -4.2323551 -4.211679 -4.1923842 -4.1734567 -4.1704574 -4.1834674 -4.2118955 -4.2489862 -4.2591195][-4.3035817 -4.2995644 -4.2816148 -4.2642169 -4.2506127 -4.2176876 -4.1733 -4.1364689 -4.1110778 -4.0924606 -4.09672 -4.1176596 -4.1527162 -4.1968055 -4.2124825][-4.275506 -4.26301 -4.2380171 -4.21505 -4.1914744 -4.1409492 -4.0732908 -4.0152025 -3.9868569 -3.9813626 -4.0104623 -4.0496645 -4.0923033 -4.1426082 -4.164732][-4.2427135 -4.2272243 -4.1999812 -4.173286 -4.13707 -4.0668468 -3.9774411 -3.9083714 -3.8860257 -3.9024589 -3.961391 -4.0244226 -4.0776749 -4.1300573 -4.1499348][-4.2327871 -4.2226667 -4.2011185 -4.1761341 -4.1364884 -4.0654607 -3.9825544 -3.9306657 -3.9271243 -3.9548693 -4.0142217 -4.0770926 -4.1276569 -4.1709809 -4.1865835][-4.2425594 -4.2435069 -4.2356725 -4.2184687 -4.1860332 -4.1317582 -4.0785532 -4.0545387 -4.0627956 -4.0873938 -4.1267433 -4.1699491 -4.2038488 -4.2329097 -4.2479815][-4.2521791 -4.26082 -4.2643619 -4.254662 -4.2325325 -4.200562 -4.1774993 -4.1747775 -4.187 -4.2053771 -4.2287092 -4.2525549 -4.2701669 -4.2838197 -4.2937593][-4.2551332 -4.2707534 -4.283205 -4.2798219 -4.2685337 -4.25869 -4.2557073 -4.2601666 -4.2677703 -4.2782021 -4.29139 -4.3030448 -4.3106418 -4.3150582 -4.3220448][-4.2554 -4.2789888 -4.299221 -4.3007593 -4.2959938 -4.2946739 -4.2966051 -4.29817 -4.3000479 -4.3042245 -4.3130836 -4.3208055 -4.3236442 -4.3246856 -4.3283486][-4.2657423 -4.2863226 -4.3048062 -4.3065262 -4.3029819 -4.3013315 -4.3013768 -4.2988243 -4.2963991 -4.2994404 -4.3081408 -4.3165612 -4.3206968 -4.321166 -4.3222604]]...]
INFO - root - 2017-12-05 21:06:08.795302: step 43810, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 76h:04m:38s remains)
INFO - root - 2017-12-05 21:06:18.161969: step 43820, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 75h:13m:42s remains)
INFO - root - 2017-12-05 21:06:27.602818: step 43830, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 73h:59m:42s remains)
INFO - root - 2017-12-05 21:06:36.884373: step 43840, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.920 sec/batch; 73h:48m:26s remains)
INFO - root - 2017-12-05 21:06:46.386055: step 43850, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 76h:16m:36s remains)
INFO - root - 2017-12-05 21:06:55.918481: step 43860, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 75h:16m:59s remains)
INFO - root - 2017-12-05 21:07:05.448813: step 43870, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 77h:07m:06s remains)
INFO - root - 2017-12-05 21:07:14.933669: step 43880, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 79h:04m:16s remains)
INFO - root - 2017-12-05 21:07:24.410262: step 43890, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 74h:55m:54s remains)
INFO - root - 2017-12-05 21:07:33.694216: step 43900, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 77h:05m:35s remains)
2017-12-05 21:07:34.414818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2111216 -4.2182107 -4.2280345 -4.2286549 -4.2230558 -4.2154875 -4.209024 -4.2063928 -4.2130847 -4.228334 -4.2397685 -4.2501664 -4.2646384 -4.2764745 -4.2766829][-4.2103152 -4.2184219 -4.2308927 -4.236496 -4.2342787 -4.2258878 -4.2138925 -4.2034254 -4.2072363 -4.2245927 -4.2384396 -4.2509236 -4.2663145 -4.27768 -4.2756162][-4.2042136 -4.2157164 -4.2329845 -4.2454262 -4.2473907 -4.2383637 -4.2191343 -4.200285 -4.1989527 -4.216187 -4.2329631 -4.2499509 -4.26883 -4.2815742 -4.278019][-4.191277 -4.206531 -4.2290735 -4.2488089 -4.2563896 -4.2479143 -4.2236662 -4.1969495 -4.1897583 -4.2054076 -4.2249222 -4.2469072 -4.2702518 -4.2857585 -4.2814369][-4.1719656 -4.1913137 -4.2193122 -4.2461 -4.2605839 -4.2546744 -4.2276821 -4.19362 -4.1802373 -4.1934495 -4.2157145 -4.24223 -4.2698917 -4.2874751 -4.2820354][-4.1547289 -4.176012 -4.2069669 -4.2395167 -4.2604165 -4.2585306 -4.2313781 -4.1923814 -4.173069 -4.1832476 -4.207458 -4.237267 -4.2681704 -4.2866025 -4.2804914][-4.1442947 -4.1647491 -4.1959939 -4.230937 -4.2555313 -4.257792 -4.2328362 -4.1918526 -4.1675024 -4.1742706 -4.1996684 -4.2316146 -4.2650285 -4.2842565 -4.2784638][-4.1414962 -4.1592445 -4.1885128 -4.2225561 -4.247817 -4.2536197 -4.2319236 -4.1908894 -4.1632104 -4.1665063 -4.1921215 -4.22563 -4.2610159 -4.2813091 -4.2769041][-4.1423435 -4.1556435 -4.1816192 -4.2130823 -4.2384825 -4.2479053 -4.2306952 -4.1917048 -4.16175 -4.1614232 -4.1860943 -4.2201037 -4.2564349 -4.2778778 -4.2760787][-4.1428657 -4.1517906 -4.1753774 -4.2053947 -4.231338 -4.2433758 -4.2302151 -4.19476 -4.1649308 -4.1621475 -4.1851707 -4.2181458 -4.252965 -4.2747536 -4.2754221][-4.1502762 -4.1555161 -4.1773791 -4.20613 -4.2315454 -4.2446256 -4.2342715 -4.2028084 -4.1749225 -4.1712546 -4.192368 -4.2226877 -4.2534008 -4.2735467 -4.2758589][-4.1625023 -4.1646757 -4.1849151 -4.211915 -4.2360544 -4.249198 -4.240756 -4.2136712 -4.1891122 -4.1855326 -4.2039709 -4.230495 -4.2560225 -4.2733912 -4.2759337][-4.1774755 -4.1774111 -4.1955376 -4.220191 -4.2423234 -4.2548184 -4.2477865 -4.2248697 -4.2038856 -4.2008033 -4.21636 -4.2388363 -4.2597094 -4.2743988 -4.2767534][-4.1911268 -4.1891322 -4.2043972 -4.2263823 -4.2470036 -4.2589788 -4.253397 -4.2340646 -4.2165 -4.2137456 -4.2265925 -4.2455912 -4.2632771 -4.2759237 -4.2783089][-4.1989408 -4.1948233 -4.206727 -4.2264223 -4.2464485 -4.2588887 -4.255517 -4.2398133 -4.2258525 -4.2236891 -4.2341762 -4.2502317 -4.265409 -4.2761526 -4.2786508]]...]
INFO - root - 2017-12-05 21:07:43.756885: step 43910, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 77h:58m:22s remains)
INFO - root - 2017-12-05 21:07:53.103244: step 43920, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 72h:59m:01s remains)
INFO - root - 2017-12-05 21:08:02.282450: step 43930, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 77h:24m:39s remains)
INFO - root - 2017-12-05 21:08:11.386217: step 43940, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 76h:23m:16s remains)
INFO - root - 2017-12-05 21:08:20.952516: step 43950, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 76h:54m:44s remains)
INFO - root - 2017-12-05 21:08:30.360313: step 43960, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 67h:34m:56s remains)
INFO - root - 2017-12-05 21:08:39.669537: step 43970, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 74h:13m:21s remains)
INFO - root - 2017-12-05 21:08:49.155881: step 43980, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 79h:32m:21s remains)
INFO - root - 2017-12-05 21:08:58.703177: step 43990, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 77h:34m:57s remains)
INFO - root - 2017-12-05 21:09:08.182920: step 44000, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.985 sec/batch; 78h:55m:00s remains)
2017-12-05 21:09:08.966093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2175708 -4.1922288 -4.1846509 -4.1965618 -4.2071013 -4.2103686 -4.2119813 -4.2130728 -4.2161636 -4.2179427 -4.2206187 -4.2254972 -4.2266316 -4.2240829 -4.22533][-4.204792 -4.1861048 -4.1880536 -4.2125773 -4.2265697 -4.2241826 -4.2133369 -4.2005024 -4.1972103 -4.1919928 -4.1871271 -4.1929708 -4.1961913 -4.195045 -4.1986632][-4.163518 -4.14731 -4.1625724 -4.1971879 -4.2167664 -4.2168484 -4.199965 -4.1810846 -4.1730103 -4.1681447 -4.1655312 -4.1729264 -4.1795406 -4.1740522 -4.1740742][-4.1159978 -4.1006279 -4.1226587 -4.1610193 -4.1834126 -4.1836486 -4.1596637 -4.1449242 -4.1394529 -4.1435719 -4.1429787 -4.1501107 -4.1571541 -4.1416736 -4.1317339][-4.0658665 -4.0560117 -4.0852885 -4.1303959 -4.150835 -4.1396389 -4.0954828 -4.0862265 -4.0958605 -4.1169386 -4.1258545 -4.1359458 -4.140245 -4.1103439 -4.0841541][-4.0022092 -3.9956846 -4.0270119 -4.07829 -4.09657 -4.0672073 -3.9954095 -3.996254 -4.0386963 -4.0948029 -4.126483 -4.14366 -4.1449094 -4.1054263 -4.059648][-3.9402509 -3.9411767 -3.9708462 -4.0148 -4.0141854 -3.9532886 -3.8465576 -3.8617845 -3.9615154 -4.0629139 -4.1204834 -4.1460114 -4.1524673 -4.1129069 -4.0522776][-3.9183841 -3.9237339 -3.9505806 -3.9740987 -3.9478877 -3.8693316 -3.755708 -3.7914758 -3.9271765 -4.0473571 -4.109941 -4.1351576 -4.1484733 -4.1218238 -4.0636516][-3.9560938 -3.9529731 -3.9724748 -3.9892817 -3.9705706 -3.9160013 -3.8476248 -3.8862438 -3.9896355 -4.081594 -4.12382 -4.1414824 -4.1569476 -4.1409111 -4.0994496][-4.0157375 -4.0028749 -4.009707 -4.0252991 -4.0234151 -3.9985056 -3.9679415 -3.9978096 -4.0663619 -4.1211343 -4.1466556 -4.1631141 -4.1797886 -4.1685 -4.1426983][-4.0885453 -4.0688515 -4.0626192 -4.0692692 -4.0744839 -4.0775671 -4.0709081 -4.0887127 -4.1266003 -4.1535621 -4.1679025 -4.1866922 -4.2086558 -4.1992068 -4.183949][-4.1678729 -4.1490178 -4.1367331 -4.137239 -4.1466889 -4.1659379 -4.1683536 -4.1720991 -4.1782489 -4.1752186 -4.1691012 -4.1808233 -4.2034593 -4.202332 -4.1987863][-4.242322 -4.2289305 -4.2147355 -4.2116046 -4.2184653 -4.23386 -4.2327576 -4.2259531 -4.2128115 -4.1865826 -4.1522326 -4.1455851 -4.1618481 -4.170639 -4.1808][-4.28995 -4.2836156 -4.273942 -4.2689524 -4.2688217 -4.2719235 -4.2677054 -4.2552719 -4.2344885 -4.1955118 -4.1441746 -4.1203847 -4.1298079 -4.14767 -4.15934][-4.3037953 -4.3012953 -4.2977142 -4.2909355 -4.2853141 -4.2827673 -4.2805495 -4.2684727 -4.2487631 -4.2093167 -4.1583457 -4.1242504 -4.125679 -4.1426454 -4.14756]]...]
INFO - root - 2017-12-05 21:09:18.452051: step 44010, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 75h:51m:55s remains)
INFO - root - 2017-12-05 21:09:27.862348: step 44020, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 78h:02m:03s remains)
INFO - root - 2017-12-05 21:09:37.087943: step 44030, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.000 sec/batch; 80h:07m:28s remains)
INFO - root - 2017-12-05 21:09:46.593213: step 44040, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 78h:36m:52s remains)
INFO - root - 2017-12-05 21:09:55.782342: step 44050, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 74h:13m:53s remains)
INFO - root - 2017-12-05 21:10:05.296033: step 44060, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 76h:23m:41s remains)
INFO - root - 2017-12-05 21:10:14.741060: step 44070, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 74h:44m:12s remains)
INFO - root - 2017-12-05 21:10:24.344168: step 44080, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 78h:10m:12s remains)
INFO - root - 2017-12-05 21:10:33.746067: step 44090, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 77h:18m:25s remains)
INFO - root - 2017-12-05 21:10:43.385678: step 44100, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.015 sec/batch; 81h:17m:34s remains)
2017-12-05 21:10:44.181128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3123941 -4.2797585 -4.23803 -4.2021275 -4.2061367 -4.2289705 -4.2353048 -4.2106023 -4.1744156 -4.1487579 -4.1522226 -4.1766567 -4.1928239 -4.2057829 -4.2186847][-4.2824044 -4.2573676 -4.2214646 -4.1837883 -4.1813507 -4.1979275 -4.2008009 -4.1839466 -4.1520805 -4.1359191 -4.1510019 -4.1804175 -4.2078018 -4.2291336 -4.2412682][-4.2572632 -4.2374525 -4.2098408 -4.1720977 -4.1593275 -4.1695638 -4.1736822 -4.1638751 -4.1341066 -4.1228585 -4.1558466 -4.1979351 -4.2356796 -4.26021 -4.2599745][-4.2468109 -4.2274585 -4.2087922 -4.1858721 -4.1761293 -4.1856885 -4.1859636 -4.1672091 -4.1320343 -4.1240439 -4.1737556 -4.2287307 -4.2678852 -4.2826324 -4.2702222][-4.2335563 -4.2141833 -4.2001963 -4.1962967 -4.2033811 -4.2181883 -4.2027478 -4.1676235 -4.1162252 -4.1038485 -4.1674423 -4.2311411 -4.2649207 -4.2622066 -4.2383108][-4.2278562 -4.2115073 -4.1991057 -4.2068386 -4.2182059 -4.2234583 -4.1876564 -4.1240659 -4.043642 -4.0314646 -4.1068144 -4.1737614 -4.2026615 -4.190186 -4.1663594][-4.2290154 -4.2085781 -4.1981559 -4.2083554 -4.2059979 -4.1951385 -4.1329031 -4.029582 -3.9163005 -3.9201589 -4.02187 -4.0960197 -4.1286945 -4.118391 -4.0964327][-4.2031164 -4.1719604 -4.1621337 -4.1691933 -4.1580362 -4.124414 -4.037673 -3.9053404 -3.7680154 -3.7990894 -3.9337983 -4.0231719 -4.0730047 -4.0813541 -4.0732322][-4.1911449 -4.1557283 -4.137414 -4.1351275 -4.1198545 -4.0771742 -3.9924955 -3.8753242 -3.7607937 -3.7983696 -3.9273841 -4.0156989 -4.0704474 -4.0873108 -4.0930548][-4.232111 -4.1974487 -4.1727529 -4.1551342 -4.1302214 -4.0942535 -4.0431905 -3.9771938 -3.9102337 -3.9353733 -4.0248909 -4.0945239 -4.1363597 -4.1446085 -4.150835][-4.2906337 -4.2577991 -4.2292843 -4.1999617 -4.1696181 -4.1447592 -4.1186562 -4.0872188 -4.0501895 -4.0637836 -4.1231222 -4.175281 -4.2065024 -4.2096634 -4.2121582][-4.3250518 -4.2976818 -4.2727165 -4.2461891 -4.2207808 -4.2064395 -4.1974897 -4.1818991 -4.1617475 -4.1675591 -4.2050128 -4.2386212 -4.2614794 -4.2631207 -4.2638087][-4.3333669 -4.315105 -4.2956505 -4.2696986 -4.2521777 -4.249198 -4.25037 -4.2439647 -4.232316 -4.2333107 -4.2555041 -4.27437 -4.2867413 -4.2873664 -4.2856636][-4.3161259 -4.308044 -4.2890148 -4.2631626 -4.2519622 -4.2568679 -4.2649446 -4.2669625 -4.2605786 -4.2592187 -4.2714415 -4.278131 -4.2771373 -4.2733111 -4.2725673][-4.2995534 -4.2966852 -4.279974 -4.2567821 -4.2449861 -4.2482967 -4.2565804 -4.2627268 -4.2611823 -4.2611895 -4.2679648 -4.2697563 -4.2631145 -4.256299 -4.2545853]]...]
INFO - root - 2017-12-05 21:10:53.755879: step 44110, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 74h:34m:45s remains)
INFO - root - 2017-12-05 21:11:02.972401: step 44120, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 68h:58m:17s remains)
INFO - root - 2017-12-05 21:11:12.163560: step 44130, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.979 sec/batch; 78h:26m:09s remains)
INFO - root - 2017-12-05 21:11:21.743549: step 44140, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 78h:58m:40s remains)
INFO - root - 2017-12-05 21:11:31.039801: step 44150, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 73h:50m:35s remains)
INFO - root - 2017-12-05 21:11:40.531771: step 44160, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 77h:08m:02s remains)
INFO - root - 2017-12-05 21:11:49.940054: step 44170, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 75h:38m:00s remains)
INFO - root - 2017-12-05 21:11:59.519479: step 44180, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 75h:13m:36s remains)
INFO - root - 2017-12-05 21:12:08.838652: step 44190, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.939 sec/batch; 75h:13m:09s remains)
INFO - root - 2017-12-05 21:12:18.150615: step 44200, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 74h:58m:14s remains)
2017-12-05 21:12:18.916891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3208928 -4.3241639 -4.3302784 -4.3344173 -4.3347616 -4.3336058 -4.3303666 -4.3263879 -4.3223896 -4.3173652 -4.3102 -4.3054585 -4.3070736 -4.3133292 -4.3231597][-4.3060312 -4.3104587 -4.3184471 -4.3231368 -4.3207755 -4.3163228 -4.3119893 -4.3062768 -4.3019032 -4.2987328 -4.2942824 -4.2904506 -4.2920861 -4.299705 -4.3096781][-4.2751036 -4.2819262 -4.29449 -4.3025861 -4.2981534 -4.2851925 -4.2736545 -4.262917 -4.2581935 -4.2606397 -4.2646952 -4.2661443 -4.2699842 -4.2810488 -4.2946072][-4.2309189 -4.2386627 -4.2578464 -4.2699614 -4.261857 -4.2343693 -4.2084279 -4.1922607 -4.19369 -4.2109656 -4.2307062 -4.2412429 -4.248208 -4.2606487 -4.2767415][-4.1840467 -4.1890697 -4.2142305 -4.2303176 -4.2159562 -4.1658363 -4.1125407 -4.0837941 -4.0955849 -4.1392622 -4.1827655 -4.2085118 -4.2226291 -4.2365274 -4.2547064][-4.1473188 -4.1492167 -4.176589 -4.1925545 -4.1680503 -4.09166 -3.9997151 -3.9449112 -3.9658921 -4.0440855 -4.1201758 -4.169126 -4.19426 -4.2102752 -4.2313161][-4.1156545 -4.1175623 -4.1483159 -4.1680574 -4.1365719 -4.0375195 -3.9049401 -3.8106484 -3.8354039 -3.9484878 -4.0591888 -4.1309524 -4.1670971 -4.1860008 -4.2094378][-4.0906954 -4.0931292 -4.1241088 -4.1486 -4.1179013 -4.0146251 -3.87077 -3.7574613 -3.7740731 -3.8952751 -4.0197768 -4.1013212 -4.141799 -4.1636906 -4.191103][-4.0787354 -4.0808892 -4.1096282 -4.1355181 -4.1149721 -4.0357637 -3.9232657 -3.8304 -3.8320148 -3.9218011 -4.0244131 -4.0947223 -4.1300206 -4.1525674 -4.1831093][-4.0719934 -4.0715623 -4.0968666 -4.1232686 -4.1202693 -4.0820885 -4.0218778 -3.9675572 -3.9600806 -4.0063925 -4.0681925 -4.1141939 -4.1421266 -4.1642175 -4.1942468][-4.0730805 -4.070394 -4.0953069 -4.1222782 -4.13527 -4.1298733 -4.11217 -4.0922351 -4.0860729 -4.1034117 -4.1317859 -4.1553078 -4.1742935 -4.1943693 -4.2222662][-4.0990829 -4.095036 -4.1219397 -4.1509924 -4.1701279 -4.17972 -4.18469 -4.1863704 -4.187201 -4.1933174 -4.2045231 -4.2147985 -4.2263689 -4.2422237 -4.2652087][-4.1675115 -4.1618137 -4.1829162 -4.2078643 -4.2248988 -4.2352381 -4.2456121 -4.2545867 -4.2598023 -4.2650347 -4.271338 -4.2754169 -4.2806344 -4.2901368 -4.3055372][-4.2544079 -4.2484031 -4.2590585 -4.2721562 -4.280973 -4.2878819 -4.2962794 -4.3045692 -4.3095517 -4.3143697 -4.3186493 -4.3201184 -4.3211141 -4.324861 -4.332984][-4.3171868 -4.3123517 -4.3150592 -4.3176165 -4.3193789 -4.3222084 -4.3268723 -4.3313642 -4.3339415 -4.3363142 -4.3385839 -4.3399005 -4.3409181 -4.3429766 -4.3472471]]...]
INFO - root - 2017-12-05 21:12:28.324870: step 44210, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 66h:57m:00s remains)
INFO - root - 2017-12-05 21:12:37.784063: step 44220, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 75h:22m:53s remains)
INFO - root - 2017-12-05 21:12:47.252619: step 44230, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 69h:00m:39s remains)
INFO - root - 2017-12-05 21:12:56.712165: step 44240, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 72h:26m:37s remains)
INFO - root - 2017-12-05 21:13:06.168629: step 44250, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 75h:52m:34s remains)
INFO - root - 2017-12-05 21:13:15.496712: step 44260, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 74h:56m:34s remains)
INFO - root - 2017-12-05 21:13:24.797191: step 44270, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 77h:05m:23s remains)
INFO - root - 2017-12-05 21:13:34.175831: step 44280, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 73h:24m:12s remains)
INFO - root - 2017-12-05 21:13:43.655654: step 44290, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.872 sec/batch; 69h:46m:58s remains)
INFO - root - 2017-12-05 21:13:53.162480: step 44300, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 75h:11m:21s remains)
2017-12-05 21:13:54.008747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3368635 -4.3405848 -4.34089 -4.3382978 -4.334712 -4.33247 -4.33195 -4.3325768 -4.334239 -4.3359575 -4.3377008 -4.3399057 -4.3419018 -4.3437939 -4.3443933][-4.3287673 -4.3323584 -4.3311148 -4.3242664 -4.3170452 -4.314168 -4.3132353 -4.3141046 -4.317276 -4.3215628 -4.3268256 -4.3315377 -4.33414 -4.33525 -4.3340416][-4.3193617 -4.3195109 -4.3144317 -4.3032742 -4.2928095 -4.288126 -4.2852111 -4.284471 -4.2875633 -4.2959762 -4.30705 -4.3153968 -4.3186951 -4.3181772 -4.3136954][-4.2944732 -4.2875609 -4.2771816 -4.2610884 -4.2470031 -4.2398105 -4.2346425 -4.2327948 -4.238668 -4.2547832 -4.2725945 -4.2849164 -4.2875028 -4.2821465 -4.2749228][-4.2536831 -4.2386775 -4.223299 -4.2013435 -4.18094 -4.1686535 -4.1612477 -4.1608653 -4.17344 -4.1972342 -4.2210355 -4.2356253 -4.2363167 -4.225729 -4.2191086][-4.2110467 -4.1913772 -4.171072 -4.1435623 -4.115808 -4.0953197 -4.0856218 -4.0864549 -4.1064262 -4.1370573 -4.1661515 -4.1838226 -4.1853704 -4.1723585 -4.168581][-4.1680207 -4.1466 -4.1232204 -4.0972757 -4.0719976 -4.0482321 -4.0358043 -4.0362167 -4.0597372 -4.0927796 -4.1223822 -4.1433868 -4.1495357 -4.1400514 -4.1398711][-4.1227722 -4.1048851 -4.0865626 -4.06843 -4.0535994 -4.0345764 -4.0219927 -4.0196528 -4.0397305 -4.0674977 -4.0927787 -4.1167254 -4.130096 -4.12967 -4.134954][-4.0883975 -4.0798 -4.0718689 -4.0673723 -4.0666041 -4.0548677 -4.043993 -4.0406213 -4.055521 -4.0752354 -4.0960813 -4.1203456 -4.1368742 -4.1416979 -4.1479926][-4.08511 -4.0861459 -4.0874834 -4.0942087 -4.1018324 -4.0973072 -4.0903745 -4.0898581 -4.1009092 -4.1135626 -4.1308675 -4.1522155 -4.1677971 -4.1720791 -4.1744685][-4.106123 -4.11131 -4.1185551 -4.13255 -4.1459274 -4.14499 -4.139287 -4.139926 -4.1470408 -4.1540117 -4.1685791 -4.1862164 -4.19847 -4.2006903 -4.2012434][-4.1398335 -4.1450205 -4.1544509 -4.1693444 -4.1797156 -4.1792431 -4.17582 -4.1756997 -4.1784482 -4.1801519 -4.1897902 -4.2022576 -4.211885 -4.2157688 -4.2184062][-4.1714215 -4.1754203 -4.186336 -4.1992722 -4.2038612 -4.2019792 -4.1993346 -4.1979437 -4.1953392 -4.1908789 -4.1934357 -4.20088 -4.2103205 -4.2180643 -4.223619][-4.2022181 -4.2062597 -4.2156243 -4.2220836 -4.2197838 -4.215034 -4.2120128 -4.2087116 -4.20298 -4.1961327 -4.1949944 -4.199007 -4.2083573 -4.2175407 -4.2233925][-4.2195163 -4.224432 -4.2330413 -4.2351604 -4.2281508 -4.2191682 -4.212604 -4.2080994 -4.2040596 -4.2000608 -4.1981111 -4.1994438 -4.2071471 -4.214613 -4.2191858]]...]
INFO - root - 2017-12-05 21:14:03.422024: step 44310, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 75h:19m:50s remains)
INFO - root - 2017-12-05 21:14:12.886441: step 44320, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.924 sec/batch; 74h:00m:00s remains)
INFO - root - 2017-12-05 21:14:22.492156: step 44330, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 74h:50m:28s remains)
INFO - root - 2017-12-05 21:14:31.818894: step 44340, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 74h:04m:40s remains)
INFO - root - 2017-12-05 21:14:41.239625: step 44350, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 73h:44m:03s remains)
INFO - root - 2017-12-05 21:14:50.703156: step 44360, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 75h:35m:39s remains)
INFO - root - 2017-12-05 21:15:00.148596: step 44370, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 76h:03m:59s remains)
INFO - root - 2017-12-05 21:15:09.713685: step 44380, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.989 sec/batch; 79h:07m:29s remains)
INFO - root - 2017-12-05 21:15:18.996495: step 44390, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 80h:18m:30s remains)
INFO - root - 2017-12-05 21:15:28.323981: step 44400, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 75h:19m:18s remains)
2017-12-05 21:15:29.077603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2360063 -4.2209396 -4.2100649 -4.2011337 -4.20287 -4.2078204 -4.2060976 -4.1995549 -4.1938224 -4.1920333 -4.1964922 -4.203939 -4.2098012 -4.2147832 -4.2172408][-4.2260122 -4.2087235 -4.1965423 -4.1884723 -4.1936312 -4.2037015 -4.2075076 -4.2051044 -4.2003465 -4.1949868 -4.192028 -4.19241 -4.1943884 -4.1990213 -4.2053328][-4.2215977 -4.202281 -4.1872954 -4.176877 -4.181479 -4.193656 -4.2028465 -4.2082462 -4.2093139 -4.2043757 -4.1952305 -4.186831 -4.1820579 -4.183609 -4.1912241][-4.2246656 -4.2033896 -4.1862292 -4.1724353 -4.1732216 -4.1833868 -4.193728 -4.2043948 -4.21325 -4.2133684 -4.2031856 -4.1898632 -4.1799612 -4.1764994 -4.1805458][-4.2327785 -4.2123613 -4.1956291 -4.178998 -4.17338 -4.1764383 -4.180388 -4.1891909 -4.2039223 -4.2126932 -4.2081447 -4.1961279 -4.1849036 -4.1767583 -4.1745028][-4.2400079 -4.2233672 -4.2091622 -4.1903634 -4.1768966 -4.1680474 -4.1599865 -4.1624794 -4.1813321 -4.2001247 -4.2049193 -4.1986623 -4.189014 -4.1785636 -4.1718059][-4.2440314 -4.2313414 -4.2203436 -4.201138 -4.1821179 -4.1631727 -4.1443396 -4.1404881 -4.1609468 -4.1859751 -4.1971784 -4.1966524 -4.1893806 -4.1786108 -4.1712546][-4.2425666 -4.2319188 -4.2240486 -4.206995 -4.1879406 -4.1698022 -4.152163 -4.1467342 -4.1627669 -4.1847129 -4.1951137 -4.1950855 -4.1881232 -4.1786489 -4.1744723][-4.234725 -4.2230554 -4.2179413 -4.2059364 -4.193882 -4.1862459 -4.1792088 -4.177496 -4.1879191 -4.20178 -4.2062492 -4.2014909 -4.1907945 -4.180057 -4.1769137][-4.2261057 -4.2095532 -4.2048497 -4.1984396 -4.1954656 -4.1995611 -4.2027903 -4.2056837 -4.2132063 -4.2215748 -4.2211275 -4.2107058 -4.1940403 -4.1778989 -4.1714487][-4.2212119 -4.197926 -4.1885271 -4.1830463 -4.1840725 -4.1944313 -4.2027049 -4.2085776 -4.2159324 -4.2242727 -4.2247658 -4.2143149 -4.1969204 -4.1777635 -4.1668143][-4.2263207 -4.196929 -4.1804295 -4.1708674 -4.1690135 -4.17826 -4.185997 -4.1925859 -4.2008414 -4.2103672 -4.2142868 -4.2093492 -4.1979008 -4.1806645 -4.167398][-4.2391386 -4.2074156 -4.184391 -4.1681356 -4.1602616 -4.163806 -4.1685424 -4.1750326 -4.1831527 -4.1907897 -4.1944151 -4.1942511 -4.1917467 -4.1833425 -4.1751056][-4.2485709 -4.2175941 -4.1929379 -4.1731596 -4.1617813 -4.1616697 -4.1640325 -4.1689477 -4.1739993 -4.1769342 -4.17775 -4.1809063 -4.1871076 -4.1908851 -4.1921821][-4.2449365 -4.2145939 -4.1916986 -4.1743584 -4.16576 -4.1666865 -4.1689768 -4.1726108 -4.1747284 -4.1738243 -4.1727915 -4.1778831 -4.1894035 -4.2026649 -4.2122612]]...]
INFO - root - 2017-12-05 21:15:38.628889: step 44410, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.000 sec/batch; 80h:00m:09s remains)
INFO - root - 2017-12-05 21:15:48.269572: step 44420, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 72h:24m:48s remains)
INFO - root - 2017-12-05 21:15:57.806746: step 44430, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 74h:58m:47s remains)
INFO - root - 2017-12-05 21:16:07.155580: step 44440, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 69h:45m:44s remains)
INFO - root - 2017-12-05 21:16:16.629039: step 44450, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 75h:08m:43s remains)
INFO - root - 2017-12-05 21:16:26.096082: step 44460, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 71h:07m:30s remains)
INFO - root - 2017-12-05 21:16:35.687661: step 44470, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 75h:08m:30s remains)
INFO - root - 2017-12-05 21:16:45.045659: step 44480, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 67h:49m:17s remains)
INFO - root - 2017-12-05 21:16:54.408570: step 44490, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.925 sec/batch; 73h:58m:06s remains)
INFO - root - 2017-12-05 21:17:03.881384: step 44500, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 75h:35m:37s remains)
2017-12-05 21:17:04.680776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1708384 -4.1843338 -4.1809611 -4.1680689 -4.1433954 -4.1029077 -4.098774 -4.1198549 -4.1363373 -4.1275358 -4.1130719 -4.1234751 -4.1557627 -4.1979833 -4.240437][-4.2058363 -4.2175512 -4.2154026 -4.2023058 -4.1767225 -4.1397882 -4.1241441 -4.135685 -4.1583705 -4.1665373 -4.1600637 -4.1694865 -4.1927366 -4.2250018 -4.2550287][-4.2464619 -4.2473073 -4.2425642 -4.2300844 -4.2058735 -4.1754365 -4.149425 -4.1578679 -4.1943469 -4.2143412 -4.2124577 -4.2148824 -4.2301574 -4.2469568 -4.2561364][-4.2630672 -4.2567143 -4.2565212 -4.2429843 -4.2200475 -4.1888819 -4.1495519 -4.1475081 -4.196434 -4.2237225 -4.2216058 -4.2254381 -4.2415781 -4.2495923 -4.2363653][-4.2459121 -4.2349873 -4.2396674 -4.2218461 -4.1946464 -4.1577749 -4.0987129 -4.0831828 -4.1440392 -4.180397 -4.1818128 -4.1948695 -4.2217889 -4.2322741 -4.2081604][-4.1980786 -4.172667 -4.1779623 -4.159502 -4.1302962 -4.0740457 -3.9785585 -3.9557564 -4.0530152 -4.115766 -4.1286864 -4.1447229 -4.1839094 -4.2055135 -4.1812][-4.1433034 -4.0929146 -4.0917826 -4.0806313 -4.0508451 -3.9710555 -3.8404584 -3.8178806 -3.9676416 -4.0667186 -4.0987296 -4.1185455 -4.1644754 -4.1957622 -4.17749][-4.1013918 -4.0254869 -4.0094056 -4.0172033 -4.0143108 -3.953964 -3.8486164 -3.8320732 -3.9672475 -4.0694084 -4.1090388 -4.1307049 -4.1781693 -4.2120118 -4.2012577][-4.0966864 -4.0242181 -4.008028 -4.0355887 -4.0705066 -4.0591278 -4.0040474 -3.9764421 -4.0407939 -4.1155739 -4.1504788 -4.173768 -4.2111068 -4.2348843 -4.2266173][-4.1084313 -4.0664287 -4.0680485 -4.1109858 -4.1564636 -4.1680617 -4.1336174 -4.091784 -4.1102571 -4.1615896 -4.1927485 -4.2191839 -4.2463288 -4.25743 -4.2421713][-4.1070457 -4.096333 -4.1128216 -4.15707 -4.198524 -4.2177687 -4.1937718 -4.1540461 -4.1606574 -4.1994781 -4.225668 -4.2511234 -4.2707405 -4.2719679 -4.2458239][-4.1010494 -4.1114864 -4.1322241 -4.1727204 -4.206615 -4.2242966 -4.2024245 -4.1711006 -4.1798573 -4.2077751 -4.2282534 -4.2518945 -4.2714968 -4.2654104 -4.2377119][-4.1170483 -4.1383572 -4.1488342 -4.1763911 -4.1989913 -4.2069573 -4.1851888 -4.1585312 -4.1701131 -4.192883 -4.2077208 -4.2278376 -4.2457628 -4.2414465 -4.2176366][-4.1608915 -4.1738629 -4.1712961 -4.1853108 -4.1942949 -4.1858687 -4.1612597 -4.1371713 -4.1468558 -4.1617508 -4.173234 -4.190949 -4.2097077 -4.2166567 -4.2036047][-4.2055259 -4.2018704 -4.1925206 -4.1998887 -4.198544 -4.1783991 -4.1486368 -4.1294975 -4.1372676 -4.1459594 -4.1546645 -4.1707168 -4.19069 -4.2079897 -4.2043386]]...]
INFO - root - 2017-12-05 21:17:14.240229: step 44510, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 78h:04m:02s remains)
INFO - root - 2017-12-05 21:17:23.713588: step 44520, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 75h:42m:48s remains)
INFO - root - 2017-12-05 21:17:33.155654: step 44530, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 74h:06m:32s remains)
INFO - root - 2017-12-05 21:17:42.515112: step 44540, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 76h:40m:31s remains)
INFO - root - 2017-12-05 21:17:51.967026: step 44550, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 73h:20m:27s remains)
INFO - root - 2017-12-05 21:18:01.379559: step 44560, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 74h:01m:00s remains)
INFO - root - 2017-12-05 21:18:10.744998: step 44570, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 75h:33m:22s remains)
INFO - root - 2017-12-05 21:18:20.037861: step 44580, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 68h:48m:33s remains)
INFO - root - 2017-12-05 21:18:29.444642: step 44590, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.958 sec/batch; 76h:37m:06s remains)
INFO - root - 2017-12-05 21:18:38.943471: step 44600, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 74h:06m:02s remains)
2017-12-05 21:18:39.712073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3033614 -4.3023133 -4.3002796 -4.29963 -4.3004141 -4.3028646 -4.30646 -4.3095474 -4.3105345 -4.3109403 -4.3118973 -4.3149233 -4.32001 -4.3261571 -4.3318958][-4.2822266 -4.2787757 -4.2755485 -4.274704 -4.2768927 -4.281312 -4.28679 -4.2904148 -4.2903428 -4.2902913 -4.2910666 -4.2946486 -4.3015885 -4.3106318 -4.3194437][-4.2624459 -4.2556534 -4.2495904 -4.2466707 -4.2477241 -4.2527184 -4.2586632 -4.2626529 -4.263505 -4.2649069 -4.2669067 -4.2710114 -4.2809362 -4.2935376 -4.3050003][-4.2435045 -4.2283096 -4.2158756 -4.2106123 -4.2123837 -4.2184048 -4.2232103 -4.2277951 -4.2320175 -4.236897 -4.2400017 -4.24387 -4.2572327 -4.2744212 -4.2890248][-4.2243915 -4.1953082 -4.1741896 -4.1692839 -4.173315 -4.17683 -4.17632 -4.1820812 -4.193068 -4.2027359 -4.206912 -4.2115879 -4.2279787 -4.2495332 -4.2690058][-4.1994948 -4.15736 -4.1304317 -4.1252108 -4.1279655 -4.1239209 -4.1126418 -4.1188564 -4.1401806 -4.1597157 -4.1663814 -4.1717834 -4.1910386 -4.2181787 -4.2452273][-4.1732192 -4.1275682 -4.0988569 -4.0892277 -4.0820613 -4.0616245 -4.0328741 -4.0391164 -4.0758958 -4.1092439 -4.1208229 -4.1264515 -4.1485319 -4.1836853 -4.2202759][-4.1498971 -4.1081738 -4.0791082 -4.0614786 -4.0360808 -3.9889603 -3.9373624 -3.9485941 -4.0051312 -4.055541 -4.0751562 -4.0821867 -4.1042814 -4.1438322 -4.1881242][-4.1255879 -4.0864363 -4.0587034 -4.0408568 -4.0068855 -3.9407995 -3.8757398 -3.896271 -3.9656076 -4.0234671 -4.044157 -4.0467443 -4.0636725 -4.1026993 -4.1511273][-4.1115179 -4.0717964 -4.0457053 -4.0365529 -4.0159569 -3.9623561 -3.9122167 -3.9396832 -3.9971569 -4.0340829 -4.037509 -4.026804 -4.033915 -4.0683742 -4.1179552][-4.115119 -4.0740447 -4.0468345 -4.0430551 -4.0379925 -4.0109792 -3.9904871 -4.017869 -4.0493188 -4.0600204 -4.0444174 -4.0250587 -4.025878 -4.0517921 -4.0953407][-4.1422873 -4.1030622 -4.0769167 -4.0741124 -4.0761294 -4.0669866 -4.0658083 -4.0871482 -4.1007781 -4.0997548 -4.0808163 -4.0654125 -4.0656242 -4.0771623 -4.1042261][-4.1927319 -4.1591387 -4.1382647 -4.1363754 -4.1394095 -4.1384621 -4.14322 -4.1579704 -4.1656032 -4.1643891 -4.1527472 -4.1451178 -4.1456175 -4.1464968 -4.1573586][-4.2480016 -4.2254248 -4.212759 -4.212564 -4.2158852 -4.2184219 -4.2236776 -4.2325025 -4.2374268 -4.2384377 -4.2346559 -4.2341781 -4.2359405 -4.2333651 -4.2343931][-4.2961445 -4.2853966 -4.2800961 -4.2814412 -4.2829738 -4.2848606 -4.2885861 -4.2935328 -4.296504 -4.2977648 -4.296937 -4.2990918 -4.3020654 -4.3004146 -4.2982078]]...]
INFO - root - 2017-12-05 21:18:49.138899: step 44610, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 72h:00m:44s remains)
INFO - root - 2017-12-05 21:18:58.454481: step 44620, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 73h:58m:39s remains)
INFO - root - 2017-12-05 21:19:07.839057: step 44630, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 76h:27m:09s remains)
INFO - root - 2017-12-05 21:19:17.294483: step 44640, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 75h:56m:13s remains)
INFO - root - 2017-12-05 21:19:26.749175: step 44650, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 76h:34m:43s remains)
INFO - root - 2017-12-05 21:19:36.094544: step 44660, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 73h:09m:59s remains)
INFO - root - 2017-12-05 21:19:45.504788: step 44670, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 74h:54m:53s remains)
INFO - root - 2017-12-05 21:19:54.762786: step 44680, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.946 sec/batch; 75h:35m:55s remains)
INFO - root - 2017-12-05 21:20:04.179790: step 44690, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 77h:12m:08s remains)
INFO - root - 2017-12-05 21:20:13.880558: step 44700, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 78h:59m:27s remains)
2017-12-05 21:20:14.632969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1828046 -4.2079744 -4.2313228 -4.2300591 -4.2137976 -4.2095304 -4.2010589 -4.1989336 -4.2089639 -4.2237234 -4.2389932 -4.2424517 -4.2262015 -4.2090397 -4.2117281][-4.2026424 -4.2280893 -4.2460313 -4.2436109 -4.2352953 -4.237257 -4.2296767 -4.225687 -4.2257476 -4.2324681 -4.2477732 -4.2560596 -4.245203 -4.2310309 -4.2310257][-4.2141509 -4.2373171 -4.2485089 -4.2426858 -4.2391739 -4.2434 -4.2341976 -4.2297869 -4.222734 -4.2245374 -4.2417059 -4.255043 -4.2477174 -4.2363076 -4.2375607][-4.2110772 -4.2325506 -4.2423849 -4.2347631 -4.2323704 -4.2305326 -4.2133412 -4.2069974 -4.1986289 -4.2052364 -4.22841 -4.2458405 -4.2454543 -4.2396989 -4.2431893][-4.1902804 -4.2124972 -4.2242985 -4.219553 -4.212338 -4.1945639 -4.1652989 -4.1556892 -4.154973 -4.1757307 -4.2115545 -4.2377319 -4.2480407 -4.248549 -4.2520161][-4.1738558 -4.1932211 -4.2011838 -4.1931705 -4.1731577 -4.1341081 -4.0922236 -4.0826297 -4.1000547 -4.1429992 -4.1935105 -4.2300243 -4.2513871 -4.2580104 -4.2606893][-4.179038 -4.1911674 -4.1935062 -4.1749969 -4.1384268 -4.0805054 -4.0315814 -4.0277004 -4.067431 -4.1256719 -4.1802959 -4.2213979 -4.2477036 -4.2577438 -4.2606544][-4.1918178 -4.1996613 -4.1957211 -4.1706438 -4.1261325 -4.0634766 -4.0169749 -4.0190477 -4.0706644 -4.1294637 -4.1762166 -4.21304 -4.2353396 -4.2429819 -4.2447143][-4.1931672 -4.2000003 -4.1955318 -4.1738672 -4.1345496 -4.0832911 -4.0500541 -4.0536361 -4.0974092 -4.1426826 -4.1735196 -4.199656 -4.2133636 -4.2171469 -4.2190957][-4.1908817 -4.1971316 -4.1953545 -4.1797872 -4.149559 -4.1113539 -4.0905533 -4.091301 -4.1219068 -4.152226 -4.1676316 -4.1829348 -4.19041 -4.1946731 -4.2001858][-4.1835122 -4.1886983 -4.1875634 -4.176559 -4.1545811 -4.1286621 -4.1185422 -4.1174674 -4.1376138 -4.1569204 -4.1631403 -4.1733718 -4.1806612 -4.1888018 -4.1984463][-4.1830764 -4.1849604 -4.1818361 -4.1725769 -4.1607723 -4.1487083 -4.150341 -4.1533422 -4.17156 -4.18524 -4.1874895 -4.1942325 -4.2003489 -4.2099762 -4.222218][-4.2008567 -4.202301 -4.2001376 -4.1941476 -4.1897192 -4.1893959 -4.1980076 -4.20283 -4.2197356 -4.2319427 -4.2321925 -4.2338505 -4.2373791 -4.2472286 -4.2594748][-4.2365727 -4.2373619 -4.2377796 -4.235096 -4.2352033 -4.2405729 -4.2485361 -4.2510986 -4.262991 -4.2723713 -4.2711105 -4.2695169 -4.27043 -4.2792454 -4.2892351][-4.2681308 -4.2689066 -4.272275 -4.2732911 -4.2756267 -4.2813954 -4.285491 -4.2853594 -4.2908044 -4.2961154 -4.2948608 -4.2930059 -4.2937474 -4.3000417 -4.306953]]...]
INFO - root - 2017-12-05 21:20:24.040932: step 44710, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 77h:20m:22s remains)
INFO - root - 2017-12-05 21:20:33.477942: step 44720, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 76h:12m:50s remains)
INFO - root - 2017-12-05 21:20:43.014690: step 44730, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 74h:15m:16s remains)
INFO - root - 2017-12-05 21:20:52.453819: step 44740, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 72h:02m:59s remains)
INFO - root - 2017-12-05 21:21:01.697294: step 44750, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 76h:20m:46s remains)
INFO - root - 2017-12-05 21:21:11.057856: step 44760, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 72h:53m:25s remains)
INFO - root - 2017-12-05 21:21:20.430772: step 44770, loss = 2.03, batch loss = 1.98 (8.3 examples/sec; 0.959 sec/batch; 76h:38m:35s remains)
INFO - root - 2017-12-05 21:21:29.711118: step 44780, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 75h:12m:03s remains)
INFO - root - 2017-12-05 21:21:39.032877: step 44790, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 74h:42m:20s remains)
INFO - root - 2017-12-05 21:21:48.361393: step 44800, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 76h:10m:14s remains)
2017-12-05 21:21:49.162062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2212992 -4.2078094 -4.2018151 -4.2061787 -4.2112055 -4.2035341 -4.1954679 -4.1918883 -4.1812983 -4.1782408 -4.1934085 -4.2234278 -4.2548728 -4.27434 -4.2873812][-4.2458334 -4.2351975 -4.2275653 -4.2300663 -4.2319708 -4.22317 -4.2184324 -4.2236028 -4.2208238 -4.2217484 -4.2376385 -4.2589021 -4.2762403 -4.2861395 -4.2929921][-4.259366 -4.2516222 -4.243834 -4.2454677 -4.2481675 -4.2422781 -4.240828 -4.2463632 -4.2457848 -4.2482615 -4.2645273 -4.2786288 -4.286212 -4.2888718 -4.2916822][-4.2554097 -4.2443204 -4.2376361 -4.2420516 -4.2485337 -4.2458839 -4.2450771 -4.2500267 -4.2517509 -4.2555361 -4.2713318 -4.28254 -4.2845716 -4.2821279 -4.2833056][-4.2480192 -4.2298179 -4.2211571 -4.226254 -4.231708 -4.230134 -4.2266393 -4.2261558 -4.2260981 -4.2347302 -4.2576823 -4.2715321 -4.2731662 -4.2700782 -4.2721496][-4.2389932 -4.214026 -4.19848 -4.201088 -4.2057896 -4.2031732 -4.1935253 -4.179966 -4.1725411 -4.186718 -4.2198415 -4.2443004 -4.256104 -4.2609382 -4.266849][-4.2190108 -4.1893444 -4.1629653 -4.1557684 -4.1557055 -4.1470375 -4.1261439 -4.0902572 -4.0672908 -4.0893178 -4.1445456 -4.1940312 -4.2260389 -4.2453613 -4.2590151][-4.2016535 -4.1683669 -4.13268 -4.1123028 -4.1021881 -4.0883718 -4.0563459 -3.9919047 -3.9424646 -3.9673927 -4.0514526 -4.1333981 -4.1875834 -4.2210474 -4.2441826][-4.1870666 -4.1582527 -4.1304288 -4.1127558 -4.1043105 -4.1062522 -4.0897493 -4.0296965 -3.9748774 -3.9851253 -4.0538945 -4.1257696 -4.1752338 -4.21066 -4.2395525][-4.1967335 -4.1818485 -4.1716003 -4.1633654 -4.1601038 -4.1760139 -4.1764812 -4.1390243 -4.1013117 -4.1015024 -4.1374645 -4.178844 -4.206769 -4.2296438 -4.2533441][-4.2261696 -4.2255721 -4.2254605 -4.2192068 -4.2184668 -4.2352691 -4.2362313 -4.2164221 -4.196701 -4.1957612 -4.2104983 -4.2295008 -4.2437716 -4.2576342 -4.2740374][-4.2350769 -4.2423649 -4.2480245 -4.2456923 -4.2465653 -4.2579527 -4.2560225 -4.2479067 -4.2426863 -4.244956 -4.2504025 -4.2585549 -4.2693844 -4.2806439 -4.2931781][-4.2238908 -4.2354379 -4.2457595 -4.2504458 -4.2550335 -4.2633505 -4.2600527 -4.2567649 -4.2566028 -4.2623658 -4.2673769 -4.2761359 -4.2889876 -4.2987475 -4.3092895][-4.2191339 -4.2344279 -4.2492914 -4.2587709 -4.2660971 -4.2737603 -4.268826 -4.2626677 -4.258883 -4.2653046 -4.2727036 -4.2857757 -4.3034387 -4.3144746 -4.3249536][-4.2307391 -4.2458534 -4.2585588 -4.2667761 -4.2714262 -4.2758574 -4.2690821 -4.2595067 -4.2528386 -4.2567334 -4.2661576 -4.2855854 -4.3083444 -4.3223758 -4.3340192]]...]
INFO - root - 2017-12-05 21:21:58.064926: step 44810, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 69h:48m:10s remains)
INFO - root - 2017-12-05 21:22:07.295546: step 44820, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 73h:13m:45s remains)
INFO - root - 2017-12-05 21:22:16.804873: step 44830, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 74h:28m:06s remains)
INFO - root - 2017-12-05 21:22:26.094083: step 44840, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 73h:19m:47s remains)
INFO - root - 2017-12-05 21:22:35.435058: step 44850, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 75h:56m:21s remains)
INFO - root - 2017-12-05 21:22:44.673025: step 44860, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 76h:24m:33s remains)
INFO - root - 2017-12-05 21:22:53.987029: step 44870, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 72h:05m:48s remains)
INFO - root - 2017-12-05 21:23:03.161244: step 44880, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 74h:38m:52s remains)
INFO - root - 2017-12-05 21:23:12.552889: step 44890, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 73h:19m:05s remains)
INFO - root - 2017-12-05 21:23:21.754364: step 44900, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.922 sec/batch; 73h:38m:42s remains)
2017-12-05 21:23:22.500727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.303957 -4.3001881 -4.2905192 -4.2798486 -4.2716632 -4.2660823 -4.2574954 -4.24953 -4.2459865 -4.2514787 -4.2596054 -4.2706017 -4.2759242 -4.2753758 -4.2743015][-4.2880855 -4.2832279 -4.2726989 -4.2620683 -4.2576451 -4.2552333 -4.2444372 -4.2309928 -4.2260051 -4.2312579 -4.239799 -4.2528892 -4.2633119 -4.2665658 -4.267601][-4.2553086 -4.2535954 -4.2442493 -4.2339282 -4.2333 -4.2329621 -4.2182837 -4.1983128 -4.192832 -4.1988592 -4.2084446 -4.2246737 -4.2404542 -4.2470627 -4.2474327][-4.2152457 -4.2192297 -4.2116127 -4.198875 -4.20182 -4.2056379 -4.1885886 -4.1625981 -4.1583743 -4.1692877 -4.1829104 -4.1999955 -4.2190762 -4.2256145 -4.2238679][-4.18652 -4.198318 -4.1941857 -4.1755142 -4.1748819 -4.1784611 -4.1486087 -4.1116009 -4.1133204 -4.1420269 -4.1718011 -4.1948595 -4.2174816 -4.2240467 -4.2197556][-4.16293 -4.18176 -4.1836839 -4.16359 -4.1532936 -4.1392574 -4.076973 -4.0143104 -4.0243311 -4.0806551 -4.1304059 -4.1658382 -4.200182 -4.2145677 -4.2164392][-4.1363611 -4.1576047 -4.166667 -4.1513338 -4.1277065 -4.0771918 -3.9596117 -3.8548813 -3.8920574 -3.991138 -4.0666571 -4.1220875 -4.1731625 -4.1994166 -4.2096882][-4.1047416 -4.1316538 -4.1449742 -4.1329284 -4.0923815 -3.9968095 -3.8076184 -3.6539767 -3.7506993 -3.9070723 -4.0056362 -4.0795131 -4.1458216 -4.1827512 -4.1976061][-4.0975909 -4.1329727 -4.1513743 -4.1372857 -4.0835738 -3.9696193 -3.7609119 -3.5972111 -3.7312365 -3.9110997 -4.01345 -4.0824862 -4.144599 -4.1846762 -4.1963429][-4.1260133 -4.1635323 -4.184608 -4.17289 -4.1293173 -4.0472174 -3.9038205 -3.8106875 -3.8991525 -4.0231671 -4.0978732 -4.1452513 -4.1858244 -4.2125163 -4.2139][-4.1729231 -4.2035446 -4.2221723 -4.2174053 -4.188838 -4.1451244 -4.0684719 -4.0303597 -4.0718026 -4.1339779 -4.178719 -4.2107153 -4.2342615 -4.2447186 -4.237432][-4.2194371 -4.2426844 -4.2538214 -4.2473059 -4.2256455 -4.2009964 -4.1603589 -4.1460958 -4.1684113 -4.2000175 -4.2341418 -4.2614994 -4.2748604 -4.2743754 -4.2628288][-4.2569962 -4.2738051 -4.275671 -4.2600856 -4.2347465 -4.2112827 -4.1794381 -4.1668262 -4.1861525 -4.215354 -4.2524467 -4.2819362 -4.2921791 -4.2880073 -4.2770624][-4.2757683 -4.2839351 -4.2789741 -4.2604313 -4.2349639 -4.2078781 -4.1746693 -4.1613116 -4.1799765 -4.2125764 -4.2492986 -4.2785811 -4.2893496 -4.2871075 -4.2806039][-4.2659993 -4.270699 -4.2643256 -4.2520862 -4.232677 -4.2069826 -4.1759634 -4.1632342 -4.1792779 -4.213428 -4.2459407 -4.2706089 -4.2832913 -4.2854447 -4.2828221]]...]
INFO - root - 2017-12-05 21:23:31.838602: step 44910, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 72h:41m:33s remains)
INFO - root - 2017-12-05 21:23:41.254008: step 44920, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 74h:27m:37s remains)
INFO - root - 2017-12-05 21:23:50.681496: step 44930, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 74h:32m:17s remains)
INFO - root - 2017-12-05 21:23:59.903694: step 44940, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 74h:29m:13s remains)
INFO - root - 2017-12-05 21:24:09.191804: step 44950, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 73h:11m:47s remains)
INFO - root - 2017-12-05 21:24:18.688930: step 44960, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 75h:23m:37s remains)
INFO - root - 2017-12-05 21:24:27.901618: step 44970, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 76h:57m:44s remains)
INFO - root - 2017-12-05 21:24:37.286809: step 44980, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 75h:18m:24s remains)
INFO - root - 2017-12-05 21:24:46.841459: step 44990, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 75h:17m:33s remains)
INFO - root - 2017-12-05 21:24:56.122784: step 45000, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 72h:41m:04s remains)
2017-12-05 21:24:56.969625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2471833 -4.2283745 -4.2057471 -4.1933184 -4.1962295 -4.2065048 -4.2147675 -4.2200465 -4.2227035 -4.2294741 -4.2415991 -4.2489948 -4.2497678 -4.2500014 -4.2587967][-4.2397809 -4.2209344 -4.2010574 -4.1902819 -4.1916051 -4.1987743 -4.2061291 -4.2064424 -4.2037363 -4.2069645 -4.2162428 -4.2230368 -4.225749 -4.2271814 -4.2358065][-4.2291622 -4.2063947 -4.1875091 -4.1730728 -4.1692019 -4.1724806 -4.180151 -4.1800928 -4.1819143 -4.1905417 -4.1979208 -4.20229 -4.2114825 -4.2192669 -4.2278638][-4.2154288 -4.1885214 -4.1708021 -4.1536713 -4.1395044 -4.1387234 -4.1535335 -4.1591535 -4.1664519 -4.1831918 -4.193531 -4.1917419 -4.1957483 -4.2013092 -4.2113442][-4.2153835 -4.188005 -4.1729951 -4.1595273 -4.1396 -4.128037 -4.1394238 -4.1482658 -4.1594982 -4.179275 -4.1869445 -4.1772251 -4.1745424 -4.1805353 -4.1923814][-4.2264619 -4.2026615 -4.186162 -4.1627126 -4.1327877 -4.1112394 -4.1143904 -4.1279411 -4.1411638 -4.1527572 -4.1536689 -4.1374359 -4.1318426 -4.1437182 -4.1673265][-4.2282705 -4.2044191 -4.1786857 -4.1365895 -4.0900936 -4.0586395 -4.058094 -4.0773606 -4.0973606 -4.1056871 -4.1060662 -4.0926437 -4.0876341 -4.1095381 -4.1428885][-4.20834 -4.1839724 -4.1567097 -4.1065435 -4.0514483 -4.0171385 -4.0156083 -4.0447726 -4.0746288 -4.0898623 -4.0996151 -4.0915961 -4.0824308 -4.1016097 -4.1325517][-4.1926408 -4.1729012 -4.162044 -4.128686 -4.0851331 -4.0564909 -4.0494776 -4.07017 -4.0960789 -4.1095219 -4.1239371 -4.1228004 -4.1118407 -4.1222792 -4.145906][-4.2087207 -4.2024159 -4.2137589 -4.2024288 -4.1690855 -4.137805 -4.1190448 -4.1229992 -4.1344604 -4.1386719 -4.1486874 -4.1484222 -4.1398115 -4.1517057 -4.18068][-4.2308354 -4.232635 -4.2575192 -4.2597466 -4.234374 -4.1996608 -4.1711307 -4.1625123 -4.168633 -4.1724129 -4.1778722 -4.1732845 -4.1632996 -4.1815176 -4.2174463][-4.2306695 -4.2355828 -4.2647142 -4.2701817 -4.2563276 -4.2312388 -4.209271 -4.1997666 -4.2054739 -4.210362 -4.2084117 -4.1996803 -4.1934872 -4.2126479 -4.2418385][-4.2217274 -4.2248149 -4.249187 -4.2561297 -4.2503333 -4.238358 -4.2289438 -4.2273517 -4.2327595 -4.2304177 -4.2182546 -4.2088041 -4.2021184 -4.2123566 -4.233829][-4.2240462 -4.2263036 -4.2427711 -4.2476153 -4.2450471 -4.2406678 -4.2396884 -4.2382994 -4.2373629 -4.226912 -4.2107625 -4.2012715 -4.1922603 -4.1972189 -4.2133408][-4.241961 -4.2463565 -4.2549558 -4.2533817 -4.2461858 -4.2408051 -4.2418613 -4.2403488 -4.2359037 -4.2225337 -4.2056665 -4.1972957 -4.1909156 -4.1965327 -4.2114797]]...]
INFO - root - 2017-12-05 21:25:06.304501: step 45010, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 75h:07m:20s remains)
INFO - root - 2017-12-05 21:25:15.783256: step 45020, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 75h:00m:01s remains)
INFO - root - 2017-12-05 21:25:25.062591: step 45030, loss = 2.03, batch loss = 1.97 (8.3 examples/sec; 0.968 sec/batch; 77h:19m:26s remains)
INFO - root - 2017-12-05 21:25:34.516816: step 45040, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.978 sec/batch; 78h:06m:59s remains)
INFO - root - 2017-12-05 21:25:43.832212: step 45050, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 73h:47m:05s remains)
INFO - root - 2017-12-05 21:25:53.244323: step 45060, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 74h:06m:10s remains)
INFO - root - 2017-12-05 21:26:02.687686: step 45070, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 76h:23m:45s remains)
INFO - root - 2017-12-05 21:26:12.021852: step 45080, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 75h:48m:42s remains)
INFO - root - 2017-12-05 21:26:21.390600: step 45090, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 78h:53m:54s remains)
INFO - root - 2017-12-05 21:26:31.049788: step 45100, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.007 sec/batch; 80h:21m:52s remains)
2017-12-05 21:26:31.804968: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0768461 -4.0390148 -4.0148172 -4.0213819 -4.0619388 -4.0961828 -4.1257396 -4.143877 -4.1467023 -4.1478224 -4.147953 -4.1479459 -4.1522989 -4.158021 -4.1613812][-4.0485988 -3.9970512 -3.9593561 -3.9608223 -4.0082664 -4.0444117 -4.0723443 -4.0910692 -4.0898008 -4.0887661 -4.0899658 -4.0938053 -4.1034822 -4.1105862 -4.1111569][-4.0810709 -4.037498 -4.0011344 -3.9914105 -4.019877 -4.0382357 -4.0541511 -4.06749 -4.0615115 -4.0579557 -4.05869 -4.0678763 -4.0847483 -4.0933452 -4.0920987][-4.1242275 -4.0957413 -4.0722947 -4.0526981 -4.0494795 -4.0430403 -4.0482397 -4.05792 -4.0509934 -4.0479665 -4.0505543 -4.0621538 -4.0787697 -4.0821919 -4.0776086][-4.1478872 -4.1180325 -4.0922613 -4.0585241 -4.0294271 -4.0098505 -4.01263 -4.0241385 -4.0225372 -4.0237455 -4.0374985 -4.0595388 -4.0833521 -4.0825572 -4.06665][-4.1560707 -4.1131773 -4.0741787 -4.0165725 -3.9633598 -3.9384723 -3.9439816 -3.9539428 -3.9551706 -3.9654694 -3.9983728 -4.0478549 -4.0989089 -4.1120238 -4.0938067][-4.150691 -4.0954938 -4.0429163 -3.9627359 -3.8886991 -3.8595595 -3.8587592 -3.8540215 -3.844254 -3.8577769 -3.9131513 -3.9947581 -4.0790048 -4.1216373 -4.1222992][-4.1540713 -4.097219 -4.03894 -3.9508181 -3.8652418 -3.8333597 -3.8250074 -3.797179 -3.7650392 -3.77317 -3.8405166 -3.9433322 -4.0513511 -4.1223431 -4.149312][-4.17255 -4.1291609 -4.0832376 -4.0162706 -3.9484196 -3.9247384 -3.9080691 -3.8574262 -3.8043115 -3.8010883 -3.8606663 -3.9625444 -4.07485 -4.1555591 -4.1916828][-4.1860814 -4.1532979 -4.1203165 -4.0852103 -4.0515451 -4.0497251 -4.0465121 -4.0037508 -3.9558 -3.9431953 -3.9783745 -4.0523891 -4.1381474 -4.2026777 -4.2337127][-4.1891217 -4.1600013 -4.13427 -4.1246467 -4.1174221 -4.13224 -4.1495161 -4.1327181 -4.1030574 -4.0886216 -4.1004515 -4.1396861 -4.1907473 -4.2324352 -4.25184][-4.1825962 -4.1505947 -4.1256814 -4.1269021 -4.1332226 -4.1584849 -4.1931691 -4.1984625 -4.18599 -4.1758347 -4.1762671 -4.1891818 -4.2100253 -4.2313061 -4.2406306][-4.1771293 -4.1389713 -4.1128674 -4.1177754 -4.1338263 -4.1626997 -4.2011786 -4.2139769 -4.2105727 -4.2045236 -4.2035847 -4.2064767 -4.2119622 -4.2213583 -4.225503][-4.1721997 -4.1309156 -4.104495 -4.1122971 -4.1390128 -4.168582 -4.2014852 -4.2148113 -4.214848 -4.2124734 -4.2134037 -4.2160869 -4.219583 -4.2263317 -4.2294436][-4.1835713 -4.1454964 -4.1233096 -4.1326361 -4.1608663 -4.1862593 -4.2092957 -4.2187243 -4.2191339 -4.2183809 -4.2208323 -4.2244072 -4.2303667 -4.2372642 -4.2401261]]...]
INFO - root - 2017-12-05 21:26:41.367910: step 45110, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.980 sec/batch; 78h:13m:53s remains)
INFO - root - 2017-12-05 21:26:50.790213: step 45120, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 78h:23m:56s remains)
INFO - root - 2017-12-05 21:27:00.354962: step 45130, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 71h:26m:39s remains)
INFO - root - 2017-12-05 21:27:09.635750: step 45140, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 70h:37m:54s remains)
INFO - root - 2017-12-05 21:27:19.023122: step 45150, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 73h:40m:59s remains)
INFO - root - 2017-12-05 21:27:28.774486: step 45160, loss = 2.06, batch loss = 2.00 (7.2 examples/sec; 1.111 sec/batch; 88h:40m:24s remains)
INFO - root - 2017-12-05 21:27:38.099063: step 45170, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 76h:25m:18s remains)
INFO - root - 2017-12-05 21:27:47.611358: step 45180, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 76h:24m:45s remains)
INFO - root - 2017-12-05 21:27:57.128636: step 45190, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 72h:56m:54s remains)
INFO - root - 2017-12-05 21:28:06.577485: step 45200, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 73h:50m:16s remains)
2017-12-05 21:28:07.312188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1812139 -4.1845069 -4.2138638 -4.2434072 -4.2578 -4.2669139 -4.2587857 -4.235168 -4.2040277 -4.1821709 -4.1690497 -4.1588159 -4.1524525 -4.1475778 -4.1412106][-4.1747551 -4.1881704 -4.2240176 -4.2574463 -4.2692738 -4.2703362 -4.2549715 -4.227694 -4.1963129 -4.1711292 -4.1502609 -4.1333551 -4.1225972 -4.1270304 -4.1367221][-4.1875305 -4.2100472 -4.2448983 -4.2758818 -4.2815719 -4.2728057 -4.2519774 -4.2253957 -4.1979208 -4.1693354 -4.1447678 -4.1301551 -4.1264377 -4.1431742 -4.1664453][-4.2138228 -4.2346077 -4.2570262 -4.275332 -4.27346 -4.2626114 -4.24191 -4.2179713 -4.1932988 -4.1654525 -4.1437831 -4.1371779 -4.1454792 -4.1735129 -4.2049632][-4.2294121 -4.2439313 -4.2520709 -4.2529254 -4.2390919 -4.2261367 -4.21126 -4.1934252 -4.171916 -4.1506734 -4.1367254 -4.1398025 -4.1618991 -4.2022328 -4.2367768][-4.2368784 -4.2402596 -4.2319374 -4.2126231 -4.183208 -4.1638284 -4.1569118 -4.1505589 -4.139163 -4.1295877 -4.126482 -4.1390162 -4.1721354 -4.2185082 -4.2490139][-4.2333956 -4.2206316 -4.1951909 -4.1608548 -4.1221991 -4.09758 -4.0954132 -4.1001458 -4.1046042 -4.1081915 -4.1167307 -4.1389623 -4.1761742 -4.2197051 -4.243021][-4.2122121 -4.1892343 -4.1568041 -4.1205359 -4.0857368 -4.0622406 -4.0606933 -4.0698462 -4.0836005 -4.0966077 -4.1143122 -4.1447644 -4.1834674 -4.2199211 -4.2342491][-4.192802 -4.17036 -4.1388512 -4.1068954 -4.0801182 -4.0634618 -4.0637789 -4.0718503 -4.0865812 -4.1031246 -4.1249795 -4.1611776 -4.2005105 -4.2258763 -4.2258849][-4.192821 -4.1716084 -4.1421461 -4.1142111 -4.0962968 -4.088131 -4.0863557 -4.0891075 -4.1000552 -4.1174541 -4.1420522 -4.1812186 -4.2157373 -4.2264905 -4.2113795][-4.20139 -4.1792593 -4.1500382 -4.1244159 -4.1138926 -4.1131358 -4.110713 -4.1080747 -4.1154418 -4.1325173 -4.1572876 -4.1937184 -4.22075 -4.2212443 -4.1955042][-4.1996808 -4.1778412 -4.1491828 -4.1270652 -4.1216702 -4.1235909 -4.1209197 -4.118701 -4.1262221 -4.1415548 -4.1615582 -4.1897979 -4.2104907 -4.2071271 -4.1800103][-4.1965952 -4.1736441 -4.1445141 -4.1239734 -4.1226859 -4.1270819 -4.126338 -4.1287074 -4.13727 -4.1475825 -4.158442 -4.173316 -4.1860213 -4.1805282 -4.1575832][-4.202909 -4.1777158 -4.1499648 -4.1320267 -4.1318684 -4.1376519 -4.141633 -4.1498251 -4.1599121 -4.1665745 -4.1676025 -4.1686468 -4.1716866 -4.1661105 -4.1523204][-4.2251735 -4.1978235 -4.1751819 -4.1651917 -4.1699729 -4.1788416 -4.1853447 -4.1941609 -4.2018633 -4.2032061 -4.1960683 -4.1874237 -4.1845222 -4.1811423 -4.17748]]...]
INFO - root - 2017-12-05 21:28:16.425349: step 45210, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 75h:27m:51s remains)
INFO - root - 2017-12-05 21:28:25.834494: step 45220, loss = 2.03, batch loss = 1.98 (8.3 examples/sec; 0.960 sec/batch; 76h:36m:52s remains)
INFO - root - 2017-12-05 21:28:35.262864: step 45230, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 76h:19m:30s remains)
INFO - root - 2017-12-05 21:28:44.764697: step 45240, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 78h:47m:11s remains)
INFO - root - 2017-12-05 21:28:54.270271: step 45250, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 75h:01m:48s remains)
INFO - root - 2017-12-05 21:29:03.413064: step 45260, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 72h:08m:00s remains)
INFO - root - 2017-12-05 21:29:12.778950: step 45270, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 76h:02m:50s remains)
INFO - root - 2017-12-05 21:29:22.123923: step 45280, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 76h:54m:28s remains)
INFO - root - 2017-12-05 21:29:31.712824: step 45290, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 76h:30m:43s remains)
INFO - root - 2017-12-05 21:29:40.868894: step 45300, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 72h:35m:02s remains)
2017-12-05 21:29:41.654000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.148334 -4.1332288 -4.138072 -4.1585755 -4.1769094 -4.1618423 -4.1140175 -4.0576382 -4.0394263 -4.0667682 -4.1261921 -4.1948009 -4.2546506 -4.297791 -4.3245783][-4.1660271 -4.1542335 -4.1579313 -4.1752658 -4.1898155 -4.1675067 -4.1074576 -4.0396852 -4.0163984 -4.0461531 -4.1102252 -4.1839809 -4.248961 -4.2954993 -4.3227143][-4.2099676 -4.2022414 -4.2055387 -4.2143636 -4.2122841 -4.170877 -4.0919476 -4.0154791 -3.9963276 -4.038013 -4.1081395 -4.1821356 -4.24671 -4.291676 -4.3189507][-4.2429252 -4.2348485 -4.2392297 -4.2365808 -4.2076592 -4.1380854 -4.0375185 -3.9595168 -3.96167 -4.0230913 -4.0998478 -4.1765552 -4.2407069 -4.2834506 -4.3125196][-4.2436895 -4.2294569 -4.228168 -4.2159705 -4.1613636 -4.0614543 -3.9304676 -3.8413663 -3.8708537 -3.9603667 -4.0574651 -4.150147 -4.2240767 -4.2703276 -4.303607][-4.2011933 -4.1809483 -4.1764212 -4.1571403 -4.0826917 -3.951865 -3.7764587 -3.6669688 -3.7296624 -3.862494 -3.9923072 -4.1099119 -4.2004328 -4.2569566 -4.295979][-4.1234913 -4.1168923 -4.1192875 -4.0939236 -4.0060925 -3.8531787 -3.6532645 -3.5383015 -3.632462 -3.7921467 -3.9422913 -4.0773411 -4.1804242 -4.2482958 -4.2939367][-4.0894284 -4.1026669 -4.1152167 -4.0962439 -4.0179243 -3.8795719 -3.7134814 -3.6202266 -3.6858907 -3.8088703 -3.9411063 -4.0684247 -4.1738062 -4.2483711 -4.2975016][-4.1226244 -4.1407113 -4.1570415 -4.1518965 -4.0973177 -3.9965594 -3.8846593 -3.8134508 -3.8401279 -3.9056773 -3.9920659 -4.0909748 -4.183795 -4.2558494 -4.3025537][-4.17536 -4.187264 -4.2006531 -4.2000318 -4.1593976 -4.0859666 -4.0101266 -3.9554088 -3.9620631 -4.0006309 -4.0591106 -4.1335554 -4.207911 -4.2695079 -4.308639][-4.2064967 -4.2162113 -4.2275391 -4.2309475 -4.2023692 -4.1524477 -4.0988159 -4.0512338 -4.0478888 -4.0735388 -4.1159315 -4.17452 -4.2348647 -4.2845492 -4.3155808][-4.2309642 -4.2381697 -4.252728 -4.2613072 -4.2419682 -4.2116365 -4.1733704 -4.127861 -4.1126847 -4.1261344 -4.1565514 -4.2054977 -4.2558241 -4.2966013 -4.3222785][-4.2565527 -4.2626047 -4.2770658 -4.2846818 -4.2741985 -4.25615 -4.225421 -4.1817575 -4.1619606 -4.1674204 -4.1892934 -4.2300577 -4.2709365 -4.3047309 -4.3266397][-4.26861 -4.2738161 -4.2851849 -4.2902088 -4.2881093 -4.2788367 -4.2552123 -4.2175803 -4.2029271 -4.2043667 -4.2185445 -4.2501335 -4.2812691 -4.3082576 -4.3275905][-4.27103 -4.2741828 -4.2792335 -4.2783189 -4.27665 -4.2712154 -4.2544823 -4.2269344 -4.2192731 -4.2254043 -4.2368584 -4.2608995 -4.2850547 -4.3062663 -4.3236923]]...]
INFO - root - 2017-12-05 21:29:50.881593: step 45310, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 73h:58m:45s remains)
INFO - root - 2017-12-05 21:30:00.563941: step 45320, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 76h:50m:18s remains)
INFO - root - 2017-12-05 21:30:09.911832: step 45330, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 74h:35m:47s remains)
INFO - root - 2017-12-05 21:30:19.374796: step 45340, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 74h:33m:01s remains)
INFO - root - 2017-12-05 21:30:28.883451: step 45350, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 79h:25m:08s remains)
INFO - root - 2017-12-05 21:30:38.380605: step 45360, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 74h:23m:55s remains)
INFO - root - 2017-12-05 21:30:47.650571: step 45370, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 74h:11m:09s remains)
INFO - root - 2017-12-05 21:30:57.195159: step 45380, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.957 sec/batch; 76h:17m:32s remains)
INFO - root - 2017-12-05 21:31:06.626568: step 45390, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 77h:18m:02s remains)
INFO - root - 2017-12-05 21:31:15.732143: step 45400, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 70h:24m:19s remains)
2017-12-05 21:31:16.453641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2131968 -4.1963239 -4.1824594 -4.18049 -4.1937041 -4.2081323 -4.2243814 -4.2413945 -4.2382751 -4.2138243 -4.1744032 -4.1408272 -4.1331143 -4.1385975 -4.14227][-4.2335424 -4.2164712 -4.1986289 -4.1921377 -4.2044673 -4.22225 -4.2439585 -4.2627921 -4.25989 -4.2377505 -4.2023754 -4.1772084 -4.1761026 -4.1807652 -4.1813073][-4.2572227 -4.238276 -4.2183704 -4.2120996 -4.2217073 -4.2379255 -4.2597575 -4.2772894 -4.2748079 -4.2559328 -4.22158 -4.199122 -4.2007704 -4.2048073 -4.1989093][-4.2635169 -4.2432709 -4.2243781 -4.218493 -4.2193742 -4.2255778 -4.2439418 -4.2610321 -4.2611794 -4.2428856 -4.2093949 -4.1945257 -4.2036934 -4.2117405 -4.1981797][-4.2545757 -4.2319775 -4.2078586 -4.1874413 -4.1634212 -4.1544042 -4.1765528 -4.2028389 -4.2072887 -4.1845326 -4.1543579 -4.1547952 -4.1789865 -4.1946397 -4.1767268][-4.2187219 -4.1919785 -4.1571279 -4.1085558 -4.0496707 -4.0240059 -4.0656033 -4.1156111 -4.1252985 -4.0930085 -4.0641418 -4.0912437 -4.1396976 -4.1673679 -4.1597738][-4.163425 -4.137496 -4.0964422 -4.0259328 -3.9361355 -3.9063325 -3.9753635 -4.0499215 -4.0681324 -4.0263038 -4.0000339 -4.0502367 -4.1202931 -4.1602755 -4.1697469][-4.1300182 -4.1059909 -4.0662851 -3.9923213 -3.9026268 -3.8826566 -3.9611082 -4.0420074 -4.0676312 -4.0329518 -4.0146275 -4.0658836 -4.1329713 -4.1737041 -4.1906595][-4.123064 -4.1089768 -4.0826435 -4.0290618 -3.971838 -3.9660859 -4.0284991 -4.0945182 -4.1235914 -4.1068883 -4.0989294 -4.1314578 -4.1743603 -4.2038922 -4.2194867][-4.1420207 -4.1387372 -4.1281018 -4.1009912 -4.0762482 -4.0794835 -4.1194615 -4.1638589 -4.1873088 -4.183239 -4.1845884 -4.2037463 -4.2258687 -4.2413926 -4.2520633][-4.1687188 -4.1715612 -4.17079 -4.1626968 -4.1574812 -4.1644764 -4.1889443 -4.215992 -4.23217 -4.2342381 -4.2419529 -4.2546878 -4.2651215 -4.2708116 -4.2744193][-4.1894188 -4.1947069 -4.2002234 -4.2040086 -4.207767 -4.2142944 -4.22942 -4.2429018 -4.251472 -4.25551 -4.2622867 -4.2726331 -4.2795186 -4.2824788 -4.2823429][-4.2089381 -4.2128906 -4.217762 -4.2241344 -4.2303123 -4.234446 -4.2434912 -4.2509336 -4.2569628 -4.2611675 -4.264833 -4.2729316 -4.2805696 -4.285728 -4.2877069][-4.2224655 -4.2246103 -4.2291436 -4.2359881 -4.241189 -4.2443457 -4.2500868 -4.2568197 -4.2625914 -4.2669492 -4.2711539 -4.2790046 -4.2875152 -4.2939253 -4.2970133][-4.2350054 -4.237257 -4.2417545 -4.2479 -4.2527771 -4.255096 -4.2589207 -4.2654686 -4.2725997 -4.2784214 -4.2842331 -4.2931347 -4.3017726 -4.3069224 -4.3092737]]...]
INFO - root - 2017-12-05 21:31:25.884629: step 45410, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 75h:37m:14s remains)
INFO - root - 2017-12-05 21:31:34.853488: step 45420, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 75h:44m:11s remains)
INFO - root - 2017-12-05 21:31:44.404580: step 45430, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.974 sec/batch; 77h:39m:09s remains)
INFO - root - 2017-12-05 21:31:53.937330: step 45440, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 75h:23m:39s remains)
INFO - root - 2017-12-05 21:32:03.129548: step 45450, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 75h:43m:29s remains)
INFO - root - 2017-12-05 21:32:12.437873: step 45460, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 71h:13m:07s remains)
INFO - root - 2017-12-05 21:32:22.024413: step 45470, loss = 2.06, batch loss = 2.01 (7.9 examples/sec; 1.018 sec/batch; 81h:12m:18s remains)
INFO - root - 2017-12-05 21:32:31.626417: step 45480, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 77h:56m:34s remains)
INFO - root - 2017-12-05 21:32:40.792583: step 45490, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 74h:25m:12s remains)
INFO - root - 2017-12-05 21:32:50.274610: step 45500, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.958 sec/batch; 76h:24m:36s remains)
2017-12-05 21:32:51.047745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2238746 -4.2207475 -4.2267013 -4.2361917 -4.239635 -4.2388344 -4.2381186 -4.2372866 -4.2334218 -4.2257085 -4.2131228 -4.2019229 -4.1921406 -4.1814189 -4.1743731][-4.2237234 -4.2236867 -4.2339683 -4.2436805 -4.2404885 -4.2281661 -4.2153554 -4.2103419 -4.2122445 -4.2187915 -4.2187552 -4.21485 -4.2081227 -4.1969619 -4.1847091][-4.2231541 -4.2219462 -4.2309184 -4.238205 -4.2296529 -4.2072687 -4.183373 -4.1732507 -4.1815543 -4.2040749 -4.2215533 -4.228776 -4.2260046 -4.215054 -4.2007484][-4.2249885 -4.219193 -4.2237339 -4.2267761 -4.2142863 -4.18657 -4.1546936 -4.1367006 -4.1472411 -4.1838965 -4.2158794 -4.2321835 -4.2329183 -4.2240176 -4.2125053][-4.2288876 -4.2156186 -4.2100463 -4.2066007 -4.1904578 -4.1607404 -4.1230559 -4.0943475 -4.1049976 -4.1535616 -4.1984982 -4.2235093 -4.22648 -4.2174706 -4.2098761][-4.2352171 -4.2151403 -4.1986732 -4.188251 -4.1658421 -4.1285825 -4.0764766 -4.0282936 -4.0342646 -4.1008339 -4.1663132 -4.2042351 -4.2149529 -4.20887 -4.2045407][-4.2451844 -4.2236571 -4.2011833 -4.1838846 -4.1522188 -4.0994139 -4.0245991 -3.9489758 -3.9436886 -4.0308018 -4.1241565 -4.1827288 -4.208662 -4.2119045 -4.2122617][-4.2522612 -4.2364931 -4.2141328 -4.1953883 -4.1637197 -4.1077704 -4.0251703 -3.9342864 -3.9142752 -4.0033045 -4.1072717 -4.1778183 -4.2145367 -4.2270365 -4.2317877][-4.247045 -4.2383809 -4.223053 -4.2112575 -4.1934323 -4.1568251 -4.0982008 -4.0277247 -4.0029826 -4.0589747 -4.1359854 -4.1937933 -4.2262468 -4.2405581 -4.2474318][-4.2381654 -4.2377281 -4.2322054 -4.228796 -4.2259846 -4.213634 -4.1838665 -4.1386166 -4.1106811 -4.1322455 -4.1750956 -4.211122 -4.2317867 -4.2434449 -4.25078][-4.2330647 -4.2380891 -4.2397776 -4.2429376 -4.2489557 -4.2501726 -4.237452 -4.2073712 -4.177454 -4.1776085 -4.1970067 -4.2154417 -4.2246342 -4.2353082 -4.2454948][-4.2257929 -4.2304163 -4.2351036 -4.2425208 -4.2522388 -4.25785 -4.2549372 -4.2370524 -4.2084074 -4.1983771 -4.2056236 -4.2131071 -4.2152386 -4.2260027 -4.2405195][-4.216857 -4.2165389 -4.2203579 -4.2303958 -4.2406712 -4.2457457 -4.248878 -4.244164 -4.2246866 -4.2143235 -4.216929 -4.2176685 -4.2131748 -4.2222085 -4.2360578][-4.2047982 -4.1940827 -4.194293 -4.2079163 -4.2203441 -4.2236748 -4.2288384 -4.2348456 -4.22579 -4.2195258 -4.2227907 -4.2225413 -4.217567 -4.22337 -4.2324471][-4.198935 -4.17904 -4.174159 -4.1893 -4.2035437 -4.2039857 -4.2032084 -4.2081127 -4.2048545 -4.2049465 -4.214263 -4.2177262 -4.2152309 -4.2199049 -4.2280188]]...]
INFO - root - 2017-12-05 21:33:00.652151: step 45510, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 77h:05m:29s remains)
INFO - root - 2017-12-05 21:33:09.957771: step 45520, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 77h:08m:13s remains)
INFO - root - 2017-12-05 21:33:19.345754: step 45530, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 78h:44m:35s remains)
INFO - root - 2017-12-05 21:33:28.916736: step 45540, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.017 sec/batch; 81h:06m:01s remains)
INFO - root - 2017-12-05 21:33:38.060025: step 45550, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 74h:07m:52s remains)
INFO - root - 2017-12-05 21:33:47.709333: step 45560, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 74h:43m:09s remains)
INFO - root - 2017-12-05 21:33:56.959066: step 45570, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 78h:35m:32s remains)
INFO - root - 2017-12-05 21:34:06.433063: step 45580, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 78h:56m:50s remains)
INFO - root - 2017-12-05 21:34:15.858727: step 45590, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 72h:35m:35s remains)
INFO - root - 2017-12-05 21:34:25.070782: step 45600, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 76h:12m:05s remains)
2017-12-05 21:34:25.931493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.255044 -4.2648296 -4.2766304 -4.2851853 -4.2966428 -4.3054829 -4.3071456 -4.3046684 -4.3017945 -4.3007727 -4.3040304 -4.3097811 -4.3137279 -4.3151488 -4.3129568][-4.2198062 -4.2136111 -4.2193909 -4.2346439 -4.2602005 -4.2828646 -4.290606 -4.2894425 -4.287477 -4.2862058 -4.2902856 -4.2978616 -4.3032618 -4.3047485 -4.2996478][-4.1972466 -4.1686244 -4.1560774 -4.1652927 -4.1968684 -4.229557 -4.2465773 -4.2523704 -4.2567735 -4.2617025 -4.2716331 -4.282865 -4.2908731 -4.2940817 -4.2897711][-4.2007823 -4.1567826 -4.1240845 -4.1167 -4.1337276 -4.1565709 -4.1721039 -4.1849775 -4.2039123 -4.2269731 -4.2500987 -4.2671785 -4.2793808 -4.2857118 -4.286397][-4.2094717 -4.167985 -4.1298685 -4.1025858 -4.0840755 -4.0667405 -4.0522494 -4.0545139 -4.0943065 -4.1530137 -4.2023444 -4.2310548 -4.2485733 -4.2603121 -4.2700453][-4.2040358 -4.1756554 -4.1485124 -4.1092672 -4.0515652 -3.9766161 -3.8937933 -3.8470731 -3.9031954 -4.0104175 -4.0982018 -4.1475668 -4.1728406 -4.1887665 -4.2099075][-4.1987319 -4.1955624 -4.192431 -4.1587663 -4.0832438 -3.9730282 -3.8289976 -3.7117519 -3.7654052 -3.9024084 -4.0126824 -4.0699415 -4.0920219 -4.1028218 -4.1265912][-4.1968017 -4.2197886 -4.241643 -4.2280769 -4.1718497 -4.0788741 -3.949245 -3.8309124 -3.8443325 -3.9370623 -4.0174437 -4.0530062 -4.0555682 -4.0518556 -4.0643997][-4.160502 -4.1992669 -4.2425289 -4.2582383 -4.2405496 -4.1881032 -4.1053433 -4.0225515 -4.00523 -4.0345211 -4.0635042 -4.0642953 -4.0462065 -4.0292196 -4.0249448][-4.1011047 -4.1382256 -4.1988134 -4.2461557 -4.264647 -4.2449889 -4.1987472 -4.1474056 -4.1184416 -4.1077862 -4.1012692 -4.0849686 -4.0629182 -4.0449028 -4.0299158][-4.0554132 -4.0734048 -4.1351342 -4.1983676 -4.2383466 -4.2434006 -4.2303467 -4.2055831 -4.1765943 -4.1472626 -4.1284223 -4.1170926 -4.1055098 -4.09981 -4.0881872][-4.0697112 -4.0620871 -4.1023817 -4.1497531 -4.188643 -4.2096848 -4.2248125 -4.2264261 -4.2043977 -4.1693869 -4.1485171 -4.1465497 -4.1473508 -4.1597409 -4.1644154][-4.13595 -4.1145329 -4.1324282 -4.1512203 -4.1734948 -4.1929126 -4.222712 -4.2419443 -4.2279005 -4.1923585 -4.1730723 -4.1763535 -4.1854057 -4.2068458 -4.2238445][-4.2181387 -4.2007427 -4.2055421 -4.2062712 -4.21296 -4.2217021 -4.2460675 -4.2631907 -4.2556272 -4.226716 -4.2123804 -4.2184062 -4.2283945 -4.2472773 -4.2679348][-4.2836103 -4.2784791 -4.2778325 -4.2698684 -4.267004 -4.2670918 -4.2798576 -4.290607 -4.2861528 -4.2661023 -4.2581615 -4.2646742 -4.2716327 -4.2832608 -4.2984819]]...]
INFO - root - 2017-12-05 21:34:35.547958: step 45610, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 78h:21m:37s remains)
INFO - root - 2017-12-05 21:34:44.911462: step 45620, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 77h:17m:04s remains)
INFO - root - 2017-12-05 21:34:54.195431: step 45630, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 76h:59m:15s remains)
INFO - root - 2017-12-05 21:35:03.460406: step 45640, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 77h:07m:41s remains)
INFO - root - 2017-12-05 21:35:12.738739: step 45650, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 77h:20m:54s remains)
INFO - root - 2017-12-05 21:35:22.214163: step 45660, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 73h:38m:41s remains)
INFO - root - 2017-12-05 21:35:31.462294: step 45670, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 74h:25m:27s remains)
INFO - root - 2017-12-05 21:35:40.908289: step 45680, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 76h:14m:12s remains)
INFO - root - 2017-12-05 21:35:50.539497: step 45690, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 79h:42m:19s remains)
INFO - root - 2017-12-05 21:35:59.953544: step 45700, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 79h:36m:27s remains)
2017-12-05 21:36:00.718960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3389969 -4.3416867 -4.3362408 -4.3254662 -4.3116555 -4.2979021 -4.2890778 -4.290556 -4.2959971 -4.2988539 -4.2997394 -4.3003273 -4.3004975 -4.2996283 -4.2982354][-4.3285375 -4.3291173 -4.3175125 -4.2985759 -4.2777829 -4.2576504 -4.24409 -4.2457795 -4.25497 -4.2612467 -4.2660174 -4.2701697 -4.2728105 -4.2736149 -4.2730675][-4.3077397 -4.3019528 -4.2795196 -4.2513294 -4.2267094 -4.2033396 -4.1879869 -4.1905885 -4.2035594 -4.2143478 -4.2256165 -4.2343259 -4.2387767 -4.2388244 -4.2379489][-4.2866616 -4.2717633 -4.2358012 -4.1981773 -4.1718073 -4.146203 -4.1303506 -4.1359029 -4.1534352 -4.1694794 -4.1890306 -4.2014928 -4.2060752 -4.2052293 -4.20374][-4.268075 -4.2457519 -4.198875 -4.1526179 -4.1253324 -4.1025853 -4.0891662 -4.0952806 -4.1136103 -4.133666 -4.1575866 -4.1702089 -4.171351 -4.1675267 -4.1631694][-4.2524352 -4.222959 -4.1624217 -4.1037245 -4.0745072 -4.0564847 -4.0414066 -4.0448427 -4.0629821 -4.0837383 -4.1099491 -4.1197968 -4.1155267 -4.1042461 -4.0942006][-4.2393804 -4.1985826 -4.1184421 -4.0461407 -4.01662 -3.9986176 -3.9747291 -3.9715836 -3.9896343 -4.0107503 -4.0376034 -4.0474992 -4.0422754 -4.0265102 -4.0137467][-4.2385683 -4.1915278 -4.1065125 -4.0336151 -4.0037656 -3.9794726 -3.943162 -3.9308352 -3.9401205 -3.9557741 -3.9805474 -3.993901 -3.9929402 -3.9847534 -3.9772785][-4.246398 -4.2068067 -4.1452837 -4.0962677 -4.0689278 -4.0374231 -3.9932582 -3.9736235 -3.9765167 -3.9848833 -4.002614 -4.0141935 -4.0123806 -4.0078464 -3.9970524][-4.2553916 -4.2254252 -4.1874065 -4.15811 -4.1349974 -4.1020317 -4.0606833 -4.0451026 -4.0539351 -4.0626955 -4.0711632 -4.07236 -4.0597553 -4.0469556 -4.0288706][-4.2637177 -4.2414155 -4.2195578 -4.2026086 -4.184701 -4.1580396 -4.1254168 -4.1165285 -4.1320672 -4.142735 -4.1450572 -4.138454 -4.1182837 -4.0987573 -4.0785995][-4.2714415 -4.2524157 -4.2349606 -4.2218251 -4.2068925 -4.1876826 -4.1659231 -4.164978 -4.1836009 -4.195507 -4.1969891 -4.1896534 -4.1720095 -4.1539869 -4.1370621][-4.279592 -4.2617545 -4.2438755 -4.2304883 -4.2162638 -4.198585 -4.1807022 -4.1802711 -4.1977668 -4.2094178 -4.2139988 -4.2143188 -4.2076368 -4.2023535 -4.1987667][-4.2871976 -4.269918 -4.2519727 -4.2389355 -4.2256207 -4.2077537 -4.1921496 -4.1938052 -4.2104263 -4.2208395 -4.2262454 -4.2323723 -4.233655 -4.2368197 -4.2431297][-4.2962174 -4.2796683 -4.2624836 -4.2498479 -4.2375488 -4.2209997 -4.2093682 -4.2145987 -4.2312689 -4.24039 -4.245594 -4.2522392 -4.2568288 -4.2633181 -4.2713928]]...]
INFO - root - 2017-12-05 21:36:09.933757: step 45710, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 70h:16m:00s remains)
INFO - root - 2017-12-05 21:36:19.296415: step 45720, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.000 sec/batch; 79h:38m:10s remains)
INFO - root - 2017-12-05 21:36:28.755891: step 45730, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 76h:38m:27s remains)
INFO - root - 2017-12-05 21:36:38.092424: step 45740, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 72h:47m:53s remains)
INFO - root - 2017-12-05 21:36:47.595280: step 45750, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 76h:13m:44s remains)
INFO - root - 2017-12-05 21:36:56.781656: step 45760, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 67h:50m:27s remains)
INFO - root - 2017-12-05 21:37:06.204667: step 45770, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 74h:12m:52s remains)
INFO - root - 2017-12-05 21:37:15.962205: step 45780, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 77h:22m:55s remains)
INFO - root - 2017-12-05 21:37:25.280423: step 45790, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 74h:51m:56s remains)
INFO - root - 2017-12-05 21:37:34.614288: step 45800, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 74h:35m:36s remains)
2017-12-05 21:37:35.413702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.36689 -4.3543129 -4.3245854 -4.287364 -4.2607794 -4.2396774 -4.2084126 -4.2192559 -4.2521935 -4.2602558 -4.2465019 -4.213347 -4.1810436 -4.1598024 -4.1511922][-4.3663874 -4.355763 -4.3259559 -4.2868824 -4.2587113 -4.2352824 -4.1978269 -4.2119913 -4.2500587 -4.261754 -4.2467484 -4.2092495 -4.1779075 -4.1634417 -4.166544][-4.3581891 -4.3437119 -4.3060093 -4.2629118 -4.2335739 -4.2085276 -4.1741257 -4.198462 -4.2474132 -4.2665849 -4.2523646 -4.2072554 -4.16899 -4.1519065 -4.16121][-4.3461885 -4.3236761 -4.274847 -4.2260704 -4.1899576 -4.1571388 -4.1303945 -4.1744661 -4.2397475 -4.2694578 -4.2616177 -4.2144823 -4.1666937 -4.1445432 -4.152916][-4.3352356 -4.3040633 -4.2433376 -4.1823506 -4.1273108 -4.0737672 -4.0458922 -4.1208997 -4.2126374 -4.2589774 -4.264894 -4.2250347 -4.1789947 -4.1550512 -4.1540556][-4.329205 -4.2916636 -4.2192912 -4.1386461 -4.0556946 -3.9650779 -3.9128456 -4.0192928 -4.1498871 -4.2220387 -4.2504735 -4.2318411 -4.1987553 -4.17681 -4.1572719][-4.3222733 -4.279098 -4.1963716 -4.0951629 -3.9898334 -3.8613207 -3.7686555 -3.8924286 -4.0600204 -4.1581087 -4.2123675 -4.2186728 -4.200686 -4.1774874 -4.1407232][-4.318305 -4.2736425 -4.190402 -4.0819154 -3.9701595 -3.8306768 -3.7055297 -3.8074636 -3.9783983 -4.0866461 -4.1619973 -4.192688 -4.1922059 -4.1716957 -4.1258039][-4.320425 -4.2806888 -4.208673 -4.1143184 -4.0245018 -3.913712 -3.7994411 -3.8467064 -3.9718051 -4.0662737 -4.1451616 -4.1868968 -4.1973348 -4.1877041 -4.1490664][-4.3293557 -4.2966905 -4.2367263 -4.1606154 -4.0965819 -4.018352 -3.9286118 -3.9420087 -4.020556 -4.09118 -4.1603322 -4.2027216 -4.2178235 -4.2187114 -4.1935959][-4.34321 -4.317296 -4.2684617 -4.2081566 -4.1643224 -4.110817 -4.0408158 -4.0439515 -4.0891948 -4.1303577 -4.1821179 -4.2212086 -4.2414083 -4.2571144 -4.2450171][-4.3571544 -4.3389215 -4.3013783 -4.2558942 -4.2279124 -4.1932297 -4.1409187 -4.141458 -4.1638389 -4.176157 -4.2037635 -4.2354736 -4.2599874 -4.2842555 -4.2814021][-4.3628082 -4.3482356 -4.3149505 -4.2747536 -4.2535954 -4.2315192 -4.1944003 -4.1965389 -4.20982 -4.2078257 -4.218039 -4.2411113 -4.2673817 -4.2926617 -4.2941775][-4.3580956 -4.3414216 -4.3048148 -4.2614284 -4.2400842 -4.2236552 -4.1949854 -4.2035141 -4.2181344 -4.2115726 -4.2125845 -4.2297511 -4.2550941 -4.2779379 -4.2828817][-4.3471427 -4.323391 -4.279335 -4.2282038 -4.2003736 -4.1820064 -4.1566906 -4.1746945 -4.1957173 -4.1866426 -4.1826282 -4.197433 -4.2213116 -4.2425833 -4.2510448]]...]
INFO - root - 2017-12-05 21:37:44.733314: step 45810, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 73h:00m:17s remains)
INFO - root - 2017-12-05 21:37:54.180000: step 45820, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 75h:07m:23s remains)
INFO - root - 2017-12-05 21:38:03.401265: step 45830, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 74h:45m:47s remains)
INFO - root - 2017-12-05 21:38:12.695720: step 45840, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 0.809 sec/batch; 64h:27m:10s remains)
INFO - root - 2017-12-05 21:38:21.947036: step 45850, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 75h:14m:31s remains)
INFO - root - 2017-12-05 21:38:31.359126: step 45860, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 67h:44m:10s remains)
INFO - root - 2017-12-05 21:38:40.818820: step 45870, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.925 sec/batch; 73h:36m:31s remains)
INFO - root - 2017-12-05 21:38:50.197198: step 45880, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 73h:46m:07s remains)
INFO - root - 2017-12-05 21:38:59.644627: step 45890, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.920 sec/batch; 73h:14m:13s remains)
INFO - root - 2017-12-05 21:39:08.951516: step 45900, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 73h:07m:31s remains)
2017-12-05 21:39:09.754972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.273304 -4.2724123 -4.2770305 -4.2851486 -4.289742 -4.2855272 -4.2784486 -4.2778192 -4.2785425 -4.2867875 -4.3041654 -4.3244405 -4.3423638 -4.3454404 -4.3450155][-4.216074 -4.2175512 -4.2239585 -4.238903 -4.2497768 -4.2475224 -4.2402453 -4.2413278 -4.2458243 -4.2613077 -4.2855644 -4.3110805 -4.3341107 -4.3378124 -4.337873][-4.1620116 -4.1624331 -4.1700912 -4.1900249 -4.2023954 -4.2000346 -4.1903439 -4.1914268 -4.1993504 -4.225358 -4.2571392 -4.2859082 -4.3150811 -4.3214555 -4.323874][-4.1030974 -4.1002045 -4.1060829 -4.1290379 -4.1355309 -4.1260734 -4.1116476 -4.1115594 -4.1240878 -4.1658297 -4.2125545 -4.253758 -4.2895026 -4.2995863 -4.3052387][-4.0452757 -4.0384145 -4.0450044 -4.072114 -4.0663757 -4.0466943 -4.0202909 -4.0160546 -4.0381351 -4.0970044 -4.1642227 -4.2166414 -4.2551665 -4.2718611 -4.2864528][-4.0026813 -3.9755211 -3.9761796 -3.9987791 -3.9732087 -3.9317665 -3.8798997 -3.8748055 -3.9256728 -4.0126038 -4.1029758 -4.1677818 -4.2144628 -4.2414088 -4.2661686][-3.976712 -3.9237454 -3.9131413 -3.9195876 -3.8677168 -3.800024 -3.7133913 -3.7088532 -3.8039491 -3.9227228 -4.034327 -4.1099148 -4.1669855 -4.206986 -4.2431307][-3.9616284 -3.8934166 -3.8732307 -3.859266 -3.7913351 -3.7148638 -3.6168649 -3.621963 -3.7524381 -3.8924344 -4.0133767 -4.090775 -4.1490636 -4.1928024 -4.233417][-4.0126557 -3.9571185 -3.9417191 -3.9178691 -3.845845 -3.7687545 -3.6754124 -3.6854024 -3.815321 -3.9438004 -4.0501537 -4.1168814 -4.1656132 -4.1995525 -4.2348876][-4.0944061 -4.0658312 -4.0639749 -4.0407944 -3.9829412 -3.9150786 -3.841058 -3.8577631 -3.9614053 -4.0564089 -4.1355414 -4.1817341 -4.2093215 -4.2258921 -4.2489614][-4.1459932 -4.1270118 -4.129179 -4.1133881 -4.0774655 -4.0277019 -3.9757779 -3.9978857 -4.0723958 -4.1386914 -4.1976194 -4.22925 -4.2466617 -4.2558851 -4.2692285][-4.1896915 -4.1783867 -4.1846843 -4.1803236 -4.1617179 -4.13031 -4.099874 -4.119658 -4.1628747 -4.2021484 -4.2410169 -4.2639027 -4.2761474 -4.2819 -4.2895961][-4.2195168 -4.2181382 -4.2274656 -4.2318029 -4.2277961 -4.2129693 -4.1999111 -4.2156653 -4.2398577 -4.2644687 -4.2880363 -4.3032417 -4.3099933 -4.3114882 -4.3144565][-4.2421308 -4.24261 -4.2513895 -4.2603936 -4.26513 -4.2599506 -4.2579083 -4.269866 -4.2843361 -4.299139 -4.3137841 -4.3254418 -4.3301935 -4.3308873 -4.33269][-4.2788367 -4.2775922 -4.2812595 -4.2867413 -4.2900357 -4.2865257 -4.2854433 -4.2901216 -4.2970033 -4.3062787 -4.3188763 -4.3315849 -4.3389573 -4.3411145 -4.3421659]]...]
INFO - root - 2017-12-05 21:39:19.223487: step 45910, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 72h:57m:08s remains)
INFO - root - 2017-12-05 21:39:28.723306: step 45920, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 73h:03m:56s remains)
INFO - root - 2017-12-05 21:39:38.098345: step 45930, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 71h:09m:53s remains)
INFO - root - 2017-12-05 21:39:47.346127: step 45940, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 68h:17m:10s remains)
INFO - root - 2017-12-05 21:39:56.846582: step 45950, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 71h:51m:45s remains)
INFO - root - 2017-12-05 21:40:06.370552: step 45960, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 77h:04m:40s remains)
INFO - root - 2017-12-05 21:40:15.917846: step 45970, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 76h:30m:54s remains)
INFO - root - 2017-12-05 21:40:25.105049: step 45980, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 76h:21m:47s remains)
INFO - root - 2017-12-05 21:40:34.366967: step 45990, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 74h:31m:14s remains)
INFO - root - 2017-12-05 21:40:43.939528: step 46000, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 76h:27m:44s remains)
2017-12-05 21:40:44.729799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0718408 -4.0501447 -4.0319757 -4.0189924 -4.0094304 -4.0126638 -4.0406437 -4.0649867 -4.0781527 -4.1107016 -4.1524119 -4.1813579 -4.180829 -4.1698084 -4.1567893][-4.1376276 -4.1079192 -4.079392 -4.056808 -4.0474305 -4.0576453 -4.0937886 -4.1151471 -4.1164517 -4.1367702 -4.1730595 -4.2025032 -4.2030983 -4.1900263 -4.1789303][-4.1895847 -4.1532989 -4.1137791 -4.0864768 -4.0800996 -4.0994134 -4.1435423 -4.1640291 -4.1580687 -4.1610327 -4.1789055 -4.20364 -4.2081718 -4.202076 -4.2000437][-4.213769 -4.1773672 -4.1416354 -4.1248765 -4.1239204 -4.1386929 -4.1699767 -4.1849537 -4.1779165 -4.1745977 -4.1770358 -4.1891403 -4.1982307 -4.20875 -4.2232623][-4.2288532 -4.1980815 -4.1745815 -4.1650586 -4.1624293 -4.1646857 -4.171349 -4.1729879 -4.1641908 -4.1629152 -4.16084 -4.1652088 -4.179502 -4.2040329 -4.2298818][-4.2390056 -4.2115531 -4.1918578 -4.1756387 -4.1612167 -4.1442666 -4.1234341 -4.1097927 -4.106987 -4.1185431 -4.1293082 -4.1442933 -4.1679034 -4.1925497 -4.2129326][-4.2510538 -4.2271285 -4.2006035 -4.1657677 -4.1353498 -4.0975852 -4.0466657 -4.0103631 -4.0158744 -4.0501208 -4.0909324 -4.1248517 -4.1521797 -4.1651082 -4.1645451][-4.2486343 -4.221632 -4.1890359 -4.1437726 -4.0962863 -4.0383258 -3.9602594 -3.8978102 -3.9048412 -3.9653645 -4.0293427 -4.0702434 -4.0941329 -4.0892849 -4.0721192][-4.2020478 -4.1719561 -4.1344028 -4.0886064 -4.0377994 -3.9752693 -3.8950396 -3.8253026 -3.8292196 -3.8817482 -3.9331064 -3.9665668 -3.98802 -3.9806991 -3.9640558][-4.1459265 -4.11385 -4.0738468 -4.0429559 -4.0152287 -3.9857876 -3.9461308 -3.9085536 -3.9078181 -3.9298708 -3.9510858 -3.9663653 -3.9789612 -3.9753785 -3.9660268][-4.130156 -4.1106381 -4.0823927 -4.0750751 -4.0824223 -4.0925283 -4.0916595 -4.0839391 -4.0837622 -4.088223 -4.0890627 -4.08626 -4.0837955 -4.0766916 -4.0688672][-4.1681433 -4.1592822 -4.1441064 -4.1466007 -4.1714935 -4.1981506 -4.2124925 -4.2191653 -4.2239513 -4.2238455 -4.2175837 -4.205133 -4.1920357 -4.18097 -4.1737294][-4.2175336 -4.2148952 -4.2066493 -4.2104797 -4.2352276 -4.2603369 -4.279089 -4.290576 -4.2918415 -4.2876925 -4.2815685 -4.2708116 -4.2581444 -4.2442365 -4.2351594][-4.2428923 -4.2488127 -4.2461538 -4.2479935 -4.2667661 -4.2875619 -4.30406 -4.3107705 -4.3075805 -4.30218 -4.2989264 -4.2948966 -4.2875185 -4.2778893 -4.2715211][-4.24852 -4.2569752 -4.2549472 -4.253973 -4.2663751 -4.2832308 -4.2960677 -4.2994113 -4.2962713 -4.2918983 -4.2918668 -4.2940974 -4.2920532 -4.2865362 -4.2847548]]...]
INFO - root - 2017-12-05 21:40:54.033774: step 46010, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.941 sec/batch; 74h:54m:27s remains)
INFO - root - 2017-12-05 21:41:03.657881: step 46020, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 78h:57m:58s remains)
INFO - root - 2017-12-05 21:41:13.001002: step 46030, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 76h:46m:54s remains)
INFO - root - 2017-12-05 21:41:22.319126: step 46040, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.967 sec/batch; 76h:57m:29s remains)
INFO - root - 2017-12-05 21:41:31.954881: step 46050, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 72h:53m:35s remains)
INFO - root - 2017-12-05 21:41:41.300470: step 46060, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 72h:37m:30s remains)
INFO - root - 2017-12-05 21:41:50.627037: step 46070, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 75h:35m:31s remains)
INFO - root - 2017-12-05 21:41:59.776645: step 46080, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 71h:59m:49s remains)
INFO - root - 2017-12-05 21:42:09.029114: step 46090, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 77h:45m:39s remains)
INFO - root - 2017-12-05 21:42:18.312290: step 46100, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 75h:49m:46s remains)
2017-12-05 21:42:19.119843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2049794 -4.203886 -4.2017093 -4.191905 -4.171629 -4.1400251 -4.0948095 -4.056108 -4.0561018 -4.0948868 -4.1582074 -4.2211156 -4.2563453 -4.2650518 -4.2600889][-4.2094841 -4.2159452 -4.2214279 -4.2200236 -4.2065339 -4.18103 -4.1405153 -4.108037 -4.1128416 -4.1498976 -4.2070303 -4.2559443 -4.273777 -4.2707953 -4.2570176][-4.2382436 -4.2442837 -4.24777 -4.2486892 -4.2411709 -4.2252841 -4.1969943 -4.1704278 -4.1722908 -4.1981349 -4.2420936 -4.2773237 -4.2850494 -4.2779264 -4.2622237][-4.264719 -4.265708 -4.2627006 -4.2572641 -4.2446146 -4.2251058 -4.1967659 -4.1719856 -4.1723084 -4.1976008 -4.2417259 -4.2784238 -4.2922292 -4.2911921 -4.2784567][-4.2782593 -4.272316 -4.2582111 -4.2357559 -4.202795 -4.162673 -4.1185718 -4.0907378 -4.1003313 -4.1443462 -4.2094884 -4.2638831 -4.2932639 -4.3030906 -4.2955017][-4.2756839 -4.259654 -4.2289114 -4.181109 -4.1223245 -4.0531859 -3.9806936 -3.9547513 -3.9927235 -4.0754652 -4.1731944 -4.2488308 -4.2901716 -4.307209 -4.3039713][-4.2639427 -4.2382431 -4.1908231 -4.1197557 -4.0348759 -3.9361408 -3.8386903 -3.8305318 -3.9189825 -4.0450058 -4.1619549 -4.245254 -4.2883139 -4.3051748 -4.3013783][-4.2456546 -4.2145128 -4.1654191 -4.0935645 -4.0082421 -3.911139 -3.8296893 -3.8540363 -3.9673467 -4.093874 -4.1935763 -4.2620783 -4.29489 -4.3046637 -4.2955904][-4.2216353 -4.1922474 -4.1594458 -4.1126456 -4.0573382 -4.001646 -3.9721589 -4.0140171 -4.1056023 -4.1917963 -4.2499852 -4.2887406 -4.3041258 -4.3033013 -4.2903862][-4.191535 -4.17517 -4.1668949 -4.1500931 -4.1312461 -4.1197433 -4.1310797 -4.173871 -4.2287107 -4.2686491 -4.2870512 -4.2995882 -4.3026433 -4.2957973 -4.2853022][-4.1616664 -4.1634316 -4.1719089 -4.1765981 -4.188766 -4.2118754 -4.24356 -4.2772818 -4.2971568 -4.2991538 -4.2881937 -4.282063 -4.2789907 -4.2751284 -4.2752423][-4.1555333 -4.1619282 -4.1745539 -4.1939821 -4.2278333 -4.2651172 -4.2961087 -4.3150749 -4.3108711 -4.2873731 -4.25946 -4.2464004 -4.2459297 -4.2522774 -4.2654023][-4.1616454 -4.1678061 -4.1830258 -4.2120209 -4.2524719 -4.2810812 -4.2953916 -4.2965388 -4.2758474 -4.2418156 -4.2140164 -4.2071834 -4.2185192 -4.2396197 -4.2645731][-4.1611733 -4.1711621 -4.1948814 -4.2266707 -4.2544012 -4.2619224 -4.2540116 -4.2383451 -4.2111745 -4.1817684 -4.1693134 -4.1846428 -4.2168951 -4.24983 -4.2774282][-4.1500506 -4.1700463 -4.2000113 -4.2267923 -4.2338772 -4.2156224 -4.187129 -4.1594357 -4.1381807 -4.1313148 -4.1499844 -4.1931367 -4.2403035 -4.2738647 -4.2939095]]...]
INFO - root - 2017-12-05 21:42:28.453523: step 46110, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 75h:59m:06s remains)
INFO - root - 2017-12-05 21:42:37.893927: step 46120, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 72h:24m:45s remains)
INFO - root - 2017-12-05 21:42:47.284384: step 46130, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 76h:29m:14s remains)
INFO - root - 2017-12-05 21:42:56.671648: step 46140, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 74h:52m:21s remains)
INFO - root - 2017-12-05 21:43:06.093583: step 46150, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 73h:28m:48s remains)
INFO - root - 2017-12-05 21:43:15.551887: step 46160, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.960 sec/batch; 76h:21m:35s remains)
INFO - root - 2017-12-05 21:43:24.641523: step 46170, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 76h:21m:13s remains)
INFO - root - 2017-12-05 21:43:34.144396: step 46180, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 75h:49m:16s remains)
INFO - root - 2017-12-05 21:43:43.596678: step 46190, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 76h:05m:09s remains)
INFO - root - 2017-12-05 21:43:52.947127: step 46200, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.966 sec/batch; 76h:47m:29s remains)
2017-12-05 21:43:53.758807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2400589 -4.2304692 -4.2168803 -4.2120147 -4.2077713 -4.2042813 -4.2242255 -4.252696 -4.2660112 -4.2569804 -4.2368784 -4.2046347 -4.1810403 -4.1807661 -4.1870995][-4.2320113 -4.2181482 -4.2036176 -4.2039976 -4.2063923 -4.2019949 -4.2169089 -4.2413416 -4.2452617 -4.2304554 -4.2038436 -4.1662593 -4.144351 -4.1514072 -4.1678143][-4.2241988 -4.2146544 -4.2065439 -4.2133713 -4.2193904 -4.2100229 -4.2137256 -4.2269163 -4.2209473 -4.2046351 -4.1737084 -4.13031 -4.105587 -4.1127267 -4.1312342][-4.2302823 -4.234417 -4.2355909 -4.2441487 -4.2450962 -4.2260957 -4.2138705 -4.2091413 -4.1902037 -4.1733561 -4.1431804 -4.1009388 -4.0801849 -4.0865588 -4.1045227][-4.2472625 -4.2684155 -4.2740889 -4.2734079 -4.2578287 -4.2237253 -4.1911225 -4.1666937 -4.134078 -4.122611 -4.1046515 -4.0804253 -4.0706844 -4.0781631 -4.0944414][-4.2704697 -4.2964048 -4.2952085 -4.2770681 -4.2426686 -4.1845613 -4.1194897 -4.0692797 -4.0314617 -4.0425229 -4.0565724 -4.0604157 -4.0704041 -4.0854082 -4.10707][-4.3023863 -4.3205147 -4.3072557 -4.266623 -4.2019825 -4.0974622 -3.9813938 -3.9054852 -3.8842835 -3.9489443 -4.0172186 -4.0598307 -4.0867138 -4.1072659 -4.1328988][-4.3409543 -4.349297 -4.3213868 -4.2557988 -4.1527405 -3.9898436 -3.8162029 -3.7208939 -3.7249498 -3.8549457 -3.9865904 -4.0734434 -4.12212 -4.1484 -4.1727562][-4.3638568 -4.3594379 -4.3210297 -4.2402487 -4.1230488 -3.9424131 -3.7601018 -3.6693263 -3.6890051 -3.8419864 -3.9961822 -4.1015778 -4.163465 -4.1923747 -4.2148342][-4.3560338 -4.3458056 -4.3047252 -4.2271843 -4.1256962 -3.9788749 -3.8509412 -3.806129 -3.8364477 -3.9571996 -4.0779157 -4.1633062 -4.2127666 -4.2317619 -4.2468286][-4.3357334 -4.3223338 -4.2862606 -4.2269583 -4.1542029 -4.0568819 -3.985091 -3.9759839 -4.01295 -4.0995808 -4.1821785 -4.2394257 -4.2647457 -4.2650962 -4.2672529][-4.3154736 -4.3038626 -4.2815309 -4.2451568 -4.20038 -4.1414108 -4.1042933 -4.1070318 -4.1419363 -4.2050548 -4.2628503 -4.2935886 -4.2961092 -4.2791066 -4.2669606][-4.299325 -4.2949204 -4.2887797 -4.2752872 -4.2551475 -4.2179689 -4.1917396 -4.1917834 -4.2183709 -4.2633939 -4.3042383 -4.3185883 -4.30676 -4.2794404 -4.2590046][-4.282227 -4.288125 -4.2978258 -4.3064961 -4.3029656 -4.2781377 -4.2511063 -4.2420511 -4.2580094 -4.2895312 -4.3174958 -4.3210926 -4.2992625 -4.2650995 -4.24073][-4.2649121 -4.276988 -4.2998543 -4.3206382 -4.3263106 -4.3098984 -4.2849684 -4.2694087 -4.2762289 -4.2957644 -4.3128014 -4.3076911 -4.2778807 -4.2364836 -4.2099557]]...]
INFO - root - 2017-12-05 21:44:03.315016: step 46210, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 73h:47m:21s remains)
INFO - root - 2017-12-05 21:44:12.669716: step 46220, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 73h:55m:44s remains)
INFO - root - 2017-12-05 21:44:22.098077: step 46230, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 74h:43m:59s remains)
INFO - root - 2017-12-05 21:44:31.425351: step 46240, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.943 sec/batch; 75h:00m:39s remains)
INFO - root - 2017-12-05 21:44:40.703933: step 46250, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 73h:07m:11s remains)
INFO - root - 2017-12-05 21:44:49.658732: step 46260, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 68h:48m:28s remains)
INFO - root - 2017-12-05 21:44:58.871530: step 46270, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 74h:14m:21s remains)
INFO - root - 2017-12-05 21:45:08.399805: step 46280, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 76h:03m:11s remains)
INFO - root - 2017-12-05 21:45:17.655047: step 46290, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 73h:03m:59s remains)
INFO - root - 2017-12-05 21:45:26.948211: step 46300, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 73h:40m:38s remains)
2017-12-05 21:45:27.745916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1969619 -4.1733317 -4.1517 -4.1500077 -4.1635308 -4.1698689 -4.1724663 -4.165452 -4.124053 -4.0707083 -4.077177 -4.1473026 -4.2132568 -4.2558584 -4.2747025][-4.201438 -4.1816521 -4.1556144 -4.1408644 -4.1449594 -4.1523585 -4.1513157 -4.1323848 -4.0682983 -3.9963086 -4.0022368 -4.0833807 -4.1615968 -4.2161784 -4.2440605][-4.2225223 -4.208498 -4.1787395 -4.1498795 -4.1460233 -4.1565223 -4.1567945 -4.1311378 -4.0596566 -3.9898806 -4.0000381 -4.0752554 -4.1449661 -4.1973991 -4.2254953][-4.2459359 -4.2361131 -4.2059193 -4.1705036 -4.162755 -4.1781235 -4.1848879 -4.1618848 -4.0976009 -4.0469322 -4.0598845 -4.1144109 -4.1619291 -4.1990771 -4.2228246][-4.259306 -4.2524548 -4.2266827 -4.1917024 -4.1803231 -4.1912274 -4.1981845 -4.178555 -4.1325078 -4.1096835 -4.1262441 -4.1625409 -4.190249 -4.2107849 -4.2240338][-4.2633772 -4.2584357 -4.2390542 -4.2071843 -4.1864467 -4.1814227 -4.165957 -4.1283965 -4.0979137 -4.1148858 -4.1531715 -4.1880336 -4.208673 -4.2223897 -4.2279167][-4.25691 -4.2510858 -4.2346792 -4.2063432 -4.1739097 -4.1443658 -4.0891585 -4.0098906 -3.9858356 -4.0517769 -4.1295991 -4.1787534 -4.2065105 -4.2283735 -4.2389355][-4.2445827 -4.2436538 -4.2275834 -4.19862 -4.1516681 -4.0916867 -3.98905 -3.8592522 -3.8429234 -3.9621072 -4.08235 -4.1526227 -4.1941571 -4.2310367 -4.249105][-4.2271719 -4.2330012 -4.2190666 -4.187253 -4.1320529 -4.0542307 -3.9279542 -3.7853642 -3.7853293 -3.928546 -4.0608115 -4.13754 -4.1885395 -4.229198 -4.246161][-4.2107897 -4.21622 -4.2053432 -4.1800661 -4.1336784 -4.0668254 -3.9616656 -3.858799 -3.8675818 -3.9813089 -4.0851822 -4.1477222 -4.1918006 -4.2212162 -4.2267985][-4.1947761 -4.19754 -4.1970506 -4.1921668 -4.1704268 -4.1267633 -4.0596557 -3.9998834 -4.0026546 -4.0635452 -4.1147447 -4.1482773 -4.1774464 -4.19667 -4.195981][-4.1751213 -4.1780257 -4.197361 -4.2188907 -4.218998 -4.193078 -4.1481428 -4.1083908 -4.0933385 -4.1011548 -4.0995321 -4.1063323 -4.12789 -4.14717 -4.1527042][-4.1541586 -4.15984 -4.1956115 -4.2324858 -4.2438874 -4.2262993 -4.1927032 -4.15539 -4.1220012 -4.0908327 -4.0539432 -4.0463762 -4.0662537 -4.0943384 -4.1127825][-4.1376762 -4.1381116 -4.1819134 -4.2212825 -4.2331934 -4.2239013 -4.20171 -4.167511 -4.1314673 -4.0925303 -4.0496149 -4.0369635 -4.0570703 -4.0887589 -4.1143665][-4.1199689 -4.1113153 -4.14779 -4.1789007 -4.1923008 -4.1994295 -4.1980615 -4.1852388 -4.1724224 -4.1542521 -4.1247287 -4.1078877 -4.1163368 -4.1331911 -4.1489787]]...]
INFO - root - 2017-12-05 21:45:37.035698: step 46310, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 70h:56m:12s remains)
INFO - root - 2017-12-05 21:45:46.550326: step 46320, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 73h:23m:29s remains)
INFO - root - 2017-12-05 21:45:55.866236: step 46330, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 72h:37m:07s remains)
INFO - root - 2017-12-05 21:46:05.414148: step 46340, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 78h:05m:54s remains)
INFO - root - 2017-12-05 21:46:14.780097: step 46350, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 74h:03m:48s remains)
INFO - root - 2017-12-05 21:46:24.161825: step 46360, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 76h:02m:23s remains)
INFO - root - 2017-12-05 21:46:33.517434: step 46370, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 67h:40m:33s remains)
INFO - root - 2017-12-05 21:46:42.852693: step 46380, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 75h:57m:14s remains)
INFO - root - 2017-12-05 21:46:52.287349: step 46390, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 71h:20m:31s remains)
INFO - root - 2017-12-05 21:47:01.396185: step 46400, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 67h:17m:55s remains)
2017-12-05 21:47:02.162711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3231916 -4.3124146 -4.3062825 -4.2893953 -4.261538 -4.2284989 -4.2017283 -4.1916466 -4.1932788 -4.1984959 -4.2022929 -4.2104673 -4.2258768 -4.2508826 -4.2731824][-4.3273878 -4.3184104 -4.3104992 -4.287159 -4.2517614 -4.2103305 -4.1791983 -4.1683745 -4.1664944 -4.1702571 -4.174746 -4.1884589 -4.2113976 -4.2424626 -4.268281][-4.3313522 -4.3246059 -4.3176551 -4.2935104 -4.2560296 -4.2078705 -4.1708207 -4.1604195 -4.1562953 -4.1580892 -4.1662712 -4.1859083 -4.2115331 -4.242496 -4.2687254][-4.3348203 -4.3290467 -4.3237362 -4.301641 -4.2648768 -4.2149758 -4.1720481 -4.1562209 -4.1478748 -4.1508856 -4.1671391 -4.1938605 -4.2165279 -4.2401423 -4.265255][-4.337172 -4.3307037 -4.3250837 -4.3026271 -4.2648115 -4.2096391 -4.1554842 -4.1276708 -4.1189327 -4.1342106 -4.1634493 -4.1939774 -4.2126169 -4.2301645 -4.2540979][-4.340632 -4.3318744 -4.3207116 -4.2929492 -4.2486844 -4.1853662 -4.11488 -4.0689216 -4.0639763 -4.105557 -4.1524024 -4.1848965 -4.2016277 -4.2169824 -4.2399364][-4.3477845 -4.3368807 -4.3188329 -4.2841716 -4.2332788 -4.1587644 -4.0658822 -4.000422 -4.0044045 -4.0698051 -4.1287317 -4.1698141 -4.194066 -4.2145085 -4.2363219][-4.3581185 -4.3478627 -4.3266449 -4.290659 -4.2394695 -4.1656742 -4.0631142 -3.9868329 -3.994436 -4.0585375 -4.1157084 -4.1594772 -4.1902843 -4.2151585 -4.239502][-4.366755 -4.3609524 -4.3379679 -4.3024817 -4.2560892 -4.1978865 -4.115839 -4.0509968 -4.0500612 -4.0881391 -4.121942 -4.1479588 -4.1731386 -4.2027965 -4.2331209][-4.3661065 -4.3641834 -4.3426542 -4.3105612 -4.269197 -4.223495 -4.1635904 -4.122869 -4.1206331 -4.1351914 -4.1382203 -4.1386847 -4.1546135 -4.1865449 -4.2215629][-4.3579483 -4.3571672 -4.3411427 -4.315083 -4.277885 -4.2346325 -4.1876068 -4.1666031 -4.1671739 -4.171 -4.16299 -4.1563158 -4.1663017 -4.1960969 -4.2290277][-4.3491416 -4.3505397 -4.3420277 -4.3253951 -4.2961564 -4.2572684 -4.2198429 -4.2098408 -4.2134972 -4.2133207 -4.205 -4.2028308 -4.2094193 -4.2309327 -4.257792][-4.3411522 -4.3423424 -4.3382921 -4.3288512 -4.3067932 -4.2772737 -4.2516747 -4.2467237 -4.2463536 -4.2469435 -4.2437348 -4.243763 -4.2475748 -4.26278 -4.2838259][-4.3369336 -4.3340778 -4.3292441 -4.3198953 -4.3023705 -4.2830992 -4.2681017 -4.2697821 -4.2723808 -4.273562 -4.2719107 -4.270195 -4.2711725 -4.2814713 -4.2979183][-4.3382912 -4.3318186 -4.32547 -4.3148222 -4.2994971 -4.2843747 -4.2737579 -4.2811985 -4.2893929 -4.29207 -4.2920418 -4.2902575 -4.2901654 -4.296998 -4.3082914]]...]
INFO - root - 2017-12-05 21:47:11.540974: step 46410, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 74h:49m:38s remains)
INFO - root - 2017-12-05 21:47:21.249731: step 46420, loss = 2.10, batch loss = 2.04 (7.3 examples/sec; 1.099 sec/batch; 87h:22m:00s remains)
INFO - root - 2017-12-05 21:47:30.612367: step 46430, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.962 sec/batch; 76h:26m:56s remains)
INFO - root - 2017-12-05 21:47:39.982455: step 46440, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 75h:06m:22s remains)
INFO - root - 2017-12-05 21:47:49.306207: step 46450, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 78h:15m:24s remains)
INFO - root - 2017-12-05 21:47:58.670298: step 46460, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 76h:13m:15s remains)
INFO - root - 2017-12-05 21:48:07.900979: step 46470, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.974 sec/batch; 77h:23m:53s remains)
INFO - root - 2017-12-05 21:48:17.299224: step 46480, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 75h:41m:14s remains)
INFO - root - 2017-12-05 21:48:26.645974: step 46490, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 79h:18m:34s remains)
INFO - root - 2017-12-05 21:48:35.822362: step 46500, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.967 sec/batch; 76h:50m:57s remains)
2017-12-05 21:48:36.672751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3277922 -4.3276744 -4.3239026 -4.3182511 -4.3127933 -4.3046527 -4.2966142 -4.2910123 -4.28769 -4.2900257 -4.2957354 -4.3003416 -4.3030529 -4.3041062 -4.3082962][-4.3223062 -4.32114 -4.3179522 -4.3112674 -4.3030868 -4.2887411 -4.2725897 -4.2625208 -4.2584777 -4.2624159 -4.2725115 -4.2782168 -4.2791681 -4.2763147 -4.2798347][-4.315516 -4.3134575 -4.3101273 -4.3010592 -4.2860451 -4.2608056 -4.2324486 -4.2164912 -4.2159524 -4.2257495 -4.2457876 -4.2555594 -4.2533836 -4.2449026 -4.2460871][-4.3087525 -4.3020883 -4.2941413 -4.2785664 -4.2513933 -4.2108145 -4.16855 -4.147871 -4.1585021 -4.1811647 -4.2173595 -4.2369256 -4.2352014 -4.224237 -4.2232633][-4.3065434 -4.2931876 -4.2765679 -4.2507849 -4.2059436 -4.1476688 -4.0876632 -4.0605779 -4.0859995 -4.132803 -4.194427 -4.2348504 -4.2418666 -4.2334118 -4.228085][-4.3047414 -4.2817326 -4.2516093 -4.206315 -4.1346979 -4.0523062 -3.9715679 -3.941422 -3.9915218 -4.0716019 -4.1641779 -4.2315426 -4.2519035 -4.2490582 -4.2410445][-4.2992759 -4.2659378 -4.2174315 -4.1489782 -4.0474153 -3.9385087 -3.8423226 -3.8150394 -3.8932767 -4.0072269 -4.1286988 -4.217402 -4.2492514 -4.2528429 -4.2466884][-4.2929 -4.2491608 -4.1856155 -4.0996823 -3.9803228 -3.8575451 -3.760169 -3.7349732 -3.8176408 -3.9506285 -4.0873342 -4.1845069 -4.2245851 -4.2366419 -4.2380371][-4.2935147 -4.2449417 -4.1762652 -4.08761 -3.9736681 -3.8607681 -3.775825 -3.7480531 -3.8156452 -3.9415445 -4.0711703 -4.1667819 -4.2117929 -4.2263665 -4.2312984][-4.29803 -4.2536016 -4.1924052 -4.1178818 -4.0285068 -3.9399712 -3.8692989 -3.8438261 -3.8959827 -4.0010529 -4.1073847 -4.1889544 -4.2276735 -4.2330637 -4.2321563][-4.3032579 -4.2660828 -4.2195091 -4.1675673 -4.1079817 -4.0483022 -3.9933772 -3.971873 -4.0169411 -4.0976496 -4.1771398 -4.2353559 -4.2567096 -4.25071 -4.2401109][-4.3100314 -4.2815938 -4.2484722 -4.2139959 -4.1784239 -4.1456442 -4.1077685 -4.0924392 -4.1273417 -4.1833797 -4.2382545 -4.2763281 -4.2854695 -4.2719626 -4.2550316][-4.3207312 -4.30142 -4.2778397 -4.2559695 -4.2380867 -4.2241139 -4.2020473 -4.1893263 -4.210197 -4.2436104 -4.2768893 -4.2989459 -4.301609 -4.2874684 -4.2694373][-4.3286595 -4.3165197 -4.3001823 -4.2855711 -4.2785921 -4.2754569 -4.26583 -4.2582374 -4.2693992 -4.2872705 -4.3039165 -4.3126822 -4.3110189 -4.2997565 -4.2858219][-4.3333015 -4.3267655 -4.3163252 -4.3069644 -4.3041053 -4.3056808 -4.3043933 -4.3031969 -4.3107677 -4.3192945 -4.3250847 -4.3270268 -4.3243484 -4.3165526 -4.3093967]]...]
INFO - root - 2017-12-05 21:48:45.858358: step 46510, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 75h:08m:25s remains)
INFO - root - 2017-12-05 21:48:55.091076: step 46520, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.865 sec/batch; 68h:43m:25s remains)
INFO - root - 2017-12-05 21:49:04.500211: step 46530, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 72h:30m:04s remains)
INFO - root - 2017-12-05 21:49:13.763296: step 46540, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.936 sec/batch; 74h:21m:00s remains)
INFO - root - 2017-12-05 21:49:23.164421: step 46550, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 73h:53m:32s remains)
INFO - root - 2017-12-05 21:49:32.506807: step 46560, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 75h:29m:13s remains)
INFO - root - 2017-12-05 21:49:41.867303: step 46570, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 71h:34m:08s remains)
INFO - root - 2017-12-05 21:49:51.258618: step 46580, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 74h:51m:11s remains)
INFO - root - 2017-12-05 21:50:00.656164: step 46590, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 74h:33m:52s remains)
INFO - root - 2017-12-05 21:50:10.174275: step 46600, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 73h:06m:45s remains)
2017-12-05 21:50:11.002066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1674871 -4.1868038 -4.2084894 -4.2224255 -4.2136841 -4.1855197 -4.1645451 -4.1544461 -4.1560326 -4.1823235 -4.219769 -4.2404256 -4.2480512 -4.2512507 -4.2550921][-4.1715517 -4.1873174 -4.2060852 -4.2256742 -4.2205119 -4.1920924 -4.1642742 -4.153522 -4.1545892 -4.17575 -4.2116575 -4.2367439 -4.2468925 -4.24723 -4.2462196][-4.1696253 -4.1815019 -4.1963282 -4.2194514 -4.2236338 -4.1987977 -4.1628661 -4.1438389 -4.1440487 -4.1625485 -4.1993837 -4.2269711 -4.2387047 -4.2380066 -4.2367544][-4.1771693 -4.1846719 -4.1914573 -4.2107391 -4.2196627 -4.1935835 -4.1449251 -4.1250291 -4.1385813 -4.16443 -4.20609 -4.2337637 -4.2414274 -4.2387776 -4.2386131][-4.1748133 -4.1670175 -4.1652079 -4.1847205 -4.1932058 -4.1532607 -4.0866127 -4.0738964 -4.1157026 -4.1585574 -4.2037921 -4.2335529 -4.243928 -4.2445321 -4.2508626][-4.1525383 -4.1252613 -4.1171365 -4.1381874 -4.13524 -4.0675588 -3.9640975 -3.9535766 -4.0375295 -4.108418 -4.156539 -4.1922112 -4.2215948 -4.2397695 -4.2536159][-4.1254754 -4.0823989 -4.0638256 -4.082274 -4.0664859 -3.9645734 -3.8075726 -3.7796834 -3.9171581 -4.0280714 -4.0901022 -4.135006 -4.1836658 -4.2221422 -4.2439933][-4.1060877 -4.0629745 -4.0451894 -4.0648928 -4.0519528 -3.9544749 -3.7960112 -3.7545974 -3.8992503 -4.017931 -4.0789447 -4.1170492 -4.161932 -4.2021918 -4.224236][-4.118382 -4.0917397 -4.088604 -4.1154504 -4.119154 -4.0631552 -3.9671867 -3.9320161 -4.0162439 -4.0898714 -4.1252108 -4.1482482 -4.1738849 -4.1972351 -4.2119813][-4.1515851 -4.1450696 -4.1534834 -4.18135 -4.1914082 -4.1628451 -4.1104226 -4.0789633 -4.1144042 -4.152607 -4.1745238 -4.1887221 -4.2013106 -4.2140908 -4.2211814][-4.197063 -4.1969733 -4.2067852 -4.2239795 -4.2242236 -4.2053618 -4.1758204 -4.1490631 -4.1551428 -4.1752529 -4.1950197 -4.2134557 -4.2272758 -4.2363586 -4.2406287][-4.2515488 -4.2531719 -4.2571325 -4.26065 -4.2513723 -4.2385411 -4.2200942 -4.1995554 -4.1924129 -4.2036405 -4.2248149 -4.24895 -4.26511 -4.2721057 -4.2722836][-4.2964439 -4.29791 -4.2993164 -4.2972536 -4.2849545 -4.275557 -4.2659874 -4.2572207 -4.2503428 -4.254334 -4.2697277 -4.2884812 -4.3014913 -4.3082 -4.3056555][-4.3097129 -4.3102751 -4.3102269 -4.30496 -4.2951427 -4.2911153 -4.290369 -4.2913184 -4.2896943 -4.2907171 -4.298593 -4.3077483 -4.3163657 -4.3225255 -4.3195][-4.3027849 -4.3023949 -4.3012328 -4.2966104 -4.2907615 -4.2895136 -4.2920313 -4.2957611 -4.2975221 -4.2993054 -4.303699 -4.3092632 -4.3156996 -4.319941 -4.3175769]]...]
INFO - root - 2017-12-05 21:50:20.417717: step 46610, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.950 sec/batch; 75h:26m:10s remains)
INFO - root - 2017-12-05 21:50:29.835677: step 46620, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 76h:09m:06s remains)
INFO - root - 2017-12-05 21:50:39.073059: step 46630, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 75h:25m:29s remains)
INFO - root - 2017-12-05 21:50:48.252118: step 46640, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.895 sec/batch; 71h:02m:14s remains)
INFO - root - 2017-12-05 21:50:57.689690: step 46650, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 75h:10m:56s remains)
INFO - root - 2017-12-05 21:51:07.255937: step 46660, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 72h:08m:49s remains)
INFO - root - 2017-12-05 21:51:16.468432: step 46670, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 73h:44m:34s remains)
INFO - root - 2017-12-05 21:51:25.799894: step 46680, loss = 2.06, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 80h:24m:03s remains)
INFO - root - 2017-12-05 21:51:35.111829: step 46690, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 73h:59m:33s remains)
INFO - root - 2017-12-05 21:51:44.507387: step 46700, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.958 sec/batch; 76h:02m:41s remains)
2017-12-05 21:51:45.308714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2375722 -4.193294 -4.1370649 -4.0857162 -4.0678277 -4.0960059 -4.1275406 -4.1492143 -4.1455569 -4.140533 -4.1473932 -4.1571679 -4.1517735 -4.1398129 -4.1305337][-4.2574511 -4.2204008 -4.1603703 -4.0941682 -4.0540409 -4.0600815 -4.0874195 -4.1181836 -4.1375542 -4.1515083 -4.1678948 -4.1846828 -4.1879544 -4.1827559 -4.1762638][-4.2612681 -4.2351565 -4.1801152 -4.1112361 -4.0545387 -4.0310397 -4.0470152 -4.0902052 -4.139873 -4.1809359 -4.2102809 -4.2327762 -4.2413859 -4.2382083 -4.2324877][-4.2664409 -4.2574739 -4.2139812 -4.146946 -4.0731883 -4.0137358 -4.0019903 -4.0443854 -4.1211357 -4.1919684 -4.237639 -4.2665029 -4.2782574 -4.2763247 -4.2705307][-4.2699127 -4.2750053 -4.2457156 -4.1849012 -4.1010389 -4.0123491 -3.9649873 -3.9897218 -4.0782733 -4.1694756 -4.2327256 -4.2717481 -4.2878389 -4.2840552 -4.2747793][-4.2861009 -4.2908583 -4.2670369 -4.2109118 -4.1255889 -4.0195208 -3.9382572 -3.9318981 -4.0208364 -4.1302648 -4.2110834 -4.2623138 -4.2839966 -4.2777281 -4.2645316][-4.300415 -4.2979927 -4.2720623 -4.2177157 -4.1371741 -4.0318513 -3.9406655 -3.9168875 -4.0018225 -4.1213 -4.2133889 -4.2695003 -4.2911754 -4.2802119 -4.2615161][-4.3045578 -4.29422 -4.2591238 -4.1996865 -4.1285305 -4.0476565 -3.9854236 -3.9751546 -4.0510654 -4.1604133 -4.2444167 -4.2885518 -4.2986565 -4.2778292 -4.2533731][-4.2908149 -4.2738905 -4.229722 -4.1633439 -4.1008596 -4.0535603 -4.0331759 -4.0491414 -4.119266 -4.2096958 -4.2761588 -4.30222 -4.297503 -4.2663989 -4.2343917][-4.2759109 -4.2497292 -4.1989021 -4.1253881 -4.0634336 -4.0360584 -4.0515738 -4.09622 -4.1658659 -4.2390356 -4.2900243 -4.3002849 -4.2830229 -4.239604 -4.1994452][-4.2619066 -4.2341604 -4.1831064 -4.1080985 -4.0404673 -4.0151229 -4.0489764 -4.1154962 -4.18925 -4.2523661 -4.2927508 -4.294188 -4.2671041 -4.2141519 -4.1678247][-4.2525349 -4.2295771 -4.1867781 -4.1188316 -4.0512357 -4.0234966 -4.0632725 -4.1425967 -4.2197952 -4.2764635 -4.3104544 -4.308805 -4.277966 -4.2208767 -4.1724396][-4.2491326 -4.236815 -4.2065525 -4.1516848 -4.0946531 -4.0707631 -4.112309 -4.1910291 -4.2614589 -4.3086529 -4.336637 -4.3341727 -4.3062549 -4.2554636 -4.2091904][-4.2382464 -4.2398539 -4.2265482 -4.1894608 -4.1487894 -4.1361914 -4.1727672 -4.2398539 -4.2955604 -4.3287158 -4.3472471 -4.3443913 -4.3235126 -4.2883387 -4.2535424][-4.2166128 -4.227704 -4.2243242 -4.2003145 -4.1734014 -4.1712604 -4.2043447 -4.2580123 -4.2991095 -4.320962 -4.3316326 -4.326272 -4.3091435 -4.2840776 -4.2658358]]...]
INFO - root - 2017-12-05 21:51:54.543014: step 46710, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 71h:33m:33s remains)
INFO - root - 2017-12-05 21:52:03.642096: step 46720, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 72h:57m:14s remains)
INFO - root - 2017-12-05 21:52:13.076186: step 46730, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 77h:48m:50s remains)
INFO - root - 2017-12-05 21:52:22.487713: step 46740, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 73h:53m:08s remains)
INFO - root - 2017-12-05 21:52:31.798805: step 46750, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 70h:08m:37s remains)
INFO - root - 2017-12-05 21:52:41.107132: step 46760, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 77h:50m:17s remains)
INFO - root - 2017-12-05 21:52:50.695605: step 46770, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 76h:46m:41s remains)
INFO - root - 2017-12-05 21:52:59.910547: step 46780, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 77h:24m:48s remains)
INFO - root - 2017-12-05 21:53:09.435553: step 46790, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 76h:23m:30s remains)
INFO - root - 2017-12-05 21:53:18.866716: step 46800, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 71h:18m:47s remains)
2017-12-05 21:53:19.652807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2108026 -4.2389727 -4.2732792 -4.3017206 -4.3126044 -4.3047538 -4.2934356 -4.2841392 -4.2765055 -4.2728868 -4.2740364 -4.2704048 -4.2675056 -4.2734175 -4.2726927][-4.209785 -4.2407174 -4.2743406 -4.299273 -4.3037419 -4.2901134 -4.2727175 -4.259428 -4.2491617 -4.2436438 -4.2442489 -4.2436461 -4.2445846 -4.2530541 -4.2587414][-4.2208695 -4.2464304 -4.2728004 -4.2871261 -4.2798266 -4.2602134 -4.2420177 -4.2323542 -4.2251539 -4.2229362 -4.2238517 -4.2263732 -4.2323813 -4.2437897 -4.2519131][-4.2309637 -4.2468476 -4.2634735 -4.2686944 -4.2486095 -4.2225938 -4.2034721 -4.1998949 -4.2017813 -4.2070813 -4.2110023 -4.2193384 -4.2314968 -4.2427583 -4.2490797][-4.2360578 -4.2423248 -4.2489958 -4.2471881 -4.219348 -4.1849918 -4.1570668 -4.1520381 -4.1619878 -4.1812553 -4.198802 -4.21841 -4.2347674 -4.2437749 -4.2489309][-4.2283688 -4.2209396 -4.2205009 -4.2147379 -4.1848173 -4.1434021 -4.1062469 -4.0922627 -4.1080689 -4.1418042 -4.1738772 -4.2058978 -4.2303395 -4.2435584 -4.2475805][-4.208652 -4.1922631 -4.1880279 -4.1870823 -4.164454 -4.1245909 -4.0827746 -4.0533161 -4.0582728 -4.0908012 -4.130681 -4.1723962 -4.2072649 -4.2321634 -4.2383761][-4.1867156 -4.1729336 -4.1736298 -4.1818976 -4.178616 -4.1571255 -4.1199946 -4.0790677 -4.0529385 -4.05427 -4.08495 -4.13069 -4.1760612 -4.2127695 -4.2247257][-4.1808214 -4.1746383 -4.1809731 -4.1941833 -4.2054729 -4.2084761 -4.192606 -4.1593857 -4.1176562 -4.0875568 -4.0914478 -4.1222348 -4.161696 -4.199223 -4.2151947][-4.199142 -4.1991496 -4.2039161 -4.214726 -4.2314978 -4.247025 -4.2509489 -4.2390151 -4.2053866 -4.1675649 -4.1507468 -4.1594586 -4.1787639 -4.2038774 -4.217042][-4.213913 -4.2158384 -4.217495 -4.2229891 -4.2397661 -4.2610598 -4.2777586 -4.28617 -4.270504 -4.2411256 -4.2178636 -4.2084022 -4.20855 -4.2174439 -4.2222004][-4.2203507 -4.2236347 -4.220665 -4.2175431 -4.2278552 -4.2503018 -4.2776747 -4.29831 -4.2971945 -4.27925 -4.2596593 -4.2411308 -4.2265391 -4.2200165 -4.2150493][-4.2251654 -4.2318597 -4.226953 -4.2157431 -4.2179103 -4.2400432 -4.2745643 -4.3045931 -4.31349 -4.3024387 -4.2836404 -4.2582774 -4.2306838 -4.2051592 -4.1803374][-4.2328663 -4.2420897 -4.2377348 -4.2223415 -4.2169466 -4.2329736 -4.2666059 -4.2992754 -4.3151388 -4.3113413 -4.2934785 -4.269876 -4.2351232 -4.1907487 -4.14168][-4.2415109 -4.2480159 -4.2420411 -4.2241073 -4.2127862 -4.2224846 -4.2510376 -4.2804694 -4.298326 -4.304605 -4.2948117 -4.276608 -4.2454376 -4.1959672 -4.1332693]]...]
INFO - root - 2017-12-05 21:53:29.150916: step 46810, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 71h:52m:13s remains)
INFO - root - 2017-12-05 21:53:38.349635: step 46820, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 74h:09m:18s remains)
INFO - root - 2017-12-05 21:53:47.723344: step 46830, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 75h:47m:36s remains)
INFO - root - 2017-12-05 21:53:57.100913: step 46840, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 75h:27m:49s remains)
INFO - root - 2017-12-05 21:54:06.508325: step 46850, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 74h:34m:39s remains)
INFO - root - 2017-12-05 21:54:16.108690: step 46860, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 74h:48m:31s remains)
INFO - root - 2017-12-05 21:54:25.754491: step 46870, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.986 sec/batch; 78h:15m:27s remains)
INFO - root - 2017-12-05 21:54:34.979425: step 46880, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 72h:38m:01s remains)
INFO - root - 2017-12-05 21:54:44.550971: step 46890, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.959 sec/batch; 76h:06m:28s remains)
INFO - root - 2017-12-05 21:54:53.932794: step 46900, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 70h:48m:33s remains)
2017-12-05 21:54:54.708879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2975378 -4.2863941 -4.2678509 -4.2487235 -4.2383485 -4.2367697 -4.2456956 -4.2678204 -4.286118 -4.2854252 -4.2789879 -4.26807 -4.2538371 -4.2439513 -4.2402358][-4.2850409 -4.2710314 -4.2496214 -4.22896 -4.21986 -4.2178116 -4.2253833 -4.2512312 -4.2698255 -4.2647405 -4.2554045 -4.2460694 -4.2348413 -4.2269688 -4.2249126][-4.2571197 -4.24045 -4.2181559 -4.2010803 -4.196321 -4.1974878 -4.2057667 -4.2304316 -4.2454085 -4.2369509 -4.2280393 -4.2265248 -4.2177434 -4.206264 -4.2021766][-4.2193351 -4.2017565 -4.1797543 -4.1687808 -4.1720786 -4.1745429 -4.1818314 -4.2031388 -4.216517 -4.2101679 -4.2041755 -4.205996 -4.1974807 -4.1834054 -4.1774158][-4.1724443 -4.1635275 -4.150456 -4.1466856 -4.1501365 -4.1478686 -4.1509948 -4.170104 -4.1861777 -4.1846862 -4.1817532 -4.182271 -4.16847 -4.1521263 -4.149169][-4.1240726 -4.1205573 -4.1207333 -4.1215963 -4.1212659 -4.1096883 -4.1099133 -4.129365 -4.1487679 -4.1534386 -4.1521583 -4.1519337 -4.1343641 -4.1136684 -4.1128278][-4.0969677 -4.0961 -4.1039438 -4.1045713 -4.0979323 -4.0805836 -4.0777407 -4.09436 -4.1134758 -4.1212025 -4.1216145 -4.1156292 -4.0941472 -4.0744705 -4.0732427][-4.0799918 -4.0834751 -4.10155 -4.1079574 -4.1011114 -4.0809765 -4.0721946 -4.0807486 -4.0920668 -4.1000075 -4.1019874 -4.0906215 -4.0668955 -4.0501466 -4.0444684][-4.067019 -4.0722084 -4.10052 -4.1181455 -4.116497 -4.0940375 -4.0818362 -4.0863605 -4.0915008 -4.096952 -4.0966725 -4.0816822 -4.0598497 -4.0430732 -4.0305386][-4.0764732 -4.0732007 -4.1028175 -4.13045 -4.1412554 -4.1300507 -4.1216736 -4.1212554 -4.1199574 -4.1202631 -4.1177626 -4.1024208 -4.0841665 -4.0653429 -4.0431409][-4.1204906 -4.1030889 -4.1180782 -4.1425505 -4.1580963 -4.155127 -4.1564364 -4.1553779 -4.1481967 -4.143692 -4.1440039 -4.1385098 -4.1298804 -4.1172056 -4.09061][-4.1677055 -4.1443744 -4.143393 -4.1575236 -4.1690278 -4.1671996 -4.1673293 -4.1592827 -4.1470408 -4.1437759 -4.1476007 -4.1481109 -4.1497989 -4.1475105 -4.1249762][-4.20444 -4.190001 -4.188446 -4.1901731 -4.1922607 -4.1878738 -4.1788979 -4.1574931 -4.1371222 -4.1306987 -4.1351013 -4.139883 -4.1507854 -4.1609626 -4.1498227][-4.2157259 -4.220284 -4.2328091 -4.2332449 -4.22725 -4.2166281 -4.1948504 -4.1601281 -4.1353836 -4.1269884 -4.1327777 -4.1480122 -4.1675029 -4.1814513 -4.1734138][-4.2066154 -4.2258377 -4.2509212 -4.2510457 -4.2426081 -4.2314978 -4.2053618 -4.1629024 -4.1381178 -4.13374 -4.1436234 -4.1648946 -4.1911573 -4.2052007 -4.1971507]]...]
INFO - root - 2017-12-05 21:55:04.061419: step 46910, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 73h:51m:14s remains)
INFO - root - 2017-12-05 21:55:13.425101: step 46920, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 73h:15m:30s remains)
INFO - root - 2017-12-05 21:55:22.750375: step 46930, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 71h:56m:13s remains)
INFO - root - 2017-12-05 21:55:32.219035: step 46940, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 70h:43m:26s remains)
INFO - root - 2017-12-05 21:55:41.535168: step 46950, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 68h:41m:48s remains)
INFO - root - 2017-12-05 21:55:50.882906: step 46960, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 72h:21m:03s remains)
INFO - root - 2017-12-05 21:56:00.391468: step 46970, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 74h:30m:02s remains)
INFO - root - 2017-12-05 21:56:09.812977: step 46980, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 73h:36m:31s remains)
INFO - root - 2017-12-05 21:56:19.067407: step 46990, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 64h:48m:37s remains)
INFO - root - 2017-12-05 21:56:28.551197: step 47000, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 75h:54m:00s remains)
2017-12-05 21:56:29.338178: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2107983 -4.2233291 -4.2253232 -4.2152557 -4.2053165 -4.2000813 -4.205534 -4.212018 -4.2135592 -4.2196655 -4.2303472 -4.2308431 -4.21825 -4.2096696 -4.2141166][-4.2092862 -4.2183642 -4.2197657 -4.2131805 -4.2109566 -4.2135653 -4.2221026 -4.2314777 -4.2362962 -4.2362876 -4.23788 -4.2360349 -4.2254057 -4.2161393 -4.2170863][-4.216794 -4.2160277 -4.2140393 -4.2101083 -4.2141962 -4.2231441 -4.2328458 -4.2441483 -4.251668 -4.2489896 -4.24269 -4.2363734 -4.2276754 -4.2218828 -4.2242446][-4.2248788 -4.2200189 -4.2156019 -4.2136955 -4.2188993 -4.2257452 -4.2320752 -4.2405396 -4.2484236 -4.2456517 -4.2361822 -4.2298231 -4.2236428 -4.2190056 -4.2204981][-4.2325711 -4.2240248 -4.2146773 -4.2066603 -4.204896 -4.2036057 -4.2035151 -4.2043681 -4.2119112 -4.2112079 -4.2071862 -4.2090788 -4.2060318 -4.2004352 -4.1990919][-4.2214327 -4.2094822 -4.1945772 -4.1761432 -4.1591444 -4.142621 -4.1282907 -4.1233444 -4.1299329 -4.1346235 -4.1457725 -4.16561 -4.1730995 -4.1723795 -4.174479][-4.1890903 -4.17317 -4.153966 -4.12667 -4.0912824 -4.0474062 -4.0111656 -4.0032506 -4.018662 -4.0403657 -4.0717382 -4.1104083 -4.1355147 -4.1466055 -4.1549134][-4.13742 -4.1211514 -4.102159 -4.0706019 -4.0237885 -3.9533341 -3.8918352 -3.8910136 -3.9367216 -3.9904225 -4.0392971 -4.0818782 -4.1171074 -4.1386671 -4.1518693][-4.0773864 -4.0667596 -4.0536017 -4.0264382 -3.9845767 -3.9122634 -3.8499274 -3.8690419 -3.9444921 -4.0143352 -4.0586133 -4.0912862 -4.1252036 -4.1495705 -4.1556468][-4.03645 -4.0356617 -4.0347266 -4.0191975 -3.9958661 -3.950094 -3.9165385 -3.9469976 -4.0217686 -4.0781946 -4.1051383 -4.1240649 -4.148633 -4.1633968 -4.156034][-4.0374479 -4.0451365 -4.0554829 -4.0518918 -4.0482216 -4.0292659 -4.0181236 -4.0478239 -4.106328 -4.1457977 -4.15834 -4.1643739 -4.1772633 -4.1792307 -4.1634474][-4.0667992 -4.0828114 -4.0971756 -4.1000671 -4.1060095 -4.1027145 -4.1002212 -4.1223307 -4.162024 -4.1898708 -4.1956735 -4.1910686 -4.1903877 -4.1853757 -4.1740003][-4.095365 -4.1160469 -4.1338453 -4.1411633 -4.1518974 -4.1538525 -4.148798 -4.1588106 -4.1833396 -4.2033982 -4.2075791 -4.2011423 -4.1931973 -4.1840496 -4.1785092][-4.0999374 -4.1270561 -4.148448 -4.1626744 -4.1769133 -4.1817269 -4.1736097 -4.1735973 -4.1854057 -4.2008734 -4.2090364 -4.2079411 -4.1988945 -4.1865921 -4.1833158][-4.0900173 -4.1211815 -4.1464486 -4.1668744 -4.1821628 -4.1890154 -4.1815829 -4.18002 -4.1881151 -4.201241 -4.2129021 -4.21618 -4.2083545 -4.19594 -4.1981406]]...]
INFO - root - 2017-12-05 21:56:38.777134: step 47010, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 76h:08m:06s remains)
INFO - root - 2017-12-05 21:56:48.207789: step 47020, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.986 sec/batch; 78h:11m:07s remains)
INFO - root - 2017-12-05 21:56:57.680744: step 47030, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 70h:27m:04s remains)
INFO - root - 2017-12-05 21:57:07.025553: step 47040, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 75h:56m:14s remains)
INFO - root - 2017-12-05 21:57:16.406237: step 47050, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 74h:19m:54s remains)
INFO - root - 2017-12-05 21:57:25.919815: step 47060, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.979 sec/batch; 77h:39m:36s remains)
INFO - root - 2017-12-05 21:57:35.326814: step 47070, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 78h:31m:54s remains)
INFO - root - 2017-12-05 21:57:44.390136: step 47080, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 0.793 sec/batch; 62h:51m:02s remains)
INFO - root - 2017-12-05 21:57:53.923822: step 47090, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.002 sec/batch; 79h:28m:38s remains)
INFO - root - 2017-12-05 21:58:03.296315: step 47100, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 68h:03m:21s remains)
2017-12-05 21:58:04.135057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2663178 -4.2673831 -4.2665343 -4.2677207 -4.2689085 -4.2702389 -4.2706518 -4.2696586 -4.267725 -4.2662749 -4.2653141 -4.264349 -4.2628469 -4.2569313 -4.2483525][-4.2889371 -4.2889619 -4.2887354 -4.2906766 -4.292779 -4.2941542 -4.2933669 -4.2902317 -4.2864261 -4.2840705 -4.2834773 -4.2836876 -4.28306 -4.2789268 -4.2737837][-4.3033204 -4.3030205 -4.3035579 -4.3064036 -4.3101487 -4.3109765 -4.3074069 -4.2998023 -4.291729 -4.2859988 -4.284112 -4.2850227 -4.2864938 -4.2849436 -4.2831111][-4.2858405 -4.2848005 -4.2864194 -4.29224 -4.2989492 -4.301013 -4.2971759 -4.2865257 -4.2720327 -4.2590089 -4.2506084 -4.247427 -4.2496891 -4.2505889 -4.2544546][-4.24688 -4.2464871 -4.2529445 -4.2655911 -4.2790041 -4.2838478 -4.2786946 -4.2632313 -4.2407427 -4.2176561 -4.1984363 -4.1888647 -4.189055 -4.1906891 -4.20155][-4.1742887 -4.178606 -4.1959643 -4.219655 -4.241303 -4.2500844 -4.2414589 -4.2174931 -4.18048 -4.1380095 -4.1028976 -4.0892034 -4.0942397 -4.1038771 -4.1270266][-4.0735707 -4.0843191 -4.1118078 -4.1474848 -4.1778116 -4.1875134 -4.1707115 -4.1292663 -4.0688314 -4.0018325 -3.9572158 -3.9525375 -3.9770722 -4.0077753 -4.04638][-4.007113 -4.0253844 -4.0632582 -4.1060205 -4.1354971 -4.1401339 -4.1125488 -4.0515442 -3.9675205 -3.8888602 -3.8585227 -3.8832588 -3.9352264 -3.9813993 -4.0210805][-4.0526443 -4.070209 -4.1061096 -4.1404796 -4.1596847 -4.1574435 -4.1233888 -4.0588207 -3.98379 -3.9308062 -3.9259937 -3.9605451 -4.0087037 -4.0475912 -4.0778337][-4.1666088 -4.1755238 -4.2003665 -4.2212071 -4.22827 -4.2211022 -4.1939397 -4.1541419 -4.1193509 -4.0988832 -4.0982075 -4.1134181 -4.1355853 -4.1557503 -4.1717582][-4.2580714 -4.2615433 -4.2746673 -4.2841525 -4.2826948 -4.2728963 -4.2570729 -4.2415185 -4.2325783 -4.2280254 -4.2293491 -4.2322969 -4.2363005 -4.2401276 -4.2407942][-4.30275 -4.3042054 -4.310864 -4.3144679 -4.3096194 -4.3009505 -4.2933941 -4.2884741 -4.2872109 -4.287684 -4.2914424 -4.2920389 -4.2905126 -4.2875414 -4.27791][-4.2979851 -4.3039961 -4.3100891 -4.3127317 -4.3094435 -4.3044968 -4.3015184 -4.3004451 -4.2994 -4.2990589 -4.3024635 -4.3033834 -4.3015327 -4.2955441 -4.2790909][-4.2505989 -4.2673564 -4.2726908 -4.2746167 -4.2737584 -4.272438 -4.2704058 -4.2685356 -4.2672629 -4.2678251 -4.2707958 -4.27175 -4.2686987 -4.2562113 -4.2299838][-4.1692786 -4.1935096 -4.1970358 -4.1982126 -4.1986318 -4.1995306 -4.1986256 -4.1959395 -4.1932893 -4.1929426 -4.194468 -4.1954341 -4.1933289 -4.17755 -4.1414871]]...]
INFO - root - 2017-12-05 21:58:13.350746: step 47110, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 76h:24m:47s remains)
INFO - root - 2017-12-05 21:58:22.844719: step 47120, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 1.002 sec/batch; 79h:28m:10s remains)
INFO - root - 2017-12-05 21:58:32.124828: step 47130, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 77h:27m:51s remains)
INFO - root - 2017-12-05 21:58:41.658785: step 47140, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.992 sec/batch; 78h:36m:05s remains)
INFO - root - 2017-12-05 21:58:51.217143: step 47150, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 79h:02m:40s remains)
INFO - root - 2017-12-05 21:59:00.681820: step 47160, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 76h:41m:04s remains)
INFO - root - 2017-12-05 21:59:10.067687: step 47170, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 76h:48m:35s remains)
INFO - root - 2017-12-05 21:59:19.286919: step 47180, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 76h:34m:26s remains)
INFO - root - 2017-12-05 21:59:28.721545: step 47190, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 72h:42m:28s remains)
INFO - root - 2017-12-05 21:59:37.978263: step 47200, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.986 sec/batch; 78h:09m:40s remains)
2017-12-05 21:59:38.864634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3322892 -4.3294454 -4.3241844 -4.3182096 -4.3118 -4.3092332 -4.3076415 -4.3044491 -4.3009152 -4.3000388 -4.2995105 -4.2995949 -4.3032527 -4.3102174 -4.3179078][-4.3152151 -4.3113546 -4.3039718 -4.292448 -4.2817736 -4.278779 -4.278338 -4.2735643 -4.2668018 -4.2661839 -4.2640176 -4.2585011 -4.2581706 -4.2671838 -4.2807612][-4.3044572 -4.2985806 -4.2876353 -4.2697525 -4.2536554 -4.2483687 -4.245204 -4.2330871 -4.2163992 -4.2136393 -4.2106676 -4.2013373 -4.1974907 -4.2090712 -4.2298355][-4.2919908 -4.2811913 -4.268949 -4.2473655 -4.2249293 -4.2141862 -4.2048974 -4.1805615 -4.1516929 -4.1474514 -4.1497126 -4.1421471 -4.1362705 -4.1454859 -4.1697617][-4.280509 -4.2678041 -4.2578397 -4.2369318 -4.2109179 -4.1902022 -4.1654344 -4.1177411 -4.0719686 -4.0738845 -4.0921383 -4.0916595 -4.0847697 -4.0914578 -4.119442][-4.2574124 -4.24333 -4.23494 -4.2144628 -4.182353 -4.147141 -4.0972905 -4.0110092 -3.9459074 -3.969435 -4.0190544 -4.0357637 -4.039227 -4.0532956 -4.0902047][-4.2260151 -4.2141314 -4.204155 -4.1801009 -4.1361384 -4.0811729 -3.9985337 -3.8603687 -3.7695143 -3.8297977 -3.9268372 -3.9728827 -3.9985878 -4.0296626 -4.0789185][-4.1980462 -4.1920042 -4.1809182 -4.1514163 -4.098249 -4.0259876 -3.8997922 -3.6916113 -3.5650322 -3.6789074 -3.8359392 -3.9224541 -3.977021 -4.0271564 -4.088026][-4.1786952 -4.1800909 -4.1699715 -4.1309485 -4.0732374 -4.003922 -3.8691769 -3.6437922 -3.5239551 -3.666327 -3.8404498 -3.9379089 -4.0020952 -4.0595932 -4.1213169][-4.1833563 -4.1898427 -4.1798344 -4.1418519 -4.0964584 -4.053268 -3.9542971 -3.7889025 -3.7117782 -3.8126757 -3.9406996 -4.0185103 -4.0702991 -4.1208439 -4.17343][-4.2061372 -4.2153692 -4.2055736 -4.1728578 -4.1396146 -4.1148753 -4.0492897 -3.9435472 -3.9000592 -3.9622614 -4.0486422 -4.1092048 -4.1491995 -4.1883206 -4.2273674][-4.2438827 -4.2546287 -4.2475972 -4.2220054 -4.197444 -4.181036 -4.1365194 -4.0718222 -4.0523562 -4.09429 -4.1514664 -4.1958041 -4.2237887 -4.2486358 -4.2739358][-4.2765131 -4.2852774 -4.2787561 -4.2584939 -4.2411304 -4.2343879 -4.2130756 -4.1797543 -4.1764116 -4.2059 -4.2391238 -4.2653456 -4.2804852 -4.29254 -4.305995][-4.2864428 -4.28959 -4.2836494 -4.2713552 -4.2640533 -4.2692542 -4.2649908 -4.2518911 -4.2577138 -4.2782707 -4.2948685 -4.3078361 -4.3152328 -4.3196659 -4.3255825][-4.2968411 -4.2968683 -4.2923713 -4.2870665 -4.287725 -4.2960753 -4.2974072 -4.2948856 -4.3029671 -4.3153424 -4.3231711 -4.3291488 -4.3319902 -4.3336878 -4.3364606]]...]
INFO - root - 2017-12-05 21:59:48.491013: step 47210, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 76h:34m:14s remains)
INFO - root - 2017-12-05 21:59:57.708965: step 47220, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 72h:26m:16s remains)
INFO - root - 2017-12-05 22:00:07.052674: step 47230, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 75h:07m:55s remains)
INFO - root - 2017-12-05 22:00:16.563936: step 47240, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 75h:31m:49s remains)
INFO - root - 2017-12-05 22:00:26.143069: step 47250, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 75h:46m:44s remains)
INFO - root - 2017-12-05 22:00:35.537637: step 47260, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 72h:20m:21s remains)
INFO - root - 2017-12-05 22:00:44.927629: step 47270, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 75h:10m:15s remains)
INFO - root - 2017-12-05 22:00:54.338417: step 47280, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.944 sec/batch; 74h:46m:03s remains)
INFO - root - 2017-12-05 22:01:03.590449: step 47290, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 72h:27m:09s remains)
INFO - root - 2017-12-05 22:01:12.713510: step 47300, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 76h:59m:43s remains)
2017-12-05 22:01:13.620850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2459455 -4.2443466 -4.2367468 -4.22701 -4.2316256 -4.2483106 -4.2558274 -4.2487578 -4.2370291 -4.2216578 -4.1935081 -4.1586976 -4.1324883 -4.1244159 -4.1351275][-4.2307892 -4.231225 -4.21852 -4.1970577 -4.1896582 -4.2032571 -4.2177167 -4.2192039 -4.2100821 -4.191884 -4.1608353 -4.130703 -4.1169729 -4.1200104 -4.1358943][-4.2236333 -4.2236714 -4.2062483 -4.1733818 -4.1499987 -4.1520629 -4.1680512 -4.1756229 -4.1691489 -4.1537189 -4.130229 -4.1207995 -4.1348577 -4.1567512 -4.1772666][-4.2242413 -4.22316 -4.2050185 -4.1671586 -4.1286907 -4.110755 -4.113627 -4.1152997 -4.1117072 -4.1137142 -4.1125712 -4.1304784 -4.1708817 -4.2079506 -4.2307963][-4.2358146 -4.2343149 -4.2168956 -4.1781259 -4.1277971 -4.0885835 -4.0640841 -4.0441222 -4.0436907 -4.0758429 -4.1073437 -4.1459589 -4.197978 -4.2404423 -4.2623534][-4.2511654 -4.2515883 -4.2382841 -4.2001872 -4.1413932 -4.0806355 -4.0180268 -3.964201 -3.9669487 -4.0348449 -4.0986309 -4.1480417 -4.198782 -4.2391882 -4.2583032][-4.2627134 -4.2659726 -4.2587404 -4.224875 -4.1641245 -4.0855618 -3.9873431 -3.9017863 -3.9084766 -4.006103 -4.0953569 -4.1508904 -4.19161 -4.2215648 -4.2319484][-4.2767553 -4.2788553 -4.2744045 -4.2487016 -4.1952982 -4.1136165 -4.006628 -3.9131234 -3.9198225 -4.0210466 -4.1179395 -4.173553 -4.1989369 -4.2097945 -4.2038465][-4.2954369 -4.2950368 -4.2923336 -4.2765279 -4.2369828 -4.1662712 -4.0709677 -3.9890277 -3.991262 -4.0760317 -4.1635957 -4.2124805 -4.2223568 -4.2140441 -4.19571][-4.3165622 -4.3155065 -4.3139482 -4.3038807 -4.2756133 -4.2164903 -4.1380858 -4.0740271 -4.0760236 -4.1415887 -4.2117748 -4.2516007 -4.2528434 -4.2363625 -4.2150583][-4.3331003 -4.3325229 -4.3299775 -4.3221188 -4.3024139 -4.2575393 -4.198597 -4.15415 -4.1560292 -4.203588 -4.2527642 -4.2817006 -4.2815647 -4.2685838 -4.2528234][-4.3451753 -4.3457847 -4.3405566 -4.33129 -4.3154812 -4.2859077 -4.2477098 -4.2194195 -4.22238 -4.2546 -4.2845373 -4.3045087 -4.307972 -4.3035865 -4.295392][-4.3509092 -4.3510013 -4.3427639 -4.3305321 -4.3160262 -4.2997189 -4.2799039 -4.2651229 -4.2712173 -4.2922287 -4.3084459 -4.3189793 -4.3239946 -4.3268013 -4.3247][-4.3525724 -4.3517847 -4.341126 -4.3262215 -4.3125515 -4.3072391 -4.3014951 -4.2975764 -4.3061213 -4.3199043 -4.3258438 -4.3268027 -4.3282676 -4.3325982 -4.3339787][-4.3501291 -4.3495994 -4.3399048 -4.3256493 -4.3152089 -4.3173776 -4.3223786 -4.3266807 -4.3347955 -4.3421969 -4.3402452 -4.3335958 -4.3302522 -4.3326969 -4.3355484]]...]
INFO - root - 2017-12-05 22:01:23.066537: step 47310, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 76h:54m:48s remains)
INFO - root - 2017-12-05 22:01:32.472738: step 47320, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 66h:13m:31s remains)
INFO - root - 2017-12-05 22:01:41.979996: step 47330, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 75h:13m:48s remains)
INFO - root - 2017-12-05 22:01:51.359835: step 47340, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 69h:47m:40s remains)
INFO - root - 2017-12-05 22:02:00.727690: step 47350, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 73h:26m:13s remains)
INFO - root - 2017-12-05 22:02:10.037906: step 47360, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 74h:44m:11s remains)
INFO - root - 2017-12-05 22:02:19.510423: step 47370, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 73h:52m:15s remains)
INFO - root - 2017-12-05 22:02:28.915167: step 47380, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 68h:18m:33s remains)
INFO - root - 2017-12-05 22:02:38.362542: step 47390, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 70h:51m:39s remains)
INFO - root - 2017-12-05 22:02:47.548354: step 47400, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.925 sec/batch; 73h:13m:14s remains)
2017-12-05 22:02:48.297662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1985083 -4.1850772 -4.1980405 -4.2306132 -4.2585831 -4.2655473 -4.252902 -4.2352767 -4.2261944 -4.2204752 -4.21184 -4.2073174 -4.2036114 -4.1955638 -4.168581][-4.1803575 -4.180263 -4.2021146 -4.2325244 -4.2512078 -4.2471194 -4.2275381 -4.2134986 -4.2153945 -4.2257185 -4.2328682 -4.2358 -4.233212 -4.2209768 -4.1901464][-4.1664205 -4.1765184 -4.1989632 -4.2260208 -4.239583 -4.2268443 -4.1968684 -4.1848969 -4.2021737 -4.2365103 -4.2648516 -4.2754235 -4.2726789 -4.2565975 -4.2238955][-4.1718612 -4.1941042 -4.2136216 -4.2288623 -4.2299728 -4.2031469 -4.1596751 -4.1474433 -4.1803808 -4.2374668 -4.2854648 -4.3029394 -4.2992563 -4.2803855 -4.249537][-4.1882296 -4.2216749 -4.2352519 -4.2343283 -4.2145967 -4.1610603 -4.0968328 -4.0861874 -4.1413078 -4.2200918 -4.2829676 -4.30696 -4.3044524 -4.2858448 -4.2620831][-4.20318 -4.2428436 -4.2519879 -4.2377944 -4.1934414 -4.10363 -4.0067725 -4.000598 -4.08667 -4.1916003 -4.2650733 -4.2892075 -4.2858253 -4.2745562 -4.2645068][-4.2044024 -4.2464237 -4.2544856 -4.2327433 -4.1684895 -4.0506115 -3.9291244 -3.9293094 -4.04735 -4.1686053 -4.2377148 -4.2519984 -4.2438426 -4.2439108 -4.254941][-4.1867485 -4.2232323 -4.2318583 -4.2116637 -4.1452236 -4.030838 -3.9211066 -3.9285684 -4.0471382 -4.1560836 -4.203743 -4.1935978 -4.1757941 -4.1869106 -4.2225633][-4.1805282 -4.2131224 -4.2227592 -4.2063055 -4.1484704 -4.0617442 -3.9898534 -4.0008769 -4.0818787 -4.1545677 -4.1696372 -4.1246405 -4.0868235 -4.1106553 -4.1753659][-4.2107496 -4.2422066 -4.2501984 -4.2320933 -4.1856608 -4.1287227 -4.0888143 -4.095593 -4.1308141 -4.1619363 -4.145793 -4.0718207 -4.0142479 -4.0494776 -4.138793][-4.2350698 -4.262239 -4.2688203 -4.2517557 -4.2186651 -4.1871548 -4.1714849 -4.1757689 -4.1828036 -4.1819782 -4.1432447 -4.0568705 -3.9977865 -4.03776 -4.1340542][-4.2397747 -4.2587962 -4.2625437 -4.2530723 -4.2371845 -4.2278781 -4.2293153 -4.2380219 -4.2359362 -4.2176914 -4.1652508 -4.0826921 -4.0345392 -4.0662203 -4.1433868][-4.2401066 -4.2533622 -4.2540412 -4.2498403 -4.2442355 -4.2499747 -4.262435 -4.2758446 -4.2717948 -4.2476106 -4.1948395 -4.12788 -4.09006 -4.1060467 -4.1522827][-4.243516 -4.2533703 -4.2520285 -4.2462778 -4.2404089 -4.2500362 -4.2708926 -4.2925458 -4.2916026 -4.2664456 -4.2172627 -4.165 -4.1383061 -4.1472197 -4.1659174][-4.2431707 -4.2495852 -4.2479935 -4.241106 -4.2325592 -4.2389522 -4.2613373 -4.290925 -4.2966924 -4.2761374 -4.2331276 -4.1899 -4.1720939 -4.1805749 -4.1900034]]...]
INFO - root - 2017-12-05 22:02:57.689084: step 47410, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 73h:45m:33s remains)
INFO - root - 2017-12-05 22:03:07.092994: step 47420, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 76h:15m:53s remains)
INFO - root - 2017-12-05 22:03:16.408603: step 47430, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 73h:42m:41s remains)
INFO - root - 2017-12-05 22:03:25.816768: step 47440, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 73h:29m:23s remains)
INFO - root - 2017-12-05 22:03:34.743378: step 47450, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 75h:00m:55s remains)
INFO - root - 2017-12-05 22:03:44.284379: step 47460, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 76h:59m:51s remains)
INFO - root - 2017-12-05 22:03:53.692821: step 47470, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 75h:35m:12s remains)
INFO - root - 2017-12-05 22:04:03.057566: step 47480, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.933 sec/batch; 73h:51m:28s remains)
INFO - root - 2017-12-05 22:04:12.286478: step 47490, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 75h:44m:22s remains)
INFO - root - 2017-12-05 22:04:21.838560: step 47500, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 73h:45m:13s remains)
2017-12-05 22:04:22.580125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2001939 -4.1939707 -4.1735268 -4.1557827 -4.15693 -4.168108 -4.1786704 -4.185833 -4.1874084 -4.1889811 -4.18266 -4.1717329 -4.1613855 -4.1647506 -4.1796308][-4.1788688 -4.1581993 -4.1298494 -4.1123829 -4.1158528 -4.1316533 -4.1514268 -4.1657372 -4.1766129 -4.184803 -4.1772475 -4.1639795 -4.1519537 -4.1591487 -4.1793828][-4.1491551 -4.114315 -4.0849156 -4.0740008 -4.0817933 -4.1011338 -4.1302509 -4.1538229 -4.1718645 -4.1811862 -4.16833 -4.1501403 -4.1358743 -4.1436787 -4.1669416][-4.1105795 -4.0677156 -4.0441346 -4.0441251 -4.0563154 -4.0725245 -4.1017518 -4.1334777 -4.1609282 -4.1737747 -4.1600575 -4.1347718 -4.1158781 -4.1217561 -4.1451421][-4.07339 -4.0294242 -4.0120115 -4.0207829 -4.0326157 -4.0346384 -4.0533414 -4.087338 -4.1257067 -4.1492071 -4.1428566 -4.1181812 -4.1032367 -4.1121554 -4.1379528][-4.0609922 -4.0236182 -4.0089517 -4.0135021 -4.0102639 -3.9861102 -3.9848304 -4.0141082 -4.0627217 -4.1031952 -4.1119952 -4.1009259 -4.0994053 -4.1141977 -4.1426787][-4.094511 -4.0655146 -4.0503354 -4.0433431 -4.0174532 -3.9609113 -3.9364862 -3.9572194 -4.010479 -4.0621128 -4.0829811 -4.0880408 -4.1029506 -4.1245933 -4.1533241][-4.1434793 -4.1205068 -4.10457 -4.0934858 -4.060812 -3.9963877 -3.9641054 -3.9769988 -4.0209618 -4.065661 -4.0829406 -4.0954175 -4.1195564 -4.1398516 -4.16301][-4.1884317 -4.1697969 -4.155 -4.1443658 -4.1152997 -4.0642009 -4.0374689 -4.0453377 -4.07622 -4.1066141 -4.1166358 -4.1281409 -4.1483059 -4.1606264 -4.1755018][-4.225152 -4.2104187 -4.1978703 -4.1866527 -4.1613035 -4.1275115 -4.1091413 -4.1122413 -4.1297207 -4.1487489 -4.1562529 -4.1644292 -4.1782584 -4.1848569 -4.1934557][-4.2526464 -4.2421756 -4.2328129 -4.2225728 -4.2030625 -4.1834407 -4.172935 -4.1715078 -4.1755829 -4.1821609 -4.1877036 -4.1925426 -4.19692 -4.1964779 -4.2011962][-4.2709594 -4.2627058 -4.25514 -4.24609 -4.2314711 -4.2210317 -4.2164207 -4.2136273 -4.2095113 -4.2082562 -4.2092428 -4.2065525 -4.19655 -4.1870813 -4.1916685][-4.2894378 -4.281992 -4.2727394 -4.2619109 -4.2487679 -4.2420611 -4.2403507 -4.23689 -4.2298436 -4.2252274 -4.2192078 -4.20365 -4.1779976 -4.16023 -4.1650119][-4.3071785 -4.300643 -4.2908278 -4.279108 -4.2645597 -4.2545958 -4.2489638 -4.2417636 -4.2322335 -4.2281938 -4.2171831 -4.1921325 -4.15713 -4.1346421 -4.138896][-4.3160543 -4.3115811 -4.3042083 -4.2943573 -4.2805972 -4.2675104 -4.2549076 -4.2395778 -4.224978 -4.2214842 -4.211236 -4.1858306 -4.1537175 -4.1331825 -4.1373148]]...]
INFO - root - 2017-12-05 22:04:31.846627: step 47510, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.899 sec/batch; 71h:08m:35s remains)
INFO - root - 2017-12-05 22:04:41.282337: step 47520, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 73h:47m:17s remains)
INFO - root - 2017-12-05 22:04:50.623142: step 47530, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 72h:27m:03s remains)
INFO - root - 2017-12-05 22:04:59.777583: step 47540, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 77h:18m:15s remains)
INFO - root - 2017-12-05 22:05:08.826702: step 47550, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 70h:17m:29s remains)
INFO - root - 2017-12-05 22:05:17.892044: step 47560, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 71h:58m:57s remains)
INFO - root - 2017-12-05 22:05:27.338525: step 47570, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 73h:20m:13s remains)
INFO - root - 2017-12-05 22:05:36.820240: step 47580, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 77h:00m:06s remains)
INFO - root - 2017-12-05 22:05:45.983095: step 47590, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 74h:09m:57s remains)
INFO - root - 2017-12-05 22:05:55.648655: step 47600, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.969 sec/batch; 76h:40m:46s remains)
2017-12-05 22:05:56.414292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3235731 -4.3418818 -4.3511238 -4.3274364 -4.2590895 -4.1748476 -4.0863113 -4.048151 -4.0921812 -4.15917 -4.2165837 -4.259829 -4.2918215 -4.2995629 -4.2834873][-4.3316574 -4.3520961 -4.3631139 -4.341723 -4.2782955 -4.20379 -4.1230655 -4.0935369 -4.1388187 -4.1991291 -4.2460814 -4.2780342 -4.3021908 -4.3069229 -4.2922897][-4.3394456 -4.3618274 -4.3737745 -4.3520403 -4.2900124 -4.22404 -4.157362 -4.1419945 -4.18827 -4.2389646 -4.2728605 -4.2876573 -4.2991843 -4.3029633 -4.2970915][-4.3449869 -4.3679609 -4.3794074 -4.3522873 -4.2834144 -4.2102704 -4.143084 -4.1370831 -4.193707 -4.2516026 -4.2835569 -4.292541 -4.2975831 -4.298059 -4.2996864][-4.3464317 -4.3678203 -4.3766546 -4.3383584 -4.2538967 -4.1605239 -4.0746489 -4.0686178 -4.149334 -4.2345352 -4.276927 -4.2905245 -4.2968483 -4.2970729 -4.3056631][-4.3447175 -4.3634281 -4.3682184 -4.318625 -4.2117319 -4.0878563 -3.9741349 -3.9658151 -4.0831637 -4.2039938 -4.2621565 -4.2820077 -4.2951 -4.3012505 -4.3156176][-4.3429413 -4.3586535 -4.3600183 -4.3027434 -4.1745582 -4.0125494 -3.8592589 -3.852509 -4.0161185 -4.1752892 -4.2531142 -4.2796555 -4.29708 -4.3044033 -4.3191757][-4.3415337 -4.3566942 -4.3583469 -4.3012896 -4.1636205 -3.9762375 -3.7873976 -3.7805667 -3.973371 -4.155406 -4.2413158 -4.2679281 -4.2822843 -4.2882457 -4.3038478][-4.340673 -4.3562207 -4.3613129 -4.3103704 -4.1802721 -4.0024719 -3.8262317 -3.8249412 -3.9994304 -4.1645985 -4.2353616 -4.2495313 -4.2548475 -4.2596879 -4.27774][-4.3400569 -4.3563271 -4.3655524 -4.323566 -4.2096443 -4.0606942 -3.9267831 -3.9364152 -4.0746951 -4.2015309 -4.2451286 -4.2420573 -4.2343879 -4.2346411 -4.2528291][-4.3396196 -4.3549209 -4.3651609 -4.3300114 -4.2308178 -4.1072869 -4.013092 -4.0364013 -4.1496949 -4.2417917 -4.2611818 -4.2419248 -4.2251797 -4.2233677 -4.243032][-4.3381085 -4.3509946 -4.3583069 -4.3258853 -4.2359877 -4.129374 -4.0631628 -4.0994883 -4.2013121 -4.271131 -4.2727156 -4.2372112 -4.2095571 -4.210042 -4.2393856][-4.334887 -4.3451414 -4.348002 -4.3145733 -4.2295113 -4.1367922 -4.0907412 -4.1350603 -4.2281528 -4.2848868 -4.2780051 -4.233489 -4.197206 -4.1976218 -4.2371626][-4.3311443 -4.3379817 -4.3380466 -4.3057256 -4.2309256 -4.1572075 -4.1279902 -4.1730328 -4.2490568 -4.2912416 -4.2815704 -4.2369328 -4.1978154 -4.1988692 -4.242713][-4.3267756 -4.3305078 -4.3310161 -4.30513 -4.246191 -4.1906891 -4.1736822 -4.2124743 -4.2671175 -4.2935472 -4.283576 -4.242805 -4.2078795 -4.2091188 -4.2528124]]...]
INFO - root - 2017-12-05 22:06:05.745043: step 47610, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 78h:29m:47s remains)
INFO - root - 2017-12-05 22:06:15.175345: step 47620, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 72h:22m:27s remains)
INFO - root - 2017-12-05 22:06:24.515573: step 47630, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 72h:56m:47s remains)
INFO - root - 2017-12-05 22:06:33.856076: step 47640, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 73h:42m:47s remains)
INFO - root - 2017-12-05 22:06:43.059373: step 47650, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 67h:32m:15s remains)
INFO - root - 2017-12-05 22:06:52.457939: step 47660, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 72h:29m:07s remains)
INFO - root - 2017-12-05 22:07:01.573249: step 47670, loss = 2.11, batch loss = 2.05 (9.0 examples/sec; 0.887 sec/batch; 70h:11m:20s remains)
INFO - root - 2017-12-05 22:07:11.014193: step 47680, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.930 sec/batch; 73h:35m:14s remains)
INFO - root - 2017-12-05 22:07:20.335090: step 47690, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 73h:32m:56s remains)
INFO - root - 2017-12-05 22:07:29.737926: step 47700, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.914 sec/batch; 72h:16m:40s remains)
2017-12-05 22:07:30.486649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.130682 -4.1180339 -4.1050334 -4.0917392 -4.0809355 -4.0806108 -4.0915976 -4.0996213 -4.0937209 -4.075922 -4.0551004 -4.0410094 -4.0356112 -4.043601 -4.0746012][-4.1239142 -4.1073189 -4.0902982 -4.0806766 -4.0810747 -4.0941658 -4.119051 -4.1387582 -4.1403003 -4.1187391 -4.0878482 -4.0661836 -4.0616879 -4.0722394 -4.1018367][-4.1224456 -4.107563 -4.0929937 -4.0913668 -4.1043825 -4.1252432 -4.1518884 -4.1766882 -4.1885695 -4.1777425 -4.1553421 -4.1384706 -4.1332736 -4.1362104 -4.1513062][-4.114439 -4.110992 -4.1071372 -4.1126857 -4.1266332 -4.1433697 -4.1656222 -4.189271 -4.2052016 -4.2071214 -4.198288 -4.1881437 -4.18452 -4.1846185 -4.1912289][-4.0995331 -4.1139088 -4.1250181 -4.1349134 -4.1385479 -4.135325 -4.1370392 -4.1487317 -4.1636829 -4.1768208 -4.1802487 -4.1793165 -4.1829429 -4.1918068 -4.2042732][-4.0874443 -4.1154 -4.1369429 -4.1433358 -4.125855 -4.0909719 -4.059124 -4.0510092 -4.0688839 -4.0969615 -4.114727 -4.1266022 -4.1420341 -4.1648889 -4.1901765][-4.0970788 -4.1262221 -4.1459975 -4.1411924 -4.1054096 -4.0474486 -3.9845111 -3.9530027 -3.9722195 -4.016922 -4.0475516 -4.0668225 -4.0927672 -4.1277232 -4.1629553][-4.1093416 -4.133276 -4.1472425 -4.1347108 -4.0938554 -4.0348315 -3.9643166 -3.9155676 -3.9237404 -3.9729397 -4.0104427 -4.0309615 -4.056828 -4.0943179 -4.1330771][-4.1248212 -4.1430626 -4.154439 -4.1442776 -4.1096749 -4.0655642 -4.0101104 -3.9618206 -3.9585624 -3.9958696 -4.0275483 -4.0409894 -4.0555673 -4.08454 -4.1197662][-4.1637321 -4.1768484 -4.1875 -4.1847081 -4.1617045 -4.1343732 -4.0992889 -4.0631895 -4.0542784 -4.0734 -4.092401 -4.0973744 -4.098424 -4.1130071 -4.1379142][-4.2223849 -4.2267966 -4.2322059 -4.2330337 -4.2226558 -4.2097535 -4.1928239 -4.17406 -4.1674004 -4.1706796 -4.1745386 -4.1711726 -4.1634412 -4.1665077 -4.179956][-4.2754745 -4.2738261 -4.2744274 -4.2769551 -4.2764511 -4.2750998 -4.2711692 -4.2656021 -4.2609968 -4.2519879 -4.24324 -4.2345276 -4.2253132 -4.2249036 -4.2324157][-4.3049588 -4.3001204 -4.2982535 -4.3018737 -4.3068628 -4.3121848 -4.3159738 -4.317214 -4.3134689 -4.3002934 -4.2865338 -4.2769208 -4.2695761 -4.2706413 -4.2774115][-4.3193974 -4.3155222 -4.3135314 -4.3154407 -4.3203197 -4.3266325 -4.3332553 -4.3378944 -4.33743 -4.3273573 -4.3157167 -4.30737 -4.3024592 -4.3047786 -4.3097219][-4.3286848 -4.3283916 -4.3282313 -4.3291559 -4.3313775 -4.3355169 -4.3401794 -4.3444128 -4.3461194 -4.3419895 -4.335609 -4.3303452 -4.3272882 -4.32843 -4.3295031]]...]
INFO - root - 2017-12-05 22:07:39.867527: step 47710, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.922 sec/batch; 72h:56m:48s remains)
INFO - root - 2017-12-05 22:07:49.155025: step 47720, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 74h:21m:53s remains)
INFO - root - 2017-12-05 22:07:58.488093: step 47730, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 71h:23m:32s remains)
INFO - root - 2017-12-05 22:08:07.781336: step 47740, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 70h:10m:19s remains)
INFO - root - 2017-12-05 22:08:17.222819: step 47750, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 73h:27m:05s remains)
INFO - root - 2017-12-05 22:08:26.606671: step 47760, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 77h:16m:19s remains)
INFO - root - 2017-12-05 22:08:35.967300: step 47770, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 71h:32m:28s remains)
INFO - root - 2017-12-05 22:08:45.235854: step 47780, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 67h:37m:11s remains)
INFO - root - 2017-12-05 22:08:54.787076: step 47790, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.017 sec/batch; 80h:26m:37s remains)
INFO - root - 2017-12-05 22:09:03.847027: step 47800, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.824 sec/batch; 65h:11m:14s remains)
2017-12-05 22:09:04.595530: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2844515 -4.2770967 -4.2613606 -4.2540607 -4.244307 -4.2286835 -4.2308545 -4.2507615 -4.2641549 -4.2688155 -4.2693152 -4.2654648 -4.2663345 -4.2666569 -4.2614732][-4.2765341 -4.2702971 -4.2556939 -4.2510381 -4.245317 -4.229053 -4.2282238 -4.2507944 -4.2700453 -4.2794371 -4.2835073 -4.2825274 -4.280283 -4.276567 -4.269012][-4.2709742 -4.26411 -4.2487893 -4.2429285 -4.2359672 -4.2159934 -4.2142444 -4.2421103 -4.2679729 -4.280982 -4.2884808 -4.2908063 -4.2880654 -4.2807655 -4.2708163][-4.2666483 -4.25724 -4.2398391 -4.2311621 -4.2208462 -4.198204 -4.198586 -4.2338462 -4.2643785 -4.2803421 -4.2911329 -4.298089 -4.2947092 -4.2818236 -4.2661753][-4.2632031 -4.2500606 -4.2299032 -4.2186146 -4.2063036 -4.1845665 -4.1895928 -4.2306333 -4.2626176 -4.2802277 -4.29424 -4.3027549 -4.2950873 -4.2760715 -4.255857][-4.2587132 -4.2417316 -4.2201905 -4.2079592 -4.1970286 -4.1804032 -4.192667 -4.2341242 -4.261188 -4.2759857 -4.2894959 -4.294507 -4.2800236 -4.2572513 -4.2373791][-4.2546916 -4.2341843 -4.2105355 -4.1977925 -4.1881213 -4.1757426 -4.1934342 -4.233489 -4.2540374 -4.2623215 -4.2702923 -4.2683897 -4.2465506 -4.2223878 -4.2070966][-4.2495718 -4.224659 -4.1969948 -4.1809397 -4.1688857 -4.1576204 -4.17902 -4.2218361 -4.2406297 -4.2439556 -4.2465839 -4.2397485 -4.2117305 -4.18694 -4.17893][-4.2433925 -4.2142372 -4.1810274 -4.1586752 -4.14211 -4.1295862 -4.1530786 -4.2017517 -4.2222948 -4.2200332 -4.2170625 -4.2083106 -4.1810803 -4.1607494 -4.160955][-4.2381258 -4.2063012 -4.1682038 -4.1397934 -4.1175604 -4.1023259 -4.1259613 -4.1788177 -4.2034154 -4.1983643 -4.1906004 -4.1824126 -4.1595588 -4.1420135 -4.1473036][-4.2351785 -4.2020378 -4.1612058 -4.1290283 -4.1023512 -4.0841246 -4.1046696 -4.1585021 -4.1842451 -4.1769547 -4.1634941 -4.1544938 -4.1361828 -4.1203184 -4.1306472][-4.2341375 -4.20005 -4.157517 -4.1227031 -4.093318 -4.0725007 -4.0881829 -4.1379757 -4.1633296 -4.1568274 -4.1415682 -4.133357 -4.1205311 -4.1091652 -4.1252751][-4.2332439 -4.1973677 -4.1538014 -4.1183004 -4.0901856 -4.0712409 -4.0851564 -4.1300697 -4.1522956 -4.1466966 -4.132772 -4.1263404 -4.1196671 -4.1142893 -4.1339412][-4.2333169 -4.1947222 -4.1502967 -4.1151938 -4.0906057 -4.07684 -4.0932617 -4.1350579 -4.1525049 -4.145257 -4.1306405 -4.1240082 -4.1218076 -4.1222816 -4.1437345][-4.2352104 -4.1942291 -4.1498222 -4.1163836 -4.0955892 -4.0869207 -4.1068368 -4.1451879 -4.1593304 -4.1523252 -4.1391459 -4.1334577 -4.1344862 -4.1387706 -4.1576176]]...]
INFO - root - 2017-12-05 22:09:13.876888: step 47810, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 72h:33m:03s remains)
INFO - root - 2017-12-05 22:09:23.223172: step 47820, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 71h:56m:35s remains)
INFO - root - 2017-12-05 22:09:32.704807: step 47830, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 77h:44m:05s remains)
INFO - root - 2017-12-05 22:09:42.062162: step 47840, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 72h:38m:31s remains)
INFO - root - 2017-12-05 22:09:51.279913: step 47850, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 74h:07m:21s remains)
INFO - root - 2017-12-05 22:10:00.687487: step 47860, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 70h:30m:24s remains)
INFO - root - 2017-12-05 22:10:10.056530: step 47870, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 74h:06m:01s remains)
INFO - root - 2017-12-05 22:10:19.254028: step 47880, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 70h:53m:35s remains)
INFO - root - 2017-12-05 22:10:28.734666: step 47890, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 73h:15m:47s remains)
INFO - root - 2017-12-05 22:10:38.092428: step 47900, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 73h:55m:40s remains)
2017-12-05 22:10:38.862757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2265277 -4.2458887 -4.261188 -4.2697549 -4.2694755 -4.2671742 -4.2665882 -4.2701631 -4.2658086 -4.2596984 -4.2563109 -4.2521706 -4.2497363 -4.2497492 -4.2435613][-4.1824079 -4.2073822 -4.2316589 -4.2449293 -4.2483196 -4.2506351 -4.2551255 -4.2670465 -4.2746768 -4.2772589 -4.2761111 -4.2724037 -4.2651105 -4.2560635 -4.2419167][-4.16111 -4.1882181 -4.2160797 -4.2300158 -4.23254 -4.2320375 -4.2335525 -4.2475805 -4.2626867 -4.2737341 -4.2777662 -4.2761807 -4.2668896 -4.252737 -4.2349281][-4.1745033 -4.1941752 -4.2145176 -4.2237577 -4.2205367 -4.2084618 -4.2011743 -4.2115779 -4.2360759 -4.2588062 -4.2678709 -4.2661853 -4.2538018 -4.235745 -4.2191391][-4.2071028 -4.2127776 -4.2168708 -4.2104282 -4.1914997 -4.1651411 -4.1456227 -4.1469555 -4.1789503 -4.2155428 -4.2326832 -4.234097 -4.2233615 -4.2040381 -4.1899776][-4.2394724 -4.2328591 -4.2233791 -4.2006783 -4.1617889 -4.1134453 -4.0717292 -4.0548224 -4.0895615 -4.1408081 -4.1741476 -4.1915035 -4.194942 -4.1829133 -4.1726532][-4.2720289 -4.2544541 -4.2303338 -4.1875381 -4.1243119 -4.0507517 -3.982615 -3.9403007 -3.9733458 -4.0456214 -4.1057811 -4.150835 -4.1799774 -4.1872754 -4.1862617][-4.2870612 -4.2586279 -4.2168164 -4.1540031 -4.0663095 -3.9677553 -3.8767502 -3.8059976 -3.8368292 -3.9316428 -4.0230389 -4.101985 -4.1638389 -4.200182 -4.2161098][-4.2856884 -4.2523971 -4.2018385 -4.13376 -4.0427155 -3.9460373 -3.8587222 -3.7890477 -3.8124919 -3.8995311 -3.9952664 -4.0882816 -4.1681914 -4.2234631 -4.2540836][-4.2819271 -4.2530785 -4.2067437 -4.1488714 -4.0765643 -4.0075622 -3.9507618 -3.9063022 -3.9222105 -3.9803803 -4.0532746 -4.133234 -4.208436 -4.2637219 -4.2970591][-4.2805948 -4.262403 -4.2293186 -4.1881976 -4.1372433 -4.0958 -4.0709367 -4.0535312 -4.0693164 -4.1075487 -4.1538072 -4.2061405 -4.2590008 -4.2997169 -4.3257275][-4.2937274 -4.2856092 -4.2684407 -4.2448778 -4.217505 -4.1983037 -4.1922717 -4.191751 -4.2042255 -4.2242956 -4.2474685 -4.274169 -4.3027277 -4.3252912 -4.3402638][-4.3125424 -4.31083 -4.3051658 -4.2975607 -4.2889371 -4.2834063 -4.2846289 -4.2888322 -4.2968121 -4.304512 -4.3109989 -4.3197532 -4.3311605 -4.3408909 -4.3473487][-4.3268819 -4.3280959 -4.3278823 -4.3288279 -4.3302932 -4.3306437 -4.333293 -4.3364348 -4.3403497 -4.3429813 -4.3433671 -4.3443708 -4.3471642 -4.3504972 -4.3527718][-4.334034 -4.335669 -4.3376417 -4.3417988 -4.3478203 -4.3500681 -4.3506165 -4.3505416 -4.3518906 -4.3533964 -4.3541393 -4.3544774 -4.3550205 -4.3557396 -4.3561878]]...]
INFO - root - 2017-12-05 22:10:48.074563: step 47910, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 70h:46m:43s remains)
INFO - root - 2017-12-05 22:10:57.521712: step 47920, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 70h:31m:33s remains)
INFO - root - 2017-12-05 22:11:06.991440: step 47930, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 75h:21m:32s remains)
INFO - root - 2017-12-05 22:11:16.258276: step 47940, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.923 sec/batch; 72h:57m:07s remains)
INFO - root - 2017-12-05 22:11:25.419232: step 47950, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.899 sec/batch; 71h:04m:58s remains)
INFO - root - 2017-12-05 22:11:34.836207: step 47960, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 71h:31m:36s remains)
INFO - root - 2017-12-05 22:11:44.206544: step 47970, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 73h:22m:07s remains)
INFO - root - 2017-12-05 22:11:53.480195: step 47980, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 72h:11m:51s remains)
INFO - root - 2017-12-05 22:12:02.751512: step 47990, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 74h:57m:11s remains)
INFO - root - 2017-12-05 22:12:12.281962: step 48000, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 76h:46m:08s remains)
2017-12-05 22:12:13.074116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2060118 -4.1913691 -4.191421 -4.1930027 -4.195569 -4.1866679 -4.1704254 -4.15596 -4.1521897 -4.1581964 -4.1536083 -4.1426234 -4.1367249 -4.1321774 -4.1282053][-4.1929531 -4.1871309 -4.1895194 -4.1864371 -4.185194 -4.1793175 -4.1675177 -4.1486712 -4.1332207 -4.1325312 -4.1313739 -4.1266518 -4.1206584 -4.1124048 -4.1092858][-4.1833367 -4.1845403 -4.1852179 -4.1784177 -4.1741796 -4.1717567 -4.1679897 -4.1513271 -4.1312871 -4.12842 -4.126997 -4.1191678 -4.1111779 -4.1039114 -4.1055613][-4.17912 -4.1815476 -4.1778374 -4.1666646 -4.1598563 -4.1603284 -4.162405 -4.1562128 -4.1430368 -4.1424041 -4.1375771 -4.1208706 -4.1134987 -4.112493 -4.1171179][-4.1646008 -4.1626072 -4.1556516 -4.1438003 -4.1324286 -4.1258979 -4.1266894 -4.1296215 -4.1317172 -4.1380696 -4.1275997 -4.1055608 -4.101675 -4.1043544 -4.1053591][-4.1459851 -4.1440744 -4.1397872 -4.1233864 -4.0998878 -4.0745115 -4.0643263 -4.0658236 -4.076148 -4.08923 -4.0789061 -4.0522709 -4.0486541 -4.0590558 -4.0629964][-4.1292849 -4.1332183 -4.1357479 -4.1147795 -4.0763817 -4.0332131 -4.0077448 -4.0042505 -4.0172071 -4.0345798 -4.026062 -4.0052314 -4.0076532 -4.0252085 -4.0250678][-4.119204 -4.1287866 -4.1411414 -4.1221561 -4.0847983 -4.0429888 -4.0144677 -4.0052009 -4.0127578 -4.0248165 -4.0208044 -4.01464 -4.0248952 -4.0395856 -4.0276694][-4.123467 -4.1356235 -4.1534004 -4.1403613 -4.1159143 -4.09164 -4.0749741 -4.0681615 -4.0656781 -4.0689197 -4.0633259 -4.063447 -4.0735397 -4.082201 -4.0655355][-4.1251478 -4.1345963 -4.1486759 -4.1445179 -4.1419411 -4.1381307 -4.1358719 -4.1360726 -4.1362495 -4.1361675 -4.1297436 -4.1305666 -4.1387548 -4.1466579 -4.1386533][-4.12429 -4.123486 -4.130888 -4.1374788 -4.1519928 -4.1628432 -4.1692696 -4.17724 -4.1862803 -4.1873531 -4.1807375 -4.1803446 -4.1900096 -4.2005577 -4.19963][-4.1375833 -4.1259446 -4.1268888 -4.1409545 -4.1599574 -4.1736255 -4.1824169 -4.1963391 -4.207015 -4.2084508 -4.2065034 -4.2073073 -4.2159 -4.2255278 -4.2273746][-4.1677618 -4.1507912 -4.145606 -4.1564536 -4.1667366 -4.1722713 -4.1772962 -4.1862717 -4.1902256 -4.1891832 -4.1918583 -4.189394 -4.1948972 -4.2029595 -4.2087264][-4.1891193 -4.1709309 -4.1610284 -4.1645217 -4.1621017 -4.1569362 -4.1549721 -4.154284 -4.149364 -4.1483669 -4.156395 -4.1532745 -4.1538343 -4.15932 -4.1671915][-4.196744 -4.1803284 -4.1711626 -4.170898 -4.1601214 -4.147438 -4.1382718 -4.1267409 -4.1106691 -4.109663 -4.1256518 -4.12669 -4.1248517 -4.1263471 -4.1313043]]...]
INFO - root - 2017-12-05 22:12:22.465654: step 48010, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 74h:06m:38s remains)
INFO - root - 2017-12-05 22:12:31.614044: step 48020, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 71h:05m:10s remains)
INFO - root - 2017-12-05 22:12:41.004828: step 48030, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.898 sec/batch; 70h:58m:40s remains)
INFO - root - 2017-12-05 22:12:50.346709: step 48040, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 75h:54m:54s remains)
INFO - root - 2017-12-05 22:12:59.784671: step 48050, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 75h:00m:27s remains)
INFO - root - 2017-12-05 22:13:09.165408: step 48060, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 72h:35m:18s remains)
INFO - root - 2017-12-05 22:13:18.623235: step 48070, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.970 sec/batch; 76h:39m:19s remains)
INFO - root - 2017-12-05 22:13:27.740473: step 48080, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.836 sec/batch; 66h:03m:24s remains)
INFO - root - 2017-12-05 22:13:36.968241: step 48090, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 75h:10m:11s remains)
INFO - root - 2017-12-05 22:13:46.393630: step 48100, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 72h:10m:32s remains)
2017-12-05 22:13:47.164337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2955213 -4.3022351 -4.3064909 -4.3028965 -4.2958636 -4.2838335 -4.2709017 -4.2650928 -4.2605929 -4.2546 -4.2537637 -4.2525415 -4.2425451 -4.22651 -4.210043][-4.3111649 -4.3184514 -4.31863 -4.3092089 -4.2945695 -4.2761397 -4.2587171 -4.2523403 -4.2522264 -4.2510996 -4.2508731 -4.2478905 -4.2378693 -4.217937 -4.1908937][-4.3156471 -4.3179445 -4.3107953 -4.295248 -4.27495 -4.2506986 -4.2306137 -4.2233844 -4.2248898 -4.2282009 -4.2306166 -4.2277279 -4.2152958 -4.1910534 -4.1600232][-4.3039527 -4.2997108 -4.2845526 -4.2642994 -4.2392797 -4.210187 -4.1905723 -4.1865931 -4.1905189 -4.1991296 -4.2029586 -4.2010131 -4.1895256 -4.1619396 -4.1330419][-4.2909646 -4.2871475 -4.2694683 -4.2452445 -4.2130518 -4.1732516 -4.1479769 -4.1495852 -4.1660986 -4.1778145 -4.1798573 -4.1785445 -4.1717615 -4.1450233 -4.1205215][-4.2756925 -4.272933 -4.2547536 -4.2288957 -4.1923075 -4.1380577 -4.0929279 -4.095562 -4.1296296 -4.1463184 -4.1484222 -4.148767 -4.1451092 -4.1241546 -4.1047807][-4.2572279 -4.2501016 -4.2255511 -4.1890044 -4.1411543 -4.0712013 -4.0016255 -4.0073256 -4.0657291 -4.0917354 -4.0938249 -4.1023636 -4.1127419 -4.1059842 -4.0939965][-4.2429342 -4.2264895 -4.1907191 -4.1364417 -4.0785108 -4.0053849 -3.9213018 -3.9331028 -4.0155149 -4.0485597 -4.0504231 -4.0687208 -4.0960364 -4.1047106 -4.0996561][-4.2351804 -4.2156587 -4.1767912 -4.1192236 -4.0717344 -4.0216045 -3.9564652 -3.9732516 -4.0470042 -4.0717406 -4.069139 -4.0884709 -4.1198153 -4.132185 -4.1250715][-4.2459912 -4.23119 -4.2002268 -4.1543431 -4.1242228 -4.0984015 -4.0608597 -4.0727534 -4.1186137 -4.1316862 -4.1287847 -4.142488 -4.1624279 -4.16666 -4.15411][-4.2630372 -4.25401 -4.2310705 -4.195498 -4.1758957 -4.1649017 -4.1456342 -4.1524658 -4.176497 -4.1844606 -4.1837869 -4.1881685 -4.1898918 -4.1859393 -4.1733785][-4.2707758 -4.2665181 -4.2495804 -4.2230134 -4.2084637 -4.200604 -4.1897926 -4.1919217 -4.20213 -4.2081637 -4.2108421 -4.2089963 -4.1977592 -4.1891537 -4.1798482][-4.2765446 -4.2762141 -4.2643676 -4.2430048 -4.2304859 -4.2226267 -4.2139087 -4.2115779 -4.2133322 -4.2186646 -4.2247024 -4.2217026 -4.209044 -4.19991 -4.1945453][-4.2722712 -4.27103 -4.2617426 -4.2445922 -4.2351928 -4.2303085 -4.2246943 -4.2226572 -4.2212052 -4.2256055 -4.2350912 -4.2351704 -4.2262993 -4.2204571 -4.2161036][-4.2651062 -4.2630048 -4.2581606 -4.2493663 -4.2454014 -4.2441893 -4.2419229 -4.2403579 -4.2365327 -4.2385173 -4.246666 -4.2486625 -4.2433834 -4.2400804 -4.2363157]]...]
INFO - root - 2017-12-05 22:13:56.352456: step 48110, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 72h:01m:51s remains)
INFO - root - 2017-12-05 22:14:05.705268: step 48120, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.956 sec/batch; 75h:29m:09s remains)
INFO - root - 2017-12-05 22:14:15.004443: step 48130, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.958 sec/batch; 75h:38m:50s remains)
INFO - root - 2017-12-05 22:14:24.324726: step 48140, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 75h:17m:04s remains)
INFO - root - 2017-12-05 22:14:33.585754: step 48150, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 75h:33m:10s remains)
INFO - root - 2017-12-05 22:14:42.857007: step 48160, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 73h:16m:35s remains)
INFO - root - 2017-12-05 22:14:52.038466: step 48170, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 72h:39m:32s remains)
INFO - root - 2017-12-05 22:15:01.318874: step 48180, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 74h:03m:57s remains)
INFO - root - 2017-12-05 22:15:10.608080: step 48190, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 72h:30m:13s remains)
INFO - root - 2017-12-05 22:15:19.946213: step 48200, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 70h:28m:04s remains)
2017-12-05 22:15:20.712823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3325129 -4.3471866 -4.3521023 -4.3435893 -4.3342462 -4.3274546 -4.3237944 -4.3163571 -4.3130255 -4.311193 -4.3126478 -4.3173223 -4.3201637 -4.3223963 -4.3249965][-4.3170824 -4.3308816 -4.3349481 -4.3237681 -4.3102021 -4.2992911 -4.2893457 -4.2745886 -4.26518 -4.2592311 -4.2631226 -4.2774963 -4.2898827 -4.2990332 -4.30499][-4.3158722 -4.3237147 -4.31948 -4.3030534 -4.2854538 -4.2664909 -4.247437 -4.2238731 -4.2096477 -4.2026377 -4.2110524 -4.23804 -4.26361 -4.2806139 -4.2895732][-4.30241 -4.2997732 -4.2857766 -4.26753 -4.2494597 -4.2293086 -4.2071714 -4.1765227 -4.1591191 -4.1561389 -4.1761303 -4.218112 -4.2537856 -4.2738404 -4.2824159][-4.2774448 -4.2639751 -4.240593 -4.220593 -4.2045646 -4.1882343 -4.16688 -4.1322207 -4.1125541 -4.1128383 -4.1417918 -4.1955643 -4.2419343 -4.26654 -4.2757745][-4.2540069 -4.2319822 -4.2035108 -4.1846328 -4.17138 -4.1585813 -4.1381183 -4.1037836 -4.0875044 -4.0937676 -4.1286507 -4.1863675 -4.2387743 -4.268261 -4.279325][-4.2261868 -4.1944137 -4.1585217 -4.1341438 -4.1139951 -4.0982695 -4.0764837 -4.045517 -4.0419641 -4.0649486 -4.1133332 -4.1810579 -4.24394 -4.2801533 -4.293107][-4.2053227 -4.1699715 -4.134675 -4.1032186 -4.0667844 -4.0319276 -3.9908705 -3.9428821 -3.9393473 -3.98727 -4.0660491 -4.1566825 -4.2312822 -4.27397 -4.2940564][-4.2078657 -4.1798062 -4.1506972 -4.1158562 -4.0739574 -4.0352726 -3.9850764 -3.9168704 -3.8903441 -3.9344616 -4.0239038 -4.1248708 -4.2051072 -4.2526174 -4.2791214][-4.2176194 -4.1959658 -4.1749711 -4.1492367 -4.1245389 -4.1110406 -4.0873232 -4.0392981 -4.0041242 -4.0094242 -4.0562391 -4.127203 -4.194417 -4.240725 -4.2664347][-4.2142587 -4.1941152 -4.1807642 -4.169126 -4.1633186 -4.1673536 -4.1642156 -4.1363225 -4.1088667 -4.1061234 -4.1256371 -4.162806 -4.2068415 -4.24262 -4.2648115][-4.2140841 -4.1931486 -4.1801949 -4.1722312 -4.1724257 -4.1855764 -4.197608 -4.1889868 -4.1768479 -4.1777067 -4.1881309 -4.2088947 -4.234827 -4.2568684 -4.27101][-4.219605 -4.20486 -4.1933413 -4.1819172 -4.1776924 -4.1886058 -4.2062292 -4.2096477 -4.2078776 -4.214911 -4.2273531 -4.2456388 -4.26326 -4.2748508 -4.2797174][-4.2174692 -4.2087865 -4.2027411 -4.1946359 -4.1898603 -4.1953721 -4.2080946 -4.2113581 -4.2097335 -4.2169118 -4.2313838 -4.250814 -4.2680054 -4.277987 -4.2821136][-4.2078233 -4.2023973 -4.2001939 -4.1972957 -4.1952372 -4.1983204 -4.2059174 -4.2061553 -4.2033639 -4.2092948 -4.2238221 -4.2424707 -4.2585473 -4.2700548 -4.2780747]]...]
INFO - root - 2017-12-05 22:15:30.085509: step 48210, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 70h:16m:55s remains)
INFO - root - 2017-12-05 22:15:39.408390: step 48220, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 74h:32m:18s remains)
INFO - root - 2017-12-05 22:15:48.935459: step 48230, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 78h:33m:44s remains)
INFO - root - 2017-12-05 22:15:57.932969: step 48240, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 76h:18m:42s remains)
INFO - root - 2017-12-05 22:16:07.209106: step 48250, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 73h:19m:26s remains)
INFO - root - 2017-12-05 22:16:16.793249: step 48260, loss = 2.04, batch loss = 1.99 (8.1 examples/sec; 0.992 sec/batch; 78h:21m:38s remains)
INFO - root - 2017-12-05 22:16:25.777510: step 48270, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 74h:24m:31s remains)
INFO - root - 2017-12-05 22:16:35.071658: step 48280, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 74h:25m:41s remains)
INFO - root - 2017-12-05 22:16:44.654002: step 48290, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 76h:56m:45s remains)
INFO - root - 2017-12-05 22:16:53.830728: step 48300, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 71h:43m:52s remains)
2017-12-05 22:16:54.652865: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2337203 -4.2291317 -4.217433 -4.1996584 -4.177392 -4.1658692 -4.1739569 -4.1868858 -4.2026038 -4.2220116 -4.2248445 -4.2186508 -4.2161007 -4.222887 -4.2328944][-4.2308912 -4.2238846 -4.2139516 -4.2030163 -4.1878114 -4.1737928 -4.1714463 -4.1828613 -4.2034121 -4.2152553 -4.2098074 -4.2070441 -4.2054629 -4.210711 -4.2248049][-4.2249079 -4.2204757 -4.214231 -4.2068434 -4.19302 -4.1796732 -4.169302 -4.1784368 -4.2022958 -4.2048965 -4.1966705 -4.1994252 -4.2024345 -4.2054067 -4.2182093][-4.2069206 -4.2046638 -4.2050185 -4.2037349 -4.1887484 -4.1711125 -4.1512694 -4.1544523 -4.1843295 -4.1861515 -4.1794705 -4.1914806 -4.2044544 -4.2077503 -4.2144351][-4.1757736 -4.1701717 -4.1715965 -4.1752634 -4.1581497 -4.1337376 -4.0966659 -4.0908656 -4.134645 -4.1499476 -4.1489429 -4.1692023 -4.1944876 -4.2032156 -4.2058444][-4.13876 -4.1239176 -4.1178384 -4.1171412 -4.0990686 -4.0696406 -4.0138888 -3.9963398 -4.064424 -4.1016479 -4.1092467 -4.1382222 -4.173564 -4.1900496 -4.1953278][-4.1122179 -4.078402 -4.0632634 -4.0549603 -4.0333238 -3.9963808 -3.9156926 -3.8796282 -3.9813812 -4.0502234 -4.0715628 -4.1088753 -4.1499939 -4.1773014 -4.1916766][-4.1228352 -4.0838 -4.05829 -4.039659 -4.0175519 -3.9807463 -3.8955584 -3.8451324 -3.9492583 -4.0317154 -4.0625892 -4.1005716 -4.1402211 -4.1745758 -4.1953855][-4.1418738 -4.1147652 -4.0908957 -4.073215 -4.0642371 -4.0513906 -3.9979637 -3.9636211 -4.0296383 -4.0855536 -4.1053667 -4.1304493 -4.1563749 -4.18572 -4.2040043][-4.1557021 -4.147346 -4.1314774 -4.1186237 -4.1249776 -4.1349616 -4.1105323 -4.0933065 -4.1312933 -4.161727 -4.1673622 -4.1749973 -4.1838832 -4.2007141 -4.2116976][-4.1534925 -4.1559925 -4.149673 -4.1432409 -4.1604872 -4.1849327 -4.180274 -4.1735029 -4.1943641 -4.2086573 -4.2053642 -4.2037883 -4.205442 -4.2174449 -4.2239227][-4.127058 -4.1296973 -4.1305223 -4.1369367 -4.166234 -4.2005925 -4.2079172 -4.2018576 -4.2096057 -4.2125125 -4.2064204 -4.2049456 -4.2107124 -4.2261515 -4.2341189][-4.0947666 -4.0899506 -4.0875182 -4.1004119 -4.1343212 -4.1739521 -4.1881757 -4.1831341 -4.1884661 -4.1921291 -4.1889668 -4.1955771 -4.2116385 -4.23159 -4.2404656][-4.0769167 -4.0646482 -4.0598536 -4.0735879 -4.1012759 -4.1347785 -4.143887 -4.1357503 -4.1424069 -4.1552892 -4.1660223 -4.1851811 -4.2082314 -4.2305083 -4.241416][-4.0858788 -4.0694237 -4.0640411 -4.077817 -4.0978065 -4.117218 -4.1104965 -4.0939269 -4.1007915 -4.1231775 -4.1500945 -4.1795459 -4.2057576 -4.22764 -4.238863]]...]
INFO - root - 2017-12-05 22:17:03.933941: step 48310, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 71h:45m:04s remains)
INFO - root - 2017-12-05 22:17:13.298130: step 48320, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 72h:11m:16s remains)
INFO - root - 2017-12-05 22:17:22.541414: step 48330, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 66h:17m:34s remains)
INFO - root - 2017-12-05 22:17:32.032983: step 48340, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 72h:51m:15s remains)
INFO - root - 2017-12-05 22:17:41.626430: step 48350, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 75h:40m:27s remains)
INFO - root - 2017-12-05 22:17:50.909037: step 48360, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 71h:37m:22s remains)
INFO - root - 2017-12-05 22:18:00.395064: step 48370, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 78h:07m:05s remains)
INFO - root - 2017-12-05 22:18:09.913998: step 48380, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 72h:58m:06s remains)
INFO - root - 2017-12-05 22:18:19.416873: step 48390, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 76h:07m:37s remains)
INFO - root - 2017-12-05 22:18:28.721045: step 48400, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 77h:29m:12s remains)
2017-12-05 22:18:29.508726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.365078 -4.3508816 -4.3315849 -4.2918677 -4.2121496 -4.1098747 -4.01489 -3.9982493 -4.0912166 -4.1833587 -4.2107153 -4.190197 -4.1745534 -4.1697779 -4.160203][-4.3676596 -4.3547134 -4.3374424 -4.2990446 -4.2174845 -4.10892 -4.0026922 -3.9852846 -4.0853648 -4.1800222 -4.208674 -4.1901836 -4.1831622 -4.1833639 -4.1724977][-4.36869 -4.3562207 -4.340414 -4.3025508 -4.219326 -4.105413 -3.9885843 -3.9689221 -4.0726986 -4.167871 -4.2024269 -4.1927729 -4.19488 -4.1995049 -4.1884217][-4.3697639 -4.3568749 -4.3418174 -4.3046184 -4.2212372 -4.105052 -3.9799087 -3.95642 -4.0630174 -4.1592951 -4.1982365 -4.1958876 -4.2034464 -4.2106662 -4.2005076][-4.3704004 -4.3568096 -4.3414292 -4.3044181 -4.2211967 -4.1038628 -3.9700398 -3.939579 -4.0493913 -4.1482582 -4.1914706 -4.1955056 -4.2035141 -4.2119145 -4.2054677][-4.370697 -4.3568797 -4.34052 -4.3027797 -4.2190728 -4.1001225 -3.9589739 -3.9202797 -4.033855 -4.1360717 -4.1827183 -4.1939459 -4.2055926 -4.2175207 -4.2156653][-4.3709011 -4.35672 -4.3390355 -4.3000641 -4.2159104 -4.0975404 -3.9556222 -3.9147229 -4.0307245 -4.1350036 -4.1831121 -4.2003469 -4.2177172 -4.2326217 -4.232409][-4.3706269 -4.3560452 -4.3371196 -4.297605 -4.2145824 -4.1012397 -3.9694293 -3.9333122 -4.0474005 -4.1491623 -4.1954627 -4.213109 -4.2314258 -4.2459393 -4.2462578][-4.3700795 -4.3551936 -4.335392 -4.2962666 -4.2156634 -4.109128 -3.9908631 -3.9632974 -4.0729394 -4.1700807 -4.2113242 -4.2252483 -4.2417722 -4.2538557 -4.2552662][-4.3693109 -4.354538 -4.3348966 -4.29708 -4.2192445 -4.1187196 -4.0118418 -3.9919653 -4.0979743 -4.1919012 -4.2272677 -4.2361894 -4.251152 -4.2622809 -4.2655797][-4.3679171 -4.3529773 -4.3334451 -4.2963696 -4.2208681 -4.1260991 -4.0292969 -4.0165873 -4.1193705 -4.2096114 -4.2399344 -4.2458348 -4.2609534 -4.2723927 -4.2775126][-4.3657465 -4.3498521 -4.3300304 -4.29222 -4.2187357 -4.1292419 -4.0413985 -4.0364547 -4.1354766 -4.2208114 -4.2479978 -4.2541404 -4.2698255 -4.2804809 -4.2862468][-4.363759 -4.3471451 -4.3276014 -4.2895317 -4.2186503 -4.1362681 -4.0611367 -4.0615773 -4.1510363 -4.2269931 -4.2503171 -4.2575831 -4.2732697 -4.2834692 -4.2896051][-4.3624 -4.3456941 -4.3273778 -4.291501 -4.2268457 -4.154458 -4.0933409 -4.0943279 -4.1679473 -4.2320824 -4.2551951 -4.2654595 -4.2812123 -4.2898955 -4.2944326][-4.3601422 -4.3442755 -4.3275647 -4.2952685 -4.2394252 -4.1782527 -4.1300616 -4.1288471 -4.186265 -4.2404771 -4.2656741 -4.2785196 -4.2939172 -4.3009815 -4.3035994]]...]
INFO - root - 2017-12-05 22:18:38.867830: step 48410, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.009 sec/batch; 79h:38m:55s remains)
INFO - root - 2017-12-05 22:18:48.394602: step 48420, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 76h:27m:08s remains)
INFO - root - 2017-12-05 22:18:57.745892: step 48430, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 70h:21m:05s remains)
INFO - root - 2017-12-05 22:19:07.038059: step 48440, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.992 sec/batch; 78h:18m:15s remains)
INFO - root - 2017-12-05 22:19:16.358472: step 48450, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.003 sec/batch; 79h:07m:48s remains)
INFO - root - 2017-12-05 22:19:25.802145: step 48460, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 70h:29m:43s remains)
INFO - root - 2017-12-05 22:19:35.119718: step 48470, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 73h:39m:09s remains)
INFO - root - 2017-12-05 22:19:44.534325: step 48480, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 74h:44m:47s remains)
INFO - root - 2017-12-05 22:19:53.731586: step 48490, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 72h:55m:14s remains)
INFO - root - 2017-12-05 22:20:03.119459: step 48500, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 67h:59m:10s remains)
2017-12-05 22:20:03.855420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0885191 -4.1305537 -4.1700177 -4.1842566 -4.1761866 -4.1591148 -4.1420112 -4.1342959 -4.1424694 -4.179122 -4.2213173 -4.2392941 -4.2274594 -4.1945486 -4.1606851][-4.1158452 -4.14223 -4.161026 -4.1680717 -4.1662064 -4.1595316 -4.1471786 -4.1363764 -4.1449652 -4.1780968 -4.2172065 -4.2254381 -4.1997933 -4.1607928 -4.12684][-4.1495848 -4.1574378 -4.1636372 -4.168057 -4.1678162 -4.1616149 -4.1432514 -4.1205173 -4.1223164 -4.1520958 -4.189074 -4.1961317 -4.1634979 -4.1168118 -4.0863585][-4.1892972 -4.1859674 -4.1836348 -4.1828117 -4.1728067 -4.151803 -4.1157708 -4.0838289 -4.0799026 -4.1085052 -4.1483541 -4.1646943 -4.1381173 -4.0907531 -4.0687461][-4.2327356 -4.2345362 -4.2256017 -4.203495 -4.162447 -4.1078072 -4.0416541 -4.0055313 -4.0177875 -4.0615716 -4.1162305 -4.1465621 -4.1354933 -4.0956411 -4.0796161][-4.2683063 -4.2800508 -4.2590942 -4.20182 -4.1082344 -4.0028567 -3.9000404 -3.8704488 -3.9288521 -4.0250716 -4.1101508 -4.1571174 -4.1601725 -4.1315861 -4.11568][-4.2859397 -4.2940288 -4.2506356 -4.148838 -4.0046616 -3.852267 -3.7325335 -3.7295685 -3.8493714 -4.0060334 -4.1233215 -4.1847835 -4.2009115 -4.1825571 -4.1636133][-4.2864337 -4.2921686 -4.2405105 -4.1210375 -3.9569702 -3.789444 -3.6839049 -3.7222936 -3.8712654 -4.0386147 -4.1553922 -4.2166891 -4.2387471 -4.2282085 -4.2082148][-4.276042 -4.2782907 -4.2393346 -4.1388907 -3.997004 -3.8446078 -3.7507303 -3.8113031 -3.9597919 -4.104681 -4.2009554 -4.2543778 -4.27289 -4.2586546 -4.2387533][-4.2615786 -4.2660728 -4.2513084 -4.1925521 -4.0967612 -3.9803259 -3.8968687 -3.9494474 -4.06805 -4.1693239 -4.2357221 -4.2780313 -4.2905669 -4.2695923 -4.2506552][-4.2497849 -4.2631683 -4.2718453 -4.2485385 -4.1879368 -4.1117415 -4.0584884 -4.0946946 -4.167068 -4.2160563 -4.2494164 -4.2773881 -4.2868266 -4.2675004 -4.254025][-4.23724 -4.2571063 -4.2787452 -4.2703676 -4.2249413 -4.1711617 -4.1401262 -4.1664529 -4.2077336 -4.2308455 -4.2504339 -4.2653036 -4.2671432 -4.2544661 -4.24697][-4.2272267 -4.2564073 -4.2810864 -4.2731867 -4.23464 -4.1946883 -4.1782665 -4.1947923 -4.2199345 -4.242321 -4.2649808 -4.2621317 -4.2460723 -4.2344813 -4.2313538][-4.2112422 -4.2488356 -4.2755113 -4.26505 -4.2282343 -4.1967168 -4.191678 -4.2029667 -4.2269125 -4.2580562 -4.2795777 -4.2637806 -4.234642 -4.217247 -4.2080641][-4.1772766 -4.2145872 -4.2378325 -4.2337976 -4.2049565 -4.1857443 -4.1926742 -4.2114339 -4.2373247 -4.2682881 -4.282661 -4.2606258 -4.2245865 -4.1967659 -4.1837335]]...]
INFO - root - 2017-12-05 22:20:13.295152: step 48510, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 74h:16m:07s remains)
INFO - root - 2017-12-05 22:20:22.711590: step 48520, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 76h:07m:30s remains)
INFO - root - 2017-12-05 22:20:32.062458: step 48530, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 75h:45m:05s remains)
INFO - root - 2017-12-05 22:20:41.220006: step 48540, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 77h:45m:58s remains)
INFO - root - 2017-12-05 22:20:50.427591: step 48550, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 69h:46m:11s remains)
INFO - root - 2017-12-05 22:20:59.650229: step 48560, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 68h:15m:25s remains)
INFO - root - 2017-12-05 22:21:09.006094: step 48570, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.961 sec/batch; 75h:47m:08s remains)
INFO - root - 2017-12-05 22:21:18.221592: step 48580, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 73h:23m:33s remains)
INFO - root - 2017-12-05 22:21:27.542633: step 48590, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 72h:50m:07s remains)
INFO - root - 2017-12-05 22:21:36.755571: step 48600, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 71h:26m:22s remains)
2017-12-05 22:21:37.512473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3392138 -4.33875 -4.3336468 -4.3281851 -4.3239641 -4.3192849 -4.3140445 -4.3087354 -4.3102093 -4.3127089 -4.3154273 -4.318841 -4.3125978 -4.3084054 -4.3169312][-4.3291116 -4.3303514 -4.3281264 -4.3251009 -4.3220739 -4.3172789 -4.3120675 -4.3047853 -4.3030205 -4.305388 -4.3072376 -4.3093376 -4.3033156 -4.2980142 -4.3061562][-4.3205395 -4.3233552 -4.3234477 -4.3208122 -4.3152046 -4.3045263 -4.2918296 -4.2748971 -4.2638817 -4.2638702 -4.2659721 -4.2698188 -4.2717466 -4.2726259 -4.2855463][-4.3108029 -4.3118711 -4.3091507 -4.3009171 -4.2854652 -4.2590375 -4.22935 -4.1978793 -4.1748 -4.1755075 -4.185153 -4.1994686 -4.2186303 -4.2332511 -4.2557831][-4.2857919 -4.2765193 -4.2601976 -4.2350769 -4.200232 -4.150702 -4.1023026 -4.0619526 -4.0335474 -4.04524 -4.0755568 -4.1130371 -4.1575637 -4.19107 -4.2238207][-4.227529 -4.1979351 -4.1567125 -4.1050353 -4.0521979 -3.9874766 -3.9337621 -3.9033709 -3.891201 -3.9303284 -3.9936059 -4.0602665 -4.1268182 -4.1722937 -4.2074618][-4.1440859 -4.0922279 -4.0318842 -3.9694593 -3.9216218 -3.8694873 -3.8352664 -3.8341904 -3.8535337 -3.9176946 -4.0005412 -4.0790653 -4.1460581 -4.1872635 -4.2150369][-4.0920959 -4.0376678 -3.9859989 -3.9465115 -3.93342 -3.9145374 -3.9027839 -3.9167595 -3.9486785 -4.0131598 -4.086482 -4.1522741 -4.201098 -4.2264552 -4.2420559][-4.1125479 -4.0779681 -4.0529928 -4.0453424 -4.0597491 -4.0625048 -4.0585351 -4.0673351 -4.0938878 -4.1436133 -4.1944637 -4.2387872 -4.2667356 -4.2759318 -4.2791386][-4.1808534 -4.1688132 -4.1659617 -4.1717243 -4.1902747 -4.1960559 -4.1928382 -4.1957622 -4.2147865 -4.2496161 -4.2820215 -4.3088226 -4.3203626 -4.3172331 -4.3105288][-4.2305789 -4.2290249 -4.2344894 -4.2416372 -4.2558556 -4.26088 -4.2576313 -4.2596936 -4.2760158 -4.3014636 -4.3239288 -4.3411202 -4.344943 -4.3367944 -4.3268967][-4.2396078 -4.2398963 -4.244998 -4.2489395 -4.2603288 -4.2677155 -4.2680469 -4.274282 -4.293457 -4.3149724 -4.3332729 -4.3478141 -4.3495216 -4.3405304 -4.3309655][-4.1997652 -4.199893 -4.2061095 -4.2116671 -4.2281623 -4.2437387 -4.2496071 -4.2621875 -4.2875018 -4.3104239 -4.3287168 -4.3435693 -4.344841 -4.3365474 -4.3283472][-4.1242576 -4.1204844 -4.1303339 -4.1458364 -4.1777072 -4.2077241 -4.2237544 -4.2449241 -4.2747483 -4.2999835 -4.3188519 -4.3341794 -4.3348408 -4.3269935 -4.3208451][-4.053854 -4.0488062 -4.0659695 -4.0970907 -4.1461835 -4.1883659 -4.2134166 -4.238481 -4.2677107 -4.2905588 -4.3070168 -4.3215094 -4.3218613 -4.3140492 -4.3093195]]...]
INFO - root - 2017-12-05 22:21:46.956193: step 48610, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 74h:52m:34s remains)
INFO - root - 2017-12-05 22:21:56.300333: step 48620, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 72h:39m:16s remains)
INFO - root - 2017-12-05 22:22:05.686876: step 48630, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.973 sec/batch; 76h:43m:45s remains)
INFO - root - 2017-12-05 22:22:15.097017: step 48640, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 78h:49m:52s remains)
INFO - root - 2017-12-05 22:22:24.297985: step 48650, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 72h:53m:24s remains)
INFO - root - 2017-12-05 22:22:33.419566: step 48660, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 72h:49m:41s remains)
INFO - root - 2017-12-05 22:22:42.587639: step 48670, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.810 sec/batch; 63h:50m:43s remains)
INFO - root - 2017-12-05 22:22:51.949017: step 48680, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 74h:12m:48s remains)
INFO - root - 2017-12-05 22:23:01.236737: step 48690, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.907 sec/batch; 71h:31m:53s remains)
INFO - root - 2017-12-05 22:23:10.725389: step 48700, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 74h:23m:08s remains)
2017-12-05 22:23:11.547899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3215442 -4.32158 -4.3077226 -4.2690983 -4.2054877 -4.1420217 -4.1185536 -4.1500878 -4.2047515 -4.2437377 -4.2670684 -4.2796187 -4.2859559 -4.2996645 -4.3188305][-4.3246689 -4.3213167 -4.2979393 -4.2416682 -4.1623135 -4.0964751 -4.088119 -4.1420183 -4.208147 -4.24638 -4.2710071 -4.2906141 -4.3039479 -4.3193603 -4.3336415][-4.3251872 -4.3168597 -4.279459 -4.2043295 -4.1166024 -4.056345 -4.0669751 -4.1365743 -4.2030854 -4.2392278 -4.2675056 -4.2941279 -4.3110595 -4.3262696 -4.3363075][-4.3265553 -4.3110414 -4.2581325 -4.1685143 -4.0774355 -4.0253797 -4.0523558 -4.1327186 -4.1970987 -4.23325 -4.2670264 -4.29747 -4.3135352 -4.3256702 -4.3323431][-4.3285837 -4.3042221 -4.2378893 -4.1407585 -4.0516109 -4.0100961 -4.0511312 -4.13871 -4.2018762 -4.237061 -4.2739711 -4.3040876 -4.31793 -4.3261557 -4.3294091][-4.3292656 -4.2963471 -4.2209921 -4.1225033 -4.0408521 -4.0115719 -4.0635562 -4.1557221 -4.2173333 -4.2520552 -4.2893777 -4.3177624 -4.3285165 -4.3311615 -4.3298755][-4.3266587 -4.2878094 -4.2067618 -4.1072989 -4.0298924 -4.0083265 -4.0694766 -4.165657 -4.226573 -4.2612753 -4.2994003 -4.3277311 -4.3378839 -4.3381414 -4.3334641][-4.3223281 -4.2804174 -4.1960182 -4.0950961 -4.0164094 -3.9949558 -4.0614214 -4.1613913 -4.22484 -4.2640247 -4.3053107 -4.3349319 -4.3454881 -4.3444505 -4.3363109][-4.3180881 -4.275239 -4.1915236 -4.0929022 -4.0150805 -3.9960039 -4.0655904 -4.1651783 -4.232203 -4.2743258 -4.3133721 -4.3413758 -4.3499961 -4.3475208 -4.3369045][-4.3130527 -4.2709136 -4.1913261 -4.0982018 -4.0301566 -4.0209203 -4.0888615 -4.184371 -4.2513614 -4.291779 -4.3246617 -4.3466749 -4.3509707 -4.3468809 -4.3333693][-4.3079395 -4.2665815 -4.1914177 -4.1053 -4.0492568 -4.0502539 -4.1133828 -4.2035394 -4.2667012 -4.3007164 -4.3273711 -4.3465652 -4.3473291 -4.3412437 -4.3241262][-4.3044882 -4.2644653 -4.1932273 -4.1112804 -4.0610862 -4.0633316 -4.1196775 -4.2029505 -4.2615585 -4.2926683 -4.3199492 -4.3409996 -4.3426075 -4.3349342 -4.31408][-4.3016181 -4.2648735 -4.1979966 -4.1164179 -4.0625648 -4.0571427 -4.1049905 -4.1803584 -4.2408824 -4.2778535 -4.3124466 -4.3390384 -4.343935 -4.3319745 -4.3049068][-4.3003674 -4.2664728 -4.2029405 -4.1181741 -4.0558434 -4.0382929 -4.0753407 -4.1493349 -4.21846 -4.2677779 -4.3114457 -4.3431454 -4.3492589 -4.3315072 -4.2969294][-4.3006444 -4.2694845 -4.2073684 -4.1184287 -4.0458317 -4.0122547 -4.0387354 -4.1160669 -4.1976237 -4.2612553 -4.3147063 -4.3491921 -4.3534665 -4.3293467 -4.287118]]...]
INFO - root - 2017-12-05 22:23:21.021118: step 48710, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 73h:43m:53s remains)
INFO - root - 2017-12-05 22:23:30.253003: step 48720, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 71h:34m:00s remains)
INFO - root - 2017-12-05 22:23:39.574612: step 48730, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 74h:43m:12s remains)
INFO - root - 2017-12-05 22:23:49.082263: step 48740, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 77h:47m:28s remains)
INFO - root - 2017-12-05 22:23:58.355153: step 48750, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 67h:52m:50s remains)
INFO - root - 2017-12-05 22:24:07.598824: step 48760, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 71h:30m:04s remains)
INFO - root - 2017-12-05 22:24:17.075991: step 48770, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 77h:21m:07s remains)
INFO - root - 2017-12-05 22:24:26.596607: step 48780, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 75h:09m:30s remains)
INFO - root - 2017-12-05 22:24:36.172677: step 48790, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 75h:57m:13s remains)
INFO - root - 2017-12-05 22:24:45.525935: step 48800, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 75h:16m:03s remains)
2017-12-05 22:24:46.298344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2601457 -4.2612686 -4.2644868 -4.266305 -4.264533 -4.2639623 -4.2648025 -4.2667451 -4.2665296 -4.262712 -4.2534704 -4.2444177 -4.2363219 -4.2338257 -4.2386875][-4.2261686 -4.2254615 -4.2298861 -4.2361403 -4.2379613 -4.2415581 -4.2461877 -4.2484412 -4.2453957 -4.2389035 -4.2287621 -4.2183933 -4.2073693 -4.2029543 -4.2065][-4.1919508 -4.1853175 -4.1891589 -4.198904 -4.2052474 -4.2126722 -4.2190213 -4.2179704 -4.2109618 -4.20078 -4.1917844 -4.183836 -4.1707258 -4.1574183 -4.1493087][-4.160368 -4.1466055 -4.1444511 -4.1511412 -4.1582656 -4.1681781 -4.1721239 -4.1664104 -4.1567731 -4.1436424 -4.1379814 -4.1376729 -4.1276588 -4.1070623 -4.0860252][-4.160037 -4.1410403 -4.1300373 -4.1261163 -4.1231747 -4.122848 -4.1128283 -4.0993047 -4.0925035 -4.0858378 -4.0889554 -4.1026139 -4.1011319 -4.0790892 -4.0524521][-4.1699495 -4.1540017 -4.1392841 -4.1235418 -4.1032319 -4.0774465 -4.0387816 -4.0139618 -4.0262823 -4.0473523 -4.0701747 -4.0957265 -4.1022015 -4.0826044 -4.0534968][-4.1696215 -4.1509395 -4.1261988 -4.0970035 -4.0604672 -4.0046744 -3.9279919 -3.8918791 -3.9469447 -4.016778 -4.0702147 -4.1056514 -4.1172733 -4.1013618 -4.0706348][-4.1545072 -4.1256776 -4.0874295 -4.0505729 -4.0113425 -3.9417007 -3.8421209 -3.8042505 -3.9037185 -4.0095167 -4.0816245 -4.1147451 -4.1221862 -4.1094036 -4.0840058][-4.1584244 -4.1203847 -4.0787997 -4.0497313 -4.0270596 -3.9734354 -3.9026551 -3.8922014 -3.9777453 -4.0644984 -4.118001 -4.1270423 -4.1185722 -4.1009192 -4.0818009][-4.1694446 -4.1325727 -4.0966063 -4.0835524 -4.0811968 -4.0558019 -4.030005 -4.0440683 -4.0944743 -4.1353693 -4.1464467 -4.1232543 -4.0953436 -4.0681658 -4.0561719][-4.1750851 -4.1459789 -4.1223364 -4.1232629 -4.1334929 -4.1255178 -4.1257973 -4.1448474 -4.1646752 -4.1649451 -4.13833 -4.0943351 -4.0567036 -4.0284553 -4.0270367][-4.1912508 -4.1768446 -4.1683078 -4.1793485 -4.1906161 -4.1839995 -4.1881938 -4.1957517 -4.192215 -4.1696854 -4.1243858 -4.0745649 -4.0421138 -4.0271688 -4.0399122][-4.216022 -4.2155194 -4.2175245 -4.2342343 -4.2447381 -4.2361588 -4.2341585 -4.2293506 -4.2159376 -4.1883698 -4.1432934 -4.1003771 -4.0793147 -4.079515 -4.0969582][-4.2173638 -4.2243242 -4.2295794 -4.246748 -4.2569208 -4.2500067 -4.2462568 -4.2359657 -4.2158756 -4.1885505 -4.155005 -4.1299582 -4.124774 -4.1350322 -4.1497884][-4.2047806 -4.2214942 -4.2314444 -4.2469006 -4.252419 -4.2437482 -4.2362094 -4.2228804 -4.2011523 -4.1802492 -4.165 -4.1621094 -4.1657186 -4.1793418 -4.1892118]]...]
INFO - root - 2017-12-05 22:24:55.819489: step 48810, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.002 sec/batch; 78h:57m:22s remains)
INFO - root - 2017-12-05 22:25:05.314825: step 48820, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 74h:53m:40s remains)
INFO - root - 2017-12-05 22:25:14.985124: step 48830, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 76h:14m:49s remains)
INFO - root - 2017-12-05 22:25:24.422258: step 48840, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 73h:58m:45s remains)
INFO - root - 2017-12-05 22:25:33.760241: step 48850, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 68h:15m:07s remains)
INFO - root - 2017-12-05 22:25:42.927410: step 48860, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 74h:02m:50s remains)
INFO - root - 2017-12-05 22:25:52.322569: step 48870, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 71h:42m:01s remains)
INFO - root - 2017-12-05 22:26:01.754379: step 48880, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 70h:28m:59s remains)
INFO - root - 2017-12-05 22:26:11.338412: step 48890, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 71h:36m:31s remains)
INFO - root - 2017-12-05 22:26:20.783405: step 48900, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 77h:24m:50s remains)
2017-12-05 22:26:21.535831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1385913 -4.1175017 -4.1018996 -4.0898919 -4.0782971 -4.0763235 -4.0877304 -4.1025662 -4.0962 -4.0658827 -4.0254059 -3.9931939 -3.9864383 -3.9998426 -4.0330257][-4.1116176 -4.0848279 -4.0688596 -4.0624514 -4.0552955 -4.0508437 -4.0568366 -4.0701656 -4.0720749 -4.0577455 -4.0342712 -4.0240855 -4.0360432 -4.059319 -4.0914135][-4.0714693 -4.0328813 -4.023078 -4.0364914 -4.045794 -4.0516391 -4.0672345 -4.0922294 -4.1109414 -4.1130524 -4.1033745 -4.1024022 -4.1142406 -4.1290097 -4.1410532][-4.0332956 -3.9875784 -3.9997344 -4.0493097 -4.0788503 -4.091146 -4.1116724 -4.134521 -4.1505241 -4.1569648 -4.155798 -4.1572952 -4.1603069 -4.1584072 -4.1528997][-4.0489364 -4.0085583 -4.030241 -4.0884409 -4.1238117 -4.135797 -4.1488547 -4.163085 -4.1712646 -4.1793733 -4.1870365 -4.1877437 -4.1839013 -4.1742887 -4.1582489][-4.1086307 -4.0680718 -4.0798264 -4.1240282 -4.1522474 -4.1590118 -4.1669097 -4.1702638 -4.1762443 -4.1922665 -4.2096043 -4.2149963 -4.2130179 -4.2005167 -4.1800232][-4.1612239 -4.1141372 -4.1014209 -4.1267304 -4.1524105 -4.1639385 -4.1708765 -4.1674857 -4.1678362 -4.1873446 -4.2168632 -4.23981 -4.2496014 -4.241323 -4.2212553][-4.1930737 -4.1423755 -4.1088037 -4.1037297 -4.1124582 -4.1296463 -4.1487417 -4.1478329 -4.1455407 -4.1623659 -4.1964278 -4.2358618 -4.2637377 -4.2681737 -4.2571397][-4.2190189 -4.1682706 -4.120625 -4.0879741 -4.0723481 -4.0838528 -4.1128254 -4.1189561 -4.1177449 -4.1270456 -4.1547866 -4.2012119 -4.2474322 -4.2699795 -4.2740188][-4.235168 -4.1919727 -4.1389952 -4.0952148 -4.0683336 -4.0720382 -4.1012192 -4.1165509 -4.1200576 -4.1269231 -4.1475725 -4.1874251 -4.2338634 -4.2654514 -4.2815413][-4.2181973 -4.1892266 -4.1469808 -4.1151214 -4.1017885 -4.1095066 -4.133069 -4.148942 -4.1562033 -4.164175 -4.1778827 -4.2055106 -4.2414832 -4.268177 -4.2821183][-4.18322 -4.1732216 -4.1563349 -4.1484308 -4.1513557 -4.1647396 -4.1868806 -4.2009268 -4.2049575 -4.2070603 -4.2149105 -4.2340236 -4.2597241 -4.2766857 -4.2827129][-4.1792393 -4.1846023 -4.1856279 -4.1920137 -4.2047744 -4.2175393 -4.23494 -4.2446833 -4.2457862 -4.241713 -4.2427878 -4.2556281 -4.275188 -4.2847257 -4.2829995][-4.2126536 -4.2213745 -4.2266335 -4.2372413 -4.247571 -4.2560525 -4.2685666 -4.2763934 -4.27904 -4.2721457 -4.2692976 -4.2765212 -4.2862058 -4.2857337 -4.2777314][-4.2486291 -4.2543674 -4.2552114 -4.26551 -4.279532 -4.2895732 -4.2992792 -4.305625 -4.3066859 -4.2967768 -4.2895832 -4.2921824 -4.2924094 -4.2819614 -4.2670918]]...]
INFO - root - 2017-12-05 22:26:30.911481: step 48910, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.981 sec/batch; 77h:16m:03s remains)
INFO - root - 2017-12-05 22:26:40.398579: step 48920, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.948 sec/batch; 74h:39m:32s remains)
INFO - root - 2017-12-05 22:26:49.828332: step 48930, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 71h:45m:35s remains)
INFO - root - 2017-12-05 22:26:59.135523: step 48940, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 76h:52m:07s remains)
INFO - root - 2017-12-05 22:27:08.518281: step 48950, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 76h:37m:23s remains)
INFO - root - 2017-12-05 22:27:17.855323: step 48960, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 78h:44m:27s remains)
INFO - root - 2017-12-05 22:27:27.323758: step 48970, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 78h:12m:54s remains)
INFO - root - 2017-12-05 22:27:36.885928: step 48980, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 70h:20m:32s remains)
INFO - root - 2017-12-05 22:27:46.299322: step 48990, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 71h:43m:04s remains)
INFO - root - 2017-12-05 22:27:55.896145: step 49000, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.975 sec/batch; 76h:46m:14s remains)
2017-12-05 22:27:56.706676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20763 -4.1962156 -4.1915927 -4.1869044 -4.1799035 -4.1752691 -4.1844344 -4.2017283 -4.22412 -4.2369766 -4.2450666 -4.2469997 -4.2476149 -4.2494364 -4.2494349][-4.2149868 -4.2048068 -4.2013192 -4.197391 -4.1888752 -4.1788645 -4.1766849 -4.18866 -4.2158246 -4.2355356 -4.2514949 -4.2564187 -4.2528882 -4.248292 -4.24478][-4.214818 -4.2049336 -4.2024665 -4.2050056 -4.1986375 -4.1819072 -4.1667438 -4.1747522 -4.2063785 -4.2331715 -4.2521014 -4.2589326 -4.2500391 -4.2394118 -4.2333565][-4.2095876 -4.1966114 -4.1920481 -4.201036 -4.199605 -4.1786003 -4.153842 -4.1564088 -4.190249 -4.2221031 -4.2445388 -4.2561693 -4.2497807 -4.2378545 -4.2285109][-4.2052245 -4.1836548 -4.1729212 -4.1793971 -4.1781249 -4.157299 -4.1274843 -4.1225495 -4.1611028 -4.2073679 -4.2426205 -4.2623506 -4.2606997 -4.2470822 -4.233357][-4.2006364 -4.1683674 -4.1485343 -4.1501479 -4.1516242 -4.1346464 -4.0955229 -4.0763254 -4.1203485 -4.1886654 -4.239213 -4.26373 -4.2624822 -4.2481127 -4.2308588][-4.198329 -4.1611371 -4.1370034 -4.1331096 -4.1306911 -4.1125932 -4.0563874 -4.0027022 -4.0413437 -4.1417665 -4.2150259 -4.2479887 -4.2509246 -4.2412176 -4.2251883][-4.2127833 -4.1764174 -4.1487918 -4.1361938 -4.1241336 -4.0961123 -4.0197544 -3.9243324 -3.9489975 -4.0837836 -4.185585 -4.2312007 -4.2422953 -4.2421322 -4.2308593][-4.2218838 -4.1936712 -4.1682005 -4.1530805 -4.1399112 -4.1123447 -4.042057 -3.9480228 -3.9596446 -4.0842729 -4.1856136 -4.2323442 -4.2469554 -4.2500052 -4.2393513][-4.2178774 -4.1992879 -4.18527 -4.1745791 -4.1643009 -4.1462531 -4.1035275 -4.0472054 -4.047359 -4.1269469 -4.2025404 -4.2381811 -4.2487931 -4.2503934 -4.2382979][-4.228961 -4.2137451 -4.2040453 -4.1927519 -4.1795006 -4.1671906 -4.1505365 -4.1278181 -4.1274319 -4.1673 -4.2096834 -4.231338 -4.24066 -4.2449331 -4.2383351][-4.2536039 -4.2365103 -4.2222142 -4.202744 -4.1861143 -4.1817951 -4.1864371 -4.1892872 -4.1940475 -4.2087674 -4.2246833 -4.234828 -4.2426796 -4.2474546 -4.2469954][-4.2787924 -4.2596054 -4.2436013 -4.220191 -4.2027917 -4.2062373 -4.2237964 -4.2408423 -4.2512331 -4.2565145 -4.2578878 -4.2557092 -4.257503 -4.25928 -4.2641053][-4.2904105 -4.2745261 -4.2639332 -4.2448092 -4.2322268 -4.2378917 -4.2548165 -4.2750278 -4.2872028 -4.2908845 -4.2876735 -4.2811041 -4.2799325 -4.2797971 -4.2854214][-4.2957315 -4.2859206 -4.2808867 -4.2673635 -4.2597589 -4.2668486 -4.27948 -4.2946358 -4.3017888 -4.3016882 -4.2977929 -4.2934284 -4.2938371 -4.2968073 -4.3020363]]...]
INFO - root - 2017-12-05 22:28:06.006104: step 49010, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 71h:52m:07s remains)
INFO - root - 2017-12-05 22:28:15.397092: step 49020, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 72h:25m:15s remains)
INFO - root - 2017-12-05 22:28:24.747161: step 49030, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.947 sec/batch; 74h:32m:13s remains)
INFO - root - 2017-12-05 22:28:34.160816: step 49040, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 74h:22m:19s remains)
INFO - root - 2017-12-05 22:28:43.433731: step 49050, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 73h:46m:14s remains)
INFO - root - 2017-12-05 22:28:52.788696: step 49060, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 72h:12m:25s remains)
INFO - root - 2017-12-05 22:29:02.177823: step 49070, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 73h:45m:48s remains)
INFO - root - 2017-12-05 22:29:11.464123: step 49080, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.977 sec/batch; 76h:54m:39s remains)
INFO - root - 2017-12-05 22:29:20.873039: step 49090, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 74h:46m:48s remains)
INFO - root - 2017-12-05 22:29:30.096156: step 49100, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 69h:09m:04s remains)
2017-12-05 22:29:30.822496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.307941 -4.2740116 -4.2228642 -4.1690521 -4.1127882 -4.0897036 -4.1058989 -4.123353 -4.1284013 -4.1375852 -4.1661911 -4.191031 -4.2089615 -4.2294431 -4.2515912][-4.3062329 -4.2647481 -4.2070589 -4.1417923 -4.0701666 -4.0413747 -4.06183 -4.0835905 -4.0881348 -4.0993977 -4.1347833 -4.16526 -4.1863942 -4.2117081 -4.2385921][-4.3034644 -4.2544365 -4.1880713 -4.1093526 -4.0282435 -3.9995761 -4.0284076 -4.0549011 -4.0595078 -4.0736704 -4.1058068 -4.128572 -4.1475196 -4.1757097 -4.2074947][-4.298315 -4.2436113 -4.1679182 -4.0819378 -4.0067081 -3.9904952 -4.03213 -4.0631647 -4.0659542 -4.0732393 -4.0851879 -4.0793977 -4.0778847 -4.1085272 -4.1556392][-4.29053 -4.2320905 -4.1441793 -4.0502596 -3.9877183 -3.994442 -4.0505567 -4.0835638 -4.0835543 -4.0807533 -4.0706673 -4.0314412 -4.0024185 -4.0420651 -4.1135817][-4.2886844 -4.2259369 -4.1205373 -4.0125079 -3.9526684 -3.9772167 -4.0495019 -4.0931697 -4.0969563 -4.0913868 -4.0734539 -4.01441 -3.9670346 -4.013576 -4.1020818][-4.2882061 -4.2235866 -4.1046219 -3.9773498 -3.9088278 -3.9429717 -4.0280652 -4.0805807 -4.0884233 -4.0877647 -4.0805678 -4.0336246 -3.9886508 -4.0298219 -4.1112118][-4.2824092 -4.2236614 -4.104785 -3.9644749 -3.8794539 -3.906388 -3.9938278 -4.0470457 -4.056283 -4.070189 -4.0800338 -4.0542474 -4.0210772 -4.0497127 -4.1180654][-4.2798243 -4.2293515 -4.1304193 -4.003077 -3.9169979 -3.9283974 -3.9948254 -4.0349894 -4.0476031 -4.0735822 -4.0954714 -4.0866642 -4.0685587 -4.0859871 -4.137218][-4.2873192 -4.2419176 -4.1630545 -4.0635152 -3.9896698 -3.9893024 -4.0392661 -4.0744219 -4.0940633 -4.1277328 -4.15632 -4.1557217 -4.1464815 -4.1543779 -4.1856518][-4.3001471 -4.2575049 -4.1897917 -4.1151276 -4.0596476 -4.0595193 -4.1034451 -4.1352348 -4.1544042 -4.1805334 -4.2057686 -4.210743 -4.2096419 -4.2142839 -4.231286][-4.3135195 -4.2732611 -4.2116361 -4.151927 -4.1101246 -4.1118908 -4.1509609 -4.1799765 -4.1952844 -4.21251 -4.2318115 -4.2408371 -4.2443213 -4.24744 -4.2578998][-4.3110418 -4.2732573 -4.2156816 -4.16447 -4.131506 -4.1374469 -4.1765 -4.2061596 -4.22299 -4.2373691 -4.2538104 -4.2616472 -4.2627091 -4.2635717 -4.2712646][-4.3004704 -4.2602568 -4.2029214 -4.152988 -4.125668 -4.1392612 -4.1870089 -4.2246056 -4.2440619 -4.255074 -4.2682095 -4.2757559 -4.2755308 -4.2750115 -4.2791219][-4.2972665 -4.2549624 -4.1936092 -4.14187 -4.1150684 -4.133728 -4.1921778 -4.2398319 -4.2635708 -4.2720537 -4.2828441 -4.289681 -4.2897859 -4.2869267 -4.2858996]]...]
INFO - root - 2017-12-05 22:29:40.253364: step 49110, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.994 sec/batch; 78h:12m:47s remains)
INFO - root - 2017-12-05 22:29:49.792267: step 49120, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 73h:25m:52s remains)
INFO - root - 2017-12-05 22:29:58.728410: step 49130, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 75h:36m:42s remains)
INFO - root - 2017-12-05 22:30:07.914617: step 49140, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 71h:34m:25s remains)
INFO - root - 2017-12-05 22:30:17.259858: step 49150, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 71h:56m:11s remains)
INFO - root - 2017-12-05 22:30:26.785192: step 49160, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.954 sec/batch; 75h:06m:29s remains)
INFO - root - 2017-12-05 22:30:36.148799: step 49170, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 74h:28m:58s remains)
INFO - root - 2017-12-05 22:30:45.391455: step 49180, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 71h:35m:19s remains)
INFO - root - 2017-12-05 22:30:54.799552: step 49190, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 75h:07m:23s remains)
INFO - root - 2017-12-05 22:31:04.304864: step 49200, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 76h:04m:03s remains)
2017-12-05 22:31:05.069800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2952414 -4.3005147 -4.2985005 -4.2886004 -4.2715735 -4.2542887 -4.2485857 -4.2586546 -4.2714963 -4.2799988 -4.2832341 -4.2822618 -4.2814088 -4.2819705 -4.281682][-4.3152728 -4.3183465 -4.3097854 -4.2900238 -4.2605996 -4.233202 -4.2255874 -4.244216 -4.2680955 -4.2848864 -4.2940631 -4.2977271 -4.3001957 -4.3036685 -4.3038225][-4.3256536 -4.3241262 -4.3059244 -4.27148 -4.2247458 -4.18171 -4.16616 -4.1914411 -4.2307105 -4.2607369 -4.2794542 -4.2926259 -4.3029404 -4.3129206 -4.3172903][-4.3255239 -4.3172665 -4.2874651 -4.2358804 -4.1697288 -4.1095309 -4.080925 -4.1058407 -4.1571622 -4.200232 -4.2295747 -4.2575507 -4.2843533 -4.3060436 -4.3184123][-4.3221483 -4.3082075 -4.27059 -4.2058496 -4.1217341 -4.0435171 -3.9982457 -4.015254 -4.0737915 -4.1264448 -4.1642475 -4.2076278 -4.2567244 -4.2950563 -4.3172746][-4.3177857 -4.2996817 -4.2592878 -4.1876631 -4.0888863 -3.9920895 -3.9191117 -3.9145374 -3.9769711 -4.0406709 -4.0890918 -4.1513438 -4.2257762 -4.2830257 -4.315537][-4.3112383 -4.2915449 -4.2515497 -4.1800475 -4.0754046 -3.9631793 -3.85615 -3.8173385 -3.8799729 -3.9611111 -4.0293245 -4.1141567 -4.207581 -4.276371 -4.3136125][-4.3038359 -4.283927 -4.2461209 -4.1778722 -4.07145 -3.9467485 -3.8136117 -3.743495 -3.8091824 -3.9165843 -4.008388 -4.1104717 -4.2129593 -4.2818584 -4.3151264][-4.3016381 -4.2834311 -4.2507992 -4.1922216 -4.0938578 -3.9761014 -3.8543961 -3.7839091 -3.8388948 -3.9486859 -4.0472326 -4.1490707 -4.242321 -4.2994204 -4.3217454][-4.3103418 -4.2959423 -4.26717 -4.2175112 -4.1332426 -4.0377235 -3.9494998 -3.8973846 -3.9392769 -4.03672 -4.1285772 -4.2136083 -4.2816029 -4.3175673 -4.3279791][-4.323688 -4.31343 -4.287147 -4.2440476 -4.1747971 -4.1017141 -4.0456114 -4.0142293 -4.0496368 -4.13261 -4.2111263 -4.2730689 -4.3133955 -4.3287935 -4.3303542][-4.3294134 -4.3185229 -4.2921352 -4.2553134 -4.2047977 -4.1577139 -4.1311707 -4.1223969 -4.1519327 -4.2164807 -4.2785563 -4.3182244 -4.335258 -4.3351293 -4.3304133][-4.325182 -4.310358 -4.2815185 -4.2471118 -4.21027 -4.1891809 -4.1871924 -4.1924219 -4.2180295 -4.2663035 -4.3147893 -4.3407369 -4.3440495 -4.3355374 -4.3280854][-4.3115745 -4.285327 -4.2476087 -4.2095437 -4.178483 -4.1761827 -4.1975713 -4.2171869 -4.2442074 -4.2827115 -4.3209739 -4.3414564 -4.3429937 -4.3341126 -4.3264742][-4.2918439 -4.2491207 -4.1948471 -4.1446214 -4.110013 -4.1155219 -4.1567469 -4.1933794 -4.2257395 -4.2638507 -4.3032532 -4.3280559 -4.33629 -4.3318925 -4.3253546]]...]
INFO - root - 2017-12-05 22:31:14.384914: step 49210, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.884 sec/batch; 69h:32m:14s remains)
INFO - root - 2017-12-05 22:31:23.580533: step 49220, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 73h:59m:42s remains)
INFO - root - 2017-12-05 22:31:32.881863: step 49230, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 68h:45m:15s remains)
INFO - root - 2017-12-05 22:31:42.247575: step 49240, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 71h:53m:58s remains)
INFO - root - 2017-12-05 22:31:51.456970: step 49250, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.862 sec/batch; 67h:50m:30s remains)
INFO - root - 2017-12-05 22:32:00.720444: step 49260, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 77h:18m:39s remains)
INFO - root - 2017-12-05 22:32:09.867395: step 49270, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 74h:23m:40s remains)
INFO - root - 2017-12-05 22:32:19.172031: step 49280, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 73h:30m:20s remains)
INFO - root - 2017-12-05 22:32:28.595057: step 49290, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 77h:58m:42s remains)
INFO - root - 2017-12-05 22:32:38.219796: step 49300, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 75h:48m:22s remains)
2017-12-05 22:32:39.021618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1826415 -4.168848 -4.1471925 -4.1261544 -4.1261296 -4.1392064 -4.1406693 -4.1234879 -4.1036911 -4.1047254 -4.1408992 -4.1923256 -4.2360296 -4.2676821 -4.2800074][-4.1301813 -4.120182 -4.1161175 -4.1072273 -4.1086721 -4.1119194 -4.1057234 -4.0879827 -4.0753717 -4.0854836 -4.1253209 -4.1787934 -4.2246637 -4.2606659 -4.2774811][-4.0680547 -4.0662441 -4.0883355 -4.0948391 -4.0984182 -4.0962915 -4.0815926 -4.0553741 -4.04463 -4.0719824 -4.1192274 -4.1714926 -4.2148485 -4.2540493 -4.2740498][-4.0296783 -4.03707 -4.0817714 -4.0980864 -4.1018109 -4.095952 -4.0714974 -4.028255 -4.0155563 -4.0624747 -4.1220608 -4.1744728 -4.2127376 -4.2508717 -4.2720027][-4.019567 -4.0314422 -4.0810823 -4.1080952 -4.1148553 -4.1023469 -4.0635953 -3.99788 -3.9758165 -4.0434728 -4.1203346 -4.1800671 -4.2182703 -4.253757 -4.2749372][-4.0332222 -4.0420046 -4.084549 -4.1158695 -4.1270537 -4.1127343 -4.063087 -3.9676397 -3.9163768 -3.9977729 -4.1011992 -4.1801271 -4.22551 -4.2603936 -4.2803187][-4.05633 -4.0558519 -4.0860376 -4.12113 -4.1408381 -4.1336212 -4.0758247 -3.94532 -3.8548932 -3.9469748 -4.07757 -4.1775055 -4.2333965 -4.26837 -4.286448][-4.0792141 -4.0677905 -4.0907326 -4.1362119 -4.1656518 -4.1626868 -4.1028647 -3.9627547 -3.8642778 -3.9519303 -4.0789652 -4.17968 -4.2371907 -4.2714481 -4.2893438][-4.1005735 -4.0822325 -4.1052241 -4.1583805 -4.1908426 -4.1893635 -4.142591 -4.0307302 -3.9571242 -4.0204034 -4.1075087 -4.1832104 -4.2367907 -4.2716622 -4.2897706][-4.1145854 -4.1010261 -4.12475 -4.1735187 -4.2049127 -4.2069273 -4.1776948 -4.0990262 -4.0493364 -4.0853128 -4.1361389 -4.18563 -4.2340221 -4.2703443 -4.288178][-4.1275306 -4.1209121 -4.1401067 -4.1783834 -4.2044725 -4.2123156 -4.1948137 -4.1365972 -4.099638 -4.1172328 -4.14828 -4.1872811 -4.2348394 -4.2728987 -4.2882051][-4.1333914 -4.1392965 -4.1577592 -4.1882958 -4.212461 -4.2205572 -4.2050414 -4.1546354 -4.1214972 -4.1287532 -4.1571565 -4.1976523 -4.2453523 -4.2810469 -4.2919621][-4.1383066 -4.1569662 -4.182982 -4.2121964 -4.2330022 -4.2322788 -4.2098393 -4.1590424 -4.1287379 -4.1349335 -4.1686778 -4.2122049 -4.2560968 -4.2879834 -4.297183][-4.1541834 -4.1748533 -4.2044058 -4.2342415 -4.2541957 -4.2451577 -4.2099566 -4.14978 -4.1186252 -4.1278024 -4.1673794 -4.2149539 -4.2591286 -4.2918515 -4.3018155][-4.1739626 -4.188025 -4.2150769 -4.2487369 -4.2705755 -4.2542129 -4.2019811 -4.1280975 -4.0907063 -4.098825 -4.14495 -4.2016826 -4.2525783 -4.2899184 -4.3021407]]...]
INFO - root - 2017-12-05 22:32:48.248640: step 49310, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.994 sec/batch; 78h:09m:11s remains)
INFO - root - 2017-12-05 22:32:57.733258: step 49320, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 75h:20m:34s remains)
INFO - root - 2017-12-05 22:33:07.185838: step 49330, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 74h:00m:39s remains)
INFO - root - 2017-12-05 22:33:16.319663: step 49340, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 73h:02m:18s remains)
INFO - root - 2017-12-05 22:33:25.600055: step 49350, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 74h:06m:39s remains)
INFO - root - 2017-12-05 22:33:34.919308: step 49360, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 75h:47m:13s remains)
INFO - root - 2017-12-05 22:33:44.372476: step 49370, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 75h:53m:20s remains)
INFO - root - 2017-12-05 22:33:53.635867: step 49380, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.900 sec/batch; 70h:48m:39s remains)
INFO - root - 2017-12-05 22:34:03.059518: step 49390, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.938 sec/batch; 73h:47m:56s remains)
INFO - root - 2017-12-05 22:34:12.376184: step 49400, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 74h:37m:17s remains)
2017-12-05 22:34:13.120903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2951813 -4.2972727 -4.2963262 -4.2932606 -4.2879863 -4.2836189 -4.282196 -4.283473 -4.2861214 -4.2877536 -4.2883492 -4.2875104 -4.2845774 -4.28123 -4.2783718][-4.3004279 -4.3024774 -4.3000855 -4.2940221 -4.2866879 -4.2830806 -4.2834496 -4.2843513 -4.2857857 -4.2869387 -4.2871871 -4.2871432 -4.2841525 -4.2796941 -4.2740607][-4.3033695 -4.306 -4.3025293 -4.2927775 -4.2830882 -4.2789488 -4.2793393 -4.2768469 -4.2736669 -4.2734413 -4.2745123 -4.2763529 -4.2753034 -4.2723541 -4.2669086][-4.3093657 -4.3122969 -4.3054075 -4.2882671 -4.2717352 -4.2615528 -4.2567034 -4.2470322 -4.2384777 -4.2387662 -4.2430978 -4.2491913 -4.2539067 -4.2565703 -4.2549744][-4.3150215 -4.315197 -4.3013086 -4.2727437 -4.2442789 -4.2229905 -4.2071552 -4.1860065 -4.1694694 -4.1719332 -4.184401 -4.2003794 -4.2159119 -4.2291541 -4.2358284][-4.3156056 -4.3102961 -4.2871103 -4.2476978 -4.2063136 -4.1725783 -4.1417723 -4.1028867 -4.0751534 -4.08339 -4.1094794 -4.1382823 -4.1668105 -4.1938996 -4.2126012][-4.305263 -4.2931056 -4.2613468 -4.2127519 -4.1595469 -4.1143327 -4.0640903 -4.002027 -3.9646888 -3.9888833 -4.036025 -4.0804758 -4.1227026 -4.1631451 -4.1948514][-4.2858963 -4.2661972 -4.2262073 -4.167675 -4.1033344 -4.0476408 -3.981601 -3.906615 -3.8759189 -3.9274409 -3.9967978 -4.0534658 -4.1036019 -4.1514363 -4.190733][-4.261497 -4.2345805 -4.1886382 -4.1243172 -4.0575242 -4.0048409 -3.9490435 -3.8901882 -3.8816793 -3.946255 -4.014616 -4.0656996 -4.1091104 -4.152442 -4.19012][-4.2382135 -4.2072372 -4.1618314 -4.1022081 -4.0459852 -4.01664 -3.9981687 -3.9755132 -3.9841542 -4.0306125 -4.0701141 -4.0967669 -4.12131 -4.1530647 -4.1839147][-4.2235494 -4.1923432 -4.1544428 -4.1087508 -4.0697646 -4.06407 -4.0769925 -4.080863 -4.0927396 -4.1143827 -4.1262479 -4.13017 -4.1382093 -4.1571407 -4.1775994][-4.2292051 -4.2041926 -4.1759691 -4.1457915 -4.1225419 -4.1275392 -4.1498065 -4.1609197 -4.169445 -4.1758122 -4.17307 -4.1666446 -4.1671681 -4.1770706 -4.1866455][-4.2500577 -4.2352242 -4.2148676 -4.1937656 -4.177907 -4.1826105 -4.2009873 -4.2111516 -4.2171159 -4.2193427 -4.2144275 -4.2082386 -4.2083292 -4.2132225 -4.2149768][-4.2722921 -4.2653918 -4.2490969 -4.2315092 -4.2177539 -4.2204051 -4.2342052 -4.2414117 -4.2448759 -4.2470207 -4.2453051 -4.2446876 -4.24822 -4.2518611 -4.2495074][-4.2865815 -4.283977 -4.271172 -4.255002 -4.24151 -4.2423773 -4.2522097 -4.2569427 -4.2583303 -4.2596536 -4.2606039 -4.2642851 -4.2699718 -4.2735114 -4.2707925]]...]
INFO - root - 2017-12-05 22:34:22.568592: step 49410, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 72h:18m:16s remains)
INFO - root - 2017-12-05 22:34:31.889382: step 49420, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 75h:42m:36s remains)
INFO - root - 2017-12-05 22:34:41.163000: step 49430, loss = 2.05, batch loss = 2.00 (10.3 examples/sec; 0.773 sec/batch; 60h:47m:12s remains)
INFO - root - 2017-12-05 22:34:50.475042: step 49440, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 73h:04m:13s remains)
INFO - root - 2017-12-05 22:34:59.594560: step 49450, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 67h:56m:17s remains)
INFO - root - 2017-12-05 22:35:09.000195: step 49460, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.939 sec/batch; 73h:51m:43s remains)
INFO - root - 2017-12-05 22:35:18.432114: step 49470, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 72h:56m:05s remains)
INFO - root - 2017-12-05 22:35:27.719339: step 49480, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 74h:40m:38s remains)
INFO - root - 2017-12-05 22:35:37.110271: step 49490, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 74h:56m:02s remains)
INFO - root - 2017-12-05 22:35:46.371101: step 49500, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 74h:26m:03s remains)
2017-12-05 22:35:47.170890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1673565 -4.1500716 -4.1395054 -4.1221948 -4.0951319 -4.0721521 -4.0685582 -4.0674191 -4.0570922 -4.0608244 -4.0907331 -4.1269426 -4.1671262 -4.2228427 -4.269938][-4.1319523 -4.1192427 -4.1208239 -4.1215916 -4.1082945 -4.09021 -4.0908456 -4.0938911 -4.0804963 -4.0781212 -4.105227 -4.1372385 -4.173213 -4.2251158 -4.2691622][-4.1031394 -4.0997238 -4.115643 -4.1370673 -4.1435351 -4.1357827 -4.1419444 -4.1491151 -4.1365891 -4.1260128 -4.1407909 -4.1615028 -4.1873093 -4.2313027 -4.2708449][-4.0997219 -4.1099682 -4.1334915 -4.1581492 -4.1707935 -4.1693635 -4.1786375 -4.1925459 -4.1908789 -4.1847162 -4.1935945 -4.2055244 -4.2201481 -4.252758 -4.2830248][-4.1180291 -4.131424 -4.1474352 -4.15851 -4.1615005 -4.1562343 -4.159718 -4.1682725 -4.1741734 -4.1813188 -4.1978049 -4.2124562 -4.22678 -4.2581124 -4.2857332][-4.1434879 -4.1431217 -4.1416011 -4.1339693 -4.11875 -4.098454 -4.0830345 -4.075295 -4.0776148 -4.0995779 -4.135498 -4.1680517 -4.1954074 -4.2352476 -4.269896][-4.1739812 -4.1539111 -4.1324768 -4.107656 -4.0762815 -4.035934 -3.994571 -3.9604657 -3.9465957 -3.978507 -4.0423479 -4.1008892 -4.1488538 -4.2016964 -4.2460995][-4.1977205 -4.1676316 -4.1373582 -4.1072426 -4.072567 -4.0263791 -3.970386 -3.9161608 -3.8822246 -3.9073169 -3.9784718 -4.0499043 -4.1141224 -4.178648 -4.232491][-4.2170839 -4.188827 -4.1640034 -4.1427126 -4.1183224 -4.0844955 -4.040123 -3.9961803 -3.9681385 -3.98694 -4.0400782 -4.09106 -4.1415691 -4.1943007 -4.2403421][-4.2207704 -4.2033424 -4.19122 -4.1823444 -4.1709905 -4.1528788 -4.1281257 -4.1045494 -4.0897837 -4.104341 -4.1389084 -4.16678 -4.194437 -4.2286077 -4.2610884][-4.2275033 -4.2220578 -4.2225218 -4.2246904 -4.2225885 -4.2105927 -4.1921773 -4.1739731 -4.157795 -4.1623764 -4.1817331 -4.1992669 -4.2187 -4.2462535 -4.2720947][-4.236721 -4.2355795 -4.2407708 -4.2474375 -4.250577 -4.2419295 -4.2237573 -4.1983237 -4.1690845 -4.1629062 -4.1737819 -4.1857514 -4.2043195 -4.2352533 -4.2653][-4.2311482 -4.2280092 -4.2303114 -4.2338095 -4.2347879 -4.2240291 -4.2023482 -4.16595 -4.1235108 -4.1135583 -4.1244912 -4.1397114 -4.1641607 -4.2037373 -4.2424779][-4.2026711 -4.1955366 -4.1942372 -4.1938367 -4.1921606 -4.18108 -4.1588607 -4.1194792 -4.074975 -4.067131 -4.0832596 -4.1032567 -4.1314793 -4.1773682 -4.221468][-4.1893144 -4.1821327 -4.17981 -4.1781983 -4.1761174 -4.1665735 -4.1483903 -4.1165204 -4.08157 -4.0764852 -4.0932617 -4.1117182 -4.1359921 -4.1793566 -4.2227044]]...]
INFO - root - 2017-12-05 22:35:56.491215: step 49510, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 76h:05m:12s remains)
INFO - root - 2017-12-05 22:36:05.869393: step 49520, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 77h:54m:32s remains)
INFO - root - 2017-12-05 22:36:15.417398: step 49530, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 76h:17m:33s remains)
INFO - root - 2017-12-05 22:36:24.846018: step 49540, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 68h:00m:19s remains)
INFO - root - 2017-12-05 22:36:34.118622: step 49550, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.920 sec/batch; 72h:16m:58s remains)
INFO - root - 2017-12-05 22:36:43.493522: step 49560, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 72h:47m:00s remains)
INFO - root - 2017-12-05 22:36:52.928077: step 49570, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 71h:20m:48s remains)
INFO - root - 2017-12-05 22:37:02.198940: step 49580, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 65h:22m:54s remains)
INFO - root - 2017-12-05 22:37:11.764130: step 49590, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 72h:02m:11s remains)
INFO - root - 2017-12-05 22:37:21.112628: step 49600, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 73h:43m:24s remains)
2017-12-05 22:37:21.905960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1854043 -4.19306 -4.1707625 -4.1263847 -4.1051483 -4.1093874 -4.1179094 -4.1176186 -4.1239343 -4.1488042 -4.181983 -4.2084508 -4.2305274 -4.2491336 -4.2664385][-4.1806612 -4.1873074 -4.1690865 -4.1344662 -4.1215506 -4.1241903 -4.1323104 -4.146121 -4.1654749 -4.1889844 -4.2147746 -4.2358379 -4.2496691 -4.2588296 -4.2697439][-4.1862803 -4.1854224 -4.1725831 -4.1473942 -4.1387286 -4.1409254 -4.1496754 -4.1720629 -4.2017522 -4.2277479 -4.2468667 -4.2603312 -4.2630095 -4.2645955 -4.2720628][-4.18116 -4.1744685 -4.1649237 -4.1429687 -4.1352906 -4.1427989 -4.1585011 -4.1856666 -4.2166729 -4.2401719 -4.2534928 -4.2608113 -4.258801 -4.2601991 -4.2692165][-4.166811 -4.1557465 -4.1481891 -4.125968 -4.1131859 -4.118896 -4.13635 -4.1636233 -4.1909409 -4.2101583 -4.2230139 -4.2321167 -4.2354479 -4.2443094 -4.2591271][-4.1453028 -4.1346536 -4.1282611 -4.1013484 -4.083426 -4.0813656 -4.0879111 -4.1101232 -4.1353106 -4.1532454 -4.1722851 -4.1886787 -4.2031264 -4.2252722 -4.24832][-4.1376643 -4.1297474 -4.124136 -4.0919371 -4.0674467 -4.0496674 -4.0318909 -4.04107 -4.07074 -4.0984392 -4.1272273 -4.1545777 -4.18318 -4.2172394 -4.2449369][-4.1526136 -4.1442838 -4.1349678 -4.1024938 -4.0748534 -4.0433316 -4.0019875 -3.9891126 -4.0228543 -4.0652461 -4.1025572 -4.1391616 -4.1789265 -4.2199278 -4.2483945][-4.1973925 -4.1886811 -4.1752186 -4.1436739 -4.1138172 -4.0716076 -4.0123568 -3.9793944 -4.010663 -4.0594521 -4.0995803 -4.1400595 -4.1862993 -4.227282 -4.2542896][-4.2456293 -4.2380972 -4.2196927 -4.1885743 -4.1637154 -4.126729 -4.0709066 -4.0364208 -4.0564957 -4.0917025 -4.1249456 -4.164978 -4.2104197 -4.2421408 -4.261764][-4.2768636 -4.2686391 -4.2514796 -4.2253017 -4.2089429 -4.1837616 -4.1431522 -4.1211081 -4.1345692 -4.1518612 -4.1729 -4.2055392 -4.2431731 -4.2614036 -4.2721567][-4.2872119 -4.2819004 -4.2712007 -4.2522535 -4.2391148 -4.2216439 -4.1994457 -4.1933146 -4.2019072 -4.2080312 -4.2178955 -4.2409582 -4.2672029 -4.2759843 -4.2819428][-4.2865458 -4.2855749 -4.281168 -4.2704644 -4.259685 -4.247189 -4.2385192 -4.2426648 -4.2494617 -4.2492537 -4.253633 -4.2673807 -4.2799096 -4.2837906 -4.2891188][-4.2879634 -4.2883844 -4.2865791 -4.28207 -4.2771163 -4.27265 -4.2734342 -4.2812414 -4.2875443 -4.2858281 -4.2864833 -4.29054 -4.291791 -4.2917676 -4.2962265][-4.2895403 -4.28868 -4.2865968 -4.2858005 -4.2855954 -4.2872252 -4.2928681 -4.3018093 -4.3063288 -4.3043528 -4.304235 -4.3032975 -4.2995644 -4.2988667 -4.3032227]]...]
INFO - root - 2017-12-05 22:37:31.343461: step 49610, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.884 sec/batch; 69h:29m:47s remains)
INFO - root - 2017-12-05 22:37:40.664558: step 49620, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 73h:23m:35s remains)
INFO - root - 2017-12-05 22:37:49.753220: step 49630, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.919 sec/batch; 72h:12m:06s remains)
INFO - root - 2017-12-05 22:37:59.044625: step 49640, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 73h:56m:31s remains)
INFO - root - 2017-12-05 22:38:08.540910: step 49650, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 74h:46m:22s remains)
INFO - root - 2017-12-05 22:38:17.909328: step 49660, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 72h:41m:16s remains)
INFO - root - 2017-12-05 22:38:27.201767: step 49670, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 76h:19m:40s remains)
INFO - root - 2017-12-05 22:38:36.414079: step 49680, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.974 sec/batch; 76h:29m:27s remains)
INFO - root - 2017-12-05 22:38:45.767461: step 49690, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 74h:48m:43s remains)
INFO - root - 2017-12-05 22:38:55.188241: step 49700, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 75h:35m:52s remains)
2017-12-05 22:38:55.903029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1846409 -4.1394506 -4.1095095 -4.115027 -4.1231465 -4.1237764 -4.1049213 -4.0778117 -4.080915 -4.1084571 -4.1444774 -4.1924849 -4.2473044 -4.2912378 -4.3158693][-4.1656184 -4.1083488 -4.0731516 -4.0843935 -4.1038613 -4.11195 -4.0943937 -4.0733538 -4.0899739 -4.1240697 -4.1569433 -4.2003722 -4.2516017 -4.2939782 -4.3153539][-4.16073 -4.0984974 -4.0601835 -4.074995 -4.1021652 -4.1116619 -4.0892739 -4.0693727 -4.0977311 -4.1400576 -4.1750607 -4.2171021 -4.2644634 -4.3013778 -4.3167958][-4.1687365 -4.1133313 -4.0809832 -4.0957003 -4.1252871 -4.1308217 -4.1009016 -4.0769868 -4.1047726 -4.1520991 -4.194571 -4.2369194 -4.2799168 -4.309144 -4.3190966][-4.1905503 -4.1433797 -4.118216 -4.13096 -4.1576176 -4.1580377 -4.1248212 -4.0943208 -4.1124425 -4.1569815 -4.2072124 -4.253902 -4.2929616 -4.3174691 -4.3236732][-4.2117476 -4.1766186 -4.1571732 -4.1631479 -4.1823416 -4.1813149 -4.1467562 -4.1082668 -4.1152673 -4.1553965 -4.2097445 -4.2617884 -4.300456 -4.3243709 -4.328208][-4.2334242 -4.2081461 -4.18815 -4.1843071 -4.1934562 -4.19152 -4.1545606 -4.1057215 -4.1056943 -4.1452351 -4.200397 -4.2578917 -4.2997236 -4.3254118 -4.3306718][-4.2535772 -4.2322893 -4.2119102 -4.202908 -4.2016606 -4.1913185 -4.1513758 -4.0985346 -4.1010942 -4.1418505 -4.1919203 -4.2474356 -4.2920909 -4.3215289 -4.3305326][-4.266036 -4.2467227 -4.2279344 -4.2174621 -4.2086926 -4.1870866 -4.1470838 -4.1006904 -4.1059747 -4.1444521 -4.1895976 -4.2381496 -4.2831988 -4.3165193 -4.3292212][-4.2713718 -4.2547936 -4.2378674 -4.2295914 -4.2177596 -4.1897435 -4.1539626 -4.1200838 -4.1239944 -4.155601 -4.1951632 -4.2368731 -4.2802262 -4.313231 -4.3274226][-4.2707987 -4.2564907 -4.242188 -4.2401423 -4.234694 -4.2058659 -4.1724582 -4.1478209 -4.1509304 -4.1744957 -4.2064714 -4.2434092 -4.2838511 -4.313097 -4.3257623][-4.2641129 -4.2555628 -4.245873 -4.2498679 -4.2506738 -4.2256169 -4.19635 -4.1811666 -4.1857729 -4.2066045 -4.2313318 -4.25957 -4.2929196 -4.3159947 -4.3256769][-4.2510839 -4.2491617 -4.2456427 -4.25036 -4.2483053 -4.2303214 -4.213161 -4.2098188 -4.2213073 -4.2452006 -4.2647915 -4.2815895 -4.3045311 -4.3204856 -4.3281083][-4.24181 -4.2420397 -4.2443156 -4.2490387 -4.2448163 -4.2331672 -4.2274065 -4.2331843 -4.2483273 -4.2720056 -4.285532 -4.2971969 -4.3159113 -4.3276386 -4.3335905][-4.2576194 -4.26034 -4.2646027 -4.2671785 -4.2570252 -4.2438707 -4.2405162 -4.2452426 -4.2578335 -4.2798142 -4.2914214 -4.305491 -4.3246455 -4.3353729 -4.3396792]]...]
INFO - root - 2017-12-05 22:39:05.338836: step 49710, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 67h:13m:39s remains)
INFO - root - 2017-12-05 22:39:14.704248: step 49720, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 74h:18m:22s remains)
INFO - root - 2017-12-05 22:39:23.997705: step 49730, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 76h:14m:04s remains)
INFO - root - 2017-12-05 22:39:33.363433: step 49740, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 71h:59m:09s remains)
INFO - root - 2017-12-05 22:39:42.649730: step 49750, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 68h:01m:43s remains)
INFO - root - 2017-12-05 22:39:51.951445: step 49760, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 71h:59m:15s remains)
INFO - root - 2017-12-05 22:40:01.264996: step 49770, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 72h:38m:56s remains)
INFO - root - 2017-12-05 22:40:10.281484: step 49780, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 67h:53m:05s remains)
INFO - root - 2017-12-05 22:40:19.477585: step 49790, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 70h:48m:34s remains)
INFO - root - 2017-12-05 22:40:28.867398: step 49800, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.914 sec/batch; 71h:48m:06s remains)
2017-12-05 22:40:29.613136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2458606 -4.2422366 -4.2314239 -4.217134 -4.19048 -4.1574855 -4.1393051 -4.140058 -4.1487789 -4.16573 -4.19219 -4.2162895 -4.2476444 -4.2795262 -4.2997169][-4.2166972 -4.2160826 -4.2071943 -4.1930685 -4.1622968 -4.119473 -4.0896344 -4.0833974 -4.0896778 -4.1102371 -4.1495271 -4.1858273 -4.2279062 -4.2706771 -4.2977805][-4.1813064 -4.1880579 -4.1858635 -4.1751204 -4.1431003 -4.0925131 -4.05129 -4.0399504 -4.0440426 -4.06624 -4.1170578 -4.1670809 -4.2175179 -4.2675614 -4.2996597][-4.1437731 -4.1566792 -4.166091 -4.16218 -4.1301756 -4.0729756 -4.0210285 -4.0056391 -4.0096273 -4.0304594 -4.0895076 -4.1529937 -4.2098975 -4.2635646 -4.3004103][-4.1181774 -4.1319818 -4.151299 -4.1563578 -4.1213045 -4.0561109 -3.9943507 -3.9770174 -3.9887354 -4.0085425 -4.0661917 -4.1381464 -4.2003951 -4.2571459 -4.2997136][-4.091804 -4.1076474 -4.1395984 -4.1566358 -4.1144567 -4.0357313 -3.9568269 -3.9395282 -3.9737425 -4.005053 -4.058825 -4.1307354 -4.19384 -4.2524414 -4.2983003][-4.0450377 -4.0685225 -4.1208153 -4.1526656 -4.1067333 -4.0085206 -3.9004409 -3.8717663 -3.9368162 -3.9962885 -4.0549493 -4.128963 -4.1944866 -4.2527642 -4.2975683][-3.974344 -4.0081291 -4.0896144 -4.136817 -4.0913606 -3.9718151 -3.8171568 -3.7559671 -3.8584833 -3.9652333 -4.0451112 -4.1306386 -4.2017469 -4.2577033 -4.2971859][-3.9148347 -3.9540334 -4.057291 -4.1192141 -4.085381 -3.9594409 -3.7587152 -3.6441913 -3.776474 -3.9294572 -4.0330157 -4.1322331 -4.2090368 -4.2648783 -4.2976093][-3.9350836 -3.9634635 -4.0576634 -4.1198559 -4.10593 -4.0071821 -3.816802 -3.6775231 -3.7851293 -3.9374325 -4.0415854 -4.1374745 -4.2143335 -4.270875 -4.2988353][-4.0056725 -4.0264626 -4.0934458 -4.1422057 -4.1406789 -4.0793056 -3.939636 -3.8160391 -3.8729646 -3.9894958 -4.0788684 -4.15773 -4.2237992 -4.2784529 -4.302784][-4.0777164 -4.0964255 -4.1408491 -4.172771 -4.17854 -4.1438317 -4.0524535 -3.9573305 -3.9803789 -4.0579915 -4.1256657 -4.18553 -4.2378 -4.2868247 -4.3068285][-4.1376967 -4.1521044 -4.1848445 -4.2071447 -4.2153707 -4.2009978 -4.1454229 -4.0785279 -4.0828128 -4.1276808 -4.17367 -4.217566 -4.25933 -4.2980442 -4.310967][-4.195261 -4.2031021 -4.2276196 -4.24317 -4.2536511 -4.2494073 -4.2166905 -4.1719766 -4.1671176 -4.1913595 -4.2192864 -4.2511811 -4.2832751 -4.3091006 -4.3142118][-4.2361178 -4.2355542 -4.2498655 -4.2651548 -4.2825217 -4.2846236 -4.2655 -4.2323842 -4.221909 -4.2340317 -4.2511921 -4.2757335 -4.3004966 -4.3159781 -4.3162074]]...]
INFO - root - 2017-12-05 22:40:38.921006: step 49810, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.915 sec/batch; 71h:49m:53s remains)
INFO - root - 2017-12-05 22:40:48.183580: step 49820, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 72h:58m:03s remains)
INFO - root - 2017-12-05 22:40:57.389583: step 49830, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 67h:47m:48s remains)
INFO - root - 2017-12-05 22:41:06.692905: step 49840, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 73h:44m:07s remains)
INFO - root - 2017-12-05 22:41:16.028443: step 49850, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.897 sec/batch; 70h:25m:27s remains)
INFO - root - 2017-12-05 22:41:25.423279: step 49860, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 74h:44m:28s remains)
INFO - root - 2017-12-05 22:41:34.737889: step 49870, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 72h:52m:33s remains)
INFO - root - 2017-12-05 22:41:44.111462: step 49880, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 71h:21m:24s remains)
INFO - root - 2017-12-05 22:41:53.395028: step 49890, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.928 sec/batch; 72h:49m:53s remains)
INFO - root - 2017-12-05 22:42:02.946240: step 49900, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.936 sec/batch; 73h:30m:53s remains)
2017-12-05 22:42:03.736676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1621242 -4.1601896 -4.1824293 -4.2157192 -4.2415032 -4.2531695 -4.2587905 -4.2596412 -4.2477336 -4.2055278 -4.1443672 -4.0912247 -4.0615897 -4.0516491 -4.0668659][-4.1627359 -4.1688986 -4.2085867 -4.2572436 -4.2888594 -4.2909012 -4.27714 -4.2557936 -4.225193 -4.170805 -4.1145048 -4.0771937 -4.0619555 -4.0631628 -4.0860014][-4.170083 -4.1883416 -4.2382531 -4.290823 -4.3214064 -4.3120484 -4.2768049 -4.2295561 -4.1808038 -4.130033 -4.0931573 -4.076292 -4.0707927 -4.0779047 -4.1103487][-4.1844745 -4.2200813 -4.2760186 -4.3211155 -4.3344975 -4.30267 -4.2394891 -4.162684 -4.1063561 -4.0817623 -4.0765638 -4.08176 -4.0861006 -4.101305 -4.1373067][-4.2176304 -4.2671609 -4.3175387 -4.3408937 -4.3201938 -4.2501807 -4.1457109 -4.0393405 -3.9944715 -4.0198941 -4.0594454 -4.092474 -4.1137567 -4.1359248 -4.16367][-4.2518988 -4.3078828 -4.3463268 -4.3398137 -4.2792263 -4.1632218 -4.0045533 -3.8594012 -3.8465824 -3.9467723 -4.0485311 -4.1166573 -4.1556463 -4.1768656 -4.18921][-4.2805519 -4.3321557 -4.3491149 -4.3117447 -4.2158628 -4.0596113 -3.8543034 -3.6881843 -3.7332363 -3.9069641 -4.0559363 -4.1483541 -4.1935315 -4.2061296 -4.2031379][-4.2955918 -4.3344688 -4.3323169 -4.2758732 -4.16171 -3.9972088 -3.804821 -3.6864085 -3.7809396 -3.9600151 -4.1055818 -4.1941724 -4.2301207 -4.2300863 -4.21748][-4.2925062 -4.316257 -4.3051505 -4.2484469 -4.1459136 -4.02018 -3.9010901 -3.8611658 -3.9471195 -4.0717034 -4.1762486 -4.2425628 -4.264317 -4.2558794 -4.2406893][-4.2824492 -4.2952194 -4.2850237 -4.2392893 -4.1658254 -4.0923762 -4.0401793 -4.0460544 -4.1053786 -4.1738482 -4.2372475 -4.2777615 -4.2858844 -4.2725077 -4.2576542][-4.2775636 -4.2827086 -4.2773886 -4.2477093 -4.203342 -4.1685352 -4.1554623 -4.1771331 -4.2161245 -4.2528152 -4.28727 -4.3037524 -4.2978287 -4.2815962 -4.2671442][-4.2783394 -4.2787695 -4.2764816 -4.2609468 -4.2389407 -4.2278981 -4.235568 -4.2606454 -4.2855716 -4.3040986 -4.3165035 -4.3132138 -4.2980208 -4.2819467 -4.2700095][-4.2820539 -4.2791739 -4.2784877 -4.2716279 -4.2645454 -4.2691765 -4.2874293 -4.3115129 -4.3254638 -4.3282785 -4.3216076 -4.304173 -4.2889175 -4.2792726 -4.2723589][-4.2787881 -4.2739196 -4.2713256 -4.2674408 -4.2705688 -4.287159 -4.3112869 -4.3311911 -4.3344531 -4.3209705 -4.2956 -4.2701058 -4.2617354 -4.2635927 -4.2647829][-4.250042 -4.2420959 -4.2332134 -4.2272696 -4.2347813 -4.2615519 -4.2933865 -4.3128924 -4.3086176 -4.2813959 -4.243681 -4.2134957 -4.2118621 -4.2252169 -4.2339568]]...]
INFO - root - 2017-12-05 22:42:12.953173: step 49910, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 70h:16m:43s remains)
INFO - root - 2017-12-05 22:42:22.207168: step 49920, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 72h:56m:46s remains)
INFO - root - 2017-12-05 22:42:31.660314: step 49930, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 77h:02m:05s remains)
INFO - root - 2017-12-05 22:42:41.019867: step 49940, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.947 sec/batch; 74h:18m:32s remains)
INFO - root - 2017-12-05 22:42:50.068408: step 49950, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 75h:55m:55s remains)
INFO - root - 2017-12-05 22:42:59.404267: step 49960, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 76h:27m:37s remains)
INFO - root - 2017-12-05 22:43:09.060167: step 49970, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 76h:51m:03s remains)
INFO - root - 2017-12-05 22:43:18.242455: step 49980, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 72h:32m:09s remains)
INFO - root - 2017-12-05 22:43:27.449832: step 49990, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 74h:21m:35s remains)
INFO - root - 2017-12-05 22:43:36.656840: step 50000, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 72h:36m:47s remains)
2017-12-05 22:43:37.431283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1636653 -4.1693826 -4.1645246 -4.1572909 -4.1635036 -4.176312 -4.1743855 -4.1754465 -4.1728559 -4.16459 -4.1679978 -4.180356 -4.1937346 -4.2040424 -4.20896][-4.1306639 -4.1397061 -4.1474795 -4.1480885 -4.151474 -4.1586394 -4.1539154 -4.15574 -4.157795 -4.1534452 -4.1623182 -4.1739016 -4.184144 -4.1910062 -4.1965442][-4.1012645 -4.1114736 -4.1305466 -4.1399274 -4.1391788 -4.1378975 -4.130322 -4.1329679 -4.1393261 -4.1357613 -4.1425471 -4.1503859 -4.1562233 -4.1597786 -4.1668892][-4.1106896 -4.113771 -4.1349711 -4.1465487 -4.140955 -4.1295376 -4.1199684 -4.1278367 -4.1352205 -4.1254225 -4.121047 -4.1191444 -4.1182213 -4.1199031 -4.1285715][-4.133872 -4.1287951 -4.1432557 -4.1489167 -4.1334662 -4.1155157 -4.1124463 -4.136971 -4.1508155 -4.1385193 -4.1207671 -4.1051855 -4.0965457 -4.0990362 -4.1090493][-4.1327658 -4.1214032 -4.1301618 -4.1254091 -4.0943947 -4.0668612 -4.0748162 -4.1209507 -4.1475372 -4.1427751 -4.1216674 -4.0986571 -4.0832949 -4.0861006 -4.0957394][-4.11307 -4.0988312 -4.1022635 -4.0860958 -4.0446129 -4.0058155 -4.0116158 -4.0671716 -4.1042271 -4.1068969 -4.0888729 -4.0692406 -4.054904 -4.06334 -4.0788894][-4.1144271 -4.0990467 -4.0970135 -4.0727844 -4.0306444 -3.9892976 -3.985815 -4.028007 -4.0615239 -4.0606365 -4.0420346 -4.0312405 -4.0244622 -4.0405602 -4.0629573][-4.1369896 -4.1156969 -4.1075377 -4.0876174 -4.0624967 -4.0492911 -4.0516992 -4.077106 -4.096067 -4.0800066 -4.0516973 -4.0382657 -4.0362506 -4.0562582 -4.0743923][-4.1612315 -4.1315837 -4.1181216 -4.1080928 -4.0977554 -4.1022625 -4.1123381 -4.131608 -4.14841 -4.1370225 -4.1162076 -4.1009321 -4.0941625 -4.1034913 -4.1089435][-4.172266 -4.1380172 -4.1184797 -4.1147962 -4.1180778 -4.1288171 -4.1359358 -4.1477633 -4.16331 -4.1627059 -4.1593151 -4.1517262 -4.1447978 -4.1433926 -4.1373825][-4.1770792 -4.1441402 -4.1272321 -4.1310277 -4.1404057 -4.1487989 -4.1465893 -4.1491089 -4.1597052 -4.1662083 -4.1777539 -4.181035 -4.1769576 -4.1645169 -4.1438055][-4.1929207 -4.1703992 -4.1671553 -4.1770339 -4.1853909 -4.1908021 -4.1858273 -4.18222 -4.1851544 -4.1941733 -4.2067027 -4.208477 -4.2011862 -4.179224 -4.14563][-4.2025166 -4.1960034 -4.20672 -4.2203684 -4.2285204 -4.2314477 -4.2286978 -4.2271934 -4.2278643 -4.2386336 -4.249001 -4.2451119 -4.2312727 -4.2022114 -4.1652536][-4.2086496 -4.2126403 -4.2294126 -4.2413692 -4.2478561 -4.2465653 -4.242507 -4.2420287 -4.2432971 -4.2563572 -4.2638426 -4.2552943 -4.2420497 -4.215929 -4.1821775]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 22:43:47.635496: step 50010, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 77h:39m:08s remains)
INFO - root - 2017-12-05 22:43:56.965458: step 50020, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.984 sec/batch; 77h:13m:51s remains)
INFO - root - 2017-12-05 22:44:06.568435: step 50030, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 74h:58m:38s remains)
INFO - root - 2017-12-05 22:44:16.051203: step 50040, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 77h:11m:51s remains)
INFO - root - 2017-12-05 22:44:25.216815: step 50050, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 73h:47m:32s remains)
INFO - root - 2017-12-05 22:44:34.558819: step 50060, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 76h:56m:24s remains)
INFO - root - 2017-12-05 22:44:44.052234: step 50070, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 69h:58m:45s remains)
INFO - root - 2017-12-05 22:44:53.298629: step 50080, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 72h:47m:22s remains)
INFO - root - 2017-12-05 22:45:02.711124: step 50090, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 76h:56m:52s remains)
INFO - root - 2017-12-05 22:45:11.974124: step 50100, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.957 sec/batch; 75h:04m:00s remains)
2017-12-05 22:45:12.720854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1722665 -4.1785383 -4.2080946 -4.2430325 -4.2459612 -4.1998873 -4.1280761 -4.083755 -4.1046233 -4.1474829 -4.1781721 -4.2015133 -4.2105613 -4.2077 -4.2079582][-4.1760139 -4.1920958 -4.2213349 -4.246511 -4.2442546 -4.1969233 -4.1323638 -4.0971093 -4.1226215 -4.157876 -4.1766334 -4.1934047 -4.2043571 -4.2114964 -4.2180305][-4.1694117 -4.19811 -4.2281466 -4.2390571 -4.2247777 -4.1747823 -4.1196532 -4.09763 -4.1275234 -4.1588869 -4.17192 -4.1862879 -4.1986423 -4.2074251 -4.2130733][-4.1615586 -4.1968279 -4.2286034 -4.2321253 -4.2083063 -4.1567869 -4.10448 -4.0891771 -4.1187725 -4.150732 -4.1700063 -4.1873536 -4.2019892 -4.2111959 -4.2152076][-4.1611133 -4.1962633 -4.2273216 -4.2265172 -4.1979647 -4.1527658 -4.1099911 -4.0979962 -4.1232285 -4.15192 -4.17068 -4.1878052 -4.2062883 -4.2181578 -4.2237096][-4.1511321 -4.1858125 -4.2196336 -4.21666 -4.1903043 -4.1526427 -4.1163449 -4.1096034 -4.1360168 -4.1647043 -4.1825323 -4.19972 -4.2200675 -4.2318511 -4.2368784][-4.1352944 -4.1681128 -4.2105393 -4.2132659 -4.1907673 -4.1555786 -4.1157165 -4.1150613 -4.1511955 -4.186553 -4.2085185 -4.225841 -4.2439642 -4.2486053 -4.2506714][-4.1150136 -4.1513224 -4.2033134 -4.2146564 -4.1941137 -4.1550307 -4.1091061 -4.1166511 -4.1632204 -4.2038507 -4.2249918 -4.2425013 -4.2593637 -4.2621131 -4.2614818][-4.0988021 -4.141736 -4.2008104 -4.2201343 -4.197731 -4.1516433 -4.10341 -4.1167941 -4.1716733 -4.21943 -4.2399487 -4.2536073 -4.2716103 -4.2765274 -4.2700758][-4.1258745 -4.1620226 -4.21138 -4.2286491 -4.2050486 -4.1621814 -4.1230235 -4.1381316 -4.1916318 -4.2391067 -4.2563567 -4.2671366 -4.28378 -4.2863164 -4.2778239][-4.16219 -4.1903353 -4.2244267 -4.239089 -4.2240639 -4.193541 -4.1698556 -4.1800451 -4.2173004 -4.2548337 -4.2723637 -4.2807722 -4.2957525 -4.29994 -4.2968578][-4.1866212 -4.2145753 -4.2405815 -4.2539511 -4.2492156 -4.2262726 -4.2118878 -4.2139568 -4.2368431 -4.2631435 -4.2757115 -4.2850962 -4.3011847 -4.307765 -4.3097773][-4.2183652 -4.2454367 -4.2551847 -4.2595134 -4.2596288 -4.2456417 -4.2363091 -4.2351727 -4.2477741 -4.2647195 -4.2734013 -4.2845874 -4.3018689 -4.314394 -4.3198552][-4.2393875 -4.2615256 -4.2567964 -4.2509527 -4.2568011 -4.2561917 -4.2505054 -4.2472014 -4.2532687 -4.2636042 -4.272109 -4.2858472 -4.30257 -4.3167624 -4.3251562][-4.2508049 -4.2626071 -4.2515192 -4.2388692 -4.2500629 -4.2624216 -4.2672877 -4.2642193 -4.266078 -4.2681236 -4.2751961 -4.2883821 -4.3022933 -4.3150129 -4.3247218]]...]
INFO - root - 2017-12-05 22:45:22.054682: step 50110, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 69h:41m:37s remains)
INFO - root - 2017-12-05 22:45:31.319657: step 50120, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 67h:49m:56s remains)
INFO - root - 2017-12-05 22:45:40.533556: step 50130, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 75h:54m:56s remains)
INFO - root - 2017-12-05 22:45:49.958507: step 50140, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 73h:56m:03s remains)
INFO - root - 2017-12-05 22:45:59.123196: step 50150, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 69h:56m:56s remains)
INFO - root - 2017-12-05 22:46:08.420571: step 50160, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 70h:41m:52s remains)
INFO - root - 2017-12-05 22:46:17.823699: step 50170, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 73h:37m:14s remains)
INFO - root - 2017-12-05 22:46:27.194161: step 50180, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 70h:54m:27s remains)
INFO - root - 2017-12-05 22:46:36.477532: step 50190, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 74h:34m:52s remains)
INFO - root - 2017-12-05 22:46:45.948555: step 50200, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 72h:05m:46s remains)
2017-12-05 22:46:46.746108: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3216105 -4.3270578 -4.3307524 -4.3290639 -4.3164 -4.2965813 -4.2799721 -4.2711987 -4.2741852 -4.2796926 -4.284832 -4.2899165 -4.2976003 -4.3083353 -4.3221569][-4.3302045 -4.3430729 -4.3496957 -4.3441339 -4.321063 -4.2884774 -4.2579403 -4.2418027 -4.2467017 -4.2555804 -4.260107 -4.2624087 -4.2722292 -4.2878366 -4.308454][-4.338697 -4.3619 -4.3704338 -4.3577943 -4.3209996 -4.272675 -4.2250867 -4.2016635 -4.2102385 -4.2243471 -4.2272553 -4.2272058 -4.2415323 -4.2642665 -4.2934818][-4.3419371 -4.3740897 -4.3836403 -4.3660264 -4.3180928 -4.2518306 -4.1857166 -4.1561136 -4.1697588 -4.190999 -4.1930385 -4.194777 -4.2168632 -4.2513947 -4.2876759][-4.331655 -4.3647027 -4.3677764 -4.3404074 -4.278234 -4.1915293 -4.1053495 -4.0754848 -4.1059537 -4.1428728 -4.1566958 -4.1724396 -4.2093353 -4.2548985 -4.2939711][-4.3126822 -4.3396168 -4.329782 -4.2885923 -4.2088795 -4.0982494 -3.9908934 -3.9644303 -4.0269866 -4.0913815 -4.129456 -4.1668787 -4.2166362 -4.2669315 -4.3055067][-4.2911777 -4.3118076 -4.2907429 -4.2368374 -4.1383371 -3.9981198 -3.8624909 -3.8413599 -3.9437711 -4.0401754 -4.1024561 -4.1577325 -4.2187428 -4.2744522 -4.3129945][-4.2772365 -4.2930026 -4.2646861 -4.2035975 -4.089983 -3.9268417 -3.7716751 -3.7707477 -3.9124203 -4.0314016 -4.1042109 -4.1622009 -4.2241039 -4.2795491 -4.3148479][-4.2803359 -4.2910881 -4.25923 -4.2005119 -4.0964084 -3.9454155 -3.8137474 -3.8401666 -3.9776087 -4.0870767 -4.1491556 -4.19643 -4.2462559 -4.2901411 -4.3173537][-4.2909436 -4.2930646 -4.2566223 -4.2013164 -4.1146269 -3.9977696 -3.9159281 -3.9621186 -4.0735192 -4.1556826 -4.1977277 -4.2304449 -4.2685142 -4.3022404 -4.3226786][-4.2989879 -4.2925167 -4.25663 -4.2093673 -4.1398697 -4.0603085 -4.0236998 -4.07729 -4.1601176 -4.2152362 -4.2401848 -4.2613454 -4.2910328 -4.3173018 -4.3322744][-4.3081965 -4.3011336 -4.2749872 -4.242146 -4.1959534 -4.1516638 -4.1421561 -4.185091 -4.2362723 -4.2649269 -4.2772393 -4.2911396 -4.3134818 -4.3318162 -4.341249][-4.3138657 -4.31182 -4.296989 -4.2776346 -4.253828 -4.2370753 -4.2393312 -4.2642388 -4.287991 -4.298234 -4.3030162 -4.31363 -4.3303733 -4.3423944 -4.3475962][-4.3190441 -4.3181148 -4.3104587 -4.3036056 -4.2966704 -4.2933221 -4.2955389 -4.3050933 -4.3119125 -4.3120623 -4.3125072 -4.321712 -4.336 -4.3456821 -4.349596][-4.3283587 -4.3276 -4.3227191 -4.3205256 -4.3198056 -4.3200922 -4.32066 -4.32251 -4.322248 -4.3184452 -4.3176384 -4.3249135 -4.3360724 -4.3442073 -4.3485365]]...]
INFO - root - 2017-12-05 22:46:56.034926: step 50210, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 73h:45m:45s remains)
INFO - root - 2017-12-05 22:47:05.352760: step 50220, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 72h:54m:21s remains)
INFO - root - 2017-12-05 22:47:14.632433: step 50230, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.906 sec/batch; 71h:00m:16s remains)
INFO - root - 2017-12-05 22:47:23.899222: step 50240, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 71h:18m:38s remains)
INFO - root - 2017-12-05 22:47:33.357781: step 50250, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 76h:42m:09s remains)
INFO - root - 2017-12-05 22:47:42.826679: step 50260, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 72h:56m:47s remains)
INFO - root - 2017-12-05 22:47:52.091681: step 50270, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 71h:09m:28s remains)
INFO - root - 2017-12-05 22:48:01.317180: step 50280, loss = 2.02, batch loss = 1.96 (8.8 examples/sec; 0.912 sec/batch; 71h:28m:00s remains)
INFO - root - 2017-12-05 22:48:10.857472: step 50290, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 71h:42m:58s remains)
INFO - root - 2017-12-05 22:48:20.017897: step 50300, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 73h:42m:18s remains)
2017-12-05 22:48:20.829913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2462029 -4.254869 -4.2698045 -4.2777066 -4.2729263 -4.2590923 -4.2500558 -4.2444482 -4.2446451 -4.2480726 -4.2498026 -4.2530694 -4.2633729 -4.2707109 -4.2716231][-4.230948 -4.2396636 -4.255208 -4.2671323 -4.2626333 -4.2459464 -4.2341623 -4.2250276 -4.219696 -4.2212338 -4.2260866 -4.2310438 -4.2439427 -4.256043 -4.2586312][-4.2190409 -4.227417 -4.2362251 -4.2485614 -4.2475019 -4.2337093 -4.2207193 -4.2034278 -4.1907496 -4.19383 -4.2091222 -4.2185912 -4.2253714 -4.2339263 -4.2351551][-4.20693 -4.2231183 -4.2321625 -4.2409344 -4.2350435 -4.2183046 -4.1991844 -4.1723676 -4.1594768 -4.1744709 -4.1998153 -4.2159953 -4.2178817 -4.2219148 -4.2211261][-4.1955585 -4.2162962 -4.2250104 -4.2247834 -4.2027974 -4.1737118 -4.1472149 -4.1286635 -4.1351533 -4.1623154 -4.1910949 -4.214252 -4.2238765 -4.227684 -4.2143745][-4.186882 -4.1978827 -4.1986384 -4.1874967 -4.1484523 -4.09802 -4.0555177 -4.0443735 -4.0799327 -4.130372 -4.1684494 -4.1973052 -4.2154922 -4.2214284 -4.1970735][-4.1689854 -4.16049 -4.1473489 -4.1297884 -4.0793514 -4.0034065 -3.9204829 -3.8954597 -3.9726758 -4.0726185 -4.1338782 -4.1631207 -4.1786003 -4.1872511 -4.1606059][-4.145143 -4.1116838 -4.0807638 -4.06357 -4.0194159 -3.9247451 -3.7849283 -3.7203646 -3.8407845 -3.9916127 -4.071166 -4.0877671 -4.08685 -4.1020756 -4.0885253][-4.1361961 -4.0910954 -4.0537477 -4.0386829 -4.0037394 -3.9156709 -3.7747655 -3.6983249 -3.8018882 -3.9397566 -4.0058184 -3.99516 -3.9643002 -3.9822636 -3.9921944][-4.1457853 -4.1205111 -4.0976667 -4.077764 -4.0514464 -3.9952538 -3.9104238 -3.8647861 -3.9070656 -3.969624 -3.9915535 -3.9647748 -3.9245303 -3.9321063 -3.9405937][-4.1585312 -4.1598263 -4.1527958 -4.1387281 -4.1238451 -4.09929 -4.0561485 -4.0309391 -4.0339694 -4.0423574 -4.0377274 -4.013432 -3.9820201 -3.972636 -3.963587][-4.1768756 -4.1946969 -4.2029572 -4.2000818 -4.1982584 -4.1901417 -4.162816 -4.1441851 -4.1379914 -4.13216 -4.1228852 -4.1022539 -4.0758328 -4.0621634 -4.0482578][-4.2087526 -4.2330046 -4.251821 -4.2583346 -4.2605515 -4.2575455 -4.2422934 -4.2318249 -4.2282805 -4.2266016 -4.2189207 -4.2006731 -4.1786337 -4.1659546 -4.1586657][-4.2227411 -4.252636 -4.2812195 -4.2975521 -4.3022304 -4.3052607 -4.2991967 -4.2967582 -4.2970395 -4.2960095 -4.2907925 -4.2798672 -4.269526 -4.2623425 -4.2570934][-4.2126765 -4.2465768 -4.2796025 -4.3016648 -4.3148112 -4.3244314 -4.3228517 -4.3217835 -4.3242283 -4.3243952 -4.3204346 -4.3162484 -4.3161464 -4.3166184 -4.3165665]]...]
INFO - root - 2017-12-05 22:48:29.875020: step 50310, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 73h:30m:46s remains)
INFO - root - 2017-12-05 22:48:39.325328: step 50320, loss = 2.05, batch loss = 1.99 (7.6 examples/sec; 1.048 sec/batch; 82h:07m:55s remains)
INFO - root - 2017-12-05 22:48:48.732918: step 50330, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.925 sec/batch; 72h:29m:20s remains)
INFO - root - 2017-12-05 22:48:57.983743: step 50340, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 74h:02m:29s remains)
INFO - root - 2017-12-05 22:49:07.114191: step 50350, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 69h:38m:04s remains)
INFO - root - 2017-12-05 22:49:16.458791: step 50360, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 76h:05m:19s remains)
INFO - root - 2017-12-05 22:49:25.952687: step 50370, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 76h:53m:45s remains)
INFO - root - 2017-12-05 22:49:35.344282: step 50380, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 75h:39m:53s remains)
INFO - root - 2017-12-05 22:49:44.653733: step 50390, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 76h:03m:55s remains)
INFO - root - 2017-12-05 22:49:53.986968: step 50400, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 76h:39m:57s remains)
2017-12-05 22:49:54.681066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1305671 -4.1256428 -4.1164 -4.130126 -4.149497 -4.174891 -4.2024126 -4.220614 -4.2243896 -4.2128239 -4.1901388 -4.1644936 -4.144279 -4.1282115 -4.1270618][-4.1473989 -4.1547084 -4.1525078 -4.17072 -4.18883 -4.208086 -4.2232871 -4.2251968 -4.2075562 -4.1728034 -4.1295986 -4.089561 -4.064168 -4.0521469 -4.05695][-4.1775255 -4.1878967 -4.1922026 -4.2145071 -4.2320991 -4.241065 -4.2377162 -4.2199984 -4.1854858 -4.133997 -4.0753636 -4.0285969 -4.0115709 -4.0151529 -4.02843][-4.1991553 -4.21089 -4.2201362 -4.2436872 -4.2555246 -4.25147 -4.2302041 -4.1988831 -4.1558189 -4.0984707 -4.0377088 -3.9991982 -3.9996948 -4.0210867 -4.0462651][-4.2076135 -4.2149816 -4.2180853 -4.2279248 -4.2308092 -4.2181196 -4.1845193 -4.1448984 -4.1065903 -4.061892 -4.0233622 -4.0032706 -4.0179567 -4.0516453 -4.084938][-4.2259917 -4.2168851 -4.20045 -4.1870284 -4.179419 -4.1583271 -4.1198721 -4.0841918 -4.060287 -4.0381279 -4.0309043 -4.0283856 -4.0465059 -4.0859003 -4.1230483][-4.2452912 -4.2174821 -4.1804509 -4.1473751 -4.13232 -4.1164603 -4.0893588 -4.0669003 -4.0585055 -4.058949 -4.0718293 -4.0731192 -4.0879431 -4.1285043 -4.1682048][-4.2508416 -4.2135191 -4.1694717 -4.1324439 -4.123929 -4.1235323 -4.1158462 -4.106029 -4.101737 -4.10777 -4.1226254 -4.1229887 -4.1383228 -4.1746688 -4.2115788][-4.226975 -4.1981535 -4.1705508 -4.1491222 -4.1542759 -4.170073 -4.1768737 -4.1675639 -4.1556363 -4.1511688 -4.1594214 -4.165957 -4.18547 -4.2150393 -4.2446685][-4.2010527 -4.1891265 -4.1785421 -4.1717529 -4.1851912 -4.2069163 -4.2174435 -4.1999722 -4.1736236 -4.1588874 -4.1663675 -4.1806273 -4.2087398 -4.235393 -4.2590475][-4.1992855 -4.1985016 -4.1982918 -4.1979551 -4.2084537 -4.2236071 -4.2245026 -4.19662 -4.158699 -4.1396847 -4.1491961 -4.1683397 -4.1974573 -4.2230954 -4.2450771][-4.2414479 -4.2435865 -4.246685 -4.2478065 -4.2521982 -4.2561631 -4.2420282 -4.1994357 -4.154788 -4.1408911 -4.15629 -4.1753144 -4.19613 -4.2205362 -4.2424545][-4.2816114 -4.284482 -4.2868271 -4.2852988 -4.2826352 -4.2769194 -4.2578735 -4.2203159 -4.1817803 -4.1729426 -4.1872239 -4.1982183 -4.2081819 -4.2274709 -4.2474561][-4.2955956 -4.2970214 -4.2972765 -4.2957568 -4.2933478 -4.2876468 -4.2743759 -4.2496996 -4.2237654 -4.2165313 -4.2244453 -4.2243452 -4.2229381 -4.2365828 -4.2512903][-4.2825956 -4.2860556 -4.2889538 -4.290833 -4.2935719 -4.2934089 -4.2844172 -4.2657027 -4.2492614 -4.2443786 -4.2476549 -4.241991 -4.2374606 -4.2458358 -4.2541542]]...]
INFO - root - 2017-12-05 22:50:03.942224: step 50410, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 73h:51m:03s remains)
INFO - root - 2017-12-05 22:50:13.468781: step 50420, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 74h:36m:07s remains)
INFO - root - 2017-12-05 22:50:22.869713: step 50430, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 74h:42m:03s remains)
INFO - root - 2017-12-05 22:50:32.241509: step 50440, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 75h:17m:22s remains)
INFO - root - 2017-12-05 22:50:41.681424: step 50450, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 73h:03m:03s remains)
INFO - root - 2017-12-05 22:50:50.946449: step 50460, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 72h:51m:08s remains)
INFO - root - 2017-12-05 22:51:00.334457: step 50470, loss = 2.03, batch loss = 1.98 (7.8 examples/sec; 1.029 sec/batch; 80h:37m:13s remains)
INFO - root - 2017-12-05 22:51:09.781440: step 50480, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 74h:56m:54s remains)
INFO - root - 2017-12-05 22:51:19.022880: step 50490, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 75h:13m:02s remains)
INFO - root - 2017-12-05 22:51:27.876908: step 50500, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 68h:19m:13s remains)
2017-12-05 22:51:28.720939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3159204 -4.313765 -4.3076072 -4.3003573 -4.2951264 -4.2893257 -4.2787237 -4.2728353 -4.2727847 -4.2755547 -4.2779813 -4.278955 -4.2744017 -4.2706175 -4.2638073][-4.303277 -4.2986407 -4.289465 -4.280129 -4.2740364 -4.2662263 -4.2534766 -4.2468891 -4.2478147 -4.2515826 -4.2551484 -4.2563334 -4.2502971 -4.2407041 -4.2288365][-4.2842956 -4.2740974 -4.2626047 -4.2551274 -4.2461586 -4.235136 -4.2224355 -4.21396 -4.2145581 -4.2204962 -4.2305574 -4.239048 -4.2328348 -4.2148809 -4.2000346][-4.2529821 -4.23443 -4.2195516 -4.2129006 -4.1993012 -4.1851854 -4.1760368 -4.1702466 -4.1765003 -4.1881084 -4.2029696 -4.2197094 -4.2144508 -4.1892862 -4.17165][-4.2088685 -4.1777377 -4.1594849 -4.1515713 -4.1337256 -4.1188869 -4.1172352 -4.1109314 -4.1256738 -4.1526775 -4.17686 -4.2023282 -4.1985731 -4.1739473 -4.1597562][-4.1638632 -4.1158452 -4.0915723 -4.0743437 -4.04722 -4.02864 -4.0384259 -4.0437479 -4.0738378 -4.1167045 -4.1425381 -4.1708055 -4.17498 -4.1599636 -4.1533523][-4.1246319 -4.0650268 -4.0342636 -4.0112195 -3.9716239 -3.9416332 -3.95831 -3.9801803 -4.0235806 -4.0686822 -4.0890827 -4.1156812 -4.1320319 -4.1309767 -4.1343355][-4.1152811 -4.0619493 -4.0341005 -4.0150504 -3.9740644 -3.9357278 -3.9414086 -3.9585328 -3.996511 -4.0336976 -4.04833 -4.0703845 -4.0949292 -4.1070089 -4.1235838][-4.1345062 -4.0954919 -4.0794072 -4.0701418 -4.0367589 -4.0073867 -4.004981 -4.008811 -4.0326805 -4.0595427 -4.0633159 -4.071281 -4.0957036 -4.113111 -4.1326661][-4.1631908 -4.1371064 -4.137157 -4.14045 -4.1145821 -4.0887666 -4.080646 -4.0712752 -4.0868917 -4.1089592 -4.105773 -4.1069064 -4.1242771 -4.1434402 -4.1649742][-4.2034793 -4.1842103 -4.1966963 -4.2119856 -4.2016344 -4.1815472 -4.1663089 -4.1501951 -4.1589303 -4.17778 -4.177793 -4.1791577 -4.1863155 -4.1952009 -4.2128186][-4.2524662 -4.2427406 -4.2603145 -4.2811718 -4.282464 -4.273087 -4.2620707 -4.2505112 -4.2512865 -4.2620287 -4.2666225 -4.2740712 -4.2754683 -4.272512 -4.2810454][-4.2911139 -4.2896519 -4.3048635 -4.3246222 -4.3344536 -4.3362155 -4.3326664 -4.327311 -4.3238416 -4.3289194 -4.3334322 -4.3401766 -4.34281 -4.3379936 -4.3402824][-4.3099194 -4.3077574 -4.3150253 -4.3288884 -4.3403749 -4.3474321 -4.3489647 -4.3473954 -4.3463359 -4.3494744 -4.351954 -4.3561354 -4.359416 -4.3571625 -4.35737][-4.3144941 -4.3088317 -4.3078666 -4.3137116 -4.3211145 -4.3275905 -4.3302794 -4.3304448 -4.3307624 -4.3325992 -4.3328071 -4.33441 -4.3363075 -4.3362026 -4.3372426]]...]
INFO - root - 2017-12-05 22:51:38.269152: step 50510, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 77h:23m:02s remains)
INFO - root - 2017-12-05 22:51:47.618875: step 50520, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 75h:13m:00s remains)
INFO - root - 2017-12-05 22:51:56.996876: step 50530, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 74h:30m:55s remains)
INFO - root - 2017-12-05 22:52:06.480197: step 50540, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 74h:31m:17s remains)
INFO - root - 2017-12-05 22:52:15.670538: step 50550, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 74h:10m:19s remains)
INFO - root - 2017-12-05 22:52:24.961865: step 50560, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 71h:10m:20s remains)
INFO - root - 2017-12-05 22:52:34.334137: step 50570, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 74h:52m:17s remains)
INFO - root - 2017-12-05 22:52:43.707661: step 50580, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 67h:37m:42s remains)
INFO - root - 2017-12-05 22:52:53.059866: step 50590, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 74h:29m:07s remains)
INFO - root - 2017-12-05 22:53:02.394483: step 50600, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 75h:14m:24s remains)
2017-12-05 22:53:03.284564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2273011 -4.2232008 -4.2172208 -4.2102461 -4.2019238 -4.1940818 -4.192 -4.1941175 -4.1968961 -4.1943355 -4.1814651 -4.1614842 -4.1403632 -4.1233315 -4.1075625][-4.24435 -4.2364473 -4.2262726 -4.2162008 -4.2049174 -4.1918421 -4.1825604 -4.1750641 -4.1668844 -4.1550775 -4.137023 -4.1169171 -4.1007919 -4.0905724 -4.0832629][-4.2445917 -4.2276607 -4.2100677 -4.19548 -4.1818309 -4.163703 -4.1448488 -4.1248469 -4.104928 -4.0903 -4.0796947 -4.0719948 -4.0695944 -4.07438 -4.0811858][-4.2296739 -4.198225 -4.1684065 -4.1465154 -4.1311212 -4.1135445 -4.0889859 -4.0609279 -4.0383062 -4.0336151 -4.0440784 -4.0568891 -4.0673337 -4.0789938 -4.0868998][-4.1931925 -4.1424384 -4.0939155 -4.0629306 -4.0539594 -4.053596 -4.0400343 -4.0196781 -4.0062342 -4.0184407 -4.0478635 -4.0722327 -4.0854149 -4.0877347 -4.0801439][-4.1386919 -4.0711861 -4.0014772 -3.9635749 -3.9763355 -4.0137291 -4.028501 -4.023499 -4.0196266 -4.0399394 -4.07557 -4.0993276 -4.1058612 -4.0907989 -4.0627174][-4.094625 -4.0238934 -3.9451847 -3.9086661 -3.9416292 -4.0127378 -4.0555992 -4.0639453 -4.0627441 -4.0814662 -4.1099133 -4.1281824 -4.1271935 -4.0971189 -4.0545306][-4.0780616 -4.0281248 -3.9673567 -3.941134 -3.9726212 -4.0444636 -4.0962152 -4.11319 -4.1124134 -4.1237235 -4.142385 -4.1538382 -4.1508503 -4.12135 -4.0789728][-4.0844193 -4.0667915 -4.0409985 -4.02821 -4.0470209 -4.0940228 -4.1359482 -4.1557512 -4.1559577 -4.1593056 -4.1678319 -4.1698494 -4.1637216 -4.1447473 -4.1158214][-4.0958276 -4.1047912 -4.1106081 -4.1128373 -4.1228795 -4.1457262 -4.17064 -4.1865077 -4.1896229 -4.1887641 -4.1864023 -4.1771712 -4.1649971 -4.1540194 -4.1408257][-4.1069951 -4.1328058 -4.1579928 -4.1724672 -4.1776314 -4.1826119 -4.1928453 -4.2043543 -4.2094016 -4.2031631 -4.1915045 -4.1733294 -4.158802 -4.1550961 -4.1564493][-4.1180735 -4.1532168 -4.1873226 -4.2072086 -4.2079072 -4.2025466 -4.207356 -4.2147641 -4.2160635 -4.2013917 -4.1782613 -4.1556339 -4.1463437 -4.1538854 -4.1681514][-4.1382241 -4.1755357 -4.2083988 -4.2261252 -4.2255554 -4.2205276 -4.2231517 -4.2248116 -4.214386 -4.1853585 -4.1522441 -4.1299634 -4.1306391 -4.1510611 -4.1805344][-4.1627517 -4.1926355 -4.2163138 -4.2310739 -4.2357697 -4.2381778 -4.2413192 -4.2330623 -4.2046156 -4.1626873 -4.1262231 -4.1117859 -4.1257672 -4.158988 -4.2011204][-4.1832581 -4.2046142 -4.2213349 -4.2361336 -4.2498827 -4.2586646 -4.2561722 -4.2310205 -4.1861329 -4.1387281 -4.1090326 -4.1120667 -4.1419339 -4.18443 -4.2309675]]...]
INFO - root - 2017-12-05 22:53:12.786360: step 50610, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 73h:54m:17s remains)
INFO - root - 2017-12-05 22:53:21.858778: step 50620, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 75h:21m:25s remains)
INFO - root - 2017-12-05 22:53:31.031803: step 50630, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.973 sec/batch; 76h:12m:31s remains)
INFO - root - 2017-12-05 22:53:40.250163: step 50640, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 73h:32m:18s remains)
INFO - root - 2017-12-05 22:53:49.404918: step 50650, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 71h:45m:22s remains)
INFO - root - 2017-12-05 22:53:58.857089: step 50660, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 75h:42m:58s remains)
INFO - root - 2017-12-05 22:54:08.251977: step 50670, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.003 sec/batch; 78h:31m:08s remains)
INFO - root - 2017-12-05 22:54:17.442500: step 50680, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 73h:30m:55s remains)
INFO - root - 2017-12-05 22:54:26.712395: step 50690, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 74h:06m:06s remains)
INFO - root - 2017-12-05 22:54:35.943607: step 50700, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 69h:42m:57s remains)
2017-12-05 22:54:36.681764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2039366 -4.1704059 -4.1308889 -4.0985408 -4.0886669 -4.0900931 -4.1056437 -4.1384921 -4.1677132 -4.185246 -4.185245 -4.1684594 -4.1469684 -4.1226149 -4.107995][-4.1802111 -4.1516867 -4.1235929 -4.0999622 -4.0958772 -4.09847 -4.1120367 -4.1521621 -4.1848965 -4.2018528 -4.1991096 -4.1806278 -4.1557469 -4.1243587 -4.0966945][-4.1584959 -4.1368432 -4.1222057 -4.1036119 -4.0965304 -4.0931349 -4.1026492 -4.1541734 -4.1984138 -4.2170258 -4.2141647 -4.2013583 -4.1843109 -4.1563382 -4.127316][-4.1380014 -4.1256557 -4.1234746 -4.1104565 -4.0946217 -4.0819449 -4.0871043 -4.1448307 -4.2009697 -4.2233067 -4.223042 -4.2200141 -4.2169509 -4.1993961 -4.1722093][-4.11769 -4.1163621 -4.1249404 -4.119298 -4.0990739 -4.0816927 -4.0808091 -4.12649 -4.1838789 -4.2102761 -4.216723 -4.2241864 -4.2322636 -4.2218418 -4.1960764][-4.1081624 -4.1186824 -4.133636 -4.1332259 -4.1148973 -4.09549 -4.0805612 -4.0915256 -4.1297097 -4.1611862 -4.1836238 -4.2084894 -4.2253075 -4.2192 -4.19449][-4.1178184 -4.1431065 -4.1633768 -4.1641254 -4.1455922 -4.1197395 -4.0815396 -4.0402403 -4.0372825 -4.0705838 -4.1233673 -4.1811819 -4.215261 -4.2166352 -4.1931577][-4.1377382 -4.1783423 -4.2058759 -4.2084932 -4.1874685 -4.1453185 -4.075367 -3.9757977 -3.9168692 -3.9534631 -4.0494514 -4.1491055 -4.2055798 -4.2148218 -4.1960363][-4.1519761 -4.2028375 -4.2372642 -4.2450752 -4.224997 -4.1698709 -4.0773425 -3.9451172 -3.8462481 -3.8764429 -3.997412 -4.1165996 -4.1845984 -4.2054152 -4.1963906][-4.1539688 -4.2126083 -4.2544427 -4.2667742 -4.2495079 -4.19593 -4.1071672 -3.9902146 -3.8993649 -3.9107566 -4.0037327 -4.0960016 -4.1536517 -4.1816535 -4.1843252][-4.153739 -4.2186632 -4.2675638 -4.2836976 -4.2676411 -4.2254362 -4.1579738 -4.0800409 -4.021996 -4.0194836 -4.0603747 -4.1031837 -4.1387715 -4.164279 -4.17014][-4.1523132 -4.215549 -4.2667818 -4.2871633 -4.2768631 -4.2435308 -4.1966872 -4.1527462 -4.1235981 -4.1114993 -4.1110663 -4.1190791 -4.1353383 -4.146893 -4.1448827][-4.1387272 -4.1894455 -4.2386446 -4.2626071 -4.2575884 -4.2339315 -4.2061057 -4.1826949 -4.1723056 -4.159977 -4.1419282 -4.1375227 -4.1398139 -4.1292911 -4.1087284][-4.114994 -4.1492949 -4.1869006 -4.2064552 -4.2049885 -4.19557 -4.188283 -4.1844172 -4.1927071 -4.1906734 -4.17639 -4.1683559 -4.1566324 -4.1232667 -4.0806813][-4.0965719 -4.1123233 -4.1315479 -4.1416445 -4.141716 -4.1420674 -4.1464348 -4.1548624 -4.1776218 -4.1947079 -4.1977429 -4.1957307 -4.1803074 -4.1419058 -4.0919433]]...]
INFO - root - 2017-12-05 22:54:46.129696: step 50710, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 77h:14m:38s remains)
INFO - root - 2017-12-05 22:54:55.734597: step 50720, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 76h:15m:46s remains)
INFO - root - 2017-12-05 22:55:04.825857: step 50730, loss = 2.06, batch loss = 2.01 (7.9 examples/sec; 1.008 sec/batch; 78h:51m:48s remains)
INFO - root - 2017-12-05 22:55:14.311202: step 50740, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.997 sec/batch; 78h:01m:45s remains)
INFO - root - 2017-12-05 22:55:23.619776: step 50750, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 67h:20m:55s remains)
INFO - root - 2017-12-05 22:55:32.989385: step 50760, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 76h:18m:36s remains)
INFO - root - 2017-12-05 22:55:42.268859: step 50770, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 72h:58m:47s remains)
INFO - root - 2017-12-05 22:55:51.711957: step 50780, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 72h:36m:36s remains)
INFO - root - 2017-12-05 22:56:01.332241: step 50790, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 77h:10m:04s remains)
INFO - root - 2017-12-05 22:56:10.531898: step 50800, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 75h:36m:51s remains)
2017-12-05 22:56:11.358706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.339416 -4.3296423 -4.3198009 -4.31379 -4.3076205 -4.2963028 -4.2802982 -4.2645583 -4.2521868 -4.2535605 -4.2714729 -4.2900052 -4.3004947 -4.3018985 -4.299818][-4.3345294 -4.3191929 -4.3045816 -4.294754 -4.28229 -4.2631931 -4.2370238 -4.2118039 -4.1931787 -4.1966004 -4.2243614 -4.2561493 -4.2762108 -4.2830706 -4.2800951][-4.3285785 -4.3074245 -4.2874579 -4.2734094 -4.2549853 -4.226995 -4.191401 -4.1531777 -4.1232533 -4.1273274 -4.1644511 -4.2081327 -4.23735 -4.2491012 -4.247685][-4.3253365 -4.2996316 -4.2758675 -4.2579436 -4.2342138 -4.1974797 -4.1509981 -4.0953069 -4.0497627 -4.0537777 -4.1026511 -4.1587572 -4.1943979 -4.2078638 -4.2064195][-4.3228207 -4.2956424 -4.2709136 -4.2490063 -4.2160616 -4.1663709 -4.1034603 -4.0284095 -3.9650702 -3.9695358 -4.0385714 -4.1104655 -4.1526604 -4.1705103 -4.1691194][-4.3229532 -4.2987194 -4.2775187 -4.2534809 -4.2094502 -4.1471858 -4.0686846 -3.976608 -3.8963103 -3.8956504 -3.9818907 -4.0684361 -4.1183357 -4.1434608 -4.1410093][-4.32356 -4.3045673 -4.2881312 -4.2636619 -4.21226 -4.1445794 -4.0598626 -3.9603767 -3.8720384 -3.8641081 -3.9513049 -4.0443921 -4.1001177 -4.13102 -4.1272492][-4.3240304 -4.31001 -4.2983289 -4.2743878 -4.2222304 -4.15771 -4.0807166 -3.9915326 -3.9103756 -3.8975832 -3.9729965 -4.0567007 -4.1085582 -4.1394496 -4.1349821][-4.3236852 -4.3090668 -4.2965822 -4.2729912 -4.224575 -4.1695342 -4.1080384 -4.0403204 -3.9802451 -3.9705391 -4.0281425 -4.087532 -4.1233263 -4.1484337 -4.1475472][-4.32379 -4.3056607 -4.2917085 -4.2703547 -4.2317543 -4.1902356 -4.1485438 -4.1073241 -4.0710721 -4.0648122 -4.1015606 -4.1332927 -4.1455183 -4.157249 -4.1631732][-4.3229671 -4.3015337 -4.2875781 -4.270874 -4.2426982 -4.2115679 -4.1845269 -4.1610947 -4.1430688 -4.1389732 -4.1612415 -4.1758647 -4.1725092 -4.173892 -4.1833806][-4.3229179 -4.2990136 -4.2827878 -4.2723064 -4.2556076 -4.2375679 -4.2254939 -4.2168088 -4.2087059 -4.206162 -4.2174778 -4.2196226 -4.2092447 -4.2051425 -4.2126312][-4.3251972 -4.3026452 -4.2867618 -4.2842731 -4.2806878 -4.2765641 -4.2767029 -4.2773013 -4.2759123 -4.2738347 -4.2749996 -4.2683215 -4.2577744 -4.25137 -4.2556891][-4.3318238 -4.3131566 -4.2995954 -4.3004785 -4.3025928 -4.3047657 -4.3107347 -4.3145576 -4.3148513 -4.3129225 -4.3094039 -4.30267 -4.2962761 -4.2919903 -4.2942777][-4.3381829 -4.3251719 -4.3148069 -4.3163424 -4.3201 -4.3237696 -4.3286567 -4.330061 -4.3281317 -4.32655 -4.3233619 -4.3172073 -4.313798 -4.3133621 -4.3158469]]...]
INFO - root - 2017-12-05 22:56:20.670224: step 50810, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 72h:41m:59s remains)
INFO - root - 2017-12-05 22:56:30.179646: step 50820, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 77h:48m:40s remains)
INFO - root - 2017-12-05 22:56:39.628993: step 50830, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 78h:03m:37s remains)
INFO - root - 2017-12-05 22:56:48.771136: step 50840, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.952 sec/batch; 74h:30m:01s remains)
INFO - root - 2017-12-05 22:56:58.326158: step 50850, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 75h:05m:53s remains)
INFO - root - 2017-12-05 22:57:07.723461: step 50860, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 74h:24m:03s remains)
INFO - root - 2017-12-05 22:57:17.033247: step 50870, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 70h:35m:44s remains)
INFO - root - 2017-12-05 22:57:26.237436: step 50880, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 71h:41m:50s remains)
INFO - root - 2017-12-05 22:57:35.646473: step 50890, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 73h:14m:05s remains)
INFO - root - 2017-12-05 22:57:45.030606: step 50900, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 75h:06m:21s remains)
2017-12-05 22:57:45.828008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2907906 -4.2633924 -4.241312 -4.2285237 -4.2253213 -4.2210517 -4.220747 -4.231225 -4.2410975 -4.23491 -4.2162786 -4.1981254 -4.1839828 -4.1661196 -4.1475654][-4.2756839 -4.2415175 -4.2139335 -4.19827 -4.1953793 -4.1957812 -4.2054873 -4.2246342 -4.2355075 -4.2231569 -4.198555 -4.1773171 -4.1673789 -4.1608119 -4.1488266][-4.2668777 -4.2271762 -4.1919231 -4.1734 -4.1696825 -4.172298 -4.1877141 -4.213099 -4.2256942 -4.212059 -4.1893177 -4.1658368 -4.1544533 -4.1541786 -4.1490641][-4.2596731 -4.2167511 -4.1800694 -4.1649685 -4.1646409 -4.1670589 -4.1803231 -4.2013431 -4.2092752 -4.198195 -4.182466 -4.1611357 -4.1454859 -4.1489735 -4.1564035][-4.2587862 -4.2165337 -4.1830592 -4.1724043 -4.171123 -4.1636319 -4.163455 -4.174397 -4.1827755 -4.1816373 -4.1798644 -4.1646938 -4.1465163 -4.1492605 -4.165184][-4.26423 -4.2253442 -4.19668 -4.1891208 -4.1808057 -4.1527505 -4.1282392 -4.1215115 -4.1313038 -4.1480489 -4.1618524 -4.1561947 -4.1414633 -4.1425662 -4.1647525][-4.2594228 -4.2198582 -4.1898518 -4.1778445 -4.1586566 -4.1102543 -4.056819 -4.0290785 -4.0421715 -4.0844722 -4.1191444 -4.1269436 -4.1255393 -4.1340742 -4.1648211][-4.2344213 -4.1861539 -4.1476245 -4.1258659 -4.0981536 -4.0357747 -3.955899 -3.9091463 -3.9307883 -4.0043688 -4.0649 -4.0905313 -4.1106968 -4.1369123 -4.1742053][-4.213438 -4.1563187 -4.1122222 -4.0876455 -4.0631037 -4.0080914 -3.9257815 -3.8754835 -3.902739 -3.9900713 -4.0598078 -4.0927091 -4.1235018 -4.1578636 -4.1885543][-4.2156215 -4.1581054 -4.11616 -4.1006031 -4.09754 -4.0765862 -4.0192871 -3.9773312 -3.994092 -4.0567865 -4.1083612 -4.1332731 -4.1608195 -4.1894031 -4.2029023][-4.224793 -4.1699781 -4.1303048 -4.125011 -4.1422276 -4.151289 -4.1188931 -4.0870495 -4.0924454 -4.1274748 -4.1637349 -4.185658 -4.2069979 -4.2221637 -4.21808][-4.2240515 -4.1706252 -4.1305113 -4.1291265 -4.1597853 -4.1896639 -4.1756964 -4.1564393 -4.1577692 -4.1775446 -4.2064505 -4.2294254 -4.2455759 -4.2466598 -4.229857][-4.2210412 -4.1688318 -4.125906 -4.11956 -4.1536937 -4.1927328 -4.19549 -4.1916084 -4.1975517 -4.2128243 -4.2354517 -4.2552276 -4.2606769 -4.2482247 -4.2279038][-4.2313228 -4.1828933 -4.1389809 -4.1249251 -4.152164 -4.1910214 -4.2075353 -4.2174091 -4.2296028 -4.2413383 -4.2561536 -4.2648172 -4.2588449 -4.2407823 -4.22294][-4.249145 -4.2092209 -4.1656609 -4.1436 -4.1589355 -4.1946955 -4.2194448 -4.2366896 -4.2493625 -4.2581544 -4.2649431 -4.2622137 -4.2509513 -4.2354603 -4.2222643]]...]
INFO - root - 2017-12-05 22:57:55.069958: step 50910, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 71h:56m:22s remains)
INFO - root - 2017-12-05 22:58:04.390050: step 50920, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 72h:37m:13s remains)
INFO - root - 2017-12-05 22:58:13.852226: step 50930, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 73h:49m:08s remains)
INFO - root - 2017-12-05 22:58:23.218266: step 50940, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 72h:56m:39s remains)
INFO - root - 2017-12-05 22:58:32.654296: step 50950, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 71h:29m:11s remains)
INFO - root - 2017-12-05 22:58:41.981223: step 50960, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.912 sec/batch; 71h:20m:56s remains)
INFO - root - 2017-12-05 22:58:51.343915: step 50970, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 76h:08m:06s remains)
INFO - root - 2017-12-05 22:59:00.864583: step 50980, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 69h:09m:01s remains)
INFO - root - 2017-12-05 22:59:10.165162: step 50990, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 72h:16m:54s remains)
INFO - root - 2017-12-05 22:59:19.557367: step 51000, loss = 2.03, batch loss = 1.98 (8.2 examples/sec; 0.972 sec/batch; 75h:59m:59s remains)
2017-12-05 22:59:20.234765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3152523 -4.3139496 -4.3151388 -4.3186378 -4.3169069 -4.3083348 -4.29919 -4.2952976 -4.2964859 -4.2995009 -4.3055129 -4.3139472 -4.3241706 -4.3335376 -4.3413339][-4.2975807 -4.2928286 -4.2938242 -4.298625 -4.29351 -4.2790365 -4.2658467 -4.2619681 -4.2645459 -4.2705255 -4.2809758 -4.29392 -4.3072762 -4.3188305 -4.3298569][-4.289228 -4.2806182 -4.2806983 -4.2823429 -4.2693181 -4.2465148 -4.225719 -4.2208896 -4.2262397 -4.2374206 -4.2565866 -4.2773051 -4.2951813 -4.3088017 -4.3220243][-4.2861862 -4.2771125 -4.2779536 -4.2747254 -4.2503057 -4.2135921 -4.1827745 -4.1803818 -4.19092 -4.2088943 -4.2330694 -4.2562928 -4.2782931 -4.2974334 -4.3139849][-4.2774191 -4.2672315 -4.2642879 -4.2545285 -4.2181549 -4.1673207 -4.1252604 -4.1270061 -4.1483068 -4.1765332 -4.2046485 -4.2324495 -4.2603612 -4.2875314 -4.3070545][-4.2397356 -4.2209191 -4.2123303 -4.2015743 -4.1601934 -4.0930681 -4.0316491 -4.0375314 -4.07821 -4.1232667 -4.1598225 -4.1976337 -4.2369423 -4.2754622 -4.3003588][-4.1843929 -4.1586308 -4.1444058 -4.1340055 -4.08628 -3.9882436 -3.8881967 -3.9050097 -3.9810886 -4.0553427 -4.1117578 -4.1637793 -4.216176 -4.2647676 -4.2956948][-4.1415887 -4.1146336 -4.0996776 -4.0909653 -4.0468378 -3.9392536 -3.8113744 -3.8358321 -3.9404256 -4.0349746 -4.1060495 -4.1640534 -4.2181888 -4.2668242 -4.2971473][-4.1248951 -4.1019711 -4.0926485 -4.0937653 -4.0756488 -4.0124955 -3.9233088 -3.9341626 -4.010572 -4.0861254 -4.1425858 -4.1894412 -4.2330589 -4.2738175 -4.3013496][-4.11733 -4.10193 -4.1024547 -4.1112857 -4.1126571 -4.0873384 -4.0436926 -4.0525746 -4.1025543 -4.1537943 -4.1903992 -4.2193813 -4.2500544 -4.2835941 -4.3098679][-4.1376061 -4.1206484 -4.1216826 -4.1273651 -4.1314154 -4.1194677 -4.0973392 -4.111433 -4.1527066 -4.1935811 -4.2230406 -4.2444358 -4.269949 -4.2987294 -4.3229055][-4.1903362 -4.16876 -4.1629863 -4.1652746 -4.1704469 -4.1674957 -4.1584463 -4.1701188 -4.1966829 -4.2229848 -4.2457371 -4.2663779 -4.292244 -4.3177018 -4.3372669][-4.2525773 -4.2333889 -4.2213368 -4.2185926 -4.2245064 -4.2286978 -4.2282405 -4.2362504 -4.2481346 -4.2598906 -4.2734618 -4.2926922 -4.316287 -4.3354039 -4.3486052][-4.2943182 -4.2810612 -4.2681847 -4.2618437 -4.2678337 -4.2754755 -4.2805834 -4.2871485 -4.2955279 -4.3025308 -4.309731 -4.3237462 -4.340157 -4.351685 -4.3576455][-4.3216839 -4.3119407 -4.3017559 -4.293663 -4.2940164 -4.2982974 -4.3036828 -4.3119917 -4.3212404 -4.3287787 -4.3357506 -4.3454442 -4.3556652 -4.3616486 -4.36336]]...]
INFO - root - 2017-12-05 22:59:29.746589: step 51010, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 73h:55m:08s remains)
INFO - root - 2017-12-05 22:59:38.987625: step 51020, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 74h:09m:50s remains)
INFO - root - 2017-12-05 22:59:48.677080: step 51030, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 77h:02m:39s remains)
INFO - root - 2017-12-05 22:59:58.098340: step 51040, loss = 2.03, batch loss = 1.97 (8.1 examples/sec; 0.992 sec/batch; 77h:31m:09s remains)
INFO - root - 2017-12-05 23:00:07.387842: step 51050, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 74h:25m:38s remains)
INFO - root - 2017-12-05 23:00:16.940824: step 51060, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 72h:57m:53s remains)
INFO - root - 2017-12-05 23:00:26.310482: step 51070, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.943 sec/batch; 73h:41m:21s remains)
INFO - root - 2017-12-05 23:00:35.566712: step 51080, loss = 2.04, batch loss = 1.98 (8.0 examples/sec; 1.002 sec/batch; 78h:20m:19s remains)
INFO - root - 2017-12-05 23:00:44.612965: step 51090, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 76h:31m:29s remains)
INFO - root - 2017-12-05 23:00:53.930902: step 51100, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 73h:29m:09s remains)
2017-12-05 23:00:54.758460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2755752 -4.2992449 -4.2997108 -4.2711167 -4.2223363 -4.1623468 -4.1160383 -4.1003504 -4.1056671 -4.1176724 -4.128706 -4.1530252 -4.189343 -4.2216415 -4.2460852][-4.2861495 -4.2952127 -4.2897696 -4.2656488 -4.2267251 -4.1792521 -4.1498008 -4.156662 -4.1787424 -4.1999907 -4.216476 -4.235961 -4.2606945 -4.2812366 -4.2941732][-4.2871246 -4.282114 -4.2718272 -4.2538028 -4.225008 -4.1864481 -4.1639929 -4.1734886 -4.1912041 -4.2092052 -4.2284756 -4.2523613 -4.2817445 -4.3084416 -4.3241749][-4.2791705 -4.2600741 -4.2415476 -4.2202911 -4.193233 -4.1548996 -4.1265459 -4.12573 -4.1370139 -4.1591229 -4.1904655 -4.2253017 -4.2665968 -4.3061876 -4.3290591][-4.2646775 -4.2364445 -4.2084036 -4.1773362 -4.1399045 -4.088892 -4.0417266 -4.0268412 -4.0447268 -4.0893068 -4.141839 -4.1909976 -4.2430363 -4.2917042 -4.3207154][-4.2495341 -4.217989 -4.1788054 -4.1307521 -4.07273 -3.9991047 -3.930923 -3.9079962 -3.9522502 -4.0340605 -4.1131077 -4.1765361 -4.2345495 -4.2833877 -4.3118043][-4.2339783 -4.1975737 -4.1458793 -4.0811582 -4.0082111 -3.9299955 -3.8621876 -3.8495629 -3.9243698 -4.0307422 -4.1215067 -4.1899762 -4.2463651 -4.2873292 -4.3094125][-4.2147093 -4.1731014 -4.1147761 -4.0476131 -3.9874671 -3.9392674 -3.906765 -3.9154859 -3.991406 -4.0889211 -4.1662917 -4.2221785 -4.2659292 -4.2945256 -4.3086882][-4.2069674 -4.1621332 -4.1065278 -4.0563178 -4.0350456 -4.0377083 -4.0439377 -4.0607214 -4.1105652 -4.1740193 -4.2207866 -4.2528539 -4.2800789 -4.2996187 -4.3082213][-4.2223496 -4.1818333 -4.1389494 -4.1140437 -4.127099 -4.1589003 -4.1791086 -4.1908278 -4.2129397 -4.2415166 -4.2595496 -4.2687492 -4.2803683 -4.2928877 -4.2995234][-4.2498522 -4.2201476 -4.1945438 -4.1880116 -4.2109017 -4.2419953 -4.25853 -4.2645216 -4.2730227 -4.2798305 -4.2759218 -4.2655306 -4.2606244 -4.265841 -4.2762613][-4.2672915 -4.2489076 -4.2380362 -4.2410631 -4.2619352 -4.2830563 -4.2923603 -4.2938795 -4.2953591 -4.2868872 -4.2634978 -4.2322278 -4.2117519 -4.2163081 -4.241632][-4.2593012 -4.2532034 -4.2571754 -4.2685142 -4.2870817 -4.2998796 -4.3033905 -4.3012109 -4.2917409 -4.2632346 -4.2147 -4.1615877 -4.1334634 -4.1470046 -4.1955438][-4.2395329 -4.2427387 -4.2576 -4.2731171 -4.2872262 -4.2921209 -4.2871246 -4.2764173 -4.252183 -4.2029529 -4.1334529 -4.0693436 -4.0474219 -4.0770116 -4.1455274][-4.2263575 -4.2321734 -4.2472224 -4.2612138 -4.2693796 -4.2642264 -4.2479653 -4.2253141 -4.1915269 -4.1362185 -4.0643845 -4.004128 -3.9903948 -4.0244541 -4.0976691]]...]
INFO - root - 2017-12-05 23:01:03.809556: step 51110, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 66h:14m:21s remains)
INFO - root - 2017-12-05 23:01:13.222641: step 51120, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.935 sec/batch; 73h:05m:11s remains)
INFO - root - 2017-12-05 23:01:22.578398: step 51130, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 71h:17m:58s remains)
INFO - root - 2017-12-05 23:01:31.928183: step 51140, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 71h:04m:21s remains)
INFO - root - 2017-12-05 23:01:41.336181: step 51150, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 71h:35m:16s remains)
INFO - root - 2017-12-05 23:01:50.807445: step 51160, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 73h:53m:33s remains)
INFO - root - 2017-12-05 23:02:00.281514: step 51170, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 72h:00m:40s remains)
INFO - root - 2017-12-05 23:02:09.752012: step 51180, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.940 sec/batch; 73h:28m:32s remains)
INFO - root - 2017-12-05 23:02:18.963272: step 51190, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 72h:50m:37s remains)
INFO - root - 2017-12-05 23:02:28.404515: step 51200, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 75h:32m:17s remains)
2017-12-05 23:02:29.199379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.319828 -4.3165421 -4.3155518 -4.3146472 -4.3130665 -4.3108726 -4.3098731 -4.3111806 -4.3133011 -4.3160629 -4.3191037 -4.3208385 -4.3199763 -4.3176327 -4.3153553][-4.3112946 -4.3078909 -4.3091784 -4.3105164 -4.3097625 -4.3062472 -4.3026352 -4.3016653 -4.3041015 -4.3089886 -4.3140354 -4.3173423 -4.3186054 -4.3193488 -4.319447][-4.2956219 -4.2937036 -4.2983589 -4.30167 -4.3007193 -4.2938914 -4.2846889 -4.2801523 -4.2837586 -4.2917 -4.297462 -4.3007188 -4.3032103 -4.3071947 -4.31077][-4.2887011 -4.2878976 -4.2929211 -4.2937303 -4.2899027 -4.2801342 -4.26582 -4.2585473 -4.2650337 -4.2760658 -4.2824116 -4.28547 -4.2884297 -4.2935944 -4.2974625][-4.280582 -4.2805853 -4.2806849 -4.2742014 -4.2652082 -4.2509489 -4.2313147 -4.22269 -4.2367973 -4.2552066 -4.2659864 -4.271193 -4.2722158 -4.2736921 -4.2719688][-4.2507133 -4.2521448 -4.2474928 -4.2328472 -4.2147865 -4.18993 -4.1589966 -4.1469345 -4.1737289 -4.2089281 -4.2324634 -4.244411 -4.2452316 -4.2413225 -4.2316856][-4.2119942 -4.212 -4.2031507 -4.182179 -4.153789 -4.1137128 -4.0646486 -4.0436096 -4.0851355 -4.1415992 -4.1815767 -4.2011971 -4.203567 -4.1974764 -4.1834369][-4.1812563 -4.1800203 -4.1705866 -4.1467609 -4.1105042 -4.0583858 -3.9946511 -3.9661465 -4.0158272 -4.0869246 -4.139863 -4.1657748 -4.1704278 -4.1650019 -4.1499162][-4.1860833 -4.185091 -4.1784768 -4.1588492 -4.124383 -4.0731611 -4.01526 -3.9921756 -4.0344081 -4.097064 -4.1486392 -4.1747727 -4.1780567 -4.169591 -4.1517324][-4.1994033 -4.2010665 -4.2002735 -4.1884594 -4.1605763 -4.1221237 -4.0846095 -4.0737252 -4.1051416 -4.1478944 -4.1851411 -4.2040839 -4.2025075 -4.1923919 -4.17486][-4.1926541 -4.1949415 -4.2023921 -4.2017126 -4.183897 -4.1571908 -4.1320906 -4.1255059 -4.1497955 -4.1830206 -4.2112026 -4.227879 -4.2290835 -4.22189 -4.2095914][-4.1626892 -4.1619029 -4.1778693 -4.1909604 -4.1863451 -4.166997 -4.1413269 -4.129283 -4.1487584 -4.1858821 -4.2195749 -4.2442279 -4.2536783 -4.2519288 -4.2435865][-4.1328449 -4.1252317 -4.1439476 -4.168438 -4.1794262 -4.1707444 -4.1461043 -4.1246829 -4.136909 -4.1762366 -4.2122979 -4.2419968 -4.258328 -4.2650981 -4.2643647][-4.1101589 -4.09923 -4.1167278 -4.1481948 -4.1745725 -4.1829324 -4.1697731 -4.148253 -4.1496162 -4.1790328 -4.2043653 -4.2292066 -4.2490025 -4.2611814 -4.26631][-4.1174097 -4.1074204 -4.1204495 -4.1478848 -4.1779814 -4.2006726 -4.2050948 -4.192997 -4.1889224 -4.2000384 -4.2071357 -4.219995 -4.2381344 -4.2509837 -4.2586989]]...]
INFO - root - 2017-12-05 23:02:38.661577: step 51210, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 75h:33m:51s remains)
INFO - root - 2017-12-05 23:02:47.900687: step 51220, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 70h:45m:49s remains)
INFO - root - 2017-12-05 23:02:57.374456: step 51230, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.994 sec/batch; 77h:41m:36s remains)
INFO - root - 2017-12-05 23:03:06.963605: step 51240, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 75h:36m:19s remains)
INFO - root - 2017-12-05 23:03:16.167676: step 51250, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 74h:48m:47s remains)
INFO - root - 2017-12-05 23:03:25.461158: step 51260, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 74h:15m:46s remains)
INFO - root - 2017-12-05 23:03:34.959553: step 51270, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.967 sec/batch; 75h:33m:39s remains)
INFO - root - 2017-12-05 23:03:44.005604: step 51280, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 69h:53m:14s remains)
INFO - root - 2017-12-05 23:03:53.341461: step 51290, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 70h:42m:55s remains)
INFO - root - 2017-12-05 23:04:02.760234: step 51300, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 71h:40m:32s remains)
2017-12-05 23:04:03.714960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3476644 -4.3409939 -4.3340912 -4.3293505 -4.3244548 -4.3190312 -4.314435 -4.3108554 -4.3117218 -4.3156214 -4.3237662 -4.3305745 -4.3361254 -4.343709 -4.3517494][-4.3410254 -4.3323307 -4.3214555 -4.3131614 -4.3038363 -4.2932425 -4.2854967 -4.2809248 -4.2814755 -4.2869339 -4.2997208 -4.3106594 -4.3184314 -4.3280821 -4.3396993][-4.3315058 -4.3209305 -4.3022437 -4.2857027 -4.2702856 -4.2542391 -4.2419291 -4.238523 -4.244318 -4.2574511 -4.2774105 -4.2948952 -4.30431 -4.3137641 -4.3275437][-4.3235965 -4.3104267 -4.2856894 -4.259829 -4.2383218 -4.2172008 -4.2013512 -4.1961613 -4.2028947 -4.2208648 -4.2458425 -4.2703133 -4.2855296 -4.2981963 -4.3153968][-4.3108778 -4.3003168 -4.2775764 -4.2464137 -4.2120419 -4.1796365 -4.15277 -4.1414433 -4.1468687 -4.1684022 -4.20011 -4.2337332 -4.2596903 -4.2812619 -4.304409][-4.2809978 -4.2689142 -4.245575 -4.2066054 -4.159534 -4.1074896 -4.0621996 -4.0434685 -4.0601058 -4.0961833 -4.1395144 -4.1883359 -4.2287903 -4.2629609 -4.2948966][-4.2477827 -4.2313118 -4.2018685 -4.1532536 -4.0874715 -4.008677 -3.9385071 -3.9139237 -3.951833 -4.01247 -4.0793557 -4.149632 -4.2044196 -4.2501574 -4.2887144][-4.2258396 -4.2018843 -4.1659675 -4.110074 -4.0281906 -3.9212646 -3.8189261 -3.7846103 -3.8463502 -3.9347334 -4.0302792 -4.1221132 -4.1913457 -4.2441893 -4.2863474][-4.2123861 -4.1826715 -4.146975 -4.099987 -4.0253267 -3.9177356 -3.8155355 -3.7888379 -3.8587821 -3.9508491 -4.0475054 -4.1340327 -4.2017107 -4.2516832 -4.2898035][-4.189713 -4.1580858 -4.1349821 -4.1129894 -4.0717931 -4.0113626 -3.9548495 -3.9430368 -3.9897909 -4.0524211 -4.1166291 -4.1745248 -4.2282228 -4.2689371 -4.2984643][-4.1780467 -4.1494517 -4.1335473 -4.1261134 -4.1097059 -4.089026 -4.0648279 -4.0544748 -4.0761967 -4.1178417 -4.1598911 -4.2025671 -4.2504873 -4.285212 -4.3088031][-4.1909919 -4.1690993 -4.1511197 -4.1389942 -4.1247053 -4.116344 -4.10538 -4.0886354 -4.0964723 -4.1348705 -4.1732006 -4.2149005 -4.2627439 -4.2973495 -4.3189836][-4.207736 -4.1900783 -4.1744709 -4.1607413 -4.1487193 -4.1480517 -4.1460366 -4.1278863 -4.1287937 -4.1645279 -4.2009058 -4.2384081 -4.2825651 -4.3151321 -4.3337483][-4.2467637 -4.2364826 -4.2282543 -4.2206316 -4.2166429 -4.2228866 -4.2272434 -4.2117896 -4.2102942 -4.2381544 -4.2670064 -4.2927275 -4.3236179 -4.345098 -4.3546906][-4.2979789 -4.2937369 -4.2917962 -4.291183 -4.292551 -4.3015842 -4.3086967 -4.2994809 -4.2972865 -4.3148479 -4.3325396 -4.3462067 -4.362607 -4.3713441 -4.3718739]]...]
INFO - root - 2017-12-05 23:04:13.075903: step 51310, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 69h:43m:55s remains)
INFO - root - 2017-12-05 23:04:22.785416: step 51320, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 75h:38m:06s remains)
INFO - root - 2017-12-05 23:04:32.239315: step 51330, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 76h:55m:30s remains)
INFO - root - 2017-12-05 23:04:41.760452: step 51340, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 74h:29m:21s remains)
INFO - root - 2017-12-05 23:04:51.135140: step 51350, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 71h:55m:15s remains)
INFO - root - 2017-12-05 23:05:00.772809: step 51360, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 75h:34m:10s remains)
INFO - root - 2017-12-05 23:05:10.183982: step 51370, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 71h:03m:20s remains)
INFO - root - 2017-12-05 23:05:19.423103: step 51380, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 70h:36m:13s remains)
INFO - root - 2017-12-05 23:05:28.723552: step 51390, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.961 sec/batch; 75h:02m:44s remains)
INFO - root - 2017-12-05 23:05:38.229483: step 51400, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.904 sec/batch; 70h:34m:26s remains)
2017-12-05 23:05:39.023929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3040485 -4.2999096 -4.2976503 -4.2953854 -4.2922897 -4.2897587 -4.2882781 -4.2886953 -4.2926264 -4.2991481 -4.306706 -4.3134513 -4.316884 -4.3168311 -4.316051][-4.2873135 -4.2812138 -4.2797174 -4.2788029 -4.274641 -4.2680283 -4.2625289 -4.2607675 -4.265029 -4.2749915 -4.2894621 -4.3029861 -4.3102107 -4.311172 -4.3114834][-4.2571816 -4.247714 -4.2455425 -4.2455935 -4.2396169 -4.2271047 -4.2136412 -4.2063313 -4.2113075 -4.2274423 -4.252738 -4.2758512 -4.2882652 -4.2920084 -4.2962317][-4.2142673 -4.1994505 -4.1953039 -4.1935043 -4.1833615 -4.1646342 -4.1438265 -4.1320887 -4.1375704 -4.1607537 -4.1974654 -4.2308068 -4.2496562 -4.25808 -4.26908][-4.1656861 -4.1459584 -4.1398349 -4.1354342 -4.1211691 -4.0968418 -4.0712934 -4.0572844 -4.0635 -4.0946064 -4.1421595 -4.1843619 -4.2086258 -4.2214079 -4.2401271][-4.1317892 -4.1110916 -4.1043935 -4.0977383 -4.0788817 -4.0491004 -4.0166225 -3.999691 -4.0095019 -4.0496035 -4.1074 -4.156281 -4.1842604 -4.198657 -4.2195954][-4.1298494 -4.1114755 -4.1091976 -4.1056924 -4.0857334 -4.0522757 -4.0144548 -3.9953561 -4.0092311 -4.0570431 -4.1191506 -4.1674061 -4.1937318 -4.2033439 -4.2164178][-4.147099 -4.1349788 -4.1423364 -4.1482315 -4.1368632 -4.1082106 -4.0682492 -4.0438156 -4.0570984 -4.1060481 -4.1637454 -4.2029214 -4.2213449 -4.2226882 -4.2259045][-4.15707 -4.1504717 -4.1665292 -4.1815352 -4.1798325 -4.1585865 -4.120224 -4.0928297 -4.1054654 -4.1542959 -4.2034278 -4.2343006 -4.24678 -4.2434287 -4.2410145][-4.1616507 -4.1604471 -4.1799397 -4.1966734 -4.197813 -4.1820087 -4.1482649 -4.1276298 -4.1447287 -4.1886892 -4.2289081 -4.2548981 -4.2637925 -4.2591629 -4.2553353][-4.1744976 -4.1792254 -4.1984735 -4.211556 -4.2118077 -4.200665 -4.1764469 -4.1663656 -4.1864691 -4.2237582 -4.2564015 -4.2762823 -4.2821631 -4.2779055 -4.2734184][-4.2025719 -4.2103047 -4.2269583 -4.2372246 -4.2368436 -4.2280245 -4.2128477 -4.2115297 -4.2316523 -4.26185 -4.2876544 -4.3023148 -4.3060346 -4.3014894 -4.295475][-4.2469082 -4.2545133 -4.2671309 -4.2748518 -4.274682 -4.2686858 -4.2602072 -4.2610922 -4.2767086 -4.2984643 -4.3165336 -4.3258767 -4.3268976 -4.3212976 -4.3143997][-4.2871571 -4.2922087 -4.2999659 -4.3049192 -4.3059368 -4.3023534 -4.2977023 -4.2986288 -4.3080206 -4.3207445 -4.3313165 -4.3358641 -4.3347573 -4.3292909 -4.3236666][-4.3106508 -4.3127756 -4.3168631 -4.319581 -4.3210416 -4.3203545 -4.3179808 -4.3180571 -4.3218436 -4.3268924 -4.3313446 -4.3328419 -4.3317046 -4.3285251 -4.3255358]]...]
INFO - root - 2017-12-05 23:05:48.314889: step 51410, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 70h:05m:56s remains)
INFO - root - 2017-12-05 23:05:57.606373: step 51420, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 75h:54m:00s remains)
INFO - root - 2017-12-05 23:06:07.067831: step 51430, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 72h:41m:13s remains)
INFO - root - 2017-12-05 23:06:16.526595: step 51440, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 75h:09m:49s remains)
INFO - root - 2017-12-05 23:06:25.981399: step 51450, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 76h:31m:44s remains)
INFO - root - 2017-12-05 23:06:35.370987: step 51460, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 75h:05m:48s remains)
INFO - root - 2017-12-05 23:06:44.684034: step 51470, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 76h:34m:11s remains)
INFO - root - 2017-12-05 23:06:53.756013: step 51480, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 73h:31m:18s remains)
INFO - root - 2017-12-05 23:07:03.278461: step 51490, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 73h:57m:53s remains)
INFO - root - 2017-12-05 23:07:12.621924: step 51500, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 72h:15m:46s remains)
2017-12-05 23:07:13.369208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1548734 -4.1653914 -4.1592107 -4.1466241 -4.1634083 -4.1972189 -4.2260671 -4.2353539 -4.23761 -4.2389073 -4.2442069 -4.25059 -4.2353997 -4.2060757 -4.17093][-4.127358 -4.1434031 -4.142952 -4.1376333 -4.1599441 -4.1917057 -4.2147803 -4.2262821 -4.2387428 -4.2526269 -4.2667222 -4.2740989 -4.259891 -4.2353439 -4.20878][-4.11125 -4.1146369 -4.1098151 -4.1116681 -4.1468749 -4.189837 -4.2174468 -4.2348819 -4.2537088 -4.2721 -4.2894363 -4.2951055 -4.2856278 -4.2739587 -4.2645307][-4.1070986 -4.0746045 -4.0496874 -4.0533466 -4.1019716 -4.1645317 -4.209528 -4.23679 -4.2606468 -4.2830615 -4.3025208 -4.3057303 -4.2978754 -4.2926283 -4.2916956][-4.1081319 -4.0350013 -3.9858625 -3.9893754 -4.0449171 -4.1188374 -4.1790338 -4.2116494 -4.2382026 -4.2666249 -4.2915978 -4.2968812 -4.2888532 -4.2836051 -4.2837758][-4.1116319 -4.0145984 -3.9494669 -3.9471755 -4.0007167 -4.0759377 -4.1362491 -4.1699014 -4.2011724 -4.23544 -4.2652454 -4.2737775 -4.2676105 -4.2610793 -4.2593913][-4.1209579 -4.0276756 -3.9690113 -3.9651775 -4.0074067 -4.0674853 -4.1096969 -4.1336689 -4.1627073 -4.1982846 -4.2308364 -4.2418933 -4.2353444 -4.2276449 -4.2262087][-4.1556296 -4.0892596 -4.0481224 -4.0438442 -4.062901 -4.0867343 -4.0932755 -4.0941205 -4.1135993 -4.1494956 -4.1893768 -4.2096171 -4.2053905 -4.1923318 -4.1875334][-4.2040439 -4.1660714 -4.141345 -4.1338477 -4.1309342 -4.1223769 -4.0977592 -4.0736389 -4.0818605 -4.1183476 -4.1640463 -4.1908245 -4.1882949 -4.1744242 -4.1682539][-4.2361517 -4.2219124 -4.214891 -4.2126102 -4.2021966 -4.1788988 -4.1443663 -4.1129045 -4.1139441 -4.1371374 -4.1683574 -4.1872573 -4.1850882 -4.1768913 -4.1771374][-4.2458568 -4.2497358 -4.2571049 -4.2603941 -4.2525177 -4.2341557 -4.2102828 -4.1843677 -4.1803913 -4.1891727 -4.2017484 -4.2094703 -4.2061405 -4.2018218 -4.2075534][-4.2495985 -4.2595177 -4.2658033 -4.2647228 -4.2586255 -4.2491212 -4.2396383 -4.2308555 -4.2342916 -4.2414403 -4.2479405 -4.2506862 -4.2481966 -4.2438574 -4.2478819][-4.2437387 -4.2463374 -4.24371 -4.2372255 -4.22903 -4.2227383 -4.2258005 -4.2340536 -4.2467036 -4.256916 -4.2692471 -4.2777915 -4.2810655 -4.2819829 -4.2863789][-4.2221651 -4.2151165 -4.2040043 -4.1907983 -4.1815615 -4.1826682 -4.1955738 -4.2109094 -4.2261944 -4.2403078 -4.2596626 -4.2781634 -4.2922812 -4.3017516 -4.3096304][-4.2046976 -4.189167 -4.1695724 -4.153573 -4.1520624 -4.1659446 -4.1842418 -4.1974764 -4.2074075 -4.2229614 -4.2480283 -4.2715607 -4.2900019 -4.3036456 -4.3134632]]...]
INFO - root - 2017-12-05 23:07:22.820838: step 51510, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 72h:32m:29s remains)
INFO - root - 2017-12-05 23:07:32.216350: step 51520, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.914 sec/batch; 71h:21m:43s remains)
INFO - root - 2017-12-05 23:07:41.448005: step 51530, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 67h:54m:53s remains)
INFO - root - 2017-12-05 23:07:50.829939: step 51540, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.977 sec/batch; 76h:13m:09s remains)
INFO - root - 2017-12-05 23:08:00.306137: step 51550, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 73h:28m:02s remains)
INFO - root - 2017-12-05 23:08:09.762645: step 51560, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 72h:56m:03s remains)
INFO - root - 2017-12-05 23:08:19.118424: step 51570, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 74h:33m:19s remains)
INFO - root - 2017-12-05 23:08:28.598179: step 51580, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 72h:17m:50s remains)
INFO - root - 2017-12-05 23:08:38.023019: step 51590, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 72h:06m:06s remains)
INFO - root - 2017-12-05 23:08:47.579610: step 51600, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.976 sec/batch; 76h:10m:10s remains)
2017-12-05 23:08:48.431456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2760582 -4.2963581 -4.2981496 -4.2886305 -4.2746134 -4.2593637 -4.2426128 -4.2336926 -4.2342043 -4.2372565 -4.2318506 -4.2190952 -4.2087154 -4.2016048 -4.1937132][-4.2764783 -4.3002396 -4.3056083 -4.2985063 -4.284831 -4.2668848 -4.2484827 -4.2443767 -4.2535191 -4.2629137 -4.2538614 -4.233479 -4.2173324 -4.2065721 -4.1931009][-4.2730508 -4.2945085 -4.3001151 -4.2948103 -4.2836671 -4.2597585 -4.2335649 -4.2289886 -4.2474532 -4.2645626 -4.2571712 -4.2335191 -4.2148771 -4.2012706 -4.1826572][-4.272646 -4.2853532 -4.2873907 -4.2848721 -4.2736568 -4.2396469 -4.1954336 -4.1814728 -4.2064419 -4.2378139 -4.2488604 -4.2397962 -4.2251906 -4.2026911 -4.1773953][-4.2764125 -4.2786131 -4.2756624 -4.2711835 -4.2570243 -4.2113471 -4.14366 -4.1103554 -4.1405082 -4.196197 -4.233849 -4.2454953 -4.2404165 -4.2117081 -4.17958][-4.2799649 -4.2762933 -4.2669497 -4.2550578 -4.2314978 -4.1662359 -4.0647631 -3.9986129 -4.0396132 -4.1307912 -4.194941 -4.22798 -4.2357297 -4.2109237 -4.179111][-4.2828751 -4.2767987 -4.2638159 -4.2438107 -4.2047796 -4.1140184 -3.9680431 -3.8438253 -3.890527 -4.0383668 -4.1408887 -4.1942768 -4.2150917 -4.2038884 -4.1808977][-4.2876983 -4.2828875 -4.2716317 -4.2462215 -4.1916571 -4.0804434 -3.9038529 -3.7236869 -3.7639475 -3.9603186 -4.1002626 -4.1670165 -4.1947169 -4.1995683 -4.1922593][-4.2929277 -4.2921877 -4.2892604 -4.2665935 -4.2089391 -4.0987763 -3.9406762 -3.7785866 -3.7936211 -3.9608376 -4.0989418 -4.1665015 -4.1925755 -4.2050118 -4.2107925][-4.2837682 -4.2859974 -4.2928114 -4.2867002 -4.2450786 -4.159965 -4.04757 -3.9397197 -3.9312937 -4.0222263 -4.1258488 -4.1857982 -4.210259 -4.221787 -4.2273221][-4.2601485 -4.2639761 -4.27908 -4.289063 -4.2705173 -4.2164359 -4.1439085 -4.0744896 -4.0560241 -4.0923109 -4.1579437 -4.2043509 -4.227366 -4.2364187 -4.234962][-4.2259531 -4.2316742 -4.2537141 -4.2773333 -4.2778931 -4.2497654 -4.2072835 -4.1635823 -4.1448293 -4.1585727 -4.19522 -4.2224331 -4.2376118 -4.2434182 -4.2351112][-4.1764808 -4.1816611 -4.2141328 -4.2538881 -4.272162 -4.2682643 -4.2495217 -4.2255554 -4.2098446 -4.2148347 -4.23294 -4.2427692 -4.2488017 -4.2503591 -4.2419887][-4.1273913 -4.1375303 -4.1834311 -4.2366047 -4.2700639 -4.2839723 -4.2795925 -4.2667484 -4.2568321 -4.2598848 -4.267766 -4.266499 -4.2648048 -4.2617059 -4.2540436][-4.0882888 -4.1020174 -4.1603069 -4.2260551 -4.2685413 -4.2937632 -4.2941022 -4.2842088 -4.2794728 -4.2850323 -4.2935915 -4.2923503 -4.2878046 -4.2801785 -4.2733693]]...]
INFO - root - 2017-12-05 23:08:57.880126: step 51610, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 73h:14m:52s remains)
INFO - root - 2017-12-05 23:09:06.994134: step 51620, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 75h:22m:26s remains)
INFO - root - 2017-12-05 23:09:16.390730: step 51630, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 75h:53m:34s remains)
INFO - root - 2017-12-05 23:09:25.641090: step 51640, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.936 sec/batch; 72h:59m:21s remains)
INFO - root - 2017-12-05 23:09:35.281627: step 51650, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 76h:10m:04s remains)
INFO - root - 2017-12-05 23:09:44.683001: step 51660, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.976 sec/batch; 76h:06m:50s remains)
INFO - root - 2017-12-05 23:09:54.056735: step 51670, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 72h:51m:25s remains)
INFO - root - 2017-12-05 23:10:03.395927: step 51680, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 77h:44m:56s remains)
INFO - root - 2017-12-05 23:10:12.578959: step 51690, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 73h:34m:41s remains)
INFO - root - 2017-12-05 23:10:22.184361: step 51700, loss = 2.04, batch loss = 1.98 (8.0 examples/sec; 0.998 sec/batch; 77h:51m:52s remains)
2017-12-05 23:10:23.095326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1811523 -4.20256 -4.1897655 -4.1706333 -4.1539254 -4.1463256 -4.1524262 -4.1693926 -4.17026 -4.1233211 -4.0557733 -4.0216727 -4.0458121 -4.1156926 -4.1905851][-4.210351 -4.2135954 -4.1880722 -4.1647 -4.1458292 -4.1270332 -4.1204567 -4.1333666 -4.1406097 -4.0992 -4.0428228 -4.0215573 -4.0513272 -4.1177158 -4.1886344][-4.2324481 -4.2245374 -4.1939025 -4.1699843 -4.1464195 -4.112412 -4.088738 -4.0929613 -4.1062136 -4.0770345 -4.0336976 -4.0277691 -4.0620718 -4.1220231 -4.1871319][-4.2445693 -4.2333446 -4.20169 -4.1764956 -4.1468239 -4.101985 -4.0623174 -4.0529881 -4.0731583 -4.0634489 -4.0316467 -4.0361619 -4.0736251 -4.1288371 -4.1900129][-4.2506204 -4.2397394 -4.2101212 -4.1826081 -4.1476126 -4.0913167 -4.03188 -4.0032325 -4.0323925 -4.04827 -4.031024 -4.0452557 -4.0853429 -4.1401114 -4.1985888][-4.250936 -4.2418456 -4.2156954 -4.1848612 -4.141839 -4.0713391 -3.9872837 -3.9316373 -3.9679179 -4.0147543 -4.0182834 -4.0415635 -4.0868835 -4.1450453 -4.2032251][-4.2478247 -4.2426238 -4.2216244 -4.18824 -4.1343851 -4.0419903 -3.9252794 -3.83629 -3.8741679 -3.9542437 -3.9851725 -4.0242238 -4.078342 -4.1402426 -4.1993451][-4.2527814 -4.2516832 -4.2391396 -4.2084494 -4.1513672 -4.0409875 -3.9006433 -3.7866163 -3.8163848 -3.9156809 -3.965745 -4.0202384 -4.0791759 -4.1410103 -4.1996479][-4.272397 -4.2738314 -4.2671227 -4.2436428 -4.1945553 -4.0901446 -3.9645219 -3.8635631 -3.8846803 -3.9666629 -4.00681 -4.0585284 -4.1109142 -4.1652842 -4.2157021][-4.2941227 -4.2959065 -4.290679 -4.2707677 -4.2336407 -4.1526937 -4.0617232 -3.9886861 -4.0012846 -4.0529389 -4.0700407 -4.1036091 -4.1460094 -4.1921644 -4.233603][-4.3071804 -4.3092251 -4.3024707 -4.2826047 -4.25365 -4.196888 -4.138617 -4.0906358 -4.0976 -4.1251287 -4.1198492 -4.1313076 -4.1631293 -4.203752 -4.2388816][-4.3113513 -4.3152204 -4.3090339 -4.2910028 -4.2675095 -4.2270331 -4.1893353 -4.1577234 -4.1622696 -4.1740127 -4.1526823 -4.1498137 -4.1735516 -4.2091131 -4.2384796][-4.3165612 -4.3215866 -4.3179383 -4.3046308 -4.2879333 -4.257884 -4.2304339 -4.2082887 -4.21126 -4.2134776 -4.1848645 -4.1745038 -4.1927786 -4.2218509 -4.2445316][-4.33131 -4.3379884 -4.3372426 -4.3268013 -4.3138223 -4.290256 -4.2695546 -4.2541165 -4.2567215 -4.2545328 -4.2251978 -4.2102356 -4.2220516 -4.2435756 -4.2593937][-4.3444257 -4.3519955 -4.3516011 -4.3418016 -4.3296165 -4.311173 -4.2960324 -4.2856879 -4.2888641 -4.2872157 -4.2640266 -4.2480197 -4.2536192 -4.2678571 -4.2780685]]...]
INFO - root - 2017-12-05 23:10:32.397036: step 51710, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 67h:45m:35s remains)
INFO - root - 2017-12-05 23:10:41.823088: step 51720, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.003 sec/batch; 78h:15m:45s remains)
INFO - root - 2017-12-05 23:10:51.275644: step 51730, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 71h:26m:07s remains)
INFO - root - 2017-12-05 23:11:00.630635: step 51740, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 73h:20m:28s remains)
INFO - root - 2017-12-05 23:11:09.982205: step 51750, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.016 sec/batch; 79h:14m:34s remains)
INFO - root - 2017-12-05 23:11:19.432403: step 51760, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 76h:40m:53s remains)
INFO - root - 2017-12-05 23:11:28.496219: step 51770, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 75h:01m:10s remains)
INFO - root - 2017-12-05 23:11:37.990216: step 51780, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 77h:22m:31s remains)
INFO - root - 2017-12-05 23:11:47.276778: step 51790, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 75h:31m:51s remains)
INFO - root - 2017-12-05 23:11:56.713240: step 51800, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.916 sec/batch; 71h:24m:47s remains)
2017-12-05 23:11:57.471994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2055779 -4.2119322 -4.2244048 -4.2265024 -4.2204013 -4.2130504 -4.2067342 -4.199079 -4.1890306 -4.177146 -4.1676369 -4.166759 -4.1713314 -4.1790466 -4.1941109][-4.2050428 -4.2109103 -4.2199087 -4.2175026 -4.2101836 -4.2029295 -4.1962781 -4.1877961 -4.1753421 -4.1613774 -4.1502414 -4.15074 -4.1599379 -4.1724877 -4.1911387][-4.2009668 -4.2017374 -4.203166 -4.1944404 -4.181541 -4.168067 -4.1562886 -4.1486416 -4.1425376 -4.1352315 -4.1286941 -4.1331258 -4.1510172 -4.1719575 -4.1932287][-4.1921983 -4.1863527 -4.1779528 -4.1584487 -4.1316428 -4.1025429 -4.0830407 -4.0821319 -4.0937462 -4.1015077 -4.1053405 -4.1164179 -4.1424384 -4.1718779 -4.1946826][-4.1832223 -4.169055 -4.152154 -4.1250739 -4.0803943 -4.0233212 -3.9885623 -4.0009651 -4.0426483 -4.0731339 -4.0874729 -4.1074805 -4.1378245 -4.1688681 -4.1902246][-4.1662412 -4.145462 -4.1261959 -4.0965672 -4.0334592 -3.9381034 -3.8724456 -3.9029942 -3.9864848 -4.0513678 -4.0842719 -4.1074128 -4.1342888 -4.1595764 -4.17606][-4.13717 -4.1151 -4.1025143 -4.0793371 -4.0148692 -3.8984425 -3.7996883 -3.8405747 -3.959096 -4.0503626 -4.0981932 -4.1259632 -4.1450195 -4.1598873 -4.1669421][-4.1059222 -4.0795364 -4.0727053 -4.069427 -4.0325427 -3.9519105 -3.8785269 -3.9032505 -3.9971383 -4.0774651 -4.1245351 -4.1545768 -4.1723075 -4.181087 -4.1803875][-4.0757151 -4.0464292 -4.0453739 -4.0682673 -4.07186 -4.0406618 -4.0063677 -4.0113015 -4.0530667 -4.1024685 -4.1433015 -4.1784625 -4.2026663 -4.2126603 -4.209321][-4.0807409 -4.0551438 -4.0550666 -4.08456 -4.1086221 -4.1060634 -4.0909495 -4.0819097 -4.0919676 -4.1214137 -4.1582489 -4.1936269 -4.2186928 -4.2279139 -4.2268162][-4.1267986 -4.1006651 -4.0936508 -4.1113377 -4.1372862 -4.1502972 -4.1493468 -4.1402946 -4.1392741 -4.1589069 -4.1896772 -4.2165303 -4.2326069 -4.2316909 -4.2260137][-4.1809397 -4.1528816 -4.1338749 -4.1315904 -4.1492023 -4.169436 -4.1838288 -4.18786 -4.1913424 -4.207829 -4.2297106 -4.2455015 -4.2450886 -4.2287054 -4.2141967][-4.2135754 -4.1818342 -4.1579494 -4.1450291 -4.1537747 -4.1775737 -4.2015047 -4.2169275 -4.2279391 -4.2432575 -4.2582188 -4.2650776 -4.2526026 -4.2259 -4.2060809][-4.2381258 -4.2096372 -4.1844 -4.1679249 -4.17152 -4.1943674 -4.2186337 -4.2336979 -4.2430286 -4.2551546 -4.2634397 -4.262743 -4.2471318 -4.2219973 -4.2025661][-4.2562771 -4.2382092 -4.2168384 -4.2016735 -4.2029376 -4.2199526 -4.2339225 -4.2381787 -4.2431831 -4.2510204 -4.2524343 -4.2477937 -4.2360849 -4.2193594 -4.2051344]]...]
INFO - root - 2017-12-05 23:12:06.897051: step 51810, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 72h:10m:10s remains)
INFO - root - 2017-12-05 23:12:16.312401: step 51820, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 67h:11m:57s remains)
INFO - root - 2017-12-05 23:12:25.878969: step 51830, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 75h:43m:48s remains)
INFO - root - 2017-12-05 23:12:35.233534: step 51840, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 74h:18m:01s remains)
INFO - root - 2017-12-05 23:12:44.683820: step 51850, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 74h:47m:02s remains)
INFO - root - 2017-12-05 23:12:53.898716: step 51860, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 67h:10m:34s remains)
INFO - root - 2017-12-05 23:13:03.234769: step 51870, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 75h:20m:40s remains)
INFO - root - 2017-12-05 23:13:12.510440: step 51880, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 70h:18m:21s remains)
INFO - root - 2017-12-05 23:13:21.890009: step 51890, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 73h:10m:37s remains)
INFO - root - 2017-12-05 23:13:31.367211: step 51900, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 73h:09m:26s remains)
2017-12-05 23:13:32.118429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2421732 -4.2552838 -4.2681379 -4.2771521 -4.2868538 -4.2846441 -4.2740321 -4.2703805 -4.2657008 -4.2532363 -4.2417693 -4.238584 -4.2374229 -4.24776 -4.2650423][-4.1998568 -4.2103839 -4.2212782 -4.2329893 -4.2471929 -4.2480412 -4.2400126 -4.2425675 -4.2463164 -4.2394629 -4.2316885 -4.2315249 -4.2365432 -4.247272 -4.2648211][-4.1842365 -4.1867433 -4.1911407 -4.2028904 -4.2167912 -4.2166734 -4.210031 -4.2205167 -4.2340407 -4.234808 -4.2317739 -4.2349749 -4.2437663 -4.2543969 -4.271812][-4.1677341 -4.1676292 -4.171483 -4.1819472 -4.1934 -4.1887555 -4.179914 -4.1990104 -4.227766 -4.2343497 -4.2337127 -4.240283 -4.25135 -4.2615314 -4.2790909][-4.1297569 -4.1207113 -4.1188431 -4.1222086 -4.1291776 -4.1149473 -4.0986948 -4.1267328 -4.1738133 -4.19204 -4.2002497 -4.2163978 -4.2342486 -4.2489309 -4.2700667][-4.0389528 -4.02556 -4.022141 -4.0141792 -4.0058804 -3.972647 -3.9440176 -3.981185 -4.050838 -4.0937967 -4.1269846 -4.1656203 -4.2021646 -4.229455 -4.2576828][-3.90872 -3.8915083 -3.8886163 -3.8734305 -3.847085 -3.7965102 -3.761445 -3.8197973 -3.9220784 -3.9979229 -4.0603447 -4.12195 -4.176837 -4.2158966 -4.2499537][-3.8970957 -3.8701692 -3.8510251 -3.8207576 -3.7734408 -3.7098002 -3.67193 -3.733773 -3.8488407 -3.9384272 -4.0172935 -4.0919166 -4.1587715 -4.2062335 -4.2444386][-4.0248923 -3.998841 -3.9697735 -3.9370494 -3.8933935 -3.8402352 -3.8078887 -3.8473949 -3.9278994 -3.986953 -4.0415845 -4.1004481 -4.1612248 -4.2070546 -4.2440033][-4.1436238 -4.1283774 -4.1027489 -4.0780864 -4.0478482 -4.0105109 -3.9870148 -4.0082617 -4.0519753 -4.079215 -4.1036582 -4.1385274 -4.1816468 -4.2167268 -4.2476149][-4.2259068 -4.2193842 -4.2021666 -4.1827979 -4.1570563 -4.1279426 -4.1085558 -4.1146545 -4.1367211 -4.1505623 -4.1588969 -4.1779656 -4.206768 -4.2299314 -4.2531519][-4.2911744 -4.2893028 -4.2794204 -4.2698078 -4.255548 -4.2367525 -4.2220373 -4.2205005 -4.2271705 -4.2334571 -4.2342834 -4.2414012 -4.2546396 -4.2632246 -4.2747545][-4.3069334 -4.3084283 -4.3077927 -4.3066864 -4.3039279 -4.2944779 -4.2878747 -4.2891617 -4.2936034 -4.2967744 -4.29707 -4.3002291 -4.3031778 -4.3019896 -4.3037362][-4.3053889 -4.3057914 -4.3066974 -4.3073645 -4.3074818 -4.3015561 -4.2995882 -4.3063068 -4.3163366 -4.3251457 -4.3310776 -4.3360252 -4.3358765 -4.3295364 -4.325038][-4.3060422 -4.304738 -4.3037796 -4.3028584 -4.30149 -4.2944336 -4.2890048 -4.2945552 -4.3072534 -4.3195276 -4.3310237 -4.3400211 -4.3412809 -4.3362379 -4.3317738]]...]
INFO - root - 2017-12-05 23:13:41.537926: step 51910, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 71h:15m:04s remains)
INFO - root - 2017-12-05 23:13:50.860812: step 51920, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 74h:07m:55s remains)
INFO - root - 2017-12-05 23:14:00.356563: step 51930, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 74h:38m:03s remains)
INFO - root - 2017-12-05 23:14:09.862555: step 51940, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 74h:01m:25s remains)
INFO - root - 2017-12-05 23:14:19.079336: step 51950, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 72h:45m:22s remains)
INFO - root - 2017-12-05 23:14:28.411031: step 51960, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 75h:20m:22s remains)
INFO - root - 2017-12-05 23:14:37.820500: step 51970, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 74h:44m:07s remains)
INFO - root - 2017-12-05 23:14:47.068717: step 51980, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 75h:35m:41s remains)
INFO - root - 2017-12-05 23:14:56.423444: step 51990, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 69h:20m:33s remains)
INFO - root - 2017-12-05 23:15:05.810535: step 52000, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 76h:24m:10s remains)
2017-12-05 23:15:06.613699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3117108 -4.3179054 -4.3245578 -4.3282604 -4.3244343 -4.3075166 -4.2770791 -4.2465706 -4.2360711 -4.255393 -4.2868133 -4.3067904 -4.3075471 -4.2937131 -4.2729993][-4.3041029 -4.3107915 -4.3167629 -4.3176374 -4.3066711 -4.2792654 -4.2365613 -4.2004704 -4.1935387 -4.2222877 -4.2670245 -4.3005395 -4.3130713 -4.3045468 -4.2803993][-4.2982 -4.3040476 -4.306736 -4.2994008 -4.2744694 -4.2299232 -4.1719093 -4.13087 -4.1302037 -4.1701727 -4.2291045 -4.2788835 -4.3076315 -4.3087893 -4.2861543][-4.2997007 -4.30317 -4.2993183 -4.2787094 -4.2309375 -4.1631684 -4.085793 -4.0396 -4.0505004 -4.1072235 -4.1827636 -4.25071 -4.2972593 -4.3112884 -4.2955751][-4.3070588 -4.3057017 -4.291419 -4.2536283 -4.18114 -4.0889235 -3.9926345 -3.9450691 -3.9761577 -4.0557132 -4.14718 -4.2305255 -4.2909136 -4.3156834 -4.3077421][-4.3177814 -4.3103895 -4.2870803 -4.2353396 -4.1450143 -4.03673 -3.9325693 -3.8928876 -3.9444251 -4.0392318 -4.1374264 -4.2274113 -4.2946482 -4.3251991 -4.3220749][-4.3271379 -4.3159046 -4.2891912 -4.2331586 -4.1361189 -4.0218911 -3.9181662 -3.891809 -3.9571173 -4.0560112 -4.151516 -4.2394543 -4.3043675 -4.3349824 -4.3339405][-4.3308744 -4.3170972 -4.2906895 -4.2385621 -4.1485977 -4.0407486 -3.9487391 -3.936409 -4.0028067 -4.09295 -4.1780248 -4.25652 -4.313508 -4.3406081 -4.3396554][-4.3289089 -4.3141384 -4.2901707 -4.2464495 -4.1717343 -4.0827494 -4.0133462 -4.0118256 -4.0690227 -4.142509 -4.2134604 -4.2764559 -4.3224373 -4.3431492 -4.3398128][-4.322093 -4.3080516 -4.2878275 -4.2546391 -4.2006359 -4.1382685 -4.093122 -4.0973897 -4.1392341 -4.1931739 -4.2495608 -4.2985497 -4.3328762 -4.3446112 -4.3370333][-4.3084784 -4.2945666 -4.2766194 -4.2536035 -4.2224183 -4.189 -4.1662951 -4.1743731 -4.2039886 -4.2424212 -4.284286 -4.3202887 -4.3412886 -4.34332 -4.331512][-4.2900233 -4.2738943 -4.2552371 -4.2417235 -4.2356353 -4.2319221 -4.2307658 -4.24179 -4.2611136 -4.2861457 -4.3135672 -4.334662 -4.3423491 -4.3361425 -4.3223085][-4.2688322 -4.2461276 -4.2237158 -4.2180142 -4.2350612 -4.2589226 -4.277492 -4.291388 -4.3019919 -4.3147178 -4.3276091 -4.3342385 -4.3315139 -4.3206797 -4.3080235][-4.2471561 -4.2165217 -4.1895409 -4.1899443 -4.2227421 -4.2655325 -4.2977657 -4.3143163 -4.3205886 -4.3237805 -4.3233809 -4.3177414 -4.3074746 -4.2956095 -4.2876945][-4.2311931 -4.1966219 -4.1705604 -4.1757503 -4.2130237 -4.2606077 -4.2961545 -4.313499 -4.3175931 -4.3141785 -4.3047619 -4.2919693 -4.2782946 -4.2684813 -4.2672009]]...]
INFO - root - 2017-12-05 23:15:15.765687: step 52010, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 74h:49m:30s remains)
INFO - root - 2017-12-05 23:15:25.535665: step 52020, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.991 sec/batch; 77h:12m:15s remains)
INFO - root - 2017-12-05 23:15:34.691175: step 52030, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 75h:27m:21s remains)
INFO - root - 2017-12-05 23:15:43.869863: step 52040, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 66h:32m:40s remains)
INFO - root - 2017-12-05 23:15:53.126453: step 52050, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 71h:59m:01s remains)
INFO - root - 2017-12-05 23:16:02.498851: step 52060, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 72h:59m:20s remains)
INFO - root - 2017-12-05 23:16:12.006965: step 52070, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 73h:28m:15s remains)
INFO - root - 2017-12-05 23:16:21.366326: step 52080, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 75h:59m:44s remains)
INFO - root - 2017-12-05 23:16:30.914348: step 52090, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 75h:39m:40s remains)
INFO - root - 2017-12-05 23:16:40.214659: step 52100, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 69h:49m:36s remains)
2017-12-05 23:16:41.015792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2740755 -4.2949991 -4.31565 -4.316596 -4.2972345 -4.273396 -4.2599797 -4.2691436 -4.2901678 -4.3053727 -4.3096309 -4.3003578 -4.2864313 -4.28813 -4.3063183][-4.2525616 -4.2830133 -4.3078055 -4.3072019 -4.2793927 -4.2396994 -4.2189612 -4.2339878 -4.2660704 -4.2884026 -4.2903776 -4.2720633 -4.2521863 -4.2593288 -4.2867103][-4.2242436 -4.2622714 -4.2868395 -4.2799129 -4.2379165 -4.1771283 -4.1500292 -4.1780624 -4.22788 -4.2609429 -4.2612553 -4.2288866 -4.19833 -4.21006 -4.2482862][-4.2038417 -4.2463489 -4.2650084 -4.2450461 -4.1839304 -4.0961161 -4.0546703 -4.101397 -4.1837549 -4.2349324 -4.23025 -4.1814885 -4.14331 -4.16183 -4.2107372][-4.2070274 -4.2461615 -4.2531343 -4.2114325 -4.1238203 -4.0021496 -3.9357049 -4.0048251 -4.1325922 -4.209991 -4.2065077 -4.1479445 -4.1084113 -4.1333323 -4.1889639][-4.2466993 -4.2728086 -4.2621403 -4.1956882 -4.0780678 -3.911483 -3.8033524 -3.8926096 -4.070128 -4.17998 -4.1887474 -4.1340351 -4.0992112 -4.127326 -4.1876388][-4.2926912 -4.3089871 -4.2933712 -4.2208438 -4.0828218 -3.8735526 -3.7034731 -3.7896435 -4.0062137 -4.1492252 -4.1811028 -4.1464925 -4.1204906 -4.1438332 -4.2005916][-4.3281274 -4.3428569 -4.3376536 -4.2788143 -4.1484175 -3.9386015 -3.7476225 -3.7908688 -3.9920278 -4.1454096 -4.1948614 -4.1797981 -4.1637673 -4.1760411 -4.2171793][-4.3292556 -4.3488307 -4.3594708 -4.3261871 -4.2240505 -4.0488095 -3.8783252 -3.8781962 -4.0299988 -4.171174 -4.2286315 -4.2250896 -4.21739 -4.221817 -4.2434907][-4.3073 -4.3330021 -4.3558564 -4.3479691 -4.2781496 -4.1407723 -3.9984736 -3.9770432 -4.0888348 -4.21075 -4.2693348 -4.276423 -4.2746453 -4.2756219 -4.282722][-4.2809029 -4.3096352 -4.3409624 -4.3516216 -4.3113222 -4.2163706 -4.1106234 -4.0811596 -4.1537361 -4.2502031 -4.3020458 -4.3151989 -4.319118 -4.3200083 -4.3201666][-4.2742758 -4.298656 -4.3317704 -4.3512392 -4.3351836 -4.2771478 -4.2041931 -4.1735439 -4.21275 -4.2807732 -4.3234458 -4.3377481 -4.3431478 -4.3443255 -4.3415203][-4.294241 -4.3124323 -4.3377743 -4.3555412 -4.3516493 -4.3179364 -4.2701869 -4.2421632 -4.2590418 -4.3029962 -4.3367686 -4.3494592 -4.3537874 -4.3541431 -4.3514705][-4.3104677 -4.3255181 -4.3411489 -4.3521757 -4.3530855 -4.3356061 -4.3089252 -4.2904682 -4.2957315 -4.3176332 -4.3374677 -4.3472066 -4.3506527 -4.3517637 -4.3513622][-4.3143373 -4.3246632 -4.3310404 -4.3374405 -4.3404274 -4.3330112 -4.3212938 -4.3138518 -4.3159986 -4.3244185 -4.3331747 -4.3388977 -4.34173 -4.3437009 -4.3458433]]...]
INFO - root - 2017-12-05 23:16:50.172748: step 52110, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 73h:34m:42s remains)
INFO - root - 2017-12-05 23:16:59.449719: step 52120, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 70h:56m:45s remains)
INFO - root - 2017-12-05 23:17:08.776341: step 52130, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 73h:44m:06s remains)
INFO - root - 2017-12-05 23:17:18.184143: step 52140, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 74h:39m:08s remains)
INFO - root - 2017-12-05 23:17:27.711992: step 52150, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.947 sec/batch; 73h:43m:59s remains)
INFO - root - 2017-12-05 23:17:37.013099: step 52160, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 68h:59m:27s remains)
INFO - root - 2017-12-05 23:17:46.192188: step 52170, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.865 sec/batch; 67h:19m:36s remains)
INFO - root - 2017-12-05 23:17:55.625270: step 52180, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 75h:50m:42s remains)
INFO - root - 2017-12-05 23:18:04.959788: step 52190, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 74h:35m:30s remains)
INFO - root - 2017-12-05 23:18:14.146346: step 52200, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 72h:28m:05s remains)
2017-12-05 23:18:15.009770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2929783 -4.289331 -4.2925692 -4.2939539 -4.2845092 -4.274034 -4.2652574 -4.2701216 -4.2823977 -4.2877522 -4.2838507 -4.2835445 -4.2872686 -4.2903037 -4.2893176][-4.2671752 -4.261291 -4.2636538 -4.2682819 -4.2603679 -4.2452035 -4.2364688 -4.2390122 -4.256135 -4.2726703 -4.2784595 -4.2833347 -4.2899036 -4.2916641 -4.28722][-4.2364116 -4.2326412 -4.2331619 -4.2386327 -4.2333994 -4.2118764 -4.1958079 -4.1942739 -4.2132735 -4.2387881 -4.2548003 -4.2659307 -4.2775021 -4.2828283 -4.280899][-4.2153549 -4.2138295 -4.2142224 -4.2195697 -4.2190409 -4.1947446 -4.1674819 -4.1579843 -4.1795235 -4.2136989 -4.2386336 -4.255671 -4.268302 -4.2718892 -4.2713604][-4.2186189 -4.2161164 -4.2143846 -4.2199531 -4.2210083 -4.191812 -4.1475306 -4.1277704 -4.153398 -4.2003036 -4.2345095 -4.258913 -4.2761493 -4.2761173 -4.2698278][-4.2408557 -4.23804 -4.2323442 -4.2362833 -4.230166 -4.1851292 -4.1140561 -4.0690756 -4.1080089 -4.1792326 -4.2303162 -4.2610712 -4.2823157 -4.2813535 -4.2705965][-4.2532048 -4.25835 -4.2536016 -4.2522054 -4.2342005 -4.1663861 -4.052176 -3.9675212 -4.0270209 -4.1372724 -4.2067261 -4.2424126 -4.2700038 -4.2778873 -4.2727852][-4.2551785 -4.2741241 -4.2717223 -4.259584 -4.2264686 -4.1372843 -3.9908693 -3.86663 -3.9447777 -4.0927572 -4.1766014 -4.2111425 -4.2362876 -4.251441 -4.2547889][-4.26322 -4.2868013 -4.286325 -4.263639 -4.2205095 -4.128046 -3.9867325 -3.8692527 -3.9413517 -4.090384 -4.1694617 -4.1938777 -4.2075629 -4.2182775 -4.2236605][-4.2783041 -4.2927651 -4.2873993 -4.262012 -4.222785 -4.1475544 -4.0475292 -3.9692812 -4.0178509 -4.1289263 -4.1869478 -4.1976218 -4.2036257 -4.2069983 -4.2085624][-4.2955542 -4.294961 -4.2817388 -4.2600827 -4.2298889 -4.1787748 -4.11942 -4.0768485 -4.1011438 -4.1697588 -4.2104435 -4.2180085 -4.2221174 -4.2248077 -4.2229185][-4.3066716 -4.2968106 -4.2808576 -4.2653904 -4.2455249 -4.2169557 -4.1859412 -4.1655164 -4.1751471 -4.2130332 -4.2380309 -4.2415638 -4.2441411 -4.2500019 -4.249383][-4.3091297 -4.2971311 -4.2860217 -4.2789717 -4.2727442 -4.2592812 -4.2405872 -4.2293835 -4.2310953 -4.2518978 -4.2656522 -4.2647886 -4.2633266 -4.2666287 -4.2681952][-4.3116927 -4.2997184 -4.2931705 -4.2943277 -4.295351 -4.2882586 -4.2779384 -4.2716084 -4.2687778 -4.2762303 -4.2853847 -4.2824564 -4.2788777 -4.2807393 -4.2824063][-4.3181839 -4.3082604 -4.3045864 -4.3103275 -4.3146358 -4.311296 -4.3068404 -4.30239 -4.2970858 -4.2977896 -4.3027225 -4.3011861 -4.299026 -4.3000593 -4.3019552]]...]
INFO - root - 2017-12-05 23:18:24.539579: step 52210, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.027 sec/batch; 79h:56m:11s remains)
INFO - root - 2017-12-05 23:18:33.753520: step 52220, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 70h:42m:11s remains)
INFO - root - 2017-12-05 23:18:42.959908: step 52230, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 72h:43m:14s remains)
INFO - root - 2017-12-05 23:18:52.522614: step 52240, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 75h:24m:53s remains)
INFO - root - 2017-12-05 23:19:01.857629: step 52250, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 68h:08m:44s remains)
INFO - root - 2017-12-05 23:19:11.293369: step 52260, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 74h:16m:24s remains)
INFO - root - 2017-12-05 23:19:20.692088: step 52270, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 73h:13m:39s remains)
INFO - root - 2017-12-05 23:19:29.840886: step 52280, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 75h:37m:33s remains)
INFO - root - 2017-12-05 23:19:39.215625: step 52290, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 70h:24m:16s remains)
INFO - root - 2017-12-05 23:19:48.630936: step 52300, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 75h:38m:07s remains)
2017-12-05 23:19:49.416986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2689319 -4.2729592 -4.2705665 -4.2556782 -4.2293611 -4.2025208 -4.1701751 -4.1414123 -4.1211376 -4.1188788 -4.1337361 -4.1510239 -4.167305 -4.1757326 -4.1673265][-4.2409215 -4.2443976 -4.2422094 -4.221158 -4.1839681 -4.1442118 -4.1030903 -4.0736351 -4.0562668 -4.0583158 -4.0771689 -4.0971532 -4.1174603 -4.13179 -4.1249428][-4.2231417 -4.2231188 -4.2182393 -4.187778 -4.1393919 -4.088212 -4.0426879 -4.0172234 -4.0043106 -4.0083365 -4.0243506 -4.0464263 -4.069943 -4.0868449 -4.0775471][-4.202745 -4.2001724 -4.1859946 -4.1437216 -4.0858235 -4.0259376 -3.9800644 -3.9640737 -3.9631054 -3.9715066 -3.988445 -4.0122924 -4.035995 -4.05325 -4.0437818][-4.1684628 -4.1655254 -4.1406608 -4.0825882 -4.0187283 -3.9589529 -3.9206333 -3.9157851 -3.9332781 -3.9547806 -3.9756536 -4.0026674 -4.0314569 -4.0448208 -4.0338225][-4.1355877 -4.1435604 -4.1122689 -4.0452 -3.9794648 -3.9202676 -3.8769693 -3.8632476 -3.898026 -3.942266 -3.9768267 -4.0138459 -4.0491657 -4.0581865 -4.0417085][-4.1331234 -4.1405244 -4.1060896 -4.0376325 -3.9716294 -3.903295 -3.8373861 -3.7967935 -3.845659 -3.9208622 -3.9758208 -4.0257921 -4.0678973 -4.0768633 -4.0603514][-4.137341 -4.1385765 -4.1092353 -4.0553193 -4.0043406 -3.9427941 -3.8765035 -3.8361106 -3.8850272 -3.9602671 -4.0119591 -4.0563579 -4.0948677 -4.1050115 -4.091857][-4.1628528 -4.1592326 -4.1325307 -4.0908842 -4.0556107 -4.0078931 -3.9617922 -3.9458368 -3.9865732 -4.0399995 -4.0727053 -4.0986 -4.1221156 -4.1249838 -4.114109][-4.2098975 -4.2042847 -4.1836891 -4.1524048 -4.1215038 -4.0773454 -4.045629 -4.0412865 -4.0726271 -4.1116533 -4.1335206 -4.1511192 -4.165688 -4.1595612 -4.1440873][-4.26044 -4.25232 -4.2370434 -4.2183442 -4.1954317 -4.1620421 -4.1362391 -4.129045 -4.151238 -4.1825309 -4.1979837 -4.2101722 -4.2197361 -4.2107735 -4.1920381][-4.2931614 -4.2857881 -4.2747278 -4.2624874 -4.2476358 -4.2266707 -4.2111111 -4.2043467 -4.2139788 -4.2348528 -4.2465568 -4.2534585 -4.2629652 -4.2585974 -4.2442651][-4.2995028 -4.2937961 -4.2880034 -4.2814693 -4.2755713 -4.2651939 -4.259964 -4.2586226 -4.2617931 -4.2736998 -4.2797608 -4.2837281 -4.2928834 -4.2934971 -4.2820449][-4.2930441 -4.2881403 -4.2813616 -4.277019 -4.2747555 -4.2727876 -4.2765369 -4.2819018 -4.2863903 -4.2940221 -4.2976608 -4.299552 -4.3032694 -4.3025737 -4.2948642][-4.2939653 -4.2897387 -4.2830634 -4.2797856 -4.2787509 -4.2795382 -4.2843127 -4.2897692 -4.293643 -4.29726 -4.2987413 -4.2992792 -4.3005075 -4.299952 -4.29756]]...]
INFO - root - 2017-12-05 23:19:59.038119: step 52310, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 74h:44m:25s remains)
INFO - root - 2017-12-05 23:20:08.119962: step 52320, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.970 sec/batch; 75h:27m:33s remains)
INFO - root - 2017-12-05 23:20:17.630352: step 52330, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 76h:30m:44s remains)
INFO - root - 2017-12-05 23:20:27.013003: step 52340, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 76h:13m:18s remains)
INFO - root - 2017-12-05 23:20:36.099641: step 52350, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 73h:58m:47s remains)
INFO - root - 2017-12-05 23:20:45.375820: step 52360, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 70h:47m:05s remains)
INFO - root - 2017-12-05 23:20:54.794354: step 52370, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 70h:19m:00s remains)
INFO - root - 2017-12-05 23:21:04.211525: step 52380, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 73h:53m:39s remains)
INFO - root - 2017-12-05 23:21:13.596122: step 52390, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.943 sec/batch; 73h:21m:29s remains)
INFO - root - 2017-12-05 23:21:23.170368: step 52400, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 70h:47m:53s remains)
2017-12-05 23:21:23.973773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.283514 -4.2860446 -4.2862468 -4.277843 -4.2745829 -4.2772288 -4.2819533 -4.2814636 -4.2785316 -4.2712388 -4.2650905 -4.2654405 -4.2582116 -4.2357278 -4.1981368][-4.2815833 -4.2791953 -4.2718821 -4.2531838 -4.2415466 -4.2456064 -4.2580113 -4.2644339 -4.2667441 -4.26409 -4.258265 -4.2584381 -4.2509508 -4.2293048 -4.1943474][-4.2819333 -4.2770796 -4.2650061 -4.2353272 -4.2099948 -4.20947 -4.2271938 -4.242116 -4.2516623 -4.2548618 -4.2538943 -4.2567282 -4.250793 -4.2303438 -4.1983442][-4.2725954 -4.2616687 -4.24504 -4.209497 -4.1724181 -4.1651087 -4.1880684 -4.217628 -4.2379718 -4.2492709 -4.255455 -4.2653818 -4.2648077 -4.2482276 -4.2166009][-4.2656965 -4.246798 -4.2216692 -4.1808853 -4.1386871 -4.1289945 -4.1588607 -4.200099 -4.2293491 -4.2451167 -4.2545295 -4.269134 -4.2775197 -4.2717538 -4.2453947][-4.2601213 -4.2376952 -4.2048416 -4.1591992 -4.11441 -4.1061511 -4.1426959 -4.188427 -4.2181373 -4.2304635 -4.2373381 -4.2523694 -4.2671404 -4.2728415 -4.257287][-4.2577319 -4.231812 -4.1927762 -4.141592 -4.0933232 -4.0878062 -4.1332364 -4.1809196 -4.2040362 -4.2050343 -4.2038822 -4.2183619 -4.2378969 -4.2518911 -4.2494683][-4.2645621 -4.2405643 -4.2050018 -4.1556377 -4.1063824 -4.1017075 -4.14748 -4.1931911 -4.2035074 -4.1892681 -4.1748738 -4.1852679 -4.2091079 -4.2304387 -4.2408595][-4.2816248 -4.2656355 -4.2366562 -4.1959715 -4.1559496 -4.1523681 -4.1885843 -4.2254376 -4.2256479 -4.1998491 -4.1718659 -4.1750484 -4.2011304 -4.2255797 -4.2409439][-4.2908525 -4.2860827 -4.2658596 -4.2365961 -4.21514 -4.2181726 -4.2422166 -4.2652068 -4.2592111 -4.2310929 -4.1981711 -4.1909719 -4.2120423 -4.2329903 -4.2445226][-4.2949543 -4.2980213 -4.2848668 -4.2629876 -4.2554502 -4.2656884 -4.2846851 -4.3028383 -4.2993789 -4.2763963 -4.2477865 -4.2347441 -4.2442269 -4.2532525 -4.2564292][-4.2956963 -4.3036294 -4.290904 -4.2684169 -4.265274 -4.2798028 -4.2992964 -4.3198409 -4.3237133 -4.3119545 -4.2928476 -4.2795734 -4.2767773 -4.272892 -4.2664938][-4.2974334 -4.3056269 -4.2897263 -4.2657919 -4.264039 -4.280972 -4.3016553 -4.3214068 -4.3272343 -4.3211069 -4.3112774 -4.2995706 -4.289083 -4.2789173 -4.2645574][-4.3038607 -4.3112183 -4.2951956 -4.274828 -4.2728133 -4.2866368 -4.30412 -4.3178425 -4.3209114 -4.3164492 -4.3124709 -4.303534 -4.2923846 -4.2799869 -4.2614212][-4.3121452 -4.3171449 -4.3035936 -4.2897444 -4.2891607 -4.2993784 -4.3108807 -4.3168578 -4.3161664 -4.3134255 -4.3126354 -4.3074265 -4.3004179 -4.2911859 -4.2730317]]...]
INFO - root - 2017-12-05 23:21:33.244113: step 52410, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.958 sec/batch; 74h:32m:55s remains)
INFO - root - 2017-12-05 23:21:42.646686: step 52420, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 73h:24m:37s remains)
INFO - root - 2017-12-05 23:21:51.920673: step 52430, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 74h:26m:25s remains)
INFO - root - 2017-12-05 23:22:01.528691: step 52440, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 73h:21m:55s remains)
INFO - root - 2017-12-05 23:22:10.856416: step 52450, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 76h:30m:29s remains)
INFO - root - 2017-12-05 23:22:20.112338: step 52460, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 68h:02m:52s remains)
INFO - root - 2017-12-05 23:22:29.605918: step 52470, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 74h:10m:53s remains)
INFO - root - 2017-12-05 23:22:39.133586: step 52480, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 74h:11m:59s remains)
INFO - root - 2017-12-05 23:22:48.337116: step 52490, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 70h:54m:58s remains)
INFO - root - 2017-12-05 23:22:57.711833: step 52500, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 72h:11m:14s remains)
2017-12-05 23:22:58.501761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3236089 -4.3135309 -4.3098183 -4.3118935 -4.31904 -4.3249574 -4.3242559 -4.3144259 -4.3025417 -4.2870121 -4.2722664 -4.26109 -4.256412 -4.2628651 -4.2781663][-4.3238735 -4.3134542 -4.309504 -4.3106441 -4.3168349 -4.3226705 -4.3198228 -4.3050957 -4.2863832 -4.2649045 -4.2442937 -4.2302523 -4.2272258 -4.2361674 -4.2531042][-4.3274493 -4.3224778 -4.3205276 -4.3214512 -4.3235712 -4.3216147 -4.3105869 -4.2888403 -4.2655325 -4.2440715 -4.2226181 -4.2111573 -4.2128758 -4.2235408 -4.2409592][-4.3291802 -4.33036 -4.3317037 -4.3318357 -4.3269529 -4.3150282 -4.294333 -4.2668195 -4.2429247 -4.2228732 -4.2030196 -4.1978755 -4.2081485 -4.2220349 -4.2377138][-4.3253126 -4.3315043 -4.3361144 -4.3333731 -4.3221807 -4.3032503 -4.275619 -4.2434278 -4.2190962 -4.1991606 -4.18267 -4.1872215 -4.2071733 -4.2217627 -4.2330575][-4.3143663 -4.323514 -4.3300209 -4.3257761 -4.3100548 -4.2854762 -4.2526183 -4.2176867 -4.1936769 -4.1762943 -4.1657844 -4.1815634 -4.210434 -4.2247639 -4.2338734][-4.2932863 -4.3032627 -4.3106227 -4.306098 -4.2867494 -4.2586889 -4.223309 -4.1907907 -4.1712461 -4.1602173 -4.15892 -4.1869516 -4.2218032 -4.2357063 -4.243371][-4.2680621 -4.2759695 -4.2818971 -4.2758741 -4.2538781 -4.223702 -4.1893291 -4.1622758 -4.1490011 -4.1404815 -4.1460514 -4.1819034 -4.2187409 -4.2302351 -4.2381029][-4.2498031 -4.2501059 -4.2500482 -4.2396154 -4.2145514 -4.1817102 -4.1484628 -4.1244454 -4.110321 -4.0968485 -4.1045136 -4.1439824 -4.1793895 -4.1895962 -4.2006574][-4.2504315 -4.2416768 -4.2339091 -4.2171912 -4.1879892 -4.1511993 -4.1152225 -4.0888934 -4.0686722 -4.044961 -4.04845 -4.0893 -4.1229792 -4.1323605 -4.1472707][-4.2696686 -4.2570348 -4.2467794 -4.2266774 -4.1954446 -4.15835 -4.123003 -4.0946407 -4.0693793 -4.0425663 -4.0457973 -4.0813084 -4.1075487 -4.1148677 -4.1296058][-4.2949286 -4.2816095 -4.2741022 -4.2557144 -4.22686 -4.1930943 -4.1601505 -4.13394 -4.1117787 -4.0930753 -4.1007066 -4.1281714 -4.1447244 -4.1502066 -4.1646204][-4.3082037 -4.2969427 -4.2924767 -4.2779684 -4.2560916 -4.229641 -4.2023449 -4.1847429 -4.1731696 -4.1684608 -4.1811585 -4.2004113 -4.2068257 -4.2062559 -4.2159619][-4.3020058 -4.2928872 -4.2900476 -4.2815113 -4.2689567 -4.2549582 -4.2387209 -4.2334914 -4.2352157 -4.241962 -4.2549987 -4.2682581 -4.2685833 -4.2611942 -4.2595954][-4.2786188 -4.2682719 -4.2675624 -4.26902 -4.2675052 -4.2655678 -4.2605357 -4.26612 -4.2750549 -4.2847 -4.2929864 -4.3001814 -4.2989211 -4.2873669 -4.277276]]...]
INFO - root - 2017-12-05 23:23:07.967714: step 52510, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 75h:05m:20s remains)
INFO - root - 2017-12-05 23:23:17.492525: step 52520, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 73h:32m:00s remains)
INFO - root - 2017-12-05 23:23:26.881391: step 52530, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 68h:30m:26s remains)
INFO - root - 2017-12-05 23:23:36.087938: step 52540, loss = 2.03, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 71h:45m:02s remains)
INFO - root - 2017-12-05 23:23:45.489358: step 52550, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.031 sec/batch; 80h:10m:23s remains)
INFO - root - 2017-12-05 23:23:54.870218: step 52560, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.973 sec/batch; 75h:37m:53s remains)
INFO - root - 2017-12-05 23:24:04.441974: step 52570, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 72h:52m:53s remains)
INFO - root - 2017-12-05 23:24:13.864294: step 52580, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 73h:47m:33s remains)
INFO - root - 2017-12-05 23:24:23.015419: step 52590, loss = 2.10, batch loss = 2.05 (8.4 examples/sec; 0.950 sec/batch; 73h:53m:01s remains)
INFO - root - 2017-12-05 23:24:32.412868: step 52600, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 69h:03m:34s remains)
2017-12-05 23:24:33.169604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2892408 -4.2936368 -4.2920671 -4.2798386 -4.2668939 -4.262567 -4.2645535 -4.2659221 -4.2663617 -4.2649088 -4.2591558 -4.2660551 -4.289897 -4.3119135 -4.3202915][-4.2852259 -4.2880907 -4.2850442 -4.27227 -4.2607994 -4.2591782 -4.2631464 -4.2672758 -4.2705226 -4.2705865 -4.2637181 -4.2678461 -4.2880387 -4.3062897 -4.3118563][-4.2832341 -4.2850838 -4.2813444 -4.2695665 -4.2615752 -4.2614336 -4.2651911 -4.2697835 -4.2737441 -4.2736883 -4.266439 -4.268682 -4.2841311 -4.2970672 -4.2996712][-4.2802835 -4.2833772 -4.2808313 -4.272027 -4.2672162 -4.2658916 -4.2650805 -4.2658663 -4.2694139 -4.2715549 -4.2677832 -4.2707486 -4.2823811 -4.2899003 -4.2900572][-4.2774539 -4.2844381 -4.2847428 -4.2779293 -4.27154 -4.2644582 -4.2524209 -4.2449241 -4.24914 -4.2585459 -4.2639008 -4.2714682 -4.2827477 -4.2875857 -4.2852464][-4.2760568 -4.2884874 -4.2915273 -4.2834277 -4.2701092 -4.2519827 -4.225841 -4.2108731 -4.2172756 -4.2364922 -4.2532525 -4.2671895 -4.2816625 -4.2860503 -4.2823544][-4.2744927 -4.2918129 -4.2986369 -4.2888374 -4.2660394 -4.233849 -4.1966648 -4.1820216 -4.1943655 -4.2202978 -4.2432489 -4.2613378 -4.2778511 -4.2830572 -4.2794933][-4.2709575 -4.2893605 -4.2988124 -4.290422 -4.2625051 -4.222394 -4.1825833 -4.1709471 -4.1836381 -4.2084446 -4.2313418 -4.2511716 -4.2697215 -4.2770581 -4.2758579][-4.2691779 -4.2850232 -4.2936029 -4.2872047 -4.2606821 -4.22458 -4.1906 -4.1793337 -4.1836429 -4.1983285 -4.2139645 -4.2343926 -4.2582726 -4.2699494 -4.2714734][-4.2756815 -4.287425 -4.2929258 -4.2887158 -4.2677135 -4.2406096 -4.2153935 -4.2045417 -4.1970448 -4.1961861 -4.1984673 -4.2162704 -4.2451253 -4.2631321 -4.2679791][-4.2869468 -4.2962732 -4.2971845 -4.2927408 -4.276268 -4.2573528 -4.2411423 -4.2326379 -4.218236 -4.2039666 -4.1934576 -4.2045183 -4.2346911 -4.258018 -4.2665868][-4.2944493 -4.3023109 -4.2987614 -4.2904944 -4.2753215 -4.2605653 -4.2519255 -4.2481136 -4.2330184 -4.2113557 -4.1914968 -4.1985064 -4.230679 -4.2589378 -4.2699418][-4.299562 -4.3041048 -4.2955675 -4.2815604 -4.2645731 -4.2528954 -4.2502146 -4.250423 -4.236342 -4.2141695 -4.1943426 -4.2036238 -4.2382936 -4.26701 -4.27728][-4.3040695 -4.3042216 -4.2905126 -4.2712173 -4.2542171 -4.246635 -4.2479138 -4.2495956 -4.2377439 -4.2190404 -4.2037144 -4.2151523 -4.2480993 -4.2749958 -4.2828994][-4.3031263 -4.3018856 -4.2864189 -4.2636566 -4.24563 -4.2398286 -4.2447262 -4.2484131 -4.2382345 -4.2223682 -4.2103949 -4.2207241 -4.2478714 -4.2708149 -4.2765527]]...]
INFO - root - 2017-12-05 23:24:42.829441: step 52610, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 76h:48m:08s remains)
INFO - root - 2017-12-05 23:24:52.154473: step 52620, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 75h:26m:39s remains)
INFO - root - 2017-12-05 23:25:01.734084: step 52630, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 74h:12m:56s remains)
INFO - root - 2017-12-05 23:25:11.158206: step 52640, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 69h:17m:33s remains)
INFO - root - 2017-12-05 23:25:20.385583: step 52650, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 72h:11m:55s remains)
INFO - root - 2017-12-05 23:25:29.891327: step 52660, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 73h:57m:45s remains)
INFO - root - 2017-12-05 23:25:39.420841: step 52670, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.961 sec/batch; 74h:43m:38s remains)
INFO - root - 2017-12-05 23:25:48.641145: step 52680, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 72h:20m:06s remains)
INFO - root - 2017-12-05 23:25:57.980555: step 52690, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.988 sec/batch; 76h:48m:35s remains)
INFO - root - 2017-12-05 23:26:07.609471: step 52700, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 72h:54m:21s remains)
2017-12-05 23:26:08.375124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2206974 -4.2167406 -4.2052526 -4.2098293 -4.2272391 -4.2386141 -4.2470584 -4.258503 -4.2631149 -4.2571464 -4.2526026 -4.2659035 -4.2826281 -4.2910018 -4.2947249][-4.1940255 -4.1791234 -4.1559663 -4.1626329 -4.1929173 -4.2096596 -4.2212768 -4.2453589 -4.2605762 -4.2571936 -4.2517366 -4.2595382 -4.2688332 -4.2739911 -4.2781715][-4.1560087 -4.1329603 -4.1014814 -4.1156912 -4.1605296 -4.1835175 -4.1983438 -4.2275844 -4.2457538 -4.2440405 -4.243608 -4.2500591 -4.2537293 -4.2581639 -4.2632904][-4.115118 -4.0937004 -4.0674653 -4.0956664 -4.15049 -4.1725912 -4.1881318 -4.2154822 -4.2343521 -4.2337427 -4.2401524 -4.2515616 -4.2531142 -4.26076 -4.2660689][-4.0797524 -4.0702233 -4.0613837 -4.0971313 -4.1438522 -4.1506958 -4.1537833 -4.175364 -4.1983633 -4.2015305 -4.2193928 -4.2423077 -4.2540464 -4.2678304 -4.2739434][-4.0540915 -4.0497975 -4.0517449 -4.0813918 -4.1079316 -4.089323 -4.0669146 -4.0859928 -4.1233196 -4.1444178 -4.1775832 -4.2132282 -4.2392 -4.2622991 -4.2751994][-4.0425138 -4.0353613 -4.0345244 -4.0479403 -4.0521979 -4.0038452 -3.9453831 -3.9562161 -4.0159931 -4.06848 -4.1211367 -4.1721277 -4.2067175 -4.2344136 -4.2562671][-4.0542178 -4.0320959 -4.0186415 -4.0116405 -3.9953701 -3.9264097 -3.8434646 -3.8437767 -3.9196184 -3.9941158 -4.0601053 -4.1280828 -4.1726322 -4.2049575 -4.2329893][-4.0965729 -4.0634441 -4.035326 -4.0100274 -3.98234 -3.9151657 -3.8393068 -3.8394203 -3.90864 -3.9777627 -4.0415931 -4.1132994 -4.158792 -4.1907392 -4.2202973][-4.154521 -4.1239438 -4.0927777 -4.0609751 -4.0313568 -3.9779265 -3.9278064 -3.9375045 -3.9871554 -4.0302873 -4.0744767 -4.1306133 -4.1722212 -4.2045474 -4.2312469][-4.1976156 -4.1819143 -4.1605787 -4.1351676 -4.1086459 -4.0658188 -4.0383391 -4.0529919 -4.0816 -4.10157 -4.1299391 -4.1744447 -4.2093649 -4.239244 -4.2612276][-4.2383881 -4.2379532 -4.2292013 -4.2117653 -4.1901374 -4.1603341 -4.1454368 -4.1576896 -4.1694927 -4.1705937 -4.1855392 -4.2179723 -4.2419448 -4.2664289 -4.285737][-4.2842956 -4.2861652 -4.2775 -4.2624159 -4.2434745 -4.221828 -4.2125568 -4.2162118 -4.2182651 -4.2127066 -4.2224417 -4.2418447 -4.2575407 -4.2791834 -4.2985353][-4.3130579 -4.3121133 -4.3023067 -4.287683 -4.2706347 -4.2563567 -4.250813 -4.2479682 -4.2441587 -4.2395973 -4.2458258 -4.25655 -4.2672868 -4.2847934 -4.3024435][-4.3305693 -4.3288646 -4.3213534 -4.3098903 -4.2972512 -4.2882466 -4.284596 -4.2793059 -4.271708 -4.2658834 -4.2673097 -4.2740011 -4.2817721 -4.2928529 -4.3061004]]...]
INFO - root - 2017-12-05 23:26:17.721566: step 52710, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.944 sec/batch; 73h:20m:57s remains)
INFO - root - 2017-12-05 23:26:27.054116: step 52720, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.988 sec/batch; 76h:45m:56s remains)
INFO - root - 2017-12-05 23:26:36.512505: step 52730, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.959 sec/batch; 74h:33m:41s remains)
INFO - root - 2017-12-05 23:26:45.853900: step 52740, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 74h:01m:45s remains)
INFO - root - 2017-12-05 23:26:55.177606: step 52750, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 74h:56m:36s remains)
INFO - root - 2017-12-05 23:27:04.599851: step 52760, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 74h:56m:54s remains)
INFO - root - 2017-12-05 23:27:13.814399: step 52770, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 66h:20m:30s remains)
INFO - root - 2017-12-05 23:27:23.081054: step 52780, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 72h:27m:33s remains)
INFO - root - 2017-12-05 23:27:32.338965: step 52790, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 67h:08m:40s remains)
INFO - root - 2017-12-05 23:27:41.759294: step 52800, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 71h:26m:44s remains)
2017-12-05 23:27:42.551847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3490868 -4.3389931 -4.3246903 -4.3119006 -4.3031297 -4.2941833 -4.2826295 -4.2859306 -4.2993937 -4.3137207 -4.3201928 -4.3243041 -4.3226409 -4.3181458 -4.3163285][-4.3512182 -4.33737 -4.3185182 -4.3057227 -4.2994337 -4.2901254 -4.2717071 -4.2725267 -4.2923036 -4.3151107 -4.3252964 -4.3290858 -4.3222013 -4.3133373 -4.3071661][-4.3477864 -4.3287268 -4.3042645 -4.2874546 -4.2790504 -4.2699823 -4.2513618 -4.2520003 -4.2762289 -4.306777 -4.3205338 -4.3224778 -4.3086214 -4.2931962 -4.2830272][-4.34197 -4.3182225 -4.2890029 -4.2678661 -4.2548027 -4.2437 -4.2237053 -4.2199802 -4.2454863 -4.2821779 -4.2994576 -4.2989078 -4.27974 -4.2609863 -4.2483273][-4.3383994 -4.3126936 -4.2803378 -4.2547922 -4.2370586 -4.2202826 -4.1888838 -4.1720591 -4.1945467 -4.2392535 -4.2650452 -4.2686515 -4.2513838 -4.2330055 -4.2235327][-4.3352652 -4.3087034 -4.2741876 -4.2425489 -4.2146606 -4.1857295 -4.137085 -4.0990791 -4.1147366 -4.17841 -4.225513 -4.2391372 -4.2274904 -4.213923 -4.21098][-4.3312321 -4.3048096 -4.2686973 -4.2306986 -4.187726 -4.1360812 -4.0596738 -3.9817431 -3.9839377 -4.0825052 -4.1625633 -4.1932483 -4.1898408 -4.1790986 -4.1772809][-4.3268561 -4.3013759 -4.2655239 -4.2257056 -4.1738863 -4.1048632 -4.0029149 -3.8785007 -3.8519766 -3.9759965 -4.0881028 -4.1398134 -4.1483126 -4.1400166 -4.1352558][-4.3203106 -4.2941608 -4.2573004 -4.2206964 -4.1759939 -4.1184483 -4.0279322 -3.9087436 -3.8683224 -3.9626067 -4.0589633 -4.1058426 -4.1168895 -4.1108336 -4.0995083][-4.31852 -4.2947168 -4.2584538 -4.224606 -4.1870241 -4.1455793 -4.0856237 -4.0104065 -3.9811294 -4.0358109 -4.1001511 -4.1333704 -4.1401167 -4.1304922 -4.1069884][-4.3218384 -4.3046231 -4.2769747 -4.2499728 -4.2186155 -4.1866565 -4.1474409 -4.100749 -4.0767641 -4.1038723 -4.1480803 -4.177598 -4.1890979 -4.1811471 -4.1508565][-4.3247747 -4.3138471 -4.2974286 -4.2812839 -4.2596631 -4.2367024 -4.2088041 -4.1750336 -4.1486626 -4.1533527 -4.1823411 -4.2102847 -4.2256842 -4.2193284 -4.1880622][-4.3291316 -4.322958 -4.3151064 -4.3081503 -4.2966251 -4.2825675 -4.263073 -4.2378936 -4.2126603 -4.2069082 -4.2247734 -4.2495227 -4.2625504 -4.2550569 -4.2275376][-4.3343015 -4.3320818 -4.3300228 -4.3279243 -4.3235531 -4.3176074 -4.3046875 -4.2851796 -4.2645907 -4.2594891 -4.2717113 -4.2902608 -4.2993956 -4.2931323 -4.2740412][-4.3339553 -4.3325238 -4.3321629 -4.3314319 -4.3302531 -4.3273673 -4.319283 -4.3072581 -4.2931728 -4.28777 -4.2946353 -4.3068814 -4.3141165 -4.3125958 -4.3022141]]...]
INFO - root - 2017-12-05 23:27:51.991533: step 52810, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 71h:50m:43s remains)
INFO - root - 2017-12-05 23:28:01.546076: step 52820, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 74h:18m:00s remains)
INFO - root - 2017-12-05 23:28:10.959773: step 52830, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 71h:25m:57s remains)
INFO - root - 2017-12-05 23:28:20.214901: step 52840, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 74h:14m:49s remains)
INFO - root - 2017-12-05 23:28:29.600226: step 52850, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 72h:35m:53s remains)
INFO - root - 2017-12-05 23:28:38.916288: step 52860, loss = 2.03, batch loss = 1.98 (8.8 examples/sec; 0.906 sec/batch; 70h:23m:22s remains)
INFO - root - 2017-12-05 23:28:48.244983: step 52870, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 72h:42m:43s remains)
INFO - root - 2017-12-05 23:28:57.724868: step 52880, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 72h:20m:02s remains)
INFO - root - 2017-12-05 23:29:07.206922: step 52890, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 72h:59m:45s remains)
INFO - root - 2017-12-05 23:29:16.662305: step 52900, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 70h:00m:33s remains)
2017-12-05 23:29:17.414432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.357779 -4.3489871 -4.3193922 -4.2781405 -4.2420459 -4.2199788 -4.2103662 -4.2147775 -4.2254043 -4.2276297 -4.2197547 -4.2107081 -4.2100906 -4.2132325 -4.2205663][-4.3614616 -4.3495069 -4.311245 -4.2629614 -4.2246003 -4.2022867 -4.1909819 -4.1959577 -4.2077694 -4.2131357 -4.2125688 -4.2060862 -4.2022681 -4.2017622 -4.2068615][-4.3603292 -4.344202 -4.299264 -4.2497005 -4.2148104 -4.1960754 -4.1841969 -4.1878943 -4.198791 -4.2045012 -4.2124305 -4.2151666 -4.2137685 -4.2128053 -4.2187033][-4.3546486 -4.33126 -4.2803574 -4.2291918 -4.1955051 -4.1795144 -4.1653862 -4.1667523 -4.1746106 -4.1823773 -4.2018347 -4.2178831 -4.2233739 -4.2262611 -4.2374272][-4.3453903 -4.3134575 -4.2571368 -4.2035222 -4.168 -4.1509843 -4.1330581 -4.1300511 -4.1331768 -4.1433506 -4.1753373 -4.2081547 -4.2239914 -4.230855 -4.2456846][-4.3366818 -4.2982063 -4.2383618 -4.1818962 -4.1441679 -4.1209841 -4.0967183 -4.0870581 -4.0837603 -4.0928626 -4.1341748 -4.1816955 -4.2078958 -4.2189808 -4.23549][-4.3287992 -4.2857337 -4.2249789 -4.1674976 -4.1271553 -4.098002 -4.0740395 -4.0665073 -4.0595188 -4.0624762 -4.1042805 -4.1606784 -4.1940222 -4.2079072 -4.2222991][-4.3255062 -4.2838058 -4.228847 -4.1807494 -4.14733 -4.121593 -4.1048117 -4.0999608 -4.0896454 -4.0839019 -4.1158547 -4.1702108 -4.2032065 -4.2148776 -4.2233944][-4.3269506 -4.2921777 -4.2477665 -4.2127552 -4.1908875 -4.1730318 -4.1656079 -4.1599731 -4.147048 -4.1364956 -4.1587029 -4.2032051 -4.2303357 -4.2364607 -4.2372003][-4.3277988 -4.2989378 -4.26225 -4.233902 -4.216557 -4.2012577 -4.1952982 -4.1879058 -4.176198 -4.1653743 -4.1860404 -4.2270541 -4.2532315 -4.260077 -4.2584023][-4.3270941 -4.3014669 -4.2691774 -4.2444706 -4.2317333 -4.2199054 -4.20985 -4.1982622 -4.1864176 -4.1780715 -4.199357 -4.2404156 -4.2691321 -4.2798676 -4.2782164][-4.3262329 -4.3020458 -4.2710743 -4.2468772 -4.2377639 -4.2306929 -4.2188349 -4.2076912 -4.1979389 -4.1952281 -4.216536 -4.2526383 -4.2816916 -4.2962155 -4.2971115][-4.3247147 -4.300405 -4.2696295 -4.2446151 -4.2363672 -4.2337728 -4.2278728 -4.223875 -4.2208843 -4.2233829 -4.2411351 -4.2699547 -4.2961235 -4.3106146 -4.3129106][-4.3235579 -4.2985454 -4.2692537 -4.243835 -4.2355089 -4.2356262 -4.2384543 -4.2405615 -4.2400866 -4.2435517 -4.2582378 -4.2809215 -4.3022456 -4.3150678 -4.3183112][-4.3265195 -4.3009505 -4.2719884 -4.2442245 -4.233007 -4.2346087 -4.2426658 -4.2485123 -4.2492418 -4.2535 -4.2666855 -4.2841 -4.3004913 -4.3112369 -4.3149638]]...]
INFO - root - 2017-12-05 23:29:26.812387: step 52910, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 71h:49m:58s remains)
INFO - root - 2017-12-05 23:29:36.236002: step 52920, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 73h:47m:20s remains)
INFO - root - 2017-12-05 23:29:45.569326: step 52930, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 70h:37m:11s remains)
INFO - root - 2017-12-05 23:29:54.765394: step 52940, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 70h:51m:01s remains)
INFO - root - 2017-12-05 23:30:04.155026: step 52950, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 76h:35m:46s remains)
INFO - root - 2017-12-05 23:30:13.479483: step 52960, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 75h:13m:59s remains)
INFO - root - 2017-12-05 23:30:22.760081: step 52970, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.973 sec/batch; 75h:33m:45s remains)
INFO - root - 2017-12-05 23:30:32.076815: step 52980, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 76h:39m:56s remains)
INFO - root - 2017-12-05 23:30:41.492931: step 52990, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 70h:37m:20s remains)
INFO - root - 2017-12-05 23:30:51.076976: step 53000, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 76h:54m:52s remains)
2017-12-05 23:30:51.832303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2140508 -4.1937733 -4.17052 -4.1552196 -4.1336 -4.1100435 -4.1053143 -4.1221318 -4.1583486 -4.1935225 -4.2049351 -4.195085 -4.1756625 -4.1239882 -4.0760789][-4.2000828 -4.1820655 -4.1626344 -4.1551986 -4.139504 -4.1178665 -4.1071873 -4.114882 -4.1457019 -4.1743183 -4.18127 -4.176466 -4.1669531 -4.1200676 -4.0716138][-4.18733 -4.1751575 -4.166029 -4.1647687 -4.1503463 -4.1317511 -4.1168294 -4.11903 -4.1468883 -4.1714582 -4.1743989 -4.17408 -4.1742897 -4.1373668 -4.0955353][-4.1878853 -4.1782441 -4.1744971 -4.17147 -4.1535759 -4.1329832 -4.1185141 -4.1269779 -4.1660972 -4.195817 -4.2016578 -4.2055769 -4.21023 -4.1794891 -4.1410794][-4.1954827 -4.1844983 -4.1825762 -4.1772366 -4.1534672 -4.1236811 -4.1095495 -4.1293745 -4.1833596 -4.2209821 -4.2310176 -4.2389388 -4.241066 -4.2128386 -4.178916][-4.1980395 -4.1833572 -4.1788607 -4.1696596 -4.1432757 -4.1112328 -4.0963573 -4.1211548 -4.182375 -4.2237949 -4.2371044 -4.2457738 -4.2404757 -4.2059569 -4.1638732][-4.1957893 -4.1782827 -4.1671767 -4.1498318 -4.1210628 -4.088995 -4.0765924 -4.1052446 -4.1715565 -4.2150822 -4.2290807 -4.2341166 -4.2158961 -4.1609697 -4.0972166][-4.1970077 -4.17714 -4.1559677 -4.1260352 -4.0894861 -4.0501051 -4.0353584 -4.0705829 -4.1463866 -4.1986303 -4.2207923 -4.2266951 -4.1977935 -4.1229496 -4.0417066][-4.208034 -4.1847243 -4.158145 -4.1187944 -4.0666256 -4.0103464 -3.990756 -4.034893 -4.1206293 -4.182795 -4.2169394 -4.2305689 -4.2016926 -4.1214352 -4.0366869][-4.2239084 -4.1992888 -4.1703248 -4.1256752 -4.0638251 -3.9969597 -3.9756482 -4.0231867 -4.1103253 -4.1759372 -4.2150588 -4.2298613 -4.208344 -4.1378317 -4.0559177][-4.231544 -4.2136259 -4.1861367 -4.1428823 -4.0870066 -4.0267892 -4.00709 -4.0486927 -4.1240888 -4.1819434 -4.2144127 -4.2247725 -4.2113104 -4.1573019 -4.0856552][-4.2354231 -4.2253442 -4.2061834 -4.1746254 -4.1341219 -4.0890784 -4.0742278 -4.1040521 -4.15943 -4.2006083 -4.2240319 -4.2320738 -4.2262759 -4.1905551 -4.1369553][-4.2430248 -4.2394042 -4.228539 -4.2106619 -4.1849751 -4.154613 -4.1425805 -4.1627231 -4.1999283 -4.2245684 -4.2389364 -4.2459192 -4.245996 -4.2269826 -4.1931391][-4.2624536 -4.259675 -4.2529092 -4.2434592 -4.2306991 -4.2125297 -4.2065525 -4.2206416 -4.2423124 -4.2551889 -4.2609105 -4.2626929 -4.2642169 -4.2579551 -4.2379651][-4.2862024 -4.2813673 -4.2754836 -4.2710061 -4.2674174 -4.2590823 -4.2575717 -4.26762 -4.2799606 -4.2840819 -4.2834134 -4.2802181 -4.2811933 -4.2807522 -4.2704558]]...]
INFO - root - 2017-12-05 23:31:01.215117: step 53010, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 72h:21m:24s remains)
INFO - root - 2017-12-05 23:31:10.672293: step 53020, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 73h:30m:59s remains)
INFO - root - 2017-12-05 23:31:19.930463: step 53030, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 68h:30m:28s remains)
INFO - root - 2017-12-05 23:31:29.152271: step 53040, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 76h:39m:36s remains)
INFO - root - 2017-12-05 23:31:38.573018: step 53050, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 75h:58m:10s remains)
INFO - root - 2017-12-05 23:31:47.991844: step 53060, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 71h:38m:38s remains)
INFO - root - 2017-12-05 23:31:57.467316: step 53070, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.028 sec/batch; 79h:45m:41s remains)
INFO - root - 2017-12-05 23:32:06.881936: step 53080, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.982 sec/batch; 76h:10m:56s remains)
INFO - root - 2017-12-05 23:32:16.335810: step 53090, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 73h:42m:21s remains)
INFO - root - 2017-12-05 23:32:25.702664: step 53100, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 77h:53m:00s remains)
2017-12-05 23:32:26.527092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3165703 -4.3060122 -4.2925267 -4.285017 -4.2820945 -4.2786431 -4.2797785 -4.2870293 -4.2913303 -4.2927132 -4.286181 -4.2743192 -4.2636437 -4.2568536 -4.2566485][-4.3149161 -4.3033438 -4.2858877 -4.27082 -4.2598891 -4.2563071 -4.2593942 -4.2703938 -4.2800055 -4.2887082 -4.2819324 -4.2686682 -4.2549405 -4.2405806 -4.2332869][-4.3044538 -4.2890272 -4.2711859 -4.2501287 -4.2262506 -4.2161245 -4.2162662 -4.2274604 -4.2463231 -4.2698507 -4.2700362 -4.2579103 -4.246345 -4.2312312 -4.2185879][-4.3009105 -4.2892022 -4.2748327 -4.2517323 -4.2167969 -4.1981349 -4.1951275 -4.20581 -4.2290254 -4.2618785 -4.2695141 -4.25575 -4.2449222 -4.2327805 -4.2213526][-4.2792287 -4.2710037 -4.2617764 -4.2402058 -4.2015047 -4.1733418 -4.160995 -4.1679611 -4.1958919 -4.2408748 -4.25912 -4.2538133 -4.2477679 -4.2379794 -4.2315044][-4.2400084 -4.2336807 -4.2261615 -4.20627 -4.1633992 -4.1167412 -4.0845971 -4.0804634 -4.1190591 -4.1824131 -4.2198582 -4.2264481 -4.224144 -4.2157307 -4.2163496][-4.1677608 -4.161459 -4.1532793 -4.1306462 -4.0821681 -4.0160384 -3.9589851 -3.9369414 -3.9893825 -4.0811415 -4.1434422 -4.166213 -4.171638 -4.1699076 -4.1790929][-4.0523214 -4.0481262 -4.0394535 -4.0118365 -3.9642329 -3.8914557 -3.8107667 -3.7638762 -3.8253944 -3.9427829 -4.0248575 -4.0642209 -4.0808811 -4.09005 -4.113512][-3.978348 -3.9772444 -3.9752126 -3.9514427 -3.9159365 -3.8598356 -3.7844164 -3.7230818 -3.7576506 -3.8537722 -3.926861 -3.96309 -3.9841309 -4.0087361 -4.0502992][-4.0188289 -4.01954 -4.0250793 -4.0107265 -3.9915407 -3.9622498 -3.9214358 -3.8802657 -3.8828926 -3.9300928 -3.9725165 -3.99269 -4.0044889 -4.0276408 -4.0672035][-4.1088266 -4.1081471 -4.114851 -4.1090937 -4.1018705 -4.0923972 -4.0779452 -4.0569043 -4.0477571 -4.0627756 -4.082118 -4.0917387 -4.0977163 -4.1152406 -4.1446323][-4.1964283 -4.1949954 -4.2003307 -4.1992588 -4.1992407 -4.1992345 -4.19747 -4.1903963 -4.1816125 -4.1819115 -4.1884365 -4.193449 -4.1972833 -4.2087803 -4.2268972][-4.2383108 -4.2367344 -4.2401381 -4.2413931 -4.2450204 -4.2480783 -4.248848 -4.2476068 -4.2427607 -4.2375693 -4.23619 -4.2379065 -4.2410784 -4.2501545 -4.2642512][-4.2603221 -4.2604971 -4.2635856 -4.2648592 -4.267324 -4.2693162 -4.2699404 -4.2694373 -4.2657046 -4.2596121 -4.256268 -4.2568126 -4.2597466 -4.2669482 -4.2783332][-4.2979164 -4.3012357 -4.3045564 -4.3057175 -4.3065033 -4.3067927 -4.3058596 -4.304018 -4.3009739 -4.296865 -4.2945867 -4.2948761 -4.2967215 -4.301023 -4.3092346]]...]
INFO - root - 2017-12-05 23:32:35.887157: step 53110, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 75h:35m:27s remains)
INFO - root - 2017-12-05 23:32:45.300841: step 53120, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 75h:22m:42s remains)
INFO - root - 2017-12-05 23:32:54.605915: step 53130, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.941 sec/batch; 73h:02m:51s remains)
INFO - root - 2017-12-05 23:33:04.242596: step 53140, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 74h:09m:17s remains)
INFO - root - 2017-12-05 23:33:13.576455: step 53150, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 72h:50m:26s remains)
INFO - root - 2017-12-05 23:33:22.601977: step 53160, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 70h:09m:17s remains)
INFO - root - 2017-12-05 23:33:32.021679: step 53170, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 70h:04m:32s remains)
INFO - root - 2017-12-05 23:33:41.463600: step 53180, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 72h:37m:32s remains)
INFO - root - 2017-12-05 23:33:50.826015: step 53190, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 66h:18m:57s remains)
INFO - root - 2017-12-05 23:33:59.993985: step 53200, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 71h:03m:37s remains)
2017-12-05 23:34:00.744817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2989902 -4.2962713 -4.3020968 -4.3055053 -4.308691 -4.3093967 -4.3017879 -4.298821 -4.2980876 -4.288168 -4.2828932 -4.2889266 -4.2978215 -4.3069463 -4.3181081][-4.2799187 -4.2723789 -4.2754993 -4.2770853 -4.2805095 -4.2811389 -4.2687697 -4.2640147 -4.2657189 -4.2533965 -4.2485876 -4.2574291 -4.2692428 -4.2822576 -4.2969074][-4.26458 -4.2516103 -4.248611 -4.2467775 -4.2506366 -4.2517295 -4.235271 -4.2257228 -4.2290587 -4.2221775 -4.2206678 -4.2300477 -4.2428842 -4.2580395 -4.2747726][-4.2594495 -4.2477818 -4.2439132 -4.2386851 -4.238502 -4.2382283 -4.216711 -4.1976113 -4.203795 -4.2076321 -4.2115273 -4.2231016 -4.23593 -4.2502894 -4.2663846][-4.2460833 -4.2378969 -4.2335324 -4.2204142 -4.2095361 -4.2023792 -4.1677723 -4.13114 -4.1386061 -4.15647 -4.1688375 -4.1904044 -4.2136254 -4.2352591 -4.2578959][-4.2237139 -4.2156153 -4.2109814 -4.1873989 -4.1589842 -4.1398883 -4.0828009 -4.0161004 -4.03065 -4.0717373 -4.0940042 -4.1256528 -4.1710734 -4.2084322 -4.2413182][-4.19633 -4.1784492 -4.1704535 -4.1320729 -4.0840478 -4.0551443 -3.9664519 -3.8506312 -3.8809266 -3.9622698 -4.0015373 -4.0496516 -4.1214514 -4.1766639 -4.2224488][-4.1790323 -4.1510797 -4.1360326 -4.076705 -4.0126629 -3.9773993 -3.8578029 -3.6860983 -3.7433496 -3.869858 -3.9190598 -3.9833179 -4.0778494 -4.1474271 -4.20644][-4.1910315 -4.1582079 -4.1377559 -4.0758896 -4.0224833 -3.9990819 -3.8930504 -3.7354176 -3.7911072 -3.9023318 -3.9333901 -3.995873 -4.0866857 -4.1480064 -4.2075768][-4.2250109 -4.1920381 -4.1722479 -4.12661 -4.0997686 -4.0970888 -4.0308423 -3.932929 -3.9636681 -4.0267377 -4.0394096 -4.0872788 -4.1500525 -4.1888676 -4.2347693][-4.2709303 -4.24122 -4.222755 -4.19438 -4.1830578 -4.1882448 -4.1562238 -4.10213 -4.1136007 -4.1454682 -4.1525912 -4.1858177 -4.222126 -4.2421241 -4.2727652][-4.3058386 -4.2853627 -4.2717562 -4.2536473 -4.2463894 -4.25336 -4.2427773 -4.21438 -4.215209 -4.2315536 -4.23802 -4.2614813 -4.2790384 -4.2880721 -4.3065863][-4.322597 -4.3111506 -4.3004522 -4.2883668 -4.2818823 -4.2857304 -4.2837996 -4.2712054 -4.2712765 -4.2822251 -4.2897906 -4.3028765 -4.3108444 -4.3164673 -4.3258753][-4.3316331 -4.323916 -4.3139849 -4.3042288 -4.2972441 -4.2962184 -4.29631 -4.2919455 -4.2928433 -4.3009214 -4.3080883 -4.3173118 -4.3237829 -4.3290749 -4.3344564][-4.3369393 -4.3327885 -4.3251786 -4.316546 -4.3103733 -4.308795 -4.3100338 -4.3098989 -4.31221 -4.3190069 -4.3249445 -4.3305917 -4.3353977 -4.3399382 -4.3426251]]...]
INFO - root - 2017-12-05 23:34:10.144724: step 53210, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 74h:08m:44s remains)
INFO - root - 2017-12-05 23:34:19.571538: step 53220, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 72h:42m:57s remains)
INFO - root - 2017-12-05 23:34:28.835255: step 53230, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 75h:11m:20s remains)
INFO - root - 2017-12-05 23:34:38.184767: step 53240, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 73h:40m:02s remains)
INFO - root - 2017-12-05 23:34:47.530192: step 53250, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 69h:17m:59s remains)
INFO - root - 2017-12-05 23:34:56.933030: step 53260, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 74h:26m:32s remains)
INFO - root - 2017-12-05 23:35:06.225790: step 53270, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 74h:56m:11s remains)
INFO - root - 2017-12-05 23:35:15.608002: step 53280, loss = 2.10, batch loss = 2.05 (8.2 examples/sec; 0.976 sec/batch; 75h:43m:14s remains)
INFO - root - 2017-12-05 23:35:24.908461: step 53290, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 74h:32m:11s remains)
INFO - root - 2017-12-05 23:35:34.544565: step 53300, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 75h:23m:49s remains)
2017-12-05 23:35:35.391737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2011695 -4.1935215 -4.2155018 -4.2309709 -4.2145872 -4.1845932 -4.152648 -4.1330113 -4.1312733 -4.1516995 -4.1914492 -4.2362323 -4.2694697 -4.2864828 -4.2968612][-4.2107005 -4.1998272 -4.2141504 -4.2189989 -4.1927114 -4.1628938 -4.1468396 -4.1404538 -4.1387215 -4.1514015 -4.1889758 -4.2380996 -4.2734156 -4.2871575 -4.2923894][-4.2244554 -4.2159138 -4.2207131 -4.214396 -4.1796451 -4.1493077 -4.1440568 -4.1532617 -4.1590996 -4.1686554 -4.20526 -4.2555094 -4.2909131 -4.2992883 -4.2966142][-4.2421746 -4.2385864 -4.2385488 -4.2238045 -4.1830173 -4.1469631 -4.1386423 -4.1533408 -4.1670003 -4.1801057 -4.2181277 -4.2701616 -4.3044415 -4.3114719 -4.3052039][-4.2512021 -4.2517147 -4.245954 -4.2179928 -4.1639771 -4.1091137 -4.0862923 -4.0998316 -4.1277142 -4.1569843 -4.203311 -4.26102 -4.3004751 -4.3138719 -4.311862][-4.2524481 -4.2542253 -4.2409654 -4.1960382 -4.1180739 -4.0300155 -3.9759884 -3.9856687 -4.0418339 -4.1082344 -4.1738334 -4.239171 -4.2842088 -4.3051043 -4.3108006][-4.254014 -4.25822 -4.245481 -4.1933646 -4.0964684 -3.9747968 -3.8782191 -3.8669665 -3.9469473 -4.0504093 -4.143889 -4.2216091 -4.2720084 -4.2982116 -4.3110161][-4.268393 -4.2724714 -4.267004 -4.2278266 -4.1502862 -4.0469594 -3.9548624 -3.9244509 -3.9787071 -4.0688915 -4.157825 -4.2329831 -4.2794127 -4.3017645 -4.3135409][-4.2831168 -4.2832241 -4.282989 -4.2662683 -4.2225976 -4.1606207 -4.1010256 -4.071825 -4.09151 -4.1432481 -4.2058573 -4.2631841 -4.2976356 -4.3111229 -4.317688][-4.2868862 -4.2824149 -4.28312 -4.2797065 -4.2594175 -4.2267294 -4.1878142 -4.1596112 -4.1622224 -4.1940131 -4.2395821 -4.2801876 -4.3031726 -4.3115139 -4.3178091][-4.2751775 -4.2663693 -4.266891 -4.2668862 -4.2540293 -4.2315907 -4.1989741 -4.1702414 -4.1698494 -4.1973619 -4.2388783 -4.277771 -4.3004169 -4.3099852 -4.3173261][-4.2561731 -4.2427807 -4.2415786 -4.2378983 -4.2199879 -4.1888237 -4.1498585 -4.1215053 -4.1237268 -4.1504178 -4.1946759 -4.2448015 -4.2809868 -4.3010669 -4.3128681][-4.2268357 -4.2107034 -4.2070184 -4.1997695 -4.1783757 -4.1409736 -4.0954218 -4.0675659 -4.0724807 -4.1018877 -4.1455512 -4.2011085 -4.2487755 -4.2798028 -4.2991271][-4.1980662 -4.184773 -4.1807828 -4.1738534 -4.1534338 -4.1181045 -4.0723553 -4.0423727 -4.04604 -4.0737538 -4.1136556 -4.1681504 -4.219243 -4.2574816 -4.2843156][-4.1899939 -4.1818223 -4.1818128 -4.1776981 -4.1615043 -4.135314 -4.0991111 -4.069984 -4.062439 -4.0783648 -4.1114659 -4.1578741 -4.2054996 -4.2470603 -4.2786322]]...]
INFO - root - 2017-12-05 23:35:44.984434: step 53310, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 77h:02m:54s remains)
INFO - root - 2017-12-05 23:35:54.332637: step 53320, loss = 2.12, batch loss = 2.06 (8.3 examples/sec; 0.965 sec/batch; 74h:49m:52s remains)
INFO - root - 2017-12-05 23:36:03.670532: step 53330, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 71h:17m:15s remains)
INFO - root - 2017-12-05 23:36:13.029086: step 53340, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 69h:26m:32s remains)
INFO - root - 2017-12-05 23:36:22.091266: step 53350, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 74h:37m:59s remains)
INFO - root - 2017-12-05 23:36:31.573645: step 53360, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 76h:27m:49s remains)
INFO - root - 2017-12-05 23:36:41.166071: step 53370, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.980 sec/batch; 75h:57m:49s remains)
INFO - root - 2017-12-05 23:36:50.302869: step 53380, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 73h:38m:24s remains)
INFO - root - 2017-12-05 23:36:59.568013: step 53390, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 74h:00m:26s remains)
INFO - root - 2017-12-05 23:37:08.966419: step 53400, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 73h:40m:13s remains)
2017-12-05 23:37:09.801916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1216445 -4.1186886 -4.1404877 -4.1772113 -4.2046475 -4.2093344 -4.2003875 -4.186295 -4.1676493 -4.1439266 -4.1390305 -4.1551795 -4.1812558 -4.19145 -4.1885557][-4.1370511 -4.1276426 -4.1452909 -4.17939 -4.2093081 -4.2130227 -4.2008061 -4.1818705 -4.1564755 -4.1263776 -4.1181507 -4.1361804 -4.1671867 -4.1774888 -4.168551][-4.151032 -4.1310349 -4.1359358 -4.1669807 -4.203258 -4.21365 -4.2026863 -4.1783938 -4.1434579 -4.1056681 -4.0941911 -4.1113577 -4.1418247 -4.1528153 -4.1368251][-4.1394076 -4.1112051 -4.1063352 -4.1353712 -4.1736655 -4.1873612 -4.1729431 -4.1425877 -4.1066523 -4.0803909 -4.0807343 -4.0982962 -4.1218381 -4.1262531 -4.1007495][-4.0984774 -4.0714164 -4.0693436 -4.0964661 -4.13002 -4.1357174 -4.1133618 -4.0808191 -4.0578694 -4.0561595 -4.0740886 -4.0938711 -4.11439 -4.1174197 -4.0954909][-4.0492244 -4.0318518 -4.038631 -4.0633478 -4.0875335 -4.0870686 -4.0627174 -4.0350218 -4.0300937 -4.0477023 -4.0722427 -4.0896611 -4.1081653 -4.1227822 -4.1216197][-4.0525632 -4.0481591 -4.0629811 -4.0842733 -4.0959558 -4.0875669 -4.0585065 -4.0313988 -4.02888 -4.0414062 -4.0567079 -4.0682726 -4.09153 -4.1231952 -4.1423883][-4.1159925 -4.1177931 -4.132719 -4.1477227 -4.14718 -4.1299095 -4.0937176 -4.0597043 -4.0488939 -4.0498362 -4.0492797 -4.0520725 -4.0792418 -4.1177168 -4.1448078][-4.1904044 -4.1928382 -4.2004113 -4.203671 -4.1914368 -4.1677074 -4.1253166 -4.0902472 -4.0752473 -4.0707169 -4.0689158 -4.0729375 -4.0959945 -4.1210604 -4.1353111][-4.2309494 -4.2348027 -4.2403426 -4.2384882 -4.221693 -4.1941333 -4.1463437 -4.1122131 -4.0971851 -4.0981493 -4.1110568 -4.127368 -4.1452732 -4.1525612 -4.1463828][-4.2202139 -4.2236123 -4.2314248 -4.236814 -4.2264404 -4.1981778 -4.1499667 -4.1179256 -4.104701 -4.1151218 -4.1458316 -4.1779766 -4.1970916 -4.1991644 -4.1880755][-4.1714478 -4.1694021 -4.1829433 -4.2077284 -4.2140179 -4.1876483 -4.1404648 -4.1084208 -4.0989537 -4.1123352 -4.1475635 -4.1904883 -4.2215915 -4.2373586 -4.2376471][-4.1203022 -4.1182933 -4.143445 -4.1868134 -4.2048354 -4.1805286 -4.132515 -4.0960417 -4.0842018 -4.0926676 -4.1195292 -4.1632013 -4.2084517 -4.2455707 -4.2674313][-4.1041727 -4.1077328 -4.1441641 -4.1948166 -4.2143722 -4.1928835 -4.1451144 -4.1025662 -4.0789447 -4.073256 -4.0840163 -4.1219954 -4.1744652 -4.2245126 -4.2581196][-4.1364326 -4.1427569 -4.1745176 -4.2161803 -4.2332859 -4.2167506 -4.1756668 -4.1330109 -4.0971146 -4.0781465 -4.0747991 -4.0992484 -4.1442146 -4.1927838 -4.2275209]]...]
INFO - root - 2017-12-05 23:37:19.227405: step 53410, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 72h:25m:54s remains)
INFO - root - 2017-12-05 23:37:28.645253: step 53420, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 74h:46m:49s remains)
INFO - root - 2017-12-05 23:37:38.380481: step 53430, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 73h:05m:44s remains)
INFO - root - 2017-12-05 23:37:47.656978: step 53440, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.012 sec/batch; 78h:27m:10s remains)
INFO - root - 2017-12-05 23:37:56.837561: step 53450, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 72h:16m:50s remains)
INFO - root - 2017-12-05 23:38:06.305757: step 53460, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 74h:25m:48s remains)
INFO - root - 2017-12-05 23:38:15.623361: step 53470, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 67h:02m:36s remains)
INFO - root - 2017-12-05 23:38:24.904582: step 53480, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 75h:12m:50s remains)
INFO - root - 2017-12-05 23:38:34.432172: step 53490, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 71h:59m:51s remains)
INFO - root - 2017-12-05 23:38:43.805771: step 53500, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 72h:21m:50s remains)
2017-12-05 23:38:44.619091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2143435 -4.2000375 -4.1886697 -4.1902332 -4.2054067 -4.2273011 -4.2403 -4.2412271 -4.2272964 -4.2088175 -4.1950893 -4.1835637 -4.1761608 -4.1796908 -4.1924973][-4.199574 -4.1778159 -4.1631556 -4.1645522 -4.1803541 -4.2027578 -4.2177444 -4.2201972 -4.2073259 -4.1930246 -4.18712 -4.1825714 -4.1763306 -4.1756339 -4.1806808][-4.188242 -4.1645918 -4.1499538 -4.1504993 -4.165659 -4.1874461 -4.20537 -4.2110648 -4.20303 -4.197403 -4.2022119 -4.2042861 -4.1972084 -4.19023 -4.1868868][-4.17357 -4.1508951 -4.1366835 -4.1350403 -4.1481051 -4.1689606 -4.188096 -4.1953263 -4.1916604 -4.1964517 -4.2134972 -4.2244306 -4.2200794 -4.2078533 -4.1937861][-4.16207 -4.1431026 -4.127861 -4.1245608 -4.1379824 -4.1579943 -4.1739626 -4.1745472 -4.1646609 -4.1712923 -4.1982555 -4.2218904 -4.2279119 -4.21885 -4.1986513][-4.1608281 -4.1486645 -4.1353607 -4.1320367 -4.1452837 -4.1598644 -4.1618643 -4.1420174 -4.1138182 -4.1167846 -4.1531372 -4.1930981 -4.2137523 -4.2131171 -4.1912193][-4.1502681 -4.1422634 -4.1296453 -4.1247091 -4.1308556 -4.1310415 -4.1104517 -4.0611038 -4.0092816 -4.0144525 -4.0758758 -4.1441841 -4.1889262 -4.2031145 -4.1877747][-4.1361747 -4.1243596 -4.1087942 -4.0985894 -4.091301 -4.0705457 -4.026175 -3.9500177 -3.8778057 -3.8909113 -3.9846294 -4.0835967 -4.1513443 -4.1790776 -4.171926][-4.1398368 -4.1241746 -4.1059103 -4.0903468 -4.0752535 -4.0490212 -4.0047483 -3.9333789 -3.8715756 -3.8924077 -3.9883249 -4.0835404 -4.1440616 -4.1679721 -4.1634727][-4.1716466 -4.1594563 -4.1414132 -4.1225629 -4.1054449 -4.08484 -4.0539918 -4.0070381 -3.9722733 -3.9921525 -4.0621881 -4.1291084 -4.1640477 -4.1728768 -4.1686783][-4.2122865 -4.2082834 -4.1953506 -4.1791878 -4.1670194 -4.1544719 -4.1352081 -4.1049762 -4.0824609 -4.0898747 -4.1280341 -4.1622934 -4.1711745 -4.1661253 -4.1616106][-4.2251558 -4.2244153 -4.2143488 -4.2030869 -4.1976843 -4.1918411 -4.1821418 -4.16644 -4.1503024 -4.148303 -4.1648865 -4.179945 -4.1759095 -4.1652679 -4.1615424][-4.2177095 -4.2140145 -4.2028584 -4.1950278 -4.1949291 -4.1942635 -4.1892033 -4.1791425 -4.1652417 -4.1587181 -4.1650105 -4.1717405 -4.1645823 -4.1540904 -4.1519151][-4.1890278 -4.1811571 -4.1707373 -4.1684666 -4.1754684 -4.1810422 -4.1803617 -4.1731238 -4.162672 -4.1588664 -4.1641469 -4.169754 -4.1652904 -4.1581 -4.1543732][-4.1671834 -4.1546807 -4.1439013 -4.1441112 -4.153903 -4.1599569 -4.1571612 -4.149158 -4.1415462 -4.1421437 -4.15145 -4.1618133 -4.1649661 -4.163835 -4.1618314]]...]
INFO - root - 2017-12-05 23:38:54.022973: step 53510, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 74h:50m:53s remains)
INFO - root - 2017-12-05 23:39:03.448179: step 53520, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 70h:58m:33s remains)
INFO - root - 2017-12-05 23:39:12.922590: step 53530, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 76h:52m:07s remains)
INFO - root - 2017-12-05 23:39:22.307242: step 53540, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 76h:02m:13s remains)
INFO - root - 2017-12-05 23:39:31.826219: step 53550, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 73h:41m:15s remains)
INFO - root - 2017-12-05 23:39:41.338332: step 53560, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 72h:27m:25s remains)
INFO - root - 2017-12-05 23:39:50.698413: step 53570, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 71h:36m:07s remains)
INFO - root - 2017-12-05 23:40:00.069558: step 53580, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 71h:58m:07s remains)
INFO - root - 2017-12-05 23:40:09.153479: step 53590, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 64h:05m:16s remains)
INFO - root - 2017-12-05 23:40:18.605923: step 53600, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 70h:55m:53s remains)
2017-12-05 23:40:19.382982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22621 -4.2295971 -4.2249432 -4.2243972 -4.2399864 -4.2667689 -4.2778 -4.2776923 -4.2776418 -4.2804 -4.276793 -4.268887 -4.2540822 -4.23433 -4.2252545][-4.2178693 -4.2196975 -4.2160654 -4.2153368 -4.2315722 -4.259994 -4.2758894 -4.2826405 -4.293674 -4.3062177 -4.3103485 -4.3070908 -4.2911739 -4.2677813 -4.2538462][-4.2085075 -4.2084851 -4.2070227 -4.2087412 -4.2234812 -4.2434297 -4.2534256 -4.2618036 -4.2827535 -4.3071804 -4.3221631 -4.3266263 -4.3150096 -4.2928429 -4.2803249][-4.1991324 -4.1983209 -4.2006478 -4.2041111 -4.2106757 -4.2119579 -4.2058463 -4.2100391 -4.2419782 -4.2819972 -4.314683 -4.3290792 -4.3226924 -4.3034525 -4.2937322][-4.1965742 -4.1950316 -4.1987205 -4.197618 -4.1864047 -4.1627593 -4.1278691 -4.1219597 -4.1660762 -4.2253065 -4.2808666 -4.3084664 -4.3098764 -4.2927127 -4.2852993][-4.2067952 -4.2018089 -4.2024322 -4.1911826 -4.1535578 -4.0948234 -4.0197177 -4.0054545 -4.0740528 -4.1545711 -4.2307029 -4.2728305 -4.2811494 -4.2632775 -4.2530236][-4.2127123 -4.2066879 -4.2038531 -4.17774 -4.1106944 -4.0131283 -3.88874 -3.8709011 -3.9789455 -4.0836911 -4.1759033 -4.2289548 -4.2459059 -4.229815 -4.2138882][-4.2139778 -4.2081547 -4.2006836 -4.1612625 -4.07646 -3.9565885 -3.7923641 -3.7730675 -3.9158959 -4.0363307 -4.1342907 -4.1915612 -4.216608 -4.205934 -4.1856461][-4.2220211 -4.2131362 -4.2029424 -4.1654124 -4.0960207 -3.9994304 -3.8561416 -3.8371527 -3.9595258 -4.0588064 -4.1435728 -4.1948795 -4.2205577 -4.2131796 -4.1910992][-4.2431879 -4.231596 -4.2200108 -4.19275 -4.1529927 -4.09825 -4.0062547 -3.9934378 -4.0724764 -4.1346312 -4.1901841 -4.2261777 -4.2479587 -4.2447171 -4.2273564][-4.2648067 -4.25201 -4.2397847 -4.2203889 -4.2001348 -4.1741161 -4.1260705 -4.1233611 -4.1716075 -4.2035518 -4.2324052 -4.2536964 -4.2711248 -4.2771072 -4.2705326][-4.2793794 -4.2704248 -4.2634063 -4.249845 -4.2385468 -4.2264924 -4.20408 -4.2084122 -4.2389975 -4.2525959 -4.2627592 -4.275125 -4.2885633 -4.2987452 -4.2997885][-4.288115 -4.2856851 -4.2850981 -4.2760715 -4.2701173 -4.2661095 -4.2566037 -4.2591763 -4.2765656 -4.2795138 -4.2799511 -4.2877636 -4.2990246 -4.3078256 -4.3104334][-4.2911825 -4.2930551 -4.2946267 -4.2886682 -4.2849736 -4.2856908 -4.2828722 -4.279685 -4.2841692 -4.2825127 -4.2810383 -4.288115 -4.2990546 -4.3056092 -4.3076882][-4.2911344 -4.2959123 -4.2984786 -4.293014 -4.2887058 -4.2901697 -4.2890906 -4.2810802 -4.276392 -4.2736588 -4.2756472 -4.2849584 -4.2961226 -4.3003774 -4.299253]]...]
INFO - root - 2017-12-05 23:40:28.813495: step 53610, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 73h:45m:38s remains)
INFO - root - 2017-12-05 23:40:37.981802: step 53620, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 70h:28m:39s remains)
INFO - root - 2017-12-05 23:40:47.394749: step 53630, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 72h:10m:40s remains)
INFO - root - 2017-12-05 23:40:56.971086: step 53640, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 75h:34m:49s remains)
INFO - root - 2017-12-05 23:41:06.179203: step 53650, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.930 sec/batch; 72h:01m:33s remains)
INFO - root - 2017-12-05 23:41:15.430527: step 53660, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 68h:07m:12s remains)
INFO - root - 2017-12-05 23:41:24.933868: step 53670, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.904 sec/batch; 69h:59m:52s remains)
INFO - root - 2017-12-05 23:41:34.305448: step 53680, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.807 sec/batch; 62h:31m:14s remains)
INFO - root - 2017-12-05 23:41:43.538785: step 53690, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 72h:28m:42s remains)
INFO - root - 2017-12-05 23:41:53.036407: step 53700, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 74h:21m:57s remains)
2017-12-05 23:41:53.816833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2732806 -4.2777014 -4.2800221 -4.2754612 -4.2668238 -4.2525764 -4.2390432 -4.2250247 -4.2103267 -4.1956739 -4.1982946 -4.2106037 -4.2193747 -4.2331133 -4.2506289][-4.3115025 -4.3137236 -4.3120584 -4.3043394 -4.295712 -4.2839065 -4.2690039 -4.2555079 -4.2490115 -4.2365561 -4.2240458 -4.2180514 -4.2138581 -4.2300692 -4.2529373][-4.3265882 -4.3278108 -4.3230052 -4.3146148 -4.3066845 -4.293138 -4.2710619 -4.2571926 -4.2580361 -4.2507696 -4.23493 -4.2183657 -4.2076941 -4.2304029 -4.2569284][-4.3063474 -4.3050981 -4.2986908 -4.2923822 -4.2867627 -4.2658186 -4.2380672 -4.2253442 -4.2305212 -4.2292857 -4.2178583 -4.2030735 -4.2005787 -4.2298131 -4.2573438][-4.2965064 -4.2925248 -4.2848344 -4.2798324 -4.2712059 -4.2434731 -4.2019653 -4.1742263 -4.1691628 -4.1659 -4.1632462 -4.1641922 -4.1828318 -4.222754 -4.2519016][-4.2968268 -4.2939878 -4.2845426 -4.27279 -4.257669 -4.2257032 -4.1720219 -4.1126838 -4.0764623 -4.0601134 -4.0700879 -4.0992484 -4.1490536 -4.2068367 -4.2393551][-4.2839952 -4.2797909 -4.2599034 -4.2308774 -4.203424 -4.174715 -4.1170278 -4.0214596 -3.934773 -3.9005075 -3.9443357 -4.0227871 -4.1073647 -4.1846795 -4.2261758][-4.253252 -4.2409053 -4.1998687 -4.1452723 -4.1081839 -4.09049 -4.0345931 -3.9201677 -3.80397 -3.761718 -3.8455431 -3.9681077 -4.0776663 -4.1673255 -4.2183967][-4.2224183 -4.1990852 -4.1416783 -4.0647306 -4.0151792 -4.0029507 -3.9565995 -3.8569157 -3.7702289 -3.7610309 -3.8602 -3.9785149 -4.0804024 -4.1628342 -4.2145348][-4.2139554 -4.1854129 -4.1277075 -4.0547261 -4.0034347 -3.9877689 -3.9481945 -3.8756757 -3.842766 -3.87445 -3.9592218 -4.0385518 -4.1087332 -4.1733174 -4.2175593][-4.2187977 -4.189199 -4.1438971 -4.090826 -4.0509224 -4.0310826 -3.9959247 -3.9473286 -3.9404294 -3.9827163 -4.0497694 -4.1007276 -4.1454945 -4.1943378 -4.2290649][-4.2209015 -4.194087 -4.1655717 -4.1344862 -4.1081953 -4.0911789 -4.0654793 -4.0327945 -4.0285654 -4.0614691 -4.11183 -4.151392 -4.183207 -4.2200541 -4.2489834][-4.2078357 -4.1903777 -4.1822672 -4.1720772 -4.1616664 -4.1587462 -4.1457925 -4.1214137 -4.1127157 -4.1310911 -4.1676469 -4.2003303 -4.2235417 -4.2479477 -4.2716784][-4.1942534 -4.1930809 -4.2019668 -4.2080255 -4.2085032 -4.2129788 -4.2081861 -4.190011 -4.179462 -4.1914191 -4.2201519 -4.2453995 -4.2588964 -4.2730794 -4.290731][-4.1829524 -4.1985459 -4.2188749 -4.2365265 -4.2439756 -4.2525258 -4.2504306 -4.2371511 -4.2273946 -4.2363777 -4.25629 -4.2706056 -4.27792 -4.2866554 -4.2998018]]...]
INFO - root - 2017-12-05 23:42:03.131532: step 53710, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.985 sec/batch; 76h:17m:35s remains)
INFO - root - 2017-12-05 23:42:12.583792: step 53720, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 75h:09m:13s remains)
INFO - root - 2017-12-05 23:42:22.069900: step 53730, loss = 2.05, batch loss = 2.00 (8.0 examples/sec; 1.000 sec/batch; 77h:24m:50s remains)
INFO - root - 2017-12-05 23:42:31.523247: step 53740, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 75h:18m:57s remains)
INFO - root - 2017-12-05 23:42:41.088727: step 53750, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 74h:39m:56s remains)
INFO - root - 2017-12-05 23:42:50.450837: step 53760, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 76h:02m:45s remains)
INFO - root - 2017-12-05 23:42:59.873206: step 53770, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 74h:20m:46s remains)
INFO - root - 2017-12-05 23:43:09.186727: step 53780, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.920 sec/batch; 71h:11m:58s remains)
INFO - root - 2017-12-05 23:43:18.429022: step 53790, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 77h:24m:23s remains)
INFO - root - 2017-12-05 23:43:27.744102: step 53800, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 72h:44m:52s remains)
2017-12-05 23:43:28.503509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2082462 -4.2176042 -4.2347655 -4.2444139 -4.2427268 -4.2363243 -4.2365394 -4.2402091 -4.2422414 -4.2384949 -4.2201295 -4.1968575 -4.1666155 -4.1681275 -4.1885448][-4.2009807 -4.2220769 -4.2514448 -4.2681661 -4.2670641 -4.2568431 -4.2525325 -4.2593236 -4.2643204 -4.2595782 -4.2433686 -4.2223072 -4.1896663 -4.1779904 -4.1894684][-4.209271 -4.2365065 -4.267446 -4.2789931 -4.2673163 -4.2447109 -4.2337108 -4.2446628 -4.2583885 -4.2581563 -4.2549047 -4.2511435 -4.2350802 -4.2250185 -4.2300196][-4.2443891 -4.2644362 -4.2832069 -4.2765546 -4.2433343 -4.2034235 -4.1837153 -4.1996055 -4.2264004 -4.2346654 -4.2432814 -4.2537618 -4.2585921 -4.2668362 -4.2788243][-4.27518 -4.2823925 -4.2838178 -4.2524219 -4.1889539 -4.1231737 -4.08556 -4.1009722 -4.1454058 -4.1708975 -4.1924906 -4.2109127 -4.2284427 -4.252903 -4.2790504][-4.2895918 -4.2836952 -4.268631 -4.2133336 -4.114984 -4.0154047 -3.9478655 -3.9460189 -4.0035324 -4.048265 -4.0861049 -4.1200204 -4.1467648 -4.18548 -4.2296057][-4.291584 -4.2747 -4.2417979 -4.163774 -4.041564 -3.9137325 -3.8132596 -3.7852352 -3.8474445 -3.9023314 -3.9494405 -4.0013905 -4.0381923 -4.0892591 -4.1544232][-4.2791491 -4.2601314 -4.22292 -4.1443443 -4.0318804 -3.9115136 -3.8138855 -3.7789681 -3.8320961 -3.868911 -3.8904216 -3.9364719 -3.9749312 -4.0247703 -4.0943193][-4.2646017 -4.252965 -4.2260585 -4.1712551 -4.0954676 -4.0142007 -3.9525905 -3.9322178 -3.958699 -3.9601388 -3.9410853 -3.9600825 -3.9845388 -4.0172615 -4.0745468][-4.2551365 -4.256206 -4.2461371 -4.2189279 -4.1798234 -4.1331773 -4.0964022 -4.080627 -4.0828419 -4.0634227 -4.028832 -4.0282607 -4.0366178 -4.0511465 -4.0958743][-4.2576509 -4.2681351 -4.270854 -4.2634735 -4.2484527 -4.2243671 -4.200438 -4.1844263 -4.1755085 -4.1565366 -4.1268425 -4.1176825 -4.1161032 -4.1198845 -4.1529288][-4.2746964 -4.2848625 -4.28662 -4.2824354 -4.2770805 -4.2644095 -4.2490439 -4.2400618 -4.236793 -4.231698 -4.21936 -4.2132092 -4.2090864 -4.20664 -4.2259569][-4.2968373 -4.3052826 -4.3024025 -4.2970004 -4.2923574 -4.2826242 -4.2728734 -4.2730465 -4.2786961 -4.2835855 -4.2838368 -4.2833719 -4.2813711 -4.2761407 -4.2849731][-4.3133073 -4.3175845 -4.3119221 -4.3058076 -4.3004303 -4.2916942 -4.2872953 -4.2942448 -4.3056421 -4.3146362 -4.3184237 -4.3176408 -4.3157034 -4.3130145 -4.3180528][-4.3190279 -4.3164845 -4.3103623 -4.30553 -4.3001695 -4.2952228 -4.2972383 -4.3098254 -4.3236246 -4.331594 -4.3342361 -4.3325791 -4.3298306 -4.32706 -4.3304234]]...]
INFO - root - 2017-12-05 23:43:37.756991: step 53810, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 75h:50m:16s remains)
INFO - root - 2017-12-05 23:43:47.153614: step 53820, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 73h:42m:34s remains)
INFO - root - 2017-12-05 23:43:56.472037: step 53830, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 77h:19m:40s remains)
INFO - root - 2017-12-05 23:44:06.083838: step 53840, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 75h:02m:16s remains)
INFO - root - 2017-12-05 23:44:15.583673: step 53850, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 67h:43m:56s remains)
INFO - root - 2017-12-05 23:44:24.490521: step 53860, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 69h:46m:02s remains)
INFO - root - 2017-12-05 23:44:33.711620: step 53870, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.958 sec/batch; 74h:10m:50s remains)
INFO - root - 2017-12-05 23:44:42.876454: step 53880, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.008 sec/batch; 78h:00m:23s remains)
INFO - root - 2017-12-05 23:44:52.295351: step 53890, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 67h:10m:40s remains)
INFO - root - 2017-12-05 23:45:01.750860: step 53900, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 75h:13m:13s remains)
2017-12-05 23:45:02.516863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2547078 -4.2534819 -4.252738 -4.2522683 -4.2537389 -4.25653 -4.2599978 -4.2628412 -4.264328 -4.26466 -4.2642674 -4.2634988 -4.2624636 -4.261435 -4.261385][-4.222652 -4.2221146 -4.2216778 -4.2215862 -4.2233 -4.2264962 -4.2308459 -4.2349548 -4.2372923 -4.2372952 -4.2352834 -4.2318215 -4.2274823 -4.2237816 -4.2225728][-4.1934843 -4.192234 -4.1903782 -4.18894 -4.1900744 -4.1932583 -4.1990194 -4.20654 -4.2127142 -4.2148929 -4.2131271 -4.2075047 -4.1990724 -4.191401 -4.1872945][-4.1788511 -4.1732645 -4.1670437 -4.1607347 -4.157424 -4.1590309 -4.1668358 -4.179111 -4.1912537 -4.199636 -4.2034326 -4.2010822 -4.1933708 -4.1838088 -4.1755638][-4.1692443 -4.1604013 -4.1509061 -4.1394706 -4.1295815 -4.1259131 -4.1298847 -4.1400557 -4.15306 -4.1662211 -4.1777582 -4.1862025 -4.1893244 -4.1872306 -4.1813517][-4.14359 -4.1312118 -4.1169829 -4.1001358 -4.0835533 -4.0718417 -4.0676804 -4.0718379 -4.0836258 -4.099566 -4.1163197 -4.1341891 -4.1505346 -4.1611066 -4.1647425][-4.1201396 -4.1065178 -4.0904784 -4.0718527 -4.0527668 -4.03442 -4.0226188 -4.0207696 -4.0287571 -4.0418396 -4.0555596 -4.0734634 -4.0948138 -4.1135917 -4.1289635][-4.1213756 -4.1099815 -4.0970612 -4.084094 -4.0686431 -4.048871 -4.0339108 -4.0288963 -4.0324664 -4.0396528 -4.0451 -4.0556602 -4.0696831 -4.0823569 -4.0968385][-4.1345797 -4.1292472 -4.1264381 -4.12477 -4.1173611 -4.1020813 -4.0883031 -4.0810742 -4.0794106 -4.07924 -4.0779848 -4.0820312 -4.08772 -4.0912724 -4.0965586][-4.1497464 -4.1508389 -4.16044 -4.1719971 -4.1748929 -4.1674356 -4.1594057 -4.1541214 -4.1490498 -4.1416464 -4.1331887 -4.1285043 -4.1231084 -4.1161561 -4.1113091][-4.1659942 -4.1678348 -4.1841693 -4.2041621 -4.2147288 -4.2123632 -4.209115 -4.2080274 -4.2065673 -4.1986117 -4.1864233 -4.174253 -4.1610756 -4.1484771 -4.1362062][-4.1700363 -4.1697288 -4.1858039 -4.2070088 -4.2206645 -4.2219696 -4.223176 -4.2279525 -4.2319641 -4.2274513 -4.2169919 -4.2043161 -4.1904593 -4.1776891 -4.1620545][-4.1669631 -4.1629829 -4.1730952 -4.1896553 -4.2023449 -4.2080269 -4.2137465 -4.2231474 -4.2311349 -4.2299981 -4.2230873 -4.2136712 -4.2046933 -4.19633 -4.1835785][-4.170105 -4.1616888 -4.1632624 -4.1738214 -4.1870232 -4.1960688 -4.2032056 -4.2138391 -4.2228613 -4.2239676 -4.2195826 -4.21369 -4.2103543 -4.2098832 -4.2040191][-4.1811733 -4.1712856 -4.1678853 -4.1730847 -4.1837692 -4.190999 -4.1937451 -4.1986895 -4.20752 -4.2141533 -4.2154231 -4.2132497 -4.2123227 -4.2166734 -4.2190495]]...]
INFO - root - 2017-12-05 23:45:11.728309: step 53910, loss = 2.05, batch loss = 2.00 (8.0 examples/sec; 1.005 sec/batch; 77h:45m:16s remains)
INFO - root - 2017-12-05 23:45:21.111641: step 53920, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 73h:58m:46s remains)
INFO - root - 2017-12-05 23:45:30.517465: step 53930, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 73h:20m:22s remains)
INFO - root - 2017-12-05 23:45:40.012760: step 53940, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 68h:22m:38s remains)
INFO - root - 2017-12-05 23:45:49.528963: step 53950, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 73h:28m:08s remains)
INFO - root - 2017-12-05 23:45:58.931642: step 53960, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 69h:52m:03s remains)
INFO - root - 2017-12-05 23:46:08.381418: step 53970, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.936 sec/batch; 72h:25m:24s remains)
INFO - root - 2017-12-05 23:46:17.651039: step 53980, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 71h:33m:06s remains)
INFO - root - 2017-12-05 23:46:27.110026: step 53990, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 72h:17m:37s remains)
INFO - root - 2017-12-05 23:46:36.513286: step 54000, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 72h:41m:23s remains)
2017-12-05 23:46:37.262236: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.270215 -4.2715926 -4.27251 -4.2732944 -4.2732358 -4.272223 -4.2702761 -4.2677112 -4.2657852 -4.2658019 -4.2674026 -4.2698789 -4.2728586 -4.2757816 -4.2790561][-4.2700067 -4.2701836 -4.2700787 -4.27003 -4.2693734 -4.2676649 -4.2650414 -4.2619395 -4.2597795 -4.2599249 -4.2618122 -4.26491 -4.2686467 -4.2725697 -4.2768655][-4.2679758 -4.2658687 -4.2634606 -4.2609749 -4.2579007 -4.2544136 -4.2506213 -4.2472062 -4.245708 -4.2465572 -4.2493844 -4.2539129 -4.2594762 -4.2655058 -4.2719116][-4.2608395 -4.256165 -4.2506657 -4.2454886 -4.2409763 -4.2375231 -4.2341356 -4.231184 -4.2297325 -4.2297378 -4.2319593 -4.2366166 -4.2435179 -4.2523661 -4.2623553][-4.2463627 -4.2407317 -4.2336874 -4.2273979 -4.2227964 -4.2190304 -4.2142463 -4.2099895 -4.2071824 -4.2043586 -4.2036867 -4.2072339 -4.2156463 -4.2279787 -4.2422957][-4.2255158 -4.220891 -4.215384 -4.2119274 -4.2106752 -4.2085452 -4.2027421 -4.196403 -4.1901588 -4.18298 -4.1775622 -4.1772032 -4.1835284 -4.1956768 -4.2110648][-4.20788 -4.2053237 -4.2031507 -4.2037449 -4.2061839 -4.2059579 -4.1995382 -4.1913052 -4.1815968 -4.1705356 -4.1607485 -4.1553488 -4.1569462 -4.1645288 -4.1761217][-4.208549 -4.2066512 -4.20627 -4.20831 -4.211082 -4.2096109 -4.20218 -4.1938243 -4.1837831 -4.1728172 -4.1628041 -4.1549373 -4.1514454 -4.1518955 -4.1554227][-4.2249327 -4.2215905 -4.2207122 -4.2231 -4.22616 -4.2253633 -4.2206955 -4.215157 -4.2081409 -4.2012777 -4.1942511 -4.1865158 -4.179677 -4.1746631 -4.1710596][-4.2331152 -4.2285233 -4.2279372 -4.2317486 -4.2388687 -4.2440968 -4.2457519 -4.2453656 -4.24381 -4.2432408 -4.2400165 -4.2328858 -4.2243605 -4.2166395 -4.2093077][-4.238728 -4.2344689 -4.2341628 -4.2394514 -4.2499628 -4.2593064 -4.2650757 -4.267993 -4.2713766 -4.2761745 -4.2769384 -4.271996 -4.2645969 -4.2577882 -4.2508626][-4.2429748 -4.2400784 -4.2407765 -4.2469468 -4.2584252 -4.2691703 -4.276814 -4.2814555 -4.2860355 -4.2909484 -4.2926283 -4.2898045 -4.2861533 -4.2832489 -4.2800035][-4.2475781 -4.2457414 -4.2478132 -4.2534227 -4.2621641 -4.2697105 -4.2757106 -4.2794571 -4.2821736 -4.2841125 -4.2850595 -4.2847733 -4.28502 -4.2860942 -4.2869749][-4.2585645 -4.2563467 -4.2577152 -4.2605953 -4.2636395 -4.2661982 -4.2681022 -4.2688766 -4.2687297 -4.2685337 -4.268981 -4.2705851 -4.274303 -4.2780728 -4.2818003][-4.2665005 -4.2637243 -4.2639933 -4.2642708 -4.2638106 -4.2630758 -4.26135 -4.2592516 -4.2571187 -4.2557206 -4.2557526 -4.2578573 -4.2632208 -4.26773 -4.2715383]]...]
INFO - root - 2017-12-05 23:46:46.564223: step 54010, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 68h:51m:46s remains)
INFO - root - 2017-12-05 23:46:56.093378: step 54020, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 73h:17m:02s remains)
INFO - root - 2017-12-05 23:47:05.481183: step 54030, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 69h:59m:02s remains)
INFO - root - 2017-12-05 23:47:14.894377: step 54040, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 73h:31m:11s remains)
INFO - root - 2017-12-05 23:47:24.288134: step 54050, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 72h:35m:18s remains)
INFO - root - 2017-12-05 23:47:33.756756: step 54060, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 71h:54m:00s remains)
INFO - root - 2017-12-05 23:47:43.390429: step 54070, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 72h:58m:45s remains)
INFO - root - 2017-12-05 23:47:52.753590: step 54080, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 74h:24m:46s remains)
INFO - root - 2017-12-05 23:48:02.181424: step 54090, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 68h:30m:14s remains)
INFO - root - 2017-12-05 23:48:11.479402: step 54100, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 71h:07m:45s remains)
2017-12-05 23:48:12.214869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.176456 -4.1541796 -4.1436343 -4.1517134 -4.1633759 -4.1757631 -4.1860547 -4.1932249 -4.2066579 -4.2264633 -4.2411366 -4.2369561 -4.2181616 -4.1996431 -4.1959925][-4.2105236 -4.1868734 -4.1756921 -4.1872892 -4.2096829 -4.23372 -4.2520576 -4.2628803 -4.272037 -4.280273 -4.2811327 -4.2675447 -4.2440338 -4.2255454 -4.222569][-4.2313652 -4.209435 -4.2024212 -4.2192016 -4.249877 -4.2789311 -4.2987795 -4.3077636 -4.3100209 -4.30841 -4.2993121 -4.2802849 -4.2567735 -4.2412491 -4.2399588][-4.2310939 -4.2106972 -4.2109413 -4.2331667 -4.2646275 -4.2882752 -4.3029127 -4.30877 -4.3058362 -4.2969522 -4.2814527 -4.2620955 -4.2459836 -4.238 -4.2396941][-4.2217965 -4.2030077 -4.2055516 -4.2257605 -4.2492628 -4.2619376 -4.2702618 -4.2754827 -4.2718639 -4.2566772 -4.234632 -4.2180753 -4.2133136 -4.2142859 -4.2188439][-4.2249556 -4.2071548 -4.2045183 -4.212369 -4.2183018 -4.214817 -4.2165356 -4.2235556 -4.2224669 -4.2032385 -4.176404 -4.1649976 -4.1714025 -4.1803274 -4.1876068][-4.2370281 -4.2147479 -4.19953 -4.1889381 -4.1719875 -4.1513491 -4.1505075 -4.1649551 -4.1711226 -4.1523461 -4.12692 -4.1229582 -4.1357083 -4.1479907 -4.1561546][-4.2428732 -4.215332 -4.1879015 -4.1590614 -4.1215949 -4.0911446 -4.0967021 -4.1264648 -4.144783 -4.1313434 -4.1123519 -4.1138988 -4.1251974 -4.1299987 -4.1308813][-4.2310257 -4.2008591 -4.1677752 -4.1300039 -4.086719 -4.0646439 -4.0853491 -4.1264043 -4.1509671 -4.1414413 -4.1268792 -4.1276741 -4.1312408 -4.12493 -4.1136031][-4.2073817 -4.1808944 -4.152019 -4.120851 -4.0931168 -4.0913582 -4.1184049 -4.15457 -4.17421 -4.165863 -4.1532235 -4.1511984 -4.1475925 -4.1312952 -4.111105][-4.1862278 -4.1675057 -4.1463819 -4.1269979 -4.1198683 -4.1336117 -4.15767 -4.1832924 -4.1985955 -4.1948137 -4.1854086 -4.1810522 -4.1731186 -4.1512117 -4.1295967][-4.1745014 -4.1619048 -4.1471643 -4.1357741 -4.1424623 -4.1636782 -4.1842957 -4.2051687 -4.220408 -4.2222648 -4.2161684 -4.2096729 -4.200366 -4.1813407 -4.163404][-4.1688018 -4.1606097 -4.1507673 -4.1430712 -4.1546445 -4.1762586 -4.1940904 -4.2097421 -4.2225609 -4.2265449 -4.2237453 -4.2166271 -4.2090344 -4.1971836 -4.185565][-4.1722918 -4.1665869 -4.1602097 -4.1521378 -4.1597958 -4.1746969 -4.1862469 -4.1949568 -4.2058854 -4.2131248 -4.2144828 -4.2078495 -4.2015238 -4.1953807 -4.1889234][-4.1920295 -4.1855607 -4.1790805 -4.1701536 -4.1732059 -4.1784472 -4.1813126 -4.1852522 -4.1959004 -4.2045169 -4.2089 -4.204236 -4.2005239 -4.1969204 -4.1893206]]...]
INFO - root - 2017-12-05 23:48:21.761080: step 54110, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 73h:56m:21s remains)
INFO - root - 2017-12-05 23:48:31.089048: step 54120, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 71h:38m:07s remains)
INFO - root - 2017-12-05 23:48:40.652907: step 54130, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 75h:12m:06s remains)
INFO - root - 2017-12-05 23:48:49.962201: step 54140, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 74h:45m:06s remains)
INFO - root - 2017-12-05 23:48:59.287992: step 54150, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 71h:23m:24s remains)
INFO - root - 2017-12-05 23:49:08.920025: step 54160, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 75h:16m:38s remains)
INFO - root - 2017-12-05 23:49:18.359705: step 54170, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.969 sec/batch; 74h:57m:07s remains)
INFO - root - 2017-12-05 23:49:27.533041: step 54180, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 68h:58m:08s remains)
INFO - root - 2017-12-05 23:49:36.875296: step 54190, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 72h:28m:57s remains)
INFO - root - 2017-12-05 23:49:46.135807: step 54200, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 76h:53m:05s remains)
2017-12-05 23:49:46.914294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1952443 -4.1959243 -4.2083211 -4.2349529 -4.2692065 -4.2991247 -4.3124938 -4.3078089 -4.291513 -4.2683268 -4.2428641 -4.2277732 -4.2240357 -4.2264924 -4.2330756][-4.1839681 -4.1824727 -4.1892123 -4.2102461 -4.2448964 -4.2795467 -4.2991462 -4.30105 -4.2885709 -4.2662907 -4.238028 -4.2144151 -4.2024093 -4.2041817 -4.2149034][-4.1724691 -4.167552 -4.166512 -4.1773973 -4.2059126 -4.2409062 -4.2652941 -4.2739511 -4.265348 -4.2453566 -4.2138319 -4.1828008 -4.1667628 -4.1706553 -4.1854639][-4.1680951 -4.1632071 -4.1560936 -4.157846 -4.1775336 -4.2107635 -4.2377243 -4.2524381 -4.2480035 -4.2304473 -4.2003555 -4.1695042 -4.1496196 -4.1500854 -4.1623654][-4.1810946 -4.1795335 -4.168015 -4.1603971 -4.1672325 -4.1881557 -4.205945 -4.2231016 -4.2288017 -4.220643 -4.2027717 -4.1809955 -4.1604452 -4.1534905 -4.158916][-4.1915808 -4.1996069 -4.19147 -4.1798048 -4.1748652 -4.1713009 -4.1650805 -4.1783671 -4.1981316 -4.2075257 -4.2098403 -4.1992559 -4.1818442 -4.1714687 -4.1718941][-4.1858253 -4.2015939 -4.1974983 -4.1811161 -4.1614208 -4.1295877 -4.0921578 -4.0968356 -4.1363688 -4.1706967 -4.1931038 -4.19528 -4.1838326 -4.1750507 -4.1799078][-4.1682053 -4.1829176 -4.1762795 -4.1551023 -4.1175585 -4.0527344 -3.9782753 -3.9829144 -4.0575223 -4.1209517 -4.1577697 -4.1713095 -4.1723161 -4.1764479 -4.1935973][-4.1346807 -4.1443009 -4.1435876 -4.1303763 -4.0931516 -4.0140271 -3.9168637 -3.918915 -4.0128179 -4.0868297 -4.1265268 -4.1454067 -4.1590033 -4.1802316 -4.2072721][-4.1127682 -4.1249285 -4.1358762 -4.139657 -4.1232629 -4.0694118 -3.992991 -3.988766 -4.0574975 -4.1091557 -4.13092 -4.1411839 -4.1555753 -4.1825352 -4.2129884][-4.116447 -4.13536 -4.1545992 -4.1638212 -4.1644664 -4.1366844 -4.0890903 -4.078794 -4.11443 -4.1417184 -4.1463032 -4.1440282 -4.1504884 -4.1733356 -4.2050095][-4.1412382 -4.162416 -4.1840415 -4.1931739 -4.199203 -4.1895165 -4.1644478 -4.1546335 -4.1710849 -4.1845756 -4.1753874 -4.1583304 -4.1531482 -4.1656036 -4.19395][-4.1769891 -4.1957374 -4.2151504 -4.2205939 -4.2223239 -4.2210531 -4.2135935 -4.2101917 -4.2191825 -4.2272487 -4.215385 -4.1962662 -4.1876545 -4.1933756 -4.2149835][-4.2177072 -4.2309251 -4.2470493 -4.2514119 -4.2531748 -4.2563567 -4.258698 -4.2631984 -4.2708769 -4.2750025 -4.2652082 -4.2498431 -4.2412052 -4.2436171 -4.25759][-4.2675543 -4.2784214 -4.2934308 -4.2958837 -4.2958245 -4.3004766 -4.3065295 -4.3134723 -4.3209119 -4.3223934 -4.3118443 -4.2954979 -4.2839017 -4.2815876 -4.2877941]]...]
INFO - root - 2017-12-05 23:49:56.288610: step 54210, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 73h:15m:27s remains)
INFO - root - 2017-12-05 23:50:05.528414: step 54220, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 75h:22m:55s remains)
INFO - root - 2017-12-05 23:50:14.843812: step 54230, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 67h:48m:48s remains)
INFO - root - 2017-12-05 23:50:24.189765: step 54240, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 73h:13m:54s remains)
INFO - root - 2017-12-05 23:50:33.707633: step 54250, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 73h:10m:57s remains)
INFO - root - 2017-12-05 23:50:43.277054: step 54260, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.970 sec/batch; 74h:56m:15s remains)
INFO - root - 2017-12-05 23:50:52.657892: step 54270, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 72h:31m:59s remains)
INFO - root - 2017-12-05 23:51:02.053380: step 54280, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.976 sec/batch; 75h:26m:06s remains)
INFO - root - 2017-12-05 23:51:11.522578: step 54290, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 76h:15m:12s remains)
INFO - root - 2017-12-05 23:51:20.595116: step 54300, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 70h:26m:22s remains)
2017-12-05 23:51:21.401765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2708898 -4.2802405 -4.2939382 -4.3015161 -4.2941313 -4.2706003 -4.2367887 -4.2044716 -4.1831732 -4.1770334 -4.1862273 -4.2027969 -4.2253108 -4.2454123 -4.2533154][-4.2653484 -4.27882 -4.2925892 -4.2963185 -4.2840991 -4.2560697 -4.219038 -4.1840339 -4.1600609 -4.1532707 -4.1603131 -4.1725368 -4.193419 -4.21866 -4.2322702][-4.250421 -4.2665586 -4.2811584 -4.2821884 -4.2666245 -4.2319989 -4.186821 -4.144896 -4.1221313 -4.1211371 -4.1310921 -4.1430888 -4.1625767 -4.1940141 -4.216][-4.2297778 -4.2453694 -4.2600765 -4.2605515 -4.2419157 -4.200943 -4.1460447 -4.0999312 -4.0866108 -4.1009717 -4.1213369 -4.13606 -4.1519928 -4.1836739 -4.2105212][-4.219522 -4.23084 -4.2410135 -4.2376251 -4.213841 -4.1661463 -4.1067171 -4.061554 -4.0631032 -4.0979996 -4.1296811 -4.1490984 -4.164444 -4.1965275 -4.2260208][-4.2268968 -4.2324934 -4.2332854 -4.221745 -4.1923738 -4.1387234 -4.0774035 -4.0323224 -4.0411358 -4.0898438 -4.13068 -4.1570034 -4.1805515 -4.2151093 -4.2440348][-4.2486572 -4.2515121 -4.2432075 -4.2217779 -4.1855111 -4.1250267 -4.0576038 -4.00733 -4.0155292 -4.0729547 -4.122838 -4.1562734 -4.1895323 -4.2283497 -4.2547641][-4.2724013 -4.2725797 -4.2574039 -4.2298059 -4.1908097 -4.1280341 -4.0544305 -3.9977355 -4.00029 -4.062016 -4.1199031 -4.15631 -4.1915588 -4.2291927 -4.249218][-4.2868552 -4.2839246 -4.2666054 -4.23493 -4.1919241 -4.1308093 -4.0613661 -4.0097303 -4.0118814 -4.0710735 -4.1288071 -4.1611695 -4.1876664 -4.2108674 -4.2166491][-4.2954736 -4.2914243 -4.2748184 -4.2414122 -4.1936355 -4.1345358 -4.0778275 -4.0393853 -4.0400872 -4.0910654 -4.145124 -4.1738315 -4.1868615 -4.1880178 -4.1751323][-4.3002715 -4.3008647 -4.2896695 -4.2621231 -4.2188573 -4.1642427 -4.1165953 -4.088521 -4.086628 -4.1265368 -4.1734996 -4.1990695 -4.201292 -4.1852274 -4.1637192][-4.2936316 -4.3032722 -4.30429 -4.2894659 -4.2577214 -4.2124467 -4.1718154 -4.1469092 -4.1422129 -4.1702671 -4.2082496 -4.229311 -4.2279634 -4.2090139 -4.1895156][-4.2733612 -4.2907066 -4.3041596 -4.3040094 -4.288877 -4.2568521 -4.2197423 -4.1953988 -4.1906829 -4.2120171 -4.2410154 -4.2562828 -4.2546082 -4.2406559 -4.2275171][-4.2551389 -4.2767272 -4.2957726 -4.3037868 -4.3010283 -4.282382 -4.251812 -4.22785 -4.2242746 -4.2418246 -4.2630353 -4.275136 -4.276659 -4.2697749 -4.2637453][-4.2551041 -4.272212 -4.2877955 -4.2967968 -4.2990088 -4.287344 -4.2646151 -4.2430234 -4.2376666 -4.2495308 -4.2640157 -4.2749767 -4.2819676 -4.2833905 -4.2850461]]...]
INFO - root - 2017-12-05 23:51:30.669295: step 54310, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 70h:26m:25s remains)
INFO - root - 2017-12-05 23:51:40.072620: step 54320, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 75h:20m:04s remains)
INFO - root - 2017-12-05 23:51:49.441447: step 54330, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 70h:44m:26s remains)
INFO - root - 2017-12-05 23:51:58.850347: step 54340, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 70h:51m:38s remains)
INFO - root - 2017-12-05 23:52:08.126569: step 54350, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 70h:33m:06s remains)
INFO - root - 2017-12-05 23:52:17.604222: step 54360, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 70h:57m:32s remains)
INFO - root - 2017-12-05 23:52:26.991289: step 54370, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 70h:19m:53s remains)
INFO - root - 2017-12-05 23:52:36.198937: step 54380, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 69h:47m:28s remains)
INFO - root - 2017-12-05 23:52:45.510680: step 54390, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 66h:13m:22s remains)
INFO - root - 2017-12-05 23:52:55.054446: step 54400, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 71h:15m:30s remains)
2017-12-05 23:52:55.805952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3343534 -4.3239164 -4.3039093 -4.2785378 -4.2382846 -4.1912203 -4.1545258 -4.1480494 -4.179841 -4.2229042 -4.2556467 -4.2786946 -4.2804503 -4.2535319 -4.2097559][-4.3343549 -4.3239737 -4.3013067 -4.2722368 -4.2284865 -4.173821 -4.1343012 -4.1319084 -4.1671124 -4.2144852 -4.2491894 -4.2714977 -4.2764473 -4.2547617 -4.2151031][-4.3342838 -4.3248091 -4.3034396 -4.2703261 -4.2157216 -4.1490178 -4.1061893 -4.1129546 -4.1534343 -4.2004576 -4.2322717 -4.2494659 -4.2554207 -4.243011 -4.2119627][-4.3345246 -4.3270245 -4.30817 -4.27234 -4.2135906 -4.1445665 -4.1024771 -4.1180029 -4.1594486 -4.1956692 -4.2153673 -4.2225151 -4.2277651 -4.22524 -4.2043996][-4.3348804 -4.3290696 -4.31333 -4.2760997 -4.2194986 -4.1534214 -4.1132364 -4.1303992 -4.1689897 -4.193244 -4.2029076 -4.2010546 -4.2061276 -4.2109566 -4.1980758][-4.3341961 -4.3285871 -4.3100133 -4.2652369 -4.2013483 -4.1344428 -4.0932083 -4.1163969 -4.1632872 -4.1871324 -4.1916194 -4.1837687 -4.1855955 -4.1945114 -4.1896458][-4.3330288 -4.3241687 -4.2982531 -4.2412233 -4.1569772 -4.0717583 -4.0188189 -4.0480962 -4.1202664 -4.1650052 -4.1762137 -4.17151 -4.1755395 -4.185771 -4.1865849][-4.3320742 -4.319593 -4.2881975 -4.219491 -4.1115212 -3.9942217 -3.9106808 -3.9293754 -4.0292711 -4.1085196 -4.1431179 -4.1544394 -4.1644959 -4.1763692 -4.1779985][-4.3313041 -4.3181915 -4.2847495 -4.2097187 -4.0866828 -3.948314 -3.8382838 -3.8362041 -3.948225 -4.0538597 -4.1121778 -4.1415381 -4.1605959 -4.1722374 -4.1743946][-4.3281884 -4.3149524 -4.2840834 -4.2130046 -4.0955296 -3.9717221 -3.8782806 -3.8805113 -3.977289 -4.072648 -4.1236372 -4.1524239 -4.1683822 -4.1723423 -4.172646][-4.322948 -4.3077359 -4.2786622 -4.2175584 -4.1220708 -4.0315213 -3.9735265 -3.9870059 -4.0608845 -4.1289134 -4.1600161 -4.1780372 -4.1837258 -4.1801634 -4.1788588][-4.3179531 -4.2994094 -4.2693968 -4.2186594 -4.1453724 -4.08498 -4.0605021 -4.0874119 -4.1473417 -4.1956153 -4.2117615 -4.2147222 -4.2130551 -4.2073512 -4.204474][-4.3155203 -4.2947059 -4.2661619 -4.2257142 -4.1728897 -4.1353011 -4.1315217 -4.1657076 -4.2144208 -4.2486215 -4.2560596 -4.2532105 -4.2497063 -4.2442584 -4.2378936][-4.3175836 -4.2989655 -4.2756171 -4.2468066 -4.2109847 -4.1880565 -4.1931028 -4.2232037 -4.2589784 -4.2840705 -4.2903132 -4.2880669 -4.285521 -4.2819943 -4.2756209][-4.3227191 -4.3075628 -4.2879639 -4.2658272 -4.2395058 -4.2233295 -4.2292824 -4.2545061 -4.2837687 -4.3039851 -4.3111262 -4.3120236 -4.3121729 -4.3108139 -4.3043022]]...]
INFO - root - 2017-12-05 23:53:04.903683: step 54410, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 70h:11m:40s remains)
INFO - root - 2017-12-05 23:53:14.450771: step 54420, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 71h:43m:03s remains)
INFO - root - 2017-12-05 23:53:23.982796: step 54430, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 70h:20m:43s remains)
INFO - root - 2017-12-05 23:53:33.476980: step 54440, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 73h:30m:06s remains)
INFO - root - 2017-12-05 23:53:42.910029: step 54450, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 71h:27m:09s remains)
INFO - root - 2017-12-05 23:53:52.251121: step 54460, loss = 2.10, batch loss = 2.05 (9.0 examples/sec; 0.892 sec/batch; 68h:52m:06s remains)
INFO - root - 2017-12-05 23:54:01.678875: step 54470, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 70h:25m:17s remains)
INFO - root - 2017-12-05 23:54:11.062188: step 54480, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 71h:47m:39s remains)
INFO - root - 2017-12-05 23:54:20.396726: step 54490, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 73h:42m:10s remains)
INFO - root - 2017-12-05 23:54:29.619474: step 54500, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 66h:49m:58s remains)
2017-12-05 23:54:30.397627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2403717 -4.2356825 -4.2347474 -4.2423549 -4.2495112 -4.2389064 -4.2310557 -4.2353015 -4.24411 -4.2449112 -4.2327113 -4.2197905 -4.2178044 -4.2217298 -4.2249832][-4.2471523 -4.2341733 -4.2267971 -4.2294607 -4.2363634 -4.2253227 -4.2190051 -4.2252312 -4.2336679 -4.234447 -4.224772 -4.2128434 -4.2081776 -4.2099285 -4.2171164][-4.2589145 -4.2355027 -4.2166305 -4.2086287 -4.2144585 -4.2126541 -4.2141538 -4.2214975 -4.2275271 -4.2281251 -4.2244005 -4.2138143 -4.2044821 -4.2045279 -4.2143831][-4.27753 -4.2497945 -4.2172117 -4.1908875 -4.1868563 -4.191761 -4.1974072 -4.206183 -4.2144918 -4.2212429 -4.2229419 -4.2110457 -4.1971922 -4.2008834 -4.2151322][-4.2786441 -4.2549758 -4.2177677 -4.176394 -4.1565938 -4.1552119 -4.1559486 -4.1713014 -4.1945 -4.2130818 -4.2218447 -4.2101851 -4.1938767 -4.2018824 -4.2174706][-4.2665296 -4.2490959 -4.2135706 -4.1577086 -4.1171012 -4.0941043 -4.0725813 -4.0951385 -4.1480846 -4.1918178 -4.2136192 -4.2023582 -4.1818633 -4.1843495 -4.1923766][-4.24591 -4.2256842 -4.1860347 -4.1170158 -4.048954 -3.9829545 -3.917001 -3.9432566 -4.038868 -4.1220841 -4.166883 -4.1620646 -4.1345811 -4.1215506 -4.1191778][-4.1978073 -4.1783309 -4.1346407 -4.0597973 -3.9714112 -3.8607252 -3.7470677 -3.7792218 -3.9209137 -4.0414467 -4.1049423 -4.1014409 -4.0653181 -4.0416613 -4.0448203][-4.1434913 -4.1343307 -4.1060505 -4.058938 -3.9940574 -3.8999355 -3.8046696 -3.8375211 -3.9661736 -4.0710711 -4.1134939 -4.0914736 -4.047647 -4.0230818 -4.0400729][-4.1255889 -4.12262 -4.1158156 -4.1038055 -4.0765882 -4.0308118 -3.9849689 -4.0093789 -4.088563 -4.1519012 -4.1644664 -4.1256456 -4.0767794 -4.0523791 -4.0697074][-4.1274443 -4.1200418 -4.1233473 -4.132987 -4.1339512 -4.1246715 -4.1147552 -4.1355824 -4.1806226 -4.2132363 -4.2111015 -4.1695991 -4.1182232 -4.0883064 -4.0969114][-4.1667914 -4.1527767 -4.1548958 -4.1717939 -4.1895623 -4.2003474 -4.207809 -4.2252679 -4.2477331 -4.2592373 -4.2472854 -4.2094331 -4.1600833 -4.1276369 -4.1313205][-4.2293234 -4.2133751 -4.2110887 -4.2250891 -4.241745 -4.2554622 -4.2641978 -4.27024 -4.2740412 -4.2736015 -4.2588139 -4.2310791 -4.1975389 -4.1712313 -4.1745148][-4.2772088 -4.2654266 -4.2634077 -4.2759624 -4.2883234 -4.2969136 -4.2956705 -4.2843285 -4.2735543 -4.2700491 -4.2623239 -4.2483573 -4.2309709 -4.2172551 -4.2223368][-4.3005013 -4.2947769 -4.2940521 -4.3019886 -4.308125 -4.308846 -4.2992229 -4.2809277 -4.2661104 -4.26605 -4.2689161 -4.2665682 -4.2587028 -4.2540727 -4.2612534]]...]
INFO - root - 2017-12-05 23:54:39.786299: step 54510, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 75h:16m:44s remains)
INFO - root - 2017-12-05 23:54:49.195972: step 54520, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 73h:39m:38s remains)
INFO - root - 2017-12-05 23:54:58.573737: step 54530, loss = 2.03, batch loss = 1.98 (8.2 examples/sec; 0.970 sec/batch; 74h:55m:52s remains)
INFO - root - 2017-12-05 23:55:08.072444: step 54540, loss = 2.04, batch loss = 1.98 (8.0 examples/sec; 0.994 sec/batch; 76h:47m:02s remains)
INFO - root - 2017-12-05 23:55:17.731455: step 54550, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 77h:12m:25s remains)
INFO - root - 2017-12-05 23:55:27.072999: step 54560, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 73h:27m:59s remains)
INFO - root - 2017-12-05 23:55:36.564797: step 54570, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 71h:29m:56s remains)
INFO - root - 2017-12-05 23:55:45.839853: step 54580, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 73h:44m:57s remains)
INFO - root - 2017-12-05 23:55:55.092491: step 54590, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 69h:52m:37s remains)
INFO - root - 2017-12-05 23:56:04.299540: step 54600, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 73h:31m:30s remains)
2017-12-05 23:56:05.237023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3125935 -4.3111219 -4.3099828 -4.3082366 -4.3058715 -4.3044353 -4.303937 -4.3036304 -4.3023572 -4.3001842 -4.2989511 -4.2994571 -4.300138 -4.2985396 -4.2952366][-4.3134375 -4.30888 -4.3075442 -4.3070145 -4.3057437 -4.3046656 -4.3041015 -4.3029094 -4.2990651 -4.2947493 -4.29416 -4.29849 -4.3042116 -4.3078179 -4.3076525][-4.3170147 -4.3089075 -4.3061008 -4.3055191 -4.3042183 -4.3021207 -4.2997589 -4.2942538 -4.2848878 -4.2767591 -4.2760839 -4.285203 -4.2990818 -4.3120494 -4.3177967][-4.3253994 -4.3150382 -4.3085442 -4.3046021 -4.2997422 -4.2921119 -4.2815809 -4.2657714 -4.2486405 -4.2364826 -4.2367425 -4.2532463 -4.2807045 -4.3080673 -4.3227472][-4.3347392 -4.3229675 -4.3107462 -4.2985988 -4.2825794 -4.2606325 -4.2332273 -4.2028394 -4.1793609 -4.1676712 -4.1729794 -4.2018003 -4.2483759 -4.2947054 -4.3219061][-4.3376846 -4.32421 -4.3043718 -4.2794886 -4.2443886 -4.1985717 -4.1471477 -4.1028113 -4.0822749 -4.08473 -4.1060157 -4.150166 -4.2146497 -4.2770886 -4.31527][-4.3303905 -4.3135738 -4.2826982 -4.2408104 -4.1802373 -4.1044459 -4.0250492 -3.9699347 -3.9677067 -4.0110993 -4.068141 -4.1316023 -4.2058907 -4.2715073 -4.3114786][-4.3130732 -4.2896037 -4.2441983 -4.1796265 -4.0899129 -3.9843521 -3.881367 -3.8233418 -3.8585367 -3.960218 -4.0593867 -4.1402187 -4.2154431 -4.2756004 -4.3110728][-4.29484 -4.2620292 -4.2021384 -4.1176033 -4.0077329 -3.8866355 -3.7747321 -3.728061 -3.8089674 -3.9601026 -4.0854578 -4.1681128 -4.2340469 -4.2830272 -4.3094234][-4.288291 -4.2491651 -4.1861572 -4.1020422 -4.0006342 -3.8980081 -3.8085337 -3.7822495 -3.872201 -4.0202546 -4.1367407 -4.2063441 -4.2570863 -4.2924457 -4.3075171][-4.2855973 -4.2478285 -4.1961493 -4.1324534 -4.060782 -3.9960902 -3.9422579 -3.9304781 -3.9963112 -4.1042552 -4.192328 -4.244566 -4.2812252 -4.3049417 -4.3113723][-4.2832727 -4.2548714 -4.222095 -4.1823368 -4.1377497 -4.1010203 -4.0687327 -4.0566769 -4.0918188 -4.1613126 -4.2271709 -4.2718086 -4.3025384 -4.319345 -4.3203635][-4.2863178 -4.2690911 -4.2531862 -4.2319031 -4.2049923 -4.1811061 -4.1559949 -4.1393814 -4.1540976 -4.1972265 -4.2462816 -4.2854657 -4.3136792 -4.3288507 -4.3298211][-4.2901697 -4.2816467 -4.2759819 -4.2655578 -4.2499876 -4.2341595 -4.2144089 -4.2005172 -4.2083869 -4.2358222 -4.2689009 -4.2986145 -4.3215079 -4.3357553 -4.3377557][-4.2986441 -4.2952971 -4.294064 -4.2893476 -4.2811928 -4.2745352 -4.2668037 -4.2612596 -4.2667837 -4.2797804 -4.2942495 -4.3095946 -4.3247662 -4.3367829 -4.3399663]]...]
INFO - root - 2017-12-05 23:56:14.772599: step 54610, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 72h:34m:13s remains)
INFO - root - 2017-12-05 23:56:24.230569: step 54620, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 71h:22m:08s remains)
INFO - root - 2017-12-05 23:56:33.642648: step 54630, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 72h:50m:56s remains)
INFO - root - 2017-12-05 23:56:42.946089: step 54640, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 70h:13m:13s remains)
INFO - root - 2017-12-05 23:56:52.178961: step 54650, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 71h:16m:13s remains)
INFO - root - 2017-12-05 23:57:01.740696: step 54660, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 74h:37m:11s remains)
INFO - root - 2017-12-05 23:57:11.105119: step 54670, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 71h:11m:31s remains)
INFO - root - 2017-12-05 23:57:20.526722: step 54680, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 64h:32m:01s remains)
INFO - root - 2017-12-05 23:57:29.752972: step 54690, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 67h:50m:17s remains)
INFO - root - 2017-12-05 23:57:39.164190: step 54700, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 71h:59m:06s remains)
2017-12-05 23:57:39.930021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2016139 -4.2260828 -4.2408457 -4.2339139 -4.2145357 -4.2005973 -4.191587 -4.1901469 -4.1741252 -4.1566291 -4.1546583 -4.1560445 -4.15071 -4.1429915 -4.1384196][-4.1913414 -4.2215939 -4.2449884 -4.2483454 -4.2436996 -4.2374992 -4.2280784 -4.2239928 -4.207448 -4.1925917 -4.1945443 -4.1978459 -4.1921978 -4.1824436 -4.1773229][-4.185288 -4.2149386 -4.2398114 -4.2479224 -4.2522812 -4.2521935 -4.2410588 -4.2325835 -4.2202606 -4.2113461 -4.2186236 -4.2254577 -4.2207046 -4.2108831 -4.2065196][-4.1905694 -4.2199087 -4.2417073 -4.2510953 -4.2539 -4.2524004 -4.2388906 -4.2317243 -4.2249346 -4.217495 -4.2247167 -4.2327633 -4.2299786 -4.2246213 -4.2243934][-4.2043929 -4.2277803 -4.2434568 -4.2495828 -4.244709 -4.2353292 -4.2143731 -4.2087455 -4.2145052 -4.216177 -4.2224188 -4.2312422 -4.2301588 -4.229351 -4.2366176][-4.2162204 -4.2341957 -4.2392497 -4.2332177 -4.2112975 -4.1762333 -4.1259823 -4.1236877 -4.1574917 -4.1849213 -4.2020922 -4.2215824 -4.2285748 -4.2322354 -4.2442946][-4.2203679 -4.2384682 -4.232501 -4.2055683 -4.1521163 -4.0637207 -3.9616306 -3.971333 -4.0521221 -4.1177435 -4.1609983 -4.1954527 -4.2126732 -4.2239528 -4.2434735][-4.2187309 -4.2330437 -4.2145514 -4.1680436 -4.0847168 -3.9469674 -3.8034778 -3.8445859 -3.9747832 -4.0735011 -4.1368461 -4.1764207 -4.1976376 -4.2134204 -4.2353029][-4.2265811 -4.24166 -4.2224298 -4.1804357 -4.1057024 -3.9900186 -3.8874173 -3.9312091 -4.0298162 -4.1051197 -4.1531825 -4.1756253 -4.1879592 -4.2020221 -4.2248378][-4.2488647 -4.2627978 -4.2528191 -4.2306075 -4.1836762 -4.1147485 -4.0595083 -4.0790877 -4.11952 -4.1490884 -4.1663265 -4.1703067 -4.1728883 -4.1839657 -4.2099066][-4.2639184 -4.2687068 -4.2637691 -4.2510991 -4.2183018 -4.1820397 -4.1561584 -4.1587186 -4.1601915 -4.1564655 -4.1505709 -4.1478758 -4.1515875 -4.1666894 -4.1989455][-4.26343 -4.25967 -4.2528391 -4.2403235 -4.2142215 -4.1953397 -4.1851044 -4.1796026 -4.1636286 -4.1438084 -4.125154 -4.1236119 -4.1410012 -4.1693268 -4.2095919][-4.2527938 -4.2503462 -4.2477865 -4.2337818 -4.210618 -4.2018924 -4.2007537 -4.1977043 -4.1814523 -4.1607265 -4.1384974 -4.1374774 -4.1610808 -4.1933584 -4.2324038][-4.2461691 -4.2429085 -4.2425089 -4.233664 -4.2211366 -4.221962 -4.227869 -4.2304707 -4.2235703 -4.2065215 -4.184638 -4.1815467 -4.200232 -4.2240109 -4.2539778][-4.242167 -4.236279 -4.2427993 -4.2470703 -4.2474751 -4.252337 -4.2603283 -4.268219 -4.2657132 -4.2508106 -4.2319112 -4.2267933 -4.2374134 -4.2501187 -4.267046]]...]
INFO - root - 2017-12-05 23:57:49.369917: step 54710, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 73h:45m:34s remains)
INFO - root - 2017-12-05 23:57:58.866195: step 54720, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 74h:03m:24s remains)
INFO - root - 2017-12-05 23:58:08.336273: step 54730, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 72h:09m:25s remains)
INFO - root - 2017-12-05 23:58:17.662346: step 54740, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 71h:37m:24s remains)
INFO - root - 2017-12-05 23:58:27.224578: step 54750, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 74h:16m:12s remains)
INFO - root - 2017-12-05 23:58:36.490819: step 54760, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 72h:07m:16s remains)
INFO - root - 2017-12-05 23:58:46.077780: step 54770, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.001 sec/batch; 77h:11m:39s remains)
INFO - root - 2017-12-05 23:58:55.284441: step 54780, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 74h:46m:09s remains)
INFO - root - 2017-12-05 23:59:04.541412: step 54790, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 71h:57m:16s remains)
INFO - root - 2017-12-05 23:59:14.002352: step 54800, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.023 sec/batch; 78h:55m:45s remains)
2017-12-05 23:59:14.790238: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2463145 -4.226954 -4.2215672 -4.224607 -4.240026 -4.2485251 -4.2501812 -4.2509036 -4.2583828 -4.2652426 -4.2657781 -4.2636561 -4.2597585 -4.251379 -4.2400684][-4.2481346 -4.2230682 -4.2141085 -4.2149472 -4.2299538 -4.2386217 -4.2355275 -4.2300415 -4.2296848 -4.2292647 -4.2260838 -4.22314 -4.2229748 -4.2186007 -4.209199][-4.2360244 -4.2140217 -4.2093039 -4.2112575 -4.2247119 -4.230145 -4.2218637 -4.2121849 -4.20721 -4.2047887 -4.2007885 -4.198523 -4.1991444 -4.1942391 -4.183023][-4.2198992 -4.205193 -4.2097864 -4.2191272 -4.231668 -4.2308764 -4.2152786 -4.2035112 -4.1967916 -4.1965909 -4.1959743 -4.1929278 -4.1896949 -4.183444 -4.1731615][-4.1960115 -4.1916633 -4.2070012 -4.2243342 -4.234447 -4.2244225 -4.2033644 -4.1912541 -4.1872339 -4.1942139 -4.2011795 -4.2017655 -4.1968632 -4.1918426 -4.189568][-4.1465464 -4.1471868 -4.172514 -4.1974192 -4.2070479 -4.1950908 -4.1761603 -4.1697822 -4.1721067 -4.1881447 -4.20469 -4.2134733 -4.2160292 -4.2206645 -4.2295184][-4.0746522 -4.0666904 -4.0983596 -4.13358 -4.1528172 -4.1480575 -4.1399851 -4.1458144 -4.1559944 -4.173521 -4.1942058 -4.2145004 -4.2295032 -4.2439179 -4.2594156][-4.0336771 -4.0031586 -4.0214972 -4.0539746 -4.0769305 -4.0782695 -4.0851479 -4.1082573 -4.1292849 -4.153151 -4.1784234 -4.2066951 -4.2296009 -4.2487264 -4.2636318][-4.0742321 -4.0323291 -4.0274382 -4.0372329 -4.0456057 -4.0448227 -4.0563397 -4.0856023 -4.1114635 -4.1386447 -4.1657553 -4.1915412 -4.2125511 -4.2311239 -4.24258][-4.1328912 -4.1019444 -4.0931172 -4.0900283 -4.0884881 -4.08752 -4.0929642 -4.1101232 -4.1230083 -4.1394224 -4.1553488 -4.164278 -4.173418 -4.1885042 -4.199213][-4.1792903 -4.1608315 -4.1582956 -4.1552596 -4.1524854 -4.1529441 -4.1541109 -4.1559086 -4.1537046 -4.157618 -4.1557679 -4.1386547 -4.1274142 -4.1330075 -4.1401963][-4.2200103 -4.2089143 -4.2093267 -4.2092872 -4.203825 -4.1991272 -4.1943603 -4.1890211 -4.1822739 -4.1773362 -4.1590409 -4.1225548 -4.0906973 -4.0813551 -4.0804005][-4.2497654 -4.2455373 -4.2461963 -4.2472472 -4.2393212 -4.2299304 -4.2185779 -4.2087173 -4.2007747 -4.1924648 -4.1684966 -4.1250224 -4.081069 -4.0598316 -4.0531297][-4.2623906 -4.2619019 -4.2611585 -4.259325 -4.2516527 -4.2427006 -4.2289662 -4.2182908 -4.2129693 -4.208365 -4.1897984 -4.1532111 -4.1142793 -4.0960283 -4.0928431][-4.2676349 -4.2700491 -4.2684536 -4.2637563 -4.2553959 -4.2457232 -4.2303958 -4.2195964 -4.2194576 -4.2207155 -4.211832 -4.1923933 -4.1713972 -4.1629696 -4.1648083]]...]
INFO - root - 2017-12-05 23:59:24.296889: step 54810, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 71h:42m:31s remains)
INFO - root - 2017-12-05 23:59:33.600294: step 54820, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 74h:18m:35s remains)
INFO - root - 2017-12-05 23:59:42.964986: step 54830, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 70h:17m:35s remains)
INFO - root - 2017-12-05 23:59:52.288498: step 54840, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 74h:35m:42s remains)
INFO - root - 2017-12-06 00:00:01.637638: step 54850, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 70h:20m:25s remains)
INFO - root - 2017-12-06 00:00:10.921065: step 54860, loss = 2.06, batch loss = 2.00 (7.7 examples/sec; 1.044 sec/batch; 80h:30m:20s remains)
INFO - root - 2017-12-06 00:00:20.344039: step 54870, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 74h:48m:52s remains)
INFO - root - 2017-12-06 00:00:29.557950: step 54880, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 74h:13m:15s remains)
INFO - root - 2017-12-06 00:00:38.731069: step 54890, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 75h:39m:33s remains)
INFO - root - 2017-12-06 00:00:48.335081: step 54900, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.994 sec/batch; 76h:40m:27s remains)
2017-12-06 00:00:49.081867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1275978 -4.1412797 -4.1502252 -4.13768 -4.1435986 -4.1704173 -4.1775742 -4.16934 -4.1766829 -4.1985693 -4.2112803 -4.2100039 -4.2010808 -4.2037883 -4.2119069][-4.0755577 -4.0977964 -4.1022434 -4.0897493 -4.1092825 -4.1398635 -4.1452608 -4.1295547 -4.1415925 -4.1776748 -4.21011 -4.2200623 -4.2124505 -4.2101569 -4.2130313][-4.0368772 -4.0682096 -4.0691543 -4.0619497 -4.1006846 -4.1301384 -4.126029 -4.1088347 -4.1317387 -4.1832576 -4.2248864 -4.2384787 -4.236279 -4.2339368 -4.23158][-4.0168524 -4.0529213 -4.0485473 -4.0409355 -4.0843468 -4.1056695 -4.0872111 -4.0661106 -4.1051049 -4.1754589 -4.2253456 -4.2408934 -4.2447362 -4.251164 -4.2516537][-4.039114 -4.0714269 -4.0606527 -4.0417957 -4.0716338 -4.0748668 -4.0288019 -3.9925067 -4.0584693 -4.1487508 -4.2026625 -4.219389 -4.23153 -4.2502303 -4.2586184][-4.0790758 -4.0950546 -4.0724506 -4.04471 -4.0586429 -4.036222 -3.9491811 -3.8934991 -4.0038648 -4.1179776 -4.1667018 -4.1764131 -4.1962457 -4.2265811 -4.2458634][-4.0899 -4.0881662 -4.0562153 -4.0278511 -4.0340867 -3.9914181 -3.8628628 -3.7911749 -3.9568381 -4.0974607 -4.1410789 -4.1370807 -4.1552253 -4.193697 -4.2207108][-4.0633836 -4.0558758 -4.0347095 -4.0276108 -4.0414705 -4.0037041 -3.88797 -3.8352807 -3.997972 -4.1264839 -4.1584258 -4.1402149 -4.1471977 -4.1821542 -4.2070637][-4.0669122 -4.0617752 -4.0616126 -4.0801444 -4.0937061 -4.0575333 -3.9790297 -3.9639606 -4.0863051 -4.1783657 -4.1919293 -4.1667452 -4.1658616 -4.1917462 -4.2080765][-4.09326 -4.0909858 -4.1047597 -4.1388688 -4.144928 -4.1005478 -4.0364218 -4.0447907 -4.1412611 -4.2069645 -4.2090392 -4.1842513 -4.1844959 -4.2056537 -4.2148294][-4.111259 -4.1157503 -4.1388659 -4.177331 -4.1796188 -4.1309 -4.0722985 -4.0917373 -4.1699514 -4.2200146 -4.21682 -4.1937938 -4.1970921 -4.2175279 -4.2231808][-4.1356521 -4.1447139 -4.1704931 -4.200954 -4.2016668 -4.1590238 -4.1124325 -4.135674 -4.1937723 -4.2280211 -4.2215266 -4.2033296 -4.2056708 -4.2212534 -4.2254643][-4.1576958 -4.1625724 -4.1832571 -4.2100892 -4.2141056 -4.1839452 -4.1568465 -4.1814847 -4.2208381 -4.2394304 -4.2307825 -4.2153568 -4.215394 -4.2242403 -4.2254529][-4.1813 -4.1864872 -4.20284 -4.2270856 -4.2329063 -4.2179446 -4.2076364 -4.2279053 -4.2482967 -4.2538376 -4.2429342 -4.2284617 -4.2243657 -4.224824 -4.2215838][-4.2136989 -4.2205415 -4.2280841 -4.2407551 -4.2458906 -4.2431216 -4.2457032 -4.2591286 -4.2641153 -4.25935 -4.2433023 -4.2261958 -4.21652 -4.2101665 -4.2041807]]...]
INFO - root - 2017-12-06 00:00:58.409065: step 54910, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 74h:12m:34s remains)
INFO - root - 2017-12-06 00:01:07.430118: step 54920, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 70h:59m:04s remains)
INFO - root - 2017-12-06 00:01:16.608313: step 54930, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 70h:38m:39s remains)
INFO - root - 2017-12-06 00:01:25.920745: step 54940, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 71h:30m:24s remains)
INFO - root - 2017-12-06 00:01:35.442782: step 54950, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 72h:50m:30s remains)
INFO - root - 2017-12-06 00:01:44.713886: step 54960, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 71h:40m:16s remains)
INFO - root - 2017-12-06 00:01:54.029242: step 54970, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 73h:12m:07s remains)
INFO - root - 2017-12-06 00:02:03.397503: step 54980, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 69h:11m:17s remains)
INFO - root - 2017-12-06 00:02:12.702100: step 54990, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 70h:32m:25s remains)
INFO - root - 2017-12-06 00:02:22.060483: step 55000, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.970 sec/batch; 74h:44m:24s remains)
2017-12-06 00:02:22.780841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2932525 -4.2860789 -4.2545061 -4.2083979 -4.1724057 -4.1584325 -4.1610579 -4.1764884 -4.2076864 -4.2390494 -4.2609649 -4.2746797 -4.2870622 -4.289897 -4.287847][-4.294848 -4.2854128 -4.2476325 -4.1911178 -4.1422181 -4.1223693 -4.1296334 -4.1541977 -4.1921773 -4.2268691 -4.2444077 -4.2535448 -4.2645464 -4.2731528 -4.2785277][-4.2924571 -4.2804251 -4.2404976 -4.1798248 -4.1240916 -4.0994868 -4.1066327 -4.136529 -4.1820555 -4.2200794 -4.2319846 -4.2352767 -4.2430286 -4.2551975 -4.2628069][-4.2919521 -4.2771492 -4.2364321 -4.1769514 -4.1218476 -4.0942307 -4.0946894 -4.1221867 -4.1689959 -4.2061276 -4.21284 -4.2126412 -4.2221942 -4.2400031 -4.2507305][-4.2948713 -4.2782879 -4.2381835 -4.1846323 -4.1336594 -4.1029768 -4.0900383 -4.1081648 -4.1509171 -4.1843209 -4.1879859 -4.1849074 -4.1956491 -4.2170115 -4.2350454][-4.3003364 -4.2865124 -4.2485995 -4.1986694 -4.1482868 -4.1106377 -4.0810566 -4.083981 -4.1204529 -4.15626 -4.1657257 -4.1677465 -4.1795745 -4.200171 -4.2191482][-4.3022594 -4.2912116 -4.2553325 -4.2058377 -4.1505733 -4.095459 -4.0408359 -4.0222435 -4.0627675 -4.122983 -4.1505923 -4.1598063 -4.1719742 -4.1901755 -4.2046237][-4.2963243 -4.2864676 -4.254137 -4.2067013 -4.1483455 -4.076417 -3.9932933 -3.9479389 -3.9941463 -4.0886431 -4.1477485 -4.1717715 -4.1838708 -4.1948175 -4.2021384][-4.2887683 -4.2802639 -4.2516432 -4.2080441 -4.1557732 -4.08563 -3.995748 -3.9383624 -3.9773624 -4.079042 -4.1561422 -4.194201 -4.2093897 -4.215169 -4.2166743][-4.2843671 -4.2752604 -4.2501755 -4.2138886 -4.1746845 -4.1218042 -4.053597 -4.0079951 -4.0288906 -4.1081619 -4.1797643 -4.2201419 -4.2371578 -4.2408352 -4.24098][-4.2847219 -4.2740765 -4.2483115 -4.2144194 -4.1843519 -4.1505919 -4.1090937 -4.0743551 -4.0772343 -4.1310439 -4.1909304 -4.231596 -4.2539482 -4.2634645 -4.26857][-4.2838869 -4.2743669 -4.2488284 -4.2126679 -4.1853824 -4.164897 -4.1419716 -4.1169991 -4.1102476 -4.1434903 -4.191359 -4.2323585 -4.259872 -4.2774348 -4.2891951][-4.2787256 -4.275228 -4.2565155 -4.2237377 -4.1957507 -4.1774359 -4.1610951 -4.1415486 -4.1315656 -4.1542692 -4.1956854 -4.2366776 -4.2649069 -4.2865553 -4.30322][-4.2594342 -4.2627931 -4.2582383 -4.2375436 -4.21426 -4.1971607 -4.1846843 -4.1711569 -4.165205 -4.1839447 -4.2198753 -4.2565184 -4.280612 -4.3012319 -4.3169184][-4.2383194 -4.2426596 -4.2487288 -4.2421031 -4.2278342 -4.2165227 -4.21129 -4.2074089 -4.2097235 -4.2263956 -4.2538223 -4.2810016 -4.2998538 -4.3177152 -4.32888]]...]
INFO - root - 2017-12-06 00:02:31.913322: step 55010, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 63h:52m:47s remains)
INFO - root - 2017-12-06 00:02:41.173843: step 55020, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 72h:48m:42s remains)
INFO - root - 2017-12-06 00:02:50.492029: step 55030, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 72h:09m:39s remains)
INFO - root - 2017-12-06 00:02:59.888668: step 55040, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 70h:51m:19s remains)
INFO - root - 2017-12-06 00:03:09.054059: step 55050, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 71h:47m:49s remains)
INFO - root - 2017-12-06 00:03:18.460859: step 55060, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 69h:22m:41s remains)
INFO - root - 2017-12-06 00:03:27.718031: step 55070, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 68h:58m:46s remains)
INFO - root - 2017-12-06 00:03:37.059195: step 55080, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 70h:13m:48s remains)
INFO - root - 2017-12-06 00:03:46.383274: step 55090, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 70h:47m:05s remains)
INFO - root - 2017-12-06 00:03:55.841065: step 55100, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 70h:35m:38s remains)
2017-12-06 00:03:56.719196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3525538 -4.3534403 -4.3520851 -4.3516397 -4.3519664 -4.3520036 -4.3508229 -4.3475885 -4.3434029 -4.3412075 -4.3430614 -4.3477712 -4.3518515 -4.3536358 -4.35084][-4.3533373 -4.354156 -4.3525491 -4.3512626 -4.3495965 -4.3462043 -4.3395233 -4.3296561 -4.320343 -4.3175883 -4.323319 -4.3357477 -4.3490686 -4.3579807 -4.3592682][-4.3532119 -4.35347 -4.3511868 -4.3477783 -4.3404236 -4.3271446 -4.3066859 -4.2821035 -4.2619252 -4.257195 -4.2708044 -4.2982216 -4.329617 -4.352704 -4.3618965][-4.3519974 -4.3508854 -4.3464489 -4.3379283 -4.3186874 -4.2855606 -4.2371058 -4.1842251 -4.145329 -4.1407766 -4.1723495 -4.2270408 -4.2866039 -4.3312492 -4.3532286][-4.3497767 -4.3462038 -4.3376575 -4.3184471 -4.2773032 -4.20944 -4.1174712 -4.0249848 -3.9661162 -3.971509 -4.0351853 -4.1299286 -4.2252069 -4.2953167 -4.3334737][-4.3462415 -4.3392348 -4.3236747 -4.2872667 -4.2158556 -4.1062217 -3.9714513 -3.8469338 -3.7816646 -3.8137069 -3.9213114 -4.0560617 -4.1796036 -4.2671843 -4.3159413][-4.3414922 -4.3301353 -4.3044114 -4.2484736 -4.1504602 -4.0146484 -3.8656735 -3.7471604 -3.7090101 -3.7797983 -3.9179773 -4.0653744 -4.188448 -4.2710638 -4.3145208][-4.3367991 -4.3214703 -4.28811 -4.2236214 -4.1226225 -3.9977069 -3.8809192 -3.8105946 -3.8147047 -3.8968973 -4.0197783 -4.141161 -4.2381454 -4.2992954 -4.3269506][-4.3341074 -4.3186903 -4.2863212 -4.2299871 -4.1517234 -4.06668 -4.0012803 -3.9767342 -3.9998896 -4.0648904 -4.149199 -4.2296557 -4.2931976 -4.3305473 -4.3429728][-4.3352489 -4.3245478 -4.3020754 -4.2654309 -4.21993 -4.1764359 -4.1494884 -4.1479115 -4.1706743 -4.21032 -4.2589245 -4.3043923 -4.3393064 -4.357039 -4.3583784][-4.3399181 -4.335381 -4.32397 -4.3062067 -4.2873011 -4.2712078 -4.2637062 -4.2692065 -4.2855206 -4.306612 -4.3313551 -4.3537159 -4.368804 -4.3731651 -4.3682718][-4.3454037 -4.3450522 -4.3408709 -4.3348312 -4.3301444 -4.3274665 -4.3286786 -4.336019 -4.3473821 -4.3581462 -4.369061 -4.3770757 -4.3799934 -4.3763523 -4.3686843][-4.350369 -4.3520074 -4.3514566 -4.3509741 -4.3526063 -4.3559775 -4.3606386 -4.3672018 -4.3739257 -4.3777757 -4.3796773 -4.3791008 -4.3753281 -4.3689432 -4.3623428][-4.3535814 -4.3552637 -4.3556356 -4.3563643 -4.3585906 -4.3617544 -4.3651118 -4.3686113 -4.371345 -4.3718781 -4.3705206 -4.3676753 -4.36331 -4.3583064 -4.3544044][-4.3552246 -4.3564978 -4.356606 -4.3567352 -4.35736 -4.3584652 -4.3599849 -4.3616066 -4.3624849 -4.3618808 -4.3600788 -4.3575492 -4.3545847 -4.3517127 -4.3495131]]...]
INFO - root - 2017-12-06 00:04:05.784777: step 55110, loss = 2.03, batch loss = 1.98 (8.8 examples/sec; 0.906 sec/batch; 69h:49m:51s remains)
INFO - root - 2017-12-06 00:04:15.071115: step 55120, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 71h:22m:17s remains)
INFO - root - 2017-12-06 00:04:24.494325: step 55130, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 73h:13m:29s remains)
INFO - root - 2017-12-06 00:04:33.828085: step 55140, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 70h:46m:43s remains)
INFO - root - 2017-12-06 00:04:43.319652: step 55150, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.966 sec/batch; 74h:26m:24s remains)
INFO - root - 2017-12-06 00:04:52.571024: step 55160, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 70h:56m:22s remains)
INFO - root - 2017-12-06 00:05:01.824285: step 55170, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 67h:10m:41s remains)
INFO - root - 2017-12-06 00:05:11.362800: step 55180, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.991 sec/batch; 76h:20m:48s remains)
INFO - root - 2017-12-06 00:05:20.726396: step 55190, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 75h:12m:35s remains)
INFO - root - 2017-12-06 00:05:29.826860: step 55200, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 70h:01m:23s remains)
2017-12-06 00:05:30.596182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1364202 -4.1687136 -4.1697035 -4.149857 -4.15648 -4.1762896 -4.1778712 -4.1747112 -4.17547 -4.173646 -4.1689048 -4.1687775 -4.1573453 -4.1376152 -4.1270533][-4.1576157 -4.1892562 -4.1862326 -4.1654043 -4.1671124 -4.1771965 -4.1651444 -4.1498075 -4.1495829 -4.1598692 -4.1733937 -4.18829 -4.1884627 -4.1770339 -4.17076][-4.2030783 -4.2193651 -4.2081265 -4.18152 -4.1727476 -4.1647754 -4.1306758 -4.1005359 -4.112988 -4.1551633 -4.1956382 -4.225493 -4.2344437 -4.2267723 -4.2192845][-4.2434916 -4.2460475 -4.2279806 -4.1967244 -4.1710939 -4.1362143 -4.0825191 -4.0562329 -4.0938716 -4.166914 -4.223536 -4.2588716 -4.2676477 -4.2574959 -4.2457495][-4.2588916 -4.2579894 -4.2401943 -4.2102251 -4.1654882 -4.1045051 -4.0453367 -4.0382142 -4.1012912 -4.188117 -4.2479119 -4.2760067 -4.27504 -4.2572975 -4.2409859][-4.2637134 -4.2670193 -4.254817 -4.2303658 -4.1741381 -4.0936346 -4.0284185 -4.0360894 -4.1152244 -4.2076311 -4.2658968 -4.2845397 -4.2707167 -4.2455292 -4.2251449][-4.2690268 -4.2744465 -4.2660646 -4.2425547 -4.1811924 -4.0878325 -4.0161638 -4.0310073 -4.1200805 -4.2173076 -4.275928 -4.2876244 -4.2661772 -4.2374306 -4.214004][-4.2983823 -4.301568 -4.2890744 -4.257411 -4.1898189 -4.0821257 -3.9941831 -4.010551 -4.1074286 -4.2139292 -4.2773056 -4.2928448 -4.2778082 -4.2549682 -4.2325282][-4.33934 -4.3355708 -4.3137732 -4.273684 -4.1947484 -4.0728955 -3.9744167 -3.9932373 -4.0931044 -4.2057729 -4.2749443 -4.3004322 -4.2968574 -4.2869539 -4.2738676][-4.3614831 -4.3547606 -4.3290148 -4.2831945 -4.2018809 -4.0758252 -3.977952 -3.9952161 -4.0874515 -4.1949296 -4.26501 -4.3005657 -4.3101225 -4.3117313 -4.3103185][-4.3561554 -4.3506155 -4.3235116 -4.27579 -4.2004457 -4.0900364 -4.00449 -4.01205 -4.0897427 -4.1829181 -4.2467036 -4.2914457 -4.3174438 -4.3283725 -4.332212][-4.3499832 -4.3414431 -4.3111215 -4.2633224 -4.1981311 -4.1102815 -4.0446639 -4.0493193 -4.1134343 -4.1906595 -4.24822 -4.2952433 -4.329123 -4.3438063 -4.3466406][-4.3500786 -4.3401923 -4.3104787 -4.2684031 -4.2150049 -4.1521287 -4.1072869 -4.1161327 -4.168694 -4.2308927 -4.2800021 -4.3209891 -4.3502388 -4.362824 -4.3608856][-4.3593426 -4.3527966 -4.3312635 -4.3014851 -4.2667489 -4.2288184 -4.20054 -4.2059855 -4.2408547 -4.2829432 -4.318922 -4.347445 -4.3688588 -4.3760285 -4.3708825][-4.3736687 -4.37132 -4.35851 -4.3399534 -4.3190613 -4.2968173 -4.2754188 -4.2702937 -4.2824507 -4.3046532 -4.3285055 -4.350287 -4.367507 -4.3716559 -4.3674021]]...]
INFO - root - 2017-12-06 00:05:39.899415: step 55210, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.952 sec/batch; 73h:21m:07s remains)
INFO - root - 2017-12-06 00:05:49.514719: step 55220, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 75h:11m:13s remains)
INFO - root - 2017-12-06 00:05:58.618702: step 55230, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 66h:56m:00s remains)
INFO - root - 2017-12-06 00:06:07.994838: step 55240, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 72h:48m:07s remains)
INFO - root - 2017-12-06 00:06:17.577411: step 55250, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 72h:31m:33s remains)
INFO - root - 2017-12-06 00:06:26.961579: step 55260, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.978 sec/batch; 75h:20m:34s remains)
INFO - root - 2017-12-06 00:06:36.106386: step 55270, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.936 sec/batch; 72h:03m:07s remains)
INFO - root - 2017-12-06 00:06:45.751849: step 55280, loss = 2.06, batch loss = 2.00 (7.6 examples/sec; 1.054 sec/batch; 81h:07m:53s remains)
INFO - root - 2017-12-06 00:06:54.706199: step 55290, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 71h:03m:56s remains)
INFO - root - 2017-12-06 00:07:04.023248: step 55300, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 66h:23m:40s remains)
2017-12-06 00:07:04.875524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1298165 -4.1110306 -4.0987186 -4.0960331 -4.0983367 -4.1062837 -4.1183009 -4.1347885 -4.1509089 -4.1613503 -4.165153 -4.1654639 -4.1723723 -4.1764727 -4.1777577][-4.1684322 -4.1525931 -4.1380053 -4.1362391 -4.14045 -4.1515441 -4.1618452 -4.17182 -4.1801472 -4.184319 -4.1842079 -4.18162 -4.1865077 -4.1915946 -4.1992846][-4.1901278 -4.18041 -4.1721272 -4.1793523 -4.189919 -4.2016273 -4.2049642 -4.2037272 -4.2010026 -4.1969924 -4.1879139 -4.1776018 -4.1758347 -4.1768527 -4.1843376][-4.1787877 -4.1778183 -4.1795359 -4.1949749 -4.2077084 -4.2160296 -4.2129421 -4.2060089 -4.1998472 -4.1928606 -4.1772156 -4.1604934 -4.1538134 -4.1537719 -4.1599536][-4.143579 -4.1441236 -4.1508822 -4.1707034 -4.1869969 -4.1969614 -4.1954846 -4.1870894 -4.1775331 -4.169023 -4.1509147 -4.1342545 -4.1285057 -4.1297126 -4.132947][-4.1000466 -4.085803 -4.0863461 -4.1074648 -4.1287 -4.1442919 -4.1454916 -4.1309934 -4.1099081 -4.09353 -4.0781231 -4.0703616 -4.076848 -4.0877681 -4.0912261][-4.0320888 -3.9985433 -3.9901786 -4.0122175 -4.0387774 -4.0597682 -4.0607691 -4.0336232 -3.996922 -3.9784858 -3.9744859 -3.984457 -4.0162673 -4.0490885 -4.0622325][-3.9606771 -3.923214 -3.9110105 -3.9318223 -3.9553657 -3.9685688 -3.9578493 -3.9099844 -3.8543205 -3.8386195 -3.8520761 -3.8813186 -3.9465711 -4.0135555 -4.0473394][-3.9619076 -3.93326 -3.9253693 -3.9402893 -3.9501438 -3.948971 -3.9291325 -3.8767633 -3.8178558 -3.8023593 -3.8154125 -3.8493726 -3.9258175 -4.0026994 -4.0423088][-4.0273261 -4.0060019 -4.00207 -4.0145578 -4.0177784 -4.0091906 -3.9917061 -3.9572268 -3.9225166 -3.9111741 -3.91469 -3.937762 -3.9898751 -4.03476 -4.04873][-4.1183033 -4.1008935 -4.1002464 -4.1095014 -4.1097894 -4.0998974 -4.0862103 -4.0654655 -4.0477662 -4.0371537 -4.0316882 -4.0416064 -4.0659161 -4.0761428 -4.0571852][-4.1906281 -4.1744289 -4.1723604 -4.1788859 -4.1810751 -4.1769047 -4.16851 -4.1542168 -4.1407552 -4.1290689 -4.1183786 -4.1169376 -4.1222687 -4.1124096 -4.0782166][-4.2332554 -4.2190514 -4.2169743 -4.2223277 -4.2266154 -4.2263021 -4.2207403 -4.2088518 -4.1955113 -4.1821103 -4.1701603 -4.1663976 -4.162056 -4.14229 -4.1099658][-4.2597542 -4.2476091 -4.24582 -4.24986 -4.2541227 -4.2554474 -4.252594 -4.242238 -4.2267022 -4.2109637 -4.1987858 -4.1944294 -4.1858258 -4.1639757 -4.1390514][-4.2751222 -4.2664971 -4.2655993 -4.26794 -4.2701721 -4.270452 -4.2678256 -4.2586207 -4.2442031 -4.2274723 -4.2131271 -4.204258 -4.1921048 -4.1730485 -4.1580763]]...]
INFO - root - 2017-12-06 00:07:14.164698: step 55310, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 71h:51m:15s remains)
INFO - root - 2017-12-06 00:07:23.423579: step 55320, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 73h:16m:38s remains)
INFO - root - 2017-12-06 00:07:32.908805: step 55330, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 70h:45m:38s remains)
INFO - root - 2017-12-06 00:07:42.308766: step 55340, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 73h:50m:28s remains)
INFO - root - 2017-12-06 00:07:51.754059: step 55350, loss = 2.03, batch loss = 1.97 (8.3 examples/sec; 0.965 sec/batch; 74h:15m:50s remains)
INFO - root - 2017-12-06 00:08:01.118416: step 55360, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.004 sec/batch; 77h:18m:56s remains)
INFO - root - 2017-12-06 00:08:10.592949: step 55370, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 68h:33m:52s remains)
INFO - root - 2017-12-06 00:08:19.910965: step 55380, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.989 sec/batch; 76h:08m:55s remains)
INFO - root - 2017-12-06 00:08:29.159839: step 55390, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 73h:31m:43s remains)
INFO - root - 2017-12-06 00:08:38.682341: step 55400, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 74h:14m:39s remains)
2017-12-06 00:08:39.485676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.143507 -4.1507978 -4.1741924 -4.2027221 -4.2373247 -4.2718358 -4.2906094 -4.2813025 -4.2557626 -4.2228308 -4.1846719 -4.149848 -4.1259317 -4.1449246 -4.1921339][-4.1539564 -4.1715622 -4.2011976 -4.2333689 -4.2673244 -4.29416 -4.3072624 -4.2971497 -4.2747679 -4.248044 -4.2155566 -4.1898274 -4.1697021 -4.1819711 -4.2187119][-4.1809707 -4.20465 -4.2343774 -4.263412 -4.2891703 -4.3003359 -4.3020821 -4.2881675 -4.2706771 -4.2540927 -4.235714 -4.225194 -4.2169948 -4.2235394 -4.2469449][-4.2000871 -4.2272906 -4.2556219 -4.278645 -4.2887559 -4.2785144 -4.2602596 -4.2374043 -4.2281866 -4.2333536 -4.2387815 -4.2434068 -4.2437544 -4.2429094 -4.2530756][-4.2024803 -4.2310877 -4.2567258 -4.268384 -4.2573891 -4.2204447 -4.1717658 -4.1307912 -4.1386828 -4.1860042 -4.2302756 -4.2530756 -4.2584715 -4.2481694 -4.2452335][-4.1967912 -4.2241192 -4.2478609 -4.2503405 -4.21603 -4.1450839 -4.0504351 -3.9683127 -3.9970307 -4.1033988 -4.1966858 -4.2442369 -4.2566276 -4.2387834 -4.2247148][-4.206018 -4.2294326 -4.2485666 -4.2448359 -4.1954813 -4.1002378 -3.9620602 -3.8323765 -3.8794682 -4.0359612 -4.1609149 -4.2253203 -4.24544 -4.2286229 -4.2054644][-4.2333312 -4.2539463 -4.2697458 -4.2644062 -4.2176609 -4.1298075 -4.00223 -3.8863735 -3.9262574 -4.0574832 -4.1624112 -4.2176166 -4.2352738 -4.2162814 -4.1830244][-4.2707386 -4.287919 -4.2991376 -4.2955542 -4.2604265 -4.1990395 -4.1156979 -4.0459795 -4.0658879 -4.1380911 -4.1917067 -4.2190466 -4.2242241 -4.1930919 -4.1436381][-4.30694 -4.3200459 -4.3278527 -4.3263574 -4.3048964 -4.2687674 -4.2228246 -4.1882343 -4.1967287 -4.2214708 -4.2304859 -4.2293873 -4.2142096 -4.1638007 -4.0977225][-4.3226123 -4.332335 -4.3363581 -4.3352652 -4.3255024 -4.3106661 -4.2912836 -4.2767968 -4.2818666 -4.2847438 -4.2735634 -4.2557325 -4.2245154 -4.1598997 -4.0859175][-4.3204966 -4.3268456 -4.3277779 -4.3293605 -4.3307142 -4.3307981 -4.3269138 -4.3248386 -4.3288713 -4.3249 -4.3058829 -4.2817597 -4.24341 -4.1845436 -4.1216512][-4.3126397 -4.3143353 -4.3134913 -4.3178496 -4.3265591 -4.3340344 -4.3379774 -4.3446555 -4.3520484 -4.3464789 -4.3305273 -4.3100567 -4.2795382 -4.2398639 -4.1992254][-4.3057446 -4.3045983 -4.3040457 -4.308342 -4.3170652 -4.3246388 -4.331511 -4.3420153 -4.3512468 -4.3472614 -4.3392072 -4.3292303 -4.3103924 -4.2896733 -4.2692986][-4.3005571 -4.2990937 -4.2990184 -4.3012762 -4.3065324 -4.3125844 -4.3199296 -4.3285604 -4.3367262 -4.3339415 -4.3317595 -4.3312273 -4.3236494 -4.3139815 -4.3044825]]...]
INFO - root - 2017-12-06 00:08:48.763611: step 55410, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 73h:08m:48s remains)
INFO - root - 2017-12-06 00:08:58.103946: step 55420, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 73h:55m:37s remains)
INFO - root - 2017-12-06 00:09:07.347377: step 55430, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 67h:05m:03s remains)
INFO - root - 2017-12-06 00:09:16.587763: step 55440, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 72h:51m:43s remains)
INFO - root - 2017-12-06 00:09:25.964276: step 55450, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 69h:46m:41s remains)
INFO - root - 2017-12-06 00:09:35.150071: step 55460, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 65h:39m:55s remains)
INFO - root - 2017-12-06 00:09:44.367878: step 55470, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 68h:32m:05s remains)
INFO - root - 2017-12-06 00:09:53.688840: step 55480, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.965 sec/batch; 74h:15m:46s remains)
INFO - root - 2017-12-06 00:10:03.116536: step 55490, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 72h:29m:58s remains)
INFO - root - 2017-12-06 00:10:12.429957: step 55500, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 70h:05m:15s remains)
2017-12-06 00:10:13.143334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1090908 -4.1034236 -4.118072 -4.161068 -4.1886611 -4.1736517 -4.125864 -4.1219931 -4.1459947 -4.1857443 -4.2289944 -4.2650514 -4.2901716 -4.300487 -4.3025146][-4.1379437 -4.136548 -4.1515036 -4.1846771 -4.20404 -4.1835384 -4.1341186 -4.1182847 -4.1229196 -4.1581845 -4.2084966 -4.2528362 -4.2871833 -4.3044381 -4.30785][-4.183044 -4.1895084 -4.2014976 -4.2163277 -4.2193975 -4.1944656 -4.1489897 -4.1225476 -4.1057391 -4.1342392 -4.1861529 -4.2351656 -4.2779994 -4.3005943 -4.3048973][-4.2218151 -4.2310319 -4.2366066 -4.2367721 -4.2264566 -4.2002363 -4.1596174 -4.1233006 -4.0960093 -4.1248736 -4.1766229 -4.2268138 -4.2736149 -4.2974467 -4.3018241][-4.245966 -4.2530522 -4.2532969 -4.2471151 -4.2325134 -4.2060146 -4.1554055 -4.0923052 -4.0606933 -4.1112261 -4.1758132 -4.2323508 -4.2776828 -4.2976503 -4.3021073][-4.262455 -4.26348 -4.2609897 -4.2525439 -4.2363353 -4.1992726 -4.1137371 -4.0027151 -3.9750359 -4.0746078 -4.1742454 -4.2427258 -4.2839994 -4.2984934 -4.304059][-4.2544966 -4.2474489 -4.241694 -4.2317681 -4.2139664 -4.1574421 -4.0291486 -3.8743482 -3.8745265 -4.0313425 -4.1648512 -4.2440233 -4.2817168 -4.2923217 -4.297967][-4.2431517 -4.2297606 -4.2219706 -4.2137508 -4.19929 -4.1393518 -4.0092497 -3.8722823 -3.9022171 -4.0559511 -4.1780052 -4.2487321 -4.2789645 -4.2872014 -4.2914181][-4.2511315 -4.2412915 -4.229465 -4.2207556 -4.2110448 -4.1666369 -4.0747757 -3.9940386 -4.0333018 -4.1398315 -4.2223654 -4.2706604 -4.2879996 -4.2921615 -4.2933979][-4.2682667 -4.2621155 -4.2485275 -4.2382865 -4.2300982 -4.196557 -4.1380348 -4.0971251 -4.139154 -4.2132993 -4.2699809 -4.299294 -4.3040166 -4.3022838 -4.2994776][-4.2903953 -4.2857251 -4.2723455 -4.2640877 -4.2541518 -4.2229867 -4.1826735 -4.1647062 -4.2063947 -4.2638831 -4.3044343 -4.3172083 -4.3125167 -4.3077011 -4.301971][-4.2975535 -4.2981043 -4.2914453 -4.2844205 -4.2735419 -4.2471614 -4.2200065 -4.2159505 -4.2530441 -4.2984405 -4.32338 -4.3233776 -4.315062 -4.3089952 -4.3017893][-4.2902355 -4.2946668 -4.2939339 -4.290669 -4.2846732 -4.26914 -4.2552543 -4.2568016 -4.2833767 -4.3128614 -4.3222752 -4.3162003 -4.3090339 -4.3026347 -4.2936506][-4.290906 -4.2961187 -4.2938185 -4.2908168 -4.2887406 -4.2812376 -4.273417 -4.27537 -4.2913733 -4.3056936 -4.3071241 -4.302032 -4.2980971 -4.2924457 -4.2833567][-4.2896667 -4.2945576 -4.2937121 -4.2939668 -4.2948079 -4.2886119 -4.2805395 -4.2787995 -4.2858663 -4.2919474 -4.2929888 -4.2917285 -4.290679 -4.2869153 -4.2798786]]...]
INFO - root - 2017-12-06 00:10:22.541199: step 55510, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 72h:05m:33s remains)
INFO - root - 2017-12-06 00:10:31.934054: step 55520, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 71h:26m:24s remains)
INFO - root - 2017-12-06 00:10:41.271589: step 55530, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 73h:53m:28s remains)
INFO - root - 2017-12-06 00:10:50.717778: step 55540, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 71h:50m:12s remains)
INFO - root - 2017-12-06 00:10:59.937234: step 55550, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 71h:18m:39s remains)
INFO - root - 2017-12-06 00:11:08.936266: step 55560, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 70h:32m:14s remains)
INFO - root - 2017-12-06 00:11:18.152670: step 55570, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 66h:02m:37s remains)
INFO - root - 2017-12-06 00:11:27.477268: step 55580, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 72h:59m:17s remains)
INFO - root - 2017-12-06 00:11:36.876064: step 55590, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 71h:10m:35s remains)
INFO - root - 2017-12-06 00:11:46.121689: step 55600, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 69h:40m:36s remains)
2017-12-06 00:11:46.891132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1491332 -4.1255016 -4.1163363 -4.1046634 -4.0875998 -4.0711923 -4.0637236 -4.0835447 -4.12499 -4.1701708 -4.199286 -4.2100444 -4.1965132 -4.1705585 -4.1435747][-4.1344557 -4.1059012 -4.0908961 -4.0774679 -4.06513 -4.0567851 -4.0571847 -4.081367 -4.1232281 -4.1674585 -4.1943431 -4.20505 -4.1863012 -4.1512108 -4.1155825][-4.1364117 -4.1113253 -4.0921221 -4.0702066 -4.0511785 -4.037281 -4.0413761 -4.0677824 -4.1059804 -4.1466627 -4.1768785 -4.1891146 -4.1674333 -4.12763 -4.0819221][-4.1609788 -4.1368556 -4.12132 -4.0975 -4.0605564 -4.023849 -4.0170012 -4.0398436 -4.0709558 -4.1097684 -4.1497917 -4.1690536 -4.1507292 -4.1069646 -4.0489964][-4.1979957 -4.1750851 -4.1624751 -4.1396303 -4.0888724 -4.0311828 -4.0071783 -4.018117 -4.0412784 -4.0738769 -4.1175919 -4.1490173 -4.1392817 -4.0952 -4.0348492][-4.2223473 -4.2041564 -4.1953011 -4.1740675 -4.1188364 -4.053648 -4.0178547 -4.0100756 -4.0231776 -4.0498586 -4.0953879 -4.1392388 -4.1440086 -4.1092277 -4.0593433][-4.2154546 -4.2005363 -4.19427 -4.1734266 -4.1210685 -4.0612311 -4.0189209 -3.997813 -4.0078483 -4.035686 -4.08286 -4.1348886 -4.1517649 -4.130713 -4.0971007][-4.198215 -4.1844325 -4.1757855 -4.1561379 -4.1109285 -4.0631852 -4.022923 -3.997628 -4.0049887 -4.0344677 -4.0836887 -4.1395125 -4.1619387 -4.150806 -4.1315603][-4.1828771 -4.166688 -4.155232 -4.143724 -4.112762 -4.0827761 -4.0582051 -4.0366716 -4.0364952 -4.0548973 -4.0964012 -4.1519313 -4.1728072 -4.1605873 -4.1430297][-4.1761465 -4.1591887 -4.1481194 -4.1480808 -4.1306224 -4.1162887 -4.1080575 -4.0955009 -4.0887775 -4.0900521 -4.1140866 -4.1570358 -4.1738348 -4.1599116 -4.1403422][-4.18269 -4.1632986 -4.1537147 -4.1543436 -4.1462502 -4.1459832 -4.1526995 -4.14913 -4.1403265 -4.1288157 -4.1309276 -4.153101 -4.1676221 -4.1579995 -4.14129][-4.2089562 -4.186758 -4.1771131 -4.1741614 -4.1679425 -4.17274 -4.1881022 -4.1909547 -4.1815419 -4.1635628 -4.1488028 -4.1553345 -4.1660585 -4.1619325 -4.1532445][-4.23061 -4.2071 -4.1975317 -4.1938033 -4.1899724 -4.19666 -4.2134919 -4.2161808 -4.2050939 -4.1867137 -4.167552 -4.1657076 -4.1709661 -4.169569 -4.1650095][-4.2363033 -4.2170238 -4.2092013 -4.2082272 -4.20615 -4.2123108 -4.2269268 -4.2328072 -4.2252479 -4.2107277 -4.193965 -4.1887622 -4.1898823 -4.18894 -4.1873426][-4.2483034 -4.2349176 -4.2334642 -4.2363219 -4.2382107 -4.2442017 -4.2541447 -4.2599163 -4.2569418 -4.2482481 -4.2369809 -4.2339072 -4.2324452 -4.2289977 -4.2279754]]...]
INFO - root - 2017-12-06 00:11:56.023849: step 55610, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 73h:06m:34s remains)
INFO - root - 2017-12-06 00:12:05.566489: step 55620, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.942 sec/batch; 72h:27m:38s remains)
INFO - root - 2017-12-06 00:12:14.923292: step 55630, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 70h:11m:47s remains)
INFO - root - 2017-12-06 00:12:24.194038: step 55640, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 68h:23m:23s remains)
INFO - root - 2017-12-06 00:12:33.316441: step 55650, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 68h:24m:16s remains)
INFO - root - 2017-12-06 00:12:42.576176: step 55660, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 71h:22m:38s remains)
INFO - root - 2017-12-06 00:12:51.935392: step 55670, loss = 2.03, batch loss = 1.97 (8.3 examples/sec; 0.964 sec/batch; 74h:09m:33s remains)
INFO - root - 2017-12-06 00:13:01.354722: step 55680, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 68h:36m:19s remains)
INFO - root - 2017-12-06 00:13:10.637431: step 55690, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.003 sec/batch; 77h:06m:48s remains)
INFO - root - 2017-12-06 00:13:19.784236: step 55700, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.907 sec/batch; 69h:45m:12s remains)
2017-12-06 00:13:20.537468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.177103 -4.1658673 -4.1694889 -4.1841211 -4.2024331 -4.2249832 -4.2457929 -4.2545033 -4.2506022 -4.2331247 -4.2019405 -4.15912 -4.1182785 -4.1083622 -4.1384997][-4.2124653 -4.2060895 -4.2051954 -4.2069316 -4.2124047 -4.2184567 -4.2256842 -4.2280211 -4.2217393 -4.2036915 -4.1774869 -4.1351709 -4.0935087 -4.0844536 -4.1159849][-4.22295 -4.2190566 -4.2150583 -4.20861 -4.204576 -4.2016258 -4.203918 -4.2058115 -4.199676 -4.18078 -4.1566558 -4.1173029 -4.0736284 -4.0628152 -4.0909061][-4.2067232 -4.2030277 -4.2007136 -4.1917605 -4.180377 -4.1728568 -4.1756525 -4.1808553 -4.1790433 -4.1611929 -4.1383963 -4.1036596 -4.0621996 -4.0513391 -4.0763216][-4.1859241 -4.1851625 -4.1860204 -4.1755266 -4.1526251 -4.134624 -4.1343212 -4.1428123 -4.1488867 -4.1408405 -4.1273665 -4.1009173 -4.0665765 -4.0590038 -4.0812531][-4.1632061 -4.1651149 -4.1664438 -4.1516476 -4.1160035 -4.0855217 -4.0844936 -4.1034513 -4.1243606 -4.1337218 -4.1365061 -4.1213975 -4.0929275 -4.0843077 -4.0986972][-4.142487 -4.1390553 -4.1379075 -4.1183519 -4.0692768 -4.021996 -4.0140438 -4.0445323 -4.0878258 -4.1219349 -4.143352 -4.141283 -4.1201692 -4.1165805 -4.1272879][-4.1372547 -4.1248884 -4.1181808 -4.0921288 -4.030962 -3.9631915 -3.9342668 -3.9600887 -4.0169778 -4.0771904 -4.1226611 -4.1396971 -4.1334014 -4.1403046 -4.1558185][-4.1446786 -4.1321115 -4.1283288 -4.1082425 -4.0561724 -3.9933391 -3.9532359 -3.9522207 -3.9877238 -4.042634 -4.0944576 -4.1192989 -4.1199551 -4.1361752 -4.1627722][-4.1736774 -4.167532 -4.16854 -4.1569757 -4.1194906 -4.076314 -4.0491118 -4.0387092 -4.05113 -4.0830135 -4.1164684 -4.1305842 -4.1262178 -4.1408839 -4.1715431][-4.214438 -4.214098 -4.2168117 -4.2077217 -4.1778741 -4.1489906 -4.139225 -4.1399088 -4.14854 -4.1624575 -4.1746054 -4.17211 -4.1564107 -4.1619353 -4.1872082][-4.2357569 -4.2377782 -4.243041 -4.2391262 -4.2172208 -4.1956749 -4.1920829 -4.1952081 -4.1979122 -4.2000241 -4.1992092 -4.1848369 -4.1579227 -4.1574359 -4.1786375][-4.2290988 -4.2280045 -4.2336593 -4.2364864 -4.223927 -4.2060676 -4.197773 -4.1927977 -4.1884532 -4.18765 -4.1834841 -4.1594343 -4.1232786 -4.1201878 -4.1454535][-4.1995659 -4.1937094 -4.19433 -4.1955361 -4.1876888 -4.1761432 -4.1694012 -4.1607952 -4.1530781 -4.1511779 -4.1440949 -4.1082497 -4.06205 -4.0605726 -4.1015587][-4.1698036 -4.1624584 -4.1595058 -4.1575627 -4.1525722 -4.1450667 -4.1380782 -4.1267867 -4.118886 -4.1178594 -4.1082807 -4.0655723 -4.0147138 -4.0124655 -4.0615926]]...]
INFO - root - 2017-12-06 00:13:29.677946: step 55710, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 67h:36m:32s remains)
INFO - root - 2017-12-06 00:13:39.105433: step 55720, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 73h:46m:00s remains)
INFO - root - 2017-12-06 00:13:48.541216: step 55730, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 69h:23m:29s remains)
INFO - root - 2017-12-06 00:13:57.847781: step 55740, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 67h:07m:12s remains)
INFO - root - 2017-12-06 00:14:07.254557: step 55750, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 74h:11m:50s remains)
INFO - root - 2017-12-06 00:14:16.670166: step 55760, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 72h:27m:28s remains)
INFO - root - 2017-12-06 00:14:26.177169: step 55770, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 73h:03m:39s remains)
INFO - root - 2017-12-06 00:14:35.425765: step 55780, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 70h:41m:10s remains)
INFO - root - 2017-12-06 00:14:44.821997: step 55790, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 69h:00m:33s remains)
INFO - root - 2017-12-06 00:14:54.213803: step 55800, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 71h:14m:49s remains)
2017-12-06 00:14:55.017327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1085911 -4.1050444 -4.0999217 -4.0988717 -4.1013789 -4.1159334 -4.1445022 -4.1601129 -4.14776 -4.1218519 -4.1059108 -4.0956225 -4.0885363 -4.0791235 -4.0938644][-4.1112752 -4.107295 -4.1099854 -4.1108923 -4.1152787 -4.1334472 -4.1667747 -4.1850657 -4.1726589 -4.145133 -4.1348052 -4.1388469 -4.1407356 -4.1361084 -4.1430435][-4.1313319 -4.1183066 -4.1105251 -4.1015797 -4.1000204 -4.1192966 -4.1593552 -4.1874094 -4.184866 -4.1630306 -4.1565151 -4.1684413 -4.1778417 -4.177887 -4.1815977][-4.1382723 -4.107172 -4.0871158 -4.07605 -4.0715876 -4.0892367 -4.129787 -4.1599731 -4.1623988 -4.1471 -4.1442742 -4.1660357 -4.1849575 -4.1904984 -4.1940575][-4.1336513 -4.0873308 -4.062561 -4.0573111 -4.0572047 -4.0677252 -4.0986338 -4.12111 -4.12735 -4.126133 -4.1376853 -4.1652231 -4.1870809 -4.1953773 -4.2040682][-4.1425114 -4.0850077 -4.0545907 -4.0528011 -4.0476794 -4.0450182 -4.0673442 -4.0931592 -4.1136518 -4.1293163 -4.1545105 -4.1843243 -4.2053118 -4.21466 -4.2271838][-4.145515 -4.0948873 -4.064414 -4.0580525 -4.0390038 -4.0208554 -4.0347075 -4.06853 -4.1065044 -4.1394348 -4.1724105 -4.2067609 -4.2292533 -4.240046 -4.2496986][-4.1233106 -4.0922303 -4.0752583 -4.064363 -4.0368848 -4.0115266 -4.0251026 -4.0578623 -4.1001925 -4.1402636 -4.1764545 -4.2118788 -4.2372603 -4.2500143 -4.2559552][-4.09683 -4.0849233 -4.0836773 -4.0767746 -4.0543456 -4.041523 -4.0613933 -4.0829611 -4.1110911 -4.1426859 -4.177186 -4.2085667 -4.2364712 -4.2511091 -4.254468][-4.0592785 -4.0605507 -4.0813866 -4.0897851 -4.0824661 -4.0890179 -4.117147 -4.1294174 -4.1378717 -4.1522684 -4.1750703 -4.2036586 -4.2315378 -4.2478781 -4.2499671][-4.0491047 -4.0530586 -4.0868511 -4.101553 -4.102015 -4.1181178 -4.1497393 -4.1636996 -4.1627 -4.1650276 -4.1726856 -4.1955452 -4.2214189 -4.2406015 -4.2460761][-4.0734763 -4.0734172 -4.1041794 -4.1139355 -4.1130409 -4.1286192 -4.1576285 -4.1760178 -4.1756516 -4.1681828 -4.1648016 -4.1805568 -4.2067084 -4.2324696 -4.2457604][-4.099247 -4.0935445 -4.1123195 -4.1180944 -4.1205158 -4.1301646 -4.1511259 -4.1695137 -4.170259 -4.156342 -4.1461105 -4.1586742 -4.1846418 -4.2140007 -4.23421][-4.11233 -4.1040363 -4.1156783 -4.1234241 -4.1324749 -4.135376 -4.14305 -4.1494832 -4.1470046 -4.1319132 -4.119967 -4.1335421 -4.1606112 -4.1946244 -4.2221417][-4.1266966 -4.1086617 -4.1119738 -4.1291738 -4.1468253 -4.1489258 -4.1454597 -4.1385441 -4.1301928 -4.1139011 -4.104116 -4.11861 -4.1493049 -4.1886024 -4.2209573]]...]
INFO - root - 2017-12-06 00:15:04.488973: step 55810, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 71h:43m:36s remains)
INFO - root - 2017-12-06 00:15:13.872461: step 55820, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 71h:10m:13s remains)
INFO - root - 2017-12-06 00:15:23.112463: step 55830, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 69h:21m:29s remains)
INFO - root - 2017-12-06 00:15:32.335979: step 55840, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 69h:36m:29s remains)
INFO - root - 2017-12-06 00:15:41.705904: step 55850, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 75h:20m:03s remains)
INFO - root - 2017-12-06 00:15:51.137600: step 55860, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 76h:39m:15s remains)
INFO - root - 2017-12-06 00:16:00.476833: step 55870, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 70h:44m:03s remains)
INFO - root - 2017-12-06 00:16:09.786241: step 55880, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 69h:20m:19s remains)
INFO - root - 2017-12-06 00:16:19.246273: step 55890, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 71h:15m:53s remains)
INFO - root - 2017-12-06 00:16:28.656038: step 55900, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.988 sec/batch; 75h:54m:16s remains)
2017-12-06 00:16:29.412892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1213679 -4.1942797 -4.2589879 -4.3041124 -4.3284359 -4.3347211 -4.3317428 -4.3226614 -4.3139977 -4.3101845 -4.3076549 -4.3017311 -4.2916532 -4.2762961 -4.2583342][-4.2222061 -4.2700133 -4.3095007 -4.3307204 -4.3373876 -4.3330665 -4.325655 -4.3186393 -4.3156981 -4.3161578 -4.3164883 -4.3136125 -4.3077626 -4.2975907 -4.2857041][-4.3018985 -4.3272405 -4.3428 -4.342947 -4.3307424 -4.3131766 -4.3006806 -4.29939 -4.3062434 -4.3152795 -4.32088 -4.3212142 -4.3170671 -4.3082514 -4.2987127][-4.3427949 -4.3538718 -4.3529677 -4.3328342 -4.3017192 -4.2721543 -4.2587113 -4.266561 -4.2879462 -4.3105979 -4.32511 -4.3288207 -4.323494 -4.3121095 -4.3009419][-4.353395 -4.3543577 -4.3402729 -4.3006988 -4.2500167 -4.2085223 -4.1974316 -4.2181253 -4.2575665 -4.29802 -4.3254142 -4.3363051 -4.3316464 -4.3172426 -4.3029447][-4.3451982 -4.3345461 -4.3053188 -4.2470121 -4.1774945 -4.1208124 -4.1056933 -4.1382189 -4.1998935 -4.2638912 -4.3078442 -4.3297739 -4.3326874 -4.3237472 -4.3110938][-4.3341851 -4.3144484 -4.26922 -4.1915212 -4.1004767 -4.0172634 -3.9818265 -4.0212584 -4.1142044 -4.20762 -4.270752 -4.3051062 -4.3194847 -4.319941 -4.3139429][-4.33243 -4.3099651 -4.2584925 -4.1702228 -4.0566936 -3.9350131 -3.8558681 -3.8869686 -4.0119705 -4.1378517 -4.2206841 -4.2682586 -4.2922659 -4.2991691 -4.2994223][-4.3385415 -4.3233781 -4.2814264 -4.2021394 -4.089416 -3.951225 -3.8330259 -3.8250151 -3.9451504 -4.0791225 -4.170989 -4.2252541 -4.2544622 -4.2641373 -4.2647462][-4.3458686 -4.3409643 -4.3177996 -4.2632556 -4.1743178 -4.0583534 -3.9498734 -3.9131255 -3.9807279 -4.0830178 -4.1616106 -4.2074122 -4.2283745 -4.2304549 -4.2250891][-4.3492727 -4.350481 -4.3431664 -4.3151369 -4.2593923 -4.1773109 -4.0961747 -4.0567451 -4.0861864 -4.1533632 -4.2107258 -4.2405643 -4.2437658 -4.2270608 -4.2057738][-4.3510056 -4.3549442 -4.3570294 -4.3494382 -4.323245 -4.2741022 -4.219543 -4.1872034 -4.1993771 -4.2429252 -4.2822776 -4.2976632 -4.2874088 -4.2571311 -4.2231421][-4.349494 -4.3540592 -4.3597922 -4.3635859 -4.3575664 -4.3348475 -4.3039713 -4.2831707 -4.2892804 -4.3150482 -4.3358254 -4.3373823 -4.3173347 -4.2837944 -4.2513971][-4.3469529 -4.3508625 -4.3559785 -4.36124 -4.3617978 -4.354218 -4.342536 -4.3350115 -4.3410392 -4.3553185 -4.3627019 -4.3499546 -4.3195324 -4.2835822 -4.25748][-4.3443036 -4.3471212 -4.3506494 -4.3538136 -4.3551955 -4.3533869 -4.3513255 -4.35201 -4.3587136 -4.3660483 -4.3634796 -4.3380136 -4.294313 -4.2522168 -4.2325439]]...]
INFO - root - 2017-12-06 00:16:38.934979: step 55910, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 73h:41m:30s remains)
INFO - root - 2017-12-06 00:16:48.316865: step 55920, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 71h:58m:00s remains)
INFO - root - 2017-12-06 00:16:57.592297: step 55930, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 72h:56m:08s remains)
INFO - root - 2017-12-06 00:17:07.071229: step 55940, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 74h:05m:46s remains)
INFO - root - 2017-12-06 00:17:16.414245: step 55950, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 73h:54m:54s remains)
INFO - root - 2017-12-06 00:17:25.765018: step 55960, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.920 sec/batch; 70h:38m:00s remains)
INFO - root - 2017-12-06 00:17:35.276406: step 55970, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 72h:41m:44s remains)
INFO - root - 2017-12-06 00:17:44.748270: step 55980, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 70h:19m:10s remains)
INFO - root - 2017-12-06 00:17:54.153726: step 55990, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 72h:18m:36s remains)
INFO - root - 2017-12-06 00:18:03.594932: step 56000, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 70h:09m:05s remains)
2017-12-06 00:18:04.396000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3233447 -4.3161626 -4.3084583 -4.3027411 -4.296525 -4.2898245 -4.2845588 -4.2799993 -4.2687521 -4.2565784 -4.2571993 -4.2601266 -4.2541203 -4.241888 -4.2250319][-4.3172588 -4.310061 -4.3007879 -4.2918005 -4.2844586 -4.2793355 -4.28035 -4.2840266 -4.2771811 -4.2643256 -4.262 -4.2657542 -4.2655854 -4.2605119 -4.2462778][-4.3100848 -4.3058224 -4.2970505 -4.2841048 -4.2730346 -4.2666254 -4.2672238 -4.272357 -4.2687049 -4.2551956 -4.2527986 -4.2621861 -4.270308 -4.272428 -4.2591391][-4.3086667 -4.3072925 -4.2974472 -4.2795858 -4.2625656 -4.2502346 -4.244813 -4.2478805 -4.2441726 -4.2302113 -4.230155 -4.24762 -4.2634168 -4.2697816 -4.2571368][-4.3142867 -4.3129077 -4.2987571 -4.27423 -4.2505603 -4.2306547 -4.2185054 -4.2170916 -4.2115946 -4.2033191 -4.2112794 -4.2367115 -4.2562056 -4.2631717 -4.2507811][-4.3239026 -4.321104 -4.3011479 -4.2711263 -4.2390876 -4.2108307 -4.1922345 -4.1872263 -4.1832333 -4.1869812 -4.2065659 -4.2364745 -4.25662 -4.26384 -4.255013][-4.3351192 -4.3329306 -4.307621 -4.2684383 -4.2264233 -4.1894913 -4.1690178 -4.1679173 -4.1717525 -4.1856809 -4.21197 -4.2427435 -4.2606516 -4.2683368 -4.2647057][-4.3455577 -4.3439407 -4.3129926 -4.261899 -4.2071643 -4.1651769 -4.1503253 -4.1589851 -4.1770592 -4.2011623 -4.2266722 -4.2509437 -4.2634311 -4.2665081 -4.2626495][-4.3499 -4.346323 -4.3084192 -4.2439966 -4.1792445 -4.1349597 -4.128056 -4.1529188 -4.1884708 -4.2204485 -4.2401633 -4.2541337 -4.2614183 -4.2608628 -4.2531576][-4.3468027 -4.3402839 -4.29578 -4.2220559 -4.1528611 -4.1091356 -4.1100698 -4.1457963 -4.1922126 -4.2273631 -4.2432508 -4.2493472 -4.2508979 -4.2501802 -4.2401404][-4.3416972 -4.3339715 -4.287684 -4.2116075 -4.14289 -4.1045189 -4.1094341 -4.1446576 -4.1911139 -4.2277427 -4.243474 -4.2448192 -4.2423553 -4.241971 -4.2308717][-4.3396854 -4.3325386 -4.29072 -4.2215085 -4.1598754 -4.126905 -4.1300683 -4.1569943 -4.1970983 -4.234807 -4.25001 -4.2507486 -4.2488136 -4.247211 -4.2336588][-4.3414354 -4.3358097 -4.3042612 -4.2495718 -4.2000442 -4.173727 -4.1735764 -4.1912947 -4.2234836 -4.2584605 -4.272521 -4.2721815 -4.2702131 -4.2683144 -4.2563353][-4.3463693 -4.3434467 -4.3245544 -4.2875853 -4.2533331 -4.2350392 -4.2316747 -4.24271 -4.2672458 -4.294785 -4.3054495 -4.3055344 -4.3055711 -4.3043556 -4.2944107][-4.3515162 -4.3511605 -4.3427863 -4.323308 -4.3047962 -4.2940211 -4.2897935 -4.2956657 -4.313396 -4.3314366 -4.3375611 -4.3373818 -4.3386316 -4.3389282 -4.33196]]...]
INFO - root - 2017-12-06 00:18:13.776637: step 56010, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 68h:49m:05s remains)
INFO - root - 2017-12-06 00:18:23.222762: step 56020, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 72h:07m:30s remains)
INFO - root - 2017-12-06 00:18:32.610559: step 56030, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 72h:12m:17s remains)
INFO - root - 2017-12-06 00:18:41.969320: step 56040, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.994 sec/batch; 76h:22m:10s remains)
INFO - root - 2017-12-06 00:18:51.236732: step 56050, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.944 sec/batch; 72h:28m:05s remains)
INFO - root - 2017-12-06 00:19:00.959074: step 56060, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 73h:56m:33s remains)
INFO - root - 2017-12-06 00:19:10.391614: step 56070, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 72h:08m:55s remains)
INFO - root - 2017-12-06 00:19:19.869591: step 56080, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 75h:53m:22s remains)
INFO - root - 2017-12-06 00:19:29.425018: step 56090, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 75h:19m:00s remains)
INFO - root - 2017-12-06 00:19:38.721356: step 56100, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 72h:59m:24s remains)
2017-12-06 00:19:39.554714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2042871 -4.1914411 -4.1761761 -4.182529 -4.1964493 -4.2045503 -4.1913152 -4.18094 -4.1770096 -4.1768942 -4.1733937 -4.1660914 -4.1589794 -4.1644654 -4.1675768][-4.2174954 -4.229445 -4.2337332 -4.2460036 -4.2621307 -4.2669277 -4.2443786 -4.224638 -4.2174039 -4.221859 -4.2229576 -4.219203 -4.2155209 -4.2239408 -4.23058][-4.2388926 -4.2613626 -4.2725344 -4.2810411 -4.2914176 -4.2884216 -4.26116 -4.2384095 -4.2329259 -4.2445168 -4.2528892 -4.2562251 -4.2591372 -4.2689662 -4.275156][-4.2752352 -4.2934108 -4.301877 -4.3008904 -4.2965746 -4.2811723 -4.2495594 -4.2231579 -4.217051 -4.2331791 -4.2514544 -4.2661519 -4.2815175 -4.2977438 -4.3050566][-4.3132463 -4.321435 -4.3191733 -4.3073807 -4.2904339 -4.2634611 -4.222302 -4.1834035 -4.171174 -4.1889958 -4.2127457 -4.2350407 -4.26204 -4.2884307 -4.3015614][-4.3234725 -4.3211775 -4.3066034 -4.2845292 -4.2574072 -4.2219944 -4.1713219 -4.1149454 -4.0873566 -4.1058431 -4.1399431 -4.170846 -4.2089019 -4.2458777 -4.2639751][-4.3011403 -4.2896461 -4.2665267 -4.2383895 -4.2028289 -4.1549077 -4.0897322 -4.011055 -3.9565446 -3.972542 -4.0229754 -4.0697718 -4.1225557 -4.1738667 -4.2029858][-4.2444506 -4.2288 -4.20721 -4.18031 -4.1410441 -4.0798612 -4.0012989 -3.8968093 -3.8051088 -3.8165832 -3.8876002 -3.9562597 -4.0307231 -4.1041288 -4.1541085][-4.1976089 -4.1869168 -4.176578 -4.1587591 -4.1247821 -4.0624671 -3.9838021 -3.8754148 -3.7679694 -3.7714632 -3.8490045 -3.9289389 -4.0130692 -4.0951118 -4.155715][-4.2133913 -4.21127 -4.2125292 -4.2061419 -4.1862373 -4.1398053 -4.0830727 -4.0040741 -3.9163196 -3.9035549 -3.95749 -4.0198359 -4.083221 -4.1497669 -4.2006278][-4.2722092 -4.2730875 -4.2775145 -4.276834 -4.2672291 -4.2365184 -4.1968074 -4.14534 -4.0861588 -4.063426 -4.0872359 -4.1264338 -4.164609 -4.2095528 -4.2465172][-4.3191133 -4.3173027 -4.3180251 -4.316833 -4.310235 -4.2910147 -4.2639213 -4.2314539 -4.1951137 -4.1724105 -4.1789579 -4.2024164 -4.2264128 -4.2557669 -4.281847][-4.3300996 -4.3238916 -4.3193865 -4.3163414 -4.3104548 -4.2989025 -4.2826228 -4.2620969 -4.2394009 -4.2178841 -4.2160392 -4.2345104 -4.2560358 -4.2790751 -4.2993402][-4.3203263 -4.3095884 -4.299932 -4.2917309 -4.2827873 -4.2752662 -4.2662396 -4.2530384 -4.2391949 -4.2251577 -4.2256312 -4.2473969 -4.2707515 -4.2909379 -4.3063149][-4.3051748 -4.2938719 -4.2813435 -4.2684264 -4.2587256 -4.2545657 -4.2513647 -4.24594 -4.2388697 -4.23047 -4.2334213 -4.2534122 -4.2742453 -4.2918792 -4.3053894]]...]
INFO - root - 2017-12-06 00:19:48.833534: step 56110, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 73h:23m:19s remains)
INFO - root - 2017-12-06 00:19:58.203778: step 56120, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 72h:45m:02s remains)
INFO - root - 2017-12-06 00:20:07.617582: step 56130, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 73h:14m:28s remains)
INFO - root - 2017-12-06 00:20:17.027902: step 56140, loss = 2.05, batch loss = 2.00 (7.9 examples/sec; 1.008 sec/batch; 77h:21m:14s remains)
INFO - root - 2017-12-06 00:20:26.111408: step 56150, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 70h:01m:09s remains)
INFO - root - 2017-12-06 00:20:35.465524: step 56160, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 71h:37m:52s remains)
INFO - root - 2017-12-06 00:20:44.809592: step 56170, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 65h:58m:15s remains)
INFO - root - 2017-12-06 00:20:54.439933: step 56180, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 72h:16m:28s remains)
INFO - root - 2017-12-06 00:21:03.911744: step 56190, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 69h:25m:30s remains)
INFO - root - 2017-12-06 00:21:13.123696: step 56200, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 69h:57m:30s remains)
2017-12-06 00:21:13.962217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.266118 -4.2698712 -4.2757578 -4.2732425 -4.2651219 -4.2492023 -4.2283988 -4.2143922 -4.2115669 -4.2236037 -4.2491989 -4.275332 -4.2957335 -4.3067837 -4.3084488][-4.2458725 -4.2504263 -4.2558804 -4.2505674 -4.236958 -4.2117853 -4.178247 -4.15427 -4.1490016 -4.169178 -4.2059965 -4.2429018 -4.2715416 -4.28962 -4.2941351][-4.2323418 -4.2392106 -4.2468853 -4.241365 -4.2225318 -4.1870875 -4.1392088 -4.100534 -4.0916238 -4.1228032 -4.1699114 -4.2097917 -4.2436252 -4.2673993 -4.2747593][-4.2369804 -4.2492762 -4.2614036 -4.2542076 -4.2248225 -4.1762719 -4.1133008 -4.0613947 -4.0495787 -4.0889297 -4.1454992 -4.1883397 -4.2229857 -4.2479095 -4.2571354][-4.2509904 -4.2696409 -4.287672 -4.2779403 -4.2394695 -4.1793661 -4.1064448 -4.0457039 -4.0327587 -4.0758519 -4.1456542 -4.1960893 -4.2303691 -4.2512918 -4.2585206][-4.2623239 -4.2870851 -4.3118482 -4.3010411 -4.2566738 -4.1890721 -4.1056275 -4.031425 -4.013454 -4.0629344 -4.1493397 -4.2171788 -4.2567935 -4.2752614 -4.2788811][-4.2650928 -4.2931652 -4.3199315 -4.3101234 -4.2663627 -4.1979189 -4.1039181 -4.0017052 -3.9620831 -4.0201449 -4.1341658 -4.22857 -4.279973 -4.300117 -4.3026323][-4.26705 -4.2934251 -4.3166857 -4.310966 -4.2743664 -4.2078824 -4.1015472 -3.9643581 -3.8827844 -3.94526 -4.0988245 -4.2228518 -4.2887578 -4.3123455 -4.3184013][-4.269918 -4.2911892 -4.310401 -4.3105588 -4.2841 -4.2207708 -4.1045265 -3.9417102 -3.8208213 -3.8797169 -4.0649738 -4.2135711 -4.2903829 -4.3173661 -4.325489][-4.2745109 -4.2899323 -4.3042536 -4.3084955 -4.2939448 -4.2405825 -4.1335258 -3.9807072 -3.8605361 -3.9008963 -4.0691471 -4.2167716 -4.295753 -4.3229895 -4.3313046][-4.2786889 -4.2924738 -4.3035274 -4.3073564 -4.3016663 -4.2623935 -4.1815434 -4.0690117 -3.9800415 -4.0007238 -4.1199255 -4.2409244 -4.3079605 -4.3284836 -4.3333993][-4.2823906 -4.2978978 -4.3083286 -4.3114195 -4.3063154 -4.2765174 -4.2208738 -4.1481152 -4.0911489 -4.1022844 -4.1792054 -4.2681251 -4.3174744 -4.3301249 -4.3320055][-4.2855535 -4.3037195 -4.3160839 -4.3194485 -4.313725 -4.2908359 -4.2539434 -4.2078295 -4.1706214 -4.1767292 -4.2239566 -4.284646 -4.3201761 -4.3293452 -4.3290496][-4.2849956 -4.3061309 -4.3216696 -4.3266873 -4.3220119 -4.3060188 -4.2822371 -4.2531147 -4.2287574 -4.2304912 -4.2590151 -4.2969017 -4.3211555 -4.3275485 -4.3254871][-4.2806616 -4.3036933 -4.3228655 -4.3315434 -4.3296189 -4.3195825 -4.3058825 -4.2895732 -4.27585 -4.2759647 -4.293489 -4.3168764 -4.3317237 -4.3353844 -4.3324842]]...]
INFO - root - 2017-12-06 00:21:23.545903: step 56210, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 73h:30m:37s remains)
INFO - root - 2017-12-06 00:21:32.970222: step 56220, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 74h:22m:49s remains)
INFO - root - 2017-12-06 00:21:42.482871: step 56230, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 75h:50m:40s remains)
INFO - root - 2017-12-06 00:21:52.003447: step 56240, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 74h:52m:33s remains)
INFO - root - 2017-12-06 00:22:01.612116: step 56250, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 71h:43m:40s remains)
INFO - root - 2017-12-06 00:22:10.924754: step 56260, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 71h:45m:28s remains)
INFO - root - 2017-12-06 00:22:20.230312: step 56270, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 73h:31m:47s remains)
INFO - root - 2017-12-06 00:22:29.731762: step 56280, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 73h:22m:05s remains)
INFO - root - 2017-12-06 00:22:39.029485: step 56290, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 74h:55m:28s remains)
INFO - root - 2017-12-06 00:22:48.571843: step 56300, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 69h:41m:58s remains)
2017-12-06 00:22:49.348731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2925825 -4.2891827 -4.2814126 -4.2755108 -4.2784448 -4.2862697 -4.2928362 -4.2952161 -4.2934141 -4.2922673 -4.28806 -4.28592 -4.2865067 -4.2893105 -4.2894325][-4.2666726 -4.2583866 -4.2448435 -4.2376528 -4.2478027 -4.2647853 -4.276247 -4.2769542 -4.2720637 -4.268518 -4.2627983 -4.2622862 -4.2645264 -4.2667494 -4.2619395][-4.2517753 -4.2352662 -4.2121425 -4.1983361 -4.2158384 -4.2424994 -4.2550755 -4.2503939 -4.2409978 -4.2388663 -4.2371511 -4.237431 -4.2397885 -4.2409811 -4.2308207][-4.2459593 -4.2216873 -4.1905375 -4.1707344 -4.1909828 -4.2167692 -4.2235994 -4.2082334 -4.1890683 -4.1923823 -4.2052965 -4.2177472 -4.22614 -4.2299619 -4.2172561][-4.2334094 -4.2043328 -4.1698909 -4.1477485 -4.1667347 -4.182189 -4.1720181 -4.1386585 -4.1104541 -4.1280622 -4.1649976 -4.1985693 -4.21861 -4.2257195 -4.2144508][-4.2149816 -4.1846566 -4.1499786 -4.1251097 -4.140202 -4.1367226 -4.10182 -4.0455666 -4.0136447 -4.0540819 -4.1150379 -4.1704893 -4.2083 -4.2261486 -4.2194848][-4.2122068 -4.1843357 -4.1464615 -4.1102934 -4.1022167 -4.0694561 -4.0064726 -3.9252005 -3.892096 -3.9695485 -4.0639939 -4.1462083 -4.201838 -4.2322273 -4.2321734][-4.2188706 -4.1901975 -4.1458716 -4.0977211 -4.0681353 -4.0096421 -3.92821 -3.8326583 -3.8108439 -3.9230423 -4.0448375 -4.1366739 -4.194663 -4.2270293 -4.2287283][-4.2149105 -4.1851807 -4.1443262 -4.106884 -4.0843773 -4.0325351 -3.971436 -3.9073496 -3.904587 -3.997313 -4.0921574 -4.1529803 -4.188262 -4.2079749 -4.2045674][-4.2019615 -4.1770654 -4.1451592 -4.1256437 -4.1220422 -4.0958896 -4.0656123 -4.0363374 -4.0387468 -4.0932474 -4.1471219 -4.1732678 -4.1820769 -4.1862311 -4.1731415][-4.1863885 -4.1650443 -4.1462092 -4.1433353 -4.1551485 -4.1491342 -4.1368227 -4.1206059 -4.1160512 -4.1471062 -4.1793189 -4.1873307 -4.1789336 -4.1689343 -4.1500826][-4.2032795 -4.1802025 -4.164607 -4.1669846 -4.1858349 -4.1933203 -4.1934118 -4.1854339 -4.1786637 -4.1958036 -4.2148132 -4.215251 -4.2015257 -4.1858354 -4.1651783][-4.2376742 -4.2150965 -4.2015724 -4.2052636 -4.2262692 -4.2371922 -4.2400522 -4.2389627 -4.233613 -4.2397094 -4.2488337 -4.2486148 -4.2399611 -4.2270932 -4.2075281][-4.2728691 -4.2552357 -4.2434607 -4.2458849 -4.2589626 -4.2624726 -4.2645392 -4.2692609 -4.2683635 -4.2736192 -4.2803388 -4.282074 -4.2797108 -4.2718949 -4.2569332][-4.2978086 -4.2869635 -4.27725 -4.2757788 -4.2785058 -4.2749505 -4.2744207 -4.2793612 -4.2824039 -4.2897353 -4.2976928 -4.3030376 -4.3042331 -4.2992797 -4.2897172]]...]
INFO - root - 2017-12-06 00:22:58.752177: step 56310, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 71h:04m:48s remains)
INFO - root - 2017-12-06 00:23:08.019585: step 56320, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 72h:05m:42s remains)
INFO - root - 2017-12-06 00:23:17.260544: step 56330, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 63h:53m:19s remains)
INFO - root - 2017-12-06 00:23:26.787721: step 56340, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 71h:15m:14s remains)
INFO - root - 2017-12-06 00:23:36.218451: step 56350, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 73h:40m:24s remains)
INFO - root - 2017-12-06 00:23:45.676624: step 56360, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.958 sec/batch; 73h:30m:37s remains)
INFO - root - 2017-12-06 00:23:54.963942: step 56370, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 72h:17m:08s remains)
INFO - root - 2017-12-06 00:24:04.131405: step 56380, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 66h:03m:54s remains)
INFO - root - 2017-12-06 00:24:13.387811: step 56390, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 70h:17m:14s remains)
INFO - root - 2017-12-06 00:24:22.538177: step 56400, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 67h:23m:59s remains)
2017-12-06 00:24:23.436646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25188 -4.2450514 -4.2481184 -4.2519097 -4.2487421 -4.24135 -4.2363515 -4.2368827 -4.2488303 -4.261507 -4.2664967 -4.2611961 -4.2596292 -4.2599163 -4.2622361][-4.2398562 -4.2324433 -4.2356925 -4.2356009 -4.2253222 -4.2133651 -4.2100873 -4.2164836 -4.2389174 -4.2601914 -4.2715564 -4.2708721 -4.2682877 -4.2618351 -4.2557139][-4.2342458 -4.2304907 -4.2343354 -4.2280078 -4.2064028 -4.1876078 -4.1851573 -4.19596 -4.2262921 -4.2543845 -4.2706304 -4.277226 -4.2785411 -4.2685213 -4.25665][-4.2294993 -4.2315426 -4.2365875 -4.2244792 -4.191771 -4.1626563 -4.152194 -4.16232 -4.2026668 -4.2408214 -4.263041 -4.27743 -4.2853837 -4.2773466 -4.263031][-4.2275805 -4.233777 -4.239459 -4.2202048 -4.1750803 -4.1284952 -4.0981436 -4.1009421 -4.1528387 -4.2113986 -4.2457747 -4.269186 -4.2797971 -4.274579 -4.26101][-4.230422 -4.2366304 -4.2390532 -4.212832 -4.1567178 -4.0916047 -4.0338993 -4.0133276 -4.0651135 -4.1466808 -4.2043114 -4.2392144 -4.2581167 -4.2613544 -4.2521024][-4.2340984 -4.2365847 -4.2325382 -4.204298 -4.1459894 -4.0691285 -3.9823382 -3.925802 -3.9595466 -4.0558295 -4.1407766 -4.1979918 -4.2296867 -4.2426229 -4.240417][-4.240818 -4.2433467 -4.2394428 -4.2169466 -4.1675935 -4.0936794 -3.9977553 -3.9137263 -3.9154155 -3.9991043 -4.0910296 -4.1624079 -4.2029934 -4.2242808 -4.2299728][-4.2354956 -4.2428608 -4.2480783 -4.2414384 -4.2125387 -4.158864 -4.0864248 -4.0225973 -4.0115519 -4.0531735 -4.1119623 -4.16559 -4.2021384 -4.2242169 -4.2337861][-4.2105088 -4.2225618 -4.2405038 -4.2519593 -4.2434378 -4.2120538 -4.1682591 -4.1341071 -4.1246066 -4.1394572 -4.1668873 -4.1974039 -4.2211246 -4.2373133 -4.2427793][-4.1938596 -4.2106547 -4.2327976 -4.250504 -4.2538238 -4.2397881 -4.2188358 -4.2061949 -4.199399 -4.2024937 -4.2151957 -4.2270255 -4.2361274 -4.2428117 -4.24166][-4.1809816 -4.1936522 -4.2134218 -4.227108 -4.232482 -4.2318587 -4.2286277 -4.2243876 -4.2216463 -4.2260385 -4.238236 -4.2458658 -4.248136 -4.2491794 -4.2373686][-4.1778769 -4.1796522 -4.1913891 -4.1939654 -4.192173 -4.1943526 -4.1979136 -4.1932712 -4.1907454 -4.20077 -4.2241955 -4.2376075 -4.2403488 -4.2432919 -4.2290688][-4.1915555 -4.1854239 -4.1862912 -4.1758165 -4.1602006 -4.1551485 -4.1519575 -4.135798 -4.1316705 -4.1530237 -4.19313 -4.2177896 -4.2262664 -4.2339568 -4.2261276][-4.2014322 -4.19602 -4.1917553 -4.1734686 -4.1442995 -4.1246963 -4.1061492 -4.0779076 -4.0697036 -4.0976663 -4.1508832 -4.1914186 -4.2151556 -4.233789 -4.2384524]]...]
INFO - root - 2017-12-06 00:24:32.503149: step 56410, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 72h:01m:45s remains)
INFO - root - 2017-12-06 00:24:42.129023: step 56420, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.992 sec/batch; 76h:06m:01s remains)
INFO - root - 2017-12-06 00:24:51.328325: step 56430, loss = 2.07, batch loss = 2.02 (7.8 examples/sec; 1.026 sec/batch; 78h:41m:14s remains)
INFO - root - 2017-12-06 00:25:00.963299: step 56440, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 72h:02m:40s remains)
INFO - root - 2017-12-06 00:25:10.407228: step 56450, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 74h:08m:26s remains)
INFO - root - 2017-12-06 00:25:19.605291: step 56460, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 75h:41m:18s remains)
INFO - root - 2017-12-06 00:25:28.871149: step 56470, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 74h:00m:55s remains)
INFO - root - 2017-12-06 00:25:38.236748: step 56480, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 72h:46m:10s remains)
INFO - root - 2017-12-06 00:25:47.686482: step 56490, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 72h:03m:52s remains)
INFO - root - 2017-12-06 00:25:57.212997: step 56500, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 75h:48m:19s remains)
2017-12-06 00:25:57.972816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.201951 -4.1853604 -4.1693563 -4.156672 -4.1537814 -4.161706 -4.1797223 -4.2078166 -4.237618 -4.2581015 -4.2577772 -4.2432666 -4.2227869 -4.2101841 -4.2054625][-4.2174182 -4.2034249 -4.1932621 -4.1874337 -4.1810732 -4.1712875 -4.1688862 -4.1848321 -4.2173724 -4.25389 -4.2730761 -4.271852 -4.2543464 -4.2390666 -4.2326827][-4.2487588 -4.2353549 -4.2285452 -4.2269692 -4.2196312 -4.1938996 -4.1638427 -4.1591635 -4.1917295 -4.2408071 -4.2745337 -4.2779093 -4.2618723 -4.2496161 -4.2448635][-4.2683005 -4.2538848 -4.2495608 -4.2534909 -4.247889 -4.2107763 -4.1550465 -4.1233315 -4.1478825 -4.2070694 -4.2546625 -4.264092 -4.2533693 -4.24532 -4.2447424][-4.254097 -4.2367005 -4.2376142 -4.2530904 -4.2583051 -4.221981 -4.14945 -4.0894146 -4.0975823 -4.1575041 -4.2136011 -4.2317467 -4.2275524 -4.2259812 -4.2330618][-4.2092462 -4.1856079 -4.1930928 -4.2254186 -4.2508821 -4.2274356 -4.1508493 -4.0681787 -4.0557046 -4.1086178 -4.1683331 -4.1949787 -4.1975751 -4.1989512 -4.2107873][-4.1467648 -4.1211667 -4.1370544 -4.1872683 -4.2342167 -4.2308092 -4.1654673 -4.0739651 -4.0391541 -4.074491 -4.1267848 -4.1579537 -4.1695004 -4.1771154 -4.1941729][-4.1010876 -4.0777769 -4.0977526 -4.1569409 -4.2173615 -4.2301011 -4.1813054 -4.098249 -4.0538535 -4.0668535 -4.1002326 -4.1277189 -4.1449862 -4.1595736 -4.1814303][-4.0981855 -4.0822949 -4.0997376 -4.1505208 -4.2059078 -4.2232752 -4.1886292 -4.1256375 -4.0891762 -4.089859 -4.1022015 -4.1168857 -4.1325278 -4.14966 -4.1728325][-4.1336102 -4.1254215 -4.1356239 -4.1690016 -4.2089195 -4.2200251 -4.190352 -4.1456022 -4.123939 -4.1251483 -4.1286378 -4.1312075 -4.1377058 -4.1508594 -4.1699243][-4.1910543 -4.188767 -4.190527 -4.2027993 -4.221642 -4.2213197 -4.1905613 -4.1578703 -4.1537919 -4.1657915 -4.1702127 -4.166923 -4.1623774 -4.1632338 -4.1705604][-4.2447805 -4.2446537 -4.2408648 -4.2396207 -4.2467384 -4.2411752 -4.2103453 -4.1832275 -4.1914816 -4.2127767 -4.2210217 -4.212966 -4.1976748 -4.1831393 -4.1753111][-4.2752304 -4.2780752 -4.2718344 -4.2652087 -4.2684121 -4.2626286 -4.2356448 -4.2105651 -4.2206903 -4.2463355 -4.2597704 -4.2519479 -4.229321 -4.1997385 -4.1769624][-4.2773457 -4.2848988 -4.2785444 -4.2698183 -4.2716737 -4.2691436 -4.2472105 -4.2232842 -4.2292976 -4.25353 -4.2714415 -4.2699103 -4.2465076 -4.20598 -4.1683769][-4.2704592 -4.2802725 -4.2757235 -4.2688003 -4.2720318 -4.2747092 -4.259481 -4.2378006 -4.2385616 -4.2586875 -4.2767873 -4.2783384 -4.2578588 -4.2128577 -4.1628709]]...]
INFO - root - 2017-12-06 00:26:07.515611: step 56510, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 75h:08m:49s remains)
INFO - root - 2017-12-06 00:26:16.720684: step 56520, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 72h:29m:42s remains)
INFO - root - 2017-12-06 00:26:26.075268: step 56530, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 74h:32m:47s remains)
INFO - root - 2017-12-06 00:26:35.529958: step 56540, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 73h:53m:14s remains)
INFO - root - 2017-12-06 00:26:44.798113: step 56550, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 72h:42m:02s remains)
INFO - root - 2017-12-06 00:26:54.135847: step 56560, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 73h:12m:30s remains)
INFO - root - 2017-12-06 00:27:03.684880: step 56570, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 73h:52m:55s remains)
INFO - root - 2017-12-06 00:27:13.016398: step 56580, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 74h:44m:23s remains)
INFO - root - 2017-12-06 00:27:22.195934: step 56590, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 68h:44m:22s remains)
INFO - root - 2017-12-06 00:27:31.526013: step 56600, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 65h:14m:51s remains)
2017-12-06 00:27:32.316065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2836246 -4.2817264 -4.2886596 -4.2978168 -4.3009605 -4.2940354 -4.2921662 -4.2981219 -4.3011723 -4.298511 -4.2879119 -4.2810755 -4.274127 -4.269855 -4.269702][-4.2712388 -4.2667279 -4.2720475 -4.283082 -4.2866569 -4.2767797 -4.274231 -4.2870193 -4.298553 -4.3014512 -4.2898912 -4.282073 -4.2741585 -4.2679462 -4.2645874][-4.265934 -4.25755 -4.2561307 -4.2659459 -4.2679572 -4.2504234 -4.2405314 -4.2549028 -4.2720528 -4.2801695 -4.2711253 -4.2677045 -4.2667575 -4.2643132 -4.2599521][-4.2699361 -4.2596316 -4.252429 -4.2576575 -4.2563653 -4.2313662 -4.2126188 -4.2224956 -4.2413082 -4.25285 -4.247674 -4.24992 -4.2578259 -4.2613759 -4.2573705][-4.2644238 -4.2480125 -4.2286158 -4.2183452 -4.2063255 -4.1706586 -4.1397257 -4.1430736 -4.1698442 -4.1903887 -4.1901946 -4.1976862 -4.2204304 -4.2411242 -4.2467136][-4.2498536 -4.2223935 -4.185863 -4.1563411 -4.1289396 -4.0793829 -4.0277491 -4.0148697 -4.0559654 -4.096128 -4.1043568 -4.1169615 -4.15766 -4.2004814 -4.2211962][-4.2265778 -4.1858277 -4.131743 -4.0860415 -4.0442195 -3.9745691 -3.89307 -3.8610675 -3.9193959 -3.9802697 -3.9970465 -4.0141625 -4.0699091 -4.1348829 -4.1754303][-4.2172484 -4.1791964 -4.1244354 -4.0792141 -4.0374975 -3.965606 -3.8790343 -3.8422222 -3.89467 -3.9547915 -3.9719121 -3.985498 -4.0355425 -4.10186 -4.1487274][-4.2302041 -4.2091475 -4.1711721 -4.1377416 -4.1067514 -4.0546365 -3.9962697 -3.97154 -4.0026727 -4.0426512 -4.0534344 -4.0595241 -4.0874829 -4.1306009 -4.1653376][-4.2534275 -4.2469826 -4.2289305 -4.2094641 -4.186923 -4.1494207 -4.1143661 -4.1038761 -4.1224213 -4.1469646 -4.15337 -4.1520152 -4.1586685 -4.178349 -4.1979494][-4.2720561 -4.2727289 -4.2667251 -4.2568064 -4.2390342 -4.2103677 -4.1925087 -4.192287 -4.2046289 -4.2189274 -4.2219405 -4.2207813 -4.2206044 -4.2276034 -4.2352567][-4.2776303 -4.2786021 -4.2772193 -4.2726803 -4.261961 -4.2421036 -4.2344937 -4.237329 -4.243957 -4.2521892 -4.2542629 -4.2584281 -4.2611446 -4.264504 -4.2663679][-4.2762542 -4.2753534 -4.2760267 -4.2764235 -4.2702107 -4.2584276 -4.2556682 -4.2577415 -4.2624874 -4.2709661 -4.2736578 -4.2801962 -4.2858882 -4.2889037 -4.2885361][-4.273303 -4.266345 -4.2620807 -4.2606683 -4.2550359 -4.2475476 -4.2479033 -4.2508335 -4.2538805 -4.2613068 -4.2664428 -4.2750115 -4.2839913 -4.28998 -4.2916412][-4.2755189 -4.2651744 -4.2569189 -4.2520294 -4.246182 -4.2404666 -4.2405224 -4.2438846 -4.2459431 -4.2508135 -4.2564855 -4.2654958 -4.2778492 -4.2867446 -4.2908263]]...]
INFO - root - 2017-12-06 00:27:41.775359: step 56610, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.985 sec/batch; 75h:27m:40s remains)
INFO - root - 2017-12-06 00:27:51.120422: step 56620, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 74h:14m:13s remains)
INFO - root - 2017-12-06 00:28:00.250652: step 56630, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 70h:24m:07s remains)
INFO - root - 2017-12-06 00:28:09.595905: step 56640, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 70h:25m:03s remains)
INFO - root - 2017-12-06 00:28:18.986355: step 56650, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 68h:00m:14s remains)
INFO - root - 2017-12-06 00:28:28.403283: step 56660, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 69h:51m:50s remains)
INFO - root - 2017-12-06 00:28:37.824558: step 56670, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 70h:41m:14s remains)
INFO - root - 2017-12-06 00:28:47.109255: step 56680, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 72h:15m:29s remains)
INFO - root - 2017-12-06 00:28:56.338211: step 56690, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 71h:31m:40s remains)
INFO - root - 2017-12-06 00:29:05.503226: step 56700, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 75h:38m:31s remains)
2017-12-06 00:29:06.324024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1790514 -4.1705823 -4.1730881 -4.1724081 -4.1758504 -4.1776319 -4.1657262 -4.151948 -4.1546664 -4.1670566 -4.1833138 -4.1918788 -4.2008953 -4.2094984 -4.2160597][-4.1539149 -4.1382103 -4.1421347 -4.1490779 -4.1565161 -4.1617403 -4.1541362 -4.1435661 -4.1488471 -4.1616416 -4.1753321 -4.1791353 -4.1824436 -4.1840591 -4.1914411][-4.13625 -4.1140347 -4.1147294 -4.1225328 -4.1303487 -4.135757 -4.1300144 -4.1210938 -4.12825 -4.1423974 -4.1529522 -4.1546249 -4.1561565 -4.1553426 -4.1633763][-4.1534004 -4.1280642 -4.1179872 -4.1179948 -4.1203012 -4.1221414 -4.1130242 -4.1026678 -4.1085973 -4.122786 -4.1281214 -4.1228662 -4.1180992 -4.1145892 -4.1236272][-4.1867137 -4.1671152 -4.1512737 -4.1425929 -4.1431775 -4.1413345 -4.1279349 -4.1169 -4.1225343 -4.1373606 -4.1386185 -4.1265388 -4.1134768 -4.1010075 -4.1003213][-4.1795611 -4.1716809 -4.1593156 -4.1490116 -4.1464491 -4.1384244 -4.1208153 -4.1106267 -4.1219177 -4.1451349 -4.1548524 -4.1491418 -4.1355405 -4.1176462 -4.1060095][-4.1131797 -4.112833 -4.1042767 -4.093998 -4.0863013 -4.0640554 -4.0310087 -4.0155182 -4.0418558 -4.0868568 -4.1179614 -4.1324477 -4.1303158 -4.1183038 -4.1094761][-4.0079293 -4.0156612 -4.0116653 -3.9986331 -3.984359 -3.9442022 -3.8844488 -3.8534088 -3.8965712 -3.9737744 -4.037024 -4.0788541 -4.0931039 -4.0903134 -4.0926371][-3.9422059 -3.9614136 -3.9674294 -3.9610744 -3.9497392 -3.906919 -3.8323193 -3.7774951 -3.8110764 -3.891633 -3.9664779 -4.0244336 -4.0491891 -4.0498433 -4.0615869][-3.9932346 -4.0144658 -4.0285416 -4.0372314 -4.0415287 -4.0187397 -3.9628534 -3.9067135 -3.9067793 -3.9481142 -3.9991982 -4.0464678 -4.0677867 -4.0644774 -4.0755715][-4.1181 -4.1316524 -4.141398 -4.1533604 -4.1637979 -4.1588473 -4.1281128 -4.0885177 -4.0720892 -4.0831423 -4.1114192 -4.1382122 -4.1487927 -4.1385818 -4.1405339][-4.2225814 -4.2307878 -4.23305 -4.2396541 -4.2471776 -4.2473369 -4.2316494 -4.2093592 -4.1921253 -4.1909714 -4.2080288 -4.2214308 -4.2202191 -4.2036963 -4.19833][-4.2815957 -4.286201 -4.2849565 -4.2870774 -4.2895527 -4.2898531 -4.2833548 -4.2706676 -4.2532091 -4.2433362 -4.2518983 -4.2553606 -4.2445478 -4.2242918 -4.2165623][-4.3052983 -4.3100643 -4.3099957 -4.3104305 -4.3095303 -4.3089919 -4.3049588 -4.2911739 -4.267952 -4.2494226 -4.2489362 -4.2462754 -4.2296562 -4.2062187 -4.1976681][-4.3055973 -4.3131418 -4.3180594 -4.3199205 -4.3188877 -4.317656 -4.310946 -4.2909713 -4.2581267 -4.2306609 -4.2212853 -4.2149711 -4.1998372 -4.181015 -4.1755528]]...]
INFO - root - 2017-12-06 00:29:15.781543: step 56710, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 73h:31m:39s remains)
INFO - root - 2017-12-06 00:29:25.133006: step 56720, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 72h:53m:20s remains)
INFO - root - 2017-12-06 00:29:34.668430: step 56730, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 71h:39m:11s remains)
INFO - root - 2017-12-06 00:29:44.131012: step 56740, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 71h:21m:32s remains)
INFO - root - 2017-12-06 00:29:53.477822: step 56750, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 69h:34m:43s remains)
INFO - root - 2017-12-06 00:30:02.848960: step 56760, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 70h:46m:06s remains)
INFO - root - 2017-12-06 00:30:12.396876: step 56770, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 72h:15m:48s remains)
INFO - root - 2017-12-06 00:30:21.683622: step 56780, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 72h:07m:33s remains)
INFO - root - 2017-12-06 00:30:30.835207: step 56790, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 71h:46m:51s remains)
INFO - root - 2017-12-06 00:30:40.258670: step 56800, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 72h:15m:20s remains)
2017-12-06 00:30:40.996468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2289777 -4.2343564 -4.2287025 -4.2280788 -4.2297616 -4.2361345 -4.252285 -4.2691326 -4.2839088 -4.2869239 -4.2911234 -4.2868786 -4.27335 -4.2581167 -4.2503414][-4.2528567 -4.248138 -4.2295647 -4.2210479 -4.2208605 -4.228394 -4.2449884 -4.2661805 -4.284215 -4.2896805 -4.2920437 -4.2767396 -4.2480006 -4.2216372 -4.2072549][-4.2629733 -4.2446318 -4.2112331 -4.1894717 -4.1826835 -4.1866541 -4.20174 -4.2263403 -4.2520342 -4.2685294 -4.2779512 -4.2599058 -4.2143764 -4.1702237 -4.1449118][-4.263803 -4.230267 -4.1796169 -4.1413093 -4.1242886 -4.1237969 -4.137639 -4.1623878 -4.1953654 -4.2292032 -4.2519722 -4.2386947 -4.1853857 -4.1262217 -4.0833116][-4.2616687 -4.2180128 -4.1514058 -4.0948339 -4.0624971 -4.0509248 -4.05526 -4.0722904 -4.1143546 -4.1735334 -4.2215195 -4.226016 -4.1769562 -4.1092067 -4.0467143][-4.2615538 -4.2133832 -4.1335492 -4.0597954 -4.0085511 -3.9695494 -3.9437058 -3.9396079 -3.9927366 -4.0894136 -4.172739 -4.213758 -4.1979623 -4.1447225 -4.0819273][-4.27403 -4.2378154 -4.16098 -4.0827613 -4.012825 -3.9305897 -3.853142 -3.8049982 -3.8570232 -3.990171 -4.1111326 -4.1872482 -4.2081585 -4.1881013 -4.1534629][-4.2888436 -4.27218 -4.2177687 -4.159276 -4.0976386 -4.0047741 -3.9068754 -3.8260374 -3.8380296 -3.9489169 -4.0692611 -4.1610174 -4.2059884 -4.2147403 -4.2096105][-4.2951593 -4.2945743 -4.2646289 -4.2316995 -4.1909389 -4.1202059 -4.0443854 -3.9790075 -3.9726532 -4.0295773 -4.103961 -4.1730866 -4.2175946 -4.2391462 -4.2505412][-4.2948279 -4.3027439 -4.2936125 -4.2824497 -4.2632828 -4.2199221 -4.1742439 -4.1337695 -4.1260657 -4.1459551 -4.1808434 -4.221673 -4.2541752 -4.2734027 -4.2874503][-4.2892346 -4.2988873 -4.3021836 -4.3070192 -4.3059478 -4.2863097 -4.267601 -4.2466731 -4.2429981 -4.2476172 -4.2575407 -4.2738376 -4.292522 -4.3049927 -4.3160524][-4.2861433 -4.294055 -4.3005219 -4.3105416 -4.3196807 -4.3155303 -4.316133 -4.3118815 -4.3128939 -4.3127928 -4.3129172 -4.3139534 -4.3172765 -4.3209476 -4.3278542][-4.2819881 -4.2884603 -4.2952209 -4.3071761 -4.3191338 -4.3217511 -4.3315673 -4.3378024 -4.3438973 -4.343101 -4.3391418 -4.3335938 -4.3268728 -4.3212371 -4.3212161][-4.277019 -4.2807069 -4.28388 -4.2946858 -4.3074436 -4.3131638 -4.324801 -4.3336649 -4.3415885 -4.3419085 -4.3365164 -4.3275871 -4.3168564 -4.3082595 -4.304656][-4.2744761 -4.2763271 -4.2758679 -4.2835174 -4.2931585 -4.2967348 -4.3058138 -4.3127537 -4.3180633 -4.31625 -4.311204 -4.3044329 -4.297081 -4.2918224 -4.2895818]]...]
INFO - root - 2017-12-06 00:30:50.534983: step 56810, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 72h:10m:50s remains)
INFO - root - 2017-12-06 00:30:59.939065: step 56820, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.954 sec/batch; 73h:02m:51s remains)
INFO - root - 2017-12-06 00:31:09.323550: step 56830, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 69h:37m:35s remains)
INFO - root - 2017-12-06 00:31:18.682579: step 56840, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 70h:55m:32s remains)
INFO - root - 2017-12-06 00:31:28.233920: step 56850, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 69h:47m:14s remains)
INFO - root - 2017-12-06 00:31:37.623957: step 56860, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 67h:59m:01s remains)
INFO - root - 2017-12-06 00:31:46.948634: step 56870, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 72h:02m:15s remains)
INFO - root - 2017-12-06 00:31:56.221266: step 56880, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 69h:49m:24s remains)
INFO - root - 2017-12-06 00:32:05.478461: step 56890, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 74h:53m:13s remains)
INFO - root - 2017-12-06 00:32:14.750143: step 56900, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 72h:34m:26s remains)
2017-12-06 00:32:15.562959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3206663 -4.3120542 -4.2826881 -4.2638407 -4.2683716 -4.2806487 -4.2837124 -4.2747784 -4.2600012 -4.2559252 -4.2692409 -4.28835 -4.3128734 -4.3311305 -4.3393126][-4.3228407 -4.3162379 -4.2896876 -4.2720633 -4.2728553 -4.2750859 -4.2618103 -4.2371821 -4.2130761 -4.2083936 -4.2305732 -4.2594414 -4.2932129 -4.3183131 -4.3318219][-4.3195796 -4.3198709 -4.2990713 -4.2819405 -4.2745242 -4.2603717 -4.2254858 -4.1805773 -4.1455975 -4.1424937 -4.1779823 -4.2207823 -4.2656593 -4.2996407 -4.3202767][-4.2947788 -4.3047004 -4.2949305 -4.2814078 -4.2683311 -4.2385674 -4.18288 -4.1167269 -4.0683579 -4.068408 -4.1184034 -4.1773176 -4.236392 -4.2818203 -4.3091364][-4.2367435 -4.2594323 -4.2634659 -4.2573862 -4.2441945 -4.2052512 -4.1350145 -4.0509887 -3.9906251 -3.9976606 -4.0635705 -4.1372547 -4.2106433 -4.2658892 -4.298048][-4.1527152 -4.1912971 -4.2107635 -4.2164979 -4.2075119 -4.1635666 -4.0827756 -3.9831142 -3.9137225 -3.9326608 -4.0185347 -4.1094408 -4.1955557 -4.2577581 -4.291585][-4.0604095 -4.1196337 -4.1597071 -4.1806831 -4.1775422 -4.1273055 -4.0350757 -3.917913 -3.8442688 -3.8820889 -3.9904256 -4.09929 -4.1948361 -4.261549 -4.29393][-4.01279 -4.0876131 -4.1436825 -4.1758609 -4.1743484 -4.119483 -4.0235829 -3.9076076 -3.8470545 -3.8966839 -4.0076818 -4.1200409 -4.2155895 -4.2812638 -4.3078871][-4.0458431 -4.1153908 -4.1704 -4.2001357 -4.1973953 -4.147624 -4.0684676 -3.9819827 -3.9421749 -3.9823351 -4.0697141 -4.1636724 -4.2495513 -4.3074522 -4.326014][-4.1128483 -4.1664515 -4.2091 -4.2274795 -4.2172985 -4.1753125 -4.11934 -4.0626106 -4.0310073 -4.0515981 -4.1131821 -4.1884117 -4.26496 -4.3162441 -4.3322377][-4.1729145 -4.2052684 -4.2283497 -4.2318029 -4.2163892 -4.1856203 -4.1482682 -4.1059566 -4.0720649 -4.0754776 -4.1186185 -4.1819572 -4.2508211 -4.2997332 -4.320889][-4.2089629 -4.2203994 -4.2251577 -4.2171497 -4.1981215 -4.1765423 -4.1502557 -4.1132312 -4.0775137 -4.0781279 -4.11585 -4.1731634 -4.2348723 -4.2818451 -4.3076973][-4.2261496 -4.2221937 -4.2169275 -4.2042413 -4.1865897 -4.1755981 -4.1604319 -4.1286325 -4.0942726 -4.0964217 -4.1319962 -4.1830621 -4.2374573 -4.2811246 -4.30731][-4.247508 -4.2397661 -4.2333865 -4.2232094 -4.2102933 -4.2057314 -4.1965532 -4.1706939 -4.1412797 -4.1456251 -4.1775112 -4.2200727 -4.2641449 -4.2990618 -4.3200092][-4.2680221 -4.26062 -4.2564459 -4.25005 -4.2429218 -4.2423759 -4.2376547 -4.2209606 -4.2017603 -4.2070346 -4.2328968 -4.2643657 -4.2964354 -4.3221121 -4.3361125]]...]
INFO - root - 2017-12-06 00:32:25.204454: step 56910, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 73h:57m:15s remains)
INFO - root - 2017-12-06 00:32:34.389912: step 56920, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 73h:28m:02s remains)
INFO - root - 2017-12-06 00:32:43.580667: step 56930, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 71h:42m:34s remains)
INFO - root - 2017-12-06 00:32:52.895835: step 56940, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 70h:48m:59s remains)
INFO - root - 2017-12-06 00:33:02.220578: step 56950, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 71h:53m:18s remains)
INFO - root - 2017-12-06 00:33:11.490451: step 56960, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 70h:47m:24s remains)
INFO - root - 2017-12-06 00:33:20.804093: step 56970, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 72h:01m:41s remains)
INFO - root - 2017-12-06 00:33:30.255056: step 56980, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 74h:06m:23s remains)
INFO - root - 2017-12-06 00:33:39.791582: step 56990, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 67h:54m:23s remains)
INFO - root - 2017-12-06 00:33:49.198135: step 57000, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 72h:27m:28s remains)
2017-12-06 00:33:49.954470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2653875 -4.2557092 -4.2379427 -4.2104654 -4.1873832 -4.1815667 -4.1948791 -4.2169161 -4.2294827 -4.2287211 -4.2181621 -4.2072845 -4.210638 -4.2157488 -4.2224059][-4.2811313 -4.2631173 -4.2372437 -4.2019248 -4.1722946 -4.1617403 -4.1767907 -4.2120461 -4.23891 -4.2508531 -4.2461915 -4.2379022 -4.2400165 -4.2439451 -4.2535586][-4.2911458 -4.2654867 -4.2316356 -4.1876526 -4.1491723 -4.1290951 -4.1434155 -4.1903949 -4.231813 -4.2568789 -4.2640843 -4.2637978 -4.2644281 -4.2659426 -4.2744956][-4.2999992 -4.2710252 -4.2299318 -4.176959 -4.1262584 -4.0919452 -4.1023345 -4.1581426 -4.2138157 -4.2493682 -4.2665973 -4.273468 -4.2732439 -4.2719545 -4.277391][-4.3109713 -4.2839422 -4.2386708 -4.177886 -4.1138053 -4.05834 -4.0562921 -4.1222119 -4.1935959 -4.2354712 -4.256629 -4.2643733 -4.2645168 -4.2624149 -4.2660065][-4.310904 -4.2903109 -4.2460837 -4.1838632 -4.1085367 -4.0315995 -4.015173 -4.0969782 -4.1829557 -4.2280092 -4.2481146 -4.2543459 -4.2555776 -4.252151 -4.2559505][-4.3042474 -4.2889128 -4.25001 -4.1926255 -4.1194029 -4.0371656 -4.0110197 -4.0961609 -4.1848693 -4.2274365 -4.2463942 -4.253139 -4.253283 -4.2472968 -4.2517214][-4.2940884 -4.2832117 -4.2520194 -4.2035174 -4.1422167 -4.0699654 -4.0428066 -4.1129217 -4.1895375 -4.2299228 -4.2490549 -4.2561369 -4.2548523 -4.2459769 -4.2473426][-4.2854123 -4.2776866 -4.2499852 -4.2107716 -4.1589565 -4.098711 -4.0788283 -4.13394 -4.1966267 -4.2313318 -4.2479591 -4.25211 -4.2484674 -4.2370663 -4.2345037][-4.27387 -4.2689052 -4.2409606 -4.2040033 -4.1538134 -4.1026683 -4.0982847 -4.1515508 -4.2034488 -4.2265968 -4.2366781 -4.2376337 -4.2327442 -4.2223392 -4.2207823][-4.2624373 -4.2557659 -4.2255826 -4.1857438 -4.1327362 -4.0920362 -4.1083503 -4.1674194 -4.2124095 -4.2250371 -4.2296567 -4.2293034 -4.2243228 -4.2171707 -4.2171297][-4.2555108 -4.2449951 -4.2081757 -4.1601653 -4.10064 -4.0715747 -4.1117311 -4.1798277 -4.2202539 -4.2285118 -4.2304597 -4.22849 -4.2241564 -4.2207112 -4.223546][-4.247324 -4.2268157 -4.180779 -4.1275339 -4.0725923 -4.0625615 -4.12113 -4.1924677 -4.2286863 -4.2355461 -4.2359266 -4.2316723 -4.2260547 -4.2241521 -4.2299576][-4.2391863 -4.2048292 -4.1545835 -4.1033654 -4.0633636 -4.07639 -4.1399951 -4.2024422 -4.2314162 -4.2362175 -4.2334723 -4.226584 -4.2205172 -4.2203689 -4.2280779][-4.2364964 -4.19093 -4.1421175 -4.0988259 -4.0761185 -4.1062789 -4.1636939 -4.2123818 -4.2312737 -4.2324471 -4.2269006 -4.2190623 -4.2156787 -4.2184138 -4.2274404]]...]
INFO - root - 2017-12-06 00:33:59.377652: step 57010, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 66h:38m:34s remains)
INFO - root - 2017-12-06 00:34:08.798765: step 57020, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 73h:36m:41s remains)
INFO - root - 2017-12-06 00:34:18.370334: step 57030, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.007 sec/batch; 77h:02m:44s remains)
INFO - root - 2017-12-06 00:34:28.037005: step 57040, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 74h:57m:14s remains)
INFO - root - 2017-12-06 00:34:37.456993: step 57050, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 75h:15m:40s remains)
INFO - root - 2017-12-06 00:34:46.971936: step 57060, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.916 sec/batch; 70h:03m:33s remains)
INFO - root - 2017-12-06 00:34:56.193140: step 57070, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 74h:06m:44s remains)
INFO - root - 2017-12-06 00:35:05.508412: step 57080, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 73h:45m:26s remains)
INFO - root - 2017-12-06 00:35:15.059988: step 57090, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 74h:15m:21s remains)
INFO - root - 2017-12-06 00:35:24.478926: step 57100, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 73h:51m:41s remains)
2017-12-06 00:35:25.355721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0906119 -4.1053948 -4.1200294 -4.1239371 -4.1156411 -4.0878782 -4.040966 -4.0229011 -4.0551496 -4.1008883 -4.1343608 -4.1592188 -4.1801567 -4.2038765 -4.2217832][-4.0982232 -4.1151485 -4.1273475 -4.1204023 -4.1018023 -4.0686593 -4.0128469 -3.9911544 -4.0298729 -4.0878072 -4.1309748 -4.16064 -4.1809883 -4.2023211 -4.2199483][-4.1105304 -4.1350636 -4.1441545 -4.1258407 -4.0933051 -4.05131 -3.9868593 -3.9649849 -4.0147357 -4.0825853 -4.1287684 -4.156445 -4.1765838 -4.1993885 -4.2209482][-4.1392717 -4.159945 -4.1687403 -4.137651 -4.0842247 -4.027422 -3.95269 -3.9365311 -4.0017681 -4.0792294 -4.1242743 -4.1491942 -4.1705666 -4.1945286 -4.2185507][-4.1732349 -4.1871085 -4.1919513 -4.1437383 -4.0671945 -3.9903965 -3.9075661 -3.9103429 -3.9965885 -4.0844979 -4.1317906 -4.1534281 -4.1673536 -4.1896353 -4.2151046][-4.1851416 -4.1871991 -4.1766906 -4.1085162 -4.0129356 -3.9039006 -3.8013349 -3.8417857 -3.9708159 -4.0737753 -4.1320052 -4.158865 -4.170434 -4.1899691 -4.2148442][-4.1587353 -4.1567011 -4.1431437 -4.0662932 -3.9538515 -3.78869 -3.6220088 -3.6996253 -3.8883343 -4.0258265 -4.1072445 -4.1478572 -4.1635284 -4.1856546 -4.2139339][-4.1318436 -4.1314654 -4.1209908 -4.0603609 -3.9625118 -3.7922535 -3.6125035 -3.6963603 -3.8896248 -4.0265312 -4.1108379 -4.1505861 -4.1631727 -4.185688 -4.2151289][-4.1230717 -4.1228051 -4.1122479 -4.0858808 -4.0376821 -3.9351146 -3.8364468 -3.8865659 -4.0006895 -4.09118 -4.1490364 -4.1739535 -4.1790795 -4.1955185 -4.2218251][-4.1053052 -4.1085033 -4.1043844 -4.1027212 -4.0883718 -4.0282559 -3.9803016 -4.0132618 -4.0820966 -4.1416888 -4.1749382 -4.1921568 -4.1970754 -4.2068696 -4.2265987][-4.0716243 -4.0923171 -4.1003795 -4.1123276 -4.1099281 -4.0655146 -4.0307145 -4.0580149 -4.1182961 -4.1644607 -4.1787419 -4.1903205 -4.2012296 -4.212224 -4.2287679][-4.0524607 -4.0894818 -4.1080036 -4.1198196 -4.1190414 -4.0826459 -4.0519304 -4.0769262 -4.1304922 -4.1669674 -4.1764531 -4.1877985 -4.1991487 -4.2135868 -4.2310109][-4.07402 -4.1196871 -4.1357274 -4.1447453 -4.1467085 -4.1213136 -4.0928621 -4.0991693 -4.1309919 -4.1586452 -4.1698327 -4.1803632 -4.1902308 -4.2077675 -4.2278018][-4.1137176 -4.1602407 -4.1684947 -4.1681724 -4.1691446 -4.1534791 -4.1249676 -4.1170769 -4.1302371 -4.1456523 -4.1557679 -4.1704364 -4.1827717 -4.2041612 -4.2254868][-4.1217041 -4.1583786 -4.1630287 -4.1556678 -4.15235 -4.142036 -4.1151695 -4.1006351 -4.1040568 -4.112359 -4.1290207 -4.1554637 -4.1767688 -4.2045484 -4.2273512]]...]
INFO - root - 2017-12-06 00:35:34.646610: step 57110, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 69h:30m:26s remains)
INFO - root - 2017-12-06 00:35:44.081530: step 57120, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 72h:08m:53s remains)
INFO - root - 2017-12-06 00:35:53.405962: step 57130, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 73h:08m:37s remains)
INFO - root - 2017-12-06 00:36:02.731364: step 57140, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 72h:23m:50s remains)
INFO - root - 2017-12-06 00:36:12.124135: step 57150, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 73h:01m:42s remains)
INFO - root - 2017-12-06 00:36:21.649105: step 57160, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 71h:03m:05s remains)
INFO - root - 2017-12-06 00:36:30.982644: step 57170, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 69h:39m:30s remains)
INFO - root - 2017-12-06 00:36:40.411777: step 57180, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 73h:15m:09s remains)
INFO - root - 2017-12-06 00:36:49.871499: step 57190, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 68h:08m:58s remains)
INFO - root - 2017-12-06 00:36:59.066427: step 57200, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 68h:35m:00s remains)
2017-12-06 00:36:59.786806: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3244505 -4.3225889 -4.3240967 -4.3249531 -4.3221068 -4.3209372 -4.3229518 -4.3290844 -4.3346095 -4.3392367 -4.343328 -4.3475003 -4.3558769 -4.3644814 -4.3713155][-4.2981977 -4.2931991 -4.29704 -4.2990227 -4.2949014 -4.2930403 -4.2963672 -4.3053713 -4.3130846 -4.3179035 -4.3207626 -4.32504 -4.3392849 -4.3548722 -4.3655958][-4.2684894 -4.2621007 -4.269536 -4.2719164 -4.2607355 -4.2575336 -4.2623281 -4.2711987 -4.2785649 -4.2812042 -4.2839069 -4.2915845 -4.314899 -4.3368921 -4.35062][-4.2359223 -4.233243 -4.2440333 -4.243319 -4.225534 -4.221797 -4.2255778 -4.2301292 -4.2333918 -4.2318454 -4.23637 -4.2526603 -4.2863622 -4.3145795 -4.3332148][-4.1927881 -4.19498 -4.2062607 -4.204793 -4.1816988 -4.1776309 -4.1825037 -4.1816812 -4.1803107 -4.1760144 -4.1843967 -4.2100973 -4.2539272 -4.2905846 -4.3167982][-4.1441193 -4.1486216 -4.157311 -4.1526203 -4.1257977 -4.1201034 -4.1275616 -4.1233878 -4.1211047 -4.1261082 -4.1446657 -4.1765537 -4.2247591 -4.2682376 -4.3024015][-4.1139708 -4.114717 -4.1212339 -4.1136456 -4.0821242 -4.0716023 -4.0785174 -4.0760541 -4.0810761 -4.0986776 -4.1257491 -4.1602321 -4.2089915 -4.2559328 -4.2961617][-4.1137571 -4.11032 -4.1171341 -4.1116848 -4.0812697 -4.0620837 -4.06205 -4.0595446 -4.0631204 -4.07973 -4.1085958 -4.1452579 -4.19635 -4.2472372 -4.29328][-4.1464276 -4.1435413 -4.1481686 -4.1452394 -4.1225481 -4.0980039 -4.0874958 -4.0762944 -4.0688362 -4.0741506 -4.0955224 -4.1315589 -4.1863 -4.24265 -4.2927437][-4.1952605 -4.19359 -4.1981025 -4.1960211 -4.1811152 -4.1580253 -4.1407104 -4.1245828 -4.1056762 -4.0932331 -4.1002231 -4.1341095 -4.1912861 -4.25018 -4.29862][-4.2295785 -4.2257504 -4.2283235 -4.2297773 -4.2243838 -4.2081742 -4.1916981 -4.1759748 -4.1540518 -4.1330051 -4.1318316 -4.1646357 -4.21908 -4.2716188 -4.3097758][-4.240118 -4.2231374 -4.2148943 -4.2187624 -4.2250705 -4.2227631 -4.2174497 -4.2131691 -4.2017336 -4.1869459 -4.1848145 -4.21203 -4.2550197 -4.2936625 -4.3182054][-4.2519612 -4.2152481 -4.187448 -4.1850505 -4.198329 -4.2129955 -4.2265372 -4.2398372 -4.2437415 -4.2372794 -4.2312622 -4.2472796 -4.2766662 -4.3020048 -4.3169279][-4.2735906 -4.2214479 -4.1739273 -4.1547189 -4.1645217 -4.1904521 -4.2212992 -4.2525282 -4.2685871 -4.2658544 -4.2539 -4.2590723 -4.278461 -4.2955551 -4.3068452][-4.2949009 -4.2458506 -4.190834 -4.15573 -4.1504669 -4.1700535 -4.20498 -4.2474227 -4.2718391 -4.2715092 -4.2577977 -4.2578826 -4.2735314 -4.2858829 -4.2945576]]...]
INFO - root - 2017-12-06 00:37:09.005171: step 57210, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 72h:15m:49s remains)
INFO - root - 2017-12-06 00:37:17.959655: step 57220, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 71h:24m:16s remains)
INFO - root - 2017-12-06 00:37:27.002281: step 57230, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 56h:05m:24s remains)
INFO - root - 2017-12-06 00:37:36.464264: step 57240, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 72h:14m:18s remains)
INFO - root - 2017-12-06 00:37:45.910426: step 57250, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 73h:02m:59s remains)
INFO - root - 2017-12-06 00:37:55.526597: step 57260, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 72h:32m:50s remains)
INFO - root - 2017-12-06 00:38:04.853371: step 57270, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 70h:48m:04s remains)
INFO - root - 2017-12-06 00:38:14.096193: step 57280, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 69h:58m:52s remains)
INFO - root - 2017-12-06 00:38:23.215080: step 57290, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 69h:57m:31s remains)
INFO - root - 2017-12-06 00:38:32.604272: step 57300, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 71h:54m:37s remains)
2017-12-06 00:38:33.372333: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.30247 -4.2931132 -4.2921834 -4.2977791 -4.3064876 -4.3129764 -4.3144565 -4.3139896 -4.3159318 -4.3204284 -4.325532 -4.3290434 -4.3311524 -4.3328323 -4.33698][-4.2769661 -4.2589846 -4.2566285 -4.2664523 -4.2818737 -4.2925048 -4.2932911 -4.29029 -4.294456 -4.3026104 -4.3100767 -4.3135943 -4.3152781 -4.3170266 -4.3223791][-4.242723 -4.2146311 -4.2066946 -4.2192 -4.2435026 -4.2568612 -4.25119 -4.244236 -4.2560458 -4.2717381 -4.2831068 -4.2864428 -4.2880106 -4.2919426 -4.2991128][-4.1957712 -4.1586509 -4.1451883 -4.1578445 -4.1906457 -4.2078924 -4.192071 -4.1801414 -4.19994 -4.2247276 -4.2405386 -4.24669 -4.250525 -4.2581434 -4.2693563][-4.135015 -4.091104 -4.0728478 -4.0873771 -4.1273818 -4.14431 -4.1162481 -4.0923948 -4.1169553 -4.1529031 -4.1754808 -4.1847138 -4.1928873 -4.2102942 -4.2300525][-4.0761375 -4.0293036 -4.0095792 -4.0239081 -4.0608096 -4.0625553 -4.0116763 -3.9779646 -4.0122256 -4.06533 -4.0988579 -4.1147652 -4.1324382 -4.1649084 -4.1955376][-4.0347514 -3.9850571 -3.9664094 -3.982641 -4.0116158 -3.99341 -3.9248958 -3.8891361 -3.928998 -3.9877765 -4.0202088 -4.0326009 -4.0578356 -4.1120605 -4.1582122][-4.014091 -3.9653273 -3.9551914 -3.9794285 -4.006947 -3.9803255 -3.91401 -3.8871694 -3.9230084 -3.9715762 -3.9952016 -4.0053215 -4.0364513 -4.1021132 -4.1522632][-4.0314012 -3.9861271 -3.9822292 -4.0108743 -4.0370955 -4.0159979 -3.9717159 -3.9589407 -3.986764 -4.0225883 -4.0428629 -4.0601058 -4.0967722 -4.1563077 -4.1938782][-4.0921617 -4.0582027 -4.060904 -4.0855722 -4.1032143 -4.0870919 -4.0623603 -4.0576472 -4.0775609 -4.10357 -4.1222825 -4.1467123 -4.1859541 -4.2335019 -4.2567978][-4.1753159 -4.1538477 -4.1607056 -4.180295 -4.191618 -4.1817155 -4.170434 -4.170105 -4.1813478 -4.1977544 -4.2133369 -4.2378049 -4.2716808 -4.3048081 -4.3152943][-4.2448797 -4.2296548 -4.2367525 -4.2551255 -4.2667675 -4.2644849 -4.2609777 -4.2619839 -4.265955 -4.27384 -4.2854624 -4.3063793 -4.3325081 -4.3521204 -4.3529711][-4.2878466 -4.2799673 -4.2861609 -4.300777 -4.3114047 -4.3131795 -4.3130217 -4.3137445 -4.3138394 -4.316267 -4.3248668 -4.3396392 -4.3558455 -4.3659525 -4.3636374][-4.3141165 -4.3128781 -4.3184767 -4.3284283 -4.3363886 -4.3421168 -4.3438034 -4.342133 -4.3389473 -4.3384094 -4.3435264 -4.3516064 -4.3594646 -4.3643317 -4.3625908][-4.3310337 -4.3326936 -4.3374767 -4.343585 -4.3486595 -4.3524933 -4.3530073 -4.349617 -4.3452678 -4.3437886 -4.3458734 -4.3493147 -4.3526769 -4.3547716 -4.3547235]]...]
INFO - root - 2017-12-06 00:38:42.866387: step 57310, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 71h:00m:36s remains)
INFO - root - 2017-12-06 00:38:52.125343: step 57320, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 69h:05m:07s remains)
INFO - root - 2017-12-06 00:39:01.573430: step 57330, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 73h:35m:14s remains)
INFO - root - 2017-12-06 00:39:10.909247: step 57340, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 67h:30m:57s remains)
INFO - root - 2017-12-06 00:39:20.161888: step 57350, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 68h:09m:56s remains)
INFO - root - 2017-12-06 00:39:29.441787: step 57360, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 74h:44m:36s remains)
INFO - root - 2017-12-06 00:39:38.576805: step 57370, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 70h:38m:55s remains)
INFO - root - 2017-12-06 00:39:48.161553: step 57380, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 72h:56m:04s remains)
INFO - root - 2017-12-06 00:39:57.338945: step 57390, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 73h:36m:30s remains)
INFO - root - 2017-12-06 00:40:06.706497: step 57400, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 68h:58m:39s remains)
2017-12-06 00:40:07.535601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3423653 -4.3318186 -4.3132839 -4.2734432 -4.2007513 -4.1222558 -4.0901451 -4.1014819 -4.124299 -4.1393013 -4.1480141 -4.1589818 -4.1822662 -4.2113972 -4.2252846][-4.33895 -4.3272996 -4.3067164 -4.2621984 -4.1805525 -4.0906396 -4.0555091 -4.0715551 -4.0983686 -4.1100149 -4.1130638 -4.1265812 -4.1594238 -4.2015867 -4.22161][-4.3360853 -4.321846 -4.2993011 -4.2513208 -4.1648707 -4.0728469 -4.0376983 -4.0607562 -4.0960121 -4.1046948 -4.0977621 -4.1077666 -4.1473708 -4.2000937 -4.2240429][-4.3350434 -4.3192554 -4.295845 -4.2444148 -4.1541853 -4.0597792 -4.0268574 -4.0630555 -4.11355 -4.1244078 -4.1093316 -4.1138034 -4.1533957 -4.2035637 -4.2214918][-4.3359342 -4.3201709 -4.2977762 -4.2456369 -4.1524186 -4.0510039 -4.0197124 -4.0753756 -4.1480565 -4.1665292 -4.1447945 -4.1413212 -4.1750784 -4.2168751 -4.225472][-4.3375168 -4.3220439 -4.3000588 -4.2482524 -4.1499386 -4.03444 -3.9999228 -4.0747409 -4.1702666 -4.1982379 -4.1716242 -4.1622248 -4.19167 -4.2287784 -4.2342291][-4.3390212 -4.3234777 -4.3013444 -4.2476616 -4.1379032 -4.0000863 -3.9509082 -4.0403152 -4.1558537 -4.1969061 -4.1801653 -4.1727333 -4.1983275 -4.2291269 -4.2302423][-4.3398604 -4.3249292 -4.3032675 -4.2482572 -4.126924 -3.9655509 -3.8932457 -3.9897141 -4.11707 -4.1743889 -4.1761966 -4.1785674 -4.20052 -4.221992 -4.2164679][-4.3404541 -4.3259463 -4.3061905 -4.2537222 -4.134398 -3.9738381 -3.8974776 -3.9836514 -4.0976324 -4.1533704 -4.1670208 -4.1768532 -4.1985173 -4.2124686 -4.2027321][-4.3407521 -4.3264832 -4.3096251 -4.2640944 -4.1614165 -4.0300126 -3.9734409 -4.0376835 -4.1195865 -4.1543584 -4.1638241 -4.1766038 -4.2014489 -4.2158351 -4.2025323][-4.3409719 -4.326232 -4.3120131 -4.2732654 -4.1876445 -4.086854 -4.05275 -4.1016197 -4.1551728 -4.1708817 -4.1735373 -4.1861029 -4.2119937 -4.2272449 -4.213335][-4.3413033 -4.3245344 -4.3100066 -4.2752986 -4.2025423 -4.1252213 -4.104465 -4.1443648 -4.1788354 -4.1834207 -4.1803722 -4.1901984 -4.2149906 -4.2322421 -4.2193232][-4.3410125 -4.3233232 -4.3068824 -4.272162 -4.2082791 -4.1474924 -4.1339564 -4.1637216 -4.187057 -4.1853023 -4.1775203 -4.1845846 -4.2079296 -4.2269225 -4.2164679][-4.3419495 -4.3246565 -4.3081574 -4.2722783 -4.2125611 -4.1596179 -4.1478591 -4.169096 -4.1851912 -4.1815462 -4.170156 -4.1732326 -4.1953955 -4.2156248 -4.208837][-4.3433933 -4.3285036 -4.3144841 -4.2793961 -4.2234192 -4.17533 -4.15899 -4.1701984 -4.1809783 -4.1754847 -4.1649332 -4.165174 -4.1870174 -4.2071366 -4.2024622]]...]
INFO - root - 2017-12-06 00:40:16.965572: step 57410, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 72h:42m:06s remains)
INFO - root - 2017-12-06 00:40:26.356782: step 57420, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 75h:03m:37s remains)
INFO - root - 2017-12-06 00:40:35.528827: step 57430, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 70h:46m:35s remains)
INFO - root - 2017-12-06 00:40:44.974337: step 57440, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.969 sec/batch; 74h:01m:24s remains)
INFO - root - 2017-12-06 00:40:54.310774: step 57450, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 71h:07m:12s remains)
INFO - root - 2017-12-06 00:41:03.737027: step 57460, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 70h:49m:47s remains)
INFO - root - 2017-12-06 00:41:13.137049: step 57470, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 68h:58m:03s remains)
INFO - root - 2017-12-06 00:41:22.449015: step 57480, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 73h:24m:53s remains)
INFO - root - 2017-12-06 00:41:31.676888: step 57490, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 68h:44m:45s remains)
INFO - root - 2017-12-06 00:41:40.772165: step 57500, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 68h:29m:50s remains)
2017-12-06 00:41:41.545663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3541784 -4.3750677 -4.3767233 -4.35704 -4.315309 -4.2714272 -4.2218447 -4.1726489 -4.1436815 -4.1471262 -4.1789255 -4.2217393 -4.2628217 -4.295352 -4.3153834][-4.3683362 -4.3912978 -4.3903093 -4.3617697 -4.3104591 -4.2543554 -4.1927571 -4.1418462 -4.1136274 -4.125905 -4.1707873 -4.2215762 -4.265275 -4.2974305 -4.319294][-4.3733091 -4.3972564 -4.3914394 -4.3579593 -4.3040977 -4.2391038 -4.170433 -4.1255198 -4.1029897 -4.1194921 -4.164567 -4.2131357 -4.2572775 -4.291471 -4.3173003][-4.3807859 -4.4025955 -4.3921089 -4.3541703 -4.300334 -4.2321887 -4.1642718 -4.1309638 -4.1187148 -4.1346812 -4.167829 -4.2073345 -4.2481937 -4.2850986 -4.3166485][-4.3886776 -4.4038277 -4.3845892 -4.3386216 -4.282052 -4.2117229 -4.1460419 -4.1170392 -4.1144361 -4.1325893 -4.1566768 -4.1932797 -4.2338037 -4.2751474 -4.3127546][-4.3868046 -4.3907409 -4.3565683 -4.2951136 -4.2292204 -4.1559706 -4.0881357 -4.0505681 -4.0512676 -4.0787582 -4.1061783 -4.1490617 -4.2044482 -4.2591853 -4.3016992][-4.3683338 -4.35557 -4.3013167 -4.2198744 -4.1391268 -4.0585752 -3.9905233 -3.9495196 -3.9576757 -3.9993784 -4.0423341 -4.1040263 -4.1789703 -4.2410727 -4.2821879][-4.3374715 -4.3104253 -4.2405381 -4.1454787 -4.0579243 -3.979604 -3.924757 -3.8893893 -3.8976567 -3.9440815 -3.9958844 -4.0689988 -4.1512303 -4.212944 -4.2486048][-4.3127365 -4.2779307 -4.2080846 -4.1176691 -4.03652 -3.9741132 -3.9394946 -3.9133885 -3.9135182 -3.9499128 -3.9997413 -4.069284 -4.13903 -4.185401 -4.2050495][-4.312614 -4.2853317 -4.232852 -4.1655989 -4.1014023 -4.0596685 -4.0413485 -4.0246258 -4.0162015 -4.0340152 -4.0704131 -4.1247697 -4.1646452 -4.1782651 -4.170105][-4.3210187 -4.3145976 -4.2924013 -4.2602444 -4.2212358 -4.1932178 -4.1773882 -4.1553674 -4.1377993 -4.1401086 -4.1656666 -4.1987214 -4.2043276 -4.1816816 -4.1427569][-4.3163228 -4.3295631 -4.334446 -4.3254685 -4.3013325 -4.2784472 -4.2597089 -4.2371607 -4.2232194 -4.2244797 -4.2471638 -4.2661405 -4.2477098 -4.1992326 -4.1375823][-4.2952023 -4.319766 -4.3379688 -4.3358531 -4.3145065 -4.2967215 -4.2776809 -4.262115 -4.2614493 -4.274292 -4.3026538 -4.3166213 -4.2930956 -4.2412953 -4.1743059][-4.2600923 -4.2866015 -4.3059812 -4.3014216 -4.2760448 -4.256424 -4.2344689 -4.2217145 -4.2359896 -4.2667828 -4.3084326 -4.3352528 -4.3286037 -4.2891212 -4.2292447][-4.2258534 -4.2437186 -4.2557058 -4.246892 -4.2161913 -4.1876884 -4.1583395 -4.1461692 -4.1698074 -4.2225642 -4.2835321 -4.3300047 -4.3454719 -4.3230228 -4.2778692]]...]
INFO - root - 2017-12-06 00:41:50.740302: step 57510, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 72h:15m:44s remains)
INFO - root - 2017-12-06 00:42:00.122592: step 57520, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 71h:28m:53s remains)
INFO - root - 2017-12-06 00:42:09.246126: step 57530, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 74h:06m:10s remains)
INFO - root - 2017-12-06 00:42:18.702320: step 57540, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 73h:58m:42s remains)
INFO - root - 2017-12-06 00:42:27.879370: step 57550, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 71h:43m:52s remains)
INFO - root - 2017-12-06 00:42:37.346727: step 57560, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 72h:53m:30s remains)
INFO - root - 2017-12-06 00:42:46.652849: step 57570, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 74h:39m:15s remains)
INFO - root - 2017-12-06 00:42:55.989201: step 57580, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 70h:37m:32s remains)
INFO - root - 2017-12-06 00:43:05.445542: step 57590, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 70h:35m:20s remains)
INFO - root - 2017-12-06 00:43:14.401031: step 57600, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 71h:32m:21s remains)
2017-12-06 00:43:15.177867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1952591 -4.1954923 -4.1897287 -4.1721416 -4.1602573 -4.1598263 -4.1534042 -4.1385884 -4.1435041 -4.1656775 -4.20105 -4.2305675 -4.2357574 -4.2196264 -4.1911087][-4.2182198 -4.2204828 -4.2107353 -4.184608 -4.1637149 -4.1556211 -4.1479764 -4.135035 -4.1452 -4.1743975 -4.2115245 -4.23858 -4.2404809 -4.2190113 -4.1831141][-4.2470269 -4.2469006 -4.231308 -4.1962938 -4.1647267 -4.144803 -4.1309505 -4.1212683 -4.1386447 -4.177114 -4.2164845 -4.2427497 -4.2455683 -4.2208128 -4.17791][-4.2629876 -4.2590218 -4.2384872 -4.1977634 -4.1555738 -4.1239953 -4.1025534 -4.0934992 -4.1173744 -4.1653256 -4.2076597 -4.2351012 -4.2400441 -4.2140703 -4.1661224][-4.26713 -4.2609415 -4.2381225 -4.1934366 -4.1435804 -4.103291 -4.0735459 -4.0621834 -4.0917139 -4.148788 -4.1927838 -4.2196612 -4.2272086 -4.2047348 -4.1569753][-4.2637844 -4.2554989 -4.2289839 -4.1836486 -4.1354547 -4.0904503 -4.0536666 -4.037962 -4.0721145 -4.1388416 -4.1873012 -4.212718 -4.2208729 -4.2004905 -4.1557832][-4.2528172 -4.2423635 -4.2137547 -4.1708207 -4.12836 -4.0806088 -4.0372443 -4.0185971 -4.0624361 -4.1403823 -4.1942091 -4.21886 -4.226265 -4.2063475 -4.1663589][-4.2432795 -4.2289209 -4.1984224 -4.1593366 -4.1230474 -4.0717125 -4.0201283 -3.999362 -4.0537233 -4.1423059 -4.2005725 -4.2280226 -4.236526 -4.2182169 -4.1841035][-4.2354374 -4.2158828 -4.1838436 -4.150795 -4.1234465 -4.0710807 -4.0090175 -3.9806819 -4.0400414 -4.1338854 -4.1941614 -4.2277141 -4.2422957 -4.2302222 -4.2026162][-4.2271128 -4.2040443 -4.1701093 -4.1423893 -4.1243005 -4.07326 -4.000741 -3.9665611 -4.0306106 -4.1233792 -4.1820688 -4.2205205 -4.2427478 -4.23441 -4.2090235][-4.2225542 -4.1964655 -4.1610546 -4.1359272 -4.1221495 -4.0749226 -4.0012789 -3.9727113 -4.0406513 -4.123826 -4.1735139 -4.2100954 -4.2367296 -4.2304597 -4.2059455][-4.2188411 -4.1950636 -4.1638169 -4.1404729 -4.1267228 -4.0853267 -4.0225711 -4.0073686 -4.06924 -4.133512 -4.1674042 -4.1987929 -4.2264838 -4.2232394 -4.2001061][-4.220314 -4.2038007 -4.180532 -4.1599836 -4.1437931 -4.1081538 -4.0597525 -4.055027 -4.1019292 -4.1426573 -4.1587267 -4.1861358 -4.2159166 -4.2184649 -4.2001138][-4.2178893 -4.2107477 -4.1975107 -4.1825027 -4.1643596 -4.1336579 -4.0998092 -4.0985303 -4.1232314 -4.1382027 -4.1395917 -4.1658845 -4.2007685 -4.21188 -4.199101][-4.2025628 -4.2030764 -4.1985931 -4.1882658 -4.1699324 -4.146647 -4.1286025 -4.12742 -4.1318951 -4.1258397 -4.1167645 -4.1423392 -4.1827593 -4.2015519 -4.194375]]...]
INFO - root - 2017-12-06 00:43:24.494425: step 57610, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 72h:28m:58s remains)
INFO - root - 2017-12-06 00:43:33.901589: step 57620, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 72h:14m:44s remains)
INFO - root - 2017-12-06 00:43:43.210916: step 57630, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.984 sec/batch; 75h:08m:47s remains)
INFO - root - 2017-12-06 00:43:52.563593: step 57640, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 75h:41m:38s remains)
INFO - root - 2017-12-06 00:44:01.870750: step 57650, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 72h:50m:44s remains)
INFO - root - 2017-12-06 00:44:10.994337: step 57660, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 70h:39m:44s remains)
INFO - root - 2017-12-06 00:44:20.714583: step 57670, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 73h:03m:58s remains)
INFO - root - 2017-12-06 00:44:29.916878: step 57680, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 75h:48m:35s remains)
INFO - root - 2017-12-06 00:44:39.003348: step 57690, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 71h:10m:12s remains)
INFO - root - 2017-12-06 00:44:48.348131: step 57700, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 71h:06m:23s remains)
2017-12-06 00:44:49.215271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3049603 -4.3089848 -4.3112607 -4.3097415 -4.3089 -4.299521 -4.2788887 -4.2468662 -4.2046051 -4.1653929 -4.1463532 -4.1507077 -4.175674 -4.2047462 -4.2098374][-4.3153963 -4.3155084 -4.3133683 -4.3082919 -4.3022332 -4.2812471 -4.2469454 -4.1992512 -4.1394815 -4.0922408 -4.075614 -4.0897388 -4.1292133 -4.1710629 -4.1760941][-4.3130412 -4.3076596 -4.3009195 -4.294107 -4.2837911 -4.2552838 -4.2099924 -4.1489816 -4.0785632 -4.0299611 -4.0250812 -4.0569773 -4.1100073 -4.1553636 -4.15224][-4.3080678 -4.2944117 -4.2832479 -4.2780261 -4.2692623 -4.2421546 -4.1913538 -4.1225166 -4.0538888 -4.0196037 -4.03534 -4.0810061 -4.13321 -4.1676993 -4.1515441][-4.2901073 -4.2696452 -4.2577696 -4.25825 -4.2561626 -4.2338228 -4.1838017 -4.1148591 -4.0573926 -4.0450716 -4.0762453 -4.1203227 -4.1609373 -4.1771703 -4.1482878][-4.2704082 -4.2454023 -4.234931 -4.2414675 -4.2479992 -4.2321334 -4.1845989 -4.1142087 -4.0633945 -4.0645151 -4.1043253 -4.1456342 -4.17865 -4.1788206 -4.1399412][-4.2527733 -4.228209 -4.2219644 -4.2356071 -4.2498894 -4.2359123 -4.1848311 -4.1076016 -4.0561423 -4.0631094 -4.1118112 -4.1597247 -4.1880956 -4.1747818 -4.1244626][-4.2259645 -4.2077351 -4.2133284 -4.2398119 -4.2607927 -4.2463927 -4.1890635 -4.1008377 -4.0459242 -4.058434 -4.1178126 -4.1720247 -4.1961093 -4.1740694 -4.1200118][-4.2004004 -4.1921539 -4.2156463 -4.2545667 -4.2818804 -4.266366 -4.2019687 -4.1088791 -4.0577188 -4.0782557 -4.1397438 -4.1912422 -4.2085543 -4.1810555 -4.1326132][-4.1906753 -4.1895547 -4.2251816 -4.2692318 -4.2990465 -4.286459 -4.2222652 -4.1361904 -4.0964265 -4.1195116 -4.1735139 -4.2176805 -4.2259684 -4.1963935 -4.1537495][-4.2052588 -4.2033577 -4.237936 -4.2794309 -4.3092766 -4.304049 -4.2532306 -4.1854086 -4.1549931 -4.1688824 -4.2099204 -4.2466283 -4.2492309 -4.2224832 -4.187027][-4.2390137 -4.2344704 -4.256958 -4.2869124 -4.3154783 -4.3194156 -4.2874026 -4.2400613 -4.211894 -4.2151132 -4.2447186 -4.2769117 -4.2809696 -4.2625237 -4.2349281][-4.2790132 -4.2741675 -4.283349 -4.2998338 -4.3239069 -4.3334551 -4.3167925 -4.2840652 -4.2559834 -4.2536416 -4.2758136 -4.3039422 -4.3140779 -4.3059273 -4.2853436][-4.3041458 -4.3016586 -4.3057113 -4.3148808 -4.3331828 -4.3450174 -4.3382597 -4.31419 -4.2877536 -4.2816849 -4.2964072 -4.3181763 -4.3323379 -4.3333278 -4.3200464][-4.3066487 -4.3091536 -4.3147106 -4.3219819 -4.3356953 -4.347476 -4.3470263 -4.3283229 -4.3037543 -4.2934947 -4.3011909 -4.3169775 -4.3315363 -4.3374395 -4.3302474]]...]
INFO - root - 2017-12-06 00:44:58.677796: step 57710, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 71h:41m:57s remains)
INFO - root - 2017-12-06 00:45:07.966086: step 57720, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 71h:51m:46s remains)
INFO - root - 2017-12-06 00:45:17.060066: step 57730, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 67h:39m:12s remains)
INFO - root - 2017-12-06 00:45:26.389612: step 57740, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 71h:35m:24s remains)
INFO - root - 2017-12-06 00:45:35.720558: step 57750, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.917 sec/batch; 69h:57m:15s remains)
INFO - root - 2017-12-06 00:45:45.067061: step 57760, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 69h:20m:11s remains)
INFO - root - 2017-12-06 00:45:54.418801: step 57770, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 73h:14m:52s remains)
INFO - root - 2017-12-06 00:46:03.635319: step 57780, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 71h:34m:53s remains)
INFO - root - 2017-12-06 00:46:12.826155: step 57790, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 72h:47m:25s remains)
INFO - root - 2017-12-06 00:46:22.272468: step 57800, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 71h:35m:39s remains)
2017-12-06 00:46:23.064473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2169819 -4.2135415 -4.2313571 -4.2500706 -4.2622371 -4.2703991 -4.2751989 -4.2808447 -4.2817354 -4.2739177 -4.2568836 -4.2389951 -4.2285862 -4.2198024 -4.2180653][-4.2146134 -4.2125545 -4.2371073 -4.2585568 -4.2684383 -4.2711892 -4.270174 -4.2755084 -4.2861619 -4.2925181 -4.2834277 -4.2687931 -4.2553082 -4.2427177 -4.2376165][-4.2240138 -4.2208209 -4.246891 -4.2675233 -4.2722406 -4.2642674 -4.2534337 -4.2551122 -4.2735219 -4.2948933 -4.29457 -4.2817574 -4.2664571 -4.2512035 -4.242836][-4.2312942 -4.2265077 -4.249835 -4.2692051 -4.2716022 -4.2507668 -4.2247529 -4.2178779 -4.2408948 -4.2740121 -4.282207 -4.2733617 -4.2594552 -4.2450843 -4.2367692][-4.2342649 -4.2298932 -4.2480941 -4.2606015 -4.2558141 -4.2205515 -4.1741953 -4.1565943 -4.1874437 -4.2373538 -4.258934 -4.2570448 -4.2456679 -4.2321825 -4.2279816][-4.2297416 -4.226572 -4.2408404 -4.2477241 -4.2329044 -4.1805959 -4.109251 -4.071981 -4.1081281 -4.1837058 -4.2295032 -4.2382464 -4.230021 -4.2168641 -4.2144284][-4.2190328 -4.21581 -4.2255583 -4.2219162 -4.1946383 -4.1230841 -4.0198979 -3.9444208 -3.9779072 -4.0908003 -4.1782608 -4.2110033 -4.2154088 -4.2072153 -4.2080441][-4.203877 -4.1944547 -4.1987829 -4.1871462 -4.14768 -4.0582461 -3.9217529 -3.7994678 -3.821888 -3.9766026 -4.1125488 -4.1806436 -4.20526 -4.204071 -4.2084303][-4.1957493 -4.1820035 -4.1836047 -4.1747751 -4.140635 -4.0587015 -3.928575 -3.8026257 -3.8101778 -3.9570577 -4.103642 -4.1865482 -4.21797 -4.2106171 -4.2037349][-4.2171583 -4.2061715 -4.2089071 -4.2041388 -4.1834545 -4.1283283 -4.0390782 -3.9505486 -3.9473867 -4.0440965 -4.1551232 -4.2228932 -4.2445478 -4.2315311 -4.2137318][-4.2535911 -4.247961 -4.2550168 -4.2543044 -4.241941 -4.2077427 -4.1533084 -4.0974932 -4.0889235 -4.142695 -4.214993 -4.2626677 -4.27674 -4.2670364 -4.2497272][-4.2754073 -4.2735181 -4.2859564 -4.2911973 -4.2854633 -4.2653885 -4.2363315 -4.2048631 -4.1977992 -4.2248335 -4.2681136 -4.3000183 -4.3079295 -4.3010745 -4.2869434][-4.2921114 -4.2948194 -4.3072495 -4.3138523 -4.311398 -4.3003321 -4.2868204 -4.27202 -4.2696266 -4.2819028 -4.302701 -4.3205209 -4.323143 -4.3165984 -4.3054676][-4.3097849 -4.3138328 -4.3210158 -4.32451 -4.3219342 -4.3151388 -4.3090944 -4.3026071 -4.3026934 -4.3088665 -4.3170676 -4.3243427 -4.3236108 -4.3177018 -4.309238][-4.3144126 -4.3170652 -4.3208613 -4.3230805 -4.3212605 -4.3168912 -4.3132572 -4.3105931 -4.311028 -4.3142676 -4.3188739 -4.3222995 -4.3213744 -4.3187037 -4.3139172]]...]
INFO - root - 2017-12-06 00:46:32.613924: step 57810, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 72h:17m:01s remains)
INFO - root - 2017-12-06 00:46:42.023098: step 57820, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 72h:48m:19s remains)
INFO - root - 2017-12-06 00:46:51.559006: step 57830, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 75h:35m:34s remains)
INFO - root - 2017-12-06 00:47:00.822400: step 57840, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 75h:05m:55s remains)
INFO - root - 2017-12-06 00:47:09.967058: step 57850, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 70h:59m:15s remains)
INFO - root - 2017-12-06 00:47:19.375236: step 57860, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.934 sec/batch; 71h:17m:20s remains)
INFO - root - 2017-12-06 00:47:28.585538: step 57870, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 67h:39m:30s remains)
INFO - root - 2017-12-06 00:47:37.974264: step 57880, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 70h:28m:52s remains)
INFO - root - 2017-12-06 00:47:47.184791: step 57890, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 70h:55m:58s remains)
INFO - root - 2017-12-06 00:47:56.709268: step 57900, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 71h:11m:48s remains)
2017-12-06 00:47:57.500288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21862 -4.2280836 -4.236506 -4.2458053 -4.2548695 -4.2543135 -4.2461109 -4.23756 -4.2271314 -4.2211843 -4.22299 -4.224721 -4.2311306 -4.2431812 -4.2518377][-4.1974864 -4.2072978 -4.2204428 -4.2295952 -4.2284932 -4.2121944 -4.1850138 -4.1602254 -4.144135 -4.1485348 -4.1708078 -4.1873035 -4.2064247 -4.224493 -4.2313552][-4.1958003 -4.2055173 -4.215703 -4.2176137 -4.2015572 -4.1592579 -4.0937076 -4.0353541 -4.0189643 -4.0562172 -4.1130109 -4.1503868 -4.1847129 -4.2090216 -4.2139788][-4.2071977 -4.21833 -4.2203722 -4.2096434 -4.1788273 -4.1059904 -3.9944365 -3.9011261 -3.8983381 -3.9782917 -4.0757 -4.1393175 -4.1822424 -4.2037535 -4.1999512][-4.2237062 -4.2344246 -4.2318811 -4.2163973 -4.1766729 -4.0845985 -3.945833 -3.8437078 -3.8592105 -3.9632936 -4.0752625 -4.1445851 -4.1799078 -4.1908731 -4.1743755][-4.2421141 -4.2512321 -4.2477779 -4.2293825 -4.1886077 -4.0962987 -3.9642148 -3.8785553 -3.911715 -4.012888 -4.1162162 -4.1730256 -4.1862507 -4.1800742 -4.1477442][-4.246356 -4.2549844 -4.2551894 -4.2436614 -4.2097545 -4.1320395 -4.0227203 -3.9612865 -3.9974399 -4.0844398 -4.1674457 -4.20928 -4.2050638 -4.1845441 -4.1416883][-4.223978 -4.2300525 -4.2343946 -4.2321525 -4.2120786 -4.1606574 -4.089777 -4.0584965 -4.089982 -4.1503143 -4.2008357 -4.2258549 -4.2113018 -4.1860704 -4.1453452][-4.1696424 -4.1766348 -4.1860452 -4.1925507 -4.1857681 -4.1600876 -4.1296496 -4.126545 -4.1528316 -4.1895971 -4.212779 -4.21891 -4.1953225 -4.1723251 -4.1425261][-4.137455 -4.143065 -4.1575475 -4.1694803 -4.1691861 -4.1532774 -4.14857 -4.1617 -4.1862111 -4.2082195 -4.2114654 -4.2021465 -4.1778793 -4.1629786 -4.145514][-4.1566372 -4.1576567 -4.1676722 -4.1756744 -4.1724596 -4.1619396 -4.1674538 -4.1819596 -4.2007036 -4.2163844 -4.2135205 -4.1983905 -4.1822753 -4.1726227 -4.1637354][-4.2009416 -4.198987 -4.1995206 -4.1982975 -4.1904907 -4.1872926 -4.1983714 -4.2089372 -4.2197027 -4.2285891 -4.2261786 -4.2109938 -4.2038832 -4.2000408 -4.1972389][-4.2481823 -4.2455459 -4.2381811 -4.2302151 -4.2218885 -4.22322 -4.23924 -4.2473621 -4.2537689 -4.257031 -4.2515531 -4.2341728 -4.2278342 -4.2298145 -4.230741][-4.29842 -4.2963576 -4.2859459 -4.2739024 -4.2680206 -4.2730384 -4.2898784 -4.2958245 -4.2977109 -4.2944655 -4.2849836 -4.2681417 -4.2612734 -4.2631459 -4.262342][-4.32881 -4.3298936 -4.3213611 -4.3113813 -4.309463 -4.3161788 -4.3277626 -4.3312745 -4.3317637 -4.3277106 -4.3186631 -4.3065343 -4.3028011 -4.3031487 -4.2984605]]...]
INFO - root - 2017-12-06 00:48:06.821713: step 57910, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 68h:04m:08s remains)
INFO - root - 2017-12-06 00:48:16.015148: step 57920, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.869 sec/batch; 66h:19m:07s remains)
INFO - root - 2017-12-06 00:48:25.389930: step 57930, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 69h:24m:29s remains)
INFO - root - 2017-12-06 00:48:34.792427: step 57940, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 69h:20m:26s remains)
INFO - root - 2017-12-06 00:48:44.110422: step 57950, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 69h:33m:04s remains)
INFO - root - 2017-12-06 00:48:53.513603: step 57960, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 68h:42m:45s remains)
INFO - root - 2017-12-06 00:49:02.852872: step 57970, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 70h:00m:21s remains)
INFO - root - 2017-12-06 00:49:12.044161: step 57980, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 68h:47m:36s remains)
INFO - root - 2017-12-06 00:49:21.372185: step 57990, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.940 sec/batch; 71h:41m:37s remains)
INFO - root - 2017-12-06 00:49:30.750990: step 58000, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 73h:29m:06s remains)
2017-12-06 00:49:31.550756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3105783 -4.3006964 -4.2993364 -4.3047366 -4.3130946 -4.316709 -4.3230715 -4.3321857 -4.338007 -4.3391409 -4.3417125 -4.3496571 -4.3614926 -4.3710613 -4.3773613][-4.2770123 -4.2618074 -4.2621493 -4.274332 -4.2914782 -4.2984385 -4.3067732 -4.3184862 -4.3228316 -4.3218842 -4.323863 -4.334764 -4.3518028 -4.3652673 -4.3749862][-4.2341032 -4.2141857 -4.2127056 -4.2288694 -4.2564282 -4.2721391 -4.2871413 -4.3013554 -4.3013811 -4.2967286 -4.2978835 -4.3133936 -4.3356962 -4.3531928 -4.3672667][-4.1812878 -4.1604896 -4.1551733 -4.1691279 -4.2018218 -4.2284937 -4.2565479 -4.2759295 -4.2725968 -4.2618723 -4.2596126 -4.2766795 -4.30446 -4.3279057 -4.3488555][-4.1151423 -4.0979819 -4.0883918 -4.0945129 -4.1234365 -4.1563263 -4.1985555 -4.2306786 -4.229651 -4.2165289 -4.2127585 -4.2317243 -4.2656059 -4.2966604 -4.3254237][-4.0498781 -4.026176 -4.0038528 -3.9982 -4.0223484 -4.0595946 -4.1161113 -4.1619349 -4.1698604 -4.1612511 -4.1632156 -4.1887145 -4.2297254 -4.2669625 -4.3033857][-4.0174727 -3.9840727 -3.951344 -3.9383504 -3.9546564 -3.9915602 -4.0559459 -4.1059766 -4.1181607 -4.1154208 -4.1246905 -4.158227 -4.2076817 -4.2504683 -4.2915797][-4.0341759 -4.0070853 -3.9761353 -3.955673 -3.959281 -3.983737 -4.0415277 -4.0885267 -4.0955157 -4.0986314 -4.1140184 -4.1539841 -4.2075052 -4.2519054 -4.2953568][-4.0726428 -4.0693197 -4.0472708 -4.021286 -4.0088921 -4.0194488 -4.064714 -4.0999751 -4.1008191 -4.1055393 -4.1225424 -4.1662841 -4.22053 -4.2634187 -4.3062525][-4.0737782 -4.0931425 -4.0892162 -4.0729232 -4.0598588 -4.0667243 -4.0981789 -4.1168919 -4.1120753 -4.1154504 -4.1336865 -4.1786866 -4.229394 -4.2719831 -4.31377][-4.0224624 -4.0472264 -4.0601215 -4.0608144 -4.0577993 -4.0698476 -4.0998712 -4.1108208 -4.1008682 -4.107738 -4.1267266 -4.1732497 -4.2262878 -4.27376 -4.3165007][-3.9815085 -3.9925621 -4.0098677 -4.0226779 -4.0316768 -4.0553389 -4.0965304 -4.1159763 -4.1059279 -4.1041651 -4.1160307 -4.1601663 -4.2199917 -4.2728057 -4.3190546][-3.9976976 -3.9904401 -4.0013885 -4.0132523 -4.0244074 -4.0529995 -4.1040273 -4.1383867 -4.1368632 -4.127099 -4.1272626 -4.1648912 -4.223083 -4.277925 -4.3251729][-4.0570183 -4.0391531 -4.0375271 -4.0380716 -4.0436463 -4.0726633 -4.1253672 -4.1680269 -4.1738787 -4.1623244 -4.1587 -4.1914587 -4.2430272 -4.2941585 -4.3353534][-4.1097364 -4.092411 -4.0827656 -4.0712404 -4.0739355 -4.1023574 -4.1509447 -4.1890936 -4.1915331 -4.1807723 -4.1799989 -4.2120051 -4.2609186 -4.3088255 -4.3451171]]...]
INFO - root - 2017-12-06 00:49:41.144484: step 58010, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 72h:39m:17s remains)
INFO - root - 2017-12-06 00:49:50.418849: step 58020, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.961 sec/batch; 73h:15m:00s remains)
INFO - root - 2017-12-06 00:49:59.745144: step 58030, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 68h:39m:45s remains)
INFO - root - 2017-12-06 00:50:08.903720: step 58040, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.968 sec/batch; 73h:47m:29s remains)
INFO - root - 2017-12-06 00:50:18.252680: step 58050, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 74h:09m:46s remains)
INFO - root - 2017-12-06 00:50:27.524409: step 58060, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 72h:57m:52s remains)
INFO - root - 2017-12-06 00:50:36.996615: step 58070, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 72h:07m:20s remains)
INFO - root - 2017-12-06 00:50:46.275277: step 58080, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 74h:39m:18s remains)
INFO - root - 2017-12-06 00:50:55.760345: step 58090, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 70h:27m:55s remains)
INFO - root - 2017-12-06 00:51:05.040550: step 58100, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 70h:18m:09s remains)
2017-12-06 00:51:05.780594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2740149 -4.309689 -4.3273721 -4.3070755 -4.2524343 -4.1889524 -4.1442885 -4.1320133 -4.1601424 -4.199934 -4.224792 -4.2108235 -4.1602716 -4.0913563 -4.0351391][-4.2504363 -4.3003249 -4.3203654 -4.2969851 -4.2387552 -4.1776018 -4.1400061 -4.1306529 -4.1499867 -4.183145 -4.2118669 -4.2035112 -4.1543164 -4.07782 -4.0142651][-4.2283812 -4.2698622 -4.2815824 -4.2548981 -4.2052994 -4.1593103 -4.1366954 -4.1297927 -4.1382 -4.1623011 -4.1893687 -4.1843452 -4.1349144 -4.0537739 -3.9856305][-4.2135053 -4.2429833 -4.2444067 -4.2120738 -4.1710424 -4.139945 -4.1293564 -4.1250353 -4.1326938 -4.1552277 -4.181231 -4.1771407 -4.129251 -4.0488434 -3.9811466][-4.1958246 -4.2183118 -4.2156496 -4.1770778 -4.1363149 -4.1196146 -4.1237397 -4.1292834 -4.1422367 -4.1686192 -4.1936121 -4.1889796 -4.138814 -4.0615988 -3.9985754][-4.1590834 -4.1820412 -4.1906924 -4.1557217 -4.1180744 -4.109426 -4.116972 -4.1241384 -4.1395397 -4.1679406 -4.1920242 -4.1862321 -4.1349831 -4.0673013 -4.0157294][-4.12072 -4.1433196 -4.1662626 -4.1419344 -4.108222 -4.0986166 -4.0959125 -4.0948591 -4.1143341 -4.1492214 -4.174778 -4.1687479 -4.122664 -4.0664783 -4.0282907][-4.09282 -4.1135683 -4.1382527 -4.1229868 -4.0948439 -4.0789638 -4.0618544 -4.0513797 -4.0763435 -4.1222458 -4.1536775 -4.1470327 -4.1016455 -4.0479774 -4.0189214][-4.0728884 -4.0963769 -4.1151376 -4.1000886 -4.0769629 -4.0595856 -4.0359387 -4.0191145 -4.0468678 -4.0981531 -4.1296277 -4.1196051 -4.070797 -4.0196447 -4.0011358][-4.0993071 -4.1192007 -4.1317892 -4.117094 -4.0983181 -4.0836239 -4.0598421 -4.0422111 -4.0659127 -4.1151972 -4.1436243 -4.1283865 -4.0812321 -4.0396409 -4.0279613][-4.153152 -4.1729679 -4.1879582 -4.1789823 -4.1624389 -4.1488361 -4.1244669 -4.1058311 -4.1240835 -4.1612687 -4.1805882 -4.1653657 -4.1308355 -4.104125 -4.09731][-4.2097182 -4.2270026 -4.2416167 -4.2372804 -4.2258191 -4.2187524 -4.202899 -4.1929188 -4.20714 -4.2294431 -4.2373929 -4.2226796 -4.1960464 -4.1776905 -4.1732855][-4.2665811 -4.2786255 -4.2890911 -4.28481 -4.2777681 -4.2735629 -4.266335 -4.2633958 -4.2743855 -4.2863555 -4.2903 -4.2790132 -4.2582564 -4.2457523 -4.2406106][-4.3065567 -4.3158526 -4.32205 -4.3177023 -4.3127251 -4.308682 -4.3054867 -4.3067846 -4.3155046 -4.322937 -4.3253984 -4.3164477 -4.3009591 -4.29199 -4.2862892][-4.3254061 -4.3311186 -4.3340425 -4.332128 -4.3301196 -4.3273335 -4.3235865 -4.3241467 -4.328361 -4.3318686 -4.3332639 -4.328589 -4.31784 -4.3091311 -4.303937]]...]
INFO - root - 2017-12-06 00:51:15.043966: step 58110, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 72h:27m:17s remains)
INFO - root - 2017-12-06 00:51:24.230252: step 58120, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 70h:54m:45s remains)
INFO - root - 2017-12-06 00:51:33.762994: step 58130, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 75h:57m:25s remains)
INFO - root - 2017-12-06 00:51:43.061498: step 58140, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 64h:48m:28s remains)
INFO - root - 2017-12-06 00:51:52.047153: step 58150, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 74h:00m:59s remains)
INFO - root - 2017-12-06 00:52:01.598213: step 58160, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 75h:57m:51s remains)
INFO - root - 2017-12-06 00:52:10.966686: step 58170, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 65h:41m:01s remains)
INFO - root - 2017-12-06 00:52:20.268544: step 58180, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 71h:53m:10s remains)
INFO - root - 2017-12-06 00:52:29.729740: step 58190, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 65h:56m:54s remains)
INFO - root - 2017-12-06 00:52:39.083930: step 58200, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 75h:12m:32s remains)
2017-12-06 00:52:39.774640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2544956 -4.2618761 -4.2672653 -4.2673545 -4.26055 -4.2479887 -4.2367816 -4.2305722 -4.2240124 -4.2168894 -4.210494 -4.2117095 -4.2189069 -4.2297244 -4.2465029][-4.2331071 -4.2380891 -4.2444906 -4.2459397 -4.238349 -4.2253566 -4.2167277 -4.2164278 -4.2156515 -4.2135367 -4.2106767 -4.2176757 -4.2325788 -4.2485375 -4.2653966][-4.2096596 -4.2144446 -4.2190723 -4.2207637 -4.2162561 -4.2096233 -4.2098813 -4.2179437 -4.2247748 -4.2292628 -4.2314687 -4.2419863 -4.2591515 -4.2735453 -4.2873087][-4.1855741 -4.1889224 -4.1861768 -4.1846094 -4.1857424 -4.1899257 -4.20401 -4.2226 -4.2381 -4.2489557 -4.2562037 -4.2697387 -4.2853837 -4.2943997 -4.3018522][-4.1568131 -4.1556478 -4.1468849 -4.1403861 -4.1428661 -4.1563516 -4.1820064 -4.2093525 -4.2331367 -4.2504869 -4.2640791 -4.2797642 -4.2917714 -4.2969575 -4.3020754][-4.122396 -4.1162224 -4.1059346 -4.0957966 -4.0950456 -4.1064339 -4.1265345 -4.1506085 -4.1790314 -4.2057333 -4.231638 -4.256259 -4.2719688 -4.2790036 -4.2868142][-4.1002197 -4.0888143 -4.0778522 -4.06734 -4.0621567 -4.0596213 -4.056354 -4.0627213 -4.0864816 -4.1235557 -4.1677904 -4.20815 -4.2343192 -4.2489834 -4.262949][-4.110024 -4.0932078 -4.080349 -4.0674896 -4.0563011 -4.039422 -4.0090246 -3.9891622 -4.0020633 -4.0459538 -4.1079812 -4.1609931 -4.1930242 -4.2135715 -4.2337956][-4.1541581 -4.1368771 -4.1244016 -4.1099281 -4.0937934 -4.0715842 -4.0325828 -3.9985456 -3.9955575 -4.0310392 -4.0924239 -4.142704 -4.1696939 -4.18736 -4.2080722][-4.2009664 -4.1874657 -4.1738629 -4.1575561 -4.1413856 -4.1229725 -4.0939584 -4.0678806 -4.06096 -4.0803103 -4.1207108 -4.1529531 -4.1670437 -4.1779809 -4.195807][-4.244822 -4.2383857 -4.2242041 -4.205864 -4.18994 -4.1754303 -4.1557751 -4.1404276 -4.1367292 -4.1482739 -4.170855 -4.1878123 -4.1931014 -4.1979051 -4.2112427][-4.2664084 -4.2695842 -4.260025 -4.2434759 -4.2295394 -4.2181163 -4.2040038 -4.193728 -4.1928568 -4.2025738 -4.21846 -4.2301168 -4.2353897 -4.240726 -4.2510996][-4.2685409 -4.2761621 -4.2740111 -4.2660656 -4.258832 -4.2530384 -4.2450156 -4.2388821 -4.2397366 -4.2491493 -4.2624536 -4.2732105 -4.281126 -4.2874556 -4.29318][-4.274682 -4.279501 -4.2809086 -4.2816248 -4.282639 -4.2834268 -4.2821646 -4.2815561 -4.2852597 -4.2944946 -4.3052344 -4.3144369 -4.3226018 -4.3259006 -4.3246918][-4.2829404 -4.286592 -4.2902436 -4.2951589 -4.2998753 -4.3030915 -4.30452 -4.3068566 -4.3123832 -4.3219237 -4.3328433 -4.3424225 -4.350019 -4.3508868 -4.3470187]]...]
INFO - root - 2017-12-06 00:52:49.002444: step 58210, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 65h:20m:21s remains)
INFO - root - 2017-12-06 00:52:58.405516: step 58220, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 70h:06m:14s remains)
INFO - root - 2017-12-06 00:53:07.833902: step 58230, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 68h:17m:20s remains)
INFO - root - 2017-12-06 00:53:17.137472: step 58240, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 74h:58m:49s remains)
INFO - root - 2017-12-06 00:53:26.415719: step 58250, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 71h:39m:05s remains)
INFO - root - 2017-12-06 00:53:35.864136: step 58260, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 71h:40m:21s remains)
INFO - root - 2017-12-06 00:53:45.298973: step 58270, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 73h:02m:19s remains)
INFO - root - 2017-12-06 00:53:54.379022: step 58280, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 70h:38m:48s remains)
INFO - root - 2017-12-06 00:54:03.783144: step 58290, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 72h:03m:58s remains)
INFO - root - 2017-12-06 00:54:13.016163: step 58300, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 68h:04m:38s remains)
2017-12-06 00:54:13.850699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2464094 -4.2447944 -4.2434421 -4.2319031 -4.2255163 -4.2165952 -4.2138987 -4.2246523 -4.2233725 -4.211771 -4.1968718 -4.1872458 -4.1857352 -4.1989317 -4.2203422][-4.2458119 -4.2442737 -4.2433991 -4.23708 -4.2342305 -4.2222905 -4.2173343 -4.2287779 -4.2245846 -4.2086682 -4.1916242 -4.1823521 -4.1811538 -4.1964684 -4.21568][-4.2384777 -4.2364154 -4.2375536 -4.2345791 -4.2328486 -4.2182255 -4.2095213 -4.2190657 -4.2149386 -4.2008252 -4.1874995 -4.1828589 -4.1799765 -4.1932664 -4.2083511][-4.2289 -4.2280788 -4.2290859 -4.2275677 -4.2246008 -4.2060437 -4.193953 -4.2014184 -4.2003751 -4.1900816 -4.181612 -4.1828184 -4.1783457 -4.1877909 -4.2002053][-4.2226052 -4.2259693 -4.2257495 -4.2195754 -4.2128291 -4.1913481 -4.1745481 -4.1750889 -4.1742811 -4.1654863 -4.1616859 -4.1690445 -4.1680837 -4.1789055 -4.19304][-4.2151151 -4.2221975 -4.2203565 -4.2036061 -4.1839404 -4.1520061 -4.1261725 -4.1135039 -4.1095281 -4.1032295 -4.1087136 -4.13059 -4.1425138 -4.16195 -4.1792884][-4.2070103 -4.2151484 -4.2066207 -4.1737967 -4.1335669 -4.0866189 -4.0473981 -4.0240064 -4.0173979 -4.0188789 -4.0424438 -4.0855145 -4.1121306 -4.1418018 -4.16332][-4.1957126 -4.1941233 -4.1758924 -4.1293311 -4.0692682 -4.0127068 -3.9707808 -3.9520192 -3.9494667 -3.9612679 -4.0004773 -4.0527906 -4.0871825 -4.1243095 -4.1500392][-4.1943331 -4.1796389 -4.1518507 -4.1024327 -4.0377784 -3.9834106 -3.9553957 -3.9528711 -3.9651456 -3.9808505 -4.0182953 -4.0633574 -4.0928564 -4.1269407 -4.1536489][-4.2066984 -4.1891489 -4.1628542 -4.1248131 -4.0726738 -4.0322952 -4.0152369 -4.01776 -4.0297251 -4.0380917 -4.0659161 -4.1031728 -4.1316662 -4.16046 -4.1826639][-4.2232461 -4.214488 -4.2006955 -4.17848 -4.1424289 -4.1115084 -4.0966578 -4.0968461 -4.0984893 -4.0955892 -4.1158161 -4.148562 -4.1784868 -4.2041368 -4.2231364][-4.2352896 -4.2373409 -4.2377629 -4.2289209 -4.208075 -4.1837716 -4.1672349 -4.1617765 -4.1532869 -4.1429515 -4.1590881 -4.1884623 -4.2175317 -4.2407746 -4.2564549][-4.2442336 -4.2505317 -4.2591839 -4.2604914 -4.2512283 -4.2344856 -4.2187576 -4.2112131 -4.1986089 -4.1870613 -4.1986966 -4.2219729 -4.2458472 -4.2635837 -4.2752161][-4.2586412 -4.2612033 -4.2677617 -4.272706 -4.2731795 -4.2695189 -4.2626963 -4.2583709 -4.2475076 -4.2371755 -4.2420745 -4.2554331 -4.2698393 -4.280818 -4.289505][-4.2830448 -4.2822556 -4.2847371 -4.2878709 -4.2907448 -4.2934761 -4.2932377 -4.2930923 -4.2875581 -4.2814651 -4.2822561 -4.2874331 -4.2937655 -4.2986732 -4.3036394]]...]
INFO - root - 2017-12-06 00:54:23.205634: step 58310, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 72h:08m:14s remains)
INFO - root - 2017-12-06 00:54:32.568714: step 58320, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 73h:41m:34s remains)
INFO - root - 2017-12-06 00:54:41.887805: step 58330, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 72h:07m:16s remains)
INFO - root - 2017-12-06 00:54:51.407675: step 58340, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 74h:14m:41s remains)
INFO - root - 2017-12-06 00:55:00.668555: step 58350, loss = 2.04, batch loss = 1.98 (8.0 examples/sec; 1.001 sec/batch; 76h:14m:38s remains)
INFO - root - 2017-12-06 00:55:10.128225: step 58360, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 71h:03m:41s remains)
INFO - root - 2017-12-06 00:55:19.264208: step 58370, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.784 sec/batch; 59h:40m:24s remains)
INFO - root - 2017-12-06 00:55:28.342652: step 58380, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 71h:01m:36s remains)
INFO - root - 2017-12-06 00:55:37.386854: step 58390, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 73h:52m:17s remains)
INFO - root - 2017-12-06 00:55:46.612163: step 58400, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 73h:22m:14s remains)
2017-12-06 00:55:47.447212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3083525 -4.3135905 -4.3182206 -4.3177938 -4.312068 -4.3038273 -4.2966971 -4.2921739 -4.2874274 -4.2802825 -4.27275 -4.2632952 -4.2539368 -4.2462893 -4.2405405][-4.3105507 -4.3149152 -4.3173819 -4.3118372 -4.2992897 -4.2846141 -4.2734313 -4.2667 -4.2601633 -4.2513537 -4.242372 -4.2327876 -4.2239084 -4.2174678 -4.2130008][-4.3103762 -4.3134685 -4.3114767 -4.2984405 -4.2768445 -4.2552238 -4.240056 -4.230617 -4.2231951 -4.2139654 -4.2033343 -4.1933074 -4.1874919 -4.1854982 -4.1860447][-4.3065343 -4.3033338 -4.2909818 -4.2654881 -4.233355 -4.206749 -4.1897717 -4.1808538 -4.1755018 -4.1697817 -4.1622839 -4.1566329 -4.1576548 -4.1652665 -4.1763649][-4.2965846 -4.2820597 -4.25418 -4.2146931 -4.1759872 -4.15117 -4.1390796 -4.1374416 -4.1409483 -4.1412148 -4.1386766 -4.1423235 -4.1539683 -4.1736054 -4.195797][-4.2833285 -4.2549887 -4.2138548 -4.1681209 -4.1319938 -4.1164179 -4.1155539 -4.12393 -4.133152 -4.1347504 -4.1359921 -4.1498446 -4.1738052 -4.2043929 -4.2332964][-4.2687993 -4.2303052 -4.184381 -4.1415854 -4.1142268 -4.1091385 -4.1142597 -4.1244245 -4.1282706 -4.1235652 -4.1282821 -4.152616 -4.1881933 -4.2281771 -4.262466][-4.2414432 -4.1993279 -4.1575618 -4.1270251 -4.1155281 -4.1208053 -4.1284742 -4.1327176 -4.1264396 -4.1156807 -4.1239309 -4.1565609 -4.2019606 -4.2464089 -4.2784615][-4.2075253 -4.1672373 -4.1360331 -4.1231031 -4.1304445 -4.1469607 -4.1576343 -4.1579361 -4.1465521 -4.134398 -4.1437235 -4.1790919 -4.225214 -4.2656493 -4.2899041][-4.1807723 -4.1474643 -4.1307745 -4.1327672 -4.1505494 -4.1753364 -4.1908731 -4.1911154 -4.1836743 -4.1779351 -4.1893835 -4.2232451 -4.2644339 -4.2963643 -4.3072867][-4.1726303 -4.1498156 -4.1453571 -4.156311 -4.178071 -4.206058 -4.2234707 -4.2240543 -4.2219276 -4.2241855 -4.2414751 -4.2734747 -4.3075552 -4.3268633 -4.32221][-4.1863074 -4.1735392 -4.175765 -4.1872549 -4.2081246 -4.2342033 -4.2487116 -4.2497368 -4.2522273 -4.2580166 -4.2764387 -4.3046122 -4.328866 -4.333519 -4.3171272][-4.2231317 -4.2162886 -4.2185912 -4.2273722 -4.2447743 -4.2640004 -4.2742152 -4.2735476 -4.2721586 -4.2746758 -4.2915339 -4.3148675 -4.3276052 -4.3198752 -4.2980294][-4.2698507 -4.2610307 -4.2578983 -4.262073 -4.2742724 -4.286171 -4.2894826 -4.2829671 -4.2746468 -4.2764435 -4.2937336 -4.31081 -4.3141952 -4.3009133 -4.28144][-4.3013477 -4.2871981 -4.2768693 -4.2763634 -4.2825947 -4.2880597 -4.28543 -4.2751808 -4.2675214 -4.2749209 -4.2946024 -4.307641 -4.307766 -4.2962604 -4.28126]]...]
INFO - root - 2017-12-06 00:55:56.783918: step 58410, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 70h:33m:59s remains)
INFO - root - 2017-12-06 00:56:06.073832: step 58420, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 71h:17m:29s remains)
INFO - root - 2017-12-06 00:56:15.421399: step 58430, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.904 sec/batch; 68h:47m:26s remains)
INFO - root - 2017-12-06 00:56:24.929489: step 58440, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 73h:58m:29s remains)
INFO - root - 2017-12-06 00:56:34.148642: step 58450, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 73h:06m:31s remains)
INFO - root - 2017-12-06 00:56:43.497386: step 58460, loss = 2.09, batch loss = 2.03 (7.6 examples/sec; 1.050 sec/batch; 79h:54m:11s remains)
INFO - root - 2017-12-06 00:56:53.034177: step 58470, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 71h:50m:29s remains)
INFO - root - 2017-12-06 00:57:02.413116: step 58480, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 71h:13m:56s remains)
INFO - root - 2017-12-06 00:57:11.724309: step 58490, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 74h:46m:16s remains)
INFO - root - 2017-12-06 00:57:21.050223: step 58500, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 69h:54m:54s remains)
2017-12-06 00:57:21.767182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3497672 -4.3560028 -4.3562136 -4.3539448 -4.34831 -4.3393412 -4.3302016 -4.3229952 -4.3172393 -4.3140879 -4.3178535 -4.3248763 -4.3324637 -4.3383851 -4.3430028][-4.3504477 -4.3573184 -4.3574176 -4.3524089 -4.3419986 -4.3300357 -4.3197737 -4.3118596 -4.305995 -4.3017473 -4.3062057 -4.3138094 -4.3216481 -4.3285308 -4.3355579][-4.3493814 -4.3560309 -4.353847 -4.3407588 -4.3203487 -4.3017516 -4.291502 -4.2855682 -4.2846088 -4.2861609 -4.2949796 -4.302196 -4.3111982 -4.3192863 -4.3262219][-4.34299 -4.3462853 -4.3367529 -4.3102818 -4.274725 -4.2471442 -4.2373219 -4.2357359 -4.2425871 -4.2559595 -4.275497 -4.2868319 -4.298564 -4.3087487 -4.3159909][-4.3299136 -4.323144 -4.2997789 -4.2560391 -4.2008138 -4.1661425 -4.1645021 -4.1745176 -4.1947246 -4.2225142 -4.2555037 -4.275773 -4.290062 -4.29977 -4.3053637][-4.315886 -4.292819 -4.2511616 -4.184598 -4.1104751 -4.0734921 -4.0825324 -4.1090841 -4.1453009 -4.18757 -4.2331595 -4.2628984 -4.2815671 -4.2912774 -4.2938056][-4.3010511 -4.2574944 -4.1924586 -4.0954833 -4.0003309 -3.9647112 -3.9876759 -4.0305543 -4.0821648 -4.1344981 -4.1927381 -4.2388978 -4.2714939 -4.2869458 -4.2874732][-4.2893372 -4.2253022 -4.1327605 -4.0051556 -3.891881 -3.866153 -3.9129109 -3.9770257 -4.0427747 -4.1066957 -4.1741486 -4.2340727 -4.2781281 -4.2951965 -4.291903][-4.2856841 -4.2084908 -4.100791 -3.9676871 -3.8559258 -3.8364606 -3.8988729 -3.9819388 -4.0661888 -4.1409822 -4.2093763 -4.2693567 -4.3086343 -4.3207278 -4.3125634][-4.2791357 -4.1972551 -4.0983739 -3.9994659 -3.9258525 -3.917335 -3.9710069 -4.0491109 -4.1346564 -4.2070022 -4.2655334 -4.3132067 -4.3414187 -4.3471909 -4.3339748][-4.260757 -4.1805553 -4.1029725 -4.047739 -4.0153656 -4.0255284 -4.0748172 -4.1385589 -4.208961 -4.266469 -4.3082104 -4.3374038 -4.3541584 -4.3543062 -4.33837][-4.2279339 -4.1518965 -4.0948882 -4.0719919 -4.0720735 -4.1024432 -4.1571279 -4.2128468 -4.2664614 -4.3035879 -4.32491 -4.3382173 -4.3416209 -4.3353372 -4.3193092][-4.195313 -4.1215792 -4.0775857 -4.0742807 -4.095015 -4.1415591 -4.2010503 -4.2527218 -4.2986212 -4.3231525 -4.3275032 -4.3231983 -4.3126912 -4.2994184 -4.2831974][-4.1821079 -4.1196132 -4.08846 -4.0958338 -4.1268225 -4.1773596 -4.2324142 -4.2787418 -4.3148379 -4.3309312 -4.3226366 -4.3034172 -4.2821369 -4.2641697 -4.2485294][-4.1949811 -4.1525989 -4.1338754 -4.1448908 -4.1731639 -4.2142062 -4.2547603 -4.2896357 -4.316308 -4.3256774 -4.311728 -4.2867727 -4.2628517 -4.2447209 -4.2322659]]...]
INFO - root - 2017-12-06 00:57:31.250922: step 58510, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.968 sec/batch; 73h:42m:13s remains)
INFO - root - 2017-12-06 00:57:40.400586: step 58520, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 70h:27m:56s remains)
INFO - root - 2017-12-06 00:57:49.750054: step 58530, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 74h:11m:34s remains)
INFO - root - 2017-12-06 00:57:59.329879: step 58540, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 73h:44m:49s remains)
INFO - root - 2017-12-06 00:58:08.766472: step 58550, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 72h:43m:48s remains)
INFO - root - 2017-12-06 00:58:18.134323: step 58560, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 74h:15m:24s remains)
INFO - root - 2017-12-06 00:58:27.282895: step 58570, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.884 sec/batch; 67h:16m:08s remains)
INFO - root - 2017-12-06 00:58:36.729737: step 58580, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 74h:19m:04s remains)
INFO - root - 2017-12-06 00:58:46.250248: step 58590, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 73h:36m:37s remains)
INFO - root - 2017-12-06 00:58:55.554955: step 58600, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 71h:42m:28s remains)
2017-12-06 00:58:56.412584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2016454 -4.2134924 -4.2296405 -4.2314734 -4.2143817 -4.209384 -4.20422 -4.1874418 -4.1705341 -4.1709714 -4.1837974 -4.1884723 -4.1897378 -4.2000704 -4.2309957][-4.1774406 -4.1882057 -4.2073469 -4.2113175 -4.1917968 -4.1914868 -4.1884437 -4.1712494 -4.1532445 -4.1520658 -4.1682196 -4.1742887 -4.1746187 -4.1820946 -4.2134056][-4.1809168 -4.1957126 -4.2171345 -4.2201338 -4.2018652 -4.2063284 -4.2062345 -4.1914997 -4.174819 -4.1705222 -4.1811919 -4.1843939 -4.1814609 -4.1821737 -4.2097549][-4.1844215 -4.203918 -4.2280369 -4.226738 -4.2119331 -4.219203 -4.2174387 -4.1946864 -4.1730256 -4.1694059 -4.1803756 -4.1801844 -4.1757317 -4.175343 -4.20234][-4.1948724 -4.2111764 -4.2309866 -4.2254572 -4.2091374 -4.2108545 -4.1983995 -4.1594396 -4.1284413 -4.1318684 -4.1566563 -4.1602235 -4.1578336 -4.1611924 -4.1934161][-4.21016 -4.2168708 -4.2258825 -4.2141833 -4.191009 -4.179728 -4.1453023 -4.0802817 -4.0335703 -4.0584669 -4.1109171 -4.1333685 -4.1434183 -4.1554718 -4.1924729][-4.2307076 -4.2271576 -4.2226157 -4.2036409 -4.1734114 -4.14418 -4.0811114 -3.9846346 -3.9156766 -3.9690793 -4.0578232 -4.1048279 -4.1335492 -4.1594682 -4.2027636][-4.243783 -4.2340341 -4.2201328 -4.1975484 -4.1606321 -4.1136322 -4.0258374 -3.9029598 -3.8268232 -3.912219 -4.0261903 -4.090642 -4.1340742 -4.16981 -4.2146087][-4.2335911 -4.2203884 -4.2050028 -4.1845016 -4.1495848 -4.0992355 -4.0096693 -3.9009564 -3.8490663 -3.9388361 -4.0381446 -4.0952139 -4.1378627 -4.1721458 -4.2158256][-4.2147946 -4.196445 -4.1800404 -4.1649513 -4.1408362 -4.1032419 -4.0367455 -3.9633389 -3.9346447 -4.0001092 -4.0671477 -4.10625 -4.1395211 -4.1690383 -4.2102222][-4.2288275 -4.2090344 -4.1923594 -4.1760836 -4.1550169 -4.1243086 -4.0764918 -4.0311294 -4.0158105 -4.0583839 -4.0999546 -4.1288409 -4.1565566 -4.1829624 -4.2187543][-4.2527771 -4.2360988 -4.2197628 -4.1993933 -4.1713176 -4.1385369 -4.0980868 -4.0658221 -4.0575275 -4.0885739 -4.1210709 -4.1494656 -4.1779361 -4.2038331 -4.2342176][-4.2754917 -4.263041 -4.2459431 -4.2197032 -4.1818056 -4.1477723 -4.1160555 -4.0955725 -4.0952249 -4.1234555 -4.1543932 -4.1842747 -4.2120414 -4.233458 -4.2559295][-4.2987738 -4.2897177 -4.2733197 -4.2466927 -4.2094927 -4.1781826 -4.1573582 -4.15007 -4.1595 -4.1863184 -4.2094908 -4.2319245 -4.2517319 -4.2669144 -4.2808776][-4.3148932 -4.309094 -4.2966118 -4.2737732 -4.247643 -4.227632 -4.2133803 -4.2130318 -4.2278333 -4.2486815 -4.2627068 -4.2756195 -4.2840862 -4.2944789 -4.3016233]]...]
INFO - root - 2017-12-06 00:59:05.916457: step 58610, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 70h:47m:39s remains)
INFO - root - 2017-12-06 00:59:15.363153: step 58620, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 71h:54m:19s remains)
INFO - root - 2017-12-06 00:59:24.799141: step 58630, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.903 sec/batch; 68h:43m:28s remains)
INFO - root - 2017-12-06 00:59:34.147323: step 58640, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.919 sec/batch; 69h:54m:36s remains)
INFO - root - 2017-12-06 00:59:43.410661: step 58650, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.928 sec/batch; 70h:33m:41s remains)
INFO - root - 2017-12-06 00:59:52.523221: step 58660, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 71h:54m:02s remains)
INFO - root - 2017-12-06 01:00:01.700460: step 58670, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 70h:58m:31s remains)
INFO - root - 2017-12-06 01:00:11.058055: step 58680, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 74h:00m:03s remains)
INFO - root - 2017-12-06 01:00:20.642478: step 58690, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 72h:05m:16s remains)
INFO - root - 2017-12-06 01:00:29.898441: step 58700, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 68h:30m:23s remains)
2017-12-06 01:00:30.637882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21066 -4.1908641 -4.1736617 -4.16172 -4.1522551 -4.157176 -4.176188 -4.1823859 -4.1726837 -4.1544309 -4.1402955 -4.141583 -4.148715 -4.1562004 -4.1610661][-4.199192 -4.1778011 -4.1618252 -4.1536732 -4.14743 -4.1541 -4.167964 -4.1714449 -4.1638336 -4.1495156 -4.136838 -4.13731 -4.1414986 -4.1461048 -4.1498356][-4.1685157 -4.1468592 -4.13539 -4.1336513 -4.13134 -4.1397643 -4.1515546 -4.1545558 -4.153017 -4.1499805 -4.1445918 -4.1469069 -4.1499357 -4.1515985 -4.1542745][-4.1367326 -4.1137996 -4.106853 -4.1104264 -4.1127238 -4.1187468 -4.1243086 -4.1262674 -4.1354203 -4.15012 -4.1556315 -4.1590672 -4.1623459 -4.1626248 -4.1603718][-4.1188326 -4.0955944 -4.0904574 -4.0943708 -4.0929275 -4.0870171 -4.0771804 -4.073348 -4.0936484 -4.1243739 -4.1402326 -4.1503191 -4.1599088 -4.1629457 -4.1519809][-4.1196918 -4.0993671 -4.0934253 -4.0919614 -4.0799079 -4.0552397 -4.0219369 -4.0072103 -4.0415239 -4.0929279 -4.1208997 -4.1397748 -4.1548357 -4.1586466 -4.1344361][-4.1363082 -4.1219912 -4.1151834 -4.1007695 -4.0690284 -4.0166483 -3.9518514 -3.919 -3.9751568 -4.0560026 -4.1029973 -4.13576 -4.1587095 -4.1613803 -4.1266012][-4.1571417 -4.1495972 -4.1416755 -4.1124616 -4.0583339 -3.9757593 -3.8707273 -3.8072903 -3.888643 -4.0019341 -4.0713239 -4.1206751 -4.1531596 -4.1529431 -4.1144075][-4.1731687 -4.1702609 -4.1658211 -4.1349769 -4.0716395 -3.9812849 -3.8585072 -3.7771127 -3.8632016 -3.9794748 -4.0508275 -4.107585 -4.1425147 -4.1413269 -4.1081505][-4.1802349 -4.176775 -4.1772642 -4.1547766 -4.1019611 -4.0389919 -3.9550309 -3.9027383 -3.9548428 -4.0225577 -4.0584764 -4.0951343 -4.1178761 -4.1135449 -4.0921874][-4.1810436 -4.1783323 -4.178606 -4.162889 -4.124332 -4.0822968 -4.0322833 -4.0106511 -4.0472474 -4.0802779 -4.0892148 -4.1023064 -4.1057816 -4.0904679 -4.0768003][-4.1773558 -4.1746216 -4.1735544 -4.1666741 -4.1455865 -4.11809 -4.0896087 -4.0855536 -4.1152458 -4.1303616 -4.1285639 -4.1312361 -4.1240873 -4.10159 -4.0893536][-4.1671009 -4.1671691 -4.1690826 -4.1736479 -4.1658549 -4.1511512 -4.1369996 -4.1397934 -4.1616049 -4.1644263 -4.1596427 -4.1631885 -4.1573348 -4.13365 -4.119894][-4.1546803 -4.1562023 -4.1619492 -4.1763277 -4.1777124 -4.1729922 -4.1673856 -4.1673126 -4.1744952 -4.1699023 -4.1654282 -4.1723747 -4.1746197 -4.1605954 -4.1504993][-4.1556978 -4.1604757 -4.1687269 -4.1853304 -4.1890187 -4.186563 -4.1835847 -4.1820979 -4.1825323 -4.1768532 -4.1777215 -4.1882854 -4.1949162 -4.1873031 -4.1804991]]...]
INFO - root - 2017-12-06 01:00:39.967827: step 58710, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 74h:37m:17s remains)
INFO - root - 2017-12-06 01:00:49.381026: step 58720, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 73h:07m:55s remains)
INFO - root - 2017-12-06 01:00:58.736453: step 58730, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 69h:34m:55s remains)
INFO - root - 2017-12-06 01:01:08.070186: step 58740, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 71h:43m:52s remains)
INFO - root - 2017-12-06 01:01:17.556096: step 58750, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 69h:09m:19s remains)
INFO - root - 2017-12-06 01:01:27.068143: step 58760, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 66h:06m:52s remains)
INFO - root - 2017-12-06 01:01:36.575589: step 58770, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.938 sec/batch; 71h:17m:30s remains)
INFO - root - 2017-12-06 01:01:45.992802: step 58780, loss = 2.01, batch loss = 1.96 (8.6 examples/sec; 0.926 sec/batch; 70h:23m:16s remains)
INFO - root - 2017-12-06 01:01:55.341991: step 58790, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 70h:28m:45s remains)
INFO - root - 2017-12-06 01:02:04.783308: step 58800, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 69h:01m:03s remains)
2017-12-06 01:02:05.553467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2447696 -4.2396126 -4.2420864 -4.2466674 -4.2514439 -4.2515965 -4.2539062 -4.2581625 -4.2548509 -4.2441006 -4.2256727 -4.207581 -4.1925111 -4.187706 -4.1913309][-4.2405982 -4.2369051 -4.2389059 -4.2429543 -4.2471805 -4.24567 -4.2466006 -4.2495656 -4.242805 -4.2272778 -4.2046986 -4.1889229 -4.1767306 -4.172668 -4.1781507][-4.2505755 -4.2477207 -4.2470031 -4.2469697 -4.2464156 -4.239603 -4.236434 -4.2362986 -4.2294269 -4.2145991 -4.1914058 -4.1816282 -4.1766839 -4.1763783 -4.1861892][-4.2527432 -4.2459507 -4.2402978 -4.23629 -4.2294507 -4.2159023 -4.207047 -4.2075977 -4.2062225 -4.1956668 -4.1749787 -4.1697769 -4.1730361 -4.1806068 -4.1977096][-4.2210417 -4.2064376 -4.1954846 -4.1887717 -4.1795197 -4.1607428 -4.1435828 -4.1446333 -4.1544614 -4.1577368 -4.1481681 -4.1469326 -4.1528392 -4.1677413 -4.1932139][-4.1841693 -4.1634331 -4.1458597 -4.13248 -4.1177273 -4.084578 -4.04179 -4.0313363 -4.0625405 -4.0987864 -4.1082649 -4.1087079 -4.1101856 -4.1294632 -4.1628408][-4.1527238 -4.1375766 -4.1204767 -4.1013846 -4.0742655 -4.0156574 -3.9337349 -3.900852 -3.961942 -4.0462227 -4.0835819 -4.088335 -4.0860944 -4.1059008 -4.1430144][-4.0960865 -4.0876746 -4.0669208 -4.0372767 -3.9969387 -3.9091754 -3.7812557 -3.720916 -3.8204741 -3.9552317 -4.0225024 -4.0373273 -4.0435767 -4.0745463 -4.1210008][-4.0523052 -4.049171 -4.0294471 -4.0002007 -3.9590428 -3.8674297 -3.7308068 -3.6561399 -3.7661853 -3.9139576 -3.9862034 -4.0037918 -4.0174408 -4.0582428 -4.1113768][-4.0612345 -4.0660152 -4.0553207 -4.0372691 -4.0137877 -3.9547136 -3.8674681 -3.8143604 -3.8807955 -3.9837334 -4.0398645 -4.0530028 -4.062377 -4.0950561 -4.137289][-4.099103 -4.1058283 -4.1011338 -4.09497 -4.0892963 -4.0633631 -4.0187197 -3.980859 -4.0075054 -4.0665603 -4.107028 -4.1225004 -4.1279511 -4.1428037 -4.1649375][-4.1397915 -4.1453648 -4.1454468 -4.14874 -4.1566639 -4.151567 -4.1315308 -4.0999932 -4.0991707 -4.1248827 -4.1493316 -4.1673884 -4.1690159 -4.1695595 -4.1768255][-4.1650634 -4.1682954 -4.1763725 -4.1905861 -4.2059412 -4.2107463 -4.2011185 -4.1743546 -4.1586471 -4.16101 -4.1706624 -4.1851072 -4.1844649 -4.1804862 -4.1855717][-4.1970391 -4.1964087 -4.2068086 -4.2244453 -4.2411671 -4.2488933 -4.2442813 -4.2250829 -4.2071919 -4.1975908 -4.1958051 -4.2046146 -4.2062707 -4.206233 -4.2145395][-4.2452269 -4.2410507 -4.2448616 -4.2559872 -4.2679 -4.2741747 -4.2732258 -4.2640586 -4.2516069 -4.239635 -4.2320614 -4.2357159 -4.2406783 -4.2484474 -4.2625694]]...]
INFO - root - 2017-12-06 01:02:14.747082: step 58810, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 70h:41m:53s remains)
INFO - root - 2017-12-06 01:02:24.289926: step 58820, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 70h:59m:46s remains)
INFO - root - 2017-12-06 01:02:33.750576: step 58830, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 70h:44m:09s remains)
INFO - root - 2017-12-06 01:02:43.085515: step 58840, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 72h:44m:07s remains)
INFO - root - 2017-12-06 01:02:52.620773: step 58850, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 70h:48m:33s remains)
INFO - root - 2017-12-06 01:03:02.032426: step 58860, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 72h:10m:34s remains)
INFO - root - 2017-12-06 01:03:10.955757: step 58870, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 64h:43m:48s remains)
INFO - root - 2017-12-06 01:03:20.442508: step 58880, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 73h:28m:34s remains)
INFO - root - 2017-12-06 01:03:29.796407: step 58890, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 70h:56m:42s remains)
INFO - root - 2017-12-06 01:03:39.081811: step 58900, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 71h:20m:05s remains)
2017-12-06 01:03:39.909093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3453345 -4.3416452 -4.3380933 -4.33721 -4.3353786 -4.3293562 -4.3172994 -4.300456 -4.283668 -4.278162 -4.2805848 -4.2907791 -4.3032188 -4.3164587 -4.3272614][-4.347055 -4.3447223 -4.341424 -4.3381782 -4.3285875 -4.3080993 -4.2776632 -4.2443652 -4.218 -4.2110772 -4.2172236 -4.2345243 -4.2581615 -4.281477 -4.3026681][-4.3486896 -4.3450117 -4.338964 -4.3287177 -4.3031688 -4.2585874 -4.202404 -4.1483479 -4.1125855 -4.1085777 -4.1263928 -4.1569567 -4.1941547 -4.2280059 -4.26076][-4.34975 -4.342639 -4.3297954 -4.307344 -4.2600965 -4.1873813 -4.101984 -4.0258412 -3.9840112 -3.9966593 -4.0411191 -4.0911694 -4.1377182 -4.174367 -4.212254][-4.3480849 -4.3356872 -4.3118687 -4.272191 -4.2008481 -4.0985737 -3.9824545 -3.8935008 -3.865834 -3.9145114 -3.9898391 -4.0550785 -4.1021819 -4.1367259 -4.1763272][-4.3443604 -4.3246031 -4.2865729 -4.2285624 -4.1328673 -4.0042162 -3.8648472 -3.7820449 -3.7911906 -3.8882637 -3.9906013 -4.0566921 -4.0909791 -4.1183848 -4.160368][-4.3433285 -4.3192 -4.2705379 -4.1961379 -4.080183 -3.9379683 -3.7950208 -3.7316446 -3.7802224 -3.9114389 -4.0249853 -4.0834913 -4.1050096 -4.1264734 -4.1709738][-4.3465776 -4.3203216 -4.2643204 -4.1811337 -4.0602875 -3.9245195 -3.7979572 -3.755877 -3.8236814 -3.9609475 -4.0747619 -4.127131 -4.1394944 -4.1551971 -4.1974339][-4.3486376 -4.3207378 -4.2651911 -4.1858373 -4.07256 -3.9535861 -3.844451 -3.8159673 -3.8884649 -4.0171924 -4.122016 -4.1671414 -4.1734943 -4.1863623 -4.2220445][-4.3454275 -4.3167953 -4.267909 -4.1987376 -4.096148 -3.9895804 -3.8899157 -3.8672214 -3.94285 -4.0630593 -4.1536851 -4.1888604 -4.1917939 -4.2035923 -4.231586][-4.3369932 -4.3096619 -4.2711372 -4.2170258 -4.1287718 -4.0347452 -3.9460645 -3.9293704 -4.0046434 -4.1094537 -4.1798625 -4.2041526 -4.20168 -4.2081113 -4.2264705][-4.3291864 -4.3025122 -4.2726526 -4.2303033 -4.1580863 -4.0805507 -4.014667 -4.006917 -4.0740218 -4.157402 -4.2096548 -4.2240162 -4.2170649 -4.2142806 -4.2220659][-4.3238511 -4.2981706 -4.2748046 -4.2402234 -4.185226 -4.12925 -4.0921078 -4.0955763 -4.1501336 -4.2139578 -4.2493134 -4.2520685 -4.2406025 -4.2300782 -4.2311835][-4.32561 -4.3032765 -4.2842493 -4.2560773 -4.2170906 -4.1817207 -4.1683226 -4.1837616 -4.2271795 -4.2708344 -4.2925444 -4.2869763 -4.2717767 -4.25893 -4.2594376][-4.3330312 -4.3128958 -4.2959208 -4.2743344 -4.2485614 -4.2313929 -4.2355304 -4.2575183 -4.2883825 -4.3134151 -4.321454 -4.3098564 -4.2943091 -4.2860413 -4.2914968]]...]
INFO - root - 2017-12-06 01:03:49.468317: step 58910, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 74h:55m:35s remains)
INFO - root - 2017-12-06 01:03:58.839273: step 58920, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 73h:59m:18s remains)
INFO - root - 2017-12-06 01:04:08.122044: step 58930, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 74h:19m:44s remains)
INFO - root - 2017-12-06 01:04:17.275453: step 58940, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 68h:51m:45s remains)
INFO - root - 2017-12-06 01:04:26.988312: step 58950, loss = 2.03, batch loss = 1.97 (8.1 examples/sec; 0.985 sec/batch; 74h:48m:57s remains)
INFO - root - 2017-12-06 01:04:36.275378: step 58960, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.936 sec/batch; 71h:05m:10s remains)
INFO - root - 2017-12-06 01:04:45.546737: step 58970, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 68h:44m:13s remains)
INFO - root - 2017-12-06 01:04:55.010648: step 58980, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 72h:57m:30s remains)
INFO - root - 2017-12-06 01:05:04.434329: step 58990, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.996 sec/batch; 75h:42m:08s remains)
INFO - root - 2017-12-06 01:05:13.920842: step 59000, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 69h:38m:48s remains)
2017-12-06 01:05:14.806646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2062111 -4.2187467 -4.2336087 -4.23992 -4.2348585 -4.2171049 -4.1942291 -4.182158 -4.1954751 -4.208488 -4.1986585 -4.1813054 -4.1727991 -4.1682224 -4.166461][-4.1955738 -4.211092 -4.2344174 -4.2432766 -4.2339997 -4.2129855 -4.1929121 -4.1860514 -4.1986265 -4.2129083 -4.204988 -4.1854386 -4.174273 -4.1706119 -4.1690974][-4.1776128 -4.1954517 -4.2213321 -4.2273588 -4.21094 -4.1902041 -4.1809554 -4.1834345 -4.1958833 -4.21293 -4.2083116 -4.186316 -4.1722822 -4.1740413 -4.1794662][-4.1496124 -4.1690803 -4.1911235 -4.1889381 -4.1675696 -4.1515913 -4.1559339 -4.1693497 -4.1861243 -4.2038207 -4.1998572 -4.1752977 -4.1576605 -4.1624207 -4.1755533][-4.1176643 -4.1435013 -4.1624823 -4.1518445 -4.1266861 -4.1128445 -4.1237822 -4.1434278 -4.1663408 -4.185204 -4.1782718 -4.149343 -4.126205 -4.1294522 -4.1453257][-4.0920191 -4.122551 -4.1402416 -4.1254497 -4.0914903 -4.0709939 -4.0819459 -4.1066008 -4.1380558 -4.1635838 -4.1607428 -4.1326642 -4.1019015 -4.0952177 -4.1100121][-4.0838828 -4.116672 -4.1314368 -4.1049123 -4.0483909 -4.0094151 -4.0119367 -4.0399809 -4.0884933 -4.133461 -4.1489844 -4.1273856 -4.0891223 -4.0683312 -4.0746546][-4.095602 -4.1222992 -4.1285539 -4.0858479 -4.0020914 -3.9349372 -3.9181256 -3.9514894 -4.0275512 -4.0998435 -4.1364574 -4.1206164 -4.0720129 -4.0340304 -4.032989][-4.1279006 -4.1413894 -4.1330838 -4.0829525 -3.9903183 -3.9045386 -3.8706222 -3.9060659 -4.0009327 -4.0874543 -4.129869 -4.110631 -4.0503311 -3.9973519 -3.9887624][-4.1765327 -4.1794958 -4.1601033 -4.11331 -4.0365853 -3.9590228 -3.9277606 -3.9602213 -4.0391431 -4.1103482 -4.138083 -4.1078358 -4.0389791 -3.9799156 -3.9697526][-4.2204676 -4.2217569 -4.20186 -4.1623526 -4.1042194 -4.04584 -4.0263324 -4.0506768 -4.102406 -4.1500049 -4.1615143 -4.1235356 -4.0526958 -3.9986901 -3.9979296][-4.2448454 -4.2453461 -4.2310371 -4.2010212 -4.1560769 -4.1158347 -4.1104569 -4.127665 -4.1555958 -4.1833506 -4.18311 -4.14577 -4.085032 -4.0464373 -4.0566063][-4.2529473 -4.2553043 -4.2477946 -4.2285089 -4.2012734 -4.1786823 -4.179184 -4.1879725 -4.1984787 -4.2098837 -4.2057319 -4.1745877 -4.1236629 -4.0953426 -4.105587][-4.2530265 -4.2562113 -4.2553644 -4.2463894 -4.2335939 -4.2234621 -4.2254844 -4.229033 -4.2308426 -4.233139 -4.226213 -4.1989617 -4.1572914 -4.1325479 -4.1357236][-4.2439775 -4.2466521 -4.2498794 -4.2509751 -4.2499189 -4.2477622 -4.2483168 -4.25076 -4.2498 -4.2457275 -4.2366428 -4.2148547 -4.1846852 -4.1643519 -4.1615543]]...]
INFO - root - 2017-12-06 01:05:24.288414: step 59010, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 75h:04m:14s remains)
INFO - root - 2017-12-06 01:05:33.709013: step 59020, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 76h:01m:40s remains)
INFO - root - 2017-12-06 01:05:42.682323: step 59030, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 65h:36m:34s remains)
INFO - root - 2017-12-06 01:05:52.088849: step 59040, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 70h:15m:34s remains)
INFO - root - 2017-12-06 01:06:01.786969: step 59050, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 76h:56m:26s remains)
INFO - root - 2017-12-06 01:06:10.854070: step 59060, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 70h:50m:52s remains)
INFO - root - 2017-12-06 01:06:20.278993: step 59070, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 73h:59m:41s remains)
INFO - root - 2017-12-06 01:06:29.983361: step 59080, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 73h:52m:33s remains)
INFO - root - 2017-12-06 01:06:39.272038: step 59090, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.003 sec/batch; 76h:10m:37s remains)
INFO - root - 2017-12-06 01:06:48.672266: step 59100, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 74h:24m:51s remains)
2017-12-06 01:06:49.540511: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3247495 -4.3264437 -4.3252745 -4.3213582 -4.3162823 -4.3127742 -4.3121419 -4.31343 -4.3161116 -4.3196211 -4.3231177 -4.3251867 -4.326345 -4.3265691 -4.3271661][-4.3306484 -4.331892 -4.3299317 -4.324276 -4.3165421 -4.30755 -4.300808 -4.298738 -4.3021865 -4.309257 -4.3184328 -4.3259573 -4.3305464 -4.3313379 -4.3305192][-4.3362193 -4.3359838 -4.33292 -4.3253331 -4.3109393 -4.2921605 -4.2756238 -4.2665596 -4.2694359 -4.2809024 -4.2981749 -4.3166389 -4.3300767 -4.3353734 -4.3345585][-4.3370504 -4.3354907 -4.3291636 -4.3146429 -4.2889371 -4.2572088 -4.2271786 -4.2075005 -4.2102141 -4.2317009 -4.2622957 -4.2938538 -4.3187571 -4.3319197 -4.3353295][-4.3253455 -4.3229451 -4.3111868 -4.2850657 -4.2419405 -4.1884875 -4.1310425 -4.0909228 -4.097702 -4.1449752 -4.20369 -4.2570062 -4.2965274 -4.320138 -4.3308315][-4.3035464 -4.302989 -4.2881265 -4.2528968 -4.1934032 -4.1090889 -4.0072217 -3.9311767 -3.9438634 -4.0289731 -4.1255393 -4.2041106 -4.262423 -4.2999835 -4.3204126][-4.2752819 -4.2816443 -4.27035 -4.2321191 -4.1642652 -4.0564966 -3.9102755 -3.7935841 -3.8197372 -3.9445469 -4.071496 -4.1680179 -4.2394681 -4.2850065 -4.3115287][-4.2475705 -4.2640433 -4.2636776 -4.2344704 -4.1763697 -4.0766764 -3.9355175 -3.8198798 -3.8465164 -3.9630377 -4.0825567 -4.1751981 -4.242878 -4.2845783 -4.3083744][-4.2334952 -4.2557034 -4.2675657 -4.2571754 -4.2222528 -4.15675 -4.06288 -3.9885228 -4.0001664 -4.0680628 -4.1485972 -4.2189264 -4.2692866 -4.2966447 -4.3094134][-4.243011 -4.2638927 -4.2831726 -4.2879519 -4.2751427 -4.2395668 -4.1848831 -4.1417761 -4.1424351 -4.1724949 -4.2173243 -4.2629337 -4.296092 -4.308991 -4.3099875][-4.2692595 -4.2844734 -4.3018022 -4.3114519 -4.3090696 -4.2900338 -4.2576656 -4.2279286 -4.2194142 -4.2293906 -4.2518706 -4.27872 -4.2983894 -4.3029275 -4.297709][-4.2874966 -4.300333 -4.314539 -4.3235855 -4.3237581 -4.3113565 -4.2888913 -4.2638803 -4.2481875 -4.2440062 -4.2515197 -4.265337 -4.2778907 -4.2807217 -4.276453][-4.2819128 -4.2961655 -4.3112583 -4.3193455 -4.3189554 -4.309093 -4.2917719 -4.2699065 -4.2501135 -4.2382908 -4.2374539 -4.244101 -4.2528753 -4.2572508 -4.25857][-4.2640128 -4.2815647 -4.2983952 -4.3042874 -4.3001552 -4.2902794 -4.2762561 -4.2563858 -4.236845 -4.2251558 -4.2240658 -4.2295294 -4.2370143 -4.2435465 -4.2495589][-4.2508349 -4.2716188 -4.2898765 -4.2942133 -4.2864456 -4.2750006 -4.2637796 -4.2504458 -4.2381167 -4.23083 -4.2284951 -4.2286606 -4.2318068 -4.2375407 -4.2460437]]...]
INFO - root - 2017-12-06 01:06:59.043967: step 59110, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 70h:25m:08s remains)
INFO - root - 2017-12-06 01:07:08.195955: step 59120, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.951 sec/batch; 72h:15m:20s remains)
INFO - root - 2017-12-06 01:07:17.542144: step 59130, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.978 sec/batch; 74h:17m:10s remains)
INFO - root - 2017-12-06 01:07:26.937037: step 59140, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 72h:05m:22s remains)
INFO - root - 2017-12-06 01:07:36.197991: step 59150, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 72h:55m:45s remains)
INFO - root - 2017-12-06 01:07:45.629068: step 59160, loss = 2.11, batch loss = 2.05 (7.9 examples/sec; 1.006 sec/batch; 76h:24m:33s remains)
INFO - root - 2017-12-06 01:07:55.094251: step 59170, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 71h:42m:31s remains)
INFO - root - 2017-12-06 01:08:04.365095: step 59180, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 63h:17m:24s remains)
INFO - root - 2017-12-06 01:08:13.536395: step 59190, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 71h:41m:44s remains)
INFO - root - 2017-12-06 01:08:22.930845: step 59200, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 70h:30m:26s remains)
2017-12-06 01:08:23.782691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.266294 -4.2505827 -4.2342391 -4.216773 -4.1958923 -4.1835337 -4.1918039 -4.207972 -4.2299719 -4.2495561 -4.2597208 -4.2541437 -4.2420039 -4.2260008 -4.207747][-4.2547617 -4.2376037 -4.2249494 -4.2099166 -4.1866245 -4.1685233 -4.1733646 -4.1881 -4.2101521 -4.23101 -4.2410274 -4.2352514 -4.2227573 -4.2011309 -4.174221][-4.2464318 -4.2366576 -4.2337737 -4.2245975 -4.1998963 -4.1741972 -4.1686144 -4.1745896 -4.1943941 -4.21871 -4.2322464 -4.22738 -4.2146831 -4.186708 -4.1500645][-4.2410831 -4.2454724 -4.2547846 -4.2507629 -4.2238841 -4.1892109 -4.1675258 -4.1590223 -4.1763363 -4.2065091 -4.2261372 -4.2252579 -4.2158146 -4.1864758 -4.1456037][-4.2371035 -4.2570353 -4.275003 -4.2712016 -4.2394891 -4.1953111 -4.157352 -4.1334958 -4.1471434 -4.1849728 -4.2151504 -4.2239428 -4.2221074 -4.1980619 -4.1613][-4.2264395 -4.257184 -4.2791576 -4.2726855 -4.2346334 -4.178751 -4.1227136 -4.0819063 -4.0939169 -4.1451178 -4.1893182 -4.2136121 -4.2250953 -4.2110205 -4.1828127][-4.2082314 -4.2419996 -4.2624259 -4.2519326 -4.2068796 -4.1390619 -4.0659847 -4.0111957 -4.02935 -4.1010585 -4.1647987 -4.2049642 -4.2283936 -4.2238097 -4.2036071][-4.198184 -4.229301 -4.243927 -4.2255325 -4.1728759 -4.0981808 -4.0175614 -3.9613595 -3.9945545 -4.0868773 -4.1647935 -4.2113695 -4.2390027 -4.2366009 -4.2188172][-4.2107396 -4.2369833 -4.2434969 -4.2196975 -4.168992 -4.1052356 -4.0413737 -4.0040665 -4.0413318 -4.1232462 -4.188509 -4.225872 -4.2479482 -4.2436795 -4.2266455][-4.2312632 -4.2522821 -4.2550492 -4.2334404 -4.1926436 -4.1473708 -4.1084161 -4.0895858 -4.1162963 -4.1697264 -4.2089767 -4.2318573 -4.2468367 -4.24185 -4.227695][-4.2501111 -4.2703376 -4.274426 -4.2586441 -4.2259951 -4.192987 -4.1688757 -4.1580162 -4.1694632 -4.1943731 -4.2107406 -4.2204514 -4.2295661 -4.2248688 -4.2121472][-4.2709508 -4.2900138 -4.294312 -4.2826962 -4.2567487 -4.2292504 -4.2076035 -4.193018 -4.1882315 -4.1906 -4.1918182 -4.1929092 -4.1991987 -4.195241 -4.1838231][-4.2916827 -4.3037734 -4.3029032 -4.2906861 -4.2668715 -4.2399807 -4.2147193 -4.1917553 -4.1728992 -4.1591783 -4.1522527 -4.1510539 -4.1594257 -4.1606593 -4.1568065][-4.3042464 -4.3051023 -4.2944503 -4.2772741 -4.2528634 -4.2262464 -4.1980271 -4.1701632 -4.1432528 -4.1233168 -4.1150723 -4.1176167 -4.1306787 -4.1402068 -4.145999][-4.2985878 -4.2908316 -4.2752733 -4.2546458 -4.2289548 -4.2020206 -4.1728392 -4.1458321 -4.121314 -4.1041975 -4.1021514 -4.11159 -4.1298852 -4.1444092 -4.1544914]]...]
INFO - root - 2017-12-06 01:08:33.284715: step 59210, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.988 sec/batch; 75h:01m:30s remains)
INFO - root - 2017-12-06 01:08:42.608373: step 59220, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 73h:51m:32s remains)
INFO - root - 2017-12-06 01:08:51.963937: step 59230, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 74h:16m:16s remains)
INFO - root - 2017-12-06 01:09:01.425718: step 59240, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 71h:16m:28s remains)
INFO - root - 2017-12-06 01:09:10.865448: step 59250, loss = 2.03, batch loss = 1.97 (8.2 examples/sec; 0.975 sec/batch; 73h:59m:19s remains)
INFO - root - 2017-12-06 01:09:20.029872: step 59260, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 71h:47m:16s remains)
INFO - root - 2017-12-06 01:09:29.441800: step 59270, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 74h:09m:02s remains)
INFO - root - 2017-12-06 01:09:39.043021: step 59280, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 72h:54m:05s remains)
INFO - root - 2017-12-06 01:09:48.468488: step 59290, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.012 sec/batch; 76h:49m:52s remains)
INFO - root - 2017-12-06 01:09:57.802534: step 59300, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 71h:35m:23s remains)
2017-12-06 01:09:58.556858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.275784 -4.2756138 -4.2699337 -4.2652311 -4.2530742 -4.2227254 -4.1865768 -4.1559892 -4.1615024 -4.1825972 -4.2174449 -4.2545433 -4.2871256 -4.3014479 -4.3027043][-4.25934 -4.2634106 -4.2596388 -4.2574472 -4.242198 -4.205904 -4.1641283 -4.1343241 -4.1412697 -4.1674976 -4.2069473 -4.249361 -4.2844877 -4.2997255 -4.3009357][-4.2208934 -4.2312603 -4.2348385 -4.2404079 -4.2245665 -4.1834617 -4.13575 -4.1091356 -4.1194644 -4.1517491 -4.1949453 -4.2395196 -4.2751322 -4.2913752 -4.2947831][-4.1575079 -4.1802559 -4.1974025 -4.2129755 -4.2046967 -4.1655874 -4.1210241 -4.1051888 -4.1223006 -4.1557374 -4.1941886 -4.233974 -4.2674813 -4.2837095 -4.2880979][-4.1061931 -4.1407332 -4.1725879 -4.1968961 -4.1968737 -4.1606355 -4.1154146 -4.1054106 -4.137917 -4.1767893 -4.20936 -4.2427773 -4.2748089 -4.2903328 -4.2929831][-4.1104074 -4.1464782 -4.1754904 -4.1893191 -4.178453 -4.1229887 -4.0520129 -4.0360241 -4.0996513 -4.1638865 -4.2058711 -4.23975 -4.2759666 -4.2972136 -4.3003531][-4.1168509 -4.1453791 -4.1570606 -4.1464953 -4.1093488 -4.0155172 -3.8962069 -3.862679 -3.9750028 -4.0833683 -4.1517258 -4.1983624 -4.24741 -4.2825742 -4.29487][-4.1140356 -4.12656 -4.1209273 -4.089262 -4.0314674 -3.9109092 -3.7549162 -3.7029386 -3.857549 -4.00523 -4.0972776 -4.1561656 -4.2137728 -4.2600503 -4.2822165][-4.1367846 -4.1381755 -4.1294036 -4.1031928 -4.0658607 -3.9798496 -3.8663843 -3.8135774 -3.9288378 -4.0533328 -4.1364331 -4.1884179 -4.2343273 -4.2678437 -4.2837849][-4.1711049 -4.1641855 -4.1596012 -4.1492853 -4.137702 -4.0894904 -4.0238433 -3.9821284 -4.0440369 -4.1317716 -4.19895 -4.241612 -4.27459 -4.2918968 -4.2961879][-4.2091737 -4.1980772 -4.1970158 -4.1963148 -4.1892753 -4.1540904 -4.1097231 -4.0738945 -4.1061945 -4.1735735 -4.2336779 -4.2747178 -4.3002615 -4.310143 -4.3082995][-4.2389655 -4.2320971 -4.2359109 -4.243 -4.23954 -4.2102141 -4.1704779 -4.137516 -4.1535873 -4.2030277 -4.2513719 -4.2876554 -4.3098278 -4.3153343 -4.3125181][-4.2770672 -4.27527 -4.2804642 -4.2886457 -4.28914 -4.2648921 -4.2281218 -4.1948862 -4.1966286 -4.2270865 -4.2647176 -4.2948089 -4.3141618 -4.3201141 -4.3173223][-4.3116875 -4.3106265 -4.3128619 -4.3177371 -4.3175964 -4.30074 -4.270318 -4.2381263 -4.2319965 -4.2474427 -4.2768559 -4.3033819 -4.3199921 -4.3264918 -4.3224235][-4.3202815 -4.317831 -4.3171515 -4.3193069 -4.3186612 -4.308032 -4.2853904 -4.2581797 -4.2506118 -4.2591019 -4.2822738 -4.3035183 -4.3150482 -4.3216586 -4.3192043]]...]
INFO - root - 2017-12-06 01:10:08.089656: step 59310, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.994 sec/batch; 75h:26m:38s remains)
INFO - root - 2017-12-06 01:10:17.643413: step 59320, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.993 sec/batch; 75h:20m:36s remains)
INFO - root - 2017-12-06 01:10:27.024610: step 59330, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 71h:42m:48s remains)
INFO - root - 2017-12-06 01:10:36.419042: step 59340, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 70h:24m:39s remains)
INFO - root - 2017-12-06 01:10:45.787747: step 59350, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 71h:41m:59s remains)
INFO - root - 2017-12-06 01:10:55.231254: step 59360, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.945 sec/batch; 71h:40m:50s remains)
INFO - root - 2017-12-06 01:11:04.546711: step 59370, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 65h:05m:40s remains)
INFO - root - 2017-12-06 01:11:13.808658: step 59380, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 71h:16m:34s remains)
INFO - root - 2017-12-06 01:11:23.229805: step 59390, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 71h:58m:04s remains)
INFO - root - 2017-12-06 01:11:32.562465: step 59400, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 72h:39m:38s remains)
2017-12-06 01:11:33.304355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25394 -4.2608919 -4.2718277 -4.2694826 -4.2562184 -4.2399473 -4.2243996 -4.2075415 -4.1997948 -4.2065921 -4.2261176 -4.2332048 -4.2232118 -4.2111349 -4.2156472][-4.2526417 -4.261838 -4.2706575 -4.26257 -4.248138 -4.2324042 -4.2087617 -4.17676 -4.1638813 -4.182519 -4.2183275 -4.233695 -4.221704 -4.2027235 -4.2014241][-4.2560863 -4.2719741 -4.2829571 -4.2697496 -4.2530622 -4.2308931 -4.1873493 -4.139122 -4.1290941 -4.1672478 -4.2164726 -4.2351718 -4.2224064 -4.195807 -4.1885633][-4.2602577 -4.2838154 -4.2977791 -4.2800736 -4.2531991 -4.2109976 -4.1397943 -4.0822883 -4.0956354 -4.1611814 -4.2191124 -4.2329745 -4.2143612 -4.184123 -4.1757183][-4.2691031 -4.2946482 -4.3066683 -4.2857485 -4.2434888 -4.1703305 -4.0662217 -4.0150342 -4.0755248 -4.1687117 -4.2243209 -4.2309957 -4.2062311 -4.1754327 -4.1699758][-4.2760057 -4.2965703 -4.303442 -4.2805128 -4.2218089 -4.1124077 -3.974026 -3.9462981 -4.0684271 -4.1834817 -4.2312465 -4.2307391 -4.2003412 -4.1697154 -4.1699672][-4.2606511 -4.2742085 -4.2789345 -4.2558494 -4.1807508 -4.0413089 -3.8847017 -3.8993134 -4.0672913 -4.1853304 -4.2216358 -4.21585 -4.1845317 -4.156991 -4.166759][-4.2415681 -4.250761 -4.256093 -4.2320447 -4.1483917 -4.0052161 -3.8788114 -3.9406435 -4.1014137 -4.1890063 -4.202785 -4.1882973 -4.1616268 -4.1462793 -4.17021][-4.2245979 -4.2334714 -4.2388272 -4.2139459 -4.1294127 -4.0041747 -3.9405549 -4.0316272 -4.1562834 -4.2028546 -4.1890526 -4.1625776 -4.1444945 -4.1452451 -4.1817722][-4.2007031 -4.2091794 -4.21405 -4.1879387 -4.1051812 -4.000113 -3.9875934 -4.0909867 -4.184546 -4.2010179 -4.1713 -4.1424203 -4.1345215 -4.1518283 -4.1941457][-4.1907 -4.1985941 -4.2028327 -4.1736622 -4.0941372 -4.0107527 -4.0256534 -4.1233344 -4.1923289 -4.1942053 -4.1672392 -4.148622 -4.150847 -4.1750369 -4.2156057][-4.2280517 -4.2328944 -4.2324667 -4.2023211 -4.1345844 -4.0778761 -4.100215 -4.1736279 -4.21725 -4.2136517 -4.196403 -4.1929574 -4.2021489 -4.2233448 -4.2570481][-4.27159 -4.2729454 -4.27014 -4.2439713 -4.1958022 -4.1610708 -4.1775551 -4.2239485 -4.250186 -4.2483873 -4.2439947 -4.253449 -4.265 -4.2796865 -4.3042197][-4.31942 -4.3180017 -4.3132591 -4.2942758 -4.2642975 -4.2450395 -4.2556376 -4.2824006 -4.2966461 -4.2951097 -4.2984529 -4.3129554 -4.3234172 -4.3308835 -4.3433208][-4.3531752 -4.3516316 -4.3461394 -4.3337517 -4.3182216 -4.310102 -4.316411 -4.3308764 -4.3372412 -4.3357873 -4.3391342 -4.350204 -4.3561983 -4.35781 -4.3604674]]...]
INFO - root - 2017-12-06 01:11:42.528290: step 59410, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.945 sec/batch; 71h:39m:59s remains)
INFO - root - 2017-12-06 01:11:51.819367: step 59420, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 70h:04m:47s remains)
INFO - root - 2017-12-06 01:12:01.234204: step 59430, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 70h:56m:09s remains)
INFO - root - 2017-12-06 01:12:10.396761: step 59440, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 64h:43m:21s remains)
INFO - root - 2017-12-06 01:12:19.863899: step 59450, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 70h:28m:47s remains)
INFO - root - 2017-12-06 01:12:29.224931: step 59460, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 71h:11m:28s remains)
INFO - root - 2017-12-06 01:12:38.624007: step 59470, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 69h:44m:08s remains)
INFO - root - 2017-12-06 01:12:47.887527: step 59480, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.942 sec/batch; 71h:25m:02s remains)
INFO - root - 2017-12-06 01:12:57.258358: step 59490, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 65h:35m:41s remains)
INFO - root - 2017-12-06 01:13:06.519459: step 59500, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 69h:22m:56s remains)
2017-12-06 01:13:07.318255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3399343 -4.3272042 -4.3025432 -4.2820454 -4.2724886 -4.2677612 -4.2607017 -4.2563219 -4.2641959 -4.2775383 -4.2878747 -4.2818913 -4.2579679 -4.2480211 -4.2560024][-4.332716 -4.3154087 -4.2821465 -4.2515359 -4.2385297 -4.23679 -4.2337012 -4.2338433 -4.2471142 -4.2692318 -4.2851205 -4.2803135 -4.2581844 -4.2417784 -4.2411718][-4.3231773 -4.3007016 -4.2579317 -4.2122517 -4.1888285 -4.1840281 -4.1842155 -4.193202 -4.2189789 -4.2516809 -4.2734346 -4.2705107 -4.2551789 -4.2392125 -4.2329245][-4.3199267 -4.2934217 -4.2425957 -4.1832781 -4.1423078 -4.1275544 -4.1298285 -4.1512942 -4.1941476 -4.2387156 -4.2675672 -4.2691655 -4.2600241 -4.2500696 -4.2449164][-4.3233242 -4.2965431 -4.2458439 -4.1835642 -4.1299238 -4.1047721 -4.1100364 -4.1371284 -4.1820769 -4.2241135 -4.2543273 -4.2626462 -4.2600336 -4.2602134 -4.2583766][-4.3270869 -4.30211 -4.258028 -4.2046261 -4.1527228 -4.1232128 -4.1250691 -4.1492562 -4.1856489 -4.2145581 -4.2374644 -4.2461662 -4.2462125 -4.2535892 -4.2574587][-4.3274827 -4.30161 -4.2623072 -4.2172809 -4.1737766 -4.1457839 -4.1417336 -4.1587968 -4.1869264 -4.2102165 -4.2281632 -4.2348089 -4.2337446 -4.238975 -4.2432876][-4.3238506 -4.2948966 -4.2546759 -4.2105031 -4.1717939 -4.1472282 -4.1358995 -4.1451526 -4.1687837 -4.1978531 -4.2210865 -4.2281046 -4.2283039 -4.2328138 -4.234489][-4.3186936 -4.28894 -4.2475982 -4.2031074 -4.1654091 -4.1397581 -4.1212473 -4.1241016 -4.14841 -4.1844931 -4.2127585 -4.2214661 -4.2242002 -4.2266765 -4.2239265][-4.3121495 -4.2827559 -4.2420068 -4.1987309 -4.163949 -4.1393361 -4.119771 -4.1212058 -4.1468315 -4.1826849 -4.2114091 -4.2207842 -4.2268772 -4.2320776 -4.228395][-4.3038754 -4.2748404 -4.234581 -4.1919484 -4.1629915 -4.1444168 -4.1309538 -4.1348209 -4.1603737 -4.1921978 -4.2140245 -4.2191076 -4.2276416 -4.2372003 -4.2389755][-4.3000865 -4.2743206 -4.2384477 -4.2008443 -4.1805162 -4.1704297 -4.1620278 -4.1637597 -4.1818972 -4.2047048 -4.2211008 -4.2254052 -4.2343192 -4.2442894 -4.2472115][-4.3028889 -4.2799997 -4.2502894 -4.2205667 -4.206727 -4.2019768 -4.1983829 -4.1995931 -4.2086067 -4.2214541 -4.2338715 -4.2359476 -4.2415466 -4.2496667 -4.2527595][-4.3119726 -4.2911768 -4.2675633 -4.2460876 -4.2370429 -4.2354937 -4.235754 -4.2405157 -4.2468076 -4.2512617 -4.255795 -4.2551761 -4.2558336 -4.2573195 -4.258111][-4.3234181 -4.30427 -4.2867064 -4.2719746 -4.2659559 -4.2660651 -4.2688551 -4.2743845 -4.2790995 -4.2799225 -4.2794714 -4.2756753 -4.273839 -4.2740927 -4.2783942]]...]
INFO - root - 2017-12-06 01:13:16.576157: step 59510, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 72h:10m:11s remains)
INFO - root - 2017-12-06 01:13:26.026381: step 59520, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 73h:40m:36s remains)
INFO - root - 2017-12-06 01:13:35.375807: step 59530, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 67h:15m:50s remains)
INFO - root - 2017-12-06 01:13:44.746048: step 59540, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 69h:20m:13s remains)
INFO - root - 2017-12-06 01:13:53.974825: step 59550, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 73h:24m:13s remains)
INFO - root - 2017-12-06 01:14:03.307429: step 59560, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 73h:40m:55s remains)
INFO - root - 2017-12-06 01:14:12.880388: step 59570, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.888 sec/batch; 67h:20m:47s remains)
INFO - root - 2017-12-06 01:14:22.272601: step 59580, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 70h:11m:39s remains)
INFO - root - 2017-12-06 01:14:31.847208: step 59590, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 74h:45m:59s remains)
INFO - root - 2017-12-06 01:14:41.125524: step 59600, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 66h:16m:01s remains)
2017-12-06 01:14:41.907376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3207245 -4.3188605 -4.3140459 -4.3096185 -4.3085656 -4.3102407 -4.3107491 -4.3133054 -4.315629 -4.3139715 -4.308948 -4.3047619 -4.3027983 -4.304873 -4.3115506][-4.3072472 -4.302238 -4.2926989 -4.2849584 -4.2863755 -4.2923608 -4.2966828 -4.3048763 -4.3100295 -4.306932 -4.2958322 -4.2856026 -4.279068 -4.2799187 -4.2894149][-4.2760034 -4.2664151 -4.2569413 -4.2520761 -4.2614961 -4.2716956 -4.2769804 -4.2860441 -4.2914672 -4.2875409 -4.2743654 -4.2632141 -4.2566175 -4.2581635 -4.2694664][-4.2291903 -4.2195673 -4.2185683 -4.2222352 -4.2389894 -4.2496409 -4.25127 -4.2573495 -4.261095 -4.2543616 -4.2384782 -4.2327151 -4.2363386 -4.2458525 -4.26034][-4.175427 -4.1650319 -4.1792688 -4.19574 -4.2136946 -4.2160854 -4.206666 -4.2108812 -4.2204041 -4.2152133 -4.19758 -4.1994882 -4.2177529 -4.2405705 -4.2584047][-4.1302743 -4.1153784 -4.13356 -4.1538963 -4.1633415 -4.145256 -4.116941 -4.1249003 -4.1532569 -4.1620874 -4.1546354 -4.1691089 -4.1999817 -4.2312469 -4.2527657][-4.1061158 -4.0854359 -4.094614 -4.1056223 -4.0958996 -4.0373421 -3.9651723 -3.9749527 -4.0443811 -4.0898175 -4.10827 -4.1382117 -4.18091 -4.2191143 -4.2468429][-4.1365986 -4.1026998 -4.0915227 -4.082973 -4.0502419 -3.9515657 -3.824744 -3.8338406 -3.9490294 -4.0335011 -4.075325 -4.1169333 -4.16734 -4.2115269 -4.2428107][-4.211791 -4.1739511 -4.1483321 -4.123796 -4.0847187 -3.9945364 -3.8809125 -3.887219 -3.9916341 -4.068212 -4.1031132 -4.1367493 -4.1790948 -4.2177768 -4.2462964][-4.2774329 -4.2499456 -4.2238674 -4.1991491 -4.1668792 -4.108727 -4.0453629 -4.0487008 -4.1153917 -4.1633458 -4.1789703 -4.1938925 -4.2173252 -4.2413349 -4.2582974][-4.3046632 -4.2856269 -4.2624826 -4.2393656 -4.2124505 -4.1776667 -4.1501632 -4.156827 -4.1969972 -4.2241297 -4.230732 -4.2373323 -4.2517 -4.2659669 -4.2721686][-4.3138976 -4.29848 -4.280416 -4.2617183 -4.2402964 -4.2185626 -4.2057033 -4.21359 -4.2387986 -4.2522168 -4.2544355 -4.2581425 -4.2703996 -4.2813783 -4.2823048][-4.3180747 -4.3019724 -4.2882085 -4.2744627 -4.2600455 -4.2491565 -4.2401714 -4.2468171 -4.2659326 -4.2762332 -4.2767663 -4.2796488 -4.2882905 -4.2946854 -4.2920461][-4.3206315 -4.304585 -4.2931018 -4.2830372 -4.2755136 -4.26863 -4.2590151 -4.2617369 -4.2760444 -4.2869925 -4.2916937 -4.2963295 -4.3032403 -4.3052473 -4.3012996][-4.3173714 -4.3034854 -4.2921109 -4.2849913 -4.2820826 -4.2777338 -4.2696228 -4.2722726 -4.2830405 -4.2931166 -4.3017812 -4.3093266 -4.3152237 -4.3157396 -4.3126922]]...]
INFO - root - 2017-12-06 01:14:51.345999: step 59610, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 71h:13m:38s remains)
INFO - root - 2017-12-06 01:15:00.782156: step 59620, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 72h:02m:09s remains)
INFO - root - 2017-12-06 01:15:09.930095: step 59630, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 68h:26m:14s remains)
INFO - root - 2017-12-06 01:15:19.368027: step 59640, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 71h:15m:23s remains)
INFO - root - 2017-12-06 01:15:28.919726: step 59650, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 75h:11m:26s remains)
INFO - root - 2017-12-06 01:15:38.103359: step 59660, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 69h:02m:01s remains)
INFO - root - 2017-12-06 01:15:47.557930: step 59670, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 72h:32m:49s remains)
INFO - root - 2017-12-06 01:15:56.975889: step 59680, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 71h:47m:09s remains)
INFO - root - 2017-12-06 01:16:06.321235: step 59690, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 69h:46m:22s remains)
INFO - root - 2017-12-06 01:16:15.642284: step 59700, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 67h:26m:46s remains)
2017-12-06 01:16:16.409842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2704687 -4.316432 -4.3439641 -4.3489676 -4.3376865 -4.3079295 -4.26827 -4.2330165 -4.216074 -4.2219315 -4.224504 -4.221498 -4.1959162 -4.1667485 -4.1736846][-4.291945 -4.3356 -4.3565521 -4.3550243 -4.3350983 -4.2973843 -4.2553039 -4.2286711 -4.2260137 -4.2437954 -4.2616739 -4.2702122 -4.2560306 -4.2233515 -4.2084241][-4.3066292 -4.3438015 -4.3546557 -4.3435383 -4.3147459 -4.2694883 -4.223547 -4.203886 -4.2195706 -4.250555 -4.2803383 -4.2986412 -4.2950997 -4.2689519 -4.2463007][-4.3097625 -4.3391876 -4.3432851 -4.3260794 -4.292851 -4.2391992 -4.1798325 -4.1537142 -4.1804867 -4.2276721 -4.273006 -4.3036509 -4.3066292 -4.2884111 -4.2637148][-4.3055778 -4.3241138 -4.3223076 -4.2991 -4.2583504 -4.1946011 -4.1132255 -4.06459 -4.0919957 -4.1633048 -4.2302275 -4.2773638 -4.2906485 -4.2824216 -4.2631397][-4.2955632 -4.3021951 -4.2887154 -4.2564387 -4.2085142 -4.1360655 -4.0313091 -3.9475739 -3.9717045 -4.0751114 -4.1734023 -4.2374377 -4.2593 -4.2600913 -4.2502446][-4.2869973 -4.2796593 -4.254127 -4.2118993 -4.1592288 -4.0898652 -3.9720051 -3.8520658 -3.8685844 -4.0025067 -4.1323528 -4.2123828 -4.2415218 -4.2512889 -4.2512856][-4.2800307 -4.2629328 -4.2302566 -4.1823778 -4.1308956 -4.0806732 -3.9835343 -3.8672149 -3.862237 -3.9832785 -4.1154232 -4.2017345 -4.240016 -4.2560906 -4.2636518][-4.2770376 -4.2554216 -4.2192488 -4.1720572 -4.12902 -4.1062765 -4.0513196 -3.9700086 -3.9421408 -4.0153041 -4.1245236 -4.2040443 -4.2427983 -4.2578988 -4.2681141][-4.2700028 -4.2525229 -4.2200732 -4.1806469 -4.1525369 -4.1574173 -4.1380911 -4.0836806 -4.0458732 -4.0772882 -4.1589336 -4.22929 -4.2643738 -4.2762642 -4.2811046][-4.2615757 -4.2462397 -4.2172971 -4.18566 -4.1743784 -4.2024117 -4.2098141 -4.1808095 -4.1467886 -4.15396 -4.2097678 -4.2686667 -4.2998652 -4.3112664 -4.3117862][-4.2609329 -4.2430305 -4.215518 -4.1915512 -4.1927319 -4.2336173 -4.2596426 -4.2531586 -4.2287807 -4.2242913 -4.2566104 -4.2995243 -4.3277755 -4.3391795 -4.3395967][-4.2620463 -4.242929 -4.215704 -4.19763 -4.204638 -4.2469459 -4.2805347 -4.2857 -4.2701106 -4.2603846 -4.2788086 -4.312686 -4.3400578 -4.3519535 -4.3500195][-4.2614326 -4.2417154 -4.2148023 -4.2004142 -4.2111154 -4.2499447 -4.2818656 -4.2879415 -4.2749333 -4.2658763 -4.2810416 -4.3120632 -4.3385305 -4.3493285 -4.3429008][-4.2644615 -4.2480454 -4.2251658 -4.212532 -4.2230887 -4.2528028 -4.27277 -4.2671766 -4.2498765 -4.2445955 -4.2642622 -4.2977738 -4.3251376 -4.3315954 -4.3160996]]...]
INFO - root - 2017-12-06 01:16:25.926763: step 59710, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 69h:34m:36s remains)
INFO - root - 2017-12-06 01:16:35.331157: step 59720, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 69h:22m:18s remains)
INFO - root - 2017-12-06 01:16:44.905150: step 59730, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 66h:46m:20s remains)
INFO - root - 2017-12-06 01:16:54.349544: step 59740, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 75h:01m:31s remains)
INFO - root - 2017-12-06 01:17:03.793438: step 59750, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 73h:05m:45s remains)
INFO - root - 2017-12-06 01:17:13.149643: step 59760, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 69h:59m:52s remains)
INFO - root - 2017-12-06 01:17:22.392063: step 59770, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 65h:26m:14s remains)
INFO - root - 2017-12-06 01:17:31.665060: step 59780, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 69h:52m:10s remains)
INFO - root - 2017-12-06 01:17:40.987906: step 59790, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 71h:57m:03s remains)
INFO - root - 2017-12-06 01:17:50.365893: step 59800, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 69h:36m:48s remains)
2017-12-06 01:17:51.103708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3317008 -4.3319798 -4.3303051 -4.3294592 -4.3282542 -4.3285985 -4.329999 -4.3306341 -4.3328071 -4.3353558 -4.3374538 -4.3381448 -4.3373656 -4.3387861 -4.3421421][-4.3136425 -4.31334 -4.3123302 -4.3122144 -4.3114104 -4.3105268 -4.3084955 -4.3065166 -4.3094339 -4.3125415 -4.31312 -4.3110328 -4.3077326 -4.3086886 -4.314055][-4.2979112 -4.2986808 -4.2976432 -4.2977047 -4.2961311 -4.2938366 -4.2862763 -4.28137 -4.2867007 -4.2898951 -4.28539 -4.2792754 -4.2711244 -4.269969 -4.2766004][-4.2850122 -4.2805557 -4.2756944 -4.2730994 -4.2699442 -4.2634125 -4.2489843 -4.2409077 -4.2502804 -4.2549 -4.2480669 -4.2363086 -4.2242165 -4.2215772 -4.2327428][-4.2748976 -4.2631674 -4.2500668 -4.2360592 -4.2231064 -4.2070146 -4.1810427 -4.1637359 -4.1748118 -4.1932907 -4.1964417 -4.188159 -4.177567 -4.17813 -4.1954927][-4.2649336 -4.2444777 -4.2187066 -4.1885953 -4.1618729 -4.1329355 -4.0846906 -4.0387278 -4.0498228 -4.1066885 -4.1463246 -4.1524739 -4.1482949 -4.1534576 -4.1740236][-4.2530928 -4.2246256 -4.1857581 -4.1415281 -4.0983853 -4.0438762 -3.9575863 -3.8676288 -3.881264 -3.994683 -4.0855255 -4.1163015 -4.1203079 -4.1279035 -4.148551][-4.2481208 -4.2182717 -4.1721611 -4.1187763 -4.0588403 -3.9834039 -3.8714724 -3.7513175 -3.7720222 -3.9271193 -4.0484977 -4.0898905 -4.0915661 -4.0993543 -4.1233711][-4.2522178 -4.2291017 -4.1879177 -4.13357 -4.0713015 -4.0012307 -3.9172993 -3.8315194 -3.8518462 -3.9765251 -4.073637 -4.0969152 -4.0864406 -4.0883555 -4.1137671][-4.2609673 -4.2469807 -4.2176876 -4.1703959 -4.118556 -4.070509 -4.028872 -3.9889169 -4.0012608 -4.0667191 -4.1177197 -4.1168003 -4.0947905 -4.0948715 -4.1219034][-4.2661352 -4.2565441 -4.2350621 -4.2010288 -4.1625504 -4.1328645 -4.1192222 -4.1053028 -4.1069098 -4.1339602 -4.1516423 -4.1354079 -4.1101689 -4.1151628 -4.1478734][-4.2724 -4.2630148 -4.2476215 -4.227015 -4.2032089 -4.1855679 -4.1880198 -4.1924276 -4.1934571 -4.1987662 -4.1956005 -4.1765275 -4.1584024 -4.1671286 -4.1990352][-4.280601 -4.2742352 -4.2653074 -4.2556973 -4.244369 -4.2370234 -4.2449579 -4.2555161 -4.2589378 -4.2585325 -4.2512016 -4.2385559 -4.2291861 -4.2370286 -4.2609062][-4.2932682 -4.2901721 -4.2875571 -4.2871475 -4.2854352 -4.2841473 -4.288579 -4.2937059 -4.2931471 -4.2904148 -4.2845426 -4.2784433 -4.2792616 -4.2887387 -4.30641][-4.3103738 -4.3090096 -4.3092647 -4.3107762 -4.311254 -4.3105736 -4.3104496 -4.3101215 -4.3075724 -4.3053613 -4.3030791 -4.3026557 -4.307457 -4.3168945 -4.3290663]]...]
INFO - root - 2017-12-06 01:18:00.432032: step 59810, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 69h:17m:19s remains)
INFO - root - 2017-12-06 01:18:09.922525: step 59820, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.978 sec/batch; 74h:05m:47s remains)
INFO - root - 2017-12-06 01:18:19.346822: step 59830, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 75h:40m:16s remains)
INFO - root - 2017-12-06 01:18:28.625914: step 59840, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.871 sec/batch; 66h:00m:16s remains)
INFO - root - 2017-12-06 01:18:37.932904: step 59850, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 72h:43m:37s remains)
INFO - root - 2017-12-06 01:18:47.620557: step 59860, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.979 sec/batch; 74h:09m:18s remains)
INFO - root - 2017-12-06 01:18:56.699575: step 59870, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 66h:27m:07s remains)
INFO - root - 2017-12-06 01:19:06.134662: step 59880, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 71h:36m:58s remains)
INFO - root - 2017-12-06 01:19:15.439861: step 59890, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.865 sec/batch; 65h:28m:39s remains)
INFO - root - 2017-12-06 01:19:24.972042: step 59900, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.967 sec/batch; 73h:13m:54s remains)
2017-12-06 01:19:25.797547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2637167 -4.2418795 -4.2394404 -4.2514715 -4.2618518 -4.2695112 -4.2788177 -4.2913957 -4.3012042 -4.3013678 -4.290247 -4.2677588 -4.2478681 -4.2412996 -4.2435617][-4.2635784 -4.2368636 -4.2279825 -4.233398 -4.2361584 -4.2368565 -4.2424984 -4.252728 -4.2619615 -4.2665534 -4.26111 -4.2405314 -4.2239456 -4.2252941 -4.2361336][-4.2905984 -4.2613144 -4.2402563 -4.2299347 -4.2161622 -4.2026887 -4.1982336 -4.2066455 -4.2211666 -4.233407 -4.2359219 -4.2211118 -4.2078958 -4.2143397 -4.2310963][-4.328022 -4.2976427 -4.2622309 -4.2318034 -4.1966696 -4.1623244 -4.143301 -4.1500468 -4.1728568 -4.1976671 -4.2125831 -4.2085738 -4.2007632 -4.2095046 -4.2287116][-4.3550873 -4.326386 -4.2817574 -4.2334628 -4.1780353 -4.1199446 -4.0827875 -4.0864959 -4.1190443 -4.1570144 -4.1852884 -4.1966124 -4.1995506 -4.2098665 -4.2300267][-4.3642077 -4.3363252 -4.2857032 -4.2214818 -4.1469841 -4.0667973 -4.0160589 -4.0184116 -4.0593462 -4.1105089 -4.154758 -4.1847219 -4.2023797 -4.2182746 -4.2382288][-4.3551331 -4.3248134 -4.2704067 -4.198009 -4.1115084 -4.0124717 -3.9475231 -3.9453182 -3.9934988 -4.05793 -4.1188846 -4.1699481 -4.2062774 -4.2312317 -4.2546177][-4.3445354 -4.312562 -4.2563663 -4.1804533 -4.084959 -3.9697969 -3.8874068 -3.8795118 -3.932682 -4.0093503 -4.085176 -4.1532845 -4.2021852 -4.2337861 -4.2605381][-4.3350034 -4.3047776 -4.2544007 -4.1873932 -4.098537 -3.9826717 -3.8853164 -3.8611352 -3.9060688 -3.983433 -4.0654178 -4.1396918 -4.1929126 -4.2289147 -4.2586069][-4.3216052 -4.29675 -4.2614017 -4.2156157 -4.1506486 -4.0587358 -3.9697592 -3.9329026 -3.9567442 -4.0168753 -4.0866795 -4.1526093 -4.1999326 -4.2319155 -4.2582016][-4.3016877 -4.2804475 -4.2621379 -4.2436166 -4.2104774 -4.151751 -4.0886607 -4.0555015 -4.0624876 -4.0989027 -4.1488891 -4.1985779 -4.232965 -4.2529674 -4.2682366][-4.2841864 -4.2612758 -4.2533417 -4.2582426 -4.2538252 -4.2253642 -4.1883283 -4.1677685 -4.1692448 -4.1867752 -4.2151985 -4.2460537 -4.2656174 -4.2732263 -4.2787042][-4.2787857 -4.250349 -4.2435851 -4.2607336 -4.2745957 -4.2674308 -4.2508578 -4.2417841 -4.2422814 -4.2489409 -4.2617245 -4.2776752 -4.2840753 -4.2812657 -4.2797236][-4.2910194 -4.2595506 -4.2483587 -4.2657342 -4.2873054 -4.2920303 -4.2883725 -4.286633 -4.2859039 -4.284626 -4.2857685 -4.2887778 -4.2843575 -4.2739029 -4.267374][-4.3030152 -4.2740641 -4.2601004 -4.2743616 -4.2959461 -4.3037019 -4.3035407 -4.3034844 -4.3028274 -4.2978806 -4.2910032 -4.28336 -4.2696075 -4.2523656 -4.243361]]...]
INFO - root - 2017-12-06 01:19:35.261460: step 59910, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.966 sec/batch; 73h:09m:48s remains)
INFO - root - 2017-12-06 01:19:44.722115: step 59920, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 74h:51m:02s remains)
INFO - root - 2017-12-06 01:19:54.234190: step 59930, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 73h:27m:34s remains)
INFO - root - 2017-12-06 01:20:03.409607: step 59940, loss = 2.10, batch loss = 2.05 (9.2 examples/sec; 0.874 sec/batch; 66h:10m:22s remains)
INFO - root - 2017-12-06 01:20:12.701437: step 59950, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 66h:44m:27s remains)
INFO - root - 2017-12-06 01:20:22.217198: step 59960, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 72h:00m:20s remains)
INFO - root - 2017-12-06 01:20:31.491015: step 59970, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 69h:14m:21s remains)
INFO - root - 2017-12-06 01:20:40.941235: step 59980, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 70h:18m:44s remains)
INFO - root - 2017-12-06 01:20:50.357139: step 59990, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 71h:05m:44s remains)
INFO - root - 2017-12-06 01:20:59.709268: step 60000, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 69h:06m:51s remains)
2017-12-06 01:21:00.502784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3250418 -4.3285637 -4.3287172 -4.326088 -4.3225412 -4.3176713 -4.3153071 -4.3169394 -4.3187885 -4.3182173 -4.3172574 -4.3170872 -4.3198352 -4.3239412 -4.3249912][-4.3277464 -4.3287916 -4.3255 -4.3170662 -4.30787 -4.3017383 -4.3010035 -4.30619 -4.31233 -4.311945 -4.3084517 -4.3070288 -4.3122544 -4.3191719 -4.3203192][-4.3183265 -4.3162231 -4.3080044 -4.2918844 -4.275681 -4.266881 -4.2684855 -4.27783 -4.2902384 -4.2939515 -4.2909946 -4.2898617 -4.2967181 -4.3056493 -4.3063989][-4.2960305 -4.2898731 -4.2738662 -4.2490215 -4.2254004 -4.21285 -4.2139254 -4.2280421 -4.2492876 -4.2611146 -4.2617941 -4.2628803 -4.27228 -4.282836 -4.2846727][-4.2725439 -4.259511 -4.2287574 -4.1903396 -4.1637621 -4.1537418 -4.1546264 -4.1683493 -4.1969943 -4.2208471 -4.2294583 -4.2360249 -4.2468667 -4.2575521 -4.263196][-4.252656 -4.2315979 -4.1831574 -4.1295342 -4.1024928 -4.1024413 -4.1080184 -4.1190181 -4.1503577 -4.1828327 -4.198853 -4.2074966 -4.2207322 -4.2309947 -4.2402258][-4.2363887 -4.2023034 -4.1387854 -4.0744395 -4.052887 -4.0700636 -4.0868535 -4.0969806 -4.1261096 -4.1597342 -4.17427 -4.177844 -4.19124 -4.2031903 -4.21494][-4.216125 -4.1698256 -4.1062961 -4.0487614 -4.0383997 -4.0644717 -4.0855236 -4.09385 -4.1167536 -4.1477246 -4.163033 -4.1640844 -4.1744866 -4.1852489 -4.1959724][-4.1891212 -4.1416016 -4.0978861 -4.0610528 -4.05367 -4.0700421 -4.0847597 -4.087749 -4.1036024 -4.1350222 -4.1577568 -4.1616859 -4.166719 -4.1720619 -4.1812382][-4.1587162 -4.1269026 -4.1121216 -4.0966063 -4.0870628 -4.091466 -4.0950074 -4.0924563 -4.1024213 -4.1313457 -4.154346 -4.156723 -4.1589828 -4.1639447 -4.1755013][-4.1379075 -4.1234469 -4.1303792 -4.1333013 -4.1275253 -4.1251354 -4.1227274 -4.1196961 -4.13125 -4.1571417 -4.17211 -4.1691704 -4.1708908 -4.1772809 -4.1880736][-4.1374049 -4.1313105 -4.1478853 -4.1619 -4.161489 -4.1583195 -4.1561604 -4.1555452 -4.1685596 -4.1900735 -4.1962209 -4.1911526 -4.1948791 -4.2036667 -4.2134066][-4.1566124 -4.1520371 -4.1697736 -4.1868639 -4.1889262 -4.18772 -4.1918907 -4.1966972 -4.2071552 -4.219614 -4.216733 -4.2099223 -4.2149291 -4.2238903 -4.2352762][-4.1842861 -4.1793952 -4.195302 -4.2120185 -4.2157521 -4.2200751 -4.2309389 -4.2390423 -4.2445817 -4.2478456 -4.2381148 -4.2278919 -4.2299318 -4.2381139 -4.2493863][-4.2195644 -4.2160735 -4.2278914 -4.2407637 -4.2458429 -4.2540417 -4.2658553 -4.272151 -4.2730417 -4.2723074 -4.2635555 -4.2558031 -4.2582788 -4.2652049 -4.2736864]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 01:21:10.555260: step 60010, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 68h:35m:59s remains)
INFO - root - 2017-12-06 01:21:20.089537: step 60020, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.914 sec/batch; 69h:11m:27s remains)
INFO - root - 2017-12-06 01:21:29.149009: step 60030, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 63h:14m:02s remains)
INFO - root - 2017-12-06 01:21:38.505007: step 60040, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 69h:20m:28s remains)
INFO - root - 2017-12-06 01:21:48.164654: step 60050, loss = 2.04, batch loss = 1.98 (8.0 examples/sec; 0.996 sec/batch; 75h:24m:24s remains)
INFO - root - 2017-12-06 01:21:57.374519: step 60060, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 72h:16m:23s remains)
INFO - root - 2017-12-06 01:22:06.813602: step 60070, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.923 sec/batch; 69h:50m:28s remains)
INFO - root - 2017-12-06 01:22:16.053543: step 60080, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 69h:30m:28s remains)
INFO - root - 2017-12-06 01:22:25.369135: step 60090, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 71h:11m:34s remains)
INFO - root - 2017-12-06 01:22:34.917298: step 60100, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 71h:10m:19s remains)
2017-12-06 01:22:35.691335: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3457146 -4.3503222 -4.3522205 -4.3514457 -4.3489714 -4.33723 -4.3232546 -4.3237844 -4.3215332 -4.3100438 -4.3020539 -4.294476 -4.2852163 -4.2832193 -4.2898512][-4.327692 -4.3358259 -4.341485 -4.3435774 -4.3433614 -4.3332958 -4.31997 -4.322948 -4.3248577 -4.3146234 -4.3073163 -4.2990594 -4.2884612 -4.2848072 -4.2893848][-4.3007226 -4.3127661 -4.3226619 -4.32863 -4.3303256 -4.32075 -4.3066335 -4.3123283 -4.3202958 -4.3133831 -4.307291 -4.2985816 -4.2853813 -4.2800927 -4.2835503][-4.2584982 -4.27528 -4.2889585 -4.2970753 -4.2986975 -4.2867427 -4.2688351 -4.2772346 -4.2921085 -4.2911077 -4.288249 -4.2815142 -4.268548 -4.2643118 -4.2698884][-4.19844 -4.2194314 -4.2375617 -4.2460527 -4.2440028 -4.2273946 -4.2031488 -4.2118888 -4.2331934 -4.2406659 -4.2439933 -4.2437897 -4.23528 -4.2346411 -4.2454548][-4.1397171 -4.1592636 -4.1766019 -4.1796513 -4.1687975 -4.1440597 -4.1100721 -4.1153193 -4.1441422 -4.165062 -4.1807418 -4.1923952 -4.1914043 -4.1962886 -4.2128987][-4.1232772 -4.1297889 -4.1338444 -4.1200871 -4.0917611 -4.0506477 -3.9981148 -3.9919686 -4.0265455 -4.0669208 -4.102767 -4.1325178 -4.1429172 -4.1544838 -4.1779895][-4.1468496 -4.1325779 -4.11584 -4.0798516 -4.0309258 -3.9730504 -3.9027932 -3.8832905 -3.9216135 -3.9789488 -4.0334659 -4.0833006 -4.1073031 -4.1242704 -4.1541991][-4.1795545 -4.1480069 -4.1159134 -4.067503 -4.0124931 -3.9542079 -3.8839226 -3.8605869 -3.8966663 -3.9562173 -4.0136471 -4.0688143 -4.0975723 -4.1161728 -4.1493134][-4.2186422 -4.1815839 -4.14813 -4.1073408 -4.066277 -4.0263147 -3.9765797 -3.9605339 -3.9844582 -4.025424 -4.0681171 -4.1112251 -4.1327233 -4.1452541 -4.1734815][-4.2570815 -4.2302723 -4.2074108 -4.1841335 -4.1640725 -4.1436234 -4.1123137 -4.09985 -4.1124177 -4.1363239 -4.1636906 -4.1910381 -4.200428 -4.202672 -4.2202826][-4.2783241 -4.2716928 -4.2665868 -4.2617416 -4.2603097 -4.2538257 -4.2335973 -4.2232761 -4.2281146 -4.241116 -4.2581625 -4.2735686 -4.2740641 -4.2684455 -4.2751985][-4.276649 -4.2869172 -4.295485 -4.3017769 -4.3103309 -4.3117533 -4.2996554 -4.2933645 -4.2985954 -4.30862 -4.3215504 -4.3312306 -4.3275247 -4.3172812 -4.3157606][-4.2539911 -4.2742395 -4.2889085 -4.2973995 -4.3067222 -4.3087425 -4.2998457 -4.2987819 -4.3087211 -4.32143 -4.33664 -4.3466434 -4.3431988 -4.3344026 -4.3309231][-4.2192345 -4.2440634 -4.2611747 -4.2675962 -4.2738972 -4.2727361 -4.2625551 -4.2640977 -4.2787356 -4.2964873 -4.3165584 -4.3302813 -4.3302078 -4.3257556 -4.32636]]...]
INFO - root - 2017-12-06 01:22:45.096246: step 60110, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 69h:02m:45s remains)
INFO - root - 2017-12-06 01:22:54.413416: step 60120, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 68h:23m:49s remains)
INFO - root - 2017-12-06 01:23:03.836941: step 60130, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.939 sec/batch; 71h:00m:25s remains)
INFO - root - 2017-12-06 01:23:13.226686: step 60140, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 69h:57m:25s remains)
INFO - root - 2017-12-06 01:23:22.735740: step 60150, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 72h:58m:35s remains)
INFO - root - 2017-12-06 01:23:32.164994: step 60160, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.938 sec/batch; 70h:57m:19s remains)
INFO - root - 2017-12-06 01:23:41.586487: step 60170, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 72h:58m:57s remains)
INFO - root - 2017-12-06 01:23:51.074247: step 60180, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 70h:14m:48s remains)
INFO - root - 2017-12-06 01:24:00.498488: step 60190, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 72h:23m:59s remains)
INFO - root - 2017-12-06 01:24:09.828535: step 60200, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.015 sec/batch; 76h:46m:47s remains)
2017-12-06 01:24:10.603683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2934151 -4.2941113 -4.2941251 -4.2913051 -4.2899628 -4.290885 -4.2895012 -4.2876949 -4.2877536 -4.2921185 -4.2983212 -4.3045797 -4.3115039 -4.315136 -4.3140588][-4.2836742 -4.288147 -4.2900257 -4.2879467 -4.286283 -4.2872877 -4.282907 -4.2781105 -4.277679 -4.2850428 -4.2948532 -4.3036757 -4.3111362 -4.3129134 -4.3082557][-4.27022 -4.2794371 -4.2837944 -4.2822738 -4.2796588 -4.27726 -4.2665644 -4.2586579 -4.2596817 -4.2709427 -4.284431 -4.2967896 -4.304759 -4.3043327 -4.2957664][-4.2467813 -4.2574763 -4.2606373 -4.2571177 -4.2515049 -4.243392 -4.2263851 -4.2168169 -4.2220287 -4.2413964 -4.2611184 -4.2781439 -4.287549 -4.2854967 -4.2741776][-4.2141657 -4.2198257 -4.2186894 -4.2101679 -4.1977329 -4.1818991 -4.1610837 -4.1567659 -4.16901 -4.1988029 -4.2285085 -4.2507453 -4.2629657 -4.25952 -4.2470584][-4.1686478 -4.167738 -4.1592178 -4.141295 -4.1189938 -4.0922389 -4.0651169 -4.0650864 -4.0865726 -4.1301064 -4.1743393 -4.2039065 -4.2211885 -4.2214117 -4.2131371][-4.124773 -4.1104064 -4.0881176 -4.0581641 -4.0264273 -3.9882216 -3.9511092 -3.9516163 -3.9859128 -4.0447383 -4.1017656 -4.1409869 -4.1667895 -4.1771388 -4.1815038][-4.1128187 -4.0877013 -4.0512381 -4.0048046 -3.9631596 -3.9152939 -3.8717866 -3.8751743 -3.9264419 -3.9966173 -4.0571623 -4.0969715 -4.1227832 -4.1398177 -4.1557865][-4.1353669 -4.1151443 -4.0847259 -4.0431662 -3.9989648 -3.9442186 -3.8944044 -3.8948002 -3.9464111 -4.0119743 -4.0673432 -4.0990605 -4.1160583 -4.1286378 -4.1439934][-4.1665325 -4.1580172 -4.1416807 -4.1148782 -4.0766163 -4.0237746 -3.9721384 -3.9600422 -3.9938157 -4.0458736 -4.0900111 -4.1160245 -4.1294322 -4.135272 -4.1446114][-4.1875558 -4.1906171 -4.1824241 -4.1618662 -4.1303196 -4.0866313 -4.0420394 -4.027791 -4.0455942 -4.0784287 -4.1090031 -4.1313319 -4.1443624 -4.1488228 -4.1560936][-4.1939445 -4.2025728 -4.2002945 -4.1873827 -4.1674347 -4.1364646 -4.1002793 -4.0860276 -4.0920138 -4.1101661 -4.1293049 -4.1468525 -4.1586871 -4.164629 -4.1733685][-4.2053094 -4.2144165 -4.2138925 -4.2036142 -4.189455 -4.1685452 -4.1444883 -4.1346822 -4.1368346 -4.1502609 -4.1662655 -4.18293 -4.1933122 -4.2007389 -4.2085495][-4.2355995 -4.2443023 -4.2437081 -4.2352276 -4.224905 -4.2119727 -4.1974707 -4.192431 -4.1956882 -4.2099404 -4.2261443 -4.2416339 -4.2493858 -4.2538376 -4.2578559][-4.2699223 -4.2785621 -4.2799053 -4.2752471 -4.2694025 -4.2621713 -4.2533574 -4.2510052 -4.2538128 -4.2649665 -4.2777452 -4.2894731 -4.29403 -4.2954888 -4.2970929]]...]
INFO - root - 2017-12-06 01:24:20.142644: step 60210, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 72h:02m:48s remains)
INFO - root - 2017-12-06 01:24:29.465495: step 60220, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.902 sec/batch; 68h:15m:28s remains)
INFO - root - 2017-12-06 01:24:38.934632: step 60230, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 70h:42m:44s remains)
INFO - root - 2017-12-06 01:24:48.108878: step 60240, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 71h:06m:04s remains)
INFO - root - 2017-12-06 01:24:57.277210: step 60250, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 72h:13m:27s remains)
INFO - root - 2017-12-06 01:25:06.844090: step 60260, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.012 sec/batch; 76h:33m:05s remains)
INFO - root - 2017-12-06 01:25:15.991630: step 60270, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 71h:55m:43s remains)
INFO - root - 2017-12-06 01:25:25.298646: step 60280, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.947 sec/batch; 71h:36m:23s remains)
INFO - root - 2017-12-06 01:25:34.591622: step 60290, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 70h:05m:47s remains)
INFO - root - 2017-12-06 01:25:43.954586: step 60300, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 73h:05m:29s remains)
2017-12-06 01:25:44.713397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2184591 -4.2274156 -4.2283773 -4.2293372 -4.2311511 -4.2212224 -4.1844912 -4.1406336 -4.1451941 -4.1842623 -4.1925793 -4.1809182 -4.1898818 -4.2076468 -4.2027678][-4.23501 -4.2474818 -4.2551074 -4.2612095 -4.2596693 -4.2436342 -4.2021394 -4.1527891 -4.1552706 -4.1908612 -4.2018909 -4.1897764 -4.1983309 -4.2144165 -4.2103596][-4.23952 -4.2584891 -4.27477 -4.2844763 -4.2811637 -4.2619872 -4.2151361 -4.1590652 -4.1600137 -4.1945744 -4.2068915 -4.1974969 -4.2040882 -4.2139044 -4.2095251][-4.2405367 -4.2598782 -4.2790332 -4.2894263 -4.2828922 -4.2605681 -4.2046609 -4.1414657 -4.1439724 -4.1842465 -4.2027636 -4.2022519 -4.2090073 -4.2154236 -4.2116389][-4.2150688 -4.2314625 -4.2541304 -4.2669592 -4.2613792 -4.2359395 -4.167479 -4.0901427 -4.1061082 -4.1643643 -4.1915355 -4.1986847 -4.2088089 -4.2136555 -4.2099133][-4.1620111 -4.1759157 -4.2083893 -4.2334185 -4.2283649 -4.2018838 -4.1162472 -4.0124187 -4.0476184 -4.1359043 -4.1751986 -4.188745 -4.1990175 -4.1994276 -4.1911039][-4.0994334 -4.1105633 -4.1551332 -4.1968131 -4.1917405 -4.1543183 -4.0409508 -3.8963754 -3.9616923 -4.0890517 -4.1399889 -4.1634994 -4.1798992 -4.1821346 -4.1759768][-4.0632048 -4.0721211 -4.1263146 -4.1764312 -4.1737013 -4.1277785 -3.9989123 -3.832093 -3.9199486 -4.0659056 -4.1148334 -4.1456232 -4.1742058 -4.1829605 -4.1828146][-4.0761237 -4.0858994 -4.1405497 -4.1909962 -4.1969767 -4.1638513 -4.0531874 -3.9249692 -3.9994824 -4.1135192 -4.14003 -4.1611609 -4.1947231 -4.2078605 -4.2061973][-4.1092033 -4.1216063 -4.1724167 -4.2183619 -4.2309446 -4.2114353 -4.1261315 -4.0332208 -4.0886078 -4.170105 -4.1798882 -4.1908684 -4.2181988 -4.2258773 -4.2194581][-4.1390162 -4.1495905 -4.1930966 -4.23221 -4.2467074 -4.2344108 -4.1689453 -4.1007719 -4.139904 -4.1991496 -4.2029982 -4.2111096 -4.2329311 -4.2358589 -4.2292037][-4.1790118 -4.1837134 -4.2140832 -4.2433496 -4.2562003 -4.2428079 -4.1890497 -4.1407146 -4.1693573 -4.2090359 -4.2094221 -4.218441 -4.2388711 -4.2408614 -4.2345777][-4.2105742 -4.2108932 -4.2291884 -4.2460074 -4.2556467 -4.2419682 -4.1944475 -4.1602263 -4.1871734 -4.2144566 -4.2108688 -4.2170415 -4.232883 -4.235507 -4.22797][-4.2090836 -4.2071314 -4.2249508 -4.236371 -4.2464533 -4.2331586 -4.1869431 -4.1637282 -4.1921716 -4.2120366 -4.2064748 -4.2075515 -4.21602 -4.2202148 -4.2131653][-4.18084 -4.1786637 -4.1980243 -4.2087116 -4.2208447 -4.2119117 -4.1649022 -4.1479607 -4.1781197 -4.1938457 -4.186101 -4.186214 -4.1936274 -4.2015553 -4.1944919]]...]
INFO - root - 2017-12-06 01:25:54.114112: step 60310, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 71h:30m:44s remains)
INFO - root - 2017-12-06 01:26:03.474680: step 60320, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.913 sec/batch; 69h:03m:17s remains)
INFO - root - 2017-12-06 01:26:12.681423: step 60330, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 63h:22m:06s remains)
INFO - root - 2017-12-06 01:26:22.237732: step 60340, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 74h:51m:20s remains)
INFO - root - 2017-12-06 01:26:31.704137: step 60350, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 70h:30m:17s remains)
INFO - root - 2017-12-06 01:26:41.015000: step 60360, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.924 sec/batch; 69h:53m:05s remains)
INFO - root - 2017-12-06 01:26:50.495492: step 60370, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 71h:05m:32s remains)
INFO - root - 2017-12-06 01:26:59.934225: step 60380, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 69h:15m:37s remains)
INFO - root - 2017-12-06 01:27:09.240209: step 60390, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 66h:43m:36s remains)
INFO - root - 2017-12-06 01:27:18.624207: step 60400, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 70h:33m:33s remains)
2017-12-06 01:27:19.457005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1942291 -4.1805139 -4.1845479 -4.2033992 -4.2204137 -4.2250485 -4.2230072 -4.2081757 -4.1798677 -4.1533971 -4.1544809 -4.174664 -4.17495 -4.1707411 -4.1987953][-4.2356381 -4.21952 -4.2216148 -4.2326245 -4.244535 -4.243197 -4.2306681 -4.2084255 -4.1865463 -4.1733212 -4.1777148 -4.1903138 -4.1846237 -4.1808548 -4.2122469][-4.2507973 -4.2341237 -4.2336802 -4.2415767 -4.2484336 -4.2422686 -4.2254543 -4.2037148 -4.1924238 -4.1959162 -4.2108617 -4.2187052 -4.2043405 -4.1954908 -4.230535][-4.2337441 -4.2179661 -4.2176275 -4.2191429 -4.2176204 -4.2064486 -4.1878433 -4.1758966 -4.1795859 -4.1976738 -4.2241435 -4.2342262 -4.2150121 -4.2016716 -4.2357693][-4.2091174 -4.18655 -4.1805339 -4.1719227 -4.16093 -4.141573 -4.1173921 -4.11621 -4.1484962 -4.1867871 -4.2208595 -4.2355418 -4.2186723 -4.201612 -4.22864][-4.1923203 -4.1633787 -4.1433568 -4.1164827 -4.0909133 -4.0615358 -4.0231175 -4.0251679 -4.0984869 -4.1671681 -4.2102308 -4.2304029 -4.2175727 -4.1965556 -4.2131267][-4.1795115 -4.1485209 -4.1171918 -4.0749269 -4.0393028 -3.9944768 -3.9240179 -3.9124959 -4.0332203 -4.1398582 -4.1976395 -4.2237544 -4.213119 -4.1937785 -4.2016878][-4.1577549 -4.1284156 -4.0975561 -4.0528936 -4.0205579 -3.9725823 -3.8888865 -3.8620903 -4.0006008 -4.1216393 -4.1865854 -4.2158909 -4.2064672 -4.187923 -4.1913047][-4.1242743 -4.0911245 -4.0746861 -4.0498 -4.0363851 -4.0082073 -3.960017 -3.9517734 -4.0485144 -4.1411633 -4.1931057 -4.2147574 -4.2031121 -4.1823153 -4.182138][-4.0981793 -4.067512 -4.0712471 -4.0748 -4.0775895 -4.0657759 -4.0518856 -4.0574641 -4.1116285 -4.16879 -4.2008018 -4.2141757 -4.2009535 -4.1809497 -4.1803913][-4.097291 -4.0799589 -4.0987368 -4.1177816 -4.1268506 -4.1246357 -4.1288557 -4.1376419 -4.1629233 -4.1910648 -4.2081289 -4.2173405 -4.2051268 -4.1875515 -4.1867733][-4.1067238 -4.106884 -4.1365523 -4.1614509 -4.1715693 -4.1739888 -4.1892929 -4.196672 -4.2016282 -4.2077847 -4.2124877 -4.2175279 -4.20663 -4.1927071 -4.1899729][-4.1213617 -4.1275711 -4.1638165 -4.1946344 -4.2056413 -4.2111621 -4.2309809 -4.2387438 -4.2307796 -4.218986 -4.2081513 -4.2068157 -4.2010436 -4.1937542 -4.1913762][-4.14547 -4.146956 -4.1784716 -4.2076669 -4.22069 -4.2287307 -4.24678 -4.2520084 -4.2369485 -4.213367 -4.1899076 -4.1832395 -4.1840458 -4.189889 -4.1965613][-4.1686692 -4.1556392 -4.17505 -4.2033696 -4.2222772 -4.2369766 -4.2552371 -4.2528977 -4.2270627 -4.1954761 -4.1689897 -4.1643033 -4.1731925 -4.1940951 -4.2148461]]...]
INFO - root - 2017-12-06 01:27:28.886838: step 60410, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 69h:40m:26s remains)
INFO - root - 2017-12-06 01:27:38.206029: step 60420, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 69h:40m:33s remains)
INFO - root - 2017-12-06 01:27:47.501813: step 60430, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 68h:55m:38s remains)
INFO - root - 2017-12-06 01:27:56.887074: step 60440, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 70h:27m:07s remains)
INFO - root - 2017-12-06 01:28:06.285339: step 60450, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 71h:14m:34s remains)
INFO - root - 2017-12-06 01:28:15.583699: step 60460, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.914 sec/batch; 69h:02m:27s remains)
INFO - root - 2017-12-06 01:28:24.958059: step 60470, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 71h:48m:31s remains)
INFO - root - 2017-12-06 01:28:34.054985: step 60480, loss = 2.05, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 75h:22m:38s remains)
INFO - root - 2017-12-06 01:28:43.352031: step 60490, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 70h:06m:26s remains)
INFO - root - 2017-12-06 01:28:52.836789: step 60500, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 72h:28m:05s remains)
2017-12-06 01:28:53.656340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.308485 -4.2965384 -4.268837 -4.2283473 -4.1812506 -4.1264343 -4.0978975 -4.1117468 -4.1565232 -4.2033744 -4.2319961 -4.2488842 -4.2679214 -4.2861342 -4.2955389][-4.3199573 -4.3071494 -4.2702427 -4.2149229 -4.1539173 -4.0942974 -4.0721588 -4.0908451 -4.1402435 -4.1928363 -4.2238665 -4.2443032 -4.2656918 -4.2865019 -4.2984271][-4.3207693 -4.3060088 -4.2621613 -4.1927328 -4.118258 -4.0515242 -4.0320282 -4.0589542 -4.1179876 -4.1783147 -4.2143927 -4.2376227 -4.2607479 -4.2807546 -4.2944446][-4.3181815 -4.2993045 -4.25117 -4.1735897 -4.0850134 -4.0034089 -3.9820132 -4.02298 -4.0965939 -4.1637464 -4.2068496 -4.2318335 -4.2534151 -4.270618 -4.2833967][-4.3079786 -4.2894249 -4.2448335 -4.164176 -4.0603008 -3.9597979 -3.9338021 -3.9931948 -4.083602 -4.1570721 -4.2069693 -4.2325826 -4.2499146 -4.262485 -4.2713046][-4.2959805 -4.2818532 -4.2441196 -4.1644759 -4.047555 -3.9231315 -3.8880205 -3.9649606 -4.07784 -4.1596932 -4.2117405 -4.2347512 -4.2461047 -4.2527428 -4.255908][-4.2849264 -4.2752447 -4.2455087 -4.17345 -4.0490737 -3.9010994 -3.8459053 -3.9339414 -4.0699029 -4.1628466 -4.2138758 -4.229877 -4.2317486 -4.2316422 -4.2298617][-4.2696643 -4.2642345 -4.2493424 -4.1957688 -4.0810785 -3.9295273 -3.8553379 -3.9357767 -4.0758805 -4.1686792 -4.2120852 -4.2164993 -4.2074437 -4.2016811 -4.1987782][-4.2492428 -4.2478719 -4.250319 -4.2199321 -4.1345506 -4.002986 -3.9207482 -3.9781997 -4.0981674 -4.1785374 -4.2081003 -4.2061453 -4.1935897 -4.1850734 -4.1828222][-4.228303 -4.2264433 -4.2405543 -4.2312379 -4.1784782 -4.0776157 -4.0022125 -4.0310597 -4.1158781 -4.1747651 -4.1888742 -4.1840606 -4.178112 -4.1785436 -4.1868348][-4.2154627 -4.2079124 -4.2240615 -4.2260137 -4.1993246 -4.1322784 -4.0669427 -4.0659351 -4.1133833 -4.1474791 -4.1499767 -4.1487947 -4.1587315 -4.1794052 -4.2018123][-4.213902 -4.1989536 -4.2137594 -4.2271075 -4.2207584 -4.1793375 -4.1240144 -4.1006341 -4.1158156 -4.1303368 -4.13118 -4.1374049 -4.162117 -4.1931162 -4.2226624][-4.2166977 -4.1977253 -4.2118549 -4.234036 -4.2423053 -4.2235446 -4.1832538 -4.151372 -4.1454744 -4.1500754 -4.14911 -4.1559095 -4.1806588 -4.2100754 -4.2379103][-4.2143917 -4.1982303 -4.2135034 -4.238687 -4.2568359 -4.2533979 -4.2285147 -4.2003527 -4.1856728 -4.1834431 -4.1782894 -4.1804233 -4.1999984 -4.22544 -4.2483878][-4.2108178 -4.197329 -4.2144513 -4.246459 -4.2733393 -4.27987 -4.2669291 -4.2455068 -4.2293739 -4.2231278 -4.2155361 -4.2136536 -4.2266388 -4.2467618 -4.2658925]]...]
INFO - root - 2017-12-06 01:29:02.925856: step 60510, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.014 sec/batch; 76h:35m:55s remains)
INFO - root - 2017-12-06 01:29:12.576230: step 60520, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 73h:03m:04s remains)
INFO - root - 2017-12-06 01:29:22.001483: step 60530, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 69h:51m:12s remains)
INFO - root - 2017-12-06 01:29:31.237242: step 60540, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 70h:30m:09s remains)
INFO - root - 2017-12-06 01:29:40.671636: step 60550, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 71h:53m:07s remains)
INFO - root - 2017-12-06 01:29:50.111133: step 60560, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 75h:16m:13s remains)
INFO - root - 2017-12-06 01:29:59.213562: step 60570, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 63h:50m:43s remains)
INFO - root - 2017-12-06 01:30:08.754495: step 60580, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 73h:07m:50s remains)
INFO - root - 2017-12-06 01:30:18.052951: step 60590, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 72h:49m:03s remains)
INFO - root - 2017-12-06 01:30:27.587107: step 60600, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 73h:24m:28s remains)
2017-12-06 01:30:28.442070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2695022 -4.2682133 -4.2595458 -4.243217 -4.2338839 -4.2334547 -4.237659 -4.2429776 -4.2517028 -4.257246 -4.2534418 -4.2494059 -4.2400241 -4.22883 -4.2330756][-4.2731032 -4.2713723 -4.2634168 -4.245501 -4.2301903 -4.2227855 -4.2194948 -4.2196593 -4.2280197 -4.2425079 -4.2550159 -4.2618155 -4.2597876 -4.2529297 -4.2547312][-4.2666421 -4.2645164 -4.2578678 -4.2401161 -4.2200189 -4.2056403 -4.1944447 -4.1876631 -4.1957588 -4.2182393 -4.2428117 -4.258924 -4.2661228 -4.2672477 -4.2695808][-4.2502508 -4.2440858 -4.2369637 -4.2212596 -4.1990542 -4.1777411 -4.15784 -4.1432152 -4.1473804 -4.1739144 -4.2085667 -4.236052 -4.2512407 -4.2610192 -4.2667036][-4.2409949 -4.2252669 -4.2125335 -4.1982822 -4.1730423 -4.1392589 -4.1005411 -4.0671644 -4.0593715 -4.0865116 -4.1334996 -4.1818652 -4.2138314 -4.2371 -4.2483592][-4.2477174 -4.2252145 -4.2092152 -4.19253 -4.1588368 -4.1045952 -4.0361948 -3.9648407 -3.9221683 -3.9413643 -4.0117793 -4.0970583 -4.1541986 -4.1921916 -4.209415][-4.2671819 -4.2425871 -4.2240133 -4.2066116 -4.1706314 -4.1018867 -4.0062451 -3.8872869 -3.7849398 -3.7712712 -3.8605638 -3.9888666 -4.074194 -4.1302409 -4.1588507][-4.2856259 -4.263061 -4.2436481 -4.2280669 -4.201149 -4.1430182 -4.0554833 -3.9317403 -3.8132308 -3.7737205 -3.8406322 -3.9603903 -4.0415363 -4.0941415 -4.1231489][-4.2992144 -4.2815847 -4.2632542 -4.2498865 -4.2381282 -4.2087746 -4.1565156 -4.07377 -3.9933686 -3.9592893 -3.9825244 -4.0459142 -4.085762 -4.109375 -4.125073][-4.3118024 -4.3001361 -4.2869849 -4.2791166 -4.2793846 -4.2704544 -4.2485676 -4.2042089 -4.1590962 -4.133 -4.1278782 -4.1477914 -4.152391 -4.1473107 -4.1470613][-4.3230715 -4.3168273 -4.3079133 -4.3029261 -4.3080297 -4.3065877 -4.2989159 -4.2739739 -4.2493215 -4.2318583 -4.220829 -4.2227244 -4.2142873 -4.19882 -4.1913252][-4.3307128 -4.3283272 -4.3218675 -4.3154683 -4.3161988 -4.3140731 -4.30694 -4.2892609 -4.2729564 -4.2618246 -4.2547603 -4.2574167 -4.2540841 -4.2437315 -4.2395153][-4.3327169 -4.3312416 -4.3272777 -4.3193369 -4.3131351 -4.3051419 -4.2947106 -4.2777719 -4.2607322 -4.2500172 -4.2476797 -4.2588453 -4.2692857 -4.2709084 -4.274941][-4.332046 -4.3292875 -4.3257384 -4.3177881 -4.3072472 -4.2948422 -4.2801971 -4.2604327 -4.2390442 -4.2256207 -4.2266173 -4.2444654 -4.26691 -4.2814116 -4.2921114][-4.3308449 -4.3283029 -4.3256259 -4.3203306 -4.309598 -4.2942667 -4.2741809 -4.25183 -4.2267475 -4.2101483 -4.2127547 -4.2336111 -4.26084 -4.2813993 -4.2950683]]...]
INFO - root - 2017-12-06 01:30:37.983419: step 60610, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 71h:23m:43s remains)
INFO - root - 2017-12-06 01:30:47.342279: step 60620, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 71h:21m:28s remains)
INFO - root - 2017-12-06 01:30:56.241903: step 60630, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 71h:34m:51s remains)
INFO - root - 2017-12-06 01:31:05.894060: step 60640, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 73h:51m:50s remains)
INFO - root - 2017-12-06 01:31:15.374801: step 60650, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.992 sec/batch; 74h:54m:49s remains)
INFO - root - 2017-12-06 01:31:24.808374: step 60660, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 72h:53m:41s remains)
INFO - root - 2017-12-06 01:31:34.324974: step 60670, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 71h:07m:29s remains)
INFO - root - 2017-12-06 01:31:43.718014: step 60680, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 75h:25m:47s remains)
INFO - root - 2017-12-06 01:31:52.917255: step 60690, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 65h:59m:07s remains)
INFO - root - 2017-12-06 01:32:02.338457: step 60700, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.781 sec/batch; 58h:56m:54s remains)
2017-12-06 01:32:03.077560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3024912 -4.3023696 -4.3040872 -4.3050451 -4.3056669 -4.3059349 -4.3057556 -4.3057714 -4.3065372 -4.3078713 -4.3088241 -4.3084717 -4.3069711 -4.3052688 -4.3040113][-4.2806478 -4.2806449 -4.2843909 -4.2873454 -4.2892594 -4.2899194 -4.2892628 -4.2889175 -4.2902112 -4.2925611 -4.2938762 -4.2927256 -4.2902741 -4.2887154 -4.2878594][-4.2360735 -4.2363696 -4.2422953 -4.2475247 -4.250134 -4.2504797 -4.2487888 -4.2473803 -4.24885 -4.252284 -4.25411 -4.2524033 -4.249228 -4.2481346 -4.248795][-4.17691 -4.1785 -4.1863103 -4.1927228 -4.1949296 -4.1946664 -4.1918316 -4.1886029 -4.1895528 -4.1933427 -4.1945534 -4.1916695 -4.1882033 -4.188345 -4.1919312][-4.128736 -4.1335611 -4.1415038 -4.1466231 -4.1472864 -4.1464028 -4.143075 -4.1386571 -4.1387105 -4.1410522 -4.1407104 -4.1367106 -4.1340084 -4.1373796 -4.1459107][-4.1110086 -4.12044 -4.1267371 -4.1281466 -4.1253424 -4.1233325 -4.1198497 -4.1156774 -4.1148133 -4.1143169 -4.1117034 -4.1097345 -4.1126165 -4.1227679 -4.1375747][-4.1111584 -4.1234932 -4.1255941 -4.1224117 -4.1164403 -4.1153808 -4.1168356 -4.1180568 -4.1199312 -4.1193509 -4.1166592 -4.119576 -4.1307931 -4.14716 -4.1655][-4.1284881 -4.140255 -4.1363888 -4.1290278 -4.1221743 -4.1250758 -4.1342278 -4.1445465 -4.153893 -4.1578026 -4.1589403 -4.1664972 -4.1819811 -4.198947 -4.2150054][-4.1576934 -4.16843 -4.16204 -4.152493 -4.1445541 -4.1499615 -4.1644454 -4.1826248 -4.198688 -4.2066031 -4.2105603 -4.2200956 -4.2351875 -4.2488575 -4.2594862][-4.1885972 -4.20109 -4.1949449 -4.1830363 -4.1717086 -4.1754665 -4.1911364 -4.2133026 -4.2336783 -4.24433 -4.24916 -4.2580137 -4.2704558 -4.2804003 -4.2874365][-4.2157559 -4.2304497 -4.22398 -4.2078013 -4.1920648 -4.1932082 -4.2087564 -4.2327466 -4.2546768 -4.2657771 -4.2699461 -4.2768583 -4.2869377 -4.2954159 -4.3020053][-4.229917 -4.2448258 -4.2368851 -4.2181334 -4.2030191 -4.2055659 -4.2226553 -4.246923 -4.26723 -4.2768064 -4.2800341 -4.28482 -4.29215 -4.2992673 -4.3058][-4.2374473 -4.2471766 -4.2363095 -4.2190428 -4.2102108 -4.2181029 -4.2374272 -4.2592325 -4.2754707 -4.282546 -4.2843995 -4.2870049 -4.2922258 -4.2983284 -4.3050284][-4.2409196 -4.2419109 -4.2299628 -4.218822 -4.2185626 -4.2321029 -4.2531853 -4.2719946 -4.2838926 -4.2884288 -4.288672 -4.2897668 -4.2931418 -4.2978392 -4.3038344][-4.2352018 -4.2321005 -4.2268176 -4.2266259 -4.2333155 -4.2482281 -4.2678528 -4.2823086 -4.2901 -4.2926989 -4.2921214 -4.2927656 -4.2948608 -4.2979779 -4.3022771]]...]
INFO - root - 2017-12-06 01:32:12.670342: step 60710, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 73h:26m:17s remains)
INFO - root - 2017-12-06 01:32:22.134710: step 60720, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.004 sec/batch; 75h:49m:48s remains)
INFO - root - 2017-12-06 01:32:31.602887: step 60730, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 71h:21m:29s remains)
INFO - root - 2017-12-06 01:32:40.828664: step 60740, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 72h:23m:24s remains)
INFO - root - 2017-12-06 01:32:50.074143: step 60750, loss = 2.03, batch loss = 1.97 (8.2 examples/sec; 0.980 sec/batch; 73h:56m:38s remains)
INFO - root - 2017-12-06 01:32:59.291647: step 60760, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 69h:38m:49s remains)
INFO - root - 2017-12-06 01:33:08.754493: step 60770, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 70h:56m:50s remains)
INFO - root - 2017-12-06 01:33:18.077661: step 60780, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 65h:34m:35s remains)
INFO - root - 2017-12-06 01:33:27.340045: step 60790, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 69h:47m:43s remains)
INFO - root - 2017-12-06 01:33:36.635992: step 60800, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 69h:50m:59s remains)
2017-12-06 01:33:37.454709: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3181105 -4.3139014 -4.3141379 -4.3178411 -4.3205037 -4.3208795 -4.3173733 -4.3112507 -4.3081021 -4.3062782 -4.3021765 -4.2971783 -4.2935095 -4.2947388 -4.3030176][-4.3107567 -4.3044281 -4.3062816 -4.3118243 -4.315834 -4.3142834 -4.3038797 -4.2907166 -4.2849369 -4.2813859 -4.2744336 -4.2698016 -4.2718997 -4.2793074 -4.2938685][-4.3042226 -4.2933407 -4.292 -4.2957726 -4.2982025 -4.2935057 -4.2728825 -4.2484045 -4.2392564 -4.2358074 -4.227654 -4.2263951 -4.2389112 -4.2571092 -4.2799892][-4.2948513 -4.2761006 -4.2686229 -4.2678547 -4.2658052 -4.2530723 -4.216711 -4.1764746 -4.1619062 -4.1637974 -4.161797 -4.1691136 -4.1966372 -4.2302337 -4.2629557][-4.2846065 -4.2553258 -4.2353272 -4.2226415 -4.2087297 -4.1791372 -4.1193142 -4.0568976 -4.0464764 -4.0705853 -4.0885539 -4.1143942 -4.1566963 -4.1986985 -4.2360444][-4.273531 -4.23196 -4.1954637 -4.164454 -4.1302752 -4.0713658 -3.9711497 -3.8798242 -3.895798 -3.9654362 -4.019238 -4.0660162 -4.1155629 -4.1564393 -4.19383][-4.2692533 -4.2209806 -4.1703463 -4.11708 -4.0522556 -3.9545722 -3.8072336 -3.6900334 -3.7522931 -3.88 -3.9753637 -4.0415173 -4.0939312 -4.1314516 -4.1669211][-4.2757359 -4.2301626 -4.177268 -4.1155019 -4.0403709 -3.9385068 -3.7925014 -3.6930208 -3.7730241 -3.9067678 -4.0097284 -4.0756536 -4.12172 -4.1505318 -4.1812077][-4.28038 -4.2431087 -4.2017179 -4.1553698 -4.09886 -4.0286059 -3.930886 -3.8695588 -3.9302068 -4.022532 -4.0942621 -4.1376624 -4.1662378 -4.189774 -4.2199483][-4.273375 -4.2420273 -4.2156229 -4.1903243 -4.1641774 -4.1341538 -4.0829577 -4.0459843 -4.082952 -4.1388078 -4.178576 -4.1995077 -4.2128544 -4.2320542 -4.2576756][-4.2599735 -4.2281351 -4.2073078 -4.1949196 -4.1908007 -4.1893296 -4.1684685 -4.1459875 -4.167644 -4.2048059 -4.2306576 -4.2395864 -4.2432685 -4.2578917 -4.276895][-4.2595005 -4.2261052 -4.2045522 -4.1967859 -4.2022223 -4.2107887 -4.2031546 -4.187562 -4.2040792 -4.2351069 -4.2557211 -4.2619658 -4.2628908 -4.2737823 -4.2883573][-4.2757363 -4.247467 -4.2281828 -4.2201953 -4.2225122 -4.2268167 -4.2206211 -4.2104688 -4.2233405 -4.2476611 -4.2625918 -4.2658944 -4.2689691 -4.2809486 -4.2966905][-4.295085 -4.2754664 -4.2605982 -4.2538414 -4.2521496 -4.2508883 -4.2449484 -4.2381067 -4.2447047 -4.2578645 -4.2666154 -4.2718921 -4.2794247 -4.2934608 -4.30837][-4.3125539 -4.3010211 -4.2927785 -4.2891507 -4.2879291 -4.2869287 -4.2836895 -4.2797132 -4.2808905 -4.2839274 -4.2876573 -4.2937164 -4.3019671 -4.3123455 -4.3217397]]...]
INFO - root - 2017-12-06 01:33:46.743102: step 60810, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 62h:48m:13s remains)
INFO - root - 2017-12-06 01:33:55.906155: step 60820, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 73h:05m:15s remains)
INFO - root - 2017-12-06 01:34:05.282626: step 60830, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 71h:31m:35s remains)
INFO - root - 2017-12-06 01:34:14.637160: step 60840, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.934 sec/batch; 70h:29m:09s remains)
INFO - root - 2017-12-06 01:34:24.139115: step 60850, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.982 sec/batch; 74h:07m:50s remains)
INFO - root - 2017-12-06 01:34:33.474058: step 60860, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 69h:32m:00s remains)
INFO - root - 2017-12-06 01:34:42.649358: step 60870, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 65h:12m:58s remains)
INFO - root - 2017-12-06 01:34:51.936733: step 60880, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.923 sec/batch; 69h:36m:36s remains)
INFO - root - 2017-12-06 01:35:01.182385: step 60890, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 70h:58m:49s remains)
INFO - root - 2017-12-06 01:35:10.396603: step 60900, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 70h:10m:32s remains)
2017-12-06 01:35:11.157638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1944036 -4.227757 -4.2651658 -4.298667 -4.3240876 -4.334064 -4.3307786 -4.320405 -4.3135242 -4.3021722 -4.285326 -4.2716 -4.258585 -4.2599683 -4.262054][-4.194406 -4.2333288 -4.2716575 -4.2957454 -4.3093681 -4.3089695 -4.2956786 -4.28337 -4.2813134 -4.2813292 -4.274416 -4.2647352 -4.2543387 -4.2577543 -4.2634525][-4.2043552 -4.2400508 -4.2672153 -4.2691331 -4.2605371 -4.2380924 -4.2099524 -4.2024355 -4.2203426 -4.2439122 -4.2539911 -4.2488012 -4.2358012 -4.2363105 -4.2470694][-4.2301087 -4.253068 -4.2563314 -4.226397 -4.1852088 -4.12979 -4.0787067 -4.0845928 -4.139503 -4.1974688 -4.2293897 -4.2268472 -4.2071795 -4.1984706 -4.2118564][-4.2599463 -4.2703538 -4.245924 -4.1818628 -4.1017985 -3.9979916 -3.9136291 -3.9409716 -4.0443473 -4.1457739 -4.206593 -4.2180071 -4.1948633 -4.1766129 -4.1881552][-4.27335 -4.274332 -4.2307 -4.1371918 -4.0122 -3.8483434 -3.7236896 -3.7951086 -3.9635541 -4.1095619 -4.19497 -4.2221317 -4.2011609 -4.1808972 -4.191936][-4.2733355 -4.2722983 -4.2178946 -4.102756 -3.9325209 -3.7021098 -3.5359731 -3.6657605 -3.9027729 -4.0859094 -4.1867504 -4.22694 -4.2129788 -4.1924076 -4.2039638][-4.2708344 -4.2686191 -4.2124944 -4.094173 -3.9110863 -3.667063 -3.5052919 -3.6624207 -3.9082813 -4.0866823 -4.185153 -4.2226748 -4.2071338 -4.1875458 -4.2029314][-4.2715478 -4.270359 -4.2166181 -4.113492 -3.9683585 -3.7920818 -3.6871862 -3.8147612 -3.9993966 -4.1314597 -4.2090726 -4.2334371 -4.2082472 -4.1818748 -4.19497][-4.2696805 -4.271203 -4.2245121 -4.142869 -4.0426664 -3.9454658 -3.8962345 -3.9808466 -4.0992694 -4.1882887 -4.2392077 -4.2450595 -4.2112985 -4.1819611 -4.1923814][-4.2600203 -4.2628117 -4.2313261 -4.1765914 -4.1169181 -4.07127 -4.04966 -4.0968781 -4.1658883 -4.2182784 -4.2454457 -4.2389579 -4.205503 -4.1809397 -4.1926265][-4.2453203 -4.2455053 -4.2247405 -4.191977 -4.165473 -4.1505642 -4.1415687 -4.1629066 -4.1989331 -4.2250218 -4.2344346 -4.2210231 -4.1931481 -4.1763783 -4.1844525][-4.2407036 -4.2358952 -4.2217069 -4.2042565 -4.1978536 -4.1970592 -4.1949191 -4.2061892 -4.2248607 -4.2365232 -4.2373075 -4.2242994 -4.2031469 -4.1873941 -4.1870613][-4.2598429 -4.2574511 -4.2488666 -4.2392735 -4.2369137 -4.2402558 -4.2422123 -4.2490544 -4.2567539 -4.2601647 -4.2586408 -4.2503738 -4.235333 -4.2194767 -4.2135754][-4.2888227 -4.2875361 -4.2826309 -4.276885 -4.27512 -4.2761173 -4.2778697 -4.2824864 -4.2867508 -4.2890186 -4.2896738 -4.2862954 -4.2749333 -4.2608294 -4.251626]]...]
INFO - root - 2017-12-06 01:35:20.393532: step 60910, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 72h:31m:40s remains)
INFO - root - 2017-12-06 01:35:29.578525: step 60920, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 67h:07m:18s remains)
INFO - root - 2017-12-06 01:35:39.016433: step 60930, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 71h:55m:39s remains)
INFO - root - 2017-12-06 01:35:48.082515: step 60940, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 70h:45m:09s remains)
INFO - root - 2017-12-06 01:35:57.471528: step 60950, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.931 sec/batch; 70h:14m:11s remains)
INFO - root - 2017-12-06 01:36:06.807483: step 60960, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 69h:34m:34s remains)
INFO - root - 2017-12-06 01:36:15.938223: step 60970, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 70h:21m:49s remains)
INFO - root - 2017-12-06 01:36:25.294951: step 60980, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.867 sec/batch; 65h:22m:48s remains)
INFO - root - 2017-12-06 01:36:34.652189: step 60990, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 70h:23m:58s remains)
INFO - root - 2017-12-06 01:36:43.775209: step 61000, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 71h:16m:30s remains)
2017-12-06 01:36:44.641433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2407565 -4.2643294 -4.2962818 -4.3178692 -4.3213754 -4.311264 -4.2822857 -4.2400804 -4.2025661 -4.1814356 -4.1759381 -4.1746254 -4.17135 -4.1695724 -4.1816745][-4.2378569 -4.2607837 -4.2945118 -4.312809 -4.30885 -4.2918839 -4.2567449 -4.209898 -4.1734614 -4.1578345 -4.16223 -4.1700115 -4.1686478 -4.161994 -4.1664586][-4.2268367 -4.2426248 -4.2710533 -4.2849369 -4.2798233 -4.2625604 -4.2278666 -4.1802683 -4.1453581 -4.130507 -4.1379647 -4.1517119 -4.1524291 -4.1451082 -4.1446948][-4.1909776 -4.2015409 -4.2234173 -4.2358327 -4.2350159 -4.2275348 -4.2039638 -4.1644182 -4.1339517 -4.1192617 -4.1220775 -4.1334763 -4.1348233 -4.1271162 -4.1193933][-4.1410375 -4.1510324 -4.1736 -4.1881542 -4.1908879 -4.1875887 -4.16925 -4.1351094 -4.1089263 -4.1004372 -4.1033454 -4.1134315 -4.1159649 -4.1045041 -4.0886803][-4.1053972 -4.1186256 -4.1470542 -4.1598096 -4.1535158 -4.1439409 -4.1264658 -4.0971117 -4.0787864 -4.0809941 -4.088603 -4.0975513 -4.0982718 -4.08292 -4.0580688][-4.0840678 -4.1020513 -4.136765 -4.1401682 -4.1174846 -4.0972285 -4.0763121 -4.0527096 -4.0458422 -4.0614018 -4.0743613 -4.0826774 -4.0797825 -4.0585089 -4.0323286][-4.03902 -4.0690365 -4.1105037 -4.1161451 -4.0880961 -4.0546012 -4.0210009 -4.0013843 -4.0125833 -4.0467315 -4.065969 -4.0744209 -4.0694818 -4.0503821 -4.0280476][-4.0178304 -4.050765 -4.09255 -4.1070013 -4.083353 -4.0412836 -3.9954276 -3.9650686 -3.9721384 -4.0105419 -4.0368423 -4.0442557 -4.04021 -4.0263376 -4.014555][-4.0643253 -4.084928 -4.1165481 -4.1331506 -4.1117134 -4.0690341 -4.0229349 -3.9837153 -3.9721372 -3.9947917 -4.0157638 -4.0144191 -4.0079656 -3.9999619 -3.9970617][-4.1275382 -4.1366477 -4.1562495 -4.1693206 -4.1485419 -4.1132212 -4.0828137 -4.0533934 -4.0438166 -4.0621333 -4.081481 -4.0734992 -4.0603666 -4.0524015 -4.0485854][-4.1623077 -4.1665711 -4.1810617 -4.1938186 -4.1814346 -4.1581826 -4.1387978 -4.12065 -4.1204028 -4.1404204 -4.1620283 -4.1601367 -4.1514821 -4.1482277 -4.1441779][-4.1689992 -4.1721272 -4.1839228 -4.1971283 -4.1978378 -4.1879611 -4.1764312 -4.16519 -4.1702371 -4.1901236 -4.208147 -4.2118087 -4.2132421 -4.2170391 -4.2172432][-4.1596951 -4.1604347 -4.1693072 -4.1851721 -4.1969552 -4.19488 -4.1848545 -4.1738291 -4.1775312 -4.1949515 -4.2102613 -4.2218757 -4.2374196 -4.2493863 -4.2541752][-4.1260519 -4.1245327 -4.1321359 -4.1501722 -4.1701312 -4.1761985 -4.1733274 -4.1658788 -4.1632948 -4.1705451 -4.1779346 -4.1923895 -4.2164445 -4.2370305 -4.2479811]]...]
INFO - root - 2017-12-06 01:36:53.949532: step 61010, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 69h:46m:05s remains)
INFO - root - 2017-12-06 01:37:03.426620: step 61020, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 71h:24m:10s remains)
INFO - root - 2017-12-06 01:37:12.567619: step 61030, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 70h:39m:03s remains)
INFO - root - 2017-12-06 01:37:21.798453: step 61040, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 65h:49m:40s remains)
INFO - root - 2017-12-06 01:37:31.230532: step 61050, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 69h:57m:45s remains)
INFO - root - 2017-12-06 01:37:40.432900: step 61060, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 71h:04m:18s remains)
INFO - root - 2017-12-06 01:37:49.596656: step 61070, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 67h:53m:25s remains)
INFO - root - 2017-12-06 01:37:58.907229: step 61080, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 70h:09m:24s remains)
INFO - root - 2017-12-06 01:38:08.284938: step 61090, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 70h:56m:28s remains)
INFO - root - 2017-12-06 01:38:17.521083: step 61100, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 69h:17m:17s remains)
2017-12-06 01:38:18.261604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3243279 -4.3246117 -4.3226709 -4.3183556 -4.313096 -4.3095703 -4.3096523 -4.3115745 -4.3141785 -4.3155184 -4.3152018 -4.3144212 -4.3115177 -4.3058419 -4.2964687][-4.3243289 -4.3248696 -4.3203068 -4.3122368 -4.3032584 -4.29743 -4.2985988 -4.3045139 -4.3117771 -4.3168697 -4.3185258 -4.3183975 -4.31621 -4.3119659 -4.3045487][-4.31767 -4.3164711 -4.3068242 -4.2924576 -4.2775497 -4.2677608 -4.2696075 -4.2820158 -4.2975359 -4.3114185 -4.3195639 -4.322084 -4.321311 -4.31893 -4.314106][-4.30631 -4.30142 -4.2824121 -4.2557068 -4.2293882 -4.2126617 -4.2154255 -4.2357221 -4.2631054 -4.2910218 -4.3105192 -4.3188357 -4.3214264 -4.3221912 -4.3204989][-4.2802811 -4.2714496 -4.239686 -4.1978908 -4.1587133 -4.135571 -4.1393523 -4.1655774 -4.202322 -4.2443771 -4.2764549 -4.2934885 -4.302731 -4.3093696 -4.3119745][-4.2330508 -4.2249975 -4.18613 -4.1335959 -4.0841937 -4.0556622 -4.0590148 -4.0869846 -4.1248465 -4.1721067 -4.212357 -4.2378259 -4.2566075 -4.2718372 -4.2780972][-4.1737089 -4.1730456 -4.1379428 -4.0840526 -4.0266685 -3.9914021 -3.9873939 -4.00847 -4.0411959 -4.0883818 -4.1315465 -4.1626844 -4.1891403 -4.209096 -4.2138824][-4.1285825 -4.1379776 -4.1153278 -4.0693426 -4.0139375 -3.9724085 -3.9514575 -3.955658 -3.9772143 -4.0201082 -4.0641561 -4.099194 -4.1270213 -4.1409931 -4.1320109][-4.1179109 -4.1403036 -4.1353745 -4.1067381 -4.0608606 -4.0156183 -3.97985 -3.9671452 -3.9784122 -4.012228 -4.0501847 -4.0821109 -4.1055923 -4.107398 -4.0789309][-4.1555071 -4.1824918 -4.1887918 -4.1770768 -4.1447568 -4.1060543 -4.072824 -4.0599871 -4.0674634 -4.0903206 -4.1152296 -4.1352124 -4.149034 -4.14038 -4.1029253][-4.2262282 -4.2485313 -4.2581906 -4.2560587 -4.2378836 -4.2132897 -4.1906619 -4.1812921 -4.1863451 -4.2008548 -4.214045 -4.2222261 -4.226387 -4.2141891 -4.1816645][-4.2859716 -4.2993603 -4.3073711 -4.3071985 -4.2977495 -4.2833819 -4.2684307 -4.2605267 -4.26103 -4.2674203 -4.2755055 -4.28117 -4.2838726 -4.2778635 -4.2592983][-4.3189654 -4.3243437 -4.3286219 -4.3258481 -4.3162556 -4.3025184 -4.2874427 -4.2739868 -4.2645988 -4.2641096 -4.2745986 -4.2896647 -4.3019443 -4.3066945 -4.3014331][-4.3317032 -4.333003 -4.3344183 -4.3269486 -4.3081594 -4.2821746 -4.2537823 -4.2240148 -4.1992712 -4.1941786 -4.2150741 -4.2505779 -4.2809162 -4.2992754 -4.3037148][-4.3276339 -4.3263559 -4.3238525 -4.3073711 -4.2716513 -4.2221527 -4.1709833 -4.1208839 -4.0831027 -4.0780344 -4.1144819 -4.17451 -4.2260375 -4.2598581 -4.2738857]]...]
INFO - root - 2017-12-06 01:38:27.626325: step 61110, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 72h:57m:16s remains)
INFO - root - 2017-12-06 01:38:36.895791: step 61120, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 71h:19m:30s remains)
INFO - root - 2017-12-06 01:38:46.130927: step 61130, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 71h:01m:24s remains)
INFO - root - 2017-12-06 01:38:55.445135: step 61140, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 65h:03m:44s remains)
INFO - root - 2017-12-06 01:39:04.598613: step 61150, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 66h:14m:53s remains)
INFO - root - 2017-12-06 01:39:13.946827: step 61160, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 64h:39m:18s remains)
INFO - root - 2017-12-06 01:39:23.314193: step 61170, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 69h:24m:00s remains)
INFO - root - 2017-12-06 01:39:32.844408: step 61180, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 69h:44m:56s remains)
INFO - root - 2017-12-06 01:39:42.166557: step 61190, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 70h:00m:37s remains)
INFO - root - 2017-12-06 01:39:51.690335: step 61200, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 70h:05m:39s remains)
2017-12-06 01:39:52.483567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2508135 -4.269938 -4.275095 -4.2425981 -4.1791282 -4.1129694 -4.0916314 -4.1199303 -4.1730103 -4.2185841 -4.2610054 -4.2883606 -4.2985606 -4.3027558 -4.3135467][-4.2333941 -4.25216 -4.2507944 -4.2074327 -4.1310625 -4.0521183 -4.0368958 -4.0771389 -4.1360869 -4.1903143 -4.2476249 -4.284359 -4.2967095 -4.2996769 -4.3113194][-4.2037516 -4.2200403 -4.2134953 -4.1618385 -4.0719924 -3.9790993 -3.9745388 -4.0294385 -4.0991492 -4.1649656 -4.2377443 -4.2813597 -4.2944937 -4.297823 -4.3091946][-4.1734214 -4.184629 -4.1764112 -4.122807 -4.0249825 -3.9307804 -3.9391313 -4.0047607 -4.0803852 -4.1532068 -4.2313933 -4.2762156 -4.29033 -4.2943082 -4.3068361][-4.1491313 -4.1529603 -4.1431923 -4.0959845 -4.0082932 -3.9273133 -3.9497459 -4.0129271 -4.0786734 -4.1501365 -4.2243829 -4.2678709 -4.2837734 -4.2899861 -4.3044562][-4.1427321 -4.1410193 -4.1303754 -4.0908771 -4.0133696 -3.9465935 -3.9778783 -4.0349393 -4.0868416 -4.1521525 -4.2216158 -4.2634754 -4.2801323 -4.2871723 -4.3028083][-4.1501679 -4.1461406 -4.1361337 -4.0970879 -4.02231 -3.9594893 -3.9926775 -4.04071 -4.0852175 -4.1502762 -4.221015 -4.2642813 -4.2808642 -4.2873383 -4.3023496][-4.1365323 -4.1370835 -4.1266041 -4.08317 -4.0063753 -3.9354951 -3.9580827 -3.9959292 -4.0546179 -4.1356936 -4.2147503 -4.2613783 -4.2794247 -4.2866712 -4.30214][-4.1139293 -4.1208611 -4.1153383 -4.0724778 -3.9881387 -3.9085932 -3.9180689 -3.949245 -4.0268459 -4.1256447 -4.2093582 -4.2552428 -4.2743535 -4.283638 -4.3010259][-4.1017013 -4.112042 -4.116519 -4.0820031 -3.9936202 -3.91772 -3.9330285 -3.9644275 -4.0369153 -4.1306682 -4.2082138 -4.2501469 -4.2694564 -4.2817073 -4.3001008][-4.1117325 -4.1243334 -4.1322527 -4.1020017 -4.0192938 -3.9571366 -3.9802809 -4.0085545 -4.0646544 -4.1379595 -4.2069912 -4.2473359 -4.266675 -4.2804852 -4.2998][-4.1370282 -4.1479011 -4.1513076 -4.1227627 -4.0449419 -3.9943941 -4.0264626 -4.0498991 -4.0926189 -4.1477284 -4.2082748 -4.2463689 -4.2675672 -4.2810936 -4.3003][-4.1647544 -4.1728005 -4.1696863 -4.134738 -4.04769 -4.0020661 -4.0425711 -4.0738134 -4.1175361 -4.1638575 -4.2173533 -4.2512569 -4.2719789 -4.2836041 -4.30099][-4.1887674 -4.1951132 -4.1893053 -4.146101 -4.0470319 -3.9963605 -4.0354605 -4.076375 -4.1293559 -4.1789784 -4.2288017 -4.2595968 -4.2773366 -4.2861018 -4.3017864][-4.1965256 -4.2024517 -4.1954951 -4.1534805 -4.0592432 -4.009747 -4.039762 -4.0820022 -4.1385007 -4.19148 -4.2415662 -4.2687273 -4.2819476 -4.2889838 -4.3028216]]...]
INFO - root - 2017-12-06 01:40:01.810023: step 61210, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 66h:24m:44s remains)
INFO - root - 2017-12-06 01:40:11.074782: step 61220, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 70h:28m:47s remains)
INFO - root - 2017-12-06 01:40:20.522780: step 61230, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 73h:31m:40s remains)
INFO - root - 2017-12-06 01:40:29.983479: step 61240, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.006 sec/batch; 75h:48m:00s remains)
INFO - root - 2017-12-06 01:40:39.152505: step 61250, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 64h:58m:19s remains)
INFO - root - 2017-12-06 01:40:48.503531: step 61260, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 69h:37m:36s remains)
INFO - root - 2017-12-06 01:40:57.956806: step 61270, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 72h:18m:24s remains)
INFO - root - 2017-12-06 01:41:07.268571: step 61280, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 72h:41m:53s remains)
INFO - root - 2017-12-06 01:41:16.601248: step 61290, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 72h:20m:59s remains)
INFO - root - 2017-12-06 01:41:25.908251: step 61300, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 73h:07m:54s remains)
2017-12-06 01:41:26.674914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.277462 -4.2762852 -4.27651 -4.2724819 -4.2629628 -4.2512527 -4.2438164 -4.2458358 -4.2548285 -4.264226 -4.2698808 -4.2710333 -4.2665572 -4.2628355 -4.2531066][-4.2617497 -4.261744 -4.2633743 -4.2590427 -4.2465482 -4.2300658 -4.2215309 -4.2274184 -4.2420135 -4.2570133 -4.2660489 -4.2664309 -4.2585335 -4.2511439 -4.2390413][-4.2288747 -4.2308226 -4.238234 -4.2379036 -4.2222981 -4.1991305 -4.1888585 -4.2007256 -4.22227 -4.2428637 -4.2554379 -4.2548275 -4.2419286 -4.2297521 -4.2168984][-4.2006454 -4.2040086 -4.221652 -4.2330327 -4.2184949 -4.187695 -4.1694407 -4.1835413 -4.2118759 -4.2366343 -4.25263 -4.2519035 -4.23291 -4.2131767 -4.1988549][-4.1665926 -4.1702075 -4.1934009 -4.2117114 -4.1992025 -4.1606517 -4.13386 -4.149857 -4.1889052 -4.2239714 -4.2461162 -4.2494712 -4.2295189 -4.2030087 -4.1824489][-4.1393785 -4.1473103 -4.171279 -4.185183 -4.1610489 -4.1033487 -4.054637 -4.0622282 -4.1192746 -4.1802168 -4.2182889 -4.2297854 -4.2130995 -4.1823239 -4.1596689][-4.1382337 -4.1547546 -4.1777296 -4.17998 -4.1337471 -4.0385442 -3.9410396 -3.9194782 -3.9970174 -4.0995994 -4.1687059 -4.1992607 -4.1905246 -4.1572003 -4.1331749][-4.1468472 -4.1675291 -4.1921363 -4.1901126 -4.1341114 -4.0147457 -3.8714542 -3.8010538 -3.8779366 -4.0138741 -4.1118455 -4.1609674 -4.1631207 -4.132628 -4.1093721][-4.1726184 -4.1907191 -4.2134476 -4.2163906 -4.1773658 -4.07974 -3.9538372 -3.8761992 -3.9194891 -4.0270615 -4.11559 -4.1664152 -4.1735754 -4.1438723 -4.1170731][-4.2191987 -4.2340851 -4.2509379 -4.2565427 -4.2363605 -4.1767879 -4.0947733 -4.0408645 -4.0604916 -4.1242189 -4.1770086 -4.20743 -4.2110558 -4.183578 -4.1508842][-4.2505946 -4.2600431 -4.2715378 -4.2768741 -4.2700214 -4.2395182 -4.1901631 -4.1516042 -4.1551328 -4.1918244 -4.2235169 -4.2374368 -4.2337232 -4.208221 -4.1726532][-4.264482 -4.26715 -4.2737007 -4.2793837 -4.2807693 -4.2695465 -4.2424922 -4.2138915 -4.2054625 -4.2234969 -4.2463098 -4.255693 -4.2489791 -4.2270389 -4.1971607][-4.2698541 -4.2643938 -4.2642012 -4.2691574 -4.2772894 -4.2804384 -4.270041 -4.2519946 -4.2409987 -4.2459669 -4.2597418 -4.2672591 -4.2608824 -4.2449408 -4.2250395][-4.2618203 -4.2556581 -4.2519364 -4.2528872 -4.2598538 -4.2682276 -4.2672119 -4.2571959 -4.2512937 -4.2546663 -4.2645144 -4.2713237 -4.2680335 -4.25891 -4.2489724][-4.2420135 -4.2383585 -4.2358389 -4.2372856 -4.2442822 -4.2538528 -4.2566609 -4.2537317 -4.2523394 -4.2566729 -4.2646275 -4.2713618 -4.2724462 -4.269733 -4.26519]]...]
INFO - root - 2017-12-06 01:41:35.981360: step 61310, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 74h:05m:24s remains)
INFO - root - 2017-12-06 01:41:45.404266: step 61320, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 72h:05m:14s remains)
INFO - root - 2017-12-06 01:41:54.783709: step 61330, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 73h:12m:19s remains)
INFO - root - 2017-12-06 01:42:04.123623: step 61340, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.876 sec/batch; 65h:57m:46s remains)
INFO - root - 2017-12-06 01:42:13.466482: step 61350, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 70h:44m:49s remains)
INFO - root - 2017-12-06 01:42:22.814245: step 61360, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.956 sec/batch; 72h:00m:21s remains)
INFO - root - 2017-12-06 01:42:32.020238: step 61370, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 70h:20m:42s remains)
INFO - root - 2017-12-06 01:42:41.456881: step 61380, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 68h:22m:32s remains)
INFO - root - 2017-12-06 01:42:50.588593: step 61390, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.921 sec/batch; 69h:19m:59s remains)
INFO - root - 2017-12-06 01:42:59.806285: step 61400, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 70h:07m:23s remains)
2017-12-06 01:43:00.637779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2402849 -4.2579923 -4.2653232 -4.26527 -4.2626014 -4.25883 -4.2560477 -4.2566066 -4.2566929 -4.251235 -4.2390265 -4.2279992 -4.2207556 -4.2183032 -4.229816][-4.2095695 -4.2280259 -4.2376113 -4.24248 -4.2505012 -4.2573195 -4.2625446 -4.2694612 -4.2752314 -4.275135 -4.2685909 -4.2617559 -4.2552981 -4.2519174 -4.2591505][-4.1925025 -4.20768 -4.2201214 -4.2283969 -4.2419286 -4.2528496 -4.2615662 -4.2749004 -4.2876868 -4.2945976 -4.2984324 -4.300498 -4.2981329 -4.2946768 -4.2950687][-4.2055826 -4.2117047 -4.2188272 -4.2224312 -4.2286425 -4.2327738 -4.2354646 -4.2506542 -4.2696109 -4.2847352 -4.3013716 -4.3168068 -4.3238492 -4.3236504 -4.3183484][-4.241292 -4.2392936 -4.2337656 -4.2199912 -4.2076426 -4.1948824 -4.182744 -4.19226 -4.2166986 -4.241652 -4.2727771 -4.3030515 -4.3217216 -4.3297806 -4.3247442][-4.2756462 -4.2700233 -4.2558603 -4.2236013 -4.1850753 -4.141058 -4.1051235 -4.1056604 -4.1354022 -4.1703506 -4.2166471 -4.2648311 -4.2968636 -4.3162708 -4.3196259][-4.2798061 -4.2724175 -4.256 -4.2194557 -4.162797 -4.0900354 -4.0288353 -4.01458 -4.0422425 -4.0855646 -4.14518 -4.2061706 -4.2509623 -4.2830324 -4.3003316][-4.2484031 -4.2413621 -4.2292829 -4.2027893 -4.1492963 -4.0712714 -3.9980412 -3.9661443 -3.9772778 -4.0170264 -4.0804682 -4.1463847 -4.1996865 -4.241065 -4.2691832][-4.196372 -4.1890659 -4.1823015 -4.1719842 -4.1367683 -4.0778441 -4.01921 -3.9844196 -3.9734919 -3.9872086 -4.0347896 -4.0965157 -4.1503296 -4.1953254 -4.2300515][-4.144577 -4.1393971 -4.1364822 -4.1351309 -4.1135311 -4.0775061 -4.0456967 -4.0249186 -4.003644 -3.9888465 -4.0107603 -4.0595212 -4.1088119 -4.1557188 -4.1922913][-4.1249876 -4.1275139 -4.1257 -4.1231341 -4.1110373 -4.0966034 -4.0871382 -4.0810671 -4.0656528 -4.0396581 -4.0351386 -4.0601554 -4.0952773 -4.1321754 -4.159914][-4.1486835 -4.155232 -4.1521988 -4.1458726 -4.14409 -4.1482182 -4.153667 -4.158433 -4.1499014 -4.1266537 -4.1140246 -4.1193571 -4.1300354 -4.1400976 -4.1459732][-4.1860166 -4.1941595 -4.1900692 -4.1820865 -4.1883039 -4.2026353 -4.2170644 -4.2272315 -4.2244558 -4.2079873 -4.1988006 -4.2003789 -4.1982603 -4.187717 -4.1731768][-4.2118845 -4.2203341 -4.2149692 -4.2042589 -4.2115426 -4.2291741 -4.2495909 -4.2645206 -4.2665896 -4.2590456 -4.2546597 -4.2576504 -4.2537775 -4.2375741 -4.2157774][-4.2350368 -4.2419572 -4.2320437 -4.2169642 -4.219738 -4.2336307 -4.25316 -4.2699442 -4.2775784 -4.2773814 -4.2774715 -4.2799006 -4.2748485 -4.25978 -4.2407341]]...]
INFO - root - 2017-12-06 01:43:09.937573: step 61410, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 69h:26m:25s remains)
INFO - root - 2017-12-06 01:43:19.204924: step 61420, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 65h:47m:51s remains)
INFO - root - 2017-12-06 01:43:28.785954: step 61430, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 70h:28m:03s remains)
INFO - root - 2017-12-06 01:43:37.945298: step 61440, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 64h:31m:17s remains)
INFO - root - 2017-12-06 01:43:47.285079: step 61450, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 68h:02m:11s remains)
INFO - root - 2017-12-06 01:43:56.706684: step 61460, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.965 sec/batch; 72h:38m:00s remains)
INFO - root - 2017-12-06 01:44:06.182237: step 61470, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 71h:51m:26s remains)
INFO - root - 2017-12-06 01:44:15.445040: step 61480, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 66h:58m:41s remains)
INFO - root - 2017-12-06 01:44:24.709800: step 61490, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 75h:04m:27s remains)
INFO - root - 2017-12-06 01:44:34.179937: step 61500, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 71h:59m:15s remains)
2017-12-06 01:44:34.893839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.268487 -4.2663569 -4.2656164 -4.2642951 -4.2601023 -4.2572546 -4.2576122 -4.2593732 -4.2641611 -4.2704334 -4.275485 -4.2787132 -4.2809024 -4.2784896 -4.2726893][-4.2581158 -4.2564235 -4.2544756 -4.250628 -4.2444119 -4.2416339 -4.2455325 -4.2521234 -4.2601309 -4.266572 -4.2708521 -4.2731204 -4.2758589 -4.2752047 -4.2700891][-4.2507596 -4.2477078 -4.2413979 -4.2329836 -4.2224436 -4.2167983 -4.2205844 -4.2292829 -4.2409654 -4.2491536 -4.2521639 -4.2523193 -4.2571988 -4.2622485 -4.2627144][-4.2467012 -4.2406378 -4.2283678 -4.2136507 -4.1956196 -4.1830988 -4.1803036 -4.1846066 -4.1969166 -4.20837 -4.2114682 -4.2096229 -4.2190351 -4.233932 -4.2433052][-4.2405348 -4.231061 -4.2140136 -4.193522 -4.1670561 -4.1437478 -4.1265306 -4.1177325 -4.1285658 -4.1457925 -4.1528177 -4.1522851 -4.168642 -4.1944141 -4.2134218][-4.2282758 -4.2162409 -4.1965165 -4.1709394 -4.1361446 -4.0998244 -4.0622916 -4.0314755 -4.0413461 -4.0723677 -4.0898585 -4.09571 -4.1208267 -4.1569891 -4.1832795][-4.2084322 -4.1966505 -4.177382 -4.1486707 -4.1079359 -4.0614262 -4.0029488 -3.9484 -3.9604795 -4.0104928 -4.0410857 -4.0555892 -4.0876789 -4.1285343 -4.157321][-4.1875067 -4.1787329 -4.1633692 -4.1367068 -4.0967298 -4.0473824 -3.9764 -3.9079318 -3.9232059 -3.9855986 -4.0247512 -4.0449672 -4.07708 -4.1138229 -4.1365919][-4.1747169 -4.1699662 -4.1599379 -4.1402321 -4.1092706 -4.0687556 -4.0044308 -3.9449508 -3.9579694 -4.0094151 -4.0403228 -4.056036 -4.0787878 -4.1026382 -4.1143808][-4.1753807 -4.1742725 -4.1671624 -4.15289 -4.132071 -4.1043105 -4.0586991 -4.01767 -4.0247383 -4.0549188 -4.0714841 -4.0782261 -4.0889411 -4.0983882 -4.0989375][-4.1948848 -4.1956038 -4.1895151 -4.177774 -4.1631994 -4.1449442 -4.1154962 -4.088459 -4.0873785 -4.0986404 -4.1021204 -4.1005344 -4.101728 -4.1009274 -4.0943484][-4.2264833 -4.2278833 -4.2223735 -4.21188 -4.2008829 -4.1883965 -4.1694732 -4.1518273 -4.1459255 -4.1454182 -4.140213 -4.1334505 -4.1301827 -4.125803 -4.1168413][-4.2619076 -4.2624755 -4.257381 -4.24875 -4.2410545 -4.2325835 -4.219975 -4.207685 -4.1987429 -4.1917253 -4.1850042 -4.180089 -4.1789885 -4.1758161 -4.1686668][-4.290484 -4.2903881 -4.2869492 -4.2818007 -4.2780643 -4.273541 -4.2655644 -4.2560186 -4.2455196 -4.2365427 -4.231214 -4.2296791 -4.2313795 -4.2299013 -4.2254524][-4.3072305 -4.3077664 -4.306035 -4.3034143 -4.3021107 -4.3004351 -4.296216 -4.2899127 -4.281724 -4.2749591 -4.2718439 -4.2722559 -4.2741623 -4.2728949 -4.2701955]]...]
INFO - root - 2017-12-06 01:44:44.186757: step 61510, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 66h:57m:41s remains)
INFO - root - 2017-12-06 01:44:53.347197: step 61520, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 67h:23m:08s remains)
INFO - root - 2017-12-06 01:45:02.839475: step 61530, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 71h:29m:09s remains)
INFO - root - 2017-12-06 01:45:12.157330: step 61540, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 72h:51m:49s remains)
INFO - root - 2017-12-06 01:45:21.360278: step 61550, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 62h:34m:28s remains)
INFO - root - 2017-12-06 01:45:31.012058: step 61560, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 73h:08m:52s remains)
INFO - root - 2017-12-06 01:45:40.121421: step 61570, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 71h:40m:02s remains)
INFO - root - 2017-12-06 01:45:49.129538: step 61580, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 64h:08m:08s remains)
INFO - root - 2017-12-06 01:45:58.408614: step 61590, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 68h:34m:29s remains)
INFO - root - 2017-12-06 01:46:07.567497: step 61600, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.994 sec/batch; 74h:48m:11s remains)
2017-12-06 01:46:08.340618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3160586 -4.3121786 -4.3194041 -4.3401556 -4.3619728 -4.3765812 -4.3790793 -4.3436275 -4.2610192 -4.1487274 -4.0602088 -4.059926 -4.1353178 -4.2219272 -4.2882304][-4.3517389 -4.3608937 -4.3792248 -4.3979187 -4.4054546 -4.3994684 -4.3802352 -4.3272467 -4.2382479 -4.135941 -4.0725212 -4.0969667 -4.1793494 -4.2590871 -4.3189197][-4.3685455 -4.3873386 -4.4097152 -4.4223089 -4.4148116 -4.3869209 -4.3447332 -4.2781019 -4.1926036 -4.1183515 -4.0964117 -4.1461625 -4.2321095 -4.3028579 -4.3496447][-4.3756447 -4.3981137 -4.416451 -4.4173317 -4.38921 -4.3346891 -4.2661972 -4.1871352 -4.118464 -4.0945597 -4.1200938 -4.1849179 -4.2654924 -4.326015 -4.3636508][-4.3725734 -4.3928819 -4.4024334 -4.3861833 -4.3269477 -4.2379813 -4.136003 -4.0364718 -3.9922161 -4.0346842 -4.11783 -4.2011256 -4.2758894 -4.328239 -4.3618][-4.3598342 -4.3776779 -4.3799853 -4.34083 -4.241322 -4.1060982 -3.9535067 -3.8128786 -3.7976851 -3.921967 -4.0667024 -4.1792083 -4.260797 -4.3145843 -4.3503366][-4.3349409 -4.3523693 -4.3494773 -4.2899904 -4.1593132 -3.979898 -3.7673781 -3.5850027 -3.615078 -3.8158393 -4.0079808 -4.1466279 -4.2403817 -4.3005772 -4.3408961][-4.3078666 -4.3255014 -4.3224525 -4.2603579 -4.126596 -3.9395587 -3.7227519 -3.5674062 -3.6416206 -3.845226 -4.0254092 -4.1574173 -4.245707 -4.3033805 -4.3415127][-4.2749014 -4.3014097 -4.3076639 -4.2633519 -4.1591735 -4.0115037 -3.8566926 -3.7725739 -3.8441508 -3.988915 -4.1184559 -4.2184024 -4.2849612 -4.3282051 -4.3556023][-4.2340984 -4.2754359 -4.2982116 -4.2794061 -4.2160125 -4.1205096 -4.0320735 -3.999037 -4.0516872 -4.1401391 -4.2220192 -4.2868042 -4.329927 -4.3581142 -4.3727617][-4.2086549 -4.2630835 -4.2949295 -4.2912693 -4.2541103 -4.1998777 -4.1578817 -4.1561322 -4.1960158 -4.2500625 -4.2991161 -4.3344059 -4.3621259 -4.3787165 -4.3825741][-4.21637 -4.2740073 -4.3053675 -4.3035922 -4.2764459 -4.2445068 -4.2284307 -4.245038 -4.2804523 -4.3173213 -4.3457546 -4.3628879 -4.37898 -4.3854456 -4.3819246][-4.2568913 -4.3053827 -4.3254652 -4.3139396 -4.283154 -4.255146 -4.2519703 -4.2785888 -4.31478 -4.3429723 -4.3597221 -4.3678117 -4.3762593 -4.3784709 -4.3738265][-4.302669 -4.3357143 -4.3423538 -4.316812 -4.2753816 -4.2437019 -4.245832 -4.2798085 -4.3193488 -4.3422918 -4.3510146 -4.3534393 -4.3586535 -4.3627176 -4.3625803][-4.3386574 -4.3584819 -4.3514729 -4.3142419 -4.2637358 -4.2280059 -4.2327805 -4.2727313 -4.3137608 -4.3350463 -4.3397007 -4.3392568 -4.3433189 -4.3503695 -4.3550191]]...]
INFO - root - 2017-12-06 01:46:17.535570: step 61610, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.941 sec/batch; 70h:47m:32s remains)
INFO - root - 2017-12-06 01:46:27.085744: step 61620, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 70h:04m:29s remains)
INFO - root - 2017-12-06 01:46:36.343458: step 61630, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 70h:31m:15s remains)
INFO - root - 2017-12-06 01:46:45.793958: step 61640, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 70h:05m:54s remains)
INFO - root - 2017-12-06 01:46:55.224183: step 61650, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.974 sec/batch; 73h:17m:01s remains)
INFO - root - 2017-12-06 01:47:04.490765: step 61660, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 72h:26m:36s remains)
INFO - root - 2017-12-06 01:47:13.853840: step 61670, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 70h:45m:56s remains)
INFO - root - 2017-12-06 01:47:23.279741: step 61680, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 70h:23m:42s remains)
INFO - root - 2017-12-06 01:47:32.475757: step 61690, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 72h:59m:15s remains)
INFO - root - 2017-12-06 01:47:41.927522: step 61700, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 71h:27m:21s remains)
2017-12-06 01:47:42.690601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1285553 -4.0604963 -4.0255408 -4.0178914 -4.0026784 -3.9922457 -4.0081439 -4.0294952 -4.0427227 -4.0575919 -4.0872307 -4.1160569 -4.1257157 -4.1162114 -4.1062841][-4.117352 -4.0580721 -4.0316596 -4.0310183 -4.0098147 -3.9820528 -3.9855373 -3.9986625 -4.0078444 -4.0333 -4.0777264 -4.1151791 -4.1223345 -4.0974946 -4.077661][-4.1142821 -4.07948 -4.0626793 -4.0542421 -4.0200396 -3.9807761 -3.9818716 -3.998395 -4.0139418 -4.0474277 -4.0920997 -4.1274729 -4.12204 -4.0760341 -4.0379934][-4.119297 -4.0955243 -4.07802 -4.0576782 -4.0162187 -3.9765885 -3.9820976 -4.0097332 -4.0408854 -4.0784292 -4.1158109 -4.1449542 -4.1361403 -4.0872345 -4.0399051][-4.1131864 -4.0909557 -4.0699539 -4.0493879 -4.0161457 -3.9871612 -3.9992087 -4.0382438 -4.0818319 -4.1208944 -4.1524634 -4.1780005 -4.1767139 -4.143908 -4.10267][-4.1088109 -4.0872521 -4.0713663 -4.0582757 -4.0336933 -4.0072522 -4.0127459 -4.0518508 -4.1016016 -4.1475549 -4.18209 -4.2088451 -4.217886 -4.2046876 -4.1766324][-4.1244292 -4.1114922 -4.1103592 -4.1081944 -4.0877233 -4.0500984 -4.029923 -4.0541611 -4.0986843 -4.1474061 -4.1867971 -4.217411 -4.2377195 -4.2399158 -4.2291255][-4.1461272 -4.1464682 -4.1610456 -4.1678162 -4.1535387 -4.1105871 -4.0709858 -4.0752721 -4.10452 -4.1460118 -4.1835442 -4.2160559 -4.2460289 -4.2580934 -4.2563438][-4.1746964 -4.1830225 -4.2007055 -4.2112188 -4.205514 -4.1680875 -4.1228914 -4.1086707 -4.1246881 -4.1579227 -4.1868744 -4.2153039 -4.24578 -4.2632909 -4.2673278][-4.1988091 -4.2088351 -4.2218695 -4.2292576 -4.229187 -4.2007785 -4.15681 -4.1297803 -4.1391807 -4.1676679 -4.1917686 -4.2130532 -4.2363582 -4.2516718 -4.2578354][-4.2077003 -4.2149963 -4.2219148 -4.2256055 -4.2254567 -4.1998935 -4.1590986 -4.1308684 -4.1405845 -4.1702852 -4.1962085 -4.214036 -4.2287111 -4.2377458 -4.2419338][-4.1936526 -4.194602 -4.1949563 -4.1960988 -4.1892834 -4.159503 -4.1250219 -4.1091404 -4.1269641 -4.1604867 -4.1919594 -4.2116594 -4.2178903 -4.2184243 -4.2199073][-4.1891708 -4.1876307 -4.1840153 -4.1832738 -4.169127 -4.1339693 -4.1024451 -4.0968528 -4.1223226 -4.1610031 -4.1971464 -4.2154546 -4.2162709 -4.2104754 -4.2071414][-4.2084403 -4.2014441 -4.1922889 -4.1880789 -4.1686516 -4.1316752 -4.1012039 -4.1025558 -4.1324949 -4.1741934 -4.2108021 -4.2266531 -4.2229505 -4.2133379 -4.2090116][-4.2369723 -4.2223372 -4.2085214 -4.2034078 -4.1860394 -4.1576252 -4.136601 -4.1432843 -4.168849 -4.1994843 -4.2263346 -4.236105 -4.22844 -4.2174506 -4.2132311]]...]
INFO - root - 2017-12-06 01:47:52.081961: step 61710, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 69h:42m:09s remains)
INFO - root - 2017-12-06 01:48:01.314037: step 61720, loss = 2.01, batch loss = 1.95 (8.6 examples/sec; 0.931 sec/batch; 70h:00m:36s remains)
INFO - root - 2017-12-06 01:48:10.622722: step 61730, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 74h:08m:48s remains)
INFO - root - 2017-12-06 01:48:19.889200: step 61740, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 69h:25m:47s remains)
INFO - root - 2017-12-06 01:48:29.340741: step 61750, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.003 sec/batch; 75h:27m:39s remains)
INFO - root - 2017-12-06 01:48:38.607762: step 61760, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.990 sec/batch; 74h:28m:17s remains)
INFO - root - 2017-12-06 01:48:47.845913: step 61770, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 73h:19m:50s remains)
INFO - root - 2017-12-06 01:48:57.311986: step 61780, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 71h:49m:27s remains)
INFO - root - 2017-12-06 01:49:06.513898: step 61790, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 71h:23m:59s remains)
INFO - root - 2017-12-06 01:49:15.713104: step 61800, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 70h:24m:38s remains)
2017-12-06 01:49:16.500857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2867241 -4.2885184 -4.2950029 -4.303472 -4.3104844 -4.3147254 -4.3167186 -4.317255 -4.3165789 -4.31538 -4.3146224 -4.3143678 -4.3129869 -4.3106041 -4.3078995][-4.304925 -4.3076291 -4.3144283 -4.3229146 -4.3297677 -4.3336496 -4.3349118 -4.3337355 -4.3311925 -4.3289466 -4.3281121 -4.3288074 -4.3296862 -4.32994 -4.3282166][-4.3191528 -4.3222322 -4.3271213 -4.3323822 -4.3356953 -4.3356552 -4.3318405 -4.325727 -4.3203158 -4.3180852 -4.3202214 -4.326437 -4.3331404 -4.3373518 -4.3375721][-4.3167553 -4.3192892 -4.3217869 -4.3227959 -4.3211641 -4.31572 -4.3055148 -4.293015 -4.2834444 -4.2806163 -4.2862086 -4.2996349 -4.3150587 -4.3268452 -4.3320885][-4.2996044 -4.3002682 -4.3003116 -4.2970953 -4.2903233 -4.2791762 -4.2621751 -4.2417712 -4.2246289 -4.2172055 -4.2235694 -4.2434497 -4.2697649 -4.2926245 -4.3070264][-4.2753067 -4.2736154 -4.2723927 -4.2667279 -4.2564006 -4.2400956 -4.2172432 -4.18895 -4.161406 -4.1450796 -4.1489658 -4.1742711 -4.210042 -4.2423973 -4.2664819][-4.25311 -4.2493391 -4.2481923 -4.24168 -4.22958 -4.2112269 -4.1880155 -4.1583805 -4.1258688 -4.1007905 -4.0965347 -4.1187711 -4.1545191 -4.190753 -4.2222662][-4.241993 -4.2370982 -4.2371278 -4.2314014 -4.2197285 -4.2031918 -4.1848116 -4.1608634 -4.1324759 -4.1060572 -4.0937958 -4.1057386 -4.132021 -4.1629968 -4.1937456][-4.2560124 -4.2527676 -4.2542911 -4.2489624 -4.2367291 -4.2211976 -4.2061105 -4.188911 -4.1702337 -4.1526232 -4.1427689 -4.1487427 -4.1643205 -4.1829853 -4.2030935][-4.2790222 -4.2768984 -4.2773619 -4.2704968 -4.2566414 -4.2413497 -4.228878 -4.2187343 -4.2111478 -4.20685 -4.2063975 -4.2129445 -4.2210279 -4.2282715 -4.2359705][-4.2936087 -4.2913632 -4.2893414 -4.2812285 -4.2674046 -4.2533994 -4.2444162 -4.2399664 -4.2397594 -4.2441344 -4.2512522 -4.26007 -4.2651086 -4.2672811 -4.2681136][-4.3004885 -4.2991138 -4.2963133 -4.2889409 -4.2778778 -4.267323 -4.2613826 -4.2586527 -4.2585807 -4.2627783 -4.2705421 -4.2803807 -4.2852421 -4.286097 -4.2841549][-4.3077483 -4.3079553 -4.3058982 -4.3003516 -4.2920194 -4.2843094 -4.2799745 -4.2765579 -4.2736917 -4.2742643 -4.2791128 -4.2861614 -4.289011 -4.2885089 -4.28438][-4.3144236 -4.3150992 -4.3131967 -4.3085189 -4.3021832 -4.2973437 -4.2955527 -4.2932348 -4.2886181 -4.2845316 -4.2828884 -4.2822437 -4.2792859 -4.2748451 -4.26815][-4.3249445 -4.3254809 -4.3228498 -4.3180294 -4.313272 -4.3108411 -4.3104405 -4.3082952 -4.3022375 -4.29481 -4.288115 -4.2809806 -4.2725477 -4.26423 -4.25553]]...]
INFO - root - 2017-12-06 01:49:25.873045: step 61810, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 74h:48m:35s remains)
INFO - root - 2017-12-06 01:49:35.287929: step 61820, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 70h:45m:54s remains)
INFO - root - 2017-12-06 01:49:44.596095: step 61830, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 70h:18m:38s remains)
INFO - root - 2017-12-06 01:49:54.120868: step 61840, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 69h:12m:48s remains)
INFO - root - 2017-12-06 01:50:03.407621: step 61850, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.941 sec/batch; 70h:46m:40s remains)
INFO - root - 2017-12-06 01:50:12.758220: step 61860, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 70h:54m:10s remains)
INFO - root - 2017-12-06 01:50:22.047868: step 61870, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 66h:14m:23s remains)
INFO - root - 2017-12-06 01:50:31.146738: step 61880, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 64h:11m:50s remains)
INFO - root - 2017-12-06 01:50:40.397290: step 61890, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 69h:32m:20s remains)
INFO - root - 2017-12-06 01:50:49.660433: step 61900, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 69h:04m:55s remains)
2017-12-06 01:50:50.417651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.291048 -4.2973075 -4.2880611 -4.2563396 -4.2088094 -4.1494441 -4.1188197 -4.1272497 -4.1630487 -4.2099662 -4.257391 -4.2907448 -4.306953 -4.3148308 -4.3194952][-4.2965322 -4.303411 -4.2926259 -4.255167 -4.1959991 -4.1267853 -4.094573 -4.1132851 -4.1607809 -4.2141285 -4.2654223 -4.2988172 -4.3145237 -4.3213768 -4.3246508][-4.29579 -4.3014021 -4.2894626 -4.2524905 -4.1865644 -4.1071129 -4.067462 -4.0858746 -4.1401882 -4.2008762 -4.258297 -4.2965021 -4.3158932 -4.3243308 -4.3267612][-4.2917786 -4.2953854 -4.281837 -4.24717 -4.1813688 -4.0932889 -4.0398722 -4.045712 -4.1002717 -4.1715918 -4.2396879 -4.2863607 -4.3135076 -4.3252993 -4.3275867][-4.2828379 -4.2837958 -4.2703619 -4.238348 -4.1744432 -4.0806441 -4.0122609 -4.0035081 -4.059207 -4.1428127 -4.2221913 -4.276041 -4.3091707 -4.3244929 -4.3267431][-4.2693906 -4.2672338 -4.2553291 -4.2269444 -4.1651983 -4.0724292 -3.9992781 -3.9835424 -4.0458889 -4.1389112 -4.2214541 -4.2759976 -4.3095136 -4.3251743 -4.326601][-4.2487116 -4.2433629 -4.2344422 -4.212625 -4.1550694 -4.0711374 -4.004827 -3.9948754 -4.0628495 -4.1544371 -4.23191 -4.2828069 -4.3141508 -4.3275356 -4.3278112][-4.2079391 -4.1991715 -4.1970496 -4.1866241 -4.1430964 -4.0767436 -4.0271959 -4.0274324 -4.0938611 -4.1755595 -4.2427835 -4.2893815 -4.3187685 -4.3298345 -4.3289337][-4.1491232 -4.1382246 -4.1484528 -4.1523905 -4.1288996 -4.083919 -4.0525804 -4.06009 -4.1184378 -4.1884093 -4.248703 -4.2947855 -4.3232484 -4.3317637 -4.3291712][-4.117415 -4.1043854 -4.1207366 -4.1320372 -4.122468 -4.0920796 -4.0704012 -4.0802579 -4.1289673 -4.1913714 -4.2512684 -4.2991395 -4.3268566 -4.3330979 -4.3291063][-4.1388226 -4.1252346 -4.1378136 -4.1462159 -4.13591 -4.1057858 -4.0815277 -4.0878487 -4.1302366 -4.1923103 -4.2537584 -4.3025622 -4.3285 -4.3337111 -4.3289132][-4.1733 -4.1570358 -4.1620226 -4.1662383 -4.151536 -4.1142697 -4.0818615 -4.0842056 -4.1259246 -4.1910553 -4.25639 -4.303443 -4.3268676 -4.3329387 -4.3285155][-4.2040963 -4.1883464 -4.1883674 -4.191699 -4.1754789 -4.136023 -4.0976114 -4.0951462 -4.136138 -4.1993828 -4.2627687 -4.3052115 -4.3261857 -4.3327875 -4.3292251][-4.2301879 -4.2175179 -4.2149391 -4.2158647 -4.19718 -4.1567159 -4.1125474 -4.1070461 -4.1471863 -4.2056155 -4.2661214 -4.3051882 -4.3249826 -4.332674 -4.3301983][-4.2445006 -4.2342744 -4.2308736 -4.228157 -4.203033 -4.1552906 -4.107305 -4.0986314 -4.1376266 -4.194262 -4.2560039 -4.2973332 -4.3209109 -4.3315487 -4.3304062]]...]
INFO - root - 2017-12-06 01:50:59.683019: step 61910, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 73h:10m:04s remains)
INFO - root - 2017-12-06 01:51:08.799711: step 61920, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 73h:41m:59s remains)
INFO - root - 2017-12-06 01:51:18.234724: step 61930, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 70h:21m:30s remains)
INFO - root - 2017-12-06 01:51:27.542223: step 61940, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 72h:29m:29s remains)
INFO - root - 2017-12-06 01:51:36.647058: step 61950, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 68h:25m:32s remains)
INFO - root - 2017-12-06 01:51:46.116814: step 61960, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.009 sec/batch; 75h:49m:26s remains)
INFO - root - 2017-12-06 01:51:55.358640: step 61970, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 71h:33m:57s remains)
INFO - root - 2017-12-06 01:52:04.782988: step 61980, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 65h:21m:11s remains)
INFO - root - 2017-12-06 01:52:14.097899: step 61990, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 74h:46m:19s remains)
INFO - root - 2017-12-06 01:52:23.332542: step 62000, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 68h:49m:51s remains)
2017-12-06 01:52:24.155059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2578425 -4.2779651 -4.292902 -4.2908044 -4.2652411 -4.23198 -4.2106676 -4.1917357 -4.1501794 -4.1074533 -4.08264 -4.0809836 -4.0994554 -4.1273355 -4.1523209][-4.2604852 -4.269558 -4.2752614 -4.2674274 -4.2388544 -4.2037086 -4.1866651 -4.1806231 -4.1608458 -4.137682 -4.1205082 -4.1087074 -4.107305 -4.1122932 -4.1158619][-4.2651286 -4.26727 -4.2653627 -4.2520285 -4.2244787 -4.1949463 -4.1838722 -4.1847987 -4.1808796 -4.1755924 -4.16753 -4.1480122 -4.1284742 -4.1112857 -4.0899363][-4.2659726 -4.2591319 -4.2524848 -4.2363071 -4.2095675 -4.1846876 -4.1758418 -4.1754713 -4.179637 -4.1915951 -4.1960382 -4.1798487 -4.1568427 -4.1313763 -4.0988045][-4.2617493 -4.2417526 -4.2274508 -4.2044678 -4.1690893 -4.1328406 -4.1103644 -4.1021876 -4.1168919 -4.1541538 -4.1860456 -4.1947646 -4.1923981 -4.1773643 -4.1455827][-4.2467685 -4.2105718 -4.1783404 -4.1324134 -4.0683517 -4.0064936 -3.96255 -3.949789 -3.9888771 -4.0672336 -4.1435056 -4.1934114 -4.2230191 -4.2274175 -4.2037416][-4.2160721 -4.1647425 -4.1091046 -4.032608 -3.9350805 -3.8483212 -3.785377 -3.7762187 -3.85082 -3.978559 -4.1026034 -4.1923723 -4.2464643 -4.2645764 -4.2525396][-4.1784415 -4.1223669 -4.0545769 -3.9646344 -3.8564913 -3.7619429 -3.6928327 -3.6964266 -3.7970767 -3.9479914 -4.091269 -4.2015448 -4.266161 -4.288249 -4.2783728][-4.1503153 -4.102694 -4.0444188 -3.9745169 -3.8907347 -3.8159213 -3.7620311 -3.7671454 -3.8501661 -3.9752314 -4.1031494 -4.2106767 -4.2760186 -4.2938786 -4.2731729][-4.1516476 -4.1191545 -4.0805583 -4.0394979 -3.98524 -3.930248 -3.8904524 -3.8914981 -3.9411979 -4.0242357 -4.1237364 -4.2181783 -4.2753716 -4.2818274 -4.2436347][-4.1812038 -4.1597137 -4.1364379 -4.115931 -4.0799046 -4.0354204 -4.0011086 -3.9983783 -4.0293369 -4.0843253 -4.1618032 -4.2365828 -4.2757835 -4.2673144 -4.21893][-4.2187147 -4.2039633 -4.1918941 -4.1839314 -4.1588621 -4.1239419 -4.0961227 -4.0916476 -4.1124787 -4.1518683 -4.2081389 -4.2580481 -4.2773814 -4.26047 -4.2139444][-4.2512856 -4.2444596 -4.2420611 -4.2382884 -4.2198868 -4.1959453 -4.1778188 -4.1750078 -4.188067 -4.213973 -4.2448626 -4.2689276 -4.2727327 -4.2520609 -4.2185359][-4.2807894 -4.2798719 -4.282198 -4.2804022 -4.2666221 -4.2515378 -4.2429733 -4.2435822 -4.2516465 -4.2653422 -4.2767377 -4.2822747 -4.2754664 -4.2568064 -4.2401395][-4.3077965 -4.3082581 -4.3109908 -4.3093905 -4.2998567 -4.2909837 -4.2880526 -4.2907686 -4.2965856 -4.3024693 -4.3031774 -4.2995043 -4.2884803 -4.274323 -4.2704206]]...]
INFO - root - 2017-12-06 01:52:33.382979: step 62010, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 69h:55m:59s remains)
INFO - root - 2017-12-06 01:52:42.860354: step 62020, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 68h:19m:17s remains)
INFO - root - 2017-12-06 01:52:52.087647: step 62030, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 68h:30m:48s remains)
INFO - root - 2017-12-06 01:53:01.268169: step 62040, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 67h:13m:22s remains)
INFO - root - 2017-12-06 01:53:10.654808: step 62050, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 71h:02m:33s remains)
INFO - root - 2017-12-06 01:53:20.144487: step 62060, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 74h:27m:24s remains)
INFO - root - 2017-12-06 01:53:29.598289: step 62070, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 69h:38m:36s remains)
INFO - root - 2017-12-06 01:53:38.938528: step 62080, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 69h:14m:08s remains)
INFO - root - 2017-12-06 01:53:48.146313: step 62090, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.968 sec/batch; 72h:41m:43s remains)
INFO - root - 2017-12-06 01:53:57.614020: step 62100, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 72h:09m:26s remains)
2017-12-06 01:53:58.338761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2270494 -4.2364459 -4.2423511 -4.2515187 -4.2605591 -4.2617569 -4.2606115 -4.2594566 -4.262764 -4.2683053 -4.2748332 -4.2819867 -4.2903194 -4.2962575 -4.2934108][-4.236167 -4.2474422 -4.255302 -4.2612782 -4.2636003 -4.2619734 -4.2634 -4.2694621 -4.275475 -4.2813973 -4.2852783 -4.2882304 -4.2939439 -4.2999158 -4.3003654][-4.2408509 -4.24681 -4.2524748 -4.2544346 -4.2482834 -4.2390532 -4.2381582 -4.249208 -4.2601709 -4.2704182 -4.2774162 -4.2808347 -4.2847896 -4.2913504 -4.2953477][-4.2385077 -4.2377887 -4.2354574 -4.2285786 -4.2101445 -4.1867089 -4.1754532 -4.1880846 -4.2092075 -4.2297068 -4.2468176 -4.2586846 -4.2664876 -4.2746472 -4.2824125][-4.225503 -4.2226191 -4.2151966 -4.1946397 -4.1549339 -4.104423 -4.07148 -4.0790811 -4.1143417 -4.1534843 -4.1902108 -4.2187939 -4.2373405 -4.2524576 -4.2661867][-4.1978369 -4.1933374 -4.1838307 -4.1542606 -4.0974741 -4.0183129 -3.9511666 -3.9372692 -3.9777529 -4.036911 -4.0974293 -4.1489387 -4.1836681 -4.21042 -4.234282][-4.1494465 -4.140944 -4.1356745 -4.1115022 -4.058331 -3.9726367 -3.8797116 -3.8295979 -3.8471694 -3.9061697 -3.9828777 -4.0540562 -4.103828 -4.1429739 -4.1787457][-4.0861149 -4.076066 -4.0757914 -4.0652041 -4.0352392 -3.9720871 -3.884768 -3.813797 -3.7954652 -3.8241465 -3.8914151 -3.9673886 -4.0226903 -4.0684414 -4.1130066][-4.0620723 -4.0526156 -4.0518708 -4.0506511 -4.0435376 -4.0130768 -3.9555273 -3.8947694 -3.8611641 -3.8601153 -3.8998053 -3.9579868 -4.0042067 -4.047864 -4.0958691][-4.1008844 -4.0928297 -4.0908461 -4.0923772 -4.0995207 -4.0920658 -4.0587959 -4.0176477 -3.9872496 -3.976799 -3.9959316 -4.0321712 -4.0622625 -4.0952191 -4.1348119][-4.1730785 -4.167026 -4.1657262 -4.1665511 -4.1750064 -4.1782818 -4.1615295 -4.1378446 -4.117413 -4.1071677 -4.1159158 -4.1364083 -4.1528397 -4.1749392 -4.2022696][-4.2506409 -4.2467637 -4.247911 -4.2493348 -4.2542953 -4.2588587 -4.2534041 -4.2419891 -4.231441 -4.2238564 -4.2258987 -4.2338676 -4.2405324 -4.25247 -4.2681746][-4.3059831 -4.3037653 -4.3080158 -4.3127413 -4.3175116 -4.3211646 -4.3202586 -4.313745 -4.3067641 -4.2999907 -4.2982521 -4.2997127 -4.3014655 -4.3059444 -4.312057][-4.3321524 -4.3312011 -4.335083 -4.3397436 -4.3442426 -4.3468261 -4.3477235 -4.3446236 -4.3392019 -4.3330793 -4.3292232 -4.3271747 -4.3262415 -4.3266406 -4.3281307][-4.33532 -4.3338203 -4.3350186 -4.3380756 -4.3414593 -4.3434935 -4.3445959 -4.34361 -4.3409538 -4.3370409 -4.3334074 -4.33038 -4.3286428 -4.3276763 -4.3281603]]...]
INFO - root - 2017-12-06 01:54:07.733576: step 62110, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.010 sec/batch; 75h:52m:04s remains)
INFO - root - 2017-12-06 01:54:17.054570: step 62120, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 70h:36m:43s remains)
INFO - root - 2017-12-06 01:54:26.158114: step 62130, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 66h:28m:27s remains)
INFO - root - 2017-12-06 01:54:35.713303: step 62140, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 69h:22m:40s remains)
INFO - root - 2017-12-06 01:54:45.147905: step 62150, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 61h:20m:29s remains)
INFO - root - 2017-12-06 01:54:54.666352: step 62160, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 71h:10m:34s remains)
INFO - root - 2017-12-06 01:55:03.964988: step 62170, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 70h:53m:46s remains)
INFO - root - 2017-12-06 01:55:13.312902: step 62180, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 69h:51m:38s remains)
INFO - root - 2017-12-06 01:55:22.470296: step 62190, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 63h:56m:41s remains)
INFO - root - 2017-12-06 01:55:31.856972: step 62200, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 68h:14m:24s remains)
2017-12-06 01:55:32.696763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2522168 -4.2734575 -4.2753606 -4.2463374 -4.2024226 -4.1628823 -4.1335697 -4.1375794 -4.1757927 -4.2221036 -4.258822 -4.2526989 -4.2138405 -4.1730223 -4.153553][-4.2394776 -4.2598157 -4.2598248 -4.2296753 -4.1816311 -4.1408434 -4.1105103 -4.1159148 -4.1630983 -4.2164345 -4.2523203 -4.2367539 -4.1856937 -4.1293936 -4.1083956][-4.2332845 -4.2499008 -4.2466474 -4.2197318 -4.1674838 -4.1174874 -4.0832939 -4.0916471 -4.1489511 -4.2085652 -4.2468586 -4.2286372 -4.1706252 -4.1048889 -4.081677][-4.2137136 -4.2239919 -4.2246475 -4.2030206 -4.1531076 -4.0947576 -4.0509295 -4.0622964 -4.1290865 -4.1933217 -4.2293582 -4.2148709 -4.1666031 -4.1040754 -4.0766273][-4.1815248 -4.191431 -4.1984897 -4.1818852 -4.143116 -4.0862556 -4.0306392 -4.0362287 -4.1076851 -4.1735029 -4.2058458 -4.2005382 -4.1696196 -4.1188331 -4.0839891][-4.1518655 -4.1675372 -4.175406 -4.159966 -4.1275525 -4.0759087 -4.0102358 -4.0094442 -4.0901313 -4.1569982 -4.1864858 -4.1866283 -4.1666441 -4.1323113 -4.0979738][-4.1305852 -4.1446748 -4.1496959 -4.135839 -4.1044359 -4.0490732 -3.9688869 -3.9731522 -4.0741386 -4.1442394 -4.1744895 -4.1788006 -4.1671839 -4.1494937 -4.1292257][-4.1123633 -4.1242328 -4.1305819 -4.1192417 -4.0822768 -4.0140147 -3.91443 -3.9229395 -4.0456281 -4.129662 -4.1699777 -4.1841288 -4.1815639 -4.1736212 -4.1679535][-4.0966315 -4.1110063 -4.1236706 -4.11537 -4.0754914 -4.0025382 -3.9034855 -3.9160085 -4.0361891 -4.1219978 -4.1630635 -4.1832085 -4.1886339 -4.1868215 -4.182909][-4.0893931 -4.1026177 -4.119226 -4.1166992 -4.0904031 -4.0381274 -3.9703362 -3.9871533 -4.0749912 -4.1304803 -4.14901 -4.1631532 -4.170969 -4.1737537 -4.1670966][-4.0937934 -4.0996437 -4.1157894 -4.1262007 -4.1194777 -4.0986304 -4.0663881 -4.0799079 -4.1248846 -4.1441774 -4.1359591 -4.1406903 -4.15289 -4.1575069 -4.1456304][-4.1081581 -4.1105256 -4.1276054 -4.1455603 -4.152112 -4.1551313 -4.1475649 -4.1550965 -4.1680741 -4.1599007 -4.1390381 -4.1359863 -4.1494126 -4.1514034 -4.1294732][-4.14103 -4.1450477 -4.1591268 -4.1724019 -4.1786833 -4.1864557 -4.1893158 -4.1920195 -4.1898971 -4.1729178 -4.1528621 -4.1459327 -4.1531081 -4.1520271 -4.1277027][-4.1879821 -4.1895685 -4.1932125 -4.1932473 -4.1918154 -4.1995587 -4.2118473 -4.2141109 -4.2115817 -4.1988478 -4.1805844 -4.1681218 -4.1685119 -4.1692934 -4.1500187][-4.226872 -4.2255211 -4.2272677 -4.2209897 -4.2172933 -4.2277522 -4.2423377 -4.2435 -4.2419906 -4.2339725 -4.2228379 -4.2115569 -4.2089891 -4.2084651 -4.19104]]...]
INFO - root - 2017-12-06 01:55:41.979027: step 62210, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 71h:24m:14s remains)
INFO - root - 2017-12-06 01:55:51.307263: step 62220, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 68h:46m:52s remains)
INFO - root - 2017-12-06 01:56:00.595812: step 62230, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 70h:40m:14s remains)
INFO - root - 2017-12-06 01:56:10.133006: step 62240, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 73h:15m:28s remains)
INFO - root - 2017-12-06 01:56:19.354662: step 62250, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 70h:03m:03s remains)
INFO - root - 2017-12-06 01:56:28.641723: step 62260, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 68h:23m:39s remains)
INFO - root - 2017-12-06 01:56:38.032686: step 62270, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 73h:04m:56s remains)
INFO - root - 2017-12-06 01:56:47.394812: step 62280, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 71h:16m:07s remains)
INFO - root - 2017-12-06 01:56:56.701964: step 62290, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 71h:05m:54s remains)
INFO - root - 2017-12-06 01:57:06.075894: step 62300, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.948 sec/batch; 71h:08m:03s remains)
2017-12-06 01:57:06.846443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2543797 -4.2644172 -4.270988 -4.261992 -4.2404947 -4.2151723 -4.1920409 -4.1822109 -4.1910524 -4.2043223 -4.216012 -4.2242785 -4.2259631 -4.2291441 -4.2420688][-4.2829604 -4.2859907 -4.2806706 -4.2558827 -4.21749 -4.1780944 -4.1479278 -4.1446157 -4.1673365 -4.1909552 -4.2126732 -4.2288404 -4.2310658 -4.2297873 -4.2367725][-4.301003 -4.2986956 -4.2837663 -4.2436814 -4.1851716 -4.1280165 -4.089982 -4.0937996 -4.1305881 -4.1677551 -4.202558 -4.2289252 -4.236969 -4.2375426 -4.2443881][-4.30028 -4.2976122 -4.2824569 -4.2377381 -4.1646042 -4.091177 -4.0456104 -4.0533738 -4.09962 -4.1501985 -4.1969814 -4.2319589 -4.245142 -4.2508645 -4.2616229][-4.2901154 -4.2896948 -4.2772546 -4.2322292 -4.1529508 -4.0696797 -4.019423 -4.0244 -4.0757055 -4.1416268 -4.1988311 -4.2380252 -4.255331 -4.265769 -4.2797966][-4.2774014 -4.281837 -4.2730088 -4.229147 -4.1493654 -4.0613832 -4.0005713 -3.9909823 -4.03863 -4.1215148 -4.1949825 -4.2437191 -4.268918 -4.2835255 -4.2979527][-4.2798333 -4.2869544 -4.2812853 -4.2405014 -4.1631875 -4.0736237 -3.9996538 -3.9671865 -4.001225 -4.0900421 -4.1770072 -4.2385721 -4.27553 -4.299149 -4.3159685][-4.2951522 -4.3041048 -4.29811 -4.2581406 -4.1827035 -4.094759 -4.0160966 -3.9711213 -3.9881725 -4.067862 -4.1567969 -4.227057 -4.2756324 -4.3085203 -4.3297243][-4.3083673 -4.3186975 -4.3066015 -4.2608304 -4.1820612 -4.0917072 -4.0112004 -3.9662318 -3.9788685 -4.050704 -4.1384411 -4.2168684 -4.2756481 -4.3134089 -4.3351483][-4.3087616 -4.3176212 -4.2947812 -4.2386317 -4.1535134 -4.0604167 -3.9857097 -3.9531391 -3.9763324 -4.0488229 -4.1365552 -4.2214231 -4.2852569 -4.3217397 -4.3388515][-4.3005142 -4.3040638 -4.2716 -4.2092843 -4.1244006 -4.0380621 -3.9826291 -3.9708409 -4.0078306 -4.0794487 -4.1628294 -4.2447071 -4.3035412 -4.3320026 -4.3417888][-4.2798338 -4.2825966 -4.2499046 -4.1927533 -4.1170678 -4.0494204 -4.0194759 -4.0301056 -4.0751543 -4.1379867 -4.2065115 -4.2741122 -4.3209658 -4.3395472 -4.3424263][-4.265605 -4.2743406 -4.2522712 -4.2072859 -4.1460209 -4.0982733 -4.0895796 -4.1108756 -4.151588 -4.1991444 -4.2480464 -4.2976704 -4.3304315 -4.3401513 -4.3374138][-4.2709413 -4.2877316 -4.2789612 -4.2489786 -4.2036896 -4.1727543 -4.173481 -4.1921835 -4.2219524 -4.2514277 -4.28027 -4.3140416 -4.3359218 -4.3387575 -4.3304462][-4.29112 -4.3089547 -4.3061047 -4.2864723 -4.2564812 -4.2393007 -4.2428036 -4.255156 -4.2727294 -4.2872839 -4.3021917 -4.324295 -4.3386049 -4.3359146 -4.3224082]]...]
INFO - root - 2017-12-06 01:57:16.187237: step 62310, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 75h:34m:21s remains)
INFO - root - 2017-12-06 01:57:25.576958: step 62320, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 73h:05m:15s remains)
INFO - root - 2017-12-06 01:57:35.026197: step 62330, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 73h:56m:03s remains)
INFO - root - 2017-12-06 01:57:44.356737: step 62340, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 72h:10m:15s remains)
INFO - root - 2017-12-06 01:57:53.903142: step 62350, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 73h:56m:52s remains)
INFO - root - 2017-12-06 01:58:03.357516: step 62360, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.979 sec/batch; 73h:25m:40s remains)
INFO - root - 2017-12-06 01:58:12.734891: step 62370, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 70h:05m:27s remains)
INFO - root - 2017-12-06 01:58:22.240041: step 62380, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 70h:29m:03s remains)
INFO - root - 2017-12-06 01:58:31.685850: step 62390, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.972 sec/batch; 72h:57m:08s remains)
INFO - root - 2017-12-06 01:58:41.045321: step 62400, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 68h:46m:51s remains)
2017-12-06 01:58:41.754042: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1773286 -4.1927838 -4.2069759 -4.2108021 -4.2125568 -4.2163854 -4.2155142 -4.2040772 -4.1903906 -4.184906 -4.19223 -4.1933451 -4.1902547 -4.1941185 -4.20162][-4.1674461 -4.1835275 -4.1978221 -4.1979074 -4.196166 -4.1996412 -4.2024779 -4.1980238 -4.1908307 -4.1866961 -4.19196 -4.19058 -4.1848259 -4.1857238 -4.1935625][-4.1816616 -4.1963296 -4.2031088 -4.191802 -4.1756768 -4.1679225 -4.1693072 -4.173902 -4.1775646 -4.1787124 -4.1844764 -4.1874723 -4.1853905 -4.1882229 -4.1959577][-4.2052488 -4.2124205 -4.2074461 -4.1829429 -4.1451015 -4.116292 -4.116107 -4.1322751 -4.1445718 -4.1528978 -4.1663465 -4.1823993 -4.1903853 -4.1940765 -4.1971369][-4.2205973 -4.2168183 -4.1986961 -4.1603794 -4.103775 -4.0524926 -4.0433965 -4.0665369 -4.0869994 -4.1080894 -4.1391969 -4.1736135 -4.1937933 -4.2032142 -4.2037406][-4.2175851 -4.2067 -4.1733055 -4.11811 -4.0447512 -3.9796562 -3.960948 -3.987227 -4.020606 -4.0651822 -4.1186934 -4.1702895 -4.2025652 -4.2206416 -4.2200637][-4.1889935 -4.1779733 -4.1367955 -4.0721574 -3.9948342 -3.9224102 -3.890177 -3.913358 -3.9623332 -4.0274916 -4.0947523 -4.15708 -4.1981034 -4.2233438 -4.2309055][-4.1516209 -4.14478 -4.1115775 -4.0543523 -3.9871502 -3.9118295 -3.8591866 -3.8721848 -3.9306781 -4.0084729 -4.0786629 -4.141202 -4.1870937 -4.22041 -4.2393813][-4.1267633 -4.12185 -4.1036134 -4.0660982 -4.0172791 -3.9456391 -3.8859782 -3.8859484 -3.9432 -4.0222759 -4.0856237 -4.1395445 -4.1822877 -4.2167149 -4.2395763][-4.1203179 -4.1160288 -4.1109066 -4.0941868 -4.0675206 -4.0149627 -3.9624197 -3.9523606 -3.9948015 -4.0590897 -4.1061721 -4.1401143 -4.1695738 -4.1998458 -4.2204304][-4.1145182 -4.1109996 -4.1119585 -4.1111021 -4.1006432 -4.0724096 -4.0397396 -4.0194612 -4.0384188 -4.0800695 -4.1110954 -4.130362 -4.1457348 -4.1696215 -4.1846676][-4.1081276 -4.1164918 -4.1262903 -4.1326704 -4.1313529 -4.1214108 -4.10286 -4.0771847 -4.0709724 -4.0897703 -4.1107383 -4.1250396 -4.1328211 -4.1493731 -4.1629248][-4.1195011 -4.1368332 -4.1530066 -4.1635394 -4.1651826 -4.1614728 -4.1493158 -4.1210332 -4.0974784 -4.0969377 -4.1083565 -4.1219568 -4.1285896 -4.1441088 -4.1632218][-4.1677532 -4.1815667 -4.1945 -4.2040997 -4.206975 -4.2043204 -4.1909189 -4.1604948 -4.1269803 -4.1134157 -4.1185393 -4.1307969 -4.1359024 -4.1513319 -4.178071][-4.233233 -4.2395616 -4.2444148 -4.2496152 -4.2524695 -4.2507243 -4.2386522 -4.2109613 -4.1778526 -4.1615129 -4.1666603 -4.1766791 -4.1764703 -4.1879249 -4.2167053]]...]
INFO - root - 2017-12-06 01:58:51.206445: step 62410, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 69h:27m:44s remains)
INFO - root - 2017-12-06 01:59:00.316677: step 62420, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.965 sec/batch; 72h:25m:05s remains)
INFO - root - 2017-12-06 01:59:09.388235: step 62430, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 68h:37m:40s remains)
INFO - root - 2017-12-06 01:59:18.903926: step 62440, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 72h:06m:21s remains)
INFO - root - 2017-12-06 01:59:28.404063: step 62450, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 72h:23m:20s remains)
INFO - root - 2017-12-06 01:59:37.630593: step 62460, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 72h:02m:01s remains)
INFO - root - 2017-12-06 01:59:46.756413: step 62470, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.909 sec/batch; 68h:11m:25s remains)
INFO - root - 2017-12-06 01:59:55.901322: step 62480, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 65h:51m:06s remains)
INFO - root - 2017-12-06 02:00:05.185381: step 62490, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 73h:43m:52s remains)
INFO - root - 2017-12-06 02:00:14.497846: step 62500, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.008 sec/batch; 75h:35m:28s remains)
2017-12-06 02:00:15.296775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2056618 -4.2115974 -4.2336679 -4.2518 -4.2579784 -4.2505493 -4.2289996 -4.210649 -4.2012677 -4.2077594 -4.2175331 -4.2118545 -4.2165089 -4.2325835 -4.2530723][-4.1411262 -4.1444716 -4.1786547 -4.2121549 -4.2248917 -4.2193141 -4.194736 -4.171638 -4.1571813 -4.1666632 -4.1816916 -4.1761484 -4.1818037 -4.1985803 -4.2204161][-4.0724788 -4.0692968 -4.110775 -4.1510563 -4.1675544 -4.163517 -4.1408415 -4.1151876 -4.1018777 -4.11949 -4.1400304 -4.1380844 -4.1444793 -4.1605206 -4.18408][-4.030951 -4.0278282 -4.0726352 -4.1115756 -4.1276259 -4.1217175 -4.098784 -4.0746737 -4.0671744 -4.0924225 -4.1155005 -4.122664 -4.1317735 -4.1438255 -4.1662283][-4.0337343 -4.0335407 -4.0738635 -4.105947 -4.1216464 -4.1166029 -4.0934744 -4.0695472 -4.0631433 -4.0910244 -4.1157403 -4.1280828 -4.140141 -4.1533933 -4.1728196][-4.0266232 -4.0280185 -4.0660591 -4.0958428 -4.1174979 -4.1150031 -4.0811305 -4.0429382 -4.0351405 -4.0766625 -4.118012 -4.1386924 -4.1575847 -4.1759849 -4.1901317][-4.0020542 -4.0023212 -4.0430789 -4.0755553 -4.1003852 -4.0870609 -4.0173364 -3.9388123 -3.9299169 -4.0064964 -4.0846291 -4.1302629 -4.1656594 -4.191577 -4.2019539][-3.9727397 -3.9754229 -4.0193977 -4.0580025 -4.0811319 -4.0432959 -3.9252799 -3.8001809 -3.7937922 -3.9041076 -4.01429 -4.0806947 -4.1293988 -4.1684976 -4.1846347][-3.9654415 -3.9746659 -4.0217938 -4.0682416 -4.0958276 -4.0556564 -3.935791 -3.8171816 -3.8117857 -3.9041586 -3.9969265 -4.0481739 -4.0888376 -4.1292238 -4.1508279][-4.0080352 -4.0158281 -4.0545855 -4.0980244 -4.1294508 -4.1057267 -4.0236745 -3.9448817 -3.9430423 -3.9975476 -4.0534735 -4.0753455 -4.0931334 -4.1195087 -4.1414742][-4.0724325 -4.0744767 -4.1013961 -4.1358252 -4.1687012 -4.1619024 -4.1152587 -4.0698981 -4.0702887 -4.0988193 -4.1280284 -4.1282754 -4.128293 -4.1396542 -4.1580729][-4.1514859 -4.15061 -4.1698141 -4.1944528 -4.2216115 -4.2244291 -4.1990623 -4.1712623 -4.1711159 -4.187376 -4.20501 -4.1978612 -4.1892147 -4.1913481 -4.2039208][-4.2297635 -4.2295847 -4.2440968 -4.2632537 -4.2845221 -4.2869368 -4.266705 -4.2446065 -4.2422571 -4.2548294 -4.2677679 -4.2634583 -4.2548141 -4.2535596 -4.2593169][-4.2922516 -4.2938795 -4.3021736 -4.3158455 -4.3308516 -4.3290467 -4.3105326 -4.291266 -4.2876859 -4.2947068 -4.3022518 -4.299819 -4.296526 -4.2962518 -4.2982564][-4.32516 -4.3250957 -4.3285031 -4.3364773 -4.3441868 -4.3408027 -4.3294106 -4.3195038 -4.3159752 -4.3175569 -4.3210778 -4.3205376 -4.3206196 -4.3226166 -4.3246336]]...]
INFO - root - 2017-12-06 02:00:24.863437: step 62510, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 70h:43m:01s remains)
INFO - root - 2017-12-06 02:00:34.207927: step 62520, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 73h:33m:11s remains)
INFO - root - 2017-12-06 02:00:43.731460: step 62530, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 72h:27m:44s remains)
INFO - root - 2017-12-06 02:00:53.262745: step 62540, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 70h:43m:30s remains)
INFO - root - 2017-12-06 02:01:02.612495: step 62550, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 69h:00m:05s remains)
INFO - root - 2017-12-06 02:01:11.620212: step 62560, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 70h:30m:09s remains)
INFO - root - 2017-12-06 02:01:20.916989: step 62570, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 73h:16m:18s remains)
INFO - root - 2017-12-06 02:01:30.300461: step 62580, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 66h:29m:55s remains)
INFO - root - 2017-12-06 02:01:39.631723: step 62590, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.968 sec/batch; 72h:32m:40s remains)
INFO - root - 2017-12-06 02:01:48.908375: step 62600, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 66h:53m:51s remains)
2017-12-06 02:01:49.687117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3571358 -4.3530278 -4.3321385 -4.30573 -4.2805867 -4.2649608 -4.2628636 -4.2689333 -4.2816544 -4.2975287 -4.3109531 -4.3219709 -4.3278503 -4.3323507 -4.3355064][-4.3586917 -4.3492708 -4.3184414 -4.279357 -4.2403932 -4.2147555 -4.2076273 -4.2139325 -4.2336717 -4.25929 -4.2843008 -4.3076153 -4.3224759 -4.3310189 -4.3348284][-4.3507276 -4.3356934 -4.2982073 -4.2456617 -4.1917129 -4.1541252 -4.1422315 -4.1499887 -4.1779885 -4.2135282 -4.25093 -4.2840242 -4.3075347 -4.3235931 -4.3313046][-4.334456 -4.3146911 -4.2728171 -4.2089767 -4.1390057 -4.088274 -4.0688334 -4.0811834 -4.1239347 -4.1691413 -4.2140565 -4.2540035 -4.2825804 -4.3061352 -4.3213596][-4.3093953 -4.2899337 -4.2424989 -4.1636248 -4.0673275 -3.9856184 -3.9431822 -3.9629683 -4.0377183 -4.1085992 -4.164897 -4.2097025 -4.2467546 -4.2792616 -4.3012347][-4.2774162 -4.2626095 -4.210505 -4.1140075 -3.9904673 -3.8613305 -3.7721024 -3.7951381 -3.9165649 -4.0274386 -4.1045475 -4.1599517 -4.2019029 -4.2395525 -4.2645464][-4.2358518 -4.2262716 -4.1824083 -4.0827193 -3.9424589 -3.7662213 -3.6066613 -3.5983491 -3.7622821 -3.920511 -4.028461 -4.103107 -4.1527224 -4.1920829 -4.2174931][-4.2167096 -4.2096839 -4.1766429 -4.0972614 -3.9726691 -3.7930944 -3.60253 -3.5415521 -3.6798809 -3.8426259 -3.9664958 -4.059361 -4.1173453 -4.1513295 -4.1684871][-4.2270813 -4.2231994 -4.2043056 -4.1551323 -4.07322 -3.9481595 -3.8162355 -3.7604718 -3.8149085 -3.9072633 -3.9999712 -4.0816479 -4.1286135 -4.1472464 -4.1501651][-4.2485075 -4.2438545 -4.2317009 -4.2039061 -4.1590514 -4.0960979 -4.0372481 -4.0106349 -4.0202556 -4.0530653 -4.106225 -4.1617479 -4.1888504 -4.1913 -4.1831975][-4.278769 -4.2759776 -4.2694583 -4.2538261 -4.2309923 -4.2021918 -4.1833105 -4.1772537 -4.177228 -4.1892753 -4.215425 -4.2406812 -4.2481656 -4.2434144 -4.23456][-4.3055434 -4.30546 -4.3037329 -4.2992048 -4.2899375 -4.2765083 -4.2703028 -4.269341 -4.2710705 -4.2779202 -4.29002 -4.2952657 -4.2893519 -4.2832417 -4.2754474][-4.3227596 -4.3266497 -4.3281159 -4.3278923 -4.3258238 -4.3197589 -4.31553 -4.3149223 -4.3176703 -4.3248019 -4.3306131 -4.3273382 -4.3178072 -4.3094821 -4.3027487][-4.3366628 -4.3392262 -4.3399134 -4.3396111 -4.3390355 -4.3368354 -4.3330321 -4.3305335 -4.3322234 -4.3380022 -4.3428984 -4.339972 -4.3330474 -4.326962 -4.3220897][-4.3415809 -4.3433375 -4.3439827 -4.3440537 -4.3428354 -4.3411441 -4.3383055 -4.3368435 -4.338541 -4.3414049 -4.3432164 -4.3420014 -4.339725 -4.3381505 -4.3369608]]...]
INFO - root - 2017-12-06 02:01:59.045037: step 62610, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 73h:57m:30s remains)
INFO - root - 2017-12-06 02:02:08.659798: step 62620, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 70h:45m:23s remains)
INFO - root - 2017-12-06 02:02:18.047062: step 62630, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.946 sec/batch; 70h:56m:38s remains)
INFO - root - 2017-12-06 02:02:27.581629: step 62640, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 72h:25m:41s remains)
INFO - root - 2017-12-06 02:02:37.067698: step 62650, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 72h:32m:35s remains)
INFO - root - 2017-12-06 02:02:46.201248: step 62660, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 67h:31m:00s remains)
INFO - root - 2017-12-06 02:02:55.718584: step 62670, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.970 sec/batch; 72h:42m:58s remains)
INFO - root - 2017-12-06 02:03:04.872817: step 62680, loss = 2.11, batch loss = 2.05 (8.9 examples/sec; 0.903 sec/batch; 67h:40m:46s remains)
INFO - root - 2017-12-06 02:03:14.072395: step 62690, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 71h:37m:37s remains)
INFO - root - 2017-12-06 02:03:23.098444: step 62700, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 63h:37m:00s remains)
2017-12-06 02:03:23.862657: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3145938 -4.3080721 -4.2909212 -4.26923 -4.2531447 -4.2447972 -4.244205 -4.2533484 -4.2691112 -4.2805381 -4.2856555 -4.2887859 -4.2853374 -4.2773933 -4.2677436][-4.3018613 -4.2990026 -4.2854447 -4.265142 -4.2461276 -4.2321243 -4.2265363 -4.2341423 -4.2523637 -4.2670455 -4.2776461 -4.2836208 -4.2804718 -4.2687526 -4.2538056][-4.2811947 -4.2869987 -4.2819681 -4.2612677 -4.231678 -4.20391 -4.1881881 -4.1949997 -4.2178278 -4.2413135 -4.2619953 -4.275794 -4.2799678 -4.2721782 -4.2556934][-4.266892 -4.2808342 -4.2842402 -4.2583222 -4.2131052 -4.1652856 -4.13197 -4.1399722 -4.1754518 -4.2100844 -4.2398949 -4.2612481 -4.2746887 -4.2743106 -4.2604623][-4.2609024 -4.2787771 -4.2873893 -4.2553959 -4.1937366 -4.1206646 -4.0658464 -4.0772943 -4.1304145 -4.1781497 -4.2134066 -4.2374125 -4.25805 -4.2643785 -4.2576356][-4.2542653 -4.2710247 -4.2809715 -4.2423654 -4.1689587 -4.0705485 -3.989841 -4.0074859 -4.0818048 -4.1466928 -4.1919432 -4.2224913 -4.2489023 -4.2594934 -4.2620325][-4.2508235 -4.2620006 -4.2666836 -4.2186809 -4.1312408 -3.9975729 -3.8836613 -3.919538 -4.0245843 -4.1129813 -4.1731648 -4.2148542 -4.249629 -4.2654438 -4.2767649][-4.2592649 -4.2582111 -4.24892 -4.185267 -4.0736594 -3.8934937 -3.7421579 -3.8137581 -3.9615107 -4.0778618 -4.1573381 -4.2112989 -4.252274 -4.2757015 -4.2924476][-4.2677751 -4.2538829 -4.2293277 -4.1509366 -4.019712 -3.8150511 -3.6548491 -3.75743 -3.9297698 -4.0578008 -4.1500092 -4.2115679 -4.2544055 -4.2846355 -4.3020959][-4.274086 -4.2549844 -4.2290969 -4.1558056 -4.0421777 -3.8744564 -3.7522261 -3.8420491 -3.9848952 -4.0849652 -4.1652446 -4.2221761 -4.2640939 -4.2952638 -4.3108468][-4.2831216 -4.2657475 -4.2416515 -4.1800222 -4.090045 -3.9725261 -3.8929691 -3.9552953 -4.0562658 -4.1254349 -4.1918082 -4.2410288 -4.2794733 -4.3068371 -4.3186092][-4.2919126 -4.2745748 -4.2521415 -4.2026625 -4.1309648 -4.053401 -4.0080652 -4.0523496 -4.1225233 -4.1698546 -4.2201395 -4.2575345 -4.2903652 -4.3139248 -4.3222561][-4.3029146 -4.2841415 -4.2678318 -4.2297783 -4.1747918 -4.1271253 -4.1089506 -4.1415205 -4.1890697 -4.2252512 -4.25897 -4.2830968 -4.3075142 -4.3235083 -4.3275018][-4.3092952 -4.2888374 -4.2773805 -4.2551317 -4.2190118 -4.1958776 -4.1967182 -4.2212582 -4.2509632 -4.277144 -4.2946157 -4.3058004 -4.3219051 -4.3321924 -4.3324261][-4.3135719 -4.2946281 -4.2895894 -4.283164 -4.265162 -4.2557826 -4.2655334 -4.2832737 -4.3002987 -4.3157444 -4.3208404 -4.3235526 -4.3334994 -4.3391895 -4.3356829]]...]
INFO - root - 2017-12-06 02:03:33.167175: step 62710, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 69h:28m:02s remains)
INFO - root - 2017-12-06 02:03:42.715794: step 62720, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 72h:51m:38s remains)
INFO - root - 2017-12-06 02:03:52.128766: step 62730, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 70h:21m:49s remains)
INFO - root - 2017-12-06 02:04:01.581869: step 62740, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 72h:11m:13s remains)
INFO - root - 2017-12-06 02:04:10.791832: step 62750, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 65h:24m:52s remains)
INFO - root - 2017-12-06 02:04:20.108182: step 62760, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 68h:50m:53s remains)
INFO - root - 2017-12-06 02:04:29.477885: step 62770, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 69h:51m:10s remains)
INFO - root - 2017-12-06 02:04:38.911947: step 62780, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 74h:20m:22s remains)
INFO - root - 2017-12-06 02:04:48.225692: step 62790, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 71h:32m:17s remains)
INFO - root - 2017-12-06 02:04:57.696202: step 62800, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 69h:56m:34s remains)
2017-12-06 02:04:58.491618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.231935 -4.238019 -4.2402411 -4.22446 -4.203753 -4.1831379 -4.1888542 -4.2012587 -4.2136226 -4.2386708 -4.24946 -4.2290597 -4.1935153 -4.1755471 -4.1833215][-4.2003255 -4.1983094 -4.1939931 -4.1794686 -4.1601686 -4.1397576 -4.1487169 -4.1671576 -4.1834841 -4.2052703 -4.2152505 -4.1977968 -4.1598678 -4.1364808 -4.142684][-4.1581721 -4.1505589 -4.1421709 -4.1337638 -4.1189876 -4.0962176 -4.10341 -4.1212597 -4.1371541 -4.1556754 -4.1673865 -4.1559238 -4.1159706 -4.0881853 -4.0931411][-4.1173525 -4.0993204 -4.0884728 -4.0853896 -4.0730138 -4.0482006 -4.05887 -4.0795245 -4.097414 -4.1166644 -4.1340389 -4.1256237 -4.0841427 -4.0546842 -4.0588622][-4.110013 -4.0737243 -4.0523968 -4.0469995 -4.0320277 -4.0016007 -4.0123425 -4.041605 -4.0738397 -4.1061969 -4.1320143 -4.1290932 -4.0865974 -4.0565104 -4.0595236][-4.1448083 -4.09419 -4.0605679 -4.0426679 -4.0192461 -3.978292 -3.9757223 -4.0084271 -4.0509124 -4.095376 -4.1333227 -4.1432614 -4.1100464 -4.0850606 -4.0881042][-4.1674075 -4.1144552 -4.0714602 -4.0411119 -4.0147409 -3.9693227 -3.9528422 -3.982296 -4.0269413 -4.0764794 -4.1187978 -4.1411529 -4.1206889 -4.1047764 -4.1137428][-4.1646309 -4.1178055 -4.0676713 -4.0280352 -4.0047846 -3.9646757 -3.9497082 -3.9753633 -4.0169382 -4.0645881 -4.0991173 -4.1182728 -4.1049542 -4.0977578 -4.117857][-4.1574554 -4.1263742 -4.08137 -4.0428338 -4.023509 -3.9910984 -3.9834735 -4.0082917 -4.0449629 -4.0822124 -4.0988178 -4.0983782 -4.0823073 -4.07854 -4.1024675][-4.1508384 -4.1323714 -4.0996923 -4.066833 -4.0484142 -4.0221353 -4.0188694 -4.0490494 -4.0809131 -4.1066909 -4.1060562 -4.0838847 -4.0602055 -4.0500321 -4.0696959][-4.142971 -4.1301441 -4.10164 -4.0725741 -4.056478 -4.0376463 -4.0378809 -4.0723653 -4.108449 -4.1274023 -4.1168704 -4.0790386 -4.0455527 -4.0228419 -4.0352612][-4.1559067 -4.1455879 -4.1152725 -4.0861897 -4.06925 -4.0597887 -4.0675416 -4.1002932 -4.1361809 -4.1496196 -4.12918 -4.0816803 -4.0456872 -4.0243125 -4.0369205][-4.1916943 -4.18268 -4.15468 -4.126955 -4.113204 -4.1167 -4.1349964 -4.1598496 -4.1844573 -4.1870689 -4.1600223 -4.1139784 -4.0839972 -4.0727372 -4.0870671][-4.2323937 -4.2250361 -4.2051487 -4.1829658 -4.1696472 -4.1788726 -4.2035046 -4.2228694 -4.2353387 -4.23294 -4.2093344 -4.1704397 -4.1430197 -4.1335487 -4.1445994][-4.2694597 -4.2621961 -4.2493572 -4.2310939 -4.2133527 -4.2157712 -4.2361889 -4.2526765 -4.2610712 -4.2614489 -4.24852 -4.2203541 -4.1950755 -4.1836395 -4.1927104]]...]
INFO - root - 2017-12-06 02:05:07.938204: step 62810, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 64h:25m:26s remains)
INFO - root - 2017-12-06 02:05:17.452160: step 62820, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 70h:47m:25s remains)
INFO - root - 2017-12-06 02:05:26.573739: step 62830, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 65h:55m:39s remains)
INFO - root - 2017-12-06 02:05:35.946714: step 62840, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 66h:21m:47s remains)
INFO - root - 2017-12-06 02:05:45.191761: step 62850, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.890 sec/batch; 66h:41m:18s remains)
INFO - root - 2017-12-06 02:05:54.501710: step 62860, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 70h:52m:58s remains)
INFO - root - 2017-12-06 02:06:03.945767: step 62870, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 71h:41m:40s remains)
INFO - root - 2017-12-06 02:06:13.312263: step 62880, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 71h:03m:35s remains)
INFO - root - 2017-12-06 02:06:22.784961: step 62890, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 70h:32m:35s remains)
INFO - root - 2017-12-06 02:06:32.161997: step 62900, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 72h:03m:26s remains)
2017-12-06 02:06:32.970971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1856093 -4.2051668 -4.2229915 -4.2287736 -4.2233391 -4.2040572 -4.1790795 -4.1471834 -4.1191459 -4.0897412 -4.0686531 -4.0758514 -4.0831308 -4.0901747 -4.0940485][-4.1759653 -4.1946011 -4.2130804 -4.2164884 -4.2132988 -4.1984391 -4.1792521 -4.1502757 -4.1187172 -4.0888047 -4.0646639 -4.0617723 -4.063139 -4.0697737 -4.0765276][-4.1511374 -4.159853 -4.1669469 -4.16895 -4.1734781 -4.1723771 -4.1604204 -4.1396008 -4.1111927 -4.0827136 -4.0623326 -4.0602889 -4.0697627 -4.0800238 -4.084672][-4.1030602 -4.103435 -4.1071072 -4.1134439 -4.1268072 -4.1354785 -4.1322803 -4.1238127 -4.1067495 -4.0908146 -4.0828962 -4.0914145 -4.1059003 -4.1131153 -4.110486][-4.0486622 -4.0446043 -4.0436993 -4.057548 -4.0759296 -4.0864339 -4.0880017 -4.08594 -4.0820432 -4.0879955 -4.1008396 -4.1168752 -4.1321168 -4.1332927 -4.12702][-4.0245256 -4.0086012 -3.9958913 -4.0002608 -4.0125036 -4.0163164 -4.0150738 -4.0090785 -4.0184159 -4.0505972 -4.0829287 -4.1068292 -4.1231718 -4.1252503 -4.1258459][-4.0224056 -4.0142293 -3.9949031 -3.9777052 -3.9619153 -3.9355645 -3.9070785 -3.8755364 -3.9042442 -3.9735639 -4.032167 -4.0661159 -4.0812697 -4.0842667 -4.0889487][-4.0257926 -4.0345254 -4.0272441 -4.0018735 -3.9671483 -3.9084489 -3.8397264 -3.7697737 -3.8060865 -3.9013548 -3.9753814 -4.013803 -4.02456 -4.0273523 -4.0313959][-4.0484352 -4.0724745 -4.0892048 -4.07379 -4.0432091 -3.9879463 -3.9187906 -3.854907 -3.8699617 -3.9274313 -3.967346 -3.981715 -3.9783151 -3.980593 -3.9813139][-4.0880451 -4.1203384 -4.1486049 -4.1423345 -4.1241617 -4.0890436 -4.0406427 -3.9994011 -3.994935 -4.014833 -4.0204954 -4.0131354 -3.9963303 -3.9852366 -3.9731646][-4.1168127 -4.1484656 -4.1771016 -4.1787224 -4.1741724 -4.1568689 -4.1266904 -4.097496 -4.0843306 -4.0909009 -4.0875325 -4.0751486 -4.0570936 -4.0406723 -4.0168715][-4.0958495 -4.1257339 -4.1594563 -4.1790605 -4.1908779 -4.1877685 -4.1692309 -4.150753 -4.1405792 -4.1435075 -4.1333776 -4.1192422 -4.1052461 -4.0869555 -4.0632067][-4.0534382 -4.0840697 -4.1205196 -4.1498313 -4.1719394 -4.17981 -4.1729908 -4.1663523 -4.1633286 -4.166738 -4.1574135 -4.1477962 -4.1409116 -4.1260691 -4.1059933][-4.046649 -4.0750332 -4.1106105 -4.1406307 -4.1656947 -4.1760144 -4.1737852 -4.1714897 -4.1707449 -4.1716847 -4.1644797 -4.1621766 -4.1655664 -4.1587172 -4.1489973][-4.0675688 -4.0897851 -4.1207724 -4.1506381 -4.1769772 -4.1912112 -4.1931362 -4.192265 -4.1903858 -4.1856418 -4.1784825 -4.1791396 -4.1863532 -4.1854854 -4.1799579]]...]
INFO - root - 2017-12-06 02:06:42.320715: step 62910, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 70h:52m:13s remains)
INFO - root - 2017-12-06 02:06:51.518178: step 62920, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 74h:01m:42s remains)
INFO - root - 2017-12-06 02:07:01.012289: step 62930, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 72h:54m:53s remains)
INFO - root - 2017-12-06 02:07:10.407788: step 62940, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.999 sec/batch; 74h:47m:02s remains)
INFO - root - 2017-12-06 02:07:19.579221: step 62950, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 68h:14m:45s remains)
INFO - root - 2017-12-06 02:07:28.997382: step 62960, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 72h:48m:37s remains)
INFO - root - 2017-12-06 02:07:38.088998: step 62970, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 64h:50m:03s remains)
INFO - root - 2017-12-06 02:07:47.573721: step 62980, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.984 sec/batch; 73h:38m:55s remains)
INFO - root - 2017-12-06 02:07:56.988042: step 62990, loss = 2.12, batch loss = 2.06 (8.3 examples/sec; 0.962 sec/batch; 71h:58m:57s remains)
INFO - root - 2017-12-06 02:08:06.293041: step 63000, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 69h:53m:57s remains)
2017-12-06 02:08:07.040049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2093806 -4.2464728 -4.2620931 -4.260015 -4.260756 -4.2659726 -4.26301 -4.262063 -4.2676854 -4.2729473 -4.2762423 -4.2796378 -4.2847042 -4.2849631 -4.2716484][-4.1888428 -4.2378607 -4.2660151 -4.271935 -4.2722473 -4.2721167 -4.2712712 -4.2781272 -4.29138 -4.2989912 -4.299685 -4.2956648 -4.2937989 -4.2915936 -4.2774081][-4.1587934 -4.2156148 -4.2553415 -4.2712936 -4.2703657 -4.2632446 -4.2622046 -4.2769604 -4.2992835 -4.31129 -4.311244 -4.3024225 -4.2952623 -4.289485 -4.2734332][-4.1300607 -4.1835995 -4.2272182 -4.2488451 -4.2434454 -4.2261786 -4.2192707 -4.2398329 -4.27177 -4.2930822 -4.2979436 -4.2918868 -4.2842884 -4.2804804 -4.2704482][-4.1124182 -4.1527634 -4.1907334 -4.2092109 -4.1932955 -4.1575832 -4.13429 -4.1580677 -4.203917 -4.2402534 -4.259582 -4.2656236 -4.2637129 -4.2671371 -4.2692776][-4.1160717 -4.1391921 -4.1625476 -4.1653419 -4.1304092 -4.0691519 -4.0196881 -4.0431042 -4.1087155 -4.1675334 -4.2066779 -4.2285376 -4.2354136 -4.2454329 -4.2579346][-4.1383672 -4.1456637 -4.1494279 -4.1286588 -4.0695162 -3.9816933 -3.9043288 -3.9242663 -4.0106072 -4.0930095 -4.1502433 -4.184402 -4.2001257 -4.2168741 -4.2393322][-4.1650081 -4.1610026 -4.1498713 -4.1114016 -4.0345016 -3.9289196 -3.8341961 -3.8477137 -3.9467196 -4.0429244 -4.1084785 -4.1473517 -4.1671839 -4.1870375 -4.2147527][-4.1833224 -4.1771808 -4.1604214 -4.1198854 -4.0439167 -3.94199 -3.852354 -3.8563426 -3.9444926 -4.0352883 -4.0955224 -4.1297007 -4.1465449 -4.1647277 -4.1924262][-4.1989374 -4.1975355 -4.1849909 -4.1560311 -4.0982475 -4.0189791 -3.949738 -3.9428303 -3.9998755 -4.0662355 -4.1078482 -4.1299958 -4.1402044 -4.156024 -4.1793737][-4.2243824 -4.2278323 -4.2212605 -4.2045264 -4.1682992 -4.1152463 -4.0668354 -4.05371 -4.0796056 -4.1167326 -4.1381817 -4.1465068 -4.1467009 -4.1582441 -4.174335][-4.2583027 -4.2628474 -4.2606816 -4.2528262 -4.2326722 -4.2003551 -4.1692104 -4.1550174 -4.1609983 -4.1767807 -4.1845007 -4.1838412 -4.1757445 -4.1783309 -4.1840816][-4.2881641 -4.2899694 -4.2898669 -4.286943 -4.2780266 -4.2622151 -4.2463989 -4.2381077 -4.2374058 -4.2402186 -4.2398772 -4.2345619 -4.2225184 -4.2171831 -4.2137575][-4.3065338 -4.3052411 -4.3065734 -4.3072133 -4.3068495 -4.3034415 -4.3003697 -4.2992344 -4.2977061 -4.2953663 -4.2913046 -4.28475 -4.2730536 -4.2644062 -4.2549267][-4.3156228 -4.3106537 -4.3123941 -4.3169632 -4.3234916 -4.3286881 -4.3337035 -4.3367047 -4.3349876 -4.3307705 -4.3260236 -4.3213129 -4.3106594 -4.2996531 -4.2864108]]...]
INFO - root - 2017-12-06 02:08:16.429755: step 63010, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 73h:53m:01s remains)
INFO - root - 2017-12-06 02:08:25.978955: step 63020, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 72h:11m:12s remains)
INFO - root - 2017-12-06 02:08:35.213933: step 63030, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 74h:06m:10s remains)
INFO - root - 2017-12-06 02:08:44.324135: step 63040, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 68h:01m:11s remains)
INFO - root - 2017-12-06 02:08:53.817692: step 63050, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 69h:19m:15s remains)
INFO - root - 2017-12-06 02:09:03.199032: step 63060, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 72h:39m:02s remains)
INFO - root - 2017-12-06 02:09:12.450117: step 63070, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 69h:24m:53s remains)
INFO - root - 2017-12-06 02:09:21.831738: step 63080, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 68h:50m:57s remains)
INFO - root - 2017-12-06 02:09:31.296693: step 63090, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 69h:30m:59s remains)
INFO - root - 2017-12-06 02:09:40.616975: step 63100, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 69h:31m:22s remains)
2017-12-06 02:09:41.408129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2208438 -4.2143979 -4.2097445 -4.2001557 -4.2027431 -4.224544 -4.2231779 -4.2065434 -4.1849246 -4.1709852 -4.174881 -4.1818166 -4.2087803 -4.2414241 -4.2698793][-4.1924982 -4.1838694 -4.1842241 -4.1802473 -4.1823163 -4.20058 -4.197022 -4.180686 -4.1591377 -4.1496267 -4.1577573 -4.166873 -4.1972461 -4.2300339 -4.260571][-4.1568241 -4.1500978 -4.1595349 -4.1702328 -4.1765933 -4.18917 -4.1847916 -4.166585 -4.1422706 -4.1374879 -4.1462164 -4.15428 -4.1853991 -4.2182336 -4.2522154][-4.1477952 -4.1494155 -4.1645937 -4.1870923 -4.1961617 -4.2018204 -4.1914663 -4.1741586 -4.1477833 -4.1437 -4.1503515 -4.1610284 -4.1900105 -4.2178645 -4.2499027][-4.1911025 -4.2025061 -4.2177429 -4.235755 -4.2365279 -4.2279782 -4.2058806 -4.18507 -4.1610518 -4.1601119 -4.1669097 -4.181232 -4.2053962 -4.2275486 -4.2565746][-4.2340894 -4.2525635 -4.2637486 -4.2650118 -4.2443361 -4.2106514 -4.1701384 -4.1449809 -4.1375785 -4.1566114 -4.1736054 -4.193768 -4.2145772 -4.23778 -4.2646437][-4.25918 -4.272789 -4.2666793 -4.2441707 -4.1976843 -4.1327405 -4.06242 -4.0287218 -4.0503349 -4.1141553 -4.1612644 -4.1982789 -4.2232227 -4.2496672 -4.2725983][-4.2588129 -4.2650189 -4.2454619 -4.1982164 -4.1212468 -4.0205803 -3.9070792 -3.8522992 -3.9057567 -4.030056 -4.1243062 -4.1827793 -4.2175174 -4.2468944 -4.2700119][-4.229682 -4.2297521 -4.2106156 -4.1602135 -4.080328 -3.97491 -3.8464162 -3.7754784 -3.8397329 -3.9902267 -4.1094375 -4.17744 -4.2150416 -4.2449832 -4.2668533][-4.1918082 -4.1875281 -4.1791716 -4.1466722 -4.1000609 -4.0380335 -3.9612398 -3.9157999 -3.9562173 -4.06284 -4.1572742 -4.211535 -4.2393694 -4.2627058 -4.2806039][-4.2013154 -4.1965179 -4.1972923 -4.1841846 -4.1661868 -4.1420503 -4.1101809 -4.0863376 -4.1045818 -4.163002 -4.2183223 -4.2525177 -4.2679029 -4.2807951 -4.2937489][-4.2295165 -4.2243705 -4.2308135 -4.2293749 -4.2281041 -4.2250166 -4.2167106 -4.2049646 -4.2094965 -4.2344685 -4.26166 -4.2792492 -4.2872982 -4.2923675 -4.2998805][-4.2321548 -4.2286448 -4.2397017 -4.2460642 -4.2552667 -4.2675662 -4.2742891 -4.2736831 -4.2739491 -4.2815289 -4.2891107 -4.2927904 -4.2962422 -4.297792 -4.304121][-4.2165518 -4.2139382 -4.2296743 -4.2431378 -4.2561283 -4.2737331 -4.2870588 -4.2940264 -4.2979956 -4.3011703 -4.3011174 -4.2999821 -4.3010683 -4.30031 -4.3054547][-4.2044144 -4.1997485 -4.2139187 -4.2287216 -4.24129 -4.2559938 -4.2671256 -4.2768407 -4.2850819 -4.2904687 -4.2927961 -4.2949629 -4.297894 -4.2980475 -4.3032193]]...]
INFO - root - 2017-12-06 02:09:50.670206: step 63110, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 68h:15m:52s remains)
INFO - root - 2017-12-06 02:10:00.043865: step 63120, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 67h:17m:32s remains)
INFO - root - 2017-12-06 02:10:09.401334: step 63130, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.949 sec/batch; 70h:59m:42s remains)
INFO - root - 2017-12-06 02:10:18.800449: step 63140, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 68h:35m:38s remains)
INFO - root - 2017-12-06 02:10:27.997526: step 63150, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 72h:01m:26s remains)
INFO - root - 2017-12-06 02:10:37.153666: step 63160, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 71h:01m:32s remains)
INFO - root - 2017-12-06 02:10:46.478496: step 63170, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 68h:50m:37s remains)
INFO - root - 2017-12-06 02:10:55.844085: step 63180, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 64h:28m:26s remains)
INFO - root - 2017-12-06 02:11:05.098431: step 63190, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 68h:04m:20s remains)
INFO - root - 2017-12-06 02:11:14.555728: step 63200, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 70h:29m:35s remains)
2017-12-06 02:11:15.355072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3167238 -4.3109035 -4.3046193 -4.3015866 -4.2986407 -4.2967238 -4.2925243 -4.2784681 -4.2581644 -4.2391438 -4.2276082 -4.2176785 -4.2087727 -4.2147651 -4.2276945][-4.3248544 -4.316958 -4.3088951 -4.3056884 -4.3035502 -4.3015323 -4.2955418 -4.279355 -4.2605166 -4.2461743 -4.2367182 -4.2284966 -4.2189631 -4.2237883 -4.2385058][-4.327384 -4.3207169 -4.3125639 -4.3075752 -4.3021369 -4.2950048 -4.2826939 -4.2636991 -4.2475662 -4.236722 -4.2306361 -4.2269764 -4.2226291 -4.2299542 -4.2467375][-4.306427 -4.3022714 -4.2945609 -4.2855115 -4.2750411 -4.2623286 -4.245039 -4.2284746 -4.21792 -4.2127953 -4.2099409 -4.2105594 -4.2151151 -4.2264671 -4.2426672][-4.2621346 -4.2605481 -4.2544503 -4.2399459 -4.2241545 -4.2075191 -4.1900773 -4.176661 -4.172039 -4.1715188 -4.1707091 -4.1734924 -4.1854706 -4.2025051 -4.2185965][-4.2121797 -4.2131219 -4.2065511 -4.1843348 -4.1621046 -4.1411757 -4.1234379 -4.1143775 -4.11716 -4.1224642 -4.1244192 -4.12817 -4.1414075 -4.1586056 -4.1716256][-4.1785097 -4.1786051 -4.1650052 -4.1320157 -4.1032786 -4.0813036 -4.0671172 -4.06918 -4.0856318 -4.0981045 -4.0977197 -4.0942893 -4.1005578 -4.1114683 -4.1169777][-4.1689868 -4.1650686 -4.1407537 -4.097827 -4.06623 -4.0509 -4.0492845 -4.0658402 -4.0947971 -4.1101828 -4.0994096 -4.084919 -4.0825105 -4.0876126 -4.0898666][-4.1791625 -4.1705337 -4.1398039 -4.0965695 -4.0715351 -4.0695205 -4.0801387 -4.1021729 -4.1317306 -4.1421027 -4.1206985 -4.0993352 -4.0966997 -4.1047406 -4.1116133][-4.1990232 -4.1892028 -4.1585412 -4.1200676 -4.102921 -4.1101394 -4.1273265 -4.1477489 -4.1716604 -4.1812406 -4.1642728 -4.1489549 -4.1522222 -4.1641626 -4.1706257][-4.2211647 -4.2091718 -4.1766348 -4.1396017 -4.125308 -4.1365418 -4.1577358 -4.178133 -4.2004247 -4.2164788 -4.2121496 -4.2070169 -4.2136192 -4.2222738 -4.2203178][-4.23853 -4.2232652 -4.1843581 -4.1430717 -4.128068 -4.1405673 -4.1650267 -4.1891255 -4.2133713 -4.2360039 -4.2436552 -4.2453222 -4.2516742 -4.2532496 -4.2398767][-4.2484193 -4.2304912 -4.1866088 -4.1419349 -4.1243653 -4.1367016 -4.1613 -4.1860137 -4.209722 -4.236187 -4.2490325 -4.2529941 -4.2553544 -4.2486882 -4.2263451][-4.2511687 -4.2320228 -4.1868639 -4.1419158 -4.1235714 -4.1348052 -4.1573138 -4.1812634 -4.2046814 -4.2285709 -4.2356906 -4.2335424 -4.2280531 -4.212357 -4.1837711][-4.2504606 -4.2327423 -4.1886969 -4.14734 -4.1322145 -4.1442738 -4.1648712 -4.1877027 -4.207469 -4.2235007 -4.219574 -4.2068005 -4.1932511 -4.1757865 -4.1509385]]...]
INFO - root - 2017-12-06 02:11:24.820384: step 63210, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 72h:03m:23s remains)
INFO - root - 2017-12-06 02:11:34.190007: step 63220, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 69h:39m:55s remains)
INFO - root - 2017-12-06 02:11:43.498741: step 63230, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 64h:12m:24s remains)
INFO - root - 2017-12-06 02:11:52.781509: step 63240, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 69h:18m:23s remains)
INFO - root - 2017-12-06 02:12:01.999633: step 63250, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 70h:25m:14s remains)
INFO - root - 2017-12-06 02:12:11.231787: step 63260, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 67h:49m:29s remains)
INFO - root - 2017-12-06 02:12:20.576389: step 63270, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 68h:10m:27s remains)
INFO - root - 2017-12-06 02:12:30.052009: step 63280, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 65h:35m:07s remains)
INFO - root - 2017-12-06 02:12:39.300391: step 63290, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 67h:35m:41s remains)
