INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "155"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-notzeroinit-larger
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-08 10:15:08.026687: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-08 10:15:08.026961: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-08 10:15:08.026980: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-08 10:15:08.026994: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-08 10:15:08.027007: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-08 10:15:13.624098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-08 10:15:13.624194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-08 10:15:13.624222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-08 10:15:13.624478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-08 10:15:26.491844: step 0, loss = 2.28, batch loss = 2.23 (0.9 examples/sec; 8.658 sec/batch; 799h:39m:33s remains)
2017-12-08 10:15:27.172684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289455 -4.4289432 -4.4289474 -4.4289517 -4.4289527 -4.4289503 -4.4289432 -4.4289308 -4.4289174 -4.4289112 -4.428916 -4.4289317 -4.4289508 -4.4289589 -4.4289594][-4.4289465 -4.4289422 -4.4289455 -4.4289484 -4.428947 -4.4289412 -4.4289284 -4.4289074 -4.4288855 -4.4288797 -4.4288936 -4.4289165 -4.4289393 -4.4289541 -4.42896][-4.4289455 -4.4289393 -4.42894 -4.4289346 -4.4289207 -4.4289021 -4.4288716 -4.428834 -4.4288015 -4.4287963 -4.4288249 -4.428865 -4.4289064 -4.428937 -4.4289541][-4.4289308 -4.4289217 -4.4289174 -4.4288979 -4.4288616 -4.4288182 -4.4287577 -4.4286942 -4.4286532 -4.4286618 -4.42872 -4.4287934 -4.4288635 -4.4289145 -4.4289455][-4.4289021 -4.4288898 -4.4288812 -4.4288478 -4.4287863 -4.4287095 -4.4286017 -4.428494 -4.4284368 -4.4284744 -4.4285789 -4.4287 -4.42881 -4.4288869 -4.4289322][-4.4288683 -4.4288507 -4.4288354 -4.4287887 -4.4287009 -4.4285779 -4.4284043 -4.4282284 -4.4281545 -4.4282417 -4.428411 -4.42859 -4.4287467 -4.4288521 -4.428915][-4.4288311 -4.4288058 -4.4287796 -4.428719 -4.4286046 -4.4284248 -4.4281721 -4.4279222 -4.4278617 -4.4280343 -4.4282742 -4.4285026 -4.4286933 -4.42882 -4.4288969][-4.4287915 -4.4287553 -4.4287181 -4.4286437 -4.4285054 -4.4282818 -4.4279704 -4.4276824 -4.4276915 -4.4279618 -4.42825 -4.4284897 -4.4286804 -4.4288063 -4.428885][-4.4287534 -4.4287081 -4.4286642 -4.4285846 -4.4284453 -4.4282222 -4.4279351 -4.42771 -4.4278088 -4.4281068 -4.4283714 -4.4285688 -4.4287214 -4.428822 -4.4288893][-4.4287338 -4.4286819 -4.4286366 -4.4285645 -4.4284463 -4.4282742 -4.428082 -4.427978 -4.4281139 -4.4283547 -4.4285455 -4.4286723 -4.4287758 -4.4288478 -4.4289026][-4.4287324 -4.428678 -4.4286346 -4.428575 -4.4284863 -4.4283724 -4.4282722 -4.4282522 -4.4283781 -4.4285493 -4.4286656 -4.4287376 -4.4288077 -4.428863 -4.4289117][-4.4287405 -4.4286919 -4.4286532 -4.4286089 -4.42855 -4.4284825 -4.4284415 -4.4284625 -4.4285617 -4.4286733 -4.4287376 -4.4287777 -4.4288292 -4.4288778 -4.4289207][-4.42875 -4.4287171 -4.4286947 -4.4286695 -4.428638 -4.4286075 -4.4285994 -4.4286275 -4.4286866 -4.4287481 -4.428781 -4.4288082 -4.4288545 -4.4289 -4.4289351][-4.4287682 -4.4287562 -4.4287543 -4.4287481 -4.4287343 -4.4287186 -4.4287124 -4.4287248 -4.4287424 -4.4287648 -4.4287834 -4.4288144 -4.4288678 -4.428915 -4.428947][-4.4287853 -4.4287887 -4.4288 -4.4288054 -4.4287972 -4.4287777 -4.4287534 -4.4287391 -4.42873 -4.4287372 -4.4287577 -4.4287982 -4.4288616 -4.4289141 -4.428947]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-notzeroinit-larger/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-notzeroinit-larger/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 10:15:33.010733: step 10, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:25m:26s remains)
INFO - root - 2017-12-08 10:15:37.625060: step 20, loss = 2.28, batch loss = 2.23 (16.7 examples/sec; 0.479 sec/batch; 44h:14m:54s remains)
INFO - root - 2017-12-08 10:15:42.130872: step 30, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 39h:33m:38s remains)
INFO - root - 2017-12-08 10:15:46.580222: step 40, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 40h:27m:03s remains)
INFO - root - 2017-12-08 10:15:50.987454: step 50, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.423 sec/batch; 39h:03m:10s remains)
INFO - root - 2017-12-08 10:15:55.317947: step 60, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 41h:02m:59s remains)
INFO - root - 2017-12-08 10:15:59.754993: step 70, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 41h:18m:57s remains)
INFO - root - 2017-12-08 10:16:04.123484: step 80, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:53m:08s remains)
INFO - root - 2017-12-08 10:16:08.522778: step 90, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 41h:09m:32s remains)
INFO - root - 2017-12-08 10:16:12.700206: step 100, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 39h:27m:56s remains)
2017-12-08 10:16:13.261520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428823 -4.4287658 -4.4287214 -4.42868 -4.4286394 -4.4286432 -4.4286861 -4.4287419 -4.4288006 -4.428833 -4.4288359 -4.4288225 -4.4288025 -4.4287643 -4.4287128][-4.4288263 -4.4287615 -4.428699 -4.4286327 -4.4285765 -4.4285741 -4.4286189 -4.4286761 -4.4287291 -4.4287572 -4.4287658 -4.4287629 -4.428751 -4.428721 -4.4286771][-4.428833 -4.4287639 -4.42869 -4.4286146 -4.428556 -4.4285464 -4.4285822 -4.4286318 -4.4286766 -4.4287081 -4.4287214 -4.4287229 -4.4287162 -4.4286942 -4.4286575][-4.4288311 -4.4287586 -4.4286804 -4.4286051 -4.4285517 -4.4285393 -4.4285679 -4.4286094 -4.4286523 -4.4286919 -4.42871 -4.4287109 -4.4287052 -4.4286885 -4.428658][-4.4288268 -4.4287519 -4.4286742 -4.4286027 -4.4285569 -4.4285417 -4.4285617 -4.4285951 -4.4286342 -4.4286723 -4.4286938 -4.4286995 -4.4287 -4.4286947 -4.4286752][-4.4288244 -4.4287515 -4.4286757 -4.4286079 -4.4285622 -4.4285355 -4.4285369 -4.4285541 -4.4285755 -4.4286036 -4.4286342 -4.4286585 -4.4286785 -4.4286861 -4.428679][-4.4288239 -4.4287539 -4.428679 -4.4286113 -4.4285607 -4.4285231 -4.4285088 -4.4285026 -4.4284925 -4.4285059 -4.4285479 -4.4285936 -4.4286346 -4.428658 -4.4286633][-4.4288211 -4.428751 -4.4286728 -4.4286027 -4.4285507 -4.4285131 -4.4284949 -4.4284773 -4.4284444 -4.4284463 -4.4284945 -4.4285483 -4.4285932 -4.4286194 -4.4286313][-4.4288187 -4.4287472 -4.4286685 -4.4285979 -4.4285479 -4.4285178 -4.4285054 -4.4284897 -4.4284573 -4.4284592 -4.4285035 -4.4285455 -4.4285707 -4.4285831 -4.4285941][-4.4288149 -4.4287405 -4.4286628 -4.4285893 -4.4285378 -4.4285121 -4.428503 -4.4284911 -4.428472 -4.4284849 -4.4285264 -4.4285502 -4.4285517 -4.4285488 -4.428556][-4.4288135 -4.4287367 -4.4286594 -4.4285831 -4.4285288 -4.428503 -4.4284892 -4.4284763 -4.4284692 -4.4284911 -4.4285307 -4.4285436 -4.4285312 -4.42852 -4.4285231][-4.4288206 -4.4287434 -4.4286675 -4.4285917 -4.4285383 -4.4285126 -4.4284978 -4.4284925 -4.4284983 -4.428524 -4.4285569 -4.4285603 -4.4285417 -4.4285293 -4.4285297][-4.4288378 -4.4287643 -4.4286942 -4.4286256 -4.4285746 -4.4285522 -4.428545 -4.4285541 -4.4285736 -4.4285984 -4.4286208 -4.4286156 -4.4285955 -4.4285846 -4.4285841][-4.428865 -4.4288025 -4.4287448 -4.4286861 -4.428638 -4.4286156 -4.428617 -4.4286404 -4.4286714 -4.4286947 -4.4287086 -4.4286995 -4.4286795 -4.4286671 -4.4286637][-4.4289017 -4.4288559 -4.4288154 -4.4287744 -4.4287362 -4.4287167 -4.4287219 -4.4287467 -4.4287744 -4.4287891 -4.4287949 -4.4287863 -4.4287715 -4.428761 -4.4287586]]...]
INFO - root - 2017-12-08 10:16:17.636870: step 110, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:39m:33s remains)
INFO - root - 2017-12-08 10:16:22.076728: step 120, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 41h:20m:45s remains)
INFO - root - 2017-12-08 10:16:26.651091: step 130, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 41h:15m:26s remains)
INFO - root - 2017-12-08 10:16:31.157362: step 140, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:25m:42s remains)
INFO - root - 2017-12-08 10:16:35.683254: step 150, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 41h:01m:58s remains)
INFO - root - 2017-12-08 10:16:40.181975: step 160, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:42m:48s remains)
INFO - root - 2017-12-08 10:16:44.650187: step 170, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.462 sec/batch; 42h:40m:02s remains)
INFO - root - 2017-12-08 10:16:49.057271: step 180, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 41h:19m:55s remains)
INFO - root - 2017-12-08 10:16:53.601702: step 190, loss = 2.28, batch loss = 2.23 (16.2 examples/sec; 0.494 sec/batch; 45h:36m:48s remains)
INFO - root - 2017-12-08 10:16:57.800439: step 200, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 41h:00m:46s remains)
2017-12-08 10:16:58.311416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428936 -4.4289255 -4.4289203 -4.4289241 -4.4289331 -4.4289403 -4.4289474 -4.4289584 -4.4289727 -4.4289889 -4.4290042 -4.4290123 -4.4290085 -4.4289942 -4.4289765][-4.4288893 -4.4288759 -4.4288788 -4.4288912 -4.4289055 -4.4289107 -4.4289112 -4.4289169 -4.4289331 -4.4289589 -4.4289856 -4.4290018 -4.4290013 -4.4289875 -4.4289694][-4.4288416 -4.4288282 -4.4288363 -4.428854 -4.4288692 -4.4288697 -4.4288578 -4.4288487 -4.4288611 -4.4288974 -4.4289389 -4.4289651 -4.4289727 -4.4289651 -4.4289546][-4.4288063 -4.4287915 -4.4287982 -4.4288111 -4.4288187 -4.4288077 -4.4287772 -4.428751 -4.4287615 -4.4288092 -4.428864 -4.4289041 -4.4289241 -4.4289303 -4.4289341][-4.4287786 -4.4287558 -4.4287505 -4.4287481 -4.4287357 -4.4286981 -4.4286389 -4.4286008 -4.4286256 -4.4286971 -4.4287696 -4.4288249 -4.4288621 -4.4288878 -4.4289093][-4.4287386 -4.4287043 -4.4286819 -4.4286537 -4.4286065 -4.42852 -4.4284205 -4.42838 -4.4284487 -4.4285622 -4.4286566 -4.4287291 -4.4287872 -4.4288363 -4.4288778][-4.4286828 -4.4286304 -4.4285822 -4.4285207 -4.428431 -4.4282904 -4.428143 -4.4281225 -4.4282641 -4.4284282 -4.4285488 -4.4286385 -4.4287176 -4.4287882 -4.4288483][-4.4286284 -4.4285607 -4.4284921 -4.4284067 -4.4282908 -4.428122 -4.4279633 -4.4279795 -4.4281707 -4.4283667 -4.4284992 -4.4285984 -4.4286904 -4.428771 -4.4288383][-4.4285684 -4.428503 -4.428442 -4.4283705 -4.4282813 -4.4281564 -4.4280539 -4.4280882 -4.4282517 -4.4284167 -4.42853 -4.4286218 -4.4287086 -4.4287844 -4.4288459][-4.4285254 -4.4284778 -4.4284458 -4.4284191 -4.4283862 -4.42833 -4.4282832 -4.4283128 -4.4284172 -4.4285264 -4.4286056 -4.4286747 -4.4287467 -4.4288096 -4.42886][-4.4285436 -4.4285164 -4.4285212 -4.428534 -4.4285445 -4.4285321 -4.4285169 -4.428535 -4.4285884 -4.428648 -4.4286919 -4.4287367 -4.42879 -4.4288397 -4.4288788][-4.428618 -4.4286103 -4.4286375 -4.4286656 -4.4286876 -4.4286938 -4.4286923 -4.4287 -4.4287243 -4.4287539 -4.4287744 -4.4287982 -4.4288359 -4.4288721 -4.4289][-4.4287138 -4.4287152 -4.4287438 -4.4287672 -4.4287825 -4.4287891 -4.4287896 -4.428791 -4.4287982 -4.4288096 -4.4288192 -4.4288363 -4.4288664 -4.428894 -4.4289126][-4.4288068 -4.4288058 -4.4288244 -4.4288368 -4.4288421 -4.4288411 -4.42884 -4.4288397 -4.4288397 -4.4288411 -4.4288445 -4.4288597 -4.4288845 -4.4289055 -4.4289184][-4.4288707 -4.4288654 -4.4288735 -4.4288778 -4.4288759 -4.4288716 -4.4288688 -4.4288673 -4.4288659 -4.4288635 -4.4288664 -4.4288778 -4.4288964 -4.4289117 -4.4289203]]...]
INFO - root - 2017-12-08 10:17:02.932648: step 210, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.466 sec/batch; 43h:00m:45s remains)
INFO - root - 2017-12-08 10:17:07.319990: step 220, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 39h:54m:49s remains)
INFO - root - 2017-12-08 10:17:11.809900: step 230, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.422 sec/batch; 38h:55m:55s remains)
INFO - root - 2017-12-08 10:17:16.383993: step 240, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.468 sec/batch; 43h:10m:42s remains)
INFO - root - 2017-12-08 10:17:20.967292: step 250, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.467 sec/batch; 43h:05m:32s remains)
INFO - root - 2017-12-08 10:17:25.597014: step 260, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:57m:35s remains)
INFO - root - 2017-12-08 10:17:30.049257: step 270, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.441 sec/batch; 40h:39m:20s remains)
INFO - root - 2017-12-08 10:17:34.531152: step 280, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:56m:48s remains)
INFO - root - 2017-12-08 10:17:39.121534: step 290, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:42m:06s remains)
INFO - root - 2017-12-08 10:17:43.391522: step 300, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.446 sec/batch; 41h:07m:14s remains)
2017-12-08 10:17:43.918095: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4284568 -4.4284611 -4.4285145 -4.4285951 -4.4286804 -4.4287453 -4.4287653 -4.4287343 -4.428669 -4.428607 -4.4285922 -4.4286189 -4.4286366 -4.4286513 -4.4286761][-4.4285703 -4.4285574 -4.4285731 -4.428628 -4.428709 -4.4287806 -4.4288092 -4.4287834 -4.4287333 -4.4286919 -4.4286909 -4.4287124 -4.4287181 -4.4287171 -4.4287219][-4.4287019 -4.4286718 -4.4286456 -4.42866 -4.4287205 -4.4287958 -4.4288378 -4.4288287 -4.428802 -4.428781 -4.4287815 -4.4287858 -4.4287672 -4.4287419 -4.4287248][-4.4287915 -4.4287519 -4.4286971 -4.4286718 -4.4287033 -4.4287667 -4.4288149 -4.4288254 -4.428823 -4.4288168 -4.4288082 -4.4287891 -4.4287467 -4.4287004 -4.4286761][-4.4288287 -4.4287958 -4.4287257 -4.4286666 -4.4286637 -4.4287024 -4.4287443 -4.4287715 -4.4287925 -4.4287968 -4.4287767 -4.4287348 -4.4286718 -4.4286132 -4.4285984][-4.4288139 -4.4287829 -4.4287176 -4.4286528 -4.4286289 -4.4286442 -4.42867 -4.4287014 -4.4287281 -4.4287329 -4.4287009 -4.4286432 -4.4285679 -4.4285083 -4.4285111][-4.4287825 -4.4287524 -4.428699 -4.4286394 -4.4286032 -4.4286022 -4.4286137 -4.4286389 -4.428658 -4.42865 -4.4286051 -4.4285336 -4.4284482 -4.4283977 -4.4284277][-4.4287624 -4.4287286 -4.4286785 -4.4286261 -4.4285841 -4.4285703 -4.428576 -4.4285989 -4.4286137 -4.4285951 -4.4285364 -4.4284506 -4.4283562 -4.428318 -4.4283724][-4.42874 -4.4287171 -4.4286804 -4.4286432 -4.4286079 -4.4285831 -4.4285779 -4.4285927 -4.4285994 -4.4285812 -4.4285293 -4.42845 -4.4283571 -4.4283295 -4.4283934][-4.4287381 -4.4287281 -4.428709 -4.4286904 -4.428669 -4.4286451 -4.428628 -4.4286337 -4.4286342 -4.4286189 -4.428586 -4.4285231 -4.428443 -4.4284182 -4.4284735][-4.4287181 -4.4287271 -4.4287286 -4.4287353 -4.4287314 -4.4287119 -4.4286919 -4.4286876 -4.4286757 -4.4286571 -4.4286404 -4.4286036 -4.4285479 -4.4285312 -4.428575][-4.4286933 -4.42873 -4.4287519 -4.4287796 -4.4287915 -4.4287744 -4.4287519 -4.4287391 -4.4287071 -4.4286771 -4.428668 -4.4286537 -4.4286265 -4.4286284 -4.4286718][-4.428647 -4.4286976 -4.42873 -4.4287724 -4.4287982 -4.4287944 -4.4287863 -4.4287715 -4.4287262 -4.428689 -4.4286876 -4.4286833 -4.4286714 -4.4286861 -4.4287276][-4.4286356 -4.4286823 -4.4287124 -4.4287505 -4.4287763 -4.4287844 -4.4287877 -4.4287696 -4.4287181 -4.4286814 -4.4286861 -4.4286952 -4.4286971 -4.42872 -4.4287553][-4.4286628 -4.42869 -4.4287047 -4.4287329 -4.428761 -4.4287853 -4.428802 -4.4287839 -4.4287281 -4.4286871 -4.4286876 -4.4287 -4.4287086 -4.4287248 -4.4287486]]...]
INFO - root - 2017-12-08 10:17:48.380600: step 310, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:48m:23s remains)
INFO - root - 2017-12-08 10:17:52.947370: step 320, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:52m:47s remains)
INFO - root - 2017-12-08 10:17:57.438529: step 330, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 41h:15m:12s remains)
INFO - root - 2017-12-08 10:18:01.925142: step 340, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.446 sec/batch; 41h:07m:10s remains)
INFO - root - 2017-12-08 10:18:06.440088: step 350, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.436 sec/batch; 40h:10m:51s remains)
INFO - root - 2017-12-08 10:18:10.835724: step 360, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.431 sec/batch; 39h:46m:51s remains)
INFO - root - 2017-12-08 10:18:15.230657: step 370, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:36m:51s remains)
INFO - root - 2017-12-08 10:18:19.793526: step 380, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 41h:20m:21s remains)
INFO - root - 2017-12-08 10:18:24.265340: step 390, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:44m:46s remains)
INFO - root - 2017-12-08 10:18:28.549900: step 400, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.446 sec/batch; 41h:06m:42s remains)
2017-12-08 10:18:29.098269: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286761 -4.4286842 -4.4287152 -4.4287367 -4.4287362 -4.4287381 -4.4287553 -4.4287677 -4.4287834 -4.4288044 -4.4288306 -4.4288473 -4.4288359 -4.42881 -4.4287992][-4.4286308 -4.4286542 -4.4287009 -4.4287314 -4.4287291 -4.4287219 -4.4287329 -4.4287457 -4.42877 -4.4288039 -4.4288416 -4.428863 -4.42885 -4.4288206 -4.4288006][-4.4285903 -4.4286242 -4.4286876 -4.4287267 -4.4287205 -4.4287014 -4.4286885 -4.4286957 -4.4287348 -4.4287906 -4.4288445 -4.4288683 -4.4288578 -4.4288349 -4.4288139][-4.4286013 -4.4286346 -4.4286928 -4.4287162 -4.4286914 -4.4286489 -4.4286017 -4.4286008 -4.4286666 -4.4287505 -4.4288187 -4.4288449 -4.4288387 -4.4288268 -4.428812][-4.4286838 -4.4286923 -4.4287109 -4.4286909 -4.4286289 -4.4285488 -4.4284573 -4.4284596 -4.428576 -4.4286995 -4.42878 -4.4288087 -4.4288092 -4.428803 -4.4287906][-4.4287467 -4.42873 -4.4287014 -4.4286366 -4.4285336 -4.4284005 -4.428256 -4.4282765 -4.4284582 -4.4286175 -4.4287119 -4.4287486 -4.4287558 -4.4287505 -4.4287338][-4.428731 -4.4286947 -4.4286351 -4.42854 -4.4284029 -4.4282222 -4.4280272 -4.4280639 -4.4282961 -4.4284744 -4.4285779 -4.4286251 -4.4286456 -4.4286447 -4.4286265][-4.4286828 -4.4286404 -4.4285674 -4.4284568 -4.4283147 -4.4281368 -4.4279447 -4.427978 -4.4281955 -4.4283528 -4.4284463 -4.4285016 -4.4285254 -4.4285192 -4.4284959][-4.4286704 -4.4286342 -4.4285617 -4.4284525 -4.4283361 -4.428216 -4.4281 -4.4281273 -4.4282665 -4.4283657 -4.4284272 -4.4284735 -4.428493 -4.42848 -4.4284482][-4.4287109 -4.42869 -4.4286265 -4.4285269 -4.4284306 -4.42836 -4.4283113 -4.4283347 -4.42841 -4.4284639 -4.4284987 -4.4285374 -4.428565 -4.4285569 -4.4285245][-4.4287763 -4.42876 -4.4287081 -4.428627 -4.4285479 -4.4285011 -4.4284849 -4.4285021 -4.42854 -4.4285703 -4.4285913 -4.4286308 -4.4286695 -4.4286709 -4.4286485][-4.4288211 -4.4288049 -4.4287643 -4.4287043 -4.4286375 -4.4286022 -4.4285984 -4.4286046 -4.4286203 -4.428638 -4.4286542 -4.4286933 -4.4287391 -4.4287491 -4.4287381][-4.4288254 -4.4288125 -4.4287882 -4.4287443 -4.4286876 -4.4286542 -4.4286509 -4.4286461 -4.4286447 -4.428648 -4.4286613 -4.4287019 -4.4287462 -4.4287577 -4.4287515][-4.4288073 -4.428802 -4.4287949 -4.4287653 -4.4287171 -4.4286842 -4.4286766 -4.4286671 -4.4286513 -4.4286342 -4.4286356 -4.4286737 -4.4287124 -4.4287181 -4.42871][-4.42877 -4.4287715 -4.4287767 -4.4287634 -4.42873 -4.4287062 -4.4287 -4.4286919 -4.4286675 -4.4286356 -4.4286242 -4.4286523 -4.428678 -4.4286737 -4.4286585]]...]
INFO - root - 2017-12-08 10:18:33.455006: step 410, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.420 sec/batch; 38h:46m:47s remains)
INFO - root - 2017-12-08 10:18:37.964331: step 420, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:27m:40s remains)
INFO - root - 2017-12-08 10:18:42.441355: step 430, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:51m:17s remains)
INFO - root - 2017-12-08 10:18:46.923231: step 440, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:34m:29s remains)
INFO - root - 2017-12-08 10:18:51.349084: step 450, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:57m:50s remains)
INFO - root - 2017-12-08 10:18:55.792194: step 460, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:39m:38s remains)
INFO - root - 2017-12-08 10:19:00.223550: step 470, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.448 sec/batch; 41h:20m:43s remains)
INFO - root - 2017-12-08 10:19:04.678419: step 480, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.446 sec/batch; 41h:06m:04s remains)
INFO - root - 2017-12-08 10:19:09.170307: step 490, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 41h:10m:14s remains)
INFO - root - 2017-12-08 10:19:13.394396: step 500, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 39h:37m:59s remains)
2017-12-08 10:19:13.932709: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287839 -4.4287896 -4.4288135 -4.42883 -4.4288263 -4.4288158 -4.4288068 -4.4288011 -4.4287968 -4.42879 -4.4287715 -4.428731 -4.4286828 -4.4286284 -4.4285769][-4.4287362 -4.4287367 -4.4287562 -4.4287639 -4.4287515 -4.4287438 -4.4287424 -4.428751 -4.428761 -4.4287567 -4.4287243 -4.428669 -4.428607 -4.428545 -4.4285035][-4.4287124 -4.4287028 -4.428709 -4.4287004 -4.4286752 -4.4286718 -4.4286876 -4.4287148 -4.4287338 -4.4287233 -4.4286742 -4.4286113 -4.4285512 -4.4284992 -4.4284759][-4.4286952 -4.4286761 -4.4286652 -4.4286356 -4.4285917 -4.4285879 -4.4286218 -4.4286671 -4.4286962 -4.428679 -4.4286208 -4.4285622 -4.4285178 -4.4284821 -4.4284768][-4.4286666 -4.4286394 -4.4286079 -4.4285531 -4.4284959 -4.4284821 -4.4285154 -4.4285741 -4.4286156 -4.4286013 -4.428544 -4.4284959 -4.4284711 -4.4284654 -4.4284873][-4.4286227 -4.4285936 -4.4285531 -4.42848 -4.4284034 -4.4283628 -4.4283667 -4.4284129 -4.4284635 -4.4284692 -4.4284296 -4.4284019 -4.4284129 -4.4284515 -4.428503][-4.4285841 -4.4285612 -4.428524 -4.4284463 -4.4283462 -4.4282546 -4.4281893 -4.4281883 -4.42825 -4.4283071 -4.4283137 -4.4283252 -4.42837 -4.42843 -4.42849][-4.4285483 -4.4285321 -4.4285054 -4.4284391 -4.4283338 -4.4282022 -4.4280591 -4.4279895 -4.4280639 -4.4281821 -4.4282517 -4.4283056 -4.4283719 -4.428431 -4.4284792][-4.4285226 -4.4284983 -4.4284725 -4.4284263 -4.4283447 -4.4282255 -4.428072 -4.4279747 -4.4280367 -4.4281654 -4.4282584 -4.4283404 -4.4284158 -4.4284596 -4.4284854][-4.4285116 -4.4284773 -4.4284515 -4.4284334 -4.4283972 -4.4283257 -4.4282169 -4.428143 -4.4281788 -4.4282584 -4.4283171 -4.4283905 -4.4284687 -4.4285059 -4.4285107][-4.4284978 -4.4284816 -4.4284821 -4.4284959 -4.4284959 -4.4284692 -4.4284067 -4.4283552 -4.4283638 -4.4283857 -4.4283972 -4.4284444 -4.4285083 -4.42853 -4.42852][-4.4285312 -4.4285426 -4.4285645 -4.428587 -4.4285941 -4.4285841 -4.4285474 -4.4285088 -4.4284987 -4.4284811 -4.4284568 -4.4284725 -4.4285111 -4.4285235 -4.4285178][-4.4286027 -4.4286232 -4.428648 -4.428659 -4.4286566 -4.4286475 -4.4286203 -4.428586 -4.4285631 -4.4285293 -4.4284987 -4.4285021 -4.4285231 -4.4285364 -4.4285483][-4.4286675 -4.4286814 -4.4286962 -4.4286952 -4.4286819 -4.428668 -4.4286466 -4.4286227 -4.428606 -4.4285841 -4.42857 -4.4285755 -4.4285836 -4.4285927 -4.4286113][-4.428688 -4.4286962 -4.4287047 -4.4286976 -4.4286819 -4.4286709 -4.4286571 -4.4286489 -4.4286528 -4.4286528 -4.4286556 -4.4286594 -4.4286532 -4.4286513 -4.4286623]]...]
INFO - root - 2017-12-08 10:19:18.331896: step 510, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 39h:30m:13s remains)
INFO - root - 2017-12-08 10:19:22.736193: step 520, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.441 sec/batch; 40h:38m:38s remains)
INFO - root - 2017-12-08 10:19:27.217581: step 530, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 42h:11m:08s remains)
INFO - root - 2017-12-08 10:19:31.657406: step 540, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:46m:57s remains)
INFO - root - 2017-12-08 10:19:36.141692: step 550, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:41m:30s remains)
INFO - root - 2017-12-08 10:19:40.837324: step 560, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.462 sec/batch; 42h:33m:30s remains)
INFO - root - 2017-12-08 10:19:45.346176: step 570, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 40h:23m:17s remains)
INFO - root - 2017-12-08 10:19:49.829641: step 580, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 40h:17m:17s remains)
INFO - root - 2017-12-08 10:19:54.353732: step 590, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 41h:00m:40s remains)
INFO - root - 2017-12-08 10:19:58.547119: step 600, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 41h:06m:44s remains)
2017-12-08 10:19:59.071252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285703 -4.4285455 -4.42859 -4.4286709 -4.4287267 -4.4287095 -4.42865 -4.4286246 -4.4286613 -4.4287324 -4.4288025 -4.4288483 -4.4288836 -4.4289079 -4.4289131][-4.4284625 -4.4284334 -4.4284806 -4.4285841 -4.428659 -4.4286513 -4.4285951 -4.4285707 -4.4286103 -4.4287047 -4.4287915 -4.4288411 -4.4288764 -4.4289026 -4.4289093][-4.4283786 -4.4283619 -4.4284258 -4.4285493 -4.4286375 -4.4286447 -4.4285965 -4.4285703 -4.428607 -4.4287066 -4.4287915 -4.4288344 -4.428865 -4.4288845 -4.4288883][-4.4283633 -4.4283609 -4.4284325 -4.4285464 -4.4286261 -4.428638 -4.4286027 -4.4285913 -4.428638 -4.4287267 -4.4287872 -4.4288111 -4.4288306 -4.4288397 -4.428834][-4.4283872 -4.4283862 -4.4284449 -4.4285259 -4.4285812 -4.4285936 -4.4285827 -4.4285936 -4.4286475 -4.4287205 -4.4287581 -4.4287686 -4.4287791 -4.4287829 -4.4287729][-4.42837 -4.4283519 -4.4283891 -4.42844 -4.4284835 -4.4285107 -4.4285312 -4.4285645 -4.4286189 -4.4286757 -4.4287038 -4.4287157 -4.4287257 -4.4287314 -4.4287295][-4.428308 -4.428247 -4.4282484 -4.42828 -4.428328 -4.4283791 -4.4284272 -4.4284821 -4.4285426 -4.4285965 -4.4286308 -4.4286547 -4.4286776 -4.4286957 -4.4287105][-4.4282742 -4.4281583 -4.4281082 -4.4281168 -4.4281664 -4.4282293 -4.4282947 -4.4283681 -4.4284434 -4.4285126 -4.428566 -4.428607 -4.4286456 -4.428679 -4.428709][-4.4283381 -4.4282045 -4.428124 -4.4281154 -4.428154 -4.428205 -4.4282608 -4.4283314 -4.4284077 -4.4284859 -4.4285555 -4.4286084 -4.4286532 -4.4286914 -4.4287267][-4.4284678 -4.4283733 -4.4283133 -4.4283075 -4.4283323 -4.428359 -4.4283862 -4.4284267 -4.4284773 -4.4285412 -4.4286051 -4.4286532 -4.4286919 -4.4287262 -4.4287605][-4.4285717 -4.4285164 -4.4284887 -4.4284992 -4.4285316 -4.4285536 -4.428565 -4.4285803 -4.4286022 -4.4286375 -4.4286776 -4.4287119 -4.4287415 -4.4287686 -4.4287977][-4.42863 -4.4285908 -4.4285755 -4.4285941 -4.4286385 -4.428668 -4.4286814 -4.4286933 -4.4287043 -4.4287205 -4.4287424 -4.4287663 -4.42879 -4.4288087 -4.4288263][-4.4286785 -4.4286404 -4.42862 -4.4286308 -4.4286695 -4.4286981 -4.428719 -4.4287381 -4.42875 -4.4287586 -4.4287744 -4.4287968 -4.4288173 -4.4288306 -4.42884][-4.4287248 -4.4286885 -4.4286618 -4.4286628 -4.4286857 -4.4287052 -4.4287219 -4.428741 -4.4287505 -4.4287524 -4.4287691 -4.4287963 -4.4288187 -4.4288316 -4.4288368][-4.4287667 -4.4287391 -4.4287171 -4.4287109 -4.4287171 -4.4287248 -4.4287348 -4.4287477 -4.428751 -4.4287477 -4.4287653 -4.4287944 -4.4288163 -4.4288282 -4.4288321]]...]
INFO - root - 2017-12-08 10:20:03.601024: step 610, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:47m:09s remains)
INFO - root - 2017-12-08 10:20:08.143588: step 620, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 41h:27m:21s remains)
INFO - root - 2017-12-08 10:20:12.689800: step 630, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 41h:17m:25s remains)
INFO - root - 2017-12-08 10:20:17.247274: step 640, loss = 2.28, batch loss = 2.23 (16.7 examples/sec; 0.479 sec/batch; 44h:10m:09s remains)
INFO - root - 2017-12-08 10:20:21.770494: step 650, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 40h:01m:32s remains)
INFO - root - 2017-12-08 10:20:26.276384: step 660, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 41h:08m:01s remains)
INFO - root - 2017-12-08 10:20:30.673047: step 670, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:31m:44s remains)
INFO - root - 2017-12-08 10:20:35.051106: step 680, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 40h:07m:11s remains)
INFO - root - 2017-12-08 10:20:39.467127: step 690, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 40h:22m:11s remains)
INFO - root - 2017-12-08 10:20:43.708491: step 700, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:41m:25s remains)
2017-12-08 10:20:44.251873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287405 -4.4287329 -4.4287214 -4.4287138 -4.4287119 -4.4287109 -4.4287071 -4.4286971 -4.4286876 -4.4286833 -4.4286847 -4.4286842 -4.42867 -4.4286418 -4.42861][-4.4287753 -4.428771 -4.42876 -4.4287505 -4.4287486 -4.4287472 -4.4287395 -4.4287219 -4.4287033 -4.4286919 -4.4286904 -4.428689 -4.4286785 -4.4286575 -4.4286304][-4.4287992 -4.4287953 -4.4287863 -4.428781 -4.4287853 -4.428791 -4.4287863 -4.4287686 -4.4287491 -4.4287367 -4.4287353 -4.4287343 -4.4287286 -4.4287162 -4.4286947][-4.428781 -4.4287682 -4.4287534 -4.4287491 -4.4287634 -4.428782 -4.4287858 -4.4287739 -4.4287605 -4.4287581 -4.4287663 -4.4287724 -4.4287748 -4.4287686 -4.4287472][-4.4287438 -4.4287148 -4.4286833 -4.428668 -4.4286847 -4.4287109 -4.4287181 -4.4287047 -4.4286976 -4.4287109 -4.4287405 -4.4287705 -4.4287934 -4.4288011 -4.4287834][-4.4287119 -4.428668 -4.4286213 -4.4285922 -4.4286008 -4.42862 -4.428618 -4.4285917 -4.4285779 -4.4286017 -4.4286556 -4.4287152 -4.4287686 -4.4288 -4.4287968][-4.4287014 -4.4286585 -4.4286137 -4.4285808 -4.4285784 -4.4285827 -4.4285626 -4.4285159 -4.428483 -4.4285026 -4.4285665 -4.4286437 -4.4287195 -4.4287715 -4.4287844][-4.4287138 -4.4286909 -4.4286637 -4.4286408 -4.4286366 -4.4286318 -4.4286056 -4.428555 -4.4285092 -4.4285097 -4.4285564 -4.4286232 -4.4286938 -4.4287457 -4.4287624][-4.4287524 -4.4287472 -4.4287333 -4.4287176 -4.4287119 -4.4287038 -4.4286833 -4.428648 -4.4286122 -4.4286065 -4.4286346 -4.428679 -4.4287238 -4.4287524 -4.4287553][-4.4287996 -4.4287977 -4.4287848 -4.4287663 -4.4287524 -4.4287372 -4.4287186 -4.4287009 -4.4286842 -4.4286852 -4.428709 -4.4287419 -4.4287682 -4.4287748 -4.4287581][-4.4288354 -4.4288287 -4.4288125 -4.4287882 -4.4287667 -4.4287457 -4.4287252 -4.4287143 -4.4287081 -4.4287143 -4.4287391 -4.4287696 -4.4287868 -4.4287815 -4.4287539][-4.4288383 -4.4288316 -4.4288149 -4.428792 -4.4287748 -4.4287581 -4.42874 -4.4287314 -4.4287252 -4.4287257 -4.4287419 -4.4287648 -4.4287753 -4.4287653 -4.4287338][-4.4288154 -4.4288073 -4.428793 -4.4287753 -4.4287653 -4.4287586 -4.4287505 -4.4287472 -4.4287429 -4.4287367 -4.4287405 -4.428751 -4.4287515 -4.4287376 -4.4287062][-4.4287848 -4.4287758 -4.4287624 -4.4287481 -4.4287424 -4.4287429 -4.4287438 -4.4287481 -4.4287491 -4.4287424 -4.4287381 -4.4287386 -4.4287319 -4.4287162 -4.4286866][-4.4287529 -4.4287448 -4.4287338 -4.4287233 -4.4287219 -4.4287271 -4.4287338 -4.4287419 -4.4287453 -4.42874 -4.4287324 -4.4287281 -4.4287186 -4.4287047 -4.4286809]]...]
INFO - root - 2017-12-08 10:20:48.762204: step 710, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:56m:13s remains)
INFO - root - 2017-12-08 10:20:53.215981: step 720, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:22m:28s remains)
INFO - root - 2017-12-08 10:20:57.734202: step 730, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:45m:37s remains)
INFO - root - 2017-12-08 10:21:02.296171: step 740, loss = 2.28, batch loss = 2.23 (16.7 examples/sec; 0.478 sec/batch; 44h:01m:34s remains)
INFO - root - 2017-12-08 10:21:06.741898: step 750, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.460 sec/batch; 42h:25m:27s remains)
INFO - root - 2017-12-08 10:21:11.307188: step 760, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.459 sec/batch; 42h:17m:08s remains)
INFO - root - 2017-12-08 10:21:15.803703: step 770, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 41h:17m:53s remains)
INFO - root - 2017-12-08 10:21:20.258504: step 780, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:49m:53s remains)
INFO - root - 2017-12-08 10:21:24.724087: step 790, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:42m:10s remains)
INFO - root - 2017-12-08 10:21:28.957890: step 800, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:35m:08s remains)
2017-12-08 10:21:29.482799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288411 -4.4288292 -4.4288249 -4.4288073 -4.4287663 -4.4287205 -4.42869 -4.42868 -4.4286933 -4.4287181 -4.4287515 -4.4287763 -4.4287782 -4.42876 -4.4287319][-4.428843 -4.4288278 -4.4288173 -4.4287963 -4.4287548 -4.4287066 -4.4286695 -4.42865 -4.428658 -4.4286809 -4.4287124 -4.4287329 -4.428731 -4.4287148 -4.4286876][-4.42882 -4.4288006 -4.4287844 -4.4287605 -4.428721 -4.4286737 -4.4286346 -4.4286132 -4.4286232 -4.4286494 -4.4286833 -4.4287062 -4.4287081 -4.4286952 -4.4286718][-4.4287648 -4.4287438 -4.4287214 -4.4286923 -4.4286518 -4.4286118 -4.4285803 -4.4285679 -4.4285879 -4.4286246 -4.4286652 -4.4286957 -4.4287043 -4.4286966 -4.4286771][-4.4287076 -4.4286809 -4.4286485 -4.4286079 -4.4285612 -4.4285235 -4.4284992 -4.428503 -4.4285455 -4.4286013 -4.4286537 -4.4286919 -4.4287071 -4.4287062 -4.4286885][-4.4286628 -4.4286275 -4.4285779 -4.4285183 -4.4284596 -4.42841 -4.4283752 -4.4283915 -4.4284649 -4.4285502 -4.4286213 -4.4286747 -4.4286976 -4.4286971 -4.4286785][-4.4286542 -4.4286175 -4.428555 -4.4284778 -4.4284005 -4.4283223 -4.428257 -4.4282651 -4.4283595 -4.4284673 -4.4285531 -4.4286146 -4.4286332 -4.4286222 -4.4285975][-4.4286723 -4.4286432 -4.4285803 -4.4285016 -4.4284258 -4.4283433 -4.428266 -4.4282618 -4.4283457 -4.4284468 -4.4285259 -4.4285779 -4.4285831 -4.4285593 -4.428524][-4.4287148 -4.4286938 -4.4286394 -4.428575 -4.4285231 -4.4284678 -4.428412 -4.4284034 -4.4284592 -4.4285293 -4.4285851 -4.428617 -4.4286094 -4.4285812 -4.4285407][-4.4287658 -4.4287481 -4.4287052 -4.4286561 -4.42862 -4.4285817 -4.4285393 -4.4285278 -4.4285636 -4.4286108 -4.42865 -4.4286695 -4.4286566 -4.428628 -4.428587][-4.428823 -4.4288063 -4.4287748 -4.4287381 -4.4287095 -4.4286795 -4.4286427 -4.4286251 -4.4286442 -4.4286742 -4.4287038 -4.428721 -4.4287105 -4.4286852 -4.4286504][-4.4288669 -4.4288535 -4.4288335 -4.4288077 -4.4287848 -4.4287591 -4.4287281 -4.4287133 -4.4287267 -4.4287491 -4.4287734 -4.4287868 -4.4287758 -4.4287515 -4.428721][-4.4288759 -4.4288678 -4.4288564 -4.4288378 -4.4288187 -4.4287992 -4.4287777 -4.4287691 -4.4287791 -4.4287968 -4.4288158 -4.4288273 -4.4288192 -4.4287996 -4.4287763][-4.4288678 -4.4288635 -4.4288583 -4.4288454 -4.42883 -4.4288149 -4.4288006 -4.4287968 -4.4288039 -4.4288163 -4.4288282 -4.4288344 -4.4288287 -4.4288158 -4.4288][-4.4288778 -4.4288735 -4.42887 -4.4288607 -4.4288492 -4.4288387 -4.4288316 -4.4288311 -4.4288359 -4.4288392 -4.4288411 -4.4288406 -4.4288335 -4.4288225 -4.4288063]]...]
INFO - root - 2017-12-08 10:21:33.871895: step 810, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.424 sec/batch; 39h:04m:42s remains)
INFO - root - 2017-12-08 10:21:38.294391: step 820, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.429 sec/batch; 39h:32m:10s remains)
INFO - root - 2017-12-08 10:21:42.628562: step 830, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 40h:02m:20s remains)
INFO - root - 2017-12-08 10:21:47.008390: step 840, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.429 sec/batch; 39h:29m:52s remains)
INFO - root - 2017-12-08 10:21:51.423135: step 850, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.422 sec/batch; 38h:54m:57s remains)
INFO - root - 2017-12-08 10:21:55.859806: step 860, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.431 sec/batch; 39h:41m:11s remains)
INFO - root - 2017-12-08 10:22:00.326310: step 870, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:19m:21s remains)
INFO - root - 2017-12-08 10:22:04.863432: step 880, loss = 2.28, batch loss = 2.23 (16.4 examples/sec; 0.488 sec/batch; 44h:55m:49s remains)
INFO - root - 2017-12-08 10:22:09.365966: step 890, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:46m:14s remains)
INFO - root - 2017-12-08 10:22:13.633772: step 900, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:29m:28s remains)
2017-12-08 10:22:14.218243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289131 -4.428906 -4.4289093 -4.4289174 -4.4289217 -4.4289241 -4.4289269 -4.4289289 -4.4289269 -4.4289212 -4.4289179 -4.4289169 -4.4289188 -4.4289193 -4.42892][-4.4288878 -4.4288783 -4.4288826 -4.4288931 -4.4288988 -4.4289036 -4.4289145 -4.4289246 -4.4289289 -4.428926 -4.4289212 -4.4289212 -4.4289222 -4.4289188 -4.4289131][-4.4288635 -4.4288478 -4.4288449 -4.4288507 -4.4288559 -4.428864 -4.4288812 -4.4289026 -4.4289217 -4.4289317 -4.4289355 -4.4289389 -4.4289417 -4.4289355 -4.4289217][-4.4288273 -4.4288015 -4.4287825 -4.4287729 -4.428771 -4.4287806 -4.4288044 -4.4288411 -4.4288821 -4.4289141 -4.4289331 -4.428947 -4.4289565 -4.4289522 -4.4289346][-4.4287896 -4.4287486 -4.4287062 -4.4286709 -4.4286475 -4.4286404 -4.4286609 -4.4287167 -4.428792 -4.4288521 -4.4288888 -4.4289145 -4.4289374 -4.428946 -4.4289346][-4.4287543 -4.4287 -4.4286351 -4.4285688 -4.4285092 -4.42846 -4.4284573 -4.428525 -4.4286404 -4.4287448 -4.4288111 -4.4288526 -4.4288898 -4.4289141 -4.4289179][-4.4287329 -4.4286685 -4.4285893 -4.4284983 -4.428401 -4.4282956 -4.4282355 -4.4282832 -4.4284291 -4.4285903 -4.4287024 -4.4287734 -4.4288297 -4.42887 -4.4288883][-4.4287281 -4.4286661 -4.4285903 -4.4284945 -4.4283772 -4.4282303 -4.4281034 -4.4280839 -4.4282212 -4.428411 -4.4285645 -4.4286757 -4.4287586 -4.42882 -4.4288554][-4.4287386 -4.4286852 -4.4286251 -4.428545 -4.428442 -4.4283085 -4.4281611 -4.4280682 -4.428124 -4.4282718 -4.4284244 -4.4285641 -4.42868 -4.4287648 -4.4288173][-4.4287543 -4.4287081 -4.4286623 -4.4286017 -4.4285245 -4.4284272 -4.4283118 -4.4282107 -4.4281917 -4.4282494 -4.4283428 -4.4284711 -4.4286032 -4.428709 -4.4287815][-4.4287753 -4.4287329 -4.4286971 -4.4286537 -4.4285994 -4.4285345 -4.4284635 -4.4283943 -4.4283576 -4.4283576 -4.4283819 -4.4284639 -4.4285774 -4.4286861 -4.4287663][-4.4288063 -4.4287682 -4.4287372 -4.4287043 -4.4286671 -4.4286242 -4.4285831 -4.4285493 -4.4285283 -4.4285207 -4.4285135 -4.4285479 -4.4286246 -4.4287119 -4.42878][-4.4288445 -4.4288116 -4.4287844 -4.4287586 -4.4287324 -4.4287047 -4.4286819 -4.4286704 -4.4286675 -4.4286695 -4.4286637 -4.4286752 -4.4287152 -4.4287691 -4.4288158][-4.4288783 -4.4288568 -4.4288378 -4.428822 -4.4288073 -4.4287915 -4.4287806 -4.4287772 -4.42878 -4.4287858 -4.4287848 -4.4287906 -4.4288096 -4.4288349 -4.42886][-4.4289021 -4.4288917 -4.4288812 -4.4288726 -4.4288654 -4.4288583 -4.4288526 -4.4288497 -4.4288521 -4.4288583 -4.4288616 -4.4288654 -4.4288731 -4.4288836 -4.4288936]]...]
INFO - root - 2017-12-08 10:22:18.807656: step 910, loss = 2.28, batch loss = 2.23 (16.2 examples/sec; 0.493 sec/batch; 45h:24m:42s remains)
INFO - root - 2017-12-08 10:22:23.379762: step 920, loss = 2.28, batch loss = 2.23 (16.1 examples/sec; 0.498 sec/batch; 45h:51m:40s remains)
INFO - root - 2017-12-08 10:22:27.809083: step 930, loss = 2.28, batch loss = 2.23 (16.2 examples/sec; 0.495 sec/batch; 45h:36m:08s remains)
INFO - root - 2017-12-08 10:22:32.176995: step 940, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:28m:04s remains)
INFO - root - 2017-12-08 10:22:36.565207: step 950, loss = 2.28, batch loss = 2.23 (20.8 examples/sec; 0.384 sec/batch; 35h:23m:38s remains)
INFO - root - 2017-12-08 10:22:41.025527: step 960, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.459 sec/batch; 42h:13m:31s remains)
INFO - root - 2017-12-08 10:22:45.457341: step 970, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:39m:04s remains)
INFO - root - 2017-12-08 10:22:49.910636: step 980, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:54m:23s remains)
INFO - root - 2017-12-08 10:22:54.353548: step 990, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.424 sec/batch; 39h:03m:59s remains)
INFO - root - 2017-12-08 10:22:58.535103: step 1000, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:42m:31s remains)
2017-12-08 10:22:59.115909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428607 -4.4286265 -4.4286528 -4.4286885 -4.42872 -4.4287148 -4.428679 -4.4286461 -4.4286289 -4.4286084 -4.4286075 -4.4286408 -4.4286876 -4.4287415 -4.4287915][-4.4285536 -4.4285789 -4.42861 -4.4286528 -4.4286919 -4.4286857 -4.4286451 -4.4286094 -4.4286017 -4.428597 -4.4286041 -4.4286413 -4.42869 -4.4287438 -4.4287939][-4.4285254 -4.4285574 -4.4285946 -4.42864 -4.4286761 -4.4286604 -4.4286094 -4.4285712 -4.4285727 -4.4285827 -4.4286 -4.4286394 -4.4286876 -4.4287424 -4.4287972][-4.42854 -4.4285746 -4.428606 -4.4286361 -4.4286537 -4.4286232 -4.4285631 -4.4285216 -4.4285321 -4.4285588 -4.4285893 -4.4286361 -4.4286876 -4.4287434 -4.4288025][-4.4285746 -4.4286056 -4.4286237 -4.428628 -4.4286146 -4.4285612 -4.4284878 -4.4284472 -4.4284792 -4.4285345 -4.4285874 -4.4286437 -4.428699 -4.4287548 -4.4288177][-4.428638 -4.4286523 -4.4286509 -4.4286261 -4.4285727 -4.4284849 -4.4283938 -4.4283605 -4.4284244 -4.4285178 -4.4285984 -4.4286685 -4.4287291 -4.4287834 -4.428844][-4.428679 -4.4286737 -4.4286537 -4.4286022 -4.4285178 -4.4284034 -4.4282923 -4.4282675 -4.4283705 -4.428504 -4.4286127 -4.4286938 -4.4287548 -4.4288044 -4.4288607][-4.4286895 -4.4286695 -4.4286423 -4.4285836 -4.42849 -4.428369 -4.4282494 -4.4282312 -4.4283581 -4.4285126 -4.4286323 -4.4287124 -4.4287677 -4.4288116 -4.4288654][-4.4286914 -4.4286671 -4.4286451 -4.4286 -4.4285283 -4.4284315 -4.4283309 -4.428318 -4.4284282 -4.4285593 -4.4286575 -4.4287219 -4.4287691 -4.4288106 -4.4288654][-4.4286866 -4.4286604 -4.4286447 -4.4286213 -4.4285846 -4.4285283 -4.4284596 -4.4284463 -4.4285159 -4.4286 -4.4286628 -4.4287081 -4.4287524 -4.4287996 -4.4288611][-4.4286823 -4.4286556 -4.4286432 -4.428638 -4.4286304 -4.4286089 -4.4285693 -4.4285483 -4.4285808 -4.4286251 -4.4286585 -4.4286923 -4.4287415 -4.4287963 -4.4288626][-4.4286828 -4.4286566 -4.4286461 -4.4286513 -4.4286618 -4.4286647 -4.4286451 -4.42862 -4.4286289 -4.4286518 -4.4286685 -4.4286985 -4.4287505 -4.4288077 -4.428874][-4.4286842 -4.4286618 -4.4286528 -4.4286618 -4.4286804 -4.4286947 -4.4286885 -4.4286633 -4.4286623 -4.4286761 -4.4286876 -4.428719 -4.4287729 -4.4288311 -4.4288936][-4.4287133 -4.4286942 -4.4286871 -4.4286966 -4.4287138 -4.4287286 -4.4287233 -4.4286981 -4.4286938 -4.4287057 -4.42872 -4.4287567 -4.428812 -4.4288673 -4.4289207][-4.4287715 -4.4287577 -4.4287524 -4.4287586 -4.4287715 -4.4287815 -4.4287739 -4.4287524 -4.4287486 -4.42876 -4.4287758 -4.4288106 -4.4288588 -4.4289041 -4.428947]]...]
INFO - root - 2017-12-08 10:23:03.624658: step 1010, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 42h:05m:18s remains)
INFO - root - 2017-12-08 10:23:08.136010: step 1020, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 40h:03m:01s remains)
INFO - root - 2017-12-08 10:23:12.646584: step 1030, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.461 sec/batch; 42h:26m:42s remains)
INFO - root - 2017-12-08 10:23:17.065465: step 1040, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.456 sec/batch; 41h:58m:51s remains)
INFO - root - 2017-12-08 10:23:21.534854: step 1050, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:34m:04s remains)
INFO - root - 2017-12-08 10:23:26.038234: step 1060, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 40h:03m:25s remains)
INFO - root - 2017-12-08 10:23:30.551546: step 1070, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:33m:10s remains)
INFO - root - 2017-12-08 10:23:35.064993: step 1080, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:31m:27s remains)
INFO - root - 2017-12-08 10:23:39.411664: step 1090, loss = 2.28, batch loss = 2.23 (28.4 examples/sec; 0.282 sec/batch; 25h:57m:22s remains)
INFO - root - 2017-12-08 10:23:43.679412: step 1100, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 39h:14m:23s remains)
2017-12-08 10:23:44.194415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287038 -4.4286928 -4.4286871 -4.4286809 -4.4286795 -4.4286761 -4.4286733 -4.4286737 -4.4286785 -4.4286842 -4.4286914 -4.4286985 -4.4287057 -4.4287148 -4.4287205][-4.4287224 -4.4287124 -4.428709 -4.4287033 -4.4287 -4.4286962 -4.4286923 -4.4286895 -4.4286914 -4.4286981 -4.428709 -4.42872 -4.428731 -4.428741 -4.4287453][-4.42875 -4.4287419 -4.428741 -4.4287362 -4.4287305 -4.4287238 -4.4287162 -4.428709 -4.4287071 -4.4287133 -4.4287286 -4.4287448 -4.4287596 -4.428771 -4.4287729][-4.4287739 -4.4287648 -4.4287639 -4.4287572 -4.4287462 -4.4287333 -4.4287195 -4.4287076 -4.4287019 -4.4287105 -4.42873 -4.4287477 -4.428762 -4.4287729 -4.4287744][-4.4287877 -4.4287715 -4.4287629 -4.428751 -4.4287314 -4.4287081 -4.4286823 -4.4286623 -4.4286575 -4.4286728 -4.4286985 -4.4287167 -4.4287286 -4.4287381 -4.42874][-4.4287796 -4.428751 -4.4287305 -4.4287105 -4.4286838 -4.4286509 -4.4286108 -4.4285774 -4.4285703 -4.4285927 -4.4286227 -4.4286418 -4.4286542 -4.4286647 -4.4286747][-4.428741 -4.4286966 -4.4286618 -4.4286361 -4.428607 -4.4285707 -4.4285169 -4.4284649 -4.428452 -4.4284821 -4.42852 -4.4285469 -4.4285674 -4.4285855 -4.4286103][-4.4286814 -4.4286213 -4.4285774 -4.4285541 -4.4285316 -4.4285011 -4.4284434 -4.4283795 -4.4283609 -4.4283938 -4.4284396 -4.4284744 -4.4285059 -4.4285378 -4.4285774][-4.4286218 -4.4285583 -4.428515 -4.4284959 -4.4284844 -4.4284682 -4.42843 -4.4283805 -4.42836 -4.4283795 -4.4284134 -4.4284453 -4.4284844 -4.42853 -4.4285769][-4.4285831 -4.4285264 -4.4284892 -4.4284735 -4.4284697 -4.4284706 -4.4284639 -4.4284415 -4.4284205 -4.4284177 -4.4284348 -4.428462 -4.4285021 -4.4285512 -4.428596][-4.428575 -4.42853 -4.4285035 -4.4284945 -4.4284997 -4.4285135 -4.4285293 -4.4285269 -4.4285016 -4.4284811 -4.4284821 -4.4284997 -4.4285326 -4.428576 -4.4286151][-4.4285955 -4.4285645 -4.4285493 -4.4285469 -4.428556 -4.428576 -4.4286008 -4.4286113 -4.4285922 -4.4285655 -4.4285541 -4.4285593 -4.4285793 -4.4286094 -4.4286356][-4.4286327 -4.4286113 -4.4286051 -4.4286075 -4.4286184 -4.428638 -4.4286623 -4.4286766 -4.4286652 -4.4286413 -4.4286265 -4.4286237 -4.4286284 -4.4286394 -4.4286509][-4.4286623 -4.4286442 -4.4286423 -4.4286475 -4.4286585 -4.4286742 -4.4286914 -4.4287009 -4.4286923 -4.4286757 -4.4286647 -4.42866 -4.4286556 -4.4286513 -4.4286513][-4.4286766 -4.428658 -4.4286537 -4.4286561 -4.4286637 -4.4286752 -4.4286852 -4.4286919 -4.4286871 -4.4286766 -4.42867 -4.4286675 -4.4286618 -4.4286537 -4.4286485]]...]
INFO - root - 2017-12-08 10:23:48.681926: step 1110, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.464 sec/batch; 42h:42m:31s remains)
INFO - root - 2017-12-08 10:23:53.100870: step 1120, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:58m:19s remains)
INFO - root - 2017-12-08 10:23:57.635397: step 1130, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:57m:06s remains)
INFO - root - 2017-12-08 10:24:02.071110: step 1140, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 40h:15m:15s remains)
INFO - root - 2017-12-08 10:24:06.484625: step 1150, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.427 sec/batch; 39h:15m:35s remains)
INFO - root - 2017-12-08 10:24:10.929313: step 1160, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:21m:49s remains)
INFO - root - 2017-12-08 10:24:15.336455: step 1170, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:58m:24s remains)
INFO - root - 2017-12-08 10:24:19.700178: step 1180, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 40h:11m:55s remains)
INFO - root - 2017-12-08 10:24:23.891812: step 1190, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 22h:05m:23s remains)
INFO - root - 2017-12-08 10:24:28.435093: step 1200, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.468 sec/batch; 43h:05m:04s remains)
2017-12-08 10:24:28.908883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42884 -4.4288454 -4.4288797 -4.4289093 -4.4289165 -4.428916 -4.4289231 -4.4289408 -4.4289541 -4.4289565 -4.4289474 -4.428926 -4.4289021 -4.4288807 -4.4288712][-4.4287596 -4.4287624 -4.4288077 -4.4288568 -4.4288769 -4.4288807 -4.4288883 -4.4289083 -4.4289255 -4.42893 -4.4289207 -4.4288974 -4.4288731 -4.4288526 -4.4288392][-4.4286776 -4.4286823 -4.4287419 -4.4288058 -4.4288325 -4.4288378 -4.4288397 -4.4288559 -4.4288812 -4.4288921 -4.4288912 -4.4288769 -4.4288611 -4.4288445 -4.4288263][-4.42864 -4.4286542 -4.4287162 -4.4287753 -4.428792 -4.428782 -4.4287643 -4.4287729 -4.4288087 -4.4288392 -4.4288588 -4.4288645 -4.4288645 -4.4288535 -4.428833][-4.4286671 -4.4286809 -4.4287281 -4.4287591 -4.4287429 -4.4286928 -4.428627 -4.4286175 -4.4286833 -4.4287572 -4.4288168 -4.4288526 -4.4288769 -4.4288774 -4.4288592][-4.4287167 -4.4287233 -4.4287424 -4.4287362 -4.4286752 -4.42857 -4.4284277 -4.4283772 -4.4284911 -4.4286361 -4.42875 -4.4288239 -4.4288735 -4.4288912 -4.4288774][-4.4287605 -4.428761 -4.4287553 -4.4287162 -4.4286127 -4.4284515 -4.4282427 -4.42815 -4.4283142 -4.4285297 -4.42869 -4.4287953 -4.4288616 -4.4288936 -4.4288883][-4.4287682 -4.428762 -4.4287481 -4.4287009 -4.42859 -4.4284286 -4.4282365 -4.4281554 -4.4283123 -4.4285188 -4.4286771 -4.4287848 -4.4288464 -4.4288783 -4.4288893][-4.4287558 -4.4287515 -4.4287405 -4.4287033 -4.428628 -4.4285326 -4.4284272 -4.4283924 -4.4284806 -4.4286008 -4.4287014 -4.4287767 -4.428822 -4.42885 -4.4288726][-4.4287629 -4.4287596 -4.4287567 -4.4287367 -4.4287004 -4.4286695 -4.4286394 -4.428638 -4.4286728 -4.4287143 -4.4287529 -4.4287877 -4.4288139 -4.4288335 -4.4288573][-4.428802 -4.4287977 -4.428792 -4.4287724 -4.4287543 -4.4287572 -4.428771 -4.4287944 -4.4288087 -4.4288092 -4.4288087 -4.4288154 -4.428823 -4.4288311 -4.4288497][-4.4288559 -4.4288459 -4.4288306 -4.4288044 -4.4287934 -4.428812 -4.4288416 -4.4288673 -4.4288697 -4.4288521 -4.4288354 -4.4288297 -4.4288316 -4.4288349 -4.4288464][-4.4288888 -4.4288716 -4.4288568 -4.428834 -4.4288235 -4.4288378 -4.4288611 -4.4288783 -4.4288731 -4.42885 -4.4288316 -4.428823 -4.4288239 -4.4288278 -4.4288321][-4.4288888 -4.4288745 -4.4288678 -4.4288535 -4.4288416 -4.4288421 -4.4288507 -4.4288573 -4.4288497 -4.4288311 -4.4288149 -4.4288011 -4.4287958 -4.4288 -4.4288044][-4.4288669 -4.4288559 -4.4288507 -4.4288392 -4.4288306 -4.4288292 -4.4288292 -4.42883 -4.428822 -4.4288068 -4.42879 -4.4287739 -4.4287653 -4.4287744 -4.4287882]]...]
INFO - root - 2017-12-08 10:24:33.400961: step 1210, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:57m:10s remains)
INFO - root - 2017-12-08 10:24:37.939187: step 1220, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:50m:35s remains)
INFO - root - 2017-12-08 10:24:42.494951: step 1230, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 41h:12m:08s remains)
INFO - root - 2017-12-08 10:24:47.042815: step 1240, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.456 sec/batch; 41h:55m:39s remains)
INFO - root - 2017-12-08 10:24:51.455731: step 1250, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.436 sec/batch; 40h:09m:05s remains)
INFO - root - 2017-12-08 10:24:55.955982: step 1260, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 42h:08m:56s remains)
INFO - root - 2017-12-08 10:25:00.413244: step 1270, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 41h:11m:00s remains)
INFO - root - 2017-12-08 10:25:04.849561: step 1280, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 41h:05m:03s remains)
INFO - root - 2017-12-08 10:25:09.020216: step 1290, loss = 2.28, batch loss = 2.23 (22.4 examples/sec; 0.357 sec/batch; 32h:50m:45s remains)
INFO - root - 2017-12-08 10:25:13.355305: step 1300, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.424 sec/batch; 38h:59m:22s remains)
2017-12-08 10:25:13.844058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287248 -4.4287343 -4.4287534 -4.4287615 -4.4287596 -4.4287348 -4.4286561 -4.4285889 -4.4285975 -4.4286442 -4.4286904 -4.4287477 -4.4288073 -4.4288468 -4.4288716][-4.428659 -4.4286618 -4.42868 -4.4286947 -4.4287109 -4.4287057 -4.4286318 -4.4285612 -4.4285655 -4.4286075 -4.4286504 -4.4287076 -4.4287744 -4.4288235 -4.4288549][-4.428597 -4.4285922 -4.4286079 -4.4286313 -4.4286704 -4.4286857 -4.428616 -4.4285421 -4.428544 -4.4285841 -4.4286261 -4.4286838 -4.4287543 -4.4288096 -4.4288435][-4.4285336 -4.4285212 -4.4285336 -4.428566 -4.428628 -4.4286609 -4.4285927 -4.428514 -4.4285183 -4.4285674 -4.428618 -4.4286828 -4.4287558 -4.42881 -4.4288416][-4.4284654 -4.42844 -4.4284492 -4.4284863 -4.428565 -4.4286075 -4.4285345 -4.4284515 -4.42847 -4.4285417 -4.4286127 -4.4286923 -4.4287696 -4.428822 -4.4288492][-4.4284344 -4.4283943 -4.42839 -4.4284124 -4.4284849 -4.4285164 -4.4284229 -4.4283342 -4.428381 -4.4284887 -4.4285917 -4.4286952 -4.428781 -4.4288344 -4.4288592][-4.4284682 -4.4284153 -4.4283905 -4.4283853 -4.4284306 -4.42843 -4.4283018 -4.4282036 -4.4282804 -4.4284229 -4.428556 -4.4286842 -4.4287815 -4.4288383 -4.428865][-4.4285488 -4.4284935 -4.4284544 -4.4284306 -4.4284458 -4.4284086 -4.4282489 -4.4281454 -4.4282393 -4.4283934 -4.4285355 -4.4286709 -4.4287734 -4.4288325 -4.4288626][-4.428638 -4.4285908 -4.4285464 -4.4285188 -4.4285216 -4.4284673 -4.4283104 -4.4282212 -4.4283152 -4.4284511 -4.42857 -4.4286861 -4.4287767 -4.4288287 -4.4288573][-4.42872 -4.4286766 -4.4286313 -4.4286089 -4.4286118 -4.4285569 -4.4284258 -4.4283624 -4.4284477 -4.4285564 -4.4286437 -4.42873 -4.4287982 -4.4288349 -4.4288549][-4.428791 -4.4287534 -4.4287105 -4.4286928 -4.4286952 -4.4286456 -4.4285421 -4.4285 -4.4285707 -4.4286542 -4.4287176 -4.4287796 -4.4288282 -4.4288511 -4.42886][-4.428833 -4.4288049 -4.4287724 -4.4287615 -4.4287615 -4.4287176 -4.4286356 -4.4286108 -4.4286671 -4.428731 -4.4287786 -4.4288216 -4.4288559 -4.4288697 -4.4288726][-4.4288511 -4.42883 -4.4288092 -4.4288039 -4.4288011 -4.4287663 -4.4287076 -4.4286966 -4.4287415 -4.4287896 -4.4288268 -4.4288588 -4.4288826 -4.4288907 -4.4288898][-4.428865 -4.4288497 -4.4288392 -4.4288383 -4.428834 -4.4288092 -4.4287682 -4.4287639 -4.4288011 -4.4288392 -4.42887 -4.4288979 -4.4289141 -4.4289169 -4.4289122][-4.4288745 -4.4288616 -4.4288578 -4.4288578 -4.4288526 -4.4288378 -4.42881 -4.4288063 -4.428833 -4.4288626 -4.4288921 -4.4289179 -4.4289322 -4.4289331 -4.4289284]]...]
INFO - root - 2017-12-08 10:25:18.302095: step 1310, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:41m:25s remains)
INFO - root - 2017-12-08 10:25:22.727038: step 1320, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:47m:58s remains)
INFO - root - 2017-12-08 10:25:27.198080: step 1330, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:46m:16s remains)
INFO - root - 2017-12-08 10:25:31.573384: step 1340, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.429 sec/batch; 39h:25m:37s remains)
INFO - root - 2017-12-08 10:25:35.986002: step 1350, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:30m:38s remains)
INFO - root - 2017-12-08 10:25:40.398242: step 1360, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:47m:29s remains)
INFO - root - 2017-12-08 10:25:44.909470: step 1370, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 41h:21m:50s remains)
INFO - root - 2017-12-08 10:25:49.420272: step 1380, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:40m:38s remains)
INFO - root - 2017-12-08 10:25:53.658456: step 1390, loss = 2.28, batch loss = 2.23 (19.3 examples/sec; 0.413 sec/batch; 38h:01m:51s remains)
INFO - root - 2017-12-08 10:25:58.074101: step 1400, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:55m:17s remains)
2017-12-08 10:25:58.542447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287138 -4.4287162 -4.4287181 -4.4287133 -4.4287062 -4.4287052 -4.4287028 -4.42869 -4.4286647 -4.4286342 -4.4286056 -4.4285922 -4.4285975 -4.4286184 -4.4286489][-4.4287133 -4.4287219 -4.4287324 -4.4287338 -4.4287305 -4.4287295 -4.4287243 -4.4287119 -4.428689 -4.4286661 -4.4286494 -4.4286413 -4.4286418 -4.4286485 -4.4286647][-4.4287043 -4.4287143 -4.4287276 -4.4287348 -4.4287362 -4.4287329 -4.4287252 -4.4287152 -4.428699 -4.4286852 -4.4286809 -4.42868 -4.428679 -4.4286752 -4.4286785][-4.42869 -4.4286957 -4.4287062 -4.4287128 -4.4287105 -4.4286962 -4.4286828 -4.4286785 -4.4286742 -4.4286742 -4.428678 -4.4286847 -4.4286847 -4.4286742 -4.4286661][-4.4286671 -4.4286633 -4.4286685 -4.4286718 -4.428659 -4.42863 -4.4286017 -4.428596 -4.4286022 -4.4286189 -4.4286332 -4.428648 -4.4286542 -4.4286451 -4.4286308][-4.4286437 -4.428627 -4.428618 -4.4286027 -4.4285674 -4.4285169 -4.42847 -4.4284606 -4.4284825 -4.4285235 -4.4285583 -4.4285855 -4.428607 -4.4286094 -4.4285965][-4.4286613 -4.4286213 -4.428586 -4.4285393 -4.42847 -4.4283872 -4.4283133 -4.428297 -4.4283381 -4.4284172 -4.4284868 -4.4285407 -4.4285893 -4.4286122 -4.428606][-4.4286976 -4.4286375 -4.4285789 -4.4285064 -4.428412 -4.428298 -4.4281974 -4.42817 -4.4282236 -4.428339 -4.4284511 -4.4285426 -4.4286203 -4.4286666 -4.4286761][-4.4287291 -4.4286594 -4.4285946 -4.4285183 -4.4284263 -4.4283137 -4.4282136 -4.4281845 -4.428236 -4.4283547 -4.4284778 -4.4285808 -4.4286661 -4.4287252 -4.4287515][-4.4287591 -4.4286923 -4.4286304 -4.4285607 -4.4284883 -4.4284048 -4.4283319 -4.4283147 -4.4283595 -4.4284573 -4.4285569 -4.4286361 -4.4287019 -4.4287477 -4.4287724][-4.4287972 -4.4287391 -4.4286828 -4.4286256 -4.4285765 -4.428525 -4.4284763 -4.4284644 -4.4284945 -4.4285583 -4.4286222 -4.4286661 -4.4286995 -4.4287181 -4.4287362][-4.4288306 -4.4287882 -4.4287395 -4.4286928 -4.42866 -4.4286294 -4.4285922 -4.4285727 -4.4285769 -4.4286032 -4.4286342 -4.4286504 -4.4286594 -4.4286642 -4.4286814][-4.4288478 -4.4288282 -4.4287953 -4.4287596 -4.4287314 -4.4287033 -4.4286609 -4.4286208 -4.4285984 -4.428597 -4.4286075 -4.4286151 -4.4286227 -4.4286342 -4.4286604][-4.4288344 -4.4288411 -4.42883 -4.4288087 -4.4287868 -4.4287605 -4.4287148 -4.4286661 -4.4286304 -4.4286132 -4.4286089 -4.428617 -4.4286342 -4.42866 -4.4286971][-4.4287939 -4.42882 -4.4288297 -4.4288216 -4.4288025 -4.4287786 -4.4287424 -4.4287052 -4.4286757 -4.4286566 -4.42865 -4.4286604 -4.4286866 -4.4287195 -4.4287577]]...]
INFO - root - 2017-12-08 10:26:02.926709: step 1410, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 41h:08m:14s remains)
INFO - root - 2017-12-08 10:26:07.332345: step 1420, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:57m:47s remains)
INFO - root - 2017-12-08 10:26:11.737237: step 1430, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 41h:08m:11s remains)
INFO - root - 2017-12-08 10:26:16.271756: step 1440, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.431 sec/batch; 39h:37m:28s remains)
INFO - root - 2017-12-08 10:26:20.754069: step 1450, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 41h:02m:26s remains)
INFO - root - 2017-12-08 10:26:25.313025: step 1460, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 42h:02m:15s remains)
INFO - root - 2017-12-08 10:26:29.805779: step 1470, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.456 sec/batch; 41h:53m:55s remains)
INFO - root - 2017-12-08 10:26:34.238922: step 1480, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.434 sec/batch; 39h:52m:10s remains)
INFO - root - 2017-12-08 10:26:38.533946: step 1490, loss = 2.28, batch loss = 2.23 (19.9 examples/sec; 0.402 sec/batch; 36h:56m:42s remains)
INFO - root - 2017-12-08 10:26:42.931872: step 1500, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:27m:54s remains)
2017-12-08 10:26:43.419237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287362 -4.4287224 -4.4287014 -4.4286804 -4.42871 -4.4287691 -4.4288239 -4.4288678 -4.4288898 -4.4288883 -4.4288764 -4.4288859 -4.4288964 -4.428894 -4.4289088][-4.4286785 -4.4286604 -4.4286284 -4.4285975 -4.4286337 -4.4287076 -4.4287748 -4.4288249 -4.428844 -4.4288316 -4.4288144 -4.4288292 -4.42885 -4.4288568 -4.4288831][-4.4286504 -4.4286189 -4.4285774 -4.4285326 -4.4285712 -4.4286566 -4.4287291 -4.4287882 -4.4288077 -4.4287825 -4.4287577 -4.4287744 -4.4288063 -4.428823 -4.4288592][-4.4286137 -4.4285727 -4.4285197 -4.4284635 -4.428514 -4.4286213 -4.4286938 -4.4287572 -4.4287748 -4.428741 -4.4287109 -4.4287281 -4.4287629 -4.4287896 -4.4288325][-4.4285808 -4.4285259 -4.4284573 -4.4283915 -4.4284611 -4.4285917 -4.4286628 -4.4287148 -4.4287271 -4.42869 -4.4286613 -4.4286838 -4.4287205 -4.4287529 -4.4288025][-4.4285235 -4.4284563 -4.4283767 -4.428297 -4.4283733 -4.4285212 -4.4286008 -4.4286489 -4.4286633 -4.4286304 -4.4286079 -4.4286385 -4.4286861 -4.4287305 -4.4287848][-4.4284806 -4.4283886 -4.428288 -4.4281864 -4.42826 -4.4284172 -4.4285145 -4.428575 -4.4285955 -4.4285703 -4.4285526 -4.4285884 -4.4286518 -4.4287124 -4.428772][-4.4284315 -4.4283175 -4.4281878 -4.4280639 -4.4281325 -4.4282942 -4.4284053 -4.4284906 -4.4285221 -4.428503 -4.4284849 -4.4285231 -4.4286008 -4.4286785 -4.428751][-4.4284 -4.4282842 -4.4281497 -4.4280305 -4.4280987 -4.4282508 -4.428359 -4.4284439 -4.4284749 -4.42846 -4.4284453 -4.4284825 -4.4285612 -4.4286442 -4.4287257][-4.42843 -4.4283366 -4.4282255 -4.4281254 -4.4281783 -4.4282951 -4.4283805 -4.4284415 -4.4284577 -4.4284434 -4.4284334 -4.4284668 -4.42854 -4.4286213 -4.4287076][-4.4284897 -4.4284196 -4.4283357 -4.4282589 -4.4282947 -4.4283719 -4.4284334 -4.4284773 -4.4284868 -4.4284711 -4.4284663 -4.4285011 -4.42857 -4.4286456 -4.4287286][-4.4285779 -4.42852 -4.4284616 -4.4284081 -4.42843 -4.4284782 -4.4285259 -4.4285636 -4.4285693 -4.4285536 -4.4285531 -4.42859 -4.428648 -4.4287124 -4.4287868][-4.4286447 -4.4286022 -4.4285722 -4.42855 -4.4285674 -4.4286 -4.428637 -4.4286637 -4.4286652 -4.4286504 -4.4286485 -4.4286776 -4.4287214 -4.428772 -4.428833][-4.4287024 -4.4286718 -4.4286594 -4.4286566 -4.4286718 -4.4286933 -4.4287171 -4.4287348 -4.4287372 -4.4287276 -4.4287238 -4.4287415 -4.4287729 -4.4288106 -4.4288611][-4.4287705 -4.4287477 -4.4287405 -4.42874 -4.4287515 -4.4287682 -4.4287868 -4.4288015 -4.4288063 -4.428803 -4.4287987 -4.4288068 -4.4288244 -4.4288487 -4.4288869]]...]
INFO - root - 2017-12-08 10:26:47.917478: step 1510, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:33m:00s remains)
INFO - root - 2017-12-08 10:26:52.388466: step 1520, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:57m:29s remains)
INFO - root - 2017-12-08 10:26:56.855032: step 1530, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:52m:33s remains)
INFO - root - 2017-12-08 10:27:01.333632: step 1540, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 39h:49m:21s remains)
INFO - root - 2017-12-08 10:27:05.812714: step 1550, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 40h:12m:13s remains)
INFO - root - 2017-12-08 10:27:10.309367: step 1560, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 39h:43m:18s remains)
INFO - root - 2017-12-08 10:27:14.866528: step 1570, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.448 sec/batch; 41h:12m:39s remains)
INFO - root - 2017-12-08 10:27:19.367701: step 1580, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:25m:51s remains)
INFO - root - 2017-12-08 10:27:23.670643: step 1590, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 39h:18m:11s remains)
INFO - root - 2017-12-08 10:27:28.210086: step 1600, loss = 2.28, batch loss = 2.23 (16.2 examples/sec; 0.492 sec/batch; 45h:15m:23s remains)
2017-12-08 10:27:28.773639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289489 -4.4289532 -4.4289589 -4.4289627 -4.428957 -4.428946 -4.4289293 -4.4289155 -4.4289179 -4.4289293 -4.4289474 -4.4289761 -4.4290004 -4.4290113 -4.4290018][-4.4289188 -4.428925 -4.4289408 -4.428957 -4.4289556 -4.4289436 -4.428925 -4.4289079 -4.4289093 -4.4289227 -4.4289432 -4.4289703 -4.4289889 -4.4289932 -4.4289804][-4.4288735 -4.4288774 -4.4288988 -4.428925 -4.4289308 -4.4289241 -4.4289093 -4.428894 -4.4288988 -4.4289174 -4.4289412 -4.4289637 -4.4289718 -4.428966 -4.4289474][-4.42882 -4.4288139 -4.4288292 -4.4288573 -4.4288664 -4.4288654 -4.4288564 -4.4288473 -4.4288568 -4.4288826 -4.428916 -4.4289379 -4.4289379 -4.4289255 -4.4289036][-4.42877 -4.4287429 -4.4287381 -4.4287519 -4.4287553 -4.4287519 -4.4287395 -4.42873 -4.4287529 -4.4288025 -4.4288549 -4.4288821 -4.4288816 -4.4288745 -4.4288626][-4.4287457 -4.4286957 -4.4286609 -4.4286451 -4.4286222 -4.428596 -4.4285569 -4.4285254 -4.4285679 -4.4286666 -4.42876 -4.4288096 -4.4288249 -4.4288335 -4.4288378][-4.4287443 -4.4286809 -4.4286242 -4.428576 -4.42851 -4.4284296 -4.4283233 -4.4282427 -4.4283042 -4.4284687 -4.4286237 -4.4287171 -4.4287643 -4.4287996 -4.4288206][-4.4287591 -4.4286966 -4.4286289 -4.4285593 -4.4284616 -4.4283276 -4.4281416 -4.4279928 -4.4280543 -4.428266 -4.4284716 -4.4286108 -4.4286985 -4.4287643 -4.4288058][-4.4287939 -4.4287519 -4.4287047 -4.4286604 -4.4285841 -4.4284587 -4.4282713 -4.42811 -4.4281378 -4.4283009 -4.428473 -4.4286075 -4.4286985 -4.4287686 -4.4288144][-4.4288149 -4.42879 -4.4287672 -4.4287543 -4.4287081 -4.4286137 -4.4284744 -4.4283509 -4.4283524 -4.428442 -4.4285479 -4.428647 -4.4287176 -4.4287744 -4.4288154][-4.428792 -4.428772 -4.4287643 -4.428771 -4.428741 -4.4286709 -4.4285769 -4.4284983 -4.4284983 -4.4285417 -4.4286003 -4.4286609 -4.4287057 -4.4287505 -4.4287891][-4.4287462 -4.42872 -4.4287181 -4.4287353 -4.4287195 -4.4286752 -4.4286246 -4.4285836 -4.4285917 -4.4286156 -4.4286466 -4.4286757 -4.4286962 -4.4287267 -4.4287605][-4.4287028 -4.4286714 -4.428669 -4.4286833 -4.4286704 -4.4286432 -4.428618 -4.4285984 -4.4286118 -4.4286337 -4.4286575 -4.4286709 -4.42868 -4.4287062 -4.428741][-4.4286847 -4.42865 -4.4286413 -4.4286461 -4.4286342 -4.4286246 -4.4286156 -4.4286051 -4.4286222 -4.4286485 -4.4286723 -4.4286804 -4.4286861 -4.4287124 -4.4287443][-4.4286985 -4.4286618 -4.4286485 -4.428647 -4.4286389 -4.4286404 -4.428638 -4.4286323 -4.4286518 -4.4286766 -4.4286971 -4.4287047 -4.4287157 -4.4287415 -4.4287682]]...]
INFO - root - 2017-12-08 10:27:33.353444: step 1610, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:43m:03s remains)
INFO - root - 2017-12-08 10:27:38.000085: step 1620, loss = 2.28, batch loss = 2.23 (16.9 examples/sec; 0.473 sec/batch; 43h:28m:11s remains)
INFO - root - 2017-12-08 10:27:42.645927: step 1630, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:25m:34s remains)
INFO - root - 2017-12-08 10:27:47.184077: step 1640, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:34m:22s remains)
INFO - root - 2017-12-08 10:27:51.768730: step 1650, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:45m:46s remains)
INFO - root - 2017-12-08 10:27:56.380230: step 1660, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.456 sec/batch; 41h:54m:55s remains)
INFO - root - 2017-12-08 10:28:00.876966: step 1670, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 41h:22m:48s remains)
INFO - root - 2017-12-08 10:28:05.442048: step 1680, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.469 sec/batch; 43h:04m:40s remains)
INFO - root - 2017-12-08 10:28:09.717591: step 1690, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 42h:06m:26s remains)
INFO - root - 2017-12-08 10:28:14.260525: step 1700, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.462 sec/batch; 42h:24m:28s remains)
2017-12-08 10:28:14.736350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287467 -4.4287415 -4.4287243 -4.4286914 -4.4286513 -4.42861 -4.4285851 -4.4285922 -4.4286323 -4.4286938 -4.4287529 -4.428781 -4.4287744 -4.428761 -4.4287386][-4.4287438 -4.4287472 -4.4287348 -4.4287033 -4.4286571 -4.4286175 -4.4286027 -4.4286165 -4.4286523 -4.4287062 -4.4287581 -4.4287844 -4.4287753 -4.4287624 -4.4287477][-4.4287777 -4.4287839 -4.4287705 -4.4287415 -4.4286985 -4.4286633 -4.4286537 -4.4286671 -4.4287028 -4.4287562 -4.4287996 -4.42882 -4.4288096 -4.4287925 -4.4287782][-4.4288111 -4.4288187 -4.4288073 -4.42878 -4.4287443 -4.4287081 -4.4286852 -4.4286814 -4.4287128 -4.4287758 -4.4288249 -4.4288478 -4.4288397 -4.42882 -4.4287982][-4.4288268 -4.4288387 -4.4288268 -4.4287949 -4.4287539 -4.4287057 -4.428658 -4.4286323 -4.4286628 -4.4287505 -4.428822 -4.4288511 -4.4288449 -4.4288259 -4.4288049][-4.4288187 -4.4288368 -4.4288216 -4.4287786 -4.4287281 -4.4286609 -4.4285717 -4.4284978 -4.4285197 -4.4286466 -4.4287634 -4.4288158 -4.4288216 -4.4288144 -4.4288073][-4.4288063 -4.4288211 -4.4288044 -4.4287562 -4.4286942 -4.4285946 -4.4284291 -4.4282532 -4.428246 -4.4284344 -4.4286213 -4.4287186 -4.4287548 -4.4287691 -4.4287839][-4.428791 -4.4288006 -4.4287872 -4.428741 -4.4286723 -4.4285507 -4.4283266 -4.4280577 -4.4280086 -4.4282413 -4.4284868 -4.428628 -4.4286942 -4.4287305 -4.4287553][-4.428762 -4.4287643 -4.4287586 -4.4287276 -4.42868 -4.428587 -4.4283981 -4.4281683 -4.4280996 -4.4282627 -4.42846 -4.4285917 -4.4286642 -4.4287114 -4.4287376][-4.4287248 -4.4287176 -4.4287214 -4.4287262 -4.4287243 -4.4286857 -4.4285769 -4.428443 -4.4283867 -4.4284482 -4.4285469 -4.4286232 -4.42867 -4.4287043 -4.4287243][-4.4287076 -4.4287009 -4.4287109 -4.4287353 -4.4287615 -4.4287596 -4.4287076 -4.4286385 -4.428606 -4.4286203 -4.4286475 -4.4286747 -4.4286962 -4.4287152 -4.4287205][-4.4287248 -4.4287262 -4.4287338 -4.4287505 -4.4287724 -4.4287806 -4.4287553 -4.42872 -4.428709 -4.4287119 -4.4287095 -4.4287143 -4.4287233 -4.4287333 -4.4287233][-4.4287457 -4.4287567 -4.4287643 -4.4287667 -4.4287748 -4.4287825 -4.4287667 -4.4287457 -4.4287448 -4.4287438 -4.4287271 -4.4287138 -4.4287171 -4.4287281 -4.4287095][-4.428761 -4.4287786 -4.4287887 -4.4287877 -4.4287925 -4.4287968 -4.428781 -4.42876 -4.4287534 -4.4287438 -4.4287148 -4.4286904 -4.4286914 -4.4287081 -4.4286919][-4.4287663 -4.4287748 -4.4287829 -4.4287858 -4.4287949 -4.4288 -4.4287872 -4.4287658 -4.4287496 -4.4287286 -4.4286942 -4.4286685 -4.4286747 -4.4286995 -4.4286957]]...]
INFO - root - 2017-12-08 10:28:19.142313: step 1710, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:18m:04s remains)
INFO - root - 2017-12-08 10:28:23.589186: step 1720, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:45m:41s remains)
INFO - root - 2017-12-08 10:28:28.095885: step 1730, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:25m:32s remains)
INFO - root - 2017-12-08 10:28:32.547983: step 1740, loss = 2.28, batch loss = 2.23 (16.9 examples/sec; 0.474 sec/batch; 43h:32m:36s remains)
INFO - root - 2017-12-08 10:28:37.012445: step 1750, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:29m:08s remains)
INFO - root - 2017-12-08 10:28:41.478740: step 1760, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 41h:20m:37s remains)
INFO - root - 2017-12-08 10:28:46.096972: step 1770, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.462 sec/batch; 42h:25m:31s remains)
INFO - root - 2017-12-08 10:28:50.689745: step 1780, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.462 sec/batch; 42h:25m:25s remains)
INFO - root - 2017-12-08 10:28:55.023183: step 1790, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:57m:13s remains)
INFO - root - 2017-12-08 10:28:59.544813: step 1800, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 41h:19m:37s remains)
2017-12-08 10:29:00.022677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428627 -4.4285965 -4.4285817 -4.4285865 -4.4285913 -4.4286108 -4.4286447 -4.4286847 -4.4287262 -4.4287505 -4.4287558 -4.4287462 -4.4287353 -4.42873 -4.428721][-4.4285874 -4.4285321 -4.4285021 -4.4285069 -4.4285226 -4.4285531 -4.4285975 -4.4286528 -4.4287114 -4.4287434 -4.4287457 -4.4287305 -4.4287114 -4.4287014 -4.4286895][-4.4286008 -4.4285378 -4.4284935 -4.4284797 -4.4284897 -4.4285245 -4.4285741 -4.4286394 -4.4287114 -4.4287572 -4.4287596 -4.4287376 -4.4287152 -4.4287047 -4.4286933][-4.42866 -4.4286089 -4.4285564 -4.4285135 -4.4284973 -4.4285164 -4.4285583 -4.4286289 -4.4287205 -4.4287763 -4.4287777 -4.428751 -4.4287262 -4.42871 -4.4286947][-4.428719 -4.4286876 -4.4286456 -4.4285893 -4.4285388 -4.4285169 -4.4285307 -4.4286003 -4.4287076 -4.4287729 -4.4287724 -4.42874 -4.4287109 -4.4286809 -4.4286528][-4.4287391 -4.4287333 -4.4287186 -4.4286695 -4.4285984 -4.4285278 -4.4284978 -4.4285522 -4.4286671 -4.4287467 -4.4287515 -4.4287152 -4.4286728 -4.4286156 -4.4285588][-4.4287233 -4.4287333 -4.4287424 -4.4287138 -4.42864 -4.4285254 -4.4284334 -4.4284463 -4.4285707 -4.4286938 -4.4287262 -4.4286838 -4.4286227 -4.4285378 -4.4284353][-4.4286809 -4.4287062 -4.4287324 -4.4287219 -4.4286485 -4.4284949 -4.4283166 -4.4282656 -4.4284072 -4.4286 -4.4286809 -4.4286494 -4.4285851 -4.42849 -4.4283504][-4.4286227 -4.4286575 -4.4287024 -4.4287024 -4.4286342 -4.4284749 -4.4282374 -4.4281173 -4.4282575 -4.428504 -4.4286332 -4.4286313 -4.4285846 -4.4285064 -4.4283676][-4.4285693 -4.4286008 -4.4286575 -4.4286714 -4.4286251 -4.4285145 -4.428309 -4.42816 -4.4282489 -4.4284754 -4.428618 -4.4286304 -4.4286079 -4.4285703 -4.42847][-4.4285583 -4.4285693 -4.4286184 -4.4286451 -4.4286323 -4.4285951 -4.4284897 -4.4283829 -4.4284105 -4.4285531 -4.4286537 -4.428659 -4.4286528 -4.428647 -4.428587][-4.4286203 -4.4285989 -4.428628 -4.4286528 -4.4286604 -4.4286747 -4.4286509 -4.4286013 -4.4286003 -4.4286613 -4.4287057 -4.4287043 -4.4287038 -4.4287119 -4.4286833][-4.42873 -4.4286804 -4.4286814 -4.4286976 -4.42871 -4.4287376 -4.428751 -4.428741 -4.4287405 -4.4287615 -4.428772 -4.4287677 -4.4287724 -4.4287882 -4.4287772][-4.428834 -4.4287815 -4.4287624 -4.4287667 -4.4287767 -4.4288025 -4.42882 -4.4288211 -4.4288268 -4.42884 -4.4288454 -4.4288464 -4.4288559 -4.4288688 -4.4288611][-4.4288869 -4.4288478 -4.4288235 -4.4288173 -4.428822 -4.42884 -4.42885 -4.4288511 -4.4288568 -4.4288697 -4.4288807 -4.4288888 -4.4288988 -4.428905 -4.4288983]]...]
INFO - root - 2017-12-08 10:29:04.378558: step 1810, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 41h:03m:36s remains)
INFO - root - 2017-12-08 10:29:08.929392: step 1820, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 40h:05m:44s remains)
INFO - root - 2017-12-08 10:29:13.351916: step 1830, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 39h:29m:12s remains)
INFO - root - 2017-12-08 10:29:17.835996: step 1840, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.446 sec/batch; 40h:56m:02s remains)
INFO - root - 2017-12-08 10:29:22.346764: step 1850, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:13m:53s remains)
INFO - root - 2017-12-08 10:29:26.918737: step 1860, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:11m:36s remains)
INFO - root - 2017-12-08 10:29:31.455928: step 1870, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:26m:27s remains)
INFO - root - 2017-12-08 10:29:35.790780: step 1880, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:23m:14s remains)
INFO - root - 2017-12-08 10:29:39.946290: step 1890, loss = 2.28, batch loss = 2.23 (19.5 examples/sec; 0.410 sec/batch; 37h:36m:27s remains)
INFO - root - 2017-12-08 10:29:44.420783: step 1900, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.456 sec/batch; 41h:52m:24s remains)
2017-12-08 10:29:44.956224: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287257 -4.4287138 -4.4287157 -4.4287286 -4.4287534 -4.4287877 -4.42882 -4.4288368 -4.4288411 -4.4288306 -4.4288111 -4.4288015 -4.4288039 -4.4288006 -4.4287815][-4.428721 -4.4287071 -4.4287152 -4.4287376 -4.4287705 -4.4288116 -4.4288478 -4.4288688 -4.4288754 -4.4288692 -4.42886 -4.4288635 -4.4288735 -4.4288726 -4.4288564][-4.4287634 -4.4287558 -4.4287643 -4.428782 -4.4288092 -4.4288411 -4.4288683 -4.4288869 -4.4288983 -4.4289041 -4.4289103 -4.428926 -4.4289379 -4.4289355 -4.4289212][-4.4288049 -4.4287996 -4.4287992 -4.428802 -4.4288106 -4.4288206 -4.4288244 -4.4288316 -4.42885 -4.4288697 -4.4288864 -4.4289045 -4.4289141 -4.4289112 -4.4289074][-4.4288044 -4.4287877 -4.428762 -4.4287305 -4.4287024 -4.4286757 -4.428648 -4.4286437 -4.4286752 -4.4287124 -4.4287376 -4.4287558 -4.4287639 -4.4287653 -4.4287767][-4.4287348 -4.4286909 -4.4286284 -4.4285469 -4.4284673 -4.4283881 -4.4283156 -4.4282966 -4.4283447 -4.4284139 -4.4284639 -4.4284997 -4.4285278 -4.4285531 -4.4285893][-4.4286308 -4.4285636 -4.4284773 -4.4283652 -4.4282522 -4.4281363 -4.428031 -4.4280057 -4.428082 -4.4281926 -4.4282794 -4.4283471 -4.4284053 -4.428453 -4.4285064][-4.42859 -4.4285245 -4.4284573 -4.4283729 -4.4282956 -4.4282231 -4.4281564 -4.4281578 -4.4282393 -4.4283385 -4.4284134 -4.4284735 -4.4285221 -4.428555 -4.42859][-4.428637 -4.4285941 -4.428565 -4.4285293 -4.4285021 -4.4284797 -4.4284544 -4.4284639 -4.4285164 -4.4285736 -4.4286175 -4.4286489 -4.4286728 -4.4286814 -4.4286942][-4.428699 -4.4286761 -4.4286747 -4.4286752 -4.4286795 -4.4286804 -4.4286709 -4.4286771 -4.4287066 -4.4287343 -4.4287534 -4.428762 -4.428761 -4.4287505 -4.4287491][-4.4287596 -4.4287586 -4.4287834 -4.4288092 -4.4288235 -4.4288225 -4.428812 -4.428812 -4.4288235 -4.428833 -4.428833 -4.4288235 -4.4288082 -4.42879 -4.428782][-4.4288254 -4.428834 -4.4288683 -4.4289012 -4.4289117 -4.4289026 -4.4288859 -4.4288774 -4.4288731 -4.428863 -4.428844 -4.4288235 -4.4288058 -4.428791 -4.4287858][-4.4288564 -4.428865 -4.4288926 -4.4289141 -4.4289136 -4.428896 -4.4288764 -4.4288607 -4.4288464 -4.4288282 -4.4288039 -4.4287815 -4.4287686 -4.4287591 -4.4287548][-4.4288454 -4.4288392 -4.428843 -4.428843 -4.4288321 -4.4288177 -4.4288135 -4.4288125 -4.4288073 -4.4287963 -4.428781 -4.4287686 -4.4287658 -4.4287605 -4.4287548][-4.4287853 -4.4287643 -4.42875 -4.4287372 -4.4287257 -4.4287262 -4.4287462 -4.4287653 -4.4287729 -4.4287758 -4.4287796 -4.42879 -4.4288034 -4.4288063 -4.4288006]]...]
INFO - root - 2017-12-08 10:29:49.495392: step 1910, loss = 2.28, batch loss = 2.23 (16.8 examples/sec; 0.476 sec/batch; 43h:42m:10s remains)
INFO - root - 2017-12-08 10:29:53.967451: step 1920, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:55m:24s remains)
INFO - root - 2017-12-08 10:29:58.387358: step 1930, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.438 sec/batch; 40h:15m:42s remains)
INFO - root - 2017-12-08 10:30:02.935804: step 1940, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:56m:46s remains)
INFO - root - 2017-12-08 10:30:07.583748: step 1950, loss = 2.28, batch loss = 2.23 (16.3 examples/sec; 0.490 sec/batch; 45h:00m:24s remains)
INFO - root - 2017-12-08 10:30:12.047195: step 1960, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:43m:47s remains)
INFO - root - 2017-12-08 10:30:16.472089: step 1970, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:53m:26s remains)
INFO - root - 2017-12-08 10:30:20.983187: step 1980, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:53m:34s remains)
INFO - root - 2017-12-08 10:30:25.269726: step 1990, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.461 sec/batch; 42h:22m:05s remains)
INFO - root - 2017-12-08 10:30:29.853724: step 2000, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:30m:31s remains)
2017-12-08 10:30:30.438481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289484 -4.4289489 -4.4289508 -4.4289551 -4.4289622 -4.428967 -4.42897 -4.4289703 -4.4289708 -4.4289746 -4.42898 -4.4289842 -4.4289894 -4.4289947 -4.4289947][-4.4289412 -4.4289384 -4.4289374 -4.4289389 -4.4289422 -4.4289441 -4.428946 -4.4289441 -4.4289412 -4.4289408 -4.4289455 -4.4289556 -4.4289732 -4.4289904 -4.428998][-4.4289269 -4.4289141 -4.4289017 -4.4288931 -4.4288878 -4.4288831 -4.4288836 -4.4288788 -4.428865 -4.4288568 -4.4288616 -4.4288831 -4.42892 -4.4289536 -4.4289765][-4.4289126 -4.4288831 -4.4288564 -4.4288354 -4.4288197 -4.4288087 -4.4288077 -4.4287958 -4.4287643 -4.4287391 -4.4287357 -4.4287634 -4.4288168 -4.4288726 -4.4289217][-4.4288936 -4.4288478 -4.42881 -4.4287825 -4.4287615 -4.4287491 -4.4287415 -4.4287138 -4.42866 -4.4286094 -4.4285846 -4.428607 -4.4286733 -4.4287562 -4.4288397][-4.4288626 -4.4288006 -4.4287515 -4.4287167 -4.4286952 -4.4286876 -4.4286795 -4.4286356 -4.4285531 -4.4284744 -4.4284244 -4.4284363 -4.428514 -4.4286265 -4.4287477][-4.428813 -4.4287333 -4.4286714 -4.4286294 -4.428606 -4.4286041 -4.4286 -4.428544 -4.4284391 -4.4283462 -4.4282856 -4.4282975 -4.4283943 -4.4285316 -4.4286819][-4.4287376 -4.4286404 -4.4285564 -4.4284978 -4.428472 -4.4284773 -4.4284849 -4.42843 -4.428329 -4.4282556 -4.4282193 -4.4282513 -4.4283586 -4.4285064 -4.4286623][-4.4286337 -4.4285092 -4.4283919 -4.4283032 -4.4282746 -4.4283023 -4.4283395 -4.4283037 -4.428236 -4.4282107 -4.4282122 -4.4282675 -4.4283795 -4.4285274 -4.4286752][-4.4285288 -4.4283743 -4.42822 -4.4280877 -4.4280682 -4.4281459 -4.4282441 -4.4282575 -4.4282422 -4.42826 -4.4282918 -4.428349 -4.4284515 -4.4285822 -4.4287086][-4.4284878 -4.4283195 -4.4281526 -4.4280057 -4.4280157 -4.4281406 -4.4282861 -4.4283476 -4.4283748 -4.4284124 -4.4284463 -4.4284873 -4.4285593 -4.42866 -4.4287567][-4.4285588 -4.4284234 -4.4283056 -4.4282074 -4.4282284 -4.4283371 -4.428463 -4.4285312 -4.4285712 -4.4286027 -4.4286227 -4.4286427 -4.4286857 -4.428751 -4.4288149][-4.4286995 -4.4286146 -4.4285536 -4.4285069 -4.4285235 -4.4285903 -4.4286704 -4.428719 -4.4287457 -4.4287605 -4.4287667 -4.4287758 -4.4287958 -4.4288311 -4.4288664][-4.4288216 -4.4287748 -4.4287477 -4.42873 -4.428741 -4.4287734 -4.4288139 -4.4288425 -4.4288583 -4.4288616 -4.42886 -4.4288626 -4.4288697 -4.4288859 -4.4289017][-4.4288983 -4.4288764 -4.4288664 -4.4288588 -4.4288611 -4.428874 -4.4288917 -4.4289088 -4.4289165 -4.428916 -4.4289122 -4.4289107 -4.4289112 -4.4289179 -4.4289236]]...]
INFO - root - 2017-12-08 10:30:34.973405: step 2010, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:28m:07s remains)
INFO - root - 2017-12-08 10:30:39.496069: step 2020, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 41h:05m:15s remains)
INFO - root - 2017-12-08 10:30:43.956612: step 2030, loss = 2.28, batch loss = 2.23 (17.0 examples/sec; 0.471 sec/batch; 43h:16m:14s remains)
INFO - root - 2017-12-08 10:30:48.380157: step 2040, loss = 2.28, batch loss = 2.23 (16.9 examples/sec; 0.473 sec/batch; 43h:27m:07s remains)
INFO - root - 2017-12-08 10:30:52.796162: step 2050, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:15m:14s remains)
INFO - root - 2017-12-08 10:30:57.216653: step 2060, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:52m:39s remains)
INFO - root - 2017-12-08 10:31:01.577323: step 2070, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:17m:04s remains)
INFO - root - 2017-12-08 10:31:06.065719: step 2080, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 41h:20m:00s remains)
INFO - root - 2017-12-08 10:31:10.269878: step 2090, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.464 sec/batch; 42h:36m:19s remains)
INFO - root - 2017-12-08 10:31:14.805338: step 2100, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:55m:14s remains)
2017-12-08 10:31:15.316154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289312 -4.4289093 -4.4288783 -4.4288316 -4.4287844 -4.4287481 -4.4287333 -4.4287372 -4.4287596 -4.4287949 -4.4288406 -4.4288816 -4.4289117 -4.4289327 -4.4289446][-4.4289131 -4.4288826 -4.4288387 -4.4287724 -4.4287009 -4.4286494 -4.4286327 -4.4286456 -4.4286771 -4.4287186 -4.4287786 -4.4288416 -4.4288917 -4.428926 -4.4289451][-4.4288988 -4.4288583 -4.4288015 -4.4287171 -4.428617 -4.4285479 -4.4285293 -4.4285469 -4.4285879 -4.4286389 -4.4287143 -4.4287982 -4.4288688 -4.4289169 -4.4289441][-4.4288845 -4.4288354 -4.4287705 -4.4286709 -4.4285445 -4.4284568 -4.4284353 -4.4284525 -4.4284973 -4.4285607 -4.42865 -4.4287524 -4.4288406 -4.4289007 -4.428937][-4.4288692 -4.4288139 -4.4287438 -4.428628 -4.4284711 -4.4283609 -4.4283457 -4.4283752 -4.4284215 -4.4284925 -4.4285922 -4.428709 -4.4288096 -4.4288778 -4.4289227][-4.4288573 -4.4287977 -4.4287238 -4.4285946 -4.4284058 -4.4282632 -4.4282656 -4.4283328 -4.4283895 -4.428453 -4.4285541 -4.4286785 -4.428781 -4.4288526 -4.4289017][-4.4288535 -4.4287963 -4.4287233 -4.4285946 -4.4283948 -4.428216 -4.4282207 -4.4283285 -4.4284058 -4.4284635 -4.4285507 -4.4286671 -4.428761 -4.4288268 -4.4288778][-4.4288645 -4.4288125 -4.4287491 -4.4286442 -4.428473 -4.4282889 -4.4282589 -4.4283671 -4.4284573 -4.4285088 -4.4285784 -4.4286747 -4.4287534 -4.4288082 -4.4288573][-4.4288855 -4.4288425 -4.4287944 -4.4287229 -4.4286046 -4.428452 -4.4283757 -4.4284291 -4.4285045 -4.4285469 -4.4286032 -4.4286847 -4.4287529 -4.4287996 -4.4288425][-4.4289017 -4.4288716 -4.4288383 -4.4287915 -4.4287152 -4.4286032 -4.4285054 -4.4284949 -4.4285297 -4.4285574 -4.4286046 -4.4286785 -4.4287472 -4.4287915 -4.4288325][-4.4289055 -4.4288888 -4.42887 -4.4288378 -4.4287853 -4.4287028 -4.4286127 -4.4285645 -4.4285517 -4.4285522 -4.4285841 -4.4286513 -4.4287271 -4.428781 -4.4288287][-4.4289036 -4.428896 -4.4288859 -4.4288626 -4.428822 -4.4287634 -4.4286928 -4.4286342 -4.4285865 -4.4285541 -4.4285665 -4.4286227 -4.4287024 -4.4287677 -4.4288249][-4.42889 -4.4288831 -4.4288793 -4.4288621 -4.4288311 -4.4287882 -4.4287357 -4.4286833 -4.4286256 -4.4285769 -4.4285736 -4.4286175 -4.4286914 -4.4287586 -4.4288192][-4.4288673 -4.4288588 -4.4288578 -4.4288464 -4.4288211 -4.4287882 -4.428751 -4.4287128 -4.4286652 -4.4286184 -4.4286041 -4.4286313 -4.4286923 -4.4287543 -4.4288116][-4.428844 -4.428833 -4.4288311 -4.4288235 -4.4288054 -4.4287806 -4.4287548 -4.4287314 -4.4286995 -4.4286623 -4.4286423 -4.4286571 -4.4287038 -4.4287572 -4.4288068]]...]
INFO - root - 2017-12-08 10:31:19.771351: step 2110, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:42m:31s remains)
INFO - root - 2017-12-08 10:31:24.271330: step 2120, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.460 sec/batch; 42h:12m:31s remains)
INFO - root - 2017-12-08 10:31:28.824688: step 2130, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.463 sec/batch; 42h:28m:22s remains)
INFO - root - 2017-12-08 10:31:33.297560: step 2140, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:30m:36s remains)
INFO - root - 2017-12-08 10:31:37.768085: step 2150, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.461 sec/batch; 42h:16m:51s remains)
INFO - root - 2017-12-08 10:31:42.177805: step 2160, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:26m:58s remains)
INFO - root - 2017-12-08 10:31:46.673880: step 2170, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.461 sec/batch; 42h:17m:19s remains)
INFO - root - 2017-12-08 10:31:51.160377: step 2180, loss = 2.28, batch loss = 2.23 (16.9 examples/sec; 0.472 sec/batch; 43h:21m:12s remains)
INFO - root - 2017-12-08 10:31:55.396945: step 2190, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:56m:57s remains)
INFO - root - 2017-12-08 10:31:59.817947: step 2200, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.446 sec/batch; 40h:52m:31s remains)
2017-12-08 10:32:00.337615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285784 -4.4286528 -4.4287639 -4.4288435 -4.4288874 -4.428905 -4.4289041 -4.4289017 -4.4289045 -4.4289031 -4.4289069 -4.4289083 -4.4289069 -4.4288993 -4.4288869][-4.4284444 -4.428535 -4.4286847 -4.4287953 -4.4288487 -4.4288607 -4.4288478 -4.4288459 -4.4288611 -4.4288788 -4.4289007 -4.4289103 -4.4289064 -4.4288921 -4.4288735][-4.4283433 -4.4284425 -4.4286261 -4.4287581 -4.4288106 -4.4288039 -4.4287748 -4.4287763 -4.4288106 -4.428854 -4.428896 -4.428916 -4.4289126 -4.4288926 -4.4288697][-4.4283786 -4.4284754 -4.4286466 -4.4287586 -4.42878 -4.4287348 -4.42868 -4.42869 -4.4287548 -4.4288278 -4.4288893 -4.4289212 -4.4289203 -4.4288974 -4.4288731][-4.4285488 -4.428617 -4.4287286 -4.4287853 -4.4287567 -4.4286604 -4.4285741 -4.4285913 -4.4286942 -4.4287987 -4.4288769 -4.4289184 -4.4289212 -4.4289 -4.4288774][-4.4286795 -4.4287195 -4.4287796 -4.4287887 -4.4287148 -4.428566 -4.4284415 -4.4284678 -4.4286151 -4.4287524 -4.4288478 -4.4289 -4.4289088 -4.428894 -4.4288759][-4.4287667 -4.4287882 -4.4288096 -4.428782 -4.4286728 -4.4284773 -4.4283051 -4.4283338 -4.4285192 -4.4286909 -4.4288054 -4.4288659 -4.4288821 -4.4288769 -4.4288654][-4.4288311 -4.428844 -4.4288335 -4.4287782 -4.4286561 -4.4284487 -4.4282551 -4.4282718 -4.428463 -4.4286456 -4.4287648 -4.428823 -4.4288416 -4.4288464 -4.4288473][-4.42883 -4.4288559 -4.42884 -4.4287829 -4.4286847 -4.4285274 -4.4283805 -4.4283838 -4.4285254 -4.4286652 -4.4287572 -4.4287977 -4.4288068 -4.4288149 -4.4288268][-4.4287653 -4.4288216 -4.4288316 -4.4287958 -4.4287362 -4.4286447 -4.4285579 -4.4285512 -4.4286313 -4.4287176 -4.4287758 -4.4287934 -4.4287853 -4.4287896 -4.4288082][-4.42865 -4.4287515 -4.4288154 -4.4288173 -4.4287868 -4.4287353 -4.4286761 -4.4286633 -4.4287038 -4.4287567 -4.4287925 -4.4287963 -4.4287782 -4.4287777 -4.4287982][-4.4285326 -4.4286747 -4.4287963 -4.4288363 -4.4288282 -4.4287968 -4.42876 -4.4287562 -4.4287753 -4.4288025 -4.4288182 -4.4288139 -4.4287953 -4.4287906 -4.4288068][-4.4285331 -4.4286785 -4.428812 -4.4288573 -4.4288445 -4.4288068 -4.428782 -4.4287906 -4.4288135 -4.4288378 -4.4288511 -4.4288492 -4.4288368 -4.4288311 -4.4288392][-4.4286332 -4.4287391 -4.4288411 -4.4288654 -4.4288244 -4.4287524 -4.4287109 -4.428731 -4.4287882 -4.4288406 -4.42887 -4.4288764 -4.4288716 -4.4288688 -4.4288712][-4.4287133 -4.4287739 -4.42884 -4.4288435 -4.4287715 -4.4286494 -4.428565 -4.42859 -4.4286971 -4.4288025 -4.4288616 -4.4288783 -4.4288769 -4.4288754 -4.4288769]]...]
INFO - root - 2017-12-08 10:32:04.864362: step 2210, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.459 sec/batch; 42h:06m:09s remains)
INFO - root - 2017-12-08 10:32:09.290985: step 2220, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:18m:43s remains)
INFO - root - 2017-12-08 10:32:13.834003: step 2230, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.459 sec/batch; 42h:06m:12s remains)
INFO - root - 2017-12-08 10:32:18.287639: step 2240, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 41h:03m:34s remains)
INFO - root - 2017-12-08 10:32:22.773559: step 2250, loss = 2.28, batch loss = 2.23 (16.9 examples/sec; 0.473 sec/batch; 43h:21m:22s remains)
INFO - root - 2017-12-08 10:32:27.223125: step 2260, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:37m:22s remains)
INFO - root - 2017-12-08 10:32:31.657362: step 2270, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 39h:37m:39s remains)
INFO - root - 2017-12-08 10:32:36.108875: step 2280, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:42m:03s remains)
INFO - root - 2017-12-08 10:32:40.280013: step 2290, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:38m:09s remains)
INFO - root - 2017-12-08 10:32:44.873036: step 2300, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:45m:00s remains)
2017-12-08 10:32:45.403145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289107 -4.428896 -4.4288764 -4.4288559 -4.4288263 -4.4287977 -4.428762 -4.4287343 -4.4287229 -4.4287157 -4.4286828 -4.4286222 -4.4285588 -4.428555 -4.428628][-4.4288883 -4.428865 -4.4288411 -4.4288182 -4.4287853 -4.428751 -4.4287043 -4.4286628 -4.4286265 -4.4286003 -4.4285884 -4.4285822 -4.428575 -4.4286017 -4.4286737][-4.4288421 -4.428802 -4.4287739 -4.4287548 -4.4287257 -4.4286842 -4.4286256 -4.4285679 -4.4284992 -4.4284525 -4.4284773 -4.4285421 -4.4285994 -4.4286618 -4.4287357][-4.428781 -4.4287243 -4.4286909 -4.4286785 -4.4286647 -4.4286308 -4.4285779 -4.4285145 -4.4284272 -4.4283776 -4.4284282 -4.428524 -4.4286089 -4.4286842 -4.4287548][-4.4287491 -4.42869 -4.4286551 -4.4286418 -4.4286366 -4.4286151 -4.42857 -4.4285145 -4.4284525 -4.4284329 -4.4284854 -4.4285679 -4.4286418 -4.4287081 -4.4287667][-4.4287586 -4.4287076 -4.4286776 -4.4286547 -4.4286346 -4.4286013 -4.4285378 -4.4284658 -4.4284396 -4.4284854 -4.4285626 -4.4286385 -4.4287043 -4.4287615 -4.428803][-4.4287724 -4.4287333 -4.4287133 -4.4286752 -4.4286127 -4.4285226 -4.4283915 -4.4282823 -4.4283113 -4.4284506 -4.4285827 -4.4286785 -4.428751 -4.4288034 -4.4288311][-4.428751 -4.42872 -4.42871 -4.42865 -4.428525 -4.428359 -4.4281678 -4.4280596 -4.4281783 -4.4284039 -4.4285774 -4.4286871 -4.4287624 -4.4288135 -4.4288383][-4.4286842 -4.4286537 -4.4286442 -4.4285765 -4.4284492 -4.428299 -4.4281664 -4.42814 -4.4282808 -4.4284735 -4.42861 -4.4286971 -4.4287553 -4.4287992 -4.4288282][-4.4285793 -4.4285583 -4.4285712 -4.4285512 -4.4284964 -4.4284344 -4.4283972 -4.42842 -4.4285059 -4.428606 -4.4286757 -4.4287262 -4.4287605 -4.4287934 -4.4288249][-4.4285154 -4.4285364 -4.4285889 -4.4286137 -4.4286056 -4.428587 -4.4285803 -4.4285989 -4.4286394 -4.4286857 -4.4287248 -4.4287577 -4.428782 -4.4288039 -4.4288335][-4.4285631 -4.4286175 -4.4286737 -4.4286914 -4.4286776 -4.4286537 -4.4286332 -4.4286385 -4.4286685 -4.4287038 -4.4287419 -4.4287791 -4.4288077 -4.4288273 -4.4288535][-4.4286485 -4.4286866 -4.4287138 -4.4287114 -4.4286847 -4.4286518 -4.4286227 -4.4286284 -4.4286642 -4.4287043 -4.4287515 -4.4287992 -4.4288321 -4.4288545 -4.4288783][-4.4286828 -4.4286852 -4.4286833 -4.4286752 -4.4286594 -4.4286447 -4.4286313 -4.4286408 -4.42868 -4.4287243 -4.4287734 -4.4288244 -4.4288588 -4.4288816 -4.4288993][-4.4286551 -4.4286437 -4.4286394 -4.4286413 -4.4286475 -4.4286604 -4.4286695 -4.4286871 -4.4287224 -4.4287648 -4.42881 -4.4288511 -4.4288778 -4.428896 -4.4289079]]...]
INFO - root - 2017-12-08 10:32:49.943897: step 2310, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 39h:24m:51s remains)
INFO - root - 2017-12-08 10:32:54.470065: step 2320, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:27m:05s remains)
INFO - root - 2017-12-08 10:32:59.002237: step 2330, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:22m:51s remains)
INFO - root - 2017-12-08 10:33:03.520202: step 2340, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.461 sec/batch; 42h:18m:53s remains)
INFO - root - 2017-12-08 10:33:07.980691: step 2350, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:41m:52s remains)
INFO - root - 2017-12-08 10:33:12.449813: step 2360, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.456 sec/batch; 41h:48m:13s remains)
INFO - root - 2017-12-08 10:33:16.851787: step 2370, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 39h:16m:18s remains)
INFO - root - 2017-12-08 10:33:21.263703: step 2380, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 39h:15m:14s remains)
INFO - root - 2017-12-08 10:33:25.552554: step 2390, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.459 sec/batch; 42h:03m:37s remains)
INFO - root - 2017-12-08 10:33:30.069158: step 2400, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:24m:48s remains)
2017-12-08 10:33:30.573341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287124 -4.4287219 -4.4287367 -4.4287539 -4.428792 -4.4288297 -4.4288273 -4.4287949 -4.4287715 -4.428781 -4.4288082 -4.4288406 -4.4288712 -4.4288836 -4.428875][-4.4286885 -4.4287157 -4.4287353 -4.42876 -4.4288058 -4.4288378 -4.4288235 -4.4287753 -4.42874 -4.4287443 -4.4287753 -4.4288197 -4.4288592 -4.4288778 -4.4288716][-4.4286723 -4.4287205 -4.4287624 -4.4287987 -4.42884 -4.42885 -4.4288144 -4.4287524 -4.4287119 -4.4287167 -4.4287529 -4.4288025 -4.4288473 -4.4288707 -4.4288659][-4.4286952 -4.4287496 -4.4288006 -4.4288325 -4.4288478 -4.4288197 -4.4287581 -4.4286947 -4.4286637 -4.4286833 -4.4287338 -4.4287896 -4.4288349 -4.4288588 -4.4288564][-4.4287343 -4.4287791 -4.4288144 -4.4288211 -4.428793 -4.4287238 -4.4286437 -4.4285884 -4.4285851 -4.4286351 -4.4287105 -4.42878 -4.4288282 -4.4288526 -4.428853][-4.4287486 -4.4287786 -4.4287858 -4.4287591 -4.4286866 -4.4285789 -4.4284749 -4.4284215 -4.4284525 -4.4285502 -4.4286613 -4.4287543 -4.4288139 -4.4288449 -4.4288507][-4.4287086 -4.42872 -4.4287086 -4.4286623 -4.4285655 -4.4284306 -4.4282904 -4.428215 -4.4282703 -4.4284124 -4.4285622 -4.4286876 -4.42877 -4.4288173 -4.4288368][-4.4286509 -4.4286561 -4.4286418 -4.4286008 -4.428514 -4.428381 -4.4282165 -4.4281068 -4.4281492 -4.4282947 -4.4284568 -4.4286017 -4.4287062 -4.4287753 -4.4288144][-4.4286323 -4.4286413 -4.4286389 -4.4286227 -4.4285727 -4.4284735 -4.4283271 -4.4282074 -4.4282064 -4.4282994 -4.4284286 -4.4285622 -4.4286695 -4.4287472 -4.4287949][-4.4286537 -4.4286737 -4.4286876 -4.4286928 -4.4286728 -4.4286108 -4.4285049 -4.4284019 -4.4283705 -4.4284077 -4.428483 -4.428576 -4.4286628 -4.4287314 -4.4287786][-4.4287014 -4.4287238 -4.4287462 -4.4287615 -4.4287562 -4.428721 -4.4286523 -4.42858 -4.4285421 -4.428545 -4.4285784 -4.4286308 -4.4286876 -4.4287405 -4.4287806][-4.4287596 -4.428782 -4.428803 -4.4288173 -4.4288197 -4.4288073 -4.4287782 -4.4287395 -4.428709 -4.4286971 -4.4287076 -4.4287286 -4.42875 -4.4287777 -4.4288039][-4.4288125 -4.4288263 -4.42884 -4.428853 -4.4288645 -4.4288731 -4.4288716 -4.4288568 -4.4288359 -4.4288187 -4.4288135 -4.4288144 -4.4288125 -4.4288187 -4.42883][-4.4288487 -4.4288526 -4.4288597 -4.42887 -4.428884 -4.4289 -4.4289093 -4.428905 -4.4288912 -4.4288769 -4.4288707 -4.4288688 -4.4288621 -4.4288588 -4.4288583][-4.4288673 -4.4288669 -4.4288716 -4.4288788 -4.4288888 -4.4289007 -4.4289088 -4.4289064 -4.4288979 -4.4288912 -4.4288921 -4.4288955 -4.4288936 -4.4288898 -4.4288845]]...]
INFO - root - 2017-12-08 10:33:35.078627: step 2410, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:29m:45s remains)
INFO - root - 2017-12-08 10:33:39.671262: step 2420, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.429 sec/batch; 39h:17m:49s remains)
INFO - root - 2017-12-08 10:33:44.159209: step 2430, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:21m:06s remains)
INFO - root - 2017-12-08 10:33:48.605460: step 2440, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:54m:04s remains)
INFO - root - 2017-12-08 10:33:53.177788: step 2450, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.460 sec/batch; 42h:11m:59s remains)
INFO - root - 2017-12-08 10:33:57.665743: step 2460, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:36m:46s remains)
INFO - root - 2017-12-08 10:34:02.135835: step 2470, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:54m:58s remains)
INFO - root - 2017-12-08 10:34:06.639348: step 2480, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:39m:19s remains)
INFO - root - 2017-12-08 10:34:10.888784: step 2490, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.460 sec/batch; 42h:09m:21s remains)
INFO - root - 2017-12-08 10:34:15.302449: step 2500, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.429 sec/batch; 39h:22m:07s remains)
2017-12-08 10:34:15.803584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285603 -4.4285417 -4.4285541 -4.4285922 -4.4286523 -4.4287105 -4.4287686 -4.4288087 -4.428822 -4.4287958 -4.4287333 -4.42867 -4.4285984 -4.4285221 -4.4284587][-4.4285007 -4.428493 -4.4285059 -4.4285479 -4.4286141 -4.4286695 -4.4287162 -4.4287524 -4.4287853 -4.4287877 -4.4287581 -4.4287143 -4.4286418 -4.4285393 -4.4284396][-4.4285135 -4.42853 -4.4285541 -4.4285889 -4.4286346 -4.428659 -4.4286604 -4.4286656 -4.4287019 -4.4287429 -4.42876 -4.4287529 -4.4286947 -4.4285889 -4.4284706][-4.42852 -4.4285636 -4.4286065 -4.4286361 -4.4286613 -4.4286456 -4.4285855 -4.4285321 -4.4285512 -4.4286289 -4.4287024 -4.4287405 -4.4287086 -4.4286237 -4.4285212][-4.4285054 -4.4285712 -4.42862 -4.4286461 -4.4286585 -4.428606 -4.4284778 -4.4283452 -4.4283395 -4.4284592 -4.4285989 -4.428689 -4.42869 -4.4286385 -4.4285636][-4.4284806 -4.428545 -4.42859 -4.4286118 -4.4286027 -4.4285078 -4.428309 -4.4280977 -4.4280834 -4.4282718 -4.4284868 -4.4286275 -4.4286642 -4.4286404 -4.428597][-4.4284511 -4.4284983 -4.4285231 -4.4285212 -4.4284797 -4.4283504 -4.4281106 -4.4278646 -4.427886 -4.4281445 -4.4284096 -4.4285784 -4.4286423 -4.4286385 -4.4286203][-4.4284492 -4.4284573 -4.4284539 -4.428432 -4.4283838 -4.4282737 -4.4280853 -4.4279 -4.4279547 -4.428206 -4.4284477 -4.4285951 -4.4286561 -4.4286585 -4.4286556][-4.4284678 -4.4284296 -4.428411 -4.4284005 -4.4283791 -4.428328 -4.428237 -4.42815 -4.4282069 -4.4283862 -4.4285426 -4.4286346 -4.4286695 -4.4286685 -4.428669][-4.4284682 -4.428412 -4.428401 -4.4284286 -4.4284515 -4.42846 -4.4284353 -4.4284067 -4.42845 -4.4285526 -4.4286246 -4.4286547 -4.4286551 -4.4286523 -4.4286494][-4.4284706 -4.4284153 -4.4284239 -4.428503 -4.4285674 -4.4285994 -4.4285951 -4.4285803 -4.4286003 -4.4286523 -4.4286718 -4.4286666 -4.4286489 -4.4286366 -4.4286222][-4.4284954 -4.428453 -4.4284825 -4.4285831 -4.4286489 -4.4286714 -4.4286728 -4.4286709 -4.4286823 -4.4287071 -4.4287 -4.4286833 -4.4286656 -4.4286442 -4.428618][-4.428504 -4.4284892 -4.4285321 -4.428618 -4.4286604 -4.4286642 -4.4286709 -4.4286923 -4.4287114 -4.4287195 -4.4286985 -4.4286752 -4.42866 -4.4286327 -4.4285941][-4.4285121 -4.4285173 -4.4285645 -4.4286265 -4.4286313 -4.4286094 -4.4286132 -4.4286518 -4.4286737 -4.4286695 -4.42864 -4.4286165 -4.4286108 -4.4285831 -4.4285436][-4.4285383 -4.4285541 -4.4285946 -4.4286232 -4.4285765 -4.4285131 -4.4285088 -4.4285612 -4.428586 -4.4285674 -4.4285183 -4.4284987 -4.42851 -4.4284959 -4.4284611]]...]
INFO - root - 2017-12-08 10:34:20.406491: step 2510, loss = 2.28, batch loss = 2.23 (16.7 examples/sec; 0.478 sec/batch; 43h:48m:30s remains)
INFO - root - 2017-12-08 10:34:24.892497: step 2520, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:52m:01s remains)
INFO - root - 2017-12-08 10:34:29.406375: step 2530, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:09m:16s remains)
INFO - root - 2017-12-08 10:34:33.896703: step 2540, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:51m:54s remains)
INFO - root - 2017-12-08 10:34:38.370324: step 2550, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.459 sec/batch; 42h:01m:42s remains)
INFO - root - 2017-12-08 10:34:42.873242: step 2560, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 40h:03m:25s remains)
INFO - root - 2017-12-08 10:34:47.388394: step 2570, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.446 sec/batch; 40h:50m:38s remains)
INFO - root - 2017-12-08 10:34:51.877854: step 2580, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.456 sec/batch; 41h:46m:18s remains)
INFO - root - 2017-12-08 10:34:56.083740: step 2590, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:51m:34s remains)
INFO - root - 2017-12-08 10:35:00.614441: step 2600, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:20m:27s remains)
2017-12-08 10:35:01.118901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289322 -4.4289279 -4.4289312 -4.4289331 -4.42893 -4.4289246 -4.4289241 -4.4289255 -4.4289265 -4.4289274 -4.4289279 -4.4289274 -4.4289227 -4.4289174 -4.4289174][-4.4288559 -4.4288554 -4.4288697 -4.428885 -4.4288926 -4.4288926 -4.4288917 -4.4288926 -4.428895 -4.4288993 -4.4289036 -4.4289036 -4.4288936 -4.42888 -4.428874][-4.4287329 -4.4287324 -4.4287605 -4.4287949 -4.428822 -4.4288344 -4.4288387 -4.4288411 -4.4288445 -4.428853 -4.4288621 -4.4288664 -4.428853 -4.428833 -4.4288197][-4.4285889 -4.4285727 -4.4286013 -4.4286523 -4.4287 -4.4287271 -4.4287395 -4.4287481 -4.4287596 -4.4287791 -4.4288006 -4.4288144 -4.4288058 -4.4287858 -4.4287691][-4.428473 -4.4284267 -4.4284387 -4.4284878 -4.4285421 -4.4285727 -4.4285851 -4.4285975 -4.4286203 -4.4286561 -4.4286928 -4.4287186 -4.42872 -4.4287095 -4.4287019][-4.428412 -4.4283347 -4.4283185 -4.428349 -4.428391 -4.4284091 -4.4284105 -4.42842 -4.4284534 -4.4285073 -4.4285603 -4.4285965 -4.428606 -4.4286065 -4.4286137][-4.4284611 -4.42837 -4.4283271 -4.4283218 -4.4283285 -4.4283142 -4.4282832 -4.4282708 -4.428297 -4.4283562 -4.42842 -4.4284654 -4.4284844 -4.4285021 -4.428534][-4.428587 -4.42852 -4.4284863 -4.4284725 -4.4284606 -4.4284234 -4.4283667 -4.4283233 -4.4283156 -4.4283438 -4.4283834 -4.4284139 -4.4284291 -4.4284511 -4.4284945][-4.4286962 -4.428669 -4.4286647 -4.4286695 -4.4286718 -4.4286475 -4.4286036 -4.4285622 -4.4285388 -4.4285369 -4.428544 -4.4285464 -4.4285407 -4.4285436 -4.42857][-4.4287419 -4.4287405 -4.4287586 -4.4287868 -4.428813 -4.4288139 -4.4287968 -4.4287772 -4.428761 -4.428751 -4.4287429 -4.4287329 -4.4287119 -4.4286966 -4.4287][-4.4287252 -4.4287252 -4.42875 -4.4287934 -4.4288378 -4.4288597 -4.4288654 -4.4288678 -4.4288683 -4.4288659 -4.42886 -4.4288464 -4.4288144 -4.428781 -4.4287634][-4.4286456 -4.4286318 -4.4286528 -4.4287047 -4.4287634 -4.4288015 -4.428822 -4.4288363 -4.4288487 -4.4288588 -4.4288616 -4.4288507 -4.4288063 -4.4287548 -4.4287205][-4.4285135 -4.4284778 -4.4284897 -4.4285421 -4.428607 -4.4286556 -4.4286847 -4.4287028 -4.428721 -4.4287405 -4.4287529 -4.4287472 -4.4286952 -4.4286275 -4.4285812][-4.4283953 -4.4283442 -4.4283485 -4.4283986 -4.4284639 -4.428514 -4.4285417 -4.428555 -4.42857 -4.4285884 -4.4286027 -4.4286003 -4.42855 -4.4284782 -4.42843][-4.4284239 -4.4283762 -4.4283819 -4.42843 -4.4284887 -4.4285307 -4.4285502 -4.4285569 -4.428566 -4.42858 -4.4285913 -4.4285903 -4.4285502 -4.428494 -4.428453]]...]
INFO - root - 2017-12-08 10:35:05.484669: step 2610, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:44m:18s remains)
INFO - root - 2017-12-08 10:35:09.849671: step 2620, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.423 sec/batch; 38h:43m:06s remains)
INFO - root - 2017-12-08 10:35:14.276122: step 2630, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:24m:45s remains)
INFO - root - 2017-12-08 10:35:18.676553: step 2640, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:47m:51s remains)
INFO - root - 2017-12-08 10:35:23.172329: step 2650, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:22m:11s remains)
INFO - root - 2017-12-08 10:35:27.774315: step 2660, loss = 2.28, batch loss = 2.23 (17.0 examples/sec; 0.469 sec/batch; 43h:00m:57s remains)
INFO - root - 2017-12-08 10:35:32.230735: step 2670, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.467 sec/batch; 42h:45m:24s remains)
INFO - root - 2017-12-08 10:35:36.597270: step 2680, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:08m:06s remains)
INFO - root - 2017-12-08 10:35:40.921516: step 2690, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:16m:39s remains)
INFO - root - 2017-12-08 10:35:45.319627: step 2700, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.427 sec/batch; 39h:04m:48s remains)
2017-12-08 10:35:45.849536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287758 -4.4287767 -4.4287906 -4.4288177 -4.4288354 -4.42884 -4.42885 -4.428865 -4.4288745 -4.4288769 -4.4288721 -4.428864 -4.4288564 -4.42885 -4.4288454][-4.4287758 -4.4287782 -4.4287848 -4.4288077 -4.4288249 -4.4288311 -4.4288397 -4.4288535 -4.428864 -4.4288673 -4.4288635 -4.4288526 -4.4288406 -4.4288321 -4.4288244][-4.428823 -4.428823 -4.4288187 -4.4288254 -4.42883 -4.4288254 -4.4288239 -4.4288325 -4.428844 -4.4288492 -4.4288483 -4.4288421 -4.4288359 -4.4288297 -4.4288239][-4.4288626 -4.428865 -4.4288511 -4.4288425 -4.428834 -4.4288187 -4.428813 -4.4288225 -4.4288392 -4.42885 -4.4288545 -4.4288559 -4.4288545 -4.4288492 -4.42884][-4.428874 -4.4288735 -4.4288583 -4.42884 -4.4288158 -4.4287872 -4.4287744 -4.4287863 -4.4288106 -4.4288316 -4.4288468 -4.4288592 -4.4288664 -4.4288645 -4.4288511][-4.428833 -4.4288249 -4.4288058 -4.4287753 -4.4287324 -4.4286904 -4.4286723 -4.428689 -4.4287248 -4.4287629 -4.4287939 -4.4288211 -4.4288411 -4.4288454 -4.4288268][-4.4287333 -4.4287152 -4.4286866 -4.4286432 -4.4285884 -4.4285412 -4.4285207 -4.4285364 -4.4285846 -4.4286442 -4.4286952 -4.4287357 -4.4287691 -4.4287782 -4.4287553][-4.4286137 -4.4285803 -4.4285383 -4.4284854 -4.4284244 -4.4283729 -4.4283414 -4.4283438 -4.4283972 -4.4284806 -4.428556 -4.4286141 -4.4286571 -4.4286671 -4.4286461][-4.42857 -4.4285269 -4.4284825 -4.4284339 -4.4283814 -4.4283304 -4.4282866 -4.4282737 -4.4283252 -4.42842 -4.4285049 -4.428565 -4.4286013 -4.4286027 -4.4285784][-4.428627 -4.4285936 -4.4285669 -4.4285431 -4.4285178 -4.4284887 -4.4284582 -4.42845 -4.4284883 -4.4285574 -4.4286175 -4.4286556 -4.4286671 -4.4286513 -4.4286218][-4.428721 -4.4287033 -4.4286909 -4.4286814 -4.4286723 -4.428659 -4.4286423 -4.4286413 -4.4286709 -4.4287148 -4.4287534 -4.4287744 -4.4287734 -4.4287562 -4.4287324][-4.4287748 -4.4287691 -4.4287634 -4.4287562 -4.4287519 -4.4287448 -4.4287372 -4.428741 -4.4287615 -4.4287891 -4.4288125 -4.428822 -4.4288135 -4.4287982 -4.4287829][-4.4287663 -4.4287572 -4.4287481 -4.4287357 -4.4287305 -4.4287291 -4.428731 -4.4287353 -4.4287462 -4.4287615 -4.4287748 -4.428781 -4.4287739 -4.4287653 -4.42876][-4.4287472 -4.42873 -4.4287081 -4.4286828 -4.4286709 -4.42867 -4.4286728 -4.4286776 -4.428688 -4.4286976 -4.4287052 -4.4287171 -4.4287233 -4.4287295 -4.4287386][-4.4287515 -4.4287305 -4.428699 -4.428668 -4.4286509 -4.4286451 -4.4286418 -4.4286451 -4.4286542 -4.4286623 -4.4286737 -4.4286947 -4.4287143 -4.4287314 -4.4287505]]...]
INFO - root - 2017-12-08 10:35:50.390714: step 2710, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:15m:08s remains)
INFO - root - 2017-12-08 10:35:54.832808: step 2720, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:42m:58s remains)
INFO - root - 2017-12-08 10:35:59.338265: step 2730, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:52m:11s remains)
INFO - root - 2017-12-08 10:36:03.873302: step 2740, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.453 sec/batch; 41h:32m:22s remains)
INFO - root - 2017-12-08 10:36:08.425483: step 2750, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.456 sec/batch; 41h:47m:39s remains)
INFO - root - 2017-12-08 10:36:12.893989: step 2760, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:49m:22s remains)
INFO - root - 2017-12-08 10:36:17.399774: step 2770, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 41h:00m:30s remains)
INFO - root - 2017-12-08 10:36:21.737010: step 2780, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:26m:59s remains)
INFO - root - 2017-12-08 10:36:26.224455: step 2790, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.448 sec/batch; 41h:04m:28s remains)
INFO - root - 2017-12-08 10:36:30.738539: step 2800, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:52m:22s remains)
2017-12-08 10:36:31.222988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42863 -4.42863 -4.4286246 -4.4286323 -4.4286485 -4.428648 -4.4285951 -4.4285131 -4.4284725 -4.4285173 -4.4286265 -4.4287367 -4.4288211 -4.42888 -4.4289122][-4.4286113 -4.4285851 -4.4285588 -4.4285545 -4.4285712 -4.42858 -4.4285374 -4.4284625 -4.4284291 -4.4284821 -4.428597 -4.4287076 -4.428791 -4.4288516 -4.4288898][-4.4286485 -4.4286056 -4.4285622 -4.42853 -4.4285173 -4.4285178 -4.4284892 -4.4284286 -4.4284058 -4.4284697 -4.4285951 -4.4287109 -4.4287915 -4.4288464 -4.4288855][-4.4287038 -4.4286566 -4.4286 -4.4285388 -4.4284868 -4.4284573 -4.4284291 -4.4283767 -4.428359 -4.4284372 -4.4285789 -4.4287057 -4.4287868 -4.4288387 -4.4288826][-4.4287391 -4.4286919 -4.4286237 -4.428535 -4.4284382 -4.4283733 -4.4283385 -4.4282951 -4.4282823 -4.4283786 -4.4285407 -4.4286838 -4.42877 -4.4288254 -4.428875][-4.4287357 -4.4286914 -4.4286203 -4.4285126 -4.428371 -4.4282656 -4.42823 -4.428195 -4.4281869 -4.4282961 -4.4284697 -4.4286342 -4.4287386 -4.4288063 -4.4288573][-4.4287114 -4.4286733 -4.4286032 -4.4284821 -4.4283104 -4.4281797 -4.4281554 -4.4281325 -4.4281211 -4.4282269 -4.4284086 -4.4285965 -4.4287238 -4.4287996 -4.4288464][-4.4287152 -4.4286885 -4.428617 -4.4284983 -4.4283361 -4.4282207 -4.4282069 -4.4281793 -4.4281478 -4.4282279 -4.4283977 -4.42859 -4.4287267 -4.4288034 -4.428844][-4.4287338 -4.4287081 -4.4286413 -4.4285474 -4.4284487 -4.4283862 -4.4283714 -4.428319 -4.4282584 -4.4283 -4.4284358 -4.4286051 -4.4287267 -4.4287996 -4.42884][-4.4287443 -4.4287033 -4.4286323 -4.428566 -4.4285326 -4.4285221 -4.4285216 -4.4284558 -4.4283814 -4.4283972 -4.4284987 -4.4286332 -4.4287295 -4.428793 -4.4288368][-4.4287338 -4.4286804 -4.4286103 -4.4285712 -4.4285674 -4.4285755 -4.428587 -4.4285345 -4.4284711 -4.4284797 -4.4285569 -4.4286542 -4.4287262 -4.428781 -4.4288273][-4.4286981 -4.4286513 -4.4286027 -4.4285855 -4.4285874 -4.4285874 -4.4285975 -4.428565 -4.4285245 -4.4285417 -4.4286075 -4.4286833 -4.4287338 -4.4287758 -4.4288211][-4.4286294 -4.428586 -4.4285603 -4.4285641 -4.4285803 -4.4285851 -4.4286 -4.4285851 -4.4285674 -4.4285979 -4.428658 -4.4287186 -4.4287553 -4.4287853 -4.4288268][-4.4285583 -4.4285259 -4.4285 -4.4285 -4.428525 -4.4285502 -4.4285803 -4.4285827 -4.4285851 -4.4286265 -4.4286861 -4.4287438 -4.4287782 -4.4287992 -4.428833][-4.4285154 -4.4285083 -4.4284878 -4.4284768 -4.4284911 -4.4285254 -4.428555 -4.4285541 -4.4285588 -4.4286094 -4.428679 -4.4287453 -4.4287863 -4.4288092 -4.4288397]]...]
INFO - root - 2017-12-08 10:36:35.733242: step 2810, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:10m:25s remains)
INFO - root - 2017-12-08 10:36:40.343220: step 2820, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 41h:00m:09s remains)
INFO - root - 2017-12-08 10:36:44.901591: step 2830, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:28m:34s remains)
INFO - root - 2017-12-08 10:36:49.350595: step 2840, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.448 sec/batch; 41h:03m:23s remains)
INFO - root - 2017-12-08 10:36:53.829332: step 2850, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:34m:14s remains)
INFO - root - 2017-12-08 10:36:58.333351: step 2860, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.453 sec/batch; 41h:30m:50s remains)
INFO - root - 2017-12-08 10:37:02.768696: step 2870, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:34m:03s remains)
INFO - root - 2017-12-08 10:37:07.038124: step 2880, loss = 2.28, batch loss = 2.23 (20.2 examples/sec; 0.397 sec/batch; 36h:19m:31s remains)
INFO - root - 2017-12-08 10:37:11.438012: step 2890, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:33m:56s remains)
INFO - root - 2017-12-08 10:37:15.856287: step 2900, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:45m:29s remains)
2017-12-08 10:37:16.372354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287682 -4.4287839 -4.4288011 -4.4288111 -4.4288149 -4.4288111 -4.4287906 -4.4287634 -4.428741 -4.4287348 -4.4287529 -4.4287767 -4.4287839 -4.4287667 -4.4287381][-4.4287848 -4.4287972 -4.4288073 -4.428812 -4.4288096 -4.428802 -4.4287772 -4.4287457 -4.4287219 -4.4287214 -4.4287524 -4.428792 -4.4288073 -4.4287887 -4.4287543][-4.4287925 -4.4287939 -4.4287896 -4.4287834 -4.428781 -4.428782 -4.4287677 -4.4287372 -4.4287114 -4.4287176 -4.4287596 -4.4288096 -4.4288321 -4.4288168 -4.4287834][-4.4287953 -4.4287858 -4.4287672 -4.4287524 -4.4287548 -4.4287729 -4.4287753 -4.4287472 -4.4287119 -4.4287114 -4.4287529 -4.4288054 -4.4288335 -4.4288254 -4.4287987][-4.4287963 -4.4287834 -4.4287605 -4.4287448 -4.4287562 -4.4287858 -4.4287939 -4.4287658 -4.4287257 -4.4287143 -4.428741 -4.4287848 -4.4288144 -4.4288149 -4.4287949][-4.428803 -4.4287949 -4.4287786 -4.4287677 -4.428782 -4.4288058 -4.4288106 -4.4287844 -4.4287462 -4.4287267 -4.4287357 -4.428762 -4.4287834 -4.4287853 -4.42877][-4.4288116 -4.428813 -4.4288073 -4.4288015 -4.4288111 -4.428822 -4.4288168 -4.4287953 -4.4287648 -4.4287453 -4.4287443 -4.4287562 -4.4287648 -4.4287586 -4.4287424][-4.4288163 -4.4288306 -4.4288349 -4.4288311 -4.4288297 -4.4288259 -4.4288111 -4.4287915 -4.428771 -4.4287624 -4.4287663 -4.4287744 -4.4287734 -4.428762 -4.4287448][-4.4288058 -4.4288378 -4.4288554 -4.4288521 -4.4288378 -4.4288187 -4.4287939 -4.4287734 -4.4287629 -4.4287667 -4.4287782 -4.4287872 -4.4287858 -4.4287748 -4.4287634][-4.4287734 -4.4288306 -4.4288626 -4.4288573 -4.42883 -4.4287963 -4.4287663 -4.4287477 -4.4287438 -4.4287534 -4.4287653 -4.4287753 -4.4287791 -4.4287777 -4.4287806][-4.4287362 -4.4288173 -4.428863 -4.4288583 -4.42882 -4.4287744 -4.4287405 -4.4287262 -4.4287238 -4.4287262 -4.4287295 -4.4287367 -4.4287472 -4.4287577 -4.4287772][-4.4287257 -4.4288192 -4.4288697 -4.428864 -4.428822 -4.428772 -4.4287372 -4.4287252 -4.4287186 -4.4287057 -4.42869 -4.4286852 -4.4286914 -4.4287052 -4.4287372][-4.4287653 -4.4288464 -4.428885 -4.428874 -4.4288344 -4.4287877 -4.4287543 -4.4287395 -4.4287276 -4.4287047 -4.4286776 -4.4286575 -4.4286494 -4.4286571 -4.4286909][-4.4288239 -4.4288716 -4.4288883 -4.4288716 -4.428843 -4.4288087 -4.4287786 -4.4287586 -4.4287438 -4.4287238 -4.4286962 -4.4286618 -4.428637 -4.4286366 -4.4286628][-4.4288416 -4.4288583 -4.4288578 -4.428843 -4.428833 -4.4288211 -4.428802 -4.42878 -4.4287658 -4.4287553 -4.4287362 -4.4286966 -4.4286628 -4.4286561 -4.4286656]]...]
INFO - root - 2017-12-08 10:37:20.777620: step 2910, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:44m:39s remains)
INFO - root - 2017-12-08 10:37:25.233082: step 2920, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.464 sec/batch; 42h:27m:06s remains)
INFO - root - 2017-12-08 10:37:29.769660: step 2930, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:04m:15s remains)
INFO - root - 2017-12-08 10:37:34.295407: step 2940, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.468 sec/batch; 42h:52m:03s remains)
INFO - root - 2017-12-08 10:37:38.820363: step 2950, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:28m:22s remains)
INFO - root - 2017-12-08 10:37:43.322084: step 2960, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:13m:44s remains)
INFO - root - 2017-12-08 10:37:47.820401: step 2970, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 41h:10m:25s remains)
INFO - root - 2017-12-08 10:37:51.939438: step 2980, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:15m:36s remains)
INFO - root - 2017-12-08 10:37:56.479870: step 2990, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.466 sec/batch; 42h:38m:26s remains)
INFO - root - 2017-12-08 10:38:01.036574: step 3000, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:20m:58s remains)
2017-12-08 10:38:01.560330: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287248 -4.4287457 -4.4287839 -4.4288177 -4.4288187 -4.4287724 -4.4287076 -4.4286675 -4.42867 -4.4286966 -4.4287162 -4.4287305 -4.4287419 -4.4287286 -4.42873][-4.4286819 -4.4287152 -4.4287586 -4.4287963 -4.4287968 -4.4287443 -4.4286594 -4.4286079 -4.4286141 -4.4286461 -4.4286704 -4.4286895 -4.4287076 -4.4286933 -4.4286909][-4.4286504 -4.4286828 -4.4287281 -4.4287672 -4.4287715 -4.4287233 -4.4286361 -4.4285855 -4.4286008 -4.4286432 -4.4286728 -4.428689 -4.4287014 -4.4286876 -4.4286842][-4.4286284 -4.4286485 -4.4286919 -4.4287338 -4.4287486 -4.4287138 -4.4286494 -4.4286256 -4.4286613 -4.4287119 -4.4287438 -4.4287515 -4.4287481 -4.4287291 -4.4287248][-4.4286132 -4.4286265 -4.4286594 -4.4286966 -4.4287138 -4.4286895 -4.4286504 -4.4286571 -4.4287143 -4.4287729 -4.428813 -4.4288206 -4.4288173 -4.4288054 -4.4288087][-4.4285636 -4.4285617 -4.4285707 -4.4285955 -4.4286113 -4.4285855 -4.4285464 -4.4285574 -4.4286323 -4.4287152 -4.4287758 -4.4288068 -4.4288282 -4.428844 -4.4288669][-4.4284873 -4.428452 -4.4284186 -4.4284182 -4.42842 -4.4283743 -4.4283061 -4.4283004 -4.4284048 -4.4285388 -4.4286351 -4.4286942 -4.4287405 -4.428782 -4.428822][-4.4284396 -4.428349 -4.4282484 -4.4282026 -4.4281797 -4.4281006 -4.4279733 -4.4279337 -4.4280953 -4.4283071 -4.4284525 -4.428534 -4.428597 -4.4286513 -4.4287024][-4.4284687 -4.4283552 -4.4282217 -4.4281392 -4.42809 -4.4279842 -4.4278107 -4.4277425 -4.4279428 -4.4282 -4.4283652 -4.4284434 -4.4284997 -4.4285522 -4.4285941][-4.4285717 -4.4284859 -4.4283776 -4.4282985 -4.4282475 -4.4281645 -4.4280443 -4.4280052 -4.4281478 -4.4283433 -4.4284735 -4.4285164 -4.4285283 -4.4285474 -4.4285593][-4.4286866 -4.428637 -4.42857 -4.4285121 -4.4284778 -4.42843 -4.4283671 -4.4283557 -4.4284382 -4.4285479 -4.4286108 -4.428607 -4.4285817 -4.4285669 -4.4285522][-4.428761 -4.4287348 -4.4287009 -4.42867 -4.4286547 -4.4286327 -4.4286008 -4.4285908 -4.4286208 -4.4286547 -4.4286489 -4.4285965 -4.428535 -4.4284873 -4.4284463][-4.4287925 -4.4287729 -4.4287553 -4.428741 -4.4287348 -4.4287262 -4.4287081 -4.4286919 -4.4286857 -4.428669 -4.4286118 -4.4285183 -4.428421 -4.4283428 -4.4282832][-4.4288106 -4.4287872 -4.4287705 -4.4287591 -4.4287548 -4.4287486 -4.4287333 -4.4287124 -4.4286876 -4.4286437 -4.4285588 -4.4284492 -4.4283471 -4.4282641 -4.4281988][-4.4288416 -4.4288211 -4.4288044 -4.428793 -4.4287868 -4.4287786 -4.4287639 -4.4287434 -4.4287128 -4.428658 -4.4285727 -4.4284706 -4.4283752 -4.4283009 -4.4282393]]...]
INFO - root - 2017-12-08 10:38:06.181098: step 3010, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.461 sec/batch; 42h:12m:52s remains)
INFO - root - 2017-12-08 10:38:10.724923: step 3020, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.438 sec/batch; 40h:07m:40s remains)
INFO - root - 2017-12-08 10:38:15.209379: step 3030, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:17m:27s remains)
INFO - root - 2017-12-08 10:38:19.741482: step 3040, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:23m:02s remains)
INFO - root - 2017-12-08 10:38:24.203521: step 3050, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:20m:41s remains)
INFO - root - 2017-12-08 10:38:28.660979: step 3060, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:08m:14s remains)
INFO - root - 2017-12-08 10:38:33.196716: step 3070, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:20m:52s remains)
INFO - root - 2017-12-08 10:38:37.475500: step 3080, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:16m:31s remains)
INFO - root - 2017-12-08 10:38:41.847506: step 3090, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.420 sec/batch; 38h:26m:36s remains)
INFO - root - 2017-12-08 10:38:46.224502: step 3100, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:24m:12s remains)
2017-12-08 10:38:46.735558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289865 -4.4289813 -4.4289846 -4.4289894 -4.4289942 -4.429 -4.4290037 -4.4290037 -4.428997 -4.4289846 -4.4289708 -4.4289594 -4.4289536 -4.4289503 -4.4289393][-4.428998 -4.428987 -4.4289856 -4.4289851 -4.4289856 -4.4289885 -4.4289908 -4.4289889 -4.4289813 -4.4289718 -4.4289584 -4.4289441 -4.4289393 -4.4289322 -4.4289141][-4.4289842 -4.4289756 -4.428968 -4.4289618 -4.4289579 -4.428957 -4.4289517 -4.4289455 -4.4289393 -4.4289351 -4.4289241 -4.4289079 -4.4289012 -4.4288878 -4.4288578][-4.42894 -4.4289341 -4.4289222 -4.4289117 -4.4289021 -4.4288917 -4.4288745 -4.4288621 -4.4288626 -4.42887 -4.4288597 -4.4288306 -4.428812 -4.4287825 -4.4287362][-4.4288669 -4.4288635 -4.4288468 -4.4288311 -4.4288139 -4.4287877 -4.4287529 -4.4287262 -4.4287319 -4.4287581 -4.4287481 -4.4286985 -4.4286613 -4.4286189 -4.428565][-4.4287825 -4.4287772 -4.4287548 -4.4287286 -4.4286985 -4.4286532 -4.4285874 -4.4285316 -4.4285488 -4.428606 -4.4286032 -4.4285398 -4.4284954 -4.428463 -4.4284205][-4.4287086 -4.4286933 -4.4286561 -4.4286079 -4.4285569 -4.428484 -4.4283705 -4.4282718 -4.4283175 -4.4284248 -4.4284468 -4.4283967 -4.4283748 -4.4283867 -4.4283886][-4.4286761 -4.4286404 -4.4285784 -4.4285016 -4.4284291 -4.428328 -4.4281588 -4.4280119 -4.4280977 -4.4282651 -4.428329 -4.42832 -4.4283442 -4.4284153 -4.4284716][-4.4287 -4.4286489 -4.4285717 -4.4284759 -4.4283862 -4.4282775 -4.4281015 -4.4279485 -4.4280405 -4.4282193 -4.4283094 -4.4283376 -4.4283962 -4.4284983 -4.4285874][-4.4287553 -4.4287066 -4.4286337 -4.42854 -4.4284506 -4.4283514 -4.4282188 -4.4281139 -4.4281731 -4.4283028 -4.4283853 -4.428431 -4.4284983 -4.4286008 -4.4286957][-4.42882 -4.4287844 -4.4287305 -4.4286571 -4.4285779 -4.4284964 -4.428411 -4.4283547 -4.4283795 -4.4284511 -4.4285131 -4.4285579 -4.4286146 -4.4286985 -4.4287786][-4.4288616 -4.4288397 -4.4288077 -4.4287605 -4.4287071 -4.4286509 -4.4286 -4.4285722 -4.4285817 -4.4286146 -4.4286523 -4.4286842 -4.428719 -4.4287744 -4.4288311][-4.4288735 -4.4288626 -4.4288492 -4.4288263 -4.4287996 -4.428772 -4.4287472 -4.4287338 -4.428741 -4.4287553 -4.4287715 -4.4287853 -4.4287963 -4.4288235 -4.4288568][-4.4288735 -4.4288669 -4.4288645 -4.4288592 -4.4288511 -4.4288416 -4.4288354 -4.4288335 -4.4288421 -4.4288492 -4.4288554 -4.4288568 -4.428853 -4.4288611 -4.4288769][-4.4288721 -4.4288683 -4.4288712 -4.428874 -4.4288759 -4.4288783 -4.4288816 -4.4288864 -4.428895 -4.4288993 -4.4289021 -4.4288988 -4.4288898 -4.4288912 -4.4288969]]...]
INFO - root - 2017-12-08 10:38:51.191735: step 3110, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:12m:51s remains)
INFO - root - 2017-12-08 10:38:55.686719: step 3120, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:30m:16s remains)
INFO - root - 2017-12-08 10:39:00.115569: step 3130, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:34m:56s remains)
INFO - root - 2017-12-08 10:39:04.477783: step 3140, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:41m:18s remains)
INFO - root - 2017-12-08 10:39:08.837639: step 3150, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.436 sec/batch; 39h:55m:44s remains)
INFO - root - 2017-12-08 10:39:13.184956: step 3160, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.423 sec/batch; 38h:43m:57s remains)
INFO - root - 2017-12-08 10:39:17.578157: step 3170, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:17m:54s remains)
INFO - root - 2017-12-08 10:39:21.842604: step 3180, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:54m:13s remains)
INFO - root - 2017-12-08 10:39:26.315435: step 3190, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.456 sec/batch; 41h:40m:53s remains)
INFO - root - 2017-12-08 10:39:30.843342: step 3200, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:13m:47s remains)
2017-12-08 10:39:31.367150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289327 -4.4289279 -4.4289188 -4.4289093 -4.4289165 -4.4289341 -4.428937 -4.4289312 -4.42893 -4.4289355 -4.4289494 -4.4289627 -4.4289665 -4.4289641 -4.4289527][-4.4289017 -4.4288878 -4.4288688 -4.428854 -4.4288611 -4.4288869 -4.4288921 -4.4288807 -4.4288769 -4.4288845 -4.428906 -4.42893 -4.4289422 -4.4289432 -4.4289293][-4.4288688 -4.4288411 -4.4288111 -4.4287891 -4.428793 -4.4288192 -4.4288182 -4.4287977 -4.4287925 -4.4288044 -4.4288363 -4.4288726 -4.4288988 -4.4289093 -4.4289012][-4.4288254 -4.4287853 -4.4287505 -4.4287267 -4.4287295 -4.42875 -4.4287376 -4.4287 -4.4286857 -4.4287014 -4.4287467 -4.4287958 -4.4288373 -4.428864 -4.4288692][-4.4287639 -4.4287138 -4.4286752 -4.4286456 -4.42864 -4.4286494 -4.4286237 -4.4285674 -4.4285469 -4.4285674 -4.4286194 -4.4286752 -4.4287357 -4.42879 -4.428823][-4.4286904 -4.4286308 -4.4285779 -4.42853 -4.4285097 -4.4285059 -4.4284682 -4.4283938 -4.4283686 -4.4283805 -4.428421 -4.4284816 -4.4285741 -4.4286766 -4.4287519][-4.4286122 -4.4285364 -4.4284539 -4.4283786 -4.4283414 -4.4283242 -4.4282746 -4.4281859 -4.4281454 -4.4281344 -4.4281549 -4.4282246 -4.4283638 -4.4285278 -4.4286561][-4.42852 -4.4284234 -4.428318 -4.428225 -4.4281793 -4.4281535 -4.4280972 -4.427999 -4.4279404 -4.4279094 -4.4279165 -4.4280024 -4.4281826 -4.4283929 -4.4285641][-4.4284534 -4.4283524 -4.42825 -4.4281554 -4.428103 -4.428071 -4.4280081 -4.4279137 -4.4278584 -4.4278326 -4.4278555 -4.4279523 -4.4281359 -4.4283462 -4.4285259][-4.4284472 -4.4283586 -4.4282727 -4.4281955 -4.42815 -4.4281249 -4.4280739 -4.4280057 -4.4279704 -4.42796 -4.4279914 -4.4280777 -4.4282289 -4.4284058 -4.4285631][-4.4285069 -4.4284329 -4.42837 -4.4283209 -4.428298 -4.4282908 -4.4282722 -4.4282436 -4.4282312 -4.4282322 -4.4282584 -4.42832 -4.4284239 -4.4285522 -4.42867][-4.4286261 -4.4285741 -4.4285364 -4.428515 -4.4285126 -4.4285183 -4.4285212 -4.428515 -4.4285121 -4.4285154 -4.428534 -4.4285727 -4.4286346 -4.4287143 -4.4287844][-4.4287438 -4.4287105 -4.4286909 -4.4286838 -4.428689 -4.4287 -4.4287109 -4.4287124 -4.4287124 -4.4287148 -4.4287291 -4.4287543 -4.42879 -4.4288335 -4.4288664][-4.428822 -4.428802 -4.4287925 -4.4287915 -4.4287987 -4.42881 -4.4288206 -4.4288239 -4.4288244 -4.4288268 -4.4288383 -4.4288554 -4.428875 -4.4288955 -4.4289083][-4.4288816 -4.4288697 -4.428864 -4.4288645 -4.4288692 -4.4288769 -4.4288831 -4.4288859 -4.4288874 -4.4288878 -4.4288926 -4.4289021 -4.4289131 -4.4289231 -4.42893]]...]
INFO - root - 2017-12-08 10:39:35.942992: step 3210, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:52m:04s remains)
INFO - root - 2017-12-08 10:39:40.448866: step 3220, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:50m:17s remains)
INFO - root - 2017-12-08 10:39:44.955631: step 3230, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:17m:22s remains)
INFO - root - 2017-12-08 10:39:49.479103: step 3240, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.461 sec/batch; 42h:08m:09s remains)
INFO - root - 2017-12-08 10:39:53.904246: step 3250, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.431 sec/batch; 39h:22m:33s remains)
INFO - root - 2017-12-08 10:39:58.424541: step 3260, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:40m:26s remains)
INFO - root - 2017-12-08 10:40:02.978213: step 3270, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.463 sec/batch; 42h:20m:55s remains)
INFO - root - 2017-12-08 10:40:07.204410: step 3280, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.469 sec/batch; 42h:51m:07s remains)
INFO - root - 2017-12-08 10:40:11.828792: step 3290, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.453 sec/batch; 41h:27m:05s remains)
INFO - root - 2017-12-08 10:40:16.334854: step 3300, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 40h:04m:03s remains)
2017-12-08 10:40:16.809692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287853 -4.4288082 -4.4288497 -4.42888 -4.4288926 -4.4288888 -4.4288731 -4.428844 -4.42881 -4.4287839 -4.4287596 -4.4287305 -4.42872 -4.4287496 -4.4287882][-4.4287672 -4.4288063 -4.4288607 -4.4289002 -4.4289174 -4.4289207 -4.4289155 -4.4289036 -4.4288917 -4.428885 -4.4288764 -4.4288578 -4.4288411 -4.4288397 -4.4288373][-4.4287395 -4.4287877 -4.4288483 -4.42889 -4.4289083 -4.4289193 -4.4289279 -4.4289341 -4.4289403 -4.4289479 -4.4289517 -4.4289427 -4.4289207 -4.4288936 -4.4288564][-4.4287386 -4.4287815 -4.4288287 -4.428854 -4.42886 -4.4288692 -4.4288812 -4.428895 -4.4289117 -4.4289322 -4.4289522 -4.4289589 -4.4289455 -4.4289169 -4.4288697][-4.4287796 -4.4287949 -4.4288073 -4.4287987 -4.4287791 -4.4287677 -4.4287624 -4.4287677 -4.4287882 -4.4288259 -4.4288697 -4.4289064 -4.4289241 -4.4289193 -4.4288888][-4.4288125 -4.4287992 -4.4287767 -4.4287353 -4.4286861 -4.4286427 -4.4286013 -4.4285793 -4.4285994 -4.4286604 -4.42874 -4.42882 -4.4288821 -4.4289093 -4.4289002][-4.4288325 -4.4288 -4.4287534 -4.4286871 -4.4286089 -4.4285259 -4.4284363 -4.4283772 -4.4283996 -4.4284911 -4.4286113 -4.4287314 -4.4288282 -4.4288745 -4.4288731][-4.4288516 -4.4288168 -4.4287577 -4.4286737 -4.4285693 -4.428443 -4.4283 -4.4282036 -4.4282341 -4.4283609 -4.4285188 -4.4286666 -4.4287796 -4.4288287 -4.4288278][-4.4288788 -4.4288507 -4.428793 -4.4287033 -4.4285841 -4.4284296 -4.4282537 -4.42814 -4.428184 -4.4283323 -4.4285064 -4.4286594 -4.4287658 -4.4288077 -4.4288054][-4.4289112 -4.42889 -4.4288378 -4.4287529 -4.4286413 -4.4284992 -4.4283452 -4.42826 -4.4283123 -4.4284439 -4.4285922 -4.4287157 -4.4287992 -4.428833 -4.4288335][-4.428946 -4.4289227 -4.428875 -4.428803 -4.4287148 -4.4286127 -4.428515 -4.4284782 -4.4285316 -4.4286265 -4.4287229 -4.4287977 -4.4288492 -4.4288764 -4.4288812][-4.42895 -4.4289246 -4.428884 -4.4288306 -4.4287739 -4.42872 -4.4286804 -4.4286847 -4.42873 -4.4287815 -4.4288244 -4.4288521 -4.4288745 -4.4288974 -4.4289107][-4.4289279 -4.4288974 -4.4288621 -4.4288278 -4.4288039 -4.428791 -4.428792 -4.4288168 -4.4288478 -4.428863 -4.4288626 -4.428853 -4.428854 -4.4288759 -4.4289007][-4.4288898 -4.4288535 -4.42882 -4.4288006 -4.428803 -4.4288192 -4.4288449 -4.4288764 -4.4288945 -4.4288864 -4.4288554 -4.4288182 -4.4288015 -4.428823 -4.4288597][-4.4288611 -4.428823 -4.428792 -4.4287839 -4.4288025 -4.4288321 -4.4288664 -4.428896 -4.4289064 -4.4288883 -4.4288449 -4.428792 -4.4287615 -4.4287753 -4.4288144]]...]
INFO - root - 2017-12-08 10:40:21.248389: step 3310, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:43m:41s remains)
INFO - root - 2017-12-08 10:40:25.789445: step 3320, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:07m:49s remains)
INFO - root - 2017-12-08 10:40:30.299629: step 3330, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:33m:27s remains)
INFO - root - 2017-12-08 10:40:34.758568: step 3340, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:44m:10s remains)
INFO - root - 2017-12-08 10:40:39.221806: step 3350, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:48m:08s remains)
INFO - root - 2017-12-08 10:40:43.557677: step 3360, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 39h:07m:32s remains)
INFO - root - 2017-12-08 10:40:47.949575: step 3370, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 39h:34m:03s remains)
INFO - root - 2017-12-08 10:40:52.212423: step 3380, loss = 2.28, batch loss = 2.23 (16.9 examples/sec; 0.473 sec/batch; 43h:16m:31s remains)
INFO - root - 2017-12-08 10:40:56.762230: step 3390, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:39m:20s remains)
INFO - root - 2017-12-08 10:41:01.207090: step 3400, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:32m:20s remains)
2017-12-08 10:41:01.689655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428628 -4.4286566 -4.4286995 -4.4287338 -4.4287462 -4.4287457 -4.4287467 -4.428761 -4.4287896 -4.428813 -4.4288335 -4.4288464 -4.4288392 -4.4288344 -4.4288378][-4.4285011 -4.4285541 -4.4286246 -4.4286795 -4.4287024 -4.4287043 -4.4287057 -4.4287181 -4.4287524 -4.4287853 -4.4288087 -4.4288211 -4.4288 -4.4287739 -4.4287577][-4.4284544 -4.4285369 -4.42863 -4.4287004 -4.4287324 -4.4287324 -4.4287271 -4.42873 -4.428751 -4.4287772 -4.4288011 -4.4288111 -4.4287748 -4.4287224 -4.4286847][-4.4284997 -4.4285965 -4.4286938 -4.4287581 -4.4287829 -4.4287724 -4.4287529 -4.42874 -4.4287515 -4.42878 -4.4288096 -4.4288235 -4.4287825 -4.4287105 -4.4286566][-4.4285927 -4.428679 -4.4287605 -4.4288054 -4.4288144 -4.4287896 -4.4287534 -4.4287233 -4.4287281 -4.4287653 -4.4288049 -4.4288259 -4.4287896 -4.4287186 -4.4286647][-4.4286547 -4.4287157 -4.4287772 -4.4288054 -4.4288011 -4.4287648 -4.4287009 -4.428647 -4.4286442 -4.4286928 -4.4287419 -4.4287677 -4.4287529 -4.4287124 -4.4286814][-4.4286642 -4.4286981 -4.4287319 -4.4287438 -4.4287276 -4.4286714 -4.4285665 -4.4284687 -4.428462 -4.428546 -4.4286342 -4.4286923 -4.4287148 -4.4287181 -4.4287157][-4.4286628 -4.4286637 -4.4286642 -4.4286585 -4.4286237 -4.4285336 -4.4283733 -4.4282317 -4.42825 -4.4284072 -4.4285626 -4.4286675 -4.4287252 -4.4287567 -4.428761][-4.4286718 -4.4286551 -4.4286361 -4.4286213 -4.4285765 -4.4284644 -4.4282913 -4.4281664 -4.4282246 -4.4284158 -4.428587 -4.4286976 -4.4287634 -4.4288011 -4.4287992][-4.428678 -4.4286613 -4.4286513 -4.428647 -4.4286108 -4.4285254 -4.4284134 -4.4283586 -4.4284134 -4.4285483 -4.4286647 -4.4287357 -4.4287796 -4.428803 -4.4287977][-4.4286947 -4.4286666 -4.4286613 -4.4286718 -4.4286561 -4.4286122 -4.428566 -4.42856 -4.4285936 -4.428669 -4.4287276 -4.4287558 -4.4287696 -4.42877 -4.4287605][-4.4287043 -4.4286666 -4.4286642 -4.4286795 -4.428678 -4.4286647 -4.4286575 -4.4286742 -4.4287043 -4.4287472 -4.4287648 -4.4287529 -4.4287324 -4.4287095 -4.4286995][-4.4287033 -4.4286528 -4.4286456 -4.4286633 -4.4286709 -4.4286776 -4.4286876 -4.4287167 -4.4287496 -4.4287763 -4.4287724 -4.4287329 -4.4286823 -4.4286389 -4.4286232][-4.4286919 -4.4286351 -4.4286256 -4.428647 -4.4286642 -4.4286733 -4.4286809 -4.4287062 -4.4287372 -4.4287581 -4.4287543 -4.428709 -4.4286442 -4.4285879 -4.4285746][-4.4286947 -4.4286437 -4.4286289 -4.4286451 -4.4286556 -4.4286561 -4.4286523 -4.4286609 -4.4286838 -4.4287119 -4.4287295 -4.4287019 -4.4286423 -4.4285903 -4.4285736]]...]
INFO - root - 2017-12-08 10:41:06.316327: step 3410, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:47m:30s remains)
INFO - root - 2017-12-08 10:41:10.805676: step 3420, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:08m:06s remains)
INFO - root - 2017-12-08 10:41:15.237950: step 3430, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:38m:08s remains)
INFO - root - 2017-12-08 10:41:19.702838: step 3440, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:08m:41s remains)
INFO - root - 2017-12-08 10:41:24.196925: step 3450, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:00m:34s remains)
INFO - root - 2017-12-08 10:41:28.663548: step 3460, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:47m:16s remains)
INFO - root - 2017-12-08 10:41:33.220903: step 3470, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.461 sec/batch; 42h:08m:26s remains)
INFO - root - 2017-12-08 10:41:37.366183: step 3480, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:20m:04s remains)
INFO - root - 2017-12-08 10:41:41.741425: step 3490, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:18m:37s remains)
INFO - root - 2017-12-08 10:41:46.129573: step 3500, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:53m:30s remains)
2017-12-08 10:41:46.668419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286485 -4.4286356 -4.4286318 -4.4286532 -4.4286823 -4.4286962 -4.4286966 -4.4287066 -4.4287205 -4.4287176 -4.42872 -4.42871 -4.4286895 -4.4286737 -4.42865][-4.4285579 -4.4285355 -4.4285345 -4.4285522 -4.4285874 -4.4286089 -4.428607 -4.42861 -4.428627 -4.4286366 -4.428648 -4.4286427 -4.4286213 -4.4285984 -4.428556][-4.4284539 -4.4284306 -4.428442 -4.4284697 -4.4285049 -4.4285321 -4.4285254 -4.4285216 -4.4285398 -4.4285603 -4.4285793 -4.4285779 -4.4285574 -4.4285221 -4.4284673][-4.4284177 -4.428391 -4.4284105 -4.4284463 -4.4284835 -4.4285111 -4.4284945 -4.4284792 -4.4284997 -4.4285288 -4.4285555 -4.4285612 -4.4285359 -4.4284883 -4.4284277][-4.428473 -4.428431 -4.428442 -4.4284711 -4.4284992 -4.4285207 -4.4284935 -4.428473 -4.4285016 -4.4285464 -4.4285908 -4.4286122 -4.4285913 -4.4285388 -4.4284716][-4.4285712 -4.4285178 -4.4285049 -4.4285107 -4.4285188 -4.4285192 -4.4284806 -4.4284639 -4.4285092 -4.4285722 -4.4286332 -4.4286733 -4.4286637 -4.4286132 -4.4285336][-4.42863 -4.4285717 -4.428544 -4.4285374 -4.4285316 -4.4285145 -4.4284658 -4.4284515 -4.4285035 -4.4285736 -4.4286385 -4.4286804 -4.4286823 -4.4286451 -4.4285712][-4.4286609 -4.4286022 -4.4285755 -4.4285707 -4.4285684 -4.42855 -4.4285064 -4.428493 -4.4285316 -4.4285874 -4.4286385 -4.4286709 -4.4286747 -4.4286451 -4.4285841][-4.4286857 -4.4286356 -4.4286222 -4.4286356 -4.4286513 -4.428648 -4.4286242 -4.4286222 -4.4286475 -4.4286757 -4.4286962 -4.4287128 -4.4287081 -4.4286785 -4.4286289][-4.4287114 -4.4286766 -4.4286795 -4.4287047 -4.4287343 -4.4287453 -4.4287405 -4.428751 -4.4287658 -4.4287715 -4.4287705 -4.4287696 -4.4287553 -4.4287286 -4.4287004][-4.42873 -4.4287071 -4.4287205 -4.4287534 -4.428791 -4.4288177 -4.4288292 -4.4288435 -4.4288507 -4.4288459 -4.4288349 -4.4288254 -4.4288082 -4.4287848 -4.4287691][-4.4287457 -4.4287305 -4.4287467 -4.4287853 -4.4288321 -4.4288692 -4.42889 -4.4289064 -4.4289112 -4.428905 -4.4288955 -4.428885 -4.4288664 -4.4288449 -4.4288321][-4.4287462 -4.4287348 -4.4287534 -4.4287925 -4.4288363 -4.4288678 -4.4288864 -4.4289012 -4.4289069 -4.4289064 -4.4289012 -4.4288936 -4.4288836 -4.4288712 -4.4288611][-4.4287229 -4.42871 -4.4287267 -4.4287639 -4.4288063 -4.4288335 -4.4288483 -4.4288592 -4.4288669 -4.4288745 -4.4288745 -4.42887 -4.4288659 -4.4288635 -4.4288588][-4.4286976 -4.4286838 -4.4286966 -4.42873 -4.4287715 -4.4288015 -4.4288173 -4.4288259 -4.4288316 -4.4288421 -4.4288473 -4.4288483 -4.42885 -4.4288545 -4.4288535]]...]
INFO - root - 2017-12-08 10:41:51.165428: step 3510, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.468 sec/batch; 42h:46m:50s remains)
INFO - root - 2017-12-08 10:41:55.609450: step 3520, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.429 sec/batch; 39h:11m:38s remains)
INFO - root - 2017-12-08 10:42:00.007923: step 3530, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:21m:49s remains)
INFO - root - 2017-12-08 10:42:04.444362: step 3540, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:13m:31s remains)
INFO - root - 2017-12-08 10:42:08.852142: step 3550, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:14m:06s remains)
INFO - root - 2017-12-08 10:42:13.400151: step 3560, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.461 sec/batch; 42h:08m:25s remains)
INFO - root - 2017-12-08 10:42:17.844968: step 3570, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:35m:49s remains)
INFO - root - 2017-12-08 10:42:21.991136: step 3580, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 39h:31m:20s remains)
INFO - root - 2017-12-08 10:42:26.389695: step 3590, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.443 sec/batch; 40h:30m:00s remains)
INFO - root - 2017-12-08 10:42:30.762718: step 3600, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 40h:01m:46s remains)
2017-12-08 10:42:31.274431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428812 -4.4287815 -4.4287424 -4.4286933 -4.4286413 -4.4286137 -4.4286184 -4.4286427 -4.4286628 -4.4286861 -4.4287181 -4.4287453 -4.4287271 -4.4286695 -4.4286051][-4.428781 -4.42877 -4.4287577 -4.4287243 -4.4286652 -4.4286137 -4.4285955 -4.4286156 -4.4286475 -4.4286857 -4.42873 -4.428761 -4.4287429 -4.4286847 -4.4286165][-4.4287739 -4.428781 -4.4287839 -4.4287629 -4.4287038 -4.4286337 -4.4285936 -4.4286027 -4.4286461 -4.4287009 -4.4287586 -4.4288011 -4.4287877 -4.4287353 -4.4286733][-4.428762 -4.4287791 -4.428791 -4.42878 -4.4287276 -4.4286518 -4.4285989 -4.428596 -4.4286423 -4.4287105 -4.4287815 -4.4288349 -4.4288311 -4.4287868 -4.4287314][-4.4287286 -4.4287515 -4.4287686 -4.4287682 -4.4287276 -4.4286532 -4.428596 -4.4285865 -4.4286289 -4.4287028 -4.4287815 -4.4288435 -4.428854 -4.4288259 -4.428782][-4.4286737 -4.4286966 -4.428721 -4.4287343 -4.428709 -4.428648 -4.4285984 -4.4285913 -4.4286251 -4.4286914 -4.4287653 -4.4288254 -4.4288478 -4.4288392 -4.4288111][-4.4286132 -4.4286294 -4.4286604 -4.4286904 -4.4286847 -4.4286447 -4.4286084 -4.4286051 -4.42863 -4.4286804 -4.4287372 -4.4287853 -4.4288139 -4.4288287 -4.42882][-4.4285793 -4.4285774 -4.4286118 -4.4286575 -4.4286747 -4.4286566 -4.4286346 -4.4286318 -4.4286432 -4.4286704 -4.4287009 -4.4287319 -4.4287639 -4.4287996 -4.4288135][-4.4286013 -4.4285803 -4.4286036 -4.4286513 -4.4286866 -4.4286914 -4.4286857 -4.4286833 -4.4286838 -4.4286838 -4.4286842 -4.4286952 -4.4287243 -4.42877 -4.4288034][-4.4286633 -4.4286313 -4.4286346 -4.4286704 -4.4287086 -4.4287276 -4.4287329 -4.4287338 -4.4287286 -4.4287109 -4.4286914 -4.4286914 -4.4287138 -4.4287529 -4.428791][-4.4287267 -4.428689 -4.4286761 -4.4286962 -4.4287286 -4.42875 -4.4287586 -4.428762 -4.4287534 -4.4287276 -4.4287047 -4.4287057 -4.4287205 -4.4287462 -4.4287777][-4.4287705 -4.428731 -4.4287076 -4.4287133 -4.4287348 -4.4287524 -4.4287572 -4.4287567 -4.4287457 -4.4287243 -4.4287109 -4.4287229 -4.4287381 -4.4287529 -4.4287763][-4.4287896 -4.4287505 -4.4287233 -4.4287205 -4.4287348 -4.4287448 -4.4287415 -4.4287333 -4.42872 -4.428709 -4.428709 -4.4287314 -4.4287505 -4.4287615 -4.4287758][-4.428791 -4.428761 -4.4287424 -4.4287415 -4.4287519 -4.4287553 -4.4287419 -4.4287243 -4.4287071 -4.4286985 -4.4287033 -4.4287291 -4.4287519 -4.4287634 -4.4287715][-4.428791 -4.4287729 -4.4287653 -4.4287696 -4.428781 -4.4287853 -4.4287744 -4.4287553 -4.4287372 -4.4287257 -4.4287267 -4.4287477 -4.4287667 -4.4287744 -4.4287772]]...]
INFO - root - 2017-12-08 10:42:35.810299: step 3610, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:23m:00s remains)
INFO - root - 2017-12-08 10:42:40.367327: step 3620, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.459 sec/batch; 41h:58m:34s remains)
INFO - root - 2017-12-08 10:42:44.726003: step 3630, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.456 sec/batch; 41h:38m:54s remains)
INFO - root - 2017-12-08 10:42:49.177997: step 3640, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.436 sec/batch; 39h:49m:48s remains)
INFO - root - 2017-12-08 10:42:53.603818: step 3650, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:45m:33s remains)
INFO - root - 2017-12-08 10:42:58.144904: step 3660, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 41h:05m:37s remains)
INFO - root - 2017-12-08 10:43:02.593518: step 3670, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:23m:57s remains)
INFO - root - 2017-12-08 10:43:06.713889: step 3680, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.429 sec/batch; 39h:10m:50s remains)
INFO - root - 2017-12-08 10:43:11.115082: step 3690, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:26m:13s remains)
INFO - root - 2017-12-08 10:43:15.433012: step 3700, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:07m:44s remains)
2017-12-08 10:43:15.993549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288292 -4.428822 -4.4288034 -4.4287648 -4.4287276 -4.42871 -4.4287095 -4.4287019 -4.4286923 -4.4286842 -4.4286795 -4.4286804 -4.4286847 -4.4286971 -4.4287157][-4.4287715 -4.4287705 -4.4287605 -4.4287238 -4.4286866 -4.4286695 -4.4286742 -4.4286685 -4.4286451 -4.4286218 -4.4286127 -4.428618 -4.4286318 -4.4286542 -4.4286828][-4.4287019 -4.4287043 -4.4286981 -4.4286604 -4.4286237 -4.4286146 -4.42863 -4.4286275 -4.4285965 -4.428565 -4.4285593 -4.4285841 -4.4286194 -4.4286547 -4.4286852][-4.4286304 -4.4286256 -4.4286189 -4.42858 -4.4285507 -4.4285541 -4.42858 -4.4285884 -4.428565 -4.4285417 -4.4285488 -4.4285955 -4.428647 -4.4286819 -4.4286985][-4.4285579 -4.4285522 -4.4285421 -4.4285049 -4.4284844 -4.4284959 -4.4285245 -4.4285407 -4.428534 -4.428534 -4.4285626 -4.4286237 -4.4286776 -4.4286962 -4.4286809][-4.4284897 -4.4284863 -4.4284735 -4.4284353 -4.4284115 -4.4284167 -4.4284344 -4.4284391 -4.428441 -4.4284687 -4.4285178 -4.428586 -4.4286404 -4.4286494 -4.4286151][-4.4284058 -4.4283929 -4.4283648 -4.428319 -4.4282861 -4.4282794 -4.4282722 -4.42825 -4.4282608 -4.4283261 -4.4284067 -4.4284911 -4.4285479 -4.4285569 -4.4285293][-4.4282751 -4.4282317 -4.4281664 -4.42809 -4.4280396 -4.4280233 -4.4279923 -4.4279485 -4.4279838 -4.4280972 -4.4282179 -4.4283237 -4.4283919 -4.4284172 -4.4284196][-4.4282227 -4.4281368 -4.42803 -4.4279165 -4.4278393 -4.4278135 -4.427783 -4.4277477 -4.4278088 -4.4279428 -4.4280715 -4.4281759 -4.4282441 -4.4282856 -4.4283152][-4.4283662 -4.428287 -4.4281969 -4.4281015 -4.4280405 -4.4280248 -4.4280152 -4.4280028 -4.4280391 -4.4281116 -4.428185 -4.4282446 -4.4282808 -4.4283047 -4.4283271][-4.4285626 -4.4285116 -4.4284558 -4.4283962 -4.4283595 -4.4283524 -4.4283547 -4.4283528 -4.4283619 -4.4283829 -4.4284077 -4.4284272 -4.4284339 -4.4284368 -4.428442][-4.4287076 -4.4286757 -4.4286427 -4.4286056 -4.4285784 -4.42857 -4.4285727 -4.4285736 -4.4285722 -4.4285712 -4.4285722 -4.4285727 -4.4285679 -4.4285636 -4.4285636][-4.4287963 -4.4287734 -4.4287505 -4.4287238 -4.4287004 -4.4286895 -4.4286885 -4.4286885 -4.4286857 -4.4286814 -4.4286785 -4.4286757 -4.4286714 -4.4286704 -4.4286747][-4.4288559 -4.4288378 -4.4288206 -4.4287996 -4.4287786 -4.4287663 -4.4287629 -4.4287629 -4.4287624 -4.4287586 -4.4287543 -4.4287515 -4.42875 -4.4287524 -4.4287586][-4.4288993 -4.428884 -4.4288707 -4.4288573 -4.428843 -4.4288335 -4.4288316 -4.4288316 -4.42883 -4.4288239 -4.4288187 -4.4288158 -4.4288163 -4.4288187 -4.4288239]]...]
INFO - root - 2017-12-08 10:43:20.477955: step 3710, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.467 sec/batch; 42h:40m:14s remains)
INFO - root - 2017-12-08 10:43:24.954671: step 3720, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 39h:25m:45s remains)
INFO - root - 2017-12-08 10:43:29.479330: step 3730, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:01m:09s remains)
INFO - root - 2017-12-08 10:43:33.818041: step 3740, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:36m:36s remains)
INFO - root - 2017-12-08 10:43:38.200290: step 3750, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:40m:25s remains)
INFO - root - 2017-12-08 10:43:42.552390: step 3760, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:08m:19s remains)
INFO - root - 2017-12-08 10:43:46.958063: step 3770, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 40h:49m:38s remains)
INFO - root - 2017-12-08 10:43:51.236684: step 3780, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:26m:18s remains)
INFO - root - 2017-12-08 10:43:55.760121: step 3790, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 39h:26m:04s remains)
INFO - root - 2017-12-08 10:44:00.267641: step 3800, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:19m:20s remains)
2017-12-08 10:44:00.794069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289174 -4.4289217 -4.4289265 -4.4289274 -4.42893 -4.428937 -4.4289465 -4.4289517 -4.4289484 -4.4289432 -4.4289403 -4.4289408 -4.4289436 -4.428946 -4.4289374][-4.4289145 -4.42892 -4.4289231 -4.4289207 -4.4289203 -4.4289236 -4.4289336 -4.4289403 -4.4289441 -4.4289446 -4.4289451 -4.4289479 -4.4289494 -4.4289517 -4.4289389][-4.4289126 -4.4289165 -4.428916 -4.42891 -4.4289069 -4.4289064 -4.4289131 -4.4289203 -4.4289284 -4.4289389 -4.428946 -4.4289541 -4.428956 -4.4289594 -4.428946][-4.4289002 -4.428895 -4.4288807 -4.4288659 -4.4288645 -4.428865 -4.4288726 -4.4288826 -4.4288969 -4.4289174 -4.4289293 -4.4289408 -4.4289455 -4.4289522 -4.42894][-4.4288707 -4.428844 -4.4288063 -4.4287729 -4.4287653 -4.428772 -4.428782 -4.4287953 -4.4288254 -4.4288678 -4.428894 -4.4289103 -4.4289155 -4.4289217 -4.4289112][-4.4288239 -4.4287667 -4.4287004 -4.4286408 -4.4286122 -4.4286122 -4.4286194 -4.4286356 -4.4286938 -4.4287786 -4.4288344 -4.4288597 -4.4288683 -4.4288783 -4.4288735][-4.4288044 -4.4287281 -4.428637 -4.4285455 -4.4284763 -4.428432 -4.428391 -4.4283719 -4.4284534 -4.4286032 -4.428709 -4.4287615 -4.4287915 -4.4288197 -4.4288268][-4.4288273 -4.4287515 -4.4286442 -4.4285288 -4.4284244 -4.4283223 -4.4281855 -4.4280696 -4.4281449 -4.42836 -4.4285235 -4.4286222 -4.428688 -4.42874 -4.4287667][-4.4288731 -4.4288158 -4.4287152 -4.428606 -4.4285054 -4.4283814 -4.4281874 -4.4279904 -4.4280009 -4.4282036 -4.42839 -4.4285212 -4.4286103 -4.4286752 -4.4287162][-4.4289031 -4.4288692 -4.4287963 -4.4287205 -4.4286613 -4.4285717 -4.4284172 -4.4282527 -4.4282122 -4.4283104 -4.4284291 -4.4285355 -4.4286137 -4.428668 -4.4287028][-4.4288878 -4.42886 -4.42881 -4.4287686 -4.4287515 -4.42871 -4.4286213 -4.4285231 -4.4284863 -4.4285188 -4.4285712 -4.4286313 -4.4286819 -4.4287143 -4.4287329][-4.4288673 -4.4288392 -4.4287977 -4.4287724 -4.4287786 -4.4287729 -4.4287338 -4.4286928 -4.4286766 -4.428688 -4.4287119 -4.4287429 -4.4287682 -4.428781 -4.4287863][-4.4288683 -4.4288435 -4.4288068 -4.4287839 -4.428793 -4.428802 -4.42879 -4.4287782 -4.4287767 -4.4287839 -4.4288011 -4.4288211 -4.4288306 -4.4288292 -4.4288244][-4.4288716 -4.4288583 -4.428834 -4.4288154 -4.4288177 -4.4288282 -4.4288306 -4.4288363 -4.4288406 -4.4288435 -4.4288507 -4.4288611 -4.4288635 -4.4288554 -4.4288464][-4.4288788 -4.4288726 -4.4288621 -4.4288521 -4.4288521 -4.4288568 -4.42886 -4.4288669 -4.4288712 -4.42887 -4.4288697 -4.4288731 -4.4288726 -4.4288659 -4.4288588]]...]
INFO - root - 2017-12-08 10:44:05.271792: step 3810, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:43m:36s remains)
INFO - root - 2017-12-08 10:44:09.675011: step 3820, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 41h:07m:39s remains)
INFO - root - 2017-12-08 10:44:14.125491: step 3830, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:02m:04s remains)
INFO - root - 2017-12-08 10:44:18.601855: step 3840, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 41h:01m:41s remains)
INFO - root - 2017-12-08 10:44:23.085921: step 3850, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:11m:42s remains)
INFO - root - 2017-12-08 10:44:27.588552: step 3860, loss = 2.28, batch loss = 2.23 (16.9 examples/sec; 0.474 sec/batch; 43h:15m:29s remains)
INFO - root - 2017-12-08 10:44:32.137287: step 3870, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:06m:18s remains)
INFO - root - 2017-12-08 10:44:36.352221: step 3880, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 40h:49m:14s remains)
INFO - root - 2017-12-08 10:44:40.831885: step 3890, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.443 sec/batch; 40h:27m:44s remains)
INFO - root - 2017-12-08 10:44:45.190180: step 3900, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.436 sec/batch; 39h:46m:29s remains)
2017-12-08 10:44:45.735388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288769 -4.4288597 -4.4288421 -4.42881 -4.4287739 -4.4287357 -4.4287243 -4.4287467 -4.4287858 -4.4288177 -4.4288392 -4.4288521 -4.4288573 -4.4288611 -4.4288678][-4.4288673 -4.4288487 -4.4288187 -4.4287653 -4.4287009 -4.4286332 -4.4286046 -4.4286308 -4.4286933 -4.4287553 -4.4287996 -4.4288273 -4.4288411 -4.4288487 -4.4288554][-4.4288697 -4.4288483 -4.4288034 -4.4287271 -4.4286313 -4.4285326 -4.4284859 -4.4285164 -4.4286017 -4.4286952 -4.4287667 -4.4288096 -4.4288297 -4.4288383 -4.428844][-4.428874 -4.4288487 -4.428793 -4.4286995 -4.4285717 -4.4284348 -4.4283633 -4.4283977 -4.4285092 -4.4286366 -4.4287348 -4.42879 -4.4288116 -4.42882 -4.4288254][-4.4288788 -4.4288521 -4.4287977 -4.4287004 -4.4285541 -4.4283848 -4.428277 -4.4282966 -4.4284286 -4.4285865 -4.4287047 -4.4287705 -4.4287906 -4.4287958 -4.4288006][-4.4288769 -4.4288564 -4.4288158 -4.4287338 -4.4285941 -4.4284091 -4.4282641 -4.4282537 -4.4283895 -4.4285626 -4.428688 -4.4287539 -4.428771 -4.4287734 -4.42878][-4.4288688 -4.4288616 -4.4288397 -4.4287815 -4.4286671 -4.4284935 -4.4283338 -4.4282908 -4.4284067 -4.4285669 -4.428678 -4.4287367 -4.428751 -4.4287539 -4.4287634][-4.4288549 -4.4288669 -4.428865 -4.4288297 -4.4287462 -4.4286046 -4.4284577 -4.4283919 -4.428462 -4.4285827 -4.4286656 -4.4287128 -4.4287291 -4.4287376 -4.4287515][-4.4288392 -4.4288654 -4.4288812 -4.428865 -4.42881 -4.4287033 -4.4285784 -4.428494 -4.42851 -4.4285855 -4.428647 -4.4286895 -4.4287138 -4.4287305 -4.4287481][-4.4288244 -4.4288588 -4.4288855 -4.4288831 -4.4288445 -4.428761 -4.4286537 -4.4285607 -4.4285378 -4.4285803 -4.4286332 -4.4286804 -4.4287181 -4.4287438 -4.428761][-4.428822 -4.4288568 -4.4288859 -4.4288864 -4.428854 -4.4287829 -4.4286909 -4.4286003 -4.4285593 -4.4285836 -4.4286332 -4.428689 -4.4287395 -4.4287739 -4.42879][-4.4288297 -4.4288611 -4.4288874 -4.4288893 -4.4288607 -4.4287982 -4.4287176 -4.4286356 -4.4285927 -4.4286056 -4.428647 -4.4287043 -4.428761 -4.4288011 -4.4288158][-4.4288359 -4.4288635 -4.4288855 -4.4288883 -4.4288673 -4.428813 -4.4287367 -4.4286628 -4.42863 -4.4286423 -4.4286742 -4.4287243 -4.428781 -4.4288206 -4.4288335][-4.4288387 -4.4288607 -4.4288812 -4.4288869 -4.4288764 -4.4288344 -4.4287634 -4.4286947 -4.4286718 -4.4286866 -4.4287114 -4.428751 -4.428803 -4.4288411 -4.42885][-4.4288526 -4.4288673 -4.4288859 -4.4288912 -4.428885 -4.4288521 -4.4287887 -4.4287257 -4.4287076 -4.4287229 -4.4287443 -4.4287786 -4.4288282 -4.4288611 -4.4288645]]...]
INFO - root - 2017-12-08 10:44:50.178302: step 3910, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 39h:15m:33s remains)
INFO - root - 2017-12-08 10:44:54.528519: step 3920, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.421 sec/batch; 38h:26m:53s remains)
INFO - root - 2017-12-08 10:44:58.969772: step 3930, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 39h:54m:51s remains)
INFO - root - 2017-12-08 10:45:03.447102: step 3940, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:29m:04s remains)
INFO - root - 2017-12-08 10:45:07.990274: step 3950, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:31m:38s remains)
INFO - root - 2017-12-08 10:45:12.580659: step 3960, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:26m:44s remains)
INFO - root - 2017-12-08 10:45:16.924743: step 3970, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:54m:13s remains)
INFO - root - 2017-12-08 10:45:21.296614: step 3980, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 41h:01m:56s remains)
INFO - root - 2017-12-08 10:45:25.779986: step 3990, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.446 sec/batch; 40h:39m:40s remains)
INFO - root - 2017-12-08 10:45:30.191923: step 4000, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 40h:53m:46s remains)
2017-12-08 10:45:30.743059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289279 -4.4289246 -4.4289179 -4.42891 -4.4289017 -4.4288907 -4.4288845 -4.4288874 -4.4289055 -4.4289322 -4.4289465 -4.4289484 -4.4289517 -4.4289522 -4.4289393][-4.4289036 -4.4288988 -4.4288883 -4.4288764 -4.4288578 -4.4288316 -4.4288039 -4.4287887 -4.4288049 -4.4288464 -4.4288764 -4.4288874 -4.428895 -4.428906 -4.4289064][-4.4288564 -4.4288559 -4.4288464 -4.4288311 -4.4287992 -4.4287496 -4.4286966 -4.4286556 -4.4286628 -4.4287143 -4.4287658 -4.4287949 -4.4288092 -4.4288273 -4.4288445][-4.4287958 -4.4288011 -4.4287982 -4.4287834 -4.4287329 -4.4286423 -4.4285417 -4.4284678 -4.4284811 -4.4285579 -4.428638 -4.4286919 -4.428721 -4.4287505 -4.4287772][-4.42873 -4.4287372 -4.4287477 -4.4287357 -4.4286609 -4.4285121 -4.4283328 -4.4282041 -4.4282475 -4.4283924 -4.4285293 -4.4286141 -4.4286623 -4.4287028 -4.4287381][-4.4286385 -4.4286427 -4.4286637 -4.4286547 -4.4285641 -4.4283581 -4.4280748 -4.4278674 -4.4279675 -4.4282279 -4.428452 -4.4285865 -4.42866 -4.4287128 -4.4287462][-4.428525 -4.428515 -4.4285483 -4.4285579 -4.4284739 -4.4282355 -4.4278564 -4.4275541 -4.4277191 -4.4281063 -4.4284153 -4.428596 -4.428688 -4.4287391 -4.428761][-4.4284315 -4.4284 -4.4284353 -4.4284754 -4.4284363 -4.4282355 -4.4278646 -4.4275494 -4.4277091 -4.4281139 -4.4284368 -4.428628 -4.4287267 -4.4287691 -4.428772][-4.4284215 -4.4283643 -4.4283915 -4.4284592 -4.4284763 -4.4283671 -4.4281344 -4.427928 -4.4280105 -4.4282761 -4.4285116 -4.4286594 -4.4287367 -4.42876 -4.4287424][-4.4284725 -4.4284005 -4.4284153 -4.42849 -4.42854 -4.4285088 -4.4284091 -4.4283133 -4.4283328 -4.4284525 -4.428586 -4.4286737 -4.4287152 -4.4287152 -4.4286776][-4.4285383 -4.4284644 -4.4284711 -4.4285393 -4.4285913 -4.4285922 -4.4285617 -4.4285226 -4.4284987 -4.4285221 -4.4285884 -4.4286489 -4.4286842 -4.428679 -4.4286366][-4.4285727 -4.4285011 -4.4285007 -4.4285545 -4.4285932 -4.428597 -4.4285946 -4.4285793 -4.4285383 -4.4285135 -4.4285421 -4.4286008 -4.4286547 -4.4286733 -4.428648][-4.4285941 -4.428525 -4.4285188 -4.4285569 -4.4285812 -4.428587 -4.4285908 -4.4285846 -4.4285536 -4.4285264 -4.4285474 -4.4286022 -4.4286652 -4.4287066 -4.4287014][-4.42864 -4.4285831 -4.428576 -4.4285989 -4.4286089 -4.42861 -4.4286151 -4.428617 -4.4285994 -4.4285851 -4.4286027 -4.4286375 -4.4286909 -4.4287367 -4.4287448][-4.4287138 -4.4286723 -4.4286661 -4.4286723 -4.4286633 -4.42865 -4.428648 -4.4286456 -4.4286366 -4.4286318 -4.4286456 -4.4286695 -4.4287157 -4.4287586 -4.4287696]]...]
INFO - root - 2017-12-08 10:45:35.124563: step 4010, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:07m:18s remains)
INFO - root - 2017-12-08 10:45:39.495671: step 4020, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:54m:29s remains)
INFO - root - 2017-12-08 10:45:43.908125: step 4030, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:10m:15s remains)
INFO - root - 2017-12-08 10:45:48.323335: step 4040, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 39h:52m:44s remains)
INFO - root - 2017-12-08 10:45:52.784382: step 4050, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.460 sec/batch; 41h:56m:05s remains)
INFO - root - 2017-12-08 10:45:57.359085: step 4060, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:14m:09s remains)
INFO - root - 2017-12-08 10:46:01.556128: step 4070, loss = 2.28, batch loss = 2.23 (27.3 examples/sec; 0.294 sec/batch; 26h:46m:58s remains)
INFO - root - 2017-12-08 10:46:06.109791: step 4080, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:36m:14s remains)
INFO - root - 2017-12-08 10:46:10.686439: step 4090, loss = 2.28, batch loss = 2.23 (16.4 examples/sec; 0.488 sec/batch; 44h:31m:57s remains)
INFO - root - 2017-12-08 10:46:15.082500: step 4100, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.431 sec/batch; 39h:19m:09s remains)
2017-12-08 10:46:15.619445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289589 -4.4289565 -4.4289522 -4.42895 -4.4289522 -4.4289565 -4.4289532 -4.428957 -4.428966 -4.4289713 -4.4289651 -4.4289503 -4.4289393 -4.4289403 -4.4289527][-4.4289336 -4.4289279 -4.428926 -4.4289279 -4.4289351 -4.4289455 -4.4289494 -4.4289556 -4.4289627 -4.4289618 -4.4289451 -4.42892 -4.4289074 -4.4289122 -4.4289312][-4.4288669 -4.4288549 -4.4288621 -4.4288797 -4.4288974 -4.428915 -4.4289246 -4.4289308 -4.4289308 -4.428915 -4.4288855 -4.4288573 -4.4288568 -4.4288778 -4.4289079][-4.4287734 -4.4287658 -4.42879 -4.4288268 -4.4288483 -4.4288597 -4.428854 -4.4288468 -4.4288373 -4.4288073 -4.4287734 -4.4287653 -4.4287934 -4.4288363 -4.4288721][-4.4287148 -4.42872 -4.4287548 -4.4287944 -4.4288092 -4.4287982 -4.4287591 -4.4287248 -4.4287105 -4.4286895 -4.4286847 -4.4287162 -4.4287672 -4.4288149 -4.42884][-4.4286966 -4.4287071 -4.4287391 -4.4287653 -4.428762 -4.4287157 -4.4286308 -4.4285688 -4.4285731 -4.4286036 -4.4286585 -4.428731 -4.4287834 -4.4288054 -4.4288015][-4.4286814 -4.4286761 -4.4286947 -4.4287024 -4.4286633 -4.4285655 -4.4284282 -4.4283643 -4.42844 -4.4285593 -4.4286737 -4.4287596 -4.42879 -4.4287791 -4.4287486][-4.4286656 -4.4286394 -4.428638 -4.4286237 -4.4285479 -4.428401 -4.4282289 -4.4282012 -4.4283776 -4.4285755 -4.4287114 -4.4287729 -4.4287667 -4.4287238 -4.428678][-4.4286709 -4.4286375 -4.4286304 -4.4286041 -4.4285259 -4.4283962 -4.4282713 -4.4282956 -4.4284911 -4.42868 -4.4287777 -4.4287868 -4.4287391 -4.4286733 -4.4286294][-4.4286952 -4.4286695 -4.4286647 -4.4286394 -4.4285879 -4.42852 -4.4284754 -4.4285278 -4.4286704 -4.4287858 -4.4288197 -4.4287777 -4.4287014 -4.4286361 -4.4286156][-4.4287291 -4.4287119 -4.4287105 -4.4286909 -4.428668 -4.4286523 -4.4286537 -4.4287038 -4.4287834 -4.4288244 -4.4288096 -4.4287457 -4.4286675 -4.4286275 -4.4286442][-4.428761 -4.4287543 -4.4287543 -4.4287343 -4.4287262 -4.428741 -4.4287653 -4.4288011 -4.4288278 -4.4288149 -4.428771 -4.4287024 -4.4286456 -4.4286432 -4.4286938][-4.4287539 -4.4287515 -4.42875 -4.4287291 -4.4287324 -4.4287624 -4.4287982 -4.4288187 -4.4288058 -4.4287634 -4.4287109 -4.4286561 -4.428627 -4.4286542 -4.4287157][-4.4286666 -4.4286637 -4.4286604 -4.4286456 -4.42866 -4.4286981 -4.4287376 -4.4287462 -4.4287171 -4.4286747 -4.4286427 -4.4286094 -4.4286022 -4.42864 -4.4286919][-4.4285603 -4.4285522 -4.4285522 -4.4285464 -4.4285612 -4.428597 -4.4286284 -4.4286346 -4.428616 -4.428596 -4.42859 -4.428575 -4.4285727 -4.4286008 -4.4286351]]...]
INFO - root - 2017-12-08 10:46:20.058967: step 4110, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 39h:03m:35s remains)
INFO - root - 2017-12-08 10:46:24.511650: step 4120, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 39h:02m:11s remains)
INFO - root - 2017-12-08 10:46:28.933996: step 4130, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 39h:51m:36s remains)
INFO - root - 2017-12-08 10:46:33.383206: step 4140, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.460 sec/batch; 41h:58m:08s remains)
INFO - root - 2017-12-08 10:46:37.925322: step 4150, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:37m:23s remains)
INFO - root - 2017-12-08 10:46:42.424954: step 4160, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:22m:36s remains)
INFO - root - 2017-12-08 10:46:46.627266: step 4170, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:13m:29s remains)
INFO - root - 2017-12-08 10:46:51.004129: step 4180, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:23m:56s remains)
INFO - root - 2017-12-08 10:46:55.428033: step 4190, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:36m:17s remains)
INFO - root - 2017-12-08 10:46:59.853398: step 4200, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:03m:09s remains)
2017-12-08 10:47:00.415516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287481 -4.4287691 -4.4287868 -4.4287863 -4.4287658 -4.4287233 -4.4286561 -4.4286003 -4.4286046 -4.4286408 -4.4286494 -4.4286141 -4.4285951 -4.4286189 -4.4286728][-4.4287724 -4.4287992 -4.4288206 -4.4288254 -4.4288025 -4.42875 -4.4286752 -4.42862 -4.4286246 -4.4286509 -4.4286504 -4.4286113 -4.4285789 -4.42858 -4.4286151][-4.4288044 -4.4288254 -4.4288416 -4.4288449 -4.4288182 -4.4287505 -4.4286609 -4.4286056 -4.4286232 -4.428659 -4.4286623 -4.428627 -4.4285975 -4.428586 -4.428606][-4.428822 -4.4288344 -4.4288416 -4.4288368 -4.4287968 -4.4287047 -4.4285908 -4.4285297 -4.4285593 -4.4286141 -4.4286385 -4.4286251 -4.4286108 -4.4286041 -4.4286222][-4.4288344 -4.4288378 -4.4288354 -4.4288216 -4.4287648 -4.4286466 -4.4285078 -4.4284358 -4.42847 -4.4285474 -4.4286032 -4.4286227 -4.4286366 -4.4286423 -4.4286594][-4.4288445 -4.4288421 -4.4288325 -4.4288149 -4.428751 -4.4286232 -4.428472 -4.4283786 -4.4283957 -4.4284811 -4.4285626 -4.428616 -4.4286585 -4.4286833 -4.4287024][-4.4288554 -4.4288497 -4.4288344 -4.4288125 -4.4287524 -4.4286423 -4.4285049 -4.4284034 -4.4283848 -4.4284458 -4.4285326 -4.4286103 -4.428679 -4.4287257 -4.4287558][-4.4288735 -4.428865 -4.4288511 -4.4288259 -4.4287724 -4.4286828 -4.4285717 -4.4284749 -4.4284315 -4.428462 -4.4285364 -4.4286222 -4.4287033 -4.42876 -4.4287963][-4.4288921 -4.4288836 -4.4288735 -4.4288535 -4.4288096 -4.4287338 -4.4286366 -4.4285431 -4.4284878 -4.4284987 -4.4285607 -4.4286432 -4.4287248 -4.4287834 -4.4288187][-4.4289117 -4.4289007 -4.42889 -4.4288793 -4.4288511 -4.428792 -4.428709 -4.4286203 -4.4285593 -4.428556 -4.4286036 -4.4286776 -4.4287562 -4.4288163 -4.4288507][-4.4289269 -4.4289131 -4.428905 -4.4288988 -4.4288821 -4.4288373 -4.4287667 -4.4286871 -4.4286308 -4.428616 -4.4286466 -4.4287057 -4.4287772 -4.4288373 -4.4288735][-4.4289217 -4.4289079 -4.4289012 -4.428896 -4.4288859 -4.4288549 -4.4288 -4.4287319 -4.4286814 -4.4286618 -4.4286795 -4.4287257 -4.4287844 -4.4288378 -4.428875][-4.4289112 -4.4288983 -4.4288931 -4.4288893 -4.4288826 -4.4288616 -4.4288211 -4.4287667 -4.4287233 -4.4287071 -4.4287224 -4.4287586 -4.4288011 -4.4288421 -4.4288726][-4.4289002 -4.4288883 -4.428885 -4.428884 -4.4288797 -4.4288635 -4.4288311 -4.4287906 -4.4287577 -4.4287453 -4.4287581 -4.4287844 -4.4288154 -4.4288449 -4.4288659][-4.4288898 -4.4288797 -4.4288764 -4.4288754 -4.4288735 -4.4288645 -4.4288397 -4.4288082 -4.4287839 -4.4287767 -4.4287863 -4.4288039 -4.4288244 -4.428843 -4.4288568]]...]
INFO - root - 2017-12-08 10:47:04.806542: step 4210, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:44m:44s remains)
INFO - root - 2017-12-08 10:47:09.212364: step 4220, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:45m:09s remains)
INFO - root - 2017-12-08 10:47:13.575532: step 4230, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:38m:43s remains)
INFO - root - 2017-12-08 10:47:17.952428: step 4240, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.464 sec/batch; 42h:18m:38s remains)
INFO - root - 2017-12-08 10:47:22.319368: step 4250, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 39h:29m:51s remains)
INFO - root - 2017-12-08 10:47:26.701782: step 4260, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.462 sec/batch; 42h:06m:42s remains)
INFO - root - 2017-12-08 10:47:30.909859: step 4270, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:36m:44s remains)
INFO - root - 2017-12-08 10:47:35.315773: step 4280, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 40h:03m:54s remains)
INFO - root - 2017-12-08 10:47:39.724231: step 4290, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:35m:54s remains)
INFO - root - 2017-12-08 10:47:44.297830: step 4300, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:27m:54s remains)
2017-12-08 10:47:44.848540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428648 -4.4286532 -4.4286618 -4.4286604 -4.4286461 -4.4286156 -4.4285812 -4.428566 -4.42858 -4.428606 -4.42863 -4.42865 -4.4286575 -4.4286542 -4.4286461][-4.4286094 -4.4286256 -4.4286404 -4.428637 -4.4286094 -4.4285574 -4.428503 -4.4284825 -4.4285083 -4.4285502 -4.4285884 -4.4286237 -4.4286437 -4.4286437 -4.4286332][-4.4285889 -4.4286094 -4.4286246 -4.4286132 -4.42857 -4.4284978 -4.4284272 -4.4284067 -4.4284458 -4.428503 -4.428555 -4.4286036 -4.4286327 -4.4286342 -4.42862][-4.4285874 -4.4286051 -4.4286113 -4.4285855 -4.4285264 -4.4284415 -4.4283643 -4.4283481 -4.4284015 -4.428472 -4.4285364 -4.4285951 -4.4286289 -4.4286289 -4.428606][-4.4286146 -4.428617 -4.4286051 -4.4285626 -4.4284906 -4.4283981 -4.428318 -4.4283104 -4.428381 -4.4284678 -4.4285464 -4.4286122 -4.4286442 -4.4286351 -4.4285955][-4.428658 -4.4286394 -4.4286075 -4.4285488 -4.4284658 -4.4283662 -4.4282813 -4.4282756 -4.4283633 -4.42847 -4.4285669 -4.4286394 -4.4286656 -4.4286418 -4.4285812][-4.4286909 -4.4286575 -4.4286118 -4.4285445 -4.4284544 -4.4283442 -4.4282446 -4.4282312 -4.4283323 -4.42846 -4.4285736 -4.4286532 -4.428679 -4.4286475 -4.4285717][-4.4287004 -4.4286571 -4.4286079 -4.428546 -4.4284663 -4.4283571 -4.4282479 -4.4282246 -4.4283276 -4.4284644 -4.4285846 -4.4286647 -4.4286909 -4.4286609 -4.4285846][-4.4286933 -4.4286442 -4.4286027 -4.4285607 -4.4285064 -4.4284191 -4.4283257 -4.4283066 -4.4283967 -4.4285169 -4.4286213 -4.4286904 -4.4287119 -4.4286885 -4.4286246][-4.4286795 -4.4286265 -4.4285975 -4.42858 -4.4285531 -4.4284954 -4.4284296 -4.4284205 -4.4284925 -4.4285889 -4.42867 -4.4287219 -4.4287367 -4.428719 -4.4286709][-4.4286528 -4.4286008 -4.428587 -4.4285932 -4.4285927 -4.4285631 -4.4285212 -4.4285169 -4.4285703 -4.4286451 -4.4287081 -4.4287453 -4.4287529 -4.4287386 -4.4287019][-4.4286146 -4.4285707 -4.4285769 -4.4286089 -4.4286332 -4.428628 -4.4286017 -4.4285965 -4.4286289 -4.4286842 -4.4287343 -4.4287639 -4.4287672 -4.4287519 -4.4287167][-4.4285836 -4.4285564 -4.4285827 -4.4286318 -4.4286704 -4.4286804 -4.4286656 -4.4286604 -4.428678 -4.4287167 -4.4287572 -4.4287834 -4.4287872 -4.42877 -4.4287295][-4.4285636 -4.4285607 -4.4286036 -4.4286609 -4.4287038 -4.4287152 -4.428699 -4.4286871 -4.4286919 -4.4287167 -4.4287515 -4.42878 -4.4287915 -4.4287758 -4.4287333][-4.428545 -4.4285688 -4.4286246 -4.4286828 -4.4287195 -4.4287224 -4.4286971 -4.4286733 -4.4286647 -4.4286795 -4.4287114 -4.4287457 -4.4287682 -4.4287591 -4.4287233]]...]
INFO - root - 2017-12-08 10:47:49.399794: step 4310, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:30m:43s remains)
INFO - root - 2017-12-08 10:47:53.944879: step 4320, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:35m:22s remains)
INFO - root - 2017-12-08 10:47:58.460178: step 4330, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:16m:15s remains)
INFO - root - 2017-12-08 10:48:02.901942: step 4340, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:45m:17s remains)
INFO - root - 2017-12-08 10:48:07.355226: step 4350, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:13m:06s remains)
INFO - root - 2017-12-08 10:48:11.846818: step 4360, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:17m:19s remains)
INFO - root - 2017-12-08 10:48:15.973746: step 4370, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.469 sec/batch; 42h:43m:06s remains)
INFO - root - 2017-12-08 10:48:20.399950: step 4380, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.460 sec/batch; 41h:57m:32s remains)
INFO - root - 2017-12-08 10:48:24.937680: step 4390, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.456 sec/batch; 41h:31m:56s remains)
INFO - root - 2017-12-08 10:48:29.432804: step 4400, loss = 2.28, batch loss = 2.23 (16.7 examples/sec; 0.479 sec/batch; 43h:37m:59s remains)
2017-12-08 10:48:29.951432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287176 -4.4287419 -4.4287395 -4.4287128 -4.4287033 -4.4287286 -4.4287643 -4.4287591 -4.4287148 -4.4286814 -4.4286966 -4.4287496 -4.428813 -4.4288321 -4.428791][-4.4286938 -4.4287162 -4.4287086 -4.4286771 -4.4286776 -4.4287076 -4.4287343 -4.428721 -4.4286785 -4.4286532 -4.4286814 -4.4287295 -4.4287705 -4.4287696 -4.4287276][-4.4286275 -4.4286513 -4.4286442 -4.4286108 -4.4286132 -4.4286418 -4.4286637 -4.4286523 -4.4286218 -4.4286013 -4.4286251 -4.4286628 -4.428689 -4.4286785 -4.4286485][-4.4284911 -4.4285254 -4.4285331 -4.428503 -4.4284992 -4.4285264 -4.4285464 -4.4285421 -4.4285254 -4.428515 -4.4285388 -4.4285703 -4.4285851 -4.4285746 -4.428555][-4.4283547 -4.4284019 -4.428431 -4.428412 -4.4284048 -4.4284368 -4.428452 -4.4284444 -4.4284387 -4.4284406 -4.4284596 -4.4284811 -4.4284887 -4.4284816 -4.4284711][-4.4283023 -4.4283576 -4.4283919 -4.4283757 -4.4283667 -4.4283867 -4.4283762 -4.4283509 -4.4283605 -4.4283805 -4.4283919 -4.4283986 -4.4283991 -4.4283881 -4.428371][-4.4283075 -4.4283481 -4.4283595 -4.4283247 -4.4282928 -4.4282675 -4.4281969 -4.4281411 -4.4281888 -4.4282484 -4.4282703 -4.4282846 -4.4282937 -4.4282908 -4.4282851][-4.4283433 -4.4283447 -4.4283161 -4.42825 -4.4281793 -4.4280882 -4.4279432 -4.4278631 -4.4279675 -4.4280858 -4.4281344 -4.4281797 -4.4282212 -4.4282508 -4.4282832][-4.4284258 -4.4283986 -4.4283428 -4.428266 -4.4281816 -4.4280696 -4.4279213 -4.4278636 -4.427989 -4.4281192 -4.4281721 -4.42823 -4.4282923 -4.4283428 -4.4284005][-4.4285288 -4.4284968 -4.42844 -4.4283738 -4.4283085 -4.4282284 -4.4281359 -4.4281158 -4.4282117 -4.4283042 -4.4283376 -4.4283834 -4.428442 -4.42849 -4.4285455][-4.4286022 -4.428576 -4.4285345 -4.4284921 -4.4284534 -4.4284105 -4.4283686 -4.4283657 -4.4284234 -4.4284797 -4.4284997 -4.4285288 -4.4285703 -4.4286027 -4.428638][-4.4286652 -4.4286423 -4.4286103 -4.4285817 -4.4285588 -4.4285359 -4.4285154 -4.4285135 -4.42854 -4.4285707 -4.4285851 -4.4286022 -4.4286289 -4.4286485 -4.4286747][-4.4287286 -4.4287028 -4.4286747 -4.4286513 -4.4286356 -4.4286208 -4.4286056 -4.4285984 -4.4286046 -4.428618 -4.4286256 -4.4286308 -4.4286351 -4.4286442 -4.4286675][-4.4287839 -4.4287567 -4.4287338 -4.4287181 -4.4287086 -4.4287 -4.4286904 -4.4286828 -4.4286823 -4.4286861 -4.4286842 -4.4286737 -4.4286542 -4.4286451 -4.4286551][-4.4288354 -4.4288139 -4.4287982 -4.4287887 -4.4287834 -4.4287791 -4.4287739 -4.4287682 -4.4287648 -4.4287643 -4.4287577 -4.428741 -4.4287095 -4.4286814 -4.4286685]]...]
INFO - root - 2017-12-08 10:48:34.415302: step 4410, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 39h:26m:05s remains)
INFO - root - 2017-12-08 10:48:38.900299: step 4420, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.461 sec/batch; 42h:00m:24s remains)
INFO - root - 2017-12-08 10:48:43.488078: step 4430, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:46m:10s remains)
INFO - root - 2017-12-08 10:48:47.964207: step 4440, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:34m:36s remains)
INFO - root - 2017-12-08 10:48:52.401524: step 4450, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:18m:56s remains)
INFO - root - 2017-12-08 10:48:56.946068: step 4460, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.464 sec/batch; 42h:16m:39s remains)
INFO - root - 2017-12-08 10:49:01.187091: step 4470, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:12m:21s remains)
INFO - root - 2017-12-08 10:49:05.599054: step 4480, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.446 sec/batch; 40h:35m:48s remains)
INFO - root - 2017-12-08 10:49:10.062542: step 4490, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.446 sec/batch; 40h:35m:51s remains)
INFO - root - 2017-12-08 10:49:14.533119: step 4500, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 40h:47m:40s remains)
2017-12-08 10:49:15.065566: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288969 -4.4288812 -4.4288745 -4.4288816 -4.4288898 -4.4289012 -4.428895 -4.4288821 -4.4288807 -4.4289055 -4.4289384 -4.4289513 -4.4289527 -4.4289427 -4.4289193][-4.4288878 -4.4288716 -4.4288611 -4.4288678 -4.4288754 -4.4288816 -4.4288597 -4.4288225 -4.4288068 -4.428844 -4.4288931 -4.4289193 -4.4289355 -4.4289308 -4.428906][-4.4288974 -4.4288745 -4.4288545 -4.4288526 -4.4288526 -4.4288464 -4.4288058 -4.4287438 -4.4287243 -4.4287796 -4.4288435 -4.4288836 -4.428916 -4.4289227 -4.4289][-4.4289045 -4.4288831 -4.4288507 -4.4288268 -4.4288058 -4.4287772 -4.4287143 -4.4286342 -4.4286304 -4.4287128 -4.42879 -4.4288411 -4.4288878 -4.4289045 -4.4288912][-4.4289031 -4.4288816 -4.4288387 -4.4287891 -4.4287386 -4.42868 -4.4285841 -4.4284892 -4.428514 -4.4286327 -4.4287305 -4.4287972 -4.4288564 -4.428884 -4.4288783][-4.428894 -4.4288712 -4.4288206 -4.4287519 -4.4286733 -4.4285755 -4.428421 -4.4282994 -4.4283762 -4.428546 -4.4286695 -4.4287524 -4.4288273 -4.4288692 -4.4288712][-4.4288921 -4.42887 -4.4288187 -4.4287405 -4.4286432 -4.428503 -4.4282775 -4.4281111 -4.4282422 -4.4284778 -4.428627 -4.4287233 -4.4288096 -4.428863 -4.4288721][-4.428894 -4.42888 -4.4288378 -4.4287634 -4.4286585 -4.4284964 -4.428236 -4.42804 -4.4281936 -4.4284534 -4.4286122 -4.4287176 -4.4288058 -4.4288621 -4.4288783][-4.4288864 -4.4288816 -4.4288559 -4.4287982 -4.42871 -4.4285693 -4.4283504 -4.4281826 -4.4282923 -4.4284983 -4.4286304 -4.4287271 -4.4288077 -4.4288635 -4.4288826][-4.4288735 -4.4288754 -4.4288578 -4.42882 -4.4287553 -4.4286485 -4.4285 -4.4283924 -4.4284596 -4.4285922 -4.428678 -4.4287462 -4.4288082 -4.4288568 -4.4288764][-4.4288497 -4.428854 -4.428844 -4.4288206 -4.4287744 -4.4286976 -4.4286065 -4.4285474 -4.4285975 -4.4286819 -4.4287319 -4.428771 -4.4288054 -4.4288421 -4.42886][-4.4288187 -4.4288268 -4.4288239 -4.4288139 -4.4287815 -4.4287252 -4.4286709 -4.4286437 -4.428678 -4.4287305 -4.4287615 -4.4287839 -4.4287953 -4.4288168 -4.4288321][-4.4287844 -4.4287992 -4.4288054 -4.4288011 -4.428772 -4.4287348 -4.4287109 -4.4287052 -4.4287271 -4.4287562 -4.4287705 -4.4287753 -4.4287758 -4.4287891 -4.4287944][-4.4287629 -4.4287844 -4.4287996 -4.4287963 -4.4287658 -4.4287381 -4.4287324 -4.428741 -4.4287586 -4.4287734 -4.4287758 -4.4287639 -4.4287553 -4.428762 -4.4287543][-4.4287591 -4.4287934 -4.4288144 -4.4288149 -4.428791 -4.4287677 -4.4287586 -4.4287596 -4.4287643 -4.4287705 -4.428772 -4.4287577 -4.4287457 -4.4287372 -4.4287162]]...]
INFO - root - 2017-12-08 10:49:19.481208: step 4510, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 41h:01m:19s remains)
INFO - root - 2017-12-08 10:49:23.914977: step 4520, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 39h:53m:59s remains)
INFO - root - 2017-12-08 10:49:28.313933: step 4530, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 40h:44m:19s remains)
INFO - root - 2017-12-08 10:49:32.810077: step 4540, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.463 sec/batch; 42h:08m:04s remains)
INFO - root - 2017-12-08 10:49:37.372595: step 4550, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:18m:28s remains)
INFO - root - 2017-12-08 10:49:41.863798: step 4560, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:25m:58s remains)
INFO - root - 2017-12-08 10:49:46.149517: step 4570, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:44m:33s remains)
INFO - root - 2017-12-08 10:49:50.735672: step 4580, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.469 sec/batch; 42h:41m:01s remains)
INFO - root - 2017-12-08 10:49:55.215777: step 4590, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 39h:11m:17s remains)
INFO - root - 2017-12-08 10:49:59.724872: step 4600, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:40m:41s remains)
2017-12-08 10:50:00.276775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289103 -4.4288983 -4.4288936 -4.4288945 -4.4288993 -4.428906 -4.4289131 -4.4289241 -4.4289379 -4.4289489 -4.4289513 -4.4289465 -4.4289389 -4.4289308 -4.4289193][-4.4288988 -4.4288797 -4.42887 -4.4288683 -4.4288688 -4.4288707 -4.4288769 -4.4288917 -4.4289169 -4.4289389 -4.4289479 -4.4289446 -4.428936 -4.4289265 -4.4289112][-4.4288797 -4.428853 -4.4288397 -4.428833 -4.4288249 -4.4288177 -4.4288182 -4.4288354 -4.4288735 -4.4289117 -4.4289303 -4.4289322 -4.4289255 -4.428915 -4.4288964][-4.4288592 -4.4288259 -4.4288058 -4.42879 -4.4287705 -4.428751 -4.428741 -4.4287572 -4.4288116 -4.42887 -4.4289045 -4.428916 -4.428916 -4.4289074 -4.4288845][-4.4288411 -4.4288044 -4.4287763 -4.4287481 -4.4287152 -4.4286838 -4.4286571 -4.4286623 -4.4287214 -4.4288015 -4.4288611 -4.4288907 -4.4289045 -4.4289069 -4.4288859][-4.4288368 -4.4287982 -4.428762 -4.4287171 -4.4286675 -4.42862 -4.4285684 -4.4285455 -4.4286 -4.4287038 -4.4287949 -4.4288507 -4.4288845 -4.4289045 -4.4288955][-4.4288497 -4.4288135 -4.428772 -4.4287128 -4.4286418 -4.4285669 -4.4284739 -4.4284053 -4.4284463 -4.4285774 -4.4287014 -4.4287839 -4.4288416 -4.4288836 -4.4288917][-4.4288759 -4.4288483 -4.4288082 -4.4287453 -4.4286604 -4.42856 -4.4284191 -4.4282913 -4.4283013 -4.4284434 -4.4285903 -4.4286952 -4.4287734 -4.4288344 -4.42886][-4.4289012 -4.428884 -4.4288535 -4.4287987 -4.4287214 -4.4286203 -4.4284678 -4.4283037 -4.4282618 -4.4283686 -4.4285045 -4.4286146 -4.4287038 -4.4287739 -4.4288135][-4.4289093 -4.4289 -4.4288816 -4.42884 -4.4287782 -4.4286981 -4.4285746 -4.4284282 -4.4283562 -4.4283986 -4.4284835 -4.4285707 -4.4286513 -4.4287186 -4.4287615][-4.4289007 -4.4288936 -4.4288836 -4.4288545 -4.4288116 -4.4287572 -4.4286752 -4.4285684 -4.4284987 -4.4284997 -4.428534 -4.4285836 -4.428638 -4.4286861 -4.4287214][-4.4288845 -4.4288745 -4.428865 -4.428844 -4.4288192 -4.4287896 -4.4287395 -4.42867 -4.4286203 -4.428607 -4.4286118 -4.42863 -4.4286561 -4.4286842 -4.428709][-4.4288726 -4.4288568 -4.428844 -4.4288268 -4.4288125 -4.4287987 -4.4287705 -4.4287295 -4.4287024 -4.4286938 -4.428688 -4.4286847 -4.428688 -4.4287014 -4.4287205][-4.4288669 -4.4288497 -4.4288359 -4.4288182 -4.4288063 -4.4287963 -4.4287772 -4.4287539 -4.4287448 -4.4287453 -4.4287395 -4.4287271 -4.4287157 -4.4287176 -4.4287324][-4.42887 -4.4288554 -4.4288425 -4.4288254 -4.4288106 -4.4287972 -4.4287782 -4.428762 -4.4287634 -4.4287734 -4.428772 -4.4287586 -4.4287395 -4.4287314 -4.4287391]]...]
INFO - root - 2017-12-08 10:50:04.810152: step 4610, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 40h:44m:59s remains)
INFO - root - 2017-12-08 10:50:09.276135: step 4620, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.436 sec/batch; 39h:42m:39s remains)
INFO - root - 2017-12-08 10:50:13.735961: step 4630, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:33m:04s remains)
INFO - root - 2017-12-08 10:50:18.238958: step 4640, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.463 sec/batch; 42h:09m:44s remains)
INFO - root - 2017-12-08 10:50:22.733864: step 4650, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:41m:47s remains)
INFO - root - 2017-12-08 10:50:27.170141: step 4660, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:39m:06s remains)
INFO - root - 2017-12-08 10:50:31.357338: step 4670, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:38m:51s remains)
INFO - root - 2017-12-08 10:50:35.898315: step 4680, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.422 sec/batch; 38h:26m:51s remains)
INFO - root - 2017-12-08 10:50:40.355162: step 4690, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.431 sec/batch; 39h:12m:51s remains)
INFO - root - 2017-12-08 10:50:44.859263: step 4700, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:37m:27s remains)
2017-12-08 10:50:45.370384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288044 -4.4287987 -4.4288192 -4.428853 -4.428863 -4.4288435 -4.428823 -4.4288144 -4.4288087 -4.4288077 -4.4287992 -4.428771 -4.4287429 -4.4287505 -4.4287734][-4.4287896 -4.4287868 -4.4288182 -4.4288669 -4.4288797 -4.4288478 -4.4288125 -4.4287996 -4.4287949 -4.428793 -4.4287853 -4.4287686 -4.4287553 -4.4287715 -4.4287992][-4.4287467 -4.4287567 -4.4288058 -4.428864 -4.4288712 -4.4288268 -4.4287839 -4.4287753 -4.4287834 -4.4287882 -4.4287772 -4.4287586 -4.4287543 -4.4287834 -4.428823][-4.4286909 -4.4287167 -4.428782 -4.4288397 -4.4288344 -4.4287767 -4.4287281 -4.4287338 -4.4287648 -4.428782 -4.4287667 -4.4287395 -4.4287353 -4.4287767 -4.4288306][-4.4286566 -4.4286871 -4.4287586 -4.4288039 -4.4287734 -4.4286847 -4.4286218 -4.4286528 -4.4287238 -4.4287596 -4.4287405 -4.4287066 -4.4287071 -4.4287615 -4.4288287][-4.4286466 -4.428668 -4.4287267 -4.4287419 -4.4286671 -4.4285278 -4.4284453 -4.4285164 -4.4286346 -4.4286947 -4.4286785 -4.4286404 -4.4286475 -4.4287181 -4.4288025][-4.4286542 -4.4286594 -4.428689 -4.4286637 -4.4285378 -4.4283442 -4.4282446 -4.4283581 -4.4285221 -4.4286046 -4.428587 -4.4285412 -4.4285536 -4.4286456 -4.4287519][-4.4286861 -4.428678 -4.4286776 -4.4286208 -4.4284697 -4.4282594 -4.4281583 -4.4282846 -4.428463 -4.4285493 -4.4285212 -4.4284654 -4.4284859 -4.4285922 -4.4287109][-4.4287314 -4.4287281 -4.42872 -4.4286523 -4.4285078 -4.4283209 -4.4282236 -4.4283247 -4.4284797 -4.428556 -4.42852 -4.4284592 -4.4284806 -4.4285846 -4.4286995][-4.4287658 -4.42877 -4.4287691 -4.4287133 -4.4285917 -4.428432 -4.4283357 -4.4284 -4.4285216 -4.428586 -4.4285507 -4.4284925 -4.4285083 -4.4285989 -4.4287019][-4.4287791 -4.4287858 -4.428792 -4.4287577 -4.4286718 -4.4285431 -4.42845 -4.4284763 -4.4285645 -4.428618 -4.428596 -4.4285474 -4.4285522 -4.4286261 -4.4287114][-4.4287834 -4.42879 -4.428803 -4.4287872 -4.4287305 -4.428638 -4.42856 -4.42856 -4.42862 -4.4286618 -4.4286489 -4.4286027 -4.4285951 -4.4286551 -4.4287257][-4.4287968 -4.428803 -4.4288182 -4.4288125 -4.4287763 -4.4287181 -4.428658 -4.428648 -4.4286866 -4.4287157 -4.4286995 -4.4286523 -4.4286375 -4.4286847 -4.4287415][-4.4288158 -4.4288249 -4.428844 -4.4288459 -4.4288259 -4.4287915 -4.42875 -4.4287391 -4.4287629 -4.4287753 -4.4287539 -4.4287076 -4.4286923 -4.4287238 -4.42876][-4.4288306 -4.4288445 -4.4288635 -4.4288712 -4.428864 -4.4288416 -4.4288144 -4.4288082 -4.4288225 -4.4288244 -4.4288015 -4.428762 -4.4287505 -4.4287691 -4.4287848]]...]
INFO - root - 2017-12-08 10:50:49.783790: step 4710, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 39h:11m:08s remains)
INFO - root - 2017-12-08 10:50:54.237170: step 4720, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:29m:55s remains)
INFO - root - 2017-12-08 10:50:58.649355: step 4730, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.464 sec/batch; 42h:14m:56s remains)
INFO - root - 2017-12-08 10:51:03.121572: step 4740, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.464 sec/batch; 42h:16m:58s remains)
INFO - root - 2017-12-08 10:51:07.620677: step 4750, loss = 2.28, batch loss = 2.23 (16.7 examples/sec; 0.478 sec/batch; 43h:29m:57s remains)
INFO - root - 2017-12-08 10:51:12.174366: step 4760, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:44m:02s remains)
INFO - root - 2017-12-08 10:51:16.349940: step 4770, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 39h:25m:56s remains)
INFO - root - 2017-12-08 10:51:20.734979: step 4780, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:08m:53s remains)
INFO - root - 2017-12-08 10:51:25.152472: step 4790, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:24m:55s remains)
INFO - root - 2017-12-08 10:51:29.613695: step 4800, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 38h:52m:36s remains)
2017-12-08 10:51:30.122735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288 -4.4287925 -4.428793 -4.4287877 -4.4287748 -4.4287596 -4.4287424 -4.4287314 -4.4287324 -4.4287353 -4.4287376 -4.4287491 -4.4287729 -4.428803 -4.4288297][-4.4288034 -4.4287872 -4.4287815 -4.4287806 -4.42878 -4.4287777 -4.428772 -4.4287643 -4.4287624 -4.428761 -4.4287629 -4.4287777 -4.4288025 -4.4288292 -4.4288526][-4.4288206 -4.428802 -4.4287977 -4.4288025 -4.4288106 -4.4288144 -4.4288111 -4.428802 -4.4287944 -4.4287868 -4.428782 -4.4287896 -4.4288096 -4.428833 -4.4288521][-4.4288421 -4.4288216 -4.4288187 -4.4288235 -4.4288282 -4.4288249 -4.4288125 -4.4287963 -4.4287853 -4.4287758 -4.4287672 -4.4287677 -4.4287825 -4.4288006 -4.4288149][-4.4288483 -4.4288254 -4.4288149 -4.4288139 -4.4288073 -4.4287825 -4.4287434 -4.4287133 -4.4287071 -4.42871 -4.4287066 -4.4287071 -4.4287219 -4.4287391 -4.4287524][-4.4288311 -4.4288058 -4.4287839 -4.4287691 -4.428741 -4.42868 -4.4286013 -4.4285603 -4.4285789 -4.4286141 -4.4286313 -4.4286418 -4.4286652 -4.4286895 -4.4287057][-4.4287925 -4.4287691 -4.4287376 -4.4287062 -4.4286513 -4.4285455 -4.428421 -4.4283786 -4.428441 -4.4285235 -4.42857 -4.4285984 -4.4286361 -4.4286747 -4.4286952][-4.4287438 -4.42872 -4.4286842 -4.4286451 -4.4285774 -4.4284472 -4.4283009 -4.4282761 -4.4283814 -4.4284959 -4.4285579 -4.428596 -4.4286385 -4.4286823 -4.4287028][-4.4286966 -4.4286747 -4.4286451 -4.4286246 -4.4285855 -4.4284935 -4.4283895 -4.4283905 -4.4284916 -4.42859 -4.428638 -4.4286628 -4.4286895 -4.4287219 -4.4287362][-4.4286561 -4.4286366 -4.4286213 -4.4286342 -4.4286418 -4.4286089 -4.4285636 -4.4285779 -4.428648 -4.4287109 -4.4287372 -4.4287438 -4.4287524 -4.4287705 -4.4287786][-4.4286318 -4.4286108 -4.4286156 -4.428658 -4.4286981 -4.4287057 -4.4286976 -4.4287133 -4.4287567 -4.4287949 -4.428812 -4.4288139 -4.4288149 -4.4288216 -4.4288259][-4.4286203 -4.4286056 -4.4286337 -4.4286933 -4.4287448 -4.4287682 -4.4287744 -4.4287863 -4.42881 -4.4288344 -4.42885 -4.4288554 -4.4288592 -4.428864 -4.4288669][-4.428648 -4.4286442 -4.4286809 -4.4287314 -4.4287705 -4.4287882 -4.4287925 -4.4288 -4.4288154 -4.4288354 -4.4288559 -4.4288697 -4.4288812 -4.4288869 -4.428884][-4.4287062 -4.4287066 -4.4287372 -4.42877 -4.42879 -4.4287987 -4.4288 -4.4288058 -4.4288182 -4.4288349 -4.4288549 -4.4288712 -4.428884 -4.4288869 -4.4288788][-4.4287438 -4.4287481 -4.4287691 -4.4287863 -4.4287963 -4.4288039 -4.4288106 -4.4288173 -4.4288235 -4.4288282 -4.4288406 -4.4288564 -4.4288645 -4.4288592 -4.4288449]]...]
INFO - root - 2017-12-08 10:51:34.589570: step 4810, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 39h:56m:19s remains)
INFO - root - 2017-12-08 10:51:39.030377: step 4820, loss = 2.28, batch loss = 2.23 (17.0 examples/sec; 0.470 sec/batch; 42h:48m:25s remains)
INFO - root - 2017-12-08 10:51:43.512052: step 4830, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:19m:07s remains)
INFO - root - 2017-12-08 10:51:48.099000: step 4840, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 40h:54m:23s remains)
INFO - root - 2017-12-08 10:51:52.555074: step 4850, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:31m:13s remains)
INFO - root - 2017-12-08 10:51:57.174471: step 4860, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 41h:04m:13s remains)
INFO - root - 2017-12-08 10:52:01.361315: step 4870, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:40m:13s remains)
INFO - root - 2017-12-08 10:52:05.805203: step 4880, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 38h:52m:38s remains)
INFO - root - 2017-12-08 10:52:10.396545: step 4890, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:31m:33s remains)
INFO - root - 2017-12-08 10:52:14.912937: step 4900, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:18m:57s remains)
2017-12-08 10:52:15.420969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286814 -4.4287529 -4.42881 -4.4288049 -4.4287491 -4.4287024 -4.4287024 -4.4287229 -4.4287705 -4.428823 -4.4288545 -4.4288697 -4.4288607 -4.4288273 -4.4287906][-4.4286833 -4.42876 -4.4288225 -4.4288239 -4.4287777 -4.4287314 -4.4287186 -4.4287362 -4.4287896 -4.4288468 -4.4288774 -4.4288917 -4.4288855 -4.4288478 -4.4288006][-4.4286752 -4.428761 -4.4288297 -4.4288373 -4.4287934 -4.42874 -4.4287128 -4.4287319 -4.4287977 -4.4288664 -4.4289 -4.4289112 -4.4289083 -4.4288716 -4.4288154][-4.4286375 -4.4287295 -4.4288044 -4.4288197 -4.4287763 -4.4287148 -4.4286718 -4.4286909 -4.4287748 -4.42886 -4.4288988 -4.4289093 -4.4289122 -4.4288783 -4.4288149][-4.4286075 -4.4286942 -4.4287696 -4.4287896 -4.4287472 -4.4286733 -4.4286065 -4.4286141 -4.4287176 -4.4288192 -4.4288683 -4.4288855 -4.42889 -4.4288535 -4.428782][-4.428606 -4.4286819 -4.4287539 -4.4287772 -4.4287362 -4.4286504 -4.4285507 -4.4285307 -4.4286437 -4.428761 -4.4288287 -4.4288607 -4.4288654 -4.428822 -4.4287462][-4.4286385 -4.4286995 -4.4287639 -4.4287882 -4.42875 -4.4286652 -4.4285483 -4.4285064 -4.4286156 -4.42874 -4.428823 -4.4288669 -4.4288664 -4.4288163 -4.4287376][-4.4286833 -4.4287224 -4.4287691 -4.4287853 -4.42875 -4.4286771 -4.4285774 -4.4285479 -4.4286475 -4.4287634 -4.4288492 -4.4288945 -4.4288869 -4.4288368 -4.4287591][-4.4286995 -4.4287214 -4.4287467 -4.4287443 -4.4287014 -4.4286437 -4.4285846 -4.4285913 -4.428689 -4.42879 -4.4288645 -4.4289017 -4.4288898 -4.4288473 -4.4287858][-4.4286852 -4.4286942 -4.4287062 -4.4286876 -4.4286413 -4.4286065 -4.4285955 -4.4286356 -4.4287271 -4.4288116 -4.4288688 -4.4288917 -4.428875 -4.4288421 -4.4287968][-4.4286551 -4.4286575 -4.4286656 -4.4286456 -4.4286084 -4.4286008 -4.4286265 -4.4286823 -4.428762 -4.4288292 -4.4288683 -4.4288778 -4.4288583 -4.4288306 -4.4287977][-4.4286509 -4.4286489 -4.4286556 -4.4286389 -4.4286127 -4.4286232 -4.4286704 -4.4287295 -4.4287953 -4.4288478 -4.4288716 -4.4288673 -4.428843 -4.4288139 -4.4287887][-4.428679 -4.428669 -4.4286728 -4.4286594 -4.4286394 -4.4286551 -4.42871 -4.4287653 -4.4288206 -4.428863 -4.4288731 -4.4288588 -4.4288316 -4.4288054 -4.4287872][-4.4287205 -4.4287062 -4.4287095 -4.4286971 -4.4286761 -4.4286871 -4.4287329 -4.4287786 -4.4288216 -4.4288521 -4.4288497 -4.428834 -4.4288116 -4.4287925 -4.4287844][-4.4287429 -4.4287343 -4.4287395 -4.4287324 -4.4287152 -4.4287195 -4.4287477 -4.4287739 -4.4288 -4.4288135 -4.4288054 -4.4287958 -4.4287829 -4.4287724 -4.4287724]]...]
INFO - root - 2017-12-08 10:52:19.943815: step 4910, loss = 2.28, batch loss = 2.23 (17.0 examples/sec; 0.470 sec/batch; 42h:44m:54s remains)
INFO - root - 2017-12-08 10:52:24.464999: step 4920, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.464 sec/batch; 42h:12m:04s remains)
INFO - root - 2017-12-08 10:52:28.948854: step 4930, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:13m:06s remains)
INFO - root - 2017-12-08 10:52:33.461726: step 4940, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:05m:16s remains)
INFO - root - 2017-12-08 10:52:37.930960: step 4950, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.461 sec/batch; 41h:58m:52s remains)
INFO - root - 2017-12-08 10:52:42.374470: step 4960, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:18m:35s remains)
INFO - root - 2017-12-08 10:52:46.579829: step 4970, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:01m:14s remains)
INFO - root - 2017-12-08 10:52:50.994895: step 4980, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:13m:53s remains)
INFO - root - 2017-12-08 10:52:55.461025: step 4990, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.460 sec/batch; 41h:52m:38s remains)
INFO - root - 2017-12-08 10:52:59.938893: step 5000, loss = 2.28, batch loss = 2.23 (17.0 examples/sec; 0.471 sec/batch; 42h:48m:43s remains)
2017-12-08 10:53:00.572574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287643 -4.4287939 -4.42878 -4.42872 -4.4286466 -4.4286036 -4.4286137 -4.4286451 -4.4286804 -4.4287305 -4.4287906 -4.4288373 -4.42886 -4.428874 -4.4288683][-4.428772 -4.4288106 -4.4287972 -4.4287295 -4.4286456 -4.4285927 -4.4286017 -4.4286375 -4.4286656 -4.4287105 -4.4287691 -4.4288154 -4.4288392 -4.4288549 -4.428854][-4.4287658 -4.4288149 -4.4288054 -4.4287367 -4.4286537 -4.4285946 -4.4285908 -4.4286141 -4.4286423 -4.4286909 -4.428751 -4.4287977 -4.4288254 -4.4288464 -4.4288554][-4.4287362 -4.4287992 -4.4288034 -4.4287515 -4.4286809 -4.4286151 -4.42858 -4.4285736 -4.4286122 -4.4286704 -4.4287329 -4.4287825 -4.4288154 -4.4288449 -4.428865][-4.4287329 -4.428812 -4.4288263 -4.4287834 -4.4287152 -4.4286327 -4.42855 -4.4285 -4.4285522 -4.42863 -4.4286971 -4.4287491 -4.4287944 -4.4288373 -4.4288712][-4.4287591 -4.4288335 -4.4288468 -4.4288006 -4.4287205 -4.4286032 -4.4284539 -4.4283524 -4.428432 -4.4285536 -4.4286404 -4.4287024 -4.4287677 -4.4288359 -4.4288826][-4.4287615 -4.4288187 -4.4288254 -4.4287758 -4.4286771 -4.428514 -4.4282846 -4.42813 -4.428266 -4.4284606 -4.4285831 -4.4286542 -4.4287329 -4.428822 -4.4288793][-4.4287491 -4.4287906 -4.4287887 -4.4287357 -4.4286265 -4.4284406 -4.4281588 -4.4279804 -4.42818 -4.4284296 -4.4285655 -4.4286213 -4.4286828 -4.4287715 -4.4288421][-4.4287443 -4.4287667 -4.4287643 -4.428721 -4.4286332 -4.4284897 -4.4282622 -4.4281349 -4.4283252 -4.4285393 -4.4286351 -4.4286504 -4.4286737 -4.4287443 -4.428812][-4.4287639 -4.4287744 -4.4287724 -4.4287419 -4.4286857 -4.4286032 -4.4284577 -4.4283805 -4.4285159 -4.4286633 -4.4287057 -4.4286838 -4.4286809 -4.4287448 -4.4288082][-4.4287887 -4.4287863 -4.42879 -4.4287777 -4.4287477 -4.4287066 -4.4286132 -4.4285593 -4.4286456 -4.4287367 -4.4287415 -4.4286928 -4.428669 -4.4287229 -4.4287834][-4.4288 -4.4287977 -4.428812 -4.4288177 -4.4288058 -4.4287872 -4.4287286 -4.4286885 -4.4287238 -4.4287634 -4.4287505 -4.4287014 -4.4286723 -4.4287086 -4.4287543][-4.42883 -4.4288316 -4.4288483 -4.4288545 -4.4288435 -4.42883 -4.428791 -4.4287577 -4.428761 -4.428772 -4.4287567 -4.4287233 -4.4286971 -4.4287081 -4.4287295][-4.4288783 -4.4288807 -4.4288912 -4.4288869 -4.4288716 -4.4288568 -4.4288297 -4.4288034 -4.4287944 -4.42879 -4.4287763 -4.4287543 -4.4287314 -4.4287257 -4.4287229][-4.4289117 -4.4289165 -4.42892 -4.4289064 -4.428884 -4.4288669 -4.428843 -4.428822 -4.4288116 -4.4288025 -4.4287939 -4.4287853 -4.4287744 -4.4287639 -4.4287534]]...]
INFO - root - 2017-12-08 10:53:05.147035: step 5010, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:25m:37s remains)
INFO - root - 2017-12-08 10:53:09.631788: step 5020, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 39h:21m:10s remains)
INFO - root - 2017-12-08 10:53:14.099263: step 5030, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.463 sec/batch; 42h:05m:02s remains)
INFO - root - 2017-12-08 10:53:18.587235: step 5040, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.456 sec/batch; 41h:29m:20s remains)
INFO - root - 2017-12-08 10:53:23.182497: step 5050, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 39h:15m:42s remains)
INFO - root - 2017-12-08 10:53:27.601932: step 5060, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:41m:06s remains)
INFO - root - 2017-12-08 10:53:31.779011: step 5070, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:15m:54s remains)
INFO - root - 2017-12-08 10:53:36.306537: step 5080, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 40h:57m:26s remains)
INFO - root - 2017-12-08 10:53:40.850404: step 5090, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:15m:11s remains)
INFO - root - 2017-12-08 10:53:45.360526: step 5100, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:06m:50s remains)
2017-12-08 10:53:45.820279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287486 -4.4287419 -4.4287457 -4.4287558 -4.4287496 -4.4287343 -4.4287357 -4.4287415 -4.4287419 -4.4287338 -4.4287176 -4.4286928 -4.428658 -4.428648 -4.4286809][-4.4287262 -4.4287271 -4.4287357 -4.4287496 -4.4287496 -4.4287415 -4.4287448 -4.4287453 -4.4287381 -4.4287095 -4.428669 -4.4286375 -4.428617 -4.4286284 -4.4286761][-4.4287167 -4.4287229 -4.4287329 -4.4287486 -4.4287515 -4.4287491 -4.4287515 -4.4287438 -4.4287257 -4.4286795 -4.4286227 -4.4285922 -4.4285941 -4.4286327 -4.4286909][-4.4287181 -4.4287357 -4.4287477 -4.42876 -4.4287596 -4.4287577 -4.4287567 -4.4287434 -4.4287157 -4.4286623 -4.42861 -4.4285893 -4.4286084 -4.4286628 -4.4287238][-4.4287267 -4.4287562 -4.4287686 -4.4287782 -4.4287763 -4.428771 -4.4287567 -4.4287262 -4.4286947 -4.4286551 -4.428628 -4.4286184 -4.4286385 -4.4286919 -4.4287496][-4.4287252 -4.4287653 -4.428781 -4.4287839 -4.4287686 -4.4287424 -4.4286995 -4.4286523 -4.4286346 -4.4286265 -4.4286332 -4.4286408 -4.4286618 -4.4287057 -4.4287539][-4.4287219 -4.4287667 -4.428781 -4.428771 -4.4287248 -4.4286485 -4.4285569 -4.4285026 -4.4285131 -4.4285479 -4.4285908 -4.4286246 -4.4286561 -4.4286938 -4.4287286][-4.4287171 -4.4287591 -4.4287624 -4.428731 -4.4286408 -4.4284968 -4.4283524 -4.4282956 -4.4283438 -4.4284272 -4.4285059 -4.4285665 -4.4286108 -4.4286456 -4.4286671][-4.428721 -4.4287524 -4.42874 -4.4286747 -4.4285278 -4.4283214 -4.4281454 -4.4281082 -4.4282036 -4.428328 -4.4284282 -4.4285 -4.4285483 -4.4285779 -4.4285889][-4.4287353 -4.4287496 -4.428721 -4.4286251 -4.4284439 -4.4282184 -4.42807 -4.4280849 -4.428206 -4.4283242 -4.4284091 -4.4284711 -4.4285135 -4.428535 -4.4285374][-4.4287572 -4.4287572 -4.4287128 -4.4286017 -4.4284291 -4.4282527 -4.4281731 -4.4282331 -4.4283462 -4.4284239 -4.4284725 -4.4285097 -4.428535 -4.4285493 -4.4285493][-4.4287925 -4.4287825 -4.4287262 -4.4286137 -4.4284697 -4.4283557 -4.4283342 -4.4284105 -4.4285054 -4.4285517 -4.4285731 -4.4285889 -4.4286036 -4.4286184 -4.4286251][-4.4288163 -4.428793 -4.4287276 -4.4286184 -4.4285007 -4.4284344 -4.4284515 -4.4285293 -4.4286036 -4.4286366 -4.42865 -4.4286613 -4.4286757 -4.4286933 -4.4287071][-4.4288163 -4.4287758 -4.4286938 -4.4285793 -4.4284782 -4.4284487 -4.4284945 -4.4285731 -4.4286327 -4.4286628 -4.4286823 -4.4287019 -4.4287205 -4.4287415 -4.428761][-4.4287982 -4.4287329 -4.4286232 -4.4284978 -4.4284124 -4.4284172 -4.4284945 -4.4285765 -4.4286304 -4.4286633 -4.4286895 -4.4287157 -4.4287391 -4.4287615 -4.4287839]]...]
INFO - root - 2017-12-08 10:53:50.245976: step 5110, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.448 sec/batch; 40h:44m:41s remains)
INFO - root - 2017-12-08 10:53:54.608565: step 5120, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:54m:25s remains)
INFO - root - 2017-12-08 10:53:59.005706: step 5130, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 39h:15m:40s remains)
INFO - root - 2017-12-08 10:54:03.360853: step 5140, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.427 sec/batch; 38h:47m:35s remains)
INFO - root - 2017-12-08 10:54:07.720569: step 5150, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:29m:41s remains)
INFO - root - 2017-12-08 10:54:12.106712: step 5160, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:06m:20s remains)
INFO - root - 2017-12-08 10:54:16.286636: step 5170, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:28m:25s remains)
INFO - root - 2017-12-08 10:54:20.737955: step 5180, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:25m:53s remains)
INFO - root - 2017-12-08 10:54:25.174515: step 5190, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:35m:23s remains)
INFO - root - 2017-12-08 10:54:29.578656: step 5200, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 40h:01m:05s remains)
2017-12-08 10:54:30.080879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288912 -4.4288764 -4.4288645 -4.4288564 -4.4288549 -4.4288568 -4.4288578 -4.4288573 -4.4288568 -4.4288559 -4.4288573 -4.4288592 -4.4288583 -4.428853 -4.4288464][-4.42891 -4.4288955 -4.4288816 -4.4288688 -4.4288621 -4.42886 -4.4288611 -4.4288621 -4.4288626 -4.4288621 -4.4288592 -4.4288549 -4.4288492 -4.428843 -4.4288378][-4.4289103 -4.4289055 -4.4288921 -4.4288735 -4.4288592 -4.4288535 -4.4288592 -4.4288716 -4.4288807 -4.4288831 -4.4288778 -4.4288688 -4.4288578 -4.4288473 -4.4288397][-4.4288659 -4.428884 -4.4288769 -4.4288487 -4.4288197 -4.4288039 -4.4288173 -4.4288492 -4.4288812 -4.4288993 -4.4288993 -4.4288893 -4.428875 -4.4288597 -4.4288478][-4.4287744 -4.4288249 -4.4288287 -4.4287877 -4.4287353 -4.4287009 -4.4287138 -4.4287691 -4.4288354 -4.4288864 -4.4289079 -4.428906 -4.4288917 -4.4288735 -4.4288578][-4.4286618 -4.4287529 -4.4287715 -4.4287143 -4.4286222 -4.4285483 -4.428545 -4.4286184 -4.4287267 -4.4288239 -4.4288831 -4.4289021 -4.4288983 -4.4288821 -4.428864][-4.4285593 -4.4286933 -4.4287305 -4.4286647 -4.4285321 -4.4283948 -4.4283433 -4.4284186 -4.428565 -4.4287114 -4.4288163 -4.4288673 -4.4288812 -4.428875 -4.4288621][-4.4285207 -4.4286704 -4.4287238 -4.4286647 -4.428515 -4.4283175 -4.4281888 -4.428236 -4.4284043 -4.4285874 -4.4287295 -4.4288135 -4.4288487 -4.4288545 -4.4288507][-4.4285936 -4.4287071 -4.4287543 -4.4287095 -4.4285789 -4.4283886 -4.4282236 -4.4282007 -4.4283333 -4.4285159 -4.42867 -4.4287724 -4.4288249 -4.4288411 -4.428843][-4.4287205 -4.4287772 -4.4287972 -4.4287615 -4.4286714 -4.428544 -4.4284286 -4.4283834 -4.4284372 -4.4285579 -4.4286838 -4.42878 -4.4288354 -4.4288549 -4.4288549][-4.4288225 -4.4288363 -4.4288297 -4.4287972 -4.4287443 -4.4286847 -4.4286323 -4.4286013 -4.42861 -4.4286628 -4.4287381 -4.4288135 -4.4288673 -4.4288898 -4.4288859][-4.4288645 -4.428854 -4.4288282 -4.4287963 -4.4287677 -4.4287515 -4.4287462 -4.4287353 -4.4287243 -4.4287338 -4.4287744 -4.4288354 -4.42889 -4.4289179 -4.4289169][-4.4288645 -4.4288325 -4.4287891 -4.4287539 -4.4287424 -4.4287519 -4.4287667 -4.4287591 -4.4287305 -4.4287133 -4.4287357 -4.4287982 -4.4288678 -4.4289141 -4.4289255][-4.4288445 -4.4287944 -4.4287405 -4.4287081 -4.4287128 -4.4287329 -4.4287386 -4.4287028 -4.4286356 -4.4285855 -4.4286027 -4.4286823 -4.428782 -4.4288588 -4.4288964][-4.4288139 -4.4287572 -4.4287095 -4.4286938 -4.4287167 -4.4287419 -4.4287281 -4.428648 -4.4285173 -4.42841 -4.4284072 -4.4285088 -4.4286456 -4.4287605 -4.4288321]]...]
INFO - root - 2017-12-08 10:54:34.491944: step 5210, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:10m:28s remains)
INFO - root - 2017-12-08 10:54:38.891640: step 5220, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:29m:55s remains)
INFO - root - 2017-12-08 10:54:43.413719: step 5230, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.467 sec/batch; 42h:29m:28s remains)
INFO - root - 2017-12-08 10:54:47.867843: step 5240, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 39h:04m:08s remains)
INFO - root - 2017-12-08 10:54:52.293749: step 5250, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.461 sec/batch; 41h:54m:15s remains)
INFO - root - 2017-12-08 10:54:56.807555: step 5260, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.441 sec/batch; 40h:05m:03s remains)
INFO - root - 2017-12-08 10:55:00.992929: step 5270, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:32m:43s remains)
INFO - root - 2017-12-08 10:55:05.401914: step 5280, loss = 2.28, batch loss = 2.23 (19.0 examples/sec; 0.420 sec/batch; 38h:13m:09s remains)
INFO - root - 2017-12-08 10:55:09.793797: step 5290, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:37m:52s remains)
INFO - root - 2017-12-08 10:55:14.186117: step 5300, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 40h:57m:55s remains)
2017-12-08 10:55:14.704754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288836 -4.42889 -4.4288955 -4.42889 -4.42887 -4.4288478 -4.4288368 -4.4288383 -4.4288445 -4.428853 -4.4288564 -4.4288659 -4.428885 -4.428906 -4.428925][-4.4288368 -4.4288516 -4.428863 -4.42886 -4.4288354 -4.4288015 -4.4287834 -4.4287815 -4.4287872 -4.4287934 -4.428792 -4.428803 -4.4288292 -4.4288607 -4.4288859][-4.4287591 -4.4287815 -4.428803 -4.4288044 -4.4287758 -4.4287281 -4.428699 -4.4286957 -4.4287062 -4.4287133 -4.428709 -4.4287205 -4.4287472 -4.4287815 -4.4288077][-4.4286628 -4.4286919 -4.4287262 -4.4287357 -4.4287 -4.4286327 -4.4285879 -4.4285846 -4.428606 -4.4286237 -4.4286237 -4.4286337 -4.4286561 -4.4286861 -4.42871][-4.4285765 -4.4286094 -4.4286537 -4.4286709 -4.428628 -4.4285407 -4.4284811 -4.4284787 -4.428515 -4.4285493 -4.4285617 -4.4285746 -4.4285922 -4.428617 -4.4286356][-4.4285145 -4.4285469 -4.428596 -4.4286132 -4.4285588 -4.4284511 -4.428371 -4.4283657 -4.4284196 -4.4284797 -4.428514 -4.4285393 -4.42856 -4.4285789 -4.4285913][-4.428484 -4.4285197 -4.4285674 -4.4285755 -4.4285021 -4.4283662 -4.4282618 -4.4282532 -4.4283266 -4.4284172 -4.428484 -4.4285259 -4.4285507 -4.4285641 -4.4285707][-4.4284945 -4.4285345 -4.4285803 -4.4285755 -4.4284849 -4.4283304 -4.4282117 -4.4282031 -4.42829 -4.4284043 -4.4284897 -4.4285383 -4.4285631 -4.4285736 -4.4285817][-4.4285126 -4.4285531 -4.4285975 -4.4285893 -4.4285007 -4.4283566 -4.4282513 -4.4282479 -4.42833 -4.4284334 -4.4285088 -4.4285469 -4.4285665 -4.4285803 -4.4286013][-4.4285131 -4.4285531 -4.4285946 -4.4285913 -4.4285207 -4.4284124 -4.4283414 -4.4283433 -4.4284034 -4.4284754 -4.4285221 -4.4285421 -4.4285588 -4.4285831 -4.4286208][-4.4285092 -4.428544 -4.4285812 -4.4285841 -4.4285359 -4.4284654 -4.4284267 -4.4284315 -4.4284711 -4.4285131 -4.4285359 -4.4285436 -4.428565 -4.4286041 -4.4286571][-4.4285069 -4.4285364 -4.4285669 -4.4285703 -4.42854 -4.4284992 -4.4284835 -4.428494 -4.428524 -4.4285483 -4.4285588 -4.428566 -4.4285941 -4.4286447 -4.4287047][-4.4285183 -4.4285388 -4.428556 -4.428555 -4.4285355 -4.4285131 -4.4285135 -4.4285331 -4.4285626 -4.4285789 -4.4285851 -4.428597 -4.4286332 -4.4286895 -4.4287467][-4.428544 -4.4285522 -4.4285541 -4.428546 -4.4285316 -4.4285231 -4.4285359 -4.4285641 -4.4285955 -4.428607 -4.4286084 -4.4286237 -4.4286623 -4.4287176 -4.4287648][-4.4285631 -4.4285631 -4.428555 -4.4285445 -4.4285355 -4.4285369 -4.4285564 -4.4285855 -4.4286146 -4.4286246 -4.4286246 -4.4286437 -4.4286842 -4.428731 -4.4287639]]...]
INFO - root - 2017-12-08 10:55:19.214252: step 5310, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.464 sec/batch; 42h:08m:57s remains)
INFO - root - 2017-12-08 10:55:23.738095: step 5320, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 40h:55m:25s remains)
INFO - root - 2017-12-08 10:55:28.212059: step 5330, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 39h:53m:36s remains)
INFO - root - 2017-12-08 10:55:32.608037: step 5340, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:53m:15s remains)
INFO - root - 2017-12-08 10:55:37.010898: step 5350, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:25m:31s remains)
INFO - root - 2017-12-08 10:55:41.390219: step 5360, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:36m:12s remains)
INFO - root - 2017-12-08 10:55:45.762848: step 5370, loss = 2.28, batch loss = 2.23 (16.8 examples/sec; 0.477 sec/batch; 43h:19m:24s remains)
INFO - root - 2017-12-08 10:55:50.328924: step 5380, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 39h:58m:03s remains)
INFO - root - 2017-12-08 10:55:54.838791: step 5390, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:14m:38s remains)
INFO - root - 2017-12-08 10:55:59.302962: step 5400, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:14m:12s remains)
2017-12-08 10:55:59.804133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288216 -4.42879 -4.4287391 -4.4286938 -4.4286914 -4.4287043 -4.4287148 -4.4287381 -4.4287891 -4.4288325 -4.428833 -4.4287915 -4.4287486 -4.4287505 -4.4287958][-4.4287834 -4.4287581 -4.4287004 -4.4286408 -4.4286213 -4.4286261 -4.428637 -4.4286666 -4.4287376 -4.4288092 -4.4288325 -4.4288015 -4.4287558 -4.4287448 -4.4287772][-4.4287391 -4.4287319 -4.428689 -4.4286308 -4.428587 -4.4285688 -4.4285727 -4.428597 -4.4286709 -4.428762 -4.4288168 -4.4288125 -4.428781 -4.4287643 -4.4287782][-4.4286757 -4.4286785 -4.428658 -4.428618 -4.4285665 -4.4285254 -4.4285083 -4.428504 -4.4285612 -4.428669 -4.4287596 -4.4287925 -4.4287896 -4.4287825 -4.4287848][-4.4286032 -4.4286041 -4.4286 -4.428576 -4.428524 -4.4284625 -4.4283967 -4.4283381 -4.428369 -4.4285 -4.4286337 -4.4287138 -4.4287462 -4.4287643 -4.4287777][-4.4285536 -4.4285388 -4.42853 -4.4285059 -4.4284449 -4.4283547 -4.42823 -4.4280972 -4.4281 -4.42826 -4.428443 -4.4285688 -4.4286408 -4.4286971 -4.4287367][-4.4285412 -4.4285131 -4.4284878 -4.4284444 -4.4283686 -4.428257 -4.428093 -4.4279089 -4.4278793 -4.4280443 -4.4282446 -4.4283957 -4.4284945 -4.4285822 -4.4286494][-4.4285564 -4.4285288 -4.4284987 -4.4284434 -4.4283624 -4.4282527 -4.4280949 -4.4279251 -4.4278793 -4.42799 -4.4281425 -4.4282713 -4.4283705 -4.4284654 -4.4285502][-4.4285851 -4.4285655 -4.4285421 -4.42849 -4.4284172 -4.428328 -4.4282041 -4.4280772 -4.4280291 -4.4280844 -4.428174 -4.4282537 -4.4283237 -4.4284043 -4.4284854][-4.4286151 -4.4286051 -4.4285941 -4.42856 -4.4285083 -4.4284492 -4.4283714 -4.42829 -4.4282556 -4.4282823 -4.4283247 -4.4283557 -4.4283876 -4.4284396 -4.4285021][-4.4286718 -4.4286685 -4.4286661 -4.4286523 -4.4286265 -4.4285946 -4.4285545 -4.4285111 -4.428493 -4.4285131 -4.4285321 -4.4285316 -4.4285326 -4.42856 -4.4286][-4.4287567 -4.4287543 -4.4287548 -4.4287496 -4.4287376 -4.4287214 -4.4287009 -4.4286819 -4.4286771 -4.4286933 -4.4287009 -4.4286919 -4.4286828 -4.4286952 -4.4287176][-4.4288292 -4.4288235 -4.428823 -4.4288187 -4.4288125 -4.4288049 -4.4287949 -4.4287863 -4.4287858 -4.4287944 -4.4287982 -4.4287925 -4.4287882 -4.4287949 -4.4288054][-4.4288669 -4.4288597 -4.4288588 -4.4288568 -4.428854 -4.4288487 -4.428843 -4.4288383 -4.4288363 -4.42884 -4.4288425 -4.428843 -4.4288454 -4.4288511 -4.4288549][-4.4288874 -4.4288807 -4.42888 -4.4288816 -4.4288821 -4.4288793 -4.4288745 -4.4288692 -4.4288669 -4.4288678 -4.4288707 -4.4288735 -4.4288778 -4.4288821 -4.428884]]...]
INFO - root - 2017-12-08 10:56:04.273954: step 5410, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.429 sec/batch; 38h:59m:52s remains)
INFO - root - 2017-12-08 10:56:08.811360: step 5420, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:12m:32s remains)
INFO - root - 2017-12-08 10:56:13.356890: step 5430, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 40h:51m:52s remains)
INFO - root - 2017-12-08 10:56:17.893817: step 5440, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:06m:16s remains)
INFO - root - 2017-12-08 10:56:22.364891: step 5450, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.461 sec/batch; 41h:51m:06s remains)
INFO - root - 2017-12-08 10:56:26.762686: step 5460, loss = 2.28, batch loss = 2.23 (26.1 examples/sec; 0.306 sec/batch; 27h:47m:45s remains)
INFO - root - 2017-12-08 10:56:31.155825: step 5470, loss = 2.28, batch loss = 2.23 (16.9 examples/sec; 0.475 sec/batch; 43h:07m:37s remains)
INFO - root - 2017-12-08 10:56:35.691374: step 5480, loss = 2.28, batch loss = 2.23 (16.7 examples/sec; 0.479 sec/batch; 43h:29m:05s remains)
INFO - root - 2017-12-08 10:56:40.097152: step 5490, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:34m:11s remains)
INFO - root - 2017-12-08 10:56:44.582496: step 5500, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 40h:46m:26s remains)
2017-12-08 10:56:45.083581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287004 -4.4287066 -4.4287076 -4.4287076 -4.428719 -4.4287195 -4.4287171 -4.4287167 -4.4287343 -4.4287534 -4.4287624 -4.4287777 -4.4287882 -4.4287806 -4.4287715][-4.4287019 -4.4287043 -4.4287057 -4.4287095 -4.4287238 -4.4287243 -4.428719 -4.4287138 -4.4287267 -4.4287424 -4.4287434 -4.4287567 -4.4287715 -4.4287739 -4.4287729][-4.4287009 -4.4286981 -4.4287066 -4.4287219 -4.4287362 -4.4287357 -4.4287353 -4.4287381 -4.42875 -4.428762 -4.4287496 -4.4287467 -4.428762 -4.4287791 -4.4287915][-4.4286909 -4.4286809 -4.4286952 -4.4287181 -4.4287248 -4.4287214 -4.4287267 -4.4287376 -4.4287515 -4.4287624 -4.4287472 -4.4287286 -4.4287367 -4.4287605 -4.4287848][-4.4286852 -4.4286633 -4.4286666 -4.4286857 -4.4286809 -4.428659 -4.4286594 -4.4286857 -4.4287152 -4.4287362 -4.4287281 -4.4287043 -4.4287038 -4.4287195 -4.4287362][-4.4286466 -4.4286137 -4.4285984 -4.4286 -4.4285679 -4.4284973 -4.4284649 -4.428535 -4.428627 -4.4286852 -4.4286966 -4.4286761 -4.4286723 -4.4286766 -4.4286776][-4.4285665 -4.4285107 -4.4284654 -4.428441 -4.4283767 -4.4282227 -4.4281034 -4.4282207 -4.4284225 -4.4285583 -4.4286118 -4.4286089 -4.4286218 -4.4286389 -4.4286418][-4.4285188 -4.4284515 -4.4283814 -4.4283442 -4.4282832 -4.4280872 -4.4278746 -4.4279795 -4.428256 -4.4284568 -4.4285483 -4.4285727 -4.4286089 -4.4286528 -4.4286666][-4.428556 -4.4285011 -4.4284425 -4.4284205 -4.4284043 -4.4282751 -4.4280925 -4.4281259 -4.4283247 -4.4284873 -4.4285688 -4.4286113 -4.4286661 -4.4287167 -4.4287229][-4.4286041 -4.4285908 -4.4285622 -4.428556 -4.4285665 -4.4285083 -4.4283996 -4.4283929 -4.4284816 -4.428565 -4.4286137 -4.4286585 -4.4287243 -4.4287744 -4.4287715][-4.4286218 -4.4286346 -4.4286332 -4.4286432 -4.4286604 -4.4286385 -4.4285822 -4.4285703 -4.4286032 -4.4286332 -4.4286389 -4.4286561 -4.4287219 -4.428782 -4.4287934][-4.4286189 -4.4286423 -4.4286556 -4.4286685 -4.4286833 -4.428678 -4.4286528 -4.4286447 -4.4286447 -4.4286432 -4.4286103 -4.4285917 -4.4286461 -4.4287214 -4.4287648][-4.4285889 -4.4286132 -4.4286423 -4.4286618 -4.4286795 -4.4286838 -4.4286795 -4.42868 -4.4286623 -4.4286327 -4.4285736 -4.4285293 -4.4285722 -4.4286518 -4.4287119][-4.4285555 -4.4285827 -4.4286208 -4.4286432 -4.4286613 -4.4286776 -4.4286909 -4.4287019 -4.4286833 -4.4286427 -4.4285812 -4.4285369 -4.428576 -4.4286466 -4.4286933][-4.4285836 -4.4286084 -4.4286404 -4.4286556 -4.4286695 -4.4286838 -4.4286933 -4.4287028 -4.4286966 -4.4286704 -4.4286304 -4.4286008 -4.428628 -4.42867 -4.4286928]]...]
INFO - root - 2017-12-08 10:56:49.552715: step 5510, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.448 sec/batch; 40h:43m:35s remains)
INFO - root - 2017-12-08 10:56:54.105269: step 5520, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 41h:08m:24s remains)
INFO - root - 2017-12-08 10:56:58.556278: step 5530, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 39h:05m:10s remains)
INFO - root - 2017-12-08 10:57:03.015679: step 5540, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:29m:47s remains)
INFO - root - 2017-12-08 10:57:07.432551: step 5550, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.461 sec/batch; 41h:52m:17s remains)
INFO - root - 2017-12-08 10:57:11.687664: step 5560, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 21h:02m:45s remains)
INFO - root - 2017-12-08 10:57:16.142401: step 5570, loss = 2.28, batch loss = 2.23 (16.9 examples/sec; 0.474 sec/batch; 43h:02m:09s remains)
INFO - root - 2017-12-08 10:57:20.735908: step 5580, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:30m:38s remains)
INFO - root - 2017-12-08 10:57:25.276789: step 5590, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:20m:36s remains)
INFO - root - 2017-12-08 10:57:29.844291: step 5600, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 39h:58m:37s remains)
2017-12-08 10:57:30.342112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287515 -4.4287152 -4.4286928 -4.4286847 -4.4286923 -4.4287086 -4.4287167 -4.4287133 -4.4286985 -4.428679 -4.4286656 -4.42867 -4.4286957 -4.4287205 -4.4287386][-4.4287062 -4.4286661 -4.4286423 -4.4286389 -4.4286494 -4.4286609 -4.4286633 -4.428659 -4.4286528 -4.4286447 -4.42864 -4.428648 -4.4286757 -4.4287081 -4.4287329][-4.4286842 -4.428648 -4.4286313 -4.4286356 -4.4286447 -4.4286442 -4.4286361 -4.42863 -4.4286284 -4.4286342 -4.4286389 -4.4286375 -4.4286523 -4.4286819 -4.4287066][-4.4286671 -4.4286304 -4.4286103 -4.4286084 -4.4286079 -4.4285989 -4.4285917 -4.4285913 -4.4285965 -4.4286156 -4.4286232 -4.428606 -4.4285965 -4.4286146 -4.4286375][-4.4286327 -4.4285827 -4.4285545 -4.428546 -4.4285393 -4.4285307 -4.4285274 -4.4285264 -4.4285297 -4.4285522 -4.428556 -4.4285259 -4.4284983 -4.4285016 -4.42852][-4.4285946 -4.4285283 -4.4284878 -4.4284678 -4.4284487 -4.428432 -4.4284124 -4.4283924 -4.4283881 -4.4284163 -4.4284368 -4.4284234 -4.4283972 -4.4283919 -4.4284039][-4.4285736 -4.4285069 -4.4284549 -4.4284167 -4.42838 -4.4283404 -4.4282913 -4.42825 -4.4282422 -4.4282846 -4.4283333 -4.4283524 -4.4283452 -4.4283409 -4.4283533][-4.4285727 -4.4285178 -4.4284706 -4.428422 -4.4283667 -4.4282966 -4.4282141 -4.4281521 -4.4281473 -4.4282074 -4.4282889 -4.42834 -4.4283509 -4.4283495 -4.4283648][-4.42861 -4.4285669 -4.4285254 -4.4284744 -4.4284143 -4.4283423 -4.4282613 -4.4282074 -4.4282074 -4.4282641 -4.4283404 -4.42839 -4.4284053 -4.4284105 -4.4284344][-4.4286695 -4.428638 -4.4286113 -4.428575 -4.4285331 -4.4284873 -4.4284344 -4.4283948 -4.4283824 -4.4284062 -4.4284554 -4.4284959 -4.4285131 -4.4285264 -4.4285541][-4.4287176 -4.4286971 -4.4286847 -4.4286671 -4.4286489 -4.4286284 -4.4286022 -4.428576 -4.4285617 -4.4285669 -4.4285922 -4.4286208 -4.4286342 -4.4286485 -4.428669][-4.4287567 -4.4287429 -4.4287381 -4.4287353 -4.4287343 -4.4287314 -4.4287224 -4.4287076 -4.4286933 -4.4286861 -4.4286909 -4.4287043 -4.4287157 -4.4287314 -4.4287443][-4.428792 -4.4287815 -4.42878 -4.42878 -4.4287844 -4.4287906 -4.42879 -4.4287829 -4.4287748 -4.4287677 -4.4287629 -4.4287663 -4.4287744 -4.4287839 -4.42879][-4.4288349 -4.4288268 -4.4288235 -4.4288225 -4.4288259 -4.428834 -4.4288397 -4.4288387 -4.4288349 -4.4288297 -4.4288254 -4.4288254 -4.4288278 -4.4288321 -4.4288354][-4.4288812 -4.428875 -4.4288707 -4.4288678 -4.4288683 -4.4288735 -4.42888 -4.4288845 -4.4288855 -4.4288821 -4.4288783 -4.4288778 -4.4288797 -4.4288816 -4.4288831]]...]
INFO - root - 2017-12-08 10:57:34.932078: step 5610, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 40h:59m:46s remains)
INFO - root - 2017-12-08 10:57:39.488120: step 5620, loss = 2.28, batch loss = 2.23 (16.7 examples/sec; 0.479 sec/batch; 43h:27m:37s remains)
INFO - root - 2017-12-08 10:57:43.980367: step 5630, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.458 sec/batch; 41h:35m:15s remains)
INFO - root - 2017-12-08 10:57:48.497218: step 5640, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:31m:26s remains)
INFO - root - 2017-12-08 10:57:53.019260: step 5650, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.466 sec/batch; 42h:18m:32s remains)
INFO - root - 2017-12-08 10:57:57.185726: step 5660, loss = 2.28, batch loss = 2.23 (25.6 examples/sec; 0.312 sec/batch; 28h:20m:29s remains)
INFO - root - 2017-12-08 10:58:01.597347: step 5670, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 39h:45m:56s remains)
INFO - root - 2017-12-08 10:58:06.026033: step 5680, loss = 2.28, batch loss = 2.23 (19.5 examples/sec; 0.409 sec/batch; 37h:09m:02s remains)
INFO - root - 2017-12-08 10:58:10.479797: step 5690, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:16m:26s remains)
INFO - root - 2017-12-08 10:58:14.915020: step 5700, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 39h:37m:29s remains)
2017-12-08 10:58:15.458758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288416 -4.4288445 -4.4288449 -4.4288526 -4.4288664 -4.428884 -4.4289 -4.428906 -4.4289055 -4.4289002 -4.4289002 -4.4289079 -4.4289193 -4.4289289 -4.4289317][-4.428771 -4.4287667 -4.4287586 -4.428762 -4.4287786 -4.4288025 -4.428822 -4.428833 -4.4288387 -4.4288344 -4.4288354 -4.4288464 -4.428863 -4.4288759 -4.4288793][-4.4287176 -4.4287071 -4.4286895 -4.4286852 -4.4286995 -4.4287267 -4.42875 -4.4287634 -4.4287772 -4.4287729 -4.4287677 -4.4287763 -4.4287939 -4.4288087 -4.4288092][-4.4286714 -4.428659 -4.4286385 -4.4286246 -4.4286323 -4.4286561 -4.4286795 -4.4286981 -4.428721 -4.4287195 -4.4287076 -4.4287095 -4.4287267 -4.4287429 -4.42874][-4.4286151 -4.4286041 -4.4285913 -4.428575 -4.4285717 -4.4285836 -4.4286008 -4.4286275 -4.428668 -4.4286828 -4.4286723 -4.4286695 -4.4286838 -4.4286957 -4.4286823][-4.4285722 -4.4285626 -4.4285583 -4.4285421 -4.4285197 -4.4285054 -4.4285078 -4.4285407 -4.428607 -4.4286523 -4.4286637 -4.4286666 -4.4286728 -4.4286661 -4.4286289][-4.4285855 -4.4285707 -4.4285617 -4.4285297 -4.4284711 -4.42841 -4.4283786 -4.4284077 -4.4285049 -4.4285946 -4.428647 -4.4286714 -4.428679 -4.4286594 -4.4285955][-4.4286575 -4.42863 -4.4285994 -4.4285345 -4.4284282 -4.4283123 -4.428226 -4.4282308 -4.4283571 -4.428503 -4.4286089 -4.4286675 -4.428689 -4.4286637 -4.4285722][-4.4287438 -4.4286976 -4.4286408 -4.4285398 -4.4283905 -4.4282212 -4.428071 -4.4280457 -4.4282074 -4.4284139 -4.4285693 -4.4286542 -4.4286819 -4.42865 -4.4285278][-4.4288068 -4.4287491 -4.4286733 -4.4285579 -4.4283872 -4.4281883 -4.4280038 -4.4279728 -4.4281569 -4.4283867 -4.4285512 -4.4286332 -4.4286566 -4.428616 -4.4284778][-4.428833 -4.4287767 -4.4287033 -4.4285941 -4.4284348 -4.4282475 -4.4280863 -4.4280849 -4.4282603 -4.4284568 -4.4285793 -4.4286275 -4.4286332 -4.4285932 -4.4284682][-4.4288363 -4.4287958 -4.4287386 -4.42865 -4.4285288 -4.4283895 -4.4282942 -4.4283242 -4.4284554 -4.4285808 -4.4286437 -4.4286485 -4.4286332 -4.4286 -4.4285059][-4.4288311 -4.4288144 -4.4287796 -4.4287219 -4.4286485 -4.4285688 -4.4285321 -4.4285693 -4.4286447 -4.4287009 -4.4287114 -4.4286838 -4.4286518 -4.4286275 -4.4285736][-4.4288044 -4.4288192 -4.4288149 -4.4287863 -4.4287496 -4.4287114 -4.4287019 -4.4287267 -4.4287596 -4.4287729 -4.4287577 -4.4287238 -4.4286914 -4.4286757 -4.4286518][-4.4287553 -4.4287934 -4.4288187 -4.4288163 -4.4288034 -4.4287882 -4.4287848 -4.4287939 -4.4288039 -4.4288006 -4.4287853 -4.4287643 -4.4287429 -4.4287348 -4.4287276]]...]
INFO - root - 2017-12-08 10:58:19.896162: step 5710, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 41h:03m:02s remains)
INFO - root - 2017-12-08 10:58:24.343945: step 5720, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 40h:46m:23s remains)
INFO - root - 2017-12-08 10:58:28.703946: step 5730, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 38h:48m:01s remains)
INFO - root - 2017-12-08 10:58:33.091168: step 5740, loss = 2.28, batch loss = 2.23 (16.4 examples/sec; 0.489 sec/batch; 44h:21m:05s remains)
INFO - root - 2017-12-08 10:58:37.492254: step 5750, loss = 2.28, batch loss = 2.23 (17.0 examples/sec; 0.471 sec/batch; 42h:42m:23s remains)
INFO - root - 2017-12-08 10:58:41.613938: step 5760, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.416 sec/batch; 37h:44m:43s remains)
INFO - root - 2017-12-08 10:58:46.050006: step 5770, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 40h:55m:50s remains)
INFO - root - 2017-12-08 10:58:50.567246: step 5780, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:30m:53s remains)
INFO - root - 2017-12-08 10:58:55.073164: step 5790, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 40h:36m:23s remains)
INFO - root - 2017-12-08 10:58:59.563222: step 5800, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:19m:15s remains)
2017-12-08 10:59:00.045519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286642 -4.4286466 -4.4286432 -4.4286666 -4.4287095 -4.4287639 -4.4288149 -4.428844 -4.4288425 -4.4288015 -4.4287224 -4.4286542 -4.4286361 -4.4286265 -4.4286351][-4.4286304 -4.4286432 -4.4286723 -4.428709 -4.4287477 -4.4287887 -4.4288282 -4.42885 -4.42885 -4.4288244 -4.4287715 -4.4287138 -4.4286876 -4.4286776 -4.4286814][-4.4286342 -4.4286585 -4.4287024 -4.4287291 -4.4287438 -4.4287605 -4.4287915 -4.4288154 -4.4288282 -4.428823 -4.4287996 -4.4287558 -4.4287238 -4.4287062 -4.4286952][-4.428627 -4.4286432 -4.4286723 -4.4286718 -4.4286466 -4.4286289 -4.4286547 -4.4286971 -4.4287324 -4.4287424 -4.4287477 -4.4287324 -4.42871 -4.4286857 -4.4286518][-4.4285812 -4.4285731 -4.4285712 -4.4285388 -4.4284687 -4.42841 -4.428432 -4.4285073 -4.4285712 -4.4285984 -4.4286318 -4.4286518 -4.4286475 -4.4286227 -4.4285755][-4.4285412 -4.428494 -4.4284463 -4.4283686 -4.4282422 -4.4281182 -4.4281087 -4.4282188 -4.4283328 -4.4284024 -4.4284868 -4.4285665 -4.4286065 -4.4286 -4.4285569][-4.4285488 -4.4284673 -4.428371 -4.4282508 -4.4280753 -4.4278727 -4.427783 -4.4279041 -4.4280725 -4.4281945 -4.42833 -4.4284711 -4.4285645 -4.4285955 -4.4285812][-4.4286017 -4.4285107 -4.4283986 -4.4282751 -4.4281263 -4.4279375 -4.4278035 -4.4278727 -4.4280262 -4.4281464 -4.4282746 -4.4284182 -4.4285259 -4.42858 -4.4285989][-4.4286919 -4.4286165 -4.4285264 -4.4284296 -4.4283319 -4.4282107 -4.4281039 -4.4281087 -4.4281836 -4.4282579 -4.4283462 -4.4284568 -4.42854 -4.4285874 -4.4286218][-4.4287686 -4.4287219 -4.4286637 -4.4285975 -4.42853 -4.4284558 -4.42839 -4.428371 -4.4283895 -4.4284244 -4.4284792 -4.4285502 -4.4286041 -4.4286323 -4.4286618][-4.4288111 -4.42879 -4.4287667 -4.4287429 -4.4287086 -4.4286737 -4.4286408 -4.4286184 -4.42861 -4.4286132 -4.4286342 -4.4286656 -4.4286914 -4.4287057 -4.428721][-4.4288435 -4.4288349 -4.4288263 -4.4288216 -4.4288116 -4.4288058 -4.4288011 -4.4287872 -4.42877 -4.4287572 -4.4287577 -4.4287658 -4.4287748 -4.4287825 -4.4287839][-4.42887 -4.4288616 -4.4288549 -4.4288526 -4.4288497 -4.4288545 -4.4288626 -4.4288621 -4.4288521 -4.42884 -4.4288325 -4.4288287 -4.4288282 -4.4288297 -4.4288316][-4.4288774 -4.4288697 -4.428864 -4.42886 -4.4288578 -4.4288616 -4.4288697 -4.428875 -4.4288745 -4.4288692 -4.4288626 -4.4288568 -4.428854 -4.4288545 -4.4288573][-4.4288774 -4.4288678 -4.428864 -4.4288607 -4.4288578 -4.4288588 -4.428864 -4.4288688 -4.4288697 -4.4288678 -4.4288659 -4.4288654 -4.4288669 -4.4288697 -4.4288745]]...]
INFO - root - 2017-12-08 10:59:04.579692: step 5810, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:26m:46s remains)
INFO - root - 2017-12-08 10:59:09.099493: step 5820, loss = 2.28, batch loss = 2.23 (16.8 examples/sec; 0.475 sec/batch; 43h:08m:12s remains)
INFO - root - 2017-12-08 10:59:13.622147: step 5830, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 39h:52m:12s remains)
INFO - root - 2017-12-08 10:59:18.111403: step 5840, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:11m:20s remains)
INFO - root - 2017-12-08 10:59:22.598170: step 5850, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:10m:29s remains)
INFO - root - 2017-12-08 10:59:26.828438: step 5860, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.439 sec/batch; 39h:49m:32s remains)
INFO - root - 2017-12-08 10:59:31.272073: step 5870, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:27m:02s remains)
INFO - root - 2017-12-08 10:59:35.627046: step 5880, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 39h:02m:00s remains)
INFO - root - 2017-12-08 10:59:40.065594: step 5890, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 39h:10m:49s remains)
INFO - root - 2017-12-08 10:59:44.440739: step 5900, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:23m:06s remains)
2017-12-08 10:59:45.005867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428721 -4.4286675 -4.4286237 -4.4285965 -4.4285855 -4.4286022 -4.4286122 -4.4286132 -4.4286265 -4.4286561 -4.42871 -4.4287906 -4.4288511 -4.4288826 -4.4288816][-4.428761 -4.4287162 -4.4286757 -4.4286432 -4.4286246 -4.42863 -4.4286275 -4.428618 -4.428616 -4.4286332 -4.4286928 -4.4287872 -4.4288559 -4.4288898 -4.4288912][-4.428781 -4.4287515 -4.4287262 -4.4287019 -4.4286833 -4.4286733 -4.4286551 -4.4286404 -4.4286356 -4.4286423 -4.4286909 -4.42877 -4.4288225 -4.4288483 -4.4288445][-4.4287963 -4.4287763 -4.4287605 -4.4287424 -4.4287205 -4.4286952 -4.4286604 -4.4286404 -4.4286284 -4.428617 -4.4286404 -4.4286985 -4.4287424 -4.4287677 -4.4287629][-4.4288116 -4.4287944 -4.4287782 -4.4287558 -4.4287238 -4.4286766 -4.4286237 -4.4285889 -4.4285579 -4.428524 -4.4285259 -4.4285817 -4.4286437 -4.4286876 -4.4286947][-4.4288015 -4.4287925 -4.4287734 -4.4287395 -4.4286933 -4.4286289 -4.4285483 -4.4284811 -4.42843 -4.4283929 -4.4283996 -4.4284697 -4.4285579 -4.428627 -4.4286489][-4.42877 -4.42877 -4.4287434 -4.4287019 -4.4286513 -4.428575 -4.428463 -4.4283605 -4.4282994 -4.428287 -4.4283314 -4.4284215 -4.42853 -4.428616 -4.4286489][-4.4287224 -4.4287324 -4.4286966 -4.4286523 -4.4286103 -4.4285398 -4.4284244 -4.4283104 -4.4282441 -4.4282541 -4.4283285 -4.4284296 -4.4285369 -4.4286227 -4.4286551][-4.4286842 -4.4286914 -4.4286575 -4.4286194 -4.4285941 -4.4285464 -4.4284568 -4.4283648 -4.4283009 -4.4283056 -4.4283814 -4.4284744 -4.428556 -4.4286222 -4.4286456][-4.4286656 -4.4286714 -4.428647 -4.4286189 -4.4286017 -4.4285693 -4.4285121 -4.4284482 -4.4283929 -4.4283895 -4.4284496 -4.4285173 -4.4285707 -4.4286203 -4.4286427][-4.4286752 -4.42868 -4.4286704 -4.4286532 -4.4286385 -4.4286146 -4.4285769 -4.4285359 -4.428493 -4.4284892 -4.4285336 -4.4285779 -4.4286113 -4.4286532 -4.4286795][-4.4287386 -4.4287343 -4.4287305 -4.4287243 -4.4287157 -4.4286976 -4.4286709 -4.4286408 -4.42861 -4.4286103 -4.4286447 -4.4286757 -4.4286995 -4.4287367 -4.4287605][-4.4288168 -4.4288025 -4.4287982 -4.4287992 -4.4287996 -4.4287915 -4.4287753 -4.4287519 -4.4287314 -4.4287324 -4.4287562 -4.4287796 -4.4287977 -4.4288259 -4.4288464][-4.4288816 -4.4288688 -4.4288654 -4.4288673 -4.4288697 -4.4288664 -4.4288573 -4.4288435 -4.4288335 -4.4288344 -4.4288483 -4.428865 -4.4288797 -4.4288988 -4.4289112][-4.428926 -4.42892 -4.42892 -4.4289217 -4.4289222 -4.4289188 -4.4289117 -4.4289041 -4.4289021 -4.4289036 -4.4289117 -4.4289227 -4.4289336 -4.4289451 -4.4289513]]...]
INFO - root - 2017-12-08 10:59:49.435346: step 5910, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:19m:10s remains)
INFO - root - 2017-12-08 10:59:53.803278: step 5920, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.424 sec/batch; 38h:25m:46s remains)
INFO - root - 2017-12-08 10:59:58.150625: step 5930, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 39h:44m:20s remains)
INFO - root - 2017-12-08 11:00:02.532276: step 5940, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 39h:41m:31s remains)
INFO - root - 2017-12-08 11:00:06.928382: step 5950, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:30m:43s remains)
INFO - root - 2017-12-08 11:00:11.132955: step 5960, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:29m:38s remains)
INFO - root - 2017-12-08 11:00:15.689146: step 5970, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:05m:32s remains)
INFO - root - 2017-12-08 11:00:20.202412: step 5980, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 40h:51m:23s remains)
INFO - root - 2017-12-08 11:00:24.697490: step 5990, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:13m:38s remains)
INFO - root - 2017-12-08 11:00:29.150853: step 6000, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:28m:55s remains)
2017-12-08 11:00:29.657799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428678 -4.42871 -4.4287596 -4.4288096 -4.428833 -4.4288311 -4.4288268 -4.4288392 -4.428854 -4.4288497 -4.4288263 -4.4287825 -4.4287176 -4.4286456 -4.42861][-4.4286036 -4.4286361 -4.4286976 -4.4287639 -4.428802 -4.4288144 -4.428813 -4.4288206 -4.4288311 -4.4288263 -4.4288006 -4.428751 -4.4286785 -4.4286113 -4.428596][-4.4285531 -4.4285827 -4.4286456 -4.4287124 -4.4287519 -4.4287734 -4.4287829 -4.4287925 -4.4288 -4.4287992 -4.4287791 -4.4287314 -4.4286628 -4.4286041 -4.4286032][-4.4285517 -4.4285717 -4.4286189 -4.428669 -4.4286909 -4.4287086 -4.4287291 -4.4287472 -4.4287605 -4.4287705 -4.4287658 -4.428719 -4.4286566 -4.4286137 -4.4286275][-4.428596 -4.42861 -4.4286351 -4.4286523 -4.4286418 -4.4286346 -4.4286523 -4.4286742 -4.4286919 -4.4287176 -4.4287386 -4.4287133 -4.4286723 -4.4286475 -4.4286747][-4.4286561 -4.4286642 -4.4286704 -4.4286571 -4.4286103 -4.4285693 -4.4285736 -4.4285927 -4.4286094 -4.4286413 -4.4286904 -4.4287033 -4.4286971 -4.4286942 -4.42873][-4.4286957 -4.4286976 -4.4286966 -4.4286757 -4.4286103 -4.4285426 -4.4285293 -4.4285326 -4.4285264 -4.4285398 -4.4286027 -4.4286547 -4.4286928 -4.4287314 -4.4287825][-4.4286923 -4.4287028 -4.428709 -4.4287009 -4.4286447 -4.4285669 -4.4285221 -4.4284825 -4.4284277 -4.4284019 -4.4284649 -4.42856 -4.4286513 -4.4287376 -4.4288092][-4.4286585 -4.4286833 -4.4287124 -4.4287367 -4.4287062 -4.4286294 -4.4285531 -4.4284563 -4.4283385 -4.428268 -4.4283285 -4.428462 -4.4286003 -4.4287214 -4.4288096][-4.4285932 -4.4286466 -4.4287133 -4.4287825 -4.4287925 -4.42873 -4.4286265 -4.4284883 -4.4283228 -4.4282169 -4.4282665 -4.42842 -4.4285822 -4.4287171 -4.4288058][-4.4285283 -4.428617 -4.4287233 -4.4288239 -4.428864 -4.4288187 -4.428709 -4.4285607 -4.4283886 -4.4282742 -4.4283071 -4.4284434 -4.4285932 -4.4287219 -4.428802][-4.4284768 -4.4285913 -4.4287128 -4.4288206 -4.4288721 -4.4288478 -4.4287534 -4.4286256 -4.4284863 -4.4283876 -4.4283996 -4.4284968 -4.4286113 -4.428719 -4.4287848][-4.4284396 -4.4285564 -4.4286623 -4.4287562 -4.4288168 -4.4288158 -4.4287539 -4.4286652 -4.4285684 -4.4284992 -4.4284992 -4.4285531 -4.4286227 -4.4286995 -4.4287481][-4.4284382 -4.4285207 -4.4285927 -4.4286718 -4.42874 -4.4287586 -4.4287333 -4.4286842 -4.4286265 -4.4285817 -4.4285741 -4.4285941 -4.428628 -4.4286766 -4.4287105][-4.4284616 -4.4285054 -4.42855 -4.4286242 -4.4286914 -4.4287148 -4.4287086 -4.4286861 -4.428658 -4.428638 -4.4286304 -4.4286332 -4.4286509 -4.4286857 -4.4287105]]...]
INFO - root - 2017-12-08 11:00:34.164453: step 6010, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.456 sec/batch; 41h:19m:00s remains)
INFO - root - 2017-12-08 11:00:38.785086: step 6020, loss = 2.28, batch loss = 2.23 (17.1 examples/sec; 0.468 sec/batch; 42h:26m:19s remains)
INFO - root - 2017-12-08 11:00:43.227233: step 6030, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:12m:51s remains)
INFO - root - 2017-12-08 11:00:47.669166: step 6040, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.449 sec/batch; 40h:45m:17s remains)
INFO - root - 2017-12-08 11:00:52.091905: step 6050, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:14m:27s remains)
INFO - root - 2017-12-08 11:00:56.342067: step 6060, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:27m:24s remains)
INFO - root - 2017-12-08 11:01:00.733369: step 6070, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:48m:21s remains)
INFO - root - 2017-12-08 11:01:05.086433: step 6080, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:22m:41s remains)
INFO - root - 2017-12-08 11:01:09.451466: step 6090, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:47m:38s remains)
INFO - root - 2017-12-08 11:01:13.946807: step 6100, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.466 sec/batch; 42h:12m:55s remains)
2017-12-08 11:01:14.503664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289184 -4.4289293 -4.4289451 -4.4289503 -4.4289374 -4.4289122 -4.4288898 -4.4288778 -4.4288812 -4.428896 -4.428916 -4.4289293 -4.4289303 -4.4289165 -4.4289045][-4.4289045 -4.4289122 -4.4289246 -4.4289222 -4.4289 -4.4288635 -4.4288321 -4.4288206 -4.4288306 -4.42885 -4.4288759 -4.4288983 -4.4289017 -4.428884 -4.4288707][-4.4288979 -4.4289026 -4.4289093 -4.4288988 -4.428865 -4.4288168 -4.4287758 -4.428762 -4.4287748 -4.4287996 -4.4288306 -4.4288607 -4.4288673 -4.4288483 -4.42883][-4.4288974 -4.4289017 -4.4289017 -4.4288759 -4.4288235 -4.428762 -4.4287105 -4.4286985 -4.4287219 -4.4287567 -4.4287915 -4.428823 -4.4288292 -4.428802 -4.4287758][-4.4288955 -4.428895 -4.4288864 -4.4288459 -4.428781 -4.4287105 -4.4286494 -4.42864 -4.4286766 -4.4287162 -4.4287472 -4.4287748 -4.428772 -4.4287333 -4.4287014][-4.4288912 -4.4288898 -4.4288721 -4.4288211 -4.4287505 -4.4286752 -4.4286079 -4.4286 -4.4286427 -4.4286852 -4.428709 -4.4287276 -4.4287152 -4.4286633 -4.4286284][-4.428884 -4.42888 -4.4288554 -4.4287963 -4.42872 -4.4286404 -4.4285684 -4.428555 -4.4286022 -4.4286561 -4.4286895 -4.4287124 -4.4286947 -4.4286318 -4.428586][-4.4288769 -4.4288645 -4.4288306 -4.4287639 -4.4286809 -4.4285908 -4.428514 -4.4284978 -4.4285469 -4.4286189 -4.4286804 -4.4287229 -4.4287124 -4.4286489 -4.4285965][-4.4288731 -4.4288526 -4.428812 -4.4287405 -4.4286504 -4.4285536 -4.4284773 -4.4284611 -4.4285083 -4.4285922 -4.4286819 -4.4287405 -4.42874 -4.4286919 -4.42865][-4.42887 -4.4288507 -4.4288077 -4.4287353 -4.4286408 -4.4285426 -4.4284658 -4.4284511 -4.428494 -4.4285769 -4.428669 -4.428731 -4.4287405 -4.4287171 -4.4286928][-4.4288669 -4.4288468 -4.4288049 -4.4287338 -4.4286413 -4.4285436 -4.4284668 -4.4284506 -4.4284921 -4.4285707 -4.4286561 -4.4287109 -4.4287262 -4.4287229 -4.4287128][-4.428864 -4.4288411 -4.4288006 -4.42873 -4.4286413 -4.4285512 -4.428484 -4.4284739 -4.4285178 -4.4285913 -4.4286613 -4.428699 -4.4287124 -4.4287162 -4.4287133][-4.428863 -4.42884 -4.4288034 -4.4287381 -4.4286618 -4.4285946 -4.4285517 -4.4285522 -4.4285903 -4.4286466 -4.4286942 -4.4287152 -4.4287219 -4.4287262 -4.4287291][-4.4288697 -4.4288554 -4.4288292 -4.4287786 -4.4287224 -4.4286823 -4.4286647 -4.4286737 -4.4287019 -4.4287367 -4.4287629 -4.4287696 -4.42877 -4.4287739 -4.4287815][-4.428874 -4.4288669 -4.4288526 -4.4288216 -4.4287887 -4.428772 -4.428771 -4.4287844 -4.4288025 -4.4288197 -4.4288297 -4.4288282 -4.4288259 -4.4288282 -4.4288335]]...]
INFO - root - 2017-12-08 11:01:18.946202: step 6110, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:17m:34s remains)
INFO - root - 2017-12-08 11:01:23.424979: step 6120, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.456 sec/batch; 41h:21m:37s remains)
INFO - root - 2017-12-08 11:01:28.006531: step 6130, loss = 2.28, batch loss = 2.23 (16.8 examples/sec; 0.477 sec/batch; 43h:14m:40s remains)
INFO - root - 2017-12-08 11:01:32.480978: step 6140, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:35m:08s remains)
INFO - root - 2017-12-08 11:01:36.884488: step 6150, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:58m:26s remains)
INFO - root - 2017-12-08 11:01:41.130063: step 6160, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 39h:16m:18s remains)
INFO - root - 2017-12-08 11:01:45.472692: step 6170, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.430 sec/batch; 38h:57m:32s remains)
INFO - root - 2017-12-08 11:01:49.871727: step 6180, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.429 sec/batch; 38h:52m:24s remains)
INFO - root - 2017-12-08 11:01:54.239776: step 6190, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 39h:10m:53s remains)
INFO - root - 2017-12-08 11:01:58.671431: step 6200, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 40h:45m:59s remains)
2017-12-08 11:01:59.215610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428771 -4.42876 -4.4287586 -4.4287505 -4.4287443 -4.4287281 -4.4286819 -4.4286056 -4.4285207 -4.42849 -4.4285579 -4.4286742 -4.4287663 -4.4288292 -4.4288697][-4.4287877 -4.4287848 -4.4287887 -4.4287796 -4.428761 -4.4287267 -4.4286633 -4.4285851 -4.4285145 -4.428494 -4.4285545 -4.4286709 -4.4287686 -4.4288421 -4.4288855][-4.4288154 -4.4288259 -4.4288335 -4.4288235 -4.428791 -4.4287391 -4.4286566 -4.42857 -4.4285083 -4.4284954 -4.4285603 -4.4286737 -4.428772 -4.4288497 -4.4288921][-4.4288459 -4.4288597 -4.4288721 -4.4288611 -4.42881 -4.4287372 -4.428628 -4.4285116 -4.4284563 -4.4284678 -4.4285541 -4.4286733 -4.4287772 -4.42886 -4.4288969][-4.4288645 -4.4288769 -4.4288883 -4.42887 -4.4287982 -4.42869 -4.428535 -4.428391 -4.4283586 -4.4284244 -4.4285483 -4.4286823 -4.4287949 -4.4288716 -4.4288983][-4.428865 -4.4288707 -4.4288769 -4.4288445 -4.4287415 -4.4285865 -4.4283872 -4.4282384 -4.4282622 -4.4284081 -4.4285722 -4.4287157 -4.428823 -4.4288869 -4.4288931][-4.4288387 -4.4288349 -4.4288354 -4.4287858 -4.4286532 -4.4284616 -4.4282494 -4.4281421 -4.4282436 -4.4284415 -4.4286184 -4.4287553 -4.42885 -4.4288983 -4.4288764][-4.4287844 -4.4287739 -4.428771 -4.428709 -4.4285588 -4.4283662 -4.4282007 -4.4281859 -4.4283395 -4.4285274 -4.428679 -4.4287906 -4.4288626 -4.428885 -4.428834][-4.4287262 -4.4287229 -4.4287214 -4.4286528 -4.4285016 -4.4283376 -4.4282508 -4.4283166 -4.4284725 -4.4286194 -4.42874 -4.4288278 -4.4288712 -4.4288597 -4.4287872][-4.42869 -4.4287062 -4.4287038 -4.4286232 -4.4284778 -4.4283528 -4.4283371 -4.4284368 -4.4285641 -4.4286752 -4.4287753 -4.4288487 -4.4288664 -4.4288263 -4.42874][-4.4286923 -4.4287105 -4.4286909 -4.4285994 -4.4284654 -4.4283829 -4.4284215 -4.4285321 -4.4286394 -4.4287276 -4.4288063 -4.4288616 -4.4288549 -4.4287891 -4.4286904][-4.4287057 -4.428709 -4.428669 -4.4285722 -4.4284649 -4.4284272 -4.4284935 -4.4286084 -4.4287114 -4.4287848 -4.4288387 -4.4288692 -4.4288392 -4.4287505 -4.4286423][-4.428719 -4.4287066 -4.4286532 -4.4285674 -4.4284992 -4.4284959 -4.4285665 -4.4286704 -4.4287558 -4.4288073 -4.428834 -4.4288468 -4.4288077 -4.428709 -4.4286][-4.42874 -4.4287133 -4.4286494 -4.4285774 -4.4285436 -4.4285645 -4.4286313 -4.4287143 -4.4287753 -4.4288039 -4.4288139 -4.4288192 -4.4287782 -4.4286842 -4.4285846][-4.4287567 -4.428731 -4.4286718 -4.4286146 -4.4285979 -4.4286227 -4.4286742 -4.4287324 -4.4287763 -4.4287968 -4.4288039 -4.4288039 -4.42876 -4.4286776 -4.4285927]]...]
INFO - root - 2017-12-08 11:02:03.595409: step 6210, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.424 sec/batch; 38h:25m:23s remains)
INFO - root - 2017-12-08 11:02:07.956150: step 6220, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.436 sec/batch; 39h:30m:59s remains)
INFO - root - 2017-12-08 11:02:12.339543: step 6230, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:21m:05s remains)
INFO - root - 2017-12-08 11:02:16.773373: step 6240, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.463 sec/batch; 41h:57m:47s remains)
INFO - root - 2017-12-08 11:02:21.213352: step 6250, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.451 sec/batch; 40h:52m:23s remains)
INFO - root - 2017-12-08 11:02:25.294188: step 6260, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 39h:10m:34s remains)
INFO - root - 2017-12-08 11:02:29.714079: step 6270, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 38h:43m:37s remains)
INFO - root - 2017-12-08 11:02:34.154046: step 6280, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.463 sec/batch; 41h:59m:09s remains)
INFO - root - 2017-12-08 11:02:38.703132: step 6290, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.447 sec/batch; 40h:29m:41s remains)
INFO - root - 2017-12-08 11:02:43.229283: step 6300, loss = 2.28, batch loss = 2.23 (16.9 examples/sec; 0.474 sec/batch; 42h:55m:03s remains)
2017-12-08 11:02:43.730606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286108 -4.4286361 -4.4286613 -4.4286804 -4.4286981 -4.4287186 -4.4287438 -4.4287639 -4.4287667 -4.4287524 -4.4287319 -4.4287276 -4.4287205 -4.4286909 -4.4286623][-4.4286542 -4.4286785 -4.42871 -4.4287333 -4.4287434 -4.4287505 -4.428762 -4.4287672 -4.428762 -4.4287457 -4.4287281 -4.4287252 -4.4287162 -4.428688 -4.4286666][-4.4287219 -4.4287438 -4.4287724 -4.4287887 -4.4287868 -4.4287753 -4.4287624 -4.4287472 -4.4287386 -4.4287319 -4.4287267 -4.4287281 -4.4287167 -4.4286904 -4.4286752][-4.4287453 -4.4287634 -4.4287844 -4.4287915 -4.4287767 -4.4287453 -4.4287028 -4.4286613 -4.4286556 -4.4286776 -4.4286976 -4.4287052 -4.428688 -4.4286647 -4.4286566][-4.4287105 -4.4287238 -4.4287362 -4.4287329 -4.4287105 -4.4286571 -4.4285665 -4.4284787 -4.4284754 -4.4285431 -4.4286051 -4.42863 -4.42862 -4.4286103 -4.4286132][-4.4286604 -4.428668 -4.4286675 -4.4286532 -4.4286213 -4.4285388 -4.4283814 -4.4282308 -4.4282355 -4.4283676 -4.428484 -4.4285345 -4.4285464 -4.4285607 -4.4285789][-4.4286375 -4.4286251 -4.4286003 -4.4285617 -4.4285154 -4.4284115 -4.4281993 -4.4279904 -4.4280105 -4.4282117 -4.4283762 -4.4284453 -4.4284749 -4.4285092 -4.4285445][-4.4286389 -4.4286094 -4.42856 -4.4284964 -4.428441 -4.4283433 -4.4281378 -4.4279289 -4.4279547 -4.4281654 -4.4283347 -4.4284048 -4.4284382 -4.4284782 -4.4285245][-4.4286642 -4.4286246 -4.428566 -4.4285 -4.4284549 -4.4283948 -4.4282589 -4.4281158 -4.4281259 -4.4282613 -4.4283824 -4.4284358 -4.4284658 -4.4285011 -4.428546][-4.4287219 -4.4286857 -4.4286318 -4.4285779 -4.4285517 -4.4285297 -4.4284616 -4.4283824 -4.4283733 -4.42843 -4.4284925 -4.4285274 -4.428556 -4.4285865 -4.4286237][-4.4287896 -4.428762 -4.4287195 -4.428678 -4.4286666 -4.4286647 -4.428637 -4.4286 -4.4285855 -4.4285946 -4.428616 -4.4286389 -4.4286656 -4.4286909 -4.42872][-4.42882 -4.4288011 -4.4287715 -4.4287419 -4.4287362 -4.4287419 -4.4287376 -4.4287276 -4.4287186 -4.4287076 -4.4287086 -4.4287262 -4.4287496 -4.4287663 -4.4287858][-4.4288239 -4.42881 -4.4287882 -4.4287629 -4.42876 -4.4287734 -4.4287825 -4.4287848 -4.428782 -4.4287705 -4.4287643 -4.4287734 -4.4287839 -4.4287829 -4.4287848][-4.4288106 -4.428802 -4.4287858 -4.4287658 -4.4287639 -4.4287758 -4.4287839 -4.4287891 -4.4287906 -4.428782 -4.428772 -4.4287672 -4.4287548 -4.4287338 -4.42872][-4.4288011 -4.4287963 -4.4287848 -4.428771 -4.4287705 -4.4287782 -4.428782 -4.4287872 -4.4287906 -4.4287848 -4.4287729 -4.4287548 -4.4287171 -4.4286728 -4.4286461]]...]
INFO - root - 2017-12-08 11:02:48.097738: step 6310, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.429 sec/batch; 38h:52m:20s remains)
INFO - root - 2017-12-08 11:02:52.559565: step 6320, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:05m:19s remains)
INFO - root - 2017-12-08 11:02:57.104408: step 6330, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:24m:50s remains)
INFO - root - 2017-12-08 11:03:01.612133: step 6340, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.438 sec/batch; 39h:43m:10s remains)
INFO - root - 2017-12-08 11:03:06.106972: step 6350, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 40h:55m:24s remains)
INFO - root - 2017-12-08 11:03:10.441565: step 6360, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.462 sec/batch; 41h:52m:06s remains)
INFO - root - 2017-12-08 11:03:14.979940: step 6370, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 40h:44m:16s remains)
INFO - root - 2017-12-08 11:03:19.482472: step 6380, loss = 2.28, batch loss = 2.23 (17.5 examples/sec; 0.457 sec/batch; 41h:25m:13s remains)
INFO - root - 2017-12-08 11:03:23.944639: step 6390, loss = 2.28, batch loss = 2.23 (17.4 examples/sec; 0.460 sec/batch; 41h:39m:48s remains)
INFO - root - 2017-12-08 11:03:28.461635: step 6400, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.429 sec/batch; 38h:49m:54s remains)
2017-12-08 11:03:28.998263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287996 -4.428812 -4.4288149 -4.4288106 -4.4288082 -4.4288011 -4.4287863 -4.4287663 -4.4287477 -4.4287419 -4.4287534 -4.4287786 -4.4287772 -4.4287267 -4.4286842][-4.4288135 -4.4288158 -4.4288116 -4.4288015 -4.4287968 -4.4287863 -4.4287729 -4.4287624 -4.4287462 -4.4287314 -4.4287391 -4.4287677 -4.4287724 -4.4287305 -4.4286981][-4.4288187 -4.4288087 -4.4287992 -4.4287839 -4.4287744 -4.4287615 -4.4287491 -4.42875 -4.4287505 -4.4287395 -4.4287429 -4.428772 -4.4287839 -4.4287558 -4.4287291][-4.4288073 -4.428791 -4.4287772 -4.4287581 -4.4287405 -4.4287148 -4.4286985 -4.4287119 -4.4287362 -4.4287395 -4.4287438 -4.4287691 -4.4287786 -4.4287577 -4.4287353][-4.4287858 -4.4287696 -4.4287567 -4.4287305 -4.4286962 -4.4286485 -4.428627 -4.4286585 -4.4287076 -4.42873 -4.4287324 -4.4287472 -4.4287472 -4.4287271 -4.4287143][-4.4287667 -4.4287553 -4.4287434 -4.4287071 -4.4286523 -4.4285846 -4.42856 -4.4286051 -4.4286747 -4.428709 -4.4287047 -4.4287057 -4.4287004 -4.4286838 -4.4286795][-4.4287462 -4.428741 -4.4287291 -4.4286852 -4.4286227 -4.428546 -4.4285178 -4.4285722 -4.42865 -4.428688 -4.4286771 -4.428669 -4.4286637 -4.42866 -4.4286675][-4.4287252 -4.4287162 -4.4287047 -4.4286618 -4.4286013 -4.4285245 -4.4284892 -4.4285436 -4.428627 -4.4286733 -4.4286723 -4.4286761 -4.428688 -4.4287038 -4.4287195][-4.4286976 -4.4286804 -4.4286685 -4.4286356 -4.4285831 -4.4285088 -4.4284644 -4.4285059 -4.4285941 -4.4286561 -4.4286747 -4.4287038 -4.4287453 -4.4287829 -4.4287972][-4.428688 -4.4286618 -4.428647 -4.4286242 -4.428587 -4.4285274 -4.4284878 -4.428514 -4.428586 -4.4286437 -4.4286723 -4.42872 -4.4287834 -4.428834 -4.4288464][-4.4286985 -4.4286666 -4.4286489 -4.4286375 -4.4286251 -4.4285936 -4.4285641 -4.4285703 -4.42861 -4.4286513 -4.4286833 -4.4287353 -4.4287963 -4.4288354 -4.4288378][-4.4287119 -4.428678 -4.428659 -4.428659 -4.4286695 -4.4286613 -4.4286423 -4.4286342 -4.4286485 -4.4286766 -4.4287019 -4.4287372 -4.4287744 -4.428791 -4.4287806][-4.4287357 -4.4287024 -4.4286842 -4.42869 -4.4287124 -4.4287229 -4.4287162 -4.4287062 -4.4287066 -4.428719 -4.4287267 -4.42873 -4.4287329 -4.428731 -4.4287138][-4.428762 -4.4287338 -4.4287162 -4.4287181 -4.4287386 -4.4287591 -4.4287663 -4.428762 -4.4287605 -4.4287615 -4.4287496 -4.4287224 -4.4287004 -4.4286942 -4.4286733][-4.4287853 -4.4287605 -4.42874 -4.4287319 -4.4287443 -4.4287667 -4.428782 -4.4287834 -4.4287839 -4.4287806 -4.4287586 -4.428721 -4.4286985 -4.4286966 -4.4286814]]...]
INFO - root - 2017-12-08 11:03:33.537664: step 6410, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.463 sec/batch; 41h:53m:56s remains)
INFO - root - 2017-12-08 11:03:38.024698: step 6420, loss = 2.28, batch loss = 2.23 (17.9 examples/sec; 0.446 sec/batch; 40h:23m:21s remains)
INFO - root - 2017-12-08 11:03:42.463061: step 6430, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 39h:49m:36s remains)
INFO - root - 2017-12-08 11:03:46.993059: step 6440, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 40h:58m:10s remains)
INFO - root - 2017-12-08 11:03:51.404213: step 6450, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:13m:10s remains)
INFO - root - 2017-12-08 11:03:55.585447: step 6460, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.432 sec/batch; 39h:07m:21s remains)
INFO - root - 2017-12-08 11:03:59.989410: step 6470, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.464 sec/batch; 42h:01m:54s remains)
INFO - root - 2017-12-08 11:04:04.402374: step 6480, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:19m:32s remains)
INFO - root - 2017-12-08 11:04:08.929498: step 6490, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.452 sec/batch; 40h:54m:03s remains)
INFO - root - 2017-12-08 11:04:13.327956: step 6500, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.433 sec/batch; 39h:10m:56s remains)
2017-12-08 11:04:13.882024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285479 -4.4285316 -4.4285283 -4.4285502 -4.4286046 -4.428668 -4.4287133 -4.4287438 -4.4287381 -4.4287043 -4.4286466 -4.4285655 -4.4285016 -4.42849 -4.4285054][-4.4285884 -4.4285803 -4.4285812 -4.4285951 -4.4286318 -4.4286814 -4.4287195 -4.4287376 -4.4287257 -4.4286923 -4.4286418 -4.4285736 -4.4285178 -4.4285107 -4.4285297][-4.4286561 -4.4286509 -4.4286509 -4.4286494 -4.4286571 -4.4286737 -4.4286871 -4.4286895 -4.4286771 -4.4286585 -4.4286304 -4.4285922 -4.4285612 -4.4285636 -4.4285851][-4.4287114 -4.4287019 -4.4286938 -4.4286819 -4.4286709 -4.4286609 -4.4286489 -4.4286356 -4.4286203 -4.4286137 -4.4286113 -4.428607 -4.4286036 -4.4286165 -4.428638][-4.4287438 -4.4287305 -4.4287181 -4.4287028 -4.4286819 -4.4286494 -4.4286094 -4.4285707 -4.4285493 -4.4285536 -4.4285774 -4.428606 -4.428628 -4.4286489 -4.42867][-4.4287348 -4.4287167 -4.4287028 -4.428688 -4.4286633 -4.4286118 -4.4285369 -4.4284639 -4.4284329 -4.4284554 -4.4285121 -4.4285793 -4.4286356 -4.4286747 -4.4287014][-4.4287109 -4.4286833 -4.4286647 -4.4286447 -4.42861 -4.4285331 -4.4284148 -4.4282985 -4.428268 -4.4283333 -4.428442 -4.4285579 -4.4286528 -4.4287109 -4.4287376][-4.4286814 -4.4286356 -4.4286017 -4.4285703 -4.4285216 -4.42843 -4.4282932 -4.4281659 -4.4281573 -4.42828 -4.4284449 -4.4285903 -4.4286914 -4.4287405 -4.42875][-4.4286466 -4.4285827 -4.4285288 -4.4284844 -4.428432 -4.4283538 -4.4282517 -4.4281721 -4.4281983 -4.4283423 -4.4285126 -4.4286447 -4.4287233 -4.4287491 -4.4287419][-4.4286165 -4.4285431 -4.4284787 -4.4284286 -4.428391 -4.4283571 -4.4283261 -4.4283171 -4.4283628 -4.4284792 -4.4286075 -4.4286976 -4.4287429 -4.428751 -4.4287353][-4.4286103 -4.4285364 -4.4284711 -4.4284325 -4.4284234 -4.4284382 -4.4284663 -4.428503 -4.4285512 -4.4286261 -4.4287014 -4.4287448 -4.4287591 -4.4287524 -4.4287286][-4.4286189 -4.4285693 -4.4285269 -4.4285073 -4.4285192 -4.42856 -4.4286108 -4.428658 -4.4286962 -4.4287391 -4.4287748 -4.4287772 -4.4287581 -4.4287329 -4.4287043][-4.4286318 -4.4286189 -4.4286084 -4.4286032 -4.428616 -4.42865 -4.4286914 -4.4287267 -4.4287534 -4.4287796 -4.4287958 -4.4287734 -4.4287281 -4.42869 -4.4286628][-4.4286437 -4.4286575 -4.4286652 -4.4286585 -4.4286594 -4.4286752 -4.4286995 -4.4287224 -4.4287391 -4.4287567 -4.4287648 -4.4287333 -4.4286761 -4.4286284 -4.4286][-4.4286571 -4.4286766 -4.428679 -4.4286618 -4.42865 -4.4286494 -4.4286604 -4.4286757 -4.4286876 -4.4287024 -4.428709 -4.4286776 -4.428618 -4.4285655 -4.4285369]]...]
INFO - root - 2017-12-08 11:04:18.330935: step 6510, loss = 2.28, batch loss = 2.23 (16.9 examples/sec; 0.472 sec/batch; 42h:44m:32s remains)
INFO - root - 2017-12-08 11:04:22.724908: step 6520, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 39h:49m:54s remains)
INFO - root - 2017-12-08 11:04:27.099578: step 6530, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 39h:49m:08s remains)
INFO - root - 2017-12-08 11:04:31.439268: step 6540, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.427 sec/batch; 38h:40m:50s remains)
INFO - root - 2017-12-08 11:04:35.799196: step 6550, loss = 2.28, batch loss = 2.23 (17.7 examples/sec; 0.453 sec/batch; 40h:58m:45s remains)
INFO - root - 2017-12-08 11:04:39.902364: step 6560, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:18m:23s remains)
INFO - root - 2017-12-08 11:04:44.264187: step 6570, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.429 sec/batch; 38h:50m:36s remains)
INFO - root - 2017-12-08 11:04:48.600896: step 6580, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.435 sec/batch; 39h:24m:24s remains)
INFO - root - 2017-12-08 11:04:53.011930: step 6590, loss = 2.28, batch loss = 2.23 (16.2 examples/sec; 0.494 sec/batch; 44h:41m:15s remains)
INFO - root - 2017-12-08 11:04:57.467943: step 6600, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:11m:13s remains)
2017-12-08 11:04:57.976920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288516 -4.4288487 -4.4288535 -4.4288568 -4.4288578 -4.4288535 -4.4288464 -4.4288392 -4.4288383 -4.4288445 -4.4288607 -4.4288845 -4.4289021 -4.428906 -4.4288955][-4.4288507 -4.4288487 -4.4288507 -4.4288507 -4.4288464 -4.4288383 -4.4288273 -4.4288197 -4.4288187 -4.4288292 -4.4288549 -4.4288883 -4.428905 -4.4289 -4.4288797][-4.428853 -4.4288511 -4.42885 -4.4288435 -4.42883 -4.428812 -4.4287939 -4.4287834 -4.4287858 -4.428803 -4.4288416 -4.4288831 -4.4288945 -4.42888 -4.4288545][-4.4288516 -4.4288473 -4.4288397 -4.428823 -4.4287992 -4.4287724 -4.4287491 -4.42874 -4.4287481 -4.4287744 -4.428822 -4.4288635 -4.4288673 -4.4288473 -4.4288239][-4.4288421 -4.4288282 -4.4288049 -4.4287724 -4.4287391 -4.4287114 -4.4286942 -4.4286933 -4.4287081 -4.4287429 -4.4287949 -4.4288316 -4.428834 -4.42882 -4.4288063][-4.428813 -4.4287806 -4.4287372 -4.4286895 -4.4286504 -4.4286284 -4.4286242 -4.4286332 -4.4286566 -4.4287 -4.4287519 -4.4287858 -4.4288 -4.4288077 -4.4288087][-4.4287691 -4.4287186 -4.4286604 -4.4286041 -4.4285693 -4.4285593 -4.4285684 -4.4285851 -4.4286113 -4.4286551 -4.4287076 -4.4287448 -4.4287758 -4.4288063 -4.42882][-4.4287305 -4.4286642 -4.4285941 -4.4285393 -4.4285231 -4.4285345 -4.4285569 -4.4285746 -4.4285955 -4.4286342 -4.4286833 -4.4287267 -4.4287734 -4.4288182 -4.4288392][-4.4287043 -4.4286237 -4.4285479 -4.428504 -4.4285107 -4.4285455 -4.4285827 -4.4286051 -4.4286222 -4.4286513 -4.4286947 -4.4287415 -4.428792 -4.4288363 -4.4288545][-4.4286909 -4.4286022 -4.4285278 -4.4284954 -4.4285212 -4.4285769 -4.4286327 -4.4286618 -4.4286761 -4.4286957 -4.4287286 -4.4287734 -4.428822 -4.4288611 -4.4288721][-4.4286819 -4.4286051 -4.42855 -4.4285321 -4.4285684 -4.4286337 -4.4286928 -4.4287238 -4.4287338 -4.4287381 -4.4287558 -4.4287958 -4.4288464 -4.428885 -4.4288831][-4.4286852 -4.4286242 -4.4285932 -4.4285913 -4.428628 -4.42869 -4.4287486 -4.4287767 -4.4287753 -4.4287624 -4.4287667 -4.4288054 -4.4288592 -4.4288917 -4.4288764][-4.4287052 -4.4286628 -4.4286475 -4.4286532 -4.4286885 -4.4287472 -4.4288006 -4.4288182 -4.4287996 -4.4287705 -4.428771 -4.42881 -4.42886 -4.428884 -4.4288597][-4.4287386 -4.4287162 -4.4287148 -4.4287252 -4.4287553 -4.4288025 -4.4288421 -4.428843 -4.4288054 -4.4287658 -4.4287672 -4.4288073 -4.4288559 -4.4288783 -4.4288535][-4.4287686 -4.4287643 -4.4287715 -4.4287829 -4.4288063 -4.4288406 -4.4288654 -4.428854 -4.4288044 -4.4287586 -4.4287639 -4.42881 -4.428864 -4.4288898 -4.4288721]]...]
INFO - root - 2017-12-08 11:05:02.429712: step 6610, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:03m:26s remains)
INFO - root - 2017-12-08 11:05:06.904120: step 6620, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 40h:42m:03s remains)
INFO - root - 2017-12-08 11:05:11.394188: step 6630, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:19m:11s remains)
INFO - root - 2017-12-08 11:05:15.860476: step 6640, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.436 sec/batch; 39h:30m:03s remains)
INFO - root - 2017-12-08 11:05:20.369155: step 6650, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 39h:59m:00s remains)
INFO - root - 2017-12-08 11:05:24.508390: step 6660, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:42m:49s remains)
INFO - root - 2017-12-08 11:05:28.925287: step 6670, loss = 2.28, batch loss = 2.23 (17.0 examples/sec; 0.470 sec/batch; 42h:31m:30s remains)
INFO - root - 2017-12-08 11:05:33.305861: step 6680, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 39h:49m:07s remains)
INFO - root - 2017-12-08 11:05:37.698224: step 6690, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.445 sec/batch; 40h:14m:08s remains)
INFO - root - 2017-12-08 11:05:42.134215: step 6700, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 38h:26m:00s remains)
2017-12-08 11:05:42.646358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287086 -4.4287214 -4.4287267 -4.4287095 -4.4286528 -4.4285693 -4.4284821 -4.428452 -4.4285164 -4.4286389 -4.4287281 -4.4287686 -4.4287906 -4.4287949 -4.428771][-4.4286966 -4.428699 -4.4287109 -4.4287076 -4.4286661 -4.4285903 -4.4285059 -4.4284644 -4.428503 -4.4285965 -4.4286647 -4.428699 -4.4287224 -4.4287424 -4.4287424][-4.4287038 -4.4287152 -4.4287391 -4.4287448 -4.4287167 -4.4286451 -4.4285622 -4.4285126 -4.4285212 -4.42857 -4.4286118 -4.4286342 -4.4286537 -4.4286857 -4.428709][-4.4287028 -4.4287324 -4.428771 -4.4287834 -4.42876 -4.4286895 -4.4286103 -4.4285583 -4.4285522 -4.4285755 -4.4285994 -4.428617 -4.4286332 -4.42866 -4.4286847][-4.4286594 -4.4286923 -4.4287434 -4.4287629 -4.4287419 -4.4286766 -4.4285984 -4.4285426 -4.4285321 -4.4285645 -4.4286013 -4.4286323 -4.4286566 -4.4286704 -4.4286828][-4.4286017 -4.4286246 -4.4286847 -4.4287138 -4.4286957 -4.4286261 -4.4285254 -4.4284511 -4.428442 -4.4284992 -4.4285741 -4.42864 -4.4286809 -4.4286904 -4.4287019][-4.4285531 -4.428565 -4.4286284 -4.4286613 -4.4286447 -4.428565 -4.4284496 -4.428359 -4.4283485 -4.42843 -4.4285374 -4.4286366 -4.4286957 -4.4287133 -4.4287281][-4.4285293 -4.4285269 -4.4285769 -4.4286065 -4.4285846 -4.4285097 -4.4284143 -4.4283438 -4.4283385 -4.4284186 -4.4285264 -4.4286308 -4.4286933 -4.42871 -4.42872][-4.4285355 -4.4285207 -4.4285531 -4.4285746 -4.4285517 -4.4284949 -4.428442 -4.4284072 -4.4284143 -4.4284754 -4.428556 -4.4286342 -4.42868 -4.4286838 -4.4286847][-4.4285254 -4.4285207 -4.4285512 -4.4285612 -4.4285326 -4.4284887 -4.4284654 -4.4284596 -4.4284868 -4.4285421 -4.4285965 -4.4286475 -4.4286761 -4.4286661 -4.42866][-4.428504 -4.4285207 -4.4285612 -4.4285626 -4.4285235 -4.4284787 -4.4284616 -4.42847 -4.4285121 -4.4285736 -4.4286203 -4.428659 -4.4286819 -4.4286704 -4.4286609][-4.42853 -4.4285517 -4.4285889 -4.4285932 -4.4285655 -4.4285283 -4.4285083 -4.4285126 -4.4285531 -4.4286127 -4.4286618 -4.4287004 -4.4287238 -4.4287157 -4.4287028][-4.4286103 -4.4286213 -4.428647 -4.4286571 -4.4286489 -4.4286275 -4.428606 -4.4286036 -4.4286308 -4.4286776 -4.4287252 -4.4287615 -4.4287829 -4.4287815 -4.4287715][-4.4287119 -4.4287138 -4.4287314 -4.4287448 -4.4287472 -4.4287348 -4.4287162 -4.4287119 -4.4287286 -4.428761 -4.4287939 -4.4288197 -4.4288373 -4.4288425 -4.4288406][-4.4287972 -4.4287963 -4.4288082 -4.4288192 -4.4288235 -4.4288149 -4.4287992 -4.4287925 -4.4288011 -4.428822 -4.4288449 -4.4288635 -4.4288788 -4.4288874 -4.4288907]]...]
INFO - root - 2017-12-08 11:05:47.118601: step 6710, loss = 2.28, batch loss = 2.23 (18.5 examples/sec; 0.431 sec/batch; 39h:02m:25s remains)
INFO - root - 2017-12-08 11:05:51.594250: step 6720, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.436 sec/batch; 39h:27m:36s remains)
INFO - root - 2017-12-08 11:05:56.080972: step 6730, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 39h:39m:28s remains)
INFO - root - 2017-12-08 11:06:00.517991: step 6740, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 40h:00m:36s remains)
INFO - root - 2017-12-08 11:06:04.956161: step 6750, loss = 2.28, batch loss = 2.23 (18.7 examples/sec; 0.428 sec/batch; 38h:41m:30s remains)
INFO - root - 2017-12-08 11:06:09.143173: step 6760, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:05m:40s remains)
INFO - root - 2017-12-08 11:06:13.666574: step 6770, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 39h:51m:11s remains)
INFO - root - 2017-12-08 11:06:18.229200: step 6780, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 39h:59m:15s remains)
INFO - root - 2017-12-08 11:06:22.744941: step 6790, loss = 2.28, batch loss = 2.23 (16.7 examples/sec; 0.478 sec/batch; 43h:15m:46s remains)
INFO - root - 2017-12-08 11:06:27.240122: step 6800, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.434 sec/batch; 39h:18m:28s remains)
2017-12-08 11:06:27.760263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287825 -4.4287391 -4.4287033 -4.4286909 -4.4287248 -4.4287715 -4.4287996 -4.4288077 -4.428822 -4.4288387 -4.4288297 -4.4288039 -4.4287825 -4.4287519 -4.4287148][-4.4287672 -4.428709 -4.4286585 -4.4286361 -4.4286633 -4.4287033 -4.4287295 -4.4287405 -4.4287648 -4.4287949 -4.4287944 -4.4287667 -4.4287448 -4.4287109 -4.4286528][-4.4287739 -4.4287109 -4.4286532 -4.4286222 -4.4286375 -4.4286647 -4.4286804 -4.4286809 -4.4287052 -4.4287467 -4.4287558 -4.4287386 -4.4287162 -4.4286728 -4.4285936][-4.4287834 -4.4287219 -4.428668 -4.4286294 -4.428627 -4.428637 -4.428638 -4.4286194 -4.4286394 -4.4286866 -4.4287105 -4.4287095 -4.4286909 -4.4286394 -4.4285588][-4.4287934 -4.4287362 -4.4286838 -4.4286385 -4.4286184 -4.4286113 -4.4286008 -4.4285727 -4.4285893 -4.4286389 -4.4286819 -4.4286981 -4.4286823 -4.4286323 -4.4285626][-4.4287982 -4.4287419 -4.428678 -4.4286275 -4.4286013 -4.4285855 -4.4285693 -4.4285412 -4.4285574 -4.4286051 -4.4286537 -4.4286823 -4.4286761 -4.4286456 -4.4285989][-4.4287848 -4.428721 -4.4286494 -4.4285975 -4.4285684 -4.4285393 -4.428514 -4.4284849 -4.4284987 -4.4285436 -4.4285979 -4.4286308 -4.4286313 -4.4286189 -4.4286008][-4.4287386 -4.4286518 -4.4285684 -4.4285192 -4.428493 -4.4284544 -4.4284153 -4.4283834 -4.4284 -4.4284573 -4.428524 -4.4285655 -4.4285755 -4.42858 -4.4285808][-4.4286871 -4.428575 -4.4284763 -4.4284253 -4.4284067 -4.4283762 -4.4283442 -4.4283171 -4.4283314 -4.4283967 -4.428473 -4.428525 -4.4285474 -4.4285636 -4.4285793][-4.4286628 -4.42854 -4.4284372 -4.4283891 -4.4283791 -4.4283695 -4.4283624 -4.4283543 -4.4283686 -4.4284258 -4.4284916 -4.4285369 -4.4285617 -4.4285822 -4.4286051][-4.4286747 -4.4285612 -4.4284663 -4.4284225 -4.42842 -4.4284339 -4.4284506 -4.4284573 -4.4284616 -4.4284978 -4.428544 -4.4285812 -4.428607 -4.4286313 -4.4286566][-4.4287338 -4.4286418 -4.4285655 -4.4285269 -4.4285207 -4.4285378 -4.4285626 -4.4285746 -4.4285722 -4.4285865 -4.4286141 -4.4286413 -4.4286652 -4.4286895 -4.4287124][-4.4288087 -4.4287424 -4.4286871 -4.4286551 -4.4286437 -4.4286566 -4.4286752 -4.4286838 -4.4286785 -4.4286819 -4.4286909 -4.4287009 -4.4287124 -4.4287271 -4.4287419][-4.4288588 -4.4288106 -4.4287686 -4.4287419 -4.4287271 -4.42873 -4.4287386 -4.4287429 -4.4287386 -4.4287367 -4.4287357 -4.4287333 -4.4287338 -4.4287415 -4.4287534][-4.4288788 -4.428843 -4.4288125 -4.428792 -4.42878 -4.4287806 -4.4287848 -4.4287872 -4.4287825 -4.4287777 -4.4287739 -4.4287705 -4.42877 -4.4287753 -4.4287863]]...]
INFO - root - 2017-12-08 11:06:32.203152: step 6810, loss = 2.28, batch loss = 2.23 (18.2 examples/sec; 0.440 sec/batch; 39h:48m:22s remains)
INFO - root - 2017-12-08 11:06:36.655865: step 6820, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.454 sec/batch; 41h:06m:56s remains)
INFO - root - 2017-12-08 11:06:40.983644: step 6830, loss = 2.28, batch loss = 2.23 (18.0 examples/sec; 0.444 sec/batch; 40h:11m:25s remains)
INFO - root - 2017-12-08 11:06:45.467481: step 6840, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.437 sec/batch; 39h:33m:36s remains)
INFO - root - 2017-12-08 11:06:49.984850: step 6850, loss = 2.28, batch loss = 2.23 (17.2 examples/sec; 0.465 sec/batch; 42h:03m:47s remains)
INFO - root - 2017-12-08 11:06:54.173870: step 6860, loss = 2.28, batch loss = 2.23 (18.4 examples/sec; 0.436 sec/batch; 39h:25m:54s remains)
INFO - root - 2017-12-08 11:06:58.613780: step 6870, loss = 2.28, batch loss = 2.23 (17.8 examples/sec; 0.450 sec/batch; 40h:44m:18s remains)
INFO - root - 2017-12-08 11:07:03.057055: step 6880, loss = 2.28, batch loss = 2.23 (18.6 examples/sec; 0.431 sec/batch; 38h:58m:36s remains)
INFO - root - 2017-12-08 11:07:07.432229: step 6890, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 39h:38m:07s remains)
INFO - root - 2017-12-08 11:07:11.904849: step 6900, loss = 2.28, batch loss = 2.23 (18.3 examples/sec; 0.438 sec/batch; 39h:34m:18s remains)
2017-12-08 11:07:12.558828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288692 -4.4288125 -4.4287853 -4.4287696 -4.4287448 -4.4287238 -4.4287291 -4.4287734 -4.4288306 -4.42888 -4.4289069 -4.4289026 -4.4288774 -4.4288435 -4.4288125][-4.4288487 -4.42878 -4.428741 -4.4287133 -4.4286833 -4.42866 -4.4286675 -4.4287148 -4.4287863 -4.42885 -4.4288859 -4.4288793 -4.4288445 -4.4287996 -4.4287491][-4.4288392 -4.4287562 -4.4287014 -4.4286637 -4.4286356 -4.4286203 -4.4286308 -4.428668 -4.4287353 -4.4288073 -4.4288526 -4.4288545 -4.4288173 -4.4287615 -4.4286842][-4.4288311 -4.4287457 -4.428678 -4.4286294 -4.4285989 -4.4285789 -4.4285669 -4.4285717 -4.4286304 -4.4287243 -4.4287949 -4.4288182 -4.4287877 -4.4287257 -4.4286327][-4.4288449 -4.4287667 -4.4286904 -4.4286308 -4.4285836 -4.4285345 -4.4284658 -4.4284191 -4.4284954 -4.4286342 -4.4287453 -4.4287939 -4.4287763 -4.4287195 -4.4286265][-4.4288688 -4.4287968 -4.4287095 -4.4286294 -4.4285464 -4.4284377 -4.4282765 -4.4281745 -4.4283094 -4.4285283 -4.4286947 -4.4287763 -4.4287858 -4.428751 -4.4286795][-4.4288921 -4.4288278 -4.4287405 -4.4286427 -4.4285126 -4.4283166 -4.4280314 -4.4278731 -4.4280949 -4.4284167 -4.4286466 -4.4287639 -4.428802 -4.4287977 -4.4287577][-4.428906 -4.4288445 -4.4287629 -4.4286733 -4.4285407 -4.4283271 -4.428019 -4.4278564 -4.4280844 -4.42841 -4.4286404 -4.428762 -4.4288135 -4.4288306 -4.4288158][-4.4289131 -4.4288425 -4.4287577 -4.4286823 -4.4285803 -4.4284286 -4.42823 -4.4281316 -4.4282804 -4.4285073 -4.4286728 -4.4287548 -4.4287891 -4.4288135 -4.428822][-4.428925 -4.4288497 -4.4287663 -4.4286995 -4.4286232 -4.4285293 -4.428412 -4.4283495 -4.4284415 -4.4285893 -4.4286895 -4.4287286 -4.4287395 -4.4287558 -4.4287777][-4.428937 -4.4288678 -4.428792 -4.4287295 -4.4286637 -4.4285889 -4.4284959 -4.4284492 -4.4285173 -4.4286227 -4.4286809 -4.428689 -4.4286823 -4.4286952 -4.4287176][-4.4289393 -4.4288774 -4.4288077 -4.4287467 -4.4286809 -4.4286103 -4.4285297 -4.4284997 -4.4285631 -4.428638 -4.4286709 -4.428669 -4.4286556 -4.4286647 -4.4286785][-4.4289336 -4.428884 -4.4288235 -4.428761 -4.4286914 -4.4286227 -4.4285483 -4.4285216 -4.4285784 -4.4286342 -4.4286566 -4.4286628 -4.428668 -4.4286819 -4.4286852][-4.4289222 -4.4288855 -4.4288363 -4.4287786 -4.4287133 -4.4286566 -4.4285955 -4.4285684 -4.4286108 -4.4286556 -4.4286771 -4.4286976 -4.4287186 -4.4287333 -4.4287281][-4.4289155 -4.4288893 -4.4288526 -4.4288039 -4.428751 -4.4287105 -4.428669 -4.4286427 -4.4286652 -4.4286971 -4.4287224 -4.4287496 -4.4287739 -4.4287882 -4.4287829]]...]
INFO - root - 2017-12-08 11:07:16.906498: step 6910, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.416 sec/batch; 37h:36m:40s remains)
INFO - root - 2017-12-08 11:07:21.426561: step 6920, loss = 2.28, batch loss = 2.23 (17.6 examples/sec; 0.455 sec/batch; 41h:11m:22s remains)
INFO - root - 2017-12-08 11:07:25.987303: step 6930, loss = 2.28, batch loss = 2.23 (16.8 examples/sec; 0.477 sec/batch; 43h:08m:38s remains)
INFO - root - 2017-12-08 11:07:30.542530: step 6940, loss = 2.28, batch loss = 2.23 (19.2 examples/sec; 0.417 sec/batch; 37h:44m:38s remains)
INFO - root - 2017-12-08 11:07:35.060783: step 6950, loss = 2.28, batch loss = 2.23 (17.3 examples/sec; 0.462 sec/batch; 41h:45m:32s remains)
INFO - root - 2017-12-08 11:07:39.183731: step 6960, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.426 sec/batch; 38h:33m:41s remains)
INFO - root - 2017-12-08 11:07:43.580225: step 6970, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.442 sec/batch; 39h:58m:34s remains)
INFO - root - 2017-12-08 11:07:47.940165: step 6980, loss = 2.28, batch loss = 2.23 (18.9 examples/sec; 0.423 sec/batch; 38h:16m:55s remains)
INFO - root - 2017-12-08 11:07:52.392995: step 6990, loss = 2.28, batch loss = 2.23 (18.1 examples/sec; 0.443 sec/batch; 40h:02m:32s remains)
