INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "283"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-15 09:21:15.242298: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 09:21:15.242338: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 09:21:15.242344: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 09:21:15.242348: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 09:21:15.242352: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 09:21:16.312211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-15 09:21:16.312249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-15 09:21:16.312256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-15 09:21:16.312270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
inputs Tensor("batch_1:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_2/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc_2/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc_2/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_2/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch_1:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_3/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_3/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_3/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_3/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection_1/add:0", shape=(8, 15, 15), dtype=float32)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-15 09:21:22.671668: step 0, loss = 4.50, batch loss = 2.22 (1.8 examples/sec; 4.478 sec/batch; 413h:34m:55s remains)
2017-12-15 09:21:23.524288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3957257 -4.3956437 -4.3956079 -4.3955736 -4.3955412 -4.3955011 -4.3954668 -4.395453 -4.3954453 -4.3954372 -4.3954291 -4.395421 -4.3954268 -4.3954439 -4.3954749][-4.3955503 -4.39547 -4.3954425 -4.3954172 -4.3953853 -4.3953309 -4.3952742 -4.3952413 -4.3952212 -4.3952055 -4.395196 -4.3952007 -4.3952289 -4.3952656 -4.3953114][-4.3954363 -4.3953557 -4.3953476 -4.3953419 -4.3953147 -4.3952465 -4.3951659 -4.39511 -4.3950796 -4.3950744 -4.3950872 -4.3951249 -4.3951869 -4.3952484 -4.3953047][-4.3953791 -4.3953085 -4.3953223 -4.3953371 -4.3953142 -4.3952322 -4.3951287 -4.3950529 -4.395021 -4.3950391 -4.39509 -4.3951693 -4.3952641 -4.3953447 -4.3954053][-4.3953648 -4.3953142 -4.3953557 -4.3953962 -4.3953824 -4.3952932 -4.3951755 -4.3950868 -4.3950596 -4.3951039 -4.3951888 -4.3952975 -4.3954148 -4.395503 -4.3955569][-4.3953729 -4.3953404 -4.3954043 -4.395463 -4.39546 -4.3953667 -4.3952351 -4.3951344 -4.3951173 -4.3951874 -4.3953009 -4.3954268 -4.3955517 -4.395638 -4.395679][-4.3953767 -4.395339 -4.3954072 -4.395474 -4.3954759 -4.3953805 -4.3952312 -4.3951144 -4.39511 -4.395206 -4.3953433 -4.3954821 -4.3956103 -4.3956952 -4.3957281][-4.3954067 -4.3953533 -4.3954105 -4.3954711 -4.3954735 -4.39538 -4.3952188 -4.3950896 -4.3950934 -4.3952 -4.3953428 -4.3954816 -4.3956051 -4.3956881 -4.3957181][-4.3954911 -4.3954358 -4.3954792 -4.3955336 -4.3955479 -4.3954816 -4.3953466 -4.3952274 -4.3952203 -4.3952975 -4.3954048 -4.3955107 -4.3956122 -4.3956838 -4.3957152][-4.3956084 -4.3955646 -4.3956084 -4.3956685 -4.3957038 -4.3956804 -4.395597 -4.3955088 -4.3954859 -4.3955178 -4.39557 -4.3956256 -4.39569 -4.395741 -4.3957696][-4.3957314 -4.3957024 -4.395752 -4.3958187 -4.3958688 -4.3958788 -4.395843 -4.3957891 -4.395762 -4.395762 -4.3957705 -4.3957849 -4.3958168 -4.3958488 -4.3958731][-4.3958368 -4.3958192 -4.3958716 -4.3959374 -4.39599 -4.3960166 -4.3960147 -4.3959951 -4.395978 -4.3959675 -4.3959546 -4.3959451 -4.3959522 -4.3959684 -4.395988][-4.395926 -4.3959155 -4.3959675 -4.396028 -4.3960762 -4.3961072 -4.3961244 -4.3961282 -4.3961253 -4.3961177 -4.3960996 -4.3960791 -4.3960676 -4.3960657 -4.3960752][-4.3959761 -4.3959718 -4.3960209 -4.3960743 -4.3961153 -4.396143 -4.396163 -4.3961778 -4.3961864 -4.3961892 -4.3961797 -4.396162 -4.3961444 -4.3961282 -4.3961229][-4.3959746 -4.3959723 -4.3960185 -4.3960705 -4.3961077 -4.3961263 -4.3961391 -4.3961563 -4.396173 -4.3961873 -4.39619 -4.39618 -4.3961635 -4.3961411 -4.3961248]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 09:21:31.265904: step 10, loss = 2.47, batch loss = 0.93 (12.9 examples/sec; 0.618 sec/batch; 57h:05m:15s remains)
INFO - root - 2017-12-15 09:21:37.528134: step 20, loss = 2.22, batch loss = 0.92 (12.7 examples/sec; 0.632 sec/batch; 58h:21m:55s remains)
INFO - root - 2017-12-15 09:21:43.789886: step 30, loss = 1.52, batch loss = 0.62 (13.1 examples/sec; 0.609 sec/batch; 56h:12m:13s remains)
INFO - root - 2017-12-15 09:21:50.134293: step 40, loss = 1.22, batch loss = 0.52 (12.7 examples/sec; 0.628 sec/batch; 57h:57m:37s remains)
INFO - root - 2017-12-15 09:21:56.511915: step 50, loss = 1.24, batch loss = 0.51 (12.4 examples/sec; 0.647 sec/batch; 59h:43m:07s remains)
INFO - root - 2017-12-15 09:22:02.831086: step 60, loss = 1.20, batch loss = 0.50 (12.7 examples/sec; 0.629 sec/batch; 58h:03m:45s remains)
INFO - root - 2017-12-15 09:22:09.104821: step 70, loss = 1.27, batch loss = 0.52 (12.5 examples/sec; 0.640 sec/batch; 59h:04m:13s remains)
INFO - root - 2017-12-15 09:22:15.423343: step 80, loss = 1.16, batch loss = 0.48 (12.9 examples/sec; 0.622 sec/batch; 57h:25m:38s remains)
INFO - root - 2017-12-15 09:22:21.702027: step 90, loss = 1.09, batch loss = 0.45 (12.7 examples/sec; 0.628 sec/batch; 57h:57m:17s remains)
INFO - root - 2017-12-15 09:22:28.001075: step 100, loss = 1.16, batch loss = 0.45 (12.9 examples/sec; 0.622 sec/batch; 57h:23m:12s remains)
2017-12-15 09:22:28.548985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1467373 -0.99412274 -0.97444534 -1.0531349 -1.0684309 -1.0168166 -0.93316436 -0.87412024 -0.7820785 -1.2518249 -1.7221063 -2.3922749 -2.8363492 -3.0972397 -3.3180833][-1.3484278 -1.1167927 -0.98855376 -0.94234371 -0.86024189 -0.813416 -0.78701782 -0.70175552 -0.60448885 -1.0714259 -1.5048947 -2.189244 -2.6039598 -2.8677409 -3.1112978][-1.5175717 -1.2949381 -1.1216788 -0.97425056 -0.8361876 -0.71034193 -0.59696531 -0.50396156 -0.42603612 -0.83933425 -1.213146 -1.9164953 -2.3629875 -2.6267505 -2.8456154][-1.6245912 -1.3620143 -1.1560857 -1.0079861 -0.85541248 -0.6642437 -0.49570656 -0.38991165 -0.30749011 -0.73606348 -1.1406393 -1.7256693 -2.0504208 -2.2950768 -2.5297809][-1.7278298 -1.4532735 -1.2895911 -1.1414902 -0.92693973 -0.72388768 -0.546859 -0.4110465 -0.32233191 -0.74457645 -1.0682404 -1.5194688 -1.7918178 -2.0247033 -2.2599518][-1.699849 -1.4523106 -1.2925847 -1.1625261 -0.96719241 -0.75801229 -0.58620358 -0.44916034 -0.2855978 -0.65567088 -0.94871449 -1.2435992 -1.4760914 -1.6935685 -1.8910277][-1.5515826 -1.3145144 -1.1855435 -1.0933917 -0.96462727 -0.82264423 -0.73557448 -0.70184875 -0.57915115 -0.88623881 -1.0982945 -1.3485675 -1.5324714 -1.6351951 -1.7682459][-1.3379338 -1.0925412 -0.98549008 -0.90158558 -0.81797576 -0.72924376 -0.65839458 -0.63441062 -0.5516696 -0.998482 -1.3395343 -1.5251696 -1.6299074 -1.664368 -1.7642498][-1.2505305 -0.93130684 -0.77520227 -0.69171643 -0.622833 -0.57006168 -0.54464173 -0.56887484 -0.55204344 -1.0492041 -1.4001091 -1.5828263 -1.7772856 -1.8576605 -1.9204054][-1.4203398 -1.1399684 -0.9761138 -0.8798933 -0.81791806 -0.72851872 -0.70400715 -0.79740429 -0.83176613 -1.2535837 -1.5204561 -1.7581891 -1.9069096 -1.9461063 -1.9890317][-1.5841259 -1.3109927 -1.1459465 -1.0528853 -0.96349239 -0.89540052 -0.87573791 -0.95539927 -1.0186603 -1.4109817 -1.6808766 -2.0003092 -2.1639132 -2.1611278 -2.1454263][-1.9154626 -1.6632679 -1.5280418 -1.4475894 -1.363482 -1.2879298 -1.1784859 -1.1998625 -1.2695191 -1.5914532 -1.8396778 -2.0885067 -2.2069139 -2.3069453 -2.3613939][-2.274261 -2.0437837 -1.9395404 -1.8809502 -1.7942891 -1.7281204 -1.6030627 -1.6066122 -1.6230458 -1.8141453 -1.96858 -2.1608348 -2.2634215 -2.3899186 -2.4656184][-2.574713 -2.3910096 -2.2871728 -2.2148037 -2.1266406 -2.0365996 -1.9149015 -1.9555782 -1.9226977 -1.9644725 -2.0332556 -2.2106757 -2.3387277 -2.4394174 -2.518815][-2.8670859 -2.7551007 -2.706599 -2.6648366 -2.6252825 -2.5703087 -2.4517677 -2.4349525 -2.3766232 -2.4471767 -2.4559271 -2.4821863 -2.6178684 -2.6974776 -2.6909461]]...]
INFO - root - 2017-12-15 09:22:34.870020: step 110, loss = 1.09, batch loss = 0.43 (13.0 examples/sec; 0.615 sec/batch; 56h:44m:31s remains)
INFO - root - 2017-12-15 09:22:41.237521: step 120, loss = 1.06, batch loss = 0.42 (12.6 examples/sec; 0.635 sec/batch; 58h:38m:38s remains)
2017-12-15 09:22:45.635955: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 59894 get requests, put_count=59891 evicted_count=1000 eviction_rate=0.016697 and unsatisfied allocation rate=0.0184159
2017-12-15 09:22:45.635997: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
INFO - root - 2017-12-15 09:22:47.497301: step 130, loss = 1.16, batch loss = 0.48 (12.8 examples/sec; 0.623 sec/batch; 57h:32m:10s remains)
INFO - root - 2017-12-15 09:22:53.792582: step 140, loss = 1.25, batch loss = 0.55 (12.6 examples/sec; 0.636 sec/batch; 58h:40m:54s remains)
INFO - root - 2017-12-15 09:23:00.130452: step 150, loss = 1.23, batch loss = 0.51 (12.7 examples/sec; 0.629 sec/batch; 58h:06m:20s remains)
INFO - root - 2017-12-15 09:23:06.451686: step 160, loss = 1.00, batch loss = 0.39 (12.5 examples/sec; 0.639 sec/batch; 59h:00m:49s remains)
INFO - root - 2017-12-15 09:23:12.776906: step 170, loss = 1.02, batch loss = 0.39 (12.8 examples/sec; 0.623 sec/batch; 57h:29m:04s remains)
INFO - root - 2017-12-15 09:23:19.102936: step 180, loss = 1.07, batch loss = 0.46 (12.5 examples/sec; 0.638 sec/batch; 58h:55m:09s remains)
INFO - root - 2017-12-15 09:23:25.537812: step 190, loss = 1.22, batch loss = 0.54 (13.1 examples/sec; 0.613 sec/batch; 56h:34m:33s remains)
INFO - root - 2017-12-15 09:23:31.795417: step 200, loss = 0.94, batch loss = 0.36 (12.9 examples/sec; 0.621 sec/batch; 57h:20m:12s remains)
2017-12-15 09:23:32.401393: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.7045722 -0.57220006 -0.532712 -0.54744196 -0.60379004 -0.72778654 -0.87576509 -1.0173447 -1.0988624 -1.6180236 -2.4319949 -3.058382 -3.9021626 -4.418467 -4.7597756][-0.30187225 -0.13059521 -0.012592793 0.079755306 0.15593243 0.18899918 0.1259141 -0.056591034 -0.18317842 -0.74327946 -1.5778654 -2.28328 -3.1645298 -3.7189746 -4.07399][-0.052327156 0.19259882 0.42352247 0.60000992 0.71848059 0.71696615 0.64761877 0.45899916 0.29379511 -0.32385278 -1.1453311 -1.8487122 -2.7089982 -3.3050749 -3.6851232][-0.031841755 0.28342724 0.64994335 1.0199761 1.3070555 1.4811201 1.5144377 1.3265657 1.1630783 0.56955671 -0.26311851 -1.0259583 -1.9151747 -2.5330019 -2.94562][-0.064557552 0.35080051 0.82314777 1.3974829 1.8228536 2.0817757 2.2281704 2.1075511 1.9607501 1.3928957 0.58683443 -0.17106581 -1.0672617 -1.7693584 -2.2162549][-0.16894674 0.31071281 0.76711941 1.3577809 1.9168215 2.2965326 2.4871225 2.4116983 2.3005514 1.7329845 0.9078908 0.0862031 -0.84391022 -1.5753608 -2.0782244][-0.18521357 0.3394556 0.84855556 1.3979149 1.8844252 2.2081752 2.4234486 2.3861518 2.3435173 1.8830853 1.1078129 0.30639553 -0.645046 -1.4314506 -1.9455347][-0.18550968 0.24889278 0.691453 1.3050652 1.8575196 2.2668662 2.4323616 2.3502207 2.3009462 1.7787337 0.9445076 0.15957165 -0.77659965 -1.518338 -2.0144868][-0.27706933 0.17798424 0.60175943 1.1446562 1.7203946 2.1067615 2.3243351 2.2596855 2.1442637 1.6450782 0.8058362 -0.048456669 -1.0652847 -1.8937526 -2.4119821][-1.0328035 -0.68569279 -0.37013984 0.11226749 0.57400894 0.89843082 1.1120319 1.1158524 1.0990148 0.67033911 0.04791069 -0.63981676 -1.4977345 -2.1654119 -2.6225796][-2.207649 -1.938103 -1.6553364 -1.2694492 -0.88067436 -0.59874916 -0.34358191 -0.3133862 -0.30731297 -0.58670545 -1.0938358 -1.6066084 -2.1900027 -2.6183805 -2.853672][-2.9837313 -2.7838817 -2.5983911 -2.2560787 -1.8420885 -1.5625656 -1.2593219 -1.1725302 -1.1027422 -1.2572134 -1.5391483 -1.8157144 -2.21574 -2.4301598 -2.5099015][-3.6545625 -3.513293 -3.3697734 -3.1039696 -2.7503715 -2.4958887 -2.2017057 -2.1222904 -1.9903886 -2.0799472 -2.1446488 -2.1116848 -2.1805904 -2.1904023 -2.1280444][-3.9209466 -3.813756 -3.7791457 -3.5732932 -3.2667251 -3.0519876 -2.7783246 -2.6923392 -2.5620637 -2.5225868 -2.4284015 -2.2442265 -2.0707259 -1.8672192 -1.7895889][-3.8642311 -3.6716876 -3.5911288 -3.3831644 -3.1429021 -2.9466629 -2.7128105 -2.6496725 -2.5398502 -2.4343424 -2.3050432 -2.1553071 -2.0085783 -1.7848873 -1.6667202]]...]
INFO - root - 2017-12-15 09:23:38.714905: step 210, loss = 0.98, batch loss = 0.38 (12.4 examples/sec; 0.645 sec/batch; 59h:30m:24s remains)
INFO - root - 2017-12-15 09:23:45.021289: step 220, loss = 0.94, batch loss = 0.33 (12.6 examples/sec; 0.633 sec/batch; 58h:23m:20s remains)
INFO - root - 2017-12-15 09:23:51.378726: step 230, loss = 1.01, batch loss = 0.39 (12.8 examples/sec; 0.627 sec/batch; 57h:51m:33s remains)
INFO - root - 2017-12-15 09:23:57.780035: step 240, loss = 0.87, batch loss = 0.31 (12.5 examples/sec; 0.638 sec/batch; 58h:54m:06s remains)
INFO - root - 2017-12-15 09:24:04.131488: step 250, loss = 0.89, batch loss = 0.32 (12.3 examples/sec; 0.652 sec/batch; 60h:11m:34s remains)
INFO - root - 2017-12-15 09:24:10.486085: step 260, loss = 0.89, batch loss = 0.27 (12.6 examples/sec; 0.633 sec/batch; 58h:22m:41s remains)
INFO - root - 2017-12-15 09:24:16.857819: step 270, loss = 0.90, batch loss = 0.35 (12.9 examples/sec; 0.622 sec/batch; 57h:26m:22s remains)
INFO - root - 2017-12-15 09:24:23.167078: step 280, loss = 0.94, batch loss = 0.38 (12.8 examples/sec; 0.627 sec/batch; 57h:49m:37s remains)
INFO - root - 2017-12-15 09:24:29.482297: step 290, loss = 0.84, batch loss = 0.28 (12.7 examples/sec; 0.632 sec/batch; 58h:18m:01s remains)
INFO - root - 2017-12-15 09:24:35.929792: step 300, loss = 0.93, batch loss = 0.32 (13.0 examples/sec; 0.617 sec/batch; 56h:55m:14s remains)
2017-12-15 09:24:36.457709: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9698811 -3.1499858 -3.3555162 -2.994447 -3.0480838 -3.1448996 -3.1379576 -2.9223175 -2.6883023 -2.4195383 -2.5840847 -3.05511 -3.8861239 -4.2047191 -4.4056664][-3.1023431 -3.5826445 -3.7597213 -3.5695114 -3.613745 -3.6088462 -3.501235 -3.5071754 -3.1940053 -3.1305585 -3.63198 -3.8093984 -4.0807505 -4.2173352 -4.2733183][-2.6648903 -2.9396977 -3.6527033 -3.718153 -3.7325006 -3.6921372 -3.5923762 -3.3587227 -3.1341219 -3.3129735 -3.6620016 -3.6385937 -3.8186336 -4.2935467 -4.4428554][-2.8945141 -3.0220506 -3.4139948 -3.1739531 -3.2029102 -3.0416155 -2.9262164 -2.8087192 -2.7252483 -2.8581734 -2.9188514 -3.0997434 -3.5036445 -3.6693401 -4.0431132][-2.4965992 -2.3758323 -2.4788933 -2.3415163 -2.4262972 -2.0772924 -2.0264008 -1.8547411 -1.6074235 -1.791121 -2.3996692 -2.6455016 -2.634799 -2.796124 -3.11033][-2.2608991 -1.853672 -1.9356782 -1.583662 -1.4736645 -1.167259 -1.0182135 -1.0493803 -0.82392311 -1.0245676 -1.5426342 -1.785913 -2.326427 -2.4708958 -2.5090961][-2.0626435 -1.5472996 -1.0876918 -0.70210576 -0.31460953 0.2192564 0.46562243 0.14258957 0.084170341 -0.090905666 -0.74733734 -1.2329621 -1.7494471 -1.7580132 -1.8175857][-2.0428786 -1.3454466 -0.93775606 0.066564083 0.7434144 1.1059146 1.2145882 0.87195253 0.524735 0.18839931 -0.4144578 -0.72771931 -1.3394074 -1.6393366 -1.7199221][-1.9661951 -1.20613 -0.72770405 0.27479267 1.0044727 1.7325315 1.9173312 1.330719 0.77042532 0.33653021 -0.39810944 -0.8829298 -1.4101884 -1.6964514 -1.8716848][-2.4770813 -1.3785892 -0.77915978 -0.10105944 0.39206505 1.1501784 1.3336263 0.98593664 0.58281994 0.020339012 -0.78118658 -1.1110885 -1.3772194 -1.6628199 -2.0318196][-2.7542853 -1.7813938 -1.5564008 -0.52725792 0.2941184 0.64187431 0.53826809 0.51625443 0.2095108 -0.27250957 -0.87541366 -1.2062075 -1.5597322 -1.7343259 -2.1625011][-3.439538 -2.476805 -1.9685278 -1.1327789 -0.65954328 -0.0046577454 0.27781296 -0.20895863 -0.734781 -0.70250177 -1.0501907 -1.5152063 -2.0267625 -2.4485331 -2.3821766][-3.99126 -3.1025517 -2.673048 -1.8857296 -1.7494009 -1.3843505 -1.102211 -1.1712739 -1.3217244 -1.5825183 -1.7830853 -1.6773698 -2.0017703 -2.2520652 -2.604557][-3.7993419 -3.1668653 -2.7518497 -2.1906755 -2.0033293 -1.9647012 -2.0710862 -1.6982234 -1.511194 -1.6256404 -1.63257 -1.7575023 -1.8970418 -2.0206821 -2.4987171][-3.2859132 -2.5696504 -2.4175119 -2.038626 -1.873421 -1.3885627 -1.3819911 -1.6103697 -1.2908635 -0.74498892 -0.75335646 -1.0766163 -1.7526555 -2.3036945 -2.4428935]]...]
INFO - root - 2017-12-15 09:24:42.739289: step 310, loss = 1.01, batch loss = 0.43 (12.5 examples/sec; 0.639 sec/batch; 58h:55m:04s remains)
INFO - root - 2017-12-15 09:24:49.105927: step 320, loss = 0.91, batch loss = 0.38 (12.2 examples/sec; 0.653 sec/batch; 60h:15m:47s remains)
INFO - root - 2017-12-15 09:24:55.502215: step 330, loss = 0.88, batch loss = 0.34 (12.5 examples/sec; 0.642 sec/batch; 59h:14m:13s remains)
INFO - root - 2017-12-15 09:25:01.819234: step 340, loss = 0.82, batch loss = 0.29 (12.5 examples/sec; 0.638 sec/batch; 58h:49m:28s remains)
INFO - root - 2017-12-15 09:25:08.123388: step 350, loss = 0.85, batch loss = 0.32 (12.4 examples/sec; 0.646 sec/batch; 59h:37m:54s remains)
INFO - root - 2017-12-15 09:25:14.557856: step 360, loss = 0.88, batch loss = 0.36 (12.5 examples/sec; 0.638 sec/batch; 58h:49m:27s remains)
INFO - root - 2017-12-15 09:25:20.919923: step 370, loss = 0.86, batch loss = 0.33 (12.1 examples/sec; 0.661 sec/batch; 60h:58m:20s remains)
INFO - root - 2017-12-15 09:25:27.320671: step 380, loss = 0.83, batch loss = 0.33 (12.7 examples/sec; 0.629 sec/batch; 58h:00m:35s remains)
INFO - root - 2017-12-15 09:25:33.639166: step 390, loss = 1.20, batch loss = 0.66 (12.6 examples/sec; 0.635 sec/batch; 58h:34m:55s remains)
INFO - root - 2017-12-15 09:25:40.028386: step 400, loss = 0.79, batch loss = 0.31 (12.3 examples/sec; 0.648 sec/batch; 59h:48m:09s remains)
2017-12-15 09:25:40.577078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8674331 -3.7845302 -3.794847 -3.3596225 -3.011173 -2.9956422 -3.0758679 -2.8203268 -2.8825231 -3.5446029 -3.7848032 -3.9614558 -4.5268126 -4.2144527 -4.0514097][-3.7605593 -3.442132 -3.0853887 -2.619339 -2.2063909 -1.9683194 -1.6262212 -1.3953228 -1.8145416 -2.8383784 -3.276567 -3.5477321 -4.0452924 -3.57584 -3.4559131][-3.3043628 -2.8171532 -2.6960225 -2.4285138 -1.7589362 -1.4239063 -1.0184331 -0.85207248 -1.1052256 -1.8199706 -2.590245 -3.3553176 -4.1446943 -3.7990527 -3.4859786][-4.3415828 -3.8132942 -3.4086654 -2.8647287 -2.170779 -1.6728251 -1.2277186 -1.0943615 -1.16417 -1.5207832 -1.8608925 -2.483485 -3.3322806 -3.1962507 -3.063602][-4.9862609 -4.4822612 -3.9316404 -3.4174218 -2.8220375 -2.4316733 -1.8046534 -1.6003273 -1.5042281 -1.463166 -1.3373873 -1.3770952 -1.8048925 -1.8490162 -1.754252][-4.4036551 -3.9053011 -3.9477232 -3.3758855 -2.7307396 -2.4981203 -2.1512942 -1.7527816 -1.1703188 -1.0131137 -0.91120768 -0.83741927 -0.87763476 -0.95004559 -1.1105247][-4.4706764 -4.2386374 -4.3042827 -3.649235 -3.004621 -2.4965386 -2.186317 -1.8411696 -1.2528858 -0.63859129 -0.078607082 0.11610508 -0.21930265 -0.083508968 -0.084264278][-4.2198677 -4.1975646 -4.3922267 -3.967808 -3.5262578 -2.9551015 -2.3126106 -2.1001875 -1.7545516 -1.1695855 -0.42123413 0.076528549 -0.052546978 -0.048654079 -0.48572111][-4.3720021 -4.0315189 -4.103548 -3.9482644 -3.7254884 -3.2196867 -2.6385007 -2.0067871 -1.1800594 -0.85905004 -0.47036409 -0.66011286 -1.076859 -0.94205165 -1.3589113][-4.8801303 -4.96757 -5.0574064 -4.4291511 -3.5031719 -2.770905 -2.2548914 -1.7335255 -1.4539649 -1.3229582 -1.1742496 -1.29846 -1.6707261 -1.8356822 -1.7920325][-5.5630283 -5.3512092 -5.3095255 -5.0565839 -4.6046462 -3.6780221 -2.8223162 -2.6882594 -2.5350604 -2.3409963 -2.3363621 -2.4563882 -2.8763266 -2.5188565 -2.1748643][-5.4571557 -5.1886444 -5.016295 -4.5499239 -3.9032362 -3.9200733 -3.7627635 -3.2874532 -3.07374 -2.8273883 -2.6432769 -3.016458 -3.4398417 -2.9255562 -2.7292261][-5.2943983 -5.3187675 -5.3630481 -4.6098523 -4.1458058 -4.1162472 -4.1091213 -4.587676 -4.7019992 -4.1766176 -3.6484218 -3.1558895 -3.1792648 -2.83779 -2.5252519][-4.4079838 -4.0942535 -4.2856412 -4.2832417 -4.2933064 -4.1679168 -3.9782407 -4.2402287 -4.312911 -3.8053784 -3.2505977 -3.0870717 -2.9738379 -2.6630807 -2.9625099][-3.0105414 -2.9671264 -3.0731995 -2.8890405 -2.904439 -2.977365 -2.7277784 -2.8796966 -2.8264725 -2.4577441 -2.3301194 -2.541733 -2.8561578 -3.0663497 -3.2518492]]...]
INFO - root - 2017-12-15 09:25:47.007684: step 410, loss = 0.76, batch loss = 0.28 (12.9 examples/sec; 0.618 sec/batch; 56h:59m:16s remains)
INFO - root - 2017-12-15 09:25:53.410243: step 420, loss = 0.71, batch loss = 0.25 (12.1 examples/sec; 0.660 sec/batch; 60h:54m:15s remains)
INFO - root - 2017-12-15 09:25:59.802767: step 430, loss = 0.67, batch loss = 0.22 (12.3 examples/sec; 0.653 sec/batch; 60h:11m:54s remains)
INFO - root - 2017-12-15 09:26:06.127933: step 440, loss = 0.72, batch loss = 0.26 (12.7 examples/sec; 0.630 sec/batch; 58h:07m:14s remains)
INFO - root - 2017-12-15 09:26:12.472528: step 450, loss = 0.78, batch loss = 0.27 (12.2 examples/sec; 0.658 sec/batch; 60h:38m:45s remains)
INFO - root - 2017-12-15 09:26:18.775569: step 460, loss = 0.82, batch loss = 0.30 (12.8 examples/sec; 0.624 sec/batch; 57h:34m:58s remains)
INFO - root - 2017-12-15 09:26:25.108289: step 470, loss = 0.84, batch loss = 0.35 (12.5 examples/sec; 0.642 sec/batch; 59h:11m:27s remains)
INFO - root - 2017-12-15 09:26:31.442260: step 480, loss = 0.79, batch loss = 0.22 (12.7 examples/sec; 0.630 sec/batch; 58h:06m:57s remains)
INFO - root - 2017-12-15 09:26:37.762985: step 490, loss = 0.74, batch loss = 0.31 (12.8 examples/sec; 0.623 sec/batch; 57h:25m:25s remains)
INFO - root - 2017-12-15 09:26:44.111439: step 500, loss = 0.79, batch loss = 0.28 (12.6 examples/sec; 0.636 sec/batch; 58h:37m:39s remains)
2017-12-15 09:26:44.685331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6105065 -3.406441 -3.3645935 -2.9966295 -2.2945518 -1.8378789 -1.5334127 -1.4565284 -1.4177864 -2.0688155 -3.281054 -4.5858097 -6.2227163 -7.1119752 -7.3462954][-4.0875316 -4.2325635 -3.7253439 -3.1371658 -2.9217956 -2.3322012 -1.6489396 -1.5174687 -1.4661069 -2.2710297 -3.639714 -4.9203973 -6.5345778 -7.6084089 -7.8666811][-3.7020655 -3.8264382 -3.2017918 -2.2576592 -1.9734747 -1.1490233 -0.1619525 -0.024659634 0.046545506 -0.78220987 -2.2238729 -3.7260108 -5.3590665 -6.4679327 -6.9798417][-3.5135145 -3.1048093 -2.7515275 -1.7032106 -0.78285217 0.069245815 1.0343323 1.132297 1.1774635 0.39405251 -1.0467677 -2.5734847 -4.3749814 -5.42965 -5.545177][-3.3174367 -2.8592839 -2.5186594 -1.179481 0.036011219 1.0849838 2.1086659 2.1257 2.0914555 1.3166442 0.07866478 -1.5426507 -3.424679 -4.5387664 -4.835012][-3.6420012 -2.6823003 -1.9231925 -0.54412556 0.98698235 2.2017946 3.2522111 3.3347039 3.2986588 2.4116249 1.1653013 -0.19493246 -1.8530886 -3.1078997 -3.9883924][-3.4284234 -2.8601274 -2.1319811 -0.6853199 0.91486883 2.1297979 3.1946635 3.1624408 3.0675683 2.28403 0.92671537 -0.51975584 -1.9312129 -2.9783175 -3.5267296][-3.4669967 -2.8412945 -2.1443236 -0.6486578 0.95195866 2.0978589 3.1479535 3.1434617 3.0483556 2.1685247 0.87457561 -0.62569952 -2.555685 -3.3689623 -3.562479][-3.6283379 -2.9504511 -2.0334764 -0.48986673 1.0605097 2.0932279 3.1345534 3.1222081 3.0507197 2.2091475 0.82854843 -0.60664463 -2.3277223 -3.6877379 -4.2955251][-5.2621336 -4.5921154 -3.6688221 -2.1920774 -0.63638449 0.3087163 1.259069 1.2146316 1.1464157 0.44583082 -0.6769948 -1.7781317 -2.9968348 -3.7442188 -4.2900157][-6.7555056 -6.0443888 -5.1753592 -3.78529 -2.188432 -1.1270778 -0.13482952 -0.21181345 -0.32345057 -0.9939754 -2.0459204 -2.9899344 -4.1015944 -4.8550892 -4.9572558][-8.320384 -7.64482 -6.7151437 -5.2980685 -3.7828813 -2.6108453 -1.5718799 -1.622232 -1.6765444 -2.0958247 -2.7113428 -3.2183111 -4.0840526 -4.4667606 -4.0520787][-9.1300316 -8.4254627 -7.4580312 -5.9333253 -4.4200182 -3.3432527 -2.332165 -2.3223989 -2.3481748 -2.5399718 -2.7209582 -2.8417168 -3.2700768 -3.4833114 -3.1888697][-9.060154 -8.3771973 -7.2792368 -5.6522536 -4.2035294 -3.1619186 -2.1202164 -2.1168189 -2.1257298 -2.2911499 -2.512521 -2.4778712 -2.3464041 -2.3861592 -2.1448886][-8.4131565 -7.5362387 -6.4361706 -4.9855976 -3.654202 -2.688513 -1.6347106 -1.6017036 -1.5918736 -1.5539348 -1.5603449 -1.5296943 -1.5117335 -1.3718665 -0.81062794]]...]
INFO - root - 2017-12-15 09:26:51.056042: step 510, loss = 0.65, batch loss = 0.19 (13.0 examples/sec; 0.617 sec/batch; 56h:51m:12s remains)
INFO - root - 2017-12-15 09:26:57.372068: step 520, loss = 0.70, batch loss = 0.31 (12.7 examples/sec; 0.628 sec/batch; 57h:55m:59s remains)
INFO - root - 2017-12-15 09:27:03.729496: step 530, loss = 0.63, batch loss = 0.21 (12.6 examples/sec; 0.634 sec/batch; 58h:28m:11s remains)
INFO - root - 2017-12-15 09:27:10.065345: step 540, loss = 0.69, batch loss = 0.26 (12.9 examples/sec; 0.620 sec/batch; 57h:08m:38s remains)
INFO - root - 2017-12-15 09:27:16.369162: step 550, loss = 0.64, batch loss = 0.23 (12.7 examples/sec; 0.632 sec/batch; 58h:17m:57s remains)
INFO - root - 2017-12-15 09:27:22.714719: step 560, loss = 0.88, batch loss = 0.38 (12.2 examples/sec; 0.654 sec/batch; 60h:16m:32s remains)
INFO - root - 2017-12-15 09:27:29.080226: step 570, loss = 0.78, batch loss = 0.38 (12.5 examples/sec; 0.639 sec/batch; 58h:54m:54s remains)
INFO - root - 2017-12-15 09:27:35.478640: step 580, loss = 0.70, batch loss = 0.25 (12.2 examples/sec; 0.658 sec/batch; 60h:37m:37s remains)
INFO - root - 2017-12-15 09:27:41.828411: step 590, loss = 0.75, batch loss = 0.33 (12.6 examples/sec; 0.635 sec/batch; 58h:30m:51s remains)
INFO - root - 2017-12-15 09:27:48.162276: step 600, loss = 0.78, batch loss = 0.38 (12.6 examples/sec; 0.636 sec/batch; 58h:40m:01s remains)
2017-12-15 09:27:48.691037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8750455 -2.0781102 -1.9364576 -0.86865187 -0.35026503 -0.11595058 0.11656332 -0.077900887 -0.68785 -1.8726261 -3.3645744 -4.6942492 -6.191864 -6.319932 -5.6885085][-2.8714957 -2.4446106 -2.1642132 -1.7103002 -1.679615 -1.2416496 -0.8439126 -0.91699648 -1.0449243 -2.6121607 -4.3897943 -5.4395981 -6.5569921 -6.4117632 -5.4070821][-3.3419132 -3.1062646 -2.7590313 -2.1836128 -1.8990481 -1.1898525 -0.43261576 -0.5632782 -0.61047983 -1.9145885 -3.569207 -4.8349695 -6.2308574 -6.4971843 -5.8470874][-4.2058864 -3.786315 -3.5716136 -2.9088242 -2.1458793 -1.2764904 -0.39479542 -0.50544071 -0.46069813 -1.563297 -2.9929006 -4.5920806 -6.6335516 -6.786808 -5.9679017][-4.6023989 -4.5191298 -4.3693185 -3.1978209 -1.7265174 -0.80234957 0.0013637543 -0.031006336 -0.044597149 -0.9829545 -2.2571108 -3.9454811 -5.9042864 -6.752501 -6.3819017][-5.554739 -5.0117435 -4.0782094 -2.6097059 -1.0734124 0.030360222 0.9482522 0.98290253 1.0210638 -0.15619564 -1.7558856 -3.2215223 -4.6745124 -5.3927164 -5.0607691][-6.0058036 -5.443594 -4.560986 -2.7663918 -1.2108741 -0.14684486 0.96382046 1.0066137 0.99360847 -0.13381577 -1.7389209 -3.2065992 -4.6336355 -5.3455539 -5.2673092][-5.9458866 -5.3711233 -4.7204642 -3.2782054 -1.8744473 -0.6519928 0.56075525 0.73188686 0.86700964 -0.18533611 -1.7300932 -3.45851 -5.2450266 -5.156662 -4.5565739][-6.2217765 -5.7960181 -4.9763741 -3.4614239 -2.2367468 -1.2643299 -0.20264101 0.22652102 0.56208181 -0.57012558 -2.0872545 -3.606854 -5.169672 -5.2033429 -4.7489982][-7.5325546 -7.2247748 -6.4867396 -4.9822731 -3.5622509 -2.4712617 -1.2295289 -1.1831884 -1.1680639 -2.08797 -3.4242864 -4.5163465 -5.2844081 -5.1970348 -4.6384377][-9.6715374 -9.2734547 -8.5746622 -7.1588144 -5.6091194 -4.2180781 -2.7526338 -2.5814009 -2.4595668 -3.4155786 -4.7592831 -5.6102357 -6.31288 -6.0585175 -4.7909641][-10.750958 -10.478365 -9.5519447 -7.7539034 -6.29935 -4.9142661 -3.5129254 -3.3402884 -3.2557797 -4.0648308 -4.9795442 -5.2034678 -5.7307472 -5.6672249 -4.2433724][-11.968428 -11.652132 -10.890407 -9.1120033 -6.9890938 -5.3887339 -4.3492351 -4.237433 -3.858974 -4.6451011 -5.688673 -5.6755548 -5.5863452 -4.682189 -2.9129055][-12.052166 -11.398512 -10.595835 -8.88904 -6.913012 -5.44133 -4.1631608 -4.1484952 -4.0551362 -4.6394091 -5.2731905 -4.7242551 -4.1055303 -3.5024927 -2.3010733][-10.569996 -9.8222427 -8.6661358 -6.8283391 -5.07222 -3.9047289 -2.7229848 -2.6162322 -2.4698911 -2.7208102 -3.1176965 -3.0281343 -2.74573 -1.6130319 -0.035381794]]...]
INFO - root - 2017-12-15 09:27:55.071854: step 610, loss = 0.67, batch loss = 0.23 (12.1 examples/sec; 0.663 sec/batch; 61h:08m:28s remains)
INFO - root - 2017-12-15 09:28:01.327366: step 620, loss = 0.65, batch loss = 0.21 (12.6 examples/sec; 0.634 sec/batch; 58h:25m:35s remains)
INFO - root - 2017-12-15 09:28:07.698203: step 630, loss = 0.70, batch loss = 0.21 (12.6 examples/sec; 0.635 sec/batch; 58h:33m:37s remains)
INFO - root - 2017-12-15 09:28:14.014684: step 640, loss = 0.66, batch loss = 0.26 (12.7 examples/sec; 0.631 sec/batch; 58h:10m:22s remains)
INFO - root - 2017-12-15 09:28:20.332435: step 650, loss = 0.79, batch loss = 0.33 (12.8 examples/sec; 0.623 sec/batch; 57h:25m:14s remains)
INFO - root - 2017-12-15 09:28:26.727903: step 660, loss = 0.76, batch loss = 0.35 (12.6 examples/sec; 0.635 sec/batch; 58h:32m:38s remains)
INFO - root - 2017-12-15 09:28:32.970734: step 670, loss = 0.59, batch loss = 0.18 (12.8 examples/sec; 0.624 sec/batch; 57h:29m:51s remains)
INFO - root - 2017-12-15 09:28:39.359394: step 680, loss = 0.71, batch loss = 0.35 (12.8 examples/sec; 0.623 sec/batch; 57h:23m:32s remains)
INFO - root - 2017-12-15 09:28:45.622307: step 690, loss = 0.63, batch loss = 0.20 (12.8 examples/sec; 0.624 sec/batch; 57h:32m:28s remains)
INFO - root - 2017-12-15 09:28:51.760544: step 700, loss = 0.59, batch loss = 0.18 (13.0 examples/sec; 0.614 sec/batch; 56h:35m:53s remains)
2017-12-15 09:28:52.209820: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.018344879 -0.7732234 -2.04355 -2.1228931 -1.7809424 -2.0455246 -2.3453915 -2.3856556 -2.3130081 -3.2354975 -4.0755243 -5.4624534 -5.7047896 -5.8266182 -6.5739756][-1.0894732 -1.4054847 -2.084996 -2.5894129 -3.2368233 -3.4755991 -3.0038769 -2.9701214 -2.8284583 -3.606513 -4.369163 -5.6790595 -6.4619722 -6.5174217 -6.7332435][-0.32888746 -0.98445415 -2.2146134 -2.7868421 -3.4771459 -3.5991657 -3.122704 -3.0246513 -2.5453677 -3.1246727 -3.7358921 -5.0935907 -5.8128743 -5.3181639 -5.9541721][0.32459974 -1.0285153 -2.1725829 -2.499754 -3.0769095 -2.8551347 -1.8586576 -1.6606786 -1.4446301 -2.24679 -2.8215251 -4.110528 -4.8315206 -4.5869665 -4.95166][0.75415373 -0.88101006 -1.9564416 -2.0239422 -2.0680549 -1.5202019 -0.48728848 -0.14618921 0.44425011 -0.59944439 -1.7708793 -2.6184721 -3.185179 -3.45755 -4.0515246][-0.18285704 -0.81357431 -0.8921771 -0.37334156 0.080240726 0.80741549 2.1599379 2.4444857 2.899879 2.0066204 0.9247756 -0.28954077 -1.3786798 -1.4738328 -2.0921481][-0.60962963 -1.2243457 -1.0034313 -0.11884451 1.1049337 2.0948596 3.552794 3.6687293 3.9036002 2.8385949 1.5496402 -0.24258089 -1.6808164 -2.2066987 -2.9260731][-0.6990428 -0.72153282 -0.080723286 0.94671535 2.0071769 2.8127952 4.318996 4.5075889 4.607368 3.2400937 1.6774778 -0.16254663 -1.6693132 -2.1805117 -2.5088446][-1.3728278 -1.2446871 -0.58910751 0.50989771 1.5625391 2.4505796 3.9482026 4.1682582 4.2198482 2.682281 0.90679216 -0.47007322 -1.5747483 -1.3958306 -1.4592834][-3.0044687 -3.2896948 -2.7821662 -1.6700933 -0.62643337 0.28195143 1.8052726 1.7486811 1.6942468 0.059315681 -1.5447738 -2.3685358 -2.7078927 -1.9467797 -1.856674][-3.8415833 -4.1476097 -3.7721171 -2.9489191 -2.0448687 -1.2468152 0.19711637 -0.020186424 -0.31847906 -1.9099476 -3.1976376 -4.0771971 -4.1964917 -3.315856 -2.3281069][-5.3591623 -5.5474749 -5.5551324 -4.8307409 -4.1413069 -3.0461147 -1.2128634 -1.5976973 -2.0884764 -3.3221655 -4.4494658 -4.7787209 -4.8101549 -4.0667496 -2.8411241][-6.544013 -6.8324156 -6.9771523 -6.1949682 -5.6754832 -4.6279407 -2.8092988 -2.0821013 -1.5175266 -3.2633717 -5.1376467 -5.0217037 -5.2152481 -4.7702107 -3.5043743][-6.9230452 -7.3519092 -7.6386218 -7.0228219 -6.6649442 -5.3128071 -3.2750614 -3.5588875 -3.7035189 -3.9436507 -4.42356 -4.1256609 -3.8858633 -3.2030814 -2.6918252][-7.3619585 -7.2622895 -7.1494083 -6.6146626 -6.3631592 -4.6954412 -2.9087894 -2.7401049 -2.59345 -3.6331115 -4.4372697 -3.4175296 -2.4230957 -2.2059774 -1.7075408]]...]
INFO - root - 2017-12-15 09:28:58.353607: step 710, loss = 0.65, batch loss = 0.24 (13.3 examples/sec; 0.602 sec/batch; 55h:29m:07s remains)
INFO - root - 2017-12-15 09:29:04.503943: step 720, loss = 0.64, batch loss = 0.24 (12.8 examples/sec; 0.627 sec/batch; 57h:46m:04s remains)
INFO - root - 2017-12-15 09:29:10.656697: step 730, loss = 0.56, batch loss = 0.18 (12.7 examples/sec; 0.630 sec/batch; 58h:01m:36s remains)
INFO - root - 2017-12-15 09:29:16.799495: step 740, loss = 0.56, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 57h:21m:38s remains)
INFO - root - 2017-12-15 09:29:22.960753: step 750, loss = 0.61, batch loss = 0.23 (13.1 examples/sec; 0.609 sec/batch; 56h:07m:36s remains)
INFO - root - 2017-12-15 09:29:29.154911: step 760, loss = 0.66, batch loss = 0.26 (13.0 examples/sec; 0.614 sec/batch; 56h:37m:23s remains)
INFO - root - 2017-12-15 09:29:35.334527: step 770, loss = 0.55, batch loss = 0.20 (13.1 examples/sec; 0.610 sec/batch; 56h:12m:11s remains)
INFO - root - 2017-12-15 09:29:41.493872: step 780, loss = 0.52, batch loss = 0.15 (12.4 examples/sec; 0.645 sec/batch; 59h:28m:35s remains)
INFO - root - 2017-12-15 09:29:47.634236: step 790, loss = 0.50, batch loss = 0.15 (13.2 examples/sec; 0.606 sec/batch; 55h:49m:53s remains)
INFO - root - 2017-12-15 09:29:53.794934: step 800, loss = 0.63, batch loss = 0.23 (13.1 examples/sec; 0.612 sec/batch; 56h:20m:41s remains)
2017-12-15 09:29:54.273437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4926934 -2.4357698 -2.3348002 -3.0579147 -3.4894991 -3.2641129 -2.7577357 -2.749213 -3.3088923 -4.1785131 -6.4706087 -7.519289 -8.704483 -8.9605913 -8.4058628][-2.1000597 -1.9321675 -1.7769868 -2.0886478 -2.127759 -1.9219351 -1.6450756 -1.5325284 -1.2710748 -2.5313618 -5.3030677 -6.6126642 -7.4398613 -6.9134612 -6.1318808][-1.62609 -1.9545331 -1.0070748 -1.1445541 -1.3367486 -0.98294687 -0.50261593 -0.54683876 -0.58264017 -1.6688008 -4.1237016 -5.8132343 -7.2395387 -7.0219688 -4.9468141][-1.0826001 -0.79742765 -0.5704484 -0.57965755 0.63642216 0.48557663 0.56074953 0.75869036 0.77526 -0.83772326 -3.6251216 -4.9291039 -6.1368756 -5.9056582 -4.5667562][-1.6892233 -0.74294758 0.085396767 0.13621855 0.41673946 1.154315 1.9770465 1.7073469 1.5553699 0.23346615 -2.6925323 -4.0436096 -5.0618467 -4.6082759 -3.0600789][-2.8577068 -2.2755327 -0.977056 0.10954285 1.1239691 1.7945933 2.0731039 1.9932194 1.8937607 0.2992897 -2.2463126 -3.1767049 -4.2938924 -3.8685474 -2.330209][-3.2402594 -2.4622307 -2.469368 -0.63109636 0.8197875 1.942903 2.5091276 2.4907246 2.1917615 0.15006208 -2.4939733 -3.1328707 -3.9755516 -2.8836188 -1.6163273][-4.1419182 -3.05798 -2.0791872 -0.8463006 0.0271039 1.2020597 1.6835232 1.9998055 2.1347342 0.34915495 -2.0132473 -2.9802144 -4.1308851 -3.7449121 -2.4089835][-3.717968 -2.8109086 -1.7507164 -0.302814 1.0062103 1.8409038 2.0505252 1.9948964 1.911747 0.18696928 -2.5828338 -3.5179806 -4.1627054 -4.1698174 -3.1900601][-4.2344 -4.0115142 -3.1543977 -1.6888256 -0.45989323 1.0740952 1.5731101 1.2338815 0.76594782 -0.80098581 -2.9659102 -3.4175694 -3.6397159 -3.4827909 -2.59359][-5.7151566 -4.9478526 -4.4553056 -3.0407805 -1.4960113 0.13928747 0.70529032 0.19339037 -0.46959162 -1.9600685 -3.9118149 -4.0759754 -3.8324587 -3.1146848 -2.7132025][-6.1211877 -5.5186477 -4.1740665 -2.7190595 -1.2937064 0.044197083 0.74156046 0.44941759 -0.001347065 -1.4451046 -2.710458 -3.1847947 -3.8841491 -3.2973502 -2.3319361][-4.6251369 -4.679821 -3.6987967 -2.3926778 -0.74557066 0.32819271 0.7768774 0.31310177 -0.994226 -2.5065305 -3.8380694 -3.6337163 -4.3999624 -4.684432 -3.851562][-4.1196003 -3.6923022 -2.5883582 -1.0506449 0.70961475 1.9222302 2.5379477 1.9511971 1.985404 0.6998806 -2.2403643 -3.3767583 -4.2840242 -3.8232279 -3.8772643][-5.9929948 -5.6096082 -4.7227063 -3.1602697 -1.0679693 0.70530081 1.2220893 0.80852318 -0.30747795 -1.2523236 -1.3789082 -1.7263331 -3.0564823 -3.2749062 -3.6761742]]...]
INFO - root - 2017-12-15 09:30:00.432558: step 810, loss = 0.61, batch loss = 0.17 (13.3 examples/sec; 0.603 sec/batch; 55h:32m:24s remains)
INFO - root - 2017-12-15 09:30:06.625831: step 820, loss = 0.60, batch loss = 0.21 (12.9 examples/sec; 0.619 sec/batch; 57h:03m:43s remains)
INFO - root - 2017-12-15 09:30:12.828013: step 830, loss = 0.62, batch loss = 0.23 (12.9 examples/sec; 0.619 sec/batch; 57h:04m:26s remains)
INFO - root - 2017-12-15 09:30:19.021867: step 840, loss = 0.57, batch loss = 0.20 (12.7 examples/sec; 0.627 sec/batch; 57h:48m:34s remains)
INFO - root - 2017-12-15 09:30:25.187765: step 850, loss = 0.52, batch loss = 0.18 (12.7 examples/sec; 0.629 sec/batch; 57h:55m:10s remains)
INFO - root - 2017-12-15 09:30:31.302542: step 860, loss = 0.57, batch loss = 0.15 (13.0 examples/sec; 0.615 sec/batch; 56h:38m:01s remains)
INFO - root - 2017-12-15 09:30:37.393621: step 870, loss = 0.62, batch loss = 0.24 (13.2 examples/sec; 0.605 sec/batch; 55h:46m:27s remains)
INFO - root - 2017-12-15 09:30:43.476051: step 880, loss = 0.56, batch loss = 0.20 (13.3 examples/sec; 0.603 sec/batch; 55h:34m:17s remains)
INFO - root - 2017-12-15 09:30:49.598543: step 890, loss = 0.56, batch loss = 0.16 (13.2 examples/sec; 0.605 sec/batch; 55h:44m:22s remains)
INFO - root - 2017-12-15 09:30:55.760362: step 900, loss = 0.59, batch loss = 0.19 (13.3 examples/sec; 0.601 sec/batch; 55h:18m:56s remains)
2017-12-15 09:30:56.217795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.37199497 -1.2703862 -2.4080977 -2.9772351 -3.6123464 -2.7432165 -1.8453145 -2.1331959 -1.9216921 -3.7900379 -6.7333479 -8.9900856 -9.4754791 -9.8984108 -9.4838886][-2.5178366 -2.895525 -3.1025462 -3.8855977 -4.6207266 -3.5850317 -2.543366 -2.7983274 -2.6142762 -4.5530643 -7.6305389 -9.9800625 -10.448779 -9.7577438 -7.8579865][-2.4007006 -3.5777576 -4.3645544 -4.4615359 -4.1970296 -3.1590846 -1.8796489 -1.6851969 -1.4338169 -3.5674334 -7.0914712 -9.3636131 -9.5900507 -9.9228506 -8.6555176][-2.9374244 -3.6773663 -4.2477417 -4.222188 -3.7250578 -2.388437 -0.51126385 -0.30585623 0.10384989 -2.2276046 -5.8714519 -8.3574944 -8.5593328 -8.3651152 -7.3336558][-3.8872547 -4.0222592 -3.9058635 -3.1512403 -2.6019893 -1.5848689 0.30424261 0.49603319 0.80217123 -1.6071253 -5.5178981 -7.8238978 -7.8905783 -7.5164542 -6.5005493][-3.2536347 -3.0411897 -2.3289227 -1.5945272 -0.32862854 0.82507849 2.5551524 2.9294667 3.191278 0.35832357 -3.7644901 -6.2154932 -6.1842422 -5.4654169 -4.2706909][-2.6280665 -2.2443357 -1.6529675 -0.7442522 0.51452351 1.9927006 3.9898238 4.01681 4.1004357 1.4825883 -2.0778737 -3.958333 -3.9560323 -3.8496947 -2.574858][-2.3771818 -2.2933085 -1.5551333 -0.4261632 0.54605341 1.9944792 4.0274978 4.1695523 4.206388 1.4229007 -1.8041246 -3.2907932 -3.2661839 -2.7047477 -2.1420596][-2.4144402 -2.2086847 -1.5096259 -0.59226894 0.62493229 1.9482102 3.6195073 3.4475865 3.4408898 1.104104 -1.7650685 -3.1830163 -3.2332585 -3.4925177 -3.272723][-4.8125858 -5.6239443 -4.7876353 -3.6133175 -2.4129553 -1.4837809 0.20241785 0.33289385 0.36099434 -2.1524847 -4.69283 -5.9115067 -5.5765648 -4.9366436 -4.4944448][-7.7431622 -8.1362667 -7.3754826 -6.0281124 -5.252284 -4.4388366 -3.0486741 -3.4482062 -3.7920947 -5.3246078 -7.7825742 -8.3859711 -7.7428503 -7.6430135 -6.9254966][-9.5519295 -9.3791981 -7.84691 -7.1541352 -6.5047216 -5.503046 -4.0922232 -4.282835 -4.418735 -6.6593242 -9.4934349 -8.8544531 -7.5512171 -6.4726887 -5.3195887][-9.6559668 -9.1274233 -7.7223673 -5.8436041 -5.1516156 -5.05342 -4.4623289 -4.106976 -3.9363966 -5.1186104 -7.1757693 -7.7891912 -7.0718117 -6.8104792 -5.9760542][-7.8884811 -6.89888 -5.0921116 -5.6959662 -5.968081 -4.7908278 -3.5569916 -4.7542224 -5.2757545 -5.4436421 -6.4447713 -5.316555 -3.7371173 -4.2304935 -4.5794473][-6.2042055 -6.4714088 -4.9320693 -3.57006 -4.0187469 -4.88788 -4.7126436 -3.8175483 -3.9513392 -5.6213527 -7.2980537 -6.5570526 -5.0895247 -4.4763393 -4.3106937]]...]
INFO - root - 2017-12-15 09:31:02.339210: step 910, loss = 0.61, batch loss = 0.18 (13.2 examples/sec; 0.608 sec/batch; 56h:00m:10s remains)
INFO - root - 2017-12-15 09:31:08.501332: step 920, loss = 0.61, batch loss = 0.18 (12.7 examples/sec; 0.628 sec/batch; 57h:52m:49s remains)
INFO - root - 2017-12-15 09:31:14.618261: step 930, loss = 0.67, batch loss = 0.25 (13.3 examples/sec; 0.603 sec/batch; 55h:32m:52s remains)
INFO - root - 2017-12-15 09:31:20.768494: step 940, loss = 0.55, batch loss = 0.18 (13.2 examples/sec; 0.607 sec/batch; 55h:52m:48s remains)
INFO - root - 2017-12-15 09:31:26.875231: step 950, loss = 0.67, batch loss = 0.22 (13.4 examples/sec; 0.597 sec/batch; 54h:58m:39s remains)
INFO - root - 2017-12-15 09:31:33.005265: step 960, loss = 0.54, batch loss = 0.16 (13.2 examples/sec; 0.605 sec/batch; 55h:42m:24s remains)
INFO - root - 2017-12-15 09:31:39.222568: step 970, loss = 0.57, batch loss = 0.23 (12.8 examples/sec; 0.625 sec/batch; 57h:34m:00s remains)
INFO - root - 2017-12-15 09:31:45.339024: step 980, loss = 0.48, batch loss = 0.14 (13.2 examples/sec; 0.607 sec/batch; 55h:54m:45s remains)
INFO - root - 2017-12-15 09:31:51.488138: step 990, loss = 0.56, batch loss = 0.20 (12.8 examples/sec; 0.625 sec/batch; 57h:35m:31s remains)
INFO - root - 2017-12-15 09:31:57.609865: step 1000, loss = 0.53, batch loss = 0.18 (12.8 examples/sec; 0.627 sec/batch; 57h:42m:26s remains)
2017-12-15 09:31:58.063393: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6456437 -3.6794062 -4.0490489 -3.5075364 -2.7328489 -2.4274261 -2.2451594 -2.3823557 -2.4615052 -3.8454118 -6.402298 -6.393167 -7.2762365 -8.7031469 -9.7535839][-3.5261238 -2.3444273 -3.158401 -3.493902 -3.1019661 -2.1493864 -1.2057686 -1.3761473 -1.7047606 -3.2697325 -6.0305729 -6.9264841 -7.8080835 -8.0525761 -7.0720258][-5.1307421 -2.8912868 -2.3189094 -2.4937816 -2.9272964 -2.0825813 -0.78918362 -0.52890158 -0.060296535 -2.0494144 -5.9032221 -7.78978 -9.4798241 -8.9491348 -7.1007996][-2.453218 -2.0598235 -1.9955194 -1.0012789 -0.040744305 0.60298634 1.7846603 2.0075612 2.0426774 -0.3410306 -4.1386 -5.9653869 -9.2282686 -9.9851627 -7.2724495][-0.21988583 -0.94964743 -1.187108 -0.53386116 0.46800852 1.8289199 3.6288772 3.5917745 3.4418893 0.7730217 -3.6739259 -5.1706185 -7.4594913 -8.5115089 -7.9470716][-0.777894 -1.3565068 -1.6216912 -0.80350018 0.69883633 2.0733147 3.7325082 4.8416629 5.5525289 1.9096398 -3.0893731 -4.2663894 -6.7889252 -7.507556 -6.6466451][-0.54101849 -0.4759841 -1.3031058 -0.35957193 1.1627707 2.825357 4.3612981 4.7801976 5.4992938 2.5025907 -1.9268627 -4.1881027 -6.3623152 -6.4811287 -5.4238191][-1.5536523 -1.1803861 -1.4277849 -0.37066984 1.307806 3.3272414 4.951405 5.220407 5.5116162 2.2293239 -2.388068 -4.7805333 -7.029407 -7.6894197 -6.4382892][-1.1625962 -0.8157382 -0.6663909 0.09137392 1.3720756 3.19494 5.0136828 5.2321372 5.2339282 2.4289432 -1.9538429 -4.2250371 -7.4131527 -7.5032263 -6.7578135][-3.6012871 -2.7278426 -2.2770989 -1.8304496 -0.93942308 0.55804539 2.4481301 3.0369792 2.8263278 -0.57303905 -5.3268242 -6.5274949 -8.0598507 -8.5607643 -7.9524593][-3.8434088 -4.3983841 -4.8488121 -3.801559 -2.5706589 -1.9374464 -0.60854864 -0.49375534 -0.49342728 -3.8474522 -8.134037 -8.5397463 -9.554863 -9.5705976 -8.0136967][-4.1867042 -5.1055908 -6.3237367 -6.7212787 -6.0188694 -4.991539 -4.3957958 -4.129993 -3.7227335 -6.0200214 -8.8650331 -9.9901619 -10.708603 -9.5294085 -7.3055811][-5.3741059 -6.527554 -7.4826446 -7.2475944 -6.3100886 -5.2281518 -4.6840467 -5.7347312 -7.1313152 -8.7570934 -10.501389 -10.622812 -10.917875 -10.037334 -7.5939198][-5.1815529 -6.3825874 -7.00889 -6.8007922 -5.4600329 -4.7708325 -5.4191709 -5.910604 -5.7816749 -7.4161453 -9.5026436 -9.832346 -10.291906 -9.1392746 -7.0489092][-5.0382133 -6.8651252 -7.556591 -7.38612 -6.0846581 -4.780149 -4.2204556 -4.4917173 -5.3928838 -6.8031411 -7.7296691 -7.6598825 -8.1476 -8.8592882 -8.42382]]...]
INFO - root - 2017-12-15 09:32:04.205218: step 1010, loss = 0.63, batch loss = 0.25 (13.3 examples/sec; 0.602 sec/batch; 55h:26m:06s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 09:32:10.315831: step 1020, loss = 0.68, batch loss = 0.24 (13.1 examples/sec; 0.609 sec/batch; 56h:02m:21s remains)
INFO - root - 2017-12-15 09:32:16.512617: step 1030, loss = 0.53, batch loss = 0.16 (12.9 examples/sec; 0.620 sec/batch; 57h:07m:32s remains)
INFO - root - 2017-12-15 09:32:22.692596: step 1040, loss = 0.56, batch loss = 0.18 (13.1 examples/sec; 0.611 sec/batch; 56h:16m:14s remains)
INFO - root - 2017-12-15 09:32:28.825799: step 1050, loss = 0.56, batch loss = 0.19 (13.3 examples/sec; 0.603 sec/batch; 55h:29m:59s remains)
INFO - root - 2017-12-15 09:32:34.953569: step 1060, loss = 0.52, batch loss = 0.19 (12.9 examples/sec; 0.621 sec/batch; 57h:10m:22s remains)
INFO - root - 2017-12-15 09:32:41.147470: step 1070, loss = 0.48, batch loss = 0.13 (13.1 examples/sec; 0.610 sec/batch; 56h:09m:59s remains)
INFO - root - 2017-12-15 09:32:47.302715: step 1080, loss = 0.47, batch loss = 0.14 (13.0 examples/sec; 0.617 sec/batch; 56h:47m:56s remains)
INFO - root - 2017-12-15 09:32:53.424263: step 1090, loss = 0.57, batch loss = 0.20 (12.8 examples/sec; 0.624 sec/batch; 57h:29m:17s remains)
INFO - root - 2017-12-15 09:32:59.614459: step 1100, loss = 0.59, batch loss = 0.20 (13.1 examples/sec; 0.613 sec/batch; 56h:24m:53s remains)
2017-12-15 09:33:00.065980: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.4758 0.0051474571 -1.4498639 -1.5626969 -2.1349068 -3.0642695 -3.3585668 -3.4663985 -3.0980589 -4.163331 -9.2804594 -10.866467 -13.436813 -13.383031 -13.054873][-0.014570236 -0.49027777 -1.4478798 -3.0731881 -3.691366 -3.1816673 -3.2999315 -3.4433246 -3.4623766 -4.5273342 -9.6497345 -11.294417 -13.353235 -12.629332 -12.778512][-1.4780664 -2.9599812 -4.9540091 -5.806881 -6.1420093 -5.007699 -4.0845594 -3.9451976 -3.8260617 -5.1148863 -10.64171 -12.404619 -14.708822 -13.801367 -13.445694][-3.6943724 -5.0673771 -6.4610214 -7.138051 -7.30947 -5.5925989 -4.7576461 -4.2675042 -3.8917959 -4.8229327 -10.469494 -12.365687 -14.93388 -13.867895 -13.83848][-3.0270746 -4.4154205 -6.6334329 -6.1607833 -5.7550707 -3.8334212 -2.3498027 -1.3506365 -0.64159966 -1.8672786 -7.56528 -9.4055586 -11.898255 -11.231955 -10.673594][-2.90179 -3.0007682 -4.6281552 -4.2343116 -3.109586 -0.7099452 0.49910259 1.4351907 2.2922072 0.95707607 -4.9820752 -6.8491511 -8.6113148 -7.6953421 -7.181304][-2.3013344 -2.0427237 -2.9887993 -1.4789243 -0.10311699 2.076139 2.89216 3.8034139 4.4138274 2.7922587 -2.4633582 -4.6192837 -7.062408 -5.5094161 -4.4923043][-2.4215462 -2.0427313 -2.7240427 -0.7974968 0.7873888 3.3876691 4.2653136 4.5486722 4.7850642 2.918673 -2.5637758 -4.2664652 -6.4037971 -5.4993157 -4.7119989][-2.8081586 -2.2315121 -3.1956191 -1.3159757 0.32043409 2.9307704 4.0707145 4.2332435 4.1687417 1.9875011 -3.2620153 -4.66745 -7.0512514 -6.6182976 -5.8961115][-4.6792636 -4.2849407 -5.1796145 -4.00432 -3.0167665 -0.70032024 0.27290487 0.63138485 0.78205967 -1.2812996 -5.9369822 -7.2753558 -8.3963556 -7.5912848 -7.581738][-5.9559321 -5.3923292 -6.5819039 -5.404726 -4.4138188 -3.6062617 -3.6994984 -3.3380072 -3.0946088 -5.334095 -8.5982151 -8.7959814 -10.035856 -8.2994623 -7.2350178][-7.255125 -6.7224379 -7.559185 -6.6529188 -6.2712131 -4.6304936 -4.8481169 -6.8522444 -6.9613605 -7.96027 -11.879118 -11.45968 -11.203423 -9.1498756 -7.7197156][-6.4708977 -5.8046947 -6.8603706 -5.3988123 -4.19138 -2.9102077 -3.1640165 -5.0651622 -6.7508883 -9.4408245 -12.796141 -12.169236 -11.416501 -9.8715029 -8.4488564][-4.9769444 -4.427289 -5.9394817 -5.6299043 -4.8603644 -2.911056 -1.8471289 -2.2415392 -2.3890364 -6.7122183 -10.661537 -11.07653 -10.88683 -9.2542191 -8.3128548][-5.5973258 -5.0422039 -6.2415142 -6.0585194 -5.902771 -3.9370241 -2.5901816 -2.6339955 -2.3710988 -4.8088565 -7.3664026 -8.7398767 -9.8193111 -9.2216454 -8.9424648]]...]
INFO - root - 2017-12-15 09:33:06.191905: step 1110, loss = 0.58, batch loss = 0.17 (12.8 examples/sec; 0.623 sec/batch; 57h:20m:21s remains)
INFO - root - 2017-12-15 09:33:12.342126: step 1120, loss = 0.52, batch loss = 0.13 (12.7 examples/sec; 0.629 sec/batch; 57h:54m:04s remains)
INFO - root - 2017-12-15 09:33:18.460360: step 1130, loss = 0.49, batch loss = 0.13 (13.1 examples/sec; 0.611 sec/batch; 56h:15m:30s remains)
INFO - root - 2017-12-15 09:33:24.599073: step 1140, loss = 0.45, batch loss = 0.10 (13.0 examples/sec; 0.614 sec/batch; 56h:31m:30s remains)
INFO - root - 2017-12-15 09:33:30.750336: step 1150, loss = 0.45, batch loss = 0.14 (12.6 examples/sec; 0.635 sec/batch; 58h:27m:11s remains)
INFO - root - 2017-12-15 09:33:36.893140: step 1160, loss = 0.58, batch loss = 0.20 (13.2 examples/sec; 0.605 sec/batch; 55h:39m:57s remains)
INFO - root - 2017-12-15 09:33:43.061212: step 1170, loss = 0.50, batch loss = 0.16 (13.1 examples/sec; 0.609 sec/batch; 56h:00m:56s remains)
INFO - root - 2017-12-15 09:33:49.175760: step 1180, loss = 0.47, batch loss = 0.13 (13.2 examples/sec; 0.605 sec/batch; 55h:41m:09s remains)
INFO - root - 2017-12-15 09:33:55.285161: step 1190, loss = 0.49, batch loss = 0.17 (12.9 examples/sec; 0.622 sec/batch; 57h:13m:22s remains)
INFO - root - 2017-12-15 09:34:01.424123: step 1200, loss = 0.49, batch loss = 0.12 (13.1 examples/sec; 0.610 sec/batch; 56h:07m:15s remains)
2017-12-15 09:34:01.881064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.48557615 -2.2508 -3.2139583 -3.8407094 -4.8964081 -4.5117922 -4.4128275 -4.177206 -3.3478677 -4.1347146 -7.4704561 -10.103466 -11.528063 -12.949024 -11.462282][-2.7932973 -3.9333568 -5.0182447 -5.7806625 -6.6270876 -6.0182085 -5.1481547 -4.8609428 -4.60419 -5.439806 -9.4160023 -12.026621 -13.207109 -14.209793 -13.05588][-3.8441162 -4.7721996 -5.5321717 -6.0520911 -7.1466732 -6.3435473 -4.0761623 -3.415643 -3.2819221 -4.572566 -8.6038809 -10.965652 -11.444369 -12.213217 -11.269907][-6.4533281 -6.8967233 -7.612134 -7.0881634 -6.3834429 -5.3460741 -2.7442031 -2.4577963 -1.9994078 -3.281518 -7.1875582 -9.3486519 -10.018104 -10.3925 -8.482707][-6.5534744 -7.1478262 -7.3149061 -6.0178742 -4.5498161 -3.5187535 -0.6195879 -0.45956993 -0.21890783 -1.9905789 -5.6303325 -7.7549458 -8.54899 -9.3120975 -7.5832829][-5.8060861 -6.113946 -5.5672483 -4.134656 -1.6985049 -0.30788374 2.1169286 2.5414414 2.7407432 0.59091139 -3.137938 -4.8303013 -5.2467055 -5.5087218 -3.5419698][-5.7668452 -5.7883439 -4.7668328 -2.8603437 -0.50397921 1.0406847 3.6351209 3.941154 4.0938783 1.6841478 -1.9086947 -3.462971 -3.9730461 -3.7217853 -1.2005925][-6.1688557 -5.756413 -4.6015978 -2.4385004 0.24562883 1.9733882 4.613173 4.6077924 4.357429 1.8288474 -1.4267478 -2.8830962 -3.2630146 -3.0849853 -0.75921774][-7.7231331 -6.3455176 -4.6833553 -2.7017813 -0.21841574 1.5808454 4.4716392 4.4760623 3.8661618 1.1702199 -1.8110375 -2.6318798 -3.1753075 -2.9588141 0.20943975][-11.351858 -9.7382326 -7.0709882 -5.0619287 -2.7245393 -1.3843932 1.0150042 1.2629914 0.97696781 -1.41191 -4.4392691 -5.1475077 -4.9565635 -4.3729448 -2.3270576][-12.444862 -11.166756 -8.7357569 -6.238472 -3.7020671 -2.8239217 -1.0992975 -1.6696711 -2.2961259 -5.0677795 -8.2033529 -8.0611219 -6.7441421 -6.0591011 -4.1051397][-12.229366 -10.896851 -8.8891563 -7.5330057 -5.4696097 -3.8555913 -1.5684285 -2.2650542 -3.5238166 -6.1879377 -9.2084236 -9.35415 -8.4478455 -6.7812662 -3.2614534][-11.745443 -10.376078 -8.697072 -7.4731636 -5.9383564 -5.6725192 -4.5049658 -4.5737796 -6.150898 -8.2201223 -10.741131 -9.8064861 -8.051631 -6.7005014 -3.0215437][-9.0463266 -8.7405214 -7.6791053 -6.8236647 -5.7922883 -6.2059441 -6.5485215 -5.4481635 -6.6017447 -7.7275209 -9.7533894 -8.5795517 -6.3187485 -5.0182557 -2.5470941][-8.0547714 -7.6781526 -6.3379521 -5.8728752 -5.9142489 -5.8110242 -6.3536954 -5.4605618 -7.0409575 -7.673173 -8.6765556 -7.4836087 -5.93628 -5.730598 -4.3243647]]...]
INFO - root - 2017-12-15 09:34:07.991125: step 1210, loss = 0.46, batch loss = 0.13 (13.0 examples/sec; 0.616 sec/batch; 56h:43m:57s remains)
INFO - root - 2017-12-15 09:34:14.157491: step 1220, loss = 0.43, batch loss = 0.10 (13.1 examples/sec; 0.611 sec/batch; 56h:13m:03s remains)
INFO - root - 2017-12-15 09:34:20.327157: step 1230, loss = 0.46, batch loss = 0.11 (13.2 examples/sec; 0.604 sec/batch; 55h:36m:00s remains)
INFO - root - 2017-12-15 09:34:26.476471: step 1240, loss = 0.43, batch loss = 0.11 (13.3 examples/sec; 0.601 sec/batch; 55h:18m:36s remains)
INFO - root - 2017-12-15 09:34:32.587867: step 1250, loss = 0.47, batch loss = 0.15 (13.2 examples/sec; 0.606 sec/batch; 55h:43m:08s remains)
INFO - root - 2017-12-15 09:34:38.735933: step 1260, loss = 0.45, batch loss = 0.11 (13.1 examples/sec; 0.611 sec/batch; 56h:10m:22s remains)
INFO - root - 2017-12-15 09:34:44.854539: step 1270, loss = 0.44, batch loss = 0.12 (13.1 examples/sec; 0.612 sec/batch; 56h:18m:32s remains)
INFO - root - 2017-12-15 09:34:50.959076: step 1280, loss = 0.51, batch loss = 0.15 (13.2 examples/sec; 0.606 sec/batch; 55h:44m:44s remains)
INFO - root - 2017-12-15 09:34:57.070362: step 1290, loss = 0.44, batch loss = 0.14 (13.3 examples/sec; 0.600 sec/batch; 55h:10m:32s remains)
INFO - root - 2017-12-15 09:35:03.219741: step 1300, loss = 0.40, batch loss = 0.09 (13.0 examples/sec; 0.617 sec/batch; 56h:44m:05s remains)
2017-12-15 09:35:03.706313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6007495 -4.4383907 -5.4273281 -4.978025 -4.6643629 -4.18157 -4.037837 -4.2081137 -4.5357304 -5.2106056 -7.137219 -6.9972591 -8.7807093 -9.2870665 -9.0171652][-3.1614575 -3.198005 -3.8003945 -3.379308 -3.1219583 -2.8911784 -2.7229247 -2.9158716 -3.3304024 -4.1865225 -5.9877582 -5.8602781 -7.4660234 -7.5564041 -7.4308558][-2.9890249 -3.0188181 -3.0103428 -2.4968374 -2.2228842 -1.5077558 -0.96759605 -1.351922 -2.1147535 -3.4019082 -5.2955256 -5.8097162 -7.4052687 -6.3693361 -5.5722141][-4.3215308 -4.7935762 -4.7998533 -2.8554544 -1.3259277 -0.52514553 0.18987513 -0.64191771 -1.4698553 -2.9677198 -5.2928295 -5.3732862 -6.3047881 -5.2533307 -4.2600622][-4.8301196 -5.0619712 -4.9317179 -3.2096086 -1.9268293 -0.65316296 0.6465745 0.31914759 -0.026810646 -1.9738128 -4.7972503 -4.7509775 -5.8465624 -4.53389 -3.4480069][-5.2407508 -5.2833886 -5.0843058 -2.8303628 -0.99625397 0.76118994 2.2193413 2.4506226 2.7716436 0.69830513 -2.7452614 -2.92308 -4.033926 -3.1110878 -2.0675907][-6.0602288 -5.582901 -4.7858219 -2.3198366 -0.12921667 1.7109375 3.1402836 3.4483738 3.8060303 1.8083582 -0.64918756 -1.1352329 -2.2831707 -1.4446368 -0.66649914][-4.8841929 -4.2949066 -3.6490507 -1.152545 0.93035078 2.5715876 4.03654 4.1954403 4.5347347 2.1396036 -0.29161406 -0.23775005 -1.7966161 -1.1523085 -0.30063057][-4.1339717 -3.4569185 -3.1862705 -0.89662838 1.1186528 2.8828888 4.5296535 4.57889 4.8806715 2.6563444 -0.28969669 -0.327096 -2.2336593 -2.5428176 -2.3393979][-6.6834917 -5.763236 -5.7400846 -3.4441292 -1.5513101 0.10263205 1.7099319 1.7169986 1.741138 -0.43370533 -3.5406983 -4.4870071 -5.5076928 -5.894093 -6.0245428][-9.1259909 -9.9924583 -10.210421 -7.1891117 -4.8449397 -3.3269911 -1.9468684 -1.8497763 -1.7391872 -3.7200103 -6.1569233 -6.1099515 -6.1001511 -5.0777922 -4.6075258][-13.612835 -14.469138 -13.535812 -10.733776 -8.5787249 -6.1777182 -4.621841 -4.3084612 -4.0120983 -5.86393 -7.7376542 -7.2182741 -6.6473141 -5.29351 -3.5221031][-15.397691 -14.677359 -13.305082 -11.128929 -8.8907442 -6.8866324 -6.0301085 -5.2148323 -4.6842179 -6.1931863 -7.7756968 -7.0841026 -6.4228592 -5.7065592 -4.6717482][-13.37331 -12.820717 -11.438484 -9.3901691 -7.5645895 -6.5312195 -6.001709 -5.1688375 -4.669096 -5.1569691 -5.9239955 -5.4328127 -5.0159459 -3.8156123 -2.770937][-13.259067 -12.811486 -11.1441 -9.3878117 -8.3377419 -6.9355106 -6.1372228 -5.91277 -5.3569379 -5.8733673 -6.5316477 -5.3866191 -3.9200826 -3.6519406 -2.9267869]]...]
INFO - root - 2017-12-15 09:35:09.838110: step 1310, loss = 0.41, batch loss = 0.11 (13.0 examples/sec; 0.613 sec/batch; 56h:24m:57s remains)
INFO - root - 2017-12-15 09:35:16.018217: step 1320, loss = 0.44, batch loss = 0.13 (13.0 examples/sec; 0.615 sec/batch; 56h:33m:01s remains)
INFO - root - 2017-12-15 09:35:22.148101: step 1330, loss = 0.46, batch loss = 0.12 (13.1 examples/sec; 0.610 sec/batch; 56h:05m:09s remains)
INFO - root - 2017-12-15 09:35:28.270213: step 1340, loss = 0.50, batch loss = 0.13 (13.3 examples/sec; 0.600 sec/batch; 55h:12m:19s remains)
INFO - root - 2017-12-15 09:35:34.370182: step 1350, loss = 0.48, batch loss = 0.13 (12.8 examples/sec; 0.627 sec/batch; 57h:39m:30s remains)
INFO - root - 2017-12-15 09:35:40.506376: step 1360, loss = 0.49, batch loss = 0.16 (13.3 examples/sec; 0.602 sec/batch; 55h:23m:46s remains)
INFO - root - 2017-12-15 09:35:46.626597: step 1370, loss = 0.48, batch loss = 0.16 (13.1 examples/sec; 0.612 sec/batch; 56h:20m:12s remains)
INFO - root - 2017-12-15 09:35:52.731975: step 1380, loss = 0.45, batch loss = 0.14 (13.1 examples/sec; 0.611 sec/batch; 56h:10m:21s remains)
INFO - root - 2017-12-15 09:35:58.899226: step 1390, loss = 0.45, batch loss = 0.12 (13.2 examples/sec; 0.605 sec/batch; 55h:39m:33s remains)
INFO - root - 2017-12-15 09:36:05.041714: step 1400, loss = 0.49, batch loss = 0.16 (13.1 examples/sec; 0.610 sec/batch; 56h:05m:56s remains)
2017-12-15 09:36:05.521429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.9669991 -7.6545 -7.9324994 -6.3298893 -4.7793283 -4.9787283 -5.1995153 -4.8304191 -4.83034 -4.8966937 -7.9726553 -10.242956 -12.360285 -11.436722 -11.107588][-5.3423972 -5.7807922 -6.97826 -5.5642366 -4.477643 -4.153151 -3.5441809 -3.1011083 -3.7563195 -4.7106471 -8.4105673 -10.197115 -11.793962 -9.98939 -9.2126741][-5.5900869 -4.6451559 -5.4160142 -4.7310896 -5.0864611 -4.2782993 -3.0136178 -2.8376367 -2.5962393 -3.5354259 -7.1726832 -9.2075663 -11.260675 -9.06555 -7.5526733][-6.1365533 -5.6216621 -4.6919231 -3.0896287 -3.3150549 -2.1273081 -0.72287607 -0.85349131 -0.61861467 -1.6153989 -5.4821095 -7.5940585 -9.33041 -7.7693176 -7.6222234][-1.5422931 -3.7117171 -4.528307 -2.5746808 -1.7597365 -0.25688648 1.3835397 1.6275225 2.2046685 0.88867092 -3.1497023 -5.6079583 -7.1630979 -5.1735168 -4.8514905][-5.410171 -3.1481318 -1.1008935 -0.79139042 -0.70026064 0.95958138 2.2454038 3.1122336 4.0283828 2.6289983 -0.71624851 -2.992202 -4.1352406 -2.8047247 -2.8831937][-7.8190527 -6.1695704 -3.8037539 -1.483253 0.20061731 2.3219771 3.9277225 3.8942294 4.03219 2.4594607 -0.34418106 -1.7330089 -1.2564101 0.78308392 1.2182217][-6.1416903 -5.8056803 -4.7627449 -2.4147339 -0.28315973 2.2858825 4.1837096 4.8810687 5.1976647 3.0504184 0.038675785 -1.4233437 -1.9825516 0.40076923 2.4984279][-8.1807861 -6.3499651 -4.5540433 -2.2324874 -0.38849163 1.9339495 3.6748328 4.1734357 4.4919286 2.5018182 -0.22163486 -1.3688192 -3.2731602 -2.0752265 0.41579819][-8.0972462 -7.0870657 -5.4021535 -3.2140808 -2.3511622 -0.39052629 1.1316738 1.6267471 2.1493154 0.56911135 -2.4417076 -3.76766 -4.1853995 -2.9515152 -1.7857313][-8.56037 -9.3186321 -7.9283514 -5.8847485 -5.2686362 -3.7103512 -2.464381 -2.2929111 -2.0516598 -3.4892066 -5.8642249 -6.5507269 -6.3810425 -5.458437 -4.0527349][-11.308069 -12.192389 -11.351107 -9.82415 -9.5463123 -8.0385647 -6.908164 -6.87195 -6.7850027 -7.8866153 -10.003663 -9.5733128 -8.1103878 -7.2456727 -5.8194542][-9.5048037 -9.0508146 -8.4054346 -8.0419769 -8.6786842 -6.8308153 -5.67021 -5.706542 -5.551177 -7.0734797 -8.9287033 -8.3387842 -6.8101745 -6.0759969 -4.4944859][-8.9026585 -9.0761032 -8.9905205 -7.5416732 -6.6333308 -4.7417307 -2.9896696 -3.7807014 -4.5052619 -4.3815522 -4.8933325 -4.133688 -2.9328854 -3.3157749 -3.8807292][-9.1968184 -7.8840661 -6.3501477 -6.6464734 -6.174964 -4.0378022 -2.6843698 -3.0021594 -2.5549991 -3.3684947 -4.1920323 -3.2597218 -3.0433674 -1.9132328 -0.76045036]]...]
INFO - root - 2017-12-15 09:36:11.651843: step 1410, loss = 0.48, batch loss = 0.13 (13.0 examples/sec; 0.615 sec/batch; 56h:34m:27s remains)
INFO - root - 2017-12-15 09:36:17.772100: step 1420, loss = 0.47, batch loss = 0.15 (13.0 examples/sec; 0.614 sec/batch; 56h:30m:31s remains)
INFO - root - 2017-12-15 09:36:23.923555: step 1430, loss = 0.41, batch loss = 0.12 (13.2 examples/sec; 0.604 sec/batch; 55h:33m:35s remains)
INFO - root - 2017-12-15 09:36:30.080794: step 1440, loss = 0.42, batch loss = 0.11 (13.2 examples/sec; 0.605 sec/batch; 55h:37m:53s remains)
INFO - root - 2017-12-15 09:36:36.290034: step 1450, loss = 0.43, batch loss = 0.13 (13.1 examples/sec; 0.613 sec/batch; 56h:21m:47s remains)
INFO - root - 2017-12-15 09:36:42.444913: step 1460, loss = 0.52, batch loss = 0.15 (13.2 examples/sec; 0.604 sec/batch; 55h:31m:39s remains)
INFO - root - 2017-12-15 09:36:48.594617: step 1470, loss = 0.49, batch loss = 0.18 (13.1 examples/sec; 0.611 sec/batch; 56h:11m:55s remains)
INFO - root - 2017-12-15 09:36:54.743676: step 1480, loss = 0.40, batch loss = 0.11 (12.8 examples/sec; 0.627 sec/batch; 57h:38m:48s remains)
INFO - root - 2017-12-15 09:37:00.861726: step 1490, loss = 0.42, batch loss = 0.11 (13.1 examples/sec; 0.611 sec/batch; 56h:08m:30s remains)
INFO - root - 2017-12-15 09:37:07.043247: step 1500, loss = 0.42, batch loss = 0.12 (12.9 examples/sec; 0.622 sec/batch; 57h:09m:36s remains)
2017-12-15 09:37:07.518556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6129048 -4.0114412 -4.5885291 -3.8932738 -3.4986062 -3.9493704 -4.4116774 -5.4065962 -5.8125672 -7.2334023 -9.7611542 -12.092734 -13.486095 -12.886408 -13.169935][-3.2312815 -3.7309425 -4.1278291 -4.4499702 -4.3018208 -3.9039128 -4.1398311 -4.7987013 -5.1030617 -5.2629104 -7.5885582 -10.374835 -11.797216 -13.126818 -13.014122][-3.3607211 -4.7275376 -4.3192334 -4.6406903 -5.4386673 -5.2413793 -4.5073614 -4.2730355 -3.7606938 -4.5509434 -6.5140948 -9.3437357 -10.946625 -12.034075 -12.440668][-1.5136895 -3.8054526 -4.6372647 -4.3482285 -4.1010337 -4.2249489 -4.2118034 -3.9099607 -3.1594169 -4.173789 -6.69312 -9.2421894 -10.522656 -10.799046 -11.286251][-2.9988182 -4.455689 -4.3671236 -3.5245507 -2.8777878 -2.1874797 -1.4356627 -1.9612532 -2.1441367 -3.3352928 -6.1600127 -8.61571 -9.8606386 -9.7759132 -9.7658558][-4.26334 -4.8943262 -4.8334846 -3.4880233 -2.2434375 -0.91518879 -0.0034351349 0.47562742 0.91514492 -1.5412965 -5.2789888 -7.2508478 -7.9958019 -7.2698627 -6.3273454][-4.5796032 -5.6608467 -5.1411891 -3.5556195 -1.84441 0.10938263 1.442802 1.6551085 1.8512774 -0.083711624 -3.0235782 -5.3660083 -6.7849193 -5.8909845 -4.1784897][-3.9659483 -5.4157014 -4.8409433 -2.9795237 -1.3754039 0.52489758 2.207037 2.5571327 2.5069542 0.055946827 -2.4497542 -4.3303361 -6.70477 -6.5686522 -5.2176523][-4.1770821 -4.9848595 -4.527442 -2.565928 -0.68238163 1.225678 2.6844416 2.9791288 2.8759317 -0.16237831 -3.3985047 -4.98151 -7.151413 -7.3070765 -6.3571367][-8.6552277 -8.7989454 -8.7336254 -6.8850956 -5.0840025 -2.7110481 -0.94705915 -0.71192741 -1.3544478 -3.9800873 -7.1261797 -8.6565266 -9.1179142 -7.9347758 -7.1231575][-11.633818 -12.233036 -12.59543 -12.121603 -10.80786 -8.6324615 -6.7993541 -6.4377155 -6.3242931 -9.1192379 -11.312213 -12.00745 -11.798225 -10.591478 -8.5523014][-13.541262 -14.569465 -13.979117 -12.702059 -11.963602 -10.039196 -8.4491758 -9.8700886 -10.811602 -12.086079 -13.00296 -13.12373 -12.197466 -10.653392 -9.2942009][-13.151039 -14.132889 -13.218897 -10.456142 -8.6555719 -9.63267 -10.290623 -7.9007716 -5.9656138 -9.9195461 -13.240524 -13.296801 -13.464279 -11.913742 -9.8461275][-13.146889 -13.901329 -13.054501 -10.76475 -9.1848822 -6.0126987 -3.8490956 -6.1703529 -7.6415424 -9.0774441 -11.149626 -10.806515 -10.483835 -10.012869 -8.73217][-13.899723 -14.14215 -13.34466 -11.623256 -10.451595 -7.714385 -6.7763844 -7.8620744 -9.7691813 -10.362841 -9.6460075 -10.143513 -11.404498 -10.385199 -9.3396673]]...]
INFO - root - 2017-12-15 09:37:13.703928: step 1510, loss = 0.38, batch loss = 0.11 (13.0 examples/sec; 0.614 sec/batch; 56h:24m:39s remains)
INFO - root - 2017-12-15 09:37:19.782787: step 1520, loss = 0.42, batch loss = 0.13 (13.2 examples/sec; 0.605 sec/batch; 55h:37m:36s remains)
INFO - root - 2017-12-15 09:37:25.950269: step 1530, loss = 0.38, batch loss = 0.11 (12.9 examples/sec; 0.619 sec/batch; 56h:52m:07s remains)
INFO - root - 2017-12-15 09:37:32.081071: step 1540, loss = 0.41, batch loss = 0.10 (12.9 examples/sec; 0.622 sec/batch; 57h:09m:18s remains)
INFO - root - 2017-12-15 09:37:38.204037: step 1550, loss = 0.41, batch loss = 0.11 (13.1 examples/sec; 0.609 sec/batch; 55h:59m:37s remains)
INFO - root - 2017-12-15 09:37:44.311569: step 1560, loss = 0.43, batch loss = 0.13 (13.0 examples/sec; 0.615 sec/batch; 56h:29m:35s remains)
INFO - root - 2017-12-15 09:37:50.413950: step 1570, loss = 0.40, batch loss = 0.11 (13.2 examples/sec; 0.607 sec/batch; 55h:48m:34s remains)
INFO - root - 2017-12-15 09:37:56.593947: step 1580, loss = 0.39, batch loss = 0.11 (13.1 examples/sec; 0.613 sec/batch; 56h:20m:10s remains)
INFO - root - 2017-12-15 09:38:02.716471: step 1590, loss = 0.38, batch loss = 0.12 (13.4 examples/sec; 0.597 sec/batch; 54h:55m:13s remains)
INFO - root - 2017-12-15 09:38:08.849179: step 1600, loss = 0.44, batch loss = 0.13 (12.8 examples/sec; 0.623 sec/batch; 57h:16m:01s remains)
2017-12-15 09:38:09.320785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-10.314604 -10.521902 -11.475429 -10.237918 -9.5172052 -9.4269524 -8.6855125 -8.1828575 -8.0376072 -9.1338263 -13.784151 -14.337258 -14.648357 -15.034215 -13.407146][-10.610788 -11.436708 -11.866918 -12.287004 -11.60262 -9.4777393 -8.0697088 -8.2502041 -8.0783072 -8.0587149 -11.712721 -12.939438 -12.599075 -11.901749 -11.259076][-11.811458 -11.738224 -11.440866 -11.732058 -11.573183 -9.9793949 -8.0589972 -7.7819738 -7.4116817 -7.9281216 -11.603718 -12.25561 -12.348678 -11.653658 -11.023497][-10.27454 -11.518167 -12.962559 -12.023641 -9.5465612 -6.9722977 -5.4120026 -5.2123489 -5.1840816 -5.9271507 -10.263111 -11.047353 -10.669504 -9.6102858 -7.0488353][-7.5156879 -8.7242355 -10.500495 -10.949759 -9.6962986 -5.5188432 -2.174587 -1.8245463 -1.7076216 -2.9692833 -7.4131751 -8.4751348 -8.80892 -7.5923638 -5.890583][-7.6224513 -5.5508981 -5.7928514 -7.4566875 -6.6611719 -3.1908271 -1.1698112 0.17447519 0.86145353 -1.0277076 -5.5243554 -6.3975768 -6.3312664 -4.61024 -2.6489625][-6.5165949 -5.1050935 -5.0590711 -3.8284094 -2.3047559 -0.062761307 1.180614 1.6768103 1.972106 0.56180811 -3.6617775 -5.008482 -4.9193187 -2.3188481 -0.1379528][-8.36071 -6.0190525 -4.324367 -3.108088 -1.4016323 2.2512226 4.0200949 3.8199215 3.6152191 1.8396783 -2.716368 -3.2216718 -3.5445242 -2.3652985 -0.29019451][-11.825521 -7.8378911 -5.9666266 -3.8134406 -0.96925545 2.064064 3.9167643 4.4824753 4.0808973 2.7143373 -1.7052693 -3.1988869 -3.8737996 -1.5842385 0.021174431][-12.299023 -11.477035 -10.809296 -8.0615292 -5.3711529 -1.5390806 0.77434254 0.89472723 0.81040096 -0.25063705 -3.9591787 -5.39001 -4.4119644 -2.6680651 -1.1292276][-14.334436 -13.318665 -12.967604 -11.911342 -10.363667 -6.6607051 -4.9315305 -4.2477942 -3.5631273 -5.0068288 -7.8162856 -6.9464068 -6.4082837 -4.4139118 -1.6019263][-16.780834 -15.463769 -15.177268 -13.306118 -11.439148 -8.5919409 -7.3685012 -7.4825816 -7.6291656 -8.3874168 -10.918562 -10.125316 -8.955492 -6.295537 -4.6045585][-15.627395 -14.666809 -14.350624 -12.886742 -10.605421 -6.9486294 -5.5724759 -6.48114 -6.813005 -7.8508062 -9.9182472 -7.9312282 -5.9091167 -5.1539392 -4.3638835][-15.453865 -14.485592 -13.645879 -12.357824 -10.246925 -6.5673809 -5.1460862 -5.3417578 -5.6565571 -7.8580842 -9.114687 -7.1946688 -4.0505271 0.074792385 1.2623544][-13.529217 -13.755744 -14.212808 -12.169265 -9.3343124 -6.0369697 -4.971385 -5.1678824 -5.9257154 -6.398159 -6.1086812 -5.429431 -4.8134117 -2.6860535 0.20041656]]...]
INFO - root - 2017-12-15 09:38:15.443318: step 1610, loss = 0.39, batch loss = 0.11 (13.0 examples/sec; 0.617 sec/batch; 56h:43m:46s remains)
INFO - root - 2017-12-15 09:38:21.636037: step 1620, loss = 0.37, batch loss = 0.10 (13.1 examples/sec; 0.610 sec/batch; 56h:05m:19s remains)
INFO - root - 2017-12-15 09:38:27.803426: step 1630, loss = 0.40, batch loss = 0.11 (13.0 examples/sec; 0.618 sec/batch; 56h:46m:21s remains)
INFO - root - 2017-12-15 09:38:33.942256: step 1640, loss = 0.38, batch loss = 0.09 (13.4 examples/sec; 0.596 sec/batch; 54h:45m:22s remains)
INFO - root - 2017-12-15 09:38:40.070493: step 1650, loss = 0.38, batch loss = 0.09 (13.2 examples/sec; 0.608 sec/batch; 55h:53m:20s remains)
INFO - root - 2017-12-15 09:38:46.201328: step 1660, loss = 0.43, batch loss = 0.12 (13.1 examples/sec; 0.612 sec/batch; 56h:16m:37s remains)
INFO - root - 2017-12-15 09:38:52.351181: step 1670, loss = 0.38, batch loss = 0.09 (12.8 examples/sec; 0.623 sec/batch; 57h:15m:39s remains)
INFO - root - 2017-12-15 09:38:58.498133: step 1680, loss = 0.36, batch loss = 0.10 (13.0 examples/sec; 0.615 sec/batch; 56h:30m:58s remains)
INFO - root - 2017-12-15 09:39:04.631926: step 1690, loss = 0.38, batch loss = 0.10 (12.8 examples/sec; 0.625 sec/batch; 57h:23m:53s remains)
INFO - root - 2017-12-15 09:39:10.759399: step 1700, loss = 0.38, batch loss = 0.09 (13.2 examples/sec; 0.605 sec/batch; 55h:33m:21s remains)
2017-12-15 09:39:11.223856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.2992887 -8.1959839 -9.1486759 -9.82975 -9.303586 -7.9601755 -8.1835518 -8.3991995 -8.6841927 -10.757877 -14.059488 -18.088295 -20.122257 -17.531109 -16.541716][-8.6791553 -7.9385309 -8.5727425 -10.031025 -10.063231 -8.1557426 -7.4936643 -8.1627769 -8.2946224 -9.8326616 -12.96441 -17.822495 -20.652485 -17.340189 -15.948109][-10.975001 -10.208186 -9.2745543 -10.31174 -10.809795 -8.0749559 -6.155417 -6.5763149 -6.8422017 -9.0351677 -13.29307 -18.037323 -20.355049 -17.569277 -17.714603][-10.772099 -11.835221 -10.45886 -9.3734579 -8.5309887 -5.6059275 -3.6359704 -4.0387759 -4.3553767 -6.8609457 -11.747789 -16.945595 -19.168636 -15.44236 -13.963386][-9.4992733 -10.102118 -9.4917412 -7.9145975 -6.7835903 -3.2437351 -0.070645332 -0.26166439 -0.81821537 -3.9577854 -9.2900381 -14.569508 -17.014351 -13.930163 -13.128334][-9.5251 -8.8727417 -6.9136553 -5.5238705 -3.6528182 0.4539752 3.8599234 3.850564 3.352128 -0.55089664 -6.1553712 -10.709299 -12.524753 -9.8188848 -8.868062][-7.317306 -7.1904612 -5.3087492 -2.8366058 -0.82549238 3.5272865 6.9587173 6.6451359 6.1101456 2.0359859 -3.1704235 -7.60014 -10.142097 -7.0055609 -4.354743][-6.5240722 -6.5368271 -4.4768572 -1.5946465 0.051040173 4.1887178 7.2364693 6.6426506 5.9757833 2.0477991 -2.423115 -6.5220065 -8.7514725 -5.384624 -3.1811197][-8.4925718 -7.40382 -4.9944005 -2.0995827 -0.17462349 3.4671421 6.3962479 5.7703257 4.8307552 0.71995115 -3.5801892 -6.9446893 -7.9002595 -6.6483583 -5.4179864][-11.404348 -11.32882 -9.2485876 -6.9923787 -5.0787649 -1.3978682 1.4864984 1.3269844 0.55913687 -3.3636153 -8.2215805 -11.238337 -11.089516 -9.0848656 -8.1906118][-11.534688 -12.566778 -11.210403 -10.034452 -8.9795933 -5.3707352 -2.3815885 -2.7191353 -3.3193688 -6.6881547 -11.081375 -12.81534 -12.903991 -10.934128 -10.008095][-13.039276 -14.035606 -12.735149 -12.100391 -12.032853 -8.1579361 -5.7474251 -5.7736888 -6.11602 -9.9543571 -12.928783 -13.737902 -14.085064 -10.469685 -8.1235914][-13.783419 -13.426899 -12.77537 -10.803907 -9.26685 -7.9142604 -5.7072535 -4.7301626 -4.6381664 -8.2610836 -11.45829 -11.601828 -10.443317 -7.8481574 -6.824739][-14.819867 -13.775328 -12.405497 -11.331236 -10.60066 -5.48938 -3.7861483 -6.125689 -5.4702005 -6.1785679 -7.8772326 -7.7046251 -6.7369709 -3.8260856 -1.6708908][-13.416994 -11.718185 -9.86801 -8.2721348 -9.6280518 -7.0748615 -3.8414967 -4.3126869 -4.6321421 -7.638495 -8.1261435 -6.1967621 -6.6497121 -5.6429968 -3.8944159]]...]
INFO - root - 2017-12-15 09:39:17.375413: step 1710, loss = 0.39, batch loss = 0.12 (13.3 examples/sec; 0.602 sec/batch; 55h:17m:03s remains)
INFO - root - 2017-12-15 09:39:23.492266: step 1720, loss = 0.40, batch loss = 0.09 (13.1 examples/sec; 0.613 sec/batch; 56h:17m:31s remains)
INFO - root - 2017-12-15 09:39:29.622884: step 1730, loss = 0.38, batch loss = 0.11 (12.8 examples/sec; 0.625 sec/batch; 57h:26m:29s remains)
INFO - root - 2017-12-15 09:39:35.754838: step 1740, loss = 0.38, batch loss = 0.11 (13.2 examples/sec; 0.608 sec/batch; 55h:50m:46s remains)
INFO - root - 2017-12-15 09:39:41.900071: step 1750, loss = 0.41, batch loss = 0.14 (13.2 examples/sec; 0.608 sec/batch; 55h:51m:29s remains)
INFO - root - 2017-12-15 09:39:48.032284: step 1760, loss = 0.37, batch loss = 0.10 (13.2 examples/sec; 0.605 sec/batch; 55h:36m:57s remains)
INFO - root - 2017-12-15 09:39:54.172800: step 1770, loss = 0.37, batch loss = 0.10 (12.8 examples/sec; 0.624 sec/batch; 57h:20m:08s remains)
INFO - root - 2017-12-15 09:40:00.304848: step 1780, loss = 0.46, batch loss = 0.16 (13.3 examples/sec; 0.604 sec/batch; 55h:27m:18s remains)
INFO - root - 2017-12-15 09:40:06.449031: step 1790, loss = 0.41, batch loss = 0.13 (13.3 examples/sec; 0.602 sec/batch; 55h:17m:36s remains)
INFO - root - 2017-12-15 09:40:12.592884: step 1800, loss = 0.38, batch loss = 0.10 (13.1 examples/sec; 0.610 sec/batch; 56h:03m:05s remains)
2017-12-15 09:40:13.032633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.768369 -4.8672404 -5.874639 -5.8102417 -7.0245428 -6.6182609 -4.9960594 -3.7127011 -3.1658859 -5.4608588 -8.64653 -11.832367 -13.871716 -13.966457 -13.343534][-4.795176 -4.1689019 -4.9646821 -5.6738396 -5.5293164 -4.5693254 -4.4328637 -4.4980531 -3.9370282 -4.4003429 -7.5562334 -11.183847 -13.534459 -14.241215 -14.012129][-3.3154786 -5.4447317 -5.8284845 -4.5737219 -4.0513206 -2.9208732 -1.7709222 -2.2753472 -3.0935888 -4.1214771 -7.8764896 -11.297272 -14.118662 -13.326504 -13.17453][-3.8214588 -3.6238537 -4.5536184 -3.4944437 -2.9997933 -1.6897602 -0.54782391 -0.45510864 -0.46353865 -2.9316237 -8.2187338 -11.74721 -13.298847 -13.837111 -12.827744][-3.7288191 -3.5532088 -4.1742458 -3.2106395 -3.0161848 -1.6051259 -0.23233938 -0.52671051 -0.64165211 -2.5152595 -7.0060549 -10.769708 -13.81335 -13.286185 -11.336641][-1.9980469 -2.5646462 -3.6432595 -2.4079921 -0.46179628 0.025413513 0.57288504 1.2768502 1.7823052 -0.49752188 -5.3256173 -9.1012173 -11.728142 -11.813934 -10.836583][-2.5875785 -2.6182768 -2.7249873 -1.3571873 -0.31535292 0.31569862 1.5493269 1.8522549 2.3155479 0.2606039 -4.523088 -8.3348932 -10.869644 -9.7094336 -7.4081259][-2.6105945 -2.7596009 -2.5472476 -0.76291561 0.801929 1.5181661 2.3434 2.699275 2.6659889 0.19787645 -4.5548124 -8.5154505 -10.500937 -9.5804977 -7.2182384][-1.7683067 -1.839066 -2.0642204 -0.86073875 0.080908775 1.392693 2.9302158 3.133482 3.0988712 0.13540649 -4.4382987 -7.0454469 -8.792078 -9.3002434 -7.3341002][-4.3293395 -5.2205167 -4.7640352 -3.7191424 -2.5787821 -1.4208083 -0.26618719 0.14324522 0.10095596 -2.6686764 -7.0367045 -9.1670151 -9.7969885 -9.2325354 -8.6903191][-7.8831406 -8.8503838 -9.0325851 -7.7442412 -6.2994657 -5.2699151 -4.4076147 -4.1413827 -4.4063358 -7.60581 -11.614325 -11.590683 -10.982996 -9.3728857 -7.8250589][-12.294661 -11.227143 -10.59526 -10.11741 -9.3620605 -8.041626 -7.5030451 -7.7011905 -7.5633945 -10.554068 -14.25049 -14.249484 -12.6945 -10.53311 -8.0847378][-12.55456 -11.873499 -11.49865 -10.190466 -8.6867476 -8.3803 -9.1231136 -8.0759287 -7.0451722 -10.2484 -13.921244 -14.75622 -13.475355 -12.303583 -9.6514034][-11.416997 -10.681122 -10.118067 -8.6540937 -7.7711058 -7.1551323 -7.2078505 -7.1221595 -7.5594883 -8.186739 -9.7526493 -10.345181 -9.68136 -9.1501112 -7.8215213][-11.163465 -11.465216 -8.407485 -7.2516632 -6.8682942 -7.2788892 -7.9416556 -8.00386 -7.9105458 -9.5984154 -11.728641 -9.5895443 -7.1968379 -7.1296539 -6.8034821]]...]
INFO - root - 2017-12-15 09:40:19.153595: step 1810, loss = 0.37, batch loss = 0.09 (13.3 examples/sec; 0.602 sec/batch; 55h:15m:29s remains)
INFO - root - 2017-12-15 09:40:25.241046: step 1820, loss = 0.40, batch loss = 0.09 (13.1 examples/sec; 0.613 sec/batch; 56h:17m:18s remains)
INFO - root - 2017-12-15 09:40:31.370239: step 1830, loss = 0.37, batch loss = 0.10 (13.1 examples/sec; 0.611 sec/batch; 56h:06m:22s remains)
INFO - root - 2017-12-15 09:40:37.496079: step 1840, loss = 0.40, batch loss = 0.11 (13.1 examples/sec; 0.613 sec/batch; 56h:15m:51s remains)
INFO - root - 2017-12-15 09:40:43.631626: step 1850, loss = 0.42, batch loss = 0.11 (13.2 examples/sec; 0.608 sec/batch; 55h:50m:33s remains)
INFO - root - 2017-12-15 09:40:49.732367: step 1860, loss = 0.41, batch loss = 0.12 (13.3 examples/sec; 0.602 sec/batch; 55h:14m:46s remains)
INFO - root - 2017-12-15 09:40:55.867152: step 1870, loss = 0.40, batch loss = 0.11 (13.2 examples/sec; 0.608 sec/batch; 55h:49m:58s remains)
INFO - root - 2017-12-15 09:41:02.041630: step 1880, loss = 0.40, batch loss = 0.12 (12.9 examples/sec; 0.619 sec/batch; 56h:50m:17s remains)
INFO - root - 2017-12-15 09:41:08.169526: step 1890, loss = 0.44, batch loss = 0.13 (13.2 examples/sec; 0.607 sec/batch; 55h:45m:31s remains)
INFO - root - 2017-12-15 09:41:14.298943: step 1900, loss = 0.38, batch loss = 0.13 (13.2 examples/sec; 0.607 sec/batch; 55h:46m:14s remains)
2017-12-15 09:41:14.761155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.6152143 -9.5888081 -10.800741 -10.984974 -10.817487 -9.4461088 -7.6639376 -6.5685582 -6.5821252 -7.60325 -10.167783 -11.878506 -13.366716 -12.205473 -9.76662][-9.6815338 -10.001727 -11.57114 -11.899368 -11.622646 -10.679647 -8.5054207 -6.5581 -4.7846031 -4.0412025 -8.394413 -12.485279 -14.886191 -10.370454 -5.8482127][-9.1744308 -10.2435 -11.353662 -11.776388 -11.87311 -10.369893 -8.1251488 -7.0755076 -5.7443252 -4.5838323 -7.4677339 -10.374901 -12.451365 -11.049635 -7.180459][-10.931532 -11.145748 -12.606556 -11.658741 -9.8283958 -7.1860447 -4.1609855 -4.06892 -4.097455 -3.84914 -8.5224972 -10.090816 -11.509739 -8.4598064 -3.6994085][-11.144579 -11.045963 -11.268883 -9.0386763 -7.133738 -3.736397 -0.11551428 0.37365913 0.71083355 -0.36256552 -5.5906491 -8.0132828 -8.7983952 -6.3538876 -3.4191113][-9.5835953 -8.934989 -9.1007118 -6.4996905 -3.4042745 0.28379488 4.3260746 4.9917469 4.9553804 3.4146376 -1.7336097 -4.18786 -5.8073287 -2.5814812 1.6613455][-8.2510052 -6.9194131 -6.5323753 -3.84214 -0.70170832 2.4317293 5.8231421 6.6522942 6.770112 4.9908152 0.64118338 -1.9629622 -2.8870523 0.94857311 5.1027184][-8.5483179 -6.844337 -6.3726573 -2.9286311 0.44180822 3.5472536 6.449544 6.5714293 6.8053236 5.1439877 -0.4269948 -2.4146233 -3.2640917 -1.3877358 1.5105944][-8.2614183 -6.8089323 -6.1257014 -3.0111156 -0.10988092 2.8457041 5.7761364 5.5925131 5.3812723 3.4946785 -1.5995746 -3.4732456 -4.84487 -3.7805028 -1.1143327][-9.3970728 -8.2718353 -8.0785561 -5.6097212 -2.8793421 -0.1577239 3.0020504 3.0456257 2.5949068 0.64795017 -3.3850808 -4.5217624 -4.7755451 -3.5148196 -1.2798204][-10.527202 -10.668602 -10.647064 -8.4176779 -5.9594159 -3.6333497 -1.0866818 -1.8950367 -1.8799453 -3.8004067 -6.3423471 -5.8308225 -5.3961849 -4.6969881 -3.1519091][-11.340874 -11.073477 -11.576815 -10.962683 -9.1316757 -6.5080447 -2.5275667 -2.8638701 -3.1899505 -5.3565264 -8.9412308 -8.1306562 -7.3849072 -5.0410328 -2.5955923][-9.177372 -9.2216768 -9.4942923 -7.0122352 -5.471735 -4.4330168 -2.3900323 -3.0189764 -3.4908988 -4.9735241 -8.2128506 -7.301353 -6.31102 -5.6367064 -4.0112638][-9.3002911 -8.0996695 -6.5491061 -5.8037992 -4.1911669 -1.8977489 0.67213058 -1.2272692 -2.5153205 -5.1072645 -8.0414581 -6.6076093 -4.5959163 -3.5366096 -1.8273654][-11.052081 -10.768819 -8.8688564 -6.1627879 -2.83106 -2.6568022 -2.5061049 -1.57306 -2.2358935 -4.5524621 -5.7488451 -5.4926882 -4.622714 -2.2984042 0.25654936]]...]
INFO - root - 2017-12-15 09:41:20.888658: step 1910, loss = 0.39, batch loss = 0.10 (12.9 examples/sec; 0.619 sec/batch; 56h:52m:39s remains)
INFO - root - 2017-12-15 09:41:27.040859: step 1920, loss = 0.35, batch loss = 0.09 (13.2 examples/sec; 0.607 sec/batch; 55h:42m:23s remains)
INFO - root - 2017-12-15 09:41:33.125382: step 1930, loss = 0.34, batch loss = 0.09 (13.2 examples/sec; 0.605 sec/batch; 55h:34m:01s remains)
INFO - root - 2017-12-15 09:41:39.249308: step 1940, loss = 0.37, batch loss = 0.10 (12.7 examples/sec; 0.630 sec/batch; 57h:48m:55s remains)
INFO - root - 2017-12-15 09:41:45.376510: step 1950, loss = 0.38, batch loss = 0.11 (13.0 examples/sec; 0.616 sec/batch; 56h:33m:31s remains)
INFO - root - 2017-12-15 09:41:51.543042: step 1960, loss = 0.40, batch loss = 0.13 (13.0 examples/sec; 0.615 sec/batch; 56h:27m:40s remains)
INFO - root - 2017-12-15 09:41:57.695325: step 1970, loss = 0.41, batch loss = 0.10 (13.0 examples/sec; 0.617 sec/batch; 56h:40m:08s remains)
INFO - root - 2017-12-15 09:42:03.810266: step 1980, loss = 0.35, batch loss = 0.09 (13.0 examples/sec; 0.615 sec/batch; 56h:28m:25s remains)
INFO - root - 2017-12-15 09:42:09.956133: step 1990, loss = 0.37, batch loss = 0.09 (13.1 examples/sec; 0.612 sec/batch; 56h:11m:14s remains)
INFO - root - 2017-12-15 09:42:16.089552: step 2000, loss = 0.36, batch loss = 0.09 (13.2 examples/sec; 0.607 sec/batch; 55h:41m:54s remains)
2017-12-15 09:42:16.518840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.6704316 -8.9470091 -9.87638 -9.817997 -8.5870275 -7.8615251 -7.6679268 -8.2279453 -9.7256 -11.078373 -14.134148 -14.031254 -12.699453 -12.835793 -11.866798][-9.7589922 -9.7015972 -11.133778 -11.902612 -12.15852 -11.320238 -10.053934 -8.7173786 -7.3037863 -7.9930253 -11.600992 -12.216563 -12.238721 -10.5034 -7.870141][-13.522982 -13.61097 -11.029251 -11.039848 -12.103567 -12.292456 -10.784107 -10.129976 -8.29898 -7.8130679 -10.248507 -10.697866 -11.058722 -9.8840075 -7.4324665][-9.7782431 -10.796682 -12.321743 -10.817335 -8.24033 -8.2634668 -7.5641675 -7.171484 -5.8408713 -5.9108038 -8.4360828 -8.785985 -8.9232492 -8.1131268 -7.3334341][-6.0580988 -7.6445165 -8.2579813 -6.190258 -5.3123341 -4.479003 -2.5882516 -3.2929337 -3.1275518 -3.6497746 -6.5068264 -7.2667847 -7.7583008 -7.2544985 -5.3037128][-7.6673374 -8.0392456 -7.2154441 -6.4516678 -4.1609578 -1.115458 1.4296522 0.937037 1.1508565 -1.1024065 -5.2792387 -5.9047484 -5.7492523 -4.8171616 -3.6018226][-5.9190154 -6.7255254 -6.0240803 -3.9673874 -2.1442447 -0.56161547 2.0376372 3.6337314 5.0589042 2.2246022 -2.6409035 -4.3280034 -4.6967325 -3.3206482 -1.5384417][-7.2373047 -7.0502229 -6.3529019 -3.5465384 -0.47422123 1.9017539 4.0514016 3.7384782 5.0333066 3.5333095 -0.062597752 -1.5956106 -3.4859707 -2.5221102 -0.71356583][-8.4648037 -7.6908021 -6.6794772 -4.1616287 -0.95731974 1.65624 4.3583903 4.322516 3.9134297 1.6265802 -1.8230891 -2.1196208 -2.3890157 -2.6400363 -2.1446986][-10.899421 -8.9398851 -7.9240789 -6.3835154 -4.0371552 -1.9516888 0.28123331 0.91770744 0.76351118 -1.517611 -6.0946779 -7.089756 -6.0039959 -4.4530311 -2.9360018][-17.143888 -14.607372 -12.695635 -10.856837 -8.157856 -6.3065696 -4.3664112 -4.27784 -4.4447312 -6.2322459 -10.509272 -10.35146 -9.9392366 -9.0062532 -5.9935713][-20.452286 -19.765934 -17.26903 -15.725697 -13.957766 -11.091105 -7.5803146 -6.907743 -6.5036511 -9.0322695 -12.744713 -11.851818 -12.008532 -10.094185 -6.1571283][-16.358513 -15.713198 -15.089487 -14.122849 -13.08119 -11.981003 -10.261598 -8.886322 -6.9869237 -8.0992842 -10.035201 -9.6464176 -8.9237537 -7.4062743 -6.5806122][-11.606742 -12.340104 -13.461151 -12.237349 -11.366075 -9.0014524 -7.9320483 -7.6713243 -6.1891065 -6.3302932 -6.6664448 -6.1548939 -5.2084785 -4.109539 -3.6591842][-11.586643 -11.475108 -12.051027 -13.185427 -15.927229 -12.043713 -10.112378 -8.3117514 -7.4550219 -8.2374725 -8.17741 -6.5679021 -4.2120905 -4.2638679 -4.0192671]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 09:42:22.642829: step 2010, loss = 0.41, batch loss = 0.13 (13.0 examples/sec; 0.616 sec/batch; 56h:31m:34s remains)
INFO - root - 2017-12-15 09:42:28.773531: step 2020, loss = 0.39, batch loss = 0.11 (12.6 examples/sec; 0.633 sec/batch; 58h:07m:53s remains)
INFO - root - 2017-12-15 09:42:34.932263: step 2030, loss = 0.34, batch loss = 0.09 (12.8 examples/sec; 0.627 sec/batch; 57h:30m:58s remains)
INFO - root - 2017-12-15 09:42:41.048777: step 2040, loss = 0.34, batch loss = 0.08 (12.8 examples/sec; 0.626 sec/batch; 57h:28m:12s remains)
INFO - root - 2017-12-15 09:42:47.142124: step 2050, loss = 0.36, batch loss = 0.10 (13.2 examples/sec; 0.606 sec/batch; 55h:36m:30s remains)
INFO - root - 2017-12-15 09:42:53.285249: step 2060, loss = 0.37, batch loss = 0.09 (12.9 examples/sec; 0.619 sec/batch; 56h:48m:03s remains)
INFO - root - 2017-12-15 09:42:59.399791: step 2070, loss = 0.33, batch loss = 0.08 (13.0 examples/sec; 0.614 sec/batch; 56h:22m:23s remains)
INFO - root - 2017-12-15 09:43:05.507481: step 2080, loss = 0.36, batch loss = 0.09 (12.9 examples/sec; 0.620 sec/batch; 56h:52m:06s remains)
INFO - root - 2017-12-15 09:43:11.666979: step 2090, loss = 0.36, batch loss = 0.10 (12.8 examples/sec; 0.627 sec/batch; 57h:32m:48s remains)
INFO - root - 2017-12-15 09:43:17.781865: step 2100, loss = 0.36, batch loss = 0.10 (12.9 examples/sec; 0.619 sec/batch; 56h:49m:27s remains)
2017-12-15 09:43:18.226503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-11.4588 -13.100506 -14.118776 -12.539461 -10.958432 -10.086997 -8.9337044 -8.684988 -8.1786556 -10.036642 -13.767481 -15.00079 -16.331312 -16.518353 -12.651703][-10.240175 -11.901583 -14.17195 -12.917912 -10.861698 -10.698664 -10.024994 -9.4442463 -9.005909 -10.725636 -14.002326 -14.297138 -14.87871 -15.964417 -13.664348][-11.476141 -11.975279 -12.448477 -13.097663 -12.111633 -10.6797 -8.4807873 -8.1367064 -7.6979494 -10.089303 -14.122698 -14.325905 -14.187693 -14.200118 -11.642552][-12.10335 -13.426981 -13.312403 -11.616216 -9.35086 -8.343811 -6.0569339 -6.22944 -5.9247804 -8.0381489 -12.072424 -13.079205 -13.512043 -13.08227 -9.88275][-10.490932 -12.292471 -13.077657 -10.647148 -6.8359323 -5.28158 -2.8905087 -2.7389319 -2.3419468 -4.7943187 -8.9852543 -10.127937 -11.058449 -10.080102 -6.4902978][-11.722778 -12.54949 -12.842175 -9.6778126 -5.1135917 -2.719636 0.34945488 0.26557064 0.21057367 -2.318464 -6.1777091 -7.0495687 -7.5061588 -6.9129143 -3.7859702][-10.80439 -11.094196 -10.654701 -7.2529674 -2.5832281 -0.23987246 2.3084254 2.5801826 3.0365405 0.010513306 -3.9732676 -5.1116047 -5.9812918 -5.1043997 -1.8424482][-9.5521145 -10.200278 -9.4210014 -4.9869056 -0.0098118782 2.4249272 4.742866 4.6177125 4.4897504 1.5501947 -2.6239448 -3.3474746 -4.746769 -3.7516613 0.13806486][-9.19002 -8.9318657 -8.6234035 -4.66516 0.52781343 2.9780383 4.9428639 4.6246753 4.1836524 1.7932944 -1.9819312 -2.8221614 -3.8128626 -3.4372778 -0.49821806][-12.289277 -11.742631 -11.420626 -7.4161639 -2.7000291 -0.24944019 2.151855 1.5151696 0.39353323 -2.6158454 -6.0591679 -6.6009536 -5.7999067 -4.6276579 -1.8488879][-15.897509 -15.889074 -14.921976 -10.19906 -6.2613683 -6.1801119 -3.7052739 -3.0706377 -3.8707669 -7.1109095 -10.511616 -10.056159 -9.5508022 -8.4405575 -6.2367659][-18.542131 -17.423845 -16.240231 -14.270956 -10.951023 -6.7939582 -5.5966978 -7.5161686 -8.1187134 -9.0913286 -12.738963 -12.012966 -11.727717 -9.9853163 -6.567956][-18.184464 -17.069622 -15.704287 -12.331467 -9.5515718 -8.8416119 -7.9663897 -6.1924987 -6.0066824 -9.1883163 -13.883875 -11.490271 -9.6852083 -7.9473085 -5.067173][-14.329531 -16.418282 -18.111526 -14.606401 -10.611246 -9.44165 -8.226264 -8.4203377 -8.6183615 -7.5565414 -7.6295824 -8.0373974 -8.9425764 -7.6356215 -4.3747492][-14.718479 -15.743637 -16.1497 -14.419281 -13.117697 -12.163925 -8.8559446 -8.2318535 -7.5013366 -8.8474474 -8.2581825 -5.693058 -5.5625987 -5.0588088 -4.23046]]...]
INFO - root - 2017-12-15 09:43:24.325531: step 2110, loss = 0.37, batch loss = 0.09 (13.3 examples/sec; 0.603 sec/batch; 55h:21m:51s remains)
INFO - root - 2017-12-15 09:43:30.430616: step 2120, loss = 0.38, batch loss = 0.13 (12.6 examples/sec; 0.634 sec/batch; 58h:09m:17s remains)
INFO - root - 2017-12-15 09:43:36.587694: step 2130, loss = 0.35, batch loss = 0.09 (12.9 examples/sec; 0.621 sec/batch; 56h:58m:45s remains)
INFO - root - 2017-12-15 09:43:42.634646: step 2140, loss = 0.34, batch loss = 0.08 (13.3 examples/sec; 0.599 sec/batch; 54h:59m:49s remains)
INFO - root - 2017-12-15 09:43:48.725057: step 2150, loss = 0.35, batch loss = 0.11 (13.2 examples/sec; 0.605 sec/batch; 55h:30m:41s remains)
INFO - root - 2017-12-15 09:43:54.905709: step 2160, loss = 0.37, batch loss = 0.11 (12.8 examples/sec; 0.623 sec/batch; 57h:10m:53s remains)
INFO - root - 2017-12-15 09:44:01.010437: step 2170, loss = 0.38, batch loss = 0.13 (13.1 examples/sec; 0.611 sec/batch; 56h:05m:45s remains)
INFO - root - 2017-12-15 09:44:07.208252: step 2180, loss = 0.35, batch loss = 0.09 (12.6 examples/sec; 0.636 sec/batch; 58h:19m:22s remains)
INFO - root - 2017-12-15 09:44:13.350214: step 2190, loss = 0.38, batch loss = 0.13 (13.1 examples/sec; 0.610 sec/batch; 56h:00m:18s remains)
INFO - root - 2017-12-15 09:44:19.483894: step 2200, loss = 0.39, batch loss = 0.13 (13.0 examples/sec; 0.617 sec/batch; 56h:34m:58s remains)
2017-12-15 09:44:19.913853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.0432167 -8.7428837 -9.3793344 -9.2174253 -7.3360786 -5.1393018 -4.5339661 -3.9573154 -3.6362886 -4.3804808 -8.5943556 -12.497698 -13.897064 -12.417228 -10.532253][-6.8692746 -5.4765587 -6.3665338 -5.9300003 -6.5695362 -5.1769385 -3.3155608 -3.0538206 -2.8403523 -4.0136137 -8.5852337 -12.338696 -13.14539 -12.335581 -8.5700674][-6.7829347 -6.1928573 -4.6804767 -4.440876 -4.2370567 -2.5947731 -0.31727219 -0.93149137 -1.2687864 -2.7786205 -7.9491205 -10.857878 -12.480718 -10.949252 -8.8322411][-6.5268126 -7.0701928 -6.1251678 -3.6489646 -1.9361029 -1.255218 0.72347069 1.3166914 1.0428395 -0.60870695 -5.7028589 -8.457449 -8.9870529 -10.034166 -9.0766611][-6.4907913 -6.1012297 -5.4842787 -3.3322024 -1.1637454 -0.25719738 1.3248482 1.6376081 1.8322158 0.027709961 -4.6342854 -6.6694031 -7.9829721 -7.8570471 -7.223608][-5.0636897 -4.8291225 -3.6093445 -1.662302 0.65767908 1.6689267 1.9538884 2.1702704 2.676888 0.80648184 -4.0291719 -6.08865 -5.7695446 -5.1911449 -4.059073][-5.6049628 -4.6285915 -2.8030698 0.13799667 1.896276 2.7522759 3.0928073 3.0462022 3.1968083 1.1636057 -3.0127754 -4.8155289 -5.3231091 -3.3703189 -1.1646967][-5.8621492 -4.9122925 -2.759027 0.57869339 2.6287313 3.728601 4.1731448 3.7498145 3.7198138 1.8144612 -1.9190187 -3.1459823 -3.8773205 -2.6730926 0.074741364][-6.0150094 -4.9710741 -2.9991353 0.010883808 2.2374816 3.1832395 3.7641225 3.9505353 3.9318585 2.1407704 -1.3110466 -1.8858213 -2.3041842 -2.722918 -0.52446222][-9.5189524 -8.0988846 -6.2250357 -3.3905802 -1.2450967 -0.25750542 0.042137623 0.30477858 0.75712872 -0.60435009 -4.2739568 -5.2576375 -4.0312748 -3.4109147 -3.5114729][-12.893274 -13.10668 -11.88156 -9.1010227 -6.53853 -5.2546096 -4.4663787 -3.9181247 -3.5361004 -5.3474979 -8.4558153 -8.7434006 -8.9446306 -7.345263 -5.5525241][-17.051657 -17.691164 -16.370806 -14.554287 -12.041642 -10.264769 -8.8559275 -9.4200182 -10.276443 -11.479533 -13.394787 -13.725055 -13.420872 -11.447954 -7.96928][-19.263529 -18.246628 -16.2985 -12.877716 -10.418097 -10.039875 -11.136316 -10.368792 -8.762888 -11.892886 -16.29282 -16.096291 -14.34996 -14.241192 -11.775814][-18.833292 -15.546793 -13.147909 -11.156413 -7.7959991 -5.639667 -5.7937126 -8.06349 -9.3932714 -10.656694 -12.929556 -12.800409 -12.648627 -12.334715 -11.107003][-15.951822 -14.991785 -12.819607 -10.287237 -9.1253185 -7.4210382 -7.4065886 -7.7573361 -10.327181 -11.290242 -12.804863 -12.941395 -13.067856 -13.024853 -12.667734]]...]
INFO - root - 2017-12-15 09:44:26.024815: step 2210, loss = 0.36, batch loss = 0.10 (13.0 examples/sec; 0.617 sec/batch; 56h:36m:54s remains)
INFO - root - 2017-12-15 09:44:32.121092: step 2220, loss = 0.32, batch loss = 0.08 (13.3 examples/sec; 0.604 sec/batch; 55h:23m:26s remains)
INFO - root - 2017-12-15 09:44:38.252655: step 2230, loss = 0.35, batch loss = 0.10 (13.0 examples/sec; 0.617 sec/batch; 56h:34m:59s remains)
INFO - root - 2017-12-15 09:44:44.364261: step 2240, loss = 0.39, batch loss = 0.13 (13.1 examples/sec; 0.613 sec/batch; 56h:12m:57s remains)
INFO - root - 2017-12-15 09:44:50.494893: step 2250, loss = 0.33, batch loss = 0.09 (13.1 examples/sec; 0.608 sec/batch; 55h:48m:45s remains)
INFO - root - 2017-12-15 09:44:56.673191: step 2260, loss = 0.37, batch loss = 0.10 (13.1 examples/sec; 0.609 sec/batch; 55h:50m:35s remains)
INFO - root - 2017-12-15 09:45:02.808144: step 2270, loss = 0.36, batch loss = 0.09 (13.3 examples/sec; 0.604 sec/batch; 55h:22m:58s remains)
INFO - root - 2017-12-15 09:45:08.970758: step 2280, loss = 0.34, batch loss = 0.10 (12.9 examples/sec; 0.621 sec/batch; 56h:57m:21s remains)
INFO - root - 2017-12-15 09:45:15.093701: step 2290, loss = 0.32, batch loss = 0.08 (12.9 examples/sec; 0.619 sec/batch; 56h:48m:22s remains)
INFO - root - 2017-12-15 09:45:21.189803: step 2300, loss = 0.32, batch loss = 0.08 (13.1 examples/sec; 0.609 sec/batch; 55h:50m:59s remains)
2017-12-15 09:45:21.627052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4273686 -5.1585708 -5.347518 -4.2904539 -3.7106833 -3.5784924 -2.7688873 -2.5464871 -2.4958069 -6.0558472 -8.0989971 -10.15556 -13.876118 -16.205244 -14.730592][-4.6066575 -5.154388 -5.2667751 -4.6203833 -4.7655544 -4.0142732 -3.2908983 -3.6916659 -3.0983031 -5.2566485 -7.4305305 -11.246404 -14.752002 -14.958084 -12.521059][-5.4778848 -7.0217805 -6.9344711 -6.5526085 -6.9890175 -6.5201039 -4.0883427 -3.5215881 -3.4248958 -6.1473708 -8.0462437 -11.148211 -14.030415 -14.133981 -13.14478][-5.3925786 -8.6086617 -9.6878433 -7.604589 -5.8997836 -4.6972857 -2.6529489 -2.7995181 -1.7670736 -3.7872181 -5.9909816 -9.3584976 -12.424418 -12.32584 -9.7469635][-5.6076183 -7.9213347 -8.3485928 -5.9936619 -3.6360745 -1.7970152 1.1294909 1.5448003 2.015337 -0.81837893 -2.9590769 -5.883976 -8.6319761 -8.6117935 -8.1953888][-6.3037071 -7.4319129 -7.2069864 -5.065506 -2.3609095 0.1755085 3.2150106 3.2348328 3.3250647 1.1360006 -0.79620028 -4.09414 -6.2231607 -4.7161279 -2.7080848][-6.0920453 -7.41642 -6.8992424 -3.7557337 -0.77791595 1.4623942 4.1923132 4.1755848 4.1464043 0.95631456 -0.718637 -2.3700249 -3.7600853 -2.2400527 -0.48354816][-6.6686907 -7.2469535 -6.0306983 -2.6771097 0.37551928 2.5893784 5.0303335 4.664896 4.6376925 1.5718951 -1.1025305 -3.2426858 -5.0681915 -3.0791454 0.36602879][-7.2438965 -7.7304053 -6.6671791 -3.1252153 0.42348051 2.8931141 5.5019655 4.9073038 4.0491734 1.1645088 -1.6057277 -3.4422905 -4.7488236 -3.7840703 -1.2008328][-10.021605 -11.081122 -9.13327 -5.68111 -2.4599826 0.045340061 2.7639246 2.0968037 1.474031 -1.6384296 -4.4937253 -4.7527161 -3.8441002 -1.4494896 -0.81874084][-10.39719 -13.167086 -11.94073 -8.036768 -5.0943413 -3.8710067 -2.6358333 -2.6356962 -1.6478167 -3.4036219 -5.2460022 -5.1202927 -5.26019 -3.8877654 -2.5826163][-15.053203 -15.837901 -14.826173 -11.870494 -9.287118 -6.3681493 -2.9119782 -4.9673867 -7.3570614 -8.2315464 -9.2686577 -8.878087 -8.214262 -6.8287888 -6.1530652][-17.174665 -16.381033 -14.449733 -11.314312 -8.4062424 -6.2610469 -5.1035552 -5.9701304 -6.468555 -8.4473295 -11.678514 -11.072224 -10.288767 -8.8247032 -7.8212643][-13.107372 -14.125753 -14.39477 -12.093556 -8.6319542 -6.4540462 -4.8371611 -5.8323359 -6.0952377 -7.60183 -10.371002 -9.5435066 -8.1451426 -8.2018986 -8.9426126][-11.761097 -12.227594 -12.17642 -11.518208 -11.439554 -10.088093 -7.8425751 -8.6797829 -9.5569038 -9.7918558 -10.470264 -9.2857 -8.6423588 -8.346488 -8.9330635]]...]
INFO - root - 2017-12-15 09:45:27.755091: step 2310, loss = 0.36, batch loss = 0.09 (13.3 examples/sec; 0.603 sec/batch; 55h:18m:34s remains)
INFO - root - 2017-12-15 09:45:33.875247: step 2320, loss = 0.34, batch loss = 0.09 (13.0 examples/sec; 0.616 sec/batch; 56h:29m:11s remains)
INFO - root - 2017-12-15 09:45:40.000320: step 2330, loss = 0.36, batch loss = 0.11 (13.0 examples/sec; 0.615 sec/batch; 56h:26m:58s remains)
INFO - root - 2017-12-15 09:45:46.124409: step 2340, loss = 0.32, batch loss = 0.08 (13.3 examples/sec; 0.603 sec/batch; 55h:20m:42s remains)
INFO - root - 2017-12-15 09:45:52.216078: step 2350, loss = 0.37, batch loss = 0.09 (13.2 examples/sec; 0.607 sec/batch; 55h:40m:50s remains)
INFO - root - 2017-12-15 09:45:58.375323: step 2360, loss = 0.33, batch loss = 0.10 (13.0 examples/sec; 0.615 sec/batch; 56h:24m:10s remains)
INFO - root - 2017-12-15 09:46:04.458256: step 2370, loss = 0.36, batch loss = 0.09 (12.9 examples/sec; 0.619 sec/batch; 56h:44m:31s remains)
INFO - root - 2017-12-15 09:46:10.602492: step 2380, loss = 0.36, batch loss = 0.10 (13.1 examples/sec; 0.610 sec/batch; 55h:56m:45s remains)
INFO - root - 2017-12-15 09:46:16.706895: step 2390, loss = 0.34, batch loss = 0.09 (13.1 examples/sec; 0.613 sec/batch; 56h:11m:57s remains)
INFO - root - 2017-12-15 09:46:22.849015: step 2400, loss = 0.36, batch loss = 0.11 (13.1 examples/sec; 0.612 sec/batch; 56h:07m:06s remains)
2017-12-15 09:46:23.328992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.4444308 -6.360157 -8.1724558 -6.2911592 -5.39562 -5.5402923 -4.781466 -4.8654404 -4.7017555 -7.7304859 -10.310433 -14.294882 -15.647196 -14.500757 -11.039673][-6.2066131 -6.4639158 -7.0061827 -7.1883554 -7.180469 -6.3790245 -4.0030918 -4.8285761 -5.3754721 -8.5031443 -11.172461 -14.423243 -15.180422 -16.834539 -14.37808][-3.0651011 -5.4218965 -6.3175936 -7.4708843 -7.2993097 -5.7492833 -3.0758624 -3.4463665 -4.0984411 -7.4293566 -10.549562 -13.636452 -14.284677 -14.710114 -14.248497][-5.2649875 -7.7878008 -7.778717 -8.6814623 -8.1110172 -6.6797709 -3.437979 -3.0088019 -3.078172 -6.7528467 -10.465209 -14.322768 -15.480474 -15.410315 -13.392629][-3.6292827 -7.2152615 -7.3927231 -7.7413325 -7.0942993 -5.8762212 -2.5664701 -2.0924053 -1.5697494 -4.6337476 -7.769011 -11.953526 -13.644474 -13.907573 -12.22323][-1.7141075 -6.2424364 -7.7249241 -7.2008595 -5.2548857 -2.896769 0.81247091 1.6363039 2.4070725 -0.83376312 -4.0805674 -7.9514141 -9.7182817 -9.6325378 -8.5505028][-3.7423213 -5.2306967 -5.0674973 -3.8127477 -2.0753117 -0.21970987 3.1677728 4.1405129 4.8450255 1.2990189 -1.5015507 -5.035439 -7.385704 -7.3001332 -5.8217368][-4.4408221 -3.944586 -2.882962 -0.85011387 0.89280987 2.3199878 4.6239295 4.6940427 4.7612338 1.4564533 -1.660809 -4.9492512 -6.9389586 -6.274374 -3.0886536][-4.3917756 -4.2379589 -3.6988766 -1.9839797 -0.28274632 1.5676928 4.4241281 4.1919055 3.7318053 0.43785 -3.2001972 -5.9713321 -7.5927238 -9.2965574 -7.3716335][-7.3091288 -8.50617 -7.9488649 -6.3118796 -4.4525251 -2.3037276 0.8478756 0.97860479 0.81978512 -2.8579633 -6.5484219 -9.9195309 -11.454268 -11.832001 -11.308085][-7.98575 -11.415318 -11.531388 -9.6035 -7.1851382 -5.2748995 -2.8218884 -3.0555675 -4.2043524 -7.3144107 -9.9305992 -13.105125 -13.4877 -12.425919 -11.471487][-11.884595 -13.912674 -13.626986 -13.464594 -13.492759 -10.56831 -7.3383341 -7.1103148 -7.415153 -10.718418 -14.677223 -15.018032 -14.235641 -13.437621 -11.744841][-14.791996 -14.379868 -14.29439 -11.822716 -10.907265 -11.407317 -11.668722 -12.008286 -10.618349 -13.592426 -16.763227 -15.781626 -14.174245 -11.749609 -9.7091179][-8.8842335 -9.4821854 -11.154669 -10.757494 -10.094223 -8.2682877 -6.4746075 -6.805037 -6.53974 -8.9146452 -11.207914 -11.909914 -13.889422 -12.776857 -10.943418][-10.337376 -10.208639 -9.5340672 -9.9598484 -10.987225 -10.829495 -9.6092033 -8.8223124 -7.8326769 -8.3246069 -9.6370745 -10.863034 -12.245899 -11.436281 -8.7452822]]...]
INFO - root - 2017-12-15 09:46:29.478162: step 2410, loss = 0.31, batch loss = 0.08 (13.2 examples/sec; 0.605 sec/batch; 55h:29m:04s remains)
INFO - root - 2017-12-15 09:46:35.589358: step 2420, loss = 0.37, batch loss = 0.09 (13.2 examples/sec; 0.607 sec/batch; 55h:38m:49s remains)
INFO - root - 2017-12-15 09:46:41.737739: step 2430, loss = 0.34, batch loss = 0.10 (13.0 examples/sec; 0.616 sec/batch; 56h:26m:52s remains)
INFO - root - 2017-12-15 09:46:47.853953: step 2440, loss = 0.34, batch loss = 0.10 (13.1 examples/sec; 0.609 sec/batch; 55h:47m:29s remains)
INFO - root - 2017-12-15 09:46:54.032540: step 2450, loss = 0.33, batch loss = 0.08 (13.0 examples/sec; 0.615 sec/batch; 56h:21m:19s remains)
INFO - root - 2017-12-15 09:47:00.162198: step 2460, loss = 0.33, batch loss = 0.09 (12.7 examples/sec; 0.629 sec/batch; 57h:37m:24s remains)
INFO - root - 2017-12-15 09:47:06.307466: step 2470, loss = 0.38, batch loss = 0.11 (13.1 examples/sec; 0.609 sec/batch; 55h:48m:22s remains)
INFO - root - 2017-12-15 09:47:12.440194: step 2480, loss = 0.36, batch loss = 0.12 (13.1 examples/sec; 0.609 sec/batch; 55h:48m:30s remains)
INFO - root - 2017-12-15 09:47:18.562588: step 2490, loss = 0.32, batch loss = 0.08 (13.0 examples/sec; 0.617 sec/batch; 56h:31m:15s remains)
INFO - root - 2017-12-15 09:47:24.691397: step 2500, loss = 0.37, batch loss = 0.08 (12.6 examples/sec; 0.634 sec/batch; 58h:06m:24s remains)
2017-12-15 09:47:25.123204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2193503 -7.3820353 -7.6162205 -6.0567203 -3.9878378 -2.9971182 -2.8532541 -3.5733333 -4.7125993 -7.7898788 -11.469864 -13.346391 -14.573587 -12.566618 -10.37151][-9.0530548 -7.371727 -8.5487022 -7.3376603 -5.3260159 -3.3784857 -3.1290827 -3.1542439 -3.0191584 -4.7418976 -9.0137167 -13.325111 -16.204996 -15.442862 -14.278953][-1.5361075 -4.6402392 -7.9469995 -9.0588646 -8.9178791 -6.2425108 -4.2663403 -3.8317912 -3.0591004 -4.0826068 -6.9335356 -9.570612 -12.497622 -12.959743 -13.21489][-2.9412265 -5.3569555 -7.6468105 -7.586132 -6.0796418 -4.7223468 -3.9839084 -3.6521633 -2.3500347 -3.3215973 -6.0272703 -8.0797539 -10.831463 -10.275474 -11.449478][1.5355034 -4.2862253 -5.6253009 -3.8604712 -2.6759281 -1.0293379 -0.26620054 -1.1417909 -1.9590507 -3.9584377 -6.689888 -8.2300768 -9.423934 -9.134985 -9.17923][5.3802962 1.4274063 -3.1628664 -1.964396 0.38997889 2.3907018 2.7862706 3.1083417 3.7253652 -0.76153183 -6.5206485 -8.4088326 -9.3023548 -7.4857893 -6.2613544][3.1811757 1.3385301 -1.2748804 -2.2217517 0.2098546 3.3536105 5.298038 6.40857 6.6777883 3.0175948 -1.7300119 -5.8575554 -8.75744 -7.3035636 -6.5936537][-3.5143385 -2.9421268 -2.36595 -1.5158982 -0.56769085 3.1529527 5.3513007 6.215611 7.57933 4.5290694 -1.2224483 -4.88233 -8.8392715 -8.2134628 -6.5465565][-9.0859318 -10.915815 -8.04712 -4.40391 -1.7697225 1.2619662 3.4091735 4.5306821 4.966248 1.836431 -3.4792733 -6.7602434 -10.081947 -10.761846 -10.879492][-8.4402695 -9.52263 -12.111919 -8.2416191 -5.1903925 -2.7465842 -1.1357946 -0.16982126 -0.19977093 -4.1418705 -10.095565 -13.290543 -15.068661 -13.517715 -13.671007][-5.5782909 -8.6238155 -11.069013 -11.535839 -8.4799109 -8.0843592 -9.15337 -10.007019 -11.399167 -15.071304 -18.668917 -21.019222 -20.907391 -17.147734 -15.845379][-7.1753173 -10.02638 -13.048111 -14.029016 -15.450817 -13.188988 -12.321758 -14.782961 -15.782915 -21.254576 -25.908873 -24.174532 -22.911903 -20.367821 -16.503445][-11.868099 -11.560448 -11.390395 -11.011848 -11.668713 -12.81172 -13.440134 -12.46102 -13.173216 -17.835886 -22.638435 -23.699024 -23.635204 -19.44713 -15.041431][-8.6800976 -8.391243 -8.5903339 -5.3189445 -6.9134808 -8.5159454 -7.997797 -8.1240339 -7.1480412 -7.8485603 -11.904791 -13.738525 -13.89611 -15.731524 -17.012835][-11.550671 -8.4818964 -6.3986259 -8.8736115 -8.4730482 -7.924222 -9.5720291 -7.8585844 -6.1883206 -8.3853569 -9.8349342 -9.5324717 -10.231056 -11.966139 -12.716156]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-15 09:47:31.836454: step 2510, loss = 0.37, batch loss = 0.12 (13.1 examples/sec; 0.610 sec/batch; 55h:52m:35s remains)
INFO - root - 2017-12-15 09:47:37.952792: step 2520, loss = 0.36, batch loss = 0.11 (13.1 examples/sec; 0.613 sec/batch; 56h:11m:13s remains)
INFO - root - 2017-12-15 09:47:44.118691: step 2530, loss = 0.37, batch loss = 0.10 (13.2 examples/sec; 0.607 sec/batch; 55h:37m:04s remains)
INFO - root - 2017-12-15 09:47:50.241837: step 2540, loss = 0.40, batch loss = 0.09 (13.1 examples/sec; 0.608 sec/batch; 55h:46m:07s remains)
INFO - root - 2017-12-15 09:47:56.374263: step 2550, loss = 0.35, batch loss = 0.09 (13.2 examples/sec; 0.607 sec/batch; 55h:36m:31s remains)
INFO - root - 2017-12-15 09:48:02.502018: step 2560, loss = 0.31, batch loss = 0.08 (13.0 examples/sec; 0.613 sec/batch; 56h:13m:32s remains)
INFO - root - 2017-12-15 09:48:08.600862: step 2570, loss = 0.35, batch loss = 0.10 (13.2 examples/sec; 0.604 sec/batch; 55h:23m:21s remains)
INFO - root - 2017-12-15 09:48:14.753189: step 2580, loss = 0.33, batch loss = 0.10 (13.0 examples/sec; 0.614 sec/batch; 56h:15m:27s remains)
INFO - root - 2017-12-15 09:48:20.863612: step 2590, loss = 0.33, batch loss = 0.08 (13.2 examples/sec; 0.605 sec/batch; 55h:29m:08s remains)
INFO - root - 2017-12-15 09:48:27.025072: step 2600, loss = 0.35, batch loss = 0.09 (13.0 examples/sec; 0.614 sec/batch; 56h:13m:47s remains)
2017-12-15 09:48:27.451078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-15.452278 -16.357933 -18.059052 -13.012153 -8.9317026 -10.019299 -10.866961 -11.088032 -10.867165 -13.572367 -19.919476 -20.436644 -21.323267 -24.542942 -22.452646][-13.562395 -16.042339 -17.373827 -13.74287 -11.648375 -11.566919 -9.71557 -9.3523779 -10.083625 -12.428593 -18.364727 -20.90941 -23.259594 -23.547625 -20.081125][-16.328051 -16.399946 -16.01092 -15.694151 -14.906059 -13.135293 -10.844453 -10.706272 -9.6205139 -11.379765 -17.905361 -20.023159 -23.297684 -24.984104 -22.120665][-16.781403 -18.04162 -18.092228 -14.070721 -10.661193 -10.319469 -9.0151272 -8.5813084 -7.8090529 -9.1180477 -14.944431 -16.057316 -19.749548 -22.448647 -21.176802][-9.1548052 -16.141405 -17.753124 -12.381215 -8.0818586 -6.3350792 -3.9435096 -4.0569506 -4.3050628 -6.1758685 -12.054506 -12.610179 -15.215803 -17.620993 -16.222069][-10.932328 -11.634838 -12.440195 -10.355678 -5.6001077 -1.2825804 1.483624 1.3357787 1.6579614 -0.8357749 -7.7153163 -8.2472038 -9.7533045 -10.820637 -9.990654][-12.039837 -10.872608 -11.560884 -6.8023143 -1.7187667 1.6912885 4.3695064 5.0405211 5.5364079 1.784348 -4.1220465 -4.4864187 -6.6912341 -6.7918062 -4.7489204][-9.2508316 -8.8181725 -7.5681934 -3.7083104 0.37116528 3.3721051 5.7427297 5.3166318 4.6719527 1.9354062 -3.5365241 -4.7135077 -6.5867476 -6.6070528 -3.3082018][-9.8533363 -7.5234947 -6.3470488 -3.542726 0.43029356 2.3151479 4.259275 4.4006267 3.7892032 0.92127943 -4.6878772 -4.887784 -5.95918 -6.6043797 -4.6213388][-14.319321 -12.945562 -11.356224 -7.1685591 -3.7279906 -2.8978434 -1.9104624 -1.6744452 -1.3928246 -3.4407721 -8.5749207 -8.88677 -8.6743889 -9.3147116 -10.747816][-12.837654 -15.034662 -14.976696 -11.603661 -8.3603706 -9.0375347 -8.022438 -5.8296185 -6.5774455 -7.981473 -11.300789 -11.345312 -12.045233 -12.754994 -10.931442][-19.340425 -18.85799 -19.709503 -18.546293 -16.886681 -14.94319 -10.696334 -13.39972 -15.780703 -14.219421 -16.0693 -15.084211 -14.630148 -14.110432 -12.197521][-21.854555 -19.908949 -18.721741 -15.310171 -12.666718 -14.535389 -15.827765 -13.278912 -9.3727207 -13.920576 -20.725145 -17.455282 -12.802724 -13.894589 -13.746275][-17.506933 -18.937946 -17.018484 -14.731644 -13.90242 -11.713116 -10.59041 -11.727848 -13.009849 -11.677527 -11.671524 -12.877619 -14.498966 -12.749244 -10.569223][-17.677395 -14.990248 -17.723198 -17.655315 -16.765574 -14.369436 -11.230339 -11.983819 -13.500738 -15.419754 -15.70981 -13.193539 -10.926725 -12.610218 -13.688785]]...]
INFO - root - 2017-12-15 09:48:33.596712: step 2610, loss = 0.33, batch loss = 0.11 (13.1 examples/sec; 0.609 sec/batch; 55h:50m:25s remains)
INFO - root - 2017-12-15 09:48:39.720435: step 2620, loss = 0.34, batch loss = 0.09 (13.2 examples/sec; 0.605 sec/batch; 55h:25m:39s remains)
INFO - root - 2017-12-15 09:48:45.852360: step 2630, loss = 0.37, batch loss = 0.13 (12.6 examples/sec; 0.633 sec/batch; 57h:58m:00s remains)
INFO - root - 2017-12-15 09:48:51.990629: step 2640, loss = 0.33, batch loss = 0.09 (13.2 examples/sec; 0.607 sec/batch; 55h:37m:39s remains)
INFO - root - 2017-12-15 09:48:58.100974: step 2650, loss = 0.32, batch loss = 0.10 (13.0 examples/sec; 0.615 sec/batch; 56h:19m:30s remains)
INFO - root - 2017-12-15 09:49:04.269991: step 2660, loss = 0.34, batch loss = 0.10 (13.3 examples/sec; 0.603 sec/batch; 55h:15m:54s remains)
INFO - root - 2017-12-15 09:49:10.415888: step 2670, loss = 0.32, batch loss = 0.09 (13.1 examples/sec; 0.610 sec/batch; 55h:54m:02s remains)
INFO - root - 2017-12-15 09:49:16.535982: step 2680, loss = 0.35, batch loss = 0.11 (13.2 examples/sec; 0.607 sec/batch; 55h:36m:48s remains)
