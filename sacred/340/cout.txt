INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "340"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-clip-10-10-test1111
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
/home/v-chaoqw/MYSFC-ORI/workspace/train_imdb.pickle
inputs Tensor("train/batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("train/siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("train/siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("train/siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("train/siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("train/batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("train/siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("train/siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("train/siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("train/siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
/home/v-chaoqw/MYSFC-ORI/workspace/val_imdb.pickle
inputs Tensor("val/batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("val/siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("val/siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("val/siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("val/siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("val/batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("val/siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("val/siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("val/siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("val/siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
2017-12-17 07:05:26.653843: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-17 07:05:26.653885: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-17 07:05:26.653891: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-17 07:05:26.653896: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-17 07:05:26.653900: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-17 07:05:27.019315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-17 07:05:27.019355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-17 07:05:27.019362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-17 07:05:27.019370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO - root - Restore from last checkpoint: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-17 07:05:30.173087: step 0, loss = 2.28, batch loss = 2.23 (3.3 examples/sec; 2.394 sec/batch; 221h:04m:34s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-clip-10-10-test1111/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-clip-10-10-test1111/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-17 07:05:33.047584: step 10, loss = 1.56, batch loss = 1.50 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:27s remains)
INFO - root - 2017-12-17 07:05:35.077741: step 20, loss = 0.75, batch loss = 0.69 (39.5 examples/sec; 0.202 sec/batch; 18h:41m:27s remains)
INFO - root - 2017-12-17 07:05:37.115479: step 30, loss = 0.65, batch loss = 0.59 (39.3 examples/sec; 0.203 sec/batch; 18h:46m:45s remains)
INFO - root - 2017-12-17 07:05:39.179278: step 40, loss = 0.49, batch loss = 0.43 (39.2 examples/sec; 0.204 sec/batch; 18h:50m:19s remains)
INFO - root - 2017-12-17 07:05:41.219717: step 50, loss = 0.67, batch loss = 0.61 (39.8 examples/sec; 0.201 sec/batch; 18h:33m:20s remains)
INFO - root - 2017-12-17 07:05:43.260720: step 60, loss = 0.48, batch loss = 0.42 (38.8 examples/sec; 0.206 sec/batch; 19h:03m:24s remains)
INFO - root - 2017-12-17 07:05:45.324265: step 70, loss = 0.53, batch loss = 0.47 (39.0 examples/sec; 0.205 sec/batch; 18h:56m:38s remains)
INFO - root - 2017-12-17 07:05:47.361380: step 80, loss = 0.61, batch loss = 0.55 (39.5 examples/sec; 0.202 sec/batch; 18h:41m:16s remains)
INFO - root - 2017-12-17 07:05:49.403510: step 90, loss = 0.45, batch loss = 0.39 (39.4 examples/sec; 0.203 sec/batch; 18h:44m:49s remains)
INFO - root - 2017-12-17 07:05:51.448523: step 100, loss = 0.82, batch loss = 0.73 (39.8 examples/sec; 0.201 sec/batch; 18h:32m:21s remains)
INFO - root - 2017-12-17 07:05:53.640744: step 110, loss = 0.73, batch loss = 0.63 (38.4 examples/sec; 0.208 sec/batch; 19h:13m:52s remains)
INFO - root - 2017-12-17 07:05:55.696539: step 120, loss = 0.69, batch loss = 0.59 (39.7 examples/sec; 0.201 sec/batch; 18h:34m:57s remains)
INFO - root - 2017-12-17 07:05:57.732061: step 130, loss = 0.61, batch loss = 0.50 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:28s remains)
INFO - root - 2017-12-17 07:05:59.775652: step 140, loss = 0.54, batch loss = 0.43 (38.7 examples/sec; 0.207 sec/batch; 19h:04m:08s remains)
INFO - root - 2017-12-17 07:06:01.828786: step 150, loss = 0.50, batch loss = 0.39 (39.7 examples/sec; 0.202 sec/batch; 18h:37m:11s remains)
INFO - root - 2017-12-17 07:06:03.873604: step 160, loss = 0.62, batch loss = 0.51 (39.3 examples/sec; 0.204 sec/batch; 18h:47m:48s remains)
INFO - root - 2017-12-17 07:06:05.920650: step 170, loss = 0.53, batch loss = 0.42 (39.3 examples/sec; 0.204 sec/batch; 18h:48m:22s remains)
INFO - root - 2017-12-17 07:06:07.966754: step 180, loss = 0.63, batch loss = 0.52 (39.6 examples/sec; 0.202 sec/batch; 18h:39m:54s remains)
INFO - root - 2017-12-17 07:06:10.008726: step 190, loss = 0.51, batch loss = 0.40 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:47s remains)
INFO - root - 2017-12-17 07:06:12.071142: step 200, loss = 0.54, batch loss = 0.43 (38.8 examples/sec; 0.206 sec/batch; 19h:02m:44s remains)
INFO - root - 2017-12-17 07:06:14.249968: step 210, loss = 0.48, batch loss = 0.37 (40.0 examples/sec; 0.200 sec/batch; 18h:27m:58s remains)
INFO - root - 2017-12-17 07:06:16.317888: step 220, loss = 0.49, batch loss = 0.38 (39.1 examples/sec; 0.205 sec/batch; 18h:52m:52s remains)
INFO - root - 2017-12-17 07:06:18.376717: step 230, loss = 0.55, batch loss = 0.44 (39.4 examples/sec; 0.203 sec/batch; 18h:45m:16s remains)
INFO - root - 2017-12-17 07:06:20.438925: step 240, loss = 0.44, batch loss = 0.33 (39.0 examples/sec; 0.205 sec/batch; 18h:54m:30s remains)
INFO - root - 2017-12-17 07:06:22.493328: step 250, loss = 0.56, batch loss = 0.45 (39.3 examples/sec; 0.204 sec/batch; 18h:47m:15s remains)
INFO - root - 2017-12-17 07:06:24.573883: step 260, loss = 0.51, batch loss = 0.40 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:19s remains)
INFO - root - 2017-12-17 07:06:26.633671: step 270, loss = 0.45, batch loss = 0.34 (38.4 examples/sec; 0.208 sec/batch; 19h:14m:01s remains)
INFO - root - 2017-12-17 07:06:28.710185: step 280, loss = 0.41, batch loss = 0.30 (38.4 examples/sec; 0.208 sec/batch; 19h:14m:14s remains)
INFO - root - 2017-12-17 07:06:30.795098: step 290, loss = 0.46, batch loss = 0.35 (38.8 examples/sec; 0.206 sec/batch; 19h:02m:08s remains)
INFO - root - 2017-12-17 07:06:32.836780: step 300, loss = 0.44, batch loss = 0.33 (39.3 examples/sec; 0.203 sec/batch; 18h:46m:33s remains)
INFO - root - 2017-12-17 07:06:35.010432: step 310, loss = 0.67, batch loss = 0.56 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:45s remains)
INFO - root - 2017-12-17 07:06:37.071374: step 320, loss = 0.58, batch loss = 0.47 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:44s remains)
INFO - root - 2017-12-17 07:06:39.167910: step 330, loss = 0.52, batch loss = 0.42 (38.5 examples/sec; 0.208 sec/batch; 19h:11m:29s remains)
INFO - root - 2017-12-17 07:06:41.226623: step 340, loss = 0.49, batch loss = 0.38 (39.3 examples/sec; 0.204 sec/batch; 18h:48m:15s remains)
INFO - root - 2017-12-17 07:06:43.302566: step 350, loss = 0.48, batch loss = 0.38 (38.8 examples/sec; 0.206 sec/batch; 19h:02m:48s remains)
INFO - root - 2017-12-17 07:06:45.369297: step 360, loss = 0.40, batch loss = 0.29 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:13s remains)
INFO - root - 2017-12-17 07:06:47.449998: step 370, loss = 0.44, batch loss = 0.33 (36.8 examples/sec; 0.217 sec/batch; 20h:03m:55s remains)
INFO - root - 2017-12-17 07:06:49.525461: step 380, loss = 0.52, batch loss = 0.41 (38.9 examples/sec; 0.206 sec/batch; 18h:59m:07s remains)
INFO - root - 2017-12-17 07:06:51.608854: step 390, loss = 0.52, batch loss = 0.42 (39.0 examples/sec; 0.205 sec/batch; 18h:55m:51s remains)
INFO - root - 2017-12-17 07:06:53.688394: step 400, loss = 0.45, batch loss = 0.35 (38.7 examples/sec; 0.207 sec/batch; 19h:05m:31s remains)
INFO - root - 2017-12-17 07:06:55.888321: step 410, loss = 0.46, batch loss = 0.35 (38.9 examples/sec; 0.205 sec/batch; 18h:57m:03s remains)
INFO - root - 2017-12-17 07:06:57.964847: step 420, loss = 0.44, batch loss = 0.33 (38.0 examples/sec; 0.210 sec/batch; 19h:24m:23s remains)
INFO - root - 2017-12-17 07:07:00.061722: step 430, loss = 0.46, batch loss = 0.36 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:56s remains)
INFO - root - 2017-12-17 07:07:02.152305: step 440, loss = 0.44, batch loss = 0.34 (38.8 examples/sec; 0.206 sec/batch; 19h:00m:47s remains)
INFO - root - 2017-12-17 07:07:04.259698: step 450, loss = 0.55, batch loss = 0.44 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:36s remains)
INFO - root - 2017-12-17 07:07:06.352286: step 460, loss = 0.38, batch loss = 0.27 (38.4 examples/sec; 0.208 sec/batch; 19h:13m:41s remains)
INFO - root - 2017-12-17 07:07:08.460535: step 470, loss = 0.51, batch loss = 0.40 (38.9 examples/sec; 0.206 sec/batch; 18h:58m:13s remains)
INFO - root - 2017-12-17 07:07:10.574753: step 480, loss = 0.50, batch loss = 0.39 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:45s remains)
INFO - root - 2017-12-17 07:07:12.655052: step 490, loss = 0.39, batch loss = 0.28 (38.5 examples/sec; 0.208 sec/batch; 19h:08m:54s remains)
INFO - root - 2017-12-17 07:07:14.763554: step 500, loss = 0.49, batch loss = 0.39 (37.8 examples/sec; 0.211 sec/batch; 19h:29m:35s remains)
INFO - root - 2017-12-17 07:07:16.948656: step 510, loss = 0.39, batch loss = 0.28 (38.2 examples/sec; 0.209 sec/batch; 19h:18m:22s remains)
INFO - root - 2017-12-17 07:07:19.031364: step 520, loss = 0.46, batch loss = 0.35 (38.8 examples/sec; 0.206 sec/batch; 19h:02m:08s remains)
INFO - root - 2017-12-17 07:07:21.105028: step 530, loss = 0.38, batch loss = 0.27 (38.0 examples/sec; 0.211 sec/batch; 19h:25m:17s remains)
INFO - root - 2017-12-17 07:07:23.197682: step 540, loss = 0.40, batch loss = 0.29 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:30s remains)
INFO - root - 2017-12-17 07:07:25.270322: step 550, loss = 0.42, batch loss = 0.32 (39.7 examples/sec; 0.202 sec/batch; 18h:35m:23s remains)
INFO - root - 2017-12-17 07:07:27.343549: step 560, loss = 0.41, batch loss = 0.31 (38.4 examples/sec; 0.208 sec/batch; 19h:12m:00s remains)
INFO - root - 2017-12-17 07:07:29.420623: step 570, loss = 0.42, batch loss = 0.32 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:06s remains)
INFO - root - 2017-12-17 07:07:31.523017: step 580, loss = 0.43, batch loss = 0.33 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:26s remains)
INFO - root - 2017-12-17 07:07:33.622495: step 590, loss = 0.41, batch loss = 0.30 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:44s remains)
INFO - root - 2017-12-17 07:07:35.718922: step 600, loss = 0.51, batch loss = 0.41 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:45s remains)
INFO - root - 2017-12-17 07:07:37.948986: step 610, loss = 0.53, batch loss = 0.43 (38.7 examples/sec; 0.207 sec/batch; 19h:04m:41s remains)
INFO - root - 2017-12-17 07:07:40.050608: step 620, loss = 0.42, batch loss = 0.31 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:20s remains)
INFO - root - 2017-12-17 07:07:42.149703: step 630, loss = 0.38, batch loss = 0.27 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:24s remains)
INFO - root - 2017-12-17 07:07:44.239721: step 640, loss = 0.36, batch loss = 0.26 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:35s remains)
INFO - root - 2017-12-17 07:07:46.337215: step 650, loss = 0.38, batch loss = 0.28 (37.8 examples/sec; 0.211 sec/batch; 19h:29m:08s remains)
INFO - root - 2017-12-17 07:07:48.443781: step 660, loss = 0.39, batch loss = 0.29 (38.7 examples/sec; 0.206 sec/batch; 19h:01m:56s remains)
INFO - root - 2017-12-17 07:07:50.552565: step 670, loss = 0.42, batch loss = 0.32 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:10s remains)
INFO - root - 2017-12-17 07:07:52.650404: step 680, loss = 0.35, batch loss = 0.24 (37.3 examples/sec; 0.214 sec/batch; 19h:45m:10s remains)
INFO - root - 2017-12-17 07:07:54.751615: step 690, loss = 0.37, batch loss = 0.27 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:29s remains)
INFO - root - 2017-12-17 07:07:56.835628: step 700, loss = 0.36, batch loss = 0.25 (38.9 examples/sec; 0.206 sec/batch; 18h:57m:54s remains)
INFO - root - 2017-12-17 07:07:59.083781: step 710, loss = 0.49, batch loss = 0.39 (38.0 examples/sec; 0.211 sec/batch; 19h:24m:36s remains)
INFO - root - 2017-12-17 07:08:01.188089: step 720, loss = 0.46, batch loss = 0.35 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:34s remains)
INFO - root - 2017-12-17 07:08:03.277586: step 730, loss = 0.39, batch loss = 0.28 (38.5 examples/sec; 0.208 sec/batch; 19h:07m:36s remains)
INFO - root - 2017-12-17 07:08:05.397509: step 740, loss = 0.37, batch loss = 0.27 (36.1 examples/sec; 0.222 sec/batch; 20h:25m:08s remains)
INFO - root - 2017-12-17 07:08:07.506913: step 750, loss = 0.35, batch loss = 0.24 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:17s remains)
INFO - root - 2017-12-17 07:08:09.624647: step 760, loss = 0.40, batch loss = 0.29 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:20s remains)
INFO - root - 2017-12-17 07:08:11.730473: step 770, loss = 0.37, batch loss = 0.26 (38.3 examples/sec; 0.209 sec/batch; 19h:15m:40s remains)
INFO - root - 2017-12-17 07:08:13.828706: step 780, loss = 0.41, batch loss = 0.30 (38.2 examples/sec; 0.209 sec/batch; 19h:17m:30s remains)
INFO - root - 2017-12-17 07:08:15.965069: step 790, loss = 0.36, batch loss = 0.25 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:59s remains)
INFO - root - 2017-12-17 07:08:18.064886: step 800, loss = 0.36, batch loss = 0.26 (38.4 examples/sec; 0.208 sec/batch; 19h:11m:06s remains)
INFO - root - 2017-12-17 07:08:20.300543: step 810, loss = 0.37, batch loss = 0.27 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:50s remains)
INFO - root - 2017-12-17 07:08:22.407305: step 820, loss = 0.39, batch loss = 0.29 (38.1 examples/sec; 0.210 sec/batch; 19h:21m:05s remains)
INFO - root - 2017-12-17 07:08:24.526707: step 830, loss = 0.44, batch loss = 0.34 (38.6 examples/sec; 0.207 sec/batch; 19h:05m:05s remains)
INFO - root - 2017-12-17 07:08:26.624754: step 840, loss = 0.39, batch loss = 0.29 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:41s remains)
INFO - root - 2017-12-17 07:08:28.735694: step 850, loss = 0.36, batch loss = 0.26 (38.0 examples/sec; 0.211 sec/batch; 19h:24m:58s remains)
INFO - root - 2017-12-17 07:08:30.835781: step 860, loss = 0.40, batch loss = 0.29 (38.7 examples/sec; 0.207 sec/batch; 19h:03m:12s remains)
INFO - root - 2017-12-17 07:08:32.939759: step 870, loss = 0.39, batch loss = 0.29 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:45s remains)
INFO - root - 2017-12-17 07:08:35.041522: step 880, loss = 0.44, batch loss = 0.34 (38.3 examples/sec; 0.209 sec/batch; 19h:15m:36s remains)
INFO - root - 2017-12-17 07:08:37.152900: step 890, loss = 0.35, batch loss = 0.24 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:32s remains)
INFO - root - 2017-12-17 07:08:39.296479: step 900, loss = 0.30, batch loss = 0.20 (38.5 examples/sec; 0.208 sec/batch; 19h:09m:16s remains)
INFO - root - 2017-12-17 07:08:41.524031: step 910, loss = 0.35, batch loss = 0.25 (39.1 examples/sec; 0.204 sec/batch; 18h:50m:05s remains)
INFO - root - 2017-12-17 07:08:43.640024: step 920, loss = 0.39, batch loss = 0.28 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:50s remains)
INFO - root - 2017-12-17 07:08:45.737600: step 930, loss = 0.40, batch loss = 0.30 (37.1 examples/sec; 0.216 sec/batch; 19h:52m:20s remains)
INFO - root - 2017-12-17 07:08:47.837714: step 940, loss = 0.39, batch loss = 0.29 (38.0 examples/sec; 0.211 sec/batch; 19h:24m:53s remains)
INFO - root - 2017-12-17 07:08:49.958484: step 950, loss = 0.33, batch loss = 0.22 (37.1 examples/sec; 0.216 sec/batch; 19h:51m:43s remains)
INFO - root - 2017-12-17 07:08:52.093393: step 960, loss = 0.36, batch loss = 0.26 (38.7 examples/sec; 0.207 sec/batch; 19h:01m:55s remains)
INFO - root - 2017-12-17 07:08:54.208599: step 970, loss = 0.35, batch loss = 0.24 (37.7 examples/sec; 0.212 sec/batch; 19h:31m:23s remains)
INFO - root - 2017-12-17 07:08:56.335950: step 980, loss = 0.33, batch loss = 0.23 (38.5 examples/sec; 0.208 sec/batch; 19h:06m:54s remains)
INFO - root - 2017-12-17 07:08:58.442800: step 990, loss = 0.36, batch loss = 0.25 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:09s remains)
INFO - root - 2017-12-17 07:09:00.566835: step 1000, loss = 0.39, batch loss = 0.28 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:27s remains)
INFO - root - 2017-12-17 07:09:02.828323: step 1010, loss = 0.36, batch loss = 0.26 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:18s remains)
INFO - root - 2017-12-17 07:09:04.949710: step 1020, loss = 0.31, batch loss = 0.21 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:00s remains)
INFO - root - 2017-12-17 07:09:07.073520: step 1030, loss = 0.34, batch loss = 0.24 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:42s remains)
INFO - root - 2017-12-17 07:09:09.240853: step 1040, loss = 0.32, batch loss = 0.22 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:19s remains)
INFO - root - 2017-12-17 07:09:11.335163: step 1050, loss = 0.36, batch loss = 0.26 (38.0 examples/sec; 0.210 sec/batch; 19h:22m:34s remains)
INFO - root - 2017-12-17 07:09:13.444738: step 1060, loss = 0.42, batch loss = 0.32 (38.5 examples/sec; 0.208 sec/batch; 19h:08m:27s remains)
INFO - root - 2017-12-17 07:09:15.548954: step 1070, loss = 0.34, batch loss = 0.24 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:00s remains)
INFO - root - 2017-12-17 07:09:17.654970: step 1080, loss = 0.40, batch loss = 0.30 (38.3 examples/sec; 0.209 sec/batch; 19h:14m:17s remains)
INFO - root - 2017-12-17 07:09:19.758910: step 1090, loss = 0.36, batch loss = 0.26 (38.0 examples/sec; 0.211 sec/batch; 19h:23m:41s remains)
INFO - root - 2017-12-17 07:09:21.871227: step 1100, loss = 0.35, batch loss = 0.25 (38.7 examples/sec; 0.207 sec/batch; 19h:00m:54s remains)
INFO - root - 2017-12-17 07:09:24.118463: step 1110, loss = 0.35, batch loss = 0.25 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:10s remains)
INFO - root - 2017-12-17 07:09:26.236027: step 1120, loss = 0.41, batch loss = 0.31 (37.1 examples/sec; 0.216 sec/batch; 19h:51m:16s remains)
INFO - root - 2017-12-17 07:09:28.349836: step 1130, loss = 0.38, batch loss = 0.28 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:12s remains)
INFO - root - 2017-12-17 07:09:30.475030: step 1140, loss = 0.34, batch loss = 0.24 (37.8 examples/sec; 0.212 sec/batch; 19h:30m:08s remains)
INFO - root - 2017-12-17 07:09:32.575757: step 1150, loss = 0.37, batch loss = 0.27 (38.9 examples/sec; 0.206 sec/batch; 18h:55m:51s remains)
INFO - root - 2017-12-17 07:09:34.696743: step 1160, loss = 0.39, batch loss = 0.29 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:53s remains)
INFO - root - 2017-12-17 07:09:36.823641: step 1170, loss = 0.37, batch loss = 0.26 (38.0 examples/sec; 0.211 sec/batch; 19h:23m:35s remains)
INFO - root - 2017-12-17 07:09:38.923225: step 1180, loss = 0.31, batch loss = 0.21 (38.1 examples/sec; 0.210 sec/batch; 19h:18m:27s remains)
INFO - root - 2017-12-17 07:09:41.027329: step 1190, loss = 0.34, batch loss = 0.24 (38.0 examples/sec; 0.211 sec/batch; 19h:23m:48s remains)
INFO - root - 2017-12-17 07:09:43.144721: step 1200, loss = 0.35, batch loss = 0.25 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:17s remains)
INFO - root - 2017-12-17 07:09:45.364973: step 1210, loss = 0.33, batch loss = 0.23 (38.6 examples/sec; 0.207 sec/batch; 19h:03m:00s remains)
INFO - root - 2017-12-17 07:09:47.467025: step 1220, loss = 0.37, batch loss = 0.27 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:00s remains)
INFO - root - 2017-12-17 07:09:49.590584: step 1230, loss = 0.29, batch loss = 0.20 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:47s remains)
INFO - root - 2017-12-17 07:09:51.687284: step 1240, loss = 0.34, batch loss = 0.24 (37.3 examples/sec; 0.214 sec/batch; 19h:43m:58s remains)
INFO - root - 2017-12-17 07:09:53.811735: step 1250, loss = 0.32, batch loss = 0.22 (35.2 examples/sec; 0.227 sec/batch; 20h:54m:18s remains)
INFO - root - 2017-12-17 07:09:55.927428: step 1260, loss = 0.29, batch loss = 0.19 (38.4 examples/sec; 0.208 sec/batch; 19h:09m:07s remains)
INFO - root - 2017-12-17 07:09:58.047372: step 1270, loss = 0.36, batch loss = 0.26 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:47s remains)
INFO - root - 2017-12-17 07:10:00.192623: step 1280, loss = 0.31, batch loss = 0.21 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:04s remains)
INFO - root - 2017-12-17 07:10:02.304830: step 1290, loss = 0.32, batch loss = 0.22 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:49s remains)
INFO - root - 2017-12-17 07:10:04.419295: step 1300, loss = 0.26, batch loss = 0.16 (38.2 examples/sec; 0.209 sec/batch; 19h:14m:33s remains)
INFO - root - 2017-12-17 07:10:06.655354: step 1310, loss = 0.32, batch loss = 0.22 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:45s remains)
INFO - root - 2017-12-17 07:10:08.766648: step 1320, loss = 0.28, batch loss = 0.19 (38.2 examples/sec; 0.210 sec/batch; 19h:17m:20s remains)
INFO - root - 2017-12-17 07:10:10.885315: step 1330, loss = 0.33, batch loss = 0.23 (38.6 examples/sec; 0.207 sec/batch; 19h:04m:08s remains)
INFO - root - 2017-12-17 07:10:12.991131: step 1340, loss = 0.35, batch loss = 0.25 (38.2 examples/sec; 0.210 sec/batch; 19h:16m:23s remains)
INFO - root - 2017-12-17 07:10:15.122456: step 1350, loss = 0.31, batch loss = 0.21 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:46s remains)
INFO - root - 2017-12-17 07:10:17.254681: step 1360, loss = 0.37, batch loss = 0.27 (38.3 examples/sec; 0.209 sec/batch; 19h:14m:11s remains)
INFO - root - 2017-12-17 07:10:19.378741: step 1370, loss = 0.34, batch loss = 0.25 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:03s remains)
INFO - root - 2017-12-17 07:10:21.478732: step 1380, loss = 0.38, batch loss = 0.28 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:12s remains)
INFO - root - 2017-12-17 07:10:23.592980: step 1390, loss = 0.36, batch loss = 0.26 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:30s remains)
INFO - root - 2017-12-17 07:10:25.725937: step 1400, loss = 0.32, batch loss = 0.22 (37.7 examples/sec; 0.212 sec/batch; 19h:31m:56s remains)
INFO - root - 2017-12-17 07:10:27.966318: step 1410, loss = 0.32, batch loss = 0.23 (38.5 examples/sec; 0.208 sec/batch; 19h:07m:57s remains)
INFO - root - 2017-12-17 07:10:30.114715: step 1420, loss = 0.34, batch loss = 0.24 (36.6 examples/sec; 0.218 sec/batch; 20h:04m:56s remains)
INFO - root - 2017-12-17 07:10:32.233978: step 1430, loss = 0.33, batch loss = 0.23 (37.4 examples/sec; 0.214 sec/batch; 19h:39m:53s remains)
INFO - root - 2017-12-17 07:10:34.379886: step 1440, loss = 0.33, batch loss = 0.23 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:38s remains)
INFO - root - 2017-12-17 07:10:36.522041: step 1450, loss = 0.36, batch loss = 0.26 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:42s remains)
INFO - root - 2017-12-17 07:10:38.688725: step 1460, loss = 0.37, batch loss = 0.27 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:53s remains)
INFO - root - 2017-12-17 07:10:40.790648: step 1470, loss = 0.29, batch loss = 0.19 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:14s remains)
INFO - root - 2017-12-17 07:10:42.906304: step 1480, loss = 0.34, batch loss = 0.24 (37.8 examples/sec; 0.211 sec/batch; 19h:26m:13s remains)
INFO - root - 2017-12-17 07:10:45.041063: step 1490, loss = 0.36, batch loss = 0.26 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:19s remains)
INFO - root - 2017-12-17 07:10:47.174159: step 1500, loss = 0.37, batch loss = 0.27 (38.8 examples/sec; 0.206 sec/batch; 18h:58m:24s remains)
INFO - root - 2017-12-17 07:10:49.399344: step 1510, loss = 0.28, batch loss = 0.18 (38.5 examples/sec; 0.208 sec/batch; 19h:04m:54s remains)
INFO - root - 2017-12-17 07:10:51.523500: step 1520, loss = 0.27, batch loss = 0.17 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:17s remains)
INFO - root - 2017-12-17 07:10:53.656412: step 1530, loss = 0.27, batch loss = 0.17 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:11s remains)
INFO - root - 2017-12-17 07:10:55.771428: step 1540, loss = 0.40, batch loss = 0.30 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:50s remains)
INFO - root - 2017-12-17 07:10:57.880684: step 1550, loss = 0.27, batch loss = 0.17 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:57s remains)
INFO - root - 2017-12-17 07:11:00.027091: step 1560, loss = 0.26, batch loss = 0.17 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:39s remains)
INFO - root - 2017-12-17 07:11:02.138838: step 1570, loss = 0.34, batch loss = 0.25 (37.8 examples/sec; 0.212 sec/batch; 19h:27m:05s remains)
INFO - root - 2017-12-17 07:11:04.274045: step 1580, loss = 0.30, batch loss = 0.21 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:30s remains)
INFO - root - 2017-12-17 07:11:06.399549: step 1590, loss = 0.39, batch loss = 0.29 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:42s remains)
INFO - root - 2017-12-17 07:11:08.566410: step 1600, loss = 0.27, batch loss = 0.17 (38.5 examples/sec; 0.208 sec/batch; 19h:05m:34s remains)
INFO - root - 2017-12-17 07:11:10.808425: step 1610, loss = 0.28, batch loss = 0.18 (38.6 examples/sec; 0.207 sec/batch; 19h:01m:49s remains)
INFO - root - 2017-12-17 07:11:12.938527: step 1620, loss = 0.32, batch loss = 0.22 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:07s remains)
INFO - root - 2017-12-17 07:11:15.071816: step 1630, loss = 0.31, batch loss = 0.21 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:40s remains)
INFO - root - 2017-12-17 07:11:17.197725: step 1640, loss = 0.29, batch loss = 0.19 (37.1 examples/sec; 0.216 sec/batch; 19h:48m:55s remains)
INFO - root - 2017-12-17 07:11:19.342407: step 1650, loss = 0.34, batch loss = 0.24 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:52s remains)
INFO - root - 2017-12-17 07:11:21.466761: step 1660, loss = 0.27, batch loss = 0.18 (38.0 examples/sec; 0.210 sec/batch; 19h:19m:56s remains)
INFO - root - 2017-12-17 07:11:23.615001: step 1670, loss = 0.33, batch loss = 0.23 (38.3 examples/sec; 0.209 sec/batch; 19h:12m:46s remains)
INFO - root - 2017-12-17 07:11:25.749927: step 1680, loss = 0.36, batch loss = 0.26 (37.5 examples/sec; 0.214 sec/batch; 19h:37m:25s remains)
INFO - root - 2017-12-17 07:11:27.881312: step 1690, loss = 0.31, batch loss = 0.22 (37.9 examples/sec; 0.211 sec/batch; 19h:24m:01s remains)
INFO - root - 2017-12-17 07:11:29.998940: step 1700, loss = 0.31, batch loss = 0.22 (37.4 examples/sec; 0.214 sec/batch; 19h:39m:19s remains)
INFO - root - 2017-12-17 07:11:32.250853: step 1710, loss = 0.29, batch loss = 0.19 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:53s remains)
INFO - root - 2017-12-17 07:11:34.390893: step 1720, loss = 0.26, batch loss = 0.17 (36.8 examples/sec; 0.217 sec/batch; 19h:57m:41s remains)
INFO - root - 2017-12-17 07:11:36.513643: step 1730, loss = 0.31, batch loss = 0.21 (38.2 examples/sec; 0.210 sec/batch; 19h:15m:57s remains)
INFO - root - 2017-12-17 07:11:38.632611: step 1740, loss = 0.33, batch loss = 0.23 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:58s remains)
INFO - root - 2017-12-17 07:11:40.786252: step 1750, loss = 0.30, batch loss = 0.21 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:46s remains)
INFO - root - 2017-12-17 07:11:42.900868: step 1760, loss = 0.31, batch loss = 0.21 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:02s remains)
INFO - root - 2017-12-17 07:11:45.025599: step 1770, loss = 0.37, batch loss = 0.28 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:10s remains)
INFO - root - 2017-12-17 07:11:47.146203: step 1780, loss = 0.28, batch loss = 0.18 (37.1 examples/sec; 0.215 sec/batch; 19h:47m:44s remains)
INFO - root - 2017-12-17 07:11:49.281110: step 1790, loss = 0.32, batch loss = 0.22 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:32s remains)
INFO - root - 2017-12-17 07:11:51.403550: step 1800, loss = 0.31, batch loss = 0.22 (36.6 examples/sec; 0.218 sec/batch; 20h:03m:28s remains)
INFO - root - 2017-12-17 07:11:53.660427: step 1810, loss = 0.28, batch loss = 0.19 (36.8 examples/sec; 0.218 sec/batch; 19h:58m:58s remains)
INFO - root - 2017-12-17 07:11:55.798568: step 1820, loss = 0.33, batch loss = 0.24 (38.4 examples/sec; 0.209 sec/batch; 19h:09m:07s remains)
INFO - root - 2017-12-17 07:11:57.925108: step 1830, loss = 0.40, batch loss = 0.30 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:52s remains)
INFO - root - 2017-12-17 07:12:00.036847: step 1840, loss = 0.29, batch loss = 0.19 (37.8 examples/sec; 0.211 sec/batch; 19h:25m:14s remains)
INFO - root - 2017-12-17 07:12:02.166208: step 1850, loss = 0.32, batch loss = 0.22 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:27s remains)
INFO - root - 2017-12-17 07:12:04.307946: step 1860, loss = 0.30, batch loss = 0.21 (38.0 examples/sec; 0.211 sec/batch; 19h:20m:10s remains)
INFO - root - 2017-12-17 07:12:06.423480: step 1870, loss = 0.27, batch loss = 0.17 (38.2 examples/sec; 0.210 sec/batch; 19h:15m:30s remains)
INFO - root - 2017-12-17 07:12:08.561645: step 1880, loss = 0.31, batch loss = 0.22 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:11s remains)
INFO - root - 2017-12-17 07:12:10.689310: step 1890, loss = 0.38, batch loss = 0.28 (38.2 examples/sec; 0.210 sec/batch; 19h:14m:57s remains)
INFO - root - 2017-12-17 07:12:12.815978: step 1900, loss = 0.29, batch loss = 0.19 (37.1 examples/sec; 0.216 sec/batch; 19h:48m:14s remains)
INFO - root - 2017-12-17 07:12:15.072244: step 1910, loss = 0.34, batch loss = 0.24 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:06s remains)
INFO - root - 2017-12-17 07:12:17.195526: step 1920, loss = 0.27, batch loss = 0.17 (37.8 examples/sec; 0.212 sec/batch; 19h:26m:38s remains)
INFO - root - 2017-12-17 07:12:19.330702: step 1930, loss = 0.30, batch loss = 0.21 (37.0 examples/sec; 0.216 sec/batch; 19h:50m:19s remains)
INFO - root - 2017-12-17 07:12:21.457333: step 1940, loss = 0.34, batch loss = 0.24 (37.3 examples/sec; 0.214 sec/batch; 19h:40m:12s remains)
INFO - root - 2017-12-17 07:12:23.569643: step 1950, loss = 0.29, batch loss = 0.20 (38.5 examples/sec; 0.208 sec/batch; 19h:04m:56s remains)
INFO - root - 2017-12-17 07:12:25.707647: step 1960, loss = 0.39, batch loss = 0.30 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:48s remains)
INFO - root - 2017-12-17 07:12:27.847192: step 1970, loss = 0.49, batch loss = 0.39 (37.3 examples/sec; 0.215 sec/batch; 19h:42m:33s remains)
INFO - root - 2017-12-17 07:12:29.956752: step 1980, loss = 0.37, batch loss = 0.28 (37.8 examples/sec; 0.212 sec/batch; 19h:27m:17s remains)
INFO - root - 2017-12-17 07:12:32.086616: step 1990, loss = 0.36, batch loss = 0.27 (38.1 examples/sec; 0.210 sec/batch; 19h:17m:21s remains)
INFO - root - 2017-12-17 07:12:34.207928: step 2000, loss = 0.34, batch loss = 0.25 (37.8 examples/sec; 0.212 sec/batch; 19h:26m:01s remains)
INFO - root - 2017-12-17 07:12:36.454401: step 2010, loss = 0.39, batch loss = 0.29 (38.1 examples/sec; 0.210 sec/batch; 19h:16m:23s remains)
INFO - root - 2017-12-17 07:12:38.578394: step 2020, loss = 0.28, batch loss = 0.19 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:31s remains)
INFO - root - 2017-12-17 07:12:40.724491: step 2030, loss = 0.30, batch loss = 0.20 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:22s remains)
INFO - root - 2017-12-17 07:12:42.861916: step 2040, loss = 0.26, batch loss = 0.17 (38.0 examples/sec; 0.210 sec/batch; 19h:19m:10s remains)
INFO - root - 2017-12-17 07:12:44.976010: step 2050, loss = 0.29, batch loss = 0.20 (38.4 examples/sec; 0.208 sec/batch; 19h:06m:46s remains)
INFO - root - 2017-12-17 07:12:47.113148: step 2060, loss = 0.32, batch loss = 0.23 (37.8 examples/sec; 0.212 sec/batch; 19h:26m:11s remains)
INFO - root - 2017-12-17 07:12:49.261063: step 2070, loss = 0.31, batch loss = 0.22 (37.5 examples/sec; 0.214 sec/batch; 19h:35m:57s remains)
INFO - root - 2017-12-17 07:12:51.413578: step 2080, loss = 0.30, batch loss = 0.21 (37.8 examples/sec; 0.211 sec/batch; 19h:24m:03s remains)
INFO - root - 2017-12-17 07:12:53.573059: step 2090, loss = 0.34, batch loss = 0.25 (35.9 examples/sec; 0.223 sec/batch; 20h:25m:43s remains)
INFO - root - 2017-12-17 07:12:55.702430: step 2100, loss = 0.33, batch loss = 0.24 (38.5 examples/sec; 0.208 sec/batch; 19h:05m:28s remains)
INFO - root - 2017-12-17 07:12:57.950824: step 2110, loss = 0.27, batch loss = 0.18 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:32s remains)
INFO - root - 2017-12-17 07:13:00.076059: step 2120, loss = 0.27, batch loss = 0.18 (38.0 examples/sec; 0.210 sec/batch; 19h:18m:42s remains)
INFO - root - 2017-12-17 07:13:02.193051: step 2130, loss = 0.33, batch loss = 0.24 (38.0 examples/sec; 0.211 sec/batch; 19h:20m:11s remains)
INFO - root - 2017-12-17 07:13:04.312399: step 2140, loss = 0.33, batch loss = 0.24 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:40s remains)
INFO - root - 2017-12-17 07:13:06.455604: step 2150, loss = 0.38, batch loss = 0.29 (36.9 examples/sec; 0.217 sec/batch; 19h:53m:01s remains)
INFO - root - 2017-12-17 07:13:08.607076: step 2160, loss = 0.24, batch loss = 0.15 (38.0 examples/sec; 0.210 sec/batch; 19h:17m:50s remains)
INFO - root - 2017-12-17 07:13:10.729896: step 2170, loss = 0.35, batch loss = 0.25 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:08s remains)
INFO - root - 2017-12-17 07:13:12.869828: step 2180, loss = 0.27, batch loss = 0.18 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:36s remains)
INFO - root - 2017-12-17 07:13:14.987677: step 2190, loss = 0.27, batch loss = 0.18 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:00s remains)
INFO - root - 2017-12-17 07:13:17.127327: step 2200, loss = 0.30, batch loss = 0.21 (36.5 examples/sec; 0.219 sec/batch; 20h:06m:00s remains)
INFO - root - 2017-12-17 07:13:19.356982: step 2210, loss = 0.31, batch loss = 0.22 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:14s remains)
INFO - root - 2017-12-17 07:13:21.503136: step 2220, loss = 0.24, batch loss = 0.15 (35.3 examples/sec; 0.227 sec/batch; 20h:48m:21s remains)
INFO - root - 2017-12-17 07:13:23.635207: step 2230, loss = 0.37, batch loss = 0.28 (38.0 examples/sec; 0.211 sec/batch; 19h:18m:54s remains)
INFO - root - 2017-12-17 07:13:25.754610: step 2240, loss = 0.29, batch loss = 0.20 (37.9 examples/sec; 0.211 sec/batch; 19h:20m:26s remains)
INFO - root - 2017-12-17 07:13:27.873811: step 2250, loss = 0.26, batch loss = 0.17 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:53s remains)
INFO - root - 2017-12-17 07:13:30.007486: step 2260, loss = 0.28, batch loss = 0.18 (35.9 examples/sec; 0.223 sec/batch; 20h:25m:53s remains)
INFO - root - 2017-12-17 07:13:32.121685: step 2270, loss = 0.31, batch loss = 0.22 (38.2 examples/sec; 0.210 sec/batch; 19h:13m:39s remains)
INFO - root - 2017-12-17 07:13:34.264484: step 2280, loss = 0.25, batch loss = 0.16 (38.3 examples/sec; 0.209 sec/batch; 19h:09m:09s remains)
INFO - root - 2017-12-17 07:13:36.382931: step 2290, loss = 0.33, batch loss = 0.23 (37.8 examples/sec; 0.212 sec/batch; 19h:25m:55s remains)
INFO - root - 2017-12-17 07:13:38.556676: step 2300, loss = 0.30, batch loss = 0.20 (37.3 examples/sec; 0.215 sec/batch; 19h:40m:34s remains)
INFO - root - 2017-12-17 07:13:40.815017: step 2310, loss = 0.33, batch loss = 0.24 (38.2 examples/sec; 0.210 sec/batch; 19h:13m:00s remains)
INFO - root - 2017-12-17 07:13:42.961866: step 2320, loss = 0.30, batch loss = 0.21 (36.5 examples/sec; 0.219 sec/batch; 20h:06m:08s remains)
INFO - root - 2017-12-17 07:13:45.080670: step 2330, loss = 0.28, batch loss = 0.19 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:52s remains)
INFO - root - 2017-12-17 07:13:47.207562: step 2340, loss = 0.33, batch loss = 0.23 (38.0 examples/sec; 0.210 sec/batch; 19h:17m:42s remains)
INFO - root - 2017-12-17 07:13:49.345399: step 2350, loss = 0.35, batch loss = 0.26 (36.8 examples/sec; 0.217 sec/batch; 19h:54m:37s remains)
INFO - root - 2017-12-17 07:13:51.452946: step 2360, loss = 0.30, batch loss = 0.21 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:06s remains)
INFO - root - 2017-12-17 07:13:53.587126: step 2370, loss = 0.29, batch loss = 0.19 (37.1 examples/sec; 0.216 sec/batch; 19h:47m:13s remains)
INFO - root - 2017-12-17 07:13:55.710583: step 2380, loss = 0.31, batch loss = 0.22 (38.0 examples/sec; 0.210 sec/batch; 19h:18m:02s remains)
INFO - root - 2017-12-17 07:13:57.851192: step 2390, loss = 0.32, batch loss = 0.23 (37.9 examples/sec; 0.211 sec/batch; 19h:20m:16s remains)
INFO - root - 2017-12-17 07:13:59.970407: step 2400, loss = 0.32, batch loss = 0.23 (38.7 examples/sec; 0.207 sec/batch; 18h:56m:59s remains)
INFO - root - 2017-12-17 07:14:02.188619: step 2410, loss = 0.28, batch loss = 0.19 (38.5 examples/sec; 0.208 sec/batch; 19h:04m:09s remains)
INFO - root - 2017-12-17 07:14:04.347644: step 2420, loss = 0.35, batch loss = 0.26 (35.7 examples/sec; 0.224 sec/batch; 20h:34m:09s remains)
INFO - root - 2017-12-17 07:14:06.496491: step 2430, loss = 0.27, batch loss = 0.18 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:13s remains)
INFO - root - 2017-12-17 07:14:08.632009: step 2440, loss = 0.26, batch loss = 0.17 (36.7 examples/sec; 0.218 sec/batch; 19h:58m:56s remains)
INFO - root - 2017-12-17 07:14:10.766828: step 2450, loss = 0.32, batch loss = 0.23 (38.3 examples/sec; 0.209 sec/batch; 19h:09m:37s remains)
INFO - root - 2017-12-17 07:14:12.906669: step 2460, loss = 0.30, batch loss = 0.21 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:15s remains)
INFO - root - 2017-12-17 07:14:15.023159: step 2470, loss = 0.33, batch loss = 0.24 (37.2 examples/sec; 0.215 sec/batch; 19h:41m:19s remains)
INFO - root - 2017-12-17 07:14:17.157792: step 2480, loss = 0.29, batch loss = 0.20 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:29s remains)
INFO - root - 2017-12-17 07:14:19.261657: step 2490, loss = 0.26, batch loss = 0.17 (37.9 examples/sec; 0.211 sec/batch; 19h:21m:37s remains)
INFO - root - 2017-12-17 07:14:21.393627: step 2500, loss = 0.29, batch loss = 0.20 (36.8 examples/sec; 0.217 sec/batch; 19h:54m:27s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-clip-10-10-test1111/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-clip-10-10-test1111/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-17 07:14:23.979665: step 2510, loss = 0.32, batch loss = 0.23 (37.5 examples/sec; 0.213 sec/batch; 19h:33m:09s remains)
INFO - root - 2017-12-17 07:14:26.100217: step 2520, loss = 0.26, batch loss = 0.17 (38.1 examples/sec; 0.210 sec/batch; 19h:14m:08s remains)
INFO - root - 2017-12-17 07:14:28.256357: step 2530, loss = 0.29, batch loss = 0.20 (38.8 examples/sec; 0.206 sec/batch; 18h:54m:04s remains)
INFO - root - 2017-12-17 07:14:30.380715: step 2540, loss = 0.29, batch loss = 0.20 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:56s remains)
INFO - root - 2017-12-17 07:14:32.502965: step 2550, loss = 0.31, batch loss = 0.22 (38.4 examples/sec; 0.208 sec/batch; 19h:05m:55s remains)
INFO - root - 2017-12-17 07:14:34.630085: step 2560, loss = 0.30, batch loss = 0.21 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:08s remains)
INFO - root - 2017-12-17 07:14:36.746160: step 2570, loss = 0.30, batch loss = 0.21 (38.0 examples/sec; 0.210 sec/batch; 19h:16m:30s remains)
INFO - root - 2017-12-17 07:14:38.889946: step 2580, loss = 0.32, batch loss = 0.24 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:06s remains)
INFO - root - 2017-12-17 07:14:41.006262: step 2590, loss = 0.31, batch loss = 0.22 (38.2 examples/sec; 0.210 sec/batch; 19h:12m:42s remains)
INFO - root - 2017-12-17 07:14:43.137698: step 2600, loss = 0.32, batch loss = 0.23 (37.2 examples/sec; 0.215 sec/batch; 19h:41m:44s remains)
INFO - root - 2017-12-17 07:14:45.389837: step 2610, loss = 0.36, batch loss = 0.27 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:14s remains)
INFO - root - 2017-12-17 07:14:47.518639: step 2620, loss = 0.24, batch loss = 0.15 (36.6 examples/sec; 0.218 sec/batch; 20h:00m:37s remains)
INFO - root - 2017-12-17 07:14:49.628736: step 2630, loss = 0.36, batch loss = 0.27 (37.9 examples/sec; 0.211 sec/batch; 19h:21m:05s remains)
INFO - root - 2017-12-17 07:14:51.785171: step 2640, loss = 0.32, batch loss = 0.23 (36.8 examples/sec; 0.217 sec/batch; 19h:53m:39s remains)
INFO - root - 2017-12-17 07:14:53.930815: step 2650, loss = 0.30, batch loss = 0.21 (37.8 examples/sec; 0.212 sec/batch; 19h:24m:56s remains)
INFO - root - 2017-12-17 07:14:56.026415: step 2660, loss = 0.31, batch loss = 0.23 (38.4 examples/sec; 0.208 sec/batch; 19h:04m:35s remains)
INFO - root - 2017-12-17 07:14:58.154570: step 2670, loss = 0.31, batch loss = 0.22 (36.2 examples/sec; 0.221 sec/batch; 20h:13m:46s remains)
INFO - root - 2017-12-17 07:15:00.298953: step 2680, loss = 0.32, batch loss = 0.23 (36.6 examples/sec; 0.219 sec/batch; 20h:01m:58s remains)
INFO - root - 2017-12-17 07:15:02.416606: step 2690, loss = 0.32, batch loss = 0.23 (38.3 examples/sec; 0.209 sec/batch; 19h:08m:05s remains)
INFO - root - 2017-12-17 07:15:04.563807: step 2700, loss = 0.25, batch loss = 0.17 (37.7 examples/sec; 0.212 sec/batch; 19h:27m:10s remains)
INFO - root - 2017-12-17 07:15:06.808552: step 2710, loss = 0.32, batch loss = 0.24 (38.0 examples/sec; 0.210 sec/batch; 19h:16m:23s remains)
INFO - root - 2017-12-17 07:15:08.968798: step 2720, loss = 0.24, batch loss = 0.15 (37.7 examples/sec; 0.212 sec/batch; 19h:25m:03s remains)
INFO - root - 2017-12-17 07:15:11.094043: step 2730, loss = 0.26, batch loss = 0.18 (36.8 examples/sec; 0.217 sec/batch; 19h:53m:24s remains)
INFO - root - 2017-12-17 07:15:13.231787: step 2740, loss = 0.43, batch loss = 0.34 (37.8 examples/sec; 0.212 sec/batch; 19h:23m:02s remains)
INFO - root - 2017-12-17 07:15:15.391731: step 2750, loss = 0.42, batch loss = 0.33 (38.5 examples/sec; 0.208 sec/batch; 19h:02m:35s remains)
INFO - root - 2017-12-17 07:15:17.510318: step 2760, loss = 0.32, batch loss = 0.23 (38.1 examples/sec; 0.210 sec/batch; 19h:14m:35s remains)
INFO - root - 2017-12-17 07:15:19.651274: step 2770, loss = 0.29, batch loss = 0.20 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:02s remains)
INFO - root - 2017-12-17 07:15:21.801452: step 2780, loss = 0.39, batch loss = 0.30 (36.9 examples/sec; 0.217 sec/batch; 19h:52m:22s remains)
INFO - root - 2017-12-17 07:15:23.933028: step 2790, loss = 0.37, batch loss = 0.29 (38.0 examples/sec; 0.211 sec/batch; 19h:16m:46s remains)
INFO - root - 2017-12-17 07:15:26.075467: step 2800, loss = 0.25, batch loss = 0.17 (37.5 examples/sec; 0.214 sec/batch; 19h:33m:17s remains)
INFO - root - 2017-12-17 07:15:28.314734: step 2810, loss = 0.31, batch loss = 0.22 (38.9 examples/sec; 0.206 sec/batch; 18h:51m:10s remains)
INFO - root - 2017-12-17 07:15:30.451769: step 2820, loss = 0.29, batch loss = 0.20 (38.2 examples/sec; 0.209 sec/batch; 19h:09m:39s remains)
INFO - root - 2017-12-17 07:15:32.576838: step 2830, loss = 0.27, batch loss = 0.18 (38.7 examples/sec; 0.207 sec/batch; 18h:56m:25s remains)
INFO - root - 2017-12-17 07:15:34.695779: step 2840, loss = 0.28, batch loss = 0.20 (38.8 examples/sec; 0.206 sec/batch; 18h:53m:42s remains)
INFO - root - 2017-12-17 07:15:36.843077: step 2850, loss = 0.34, batch loss = 0.25 (36.9 examples/sec; 0.217 sec/batch; 19h:50m:18s remains)
INFO - root - 2017-12-17 07:15:38.994899: step 2860, loss = 0.28, batch loss = 0.20 (37.0 examples/sec; 0.216 sec/batch; 19h:47m:58s remains)
INFO - root - 2017-12-17 07:15:41.127237: step 2870, loss = 0.31, batch loss = 0.22 (38.7 examples/sec; 0.207 sec/batch; 18h:56m:25s remains)
INFO - root - 2017-12-17 07:15:43.254639: step 2880, loss = 0.32, batch loss = 0.23 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:16s remains)
INFO - root - 2017-12-17 07:15:45.380792: step 2890, loss = 0.28, batch loss = 0.19 (37.5 examples/sec; 0.213 sec/batch; 19h:31m:27s remains)
INFO - root - 2017-12-17 07:15:47.506857: step 2900, loss = 0.26, batch loss = 0.17 (37.0 examples/sec; 0.216 sec/batch; 19h:46m:49s remains)
INFO - root - 2017-12-17 07:15:49.806977: step 2910, loss = 0.23, batch loss = 0.14 (37.7 examples/sec; 0.212 sec/batch; 19h:25m:54s remains)
INFO - root - 2017-12-17 07:15:51.949496: step 2920, loss = 0.35, batch loss = 0.26 (37.5 examples/sec; 0.213 sec/batch; 19h:31m:50s remains)
INFO - root - 2017-12-17 07:15:54.062907: step 2930, loss = 0.35, batch loss = 0.26 (38.0 examples/sec; 0.210 sec/batch; 19h:15m:52s remains)
INFO - root - 2017-12-17 07:15:56.203589: step 2940, loss = 0.24, batch loss = 0.16 (37.3 examples/sec; 0.214 sec/batch; 19h:36m:36s remains)
INFO - root - 2017-12-17 07:15:58.320718: step 2950, loss = 0.22, batch loss = 0.14 (38.0 examples/sec; 0.211 sec/batch; 19h:17m:48s remains)
INFO - root - 2017-12-17 07:16:00.470692: step 2960, loss = 0.30, batch loss = 0.21 (36.9 examples/sec; 0.217 sec/batch; 19h:50m:46s remains)
INFO - root - 2017-12-17 07:16:02.614011: step 2970, loss = 0.30, batch loss = 0.22 (37.4 examples/sec; 0.214 sec/batch; 19h:34m:41s remains)
INFO - root - 2017-12-17 07:16:04.757058: step 2980, loss = 0.31, batch loss = 0.23 (37.7 examples/sec; 0.212 sec/batch; 19h:24m:50s remains)
INFO - root - 2017-12-17 07:16:06.896245: step 2990, loss = 0.29, batch loss = 0.20 (37.1 examples/sec; 0.216 sec/batch; 19h:44m:25s remains)
INFO - root - 2017-12-17 07:16:09.036034: step 3000, loss = 0.26, batch loss = 0.18 (38.1 examples/sec; 0.210 sec/batch; 19h:14m:36s remains)
INFO - root - 2017-12-17 07:16:11.261018: step 3010, loss = 0.27, batch loss = 0.18 (37.9 examples/sec; 0.211 sec/batch; 19h:18m:45s remains)
INFO - root - 2017-12-17 07:16:13.375852: step 3020, loss = 0.30, batch loss = 0.21 (37.3 examples/sec; 0.215 sec/batch; 19h:38m:52s remains)
INFO - root - 2017-12-17 07:16:15.522056: step 3030, loss = 0.25, batch loss = 0.16 (36.8 examples/sec; 0.218 sec/batch; 19h:54m:27s remains)
INFO - root - 2017-12-17 07:16:17.654925: step 3040, loss = 0.28, batch loss = 0.19 (37.4 examples/sec; 0.214 sec/batch; 19h:35m:01s remains)
INFO - root - 2017-12-17 07:16:19.766960: step 3050, loss = 0.31, batch loss = 0.22 (38.5 examples/sec; 0.208 sec/batch; 19h:00m:16s remains)
INFO - root - 2017-12-17 07:16:21.907880: step 3060, loss = 0.22, batch loss = 0.14 (36.9 examples/sec; 0.217 sec/batch; 19h:51m:56s remains)
INFO - root - 2017-12-17 07:16:24.039859: step 3070, loss = 0.26, batch loss = 0.18 (38.1 examples/sec; 0.210 sec/batch; 19h:13m:17s remains)
INFO - root - 2017-12-17 07:16:26.163646: step 3080, loss = 0.30, batch loss = 0.21 (36.8 examples/sec; 0.217 sec/batch; 19h:52m:07s remains)
INFO - root - 2017-12-17 07:16:28.280527: step 3090, loss = 0.27, batch loss = 0.18 (37.4 examples/sec; 0.214 sec/batch; 19h:33m:24s remains)
INFO - root - 2017-12-17 07:16:30.407152: step 3100, loss = 0.37, batch loss = 0.28 (36.7 examples/sec; 0.218 sec/batch; 19h:57m:14s remains)
INFO - root - 2017-12-17 07:16:32.663130: step 3110, loss = 0.30, batch loss = 0.21 (38.5 examples/sec; 0.208 sec/batch; 19h:00m:12s remains)
INFO - root - 2017-12-17 07:16:34.795088: step 3120, loss = 0.32, batch loss = 0.23 (37.1 examples/sec; 0.215 sec/batch; 19h:42m:26s remains)
INFO - root - 2017-12-17 07:16:36.907508: step 3130, loss = 0.33, batch loss = 0.24 (37.9 examples/sec; 0.211 sec/batch; 19h:19m:31s remains)
INFO - root - 2017-12-17 07:16:39.031634: step 3140, loss = 0.23, batch loss = 0.15 (38.6 examples/sec; 0.207 sec/batch; 18h:58m:41s remains)
INFO - root - 2017-12-17 07:16:41.160729: step 3150, loss = 0.35, batch loss = 0.27 (37.1 examples/sec; 0.215 sec/batch; 19h:42m:45s remains)
INFO - root - 2017-12-17 07:16:43.286231: step 3160, loss = 0.29, batch loss = 0.21 (37.5 examples/sec; 0.213 sec/batch; 19h:30m:21s remains)
INFO - root - 2017-12-17 07:16:45.424032: step 3170, loss = 0.24, batch loss = 0.16 (38.4 examples/sec; 0.209 sec/batch; 19h:04m:54s remains)
INFO - root - 2017-12-17 07:16:47.549685: step 3180, loss = 0.22, batch loss = 0.13 (38.2 examples/sec; 0.210 sec/batch; 19h:10m:27s remains)
INFO - root - 2017-12-17 07:16:49.696627: step 3190, loss = 0.33, batch loss = 0.25 (37.5 examples/sec; 0.213 sec/batch; 19h:30m:05s remains)
INFO - root - 2017-12-17 07:16:51.815851: step 3200, loss = 0.35, batch loss = 0.26 (37.1 examples/sec; 0.215 sec/batch; 19h:42m:20s remains)
INFO - root - 2017-12-17 07:16:54.114508: step 3210, loss = 0.38, batch loss = 0.29 (38.9 examples/sec; 0.206 sec/batch; 18h:49m:55s remains)
INFO - root - 2017-12-17 07:16:56.239543: step 3220, loss = 0.24, batch loss = 0.15 (37.3 examples/sec; 0.215 sec/batch; 19h:38m:17s remains)
INFO - root - 2017-12-17 07:16:58.391079: step 3230, loss = 0.21, batch loss = 0.13 (37.4 examples/sec; 0.214 sec/batch; 19h:32m:23s remains)
INFO - root - 2017-12-17 07:17:00.528911: step 3240, loss = 0.24, batch loss = 0.16 (37.3 examples/sec; 0.215 sec/batch; 19h:37m:55s remains)
INFO - root - 2017-12-17 07:17:02.660162: step 3250, loss = 0.23, batch loss = 0.14 (37.7 examples/sec; 0.212 sec/batch; 19h:25m:58s remains)
INFO - root - 2017-12-17 07:17:04.809204: step 3260, loss = 0.25, batch loss = 0.16 (36.8 examples/sec; 0.218 sec/batch; 19h:54m:10s remains)
INFO - root - 2017-12-17 07:17:06.914182: step 3270, loss = 0.29, batch loss = 0.21 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:25s remains)
INFO - root - 2017-12-17 07:17:09.052677: step 3280, loss = 0.28, batch loss = 0.20 (38.0 examples/sec; 0.210 sec/batch; 19h:14m:35s remains)
INFO - root - 2017-12-17 07:17:11.193884: step 3290, loss = 0.28, batch loss = 0.19 (36.9 examples/sec; 0.217 sec/batch; 19h:48m:31s remains)
INFO - root - 2017-12-17 07:17:13.342775: step 3300, loss = 0.27, batch loss = 0.19 (38.0 examples/sec; 0.210 sec/batch; 19h:14m:50s remains)
INFO - root - 2017-12-17 07:17:15.622558: step 3310, loss = 0.28, batch loss = 0.19 (37.2 examples/sec; 0.215 sec/batch; 19h:40m:07s remains)
INFO - root - 2017-12-17 07:17:17.774821: step 3320, loss = 0.26, batch loss = 0.18 (38.2 examples/sec; 0.210 sec/batch; 19h:09m:34s remains)
INFO - root - 2017-12-17 07:17:19.890439: step 3330, loss = 0.25, batch loss = 0.16 (37.7 examples/sec; 0.212 sec/batch; 19h:24m:10s remains)
INFO - root - 2017-12-17 07:17:22.028724: step 3340, loss = 0.29, batch loss = 0.21 (38.4 examples/sec; 0.208 sec/batch; 19h:02m:16s remains)
INFO - root - 2017-12-17 07:17:24.164155: step 3350, loss = 0.23, batch loss = 0.15 (37.9 examples/sec; 0.211 sec/batch; 19h:17m:53s remains)
INFO - root - 2017-12-17 07:17:26.286486: step 3360, loss = 0.32, batch loss = 0.24 (38.0 examples/sec; 0.211 sec/batch; 19h:16m:22s remains)
INFO - root - 2017-12-17 07:17:28.407162: step 3370, loss = 0.33, batch loss = 0.25 (37.3 examples/sec; 0.214 sec/batch; 19h:35m:54s remains)
INFO - root - 2017-12-17 07:17:30.540075: step 3380, loss = 0.32, batch loss = 0.24 (36.6 examples/sec; 0.218 sec/batch; 19h:58m:09s remains)
INFO - root - 2017-12-17 07:17:32.689024: step 3390, loss = 0.35, batch loss = 0.26 (37.4 examples/sec; 0.214 sec/batch; 19h:34m:37s remains)
INFO - root - 2017-12-17 07:17:34.838398: step 3400, loss = 0.33, batch loss = 0.25 (36.8 examples/sec; 0.218 sec/batch; 19h:53m:52s remains)
INFO - root - 2017-12-17 07:17:37.073804: step 3410, loss = 0.31, batch loss = 0.23 (37.5 examples/sec; 0.213 sec/batch; 19h:29m:47s remains)
INFO - root - 2017-12-17 07:17:39.212404: step 3420, loss = 0.31, batch loss = 0.23 (36.9 examples/sec; 0.217 sec/batch; 19h:50m:31s remains)
INFO - root - 2017-12-17 07:17:41.348338: step 3430, loss = 0.23, batch loss = 0.15 (37.6 examples/sec; 0.213 sec/batch; 19h:28m:18s remains)
INFO - root - 2017-12-17 07:17:43.492246: step 3440, loss = 0.28, batch loss = 0.20 (36.4 examples/sec; 0.220 sec/batch; 20h:04m:43s remains)
INFO - root - 2017-12-17 07:17:45.646560: step 3450, loss = 0.24, batch loss = 0.15 (37.7 examples/sec; 0.212 sec/batch; 19h:24m:45s remains)
INFO - root - 2017-12-17 07:17:47.767059: step 3460, loss = 0.27, batch loss = 0.19 (37.5 examples/sec; 0.214 sec/batch; 19h:31m:06s remains)
INFO - root - 2017-12-17 07:17:49.904746: step 3470, loss = 0.31, batch loss = 0.23 (37.1 examples/sec; 0.216 sec/batch; 19h:42m:05s remains)
INFO - root - 2017-12-17 07:17:52.026030: step 3480, loss = 0.25, batch loss = 0.17 (37.8 examples/sec; 0.212 sec/batch; 19h:20m:12s remains)
INFO - root - 2017-12-17 07:17:54.198324: step 3490, loss = 0.35, batch loss = 0.27 (35.6 examples/sec; 0.225 sec/batch; 20h:33m:38s remains)
INFO - root - 2017-12-17 07:17:56.328149: step 3500, loss = 0.26, batch loss = 0.18 (38.0 examples/sec; 0.210 sec/batch; 19h:13m:50s remains)
INFO - root - 2017-12-17 07:17:58.582071: step 3510, loss = 0.27, batch loss = 0.19 (36.4 examples/sec; 0.220 sec/batch; 20h:05m:55s remains)
INFO - root - 2017-12-17 07:18:00.734340: step 3520, loss = 0.24, batch loss = 0.16 (38.6 examples/sec; 0.207 sec/batch; 18h:55m:05s remains)
INFO - root - 2017-12-17 07:18:02.880058: step 3530, loss = 0.26, batch loss = 0.17 (36.2 examples/sec; 0.221 sec/batch; 20h:12m:42s remains)
INFO - root - 2017-12-17 07:18:05.000026: step 3540, loss = 0.24, batch loss = 0.15 (38.5 examples/sec; 0.208 sec/batch; 18h:58m:56s remains)
INFO - root - 2017-12-17 07:18:07.147988: step 3550, loss = 0.35, batch loss = 0.26 (38.1 examples/sec; 0.210 sec/batch; 19h:12m:03s remains)
INFO - root - 2017-12-17 07:18:09.308057: step 3560, loss = 0.22, batch loss = 0.14 (36.8 examples/sec; 0.218 sec/batch; 19h:52m:45s remains)
INFO - root - 2017-12-17 07:18:11.440231: step 3570, loss = 0.33, batch loss = 0.25 (36.6 examples/sec; 0.218 sec/batch; 19h:56m:53s remains)
INFO - root - 2017-12-17 07:18:13.571386: step 3580, loss = 0.24, batch loss = 0.16 (37.3 examples/sec; 0.215 sec/batch; 19h:36m:28s remains)
INFO - root - 2017-12-17 07:18:15.694084: step 3590, loss = 0.27, batch loss = 0.18 (37.6 examples/sec; 0.213 sec/batch; 19h:26m:13s remains)
INFO - root - 2017-12-17 07:18:17.830230: step 3600, loss = 0.28, batch loss = 0.20 (36.1 examples/sec; 0.221 sec/batch; 20h:13m:09s remains)
INFO - root - 2017-12-17 07:18:20.072314: step 3610, loss = 0.28, batch loss = 0.20 (38.7 examples/sec; 0.207 sec/batch; 18h:53m:53s remains)
INFO - root - 2017-12-17 07:18:22.240158: step 3620, loss = 0.29, batch loss = 0.21 (35.8 examples/sec; 0.223 sec/batch; 20h:23m:41s remains)
INFO - root - 2017-12-17 07:18:24.362250: step 3630, loss = 0.26, batch loss = 0.17 (38.1 examples/sec; 0.210 sec/batch; 19h:11m:22s remains)
INFO - root - 2017-12-17 07:18:26.492958: step 3640, loss = 0.23, batch loss = 0.15 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:25s remains)
INFO - root - 2017-12-17 07:18:28.630845: step 3650, loss = 0.25, batch loss = 0.17 (38.1 examples/sec; 0.210 sec/batch; 19h:10m:28s remains)
INFO - root - 2017-12-17 07:18:30.771452: step 3660, loss = 0.28, batch loss = 0.20 (38.0 examples/sec; 0.210 sec/batch; 19h:12m:29s remains)
INFO - root - 2017-12-17 07:18:32.892683: step 3670, loss = 0.33, batch loss = 0.25 (37.9 examples/sec; 0.211 sec/batch; 19h:17m:31s remains)
INFO - root - 2017-12-17 07:18:35.027987: step 3680, loss = 0.30, batch loss = 0.22 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:23s remains)
INFO - root - 2017-12-17 07:18:37.172036: step 3690, loss = 0.38, batch loss = 0.30 (38.1 examples/sec; 0.210 sec/batch; 19h:11m:41s remains)
INFO - root - 2017-12-17 07:18:39.313075: step 3700, loss = 0.28, batch loss = 0.19 (37.8 examples/sec; 0.212 sec/batch; 19h:21m:18s remains)
INFO - root - 2017-12-17 07:18:41.566233: step 3710, loss = 0.29, batch loss = 0.21 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:02s remains)
INFO - root - 2017-12-17 07:18:43.702843: step 3720, loss = 0.31, batch loss = 0.23 (36.7 examples/sec; 0.218 sec/batch; 19h:55m:55s remains)
INFO - root - 2017-12-17 07:18:45.836308: step 3730, loss = 0.25, batch loss = 0.17 (38.6 examples/sec; 0.207 sec/batch; 18h:55m:41s remains)
INFO - root - 2017-12-17 07:18:47.988506: step 3740, loss = 0.24, batch loss = 0.16 (36.2 examples/sec; 0.221 sec/batch; 20h:10m:52s remains)
INFO - root - 2017-12-17 07:18:50.123432: step 3750, loss = 0.26, batch loss = 0.18 (36.8 examples/sec; 0.218 sec/batch; 19h:51m:56s remains)
INFO - root - 2017-12-17 07:18:52.280645: step 3760, loss = 0.30, batch loss = 0.22 (37.5 examples/sec; 0.213 sec/batch; 19h:27m:49s remains)
INFO - root - 2017-12-17 07:18:54.454262: step 3770, loss = 0.26, batch loss = 0.18 (38.3 examples/sec; 0.209 sec/batch; 19h:03m:10s remains)
